{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "%matplotlib inline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "import time\n",
    "import copy\n",
    "import os\n",
    "from queue import LifoQueue\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "from scipy.stats import kurtosis\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import network.cpc\n",
    "from network.cpc import CDCK2\n",
    "from utils.MatplotlibUtils import reduce_dims_and_plot\n",
    "from utils.ClassificationUtiols import onehot_coding\n",
    "from soft_decision_tree.sdt_model import SDT\n",
    "from sklearn import tree as tt\n",
    "\n",
    "# IPython.OutputArea.auto_scroll_threshold = 9999;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the model and the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load the model from: /home/eitan.k/EntangledExplainableClustering/knn_loss_batch_512_k_32/models/epoch_45.pt\n",
      "sensor names: (18 total)\n",
      "- speed\n",
      "- steering_angle\n",
      "- wheel_speed_0\n",
      "- wheel_speed_1\n",
      "- wheel_speed_2\n",
      "- wheel_speed_3\n",
      "- accelerometer_0\n",
      "- accelerometer_1\n",
      "- accelerometer_2\n",
      "- gyro_0\n",
      "- gyro_1\n",
      "- gyro_2\n",
      "- gyro_bias_0\n",
      "- gyro_bias_1\n",
      "- gyro_bias_2\n",
      "- gyro_uncalibrated_0\n",
      "- gyro_uncalibrated_1\n",
      "- gyro_uncalibrated_2\n",
      "Multihorizon size of the model: 30\n",
      "Test split ratio: 0.2\n",
      "Total number of windows in the dataset (without splitting): 101465\n"
     ]
    }
   ],
   "source": [
    "model_path = r'/home/eitan.k/EntangledExplainableClustering/knn_loss_batch_512_k_32/models/epoch_45.pt'\n",
    "dataset_path = r'/home/eitan.k/EntangledExplainableClustering/knn_loss_batch_512_k_32/data/test_data_protocol_4.file'\n",
    "\n",
    "print(f\"Load the model from: {model_path}\")\n",
    "model = torch.load(model_path, map_location='cpu')\n",
    "\n",
    "with open(dataset_path, 'rb') as fp:\n",
    "    dataset = pickle.load(fp)\n",
    "\n",
    "all_sensors = dataset.dataset.all_signals    \n",
    "print(f\"sensor names: ({len(all_sensors)} total)\")\n",
    "\n",
    "for s in all_sensors:\n",
    "    print(f\"- {s}\")\n",
    "    \n",
    "print(f\"Multihorizon size of the model: {model.timestep}\")\n",
    "print(f\"Test split ratio: {len(dataset) / len(dataset.dataset)}\")\n",
    "print(f\"Total number of windows in the dataset (without splitting): {len(dataset.dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 500\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Extract representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c2b0a0368184770be541bac77cce8bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=20293.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "projections = torch.tensor([])\n",
    "samples = torch.tensor([])\n",
    "device = 'cuda'\n",
    "model = model.to(device).eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    bar = tqdm(total=len(loader.dataset))\n",
    "    for batch in loader:\n",
    "        hidden = CDCK2.init_hidden(len(batch))\n",
    "        batch = batch.to(device)\n",
    "        hidden = hidden.to(device)\n",
    "\n",
    "        y = model.predict(batch, hidden).detach().cpu()\n",
    "        projections = torch.cat([projections, y.detach().cpu()])\n",
    "        samples = torch.cat([samples, batch.detach().cpu()])\n",
    "        bar.update(y.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit GMM and calculate indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ad25a5983304408bdf4fa509b72eeeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=75.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "best_score = float('inf')\n",
    "clusters = None\n",
    "range_ = list(range(5, 80))\n",
    "for k in tqdm(range_):\n",
    "    y = GaussianMixture(n_components=k).fit_predict(projections)\n",
    "    cur_score = davies_bouldin_score(projections, y)\n",
    "    scores.append(cur_score)\n",
    "    \n",
    "    if cur_score < best_score:\n",
    "        best_score = cur_score\n",
    "        clusters = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9}\n"
     ]
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('DB Score')\n",
    "plt.plot(range_, scores)\n",
    "best_k = range_[np.argmin(scores)]\n",
    "plt.axvline(best_k, color='r')\n",
    "plt.show()\n",
    "\n",
    "labels = set(clusters)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "distances = pairwise_distances(projections)\n",
    "# distances = np.triu(distances)\n",
    "distances_f = distances.flatten()\n",
    "\n",
    "plt.matshow(distances)\n",
    "plt.colorbar()\n",
    "plt.figure()\n",
    "plt.hist(distances_f[distances_f > 0], bins=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize with T-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "perplexity = 200\n",
    "\n",
    "p = reduce_dims_and_plot(projections,\n",
    "                         y=clusters,\n",
    "                         title=f'perplexity: {perplexity}',\n",
    "                         file_name=None,\n",
    "                         perplexity=perplexity,\n",
    "                         library='Multicore-TSNE',\n",
    "                         perform_PCA=False,\n",
    "                         projected=None,\n",
    "                         figure_type='2d',\n",
    "                         show_figure=True,\n",
    "                         close_figure=False,\n",
    "                         text=None)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Soft-Decision-Tree given the self-labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unnormalized_samples = samples.clone()\n",
    "\n",
    "# for col, sensor in enumerate(tqdm(dataset.dataset.all_signals)):\n",
    "#     denormalizer = dataset.dataset.get_denormalization_for_sensor(sensor)\n",
    "#     unnormalized_samples[:, col, :] = denormalizer(unnormalized_samples[:, col, :])\n",
    "\n",
    "sampled = samples[..., range(0, samples.shape[-1], 200)]\n",
    "\n",
    "samples_f = sampled.flatten(1)\n",
    "tree_dataset = list(zip(samples_f, clusters))\n",
    "batch_size = 2000\n",
    "tree_loader = torch.utils.data.DataLoader(tree_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define how we prune the weights of a node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_node(node_weights, factor=1):\n",
    "    w = node_weights.cpu().detach().numpy()\n",
    "    mean_ = np.mean(w)\n",
    "    std_ = np.std(w)\n",
    "    node_weights[((mean_ - std_ * factor) < node_weights) & (node_weights < (mean_ + std_ * factor))] = 0\n",
    "    return node_weights\n",
    "\n",
    "def prune_node_keep(node_weights, keep=4):\n",
    "    w = node_weights.cpu().detach().numpy()\n",
    "    throw_idx = np.argsort(abs(w))[:-keep]\n",
    "    node_weights[throw_idx] = 0\n",
    "    return node_weights\n",
    "\n",
    "def prune_tree(tree_, factor):\n",
    "    new_weights = tree_.inner_nodes.weight.clone()\n",
    "    for i in range(new_weights.shape[0]):\n",
    "        res = prune_node_keep(new_weights[i, :], factor)\n",
    "        new_weights[i, :] = res\n",
    "\n",
    "    with torch.no_grad():\n",
    "        tree_.inner_nodes.weight.copy_(new_weights)\n",
    "        \n",
    "def sparseness(x):\n",
    "    s = []\n",
    "    for i in range(x.shape[0]):\n",
    "        x_ = x[i, :]\n",
    "        sp = (len(x_) - torch.norm(x_, 0).item()) / len(x_)\n",
    "        s.append(sp)\n",
    "    return np.mean(s)\n",
    "\n",
    "def compute_regularization_by_level(tree):\n",
    "    total_reg = 0\n",
    "    for i in range(tree.inner_nodes.weight.shape[0]):\n",
    "        cur_level = np.floor(np.log2(i+1))\n",
    "        node_reg = torch.norm(tree.inner_nodes.weight[i].view(-1), 2)\n",
    "        total_reg += 2**(-cur_level) * node_reg\n",
    "    return total_reg\n",
    "\n",
    "def show_sparseness(tree):\n",
    "    avg_sp = sparseness(tree.inner_nodes.weight)\n",
    "    print(f\"Average sparseness: {avg_sp}\")\n",
    "    layer = 0\n",
    "    sps = []\n",
    "    for i in range(tree.inner_nodes.weight.shape[0]):\n",
    "        cur_layer = int(np.floor(np.log2(i+1)))\n",
    "        if cur_layer != layer:\n",
    "            print(f\"layer {layer}: {np.mean(sps)}\")\n",
    "            sps = []\n",
    "            layer = cur_layer\n",
    "\n",
    "        x_ = tree.inner_nodes.weight[i, :]\n",
    "        sp = (len(x_) - torch.norm(x_, 0).item()) / len(x_)\n",
    "        sps.append(sp)\n",
    "        \n",
    "    return avg_sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_epoch(model, loader, device, log_interval, losses, accs, epoch, iteration):\n",
    "    model = model.train()\n",
    "    for batch_idx, (data, target) in enumerate(loader):\n",
    "        iteration += 1\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        output, penalty = tree.forward(data)\n",
    "\n",
    "        # Loss\n",
    "        loss_tree = criterion(output, target.view(-1))\n",
    "\n",
    "        # Penalty\n",
    "        loss_tree += penalty\n",
    "\n",
    "        # Sparse regularization\n",
    "#         fc_params = torch.cat([x.view(-1) for x in tree.inner_nodes.parameters()])\n",
    "#         regularization = sparsity_lamda * torch.norm(fc_params, 2)\n",
    "        regularization = sparsity_lamda * compute_regularization_by_level(tree)\n",
    "        loss = loss_tree\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        pred = output.data.max(1)[1]\n",
    "        correct = pred.eq(target.view(-1).data).sum()\n",
    "        accs.append(correct.item() / data.size()[0])\n",
    "\n",
    "        # Print training status\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(f\"Epoch: {epoch:02d} | Batch: {batch_idx:03d} / {len(loader):03d} | Total loss: {loss.item():.3f} | Reg loss: {regularization.item():.3f} | Tree loss: {loss_tree.item():.3f} | Accuracy: {correct.item() / data.size()[0]:03f} | {round((time.time() - start_time) / iteration, 3)} sec/iter\")\n",
    "            \n",
    "    return iteration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 5e-3\n",
    "weight_decay = 5e-4\n",
    "sparsity_lamda = 2e-3\n",
    "epochs = 500\n",
    "output_dim = len(set(clusters))\n",
    "log_interval = 1\n",
    "tree_depth = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT accuracy: 0.9920169516582072\n"
     ]
    }
   ],
   "source": [
    "tree = SDT(input_dim=samples_f.shape[1], output_dim=len(labels), depth=tree_depth, lamda=1e-3, use_cuda=True)\n",
    "optimizer = torch.optim.Adam(tree.parameters(),\n",
    "                                 lr=lr,\n",
    "                                 weight_decay=weight_decay)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "tree = tree.to(device)\n",
    "clf = DecisionTreeClassifier(max_depth=tree_depth).fit(samples_f, clusters)\n",
    "print(f\"DT accuracy: {clf.score(samples_f, clusters)}\")\n",
    "tree.initialize_from_decision_tree(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "accs = []\n",
    "sparsity = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.2600465535771186\n",
      "layer 0: 0.988950276243094\n",
      "layer 1: 0.988950276243094\n",
      "layer 2: 0.988950276243094\n",
      "layer 3: 0.988950276243094\n",
      "layer 4: 0.9271408839779005\n",
      "layer 5: 0.7108080110497237\n",
      "layer 6: 0.587189226519337\n",
      "layer 7: 0.4558442679558012\n",
      "layer 8: 0.25496374309392267\n",
      "Epoch: 00 | Batch: 000 / 011 | Total loss: 2.300 | Reg loss: 0.019 | Tree loss: 2.300 | Accuracy: 0.102500 | 1.388 sec/iter\n",
      "Epoch: 00 | Batch: 001 / 011 | Total loss: 2.289 | Reg loss: 0.018 | Tree loss: 2.289 | Accuracy: 0.127500 | 1.206 sec/iter\n",
      "Epoch: 00 | Batch: 002 / 011 | Total loss: 2.277 | Reg loss: 0.018 | Tree loss: 2.277 | Accuracy: 0.239500 | 1.154 sec/iter\n",
      "Epoch: 00 | Batch: 003 / 011 | Total loss: 2.267 | Reg loss: 0.018 | Tree loss: 2.267 | Accuracy: 0.313500 | 1.121 sec/iter\n",
      "Epoch: 00 | Batch: 004 / 011 | Total loss: 2.262 | Reg loss: 0.018 | Tree loss: 2.262 | Accuracy: 0.337000 | 1.102 sec/iter\n",
      "Epoch: 00 | Batch: 005 / 011 | Total loss: 2.253 | Reg loss: 0.018 | Tree loss: 2.253 | Accuracy: 0.349500 | 1.092 sec/iter\n",
      "Epoch: 00 | Batch: 006 / 011 | Total loss: 2.245 | Reg loss: 0.017 | Tree loss: 2.245 | Accuracy: 0.327000 | 1.07 sec/iter\n",
      "Epoch: 00 | Batch: 007 / 011 | Total loss: 2.238 | Reg loss: 0.017 | Tree loss: 2.238 | Accuracy: 0.332500 | 1.045 sec/iter\n",
      "Epoch: 00 | Batch: 008 / 011 | Total loss: 2.226 | Reg loss: 0.017 | Tree loss: 2.226 | Accuracy: 0.364000 | 1.036 sec/iter\n",
      "Epoch: 00 | Batch: 009 / 011 | Total loss: 2.214 | Reg loss: 0.017 | Tree loss: 2.214 | Accuracy: 0.388000 | 1.036 sec/iter\n",
      "Epoch: 00 | Batch: 010 / 011 | Total loss: 2.220 | Reg loss: 0.017 | Tree loss: 2.220 | Accuracy: 0.327645 | 1.023 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 01 | Batch: 000 / 011 | Total loss: 2.288 | Reg loss: 0.016 | Tree loss: 2.288 | Accuracy: 0.186500 | 1.221 sec/iter\n",
      "Epoch: 01 | Batch: 001 / 011 | Total loss: 2.279 | Reg loss: 0.016 | Tree loss: 2.279 | Accuracy: 0.196000 | 1.2 sec/iter\n",
      "Epoch: 01 | Batch: 002 / 011 | Total loss: 2.269 | Reg loss: 0.016 | Tree loss: 2.269 | Accuracy: 0.289000 | 1.19 sec/iter\n",
      "Epoch: 01 | Batch: 003 / 011 | Total loss: 2.262 | Reg loss: 0.016 | Tree loss: 2.262 | Accuracy: 0.340000 | 1.179 sec/iter\n",
      "Epoch: 01 | Batch: 004 / 011 | Total loss: 2.246 | Reg loss: 0.016 | Tree loss: 2.246 | Accuracy: 0.365000 | 1.169 sec/iter\n",
      "Epoch: 01 | Batch: 005 / 011 | Total loss: 2.239 | Reg loss: 0.016 | Tree loss: 2.239 | Accuracy: 0.371500 | 1.162 sec/iter\n",
      "Epoch: 01 | Batch: 006 / 011 | Total loss: 2.225 | Reg loss: 0.016 | Tree loss: 2.225 | Accuracy: 0.408500 | 1.155 sec/iter\n",
      "Epoch: 01 | Batch: 007 / 011 | Total loss: 2.216 | Reg loss: 0.017 | Tree loss: 2.216 | Accuracy: 0.404000 | 1.142 sec/iter\n",
      "Epoch: 01 | Batch: 008 / 011 | Total loss: 2.208 | Reg loss: 0.017 | Tree loss: 2.208 | Accuracy: 0.403500 | 1.13 sec/iter\n",
      "Epoch: 01 | Batch: 009 / 011 | Total loss: 2.197 | Reg loss: 0.017 | Tree loss: 2.197 | Accuracy: 0.401500 | 1.126 sec/iter\n",
      "Epoch: 01 | Batch: 010 / 011 | Total loss: 2.187 | Reg loss: 0.017 | Tree loss: 2.187 | Accuracy: 0.392491 | 1.115 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 02 | Batch: 000 / 011 | Total loss: 2.275 | Reg loss: 0.016 | Tree loss: 2.275 | Accuracy: 0.270000 | 1.225 sec/iter\n",
      "Epoch: 02 | Batch: 001 / 011 | Total loss: 2.265 | Reg loss: 0.016 | Tree loss: 2.265 | Accuracy: 0.340500 | 1.209 sec/iter\n",
      "Epoch: 02 | Batch: 002 / 011 | Total loss: 2.256 | Reg loss: 0.016 | Tree loss: 2.256 | Accuracy: 0.362500 | 1.2 sec/iter\n",
      "Epoch: 02 | Batch: 003 / 011 | Total loss: 2.241 | Reg loss: 0.016 | Tree loss: 2.241 | Accuracy: 0.442000 | 1.189 sec/iter\n",
      "Epoch: 02 | Batch: 004 / 011 | Total loss: 2.232 | Reg loss: 0.016 | Tree loss: 2.232 | Accuracy: 0.411500 | 1.178 sec/iter\n",
      "Epoch: 02 | Batch: 005 / 011 | Total loss: 2.220 | Reg loss: 0.016 | Tree loss: 2.220 | Accuracy: 0.374000 | 1.168 sec/iter\n",
      "Epoch: 02 | Batch: 006 / 011 | Total loss: 2.211 | Reg loss: 0.016 | Tree loss: 2.211 | Accuracy: 0.357000 | 1.159 sec/iter\n",
      "Epoch: 02 | Batch: 007 / 011 | Total loss: 2.198 | Reg loss: 0.017 | Tree loss: 2.198 | Accuracy: 0.337500 | 1.15 sec/iter\n",
      "Epoch: 02 | Batch: 008 / 011 | Total loss: 2.188 | Reg loss: 0.017 | Tree loss: 2.188 | Accuracy: 0.363500 | 1.142 sec/iter\n",
      "Epoch: 02 | Batch: 009 / 011 | Total loss: 2.180 | Reg loss: 0.017 | Tree loss: 2.180 | Accuracy: 0.358500 | 1.133 sec/iter\n",
      "Epoch: 02 | Batch: 010 / 011 | Total loss: 2.177 | Reg loss: 0.017 | Tree loss: 2.177 | Accuracy: 0.382253 | 1.126 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 03 | Batch: 000 / 011 | Total loss: 2.268 | Reg loss: 0.016 | Tree loss: 2.268 | Accuracy: 0.252000 | 1.217 sec/iter\n",
      "Epoch: 03 | Batch: 001 / 011 | Total loss: 2.257 | Reg loss: 0.016 | Tree loss: 2.257 | Accuracy: 0.389500 | 1.21 sec/iter\n",
      "Epoch: 03 | Batch: 002 / 011 | Total loss: 2.247 | Reg loss: 0.016 | Tree loss: 2.247 | Accuracy: 0.362500 | 1.204 sec/iter\n",
      "Epoch: 03 | Batch: 003 / 011 | Total loss: 2.233 | Reg loss: 0.016 | Tree loss: 2.233 | Accuracy: 0.347500 | 1.199 sec/iter\n",
      "Epoch: 03 | Batch: 004 / 011 | Total loss: 2.216 | Reg loss: 0.016 | Tree loss: 2.216 | Accuracy: 0.403000 | 1.195 sec/iter\n",
      "Epoch: 03 | Batch: 005 / 011 | Total loss: 2.205 | Reg loss: 0.016 | Tree loss: 2.205 | Accuracy: 0.401000 | 1.192 sec/iter\n",
      "Epoch: 03 | Batch: 006 / 011 | Total loss: 2.192 | Reg loss: 0.017 | Tree loss: 2.192 | Accuracy: 0.396000 | 1.188 sec/iter\n",
      "Epoch: 03 | Batch: 007 / 011 | Total loss: 2.178 | Reg loss: 0.017 | Tree loss: 2.178 | Accuracy: 0.392000 | 1.18 sec/iter\n",
      "Epoch: 03 | Batch: 008 / 011 | Total loss: 2.170 | Reg loss: 0.017 | Tree loss: 2.170 | Accuracy: 0.371000 | 1.172 sec/iter\n",
      "Epoch: 03 | Batch: 009 / 011 | Total loss: 2.153 | Reg loss: 0.017 | Tree loss: 2.153 | Accuracy: 0.380500 | 1.169 sec/iter\n",
      "Epoch: 03 | Batch: 010 / 011 | Total loss: 2.146 | Reg loss: 0.018 | Tree loss: 2.146 | Accuracy: 0.365188 | 1.164 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 04 | Batch: 000 / 011 | Total loss: 2.261 | Reg loss: 0.016 | Tree loss: 2.261 | Accuracy: 0.270000 | 1.217 sec/iter\n",
      "Epoch: 04 | Batch: 001 / 011 | Total loss: 2.248 | Reg loss: 0.016 | Tree loss: 2.248 | Accuracy: 0.412500 | 1.213 sec/iter\n",
      "Epoch: 04 | Batch: 002 / 011 | Total loss: 2.232 | Reg loss: 0.016 | Tree loss: 2.232 | Accuracy: 0.447000 | 1.209 sec/iter\n",
      "Epoch: 04 | Batch: 003 / 011 | Total loss: 2.213 | Reg loss: 0.016 | Tree loss: 2.213 | Accuracy: 0.441000 | 1.206 sec/iter\n",
      "Epoch: 04 | Batch: 004 / 011 | Total loss: 2.196 | Reg loss: 0.016 | Tree loss: 2.196 | Accuracy: 0.396500 | 1.202 sec/iter\n",
      "Epoch: 04 | Batch: 005 / 011 | Total loss: 2.178 | Reg loss: 0.017 | Tree loss: 2.178 | Accuracy: 0.375500 | 1.199 sec/iter\n",
      "Epoch: 04 | Batch: 006 / 011 | Total loss: 2.165 | Reg loss: 0.017 | Tree loss: 2.165 | Accuracy: 0.362500 | 1.194 sec/iter\n",
      "Epoch: 04 | Batch: 007 / 011 | Total loss: 2.153 | Reg loss: 0.017 | Tree loss: 2.153 | Accuracy: 0.368000 | 1.188 sec/iter\n",
      "Epoch: 04 | Batch: 008 / 011 | Total loss: 2.139 | Reg loss: 0.017 | Tree loss: 2.139 | Accuracy: 0.378000 | 1.186 sec/iter\n",
      "Epoch: 04 | Batch: 009 / 011 | Total loss: 2.131 | Reg loss: 0.018 | Tree loss: 2.131 | Accuracy: 0.376500 | 1.184 sec/iter\n",
      "Epoch: 04 | Batch: 010 / 011 | Total loss: 2.116 | Reg loss: 0.018 | Tree loss: 2.116 | Accuracy: 0.372014 | 1.179 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 05 | Batch: 000 / 011 | Total loss: 2.250 | Reg loss: 0.016 | Tree loss: 2.250 | Accuracy: 0.293000 | 1.215 sec/iter\n",
      "Epoch: 05 | Batch: 001 / 011 | Total loss: 2.236 | Reg loss: 0.016 | Tree loss: 2.236 | Accuracy: 0.407000 | 1.21 sec/iter\n",
      "Epoch: 05 | Batch: 002 / 011 | Total loss: 2.223 | Reg loss: 0.017 | Tree loss: 2.223 | Accuracy: 0.423000 | 1.206 sec/iter\n",
      "Epoch: 05 | Batch: 003 / 011 | Total loss: 2.204 | Reg loss: 0.017 | Tree loss: 2.204 | Accuracy: 0.421000 | 1.203 sec/iter\n",
      "Epoch: 05 | Batch: 004 / 011 | Total loss: 2.178 | Reg loss: 0.017 | Tree loss: 2.178 | Accuracy: 0.389000 | 1.201 sec/iter\n",
      "Epoch: 05 | Batch: 005 / 011 | Total loss: 2.162 | Reg loss: 0.017 | Tree loss: 2.162 | Accuracy: 0.388500 | 1.198 sec/iter\n",
      "Epoch: 05 | Batch: 006 / 011 | Total loss: 2.136 | Reg loss: 0.017 | Tree loss: 2.136 | Accuracy: 0.388000 | 1.195 sec/iter\n",
      "Epoch: 05 | Batch: 007 / 011 | Total loss: 2.115 | Reg loss: 0.018 | Tree loss: 2.115 | Accuracy: 0.383500 | 1.191 sec/iter\n",
      "Epoch: 05 | Batch: 008 / 011 | Total loss: 2.102 | Reg loss: 0.018 | Tree loss: 2.102 | Accuracy: 0.374500 | 1.186 sec/iter\n",
      "Epoch: 05 | Batch: 009 / 011 | Total loss: 2.093 | Reg loss: 0.018 | Tree loss: 2.093 | Accuracy: 0.378500 | 1.184 sec/iter\n",
      "Epoch: 05 | Batch: 010 / 011 | Total loss: 2.087 | Reg loss: 0.018 | Tree loss: 2.087 | Accuracy: 0.423208 | 1.179 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 06 | Batch: 000 / 011 | Total loss: 2.241 | Reg loss: 0.017 | Tree loss: 2.241 | Accuracy: 0.269500 | 1.217 sec/iter\n",
      "Epoch: 06 | Batch: 001 / 011 | Total loss: 2.223 | Reg loss: 0.017 | Tree loss: 2.223 | Accuracy: 0.385000 | 1.213 sec/iter\n",
      "Epoch: 06 | Batch: 002 / 011 | Total loss: 2.201 | Reg loss: 0.017 | Tree loss: 2.201 | Accuracy: 0.449500 | 1.21 sec/iter\n",
      "Epoch: 06 | Batch: 003 / 011 | Total loss: 2.176 | Reg loss: 0.017 | Tree loss: 2.176 | Accuracy: 0.441000 | 1.207 sec/iter\n",
      "Epoch: 06 | Batch: 004 / 011 | Total loss: 2.149 | Reg loss: 0.017 | Tree loss: 2.149 | Accuracy: 0.410000 | 1.205 sec/iter\n",
      "Epoch: 06 | Batch: 005 / 011 | Total loss: 2.124 | Reg loss: 0.018 | Tree loss: 2.124 | Accuracy: 0.400500 | 1.203 sec/iter\n",
      "Epoch: 06 | Batch: 006 / 011 | Total loss: 2.111 | Reg loss: 0.018 | Tree loss: 2.111 | Accuracy: 0.401000 | 1.201 sec/iter\n",
      "Epoch: 06 | Batch: 007 / 011 | Total loss: 2.080 | Reg loss: 0.018 | Tree loss: 2.080 | Accuracy: 0.422500 | 1.197 sec/iter\n",
      "Epoch: 06 | Batch: 008 / 011 | Total loss: 2.073 | Reg loss: 0.018 | Tree loss: 2.073 | Accuracy: 0.391000 | 1.193 sec/iter\n",
      "Epoch: 06 | Batch: 009 / 011 | Total loss: 2.047 | Reg loss: 0.019 | Tree loss: 2.047 | Accuracy: 0.370000 | 1.192 sec/iter\n",
      "Epoch: 06 | Batch: 010 / 011 | Total loss: 2.030 | Reg loss: 0.019 | Tree loss: 2.030 | Accuracy: 0.372014 | 1.188 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 07 | Batch: 000 / 011 | Total loss: 2.228 | Reg loss: 0.017 | Tree loss: 2.228 | Accuracy: 0.299500 | 1.219 sec/iter\n",
      "Epoch: 07 | Batch: 001 / 011 | Total loss: 2.208 | Reg loss: 0.017 | Tree loss: 2.208 | Accuracy: 0.381500 | 1.214 sec/iter\n",
      "Epoch: 07 | Batch: 002 / 011 | Total loss: 2.181 | Reg loss: 0.018 | Tree loss: 2.181 | Accuracy: 0.452500 | 1.212 sec/iter\n",
      "Epoch: 07 | Batch: 003 / 011 | Total loss: 2.147 | Reg loss: 0.018 | Tree loss: 2.147 | Accuracy: 0.476500 | 1.21 sec/iter\n",
      "Epoch: 07 | Batch: 004 / 011 | Total loss: 2.119 | Reg loss: 0.018 | Tree loss: 2.119 | Accuracy: 0.410000 | 1.208 sec/iter\n",
      "Epoch: 07 | Batch: 005 / 011 | Total loss: 2.090 | Reg loss: 0.018 | Tree loss: 2.090 | Accuracy: 0.413500 | 1.206 sec/iter\n",
      "Epoch: 07 | Batch: 006 / 011 | Total loss: 2.068 | Reg loss: 0.018 | Tree loss: 2.068 | Accuracy: 0.408000 | 1.203 sec/iter\n",
      "Epoch: 07 | Batch: 007 / 011 | Total loss: 2.048 | Reg loss: 0.019 | Tree loss: 2.048 | Accuracy: 0.359000 | 1.199 sec/iter\n",
      "Epoch: 07 | Batch: 008 / 011 | Total loss: 2.038 | Reg loss: 0.019 | Tree loss: 2.038 | Accuracy: 0.364000 | 1.195 sec/iter\n",
      "Epoch: 07 | Batch: 009 / 011 | Total loss: 2.016 | Reg loss: 0.019 | Tree loss: 2.016 | Accuracy: 0.360500 | 1.193 sec/iter\n",
      "Epoch: 07 | Batch: 010 / 011 | Total loss: 2.009 | Reg loss: 0.019 | Tree loss: 2.009 | Accuracy: 0.361775 | 1.189 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 08 | Batch: 000 / 011 | Total loss: 2.214 | Reg loss: 0.018 | Tree loss: 2.214 | Accuracy: 0.296500 | 1.192 sec/iter\n",
      "Epoch: 08 | Batch: 001 / 011 | Total loss: 2.184 | Reg loss: 0.018 | Tree loss: 2.184 | Accuracy: 0.370500 | 1.188 sec/iter\n",
      "Epoch: 08 | Batch: 002 / 011 | Total loss: 2.161 | Reg loss: 0.018 | Tree loss: 2.161 | Accuracy: 0.386500 | 1.184 sec/iter\n",
      "Epoch: 08 | Batch: 003 / 011 | Total loss: 2.127 | Reg loss: 0.018 | Tree loss: 2.127 | Accuracy: 0.408000 | 1.181 sec/iter\n",
      "Epoch: 08 | Batch: 004 / 011 | Total loss: 2.090 | Reg loss: 0.018 | Tree loss: 2.090 | Accuracy: 0.435000 | 1.178 sec/iter\n",
      "Epoch: 08 | Batch: 005 / 011 | Total loss: 2.051 | Reg loss: 0.019 | Tree loss: 2.051 | Accuracy: 0.432000 | 1.175 sec/iter\n",
      "Epoch: 08 | Batch: 006 / 011 | Total loss: 2.020 | Reg loss: 0.019 | Tree loss: 2.020 | Accuracy: 0.396500 | 1.174 sec/iter\n",
      "Epoch: 08 | Batch: 007 / 011 | Total loss: 1.991 | Reg loss: 0.019 | Tree loss: 1.991 | Accuracy: 0.383500 | 1.172 sec/iter\n",
      "Epoch: 08 | Batch: 008 / 011 | Total loss: 1.979 | Reg loss: 0.019 | Tree loss: 1.979 | Accuracy: 0.370000 | 1.171 sec/iter\n",
      "Epoch: 08 | Batch: 009 / 011 | Total loss: 1.978 | Reg loss: 0.020 | Tree loss: 1.978 | Accuracy: 0.389500 | 1.17 sec/iter\n",
      "Epoch: 08 | Batch: 010 / 011 | Total loss: 1.947 | Reg loss: 0.020 | Tree loss: 1.947 | Accuracy: 0.372014 | 1.167 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 09 | Batch: 000 / 011 | Total loss: 2.193 | Reg loss: 0.018 | Tree loss: 2.193 | Accuracy: 0.318500 | 1.175 sec/iter\n",
      "Epoch: 09 | Batch: 001 / 011 | Total loss: 2.165 | Reg loss: 0.018 | Tree loss: 2.165 | Accuracy: 0.378000 | 1.173 sec/iter\n",
      "Epoch: 09 | Batch: 002 / 011 | Total loss: 2.136 | Reg loss: 0.019 | Tree loss: 2.136 | Accuracy: 0.368000 | 1.171 sec/iter\n",
      "Epoch: 09 | Batch: 003 / 011 | Total loss: 2.086 | Reg loss: 0.019 | Tree loss: 2.086 | Accuracy: 0.423500 | 1.169 sec/iter\n",
      "Epoch: 09 | Batch: 004 / 011 | Total loss: 2.050 | Reg loss: 0.019 | Tree loss: 2.050 | Accuracy: 0.412000 | 1.168 sec/iter\n",
      "Epoch: 09 | Batch: 005 / 011 | Total loss: 2.022 | Reg loss: 0.019 | Tree loss: 2.022 | Accuracy: 0.411500 | 1.167 sec/iter\n",
      "Epoch: 09 | Batch: 006 / 011 | Total loss: 1.984 | Reg loss: 0.019 | Tree loss: 1.984 | Accuracy: 0.396500 | 1.165 sec/iter\n",
      "Epoch: 09 | Batch: 007 / 011 | Total loss: 1.961 | Reg loss: 0.020 | Tree loss: 1.961 | Accuracy: 0.387500 | 1.162 sec/iter\n",
      "Epoch: 09 | Batch: 008 / 011 | Total loss: 1.934 | Reg loss: 0.020 | Tree loss: 1.934 | Accuracy: 0.378500 | 1.161 sec/iter\n",
      "Epoch: 09 | Batch: 009 / 011 | Total loss: 1.916 | Reg loss: 0.020 | Tree loss: 1.916 | Accuracy: 0.387500 | 1.16 sec/iter\n",
      "Epoch: 09 | Batch: 010 / 011 | Total loss: 1.902 | Reg loss: 0.020 | Tree loss: 1.902 | Accuracy: 0.382253 | 1.157 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 10 | Batch: 000 / 011 | Total loss: 2.173 | Reg loss: 0.019 | Tree loss: 2.173 | Accuracy: 0.321500 | 1.176 sec/iter\n",
      "Epoch: 10 | Batch: 001 / 011 | Total loss: 2.141 | Reg loss: 0.019 | Tree loss: 2.141 | Accuracy: 0.342000 | 1.174 sec/iter\n",
      "Epoch: 10 | Batch: 002 / 011 | Total loss: 2.104 | Reg loss: 0.019 | Tree loss: 2.104 | Accuracy: 0.313000 | 1.173 sec/iter\n",
      "Epoch: 10 | Batch: 003 / 011 | Total loss: 2.056 | Reg loss: 0.019 | Tree loss: 2.056 | Accuracy: 0.395500 | 1.172 sec/iter\n",
      "Epoch: 10 | Batch: 004 / 011 | Total loss: 2.007 | Reg loss: 0.019 | Tree loss: 2.007 | Accuracy: 0.429500 | 1.171 sec/iter\n",
      "Epoch: 10 | Batch: 005 / 011 | Total loss: 1.971 | Reg loss: 0.020 | Tree loss: 1.971 | Accuracy: 0.412500 | 1.17 sec/iter\n",
      "Epoch: 10 | Batch: 006 / 011 | Total loss: 1.933 | Reg loss: 0.020 | Tree loss: 1.933 | Accuracy: 0.419000 | 1.168 sec/iter\n",
      "Epoch: 10 | Batch: 007 / 011 | Total loss: 1.918 | Reg loss: 0.020 | Tree loss: 1.918 | Accuracy: 0.385500 | 1.166 sec/iter\n",
      "Epoch: 10 | Batch: 008 / 011 | Total loss: 1.913 | Reg loss: 0.020 | Tree loss: 1.913 | Accuracy: 0.367000 | 1.164 sec/iter\n",
      "Epoch: 10 | Batch: 009 / 011 | Total loss: 1.880 | Reg loss: 0.021 | Tree loss: 1.880 | Accuracy: 0.387500 | 1.163 sec/iter\n",
      "Epoch: 10 | Batch: 010 / 011 | Total loss: 1.834 | Reg loss: 0.021 | Tree loss: 1.834 | Accuracy: 0.426621 | 1.161 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 11 | Batch: 000 / 011 | Total loss: 2.153 | Reg loss: 0.019 | Tree loss: 2.153 | Accuracy: 0.316500 | 1.178 sec/iter\n",
      "Epoch: 11 | Batch: 001 / 011 | Total loss: 2.115 | Reg loss: 0.019 | Tree loss: 2.115 | Accuracy: 0.324500 | 1.176 sec/iter\n",
      "Epoch: 11 | Batch: 002 / 011 | Total loss: 2.065 | Reg loss: 0.019 | Tree loss: 2.065 | Accuracy: 0.327500 | 1.175 sec/iter\n",
      "Epoch: 11 | Batch: 003 / 011 | Total loss: 2.020 | Reg loss: 0.020 | Tree loss: 2.020 | Accuracy: 0.365500 | 1.174 sec/iter\n",
      "Epoch: 11 | Batch: 004 / 011 | Total loss: 1.987 | Reg loss: 0.020 | Tree loss: 1.987 | Accuracy: 0.386000 | 1.173 sec/iter\n",
      "Epoch: 11 | Batch: 005 / 011 | Total loss: 1.941 | Reg loss: 0.020 | Tree loss: 1.941 | Accuracy: 0.414000 | 1.172 sec/iter\n",
      "Epoch: 11 | Batch: 006 / 011 | Total loss: 1.906 | Reg loss: 0.020 | Tree loss: 1.906 | Accuracy: 0.423000 | 1.171 sec/iter\n",
      "Epoch: 11 | Batch: 007 / 011 | Total loss: 1.893 | Reg loss: 0.020 | Tree loss: 1.893 | Accuracy: 0.421000 | 1.169 sec/iter\n",
      "Epoch: 11 | Batch: 008 / 011 | Total loss: 1.862 | Reg loss: 0.021 | Tree loss: 1.862 | Accuracy: 0.395000 | 1.167 sec/iter\n",
      "Epoch: 11 | Batch: 009 / 011 | Total loss: 1.841 | Reg loss: 0.021 | Tree loss: 1.841 | Accuracy: 0.396000 | 1.166 sec/iter\n",
      "Epoch: 11 | Batch: 010 / 011 | Total loss: 1.829 | Reg loss: 0.021 | Tree loss: 1.829 | Accuracy: 0.423208 | 1.164 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 12 | Batch: 000 / 011 | Total loss: 2.128 | Reg loss: 0.020 | Tree loss: 2.128 | Accuracy: 0.324000 | 1.184 sec/iter\n",
      "Epoch: 12 | Batch: 001 / 011 | Total loss: 2.089 | Reg loss: 0.020 | Tree loss: 2.089 | Accuracy: 0.330500 | 1.182 sec/iter\n",
      "Epoch: 12 | Batch: 002 / 011 | Total loss: 2.034 | Reg loss: 0.020 | Tree loss: 2.034 | Accuracy: 0.360500 | 1.181 sec/iter\n",
      "Epoch: 12 | Batch: 003 / 011 | Total loss: 1.986 | Reg loss: 0.020 | Tree loss: 1.986 | Accuracy: 0.420000 | 1.18 sec/iter\n",
      "Epoch: 12 | Batch: 004 / 011 | Total loss: 1.940 | Reg loss: 0.020 | Tree loss: 1.940 | Accuracy: 0.468000 | 1.179 sec/iter\n",
      "Epoch: 12 | Batch: 005 / 011 | Total loss: 1.896 | Reg loss: 0.020 | Tree loss: 1.896 | Accuracy: 0.464500 | 1.178 sec/iter\n",
      "Epoch: 12 | Batch: 006 / 011 | Total loss: 1.859 | Reg loss: 0.021 | Tree loss: 1.859 | Accuracy: 0.449000 | 1.177 sec/iter\n",
      "Epoch: 12 | Batch: 007 / 011 | Total loss: 1.842 | Reg loss: 0.021 | Tree loss: 1.842 | Accuracy: 0.422500 | 1.175 sec/iter\n",
      "Epoch: 12 | Batch: 008 / 011 | Total loss: 1.823 | Reg loss: 0.021 | Tree loss: 1.823 | Accuracy: 0.394000 | 1.172 sec/iter\n",
      "Epoch: 12 | Batch: 009 / 011 | Total loss: 1.809 | Reg loss: 0.021 | Tree loss: 1.809 | Accuracy: 0.400000 | 1.171 sec/iter\n",
      "Epoch: 12 | Batch: 010 / 011 | Total loss: 1.809 | Reg loss: 0.021 | Tree loss: 1.809 | Accuracy: 0.440273 | 1.169 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 13 | Batch: 000 / 011 | Total loss: 2.103 | Reg loss: 0.020 | Tree loss: 2.103 | Accuracy: 0.339000 | 1.187 sec/iter\n",
      "Epoch: 13 | Batch: 001 / 011 | Total loss: 2.054 | Reg loss: 0.020 | Tree loss: 2.054 | Accuracy: 0.334000 | 1.185 sec/iter\n",
      "Epoch: 13 | Batch: 002 / 011 | Total loss: 2.001 | Reg loss: 0.020 | Tree loss: 2.001 | Accuracy: 0.337500 | 1.184 sec/iter\n",
      "Epoch: 13 | Batch: 003 / 011 | Total loss: 1.954 | Reg loss: 0.020 | Tree loss: 1.954 | Accuracy: 0.391000 | 1.183 sec/iter\n",
      "Epoch: 13 | Batch: 004 / 011 | Total loss: 1.897 | Reg loss: 0.021 | Tree loss: 1.897 | Accuracy: 0.457500 | 1.182 sec/iter\n",
      "Epoch: 13 | Batch: 005 / 011 | Total loss: 1.865 | Reg loss: 0.021 | Tree loss: 1.865 | Accuracy: 0.469500 | 1.181 sec/iter\n",
      "Epoch: 13 | Batch: 006 / 011 | Total loss: 1.825 | Reg loss: 0.021 | Tree loss: 1.825 | Accuracy: 0.465000 | 1.18 sec/iter\n",
      "Epoch: 13 | Batch: 007 / 011 | Total loss: 1.793 | Reg loss: 0.021 | Tree loss: 1.793 | Accuracy: 0.462500 | 1.178 sec/iter\n",
      "Epoch: 13 | Batch: 008 / 011 | Total loss: 1.791 | Reg loss: 0.021 | Tree loss: 1.791 | Accuracy: 0.436500 | 1.176 sec/iter\n",
      "Epoch: 13 | Batch: 009 / 011 | Total loss: 1.776 | Reg loss: 0.021 | Tree loss: 1.776 | Accuracy: 0.420000 | 1.175 sec/iter\n",
      "Epoch: 13 | Batch: 010 / 011 | Total loss: 1.778 | Reg loss: 0.022 | Tree loss: 1.778 | Accuracy: 0.385666 | 1.173 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 14 | Batch: 000 / 011 | Total loss: 2.061 | Reg loss: 0.021 | Tree loss: 2.061 | Accuracy: 0.359000 | 1.173 sec/iter\n",
      "Epoch: 14 | Batch: 001 / 011 | Total loss: 2.023 | Reg loss: 0.021 | Tree loss: 2.023 | Accuracy: 0.349000 | 1.171 sec/iter\n",
      "Epoch: 14 | Batch: 002 / 011 | Total loss: 1.969 | Reg loss: 0.021 | Tree loss: 1.969 | Accuracy: 0.359000 | 1.169 sec/iter\n",
      "Epoch: 14 | Batch: 003 / 011 | Total loss: 1.924 | Reg loss: 0.021 | Tree loss: 1.924 | Accuracy: 0.349000 | 1.167 sec/iter\n",
      "Epoch: 14 | Batch: 004 / 011 | Total loss: 1.880 | Reg loss: 0.021 | Tree loss: 1.880 | Accuracy: 0.385500 | 1.165 sec/iter\n",
      "Epoch: 14 | Batch: 005 / 011 | Total loss: 1.821 | Reg loss: 0.021 | Tree loss: 1.821 | Accuracy: 0.449000 | 1.164 sec/iter\n",
      "Epoch: 14 | Batch: 006 / 011 | Total loss: 1.790 | Reg loss: 0.021 | Tree loss: 1.790 | Accuracy: 0.476500 | 1.163 sec/iter\n",
      "Epoch: 14 | Batch: 007 / 011 | Total loss: 1.747 | Reg loss: 0.021 | Tree loss: 1.747 | Accuracy: 0.483000 | 1.162 sec/iter\n",
      "Epoch: 14 | Batch: 008 / 011 | Total loss: 1.736 | Reg loss: 0.022 | Tree loss: 1.736 | Accuracy: 0.446000 | 1.161 sec/iter\n",
      "Epoch: 14 | Batch: 009 / 011 | Total loss: 1.749 | Reg loss: 0.022 | Tree loss: 1.749 | Accuracy: 0.434500 | 1.161 sec/iter\n",
      "Epoch: 14 | Batch: 010 / 011 | Total loss: 1.761 | Reg loss: 0.022 | Tree loss: 1.761 | Accuracy: 0.399317 | 1.159 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 15 | Batch: 000 / 011 | Total loss: 2.041 | Reg loss: 0.021 | Tree loss: 2.041 | Accuracy: 0.345500 | 1.164 sec/iter\n",
      "Epoch: 15 | Batch: 001 / 011 | Total loss: 1.992 | Reg loss: 0.021 | Tree loss: 1.992 | Accuracy: 0.358500 | 1.164 sec/iter\n",
      "Epoch: 15 | Batch: 002 / 011 | Total loss: 1.932 | Reg loss: 0.021 | Tree loss: 1.932 | Accuracy: 0.365000 | 1.163 sec/iter\n",
      "Epoch: 15 | Batch: 003 / 011 | Total loss: 1.873 | Reg loss: 0.021 | Tree loss: 1.873 | Accuracy: 0.381500 | 1.162 sec/iter\n",
      "Epoch: 15 | Batch: 004 / 011 | Total loss: 1.832 | Reg loss: 0.021 | Tree loss: 1.832 | Accuracy: 0.421000 | 1.162 sec/iter\n",
      "Epoch: 15 | Batch: 005 / 011 | Total loss: 1.784 | Reg loss: 0.021 | Tree loss: 1.784 | Accuracy: 0.486000 | 1.161 sec/iter\n",
      "Epoch: 15 | Batch: 006 / 011 | Total loss: 1.757 | Reg loss: 0.022 | Tree loss: 1.757 | Accuracy: 0.477000 | 1.16 sec/iter\n",
      "Epoch: 15 | Batch: 007 / 011 | Total loss: 1.738 | Reg loss: 0.022 | Tree loss: 1.738 | Accuracy: 0.462500 | 1.158 sec/iter\n",
      "Epoch: 15 | Batch: 008 / 011 | Total loss: 1.708 | Reg loss: 0.022 | Tree loss: 1.708 | Accuracy: 0.466000 | 1.158 sec/iter\n",
      "Epoch: 15 | Batch: 009 / 011 | Total loss: 1.722 | Reg loss: 0.022 | Tree loss: 1.722 | Accuracy: 0.445000 | 1.157 sec/iter\n",
      "Epoch: 15 | Batch: 010 / 011 | Total loss: 1.724 | Reg loss: 0.022 | Tree loss: 1.724 | Accuracy: 0.416382 | 1.156 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 16 | Batch: 000 / 011 | Total loss: 2.012 | Reg loss: 0.021 | Tree loss: 2.012 | Accuracy: 0.336500 | 1.166 sec/iter\n",
      "Epoch: 16 | Batch: 001 / 011 | Total loss: 1.963 | Reg loss: 0.021 | Tree loss: 1.963 | Accuracy: 0.354500 | 1.165 sec/iter\n",
      "Epoch: 16 | Batch: 002 / 011 | Total loss: 1.905 | Reg loss: 0.021 | Tree loss: 1.905 | Accuracy: 0.349000 | 1.164 sec/iter\n",
      "Epoch: 16 | Batch: 003 / 011 | Total loss: 1.846 | Reg loss: 0.021 | Tree loss: 1.846 | Accuracy: 0.396000 | 1.164 sec/iter\n",
      "Epoch: 16 | Batch: 004 / 011 | Total loss: 1.792 | Reg loss: 0.022 | Tree loss: 1.792 | Accuracy: 0.436000 | 1.163 sec/iter\n",
      "Epoch: 16 | Batch: 005 / 011 | Total loss: 1.752 | Reg loss: 0.022 | Tree loss: 1.752 | Accuracy: 0.470500 | 1.162 sec/iter\n",
      "Epoch: 16 | Batch: 006 / 011 | Total loss: 1.735 | Reg loss: 0.022 | Tree loss: 1.735 | Accuracy: 0.464000 | 1.161 sec/iter\n",
      "Epoch: 16 | Batch: 007 / 011 | Total loss: 1.700 | Reg loss: 0.022 | Tree loss: 1.700 | Accuracy: 0.488500 | 1.16 sec/iter\n",
      "Epoch: 16 | Batch: 008 / 011 | Total loss: 1.688 | Reg loss: 0.022 | Tree loss: 1.688 | Accuracy: 0.474500 | 1.159 sec/iter\n",
      "Epoch: 16 | Batch: 009 / 011 | Total loss: 1.669 | Reg loss: 0.022 | Tree loss: 1.669 | Accuracy: 0.477000 | 1.158 sec/iter\n",
      "Epoch: 16 | Batch: 010 / 011 | Total loss: 1.679 | Reg loss: 0.022 | Tree loss: 1.679 | Accuracy: 0.453925 | 1.157 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 17 | Batch: 000 / 011 | Total loss: 1.966 | Reg loss: 0.022 | Tree loss: 1.966 | Accuracy: 0.375500 | 1.169 sec/iter\n",
      "Epoch: 17 | Batch: 001 / 011 | Total loss: 1.927 | Reg loss: 0.022 | Tree loss: 1.927 | Accuracy: 0.339000 | 1.167 sec/iter\n",
      "Epoch: 17 | Batch: 002 / 011 | Total loss: 1.872 | Reg loss: 0.022 | Tree loss: 1.872 | Accuracy: 0.376500 | 1.167 sec/iter\n",
      "Epoch: 17 | Batch: 003 / 011 | Total loss: 1.823 | Reg loss: 0.022 | Tree loss: 1.823 | Accuracy: 0.426000 | 1.166 sec/iter\n",
      "Epoch: 17 | Batch: 004 / 011 | Total loss: 1.760 | Reg loss: 0.022 | Tree loss: 1.760 | Accuracy: 0.451500 | 1.166 sec/iter\n",
      "Epoch: 17 | Batch: 005 / 011 | Total loss: 1.714 | Reg loss: 0.022 | Tree loss: 1.714 | Accuracy: 0.502500 | 1.165 sec/iter\n",
      "Epoch: 17 | Batch: 006 / 011 | Total loss: 1.692 | Reg loss: 0.022 | Tree loss: 1.692 | Accuracy: 0.515500 | 1.164 sec/iter\n",
      "Epoch: 17 | Batch: 007 / 011 | Total loss: 1.682 | Reg loss: 0.022 | Tree loss: 1.682 | Accuracy: 0.489500 | 1.163 sec/iter\n",
      "Epoch: 17 | Batch: 008 / 011 | Total loss: 1.670 | Reg loss: 0.022 | Tree loss: 1.670 | Accuracy: 0.464500 | 1.162 sec/iter\n",
      "Epoch: 17 | Batch: 009 / 011 | Total loss: 1.654 | Reg loss: 0.022 | Tree loss: 1.654 | Accuracy: 0.455000 | 1.161 sec/iter\n",
      "Epoch: 17 | Batch: 010 / 011 | Total loss: 1.617 | Reg loss: 0.023 | Tree loss: 1.617 | Accuracy: 0.450512 | 1.16 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 18 | Batch: 000 / 011 | Total loss: 1.930 | Reg loss: 0.022 | Tree loss: 1.930 | Accuracy: 0.381000 | 1.172 sec/iter\n",
      "Epoch: 18 | Batch: 001 / 011 | Total loss: 1.887 | Reg loss: 0.022 | Tree loss: 1.887 | Accuracy: 0.360500 | 1.171 sec/iter\n",
      "Epoch: 18 | Batch: 002 / 011 | Total loss: 1.840 | Reg loss: 0.022 | Tree loss: 1.840 | Accuracy: 0.386000 | 1.17 sec/iter\n",
      "Epoch: 18 | Batch: 003 / 011 | Total loss: 1.781 | Reg loss: 0.022 | Tree loss: 1.781 | Accuracy: 0.427500 | 1.17 sec/iter\n",
      "Epoch: 18 | Batch: 004 / 011 | Total loss: 1.720 | Reg loss: 0.022 | Tree loss: 1.720 | Accuracy: 0.486000 | 1.169 sec/iter\n",
      "Epoch: 18 | Batch: 005 / 011 | Total loss: 1.678 | Reg loss: 0.022 | Tree loss: 1.678 | Accuracy: 0.513500 | 1.168 sec/iter\n",
      "Epoch: 18 | Batch: 006 / 011 | Total loss: 1.663 | Reg loss: 0.022 | Tree loss: 1.663 | Accuracy: 0.502000 | 1.168 sec/iter\n",
      "Epoch: 18 | Batch: 007 / 011 | Total loss: 1.654 | Reg loss: 0.022 | Tree loss: 1.654 | Accuracy: 0.482000 | 1.167 sec/iter\n",
      "Epoch: 18 | Batch: 008 / 011 | Total loss: 1.655 | Reg loss: 0.023 | Tree loss: 1.655 | Accuracy: 0.460000 | 1.165 sec/iter\n",
      "Epoch: 18 | Batch: 009 / 011 | Total loss: 1.621 | Reg loss: 0.023 | Tree loss: 1.621 | Accuracy: 0.473500 | 1.165 sec/iter\n",
      "Epoch: 18 | Batch: 010 / 011 | Total loss: 1.617 | Reg loss: 0.023 | Tree loss: 1.617 | Accuracy: 0.433447 | 1.164 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 19 | Batch: 000 / 011 | Total loss: 1.888 | Reg loss: 0.022 | Tree loss: 1.888 | Accuracy: 0.361000 | 1.175 sec/iter\n",
      "Epoch: 19 | Batch: 001 / 011 | Total loss: 1.844 | Reg loss: 0.022 | Tree loss: 1.844 | Accuracy: 0.360000 | 1.174 sec/iter\n",
      "Epoch: 19 | Batch: 002 / 011 | Total loss: 1.799 | Reg loss: 0.022 | Tree loss: 1.799 | Accuracy: 0.391000 | 1.174 sec/iter\n",
      "Epoch: 19 | Batch: 003 / 011 | Total loss: 1.754 | Reg loss: 0.022 | Tree loss: 1.754 | Accuracy: 0.443500 | 1.173 sec/iter\n",
      "Epoch: 19 | Batch: 004 / 011 | Total loss: 1.700 | Reg loss: 0.022 | Tree loss: 1.700 | Accuracy: 0.471500 | 1.173 sec/iter\n",
      "Epoch: 19 | Batch: 005 / 011 | Total loss: 1.662 | Reg loss: 0.022 | Tree loss: 1.662 | Accuracy: 0.496000 | 1.172 sec/iter\n",
      "Epoch: 19 | Batch: 006 / 011 | Total loss: 1.642 | Reg loss: 0.022 | Tree loss: 1.642 | Accuracy: 0.488000 | 1.171 sec/iter\n",
      "Epoch: 19 | Batch: 007 / 011 | Total loss: 1.615 | Reg loss: 0.023 | Tree loss: 1.615 | Accuracy: 0.504000 | 1.17 sec/iter\n",
      "Epoch: 19 | Batch: 008 / 011 | Total loss: 1.611 | Reg loss: 0.023 | Tree loss: 1.611 | Accuracy: 0.488000 | 1.168 sec/iter\n",
      "Epoch: 19 | Batch: 009 / 011 | Total loss: 1.606 | Reg loss: 0.023 | Tree loss: 1.606 | Accuracy: 0.479000 | 1.168 sec/iter\n",
      "Epoch: 19 | Batch: 010 / 011 | Total loss: 1.604 | Reg loss: 0.023 | Tree loss: 1.604 | Accuracy: 0.460751 | 1.166 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 20 | Batch: 000 / 011 | Total loss: 1.872 | Reg loss: 0.022 | Tree loss: 1.872 | Accuracy: 0.361000 | 1.166 sec/iter\n",
      "Epoch: 20 | Batch: 001 / 011 | Total loss: 1.817 | Reg loss: 0.022 | Tree loss: 1.817 | Accuracy: 0.362500 | 1.165 sec/iter\n",
      "Epoch: 20 | Batch: 002 / 011 | Total loss: 1.762 | Reg loss: 0.022 | Tree loss: 1.762 | Accuracy: 0.409500 | 1.163 sec/iter\n",
      "Epoch: 20 | Batch: 003 / 011 | Total loss: 1.719 | Reg loss: 0.022 | Tree loss: 1.719 | Accuracy: 0.423500 | 1.162 sec/iter\n",
      "Epoch: 20 | Batch: 004 / 011 | Total loss: 1.665 | Reg loss: 0.022 | Tree loss: 1.665 | Accuracy: 0.486000 | 1.161 sec/iter\n",
      "Epoch: 20 | Batch: 005 / 011 | Total loss: 1.630 | Reg loss: 0.023 | Tree loss: 1.630 | Accuracy: 0.528000 | 1.16 sec/iter\n",
      "Epoch: 20 | Batch: 006 / 011 | Total loss: 1.608 | Reg loss: 0.023 | Tree loss: 1.608 | Accuracy: 0.513500 | 1.159 sec/iter\n",
      "Epoch: 20 | Batch: 007 / 011 | Total loss: 1.590 | Reg loss: 0.023 | Tree loss: 1.590 | Accuracy: 0.505000 | 1.159 sec/iter\n",
      "Epoch: 20 | Batch: 008 / 011 | Total loss: 1.608 | Reg loss: 0.023 | Tree loss: 1.608 | Accuracy: 0.464500 | 1.158 sec/iter\n",
      "Epoch: 20 | Batch: 009 / 011 | Total loss: 1.568 | Reg loss: 0.023 | Tree loss: 1.568 | Accuracy: 0.484500 | 1.158 sec/iter\n",
      "Epoch: 20 | Batch: 010 / 011 | Total loss: 1.567 | Reg loss: 0.023 | Tree loss: 1.567 | Accuracy: 0.491468 | 1.157 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 21 | Batch: 000 / 011 | Total loss: 1.829 | Reg loss: 0.022 | Tree loss: 1.829 | Accuracy: 0.363500 | 1.16 sec/iter\n",
      "Epoch: 21 | Batch: 001 / 011 | Total loss: 1.782 | Reg loss: 0.022 | Tree loss: 1.782 | Accuracy: 0.376500 | 1.16 sec/iter\n",
      "Epoch: 21 | Batch: 002 / 011 | Total loss: 1.736 | Reg loss: 0.022 | Tree loss: 1.736 | Accuracy: 0.420500 | 1.159 sec/iter\n",
      "Epoch: 21 | Batch: 003 / 011 | Total loss: 1.689 | Reg loss: 0.022 | Tree loss: 1.689 | Accuracy: 0.475500 | 1.158 sec/iter\n",
      "Epoch: 21 | Batch: 004 / 011 | Total loss: 1.628 | Reg loss: 0.023 | Tree loss: 1.628 | Accuracy: 0.503000 | 1.158 sec/iter\n",
      "Epoch: 21 | Batch: 005 / 011 | Total loss: 1.619 | Reg loss: 0.023 | Tree loss: 1.619 | Accuracy: 0.506500 | 1.157 sec/iter\n",
      "Epoch: 21 | Batch: 006 / 011 | Total loss: 1.578 | Reg loss: 0.023 | Tree loss: 1.578 | Accuracy: 0.512500 | 1.156 sec/iter\n",
      "Epoch: 21 | Batch: 007 / 011 | Total loss: 1.578 | Reg loss: 0.023 | Tree loss: 1.578 | Accuracy: 0.500000 | 1.155 sec/iter\n",
      "Epoch: 21 | Batch: 008 / 011 | Total loss: 1.565 | Reg loss: 0.023 | Tree loss: 1.565 | Accuracy: 0.492000 | 1.155 sec/iter\n",
      "Epoch: 21 | Batch: 009 / 011 | Total loss: 1.544 | Reg loss: 0.023 | Tree loss: 1.544 | Accuracy: 0.496500 | 1.154 sec/iter\n",
      "Epoch: 21 | Batch: 010 / 011 | Total loss: 1.556 | Reg loss: 0.023 | Tree loss: 1.556 | Accuracy: 0.460751 | 1.153 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 22 | Batch: 000 / 011 | Total loss: 1.809 | Reg loss: 0.023 | Tree loss: 1.809 | Accuracy: 0.371000 | 1.161 sec/iter\n",
      "Epoch: 22 | Batch: 001 / 011 | Total loss: 1.750 | Reg loss: 0.023 | Tree loss: 1.750 | Accuracy: 0.390000 | 1.16 sec/iter\n",
      "Epoch: 22 | Batch: 002 / 011 | Total loss: 1.706 | Reg loss: 0.023 | Tree loss: 1.706 | Accuracy: 0.401500 | 1.16 sec/iter\n",
      "Epoch: 22 | Batch: 003 / 011 | Total loss: 1.655 | Reg loss: 0.023 | Tree loss: 1.655 | Accuracy: 0.468500 | 1.159 sec/iter\n",
      "Epoch: 22 | Batch: 004 / 011 | Total loss: 1.614 | Reg loss: 0.023 | Tree loss: 1.614 | Accuracy: 0.465500 | 1.159 sec/iter\n",
      "Epoch: 22 | Batch: 005 / 011 | Total loss: 1.582 | Reg loss: 0.023 | Tree loss: 1.582 | Accuracy: 0.506000 | 1.159 sec/iter\n",
      "Epoch: 22 | Batch: 006 / 011 | Total loss: 1.559 | Reg loss: 0.023 | Tree loss: 1.559 | Accuracy: 0.505500 | 1.158 sec/iter\n",
      "Epoch: 22 | Batch: 007 / 011 | Total loss: 1.557 | Reg loss: 0.023 | Tree loss: 1.557 | Accuracy: 0.476500 | 1.157 sec/iter\n",
      "Epoch: 22 | Batch: 008 / 011 | Total loss: 1.522 | Reg loss: 0.023 | Tree loss: 1.522 | Accuracy: 0.515500 | 1.156 sec/iter\n",
      "Epoch: 22 | Batch: 009 / 011 | Total loss: 1.531 | Reg loss: 0.023 | Tree loss: 1.531 | Accuracy: 0.493000 | 1.155 sec/iter\n",
      "Epoch: 22 | Batch: 010 / 011 | Total loss: 1.499 | Reg loss: 0.023 | Tree loss: 1.499 | Accuracy: 0.501706 | 1.154 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 23 | Batch: 000 / 011 | Total loss: 1.778 | Reg loss: 0.023 | Tree loss: 1.778 | Accuracy: 0.390000 | 1.163 sec/iter\n",
      "Epoch: 23 | Batch: 001 / 011 | Total loss: 1.743 | Reg loss: 0.023 | Tree loss: 1.743 | Accuracy: 0.405000 | 1.163 sec/iter\n",
      "Epoch: 23 | Batch: 002 / 011 | Total loss: 1.684 | Reg loss: 0.023 | Tree loss: 1.684 | Accuracy: 0.441000 | 1.162 sec/iter\n",
      "Epoch: 23 | Batch: 003 / 011 | Total loss: 1.620 | Reg loss: 0.023 | Tree loss: 1.620 | Accuracy: 0.482000 | 1.162 sec/iter\n",
      "Epoch: 23 | Batch: 004 / 011 | Total loss: 1.585 | Reg loss: 0.023 | Tree loss: 1.585 | Accuracy: 0.498000 | 1.161 sec/iter\n",
      "Epoch: 23 | Batch: 005 / 011 | Total loss: 1.562 | Reg loss: 0.023 | Tree loss: 1.562 | Accuracy: 0.504500 | 1.161 sec/iter\n",
      "Epoch: 23 | Batch: 006 / 011 | Total loss: 1.533 | Reg loss: 0.023 | Tree loss: 1.533 | Accuracy: 0.492000 | 1.16 sec/iter\n",
      "Epoch: 23 | Batch: 007 / 011 | Total loss: 1.527 | Reg loss: 0.023 | Tree loss: 1.527 | Accuracy: 0.497500 | 1.159 sec/iter\n",
      "Epoch: 23 | Batch: 008 / 011 | Total loss: 1.521 | Reg loss: 0.023 | Tree loss: 1.521 | Accuracy: 0.495000 | 1.158 sec/iter\n",
      "Epoch: 23 | Batch: 009 / 011 | Total loss: 1.507 | Reg loss: 0.023 | Tree loss: 1.507 | Accuracy: 0.482000 | 1.158 sec/iter\n",
      "Epoch: 23 | Batch: 010 / 011 | Total loss: 1.501 | Reg loss: 0.023 | Tree loss: 1.501 | Accuracy: 0.477816 | 1.157 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 24 | Batch: 000 / 011 | Total loss: 1.750 | Reg loss: 0.023 | Tree loss: 1.750 | Accuracy: 0.412500 | 1.166 sec/iter\n",
      "Epoch: 24 | Batch: 001 / 011 | Total loss: 1.713 | Reg loss: 0.023 | Tree loss: 1.713 | Accuracy: 0.414500 | 1.165 sec/iter\n",
      "Epoch: 24 | Batch: 002 / 011 | Total loss: 1.662 | Reg loss: 0.023 | Tree loss: 1.662 | Accuracy: 0.423500 | 1.164 sec/iter\n",
      "Epoch: 24 | Batch: 003 / 011 | Total loss: 1.609 | Reg loss: 0.023 | Tree loss: 1.609 | Accuracy: 0.453500 | 1.164 sec/iter\n",
      "Epoch: 24 | Batch: 004 / 011 | Total loss: 1.557 | Reg loss: 0.023 | Tree loss: 1.557 | Accuracy: 0.483000 | 1.163 sec/iter\n",
      "Epoch: 24 | Batch: 005 / 011 | Total loss: 1.529 | Reg loss: 0.023 | Tree loss: 1.529 | Accuracy: 0.490000 | 1.163 sec/iter\n",
      "Epoch: 24 | Batch: 006 / 011 | Total loss: 1.525 | Reg loss: 0.023 | Tree loss: 1.525 | Accuracy: 0.509500 | 1.162 sec/iter\n",
      "Epoch: 24 | Batch: 007 / 011 | Total loss: 1.495 | Reg loss: 0.023 | Tree loss: 1.495 | Accuracy: 0.506000 | 1.161 sec/iter\n",
      "Epoch: 24 | Batch: 008 / 011 | Total loss: 1.499 | Reg loss: 0.023 | Tree loss: 1.499 | Accuracy: 0.506500 | 1.16 sec/iter\n",
      "Epoch: 24 | Batch: 009 / 011 | Total loss: 1.466 | Reg loss: 0.023 | Tree loss: 1.466 | Accuracy: 0.507500 | 1.16 sec/iter\n",
      "Epoch: 24 | Batch: 010 / 011 | Total loss: 1.465 | Reg loss: 0.023 | Tree loss: 1.465 | Accuracy: 0.508532 | 1.159 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 25 | Batch: 000 / 011 | Total loss: 1.725 | Reg loss: 0.023 | Tree loss: 1.725 | Accuracy: 0.412000 | 1.168 sec/iter\n",
      "Epoch: 25 | Batch: 001 / 011 | Total loss: 1.670 | Reg loss: 0.023 | Tree loss: 1.670 | Accuracy: 0.423500 | 1.167 sec/iter\n",
      "Epoch: 25 | Batch: 002 / 011 | Total loss: 1.628 | Reg loss: 0.023 | Tree loss: 1.628 | Accuracy: 0.449500 | 1.166 sec/iter\n",
      "Epoch: 25 | Batch: 003 / 011 | Total loss: 1.582 | Reg loss: 0.023 | Tree loss: 1.582 | Accuracy: 0.457500 | 1.166 sec/iter\n",
      "Epoch: 25 | Batch: 004 / 011 | Total loss: 1.545 | Reg loss: 0.023 | Tree loss: 1.545 | Accuracy: 0.502500 | 1.165 sec/iter\n",
      "Epoch: 25 | Batch: 005 / 011 | Total loss: 1.490 | Reg loss: 0.023 | Tree loss: 1.490 | Accuracy: 0.519500 | 1.165 sec/iter\n",
      "Epoch: 25 | Batch: 006 / 011 | Total loss: 1.487 | Reg loss: 0.023 | Tree loss: 1.487 | Accuracy: 0.519500 | 1.164 sec/iter\n",
      "Epoch: 25 | Batch: 007 / 011 | Total loss: 1.470 | Reg loss: 0.023 | Tree loss: 1.470 | Accuracy: 0.516000 | 1.164 sec/iter\n",
      "Epoch: 25 | Batch: 008 / 011 | Total loss: 1.479 | Reg loss: 0.023 | Tree loss: 1.479 | Accuracy: 0.516500 | 1.163 sec/iter\n",
      "Epoch: 25 | Batch: 009 / 011 | Total loss: 1.490 | Reg loss: 0.023 | Tree loss: 1.490 | Accuracy: 0.488500 | 1.162 sec/iter\n",
      "Epoch: 25 | Batch: 010 / 011 | Total loss: 1.474 | Reg loss: 0.023 | Tree loss: 1.474 | Accuracy: 0.481229 | 1.161 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 26 | Batch: 000 / 011 | Total loss: 1.724 | Reg loss: 0.023 | Tree loss: 1.724 | Accuracy: 0.387000 | 1.162 sec/iter\n",
      "Epoch: 26 | Batch: 001 / 011 | Total loss: 1.641 | Reg loss: 0.023 | Tree loss: 1.641 | Accuracy: 0.422000 | 1.16 sec/iter\n",
      "Epoch: 26 | Batch: 002 / 011 | Total loss: 1.610 | Reg loss: 0.023 | Tree loss: 1.610 | Accuracy: 0.450500 | 1.159 sec/iter\n",
      "Epoch: 26 | Batch: 003 / 011 | Total loss: 1.560 | Reg loss: 0.023 | Tree loss: 1.560 | Accuracy: 0.472500 | 1.158 sec/iter\n",
      "Epoch: 26 | Batch: 004 / 011 | Total loss: 1.535 | Reg loss: 0.023 | Tree loss: 1.535 | Accuracy: 0.492500 | 1.157 sec/iter\n",
      "Epoch: 26 | Batch: 005 / 011 | Total loss: 1.488 | Reg loss: 0.023 | Tree loss: 1.488 | Accuracy: 0.502000 | 1.156 sec/iter\n",
      "Epoch: 26 | Batch: 006 / 011 | Total loss: 1.478 | Reg loss: 0.023 | Tree loss: 1.478 | Accuracy: 0.491500 | 1.156 sec/iter\n",
      "Epoch: 26 | Batch: 007 / 011 | Total loss: 1.447 | Reg loss: 0.023 | Tree loss: 1.447 | Accuracy: 0.517000 | 1.156 sec/iter\n",
      "Epoch: 26 | Batch: 008 / 011 | Total loss: 1.441 | Reg loss: 0.023 | Tree loss: 1.441 | Accuracy: 0.524500 | 1.155 sec/iter\n",
      "Epoch: 26 | Batch: 009 / 011 | Total loss: 1.442 | Reg loss: 0.023 | Tree loss: 1.442 | Accuracy: 0.510500 | 1.155 sec/iter\n",
      "Epoch: 26 | Batch: 010 / 011 | Total loss: 1.500 | Reg loss: 0.023 | Tree loss: 1.500 | Accuracy: 0.460751 | 1.154 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 27 | Batch: 000 / 011 | Total loss: 1.651 | Reg loss: 0.023 | Tree loss: 1.651 | Accuracy: 0.409500 | 1.157 sec/iter\n",
      "Epoch: 27 | Batch: 001 / 011 | Total loss: 1.630 | Reg loss: 0.023 | Tree loss: 1.630 | Accuracy: 0.431000 | 1.156 sec/iter\n",
      "Epoch: 27 | Batch: 002 / 011 | Total loss: 1.598 | Reg loss: 0.023 | Tree loss: 1.598 | Accuracy: 0.445500 | 1.156 sec/iter\n",
      "Epoch: 27 | Batch: 003 / 011 | Total loss: 1.526 | Reg loss: 0.023 | Tree loss: 1.526 | Accuracy: 0.514500 | 1.156 sec/iter\n",
      "Epoch: 27 | Batch: 004 / 011 | Total loss: 1.507 | Reg loss: 0.023 | Tree loss: 1.507 | Accuracy: 0.494500 | 1.155 sec/iter\n",
      "Epoch: 27 | Batch: 005 / 011 | Total loss: 1.457 | Reg loss: 0.023 | Tree loss: 1.457 | Accuracy: 0.514000 | 1.155 sec/iter\n",
      "Epoch: 27 | Batch: 006 / 011 | Total loss: 1.467 | Reg loss: 0.023 | Tree loss: 1.467 | Accuracy: 0.493000 | 1.154 sec/iter\n",
      "Epoch: 27 | Batch: 007 / 011 | Total loss: 1.433 | Reg loss: 0.023 | Tree loss: 1.433 | Accuracy: 0.509500 | 1.154 sec/iter\n",
      "Epoch: 27 | Batch: 008 / 011 | Total loss: 1.438 | Reg loss: 0.023 | Tree loss: 1.438 | Accuracy: 0.533000 | 1.153 sec/iter\n",
      "Epoch: 27 | Batch: 009 / 011 | Total loss: 1.432 | Reg loss: 0.023 | Tree loss: 1.432 | Accuracy: 0.507500 | 1.153 sec/iter\n",
      "Epoch: 27 | Batch: 010 / 011 | Total loss: 1.392 | Reg loss: 0.023 | Tree loss: 1.392 | Accuracy: 0.518771 | 1.152 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 28 | Batch: 000 / 011 | Total loss: 1.647 | Reg loss: 0.023 | Tree loss: 1.647 | Accuracy: 0.434000 | 1.158 sec/iter\n",
      "Epoch: 28 | Batch: 001 / 011 | Total loss: 1.611 | Reg loss: 0.023 | Tree loss: 1.611 | Accuracy: 0.421500 | 1.157 sec/iter\n",
      "Epoch: 28 | Batch: 002 / 011 | Total loss: 1.540 | Reg loss: 0.023 | Tree loss: 1.540 | Accuracy: 0.476500 | 1.157 sec/iter\n",
      "Epoch: 28 | Batch: 003 / 011 | Total loss: 1.516 | Reg loss: 0.023 | Tree loss: 1.516 | Accuracy: 0.477000 | 1.157 sec/iter\n",
      "Epoch: 28 | Batch: 004 / 011 | Total loss: 1.487 | Reg loss: 0.023 | Tree loss: 1.487 | Accuracy: 0.490500 | 1.156 sec/iter\n",
      "Epoch: 28 | Batch: 005 / 011 | Total loss: 1.449 | Reg loss: 0.023 | Tree loss: 1.449 | Accuracy: 0.491000 | 1.156 sec/iter\n",
      "Epoch: 28 | Batch: 006 / 011 | Total loss: 1.439 | Reg loss: 0.023 | Tree loss: 1.439 | Accuracy: 0.507000 | 1.156 sec/iter\n",
      "Epoch: 28 | Batch: 007 / 011 | Total loss: 1.400 | Reg loss: 0.023 | Tree loss: 1.400 | Accuracy: 0.531500 | 1.155 sec/iter\n",
      "Epoch: 28 | Batch: 008 / 011 | Total loss: 1.407 | Reg loss: 0.023 | Tree loss: 1.407 | Accuracy: 0.532000 | 1.155 sec/iter\n",
      "Epoch: 28 | Batch: 009 / 011 | Total loss: 1.410 | Reg loss: 0.023 | Tree loss: 1.410 | Accuracy: 0.522000 | 1.154 sec/iter\n",
      "Epoch: 28 | Batch: 010 / 011 | Total loss: 1.393 | Reg loss: 0.023 | Tree loss: 1.393 | Accuracy: 0.508532 | 1.153 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 29 | Batch: 000 / 011 | Total loss: 1.612 | Reg loss: 0.023 | Tree loss: 1.612 | Accuracy: 0.426500 | 1.16 sec/iter\n",
      "Epoch: 29 | Batch: 001 / 011 | Total loss: 1.575 | Reg loss: 0.023 | Tree loss: 1.575 | Accuracy: 0.464000 | 1.159 sec/iter\n",
      "Epoch: 29 | Batch: 002 / 011 | Total loss: 1.552 | Reg loss: 0.023 | Tree loss: 1.552 | Accuracy: 0.455500 | 1.159 sec/iter\n",
      "Epoch: 29 | Batch: 003 / 011 | Total loss: 1.486 | Reg loss: 0.023 | Tree loss: 1.486 | Accuracy: 0.502500 | 1.159 sec/iter\n",
      "Epoch: 29 | Batch: 004 / 011 | Total loss: 1.447 | Reg loss: 0.023 | Tree loss: 1.447 | Accuracy: 0.521500 | 1.158 sec/iter\n",
      "Epoch: 29 | Batch: 005 / 011 | Total loss: 1.431 | Reg loss: 0.023 | Tree loss: 1.431 | Accuracy: 0.528000 | 1.158 sec/iter\n",
      "Epoch: 29 | Batch: 006 / 011 | Total loss: 1.413 | Reg loss: 0.023 | Tree loss: 1.413 | Accuracy: 0.521000 | 1.158 sec/iter\n",
      "Epoch: 29 | Batch: 007 / 011 | Total loss: 1.401 | Reg loss: 0.023 | Tree loss: 1.401 | Accuracy: 0.539000 | 1.157 sec/iter\n",
      "Epoch: 29 | Batch: 008 / 011 | Total loss: 1.391 | Reg loss: 0.023 | Tree loss: 1.391 | Accuracy: 0.540000 | 1.156 sec/iter\n",
      "Epoch: 29 | Batch: 009 / 011 | Total loss: 1.387 | Reg loss: 0.023 | Tree loss: 1.387 | Accuracy: 0.526000 | 1.156 sec/iter\n",
      "Epoch: 29 | Batch: 010 / 011 | Total loss: 1.379 | Reg loss: 0.023 | Tree loss: 1.379 | Accuracy: 0.505119 | 1.155 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 30 | Batch: 000 / 011 | Total loss: 1.598 | Reg loss: 0.023 | Tree loss: 1.598 | Accuracy: 0.436000 | 1.162 sec/iter\n",
      "Epoch: 30 | Batch: 001 / 011 | Total loss: 1.538 | Reg loss: 0.023 | Tree loss: 1.538 | Accuracy: 0.490500 | 1.162 sec/iter\n",
      "Epoch: 30 | Batch: 002 / 011 | Total loss: 1.513 | Reg loss: 0.023 | Tree loss: 1.513 | Accuracy: 0.485000 | 1.161 sec/iter\n",
      "Epoch: 30 | Batch: 003 / 011 | Total loss: 1.481 | Reg loss: 0.023 | Tree loss: 1.481 | Accuracy: 0.469500 | 1.161 sec/iter\n",
      "Epoch: 30 | Batch: 004 / 011 | Total loss: 1.434 | Reg loss: 0.023 | Tree loss: 1.434 | Accuracy: 0.528000 | 1.16 sec/iter\n",
      "Epoch: 30 | Batch: 005 / 011 | Total loss: 1.376 | Reg loss: 0.023 | Tree loss: 1.376 | Accuracy: 0.545000 | 1.16 sec/iter\n",
      "Epoch: 30 | Batch: 006 / 011 | Total loss: 1.389 | Reg loss: 0.023 | Tree loss: 1.389 | Accuracy: 0.519500 | 1.16 sec/iter\n",
      "Epoch: 30 | Batch: 007 / 011 | Total loss: 1.406 | Reg loss: 0.023 | Tree loss: 1.406 | Accuracy: 0.512000 | 1.159 sec/iter\n",
      "Epoch: 30 | Batch: 008 / 011 | Total loss: 1.396 | Reg loss: 0.023 | Tree loss: 1.396 | Accuracy: 0.500500 | 1.158 sec/iter\n",
      "Epoch: 30 | Batch: 009 / 011 | Total loss: 1.378 | Reg loss: 0.023 | Tree loss: 1.378 | Accuracy: 0.533000 | 1.158 sec/iter\n",
      "Epoch: 30 | Batch: 010 / 011 | Total loss: 1.346 | Reg loss: 0.024 | Tree loss: 1.346 | Accuracy: 0.529010 | 1.157 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 31 | Batch: 000 / 011 | Total loss: 1.570 | Reg loss: 0.023 | Tree loss: 1.570 | Accuracy: 0.465500 | 1.165 sec/iter\n",
      "Epoch: 31 | Batch: 001 / 011 | Total loss: 1.508 | Reg loss: 0.023 | Tree loss: 1.508 | Accuracy: 0.491500 | 1.164 sec/iter\n",
      "Epoch: 31 | Batch: 002 / 011 | Total loss: 1.493 | Reg loss: 0.023 | Tree loss: 1.493 | Accuracy: 0.488000 | 1.164 sec/iter\n",
      "Epoch: 31 | Batch: 003 / 011 | Total loss: 1.464 | Reg loss: 0.023 | Tree loss: 1.464 | Accuracy: 0.473000 | 1.163 sec/iter\n",
      "Epoch: 31 | Batch: 004 / 011 | Total loss: 1.417 | Reg loss: 0.023 | Tree loss: 1.417 | Accuracy: 0.541000 | 1.163 sec/iter\n",
      "Epoch: 31 | Batch: 005 / 011 | Total loss: 1.402 | Reg loss: 0.023 | Tree loss: 1.402 | Accuracy: 0.517000 | 1.163 sec/iter\n",
      "Epoch: 31 | Batch: 006 / 011 | Total loss: 1.390 | Reg loss: 0.023 | Tree loss: 1.390 | Accuracy: 0.507500 | 1.162 sec/iter\n",
      "Epoch: 31 | Batch: 007 / 011 | Total loss: 1.362 | Reg loss: 0.023 | Tree loss: 1.362 | Accuracy: 0.534500 | 1.161 sec/iter\n",
      "Epoch: 31 | Batch: 008 / 011 | Total loss: 1.358 | Reg loss: 0.023 | Tree loss: 1.358 | Accuracy: 0.533500 | 1.161 sec/iter\n",
      "Epoch: 31 | Batch: 009 / 011 | Total loss: 1.367 | Reg loss: 0.024 | Tree loss: 1.367 | Accuracy: 0.512000 | 1.16 sec/iter\n",
      "Epoch: 31 | Batch: 010 / 011 | Total loss: 1.349 | Reg loss: 0.024 | Tree loss: 1.349 | Accuracy: 0.505119 | 1.159 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 32 | Batch: 000 / 011 | Total loss: 1.561 | Reg loss: 0.023 | Tree loss: 1.561 | Accuracy: 0.445500 | 1.159 sec/iter\n",
      "Epoch: 32 | Batch: 001 / 011 | Total loss: 1.526 | Reg loss: 0.023 | Tree loss: 1.526 | Accuracy: 0.479000 | 1.158 sec/iter\n",
      "Epoch: 32 | Batch: 002 / 011 | Total loss: 1.478 | Reg loss: 0.023 | Tree loss: 1.478 | Accuracy: 0.493500 | 1.157 sec/iter\n",
      "Epoch: 32 | Batch: 003 / 011 | Total loss: 1.435 | Reg loss: 0.023 | Tree loss: 1.435 | Accuracy: 0.490500 | 1.156 sec/iter\n",
      "Epoch: 32 | Batch: 004 / 011 | Total loss: 1.410 | Reg loss: 0.023 | Tree loss: 1.410 | Accuracy: 0.507000 | 1.156 sec/iter\n",
      "Epoch: 32 | Batch: 005 / 011 | Total loss: 1.387 | Reg loss: 0.023 | Tree loss: 1.387 | Accuracy: 0.513500 | 1.155 sec/iter\n",
      "Epoch: 32 | Batch: 006 / 011 | Total loss: 1.364 | Reg loss: 0.023 | Tree loss: 1.364 | Accuracy: 0.509000 | 1.154 sec/iter\n",
      "Epoch: 32 | Batch: 007 / 011 | Total loss: 1.338 | Reg loss: 0.023 | Tree loss: 1.338 | Accuracy: 0.529500 | 1.154 sec/iter\n",
      "Epoch: 32 | Batch: 008 / 011 | Total loss: 1.341 | Reg loss: 0.024 | Tree loss: 1.341 | Accuracy: 0.545000 | 1.154 sec/iter\n",
      "Epoch: 32 | Batch: 009 / 011 | Total loss: 1.330 | Reg loss: 0.024 | Tree loss: 1.330 | Accuracy: 0.537500 | 1.154 sec/iter\n",
      "Epoch: 32 | Batch: 010 / 011 | Total loss: 1.345 | Reg loss: 0.024 | Tree loss: 1.345 | Accuracy: 0.556314 | 1.153 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 33 | Batch: 000 / 011 | Total loss: 1.521 | Reg loss: 0.023 | Tree loss: 1.521 | Accuracy: 0.486000 | 1.156 sec/iter\n",
      "Epoch: 33 | Batch: 001 / 011 | Total loss: 1.472 | Reg loss: 0.023 | Tree loss: 1.472 | Accuracy: 0.477500 | 1.155 sec/iter\n",
      "Epoch: 33 | Batch: 002 / 011 | Total loss: 1.445 | Reg loss: 0.023 | Tree loss: 1.445 | Accuracy: 0.489000 | 1.155 sec/iter\n",
      "Epoch: 33 | Batch: 003 / 011 | Total loss: 1.427 | Reg loss: 0.023 | Tree loss: 1.427 | Accuracy: 0.489500 | 1.155 sec/iter\n",
      "Epoch: 33 | Batch: 004 / 011 | Total loss: 1.384 | Reg loss: 0.023 | Tree loss: 1.384 | Accuracy: 0.529500 | 1.154 sec/iter\n",
      "Epoch: 33 | Batch: 005 / 011 | Total loss: 1.378 | Reg loss: 0.023 | Tree loss: 1.378 | Accuracy: 0.507500 | 1.154 sec/iter\n",
      "Epoch: 33 | Batch: 006 / 011 | Total loss: 1.337 | Reg loss: 0.024 | Tree loss: 1.337 | Accuracy: 0.529500 | 1.154 sec/iter\n",
      "Epoch: 33 | Batch: 007 / 011 | Total loss: 1.348 | Reg loss: 0.024 | Tree loss: 1.348 | Accuracy: 0.529000 | 1.153 sec/iter\n",
      "Epoch: 33 | Batch: 008 / 011 | Total loss: 1.337 | Reg loss: 0.024 | Tree loss: 1.337 | Accuracy: 0.530500 | 1.153 sec/iter\n",
      "Epoch: 33 | Batch: 009 / 011 | Total loss: 1.338 | Reg loss: 0.024 | Tree loss: 1.338 | Accuracy: 0.506500 | 1.152 sec/iter\n",
      "Epoch: 33 | Batch: 010 / 011 | Total loss: 1.339 | Reg loss: 0.024 | Tree loss: 1.339 | Accuracy: 0.532423 | 1.152 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 34 | Batch: 000 / 011 | Total loss: 1.508 | Reg loss: 0.023 | Tree loss: 1.508 | Accuracy: 0.481500 | 1.157 sec/iter\n",
      "Epoch: 34 | Batch: 001 / 011 | Total loss: 1.466 | Reg loss: 0.023 | Tree loss: 1.466 | Accuracy: 0.483500 | 1.157 sec/iter\n",
      "Epoch: 34 | Batch: 002 / 011 | Total loss: 1.443 | Reg loss: 0.023 | Tree loss: 1.443 | Accuracy: 0.472500 | 1.156 sec/iter\n",
      "Epoch: 34 | Batch: 003 / 011 | Total loss: 1.423 | Reg loss: 0.023 | Tree loss: 1.423 | Accuracy: 0.527000 | 1.156 sec/iter\n",
      "Epoch: 34 | Batch: 004 / 011 | Total loss: 1.397 | Reg loss: 0.023 | Tree loss: 1.397 | Accuracy: 0.508000 | 1.156 sec/iter\n",
      "Epoch: 34 | Batch: 005 / 011 | Total loss: 1.354 | Reg loss: 0.024 | Tree loss: 1.354 | Accuracy: 0.537000 | 1.156 sec/iter\n",
      "Epoch: 34 | Batch: 006 / 011 | Total loss: 1.352 | Reg loss: 0.024 | Tree loss: 1.352 | Accuracy: 0.516000 | 1.155 sec/iter\n",
      "Epoch: 34 | Batch: 007 / 011 | Total loss: 1.305 | Reg loss: 0.024 | Tree loss: 1.305 | Accuracy: 0.556500 | 1.155 sec/iter\n",
      "Epoch: 34 | Batch: 008 / 011 | Total loss: 1.349 | Reg loss: 0.024 | Tree loss: 1.349 | Accuracy: 0.523500 | 1.154 sec/iter\n",
      "Epoch: 34 | Batch: 009 / 011 | Total loss: 1.299 | Reg loss: 0.024 | Tree loss: 1.299 | Accuracy: 0.556000 | 1.154 sec/iter\n",
      "Epoch: 34 | Batch: 010 / 011 | Total loss: 1.293 | Reg loss: 0.024 | Tree loss: 1.293 | Accuracy: 0.525597 | 1.153 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 35 | Batch: 000 / 011 | Total loss: 1.498 | Reg loss: 0.023 | Tree loss: 1.498 | Accuracy: 0.469500 | 1.159 sec/iter\n",
      "Epoch: 35 | Batch: 001 / 011 | Total loss: 1.452 | Reg loss: 0.023 | Tree loss: 1.452 | Accuracy: 0.496000 | 1.158 sec/iter\n",
      "Epoch: 35 | Batch: 002 / 011 | Total loss: 1.406 | Reg loss: 0.024 | Tree loss: 1.406 | Accuracy: 0.495500 | 1.158 sec/iter\n",
      "Epoch: 35 | Batch: 003 / 011 | Total loss: 1.399 | Reg loss: 0.024 | Tree loss: 1.399 | Accuracy: 0.519000 | 1.157 sec/iter\n",
      "Epoch: 35 | Batch: 004 / 011 | Total loss: 1.349 | Reg loss: 0.024 | Tree loss: 1.349 | Accuracy: 0.532500 | 1.157 sec/iter\n",
      "Epoch: 35 | Batch: 005 / 011 | Total loss: 1.319 | Reg loss: 0.024 | Tree loss: 1.319 | Accuracy: 0.526500 | 1.157 sec/iter\n",
      "Epoch: 35 | Batch: 006 / 011 | Total loss: 1.304 | Reg loss: 0.024 | Tree loss: 1.304 | Accuracy: 0.525000 | 1.157 sec/iter\n",
      "Epoch: 35 | Batch: 007 / 011 | Total loss: 1.313 | Reg loss: 0.024 | Tree loss: 1.313 | Accuracy: 0.535000 | 1.156 sec/iter\n",
      "Epoch: 35 | Batch: 008 / 011 | Total loss: 1.324 | Reg loss: 0.024 | Tree loss: 1.324 | Accuracy: 0.552000 | 1.155 sec/iter\n",
      "Epoch: 35 | Batch: 009 / 011 | Total loss: 1.304 | Reg loss: 0.024 | Tree loss: 1.304 | Accuracy: 0.526500 | 1.155 sec/iter\n",
      "Epoch: 35 | Batch: 010 / 011 | Total loss: 1.245 | Reg loss: 0.024 | Tree loss: 1.245 | Accuracy: 0.559727 | 1.154 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 36 | Batch: 000 / 011 | Total loss: 1.477 | Reg loss: 0.024 | Tree loss: 1.477 | Accuracy: 0.489500 | 1.16 sec/iter\n",
      "Epoch: 36 | Batch: 001 / 011 | Total loss: 1.448 | Reg loss: 0.024 | Tree loss: 1.448 | Accuracy: 0.498500 | 1.16 sec/iter\n",
      "Epoch: 36 | Batch: 002 / 011 | Total loss: 1.432 | Reg loss: 0.024 | Tree loss: 1.432 | Accuracy: 0.492500 | 1.159 sec/iter\n",
      "Epoch: 36 | Batch: 003 / 011 | Total loss: 1.386 | Reg loss: 0.024 | Tree loss: 1.386 | Accuracy: 0.508500 | 1.159 sec/iter\n",
      "Epoch: 36 | Batch: 004 / 011 | Total loss: 1.361 | Reg loss: 0.024 | Tree loss: 1.361 | Accuracy: 0.527000 | 1.159 sec/iter\n",
      "Epoch: 36 | Batch: 005 / 011 | Total loss: 1.319 | Reg loss: 0.024 | Tree loss: 1.319 | Accuracy: 0.537000 | 1.158 sec/iter\n",
      "Epoch: 36 | Batch: 006 / 011 | Total loss: 1.324 | Reg loss: 0.024 | Tree loss: 1.324 | Accuracy: 0.531000 | 1.158 sec/iter\n",
      "Epoch: 36 | Batch: 007 / 011 | Total loss: 1.303 | Reg loss: 0.024 | Tree loss: 1.303 | Accuracy: 0.551500 | 1.157 sec/iter\n",
      "Epoch: 36 | Batch: 008 / 011 | Total loss: 1.290 | Reg loss: 0.024 | Tree loss: 1.290 | Accuracy: 0.542500 | 1.157 sec/iter\n",
      "Epoch: 36 | Batch: 009 / 011 | Total loss: 1.262 | Reg loss: 0.024 | Tree loss: 1.262 | Accuracy: 0.548500 | 1.157 sec/iter\n",
      "Epoch: 36 | Batch: 010 / 011 | Total loss: 1.252 | Reg loss: 0.024 | Tree loss: 1.252 | Accuracy: 0.552901 | 1.156 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 37 | Batch: 000 / 011 | Total loss: 1.476 | Reg loss: 0.024 | Tree loss: 1.476 | Accuracy: 0.492000 | 1.162 sec/iter\n",
      "Epoch: 37 | Batch: 001 / 011 | Total loss: 1.442 | Reg loss: 0.024 | Tree loss: 1.442 | Accuracy: 0.491500 | 1.161 sec/iter\n",
      "Epoch: 37 | Batch: 002 / 011 | Total loss: 1.387 | Reg loss: 0.024 | Tree loss: 1.387 | Accuracy: 0.488500 | 1.161 sec/iter\n",
      "Epoch: 37 | Batch: 003 / 011 | Total loss: 1.371 | Reg loss: 0.024 | Tree loss: 1.371 | Accuracy: 0.531000 | 1.161 sec/iter\n",
      "Epoch: 37 | Batch: 004 / 011 | Total loss: 1.335 | Reg loss: 0.024 | Tree loss: 1.335 | Accuracy: 0.526000 | 1.16 sec/iter\n",
      "Epoch: 37 | Batch: 005 / 011 | Total loss: 1.300 | Reg loss: 0.024 | Tree loss: 1.300 | Accuracy: 0.516500 | 1.16 sec/iter\n",
      "Epoch: 37 | Batch: 006 / 011 | Total loss: 1.280 | Reg loss: 0.024 | Tree loss: 1.280 | Accuracy: 0.522500 | 1.16 sec/iter\n",
      "Epoch: 37 | Batch: 007 / 011 | Total loss: 1.285 | Reg loss: 0.024 | Tree loss: 1.285 | Accuracy: 0.548500 | 1.159 sec/iter\n",
      "Epoch: 37 | Batch: 008 / 011 | Total loss: 1.290 | Reg loss: 0.024 | Tree loss: 1.290 | Accuracy: 0.541000 | 1.159 sec/iter\n",
      "Epoch: 37 | Batch: 009 / 011 | Total loss: 1.266 | Reg loss: 0.024 | Tree loss: 1.266 | Accuracy: 0.564000 | 1.158 sec/iter\n",
      "Epoch: 37 | Batch: 010 / 011 | Total loss: 1.284 | Reg loss: 0.024 | Tree loss: 1.284 | Accuracy: 0.529010 | 1.158 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 38 | Batch: 000 / 011 | Total loss: 1.466 | Reg loss: 0.024 | Tree loss: 1.466 | Accuracy: 0.483000 | 1.158 sec/iter\n",
      "Epoch: 38 | Batch: 001 / 011 | Total loss: 1.428 | Reg loss: 0.024 | Tree loss: 1.428 | Accuracy: 0.477000 | 1.157 sec/iter\n",
      "Epoch: 38 | Batch: 002 / 011 | Total loss: 1.378 | Reg loss: 0.024 | Tree loss: 1.378 | Accuracy: 0.509500 | 1.157 sec/iter\n",
      "Epoch: 38 | Batch: 003 / 011 | Total loss: 1.362 | Reg loss: 0.024 | Tree loss: 1.362 | Accuracy: 0.534500 | 1.156 sec/iter\n",
      "Epoch: 38 | Batch: 004 / 011 | Total loss: 1.352 | Reg loss: 0.024 | Tree loss: 1.352 | Accuracy: 0.513500 | 1.155 sec/iter\n",
      "Epoch: 38 | Batch: 005 / 011 | Total loss: 1.309 | Reg loss: 0.024 | Tree loss: 1.309 | Accuracy: 0.550500 | 1.154 sec/iter\n",
      "Epoch: 38 | Batch: 006 / 011 | Total loss: 1.271 | Reg loss: 0.024 | Tree loss: 1.271 | Accuracy: 0.545500 | 1.154 sec/iter\n",
      "Epoch: 38 | Batch: 007 / 011 | Total loss: 1.260 | Reg loss: 0.024 | Tree loss: 1.260 | Accuracy: 0.558500 | 1.154 sec/iter\n",
      "Epoch: 38 | Batch: 008 / 011 | Total loss: 1.266 | Reg loss: 0.024 | Tree loss: 1.266 | Accuracy: 0.529500 | 1.154 sec/iter\n",
      "Epoch: 38 | Batch: 009 / 011 | Total loss: 1.268 | Reg loss: 0.024 | Tree loss: 1.268 | Accuracy: 0.539000 | 1.153 sec/iter\n",
      "Epoch: 38 | Batch: 010 / 011 | Total loss: 1.254 | Reg loss: 0.024 | Tree loss: 1.254 | Accuracy: 0.539249 | 1.153 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 39 | Batch: 000 / 011 | Total loss: 1.449 | Reg loss: 0.024 | Tree loss: 1.449 | Accuracy: 0.487000 | 1.155 sec/iter\n",
      "Epoch: 39 | Batch: 001 / 011 | Total loss: 1.411 | Reg loss: 0.024 | Tree loss: 1.411 | Accuracy: 0.489000 | 1.155 sec/iter\n",
      "Epoch: 39 | Batch: 002 / 011 | Total loss: 1.383 | Reg loss: 0.024 | Tree loss: 1.383 | Accuracy: 0.505500 | 1.155 sec/iter\n",
      "Epoch: 39 | Batch: 003 / 011 | Total loss: 1.347 | Reg loss: 0.024 | Tree loss: 1.347 | Accuracy: 0.556500 | 1.154 sec/iter\n",
      "Epoch: 39 | Batch: 004 / 011 | Total loss: 1.301 | Reg loss: 0.024 | Tree loss: 1.301 | Accuracy: 0.544500 | 1.154 sec/iter\n",
      "Epoch: 39 | Batch: 005 / 011 | Total loss: 1.295 | Reg loss: 0.024 | Tree loss: 1.295 | Accuracy: 0.531500 | 1.154 sec/iter\n",
      "Epoch: 39 | Batch: 006 / 011 | Total loss: 1.250 | Reg loss: 0.024 | Tree loss: 1.250 | Accuracy: 0.546000 | 1.153 sec/iter\n",
      "Epoch: 39 | Batch: 007 / 011 | Total loss: 1.263 | Reg loss: 0.024 | Tree loss: 1.263 | Accuracy: 0.555000 | 1.153 sec/iter\n",
      "Epoch: 39 | Batch: 008 / 011 | Total loss: 1.252 | Reg loss: 0.024 | Tree loss: 1.252 | Accuracy: 0.548000 | 1.152 sec/iter\n",
      "Epoch: 39 | Batch: 009 / 011 | Total loss: 1.260 | Reg loss: 0.024 | Tree loss: 1.260 | Accuracy: 0.542000 | 1.152 sec/iter\n",
      "Epoch: 39 | Batch: 010 / 011 | Total loss: 1.313 | Reg loss: 0.024 | Tree loss: 1.313 | Accuracy: 0.494881 | 1.151 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 40 | Batch: 000 / 011 | Total loss: 1.419 | Reg loss: 0.024 | Tree loss: 1.419 | Accuracy: 0.507000 | 1.156 sec/iter\n",
      "Epoch: 40 | Batch: 001 / 011 | Total loss: 1.404 | Reg loss: 0.024 | Tree loss: 1.404 | Accuracy: 0.495500 | 1.155 sec/iter\n",
      "Epoch: 40 | Batch: 002 / 011 | Total loss: 1.377 | Reg loss: 0.024 | Tree loss: 1.377 | Accuracy: 0.520000 | 1.155 sec/iter\n",
      "Epoch: 40 | Batch: 003 / 011 | Total loss: 1.337 | Reg loss: 0.024 | Tree loss: 1.337 | Accuracy: 0.549000 | 1.155 sec/iter\n",
      "Epoch: 40 | Batch: 004 / 011 | Total loss: 1.285 | Reg loss: 0.024 | Tree loss: 1.285 | Accuracy: 0.563000 | 1.155 sec/iter\n",
      "Epoch: 40 | Batch: 005 / 011 | Total loss: 1.291 | Reg loss: 0.024 | Tree loss: 1.291 | Accuracy: 0.533500 | 1.154 sec/iter\n",
      "Epoch: 40 | Batch: 006 / 011 | Total loss: 1.282 | Reg loss: 0.024 | Tree loss: 1.282 | Accuracy: 0.523500 | 1.154 sec/iter\n",
      "Epoch: 40 | Batch: 007 / 011 | Total loss: 1.252 | Reg loss: 0.024 | Tree loss: 1.252 | Accuracy: 0.543000 | 1.154 sec/iter\n",
      "Epoch: 40 | Batch: 008 / 011 | Total loss: 1.249 | Reg loss: 0.024 | Tree loss: 1.249 | Accuracy: 0.548000 | 1.153 sec/iter\n",
      "Epoch: 40 | Batch: 009 / 011 | Total loss: 1.238 | Reg loss: 0.024 | Tree loss: 1.238 | Accuracy: 0.560000 | 1.153 sec/iter\n",
      "Epoch: 40 | Batch: 010 / 011 | Total loss: 1.231 | Reg loss: 0.024 | Tree loss: 1.231 | Accuracy: 0.532423 | 1.152 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 41 | Batch: 000 / 011 | Total loss: 1.431 | Reg loss: 0.024 | Tree loss: 1.431 | Accuracy: 0.477500 | 1.158 sec/iter\n",
      "Epoch: 41 | Batch: 001 / 011 | Total loss: 1.392 | Reg loss: 0.024 | Tree loss: 1.392 | Accuracy: 0.487500 | 1.157 sec/iter\n",
      "Epoch: 41 | Batch: 002 / 011 | Total loss: 1.348 | Reg loss: 0.024 | Tree loss: 1.348 | Accuracy: 0.552500 | 1.157 sec/iter\n",
      "Epoch: 41 | Batch: 003 / 011 | Total loss: 1.327 | Reg loss: 0.024 | Tree loss: 1.327 | Accuracy: 0.528500 | 1.157 sec/iter\n",
      "Epoch: 41 | Batch: 004 / 011 | Total loss: 1.295 | Reg loss: 0.024 | Tree loss: 1.295 | Accuracy: 0.536500 | 1.156 sec/iter\n",
      "Epoch: 41 | Batch: 005 / 011 | Total loss: 1.267 | Reg loss: 0.024 | Tree loss: 1.267 | Accuracy: 0.541500 | 1.156 sec/iter\n",
      "Epoch: 41 | Batch: 006 / 011 | Total loss: 1.235 | Reg loss: 0.024 | Tree loss: 1.235 | Accuracy: 0.526000 | 1.156 sec/iter\n",
      "Epoch: 41 | Batch: 007 / 011 | Total loss: 1.241 | Reg loss: 0.024 | Tree loss: 1.241 | Accuracy: 0.563000 | 1.155 sec/iter\n",
      "Epoch: 41 | Batch: 008 / 011 | Total loss: 1.243 | Reg loss: 0.024 | Tree loss: 1.243 | Accuracy: 0.551000 | 1.155 sec/iter\n",
      "Epoch: 41 | Batch: 009 / 011 | Total loss: 1.235 | Reg loss: 0.024 | Tree loss: 1.235 | Accuracy: 0.544000 | 1.154 sec/iter\n",
      "Epoch: 41 | Batch: 010 / 011 | Total loss: 1.244 | Reg loss: 0.024 | Tree loss: 1.244 | Accuracy: 0.539249 | 1.154 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 42 | Batch: 000 / 011 | Total loss: 1.417 | Reg loss: 0.024 | Tree loss: 1.417 | Accuracy: 0.492500 | 1.159 sec/iter\n",
      "Epoch: 42 | Batch: 001 / 011 | Total loss: 1.371 | Reg loss: 0.024 | Tree loss: 1.371 | Accuracy: 0.513000 | 1.158 sec/iter\n",
      "Epoch: 42 | Batch: 002 / 011 | Total loss: 1.355 | Reg loss: 0.024 | Tree loss: 1.355 | Accuracy: 0.528500 | 1.158 sec/iter\n",
      "Epoch: 42 | Batch: 003 / 011 | Total loss: 1.344 | Reg loss: 0.024 | Tree loss: 1.344 | Accuracy: 0.512000 | 1.158 sec/iter\n",
      "Epoch: 42 | Batch: 004 / 011 | Total loss: 1.299 | Reg loss: 0.024 | Tree loss: 1.299 | Accuracy: 0.547000 | 1.158 sec/iter\n",
      "Epoch: 42 | Batch: 005 / 011 | Total loss: 1.261 | Reg loss: 0.024 | Tree loss: 1.261 | Accuracy: 0.540500 | 1.157 sec/iter\n",
      "Epoch: 42 | Batch: 006 / 011 | Total loss: 1.250 | Reg loss: 0.024 | Tree loss: 1.250 | Accuracy: 0.538000 | 1.157 sec/iter\n",
      "Epoch: 42 | Batch: 007 / 011 | Total loss: 1.250 | Reg loss: 0.024 | Tree loss: 1.250 | Accuracy: 0.540500 | 1.157 sec/iter\n",
      "Epoch: 42 | Batch: 008 / 011 | Total loss: 1.214 | Reg loss: 0.024 | Tree loss: 1.214 | Accuracy: 0.559500 | 1.156 sec/iter\n",
      "Epoch: 42 | Batch: 009 / 011 | Total loss: 1.221 | Reg loss: 0.024 | Tree loss: 1.221 | Accuracy: 0.553000 | 1.156 sec/iter\n",
      "Epoch: 42 | Batch: 010 / 011 | Total loss: 1.250 | Reg loss: 0.024 | Tree loss: 1.250 | Accuracy: 0.522184 | 1.155 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 43 | Batch: 000 / 011 | Total loss: 1.405 | Reg loss: 0.024 | Tree loss: 1.405 | Accuracy: 0.499000 | 1.161 sec/iter\n",
      "Epoch: 43 | Batch: 001 / 011 | Total loss: 1.372 | Reg loss: 0.024 | Tree loss: 1.372 | Accuracy: 0.518000 | 1.16 sec/iter\n",
      "Epoch: 43 | Batch: 002 / 011 | Total loss: 1.320 | Reg loss: 0.024 | Tree loss: 1.320 | Accuracy: 0.562000 | 1.16 sec/iter\n",
      "Epoch: 43 | Batch: 003 / 011 | Total loss: 1.277 | Reg loss: 0.024 | Tree loss: 1.277 | Accuracy: 0.556000 | 1.159 sec/iter\n",
      "Epoch: 43 | Batch: 004 / 011 | Total loss: 1.288 | Reg loss: 0.024 | Tree loss: 1.288 | Accuracy: 0.535000 | 1.159 sec/iter\n",
      "Epoch: 43 | Batch: 005 / 011 | Total loss: 1.259 | Reg loss: 0.024 | Tree loss: 1.259 | Accuracy: 0.523500 | 1.159 sec/iter\n",
      "Epoch: 43 | Batch: 006 / 011 | Total loss: 1.252 | Reg loss: 0.024 | Tree loss: 1.252 | Accuracy: 0.518000 | 1.159 sec/iter\n",
      "Epoch: 43 | Batch: 007 / 011 | Total loss: 1.238 | Reg loss: 0.024 | Tree loss: 1.238 | Accuracy: 0.547000 | 1.159 sec/iter\n",
      "Epoch: 43 | Batch: 008 / 011 | Total loss: 1.230 | Reg loss: 0.024 | Tree loss: 1.230 | Accuracy: 0.579000 | 1.158 sec/iter\n",
      "Epoch: 43 | Batch: 009 / 011 | Total loss: 1.230 | Reg loss: 0.024 | Tree loss: 1.230 | Accuracy: 0.558000 | 1.158 sec/iter\n",
      "Epoch: 43 | Batch: 010 / 011 | Total loss: 1.221 | Reg loss: 0.024 | Tree loss: 1.221 | Accuracy: 0.552901 | 1.157 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 44 | Batch: 000 / 011 | Total loss: 1.386 | Reg loss: 0.024 | Tree loss: 1.386 | Accuracy: 0.502000 | 1.158 sec/iter\n",
      "Epoch: 44 | Batch: 001 / 011 | Total loss: 1.372 | Reg loss: 0.024 | Tree loss: 1.372 | Accuracy: 0.506500 | 1.158 sec/iter\n",
      "Epoch: 44 | Batch: 002 / 011 | Total loss: 1.347 | Reg loss: 0.024 | Tree loss: 1.347 | Accuracy: 0.531000 | 1.157 sec/iter\n",
      "Epoch: 44 | Batch: 003 / 011 | Total loss: 1.305 | Reg loss: 0.024 | Tree loss: 1.305 | Accuracy: 0.551000 | 1.156 sec/iter\n",
      "Epoch: 44 | Batch: 004 / 011 | Total loss: 1.270 | Reg loss: 0.024 | Tree loss: 1.270 | Accuracy: 0.547000 | 1.156 sec/iter\n",
      "Epoch: 44 | Batch: 005 / 011 | Total loss: 1.259 | Reg loss: 0.024 | Tree loss: 1.259 | Accuracy: 0.533500 | 1.155 sec/iter\n",
      "Epoch: 44 | Batch: 006 / 011 | Total loss: 1.225 | Reg loss: 0.024 | Tree loss: 1.225 | Accuracy: 0.543500 | 1.155 sec/iter\n",
      "Epoch: 44 | Batch: 007 / 011 | Total loss: 1.222 | Reg loss: 0.024 | Tree loss: 1.222 | Accuracy: 0.527000 | 1.155 sec/iter\n",
      "Epoch: 44 | Batch: 008 / 011 | Total loss: 1.236 | Reg loss: 0.024 | Tree loss: 1.236 | Accuracy: 0.549500 | 1.155 sec/iter\n",
      "Epoch: 44 | Batch: 009 / 011 | Total loss: 1.200 | Reg loss: 0.024 | Tree loss: 1.200 | Accuracy: 0.548500 | 1.155 sec/iter\n",
      "Epoch: 44 | Batch: 010 / 011 | Total loss: 1.148 | Reg loss: 0.024 | Tree loss: 1.148 | Accuracy: 0.607509 | 1.154 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 45 | Batch: 000 / 011 | Total loss: 1.393 | Reg loss: 0.024 | Tree loss: 1.393 | Accuracy: 0.512500 | 1.156 sec/iter\n",
      "Epoch: 45 | Batch: 001 / 011 | Total loss: 1.335 | Reg loss: 0.024 | Tree loss: 1.335 | Accuracy: 0.535000 | 1.156 sec/iter\n",
      "Epoch: 45 | Batch: 002 / 011 | Total loss: 1.331 | Reg loss: 0.024 | Tree loss: 1.331 | Accuracy: 0.537500 | 1.155 sec/iter\n",
      "Epoch: 45 | Batch: 003 / 011 | Total loss: 1.272 | Reg loss: 0.024 | Tree loss: 1.272 | Accuracy: 0.551000 | 1.155 sec/iter\n",
      "Epoch: 45 | Batch: 004 / 011 | Total loss: 1.244 | Reg loss: 0.024 | Tree loss: 1.244 | Accuracy: 0.556000 | 1.155 sec/iter\n",
      "Epoch: 45 | Batch: 005 / 011 | Total loss: 1.238 | Reg loss: 0.024 | Tree loss: 1.238 | Accuracy: 0.538500 | 1.154 sec/iter\n",
      "Epoch: 45 | Batch: 006 / 011 | Total loss: 1.237 | Reg loss: 0.024 | Tree loss: 1.237 | Accuracy: 0.534000 | 1.154 sec/iter\n",
      "Epoch: 45 | Batch: 007 / 011 | Total loss: 1.227 | Reg loss: 0.024 | Tree loss: 1.227 | Accuracy: 0.537500 | 1.154 sec/iter\n",
      "Epoch: 45 | Batch: 008 / 011 | Total loss: 1.209 | Reg loss: 0.024 | Tree loss: 1.209 | Accuracy: 0.539500 | 1.153 sec/iter\n",
      "Epoch: 45 | Batch: 009 / 011 | Total loss: 1.204 | Reg loss: 0.024 | Tree loss: 1.204 | Accuracy: 0.562500 | 1.153 sec/iter\n",
      "Epoch: 45 | Batch: 010 / 011 | Total loss: 1.175 | Reg loss: 0.024 | Tree loss: 1.175 | Accuracy: 0.566553 | 1.153 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 46 | Batch: 000 / 011 | Total loss: 1.364 | Reg loss: 0.024 | Tree loss: 1.364 | Accuracy: 0.496500 | 1.156 sec/iter\n",
      "Epoch: 46 | Batch: 001 / 011 | Total loss: 1.336 | Reg loss: 0.024 | Tree loss: 1.336 | Accuracy: 0.518000 | 1.155 sec/iter\n",
      "Epoch: 46 | Batch: 002 / 011 | Total loss: 1.312 | Reg loss: 0.024 | Tree loss: 1.312 | Accuracy: 0.560000 | 1.155 sec/iter\n",
      "Epoch: 46 | Batch: 003 / 011 | Total loss: 1.285 | Reg loss: 0.024 | Tree loss: 1.285 | Accuracy: 0.556000 | 1.155 sec/iter\n",
      "Epoch: 46 | Batch: 004 / 011 | Total loss: 1.245 | Reg loss: 0.024 | Tree loss: 1.245 | Accuracy: 0.548500 | 1.155 sec/iter\n",
      "Epoch: 46 | Batch: 005 / 011 | Total loss: 1.257 | Reg loss: 0.024 | Tree loss: 1.257 | Accuracy: 0.520500 | 1.154 sec/iter\n",
      "Epoch: 46 | Batch: 006 / 011 | Total loss: 1.236 | Reg loss: 0.024 | Tree loss: 1.236 | Accuracy: 0.521500 | 1.154 sec/iter\n",
      "Epoch: 46 | Batch: 007 / 011 | Total loss: 1.221 | Reg loss: 0.024 | Tree loss: 1.221 | Accuracy: 0.525500 | 1.154 sec/iter\n",
      "Epoch: 46 | Batch: 008 / 011 | Total loss: 1.189 | Reg loss: 0.024 | Tree loss: 1.189 | Accuracy: 0.573500 | 1.153 sec/iter\n",
      "Epoch: 46 | Batch: 009 / 011 | Total loss: 1.189 | Reg loss: 0.024 | Tree loss: 1.189 | Accuracy: 0.562000 | 1.153 sec/iter\n",
      "Epoch: 46 | Batch: 010 / 011 | Total loss: 1.197 | Reg loss: 0.024 | Tree loss: 1.197 | Accuracy: 0.511945 | 1.153 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 47 | Batch: 000 / 011 | Total loss: 1.362 | Reg loss: 0.024 | Tree loss: 1.362 | Accuracy: 0.513000 | 1.157 sec/iter\n",
      "Epoch: 47 | Batch: 001 / 011 | Total loss: 1.334 | Reg loss: 0.024 | Tree loss: 1.334 | Accuracy: 0.524500 | 1.157 sec/iter\n",
      "Epoch: 47 | Batch: 002 / 011 | Total loss: 1.304 | Reg loss: 0.024 | Tree loss: 1.304 | Accuracy: 0.566500 | 1.157 sec/iter\n",
      "Epoch: 47 | Batch: 003 / 011 | Total loss: 1.258 | Reg loss: 0.024 | Tree loss: 1.258 | Accuracy: 0.560000 | 1.156 sec/iter\n",
      "Epoch: 47 | Batch: 004 / 011 | Total loss: 1.249 | Reg loss: 0.024 | Tree loss: 1.249 | Accuracy: 0.564000 | 1.156 sec/iter\n",
      "Epoch: 47 | Batch: 005 / 011 | Total loss: 1.233 | Reg loss: 0.024 | Tree loss: 1.233 | Accuracy: 0.559000 | 1.156 sec/iter\n",
      "Epoch: 47 | Batch: 006 / 011 | Total loss: 1.203 | Reg loss: 0.024 | Tree loss: 1.203 | Accuracy: 0.586000 | 1.156 sec/iter\n",
      "Epoch: 47 | Batch: 007 / 011 | Total loss: 1.224 | Reg loss: 0.024 | Tree loss: 1.224 | Accuracy: 0.594000 | 1.155 sec/iter\n",
      "Epoch: 47 | Batch: 008 / 011 | Total loss: 1.193 | Reg loss: 0.024 | Tree loss: 1.193 | Accuracy: 0.622000 | 1.155 sec/iter\n",
      "Epoch: 47 | Batch: 009 / 011 | Total loss: 1.201 | Reg loss: 0.024 | Tree loss: 1.201 | Accuracy: 0.600500 | 1.154 sec/iter\n",
      "Epoch: 47 | Batch: 010 / 011 | Total loss: 1.184 | Reg loss: 0.024 | Tree loss: 1.184 | Accuracy: 0.624573 | 1.154 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 48 | Batch: 000 / 011 | Total loss: 1.354 | Reg loss: 0.024 | Tree loss: 1.354 | Accuracy: 0.522000 | 1.159 sec/iter\n",
      "Epoch: 48 | Batch: 001 / 011 | Total loss: 1.325 | Reg loss: 0.024 | Tree loss: 1.325 | Accuracy: 0.555500 | 1.158 sec/iter\n",
      "Epoch: 48 | Batch: 002 / 011 | Total loss: 1.286 | Reg loss: 0.024 | Tree loss: 1.286 | Accuracy: 0.553000 | 1.158 sec/iter\n",
      "Epoch: 48 | Batch: 003 / 011 | Total loss: 1.280 | Reg loss: 0.024 | Tree loss: 1.280 | Accuracy: 0.565500 | 1.158 sec/iter\n",
      "Epoch: 48 | Batch: 004 / 011 | Total loss: 1.256 | Reg loss: 0.024 | Tree loss: 1.256 | Accuracy: 0.559500 | 1.158 sec/iter\n",
      "Epoch: 48 | Batch: 005 / 011 | Total loss: 1.240 | Reg loss: 0.024 | Tree loss: 1.240 | Accuracy: 0.572500 | 1.157 sec/iter\n",
      "Epoch: 48 | Batch: 006 / 011 | Total loss: 1.210 | Reg loss: 0.024 | Tree loss: 1.210 | Accuracy: 0.574000 | 1.157 sec/iter\n",
      "Epoch: 48 | Batch: 007 / 011 | Total loss: 1.199 | Reg loss: 0.024 | Tree loss: 1.199 | Accuracy: 0.579000 | 1.157 sec/iter\n",
      "Epoch: 48 | Batch: 008 / 011 | Total loss: 1.194 | Reg loss: 0.024 | Tree loss: 1.194 | Accuracy: 0.619000 | 1.156 sec/iter\n",
      "Epoch: 48 | Batch: 009 / 011 | Total loss: 1.180 | Reg loss: 0.024 | Tree loss: 1.180 | Accuracy: 0.604500 | 1.156 sec/iter\n",
      "Epoch: 48 | Batch: 010 / 011 | Total loss: 1.204 | Reg loss: 0.024 | Tree loss: 1.204 | Accuracy: 0.614334 | 1.155 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 49 | Batch: 000 / 011 | Total loss: 1.342 | Reg loss: 0.024 | Tree loss: 1.342 | Accuracy: 0.529000 | 1.16 sec/iter\n",
      "Epoch: 49 | Batch: 001 / 011 | Total loss: 1.322 | Reg loss: 0.024 | Tree loss: 1.322 | Accuracy: 0.539500 | 1.159 sec/iter\n",
      "Epoch: 49 | Batch: 002 / 011 | Total loss: 1.295 | Reg loss: 0.024 | Tree loss: 1.295 | Accuracy: 0.576000 | 1.159 sec/iter\n",
      "Epoch: 49 | Batch: 003 / 011 | Total loss: 1.258 | Reg loss: 0.024 | Tree loss: 1.258 | Accuracy: 0.573500 | 1.159 sec/iter\n",
      "Epoch: 49 | Batch: 004 / 011 | Total loss: 1.232 | Reg loss: 0.024 | Tree loss: 1.232 | Accuracy: 0.587000 | 1.159 sec/iter\n",
      "Epoch: 49 | Batch: 005 / 011 | Total loss: 1.223 | Reg loss: 0.024 | Tree loss: 1.223 | Accuracy: 0.585500 | 1.158 sec/iter\n",
      "Epoch: 49 | Batch: 006 / 011 | Total loss: 1.175 | Reg loss: 0.024 | Tree loss: 1.175 | Accuracy: 0.616000 | 1.158 sec/iter\n",
      "Epoch: 49 | Batch: 007 / 011 | Total loss: 1.214 | Reg loss: 0.024 | Tree loss: 1.214 | Accuracy: 0.578500 | 1.158 sec/iter\n",
      "Epoch: 49 | Batch: 008 / 011 | Total loss: 1.196 | Reg loss: 0.024 | Tree loss: 1.196 | Accuracy: 0.615000 | 1.157 sec/iter\n",
      "Epoch: 49 | Batch: 009 / 011 | Total loss: 1.182 | Reg loss: 0.024 | Tree loss: 1.182 | Accuracy: 0.613000 | 1.157 sec/iter\n",
      "Epoch: 49 | Batch: 010 / 011 | Total loss: 1.186 | Reg loss: 0.024 | Tree loss: 1.186 | Accuracy: 0.593857 | 1.156 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 50 | Batch: 000 / 011 | Total loss: 1.330 | Reg loss: 0.024 | Tree loss: 1.330 | Accuracy: 0.546000 | 1.157 sec/iter\n",
      "Epoch: 50 | Batch: 001 / 011 | Total loss: 1.306 | Reg loss: 0.024 | Tree loss: 1.306 | Accuracy: 0.576500 | 1.156 sec/iter\n",
      "Epoch: 50 | Batch: 002 / 011 | Total loss: 1.299 | Reg loss: 0.024 | Tree loss: 1.299 | Accuracy: 0.597000 | 1.156 sec/iter\n",
      "Epoch: 50 | Batch: 003 / 011 | Total loss: 1.272 | Reg loss: 0.024 | Tree loss: 1.272 | Accuracy: 0.585500 | 1.155 sec/iter\n",
      "Epoch: 50 | Batch: 004 / 011 | Total loss: 1.228 | Reg loss: 0.024 | Tree loss: 1.228 | Accuracy: 0.600500 | 1.155 sec/iter\n",
      "Epoch: 50 | Batch: 005 / 011 | Total loss: 1.216 | Reg loss: 0.024 | Tree loss: 1.216 | Accuracy: 0.578000 | 1.154 sec/iter\n",
      "Epoch: 50 | Batch: 006 / 011 | Total loss: 1.202 | Reg loss: 0.024 | Tree loss: 1.202 | Accuracy: 0.587500 | 1.154 sec/iter\n",
      "Epoch: 50 | Batch: 007 / 011 | Total loss: 1.200 | Reg loss: 0.024 | Tree loss: 1.200 | Accuracy: 0.578500 | 1.154 sec/iter\n",
      "Epoch: 50 | Batch: 008 / 011 | Total loss: 1.182 | Reg loss: 0.024 | Tree loss: 1.182 | Accuracy: 0.602000 | 1.154 sec/iter\n",
      "Epoch: 50 | Batch: 009 / 011 | Total loss: 1.177 | Reg loss: 0.024 | Tree loss: 1.177 | Accuracy: 0.590000 | 1.153 sec/iter\n",
      "Epoch: 50 | Batch: 010 / 011 | Total loss: 1.130 | Reg loss: 0.024 | Tree loss: 1.130 | Accuracy: 0.641638 | 1.153 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 51 | Batch: 000 / 011 | Total loss: 1.345 | Reg loss: 0.024 | Tree loss: 1.345 | Accuracy: 0.556500 | 1.154 sec/iter\n",
      "Epoch: 51 | Batch: 001 / 011 | Total loss: 1.301 | Reg loss: 0.024 | Tree loss: 1.301 | Accuracy: 0.588000 | 1.154 sec/iter\n",
      "Epoch: 51 | Batch: 002 / 011 | Total loss: 1.277 | Reg loss: 0.024 | Tree loss: 1.277 | Accuracy: 0.598000 | 1.154 sec/iter\n",
      "Epoch: 51 | Batch: 003 / 011 | Total loss: 1.251 | Reg loss: 0.024 | Tree loss: 1.251 | Accuracy: 0.591000 | 1.154 sec/iter\n",
      "Epoch: 51 | Batch: 004 / 011 | Total loss: 1.219 | Reg loss: 0.024 | Tree loss: 1.219 | Accuracy: 0.598000 | 1.153 sec/iter\n",
      "Epoch: 51 | Batch: 005 / 011 | Total loss: 1.185 | Reg loss: 0.024 | Tree loss: 1.185 | Accuracy: 0.626000 | 1.153 sec/iter\n",
      "Epoch: 51 | Batch: 006 / 011 | Total loss: 1.207 | Reg loss: 0.024 | Tree loss: 1.207 | Accuracy: 0.586000 | 1.153 sec/iter\n",
      "Epoch: 51 | Batch: 007 / 011 | Total loss: 1.177 | Reg loss: 0.024 | Tree loss: 1.177 | Accuracy: 0.606000 | 1.152 sec/iter\n",
      "Epoch: 51 | Batch: 008 / 011 | Total loss: 1.193 | Reg loss: 0.024 | Tree loss: 1.193 | Accuracy: 0.604500 | 1.152 sec/iter\n",
      "Epoch: 51 | Batch: 009 / 011 | Total loss: 1.164 | Reg loss: 0.024 | Tree loss: 1.164 | Accuracy: 0.614000 | 1.152 sec/iter\n",
      "Epoch: 51 | Batch: 010 / 011 | Total loss: 1.208 | Reg loss: 0.024 | Tree loss: 1.208 | Accuracy: 0.563140 | 1.152 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 52 | Batch: 000 / 011 | Total loss: 1.321 | Reg loss: 0.024 | Tree loss: 1.321 | Accuracy: 0.562500 | 1.155 sec/iter\n",
      "Epoch: 52 | Batch: 001 / 011 | Total loss: 1.312 | Reg loss: 0.024 | Tree loss: 1.312 | Accuracy: 0.592000 | 1.155 sec/iter\n",
      "Epoch: 52 | Batch: 002 / 011 | Total loss: 1.275 | Reg loss: 0.024 | Tree loss: 1.275 | Accuracy: 0.624000 | 1.155 sec/iter\n",
      "Epoch: 52 | Batch: 003 / 011 | Total loss: 1.256 | Reg loss: 0.024 | Tree loss: 1.256 | Accuracy: 0.603500 | 1.154 sec/iter\n",
      "Epoch: 52 | Batch: 004 / 011 | Total loss: 1.241 | Reg loss: 0.024 | Tree loss: 1.241 | Accuracy: 0.589500 | 1.154 sec/iter\n",
      "Epoch: 52 | Batch: 005 / 011 | Total loss: 1.185 | Reg loss: 0.024 | Tree loss: 1.185 | Accuracy: 0.616000 | 1.154 sec/iter\n",
      "Epoch: 52 | Batch: 006 / 011 | Total loss: 1.197 | Reg loss: 0.024 | Tree loss: 1.197 | Accuracy: 0.580000 | 1.154 sec/iter\n",
      "Epoch: 52 | Batch: 007 / 011 | Total loss: 1.163 | Reg loss: 0.024 | Tree loss: 1.163 | Accuracy: 0.593500 | 1.153 sec/iter\n",
      "Epoch: 52 | Batch: 008 / 011 | Total loss: 1.162 | Reg loss: 0.024 | Tree loss: 1.162 | Accuracy: 0.581000 | 1.153 sec/iter\n",
      "Epoch: 52 | Batch: 009 / 011 | Total loss: 1.178 | Reg loss: 0.024 | Tree loss: 1.178 | Accuracy: 0.591000 | 1.153 sec/iter\n",
      "Epoch: 52 | Batch: 010 / 011 | Total loss: 1.233 | Reg loss: 0.024 | Tree loss: 1.233 | Accuracy: 0.525597 | 1.152 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 53 | Batch: 000 / 011 | Total loss: 1.328 | Reg loss: 0.024 | Tree loss: 1.328 | Accuracy: 0.559000 | 1.156 sec/iter\n",
      "Epoch: 53 | Batch: 001 / 011 | Total loss: 1.302 | Reg loss: 0.024 | Tree loss: 1.302 | Accuracy: 0.581500 | 1.156 sec/iter\n",
      "Epoch: 53 | Batch: 002 / 011 | Total loss: 1.250 | Reg loss: 0.024 | Tree loss: 1.250 | Accuracy: 0.617000 | 1.155 sec/iter\n",
      "Epoch: 53 | Batch: 003 / 011 | Total loss: 1.244 | Reg loss: 0.024 | Tree loss: 1.244 | Accuracy: 0.586000 | 1.155 sec/iter\n",
      "Epoch: 53 | Batch: 004 / 011 | Total loss: 1.203 | Reg loss: 0.024 | Tree loss: 1.203 | Accuracy: 0.611000 | 1.155 sec/iter\n",
      "Epoch: 53 | Batch: 005 / 011 | Total loss: 1.200 | Reg loss: 0.024 | Tree loss: 1.200 | Accuracy: 0.574000 | 1.155 sec/iter\n",
      "Epoch: 53 | Batch: 006 / 011 | Total loss: 1.196 | Reg loss: 0.024 | Tree loss: 1.196 | Accuracy: 0.593000 | 1.155 sec/iter\n",
      "Epoch: 53 | Batch: 007 / 011 | Total loss: 1.179 | Reg loss: 0.024 | Tree loss: 1.179 | Accuracy: 0.604500 | 1.154 sec/iter\n",
      "Epoch: 53 | Batch: 008 / 011 | Total loss: 1.183 | Reg loss: 0.024 | Tree loss: 1.183 | Accuracy: 0.605000 | 1.154 sec/iter\n",
      "Epoch: 53 | Batch: 009 / 011 | Total loss: 1.136 | Reg loss: 0.024 | Tree loss: 1.136 | Accuracy: 0.609500 | 1.154 sec/iter\n",
      "Epoch: 53 | Batch: 010 / 011 | Total loss: 1.146 | Reg loss: 0.024 | Tree loss: 1.146 | Accuracy: 0.597270 | 1.153 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 54 | Batch: 000 / 011 | Total loss: 1.314 | Reg loss: 0.024 | Tree loss: 1.314 | Accuracy: 0.560000 | 1.158 sec/iter\n",
      "Epoch: 54 | Batch: 001 / 011 | Total loss: 1.283 | Reg loss: 0.024 | Tree loss: 1.283 | Accuracy: 0.599500 | 1.157 sec/iter\n",
      "Epoch: 54 | Batch: 002 / 011 | Total loss: 1.259 | Reg loss: 0.024 | Tree loss: 1.259 | Accuracy: 0.606500 | 1.157 sec/iter\n",
      "Epoch: 54 | Batch: 003 / 011 | Total loss: 1.234 | Reg loss: 0.024 | Tree loss: 1.234 | Accuracy: 0.603500 | 1.157 sec/iter\n",
      "Epoch: 54 | Batch: 004 / 011 | Total loss: 1.193 | Reg loss: 0.024 | Tree loss: 1.193 | Accuracy: 0.612000 | 1.157 sec/iter\n",
      "Epoch: 54 | Batch: 005 / 011 | Total loss: 1.197 | Reg loss: 0.024 | Tree loss: 1.197 | Accuracy: 0.591500 | 1.156 sec/iter\n",
      "Epoch: 54 | Batch: 006 / 011 | Total loss: 1.194 | Reg loss: 0.024 | Tree loss: 1.194 | Accuracy: 0.590000 | 1.156 sec/iter\n",
      "Epoch: 54 | Batch: 007 / 011 | Total loss: 1.150 | Reg loss: 0.024 | Tree loss: 1.150 | Accuracy: 0.609500 | 1.156 sec/iter\n",
      "Epoch: 54 | Batch: 008 / 011 | Total loss: 1.174 | Reg loss: 0.024 | Tree loss: 1.174 | Accuracy: 0.595000 | 1.155 sec/iter\n",
      "Epoch: 54 | Batch: 009 / 011 | Total loss: 1.166 | Reg loss: 0.024 | Tree loss: 1.166 | Accuracy: 0.597500 | 1.155 sec/iter\n",
      "Epoch: 54 | Batch: 010 / 011 | Total loss: 1.205 | Reg loss: 0.024 | Tree loss: 1.205 | Accuracy: 0.566553 | 1.155 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 55 | Batch: 000 / 011 | Total loss: 1.311 | Reg loss: 0.024 | Tree loss: 1.311 | Accuracy: 0.544500 | 1.159 sec/iter\n",
      "Epoch: 55 | Batch: 001 / 011 | Total loss: 1.286 | Reg loss: 0.024 | Tree loss: 1.286 | Accuracy: 0.585500 | 1.158 sec/iter\n",
      "Epoch: 55 | Batch: 002 / 011 | Total loss: 1.244 | Reg loss: 0.024 | Tree loss: 1.244 | Accuracy: 0.596500 | 1.158 sec/iter\n",
      "Epoch: 55 | Batch: 003 / 011 | Total loss: 1.245 | Reg loss: 0.024 | Tree loss: 1.245 | Accuracy: 0.589000 | 1.158 sec/iter\n",
      "Epoch: 55 | Batch: 004 / 011 | Total loss: 1.207 | Reg loss: 0.024 | Tree loss: 1.207 | Accuracy: 0.605500 | 1.158 sec/iter\n",
      "Epoch: 55 | Batch: 005 / 011 | Total loss: 1.182 | Reg loss: 0.024 | Tree loss: 1.182 | Accuracy: 0.606500 | 1.158 sec/iter\n",
      "Epoch: 55 | Batch: 006 / 011 | Total loss: 1.189 | Reg loss: 0.024 | Tree loss: 1.189 | Accuracy: 0.586500 | 1.157 sec/iter\n",
      "Epoch: 55 | Batch: 007 / 011 | Total loss: 1.153 | Reg loss: 0.024 | Tree loss: 1.153 | Accuracy: 0.608500 | 1.157 sec/iter\n",
      "Epoch: 55 | Batch: 008 / 011 | Total loss: 1.148 | Reg loss: 0.024 | Tree loss: 1.148 | Accuracy: 0.611000 | 1.156 sec/iter\n",
      "Epoch: 55 | Batch: 009 / 011 | Total loss: 1.127 | Reg loss: 0.024 | Tree loss: 1.127 | Accuracy: 0.622000 | 1.156 sec/iter\n",
      "Epoch: 55 | Batch: 010 / 011 | Total loss: 1.178 | Reg loss: 0.024 | Tree loss: 1.178 | Accuracy: 0.569966 | 1.156 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 56 | Batch: 000 / 011 | Total loss: 1.293 | Reg loss: 0.024 | Tree loss: 1.293 | Accuracy: 0.558500 | 1.156 sec/iter\n",
      "Epoch: 56 | Batch: 001 / 011 | Total loss: 1.296 | Reg loss: 0.024 | Tree loss: 1.296 | Accuracy: 0.572000 | 1.156 sec/iter\n",
      "Epoch: 56 | Batch: 002 / 011 | Total loss: 1.254 | Reg loss: 0.024 | Tree loss: 1.254 | Accuracy: 0.602500 | 1.155 sec/iter\n",
      "Epoch: 56 | Batch: 003 / 011 | Total loss: 1.251 | Reg loss: 0.024 | Tree loss: 1.251 | Accuracy: 0.575000 | 1.155 sec/iter\n",
      "Epoch: 56 | Batch: 004 / 011 | Total loss: 1.195 | Reg loss: 0.024 | Tree loss: 1.195 | Accuracy: 0.599500 | 1.154 sec/iter\n",
      "Epoch: 56 | Batch: 005 / 011 | Total loss: 1.177 | Reg loss: 0.024 | Tree loss: 1.177 | Accuracy: 0.607000 | 1.154 sec/iter\n",
      "Epoch: 56 | Batch: 006 / 011 | Total loss: 1.157 | Reg loss: 0.024 | Tree loss: 1.157 | Accuracy: 0.592500 | 1.154 sec/iter\n",
      "Epoch: 56 | Batch: 007 / 011 | Total loss: 1.159 | Reg loss: 0.024 | Tree loss: 1.159 | Accuracy: 0.598500 | 1.153 sec/iter\n",
      "Epoch: 56 | Batch: 008 / 011 | Total loss: 1.158 | Reg loss: 0.024 | Tree loss: 1.158 | Accuracy: 0.608500 | 1.153 sec/iter\n",
      "Epoch: 56 | Batch: 009 / 011 | Total loss: 1.137 | Reg loss: 0.024 | Tree loss: 1.137 | Accuracy: 0.609000 | 1.153 sec/iter\n",
      "Epoch: 56 | Batch: 010 / 011 | Total loss: 1.159 | Reg loss: 0.024 | Tree loss: 1.159 | Accuracy: 0.638225 | 1.153 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 57 | Batch: 000 / 011 | Total loss: 1.301 | Reg loss: 0.024 | Tree loss: 1.301 | Accuracy: 0.552500 | 1.154 sec/iter\n",
      "Epoch: 57 | Batch: 001 / 011 | Total loss: 1.270 | Reg loss: 0.024 | Tree loss: 1.270 | Accuracy: 0.580500 | 1.154 sec/iter\n",
      "Epoch: 57 | Batch: 002 / 011 | Total loss: 1.237 | Reg loss: 0.024 | Tree loss: 1.237 | Accuracy: 0.607000 | 1.153 sec/iter\n",
      "Epoch: 57 | Batch: 003 / 011 | Total loss: 1.219 | Reg loss: 0.024 | Tree loss: 1.219 | Accuracy: 0.593000 | 1.153 sec/iter\n",
      "Epoch: 57 | Batch: 004 / 011 | Total loss: 1.205 | Reg loss: 0.024 | Tree loss: 1.205 | Accuracy: 0.589000 | 1.153 sec/iter\n",
      "Epoch: 57 | Batch: 005 / 011 | Total loss: 1.192 | Reg loss: 0.024 | Tree loss: 1.192 | Accuracy: 0.583500 | 1.153 sec/iter\n",
      "Epoch: 57 | Batch: 006 / 011 | Total loss: 1.152 | Reg loss: 0.024 | Tree loss: 1.152 | Accuracy: 0.592000 | 1.152 sec/iter\n",
      "Epoch: 57 | Batch: 007 / 011 | Total loss: 1.151 | Reg loss: 0.024 | Tree loss: 1.151 | Accuracy: 0.607500 | 1.152 sec/iter\n",
      "Epoch: 57 | Batch: 008 / 011 | Total loss: 1.158 | Reg loss: 0.024 | Tree loss: 1.158 | Accuracy: 0.610000 | 1.152 sec/iter\n",
      "Epoch: 57 | Batch: 009 / 011 | Total loss: 1.160 | Reg loss: 0.024 | Tree loss: 1.160 | Accuracy: 0.601500 | 1.152 sec/iter\n",
      "Epoch: 57 | Batch: 010 / 011 | Total loss: 1.088 | Reg loss: 0.024 | Tree loss: 1.088 | Accuracy: 0.627986 | 1.151 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 58 | Batch: 000 / 011 | Total loss: 1.306 | Reg loss: 0.024 | Tree loss: 1.306 | Accuracy: 0.561000 | 1.155 sec/iter\n",
      "Epoch: 58 | Batch: 001 / 011 | Total loss: 1.275 | Reg loss: 0.024 | Tree loss: 1.275 | Accuracy: 0.594500 | 1.154 sec/iter\n",
      "Epoch: 58 | Batch: 002 / 011 | Total loss: 1.235 | Reg loss: 0.024 | Tree loss: 1.235 | Accuracy: 0.594000 | 1.154 sec/iter\n",
      "Epoch: 58 | Batch: 003 / 011 | Total loss: 1.223 | Reg loss: 0.024 | Tree loss: 1.223 | Accuracy: 0.595500 | 1.154 sec/iter\n",
      "Epoch: 58 | Batch: 004 / 011 | Total loss: 1.186 | Reg loss: 0.024 | Tree loss: 1.186 | Accuracy: 0.616500 | 1.154 sec/iter\n",
      "Epoch: 58 | Batch: 005 / 011 | Total loss: 1.176 | Reg loss: 0.024 | Tree loss: 1.176 | Accuracy: 0.597000 | 1.154 sec/iter\n",
      "Epoch: 58 | Batch: 006 / 011 | Total loss: 1.178 | Reg loss: 0.024 | Tree loss: 1.178 | Accuracy: 0.573000 | 1.154 sec/iter\n",
      "Epoch: 58 | Batch: 007 / 011 | Total loss: 1.153 | Reg loss: 0.024 | Tree loss: 1.153 | Accuracy: 0.584500 | 1.153 sec/iter\n",
      "Epoch: 58 | Batch: 008 / 011 | Total loss: 1.133 | Reg loss: 0.024 | Tree loss: 1.133 | Accuracy: 0.608500 | 1.153 sec/iter\n",
      "Epoch: 58 | Batch: 009 / 011 | Total loss: 1.136 | Reg loss: 0.024 | Tree loss: 1.136 | Accuracy: 0.613500 | 1.153 sec/iter\n",
      "Epoch: 58 | Batch: 010 / 011 | Total loss: 1.082 | Reg loss: 0.024 | Tree loss: 1.082 | Accuracy: 0.604096 | 1.152 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 59 | Batch: 000 / 011 | Total loss: 1.280 | Reg loss: 0.024 | Tree loss: 1.280 | Accuracy: 0.574000 | 1.156 sec/iter\n",
      "Epoch: 59 | Batch: 001 / 011 | Total loss: 1.272 | Reg loss: 0.024 | Tree loss: 1.272 | Accuracy: 0.600000 | 1.156 sec/iter\n",
      "Epoch: 59 | Batch: 002 / 011 | Total loss: 1.242 | Reg loss: 0.024 | Tree loss: 1.242 | Accuracy: 0.621500 | 1.155 sec/iter\n",
      "Epoch: 59 | Batch: 003 / 011 | Total loss: 1.199 | Reg loss: 0.024 | Tree loss: 1.199 | Accuracy: 0.593000 | 1.155 sec/iter\n",
      "Epoch: 59 | Batch: 004 / 011 | Total loss: 1.192 | Reg loss: 0.024 | Tree loss: 1.192 | Accuracy: 0.603500 | 1.155 sec/iter\n",
      "Epoch: 59 | Batch: 005 / 011 | Total loss: 1.168 | Reg loss: 0.024 | Tree loss: 1.168 | Accuracy: 0.587000 | 1.155 sec/iter\n",
      "Epoch: 59 | Batch: 006 / 011 | Total loss: 1.145 | Reg loss: 0.024 | Tree loss: 1.145 | Accuracy: 0.599500 | 1.155 sec/iter\n",
      "Epoch: 59 | Batch: 007 / 011 | Total loss: 1.152 | Reg loss: 0.024 | Tree loss: 1.152 | Accuracy: 0.584500 | 1.154 sec/iter\n",
      "Epoch: 59 | Batch: 008 / 011 | Total loss: 1.166 | Reg loss: 0.024 | Tree loss: 1.166 | Accuracy: 0.573500 | 1.154 sec/iter\n",
      "Epoch: 59 | Batch: 009 / 011 | Total loss: 1.128 | Reg loss: 0.024 | Tree loss: 1.128 | Accuracy: 0.611500 | 1.154 sec/iter\n",
      "Epoch: 59 | Batch: 010 / 011 | Total loss: 1.125 | Reg loss: 0.024 | Tree loss: 1.125 | Accuracy: 0.542662 | 1.153 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 60 | Batch: 000 / 011 | Total loss: 1.276 | Reg loss: 0.024 | Tree loss: 1.276 | Accuracy: 0.592500 | 1.157 sec/iter\n",
      "Epoch: 60 | Batch: 001 / 011 | Total loss: 1.257 | Reg loss: 0.024 | Tree loss: 1.257 | Accuracy: 0.609500 | 1.157 sec/iter\n",
      "Epoch: 60 | Batch: 002 / 011 | Total loss: 1.233 | Reg loss: 0.024 | Tree loss: 1.233 | Accuracy: 0.611000 | 1.156 sec/iter\n",
      "Epoch: 60 | Batch: 003 / 011 | Total loss: 1.188 | Reg loss: 0.024 | Tree loss: 1.188 | Accuracy: 0.613000 | 1.156 sec/iter\n",
      "Epoch: 60 | Batch: 004 / 011 | Total loss: 1.187 | Reg loss: 0.024 | Tree loss: 1.187 | Accuracy: 0.604000 | 1.156 sec/iter\n",
      "Epoch: 60 | Batch: 005 / 011 | Total loss: 1.163 | Reg loss: 0.024 | Tree loss: 1.163 | Accuracy: 0.594000 | 1.156 sec/iter\n",
      "Epoch: 60 | Batch: 006 / 011 | Total loss: 1.151 | Reg loss: 0.024 | Tree loss: 1.151 | Accuracy: 0.601000 | 1.156 sec/iter\n",
      "Epoch: 60 | Batch: 007 / 011 | Total loss: 1.158 | Reg loss: 0.024 | Tree loss: 1.158 | Accuracy: 0.594000 | 1.155 sec/iter\n",
      "Epoch: 60 | Batch: 008 / 011 | Total loss: 1.151 | Reg loss: 0.024 | Tree loss: 1.151 | Accuracy: 0.587500 | 1.155 sec/iter\n",
      "Epoch: 60 | Batch: 009 / 011 | Total loss: 1.151 | Reg loss: 0.024 | Tree loss: 1.151 | Accuracy: 0.595500 | 1.155 sec/iter\n",
      "Epoch: 60 | Batch: 010 / 011 | Total loss: 1.068 | Reg loss: 0.024 | Tree loss: 1.068 | Accuracy: 0.645051 | 1.155 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 61 | Batch: 000 / 011 | Total loss: 1.290 | Reg loss: 0.024 | Tree loss: 1.290 | Accuracy: 0.568500 | 1.158 sec/iter\n",
      "Epoch: 61 | Batch: 001 / 011 | Total loss: 1.249 | Reg loss: 0.024 | Tree loss: 1.249 | Accuracy: 0.635500 | 1.158 sec/iter\n",
      "Epoch: 61 | Batch: 002 / 011 | Total loss: 1.249 | Reg loss: 0.024 | Tree loss: 1.249 | Accuracy: 0.599500 | 1.157 sec/iter\n",
      "Epoch: 61 | Batch: 003 / 011 | Total loss: 1.201 | Reg loss: 0.024 | Tree loss: 1.201 | Accuracy: 0.610500 | 1.157 sec/iter\n",
      "Epoch: 61 | Batch: 004 / 011 | Total loss: 1.177 | Reg loss: 0.024 | Tree loss: 1.177 | Accuracy: 0.616000 | 1.157 sec/iter\n",
      "Epoch: 61 | Batch: 005 / 011 | Total loss: 1.164 | Reg loss: 0.024 | Tree loss: 1.164 | Accuracy: 0.585500 | 1.157 sec/iter\n",
      "Epoch: 61 | Batch: 006 / 011 | Total loss: 1.132 | Reg loss: 0.024 | Tree loss: 1.132 | Accuracy: 0.588500 | 1.157 sec/iter\n",
      "Epoch: 61 | Batch: 007 / 011 | Total loss: 1.154 | Reg loss: 0.024 | Tree loss: 1.154 | Accuracy: 0.563000 | 1.156 sec/iter\n",
      "Epoch: 61 | Batch: 008 / 011 | Total loss: 1.113 | Reg loss: 0.024 | Tree loss: 1.113 | Accuracy: 0.597000 | 1.156 sec/iter\n",
      "Epoch: 61 | Batch: 009 / 011 | Total loss: 1.146 | Reg loss: 0.024 | Tree loss: 1.146 | Accuracy: 0.576000 | 1.156 sec/iter\n",
      "Epoch: 61 | Batch: 010 / 011 | Total loss: 1.107 | Reg loss: 0.024 | Tree loss: 1.107 | Accuracy: 0.624573 | 1.155 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 62 | Batch: 000 / 011 | Total loss: 1.270 | Reg loss: 0.024 | Tree loss: 1.270 | Accuracy: 0.604000 | 1.155 sec/iter\n",
      "Epoch: 62 | Batch: 001 / 011 | Total loss: 1.237 | Reg loss: 0.024 | Tree loss: 1.237 | Accuracy: 0.625000 | 1.155 sec/iter\n",
      "Epoch: 62 | Batch: 002 / 011 | Total loss: 1.229 | Reg loss: 0.024 | Tree loss: 1.229 | Accuracy: 0.609000 | 1.154 sec/iter\n",
      "Epoch: 62 | Batch: 003 / 011 | Total loss: 1.218 | Reg loss: 0.024 | Tree loss: 1.218 | Accuracy: 0.588000 | 1.154 sec/iter\n",
      "Epoch: 62 | Batch: 004 / 011 | Total loss: 1.172 | Reg loss: 0.024 | Tree loss: 1.172 | Accuracy: 0.611000 | 1.154 sec/iter\n",
      "Epoch: 62 | Batch: 005 / 011 | Total loss: 1.150 | Reg loss: 0.024 | Tree loss: 1.150 | Accuracy: 0.597500 | 1.153 sec/iter\n",
      "Epoch: 62 | Batch: 006 / 011 | Total loss: 1.150 | Reg loss: 0.024 | Tree loss: 1.150 | Accuracy: 0.590000 | 1.153 sec/iter\n",
      "Epoch: 62 | Batch: 007 / 011 | Total loss: 1.132 | Reg loss: 0.024 | Tree loss: 1.132 | Accuracy: 0.606500 | 1.153 sec/iter\n",
      "Epoch: 62 | Batch: 008 / 011 | Total loss: 1.137 | Reg loss: 0.024 | Tree loss: 1.137 | Accuracy: 0.577000 | 1.153 sec/iter\n",
      "Epoch: 62 | Batch: 009 / 011 | Total loss: 1.142 | Reg loss: 0.024 | Tree loss: 1.142 | Accuracy: 0.580000 | 1.153 sec/iter\n",
      "Epoch: 62 | Batch: 010 / 011 | Total loss: 1.112 | Reg loss: 0.024 | Tree loss: 1.112 | Accuracy: 0.614334 | 1.152 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 63 | Batch: 000 / 011 | Total loss: 1.284 | Reg loss: 0.024 | Tree loss: 1.284 | Accuracy: 0.584000 | 1.153 sec/iter\n",
      "Epoch: 63 | Batch: 001 / 011 | Total loss: 1.240 | Reg loss: 0.024 | Tree loss: 1.240 | Accuracy: 0.620500 | 1.153 sec/iter\n",
      "Epoch: 63 | Batch: 002 / 011 | Total loss: 1.233 | Reg loss: 0.024 | Tree loss: 1.233 | Accuracy: 0.607000 | 1.153 sec/iter\n",
      "Epoch: 63 | Batch: 003 / 011 | Total loss: 1.180 | Reg loss: 0.024 | Tree loss: 1.180 | Accuracy: 0.613000 | 1.153 sec/iter\n",
      "Epoch: 63 | Batch: 004 / 011 | Total loss: 1.198 | Reg loss: 0.024 | Tree loss: 1.198 | Accuracy: 0.583000 | 1.153 sec/iter\n",
      "Epoch: 63 | Batch: 005 / 011 | Total loss: 1.147 | Reg loss: 0.024 | Tree loss: 1.147 | Accuracy: 0.578500 | 1.153 sec/iter\n",
      "Epoch: 63 | Batch: 006 / 011 | Total loss: 1.166 | Reg loss: 0.024 | Tree loss: 1.166 | Accuracy: 0.541500 | 1.152 sec/iter\n",
      "Epoch: 63 | Batch: 007 / 011 | Total loss: 1.108 | Reg loss: 0.024 | Tree loss: 1.108 | Accuracy: 0.576500 | 1.152 sec/iter\n",
      "Epoch: 63 | Batch: 008 / 011 | Total loss: 1.110 | Reg loss: 0.024 | Tree loss: 1.110 | Accuracy: 0.586000 | 1.152 sec/iter\n",
      "Epoch: 63 | Batch: 009 / 011 | Total loss: 1.129 | Reg loss: 0.024 | Tree loss: 1.129 | Accuracy: 0.572000 | 1.152 sec/iter\n",
      "Epoch: 63 | Batch: 010 / 011 | Total loss: 1.134 | Reg loss: 0.024 | Tree loss: 1.134 | Accuracy: 0.569966 | 1.151 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 64 | Batch: 000 / 011 | Total loss: 1.262 | Reg loss: 0.024 | Tree loss: 1.262 | Accuracy: 0.598000 | 1.154 sec/iter\n",
      "Epoch: 64 | Batch: 001 / 011 | Total loss: 1.240 | Reg loss: 0.024 | Tree loss: 1.240 | Accuracy: 0.625500 | 1.154 sec/iter\n",
      "Epoch: 64 | Batch: 002 / 011 | Total loss: 1.229 | Reg loss: 0.024 | Tree loss: 1.229 | Accuracy: 0.605000 | 1.153 sec/iter\n",
      "Epoch: 64 | Batch: 003 / 011 | Total loss: 1.178 | Reg loss: 0.024 | Tree loss: 1.178 | Accuracy: 0.610000 | 1.153 sec/iter\n",
      "Epoch: 64 | Batch: 004 / 011 | Total loss: 1.160 | Reg loss: 0.024 | Tree loss: 1.160 | Accuracy: 0.609000 | 1.153 sec/iter\n",
      "Epoch: 64 | Batch: 005 / 011 | Total loss: 1.132 | Reg loss: 0.024 | Tree loss: 1.132 | Accuracy: 0.590500 | 1.153 sec/iter\n",
      "Epoch: 64 | Batch: 006 / 011 | Total loss: 1.135 | Reg loss: 0.024 | Tree loss: 1.135 | Accuracy: 0.580500 | 1.153 sec/iter\n",
      "Epoch: 64 | Batch: 007 / 011 | Total loss: 1.152 | Reg loss: 0.024 | Tree loss: 1.152 | Accuracy: 0.564500 | 1.152 sec/iter\n",
      "Epoch: 64 | Batch: 008 / 011 | Total loss: 1.121 | Reg loss: 0.024 | Tree loss: 1.121 | Accuracy: 0.581000 | 1.152 sec/iter\n",
      "Epoch: 64 | Batch: 009 / 011 | Total loss: 1.145 | Reg loss: 0.024 | Tree loss: 1.145 | Accuracy: 0.562500 | 1.152 sec/iter\n",
      "Epoch: 64 | Batch: 010 / 011 | Total loss: 1.165 | Reg loss: 0.025 | Tree loss: 1.165 | Accuracy: 0.559727 | 1.152 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 65 | Batch: 000 / 011 | Total loss: 1.277 | Reg loss: 0.024 | Tree loss: 1.277 | Accuracy: 0.599500 | 1.155 sec/iter\n",
      "Epoch: 65 | Batch: 001 / 011 | Total loss: 1.252 | Reg loss: 0.024 | Tree loss: 1.252 | Accuracy: 0.624000 | 1.154 sec/iter\n",
      "Epoch: 65 | Batch: 002 / 011 | Total loss: 1.210 | Reg loss: 0.024 | Tree loss: 1.210 | Accuracy: 0.616000 | 1.154 sec/iter\n",
      "Epoch: 65 | Batch: 003 / 011 | Total loss: 1.201 | Reg loss: 0.024 | Tree loss: 1.201 | Accuracy: 0.593500 | 1.154 sec/iter\n",
      "Epoch: 65 | Batch: 004 / 011 | Total loss: 1.164 | Reg loss: 0.024 | Tree loss: 1.164 | Accuracy: 0.614500 | 1.154 sec/iter\n",
      "Epoch: 65 | Batch: 005 / 011 | Total loss: 1.145 | Reg loss: 0.024 | Tree loss: 1.145 | Accuracy: 0.587500 | 1.154 sec/iter\n",
      "Epoch: 65 | Batch: 006 / 011 | Total loss: 1.138 | Reg loss: 0.024 | Tree loss: 1.138 | Accuracy: 0.568500 | 1.154 sec/iter\n",
      "Epoch: 65 | Batch: 007 / 011 | Total loss: 1.135 | Reg loss: 0.024 | Tree loss: 1.135 | Accuracy: 0.576000 | 1.153 sec/iter\n",
      "Epoch: 65 | Batch: 008 / 011 | Total loss: 1.119 | Reg loss: 0.024 | Tree loss: 1.119 | Accuracy: 0.580000 | 1.153 sec/iter\n",
      "Epoch: 65 | Batch: 009 / 011 | Total loss: 1.112 | Reg loss: 0.025 | Tree loss: 1.112 | Accuracy: 0.580500 | 1.153 sec/iter\n",
      "Epoch: 65 | Batch: 010 / 011 | Total loss: 1.106 | Reg loss: 0.025 | Tree loss: 1.106 | Accuracy: 0.600683 | 1.153 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 66 | Batch: 000 / 011 | Total loss: 1.246 | Reg loss: 0.024 | Tree loss: 1.246 | Accuracy: 0.614000 | 1.156 sec/iter\n",
      "Epoch: 66 | Batch: 001 / 011 | Total loss: 1.249 | Reg loss: 0.024 | Tree loss: 1.249 | Accuracy: 0.629500 | 1.156 sec/iter\n",
      "Epoch: 66 | Batch: 002 / 011 | Total loss: 1.182 | Reg loss: 0.024 | Tree loss: 1.182 | Accuracy: 0.634500 | 1.155 sec/iter\n",
      "Epoch: 66 | Batch: 003 / 011 | Total loss: 1.195 | Reg loss: 0.024 | Tree loss: 1.195 | Accuracy: 0.599000 | 1.155 sec/iter\n",
      "Epoch: 66 | Batch: 004 / 011 | Total loss: 1.174 | Reg loss: 0.024 | Tree loss: 1.174 | Accuracy: 0.598500 | 1.155 sec/iter\n",
      "Epoch: 66 | Batch: 005 / 011 | Total loss: 1.127 | Reg loss: 0.024 | Tree loss: 1.127 | Accuracy: 0.607000 | 1.155 sec/iter\n",
      "Epoch: 66 | Batch: 006 / 011 | Total loss: 1.137 | Reg loss: 0.024 | Tree loss: 1.137 | Accuracy: 0.581500 | 1.155 sec/iter\n",
      "Epoch: 66 | Batch: 007 / 011 | Total loss: 1.131 | Reg loss: 0.024 | Tree loss: 1.131 | Accuracy: 0.576000 | 1.154 sec/iter\n",
      "Epoch: 66 | Batch: 008 / 011 | Total loss: 1.113 | Reg loss: 0.025 | Tree loss: 1.113 | Accuracy: 0.570000 | 1.154 sec/iter\n",
      "Epoch: 66 | Batch: 009 / 011 | Total loss: 1.130 | Reg loss: 0.025 | Tree loss: 1.130 | Accuracy: 0.552000 | 1.154 sec/iter\n",
      "Epoch: 66 | Batch: 010 / 011 | Total loss: 1.062 | Reg loss: 0.025 | Tree loss: 1.062 | Accuracy: 0.624573 | 1.153 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 67 | Batch: 000 / 011 | Total loss: 1.266 | Reg loss: 0.024 | Tree loss: 1.266 | Accuracy: 0.611000 | 1.157 sec/iter\n",
      "Epoch: 67 | Batch: 001 / 011 | Total loss: 1.224 | Reg loss: 0.024 | Tree loss: 1.224 | Accuracy: 0.641500 | 1.157 sec/iter\n",
      "Epoch: 67 | Batch: 002 / 011 | Total loss: 1.198 | Reg loss: 0.024 | Tree loss: 1.198 | Accuracy: 0.610500 | 1.156 sec/iter\n",
      "Epoch: 67 | Batch: 003 / 011 | Total loss: 1.174 | Reg loss: 0.024 | Tree loss: 1.174 | Accuracy: 0.599500 | 1.156 sec/iter\n",
      "Epoch: 67 | Batch: 004 / 011 | Total loss: 1.167 | Reg loss: 0.024 | Tree loss: 1.167 | Accuracy: 0.571500 | 1.156 sec/iter\n",
      "Epoch: 67 | Batch: 005 / 011 | Total loss: 1.142 | Reg loss: 0.024 | Tree loss: 1.142 | Accuracy: 0.558000 | 1.156 sec/iter\n",
      "Epoch: 67 | Batch: 006 / 011 | Total loss: 1.114 | Reg loss: 0.025 | Tree loss: 1.114 | Accuracy: 0.550500 | 1.156 sec/iter\n",
      "Epoch: 67 | Batch: 007 / 011 | Total loss: 1.125 | Reg loss: 0.025 | Tree loss: 1.125 | Accuracy: 0.561500 | 1.156 sec/iter\n",
      "Epoch: 67 | Batch: 008 / 011 | Total loss: 1.126 | Reg loss: 0.025 | Tree loss: 1.126 | Accuracy: 0.559500 | 1.155 sec/iter\n",
      "Epoch: 67 | Batch: 009 / 011 | Total loss: 1.128 | Reg loss: 0.025 | Tree loss: 1.128 | Accuracy: 0.554500 | 1.155 sec/iter\n",
      "Epoch: 67 | Batch: 010 / 011 | Total loss: 1.084 | Reg loss: 0.025 | Tree loss: 1.084 | Accuracy: 0.600683 | 1.155 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 68 | Batch: 000 / 011 | Total loss: 1.235 | Reg loss: 0.024 | Tree loss: 1.235 | Accuracy: 0.632000 | 1.155 sec/iter\n",
      "Epoch: 68 | Batch: 001 / 011 | Total loss: 1.234 | Reg loss: 0.024 | Tree loss: 1.234 | Accuracy: 0.631500 | 1.154 sec/iter\n",
      "Epoch: 68 | Batch: 002 / 011 | Total loss: 1.209 | Reg loss: 0.024 | Tree loss: 1.209 | Accuracy: 0.626000 | 1.154 sec/iter\n",
      "Epoch: 68 | Batch: 003 / 011 | Total loss: 1.185 | Reg loss: 0.024 | Tree loss: 1.185 | Accuracy: 0.606000 | 1.153 sec/iter\n",
      "Epoch: 68 | Batch: 004 / 011 | Total loss: 1.158 | Reg loss: 0.024 | Tree loss: 1.158 | Accuracy: 0.588000 | 1.153 sec/iter\n",
      "Epoch: 68 | Batch: 005 / 011 | Total loss: 1.131 | Reg loss: 0.025 | Tree loss: 1.131 | Accuracy: 0.572500 | 1.153 sec/iter\n",
      "Epoch: 68 | Batch: 006 / 011 | Total loss: 1.127 | Reg loss: 0.025 | Tree loss: 1.127 | Accuracy: 0.555500 | 1.153 sec/iter\n",
      "Epoch: 68 | Batch: 007 / 011 | Total loss: 1.130 | Reg loss: 0.025 | Tree loss: 1.130 | Accuracy: 0.576000 | 1.152 sec/iter\n",
      "Epoch: 68 | Batch: 008 / 011 | Total loss: 1.121 | Reg loss: 0.025 | Tree loss: 1.121 | Accuracy: 0.557000 | 1.152 sec/iter\n",
      "Epoch: 68 | Batch: 009 / 011 | Total loss: 1.090 | Reg loss: 0.025 | Tree loss: 1.090 | Accuracy: 0.595000 | 1.152 sec/iter\n",
      "Epoch: 68 | Batch: 010 / 011 | Total loss: 1.089 | Reg loss: 0.025 | Tree loss: 1.089 | Accuracy: 0.597270 | 1.152 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 69 | Batch: 000 / 011 | Total loss: 1.239 | Reg loss: 0.024 | Tree loss: 1.239 | Accuracy: 0.629000 | 1.153 sec/iter\n",
      "Epoch: 69 | Batch: 001 / 011 | Total loss: 1.226 | Reg loss: 0.024 | Tree loss: 1.226 | Accuracy: 0.625000 | 1.153 sec/iter\n",
      "Epoch: 69 | Batch: 002 / 011 | Total loss: 1.218 | Reg loss: 0.024 | Tree loss: 1.218 | Accuracy: 0.598000 | 1.153 sec/iter\n",
      "Epoch: 69 | Batch: 003 / 011 | Total loss: 1.171 | Reg loss: 0.024 | Tree loss: 1.171 | Accuracy: 0.607500 | 1.152 sec/iter\n",
      "Epoch: 69 | Batch: 004 / 011 | Total loss: 1.156 | Reg loss: 0.025 | Tree loss: 1.156 | Accuracy: 0.583500 | 1.152 sec/iter\n",
      "Epoch: 69 | Batch: 005 / 011 | Total loss: 1.133 | Reg loss: 0.025 | Tree loss: 1.133 | Accuracy: 0.577000 | 1.152 sec/iter\n",
      "Epoch: 69 | Batch: 006 / 011 | Total loss: 1.119 | Reg loss: 0.025 | Tree loss: 1.119 | Accuracy: 0.568500 | 1.152 sec/iter\n",
      "Epoch: 69 | Batch: 007 / 011 | Total loss: 1.100 | Reg loss: 0.025 | Tree loss: 1.100 | Accuracy: 0.564500 | 1.152 sec/iter\n",
      "Epoch: 69 | Batch: 008 / 011 | Total loss: 1.132 | Reg loss: 0.025 | Tree loss: 1.132 | Accuracy: 0.557500 | 1.151 sec/iter\n",
      "Epoch: 69 | Batch: 009 / 011 | Total loss: 1.101 | Reg loss: 0.025 | Tree loss: 1.101 | Accuracy: 0.567500 | 1.151 sec/iter\n",
      "Epoch: 69 | Batch: 010 / 011 | Total loss: 1.084 | Reg loss: 0.025 | Tree loss: 1.084 | Accuracy: 0.593857 | 1.151 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 70 | Batch: 000 / 011 | Total loss: 1.242 | Reg loss: 0.024 | Tree loss: 1.242 | Accuracy: 0.621500 | 1.154 sec/iter\n",
      "Epoch: 70 | Batch: 001 / 011 | Total loss: 1.244 | Reg loss: 0.024 | Tree loss: 1.244 | Accuracy: 0.613500 | 1.153 sec/iter\n",
      "Epoch: 70 | Batch: 002 / 011 | Total loss: 1.208 | Reg loss: 0.024 | Tree loss: 1.208 | Accuracy: 0.611500 | 1.153 sec/iter\n",
      "Epoch: 70 | Batch: 003 / 011 | Total loss: 1.159 | Reg loss: 0.025 | Tree loss: 1.159 | Accuracy: 0.611000 | 1.153 sec/iter\n",
      "Epoch: 70 | Batch: 004 / 011 | Total loss: 1.164 | Reg loss: 0.025 | Tree loss: 1.164 | Accuracy: 0.572000 | 1.153 sec/iter\n",
      "Epoch: 70 | Batch: 005 / 011 | Total loss: 1.112 | Reg loss: 0.025 | Tree loss: 1.112 | Accuracy: 0.582500 | 1.153 sec/iter\n",
      "Epoch: 70 | Batch: 006 / 011 | Total loss: 1.130 | Reg loss: 0.025 | Tree loss: 1.130 | Accuracy: 0.565500 | 1.152 sec/iter\n",
      "Epoch: 70 | Batch: 007 / 011 | Total loss: 1.086 | Reg loss: 0.025 | Tree loss: 1.086 | Accuracy: 0.585000 | 1.152 sec/iter\n",
      "Epoch: 70 | Batch: 008 / 011 | Total loss: 1.107 | Reg loss: 0.025 | Tree loss: 1.107 | Accuracy: 0.560500 | 1.152 sec/iter\n",
      "Epoch: 70 | Batch: 009 / 011 | Total loss: 1.100 | Reg loss: 0.025 | Tree loss: 1.100 | Accuracy: 0.582500 | 1.152 sec/iter\n",
      "Epoch: 70 | Batch: 010 / 011 | Total loss: 1.144 | Reg loss: 0.025 | Tree loss: 1.144 | Accuracy: 0.563140 | 1.152 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 71 | Batch: 000 / 011 | Total loss: 1.235 | Reg loss: 0.025 | Tree loss: 1.235 | Accuracy: 0.624500 | 1.154 sec/iter\n",
      "Epoch: 71 | Batch: 001 / 011 | Total loss: 1.232 | Reg loss: 0.025 | Tree loss: 1.232 | Accuracy: 0.619500 | 1.154 sec/iter\n",
      "Epoch: 71 | Batch: 002 / 011 | Total loss: 1.189 | Reg loss: 0.025 | Tree loss: 1.189 | Accuracy: 0.633500 | 1.154 sec/iter\n",
      "Epoch: 71 | Batch: 003 / 011 | Total loss: 1.178 | Reg loss: 0.025 | Tree loss: 1.178 | Accuracy: 0.602500 | 1.154 sec/iter\n",
      "Epoch: 71 | Batch: 004 / 011 | Total loss: 1.143 | Reg loss: 0.025 | Tree loss: 1.143 | Accuracy: 0.605500 | 1.154 sec/iter\n",
      "Epoch: 71 | Batch: 005 / 011 | Total loss: 1.124 | Reg loss: 0.025 | Tree loss: 1.124 | Accuracy: 0.585000 | 1.153 sec/iter\n",
      "Epoch: 71 | Batch: 006 / 011 | Total loss: 1.122 | Reg loss: 0.025 | Tree loss: 1.122 | Accuracy: 0.581000 | 1.153 sec/iter\n",
      "Epoch: 71 | Batch: 007 / 011 | Total loss: 1.113 | Reg loss: 0.025 | Tree loss: 1.113 | Accuracy: 0.560000 | 1.153 sec/iter\n",
      "Epoch: 71 | Batch: 008 / 011 | Total loss: 1.097 | Reg loss: 0.025 | Tree loss: 1.097 | Accuracy: 0.577500 | 1.153 sec/iter\n",
      "Epoch: 71 | Batch: 009 / 011 | Total loss: 1.113 | Reg loss: 0.025 | Tree loss: 1.113 | Accuracy: 0.555000 | 1.152 sec/iter\n",
      "Epoch: 71 | Batch: 010 / 011 | Total loss: 1.116 | Reg loss: 0.025 | Tree loss: 1.116 | Accuracy: 0.505119 | 1.152 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 72 | Batch: 000 / 011 | Total loss: 1.246 | Reg loss: 0.025 | Tree loss: 1.246 | Accuracy: 0.616500 | 1.155 sec/iter\n",
      "Epoch: 72 | Batch: 001 / 011 | Total loss: 1.237 | Reg loss: 0.025 | Tree loss: 1.237 | Accuracy: 0.620500 | 1.155 sec/iter\n",
      "Epoch: 72 | Batch: 002 / 011 | Total loss: 1.175 | Reg loss: 0.025 | Tree loss: 1.175 | Accuracy: 0.632000 | 1.155 sec/iter\n",
      "Epoch: 72 | Batch: 003 / 011 | Total loss: 1.160 | Reg loss: 0.025 | Tree loss: 1.160 | Accuracy: 0.603500 | 1.155 sec/iter\n",
      "Epoch: 72 | Batch: 004 / 011 | Total loss: 1.139 | Reg loss: 0.025 | Tree loss: 1.139 | Accuracy: 0.580500 | 1.154 sec/iter\n",
      "Epoch: 72 | Batch: 005 / 011 | Total loss: 1.133 | Reg loss: 0.025 | Tree loss: 1.133 | Accuracy: 0.567000 | 1.154 sec/iter\n",
      "Epoch: 72 | Batch: 006 / 011 | Total loss: 1.133 | Reg loss: 0.025 | Tree loss: 1.133 | Accuracy: 0.546500 | 1.154 sec/iter\n",
      "Epoch: 72 | Batch: 007 / 011 | Total loss: 1.094 | Reg loss: 0.025 | Tree loss: 1.094 | Accuracy: 0.563000 | 1.154 sec/iter\n",
      "Epoch: 72 | Batch: 008 / 011 | Total loss: 1.104 | Reg loss: 0.025 | Tree loss: 1.104 | Accuracy: 0.569500 | 1.154 sec/iter\n",
      "Epoch: 72 | Batch: 009 / 011 | Total loss: 1.098 | Reg loss: 0.025 | Tree loss: 1.098 | Accuracy: 0.573500 | 1.153 sec/iter\n",
      "Epoch: 72 | Batch: 010 / 011 | Total loss: 1.058 | Reg loss: 0.025 | Tree loss: 1.058 | Accuracy: 0.576792 | 1.153 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 73 | Batch: 000 / 011 | Total loss: 1.233 | Reg loss: 0.025 | Tree loss: 1.233 | Accuracy: 0.630500 | 1.156 sec/iter\n",
      "Epoch: 73 | Batch: 001 / 011 | Total loss: 1.227 | Reg loss: 0.025 | Tree loss: 1.227 | Accuracy: 0.632000 | 1.156 sec/iter\n",
      "Epoch: 73 | Batch: 002 / 011 | Total loss: 1.173 | Reg loss: 0.025 | Tree loss: 1.173 | Accuracy: 0.633500 | 1.156 sec/iter\n",
      "Epoch: 73 | Batch: 003 / 011 | Total loss: 1.167 | Reg loss: 0.025 | Tree loss: 1.167 | Accuracy: 0.602000 | 1.156 sec/iter\n",
      "Epoch: 73 | Batch: 004 / 011 | Total loss: 1.138 | Reg loss: 0.025 | Tree loss: 1.138 | Accuracy: 0.604500 | 1.155 sec/iter\n",
      "Epoch: 73 | Batch: 005 / 011 | Total loss: 1.130 | Reg loss: 0.025 | Tree loss: 1.130 | Accuracy: 0.599500 | 1.155 sec/iter\n",
      "Epoch: 73 | Batch: 006 / 011 | Total loss: 1.090 | Reg loss: 0.025 | Tree loss: 1.090 | Accuracy: 0.599000 | 1.155 sec/iter\n",
      "Epoch: 73 | Batch: 007 / 011 | Total loss: 1.109 | Reg loss: 0.025 | Tree loss: 1.109 | Accuracy: 0.569000 | 1.155 sec/iter\n",
      "Epoch: 73 | Batch: 008 / 011 | Total loss: 1.103 | Reg loss: 0.025 | Tree loss: 1.103 | Accuracy: 0.565000 | 1.154 sec/iter\n",
      "Epoch: 73 | Batch: 009 / 011 | Total loss: 1.110 | Reg loss: 0.025 | Tree loss: 1.110 | Accuracy: 0.562500 | 1.154 sec/iter\n",
      "Epoch: 73 | Batch: 010 / 011 | Total loss: 1.106 | Reg loss: 0.025 | Tree loss: 1.106 | Accuracy: 0.549488 | 1.154 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 74 | Batch: 000 / 011 | Total loss: 1.233 | Reg loss: 0.025 | Tree loss: 1.233 | Accuracy: 0.624500 | 1.154 sec/iter\n",
      "Epoch: 74 | Batch: 001 / 011 | Total loss: 1.210 | Reg loss: 0.025 | Tree loss: 1.210 | Accuracy: 0.642000 | 1.154 sec/iter\n",
      "Epoch: 74 | Batch: 002 / 011 | Total loss: 1.196 | Reg loss: 0.025 | Tree loss: 1.196 | Accuracy: 0.622000 | 1.154 sec/iter\n",
      "Epoch: 74 | Batch: 003 / 011 | Total loss: 1.165 | Reg loss: 0.025 | Tree loss: 1.165 | Accuracy: 0.612000 | 1.153 sec/iter\n",
      "Epoch: 74 | Batch: 004 / 011 | Total loss: 1.138 | Reg loss: 0.025 | Tree loss: 1.138 | Accuracy: 0.603000 | 1.153 sec/iter\n",
      "Epoch: 74 | Batch: 005 / 011 | Total loss: 1.119 | Reg loss: 0.025 | Tree loss: 1.119 | Accuracy: 0.579000 | 1.153 sec/iter\n",
      "Epoch: 74 | Batch: 006 / 011 | Total loss: 1.097 | Reg loss: 0.025 | Tree loss: 1.097 | Accuracy: 0.582000 | 1.153 sec/iter\n",
      "Epoch: 74 | Batch: 007 / 011 | Total loss: 1.102 | Reg loss: 0.025 | Tree loss: 1.102 | Accuracy: 0.563500 | 1.153 sec/iter\n",
      "Epoch: 74 | Batch: 008 / 011 | Total loss: 1.099 | Reg loss: 0.025 | Tree loss: 1.099 | Accuracy: 0.567000 | 1.153 sec/iter\n",
      "Epoch: 74 | Batch: 009 / 011 | Total loss: 1.087 | Reg loss: 0.025 | Tree loss: 1.087 | Accuracy: 0.575000 | 1.152 sec/iter\n",
      "Epoch: 74 | Batch: 010 / 011 | Total loss: 1.134 | Reg loss: 0.025 | Tree loss: 1.134 | Accuracy: 0.556314 | 1.152 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 75 | Batch: 000 / 011 | Total loss: 1.228 | Reg loss: 0.025 | Tree loss: 1.228 | Accuracy: 0.643500 | 1.153 sec/iter\n",
      "Epoch: 75 | Batch: 001 / 011 | Total loss: 1.220 | Reg loss: 0.025 | Tree loss: 1.220 | Accuracy: 0.624000 | 1.153 sec/iter\n",
      "Epoch: 75 | Batch: 002 / 011 | Total loss: 1.162 | Reg loss: 0.025 | Tree loss: 1.162 | Accuracy: 0.638000 | 1.153 sec/iter\n",
      "Epoch: 75 | Batch: 003 / 011 | Total loss: 1.142 | Reg loss: 0.025 | Tree loss: 1.142 | Accuracy: 0.625000 | 1.153 sec/iter\n",
      "Epoch: 75 | Batch: 004 / 011 | Total loss: 1.162 | Reg loss: 0.025 | Tree loss: 1.162 | Accuracy: 0.599500 | 1.153 sec/iter\n",
      "Epoch: 75 | Batch: 005 / 011 | Total loss: 1.128 | Reg loss: 0.025 | Tree loss: 1.128 | Accuracy: 0.565500 | 1.152 sec/iter\n",
      "Epoch: 75 | Batch: 006 / 011 | Total loss: 1.122 | Reg loss: 0.025 | Tree loss: 1.122 | Accuracy: 0.555500 | 1.152 sec/iter\n",
      "Epoch: 75 | Batch: 007 / 011 | Total loss: 1.079 | Reg loss: 0.025 | Tree loss: 1.079 | Accuracy: 0.577500 | 1.152 sec/iter\n",
      "Epoch: 75 | Batch: 008 / 011 | Total loss: 1.095 | Reg loss: 0.025 | Tree loss: 1.095 | Accuracy: 0.556500 | 1.152 sec/iter\n",
      "Epoch: 75 | Batch: 009 / 011 | Total loss: 1.084 | Reg loss: 0.025 | Tree loss: 1.084 | Accuracy: 0.584500 | 1.152 sec/iter\n",
      "Epoch: 75 | Batch: 010 / 011 | Total loss: 1.094 | Reg loss: 0.025 | Tree loss: 1.094 | Accuracy: 0.535836 | 1.151 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 76 | Batch: 000 / 011 | Total loss: 1.231 | Reg loss: 0.025 | Tree loss: 1.231 | Accuracy: 0.631500 | 1.153 sec/iter\n",
      "Epoch: 76 | Batch: 001 / 011 | Total loss: 1.207 | Reg loss: 0.025 | Tree loss: 1.207 | Accuracy: 0.637000 | 1.153 sec/iter\n",
      "Epoch: 76 | Batch: 002 / 011 | Total loss: 1.185 | Reg loss: 0.025 | Tree loss: 1.185 | Accuracy: 0.616500 | 1.153 sec/iter\n",
      "Epoch: 76 | Batch: 003 / 011 | Total loss: 1.137 | Reg loss: 0.025 | Tree loss: 1.137 | Accuracy: 0.637000 | 1.153 sec/iter\n",
      "Epoch: 76 | Batch: 004 / 011 | Total loss: 1.152 | Reg loss: 0.025 | Tree loss: 1.152 | Accuracy: 0.593500 | 1.153 sec/iter\n",
      "Epoch: 76 | Batch: 005 / 011 | Total loss: 1.107 | Reg loss: 0.025 | Tree loss: 1.107 | Accuracy: 0.600500 | 1.153 sec/iter\n",
      "Epoch: 76 | Batch: 006 / 011 | Total loss: 1.127 | Reg loss: 0.025 | Tree loss: 1.127 | Accuracy: 0.588000 | 1.152 sec/iter\n",
      "Epoch: 76 | Batch: 007 / 011 | Total loss: 1.085 | Reg loss: 0.025 | Tree loss: 1.085 | Accuracy: 0.612000 | 1.152 sec/iter\n",
      "Epoch: 76 | Batch: 008 / 011 | Total loss: 1.113 | Reg loss: 0.025 | Tree loss: 1.113 | Accuracy: 0.584500 | 1.152 sec/iter\n",
      "Epoch: 76 | Batch: 009 / 011 | Total loss: 1.077 | Reg loss: 0.025 | Tree loss: 1.077 | Accuracy: 0.583500 | 1.152 sec/iter\n",
      "Epoch: 76 | Batch: 010 / 011 | Total loss: 1.052 | Reg loss: 0.025 | Tree loss: 1.052 | Accuracy: 0.563140 | 1.151 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 77 | Batch: 000 / 011 | Total loss: 1.232 | Reg loss: 0.025 | Tree loss: 1.232 | Accuracy: 0.619000 | 1.154 sec/iter\n",
      "Epoch: 77 | Batch: 001 / 011 | Total loss: 1.227 | Reg loss: 0.025 | Tree loss: 1.227 | Accuracy: 0.620000 | 1.154 sec/iter\n",
      "Epoch: 77 | Batch: 002 / 011 | Total loss: 1.167 | Reg loss: 0.025 | Tree loss: 1.167 | Accuracy: 0.630500 | 1.154 sec/iter\n",
      "Epoch: 77 | Batch: 003 / 011 | Total loss: 1.160 | Reg loss: 0.025 | Tree loss: 1.160 | Accuracy: 0.608000 | 1.154 sec/iter\n",
      "Epoch: 77 | Batch: 004 / 011 | Total loss: 1.124 | Reg loss: 0.025 | Tree loss: 1.124 | Accuracy: 0.598500 | 1.153 sec/iter\n",
      "Epoch: 77 | Batch: 005 / 011 | Total loss: 1.142 | Reg loss: 0.025 | Tree loss: 1.142 | Accuracy: 0.556000 | 1.153 sec/iter\n",
      "Epoch: 77 | Batch: 006 / 011 | Total loss: 1.096 | Reg loss: 0.025 | Tree loss: 1.096 | Accuracy: 0.568000 | 1.153 sec/iter\n",
      "Epoch: 77 | Batch: 007 / 011 | Total loss: 1.105 | Reg loss: 0.025 | Tree loss: 1.105 | Accuracy: 0.544500 | 1.153 sec/iter\n",
      "Epoch: 77 | Batch: 008 / 011 | Total loss: 1.065 | Reg loss: 0.025 | Tree loss: 1.065 | Accuracy: 0.573000 | 1.153 sec/iter\n",
      "Epoch: 77 | Batch: 009 / 011 | Total loss: 1.087 | Reg loss: 0.025 | Tree loss: 1.087 | Accuracy: 0.576000 | 1.153 sec/iter\n",
      "Epoch: 77 | Batch: 010 / 011 | Total loss: 1.103 | Reg loss: 0.025 | Tree loss: 1.103 | Accuracy: 0.559727 | 1.152 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 78 | Batch: 000 / 011 | Total loss: 1.218 | Reg loss: 0.025 | Tree loss: 1.218 | Accuracy: 0.631000 | 1.155 sec/iter\n",
      "Epoch: 78 | Batch: 001 / 011 | Total loss: 1.194 | Reg loss: 0.025 | Tree loss: 1.194 | Accuracy: 0.653500 | 1.155 sec/iter\n",
      "Epoch: 78 | Batch: 002 / 011 | Total loss: 1.176 | Reg loss: 0.025 | Tree loss: 1.176 | Accuracy: 0.626000 | 1.154 sec/iter\n",
      "Epoch: 78 | Batch: 003 / 011 | Total loss: 1.157 | Reg loss: 0.025 | Tree loss: 1.157 | Accuracy: 0.609500 | 1.154 sec/iter\n",
      "Epoch: 78 | Batch: 004 / 011 | Total loss: 1.124 | Reg loss: 0.025 | Tree loss: 1.124 | Accuracy: 0.618500 | 1.154 sec/iter\n",
      "Epoch: 78 | Batch: 005 / 011 | Total loss: 1.105 | Reg loss: 0.025 | Tree loss: 1.105 | Accuracy: 0.604000 | 1.154 sec/iter\n",
      "Epoch: 78 | Batch: 006 / 011 | Total loss: 1.100 | Reg loss: 0.025 | Tree loss: 1.100 | Accuracy: 0.584500 | 1.154 sec/iter\n",
      "Epoch: 78 | Batch: 007 / 011 | Total loss: 1.088 | Reg loss: 0.025 | Tree loss: 1.088 | Accuracy: 0.576000 | 1.154 sec/iter\n",
      "Epoch: 78 | Batch: 008 / 011 | Total loss: 1.099 | Reg loss: 0.025 | Tree loss: 1.099 | Accuracy: 0.576000 | 1.153 sec/iter\n",
      "Epoch: 78 | Batch: 009 / 011 | Total loss: 1.094 | Reg loss: 0.025 | Tree loss: 1.094 | Accuracy: 0.566000 | 1.153 sec/iter\n",
      "Epoch: 78 | Batch: 010 / 011 | Total loss: 1.096 | Reg loss: 0.025 | Tree loss: 1.096 | Accuracy: 0.607509 | 1.153 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 79 | Batch: 000 / 011 | Total loss: 1.230 | Reg loss: 0.025 | Tree loss: 1.230 | Accuracy: 0.630000 | 1.156 sec/iter\n",
      "Epoch: 79 | Batch: 001 / 011 | Total loss: 1.206 | Reg loss: 0.025 | Tree loss: 1.206 | Accuracy: 0.636000 | 1.156 sec/iter\n",
      "Epoch: 79 | Batch: 002 / 011 | Total loss: 1.179 | Reg loss: 0.025 | Tree loss: 1.179 | Accuracy: 0.626500 | 1.155 sec/iter\n",
      "Epoch: 79 | Batch: 003 / 011 | Total loss: 1.141 | Reg loss: 0.025 | Tree loss: 1.141 | Accuracy: 0.628500 | 1.155 sec/iter\n",
      "Epoch: 79 | Batch: 004 / 011 | Total loss: 1.118 | Reg loss: 0.025 | Tree loss: 1.118 | Accuracy: 0.612500 | 1.155 sec/iter\n",
      "Epoch: 79 | Batch: 005 / 011 | Total loss: 1.099 | Reg loss: 0.025 | Tree loss: 1.099 | Accuracy: 0.586500 | 1.155 sec/iter\n",
      "Epoch: 79 | Batch: 006 / 011 | Total loss: 1.098 | Reg loss: 0.025 | Tree loss: 1.098 | Accuracy: 0.579500 | 1.155 sec/iter\n",
      "Epoch: 79 | Batch: 007 / 011 | Total loss: 1.088 | Reg loss: 0.025 | Tree loss: 1.088 | Accuracy: 0.577500 | 1.155 sec/iter\n",
      "Epoch: 79 | Batch: 008 / 011 | Total loss: 1.083 | Reg loss: 0.025 | Tree loss: 1.083 | Accuracy: 0.580000 | 1.154 sec/iter\n",
      "Epoch: 79 | Batch: 009 / 011 | Total loss: 1.092 | Reg loss: 0.025 | Tree loss: 1.092 | Accuracy: 0.556000 | 1.154 sec/iter\n",
      "Epoch: 79 | Batch: 010 / 011 | Total loss: 1.084 | Reg loss: 0.025 | Tree loss: 1.084 | Accuracy: 0.607509 | 1.154 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 80 | Batch: 000 / 011 | Total loss: 1.215 | Reg loss: 0.025 | Tree loss: 1.215 | Accuracy: 0.627000 | 1.154 sec/iter\n",
      "Epoch: 80 | Batch: 001 / 011 | Total loss: 1.199 | Reg loss: 0.025 | Tree loss: 1.199 | Accuracy: 0.636500 | 1.154 sec/iter\n",
      "Epoch: 80 | Batch: 002 / 011 | Total loss: 1.153 | Reg loss: 0.025 | Tree loss: 1.153 | Accuracy: 0.660500 | 1.153 sec/iter\n",
      "Epoch: 80 | Batch: 003 / 011 | Total loss: 1.149 | Reg loss: 0.025 | Tree loss: 1.149 | Accuracy: 0.610500 | 1.153 sec/iter\n",
      "Epoch: 80 | Batch: 004 / 011 | Total loss: 1.111 | Reg loss: 0.025 | Tree loss: 1.111 | Accuracy: 0.622500 | 1.153 sec/iter\n",
      "Epoch: 80 | Batch: 005 / 011 | Total loss: 1.113 | Reg loss: 0.025 | Tree loss: 1.113 | Accuracy: 0.571000 | 1.152 sec/iter\n",
      "Epoch: 80 | Batch: 006 / 011 | Total loss: 1.094 | Reg loss: 0.025 | Tree loss: 1.094 | Accuracy: 0.556000 | 1.152 sec/iter\n",
      "Epoch: 80 | Batch: 007 / 011 | Total loss: 1.104 | Reg loss: 0.025 | Tree loss: 1.104 | Accuracy: 0.547000 | 1.152 sec/iter\n",
      "Epoch: 80 | Batch: 008 / 011 | Total loss: 1.082 | Reg loss: 0.025 | Tree loss: 1.082 | Accuracy: 0.559000 | 1.152 sec/iter\n",
      "Epoch: 80 | Batch: 009 / 011 | Total loss: 1.077 | Reg loss: 0.025 | Tree loss: 1.077 | Accuracy: 0.581500 | 1.152 sec/iter\n",
      "Epoch: 80 | Batch: 010 / 011 | Total loss: 1.051 | Reg loss: 0.025 | Tree loss: 1.051 | Accuracy: 0.566553 | 1.152 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 81 | Batch: 000 / 011 | Total loss: 1.221 | Reg loss: 0.025 | Tree loss: 1.221 | Accuracy: 0.643000 | 1.152 sec/iter\n",
      "Epoch: 81 | Batch: 001 / 011 | Total loss: 1.191 | Reg loss: 0.025 | Tree loss: 1.191 | Accuracy: 0.645500 | 1.152 sec/iter\n",
      "Epoch: 81 | Batch: 002 / 011 | Total loss: 1.168 | Reg loss: 0.025 | Tree loss: 1.168 | Accuracy: 0.622000 | 1.152 sec/iter\n",
      "Epoch: 81 | Batch: 003 / 011 | Total loss: 1.140 | Reg loss: 0.025 | Tree loss: 1.140 | Accuracy: 0.615500 | 1.152 sec/iter\n",
      "Epoch: 81 | Batch: 004 / 011 | Total loss: 1.116 | Reg loss: 0.025 | Tree loss: 1.116 | Accuracy: 0.604000 | 1.152 sec/iter\n",
      "Epoch: 81 | Batch: 005 / 011 | Total loss: 1.115 | Reg loss: 0.025 | Tree loss: 1.115 | Accuracy: 0.553000 | 1.152 sec/iter\n",
      "Epoch: 81 | Batch: 006 / 011 | Total loss: 1.078 | Reg loss: 0.025 | Tree loss: 1.078 | Accuracy: 0.579000 | 1.151 sec/iter\n",
      "Epoch: 81 | Batch: 007 / 011 | Total loss: 1.091 | Reg loss: 0.025 | Tree loss: 1.091 | Accuracy: 0.570000 | 1.151 sec/iter\n",
      "Epoch: 81 | Batch: 008 / 011 | Total loss: 1.088 | Reg loss: 0.025 | Tree loss: 1.088 | Accuracy: 0.557000 | 1.151 sec/iter\n",
      "Epoch: 81 | Batch: 009 / 011 | Total loss: 1.079 | Reg loss: 0.025 | Tree loss: 1.079 | Accuracy: 0.579000 | 1.151 sec/iter\n",
      "Epoch: 81 | Batch: 010 / 011 | Total loss: 1.080 | Reg loss: 0.025 | Tree loss: 1.080 | Accuracy: 0.559727 | 1.151 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 82 | Batch: 000 / 011 | Total loss: 1.198 | Reg loss: 0.025 | Tree loss: 1.198 | Accuracy: 0.658000 | 1.153 sec/iter\n",
      "Epoch: 82 | Batch: 001 / 011 | Total loss: 1.199 | Reg loss: 0.025 | Tree loss: 1.199 | Accuracy: 0.629500 | 1.153 sec/iter\n",
      "Epoch: 82 | Batch: 002 / 011 | Total loss: 1.148 | Reg loss: 0.025 | Tree loss: 1.148 | Accuracy: 0.653500 | 1.152 sec/iter\n",
      "Epoch: 82 | Batch: 003 / 011 | Total loss: 1.150 | Reg loss: 0.025 | Tree loss: 1.150 | Accuracy: 0.620000 | 1.152 sec/iter\n",
      "Epoch: 82 | Batch: 004 / 011 | Total loss: 1.119 | Reg loss: 0.025 | Tree loss: 1.119 | Accuracy: 0.610500 | 1.152 sec/iter\n",
      "Epoch: 82 | Batch: 005 / 011 | Total loss: 1.095 | Reg loss: 0.025 | Tree loss: 1.095 | Accuracy: 0.600500 | 1.152 sec/iter\n",
      "Epoch: 82 | Batch: 006 / 011 | Total loss: 1.089 | Reg loss: 0.025 | Tree loss: 1.089 | Accuracy: 0.585500 | 1.152 sec/iter\n",
      "Epoch: 82 | Batch: 007 / 011 | Total loss: 1.081 | Reg loss: 0.025 | Tree loss: 1.081 | Accuracy: 0.583000 | 1.152 sec/iter\n",
      "Epoch: 82 | Batch: 008 / 011 | Total loss: 1.115 | Reg loss: 0.025 | Tree loss: 1.115 | Accuracy: 0.551000 | 1.151 sec/iter\n",
      "Epoch: 82 | Batch: 009 / 011 | Total loss: 1.084 | Reg loss: 0.025 | Tree loss: 1.084 | Accuracy: 0.567500 | 1.151 sec/iter\n",
      "Epoch: 82 | Batch: 010 / 011 | Total loss: 1.054 | Reg loss: 0.025 | Tree loss: 1.054 | Accuracy: 0.593857 | 1.151 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 83 | Batch: 000 / 011 | Total loss: 1.203 | Reg loss: 0.025 | Tree loss: 1.203 | Accuracy: 0.650500 | 1.153 sec/iter\n",
      "Epoch: 83 | Batch: 001 / 011 | Total loss: 1.180 | Reg loss: 0.025 | Tree loss: 1.180 | Accuracy: 0.650000 | 1.153 sec/iter\n",
      "Epoch: 83 | Batch: 002 / 011 | Total loss: 1.162 | Reg loss: 0.025 | Tree loss: 1.162 | Accuracy: 0.641000 | 1.153 sec/iter\n",
      "Epoch: 83 | Batch: 003 / 011 | Total loss: 1.147 | Reg loss: 0.025 | Tree loss: 1.147 | Accuracy: 0.614500 | 1.153 sec/iter\n",
      "Epoch: 83 | Batch: 004 / 011 | Total loss: 1.124 | Reg loss: 0.025 | Tree loss: 1.124 | Accuracy: 0.603500 | 1.153 sec/iter\n",
      "Epoch: 83 | Batch: 005 / 011 | Total loss: 1.099 | Reg loss: 0.025 | Tree loss: 1.099 | Accuracy: 0.589000 | 1.153 sec/iter\n",
      "Epoch: 83 | Batch: 006 / 011 | Total loss: 1.102 | Reg loss: 0.025 | Tree loss: 1.102 | Accuracy: 0.553500 | 1.153 sec/iter\n",
      "Epoch: 83 | Batch: 007 / 011 | Total loss: 1.073 | Reg loss: 0.025 | Tree loss: 1.073 | Accuracy: 0.574000 | 1.152 sec/iter\n",
      "Epoch: 83 | Batch: 008 / 011 | Total loss: 1.076 | Reg loss: 0.025 | Tree loss: 1.076 | Accuracy: 0.564500 | 1.152 sec/iter\n",
      "Epoch: 83 | Batch: 009 / 011 | Total loss: 1.062 | Reg loss: 0.025 | Tree loss: 1.062 | Accuracy: 0.577500 | 1.152 sec/iter\n",
      "Epoch: 83 | Batch: 010 / 011 | Total loss: 1.062 | Reg loss: 0.025 | Tree loss: 1.062 | Accuracy: 0.563140 | 1.152 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 84 | Batch: 000 / 011 | Total loss: 1.221 | Reg loss: 0.025 | Tree loss: 1.221 | Accuracy: 0.636500 | 1.154 sec/iter\n",
      "Epoch: 84 | Batch: 001 / 011 | Total loss: 1.190 | Reg loss: 0.025 | Tree loss: 1.190 | Accuracy: 0.638000 | 1.154 sec/iter\n",
      "Epoch: 84 | Batch: 002 / 011 | Total loss: 1.155 | Reg loss: 0.025 | Tree loss: 1.155 | Accuracy: 0.642000 | 1.154 sec/iter\n",
      "Epoch: 84 | Batch: 003 / 011 | Total loss: 1.141 | Reg loss: 0.025 | Tree loss: 1.141 | Accuracy: 0.609500 | 1.154 sec/iter\n",
      "Epoch: 84 | Batch: 004 / 011 | Total loss: 1.087 | Reg loss: 0.025 | Tree loss: 1.087 | Accuracy: 0.626000 | 1.154 sec/iter\n",
      "Epoch: 84 | Batch: 005 / 011 | Total loss: 1.095 | Reg loss: 0.025 | Tree loss: 1.095 | Accuracy: 0.588000 | 1.154 sec/iter\n",
      "Epoch: 84 | Batch: 006 / 011 | Total loss: 1.101 | Reg loss: 0.025 | Tree loss: 1.101 | Accuracy: 0.561500 | 1.153 sec/iter\n",
      "Epoch: 84 | Batch: 007 / 011 | Total loss: 1.072 | Reg loss: 0.025 | Tree loss: 1.072 | Accuracy: 0.561500 | 1.153 sec/iter\n",
      "Epoch: 84 | Batch: 008 / 011 | Total loss: 1.078 | Reg loss: 0.025 | Tree loss: 1.078 | Accuracy: 0.571000 | 1.153 sec/iter\n",
      "Epoch: 84 | Batch: 009 / 011 | Total loss: 1.086 | Reg loss: 0.025 | Tree loss: 1.086 | Accuracy: 0.550000 | 1.153 sec/iter\n",
      "Epoch: 84 | Batch: 010 / 011 | Total loss: 1.096 | Reg loss: 0.025 | Tree loss: 1.096 | Accuracy: 0.546075 | 1.153 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 85 | Batch: 000 / 011 | Total loss: 1.206 | Reg loss: 0.025 | Tree loss: 1.206 | Accuracy: 0.654500 | 1.155 sec/iter\n",
      "Epoch: 85 | Batch: 001 / 011 | Total loss: 1.188 | Reg loss: 0.025 | Tree loss: 1.188 | Accuracy: 0.661000 | 1.155 sec/iter\n",
      "Epoch: 85 | Batch: 002 / 011 | Total loss: 1.160 | Reg loss: 0.025 | Tree loss: 1.160 | Accuracy: 0.632500 | 1.155 sec/iter\n",
      "Epoch: 85 | Batch: 003 / 011 | Total loss: 1.122 | Reg loss: 0.025 | Tree loss: 1.122 | Accuracy: 0.623500 | 1.155 sec/iter\n",
      "Epoch: 85 | Batch: 004 / 011 | Total loss: 1.122 | Reg loss: 0.025 | Tree loss: 1.122 | Accuracy: 0.604500 | 1.155 sec/iter\n",
      "Epoch: 85 | Batch: 005 / 011 | Total loss: 1.091 | Reg loss: 0.025 | Tree loss: 1.091 | Accuracy: 0.593500 | 1.154 sec/iter\n",
      "Epoch: 85 | Batch: 006 / 011 | Total loss: 1.089 | Reg loss: 0.025 | Tree loss: 1.089 | Accuracy: 0.575500 | 1.154 sec/iter\n",
      "Epoch: 85 | Batch: 007 / 011 | Total loss: 1.079 | Reg loss: 0.025 | Tree loss: 1.079 | Accuracy: 0.575500 | 1.154 sec/iter\n",
      "Epoch: 85 | Batch: 008 / 011 | Total loss: 1.070 | Reg loss: 0.025 | Tree loss: 1.070 | Accuracy: 0.576000 | 1.154 sec/iter\n",
      "Epoch: 85 | Batch: 009 / 011 | Total loss: 1.085 | Reg loss: 0.025 | Tree loss: 1.085 | Accuracy: 0.560000 | 1.154 sec/iter\n",
      "Epoch: 85 | Batch: 010 / 011 | Total loss: 1.040 | Reg loss: 0.025 | Tree loss: 1.040 | Accuracy: 0.600683 | 1.153 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 86 | Batch: 000 / 011 | Total loss: 1.198 | Reg loss: 0.025 | Tree loss: 1.198 | Accuracy: 0.657500 | 1.153 sec/iter\n",
      "Epoch: 86 | Batch: 001 / 011 | Total loss: 1.179 | Reg loss: 0.025 | Tree loss: 1.179 | Accuracy: 0.641500 | 1.153 sec/iter\n",
      "Epoch: 86 | Batch: 002 / 011 | Total loss: 1.156 | Reg loss: 0.025 | Tree loss: 1.156 | Accuracy: 0.618000 | 1.153 sec/iter\n",
      "Epoch: 86 | Batch: 003 / 011 | Total loss: 1.116 | Reg loss: 0.025 | Tree loss: 1.116 | Accuracy: 0.613000 | 1.152 sec/iter\n",
      "Epoch: 86 | Batch: 004 / 011 | Total loss: 1.113 | Reg loss: 0.025 | Tree loss: 1.113 | Accuracy: 0.583500 | 1.152 sec/iter\n",
      "Epoch: 86 | Batch: 005 / 011 | Total loss: 1.100 | Reg loss: 0.025 | Tree loss: 1.100 | Accuracy: 0.562500 | 1.152 sec/iter\n",
      "Epoch: 86 | Batch: 006 / 011 | Total loss: 1.100 | Reg loss: 0.025 | Tree loss: 1.100 | Accuracy: 0.552000 | 1.152 sec/iter\n",
      "Epoch: 86 | Batch: 007 / 011 | Total loss: 1.084 | Reg loss: 0.025 | Tree loss: 1.084 | Accuracy: 0.545500 | 1.152 sec/iter\n",
      "Epoch: 86 | Batch: 008 / 011 | Total loss: 1.085 | Reg loss: 0.025 | Tree loss: 1.085 | Accuracy: 0.570000 | 1.152 sec/iter\n",
      "Epoch: 86 | Batch: 009 / 011 | Total loss: 1.056 | Reg loss: 0.025 | Tree loss: 1.056 | Accuracy: 0.573000 | 1.152 sec/iter\n",
      "Epoch: 86 | Batch: 010 / 011 | Total loss: 1.050 | Reg loss: 0.025 | Tree loss: 1.050 | Accuracy: 0.573379 | 1.151 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 87 | Batch: 000 / 011 | Total loss: 1.200 | Reg loss: 0.025 | Tree loss: 1.200 | Accuracy: 0.649500 | 1.152 sec/iter\n",
      "Epoch: 87 | Batch: 001 / 011 | Total loss: 1.182 | Reg loss: 0.025 | Tree loss: 1.182 | Accuracy: 0.653000 | 1.152 sec/iter\n",
      "Epoch: 87 | Batch: 002 / 011 | Total loss: 1.157 | Reg loss: 0.025 | Tree loss: 1.157 | Accuracy: 0.635500 | 1.152 sec/iter\n",
      "Epoch: 87 | Batch: 003 / 011 | Total loss: 1.127 | Reg loss: 0.025 | Tree loss: 1.127 | Accuracy: 0.613000 | 1.152 sec/iter\n",
      "Epoch: 87 | Batch: 004 / 011 | Total loss: 1.130 | Reg loss: 0.025 | Tree loss: 1.130 | Accuracy: 0.586000 | 1.152 sec/iter\n",
      "Epoch: 87 | Batch: 005 / 011 | Total loss: 1.087 | Reg loss: 0.025 | Tree loss: 1.087 | Accuracy: 0.585000 | 1.152 sec/iter\n",
      "Epoch: 87 | Batch: 006 / 011 | Total loss: 1.091 | Reg loss: 0.025 | Tree loss: 1.091 | Accuracy: 0.565000 | 1.151 sec/iter\n",
      "Epoch: 87 | Batch: 007 / 011 | Total loss: 1.080 | Reg loss: 0.025 | Tree loss: 1.080 | Accuracy: 0.573000 | 1.151 sec/iter\n",
      "Epoch: 87 | Batch: 008 / 011 | Total loss: 1.047 | Reg loss: 0.025 | Tree loss: 1.047 | Accuracy: 0.599000 | 1.151 sec/iter\n",
      "Epoch: 87 | Batch: 009 / 011 | Total loss: 1.078 | Reg loss: 0.025 | Tree loss: 1.078 | Accuracy: 0.569000 | 1.151 sec/iter\n",
      "Epoch: 87 | Batch: 010 / 011 | Total loss: 1.071 | Reg loss: 0.025 | Tree loss: 1.071 | Accuracy: 0.573379 | 1.151 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 88 | Batch: 000 / 011 | Total loss: 1.194 | Reg loss: 0.025 | Tree loss: 1.194 | Accuracy: 0.651500 | 1.153 sec/iter\n",
      "Epoch: 88 | Batch: 001 / 011 | Total loss: 1.160 | Reg loss: 0.025 | Tree loss: 1.160 | Accuracy: 0.664500 | 1.152 sec/iter\n",
      "Epoch: 88 | Batch: 002 / 011 | Total loss: 1.140 | Reg loss: 0.025 | Tree loss: 1.140 | Accuracy: 0.642000 | 1.152 sec/iter\n",
      "Epoch: 88 | Batch: 003 / 011 | Total loss: 1.141 | Reg loss: 0.025 | Tree loss: 1.141 | Accuracy: 0.594500 | 1.152 sec/iter\n",
      "Epoch: 88 | Batch: 004 / 011 | Total loss: 1.112 | Reg loss: 0.025 | Tree loss: 1.112 | Accuracy: 0.590000 | 1.152 sec/iter\n",
      "Epoch: 88 | Batch: 005 / 011 | Total loss: 1.072 | Reg loss: 0.025 | Tree loss: 1.072 | Accuracy: 0.578500 | 1.152 sec/iter\n",
      "Epoch: 88 | Batch: 006 / 011 | Total loss: 1.077 | Reg loss: 0.025 | Tree loss: 1.077 | Accuracy: 0.559500 | 1.152 sec/iter\n",
      "Epoch: 88 | Batch: 007 / 011 | Total loss: 1.071 | Reg loss: 0.025 | Tree loss: 1.071 | Accuracy: 0.557500 | 1.152 sec/iter\n",
      "Epoch: 88 | Batch: 008 / 011 | Total loss: 1.088 | Reg loss: 0.025 | Tree loss: 1.088 | Accuracy: 0.560500 | 1.152 sec/iter\n",
      "Epoch: 88 | Batch: 009 / 011 | Total loss: 1.080 | Reg loss: 0.025 | Tree loss: 1.080 | Accuracy: 0.567000 | 1.151 sec/iter\n",
      "Epoch: 88 | Batch: 010 / 011 | Total loss: 1.060 | Reg loss: 0.025 | Tree loss: 1.060 | Accuracy: 0.593857 | 1.151 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 89 | Batch: 000 / 011 | Total loss: 1.192 | Reg loss: 0.025 | Tree loss: 1.192 | Accuracy: 0.640000 | 1.153 sec/iter\n",
      "Epoch: 89 | Batch: 001 / 011 | Total loss: 1.189 | Reg loss: 0.025 | Tree loss: 1.189 | Accuracy: 0.640500 | 1.153 sec/iter\n",
      "Epoch: 89 | Batch: 002 / 011 | Total loss: 1.120 | Reg loss: 0.025 | Tree loss: 1.120 | Accuracy: 0.653000 | 1.153 sec/iter\n",
      "Epoch: 89 | Batch: 003 / 011 | Total loss: 1.146 | Reg loss: 0.025 | Tree loss: 1.146 | Accuracy: 0.611500 | 1.153 sec/iter\n",
      "Epoch: 89 | Batch: 004 / 011 | Total loss: 1.113 | Reg loss: 0.025 | Tree loss: 1.113 | Accuracy: 0.595500 | 1.153 sec/iter\n",
      "Epoch: 89 | Batch: 005 / 011 | Total loss: 1.110 | Reg loss: 0.025 | Tree loss: 1.110 | Accuracy: 0.563000 | 1.153 sec/iter\n",
      "Epoch: 89 | Batch: 006 / 011 | Total loss: 1.067 | Reg loss: 0.025 | Tree loss: 1.067 | Accuracy: 0.575000 | 1.153 sec/iter\n",
      "Epoch: 89 | Batch: 007 / 011 | Total loss: 1.067 | Reg loss: 0.025 | Tree loss: 1.067 | Accuracy: 0.575500 | 1.152 sec/iter\n",
      "Epoch: 89 | Batch: 008 / 011 | Total loss: 1.053 | Reg loss: 0.025 | Tree loss: 1.053 | Accuracy: 0.576500 | 1.152 sec/iter\n",
      "Epoch: 89 | Batch: 009 / 011 | Total loss: 1.069 | Reg loss: 0.025 | Tree loss: 1.069 | Accuracy: 0.575000 | 1.152 sec/iter\n",
      "Epoch: 89 | Batch: 010 / 011 | Total loss: 1.048 | Reg loss: 0.025 | Tree loss: 1.048 | Accuracy: 0.614334 | 1.152 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 90 | Batch: 000 / 011 | Total loss: 1.199 | Reg loss: 0.025 | Tree loss: 1.199 | Accuracy: 0.659000 | 1.154 sec/iter\n",
      "Epoch: 90 | Batch: 001 / 011 | Total loss: 1.181 | Reg loss: 0.025 | Tree loss: 1.181 | Accuracy: 0.639000 | 1.154 sec/iter\n",
      "Epoch: 90 | Batch: 002 / 011 | Total loss: 1.148 | Reg loss: 0.025 | Tree loss: 1.148 | Accuracy: 0.624500 | 1.154 sec/iter\n",
      "Epoch: 90 | Batch: 003 / 011 | Total loss: 1.109 | Reg loss: 0.025 | Tree loss: 1.109 | Accuracy: 0.626500 | 1.154 sec/iter\n",
      "Epoch: 90 | Batch: 004 / 011 | Total loss: 1.120 | Reg loss: 0.025 | Tree loss: 1.120 | Accuracy: 0.599500 | 1.154 sec/iter\n",
      "Epoch: 90 | Batch: 005 / 011 | Total loss: 1.085 | Reg loss: 0.025 | Tree loss: 1.085 | Accuracy: 0.600000 | 1.153 sec/iter\n",
      "Epoch: 90 | Batch: 006 / 011 | Total loss: 1.078 | Reg loss: 0.025 | Tree loss: 1.078 | Accuracy: 0.590000 | 1.153 sec/iter\n",
      "Epoch: 90 | Batch: 007 / 011 | Total loss: 1.067 | Reg loss: 0.025 | Tree loss: 1.067 | Accuracy: 0.581000 | 1.153 sec/iter\n",
      "Epoch: 90 | Batch: 008 / 011 | Total loss: 1.064 | Reg loss: 0.025 | Tree loss: 1.064 | Accuracy: 0.580500 | 1.153 sec/iter\n",
      "Epoch: 90 | Batch: 009 / 011 | Total loss: 1.046 | Reg loss: 0.025 | Tree loss: 1.046 | Accuracy: 0.577500 | 1.153 sec/iter\n",
      "Epoch: 90 | Batch: 010 / 011 | Total loss: 1.059 | Reg loss: 0.025 | Tree loss: 1.059 | Accuracy: 0.593857 | 1.152 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 91 | Batch: 000 / 011 | Total loss: 1.214 | Reg loss: 0.025 | Tree loss: 1.214 | Accuracy: 0.641500 | 1.155 sec/iter\n",
      "Epoch: 91 | Batch: 001 / 011 | Total loss: 1.169 | Reg loss: 0.025 | Tree loss: 1.169 | Accuracy: 0.659000 | 1.155 sec/iter\n",
      "Epoch: 91 | Batch: 002 / 011 | Total loss: 1.140 | Reg loss: 0.025 | Tree loss: 1.140 | Accuracy: 0.640500 | 1.155 sec/iter\n",
      "Epoch: 91 | Batch: 003 / 011 | Total loss: 1.121 | Reg loss: 0.025 | Tree loss: 1.121 | Accuracy: 0.617000 | 1.154 sec/iter\n",
      "Epoch: 91 | Batch: 004 / 011 | Total loss: 1.101 | Reg loss: 0.025 | Tree loss: 1.101 | Accuracy: 0.609500 | 1.154 sec/iter\n",
      "Epoch: 91 | Batch: 005 / 011 | Total loss: 1.078 | Reg loss: 0.025 | Tree loss: 1.078 | Accuracy: 0.590500 | 1.154 sec/iter\n",
      "Epoch: 91 | Batch: 006 / 011 | Total loss: 1.074 | Reg loss: 0.025 | Tree loss: 1.074 | Accuracy: 0.577500 | 1.154 sec/iter\n",
      "Epoch: 91 | Batch: 007 / 011 | Total loss: 1.061 | Reg loss: 0.025 | Tree loss: 1.061 | Accuracy: 0.573000 | 1.154 sec/iter\n",
      "Epoch: 91 | Batch: 008 / 011 | Total loss: 1.067 | Reg loss: 0.025 | Tree loss: 1.067 | Accuracy: 0.565000 | 1.154 sec/iter\n",
      "Epoch: 91 | Batch: 009 / 011 | Total loss: 1.072 | Reg loss: 0.025 | Tree loss: 1.072 | Accuracy: 0.566500 | 1.153 sec/iter\n",
      "Epoch: 91 | Batch: 010 / 011 | Total loss: 1.068 | Reg loss: 0.025 | Tree loss: 1.068 | Accuracy: 0.532423 | 1.153 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 92 | Batch: 000 / 011 | Total loss: 1.178 | Reg loss: 0.025 | Tree loss: 1.178 | Accuracy: 0.651000 | 1.153 sec/iter\n",
      "Epoch: 92 | Batch: 001 / 011 | Total loss: 1.178 | Reg loss: 0.025 | Tree loss: 1.178 | Accuracy: 0.643000 | 1.153 sec/iter\n",
      "Epoch: 92 | Batch: 002 / 011 | Total loss: 1.139 | Reg loss: 0.025 | Tree loss: 1.139 | Accuracy: 0.655500 | 1.153 sec/iter\n",
      "Epoch: 92 | Batch: 003 / 011 | Total loss: 1.089 | Reg loss: 0.025 | Tree loss: 1.089 | Accuracy: 0.665500 | 1.152 sec/iter\n",
      "Epoch: 92 | Batch: 004 / 011 | Total loss: 1.095 | Reg loss: 0.025 | Tree loss: 1.095 | Accuracy: 0.610500 | 1.152 sec/iter\n",
      "Epoch: 92 | Batch: 005 / 011 | Total loss: 1.090 | Reg loss: 0.025 | Tree loss: 1.090 | Accuracy: 0.593500 | 1.152 sec/iter\n",
      "Epoch: 92 | Batch: 006 / 011 | Total loss: 1.082 | Reg loss: 0.025 | Tree loss: 1.082 | Accuracy: 0.573500 | 1.152 sec/iter\n",
      "Epoch: 92 | Batch: 007 / 011 | Total loss: 1.101 | Reg loss: 0.025 | Tree loss: 1.101 | Accuracy: 0.551000 | 1.152 sec/iter\n",
      "Epoch: 92 | Batch: 008 / 011 | Total loss: 1.062 | Reg loss: 0.025 | Tree loss: 1.062 | Accuracy: 0.570500 | 1.151 sec/iter\n",
      "Epoch: 92 | Batch: 009 / 011 | Total loss: 1.073 | Reg loss: 0.025 | Tree loss: 1.073 | Accuracy: 0.558500 | 1.151 sec/iter\n",
      "Epoch: 92 | Batch: 010 / 011 | Total loss: 0.997 | Reg loss: 0.025 | Tree loss: 0.997 | Accuracy: 0.580205 | 1.151 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 93 | Batch: 000 / 011 | Total loss: 1.204 | Reg loss: 0.025 | Tree loss: 1.204 | Accuracy: 0.637500 | 1.152 sec/iter\n",
      "Epoch: 93 | Batch: 001 / 011 | Total loss: 1.162 | Reg loss: 0.025 | Tree loss: 1.162 | Accuracy: 0.662000 | 1.152 sec/iter\n",
      "Epoch: 93 | Batch: 002 / 011 | Total loss: 1.135 | Reg loss: 0.025 | Tree loss: 1.135 | Accuracy: 0.646000 | 1.152 sec/iter\n",
      "Epoch: 93 | Batch: 003 / 011 | Total loss: 1.095 | Reg loss: 0.025 | Tree loss: 1.095 | Accuracy: 0.648500 | 1.151 sec/iter\n",
      "Epoch: 93 | Batch: 004 / 011 | Total loss: 1.093 | Reg loss: 0.025 | Tree loss: 1.093 | Accuracy: 0.607000 | 1.151 sec/iter\n",
      "Epoch: 93 | Batch: 005 / 011 | Total loss: 1.099 | Reg loss: 0.025 | Tree loss: 1.099 | Accuracy: 0.571500 | 1.152 sec/iter\n",
      "Epoch: 93 | Batch: 006 / 011 | Total loss: 1.079 | Reg loss: 0.025 | Tree loss: 1.079 | Accuracy: 0.557000 | 1.151 sec/iter\n",
      "Epoch: 93 | Batch: 007 / 011 | Total loss: 1.074 | Reg loss: 0.025 | Tree loss: 1.074 | Accuracy: 0.553500 | 1.151 sec/iter\n",
      "Epoch: 93 | Batch: 008 / 011 | Total loss: 1.056 | Reg loss: 0.025 | Tree loss: 1.056 | Accuracy: 0.578000 | 1.151 sec/iter\n",
      "Epoch: 93 | Batch: 009 / 011 | Total loss: 1.056 | Reg loss: 0.025 | Tree loss: 1.056 | Accuracy: 0.581000 | 1.151 sec/iter\n",
      "Epoch: 93 | Batch: 010 / 011 | Total loss: 1.135 | Reg loss: 0.025 | Tree loss: 1.135 | Accuracy: 0.549488 | 1.151 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 94 | Batch: 000 / 011 | Total loss: 1.196 | Reg loss: 0.025 | Tree loss: 1.196 | Accuracy: 0.642000 | 1.152 sec/iter\n",
      "Epoch: 94 | Batch: 001 / 011 | Total loss: 1.164 | Reg loss: 0.025 | Tree loss: 1.164 | Accuracy: 0.666500 | 1.152 sec/iter\n",
      "Epoch: 94 | Batch: 002 / 011 | Total loss: 1.159 | Reg loss: 0.025 | Tree loss: 1.159 | Accuracy: 0.640500 | 1.152 sec/iter\n",
      "Epoch: 94 | Batch: 003 / 011 | Total loss: 1.122 | Reg loss: 0.025 | Tree loss: 1.122 | Accuracy: 0.626500 | 1.152 sec/iter\n",
      "Epoch: 94 | Batch: 004 / 011 | Total loss: 1.086 | Reg loss: 0.025 | Tree loss: 1.086 | Accuracy: 0.636000 | 1.152 sec/iter\n",
      "Epoch: 94 | Batch: 005 / 011 | Total loss: 1.102 | Reg loss: 0.025 | Tree loss: 1.102 | Accuracy: 0.565500 | 1.152 sec/iter\n",
      "Epoch: 94 | Batch: 006 / 011 | Total loss: 1.057 | Reg loss: 0.025 | Tree loss: 1.057 | Accuracy: 0.596500 | 1.152 sec/iter\n",
      "Epoch: 94 | Batch: 007 / 011 | Total loss: 1.050 | Reg loss: 0.025 | Tree loss: 1.050 | Accuracy: 0.592000 | 1.152 sec/iter\n",
      "Epoch: 94 | Batch: 008 / 011 | Total loss: 1.077 | Reg loss: 0.025 | Tree loss: 1.077 | Accuracy: 0.564000 | 1.151 sec/iter\n",
      "Epoch: 94 | Batch: 009 / 011 | Total loss: 1.056 | Reg loss: 0.025 | Tree loss: 1.056 | Accuracy: 0.572500 | 1.151 sec/iter\n",
      "Epoch: 94 | Batch: 010 / 011 | Total loss: 1.058 | Reg loss: 0.025 | Tree loss: 1.058 | Accuracy: 0.542662 | 1.151 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 95 | Batch: 000 / 011 | Total loss: 1.183 | Reg loss: 0.025 | Tree loss: 1.183 | Accuracy: 0.653500 | 1.153 sec/iter\n",
      "Epoch: 95 | Batch: 001 / 011 | Total loss: 1.154 | Reg loss: 0.025 | Tree loss: 1.154 | Accuracy: 0.664000 | 1.153 sec/iter\n",
      "Epoch: 95 | Batch: 002 / 011 | Total loss: 1.142 | Reg loss: 0.025 | Tree loss: 1.142 | Accuracy: 0.636000 | 1.153 sec/iter\n",
      "Epoch: 95 | Batch: 003 / 011 | Total loss: 1.107 | Reg loss: 0.025 | Tree loss: 1.107 | Accuracy: 0.636500 | 1.153 sec/iter\n",
      "Epoch: 95 | Batch: 004 / 011 | Total loss: 1.102 | Reg loss: 0.025 | Tree loss: 1.102 | Accuracy: 0.628000 | 1.153 sec/iter\n",
      "Epoch: 95 | Batch: 005 / 011 | Total loss: 1.079 | Reg loss: 0.025 | Tree loss: 1.079 | Accuracy: 0.603000 | 1.153 sec/iter\n",
      "Epoch: 95 | Batch: 006 / 011 | Total loss: 1.060 | Reg loss: 0.025 | Tree loss: 1.060 | Accuracy: 0.594500 | 1.152 sec/iter\n",
      "Epoch: 95 | Batch: 007 / 011 | Total loss: 1.057 | Reg loss: 0.025 | Tree loss: 1.057 | Accuracy: 0.570000 | 1.152 sec/iter\n",
      "Epoch: 95 | Batch: 008 / 011 | Total loss: 1.081 | Reg loss: 0.025 | Tree loss: 1.081 | Accuracy: 0.552000 | 1.152 sec/iter\n",
      "Epoch: 95 | Batch: 009 / 011 | Total loss: 1.060 | Reg loss: 0.025 | Tree loss: 1.060 | Accuracy: 0.573000 | 1.152 sec/iter\n",
      "Epoch: 95 | Batch: 010 / 011 | Total loss: 0.984 | Reg loss: 0.025 | Tree loss: 0.984 | Accuracy: 0.645051 | 1.152 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 96 | Batch: 000 / 011 | Total loss: 1.193 | Reg loss: 0.025 | Tree loss: 1.193 | Accuracy: 0.634500 | 1.154 sec/iter\n",
      "Epoch: 96 | Batch: 001 / 011 | Total loss: 1.141 | Reg loss: 0.025 | Tree loss: 1.141 | Accuracy: 0.680000 | 1.154 sec/iter\n",
      "Epoch: 96 | Batch: 002 / 011 | Total loss: 1.141 | Reg loss: 0.025 | Tree loss: 1.141 | Accuracy: 0.636000 | 1.154 sec/iter\n",
      "Epoch: 96 | Batch: 003 / 011 | Total loss: 1.101 | Reg loss: 0.025 | Tree loss: 1.101 | Accuracy: 0.639500 | 1.153 sec/iter\n",
      "Epoch: 96 | Batch: 004 / 011 | Total loss: 1.096 | Reg loss: 0.025 | Tree loss: 1.096 | Accuracy: 0.613000 | 1.153 sec/iter\n",
      "Epoch: 96 | Batch: 005 / 011 | Total loss: 1.084 | Reg loss: 0.025 | Tree loss: 1.084 | Accuracy: 0.598000 | 1.153 sec/iter\n",
      "Epoch: 96 | Batch: 006 / 011 | Total loss: 1.076 | Reg loss: 0.025 | Tree loss: 1.076 | Accuracy: 0.580000 | 1.153 sec/iter\n",
      "Epoch: 96 | Batch: 007 / 011 | Total loss: 1.051 | Reg loss: 0.025 | Tree loss: 1.051 | Accuracy: 0.584000 | 1.153 sec/iter\n",
      "Epoch: 96 | Batch: 008 / 011 | Total loss: 1.060 | Reg loss: 0.025 | Tree loss: 1.060 | Accuracy: 0.581500 | 1.153 sec/iter\n",
      "Epoch: 96 | Batch: 009 / 011 | Total loss: 1.047 | Reg loss: 0.025 | Tree loss: 1.047 | Accuracy: 0.584000 | 1.153 sec/iter\n",
      "Epoch: 96 | Batch: 010 / 011 | Total loss: 1.056 | Reg loss: 0.025 | Tree loss: 1.056 | Accuracy: 0.539249 | 1.152 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 97 | Batch: 000 / 011 | Total loss: 1.183 | Reg loss: 0.025 | Tree loss: 1.183 | Accuracy: 0.651000 | 1.155 sec/iter\n",
      "Epoch: 97 | Batch: 001 / 011 | Total loss: 1.157 | Reg loss: 0.025 | Tree loss: 1.157 | Accuracy: 0.667500 | 1.154 sec/iter\n",
      "Epoch: 97 | Batch: 002 / 011 | Total loss: 1.116 | Reg loss: 0.025 | Tree loss: 1.116 | Accuracy: 0.655000 | 1.154 sec/iter\n",
      "Epoch: 97 | Batch: 003 / 011 | Total loss: 1.106 | Reg loss: 0.025 | Tree loss: 1.106 | Accuracy: 0.635000 | 1.154 sec/iter\n",
      "Epoch: 97 | Batch: 004 / 011 | Total loss: 1.091 | Reg loss: 0.025 | Tree loss: 1.091 | Accuracy: 0.606000 | 1.154 sec/iter\n",
      "Epoch: 97 | Batch: 005 / 011 | Total loss: 1.085 | Reg loss: 0.025 | Tree loss: 1.085 | Accuracy: 0.583500 | 1.154 sec/iter\n",
      "Epoch: 97 | Batch: 006 / 011 | Total loss: 1.072 | Reg loss: 0.025 | Tree loss: 1.072 | Accuracy: 0.573000 | 1.154 sec/iter\n",
      "Epoch: 97 | Batch: 007 / 011 | Total loss: 1.063 | Reg loss: 0.025 | Tree loss: 1.063 | Accuracy: 0.556500 | 1.154 sec/iter\n",
      "Epoch: 97 | Batch: 008 / 011 | Total loss: 1.064 | Reg loss: 0.025 | Tree loss: 1.064 | Accuracy: 0.559000 | 1.153 sec/iter\n",
      "Epoch: 97 | Batch: 009 / 011 | Total loss: 1.064 | Reg loss: 0.025 | Tree loss: 1.064 | Accuracy: 0.572500 | 1.153 sec/iter\n",
      "Epoch: 97 | Batch: 010 / 011 | Total loss: 1.012 | Reg loss: 0.025 | Tree loss: 1.012 | Accuracy: 0.587031 | 1.153 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 98 | Batch: 000 / 011 | Total loss: 1.184 | Reg loss: 0.025 | Tree loss: 1.184 | Accuracy: 0.636500 | 1.153 sec/iter\n",
      "Epoch: 98 | Batch: 001 / 011 | Total loss: 1.175 | Reg loss: 0.025 | Tree loss: 1.175 | Accuracy: 0.659000 | 1.153 sec/iter\n",
      "Epoch: 98 | Batch: 002 / 011 | Total loss: 1.135 | Reg loss: 0.025 | Tree loss: 1.135 | Accuracy: 0.649000 | 1.153 sec/iter\n",
      "Epoch: 98 | Batch: 003 / 011 | Total loss: 1.090 | Reg loss: 0.025 | Tree loss: 1.090 | Accuracy: 0.644500 | 1.152 sec/iter\n",
      "Epoch: 98 | Batch: 004 / 011 | Total loss: 1.096 | Reg loss: 0.025 | Tree loss: 1.096 | Accuracy: 0.621000 | 1.152 sec/iter\n",
      "Epoch: 98 | Batch: 005 / 011 | Total loss: 1.070 | Reg loss: 0.025 | Tree loss: 1.070 | Accuracy: 0.599000 | 1.152 sec/iter\n",
      "Epoch: 98 | Batch: 006 / 011 | Total loss: 1.068 | Reg loss: 0.025 | Tree loss: 1.068 | Accuracy: 0.583500 | 1.152 sec/iter\n",
      "Epoch: 98 | Batch: 007 / 011 | Total loss: 1.069 | Reg loss: 0.025 | Tree loss: 1.069 | Accuracy: 0.569500 | 1.152 sec/iter\n",
      "Epoch: 98 | Batch: 008 / 011 | Total loss: 1.052 | Reg loss: 0.025 | Tree loss: 1.052 | Accuracy: 0.577000 | 1.152 sec/iter\n",
      "Epoch: 98 | Batch: 009 / 011 | Total loss: 1.062 | Reg loss: 0.025 | Tree loss: 1.062 | Accuracy: 0.581500 | 1.152 sec/iter\n",
      "Epoch: 98 | Batch: 010 / 011 | Total loss: 1.017 | Reg loss: 0.025 | Tree loss: 1.017 | Accuracy: 0.590444 | 1.152 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 99 | Batch: 000 / 011 | Total loss: 1.186 | Reg loss: 0.025 | Tree loss: 1.186 | Accuracy: 0.639000 | 1.152 sec/iter\n",
      "Epoch: 99 | Batch: 001 / 011 | Total loss: 1.163 | Reg loss: 0.025 | Tree loss: 1.163 | Accuracy: 0.660500 | 1.152 sec/iter\n",
      "Epoch: 99 | Batch: 002 / 011 | Total loss: 1.113 | Reg loss: 0.025 | Tree loss: 1.113 | Accuracy: 0.663000 | 1.152 sec/iter\n",
      "Epoch: 99 | Batch: 003 / 011 | Total loss: 1.099 | Reg loss: 0.025 | Tree loss: 1.099 | Accuracy: 0.637500 | 1.152 sec/iter\n",
      "Epoch: 99 | Batch: 004 / 011 | Total loss: 1.082 | Reg loss: 0.025 | Tree loss: 1.082 | Accuracy: 0.623500 | 1.152 sec/iter\n",
      "Epoch: 99 | Batch: 005 / 011 | Total loss: 1.084 | Reg loss: 0.025 | Tree loss: 1.084 | Accuracy: 0.599500 | 1.152 sec/iter\n",
      "Epoch: 99 | Batch: 006 / 011 | Total loss: 1.071 | Reg loss: 0.025 | Tree loss: 1.071 | Accuracy: 0.576000 | 1.151 sec/iter\n",
      "Epoch: 99 | Batch: 007 / 011 | Total loss: 1.053 | Reg loss: 0.025 | Tree loss: 1.053 | Accuracy: 0.566000 | 1.151 sec/iter\n",
      "Epoch: 99 | Batch: 008 / 011 | Total loss: 1.066 | Reg loss: 0.025 | Tree loss: 1.066 | Accuracy: 0.579500 | 1.151 sec/iter\n",
      "Epoch: 99 | Batch: 009 / 011 | Total loss: 1.067 | Reg loss: 0.025 | Tree loss: 1.067 | Accuracy: 0.576000 | 1.151 sec/iter\n",
      "Epoch: 99 | Batch: 010 / 011 | Total loss: 1.062 | Reg loss: 0.025 | Tree loss: 1.062 | Accuracy: 0.604096 | 1.151 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 100 | Batch: 000 / 011 | Total loss: 1.179 | Reg loss: 0.025 | Tree loss: 1.179 | Accuracy: 0.640500 | 1.152 sec/iter\n",
      "Epoch: 100 | Batch: 001 / 011 | Total loss: 1.143 | Reg loss: 0.025 | Tree loss: 1.143 | Accuracy: 0.670500 | 1.152 sec/iter\n",
      "Epoch: 100 | Batch: 002 / 011 | Total loss: 1.119 | Reg loss: 0.025 | Tree loss: 1.119 | Accuracy: 0.657500 | 1.152 sec/iter\n",
      "Epoch: 100 | Batch: 003 / 011 | Total loss: 1.120 | Reg loss: 0.025 | Tree loss: 1.120 | Accuracy: 0.618000 | 1.152 sec/iter\n",
      "Epoch: 100 | Batch: 004 / 011 | Total loss: 1.085 | Reg loss: 0.025 | Tree loss: 1.085 | Accuracy: 0.618000 | 1.152 sec/iter\n",
      "Epoch: 100 | Batch: 005 / 011 | Total loss: 1.084 | Reg loss: 0.025 | Tree loss: 1.084 | Accuracy: 0.590500 | 1.152 sec/iter\n",
      "Epoch: 100 | Batch: 006 / 011 | Total loss: 1.061 | Reg loss: 0.025 | Tree loss: 1.061 | Accuracy: 0.585500 | 1.152 sec/iter\n",
      "Epoch: 100 | Batch: 007 / 011 | Total loss: 1.068 | Reg loss: 0.025 | Tree loss: 1.068 | Accuracy: 0.571500 | 1.152 sec/iter\n",
      "Epoch: 100 | Batch: 008 / 011 | Total loss: 1.022 | Reg loss: 0.025 | Tree loss: 1.022 | Accuracy: 0.601000 | 1.151 sec/iter\n",
      "Epoch: 100 | Batch: 009 / 011 | Total loss: 1.047 | Reg loss: 0.025 | Tree loss: 1.047 | Accuracy: 0.599500 | 1.151 sec/iter\n",
      "Epoch: 100 | Batch: 010 / 011 | Total loss: 1.055 | Reg loss: 0.025 | Tree loss: 1.055 | Accuracy: 0.566553 | 1.151 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 101 | Batch: 000 / 011 | Total loss: 1.153 | Reg loss: 0.025 | Tree loss: 1.153 | Accuracy: 0.668000 | 1.153 sec/iter\n",
      "Epoch: 101 | Batch: 001 / 011 | Total loss: 1.160 | Reg loss: 0.025 | Tree loss: 1.160 | Accuracy: 0.665500 | 1.153 sec/iter\n",
      "Epoch: 101 | Batch: 002 / 011 | Total loss: 1.114 | Reg loss: 0.025 | Tree loss: 1.114 | Accuracy: 0.661000 | 1.153 sec/iter\n",
      "Epoch: 101 | Batch: 003 / 011 | Total loss: 1.103 | Reg loss: 0.025 | Tree loss: 1.103 | Accuracy: 0.621000 | 1.153 sec/iter\n",
      "Epoch: 101 | Batch: 004 / 011 | Total loss: 1.089 | Reg loss: 0.025 | Tree loss: 1.089 | Accuracy: 0.601000 | 1.153 sec/iter\n",
      "Epoch: 101 | Batch: 005 / 011 | Total loss: 1.061 | Reg loss: 0.025 | Tree loss: 1.061 | Accuracy: 0.604000 | 1.152 sec/iter\n",
      "Epoch: 101 | Batch: 006 / 011 | Total loss: 1.057 | Reg loss: 0.025 | Tree loss: 1.057 | Accuracy: 0.583000 | 1.152 sec/iter\n",
      "Epoch: 101 | Batch: 007 / 011 | Total loss: 1.064 | Reg loss: 0.025 | Tree loss: 1.064 | Accuracy: 0.571000 | 1.152 sec/iter\n",
      "Epoch: 101 | Batch: 008 / 011 | Total loss: 1.062 | Reg loss: 0.025 | Tree loss: 1.062 | Accuracy: 0.566000 | 1.152 sec/iter\n",
      "Epoch: 101 | Batch: 009 / 011 | Total loss: 1.058 | Reg loss: 0.025 | Tree loss: 1.058 | Accuracy: 0.567500 | 1.152 sec/iter\n",
      "Epoch: 101 | Batch: 010 / 011 | Total loss: 1.032 | Reg loss: 0.025 | Tree loss: 1.032 | Accuracy: 0.542662 | 1.152 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 102 | Batch: 000 / 011 | Total loss: 1.182 | Reg loss: 0.025 | Tree loss: 1.182 | Accuracy: 0.645000 | 1.154 sec/iter\n",
      "Epoch: 102 | Batch: 001 / 011 | Total loss: 1.146 | Reg loss: 0.025 | Tree loss: 1.146 | Accuracy: 0.673000 | 1.153 sec/iter\n",
      "Epoch: 102 | Batch: 002 / 011 | Total loss: 1.130 | Reg loss: 0.025 | Tree loss: 1.130 | Accuracy: 0.648500 | 1.153 sec/iter\n",
      "Epoch: 102 | Batch: 003 / 011 | Total loss: 1.086 | Reg loss: 0.025 | Tree loss: 1.086 | Accuracy: 0.649500 | 1.153 sec/iter\n",
      "Epoch: 102 | Batch: 004 / 011 | Total loss: 1.080 | Reg loss: 0.025 | Tree loss: 1.080 | Accuracy: 0.617500 | 1.153 sec/iter\n",
      "Epoch: 102 | Batch: 005 / 011 | Total loss: 1.059 | Reg loss: 0.025 | Tree loss: 1.059 | Accuracy: 0.619500 | 1.153 sec/iter\n",
      "Epoch: 102 | Batch: 006 / 011 | Total loss: 1.045 | Reg loss: 0.025 | Tree loss: 1.045 | Accuracy: 0.609000 | 1.153 sec/iter\n",
      "Epoch: 102 | Batch: 007 / 011 | Total loss: 1.080 | Reg loss: 0.025 | Tree loss: 1.080 | Accuracy: 0.562000 | 1.153 sec/iter\n",
      "Epoch: 102 | Batch: 008 / 011 | Total loss: 1.051 | Reg loss: 0.025 | Tree loss: 1.051 | Accuracy: 0.584500 | 1.152 sec/iter\n",
      "Epoch: 102 | Batch: 009 / 011 | Total loss: 1.066 | Reg loss: 0.025 | Tree loss: 1.066 | Accuracy: 0.572000 | 1.152 sec/iter\n",
      "Epoch: 102 | Batch: 010 / 011 | Total loss: 1.004 | Reg loss: 0.025 | Tree loss: 1.004 | Accuracy: 0.610922 | 1.152 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 103 | Batch: 000 / 011 | Total loss: 1.166 | Reg loss: 0.025 | Tree loss: 1.166 | Accuracy: 0.638500 | 1.154 sec/iter\n",
      "Epoch: 103 | Batch: 001 / 011 | Total loss: 1.141 | Reg loss: 0.025 | Tree loss: 1.141 | Accuracy: 0.684000 | 1.154 sec/iter\n",
      "Epoch: 103 | Batch: 002 / 011 | Total loss: 1.132 | Reg loss: 0.025 | Tree loss: 1.132 | Accuracy: 0.647000 | 1.154 sec/iter\n",
      "Epoch: 103 | Batch: 003 / 011 | Total loss: 1.088 | Reg loss: 0.025 | Tree loss: 1.088 | Accuracy: 0.637500 | 1.154 sec/iter\n",
      "Epoch: 103 | Batch: 004 / 011 | Total loss: 1.095 | Reg loss: 0.025 | Tree loss: 1.095 | Accuracy: 0.603500 | 1.154 sec/iter\n",
      "Epoch: 103 | Batch: 005 / 011 | Total loss: 1.064 | Reg loss: 0.025 | Tree loss: 1.064 | Accuracy: 0.608000 | 1.154 sec/iter\n",
      "Epoch: 103 | Batch: 006 / 011 | Total loss: 1.073 | Reg loss: 0.025 | Tree loss: 1.073 | Accuracy: 0.583500 | 1.154 sec/iter\n",
      "Epoch: 103 | Batch: 007 / 011 | Total loss: 1.068 | Reg loss: 0.025 | Tree loss: 1.068 | Accuracy: 0.561500 | 1.154 sec/iter\n",
      "Epoch: 103 | Batch: 008 / 011 | Total loss: 1.046 | Reg loss: 0.025 | Tree loss: 1.046 | Accuracy: 0.585500 | 1.153 sec/iter\n",
      "Epoch: 103 | Batch: 009 / 011 | Total loss: 1.038 | Reg loss: 0.025 | Tree loss: 1.038 | Accuracy: 0.596000 | 1.153 sec/iter\n",
      "Epoch: 103 | Batch: 010 / 011 | Total loss: 1.013 | Reg loss: 0.025 | Tree loss: 1.013 | Accuracy: 0.587031 | 1.153 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 104 | Batch: 000 / 011 | Total loss: 1.162 | Reg loss: 0.025 | Tree loss: 1.162 | Accuracy: 0.642500 | 1.153 sec/iter\n",
      "Epoch: 104 | Batch: 001 / 011 | Total loss: 1.126 | Reg loss: 0.025 | Tree loss: 1.126 | Accuracy: 0.686500 | 1.153 sec/iter\n",
      "Epoch: 104 | Batch: 002 / 011 | Total loss: 1.146 | Reg loss: 0.025 | Tree loss: 1.146 | Accuracy: 0.640000 | 1.152 sec/iter\n",
      "Epoch: 104 | Batch: 003 / 011 | Total loss: 1.091 | Reg loss: 0.025 | Tree loss: 1.091 | Accuracy: 0.634000 | 1.152 sec/iter\n",
      "Epoch: 104 | Batch: 004 / 011 | Total loss: 1.076 | Reg loss: 0.025 | Tree loss: 1.076 | Accuracy: 0.584000 | 1.152 sec/iter\n",
      "Epoch: 104 | Batch: 005 / 011 | Total loss: 1.058 | Reg loss: 0.025 | Tree loss: 1.058 | Accuracy: 0.593000 | 1.152 sec/iter\n",
      "Epoch: 104 | Batch: 006 / 011 | Total loss: 1.060 | Reg loss: 0.025 | Tree loss: 1.060 | Accuracy: 0.550500 | 1.152 sec/iter\n",
      "Epoch: 104 | Batch: 007 / 011 | Total loss: 1.063 | Reg loss: 0.025 | Tree loss: 1.063 | Accuracy: 0.561000 | 1.151 sec/iter\n",
      "Epoch: 104 | Batch: 008 / 011 | Total loss: 1.051 | Reg loss: 0.025 | Tree loss: 1.051 | Accuracy: 0.565500 | 1.151 sec/iter\n",
      "Epoch: 104 | Batch: 009 / 011 | Total loss: 1.053 | Reg loss: 0.025 | Tree loss: 1.053 | Accuracy: 0.571000 | 1.151 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 104 | Batch: 010 / 011 | Total loss: 1.041 | Reg loss: 0.025 | Tree loss: 1.041 | Accuracy: 0.593857 | 1.151 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 105 | Batch: 000 / 011 | Total loss: 1.162 | Reg loss: 0.025 | Tree loss: 1.162 | Accuracy: 0.629500 | 1.152 sec/iter\n",
      "Epoch: 105 | Batch: 001 / 011 | Total loss: 1.147 | Reg loss: 0.025 | Tree loss: 1.147 | Accuracy: 0.669000 | 1.152 sec/iter\n",
      "Epoch: 105 | Batch: 002 / 011 | Total loss: 1.117 | Reg loss: 0.025 | Tree loss: 1.117 | Accuracy: 0.662000 | 1.151 sec/iter\n",
      "Epoch: 105 | Batch: 003 / 011 | Total loss: 1.093 | Reg loss: 0.025 | Tree loss: 1.093 | Accuracy: 0.643500 | 1.151 sec/iter\n",
      "Epoch: 105 | Batch: 004 / 011 | Total loss: 1.101 | Reg loss: 0.025 | Tree loss: 1.101 | Accuracy: 0.610000 | 1.151 sec/iter\n",
      "Epoch: 105 | Batch: 005 / 011 | Total loss: 1.059 | Reg loss: 0.025 | Tree loss: 1.059 | Accuracy: 0.608000 | 1.151 sec/iter\n",
      "Epoch: 105 | Batch: 006 / 011 | Total loss: 1.066 | Reg loss: 0.025 | Tree loss: 1.066 | Accuracy: 0.564500 | 1.151 sec/iter\n",
      "Epoch: 105 | Batch: 007 / 011 | Total loss: 1.050 | Reg loss: 0.025 | Tree loss: 1.050 | Accuracy: 0.583500 | 1.151 sec/iter\n",
      "Epoch: 105 | Batch: 008 / 011 | Total loss: 1.040 | Reg loss: 0.025 | Tree loss: 1.040 | Accuracy: 0.576500 | 1.151 sec/iter\n",
      "Epoch: 105 | Batch: 009 / 011 | Total loss: 1.025 | Reg loss: 0.025 | Tree loss: 1.025 | Accuracy: 0.593000 | 1.151 sec/iter\n",
      "Epoch: 105 | Batch: 010 / 011 | Total loss: 1.083 | Reg loss: 0.025 | Tree loss: 1.083 | Accuracy: 0.559727 | 1.15 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 106 | Batch: 000 / 011 | Total loss: 1.163 | Reg loss: 0.025 | Tree loss: 1.163 | Accuracy: 0.642000 | 1.152 sec/iter\n",
      "Epoch: 106 | Batch: 001 / 011 | Total loss: 1.158 | Reg loss: 0.025 | Tree loss: 1.158 | Accuracy: 0.659000 | 1.152 sec/iter\n",
      "Epoch: 106 | Batch: 002 / 011 | Total loss: 1.124 | Reg loss: 0.025 | Tree loss: 1.124 | Accuracy: 0.657500 | 1.152 sec/iter\n",
      "Epoch: 106 | Batch: 003 / 011 | Total loss: 1.078 | Reg loss: 0.025 | Tree loss: 1.078 | Accuracy: 0.651500 | 1.152 sec/iter\n",
      "Epoch: 106 | Batch: 004 / 011 | Total loss: 1.092 | Reg loss: 0.025 | Tree loss: 1.092 | Accuracy: 0.621500 | 1.152 sec/iter\n",
      "Epoch: 106 | Batch: 005 / 011 | Total loss: 1.051 | Reg loss: 0.025 | Tree loss: 1.051 | Accuracy: 0.610000 | 1.152 sec/iter\n",
      "Epoch: 106 | Batch: 006 / 011 | Total loss: 1.051 | Reg loss: 0.025 | Tree loss: 1.051 | Accuracy: 0.611500 | 1.152 sec/iter\n",
      "Epoch: 106 | Batch: 007 / 011 | Total loss: 1.051 | Reg loss: 0.025 | Tree loss: 1.051 | Accuracy: 0.577500 | 1.151 sec/iter\n",
      "Epoch: 106 | Batch: 008 / 011 | Total loss: 1.055 | Reg loss: 0.025 | Tree loss: 1.055 | Accuracy: 0.568500 | 1.151 sec/iter\n",
      "Epoch: 106 | Batch: 009 / 011 | Total loss: 1.052 | Reg loss: 0.025 | Tree loss: 1.052 | Accuracy: 0.585500 | 1.151 sec/iter\n",
      "Epoch: 106 | Batch: 010 / 011 | Total loss: 1.012 | Reg loss: 0.025 | Tree loss: 1.012 | Accuracy: 0.621160 | 1.151 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 107 | Batch: 000 / 011 | Total loss: 1.163 | Reg loss: 0.025 | Tree loss: 1.163 | Accuracy: 0.648000 | 1.153 sec/iter\n",
      "Epoch: 107 | Batch: 001 / 011 | Total loss: 1.146 | Reg loss: 0.025 | Tree loss: 1.146 | Accuracy: 0.672000 | 1.153 sec/iter\n",
      "Epoch: 107 | Batch: 002 / 011 | Total loss: 1.109 | Reg loss: 0.025 | Tree loss: 1.109 | Accuracy: 0.657000 | 1.152 sec/iter\n",
      "Epoch: 107 | Batch: 003 / 011 | Total loss: 1.118 | Reg loss: 0.025 | Tree loss: 1.118 | Accuracy: 0.620000 | 1.152 sec/iter\n",
      "Epoch: 107 | Batch: 004 / 011 | Total loss: 1.072 | Reg loss: 0.025 | Tree loss: 1.072 | Accuracy: 0.637500 | 1.152 sec/iter\n",
      "Epoch: 107 | Batch: 005 / 011 | Total loss: 1.042 | Reg loss: 0.025 | Tree loss: 1.042 | Accuracy: 0.618500 | 1.152 sec/iter\n",
      "Epoch: 107 | Batch: 006 / 011 | Total loss: 1.072 | Reg loss: 0.025 | Tree loss: 1.072 | Accuracy: 0.577000 | 1.152 sec/iter\n",
      "Epoch: 107 | Batch: 007 / 011 | Total loss: 1.039 | Reg loss: 0.025 | Tree loss: 1.039 | Accuracy: 0.584000 | 1.152 sec/iter\n",
      "Epoch: 107 | Batch: 008 / 011 | Total loss: 1.061 | Reg loss: 0.025 | Tree loss: 1.061 | Accuracy: 0.564000 | 1.152 sec/iter\n",
      "Epoch: 107 | Batch: 009 / 011 | Total loss: 1.043 | Reg loss: 0.025 | Tree loss: 1.043 | Accuracy: 0.586000 | 1.152 sec/iter\n",
      "Epoch: 107 | Batch: 010 / 011 | Total loss: 1.058 | Reg loss: 0.025 | Tree loss: 1.058 | Accuracy: 0.600683 | 1.151 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 108 | Batch: 000 / 011 | Total loss: 1.164 | Reg loss: 0.025 | Tree loss: 1.164 | Accuracy: 0.637000 | 1.153 sec/iter\n",
      "Epoch: 108 | Batch: 001 / 011 | Total loss: 1.141 | Reg loss: 0.025 | Tree loss: 1.141 | Accuracy: 0.681500 | 1.153 sec/iter\n",
      "Epoch: 108 | Batch: 002 / 011 | Total loss: 1.107 | Reg loss: 0.025 | Tree loss: 1.107 | Accuracy: 0.675000 | 1.153 sec/iter\n",
      "Epoch: 108 | Batch: 003 / 011 | Total loss: 1.090 | Reg loss: 0.025 | Tree loss: 1.090 | Accuracy: 0.621000 | 1.153 sec/iter\n",
      "Epoch: 108 | Batch: 004 / 011 | Total loss: 1.072 | Reg loss: 0.025 | Tree loss: 1.072 | Accuracy: 0.611000 | 1.153 sec/iter\n",
      "Epoch: 108 | Batch: 005 / 011 | Total loss: 1.067 | Reg loss: 0.025 | Tree loss: 1.067 | Accuracy: 0.586500 | 1.153 sec/iter\n",
      "Epoch: 108 | Batch: 006 / 011 | Total loss: 1.063 | Reg loss: 0.025 | Tree loss: 1.063 | Accuracy: 0.583000 | 1.153 sec/iter\n",
      "Epoch: 108 | Batch: 007 / 011 | Total loss: 1.064 | Reg loss: 0.025 | Tree loss: 1.064 | Accuracy: 0.574000 | 1.153 sec/iter\n",
      "Epoch: 108 | Batch: 008 / 011 | Total loss: 1.038 | Reg loss: 0.025 | Tree loss: 1.038 | Accuracy: 0.583500 | 1.152 sec/iter\n",
      "Epoch: 108 | Batch: 009 / 011 | Total loss: 1.036 | Reg loss: 0.025 | Tree loss: 1.036 | Accuracy: 0.587500 | 1.152 sec/iter\n",
      "Epoch: 108 | Batch: 010 / 011 | Total loss: 1.033 | Reg loss: 0.025 | Tree loss: 1.033 | Accuracy: 0.614334 | 1.152 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 109 | Batch: 000 / 011 | Total loss: 1.153 | Reg loss: 0.025 | Tree loss: 1.153 | Accuracy: 0.645000 | 1.154 sec/iter\n",
      "Epoch: 109 | Batch: 001 / 011 | Total loss: 1.140 | Reg loss: 0.025 | Tree loss: 1.140 | Accuracy: 0.670500 | 1.154 sec/iter\n",
      "Epoch: 109 | Batch: 002 / 011 | Total loss: 1.117 | Reg loss: 0.025 | Tree loss: 1.117 | Accuracy: 0.664000 | 1.154 sec/iter\n",
      "Epoch: 109 | Batch: 003 / 011 | Total loss: 1.082 | Reg loss: 0.025 | Tree loss: 1.082 | Accuracy: 0.646500 | 1.154 sec/iter\n",
      "Epoch: 109 | Batch: 004 / 011 | Total loss: 1.068 | Reg loss: 0.025 | Tree loss: 1.068 | Accuracy: 0.631500 | 1.154 sec/iter\n",
      "Epoch: 109 | Batch: 005 / 011 | Total loss: 1.061 | Reg loss: 0.025 | Tree loss: 1.061 | Accuracy: 0.626000 | 1.153 sec/iter\n",
      "Epoch: 109 | Batch: 006 / 011 | Total loss: 1.068 | Reg loss: 0.025 | Tree loss: 1.068 | Accuracy: 0.582500 | 1.153 sec/iter\n",
      "Epoch: 109 | Batch: 007 / 011 | Total loss: 1.056 | Reg loss: 0.025 | Tree loss: 1.056 | Accuracy: 0.583000 | 1.153 sec/iter\n",
      "Epoch: 109 | Batch: 008 / 011 | Total loss: 1.044 | Reg loss: 0.025 | Tree loss: 1.044 | Accuracy: 0.590000 | 1.153 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 109 | Batch: 009 / 011 | Total loss: 1.032 | Reg loss: 0.025 | Tree loss: 1.032 | Accuracy: 0.606500 | 1.153 sec/iter\n",
      "Epoch: 109 | Batch: 010 / 011 | Total loss: 1.071 | Reg loss: 0.025 | Tree loss: 1.071 | Accuracy: 0.569966 | 1.153 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 110 | Batch: 000 / 011 | Total loss: 1.150 | Reg loss: 0.025 | Tree loss: 1.150 | Accuracy: 0.640500 | 1.153 sec/iter\n",
      "Epoch: 110 | Batch: 001 / 011 | Total loss: 1.127 | Reg loss: 0.025 | Tree loss: 1.127 | Accuracy: 0.679000 | 1.152 sec/iter\n",
      "Epoch: 110 | Batch: 002 / 011 | Total loss: 1.112 | Reg loss: 0.025 | Tree loss: 1.112 | Accuracy: 0.666000 | 1.152 sec/iter\n",
      "Epoch: 110 | Batch: 003 / 011 | Total loss: 1.082 | Reg loss: 0.025 | Tree loss: 1.082 | Accuracy: 0.653000 | 1.152 sec/iter\n",
      "Epoch: 110 | Batch: 004 / 011 | Total loss: 1.077 | Reg loss: 0.025 | Tree loss: 1.077 | Accuracy: 0.618500 | 1.152 sec/iter\n",
      "Epoch: 110 | Batch: 005 / 011 | Total loss: 1.090 | Reg loss: 0.025 | Tree loss: 1.090 | Accuracy: 0.587500 | 1.152 sec/iter\n",
      "Epoch: 110 | Batch: 006 / 011 | Total loss: 1.059 | Reg loss: 0.025 | Tree loss: 1.059 | Accuracy: 0.584000 | 1.151 sec/iter\n",
      "Epoch: 110 | Batch: 007 / 011 | Total loss: 1.017 | Reg loss: 0.025 | Tree loss: 1.017 | Accuracy: 0.599500 | 1.151 sec/iter\n",
      "Epoch: 110 | Batch: 008 / 011 | Total loss: 1.049 | Reg loss: 0.025 | Tree loss: 1.049 | Accuracy: 0.573000 | 1.151 sec/iter\n",
      "Epoch: 110 | Batch: 009 / 011 | Total loss: 1.041 | Reg loss: 0.025 | Tree loss: 1.041 | Accuracy: 0.589000 | 1.151 sec/iter\n",
      "Epoch: 110 | Batch: 010 / 011 | Total loss: 0.988 | Reg loss: 0.025 | Tree loss: 0.988 | Accuracy: 0.614334 | 1.151 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 111 | Batch: 000 / 011 | Total loss: 1.145 | Reg loss: 0.025 | Tree loss: 1.145 | Accuracy: 0.631500 | 1.152 sec/iter\n",
      "Epoch: 111 | Batch: 001 / 011 | Total loss: 1.160 | Reg loss: 0.025 | Tree loss: 1.160 | Accuracy: 0.660500 | 1.152 sec/iter\n",
      "Epoch: 111 | Batch: 002 / 011 | Total loss: 1.112 | Reg loss: 0.025 | Tree loss: 1.112 | Accuracy: 0.655000 | 1.151 sec/iter\n",
      "Epoch: 111 | Batch: 003 / 011 | Total loss: 1.085 | Reg loss: 0.025 | Tree loss: 1.085 | Accuracy: 0.650500 | 1.151 sec/iter\n",
      "Epoch: 111 | Batch: 004 / 011 | Total loss: 1.069 | Reg loss: 0.025 | Tree loss: 1.069 | Accuracy: 0.636500 | 1.151 sec/iter\n",
      "Epoch: 111 | Batch: 005 / 011 | Total loss: 1.053 | Reg loss: 0.025 | Tree loss: 1.053 | Accuracy: 0.604000 | 1.151 sec/iter\n",
      "Epoch: 111 | Batch: 006 / 011 | Total loss: 1.061 | Reg loss: 0.025 | Tree loss: 1.061 | Accuracy: 0.599000 | 1.151 sec/iter\n",
      "Epoch: 111 | Batch: 007 / 011 | Total loss: 1.053 | Reg loss: 0.025 | Tree loss: 1.053 | Accuracy: 0.592000 | 1.151 sec/iter\n",
      "Epoch: 111 | Batch: 008 / 011 | Total loss: 1.034 | Reg loss: 0.025 | Tree loss: 1.034 | Accuracy: 0.599000 | 1.151 sec/iter\n",
      "Epoch: 111 | Batch: 009 / 011 | Total loss: 1.022 | Reg loss: 0.025 | Tree loss: 1.022 | Accuracy: 0.610000 | 1.151 sec/iter\n",
      "Epoch: 111 | Batch: 010 / 011 | Total loss: 1.059 | Reg loss: 0.025 | Tree loss: 1.059 | Accuracy: 0.566553 | 1.15 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 112 | Batch: 000 / 011 | Total loss: 1.160 | Reg loss: 0.025 | Tree loss: 1.160 | Accuracy: 0.629500 | 1.152 sec/iter\n",
      "Epoch: 112 | Batch: 001 / 011 | Total loss: 1.129 | Reg loss: 0.025 | Tree loss: 1.129 | Accuracy: 0.664500 | 1.152 sec/iter\n",
      "Epoch: 112 | Batch: 002 / 011 | Total loss: 1.107 | Reg loss: 0.025 | Tree loss: 1.107 | Accuracy: 0.677500 | 1.152 sec/iter\n",
      "Epoch: 112 | Batch: 003 / 011 | Total loss: 1.075 | Reg loss: 0.025 | Tree loss: 1.075 | Accuracy: 0.641000 | 1.152 sec/iter\n",
      "Epoch: 112 | Batch: 004 / 011 | Total loss: 1.060 | Reg loss: 0.025 | Tree loss: 1.060 | Accuracy: 0.624500 | 1.152 sec/iter\n",
      "Epoch: 112 | Batch: 005 / 011 | Total loss: 1.084 | Reg loss: 0.025 | Tree loss: 1.084 | Accuracy: 0.588000 | 1.152 sec/iter\n",
      "Epoch: 112 | Batch: 006 / 011 | Total loss: 1.041 | Reg loss: 0.025 | Tree loss: 1.041 | Accuracy: 0.588500 | 1.151 sec/iter\n",
      "Epoch: 112 | Batch: 007 / 011 | Total loss: 1.069 | Reg loss: 0.025 | Tree loss: 1.069 | Accuracy: 0.570000 | 1.151 sec/iter\n",
      "Epoch: 112 | Batch: 008 / 011 | Total loss: 1.033 | Reg loss: 0.025 | Tree loss: 1.033 | Accuracy: 0.594500 | 1.151 sec/iter\n",
      "Epoch: 112 | Batch: 009 / 011 | Total loss: 1.037 | Reg loss: 0.025 | Tree loss: 1.037 | Accuracy: 0.582000 | 1.151 sec/iter\n",
      "Epoch: 112 | Batch: 010 / 011 | Total loss: 1.021 | Reg loss: 0.025 | Tree loss: 1.021 | Accuracy: 0.593857 | 1.151 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 113 | Batch: 000 / 011 | Total loss: 1.154 | Reg loss: 0.025 | Tree loss: 1.154 | Accuracy: 0.640500 | 1.153 sec/iter\n",
      "Epoch: 113 | Batch: 001 / 011 | Total loss: 1.124 | Reg loss: 0.025 | Tree loss: 1.124 | Accuracy: 0.673000 | 1.152 sec/iter\n",
      "Epoch: 113 | Batch: 002 / 011 | Total loss: 1.095 | Reg loss: 0.025 | Tree loss: 1.095 | Accuracy: 0.676500 | 1.152 sec/iter\n",
      "Epoch: 113 | Batch: 003 / 011 | Total loss: 1.080 | Reg loss: 0.025 | Tree loss: 1.080 | Accuracy: 0.652000 | 1.152 sec/iter\n",
      "Epoch: 113 | Batch: 004 / 011 | Total loss: 1.048 | Reg loss: 0.025 | Tree loss: 1.048 | Accuracy: 0.638000 | 1.152 sec/iter\n",
      "Epoch: 113 | Batch: 005 / 011 | Total loss: 1.081 | Reg loss: 0.025 | Tree loss: 1.081 | Accuracy: 0.586000 | 1.152 sec/iter\n",
      "Epoch: 113 | Batch: 006 / 011 | Total loss: 1.047 | Reg loss: 0.025 | Tree loss: 1.047 | Accuracy: 0.586500 | 1.152 sec/iter\n",
      "Epoch: 113 | Batch: 007 / 011 | Total loss: 1.050 | Reg loss: 0.025 | Tree loss: 1.050 | Accuracy: 0.572500 | 1.152 sec/iter\n",
      "Epoch: 113 | Batch: 008 / 011 | Total loss: 1.053 | Reg loss: 0.025 | Tree loss: 1.053 | Accuracy: 0.554500 | 1.152 sec/iter\n",
      "Epoch: 113 | Batch: 009 / 011 | Total loss: 1.050 | Reg loss: 0.025 | Tree loss: 1.050 | Accuracy: 0.576000 | 1.151 sec/iter\n",
      "Epoch: 113 | Batch: 010 / 011 | Total loss: 0.984 | Reg loss: 0.025 | Tree loss: 0.984 | Accuracy: 0.604096 | 1.151 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 114 | Batch: 000 / 011 | Total loss: 1.146 | Reg loss: 0.025 | Tree loss: 1.146 | Accuracy: 0.638000 | 1.153 sec/iter\n",
      "Epoch: 114 | Batch: 001 / 011 | Total loss: 1.128 | Reg loss: 0.025 | Tree loss: 1.128 | Accuracy: 0.675000 | 1.153 sec/iter\n",
      "Epoch: 114 | Batch: 002 / 011 | Total loss: 1.101 | Reg loss: 0.025 | Tree loss: 1.101 | Accuracy: 0.658000 | 1.153 sec/iter\n",
      "Epoch: 114 | Batch: 003 / 011 | Total loss: 1.076 | Reg loss: 0.025 | Tree loss: 1.076 | Accuracy: 0.653500 | 1.153 sec/iter\n",
      "Epoch: 114 | Batch: 004 / 011 | Total loss: 1.061 | Reg loss: 0.025 | Tree loss: 1.061 | Accuracy: 0.643500 | 1.153 sec/iter\n",
      "Epoch: 114 | Batch: 005 / 011 | Total loss: 1.044 | Reg loss: 0.025 | Tree loss: 1.044 | Accuracy: 0.637500 | 1.153 sec/iter\n",
      "Epoch: 114 | Batch: 006 / 011 | Total loss: 1.061 | Reg loss: 0.025 | Tree loss: 1.061 | Accuracy: 0.584500 | 1.153 sec/iter\n",
      "Epoch: 114 | Batch: 007 / 011 | Total loss: 1.054 | Reg loss: 0.025 | Tree loss: 1.054 | Accuracy: 0.596500 | 1.152 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 114 | Batch: 008 / 011 | Total loss: 1.030 | Reg loss: 0.025 | Tree loss: 1.030 | Accuracy: 0.604500 | 1.152 sec/iter\n",
      "Epoch: 114 | Batch: 009 / 011 | Total loss: 1.040 | Reg loss: 0.025 | Tree loss: 1.040 | Accuracy: 0.591000 | 1.152 sec/iter\n",
      "Epoch: 114 | Batch: 010 / 011 | Total loss: 1.013 | Reg loss: 0.025 | Tree loss: 1.013 | Accuracy: 0.614334 | 1.152 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 115 | Batch: 000 / 011 | Total loss: 1.172 | Reg loss: 0.025 | Tree loss: 1.172 | Accuracy: 0.613500 | 1.154 sec/iter\n",
      "Epoch: 115 | Batch: 001 / 011 | Total loss: 1.120 | Reg loss: 0.025 | Tree loss: 1.120 | Accuracy: 0.677000 | 1.154 sec/iter\n",
      "Epoch: 115 | Batch: 002 / 011 | Total loss: 1.111 | Reg loss: 0.025 | Tree loss: 1.111 | Accuracy: 0.658000 | 1.153 sec/iter\n",
      "Epoch: 115 | Batch: 003 / 011 | Total loss: 1.081 | Reg loss: 0.025 | Tree loss: 1.081 | Accuracy: 0.639500 | 1.153 sec/iter\n",
      "Epoch: 115 | Batch: 004 / 011 | Total loss: 1.070 | Reg loss: 0.025 | Tree loss: 1.070 | Accuracy: 0.629500 | 1.153 sec/iter\n",
      "Epoch: 115 | Batch: 005 / 011 | Total loss: 1.048 | Reg loss: 0.025 | Tree loss: 1.048 | Accuracy: 0.608000 | 1.153 sec/iter\n",
      "Epoch: 115 | Batch: 006 / 011 | Total loss: 1.042 | Reg loss: 0.025 | Tree loss: 1.042 | Accuracy: 0.603000 | 1.153 sec/iter\n",
      "Epoch: 115 | Batch: 007 / 011 | Total loss: 1.023 | Reg loss: 0.025 | Tree loss: 1.023 | Accuracy: 0.605500 | 1.153 sec/iter\n",
      "Epoch: 115 | Batch: 008 / 011 | Total loss: 1.041 | Reg loss: 0.025 | Tree loss: 1.041 | Accuracy: 0.580500 | 1.153 sec/iter\n",
      "Epoch: 115 | Batch: 009 / 011 | Total loss: 1.025 | Reg loss: 0.025 | Tree loss: 1.025 | Accuracy: 0.597000 | 1.153 sec/iter\n",
      "Epoch: 115 | Batch: 010 / 011 | Total loss: 1.057 | Reg loss: 0.025 | Tree loss: 1.057 | Accuracy: 0.556314 | 1.152 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 116 | Batch: 000 / 011 | Total loss: 1.159 | Reg loss: 0.025 | Tree loss: 1.159 | Accuracy: 0.624000 | 1.152 sec/iter\n",
      "Epoch: 116 | Batch: 001 / 011 | Total loss: 1.159 | Reg loss: 0.025 | Tree loss: 1.159 | Accuracy: 0.652500 | 1.152 sec/iter\n",
      "Epoch: 116 | Batch: 002 / 011 | Total loss: 1.093 | Reg loss: 0.025 | Tree loss: 1.093 | Accuracy: 0.682000 | 1.152 sec/iter\n",
      "Epoch: 116 | Batch: 003 / 011 | Total loss: 1.090 | Reg loss: 0.025 | Tree loss: 1.090 | Accuracy: 0.650000 | 1.152 sec/iter\n",
      "Epoch: 116 | Batch: 004 / 011 | Total loss: 1.048 | Reg loss: 0.025 | Tree loss: 1.048 | Accuracy: 0.637500 | 1.152 sec/iter\n",
      "Epoch: 116 | Batch: 005 / 011 | Total loss: 1.052 | Reg loss: 0.025 | Tree loss: 1.052 | Accuracy: 0.599500 | 1.151 sec/iter\n",
      "Epoch: 116 | Batch: 006 / 011 | Total loss: 1.033 | Reg loss: 0.025 | Tree loss: 1.033 | Accuracy: 0.604000 | 1.151 sec/iter\n",
      "Epoch: 116 | Batch: 007 / 011 | Total loss: 1.040 | Reg loss: 0.025 | Tree loss: 1.040 | Accuracy: 0.578000 | 1.151 sec/iter\n",
      "Epoch: 116 | Batch: 008 / 011 | Total loss: 1.028 | Reg loss: 0.025 | Tree loss: 1.028 | Accuracy: 0.602500 | 1.151 sec/iter\n",
      "Epoch: 116 | Batch: 009 / 011 | Total loss: 1.030 | Reg loss: 0.025 | Tree loss: 1.030 | Accuracy: 0.593500 | 1.151 sec/iter\n",
      "Epoch: 116 | Batch: 010 / 011 | Total loss: 1.025 | Reg loss: 0.025 | Tree loss: 1.025 | Accuracy: 0.580205 | 1.151 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 117 | Batch: 000 / 011 | Total loss: 1.152 | Reg loss: 0.025 | Tree loss: 1.152 | Accuracy: 0.647000 | 1.151 sec/iter\n",
      "Epoch: 117 | Batch: 001 / 011 | Total loss: 1.093 | Reg loss: 0.025 | Tree loss: 1.093 | Accuracy: 0.685500 | 1.151 sec/iter\n",
      "Epoch: 117 | Batch: 002 / 011 | Total loss: 1.104 | Reg loss: 0.025 | Tree loss: 1.104 | Accuracy: 0.669500 | 1.151 sec/iter\n",
      "Epoch: 117 | Batch: 003 / 011 | Total loss: 1.087 | Reg loss: 0.025 | Tree loss: 1.087 | Accuracy: 0.633000 | 1.151 sec/iter\n",
      "Epoch: 117 | Batch: 004 / 011 | Total loss: 1.074 | Reg loss: 0.025 | Tree loss: 1.074 | Accuracy: 0.614500 | 1.151 sec/iter\n",
      "Epoch: 117 | Batch: 005 / 011 | Total loss: 1.044 | Reg loss: 0.025 | Tree loss: 1.044 | Accuracy: 0.624500 | 1.151 sec/iter\n",
      "Epoch: 117 | Batch: 006 / 011 | Total loss: 1.089 | Reg loss: 0.025 | Tree loss: 1.089 | Accuracy: 0.565000 | 1.151 sec/iter\n",
      "Epoch: 117 | Batch: 007 / 011 | Total loss: 1.045 | Reg loss: 0.025 | Tree loss: 1.045 | Accuracy: 0.584500 | 1.15 sec/iter\n",
      "Epoch: 117 | Batch: 008 / 011 | Total loss: 1.012 | Reg loss: 0.025 | Tree loss: 1.012 | Accuracy: 0.618000 | 1.15 sec/iter\n",
      "Epoch: 117 | Batch: 009 / 011 | Total loss: 1.050 | Reg loss: 0.025 | Tree loss: 1.050 | Accuracy: 0.576000 | 1.15 sec/iter\n",
      "Epoch: 117 | Batch: 010 / 011 | Total loss: 0.987 | Reg loss: 0.025 | Tree loss: 0.987 | Accuracy: 0.593857 | 1.15 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 118 | Batch: 000 / 011 | Total loss: 1.173 | Reg loss: 0.025 | Tree loss: 1.173 | Accuracy: 0.609500 | 1.152 sec/iter\n",
      "Epoch: 118 | Batch: 001 / 011 | Total loss: 1.129 | Reg loss: 0.025 | Tree loss: 1.129 | Accuracy: 0.666500 | 1.152 sec/iter\n",
      "Epoch: 118 | Batch: 002 / 011 | Total loss: 1.091 | Reg loss: 0.025 | Tree loss: 1.091 | Accuracy: 0.675000 | 1.151 sec/iter\n",
      "Epoch: 118 | Batch: 003 / 011 | Total loss: 1.076 | Reg loss: 0.025 | Tree loss: 1.076 | Accuracy: 0.643000 | 1.151 sec/iter\n",
      "Epoch: 118 | Batch: 004 / 011 | Total loss: 1.057 | Reg loss: 0.025 | Tree loss: 1.057 | Accuracy: 0.639500 | 1.151 sec/iter\n",
      "Epoch: 118 | Batch: 005 / 011 | Total loss: 1.047 | Reg loss: 0.025 | Tree loss: 1.047 | Accuracy: 0.613000 | 1.151 sec/iter\n",
      "Epoch: 118 | Batch: 006 / 011 | Total loss: 1.042 | Reg loss: 0.025 | Tree loss: 1.042 | Accuracy: 0.612000 | 1.151 sec/iter\n",
      "Epoch: 118 | Batch: 007 / 011 | Total loss: 1.055 | Reg loss: 0.025 | Tree loss: 1.055 | Accuracy: 0.570000 | 1.151 sec/iter\n",
      "Epoch: 118 | Batch: 008 / 011 | Total loss: 1.029 | Reg loss: 0.025 | Tree loss: 1.029 | Accuracy: 0.593000 | 1.151 sec/iter\n",
      "Epoch: 118 | Batch: 009 / 011 | Total loss: 1.025 | Reg loss: 0.025 | Tree loss: 1.025 | Accuracy: 0.599500 | 1.151 sec/iter\n",
      "Epoch: 118 | Batch: 010 / 011 | Total loss: 1.034 | Reg loss: 0.025 | Tree loss: 1.034 | Accuracy: 0.600683 | 1.151 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 119 | Batch: 000 / 011 | Total loss: 1.142 | Reg loss: 0.025 | Tree loss: 1.142 | Accuracy: 0.659500 | 1.152 sec/iter\n",
      "Epoch: 119 | Batch: 001 / 011 | Total loss: 1.115 | Reg loss: 0.025 | Tree loss: 1.115 | Accuracy: 0.668000 | 1.152 sec/iter\n",
      "Epoch: 119 | Batch: 002 / 011 | Total loss: 1.096 | Reg loss: 0.025 | Tree loss: 1.096 | Accuracy: 0.676500 | 1.152 sec/iter\n",
      "Epoch: 119 | Batch: 003 / 011 | Total loss: 1.072 | Reg loss: 0.025 | Tree loss: 1.072 | Accuracy: 0.648500 | 1.152 sec/iter\n",
      "Epoch: 119 | Batch: 004 / 011 | Total loss: 1.061 | Reg loss: 0.025 | Tree loss: 1.061 | Accuracy: 0.641000 | 1.152 sec/iter\n",
      "Epoch: 119 | Batch: 005 / 011 | Total loss: 1.045 | Reg loss: 0.025 | Tree loss: 1.045 | Accuracy: 0.612500 | 1.152 sec/iter\n",
      "Epoch: 119 | Batch: 006 / 011 | Total loss: 1.060 | Reg loss: 0.025 | Tree loss: 1.060 | Accuracy: 0.610500 | 1.152 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 119 | Batch: 007 / 011 | Total loss: 1.042 | Reg loss: 0.025 | Tree loss: 1.042 | Accuracy: 0.582500 | 1.151 sec/iter\n",
      "Epoch: 119 | Batch: 008 / 011 | Total loss: 1.070 | Reg loss: 0.025 | Tree loss: 1.070 | Accuracy: 0.554500 | 1.151 sec/iter\n",
      "Epoch: 119 | Batch: 009 / 011 | Total loss: 1.029 | Reg loss: 0.025 | Tree loss: 1.029 | Accuracy: 0.593000 | 1.151 sec/iter\n",
      "Epoch: 119 | Batch: 010 / 011 | Total loss: 1.044 | Reg loss: 0.025 | Tree loss: 1.044 | Accuracy: 0.587031 | 1.151 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 120 | Batch: 000 / 011 | Total loss: 1.169 | Reg loss: 0.025 | Tree loss: 1.169 | Accuracy: 0.634500 | 1.153 sec/iter\n",
      "Epoch: 120 | Batch: 001 / 011 | Total loss: 1.110 | Reg loss: 0.025 | Tree loss: 1.110 | Accuracy: 0.680000 | 1.153 sec/iter\n",
      "Epoch: 120 | Batch: 002 / 011 | Total loss: 1.111 | Reg loss: 0.025 | Tree loss: 1.111 | Accuracy: 0.663000 | 1.152 sec/iter\n",
      "Epoch: 120 | Batch: 003 / 011 | Total loss: 1.070 | Reg loss: 0.025 | Tree loss: 1.070 | Accuracy: 0.645000 | 1.152 sec/iter\n",
      "Epoch: 120 | Batch: 004 / 011 | Total loss: 1.070 | Reg loss: 0.025 | Tree loss: 1.070 | Accuracy: 0.623500 | 1.152 sec/iter\n",
      "Epoch: 120 | Batch: 005 / 011 | Total loss: 1.053 | Reg loss: 0.025 | Tree loss: 1.053 | Accuracy: 0.611000 | 1.152 sec/iter\n",
      "Epoch: 120 | Batch: 006 / 011 | Total loss: 1.045 | Reg loss: 0.025 | Tree loss: 1.045 | Accuracy: 0.600000 | 1.152 sec/iter\n",
      "Epoch: 120 | Batch: 007 / 011 | Total loss: 1.038 | Reg loss: 0.025 | Tree loss: 1.038 | Accuracy: 0.604000 | 1.152 sec/iter\n",
      "Epoch: 120 | Batch: 008 / 011 | Total loss: 1.025 | Reg loss: 0.025 | Tree loss: 1.025 | Accuracy: 0.605000 | 1.152 sec/iter\n",
      "Epoch: 120 | Batch: 009 / 011 | Total loss: 0.987 | Reg loss: 0.025 | Tree loss: 0.987 | Accuracy: 0.625000 | 1.152 sec/iter\n",
      "Epoch: 120 | Batch: 010 / 011 | Total loss: 1.060 | Reg loss: 0.025 | Tree loss: 1.060 | Accuracy: 0.552901 | 1.151 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 121 | Batch: 000 / 011 | Total loss: 1.165 | Reg loss: 0.025 | Tree loss: 1.165 | Accuracy: 0.626500 | 1.153 sec/iter\n",
      "Epoch: 121 | Batch: 001 / 011 | Total loss: 1.122 | Reg loss: 0.025 | Tree loss: 1.122 | Accuracy: 0.668000 | 1.153 sec/iter\n",
      "Epoch: 121 | Batch: 002 / 011 | Total loss: 1.088 | Reg loss: 0.025 | Tree loss: 1.088 | Accuracy: 0.686000 | 1.153 sec/iter\n",
      "Epoch: 121 | Batch: 003 / 011 | Total loss: 1.059 | Reg loss: 0.025 | Tree loss: 1.059 | Accuracy: 0.661000 | 1.153 sec/iter\n",
      "Epoch: 121 | Batch: 004 / 011 | Total loss: 1.074 | Reg loss: 0.025 | Tree loss: 1.074 | Accuracy: 0.587000 | 1.153 sec/iter\n",
      "Epoch: 121 | Batch: 005 / 011 | Total loss: 1.045 | Reg loss: 0.025 | Tree loss: 1.045 | Accuracy: 0.612500 | 1.153 sec/iter\n",
      "Epoch: 121 | Batch: 006 / 011 | Total loss: 1.029 | Reg loss: 0.025 | Tree loss: 1.029 | Accuracy: 0.600500 | 1.153 sec/iter\n",
      "Epoch: 121 | Batch: 007 / 011 | Total loss: 1.051 | Reg loss: 0.025 | Tree loss: 1.051 | Accuracy: 0.587000 | 1.153 sec/iter\n",
      "Epoch: 121 | Batch: 008 / 011 | Total loss: 1.043 | Reg loss: 0.025 | Tree loss: 1.043 | Accuracy: 0.584500 | 1.152 sec/iter\n",
      "Epoch: 121 | Batch: 009 / 011 | Total loss: 1.022 | Reg loss: 0.025 | Tree loss: 1.022 | Accuracy: 0.603000 | 1.152 sec/iter\n",
      "Epoch: 121 | Batch: 010 / 011 | Total loss: 1.008 | Reg loss: 0.025 | Tree loss: 1.008 | Accuracy: 0.607509 | 1.152 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 122 | Batch: 000 / 011 | Total loss: 1.147 | Reg loss: 0.025 | Tree loss: 1.147 | Accuracy: 0.632000 | 1.152 sec/iter\n",
      "Epoch: 122 | Batch: 001 / 011 | Total loss: 1.121 | Reg loss: 0.025 | Tree loss: 1.121 | Accuracy: 0.681500 | 1.152 sec/iter\n",
      "Epoch: 122 | Batch: 002 / 011 | Total loss: 1.089 | Reg loss: 0.025 | Tree loss: 1.089 | Accuracy: 0.676500 | 1.152 sec/iter\n",
      "Epoch: 122 | Batch: 003 / 011 | Total loss: 1.076 | Reg loss: 0.025 | Tree loss: 1.076 | Accuracy: 0.648000 | 1.151 sec/iter\n",
      "Epoch: 122 | Batch: 004 / 011 | Total loss: 1.061 | Reg loss: 0.025 | Tree loss: 1.061 | Accuracy: 0.645000 | 1.151 sec/iter\n",
      "Epoch: 122 | Batch: 005 / 011 | Total loss: 1.044 | Reg loss: 0.025 | Tree loss: 1.044 | Accuracy: 0.628000 | 1.151 sec/iter\n",
      "Epoch: 122 | Batch: 006 / 011 | Total loss: 1.066 | Reg loss: 0.025 | Tree loss: 1.066 | Accuracy: 0.598500 | 1.151 sec/iter\n",
      "Epoch: 122 | Batch: 007 / 011 | Total loss: 1.032 | Reg loss: 0.025 | Tree loss: 1.032 | Accuracy: 0.597000 | 1.151 sec/iter\n",
      "Epoch: 122 | Batch: 008 / 011 | Total loss: 1.034 | Reg loss: 0.025 | Tree loss: 1.034 | Accuracy: 0.591500 | 1.151 sec/iter\n",
      "Epoch: 122 | Batch: 009 / 011 | Total loss: 1.019 | Reg loss: 0.025 | Tree loss: 1.019 | Accuracy: 0.591000 | 1.151 sec/iter\n",
      "Epoch: 122 | Batch: 010 / 011 | Total loss: 1.016 | Reg loss: 0.025 | Tree loss: 1.016 | Accuracy: 0.604096 | 1.151 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 123 | Batch: 000 / 011 | Total loss: 1.159 | Reg loss: 0.025 | Tree loss: 1.159 | Accuracy: 0.628000 | 1.151 sec/iter\n",
      "Epoch: 123 | Batch: 001 / 011 | Total loss: 1.128 | Reg loss: 0.025 | Tree loss: 1.128 | Accuracy: 0.647000 | 1.151 sec/iter\n",
      "Epoch: 123 | Batch: 002 / 011 | Total loss: 1.095 | Reg loss: 0.025 | Tree loss: 1.095 | Accuracy: 0.676000 | 1.151 sec/iter\n",
      "Epoch: 123 | Batch: 003 / 011 | Total loss: 1.064 | Reg loss: 0.025 | Tree loss: 1.064 | Accuracy: 0.663000 | 1.151 sec/iter\n",
      "Epoch: 123 | Batch: 004 / 011 | Total loss: 1.062 | Reg loss: 0.025 | Tree loss: 1.062 | Accuracy: 0.623000 | 1.151 sec/iter\n",
      "Epoch: 123 | Batch: 005 / 011 | Total loss: 1.045 | Reg loss: 0.025 | Tree loss: 1.045 | Accuracy: 0.620500 | 1.151 sec/iter\n",
      "Epoch: 123 | Batch: 006 / 011 | Total loss: 1.053 | Reg loss: 0.025 | Tree loss: 1.053 | Accuracy: 0.590500 | 1.151 sec/iter\n",
      "Epoch: 123 | Batch: 007 / 011 | Total loss: 1.039 | Reg loss: 0.025 | Tree loss: 1.039 | Accuracy: 0.579500 | 1.15 sec/iter\n",
      "Epoch: 123 | Batch: 008 / 011 | Total loss: 1.013 | Reg loss: 0.025 | Tree loss: 1.013 | Accuracy: 0.598000 | 1.15 sec/iter\n",
      "Epoch: 123 | Batch: 009 / 011 | Total loss: 1.032 | Reg loss: 0.025 | Tree loss: 1.032 | Accuracy: 0.589000 | 1.15 sec/iter\n",
      "Epoch: 123 | Batch: 010 / 011 | Total loss: 0.921 | Reg loss: 0.025 | Tree loss: 0.921 | Accuracy: 0.655290 | 1.15 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 124 | Batch: 000 / 011 | Total loss: 1.165 | Reg loss: 0.025 | Tree loss: 1.165 | Accuracy: 0.615500 | 1.151 sec/iter\n",
      "Epoch: 124 | Batch: 001 / 011 | Total loss: 1.116 | Reg loss: 0.025 | Tree loss: 1.116 | Accuracy: 0.661500 | 1.151 sec/iter\n",
      "Epoch: 124 | Batch: 002 / 011 | Total loss: 1.099 | Reg loss: 0.025 | Tree loss: 1.099 | Accuracy: 0.686000 | 1.151 sec/iter\n",
      "Epoch: 124 | Batch: 003 / 011 | Total loss: 1.081 | Reg loss: 0.025 | Tree loss: 1.081 | Accuracy: 0.643500 | 1.151 sec/iter\n",
      "Epoch: 124 | Batch: 004 / 011 | Total loss: 1.072 | Reg loss: 0.025 | Tree loss: 1.072 | Accuracy: 0.612000 | 1.151 sec/iter\n",
      "Epoch: 124 | Batch: 005 / 011 | Total loss: 1.023 | Reg loss: 0.025 | Tree loss: 1.023 | Accuracy: 0.641000 | 1.151 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 124 | Batch: 006 / 011 | Total loss: 1.038 | Reg loss: 0.025 | Tree loss: 1.038 | Accuracy: 0.605000 | 1.151 sec/iter\n",
      "Epoch: 124 | Batch: 007 / 011 | Total loss: 1.021 | Reg loss: 0.025 | Tree loss: 1.021 | Accuracy: 0.603000 | 1.151 sec/iter\n",
      "Epoch: 124 | Batch: 008 / 011 | Total loss: 1.002 | Reg loss: 0.025 | Tree loss: 1.002 | Accuracy: 0.610500 | 1.151 sec/iter\n",
      "Epoch: 124 | Batch: 009 / 011 | Total loss: 1.028 | Reg loss: 0.025 | Tree loss: 1.028 | Accuracy: 0.594500 | 1.151 sec/iter\n",
      "Epoch: 124 | Batch: 010 / 011 | Total loss: 1.017 | Reg loss: 0.025 | Tree loss: 1.017 | Accuracy: 0.604096 | 1.15 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 125 | Batch: 000 / 011 | Total loss: 1.160 | Reg loss: 0.025 | Tree loss: 1.160 | Accuracy: 0.622000 | 1.152 sec/iter\n",
      "Epoch: 125 | Batch: 001 / 011 | Total loss: 1.110 | Reg loss: 0.025 | Tree loss: 1.110 | Accuracy: 0.676500 | 1.152 sec/iter\n",
      "Epoch: 125 | Batch: 002 / 011 | Total loss: 1.089 | Reg loss: 0.025 | Tree loss: 1.089 | Accuracy: 0.683500 | 1.152 sec/iter\n",
      "Epoch: 125 | Batch: 003 / 011 | Total loss: 1.054 | Reg loss: 0.025 | Tree loss: 1.054 | Accuracy: 0.670000 | 1.152 sec/iter\n",
      "Epoch: 125 | Batch: 004 / 011 | Total loss: 1.060 | Reg loss: 0.025 | Tree loss: 1.060 | Accuracy: 0.620000 | 1.152 sec/iter\n",
      "Epoch: 125 | Batch: 005 / 011 | Total loss: 1.053 | Reg loss: 0.025 | Tree loss: 1.053 | Accuracy: 0.590000 | 1.152 sec/iter\n",
      "Epoch: 125 | Batch: 006 / 011 | Total loss: 1.042 | Reg loss: 0.025 | Tree loss: 1.042 | Accuracy: 0.592000 | 1.151 sec/iter\n",
      "Epoch: 125 | Batch: 007 / 011 | Total loss: 1.031 | Reg loss: 0.025 | Tree loss: 1.031 | Accuracy: 0.585500 | 1.151 sec/iter\n",
      "Epoch: 125 | Batch: 008 / 011 | Total loss: 1.030 | Reg loss: 0.025 | Tree loss: 1.030 | Accuracy: 0.582000 | 1.151 sec/iter\n",
      "Epoch: 125 | Batch: 009 / 011 | Total loss: 1.026 | Reg loss: 0.025 | Tree loss: 1.026 | Accuracy: 0.598000 | 1.151 sec/iter\n",
      "Epoch: 125 | Batch: 010 / 011 | Total loss: 0.998 | Reg loss: 0.025 | Tree loss: 0.998 | Accuracy: 0.617747 | 1.151 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 126 | Batch: 000 / 011 | Total loss: 1.163 | Reg loss: 0.025 | Tree loss: 1.163 | Accuracy: 0.618500 | 1.153 sec/iter\n",
      "Epoch: 126 | Batch: 001 / 011 | Total loss: 1.111 | Reg loss: 0.025 | Tree loss: 1.111 | Accuracy: 0.673500 | 1.152 sec/iter\n",
      "Epoch: 126 | Batch: 002 / 011 | Total loss: 1.095 | Reg loss: 0.025 | Tree loss: 1.095 | Accuracy: 0.677500 | 1.152 sec/iter\n",
      "Epoch: 126 | Batch: 003 / 011 | Total loss: 1.051 | Reg loss: 0.025 | Tree loss: 1.051 | Accuracy: 0.678000 | 1.152 sec/iter\n",
      "Epoch: 126 | Batch: 004 / 011 | Total loss: 1.057 | Reg loss: 0.025 | Tree loss: 1.057 | Accuracy: 0.635000 | 1.152 sec/iter\n",
      "Epoch: 126 | Batch: 005 / 011 | Total loss: 1.043 | Reg loss: 0.025 | Tree loss: 1.043 | Accuracy: 0.632500 | 1.152 sec/iter\n",
      "Epoch: 126 | Batch: 006 / 011 | Total loss: 1.018 | Reg loss: 0.025 | Tree loss: 1.018 | Accuracy: 0.630500 | 1.152 sec/iter\n",
      "Epoch: 126 | Batch: 007 / 011 | Total loss: 1.041 | Reg loss: 0.025 | Tree loss: 1.041 | Accuracy: 0.593500 | 1.152 sec/iter\n",
      "Epoch: 126 | Batch: 008 / 011 | Total loss: 1.022 | Reg loss: 0.025 | Tree loss: 1.022 | Accuracy: 0.596500 | 1.152 sec/iter\n",
      "Epoch: 126 | Batch: 009 / 011 | Total loss: 1.010 | Reg loss: 0.025 | Tree loss: 1.010 | Accuracy: 0.600000 | 1.152 sec/iter\n",
      "Epoch: 126 | Batch: 010 / 011 | Total loss: 1.068 | Reg loss: 0.025 | Tree loss: 1.068 | Accuracy: 0.566553 | 1.151 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 127 | Batch: 000 / 011 | Total loss: 1.131 | Reg loss: 0.025 | Tree loss: 1.131 | Accuracy: 0.648000 | 1.153 sec/iter\n",
      "Epoch: 127 | Batch: 001 / 011 | Total loss: 1.124 | Reg loss: 0.025 | Tree loss: 1.124 | Accuracy: 0.661500 | 1.153 sec/iter\n",
      "Epoch: 127 | Batch: 002 / 011 | Total loss: 1.094 | Reg loss: 0.025 | Tree loss: 1.094 | Accuracy: 0.680000 | 1.153 sec/iter\n",
      "Epoch: 127 | Batch: 003 / 011 | Total loss: 1.072 | Reg loss: 0.025 | Tree loss: 1.072 | Accuracy: 0.648500 | 1.153 sec/iter\n",
      "Epoch: 127 | Batch: 004 / 011 | Total loss: 1.061 | Reg loss: 0.025 | Tree loss: 1.061 | Accuracy: 0.622500 | 1.153 sec/iter\n",
      "Epoch: 127 | Batch: 005 / 011 | Total loss: 1.028 | Reg loss: 0.025 | Tree loss: 1.028 | Accuracy: 0.616000 | 1.153 sec/iter\n",
      "Epoch: 127 | Batch: 006 / 011 | Total loss: 1.021 | Reg loss: 0.025 | Tree loss: 1.021 | Accuracy: 0.621000 | 1.153 sec/iter\n",
      "Epoch: 127 | Batch: 007 / 011 | Total loss: 1.030 | Reg loss: 0.025 | Tree loss: 1.030 | Accuracy: 0.576000 | 1.152 sec/iter\n",
      "Epoch: 127 | Batch: 008 / 011 | Total loss: 1.043 | Reg loss: 0.025 | Tree loss: 1.043 | Accuracy: 0.581000 | 1.152 sec/iter\n",
      "Epoch: 127 | Batch: 009 / 011 | Total loss: 1.020 | Reg loss: 0.025 | Tree loss: 1.020 | Accuracy: 0.589000 | 1.152 sec/iter\n",
      "Epoch: 127 | Batch: 010 / 011 | Total loss: 1.013 | Reg loss: 0.025 | Tree loss: 1.013 | Accuracy: 0.597270 | 1.152 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 128 | Batch: 000 / 011 | Total loss: 1.127 | Reg loss: 0.025 | Tree loss: 1.127 | Accuracy: 0.641000 | 1.152 sec/iter\n",
      "Epoch: 128 | Batch: 001 / 011 | Total loss: 1.114 | Reg loss: 0.025 | Tree loss: 1.114 | Accuracy: 0.657000 | 1.152 sec/iter\n",
      "Epoch: 128 | Batch: 002 / 011 | Total loss: 1.080 | Reg loss: 0.025 | Tree loss: 1.080 | Accuracy: 0.697500 | 1.152 sec/iter\n",
      "Epoch: 128 | Batch: 003 / 011 | Total loss: 1.089 | Reg loss: 0.025 | Tree loss: 1.089 | Accuracy: 0.640500 | 1.151 sec/iter\n",
      "Epoch: 128 | Batch: 004 / 011 | Total loss: 1.035 | Reg loss: 0.025 | Tree loss: 1.035 | Accuracy: 0.648500 | 1.151 sec/iter\n",
      "Epoch: 128 | Batch: 005 / 011 | Total loss: 1.047 | Reg loss: 0.025 | Tree loss: 1.047 | Accuracy: 0.611000 | 1.151 sec/iter\n",
      "Epoch: 128 | Batch: 006 / 011 | Total loss: 1.043 | Reg loss: 0.025 | Tree loss: 1.043 | Accuracy: 0.607000 | 1.151 sec/iter\n",
      "Epoch: 128 | Batch: 007 / 011 | Total loss: 1.029 | Reg loss: 0.025 | Tree loss: 1.029 | Accuracy: 0.610500 | 1.151 sec/iter\n",
      "Epoch: 128 | Batch: 008 / 011 | Total loss: 1.035 | Reg loss: 0.025 | Tree loss: 1.035 | Accuracy: 0.581000 | 1.151 sec/iter\n",
      "Epoch: 128 | Batch: 009 / 011 | Total loss: 1.004 | Reg loss: 0.025 | Tree loss: 1.004 | Accuracy: 0.604000 | 1.151 sec/iter\n",
      "Epoch: 128 | Batch: 010 / 011 | Total loss: 1.057 | Reg loss: 0.025 | Tree loss: 1.057 | Accuracy: 0.607509 | 1.151 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 129 | Batch: 000 / 011 | Total loss: 1.117 | Reg loss: 0.025 | Tree loss: 1.117 | Accuracy: 0.648500 | 1.151 sec/iter\n",
      "Epoch: 129 | Batch: 001 / 011 | Total loss: 1.101 | Reg loss: 0.025 | Tree loss: 1.101 | Accuracy: 0.660000 | 1.151 sec/iter\n",
      "Epoch: 129 | Batch: 002 / 011 | Total loss: 1.070 | Reg loss: 0.025 | Tree loss: 1.070 | Accuracy: 0.672000 | 1.151 sec/iter\n",
      "Epoch: 129 | Batch: 003 / 011 | Total loss: 1.059 | Reg loss: 0.025 | Tree loss: 1.059 | Accuracy: 0.650000 | 1.151 sec/iter\n",
      "Epoch: 129 | Batch: 004 / 011 | Total loss: 1.071 | Reg loss: 0.025 | Tree loss: 1.071 | Accuracy: 0.623500 | 1.151 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 129 | Batch: 005 / 011 | Total loss: 1.027 | Reg loss: 0.025 | Tree loss: 1.027 | Accuracy: 0.648500 | 1.151 sec/iter\n",
      "Epoch: 129 | Batch: 006 / 011 | Total loss: 1.050 | Reg loss: 0.025 | Tree loss: 1.050 | Accuracy: 0.608500 | 1.15 sec/iter\n",
      "Epoch: 129 | Batch: 007 / 011 | Total loss: 1.045 | Reg loss: 0.025 | Tree loss: 1.045 | Accuracy: 0.602000 | 1.15 sec/iter\n",
      "Epoch: 129 | Batch: 008 / 011 | Total loss: 1.027 | Reg loss: 0.025 | Tree loss: 1.027 | Accuracy: 0.597000 | 1.15 sec/iter\n",
      "Epoch: 129 | Batch: 009 / 011 | Total loss: 1.021 | Reg loss: 0.025 | Tree loss: 1.021 | Accuracy: 0.598500 | 1.15 sec/iter\n",
      "Epoch: 129 | Batch: 010 / 011 | Total loss: 1.050 | Reg loss: 0.025 | Tree loss: 1.050 | Accuracy: 0.559727 | 1.15 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 130 | Batch: 000 / 011 | Total loss: 1.123 | Reg loss: 0.025 | Tree loss: 1.123 | Accuracy: 0.647500 | 1.151 sec/iter\n",
      "Epoch: 130 | Batch: 001 / 011 | Total loss: 1.107 | Reg loss: 0.025 | Tree loss: 1.107 | Accuracy: 0.658500 | 1.151 sec/iter\n",
      "Epoch: 130 | Batch: 002 / 011 | Total loss: 1.092 | Reg loss: 0.025 | Tree loss: 1.092 | Accuracy: 0.683000 | 1.151 sec/iter\n",
      "Epoch: 130 | Batch: 003 / 011 | Total loss: 1.053 | Reg loss: 0.025 | Tree loss: 1.053 | Accuracy: 0.689500 | 1.151 sec/iter\n",
      "Epoch: 130 | Batch: 004 / 011 | Total loss: 1.066 | Reg loss: 0.025 | Tree loss: 1.066 | Accuracy: 0.622000 | 1.151 sec/iter\n",
      "Epoch: 130 | Batch: 005 / 011 | Total loss: 1.014 | Reg loss: 0.025 | Tree loss: 1.014 | Accuracy: 0.632000 | 1.151 sec/iter\n",
      "Epoch: 130 | Batch: 006 / 011 | Total loss: 1.036 | Reg loss: 0.025 | Tree loss: 1.036 | Accuracy: 0.609500 | 1.151 sec/iter\n",
      "Epoch: 130 | Batch: 007 / 011 | Total loss: 1.046 | Reg loss: 0.025 | Tree loss: 1.046 | Accuracy: 0.588500 | 1.15 sec/iter\n",
      "Epoch: 130 | Batch: 008 / 011 | Total loss: 1.035 | Reg loss: 0.025 | Tree loss: 1.035 | Accuracy: 0.592000 | 1.15 sec/iter\n",
      "Epoch: 130 | Batch: 009 / 011 | Total loss: 1.030 | Reg loss: 0.025 | Tree loss: 1.030 | Accuracy: 0.618000 | 1.15 sec/iter\n",
      "Epoch: 130 | Batch: 010 / 011 | Total loss: 0.982 | Reg loss: 0.025 | Tree loss: 0.982 | Accuracy: 0.638225 | 1.15 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 131 | Batch: 000 / 011 | Total loss: 1.153 | Reg loss: 0.025 | Tree loss: 1.153 | Accuracy: 0.629500 | 1.152 sec/iter\n",
      "Epoch: 131 | Batch: 001 / 011 | Total loss: 1.106 | Reg loss: 0.025 | Tree loss: 1.106 | Accuracy: 0.672500 | 1.151 sec/iter\n",
      "Epoch: 131 | Batch: 002 / 011 | Total loss: 1.068 | Reg loss: 0.025 | Tree loss: 1.068 | Accuracy: 0.695500 | 1.151 sec/iter\n",
      "Epoch: 131 | Batch: 003 / 011 | Total loss: 1.072 | Reg loss: 0.025 | Tree loss: 1.072 | Accuracy: 0.638500 | 1.151 sec/iter\n",
      "Epoch: 131 | Batch: 004 / 011 | Total loss: 1.028 | Reg loss: 0.025 | Tree loss: 1.028 | Accuracy: 0.643500 | 1.151 sec/iter\n",
      "Epoch: 131 | Batch: 005 / 011 | Total loss: 1.034 | Reg loss: 0.025 | Tree loss: 1.034 | Accuracy: 0.630000 | 1.151 sec/iter\n",
      "Epoch: 131 | Batch: 006 / 011 | Total loss: 1.022 | Reg loss: 0.025 | Tree loss: 1.022 | Accuracy: 0.620500 | 1.151 sec/iter\n",
      "Epoch: 131 | Batch: 007 / 011 | Total loss: 1.052 | Reg loss: 0.025 | Tree loss: 1.052 | Accuracy: 0.581500 | 1.151 sec/iter\n",
      "Epoch: 131 | Batch: 008 / 011 | Total loss: 1.026 | Reg loss: 0.025 | Tree loss: 1.026 | Accuracy: 0.587000 | 1.151 sec/iter\n",
      "Epoch: 131 | Batch: 009 / 011 | Total loss: 1.033 | Reg loss: 0.025 | Tree loss: 1.033 | Accuracy: 0.582000 | 1.151 sec/iter\n",
      "Epoch: 131 | Batch: 010 / 011 | Total loss: 0.979 | Reg loss: 0.025 | Tree loss: 0.979 | Accuracy: 0.604096 | 1.151 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 132 | Batch: 000 / 011 | Total loss: 1.127 | Reg loss: 0.025 | Tree loss: 1.127 | Accuracy: 0.637000 | 1.152 sec/iter\n",
      "Epoch: 132 | Batch: 001 / 011 | Total loss: 1.108 | Reg loss: 0.025 | Tree loss: 1.108 | Accuracy: 0.657000 | 1.152 sec/iter\n",
      "Epoch: 132 | Batch: 002 / 011 | Total loss: 1.082 | Reg loss: 0.025 | Tree loss: 1.082 | Accuracy: 0.682000 | 1.152 sec/iter\n",
      "Epoch: 132 | Batch: 003 / 011 | Total loss: 1.047 | Reg loss: 0.025 | Tree loss: 1.047 | Accuracy: 0.674500 | 1.152 sec/iter\n",
      "Epoch: 132 | Batch: 004 / 011 | Total loss: 1.050 | Reg loss: 0.025 | Tree loss: 1.050 | Accuracy: 0.636500 | 1.152 sec/iter\n",
      "Epoch: 132 | Batch: 005 / 011 | Total loss: 1.043 | Reg loss: 0.025 | Tree loss: 1.043 | Accuracy: 0.616500 | 1.152 sec/iter\n",
      "Epoch: 132 | Batch: 006 / 011 | Total loss: 1.031 | Reg loss: 0.025 | Tree loss: 1.031 | Accuracy: 0.617500 | 1.152 sec/iter\n",
      "Epoch: 132 | Batch: 007 / 011 | Total loss: 1.031 | Reg loss: 0.025 | Tree loss: 1.031 | Accuracy: 0.598000 | 1.151 sec/iter\n",
      "Epoch: 132 | Batch: 008 / 011 | Total loss: 1.029 | Reg loss: 0.025 | Tree loss: 1.029 | Accuracy: 0.591000 | 1.151 sec/iter\n",
      "Epoch: 132 | Batch: 009 / 011 | Total loss: 1.019 | Reg loss: 0.025 | Tree loss: 1.019 | Accuracy: 0.618500 | 1.151 sec/iter\n",
      "Epoch: 132 | Batch: 010 / 011 | Total loss: 0.997 | Reg loss: 0.025 | Tree loss: 0.997 | Accuracy: 0.634812 | 1.151 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 133 | Batch: 000 / 011 | Total loss: 1.130 | Reg loss: 0.025 | Tree loss: 1.130 | Accuracy: 0.633500 | 1.153 sec/iter\n",
      "Epoch: 133 | Batch: 001 / 011 | Total loss: 1.108 | Reg loss: 0.025 | Tree loss: 1.108 | Accuracy: 0.659000 | 1.152 sec/iter\n",
      "Epoch: 133 | Batch: 002 / 011 | Total loss: 1.070 | Reg loss: 0.025 | Tree loss: 1.070 | Accuracy: 0.673500 | 1.152 sec/iter\n",
      "Epoch: 133 | Batch: 003 / 011 | Total loss: 1.071 | Reg loss: 0.025 | Tree loss: 1.071 | Accuracy: 0.666000 | 1.152 sec/iter\n",
      "Epoch: 133 | Batch: 004 / 011 | Total loss: 1.033 | Reg loss: 0.025 | Tree loss: 1.033 | Accuracy: 0.635500 | 1.152 sec/iter\n",
      "Epoch: 133 | Batch: 005 / 011 | Total loss: 1.036 | Reg loss: 0.025 | Tree loss: 1.036 | Accuracy: 0.624000 | 1.152 sec/iter\n",
      "Epoch: 133 | Batch: 006 / 011 | Total loss: 1.037 | Reg loss: 0.025 | Tree loss: 1.037 | Accuracy: 0.600500 | 1.152 sec/iter\n",
      "Epoch: 133 | Batch: 007 / 011 | Total loss: 1.017 | Reg loss: 0.025 | Tree loss: 1.017 | Accuracy: 0.601000 | 1.152 sec/iter\n",
      "Epoch: 133 | Batch: 008 / 011 | Total loss: 1.022 | Reg loss: 0.025 | Tree loss: 1.022 | Accuracy: 0.591000 | 1.152 sec/iter\n",
      "Epoch: 133 | Batch: 009 / 011 | Total loss: 1.028 | Reg loss: 0.025 | Tree loss: 1.028 | Accuracy: 0.591000 | 1.152 sec/iter\n",
      "Epoch: 133 | Batch: 010 / 011 | Total loss: 1.002 | Reg loss: 0.025 | Tree loss: 1.002 | Accuracy: 0.587031 | 1.151 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 134 | Batch: 000 / 011 | Total loss: 1.128 | Reg loss: 0.025 | Tree loss: 1.128 | Accuracy: 0.618000 | 1.152 sec/iter\n",
      "Epoch: 134 | Batch: 001 / 011 | Total loss: 1.118 | Reg loss: 0.025 | Tree loss: 1.118 | Accuracy: 0.660000 | 1.151 sec/iter\n",
      "Epoch: 134 | Batch: 002 / 011 | Total loss: 1.097 | Reg loss: 0.025 | Tree loss: 1.097 | Accuracy: 0.677000 | 1.151 sec/iter\n",
      "Epoch: 134 | Batch: 003 / 011 | Total loss: 1.037 | Reg loss: 0.025 | Tree loss: 1.037 | Accuracy: 0.689000 | 1.151 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 134 | Batch: 004 / 011 | Total loss: 1.035 | Reg loss: 0.025 | Tree loss: 1.035 | Accuracy: 0.648000 | 1.151 sec/iter\n",
      "Epoch: 134 | Batch: 005 / 011 | Total loss: 1.044 | Reg loss: 0.025 | Tree loss: 1.044 | Accuracy: 0.615000 | 1.151 sec/iter\n",
      "Epoch: 134 | Batch: 006 / 011 | Total loss: 1.030 | Reg loss: 0.025 | Tree loss: 1.030 | Accuracy: 0.608500 | 1.151 sec/iter\n",
      "Epoch: 134 | Batch: 007 / 011 | Total loss: 1.018 | Reg loss: 0.025 | Tree loss: 1.018 | Accuracy: 0.603500 | 1.151 sec/iter\n",
      "Epoch: 134 | Batch: 008 / 011 | Total loss: 1.041 | Reg loss: 0.025 | Tree loss: 1.041 | Accuracy: 0.582500 | 1.151 sec/iter\n",
      "Epoch: 134 | Batch: 009 / 011 | Total loss: 0.992 | Reg loss: 0.025 | Tree loss: 0.992 | Accuracy: 0.624500 | 1.151 sec/iter\n",
      "Epoch: 134 | Batch: 010 / 011 | Total loss: 1.045 | Reg loss: 0.025 | Tree loss: 1.045 | Accuracy: 0.583618 | 1.15 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 135 | Batch: 000 / 011 | Total loss: 1.125 | Reg loss: 0.025 | Tree loss: 1.125 | Accuracy: 0.644000 | 1.151 sec/iter\n",
      "Epoch: 135 | Batch: 001 / 011 | Total loss: 1.107 | Reg loss: 0.025 | Tree loss: 1.107 | Accuracy: 0.661500 | 1.151 sec/iter\n",
      "Epoch: 135 | Batch: 002 / 011 | Total loss: 1.089 | Reg loss: 0.025 | Tree loss: 1.089 | Accuracy: 0.683000 | 1.151 sec/iter\n",
      "Epoch: 135 | Batch: 003 / 011 | Total loss: 1.046 | Reg loss: 0.025 | Tree loss: 1.046 | Accuracy: 0.669000 | 1.151 sec/iter\n",
      "Epoch: 135 | Batch: 004 / 011 | Total loss: 1.061 | Reg loss: 0.025 | Tree loss: 1.061 | Accuracy: 0.648500 | 1.151 sec/iter\n",
      "Epoch: 135 | Batch: 005 / 011 | Total loss: 1.042 | Reg loss: 0.025 | Tree loss: 1.042 | Accuracy: 0.619000 | 1.15 sec/iter\n",
      "Epoch: 135 | Batch: 006 / 011 | Total loss: 1.037 | Reg loss: 0.025 | Tree loss: 1.037 | Accuracy: 0.596500 | 1.15 sec/iter\n",
      "Epoch: 135 | Batch: 007 / 011 | Total loss: 1.005 | Reg loss: 0.025 | Tree loss: 1.005 | Accuracy: 0.624000 | 1.15 sec/iter\n",
      "Epoch: 135 | Batch: 008 / 011 | Total loss: 1.011 | Reg loss: 0.025 | Tree loss: 1.011 | Accuracy: 0.605000 | 1.15 sec/iter\n",
      "Epoch: 135 | Batch: 009 / 011 | Total loss: 1.007 | Reg loss: 0.025 | Tree loss: 1.007 | Accuracy: 0.605000 | 1.15 sec/iter\n",
      "Epoch: 135 | Batch: 010 / 011 | Total loss: 0.981 | Reg loss: 0.025 | Tree loss: 0.981 | Accuracy: 0.631399 | 1.15 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 136 | Batch: 000 / 011 | Total loss: 1.124 | Reg loss: 0.025 | Tree loss: 1.124 | Accuracy: 0.622500 | 1.151 sec/iter\n",
      "Epoch: 136 | Batch: 001 / 011 | Total loss: 1.104 | Reg loss: 0.025 | Tree loss: 1.104 | Accuracy: 0.670000 | 1.151 sec/iter\n",
      "Epoch: 136 | Batch: 002 / 011 | Total loss: 1.082 | Reg loss: 0.025 | Tree loss: 1.082 | Accuracy: 0.678000 | 1.151 sec/iter\n",
      "Epoch: 136 | Batch: 003 / 011 | Total loss: 1.028 | Reg loss: 0.025 | Tree loss: 1.028 | Accuracy: 0.677000 | 1.151 sec/iter\n",
      "Epoch: 136 | Batch: 004 / 011 | Total loss: 1.039 | Reg loss: 0.025 | Tree loss: 1.039 | Accuracy: 0.644500 | 1.151 sec/iter\n",
      "Epoch: 136 | Batch: 005 / 011 | Total loss: 1.044 | Reg loss: 0.025 | Tree loss: 1.044 | Accuracy: 0.618500 | 1.151 sec/iter\n",
      "Epoch: 136 | Batch: 006 / 011 | Total loss: 1.053 | Reg loss: 0.025 | Tree loss: 1.053 | Accuracy: 0.609500 | 1.15 sec/iter\n",
      "Epoch: 136 | Batch: 007 / 011 | Total loss: 1.018 | Reg loss: 0.025 | Tree loss: 1.018 | Accuracy: 0.614500 | 1.15 sec/iter\n",
      "Epoch: 136 | Batch: 008 / 011 | Total loss: 1.022 | Reg loss: 0.025 | Tree loss: 1.022 | Accuracy: 0.615000 | 1.15 sec/iter\n",
      "Epoch: 136 | Batch: 009 / 011 | Total loss: 1.010 | Reg loss: 0.025 | Tree loss: 1.010 | Accuracy: 0.593000 | 1.15 sec/iter\n",
      "Epoch: 136 | Batch: 010 / 011 | Total loss: 0.990 | Reg loss: 0.025 | Tree loss: 0.990 | Accuracy: 0.621160 | 1.15 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 137 | Batch: 000 / 011 | Total loss: 1.117 | Reg loss: 0.025 | Tree loss: 1.117 | Accuracy: 0.635000 | 1.151 sec/iter\n",
      "Epoch: 137 | Batch: 001 / 011 | Total loss: 1.114 | Reg loss: 0.025 | Tree loss: 1.114 | Accuracy: 0.661000 | 1.151 sec/iter\n",
      "Epoch: 137 | Batch: 002 / 011 | Total loss: 1.076 | Reg loss: 0.025 | Tree loss: 1.076 | Accuracy: 0.683000 | 1.151 sec/iter\n",
      "Epoch: 137 | Batch: 003 / 011 | Total loss: 1.060 | Reg loss: 0.025 | Tree loss: 1.060 | Accuracy: 0.671500 | 1.151 sec/iter\n",
      "Epoch: 137 | Batch: 004 / 011 | Total loss: 1.024 | Reg loss: 0.025 | Tree loss: 1.024 | Accuracy: 0.647000 | 1.151 sec/iter\n",
      "Epoch: 137 | Batch: 005 / 011 | Total loss: 1.025 | Reg loss: 0.025 | Tree loss: 1.025 | Accuracy: 0.630000 | 1.151 sec/iter\n",
      "Epoch: 137 | Batch: 006 / 011 | Total loss: 1.030 | Reg loss: 0.025 | Tree loss: 1.030 | Accuracy: 0.608000 | 1.151 sec/iter\n",
      "Epoch: 137 | Batch: 007 / 011 | Total loss: 1.033 | Reg loss: 0.025 | Tree loss: 1.033 | Accuracy: 0.588000 | 1.151 sec/iter\n",
      "Epoch: 137 | Batch: 008 / 011 | Total loss: 1.002 | Reg loss: 0.025 | Tree loss: 1.002 | Accuracy: 0.607500 | 1.151 sec/iter\n",
      "Epoch: 137 | Batch: 009 / 011 | Total loss: 1.024 | Reg loss: 0.025 | Tree loss: 1.024 | Accuracy: 0.596000 | 1.15 sec/iter\n",
      "Epoch: 137 | Batch: 010 / 011 | Total loss: 0.962 | Reg loss: 0.025 | Tree loss: 0.962 | Accuracy: 0.672355 | 1.15 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 138 | Batch: 000 / 011 | Total loss: 1.119 | Reg loss: 0.025 | Tree loss: 1.119 | Accuracy: 0.641000 | 1.152 sec/iter\n",
      "Epoch: 138 | Batch: 001 / 011 | Total loss: 1.096 | Reg loss: 0.025 | Tree loss: 1.096 | Accuracy: 0.669500 | 1.152 sec/iter\n",
      "Epoch: 138 | Batch: 002 / 011 | Total loss: 1.085 | Reg loss: 0.025 | Tree loss: 1.085 | Accuracy: 0.655000 | 1.152 sec/iter\n",
      "Epoch: 138 | Batch: 003 / 011 | Total loss: 1.052 | Reg loss: 0.025 | Tree loss: 1.052 | Accuracy: 0.664500 | 1.152 sec/iter\n",
      "Epoch: 138 | Batch: 004 / 011 | Total loss: 1.048 | Reg loss: 0.025 | Tree loss: 1.048 | Accuracy: 0.647000 | 1.152 sec/iter\n",
      "Epoch: 138 | Batch: 005 / 011 | Total loss: 1.027 | Reg loss: 0.025 | Tree loss: 1.027 | Accuracy: 0.621500 | 1.151 sec/iter\n",
      "Epoch: 138 | Batch: 006 / 011 | Total loss: 1.041 | Reg loss: 0.025 | Tree loss: 1.041 | Accuracy: 0.606500 | 1.151 sec/iter\n",
      "Epoch: 138 | Batch: 007 / 011 | Total loss: 1.015 | Reg loss: 0.025 | Tree loss: 1.015 | Accuracy: 0.606500 | 1.151 sec/iter\n",
      "Epoch: 138 | Batch: 008 / 011 | Total loss: 1.020 | Reg loss: 0.025 | Tree loss: 1.020 | Accuracy: 0.594500 | 1.151 sec/iter\n",
      "Epoch: 138 | Batch: 009 / 011 | Total loss: 1.016 | Reg loss: 0.025 | Tree loss: 1.016 | Accuracy: 0.581500 | 1.151 sec/iter\n",
      "Epoch: 138 | Batch: 010 / 011 | Total loss: 1.028 | Reg loss: 0.025 | Tree loss: 1.028 | Accuracy: 0.607509 | 1.151 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 139 | Batch: 000 / 011 | Total loss: 1.110 | Reg loss: 0.025 | Tree loss: 1.110 | Accuracy: 0.652000 | 1.152 sec/iter\n",
      "Epoch: 139 | Batch: 001 / 011 | Total loss: 1.114 | Reg loss: 0.025 | Tree loss: 1.114 | Accuracy: 0.654500 | 1.152 sec/iter\n",
      "Epoch: 139 | Batch: 002 / 011 | Total loss: 1.081 | Reg loss: 0.025 | Tree loss: 1.081 | Accuracy: 0.681500 | 1.152 sec/iter\n",
      "Epoch: 139 | Batch: 003 / 011 | Total loss: 1.049 | Reg loss: 0.025 | Tree loss: 1.049 | Accuracy: 0.683000 | 1.152 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 139 | Batch: 004 / 011 | Total loss: 1.038 | Reg loss: 0.025 | Tree loss: 1.038 | Accuracy: 0.652500 | 1.152 sec/iter\n",
      "Epoch: 139 | Batch: 005 / 011 | Total loss: 1.038 | Reg loss: 0.025 | Tree loss: 1.038 | Accuracy: 0.618500 | 1.152 sec/iter\n",
      "Epoch: 139 | Batch: 006 / 011 | Total loss: 1.021 | Reg loss: 0.025 | Tree loss: 1.021 | Accuracy: 0.610500 | 1.152 sec/iter\n",
      "Epoch: 139 | Batch: 007 / 011 | Total loss: 1.006 | Reg loss: 0.025 | Tree loss: 1.006 | Accuracy: 0.608500 | 1.152 sec/iter\n",
      "Epoch: 139 | Batch: 008 / 011 | Total loss: 1.017 | Reg loss: 0.025 | Tree loss: 1.017 | Accuracy: 0.623000 | 1.151 sec/iter\n",
      "Epoch: 139 | Batch: 009 / 011 | Total loss: 1.019 | Reg loss: 0.025 | Tree loss: 1.019 | Accuracy: 0.594500 | 1.151 sec/iter\n",
      "Epoch: 139 | Batch: 010 / 011 | Total loss: 1.015 | Reg loss: 0.025 | Tree loss: 1.015 | Accuracy: 0.651877 | 1.151 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 140 | Batch: 000 / 011 | Total loss: 1.128 | Reg loss: 0.025 | Tree loss: 1.128 | Accuracy: 0.640000 | 1.151 sec/iter\n",
      "Epoch: 140 | Batch: 001 / 011 | Total loss: 1.103 | Reg loss: 0.025 | Tree loss: 1.103 | Accuracy: 0.674500 | 1.151 sec/iter\n",
      "Epoch: 140 | Batch: 002 / 011 | Total loss: 1.081 | Reg loss: 0.025 | Tree loss: 1.081 | Accuracy: 0.684500 | 1.151 sec/iter\n",
      "Epoch: 140 | Batch: 003 / 011 | Total loss: 1.054 | Reg loss: 0.025 | Tree loss: 1.054 | Accuracy: 0.672000 | 1.151 sec/iter\n",
      "Epoch: 140 | Batch: 004 / 011 | Total loss: 1.025 | Reg loss: 0.025 | Tree loss: 1.025 | Accuracy: 0.646000 | 1.151 sec/iter\n",
      "Epoch: 140 | Batch: 005 / 011 | Total loss: 1.054 | Reg loss: 0.025 | Tree loss: 1.054 | Accuracy: 0.613000 | 1.15 sec/iter\n",
      "Epoch: 140 | Batch: 006 / 011 | Total loss: 1.006 | Reg loss: 0.025 | Tree loss: 1.006 | Accuracy: 0.619000 | 1.15 sec/iter\n",
      "Epoch: 140 | Batch: 007 / 011 | Total loss: 1.004 | Reg loss: 0.025 | Tree loss: 1.004 | Accuracy: 0.615000 | 1.15 sec/iter\n",
      "Epoch: 140 | Batch: 008 / 011 | Total loss: 1.000 | Reg loss: 0.025 | Tree loss: 1.000 | Accuracy: 0.602000 | 1.15 sec/iter\n",
      "Epoch: 140 | Batch: 009 / 011 | Total loss: 1.018 | Reg loss: 0.025 | Tree loss: 1.018 | Accuracy: 0.592500 | 1.15 sec/iter\n",
      "Epoch: 140 | Batch: 010 / 011 | Total loss: 0.996 | Reg loss: 0.025 | Tree loss: 0.996 | Accuracy: 0.621160 | 1.15 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 141 | Batch: 000 / 011 | Total loss: 1.122 | Reg loss: 0.025 | Tree loss: 1.122 | Accuracy: 0.638500 | 1.15 sec/iter\n",
      "Epoch: 141 | Batch: 001 / 011 | Total loss: 1.073 | Reg loss: 0.025 | Tree loss: 1.073 | Accuracy: 0.669500 | 1.15 sec/iter\n",
      "Epoch: 141 | Batch: 002 / 011 | Total loss: 1.070 | Reg loss: 0.025 | Tree loss: 1.070 | Accuracy: 0.669500 | 1.15 sec/iter\n",
      "Epoch: 141 | Batch: 003 / 011 | Total loss: 1.058 | Reg loss: 0.025 | Tree loss: 1.058 | Accuracy: 0.664500 | 1.15 sec/iter\n",
      "Epoch: 141 | Batch: 004 / 011 | Total loss: 1.035 | Reg loss: 0.025 | Tree loss: 1.035 | Accuracy: 0.650500 | 1.15 sec/iter\n",
      "Epoch: 141 | Batch: 005 / 011 | Total loss: 1.034 | Reg loss: 0.025 | Tree loss: 1.034 | Accuracy: 0.620500 | 1.15 sec/iter\n",
      "Epoch: 141 | Batch: 006 / 011 | Total loss: 1.027 | Reg loss: 0.025 | Tree loss: 1.027 | Accuracy: 0.609000 | 1.15 sec/iter\n",
      "Epoch: 141 | Batch: 007 / 011 | Total loss: 1.031 | Reg loss: 0.025 | Tree loss: 1.031 | Accuracy: 0.599500 | 1.15 sec/iter\n",
      "Epoch: 141 | Batch: 008 / 011 | Total loss: 1.019 | Reg loss: 0.025 | Tree loss: 1.019 | Accuracy: 0.608000 | 1.15 sec/iter\n",
      "Epoch: 141 | Batch: 009 / 011 | Total loss: 1.011 | Reg loss: 0.025 | Tree loss: 1.011 | Accuracy: 0.598000 | 1.15 sec/iter\n",
      "Epoch: 141 | Batch: 010 / 011 | Total loss: 0.974 | Reg loss: 0.025 | Tree loss: 0.974 | Accuracy: 0.621160 | 1.149 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 142 | Batch: 000 / 011 | Total loss: 1.127 | Reg loss: 0.025 | Tree loss: 1.127 | Accuracy: 0.629500 | 1.151 sec/iter\n",
      "Epoch: 142 | Batch: 001 / 011 | Total loss: 1.103 | Reg loss: 0.025 | Tree loss: 1.103 | Accuracy: 0.662000 | 1.151 sec/iter\n",
      "Epoch: 142 | Batch: 002 / 011 | Total loss: 1.066 | Reg loss: 0.025 | Tree loss: 1.066 | Accuracy: 0.694500 | 1.151 sec/iter\n",
      "Epoch: 142 | Batch: 003 / 011 | Total loss: 1.039 | Reg loss: 0.025 | Tree loss: 1.039 | Accuracy: 0.674000 | 1.151 sec/iter\n",
      "Epoch: 142 | Batch: 004 / 011 | Total loss: 1.022 | Reg loss: 0.025 | Tree loss: 1.022 | Accuracy: 0.650500 | 1.15 sec/iter\n",
      "Epoch: 142 | Batch: 005 / 011 | Total loss: 1.022 | Reg loss: 0.025 | Tree loss: 1.022 | Accuracy: 0.619000 | 1.15 sec/iter\n",
      "Epoch: 142 | Batch: 006 / 011 | Total loss: 1.043 | Reg loss: 0.025 | Tree loss: 1.043 | Accuracy: 0.603500 | 1.15 sec/iter\n",
      "Epoch: 142 | Batch: 007 / 011 | Total loss: 1.036 | Reg loss: 0.025 | Tree loss: 1.036 | Accuracy: 0.589000 | 1.15 sec/iter\n",
      "Epoch: 142 | Batch: 008 / 011 | Total loss: 1.008 | Reg loss: 0.025 | Tree loss: 1.008 | Accuracy: 0.599500 | 1.15 sec/iter\n",
      "Epoch: 142 | Batch: 009 / 011 | Total loss: 0.988 | Reg loss: 0.025 | Tree loss: 0.988 | Accuracy: 0.626500 | 1.15 sec/iter\n",
      "Epoch: 142 | Batch: 010 / 011 | Total loss: 1.000 | Reg loss: 0.025 | Tree loss: 1.000 | Accuracy: 0.614334 | 1.15 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 143 | Batch: 000 / 011 | Total loss: 1.111 | Reg loss: 0.025 | Tree loss: 1.111 | Accuracy: 0.648500 | 1.151 sec/iter\n",
      "Epoch: 143 | Batch: 001 / 011 | Total loss: 1.100 | Reg loss: 0.025 | Tree loss: 1.100 | Accuracy: 0.654000 | 1.151 sec/iter\n",
      "Epoch: 143 | Batch: 002 / 011 | Total loss: 1.078 | Reg loss: 0.025 | Tree loss: 1.078 | Accuracy: 0.675000 | 1.151 sec/iter\n",
      "Epoch: 143 | Batch: 003 / 011 | Total loss: 1.053 | Reg loss: 0.025 | Tree loss: 1.053 | Accuracy: 0.691000 | 1.151 sec/iter\n",
      "Epoch: 143 | Batch: 004 / 011 | Total loss: 1.023 | Reg loss: 0.025 | Tree loss: 1.023 | Accuracy: 0.652500 | 1.151 sec/iter\n",
      "Epoch: 143 | Batch: 005 / 011 | Total loss: 1.040 | Reg loss: 0.025 | Tree loss: 1.040 | Accuracy: 0.625000 | 1.151 sec/iter\n",
      "Epoch: 143 | Batch: 006 / 011 | Total loss: 1.033 | Reg loss: 0.025 | Tree loss: 1.033 | Accuracy: 0.616500 | 1.151 sec/iter\n",
      "Epoch: 143 | Batch: 007 / 011 | Total loss: 1.012 | Reg loss: 0.025 | Tree loss: 1.012 | Accuracy: 0.615000 | 1.15 sec/iter\n",
      "Epoch: 143 | Batch: 008 / 011 | Total loss: 0.995 | Reg loss: 0.025 | Tree loss: 0.995 | Accuracy: 0.624500 | 1.15 sec/iter\n",
      "Epoch: 143 | Batch: 009 / 011 | Total loss: 1.014 | Reg loss: 0.025 | Tree loss: 1.014 | Accuracy: 0.612500 | 1.15 sec/iter\n",
      "Epoch: 143 | Batch: 010 / 011 | Total loss: 1.003 | Reg loss: 0.025 | Tree loss: 1.003 | Accuracy: 0.627986 | 1.15 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 144 | Batch: 000 / 011 | Total loss: 1.094 | Reg loss: 0.025 | Tree loss: 1.094 | Accuracy: 0.645500 | 1.152 sec/iter\n",
      "Epoch: 144 | Batch: 001 / 011 | Total loss: 1.098 | Reg loss: 0.025 | Tree loss: 1.098 | Accuracy: 0.666000 | 1.152 sec/iter\n",
      "Epoch: 144 | Batch: 002 / 011 | Total loss: 1.063 | Reg loss: 0.025 | Tree loss: 1.063 | Accuracy: 0.696000 | 1.152 sec/iter\n",
      "Epoch: 144 | Batch: 003 / 011 | Total loss: 1.052 | Reg loss: 0.025 | Tree loss: 1.052 | Accuracy: 0.682500 | 1.151 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 144 | Batch: 004 / 011 | Total loss: 1.055 | Reg loss: 0.025 | Tree loss: 1.055 | Accuracy: 0.624500 | 1.151 sec/iter\n",
      "Epoch: 144 | Batch: 005 / 011 | Total loss: 1.036 | Reg loss: 0.025 | Tree loss: 1.036 | Accuracy: 0.626500 | 1.151 sec/iter\n",
      "Epoch: 144 | Batch: 006 / 011 | Total loss: 1.011 | Reg loss: 0.025 | Tree loss: 1.011 | Accuracy: 0.633000 | 1.151 sec/iter\n",
      "Epoch: 144 | Batch: 007 / 011 | Total loss: 1.027 | Reg loss: 0.025 | Tree loss: 1.027 | Accuracy: 0.599000 | 1.151 sec/iter\n",
      "Epoch: 144 | Batch: 008 / 011 | Total loss: 1.016 | Reg loss: 0.025 | Tree loss: 1.016 | Accuracy: 0.609000 | 1.151 sec/iter\n",
      "Epoch: 144 | Batch: 009 / 011 | Total loss: 0.993 | Reg loss: 0.025 | Tree loss: 0.993 | Accuracy: 0.617000 | 1.151 sec/iter\n",
      "Epoch: 144 | Batch: 010 / 011 | Total loss: 1.007 | Reg loss: 0.025 | Tree loss: 1.007 | Accuracy: 0.614334 | 1.151 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 145 | Batch: 000 / 011 | Total loss: 1.113 | Reg loss: 0.025 | Tree loss: 1.113 | Accuracy: 0.631500 | 1.152 sec/iter\n",
      "Epoch: 145 | Batch: 001 / 011 | Total loss: 1.103 | Reg loss: 0.025 | Tree loss: 1.103 | Accuracy: 0.646000 | 1.152 sec/iter\n",
      "Epoch: 145 | Batch: 002 / 011 | Total loss: 1.070 | Reg loss: 0.025 | Tree loss: 1.070 | Accuracy: 0.678000 | 1.152 sec/iter\n",
      "Epoch: 145 | Batch: 003 / 011 | Total loss: 1.050 | Reg loss: 0.025 | Tree loss: 1.050 | Accuracy: 0.664500 | 1.152 sec/iter\n",
      "Epoch: 145 | Batch: 004 / 011 | Total loss: 1.021 | Reg loss: 0.025 | Tree loss: 1.021 | Accuracy: 0.658000 | 1.152 sec/iter\n",
      "Epoch: 145 | Batch: 005 / 011 | Total loss: 1.046 | Reg loss: 0.025 | Tree loss: 1.046 | Accuracy: 0.621000 | 1.152 sec/iter\n",
      "Epoch: 145 | Batch: 006 / 011 | Total loss: 1.026 | Reg loss: 0.025 | Tree loss: 1.026 | Accuracy: 0.617000 | 1.152 sec/iter\n",
      "Epoch: 145 | Batch: 007 / 011 | Total loss: 1.009 | Reg loss: 0.025 | Tree loss: 1.009 | Accuracy: 0.610500 | 1.152 sec/iter\n",
      "Epoch: 145 | Batch: 008 / 011 | Total loss: 1.023 | Reg loss: 0.025 | Tree loss: 1.023 | Accuracy: 0.592000 | 1.151 sec/iter\n",
      "Epoch: 145 | Batch: 009 / 011 | Total loss: 1.010 | Reg loss: 0.025 | Tree loss: 1.010 | Accuracy: 0.604500 | 1.151 sec/iter\n",
      "Epoch: 145 | Batch: 010 / 011 | Total loss: 0.954 | Reg loss: 0.025 | Tree loss: 0.954 | Accuracy: 0.648464 | 1.151 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 146 | Batch: 000 / 011 | Total loss: 1.109 | Reg loss: 0.025 | Tree loss: 1.109 | Accuracy: 0.632500 | 1.151 sec/iter\n",
      "Epoch: 146 | Batch: 001 / 011 | Total loss: 1.094 | Reg loss: 0.025 | Tree loss: 1.094 | Accuracy: 0.663000 | 1.151 sec/iter\n",
      "Epoch: 146 | Batch: 002 / 011 | Total loss: 1.086 | Reg loss: 0.025 | Tree loss: 1.086 | Accuracy: 0.695500 | 1.151 sec/iter\n",
      "Epoch: 146 | Batch: 003 / 011 | Total loss: 1.054 | Reg loss: 0.025 | Tree loss: 1.054 | Accuracy: 0.675000 | 1.151 sec/iter\n",
      "Epoch: 146 | Batch: 004 / 011 | Total loss: 1.025 | Reg loss: 0.025 | Tree loss: 1.025 | Accuracy: 0.668500 | 1.151 sec/iter\n",
      "Epoch: 146 | Batch: 005 / 011 | Total loss: 1.016 | Reg loss: 0.025 | Tree loss: 1.016 | Accuracy: 0.650000 | 1.151 sec/iter\n",
      "Epoch: 146 | Batch: 006 / 011 | Total loss: 1.013 | Reg loss: 0.025 | Tree loss: 1.013 | Accuracy: 0.623000 | 1.151 sec/iter\n",
      "Epoch: 146 | Batch: 007 / 011 | Total loss: 1.014 | Reg loss: 0.025 | Tree loss: 1.014 | Accuracy: 0.620000 | 1.151 sec/iter\n",
      "Epoch: 146 | Batch: 008 / 011 | Total loss: 1.010 | Reg loss: 0.025 | Tree loss: 1.010 | Accuracy: 0.607000 | 1.15 sec/iter\n",
      "Epoch: 146 | Batch: 009 / 011 | Total loss: 1.006 | Reg loss: 0.025 | Tree loss: 1.006 | Accuracy: 0.626000 | 1.15 sec/iter\n",
      "Epoch: 146 | Batch: 010 / 011 | Total loss: 1.021 | Reg loss: 0.025 | Tree loss: 1.021 | Accuracy: 0.597270 | 1.15 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 147 | Batch: 000 / 011 | Total loss: 1.103 | Reg loss: 0.025 | Tree loss: 1.103 | Accuracy: 0.647000 | 1.151 sec/iter\n",
      "Epoch: 147 | Batch: 001 / 011 | Total loss: 1.087 | Reg loss: 0.025 | Tree loss: 1.087 | Accuracy: 0.674000 | 1.151 sec/iter\n",
      "Epoch: 147 | Batch: 002 / 011 | Total loss: 1.061 | Reg loss: 0.025 | Tree loss: 1.061 | Accuracy: 0.684500 | 1.151 sec/iter\n",
      "Epoch: 147 | Batch: 003 / 011 | Total loss: 1.057 | Reg loss: 0.025 | Tree loss: 1.057 | Accuracy: 0.668000 | 1.15 sec/iter\n",
      "Epoch: 147 | Batch: 004 / 011 | Total loss: 1.023 | Reg loss: 0.025 | Tree loss: 1.023 | Accuracy: 0.645500 | 1.15 sec/iter\n",
      "Epoch: 147 | Batch: 005 / 011 | Total loss: 1.036 | Reg loss: 0.025 | Tree loss: 1.036 | Accuracy: 0.620000 | 1.15 sec/iter\n",
      "Epoch: 147 | Batch: 006 / 011 | Total loss: 1.033 | Reg loss: 0.025 | Tree loss: 1.033 | Accuracy: 0.603500 | 1.15 sec/iter\n",
      "Epoch: 147 | Batch: 007 / 011 | Total loss: 1.009 | Reg loss: 0.025 | Tree loss: 1.009 | Accuracy: 0.612500 | 1.15 sec/iter\n",
      "Epoch: 147 | Batch: 008 / 011 | Total loss: 1.006 | Reg loss: 0.025 | Tree loss: 1.006 | Accuracy: 0.620000 | 1.15 sec/iter\n",
      "Epoch: 147 | Batch: 009 / 011 | Total loss: 1.000 | Reg loss: 0.025 | Tree loss: 1.000 | Accuracy: 0.615500 | 1.15 sec/iter\n",
      "Epoch: 147 | Batch: 010 / 011 | Total loss: 1.011 | Reg loss: 0.025 | Tree loss: 1.011 | Accuracy: 0.604096 | 1.15 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 148 | Batch: 000 / 011 | Total loss: 1.098 | Reg loss: 0.025 | Tree loss: 1.098 | Accuracy: 0.647500 | 1.151 sec/iter\n",
      "Epoch: 148 | Batch: 001 / 011 | Total loss: 1.126 | Reg loss: 0.025 | Tree loss: 1.126 | Accuracy: 0.632500 | 1.151 sec/iter\n",
      "Epoch: 148 | Batch: 002 / 011 | Total loss: 1.059 | Reg loss: 0.025 | Tree loss: 1.059 | Accuracy: 0.695000 | 1.15 sec/iter\n",
      "Epoch: 148 | Batch: 003 / 011 | Total loss: 1.045 | Reg loss: 0.025 | Tree loss: 1.045 | Accuracy: 0.675500 | 1.15 sec/iter\n",
      "Epoch: 148 | Batch: 004 / 011 | Total loss: 1.039 | Reg loss: 0.025 | Tree loss: 1.039 | Accuracy: 0.666500 | 1.15 sec/iter\n",
      "Epoch: 148 | Batch: 005 / 011 | Total loss: 1.025 | Reg loss: 0.025 | Tree loss: 1.025 | Accuracy: 0.631000 | 1.15 sec/iter\n",
      "Epoch: 148 | Batch: 006 / 011 | Total loss: 1.010 | Reg loss: 0.025 | Tree loss: 1.010 | Accuracy: 0.624000 | 1.15 sec/iter\n",
      "Epoch: 148 | Batch: 007 / 011 | Total loss: 1.036 | Reg loss: 0.025 | Tree loss: 1.036 | Accuracy: 0.584500 | 1.15 sec/iter\n",
      "Epoch: 148 | Batch: 008 / 011 | Total loss: 0.994 | Reg loss: 0.025 | Tree loss: 0.994 | Accuracy: 0.618500 | 1.15 sec/iter\n",
      "Epoch: 148 | Batch: 009 / 011 | Total loss: 0.981 | Reg loss: 0.025 | Tree loss: 0.981 | Accuracy: 0.627000 | 1.15 sec/iter\n",
      "Epoch: 148 | Batch: 010 / 011 | Total loss: 0.962 | Reg loss: 0.025 | Tree loss: 0.962 | Accuracy: 0.624573 | 1.15 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 149 | Batch: 000 / 011 | Total loss: 1.100 | Reg loss: 0.025 | Tree loss: 1.100 | Accuracy: 0.653000 | 1.151 sec/iter\n",
      "Epoch: 149 | Batch: 001 / 011 | Total loss: 1.088 | Reg loss: 0.025 | Tree loss: 1.088 | Accuracy: 0.657000 | 1.151 sec/iter\n",
      "Epoch: 149 | Batch: 002 / 011 | Total loss: 1.082 | Reg loss: 0.025 | Tree loss: 1.082 | Accuracy: 0.658500 | 1.151 sec/iter\n",
      "Epoch: 149 | Batch: 003 / 011 | Total loss: 1.045 | Reg loss: 0.025 | Tree loss: 1.045 | Accuracy: 0.682500 | 1.151 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 149 | Batch: 004 / 011 | Total loss: 1.021 | Reg loss: 0.025 | Tree loss: 1.021 | Accuracy: 0.647000 | 1.151 sec/iter\n",
      "Epoch: 149 | Batch: 005 / 011 | Total loss: 1.016 | Reg loss: 0.025 | Tree loss: 1.016 | Accuracy: 0.645500 | 1.151 sec/iter\n",
      "Epoch: 149 | Batch: 006 / 011 | Total loss: 1.012 | Reg loss: 0.025 | Tree loss: 1.012 | Accuracy: 0.621500 | 1.15 sec/iter\n",
      "Epoch: 149 | Batch: 007 / 011 | Total loss: 1.021 | Reg loss: 0.025 | Tree loss: 1.021 | Accuracy: 0.615000 | 1.15 sec/iter\n",
      "Epoch: 149 | Batch: 008 / 011 | Total loss: 1.008 | Reg loss: 0.025 | Tree loss: 1.008 | Accuracy: 0.604000 | 1.15 sec/iter\n",
      "Epoch: 149 | Batch: 009 / 011 | Total loss: 1.003 | Reg loss: 0.025 | Tree loss: 1.003 | Accuracy: 0.606000 | 1.15 sec/iter\n",
      "Epoch: 149 | Batch: 010 / 011 | Total loss: 0.990 | Reg loss: 0.025 | Tree loss: 0.990 | Accuracy: 0.624573 | 1.15 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 150 | Batch: 000 / 011 | Total loss: 1.115 | Reg loss: 0.025 | Tree loss: 1.115 | Accuracy: 0.624000 | 1.152 sec/iter\n",
      "Epoch: 150 | Batch: 001 / 011 | Total loss: 1.093 | Reg loss: 0.025 | Tree loss: 1.093 | Accuracy: 0.657500 | 1.151 sec/iter\n",
      "Epoch: 150 | Batch: 002 / 011 | Total loss: 1.052 | Reg loss: 0.025 | Tree loss: 1.052 | Accuracy: 0.697500 | 1.151 sec/iter\n",
      "Epoch: 150 | Batch: 003 / 011 | Total loss: 1.027 | Reg loss: 0.025 | Tree loss: 1.027 | Accuracy: 0.680000 | 1.151 sec/iter\n",
      "Epoch: 150 | Batch: 004 / 011 | Total loss: 1.043 | Reg loss: 0.025 | Tree loss: 1.043 | Accuracy: 0.659000 | 1.151 sec/iter\n",
      "Epoch: 150 | Batch: 005 / 011 | Total loss: 1.002 | Reg loss: 0.025 | Tree loss: 1.002 | Accuracy: 0.634500 | 1.151 sec/iter\n",
      "Epoch: 150 | Batch: 006 / 011 | Total loss: 1.015 | Reg loss: 0.025 | Tree loss: 1.015 | Accuracy: 0.615000 | 1.151 sec/iter\n",
      "Epoch: 150 | Batch: 007 / 011 | Total loss: 1.002 | Reg loss: 0.025 | Tree loss: 1.002 | Accuracy: 0.626500 | 1.151 sec/iter\n",
      "Epoch: 150 | Batch: 008 / 011 | Total loss: 1.024 | Reg loss: 0.025 | Tree loss: 1.024 | Accuracy: 0.591000 | 1.151 sec/iter\n",
      "Epoch: 150 | Batch: 009 / 011 | Total loss: 1.016 | Reg loss: 0.025 | Tree loss: 1.016 | Accuracy: 0.604500 | 1.151 sec/iter\n",
      "Epoch: 150 | Batch: 010 / 011 | Total loss: 0.995 | Reg loss: 0.025 | Tree loss: 0.995 | Accuracy: 0.651877 | 1.151 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 151 | Batch: 000 / 011 | Total loss: 1.098 | Reg loss: 0.025 | Tree loss: 1.098 | Accuracy: 0.648500 | 1.152 sec/iter\n",
      "Epoch: 151 | Batch: 001 / 011 | Total loss: 1.078 | Reg loss: 0.025 | Tree loss: 1.078 | Accuracy: 0.657500 | 1.152 sec/iter\n",
      "Epoch: 151 | Batch: 002 / 011 | Total loss: 1.073 | Reg loss: 0.025 | Tree loss: 1.073 | Accuracy: 0.677500 | 1.152 sec/iter\n",
      "Epoch: 151 | Batch: 003 / 011 | Total loss: 1.046 | Reg loss: 0.025 | Tree loss: 1.046 | Accuracy: 0.688000 | 1.152 sec/iter\n",
      "Epoch: 151 | Batch: 004 / 011 | Total loss: 1.016 | Reg loss: 0.025 | Tree loss: 1.016 | Accuracy: 0.680000 | 1.152 sec/iter\n",
      "Epoch: 151 | Batch: 005 / 011 | Total loss: 0.997 | Reg loss: 0.025 | Tree loss: 0.997 | Accuracy: 0.659500 | 1.152 sec/iter\n",
      "Epoch: 151 | Batch: 006 / 011 | Total loss: 1.009 | Reg loss: 0.025 | Tree loss: 1.009 | Accuracy: 0.623000 | 1.152 sec/iter\n",
      "Epoch: 151 | Batch: 007 / 011 | Total loss: 1.024 | Reg loss: 0.025 | Tree loss: 1.024 | Accuracy: 0.605000 | 1.151 sec/iter\n",
      "Epoch: 151 | Batch: 008 / 011 | Total loss: 1.010 | Reg loss: 0.025 | Tree loss: 1.010 | Accuracy: 0.606000 | 1.151 sec/iter\n",
      "Epoch: 151 | Batch: 009 / 011 | Total loss: 1.019 | Reg loss: 0.025 | Tree loss: 1.019 | Accuracy: 0.598000 | 1.151 sec/iter\n",
      "Epoch: 151 | Batch: 010 / 011 | Total loss: 1.037 | Reg loss: 0.025 | Tree loss: 1.037 | Accuracy: 0.604096 | 1.151 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 152 | Batch: 000 / 011 | Total loss: 1.105 | Reg loss: 0.025 | Tree loss: 1.105 | Accuracy: 0.637000 | 1.151 sec/iter\n",
      "Epoch: 152 | Batch: 001 / 011 | Total loss: 1.073 | Reg loss: 0.025 | Tree loss: 1.073 | Accuracy: 0.650500 | 1.151 sec/iter\n",
      "Epoch: 152 | Batch: 002 / 011 | Total loss: 1.072 | Reg loss: 0.025 | Tree loss: 1.072 | Accuracy: 0.686500 | 1.151 sec/iter\n",
      "Epoch: 152 | Batch: 003 / 011 | Total loss: 1.018 | Reg loss: 0.025 | Tree loss: 1.018 | Accuracy: 0.681500 | 1.151 sec/iter\n",
      "Epoch: 152 | Batch: 004 / 011 | Total loss: 1.025 | Reg loss: 0.025 | Tree loss: 1.025 | Accuracy: 0.665500 | 1.15 sec/iter\n",
      "Epoch: 152 | Batch: 005 / 011 | Total loss: 1.028 | Reg loss: 0.025 | Tree loss: 1.028 | Accuracy: 0.637500 | 1.15 sec/iter\n",
      "Epoch: 152 | Batch: 006 / 011 | Total loss: 1.011 | Reg loss: 0.025 | Tree loss: 1.011 | Accuracy: 0.627500 | 1.15 sec/iter\n",
      "Epoch: 152 | Batch: 007 / 011 | Total loss: 1.036 | Reg loss: 0.025 | Tree loss: 1.036 | Accuracy: 0.594000 | 1.15 sec/iter\n",
      "Epoch: 152 | Batch: 008 / 011 | Total loss: 0.985 | Reg loss: 0.025 | Tree loss: 0.985 | Accuracy: 0.620500 | 1.15 sec/iter\n",
      "Epoch: 152 | Batch: 009 / 011 | Total loss: 1.030 | Reg loss: 0.025 | Tree loss: 1.030 | Accuracy: 0.585500 | 1.15 sec/iter\n",
      "Epoch: 152 | Batch: 010 / 011 | Total loss: 1.041 | Reg loss: 0.025 | Tree loss: 1.041 | Accuracy: 0.593857 | 1.15 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 153 | Batch: 000 / 011 | Total loss: 1.107 | Reg loss: 0.025 | Tree loss: 1.107 | Accuracy: 0.631500 | 1.15 sec/iter\n",
      "Epoch: 153 | Batch: 001 / 011 | Total loss: 1.097 | Reg loss: 0.025 | Tree loss: 1.097 | Accuracy: 0.644500 | 1.15 sec/iter\n",
      "Epoch: 153 | Batch: 002 / 011 | Total loss: 1.074 | Reg loss: 0.025 | Tree loss: 1.074 | Accuracy: 0.667500 | 1.15 sec/iter\n",
      "Epoch: 153 | Batch: 003 / 011 | Total loss: 1.033 | Reg loss: 0.025 | Tree loss: 1.033 | Accuracy: 0.699000 | 1.15 sec/iter\n",
      "Epoch: 153 | Batch: 004 / 011 | Total loss: 1.000 | Reg loss: 0.025 | Tree loss: 1.000 | Accuracy: 0.688500 | 1.15 sec/iter\n",
      "Epoch: 153 | Batch: 005 / 011 | Total loss: 1.018 | Reg loss: 0.025 | Tree loss: 1.018 | Accuracy: 0.636500 | 1.15 sec/iter\n",
      "Epoch: 153 | Batch: 006 / 011 | Total loss: 1.002 | Reg loss: 0.025 | Tree loss: 1.002 | Accuracy: 0.624000 | 1.15 sec/iter\n",
      "Epoch: 153 | Batch: 007 / 011 | Total loss: 1.019 | Reg loss: 0.025 | Tree loss: 1.019 | Accuracy: 0.597000 | 1.15 sec/iter\n",
      "Epoch: 153 | Batch: 008 / 011 | Total loss: 0.997 | Reg loss: 0.025 | Tree loss: 0.997 | Accuracy: 0.613000 | 1.15 sec/iter\n",
      "Epoch: 153 | Batch: 009 / 011 | Total loss: 1.006 | Reg loss: 0.025 | Tree loss: 1.006 | Accuracy: 0.612000 | 1.15 sec/iter\n",
      "Epoch: 153 | Batch: 010 / 011 | Total loss: 1.021 | Reg loss: 0.025 | Tree loss: 1.021 | Accuracy: 0.587031 | 1.149 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 154 | Batch: 000 / 011 | Total loss: 1.103 | Reg loss: 0.025 | Tree loss: 1.103 | Accuracy: 0.629000 | 1.151 sec/iter\n",
      "Epoch: 154 | Batch: 001 / 011 | Total loss: 1.089 | Reg loss: 0.025 | Tree loss: 1.089 | Accuracy: 0.657000 | 1.151 sec/iter\n",
      "Epoch: 154 | Batch: 002 / 011 | Total loss: 1.058 | Reg loss: 0.025 | Tree loss: 1.058 | Accuracy: 0.686000 | 1.15 sec/iter\n",
      "Epoch: 154 | Batch: 003 / 011 | Total loss: 1.031 | Reg loss: 0.025 | Tree loss: 1.031 | Accuracy: 0.688500 | 1.15 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 154 | Batch: 004 / 011 | Total loss: 1.032 | Reg loss: 0.025 | Tree loss: 1.032 | Accuracy: 0.649000 | 1.15 sec/iter\n",
      "Epoch: 154 | Batch: 005 / 011 | Total loss: 1.022 | Reg loss: 0.025 | Tree loss: 1.022 | Accuracy: 0.637500 | 1.15 sec/iter\n",
      "Epoch: 154 | Batch: 006 / 011 | Total loss: 0.998 | Reg loss: 0.025 | Tree loss: 0.998 | Accuracy: 0.619000 | 1.15 sec/iter\n",
      "Epoch: 154 | Batch: 007 / 011 | Total loss: 1.026 | Reg loss: 0.025 | Tree loss: 1.026 | Accuracy: 0.609500 | 1.15 sec/iter\n",
      "Epoch: 154 | Batch: 008 / 011 | Total loss: 1.013 | Reg loss: 0.025 | Tree loss: 1.013 | Accuracy: 0.584500 | 1.15 sec/iter\n",
      "Epoch: 154 | Batch: 009 / 011 | Total loss: 0.968 | Reg loss: 0.025 | Tree loss: 0.968 | Accuracy: 0.640000 | 1.15 sec/iter\n",
      "Epoch: 154 | Batch: 010 / 011 | Total loss: 1.008 | Reg loss: 0.025 | Tree loss: 1.008 | Accuracy: 0.600683 | 1.15 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 155 | Batch: 000 / 011 | Total loss: 1.100 | Reg loss: 0.025 | Tree loss: 1.100 | Accuracy: 0.639000 | 1.151 sec/iter\n",
      "Epoch: 155 | Batch: 001 / 011 | Total loss: 1.085 | Reg loss: 0.025 | Tree loss: 1.085 | Accuracy: 0.646000 | 1.151 sec/iter\n",
      "Epoch: 155 | Batch: 002 / 011 | Total loss: 1.044 | Reg loss: 0.025 | Tree loss: 1.044 | Accuracy: 0.694500 | 1.151 sec/iter\n",
      "Epoch: 155 | Batch: 003 / 011 | Total loss: 1.039 | Reg loss: 0.025 | Tree loss: 1.039 | Accuracy: 0.683000 | 1.151 sec/iter\n",
      "Epoch: 155 | Batch: 004 / 011 | Total loss: 1.015 | Reg loss: 0.025 | Tree loss: 1.015 | Accuracy: 0.664500 | 1.151 sec/iter\n",
      "Epoch: 155 | Batch: 005 / 011 | Total loss: 1.022 | Reg loss: 0.025 | Tree loss: 1.022 | Accuracy: 0.624500 | 1.151 sec/iter\n",
      "Epoch: 155 | Batch: 006 / 011 | Total loss: 1.027 | Reg loss: 0.025 | Tree loss: 1.027 | Accuracy: 0.621500 | 1.151 sec/iter\n",
      "Epoch: 155 | Batch: 007 / 011 | Total loss: 0.999 | Reg loss: 0.025 | Tree loss: 0.999 | Accuracy: 0.616500 | 1.15 sec/iter\n",
      "Epoch: 155 | Batch: 008 / 011 | Total loss: 0.992 | Reg loss: 0.025 | Tree loss: 0.992 | Accuracy: 0.630500 | 1.15 sec/iter\n",
      "Epoch: 155 | Batch: 009 / 011 | Total loss: 1.023 | Reg loss: 0.025 | Tree loss: 1.023 | Accuracy: 0.601500 | 1.15 sec/iter\n",
      "Epoch: 155 | Batch: 010 / 011 | Total loss: 1.011 | Reg loss: 0.025 | Tree loss: 1.011 | Accuracy: 0.627986 | 1.15 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 156 | Batch: 000 / 011 | Total loss: 1.089 | Reg loss: 0.025 | Tree loss: 1.089 | Accuracy: 0.633500 | 1.151 sec/iter\n",
      "Epoch: 156 | Batch: 001 / 011 | Total loss: 1.070 | Reg loss: 0.025 | Tree loss: 1.070 | Accuracy: 0.668500 | 1.151 sec/iter\n",
      "Epoch: 156 | Batch: 002 / 011 | Total loss: 1.052 | Reg loss: 0.025 | Tree loss: 1.052 | Accuracy: 0.684000 | 1.151 sec/iter\n",
      "Epoch: 156 | Batch: 003 / 011 | Total loss: 1.062 | Reg loss: 0.025 | Tree loss: 1.062 | Accuracy: 0.676000 | 1.151 sec/iter\n",
      "Epoch: 156 | Batch: 004 / 011 | Total loss: 1.008 | Reg loss: 0.025 | Tree loss: 1.008 | Accuracy: 0.679500 | 1.151 sec/iter\n",
      "Epoch: 156 | Batch: 005 / 011 | Total loss: 1.018 | Reg loss: 0.025 | Tree loss: 1.018 | Accuracy: 0.635500 | 1.151 sec/iter\n",
      "Epoch: 156 | Batch: 006 / 011 | Total loss: 1.004 | Reg loss: 0.025 | Tree loss: 1.004 | Accuracy: 0.629000 | 1.151 sec/iter\n",
      "Epoch: 156 | Batch: 007 / 011 | Total loss: 1.035 | Reg loss: 0.025 | Tree loss: 1.035 | Accuracy: 0.587000 | 1.151 sec/iter\n",
      "Epoch: 156 | Batch: 008 / 011 | Total loss: 0.988 | Reg loss: 0.025 | Tree loss: 0.988 | Accuracy: 0.612500 | 1.151 sec/iter\n",
      "Epoch: 156 | Batch: 009 / 011 | Total loss: 1.010 | Reg loss: 0.025 | Tree loss: 1.010 | Accuracy: 0.608500 | 1.151 sec/iter\n",
      "Epoch: 156 | Batch: 010 / 011 | Total loss: 0.992 | Reg loss: 0.025 | Tree loss: 0.992 | Accuracy: 0.610922 | 1.151 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 157 | Batch: 000 / 011 | Total loss: 1.112 | Reg loss: 0.025 | Tree loss: 1.112 | Accuracy: 0.630000 | 1.152 sec/iter\n",
      "Epoch: 157 | Batch: 001 / 011 | Total loss: 1.083 | Reg loss: 0.025 | Tree loss: 1.083 | Accuracy: 0.646500 | 1.152 sec/iter\n",
      "Epoch: 157 | Batch: 002 / 011 | Total loss: 1.069 | Reg loss: 0.025 | Tree loss: 1.069 | Accuracy: 0.681500 | 1.152 sec/iter\n",
      "Epoch: 157 | Batch: 003 / 011 | Total loss: 1.034 | Reg loss: 0.025 | Tree loss: 1.034 | Accuracy: 0.691500 | 1.152 sec/iter\n",
      "Epoch: 157 | Batch: 004 / 011 | Total loss: 1.024 | Reg loss: 0.025 | Tree loss: 1.024 | Accuracy: 0.664000 | 1.152 sec/iter\n",
      "Epoch: 157 | Batch: 005 / 011 | Total loss: 1.011 | Reg loss: 0.025 | Tree loss: 1.011 | Accuracy: 0.640500 | 1.152 sec/iter\n",
      "Epoch: 157 | Batch: 006 / 011 | Total loss: 1.034 | Reg loss: 0.025 | Tree loss: 1.034 | Accuracy: 0.604000 | 1.152 sec/iter\n",
      "Epoch: 157 | Batch: 007 / 011 | Total loss: 0.989 | Reg loss: 0.025 | Tree loss: 0.989 | Accuracy: 0.615500 | 1.151 sec/iter\n",
      "Epoch: 157 | Batch: 008 / 011 | Total loss: 1.003 | Reg loss: 0.025 | Tree loss: 1.003 | Accuracy: 0.614000 | 1.151 sec/iter\n",
      "Epoch: 157 | Batch: 009 / 011 | Total loss: 0.992 | Reg loss: 0.025 | Tree loss: 0.992 | Accuracy: 0.622000 | 1.151 sec/iter\n",
      "Epoch: 157 | Batch: 010 / 011 | Total loss: 0.961 | Reg loss: 0.025 | Tree loss: 0.961 | Accuracy: 0.641638 | 1.151 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 158 | Batch: 000 / 011 | Total loss: 1.101 | Reg loss: 0.025 | Tree loss: 1.101 | Accuracy: 0.632000 | 1.151 sec/iter\n",
      "Epoch: 158 | Batch: 001 / 011 | Total loss: 1.088 | Reg loss: 0.025 | Tree loss: 1.088 | Accuracy: 0.639000 | 1.151 sec/iter\n",
      "Epoch: 158 | Batch: 002 / 011 | Total loss: 1.050 | Reg loss: 0.025 | Tree loss: 1.050 | Accuracy: 0.683000 | 1.151 sec/iter\n",
      "Epoch: 158 | Batch: 003 / 011 | Total loss: 1.033 | Reg loss: 0.025 | Tree loss: 1.033 | Accuracy: 0.686500 | 1.151 sec/iter\n",
      "Epoch: 158 | Batch: 004 / 011 | Total loss: 1.039 | Reg loss: 0.025 | Tree loss: 1.039 | Accuracy: 0.645000 | 1.151 sec/iter\n",
      "Epoch: 158 | Batch: 005 / 011 | Total loss: 1.006 | Reg loss: 0.025 | Tree loss: 1.006 | Accuracy: 0.642500 | 1.15 sec/iter\n",
      "Epoch: 158 | Batch: 006 / 011 | Total loss: 1.006 | Reg loss: 0.025 | Tree loss: 1.006 | Accuracy: 0.629000 | 1.15 sec/iter\n",
      "Epoch: 158 | Batch: 007 / 011 | Total loss: 1.028 | Reg loss: 0.025 | Tree loss: 1.028 | Accuracy: 0.615500 | 1.15 sec/iter\n",
      "Epoch: 158 | Batch: 008 / 011 | Total loss: 0.961 | Reg loss: 0.025 | Tree loss: 0.961 | Accuracy: 0.645500 | 1.15 sec/iter\n",
      "Epoch: 158 | Batch: 009 / 011 | Total loss: 1.010 | Reg loss: 0.025 | Tree loss: 1.010 | Accuracy: 0.615500 | 1.15 sec/iter\n",
      "Epoch: 158 | Batch: 010 / 011 | Total loss: 0.992 | Reg loss: 0.025 | Tree loss: 0.992 | Accuracy: 0.621160 | 1.15 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 159 | Batch: 000 / 011 | Total loss: 1.090 | Reg loss: 0.025 | Tree loss: 1.090 | Accuracy: 0.628500 | 1.15 sec/iter\n",
      "Epoch: 159 | Batch: 001 / 011 | Total loss: 1.074 | Reg loss: 0.025 | Tree loss: 1.074 | Accuracy: 0.663500 | 1.15 sec/iter\n",
      "Epoch: 159 | Batch: 002 / 011 | Total loss: 1.062 | Reg loss: 0.025 | Tree loss: 1.062 | Accuracy: 0.673500 | 1.15 sec/iter\n",
      "Epoch: 159 | Batch: 003 / 011 | Total loss: 1.031 | Reg loss: 0.025 | Tree loss: 1.031 | Accuracy: 0.692000 | 1.15 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 159 | Batch: 004 / 011 | Total loss: 1.001 | Reg loss: 0.025 | Tree loss: 1.001 | Accuracy: 0.690000 | 1.15 sec/iter\n",
      "Epoch: 159 | Batch: 005 / 011 | Total loss: 1.005 | Reg loss: 0.025 | Tree loss: 1.005 | Accuracy: 0.632500 | 1.15 sec/iter\n",
      "Epoch: 159 | Batch: 006 / 011 | Total loss: 1.007 | Reg loss: 0.025 | Tree loss: 1.007 | Accuracy: 0.622500 | 1.15 sec/iter\n",
      "Epoch: 159 | Batch: 007 / 011 | Total loss: 1.033 | Reg loss: 0.025 | Tree loss: 1.033 | Accuracy: 0.600000 | 1.15 sec/iter\n",
      "Epoch: 159 | Batch: 008 / 011 | Total loss: 1.009 | Reg loss: 0.025 | Tree loss: 1.009 | Accuracy: 0.603000 | 1.15 sec/iter\n",
      "Epoch: 159 | Batch: 009 / 011 | Total loss: 1.002 | Reg loss: 0.025 | Tree loss: 1.002 | Accuracy: 0.620500 | 1.15 sec/iter\n",
      "Epoch: 159 | Batch: 010 / 011 | Total loss: 1.008 | Reg loss: 0.025 | Tree loss: 1.008 | Accuracy: 0.580205 | 1.15 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 160 | Batch: 000 / 011 | Total loss: 1.099 | Reg loss: 0.025 | Tree loss: 1.099 | Accuracy: 0.620500 | 1.151 sec/iter\n",
      "Epoch: 160 | Batch: 001 / 011 | Total loss: 1.080 | Reg loss: 0.025 | Tree loss: 1.080 | Accuracy: 0.646000 | 1.15 sec/iter\n",
      "Epoch: 160 | Batch: 002 / 011 | Total loss: 1.055 | Reg loss: 0.025 | Tree loss: 1.055 | Accuracy: 0.674500 | 1.15 sec/iter\n",
      "Epoch: 160 | Batch: 003 / 011 | Total loss: 1.034 | Reg loss: 0.025 | Tree loss: 1.034 | Accuracy: 0.678500 | 1.15 sec/iter\n",
      "Epoch: 160 | Batch: 004 / 011 | Total loss: 1.034 | Reg loss: 0.025 | Tree loss: 1.034 | Accuracy: 0.668000 | 1.15 sec/iter\n",
      "Epoch: 160 | Batch: 005 / 011 | Total loss: 1.015 | Reg loss: 0.025 | Tree loss: 1.015 | Accuracy: 0.637500 | 1.15 sec/iter\n",
      "Epoch: 160 | Batch: 006 / 011 | Total loss: 1.006 | Reg loss: 0.025 | Tree loss: 1.006 | Accuracy: 0.623500 | 1.15 sec/iter\n",
      "Epoch: 160 | Batch: 007 / 011 | Total loss: 1.000 | Reg loss: 0.025 | Tree loss: 1.000 | Accuracy: 0.626000 | 1.15 sec/iter\n",
      "Epoch: 160 | Batch: 008 / 011 | Total loss: 0.991 | Reg loss: 0.025 | Tree loss: 0.991 | Accuracy: 0.624500 | 1.15 sec/iter\n",
      "Epoch: 160 | Batch: 009 / 011 | Total loss: 0.996 | Reg loss: 0.025 | Tree loss: 0.996 | Accuracy: 0.620500 | 1.15 sec/iter\n",
      "Epoch: 160 | Batch: 010 / 011 | Total loss: 1.033 | Reg loss: 0.025 | Tree loss: 1.033 | Accuracy: 0.621160 | 1.15 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 161 | Batch: 000 / 011 | Total loss: 1.086 | Reg loss: 0.025 | Tree loss: 1.086 | Accuracy: 0.641500 | 1.151 sec/iter\n",
      "Epoch: 161 | Batch: 001 / 011 | Total loss: 1.081 | Reg loss: 0.025 | Tree loss: 1.081 | Accuracy: 0.651500 | 1.151 sec/iter\n",
      "Epoch: 161 | Batch: 002 / 011 | Total loss: 1.048 | Reg loss: 0.025 | Tree loss: 1.048 | Accuracy: 0.678500 | 1.151 sec/iter\n",
      "Epoch: 161 | Batch: 003 / 011 | Total loss: 1.027 | Reg loss: 0.025 | Tree loss: 1.027 | Accuracy: 0.694000 | 1.151 sec/iter\n",
      "Epoch: 161 | Batch: 004 / 011 | Total loss: 1.022 | Reg loss: 0.025 | Tree loss: 1.022 | Accuracy: 0.657500 | 1.151 sec/iter\n",
      "Epoch: 161 | Batch: 005 / 011 | Total loss: 1.024 | Reg loss: 0.025 | Tree loss: 1.024 | Accuracy: 0.641000 | 1.151 sec/iter\n",
      "Epoch: 161 | Batch: 006 / 011 | Total loss: 1.018 | Reg loss: 0.025 | Tree loss: 1.018 | Accuracy: 0.614000 | 1.15 sec/iter\n",
      "Epoch: 161 | Batch: 007 / 011 | Total loss: 1.018 | Reg loss: 0.025 | Tree loss: 1.018 | Accuracy: 0.627500 | 1.15 sec/iter\n",
      "Epoch: 161 | Batch: 008 / 011 | Total loss: 1.006 | Reg loss: 0.025 | Tree loss: 1.006 | Accuracy: 0.615500 | 1.15 sec/iter\n",
      "Epoch: 161 | Batch: 009 / 011 | Total loss: 0.977 | Reg loss: 0.025 | Tree loss: 0.977 | Accuracy: 0.629500 | 1.15 sec/iter\n",
      "Epoch: 161 | Batch: 010 / 011 | Total loss: 0.971 | Reg loss: 0.025 | Tree loss: 0.971 | Accuracy: 0.648464 | 1.15 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 162 | Batch: 000 / 011 | Total loss: 1.093 | Reg loss: 0.025 | Tree loss: 1.093 | Accuracy: 0.633000 | 1.151 sec/iter\n",
      "Epoch: 162 | Batch: 001 / 011 | Total loss: 1.089 | Reg loss: 0.025 | Tree loss: 1.089 | Accuracy: 0.655000 | 1.151 sec/iter\n",
      "Epoch: 162 | Batch: 002 / 011 | Total loss: 1.064 | Reg loss: 0.025 | Tree loss: 1.064 | Accuracy: 0.684500 | 1.151 sec/iter\n",
      "Epoch: 162 | Batch: 003 / 011 | Total loss: 1.024 | Reg loss: 0.025 | Tree loss: 1.024 | Accuracy: 0.698500 | 1.151 sec/iter\n",
      "Epoch: 162 | Batch: 004 / 011 | Total loss: 1.020 | Reg loss: 0.025 | Tree loss: 1.020 | Accuracy: 0.656500 | 1.151 sec/iter\n",
      "Epoch: 162 | Batch: 005 / 011 | Total loss: 1.003 | Reg loss: 0.025 | Tree loss: 1.003 | Accuracy: 0.647000 | 1.151 sec/iter\n",
      "Epoch: 162 | Batch: 006 / 011 | Total loss: 0.993 | Reg loss: 0.025 | Tree loss: 0.993 | Accuracy: 0.630500 | 1.151 sec/iter\n",
      "Epoch: 162 | Batch: 007 / 011 | Total loss: 1.022 | Reg loss: 0.025 | Tree loss: 1.022 | Accuracy: 0.614500 | 1.151 sec/iter\n",
      "Epoch: 162 | Batch: 008 / 011 | Total loss: 0.973 | Reg loss: 0.025 | Tree loss: 0.973 | Accuracy: 0.640500 | 1.15 sec/iter\n",
      "Epoch: 162 | Batch: 009 / 011 | Total loss: 1.012 | Reg loss: 0.025 | Tree loss: 1.012 | Accuracy: 0.610500 | 1.15 sec/iter\n",
      "Epoch: 162 | Batch: 010 / 011 | Total loss: 0.996 | Reg loss: 0.025 | Tree loss: 0.996 | Accuracy: 0.641638 | 1.15 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 163 | Batch: 000 / 011 | Total loss: 1.094 | Reg loss: 0.025 | Tree loss: 1.094 | Accuracy: 0.624000 | 1.152 sec/iter\n",
      "Epoch: 163 | Batch: 001 / 011 | Total loss: 1.091 | Reg loss: 0.025 | Tree loss: 1.091 | Accuracy: 0.636000 | 1.152 sec/iter\n",
      "Epoch: 163 | Batch: 002 / 011 | Total loss: 1.055 | Reg loss: 0.025 | Tree loss: 1.055 | Accuracy: 0.685500 | 1.152 sec/iter\n",
      "Epoch: 163 | Batch: 003 / 011 | Total loss: 1.040 | Reg loss: 0.025 | Tree loss: 1.040 | Accuracy: 0.672000 | 1.152 sec/iter\n",
      "Epoch: 163 | Batch: 004 / 011 | Total loss: 1.009 | Reg loss: 0.025 | Tree loss: 1.009 | Accuracy: 0.677500 | 1.151 sec/iter\n",
      "Epoch: 163 | Batch: 005 / 011 | Total loss: 1.007 | Reg loss: 0.025 | Tree loss: 1.007 | Accuracy: 0.643500 | 1.151 sec/iter\n",
      "Epoch: 163 | Batch: 006 / 011 | Total loss: 0.990 | Reg loss: 0.025 | Tree loss: 0.990 | Accuracy: 0.627000 | 1.151 sec/iter\n",
      "Epoch: 163 | Batch: 007 / 011 | Total loss: 0.993 | Reg loss: 0.025 | Tree loss: 0.993 | Accuracy: 0.611000 | 1.151 sec/iter\n",
      "Epoch: 163 | Batch: 008 / 011 | Total loss: 0.992 | Reg loss: 0.025 | Tree loss: 0.992 | Accuracy: 0.617500 | 1.151 sec/iter\n",
      "Epoch: 163 | Batch: 009 / 011 | Total loss: 1.003 | Reg loss: 0.025 | Tree loss: 1.003 | Accuracy: 0.625500 | 1.151 sec/iter\n",
      "Epoch: 163 | Batch: 010 / 011 | Total loss: 1.030 | Reg loss: 0.025 | Tree loss: 1.030 | Accuracy: 0.597270 | 1.151 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 164 | Batch: 000 / 011 | Total loss: 1.098 | Reg loss: 0.025 | Tree loss: 1.098 | Accuracy: 0.626000 | 1.151 sec/iter\n",
      "Epoch: 164 | Batch: 001 / 011 | Total loss: 1.085 | Reg loss: 0.025 | Tree loss: 1.085 | Accuracy: 0.634500 | 1.151 sec/iter\n",
      "Epoch: 164 | Batch: 002 / 011 | Total loss: 1.069 | Reg loss: 0.025 | Tree loss: 1.069 | Accuracy: 0.658500 | 1.151 sec/iter\n",
      "Epoch: 164 | Batch: 003 / 011 | Total loss: 1.025 | Reg loss: 0.025 | Tree loss: 1.025 | Accuracy: 0.695500 | 1.15 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 164 | Batch: 004 / 011 | Total loss: 1.000 | Reg loss: 0.025 | Tree loss: 1.000 | Accuracy: 0.695500 | 1.15 sec/iter\n",
      "Epoch: 164 | Batch: 005 / 011 | Total loss: 1.009 | Reg loss: 0.025 | Tree loss: 1.009 | Accuracy: 0.645000 | 1.15 sec/iter\n",
      "Epoch: 164 | Batch: 006 / 011 | Total loss: 0.986 | Reg loss: 0.025 | Tree loss: 0.986 | Accuracy: 0.631000 | 1.15 sec/iter\n",
      "Epoch: 164 | Batch: 007 / 011 | Total loss: 0.987 | Reg loss: 0.025 | Tree loss: 0.987 | Accuracy: 0.609000 | 1.15 sec/iter\n",
      "Epoch: 164 | Batch: 008 / 011 | Total loss: 0.998 | Reg loss: 0.025 | Tree loss: 0.998 | Accuracy: 0.619000 | 1.15 sec/iter\n",
      "Epoch: 164 | Batch: 009 / 011 | Total loss: 1.025 | Reg loss: 0.025 | Tree loss: 1.025 | Accuracy: 0.600000 | 1.15 sec/iter\n",
      "Epoch: 164 | Batch: 010 / 011 | Total loss: 0.994 | Reg loss: 0.025 | Tree loss: 0.994 | Accuracy: 0.634812 | 1.15 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 165 | Batch: 000 / 011 | Total loss: 1.074 | Reg loss: 0.025 | Tree loss: 1.074 | Accuracy: 0.635000 | 1.15 sec/iter\n",
      "Epoch: 165 | Batch: 001 / 011 | Total loss: 1.080 | Reg loss: 0.025 | Tree loss: 1.080 | Accuracy: 0.651500 | 1.15 sec/iter\n",
      "Epoch: 165 | Batch: 002 / 011 | Total loss: 1.042 | Reg loss: 0.025 | Tree loss: 1.042 | Accuracy: 0.689500 | 1.15 sec/iter\n",
      "Epoch: 165 | Batch: 003 / 011 | Total loss: 1.027 | Reg loss: 0.025 | Tree loss: 1.027 | Accuracy: 0.689500 | 1.15 sec/iter\n",
      "Epoch: 165 | Batch: 004 / 011 | Total loss: 1.010 | Reg loss: 0.025 | Tree loss: 1.010 | Accuracy: 0.665000 | 1.15 sec/iter\n",
      "Epoch: 165 | Batch: 005 / 011 | Total loss: 1.013 | Reg loss: 0.025 | Tree loss: 1.013 | Accuracy: 0.629000 | 1.15 sec/iter\n",
      "Epoch: 165 | Batch: 006 / 011 | Total loss: 1.011 | Reg loss: 0.025 | Tree loss: 1.011 | Accuracy: 0.625500 | 1.15 sec/iter\n",
      "Epoch: 165 | Batch: 007 / 011 | Total loss: 1.014 | Reg loss: 0.025 | Tree loss: 1.014 | Accuracy: 0.612000 | 1.15 sec/iter\n",
      "Epoch: 165 | Batch: 008 / 011 | Total loss: 1.006 | Reg loss: 0.025 | Tree loss: 1.006 | Accuracy: 0.614000 | 1.15 sec/iter\n",
      "Epoch: 165 | Batch: 009 / 011 | Total loss: 0.982 | Reg loss: 0.025 | Tree loss: 0.982 | Accuracy: 0.628000 | 1.15 sec/iter\n",
      "Epoch: 165 | Batch: 010 / 011 | Total loss: 0.923 | Reg loss: 0.025 | Tree loss: 0.923 | Accuracy: 0.662116 | 1.15 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 166 | Batch: 000 / 011 | Total loss: 1.087 | Reg loss: 0.025 | Tree loss: 1.087 | Accuracy: 0.626000 | 1.151 sec/iter\n",
      "Epoch: 166 | Batch: 001 / 011 | Total loss: 1.068 | Reg loss: 0.025 | Tree loss: 1.068 | Accuracy: 0.649000 | 1.15 sec/iter\n",
      "Epoch: 166 | Batch: 002 / 011 | Total loss: 1.059 | Reg loss: 0.025 | Tree loss: 1.059 | Accuracy: 0.669000 | 1.15 sec/iter\n",
      "Epoch: 166 | Batch: 003 / 011 | Total loss: 1.007 | Reg loss: 0.025 | Tree loss: 1.007 | Accuracy: 0.688500 | 1.15 sec/iter\n",
      "Epoch: 166 | Batch: 004 / 011 | Total loss: 1.005 | Reg loss: 0.025 | Tree loss: 1.005 | Accuracy: 0.660000 | 1.15 sec/iter\n",
      "Epoch: 166 | Batch: 005 / 011 | Total loss: 1.002 | Reg loss: 0.025 | Tree loss: 1.002 | Accuracy: 0.648000 | 1.15 sec/iter\n",
      "Epoch: 166 | Batch: 006 / 011 | Total loss: 1.001 | Reg loss: 0.025 | Tree loss: 1.001 | Accuracy: 0.633000 | 1.15 sec/iter\n",
      "Epoch: 166 | Batch: 007 / 011 | Total loss: 0.999 | Reg loss: 0.025 | Tree loss: 0.999 | Accuracy: 0.626000 | 1.15 sec/iter\n",
      "Epoch: 166 | Batch: 008 / 011 | Total loss: 1.025 | Reg loss: 0.025 | Tree loss: 1.025 | Accuracy: 0.589000 | 1.15 sec/iter\n",
      "Epoch: 166 | Batch: 009 / 011 | Total loss: 1.002 | Reg loss: 0.025 | Tree loss: 1.002 | Accuracy: 0.612000 | 1.15 sec/iter\n",
      "Epoch: 166 | Batch: 010 / 011 | Total loss: 0.983 | Reg loss: 0.025 | Tree loss: 0.983 | Accuracy: 0.614334 | 1.15 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 167 | Batch: 000 / 011 | Total loss: 1.092 | Reg loss: 0.025 | Tree loss: 1.092 | Accuracy: 0.634500 | 1.151 sec/iter\n",
      "Epoch: 167 | Batch: 001 / 011 | Total loss: 1.061 | Reg loss: 0.025 | Tree loss: 1.061 | Accuracy: 0.646000 | 1.151 sec/iter\n",
      "Epoch: 167 | Batch: 002 / 011 | Total loss: 1.041 | Reg loss: 0.025 | Tree loss: 1.041 | Accuracy: 0.691000 | 1.151 sec/iter\n",
      "Epoch: 167 | Batch: 003 / 011 | Total loss: 1.021 | Reg loss: 0.025 | Tree loss: 1.021 | Accuracy: 0.700000 | 1.151 sec/iter\n",
      "Epoch: 167 | Batch: 004 / 011 | Total loss: 1.036 | Reg loss: 0.025 | Tree loss: 1.036 | Accuracy: 0.639500 | 1.151 sec/iter\n",
      "Epoch: 167 | Batch: 005 / 011 | Total loss: 1.020 | Reg loss: 0.025 | Tree loss: 1.020 | Accuracy: 0.629000 | 1.151 sec/iter\n",
      "Epoch: 167 | Batch: 006 / 011 | Total loss: 1.000 | Reg loss: 0.025 | Tree loss: 1.000 | Accuracy: 0.632000 | 1.15 sec/iter\n",
      "Epoch: 167 | Batch: 007 / 011 | Total loss: 1.009 | Reg loss: 0.025 | Tree loss: 1.009 | Accuracy: 0.624500 | 1.15 sec/iter\n",
      "Epoch: 167 | Batch: 008 / 011 | Total loss: 1.015 | Reg loss: 0.025 | Tree loss: 1.015 | Accuracy: 0.608500 | 1.15 sec/iter\n",
      "Epoch: 167 | Batch: 009 / 011 | Total loss: 0.973 | Reg loss: 0.025 | Tree loss: 0.973 | Accuracy: 0.634000 | 1.15 sec/iter\n",
      "Epoch: 167 | Batch: 010 / 011 | Total loss: 0.965 | Reg loss: 0.025 | Tree loss: 0.965 | Accuracy: 0.655290 | 1.15 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 168 | Batch: 000 / 011 | Total loss: 1.074 | Reg loss: 0.025 | Tree loss: 1.074 | Accuracy: 0.648000 | 1.151 sec/iter\n",
      "Epoch: 168 | Batch: 001 / 011 | Total loss: 1.075 | Reg loss: 0.025 | Tree loss: 1.075 | Accuracy: 0.632500 | 1.151 sec/iter\n",
      "Epoch: 168 | Batch: 002 / 011 | Total loss: 1.057 | Reg loss: 0.025 | Tree loss: 1.057 | Accuracy: 0.666000 | 1.151 sec/iter\n",
      "Epoch: 168 | Batch: 003 / 011 | Total loss: 1.016 | Reg loss: 0.025 | Tree loss: 1.016 | Accuracy: 0.687000 | 1.151 sec/iter\n",
      "Epoch: 168 | Batch: 004 / 011 | Total loss: 1.015 | Reg loss: 0.025 | Tree loss: 1.015 | Accuracy: 0.684500 | 1.151 sec/iter\n",
      "Epoch: 168 | Batch: 005 / 011 | Total loss: 1.006 | Reg loss: 0.025 | Tree loss: 1.006 | Accuracy: 0.651500 | 1.151 sec/iter\n",
      "Epoch: 168 | Batch: 006 / 011 | Total loss: 1.020 | Reg loss: 0.025 | Tree loss: 1.020 | Accuracy: 0.613000 | 1.151 sec/iter\n",
      "Epoch: 168 | Batch: 007 / 011 | Total loss: 0.988 | Reg loss: 0.025 | Tree loss: 0.988 | Accuracy: 0.628500 | 1.151 sec/iter\n",
      "Epoch: 168 | Batch: 008 / 011 | Total loss: 0.991 | Reg loss: 0.025 | Tree loss: 0.991 | Accuracy: 0.614500 | 1.151 sec/iter\n",
      "Epoch: 168 | Batch: 009 / 011 | Total loss: 0.978 | Reg loss: 0.025 | Tree loss: 0.978 | Accuracy: 0.644000 | 1.151 sec/iter\n",
      "Epoch: 168 | Batch: 010 / 011 | Total loss: 1.039 | Reg loss: 0.025 | Tree loss: 1.039 | Accuracy: 0.573379 | 1.151 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 169 | Batch: 000 / 011 | Total loss: 1.088 | Reg loss: 0.025 | Tree loss: 1.088 | Accuracy: 0.623000 | 1.152 sec/iter\n",
      "Epoch: 169 | Batch: 001 / 011 | Total loss: 1.087 | Reg loss: 0.025 | Tree loss: 1.087 | Accuracy: 0.647500 | 1.152 sec/iter\n",
      "Epoch: 169 | Batch: 002 / 011 | Total loss: 1.040 | Reg loss: 0.025 | Tree loss: 1.040 | Accuracy: 0.687500 | 1.152 sec/iter\n",
      "Epoch: 169 | Batch: 003 / 011 | Total loss: 1.036 | Reg loss: 0.025 | Tree loss: 1.036 | Accuracy: 0.683000 | 1.152 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 169 | Batch: 004 / 011 | Total loss: 1.009 | Reg loss: 0.025 | Tree loss: 1.009 | Accuracy: 0.663500 | 1.152 sec/iter\n",
      "Epoch: 169 | Batch: 005 / 011 | Total loss: 1.002 | Reg loss: 0.025 | Tree loss: 1.002 | Accuracy: 0.627000 | 1.151 sec/iter\n",
      "Epoch: 169 | Batch: 006 / 011 | Total loss: 1.007 | Reg loss: 0.025 | Tree loss: 1.007 | Accuracy: 0.616500 | 1.151 sec/iter\n",
      "Epoch: 169 | Batch: 007 / 011 | Total loss: 0.994 | Reg loss: 0.025 | Tree loss: 0.994 | Accuracy: 0.622000 | 1.151 sec/iter\n",
      "Epoch: 169 | Batch: 008 / 011 | Total loss: 0.994 | Reg loss: 0.025 | Tree loss: 0.994 | Accuracy: 0.616500 | 1.151 sec/iter\n",
      "Epoch: 169 | Batch: 009 / 011 | Total loss: 0.978 | Reg loss: 0.025 | Tree loss: 0.978 | Accuracy: 0.629500 | 1.151 sec/iter\n",
      "Epoch: 169 | Batch: 010 / 011 | Total loss: 1.038 | Reg loss: 0.025 | Tree loss: 1.038 | Accuracy: 0.634812 | 1.151 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 170 | Batch: 000 / 011 | Total loss: 1.089 | Reg loss: 0.025 | Tree loss: 1.089 | Accuracy: 0.602500 | 1.151 sec/iter\n",
      "Epoch: 170 | Batch: 001 / 011 | Total loss: 1.070 | Reg loss: 0.025 | Tree loss: 1.070 | Accuracy: 0.645500 | 1.151 sec/iter\n",
      "Epoch: 170 | Batch: 002 / 011 | Total loss: 1.049 | Reg loss: 0.025 | Tree loss: 1.049 | Accuracy: 0.662500 | 1.151 sec/iter\n",
      "Epoch: 170 | Batch: 003 / 011 | Total loss: 1.057 | Reg loss: 0.025 | Tree loss: 1.057 | Accuracy: 0.660000 | 1.151 sec/iter\n",
      "Epoch: 170 | Batch: 004 / 011 | Total loss: 0.978 | Reg loss: 0.025 | Tree loss: 0.978 | Accuracy: 0.697000 | 1.151 sec/iter\n",
      "Epoch: 170 | Batch: 005 / 011 | Total loss: 0.997 | Reg loss: 0.025 | Tree loss: 0.997 | Accuracy: 0.648000 | 1.15 sec/iter\n",
      "Epoch: 170 | Batch: 006 / 011 | Total loss: 1.003 | Reg loss: 0.025 | Tree loss: 1.003 | Accuracy: 0.630000 | 1.15 sec/iter\n",
      "Epoch: 170 | Batch: 007 / 011 | Total loss: 1.013 | Reg loss: 0.025 | Tree loss: 1.013 | Accuracy: 0.616000 | 1.15 sec/iter\n",
      "Epoch: 170 | Batch: 008 / 011 | Total loss: 1.001 | Reg loss: 0.025 | Tree loss: 1.001 | Accuracy: 0.604500 | 1.15 sec/iter\n",
      "Epoch: 170 | Batch: 009 / 011 | Total loss: 0.969 | Reg loss: 0.025 | Tree loss: 0.969 | Accuracy: 0.622500 | 1.15 sec/iter\n",
      "Epoch: 170 | Batch: 010 / 011 | Total loss: 1.019 | Reg loss: 0.025 | Tree loss: 1.019 | Accuracy: 0.600683 | 1.15 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 171 | Batch: 000 / 011 | Total loss: 1.078 | Reg loss: 0.025 | Tree loss: 1.078 | Accuracy: 0.635500 | 1.151 sec/iter\n",
      "Epoch: 171 | Batch: 001 / 011 | Total loss: 1.066 | Reg loss: 0.025 | Tree loss: 1.066 | Accuracy: 0.630000 | 1.151 sec/iter\n",
      "Epoch: 171 | Batch: 002 / 011 | Total loss: 1.059 | Reg loss: 0.025 | Tree loss: 1.059 | Accuracy: 0.655500 | 1.15 sec/iter\n",
      "Epoch: 171 | Batch: 003 / 011 | Total loss: 1.051 | Reg loss: 0.025 | Tree loss: 1.051 | Accuracy: 0.670000 | 1.15 sec/iter\n",
      "Epoch: 171 | Batch: 004 / 011 | Total loss: 1.019 | Reg loss: 0.025 | Tree loss: 1.019 | Accuracy: 0.672500 | 1.15 sec/iter\n",
      "Epoch: 171 | Batch: 005 / 011 | Total loss: 0.995 | Reg loss: 0.025 | Tree loss: 0.995 | Accuracy: 0.649500 | 1.15 sec/iter\n",
      "Epoch: 171 | Batch: 006 / 011 | Total loss: 0.988 | Reg loss: 0.025 | Tree loss: 0.988 | Accuracy: 0.630500 | 1.15 sec/iter\n",
      "Epoch: 171 | Batch: 007 / 011 | Total loss: 0.987 | Reg loss: 0.025 | Tree loss: 0.987 | Accuracy: 0.621500 | 1.15 sec/iter\n",
      "Epoch: 171 | Batch: 008 / 011 | Total loss: 1.008 | Reg loss: 0.025 | Tree loss: 1.008 | Accuracy: 0.596500 | 1.15 sec/iter\n",
      "Epoch: 171 | Batch: 009 / 011 | Total loss: 0.983 | Reg loss: 0.025 | Tree loss: 0.983 | Accuracy: 0.622000 | 1.15 sec/iter\n",
      "Epoch: 171 | Batch: 010 / 011 | Total loss: 0.991 | Reg loss: 0.025 | Tree loss: 0.991 | Accuracy: 0.638225 | 1.15 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 172 | Batch: 000 / 011 | Total loss: 1.068 | Reg loss: 0.025 | Tree loss: 1.068 | Accuracy: 0.641500 | 1.151 sec/iter\n",
      "Epoch: 172 | Batch: 001 / 011 | Total loss: 1.068 | Reg loss: 0.025 | Tree loss: 1.068 | Accuracy: 0.654000 | 1.151 sec/iter\n",
      "Epoch: 172 | Batch: 002 / 011 | Total loss: 1.053 | Reg loss: 0.025 | Tree loss: 1.053 | Accuracy: 0.668000 | 1.151 sec/iter\n",
      "Epoch: 172 | Batch: 003 / 011 | Total loss: 1.025 | Reg loss: 0.025 | Tree loss: 1.025 | Accuracy: 0.695000 | 1.151 sec/iter\n",
      "Epoch: 172 | Batch: 004 / 011 | Total loss: 1.023 | Reg loss: 0.025 | Tree loss: 1.023 | Accuracy: 0.658500 | 1.15 sec/iter\n",
      "Epoch: 172 | Batch: 005 / 011 | Total loss: 0.994 | Reg loss: 0.025 | Tree loss: 0.994 | Accuracy: 0.647500 | 1.15 sec/iter\n",
      "Epoch: 172 | Batch: 006 / 011 | Total loss: 0.990 | Reg loss: 0.025 | Tree loss: 0.990 | Accuracy: 0.624000 | 1.15 sec/iter\n",
      "Epoch: 172 | Batch: 007 / 011 | Total loss: 0.991 | Reg loss: 0.025 | Tree loss: 0.991 | Accuracy: 0.619000 | 1.15 sec/iter\n",
      "Epoch: 172 | Batch: 008 / 011 | Total loss: 0.987 | Reg loss: 0.025 | Tree loss: 0.987 | Accuracy: 0.631500 | 1.15 sec/iter\n",
      "Epoch: 172 | Batch: 009 / 011 | Total loss: 1.001 | Reg loss: 0.025 | Tree loss: 1.001 | Accuracy: 0.601500 | 1.15 sec/iter\n",
      "Epoch: 172 | Batch: 010 / 011 | Total loss: 1.042 | Reg loss: 0.025 | Tree loss: 1.042 | Accuracy: 0.607509 | 1.15 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 173 | Batch: 000 / 011 | Total loss: 1.091 | Reg loss: 0.025 | Tree loss: 1.091 | Accuracy: 0.623000 | 1.151 sec/iter\n",
      "Epoch: 173 | Batch: 001 / 011 | Total loss: 1.065 | Reg loss: 0.025 | Tree loss: 1.065 | Accuracy: 0.647500 | 1.151 sec/iter\n",
      "Epoch: 173 | Batch: 002 / 011 | Total loss: 1.031 | Reg loss: 0.025 | Tree loss: 1.031 | Accuracy: 0.673500 | 1.151 sec/iter\n",
      "Epoch: 173 | Batch: 003 / 011 | Total loss: 1.026 | Reg loss: 0.025 | Tree loss: 1.026 | Accuracy: 0.690500 | 1.151 sec/iter\n",
      "Epoch: 173 | Batch: 004 / 011 | Total loss: 1.001 | Reg loss: 0.025 | Tree loss: 1.001 | Accuracy: 0.685000 | 1.151 sec/iter\n",
      "Epoch: 173 | Batch: 005 / 011 | Total loss: 0.994 | Reg loss: 0.025 | Tree loss: 0.994 | Accuracy: 0.645500 | 1.151 sec/iter\n",
      "Epoch: 173 | Batch: 006 / 011 | Total loss: 1.009 | Reg loss: 0.025 | Tree loss: 1.009 | Accuracy: 0.627500 | 1.151 sec/iter\n",
      "Epoch: 173 | Batch: 007 / 011 | Total loss: 0.996 | Reg loss: 0.025 | Tree loss: 0.996 | Accuracy: 0.621500 | 1.151 sec/iter\n",
      "Epoch: 173 | Batch: 008 / 011 | Total loss: 0.974 | Reg loss: 0.025 | Tree loss: 0.974 | Accuracy: 0.626000 | 1.151 sec/iter\n",
      "Epoch: 173 | Batch: 009 / 011 | Total loss: 1.009 | Reg loss: 0.025 | Tree loss: 1.009 | Accuracy: 0.616000 | 1.151 sec/iter\n",
      "Epoch: 173 | Batch: 010 / 011 | Total loss: 1.020 | Reg loss: 0.025 | Tree loss: 1.020 | Accuracy: 0.604096 | 1.15 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 174 | Batch: 000 / 011 | Total loss: 1.123 | Reg loss: 0.025 | Tree loss: 1.123 | Accuracy: 0.598000 | 1.152 sec/iter\n",
      "Epoch: 174 | Batch: 001 / 011 | Total loss: 1.070 | Reg loss: 0.025 | Tree loss: 1.070 | Accuracy: 0.665000 | 1.151 sec/iter\n",
      "Epoch: 174 | Batch: 002 / 011 | Total loss: 1.059 | Reg loss: 0.025 | Tree loss: 1.059 | Accuracy: 0.668500 | 1.151 sec/iter\n",
      "Epoch: 174 | Batch: 003 / 011 | Total loss: 0.979 | Reg loss: 0.025 | Tree loss: 0.979 | Accuracy: 0.706500 | 1.151 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 174 | Batch: 004 / 011 | Total loss: 0.999 | Reg loss: 0.025 | Tree loss: 0.999 | Accuracy: 0.650500 | 1.151 sec/iter\n",
      "Epoch: 174 | Batch: 005 / 011 | Total loss: 1.021 | Reg loss: 0.025 | Tree loss: 1.021 | Accuracy: 0.639500 | 1.151 sec/iter\n",
      "Epoch: 174 | Batch: 006 / 011 | Total loss: 0.982 | Reg loss: 0.025 | Tree loss: 0.982 | Accuracy: 0.636000 | 1.151 sec/iter\n",
      "Epoch: 174 | Batch: 007 / 011 | Total loss: 0.984 | Reg loss: 0.025 | Tree loss: 0.984 | Accuracy: 0.625000 | 1.151 sec/iter\n",
      "Epoch: 174 | Batch: 008 / 011 | Total loss: 0.989 | Reg loss: 0.025 | Tree loss: 0.989 | Accuracy: 0.615000 | 1.151 sec/iter\n",
      "Epoch: 174 | Batch: 009 / 011 | Total loss: 0.979 | Reg loss: 0.025 | Tree loss: 0.979 | Accuracy: 0.625000 | 1.151 sec/iter\n",
      "Epoch: 174 | Batch: 010 / 011 | Total loss: 1.021 | Reg loss: 0.025 | Tree loss: 1.021 | Accuracy: 0.621160 | 1.151 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 175 | Batch: 000 / 011 | Total loss: 1.071 | Reg loss: 0.025 | Tree loss: 1.071 | Accuracy: 0.637000 | 1.152 sec/iter\n",
      "Epoch: 175 | Batch: 001 / 011 | Total loss: 1.068 | Reg loss: 0.025 | Tree loss: 1.068 | Accuracy: 0.645000 | 1.152 sec/iter\n",
      "Epoch: 175 | Batch: 002 / 011 | Total loss: 1.053 | Reg loss: 0.025 | Tree loss: 1.053 | Accuracy: 0.671000 | 1.152 sec/iter\n",
      "Epoch: 175 | Batch: 003 / 011 | Total loss: 1.020 | Reg loss: 0.025 | Tree loss: 1.020 | Accuracy: 0.689500 | 1.152 sec/iter\n",
      "Epoch: 175 | Batch: 004 / 011 | Total loss: 1.006 | Reg loss: 0.025 | Tree loss: 1.006 | Accuracy: 0.672000 | 1.152 sec/iter\n",
      "Epoch: 175 | Batch: 005 / 011 | Total loss: 1.027 | Reg loss: 0.025 | Tree loss: 1.027 | Accuracy: 0.639500 | 1.152 sec/iter\n",
      "Epoch: 175 | Batch: 006 / 011 | Total loss: 0.974 | Reg loss: 0.025 | Tree loss: 0.974 | Accuracy: 0.643000 | 1.152 sec/iter\n",
      "Epoch: 175 | Batch: 007 / 011 | Total loss: 0.974 | Reg loss: 0.025 | Tree loss: 0.974 | Accuracy: 0.633000 | 1.151 sec/iter\n",
      "Epoch: 175 | Batch: 008 / 011 | Total loss: 1.003 | Reg loss: 0.025 | Tree loss: 1.003 | Accuracy: 0.610500 | 1.151 sec/iter\n",
      "Epoch: 175 | Batch: 009 / 011 | Total loss: 0.987 | Reg loss: 0.025 | Tree loss: 0.987 | Accuracy: 0.608000 | 1.151 sec/iter\n",
      "Epoch: 175 | Batch: 010 / 011 | Total loss: 0.980 | Reg loss: 0.025 | Tree loss: 0.980 | Accuracy: 0.610922 | 1.151 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 176 | Batch: 000 / 011 | Total loss: 1.073 | Reg loss: 0.025 | Tree loss: 1.073 | Accuracy: 0.621000 | 1.151 sec/iter\n",
      "Epoch: 176 | Batch: 001 / 011 | Total loss: 1.068 | Reg loss: 0.025 | Tree loss: 1.068 | Accuracy: 0.649000 | 1.151 sec/iter\n",
      "Epoch: 176 | Batch: 002 / 011 | Total loss: 1.026 | Reg loss: 0.025 | Tree loss: 1.026 | Accuracy: 0.683500 | 1.151 sec/iter\n",
      "Epoch: 176 | Batch: 003 / 011 | Total loss: 1.027 | Reg loss: 0.025 | Tree loss: 1.027 | Accuracy: 0.683500 | 1.151 sec/iter\n",
      "Epoch: 176 | Batch: 004 / 011 | Total loss: 1.004 | Reg loss: 0.025 | Tree loss: 1.004 | Accuracy: 0.680500 | 1.151 sec/iter\n",
      "Epoch: 176 | Batch: 005 / 011 | Total loss: 1.002 | Reg loss: 0.025 | Tree loss: 1.002 | Accuracy: 0.653500 | 1.151 sec/iter\n",
      "Epoch: 176 | Batch: 006 / 011 | Total loss: 0.973 | Reg loss: 0.025 | Tree loss: 0.973 | Accuracy: 0.645500 | 1.15 sec/iter\n",
      "Epoch: 176 | Batch: 007 / 011 | Total loss: 0.992 | Reg loss: 0.025 | Tree loss: 0.992 | Accuracy: 0.621500 | 1.15 sec/iter\n",
      "Epoch: 176 | Batch: 008 / 011 | Total loss: 1.004 | Reg loss: 0.025 | Tree loss: 1.004 | Accuracy: 0.612500 | 1.15 sec/iter\n",
      "Epoch: 176 | Batch: 009 / 011 | Total loss: 0.997 | Reg loss: 0.025 | Tree loss: 0.997 | Accuracy: 0.604500 | 1.15 sec/iter\n",
      "Epoch: 176 | Batch: 010 / 011 | Total loss: 1.060 | Reg loss: 0.025 | Tree loss: 1.060 | Accuracy: 0.576792 | 1.15 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 177 | Batch: 000 / 011 | Total loss: 1.075 | Reg loss: 0.025 | Tree loss: 1.075 | Accuracy: 0.637500 | 1.151 sec/iter\n",
      "Epoch: 177 | Batch: 001 / 011 | Total loss: 1.053 | Reg loss: 0.025 | Tree loss: 1.053 | Accuracy: 0.661000 | 1.151 sec/iter\n",
      "Epoch: 177 | Batch: 002 / 011 | Total loss: 1.039 | Reg loss: 0.025 | Tree loss: 1.039 | Accuracy: 0.691500 | 1.15 sec/iter\n",
      "Epoch: 177 | Batch: 003 / 011 | Total loss: 1.025 | Reg loss: 0.025 | Tree loss: 1.025 | Accuracy: 0.701000 | 1.15 sec/iter\n",
      "Epoch: 177 | Batch: 004 / 011 | Total loss: 1.017 | Reg loss: 0.025 | Tree loss: 1.017 | Accuracy: 0.678000 | 1.15 sec/iter\n",
      "Epoch: 177 | Batch: 005 / 011 | Total loss: 1.022 | Reg loss: 0.025 | Tree loss: 1.022 | Accuracy: 0.638500 | 1.15 sec/iter\n",
      "Epoch: 177 | Batch: 006 / 011 | Total loss: 0.995 | Reg loss: 0.025 | Tree loss: 0.995 | Accuracy: 0.637000 | 1.15 sec/iter\n",
      "Epoch: 177 | Batch: 007 / 011 | Total loss: 0.992 | Reg loss: 0.025 | Tree loss: 0.992 | Accuracy: 0.626500 | 1.15 sec/iter\n",
      "Epoch: 177 | Batch: 008 / 011 | Total loss: 0.982 | Reg loss: 0.025 | Tree loss: 0.982 | Accuracy: 0.614000 | 1.15 sec/iter\n",
      "Epoch: 177 | Batch: 009 / 011 | Total loss: 0.986 | Reg loss: 0.025 | Tree loss: 0.986 | Accuracy: 0.621000 | 1.15 sec/iter\n",
      "Epoch: 177 | Batch: 010 / 011 | Total loss: 1.010 | Reg loss: 0.025 | Tree loss: 1.010 | Accuracy: 0.600683 | 1.15 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 178 | Batch: 000 / 011 | Total loss: 1.077 | Reg loss: 0.025 | Tree loss: 1.077 | Accuracy: 0.633000 | 1.151 sec/iter\n",
      "Epoch: 178 | Batch: 001 / 011 | Total loss: 1.059 | Reg loss: 0.025 | Tree loss: 1.059 | Accuracy: 0.641000 | 1.151 sec/iter\n",
      "Epoch: 178 | Batch: 002 / 011 | Total loss: 1.033 | Reg loss: 0.025 | Tree loss: 1.033 | Accuracy: 0.676000 | 1.151 sec/iter\n",
      "Epoch: 178 | Batch: 003 / 011 | Total loss: 1.004 | Reg loss: 0.025 | Tree loss: 1.004 | Accuracy: 0.706000 | 1.151 sec/iter\n",
      "Epoch: 178 | Batch: 004 / 011 | Total loss: 0.997 | Reg loss: 0.025 | Tree loss: 0.997 | Accuracy: 0.671000 | 1.151 sec/iter\n",
      "Epoch: 178 | Batch: 005 / 011 | Total loss: 1.018 | Reg loss: 0.025 | Tree loss: 1.018 | Accuracy: 0.625500 | 1.151 sec/iter\n",
      "Epoch: 178 | Batch: 006 / 011 | Total loss: 0.979 | Reg loss: 0.025 | Tree loss: 0.979 | Accuracy: 0.643500 | 1.15 sec/iter\n",
      "Epoch: 178 | Batch: 007 / 011 | Total loss: 1.003 | Reg loss: 0.025 | Tree loss: 1.003 | Accuracy: 0.626500 | 1.15 sec/iter\n",
      "Epoch: 178 | Batch: 008 / 011 | Total loss: 1.021 | Reg loss: 0.025 | Tree loss: 1.021 | Accuracy: 0.603500 | 1.15 sec/iter\n",
      "Epoch: 178 | Batch: 009 / 011 | Total loss: 1.005 | Reg loss: 0.025 | Tree loss: 1.005 | Accuracy: 0.600000 | 1.15 sec/iter\n",
      "Epoch: 178 | Batch: 010 / 011 | Total loss: 0.977 | Reg loss: 0.025 | Tree loss: 0.977 | Accuracy: 0.590444 | 1.15 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 179 | Batch: 000 / 011 | Total loss: 1.101 | Reg loss: 0.025 | Tree loss: 1.101 | Accuracy: 0.616500 | 1.151 sec/iter\n",
      "Epoch: 179 | Batch: 001 / 011 | Total loss: 1.071 | Reg loss: 0.025 | Tree loss: 1.071 | Accuracy: 0.626500 | 1.151 sec/iter\n",
      "Epoch: 179 | Batch: 002 / 011 | Total loss: 1.042 | Reg loss: 0.025 | Tree loss: 1.042 | Accuracy: 0.660500 | 1.151 sec/iter\n",
      "Epoch: 179 | Batch: 003 / 011 | Total loss: 1.017 | Reg loss: 0.025 | Tree loss: 1.017 | Accuracy: 0.691500 | 1.151 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 179 | Batch: 004 / 011 | Total loss: 1.028 | Reg loss: 0.025 | Tree loss: 1.028 | Accuracy: 0.663000 | 1.151 sec/iter\n",
      "Epoch: 179 | Batch: 005 / 011 | Total loss: 0.994 | Reg loss: 0.025 | Tree loss: 0.994 | Accuracy: 0.651500 | 1.151 sec/iter\n",
      "Epoch: 179 | Batch: 006 / 011 | Total loss: 1.002 | Reg loss: 0.025 | Tree loss: 1.002 | Accuracy: 0.634000 | 1.151 sec/iter\n",
      "Epoch: 179 | Batch: 007 / 011 | Total loss: 0.966 | Reg loss: 0.025 | Tree loss: 0.966 | Accuracy: 0.636000 | 1.151 sec/iter\n",
      "Epoch: 179 | Batch: 008 / 011 | Total loss: 1.002 | Reg loss: 0.025 | Tree loss: 1.002 | Accuracy: 0.604000 | 1.151 sec/iter\n",
      "Epoch: 179 | Batch: 009 / 011 | Total loss: 0.956 | Reg loss: 0.025 | Tree loss: 0.956 | Accuracy: 0.634500 | 1.151 sec/iter\n",
      "Epoch: 179 | Batch: 010 / 011 | Total loss: 0.978 | Reg loss: 0.025 | Tree loss: 0.978 | Accuracy: 0.638225 | 1.15 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 180 | Batch: 000 / 011 | Total loss: 1.085 | Reg loss: 0.025 | Tree loss: 1.085 | Accuracy: 0.617500 | 1.152 sec/iter\n",
      "Epoch: 180 | Batch: 001 / 011 | Total loss: 1.080 | Reg loss: 0.025 | Tree loss: 1.080 | Accuracy: 0.622500 | 1.152 sec/iter\n",
      "Epoch: 180 | Batch: 002 / 011 | Total loss: 1.038 | Reg loss: 0.025 | Tree loss: 1.038 | Accuracy: 0.668500 | 1.152 sec/iter\n",
      "Epoch: 180 | Batch: 003 / 011 | Total loss: 1.050 | Reg loss: 0.025 | Tree loss: 1.050 | Accuracy: 0.663500 | 1.151 sec/iter\n",
      "Epoch: 180 | Batch: 004 / 011 | Total loss: 0.983 | Reg loss: 0.025 | Tree loss: 0.983 | Accuracy: 0.684500 | 1.151 sec/iter\n",
      "Epoch: 180 | Batch: 005 / 011 | Total loss: 0.990 | Reg loss: 0.025 | Tree loss: 0.990 | Accuracy: 0.653500 | 1.151 sec/iter\n",
      "Epoch: 180 | Batch: 006 / 011 | Total loss: 0.985 | Reg loss: 0.025 | Tree loss: 0.985 | Accuracy: 0.645500 | 1.151 sec/iter\n",
      "Epoch: 180 | Batch: 007 / 011 | Total loss: 0.993 | Reg loss: 0.025 | Tree loss: 0.993 | Accuracy: 0.619000 | 1.151 sec/iter\n",
      "Epoch: 180 | Batch: 008 / 011 | Total loss: 0.983 | Reg loss: 0.025 | Tree loss: 0.983 | Accuracy: 0.632500 | 1.151 sec/iter\n",
      "Epoch: 180 | Batch: 009 / 011 | Total loss: 0.975 | Reg loss: 0.025 | Tree loss: 0.975 | Accuracy: 0.633000 | 1.151 sec/iter\n",
      "Epoch: 180 | Batch: 010 / 011 | Total loss: 0.964 | Reg loss: 0.025 | Tree loss: 0.964 | Accuracy: 0.631399 | 1.151 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 181 | Batch: 000 / 011 | Total loss: 1.099 | Reg loss: 0.025 | Tree loss: 1.099 | Accuracy: 0.616000 | 1.152 sec/iter\n",
      "Epoch: 181 | Batch: 001 / 011 | Total loss: 1.059 | Reg loss: 0.025 | Tree loss: 1.059 | Accuracy: 0.656000 | 1.152 sec/iter\n",
      "Epoch: 181 | Batch: 002 / 011 | Total loss: 1.020 | Reg loss: 0.025 | Tree loss: 1.020 | Accuracy: 0.685000 | 1.152 sec/iter\n",
      "Epoch: 181 | Batch: 003 / 011 | Total loss: 1.019 | Reg loss: 0.025 | Tree loss: 1.019 | Accuracy: 0.692000 | 1.152 sec/iter\n",
      "Epoch: 181 | Batch: 004 / 011 | Total loss: 1.019 | Reg loss: 0.025 | Tree loss: 1.019 | Accuracy: 0.647000 | 1.152 sec/iter\n",
      "Epoch: 181 | Batch: 005 / 011 | Total loss: 1.008 | Reg loss: 0.025 | Tree loss: 1.008 | Accuracy: 0.645500 | 1.152 sec/iter\n",
      "Epoch: 181 | Batch: 006 / 011 | Total loss: 0.989 | Reg loss: 0.025 | Tree loss: 0.989 | Accuracy: 0.641500 | 1.152 sec/iter\n",
      "Epoch: 181 | Batch: 007 / 011 | Total loss: 0.983 | Reg loss: 0.025 | Tree loss: 0.983 | Accuracy: 0.641000 | 1.152 sec/iter\n",
      "Epoch: 181 | Batch: 008 / 011 | Total loss: 0.979 | Reg loss: 0.025 | Tree loss: 0.979 | Accuracy: 0.622000 | 1.152 sec/iter\n",
      "Epoch: 181 | Batch: 009 / 011 | Total loss: 0.975 | Reg loss: 0.025 | Tree loss: 0.975 | Accuracy: 0.620500 | 1.151 sec/iter\n",
      "Epoch: 181 | Batch: 010 / 011 | Total loss: 0.999 | Reg loss: 0.025 | Tree loss: 0.999 | Accuracy: 0.621160 | 1.151 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 182 | Batch: 000 / 011 | Total loss: 1.072 | Reg loss: 0.025 | Tree loss: 1.072 | Accuracy: 0.638500 | 1.151 sec/iter\n",
      "Epoch: 182 | Batch: 001 / 011 | Total loss: 1.064 | Reg loss: 0.025 | Tree loss: 1.064 | Accuracy: 0.653500 | 1.151 sec/iter\n",
      "Epoch: 182 | Batch: 002 / 011 | Total loss: 1.043 | Reg loss: 0.025 | Tree loss: 1.043 | Accuracy: 0.693000 | 1.151 sec/iter\n",
      "Epoch: 182 | Batch: 003 / 011 | Total loss: 1.015 | Reg loss: 0.025 | Tree loss: 1.015 | Accuracy: 0.684500 | 1.151 sec/iter\n",
      "Epoch: 182 | Batch: 004 / 011 | Total loss: 1.000 | Reg loss: 0.025 | Tree loss: 1.000 | Accuracy: 0.672500 | 1.151 sec/iter\n",
      "Epoch: 182 | Batch: 005 / 011 | Total loss: 1.001 | Reg loss: 0.025 | Tree loss: 1.001 | Accuracy: 0.639000 | 1.151 sec/iter\n",
      "Epoch: 182 | Batch: 006 / 011 | Total loss: 0.996 | Reg loss: 0.025 | Tree loss: 0.996 | Accuracy: 0.633500 | 1.151 sec/iter\n",
      "Epoch: 182 | Batch: 007 / 011 | Total loss: 0.991 | Reg loss: 0.025 | Tree loss: 0.991 | Accuracy: 0.612500 | 1.151 sec/iter\n",
      "Epoch: 182 | Batch: 008 / 011 | Total loss: 1.013 | Reg loss: 0.025 | Tree loss: 1.013 | Accuracy: 0.609500 | 1.151 sec/iter\n",
      "Epoch: 182 | Batch: 009 / 011 | Total loss: 0.969 | Reg loss: 0.025 | Tree loss: 0.969 | Accuracy: 0.640500 | 1.151 sec/iter\n",
      "Epoch: 182 | Batch: 010 / 011 | Total loss: 0.937 | Reg loss: 0.025 | Tree loss: 0.937 | Accuracy: 0.686007 | 1.15 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 183 | Batch: 000 / 011 | Total loss: 1.052 | Reg loss: 0.025 | Tree loss: 1.052 | Accuracy: 0.649000 | 1.151 sec/iter\n",
      "Epoch: 183 | Batch: 001 / 011 | Total loss: 1.057 | Reg loss: 0.025 | Tree loss: 1.057 | Accuracy: 0.639500 | 1.151 sec/iter\n",
      "Epoch: 183 | Batch: 002 / 011 | Total loss: 1.056 | Reg loss: 0.025 | Tree loss: 1.056 | Accuracy: 0.671000 | 1.151 sec/iter\n",
      "Epoch: 183 | Batch: 003 / 011 | Total loss: 1.024 | Reg loss: 0.025 | Tree loss: 1.024 | Accuracy: 0.682500 | 1.151 sec/iter\n",
      "Epoch: 183 | Batch: 004 / 011 | Total loss: 0.967 | Reg loss: 0.025 | Tree loss: 0.967 | Accuracy: 0.698000 | 1.151 sec/iter\n",
      "Epoch: 183 | Batch: 005 / 011 | Total loss: 1.031 | Reg loss: 0.025 | Tree loss: 1.031 | Accuracy: 0.635000 | 1.151 sec/iter\n",
      "Epoch: 183 | Batch: 006 / 011 | Total loss: 0.985 | Reg loss: 0.025 | Tree loss: 0.985 | Accuracy: 0.638500 | 1.15 sec/iter\n",
      "Epoch: 183 | Batch: 007 / 011 | Total loss: 0.976 | Reg loss: 0.025 | Tree loss: 0.976 | Accuracy: 0.624000 | 1.15 sec/iter\n",
      "Epoch: 183 | Batch: 008 / 011 | Total loss: 0.973 | Reg loss: 0.025 | Tree loss: 0.973 | Accuracy: 0.629500 | 1.15 sec/iter\n",
      "Epoch: 183 | Batch: 009 / 011 | Total loss: 1.000 | Reg loss: 0.025 | Tree loss: 1.000 | Accuracy: 0.619000 | 1.15 sec/iter\n",
      "Epoch: 183 | Batch: 010 / 011 | Total loss: 0.982 | Reg loss: 0.025 | Tree loss: 0.982 | Accuracy: 0.665529 | 1.15 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 184 | Batch: 000 / 011 | Total loss: 1.082 | Reg loss: 0.025 | Tree loss: 1.082 | Accuracy: 0.628000 | 1.151 sec/iter\n",
      "Epoch: 184 | Batch: 001 / 011 | Total loss: 1.050 | Reg loss: 0.025 | Tree loss: 1.050 | Accuracy: 0.641500 | 1.151 sec/iter\n",
      "Epoch: 184 | Batch: 002 / 011 | Total loss: 1.037 | Reg loss: 0.025 | Tree loss: 1.037 | Accuracy: 0.677000 | 1.151 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 184 | Batch: 003 / 011 | Total loss: 1.014 | Reg loss: 0.025 | Tree loss: 1.014 | Accuracy: 0.694500 | 1.151 sec/iter\n",
      "Epoch: 184 | Batch: 004 / 011 | Total loss: 1.008 | Reg loss: 0.025 | Tree loss: 1.008 | Accuracy: 0.666500 | 1.151 sec/iter\n",
      "Epoch: 184 | Batch: 005 / 011 | Total loss: 1.003 | Reg loss: 0.025 | Tree loss: 1.003 | Accuracy: 0.650000 | 1.151 sec/iter\n",
      "Epoch: 184 | Batch: 006 / 011 | Total loss: 0.982 | Reg loss: 0.025 | Tree loss: 0.982 | Accuracy: 0.627000 | 1.151 sec/iter\n",
      "Epoch: 184 | Batch: 007 / 011 | Total loss: 0.983 | Reg loss: 0.025 | Tree loss: 0.983 | Accuracy: 0.635000 | 1.151 sec/iter\n",
      "Epoch: 184 | Batch: 008 / 011 | Total loss: 0.979 | Reg loss: 0.025 | Tree loss: 0.979 | Accuracy: 0.621000 | 1.15 sec/iter\n",
      "Epoch: 184 | Batch: 009 / 011 | Total loss: 0.996 | Reg loss: 0.025 | Tree loss: 0.996 | Accuracy: 0.615000 | 1.15 sec/iter\n",
      "Epoch: 184 | Batch: 010 / 011 | Total loss: 0.989 | Reg loss: 0.025 | Tree loss: 0.989 | Accuracy: 0.641638 | 1.15 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 185 | Batch: 000 / 011 | Total loss: 1.082 | Reg loss: 0.025 | Tree loss: 1.082 | Accuracy: 0.629000 | 1.151 sec/iter\n",
      "Epoch: 185 | Batch: 001 / 011 | Total loss: 1.042 | Reg loss: 0.025 | Tree loss: 1.042 | Accuracy: 0.651000 | 1.151 sec/iter\n",
      "Epoch: 185 | Batch: 002 / 011 | Total loss: 1.040 | Reg loss: 0.025 | Tree loss: 1.040 | Accuracy: 0.675500 | 1.151 sec/iter\n",
      "Epoch: 185 | Batch: 003 / 011 | Total loss: 1.033 | Reg loss: 0.025 | Tree loss: 1.033 | Accuracy: 0.665500 | 1.151 sec/iter\n",
      "Epoch: 185 | Batch: 004 / 011 | Total loss: 1.006 | Reg loss: 0.025 | Tree loss: 1.006 | Accuracy: 0.678500 | 1.151 sec/iter\n",
      "Epoch: 185 | Batch: 005 / 011 | Total loss: 1.000 | Reg loss: 0.025 | Tree loss: 1.000 | Accuracy: 0.637500 | 1.151 sec/iter\n",
      "Epoch: 185 | Batch: 006 / 011 | Total loss: 0.987 | Reg loss: 0.025 | Tree loss: 0.987 | Accuracy: 0.632000 | 1.151 sec/iter\n",
      "Epoch: 185 | Batch: 007 / 011 | Total loss: 0.971 | Reg loss: 0.025 | Tree loss: 0.971 | Accuracy: 0.642500 | 1.151 sec/iter\n",
      "Epoch: 185 | Batch: 008 / 011 | Total loss: 0.993 | Reg loss: 0.025 | Tree loss: 0.993 | Accuracy: 0.616000 | 1.151 sec/iter\n",
      "Epoch: 185 | Batch: 009 / 011 | Total loss: 0.964 | Reg loss: 0.025 | Tree loss: 0.964 | Accuracy: 0.628500 | 1.151 sec/iter\n",
      "Epoch: 185 | Batch: 010 / 011 | Total loss: 1.048 | Reg loss: 0.025 | Tree loss: 1.048 | Accuracy: 0.607509 | 1.151 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 186 | Batch: 000 / 011 | Total loss: 1.069 | Reg loss: 0.025 | Tree loss: 1.069 | Accuracy: 0.637500 | 1.152 sec/iter\n",
      "Epoch: 186 | Batch: 001 / 011 | Total loss: 1.062 | Reg loss: 0.025 | Tree loss: 1.062 | Accuracy: 0.624500 | 1.152 sec/iter\n",
      "Epoch: 186 | Batch: 002 / 011 | Total loss: 1.034 | Reg loss: 0.025 | Tree loss: 1.034 | Accuracy: 0.670500 | 1.152 sec/iter\n",
      "Epoch: 186 | Batch: 003 / 011 | Total loss: 1.015 | Reg loss: 0.025 | Tree loss: 1.015 | Accuracy: 0.690000 | 1.152 sec/iter\n",
      "Epoch: 186 | Batch: 004 / 011 | Total loss: 0.994 | Reg loss: 0.025 | Tree loss: 0.994 | Accuracy: 0.677500 | 1.152 sec/iter\n",
      "Epoch: 186 | Batch: 005 / 011 | Total loss: 0.975 | Reg loss: 0.025 | Tree loss: 0.975 | Accuracy: 0.676500 | 1.152 sec/iter\n",
      "Epoch: 186 | Batch: 006 / 011 | Total loss: 0.996 | Reg loss: 0.025 | Tree loss: 0.996 | Accuracy: 0.626000 | 1.151 sec/iter\n",
      "Epoch: 186 | Batch: 007 / 011 | Total loss: 0.999 | Reg loss: 0.025 | Tree loss: 0.999 | Accuracy: 0.616000 | 1.151 sec/iter\n",
      "Epoch: 186 | Batch: 008 / 011 | Total loss: 1.018 | Reg loss: 0.025 | Tree loss: 1.018 | Accuracy: 0.592500 | 1.151 sec/iter\n",
      "Epoch: 186 | Batch: 009 / 011 | Total loss: 0.982 | Reg loss: 0.025 | Tree loss: 0.982 | Accuracy: 0.620000 | 1.151 sec/iter\n",
      "Epoch: 186 | Batch: 010 / 011 | Total loss: 1.082 | Reg loss: 0.025 | Tree loss: 1.082 | Accuracy: 0.559727 | 1.151 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 187 | Batch: 000 / 011 | Total loss: 1.079 | Reg loss: 0.025 | Tree loss: 1.079 | Accuracy: 0.637500 | 1.152 sec/iter\n",
      "Epoch: 187 | Batch: 001 / 011 | Total loss: 1.081 | Reg loss: 0.025 | Tree loss: 1.081 | Accuracy: 0.620500 | 1.152 sec/iter\n",
      "Epoch: 187 | Batch: 002 / 011 | Total loss: 1.039 | Reg loss: 0.025 | Tree loss: 1.039 | Accuracy: 0.668500 | 1.152 sec/iter\n",
      "Epoch: 187 | Batch: 003 / 011 | Total loss: 1.003 | Reg loss: 0.025 | Tree loss: 1.003 | Accuracy: 0.688000 | 1.152 sec/iter\n",
      "Epoch: 187 | Batch: 004 / 011 | Total loss: 1.010 | Reg loss: 0.025 | Tree loss: 1.010 | Accuracy: 0.674000 | 1.152 sec/iter\n",
      "Epoch: 187 | Batch: 005 / 011 | Total loss: 1.000 | Reg loss: 0.025 | Tree loss: 1.000 | Accuracy: 0.647500 | 1.152 sec/iter\n",
      "Epoch: 187 | Batch: 006 / 011 | Total loss: 0.962 | Reg loss: 0.025 | Tree loss: 0.962 | Accuracy: 0.660000 | 1.152 sec/iter\n",
      "Epoch: 187 | Batch: 007 / 011 | Total loss: 1.001 | Reg loss: 0.025 | Tree loss: 1.001 | Accuracy: 0.615000 | 1.152 sec/iter\n",
      "Epoch: 187 | Batch: 008 / 011 | Total loss: 0.975 | Reg loss: 0.025 | Tree loss: 0.975 | Accuracy: 0.622500 | 1.152 sec/iter\n",
      "Epoch: 187 | Batch: 009 / 011 | Total loss: 0.980 | Reg loss: 0.025 | Tree loss: 0.980 | Accuracy: 0.622500 | 1.152 sec/iter\n",
      "Epoch: 187 | Batch: 010 / 011 | Total loss: 0.969 | Reg loss: 0.025 | Tree loss: 0.969 | Accuracy: 0.627986 | 1.151 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 188 | Batch: 000 / 011 | Total loss: 1.085 | Reg loss: 0.025 | Tree loss: 1.085 | Accuracy: 0.616500 | 1.151 sec/iter\n",
      "Epoch: 188 | Batch: 001 / 011 | Total loss: 1.056 | Reg loss: 0.025 | Tree loss: 1.056 | Accuracy: 0.657500 | 1.151 sec/iter\n",
      "Epoch: 188 | Batch: 002 / 011 | Total loss: 1.032 | Reg loss: 0.025 | Tree loss: 1.032 | Accuracy: 0.684500 | 1.151 sec/iter\n",
      "Epoch: 188 | Batch: 003 / 011 | Total loss: 1.008 | Reg loss: 0.025 | Tree loss: 1.008 | Accuracy: 0.675500 | 1.151 sec/iter\n",
      "Epoch: 188 | Batch: 004 / 011 | Total loss: 0.993 | Reg loss: 0.025 | Tree loss: 0.993 | Accuracy: 0.675000 | 1.151 sec/iter\n",
      "Epoch: 188 | Batch: 005 / 011 | Total loss: 0.997 | Reg loss: 0.025 | Tree loss: 0.997 | Accuracy: 0.635500 | 1.151 sec/iter\n",
      "Epoch: 188 | Batch: 006 / 011 | Total loss: 0.990 | Reg loss: 0.025 | Tree loss: 0.990 | Accuracy: 0.644000 | 1.151 sec/iter\n",
      "Epoch: 188 | Batch: 007 / 011 | Total loss: 0.972 | Reg loss: 0.025 | Tree loss: 0.972 | Accuracy: 0.636500 | 1.151 sec/iter\n",
      "Epoch: 188 | Batch: 008 / 011 | Total loss: 0.972 | Reg loss: 0.025 | Tree loss: 0.972 | Accuracy: 0.633500 | 1.151 sec/iter\n",
      "Epoch: 188 | Batch: 009 / 011 | Total loss: 0.983 | Reg loss: 0.025 | Tree loss: 0.983 | Accuracy: 0.625000 | 1.151 sec/iter\n",
      "Epoch: 188 | Batch: 010 / 011 | Total loss: 1.000 | Reg loss: 0.025 | Tree loss: 1.000 | Accuracy: 0.587031 | 1.151 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 189 | Batch: 000 / 011 | Total loss: 1.073 | Reg loss: 0.025 | Tree loss: 1.073 | Accuracy: 0.631000 | 1.151 sec/iter\n",
      "Epoch: 189 | Batch: 001 / 011 | Total loss: 1.052 | Reg loss: 0.025 | Tree loss: 1.052 | Accuracy: 0.650000 | 1.151 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 189 | Batch: 002 / 011 | Total loss: 1.030 | Reg loss: 0.025 | Tree loss: 1.030 | Accuracy: 0.681000 | 1.151 sec/iter\n",
      "Epoch: 189 | Batch: 003 / 011 | Total loss: 1.009 | Reg loss: 0.025 | Tree loss: 1.009 | Accuracy: 0.691500 | 1.151 sec/iter\n",
      "Epoch: 189 | Batch: 004 / 011 | Total loss: 0.991 | Reg loss: 0.025 | Tree loss: 0.991 | Accuracy: 0.681000 | 1.151 sec/iter\n",
      "Epoch: 189 | Batch: 005 / 011 | Total loss: 0.983 | Reg loss: 0.025 | Tree loss: 0.983 | Accuracy: 0.663500 | 1.151 sec/iter\n",
      "Epoch: 189 | Batch: 006 / 011 | Total loss: 0.975 | Reg loss: 0.025 | Tree loss: 0.975 | Accuracy: 0.638500 | 1.151 sec/iter\n",
      "Epoch: 189 | Batch: 007 / 011 | Total loss: 0.995 | Reg loss: 0.025 | Tree loss: 0.995 | Accuracy: 0.617000 | 1.15 sec/iter\n",
      "Epoch: 189 | Batch: 008 / 011 | Total loss: 0.995 | Reg loss: 0.025 | Tree loss: 0.995 | Accuracy: 0.626000 | 1.15 sec/iter\n",
      "Epoch: 189 | Batch: 009 / 011 | Total loss: 0.979 | Reg loss: 0.025 | Tree loss: 0.979 | Accuracy: 0.621000 | 1.15 sec/iter\n",
      "Epoch: 189 | Batch: 010 / 011 | Total loss: 0.992 | Reg loss: 0.025 | Tree loss: 0.992 | Accuracy: 0.631399 | 1.15 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 190 | Batch: 000 / 011 | Total loss: 1.077 | Reg loss: 0.025 | Tree loss: 1.077 | Accuracy: 0.635500 | 1.151 sec/iter\n",
      "Epoch: 190 | Batch: 001 / 011 | Total loss: 1.072 | Reg loss: 0.025 | Tree loss: 1.072 | Accuracy: 0.624500 | 1.151 sec/iter\n",
      "Epoch: 190 | Batch: 002 / 011 | Total loss: 1.041 | Reg loss: 0.025 | Tree loss: 1.041 | Accuracy: 0.670000 | 1.151 sec/iter\n",
      "Epoch: 190 | Batch: 003 / 011 | Total loss: 1.018 | Reg loss: 0.025 | Tree loss: 1.018 | Accuracy: 0.680500 | 1.151 sec/iter\n",
      "Epoch: 190 | Batch: 004 / 011 | Total loss: 0.982 | Reg loss: 0.025 | Tree loss: 0.982 | Accuracy: 0.684000 | 1.151 sec/iter\n",
      "Epoch: 190 | Batch: 005 / 011 | Total loss: 1.005 | Reg loss: 0.025 | Tree loss: 1.005 | Accuracy: 0.643500 | 1.151 sec/iter\n",
      "Epoch: 190 | Batch: 006 / 011 | Total loss: 0.964 | Reg loss: 0.025 | Tree loss: 0.964 | Accuracy: 0.647500 | 1.151 sec/iter\n",
      "Epoch: 190 | Batch: 007 / 011 | Total loss: 1.008 | Reg loss: 0.025 | Tree loss: 1.008 | Accuracy: 0.598000 | 1.151 sec/iter\n",
      "Epoch: 190 | Batch: 008 / 011 | Total loss: 0.971 | Reg loss: 0.025 | Tree loss: 0.971 | Accuracy: 0.624500 | 1.151 sec/iter\n",
      "Epoch: 190 | Batch: 009 / 011 | Total loss: 0.957 | Reg loss: 0.025 | Tree loss: 0.957 | Accuracy: 0.631000 | 1.151 sec/iter\n",
      "Epoch: 190 | Batch: 010 / 011 | Total loss: 0.995 | Reg loss: 0.025 | Tree loss: 0.995 | Accuracy: 0.658703 | 1.15 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 191 | Batch: 000 / 011 | Total loss: 1.072 | Reg loss: 0.025 | Tree loss: 1.072 | Accuracy: 0.624500 | 1.151 sec/iter\n",
      "Epoch: 191 | Batch: 001 / 011 | Total loss: 1.062 | Reg loss: 0.025 | Tree loss: 1.062 | Accuracy: 0.640500 | 1.151 sec/iter\n",
      "Epoch: 191 | Batch: 002 / 011 | Total loss: 1.036 | Reg loss: 0.025 | Tree loss: 1.036 | Accuracy: 0.675500 | 1.151 sec/iter\n",
      "Epoch: 191 | Batch: 003 / 011 | Total loss: 1.021 | Reg loss: 0.025 | Tree loss: 1.021 | Accuracy: 0.668500 | 1.151 sec/iter\n",
      "Epoch: 191 | Batch: 004 / 011 | Total loss: 0.975 | Reg loss: 0.025 | Tree loss: 0.975 | Accuracy: 0.681500 | 1.151 sec/iter\n",
      "Epoch: 191 | Batch: 005 / 011 | Total loss: 1.005 | Reg loss: 0.025 | Tree loss: 1.005 | Accuracy: 0.644000 | 1.151 sec/iter\n",
      "Epoch: 191 | Batch: 006 / 011 | Total loss: 0.992 | Reg loss: 0.025 | Tree loss: 0.992 | Accuracy: 0.640000 | 1.151 sec/iter\n",
      "Epoch: 191 | Batch: 007 / 011 | Total loss: 0.984 | Reg loss: 0.025 | Tree loss: 0.984 | Accuracy: 0.640500 | 1.151 sec/iter\n",
      "Epoch: 191 | Batch: 008 / 011 | Total loss: 0.990 | Reg loss: 0.025 | Tree loss: 0.990 | Accuracy: 0.616500 | 1.151 sec/iter\n",
      "Epoch: 191 | Batch: 009 / 011 | Total loss: 0.959 | Reg loss: 0.025 | Tree loss: 0.959 | Accuracy: 0.631500 | 1.151 sec/iter\n",
      "Epoch: 191 | Batch: 010 / 011 | Total loss: 0.969 | Reg loss: 0.025 | Tree loss: 0.969 | Accuracy: 0.638225 | 1.151 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 192 | Batch: 000 / 011 | Total loss: 1.090 | Reg loss: 0.025 | Tree loss: 1.090 | Accuracy: 0.615500 | 1.152 sec/iter\n",
      "Epoch: 192 | Batch: 001 / 011 | Total loss: 1.077 | Reg loss: 0.025 | Tree loss: 1.077 | Accuracy: 0.645000 | 1.152 sec/iter\n",
      "Epoch: 192 | Batch: 002 / 011 | Total loss: 1.007 | Reg loss: 0.025 | Tree loss: 1.007 | Accuracy: 0.694500 | 1.152 sec/iter\n",
      "Epoch: 192 | Batch: 003 / 011 | Total loss: 1.029 | Reg loss: 0.025 | Tree loss: 1.029 | Accuracy: 0.681500 | 1.152 sec/iter\n",
      "Epoch: 192 | Batch: 004 / 011 | Total loss: 1.006 | Reg loss: 0.025 | Tree loss: 1.006 | Accuracy: 0.663000 | 1.152 sec/iter\n",
      "Epoch: 192 | Batch: 005 / 011 | Total loss: 0.990 | Reg loss: 0.025 | Tree loss: 0.990 | Accuracy: 0.666000 | 1.152 sec/iter\n",
      "Epoch: 192 | Batch: 006 / 011 | Total loss: 0.964 | Reg loss: 0.025 | Tree loss: 0.964 | Accuracy: 0.639500 | 1.151 sec/iter\n",
      "Epoch: 192 | Batch: 007 / 011 | Total loss: 0.970 | Reg loss: 0.025 | Tree loss: 0.970 | Accuracy: 0.641000 | 1.151 sec/iter\n",
      "Epoch: 192 | Batch: 008 / 011 | Total loss: 0.974 | Reg loss: 0.025 | Tree loss: 0.974 | Accuracy: 0.634500 | 1.151 sec/iter\n",
      "Epoch: 192 | Batch: 009 / 011 | Total loss: 0.965 | Reg loss: 0.025 | Tree loss: 0.965 | Accuracy: 0.650000 | 1.151 sec/iter\n",
      "Epoch: 192 | Batch: 010 / 011 | Total loss: 0.966 | Reg loss: 0.025 | Tree loss: 0.966 | Accuracy: 0.624573 | 1.151 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 193 | Batch: 000 / 011 | Total loss: 1.055 | Reg loss: 0.025 | Tree loss: 1.055 | Accuracy: 0.624000 | 1.152 sec/iter\n",
      "Epoch: 193 | Batch: 001 / 011 | Total loss: 1.054 | Reg loss: 0.025 | Tree loss: 1.054 | Accuracy: 0.651500 | 1.152 sec/iter\n",
      "Epoch: 193 | Batch: 002 / 011 | Total loss: 1.025 | Reg loss: 0.025 | Tree loss: 1.025 | Accuracy: 0.674000 | 1.152 sec/iter\n",
      "Epoch: 193 | Batch: 003 / 011 | Total loss: 0.999 | Reg loss: 0.025 | Tree loss: 0.999 | Accuracy: 0.695500 | 1.152 sec/iter\n",
      "Epoch: 193 | Batch: 004 / 011 | Total loss: 1.002 | Reg loss: 0.025 | Tree loss: 1.002 | Accuracy: 0.670000 | 1.152 sec/iter\n",
      "Epoch: 193 | Batch: 005 / 011 | Total loss: 0.990 | Reg loss: 0.025 | Tree loss: 0.990 | Accuracy: 0.643500 | 1.152 sec/iter\n",
      "Epoch: 193 | Batch: 006 / 011 | Total loss: 0.996 | Reg loss: 0.025 | Tree loss: 0.996 | Accuracy: 0.624500 | 1.152 sec/iter\n",
      "Epoch: 193 | Batch: 007 / 011 | Total loss: 0.988 | Reg loss: 0.025 | Tree loss: 0.988 | Accuracy: 0.630000 | 1.152 sec/iter\n",
      "Epoch: 193 | Batch: 008 / 011 | Total loss: 0.970 | Reg loss: 0.025 | Tree loss: 0.970 | Accuracy: 0.641000 | 1.152 sec/iter\n",
      "Epoch: 193 | Batch: 009 / 011 | Total loss: 0.994 | Reg loss: 0.025 | Tree loss: 0.994 | Accuracy: 0.627000 | 1.152 sec/iter\n",
      "Epoch: 193 | Batch: 010 / 011 | Total loss: 1.010 | Reg loss: 0.025 | Tree loss: 1.010 | Accuracy: 0.593857 | 1.152 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 194 | Batch: 000 / 011 | Total loss: 1.062 | Reg loss: 0.025 | Tree loss: 1.062 | Accuracy: 0.638500 | 1.152 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 194 | Batch: 001 / 011 | Total loss: 1.063 | Reg loss: 0.025 | Tree loss: 1.063 | Accuracy: 0.647500 | 1.151 sec/iter\n",
      "Epoch: 194 | Batch: 002 / 011 | Total loss: 1.041 | Reg loss: 0.025 | Tree loss: 1.041 | Accuracy: 0.665000 | 1.151 sec/iter\n",
      "Epoch: 194 | Batch: 003 / 011 | Total loss: 1.007 | Reg loss: 0.025 | Tree loss: 1.007 | Accuracy: 0.680500 | 1.151 sec/iter\n",
      "Epoch: 194 | Batch: 004 / 011 | Total loss: 1.001 | Reg loss: 0.025 | Tree loss: 1.001 | Accuracy: 0.665500 | 1.151 sec/iter\n",
      "Epoch: 194 | Batch: 005 / 011 | Total loss: 0.977 | Reg loss: 0.025 | Tree loss: 0.977 | Accuracy: 0.659500 | 1.151 sec/iter\n",
      "Epoch: 194 | Batch: 006 / 011 | Total loss: 0.994 | Reg loss: 0.025 | Tree loss: 0.994 | Accuracy: 0.632500 | 1.151 sec/iter\n",
      "Epoch: 194 | Batch: 007 / 011 | Total loss: 0.993 | Reg loss: 0.025 | Tree loss: 0.993 | Accuracy: 0.615000 | 1.151 sec/iter\n",
      "Epoch: 194 | Batch: 008 / 011 | Total loss: 0.976 | Reg loss: 0.025 | Tree loss: 0.976 | Accuracy: 0.621000 | 1.151 sec/iter\n",
      "Epoch: 194 | Batch: 009 / 011 | Total loss: 0.982 | Reg loss: 0.025 | Tree loss: 0.982 | Accuracy: 0.634500 | 1.151 sec/iter\n",
      "Epoch: 194 | Batch: 010 / 011 | Total loss: 0.981 | Reg loss: 0.025 | Tree loss: 0.981 | Accuracy: 0.607509 | 1.151 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 195 | Batch: 000 / 011 | Total loss: 1.066 | Reg loss: 0.025 | Tree loss: 1.066 | Accuracy: 0.639000 | 1.151 sec/iter\n",
      "Epoch: 195 | Batch: 001 / 011 | Total loss: 1.068 | Reg loss: 0.025 | Tree loss: 1.068 | Accuracy: 0.621000 | 1.151 sec/iter\n",
      "Epoch: 195 | Batch: 002 / 011 | Total loss: 1.044 | Reg loss: 0.025 | Tree loss: 1.044 | Accuracy: 0.643500 | 1.151 sec/iter\n",
      "Epoch: 195 | Batch: 003 / 011 | Total loss: 1.004 | Reg loss: 0.025 | Tree loss: 1.004 | Accuracy: 0.676500 | 1.151 sec/iter\n",
      "Epoch: 195 | Batch: 004 / 011 | Total loss: 1.014 | Reg loss: 0.025 | Tree loss: 1.014 | Accuracy: 0.672000 | 1.151 sec/iter\n",
      "Epoch: 195 | Batch: 005 / 011 | Total loss: 0.990 | Reg loss: 0.025 | Tree loss: 0.990 | Accuracy: 0.662000 | 1.151 sec/iter\n",
      "Epoch: 195 | Batch: 006 / 011 | Total loss: 0.958 | Reg loss: 0.025 | Tree loss: 0.958 | Accuracy: 0.645000 | 1.151 sec/iter\n",
      "Epoch: 195 | Batch: 007 / 011 | Total loss: 0.986 | Reg loss: 0.025 | Tree loss: 0.986 | Accuracy: 0.641500 | 1.151 sec/iter\n",
      "Epoch: 195 | Batch: 008 / 011 | Total loss: 0.980 | Reg loss: 0.025 | Tree loss: 0.980 | Accuracy: 0.621500 | 1.151 sec/iter\n",
      "Epoch: 195 | Batch: 009 / 011 | Total loss: 0.979 | Reg loss: 0.025 | Tree loss: 0.979 | Accuracy: 0.623500 | 1.15 sec/iter\n",
      "Epoch: 195 | Batch: 010 / 011 | Total loss: 0.982 | Reg loss: 0.025 | Tree loss: 0.982 | Accuracy: 0.614334 | 1.15 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 196 | Batch: 000 / 011 | Total loss: 1.063 | Reg loss: 0.025 | Tree loss: 1.063 | Accuracy: 0.638000 | 1.151 sec/iter\n",
      "Epoch: 196 | Batch: 001 / 011 | Total loss: 1.060 | Reg loss: 0.025 | Tree loss: 1.060 | Accuracy: 0.651500 | 1.151 sec/iter\n",
      "Epoch: 196 | Batch: 002 / 011 | Total loss: 1.054 | Reg loss: 0.025 | Tree loss: 1.054 | Accuracy: 0.665000 | 1.151 sec/iter\n",
      "Epoch: 196 | Batch: 003 / 011 | Total loss: 1.016 | Reg loss: 0.025 | Tree loss: 1.016 | Accuracy: 0.674500 | 1.151 sec/iter\n",
      "Epoch: 196 | Batch: 004 / 011 | Total loss: 0.990 | Reg loss: 0.025 | Tree loss: 0.990 | Accuracy: 0.674000 | 1.151 sec/iter\n",
      "Epoch: 196 | Batch: 005 / 011 | Total loss: 0.969 | Reg loss: 0.025 | Tree loss: 0.969 | Accuracy: 0.663500 | 1.151 sec/iter\n",
      "Epoch: 196 | Batch: 006 / 011 | Total loss: 0.995 | Reg loss: 0.025 | Tree loss: 0.995 | Accuracy: 0.629000 | 1.151 sec/iter\n",
      "Epoch: 196 | Batch: 007 / 011 | Total loss: 0.979 | Reg loss: 0.025 | Tree loss: 0.979 | Accuracy: 0.621000 | 1.151 sec/iter\n",
      "Epoch: 196 | Batch: 008 / 011 | Total loss: 0.996 | Reg loss: 0.025 | Tree loss: 0.996 | Accuracy: 0.619500 | 1.151 sec/iter\n",
      "Epoch: 196 | Batch: 009 / 011 | Total loss: 0.971 | Reg loss: 0.025 | Tree loss: 0.971 | Accuracy: 0.632000 | 1.151 sec/iter\n",
      "Epoch: 196 | Batch: 010 / 011 | Total loss: 1.002 | Reg loss: 0.025 | Tree loss: 1.002 | Accuracy: 0.621160 | 1.15 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 197 | Batch: 000 / 011 | Total loss: 1.068 | Reg loss: 0.025 | Tree loss: 1.068 | Accuracy: 0.621000 | 1.151 sec/iter\n",
      "Epoch: 197 | Batch: 001 / 011 | Total loss: 1.024 | Reg loss: 0.025 | Tree loss: 1.024 | Accuracy: 0.659500 | 1.151 sec/iter\n",
      "Epoch: 197 | Batch: 002 / 011 | Total loss: 1.033 | Reg loss: 0.025 | Tree loss: 1.033 | Accuracy: 0.689000 | 1.151 sec/iter\n",
      "Epoch: 197 | Batch: 003 / 011 | Total loss: 1.011 | Reg loss: 0.025 | Tree loss: 1.011 | Accuracy: 0.681500 | 1.151 sec/iter\n",
      "Epoch: 197 | Batch: 004 / 011 | Total loss: 0.986 | Reg loss: 0.025 | Tree loss: 0.986 | Accuracy: 0.668000 | 1.151 sec/iter\n",
      "Epoch: 197 | Batch: 005 / 011 | Total loss: 1.010 | Reg loss: 0.025 | Tree loss: 1.010 | Accuracy: 0.649000 | 1.151 sec/iter\n",
      "Epoch: 197 | Batch: 006 / 011 | Total loss: 0.971 | Reg loss: 0.025 | Tree loss: 0.971 | Accuracy: 0.651000 | 1.151 sec/iter\n",
      "Epoch: 197 | Batch: 007 / 011 | Total loss: 0.989 | Reg loss: 0.025 | Tree loss: 0.989 | Accuracy: 0.638000 | 1.151 sec/iter\n",
      "Epoch: 197 | Batch: 008 / 011 | Total loss: 0.980 | Reg loss: 0.025 | Tree loss: 0.980 | Accuracy: 0.634500 | 1.151 sec/iter\n",
      "Epoch: 197 | Batch: 009 / 011 | Total loss: 0.985 | Reg loss: 0.025 | Tree loss: 0.985 | Accuracy: 0.635000 | 1.151 sec/iter\n",
      "Epoch: 197 | Batch: 010 / 011 | Total loss: 0.915 | Reg loss: 0.025 | Tree loss: 0.915 | Accuracy: 0.645051 | 1.151 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 198 | Batch: 000 / 011 | Total loss: 1.072 | Reg loss: 0.025 | Tree loss: 1.072 | Accuracy: 0.631000 | 1.152 sec/iter\n",
      "Epoch: 198 | Batch: 001 / 011 | Total loss: 1.047 | Reg loss: 0.025 | Tree loss: 1.047 | Accuracy: 0.656000 | 1.152 sec/iter\n",
      "Epoch: 198 | Batch: 002 / 011 | Total loss: 1.028 | Reg loss: 0.025 | Tree loss: 1.028 | Accuracy: 0.678000 | 1.152 sec/iter\n",
      "Epoch: 198 | Batch: 003 / 011 | Total loss: 0.978 | Reg loss: 0.025 | Tree loss: 0.978 | Accuracy: 0.700500 | 1.152 sec/iter\n",
      "Epoch: 198 | Batch: 004 / 011 | Total loss: 1.013 | Reg loss: 0.025 | Tree loss: 1.013 | Accuracy: 0.657000 | 1.152 sec/iter\n",
      "Epoch: 198 | Batch: 005 / 011 | Total loss: 0.977 | Reg loss: 0.025 | Tree loss: 0.977 | Accuracy: 0.652500 | 1.152 sec/iter\n",
      "Epoch: 198 | Batch: 006 / 011 | Total loss: 0.973 | Reg loss: 0.025 | Tree loss: 0.973 | Accuracy: 0.635000 | 1.152 sec/iter\n",
      "Epoch: 198 | Batch: 007 / 011 | Total loss: 0.973 | Reg loss: 0.025 | Tree loss: 0.973 | Accuracy: 0.628000 | 1.151 sec/iter\n",
      "Epoch: 198 | Batch: 008 / 011 | Total loss: 0.983 | Reg loss: 0.025 | Tree loss: 0.983 | Accuracy: 0.607500 | 1.151 sec/iter\n",
      "Epoch: 198 | Batch: 009 / 011 | Total loss: 0.994 | Reg loss: 0.025 | Tree loss: 0.994 | Accuracy: 0.608000 | 1.151 sec/iter\n",
      "Epoch: 198 | Batch: 010 / 011 | Total loss: 0.929 | Reg loss: 0.025 | Tree loss: 0.929 | Accuracy: 0.665529 | 1.151 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 199 | Batch: 000 / 011 | Total loss: 1.055 | Reg loss: 0.025 | Tree loss: 1.055 | Accuracy: 0.631000 | 1.152 sec/iter\n",
      "Epoch: 199 | Batch: 001 / 011 | Total loss: 1.073 | Reg loss: 0.025 | Tree loss: 1.073 | Accuracy: 0.635500 | 1.152 sec/iter\n",
      "Epoch: 199 | Batch: 002 / 011 | Total loss: 1.034 | Reg loss: 0.025 | Tree loss: 1.034 | Accuracy: 0.666500 | 1.152 sec/iter\n",
      "Epoch: 199 | Batch: 003 / 011 | Total loss: 1.001 | Reg loss: 0.025 | Tree loss: 1.001 | Accuracy: 0.670500 | 1.152 sec/iter\n",
      "Epoch: 199 | Batch: 004 / 011 | Total loss: 0.993 | Reg loss: 0.025 | Tree loss: 0.993 | Accuracy: 0.672000 | 1.152 sec/iter\n",
      "Epoch: 199 | Batch: 005 / 011 | Total loss: 0.960 | Reg loss: 0.025 | Tree loss: 0.960 | Accuracy: 0.668000 | 1.152 sec/iter\n",
      "Epoch: 199 | Batch: 006 / 011 | Total loss: 0.976 | Reg loss: 0.025 | Tree loss: 0.976 | Accuracy: 0.637000 | 1.152 sec/iter\n",
      "Epoch: 199 | Batch: 007 / 011 | Total loss: 1.001 | Reg loss: 0.025 | Tree loss: 1.001 | Accuracy: 0.616500 | 1.152 sec/iter\n",
      "Epoch: 199 | Batch: 008 / 011 | Total loss: 0.984 | Reg loss: 0.025 | Tree loss: 0.984 | Accuracy: 0.624000 | 1.152 sec/iter\n",
      "Epoch: 199 | Batch: 009 / 011 | Total loss: 0.974 | Reg loss: 0.025 | Tree loss: 0.974 | Accuracy: 0.623000 | 1.152 sec/iter\n",
      "Epoch: 199 | Batch: 010 / 011 | Total loss: 0.944 | Reg loss: 0.025 | Tree loss: 0.944 | Accuracy: 0.662116 | 1.152 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 200 | Batch: 000 / 011 | Total loss: 1.079 | Reg loss: 0.025 | Tree loss: 1.079 | Accuracy: 0.614000 | 1.152 sec/iter\n",
      "Epoch: 200 | Batch: 001 / 011 | Total loss: 1.046 | Reg loss: 0.025 | Tree loss: 1.046 | Accuracy: 0.659000 | 1.152 sec/iter\n",
      "Epoch: 200 | Batch: 002 / 011 | Total loss: 1.016 | Reg loss: 0.025 | Tree loss: 1.016 | Accuracy: 0.687000 | 1.151 sec/iter\n",
      "Epoch: 200 | Batch: 003 / 011 | Total loss: 1.016 | Reg loss: 0.025 | Tree loss: 1.016 | Accuracy: 0.684500 | 1.151 sec/iter\n",
      "Epoch: 200 | Batch: 004 / 011 | Total loss: 1.003 | Reg loss: 0.025 | Tree loss: 1.003 | Accuracy: 0.658500 | 1.151 sec/iter\n",
      "Epoch: 200 | Batch: 005 / 011 | Total loss: 0.976 | Reg loss: 0.025 | Tree loss: 0.976 | Accuracy: 0.659500 | 1.151 sec/iter\n",
      "Epoch: 200 | Batch: 006 / 011 | Total loss: 0.975 | Reg loss: 0.025 | Tree loss: 0.975 | Accuracy: 0.641000 | 1.151 sec/iter\n",
      "Epoch: 200 | Batch: 007 / 011 | Total loss: 0.962 | Reg loss: 0.025 | Tree loss: 0.962 | Accuracy: 0.648000 | 1.151 sec/iter\n",
      "Epoch: 200 | Batch: 008 / 011 | Total loss: 0.981 | Reg loss: 0.025 | Tree loss: 0.981 | Accuracy: 0.625000 | 1.151 sec/iter\n",
      "Epoch: 200 | Batch: 009 / 011 | Total loss: 0.982 | Reg loss: 0.025 | Tree loss: 0.982 | Accuracy: 0.618500 | 1.151 sec/iter\n",
      "Epoch: 200 | Batch: 010 / 011 | Total loss: 0.974 | Reg loss: 0.025 | Tree loss: 0.974 | Accuracy: 0.624573 | 1.151 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 201 | Batch: 000 / 011 | Total loss: 1.063 | Reg loss: 0.025 | Tree loss: 1.063 | Accuracy: 0.632500 | 1.151 sec/iter\n",
      "Epoch: 201 | Batch: 001 / 011 | Total loss: 1.052 | Reg loss: 0.025 | Tree loss: 1.052 | Accuracy: 0.645000 | 1.151 sec/iter\n",
      "Epoch: 201 | Batch: 002 / 011 | Total loss: 1.034 | Reg loss: 0.025 | Tree loss: 1.034 | Accuracy: 0.652000 | 1.151 sec/iter\n",
      "Epoch: 201 | Batch: 003 / 011 | Total loss: 1.006 | Reg loss: 0.025 | Tree loss: 1.006 | Accuracy: 0.690500 | 1.151 sec/iter\n",
      "Epoch: 201 | Batch: 004 / 011 | Total loss: 1.007 | Reg loss: 0.025 | Tree loss: 1.007 | Accuracy: 0.677000 | 1.151 sec/iter\n",
      "Epoch: 201 | Batch: 005 / 011 | Total loss: 0.988 | Reg loss: 0.025 | Tree loss: 0.988 | Accuracy: 0.661500 | 1.151 sec/iter\n",
      "Epoch: 201 | Batch: 006 / 011 | Total loss: 0.981 | Reg loss: 0.025 | Tree loss: 0.981 | Accuracy: 0.636500 | 1.151 sec/iter\n",
      "Epoch: 201 | Batch: 007 / 011 | Total loss: 0.971 | Reg loss: 0.025 | Tree loss: 0.971 | Accuracy: 0.632000 | 1.151 sec/iter\n",
      "Epoch: 201 | Batch: 008 / 011 | Total loss: 0.964 | Reg loss: 0.025 | Tree loss: 0.964 | Accuracy: 0.638500 | 1.151 sec/iter\n",
      "Epoch: 201 | Batch: 009 / 011 | Total loss: 0.975 | Reg loss: 0.025 | Tree loss: 0.975 | Accuracy: 0.626000 | 1.151 sec/iter\n",
      "Epoch: 201 | Batch: 010 / 011 | Total loss: 0.970 | Reg loss: 0.025 | Tree loss: 0.970 | Accuracy: 0.607509 | 1.151 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 202 | Batch: 000 / 011 | Total loss: 1.068 | Reg loss: 0.025 | Tree loss: 1.068 | Accuracy: 0.630000 | 1.152 sec/iter\n",
      "Epoch: 202 | Batch: 001 / 011 | Total loss: 1.056 | Reg loss: 0.025 | Tree loss: 1.056 | Accuracy: 0.615000 | 1.152 sec/iter\n",
      "Epoch: 202 | Batch: 002 / 011 | Total loss: 1.030 | Reg loss: 0.025 | Tree loss: 1.030 | Accuracy: 0.652500 | 1.151 sec/iter\n",
      "Epoch: 202 | Batch: 003 / 011 | Total loss: 0.995 | Reg loss: 0.025 | Tree loss: 0.995 | Accuracy: 0.681000 | 1.151 sec/iter\n",
      "Epoch: 202 | Batch: 004 / 011 | Total loss: 1.001 | Reg loss: 0.025 | Tree loss: 1.001 | Accuracy: 0.677500 | 1.151 sec/iter\n",
      "Epoch: 202 | Batch: 005 / 011 | Total loss: 0.981 | Reg loss: 0.025 | Tree loss: 0.981 | Accuracy: 0.663500 | 1.151 sec/iter\n",
      "Epoch: 202 | Batch: 006 / 011 | Total loss: 0.986 | Reg loss: 0.025 | Tree loss: 0.986 | Accuracy: 0.636500 | 1.151 sec/iter\n",
      "Epoch: 202 | Batch: 007 / 011 | Total loss: 0.974 | Reg loss: 0.025 | Tree loss: 0.974 | Accuracy: 0.637500 | 1.151 sec/iter\n",
      "Epoch: 202 | Batch: 008 / 011 | Total loss: 0.978 | Reg loss: 0.025 | Tree loss: 0.978 | Accuracy: 0.628000 | 1.151 sec/iter\n",
      "Epoch: 202 | Batch: 009 / 011 | Total loss: 0.971 | Reg loss: 0.025 | Tree loss: 0.971 | Accuracy: 0.627500 | 1.151 sec/iter\n",
      "Epoch: 202 | Batch: 010 / 011 | Total loss: 0.979 | Reg loss: 0.025 | Tree loss: 0.979 | Accuracy: 0.597270 | 1.151 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 203 | Batch: 000 / 011 | Total loss: 1.066 | Reg loss: 0.025 | Tree loss: 1.066 | Accuracy: 0.629000 | 1.152 sec/iter\n",
      "Epoch: 203 | Batch: 001 / 011 | Total loss: 1.053 | Reg loss: 0.025 | Tree loss: 1.053 | Accuracy: 0.640500 | 1.152 sec/iter\n",
      "Epoch: 203 | Batch: 002 / 011 | Total loss: 1.039 | Reg loss: 0.025 | Tree loss: 1.039 | Accuracy: 0.678500 | 1.152 sec/iter\n",
      "Epoch: 203 | Batch: 003 / 011 | Total loss: 1.008 | Reg loss: 0.025 | Tree loss: 1.008 | Accuracy: 0.692000 | 1.152 sec/iter\n",
      "Epoch: 203 | Batch: 004 / 011 | Total loss: 0.985 | Reg loss: 0.025 | Tree loss: 0.985 | Accuracy: 0.693500 | 1.152 sec/iter\n",
      "Epoch: 203 | Batch: 005 / 011 | Total loss: 0.965 | Reg loss: 0.025 | Tree loss: 0.965 | Accuracy: 0.669500 | 1.152 sec/iter\n",
      "Epoch: 203 | Batch: 006 / 011 | Total loss: 0.986 | Reg loss: 0.025 | Tree loss: 0.986 | Accuracy: 0.634500 | 1.152 sec/iter\n",
      "Epoch: 203 | Batch: 007 / 011 | Total loss: 0.971 | Reg loss: 0.025 | Tree loss: 0.971 | Accuracy: 0.627500 | 1.152 sec/iter\n",
      "Epoch: 203 | Batch: 008 / 011 | Total loss: 0.969 | Reg loss: 0.025 | Tree loss: 0.969 | Accuracy: 0.629000 | 1.152 sec/iter\n",
      "Epoch: 203 | Batch: 009 / 011 | Total loss: 0.968 | Reg loss: 0.025 | Tree loss: 0.968 | Accuracy: 0.631000 | 1.152 sec/iter\n",
      "Epoch: 203 | Batch: 010 / 011 | Total loss: 0.966 | Reg loss: 0.025 | Tree loss: 0.966 | Accuracy: 0.621160 | 1.152 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 204 | Batch: 000 / 011 | Total loss: 1.057 | Reg loss: 0.025 | Tree loss: 1.057 | Accuracy: 0.624500 | 1.153 sec/iter\n",
      "Epoch: 204 | Batch: 001 / 011 | Total loss: 1.049 | Reg loss: 0.025 | Tree loss: 1.049 | Accuracy: 0.660500 | 1.153 sec/iter\n",
      "Epoch: 204 | Batch: 002 / 011 | Total loss: 1.022 | Reg loss: 0.025 | Tree loss: 1.022 | Accuracy: 0.668000 | 1.153 sec/iter\n",
      "Epoch: 204 | Batch: 003 / 011 | Total loss: 0.983 | Reg loss: 0.025 | Tree loss: 0.983 | Accuracy: 0.695500 | 1.153 sec/iter\n",
      "Epoch: 204 | Batch: 004 / 011 | Total loss: 0.976 | Reg loss: 0.025 | Tree loss: 0.976 | Accuracy: 0.683500 | 1.153 sec/iter\n",
      "Epoch: 204 | Batch: 005 / 011 | Total loss: 0.951 | Reg loss: 0.025 | Tree loss: 0.951 | Accuracy: 0.659500 | 1.153 sec/iter\n",
      "Epoch: 204 | Batch: 006 / 011 | Total loss: 0.990 | Reg loss: 0.025 | Tree loss: 0.990 | Accuracy: 0.646000 | 1.153 sec/iter\n",
      "Epoch: 204 | Batch: 007 / 011 | Total loss: 0.992 | Reg loss: 0.025 | Tree loss: 0.992 | Accuracy: 0.636000 | 1.154 sec/iter\n",
      "Epoch: 204 | Batch: 008 / 011 | Total loss: 1.012 | Reg loss: 0.025 | Tree loss: 1.012 | Accuracy: 0.593000 | 1.154 sec/iter\n",
      "Epoch: 204 | Batch: 009 / 011 | Total loss: 0.970 | Reg loss: 0.025 | Tree loss: 0.970 | Accuracy: 0.635000 | 1.154 sec/iter\n",
      "Epoch: 204 | Batch: 010 / 011 | Total loss: 0.980 | Reg loss: 0.025 | Tree loss: 0.980 | Accuracy: 0.621160 | 1.154 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 205 | Batch: 000 / 011 | Total loss: 1.050 | Reg loss: 0.025 | Tree loss: 1.050 | Accuracy: 0.652500 | 1.154 sec/iter\n",
      "Epoch: 205 | Batch: 001 / 011 | Total loss: 1.047 | Reg loss: 0.025 | Tree loss: 1.047 | Accuracy: 0.633500 | 1.154 sec/iter\n",
      "Epoch: 205 | Batch: 002 / 011 | Total loss: 1.024 | Reg loss: 0.025 | Tree loss: 1.024 | Accuracy: 0.660500 | 1.155 sec/iter\n",
      "Epoch: 205 | Batch: 003 / 011 | Total loss: 1.028 | Reg loss: 0.025 | Tree loss: 1.028 | Accuracy: 0.676500 | 1.155 sec/iter\n",
      "Epoch: 205 | Batch: 004 / 011 | Total loss: 0.995 | Reg loss: 0.025 | Tree loss: 0.995 | Accuracy: 0.685000 | 1.155 sec/iter\n",
      "Epoch: 205 | Batch: 005 / 011 | Total loss: 0.960 | Reg loss: 0.025 | Tree loss: 0.960 | Accuracy: 0.677500 | 1.155 sec/iter\n",
      "Epoch: 205 | Batch: 006 / 011 | Total loss: 0.976 | Reg loss: 0.025 | Tree loss: 0.976 | Accuracy: 0.641000 | 1.155 sec/iter\n",
      "Epoch: 205 | Batch: 007 / 011 | Total loss: 0.981 | Reg loss: 0.025 | Tree loss: 0.981 | Accuracy: 0.618500 | 1.155 sec/iter\n",
      "Epoch: 205 | Batch: 008 / 011 | Total loss: 0.973 | Reg loss: 0.025 | Tree loss: 0.973 | Accuracy: 0.628000 | 1.155 sec/iter\n",
      "Epoch: 205 | Batch: 009 / 011 | Total loss: 0.965 | Reg loss: 0.025 | Tree loss: 0.965 | Accuracy: 0.631500 | 1.155 sec/iter\n",
      "Epoch: 205 | Batch: 010 / 011 | Total loss: 1.035 | Reg loss: 0.025 | Tree loss: 1.035 | Accuracy: 0.576792 | 1.156 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 206 | Batch: 000 / 011 | Total loss: 1.045 | Reg loss: 0.025 | Tree loss: 1.045 | Accuracy: 0.644500 | 1.156 sec/iter\n",
      "Epoch: 206 | Batch: 001 / 011 | Total loss: 1.056 | Reg loss: 0.025 | Tree loss: 1.056 | Accuracy: 0.654500 | 1.156 sec/iter\n",
      "Epoch: 206 | Batch: 002 / 011 | Total loss: 1.021 | Reg loss: 0.025 | Tree loss: 1.021 | Accuracy: 0.683500 | 1.156 sec/iter\n",
      "Epoch: 206 | Batch: 003 / 011 | Total loss: 0.995 | Reg loss: 0.025 | Tree loss: 0.995 | Accuracy: 0.704000 | 1.156 sec/iter\n",
      "Epoch: 206 | Batch: 004 / 011 | Total loss: 0.971 | Reg loss: 0.025 | Tree loss: 0.971 | Accuracy: 0.687000 | 1.156 sec/iter\n",
      "Epoch: 206 | Batch: 005 / 011 | Total loss: 0.975 | Reg loss: 0.025 | Tree loss: 0.975 | Accuracy: 0.667500 | 1.156 sec/iter\n",
      "Epoch: 206 | Batch: 006 / 011 | Total loss: 1.006 | Reg loss: 0.025 | Tree loss: 1.006 | Accuracy: 0.632000 | 1.157 sec/iter\n",
      "Epoch: 206 | Batch: 007 / 011 | Total loss: 0.968 | Reg loss: 0.025 | Tree loss: 0.968 | Accuracy: 0.641500 | 1.157 sec/iter\n",
      "Epoch: 206 | Batch: 008 / 011 | Total loss: 0.975 | Reg loss: 0.025 | Tree loss: 0.975 | Accuracy: 0.634000 | 1.157 sec/iter\n",
      "Epoch: 206 | Batch: 009 / 011 | Total loss: 0.995 | Reg loss: 0.025 | Tree loss: 0.995 | Accuracy: 0.604500 | 1.157 sec/iter\n",
      "Epoch: 206 | Batch: 010 / 011 | Total loss: 0.899 | Reg loss: 0.025 | Tree loss: 0.899 | Accuracy: 0.665529 | 1.157 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 207 | Batch: 000 / 011 | Total loss: 1.064 | Reg loss: 0.025 | Tree loss: 1.064 | Accuracy: 0.631500 | 1.159 sec/iter\n",
      "Epoch: 207 | Batch: 001 / 011 | Total loss: 1.037 | Reg loss: 0.025 | Tree loss: 1.037 | Accuracy: 0.660500 | 1.159 sec/iter\n",
      "Epoch: 207 | Batch: 002 / 011 | Total loss: 1.050 | Reg loss: 0.025 | Tree loss: 1.050 | Accuracy: 0.665000 | 1.159 sec/iter\n",
      "Epoch: 207 | Batch: 003 / 011 | Total loss: 0.996 | Reg loss: 0.025 | Tree loss: 0.996 | Accuracy: 0.694500 | 1.159 sec/iter\n",
      "Epoch: 207 | Batch: 004 / 011 | Total loss: 0.997 | Reg loss: 0.025 | Tree loss: 0.997 | Accuracy: 0.673500 | 1.159 sec/iter\n",
      "Epoch: 207 | Batch: 005 / 011 | Total loss: 0.975 | Reg loss: 0.025 | Tree loss: 0.975 | Accuracy: 0.666500 | 1.159 sec/iter\n",
      "Epoch: 207 | Batch: 006 / 011 | Total loss: 0.982 | Reg loss: 0.025 | Tree loss: 0.982 | Accuracy: 0.650500 | 1.159 sec/iter\n",
      "Epoch: 207 | Batch: 007 / 011 | Total loss: 0.985 | Reg loss: 0.025 | Tree loss: 0.985 | Accuracy: 0.625000 | 1.159 sec/iter\n",
      "Epoch: 207 | Batch: 008 / 011 | Total loss: 0.958 | Reg loss: 0.025 | Tree loss: 0.958 | Accuracy: 0.642500 | 1.16 sec/iter\n",
      "Epoch: 207 | Batch: 009 / 011 | Total loss: 0.948 | Reg loss: 0.025 | Tree loss: 0.948 | Accuracy: 0.634000 | 1.16 sec/iter\n",
      "Epoch: 207 | Batch: 010 / 011 | Total loss: 0.881 | Reg loss: 0.025 | Tree loss: 0.881 | Accuracy: 0.692833 | 1.16 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 208 | Batch: 000 / 011 | Total loss: 1.092 | Reg loss: 0.025 | Tree loss: 1.092 | Accuracy: 0.616500 | 1.16 sec/iter\n",
      "Epoch: 208 | Batch: 001 / 011 | Total loss: 1.049 | Reg loss: 0.025 | Tree loss: 1.049 | Accuracy: 0.638500 | 1.16 sec/iter\n",
      "Epoch: 208 | Batch: 002 / 011 | Total loss: 1.023 | Reg loss: 0.025 | Tree loss: 1.023 | Accuracy: 0.678000 | 1.16 sec/iter\n",
      "Epoch: 208 | Batch: 003 / 011 | Total loss: 1.006 | Reg loss: 0.025 | Tree loss: 1.006 | Accuracy: 0.691000 | 1.16 sec/iter\n",
      "Epoch: 208 | Batch: 004 / 011 | Total loss: 0.985 | Reg loss: 0.025 | Tree loss: 0.985 | Accuracy: 0.685000 | 1.161 sec/iter\n",
      "Epoch: 208 | Batch: 005 / 011 | Total loss: 0.955 | Reg loss: 0.025 | Tree loss: 0.955 | Accuracy: 0.671000 | 1.161 sec/iter\n",
      "Epoch: 208 | Batch: 006 / 011 | Total loss: 0.995 | Reg loss: 0.025 | Tree loss: 0.995 | Accuracy: 0.637000 | 1.161 sec/iter\n",
      "Epoch: 208 | Batch: 007 / 011 | Total loss: 0.967 | Reg loss: 0.025 | Tree loss: 0.967 | Accuracy: 0.624500 | 1.161 sec/iter\n",
      "Epoch: 208 | Batch: 008 / 011 | Total loss: 0.957 | Reg loss: 0.025 | Tree loss: 0.957 | Accuracy: 0.633000 | 1.161 sec/iter\n",
      "Epoch: 208 | Batch: 009 / 011 | Total loss: 0.966 | Reg loss: 0.025 | Tree loss: 0.966 | Accuracy: 0.633500 | 1.161 sec/iter\n",
      "Epoch: 208 | Batch: 010 / 011 | Total loss: 0.891 | Reg loss: 0.025 | Tree loss: 0.891 | Accuracy: 0.665529 | 1.161 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 209 | Batch: 000 / 011 | Total loss: 1.080 | Reg loss: 0.025 | Tree loss: 1.080 | Accuracy: 0.635500 | 1.163 sec/iter\n",
      "Epoch: 209 | Batch: 001 / 011 | Total loss: 1.053 | Reg loss: 0.025 | Tree loss: 1.053 | Accuracy: 0.640000 | 1.163 sec/iter\n",
      "Epoch: 209 | Batch: 002 / 011 | Total loss: 1.032 | Reg loss: 0.025 | Tree loss: 1.032 | Accuracy: 0.674500 | 1.163 sec/iter\n",
      "Epoch: 209 | Batch: 003 / 011 | Total loss: 1.001 | Reg loss: 0.025 | Tree loss: 1.001 | Accuracy: 0.681000 | 1.163 sec/iter\n",
      "Epoch: 209 | Batch: 004 / 011 | Total loss: 0.969 | Reg loss: 0.025 | Tree loss: 0.969 | Accuracy: 0.685000 | 1.163 sec/iter\n",
      "Epoch: 209 | Batch: 005 / 011 | Total loss: 0.973 | Reg loss: 0.025 | Tree loss: 0.973 | Accuracy: 0.657000 | 1.163 sec/iter\n",
      "Epoch: 209 | Batch: 006 / 011 | Total loss: 0.979 | Reg loss: 0.025 | Tree loss: 0.979 | Accuracy: 0.623000 | 1.164 sec/iter\n",
      "Epoch: 209 | Batch: 007 / 011 | Total loss: 0.990 | Reg loss: 0.025 | Tree loss: 0.990 | Accuracy: 0.634500 | 1.164 sec/iter\n",
      "Epoch: 209 | Batch: 008 / 011 | Total loss: 0.950 | Reg loss: 0.025 | Tree loss: 0.950 | Accuracy: 0.641000 | 1.164 sec/iter\n",
      "Epoch: 209 | Batch: 009 / 011 | Total loss: 0.955 | Reg loss: 0.025 | Tree loss: 0.955 | Accuracy: 0.639500 | 1.164 sec/iter\n",
      "Epoch: 209 | Batch: 010 / 011 | Total loss: 0.951 | Reg loss: 0.025 | Tree loss: 0.951 | Accuracy: 0.645051 | 1.164 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 210 | Batch: 000 / 011 | Total loss: 1.081 | Reg loss: 0.025 | Tree loss: 1.081 | Accuracy: 0.613000 | 1.164 sec/iter\n",
      "Epoch: 210 | Batch: 001 / 011 | Total loss: 1.071 | Reg loss: 0.025 | Tree loss: 1.071 | Accuracy: 0.633000 | 1.164 sec/iter\n",
      "Epoch: 210 | Batch: 002 / 011 | Total loss: 1.017 | Reg loss: 0.025 | Tree loss: 1.017 | Accuracy: 0.683000 | 1.165 sec/iter\n",
      "Epoch: 210 | Batch: 003 / 011 | Total loss: 1.005 | Reg loss: 0.025 | Tree loss: 1.005 | Accuracy: 0.676500 | 1.165 sec/iter\n",
      "Epoch: 210 | Batch: 004 / 011 | Total loss: 0.967 | Reg loss: 0.025 | Tree loss: 0.967 | Accuracy: 0.684000 | 1.165 sec/iter\n",
      "Epoch: 210 | Batch: 005 / 011 | Total loss: 0.969 | Reg loss: 0.025 | Tree loss: 0.969 | Accuracy: 0.664000 | 1.165 sec/iter\n",
      "Epoch: 210 | Batch: 006 / 011 | Total loss: 0.974 | Reg loss: 0.025 | Tree loss: 0.974 | Accuracy: 0.639500 | 1.165 sec/iter\n",
      "Epoch: 210 | Batch: 007 / 011 | Total loss: 0.952 | Reg loss: 0.025 | Tree loss: 0.952 | Accuracy: 0.635500 | 1.165 sec/iter\n",
      "Epoch: 210 | Batch: 008 / 011 | Total loss: 0.977 | Reg loss: 0.025 | Tree loss: 0.977 | Accuracy: 0.636500 | 1.165 sec/iter\n",
      "Epoch: 210 | Batch: 009 / 011 | Total loss: 0.965 | Reg loss: 0.025 | Tree loss: 0.965 | Accuracy: 0.631500 | 1.165 sec/iter\n",
      "Epoch: 210 | Batch: 010 / 011 | Total loss: 0.949 | Reg loss: 0.025 | Tree loss: 0.949 | Accuracy: 0.655290 | 1.165 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 211 | Batch: 000 / 011 | Total loss: 1.048 | Reg loss: 0.025 | Tree loss: 1.048 | Accuracy: 0.636500 | 1.166 sec/iter\n",
      "Epoch: 211 | Batch: 001 / 011 | Total loss: 1.014 | Reg loss: 0.025 | Tree loss: 1.014 | Accuracy: 0.670000 | 1.166 sec/iter\n",
      "Epoch: 211 | Batch: 002 / 011 | Total loss: 1.018 | Reg loss: 0.025 | Tree loss: 1.018 | Accuracy: 0.680500 | 1.166 sec/iter\n",
      "Epoch: 211 | Batch: 003 / 011 | Total loss: 1.004 | Reg loss: 0.025 | Tree loss: 1.004 | Accuracy: 0.689500 | 1.166 sec/iter\n",
      "Epoch: 211 | Batch: 004 / 011 | Total loss: 0.996 | Reg loss: 0.025 | Tree loss: 0.996 | Accuracy: 0.679000 | 1.166 sec/iter\n",
      "Epoch: 211 | Batch: 005 / 011 | Total loss: 0.985 | Reg loss: 0.025 | Tree loss: 0.985 | Accuracy: 0.656000 | 1.166 sec/iter\n",
      "Epoch: 211 | Batch: 006 / 011 | Total loss: 0.985 | Reg loss: 0.025 | Tree loss: 0.985 | Accuracy: 0.640000 | 1.166 sec/iter\n",
      "Epoch: 211 | Batch: 007 / 011 | Total loss: 0.987 | Reg loss: 0.025 | Tree loss: 0.987 | Accuracy: 0.623000 | 1.166 sec/iter\n",
      "Epoch: 211 | Batch: 008 / 011 | Total loss: 0.967 | Reg loss: 0.025 | Tree loss: 0.967 | Accuracy: 0.631500 | 1.166 sec/iter\n",
      "Epoch: 211 | Batch: 009 / 011 | Total loss: 0.963 | Reg loss: 0.025 | Tree loss: 0.963 | Accuracy: 0.623000 | 1.167 sec/iter\n",
      "Epoch: 211 | Batch: 010 / 011 | Total loss: 0.979 | Reg loss: 0.025 | Tree loss: 0.979 | Accuracy: 0.648464 | 1.167 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 212 | Batch: 000 / 011 | Total loss: 1.038 | Reg loss: 0.025 | Tree loss: 1.038 | Accuracy: 0.651500 | 1.167 sec/iter\n",
      "Epoch: 212 | Batch: 001 / 011 | Total loss: 1.054 | Reg loss: 0.025 | Tree loss: 1.054 | Accuracy: 0.645500 | 1.167 sec/iter\n",
      "Epoch: 212 | Batch: 002 / 011 | Total loss: 1.019 | Reg loss: 0.025 | Tree loss: 1.019 | Accuracy: 0.677000 | 1.167 sec/iter\n",
      "Epoch: 212 | Batch: 003 / 011 | Total loss: 1.019 | Reg loss: 0.025 | Tree loss: 1.019 | Accuracy: 0.671500 | 1.167 sec/iter\n",
      "Epoch: 212 | Batch: 004 / 011 | Total loss: 0.976 | Reg loss: 0.025 | Tree loss: 0.976 | Accuracy: 0.680000 | 1.168 sec/iter\n",
      "Epoch: 212 | Batch: 005 / 011 | Total loss: 0.972 | Reg loss: 0.025 | Tree loss: 0.972 | Accuracy: 0.660500 | 1.168 sec/iter\n",
      "Epoch: 212 | Batch: 006 / 011 | Total loss: 0.980 | Reg loss: 0.025 | Tree loss: 0.980 | Accuracy: 0.635500 | 1.168 sec/iter\n",
      "Epoch: 212 | Batch: 007 / 011 | Total loss: 0.965 | Reg loss: 0.025 | Tree loss: 0.965 | Accuracy: 0.628000 | 1.168 sec/iter\n",
      "Epoch: 212 | Batch: 008 / 011 | Total loss: 0.977 | Reg loss: 0.025 | Tree loss: 0.977 | Accuracy: 0.630000 | 1.168 sec/iter\n",
      "Epoch: 212 | Batch: 009 / 011 | Total loss: 0.965 | Reg loss: 0.025 | Tree loss: 0.965 | Accuracy: 0.636500 | 1.168 sec/iter\n",
      "Epoch: 212 | Batch: 010 / 011 | Total loss: 0.964 | Reg loss: 0.025 | Tree loss: 0.964 | Accuracy: 0.617747 | 1.168 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 213 | Batch: 000 / 011 | Total loss: 1.050 | Reg loss: 0.025 | Tree loss: 1.050 | Accuracy: 0.633000 | 1.17 sec/iter\n",
      "Epoch: 213 | Batch: 001 / 011 | Total loss: 1.051 | Reg loss: 0.025 | Tree loss: 1.051 | Accuracy: 0.641500 | 1.17 sec/iter\n",
      "Epoch: 213 | Batch: 002 / 011 | Total loss: 1.040 | Reg loss: 0.025 | Tree loss: 1.040 | Accuracy: 0.667000 | 1.17 sec/iter\n",
      "Epoch: 213 | Batch: 003 / 011 | Total loss: 0.974 | Reg loss: 0.025 | Tree loss: 0.974 | Accuracy: 0.710500 | 1.17 sec/iter\n",
      "Epoch: 213 | Batch: 004 / 011 | Total loss: 0.989 | Reg loss: 0.025 | Tree loss: 0.989 | Accuracy: 0.689000 | 1.17 sec/iter\n",
      "Epoch: 213 | Batch: 005 / 011 | Total loss: 0.980 | Reg loss: 0.025 | Tree loss: 0.980 | Accuracy: 0.660500 | 1.17 sec/iter\n",
      "Epoch: 213 | Batch: 006 / 011 | Total loss: 0.986 | Reg loss: 0.025 | Tree loss: 0.986 | Accuracy: 0.628500 | 1.17 sec/iter\n",
      "Epoch: 213 | Batch: 007 / 011 | Total loss: 0.976 | Reg loss: 0.025 | Tree loss: 0.976 | Accuracy: 0.617000 | 1.17 sec/iter\n",
      "Epoch: 213 | Batch: 008 / 011 | Total loss: 0.960 | Reg loss: 0.025 | Tree loss: 0.960 | Accuracy: 0.643500 | 1.17 sec/iter\n",
      "Epoch: 213 | Batch: 009 / 011 | Total loss: 0.962 | Reg loss: 0.025 | Tree loss: 0.962 | Accuracy: 0.630500 | 1.17 sec/iter\n",
      "Epoch: 213 | Batch: 010 / 011 | Total loss: 0.918 | Reg loss: 0.025 | Tree loss: 0.918 | Accuracy: 0.658703 | 1.17 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 214 | Batch: 000 / 011 | Total loss: 1.069 | Reg loss: 0.025 | Tree loss: 1.069 | Accuracy: 0.626000 | 1.171 sec/iter\n",
      "Epoch: 214 | Batch: 001 / 011 | Total loss: 1.055 | Reg loss: 0.025 | Tree loss: 1.055 | Accuracy: 0.652000 | 1.171 sec/iter\n",
      "Epoch: 214 | Batch: 002 / 011 | Total loss: 0.993 | Reg loss: 0.025 | Tree loss: 0.993 | Accuracy: 0.690000 | 1.171 sec/iter\n",
      "Epoch: 214 | Batch: 003 / 011 | Total loss: 1.008 | Reg loss: 0.025 | Tree loss: 1.008 | Accuracy: 0.680500 | 1.172 sec/iter\n",
      "Epoch: 214 | Batch: 004 / 011 | Total loss: 0.979 | Reg loss: 0.025 | Tree loss: 0.979 | Accuracy: 0.677000 | 1.172 sec/iter\n",
      "Epoch: 214 | Batch: 005 / 011 | Total loss: 0.973 | Reg loss: 0.025 | Tree loss: 0.973 | Accuracy: 0.647500 | 1.172 sec/iter\n",
      "Epoch: 214 | Batch: 006 / 011 | Total loss: 0.982 | Reg loss: 0.025 | Tree loss: 0.982 | Accuracy: 0.632000 | 1.172 sec/iter\n",
      "Epoch: 214 | Batch: 007 / 011 | Total loss: 0.974 | Reg loss: 0.025 | Tree loss: 0.974 | Accuracy: 0.617500 | 1.172 sec/iter\n",
      "Epoch: 214 | Batch: 008 / 011 | Total loss: 0.939 | Reg loss: 0.025 | Tree loss: 0.939 | Accuracy: 0.658500 | 1.172 sec/iter\n",
      "Epoch: 214 | Batch: 009 / 011 | Total loss: 0.978 | Reg loss: 0.025 | Tree loss: 0.978 | Accuracy: 0.627500 | 1.172 sec/iter\n",
      "Epoch: 214 | Batch: 010 / 011 | Total loss: 0.964 | Reg loss: 0.025 | Tree loss: 0.964 | Accuracy: 0.658703 | 1.172 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 215 | Batch: 000 / 011 | Total loss: 1.075 | Reg loss: 0.025 | Tree loss: 1.075 | Accuracy: 0.630500 | 1.173 sec/iter\n",
      "Epoch: 215 | Batch: 001 / 011 | Total loss: 1.036 | Reg loss: 0.025 | Tree loss: 1.036 | Accuracy: 0.653500 | 1.173 sec/iter\n",
      "Epoch: 215 | Batch: 002 / 011 | Total loss: 0.997 | Reg loss: 0.025 | Tree loss: 0.997 | Accuracy: 0.690000 | 1.173 sec/iter\n",
      "Epoch: 215 | Batch: 003 / 011 | Total loss: 1.014 | Reg loss: 0.025 | Tree loss: 1.014 | Accuracy: 0.667500 | 1.173 sec/iter\n",
      "Epoch: 215 | Batch: 004 / 011 | Total loss: 0.965 | Reg loss: 0.025 | Tree loss: 0.965 | Accuracy: 0.690500 | 1.173 sec/iter\n",
      "Epoch: 215 | Batch: 005 / 011 | Total loss: 0.962 | Reg loss: 0.025 | Tree loss: 0.962 | Accuracy: 0.671500 | 1.173 sec/iter\n",
      "Epoch: 215 | Batch: 006 / 011 | Total loss: 0.980 | Reg loss: 0.025 | Tree loss: 0.980 | Accuracy: 0.631000 | 1.173 sec/iter\n",
      "Epoch: 215 | Batch: 007 / 011 | Total loss: 0.986 | Reg loss: 0.025 | Tree loss: 0.986 | Accuracy: 0.619000 | 1.173 sec/iter\n",
      "Epoch: 215 | Batch: 008 / 011 | Total loss: 0.962 | Reg loss: 0.025 | Tree loss: 0.962 | Accuracy: 0.638000 | 1.173 sec/iter\n",
      "Epoch: 215 | Batch: 009 / 011 | Total loss: 0.969 | Reg loss: 0.025 | Tree loss: 0.969 | Accuracy: 0.641000 | 1.173 sec/iter\n",
      "Epoch: 215 | Batch: 010 / 011 | Total loss: 0.960 | Reg loss: 0.025 | Tree loss: 0.960 | Accuracy: 0.641638 | 1.173 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 216 | Batch: 000 / 011 | Total loss: 1.076 | Reg loss: 0.025 | Tree loss: 1.076 | Accuracy: 0.621500 | 1.174 sec/iter\n",
      "Epoch: 216 | Batch: 001 / 011 | Total loss: 1.024 | Reg loss: 0.025 | Tree loss: 1.024 | Accuracy: 0.667500 | 1.174 sec/iter\n",
      "Epoch: 216 | Batch: 002 / 011 | Total loss: 1.020 | Reg loss: 0.025 | Tree loss: 1.020 | Accuracy: 0.696000 | 1.174 sec/iter\n",
      "Epoch: 216 | Batch: 003 / 011 | Total loss: 1.014 | Reg loss: 0.025 | Tree loss: 1.014 | Accuracy: 0.684000 | 1.174 sec/iter\n",
      "Epoch: 216 | Batch: 004 / 011 | Total loss: 0.965 | Reg loss: 0.025 | Tree loss: 0.965 | Accuracy: 0.676500 | 1.174 sec/iter\n",
      "Epoch: 216 | Batch: 005 / 011 | Total loss: 1.008 | Reg loss: 0.025 | Tree loss: 1.008 | Accuracy: 0.634000 | 1.174 sec/iter\n",
      "Epoch: 216 | Batch: 006 / 011 | Total loss: 0.964 | Reg loss: 0.025 | Tree loss: 0.964 | Accuracy: 0.645500 | 1.174 sec/iter\n",
      "Epoch: 216 | Batch: 007 / 011 | Total loss: 0.952 | Reg loss: 0.025 | Tree loss: 0.952 | Accuracy: 0.639500 | 1.174 sec/iter\n",
      "Epoch: 216 | Batch: 008 / 011 | Total loss: 0.977 | Reg loss: 0.025 | Tree loss: 0.977 | Accuracy: 0.633500 | 1.174 sec/iter\n",
      "Epoch: 216 | Batch: 009 / 011 | Total loss: 0.952 | Reg loss: 0.025 | Tree loss: 0.952 | Accuracy: 0.648000 | 1.174 sec/iter\n",
      "Epoch: 216 | Batch: 010 / 011 | Total loss: 0.984 | Reg loss: 0.025 | Tree loss: 0.984 | Accuracy: 0.648464 | 1.175 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 217 | Batch: 000 / 011 | Total loss: 1.045 | Reg loss: 0.025 | Tree loss: 1.045 | Accuracy: 0.631500 | 1.176 sec/iter\n",
      "Epoch: 217 | Batch: 001 / 011 | Total loss: 1.049 | Reg loss: 0.025 | Tree loss: 1.049 | Accuracy: 0.657500 | 1.176 sec/iter\n",
      "Epoch: 217 | Batch: 002 / 011 | Total loss: 1.036 | Reg loss: 0.025 | Tree loss: 1.036 | Accuracy: 0.661000 | 1.177 sec/iter\n",
      "Epoch: 217 | Batch: 003 / 011 | Total loss: 0.990 | Reg loss: 0.025 | Tree loss: 0.990 | Accuracy: 0.681000 | 1.177 sec/iter\n",
      "Epoch: 217 | Batch: 004 / 011 | Total loss: 0.960 | Reg loss: 0.025 | Tree loss: 0.960 | Accuracy: 0.687000 | 1.177 sec/iter\n",
      "Epoch: 217 | Batch: 005 / 011 | Total loss: 0.987 | Reg loss: 0.025 | Tree loss: 0.987 | Accuracy: 0.647500 | 1.177 sec/iter\n",
      "Epoch: 217 | Batch: 006 / 011 | Total loss: 0.975 | Reg loss: 0.025 | Tree loss: 0.975 | Accuracy: 0.650000 | 1.177 sec/iter\n",
      "Epoch: 217 | Batch: 007 / 011 | Total loss: 0.964 | Reg loss: 0.025 | Tree loss: 0.964 | Accuracy: 0.638500 | 1.177 sec/iter\n",
      "Epoch: 217 | Batch: 008 / 011 | Total loss: 0.952 | Reg loss: 0.025 | Tree loss: 0.952 | Accuracy: 0.639500 | 1.177 sec/iter\n",
      "Epoch: 217 | Batch: 009 / 011 | Total loss: 0.971 | Reg loss: 0.025 | Tree loss: 0.971 | Accuracy: 0.637500 | 1.178 sec/iter\n",
      "Epoch: 217 | Batch: 010 / 011 | Total loss: 1.010 | Reg loss: 0.025 | Tree loss: 1.010 | Accuracy: 0.655290 | 1.178 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 218 | Batch: 000 / 011 | Total loss: 1.052 | Reg loss: 0.025 | Tree loss: 1.052 | Accuracy: 0.638500 | 1.179 sec/iter\n",
      "Epoch: 218 | Batch: 001 / 011 | Total loss: 1.048 | Reg loss: 0.025 | Tree loss: 1.048 | Accuracy: 0.647000 | 1.18 sec/iter\n",
      "Epoch: 218 | Batch: 002 / 011 | Total loss: 1.015 | Reg loss: 0.025 | Tree loss: 1.015 | Accuracy: 0.691500 | 1.18 sec/iter\n",
      "Epoch: 218 | Batch: 003 / 011 | Total loss: 1.013 | Reg loss: 0.025 | Tree loss: 1.013 | Accuracy: 0.667500 | 1.18 sec/iter\n",
      "Epoch: 218 | Batch: 004 / 011 | Total loss: 0.968 | Reg loss: 0.025 | Tree loss: 0.968 | Accuracy: 0.686000 | 1.18 sec/iter\n",
      "Epoch: 218 | Batch: 005 / 011 | Total loss: 0.980 | Reg loss: 0.025 | Tree loss: 0.980 | Accuracy: 0.664000 | 1.18 sec/iter\n",
      "Epoch: 218 | Batch: 006 / 011 | Total loss: 0.969 | Reg loss: 0.025 | Tree loss: 0.969 | Accuracy: 0.635000 | 1.18 sec/iter\n",
      "Epoch: 218 | Batch: 007 / 011 | Total loss: 0.991 | Reg loss: 0.025 | Tree loss: 0.991 | Accuracy: 0.624000 | 1.18 sec/iter\n",
      "Epoch: 218 | Batch: 008 / 011 | Total loss: 0.961 | Reg loss: 0.025 | Tree loss: 0.961 | Accuracy: 0.639500 | 1.181 sec/iter\n",
      "Epoch: 218 | Batch: 009 / 011 | Total loss: 0.947 | Reg loss: 0.025 | Tree loss: 0.947 | Accuracy: 0.637000 | 1.181 sec/iter\n",
      "Epoch: 218 | Batch: 010 / 011 | Total loss: 0.984 | Reg loss: 0.025 | Tree loss: 0.984 | Accuracy: 0.604096 | 1.181 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 219 | Batch: 000 / 011 | Total loss: 1.042 | Reg loss: 0.025 | Tree loss: 1.042 | Accuracy: 0.631500 | 1.183 sec/iter\n",
      "Epoch: 219 | Batch: 001 / 011 | Total loss: 1.050 | Reg loss: 0.025 | Tree loss: 1.050 | Accuracy: 0.646000 | 1.183 sec/iter\n",
      "Epoch: 219 | Batch: 002 / 011 | Total loss: 1.035 | Reg loss: 0.025 | Tree loss: 1.035 | Accuracy: 0.667000 | 1.183 sec/iter\n",
      "Epoch: 219 | Batch: 003 / 011 | Total loss: 0.994 | Reg loss: 0.025 | Tree loss: 0.994 | Accuracy: 0.690500 | 1.183 sec/iter\n",
      "Epoch: 219 | Batch: 004 / 011 | Total loss: 0.996 | Reg loss: 0.025 | Tree loss: 0.996 | Accuracy: 0.668000 | 1.183 sec/iter\n",
      "Epoch: 219 | Batch: 005 / 011 | Total loss: 0.960 | Reg loss: 0.025 | Tree loss: 0.960 | Accuracy: 0.669500 | 1.183 sec/iter\n",
      "Epoch: 219 | Batch: 006 / 011 | Total loss: 0.949 | Reg loss: 0.025 | Tree loss: 0.949 | Accuracy: 0.649000 | 1.183 sec/iter\n",
      "Epoch: 219 | Batch: 007 / 011 | Total loss: 0.969 | Reg loss: 0.025 | Tree loss: 0.969 | Accuracy: 0.629500 | 1.183 sec/iter\n",
      "Epoch: 219 | Batch: 008 / 011 | Total loss: 0.973 | Reg loss: 0.025 | Tree loss: 0.973 | Accuracy: 0.629500 | 1.183 sec/iter\n",
      "Epoch: 219 | Batch: 009 / 011 | Total loss: 0.953 | Reg loss: 0.025 | Tree loss: 0.953 | Accuracy: 0.632000 | 1.183 sec/iter\n",
      "Epoch: 219 | Batch: 010 / 011 | Total loss: 0.954 | Reg loss: 0.025 | Tree loss: 0.954 | Accuracy: 0.648464 | 1.183 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 220 | Batch: 000 / 011 | Total loss: 1.047 | Reg loss: 0.025 | Tree loss: 1.047 | Accuracy: 0.651000 | 1.184 sec/iter\n",
      "Epoch: 220 | Batch: 001 / 011 | Total loss: 1.022 | Reg loss: 0.025 | Tree loss: 1.022 | Accuracy: 0.653500 | 1.184 sec/iter\n",
      "Epoch: 220 | Batch: 002 / 011 | Total loss: 1.025 | Reg loss: 0.025 | Tree loss: 1.025 | Accuracy: 0.658500 | 1.184 sec/iter\n",
      "Epoch: 220 | Batch: 003 / 011 | Total loss: 1.013 | Reg loss: 0.025 | Tree loss: 1.013 | Accuracy: 0.680000 | 1.184 sec/iter\n",
      "Epoch: 220 | Batch: 004 / 011 | Total loss: 0.994 | Reg loss: 0.025 | Tree loss: 0.994 | Accuracy: 0.675500 | 1.184 sec/iter\n",
      "Epoch: 220 | Batch: 005 / 011 | Total loss: 0.934 | Reg loss: 0.025 | Tree loss: 0.934 | Accuracy: 0.690500 | 1.184 sec/iter\n",
      "Epoch: 220 | Batch: 006 / 011 | Total loss: 0.961 | Reg loss: 0.025 | Tree loss: 0.961 | Accuracy: 0.638000 | 1.184 sec/iter\n",
      "Epoch: 220 | Batch: 007 / 011 | Total loss: 0.952 | Reg loss: 0.025 | Tree loss: 0.952 | Accuracy: 0.633500 | 1.185 sec/iter\n",
      "Epoch: 220 | Batch: 008 / 011 | Total loss: 0.990 | Reg loss: 0.025 | Tree loss: 0.990 | Accuracy: 0.619000 | 1.185 sec/iter\n",
      "Epoch: 220 | Batch: 009 / 011 | Total loss: 0.976 | Reg loss: 0.025 | Tree loss: 0.976 | Accuracy: 0.626500 | 1.185 sec/iter\n",
      "Epoch: 220 | Batch: 010 / 011 | Total loss: 1.004 | Reg loss: 0.025 | Tree loss: 1.004 | Accuracy: 0.597270 | 1.185 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 221 | Batch: 000 / 011 | Total loss: 1.050 | Reg loss: 0.025 | Tree loss: 1.050 | Accuracy: 0.642000 | 1.186 sec/iter\n",
      "Epoch: 221 | Batch: 001 / 011 | Total loss: 1.043 | Reg loss: 0.025 | Tree loss: 1.043 | Accuracy: 0.653500 | 1.187 sec/iter\n",
      "Epoch: 221 | Batch: 002 / 011 | Total loss: 1.030 | Reg loss: 0.025 | Tree loss: 1.030 | Accuracy: 0.677500 | 1.187 sec/iter\n",
      "Epoch: 221 | Batch: 003 / 011 | Total loss: 0.994 | Reg loss: 0.025 | Tree loss: 0.994 | Accuracy: 0.674000 | 1.187 sec/iter\n",
      "Epoch: 221 | Batch: 004 / 011 | Total loss: 0.995 | Reg loss: 0.025 | Tree loss: 0.995 | Accuracy: 0.668500 | 1.187 sec/iter\n",
      "Epoch: 221 | Batch: 005 / 011 | Total loss: 0.955 | Reg loss: 0.025 | Tree loss: 0.955 | Accuracy: 0.672500 | 1.188 sec/iter\n",
      "Epoch: 221 | Batch: 006 / 011 | Total loss: 0.955 | Reg loss: 0.025 | Tree loss: 0.955 | Accuracy: 0.655500 | 1.188 sec/iter\n",
      "Epoch: 221 | Batch: 007 / 011 | Total loss: 0.971 | Reg loss: 0.025 | Tree loss: 0.971 | Accuracy: 0.621500 | 1.188 sec/iter\n",
      "Epoch: 221 | Batch: 008 / 011 | Total loss: 0.985 | Reg loss: 0.025 | Tree loss: 0.985 | Accuracy: 0.613000 | 1.189 sec/iter\n",
      "Epoch: 221 | Batch: 009 / 011 | Total loss: 0.925 | Reg loss: 0.025 | Tree loss: 0.925 | Accuracy: 0.661500 | 1.189 sec/iter\n",
      "Epoch: 221 | Batch: 010 / 011 | Total loss: 0.933 | Reg loss: 0.025 | Tree loss: 0.933 | Accuracy: 0.645051 | 1.189 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 222 | Batch: 000 / 011 | Total loss: 1.042 | Reg loss: 0.025 | Tree loss: 1.042 | Accuracy: 0.638500 | 1.189 sec/iter\n",
      "Epoch: 222 | Batch: 001 / 011 | Total loss: 1.046 | Reg loss: 0.025 | Tree loss: 1.046 | Accuracy: 0.642500 | 1.19 sec/iter\n",
      "Epoch: 222 | Batch: 002 / 011 | Total loss: 1.015 | Reg loss: 0.025 | Tree loss: 1.015 | Accuracy: 0.680500 | 1.19 sec/iter\n",
      "Epoch: 222 | Batch: 003 / 011 | Total loss: 0.987 | Reg loss: 0.025 | Tree loss: 0.987 | Accuracy: 0.687500 | 1.19 sec/iter\n",
      "Epoch: 222 | Batch: 004 / 011 | Total loss: 0.984 | Reg loss: 0.025 | Tree loss: 0.984 | Accuracy: 0.675500 | 1.19 sec/iter\n",
      "Epoch: 222 | Batch: 005 / 011 | Total loss: 0.960 | Reg loss: 0.025 | Tree loss: 0.960 | Accuracy: 0.664000 | 1.19 sec/iter\n",
      "Epoch: 222 | Batch: 006 / 011 | Total loss: 0.977 | Reg loss: 0.025 | Tree loss: 0.977 | Accuracy: 0.633500 | 1.19 sec/iter\n",
      "Epoch: 222 | Batch: 007 / 011 | Total loss: 1.009 | Reg loss: 0.025 | Tree loss: 1.009 | Accuracy: 0.594000 | 1.19 sec/iter\n",
      "Epoch: 222 | Batch: 008 / 011 | Total loss: 0.947 | Reg loss: 0.025 | Tree loss: 0.947 | Accuracy: 0.635500 | 1.19 sec/iter\n",
      "Epoch: 222 | Batch: 009 / 011 | Total loss: 0.966 | Reg loss: 0.025 | Tree loss: 0.966 | Accuracy: 0.629500 | 1.191 sec/iter\n",
      "Epoch: 222 | Batch: 010 / 011 | Total loss: 0.903 | Reg loss: 0.025 | Tree loss: 0.903 | Accuracy: 0.662116 | 1.191 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 223 | Batch: 000 / 011 | Total loss: 1.044 | Reg loss: 0.025 | Tree loss: 1.044 | Accuracy: 0.638000 | 1.192 sec/iter\n",
      "Epoch: 223 | Batch: 001 / 011 | Total loss: 1.060 | Reg loss: 0.025 | Tree loss: 1.060 | Accuracy: 0.632000 | 1.192 sec/iter\n",
      "Epoch: 223 | Batch: 002 / 011 | Total loss: 1.013 | Reg loss: 0.025 | Tree loss: 1.013 | Accuracy: 0.668500 | 1.192 sec/iter\n",
      "Epoch: 223 | Batch: 003 / 011 | Total loss: 0.978 | Reg loss: 0.025 | Tree loss: 0.978 | Accuracy: 0.700500 | 1.192 sec/iter\n",
      "Epoch: 223 | Batch: 004 / 011 | Total loss: 0.982 | Reg loss: 0.025 | Tree loss: 0.982 | Accuracy: 0.684500 | 1.192 sec/iter\n",
      "Epoch: 223 | Batch: 005 / 011 | Total loss: 0.977 | Reg loss: 0.025 | Tree loss: 0.977 | Accuracy: 0.645500 | 1.193 sec/iter\n",
      "Epoch: 223 | Batch: 006 / 011 | Total loss: 0.950 | Reg loss: 0.025 | Tree loss: 0.950 | Accuracy: 0.648500 | 1.193 sec/iter\n",
      "Epoch: 223 | Batch: 007 / 011 | Total loss: 0.965 | Reg loss: 0.025 | Tree loss: 0.965 | Accuracy: 0.643000 | 1.193 sec/iter\n",
      "Epoch: 223 | Batch: 008 / 011 | Total loss: 0.958 | Reg loss: 0.025 | Tree loss: 0.958 | Accuracy: 0.634500 | 1.193 sec/iter\n",
      "Epoch: 223 | Batch: 009 / 011 | Total loss: 0.983 | Reg loss: 0.025 | Tree loss: 0.983 | Accuracy: 0.614500 | 1.193 sec/iter\n",
      "Epoch: 223 | Batch: 010 / 011 | Total loss: 1.001 | Reg loss: 0.025 | Tree loss: 1.001 | Accuracy: 0.610922 | 1.193 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 224 | Batch: 000 / 011 | Total loss: 1.070 | Reg loss: 0.025 | Tree loss: 1.070 | Accuracy: 0.625500 | 1.195 sec/iter\n",
      "Epoch: 224 | Batch: 001 / 011 | Total loss: 1.038 | Reg loss: 0.025 | Tree loss: 1.038 | Accuracy: 0.645500 | 1.195 sec/iter\n",
      "Epoch: 224 | Batch: 002 / 011 | Total loss: 1.033 | Reg loss: 0.025 | Tree loss: 1.033 | Accuracy: 0.657000 | 1.195 sec/iter\n",
      "Epoch: 224 | Batch: 003 / 011 | Total loss: 0.980 | Reg loss: 0.025 | Tree loss: 0.980 | Accuracy: 0.696500 | 1.195 sec/iter\n",
      "Epoch: 224 | Batch: 004 / 011 | Total loss: 0.987 | Reg loss: 0.025 | Tree loss: 0.987 | Accuracy: 0.677000 | 1.195 sec/iter\n",
      "Epoch: 224 | Batch: 005 / 011 | Total loss: 0.962 | Reg loss: 0.025 | Tree loss: 0.962 | Accuracy: 0.676000 | 1.195 sec/iter\n",
      "Epoch: 224 | Batch: 006 / 011 | Total loss: 0.947 | Reg loss: 0.025 | Tree loss: 0.947 | Accuracy: 0.653500 | 1.195 sec/iter\n",
      "Epoch: 224 | Batch: 007 / 011 | Total loss: 0.982 | Reg loss: 0.025 | Tree loss: 0.982 | Accuracy: 0.619000 | 1.196 sec/iter\n",
      "Epoch: 224 | Batch: 008 / 011 | Total loss: 0.983 | Reg loss: 0.025 | Tree loss: 0.983 | Accuracy: 0.620000 | 1.196 sec/iter\n",
      "Epoch: 224 | Batch: 009 / 011 | Total loss: 0.949 | Reg loss: 0.025 | Tree loss: 0.949 | Accuracy: 0.639500 | 1.196 sec/iter\n",
      "Epoch: 224 | Batch: 010 / 011 | Total loss: 0.909 | Reg loss: 0.025 | Tree loss: 0.909 | Accuracy: 0.655290 | 1.196 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 225 | Batch: 000 / 011 | Total loss: 1.035 | Reg loss: 0.025 | Tree loss: 1.035 | Accuracy: 0.649500 | 1.197 sec/iter\n",
      "Epoch: 225 | Batch: 001 / 011 | Total loss: 1.067 | Reg loss: 0.025 | Tree loss: 1.067 | Accuracy: 0.639500 | 1.197 sec/iter\n",
      "Epoch: 225 | Batch: 002 / 011 | Total loss: 1.021 | Reg loss: 0.025 | Tree loss: 1.021 | Accuracy: 0.680000 | 1.197 sec/iter\n",
      "Epoch: 225 | Batch: 003 / 011 | Total loss: 0.990 | Reg loss: 0.025 | Tree loss: 0.990 | Accuracy: 0.696000 | 1.198 sec/iter\n",
      "Epoch: 225 | Batch: 004 / 011 | Total loss: 0.978 | Reg loss: 0.025 | Tree loss: 0.978 | Accuracy: 0.689000 | 1.198 sec/iter\n",
      "Epoch: 225 | Batch: 005 / 011 | Total loss: 0.964 | Reg loss: 0.025 | Tree loss: 0.964 | Accuracy: 0.663000 | 1.198 sec/iter\n",
      "Epoch: 225 | Batch: 006 / 011 | Total loss: 0.989 | Reg loss: 0.025 | Tree loss: 0.989 | Accuracy: 0.631500 | 1.198 sec/iter\n",
      "Epoch: 225 | Batch: 007 / 011 | Total loss: 0.949 | Reg loss: 0.025 | Tree loss: 0.949 | Accuracy: 0.649500 | 1.198 sec/iter\n",
      "Epoch: 225 | Batch: 008 / 011 | Total loss: 0.944 | Reg loss: 0.025 | Tree loss: 0.944 | Accuracy: 0.652000 | 1.198 sec/iter\n",
      "Epoch: 225 | Batch: 009 / 011 | Total loss: 0.956 | Reg loss: 0.025 | Tree loss: 0.956 | Accuracy: 0.636000 | 1.198 sec/iter\n",
      "Epoch: 225 | Batch: 010 / 011 | Total loss: 0.984 | Reg loss: 0.025 | Tree loss: 0.984 | Accuracy: 0.621160 | 1.199 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 226 | Batch: 000 / 011 | Total loss: 1.025 | Reg loss: 0.025 | Tree loss: 1.025 | Accuracy: 0.650500 | 1.2 sec/iter\n",
      "Epoch: 226 | Batch: 001 / 011 | Total loss: 1.043 | Reg loss: 0.025 | Tree loss: 1.043 | Accuracy: 0.643000 | 1.2 sec/iter\n",
      "Epoch: 226 | Batch: 002 / 011 | Total loss: 1.022 | Reg loss: 0.025 | Tree loss: 1.022 | Accuracy: 0.671500 | 1.2 sec/iter\n",
      "Epoch: 226 | Batch: 003 / 011 | Total loss: 1.017 | Reg loss: 0.025 | Tree loss: 1.017 | Accuracy: 0.668000 | 1.2 sec/iter\n",
      "Epoch: 226 | Batch: 004 / 011 | Total loss: 0.987 | Reg loss: 0.025 | Tree loss: 0.987 | Accuracy: 0.667000 | 1.2 sec/iter\n",
      "Epoch: 226 | Batch: 005 / 011 | Total loss: 0.967 | Reg loss: 0.025 | Tree loss: 0.967 | Accuracy: 0.674500 | 1.201 sec/iter\n",
      "Epoch: 226 | Batch: 006 / 011 | Total loss: 0.947 | Reg loss: 0.025 | Tree loss: 0.947 | Accuracy: 0.656000 | 1.201 sec/iter\n",
      "Epoch: 226 | Batch: 007 / 011 | Total loss: 0.963 | Reg loss: 0.025 | Tree loss: 0.963 | Accuracy: 0.636500 | 1.201 sec/iter\n",
      "Epoch: 226 | Batch: 008 / 011 | Total loss: 0.941 | Reg loss: 0.025 | Tree loss: 0.941 | Accuracy: 0.648000 | 1.201 sec/iter\n",
      "Epoch: 226 | Batch: 009 / 011 | Total loss: 0.975 | Reg loss: 0.025 | Tree loss: 0.975 | Accuracy: 0.619500 | 1.201 sec/iter\n",
      "Epoch: 226 | Batch: 010 / 011 | Total loss: 0.899 | Reg loss: 0.025 | Tree loss: 0.899 | Accuracy: 0.682594 | 1.201 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 227 | Batch: 000 / 011 | Total loss: 1.056 | Reg loss: 0.025 | Tree loss: 1.056 | Accuracy: 0.642000 | 1.202 sec/iter\n",
      "Epoch: 227 | Batch: 001 / 011 | Total loss: 1.037 | Reg loss: 0.025 | Tree loss: 1.037 | Accuracy: 0.663500 | 1.202 sec/iter\n",
      "Epoch: 227 | Batch: 002 / 011 | Total loss: 1.019 | Reg loss: 0.025 | Tree loss: 1.019 | Accuracy: 0.673500 | 1.202 sec/iter\n",
      "Epoch: 227 | Batch: 003 / 011 | Total loss: 1.001 | Reg loss: 0.025 | Tree loss: 1.001 | Accuracy: 0.682500 | 1.202 sec/iter\n",
      "Epoch: 227 | Batch: 004 / 011 | Total loss: 0.971 | Reg loss: 0.025 | Tree loss: 0.971 | Accuracy: 0.677500 | 1.202 sec/iter\n",
      "Epoch: 227 | Batch: 005 / 011 | Total loss: 0.962 | Reg loss: 0.025 | Tree loss: 0.962 | Accuracy: 0.670000 | 1.202 sec/iter\n",
      "Epoch: 227 | Batch: 006 / 011 | Total loss: 0.972 | Reg loss: 0.025 | Tree loss: 0.972 | Accuracy: 0.644000 | 1.203 sec/iter\n",
      "Epoch: 227 | Batch: 007 / 011 | Total loss: 0.957 | Reg loss: 0.025 | Tree loss: 0.957 | Accuracy: 0.644500 | 1.203 sec/iter\n",
      "Epoch: 227 | Batch: 008 / 011 | Total loss: 0.967 | Reg loss: 0.025 | Tree loss: 0.967 | Accuracy: 0.621500 | 1.203 sec/iter\n",
      "Epoch: 227 | Batch: 009 / 011 | Total loss: 0.940 | Reg loss: 0.025 | Tree loss: 0.940 | Accuracy: 0.642000 | 1.203 sec/iter\n",
      "Epoch: 227 | Batch: 010 / 011 | Total loss: 0.921 | Reg loss: 0.025 | Tree loss: 0.921 | Accuracy: 0.675768 | 1.203 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 228 | Batch: 000 / 011 | Total loss: 1.064 | Reg loss: 0.025 | Tree loss: 1.064 | Accuracy: 0.634000 | 1.204 sec/iter\n",
      "Epoch: 228 | Batch: 001 / 011 | Total loss: 1.030 | Reg loss: 0.025 | Tree loss: 1.030 | Accuracy: 0.657000 | 1.204 sec/iter\n",
      "Epoch: 228 | Batch: 002 / 011 | Total loss: 1.022 | Reg loss: 0.025 | Tree loss: 1.022 | Accuracy: 0.673000 | 1.204 sec/iter\n",
      "Epoch: 228 | Batch: 003 / 011 | Total loss: 0.975 | Reg loss: 0.025 | Tree loss: 0.975 | Accuracy: 0.698000 | 1.204 sec/iter\n",
      "Epoch: 228 | Batch: 004 / 011 | Total loss: 0.964 | Reg loss: 0.025 | Tree loss: 0.964 | Accuracy: 0.688000 | 1.204 sec/iter\n",
      "Epoch: 228 | Batch: 005 / 011 | Total loss: 0.974 | Reg loss: 0.025 | Tree loss: 0.974 | Accuracy: 0.662500 | 1.205 sec/iter\n",
      "Epoch: 228 | Batch: 006 / 011 | Total loss: 0.968 | Reg loss: 0.025 | Tree loss: 0.968 | Accuracy: 0.649000 | 1.205 sec/iter\n",
      "Epoch: 228 | Batch: 007 / 011 | Total loss: 0.968 | Reg loss: 0.025 | Tree loss: 0.968 | Accuracy: 0.619500 | 1.205 sec/iter\n",
      "Epoch: 228 | Batch: 008 / 011 | Total loss: 0.954 | Reg loss: 0.025 | Tree loss: 0.954 | Accuracy: 0.644000 | 1.205 sec/iter\n",
      "Epoch: 228 | Batch: 009 / 011 | Total loss: 0.961 | Reg loss: 0.025 | Tree loss: 0.961 | Accuracy: 0.637000 | 1.205 sec/iter\n",
      "Epoch: 228 | Batch: 010 / 011 | Total loss: 0.972 | Reg loss: 0.025 | Tree loss: 0.972 | Accuracy: 0.655290 | 1.205 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 229 | Batch: 000 / 011 | Total loss: 1.059 | Reg loss: 0.025 | Tree loss: 1.059 | Accuracy: 0.629500 | 1.206 sec/iter\n",
      "Epoch: 229 | Batch: 001 / 011 | Total loss: 1.020 | Reg loss: 0.025 | Tree loss: 1.020 | Accuracy: 0.683000 | 1.207 sec/iter\n",
      "Epoch: 229 | Batch: 002 / 011 | Total loss: 1.024 | Reg loss: 0.025 | Tree loss: 1.024 | Accuracy: 0.684000 | 1.207 sec/iter\n",
      "Epoch: 229 | Batch: 003 / 011 | Total loss: 1.006 | Reg loss: 0.025 | Tree loss: 1.006 | Accuracy: 0.674500 | 1.207 sec/iter\n",
      "Epoch: 229 | Batch: 004 / 011 | Total loss: 0.974 | Reg loss: 0.025 | Tree loss: 0.974 | Accuracy: 0.689000 | 1.207 sec/iter\n",
      "Epoch: 229 | Batch: 005 / 011 | Total loss: 0.974 | Reg loss: 0.025 | Tree loss: 0.974 | Accuracy: 0.654500 | 1.207 sec/iter\n",
      "Epoch: 229 | Batch: 006 / 011 | Total loss: 0.940 | Reg loss: 0.025 | Tree loss: 0.940 | Accuracy: 0.661500 | 1.207 sec/iter\n",
      "Epoch: 229 | Batch: 007 / 011 | Total loss: 0.974 | Reg loss: 0.025 | Tree loss: 0.974 | Accuracy: 0.629500 | 1.208 sec/iter\n",
      "Epoch: 229 | Batch: 008 / 011 | Total loss: 0.970 | Reg loss: 0.025 | Tree loss: 0.970 | Accuracy: 0.639000 | 1.208 sec/iter\n",
      "Epoch: 229 | Batch: 009 / 011 | Total loss: 0.932 | Reg loss: 0.025 | Tree loss: 0.932 | Accuracy: 0.652000 | 1.208 sec/iter\n",
      "Epoch: 229 | Batch: 010 / 011 | Total loss: 0.950 | Reg loss: 0.025 | Tree loss: 0.950 | Accuracy: 0.655290 | 1.208 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 230 | Batch: 000 / 011 | Total loss: 1.061 | Reg loss: 0.025 | Tree loss: 1.061 | Accuracy: 0.632000 | 1.209 sec/iter\n",
      "Epoch: 230 | Batch: 001 / 011 | Total loss: 1.031 | Reg loss: 0.025 | Tree loss: 1.031 | Accuracy: 0.659500 | 1.21 sec/iter\n",
      "Epoch: 230 | Batch: 002 / 011 | Total loss: 1.002 | Reg loss: 0.025 | Tree loss: 1.002 | Accuracy: 0.692000 | 1.21 sec/iter\n",
      "Epoch: 230 | Batch: 003 / 011 | Total loss: 0.989 | Reg loss: 0.025 | Tree loss: 0.989 | Accuracy: 0.699000 | 1.21 sec/iter\n",
      "Epoch: 230 | Batch: 004 / 011 | Total loss: 0.971 | Reg loss: 0.025 | Tree loss: 0.971 | Accuracy: 0.674500 | 1.21 sec/iter\n",
      "Epoch: 230 | Batch: 005 / 011 | Total loss: 0.966 | Reg loss: 0.025 | Tree loss: 0.966 | Accuracy: 0.664000 | 1.211 sec/iter\n",
      "Epoch: 230 | Batch: 006 / 011 | Total loss: 0.979 | Reg loss: 0.025 | Tree loss: 0.979 | Accuracy: 0.631500 | 1.211 sec/iter\n",
      "Epoch: 230 | Batch: 007 / 011 | Total loss: 0.968 | Reg loss: 0.025 | Tree loss: 0.968 | Accuracy: 0.622500 | 1.211 sec/iter\n",
      "Epoch: 230 | Batch: 008 / 011 | Total loss: 0.958 | Reg loss: 0.025 | Tree loss: 0.958 | Accuracy: 0.632500 | 1.211 sec/iter\n",
      "Epoch: 230 | Batch: 009 / 011 | Total loss: 0.937 | Reg loss: 0.025 | Tree loss: 0.937 | Accuracy: 0.653500 | 1.211 sec/iter\n",
      "Epoch: 230 | Batch: 010 / 011 | Total loss: 0.992 | Reg loss: 0.025 | Tree loss: 0.992 | Accuracy: 0.641638 | 1.211 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 231 | Batch: 000 / 011 | Total loss: 1.042 | Reg loss: 0.025 | Tree loss: 1.042 | Accuracy: 0.642000 | 1.212 sec/iter\n",
      "Epoch: 231 | Batch: 001 / 011 | Total loss: 1.053 | Reg loss: 0.025 | Tree loss: 1.053 | Accuracy: 0.621500 | 1.212 sec/iter\n",
      "Epoch: 231 | Batch: 002 / 011 | Total loss: 1.013 | Reg loss: 0.025 | Tree loss: 1.013 | Accuracy: 0.668500 | 1.212 sec/iter\n",
      "Epoch: 231 | Batch: 003 / 011 | Total loss: 0.991 | Reg loss: 0.025 | Tree loss: 0.991 | Accuracy: 0.679500 | 1.212 sec/iter\n",
      "Epoch: 231 | Batch: 004 / 011 | Total loss: 0.984 | Reg loss: 0.025 | Tree loss: 0.984 | Accuracy: 0.675000 | 1.212 sec/iter\n",
      "Epoch: 231 | Batch: 005 / 011 | Total loss: 0.951 | Reg loss: 0.025 | Tree loss: 0.951 | Accuracy: 0.687500 | 1.212 sec/iter\n",
      "Epoch: 231 | Batch: 006 / 011 | Total loss: 0.978 | Reg loss: 0.025 | Tree loss: 0.978 | Accuracy: 0.635000 | 1.212 sec/iter\n",
      "Epoch: 231 | Batch: 007 / 011 | Total loss: 0.947 | Reg loss: 0.025 | Tree loss: 0.947 | Accuracy: 0.648000 | 1.213 sec/iter\n",
      "Epoch: 231 | Batch: 008 / 011 | Total loss: 0.961 | Reg loss: 0.025 | Tree loss: 0.961 | Accuracy: 0.629000 | 1.213 sec/iter\n",
      "Epoch: 231 | Batch: 009 / 011 | Total loss: 0.964 | Reg loss: 0.025 | Tree loss: 0.964 | Accuracy: 0.633500 | 1.213 sec/iter\n",
      "Epoch: 231 | Batch: 010 / 011 | Total loss: 0.907 | Reg loss: 0.025 | Tree loss: 0.907 | Accuracy: 0.675768 | 1.213 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 232 | Batch: 000 / 011 | Total loss: 1.054 | Reg loss: 0.025 | Tree loss: 1.054 | Accuracy: 0.642000 | 1.215 sec/iter\n",
      "Epoch: 232 | Batch: 001 / 011 | Total loss: 1.049 | Reg loss: 0.025 | Tree loss: 1.049 | Accuracy: 0.651500 | 1.215 sec/iter\n",
      "Epoch: 232 | Batch: 002 / 011 | Total loss: 1.006 | Reg loss: 0.025 | Tree loss: 1.006 | Accuracy: 0.687000 | 1.215 sec/iter\n",
      "Epoch: 232 | Batch: 003 / 011 | Total loss: 0.993 | Reg loss: 0.025 | Tree loss: 0.993 | Accuracy: 0.683500 | 1.215 sec/iter\n",
      "Epoch: 232 | Batch: 004 / 011 | Total loss: 0.956 | Reg loss: 0.025 | Tree loss: 0.956 | Accuracy: 0.693000 | 1.215 sec/iter\n",
      "Epoch: 232 | Batch: 005 / 011 | Total loss: 0.993 | Reg loss: 0.025 | Tree loss: 0.993 | Accuracy: 0.635500 | 1.215 sec/iter\n",
      "Epoch: 232 | Batch: 006 / 011 | Total loss: 0.958 | Reg loss: 0.025 | Tree loss: 0.958 | Accuracy: 0.652000 | 1.215 sec/iter\n",
      "Epoch: 232 | Batch: 007 / 011 | Total loss: 0.968 | Reg loss: 0.025 | Tree loss: 0.968 | Accuracy: 0.638000 | 1.215 sec/iter\n",
      "Epoch: 232 | Batch: 008 / 011 | Total loss: 0.939 | Reg loss: 0.025 | Tree loss: 0.939 | Accuracy: 0.654500 | 1.216 sec/iter\n",
      "Epoch: 232 | Batch: 009 / 011 | Total loss: 0.959 | Reg loss: 0.025 | Tree loss: 0.959 | Accuracy: 0.630000 | 1.216 sec/iter\n",
      "Epoch: 232 | Batch: 010 / 011 | Total loss: 0.910 | Reg loss: 0.025 | Tree loss: 0.910 | Accuracy: 0.672355 | 1.216 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 233 | Batch: 000 / 011 | Total loss: 1.036 | Reg loss: 0.025 | Tree loss: 1.036 | Accuracy: 0.645000 | 1.217 sec/iter\n",
      "Epoch: 233 | Batch: 001 / 011 | Total loss: 1.036 | Reg loss: 0.025 | Tree loss: 1.036 | Accuracy: 0.656500 | 1.217 sec/iter\n",
      "Epoch: 233 | Batch: 002 / 011 | Total loss: 1.034 | Reg loss: 0.025 | Tree loss: 1.034 | Accuracy: 0.662500 | 1.218 sec/iter\n",
      "Epoch: 233 | Batch: 003 / 011 | Total loss: 0.968 | Reg loss: 0.025 | Tree loss: 0.968 | Accuracy: 0.697500 | 1.218 sec/iter\n",
      "Epoch: 233 | Batch: 004 / 011 | Total loss: 0.992 | Reg loss: 0.025 | Tree loss: 0.992 | Accuracy: 0.670500 | 1.218 sec/iter\n",
      "Epoch: 233 | Batch: 005 / 011 | Total loss: 0.943 | Reg loss: 0.025 | Tree loss: 0.943 | Accuracy: 0.696000 | 1.218 sec/iter\n",
      "Epoch: 233 | Batch: 006 / 011 | Total loss: 0.970 | Reg loss: 0.025 | Tree loss: 0.970 | Accuracy: 0.652500 | 1.218 sec/iter\n",
      "Epoch: 233 | Batch: 007 / 011 | Total loss: 0.952 | Reg loss: 0.025 | Tree loss: 0.952 | Accuracy: 0.635500 | 1.218 sec/iter\n",
      "Epoch: 233 | Batch: 008 / 011 | Total loss: 0.953 | Reg loss: 0.025 | Tree loss: 0.953 | Accuracy: 0.631000 | 1.218 sec/iter\n",
      "Epoch: 233 | Batch: 009 / 011 | Total loss: 0.968 | Reg loss: 0.025 | Tree loss: 0.968 | Accuracy: 0.629500 | 1.218 sec/iter\n",
      "Epoch: 233 | Batch: 010 / 011 | Total loss: 0.996 | Reg loss: 0.025 | Tree loss: 0.996 | Accuracy: 0.600683 | 1.219 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 234 | Batch: 000 / 011 | Total loss: 1.044 | Reg loss: 0.025 | Tree loss: 1.044 | Accuracy: 0.650000 | 1.22 sec/iter\n",
      "Epoch: 234 | Batch: 001 / 011 | Total loss: 1.019 | Reg loss: 0.025 | Tree loss: 1.019 | Accuracy: 0.664000 | 1.22 sec/iter\n",
      "Epoch: 234 | Batch: 002 / 011 | Total loss: 1.025 | Reg loss: 0.025 | Tree loss: 1.025 | Accuracy: 0.662500 | 1.22 sec/iter\n",
      "Epoch: 234 | Batch: 003 / 011 | Total loss: 0.982 | Reg loss: 0.025 | Tree loss: 0.982 | Accuracy: 0.674000 | 1.22 sec/iter\n",
      "Epoch: 234 | Batch: 004 / 011 | Total loss: 0.982 | Reg loss: 0.025 | Tree loss: 0.982 | Accuracy: 0.676500 | 1.22 sec/iter\n",
      "Epoch: 234 | Batch: 005 / 011 | Total loss: 0.981 | Reg loss: 0.025 | Tree loss: 0.981 | Accuracy: 0.663500 | 1.22 sec/iter\n",
      "Epoch: 234 | Batch: 006 / 011 | Total loss: 0.966 | Reg loss: 0.025 | Tree loss: 0.966 | Accuracy: 0.648500 | 1.22 sec/iter\n",
      "Epoch: 234 | Batch: 007 / 011 | Total loss: 0.962 | Reg loss: 0.025 | Tree loss: 0.962 | Accuracy: 0.642000 | 1.221 sec/iter\n",
      "Epoch: 234 | Batch: 008 / 011 | Total loss: 0.953 | Reg loss: 0.025 | Tree loss: 0.953 | Accuracy: 0.644500 | 1.221 sec/iter\n",
      "Epoch: 234 | Batch: 009 / 011 | Total loss: 0.936 | Reg loss: 0.025 | Tree loss: 0.936 | Accuracy: 0.651500 | 1.221 sec/iter\n",
      "Epoch: 234 | Batch: 010 / 011 | Total loss: 0.966 | Reg loss: 0.025 | Tree loss: 0.966 | Accuracy: 0.627986 | 1.221 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 235 | Batch: 000 / 011 | Total loss: 1.048 | Reg loss: 0.025 | Tree loss: 1.048 | Accuracy: 0.651500 | 1.222 sec/iter\n",
      "Epoch: 235 | Batch: 001 / 011 | Total loss: 1.043 | Reg loss: 0.025 | Tree loss: 1.043 | Accuracy: 0.644500 | 1.222 sec/iter\n",
      "Epoch: 235 | Batch: 002 / 011 | Total loss: 1.018 | Reg loss: 0.025 | Tree loss: 1.018 | Accuracy: 0.678000 | 1.223 sec/iter\n",
      "Epoch: 235 | Batch: 003 / 011 | Total loss: 0.986 | Reg loss: 0.025 | Tree loss: 0.986 | Accuracy: 0.701500 | 1.223 sec/iter\n",
      "Epoch: 235 | Batch: 004 / 011 | Total loss: 0.971 | Reg loss: 0.025 | Tree loss: 0.971 | Accuracy: 0.680000 | 1.223 sec/iter\n",
      "Epoch: 235 | Batch: 005 / 011 | Total loss: 0.967 | Reg loss: 0.025 | Tree loss: 0.967 | Accuracy: 0.665000 | 1.223 sec/iter\n",
      "Epoch: 235 | Batch: 006 / 011 | Total loss: 0.956 | Reg loss: 0.025 | Tree loss: 0.956 | Accuracy: 0.645000 | 1.223 sec/iter\n",
      "Epoch: 235 | Batch: 007 / 011 | Total loss: 0.960 | Reg loss: 0.025 | Tree loss: 0.960 | Accuracy: 0.632000 | 1.223 sec/iter\n",
      "Epoch: 235 | Batch: 008 / 011 | Total loss: 0.961 | Reg loss: 0.025 | Tree loss: 0.961 | Accuracy: 0.647000 | 1.223 sec/iter\n",
      "Epoch: 235 | Batch: 009 / 011 | Total loss: 0.950 | Reg loss: 0.025 | Tree loss: 0.950 | Accuracy: 0.636000 | 1.224 sec/iter\n",
      "Epoch: 235 | Batch: 010 / 011 | Total loss: 0.892 | Reg loss: 0.025 | Tree loss: 0.892 | Accuracy: 0.665529 | 1.224 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 236 | Batch: 000 / 011 | Total loss: 1.058 | Reg loss: 0.025 | Tree loss: 1.058 | Accuracy: 0.644500 | 1.225 sec/iter\n",
      "Epoch: 236 | Batch: 001 / 011 | Total loss: 1.023 | Reg loss: 0.025 | Tree loss: 1.023 | Accuracy: 0.666000 | 1.225 sec/iter\n",
      "Epoch: 236 | Batch: 002 / 011 | Total loss: 0.994 | Reg loss: 0.025 | Tree loss: 0.994 | Accuracy: 0.686500 | 1.225 sec/iter\n",
      "Epoch: 236 | Batch: 003 / 011 | Total loss: 0.977 | Reg loss: 0.025 | Tree loss: 0.977 | Accuracy: 0.693500 | 1.225 sec/iter\n",
      "Epoch: 236 | Batch: 004 / 011 | Total loss: 0.964 | Reg loss: 0.025 | Tree loss: 0.964 | Accuracy: 0.675000 | 1.225 sec/iter\n",
      "Epoch: 236 | Batch: 005 / 011 | Total loss: 0.981 | Reg loss: 0.025 | Tree loss: 0.981 | Accuracy: 0.651500 | 1.225 sec/iter\n",
      "Epoch: 236 | Batch: 006 / 011 | Total loss: 0.977 | Reg loss: 0.025 | Tree loss: 0.977 | Accuracy: 0.625500 | 1.225 sec/iter\n",
      "Epoch: 236 | Batch: 007 / 011 | Total loss: 0.940 | Reg loss: 0.025 | Tree loss: 0.940 | Accuracy: 0.641000 | 1.225 sec/iter\n",
      "Epoch: 236 | Batch: 008 / 011 | Total loss: 0.978 | Reg loss: 0.025 | Tree loss: 0.978 | Accuracy: 0.629500 | 1.226 sec/iter\n",
      "Epoch: 236 | Batch: 009 / 011 | Total loss: 0.970 | Reg loss: 0.025 | Tree loss: 0.970 | Accuracy: 0.641000 | 1.226 sec/iter\n",
      "Epoch: 236 | Batch: 010 / 011 | Total loss: 0.925 | Reg loss: 0.025 | Tree loss: 0.925 | Accuracy: 0.645051 | 1.226 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 237 | Batch: 000 / 011 | Total loss: 1.043 | Reg loss: 0.025 | Tree loss: 1.043 | Accuracy: 0.646000 | 1.227 sec/iter\n",
      "Epoch: 237 | Batch: 001 / 011 | Total loss: 1.037 | Reg loss: 0.025 | Tree loss: 1.037 | Accuracy: 0.649000 | 1.227 sec/iter\n",
      "Epoch: 237 | Batch: 002 / 011 | Total loss: 0.999 | Reg loss: 0.025 | Tree loss: 0.999 | Accuracy: 0.690500 | 1.227 sec/iter\n",
      "Epoch: 237 | Batch: 003 / 011 | Total loss: 1.002 | Reg loss: 0.025 | Tree loss: 1.002 | Accuracy: 0.680000 | 1.227 sec/iter\n",
      "Epoch: 237 | Batch: 004 / 011 | Total loss: 0.968 | Reg loss: 0.025 | Tree loss: 0.968 | Accuracy: 0.679500 | 1.227 sec/iter\n",
      "Epoch: 237 | Batch: 005 / 011 | Total loss: 0.965 | Reg loss: 0.025 | Tree loss: 0.965 | Accuracy: 0.667500 | 1.228 sec/iter\n",
      "Epoch: 237 | Batch: 006 / 011 | Total loss: 0.939 | Reg loss: 0.025 | Tree loss: 0.939 | Accuracy: 0.675500 | 1.228 sec/iter\n",
      "Epoch: 237 | Batch: 007 / 011 | Total loss: 0.976 | Reg loss: 0.025 | Tree loss: 0.976 | Accuracy: 0.623000 | 1.228 sec/iter\n",
      "Epoch: 237 | Batch: 008 / 011 | Total loss: 0.956 | Reg loss: 0.025 | Tree loss: 0.956 | Accuracy: 0.638500 | 1.228 sec/iter\n",
      "Epoch: 237 | Batch: 009 / 011 | Total loss: 0.952 | Reg loss: 0.025 | Tree loss: 0.952 | Accuracy: 0.636500 | 1.228 sec/iter\n",
      "Epoch: 237 | Batch: 010 / 011 | Total loss: 0.969 | Reg loss: 0.025 | Tree loss: 0.969 | Accuracy: 0.597270 | 1.228 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 238 | Batch: 000 / 011 | Total loss: 1.039 | Reg loss: 0.025 | Tree loss: 1.039 | Accuracy: 0.645500 | 1.229 sec/iter\n",
      "Epoch: 238 | Batch: 001 / 011 | Total loss: 1.030 | Reg loss: 0.025 | Tree loss: 1.030 | Accuracy: 0.640500 | 1.229 sec/iter\n",
      "Epoch: 238 | Batch: 002 / 011 | Total loss: 1.019 | Reg loss: 0.025 | Tree loss: 1.019 | Accuracy: 0.678000 | 1.229 sec/iter\n",
      "Epoch: 238 | Batch: 003 / 011 | Total loss: 0.979 | Reg loss: 0.025 | Tree loss: 0.979 | Accuracy: 0.695000 | 1.23 sec/iter\n",
      "Epoch: 238 | Batch: 004 / 011 | Total loss: 0.972 | Reg loss: 0.025 | Tree loss: 0.972 | Accuracy: 0.672000 | 1.23 sec/iter\n",
      "Epoch: 238 | Batch: 005 / 011 | Total loss: 0.963 | Reg loss: 0.025 | Tree loss: 0.963 | Accuracy: 0.663000 | 1.23 sec/iter\n",
      "Epoch: 238 | Batch: 006 / 011 | Total loss: 0.955 | Reg loss: 0.025 | Tree loss: 0.955 | Accuracy: 0.639500 | 1.23 sec/iter\n",
      "Epoch: 238 | Batch: 007 / 011 | Total loss: 0.946 | Reg loss: 0.025 | Tree loss: 0.946 | Accuracy: 0.638500 | 1.23 sec/iter\n",
      "Epoch: 238 | Batch: 008 / 011 | Total loss: 0.984 | Reg loss: 0.025 | Tree loss: 0.984 | Accuracy: 0.616000 | 1.23 sec/iter\n",
      "Epoch: 238 | Batch: 009 / 011 | Total loss: 0.963 | Reg loss: 0.025 | Tree loss: 0.963 | Accuracy: 0.630000 | 1.23 sec/iter\n",
      "Epoch: 238 | Batch: 010 / 011 | Total loss: 0.938 | Reg loss: 0.025 | Tree loss: 0.938 | Accuracy: 0.658703 | 1.23 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 239 | Batch: 000 / 011 | Total loss: 1.026 | Reg loss: 0.025 | Tree loss: 1.026 | Accuracy: 0.650500 | 1.232 sec/iter\n",
      "Epoch: 239 | Batch: 001 / 011 | Total loss: 1.037 | Reg loss: 0.025 | Tree loss: 1.037 | Accuracy: 0.646000 | 1.232 sec/iter\n",
      "Epoch: 239 | Batch: 002 / 011 | Total loss: 1.031 | Reg loss: 0.025 | Tree loss: 1.031 | Accuracy: 0.661000 | 1.232 sec/iter\n",
      "Epoch: 239 | Batch: 003 / 011 | Total loss: 0.978 | Reg loss: 0.025 | Tree loss: 0.978 | Accuracy: 0.695500 | 1.232 sec/iter\n",
      "Epoch: 239 | Batch: 004 / 011 | Total loss: 0.973 | Reg loss: 0.025 | Tree loss: 0.973 | Accuracy: 0.675500 | 1.232 sec/iter\n",
      "Epoch: 239 | Batch: 005 / 011 | Total loss: 0.984 | Reg loss: 0.025 | Tree loss: 0.984 | Accuracy: 0.667500 | 1.232 sec/iter\n",
      "Epoch: 239 | Batch: 006 / 011 | Total loss: 0.945 | Reg loss: 0.025 | Tree loss: 0.945 | Accuracy: 0.667000 | 1.232 sec/iter\n",
      "Epoch: 239 | Batch: 007 / 011 | Total loss: 0.964 | Reg loss: 0.025 | Tree loss: 0.964 | Accuracy: 0.638000 | 1.233 sec/iter\n",
      "Epoch: 239 | Batch: 008 / 011 | Total loss: 0.956 | Reg loss: 0.025 | Tree loss: 0.956 | Accuracy: 0.637500 | 1.233 sec/iter\n",
      "Epoch: 239 | Batch: 009 / 011 | Total loss: 0.951 | Reg loss: 0.025 | Tree loss: 0.951 | Accuracy: 0.625500 | 1.233 sec/iter\n",
      "Epoch: 239 | Batch: 010 / 011 | Total loss: 0.961 | Reg loss: 0.025 | Tree loss: 0.961 | Accuracy: 0.624573 | 1.233 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 240 | Batch: 000 / 011 | Total loss: 1.023 | Reg loss: 0.025 | Tree loss: 1.023 | Accuracy: 0.659500 | 1.234 sec/iter\n",
      "Epoch: 240 | Batch: 001 / 011 | Total loss: 1.032 | Reg loss: 0.025 | Tree loss: 1.032 | Accuracy: 0.655000 | 1.235 sec/iter\n",
      "Epoch: 240 | Batch: 002 / 011 | Total loss: 1.019 | Reg loss: 0.025 | Tree loss: 1.019 | Accuracy: 0.672500 | 1.235 sec/iter\n",
      "Epoch: 240 | Batch: 003 / 011 | Total loss: 1.018 | Reg loss: 0.025 | Tree loss: 1.018 | Accuracy: 0.690500 | 1.235 sec/iter\n",
      "Epoch: 240 | Batch: 004 / 011 | Total loss: 0.974 | Reg loss: 0.025 | Tree loss: 0.974 | Accuracy: 0.670500 | 1.235 sec/iter\n",
      "Epoch: 240 | Batch: 005 / 011 | Total loss: 0.963 | Reg loss: 0.025 | Tree loss: 0.963 | Accuracy: 0.669500 | 1.235 sec/iter\n",
      "Epoch: 240 | Batch: 006 / 011 | Total loss: 0.955 | Reg loss: 0.025 | Tree loss: 0.955 | Accuracy: 0.660000 | 1.235 sec/iter\n",
      "Epoch: 240 | Batch: 007 / 011 | Total loss: 0.948 | Reg loss: 0.025 | Tree loss: 0.948 | Accuracy: 0.648500 | 1.235 sec/iter\n",
      "Epoch: 240 | Batch: 008 / 011 | Total loss: 0.968 | Reg loss: 0.025 | Tree loss: 0.968 | Accuracy: 0.628500 | 1.235 sec/iter\n",
      "Epoch: 240 | Batch: 009 / 011 | Total loss: 0.940 | Reg loss: 0.025 | Tree loss: 0.940 | Accuracy: 0.652500 | 1.236 sec/iter\n",
      "Epoch: 240 | Batch: 010 / 011 | Total loss: 0.947 | Reg loss: 0.025 | Tree loss: 0.947 | Accuracy: 0.621160 | 1.236 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 241 | Batch: 000 / 011 | Total loss: 1.044 | Reg loss: 0.025 | Tree loss: 1.044 | Accuracy: 0.637000 | 1.237 sec/iter\n",
      "Epoch: 241 | Batch: 001 / 011 | Total loss: 1.044 | Reg loss: 0.025 | Tree loss: 1.044 | Accuracy: 0.646000 | 1.237 sec/iter\n",
      "Epoch: 241 | Batch: 002 / 011 | Total loss: 1.006 | Reg loss: 0.025 | Tree loss: 1.006 | Accuracy: 0.686000 | 1.237 sec/iter\n",
      "Epoch: 241 | Batch: 003 / 011 | Total loss: 0.979 | Reg loss: 0.025 | Tree loss: 0.979 | Accuracy: 0.681000 | 1.237 sec/iter\n",
      "Epoch: 241 | Batch: 004 / 011 | Total loss: 0.991 | Reg loss: 0.025 | Tree loss: 0.991 | Accuracy: 0.676000 | 1.237 sec/iter\n",
      "Epoch: 241 | Batch: 005 / 011 | Total loss: 0.959 | Reg loss: 0.025 | Tree loss: 0.959 | Accuracy: 0.681500 | 1.237 sec/iter\n",
      "Epoch: 241 | Batch: 006 / 011 | Total loss: 0.948 | Reg loss: 0.025 | Tree loss: 0.948 | Accuracy: 0.651000 | 1.237 sec/iter\n",
      "Epoch: 241 | Batch: 007 / 011 | Total loss: 0.973 | Reg loss: 0.025 | Tree loss: 0.973 | Accuracy: 0.621500 | 1.238 sec/iter\n",
      "Epoch: 241 | Batch: 008 / 011 | Total loss: 0.942 | Reg loss: 0.025 | Tree loss: 0.942 | Accuracy: 0.647000 | 1.238 sec/iter\n",
      "Epoch: 241 | Batch: 009 / 011 | Total loss: 0.967 | Reg loss: 0.025 | Tree loss: 0.967 | Accuracy: 0.635000 | 1.238 sec/iter\n",
      "Epoch: 241 | Batch: 010 / 011 | Total loss: 0.926 | Reg loss: 0.025 | Tree loss: 0.926 | Accuracy: 0.627986 | 1.238 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 242 | Batch: 000 / 011 | Total loss: 1.059 | Reg loss: 0.025 | Tree loss: 1.059 | Accuracy: 0.625000 | 1.238 sec/iter\n",
      "Epoch: 242 | Batch: 001 / 011 | Total loss: 1.022 | Reg loss: 0.025 | Tree loss: 1.022 | Accuracy: 0.664500 | 1.238 sec/iter\n",
      "Epoch: 242 | Batch: 002 / 011 | Total loss: 1.009 | Reg loss: 0.025 | Tree loss: 1.009 | Accuracy: 0.666000 | 1.239 sec/iter\n",
      "Epoch: 242 | Batch: 003 / 011 | Total loss: 0.983 | Reg loss: 0.025 | Tree loss: 0.983 | Accuracy: 0.689000 | 1.239 sec/iter\n",
      "Epoch: 242 | Batch: 004 / 011 | Total loss: 0.977 | Reg loss: 0.025 | Tree loss: 0.977 | Accuracy: 0.676000 | 1.239 sec/iter\n",
      "Epoch: 242 | Batch: 005 / 011 | Total loss: 0.974 | Reg loss: 0.025 | Tree loss: 0.974 | Accuracy: 0.657000 | 1.239 sec/iter\n",
      "Epoch: 242 | Batch: 006 / 011 | Total loss: 0.951 | Reg loss: 0.025 | Tree loss: 0.951 | Accuracy: 0.658000 | 1.239 sec/iter\n",
      "Epoch: 242 | Batch: 007 / 011 | Total loss: 0.938 | Reg loss: 0.025 | Tree loss: 0.938 | Accuracy: 0.654500 | 1.239 sec/iter\n",
      "Epoch: 242 | Batch: 008 / 011 | Total loss: 0.944 | Reg loss: 0.025 | Tree loss: 0.944 | Accuracy: 0.642000 | 1.239 sec/iter\n",
      "Epoch: 242 | Batch: 009 / 011 | Total loss: 0.971 | Reg loss: 0.025 | Tree loss: 0.971 | Accuracy: 0.623500 | 1.239 sec/iter\n",
      "Epoch: 242 | Batch: 010 / 011 | Total loss: 0.955 | Reg loss: 0.025 | Tree loss: 0.955 | Accuracy: 0.648464 | 1.24 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 243 | Batch: 000 / 011 | Total loss: 1.050 | Reg loss: 0.025 | Tree loss: 1.050 | Accuracy: 0.633000 | 1.241 sec/iter\n",
      "Epoch: 243 | Batch: 001 / 011 | Total loss: 1.023 | Reg loss: 0.025 | Tree loss: 1.023 | Accuracy: 0.647000 | 1.241 sec/iter\n",
      "Epoch: 243 | Batch: 002 / 011 | Total loss: 1.028 | Reg loss: 0.025 | Tree loss: 1.028 | Accuracy: 0.661500 | 1.241 sec/iter\n",
      "Epoch: 243 | Batch: 003 / 011 | Total loss: 0.985 | Reg loss: 0.025 | Tree loss: 0.985 | Accuracy: 0.683500 | 1.241 sec/iter\n",
      "Epoch: 243 | Batch: 004 / 011 | Total loss: 0.976 | Reg loss: 0.025 | Tree loss: 0.976 | Accuracy: 0.689500 | 1.241 sec/iter\n",
      "Epoch: 243 | Batch: 005 / 011 | Total loss: 0.951 | Reg loss: 0.025 | Tree loss: 0.951 | Accuracy: 0.680000 | 1.241 sec/iter\n",
      "Epoch: 243 | Batch: 006 / 011 | Total loss: 0.961 | Reg loss: 0.025 | Tree loss: 0.961 | Accuracy: 0.653500 | 1.242 sec/iter\n",
      "Epoch: 243 | Batch: 007 / 011 | Total loss: 0.934 | Reg loss: 0.025 | Tree loss: 0.934 | Accuracy: 0.658000 | 1.242 sec/iter\n",
      "Epoch: 243 | Batch: 008 / 011 | Total loss: 0.958 | Reg loss: 0.025 | Tree loss: 0.958 | Accuracy: 0.638500 | 1.242 sec/iter\n",
      "Epoch: 243 | Batch: 009 / 011 | Total loss: 0.958 | Reg loss: 0.025 | Tree loss: 0.958 | Accuracy: 0.632000 | 1.242 sec/iter\n",
      "Epoch: 243 | Batch: 010 / 011 | Total loss: 0.917 | Reg loss: 0.025 | Tree loss: 0.917 | Accuracy: 0.689420 | 1.242 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 244 | Batch: 000 / 011 | Total loss: 1.028 | Reg loss: 0.025 | Tree loss: 1.028 | Accuracy: 0.652000 | 1.243 sec/iter\n",
      "Epoch: 244 | Batch: 001 / 011 | Total loss: 1.016 | Reg loss: 0.025 | Tree loss: 1.016 | Accuracy: 0.678500 | 1.244 sec/iter\n",
      "Epoch: 244 | Batch: 002 / 011 | Total loss: 1.014 | Reg loss: 0.025 | Tree loss: 1.014 | Accuracy: 0.673000 | 1.244 sec/iter\n",
      "Epoch: 244 | Batch: 003 / 011 | Total loss: 0.990 | Reg loss: 0.025 | Tree loss: 0.990 | Accuracy: 0.688500 | 1.244 sec/iter\n",
      "Epoch: 244 | Batch: 004 / 011 | Total loss: 0.995 | Reg loss: 0.025 | Tree loss: 0.995 | Accuracy: 0.661000 | 1.245 sec/iter\n",
      "Epoch: 244 | Batch: 005 / 011 | Total loss: 0.987 | Reg loss: 0.025 | Tree loss: 0.987 | Accuracy: 0.657000 | 1.245 sec/iter\n",
      "Epoch: 244 | Batch: 006 / 011 | Total loss: 0.962 | Reg loss: 0.025 | Tree loss: 0.962 | Accuracy: 0.643500 | 1.245 sec/iter\n",
      "Epoch: 244 | Batch: 007 / 011 | Total loss: 0.953 | Reg loss: 0.025 | Tree loss: 0.953 | Accuracy: 0.648500 | 1.246 sec/iter\n",
      "Epoch: 244 | Batch: 008 / 011 | Total loss: 0.941 | Reg loss: 0.025 | Tree loss: 0.941 | Accuracy: 0.655000 | 1.246 sec/iter\n",
      "Epoch: 244 | Batch: 009 / 011 | Total loss: 0.920 | Reg loss: 0.025 | Tree loss: 0.920 | Accuracy: 0.652000 | 1.246 sec/iter\n",
      "Epoch: 244 | Batch: 010 / 011 | Total loss: 0.955 | Reg loss: 0.025 | Tree loss: 0.955 | Accuracy: 0.675768 | 1.247 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 245 | Batch: 000 / 011 | Total loss: 1.029 | Reg loss: 0.025 | Tree loss: 1.029 | Accuracy: 0.655000 | 1.248 sec/iter\n",
      "Epoch: 245 | Batch: 001 / 011 | Total loss: 1.042 | Reg loss: 0.025 | Tree loss: 1.042 | Accuracy: 0.652000 | 1.248 sec/iter\n",
      "Epoch: 245 | Batch: 002 / 011 | Total loss: 0.997 | Reg loss: 0.025 | Tree loss: 0.997 | Accuracy: 0.683500 | 1.249 sec/iter\n",
      "Epoch: 245 | Batch: 003 / 011 | Total loss: 0.956 | Reg loss: 0.025 | Tree loss: 0.956 | Accuracy: 0.720500 | 1.249 sec/iter\n",
      "Epoch: 245 | Batch: 004 / 011 | Total loss: 0.958 | Reg loss: 0.025 | Tree loss: 0.958 | Accuracy: 0.700500 | 1.249 sec/iter\n",
      "Epoch: 245 | Batch: 005 / 011 | Total loss: 0.974 | Reg loss: 0.025 | Tree loss: 0.974 | Accuracy: 0.655500 | 1.249 sec/iter\n",
      "Epoch: 245 | Batch: 006 / 011 | Total loss: 0.961 | Reg loss: 0.025 | Tree loss: 0.961 | Accuracy: 0.639000 | 1.249 sec/iter\n",
      "Epoch: 245 | Batch: 007 / 011 | Total loss: 0.965 | Reg loss: 0.025 | Tree loss: 0.965 | Accuracy: 0.645500 | 1.249 sec/iter\n",
      "Epoch: 245 | Batch: 008 / 011 | Total loss: 0.960 | Reg loss: 0.025 | Tree loss: 0.960 | Accuracy: 0.625000 | 1.249 sec/iter\n",
      "Epoch: 245 | Batch: 009 / 011 | Total loss: 0.986 | Reg loss: 0.025 | Tree loss: 0.986 | Accuracy: 0.621000 | 1.25 sec/iter\n",
      "Epoch: 245 | Batch: 010 / 011 | Total loss: 0.983 | Reg loss: 0.025 | Tree loss: 0.983 | Accuracy: 0.604096 | 1.25 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 246 | Batch: 000 / 011 | Total loss: 1.039 | Reg loss: 0.025 | Tree loss: 1.039 | Accuracy: 0.652500 | 1.25 sec/iter\n",
      "Epoch: 246 | Batch: 001 / 011 | Total loss: 1.043 | Reg loss: 0.025 | Tree loss: 1.043 | Accuracy: 0.640000 | 1.251 sec/iter\n",
      "Epoch: 246 | Batch: 002 / 011 | Total loss: 1.019 | Reg loss: 0.025 | Tree loss: 1.019 | Accuracy: 0.674000 | 1.251 sec/iter\n",
      "Epoch: 246 | Batch: 003 / 011 | Total loss: 0.990 | Reg loss: 0.025 | Tree loss: 0.990 | Accuracy: 0.673000 | 1.251 sec/iter\n",
      "Epoch: 246 | Batch: 004 / 011 | Total loss: 0.987 | Reg loss: 0.025 | Tree loss: 0.987 | Accuracy: 0.683000 | 1.251 sec/iter\n",
      "Epoch: 246 | Batch: 005 / 011 | Total loss: 0.946 | Reg loss: 0.025 | Tree loss: 0.946 | Accuracy: 0.687500 | 1.251 sec/iter\n",
      "Epoch: 246 | Batch: 006 / 011 | Total loss: 0.942 | Reg loss: 0.025 | Tree loss: 0.942 | Accuracy: 0.667500 | 1.251 sec/iter\n",
      "Epoch: 246 | Batch: 007 / 011 | Total loss: 0.951 | Reg loss: 0.025 | Tree loss: 0.951 | Accuracy: 0.651000 | 1.252 sec/iter\n",
      "Epoch: 246 | Batch: 008 / 011 | Total loss: 0.952 | Reg loss: 0.025 | Tree loss: 0.952 | Accuracy: 0.632000 | 1.252 sec/iter\n",
      "Epoch: 246 | Batch: 009 / 011 | Total loss: 0.945 | Reg loss: 0.025 | Tree loss: 0.945 | Accuracy: 0.636000 | 1.252 sec/iter\n",
      "Epoch: 246 | Batch: 010 / 011 | Total loss: 0.967 | Reg loss: 0.025 | Tree loss: 0.967 | Accuracy: 0.614334 | 1.252 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 247 | Batch: 000 / 011 | Total loss: 1.051 | Reg loss: 0.025 | Tree loss: 1.051 | Accuracy: 0.638000 | 1.253 sec/iter\n",
      "Epoch: 247 | Batch: 001 / 011 | Total loss: 1.048 | Reg loss: 0.025 | Tree loss: 1.048 | Accuracy: 0.664000 | 1.253 sec/iter\n",
      "Epoch: 247 | Batch: 002 / 011 | Total loss: 1.005 | Reg loss: 0.025 | Tree loss: 1.005 | Accuracy: 0.688500 | 1.253 sec/iter\n",
      "Epoch: 247 | Batch: 003 / 011 | Total loss: 0.999 | Reg loss: 0.025 | Tree loss: 0.999 | Accuracy: 0.686000 | 1.253 sec/iter\n",
      "Epoch: 247 | Batch: 004 / 011 | Total loss: 0.966 | Reg loss: 0.025 | Tree loss: 0.966 | Accuracy: 0.681500 | 1.254 sec/iter\n",
      "Epoch: 247 | Batch: 005 / 011 | Total loss: 0.967 | Reg loss: 0.025 | Tree loss: 0.967 | Accuracy: 0.675000 | 1.254 sec/iter\n",
      "Epoch: 247 | Batch: 006 / 011 | Total loss: 0.960 | Reg loss: 0.025 | Tree loss: 0.960 | Accuracy: 0.647500 | 1.254 sec/iter\n",
      "Epoch: 247 | Batch: 007 / 011 | Total loss: 0.964 | Reg loss: 0.025 | Tree loss: 0.964 | Accuracy: 0.627000 | 1.254 sec/iter\n",
      "Epoch: 247 | Batch: 008 / 011 | Total loss: 0.945 | Reg loss: 0.025 | Tree loss: 0.945 | Accuracy: 0.646500 | 1.254 sec/iter\n",
      "Epoch: 247 | Batch: 009 / 011 | Total loss: 0.918 | Reg loss: 0.025 | Tree loss: 0.918 | Accuracy: 0.657000 | 1.255 sec/iter\n",
      "Epoch: 247 | Batch: 010 / 011 | Total loss: 0.859 | Reg loss: 0.025 | Tree loss: 0.859 | Accuracy: 0.692833 | 1.255 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 248 | Batch: 000 / 011 | Total loss: 1.039 | Reg loss: 0.025 | Tree loss: 1.039 | Accuracy: 0.638500 | 1.255 sec/iter\n",
      "Epoch: 248 | Batch: 001 / 011 | Total loss: 1.031 | Reg loss: 0.025 | Tree loss: 1.031 | Accuracy: 0.658000 | 1.255 sec/iter\n",
      "Epoch: 248 | Batch: 002 / 011 | Total loss: 0.997 | Reg loss: 0.025 | Tree loss: 0.997 | Accuracy: 0.686500 | 1.255 sec/iter\n",
      "Epoch: 248 | Batch: 003 / 011 | Total loss: 0.993 | Reg loss: 0.025 | Tree loss: 0.993 | Accuracy: 0.676500 | 1.255 sec/iter\n",
      "Epoch: 248 | Batch: 004 / 011 | Total loss: 0.974 | Reg loss: 0.025 | Tree loss: 0.974 | Accuracy: 0.678500 | 1.255 sec/iter\n",
      "Epoch: 248 | Batch: 005 / 011 | Total loss: 0.966 | Reg loss: 0.025 | Tree loss: 0.966 | Accuracy: 0.665500 | 1.255 sec/iter\n",
      "Epoch: 248 | Batch: 006 / 011 | Total loss: 0.960 | Reg loss: 0.025 | Tree loss: 0.960 | Accuracy: 0.645500 | 1.255 sec/iter\n",
      "Epoch: 248 | Batch: 007 / 011 | Total loss: 0.943 | Reg loss: 0.025 | Tree loss: 0.943 | Accuracy: 0.661500 | 1.255 sec/iter\n",
      "Epoch: 248 | Batch: 008 / 011 | Total loss: 0.965 | Reg loss: 0.025 | Tree loss: 0.965 | Accuracy: 0.631500 | 1.256 sec/iter\n",
      "Epoch: 248 | Batch: 009 / 011 | Total loss: 0.927 | Reg loss: 0.025 | Tree loss: 0.927 | Accuracy: 0.650500 | 1.256 sec/iter\n",
      "Epoch: 248 | Batch: 010 / 011 | Total loss: 1.016 | Reg loss: 0.025 | Tree loss: 1.016 | Accuracy: 0.648464 | 1.256 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 249 | Batch: 000 / 011 | Total loss: 1.042 | Reg loss: 0.025 | Tree loss: 1.042 | Accuracy: 0.635000 | 1.256 sec/iter\n",
      "Epoch: 249 | Batch: 001 / 011 | Total loss: 1.043 | Reg loss: 0.025 | Tree loss: 1.043 | Accuracy: 0.646000 | 1.256 sec/iter\n",
      "Epoch: 249 | Batch: 002 / 011 | Total loss: 1.015 | Reg loss: 0.025 | Tree loss: 1.015 | Accuracy: 0.678000 | 1.256 sec/iter\n",
      "Epoch: 249 | Batch: 003 / 011 | Total loss: 0.972 | Reg loss: 0.025 | Tree loss: 0.972 | Accuracy: 0.690000 | 1.256 sec/iter\n",
      "Epoch: 249 | Batch: 004 / 011 | Total loss: 0.971 | Reg loss: 0.025 | Tree loss: 0.971 | Accuracy: 0.689500 | 1.257 sec/iter\n",
      "Epoch: 249 | Batch: 005 / 011 | Total loss: 0.953 | Reg loss: 0.025 | Tree loss: 0.953 | Accuracy: 0.665500 | 1.257 sec/iter\n",
      "Epoch: 249 | Batch: 006 / 011 | Total loss: 0.966 | Reg loss: 0.025 | Tree loss: 0.966 | Accuracy: 0.652000 | 1.257 sec/iter\n",
      "Epoch: 249 | Batch: 007 / 011 | Total loss: 0.932 | Reg loss: 0.025 | Tree loss: 0.932 | Accuracy: 0.658500 | 1.257 sec/iter\n",
      "Epoch: 249 | Batch: 008 / 011 | Total loss: 0.976 | Reg loss: 0.025 | Tree loss: 0.976 | Accuracy: 0.624500 | 1.257 sec/iter\n",
      "Epoch: 249 | Batch: 009 / 011 | Total loss: 0.960 | Reg loss: 0.025 | Tree loss: 0.960 | Accuracy: 0.632500 | 1.257 sec/iter\n",
      "Epoch: 249 | Batch: 010 / 011 | Total loss: 0.893 | Reg loss: 0.025 | Tree loss: 0.893 | Accuracy: 0.686007 | 1.257 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 250 | Batch: 000 / 011 | Total loss: 1.037 | Reg loss: 0.025 | Tree loss: 1.037 | Accuracy: 0.645500 | 1.258 sec/iter\n",
      "Epoch: 250 | Batch: 001 / 011 | Total loss: 1.028 | Reg loss: 0.025 | Tree loss: 1.028 | Accuracy: 0.654500 | 1.258 sec/iter\n",
      "Epoch: 250 | Batch: 002 / 011 | Total loss: 0.995 | Reg loss: 0.025 | Tree loss: 0.995 | Accuracy: 0.679500 | 1.258 sec/iter\n",
      "Epoch: 250 | Batch: 003 / 011 | Total loss: 0.970 | Reg loss: 0.025 | Tree loss: 0.970 | Accuracy: 0.703500 | 1.258 sec/iter\n",
      "Epoch: 250 | Batch: 004 / 011 | Total loss: 0.971 | Reg loss: 0.025 | Tree loss: 0.971 | Accuracy: 0.690000 | 1.258 sec/iter\n",
      "Epoch: 250 | Batch: 005 / 011 | Total loss: 0.975 | Reg loss: 0.025 | Tree loss: 0.975 | Accuracy: 0.666500 | 1.258 sec/iter\n",
      "Epoch: 250 | Batch: 006 / 011 | Total loss: 0.951 | Reg loss: 0.025 | Tree loss: 0.951 | Accuracy: 0.655000 | 1.258 sec/iter\n",
      "Epoch: 250 | Batch: 007 / 011 | Total loss: 0.973 | Reg loss: 0.025 | Tree loss: 0.973 | Accuracy: 0.619000 | 1.258 sec/iter\n",
      "Epoch: 250 | Batch: 008 / 011 | Total loss: 0.969 | Reg loss: 0.025 | Tree loss: 0.969 | Accuracy: 0.628500 | 1.259 sec/iter\n",
      "Epoch: 250 | Batch: 009 / 011 | Total loss: 0.922 | Reg loss: 0.025 | Tree loss: 0.922 | Accuracy: 0.655000 | 1.259 sec/iter\n",
      "Epoch: 250 | Batch: 010 / 011 | Total loss: 0.999 | Reg loss: 0.025 | Tree loss: 0.999 | Accuracy: 0.624573 | 1.259 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 251 | Batch: 000 / 011 | Total loss: 1.030 | Reg loss: 0.025 | Tree loss: 1.030 | Accuracy: 0.655500 | 1.26 sec/iter\n",
      "Epoch: 251 | Batch: 001 / 011 | Total loss: 1.046 | Reg loss: 0.025 | Tree loss: 1.046 | Accuracy: 0.641500 | 1.26 sec/iter\n",
      "Epoch: 251 | Batch: 002 / 011 | Total loss: 0.994 | Reg loss: 0.025 | Tree loss: 0.994 | Accuracy: 0.685500 | 1.26 sec/iter\n",
      "Epoch: 251 | Batch: 003 / 011 | Total loss: 0.985 | Reg loss: 0.025 | Tree loss: 0.985 | Accuracy: 0.690000 | 1.26 sec/iter\n",
      "Epoch: 251 | Batch: 004 / 011 | Total loss: 0.949 | Reg loss: 0.025 | Tree loss: 0.949 | Accuracy: 0.694000 | 1.26 sec/iter\n",
      "Epoch: 251 | Batch: 005 / 011 | Total loss: 0.962 | Reg loss: 0.025 | Tree loss: 0.962 | Accuracy: 0.664500 | 1.26 sec/iter\n",
      "Epoch: 251 | Batch: 006 / 011 | Total loss: 0.951 | Reg loss: 0.025 | Tree loss: 0.951 | Accuracy: 0.651000 | 1.26 sec/iter\n",
      "Epoch: 251 | Batch: 007 / 011 | Total loss: 0.972 | Reg loss: 0.025 | Tree loss: 0.972 | Accuracy: 0.629000 | 1.26 sec/iter\n",
      "Epoch: 251 | Batch: 008 / 011 | Total loss: 0.968 | Reg loss: 0.025 | Tree loss: 0.968 | Accuracy: 0.635000 | 1.261 sec/iter\n",
      "Epoch: 251 | Batch: 009 / 011 | Total loss: 0.940 | Reg loss: 0.025 | Tree loss: 0.940 | Accuracy: 0.637000 | 1.261 sec/iter\n",
      "Epoch: 251 | Batch: 010 / 011 | Total loss: 0.889 | Reg loss: 0.025 | Tree loss: 0.889 | Accuracy: 0.668942 | 1.261 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 252 | Batch: 000 / 011 | Total loss: 1.034 | Reg loss: 0.025 | Tree loss: 1.034 | Accuracy: 0.649500 | 1.262 sec/iter\n",
      "Epoch: 252 | Batch: 001 / 011 | Total loss: 1.024 | Reg loss: 0.025 | Tree loss: 1.024 | Accuracy: 0.645500 | 1.262 sec/iter\n",
      "Epoch: 252 | Batch: 002 / 011 | Total loss: 1.000 | Reg loss: 0.025 | Tree loss: 1.000 | Accuracy: 0.681000 | 1.262 sec/iter\n",
      "Epoch: 252 | Batch: 003 / 011 | Total loss: 0.971 | Reg loss: 0.025 | Tree loss: 0.971 | Accuracy: 0.692500 | 1.262 sec/iter\n",
      "Epoch: 252 | Batch: 004 / 011 | Total loss: 0.985 | Reg loss: 0.025 | Tree loss: 0.985 | Accuracy: 0.676000 | 1.262 sec/iter\n",
      "Epoch: 252 | Batch: 005 / 011 | Total loss: 0.983 | Reg loss: 0.025 | Tree loss: 0.983 | Accuracy: 0.650000 | 1.262 sec/iter\n",
      "Epoch: 252 | Batch: 006 / 011 | Total loss: 0.959 | Reg loss: 0.025 | Tree loss: 0.959 | Accuracy: 0.651500 | 1.263 sec/iter\n",
      "Epoch: 252 | Batch: 007 / 011 | Total loss: 0.938 | Reg loss: 0.025 | Tree loss: 0.938 | Accuracy: 0.658000 | 1.263 sec/iter\n",
      "Epoch: 252 | Batch: 008 / 011 | Total loss: 0.958 | Reg loss: 0.025 | Tree loss: 0.958 | Accuracy: 0.647500 | 1.263 sec/iter\n",
      "Epoch: 252 | Batch: 009 / 011 | Total loss: 0.954 | Reg loss: 0.025 | Tree loss: 0.954 | Accuracy: 0.628500 | 1.263 sec/iter\n",
      "Epoch: 252 | Batch: 010 / 011 | Total loss: 0.884 | Reg loss: 0.025 | Tree loss: 0.884 | Accuracy: 0.672355 | 1.263 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 253 | Batch: 000 / 011 | Total loss: 1.013 | Reg loss: 0.025 | Tree loss: 1.013 | Accuracy: 0.671000 | 1.264 sec/iter\n",
      "Epoch: 253 | Batch: 001 / 011 | Total loss: 1.013 | Reg loss: 0.025 | Tree loss: 1.013 | Accuracy: 0.664000 | 1.264 sec/iter\n",
      "Epoch: 253 | Batch: 002 / 011 | Total loss: 1.018 | Reg loss: 0.025 | Tree loss: 1.018 | Accuracy: 0.675500 | 1.264 sec/iter\n",
      "Epoch: 253 | Batch: 003 / 011 | Total loss: 0.978 | Reg loss: 0.025 | Tree loss: 0.978 | Accuracy: 0.690000 | 1.264 sec/iter\n",
      "Epoch: 253 | Batch: 004 / 011 | Total loss: 0.963 | Reg loss: 0.025 | Tree loss: 0.963 | Accuracy: 0.687000 | 1.264 sec/iter\n",
      "Epoch: 253 | Batch: 005 / 011 | Total loss: 0.956 | Reg loss: 0.025 | Tree loss: 0.956 | Accuracy: 0.683000 | 1.264 sec/iter\n",
      "Epoch: 253 | Batch: 006 / 011 | Total loss: 0.961 | Reg loss: 0.025 | Tree loss: 0.961 | Accuracy: 0.655000 | 1.264 sec/iter\n",
      "Epoch: 253 | Batch: 007 / 011 | Total loss: 0.959 | Reg loss: 0.025 | Tree loss: 0.959 | Accuracy: 0.640500 | 1.264 sec/iter\n",
      "Epoch: 253 | Batch: 008 / 011 | Total loss: 0.966 | Reg loss: 0.025 | Tree loss: 0.966 | Accuracy: 0.630000 | 1.264 sec/iter\n",
      "Epoch: 253 | Batch: 009 / 011 | Total loss: 0.959 | Reg loss: 0.025 | Tree loss: 0.959 | Accuracy: 0.633000 | 1.264 sec/iter\n",
      "Epoch: 253 | Batch: 010 / 011 | Total loss: 0.971 | Reg loss: 0.025 | Tree loss: 0.971 | Accuracy: 0.631399 | 1.264 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 254 | Batch: 000 / 011 | Total loss: 1.046 | Reg loss: 0.025 | Tree loss: 1.046 | Accuracy: 0.645000 | 1.266 sec/iter\n",
      "Epoch: 254 | Batch: 001 / 011 | Total loss: 1.041 | Reg loss: 0.025 | Tree loss: 1.041 | Accuracy: 0.655500 | 1.266 sec/iter\n",
      "Epoch: 254 | Batch: 002 / 011 | Total loss: 1.009 | Reg loss: 0.025 | Tree loss: 1.009 | Accuracy: 0.674500 | 1.266 sec/iter\n",
      "Epoch: 254 | Batch: 003 / 011 | Total loss: 0.972 | Reg loss: 0.025 | Tree loss: 0.972 | Accuracy: 0.690500 | 1.266 sec/iter\n",
      "Epoch: 254 | Batch: 004 / 011 | Total loss: 0.970 | Reg loss: 0.025 | Tree loss: 0.970 | Accuracy: 0.683500 | 1.266 sec/iter\n",
      "Epoch: 254 | Batch: 005 / 011 | Total loss: 0.955 | Reg loss: 0.025 | Tree loss: 0.955 | Accuracy: 0.672500 | 1.266 sec/iter\n",
      "Epoch: 254 | Batch: 006 / 011 | Total loss: 0.947 | Reg loss: 0.025 | Tree loss: 0.947 | Accuracy: 0.661000 | 1.266 sec/iter\n",
      "Epoch: 254 | Batch: 007 / 011 | Total loss: 0.955 | Reg loss: 0.025 | Tree loss: 0.955 | Accuracy: 0.638500 | 1.266 sec/iter\n",
      "Epoch: 254 | Batch: 008 / 011 | Total loss: 0.938 | Reg loss: 0.025 | Tree loss: 0.938 | Accuracy: 0.655000 | 1.266 sec/iter\n",
      "Epoch: 254 | Batch: 009 / 011 | Total loss: 0.953 | Reg loss: 0.025 | Tree loss: 0.953 | Accuracy: 0.636500 | 1.266 sec/iter\n",
      "Epoch: 254 | Batch: 010 / 011 | Total loss: 0.956 | Reg loss: 0.025 | Tree loss: 0.956 | Accuracy: 0.604096 | 1.266 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 255 | Batch: 000 / 011 | Total loss: 1.054 | Reg loss: 0.025 | Tree loss: 1.054 | Accuracy: 0.630500 | 1.266 sec/iter\n",
      "Epoch: 255 | Batch: 001 / 011 | Total loss: 1.017 | Reg loss: 0.025 | Tree loss: 1.017 | Accuracy: 0.658500 | 1.266 sec/iter\n",
      "Epoch: 255 | Batch: 002 / 011 | Total loss: 1.003 | Reg loss: 0.025 | Tree loss: 1.003 | Accuracy: 0.675000 | 1.267 sec/iter\n",
      "Epoch: 255 | Batch: 003 / 011 | Total loss: 0.968 | Reg loss: 0.025 | Tree loss: 0.968 | Accuracy: 0.684000 | 1.267 sec/iter\n",
      "Epoch: 255 | Batch: 004 / 011 | Total loss: 0.980 | Reg loss: 0.025 | Tree loss: 0.980 | Accuracy: 0.670000 | 1.267 sec/iter\n",
      "Epoch: 255 | Batch: 005 / 011 | Total loss: 0.970 | Reg loss: 0.025 | Tree loss: 0.970 | Accuracy: 0.679500 | 1.267 sec/iter\n",
      "Epoch: 255 | Batch: 006 / 011 | Total loss: 0.943 | Reg loss: 0.025 | Tree loss: 0.943 | Accuracy: 0.666500 | 1.267 sec/iter\n",
      "Epoch: 255 | Batch: 007 / 011 | Total loss: 0.965 | Reg loss: 0.025 | Tree loss: 0.965 | Accuracy: 0.656500 | 1.267 sec/iter\n",
      "Epoch: 255 | Batch: 008 / 011 | Total loss: 0.923 | Reg loss: 0.025 | Tree loss: 0.923 | Accuracy: 0.662500 | 1.267 sec/iter\n",
      "Epoch: 255 | Batch: 009 / 011 | Total loss: 0.956 | Reg loss: 0.025 | Tree loss: 0.956 | Accuracy: 0.637500 | 1.267 sec/iter\n",
      "Epoch: 255 | Batch: 010 / 011 | Total loss: 0.875 | Reg loss: 0.025 | Tree loss: 0.875 | Accuracy: 0.679181 | 1.267 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 256 | Batch: 000 / 011 | Total loss: 1.049 | Reg loss: 0.025 | Tree loss: 1.049 | Accuracy: 0.635000 | 1.268 sec/iter\n",
      "Epoch: 256 | Batch: 001 / 011 | Total loss: 1.016 | Reg loss: 0.025 | Tree loss: 1.016 | Accuracy: 0.683500 | 1.268 sec/iter\n",
      "Epoch: 256 | Batch: 002 / 011 | Total loss: 0.996 | Reg loss: 0.025 | Tree loss: 0.996 | Accuracy: 0.698000 | 1.268 sec/iter\n",
      "Epoch: 256 | Batch: 003 / 011 | Total loss: 0.990 | Reg loss: 0.025 | Tree loss: 0.990 | Accuracy: 0.692000 | 1.268 sec/iter\n",
      "Epoch: 256 | Batch: 004 / 011 | Total loss: 0.985 | Reg loss: 0.025 | Tree loss: 0.985 | Accuracy: 0.672000 | 1.268 sec/iter\n",
      "Epoch: 256 | Batch: 005 / 011 | Total loss: 0.929 | Reg loss: 0.025 | Tree loss: 0.929 | Accuracy: 0.681500 | 1.269 sec/iter\n",
      "Epoch: 256 | Batch: 006 / 011 | Total loss: 0.975 | Reg loss: 0.025 | Tree loss: 0.975 | Accuracy: 0.631500 | 1.269 sec/iter\n",
      "Epoch: 256 | Batch: 007 / 011 | Total loss: 0.971 | Reg loss: 0.025 | Tree loss: 0.971 | Accuracy: 0.626500 | 1.269 sec/iter\n",
      "Epoch: 256 | Batch: 008 / 011 | Total loss: 0.936 | Reg loss: 0.025 | Tree loss: 0.936 | Accuracy: 0.658500 | 1.269 sec/iter\n",
      "Epoch: 256 | Batch: 009 / 011 | Total loss: 0.933 | Reg loss: 0.025 | Tree loss: 0.933 | Accuracy: 0.634000 | 1.269 sec/iter\n",
      "Epoch: 256 | Batch: 010 / 011 | Total loss: 0.966 | Reg loss: 0.025 | Tree loss: 0.966 | Accuracy: 0.631399 | 1.269 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 257 | Batch: 000 / 011 | Total loss: 1.035 | Reg loss: 0.025 | Tree loss: 1.035 | Accuracy: 0.647000 | 1.269 sec/iter\n",
      "Epoch: 257 | Batch: 001 / 011 | Total loss: 1.041 | Reg loss: 0.025 | Tree loss: 1.041 | Accuracy: 0.634500 | 1.27 sec/iter\n",
      "Epoch: 257 | Batch: 002 / 011 | Total loss: 1.004 | Reg loss: 0.025 | Tree loss: 1.004 | Accuracy: 0.670000 | 1.27 sec/iter\n",
      "Epoch: 257 | Batch: 003 / 011 | Total loss: 0.987 | Reg loss: 0.025 | Tree loss: 0.987 | Accuracy: 0.678500 | 1.27 sec/iter\n",
      "Epoch: 257 | Batch: 004 / 011 | Total loss: 0.969 | Reg loss: 0.025 | Tree loss: 0.969 | Accuracy: 0.686000 | 1.27 sec/iter\n",
      "Epoch: 257 | Batch: 005 / 011 | Total loss: 0.956 | Reg loss: 0.025 | Tree loss: 0.956 | Accuracy: 0.680500 | 1.27 sec/iter\n",
      "Epoch: 257 | Batch: 006 / 011 | Total loss: 0.956 | Reg loss: 0.025 | Tree loss: 0.956 | Accuracy: 0.646000 | 1.27 sec/iter\n",
      "Epoch: 257 | Batch: 007 / 011 | Total loss: 0.941 | Reg loss: 0.025 | Tree loss: 0.941 | Accuracy: 0.648000 | 1.27 sec/iter\n",
      "Epoch: 257 | Batch: 008 / 011 | Total loss: 0.956 | Reg loss: 0.025 | Tree loss: 0.956 | Accuracy: 0.639000 | 1.27 sec/iter\n",
      "Epoch: 257 | Batch: 009 / 011 | Total loss: 0.941 | Reg loss: 0.025 | Tree loss: 0.941 | Accuracy: 0.645000 | 1.27 sec/iter\n",
      "Epoch: 257 | Batch: 010 / 011 | Total loss: 0.891 | Reg loss: 0.025 | Tree loss: 0.891 | Accuracy: 0.672355 | 1.27 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 258 | Batch: 000 / 011 | Total loss: 1.040 | Reg loss: 0.025 | Tree loss: 1.040 | Accuracy: 0.649000 | 1.271 sec/iter\n",
      "Epoch: 258 | Batch: 001 / 011 | Total loss: 0.992 | Reg loss: 0.025 | Tree loss: 0.992 | Accuracy: 0.692500 | 1.271 sec/iter\n",
      "Epoch: 258 | Batch: 002 / 011 | Total loss: 1.004 | Reg loss: 0.025 | Tree loss: 1.004 | Accuracy: 0.686500 | 1.271 sec/iter\n",
      "Epoch: 258 | Batch: 003 / 011 | Total loss: 0.990 | Reg loss: 0.025 | Tree loss: 0.990 | Accuracy: 0.676500 | 1.271 sec/iter\n",
      "Epoch: 258 | Batch: 004 / 011 | Total loss: 0.974 | Reg loss: 0.025 | Tree loss: 0.974 | Accuracy: 0.671500 | 1.271 sec/iter\n",
      "Epoch: 258 | Batch: 005 / 011 | Total loss: 0.949 | Reg loss: 0.025 | Tree loss: 0.949 | Accuracy: 0.665000 | 1.271 sec/iter\n",
      "Epoch: 258 | Batch: 006 / 011 | Total loss: 0.961 | Reg loss: 0.025 | Tree loss: 0.961 | Accuracy: 0.653000 | 1.271 sec/iter\n",
      "Epoch: 258 | Batch: 007 / 011 | Total loss: 0.961 | Reg loss: 0.025 | Tree loss: 0.961 | Accuracy: 0.629000 | 1.271 sec/iter\n",
      "Epoch: 258 | Batch: 008 / 011 | Total loss: 0.928 | Reg loss: 0.025 | Tree loss: 0.928 | Accuracy: 0.654500 | 1.272 sec/iter\n",
      "Epoch: 258 | Batch: 009 / 011 | Total loss: 0.959 | Reg loss: 0.025 | Tree loss: 0.959 | Accuracy: 0.623500 | 1.272 sec/iter\n",
      "Epoch: 258 | Batch: 010 / 011 | Total loss: 0.986 | Reg loss: 0.025 | Tree loss: 0.986 | Accuracy: 0.658703 | 1.272 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 259 | Batch: 000 / 011 | Total loss: 1.010 | Reg loss: 0.025 | Tree loss: 1.010 | Accuracy: 0.666500 | 1.273 sec/iter\n",
      "Epoch: 259 | Batch: 001 / 011 | Total loss: 1.024 | Reg loss: 0.025 | Tree loss: 1.024 | Accuracy: 0.657500 | 1.273 sec/iter\n",
      "Epoch: 259 | Batch: 002 / 011 | Total loss: 0.986 | Reg loss: 0.025 | Tree loss: 0.986 | Accuracy: 0.684500 | 1.273 sec/iter\n",
      "Epoch: 259 | Batch: 003 / 011 | Total loss: 0.991 | Reg loss: 0.025 | Tree loss: 0.991 | Accuracy: 0.678500 | 1.273 sec/iter\n",
      "Epoch: 259 | Batch: 004 / 011 | Total loss: 0.982 | Reg loss: 0.025 | Tree loss: 0.982 | Accuracy: 0.678500 | 1.273 sec/iter\n",
      "Epoch: 259 | Batch: 005 / 011 | Total loss: 0.956 | Reg loss: 0.025 | Tree loss: 0.956 | Accuracy: 0.672000 | 1.273 sec/iter\n",
      "Epoch: 259 | Batch: 006 / 011 | Total loss: 0.943 | Reg loss: 0.025 | Tree loss: 0.943 | Accuracy: 0.677500 | 1.272 sec/iter\n",
      "Epoch: 259 | Batch: 007 / 011 | Total loss: 0.944 | Reg loss: 0.025 | Tree loss: 0.944 | Accuracy: 0.644000 | 1.272 sec/iter\n",
      "Epoch: 259 | Batch: 008 / 011 | Total loss: 0.939 | Reg loss: 0.025 | Tree loss: 0.939 | Accuracy: 0.644000 | 1.272 sec/iter\n",
      "Epoch: 259 | Batch: 009 / 011 | Total loss: 0.987 | Reg loss: 0.025 | Tree loss: 0.987 | Accuracy: 0.617000 | 1.272 sec/iter\n",
      "Epoch: 259 | Batch: 010 / 011 | Total loss: 0.990 | Reg loss: 0.025 | Tree loss: 0.990 | Accuracy: 0.610922 | 1.272 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 260 | Batch: 000 / 011 | Total loss: 1.031 | Reg loss: 0.025 | Tree loss: 1.031 | Accuracy: 0.655000 | 1.273 sec/iter\n",
      "Epoch: 260 | Batch: 001 / 011 | Total loss: 1.013 | Reg loss: 0.025 | Tree loss: 1.013 | Accuracy: 0.664500 | 1.273 sec/iter\n",
      "Epoch: 260 | Batch: 002 / 011 | Total loss: 1.002 | Reg loss: 0.025 | Tree loss: 1.002 | Accuracy: 0.685000 | 1.273 sec/iter\n",
      "Epoch: 260 | Batch: 003 / 011 | Total loss: 0.992 | Reg loss: 0.025 | Tree loss: 0.992 | Accuracy: 0.687500 | 1.273 sec/iter\n",
      "Epoch: 260 | Batch: 004 / 011 | Total loss: 0.975 | Reg loss: 0.025 | Tree loss: 0.975 | Accuracy: 0.673500 | 1.273 sec/iter\n",
      "Epoch: 260 | Batch: 005 / 011 | Total loss: 0.973 | Reg loss: 0.025 | Tree loss: 0.973 | Accuracy: 0.666500 | 1.273 sec/iter\n",
      "Epoch: 260 | Batch: 006 / 011 | Total loss: 0.964 | Reg loss: 0.025 | Tree loss: 0.964 | Accuracy: 0.647500 | 1.273 sec/iter\n",
      "Epoch: 260 | Batch: 007 / 011 | Total loss: 0.913 | Reg loss: 0.025 | Tree loss: 0.913 | Accuracy: 0.665000 | 1.273 sec/iter\n",
      "Epoch: 260 | Batch: 008 / 011 | Total loss: 0.947 | Reg loss: 0.025 | Tree loss: 0.947 | Accuracy: 0.662500 | 1.273 sec/iter\n",
      "Epoch: 260 | Batch: 009 / 011 | Total loss: 0.935 | Reg loss: 0.025 | Tree loss: 0.935 | Accuracy: 0.646000 | 1.273 sec/iter\n",
      "Epoch: 260 | Batch: 010 / 011 | Total loss: 1.004 | Reg loss: 0.025 | Tree loss: 1.004 | Accuracy: 0.590444 | 1.273 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 261 | Batch: 000 / 011 | Total loss: 1.037 | Reg loss: 0.025 | Tree loss: 1.037 | Accuracy: 0.661000 | 1.273 sec/iter\n",
      "Epoch: 261 | Batch: 001 / 011 | Total loss: 1.014 | Reg loss: 0.025 | Tree loss: 1.014 | Accuracy: 0.667000 | 1.273 sec/iter\n",
      "Epoch: 261 | Batch: 002 / 011 | Total loss: 0.992 | Reg loss: 0.025 | Tree loss: 0.992 | Accuracy: 0.677500 | 1.273 sec/iter\n",
      "Epoch: 261 | Batch: 003 / 011 | Total loss: 0.986 | Reg loss: 0.025 | Tree loss: 0.986 | Accuracy: 0.681000 | 1.273 sec/iter\n",
      "Epoch: 261 | Batch: 004 / 011 | Total loss: 0.967 | Reg loss: 0.025 | Tree loss: 0.967 | Accuracy: 0.683000 | 1.273 sec/iter\n",
      "Epoch: 261 | Batch: 005 / 011 | Total loss: 0.965 | Reg loss: 0.025 | Tree loss: 0.965 | Accuracy: 0.665500 | 1.274 sec/iter\n",
      "Epoch: 261 | Batch: 006 / 011 | Total loss: 0.950 | Reg loss: 0.025 | Tree loss: 0.950 | Accuracy: 0.658000 | 1.274 sec/iter\n",
      "Epoch: 261 | Batch: 007 / 011 | Total loss: 0.939 | Reg loss: 0.025 | Tree loss: 0.939 | Accuracy: 0.661000 | 1.274 sec/iter\n",
      "Epoch: 261 | Batch: 008 / 011 | Total loss: 0.981 | Reg loss: 0.025 | Tree loss: 0.981 | Accuracy: 0.636500 | 1.274 sec/iter\n",
      "Epoch: 261 | Batch: 009 / 011 | Total loss: 0.937 | Reg loss: 0.025 | Tree loss: 0.937 | Accuracy: 0.646000 | 1.274 sec/iter\n",
      "Epoch: 261 | Batch: 010 / 011 | Total loss: 0.916 | Reg loss: 0.025 | Tree loss: 0.916 | Accuracy: 0.621160 | 1.274 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 262 | Batch: 000 / 011 | Total loss: 1.024 | Reg loss: 0.025 | Tree loss: 1.024 | Accuracy: 0.650000 | 1.275 sec/iter\n",
      "Epoch: 262 | Batch: 001 / 011 | Total loss: 1.031 | Reg loss: 0.025 | Tree loss: 1.031 | Accuracy: 0.663000 | 1.275 sec/iter\n",
      "Epoch: 262 | Batch: 002 / 011 | Total loss: 0.984 | Reg loss: 0.025 | Tree loss: 0.984 | Accuracy: 0.689500 | 1.275 sec/iter\n",
      "Epoch: 262 | Batch: 003 / 011 | Total loss: 0.980 | Reg loss: 0.025 | Tree loss: 0.980 | Accuracy: 0.687000 | 1.275 sec/iter\n",
      "Epoch: 262 | Batch: 004 / 011 | Total loss: 0.969 | Reg loss: 0.025 | Tree loss: 0.969 | Accuracy: 0.682000 | 1.275 sec/iter\n",
      "Epoch: 262 | Batch: 005 / 011 | Total loss: 0.933 | Reg loss: 0.025 | Tree loss: 0.933 | Accuracy: 0.682500 | 1.275 sec/iter\n",
      "Epoch: 262 | Batch: 006 / 011 | Total loss: 0.974 | Reg loss: 0.025 | Tree loss: 0.974 | Accuracy: 0.643000 | 1.275 sec/iter\n",
      "Epoch: 262 | Batch: 007 / 011 | Total loss: 0.987 | Reg loss: 0.025 | Tree loss: 0.987 | Accuracy: 0.616000 | 1.275 sec/iter\n",
      "Epoch: 262 | Batch: 008 / 011 | Total loss: 0.975 | Reg loss: 0.025 | Tree loss: 0.975 | Accuracy: 0.628000 | 1.275 sec/iter\n",
      "Epoch: 262 | Batch: 009 / 011 | Total loss: 0.926 | Reg loss: 0.025 | Tree loss: 0.926 | Accuracy: 0.656000 | 1.275 sec/iter\n",
      "Epoch: 262 | Batch: 010 / 011 | Total loss: 0.884 | Reg loss: 0.025 | Tree loss: 0.884 | Accuracy: 0.692833 | 1.275 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 263 | Batch: 000 / 011 | Total loss: 1.034 | Reg loss: 0.025 | Tree loss: 1.034 | Accuracy: 0.645500 | 1.276 sec/iter\n",
      "Epoch: 263 | Batch: 001 / 011 | Total loss: 1.048 | Reg loss: 0.025 | Tree loss: 1.048 | Accuracy: 0.651000 | 1.276 sec/iter\n",
      "Epoch: 263 | Batch: 002 / 011 | Total loss: 1.017 | Reg loss: 0.025 | Tree loss: 1.017 | Accuracy: 0.683000 | 1.276 sec/iter\n",
      "Epoch: 263 | Batch: 003 / 011 | Total loss: 0.976 | Reg loss: 0.025 | Tree loss: 0.976 | Accuracy: 0.681500 | 1.276 sec/iter\n",
      "Epoch: 263 | Batch: 004 / 011 | Total loss: 0.951 | Reg loss: 0.025 | Tree loss: 0.951 | Accuracy: 0.700000 | 1.276 sec/iter\n",
      "Epoch: 263 | Batch: 005 / 011 | Total loss: 0.949 | Reg loss: 0.025 | Tree loss: 0.949 | Accuracy: 0.689500 | 1.276 sec/iter\n",
      "Epoch: 263 | Batch: 006 / 011 | Total loss: 0.914 | Reg loss: 0.025 | Tree loss: 0.914 | Accuracy: 0.674000 | 1.276 sec/iter\n",
      "Epoch: 263 | Batch: 007 / 011 | Total loss: 0.937 | Reg loss: 0.025 | Tree loss: 0.937 | Accuracy: 0.652500 | 1.276 sec/iter\n",
      "Epoch: 263 | Batch: 008 / 011 | Total loss: 0.977 | Reg loss: 0.025 | Tree loss: 0.977 | Accuracy: 0.630000 | 1.276 sec/iter\n",
      "Epoch: 263 | Batch: 009 / 011 | Total loss: 0.956 | Reg loss: 0.025 | Tree loss: 0.956 | Accuracy: 0.648500 | 1.276 sec/iter\n",
      "Epoch: 263 | Batch: 010 / 011 | Total loss: 0.940 | Reg loss: 0.025 | Tree loss: 0.940 | Accuracy: 0.634812 | 1.276 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 8: 0.9723756906077345\n",
      "Epoch: 264 | Batch: 000 / 011 | Total loss: 1.050 | Reg loss: 0.025 | Tree loss: 1.050 | Accuracy: 0.625500 | 1.277 sec/iter\n",
      "Epoch: 264 | Batch: 001 / 011 | Total loss: 1.028 | Reg loss: 0.025 | Tree loss: 1.028 | Accuracy: 0.658000 | 1.277 sec/iter\n",
      "Epoch: 264 | Batch: 002 / 011 | Total loss: 0.998 | Reg loss: 0.025 | Tree loss: 0.998 | Accuracy: 0.677500 | 1.277 sec/iter\n",
      "Epoch: 264 | Batch: 003 / 011 | Total loss: 0.977 | Reg loss: 0.025 | Tree loss: 0.977 | Accuracy: 0.690500 | 1.277 sec/iter\n",
      "Epoch: 264 | Batch: 004 / 011 | Total loss: 0.964 | Reg loss: 0.025 | Tree loss: 0.964 | Accuracy: 0.693000 | 1.277 sec/iter\n",
      "Epoch: 264 | Batch: 005 / 011 | Total loss: 0.984 | Reg loss: 0.025 | Tree loss: 0.984 | Accuracy: 0.661500 | 1.277 sec/iter\n",
      "Epoch: 264 | Batch: 006 / 011 | Total loss: 0.953 | Reg loss: 0.025 | Tree loss: 0.953 | Accuracy: 0.654000 | 1.277 sec/iter\n",
      "Epoch: 264 | Batch: 007 / 011 | Total loss: 0.917 | Reg loss: 0.025 | Tree loss: 0.917 | Accuracy: 0.677000 | 1.277 sec/iter\n",
      "Epoch: 264 | Batch: 008 / 011 | Total loss: 0.950 | Reg loss: 0.025 | Tree loss: 0.950 | Accuracy: 0.641500 | 1.277 sec/iter\n",
      "Epoch: 264 | Batch: 009 / 011 | Total loss: 0.941 | Reg loss: 0.025 | Tree loss: 0.941 | Accuracy: 0.644500 | 1.277 sec/iter\n",
      "Epoch: 264 | Batch: 010 / 011 | Total loss: 0.887 | Reg loss: 0.025 | Tree loss: 0.887 | Accuracy: 0.662116 | 1.277 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 265 | Batch: 000 / 011 | Total loss: 1.035 | Reg loss: 0.025 | Tree loss: 1.035 | Accuracy: 0.648500 | 1.278 sec/iter\n",
      "Epoch: 265 | Batch: 001 / 011 | Total loss: 1.009 | Reg loss: 0.025 | Tree loss: 1.009 | Accuracy: 0.666000 | 1.278 sec/iter\n",
      "Epoch: 265 | Batch: 002 / 011 | Total loss: 1.025 | Reg loss: 0.025 | Tree loss: 1.025 | Accuracy: 0.672500 | 1.278 sec/iter\n",
      "Epoch: 265 | Batch: 003 / 011 | Total loss: 0.978 | Reg loss: 0.025 | Tree loss: 0.978 | Accuracy: 0.683500 | 1.278 sec/iter\n",
      "Epoch: 265 | Batch: 004 / 011 | Total loss: 0.982 | Reg loss: 0.025 | Tree loss: 0.982 | Accuracy: 0.679000 | 1.278 sec/iter\n",
      "Epoch: 265 | Batch: 005 / 011 | Total loss: 0.958 | Reg loss: 0.025 | Tree loss: 0.958 | Accuracy: 0.655000 | 1.278 sec/iter\n",
      "Epoch: 265 | Batch: 006 / 011 | Total loss: 0.976 | Reg loss: 0.025 | Tree loss: 0.976 | Accuracy: 0.638500 | 1.278 sec/iter\n",
      "Epoch: 265 | Batch: 007 / 011 | Total loss: 0.939 | Reg loss: 0.025 | Tree loss: 0.939 | Accuracy: 0.647000 | 1.278 sec/iter\n",
      "Epoch: 265 | Batch: 008 / 011 | Total loss: 0.913 | Reg loss: 0.025 | Tree loss: 0.913 | Accuracy: 0.657500 | 1.278 sec/iter\n",
      "Epoch: 265 | Batch: 009 / 011 | Total loss: 0.939 | Reg loss: 0.025 | Tree loss: 0.939 | Accuracy: 0.634000 | 1.279 sec/iter\n",
      "Epoch: 265 | Batch: 010 / 011 | Total loss: 0.935 | Reg loss: 0.025 | Tree loss: 0.935 | Accuracy: 0.641638 | 1.279 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 266 | Batch: 000 / 011 | Total loss: 1.024 | Reg loss: 0.025 | Tree loss: 1.024 | Accuracy: 0.650500 | 1.279 sec/iter\n",
      "Epoch: 266 | Batch: 001 / 011 | Total loss: 1.015 | Reg loss: 0.025 | Tree loss: 1.015 | Accuracy: 0.651500 | 1.279 sec/iter\n",
      "Epoch: 266 | Batch: 002 / 011 | Total loss: 0.978 | Reg loss: 0.025 | Tree loss: 0.978 | Accuracy: 0.711500 | 1.279 sec/iter\n",
      "Epoch: 266 | Batch: 003 / 011 | Total loss: 0.996 | Reg loss: 0.025 | Tree loss: 0.996 | Accuracy: 0.672000 | 1.279 sec/iter\n",
      "Epoch: 266 | Batch: 004 / 011 | Total loss: 0.954 | Reg loss: 0.025 | Tree loss: 0.954 | Accuracy: 0.693000 | 1.279 sec/iter\n",
      "Epoch: 266 | Batch: 005 / 011 | Total loss: 0.964 | Reg loss: 0.025 | Tree loss: 0.964 | Accuracy: 0.676500 | 1.279 sec/iter\n",
      "Epoch: 266 | Batch: 006 / 011 | Total loss: 0.958 | Reg loss: 0.025 | Tree loss: 0.958 | Accuracy: 0.654500 | 1.279 sec/iter\n",
      "Epoch: 266 | Batch: 007 / 011 | Total loss: 0.941 | Reg loss: 0.025 | Tree loss: 0.941 | Accuracy: 0.655000 | 1.279 sec/iter\n",
      "Epoch: 266 | Batch: 008 / 011 | Total loss: 0.965 | Reg loss: 0.025 | Tree loss: 0.965 | Accuracy: 0.626500 | 1.279 sec/iter\n",
      "Epoch: 266 | Batch: 009 / 011 | Total loss: 0.957 | Reg loss: 0.025 | Tree loss: 0.957 | Accuracy: 0.641500 | 1.279 sec/iter\n",
      "Epoch: 266 | Batch: 010 / 011 | Total loss: 0.930 | Reg loss: 0.025 | Tree loss: 0.930 | Accuracy: 0.614334 | 1.28 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 267 | Batch: 000 / 011 | Total loss: 1.053 | Reg loss: 0.025 | Tree loss: 1.053 | Accuracy: 0.634500 | 1.281 sec/iter\n",
      "Epoch: 267 | Batch: 001 / 011 | Total loss: 1.001 | Reg loss: 0.025 | Tree loss: 1.001 | Accuracy: 0.669000 | 1.281 sec/iter\n",
      "Epoch: 267 | Batch: 002 / 011 | Total loss: 0.996 | Reg loss: 0.025 | Tree loss: 0.996 | Accuracy: 0.690500 | 1.281 sec/iter\n",
      "Epoch: 267 | Batch: 003 / 011 | Total loss: 0.960 | Reg loss: 0.025 | Tree loss: 0.960 | Accuracy: 0.706500 | 1.281 sec/iter\n",
      "Epoch: 267 | Batch: 004 / 011 | Total loss: 0.991 | Reg loss: 0.025 | Tree loss: 0.991 | Accuracy: 0.674500 | 1.281 sec/iter\n",
      "Epoch: 267 | Batch: 005 / 011 | Total loss: 0.966 | Reg loss: 0.025 | Tree loss: 0.966 | Accuracy: 0.669500 | 1.281 sec/iter\n",
      "Epoch: 267 | Batch: 006 / 011 | Total loss: 0.947 | Reg loss: 0.025 | Tree loss: 0.947 | Accuracy: 0.649000 | 1.281 sec/iter\n",
      "Epoch: 267 | Batch: 007 / 011 | Total loss: 0.938 | Reg loss: 0.025 | Tree loss: 0.938 | Accuracy: 0.657500 | 1.281 sec/iter\n",
      "Epoch: 267 | Batch: 008 / 011 | Total loss: 0.948 | Reg loss: 0.025 | Tree loss: 0.948 | Accuracy: 0.641000 | 1.281 sec/iter\n",
      "Epoch: 267 | Batch: 009 / 011 | Total loss: 0.966 | Reg loss: 0.025 | Tree loss: 0.966 | Accuracy: 0.626500 | 1.281 sec/iter\n",
      "Epoch: 267 | Batch: 010 / 011 | Total loss: 0.925 | Reg loss: 0.025 | Tree loss: 0.925 | Accuracy: 0.627986 | 1.281 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 268 | Batch: 000 / 011 | Total loss: 1.042 | Reg loss: 0.025 | Tree loss: 1.042 | Accuracy: 0.648000 | 1.283 sec/iter\n",
      "Epoch: 268 | Batch: 001 / 011 | Total loss: 1.023 | Reg loss: 0.025 | Tree loss: 1.023 | Accuracy: 0.647500 | 1.283 sec/iter\n",
      "Epoch: 268 | Batch: 002 / 011 | Total loss: 0.987 | Reg loss: 0.025 | Tree loss: 0.987 | Accuracy: 0.685000 | 1.283 sec/iter\n",
      "Epoch: 268 | Batch: 003 / 011 | Total loss: 0.970 | Reg loss: 0.025 | Tree loss: 0.970 | Accuracy: 0.679000 | 1.283 sec/iter\n",
      "Epoch: 268 | Batch: 004 / 011 | Total loss: 0.954 | Reg loss: 0.025 | Tree loss: 0.954 | Accuracy: 0.675500 | 1.283 sec/iter\n",
      "Epoch: 268 | Batch: 005 / 011 | Total loss: 0.953 | Reg loss: 0.025 | Tree loss: 0.953 | Accuracy: 0.656500 | 1.283 sec/iter\n",
      "Epoch: 268 | Batch: 006 / 011 | Total loss: 0.964 | Reg loss: 0.025 | Tree loss: 0.964 | Accuracy: 0.664000 | 1.283 sec/iter\n",
      "Epoch: 268 | Batch: 007 / 011 | Total loss: 0.972 | Reg loss: 0.025 | Tree loss: 0.972 | Accuracy: 0.637500 | 1.283 sec/iter\n",
      "Epoch: 268 | Batch: 008 / 011 | Total loss: 0.949 | Reg loss: 0.025 | Tree loss: 0.949 | Accuracy: 0.646500 | 1.283 sec/iter\n",
      "Epoch: 268 | Batch: 009 / 011 | Total loss: 0.941 | Reg loss: 0.025 | Tree loss: 0.941 | Accuracy: 0.640500 | 1.283 sec/iter\n",
      "Epoch: 268 | Batch: 010 / 011 | Total loss: 0.940 | Reg loss: 0.025 | Tree loss: 0.940 | Accuracy: 0.634812 | 1.283 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 269 | Batch: 000 / 011 | Total loss: 1.041 | Reg loss: 0.025 | Tree loss: 1.041 | Accuracy: 0.636000 | 1.284 sec/iter\n",
      "Epoch: 269 | Batch: 001 / 011 | Total loss: 1.043 | Reg loss: 0.025 | Tree loss: 1.043 | Accuracy: 0.646000 | 1.284 sec/iter\n",
      "Epoch: 269 | Batch: 002 / 011 | Total loss: 1.009 | Reg loss: 0.025 | Tree loss: 1.009 | Accuracy: 0.673000 | 1.284 sec/iter\n",
      "Epoch: 269 | Batch: 003 / 011 | Total loss: 0.954 | Reg loss: 0.025 | Tree loss: 0.954 | Accuracy: 0.701000 | 1.284 sec/iter\n",
      "Epoch: 269 | Batch: 004 / 011 | Total loss: 0.982 | Reg loss: 0.025 | Tree loss: 0.982 | Accuracy: 0.677000 | 1.284 sec/iter\n",
      "Epoch: 269 | Batch: 005 / 011 | Total loss: 0.947 | Reg loss: 0.025 | Tree loss: 0.947 | Accuracy: 0.675500 | 1.284 sec/iter\n",
      "Epoch: 269 | Batch: 006 / 011 | Total loss: 0.943 | Reg loss: 0.025 | Tree loss: 0.943 | Accuracy: 0.661500 | 1.284 sec/iter\n",
      "Epoch: 269 | Batch: 007 / 011 | Total loss: 0.949 | Reg loss: 0.025 | Tree loss: 0.949 | Accuracy: 0.656500 | 1.284 sec/iter\n",
      "Epoch: 269 | Batch: 008 / 011 | Total loss: 0.934 | Reg loss: 0.025 | Tree loss: 0.934 | Accuracy: 0.638000 | 1.284 sec/iter\n",
      "Epoch: 269 | Batch: 009 / 011 | Total loss: 0.950 | Reg loss: 0.025 | Tree loss: 0.950 | Accuracy: 0.645500 | 1.284 sec/iter\n",
      "Epoch: 269 | Batch: 010 / 011 | Total loss: 1.008 | Reg loss: 0.025 | Tree loss: 1.008 | Accuracy: 0.576792 | 1.284 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 270 | Batch: 000 / 011 | Total loss: 1.044 | Reg loss: 0.025 | Tree loss: 1.044 | Accuracy: 0.641000 | 1.285 sec/iter\n",
      "Epoch: 270 | Batch: 001 / 011 | Total loss: 1.013 | Reg loss: 0.025 | Tree loss: 1.013 | Accuracy: 0.661000 | 1.285 sec/iter\n",
      "Epoch: 270 | Batch: 002 / 011 | Total loss: 1.014 | Reg loss: 0.025 | Tree loss: 1.014 | Accuracy: 0.675000 | 1.285 sec/iter\n",
      "Epoch: 270 | Batch: 003 / 011 | Total loss: 0.968 | Reg loss: 0.025 | Tree loss: 0.968 | Accuracy: 0.696000 | 1.285 sec/iter\n",
      "Epoch: 270 | Batch: 004 / 011 | Total loss: 0.984 | Reg loss: 0.025 | Tree loss: 0.984 | Accuracy: 0.675500 | 1.285 sec/iter\n",
      "Epoch: 270 | Batch: 005 / 011 | Total loss: 0.975 | Reg loss: 0.025 | Tree loss: 0.975 | Accuracy: 0.658000 | 1.285 sec/iter\n",
      "Epoch: 270 | Batch: 006 / 011 | Total loss: 0.937 | Reg loss: 0.025 | Tree loss: 0.937 | Accuracy: 0.664000 | 1.286 sec/iter\n",
      "Epoch: 270 | Batch: 007 / 011 | Total loss: 0.951 | Reg loss: 0.025 | Tree loss: 0.951 | Accuracy: 0.632000 | 1.286 sec/iter\n",
      "Epoch: 270 | Batch: 008 / 011 | Total loss: 0.939 | Reg loss: 0.025 | Tree loss: 0.939 | Accuracy: 0.653500 | 1.286 sec/iter\n",
      "Epoch: 270 | Batch: 009 / 011 | Total loss: 0.936 | Reg loss: 0.025 | Tree loss: 0.936 | Accuracy: 0.651000 | 1.286 sec/iter\n",
      "Epoch: 270 | Batch: 010 / 011 | Total loss: 1.004 | Reg loss: 0.025 | Tree loss: 1.004 | Accuracy: 0.614334 | 1.286 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 271 | Batch: 000 / 011 | Total loss: 1.027 | Reg loss: 0.025 | Tree loss: 1.027 | Accuracy: 0.654500 | 1.286 sec/iter\n",
      "Epoch: 271 | Batch: 001 / 011 | Total loss: 1.031 | Reg loss: 0.025 | Tree loss: 1.031 | Accuracy: 0.631500 | 1.286 sec/iter\n",
      "Epoch: 271 | Batch: 002 / 011 | Total loss: 1.005 | Reg loss: 0.025 | Tree loss: 1.005 | Accuracy: 0.666000 | 1.286 sec/iter\n",
      "Epoch: 271 | Batch: 003 / 011 | Total loss: 0.980 | Reg loss: 0.025 | Tree loss: 0.980 | Accuracy: 0.669500 | 1.286 sec/iter\n",
      "Epoch: 271 | Batch: 004 / 011 | Total loss: 0.948 | Reg loss: 0.025 | Tree loss: 0.948 | Accuracy: 0.685500 | 1.286 sec/iter\n",
      "Epoch: 271 | Batch: 005 / 011 | Total loss: 0.971 | Reg loss: 0.025 | Tree loss: 0.971 | Accuracy: 0.665000 | 1.286 sec/iter\n",
      "Epoch: 271 | Batch: 006 / 011 | Total loss: 0.950 | Reg loss: 0.025 | Tree loss: 0.950 | Accuracy: 0.650500 | 1.286 sec/iter\n",
      "Epoch: 271 | Batch: 007 / 011 | Total loss: 0.929 | Reg loss: 0.025 | Tree loss: 0.929 | Accuracy: 0.662500 | 1.286 sec/iter\n",
      "Epoch: 271 | Batch: 008 / 011 | Total loss: 0.953 | Reg loss: 0.025 | Tree loss: 0.953 | Accuracy: 0.641000 | 1.286 sec/iter\n",
      "Epoch: 271 | Batch: 009 / 011 | Total loss: 0.952 | Reg loss: 0.025 | Tree loss: 0.952 | Accuracy: 0.639500 | 1.286 sec/iter\n",
      "Epoch: 271 | Batch: 010 / 011 | Total loss: 0.931 | Reg loss: 0.025 | Tree loss: 0.931 | Accuracy: 0.638225 | 1.286 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 272 | Batch: 000 / 011 | Total loss: 1.050 | Reg loss: 0.025 | Tree loss: 1.050 | Accuracy: 0.645000 | 1.287 sec/iter\n",
      "Epoch: 272 | Batch: 001 / 011 | Total loss: 1.015 | Reg loss: 0.025 | Tree loss: 1.015 | Accuracy: 0.662000 | 1.286 sec/iter\n",
      "Epoch: 272 | Batch: 002 / 011 | Total loss: 0.988 | Reg loss: 0.025 | Tree loss: 0.988 | Accuracy: 0.695500 | 1.286 sec/iter\n",
      "Epoch: 272 | Batch: 003 / 011 | Total loss: 0.971 | Reg loss: 0.025 | Tree loss: 0.971 | Accuracy: 0.690500 | 1.287 sec/iter\n",
      "Epoch: 272 | Batch: 004 / 011 | Total loss: 0.979 | Reg loss: 0.025 | Tree loss: 0.979 | Accuracy: 0.682000 | 1.287 sec/iter\n",
      "Epoch: 272 | Batch: 005 / 011 | Total loss: 0.949 | Reg loss: 0.025 | Tree loss: 0.949 | Accuracy: 0.681000 | 1.287 sec/iter\n",
      "Epoch: 272 | Batch: 006 / 011 | Total loss: 0.945 | Reg loss: 0.025 | Tree loss: 0.945 | Accuracy: 0.659000 | 1.287 sec/iter\n",
      "Epoch: 272 | Batch: 007 / 011 | Total loss: 0.929 | Reg loss: 0.025 | Tree loss: 0.929 | Accuracy: 0.661000 | 1.287 sec/iter\n",
      "Epoch: 272 | Batch: 008 / 011 | Total loss: 0.956 | Reg loss: 0.025 | Tree loss: 0.956 | Accuracy: 0.633500 | 1.287 sec/iter\n",
      "Epoch: 272 | Batch: 009 / 011 | Total loss: 0.943 | Reg loss: 0.025 | Tree loss: 0.943 | Accuracy: 0.640500 | 1.287 sec/iter\n",
      "Epoch: 272 | Batch: 010 / 011 | Total loss: 0.999 | Reg loss: 0.025 | Tree loss: 0.999 | Accuracy: 0.610922 | 1.287 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 273 | Batch: 000 / 011 | Total loss: 1.057 | Reg loss: 0.025 | Tree loss: 1.057 | Accuracy: 0.628500 | 1.287 sec/iter\n",
      "Epoch: 273 | Batch: 001 / 011 | Total loss: 1.037 | Reg loss: 0.025 | Tree loss: 1.037 | Accuracy: 0.644500 | 1.287 sec/iter\n",
      "Epoch: 273 | Batch: 002 / 011 | Total loss: 0.999 | Reg loss: 0.025 | Tree loss: 0.999 | Accuracy: 0.673500 | 1.287 sec/iter\n",
      "Epoch: 273 | Batch: 003 / 011 | Total loss: 0.970 | Reg loss: 0.025 | Tree loss: 0.970 | Accuracy: 0.691500 | 1.287 sec/iter\n",
      "Epoch: 273 | Batch: 004 / 011 | Total loss: 0.949 | Reg loss: 0.025 | Tree loss: 0.949 | Accuracy: 0.699500 | 1.287 sec/iter\n",
      "Epoch: 273 | Batch: 005 / 011 | Total loss: 0.959 | Reg loss: 0.025 | Tree loss: 0.959 | Accuracy: 0.677500 | 1.287 sec/iter\n",
      "Epoch: 273 | Batch: 006 / 011 | Total loss: 0.935 | Reg loss: 0.025 | Tree loss: 0.935 | Accuracy: 0.667500 | 1.287 sec/iter\n",
      "Epoch: 273 | Batch: 007 / 011 | Total loss: 0.961 | Reg loss: 0.025 | Tree loss: 0.961 | Accuracy: 0.640000 | 1.287 sec/iter\n",
      "Epoch: 273 | Batch: 008 / 011 | Total loss: 0.949 | Reg loss: 0.025 | Tree loss: 0.949 | Accuracy: 0.639500 | 1.287 sec/iter\n",
      "Epoch: 273 | Batch: 009 / 011 | Total loss: 0.935 | Reg loss: 0.025 | Tree loss: 0.935 | Accuracy: 0.645500 | 1.287 sec/iter\n",
      "Epoch: 273 | Batch: 010 / 011 | Total loss: 0.946 | Reg loss: 0.025 | Tree loss: 0.946 | Accuracy: 0.665529 | 1.287 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 274 | Batch: 000 / 011 | Total loss: 1.033 | Reg loss: 0.025 | Tree loss: 1.033 | Accuracy: 0.654500 | 1.288 sec/iter\n",
      "Epoch: 274 | Batch: 001 / 011 | Total loss: 1.019 | Reg loss: 0.025 | Tree loss: 1.019 | Accuracy: 0.641500 | 1.288 sec/iter\n",
      "Epoch: 274 | Batch: 002 / 011 | Total loss: 1.022 | Reg loss: 0.025 | Tree loss: 1.022 | Accuracy: 0.654000 | 1.288 sec/iter\n",
      "Epoch: 274 | Batch: 003 / 011 | Total loss: 0.973 | Reg loss: 0.025 | Tree loss: 0.973 | Accuracy: 0.691500 | 1.288 sec/iter\n",
      "Epoch: 274 | Batch: 004 / 011 | Total loss: 0.952 | Reg loss: 0.025 | Tree loss: 0.952 | Accuracy: 0.685000 | 1.288 sec/iter\n",
      "Epoch: 274 | Batch: 005 / 011 | Total loss: 0.958 | Reg loss: 0.025 | Tree loss: 0.958 | Accuracy: 0.668000 | 1.288 sec/iter\n",
      "Epoch: 274 | Batch: 006 / 011 | Total loss: 0.944 | Reg loss: 0.025 | Tree loss: 0.944 | Accuracy: 0.662500 | 1.288 sec/iter\n",
      "Epoch: 274 | Batch: 007 / 011 | Total loss: 0.973 | Reg loss: 0.025 | Tree loss: 0.973 | Accuracy: 0.635500 | 1.288 sec/iter\n",
      "Epoch: 274 | Batch: 008 / 011 | Total loss: 0.925 | Reg loss: 0.025 | Tree loss: 0.925 | Accuracy: 0.653500 | 1.288 sec/iter\n",
      "Epoch: 274 | Batch: 009 / 011 | Total loss: 0.954 | Reg loss: 0.025 | Tree loss: 0.954 | Accuracy: 0.639500 | 1.288 sec/iter\n",
      "Epoch: 274 | Batch: 010 / 011 | Total loss: 0.898 | Reg loss: 0.025 | Tree loss: 0.898 | Accuracy: 0.682594 | 1.288 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 275 | Batch: 000 / 011 | Total loss: 1.033 | Reg loss: 0.025 | Tree loss: 1.033 | Accuracy: 0.650500 | 1.288 sec/iter\n",
      "Epoch: 275 | Batch: 001 / 011 | Total loss: 1.016 | Reg loss: 0.025 | Tree loss: 1.016 | Accuracy: 0.662500 | 1.288 sec/iter\n",
      "Epoch: 275 | Batch: 002 / 011 | Total loss: 0.982 | Reg loss: 0.025 | Tree loss: 0.982 | Accuracy: 0.690000 | 1.288 sec/iter\n",
      "Epoch: 275 | Batch: 003 / 011 | Total loss: 0.973 | Reg loss: 0.025 | Tree loss: 0.973 | Accuracy: 0.685000 | 1.288 sec/iter\n",
      "Epoch: 275 | Batch: 004 / 011 | Total loss: 0.986 | Reg loss: 0.025 | Tree loss: 0.986 | Accuracy: 0.669500 | 1.288 sec/iter\n",
      "Epoch: 275 | Batch: 005 / 011 | Total loss: 0.958 | Reg loss: 0.025 | Tree loss: 0.958 | Accuracy: 0.661000 | 1.288 sec/iter\n",
      "Epoch: 275 | Batch: 006 / 011 | Total loss: 0.959 | Reg loss: 0.025 | Tree loss: 0.959 | Accuracy: 0.658000 | 1.288 sec/iter\n",
      "Epoch: 275 | Batch: 007 / 011 | Total loss: 0.966 | Reg loss: 0.025 | Tree loss: 0.966 | Accuracy: 0.639500 | 1.288 sec/iter\n",
      "Epoch: 275 | Batch: 008 / 011 | Total loss: 0.917 | Reg loss: 0.025 | Tree loss: 0.917 | Accuracy: 0.673000 | 1.288 sec/iter\n",
      "Epoch: 275 | Batch: 009 / 011 | Total loss: 0.942 | Reg loss: 0.025 | Tree loss: 0.942 | Accuracy: 0.646000 | 1.288 sec/iter\n",
      "Epoch: 275 | Batch: 010 / 011 | Total loss: 0.930 | Reg loss: 0.025 | Tree loss: 0.930 | Accuracy: 0.655290 | 1.288 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 276 | Batch: 000 / 011 | Total loss: 1.054 | Reg loss: 0.025 | Tree loss: 1.054 | Accuracy: 0.633000 | 1.288 sec/iter\n",
      "Epoch: 276 | Batch: 001 / 011 | Total loss: 1.017 | Reg loss: 0.025 | Tree loss: 1.017 | Accuracy: 0.673000 | 1.288 sec/iter\n",
      "Epoch: 276 | Batch: 002 / 011 | Total loss: 0.987 | Reg loss: 0.025 | Tree loss: 0.987 | Accuracy: 0.678500 | 1.288 sec/iter\n",
      "Epoch: 276 | Batch: 003 / 011 | Total loss: 0.989 | Reg loss: 0.025 | Tree loss: 0.989 | Accuracy: 0.682500 | 1.288 sec/iter\n",
      "Epoch: 276 | Batch: 004 / 011 | Total loss: 0.987 | Reg loss: 0.025 | Tree loss: 0.987 | Accuracy: 0.675500 | 1.288 sec/iter\n",
      "Epoch: 276 | Batch: 005 / 011 | Total loss: 0.974 | Reg loss: 0.025 | Tree loss: 0.974 | Accuracy: 0.650000 | 1.288 sec/iter\n",
      "Epoch: 276 | Batch: 006 / 011 | Total loss: 0.933 | Reg loss: 0.025 | Tree loss: 0.933 | Accuracy: 0.657000 | 1.288 sec/iter\n",
      "Epoch: 276 | Batch: 007 / 011 | Total loss: 0.957 | Reg loss: 0.025 | Tree loss: 0.957 | Accuracy: 0.652000 | 1.288 sec/iter\n",
      "Epoch: 276 | Batch: 008 / 011 | Total loss: 0.905 | Reg loss: 0.025 | Tree loss: 0.905 | Accuracy: 0.661000 | 1.288 sec/iter\n",
      "Epoch: 276 | Batch: 009 / 011 | Total loss: 0.940 | Reg loss: 0.025 | Tree loss: 0.940 | Accuracy: 0.642000 | 1.288 sec/iter\n",
      "Epoch: 276 | Batch: 010 / 011 | Total loss: 0.960 | Reg loss: 0.025 | Tree loss: 0.960 | Accuracy: 0.658703 | 1.288 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 277 | Batch: 000 / 011 | Total loss: 1.036 | Reg loss: 0.025 | Tree loss: 1.036 | Accuracy: 0.636500 | 1.289 sec/iter\n",
      "Epoch: 277 | Batch: 001 / 011 | Total loss: 1.027 | Reg loss: 0.025 | Tree loss: 1.027 | Accuracy: 0.660500 | 1.289 sec/iter\n",
      "Epoch: 277 | Batch: 002 / 011 | Total loss: 1.009 | Reg loss: 0.025 | Tree loss: 1.009 | Accuracy: 0.659500 | 1.289 sec/iter\n",
      "Epoch: 277 | Batch: 003 / 011 | Total loss: 0.992 | Reg loss: 0.025 | Tree loss: 0.992 | Accuracy: 0.685500 | 1.289 sec/iter\n",
      "Epoch: 277 | Batch: 004 / 011 | Total loss: 0.952 | Reg loss: 0.025 | Tree loss: 0.952 | Accuracy: 0.679000 | 1.289 sec/iter\n",
      "Epoch: 277 | Batch: 005 / 011 | Total loss: 0.959 | Reg loss: 0.025 | Tree loss: 0.959 | Accuracy: 0.678000 | 1.289 sec/iter\n",
      "Epoch: 277 | Batch: 006 / 011 | Total loss: 0.950 | Reg loss: 0.025 | Tree loss: 0.950 | Accuracy: 0.663000 | 1.289 sec/iter\n",
      "Epoch: 277 | Batch: 007 / 011 | Total loss: 0.952 | Reg loss: 0.025 | Tree loss: 0.952 | Accuracy: 0.645000 | 1.29 sec/iter\n",
      "Epoch: 277 | Batch: 008 / 011 | Total loss: 0.948 | Reg loss: 0.025 | Tree loss: 0.948 | Accuracy: 0.626500 | 1.29 sec/iter\n",
      "Epoch: 277 | Batch: 009 / 011 | Total loss: 0.931 | Reg loss: 0.025 | Tree loss: 0.931 | Accuracy: 0.644000 | 1.29 sec/iter\n",
      "Epoch: 277 | Batch: 010 / 011 | Total loss: 0.937 | Reg loss: 0.025 | Tree loss: 0.937 | Accuracy: 0.638225 | 1.29 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 278 | Batch: 000 / 011 | Total loss: 1.023 | Reg loss: 0.025 | Tree loss: 1.023 | Accuracy: 0.642500 | 1.29 sec/iter\n",
      "Epoch: 278 | Batch: 001 / 011 | Total loss: 1.022 | Reg loss: 0.025 | Tree loss: 1.022 | Accuracy: 0.637500 | 1.29 sec/iter\n",
      "Epoch: 278 | Batch: 002 / 011 | Total loss: 1.028 | Reg loss: 0.025 | Tree loss: 1.028 | Accuracy: 0.646000 | 1.29 sec/iter\n",
      "Epoch: 278 | Batch: 003 / 011 | Total loss: 0.978 | Reg loss: 0.025 | Tree loss: 0.978 | Accuracy: 0.675500 | 1.29 sec/iter\n",
      "Epoch: 278 | Batch: 004 / 011 | Total loss: 0.969 | Reg loss: 0.025 | Tree loss: 0.969 | Accuracy: 0.689000 | 1.29 sec/iter\n",
      "Epoch: 278 | Batch: 005 / 011 | Total loss: 0.975 | Reg loss: 0.025 | Tree loss: 0.975 | Accuracy: 0.672000 | 1.29 sec/iter\n",
      "Epoch: 278 | Batch: 006 / 011 | Total loss: 0.924 | Reg loss: 0.025 | Tree loss: 0.924 | Accuracy: 0.678000 | 1.291 sec/iter\n",
      "Epoch: 278 | Batch: 007 / 011 | Total loss: 0.921 | Reg loss: 0.025 | Tree loss: 0.921 | Accuracy: 0.671500 | 1.291 sec/iter\n",
      "Epoch: 278 | Batch: 008 / 011 | Total loss: 0.968 | Reg loss: 0.025 | Tree loss: 0.968 | Accuracy: 0.624000 | 1.291 sec/iter\n",
      "Epoch: 278 | Batch: 009 / 011 | Total loss: 0.913 | Reg loss: 0.025 | Tree loss: 0.913 | Accuracy: 0.654500 | 1.291 sec/iter\n",
      "Epoch: 278 | Batch: 010 / 011 | Total loss: 0.925 | Reg loss: 0.025 | Tree loss: 0.925 | Accuracy: 0.665529 | 1.291 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 279 | Batch: 000 / 011 | Total loss: 1.039 | Reg loss: 0.025 | Tree loss: 1.039 | Accuracy: 0.638000 | 1.291 sec/iter\n",
      "Epoch: 279 | Batch: 001 / 011 | Total loss: 1.013 | Reg loss: 0.025 | Tree loss: 1.013 | Accuracy: 0.664000 | 1.291 sec/iter\n",
      "Epoch: 279 | Batch: 002 / 011 | Total loss: 1.010 | Reg loss: 0.025 | Tree loss: 1.010 | Accuracy: 0.680000 | 1.291 sec/iter\n",
      "Epoch: 279 | Batch: 003 / 011 | Total loss: 0.976 | Reg loss: 0.025 | Tree loss: 0.976 | Accuracy: 0.683000 | 1.291 sec/iter\n",
      "Epoch: 279 | Batch: 004 / 011 | Total loss: 0.982 | Reg loss: 0.025 | Tree loss: 0.982 | Accuracy: 0.670000 | 1.291 sec/iter\n",
      "Epoch: 279 | Batch: 005 / 011 | Total loss: 0.942 | Reg loss: 0.025 | Tree loss: 0.942 | Accuracy: 0.671000 | 1.291 sec/iter\n",
      "Epoch: 279 | Batch: 006 / 011 | Total loss: 0.950 | Reg loss: 0.025 | Tree loss: 0.950 | Accuracy: 0.652500 | 1.291 sec/iter\n",
      "Epoch: 279 | Batch: 007 / 011 | Total loss: 0.905 | Reg loss: 0.025 | Tree loss: 0.905 | Accuracy: 0.664500 | 1.291 sec/iter\n",
      "Epoch: 279 | Batch: 008 / 011 | Total loss: 0.966 | Reg loss: 0.025 | Tree loss: 0.966 | Accuracy: 0.639000 | 1.291 sec/iter\n",
      "Epoch: 279 | Batch: 009 / 011 | Total loss: 0.925 | Reg loss: 0.025 | Tree loss: 0.925 | Accuracy: 0.658500 | 1.291 sec/iter\n",
      "Epoch: 279 | Batch: 010 / 011 | Total loss: 0.943 | Reg loss: 0.025 | Tree loss: 0.943 | Accuracy: 0.645051 | 1.291 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 280 | Batch: 000 / 011 | Total loss: 1.041 | Reg loss: 0.025 | Tree loss: 1.041 | Accuracy: 0.637500 | 1.292 sec/iter\n",
      "Epoch: 280 | Batch: 001 / 011 | Total loss: 1.048 | Reg loss: 0.025 | Tree loss: 1.048 | Accuracy: 0.641500 | 1.292 sec/iter\n",
      "Epoch: 280 | Batch: 002 / 011 | Total loss: 0.989 | Reg loss: 0.025 | Tree loss: 0.989 | Accuracy: 0.674000 | 1.292 sec/iter\n",
      "Epoch: 280 | Batch: 003 / 011 | Total loss: 0.979 | Reg loss: 0.025 | Tree loss: 0.979 | Accuracy: 0.685000 | 1.292 sec/iter\n",
      "Epoch: 280 | Batch: 004 / 011 | Total loss: 0.940 | Reg loss: 0.025 | Tree loss: 0.940 | Accuracy: 0.696000 | 1.292 sec/iter\n",
      "Epoch: 280 | Batch: 005 / 011 | Total loss: 0.957 | Reg loss: 0.025 | Tree loss: 0.957 | Accuracy: 0.650000 | 1.292 sec/iter\n",
      "Epoch: 280 | Batch: 006 / 011 | Total loss: 0.944 | Reg loss: 0.025 | Tree loss: 0.944 | Accuracy: 0.665000 | 1.293 sec/iter\n",
      "Epoch: 280 | Batch: 007 / 011 | Total loss: 0.970 | Reg loss: 0.025 | Tree loss: 0.970 | Accuracy: 0.636000 | 1.293 sec/iter\n",
      "Epoch: 280 | Batch: 008 / 011 | Total loss: 0.944 | Reg loss: 0.025 | Tree loss: 0.944 | Accuracy: 0.653500 | 1.293 sec/iter\n",
      "Epoch: 280 | Batch: 009 / 011 | Total loss: 0.941 | Reg loss: 0.025 | Tree loss: 0.941 | Accuracy: 0.652500 | 1.293 sec/iter\n",
      "Epoch: 280 | Batch: 010 / 011 | Total loss: 0.906 | Reg loss: 0.025 | Tree loss: 0.906 | Accuracy: 0.689420 | 1.293 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 281 | Batch: 000 / 011 | Total loss: 1.028 | Reg loss: 0.025 | Tree loss: 1.028 | Accuracy: 0.647000 | 1.293 sec/iter\n",
      "Epoch: 281 | Batch: 001 / 011 | Total loss: 1.024 | Reg loss: 0.025 | Tree loss: 1.024 | Accuracy: 0.653000 | 1.294 sec/iter\n",
      "Epoch: 281 | Batch: 002 / 011 | Total loss: 0.984 | Reg loss: 0.025 | Tree loss: 0.984 | Accuracy: 0.669000 | 1.294 sec/iter\n",
      "Epoch: 281 | Batch: 003 / 011 | Total loss: 0.989 | Reg loss: 0.025 | Tree loss: 0.989 | Accuracy: 0.688000 | 1.294 sec/iter\n",
      "Epoch: 281 | Batch: 004 / 011 | Total loss: 0.974 | Reg loss: 0.025 | Tree loss: 0.974 | Accuracy: 0.677000 | 1.294 sec/iter\n",
      "Epoch: 281 | Batch: 005 / 011 | Total loss: 0.941 | Reg loss: 0.025 | Tree loss: 0.941 | Accuracy: 0.665000 | 1.294 sec/iter\n",
      "Epoch: 281 | Batch: 006 / 011 | Total loss: 0.941 | Reg loss: 0.025 | Tree loss: 0.941 | Accuracy: 0.668500 | 1.294 sec/iter\n",
      "Epoch: 281 | Batch: 007 / 011 | Total loss: 0.941 | Reg loss: 0.025 | Tree loss: 0.941 | Accuracy: 0.651000 | 1.294 sec/iter\n",
      "Epoch: 281 | Batch: 008 / 011 | Total loss: 0.953 | Reg loss: 0.025 | Tree loss: 0.953 | Accuracy: 0.633000 | 1.294 sec/iter\n",
      "Epoch: 281 | Batch: 009 / 011 | Total loss: 0.967 | Reg loss: 0.025 | Tree loss: 0.967 | Accuracy: 0.629500 | 1.294 sec/iter\n",
      "Epoch: 281 | Batch: 010 / 011 | Total loss: 0.968 | Reg loss: 0.025 | Tree loss: 0.968 | Accuracy: 0.614334 | 1.294 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 282 | Batch: 000 / 011 | Total loss: 1.043 | Reg loss: 0.025 | Tree loss: 1.043 | Accuracy: 0.644500 | 1.294 sec/iter\n",
      "Epoch: 282 | Batch: 001 / 011 | Total loss: 1.002 | Reg loss: 0.025 | Tree loss: 1.002 | Accuracy: 0.664500 | 1.294 sec/iter\n",
      "Epoch: 282 | Batch: 002 / 011 | Total loss: 0.976 | Reg loss: 0.025 | Tree loss: 0.976 | Accuracy: 0.674500 | 1.295 sec/iter\n",
      "Epoch: 282 | Batch: 003 / 011 | Total loss: 0.992 | Reg loss: 0.025 | Tree loss: 0.992 | Accuracy: 0.671500 | 1.295 sec/iter\n",
      "Epoch: 282 | Batch: 004 / 011 | Total loss: 0.960 | Reg loss: 0.025 | Tree loss: 0.960 | Accuracy: 0.683000 | 1.295 sec/iter\n",
      "Epoch: 282 | Batch: 005 / 011 | Total loss: 0.994 | Reg loss: 0.025 | Tree loss: 0.994 | Accuracy: 0.655500 | 1.295 sec/iter\n",
      "Epoch: 282 | Batch: 006 / 011 | Total loss: 0.953 | Reg loss: 0.025 | Tree loss: 0.953 | Accuracy: 0.661000 | 1.295 sec/iter\n",
      "Epoch: 282 | Batch: 007 / 011 | Total loss: 0.942 | Reg loss: 0.025 | Tree loss: 0.942 | Accuracy: 0.671000 | 1.295 sec/iter\n",
      "Epoch: 282 | Batch: 008 / 011 | Total loss: 0.943 | Reg loss: 0.025 | Tree loss: 0.943 | Accuracy: 0.645500 | 1.295 sec/iter\n",
      "Epoch: 282 | Batch: 009 / 011 | Total loss: 0.919 | Reg loss: 0.025 | Tree loss: 0.919 | Accuracy: 0.666500 | 1.295 sec/iter\n",
      "Epoch: 282 | Batch: 010 / 011 | Total loss: 0.904 | Reg loss: 0.025 | Tree loss: 0.904 | Accuracy: 0.631399 | 1.295 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 283 | Batch: 000 / 011 | Total loss: 1.043 | Reg loss: 0.025 | Tree loss: 1.043 | Accuracy: 0.643000 | 1.296 sec/iter\n",
      "Epoch: 283 | Batch: 001 / 011 | Total loss: 1.034 | Reg loss: 0.025 | Tree loss: 1.034 | Accuracy: 0.651000 | 1.296 sec/iter\n",
      "Epoch: 283 | Batch: 002 / 011 | Total loss: 0.980 | Reg loss: 0.025 | Tree loss: 0.980 | Accuracy: 0.683000 | 1.296 sec/iter\n",
      "Epoch: 283 | Batch: 003 / 011 | Total loss: 0.975 | Reg loss: 0.025 | Tree loss: 0.975 | Accuracy: 0.686000 | 1.296 sec/iter\n",
      "Epoch: 283 | Batch: 004 / 011 | Total loss: 0.976 | Reg loss: 0.025 | Tree loss: 0.976 | Accuracy: 0.678500 | 1.296 sec/iter\n",
      "Epoch: 283 | Batch: 005 / 011 | Total loss: 0.946 | Reg loss: 0.025 | Tree loss: 0.946 | Accuracy: 0.672000 | 1.296 sec/iter\n",
      "Epoch: 283 | Batch: 006 / 011 | Total loss: 0.929 | Reg loss: 0.025 | Tree loss: 0.929 | Accuracy: 0.669500 | 1.297 sec/iter\n",
      "Epoch: 283 | Batch: 007 / 011 | Total loss: 0.959 | Reg loss: 0.025 | Tree loss: 0.959 | Accuracy: 0.649500 | 1.297 sec/iter\n",
      "Epoch: 283 | Batch: 008 / 011 | Total loss: 0.953 | Reg loss: 0.025 | Tree loss: 0.953 | Accuracy: 0.636000 | 1.296 sec/iter\n",
      "Epoch: 283 | Batch: 009 / 011 | Total loss: 0.936 | Reg loss: 0.025 | Tree loss: 0.936 | Accuracy: 0.640000 | 1.296 sec/iter\n",
      "Epoch: 283 | Batch: 010 / 011 | Total loss: 0.836 | Reg loss: 0.025 | Tree loss: 0.836 | Accuracy: 0.716724 | 1.296 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 284 | Batch: 000 / 011 | Total loss: 1.034 | Reg loss: 0.025 | Tree loss: 1.034 | Accuracy: 0.638500 | 1.297 sec/iter\n",
      "Epoch: 284 | Batch: 001 / 011 | Total loss: 1.000 | Reg loss: 0.025 | Tree loss: 1.000 | Accuracy: 0.657500 | 1.297 sec/iter\n",
      "Epoch: 284 | Batch: 002 / 011 | Total loss: 0.988 | Reg loss: 0.025 | Tree loss: 0.988 | Accuracy: 0.688000 | 1.297 sec/iter\n",
      "Epoch: 284 | Batch: 003 / 011 | Total loss: 0.992 | Reg loss: 0.025 | Tree loss: 0.992 | Accuracy: 0.675000 | 1.297 sec/iter\n",
      "Epoch: 284 | Batch: 004 / 011 | Total loss: 0.971 | Reg loss: 0.025 | Tree loss: 0.971 | Accuracy: 0.681500 | 1.297 sec/iter\n",
      "Epoch: 284 | Batch: 005 / 011 | Total loss: 0.939 | Reg loss: 0.025 | Tree loss: 0.939 | Accuracy: 0.686000 | 1.297 sec/iter\n",
      "Epoch: 284 | Batch: 006 / 011 | Total loss: 0.953 | Reg loss: 0.025 | Tree loss: 0.953 | Accuracy: 0.650500 | 1.297 sec/iter\n",
      "Epoch: 284 | Batch: 007 / 011 | Total loss: 0.950 | Reg loss: 0.025 | Tree loss: 0.950 | Accuracy: 0.651000 | 1.297 sec/iter\n",
      "Epoch: 284 | Batch: 008 / 011 | Total loss: 0.926 | Reg loss: 0.025 | Tree loss: 0.926 | Accuracy: 0.663000 | 1.297 sec/iter\n",
      "Epoch: 284 | Batch: 009 / 011 | Total loss: 0.930 | Reg loss: 0.025 | Tree loss: 0.930 | Accuracy: 0.652000 | 1.297 sec/iter\n",
      "Epoch: 284 | Batch: 010 / 011 | Total loss: 0.946 | Reg loss: 0.025 | Tree loss: 0.946 | Accuracy: 0.641638 | 1.297 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 285 | Batch: 000 / 011 | Total loss: 1.026 | Reg loss: 0.025 | Tree loss: 1.026 | Accuracy: 0.657500 | 1.298 sec/iter\n",
      "Epoch: 285 | Batch: 001 / 011 | Total loss: 0.992 | Reg loss: 0.025 | Tree loss: 0.992 | Accuracy: 0.685500 | 1.298 sec/iter\n",
      "Epoch: 285 | Batch: 002 / 011 | Total loss: 0.994 | Reg loss: 0.025 | Tree loss: 0.994 | Accuracy: 0.683500 | 1.298 sec/iter\n",
      "Epoch: 285 | Batch: 003 / 011 | Total loss: 1.004 | Reg loss: 0.025 | Tree loss: 1.004 | Accuracy: 0.668000 | 1.298 sec/iter\n",
      "Epoch: 285 | Batch: 004 / 011 | Total loss: 0.953 | Reg loss: 0.025 | Tree loss: 0.953 | Accuracy: 0.689500 | 1.298 sec/iter\n",
      "Epoch: 285 | Batch: 005 / 011 | Total loss: 0.950 | Reg loss: 0.025 | Tree loss: 0.950 | Accuracy: 0.669000 | 1.298 sec/iter\n",
      "Epoch: 285 | Batch: 006 / 011 | Total loss: 0.966 | Reg loss: 0.025 | Tree loss: 0.966 | Accuracy: 0.649000 | 1.299 sec/iter\n",
      "Epoch: 285 | Batch: 007 / 011 | Total loss: 0.942 | Reg loss: 0.025 | Tree loss: 0.942 | Accuracy: 0.647000 | 1.299 sec/iter\n",
      "Epoch: 285 | Batch: 008 / 011 | Total loss: 0.940 | Reg loss: 0.025 | Tree loss: 0.940 | Accuracy: 0.638500 | 1.299 sec/iter\n",
      "Epoch: 285 | Batch: 009 / 011 | Total loss: 0.927 | Reg loss: 0.025 | Tree loss: 0.927 | Accuracy: 0.655000 | 1.299 sec/iter\n",
      "Epoch: 285 | Batch: 010 / 011 | Total loss: 0.946 | Reg loss: 0.025 | Tree loss: 0.946 | Accuracy: 0.648464 | 1.299 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 286 | Batch: 000 / 011 | Total loss: 1.022 | Reg loss: 0.025 | Tree loss: 1.022 | Accuracy: 0.646500 | 1.3 sec/iter\n",
      "Epoch: 286 | Batch: 001 / 011 | Total loss: 1.014 | Reg loss: 0.025 | Tree loss: 1.014 | Accuracy: 0.658500 | 1.3 sec/iter\n",
      "Epoch: 286 | Batch: 002 / 011 | Total loss: 1.002 | Reg loss: 0.025 | Tree loss: 1.002 | Accuracy: 0.690000 | 1.301 sec/iter\n",
      "Epoch: 286 | Batch: 003 / 011 | Total loss: 0.985 | Reg loss: 0.025 | Tree loss: 0.985 | Accuracy: 0.685000 | 1.301 sec/iter\n",
      "Epoch: 286 | Batch: 004 / 011 | Total loss: 0.964 | Reg loss: 0.025 | Tree loss: 0.964 | Accuracy: 0.680500 | 1.301 sec/iter\n",
      "Epoch: 286 | Batch: 005 / 011 | Total loss: 0.950 | Reg loss: 0.025 | Tree loss: 0.950 | Accuracy: 0.666000 | 1.301 sec/iter\n",
      "Epoch: 286 | Batch: 006 / 011 | Total loss: 0.943 | Reg loss: 0.025 | Tree loss: 0.943 | Accuracy: 0.677000 | 1.301 sec/iter\n",
      "Epoch: 286 | Batch: 007 / 011 | Total loss: 0.946 | Reg loss: 0.025 | Tree loss: 0.946 | Accuracy: 0.667000 | 1.301 sec/iter\n",
      "Epoch: 286 | Batch: 008 / 011 | Total loss: 0.925 | Reg loss: 0.025 | Tree loss: 0.925 | Accuracy: 0.656500 | 1.301 sec/iter\n",
      "Epoch: 286 | Batch: 009 / 011 | Total loss: 0.939 | Reg loss: 0.025 | Tree loss: 0.939 | Accuracy: 0.636500 | 1.301 sec/iter\n",
      "Epoch: 286 | Batch: 010 / 011 | Total loss: 0.960 | Reg loss: 0.025 | Tree loss: 0.960 | Accuracy: 0.638225 | 1.301 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 287 | Batch: 000 / 011 | Total loss: 1.018 | Reg loss: 0.025 | Tree loss: 1.018 | Accuracy: 0.644500 | 1.302 sec/iter\n",
      "Epoch: 287 | Batch: 001 / 011 | Total loss: 1.016 | Reg loss: 0.025 | Tree loss: 1.016 | Accuracy: 0.673000 | 1.302 sec/iter\n",
      "Epoch: 287 | Batch: 002 / 011 | Total loss: 1.004 | Reg loss: 0.025 | Tree loss: 1.004 | Accuracy: 0.681000 | 1.302 sec/iter\n",
      "Epoch: 287 | Batch: 003 / 011 | Total loss: 0.974 | Reg loss: 0.025 | Tree loss: 0.974 | Accuracy: 0.695000 | 1.302 sec/iter\n",
      "Epoch: 287 | Batch: 004 / 011 | Total loss: 0.967 | Reg loss: 0.025 | Tree loss: 0.967 | Accuracy: 0.677500 | 1.302 sec/iter\n",
      "Epoch: 287 | Batch: 005 / 011 | Total loss: 0.951 | Reg loss: 0.025 | Tree loss: 0.951 | Accuracy: 0.670000 | 1.303 sec/iter\n",
      "Epoch: 287 | Batch: 006 / 011 | Total loss: 0.947 | Reg loss: 0.025 | Tree loss: 0.947 | Accuracy: 0.668500 | 1.303 sec/iter\n",
      "Epoch: 287 | Batch: 007 / 011 | Total loss: 0.945 | Reg loss: 0.025 | Tree loss: 0.945 | Accuracy: 0.642500 | 1.303 sec/iter\n",
      "Epoch: 287 | Batch: 008 / 011 | Total loss: 0.938 | Reg loss: 0.025 | Tree loss: 0.938 | Accuracy: 0.634000 | 1.303 sec/iter\n",
      "Epoch: 287 | Batch: 009 / 011 | Total loss: 0.943 | Reg loss: 0.025 | Tree loss: 0.943 | Accuracy: 0.633000 | 1.303 sec/iter\n",
      "Epoch: 287 | Batch: 010 / 011 | Total loss: 0.917 | Reg loss: 0.025 | Tree loss: 0.917 | Accuracy: 0.638225 | 1.303 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 288 | Batch: 000 / 011 | Total loss: 1.053 | Reg loss: 0.025 | Tree loss: 1.053 | Accuracy: 0.627000 | 1.304 sec/iter\n",
      "Epoch: 288 | Batch: 001 / 011 | Total loss: 1.010 | Reg loss: 0.025 | Tree loss: 1.010 | Accuracy: 0.649500 | 1.304 sec/iter\n",
      "Epoch: 288 | Batch: 002 / 011 | Total loss: 0.985 | Reg loss: 0.025 | Tree loss: 0.985 | Accuracy: 0.684500 | 1.304 sec/iter\n",
      "Epoch: 288 | Batch: 003 / 011 | Total loss: 0.987 | Reg loss: 0.025 | Tree loss: 0.987 | Accuracy: 0.678500 | 1.304 sec/iter\n",
      "Epoch: 288 | Batch: 004 / 011 | Total loss: 0.972 | Reg loss: 0.025 | Tree loss: 0.972 | Accuracy: 0.674000 | 1.304 sec/iter\n",
      "Epoch: 288 | Batch: 005 / 011 | Total loss: 0.930 | Reg loss: 0.025 | Tree loss: 0.930 | Accuracy: 0.691500 | 1.304 sec/iter\n",
      "Epoch: 288 | Batch: 006 / 011 | Total loss: 0.953 | Reg loss: 0.025 | Tree loss: 0.953 | Accuracy: 0.647500 | 1.304 sec/iter\n",
      "Epoch: 288 | Batch: 007 / 011 | Total loss: 0.937 | Reg loss: 0.025 | Tree loss: 0.937 | Accuracy: 0.650500 | 1.304 sec/iter\n",
      "Epoch: 288 | Batch: 008 / 011 | Total loss: 0.944 | Reg loss: 0.025 | Tree loss: 0.944 | Accuracy: 0.649000 | 1.305 sec/iter\n",
      "Epoch: 288 | Batch: 009 / 011 | Total loss: 0.957 | Reg loss: 0.025 | Tree loss: 0.957 | Accuracy: 0.639500 | 1.305 sec/iter\n",
      "Epoch: 288 | Batch: 010 / 011 | Total loss: 0.857 | Reg loss: 0.025 | Tree loss: 0.857 | Accuracy: 0.662116 | 1.305 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 289 | Batch: 000 / 011 | Total loss: 1.048 | Reg loss: 0.025 | Tree loss: 1.048 | Accuracy: 0.629000 | 1.305 sec/iter\n",
      "Epoch: 289 | Batch: 001 / 011 | Total loss: 1.019 | Reg loss: 0.025 | Tree loss: 1.019 | Accuracy: 0.658500 | 1.305 sec/iter\n",
      "Epoch: 289 | Batch: 002 / 011 | Total loss: 0.983 | Reg loss: 0.025 | Tree loss: 0.983 | Accuracy: 0.686000 | 1.305 sec/iter\n",
      "Epoch: 289 | Batch: 003 / 011 | Total loss: 0.975 | Reg loss: 0.025 | Tree loss: 0.975 | Accuracy: 0.701000 | 1.305 sec/iter\n",
      "Epoch: 289 | Batch: 004 / 011 | Total loss: 0.971 | Reg loss: 0.025 | Tree loss: 0.971 | Accuracy: 0.684000 | 1.305 sec/iter\n",
      "Epoch: 289 | Batch: 005 / 011 | Total loss: 0.946 | Reg loss: 0.025 | Tree loss: 0.946 | Accuracy: 0.677000 | 1.305 sec/iter\n",
      "Epoch: 289 | Batch: 006 / 011 | Total loss: 0.956 | Reg loss: 0.025 | Tree loss: 0.956 | Accuracy: 0.663500 | 1.305 sec/iter\n",
      "Epoch: 289 | Batch: 007 / 011 | Total loss: 0.948 | Reg loss: 0.025 | Tree loss: 0.948 | Accuracy: 0.654000 | 1.305 sec/iter\n",
      "Epoch: 289 | Batch: 008 / 011 | Total loss: 0.922 | Reg loss: 0.025 | Tree loss: 0.922 | Accuracy: 0.642500 | 1.305 sec/iter\n",
      "Epoch: 289 | Batch: 009 / 011 | Total loss: 0.928 | Reg loss: 0.025 | Tree loss: 0.928 | Accuracy: 0.648000 | 1.306 sec/iter\n",
      "Epoch: 289 | Batch: 010 / 011 | Total loss: 0.891 | Reg loss: 0.025 | Tree loss: 0.891 | Accuracy: 0.638225 | 1.306 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 290 | Batch: 000 / 011 | Total loss: 1.033 | Reg loss: 0.025 | Tree loss: 1.033 | Accuracy: 0.656500 | 1.306 sec/iter\n",
      "Epoch: 290 | Batch: 001 / 011 | Total loss: 1.027 | Reg loss: 0.025 | Tree loss: 1.027 | Accuracy: 0.645500 | 1.306 sec/iter\n",
      "Epoch: 290 | Batch: 002 / 011 | Total loss: 0.993 | Reg loss: 0.025 | Tree loss: 0.993 | Accuracy: 0.676500 | 1.306 sec/iter\n",
      "Epoch: 290 | Batch: 003 / 011 | Total loss: 0.935 | Reg loss: 0.025 | Tree loss: 0.935 | Accuracy: 0.689500 | 1.307 sec/iter\n",
      "Epoch: 290 | Batch: 004 / 011 | Total loss: 0.951 | Reg loss: 0.025 | Tree loss: 0.951 | Accuracy: 0.662500 | 1.307 sec/iter\n",
      "Epoch: 290 | Batch: 005 / 011 | Total loss: 0.950 | Reg loss: 0.025 | Tree loss: 0.950 | Accuracy: 0.673500 | 1.307 sec/iter\n",
      "Epoch: 290 | Batch: 006 / 011 | Total loss: 0.930 | Reg loss: 0.025 | Tree loss: 0.930 | Accuracy: 0.674000 | 1.307 sec/iter\n",
      "Epoch: 290 | Batch: 007 / 011 | Total loss: 0.941 | Reg loss: 0.025 | Tree loss: 0.941 | Accuracy: 0.657000 | 1.307 sec/iter\n",
      "Epoch: 290 | Batch: 008 / 011 | Total loss: 0.979 | Reg loss: 0.025 | Tree loss: 0.979 | Accuracy: 0.636000 | 1.307 sec/iter\n",
      "Epoch: 290 | Batch: 009 / 011 | Total loss: 0.946 | Reg loss: 0.025 | Tree loss: 0.946 | Accuracy: 0.635000 | 1.307 sec/iter\n",
      "Epoch: 290 | Batch: 010 / 011 | Total loss: 1.084 | Reg loss: 0.025 | Tree loss: 1.084 | Accuracy: 0.563140 | 1.307 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 291 | Batch: 000 / 011 | Total loss: 1.026 | Reg loss: 0.025 | Tree loss: 1.026 | Accuracy: 0.646500 | 1.308 sec/iter\n",
      "Epoch: 291 | Batch: 001 / 011 | Total loss: 1.035 | Reg loss: 0.025 | Tree loss: 1.035 | Accuracy: 0.646000 | 1.308 sec/iter\n",
      "Epoch: 291 | Batch: 002 / 011 | Total loss: 1.017 | Reg loss: 0.025 | Tree loss: 1.017 | Accuracy: 0.669500 | 1.308 sec/iter\n",
      "Epoch: 291 | Batch: 003 / 011 | Total loss: 0.962 | Reg loss: 0.025 | Tree loss: 0.962 | Accuracy: 0.693000 | 1.308 sec/iter\n",
      "Epoch: 291 | Batch: 004 / 011 | Total loss: 0.949 | Reg loss: 0.025 | Tree loss: 0.949 | Accuracy: 0.690000 | 1.309 sec/iter\n",
      "Epoch: 291 | Batch: 005 / 011 | Total loss: 0.944 | Reg loss: 0.025 | Tree loss: 0.944 | Accuracy: 0.686500 | 1.309 sec/iter\n",
      "Epoch: 291 | Batch: 006 / 011 | Total loss: 0.946 | Reg loss: 0.025 | Tree loss: 0.946 | Accuracy: 0.675000 | 1.309 sec/iter\n",
      "Epoch: 291 | Batch: 007 / 011 | Total loss: 0.930 | Reg loss: 0.025 | Tree loss: 0.930 | Accuracy: 0.646000 | 1.309 sec/iter\n",
      "Epoch: 291 | Batch: 008 / 011 | Total loss: 0.955 | Reg loss: 0.025 | Tree loss: 0.955 | Accuracy: 0.635000 | 1.309 sec/iter\n",
      "Epoch: 291 | Batch: 009 / 011 | Total loss: 0.953 | Reg loss: 0.025 | Tree loss: 0.953 | Accuracy: 0.639500 | 1.309 sec/iter\n",
      "Epoch: 291 | Batch: 010 / 011 | Total loss: 0.840 | Reg loss: 0.025 | Tree loss: 0.840 | Accuracy: 0.689420 | 1.309 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 292 | Batch: 000 / 011 | Total loss: 1.031 | Reg loss: 0.025 | Tree loss: 1.031 | Accuracy: 0.649500 | 1.31 sec/iter\n",
      "Epoch: 292 | Batch: 001 / 011 | Total loss: 1.015 | Reg loss: 0.025 | Tree loss: 1.015 | Accuracy: 0.662000 | 1.31 sec/iter\n",
      "Epoch: 292 | Batch: 002 / 011 | Total loss: 0.980 | Reg loss: 0.025 | Tree loss: 0.980 | Accuracy: 0.686000 | 1.31 sec/iter\n",
      "Epoch: 292 | Batch: 003 / 011 | Total loss: 0.950 | Reg loss: 0.025 | Tree loss: 0.950 | Accuracy: 0.690500 | 1.31 sec/iter\n",
      "Epoch: 292 | Batch: 004 / 011 | Total loss: 0.990 | Reg loss: 0.025 | Tree loss: 0.990 | Accuracy: 0.665000 | 1.31 sec/iter\n",
      "Epoch: 292 | Batch: 005 / 011 | Total loss: 0.942 | Reg loss: 0.025 | Tree loss: 0.942 | Accuracy: 0.681000 | 1.31 sec/iter\n",
      "Epoch: 292 | Batch: 006 / 011 | Total loss: 0.948 | Reg loss: 0.025 | Tree loss: 0.948 | Accuracy: 0.656000 | 1.31 sec/iter\n",
      "Epoch: 292 | Batch: 007 / 011 | Total loss: 0.943 | Reg loss: 0.025 | Tree loss: 0.943 | Accuracy: 0.647500 | 1.31 sec/iter\n",
      "Epoch: 292 | Batch: 008 / 011 | Total loss: 0.930 | Reg loss: 0.025 | Tree loss: 0.930 | Accuracy: 0.650000 | 1.311 sec/iter\n",
      "Epoch: 292 | Batch: 009 / 011 | Total loss: 0.954 | Reg loss: 0.025 | Tree loss: 0.954 | Accuracy: 0.635500 | 1.311 sec/iter\n",
      "Epoch: 292 | Batch: 010 / 011 | Total loss: 0.973 | Reg loss: 0.025 | Tree loss: 0.973 | Accuracy: 0.624573 | 1.311 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 293 | Batch: 000 / 011 | Total loss: 1.027 | Reg loss: 0.025 | Tree loss: 1.027 | Accuracy: 0.656500 | 1.311 sec/iter\n",
      "Epoch: 293 | Batch: 001 / 011 | Total loss: 0.988 | Reg loss: 0.025 | Tree loss: 0.988 | Accuracy: 0.693000 | 1.311 sec/iter\n",
      "Epoch: 293 | Batch: 002 / 011 | Total loss: 1.009 | Reg loss: 0.025 | Tree loss: 1.009 | Accuracy: 0.672500 | 1.311 sec/iter\n",
      "Epoch: 293 | Batch: 003 / 011 | Total loss: 0.963 | Reg loss: 0.025 | Tree loss: 0.963 | Accuracy: 0.699000 | 1.311 sec/iter\n",
      "Epoch: 293 | Batch: 004 / 011 | Total loss: 0.941 | Reg loss: 0.025 | Tree loss: 0.941 | Accuracy: 0.690500 | 1.311 sec/iter\n",
      "Epoch: 293 | Batch: 005 / 011 | Total loss: 0.958 | Reg loss: 0.025 | Tree loss: 0.958 | Accuracy: 0.664000 | 1.311 sec/iter\n",
      "Epoch: 293 | Batch: 006 / 011 | Total loss: 0.944 | Reg loss: 0.025 | Tree loss: 0.944 | Accuracy: 0.658000 | 1.311 sec/iter\n",
      "Epoch: 293 | Batch: 007 / 011 | Total loss: 0.930 | Reg loss: 0.025 | Tree loss: 0.930 | Accuracy: 0.665500 | 1.311 sec/iter\n",
      "Epoch: 293 | Batch: 008 / 011 | Total loss: 0.979 | Reg loss: 0.025 | Tree loss: 0.979 | Accuracy: 0.615000 | 1.311 sec/iter\n",
      "Epoch: 293 | Batch: 009 / 011 | Total loss: 0.942 | Reg loss: 0.025 | Tree loss: 0.942 | Accuracy: 0.644000 | 1.311 sec/iter\n",
      "Epoch: 293 | Batch: 010 / 011 | Total loss: 0.908 | Reg loss: 0.025 | Tree loss: 0.908 | Accuracy: 0.651877 | 1.311 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 294 | Batch: 000 / 011 | Total loss: 1.022 | Reg loss: 0.025 | Tree loss: 1.022 | Accuracy: 0.650000 | 1.312 sec/iter\n",
      "Epoch: 294 | Batch: 001 / 011 | Total loss: 1.018 | Reg loss: 0.025 | Tree loss: 1.018 | Accuracy: 0.676500 | 1.312 sec/iter\n",
      "Epoch: 294 | Batch: 002 / 011 | Total loss: 0.992 | Reg loss: 0.025 | Tree loss: 0.992 | Accuracy: 0.684000 | 1.312 sec/iter\n",
      "Epoch: 294 | Batch: 003 / 011 | Total loss: 0.965 | Reg loss: 0.025 | Tree loss: 0.965 | Accuracy: 0.692500 | 1.312 sec/iter\n",
      "Epoch: 294 | Batch: 004 / 011 | Total loss: 0.951 | Reg loss: 0.025 | Tree loss: 0.951 | Accuracy: 0.686000 | 1.313 sec/iter\n",
      "Epoch: 294 | Batch: 005 / 011 | Total loss: 0.973 | Reg loss: 0.025 | Tree loss: 0.973 | Accuracy: 0.649500 | 1.313 sec/iter\n",
      "Epoch: 294 | Batch: 006 / 011 | Total loss: 0.950 | Reg loss: 0.025 | Tree loss: 0.950 | Accuracy: 0.658000 | 1.313 sec/iter\n",
      "Epoch: 294 | Batch: 007 / 011 | Total loss: 0.951 | Reg loss: 0.025 | Tree loss: 0.951 | Accuracy: 0.647000 | 1.313 sec/iter\n",
      "Epoch: 294 | Batch: 008 / 011 | Total loss: 0.942 | Reg loss: 0.025 | Tree loss: 0.942 | Accuracy: 0.643500 | 1.313 sec/iter\n",
      "Epoch: 294 | Batch: 009 / 011 | Total loss: 0.934 | Reg loss: 0.025 | Tree loss: 0.934 | Accuracy: 0.648000 | 1.313 sec/iter\n",
      "Epoch: 294 | Batch: 010 / 011 | Total loss: 0.863 | Reg loss: 0.025 | Tree loss: 0.863 | Accuracy: 0.699659 | 1.313 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 295 | Batch: 000 / 011 | Total loss: 1.029 | Reg loss: 0.025 | Tree loss: 1.029 | Accuracy: 0.646500 | 1.314 sec/iter\n",
      "Epoch: 295 | Batch: 001 / 011 | Total loss: 1.003 | Reg loss: 0.025 | Tree loss: 1.003 | Accuracy: 0.663500 | 1.314 sec/iter\n",
      "Epoch: 295 | Batch: 002 / 011 | Total loss: 1.022 | Reg loss: 0.025 | Tree loss: 1.022 | Accuracy: 0.664500 | 1.314 sec/iter\n",
      "Epoch: 295 | Batch: 003 / 011 | Total loss: 0.981 | Reg loss: 0.025 | Tree loss: 0.981 | Accuracy: 0.694000 | 1.314 sec/iter\n",
      "Epoch: 295 | Batch: 004 / 011 | Total loss: 0.961 | Reg loss: 0.025 | Tree loss: 0.961 | Accuracy: 0.692000 | 1.314 sec/iter\n",
      "Epoch: 295 | Batch: 005 / 011 | Total loss: 0.927 | Reg loss: 0.025 | Tree loss: 0.927 | Accuracy: 0.691000 | 1.315 sec/iter\n",
      "Epoch: 295 | Batch: 006 / 011 | Total loss: 0.940 | Reg loss: 0.025 | Tree loss: 0.940 | Accuracy: 0.666500 | 1.315 sec/iter\n",
      "Epoch: 295 | Batch: 007 / 011 | Total loss: 0.953 | Reg loss: 0.025 | Tree loss: 0.953 | Accuracy: 0.657000 | 1.315 sec/iter\n",
      "Epoch: 295 | Batch: 008 / 011 | Total loss: 0.935 | Reg loss: 0.025 | Tree loss: 0.935 | Accuracy: 0.649000 | 1.315 sec/iter\n",
      "Epoch: 295 | Batch: 009 / 011 | Total loss: 0.917 | Reg loss: 0.025 | Tree loss: 0.917 | Accuracy: 0.647500 | 1.315 sec/iter\n",
      "Epoch: 295 | Batch: 010 / 011 | Total loss: 0.904 | Reg loss: 0.025 | Tree loss: 0.904 | Accuracy: 0.641638 | 1.315 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 296 | Batch: 000 / 011 | Total loss: 1.035 | Reg loss: 0.025 | Tree loss: 1.035 | Accuracy: 0.646000 | 1.316 sec/iter\n",
      "Epoch: 296 | Batch: 001 / 011 | Total loss: 1.008 | Reg loss: 0.025 | Tree loss: 1.008 | Accuracy: 0.662500 | 1.316 sec/iter\n",
      "Epoch: 296 | Batch: 002 / 011 | Total loss: 0.970 | Reg loss: 0.025 | Tree loss: 0.970 | Accuracy: 0.692000 | 1.316 sec/iter\n",
      "Epoch: 296 | Batch: 003 / 011 | Total loss: 0.972 | Reg loss: 0.025 | Tree loss: 0.972 | Accuracy: 0.683500 | 1.316 sec/iter\n",
      "Epoch: 296 | Batch: 004 / 011 | Total loss: 0.980 | Reg loss: 0.025 | Tree loss: 0.980 | Accuracy: 0.663000 | 1.316 sec/iter\n",
      "Epoch: 296 | Batch: 005 / 011 | Total loss: 0.953 | Reg loss: 0.025 | Tree loss: 0.953 | Accuracy: 0.664500 | 1.316 sec/iter\n",
      "Epoch: 296 | Batch: 006 / 011 | Total loss: 0.950 | Reg loss: 0.025 | Tree loss: 0.950 | Accuracy: 0.667000 | 1.316 sec/iter\n",
      "Epoch: 296 | Batch: 007 / 011 | Total loss: 0.940 | Reg loss: 0.025 | Tree loss: 0.940 | Accuracy: 0.645500 | 1.317 sec/iter\n",
      "Epoch: 296 | Batch: 008 / 011 | Total loss: 0.925 | Reg loss: 0.025 | Tree loss: 0.925 | Accuracy: 0.651500 | 1.317 sec/iter\n",
      "Epoch: 296 | Batch: 009 / 011 | Total loss: 0.943 | Reg loss: 0.025 | Tree loss: 0.943 | Accuracy: 0.652500 | 1.317 sec/iter\n",
      "Epoch: 296 | Batch: 010 / 011 | Total loss: 0.927 | Reg loss: 0.025 | Tree loss: 0.927 | Accuracy: 0.648464 | 1.317 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 297 | Batch: 000 / 011 | Total loss: 1.029 | Reg loss: 0.025 | Tree loss: 1.029 | Accuracy: 0.653500 | 1.317 sec/iter\n",
      "Epoch: 297 | Batch: 001 / 011 | Total loss: 1.030 | Reg loss: 0.025 | Tree loss: 1.030 | Accuracy: 0.662000 | 1.317 sec/iter\n",
      "Epoch: 297 | Batch: 002 / 011 | Total loss: 1.008 | Reg loss: 0.025 | Tree loss: 1.008 | Accuracy: 0.668500 | 1.317 sec/iter\n",
      "Epoch: 297 | Batch: 003 / 011 | Total loss: 0.981 | Reg loss: 0.025 | Tree loss: 0.981 | Accuracy: 0.682500 | 1.317 sec/iter\n",
      "Epoch: 297 | Batch: 004 / 011 | Total loss: 0.945 | Reg loss: 0.025 | Tree loss: 0.945 | Accuracy: 0.700500 | 1.317 sec/iter\n",
      "Epoch: 297 | Batch: 005 / 011 | Total loss: 0.961 | Reg loss: 0.025 | Tree loss: 0.961 | Accuracy: 0.667500 | 1.317 sec/iter\n",
      "Epoch: 297 | Batch: 006 / 011 | Total loss: 0.933 | Reg loss: 0.025 | Tree loss: 0.933 | Accuracy: 0.659500 | 1.317 sec/iter\n",
      "Epoch: 297 | Batch: 007 / 011 | Total loss: 0.930 | Reg loss: 0.025 | Tree loss: 0.930 | Accuracy: 0.656500 | 1.318 sec/iter\n",
      "Epoch: 297 | Batch: 008 / 011 | Total loss: 0.918 | Reg loss: 0.025 | Tree loss: 0.918 | Accuracy: 0.651500 | 1.318 sec/iter\n",
      "Epoch: 297 | Batch: 009 / 011 | Total loss: 0.937 | Reg loss: 0.025 | Tree loss: 0.937 | Accuracy: 0.640000 | 1.318 sec/iter\n",
      "Epoch: 297 | Batch: 010 / 011 | Total loss: 0.954 | Reg loss: 0.025 | Tree loss: 0.954 | Accuracy: 0.627986 | 1.318 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 298 | Batch: 000 / 011 | Total loss: 1.024 | Reg loss: 0.025 | Tree loss: 1.024 | Accuracy: 0.656000 | 1.318 sec/iter\n",
      "Epoch: 298 | Batch: 001 / 011 | Total loss: 1.010 | Reg loss: 0.025 | Tree loss: 1.010 | Accuracy: 0.668500 | 1.318 sec/iter\n",
      "Epoch: 298 | Batch: 002 / 011 | Total loss: 0.994 | Reg loss: 0.025 | Tree loss: 0.994 | Accuracy: 0.679500 | 1.318 sec/iter\n",
      "Epoch: 298 | Batch: 003 / 011 | Total loss: 0.973 | Reg loss: 0.025 | Tree loss: 0.973 | Accuracy: 0.682500 | 1.318 sec/iter\n",
      "Epoch: 298 | Batch: 004 / 011 | Total loss: 0.968 | Reg loss: 0.025 | Tree loss: 0.968 | Accuracy: 0.663500 | 1.319 sec/iter\n",
      "Epoch: 298 | Batch: 005 / 011 | Total loss: 0.939 | Reg loss: 0.025 | Tree loss: 0.939 | Accuracy: 0.675500 | 1.319 sec/iter\n",
      "Epoch: 298 | Batch: 006 / 011 | Total loss: 0.971 | Reg loss: 0.025 | Tree loss: 0.971 | Accuracy: 0.648500 | 1.319 sec/iter\n",
      "Epoch: 298 | Batch: 007 / 011 | Total loss: 0.923 | Reg loss: 0.025 | Tree loss: 0.923 | Accuracy: 0.662500 | 1.319 sec/iter\n",
      "Epoch: 298 | Batch: 008 / 011 | Total loss: 0.939 | Reg loss: 0.025 | Tree loss: 0.939 | Accuracy: 0.646000 | 1.319 sec/iter\n",
      "Epoch: 298 | Batch: 009 / 011 | Total loss: 0.943 | Reg loss: 0.025 | Tree loss: 0.943 | Accuracy: 0.633500 | 1.319 sec/iter\n",
      "Epoch: 298 | Batch: 010 / 011 | Total loss: 0.872 | Reg loss: 0.025 | Tree loss: 0.872 | Accuracy: 0.709898 | 1.319 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 299 | Batch: 000 / 011 | Total loss: 1.022 | Reg loss: 0.025 | Tree loss: 1.022 | Accuracy: 0.649500 | 1.32 sec/iter\n",
      "Epoch: 299 | Batch: 001 / 011 | Total loss: 1.025 | Reg loss: 0.025 | Tree loss: 1.025 | Accuracy: 0.655500 | 1.32 sec/iter\n",
      "Epoch: 299 | Batch: 002 / 011 | Total loss: 0.998 | Reg loss: 0.025 | Tree loss: 0.998 | Accuracy: 0.678000 | 1.32 sec/iter\n",
      "Epoch: 299 | Batch: 003 / 011 | Total loss: 0.965 | Reg loss: 0.025 | Tree loss: 0.965 | Accuracy: 0.687500 | 1.32 sec/iter\n",
      "Epoch: 299 | Batch: 004 / 011 | Total loss: 0.935 | Reg loss: 0.025 | Tree loss: 0.935 | Accuracy: 0.687500 | 1.32 sec/iter\n",
      "Epoch: 299 | Batch: 005 / 011 | Total loss: 0.953 | Reg loss: 0.025 | Tree loss: 0.953 | Accuracy: 0.663500 | 1.32 sec/iter\n",
      "Epoch: 299 | Batch: 006 / 011 | Total loss: 0.983 | Reg loss: 0.025 | Tree loss: 0.983 | Accuracy: 0.657500 | 1.32 sec/iter\n",
      "Epoch: 299 | Batch: 007 / 011 | Total loss: 0.927 | Reg loss: 0.025 | Tree loss: 0.927 | Accuracy: 0.659000 | 1.321 sec/iter\n",
      "Epoch: 299 | Batch: 008 / 011 | Total loss: 0.939 | Reg loss: 0.025 | Tree loss: 0.939 | Accuracy: 0.646000 | 1.321 sec/iter\n",
      "Epoch: 299 | Batch: 009 / 011 | Total loss: 0.935 | Reg loss: 0.025 | Tree loss: 0.935 | Accuracy: 0.651500 | 1.321 sec/iter\n",
      "Epoch: 299 | Batch: 010 / 011 | Total loss: 0.882 | Reg loss: 0.025 | Tree loss: 0.882 | Accuracy: 0.668942 | 1.321 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 300 | Batch: 000 / 011 | Total loss: 1.018 | Reg loss: 0.025 | Tree loss: 1.018 | Accuracy: 0.649500 | 1.322 sec/iter\n",
      "Epoch: 300 | Batch: 001 / 011 | Total loss: 0.998 | Reg loss: 0.025 | Tree loss: 0.998 | Accuracy: 0.678000 | 1.322 sec/iter\n",
      "Epoch: 300 | Batch: 002 / 011 | Total loss: 1.019 | Reg loss: 0.025 | Tree loss: 1.019 | Accuracy: 0.658500 | 1.322 sec/iter\n",
      "Epoch: 300 | Batch: 003 / 011 | Total loss: 0.965 | Reg loss: 0.025 | Tree loss: 0.965 | Accuracy: 0.689000 | 1.322 sec/iter\n",
      "Epoch: 300 | Batch: 004 / 011 | Total loss: 0.969 | Reg loss: 0.025 | Tree loss: 0.969 | Accuracy: 0.682000 | 1.322 sec/iter\n",
      "Epoch: 300 | Batch: 005 / 011 | Total loss: 0.943 | Reg loss: 0.025 | Tree loss: 0.943 | Accuracy: 0.671000 | 1.322 sec/iter\n",
      "Epoch: 300 | Batch: 006 / 011 | Total loss: 0.968 | Reg loss: 0.025 | Tree loss: 0.968 | Accuracy: 0.630500 | 1.322 sec/iter\n",
      "Epoch: 300 | Batch: 007 / 011 | Total loss: 0.970 | Reg loss: 0.025 | Tree loss: 0.970 | Accuracy: 0.639500 | 1.323 sec/iter\n",
      "Epoch: 300 | Batch: 008 / 011 | Total loss: 0.905 | Reg loss: 0.025 | Tree loss: 0.905 | Accuracy: 0.660500 | 1.323 sec/iter\n",
      "Epoch: 300 | Batch: 009 / 011 | Total loss: 0.938 | Reg loss: 0.025 | Tree loss: 0.938 | Accuracy: 0.649000 | 1.323 sec/iter\n",
      "Epoch: 300 | Batch: 010 / 011 | Total loss: 0.934 | Reg loss: 0.025 | Tree loss: 0.934 | Accuracy: 0.634812 | 1.323 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 301 | Batch: 000 / 011 | Total loss: 1.015 | Reg loss: 0.025 | Tree loss: 1.015 | Accuracy: 0.658000 | 1.323 sec/iter\n",
      "Epoch: 301 | Batch: 001 / 011 | Total loss: 1.007 | Reg loss: 0.025 | Tree loss: 1.007 | Accuracy: 0.661500 | 1.323 sec/iter\n",
      "Epoch: 301 | Batch: 002 / 011 | Total loss: 0.995 | Reg loss: 0.025 | Tree loss: 0.995 | Accuracy: 0.668000 | 1.323 sec/iter\n",
      "Epoch: 301 | Batch: 003 / 011 | Total loss: 0.975 | Reg loss: 0.025 | Tree loss: 0.975 | Accuracy: 0.682000 | 1.323 sec/iter\n",
      "Epoch: 301 | Batch: 004 / 011 | Total loss: 0.960 | Reg loss: 0.025 | Tree loss: 0.960 | Accuracy: 0.682000 | 1.323 sec/iter\n",
      "Epoch: 301 | Batch: 005 / 011 | Total loss: 0.938 | Reg loss: 0.025 | Tree loss: 0.938 | Accuracy: 0.683500 | 1.323 sec/iter\n",
      "Epoch: 301 | Batch: 006 / 011 | Total loss: 0.956 | Reg loss: 0.025 | Tree loss: 0.956 | Accuracy: 0.660500 | 1.323 sec/iter\n",
      "Epoch: 301 | Batch: 007 / 011 | Total loss: 0.963 | Reg loss: 0.025 | Tree loss: 0.963 | Accuracy: 0.646000 | 1.323 sec/iter\n",
      "Epoch: 301 | Batch: 008 / 011 | Total loss: 0.924 | Reg loss: 0.025 | Tree loss: 0.924 | Accuracy: 0.652000 | 1.323 sec/iter\n",
      "Epoch: 301 | Batch: 009 / 011 | Total loss: 0.939 | Reg loss: 0.025 | Tree loss: 0.939 | Accuracy: 0.650000 | 1.323 sec/iter\n",
      "Epoch: 301 | Batch: 010 / 011 | Total loss: 1.028 | Reg loss: 0.025 | Tree loss: 1.028 | Accuracy: 0.583618 | 1.323 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 302 | Batch: 000 / 011 | Total loss: 1.006 | Reg loss: 0.025 | Tree loss: 1.006 | Accuracy: 0.656000 | 1.324 sec/iter\n",
      "Epoch: 302 | Batch: 001 / 011 | Total loss: 1.005 | Reg loss: 0.025 | Tree loss: 1.005 | Accuracy: 0.681000 | 1.324 sec/iter\n",
      "Epoch: 302 | Batch: 002 / 011 | Total loss: 0.989 | Reg loss: 0.025 | Tree loss: 0.989 | Accuracy: 0.693500 | 1.324 sec/iter\n",
      "Epoch: 302 | Batch: 003 / 011 | Total loss: 0.987 | Reg loss: 0.025 | Tree loss: 0.987 | Accuracy: 0.689000 | 1.324 sec/iter\n",
      "Epoch: 302 | Batch: 004 / 011 | Total loss: 0.962 | Reg loss: 0.025 | Tree loss: 0.962 | Accuracy: 0.689000 | 1.324 sec/iter\n",
      "Epoch: 302 | Batch: 005 / 011 | Total loss: 0.954 | Reg loss: 0.025 | Tree loss: 0.954 | Accuracy: 0.672500 | 1.324 sec/iter\n",
      "Epoch: 302 | Batch: 006 / 011 | Total loss: 0.965 | Reg loss: 0.025 | Tree loss: 0.965 | Accuracy: 0.648500 | 1.324 sec/iter\n",
      "Epoch: 302 | Batch: 007 / 011 | Total loss: 0.954 | Reg loss: 0.025 | Tree loss: 0.954 | Accuracy: 0.643500 | 1.324 sec/iter\n",
      "Epoch: 302 | Batch: 008 / 011 | Total loss: 0.936 | Reg loss: 0.025 | Tree loss: 0.936 | Accuracy: 0.654000 | 1.324 sec/iter\n",
      "Epoch: 302 | Batch: 009 / 011 | Total loss: 0.915 | Reg loss: 0.025 | Tree loss: 0.915 | Accuracy: 0.670500 | 1.324 sec/iter\n",
      "Epoch: 302 | Batch: 010 / 011 | Total loss: 0.922 | Reg loss: 0.025 | Tree loss: 0.922 | Accuracy: 0.648464 | 1.325 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 303 | Batch: 000 / 011 | Total loss: 1.022 | Reg loss: 0.025 | Tree loss: 1.022 | Accuracy: 0.645500 | 1.326 sec/iter\n",
      "Epoch: 303 | Batch: 001 / 011 | Total loss: 1.018 | Reg loss: 0.025 | Tree loss: 1.018 | Accuracy: 0.666000 | 1.326 sec/iter\n",
      "Epoch: 303 | Batch: 002 / 011 | Total loss: 0.976 | Reg loss: 0.025 | Tree loss: 0.976 | Accuracy: 0.693000 | 1.326 sec/iter\n",
      "Epoch: 303 | Batch: 003 / 011 | Total loss: 0.975 | Reg loss: 0.025 | Tree loss: 0.975 | Accuracy: 0.684000 | 1.326 sec/iter\n",
      "Epoch: 303 | Batch: 004 / 011 | Total loss: 0.951 | Reg loss: 0.025 | Tree loss: 0.951 | Accuracy: 0.683000 | 1.326 sec/iter\n",
      "Epoch: 303 | Batch: 005 / 011 | Total loss: 0.940 | Reg loss: 0.025 | Tree loss: 0.940 | Accuracy: 0.676000 | 1.326 sec/iter\n",
      "Epoch: 303 | Batch: 006 / 011 | Total loss: 0.960 | Reg loss: 0.025 | Tree loss: 0.960 | Accuracy: 0.671500 | 1.326 sec/iter\n",
      "Epoch: 303 | Batch: 007 / 011 | Total loss: 0.943 | Reg loss: 0.025 | Tree loss: 0.943 | Accuracy: 0.648000 | 1.326 sec/iter\n",
      "Epoch: 303 | Batch: 008 / 011 | Total loss: 0.951 | Reg loss: 0.025 | Tree loss: 0.951 | Accuracy: 0.636500 | 1.326 sec/iter\n",
      "Epoch: 303 | Batch: 009 / 011 | Total loss: 0.941 | Reg loss: 0.025 | Tree loss: 0.941 | Accuracy: 0.665000 | 1.326 sec/iter\n",
      "Epoch: 303 | Batch: 010 / 011 | Total loss: 0.921 | Reg loss: 0.025 | Tree loss: 0.921 | Accuracy: 0.672355 | 1.326 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 304 | Batch: 000 / 011 | Total loss: 1.025 | Reg loss: 0.025 | Tree loss: 1.025 | Accuracy: 0.660000 | 1.326 sec/iter\n",
      "Epoch: 304 | Batch: 001 / 011 | Total loss: 1.024 | Reg loss: 0.025 | Tree loss: 1.024 | Accuracy: 0.653500 | 1.326 sec/iter\n",
      "Epoch: 304 | Batch: 002 / 011 | Total loss: 1.004 | Reg loss: 0.025 | Tree loss: 1.004 | Accuracy: 0.671500 | 1.327 sec/iter\n",
      "Epoch: 304 | Batch: 003 / 011 | Total loss: 0.952 | Reg loss: 0.025 | Tree loss: 0.952 | Accuracy: 0.695500 | 1.327 sec/iter\n",
      "Epoch: 304 | Batch: 004 / 011 | Total loss: 0.970 | Reg loss: 0.025 | Tree loss: 0.970 | Accuracy: 0.681500 | 1.327 sec/iter\n",
      "Epoch: 304 | Batch: 005 / 011 | Total loss: 0.966 | Reg loss: 0.025 | Tree loss: 0.966 | Accuracy: 0.660000 | 1.327 sec/iter\n",
      "Epoch: 304 | Batch: 006 / 011 | Total loss: 0.959 | Reg loss: 0.025 | Tree loss: 0.959 | Accuracy: 0.654000 | 1.327 sec/iter\n",
      "Epoch: 304 | Batch: 007 / 011 | Total loss: 0.935 | Reg loss: 0.025 | Tree loss: 0.935 | Accuracy: 0.654500 | 1.327 sec/iter\n",
      "Epoch: 304 | Batch: 008 / 011 | Total loss: 0.915 | Reg loss: 0.025 | Tree loss: 0.915 | Accuracy: 0.664500 | 1.327 sec/iter\n",
      "Epoch: 304 | Batch: 009 / 011 | Total loss: 0.922 | Reg loss: 0.025 | Tree loss: 0.922 | Accuracy: 0.657500 | 1.327 sec/iter\n",
      "Epoch: 304 | Batch: 010 / 011 | Total loss: 0.879 | Reg loss: 0.025 | Tree loss: 0.879 | Accuracy: 0.692833 | 1.327 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 305 | Batch: 000 / 011 | Total loss: 1.040 | Reg loss: 0.025 | Tree loss: 1.040 | Accuracy: 0.638500 | 1.328 sec/iter\n",
      "Epoch: 305 | Batch: 001 / 011 | Total loss: 0.990 | Reg loss: 0.025 | Tree loss: 0.990 | Accuracy: 0.678000 | 1.328 sec/iter\n",
      "Epoch: 305 | Batch: 002 / 011 | Total loss: 0.994 | Reg loss: 0.025 | Tree loss: 0.994 | Accuracy: 0.683000 | 1.328 sec/iter\n",
      "Epoch: 305 | Batch: 003 / 011 | Total loss: 0.983 | Reg loss: 0.025 | Tree loss: 0.983 | Accuracy: 0.692000 | 1.328 sec/iter\n",
      "Epoch: 305 | Batch: 004 / 011 | Total loss: 0.947 | Reg loss: 0.025 | Tree loss: 0.947 | Accuracy: 0.692500 | 1.328 sec/iter\n",
      "Epoch: 305 | Batch: 005 / 011 | Total loss: 0.953 | Reg loss: 0.025 | Tree loss: 0.953 | Accuracy: 0.681500 | 1.328 sec/iter\n",
      "Epoch: 305 | Batch: 006 / 011 | Total loss: 0.926 | Reg loss: 0.025 | Tree loss: 0.926 | Accuracy: 0.686000 | 1.328 sec/iter\n",
      "Epoch: 305 | Batch: 007 / 011 | Total loss: 0.935 | Reg loss: 0.025 | Tree loss: 0.935 | Accuracy: 0.647500 | 1.328 sec/iter\n",
      "Epoch: 305 | Batch: 008 / 011 | Total loss: 0.935 | Reg loss: 0.025 | Tree loss: 0.935 | Accuracy: 0.637500 | 1.328 sec/iter\n",
      "Epoch: 305 | Batch: 009 / 011 | Total loss: 0.944 | Reg loss: 0.025 | Tree loss: 0.944 | Accuracy: 0.641500 | 1.328 sec/iter\n",
      "Epoch: 305 | Batch: 010 / 011 | Total loss: 0.961 | Reg loss: 0.025 | Tree loss: 0.961 | Accuracy: 0.634812 | 1.328 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 306 | Batch: 000 / 011 | Total loss: 1.018 | Reg loss: 0.025 | Tree loss: 1.018 | Accuracy: 0.642000 | 1.329 sec/iter\n",
      "Epoch: 306 | Batch: 001 / 011 | Total loss: 1.010 | Reg loss: 0.025 | Tree loss: 1.010 | Accuracy: 0.661500 | 1.329 sec/iter\n",
      "Epoch: 306 | Batch: 002 / 011 | Total loss: 0.996 | Reg loss: 0.025 | Tree loss: 0.996 | Accuracy: 0.677000 | 1.329 sec/iter\n",
      "Epoch: 306 | Batch: 003 / 011 | Total loss: 0.986 | Reg loss: 0.025 | Tree loss: 0.986 | Accuracy: 0.671000 | 1.33 sec/iter\n",
      "Epoch: 306 | Batch: 004 / 011 | Total loss: 0.946 | Reg loss: 0.025 | Tree loss: 0.946 | Accuracy: 0.676500 | 1.33 sec/iter\n",
      "Epoch: 306 | Batch: 005 / 011 | Total loss: 0.949 | Reg loss: 0.025 | Tree loss: 0.949 | Accuracy: 0.662000 | 1.33 sec/iter\n",
      "Epoch: 306 | Batch: 006 / 011 | Total loss: 0.952 | Reg loss: 0.025 | Tree loss: 0.952 | Accuracy: 0.655500 | 1.33 sec/iter\n",
      "Epoch: 306 | Batch: 007 / 011 | Total loss: 0.943 | Reg loss: 0.025 | Tree loss: 0.943 | Accuracy: 0.654000 | 1.33 sec/iter\n",
      "Epoch: 306 | Batch: 008 / 011 | Total loss: 0.921 | Reg loss: 0.025 | Tree loss: 0.921 | Accuracy: 0.665000 | 1.33 sec/iter\n",
      "Epoch: 306 | Batch: 009 / 011 | Total loss: 0.944 | Reg loss: 0.025 | Tree loss: 0.944 | Accuracy: 0.645500 | 1.33 sec/iter\n",
      "Epoch: 306 | Batch: 010 / 011 | Total loss: 0.939 | Reg loss: 0.025 | Tree loss: 0.939 | Accuracy: 0.668942 | 1.33 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 307 | Batch: 000 / 011 | Total loss: 1.025 | Reg loss: 0.025 | Tree loss: 1.025 | Accuracy: 0.648000 | 1.331 sec/iter\n",
      "Epoch: 307 | Batch: 001 / 011 | Total loss: 1.002 | Reg loss: 0.025 | Tree loss: 1.002 | Accuracy: 0.673500 | 1.331 sec/iter\n",
      "Epoch: 307 | Batch: 002 / 011 | Total loss: 0.991 | Reg loss: 0.025 | Tree loss: 0.991 | Accuracy: 0.673500 | 1.331 sec/iter\n",
      "Epoch: 307 | Batch: 003 / 011 | Total loss: 0.983 | Reg loss: 0.025 | Tree loss: 0.983 | Accuracy: 0.687500 | 1.331 sec/iter\n",
      "Epoch: 307 | Batch: 004 / 011 | Total loss: 0.951 | Reg loss: 0.025 | Tree loss: 0.951 | Accuracy: 0.703000 | 1.331 sec/iter\n",
      "Epoch: 307 | Batch: 005 / 011 | Total loss: 0.942 | Reg loss: 0.025 | Tree loss: 0.942 | Accuracy: 0.681000 | 1.331 sec/iter\n",
      "Epoch: 307 | Batch: 006 / 011 | Total loss: 0.939 | Reg loss: 0.025 | Tree loss: 0.939 | Accuracy: 0.665500 | 1.331 sec/iter\n",
      "Epoch: 307 | Batch: 007 / 011 | Total loss: 0.930 | Reg loss: 0.025 | Tree loss: 0.930 | Accuracy: 0.665000 | 1.331 sec/iter\n",
      "Epoch: 307 | Batch: 008 / 011 | Total loss: 0.944 | Reg loss: 0.025 | Tree loss: 0.944 | Accuracy: 0.648500 | 1.331 sec/iter\n",
      "Epoch: 307 | Batch: 009 / 011 | Total loss: 0.940 | Reg loss: 0.025 | Tree loss: 0.940 | Accuracy: 0.642000 | 1.331 sec/iter\n",
      "Epoch: 307 | Batch: 010 / 011 | Total loss: 0.921 | Reg loss: 0.025 | Tree loss: 0.921 | Accuracy: 0.658703 | 1.332 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 308 | Batch: 000 / 011 | Total loss: 1.031 | Reg loss: 0.025 | Tree loss: 1.031 | Accuracy: 0.651500 | 1.333 sec/iter\n",
      "Epoch: 308 | Batch: 001 / 011 | Total loss: 1.012 | Reg loss: 0.025 | Tree loss: 1.012 | Accuracy: 0.650000 | 1.333 sec/iter\n",
      "Epoch: 308 | Batch: 002 / 011 | Total loss: 0.982 | Reg loss: 0.025 | Tree loss: 0.982 | Accuracy: 0.673000 | 1.333 sec/iter\n",
      "Epoch: 308 | Batch: 003 / 011 | Total loss: 0.968 | Reg loss: 0.025 | Tree loss: 0.968 | Accuracy: 0.687000 | 1.333 sec/iter\n",
      "Epoch: 308 | Batch: 004 / 011 | Total loss: 0.946 | Reg loss: 0.025 | Tree loss: 0.946 | Accuracy: 0.683500 | 1.333 sec/iter\n",
      "Epoch: 308 | Batch: 005 / 011 | Total loss: 0.939 | Reg loss: 0.025 | Tree loss: 0.939 | Accuracy: 0.681500 | 1.333 sec/iter\n",
      "Epoch: 308 | Batch: 006 / 011 | Total loss: 0.961 | Reg loss: 0.025 | Tree loss: 0.961 | Accuracy: 0.655500 | 1.333 sec/iter\n",
      "Epoch: 308 | Batch: 007 / 011 | Total loss: 0.936 | Reg loss: 0.025 | Tree loss: 0.936 | Accuracy: 0.665000 | 1.333 sec/iter\n",
      "Epoch: 308 | Batch: 008 / 011 | Total loss: 0.955 | Reg loss: 0.025 | Tree loss: 0.955 | Accuracy: 0.638000 | 1.333 sec/iter\n",
      "Epoch: 308 | Batch: 009 / 011 | Total loss: 0.927 | Reg loss: 0.025 | Tree loss: 0.927 | Accuracy: 0.654500 | 1.333 sec/iter\n",
      "Epoch: 308 | Batch: 010 / 011 | Total loss: 0.900 | Reg loss: 0.025 | Tree loss: 0.900 | Accuracy: 0.668942 | 1.333 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 309 | Batch: 000 / 011 | Total loss: 1.031 | Reg loss: 0.025 | Tree loss: 1.031 | Accuracy: 0.645000 | 1.334 sec/iter\n",
      "Epoch: 309 | Batch: 001 / 011 | Total loss: 0.999 | Reg loss: 0.025 | Tree loss: 0.999 | Accuracy: 0.663000 | 1.334 sec/iter\n",
      "Epoch: 309 | Batch: 002 / 011 | Total loss: 0.994 | Reg loss: 0.025 | Tree loss: 0.994 | Accuracy: 0.681000 | 1.334 sec/iter\n",
      "Epoch: 309 | Batch: 003 / 011 | Total loss: 0.983 | Reg loss: 0.025 | Tree loss: 0.983 | Accuracy: 0.684000 | 1.334 sec/iter\n",
      "Epoch: 309 | Batch: 004 / 011 | Total loss: 0.960 | Reg loss: 0.025 | Tree loss: 0.960 | Accuracy: 0.680500 | 1.334 sec/iter\n",
      "Epoch: 309 | Batch: 005 / 011 | Total loss: 0.933 | Reg loss: 0.025 | Tree loss: 0.933 | Accuracy: 0.681500 | 1.334 sec/iter\n",
      "Epoch: 309 | Batch: 006 / 011 | Total loss: 0.949 | Reg loss: 0.025 | Tree loss: 0.949 | Accuracy: 0.651000 | 1.334 sec/iter\n",
      "Epoch: 309 | Batch: 007 / 011 | Total loss: 0.924 | Reg loss: 0.025 | Tree loss: 0.924 | Accuracy: 0.665000 | 1.334 sec/iter\n",
      "Epoch: 309 | Batch: 008 / 011 | Total loss: 0.954 | Reg loss: 0.025 | Tree loss: 0.954 | Accuracy: 0.644500 | 1.334 sec/iter\n",
      "Epoch: 309 | Batch: 009 / 011 | Total loss: 0.935 | Reg loss: 0.025 | Tree loss: 0.935 | Accuracy: 0.669000 | 1.334 sec/iter\n",
      "Epoch: 309 | Batch: 010 / 011 | Total loss: 0.847 | Reg loss: 0.025 | Tree loss: 0.847 | Accuracy: 0.706485 | 1.334 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 310 | Batch: 000 / 011 | Total loss: 1.035 | Reg loss: 0.025 | Tree loss: 1.035 | Accuracy: 0.651500 | 1.335 sec/iter\n",
      "Epoch: 310 | Batch: 001 / 011 | Total loss: 1.004 | Reg loss: 0.025 | Tree loss: 1.004 | Accuracy: 0.682000 | 1.335 sec/iter\n",
      "Epoch: 310 | Batch: 002 / 011 | Total loss: 0.973 | Reg loss: 0.025 | Tree loss: 0.973 | Accuracy: 0.699500 | 1.335 sec/iter\n",
      "Epoch: 310 | Batch: 003 / 011 | Total loss: 0.963 | Reg loss: 0.025 | Tree loss: 0.963 | Accuracy: 0.689000 | 1.335 sec/iter\n",
      "Epoch: 310 | Batch: 004 / 011 | Total loss: 0.947 | Reg loss: 0.025 | Tree loss: 0.947 | Accuracy: 0.673500 | 1.335 sec/iter\n",
      "Epoch: 310 | Batch: 005 / 011 | Total loss: 0.946 | Reg loss: 0.025 | Tree loss: 0.946 | Accuracy: 0.674500 | 1.335 sec/iter\n",
      "Epoch: 310 | Batch: 006 / 011 | Total loss: 0.954 | Reg loss: 0.025 | Tree loss: 0.954 | Accuracy: 0.651000 | 1.335 sec/iter\n",
      "Epoch: 310 | Batch: 007 / 011 | Total loss: 0.942 | Reg loss: 0.025 | Tree loss: 0.942 | Accuracy: 0.649000 | 1.335 sec/iter\n",
      "Epoch: 310 | Batch: 008 / 011 | Total loss: 0.969 | Reg loss: 0.025 | Tree loss: 0.969 | Accuracy: 0.629000 | 1.336 sec/iter\n",
      "Epoch: 310 | Batch: 009 / 011 | Total loss: 0.918 | Reg loss: 0.025 | Tree loss: 0.918 | Accuracy: 0.672500 | 1.336 sec/iter\n",
      "Epoch: 310 | Batch: 010 / 011 | Total loss: 0.934 | Reg loss: 0.025 | Tree loss: 0.934 | Accuracy: 0.631399 | 1.336 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 311 | Batch: 000 / 011 | Total loss: 1.033 | Reg loss: 0.025 | Tree loss: 1.033 | Accuracy: 0.652500 | 1.336 sec/iter\n",
      "Epoch: 311 | Batch: 001 / 011 | Total loss: 0.992 | Reg loss: 0.025 | Tree loss: 0.992 | Accuracy: 0.675500 | 1.337 sec/iter\n",
      "Epoch: 311 | Batch: 002 / 011 | Total loss: 0.984 | Reg loss: 0.025 | Tree loss: 0.984 | Accuracy: 0.670500 | 1.337 sec/iter\n",
      "Epoch: 311 | Batch: 003 / 011 | Total loss: 0.965 | Reg loss: 0.025 | Tree loss: 0.965 | Accuracy: 0.692000 | 1.337 sec/iter\n",
      "Epoch: 311 | Batch: 004 / 011 | Total loss: 0.979 | Reg loss: 0.025 | Tree loss: 0.979 | Accuracy: 0.669000 | 1.337 sec/iter\n",
      "Epoch: 311 | Batch: 005 / 011 | Total loss: 0.955 | Reg loss: 0.025 | Tree loss: 0.955 | Accuracy: 0.680000 | 1.337 sec/iter\n",
      "Epoch: 311 | Batch: 006 / 011 | Total loss: 0.956 | Reg loss: 0.025 | Tree loss: 0.956 | Accuracy: 0.658000 | 1.337 sec/iter\n",
      "Epoch: 311 | Batch: 007 / 011 | Total loss: 0.916 | Reg loss: 0.025 | Tree loss: 0.916 | Accuracy: 0.660500 | 1.337 sec/iter\n",
      "Epoch: 311 | Batch: 008 / 011 | Total loss: 0.929 | Reg loss: 0.025 | Tree loss: 0.929 | Accuracy: 0.658000 | 1.337 sec/iter\n",
      "Epoch: 311 | Batch: 009 / 011 | Total loss: 0.966 | Reg loss: 0.025 | Tree loss: 0.966 | Accuracy: 0.637500 | 1.337 sec/iter\n",
      "Epoch: 311 | Batch: 010 / 011 | Total loss: 0.913 | Reg loss: 0.025 | Tree loss: 0.913 | Accuracy: 0.679181 | 1.337 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 312 | Batch: 000 / 011 | Total loss: 1.023 | Reg loss: 0.025 | Tree loss: 1.023 | Accuracy: 0.639000 | 1.338 sec/iter\n",
      "Epoch: 312 | Batch: 001 / 011 | Total loss: 1.009 | Reg loss: 0.025 | Tree loss: 1.009 | Accuracy: 0.670500 | 1.338 sec/iter\n",
      "Epoch: 312 | Batch: 002 / 011 | Total loss: 1.009 | Reg loss: 0.025 | Tree loss: 1.009 | Accuracy: 0.679000 | 1.338 sec/iter\n",
      "Epoch: 312 | Batch: 003 / 011 | Total loss: 0.974 | Reg loss: 0.025 | Tree loss: 0.974 | Accuracy: 0.692500 | 1.338 sec/iter\n",
      "Epoch: 312 | Batch: 004 / 011 | Total loss: 0.981 | Reg loss: 0.025 | Tree loss: 0.981 | Accuracy: 0.678500 | 1.338 sec/iter\n",
      "Epoch: 312 | Batch: 005 / 011 | Total loss: 0.940 | Reg loss: 0.025 | Tree loss: 0.940 | Accuracy: 0.685000 | 1.338 sec/iter\n",
      "Epoch: 312 | Batch: 006 / 011 | Total loss: 0.933 | Reg loss: 0.025 | Tree loss: 0.933 | Accuracy: 0.659000 | 1.338 sec/iter\n",
      "Epoch: 312 | Batch: 007 / 011 | Total loss: 0.923 | Reg loss: 0.025 | Tree loss: 0.923 | Accuracy: 0.656000 | 1.338 sec/iter\n",
      "Epoch: 312 | Batch: 008 / 011 | Total loss: 0.928 | Reg loss: 0.025 | Tree loss: 0.928 | Accuracy: 0.660500 | 1.338 sec/iter\n",
      "Epoch: 312 | Batch: 009 / 011 | Total loss: 0.921 | Reg loss: 0.025 | Tree loss: 0.921 | Accuracy: 0.652500 | 1.338 sec/iter\n",
      "Epoch: 312 | Batch: 010 / 011 | Total loss: 0.898 | Reg loss: 0.025 | Tree loss: 0.898 | Accuracy: 0.655290 | 1.338 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 313 | Batch: 000 / 011 | Total loss: 1.001 | Reg loss: 0.025 | Tree loss: 1.001 | Accuracy: 0.676500 | 1.339 sec/iter\n",
      "Epoch: 313 | Batch: 001 / 011 | Total loss: 1.020 | Reg loss: 0.025 | Tree loss: 1.020 | Accuracy: 0.657500 | 1.339 sec/iter\n",
      "Epoch: 313 | Batch: 002 / 011 | Total loss: 0.978 | Reg loss: 0.025 | Tree loss: 0.978 | Accuracy: 0.687000 | 1.339 sec/iter\n",
      "Epoch: 313 | Batch: 003 / 011 | Total loss: 0.993 | Reg loss: 0.025 | Tree loss: 0.993 | Accuracy: 0.669000 | 1.339 sec/iter\n",
      "Epoch: 313 | Batch: 004 / 011 | Total loss: 0.960 | Reg loss: 0.025 | Tree loss: 0.960 | Accuracy: 0.691500 | 1.339 sec/iter\n",
      "Epoch: 313 | Batch: 005 / 011 | Total loss: 0.938 | Reg loss: 0.025 | Tree loss: 0.938 | Accuracy: 0.673500 | 1.339 sec/iter\n",
      "Epoch: 313 | Batch: 006 / 011 | Total loss: 0.924 | Reg loss: 0.025 | Tree loss: 0.924 | Accuracy: 0.670500 | 1.339 sec/iter\n",
      "Epoch: 313 | Batch: 007 / 011 | Total loss: 0.950 | Reg loss: 0.025 | Tree loss: 0.950 | Accuracy: 0.649000 | 1.339 sec/iter\n",
      "Epoch: 313 | Batch: 008 / 011 | Total loss: 0.945 | Reg loss: 0.025 | Tree loss: 0.945 | Accuracy: 0.644000 | 1.339 sec/iter\n",
      "Epoch: 313 | Batch: 009 / 011 | Total loss: 0.952 | Reg loss: 0.025 | Tree loss: 0.952 | Accuracy: 0.642500 | 1.339 sec/iter\n",
      "Epoch: 313 | Batch: 010 / 011 | Total loss: 0.864 | Reg loss: 0.025 | Tree loss: 0.864 | Accuracy: 0.703072 | 1.339 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 314 | Batch: 000 / 011 | Total loss: 1.054 | Reg loss: 0.025 | Tree loss: 1.054 | Accuracy: 0.640500 | 1.34 sec/iter\n",
      "Epoch: 314 | Batch: 001 / 011 | Total loss: 1.016 | Reg loss: 0.025 | Tree loss: 1.016 | Accuracy: 0.666500 | 1.34 sec/iter\n",
      "Epoch: 314 | Batch: 002 / 011 | Total loss: 0.999 | Reg loss: 0.025 | Tree loss: 0.999 | Accuracy: 0.681500 | 1.34 sec/iter\n",
      "Epoch: 314 | Batch: 003 / 011 | Total loss: 0.971 | Reg loss: 0.025 | Tree loss: 0.971 | Accuracy: 0.688000 | 1.34 sec/iter\n",
      "Epoch: 314 | Batch: 004 / 011 | Total loss: 0.978 | Reg loss: 0.025 | Tree loss: 0.978 | Accuracy: 0.675500 | 1.34 sec/iter\n",
      "Epoch: 314 | Batch: 005 / 011 | Total loss: 0.936 | Reg loss: 0.025 | Tree loss: 0.936 | Accuracy: 0.684500 | 1.34 sec/iter\n",
      "Epoch: 314 | Batch: 006 / 011 | Total loss: 0.938 | Reg loss: 0.025 | Tree loss: 0.938 | Accuracy: 0.660000 | 1.34 sec/iter\n",
      "Epoch: 314 | Batch: 007 / 011 | Total loss: 0.932 | Reg loss: 0.025 | Tree loss: 0.932 | Accuracy: 0.652000 | 1.34 sec/iter\n",
      "Epoch: 314 | Batch: 008 / 011 | Total loss: 0.920 | Reg loss: 0.025 | Tree loss: 0.920 | Accuracy: 0.667000 | 1.34 sec/iter\n",
      "Epoch: 314 | Batch: 009 / 011 | Total loss: 0.910 | Reg loss: 0.025 | Tree loss: 0.910 | Accuracy: 0.667000 | 1.34 sec/iter\n",
      "Epoch: 314 | Batch: 010 / 011 | Total loss: 0.913 | Reg loss: 0.025 | Tree loss: 0.913 | Accuracy: 0.655290 | 1.34 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 315 | Batch: 000 / 011 | Total loss: 1.035 | Reg loss: 0.025 | Tree loss: 1.035 | Accuracy: 0.645500 | 1.341 sec/iter\n",
      "Epoch: 315 | Batch: 001 / 011 | Total loss: 1.013 | Reg loss: 0.025 | Tree loss: 1.013 | Accuracy: 0.665500 | 1.341 sec/iter\n",
      "Epoch: 315 | Batch: 002 / 011 | Total loss: 0.976 | Reg loss: 0.025 | Tree loss: 0.976 | Accuracy: 0.675000 | 1.341 sec/iter\n",
      "Epoch: 315 | Batch: 003 / 011 | Total loss: 0.965 | Reg loss: 0.025 | Tree loss: 0.965 | Accuracy: 0.682500 | 1.341 sec/iter\n",
      "Epoch: 315 | Batch: 004 / 011 | Total loss: 0.949 | Reg loss: 0.025 | Tree loss: 0.949 | Accuracy: 0.667500 | 1.341 sec/iter\n",
      "Epoch: 315 | Batch: 005 / 011 | Total loss: 0.953 | Reg loss: 0.025 | Tree loss: 0.953 | Accuracy: 0.656500 | 1.341 sec/iter\n",
      "Epoch: 315 | Batch: 006 / 011 | Total loss: 0.943 | Reg loss: 0.025 | Tree loss: 0.943 | Accuracy: 0.660500 | 1.341 sec/iter\n",
      "Epoch: 315 | Batch: 007 / 011 | Total loss: 0.943 | Reg loss: 0.025 | Tree loss: 0.943 | Accuracy: 0.653500 | 1.342 sec/iter\n",
      "Epoch: 315 | Batch: 008 / 011 | Total loss: 0.943 | Reg loss: 0.025 | Tree loss: 0.943 | Accuracy: 0.638500 | 1.342 sec/iter\n",
      "Epoch: 315 | Batch: 009 / 011 | Total loss: 0.936 | Reg loss: 0.025 | Tree loss: 0.936 | Accuracy: 0.653000 | 1.342 sec/iter\n",
      "Epoch: 315 | Batch: 010 / 011 | Total loss: 0.885 | Reg loss: 0.025 | Tree loss: 0.885 | Accuracy: 0.709898 | 1.342 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 316 | Batch: 000 / 011 | Total loss: 1.028 | Reg loss: 0.025 | Tree loss: 1.028 | Accuracy: 0.643000 | 1.343 sec/iter\n",
      "Epoch: 316 | Batch: 001 / 011 | Total loss: 1.003 | Reg loss: 0.025 | Tree loss: 1.003 | Accuracy: 0.661500 | 1.343 sec/iter\n",
      "Epoch: 316 | Batch: 002 / 011 | Total loss: 0.975 | Reg loss: 0.025 | Tree loss: 0.975 | Accuracy: 0.685000 | 1.343 sec/iter\n",
      "Epoch: 316 | Batch: 003 / 011 | Total loss: 0.965 | Reg loss: 0.025 | Tree loss: 0.965 | Accuracy: 0.694500 | 1.343 sec/iter\n",
      "Epoch: 316 | Batch: 004 / 011 | Total loss: 0.984 | Reg loss: 0.025 | Tree loss: 0.984 | Accuracy: 0.667500 | 1.343 sec/iter\n",
      "Epoch: 316 | Batch: 005 / 011 | Total loss: 0.961 | Reg loss: 0.025 | Tree loss: 0.961 | Accuracy: 0.668500 | 1.343 sec/iter\n",
      "Epoch: 316 | Batch: 006 / 011 | Total loss: 0.933 | Reg loss: 0.025 | Tree loss: 0.933 | Accuracy: 0.666500 | 1.343 sec/iter\n",
      "Epoch: 316 | Batch: 007 / 011 | Total loss: 0.966 | Reg loss: 0.025 | Tree loss: 0.966 | Accuracy: 0.622500 | 1.343 sec/iter\n",
      "Epoch: 316 | Batch: 008 / 011 | Total loss: 0.940 | Reg loss: 0.025 | Tree loss: 0.940 | Accuracy: 0.640500 | 1.343 sec/iter\n",
      "Epoch: 316 | Batch: 009 / 011 | Total loss: 0.922 | Reg loss: 0.025 | Tree loss: 0.922 | Accuracy: 0.648500 | 1.343 sec/iter\n",
      "Epoch: 316 | Batch: 010 / 011 | Total loss: 0.878 | Reg loss: 0.025 | Tree loss: 0.878 | Accuracy: 0.689420 | 1.343 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 317 | Batch: 000 / 011 | Total loss: 1.053 | Reg loss: 0.025 | Tree loss: 1.053 | Accuracy: 0.635500 | 1.343 sec/iter\n",
      "Epoch: 317 | Batch: 001 / 011 | Total loss: 1.025 | Reg loss: 0.025 | Tree loss: 1.025 | Accuracy: 0.649000 | 1.343 sec/iter\n",
      "Epoch: 317 | Batch: 002 / 011 | Total loss: 0.981 | Reg loss: 0.025 | Tree loss: 0.981 | Accuracy: 0.691500 | 1.344 sec/iter\n",
      "Epoch: 317 | Batch: 003 / 011 | Total loss: 0.987 | Reg loss: 0.025 | Tree loss: 0.987 | Accuracy: 0.683500 | 1.344 sec/iter\n",
      "Epoch: 317 | Batch: 004 / 011 | Total loss: 0.983 | Reg loss: 0.025 | Tree loss: 0.983 | Accuracy: 0.677000 | 1.344 sec/iter\n",
      "Epoch: 317 | Batch: 005 / 011 | Total loss: 0.936 | Reg loss: 0.025 | Tree loss: 0.936 | Accuracy: 0.681500 | 1.344 sec/iter\n",
      "Epoch: 317 | Batch: 006 / 011 | Total loss: 0.928 | Reg loss: 0.025 | Tree loss: 0.928 | Accuracy: 0.669500 | 1.344 sec/iter\n",
      "Epoch: 317 | Batch: 007 / 011 | Total loss: 0.924 | Reg loss: 0.025 | Tree loss: 0.924 | Accuracy: 0.651500 | 1.344 sec/iter\n",
      "Epoch: 317 | Batch: 008 / 011 | Total loss: 0.926 | Reg loss: 0.025 | Tree loss: 0.926 | Accuracy: 0.650000 | 1.344 sec/iter\n",
      "Epoch: 317 | Batch: 009 / 011 | Total loss: 0.920 | Reg loss: 0.025 | Tree loss: 0.920 | Accuracy: 0.654500 | 1.344 sec/iter\n",
      "Epoch: 317 | Batch: 010 / 011 | Total loss: 0.889 | Reg loss: 0.025 | Tree loss: 0.889 | Accuracy: 0.651877 | 1.344 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 318 | Batch: 000 / 011 | Total loss: 1.030 | Reg loss: 0.025 | Tree loss: 1.030 | Accuracy: 0.649000 | 1.344 sec/iter\n",
      "Epoch: 318 | Batch: 001 / 011 | Total loss: 1.010 | Reg loss: 0.025 | Tree loss: 1.010 | Accuracy: 0.661000 | 1.344 sec/iter\n",
      "Epoch: 318 | Batch: 002 / 011 | Total loss: 0.990 | Reg loss: 0.025 | Tree loss: 0.990 | Accuracy: 0.674500 | 1.344 sec/iter\n",
      "Epoch: 318 | Batch: 003 / 011 | Total loss: 0.982 | Reg loss: 0.025 | Tree loss: 0.982 | Accuracy: 0.687500 | 1.344 sec/iter\n",
      "Epoch: 318 | Batch: 004 / 011 | Total loss: 0.943 | Reg loss: 0.025 | Tree loss: 0.943 | Accuracy: 0.681500 | 1.345 sec/iter\n",
      "Epoch: 318 | Batch: 005 / 011 | Total loss: 0.987 | Reg loss: 0.025 | Tree loss: 0.987 | Accuracy: 0.649000 | 1.345 sec/iter\n",
      "Epoch: 318 | Batch: 006 / 011 | Total loss: 0.917 | Reg loss: 0.025 | Tree loss: 0.917 | Accuracy: 0.677500 | 1.345 sec/iter\n",
      "Epoch: 318 | Batch: 007 / 011 | Total loss: 0.935 | Reg loss: 0.025 | Tree loss: 0.935 | Accuracy: 0.659500 | 1.345 sec/iter\n",
      "Epoch: 318 | Batch: 008 / 011 | Total loss: 0.910 | Reg loss: 0.025 | Tree loss: 0.910 | Accuracy: 0.663000 | 1.345 sec/iter\n",
      "Epoch: 318 | Batch: 009 / 011 | Total loss: 0.945 | Reg loss: 0.025 | Tree loss: 0.945 | Accuracy: 0.634000 | 1.345 sec/iter\n",
      "Epoch: 318 | Batch: 010 / 011 | Total loss: 0.927 | Reg loss: 0.025 | Tree loss: 0.927 | Accuracy: 0.631399 | 1.345 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 319 | Batch: 000 / 011 | Total loss: 1.012 | Reg loss: 0.025 | Tree loss: 1.012 | Accuracy: 0.663500 | 1.346 sec/iter\n",
      "Epoch: 319 | Batch: 001 / 011 | Total loss: 1.017 | Reg loss: 0.025 | Tree loss: 1.017 | Accuracy: 0.654500 | 1.346 sec/iter\n",
      "Epoch: 319 | Batch: 002 / 011 | Total loss: 1.017 | Reg loss: 0.025 | Tree loss: 1.017 | Accuracy: 0.661000 | 1.346 sec/iter\n",
      "Epoch: 319 | Batch: 003 / 011 | Total loss: 0.967 | Reg loss: 0.025 | Tree loss: 0.967 | Accuracy: 0.685500 | 1.346 sec/iter\n",
      "Epoch: 319 | Batch: 004 / 011 | Total loss: 0.969 | Reg loss: 0.025 | Tree loss: 0.969 | Accuracy: 0.694500 | 1.346 sec/iter\n",
      "Epoch: 319 | Batch: 005 / 011 | Total loss: 0.923 | Reg loss: 0.025 | Tree loss: 0.923 | Accuracy: 0.684000 | 1.346 sec/iter\n",
      "Epoch: 319 | Batch: 006 / 011 | Total loss: 0.934 | Reg loss: 0.025 | Tree loss: 0.934 | Accuracy: 0.667500 | 1.346 sec/iter\n",
      "Epoch: 319 | Batch: 007 / 011 | Total loss: 0.938 | Reg loss: 0.025 | Tree loss: 0.938 | Accuracy: 0.658000 | 1.346 sec/iter\n",
      "Epoch: 319 | Batch: 008 / 011 | Total loss: 0.926 | Reg loss: 0.025 | Tree loss: 0.926 | Accuracy: 0.647000 | 1.346 sec/iter\n",
      "Epoch: 319 | Batch: 009 / 011 | Total loss: 0.927 | Reg loss: 0.025 | Tree loss: 0.927 | Accuracy: 0.642500 | 1.346 sec/iter\n",
      "Epoch: 319 | Batch: 010 / 011 | Total loss: 0.992 | Reg loss: 0.025 | Tree loss: 0.992 | Accuracy: 0.610922 | 1.346 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 320 | Batch: 000 / 011 | Total loss: 1.022 | Reg loss: 0.025 | Tree loss: 1.022 | Accuracy: 0.651000 | 1.347 sec/iter\n",
      "Epoch: 320 | Batch: 001 / 011 | Total loss: 1.000 | Reg loss: 0.025 | Tree loss: 1.000 | Accuracy: 0.667500 | 1.347 sec/iter\n",
      "Epoch: 320 | Batch: 002 / 011 | Total loss: 1.017 | Reg loss: 0.025 | Tree loss: 1.017 | Accuracy: 0.659000 | 1.347 sec/iter\n",
      "Epoch: 320 | Batch: 003 / 011 | Total loss: 0.973 | Reg loss: 0.025 | Tree loss: 0.973 | Accuracy: 0.676000 | 1.347 sec/iter\n",
      "Epoch: 320 | Batch: 004 / 011 | Total loss: 0.960 | Reg loss: 0.025 | Tree loss: 0.960 | Accuracy: 0.673500 | 1.347 sec/iter\n",
      "Epoch: 320 | Batch: 005 / 011 | Total loss: 0.959 | Reg loss: 0.025 | Tree loss: 0.959 | Accuracy: 0.663500 | 1.347 sec/iter\n",
      "Epoch: 320 | Batch: 006 / 011 | Total loss: 0.944 | Reg loss: 0.025 | Tree loss: 0.944 | Accuracy: 0.661000 | 1.347 sec/iter\n",
      "Epoch: 320 | Batch: 007 / 011 | Total loss: 0.939 | Reg loss: 0.025 | Tree loss: 0.939 | Accuracy: 0.659000 | 1.347 sec/iter\n",
      "Epoch: 320 | Batch: 008 / 011 | Total loss: 0.946 | Reg loss: 0.025 | Tree loss: 0.946 | Accuracy: 0.638500 | 1.348 sec/iter\n",
      "Epoch: 320 | Batch: 009 / 011 | Total loss: 0.906 | Reg loss: 0.025 | Tree loss: 0.906 | Accuracy: 0.659000 | 1.348 sec/iter\n",
      "Epoch: 320 | Batch: 010 / 011 | Total loss: 0.892 | Reg loss: 0.025 | Tree loss: 0.892 | Accuracy: 0.672355 | 1.348 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 321 | Batch: 000 / 011 | Total loss: 1.006 | Reg loss: 0.025 | Tree loss: 1.006 | Accuracy: 0.655000 | 1.348 sec/iter\n",
      "Epoch: 321 | Batch: 001 / 011 | Total loss: 1.009 | Reg loss: 0.025 | Tree loss: 1.009 | Accuracy: 0.639500 | 1.348 sec/iter\n",
      "Epoch: 321 | Batch: 002 / 011 | Total loss: 1.019 | Reg loss: 0.025 | Tree loss: 1.019 | Accuracy: 0.651500 | 1.348 sec/iter\n",
      "Epoch: 321 | Batch: 003 / 011 | Total loss: 0.976 | Reg loss: 0.025 | Tree loss: 0.976 | Accuracy: 0.682000 | 1.348 sec/iter\n",
      "Epoch: 321 | Batch: 004 / 011 | Total loss: 0.937 | Reg loss: 0.025 | Tree loss: 0.937 | Accuracy: 0.689500 | 1.348 sec/iter\n",
      "Epoch: 321 | Batch: 005 / 011 | Total loss: 0.930 | Reg loss: 0.025 | Tree loss: 0.930 | Accuracy: 0.694500 | 1.348 sec/iter\n",
      "Epoch: 321 | Batch: 006 / 011 | Total loss: 0.939 | Reg loss: 0.025 | Tree loss: 0.939 | Accuracy: 0.678500 | 1.348 sec/iter\n",
      "Epoch: 321 | Batch: 007 / 011 | Total loss: 0.938 | Reg loss: 0.025 | Tree loss: 0.938 | Accuracy: 0.653500 | 1.348 sec/iter\n",
      "Epoch: 321 | Batch: 008 / 011 | Total loss: 0.931 | Reg loss: 0.025 | Tree loss: 0.931 | Accuracy: 0.652500 | 1.348 sec/iter\n",
      "Epoch: 321 | Batch: 009 / 011 | Total loss: 0.960 | Reg loss: 0.025 | Tree loss: 0.960 | Accuracy: 0.613500 | 1.348 sec/iter\n",
      "Epoch: 321 | Batch: 010 / 011 | Total loss: 0.956 | Reg loss: 0.025 | Tree loss: 0.956 | Accuracy: 0.631399 | 1.348 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 322 | Batch: 000 / 011 | Total loss: 1.024 | Reg loss: 0.025 | Tree loss: 1.024 | Accuracy: 0.657500 | 1.349 sec/iter\n",
      "Epoch: 322 | Batch: 001 / 011 | Total loss: 1.023 | Reg loss: 0.025 | Tree loss: 1.023 | Accuracy: 0.654500 | 1.349 sec/iter\n",
      "Epoch: 322 | Batch: 002 / 011 | Total loss: 0.983 | Reg loss: 0.025 | Tree loss: 0.983 | Accuracy: 0.687500 | 1.349 sec/iter\n",
      "Epoch: 322 | Batch: 003 / 011 | Total loss: 0.945 | Reg loss: 0.025 | Tree loss: 0.945 | Accuracy: 0.695500 | 1.349 sec/iter\n",
      "Epoch: 322 | Batch: 004 / 011 | Total loss: 0.961 | Reg loss: 0.025 | Tree loss: 0.961 | Accuracy: 0.694000 | 1.349 sec/iter\n",
      "Epoch: 322 | Batch: 005 / 011 | Total loss: 0.973 | Reg loss: 0.025 | Tree loss: 0.973 | Accuracy: 0.654000 | 1.349 sec/iter\n",
      "Epoch: 322 | Batch: 006 / 011 | Total loss: 0.935 | Reg loss: 0.025 | Tree loss: 0.935 | Accuracy: 0.676500 | 1.35 sec/iter\n",
      "Epoch: 322 | Batch: 007 / 011 | Total loss: 0.950 | Reg loss: 0.025 | Tree loss: 0.950 | Accuracy: 0.634000 | 1.35 sec/iter\n",
      "Epoch: 322 | Batch: 008 / 011 | Total loss: 0.932 | Reg loss: 0.025 | Tree loss: 0.932 | Accuracy: 0.661000 | 1.35 sec/iter\n",
      "Epoch: 322 | Batch: 009 / 011 | Total loss: 0.912 | Reg loss: 0.025 | Tree loss: 0.912 | Accuracy: 0.661500 | 1.35 sec/iter\n",
      "Epoch: 322 | Batch: 010 / 011 | Total loss: 0.881 | Reg loss: 0.025 | Tree loss: 0.881 | Accuracy: 0.686007 | 1.35 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 323 | Batch: 000 / 011 | Total loss: 1.039 | Reg loss: 0.025 | Tree loss: 1.039 | Accuracy: 0.645000 | 1.351 sec/iter\n",
      "Epoch: 323 | Batch: 001 / 011 | Total loss: 1.013 | Reg loss: 0.025 | Tree loss: 1.013 | Accuracy: 0.676500 | 1.351 sec/iter\n",
      "Epoch: 323 | Batch: 002 / 011 | Total loss: 0.992 | Reg loss: 0.025 | Tree loss: 0.992 | Accuracy: 0.669500 | 1.351 sec/iter\n",
      "Epoch: 323 | Batch: 003 / 011 | Total loss: 0.966 | Reg loss: 0.025 | Tree loss: 0.966 | Accuracy: 0.699000 | 1.351 sec/iter\n",
      "Epoch: 323 | Batch: 004 / 011 | Total loss: 0.962 | Reg loss: 0.025 | Tree loss: 0.962 | Accuracy: 0.662500 | 1.351 sec/iter\n",
      "Epoch: 323 | Batch: 005 / 011 | Total loss: 0.927 | Reg loss: 0.025 | Tree loss: 0.927 | Accuracy: 0.684500 | 1.351 sec/iter\n",
      "Epoch: 323 | Batch: 006 / 011 | Total loss: 0.933 | Reg loss: 0.025 | Tree loss: 0.933 | Accuracy: 0.656000 | 1.351 sec/iter\n",
      "Epoch: 323 | Batch: 007 / 011 | Total loss: 0.920 | Reg loss: 0.025 | Tree loss: 0.920 | Accuracy: 0.655500 | 1.351 sec/iter\n",
      "Epoch: 323 | Batch: 008 / 011 | Total loss: 0.932 | Reg loss: 0.025 | Tree loss: 0.932 | Accuracy: 0.653500 | 1.351 sec/iter\n",
      "Epoch: 323 | Batch: 009 / 011 | Total loss: 0.941 | Reg loss: 0.025 | Tree loss: 0.941 | Accuracy: 0.645000 | 1.351 sec/iter\n",
      "Epoch: 323 | Batch: 010 / 011 | Total loss: 0.863 | Reg loss: 0.025 | Tree loss: 0.863 | Accuracy: 0.713311 | 1.351 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 324 | Batch: 000 / 011 | Total loss: 1.039 | Reg loss: 0.025 | Tree loss: 1.039 | Accuracy: 0.635000 | 1.352 sec/iter\n",
      "Epoch: 324 | Batch: 001 / 011 | Total loss: 0.998 | Reg loss: 0.025 | Tree loss: 0.998 | Accuracy: 0.667500 | 1.352 sec/iter\n",
      "Epoch: 324 | Batch: 002 / 011 | Total loss: 1.007 | Reg loss: 0.025 | Tree loss: 1.007 | Accuracy: 0.669000 | 1.352 sec/iter\n",
      "Epoch: 324 | Batch: 003 / 011 | Total loss: 0.962 | Reg loss: 0.025 | Tree loss: 0.962 | Accuracy: 0.684500 | 1.352 sec/iter\n",
      "Epoch: 324 | Batch: 004 / 011 | Total loss: 0.949 | Reg loss: 0.025 | Tree loss: 0.949 | Accuracy: 0.668500 | 1.353 sec/iter\n",
      "Epoch: 324 | Batch: 005 / 011 | Total loss: 0.967 | Reg loss: 0.025 | Tree loss: 0.967 | Accuracy: 0.668000 | 1.353 sec/iter\n",
      "Epoch: 324 | Batch: 006 / 011 | Total loss: 0.939 | Reg loss: 0.025 | Tree loss: 0.939 | Accuracy: 0.665500 | 1.353 sec/iter\n",
      "Epoch: 324 | Batch: 007 / 011 | Total loss: 0.933 | Reg loss: 0.025 | Tree loss: 0.933 | Accuracy: 0.667500 | 1.353 sec/iter\n",
      "Epoch: 324 | Batch: 008 / 011 | Total loss: 0.917 | Reg loss: 0.025 | Tree loss: 0.917 | Accuracy: 0.664000 | 1.353 sec/iter\n",
      "Epoch: 324 | Batch: 009 / 011 | Total loss: 0.931 | Reg loss: 0.025 | Tree loss: 0.931 | Accuracy: 0.654500 | 1.353 sec/iter\n",
      "Epoch: 324 | Batch: 010 / 011 | Total loss: 0.885 | Reg loss: 0.025 | Tree loss: 0.885 | Accuracy: 0.686007 | 1.353 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 325 | Batch: 000 / 011 | Total loss: 1.032 | Reg loss: 0.025 | Tree loss: 1.032 | Accuracy: 0.640000 | 1.353 sec/iter\n",
      "Epoch: 325 | Batch: 001 / 011 | Total loss: 1.004 | Reg loss: 0.025 | Tree loss: 1.004 | Accuracy: 0.670000 | 1.353 sec/iter\n",
      "Epoch: 325 | Batch: 002 / 011 | Total loss: 0.987 | Reg loss: 0.025 | Tree loss: 0.987 | Accuracy: 0.686000 | 1.353 sec/iter\n",
      "Epoch: 325 | Batch: 003 / 011 | Total loss: 0.979 | Reg loss: 0.025 | Tree loss: 0.979 | Accuracy: 0.676500 | 1.353 sec/iter\n",
      "Epoch: 325 | Batch: 004 / 011 | Total loss: 0.951 | Reg loss: 0.025 | Tree loss: 0.951 | Accuracy: 0.683000 | 1.353 sec/iter\n",
      "Epoch: 325 | Batch: 005 / 011 | Total loss: 0.948 | Reg loss: 0.025 | Tree loss: 0.948 | Accuracy: 0.673000 | 1.353 sec/iter\n",
      "Epoch: 325 | Batch: 006 / 011 | Total loss: 0.900 | Reg loss: 0.025 | Tree loss: 0.900 | Accuracy: 0.683500 | 1.353 sec/iter\n",
      "Epoch: 325 | Batch: 007 / 011 | Total loss: 0.945 | Reg loss: 0.025 | Tree loss: 0.945 | Accuracy: 0.644000 | 1.353 sec/iter\n",
      "Epoch: 325 | Batch: 008 / 011 | Total loss: 0.948 | Reg loss: 0.025 | Tree loss: 0.948 | Accuracy: 0.636000 | 1.353 sec/iter\n",
      "Epoch: 325 | Batch: 009 / 011 | Total loss: 0.930 | Reg loss: 0.025 | Tree loss: 0.930 | Accuracy: 0.658000 | 1.353 sec/iter\n",
      "Epoch: 325 | Batch: 010 / 011 | Total loss: 0.914 | Reg loss: 0.025 | Tree loss: 0.914 | Accuracy: 0.648464 | 1.353 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 326 | Batch: 000 / 011 | Total loss: 1.044 | Reg loss: 0.025 | Tree loss: 1.044 | Accuracy: 0.648500 | 1.354 sec/iter\n",
      "Epoch: 326 | Batch: 001 / 011 | Total loss: 0.988 | Reg loss: 0.025 | Tree loss: 0.988 | Accuracy: 0.656000 | 1.354 sec/iter\n",
      "Epoch: 326 | Batch: 002 / 011 | Total loss: 0.967 | Reg loss: 0.025 | Tree loss: 0.967 | Accuracy: 0.690000 | 1.354 sec/iter\n",
      "Epoch: 326 | Batch: 003 / 011 | Total loss: 1.008 | Reg loss: 0.025 | Tree loss: 1.008 | Accuracy: 0.659500 | 1.354 sec/iter\n",
      "Epoch: 326 | Batch: 004 / 011 | Total loss: 0.946 | Reg loss: 0.025 | Tree loss: 0.946 | Accuracy: 0.693000 | 1.354 sec/iter\n",
      "Epoch: 326 | Batch: 005 / 011 | Total loss: 0.942 | Reg loss: 0.025 | Tree loss: 0.942 | Accuracy: 0.676500 | 1.354 sec/iter\n",
      "Epoch: 326 | Batch: 006 / 011 | Total loss: 0.947 | Reg loss: 0.025 | Tree loss: 0.947 | Accuracy: 0.658500 | 1.354 sec/iter\n",
      "Epoch: 326 | Batch: 007 / 011 | Total loss: 0.935 | Reg loss: 0.025 | Tree loss: 0.935 | Accuracy: 0.675500 | 1.354 sec/iter\n",
      "Epoch: 326 | Batch: 008 / 011 | Total loss: 0.928 | Reg loss: 0.025 | Tree loss: 0.928 | Accuracy: 0.667500 | 1.354 sec/iter\n",
      "Epoch: 326 | Batch: 009 / 011 | Total loss: 0.932 | Reg loss: 0.025 | Tree loss: 0.932 | Accuracy: 0.663000 | 1.355 sec/iter\n",
      "Epoch: 326 | Batch: 010 / 011 | Total loss: 0.954 | Reg loss: 0.025 | Tree loss: 0.954 | Accuracy: 0.638225 | 1.355 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 327 | Batch: 000 / 011 | Total loss: 1.019 | Reg loss: 0.025 | Tree loss: 1.019 | Accuracy: 0.656000 | 1.356 sec/iter\n",
      "Epoch: 327 | Batch: 001 / 011 | Total loss: 0.987 | Reg loss: 0.025 | Tree loss: 0.987 | Accuracy: 0.682000 | 1.356 sec/iter\n",
      "Epoch: 327 | Batch: 002 / 011 | Total loss: 1.005 | Reg loss: 0.025 | Tree loss: 1.005 | Accuracy: 0.658000 | 1.356 sec/iter\n",
      "Epoch: 327 | Batch: 003 / 011 | Total loss: 0.997 | Reg loss: 0.025 | Tree loss: 0.997 | Accuracy: 0.661500 | 1.356 sec/iter\n",
      "Epoch: 327 | Batch: 004 / 011 | Total loss: 0.959 | Reg loss: 0.025 | Tree loss: 0.959 | Accuracy: 0.676500 | 1.356 sec/iter\n",
      "Epoch: 327 | Batch: 005 / 011 | Total loss: 0.960 | Reg loss: 0.025 | Tree loss: 0.960 | Accuracy: 0.671000 | 1.356 sec/iter\n",
      "Epoch: 327 | Batch: 006 / 011 | Total loss: 0.957 | Reg loss: 0.025 | Tree loss: 0.957 | Accuracy: 0.657000 | 1.356 sec/iter\n",
      "Epoch: 327 | Batch: 007 / 011 | Total loss: 0.926 | Reg loss: 0.025 | Tree loss: 0.926 | Accuracy: 0.664000 | 1.356 sec/iter\n",
      "Epoch: 327 | Batch: 008 / 011 | Total loss: 0.927 | Reg loss: 0.025 | Tree loss: 0.927 | Accuracy: 0.658000 | 1.356 sec/iter\n",
      "Epoch: 327 | Batch: 009 / 011 | Total loss: 0.921 | Reg loss: 0.025 | Tree loss: 0.921 | Accuracy: 0.656500 | 1.356 sec/iter\n",
      "Epoch: 327 | Batch: 010 / 011 | Total loss: 0.952 | Reg loss: 0.025 | Tree loss: 0.952 | Accuracy: 0.645051 | 1.356 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 328 | Batch: 000 / 011 | Total loss: 0.998 | Reg loss: 0.025 | Tree loss: 0.998 | Accuracy: 0.667000 | 1.356 sec/iter\n",
      "Epoch: 328 | Batch: 001 / 011 | Total loss: 1.019 | Reg loss: 0.025 | Tree loss: 1.019 | Accuracy: 0.645000 | 1.356 sec/iter\n",
      "Epoch: 328 | Batch: 002 / 011 | Total loss: 0.987 | Reg loss: 0.025 | Tree loss: 0.987 | Accuracy: 0.665500 | 1.356 sec/iter\n",
      "Epoch: 328 | Batch: 003 / 011 | Total loss: 0.969 | Reg loss: 0.025 | Tree loss: 0.969 | Accuracy: 0.694000 | 1.356 sec/iter\n",
      "Epoch: 328 | Batch: 004 / 011 | Total loss: 0.947 | Reg loss: 0.025 | Tree loss: 0.947 | Accuracy: 0.672000 | 1.356 sec/iter\n",
      "Epoch: 328 | Batch: 005 / 011 | Total loss: 0.963 | Reg loss: 0.025 | Tree loss: 0.963 | Accuracy: 0.675000 | 1.356 sec/iter\n",
      "Epoch: 328 | Batch: 006 / 011 | Total loss: 0.964 | Reg loss: 0.025 | Tree loss: 0.964 | Accuracy: 0.652000 | 1.356 sec/iter\n",
      "Epoch: 328 | Batch: 007 / 011 | Total loss: 0.942 | Reg loss: 0.025 | Tree loss: 0.942 | Accuracy: 0.669000 | 1.356 sec/iter\n",
      "Epoch: 328 | Batch: 008 / 011 | Total loss: 0.938 | Reg loss: 0.025 | Tree loss: 0.938 | Accuracy: 0.661500 | 1.357 sec/iter\n",
      "Epoch: 328 | Batch: 009 / 011 | Total loss: 0.910 | Reg loss: 0.025 | Tree loss: 0.910 | Accuracy: 0.663000 | 1.357 sec/iter\n",
      "Epoch: 328 | Batch: 010 / 011 | Total loss: 0.927 | Reg loss: 0.025 | Tree loss: 0.927 | Accuracy: 0.634812 | 1.357 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 329 | Batch: 000 / 011 | Total loss: 1.012 | Reg loss: 0.025 | Tree loss: 1.012 | Accuracy: 0.662500 | 1.357 sec/iter\n",
      "Epoch: 329 | Batch: 001 / 011 | Total loss: 1.051 | Reg loss: 0.025 | Tree loss: 1.051 | Accuracy: 0.628500 | 1.358 sec/iter\n",
      "Epoch: 329 | Batch: 002 / 011 | Total loss: 0.985 | Reg loss: 0.025 | Tree loss: 0.985 | Accuracy: 0.672500 | 1.358 sec/iter\n",
      "Epoch: 329 | Batch: 003 / 011 | Total loss: 0.980 | Reg loss: 0.025 | Tree loss: 0.980 | Accuracy: 0.690500 | 1.358 sec/iter\n",
      "Epoch: 329 | Batch: 004 / 011 | Total loss: 0.961 | Reg loss: 0.025 | Tree loss: 0.961 | Accuracy: 0.678500 | 1.358 sec/iter\n",
      "Epoch: 329 | Batch: 005 / 011 | Total loss: 0.958 | Reg loss: 0.025 | Tree loss: 0.958 | Accuracy: 0.665500 | 1.358 sec/iter\n",
      "Epoch: 329 | Batch: 006 / 011 | Total loss: 0.920 | Reg loss: 0.025 | Tree loss: 0.920 | Accuracy: 0.680500 | 1.358 sec/iter\n",
      "Epoch: 329 | Batch: 007 / 011 | Total loss: 0.928 | Reg loss: 0.025 | Tree loss: 0.928 | Accuracy: 0.650000 | 1.358 sec/iter\n",
      "Epoch: 329 | Batch: 008 / 011 | Total loss: 0.951 | Reg loss: 0.025 | Tree loss: 0.951 | Accuracy: 0.639500 | 1.358 sec/iter\n",
      "Epoch: 329 | Batch: 009 / 011 | Total loss: 0.909 | Reg loss: 0.025 | Tree loss: 0.909 | Accuracy: 0.660500 | 1.358 sec/iter\n",
      "Epoch: 329 | Batch: 010 / 011 | Total loss: 0.888 | Reg loss: 0.025 | Tree loss: 0.888 | Accuracy: 0.627986 | 1.358 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 330 | Batch: 000 / 011 | Total loss: 1.010 | Reg loss: 0.025 | Tree loss: 1.010 | Accuracy: 0.660000 | 1.359 sec/iter\n",
      "Epoch: 330 | Batch: 001 / 011 | Total loss: 1.011 | Reg loss: 0.025 | Tree loss: 1.011 | Accuracy: 0.650500 | 1.359 sec/iter\n",
      "Epoch: 330 | Batch: 002 / 011 | Total loss: 0.983 | Reg loss: 0.025 | Tree loss: 0.983 | Accuracy: 0.690000 | 1.359 sec/iter\n",
      "Epoch: 330 | Batch: 003 / 011 | Total loss: 0.987 | Reg loss: 0.025 | Tree loss: 0.987 | Accuracy: 0.682000 | 1.359 sec/iter\n",
      "Epoch: 330 | Batch: 004 / 011 | Total loss: 0.954 | Reg loss: 0.025 | Tree loss: 0.954 | Accuracy: 0.688000 | 1.359 sec/iter\n",
      "Epoch: 330 | Batch: 005 / 011 | Total loss: 0.932 | Reg loss: 0.025 | Tree loss: 0.932 | Accuracy: 0.673000 | 1.359 sec/iter\n",
      "Epoch: 330 | Batch: 006 / 011 | Total loss: 0.960 | Reg loss: 0.025 | Tree loss: 0.960 | Accuracy: 0.658500 | 1.359 sec/iter\n",
      "Epoch: 330 | Batch: 007 / 011 | Total loss: 0.936 | Reg loss: 0.025 | Tree loss: 0.936 | Accuracy: 0.647000 | 1.359 sec/iter\n",
      "Epoch: 330 | Batch: 008 / 011 | Total loss: 0.955 | Reg loss: 0.025 | Tree loss: 0.955 | Accuracy: 0.647000 | 1.359 sec/iter\n",
      "Epoch: 330 | Batch: 009 / 011 | Total loss: 0.896 | Reg loss: 0.025 | Tree loss: 0.896 | Accuracy: 0.673000 | 1.36 sec/iter\n",
      "Epoch: 330 | Batch: 010 / 011 | Total loss: 0.933 | Reg loss: 0.025 | Tree loss: 0.933 | Accuracy: 0.648464 | 1.36 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 331 | Batch: 000 / 011 | Total loss: 1.023 | Reg loss: 0.025 | Tree loss: 1.023 | Accuracy: 0.662500 | 1.361 sec/iter\n",
      "Epoch: 331 | Batch: 001 / 011 | Total loss: 1.005 | Reg loss: 0.025 | Tree loss: 1.005 | Accuracy: 0.669500 | 1.361 sec/iter\n",
      "Epoch: 331 | Batch: 002 / 011 | Total loss: 0.990 | Reg loss: 0.025 | Tree loss: 0.990 | Accuracy: 0.671500 | 1.361 sec/iter\n",
      "Epoch: 331 | Batch: 003 / 011 | Total loss: 0.973 | Reg loss: 0.025 | Tree loss: 0.973 | Accuracy: 0.671500 | 1.361 sec/iter\n",
      "Epoch: 331 | Batch: 004 / 011 | Total loss: 0.958 | Reg loss: 0.025 | Tree loss: 0.958 | Accuracy: 0.686000 | 1.361 sec/iter\n",
      "Epoch: 331 | Batch: 005 / 011 | Total loss: 0.945 | Reg loss: 0.025 | Tree loss: 0.945 | Accuracy: 0.678500 | 1.361 sec/iter\n",
      "Epoch: 331 | Batch: 006 / 011 | Total loss: 0.945 | Reg loss: 0.025 | Tree loss: 0.945 | Accuracy: 0.662500 | 1.361 sec/iter\n",
      "Epoch: 331 | Batch: 007 / 011 | Total loss: 0.918 | Reg loss: 0.025 | Tree loss: 0.918 | Accuracy: 0.655000 | 1.361 sec/iter\n",
      "Epoch: 331 | Batch: 008 / 011 | Total loss: 0.929 | Reg loss: 0.025 | Tree loss: 0.929 | Accuracy: 0.643000 | 1.361 sec/iter\n",
      "Epoch: 331 | Batch: 009 / 011 | Total loss: 0.924 | Reg loss: 0.025 | Tree loss: 0.924 | Accuracy: 0.647500 | 1.361 sec/iter\n",
      "Epoch: 331 | Batch: 010 / 011 | Total loss: 0.898 | Reg loss: 0.025 | Tree loss: 0.898 | Accuracy: 0.689420 | 1.361 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 332 | Batch: 000 / 011 | Total loss: 1.042 | Reg loss: 0.025 | Tree loss: 1.042 | Accuracy: 0.641500 | 1.362 sec/iter\n",
      "Epoch: 332 | Batch: 001 / 011 | Total loss: 1.016 | Reg loss: 0.025 | Tree loss: 1.016 | Accuracy: 0.660000 | 1.362 sec/iter\n",
      "Epoch: 332 | Batch: 002 / 011 | Total loss: 0.974 | Reg loss: 0.025 | Tree loss: 0.974 | Accuracy: 0.695000 | 1.362 sec/iter\n",
      "Epoch: 332 | Batch: 003 / 011 | Total loss: 0.962 | Reg loss: 0.025 | Tree loss: 0.962 | Accuracy: 0.694500 | 1.362 sec/iter\n",
      "Epoch: 332 | Batch: 004 / 011 | Total loss: 0.959 | Reg loss: 0.025 | Tree loss: 0.959 | Accuracy: 0.670500 | 1.362 sec/iter\n",
      "Epoch: 332 | Batch: 005 / 011 | Total loss: 0.953 | Reg loss: 0.025 | Tree loss: 0.953 | Accuracy: 0.678500 | 1.362 sec/iter\n",
      "Epoch: 332 | Batch: 006 / 011 | Total loss: 0.921 | Reg loss: 0.025 | Tree loss: 0.921 | Accuracy: 0.671000 | 1.362 sec/iter\n",
      "Epoch: 332 | Batch: 007 / 011 | Total loss: 0.918 | Reg loss: 0.025 | Tree loss: 0.918 | Accuracy: 0.665500 | 1.362 sec/iter\n",
      "Epoch: 332 | Batch: 008 / 011 | Total loss: 0.925 | Reg loss: 0.025 | Tree loss: 0.925 | Accuracy: 0.657000 | 1.363 sec/iter\n",
      "Epoch: 332 | Batch: 009 / 011 | Total loss: 0.940 | Reg loss: 0.025 | Tree loss: 0.940 | Accuracy: 0.657500 | 1.363 sec/iter\n",
      "Epoch: 332 | Batch: 010 / 011 | Total loss: 0.952 | Reg loss: 0.025 | Tree loss: 0.952 | Accuracy: 0.634812 | 1.363 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 333 | Batch: 000 / 011 | Total loss: 1.016 | Reg loss: 0.025 | Tree loss: 1.016 | Accuracy: 0.648000 | 1.363 sec/iter\n",
      "Epoch: 333 | Batch: 001 / 011 | Total loss: 0.999 | Reg loss: 0.025 | Tree loss: 0.999 | Accuracy: 0.666500 | 1.363 sec/iter\n",
      "Epoch: 333 | Batch: 002 / 011 | Total loss: 1.013 | Reg loss: 0.025 | Tree loss: 1.013 | Accuracy: 0.665500 | 1.363 sec/iter\n",
      "Epoch: 333 | Batch: 003 / 011 | Total loss: 0.977 | Reg loss: 0.025 | Tree loss: 0.977 | Accuracy: 0.684000 | 1.363 sec/iter\n",
      "Epoch: 333 | Batch: 004 / 011 | Total loss: 0.939 | Reg loss: 0.025 | Tree loss: 0.939 | Accuracy: 0.694500 | 1.363 sec/iter\n",
      "Epoch: 333 | Batch: 005 / 011 | Total loss: 0.938 | Reg loss: 0.025 | Tree loss: 0.938 | Accuracy: 0.679500 | 1.363 sec/iter\n",
      "Epoch: 333 | Batch: 006 / 011 | Total loss: 0.944 | Reg loss: 0.025 | Tree loss: 0.944 | Accuracy: 0.652500 | 1.363 sec/iter\n",
      "Epoch: 333 | Batch: 007 / 011 | Total loss: 0.964 | Reg loss: 0.025 | Tree loss: 0.964 | Accuracy: 0.640500 | 1.363 sec/iter\n",
      "Epoch: 333 | Batch: 008 / 011 | Total loss: 0.941 | Reg loss: 0.025 | Tree loss: 0.941 | Accuracy: 0.650500 | 1.363 sec/iter\n",
      "Epoch: 333 | Batch: 009 / 011 | Total loss: 0.906 | Reg loss: 0.025 | Tree loss: 0.906 | Accuracy: 0.666500 | 1.363 sec/iter\n",
      "Epoch: 333 | Batch: 010 / 011 | Total loss: 0.896 | Reg loss: 0.025 | Tree loss: 0.896 | Accuracy: 0.668942 | 1.363 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 334 | Batch: 000 / 011 | Total loss: 1.007 | Reg loss: 0.025 | Tree loss: 1.007 | Accuracy: 0.672500 | 1.364 sec/iter\n",
      "Epoch: 334 | Batch: 001 / 011 | Total loss: 0.996 | Reg loss: 0.025 | Tree loss: 0.996 | Accuracy: 0.674500 | 1.364 sec/iter\n",
      "Epoch: 334 | Batch: 002 / 011 | Total loss: 0.988 | Reg loss: 0.025 | Tree loss: 0.988 | Accuracy: 0.679000 | 1.364 sec/iter\n",
      "Epoch: 334 | Batch: 003 / 011 | Total loss: 0.976 | Reg loss: 0.025 | Tree loss: 0.976 | Accuracy: 0.684500 | 1.364 sec/iter\n",
      "Epoch: 334 | Batch: 004 / 011 | Total loss: 0.953 | Reg loss: 0.025 | Tree loss: 0.953 | Accuracy: 0.687500 | 1.364 sec/iter\n",
      "Epoch: 334 | Batch: 005 / 011 | Total loss: 0.946 | Reg loss: 0.025 | Tree loss: 0.946 | Accuracy: 0.667000 | 1.364 sec/iter\n",
      "Epoch: 334 | Batch: 006 / 011 | Total loss: 0.937 | Reg loss: 0.025 | Tree loss: 0.937 | Accuracy: 0.665000 | 1.364 sec/iter\n",
      "Epoch: 334 | Batch: 007 / 011 | Total loss: 0.911 | Reg loss: 0.025 | Tree loss: 0.911 | Accuracy: 0.682500 | 1.364 sec/iter\n",
      "Epoch: 334 | Batch: 008 / 011 | Total loss: 0.962 | Reg loss: 0.025 | Tree loss: 0.962 | Accuracy: 0.652000 | 1.364 sec/iter\n",
      "Epoch: 334 | Batch: 009 / 011 | Total loss: 0.956 | Reg loss: 0.025 | Tree loss: 0.956 | Accuracy: 0.629000 | 1.364 sec/iter\n",
      "Epoch: 334 | Batch: 010 / 011 | Total loss: 0.865 | Reg loss: 0.025 | Tree loss: 0.865 | Accuracy: 0.716724 | 1.364 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 335 | Batch: 000 / 011 | Total loss: 1.005 | Reg loss: 0.025 | Tree loss: 1.005 | Accuracy: 0.660000 | 1.365 sec/iter\n",
      "Epoch: 335 | Batch: 001 / 011 | Total loss: 1.005 | Reg loss: 0.025 | Tree loss: 1.005 | Accuracy: 0.654000 | 1.365 sec/iter\n",
      "Epoch: 335 | Batch: 002 / 011 | Total loss: 0.985 | Reg loss: 0.025 | Tree loss: 0.985 | Accuracy: 0.682000 | 1.365 sec/iter\n",
      "Epoch: 335 | Batch: 003 / 011 | Total loss: 0.947 | Reg loss: 0.025 | Tree loss: 0.947 | Accuracy: 0.703000 | 1.365 sec/iter\n",
      "Epoch: 335 | Batch: 004 / 011 | Total loss: 0.955 | Reg loss: 0.025 | Tree loss: 0.955 | Accuracy: 0.683000 | 1.365 sec/iter\n",
      "Epoch: 335 | Batch: 005 / 011 | Total loss: 0.967 | Reg loss: 0.025 | Tree loss: 0.967 | Accuracy: 0.668500 | 1.366 sec/iter\n",
      "Epoch: 335 | Batch: 006 / 011 | Total loss: 0.956 | Reg loss: 0.025 | Tree loss: 0.956 | Accuracy: 0.650000 | 1.366 sec/iter\n",
      "Epoch: 335 | Batch: 007 / 011 | Total loss: 0.940 | Reg loss: 0.025 | Tree loss: 0.940 | Accuracy: 0.669000 | 1.366 sec/iter\n",
      "Epoch: 335 | Batch: 008 / 011 | Total loss: 0.932 | Reg loss: 0.025 | Tree loss: 0.932 | Accuracy: 0.657500 | 1.366 sec/iter\n",
      "Epoch: 335 | Batch: 009 / 011 | Total loss: 0.913 | Reg loss: 0.025 | Tree loss: 0.913 | Accuracy: 0.658500 | 1.366 sec/iter\n",
      "Epoch: 335 | Batch: 010 / 011 | Total loss: 0.992 | Reg loss: 0.025 | Tree loss: 0.992 | Accuracy: 0.621160 | 1.366 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 336 | Batch: 000 / 011 | Total loss: 1.030 | Reg loss: 0.025 | Tree loss: 1.030 | Accuracy: 0.655500 | 1.367 sec/iter\n",
      "Epoch: 336 | Batch: 001 / 011 | Total loss: 1.024 | Reg loss: 0.025 | Tree loss: 1.024 | Accuracy: 0.648500 | 1.367 sec/iter\n",
      "Epoch: 336 | Batch: 002 / 011 | Total loss: 0.995 | Reg loss: 0.025 | Tree loss: 0.995 | Accuracy: 0.669500 | 1.367 sec/iter\n",
      "Epoch: 336 | Batch: 003 / 011 | Total loss: 0.960 | Reg loss: 0.025 | Tree loss: 0.960 | Accuracy: 0.677500 | 1.367 sec/iter\n",
      "Epoch: 336 | Batch: 004 / 011 | Total loss: 0.944 | Reg loss: 0.025 | Tree loss: 0.944 | Accuracy: 0.676500 | 1.367 sec/iter\n",
      "Epoch: 336 | Batch: 005 / 011 | Total loss: 0.948 | Reg loss: 0.025 | Tree loss: 0.948 | Accuracy: 0.659500 | 1.367 sec/iter\n",
      "Epoch: 336 | Batch: 006 / 011 | Total loss: 0.956 | Reg loss: 0.025 | Tree loss: 0.956 | Accuracy: 0.651500 | 1.367 sec/iter\n",
      "Epoch: 336 | Batch: 007 / 011 | Total loss: 0.930 | Reg loss: 0.025 | Tree loss: 0.930 | Accuracy: 0.657000 | 1.367 sec/iter\n",
      "Epoch: 336 | Batch: 008 / 011 | Total loss: 0.940 | Reg loss: 0.025 | Tree loss: 0.940 | Accuracy: 0.649500 | 1.367 sec/iter\n",
      "Epoch: 336 | Batch: 009 / 011 | Total loss: 0.903 | Reg loss: 0.025 | Tree loss: 0.903 | Accuracy: 0.671500 | 1.367 sec/iter\n",
      "Epoch: 336 | Batch: 010 / 011 | Total loss: 0.883 | Reg loss: 0.025 | Tree loss: 0.883 | Accuracy: 0.658703 | 1.367 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 337 | Batch: 000 / 011 | Total loss: 1.024 | Reg loss: 0.025 | Tree loss: 1.024 | Accuracy: 0.656500 | 1.368 sec/iter\n",
      "Epoch: 337 | Batch: 001 / 011 | Total loss: 0.992 | Reg loss: 0.025 | Tree loss: 0.992 | Accuracy: 0.683000 | 1.368 sec/iter\n",
      "Epoch: 337 | Batch: 002 / 011 | Total loss: 1.011 | Reg loss: 0.025 | Tree loss: 1.011 | Accuracy: 0.651000 | 1.368 sec/iter\n",
      "Epoch: 337 | Batch: 003 / 011 | Total loss: 0.979 | Reg loss: 0.025 | Tree loss: 0.979 | Accuracy: 0.677000 | 1.368 sec/iter\n",
      "Epoch: 337 | Batch: 004 / 011 | Total loss: 0.959 | Reg loss: 0.025 | Tree loss: 0.959 | Accuracy: 0.683000 | 1.368 sec/iter\n",
      "Epoch: 337 | Batch: 005 / 011 | Total loss: 0.932 | Reg loss: 0.025 | Tree loss: 0.932 | Accuracy: 0.677500 | 1.368 sec/iter\n",
      "Epoch: 337 | Batch: 006 / 011 | Total loss: 0.939 | Reg loss: 0.025 | Tree loss: 0.939 | Accuracy: 0.659500 | 1.368 sec/iter\n",
      "Epoch: 337 | Batch: 007 / 011 | Total loss: 0.925 | Reg loss: 0.025 | Tree loss: 0.925 | Accuracy: 0.656000 | 1.368 sec/iter\n",
      "Epoch: 337 | Batch: 008 / 011 | Total loss: 0.949 | Reg loss: 0.025 | Tree loss: 0.949 | Accuracy: 0.633000 | 1.368 sec/iter\n",
      "Epoch: 337 | Batch: 009 / 011 | Total loss: 0.909 | Reg loss: 0.025 | Tree loss: 0.909 | Accuracy: 0.661500 | 1.368 sec/iter\n",
      "Epoch: 337 | Batch: 010 / 011 | Total loss: 0.948 | Reg loss: 0.025 | Tree loss: 0.948 | Accuracy: 0.655290 | 1.368 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 338 | Batch: 000 / 011 | Total loss: 1.020 | Reg loss: 0.025 | Tree loss: 1.020 | Accuracy: 0.646500 | 1.369 sec/iter\n",
      "Epoch: 338 | Batch: 001 / 011 | Total loss: 1.019 | Reg loss: 0.025 | Tree loss: 1.019 | Accuracy: 0.656000 | 1.369 sec/iter\n",
      "Epoch: 338 | Batch: 002 / 011 | Total loss: 0.992 | Reg loss: 0.025 | Tree loss: 0.992 | Accuracy: 0.672500 | 1.369 sec/iter\n",
      "Epoch: 338 | Batch: 003 / 011 | Total loss: 0.977 | Reg loss: 0.025 | Tree loss: 0.977 | Accuracy: 0.675500 | 1.369 sec/iter\n",
      "Epoch: 338 | Batch: 004 / 011 | Total loss: 0.953 | Reg loss: 0.025 | Tree loss: 0.953 | Accuracy: 0.679500 | 1.369 sec/iter\n",
      "Epoch: 338 | Batch: 005 / 011 | Total loss: 0.919 | Reg loss: 0.025 | Tree loss: 0.919 | Accuracy: 0.690000 | 1.369 sec/iter\n",
      "Epoch: 338 | Batch: 006 / 011 | Total loss: 0.929 | Reg loss: 0.025 | Tree loss: 0.929 | Accuracy: 0.667500 | 1.369 sec/iter\n",
      "Epoch: 338 | Batch: 007 / 011 | Total loss: 0.921 | Reg loss: 0.025 | Tree loss: 0.921 | Accuracy: 0.664500 | 1.369 sec/iter\n",
      "Epoch: 338 | Batch: 008 / 011 | Total loss: 0.939 | Reg loss: 0.025 | Tree loss: 0.939 | Accuracy: 0.660500 | 1.369 sec/iter\n",
      "Epoch: 338 | Batch: 009 / 011 | Total loss: 0.944 | Reg loss: 0.025 | Tree loss: 0.944 | Accuracy: 0.638000 | 1.37 sec/iter\n",
      "Epoch: 338 | Batch: 010 / 011 | Total loss: 0.933 | Reg loss: 0.025 | Tree loss: 0.933 | Accuracy: 0.658703 | 1.37 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 339 | Batch: 000 / 011 | Total loss: 1.013 | Reg loss: 0.025 | Tree loss: 1.013 | Accuracy: 0.656500 | 1.37 sec/iter\n",
      "Epoch: 339 | Batch: 001 / 011 | Total loss: 1.016 | Reg loss: 0.025 | Tree loss: 1.016 | Accuracy: 0.641000 | 1.37 sec/iter\n",
      "Epoch: 339 | Batch: 002 / 011 | Total loss: 1.007 | Reg loss: 0.025 | Tree loss: 1.007 | Accuracy: 0.658000 | 1.37 sec/iter\n",
      "Epoch: 339 | Batch: 003 / 011 | Total loss: 0.949 | Reg loss: 0.025 | Tree loss: 0.949 | Accuracy: 0.692500 | 1.37 sec/iter\n",
      "Epoch: 339 | Batch: 004 / 011 | Total loss: 0.942 | Reg loss: 0.025 | Tree loss: 0.942 | Accuracy: 0.691500 | 1.371 sec/iter\n",
      "Epoch: 339 | Batch: 005 / 011 | Total loss: 0.959 | Reg loss: 0.025 | Tree loss: 0.959 | Accuracy: 0.674000 | 1.371 sec/iter\n",
      "Epoch: 339 | Batch: 006 / 011 | Total loss: 0.915 | Reg loss: 0.025 | Tree loss: 0.915 | Accuracy: 0.670500 | 1.371 sec/iter\n",
      "Epoch: 339 | Batch: 007 / 011 | Total loss: 0.959 | Reg loss: 0.025 | Tree loss: 0.959 | Accuracy: 0.653000 | 1.371 sec/iter\n",
      "Epoch: 339 | Batch: 008 / 011 | Total loss: 0.921 | Reg loss: 0.025 | Tree loss: 0.921 | Accuracy: 0.664500 | 1.371 sec/iter\n",
      "Epoch: 339 | Batch: 009 / 011 | Total loss: 0.950 | Reg loss: 0.025 | Tree loss: 0.950 | Accuracy: 0.639500 | 1.371 sec/iter\n",
      "Epoch: 339 | Batch: 010 / 011 | Total loss: 0.895 | Reg loss: 0.025 | Tree loss: 0.895 | Accuracy: 0.696246 | 1.371 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 340 | Batch: 000 / 011 | Total loss: 1.021 | Reg loss: 0.025 | Tree loss: 1.021 | Accuracy: 0.661000 | 1.372 sec/iter\n",
      "Epoch: 340 | Batch: 001 / 011 | Total loss: 1.005 | Reg loss: 0.025 | Tree loss: 1.005 | Accuracy: 0.664000 | 1.372 sec/iter\n",
      "Epoch: 340 | Batch: 002 / 011 | Total loss: 0.978 | Reg loss: 0.025 | Tree loss: 0.978 | Accuracy: 0.693000 | 1.372 sec/iter\n",
      "Epoch: 340 | Batch: 003 / 011 | Total loss: 0.956 | Reg loss: 0.025 | Tree loss: 0.956 | Accuracy: 0.691500 | 1.372 sec/iter\n",
      "Epoch: 340 | Batch: 004 / 011 | Total loss: 0.957 | Reg loss: 0.025 | Tree loss: 0.957 | Accuracy: 0.679000 | 1.372 sec/iter\n",
      "Epoch: 340 | Batch: 005 / 011 | Total loss: 0.954 | Reg loss: 0.025 | Tree loss: 0.954 | Accuracy: 0.665500 | 1.372 sec/iter\n",
      "Epoch: 340 | Batch: 006 / 011 | Total loss: 0.916 | Reg loss: 0.025 | Tree loss: 0.916 | Accuracy: 0.673500 | 1.373 sec/iter\n",
      "Epoch: 340 | Batch: 007 / 011 | Total loss: 0.931 | Reg loss: 0.025 | Tree loss: 0.931 | Accuracy: 0.674000 | 1.373 sec/iter\n",
      "Epoch: 340 | Batch: 008 / 011 | Total loss: 0.932 | Reg loss: 0.025 | Tree loss: 0.932 | Accuracy: 0.657500 | 1.373 sec/iter\n",
      "Epoch: 340 | Batch: 009 / 011 | Total loss: 0.943 | Reg loss: 0.025 | Tree loss: 0.943 | Accuracy: 0.650000 | 1.373 sec/iter\n",
      "Epoch: 340 | Batch: 010 / 011 | Total loss: 0.957 | Reg loss: 0.025 | Tree loss: 0.957 | Accuracy: 0.638225 | 1.373 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 341 | Batch: 000 / 011 | Total loss: 1.024 | Reg loss: 0.025 | Tree loss: 1.024 | Accuracy: 0.660500 | 1.374 sec/iter\n",
      "Epoch: 341 | Batch: 001 / 011 | Total loss: 0.987 | Reg loss: 0.025 | Tree loss: 0.987 | Accuracy: 0.671000 | 1.374 sec/iter\n",
      "Epoch: 341 | Batch: 002 / 011 | Total loss: 0.990 | Reg loss: 0.025 | Tree loss: 0.990 | Accuracy: 0.681000 | 1.374 sec/iter\n",
      "Epoch: 341 | Batch: 003 / 011 | Total loss: 0.949 | Reg loss: 0.025 | Tree loss: 0.949 | Accuracy: 0.699500 | 1.374 sec/iter\n",
      "Epoch: 341 | Batch: 004 / 011 | Total loss: 0.936 | Reg loss: 0.025 | Tree loss: 0.936 | Accuracy: 0.700000 | 1.374 sec/iter\n",
      "Epoch: 341 | Batch: 005 / 011 | Total loss: 0.953 | Reg loss: 0.025 | Tree loss: 0.953 | Accuracy: 0.659500 | 1.374 sec/iter\n",
      "Epoch: 341 | Batch: 006 / 011 | Total loss: 0.949 | Reg loss: 0.025 | Tree loss: 0.949 | Accuracy: 0.669500 | 1.374 sec/iter\n",
      "Epoch: 341 | Batch: 007 / 011 | Total loss: 0.946 | Reg loss: 0.025 | Tree loss: 0.946 | Accuracy: 0.653500 | 1.374 sec/iter\n",
      "Epoch: 341 | Batch: 008 / 011 | Total loss: 0.954 | Reg loss: 0.025 | Tree loss: 0.954 | Accuracy: 0.635500 | 1.375 sec/iter\n",
      "Epoch: 341 | Batch: 009 / 011 | Total loss: 0.934 | Reg loss: 0.025 | Tree loss: 0.934 | Accuracy: 0.658000 | 1.375 sec/iter\n",
      "Epoch: 341 | Batch: 010 / 011 | Total loss: 0.906 | Reg loss: 0.025 | Tree loss: 0.906 | Accuracy: 0.662116 | 1.375 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 342 | Batch: 000 / 011 | Total loss: 0.998 | Reg loss: 0.025 | Tree loss: 0.998 | Accuracy: 0.661500 | 1.375 sec/iter\n",
      "Epoch: 342 | Batch: 001 / 011 | Total loss: 1.010 | Reg loss: 0.025 | Tree loss: 1.010 | Accuracy: 0.648000 | 1.375 sec/iter\n",
      "Epoch: 342 | Batch: 002 / 011 | Total loss: 0.985 | Reg loss: 0.025 | Tree loss: 0.985 | Accuracy: 0.671000 | 1.375 sec/iter\n",
      "Epoch: 342 | Batch: 003 / 011 | Total loss: 0.979 | Reg loss: 0.025 | Tree loss: 0.979 | Accuracy: 0.681000 | 1.375 sec/iter\n",
      "Epoch: 342 | Batch: 004 / 011 | Total loss: 0.940 | Reg loss: 0.025 | Tree loss: 0.940 | Accuracy: 0.709000 | 1.376 sec/iter\n",
      "Epoch: 342 | Batch: 005 / 011 | Total loss: 0.957 | Reg loss: 0.025 | Tree loss: 0.957 | Accuracy: 0.675000 | 1.376 sec/iter\n",
      "Epoch: 342 | Batch: 006 / 011 | Total loss: 0.928 | Reg loss: 0.025 | Tree loss: 0.928 | Accuracy: 0.679500 | 1.376 sec/iter\n",
      "Epoch: 342 | Batch: 007 / 011 | Total loss: 0.936 | Reg loss: 0.025 | Tree loss: 0.936 | Accuracy: 0.645500 | 1.376 sec/iter\n",
      "Epoch: 342 | Batch: 008 / 011 | Total loss: 0.968 | Reg loss: 0.025 | Tree loss: 0.968 | Accuracy: 0.631000 | 1.376 sec/iter\n",
      "Epoch: 342 | Batch: 009 / 011 | Total loss: 0.928 | Reg loss: 0.025 | Tree loss: 0.928 | Accuracy: 0.645500 | 1.376 sec/iter\n",
      "Epoch: 342 | Batch: 010 / 011 | Total loss: 0.877 | Reg loss: 0.025 | Tree loss: 0.877 | Accuracy: 0.648464 | 1.376 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 343 | Batch: 000 / 011 | Total loss: 1.027 | Reg loss: 0.025 | Tree loss: 1.027 | Accuracy: 0.647000 | 1.377 sec/iter\n",
      "Epoch: 343 | Batch: 001 / 011 | Total loss: 1.017 | Reg loss: 0.025 | Tree loss: 1.017 | Accuracy: 0.659000 | 1.377 sec/iter\n",
      "Epoch: 343 | Batch: 002 / 011 | Total loss: 0.967 | Reg loss: 0.025 | Tree loss: 0.967 | Accuracy: 0.691500 | 1.377 sec/iter\n",
      "Epoch: 343 | Batch: 003 / 011 | Total loss: 0.953 | Reg loss: 0.025 | Tree loss: 0.953 | Accuracy: 0.697500 | 1.377 sec/iter\n",
      "Epoch: 343 | Batch: 004 / 011 | Total loss: 0.948 | Reg loss: 0.025 | Tree loss: 0.948 | Accuracy: 0.680000 | 1.377 sec/iter\n",
      "Epoch: 343 | Batch: 005 / 011 | Total loss: 0.931 | Reg loss: 0.025 | Tree loss: 0.931 | Accuracy: 0.680500 | 1.377 sec/iter\n",
      "Epoch: 343 | Batch: 006 / 011 | Total loss: 0.949 | Reg loss: 0.025 | Tree loss: 0.949 | Accuracy: 0.650500 | 1.378 sec/iter\n",
      "Epoch: 343 | Batch: 007 / 011 | Total loss: 0.922 | Reg loss: 0.025 | Tree loss: 0.922 | Accuracy: 0.664000 | 1.378 sec/iter\n",
      "Epoch: 343 | Batch: 008 / 011 | Total loss: 0.969 | Reg loss: 0.025 | Tree loss: 0.969 | Accuracy: 0.630000 | 1.378 sec/iter\n",
      "Epoch: 343 | Batch: 009 / 011 | Total loss: 0.922 | Reg loss: 0.025 | Tree loss: 0.922 | Accuracy: 0.660500 | 1.378 sec/iter\n",
      "Epoch: 343 | Batch: 010 / 011 | Total loss: 0.920 | Reg loss: 0.025 | Tree loss: 0.920 | Accuracy: 0.655290 | 1.378 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 344 | Batch: 000 / 011 | Total loss: 1.013 | Reg loss: 0.025 | Tree loss: 1.013 | Accuracy: 0.658000 | 1.378 sec/iter\n",
      "Epoch: 344 | Batch: 001 / 011 | Total loss: 1.025 | Reg loss: 0.025 | Tree loss: 1.025 | Accuracy: 0.652000 | 1.378 sec/iter\n",
      "Epoch: 344 | Batch: 002 / 011 | Total loss: 0.993 | Reg loss: 0.025 | Tree loss: 0.993 | Accuracy: 0.667500 | 1.378 sec/iter\n",
      "Epoch: 344 | Batch: 003 / 011 | Total loss: 0.945 | Reg loss: 0.025 | Tree loss: 0.945 | Accuracy: 0.696500 | 1.378 sec/iter\n",
      "Epoch: 344 | Batch: 004 / 011 | Total loss: 0.934 | Reg loss: 0.025 | Tree loss: 0.934 | Accuracy: 0.700500 | 1.378 sec/iter\n",
      "Epoch: 344 | Batch: 005 / 011 | Total loss: 0.949 | Reg loss: 0.025 | Tree loss: 0.949 | Accuracy: 0.684500 | 1.378 sec/iter\n",
      "Epoch: 344 | Batch: 006 / 011 | Total loss: 0.940 | Reg loss: 0.025 | Tree loss: 0.940 | Accuracy: 0.675000 | 1.378 sec/iter\n",
      "Epoch: 344 | Batch: 007 / 011 | Total loss: 0.937 | Reg loss: 0.025 | Tree loss: 0.937 | Accuracy: 0.645500 | 1.378 sec/iter\n",
      "Epoch: 344 | Batch: 008 / 011 | Total loss: 0.943 | Reg loss: 0.025 | Tree loss: 0.943 | Accuracy: 0.625000 | 1.378 sec/iter\n",
      "Epoch: 344 | Batch: 009 / 011 | Total loss: 0.931 | Reg loss: 0.025 | Tree loss: 0.931 | Accuracy: 0.654000 | 1.378 sec/iter\n",
      "Epoch: 344 | Batch: 010 / 011 | Total loss: 0.902 | Reg loss: 0.025 | Tree loss: 0.902 | Accuracy: 0.648464 | 1.378 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 345 | Batch: 000 / 011 | Total loss: 1.005 | Reg loss: 0.025 | Tree loss: 1.005 | Accuracy: 0.677000 | 1.379 sec/iter\n",
      "Epoch: 345 | Batch: 001 / 011 | Total loss: 1.011 | Reg loss: 0.025 | Tree loss: 1.011 | Accuracy: 0.666000 | 1.379 sec/iter\n",
      "Epoch: 345 | Batch: 002 / 011 | Total loss: 0.981 | Reg loss: 0.025 | Tree loss: 0.981 | Accuracy: 0.684500 | 1.379 sec/iter\n",
      "Epoch: 345 | Batch: 003 / 011 | Total loss: 0.969 | Reg loss: 0.025 | Tree loss: 0.969 | Accuracy: 0.682000 | 1.379 sec/iter\n",
      "Epoch: 345 | Batch: 004 / 011 | Total loss: 0.940 | Reg loss: 0.025 | Tree loss: 0.940 | Accuracy: 0.682500 | 1.379 sec/iter\n",
      "Epoch: 345 | Batch: 005 / 011 | Total loss: 0.926 | Reg loss: 0.025 | Tree loss: 0.926 | Accuracy: 0.674500 | 1.379 sec/iter\n",
      "Epoch: 345 | Batch: 006 / 011 | Total loss: 0.967 | Reg loss: 0.025 | Tree loss: 0.967 | Accuracy: 0.646500 | 1.379 sec/iter\n",
      "Epoch: 345 | Batch: 007 / 011 | Total loss: 0.929 | Reg loss: 0.025 | Tree loss: 0.929 | Accuracy: 0.665500 | 1.379 sec/iter\n",
      "Epoch: 345 | Batch: 008 / 011 | Total loss: 0.923 | Reg loss: 0.025 | Tree loss: 0.923 | Accuracy: 0.660000 | 1.379 sec/iter\n",
      "Epoch: 345 | Batch: 009 / 011 | Total loss: 0.957 | Reg loss: 0.025 | Tree loss: 0.957 | Accuracy: 0.654000 | 1.379 sec/iter\n",
      "Epoch: 345 | Batch: 010 / 011 | Total loss: 0.926 | Reg loss: 0.025 | Tree loss: 0.926 | Accuracy: 0.634812 | 1.379 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 346 | Batch: 000 / 011 | Total loss: 1.042 | Reg loss: 0.025 | Tree loss: 1.042 | Accuracy: 0.627000 | 1.38 sec/iter\n",
      "Epoch: 346 | Batch: 001 / 011 | Total loss: 1.011 | Reg loss: 0.025 | Tree loss: 1.011 | Accuracy: 0.662000 | 1.38 sec/iter\n",
      "Epoch: 346 | Batch: 002 / 011 | Total loss: 0.961 | Reg loss: 0.025 | Tree loss: 0.961 | Accuracy: 0.697500 | 1.38 sec/iter\n",
      "Epoch: 346 | Batch: 003 / 011 | Total loss: 0.968 | Reg loss: 0.025 | Tree loss: 0.968 | Accuracy: 0.675500 | 1.38 sec/iter\n",
      "Epoch: 346 | Batch: 004 / 011 | Total loss: 0.950 | Reg loss: 0.025 | Tree loss: 0.950 | Accuracy: 0.677000 | 1.381 sec/iter\n",
      "Epoch: 346 | Batch: 005 / 011 | Total loss: 0.927 | Reg loss: 0.025 | Tree loss: 0.927 | Accuracy: 0.693500 | 1.381 sec/iter\n",
      "Epoch: 346 | Batch: 006 / 011 | Total loss: 0.952 | Reg loss: 0.025 | Tree loss: 0.952 | Accuracy: 0.660500 | 1.381 sec/iter\n",
      "Epoch: 346 | Batch: 007 / 011 | Total loss: 0.958 | Reg loss: 0.025 | Tree loss: 0.958 | Accuracy: 0.652000 | 1.381 sec/iter\n",
      "Epoch: 346 | Batch: 008 / 011 | Total loss: 0.910 | Reg loss: 0.025 | Tree loss: 0.910 | Accuracy: 0.667000 | 1.381 sec/iter\n",
      "Epoch: 346 | Batch: 009 / 011 | Total loss: 0.936 | Reg loss: 0.025 | Tree loss: 0.936 | Accuracy: 0.658000 | 1.381 sec/iter\n",
      "Epoch: 346 | Batch: 010 / 011 | Total loss: 0.900 | Reg loss: 0.025 | Tree loss: 0.900 | Accuracy: 0.679181 | 1.381 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 347 | Batch: 000 / 011 | Total loss: 1.009 | Reg loss: 0.025 | Tree loss: 1.009 | Accuracy: 0.654500 | 1.382 sec/iter\n",
      "Epoch: 347 | Batch: 001 / 011 | Total loss: 1.017 | Reg loss: 0.025 | Tree loss: 1.017 | Accuracy: 0.665000 | 1.382 sec/iter\n",
      "Epoch: 347 | Batch: 002 / 011 | Total loss: 0.979 | Reg loss: 0.025 | Tree loss: 0.979 | Accuracy: 0.686500 | 1.382 sec/iter\n",
      "Epoch: 347 | Batch: 003 / 011 | Total loss: 0.974 | Reg loss: 0.025 | Tree loss: 0.974 | Accuracy: 0.695000 | 1.382 sec/iter\n",
      "Epoch: 347 | Batch: 004 / 011 | Total loss: 0.942 | Reg loss: 0.025 | Tree loss: 0.942 | Accuracy: 0.677000 | 1.382 sec/iter\n",
      "Epoch: 347 | Batch: 005 / 011 | Total loss: 0.937 | Reg loss: 0.025 | Tree loss: 0.937 | Accuracy: 0.680000 | 1.382 sec/iter\n",
      "Epoch: 347 | Batch: 006 / 011 | Total loss: 0.956 | Reg loss: 0.025 | Tree loss: 0.956 | Accuracy: 0.653500 | 1.382 sec/iter\n",
      "Epoch: 347 | Batch: 007 / 011 | Total loss: 0.940 | Reg loss: 0.025 | Tree loss: 0.940 | Accuracy: 0.656500 | 1.382 sec/iter\n",
      "Epoch: 347 | Batch: 008 / 011 | Total loss: 0.919 | Reg loss: 0.025 | Tree loss: 0.919 | Accuracy: 0.662500 | 1.382 sec/iter\n",
      "Epoch: 347 | Batch: 009 / 011 | Total loss: 0.934 | Reg loss: 0.025 | Tree loss: 0.934 | Accuracy: 0.651000 | 1.382 sec/iter\n",
      "Epoch: 347 | Batch: 010 / 011 | Total loss: 0.894 | Reg loss: 0.025 | Tree loss: 0.894 | Accuracy: 0.679181 | 1.382 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 348 | Batch: 000 / 011 | Total loss: 1.039 | Reg loss: 0.025 | Tree loss: 1.039 | Accuracy: 0.642500 | 1.382 sec/iter\n",
      "Epoch: 348 | Batch: 001 / 011 | Total loss: 1.006 | Reg loss: 0.025 | Tree loss: 1.006 | Accuracy: 0.668500 | 1.382 sec/iter\n",
      "Epoch: 348 | Batch: 002 / 011 | Total loss: 0.976 | Reg loss: 0.025 | Tree loss: 0.976 | Accuracy: 0.681500 | 1.382 sec/iter\n",
      "Epoch: 348 | Batch: 003 / 011 | Total loss: 0.978 | Reg loss: 0.025 | Tree loss: 0.978 | Accuracy: 0.674500 | 1.382 sec/iter\n",
      "Epoch: 348 | Batch: 004 / 011 | Total loss: 0.956 | Reg loss: 0.025 | Tree loss: 0.956 | Accuracy: 0.673000 | 1.382 sec/iter\n",
      "Epoch: 348 | Batch: 005 / 011 | Total loss: 0.921 | Reg loss: 0.025 | Tree loss: 0.921 | Accuracy: 0.691000 | 1.382 sec/iter\n",
      "Epoch: 348 | Batch: 006 / 011 | Total loss: 0.901 | Reg loss: 0.025 | Tree loss: 0.901 | Accuracy: 0.693000 | 1.382 sec/iter\n",
      "Epoch: 348 | Batch: 007 / 011 | Total loss: 0.926 | Reg loss: 0.025 | Tree loss: 0.926 | Accuracy: 0.683500 | 1.382 sec/iter\n",
      "Epoch: 348 | Batch: 008 / 011 | Total loss: 0.930 | Reg loss: 0.025 | Tree loss: 0.930 | Accuracy: 0.660000 | 1.382 sec/iter\n",
      "Epoch: 348 | Batch: 009 / 011 | Total loss: 0.938 | Reg loss: 0.025 | Tree loss: 0.938 | Accuracy: 0.659500 | 1.382 sec/iter\n",
      "Epoch: 348 | Batch: 010 / 011 | Total loss: 0.990 | Reg loss: 0.025 | Tree loss: 0.990 | Accuracy: 0.641638 | 1.382 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 349 | Batch: 000 / 011 | Total loss: 1.023 | Reg loss: 0.025 | Tree loss: 1.023 | Accuracy: 0.661000 | 1.383 sec/iter\n",
      "Epoch: 349 | Batch: 001 / 011 | Total loss: 1.001 | Reg loss: 0.025 | Tree loss: 1.001 | Accuracy: 0.671500 | 1.383 sec/iter\n",
      "Epoch: 349 | Batch: 002 / 011 | Total loss: 1.009 | Reg loss: 0.025 | Tree loss: 1.009 | Accuracy: 0.664500 | 1.383 sec/iter\n",
      "Epoch: 349 | Batch: 003 / 011 | Total loss: 0.960 | Reg loss: 0.025 | Tree loss: 0.960 | Accuracy: 0.691000 | 1.383 sec/iter\n",
      "Epoch: 349 | Batch: 004 / 011 | Total loss: 0.936 | Reg loss: 0.025 | Tree loss: 0.936 | Accuracy: 0.697500 | 1.383 sec/iter\n",
      "Epoch: 349 | Batch: 005 / 011 | Total loss: 0.943 | Reg loss: 0.025 | Tree loss: 0.943 | Accuracy: 0.687500 | 1.383 sec/iter\n",
      "Epoch: 349 | Batch: 006 / 011 | Total loss: 0.949 | Reg loss: 0.025 | Tree loss: 0.949 | Accuracy: 0.667500 | 1.383 sec/iter\n",
      "Epoch: 349 | Batch: 007 / 011 | Total loss: 0.932 | Reg loss: 0.025 | Tree loss: 0.932 | Accuracy: 0.657500 | 1.383 sec/iter\n",
      "Epoch: 349 | Batch: 008 / 011 | Total loss: 0.929 | Reg loss: 0.025 | Tree loss: 0.929 | Accuracy: 0.655500 | 1.383 sec/iter\n",
      "Epoch: 349 | Batch: 009 / 011 | Total loss: 0.920 | Reg loss: 0.025 | Tree loss: 0.920 | Accuracy: 0.661500 | 1.383 sec/iter\n",
      "Epoch: 349 | Batch: 010 / 011 | Total loss: 0.890 | Reg loss: 0.025 | Tree loss: 0.890 | Accuracy: 0.658703 | 1.383 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 350 | Batch: 000 / 011 | Total loss: 1.012 | Reg loss: 0.025 | Tree loss: 1.012 | Accuracy: 0.669500 | 1.384 sec/iter\n",
      "Epoch: 350 | Batch: 001 / 011 | Total loss: 1.027 | Reg loss: 0.025 | Tree loss: 1.027 | Accuracy: 0.660500 | 1.384 sec/iter\n",
      "Epoch: 350 | Batch: 002 / 011 | Total loss: 0.980 | Reg loss: 0.025 | Tree loss: 0.980 | Accuracy: 0.685500 | 1.384 sec/iter\n",
      "Epoch: 350 | Batch: 003 / 011 | Total loss: 0.958 | Reg loss: 0.025 | Tree loss: 0.958 | Accuracy: 0.684000 | 1.384 sec/iter\n",
      "Epoch: 350 | Batch: 004 / 011 | Total loss: 0.926 | Reg loss: 0.025 | Tree loss: 0.926 | Accuracy: 0.707000 | 1.384 sec/iter\n",
      "Epoch: 350 | Batch: 005 / 011 | Total loss: 0.947 | Reg loss: 0.025 | Tree loss: 0.947 | Accuracy: 0.668500 | 1.384 sec/iter\n",
      "Epoch: 350 | Batch: 006 / 011 | Total loss: 0.933 | Reg loss: 0.025 | Tree loss: 0.933 | Accuracy: 0.674000 | 1.384 sec/iter\n",
      "Epoch: 350 | Batch: 007 / 011 | Total loss: 0.967 | Reg loss: 0.025 | Tree loss: 0.967 | Accuracy: 0.641000 | 1.384 sec/iter\n",
      "Epoch: 350 | Batch: 008 / 011 | Total loss: 0.935 | Reg loss: 0.025 | Tree loss: 0.935 | Accuracy: 0.646500 | 1.384 sec/iter\n",
      "Epoch: 350 | Batch: 009 / 011 | Total loss: 0.902 | Reg loss: 0.025 | Tree loss: 0.902 | Accuracy: 0.672500 | 1.384 sec/iter\n",
      "Epoch: 350 | Batch: 010 / 011 | Total loss: 0.984 | Reg loss: 0.025 | Tree loss: 0.984 | Accuracy: 0.631399 | 1.384 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 351 | Batch: 000 / 011 | Total loss: 1.006 | Reg loss: 0.025 | Tree loss: 1.006 | Accuracy: 0.665000 | 1.385 sec/iter\n",
      "Epoch: 351 | Batch: 001 / 011 | Total loss: 1.019 | Reg loss: 0.025 | Tree loss: 1.019 | Accuracy: 0.656500 | 1.385 sec/iter\n",
      "Epoch: 351 | Batch: 002 / 011 | Total loss: 0.982 | Reg loss: 0.025 | Tree loss: 0.982 | Accuracy: 0.685000 | 1.385 sec/iter\n",
      "Epoch: 351 | Batch: 003 / 011 | Total loss: 0.981 | Reg loss: 0.025 | Tree loss: 0.981 | Accuracy: 0.680500 | 1.385 sec/iter\n",
      "Epoch: 351 | Batch: 004 / 011 | Total loss: 0.962 | Reg loss: 0.025 | Tree loss: 0.962 | Accuracy: 0.671000 | 1.385 sec/iter\n",
      "Epoch: 351 | Batch: 005 / 011 | Total loss: 0.944 | Reg loss: 0.025 | Tree loss: 0.944 | Accuracy: 0.672500 | 1.385 sec/iter\n",
      "Epoch: 351 | Batch: 006 / 011 | Total loss: 0.935 | Reg loss: 0.025 | Tree loss: 0.935 | Accuracy: 0.674500 | 1.385 sec/iter\n",
      "Epoch: 351 | Batch: 007 / 011 | Total loss: 0.938 | Reg loss: 0.025 | Tree loss: 0.938 | Accuracy: 0.664000 | 1.385 sec/iter\n",
      "Epoch: 351 | Batch: 008 / 011 | Total loss: 0.917 | Reg loss: 0.025 | Tree loss: 0.917 | Accuracy: 0.663500 | 1.385 sec/iter\n",
      "Epoch: 351 | Batch: 009 / 011 | Total loss: 0.919 | Reg loss: 0.025 | Tree loss: 0.919 | Accuracy: 0.654500 | 1.385 sec/iter\n",
      "Epoch: 351 | Batch: 010 / 011 | Total loss: 0.903 | Reg loss: 0.025 | Tree loss: 0.903 | Accuracy: 0.658703 | 1.385 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 352 | Batch: 000 / 011 | Total loss: 1.024 | Reg loss: 0.025 | Tree loss: 1.024 | Accuracy: 0.659500 | 1.385 sec/iter\n",
      "Epoch: 352 | Batch: 001 / 011 | Total loss: 1.003 | Reg loss: 0.025 | Tree loss: 1.003 | Accuracy: 0.667500 | 1.385 sec/iter\n",
      "Epoch: 352 | Batch: 002 / 011 | Total loss: 0.977 | Reg loss: 0.025 | Tree loss: 0.977 | Accuracy: 0.694500 | 1.385 sec/iter\n",
      "Epoch: 352 | Batch: 003 / 011 | Total loss: 0.965 | Reg loss: 0.025 | Tree loss: 0.965 | Accuracy: 0.679000 | 1.385 sec/iter\n",
      "Epoch: 352 | Batch: 004 / 011 | Total loss: 0.950 | Reg loss: 0.025 | Tree loss: 0.950 | Accuracy: 0.681000 | 1.385 sec/iter\n",
      "Epoch: 352 | Batch: 005 / 011 | Total loss: 0.950 | Reg loss: 0.025 | Tree loss: 0.950 | Accuracy: 0.658500 | 1.386 sec/iter\n",
      "Epoch: 352 | Batch: 006 / 011 | Total loss: 0.948 | Reg loss: 0.025 | Tree loss: 0.948 | Accuracy: 0.662500 | 1.386 sec/iter\n",
      "Epoch: 352 | Batch: 007 / 011 | Total loss: 0.952 | Reg loss: 0.025 | Tree loss: 0.952 | Accuracy: 0.649500 | 1.386 sec/iter\n",
      "Epoch: 352 | Batch: 008 / 011 | Total loss: 0.908 | Reg loss: 0.025 | Tree loss: 0.908 | Accuracy: 0.670000 | 1.386 sec/iter\n",
      "Epoch: 352 | Batch: 009 / 011 | Total loss: 0.909 | Reg loss: 0.025 | Tree loss: 0.909 | Accuracy: 0.668000 | 1.386 sec/iter\n",
      "Epoch: 352 | Batch: 010 / 011 | Total loss: 0.953 | Reg loss: 0.025 | Tree loss: 0.953 | Accuracy: 0.655290 | 1.386 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 353 | Batch: 000 / 011 | Total loss: 0.995 | Reg loss: 0.025 | Tree loss: 0.995 | Accuracy: 0.669000 | 1.386 sec/iter\n",
      "Epoch: 353 | Batch: 001 / 011 | Total loss: 0.991 | Reg loss: 0.025 | Tree loss: 0.991 | Accuracy: 0.667500 | 1.386 sec/iter\n",
      "Epoch: 353 | Batch: 002 / 011 | Total loss: 0.993 | Reg loss: 0.025 | Tree loss: 0.993 | Accuracy: 0.664000 | 1.386 sec/iter\n",
      "Epoch: 353 | Batch: 003 / 011 | Total loss: 0.976 | Reg loss: 0.025 | Tree loss: 0.976 | Accuracy: 0.676500 | 1.386 sec/iter\n",
      "Epoch: 353 | Batch: 004 / 011 | Total loss: 0.972 | Reg loss: 0.025 | Tree loss: 0.972 | Accuracy: 0.663000 | 1.386 sec/iter\n",
      "Epoch: 353 | Batch: 005 / 011 | Total loss: 0.957 | Reg loss: 0.025 | Tree loss: 0.957 | Accuracy: 0.664500 | 1.387 sec/iter\n",
      "Epoch: 353 | Batch: 006 / 011 | Total loss: 0.927 | Reg loss: 0.025 | Tree loss: 0.927 | Accuracy: 0.664000 | 1.387 sec/iter\n",
      "Epoch: 353 | Batch: 007 / 011 | Total loss: 0.924 | Reg loss: 0.025 | Tree loss: 0.924 | Accuracy: 0.665000 | 1.387 sec/iter\n",
      "Epoch: 353 | Batch: 008 / 011 | Total loss: 0.959 | Reg loss: 0.025 | Tree loss: 0.959 | Accuracy: 0.642000 | 1.387 sec/iter\n",
      "Epoch: 353 | Batch: 009 / 011 | Total loss: 0.904 | Reg loss: 0.025 | Tree loss: 0.904 | Accuracy: 0.673500 | 1.387 sec/iter\n",
      "Epoch: 353 | Batch: 010 / 011 | Total loss: 0.866 | Reg loss: 0.025 | Tree loss: 0.866 | Accuracy: 0.689420 | 1.387 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 354 | Batch: 000 / 011 | Total loss: 1.025 | Reg loss: 0.025 | Tree loss: 1.025 | Accuracy: 0.660000 | 1.387 sec/iter\n",
      "Epoch: 354 | Batch: 001 / 011 | Total loss: 1.002 | Reg loss: 0.025 | Tree loss: 1.002 | Accuracy: 0.656000 | 1.388 sec/iter\n",
      "Epoch: 354 | Batch: 002 / 011 | Total loss: 0.996 | Reg loss: 0.025 | Tree loss: 0.996 | Accuracy: 0.671500 | 1.388 sec/iter\n",
      "Epoch: 354 | Batch: 003 / 011 | Total loss: 0.960 | Reg loss: 0.025 | Tree loss: 0.960 | Accuracy: 0.684500 | 1.388 sec/iter\n",
      "Epoch: 354 | Batch: 004 / 011 | Total loss: 0.948 | Reg loss: 0.025 | Tree loss: 0.948 | Accuracy: 0.688000 | 1.388 sec/iter\n",
      "Epoch: 354 | Batch: 005 / 011 | Total loss: 0.934 | Reg loss: 0.025 | Tree loss: 0.934 | Accuracy: 0.684000 | 1.388 sec/iter\n",
      "Epoch: 354 | Batch: 006 / 011 | Total loss: 0.958 | Reg loss: 0.025 | Tree loss: 0.958 | Accuracy: 0.656000 | 1.388 sec/iter\n",
      "Epoch: 354 | Batch: 007 / 011 | Total loss: 0.919 | Reg loss: 0.025 | Tree loss: 0.919 | Accuracy: 0.663000 | 1.388 sec/iter\n",
      "Epoch: 354 | Batch: 008 / 011 | Total loss: 0.908 | Reg loss: 0.025 | Tree loss: 0.908 | Accuracy: 0.670500 | 1.388 sec/iter\n",
      "Epoch: 354 | Batch: 009 / 011 | Total loss: 0.938 | Reg loss: 0.025 | Tree loss: 0.938 | Accuracy: 0.654000 | 1.388 sec/iter\n",
      "Epoch: 354 | Batch: 010 / 011 | Total loss: 0.925 | Reg loss: 0.025 | Tree loss: 0.925 | Accuracy: 0.648464 | 1.388 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 355 | Batch: 000 / 011 | Total loss: 1.054 | Reg loss: 0.025 | Tree loss: 1.054 | Accuracy: 0.645000 | 1.388 sec/iter\n",
      "Epoch: 355 | Batch: 001 / 011 | Total loss: 1.010 | Reg loss: 0.025 | Tree loss: 1.010 | Accuracy: 0.663000 | 1.388 sec/iter\n",
      "Epoch: 355 | Batch: 002 / 011 | Total loss: 0.975 | Reg loss: 0.025 | Tree loss: 0.975 | Accuracy: 0.677000 | 1.388 sec/iter\n",
      "Epoch: 355 | Batch: 003 / 011 | Total loss: 0.958 | Reg loss: 0.025 | Tree loss: 0.958 | Accuracy: 0.694500 | 1.388 sec/iter\n",
      "Epoch: 355 | Batch: 004 / 011 | Total loss: 0.931 | Reg loss: 0.025 | Tree loss: 0.931 | Accuracy: 0.697000 | 1.388 sec/iter\n",
      "Epoch: 355 | Batch: 005 / 011 | Total loss: 0.930 | Reg loss: 0.025 | Tree loss: 0.930 | Accuracy: 0.679500 | 1.388 sec/iter\n",
      "Epoch: 355 | Batch: 006 / 011 | Total loss: 0.932 | Reg loss: 0.025 | Tree loss: 0.932 | Accuracy: 0.653500 | 1.389 sec/iter\n",
      "Epoch: 355 | Batch: 007 / 011 | Total loss: 0.950 | Reg loss: 0.025 | Tree loss: 0.950 | Accuracy: 0.643500 | 1.389 sec/iter\n",
      "Epoch: 355 | Batch: 008 / 011 | Total loss: 0.930 | Reg loss: 0.025 | Tree loss: 0.930 | Accuracy: 0.657000 | 1.389 sec/iter\n",
      "Epoch: 355 | Batch: 009 / 011 | Total loss: 0.916 | Reg loss: 0.025 | Tree loss: 0.916 | Accuracy: 0.676000 | 1.389 sec/iter\n",
      "Epoch: 355 | Batch: 010 / 011 | Total loss: 0.879 | Reg loss: 0.025 | Tree loss: 0.879 | Accuracy: 0.692833 | 1.389 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 356 | Batch: 000 / 011 | Total loss: 1.026 | Reg loss: 0.025 | Tree loss: 1.026 | Accuracy: 0.660500 | 1.389 sec/iter\n",
      "Epoch: 356 | Batch: 001 / 011 | Total loss: 0.993 | Reg loss: 0.025 | Tree loss: 0.993 | Accuracy: 0.673500 | 1.389 sec/iter\n",
      "Epoch: 356 | Batch: 002 / 011 | Total loss: 0.977 | Reg loss: 0.025 | Tree loss: 0.977 | Accuracy: 0.683000 | 1.389 sec/iter\n",
      "Epoch: 356 | Batch: 003 / 011 | Total loss: 0.976 | Reg loss: 0.025 | Tree loss: 0.976 | Accuracy: 0.682500 | 1.389 sec/iter\n",
      "Epoch: 356 | Batch: 004 / 011 | Total loss: 0.939 | Reg loss: 0.025 | Tree loss: 0.939 | Accuracy: 0.694500 | 1.389 sec/iter\n",
      "Epoch: 356 | Batch: 005 / 011 | Total loss: 0.938 | Reg loss: 0.025 | Tree loss: 0.938 | Accuracy: 0.668500 | 1.389 sec/iter\n",
      "Epoch: 356 | Batch: 006 / 011 | Total loss: 0.951 | Reg loss: 0.025 | Tree loss: 0.951 | Accuracy: 0.660000 | 1.389 sec/iter\n",
      "Epoch: 356 | Batch: 007 / 011 | Total loss: 0.948 | Reg loss: 0.025 | Tree loss: 0.948 | Accuracy: 0.641500 | 1.389 sec/iter\n",
      "Epoch: 356 | Batch: 008 / 011 | Total loss: 0.909 | Reg loss: 0.025 | Tree loss: 0.909 | Accuracy: 0.670500 | 1.389 sec/iter\n",
      "Epoch: 356 | Batch: 009 / 011 | Total loss: 0.928 | Reg loss: 0.025 | Tree loss: 0.928 | Accuracy: 0.652500 | 1.389 sec/iter\n",
      "Epoch: 356 | Batch: 010 / 011 | Total loss: 0.910 | Reg loss: 0.025 | Tree loss: 0.910 | Accuracy: 0.645051 | 1.389 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 357 | Batch: 000 / 011 | Total loss: 1.013 | Reg loss: 0.025 | Tree loss: 1.013 | Accuracy: 0.662500 | 1.39 sec/iter\n",
      "Epoch: 357 | Batch: 001 / 011 | Total loss: 0.978 | Reg loss: 0.025 | Tree loss: 0.978 | Accuracy: 0.685500 | 1.39 sec/iter\n",
      "Epoch: 357 | Batch: 002 / 011 | Total loss: 0.991 | Reg loss: 0.025 | Tree loss: 0.991 | Accuracy: 0.665500 | 1.39 sec/iter\n",
      "Epoch: 357 | Batch: 003 / 011 | Total loss: 0.982 | Reg loss: 0.025 | Tree loss: 0.982 | Accuracy: 0.676500 | 1.39 sec/iter\n",
      "Epoch: 357 | Batch: 004 / 011 | Total loss: 0.966 | Reg loss: 0.025 | Tree loss: 0.966 | Accuracy: 0.685000 | 1.39 sec/iter\n",
      "Epoch: 357 | Batch: 005 / 011 | Total loss: 0.945 | Reg loss: 0.025 | Tree loss: 0.945 | Accuracy: 0.677000 | 1.39 sec/iter\n",
      "Epoch: 357 | Batch: 006 / 011 | Total loss: 0.925 | Reg loss: 0.025 | Tree loss: 0.925 | Accuracy: 0.671000 | 1.39 sec/iter\n",
      "Epoch: 357 | Batch: 007 / 011 | Total loss: 0.922 | Reg loss: 0.025 | Tree loss: 0.922 | Accuracy: 0.668000 | 1.39 sec/iter\n",
      "Epoch: 357 | Batch: 008 / 011 | Total loss: 0.921 | Reg loss: 0.025 | Tree loss: 0.921 | Accuracy: 0.677000 | 1.39 sec/iter\n",
      "Epoch: 357 | Batch: 009 / 011 | Total loss: 0.918 | Reg loss: 0.025 | Tree loss: 0.918 | Accuracy: 0.656500 | 1.39 sec/iter\n",
      "Epoch: 357 | Batch: 010 / 011 | Total loss: 0.969 | Reg loss: 0.025 | Tree loss: 0.969 | Accuracy: 0.634812 | 1.39 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 358 | Batch: 000 / 011 | Total loss: 1.010 | Reg loss: 0.025 | Tree loss: 1.010 | Accuracy: 0.663000 | 1.391 sec/iter\n",
      "Epoch: 358 | Batch: 001 / 011 | Total loss: 0.987 | Reg loss: 0.025 | Tree loss: 0.987 | Accuracy: 0.668500 | 1.391 sec/iter\n",
      "Epoch: 358 | Batch: 002 / 011 | Total loss: 0.982 | Reg loss: 0.025 | Tree loss: 0.982 | Accuracy: 0.685500 | 1.391 sec/iter\n",
      "Epoch: 358 | Batch: 003 / 011 | Total loss: 0.966 | Reg loss: 0.025 | Tree loss: 0.966 | Accuracy: 0.695500 | 1.391 sec/iter\n",
      "Epoch: 358 | Batch: 004 / 011 | Total loss: 0.952 | Reg loss: 0.025 | Tree loss: 0.952 | Accuracy: 0.674500 | 1.391 sec/iter\n",
      "Epoch: 358 | Batch: 005 / 011 | Total loss: 0.946 | Reg loss: 0.025 | Tree loss: 0.946 | Accuracy: 0.668000 | 1.391 sec/iter\n",
      "Epoch: 358 | Batch: 006 / 011 | Total loss: 0.927 | Reg loss: 0.025 | Tree loss: 0.927 | Accuracy: 0.666500 | 1.391 sec/iter\n",
      "Epoch: 358 | Batch: 007 / 011 | Total loss: 0.959 | Reg loss: 0.025 | Tree loss: 0.959 | Accuracy: 0.654000 | 1.391 sec/iter\n",
      "Epoch: 358 | Batch: 008 / 011 | Total loss: 0.938 | Reg loss: 0.025 | Tree loss: 0.938 | Accuracy: 0.654000 | 1.391 sec/iter\n",
      "Epoch: 358 | Batch: 009 / 011 | Total loss: 0.939 | Reg loss: 0.025 | Tree loss: 0.939 | Accuracy: 0.653500 | 1.391 sec/iter\n",
      "Epoch: 358 | Batch: 010 / 011 | Total loss: 0.812 | Reg loss: 0.025 | Tree loss: 0.812 | Accuracy: 0.733788 | 1.391 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 359 | Batch: 000 / 011 | Total loss: 1.002 | Reg loss: 0.025 | Tree loss: 1.002 | Accuracy: 0.670000 | 1.392 sec/iter\n",
      "Epoch: 359 | Batch: 001 / 011 | Total loss: 1.014 | Reg loss: 0.025 | Tree loss: 1.014 | Accuracy: 0.669000 | 1.392 sec/iter\n",
      "Epoch: 359 | Batch: 002 / 011 | Total loss: 0.971 | Reg loss: 0.025 | Tree loss: 0.971 | Accuracy: 0.676000 | 1.392 sec/iter\n",
      "Epoch: 359 | Batch: 003 / 011 | Total loss: 0.971 | Reg loss: 0.025 | Tree loss: 0.971 | Accuracy: 0.681500 | 1.392 sec/iter\n",
      "Epoch: 359 | Batch: 004 / 011 | Total loss: 0.929 | Reg loss: 0.025 | Tree loss: 0.929 | Accuracy: 0.692500 | 1.392 sec/iter\n",
      "Epoch: 359 | Batch: 005 / 011 | Total loss: 0.948 | Reg loss: 0.025 | Tree loss: 0.948 | Accuracy: 0.676000 | 1.392 sec/iter\n",
      "Epoch: 359 | Batch: 006 / 011 | Total loss: 0.949 | Reg loss: 0.025 | Tree loss: 0.949 | Accuracy: 0.653000 | 1.392 sec/iter\n",
      "Epoch: 359 | Batch: 007 / 011 | Total loss: 0.928 | Reg loss: 0.025 | Tree loss: 0.928 | Accuracy: 0.678000 | 1.392 sec/iter\n",
      "Epoch: 359 | Batch: 008 / 011 | Total loss: 0.928 | Reg loss: 0.025 | Tree loss: 0.928 | Accuracy: 0.652500 | 1.392 sec/iter\n",
      "Epoch: 359 | Batch: 009 / 011 | Total loss: 0.927 | Reg loss: 0.025 | Tree loss: 0.927 | Accuracy: 0.645000 | 1.392 sec/iter\n",
      "Epoch: 359 | Batch: 010 / 011 | Total loss: 0.932 | Reg loss: 0.025 | Tree loss: 0.932 | Accuracy: 0.668942 | 1.392 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 360 | Batch: 000 / 011 | Total loss: 1.020 | Reg loss: 0.025 | Tree loss: 1.020 | Accuracy: 0.661500 | 1.392 sec/iter\n",
      "Epoch: 360 | Batch: 001 / 011 | Total loss: 1.041 | Reg loss: 0.025 | Tree loss: 1.041 | Accuracy: 0.628500 | 1.392 sec/iter\n",
      "Epoch: 360 | Batch: 002 / 011 | Total loss: 0.999 | Reg loss: 0.025 | Tree loss: 0.999 | Accuracy: 0.673500 | 1.393 sec/iter\n",
      "Epoch: 360 | Batch: 003 / 011 | Total loss: 0.952 | Reg loss: 0.025 | Tree loss: 0.952 | Accuracy: 0.690500 | 1.393 sec/iter\n",
      "Epoch: 360 | Batch: 004 / 011 | Total loss: 0.934 | Reg loss: 0.025 | Tree loss: 0.934 | Accuracy: 0.700000 | 1.393 sec/iter\n",
      "Epoch: 360 | Batch: 005 / 011 | Total loss: 0.951 | Reg loss: 0.025 | Tree loss: 0.951 | Accuracy: 0.672000 | 1.393 sec/iter\n",
      "Epoch: 360 | Batch: 006 / 011 | Total loss: 0.936 | Reg loss: 0.025 | Tree loss: 0.936 | Accuracy: 0.669500 | 1.393 sec/iter\n",
      "Epoch: 360 | Batch: 007 / 011 | Total loss: 0.903 | Reg loss: 0.025 | Tree loss: 0.903 | Accuracy: 0.666500 | 1.393 sec/iter\n",
      "Epoch: 360 | Batch: 008 / 011 | Total loss: 0.931 | Reg loss: 0.025 | Tree loss: 0.931 | Accuracy: 0.645000 | 1.393 sec/iter\n",
      "Epoch: 360 | Batch: 009 / 011 | Total loss: 0.940 | Reg loss: 0.025 | Tree loss: 0.940 | Accuracy: 0.651500 | 1.393 sec/iter\n",
      "Epoch: 360 | Batch: 010 / 011 | Total loss: 0.942 | Reg loss: 0.025 | Tree loss: 0.942 | Accuracy: 0.627986 | 1.393 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 361 | Batch: 000 / 011 | Total loss: 1.006 | Reg loss: 0.025 | Tree loss: 1.006 | Accuracy: 0.670000 | 1.394 sec/iter\n",
      "Epoch: 361 | Batch: 001 / 011 | Total loss: 0.992 | Reg loss: 0.025 | Tree loss: 0.992 | Accuracy: 0.665000 | 1.394 sec/iter\n",
      "Epoch: 361 | Batch: 002 / 011 | Total loss: 0.992 | Reg loss: 0.025 | Tree loss: 0.992 | Accuracy: 0.670500 | 1.394 sec/iter\n",
      "Epoch: 361 | Batch: 003 / 011 | Total loss: 0.979 | Reg loss: 0.025 | Tree loss: 0.979 | Accuracy: 0.685500 | 1.394 sec/iter\n",
      "Epoch: 361 | Batch: 004 / 011 | Total loss: 0.951 | Reg loss: 0.025 | Tree loss: 0.951 | Accuracy: 0.694500 | 1.394 sec/iter\n",
      "Epoch: 361 | Batch: 005 / 011 | Total loss: 0.940 | Reg loss: 0.025 | Tree loss: 0.940 | Accuracy: 0.695500 | 1.394 sec/iter\n",
      "Epoch: 361 | Batch: 006 / 011 | Total loss: 0.935 | Reg loss: 0.025 | Tree loss: 0.935 | Accuracy: 0.667000 | 1.394 sec/iter\n",
      "Epoch: 361 | Batch: 007 / 011 | Total loss: 0.965 | Reg loss: 0.025 | Tree loss: 0.965 | Accuracy: 0.640500 | 1.394 sec/iter\n",
      "Epoch: 361 | Batch: 008 / 011 | Total loss: 0.910 | Reg loss: 0.025 | Tree loss: 0.910 | Accuracy: 0.642500 | 1.394 sec/iter\n",
      "Epoch: 361 | Batch: 009 / 011 | Total loss: 0.918 | Reg loss: 0.025 | Tree loss: 0.918 | Accuracy: 0.656000 | 1.394 sec/iter\n",
      "Epoch: 361 | Batch: 010 / 011 | Total loss: 0.860 | Reg loss: 0.025 | Tree loss: 0.860 | Accuracy: 0.668942 | 1.394 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 362 | Batch: 000 / 011 | Total loss: 0.983 | Reg loss: 0.025 | Tree loss: 0.983 | Accuracy: 0.691000 | 1.395 sec/iter\n",
      "Epoch: 362 | Batch: 001 / 011 | Total loss: 1.004 | Reg loss: 0.025 | Tree loss: 1.004 | Accuracy: 0.679500 | 1.395 sec/iter\n",
      "Epoch: 362 | Batch: 002 / 011 | Total loss: 0.967 | Reg loss: 0.025 | Tree loss: 0.967 | Accuracy: 0.695500 | 1.395 sec/iter\n",
      "Epoch: 362 | Batch: 003 / 011 | Total loss: 0.957 | Reg loss: 0.025 | Tree loss: 0.957 | Accuracy: 0.678000 | 1.395 sec/iter\n",
      "Epoch: 362 | Batch: 004 / 011 | Total loss: 0.960 | Reg loss: 0.025 | Tree loss: 0.960 | Accuracy: 0.681500 | 1.395 sec/iter\n",
      "Epoch: 362 | Batch: 005 / 011 | Total loss: 0.949 | Reg loss: 0.025 | Tree loss: 0.949 | Accuracy: 0.677000 | 1.395 sec/iter\n",
      "Epoch: 362 | Batch: 006 / 011 | Total loss: 0.957 | Reg loss: 0.025 | Tree loss: 0.957 | Accuracy: 0.648000 | 1.395 sec/iter\n",
      "Epoch: 362 | Batch: 007 / 011 | Total loss: 0.934 | Reg loss: 0.025 | Tree loss: 0.934 | Accuracy: 0.662500 | 1.395 sec/iter\n",
      "Epoch: 362 | Batch: 008 / 011 | Total loss: 0.929 | Reg loss: 0.025 | Tree loss: 0.929 | Accuracy: 0.661500 | 1.395 sec/iter\n",
      "Epoch: 362 | Batch: 009 / 011 | Total loss: 0.928 | Reg loss: 0.025 | Tree loss: 0.928 | Accuracy: 0.651000 | 1.395 sec/iter\n",
      "Epoch: 362 | Batch: 010 / 011 | Total loss: 0.882 | Reg loss: 0.025 | Tree loss: 0.882 | Accuracy: 0.672355 | 1.395 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 363 | Batch: 000 / 011 | Total loss: 1.013 | Reg loss: 0.025 | Tree loss: 1.013 | Accuracy: 0.656500 | 1.396 sec/iter\n",
      "Epoch: 363 | Batch: 001 / 011 | Total loss: 0.996 | Reg loss: 0.025 | Tree loss: 0.996 | Accuracy: 0.678000 | 1.396 sec/iter\n",
      "Epoch: 363 | Batch: 002 / 011 | Total loss: 0.975 | Reg loss: 0.025 | Tree loss: 0.975 | Accuracy: 0.679000 | 1.396 sec/iter\n",
      "Epoch: 363 | Batch: 003 / 011 | Total loss: 0.974 | Reg loss: 0.025 | Tree loss: 0.974 | Accuracy: 0.675500 | 1.396 sec/iter\n",
      "Epoch: 363 | Batch: 004 / 011 | Total loss: 0.960 | Reg loss: 0.025 | Tree loss: 0.960 | Accuracy: 0.674000 | 1.396 sec/iter\n",
      "Epoch: 363 | Batch: 005 / 011 | Total loss: 0.940 | Reg loss: 0.025 | Tree loss: 0.940 | Accuracy: 0.662500 | 1.396 sec/iter\n",
      "Epoch: 363 | Batch: 006 / 011 | Total loss: 0.928 | Reg loss: 0.025 | Tree loss: 0.928 | Accuracy: 0.686500 | 1.396 sec/iter\n",
      "Epoch: 363 | Batch: 007 / 011 | Total loss: 0.911 | Reg loss: 0.025 | Tree loss: 0.911 | Accuracy: 0.681500 | 1.396 sec/iter\n",
      "Epoch: 363 | Batch: 008 / 011 | Total loss: 0.926 | Reg loss: 0.025 | Tree loss: 0.926 | Accuracy: 0.653000 | 1.396 sec/iter\n",
      "Epoch: 363 | Batch: 009 / 011 | Total loss: 0.926 | Reg loss: 0.025 | Tree loss: 0.926 | Accuracy: 0.658000 | 1.396 sec/iter\n",
      "Epoch: 363 | Batch: 010 / 011 | Total loss: 0.931 | Reg loss: 0.025 | Tree loss: 0.931 | Accuracy: 0.668942 | 1.396 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 364 | Batch: 000 / 011 | Total loss: 1.023 | Reg loss: 0.025 | Tree loss: 1.023 | Accuracy: 0.653500 | 1.396 sec/iter\n",
      "Epoch: 364 | Batch: 001 / 011 | Total loss: 1.006 | Reg loss: 0.025 | Tree loss: 1.006 | Accuracy: 0.670000 | 1.396 sec/iter\n",
      "Epoch: 364 | Batch: 002 / 011 | Total loss: 0.980 | Reg loss: 0.025 | Tree loss: 0.980 | Accuracy: 0.682000 | 1.396 sec/iter\n",
      "Epoch: 364 | Batch: 003 / 011 | Total loss: 0.978 | Reg loss: 0.025 | Tree loss: 0.978 | Accuracy: 0.688500 | 1.396 sec/iter\n",
      "Epoch: 364 | Batch: 004 / 011 | Total loss: 0.960 | Reg loss: 0.025 | Tree loss: 0.960 | Accuracy: 0.675500 | 1.396 sec/iter\n",
      "Epoch: 364 | Batch: 005 / 011 | Total loss: 0.925 | Reg loss: 0.025 | Tree loss: 0.925 | Accuracy: 0.676000 | 1.396 sec/iter\n",
      "Epoch: 364 | Batch: 006 / 011 | Total loss: 0.947 | Reg loss: 0.025 | Tree loss: 0.947 | Accuracy: 0.648500 | 1.396 sec/iter\n",
      "Epoch: 364 | Batch: 007 / 011 | Total loss: 0.927 | Reg loss: 0.025 | Tree loss: 0.927 | Accuracy: 0.661500 | 1.396 sec/iter\n",
      "Epoch: 364 | Batch: 008 / 011 | Total loss: 0.926 | Reg loss: 0.025 | Tree loss: 0.926 | Accuracy: 0.661000 | 1.396 sec/iter\n",
      "Epoch: 364 | Batch: 009 / 011 | Total loss: 0.903 | Reg loss: 0.025 | Tree loss: 0.903 | Accuracy: 0.663000 | 1.396 sec/iter\n",
      "Epoch: 364 | Batch: 010 / 011 | Total loss: 0.867 | Reg loss: 0.025 | Tree loss: 0.867 | Accuracy: 0.703072 | 1.396 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 365 | Batch: 000 / 011 | Total loss: 1.034 | Reg loss: 0.025 | Tree loss: 1.034 | Accuracy: 0.649500 | 1.397 sec/iter\n",
      "Epoch: 365 | Batch: 001 / 011 | Total loss: 1.019 | Reg loss: 0.025 | Tree loss: 1.019 | Accuracy: 0.668500 | 1.397 sec/iter\n",
      "Epoch: 365 | Batch: 002 / 011 | Total loss: 1.007 | Reg loss: 0.025 | Tree loss: 1.007 | Accuracy: 0.667000 | 1.397 sec/iter\n",
      "Epoch: 365 | Batch: 003 / 011 | Total loss: 0.946 | Reg loss: 0.025 | Tree loss: 0.946 | Accuracy: 0.704500 | 1.397 sec/iter\n",
      "Epoch: 365 | Batch: 004 / 011 | Total loss: 0.941 | Reg loss: 0.025 | Tree loss: 0.941 | Accuracy: 0.683500 | 1.397 sec/iter\n",
      "Epoch: 365 | Batch: 005 / 011 | Total loss: 0.932 | Reg loss: 0.025 | Tree loss: 0.932 | Accuracy: 0.687500 | 1.397 sec/iter\n",
      "Epoch: 365 | Batch: 006 / 011 | Total loss: 0.930 | Reg loss: 0.025 | Tree loss: 0.930 | Accuracy: 0.662500 | 1.397 sec/iter\n",
      "Epoch: 365 | Batch: 007 / 011 | Total loss: 0.937 | Reg loss: 0.025 | Tree loss: 0.937 | Accuracy: 0.656000 | 1.397 sec/iter\n",
      "Epoch: 365 | Batch: 008 / 011 | Total loss: 0.905 | Reg loss: 0.025 | Tree loss: 0.905 | Accuracy: 0.671000 | 1.397 sec/iter\n",
      "Epoch: 365 | Batch: 009 / 011 | Total loss: 0.910 | Reg loss: 0.025 | Tree loss: 0.910 | Accuracy: 0.664500 | 1.397 sec/iter\n",
      "Epoch: 365 | Batch: 010 / 011 | Total loss: 0.897 | Reg loss: 0.025 | Tree loss: 0.897 | Accuracy: 0.692833 | 1.397 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 366 | Batch: 000 / 011 | Total loss: 1.016 | Reg loss: 0.025 | Tree loss: 1.016 | Accuracy: 0.655500 | 1.397 sec/iter\n",
      "Epoch: 366 | Batch: 001 / 011 | Total loss: 1.000 | Reg loss: 0.025 | Tree loss: 1.000 | Accuracy: 0.672500 | 1.397 sec/iter\n",
      "Epoch: 366 | Batch: 002 / 011 | Total loss: 0.943 | Reg loss: 0.025 | Tree loss: 0.943 | Accuracy: 0.724000 | 1.397 sec/iter\n",
      "Epoch: 366 | Batch: 003 / 011 | Total loss: 0.943 | Reg loss: 0.025 | Tree loss: 0.943 | Accuracy: 0.693000 | 1.397 sec/iter\n",
      "Epoch: 366 | Batch: 004 / 011 | Total loss: 0.935 | Reg loss: 0.025 | Tree loss: 0.935 | Accuracy: 0.698000 | 1.397 sec/iter\n",
      "Epoch: 366 | Batch: 005 / 011 | Total loss: 0.943 | Reg loss: 0.025 | Tree loss: 0.943 | Accuracy: 0.679500 | 1.397 sec/iter\n",
      "Epoch: 366 | Batch: 006 / 011 | Total loss: 0.961 | Reg loss: 0.025 | Tree loss: 0.961 | Accuracy: 0.642000 | 1.397 sec/iter\n",
      "Epoch: 366 | Batch: 007 / 011 | Total loss: 0.943 | Reg loss: 0.025 | Tree loss: 0.943 | Accuracy: 0.641500 | 1.397 sec/iter\n",
      "Epoch: 366 | Batch: 008 / 011 | Total loss: 0.936 | Reg loss: 0.025 | Tree loss: 0.936 | Accuracy: 0.658500 | 1.397 sec/iter\n",
      "Epoch: 366 | Batch: 009 / 011 | Total loss: 0.945 | Reg loss: 0.025 | Tree loss: 0.945 | Accuracy: 0.650000 | 1.397 sec/iter\n",
      "Epoch: 366 | Batch: 010 / 011 | Total loss: 0.920 | Reg loss: 0.025 | Tree loss: 0.920 | Accuracy: 0.682594 | 1.397 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 367 | Batch: 000 / 011 | Total loss: 1.032 | Reg loss: 0.025 | Tree loss: 1.032 | Accuracy: 0.650500 | 1.398 sec/iter\n",
      "Epoch: 367 | Batch: 001 / 011 | Total loss: 0.992 | Reg loss: 0.025 | Tree loss: 0.992 | Accuracy: 0.678000 | 1.398 sec/iter\n",
      "Epoch: 367 | Batch: 002 / 011 | Total loss: 0.980 | Reg loss: 0.025 | Tree loss: 0.980 | Accuracy: 0.683500 | 1.398 sec/iter\n",
      "Epoch: 367 | Batch: 003 / 011 | Total loss: 0.953 | Reg loss: 0.025 | Tree loss: 0.953 | Accuracy: 0.696000 | 1.398 sec/iter\n",
      "Epoch: 367 | Batch: 004 / 011 | Total loss: 0.943 | Reg loss: 0.025 | Tree loss: 0.943 | Accuracy: 0.697500 | 1.398 sec/iter\n",
      "Epoch: 367 | Batch: 005 / 011 | Total loss: 0.953 | Reg loss: 0.025 | Tree loss: 0.953 | Accuracy: 0.677000 | 1.398 sec/iter\n",
      "Epoch: 367 | Batch: 006 / 011 | Total loss: 0.945 | Reg loss: 0.025 | Tree loss: 0.945 | Accuracy: 0.669500 | 1.398 sec/iter\n",
      "Epoch: 367 | Batch: 007 / 011 | Total loss: 0.944 | Reg loss: 0.025 | Tree loss: 0.944 | Accuracy: 0.661000 | 1.398 sec/iter\n",
      "Epoch: 367 | Batch: 008 / 011 | Total loss: 0.916 | Reg loss: 0.025 | Tree loss: 0.916 | Accuracy: 0.661500 | 1.398 sec/iter\n",
      "Epoch: 367 | Batch: 009 / 011 | Total loss: 0.914 | Reg loss: 0.025 | Tree loss: 0.914 | Accuracy: 0.659000 | 1.398 sec/iter\n",
      "Epoch: 367 | Batch: 010 / 011 | Total loss: 0.886 | Reg loss: 0.025 | Tree loss: 0.886 | Accuracy: 0.672355 | 1.398 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 368 | Batch: 000 / 011 | Total loss: 0.993 | Reg loss: 0.025 | Tree loss: 0.993 | Accuracy: 0.672000 | 1.398 sec/iter\n",
      "Epoch: 368 | Batch: 001 / 011 | Total loss: 0.989 | Reg loss: 0.025 | Tree loss: 0.989 | Accuracy: 0.680000 | 1.398 sec/iter\n",
      "Epoch: 368 | Batch: 002 / 011 | Total loss: 1.003 | Reg loss: 0.025 | Tree loss: 1.003 | Accuracy: 0.674000 | 1.398 sec/iter\n",
      "Epoch: 368 | Batch: 003 / 011 | Total loss: 0.974 | Reg loss: 0.025 | Tree loss: 0.974 | Accuracy: 0.681500 | 1.398 sec/iter\n",
      "Epoch: 368 | Batch: 004 / 011 | Total loss: 0.942 | Reg loss: 0.025 | Tree loss: 0.942 | Accuracy: 0.685000 | 1.398 sec/iter\n",
      "Epoch: 368 | Batch: 005 / 011 | Total loss: 0.941 | Reg loss: 0.025 | Tree loss: 0.941 | Accuracy: 0.672000 | 1.398 sec/iter\n",
      "Epoch: 368 | Batch: 006 / 011 | Total loss: 0.962 | Reg loss: 0.025 | Tree loss: 0.962 | Accuracy: 0.651500 | 1.398 sec/iter\n",
      "Epoch: 368 | Batch: 007 / 011 | Total loss: 0.918 | Reg loss: 0.025 | Tree loss: 0.918 | Accuracy: 0.646500 | 1.398 sec/iter\n",
      "Epoch: 368 | Batch: 008 / 011 | Total loss: 0.920 | Reg loss: 0.025 | Tree loss: 0.920 | Accuracy: 0.663500 | 1.398 sec/iter\n",
      "Epoch: 368 | Batch: 009 / 011 | Total loss: 0.910 | Reg loss: 0.025 | Tree loss: 0.910 | Accuracy: 0.668500 | 1.398 sec/iter\n",
      "Epoch: 368 | Batch: 010 / 011 | Total loss: 0.939 | Reg loss: 0.025 | Tree loss: 0.939 | Accuracy: 0.658703 | 1.398 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 369 | Batch: 000 / 011 | Total loss: 1.005 | Reg loss: 0.025 | Tree loss: 1.005 | Accuracy: 0.667000 | 1.399 sec/iter\n",
      "Epoch: 369 | Batch: 001 / 011 | Total loss: 1.019 | Reg loss: 0.025 | Tree loss: 1.019 | Accuracy: 0.662000 | 1.399 sec/iter\n",
      "Epoch: 369 | Batch: 002 / 011 | Total loss: 1.007 | Reg loss: 0.025 | Tree loss: 1.007 | Accuracy: 0.653000 | 1.399 sec/iter\n",
      "Epoch: 369 | Batch: 003 / 011 | Total loss: 0.953 | Reg loss: 0.025 | Tree loss: 0.953 | Accuracy: 0.679500 | 1.399 sec/iter\n",
      "Epoch: 369 | Batch: 004 / 011 | Total loss: 0.963 | Reg loss: 0.025 | Tree loss: 0.963 | Accuracy: 0.676000 | 1.399 sec/iter\n",
      "Epoch: 369 | Batch: 005 / 011 | Total loss: 0.936 | Reg loss: 0.025 | Tree loss: 0.936 | Accuracy: 0.690000 | 1.399 sec/iter\n",
      "Epoch: 369 | Batch: 006 / 011 | Total loss: 0.922 | Reg loss: 0.025 | Tree loss: 0.922 | Accuracy: 0.677000 | 1.399 sec/iter\n",
      "Epoch: 369 | Batch: 007 / 011 | Total loss: 0.908 | Reg loss: 0.025 | Tree loss: 0.908 | Accuracy: 0.675500 | 1.399 sec/iter\n",
      "Epoch: 369 | Batch: 008 / 011 | Total loss: 0.939 | Reg loss: 0.025 | Tree loss: 0.939 | Accuracy: 0.653500 | 1.399 sec/iter\n",
      "Epoch: 369 | Batch: 009 / 011 | Total loss: 0.910 | Reg loss: 0.025 | Tree loss: 0.910 | Accuracy: 0.665000 | 1.399 sec/iter\n",
      "Epoch: 369 | Batch: 010 / 011 | Total loss: 0.911 | Reg loss: 0.025 | Tree loss: 0.911 | Accuracy: 0.641638 | 1.399 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 370 | Batch: 000 / 011 | Total loss: 1.013 | Reg loss: 0.025 | Tree loss: 1.013 | Accuracy: 0.654500 | 1.399 sec/iter\n",
      "Epoch: 370 | Batch: 001 / 011 | Total loss: 1.019 | Reg loss: 0.025 | Tree loss: 1.019 | Accuracy: 0.641000 | 1.399 sec/iter\n",
      "Epoch: 370 | Batch: 002 / 011 | Total loss: 0.980 | Reg loss: 0.025 | Tree loss: 0.980 | Accuracy: 0.683500 | 1.399 sec/iter\n",
      "Epoch: 370 | Batch: 003 / 011 | Total loss: 0.972 | Reg loss: 0.025 | Tree loss: 0.972 | Accuracy: 0.676500 | 1.399 sec/iter\n",
      "Epoch: 370 | Batch: 004 / 011 | Total loss: 0.944 | Reg loss: 0.025 | Tree loss: 0.944 | Accuracy: 0.687500 | 1.399 sec/iter\n",
      "Epoch: 370 | Batch: 005 / 011 | Total loss: 0.936 | Reg loss: 0.025 | Tree loss: 0.936 | Accuracy: 0.673500 | 1.399 sec/iter\n",
      "Epoch: 370 | Batch: 006 / 011 | Total loss: 0.908 | Reg loss: 0.025 | Tree loss: 0.908 | Accuracy: 0.690000 | 1.399 sec/iter\n",
      "Epoch: 370 | Batch: 007 / 011 | Total loss: 0.916 | Reg loss: 0.025 | Tree loss: 0.916 | Accuracy: 0.670500 | 1.399 sec/iter\n",
      "Epoch: 370 | Batch: 008 / 011 | Total loss: 0.937 | Reg loss: 0.025 | Tree loss: 0.937 | Accuracy: 0.646000 | 1.399 sec/iter\n",
      "Epoch: 370 | Batch: 009 / 011 | Total loss: 0.927 | Reg loss: 0.025 | Tree loss: 0.927 | Accuracy: 0.657500 | 1.399 sec/iter\n",
      "Epoch: 370 | Batch: 010 / 011 | Total loss: 0.938 | Reg loss: 0.025 | Tree loss: 0.938 | Accuracy: 0.658703 | 1.399 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 371 | Batch: 000 / 011 | Total loss: 1.005 | Reg loss: 0.025 | Tree loss: 1.005 | Accuracy: 0.668500 | 1.4 sec/iter\n",
      "Epoch: 371 | Batch: 001 / 011 | Total loss: 1.015 | Reg loss: 0.025 | Tree loss: 1.015 | Accuracy: 0.657000 | 1.4 sec/iter\n",
      "Epoch: 371 | Batch: 002 / 011 | Total loss: 0.992 | Reg loss: 0.025 | Tree loss: 0.992 | Accuracy: 0.677000 | 1.4 sec/iter\n",
      "Epoch: 371 | Batch: 003 / 011 | Total loss: 0.962 | Reg loss: 0.025 | Tree loss: 0.962 | Accuracy: 0.683000 | 1.4 sec/iter\n",
      "Epoch: 371 | Batch: 004 / 011 | Total loss: 0.942 | Reg loss: 0.025 | Tree loss: 0.942 | Accuracy: 0.687000 | 1.4 sec/iter\n",
      "Epoch: 371 | Batch: 005 / 011 | Total loss: 0.934 | Reg loss: 0.025 | Tree loss: 0.934 | Accuracy: 0.682500 | 1.4 sec/iter\n",
      "Epoch: 371 | Batch: 006 / 011 | Total loss: 0.930 | Reg loss: 0.025 | Tree loss: 0.930 | Accuracy: 0.677000 | 1.4 sec/iter\n",
      "Epoch: 371 | Batch: 007 / 011 | Total loss: 0.943 | Reg loss: 0.025 | Tree loss: 0.943 | Accuracy: 0.670500 | 1.4 sec/iter\n",
      "Epoch: 371 | Batch: 008 / 011 | Total loss: 0.918 | Reg loss: 0.025 | Tree loss: 0.918 | Accuracy: 0.669500 | 1.4 sec/iter\n",
      "Epoch: 371 | Batch: 009 / 011 | Total loss: 0.916 | Reg loss: 0.025 | Tree loss: 0.916 | Accuracy: 0.652000 | 1.4 sec/iter\n",
      "Epoch: 371 | Batch: 010 / 011 | Total loss: 0.982 | Reg loss: 0.025 | Tree loss: 0.982 | Accuracy: 0.631399 | 1.4 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 372 | Batch: 000 / 011 | Total loss: 1.005 | Reg loss: 0.025 | Tree loss: 1.005 | Accuracy: 0.664500 | 1.4 sec/iter\n",
      "Epoch: 372 | Batch: 001 / 011 | Total loss: 1.002 | Reg loss: 0.025 | Tree loss: 1.002 | Accuracy: 0.677000 | 1.4 sec/iter\n",
      "Epoch: 372 | Batch: 002 / 011 | Total loss: 0.996 | Reg loss: 0.025 | Tree loss: 0.996 | Accuracy: 0.669500 | 1.4 sec/iter\n",
      "Epoch: 372 | Batch: 003 / 011 | Total loss: 0.948 | Reg loss: 0.025 | Tree loss: 0.948 | Accuracy: 0.711000 | 1.4 sec/iter\n",
      "Epoch: 372 | Batch: 004 / 011 | Total loss: 0.939 | Reg loss: 0.025 | Tree loss: 0.939 | Accuracy: 0.685000 | 1.4 sec/iter\n",
      "Epoch: 372 | Batch: 005 / 011 | Total loss: 0.918 | Reg loss: 0.025 | Tree loss: 0.918 | Accuracy: 0.688000 | 1.4 sec/iter\n",
      "Epoch: 372 | Batch: 006 / 011 | Total loss: 0.959 | Reg loss: 0.025 | Tree loss: 0.959 | Accuracy: 0.651000 | 1.4 sec/iter\n",
      "Epoch: 372 | Batch: 007 / 011 | Total loss: 0.925 | Reg loss: 0.025 | Tree loss: 0.925 | Accuracy: 0.654500 | 1.4 sec/iter\n",
      "Epoch: 372 | Batch: 008 / 011 | Total loss: 0.957 | Reg loss: 0.025 | Tree loss: 0.957 | Accuracy: 0.648000 | 1.4 sec/iter\n",
      "Epoch: 372 | Batch: 009 / 011 | Total loss: 0.905 | Reg loss: 0.025 | Tree loss: 0.905 | Accuracy: 0.675000 | 1.4 sec/iter\n",
      "Epoch: 372 | Batch: 010 / 011 | Total loss: 0.977 | Reg loss: 0.025 | Tree loss: 0.977 | Accuracy: 0.624573 | 1.4 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 373 | Batch: 000 / 011 | Total loss: 1.014 | Reg loss: 0.025 | Tree loss: 1.014 | Accuracy: 0.669000 | 1.401 sec/iter\n",
      "Epoch: 373 | Batch: 001 / 011 | Total loss: 1.022 | Reg loss: 0.025 | Tree loss: 1.022 | Accuracy: 0.649500 | 1.401 sec/iter\n",
      "Epoch: 373 | Batch: 002 / 011 | Total loss: 0.973 | Reg loss: 0.025 | Tree loss: 0.973 | Accuracy: 0.695000 | 1.401 sec/iter\n",
      "Epoch: 373 | Batch: 003 / 011 | Total loss: 0.980 | Reg loss: 0.025 | Tree loss: 0.980 | Accuracy: 0.684000 | 1.401 sec/iter\n",
      "Epoch: 373 | Batch: 004 / 011 | Total loss: 0.949 | Reg loss: 0.025 | Tree loss: 0.949 | Accuracy: 0.680000 | 1.401 sec/iter\n",
      "Epoch: 373 | Batch: 005 / 011 | Total loss: 0.924 | Reg loss: 0.025 | Tree loss: 0.924 | Accuracy: 0.683500 | 1.401 sec/iter\n",
      "Epoch: 373 | Batch: 006 / 011 | Total loss: 0.941 | Reg loss: 0.025 | Tree loss: 0.941 | Accuracy: 0.668000 | 1.401 sec/iter\n",
      "Epoch: 373 | Batch: 007 / 011 | Total loss: 0.895 | Reg loss: 0.025 | Tree loss: 0.895 | Accuracy: 0.673500 | 1.401 sec/iter\n",
      "Epoch: 373 | Batch: 008 / 011 | Total loss: 0.936 | Reg loss: 0.025 | Tree loss: 0.936 | Accuracy: 0.649000 | 1.401 sec/iter\n",
      "Epoch: 373 | Batch: 009 / 011 | Total loss: 0.929 | Reg loss: 0.025 | Tree loss: 0.929 | Accuracy: 0.654000 | 1.401 sec/iter\n",
      "Epoch: 373 | Batch: 010 / 011 | Total loss: 0.906 | Reg loss: 0.025 | Tree loss: 0.906 | Accuracy: 0.686007 | 1.401 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 374 | Batch: 000 / 011 | Total loss: 1.001 | Reg loss: 0.025 | Tree loss: 1.001 | Accuracy: 0.662500 | 1.401 sec/iter\n",
      "Epoch: 374 | Batch: 001 / 011 | Total loss: 1.005 | Reg loss: 0.025 | Tree loss: 1.005 | Accuracy: 0.667000 | 1.401 sec/iter\n",
      "Epoch: 374 | Batch: 002 / 011 | Total loss: 0.974 | Reg loss: 0.025 | Tree loss: 0.974 | Accuracy: 0.687000 | 1.401 sec/iter\n",
      "Epoch: 374 | Batch: 003 / 011 | Total loss: 0.953 | Reg loss: 0.025 | Tree loss: 0.953 | Accuracy: 0.691000 | 1.401 sec/iter\n",
      "Epoch: 374 | Batch: 004 / 011 | Total loss: 0.941 | Reg loss: 0.025 | Tree loss: 0.941 | Accuracy: 0.694000 | 1.401 sec/iter\n",
      "Epoch: 374 | Batch: 005 / 011 | Total loss: 0.926 | Reg loss: 0.025 | Tree loss: 0.926 | Accuracy: 0.680000 | 1.401 sec/iter\n",
      "Epoch: 374 | Batch: 006 / 011 | Total loss: 0.946 | Reg loss: 0.025 | Tree loss: 0.946 | Accuracy: 0.667000 | 1.401 sec/iter\n",
      "Epoch: 374 | Batch: 007 / 011 | Total loss: 0.934 | Reg loss: 0.025 | Tree loss: 0.934 | Accuracy: 0.653500 | 1.401 sec/iter\n",
      "Epoch: 374 | Batch: 008 / 011 | Total loss: 0.937 | Reg loss: 0.025 | Tree loss: 0.937 | Accuracy: 0.651500 | 1.401 sec/iter\n",
      "Epoch: 374 | Batch: 009 / 011 | Total loss: 0.943 | Reg loss: 0.025 | Tree loss: 0.943 | Accuracy: 0.650000 | 1.401 sec/iter\n",
      "Epoch: 374 | Batch: 010 / 011 | Total loss: 0.992 | Reg loss: 0.025 | Tree loss: 0.992 | Accuracy: 0.614334 | 1.401 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 375 | Batch: 000 / 011 | Total loss: 1.013 | Reg loss: 0.025 | Tree loss: 1.013 | Accuracy: 0.651000 | 1.402 sec/iter\n",
      "Epoch: 375 | Batch: 001 / 011 | Total loss: 0.998 | Reg loss: 0.025 | Tree loss: 0.998 | Accuracy: 0.653500 | 1.402 sec/iter\n",
      "Epoch: 375 | Batch: 002 / 011 | Total loss: 1.012 | Reg loss: 0.025 | Tree loss: 1.012 | Accuracy: 0.657000 | 1.402 sec/iter\n",
      "Epoch: 375 | Batch: 003 / 011 | Total loss: 0.959 | Reg loss: 0.025 | Tree loss: 0.959 | Accuracy: 0.686000 | 1.402 sec/iter\n",
      "Epoch: 375 | Batch: 004 / 011 | Total loss: 0.938 | Reg loss: 0.025 | Tree loss: 0.938 | Accuracy: 0.689000 | 1.402 sec/iter\n",
      "Epoch: 375 | Batch: 005 / 011 | Total loss: 0.932 | Reg loss: 0.025 | Tree loss: 0.932 | Accuracy: 0.684500 | 1.402 sec/iter\n",
      "Epoch: 375 | Batch: 006 / 011 | Total loss: 0.931 | Reg loss: 0.025 | Tree loss: 0.931 | Accuracy: 0.666500 | 1.402 sec/iter\n",
      "Epoch: 375 | Batch: 007 / 011 | Total loss: 0.921 | Reg loss: 0.025 | Tree loss: 0.921 | Accuracy: 0.671000 | 1.402 sec/iter\n",
      "Epoch: 375 | Batch: 008 / 011 | Total loss: 0.933 | Reg loss: 0.025 | Tree loss: 0.933 | Accuracy: 0.657000 | 1.402 sec/iter\n",
      "Epoch: 375 | Batch: 009 / 011 | Total loss: 0.931 | Reg loss: 0.025 | Tree loss: 0.931 | Accuracy: 0.655000 | 1.402 sec/iter\n",
      "Epoch: 375 | Batch: 010 / 011 | Total loss: 0.916 | Reg loss: 0.025 | Tree loss: 0.916 | Accuracy: 0.627986 | 1.402 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 376 | Batch: 000 / 011 | Total loss: 1.000 | Reg loss: 0.025 | Tree loss: 1.000 | Accuracy: 0.672500 | 1.402 sec/iter\n",
      "Epoch: 376 | Batch: 001 / 011 | Total loss: 0.998 | Reg loss: 0.025 | Tree loss: 0.998 | Accuracy: 0.655000 | 1.402 sec/iter\n",
      "Epoch: 376 | Batch: 002 / 011 | Total loss: 0.998 | Reg loss: 0.025 | Tree loss: 0.998 | Accuracy: 0.656500 | 1.402 sec/iter\n",
      "Epoch: 376 | Batch: 003 / 011 | Total loss: 0.967 | Reg loss: 0.025 | Tree loss: 0.967 | Accuracy: 0.678000 | 1.402 sec/iter\n",
      "Epoch: 376 | Batch: 004 / 011 | Total loss: 0.947 | Reg loss: 0.025 | Tree loss: 0.947 | Accuracy: 0.682000 | 1.402 sec/iter\n",
      "Epoch: 376 | Batch: 005 / 011 | Total loss: 0.926 | Reg loss: 0.025 | Tree loss: 0.926 | Accuracy: 0.685500 | 1.402 sec/iter\n",
      "Epoch: 376 | Batch: 006 / 011 | Total loss: 0.907 | Reg loss: 0.025 | Tree loss: 0.907 | Accuracy: 0.689000 | 1.402 sec/iter\n",
      "Epoch: 376 | Batch: 007 / 011 | Total loss: 0.948 | Reg loss: 0.025 | Tree loss: 0.948 | Accuracy: 0.659500 | 1.402 sec/iter\n",
      "Epoch: 376 | Batch: 008 / 011 | Total loss: 0.956 | Reg loss: 0.025 | Tree loss: 0.956 | Accuracy: 0.637000 | 1.402 sec/iter\n",
      "Epoch: 376 | Batch: 009 / 011 | Total loss: 0.932 | Reg loss: 0.025 | Tree loss: 0.932 | Accuracy: 0.644000 | 1.402 sec/iter\n",
      "Epoch: 376 | Batch: 010 / 011 | Total loss: 0.890 | Reg loss: 0.025 | Tree loss: 0.890 | Accuracy: 0.682594 | 1.402 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 377 | Batch: 000 / 011 | Total loss: 1.034 | Reg loss: 0.025 | Tree loss: 1.034 | Accuracy: 0.646000 | 1.402 sec/iter\n",
      "Epoch: 377 | Batch: 001 / 011 | Total loss: 0.999 | Reg loss: 0.025 | Tree loss: 0.999 | Accuracy: 0.667500 | 1.402 sec/iter\n",
      "Epoch: 377 | Batch: 002 / 011 | Total loss: 0.977 | Reg loss: 0.025 | Tree loss: 0.977 | Accuracy: 0.694500 | 1.402 sec/iter\n",
      "Epoch: 377 | Batch: 003 / 011 | Total loss: 0.967 | Reg loss: 0.025 | Tree loss: 0.967 | Accuracy: 0.675500 | 1.402 sec/iter\n",
      "Epoch: 377 | Batch: 004 / 011 | Total loss: 0.954 | Reg loss: 0.025 | Tree loss: 0.954 | Accuracy: 0.683000 | 1.402 sec/iter\n",
      "Epoch: 377 | Batch: 005 / 011 | Total loss: 0.936 | Reg loss: 0.025 | Tree loss: 0.936 | Accuracy: 0.675000 | 1.402 sec/iter\n",
      "Epoch: 377 | Batch: 006 / 011 | Total loss: 0.960 | Reg loss: 0.025 | Tree loss: 0.960 | Accuracy: 0.645500 | 1.402 sec/iter\n",
      "Epoch: 377 | Batch: 007 / 011 | Total loss: 0.906 | Reg loss: 0.025 | Tree loss: 0.906 | Accuracy: 0.690000 | 1.402 sec/iter\n",
      "Epoch: 377 | Batch: 008 / 011 | Total loss: 0.917 | Reg loss: 0.025 | Tree loss: 0.917 | Accuracy: 0.667500 | 1.402 sec/iter\n",
      "Epoch: 377 | Batch: 009 / 011 | Total loss: 0.902 | Reg loss: 0.025 | Tree loss: 0.902 | Accuracy: 0.659000 | 1.402 sec/iter\n",
      "Epoch: 377 | Batch: 010 / 011 | Total loss: 0.885 | Reg loss: 0.025 | Tree loss: 0.885 | Accuracy: 0.692833 | 1.402 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 378 | Batch: 000 / 011 | Total loss: 1.019 | Reg loss: 0.025 | Tree loss: 1.019 | Accuracy: 0.654000 | 1.403 sec/iter\n",
      "Epoch: 378 | Batch: 001 / 011 | Total loss: 1.014 | Reg loss: 0.025 | Tree loss: 1.014 | Accuracy: 0.659000 | 1.403 sec/iter\n",
      "Epoch: 378 | Batch: 002 / 011 | Total loss: 0.968 | Reg loss: 0.025 | Tree loss: 0.968 | Accuracy: 0.681000 | 1.403 sec/iter\n",
      "Epoch: 378 | Batch: 003 / 011 | Total loss: 0.953 | Reg loss: 0.025 | Tree loss: 0.953 | Accuracy: 0.692000 | 1.403 sec/iter\n",
      "Epoch: 378 | Batch: 004 / 011 | Total loss: 0.922 | Reg loss: 0.025 | Tree loss: 0.922 | Accuracy: 0.698500 | 1.403 sec/iter\n",
      "Epoch: 378 | Batch: 005 / 011 | Total loss: 0.940 | Reg loss: 0.025 | Tree loss: 0.940 | Accuracy: 0.681500 | 1.403 sec/iter\n",
      "Epoch: 378 | Batch: 006 / 011 | Total loss: 0.939 | Reg loss: 0.025 | Tree loss: 0.939 | Accuracy: 0.666000 | 1.403 sec/iter\n",
      "Epoch: 378 | Batch: 007 / 011 | Total loss: 0.941 | Reg loss: 0.025 | Tree loss: 0.941 | Accuracy: 0.657500 | 1.403 sec/iter\n",
      "Epoch: 378 | Batch: 008 / 011 | Total loss: 0.943 | Reg loss: 0.025 | Tree loss: 0.943 | Accuracy: 0.670000 | 1.403 sec/iter\n",
      "Epoch: 378 | Batch: 009 / 011 | Total loss: 0.933 | Reg loss: 0.025 | Tree loss: 0.933 | Accuracy: 0.655000 | 1.403 sec/iter\n",
      "Epoch: 378 | Batch: 010 / 011 | Total loss: 0.900 | Reg loss: 0.025 | Tree loss: 0.900 | Accuracy: 0.672355 | 1.403 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 379 | Batch: 000 / 011 | Total loss: 1.020 | Reg loss: 0.025 | Tree loss: 1.020 | Accuracy: 0.656000 | 1.404 sec/iter\n",
      "Epoch: 379 | Batch: 001 / 011 | Total loss: 1.015 | Reg loss: 0.025 | Tree loss: 1.015 | Accuracy: 0.668000 | 1.404 sec/iter\n",
      "Epoch: 379 | Batch: 002 / 011 | Total loss: 0.990 | Reg loss: 0.025 | Tree loss: 0.990 | Accuracy: 0.677000 | 1.404 sec/iter\n",
      "Epoch: 379 | Batch: 003 / 011 | Total loss: 0.936 | Reg loss: 0.025 | Tree loss: 0.936 | Accuracy: 0.704500 | 1.404 sec/iter\n",
      "Epoch: 379 | Batch: 004 / 011 | Total loss: 0.950 | Reg loss: 0.025 | Tree loss: 0.950 | Accuracy: 0.683500 | 1.404 sec/iter\n",
      "Epoch: 379 | Batch: 005 / 011 | Total loss: 0.936 | Reg loss: 0.025 | Tree loss: 0.936 | Accuracy: 0.679000 | 1.403 sec/iter\n",
      "Epoch: 379 | Batch: 006 / 011 | Total loss: 0.917 | Reg loss: 0.025 | Tree loss: 0.917 | Accuracy: 0.674500 | 1.403 sec/iter\n",
      "Epoch: 379 | Batch: 007 / 011 | Total loss: 0.958 | Reg loss: 0.025 | Tree loss: 0.958 | Accuracy: 0.648000 | 1.403 sec/iter\n",
      "Epoch: 379 | Batch: 008 / 011 | Total loss: 0.913 | Reg loss: 0.025 | Tree loss: 0.913 | Accuracy: 0.658500 | 1.403 sec/iter\n",
      "Epoch: 379 | Batch: 009 / 011 | Total loss: 0.948 | Reg loss: 0.025 | Tree loss: 0.948 | Accuracy: 0.654500 | 1.403 sec/iter\n",
      "Epoch: 379 | Batch: 010 / 011 | Total loss: 0.876 | Reg loss: 0.025 | Tree loss: 0.876 | Accuracy: 0.686007 | 1.403 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 380 | Batch: 000 / 011 | Total loss: 1.021 | Reg loss: 0.025 | Tree loss: 1.021 | Accuracy: 0.654000 | 1.404 sec/iter\n",
      "Epoch: 380 | Batch: 001 / 011 | Total loss: 1.000 | Reg loss: 0.025 | Tree loss: 1.000 | Accuracy: 0.661500 | 1.404 sec/iter\n",
      "Epoch: 380 | Batch: 002 / 011 | Total loss: 0.976 | Reg loss: 0.025 | Tree loss: 0.976 | Accuracy: 0.687000 | 1.403 sec/iter\n",
      "Epoch: 380 | Batch: 003 / 011 | Total loss: 0.998 | Reg loss: 0.025 | Tree loss: 0.998 | Accuracy: 0.674000 | 1.403 sec/iter\n",
      "Epoch: 380 | Batch: 004 / 011 | Total loss: 0.937 | Reg loss: 0.025 | Tree loss: 0.937 | Accuracy: 0.702500 | 1.403 sec/iter\n",
      "Epoch: 380 | Batch: 005 / 011 | Total loss: 0.933 | Reg loss: 0.025 | Tree loss: 0.933 | Accuracy: 0.680500 | 1.403 sec/iter\n",
      "Epoch: 380 | Batch: 006 / 011 | Total loss: 0.927 | Reg loss: 0.025 | Tree loss: 0.927 | Accuracy: 0.671000 | 1.403 sec/iter\n",
      "Epoch: 380 | Batch: 007 / 011 | Total loss: 0.949 | Reg loss: 0.025 | Tree loss: 0.949 | Accuracy: 0.650000 | 1.403 sec/iter\n",
      "Epoch: 380 | Batch: 008 / 011 | Total loss: 0.919 | Reg loss: 0.025 | Tree loss: 0.919 | Accuracy: 0.665000 | 1.403 sec/iter\n",
      "Epoch: 380 | Batch: 009 / 011 | Total loss: 0.904 | Reg loss: 0.025 | Tree loss: 0.904 | Accuracy: 0.673500 | 1.403 sec/iter\n",
      "Epoch: 380 | Batch: 010 / 011 | Total loss: 0.906 | Reg loss: 0.025 | Tree loss: 0.906 | Accuracy: 0.645051 | 1.403 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 381 | Batch: 000 / 011 | Total loss: 1.020 | Reg loss: 0.025 | Tree loss: 1.020 | Accuracy: 0.666000 | 1.404 sec/iter\n",
      "Epoch: 381 | Batch: 001 / 011 | Total loss: 0.993 | Reg loss: 0.025 | Tree loss: 0.993 | Accuracy: 0.668500 | 1.404 sec/iter\n",
      "Epoch: 381 | Batch: 002 / 011 | Total loss: 0.989 | Reg loss: 0.025 | Tree loss: 0.989 | Accuracy: 0.677500 | 1.404 sec/iter\n",
      "Epoch: 381 | Batch: 003 / 011 | Total loss: 0.949 | Reg loss: 0.025 | Tree loss: 0.949 | Accuracy: 0.691000 | 1.404 sec/iter\n",
      "Epoch: 381 | Batch: 004 / 011 | Total loss: 0.942 | Reg loss: 0.025 | Tree loss: 0.942 | Accuracy: 0.695500 | 1.404 sec/iter\n",
      "Epoch: 381 | Batch: 005 / 011 | Total loss: 0.945 | Reg loss: 0.025 | Tree loss: 0.945 | Accuracy: 0.678000 | 1.404 sec/iter\n",
      "Epoch: 381 | Batch: 006 / 011 | Total loss: 0.953 | Reg loss: 0.025 | Tree loss: 0.953 | Accuracy: 0.658500 | 1.404 sec/iter\n",
      "Epoch: 381 | Batch: 007 / 011 | Total loss: 0.907 | Reg loss: 0.025 | Tree loss: 0.907 | Accuracy: 0.684000 | 1.404 sec/iter\n",
      "Epoch: 381 | Batch: 008 / 011 | Total loss: 0.924 | Reg loss: 0.025 | Tree loss: 0.924 | Accuracy: 0.654000 | 1.404 sec/iter\n",
      "Epoch: 381 | Batch: 009 / 011 | Total loss: 0.921 | Reg loss: 0.025 | Tree loss: 0.921 | Accuracy: 0.657500 | 1.404 sec/iter\n",
      "Epoch: 381 | Batch: 010 / 011 | Total loss: 0.915 | Reg loss: 0.025 | Tree loss: 0.915 | Accuracy: 0.665529 | 1.404 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 382 | Batch: 000 / 011 | Total loss: 1.009 | Reg loss: 0.025 | Tree loss: 1.009 | Accuracy: 0.661000 | 1.404 sec/iter\n",
      "Epoch: 382 | Batch: 001 / 011 | Total loss: 0.997 | Reg loss: 0.025 | Tree loss: 0.997 | Accuracy: 0.679500 | 1.404 sec/iter\n",
      "Epoch: 382 | Batch: 002 / 011 | Total loss: 0.987 | Reg loss: 0.025 | Tree loss: 0.987 | Accuracy: 0.674500 | 1.404 sec/iter\n",
      "Epoch: 382 | Batch: 003 / 011 | Total loss: 0.946 | Reg loss: 0.025 | Tree loss: 0.946 | Accuracy: 0.687500 | 1.404 sec/iter\n",
      "Epoch: 382 | Batch: 004 / 011 | Total loss: 0.953 | Reg loss: 0.025 | Tree loss: 0.953 | Accuracy: 0.682500 | 1.404 sec/iter\n",
      "Epoch: 382 | Batch: 005 / 011 | Total loss: 0.954 | Reg loss: 0.025 | Tree loss: 0.954 | Accuracy: 0.667000 | 1.404 sec/iter\n",
      "Epoch: 382 | Batch: 006 / 011 | Total loss: 0.951 | Reg loss: 0.025 | Tree loss: 0.951 | Accuracy: 0.673000 | 1.404 sec/iter\n",
      "Epoch: 382 | Batch: 007 / 011 | Total loss: 0.942 | Reg loss: 0.025 | Tree loss: 0.942 | Accuracy: 0.669000 | 1.404 sec/iter\n",
      "Epoch: 382 | Batch: 008 / 011 | Total loss: 0.901 | Reg loss: 0.025 | Tree loss: 0.901 | Accuracy: 0.669000 | 1.404 sec/iter\n",
      "Epoch: 382 | Batch: 009 / 011 | Total loss: 0.900 | Reg loss: 0.025 | Tree loss: 0.900 | Accuracy: 0.678000 | 1.404 sec/iter\n",
      "Epoch: 382 | Batch: 010 / 011 | Total loss: 0.933 | Reg loss: 0.025 | Tree loss: 0.933 | Accuracy: 0.658703 | 1.404 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 383 | Batch: 000 / 011 | Total loss: 1.023 | Reg loss: 0.025 | Tree loss: 1.023 | Accuracy: 0.657000 | 1.405 sec/iter\n",
      "Epoch: 383 | Batch: 001 / 011 | Total loss: 1.001 | Reg loss: 0.025 | Tree loss: 1.001 | Accuracy: 0.661500 | 1.405 sec/iter\n",
      "Epoch: 383 | Batch: 002 / 011 | Total loss: 0.988 | Reg loss: 0.025 | Tree loss: 0.988 | Accuracy: 0.671000 | 1.405 sec/iter\n",
      "Epoch: 383 | Batch: 003 / 011 | Total loss: 0.940 | Reg loss: 0.025 | Tree loss: 0.940 | Accuracy: 0.706500 | 1.405 sec/iter\n",
      "Epoch: 383 | Batch: 004 / 011 | Total loss: 0.943 | Reg loss: 0.025 | Tree loss: 0.943 | Accuracy: 0.681000 | 1.405 sec/iter\n",
      "Epoch: 383 | Batch: 005 / 011 | Total loss: 0.931 | Reg loss: 0.025 | Tree loss: 0.931 | Accuracy: 0.690000 | 1.405 sec/iter\n",
      "Epoch: 383 | Batch: 006 / 011 | Total loss: 0.952 | Reg loss: 0.025 | Tree loss: 0.952 | Accuracy: 0.661500 | 1.405 sec/iter\n",
      "Epoch: 383 | Batch: 007 / 011 | Total loss: 0.912 | Reg loss: 0.025 | Tree loss: 0.912 | Accuracy: 0.668500 | 1.405 sec/iter\n",
      "Epoch: 383 | Batch: 008 / 011 | Total loss: 0.922 | Reg loss: 0.025 | Tree loss: 0.922 | Accuracy: 0.651500 | 1.405 sec/iter\n",
      "Epoch: 383 | Batch: 009 / 011 | Total loss: 0.945 | Reg loss: 0.025 | Tree loss: 0.945 | Accuracy: 0.637500 | 1.405 sec/iter\n",
      "Epoch: 383 | Batch: 010 / 011 | Total loss: 0.865 | Reg loss: 0.025 | Tree loss: 0.865 | Accuracy: 0.682594 | 1.405 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 384 | Batch: 000 / 011 | Total loss: 1.008 | Reg loss: 0.025 | Tree loss: 1.008 | Accuracy: 0.651500 | 1.405 sec/iter\n",
      "Epoch: 384 | Batch: 001 / 011 | Total loss: 1.020 | Reg loss: 0.025 | Tree loss: 1.020 | Accuracy: 0.663500 | 1.405 sec/iter\n",
      "Epoch: 384 | Batch: 002 / 011 | Total loss: 0.986 | Reg loss: 0.025 | Tree loss: 0.986 | Accuracy: 0.670500 | 1.405 sec/iter\n",
      "Epoch: 384 | Batch: 003 / 011 | Total loss: 0.942 | Reg loss: 0.025 | Tree loss: 0.942 | Accuracy: 0.690500 | 1.405 sec/iter\n",
      "Epoch: 384 | Batch: 004 / 011 | Total loss: 0.942 | Reg loss: 0.025 | Tree loss: 0.942 | Accuracy: 0.701500 | 1.405 sec/iter\n",
      "Epoch: 384 | Batch: 005 / 011 | Total loss: 0.945 | Reg loss: 0.025 | Tree loss: 0.945 | Accuracy: 0.678500 | 1.405 sec/iter\n",
      "Epoch: 384 | Batch: 006 / 011 | Total loss: 0.916 | Reg loss: 0.025 | Tree loss: 0.916 | Accuracy: 0.687000 | 1.405 sec/iter\n",
      "Epoch: 384 | Batch: 007 / 011 | Total loss: 0.926 | Reg loss: 0.025 | Tree loss: 0.926 | Accuracy: 0.660000 | 1.405 sec/iter\n",
      "Epoch: 384 | Batch: 008 / 011 | Total loss: 0.929 | Reg loss: 0.025 | Tree loss: 0.929 | Accuracy: 0.653500 | 1.405 sec/iter\n",
      "Epoch: 384 | Batch: 009 / 011 | Total loss: 0.948 | Reg loss: 0.025 | Tree loss: 0.948 | Accuracy: 0.644500 | 1.405 sec/iter\n",
      "Epoch: 384 | Batch: 010 / 011 | Total loss: 0.921 | Reg loss: 0.025 | Tree loss: 0.921 | Accuracy: 0.645051 | 1.405 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 385 | Batch: 000 / 011 | Total loss: 1.034 | Reg loss: 0.025 | Tree loss: 1.034 | Accuracy: 0.649000 | 1.406 sec/iter\n",
      "Epoch: 385 | Batch: 001 / 011 | Total loss: 0.994 | Reg loss: 0.025 | Tree loss: 0.994 | Accuracy: 0.681500 | 1.406 sec/iter\n",
      "Epoch: 385 | Batch: 002 / 011 | Total loss: 1.001 | Reg loss: 0.025 | Tree loss: 1.001 | Accuracy: 0.673000 | 1.406 sec/iter\n",
      "Epoch: 385 | Batch: 003 / 011 | Total loss: 0.952 | Reg loss: 0.025 | Tree loss: 0.952 | Accuracy: 0.690000 | 1.406 sec/iter\n",
      "Epoch: 385 | Batch: 004 / 011 | Total loss: 0.962 | Reg loss: 0.025 | Tree loss: 0.962 | Accuracy: 0.678500 | 1.406 sec/iter\n",
      "Epoch: 385 | Batch: 005 / 011 | Total loss: 0.937 | Reg loss: 0.025 | Tree loss: 0.937 | Accuracy: 0.685500 | 1.406 sec/iter\n",
      "Epoch: 385 | Batch: 006 / 011 | Total loss: 0.953 | Reg loss: 0.025 | Tree loss: 0.953 | Accuracy: 0.672500 | 1.406 sec/iter\n",
      "Epoch: 385 | Batch: 007 / 011 | Total loss: 0.886 | Reg loss: 0.025 | Tree loss: 0.886 | Accuracy: 0.685500 | 1.406 sec/iter\n",
      "Epoch: 385 | Batch: 008 / 011 | Total loss: 0.930 | Reg loss: 0.025 | Tree loss: 0.930 | Accuracy: 0.663000 | 1.406 sec/iter\n",
      "Epoch: 385 | Batch: 009 / 011 | Total loss: 0.908 | Reg loss: 0.025 | Tree loss: 0.908 | Accuracy: 0.678000 | 1.406 sec/iter\n",
      "Epoch: 385 | Batch: 010 / 011 | Total loss: 0.907 | Reg loss: 0.025 | Tree loss: 0.907 | Accuracy: 0.682594 | 1.406 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 386 | Batch: 000 / 011 | Total loss: 1.020 | Reg loss: 0.025 | Tree loss: 1.020 | Accuracy: 0.654000 | 1.406 sec/iter\n",
      "Epoch: 386 | Batch: 001 / 011 | Total loss: 1.023 | Reg loss: 0.025 | Tree loss: 1.023 | Accuracy: 0.652000 | 1.406 sec/iter\n",
      "Epoch: 386 | Batch: 002 / 011 | Total loss: 0.977 | Reg loss: 0.025 | Tree loss: 0.977 | Accuracy: 0.682500 | 1.406 sec/iter\n",
      "Epoch: 386 | Batch: 003 / 011 | Total loss: 0.920 | Reg loss: 0.025 | Tree loss: 0.920 | Accuracy: 0.714000 | 1.406 sec/iter\n",
      "Epoch: 386 | Batch: 004 / 011 | Total loss: 0.948 | Reg loss: 0.025 | Tree loss: 0.948 | Accuracy: 0.682000 | 1.406 sec/iter\n",
      "Epoch: 386 | Batch: 005 / 011 | Total loss: 0.947 | Reg loss: 0.025 | Tree loss: 0.947 | Accuracy: 0.672000 | 1.406 sec/iter\n",
      "Epoch: 386 | Batch: 006 / 011 | Total loss: 0.930 | Reg loss: 0.025 | Tree loss: 0.930 | Accuracy: 0.664000 | 1.406 sec/iter\n",
      "Epoch: 386 | Batch: 007 / 011 | Total loss: 0.925 | Reg loss: 0.025 | Tree loss: 0.925 | Accuracy: 0.665000 | 1.406 sec/iter\n",
      "Epoch: 386 | Batch: 008 / 011 | Total loss: 0.945 | Reg loss: 0.025 | Tree loss: 0.945 | Accuracy: 0.660000 | 1.406 sec/iter\n",
      "Epoch: 386 | Batch: 009 / 011 | Total loss: 0.917 | Reg loss: 0.025 | Tree loss: 0.917 | Accuracy: 0.675500 | 1.406 sec/iter\n",
      "Epoch: 386 | Batch: 010 / 011 | Total loss: 0.910 | Reg loss: 0.025 | Tree loss: 0.910 | Accuracy: 0.679181 | 1.406 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 387 | Batch: 000 / 011 | Total loss: 1.029 | Reg loss: 0.025 | Tree loss: 1.029 | Accuracy: 0.647000 | 1.407 sec/iter\n",
      "Epoch: 387 | Batch: 001 / 011 | Total loss: 0.989 | Reg loss: 0.025 | Tree loss: 0.989 | Accuracy: 0.683500 | 1.407 sec/iter\n",
      "Epoch: 387 | Batch: 002 / 011 | Total loss: 0.991 | Reg loss: 0.025 | Tree loss: 0.991 | Accuracy: 0.680500 | 1.407 sec/iter\n",
      "Epoch: 387 | Batch: 003 / 011 | Total loss: 0.960 | Reg loss: 0.025 | Tree loss: 0.960 | Accuracy: 0.682000 | 1.407 sec/iter\n",
      "Epoch: 387 | Batch: 004 / 011 | Total loss: 0.942 | Reg loss: 0.025 | Tree loss: 0.942 | Accuracy: 0.685500 | 1.407 sec/iter\n",
      "Epoch: 387 | Batch: 005 / 011 | Total loss: 0.935 | Reg loss: 0.025 | Tree loss: 0.935 | Accuracy: 0.676000 | 1.407 sec/iter\n",
      "Epoch: 387 | Batch: 006 / 011 | Total loss: 0.940 | Reg loss: 0.025 | Tree loss: 0.940 | Accuracy: 0.653500 | 1.407 sec/iter\n",
      "Epoch: 387 | Batch: 007 / 011 | Total loss: 0.924 | Reg loss: 0.025 | Tree loss: 0.924 | Accuracy: 0.659500 | 1.407 sec/iter\n",
      "Epoch: 387 | Batch: 008 / 011 | Total loss: 0.937 | Reg loss: 0.025 | Tree loss: 0.937 | Accuracy: 0.659000 | 1.407 sec/iter\n",
      "Epoch: 387 | Batch: 009 / 011 | Total loss: 0.924 | Reg loss: 0.025 | Tree loss: 0.924 | Accuracy: 0.660500 | 1.407 sec/iter\n",
      "Epoch: 387 | Batch: 010 / 011 | Total loss: 0.919 | Reg loss: 0.025 | Tree loss: 0.919 | Accuracy: 0.668942 | 1.407 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 388 | Batch: 000 / 011 | Total loss: 1.003 | Reg loss: 0.025 | Tree loss: 1.003 | Accuracy: 0.671000 | 1.407 sec/iter\n",
      "Epoch: 388 | Batch: 001 / 011 | Total loss: 0.986 | Reg loss: 0.025 | Tree loss: 0.986 | Accuracy: 0.672500 | 1.407 sec/iter\n",
      "Epoch: 388 | Batch: 002 / 011 | Total loss: 0.995 | Reg loss: 0.025 | Tree loss: 0.995 | Accuracy: 0.682500 | 1.407 sec/iter\n",
      "Epoch: 388 | Batch: 003 / 011 | Total loss: 0.954 | Reg loss: 0.025 | Tree loss: 0.954 | Accuracy: 0.692500 | 1.407 sec/iter\n",
      "Epoch: 388 | Batch: 004 / 011 | Total loss: 0.936 | Reg loss: 0.025 | Tree loss: 0.936 | Accuracy: 0.700500 | 1.407 sec/iter\n",
      "Epoch: 388 | Batch: 005 / 011 | Total loss: 0.926 | Reg loss: 0.025 | Tree loss: 0.926 | Accuracy: 0.680000 | 1.407 sec/iter\n",
      "Epoch: 388 | Batch: 006 / 011 | Total loss: 0.970 | Reg loss: 0.025 | Tree loss: 0.970 | Accuracy: 0.645500 | 1.407 sec/iter\n",
      "Epoch: 388 | Batch: 007 / 011 | Total loss: 0.926 | Reg loss: 0.025 | Tree loss: 0.926 | Accuracy: 0.661000 | 1.407 sec/iter\n",
      "Epoch: 388 | Batch: 008 / 011 | Total loss: 0.952 | Reg loss: 0.025 | Tree loss: 0.952 | Accuracy: 0.639500 | 1.407 sec/iter\n",
      "Epoch: 388 | Batch: 009 / 011 | Total loss: 0.906 | Reg loss: 0.025 | Tree loss: 0.906 | Accuracy: 0.661500 | 1.407 sec/iter\n",
      "Epoch: 388 | Batch: 010 / 011 | Total loss: 1.004 | Reg loss: 0.025 | Tree loss: 1.004 | Accuracy: 0.600683 | 1.407 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 389 | Batch: 000 / 011 | Total loss: 1.015 | Reg loss: 0.025 | Tree loss: 1.015 | Accuracy: 0.653000 | 1.407 sec/iter\n",
      "Epoch: 389 | Batch: 001 / 011 | Total loss: 1.019 | Reg loss: 0.025 | Tree loss: 1.019 | Accuracy: 0.658000 | 1.407 sec/iter\n",
      "Epoch: 389 | Batch: 002 / 011 | Total loss: 0.988 | Reg loss: 0.025 | Tree loss: 0.988 | Accuracy: 0.683000 | 1.407 sec/iter\n",
      "Epoch: 389 | Batch: 003 / 011 | Total loss: 0.985 | Reg loss: 0.025 | Tree loss: 0.985 | Accuracy: 0.682500 | 1.407 sec/iter\n",
      "Epoch: 389 | Batch: 004 / 011 | Total loss: 0.946 | Reg loss: 0.025 | Tree loss: 0.946 | Accuracy: 0.685500 | 1.407 sec/iter\n",
      "Epoch: 389 | Batch: 005 / 011 | Total loss: 0.933 | Reg loss: 0.025 | Tree loss: 0.933 | Accuracy: 0.691000 | 1.407 sec/iter\n",
      "Epoch: 389 | Batch: 006 / 011 | Total loss: 0.923 | Reg loss: 0.025 | Tree loss: 0.923 | Accuracy: 0.672500 | 1.407 sec/iter\n",
      "Epoch: 389 | Batch: 007 / 011 | Total loss: 0.923 | Reg loss: 0.025 | Tree loss: 0.923 | Accuracy: 0.659000 | 1.407 sec/iter\n",
      "Epoch: 389 | Batch: 008 / 011 | Total loss: 0.910 | Reg loss: 0.025 | Tree loss: 0.910 | Accuracy: 0.675000 | 1.407 sec/iter\n",
      "Epoch: 389 | Batch: 009 / 011 | Total loss: 0.917 | Reg loss: 0.025 | Tree loss: 0.917 | Accuracy: 0.670500 | 1.407 sec/iter\n",
      "Epoch: 389 | Batch: 010 / 011 | Total loss: 0.871 | Reg loss: 0.025 | Tree loss: 0.871 | Accuracy: 0.672355 | 1.407 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 390 | Batch: 000 / 011 | Total loss: 1.013 | Reg loss: 0.025 | Tree loss: 1.013 | Accuracy: 0.661000 | 1.407 sec/iter\n",
      "Epoch: 390 | Batch: 001 / 011 | Total loss: 0.990 | Reg loss: 0.025 | Tree loss: 0.990 | Accuracy: 0.686000 | 1.407 sec/iter\n",
      "Epoch: 390 | Batch: 002 / 011 | Total loss: 1.003 | Reg loss: 0.025 | Tree loss: 1.003 | Accuracy: 0.657500 | 1.407 sec/iter\n",
      "Epoch: 390 | Batch: 003 / 011 | Total loss: 0.954 | Reg loss: 0.025 | Tree loss: 0.954 | Accuracy: 0.687500 | 1.407 sec/iter\n",
      "Epoch: 390 | Batch: 004 / 011 | Total loss: 0.942 | Reg loss: 0.025 | Tree loss: 0.942 | Accuracy: 0.686500 | 1.407 sec/iter\n",
      "Epoch: 390 | Batch: 005 / 011 | Total loss: 0.945 | Reg loss: 0.025 | Tree loss: 0.945 | Accuracy: 0.686500 | 1.407 sec/iter\n",
      "Epoch: 390 | Batch: 006 / 011 | Total loss: 0.937 | Reg loss: 0.025 | Tree loss: 0.937 | Accuracy: 0.658500 | 1.407 sec/iter\n",
      "Epoch: 390 | Batch: 007 / 011 | Total loss: 0.932 | Reg loss: 0.025 | Tree loss: 0.932 | Accuracy: 0.658500 | 1.407 sec/iter\n",
      "Epoch: 390 | Batch: 008 / 011 | Total loss: 0.906 | Reg loss: 0.025 | Tree loss: 0.906 | Accuracy: 0.661500 | 1.407 sec/iter\n",
      "Epoch: 390 | Batch: 009 / 011 | Total loss: 0.922 | Reg loss: 0.025 | Tree loss: 0.922 | Accuracy: 0.656000 | 1.407 sec/iter\n",
      "Epoch: 390 | Batch: 010 / 011 | Total loss: 0.907 | Reg loss: 0.025 | Tree loss: 0.907 | Accuracy: 0.679181 | 1.407 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 391 | Batch: 000 / 011 | Total loss: 1.014 | Reg loss: 0.025 | Tree loss: 1.014 | Accuracy: 0.663500 | 1.407 sec/iter\n",
      "Epoch: 391 | Batch: 001 / 011 | Total loss: 1.033 | Reg loss: 0.025 | Tree loss: 1.033 | Accuracy: 0.650000 | 1.407 sec/iter\n",
      "Epoch: 391 | Batch: 002 / 011 | Total loss: 0.983 | Reg loss: 0.025 | Tree loss: 0.983 | Accuracy: 0.694500 | 1.407 sec/iter\n",
      "Epoch: 391 | Batch: 003 / 011 | Total loss: 0.976 | Reg loss: 0.025 | Tree loss: 0.976 | Accuracy: 0.691000 | 1.407 sec/iter\n",
      "Epoch: 391 | Batch: 004 / 011 | Total loss: 0.940 | Reg loss: 0.025 | Tree loss: 0.940 | Accuracy: 0.688500 | 1.407 sec/iter\n",
      "Epoch: 391 | Batch: 005 / 011 | Total loss: 0.944 | Reg loss: 0.025 | Tree loss: 0.944 | Accuracy: 0.683000 | 1.407 sec/iter\n",
      "Epoch: 391 | Batch: 006 / 011 | Total loss: 0.918 | Reg loss: 0.025 | Tree loss: 0.918 | Accuracy: 0.683000 | 1.407 sec/iter\n",
      "Epoch: 391 | Batch: 007 / 011 | Total loss: 0.946 | Reg loss: 0.025 | Tree loss: 0.946 | Accuracy: 0.654500 | 1.407 sec/iter\n",
      "Epoch: 391 | Batch: 008 / 011 | Total loss: 0.905 | Reg loss: 0.025 | Tree loss: 0.905 | Accuracy: 0.677000 | 1.407 sec/iter\n",
      "Epoch: 391 | Batch: 009 / 011 | Total loss: 0.907 | Reg loss: 0.025 | Tree loss: 0.907 | Accuracy: 0.660500 | 1.407 sec/iter\n",
      "Epoch: 391 | Batch: 010 / 011 | Total loss: 0.899 | Reg loss: 0.025 | Tree loss: 0.899 | Accuracy: 0.658703 | 1.407 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 392 | Batch: 000 / 011 | Total loss: 0.999 | Reg loss: 0.025 | Tree loss: 0.999 | Accuracy: 0.663000 | 1.408 sec/iter\n",
      "Epoch: 392 | Batch: 001 / 011 | Total loss: 0.981 | Reg loss: 0.025 | Tree loss: 0.981 | Accuracy: 0.677500 | 1.408 sec/iter\n",
      "Epoch: 392 | Batch: 002 / 011 | Total loss: 0.997 | Reg loss: 0.025 | Tree loss: 0.997 | Accuracy: 0.664000 | 1.408 sec/iter\n",
      "Epoch: 392 | Batch: 003 / 011 | Total loss: 0.939 | Reg loss: 0.025 | Tree loss: 0.939 | Accuracy: 0.705500 | 1.408 sec/iter\n",
      "Epoch: 392 | Batch: 004 / 011 | Total loss: 0.944 | Reg loss: 0.025 | Tree loss: 0.944 | Accuracy: 0.677500 | 1.408 sec/iter\n",
      "Epoch: 392 | Batch: 005 / 011 | Total loss: 0.947 | Reg loss: 0.025 | Tree loss: 0.947 | Accuracy: 0.678500 | 1.408 sec/iter\n",
      "Epoch: 392 | Batch: 006 / 011 | Total loss: 0.946 | Reg loss: 0.025 | Tree loss: 0.946 | Accuracy: 0.665500 | 1.408 sec/iter\n",
      "Epoch: 392 | Batch: 007 / 011 | Total loss: 0.937 | Reg loss: 0.025 | Tree loss: 0.937 | Accuracy: 0.665000 | 1.408 sec/iter\n",
      "Epoch: 392 | Batch: 008 / 011 | Total loss: 0.919 | Reg loss: 0.025 | Tree loss: 0.919 | Accuracy: 0.662000 | 1.408 sec/iter\n",
      "Epoch: 392 | Batch: 009 / 011 | Total loss: 0.942 | Reg loss: 0.025 | Tree loss: 0.942 | Accuracy: 0.657500 | 1.408 sec/iter\n",
      "Epoch: 392 | Batch: 010 / 011 | Total loss: 0.957 | Reg loss: 0.025 | Tree loss: 0.957 | Accuracy: 0.638225 | 1.408 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 393 | Batch: 000 / 011 | Total loss: 1.004 | Reg loss: 0.025 | Tree loss: 1.004 | Accuracy: 0.659000 | 1.408 sec/iter\n",
      "Epoch: 393 | Batch: 001 / 011 | Total loss: 0.970 | Reg loss: 0.025 | Tree loss: 0.970 | Accuracy: 0.679500 | 1.408 sec/iter\n",
      "Epoch: 393 | Batch: 002 / 011 | Total loss: 0.992 | Reg loss: 0.025 | Tree loss: 0.992 | Accuracy: 0.680000 | 1.408 sec/iter\n",
      "Epoch: 393 | Batch: 003 / 011 | Total loss: 0.959 | Reg loss: 0.025 | Tree loss: 0.959 | Accuracy: 0.698500 | 1.408 sec/iter\n",
      "Epoch: 393 | Batch: 004 / 011 | Total loss: 0.948 | Reg loss: 0.025 | Tree loss: 0.948 | Accuracy: 0.691500 | 1.408 sec/iter\n",
      "Epoch: 393 | Batch: 005 / 011 | Total loss: 0.918 | Reg loss: 0.025 | Tree loss: 0.918 | Accuracy: 0.694500 | 1.408 sec/iter\n",
      "Epoch: 393 | Batch: 006 / 011 | Total loss: 0.941 | Reg loss: 0.025 | Tree loss: 0.941 | Accuracy: 0.665500 | 1.408 sec/iter\n",
      "Epoch: 393 | Batch: 007 / 011 | Total loss: 0.942 | Reg loss: 0.025 | Tree loss: 0.942 | Accuracy: 0.650000 | 1.408 sec/iter\n",
      "Epoch: 393 | Batch: 008 / 011 | Total loss: 0.939 | Reg loss: 0.025 | Tree loss: 0.939 | Accuracy: 0.653000 | 1.408 sec/iter\n",
      "Epoch: 393 | Batch: 009 / 011 | Total loss: 0.934 | Reg loss: 0.025 | Tree loss: 0.934 | Accuracy: 0.642500 | 1.408 sec/iter\n",
      "Epoch: 393 | Batch: 010 / 011 | Total loss: 0.934 | Reg loss: 0.025 | Tree loss: 0.934 | Accuracy: 0.638225 | 1.408 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 394 | Batch: 000 / 011 | Total loss: 1.026 | Reg loss: 0.025 | Tree loss: 1.026 | Accuracy: 0.652000 | 1.409 sec/iter\n",
      "Epoch: 394 | Batch: 001 / 011 | Total loss: 0.986 | Reg loss: 0.025 | Tree loss: 0.986 | Accuracy: 0.673000 | 1.409 sec/iter\n",
      "Epoch: 394 | Batch: 002 / 011 | Total loss: 0.967 | Reg loss: 0.025 | Tree loss: 0.967 | Accuracy: 0.678500 | 1.409 sec/iter\n",
      "Epoch: 394 | Batch: 003 / 011 | Total loss: 0.961 | Reg loss: 0.025 | Tree loss: 0.961 | Accuracy: 0.680500 | 1.409 sec/iter\n",
      "Epoch: 394 | Batch: 004 / 011 | Total loss: 0.928 | Reg loss: 0.025 | Tree loss: 0.928 | Accuracy: 0.694000 | 1.409 sec/iter\n",
      "Epoch: 394 | Batch: 005 / 011 | Total loss: 0.943 | Reg loss: 0.025 | Tree loss: 0.943 | Accuracy: 0.674000 | 1.409 sec/iter\n",
      "Epoch: 394 | Batch: 006 / 011 | Total loss: 0.939 | Reg loss: 0.025 | Tree loss: 0.939 | Accuracy: 0.664000 | 1.409 sec/iter\n",
      "Epoch: 394 | Batch: 007 / 011 | Total loss: 0.950 | Reg loss: 0.025 | Tree loss: 0.950 | Accuracy: 0.652000 | 1.409 sec/iter\n",
      "Epoch: 394 | Batch: 008 / 011 | Total loss: 0.941 | Reg loss: 0.025 | Tree loss: 0.941 | Accuracy: 0.652500 | 1.409 sec/iter\n",
      "Epoch: 394 | Batch: 009 / 011 | Total loss: 0.905 | Reg loss: 0.025 | Tree loss: 0.905 | Accuracy: 0.669500 | 1.409 sec/iter\n",
      "Epoch: 394 | Batch: 010 / 011 | Total loss: 0.939 | Reg loss: 0.025 | Tree loss: 0.939 | Accuracy: 0.672355 | 1.409 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 395 | Batch: 000 / 011 | Total loss: 1.027 | Reg loss: 0.025 | Tree loss: 1.027 | Accuracy: 0.662500 | 1.409 sec/iter\n",
      "Epoch: 395 | Batch: 001 / 011 | Total loss: 1.007 | Reg loss: 0.025 | Tree loss: 1.007 | Accuracy: 0.669000 | 1.409 sec/iter\n",
      "Epoch: 395 | Batch: 002 / 011 | Total loss: 0.953 | Reg loss: 0.025 | Tree loss: 0.953 | Accuracy: 0.693000 | 1.409 sec/iter\n",
      "Epoch: 395 | Batch: 003 / 011 | Total loss: 0.960 | Reg loss: 0.025 | Tree loss: 0.960 | Accuracy: 0.677000 | 1.409 sec/iter\n",
      "Epoch: 395 | Batch: 004 / 011 | Total loss: 0.951 | Reg loss: 0.025 | Tree loss: 0.951 | Accuracy: 0.686000 | 1.409 sec/iter\n",
      "Epoch: 395 | Batch: 005 / 011 | Total loss: 0.937 | Reg loss: 0.025 | Tree loss: 0.937 | Accuracy: 0.675500 | 1.409 sec/iter\n",
      "Epoch: 395 | Batch: 006 / 011 | Total loss: 0.923 | Reg loss: 0.025 | Tree loss: 0.923 | Accuracy: 0.666500 | 1.409 sec/iter\n",
      "Epoch: 395 | Batch: 007 / 011 | Total loss: 0.934 | Reg loss: 0.025 | Tree loss: 0.934 | Accuracy: 0.656500 | 1.409 sec/iter\n",
      "Epoch: 395 | Batch: 008 / 011 | Total loss: 0.934 | Reg loss: 0.025 | Tree loss: 0.934 | Accuracy: 0.644500 | 1.409 sec/iter\n",
      "Epoch: 395 | Batch: 009 / 011 | Total loss: 0.925 | Reg loss: 0.025 | Tree loss: 0.925 | Accuracy: 0.659500 | 1.409 sec/iter\n",
      "Epoch: 395 | Batch: 010 / 011 | Total loss: 0.924 | Reg loss: 0.025 | Tree loss: 0.924 | Accuracy: 0.658703 | 1.409 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 396 | Batch: 000 / 011 | Total loss: 1.025 | Reg loss: 0.025 | Tree loss: 1.025 | Accuracy: 0.654000 | 1.41 sec/iter\n",
      "Epoch: 396 | Batch: 001 / 011 | Total loss: 1.005 | Reg loss: 0.025 | Tree loss: 1.005 | Accuracy: 0.661000 | 1.41 sec/iter\n",
      "Epoch: 396 | Batch: 002 / 011 | Total loss: 0.948 | Reg loss: 0.025 | Tree loss: 0.948 | Accuracy: 0.706000 | 1.41 sec/iter\n",
      "Epoch: 396 | Batch: 003 / 011 | Total loss: 0.985 | Reg loss: 0.025 | Tree loss: 0.985 | Accuracy: 0.670500 | 1.41 sec/iter\n",
      "Epoch: 396 | Batch: 004 / 011 | Total loss: 0.926 | Reg loss: 0.025 | Tree loss: 0.926 | Accuracy: 0.697500 | 1.41 sec/iter\n",
      "Epoch: 396 | Batch: 005 / 011 | Total loss: 0.964 | Reg loss: 0.025 | Tree loss: 0.964 | Accuracy: 0.666000 | 1.41 sec/iter\n",
      "Epoch: 396 | Batch: 006 / 011 | Total loss: 0.925 | Reg loss: 0.025 | Tree loss: 0.925 | Accuracy: 0.665000 | 1.41 sec/iter\n",
      "Epoch: 396 | Batch: 007 / 011 | Total loss: 0.919 | Reg loss: 0.025 | Tree loss: 0.919 | Accuracy: 0.666500 | 1.41 sec/iter\n",
      "Epoch: 396 | Batch: 008 / 011 | Total loss: 0.919 | Reg loss: 0.025 | Tree loss: 0.919 | Accuracy: 0.668500 | 1.41 sec/iter\n",
      "Epoch: 396 | Batch: 009 / 011 | Total loss: 0.916 | Reg loss: 0.025 | Tree loss: 0.916 | Accuracy: 0.658000 | 1.41 sec/iter\n",
      "Epoch: 396 | Batch: 010 / 011 | Total loss: 0.937 | Reg loss: 0.025 | Tree loss: 0.937 | Accuracy: 0.682594 | 1.41 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 397 | Batch: 000 / 011 | Total loss: 1.014 | Reg loss: 0.025 | Tree loss: 1.014 | Accuracy: 0.651500 | 1.41 sec/iter\n",
      "Epoch: 397 | Batch: 001 / 011 | Total loss: 0.995 | Reg loss: 0.025 | Tree loss: 0.995 | Accuracy: 0.675500 | 1.41 sec/iter\n",
      "Epoch: 397 | Batch: 002 / 011 | Total loss: 0.985 | Reg loss: 0.025 | Tree loss: 0.985 | Accuracy: 0.671000 | 1.41 sec/iter\n",
      "Epoch: 397 | Batch: 003 / 011 | Total loss: 0.950 | Reg loss: 0.025 | Tree loss: 0.950 | Accuracy: 0.703500 | 1.41 sec/iter\n",
      "Epoch: 397 | Batch: 004 / 011 | Total loss: 0.949 | Reg loss: 0.025 | Tree loss: 0.949 | Accuracy: 0.680500 | 1.41 sec/iter\n",
      "Epoch: 397 | Batch: 005 / 011 | Total loss: 0.954 | Reg loss: 0.025 | Tree loss: 0.954 | Accuracy: 0.680500 | 1.41 sec/iter\n",
      "Epoch: 397 | Batch: 006 / 011 | Total loss: 0.910 | Reg loss: 0.025 | Tree loss: 0.910 | Accuracy: 0.674500 | 1.41 sec/iter\n",
      "Epoch: 397 | Batch: 007 / 011 | Total loss: 0.922 | Reg loss: 0.025 | Tree loss: 0.922 | Accuracy: 0.671000 | 1.41 sec/iter\n",
      "Epoch: 397 | Batch: 008 / 011 | Total loss: 0.945 | Reg loss: 0.025 | Tree loss: 0.945 | Accuracy: 0.650500 | 1.41 sec/iter\n",
      "Epoch: 397 | Batch: 009 / 011 | Total loss: 0.920 | Reg loss: 0.025 | Tree loss: 0.920 | Accuracy: 0.663500 | 1.41 sec/iter\n",
      "Epoch: 397 | Batch: 010 / 011 | Total loss: 0.939 | Reg loss: 0.025 | Tree loss: 0.939 | Accuracy: 0.665529 | 1.41 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 398 | Batch: 000 / 011 | Total loss: 1.005 | Reg loss: 0.025 | Tree loss: 1.005 | Accuracy: 0.670000 | 1.411 sec/iter\n",
      "Epoch: 398 | Batch: 001 / 011 | Total loss: 0.999 | Reg loss: 0.025 | Tree loss: 0.999 | Accuracy: 0.661000 | 1.411 sec/iter\n",
      "Epoch: 398 | Batch: 002 / 011 | Total loss: 0.985 | Reg loss: 0.025 | Tree loss: 0.985 | Accuracy: 0.668000 | 1.411 sec/iter\n",
      "Epoch: 398 | Batch: 003 / 011 | Total loss: 0.933 | Reg loss: 0.025 | Tree loss: 0.933 | Accuracy: 0.713500 | 1.411 sec/iter\n",
      "Epoch: 398 | Batch: 004 / 011 | Total loss: 0.969 | Reg loss: 0.025 | Tree loss: 0.969 | Accuracy: 0.662000 | 1.411 sec/iter\n",
      "Epoch: 398 | Batch: 005 / 011 | Total loss: 0.947 | Reg loss: 0.025 | Tree loss: 0.947 | Accuracy: 0.666500 | 1.411 sec/iter\n",
      "Epoch: 398 | Batch: 006 / 011 | Total loss: 0.950 | Reg loss: 0.025 | Tree loss: 0.950 | Accuracy: 0.673000 | 1.411 sec/iter\n",
      "Epoch: 398 | Batch: 007 / 011 | Total loss: 0.883 | Reg loss: 0.025 | Tree loss: 0.883 | Accuracy: 0.698500 | 1.411 sec/iter\n",
      "Epoch: 398 | Batch: 008 / 011 | Total loss: 0.926 | Reg loss: 0.025 | Tree loss: 0.926 | Accuracy: 0.657500 | 1.411 sec/iter\n",
      "Epoch: 398 | Batch: 009 / 011 | Total loss: 0.947 | Reg loss: 0.025 | Tree loss: 0.947 | Accuracy: 0.648500 | 1.411 sec/iter\n",
      "Epoch: 398 | Batch: 010 / 011 | Total loss: 0.920 | Reg loss: 0.025 | Tree loss: 0.920 | Accuracy: 0.682594 | 1.411 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 399 | Batch: 000 / 011 | Total loss: 1.013 | Reg loss: 0.025 | Tree loss: 1.013 | Accuracy: 0.663500 | 1.411 sec/iter\n",
      "Epoch: 399 | Batch: 001 / 011 | Total loss: 1.001 | Reg loss: 0.025 | Tree loss: 1.001 | Accuracy: 0.674000 | 1.411 sec/iter\n",
      "Epoch: 399 | Batch: 002 / 011 | Total loss: 0.956 | Reg loss: 0.025 | Tree loss: 0.956 | Accuracy: 0.698000 | 1.411 sec/iter\n",
      "Epoch: 399 | Batch: 003 / 011 | Total loss: 0.962 | Reg loss: 0.025 | Tree loss: 0.962 | Accuracy: 0.685500 | 1.411 sec/iter\n",
      "Epoch: 399 | Batch: 004 / 011 | Total loss: 0.952 | Reg loss: 0.025 | Tree loss: 0.952 | Accuracy: 0.682500 | 1.411 sec/iter\n",
      "Epoch: 399 | Batch: 005 / 011 | Total loss: 0.951 | Reg loss: 0.025 | Tree loss: 0.951 | Accuracy: 0.668000 | 1.411 sec/iter\n",
      "Epoch: 399 | Batch: 006 / 011 | Total loss: 0.920 | Reg loss: 0.025 | Tree loss: 0.920 | Accuracy: 0.665500 | 1.411 sec/iter\n",
      "Epoch: 399 | Batch: 007 / 011 | Total loss: 0.932 | Reg loss: 0.025 | Tree loss: 0.932 | Accuracy: 0.662500 | 1.411 sec/iter\n",
      "Epoch: 399 | Batch: 008 / 011 | Total loss: 0.913 | Reg loss: 0.025 | Tree loss: 0.913 | Accuracy: 0.665000 | 1.411 sec/iter\n",
      "Epoch: 399 | Batch: 009 / 011 | Total loss: 0.922 | Reg loss: 0.025 | Tree loss: 0.922 | Accuracy: 0.668000 | 1.411 sec/iter\n",
      "Epoch: 399 | Batch: 010 / 011 | Total loss: 0.975 | Reg loss: 0.025 | Tree loss: 0.975 | Accuracy: 0.634812 | 1.411 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 400 | Batch: 000 / 011 | Total loss: 0.989 | Reg loss: 0.025 | Tree loss: 0.989 | Accuracy: 0.672000 | 1.412 sec/iter\n",
      "Epoch: 400 | Batch: 001 / 011 | Total loss: 1.000 | Reg loss: 0.025 | Tree loss: 1.000 | Accuracy: 0.654500 | 1.412 sec/iter\n",
      "Epoch: 400 | Batch: 002 / 011 | Total loss: 1.003 | Reg loss: 0.025 | Tree loss: 1.003 | Accuracy: 0.676000 | 1.412 sec/iter\n",
      "Epoch: 400 | Batch: 003 / 011 | Total loss: 0.950 | Reg loss: 0.025 | Tree loss: 0.950 | Accuracy: 0.689000 | 1.412 sec/iter\n",
      "Epoch: 400 | Batch: 004 / 011 | Total loss: 0.969 | Reg loss: 0.025 | Tree loss: 0.969 | Accuracy: 0.672500 | 1.412 sec/iter\n",
      "Epoch: 400 | Batch: 005 / 011 | Total loss: 0.939 | Reg loss: 0.025 | Tree loss: 0.939 | Accuracy: 0.672000 | 1.412 sec/iter\n",
      "Epoch: 400 | Batch: 006 / 011 | Total loss: 0.921 | Reg loss: 0.025 | Tree loss: 0.921 | Accuracy: 0.685000 | 1.412 sec/iter\n",
      "Epoch: 400 | Batch: 007 / 011 | Total loss: 0.936 | Reg loss: 0.025 | Tree loss: 0.936 | Accuracy: 0.660000 | 1.412 sec/iter\n",
      "Epoch: 400 | Batch: 008 / 011 | Total loss: 0.922 | Reg loss: 0.025 | Tree loss: 0.922 | Accuracy: 0.662500 | 1.412 sec/iter\n",
      "Epoch: 400 | Batch: 009 / 011 | Total loss: 0.919 | Reg loss: 0.025 | Tree loss: 0.919 | Accuracy: 0.659000 | 1.412 sec/iter\n",
      "Epoch: 400 | Batch: 010 / 011 | Total loss: 0.933 | Reg loss: 0.025 | Tree loss: 0.933 | Accuracy: 0.651877 | 1.412 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 401 | Batch: 000 / 011 | Total loss: 1.002 | Reg loss: 0.025 | Tree loss: 1.002 | Accuracy: 0.669500 | 1.412 sec/iter\n",
      "Epoch: 401 | Batch: 001 / 011 | Total loss: 1.001 | Reg loss: 0.025 | Tree loss: 1.001 | Accuracy: 0.668000 | 1.412 sec/iter\n",
      "Epoch: 401 | Batch: 002 / 011 | Total loss: 0.962 | Reg loss: 0.025 | Tree loss: 0.962 | Accuracy: 0.691000 | 1.412 sec/iter\n",
      "Epoch: 401 | Batch: 003 / 011 | Total loss: 0.949 | Reg loss: 0.025 | Tree loss: 0.949 | Accuracy: 0.677000 | 1.412 sec/iter\n",
      "Epoch: 401 | Batch: 004 / 011 | Total loss: 0.934 | Reg loss: 0.025 | Tree loss: 0.934 | Accuracy: 0.705500 | 1.412 sec/iter\n",
      "Epoch: 401 | Batch: 005 / 011 | Total loss: 0.958 | Reg loss: 0.025 | Tree loss: 0.958 | Accuracy: 0.683000 | 1.412 sec/iter\n",
      "Epoch: 401 | Batch: 006 / 011 | Total loss: 0.941 | Reg loss: 0.025 | Tree loss: 0.941 | Accuracy: 0.654000 | 1.412 sec/iter\n",
      "Epoch: 401 | Batch: 007 / 011 | Total loss: 0.928 | Reg loss: 0.025 | Tree loss: 0.928 | Accuracy: 0.676000 | 1.412 sec/iter\n",
      "Epoch: 401 | Batch: 008 / 011 | Total loss: 0.931 | Reg loss: 0.025 | Tree loss: 0.931 | Accuracy: 0.662500 | 1.412 sec/iter\n",
      "Epoch: 401 | Batch: 009 / 011 | Total loss: 0.923 | Reg loss: 0.025 | Tree loss: 0.923 | Accuracy: 0.665000 | 1.412 sec/iter\n",
      "Epoch: 401 | Batch: 010 / 011 | Total loss: 0.921 | Reg loss: 0.025 | Tree loss: 0.921 | Accuracy: 0.645051 | 1.412 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 402 | Batch: 000 / 011 | Total loss: 0.993 | Reg loss: 0.025 | Tree loss: 0.993 | Accuracy: 0.674000 | 1.413 sec/iter\n",
      "Epoch: 402 | Batch: 001 / 011 | Total loss: 0.996 | Reg loss: 0.025 | Tree loss: 0.996 | Accuracy: 0.667000 | 1.413 sec/iter\n",
      "Epoch: 402 | Batch: 002 / 011 | Total loss: 0.975 | Reg loss: 0.025 | Tree loss: 0.975 | Accuracy: 0.687000 | 1.413 sec/iter\n",
      "Epoch: 402 | Batch: 003 / 011 | Total loss: 0.945 | Reg loss: 0.025 | Tree loss: 0.945 | Accuracy: 0.699000 | 1.413 sec/iter\n",
      "Epoch: 402 | Batch: 004 / 011 | Total loss: 0.950 | Reg loss: 0.025 | Tree loss: 0.950 | Accuracy: 0.679500 | 1.413 sec/iter\n",
      "Epoch: 402 | Batch: 005 / 011 | Total loss: 0.940 | Reg loss: 0.025 | Tree loss: 0.940 | Accuracy: 0.679500 | 1.413 sec/iter\n",
      "Epoch: 402 | Batch: 006 / 011 | Total loss: 0.946 | Reg loss: 0.025 | Tree loss: 0.946 | Accuracy: 0.665000 | 1.413 sec/iter\n",
      "Epoch: 402 | Batch: 007 / 011 | Total loss: 0.921 | Reg loss: 0.025 | Tree loss: 0.921 | Accuracy: 0.667000 | 1.413 sec/iter\n",
      "Epoch: 402 | Batch: 008 / 011 | Total loss: 0.919 | Reg loss: 0.025 | Tree loss: 0.919 | Accuracy: 0.668000 | 1.413 sec/iter\n",
      "Epoch: 402 | Batch: 009 / 011 | Total loss: 0.923 | Reg loss: 0.025 | Tree loss: 0.923 | Accuracy: 0.664000 | 1.413 sec/iter\n",
      "Epoch: 402 | Batch: 010 / 011 | Total loss: 0.938 | Reg loss: 0.025 | Tree loss: 0.938 | Accuracy: 0.662116 | 1.413 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 403 | Batch: 000 / 011 | Total loss: 1.021 | Reg loss: 0.025 | Tree loss: 1.021 | Accuracy: 0.658000 | 1.413 sec/iter\n",
      "Epoch: 403 | Batch: 001 / 011 | Total loss: 0.988 | Reg loss: 0.025 | Tree loss: 0.988 | Accuracy: 0.676000 | 1.413 sec/iter\n",
      "Epoch: 403 | Batch: 002 / 011 | Total loss: 0.989 | Reg loss: 0.025 | Tree loss: 0.989 | Accuracy: 0.674500 | 1.413 sec/iter\n",
      "Epoch: 403 | Batch: 003 / 011 | Total loss: 0.962 | Reg loss: 0.025 | Tree loss: 0.962 | Accuracy: 0.684000 | 1.413 sec/iter\n",
      "Epoch: 403 | Batch: 004 / 011 | Total loss: 0.944 | Reg loss: 0.025 | Tree loss: 0.944 | Accuracy: 0.685000 | 1.413 sec/iter\n",
      "Epoch: 403 | Batch: 005 / 011 | Total loss: 0.955 | Reg loss: 0.025 | Tree loss: 0.955 | Accuracy: 0.673500 | 1.413 sec/iter\n",
      "Epoch: 403 | Batch: 006 / 011 | Total loss: 0.930 | Reg loss: 0.025 | Tree loss: 0.930 | Accuracy: 0.678500 | 1.413 sec/iter\n",
      "Epoch: 403 | Batch: 007 / 011 | Total loss: 0.912 | Reg loss: 0.025 | Tree loss: 0.912 | Accuracy: 0.672500 | 1.413 sec/iter\n",
      "Epoch: 403 | Batch: 008 / 011 | Total loss: 0.916 | Reg loss: 0.025 | Tree loss: 0.916 | Accuracy: 0.673500 | 1.413 sec/iter\n",
      "Epoch: 403 | Batch: 009 / 011 | Total loss: 0.923 | Reg loss: 0.025 | Tree loss: 0.923 | Accuracy: 0.671000 | 1.413 sec/iter\n",
      "Epoch: 403 | Batch: 010 / 011 | Total loss: 0.967 | Reg loss: 0.025 | Tree loss: 0.967 | Accuracy: 0.617747 | 1.413 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 8: 0.9723756906077345\n",
      "Epoch: 404 | Batch: 000 / 011 | Total loss: 1.021 | Reg loss: 0.025 | Tree loss: 1.021 | Accuracy: 0.662000 | 1.413 sec/iter\n",
      "Epoch: 404 | Batch: 001 / 011 | Total loss: 1.017 | Reg loss: 0.025 | Tree loss: 1.017 | Accuracy: 0.655000 | 1.413 sec/iter\n",
      "Epoch: 404 | Batch: 002 / 011 | Total loss: 0.999 | Reg loss: 0.025 | Tree loss: 0.999 | Accuracy: 0.669000 | 1.413 sec/iter\n",
      "Epoch: 404 | Batch: 003 / 011 | Total loss: 0.969 | Reg loss: 0.025 | Tree loss: 0.969 | Accuracy: 0.678500 | 1.413 sec/iter\n",
      "Epoch: 404 | Batch: 004 / 011 | Total loss: 0.915 | Reg loss: 0.025 | Tree loss: 0.915 | Accuracy: 0.712000 | 1.413 sec/iter\n",
      "Epoch: 404 | Batch: 005 / 011 | Total loss: 0.916 | Reg loss: 0.025 | Tree loss: 0.916 | Accuracy: 0.696500 | 1.413 sec/iter\n",
      "Epoch: 404 | Batch: 006 / 011 | Total loss: 0.906 | Reg loss: 0.025 | Tree loss: 0.906 | Accuracy: 0.691500 | 1.413 sec/iter\n",
      "Epoch: 404 | Batch: 007 / 011 | Total loss: 0.916 | Reg loss: 0.025 | Tree loss: 0.916 | Accuracy: 0.672000 | 1.413 sec/iter\n",
      "Epoch: 404 | Batch: 008 / 011 | Total loss: 0.953 | Reg loss: 0.025 | Tree loss: 0.953 | Accuracy: 0.642500 | 1.413 sec/iter\n",
      "Epoch: 404 | Batch: 009 / 011 | Total loss: 0.906 | Reg loss: 0.025 | Tree loss: 0.906 | Accuracy: 0.662500 | 1.413 sec/iter\n",
      "Epoch: 404 | Batch: 010 / 011 | Total loss: 0.954 | Reg loss: 0.025 | Tree loss: 0.954 | Accuracy: 0.651877 | 1.413 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 405 | Batch: 000 / 011 | Total loss: 1.017 | Reg loss: 0.025 | Tree loss: 1.017 | Accuracy: 0.649500 | 1.414 sec/iter\n",
      "Epoch: 405 | Batch: 001 / 011 | Total loss: 0.986 | Reg loss: 0.025 | Tree loss: 0.986 | Accuracy: 0.689000 | 1.413 sec/iter\n",
      "Epoch: 405 | Batch: 002 / 011 | Total loss: 0.977 | Reg loss: 0.025 | Tree loss: 0.977 | Accuracy: 0.671500 | 1.413 sec/iter\n",
      "Epoch: 405 | Batch: 003 / 011 | Total loss: 0.960 | Reg loss: 0.025 | Tree loss: 0.960 | Accuracy: 0.692500 | 1.413 sec/iter\n",
      "Epoch: 405 | Batch: 004 / 011 | Total loss: 0.964 | Reg loss: 0.025 | Tree loss: 0.964 | Accuracy: 0.676500 | 1.413 sec/iter\n",
      "Epoch: 405 | Batch: 005 / 011 | Total loss: 0.927 | Reg loss: 0.025 | Tree loss: 0.927 | Accuracy: 0.704500 | 1.413 sec/iter\n",
      "Epoch: 405 | Batch: 006 / 011 | Total loss: 0.921 | Reg loss: 0.025 | Tree loss: 0.921 | Accuracy: 0.677500 | 1.413 sec/iter\n",
      "Epoch: 405 | Batch: 007 / 011 | Total loss: 0.938 | Reg loss: 0.025 | Tree loss: 0.938 | Accuracy: 0.649500 | 1.413 sec/iter\n",
      "Epoch: 405 | Batch: 008 / 011 | Total loss: 0.909 | Reg loss: 0.025 | Tree loss: 0.909 | Accuracy: 0.673500 | 1.413 sec/iter\n",
      "Epoch: 405 | Batch: 009 / 011 | Total loss: 0.924 | Reg loss: 0.025 | Tree loss: 0.924 | Accuracy: 0.650000 | 1.413 sec/iter\n",
      "Epoch: 405 | Batch: 010 / 011 | Total loss: 0.914 | Reg loss: 0.025 | Tree loss: 0.914 | Accuracy: 0.651877 | 1.413 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 406 | Batch: 000 / 011 | Total loss: 1.013 | Reg loss: 0.025 | Tree loss: 1.013 | Accuracy: 0.664000 | 1.414 sec/iter\n",
      "Epoch: 406 | Batch: 001 / 011 | Total loss: 1.003 | Reg loss: 0.025 | Tree loss: 1.003 | Accuracy: 0.669000 | 1.413 sec/iter\n",
      "Epoch: 406 | Batch: 002 / 011 | Total loss: 0.958 | Reg loss: 0.025 | Tree loss: 0.958 | Accuracy: 0.698500 | 1.413 sec/iter\n",
      "Epoch: 406 | Batch: 003 / 011 | Total loss: 0.946 | Reg loss: 0.025 | Tree loss: 0.946 | Accuracy: 0.705500 | 1.413 sec/iter\n",
      "Epoch: 406 | Batch: 004 / 011 | Total loss: 0.946 | Reg loss: 0.025 | Tree loss: 0.946 | Accuracy: 0.672000 | 1.413 sec/iter\n",
      "Epoch: 406 | Batch: 005 / 011 | Total loss: 0.944 | Reg loss: 0.025 | Tree loss: 0.944 | Accuracy: 0.658000 | 1.414 sec/iter\n",
      "Epoch: 406 | Batch: 006 / 011 | Total loss: 0.922 | Reg loss: 0.025 | Tree loss: 0.922 | Accuracy: 0.675500 | 1.414 sec/iter\n",
      "Epoch: 406 | Batch: 007 / 011 | Total loss: 0.957 | Reg loss: 0.025 | Tree loss: 0.957 | Accuracy: 0.643000 | 1.414 sec/iter\n",
      "Epoch: 406 | Batch: 008 / 011 | Total loss: 0.927 | Reg loss: 0.025 | Tree loss: 0.927 | Accuracy: 0.658000 | 1.414 sec/iter\n",
      "Epoch: 406 | Batch: 009 / 011 | Total loss: 0.920 | Reg loss: 0.025 | Tree loss: 0.920 | Accuracy: 0.670500 | 1.414 sec/iter\n",
      "Epoch: 406 | Batch: 010 / 011 | Total loss: 0.907 | Reg loss: 0.025 | Tree loss: 0.907 | Accuracy: 0.679181 | 1.414 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 407 | Batch: 000 / 011 | Total loss: 1.033 | Reg loss: 0.025 | Tree loss: 1.033 | Accuracy: 0.643500 | 1.414 sec/iter\n",
      "Epoch: 407 | Batch: 001 / 011 | Total loss: 0.994 | Reg loss: 0.025 | Tree loss: 0.994 | Accuracy: 0.656500 | 1.414 sec/iter\n",
      "Epoch: 407 | Batch: 002 / 011 | Total loss: 0.958 | Reg loss: 0.025 | Tree loss: 0.958 | Accuracy: 0.692000 | 1.414 sec/iter\n",
      "Epoch: 407 | Batch: 003 / 011 | Total loss: 0.970 | Reg loss: 0.025 | Tree loss: 0.970 | Accuracy: 0.686000 | 1.414 sec/iter\n",
      "Epoch: 407 | Batch: 004 / 011 | Total loss: 0.923 | Reg loss: 0.025 | Tree loss: 0.923 | Accuracy: 0.697500 | 1.414 sec/iter\n",
      "Epoch: 407 | Batch: 005 / 011 | Total loss: 0.914 | Reg loss: 0.025 | Tree loss: 0.914 | Accuracy: 0.690500 | 1.414 sec/iter\n",
      "Epoch: 407 | Batch: 006 / 011 | Total loss: 0.932 | Reg loss: 0.025 | Tree loss: 0.932 | Accuracy: 0.673500 | 1.414 sec/iter\n",
      "Epoch: 407 | Batch: 007 / 011 | Total loss: 0.935 | Reg loss: 0.025 | Tree loss: 0.935 | Accuracy: 0.664500 | 1.414 sec/iter\n",
      "Epoch: 407 | Batch: 008 / 011 | Total loss: 0.936 | Reg loss: 0.025 | Tree loss: 0.936 | Accuracy: 0.661500 | 1.414 sec/iter\n",
      "Epoch: 407 | Batch: 009 / 011 | Total loss: 0.925 | Reg loss: 0.025 | Tree loss: 0.925 | Accuracy: 0.655500 | 1.414 sec/iter\n",
      "Epoch: 407 | Batch: 010 / 011 | Total loss: 0.934 | Reg loss: 0.025 | Tree loss: 0.934 | Accuracy: 0.645051 | 1.414 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 408 | Batch: 000 / 011 | Total loss: 1.015 | Reg loss: 0.025 | Tree loss: 1.015 | Accuracy: 0.668500 | 1.414 sec/iter\n",
      "Epoch: 408 | Batch: 001 / 011 | Total loss: 0.985 | Reg loss: 0.025 | Tree loss: 0.985 | Accuracy: 0.678500 | 1.414 sec/iter\n",
      "Epoch: 408 | Batch: 002 / 011 | Total loss: 0.984 | Reg loss: 0.025 | Tree loss: 0.984 | Accuracy: 0.672000 | 1.414 sec/iter\n",
      "Epoch: 408 | Batch: 003 / 011 | Total loss: 0.937 | Reg loss: 0.025 | Tree loss: 0.937 | Accuracy: 0.702500 | 1.414 sec/iter\n",
      "Epoch: 408 | Batch: 004 / 011 | Total loss: 0.953 | Reg loss: 0.025 | Tree loss: 0.953 | Accuracy: 0.679500 | 1.414 sec/iter\n",
      "Epoch: 408 | Batch: 005 / 011 | Total loss: 0.968 | Reg loss: 0.025 | Tree loss: 0.968 | Accuracy: 0.664500 | 1.414 sec/iter\n",
      "Epoch: 408 | Batch: 006 / 011 | Total loss: 0.911 | Reg loss: 0.025 | Tree loss: 0.911 | Accuracy: 0.678500 | 1.414 sec/iter\n",
      "Epoch: 408 | Batch: 007 / 011 | Total loss: 0.895 | Reg loss: 0.025 | Tree loss: 0.895 | Accuracy: 0.678000 | 1.414 sec/iter\n",
      "Epoch: 408 | Batch: 008 / 011 | Total loss: 0.938 | Reg loss: 0.025 | Tree loss: 0.938 | Accuracy: 0.656500 | 1.414 sec/iter\n",
      "Epoch: 408 | Batch: 009 / 011 | Total loss: 0.929 | Reg loss: 0.025 | Tree loss: 0.929 | Accuracy: 0.650500 | 1.414 sec/iter\n",
      "Epoch: 408 | Batch: 010 / 011 | Total loss: 0.938 | Reg loss: 0.025 | Tree loss: 0.938 | Accuracy: 0.627986 | 1.414 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 409 | Batch: 000 / 011 | Total loss: 0.994 | Reg loss: 0.025 | Tree loss: 0.994 | Accuracy: 0.666500 | 1.414 sec/iter\n",
      "Epoch: 409 | Batch: 001 / 011 | Total loss: 1.017 | Reg loss: 0.025 | Tree loss: 1.017 | Accuracy: 0.660500 | 1.414 sec/iter\n",
      "Epoch: 409 | Batch: 002 / 011 | Total loss: 0.991 | Reg loss: 0.025 | Tree loss: 0.991 | Accuracy: 0.678000 | 1.414 sec/iter\n",
      "Epoch: 409 | Batch: 003 / 011 | Total loss: 0.977 | Reg loss: 0.025 | Tree loss: 0.977 | Accuracy: 0.667000 | 1.414 sec/iter\n",
      "Epoch: 409 | Batch: 004 / 011 | Total loss: 0.956 | Reg loss: 0.025 | Tree loss: 0.956 | Accuracy: 0.674500 | 1.414 sec/iter\n",
      "Epoch: 409 | Batch: 005 / 011 | Total loss: 0.945 | Reg loss: 0.025 | Tree loss: 0.945 | Accuracy: 0.683000 | 1.414 sec/iter\n",
      "Epoch: 409 | Batch: 006 / 011 | Total loss: 0.916 | Reg loss: 0.025 | Tree loss: 0.916 | Accuracy: 0.678500 | 1.414 sec/iter\n",
      "Epoch: 409 | Batch: 007 / 011 | Total loss: 0.909 | Reg loss: 0.025 | Tree loss: 0.909 | Accuracy: 0.685000 | 1.414 sec/iter\n",
      "Epoch: 409 | Batch: 008 / 011 | Total loss: 0.910 | Reg loss: 0.025 | Tree loss: 0.910 | Accuracy: 0.669500 | 1.414 sec/iter\n",
      "Epoch: 409 | Batch: 009 / 011 | Total loss: 0.928 | Reg loss: 0.025 | Tree loss: 0.928 | Accuracy: 0.661500 | 1.414 sec/iter\n",
      "Epoch: 409 | Batch: 010 / 011 | Total loss: 0.884 | Reg loss: 0.025 | Tree loss: 0.884 | Accuracy: 0.679181 | 1.414 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 410 | Batch: 000 / 011 | Total loss: 1.009 | Reg loss: 0.025 | Tree loss: 1.009 | Accuracy: 0.668500 | 1.415 sec/iter\n",
      "Epoch: 410 | Batch: 001 / 011 | Total loss: 0.989 | Reg loss: 0.025 | Tree loss: 0.989 | Accuracy: 0.678500 | 1.415 sec/iter\n",
      "Epoch: 410 | Batch: 002 / 011 | Total loss: 0.980 | Reg loss: 0.025 | Tree loss: 0.980 | Accuracy: 0.680500 | 1.415 sec/iter\n",
      "Epoch: 410 | Batch: 003 / 011 | Total loss: 0.920 | Reg loss: 0.025 | Tree loss: 0.920 | Accuracy: 0.707500 | 1.415 sec/iter\n",
      "Epoch: 410 | Batch: 004 / 011 | Total loss: 0.955 | Reg loss: 0.025 | Tree loss: 0.955 | Accuracy: 0.698500 | 1.415 sec/iter\n",
      "Epoch: 410 | Batch: 005 / 011 | Total loss: 0.951 | Reg loss: 0.025 | Tree loss: 0.951 | Accuracy: 0.665500 | 1.415 sec/iter\n",
      "Epoch: 410 | Batch: 006 / 011 | Total loss: 0.929 | Reg loss: 0.025 | Tree loss: 0.929 | Accuracy: 0.670500 | 1.415 sec/iter\n",
      "Epoch: 410 | Batch: 007 / 011 | Total loss: 0.927 | Reg loss: 0.025 | Tree loss: 0.927 | Accuracy: 0.653000 | 1.415 sec/iter\n",
      "Epoch: 410 | Batch: 008 / 011 | Total loss: 0.929 | Reg loss: 0.025 | Tree loss: 0.929 | Accuracy: 0.650000 | 1.415 sec/iter\n",
      "Epoch: 410 | Batch: 009 / 011 | Total loss: 0.931 | Reg loss: 0.025 | Tree loss: 0.931 | Accuracy: 0.655500 | 1.415 sec/iter\n",
      "Epoch: 410 | Batch: 010 / 011 | Total loss: 0.947 | Reg loss: 0.025 | Tree loss: 0.947 | Accuracy: 0.648464 | 1.415 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 411 | Batch: 000 / 011 | Total loss: 0.997 | Reg loss: 0.025 | Tree loss: 0.997 | Accuracy: 0.670000 | 1.415 sec/iter\n",
      "Epoch: 411 | Batch: 001 / 011 | Total loss: 0.995 | Reg loss: 0.025 | Tree loss: 0.995 | Accuracy: 0.664500 | 1.415 sec/iter\n",
      "Epoch: 411 | Batch: 002 / 011 | Total loss: 0.975 | Reg loss: 0.025 | Tree loss: 0.975 | Accuracy: 0.676500 | 1.415 sec/iter\n",
      "Epoch: 411 | Batch: 003 / 011 | Total loss: 0.966 | Reg loss: 0.025 | Tree loss: 0.966 | Accuracy: 0.680500 | 1.415 sec/iter\n",
      "Epoch: 411 | Batch: 004 / 011 | Total loss: 0.931 | Reg loss: 0.025 | Tree loss: 0.931 | Accuracy: 0.678000 | 1.415 sec/iter\n",
      "Epoch: 411 | Batch: 005 / 011 | Total loss: 0.937 | Reg loss: 0.025 | Tree loss: 0.937 | Accuracy: 0.682500 | 1.415 sec/iter\n",
      "Epoch: 411 | Batch: 006 / 011 | Total loss: 0.939 | Reg loss: 0.025 | Tree loss: 0.939 | Accuracy: 0.659500 | 1.415 sec/iter\n",
      "Epoch: 411 | Batch: 007 / 011 | Total loss: 0.946 | Reg loss: 0.025 | Tree loss: 0.946 | Accuracy: 0.660500 | 1.415 sec/iter\n",
      "Epoch: 411 | Batch: 008 / 011 | Total loss: 0.914 | Reg loss: 0.025 | Tree loss: 0.914 | Accuracy: 0.675500 | 1.415 sec/iter\n",
      "Epoch: 411 | Batch: 009 / 011 | Total loss: 0.929 | Reg loss: 0.025 | Tree loss: 0.929 | Accuracy: 0.655000 | 1.415 sec/iter\n",
      "Epoch: 411 | Batch: 010 / 011 | Total loss: 0.906 | Reg loss: 0.025 | Tree loss: 0.906 | Accuracy: 0.692833 | 1.415 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 412 | Batch: 000 / 011 | Total loss: 1.021 | Reg loss: 0.025 | Tree loss: 1.021 | Accuracy: 0.663500 | 1.416 sec/iter\n",
      "Epoch: 412 | Batch: 001 / 011 | Total loss: 0.993 | Reg loss: 0.025 | Tree loss: 0.993 | Accuracy: 0.672000 | 1.416 sec/iter\n",
      "Epoch: 412 | Batch: 002 / 011 | Total loss: 0.990 | Reg loss: 0.025 | Tree loss: 0.990 | Accuracy: 0.677500 | 1.416 sec/iter\n",
      "Epoch: 412 | Batch: 003 / 011 | Total loss: 0.974 | Reg loss: 0.025 | Tree loss: 0.974 | Accuracy: 0.685000 | 1.416 sec/iter\n",
      "Epoch: 412 | Batch: 004 / 011 | Total loss: 0.922 | Reg loss: 0.025 | Tree loss: 0.922 | Accuracy: 0.694000 | 1.416 sec/iter\n",
      "Epoch: 412 | Batch: 005 / 011 | Total loss: 0.934 | Reg loss: 0.025 | Tree loss: 0.934 | Accuracy: 0.673000 | 1.416 sec/iter\n",
      "Epoch: 412 | Batch: 006 / 011 | Total loss: 0.947 | Reg loss: 0.025 | Tree loss: 0.947 | Accuracy: 0.658500 | 1.416 sec/iter\n",
      "Epoch: 412 | Batch: 007 / 011 | Total loss: 0.914 | Reg loss: 0.025 | Tree loss: 0.914 | Accuracy: 0.677000 | 1.416 sec/iter\n",
      "Epoch: 412 | Batch: 008 / 011 | Total loss: 0.903 | Reg loss: 0.025 | Tree loss: 0.903 | Accuracy: 0.678000 | 1.416 sec/iter\n",
      "Epoch: 412 | Batch: 009 / 011 | Total loss: 0.919 | Reg loss: 0.025 | Tree loss: 0.919 | Accuracy: 0.658000 | 1.416 sec/iter\n",
      "Epoch: 412 | Batch: 010 / 011 | Total loss: 0.895 | Reg loss: 0.025 | Tree loss: 0.895 | Accuracy: 0.692833 | 1.416 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 413 | Batch: 000 / 011 | Total loss: 1.023 | Reg loss: 0.025 | Tree loss: 1.023 | Accuracy: 0.653500 | 1.416 sec/iter\n",
      "Epoch: 413 | Batch: 001 / 011 | Total loss: 0.984 | Reg loss: 0.025 | Tree loss: 0.984 | Accuracy: 0.677000 | 1.416 sec/iter\n",
      "Epoch: 413 | Batch: 002 / 011 | Total loss: 0.997 | Reg loss: 0.025 | Tree loss: 0.997 | Accuracy: 0.669000 | 1.416 sec/iter\n",
      "Epoch: 413 | Batch: 003 / 011 | Total loss: 0.948 | Reg loss: 0.025 | Tree loss: 0.948 | Accuracy: 0.686500 | 1.416 sec/iter\n",
      "Epoch: 413 | Batch: 004 / 011 | Total loss: 0.929 | Reg loss: 0.025 | Tree loss: 0.929 | Accuracy: 0.697000 | 1.416 sec/iter\n",
      "Epoch: 413 | Batch: 005 / 011 | Total loss: 0.935 | Reg loss: 0.025 | Tree loss: 0.935 | Accuracy: 0.672000 | 1.416 sec/iter\n",
      "Epoch: 413 | Batch: 006 / 011 | Total loss: 0.931 | Reg loss: 0.025 | Tree loss: 0.931 | Accuracy: 0.666500 | 1.416 sec/iter\n",
      "Epoch: 413 | Batch: 007 / 011 | Total loss: 0.931 | Reg loss: 0.025 | Tree loss: 0.931 | Accuracy: 0.666500 | 1.416 sec/iter\n",
      "Epoch: 413 | Batch: 008 / 011 | Total loss: 0.923 | Reg loss: 0.025 | Tree loss: 0.923 | Accuracy: 0.663500 | 1.416 sec/iter\n",
      "Epoch: 413 | Batch: 009 / 011 | Total loss: 0.923 | Reg loss: 0.025 | Tree loss: 0.923 | Accuracy: 0.661000 | 1.416 sec/iter\n",
      "Epoch: 413 | Batch: 010 / 011 | Total loss: 0.935 | Reg loss: 0.025 | Tree loss: 0.935 | Accuracy: 0.658703 | 1.416 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 414 | Batch: 000 / 011 | Total loss: 0.989 | Reg loss: 0.025 | Tree loss: 0.989 | Accuracy: 0.667000 | 1.417 sec/iter\n",
      "Epoch: 414 | Batch: 001 / 011 | Total loss: 1.007 | Reg loss: 0.025 | Tree loss: 1.007 | Accuracy: 0.662000 | 1.417 sec/iter\n",
      "Epoch: 414 | Batch: 002 / 011 | Total loss: 0.990 | Reg loss: 0.025 | Tree loss: 0.990 | Accuracy: 0.665500 | 1.417 sec/iter\n",
      "Epoch: 414 | Batch: 003 / 011 | Total loss: 0.960 | Reg loss: 0.025 | Tree loss: 0.960 | Accuracy: 0.685000 | 1.417 sec/iter\n",
      "Epoch: 414 | Batch: 004 / 011 | Total loss: 0.927 | Reg loss: 0.025 | Tree loss: 0.927 | Accuracy: 0.700500 | 1.417 sec/iter\n",
      "Epoch: 414 | Batch: 005 / 011 | Total loss: 0.960 | Reg loss: 0.025 | Tree loss: 0.960 | Accuracy: 0.662000 | 1.417 sec/iter\n",
      "Epoch: 414 | Batch: 006 / 011 | Total loss: 0.929 | Reg loss: 0.025 | Tree loss: 0.929 | Accuracy: 0.677000 | 1.417 sec/iter\n",
      "Epoch: 414 | Batch: 007 / 011 | Total loss: 0.944 | Reg loss: 0.025 | Tree loss: 0.944 | Accuracy: 0.664000 | 1.417 sec/iter\n",
      "Epoch: 414 | Batch: 008 / 011 | Total loss: 0.919 | Reg loss: 0.025 | Tree loss: 0.919 | Accuracy: 0.676000 | 1.417 sec/iter\n",
      "Epoch: 414 | Batch: 009 / 011 | Total loss: 0.916 | Reg loss: 0.025 | Tree loss: 0.916 | Accuracy: 0.669000 | 1.417 sec/iter\n",
      "Epoch: 414 | Batch: 010 / 011 | Total loss: 0.941 | Reg loss: 0.025 | Tree loss: 0.941 | Accuracy: 0.662116 | 1.417 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 415 | Batch: 000 / 011 | Total loss: 1.027 | Reg loss: 0.025 | Tree loss: 1.027 | Accuracy: 0.661000 | 1.417 sec/iter\n",
      "Epoch: 415 | Batch: 001 / 011 | Total loss: 0.988 | Reg loss: 0.025 | Tree loss: 0.988 | Accuracy: 0.679000 | 1.417 sec/iter\n",
      "Epoch: 415 | Batch: 002 / 011 | Total loss: 0.998 | Reg loss: 0.025 | Tree loss: 0.998 | Accuracy: 0.655000 | 1.417 sec/iter\n",
      "Epoch: 415 | Batch: 003 / 011 | Total loss: 0.960 | Reg loss: 0.025 | Tree loss: 0.960 | Accuracy: 0.673500 | 1.417 sec/iter\n",
      "Epoch: 415 | Batch: 004 / 011 | Total loss: 0.922 | Reg loss: 0.025 | Tree loss: 0.922 | Accuracy: 0.693500 | 1.417 sec/iter\n",
      "Epoch: 415 | Batch: 005 / 011 | Total loss: 0.918 | Reg loss: 0.025 | Tree loss: 0.918 | Accuracy: 0.685000 | 1.417 sec/iter\n",
      "Epoch: 415 | Batch: 006 / 011 | Total loss: 0.920 | Reg loss: 0.025 | Tree loss: 0.920 | Accuracy: 0.690500 | 1.417 sec/iter\n",
      "Epoch: 415 | Batch: 007 / 011 | Total loss: 0.925 | Reg loss: 0.025 | Tree loss: 0.925 | Accuracy: 0.670000 | 1.417 sec/iter\n",
      "Epoch: 415 | Batch: 008 / 011 | Total loss: 0.944 | Reg loss: 0.025 | Tree loss: 0.944 | Accuracy: 0.662000 | 1.417 sec/iter\n",
      "Epoch: 415 | Batch: 009 / 011 | Total loss: 0.921 | Reg loss: 0.025 | Tree loss: 0.921 | Accuracy: 0.656500 | 1.417 sec/iter\n",
      "Epoch: 415 | Batch: 010 / 011 | Total loss: 0.932 | Reg loss: 0.025 | Tree loss: 0.932 | Accuracy: 0.645051 | 1.417 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 416 | Batch: 000 / 011 | Total loss: 1.011 | Reg loss: 0.025 | Tree loss: 1.011 | Accuracy: 0.657500 | 1.418 sec/iter\n",
      "Epoch: 416 | Batch: 001 / 011 | Total loss: 1.002 | Reg loss: 0.025 | Tree loss: 1.002 | Accuracy: 0.666500 | 1.418 sec/iter\n",
      "Epoch: 416 | Batch: 002 / 011 | Total loss: 0.964 | Reg loss: 0.025 | Tree loss: 0.964 | Accuracy: 0.698000 | 1.418 sec/iter\n",
      "Epoch: 416 | Batch: 003 / 011 | Total loss: 0.968 | Reg loss: 0.025 | Tree loss: 0.968 | Accuracy: 0.688000 | 1.418 sec/iter\n",
      "Epoch: 416 | Batch: 004 / 011 | Total loss: 0.943 | Reg loss: 0.025 | Tree loss: 0.943 | Accuracy: 0.675500 | 1.418 sec/iter\n",
      "Epoch: 416 | Batch: 005 / 011 | Total loss: 0.945 | Reg loss: 0.025 | Tree loss: 0.945 | Accuracy: 0.684500 | 1.418 sec/iter\n",
      "Epoch: 416 | Batch: 006 / 011 | Total loss: 0.941 | Reg loss: 0.025 | Tree loss: 0.941 | Accuracy: 0.667500 | 1.418 sec/iter\n",
      "Epoch: 416 | Batch: 007 / 011 | Total loss: 0.919 | Reg loss: 0.025 | Tree loss: 0.919 | Accuracy: 0.674000 | 1.418 sec/iter\n",
      "Epoch: 416 | Batch: 008 / 011 | Total loss: 0.918 | Reg loss: 0.025 | Tree loss: 0.918 | Accuracy: 0.670000 | 1.418 sec/iter\n",
      "Epoch: 416 | Batch: 009 / 011 | Total loss: 0.914 | Reg loss: 0.025 | Tree loss: 0.914 | Accuracy: 0.659000 | 1.418 sec/iter\n",
      "Epoch: 416 | Batch: 010 / 011 | Total loss: 0.965 | Reg loss: 0.025 | Tree loss: 0.965 | Accuracy: 0.614334 | 1.418 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 417 | Batch: 000 / 011 | Total loss: 1.020 | Reg loss: 0.025 | Tree loss: 1.020 | Accuracy: 0.661000 | 1.418 sec/iter\n",
      "Epoch: 417 | Batch: 001 / 011 | Total loss: 1.003 | Reg loss: 0.025 | Tree loss: 1.003 | Accuracy: 0.657500 | 1.418 sec/iter\n",
      "Epoch: 417 | Batch: 002 / 011 | Total loss: 0.966 | Reg loss: 0.025 | Tree loss: 0.966 | Accuracy: 0.683000 | 1.418 sec/iter\n",
      "Epoch: 417 | Batch: 003 / 011 | Total loss: 0.955 | Reg loss: 0.025 | Tree loss: 0.955 | Accuracy: 0.688500 | 1.418 sec/iter\n",
      "Epoch: 417 | Batch: 004 / 011 | Total loss: 0.945 | Reg loss: 0.025 | Tree loss: 0.945 | Accuracy: 0.683500 | 1.418 sec/iter\n",
      "Epoch: 417 | Batch: 005 / 011 | Total loss: 0.919 | Reg loss: 0.025 | Tree loss: 0.919 | Accuracy: 0.698000 | 1.418 sec/iter\n",
      "Epoch: 417 | Batch: 006 / 011 | Total loss: 0.958 | Reg loss: 0.025 | Tree loss: 0.958 | Accuracy: 0.651500 | 1.418 sec/iter\n",
      "Epoch: 417 | Batch: 007 / 011 | Total loss: 0.925 | Reg loss: 0.025 | Tree loss: 0.925 | Accuracy: 0.671000 | 1.418 sec/iter\n",
      "Epoch: 417 | Batch: 008 / 011 | Total loss: 0.924 | Reg loss: 0.025 | Tree loss: 0.924 | Accuracy: 0.650500 | 1.418 sec/iter\n",
      "Epoch: 417 | Batch: 009 / 011 | Total loss: 0.932 | Reg loss: 0.025 | Tree loss: 0.932 | Accuracy: 0.663500 | 1.418 sec/iter\n",
      "Epoch: 417 | Batch: 010 / 011 | Total loss: 0.908 | Reg loss: 0.025 | Tree loss: 0.908 | Accuracy: 0.679181 | 1.418 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 418 | Batch: 000 / 011 | Total loss: 1.011 | Reg loss: 0.025 | Tree loss: 1.011 | Accuracy: 0.658000 | 1.419 sec/iter\n",
      "Epoch: 418 | Batch: 001 / 011 | Total loss: 0.979 | Reg loss: 0.025 | Tree loss: 0.979 | Accuracy: 0.690000 | 1.419 sec/iter\n",
      "Epoch: 418 | Batch: 002 / 011 | Total loss: 0.994 | Reg loss: 0.025 | Tree loss: 0.994 | Accuracy: 0.672500 | 1.419 sec/iter\n",
      "Epoch: 418 | Batch: 003 / 011 | Total loss: 0.976 | Reg loss: 0.025 | Tree loss: 0.976 | Accuracy: 0.692000 | 1.419 sec/iter\n",
      "Epoch: 418 | Batch: 004 / 011 | Total loss: 0.939 | Reg loss: 0.025 | Tree loss: 0.939 | Accuracy: 0.686500 | 1.419 sec/iter\n",
      "Epoch: 418 | Batch: 005 / 011 | Total loss: 0.927 | Reg loss: 0.025 | Tree loss: 0.927 | Accuracy: 0.680500 | 1.419 sec/iter\n",
      "Epoch: 418 | Batch: 006 / 011 | Total loss: 0.933 | Reg loss: 0.025 | Tree loss: 0.933 | Accuracy: 0.666500 | 1.419 sec/iter\n",
      "Epoch: 418 | Batch: 007 / 011 | Total loss: 0.920 | Reg loss: 0.025 | Tree loss: 0.920 | Accuracy: 0.662000 | 1.419 sec/iter\n",
      "Epoch: 418 | Batch: 008 / 011 | Total loss: 0.912 | Reg loss: 0.025 | Tree loss: 0.912 | Accuracy: 0.682000 | 1.419 sec/iter\n",
      "Epoch: 418 | Batch: 009 / 011 | Total loss: 0.919 | Reg loss: 0.025 | Tree loss: 0.919 | Accuracy: 0.670000 | 1.419 sec/iter\n",
      "Epoch: 418 | Batch: 010 / 011 | Total loss: 0.881 | Reg loss: 0.025 | Tree loss: 0.881 | Accuracy: 0.675768 | 1.419 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 419 | Batch: 000 / 011 | Total loss: 1.028 | Reg loss: 0.025 | Tree loss: 1.028 | Accuracy: 0.655000 | 1.42 sec/iter\n",
      "Epoch: 419 | Batch: 001 / 011 | Total loss: 0.990 | Reg loss: 0.025 | Tree loss: 0.990 | Accuracy: 0.671000 | 1.42 sec/iter\n",
      "Epoch: 419 | Batch: 002 / 011 | Total loss: 0.969 | Reg loss: 0.025 | Tree loss: 0.969 | Accuracy: 0.683000 | 1.42 sec/iter\n",
      "Epoch: 419 | Batch: 003 / 011 | Total loss: 0.966 | Reg loss: 0.025 | Tree loss: 0.966 | Accuracy: 0.680000 | 1.42 sec/iter\n",
      "Epoch: 419 | Batch: 004 / 011 | Total loss: 0.942 | Reg loss: 0.025 | Tree loss: 0.942 | Accuracy: 0.686000 | 1.42 sec/iter\n",
      "Epoch: 419 | Batch: 005 / 011 | Total loss: 0.925 | Reg loss: 0.025 | Tree loss: 0.925 | Accuracy: 0.676500 | 1.42 sec/iter\n",
      "Epoch: 419 | Batch: 006 / 011 | Total loss: 0.942 | Reg loss: 0.025 | Tree loss: 0.942 | Accuracy: 0.646000 | 1.42 sec/iter\n",
      "Epoch: 419 | Batch: 007 / 011 | Total loss: 0.920 | Reg loss: 0.025 | Tree loss: 0.920 | Accuracy: 0.682000 | 1.42 sec/iter\n",
      "Epoch: 419 | Batch: 008 / 011 | Total loss: 0.928 | Reg loss: 0.025 | Tree loss: 0.928 | Accuracy: 0.666000 | 1.42 sec/iter\n",
      "Epoch: 419 | Batch: 009 / 011 | Total loss: 0.898 | Reg loss: 0.025 | Tree loss: 0.898 | Accuracy: 0.674500 | 1.42 sec/iter\n",
      "Epoch: 419 | Batch: 010 / 011 | Total loss: 0.922 | Reg loss: 0.025 | Tree loss: 0.922 | Accuracy: 0.706485 | 1.42 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 420 | Batch: 000 / 011 | Total loss: 0.998 | Reg loss: 0.025 | Tree loss: 0.998 | Accuracy: 0.671500 | 1.42 sec/iter\n",
      "Epoch: 420 | Batch: 001 / 011 | Total loss: 0.999 | Reg loss: 0.025 | Tree loss: 0.999 | Accuracy: 0.662500 | 1.42 sec/iter\n",
      "Epoch: 420 | Batch: 002 / 011 | Total loss: 0.998 | Reg loss: 0.025 | Tree loss: 0.998 | Accuracy: 0.672500 | 1.42 sec/iter\n",
      "Epoch: 420 | Batch: 003 / 011 | Total loss: 0.973 | Reg loss: 0.025 | Tree loss: 0.973 | Accuracy: 0.683000 | 1.421 sec/iter\n",
      "Epoch: 420 | Batch: 004 / 011 | Total loss: 0.946 | Reg loss: 0.025 | Tree loss: 0.946 | Accuracy: 0.693000 | 1.421 sec/iter\n",
      "Epoch: 420 | Batch: 005 / 011 | Total loss: 0.930 | Reg loss: 0.025 | Tree loss: 0.930 | Accuracy: 0.680000 | 1.421 sec/iter\n",
      "Epoch: 420 | Batch: 006 / 011 | Total loss: 0.919 | Reg loss: 0.025 | Tree loss: 0.919 | Accuracy: 0.694000 | 1.421 sec/iter\n",
      "Epoch: 420 | Batch: 007 / 011 | Total loss: 0.909 | Reg loss: 0.025 | Tree loss: 0.909 | Accuracy: 0.669500 | 1.421 sec/iter\n",
      "Epoch: 420 | Batch: 008 / 011 | Total loss: 0.941 | Reg loss: 0.025 | Tree loss: 0.941 | Accuracy: 0.658000 | 1.421 sec/iter\n",
      "Epoch: 420 | Batch: 009 / 011 | Total loss: 0.900 | Reg loss: 0.025 | Tree loss: 0.900 | Accuracy: 0.666000 | 1.421 sec/iter\n",
      "Epoch: 420 | Batch: 010 / 011 | Total loss: 0.856 | Reg loss: 0.025 | Tree loss: 0.856 | Accuracy: 0.689420 | 1.421 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 421 | Batch: 000 / 011 | Total loss: 1.026 | Reg loss: 0.025 | Tree loss: 1.026 | Accuracy: 0.654500 | 1.421 sec/iter\n",
      "Epoch: 421 | Batch: 001 / 011 | Total loss: 0.989 | Reg loss: 0.025 | Tree loss: 0.989 | Accuracy: 0.674500 | 1.421 sec/iter\n",
      "Epoch: 421 | Batch: 002 / 011 | Total loss: 0.973 | Reg loss: 0.025 | Tree loss: 0.973 | Accuracy: 0.687000 | 1.421 sec/iter\n",
      "Epoch: 421 | Batch: 003 / 011 | Total loss: 0.974 | Reg loss: 0.025 | Tree loss: 0.974 | Accuracy: 0.675000 | 1.421 sec/iter\n",
      "Epoch: 421 | Batch: 004 / 011 | Total loss: 0.943 | Reg loss: 0.025 | Tree loss: 0.943 | Accuracy: 0.693500 | 1.422 sec/iter\n",
      "Epoch: 421 | Batch: 005 / 011 | Total loss: 0.919 | Reg loss: 0.025 | Tree loss: 0.919 | Accuracy: 0.704000 | 1.422 sec/iter\n",
      "Epoch: 421 | Batch: 006 / 011 | Total loss: 0.907 | Reg loss: 0.025 | Tree loss: 0.907 | Accuracy: 0.693500 | 1.422 sec/iter\n",
      "Epoch: 421 | Batch: 007 / 011 | Total loss: 0.937 | Reg loss: 0.025 | Tree loss: 0.937 | Accuracy: 0.655000 | 1.422 sec/iter\n",
      "Epoch: 421 | Batch: 008 / 011 | Total loss: 0.920 | Reg loss: 0.025 | Tree loss: 0.920 | Accuracy: 0.654500 | 1.422 sec/iter\n",
      "Epoch: 421 | Batch: 009 / 011 | Total loss: 0.927 | Reg loss: 0.025 | Tree loss: 0.927 | Accuracy: 0.666000 | 1.422 sec/iter\n",
      "Epoch: 421 | Batch: 010 / 011 | Total loss: 0.848 | Reg loss: 0.025 | Tree loss: 0.848 | Accuracy: 0.686007 | 1.422 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 422 | Batch: 000 / 011 | Total loss: 1.017 | Reg loss: 0.025 | Tree loss: 1.017 | Accuracy: 0.668000 | 1.423 sec/iter\n",
      "Epoch: 422 | Batch: 001 / 011 | Total loss: 0.992 | Reg loss: 0.025 | Tree loss: 0.992 | Accuracy: 0.665000 | 1.423 sec/iter\n",
      "Epoch: 422 | Batch: 002 / 011 | Total loss: 0.983 | Reg loss: 0.025 | Tree loss: 0.983 | Accuracy: 0.678000 | 1.423 sec/iter\n",
      "Epoch: 422 | Batch: 003 / 011 | Total loss: 0.958 | Reg loss: 0.025 | Tree loss: 0.958 | Accuracy: 0.685500 | 1.423 sec/iter\n",
      "Epoch: 422 | Batch: 004 / 011 | Total loss: 0.931 | Reg loss: 0.025 | Tree loss: 0.931 | Accuracy: 0.695000 | 1.423 sec/iter\n",
      "Epoch: 422 | Batch: 005 / 011 | Total loss: 0.934 | Reg loss: 0.025 | Tree loss: 0.934 | Accuracy: 0.679500 | 1.423 sec/iter\n",
      "Epoch: 422 | Batch: 006 / 011 | Total loss: 0.920 | Reg loss: 0.025 | Tree loss: 0.920 | Accuracy: 0.683000 | 1.423 sec/iter\n",
      "Epoch: 422 | Batch: 007 / 011 | Total loss: 0.937 | Reg loss: 0.025 | Tree loss: 0.937 | Accuracy: 0.669000 | 1.423 sec/iter\n",
      "Epoch: 422 | Batch: 008 / 011 | Total loss: 0.936 | Reg loss: 0.025 | Tree loss: 0.936 | Accuracy: 0.648000 | 1.423 sec/iter\n",
      "Epoch: 422 | Batch: 009 / 011 | Total loss: 0.917 | Reg loss: 0.025 | Tree loss: 0.917 | Accuracy: 0.659500 | 1.423 sec/iter\n",
      "Epoch: 422 | Batch: 010 / 011 | Total loss: 0.860 | Reg loss: 0.025 | Tree loss: 0.860 | Accuracy: 0.672355 | 1.423 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 423 | Batch: 000 / 011 | Total loss: 1.006 | Reg loss: 0.025 | Tree loss: 1.006 | Accuracy: 0.660500 | 1.423 sec/iter\n",
      "Epoch: 423 | Batch: 001 / 011 | Total loss: 0.989 | Reg loss: 0.025 | Tree loss: 0.989 | Accuracy: 0.664000 | 1.423 sec/iter\n",
      "Epoch: 423 | Batch: 002 / 011 | Total loss: 0.964 | Reg loss: 0.025 | Tree loss: 0.964 | Accuracy: 0.691000 | 1.423 sec/iter\n",
      "Epoch: 423 | Batch: 003 / 011 | Total loss: 0.941 | Reg loss: 0.025 | Tree loss: 0.941 | Accuracy: 0.680000 | 1.423 sec/iter\n",
      "Epoch: 423 | Batch: 004 / 011 | Total loss: 0.947 | Reg loss: 0.025 | Tree loss: 0.947 | Accuracy: 0.680500 | 1.423 sec/iter\n",
      "Epoch: 423 | Batch: 005 / 011 | Total loss: 0.928 | Reg loss: 0.025 | Tree loss: 0.928 | Accuracy: 0.698000 | 1.423 sec/iter\n",
      "Epoch: 423 | Batch: 006 / 011 | Total loss: 0.940 | Reg loss: 0.025 | Tree loss: 0.940 | Accuracy: 0.663500 | 1.423 sec/iter\n",
      "Epoch: 423 | Batch: 007 / 011 | Total loss: 0.926 | Reg loss: 0.025 | Tree loss: 0.926 | Accuracy: 0.666000 | 1.423 sec/iter\n",
      "Epoch: 423 | Batch: 008 / 011 | Total loss: 0.927 | Reg loss: 0.025 | Tree loss: 0.927 | Accuracy: 0.679500 | 1.423 sec/iter\n",
      "Epoch: 423 | Batch: 009 / 011 | Total loss: 0.940 | Reg loss: 0.025 | Tree loss: 0.940 | Accuracy: 0.657500 | 1.424 sec/iter\n",
      "Epoch: 423 | Batch: 010 / 011 | Total loss: 0.921 | Reg loss: 0.025 | Tree loss: 0.921 | Accuracy: 0.662116 | 1.424 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 424 | Batch: 000 / 011 | Total loss: 1.009 | Reg loss: 0.025 | Tree loss: 1.009 | Accuracy: 0.652500 | 1.424 sec/iter\n",
      "Epoch: 424 | Batch: 001 / 011 | Total loss: 0.999 | Reg loss: 0.025 | Tree loss: 0.999 | Accuracy: 0.669000 | 1.424 sec/iter\n",
      "Epoch: 424 | Batch: 002 / 011 | Total loss: 0.972 | Reg loss: 0.025 | Tree loss: 0.972 | Accuracy: 0.674000 | 1.424 sec/iter\n",
      "Epoch: 424 | Batch: 003 / 011 | Total loss: 0.965 | Reg loss: 0.025 | Tree loss: 0.965 | Accuracy: 0.685000 | 1.424 sec/iter\n",
      "Epoch: 424 | Batch: 004 / 011 | Total loss: 0.937 | Reg loss: 0.025 | Tree loss: 0.937 | Accuracy: 0.702500 | 1.424 sec/iter\n",
      "Epoch: 424 | Batch: 005 / 011 | Total loss: 0.930 | Reg loss: 0.025 | Tree loss: 0.930 | Accuracy: 0.680000 | 1.424 sec/iter\n",
      "Epoch: 424 | Batch: 006 / 011 | Total loss: 0.919 | Reg loss: 0.025 | Tree loss: 0.919 | Accuracy: 0.676000 | 1.424 sec/iter\n",
      "Epoch: 424 | Batch: 007 / 011 | Total loss: 0.924 | Reg loss: 0.025 | Tree loss: 0.924 | Accuracy: 0.668000 | 1.424 sec/iter\n",
      "Epoch: 424 | Batch: 008 / 011 | Total loss: 0.913 | Reg loss: 0.025 | Tree loss: 0.913 | Accuracy: 0.676000 | 1.424 sec/iter\n",
      "Epoch: 424 | Batch: 009 / 011 | Total loss: 0.923 | Reg loss: 0.025 | Tree loss: 0.923 | Accuracy: 0.653000 | 1.424 sec/iter\n",
      "Epoch: 424 | Batch: 010 / 011 | Total loss: 0.986 | Reg loss: 0.025 | Tree loss: 0.986 | Accuracy: 0.617747 | 1.424 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 425 | Batch: 000 / 011 | Total loss: 0.996 | Reg loss: 0.025 | Tree loss: 0.996 | Accuracy: 0.662500 | 1.425 sec/iter\n",
      "Epoch: 425 | Batch: 001 / 011 | Total loss: 0.981 | Reg loss: 0.025 | Tree loss: 0.981 | Accuracy: 0.664500 | 1.425 sec/iter\n",
      "Epoch: 425 | Batch: 002 / 011 | Total loss: 0.984 | Reg loss: 0.025 | Tree loss: 0.984 | Accuracy: 0.665500 | 1.425 sec/iter\n",
      "Epoch: 425 | Batch: 003 / 011 | Total loss: 0.964 | Reg loss: 0.025 | Tree loss: 0.964 | Accuracy: 0.676500 | 1.425 sec/iter\n",
      "Epoch: 425 | Batch: 004 / 011 | Total loss: 0.971 | Reg loss: 0.025 | Tree loss: 0.971 | Accuracy: 0.678500 | 1.425 sec/iter\n",
      "Epoch: 425 | Batch: 005 / 011 | Total loss: 0.937 | Reg loss: 0.025 | Tree loss: 0.937 | Accuracy: 0.681500 | 1.425 sec/iter\n",
      "Epoch: 425 | Batch: 006 / 011 | Total loss: 0.931 | Reg loss: 0.025 | Tree loss: 0.931 | Accuracy: 0.665500 | 1.425 sec/iter\n",
      "Epoch: 425 | Batch: 007 / 011 | Total loss: 0.917 | Reg loss: 0.025 | Tree loss: 0.917 | Accuracy: 0.685500 | 1.425 sec/iter\n",
      "Epoch: 425 | Batch: 008 / 011 | Total loss: 0.899 | Reg loss: 0.025 | Tree loss: 0.899 | Accuracy: 0.679500 | 1.425 sec/iter\n",
      "Epoch: 425 | Batch: 009 / 011 | Total loss: 0.912 | Reg loss: 0.025 | Tree loss: 0.912 | Accuracy: 0.673500 | 1.425 sec/iter\n",
      "Epoch: 425 | Batch: 010 / 011 | Total loss: 0.962 | Reg loss: 0.025 | Tree loss: 0.962 | Accuracy: 0.648464 | 1.425 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 426 | Batch: 000 / 011 | Total loss: 1.000 | Reg loss: 0.025 | Tree loss: 1.000 | Accuracy: 0.668500 | 1.425 sec/iter\n",
      "Epoch: 426 | Batch: 001 / 011 | Total loss: 1.004 | Reg loss: 0.025 | Tree loss: 1.004 | Accuracy: 0.659500 | 1.425 sec/iter\n",
      "Epoch: 426 | Batch: 002 / 011 | Total loss: 0.970 | Reg loss: 0.025 | Tree loss: 0.970 | Accuracy: 0.680500 | 1.425 sec/iter\n",
      "Epoch: 426 | Batch: 003 / 011 | Total loss: 0.944 | Reg loss: 0.025 | Tree loss: 0.944 | Accuracy: 0.694500 | 1.425 sec/iter\n",
      "Epoch: 426 | Batch: 004 / 011 | Total loss: 0.966 | Reg loss: 0.025 | Tree loss: 0.966 | Accuracy: 0.672500 | 1.425 sec/iter\n",
      "Epoch: 426 | Batch: 005 / 011 | Total loss: 0.908 | Reg loss: 0.025 | Tree loss: 0.908 | Accuracy: 0.696000 | 1.425 sec/iter\n",
      "Epoch: 426 | Batch: 006 / 011 | Total loss: 0.940 | Reg loss: 0.025 | Tree loss: 0.940 | Accuracy: 0.669500 | 1.425 sec/iter\n",
      "Epoch: 426 | Batch: 007 / 011 | Total loss: 0.936 | Reg loss: 0.025 | Tree loss: 0.936 | Accuracy: 0.658000 | 1.425 sec/iter\n",
      "Epoch: 426 | Batch: 008 / 011 | Total loss: 0.923 | Reg loss: 0.025 | Tree loss: 0.923 | Accuracy: 0.668000 | 1.425 sec/iter\n",
      "Epoch: 426 | Batch: 009 / 011 | Total loss: 0.927 | Reg loss: 0.025 | Tree loss: 0.927 | Accuracy: 0.671500 | 1.425 sec/iter\n",
      "Epoch: 426 | Batch: 010 / 011 | Total loss: 0.882 | Reg loss: 0.025 | Tree loss: 0.882 | Accuracy: 0.696246 | 1.425 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 427 | Batch: 000 / 011 | Total loss: 1.032 | Reg loss: 0.025 | Tree loss: 1.032 | Accuracy: 0.662500 | 1.426 sec/iter\n",
      "Epoch: 427 | Batch: 001 / 011 | Total loss: 0.990 | Reg loss: 0.025 | Tree loss: 0.990 | Accuracy: 0.672000 | 1.426 sec/iter\n",
      "Epoch: 427 | Batch: 002 / 011 | Total loss: 0.981 | Reg loss: 0.025 | Tree loss: 0.981 | Accuracy: 0.688500 | 1.426 sec/iter\n",
      "Epoch: 427 | Batch: 003 / 011 | Total loss: 0.953 | Reg loss: 0.025 | Tree loss: 0.953 | Accuracy: 0.694500 | 1.426 sec/iter\n",
      "Epoch: 427 | Batch: 004 / 011 | Total loss: 0.948 | Reg loss: 0.025 | Tree loss: 0.948 | Accuracy: 0.676000 | 1.426 sec/iter\n",
      "Epoch: 427 | Batch: 005 / 011 | Total loss: 0.923 | Reg loss: 0.025 | Tree loss: 0.923 | Accuracy: 0.679500 | 1.426 sec/iter\n",
      "Epoch: 427 | Batch: 006 / 011 | Total loss: 0.948 | Reg loss: 0.025 | Tree loss: 0.948 | Accuracy: 0.659000 | 1.426 sec/iter\n",
      "Epoch: 427 | Batch: 007 / 011 | Total loss: 0.899 | Reg loss: 0.025 | Tree loss: 0.899 | Accuracy: 0.682500 | 1.426 sec/iter\n",
      "Epoch: 427 | Batch: 008 / 011 | Total loss: 0.936 | Reg loss: 0.025 | Tree loss: 0.936 | Accuracy: 0.647000 | 1.426 sec/iter\n",
      "Epoch: 427 | Batch: 009 / 011 | Total loss: 0.916 | Reg loss: 0.025 | Tree loss: 0.916 | Accuracy: 0.658500 | 1.426 sec/iter\n",
      "Epoch: 427 | Batch: 010 / 011 | Total loss: 0.957 | Reg loss: 0.025 | Tree loss: 0.957 | Accuracy: 0.686007 | 1.426 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 428 | Batch: 000 / 011 | Total loss: 1.030 | Reg loss: 0.025 | Tree loss: 1.030 | Accuracy: 0.640000 | 1.427 sec/iter\n",
      "Epoch: 428 | Batch: 001 / 011 | Total loss: 1.004 | Reg loss: 0.025 | Tree loss: 1.004 | Accuracy: 0.649000 | 1.427 sec/iter\n",
      "Epoch: 428 | Batch: 002 / 011 | Total loss: 0.978 | Reg loss: 0.025 | Tree loss: 0.978 | Accuracy: 0.664000 | 1.427 sec/iter\n",
      "Epoch: 428 | Batch: 003 / 011 | Total loss: 0.941 | Reg loss: 0.025 | Tree loss: 0.941 | Accuracy: 0.691000 | 1.427 sec/iter\n",
      "Epoch: 428 | Batch: 004 / 011 | Total loss: 0.952 | Reg loss: 0.025 | Tree loss: 0.952 | Accuracy: 0.670500 | 1.427 sec/iter\n",
      "Epoch: 428 | Batch: 005 / 011 | Total loss: 0.920 | Reg loss: 0.025 | Tree loss: 0.920 | Accuracy: 0.685000 | 1.427 sec/iter\n",
      "Epoch: 428 | Batch: 006 / 011 | Total loss: 0.932 | Reg loss: 0.025 | Tree loss: 0.932 | Accuracy: 0.666500 | 1.428 sec/iter\n",
      "Epoch: 428 | Batch: 007 / 011 | Total loss: 0.929 | Reg loss: 0.025 | Tree loss: 0.929 | Accuracy: 0.663000 | 1.428 sec/iter\n",
      "Epoch: 428 | Batch: 008 / 011 | Total loss: 0.933 | Reg loss: 0.025 | Tree loss: 0.933 | Accuracy: 0.664000 | 1.428 sec/iter\n",
      "Epoch: 428 | Batch: 009 / 011 | Total loss: 0.910 | Reg loss: 0.025 | Tree loss: 0.910 | Accuracy: 0.662000 | 1.428 sec/iter\n",
      "Epoch: 428 | Batch: 010 / 011 | Total loss: 0.888 | Reg loss: 0.025 | Tree loss: 0.888 | Accuracy: 0.668942 | 1.428 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 429 | Batch: 000 / 011 | Total loss: 1.021 | Reg loss: 0.025 | Tree loss: 1.021 | Accuracy: 0.658000 | 1.428 sec/iter\n",
      "Epoch: 429 | Batch: 001 / 011 | Total loss: 1.000 | Reg loss: 0.025 | Tree loss: 1.000 | Accuracy: 0.671000 | 1.428 sec/iter\n",
      "Epoch: 429 | Batch: 002 / 011 | Total loss: 0.969 | Reg loss: 0.025 | Tree loss: 0.969 | Accuracy: 0.691000 | 1.428 sec/iter\n",
      "Epoch: 429 | Batch: 003 / 011 | Total loss: 0.948 | Reg loss: 0.025 | Tree loss: 0.948 | Accuracy: 0.689000 | 1.428 sec/iter\n",
      "Epoch: 429 | Batch: 004 / 011 | Total loss: 0.944 | Reg loss: 0.025 | Tree loss: 0.944 | Accuracy: 0.686500 | 1.429 sec/iter\n",
      "Epoch: 429 | Batch: 005 / 011 | Total loss: 0.933 | Reg loss: 0.025 | Tree loss: 0.933 | Accuracy: 0.680500 | 1.429 sec/iter\n",
      "Epoch: 429 | Batch: 006 / 011 | Total loss: 0.932 | Reg loss: 0.025 | Tree loss: 0.932 | Accuracy: 0.668000 | 1.429 sec/iter\n",
      "Epoch: 429 | Batch: 007 / 011 | Total loss: 0.916 | Reg loss: 0.025 | Tree loss: 0.916 | Accuracy: 0.659500 | 1.429 sec/iter\n",
      "Epoch: 429 | Batch: 008 / 011 | Total loss: 0.930 | Reg loss: 0.025 | Tree loss: 0.930 | Accuracy: 0.660000 | 1.429 sec/iter\n",
      "Epoch: 429 | Batch: 009 / 011 | Total loss: 0.904 | Reg loss: 0.025 | Tree loss: 0.904 | Accuracy: 0.668500 | 1.429 sec/iter\n",
      "Epoch: 429 | Batch: 010 / 011 | Total loss: 0.950 | Reg loss: 0.025 | Tree loss: 0.950 | Accuracy: 0.641638 | 1.429 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 430 | Batch: 000 / 011 | Total loss: 1.006 | Reg loss: 0.025 | Tree loss: 1.006 | Accuracy: 0.661500 | 1.43 sec/iter\n",
      "Epoch: 430 | Batch: 001 / 011 | Total loss: 1.004 | Reg loss: 0.025 | Tree loss: 1.004 | Accuracy: 0.673500 | 1.43 sec/iter\n",
      "Epoch: 430 | Batch: 002 / 011 | Total loss: 0.959 | Reg loss: 0.025 | Tree loss: 0.959 | Accuracy: 0.685500 | 1.43 sec/iter\n",
      "Epoch: 430 | Batch: 003 / 011 | Total loss: 0.959 | Reg loss: 0.025 | Tree loss: 0.959 | Accuracy: 0.677500 | 1.43 sec/iter\n",
      "Epoch: 430 | Batch: 004 / 011 | Total loss: 0.962 | Reg loss: 0.025 | Tree loss: 0.962 | Accuracy: 0.669500 | 1.43 sec/iter\n",
      "Epoch: 430 | Batch: 005 / 011 | Total loss: 0.932 | Reg loss: 0.025 | Tree loss: 0.932 | Accuracy: 0.676000 | 1.43 sec/iter\n",
      "Epoch: 430 | Batch: 006 / 011 | Total loss: 0.942 | Reg loss: 0.025 | Tree loss: 0.942 | Accuracy: 0.671500 | 1.43 sec/iter\n",
      "Epoch: 430 | Batch: 007 / 011 | Total loss: 0.913 | Reg loss: 0.025 | Tree loss: 0.913 | Accuracy: 0.677000 | 1.43 sec/iter\n",
      "Epoch: 430 | Batch: 008 / 011 | Total loss: 0.926 | Reg loss: 0.025 | Tree loss: 0.926 | Accuracy: 0.658500 | 1.43 sec/iter\n",
      "Epoch: 430 | Batch: 009 / 011 | Total loss: 0.903 | Reg loss: 0.025 | Tree loss: 0.903 | Accuracy: 0.675500 | 1.43 sec/iter\n",
      "Epoch: 430 | Batch: 010 / 011 | Total loss: 0.849 | Reg loss: 0.025 | Tree loss: 0.849 | Accuracy: 0.709898 | 1.43 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 431 | Batch: 000 / 011 | Total loss: 1.011 | Reg loss: 0.025 | Tree loss: 1.011 | Accuracy: 0.657000 | 1.431 sec/iter\n",
      "Epoch: 431 | Batch: 001 / 011 | Total loss: 1.003 | Reg loss: 0.025 | Tree loss: 1.003 | Accuracy: 0.676500 | 1.431 sec/iter\n",
      "Epoch: 431 | Batch: 002 / 011 | Total loss: 0.977 | Reg loss: 0.025 | Tree loss: 0.977 | Accuracy: 0.688500 | 1.431 sec/iter\n",
      "Epoch: 431 | Batch: 003 / 011 | Total loss: 0.965 | Reg loss: 0.025 | Tree loss: 0.965 | Accuracy: 0.684000 | 1.431 sec/iter\n",
      "Epoch: 431 | Batch: 004 / 011 | Total loss: 0.930 | Reg loss: 0.025 | Tree loss: 0.930 | Accuracy: 0.697500 | 1.431 sec/iter\n",
      "Epoch: 431 | Batch: 005 / 011 | Total loss: 0.941 | Reg loss: 0.025 | Tree loss: 0.941 | Accuracy: 0.674000 | 1.431 sec/iter\n",
      "Epoch: 431 | Batch: 006 / 011 | Total loss: 0.921 | Reg loss: 0.025 | Tree loss: 0.921 | Accuracy: 0.669500 | 1.431 sec/iter\n",
      "Epoch: 431 | Batch: 007 / 011 | Total loss: 0.950 | Reg loss: 0.025 | Tree loss: 0.950 | Accuracy: 0.644000 | 1.431 sec/iter\n",
      "Epoch: 431 | Batch: 008 / 011 | Total loss: 0.898 | Reg loss: 0.025 | Tree loss: 0.898 | Accuracy: 0.667000 | 1.431 sec/iter\n",
      "Epoch: 431 | Batch: 009 / 011 | Total loss: 0.909 | Reg loss: 0.025 | Tree loss: 0.909 | Accuracy: 0.666000 | 1.431 sec/iter\n",
      "Epoch: 431 | Batch: 010 / 011 | Total loss: 0.860 | Reg loss: 0.025 | Tree loss: 0.860 | Accuracy: 0.709898 | 1.431 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 432 | Batch: 000 / 011 | Total loss: 1.015 | Reg loss: 0.025 | Tree loss: 1.015 | Accuracy: 0.667000 | 1.432 sec/iter\n",
      "Epoch: 432 | Batch: 001 / 011 | Total loss: 0.989 | Reg loss: 0.025 | Tree loss: 0.989 | Accuracy: 0.671500 | 1.432 sec/iter\n",
      "Epoch: 432 | Batch: 002 / 011 | Total loss: 0.979 | Reg loss: 0.025 | Tree loss: 0.979 | Accuracy: 0.685000 | 1.432 sec/iter\n",
      "Epoch: 432 | Batch: 003 / 011 | Total loss: 0.983 | Reg loss: 0.025 | Tree loss: 0.983 | Accuracy: 0.662500 | 1.432 sec/iter\n",
      "Epoch: 432 | Batch: 004 / 011 | Total loss: 0.945 | Reg loss: 0.025 | Tree loss: 0.945 | Accuracy: 0.690000 | 1.432 sec/iter\n",
      "Epoch: 432 | Batch: 005 / 011 | Total loss: 0.946 | Reg loss: 0.025 | Tree loss: 0.946 | Accuracy: 0.671000 | 1.432 sec/iter\n",
      "Epoch: 432 | Batch: 006 / 011 | Total loss: 0.914 | Reg loss: 0.025 | Tree loss: 0.914 | Accuracy: 0.681000 | 1.432 sec/iter\n",
      "Epoch: 432 | Batch: 007 / 011 | Total loss: 0.922 | Reg loss: 0.025 | Tree loss: 0.922 | Accuracy: 0.662500 | 1.432 sec/iter\n",
      "Epoch: 432 | Batch: 008 / 011 | Total loss: 0.908 | Reg loss: 0.025 | Tree loss: 0.908 | Accuracy: 0.680500 | 1.432 sec/iter\n",
      "Epoch: 432 | Batch: 009 / 011 | Total loss: 0.903 | Reg loss: 0.025 | Tree loss: 0.903 | Accuracy: 0.663500 | 1.432 sec/iter\n",
      "Epoch: 432 | Batch: 010 / 011 | Total loss: 0.888 | Reg loss: 0.025 | Tree loss: 0.888 | Accuracy: 0.675768 | 1.432 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 433 | Batch: 000 / 011 | Total loss: 0.989 | Reg loss: 0.025 | Tree loss: 0.989 | Accuracy: 0.668500 | 1.433 sec/iter\n",
      "Epoch: 433 | Batch: 001 / 011 | Total loss: 0.996 | Reg loss: 0.025 | Tree loss: 0.996 | Accuracy: 0.675500 | 1.433 sec/iter\n",
      "Epoch: 433 | Batch: 002 / 011 | Total loss: 0.992 | Reg loss: 0.025 | Tree loss: 0.992 | Accuracy: 0.670000 | 1.433 sec/iter\n",
      "Epoch: 433 | Batch: 003 / 011 | Total loss: 0.969 | Reg loss: 0.025 | Tree loss: 0.969 | Accuracy: 0.683000 | 1.433 sec/iter\n",
      "Epoch: 433 | Batch: 004 / 011 | Total loss: 0.946 | Reg loss: 0.025 | Tree loss: 0.946 | Accuracy: 0.697500 | 1.433 sec/iter\n",
      "Epoch: 433 | Batch: 005 / 011 | Total loss: 0.930 | Reg loss: 0.025 | Tree loss: 0.930 | Accuracy: 0.674500 | 1.433 sec/iter\n",
      "Epoch: 433 | Batch: 006 / 011 | Total loss: 0.931 | Reg loss: 0.025 | Tree loss: 0.931 | Accuracy: 0.668000 | 1.433 sec/iter\n",
      "Epoch: 433 | Batch: 007 / 011 | Total loss: 0.934 | Reg loss: 0.025 | Tree loss: 0.934 | Accuracy: 0.664000 | 1.433 sec/iter\n",
      "Epoch: 433 | Batch: 008 / 011 | Total loss: 0.910 | Reg loss: 0.025 | Tree loss: 0.910 | Accuracy: 0.675000 | 1.433 sec/iter\n",
      "Epoch: 433 | Batch: 009 / 011 | Total loss: 0.910 | Reg loss: 0.025 | Tree loss: 0.910 | Accuracy: 0.661000 | 1.433 sec/iter\n",
      "Epoch: 433 | Batch: 010 / 011 | Total loss: 0.890 | Reg loss: 0.025 | Tree loss: 0.890 | Accuracy: 0.665529 | 1.433 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 434 | Batch: 000 / 011 | Total loss: 1.011 | Reg loss: 0.025 | Tree loss: 1.011 | Accuracy: 0.660500 | 1.434 sec/iter\n",
      "Epoch: 434 | Batch: 001 / 011 | Total loss: 0.987 | Reg loss: 0.025 | Tree loss: 0.987 | Accuracy: 0.668500 | 1.434 sec/iter\n",
      "Epoch: 434 | Batch: 002 / 011 | Total loss: 0.952 | Reg loss: 0.025 | Tree loss: 0.952 | Accuracy: 0.688500 | 1.434 sec/iter\n",
      "Epoch: 434 | Batch: 003 / 011 | Total loss: 0.965 | Reg loss: 0.025 | Tree loss: 0.965 | Accuracy: 0.667000 | 1.434 sec/iter\n",
      "Epoch: 434 | Batch: 004 / 011 | Total loss: 0.942 | Reg loss: 0.025 | Tree loss: 0.942 | Accuracy: 0.687500 | 1.434 sec/iter\n",
      "Epoch: 434 | Batch: 005 / 011 | Total loss: 0.936 | Reg loss: 0.025 | Tree loss: 0.936 | Accuracy: 0.685000 | 1.434 sec/iter\n",
      "Epoch: 434 | Batch: 006 / 011 | Total loss: 0.930 | Reg loss: 0.025 | Tree loss: 0.930 | Accuracy: 0.674500 | 1.434 sec/iter\n",
      "Epoch: 434 | Batch: 007 / 011 | Total loss: 0.941 | Reg loss: 0.025 | Tree loss: 0.941 | Accuracy: 0.659500 | 1.434 sec/iter\n",
      "Epoch: 434 | Batch: 008 / 011 | Total loss: 0.928 | Reg loss: 0.025 | Tree loss: 0.928 | Accuracy: 0.662000 | 1.434 sec/iter\n",
      "Epoch: 434 | Batch: 009 / 011 | Total loss: 0.917 | Reg loss: 0.025 | Tree loss: 0.917 | Accuracy: 0.678000 | 1.434 sec/iter\n",
      "Epoch: 434 | Batch: 010 / 011 | Total loss: 0.899 | Reg loss: 0.025 | Tree loss: 0.899 | Accuracy: 0.675768 | 1.434 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 435 | Batch: 000 / 011 | Total loss: 1.010 | Reg loss: 0.025 | Tree loss: 1.010 | Accuracy: 0.661500 | 1.434 sec/iter\n",
      "Epoch: 435 | Batch: 001 / 011 | Total loss: 0.979 | Reg loss: 0.025 | Tree loss: 0.979 | Accuracy: 0.678500 | 1.434 sec/iter\n",
      "Epoch: 435 | Batch: 002 / 011 | Total loss: 0.984 | Reg loss: 0.025 | Tree loss: 0.984 | Accuracy: 0.681500 | 1.434 sec/iter\n",
      "Epoch: 435 | Batch: 003 / 011 | Total loss: 0.965 | Reg loss: 0.025 | Tree loss: 0.965 | Accuracy: 0.681000 | 1.434 sec/iter\n",
      "Epoch: 435 | Batch: 004 / 011 | Total loss: 0.930 | Reg loss: 0.025 | Tree loss: 0.930 | Accuracy: 0.685000 | 1.434 sec/iter\n",
      "Epoch: 435 | Batch: 005 / 011 | Total loss: 0.910 | Reg loss: 0.025 | Tree loss: 0.910 | Accuracy: 0.693500 | 1.434 sec/iter\n",
      "Epoch: 435 | Batch: 006 / 011 | Total loss: 0.928 | Reg loss: 0.025 | Tree loss: 0.928 | Accuracy: 0.661500 | 1.434 sec/iter\n",
      "Epoch: 435 | Batch: 007 / 011 | Total loss: 0.920 | Reg loss: 0.025 | Tree loss: 0.920 | Accuracy: 0.660000 | 1.434 sec/iter\n",
      "Epoch: 435 | Batch: 008 / 011 | Total loss: 0.918 | Reg loss: 0.025 | Tree loss: 0.918 | Accuracy: 0.673500 | 1.435 sec/iter\n",
      "Epoch: 435 | Batch: 009 / 011 | Total loss: 0.956 | Reg loss: 0.025 | Tree loss: 0.956 | Accuracy: 0.649500 | 1.435 sec/iter\n",
      "Epoch: 435 | Batch: 010 / 011 | Total loss: 0.921 | Reg loss: 0.025 | Tree loss: 0.921 | Accuracy: 0.665529 | 1.435 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 436 | Batch: 000 / 011 | Total loss: 0.983 | Reg loss: 0.025 | Tree loss: 0.983 | Accuracy: 0.679500 | 1.435 sec/iter\n",
      "Epoch: 436 | Batch: 001 / 011 | Total loss: 1.017 | Reg loss: 0.025 | Tree loss: 1.017 | Accuracy: 0.653500 | 1.435 sec/iter\n",
      "Epoch: 436 | Batch: 002 / 011 | Total loss: 0.986 | Reg loss: 0.025 | Tree loss: 0.986 | Accuracy: 0.670500 | 1.435 sec/iter\n",
      "Epoch: 436 | Batch: 003 / 011 | Total loss: 0.987 | Reg loss: 0.025 | Tree loss: 0.987 | Accuracy: 0.659000 | 1.435 sec/iter\n",
      "Epoch: 436 | Batch: 004 / 011 | Total loss: 0.942 | Reg loss: 0.025 | Tree loss: 0.942 | Accuracy: 0.697000 | 1.435 sec/iter\n",
      "Epoch: 436 | Batch: 005 / 011 | Total loss: 0.911 | Reg loss: 0.025 | Tree loss: 0.911 | Accuracy: 0.693500 | 1.435 sec/iter\n",
      "Epoch: 436 | Batch: 006 / 011 | Total loss: 0.910 | Reg loss: 0.025 | Tree loss: 0.910 | Accuracy: 0.679500 | 1.435 sec/iter\n",
      "Epoch: 436 | Batch: 007 / 011 | Total loss: 0.924 | Reg loss: 0.025 | Tree loss: 0.924 | Accuracy: 0.662500 | 1.435 sec/iter\n",
      "Epoch: 436 | Batch: 008 / 011 | Total loss: 0.938 | Reg loss: 0.025 | Tree loss: 0.938 | Accuracy: 0.650500 | 1.435 sec/iter\n",
      "Epoch: 436 | Batch: 009 / 011 | Total loss: 0.913 | Reg loss: 0.025 | Tree loss: 0.913 | Accuracy: 0.672000 | 1.435 sec/iter\n",
      "Epoch: 436 | Batch: 010 / 011 | Total loss: 0.883 | Reg loss: 0.025 | Tree loss: 0.883 | Accuracy: 0.682594 | 1.435 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 437 | Batch: 000 / 011 | Total loss: 1.006 | Reg loss: 0.025 | Tree loss: 1.006 | Accuracy: 0.666500 | 1.435 sec/iter\n",
      "Epoch: 437 | Batch: 001 / 011 | Total loss: 1.004 | Reg loss: 0.025 | Tree loss: 1.004 | Accuracy: 0.661500 | 1.435 sec/iter\n",
      "Epoch: 437 | Batch: 002 / 011 | Total loss: 0.982 | Reg loss: 0.025 | Tree loss: 0.982 | Accuracy: 0.672500 | 1.435 sec/iter\n",
      "Epoch: 437 | Batch: 003 / 011 | Total loss: 0.948 | Reg loss: 0.025 | Tree loss: 0.948 | Accuracy: 0.695500 | 1.435 sec/iter\n",
      "Epoch: 437 | Batch: 004 / 011 | Total loss: 0.921 | Reg loss: 0.025 | Tree loss: 0.921 | Accuracy: 0.708000 | 1.435 sec/iter\n",
      "Epoch: 437 | Batch: 005 / 011 | Total loss: 0.908 | Reg loss: 0.025 | Tree loss: 0.908 | Accuracy: 0.693500 | 1.435 sec/iter\n",
      "Epoch: 437 | Batch: 006 / 011 | Total loss: 0.925 | Reg loss: 0.025 | Tree loss: 0.925 | Accuracy: 0.672500 | 1.435 sec/iter\n",
      "Epoch: 437 | Batch: 007 / 011 | Total loss: 0.922 | Reg loss: 0.025 | Tree loss: 0.922 | Accuracy: 0.667500 | 1.435 sec/iter\n",
      "Epoch: 437 | Batch: 008 / 011 | Total loss: 0.946 | Reg loss: 0.025 | Tree loss: 0.946 | Accuracy: 0.657500 | 1.436 sec/iter\n",
      "Epoch: 437 | Batch: 009 / 011 | Total loss: 0.923 | Reg loss: 0.025 | Tree loss: 0.923 | Accuracy: 0.654500 | 1.436 sec/iter\n",
      "Epoch: 437 | Batch: 010 / 011 | Total loss: 0.949 | Reg loss: 0.025 | Tree loss: 0.949 | Accuracy: 0.631399 | 1.436 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 438 | Batch: 000 / 011 | Total loss: 1.030 | Reg loss: 0.025 | Tree loss: 1.030 | Accuracy: 0.658500 | 1.436 sec/iter\n",
      "Epoch: 438 | Batch: 001 / 011 | Total loss: 0.993 | Reg loss: 0.025 | Tree loss: 0.993 | Accuracy: 0.683000 | 1.436 sec/iter\n",
      "Epoch: 438 | Batch: 002 / 011 | Total loss: 0.957 | Reg loss: 0.025 | Tree loss: 0.957 | Accuracy: 0.684500 | 1.436 sec/iter\n",
      "Epoch: 438 | Batch: 003 / 011 | Total loss: 0.940 | Reg loss: 0.025 | Tree loss: 0.940 | Accuracy: 0.697000 | 1.436 sec/iter\n",
      "Epoch: 438 | Batch: 004 / 011 | Total loss: 0.935 | Reg loss: 0.025 | Tree loss: 0.935 | Accuracy: 0.686500 | 1.436 sec/iter\n",
      "Epoch: 438 | Batch: 005 / 011 | Total loss: 0.927 | Reg loss: 0.025 | Tree loss: 0.927 | Accuracy: 0.688500 | 1.436 sec/iter\n",
      "Epoch: 438 | Batch: 006 / 011 | Total loss: 0.921 | Reg loss: 0.025 | Tree loss: 0.921 | Accuracy: 0.683000 | 1.436 sec/iter\n",
      "Epoch: 438 | Batch: 007 / 011 | Total loss: 0.926 | Reg loss: 0.025 | Tree loss: 0.926 | Accuracy: 0.650000 | 1.436 sec/iter\n",
      "Epoch: 438 | Batch: 008 / 011 | Total loss: 0.955 | Reg loss: 0.025 | Tree loss: 0.955 | Accuracy: 0.646000 | 1.436 sec/iter\n",
      "Epoch: 438 | Batch: 009 / 011 | Total loss: 0.922 | Reg loss: 0.025 | Tree loss: 0.922 | Accuracy: 0.654000 | 1.436 sec/iter\n",
      "Epoch: 438 | Batch: 010 / 011 | Total loss: 0.860 | Reg loss: 0.025 | Tree loss: 0.860 | Accuracy: 0.713311 | 1.436 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 439 | Batch: 000 / 011 | Total loss: 1.004 | Reg loss: 0.025 | Tree loss: 1.004 | Accuracy: 0.663000 | 1.437 sec/iter\n",
      "Epoch: 439 | Batch: 001 / 011 | Total loss: 1.006 | Reg loss: 0.025 | Tree loss: 1.006 | Accuracy: 0.656000 | 1.437 sec/iter\n",
      "Epoch: 439 | Batch: 002 / 011 | Total loss: 0.984 | Reg loss: 0.025 | Tree loss: 0.984 | Accuracy: 0.672500 | 1.437 sec/iter\n",
      "Epoch: 439 | Batch: 003 / 011 | Total loss: 0.966 | Reg loss: 0.025 | Tree loss: 0.966 | Accuracy: 0.686500 | 1.437 sec/iter\n",
      "Epoch: 439 | Batch: 004 / 011 | Total loss: 0.955 | Reg loss: 0.025 | Tree loss: 0.955 | Accuracy: 0.677000 | 1.437 sec/iter\n",
      "Epoch: 439 | Batch: 005 / 011 | Total loss: 0.928 | Reg loss: 0.025 | Tree loss: 0.928 | Accuracy: 0.680500 | 1.437 sec/iter\n",
      "Epoch: 439 | Batch: 006 / 011 | Total loss: 0.925 | Reg loss: 0.025 | Tree loss: 0.925 | Accuracy: 0.679500 | 1.437 sec/iter\n",
      "Epoch: 439 | Batch: 007 / 011 | Total loss: 0.911 | Reg loss: 0.025 | Tree loss: 0.911 | Accuracy: 0.672500 | 1.437 sec/iter\n",
      "Epoch: 439 | Batch: 008 / 011 | Total loss: 0.916 | Reg loss: 0.025 | Tree loss: 0.916 | Accuracy: 0.668000 | 1.437 sec/iter\n",
      "Epoch: 439 | Batch: 009 / 011 | Total loss: 0.887 | Reg loss: 0.025 | Tree loss: 0.887 | Accuracy: 0.694500 | 1.437 sec/iter\n",
      "Epoch: 439 | Batch: 010 / 011 | Total loss: 0.988 | Reg loss: 0.025 | Tree loss: 0.988 | Accuracy: 0.610922 | 1.437 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 440 | Batch: 000 / 011 | Total loss: 1.031 | Reg loss: 0.025 | Tree loss: 1.031 | Accuracy: 0.653000 | 1.437 sec/iter\n",
      "Epoch: 440 | Batch: 001 / 011 | Total loss: 0.994 | Reg loss: 0.025 | Tree loss: 0.994 | Accuracy: 0.663000 | 1.437 sec/iter\n",
      "Epoch: 440 | Batch: 002 / 011 | Total loss: 0.957 | Reg loss: 0.025 | Tree loss: 0.957 | Accuracy: 0.689000 | 1.437 sec/iter\n",
      "Epoch: 440 | Batch: 003 / 011 | Total loss: 0.962 | Reg loss: 0.025 | Tree loss: 0.962 | Accuracy: 0.694500 | 1.437 sec/iter\n",
      "Epoch: 440 | Batch: 004 / 011 | Total loss: 0.922 | Reg loss: 0.025 | Tree loss: 0.922 | Accuracy: 0.706500 | 1.437 sec/iter\n",
      "Epoch: 440 | Batch: 005 / 011 | Total loss: 0.936 | Reg loss: 0.025 | Tree loss: 0.936 | Accuracy: 0.674500 | 1.437 sec/iter\n",
      "Epoch: 440 | Batch: 006 / 011 | Total loss: 0.914 | Reg loss: 0.025 | Tree loss: 0.914 | Accuracy: 0.686000 | 1.437 sec/iter\n",
      "Epoch: 440 | Batch: 007 / 011 | Total loss: 0.900 | Reg loss: 0.025 | Tree loss: 0.900 | Accuracy: 0.684500 | 1.437 sec/iter\n",
      "Epoch: 440 | Batch: 008 / 011 | Total loss: 0.931 | Reg loss: 0.025 | Tree loss: 0.931 | Accuracy: 0.645500 | 1.437 sec/iter\n",
      "Epoch: 440 | Batch: 009 / 011 | Total loss: 0.947 | Reg loss: 0.025 | Tree loss: 0.947 | Accuracy: 0.637500 | 1.437 sec/iter\n",
      "Epoch: 440 | Batch: 010 / 011 | Total loss: 0.899 | Reg loss: 0.025 | Tree loss: 0.899 | Accuracy: 0.662116 | 1.437 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 441 | Batch: 000 / 011 | Total loss: 1.000 | Reg loss: 0.025 | Tree loss: 1.000 | Accuracy: 0.669500 | 1.437 sec/iter\n",
      "Epoch: 441 | Batch: 001 / 011 | Total loss: 1.008 | Reg loss: 0.025 | Tree loss: 1.008 | Accuracy: 0.663000 | 1.437 sec/iter\n",
      "Epoch: 441 | Batch: 002 / 011 | Total loss: 0.992 | Reg loss: 0.025 | Tree loss: 0.992 | Accuracy: 0.677000 | 1.437 sec/iter\n",
      "Epoch: 441 | Batch: 003 / 011 | Total loss: 0.967 | Reg loss: 0.025 | Tree loss: 0.967 | Accuracy: 0.681500 | 1.437 sec/iter\n",
      "Epoch: 441 | Batch: 004 / 011 | Total loss: 0.946 | Reg loss: 0.025 | Tree loss: 0.946 | Accuracy: 0.674500 | 1.437 sec/iter\n",
      "Epoch: 441 | Batch: 005 / 011 | Total loss: 0.916 | Reg loss: 0.025 | Tree loss: 0.916 | Accuracy: 0.695000 | 1.438 sec/iter\n",
      "Epoch: 441 | Batch: 006 / 011 | Total loss: 0.929 | Reg loss: 0.025 | Tree loss: 0.929 | Accuracy: 0.680500 | 1.438 sec/iter\n",
      "Epoch: 441 | Batch: 007 / 011 | Total loss: 0.911 | Reg loss: 0.025 | Tree loss: 0.911 | Accuracy: 0.684000 | 1.438 sec/iter\n",
      "Epoch: 441 | Batch: 008 / 011 | Total loss: 0.919 | Reg loss: 0.025 | Tree loss: 0.919 | Accuracy: 0.670500 | 1.438 sec/iter\n",
      "Epoch: 441 | Batch: 009 / 011 | Total loss: 0.895 | Reg loss: 0.025 | Tree loss: 0.895 | Accuracy: 0.676000 | 1.438 sec/iter\n",
      "Epoch: 441 | Batch: 010 / 011 | Total loss: 0.952 | Reg loss: 0.025 | Tree loss: 0.952 | Accuracy: 0.614334 | 1.438 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 442 | Batch: 000 / 011 | Total loss: 1.029 | Reg loss: 0.025 | Tree loss: 1.029 | Accuracy: 0.642000 | 1.438 sec/iter\n",
      "Epoch: 442 | Batch: 001 / 011 | Total loss: 0.969 | Reg loss: 0.025 | Tree loss: 0.969 | Accuracy: 0.675500 | 1.438 sec/iter\n",
      "Epoch: 442 | Batch: 002 / 011 | Total loss: 0.968 | Reg loss: 0.025 | Tree loss: 0.968 | Accuracy: 0.685500 | 1.438 sec/iter\n",
      "Epoch: 442 | Batch: 003 / 011 | Total loss: 0.929 | Reg loss: 0.025 | Tree loss: 0.929 | Accuracy: 0.701000 | 1.438 sec/iter\n",
      "Epoch: 442 | Batch: 004 / 011 | Total loss: 0.941 | Reg loss: 0.025 | Tree loss: 0.941 | Accuracy: 0.697000 | 1.438 sec/iter\n",
      "Epoch: 442 | Batch: 005 / 011 | Total loss: 0.929 | Reg loss: 0.025 | Tree loss: 0.929 | Accuracy: 0.681000 | 1.438 sec/iter\n",
      "Epoch: 442 | Batch: 006 / 011 | Total loss: 0.960 | Reg loss: 0.025 | Tree loss: 0.960 | Accuracy: 0.666000 | 1.438 sec/iter\n",
      "Epoch: 442 | Batch: 007 / 011 | Total loss: 0.961 | Reg loss: 0.025 | Tree loss: 0.961 | Accuracy: 0.645500 | 1.438 sec/iter\n",
      "Epoch: 442 | Batch: 008 / 011 | Total loss: 0.925 | Reg loss: 0.025 | Tree loss: 0.925 | Accuracy: 0.660000 | 1.438 sec/iter\n",
      "Epoch: 442 | Batch: 009 / 011 | Total loss: 0.897 | Reg loss: 0.025 | Tree loss: 0.897 | Accuracy: 0.687000 | 1.438 sec/iter\n",
      "Epoch: 442 | Batch: 010 / 011 | Total loss: 0.862 | Reg loss: 0.025 | Tree loss: 0.862 | Accuracy: 0.716724 | 1.438 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 443 | Batch: 000 / 011 | Total loss: 1.009 | Reg loss: 0.025 | Tree loss: 1.009 | Accuracy: 0.674000 | 1.439 sec/iter\n",
      "Epoch: 443 | Batch: 001 / 011 | Total loss: 0.986 | Reg loss: 0.025 | Tree loss: 0.986 | Accuracy: 0.675000 | 1.439 sec/iter\n",
      "Epoch: 443 | Batch: 002 / 011 | Total loss: 0.976 | Reg loss: 0.025 | Tree loss: 0.976 | Accuracy: 0.687000 | 1.439 sec/iter\n",
      "Epoch: 443 | Batch: 003 / 011 | Total loss: 0.948 | Reg loss: 0.025 | Tree loss: 0.948 | Accuracy: 0.691000 | 1.439 sec/iter\n",
      "Epoch: 443 | Batch: 004 / 011 | Total loss: 0.950 | Reg loss: 0.025 | Tree loss: 0.950 | Accuracy: 0.670500 | 1.439 sec/iter\n",
      "Epoch: 443 | Batch: 005 / 011 | Total loss: 0.935 | Reg loss: 0.025 | Tree loss: 0.935 | Accuracy: 0.691000 | 1.439 sec/iter\n",
      "Epoch: 443 | Batch: 006 / 011 | Total loss: 0.944 | Reg loss: 0.025 | Tree loss: 0.944 | Accuracy: 0.662000 | 1.439 sec/iter\n",
      "Epoch: 443 | Batch: 007 / 011 | Total loss: 0.908 | Reg loss: 0.025 | Tree loss: 0.908 | Accuracy: 0.675500 | 1.439 sec/iter\n",
      "Epoch: 443 | Batch: 008 / 011 | Total loss: 0.921 | Reg loss: 0.025 | Tree loss: 0.921 | Accuracy: 0.663500 | 1.439 sec/iter\n",
      "Epoch: 443 | Batch: 009 / 011 | Total loss: 0.934 | Reg loss: 0.025 | Tree loss: 0.934 | Accuracy: 0.660500 | 1.439 sec/iter\n",
      "Epoch: 443 | Batch: 010 / 011 | Total loss: 0.834 | Reg loss: 0.025 | Tree loss: 0.834 | Accuracy: 0.699659 | 1.439 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 444 | Batch: 000 / 011 | Total loss: 1.037 | Reg loss: 0.025 | Tree loss: 1.037 | Accuracy: 0.645500 | 1.439 sec/iter\n",
      "Epoch: 444 | Batch: 001 / 011 | Total loss: 0.996 | Reg loss: 0.025 | Tree loss: 0.996 | Accuracy: 0.666500 | 1.439 sec/iter\n",
      "Epoch: 444 | Batch: 002 / 011 | Total loss: 0.970 | Reg loss: 0.025 | Tree loss: 0.970 | Accuracy: 0.678000 | 1.439 sec/iter\n",
      "Epoch: 444 | Batch: 003 / 011 | Total loss: 0.943 | Reg loss: 0.025 | Tree loss: 0.943 | Accuracy: 0.684000 | 1.439 sec/iter\n",
      "Epoch: 444 | Batch: 004 / 011 | Total loss: 0.955 | Reg loss: 0.025 | Tree loss: 0.955 | Accuracy: 0.669500 | 1.439 sec/iter\n",
      "Epoch: 444 | Batch: 005 / 011 | Total loss: 0.941 | Reg loss: 0.025 | Tree loss: 0.941 | Accuracy: 0.682500 | 1.439 sec/iter\n",
      "Epoch: 444 | Batch: 006 / 011 | Total loss: 0.945 | Reg loss: 0.025 | Tree loss: 0.945 | Accuracy: 0.666500 | 1.439 sec/iter\n",
      "Epoch: 444 | Batch: 007 / 011 | Total loss: 0.926 | Reg loss: 0.025 | Tree loss: 0.926 | Accuracy: 0.666500 | 1.439 sec/iter\n",
      "Epoch: 444 | Batch: 008 / 011 | Total loss: 0.916 | Reg loss: 0.025 | Tree loss: 0.916 | Accuracy: 0.671500 | 1.439 sec/iter\n",
      "Epoch: 444 | Batch: 009 / 011 | Total loss: 0.868 | Reg loss: 0.025 | Tree loss: 0.868 | Accuracy: 0.694000 | 1.439 sec/iter\n",
      "Epoch: 444 | Batch: 010 / 011 | Total loss: 0.863 | Reg loss: 0.025 | Tree loss: 0.863 | Accuracy: 0.706485 | 1.439 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 445 | Batch: 000 / 011 | Total loss: 1.017 | Reg loss: 0.025 | Tree loss: 1.017 | Accuracy: 0.668500 | 1.44 sec/iter\n",
      "Epoch: 445 | Batch: 001 / 011 | Total loss: 0.992 | Reg loss: 0.025 | Tree loss: 0.992 | Accuracy: 0.671000 | 1.44 sec/iter\n",
      "Epoch: 445 | Batch: 002 / 011 | Total loss: 0.979 | Reg loss: 0.025 | Tree loss: 0.979 | Accuracy: 0.688500 | 1.44 sec/iter\n",
      "Epoch: 445 | Batch: 003 / 011 | Total loss: 0.929 | Reg loss: 0.025 | Tree loss: 0.929 | Accuracy: 0.706000 | 1.44 sec/iter\n",
      "Epoch: 445 | Batch: 004 / 011 | Total loss: 0.951 | Reg loss: 0.025 | Tree loss: 0.951 | Accuracy: 0.675500 | 1.44 sec/iter\n",
      "Epoch: 445 | Batch: 005 / 011 | Total loss: 0.932 | Reg loss: 0.025 | Tree loss: 0.932 | Accuracy: 0.678000 | 1.44 sec/iter\n",
      "Epoch: 445 | Batch: 006 / 011 | Total loss: 0.916 | Reg loss: 0.025 | Tree loss: 0.916 | Accuracy: 0.679000 | 1.44 sec/iter\n",
      "Epoch: 445 | Batch: 007 / 011 | Total loss: 0.945 | Reg loss: 0.025 | Tree loss: 0.945 | Accuracy: 0.664000 | 1.44 sec/iter\n",
      "Epoch: 445 | Batch: 008 / 011 | Total loss: 0.952 | Reg loss: 0.025 | Tree loss: 0.952 | Accuracy: 0.653000 | 1.44 sec/iter\n",
      "Epoch: 445 | Batch: 009 / 011 | Total loss: 0.899 | Reg loss: 0.025 | Tree loss: 0.899 | Accuracy: 0.673500 | 1.44 sec/iter\n",
      "Epoch: 445 | Batch: 010 / 011 | Total loss: 0.861 | Reg loss: 0.025 | Tree loss: 0.861 | Accuracy: 0.726962 | 1.44 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 446 | Batch: 000 / 011 | Total loss: 1.023 | Reg loss: 0.025 | Tree loss: 1.023 | Accuracy: 0.661000 | 1.44 sec/iter\n",
      "Epoch: 446 | Batch: 001 / 011 | Total loss: 0.981 | Reg loss: 0.025 | Tree loss: 0.981 | Accuracy: 0.672000 | 1.44 sec/iter\n",
      "Epoch: 446 | Batch: 002 / 011 | Total loss: 0.974 | Reg loss: 0.025 | Tree loss: 0.974 | Accuracy: 0.684000 | 1.44 sec/iter\n",
      "Epoch: 446 | Batch: 003 / 011 | Total loss: 0.933 | Reg loss: 0.025 | Tree loss: 0.933 | Accuracy: 0.702000 | 1.44 sec/iter\n",
      "Epoch: 446 | Batch: 004 / 011 | Total loss: 0.951 | Reg loss: 0.025 | Tree loss: 0.951 | Accuracy: 0.692000 | 1.44 sec/iter\n",
      "Epoch: 446 | Batch: 005 / 011 | Total loss: 0.949 | Reg loss: 0.025 | Tree loss: 0.949 | Accuracy: 0.663500 | 1.44 sec/iter\n",
      "Epoch: 446 | Batch: 006 / 011 | Total loss: 0.914 | Reg loss: 0.025 | Tree loss: 0.914 | Accuracy: 0.697500 | 1.44 sec/iter\n",
      "Epoch: 446 | Batch: 007 / 011 | Total loss: 0.932 | Reg loss: 0.025 | Tree loss: 0.932 | Accuracy: 0.667500 | 1.44 sec/iter\n",
      "Epoch: 446 | Batch: 008 / 011 | Total loss: 0.918 | Reg loss: 0.025 | Tree loss: 0.918 | Accuracy: 0.659000 | 1.44 sec/iter\n",
      "Epoch: 446 | Batch: 009 / 011 | Total loss: 0.925 | Reg loss: 0.025 | Tree loss: 0.925 | Accuracy: 0.670500 | 1.441 sec/iter\n",
      "Epoch: 446 | Batch: 010 / 011 | Total loss: 0.913 | Reg loss: 0.025 | Tree loss: 0.913 | Accuracy: 0.682594 | 1.441 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 447 | Batch: 000 / 011 | Total loss: 1.017 | Reg loss: 0.025 | Tree loss: 1.017 | Accuracy: 0.641500 | 1.441 sec/iter\n",
      "Epoch: 447 | Batch: 001 / 011 | Total loss: 0.999 | Reg loss: 0.025 | Tree loss: 0.999 | Accuracy: 0.674500 | 1.441 sec/iter\n",
      "Epoch: 447 | Batch: 002 / 011 | Total loss: 0.962 | Reg loss: 0.025 | Tree loss: 0.962 | Accuracy: 0.685500 | 1.441 sec/iter\n",
      "Epoch: 447 | Batch: 003 / 011 | Total loss: 0.976 | Reg loss: 0.025 | Tree loss: 0.976 | Accuracy: 0.689500 | 1.441 sec/iter\n",
      "Epoch: 447 | Batch: 004 / 011 | Total loss: 0.933 | Reg loss: 0.025 | Tree loss: 0.933 | Accuracy: 0.697000 | 1.441 sec/iter\n",
      "Epoch: 447 | Batch: 005 / 011 | Total loss: 0.918 | Reg loss: 0.025 | Tree loss: 0.918 | Accuracy: 0.692000 | 1.441 sec/iter\n",
      "Epoch: 447 | Batch: 006 / 011 | Total loss: 0.928 | Reg loss: 0.025 | Tree loss: 0.928 | Accuracy: 0.680000 | 1.441 sec/iter\n",
      "Epoch: 447 | Batch: 007 / 011 | Total loss: 0.910 | Reg loss: 0.025 | Tree loss: 0.910 | Accuracy: 0.677500 | 1.441 sec/iter\n",
      "Epoch: 447 | Batch: 008 / 011 | Total loss: 0.938 | Reg loss: 0.025 | Tree loss: 0.938 | Accuracy: 0.654500 | 1.441 sec/iter\n",
      "Epoch: 447 | Batch: 009 / 011 | Total loss: 0.923 | Reg loss: 0.025 | Tree loss: 0.923 | Accuracy: 0.657000 | 1.441 sec/iter\n",
      "Epoch: 447 | Batch: 010 / 011 | Total loss: 0.981 | Reg loss: 0.025 | Tree loss: 0.981 | Accuracy: 0.624573 | 1.441 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 448 | Batch: 000 / 011 | Total loss: 1.016 | Reg loss: 0.025 | Tree loss: 1.016 | Accuracy: 0.658500 | 1.441 sec/iter\n",
      "Epoch: 448 | Batch: 001 / 011 | Total loss: 0.978 | Reg loss: 0.025 | Tree loss: 0.978 | Accuracy: 0.665500 | 1.441 sec/iter\n",
      "Epoch: 448 | Batch: 002 / 011 | Total loss: 0.978 | Reg loss: 0.025 | Tree loss: 0.978 | Accuracy: 0.661500 | 1.441 sec/iter\n",
      "Epoch: 448 | Batch: 003 / 011 | Total loss: 0.956 | Reg loss: 0.025 | Tree loss: 0.956 | Accuracy: 0.671000 | 1.441 sec/iter\n",
      "Epoch: 448 | Batch: 004 / 011 | Total loss: 0.957 | Reg loss: 0.025 | Tree loss: 0.957 | Accuracy: 0.683500 | 1.441 sec/iter\n",
      "Epoch: 448 | Batch: 005 / 011 | Total loss: 0.940 | Reg loss: 0.025 | Tree loss: 0.940 | Accuracy: 0.671500 | 1.441 sec/iter\n",
      "Epoch: 448 | Batch: 006 / 011 | Total loss: 0.939 | Reg loss: 0.025 | Tree loss: 0.939 | Accuracy: 0.671500 | 1.441 sec/iter\n",
      "Epoch: 448 | Batch: 007 / 011 | Total loss: 0.930 | Reg loss: 0.025 | Tree loss: 0.930 | Accuracy: 0.671000 | 1.441 sec/iter\n",
      "Epoch: 448 | Batch: 008 / 011 | Total loss: 0.921 | Reg loss: 0.025 | Tree loss: 0.921 | Accuracy: 0.659500 | 1.442 sec/iter\n",
      "Epoch: 448 | Batch: 009 / 011 | Total loss: 0.886 | Reg loss: 0.025 | Tree loss: 0.886 | Accuracy: 0.680000 | 1.442 sec/iter\n",
      "Epoch: 448 | Batch: 010 / 011 | Total loss: 0.927 | Reg loss: 0.025 | Tree loss: 0.927 | Accuracy: 0.662116 | 1.442 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 449 | Batch: 000 / 011 | Total loss: 1.007 | Reg loss: 0.025 | Tree loss: 1.007 | Accuracy: 0.668500 | 1.442 sec/iter\n",
      "Epoch: 449 | Batch: 001 / 011 | Total loss: 0.988 | Reg loss: 0.025 | Tree loss: 0.988 | Accuracy: 0.681500 | 1.442 sec/iter\n",
      "Epoch: 449 | Batch: 002 / 011 | Total loss: 0.966 | Reg loss: 0.025 | Tree loss: 0.966 | Accuracy: 0.675000 | 1.442 sec/iter\n",
      "Epoch: 449 | Batch: 003 / 011 | Total loss: 0.954 | Reg loss: 0.025 | Tree loss: 0.954 | Accuracy: 0.673500 | 1.442 sec/iter\n",
      "Epoch: 449 | Batch: 004 / 011 | Total loss: 0.951 | Reg loss: 0.025 | Tree loss: 0.951 | Accuracy: 0.680500 | 1.442 sec/iter\n",
      "Epoch: 449 | Batch: 005 / 011 | Total loss: 0.975 | Reg loss: 0.025 | Tree loss: 0.975 | Accuracy: 0.668500 | 1.442 sec/iter\n",
      "Epoch: 449 | Batch: 006 / 011 | Total loss: 0.899 | Reg loss: 0.025 | Tree loss: 0.899 | Accuracy: 0.691000 | 1.442 sec/iter\n",
      "Epoch: 449 | Batch: 007 / 011 | Total loss: 0.933 | Reg loss: 0.025 | Tree loss: 0.933 | Accuracy: 0.659000 | 1.442 sec/iter\n",
      "Epoch: 449 | Batch: 008 / 011 | Total loss: 0.905 | Reg loss: 0.025 | Tree loss: 0.905 | Accuracy: 0.680500 | 1.442 sec/iter\n",
      "Epoch: 449 | Batch: 009 / 011 | Total loss: 0.908 | Reg loss: 0.025 | Tree loss: 0.908 | Accuracy: 0.679500 | 1.442 sec/iter\n",
      "Epoch: 449 | Batch: 010 / 011 | Total loss: 0.896 | Reg loss: 0.025 | Tree loss: 0.896 | Accuracy: 0.641638 | 1.442 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 450 | Batch: 000 / 011 | Total loss: 1.009 | Reg loss: 0.025 | Tree loss: 1.009 | Accuracy: 0.662000 | 1.442 sec/iter\n",
      "Epoch: 450 | Batch: 001 / 011 | Total loss: 1.003 | Reg loss: 0.025 | Tree loss: 1.003 | Accuracy: 0.666000 | 1.442 sec/iter\n",
      "Epoch: 450 | Batch: 002 / 011 | Total loss: 0.990 | Reg loss: 0.025 | Tree loss: 0.990 | Accuracy: 0.656000 | 1.442 sec/iter\n",
      "Epoch: 450 | Batch: 003 / 011 | Total loss: 0.949 | Reg loss: 0.025 | Tree loss: 0.949 | Accuracy: 0.682500 | 1.442 sec/iter\n",
      "Epoch: 450 | Batch: 004 / 011 | Total loss: 0.933 | Reg loss: 0.025 | Tree loss: 0.933 | Accuracy: 0.680000 | 1.442 sec/iter\n",
      "Epoch: 450 | Batch: 005 / 011 | Total loss: 0.932 | Reg loss: 0.025 | Tree loss: 0.932 | Accuracy: 0.674000 | 1.442 sec/iter\n",
      "Epoch: 450 | Batch: 006 / 011 | Total loss: 0.917 | Reg loss: 0.025 | Tree loss: 0.917 | Accuracy: 0.673000 | 1.442 sec/iter\n",
      "Epoch: 450 | Batch: 007 / 011 | Total loss: 0.917 | Reg loss: 0.025 | Tree loss: 0.917 | Accuracy: 0.672500 | 1.442 sec/iter\n",
      "Epoch: 450 | Batch: 008 / 011 | Total loss: 0.937 | Reg loss: 0.025 | Tree loss: 0.937 | Accuracy: 0.657000 | 1.443 sec/iter\n",
      "Epoch: 450 | Batch: 009 / 011 | Total loss: 0.914 | Reg loss: 0.025 | Tree loss: 0.914 | Accuracy: 0.668000 | 1.443 sec/iter\n",
      "Epoch: 450 | Batch: 010 / 011 | Total loss: 0.905 | Reg loss: 0.025 | Tree loss: 0.905 | Accuracy: 0.682594 | 1.443 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 451 | Batch: 000 / 011 | Total loss: 0.994 | Reg loss: 0.025 | Tree loss: 0.994 | Accuracy: 0.667500 | 1.443 sec/iter\n",
      "Epoch: 451 | Batch: 001 / 011 | Total loss: 0.982 | Reg loss: 0.025 | Tree loss: 0.982 | Accuracy: 0.686000 | 1.443 sec/iter\n",
      "Epoch: 451 | Batch: 002 / 011 | Total loss: 0.977 | Reg loss: 0.025 | Tree loss: 0.977 | Accuracy: 0.684500 | 1.443 sec/iter\n",
      "Epoch: 451 | Batch: 003 / 011 | Total loss: 0.944 | Reg loss: 0.025 | Tree loss: 0.944 | Accuracy: 0.710000 | 1.443 sec/iter\n",
      "Epoch: 451 | Batch: 004 / 011 | Total loss: 0.939 | Reg loss: 0.025 | Tree loss: 0.939 | Accuracy: 0.672000 | 1.443 sec/iter\n",
      "Epoch: 451 | Batch: 005 / 011 | Total loss: 0.930 | Reg loss: 0.025 | Tree loss: 0.930 | Accuracy: 0.675500 | 1.443 sec/iter\n",
      "Epoch: 451 | Batch: 006 / 011 | Total loss: 0.930 | Reg loss: 0.025 | Tree loss: 0.930 | Accuracy: 0.676500 | 1.443 sec/iter\n",
      "Epoch: 451 | Batch: 007 / 011 | Total loss: 0.949 | Reg loss: 0.025 | Tree loss: 0.949 | Accuracy: 0.650500 | 1.443 sec/iter\n",
      "Epoch: 451 | Batch: 008 / 011 | Total loss: 0.912 | Reg loss: 0.025 | Tree loss: 0.912 | Accuracy: 0.672500 | 1.443 sec/iter\n",
      "Epoch: 451 | Batch: 009 / 011 | Total loss: 0.923 | Reg loss: 0.025 | Tree loss: 0.923 | Accuracy: 0.668500 | 1.443 sec/iter\n",
      "Epoch: 451 | Batch: 010 / 011 | Total loss: 0.918 | Reg loss: 0.025 | Tree loss: 0.918 | Accuracy: 0.686007 | 1.443 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 452 | Batch: 000 / 011 | Total loss: 0.997 | Reg loss: 0.025 | Tree loss: 0.997 | Accuracy: 0.670500 | 1.443 sec/iter\n",
      "Epoch: 452 | Batch: 001 / 011 | Total loss: 0.997 | Reg loss: 0.025 | Tree loss: 0.997 | Accuracy: 0.667500 | 1.443 sec/iter\n",
      "Epoch: 452 | Batch: 002 / 011 | Total loss: 0.984 | Reg loss: 0.025 | Tree loss: 0.984 | Accuracy: 0.674000 | 1.443 sec/iter\n",
      "Epoch: 452 | Batch: 003 / 011 | Total loss: 0.960 | Reg loss: 0.025 | Tree loss: 0.960 | Accuracy: 0.674000 | 1.443 sec/iter\n",
      "Epoch: 452 | Batch: 004 / 011 | Total loss: 0.948 | Reg loss: 0.025 | Tree loss: 0.948 | Accuracy: 0.677500 | 1.443 sec/iter\n",
      "Epoch: 452 | Batch: 005 / 011 | Total loss: 0.940 | Reg loss: 0.025 | Tree loss: 0.940 | Accuracy: 0.671500 | 1.443 sec/iter\n",
      "Epoch: 452 | Batch: 006 / 011 | Total loss: 0.911 | Reg loss: 0.025 | Tree loss: 0.911 | Accuracy: 0.685500 | 1.443 sec/iter\n",
      "Epoch: 452 | Batch: 007 / 011 | Total loss: 0.902 | Reg loss: 0.025 | Tree loss: 0.902 | Accuracy: 0.687500 | 1.443 sec/iter\n",
      "Epoch: 452 | Batch: 008 / 011 | Total loss: 0.940 | Reg loss: 0.025 | Tree loss: 0.940 | Accuracy: 0.655000 | 1.443 sec/iter\n",
      "Epoch: 452 | Batch: 009 / 011 | Total loss: 0.908 | Reg loss: 0.025 | Tree loss: 0.908 | Accuracy: 0.678500 | 1.443 sec/iter\n",
      "Epoch: 452 | Batch: 010 / 011 | Total loss: 0.881 | Reg loss: 0.025 | Tree loss: 0.881 | Accuracy: 0.645051 | 1.443 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 453 | Batch: 000 / 011 | Total loss: 0.996 | Reg loss: 0.025 | Tree loss: 0.996 | Accuracy: 0.664000 | 1.443 sec/iter\n",
      "Epoch: 453 | Batch: 001 / 011 | Total loss: 0.998 | Reg loss: 0.025 | Tree loss: 0.998 | Accuracy: 0.661000 | 1.443 sec/iter\n",
      "Epoch: 453 | Batch: 002 / 011 | Total loss: 0.963 | Reg loss: 0.025 | Tree loss: 0.963 | Accuracy: 0.681000 | 1.443 sec/iter\n",
      "Epoch: 453 | Batch: 003 / 011 | Total loss: 0.961 | Reg loss: 0.025 | Tree loss: 0.961 | Accuracy: 0.692500 | 1.443 sec/iter\n",
      "Epoch: 453 | Batch: 004 / 011 | Total loss: 0.935 | Reg loss: 0.025 | Tree loss: 0.935 | Accuracy: 0.686500 | 1.442 sec/iter\n",
      "Epoch: 453 | Batch: 005 / 011 | Total loss: 0.927 | Reg loss: 0.025 | Tree loss: 0.927 | Accuracy: 0.688000 | 1.442 sec/iter\n",
      "Epoch: 453 | Batch: 006 / 011 | Total loss: 0.919 | Reg loss: 0.025 | Tree loss: 0.919 | Accuracy: 0.675000 | 1.442 sec/iter\n",
      "Epoch: 453 | Batch: 007 / 011 | Total loss: 0.962 | Reg loss: 0.025 | Tree loss: 0.962 | Accuracy: 0.641000 | 1.442 sec/iter\n",
      "Epoch: 453 | Batch: 008 / 011 | Total loss: 0.910 | Reg loss: 0.025 | Tree loss: 0.910 | Accuracy: 0.681000 | 1.442 sec/iter\n",
      "Epoch: 453 | Batch: 009 / 011 | Total loss: 0.926 | Reg loss: 0.025 | Tree loss: 0.926 | Accuracy: 0.649500 | 1.442 sec/iter\n",
      "Epoch: 453 | Batch: 010 / 011 | Total loss: 0.899 | Reg loss: 0.025 | Tree loss: 0.899 | Accuracy: 0.672355 | 1.442 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 454 | Batch: 000 / 011 | Total loss: 1.032 | Reg loss: 0.025 | Tree loss: 1.032 | Accuracy: 0.657000 | 1.442 sec/iter\n",
      "Epoch: 454 | Batch: 001 / 011 | Total loss: 0.986 | Reg loss: 0.025 | Tree loss: 0.986 | Accuracy: 0.673500 | 1.442 sec/iter\n",
      "Epoch: 454 | Batch: 002 / 011 | Total loss: 0.983 | Reg loss: 0.025 | Tree loss: 0.983 | Accuracy: 0.654500 | 1.442 sec/iter\n",
      "Epoch: 454 | Batch: 003 / 011 | Total loss: 0.970 | Reg loss: 0.025 | Tree loss: 0.970 | Accuracy: 0.675000 | 1.442 sec/iter\n",
      "Epoch: 454 | Batch: 004 / 011 | Total loss: 0.953 | Reg loss: 0.025 | Tree loss: 0.953 | Accuracy: 0.671000 | 1.442 sec/iter\n",
      "Epoch: 454 | Batch: 005 / 011 | Total loss: 0.926 | Reg loss: 0.025 | Tree loss: 0.926 | Accuracy: 0.703500 | 1.442 sec/iter\n",
      "Epoch: 454 | Batch: 006 / 011 | Total loss: 0.903 | Reg loss: 0.025 | Tree loss: 0.903 | Accuracy: 0.693000 | 1.442 sec/iter\n",
      "Epoch: 454 | Batch: 007 / 011 | Total loss: 0.926 | Reg loss: 0.025 | Tree loss: 0.926 | Accuracy: 0.689500 | 1.442 sec/iter\n",
      "Epoch: 454 | Batch: 008 / 011 | Total loss: 0.928 | Reg loss: 0.025 | Tree loss: 0.928 | Accuracy: 0.648000 | 1.441 sec/iter\n",
      "Epoch: 454 | Batch: 009 / 011 | Total loss: 0.897 | Reg loss: 0.025 | Tree loss: 0.897 | Accuracy: 0.683500 | 1.441 sec/iter\n",
      "Epoch: 454 | Batch: 010 / 011 | Total loss: 0.840 | Reg loss: 0.025 | Tree loss: 0.840 | Accuracy: 0.706485 | 1.441 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 455 | Batch: 000 / 011 | Total loss: 0.997 | Reg loss: 0.025 | Tree loss: 0.997 | Accuracy: 0.668000 | 1.442 sec/iter\n",
      "Epoch: 455 | Batch: 001 / 011 | Total loss: 0.992 | Reg loss: 0.025 | Tree loss: 0.992 | Accuracy: 0.654500 | 1.442 sec/iter\n",
      "Epoch: 455 | Batch: 002 / 011 | Total loss: 0.972 | Reg loss: 0.025 | Tree loss: 0.972 | Accuracy: 0.686000 | 1.441 sec/iter\n",
      "Epoch: 455 | Batch: 003 / 011 | Total loss: 0.957 | Reg loss: 0.025 | Tree loss: 0.957 | Accuracy: 0.691500 | 1.441 sec/iter\n",
      "Epoch: 455 | Batch: 004 / 011 | Total loss: 0.936 | Reg loss: 0.025 | Tree loss: 0.936 | Accuracy: 0.681500 | 1.441 sec/iter\n",
      "Epoch: 455 | Batch: 005 / 011 | Total loss: 0.940 | Reg loss: 0.025 | Tree loss: 0.940 | Accuracy: 0.680500 | 1.441 sec/iter\n",
      "Epoch: 455 | Batch: 006 / 011 | Total loss: 0.906 | Reg loss: 0.025 | Tree loss: 0.906 | Accuracy: 0.689500 | 1.441 sec/iter\n",
      "Epoch: 455 | Batch: 007 / 011 | Total loss: 0.921 | Reg loss: 0.025 | Tree loss: 0.921 | Accuracy: 0.679500 | 1.441 sec/iter\n",
      "Epoch: 455 | Batch: 008 / 011 | Total loss: 0.924 | Reg loss: 0.025 | Tree loss: 0.924 | Accuracy: 0.670500 | 1.441 sec/iter\n",
      "Epoch: 455 | Batch: 009 / 011 | Total loss: 0.947 | Reg loss: 0.025 | Tree loss: 0.947 | Accuracy: 0.651500 | 1.441 sec/iter\n",
      "Epoch: 455 | Batch: 010 / 011 | Total loss: 0.864 | Reg loss: 0.025 | Tree loss: 0.864 | Accuracy: 0.692833 | 1.441 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 456 | Batch: 000 / 011 | Total loss: 0.996 | Reg loss: 0.025 | Tree loss: 0.996 | Accuracy: 0.660500 | 1.441 sec/iter\n",
      "Epoch: 456 | Batch: 001 / 011 | Total loss: 1.015 | Reg loss: 0.025 | Tree loss: 1.015 | Accuracy: 0.654000 | 1.441 sec/iter\n",
      "Epoch: 456 | Batch: 002 / 011 | Total loss: 0.959 | Reg loss: 0.025 | Tree loss: 0.959 | Accuracy: 0.678500 | 1.441 sec/iter\n",
      "Epoch: 456 | Batch: 003 / 011 | Total loss: 0.955 | Reg loss: 0.025 | Tree loss: 0.955 | Accuracy: 0.692000 | 1.441 sec/iter\n",
      "Epoch: 456 | Batch: 004 / 011 | Total loss: 0.906 | Reg loss: 0.025 | Tree loss: 0.906 | Accuracy: 0.720500 | 1.441 sec/iter\n",
      "Epoch: 456 | Batch: 005 / 011 | Total loss: 0.940 | Reg loss: 0.025 | Tree loss: 0.940 | Accuracy: 0.684500 | 1.441 sec/iter\n",
      "Epoch: 456 | Batch: 006 / 011 | Total loss: 0.936 | Reg loss: 0.025 | Tree loss: 0.936 | Accuracy: 0.670000 | 1.441 sec/iter\n",
      "Epoch: 456 | Batch: 007 / 011 | Total loss: 0.917 | Reg loss: 0.025 | Tree loss: 0.917 | Accuracy: 0.669000 | 1.441 sec/iter\n",
      "Epoch: 456 | Batch: 008 / 011 | Total loss: 0.954 | Reg loss: 0.025 | Tree loss: 0.954 | Accuracy: 0.650000 | 1.44 sec/iter\n",
      "Epoch: 456 | Batch: 009 / 011 | Total loss: 0.918 | Reg loss: 0.025 | Tree loss: 0.918 | Accuracy: 0.658000 | 1.44 sec/iter\n",
      "Epoch: 456 | Batch: 010 / 011 | Total loss: 0.857 | Reg loss: 0.025 | Tree loss: 0.857 | Accuracy: 0.699659 | 1.44 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 457 | Batch: 000 / 011 | Total loss: 1.007 | Reg loss: 0.025 | Tree loss: 1.007 | Accuracy: 0.672500 | 1.441 sec/iter\n",
      "Epoch: 457 | Batch: 001 / 011 | Total loss: 1.013 | Reg loss: 0.025 | Tree loss: 1.013 | Accuracy: 0.650000 | 1.441 sec/iter\n",
      "Epoch: 457 | Batch: 002 / 011 | Total loss: 0.967 | Reg loss: 0.025 | Tree loss: 0.967 | Accuracy: 0.675500 | 1.441 sec/iter\n",
      "Epoch: 457 | Batch: 003 / 011 | Total loss: 0.956 | Reg loss: 0.025 | Tree loss: 0.956 | Accuracy: 0.684500 | 1.44 sec/iter\n",
      "Epoch: 457 | Batch: 004 / 011 | Total loss: 0.926 | Reg loss: 0.025 | Tree loss: 0.926 | Accuracy: 0.702500 | 1.44 sec/iter\n",
      "Epoch: 457 | Batch: 005 / 011 | Total loss: 0.949 | Reg loss: 0.025 | Tree loss: 0.949 | Accuracy: 0.663500 | 1.44 sec/iter\n",
      "Epoch: 457 | Batch: 006 / 011 | Total loss: 0.892 | Reg loss: 0.025 | Tree loss: 0.892 | Accuracy: 0.703500 | 1.44 sec/iter\n",
      "Epoch: 457 | Batch: 007 / 011 | Total loss: 0.914 | Reg loss: 0.025 | Tree loss: 0.914 | Accuracy: 0.676500 | 1.44 sec/iter\n",
      "Epoch: 457 | Batch: 008 / 011 | Total loss: 0.921 | Reg loss: 0.025 | Tree loss: 0.921 | Accuracy: 0.664500 | 1.44 sec/iter\n",
      "Epoch: 457 | Batch: 009 / 011 | Total loss: 0.930 | Reg loss: 0.025 | Tree loss: 0.930 | Accuracy: 0.648500 | 1.44 sec/iter\n",
      "Epoch: 457 | Batch: 010 / 011 | Total loss: 0.910 | Reg loss: 0.025 | Tree loss: 0.910 | Accuracy: 0.627986 | 1.44 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 458 | Batch: 000 / 011 | Total loss: 1.016 | Reg loss: 0.025 | Tree loss: 1.016 | Accuracy: 0.668000 | 1.44 sec/iter\n",
      "Epoch: 458 | Batch: 001 / 011 | Total loss: 0.994 | Reg loss: 0.025 | Tree loss: 0.994 | Accuracy: 0.673500 | 1.44 sec/iter\n",
      "Epoch: 458 | Batch: 002 / 011 | Total loss: 0.981 | Reg loss: 0.025 | Tree loss: 0.981 | Accuracy: 0.678500 | 1.44 sec/iter\n",
      "Epoch: 458 | Batch: 003 / 011 | Total loss: 0.952 | Reg loss: 0.025 | Tree loss: 0.952 | Accuracy: 0.679500 | 1.44 sec/iter\n",
      "Epoch: 458 | Batch: 004 / 011 | Total loss: 0.949 | Reg loss: 0.025 | Tree loss: 0.949 | Accuracy: 0.687000 | 1.44 sec/iter\n",
      "Epoch: 458 | Batch: 005 / 011 | Total loss: 0.946 | Reg loss: 0.025 | Tree loss: 0.946 | Accuracy: 0.664500 | 1.44 sec/iter\n",
      "Epoch: 458 | Batch: 006 / 011 | Total loss: 0.925 | Reg loss: 0.025 | Tree loss: 0.925 | Accuracy: 0.679500 | 1.439 sec/iter\n",
      "Epoch: 458 | Batch: 007 / 011 | Total loss: 0.923 | Reg loss: 0.025 | Tree loss: 0.923 | Accuracy: 0.681500 | 1.439 sec/iter\n",
      "Epoch: 458 | Batch: 008 / 011 | Total loss: 0.915 | Reg loss: 0.025 | Tree loss: 0.915 | Accuracy: 0.650000 | 1.439 sec/iter\n",
      "Epoch: 458 | Batch: 009 / 011 | Total loss: 0.894 | Reg loss: 0.025 | Tree loss: 0.894 | Accuracy: 0.678500 | 1.439 sec/iter\n",
      "Epoch: 458 | Batch: 010 / 011 | Total loss: 0.913 | Reg loss: 0.025 | Tree loss: 0.913 | Accuracy: 0.645051 | 1.439 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 459 | Batch: 000 / 011 | Total loss: 1.032 | Reg loss: 0.025 | Tree loss: 1.032 | Accuracy: 0.655000 | 1.44 sec/iter\n",
      "Epoch: 459 | Batch: 001 / 011 | Total loss: 0.993 | Reg loss: 0.025 | Tree loss: 0.993 | Accuracy: 0.663000 | 1.44 sec/iter\n",
      "Epoch: 459 | Batch: 002 / 011 | Total loss: 0.983 | Reg loss: 0.025 | Tree loss: 0.983 | Accuracy: 0.676000 | 1.44 sec/iter\n",
      "Epoch: 459 | Batch: 003 / 011 | Total loss: 0.971 | Reg loss: 0.025 | Tree loss: 0.971 | Accuracy: 0.674500 | 1.44 sec/iter\n",
      "Epoch: 459 | Batch: 004 / 011 | Total loss: 0.939 | Reg loss: 0.025 | Tree loss: 0.939 | Accuracy: 0.689500 | 1.44 sec/iter\n",
      "Epoch: 459 | Batch: 005 / 011 | Total loss: 0.933 | Reg loss: 0.025 | Tree loss: 0.933 | Accuracy: 0.681000 | 1.439 sec/iter\n",
      "Epoch: 459 | Batch: 006 / 011 | Total loss: 0.923 | Reg loss: 0.025 | Tree loss: 0.923 | Accuracy: 0.680500 | 1.439 sec/iter\n",
      "Epoch: 459 | Batch: 007 / 011 | Total loss: 0.918 | Reg loss: 0.025 | Tree loss: 0.918 | Accuracy: 0.680500 | 1.439 sec/iter\n",
      "Epoch: 459 | Batch: 008 / 011 | Total loss: 0.914 | Reg loss: 0.025 | Tree loss: 0.914 | Accuracy: 0.681000 | 1.439 sec/iter\n",
      "Epoch: 459 | Batch: 009 / 011 | Total loss: 0.906 | Reg loss: 0.025 | Tree loss: 0.906 | Accuracy: 0.659500 | 1.439 sec/iter\n",
      "Epoch: 459 | Batch: 010 / 011 | Total loss: 0.865 | Reg loss: 0.025 | Tree loss: 0.865 | Accuracy: 0.706485 | 1.439 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 460 | Batch: 000 / 011 | Total loss: 1.000 | Reg loss: 0.025 | Tree loss: 1.000 | Accuracy: 0.673500 | 1.439 sec/iter\n",
      "Epoch: 460 | Batch: 001 / 011 | Total loss: 1.003 | Reg loss: 0.025 | Tree loss: 1.003 | Accuracy: 0.663000 | 1.439 sec/iter\n",
      "Epoch: 460 | Batch: 002 / 011 | Total loss: 1.004 | Reg loss: 0.025 | Tree loss: 1.004 | Accuracy: 0.660500 | 1.439 sec/iter\n",
      "Epoch: 460 | Batch: 003 / 011 | Total loss: 0.951 | Reg loss: 0.025 | Tree loss: 0.951 | Accuracy: 0.686500 | 1.439 sec/iter\n",
      "Epoch: 460 | Batch: 004 / 011 | Total loss: 0.934 | Reg loss: 0.025 | Tree loss: 0.934 | Accuracy: 0.683000 | 1.439 sec/iter\n",
      "Epoch: 460 | Batch: 005 / 011 | Total loss: 0.924 | Reg loss: 0.025 | Tree loss: 0.924 | Accuracy: 0.691500 | 1.439 sec/iter\n",
      "Epoch: 460 | Batch: 006 / 011 | Total loss: 0.926 | Reg loss: 0.025 | Tree loss: 0.926 | Accuracy: 0.670500 | 1.439 sec/iter\n",
      "Epoch: 460 | Batch: 007 / 011 | Total loss: 0.910 | Reg loss: 0.025 | Tree loss: 0.910 | Accuracy: 0.673000 | 1.439 sec/iter\n",
      "Epoch: 460 | Batch: 008 / 011 | Total loss: 0.949 | Reg loss: 0.025 | Tree loss: 0.949 | Accuracy: 0.655000 | 1.439 sec/iter\n",
      "Epoch: 460 | Batch: 009 / 011 | Total loss: 0.899 | Reg loss: 0.025 | Tree loss: 0.899 | Accuracy: 0.679000 | 1.439 sec/iter\n",
      "Epoch: 460 | Batch: 010 / 011 | Total loss: 0.876 | Reg loss: 0.025 | Tree loss: 0.876 | Accuracy: 0.692833 | 1.439 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 461 | Batch: 000 / 011 | Total loss: 1.010 | Reg loss: 0.025 | Tree loss: 1.010 | Accuracy: 0.655500 | 1.44 sec/iter\n",
      "Epoch: 461 | Batch: 001 / 011 | Total loss: 0.986 | Reg loss: 0.025 | Tree loss: 0.986 | Accuracy: 0.667500 | 1.44 sec/iter\n",
      "Epoch: 461 | Batch: 002 / 011 | Total loss: 0.981 | Reg loss: 0.025 | Tree loss: 0.981 | Accuracy: 0.670000 | 1.44 sec/iter\n",
      "Epoch: 461 | Batch: 003 / 011 | Total loss: 0.948 | Reg loss: 0.025 | Tree loss: 0.948 | Accuracy: 0.673500 | 1.44 sec/iter\n",
      "Epoch: 461 | Batch: 004 / 011 | Total loss: 0.932 | Reg loss: 0.025 | Tree loss: 0.932 | Accuracy: 0.690500 | 1.439 sec/iter\n",
      "Epoch: 461 | Batch: 005 / 011 | Total loss: 0.937 | Reg loss: 0.025 | Tree loss: 0.937 | Accuracy: 0.673500 | 1.439 sec/iter\n",
      "Epoch: 461 | Batch: 006 / 011 | Total loss: 0.954 | Reg loss: 0.025 | Tree loss: 0.954 | Accuracy: 0.653000 | 1.439 sec/iter\n",
      "Epoch: 461 | Batch: 007 / 011 | Total loss: 0.941 | Reg loss: 0.025 | Tree loss: 0.941 | Accuracy: 0.654000 | 1.439 sec/iter\n",
      "Epoch: 461 | Batch: 008 / 011 | Total loss: 0.893 | Reg loss: 0.025 | Tree loss: 0.893 | Accuracy: 0.679000 | 1.439 sec/iter\n",
      "Epoch: 461 | Batch: 009 / 011 | Total loss: 0.910 | Reg loss: 0.025 | Tree loss: 0.910 | Accuracy: 0.670500 | 1.439 sec/iter\n",
      "Epoch: 461 | Batch: 010 / 011 | Total loss: 0.937 | Reg loss: 0.025 | Tree loss: 0.937 | Accuracy: 0.696246 | 1.439 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 462 | Batch: 000 / 011 | Total loss: 1.039 | Reg loss: 0.025 | Tree loss: 1.039 | Accuracy: 0.640000 | 1.44 sec/iter\n",
      "Epoch: 462 | Batch: 001 / 011 | Total loss: 1.004 | Reg loss: 0.025 | Tree loss: 1.004 | Accuracy: 0.655000 | 1.44 sec/iter\n",
      "Epoch: 462 | Batch: 002 / 011 | Total loss: 0.964 | Reg loss: 0.025 | Tree loss: 0.964 | Accuracy: 0.670000 | 1.44 sec/iter\n",
      "Epoch: 462 | Batch: 003 / 011 | Total loss: 0.934 | Reg loss: 0.025 | Tree loss: 0.934 | Accuracy: 0.703500 | 1.44 sec/iter\n",
      "Epoch: 462 | Batch: 004 / 011 | Total loss: 0.923 | Reg loss: 0.025 | Tree loss: 0.923 | Accuracy: 0.691000 | 1.44 sec/iter\n",
      "Epoch: 462 | Batch: 005 / 011 | Total loss: 0.922 | Reg loss: 0.025 | Tree loss: 0.922 | Accuracy: 0.683000 | 1.44 sec/iter\n",
      "Epoch: 462 | Batch: 006 / 011 | Total loss: 0.926 | Reg loss: 0.025 | Tree loss: 0.926 | Accuracy: 0.668000 | 1.44 sec/iter\n",
      "Epoch: 462 | Batch: 007 / 011 | Total loss: 0.955 | Reg loss: 0.025 | Tree loss: 0.955 | Accuracy: 0.646000 | 1.439 sec/iter\n",
      "Epoch: 462 | Batch: 008 / 011 | Total loss: 0.905 | Reg loss: 0.025 | Tree loss: 0.905 | Accuracy: 0.685000 | 1.439 sec/iter\n",
      "Epoch: 462 | Batch: 009 / 011 | Total loss: 0.912 | Reg loss: 0.025 | Tree loss: 0.912 | Accuracy: 0.676500 | 1.439 sec/iter\n",
      "Epoch: 462 | Batch: 010 / 011 | Total loss: 0.943 | Reg loss: 0.025 | Tree loss: 0.943 | Accuracy: 0.658703 | 1.439 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 463 | Batch: 000 / 011 | Total loss: 1.014 | Reg loss: 0.025 | Tree loss: 1.014 | Accuracy: 0.645000 | 1.44 sec/iter\n",
      "Epoch: 463 | Batch: 001 / 011 | Total loss: 0.999 | Reg loss: 0.025 | Tree loss: 0.999 | Accuracy: 0.653500 | 1.44 sec/iter\n",
      "Epoch: 463 | Batch: 002 / 011 | Total loss: 0.975 | Reg loss: 0.025 | Tree loss: 0.975 | Accuracy: 0.683500 | 1.44 sec/iter\n",
      "Epoch: 463 | Batch: 003 / 011 | Total loss: 0.938 | Reg loss: 0.025 | Tree loss: 0.938 | Accuracy: 0.703000 | 1.44 sec/iter\n",
      "Epoch: 463 | Batch: 004 / 011 | Total loss: 0.928 | Reg loss: 0.025 | Tree loss: 0.928 | Accuracy: 0.703500 | 1.44 sec/iter\n",
      "Epoch: 463 | Batch: 005 / 011 | Total loss: 0.935 | Reg loss: 0.025 | Tree loss: 0.935 | Accuracy: 0.673500 | 1.44 sec/iter\n",
      "Epoch: 463 | Batch: 006 / 011 | Total loss: 0.932 | Reg loss: 0.025 | Tree loss: 0.932 | Accuracy: 0.661000 | 1.44 sec/iter\n",
      "Epoch: 463 | Batch: 007 / 011 | Total loss: 0.927 | Reg loss: 0.025 | Tree loss: 0.927 | Accuracy: 0.665500 | 1.44 sec/iter\n",
      "Epoch: 463 | Batch: 008 / 011 | Total loss: 0.931 | Reg loss: 0.025 | Tree loss: 0.931 | Accuracy: 0.663000 | 1.439 sec/iter\n",
      "Epoch: 463 | Batch: 009 / 011 | Total loss: 0.923 | Reg loss: 0.025 | Tree loss: 0.923 | Accuracy: 0.672500 | 1.439 sec/iter\n",
      "Epoch: 463 | Batch: 010 / 011 | Total loss: 0.923 | Reg loss: 0.025 | Tree loss: 0.923 | Accuracy: 0.631399 | 1.439 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 464 | Batch: 000 / 011 | Total loss: 1.017 | Reg loss: 0.025 | Tree loss: 1.017 | Accuracy: 0.661500 | 1.44 sec/iter\n",
      "Epoch: 464 | Batch: 001 / 011 | Total loss: 1.017 | Reg loss: 0.025 | Tree loss: 1.017 | Accuracy: 0.651500 | 1.439 sec/iter\n",
      "Epoch: 464 | Batch: 002 / 011 | Total loss: 0.977 | Reg loss: 0.025 | Tree loss: 0.977 | Accuracy: 0.678500 | 1.439 sec/iter\n",
      "Epoch: 464 | Batch: 003 / 011 | Total loss: 0.975 | Reg loss: 0.025 | Tree loss: 0.975 | Accuracy: 0.672000 | 1.439 sec/iter\n",
      "Epoch: 464 | Batch: 004 / 011 | Total loss: 0.924 | Reg loss: 0.025 | Tree loss: 0.924 | Accuracy: 0.699000 | 1.439 sec/iter\n",
      "Epoch: 464 | Batch: 005 / 011 | Total loss: 0.933 | Reg loss: 0.025 | Tree loss: 0.933 | Accuracy: 0.689500 | 1.439 sec/iter\n",
      "Epoch: 464 | Batch: 006 / 011 | Total loss: 0.924 | Reg loss: 0.025 | Tree loss: 0.924 | Accuracy: 0.671000 | 1.439 sec/iter\n",
      "Epoch: 464 | Batch: 007 / 011 | Total loss: 0.917 | Reg loss: 0.025 | Tree loss: 0.917 | Accuracy: 0.678500 | 1.439 sec/iter\n",
      "Epoch: 464 | Batch: 008 / 011 | Total loss: 0.906 | Reg loss: 0.025 | Tree loss: 0.906 | Accuracy: 0.670500 | 1.439 sec/iter\n",
      "Epoch: 464 | Batch: 009 / 011 | Total loss: 0.909 | Reg loss: 0.025 | Tree loss: 0.909 | Accuracy: 0.671500 | 1.439 sec/iter\n",
      "Epoch: 464 | Batch: 010 / 011 | Total loss: 0.892 | Reg loss: 0.025 | Tree loss: 0.892 | Accuracy: 0.692833 | 1.439 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 465 | Batch: 000 / 011 | Total loss: 0.989 | Reg loss: 0.025 | Tree loss: 0.989 | Accuracy: 0.670500 | 1.44 sec/iter\n",
      "Epoch: 465 | Batch: 001 / 011 | Total loss: 1.014 | Reg loss: 0.025 | Tree loss: 1.014 | Accuracy: 0.639500 | 1.44 sec/iter\n",
      "Epoch: 465 | Batch: 002 / 011 | Total loss: 0.965 | Reg loss: 0.025 | Tree loss: 0.965 | Accuracy: 0.681000 | 1.44 sec/iter\n",
      "Epoch: 465 | Batch: 003 / 011 | Total loss: 0.978 | Reg loss: 0.025 | Tree loss: 0.978 | Accuracy: 0.660500 | 1.44 sec/iter\n",
      "Epoch: 465 | Batch: 004 / 011 | Total loss: 0.953 | Reg loss: 0.025 | Tree loss: 0.953 | Accuracy: 0.663500 | 1.44 sec/iter\n",
      "Epoch: 465 | Batch: 005 / 011 | Total loss: 0.936 | Reg loss: 0.025 | Tree loss: 0.936 | Accuracy: 0.672500 | 1.44 sec/iter\n",
      "Epoch: 465 | Batch: 006 / 011 | Total loss: 0.930 | Reg loss: 0.025 | Tree loss: 0.930 | Accuracy: 0.678500 | 1.439 sec/iter\n",
      "Epoch: 465 | Batch: 007 / 011 | Total loss: 0.902 | Reg loss: 0.025 | Tree loss: 0.902 | Accuracy: 0.682500 | 1.439 sec/iter\n",
      "Epoch: 465 | Batch: 008 / 011 | Total loss: 0.899 | Reg loss: 0.025 | Tree loss: 0.899 | Accuracy: 0.679000 | 1.439 sec/iter\n",
      "Epoch: 465 | Batch: 009 / 011 | Total loss: 0.900 | Reg loss: 0.025 | Tree loss: 0.900 | Accuracy: 0.679000 | 1.439 sec/iter\n",
      "Epoch: 465 | Batch: 010 / 011 | Total loss: 0.933 | Reg loss: 0.025 | Tree loss: 0.933 | Accuracy: 0.668942 | 1.439 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 466 | Batch: 000 / 011 | Total loss: 1.029 | Reg loss: 0.025 | Tree loss: 1.029 | Accuracy: 0.654500 | 1.44 sec/iter\n",
      "Epoch: 466 | Batch: 001 / 011 | Total loss: 0.994 | Reg loss: 0.025 | Tree loss: 0.994 | Accuracy: 0.670000 | 1.44 sec/iter\n",
      "Epoch: 466 | Batch: 002 / 011 | Total loss: 0.966 | Reg loss: 0.025 | Tree loss: 0.966 | Accuracy: 0.681500 | 1.44 sec/iter\n",
      "Epoch: 466 | Batch: 003 / 011 | Total loss: 0.942 | Reg loss: 0.025 | Tree loss: 0.942 | Accuracy: 0.698500 | 1.44 sec/iter\n",
      "Epoch: 466 | Batch: 004 / 011 | Total loss: 0.942 | Reg loss: 0.025 | Tree loss: 0.942 | Accuracy: 0.685500 | 1.44 sec/iter\n",
      "Epoch: 466 | Batch: 005 / 011 | Total loss: 0.956 | Reg loss: 0.025 | Tree loss: 0.956 | Accuracy: 0.667500 | 1.44 sec/iter\n",
      "Epoch: 466 | Batch: 006 / 011 | Total loss: 0.942 | Reg loss: 0.025 | Tree loss: 0.942 | Accuracy: 0.671000 | 1.44 sec/iter\n",
      "Epoch: 466 | Batch: 007 / 011 | Total loss: 0.908 | Reg loss: 0.025 | Tree loss: 0.908 | Accuracy: 0.688000 | 1.44 sec/iter\n",
      "Epoch: 466 | Batch: 008 / 011 | Total loss: 0.919 | Reg loss: 0.025 | Tree loss: 0.919 | Accuracy: 0.676500 | 1.44 sec/iter\n",
      "Epoch: 466 | Batch: 009 / 011 | Total loss: 0.906 | Reg loss: 0.025 | Tree loss: 0.906 | Accuracy: 0.662500 | 1.439 sec/iter\n",
      "Epoch: 466 | Batch: 010 / 011 | Total loss: 0.893 | Reg loss: 0.025 | Tree loss: 0.893 | Accuracy: 0.658703 | 1.439 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 467 | Batch: 000 / 011 | Total loss: 1.018 | Reg loss: 0.025 | Tree loss: 1.018 | Accuracy: 0.665000 | 1.44 sec/iter\n",
      "Epoch: 467 | Batch: 001 / 011 | Total loss: 0.985 | Reg loss: 0.025 | Tree loss: 0.985 | Accuracy: 0.668000 | 1.44 sec/iter\n",
      "Epoch: 467 | Batch: 002 / 011 | Total loss: 0.980 | Reg loss: 0.025 | Tree loss: 0.980 | Accuracy: 0.676000 | 1.44 sec/iter\n",
      "Epoch: 467 | Batch: 003 / 011 | Total loss: 0.951 | Reg loss: 0.025 | Tree loss: 0.951 | Accuracy: 0.683000 | 1.44 sec/iter\n",
      "Epoch: 467 | Batch: 004 / 011 | Total loss: 0.949 | Reg loss: 0.025 | Tree loss: 0.949 | Accuracy: 0.697000 | 1.44 sec/iter\n",
      "Epoch: 467 | Batch: 005 / 011 | Total loss: 0.926 | Reg loss: 0.025 | Tree loss: 0.926 | Accuracy: 0.682500 | 1.44 sec/iter\n",
      "Epoch: 467 | Batch: 006 / 011 | Total loss: 0.959 | Reg loss: 0.025 | Tree loss: 0.959 | Accuracy: 0.664000 | 1.44 sec/iter\n",
      "Epoch: 467 | Batch: 007 / 011 | Total loss: 0.909 | Reg loss: 0.025 | Tree loss: 0.909 | Accuracy: 0.673500 | 1.44 sec/iter\n",
      "Epoch: 467 | Batch: 008 / 011 | Total loss: 0.893 | Reg loss: 0.025 | Tree loss: 0.893 | Accuracy: 0.686500 | 1.44 sec/iter\n",
      "Epoch: 467 | Batch: 009 / 011 | Total loss: 0.896 | Reg loss: 0.025 | Tree loss: 0.896 | Accuracy: 0.675000 | 1.44 sec/iter\n",
      "Epoch: 467 | Batch: 010 / 011 | Total loss: 0.910 | Reg loss: 0.025 | Tree loss: 0.910 | Accuracy: 0.716724 | 1.44 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 468 | Batch: 000 / 011 | Total loss: 1.009 | Reg loss: 0.025 | Tree loss: 1.009 | Accuracy: 0.660500 | 1.441 sec/iter\n",
      "Epoch: 468 | Batch: 001 / 011 | Total loss: 1.008 | Reg loss: 0.025 | Tree loss: 1.008 | Accuracy: 0.653000 | 1.44 sec/iter\n",
      "Epoch: 468 | Batch: 002 / 011 | Total loss: 0.970 | Reg loss: 0.025 | Tree loss: 0.970 | Accuracy: 0.696000 | 1.44 sec/iter\n",
      "Epoch: 468 | Batch: 003 / 011 | Total loss: 0.948 | Reg loss: 0.025 | Tree loss: 0.948 | Accuracy: 0.698000 | 1.44 sec/iter\n",
      "Epoch: 468 | Batch: 004 / 011 | Total loss: 0.948 | Reg loss: 0.025 | Tree loss: 0.948 | Accuracy: 0.672500 | 1.44 sec/iter\n",
      "Epoch: 468 | Batch: 005 / 011 | Total loss: 0.925 | Reg loss: 0.025 | Tree loss: 0.925 | Accuracy: 0.684500 | 1.44 sec/iter\n",
      "Epoch: 468 | Batch: 006 / 011 | Total loss: 0.928 | Reg loss: 0.025 | Tree loss: 0.928 | Accuracy: 0.671000 | 1.44 sec/iter\n",
      "Epoch: 468 | Batch: 007 / 011 | Total loss: 0.902 | Reg loss: 0.025 | Tree loss: 0.902 | Accuracy: 0.684000 | 1.44 sec/iter\n",
      "Epoch: 468 | Batch: 008 / 011 | Total loss: 0.910 | Reg loss: 0.025 | Tree loss: 0.910 | Accuracy: 0.679500 | 1.44 sec/iter\n",
      "Epoch: 468 | Batch: 009 / 011 | Total loss: 0.912 | Reg loss: 0.025 | Tree loss: 0.912 | Accuracy: 0.659500 | 1.44 sec/iter\n",
      "Epoch: 468 | Batch: 010 / 011 | Total loss: 0.908 | Reg loss: 0.025 | Tree loss: 0.908 | Accuracy: 0.641638 | 1.44 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 8: 0.9723756906077345\n",
      "Epoch: 469 | Batch: 000 / 011 | Total loss: 1.015 | Reg loss: 0.025 | Tree loss: 1.015 | Accuracy: 0.665500 | 1.441 sec/iter\n",
      "Epoch: 469 | Batch: 001 / 011 | Total loss: 0.995 | Reg loss: 0.025 | Tree loss: 0.995 | Accuracy: 0.654500 | 1.441 sec/iter\n",
      "Epoch: 469 | Batch: 002 / 011 | Total loss: 0.953 | Reg loss: 0.025 | Tree loss: 0.953 | Accuracy: 0.683000 | 1.441 sec/iter\n",
      "Epoch: 469 | Batch: 003 / 011 | Total loss: 0.957 | Reg loss: 0.025 | Tree loss: 0.957 | Accuracy: 0.699500 | 1.441 sec/iter\n",
      "Epoch: 469 | Batch: 004 / 011 | Total loss: 0.943 | Reg loss: 0.025 | Tree loss: 0.943 | Accuracy: 0.681500 | 1.44 sec/iter\n",
      "Epoch: 469 | Batch: 005 / 011 | Total loss: 0.929 | Reg loss: 0.025 | Tree loss: 0.929 | Accuracy: 0.689500 | 1.44 sec/iter\n",
      "Epoch: 469 | Batch: 006 / 011 | Total loss: 0.932 | Reg loss: 0.025 | Tree loss: 0.932 | Accuracy: 0.672500 | 1.44 sec/iter\n",
      "Epoch: 469 | Batch: 007 / 011 | Total loss: 0.935 | Reg loss: 0.025 | Tree loss: 0.935 | Accuracy: 0.665000 | 1.44 sec/iter\n",
      "Epoch: 469 | Batch: 008 / 011 | Total loss: 0.885 | Reg loss: 0.025 | Tree loss: 0.885 | Accuracy: 0.676500 | 1.44 sec/iter\n",
      "Epoch: 469 | Batch: 009 / 011 | Total loss: 0.912 | Reg loss: 0.025 | Tree loss: 0.912 | Accuracy: 0.674500 | 1.44 sec/iter\n",
      "Epoch: 469 | Batch: 010 / 011 | Total loss: 0.962 | Reg loss: 0.025 | Tree loss: 0.962 | Accuracy: 0.627986 | 1.44 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 470 | Batch: 000 / 011 | Total loss: 1.013 | Reg loss: 0.025 | Tree loss: 1.013 | Accuracy: 0.659000 | 1.44 sec/iter\n",
      "Epoch: 470 | Batch: 001 / 011 | Total loss: 0.999 | Reg loss: 0.025 | Tree loss: 0.999 | Accuracy: 0.658500 | 1.44 sec/iter\n",
      "Epoch: 470 | Batch: 002 / 011 | Total loss: 0.956 | Reg loss: 0.025 | Tree loss: 0.956 | Accuracy: 0.693000 | 1.44 sec/iter\n",
      "Epoch: 470 | Batch: 003 / 011 | Total loss: 0.967 | Reg loss: 0.025 | Tree loss: 0.967 | Accuracy: 0.681500 | 1.44 sec/iter\n",
      "Epoch: 470 | Batch: 004 / 011 | Total loss: 0.947 | Reg loss: 0.025 | Tree loss: 0.947 | Accuracy: 0.689000 | 1.44 sec/iter\n",
      "Epoch: 470 | Batch: 005 / 011 | Total loss: 0.921 | Reg loss: 0.025 | Tree loss: 0.921 | Accuracy: 0.690500 | 1.44 sec/iter\n",
      "Epoch: 470 | Batch: 006 / 011 | Total loss: 0.914 | Reg loss: 0.025 | Tree loss: 0.914 | Accuracy: 0.681000 | 1.44 sec/iter\n",
      "Epoch: 470 | Batch: 007 / 011 | Total loss: 0.927 | Reg loss: 0.025 | Tree loss: 0.927 | Accuracy: 0.673000 | 1.44 sec/iter\n",
      "Epoch: 470 | Batch: 008 / 011 | Total loss: 0.927 | Reg loss: 0.025 | Tree loss: 0.927 | Accuracy: 0.665500 | 1.44 sec/iter\n",
      "Epoch: 470 | Batch: 009 / 011 | Total loss: 0.915 | Reg loss: 0.025 | Tree loss: 0.915 | Accuracy: 0.666000 | 1.44 sec/iter\n",
      "Epoch: 470 | Batch: 010 / 011 | Total loss: 0.898 | Reg loss: 0.025 | Tree loss: 0.898 | Accuracy: 0.655290 | 1.44 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 471 | Batch: 000 / 011 | Total loss: 1.003 | Reg loss: 0.025 | Tree loss: 1.003 | Accuracy: 0.664000 | 1.44 sec/iter\n",
      "Epoch: 471 | Batch: 001 / 011 | Total loss: 0.984 | Reg loss: 0.025 | Tree loss: 0.984 | Accuracy: 0.670500 | 1.44 sec/iter\n",
      "Epoch: 471 | Batch: 002 / 011 | Total loss: 0.985 | Reg loss: 0.025 | Tree loss: 0.985 | Accuracy: 0.667000 | 1.44 sec/iter\n",
      "Epoch: 471 | Batch: 003 / 011 | Total loss: 0.971 | Reg loss: 0.025 | Tree loss: 0.971 | Accuracy: 0.669000 | 1.44 sec/iter\n",
      "Epoch: 471 | Batch: 004 / 011 | Total loss: 0.946 | Reg loss: 0.025 | Tree loss: 0.946 | Accuracy: 0.695500 | 1.44 sec/iter\n",
      "Epoch: 471 | Batch: 005 / 011 | Total loss: 0.936 | Reg loss: 0.025 | Tree loss: 0.936 | Accuracy: 0.702000 | 1.44 sec/iter\n",
      "Epoch: 471 | Batch: 006 / 011 | Total loss: 0.936 | Reg loss: 0.025 | Tree loss: 0.936 | Accuracy: 0.675500 | 1.44 sec/iter\n",
      "Epoch: 471 | Batch: 007 / 011 | Total loss: 0.885 | Reg loss: 0.025 | Tree loss: 0.885 | Accuracy: 0.694000 | 1.44 sec/iter\n",
      "Epoch: 471 | Batch: 008 / 011 | Total loss: 0.916 | Reg loss: 0.025 | Tree loss: 0.916 | Accuracy: 0.669000 | 1.44 sec/iter\n",
      "Epoch: 471 | Batch: 009 / 011 | Total loss: 0.920 | Reg loss: 0.025 | Tree loss: 0.920 | Accuracy: 0.673000 | 1.44 sec/iter\n",
      "Epoch: 471 | Batch: 010 / 011 | Total loss: 0.888 | Reg loss: 0.025 | Tree loss: 0.888 | Accuracy: 0.679181 | 1.44 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 472 | Batch: 000 / 011 | Total loss: 1.025 | Reg loss: 0.025 | Tree loss: 1.025 | Accuracy: 0.649500 | 1.441 sec/iter\n",
      "Epoch: 472 | Batch: 001 / 011 | Total loss: 0.978 | Reg loss: 0.025 | Tree loss: 0.978 | Accuracy: 0.673000 | 1.441 sec/iter\n",
      "Epoch: 472 | Batch: 002 / 011 | Total loss: 0.979 | Reg loss: 0.025 | Tree loss: 0.979 | Accuracy: 0.666500 | 1.44 sec/iter\n",
      "Epoch: 472 | Batch: 003 / 011 | Total loss: 0.946 | Reg loss: 0.025 | Tree loss: 0.946 | Accuracy: 0.701000 | 1.44 sec/iter\n",
      "Epoch: 472 | Batch: 004 / 011 | Total loss: 0.929 | Reg loss: 0.025 | Tree loss: 0.929 | Accuracy: 0.681500 | 1.44 sec/iter\n",
      "Epoch: 472 | Batch: 005 / 011 | Total loss: 0.929 | Reg loss: 0.025 | Tree loss: 0.929 | Accuracy: 0.674000 | 1.44 sec/iter\n",
      "Epoch: 472 | Batch: 006 / 011 | Total loss: 0.933 | Reg loss: 0.025 | Tree loss: 0.933 | Accuracy: 0.681500 | 1.44 sec/iter\n",
      "Epoch: 472 | Batch: 007 / 011 | Total loss: 0.928 | Reg loss: 0.025 | Tree loss: 0.928 | Accuracy: 0.670500 | 1.44 sec/iter\n",
      "Epoch: 472 | Batch: 008 / 011 | Total loss: 0.929 | Reg loss: 0.025 | Tree loss: 0.929 | Accuracy: 0.669000 | 1.44 sec/iter\n",
      "Epoch: 472 | Batch: 009 / 011 | Total loss: 0.903 | Reg loss: 0.025 | Tree loss: 0.903 | Accuracy: 0.686000 | 1.44 sec/iter\n",
      "Epoch: 472 | Batch: 010 / 011 | Total loss: 0.846 | Reg loss: 0.025 | Tree loss: 0.846 | Accuracy: 0.692833 | 1.44 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 473 | Batch: 000 / 011 | Total loss: 1.027 | Reg loss: 0.025 | Tree loss: 1.027 | Accuracy: 0.647500 | 1.441 sec/iter\n",
      "Epoch: 473 | Batch: 001 / 011 | Total loss: 1.000 | Reg loss: 0.025 | Tree loss: 1.000 | Accuracy: 0.662500 | 1.441 sec/iter\n",
      "Epoch: 473 | Batch: 002 / 011 | Total loss: 0.973 | Reg loss: 0.025 | Tree loss: 0.973 | Accuracy: 0.693000 | 1.441 sec/iter\n",
      "Epoch: 473 | Batch: 003 / 011 | Total loss: 0.962 | Reg loss: 0.025 | Tree loss: 0.962 | Accuracy: 0.678500 | 1.441 sec/iter\n",
      "Epoch: 473 | Batch: 004 / 011 | Total loss: 0.930 | Reg loss: 0.025 | Tree loss: 0.930 | Accuracy: 0.683500 | 1.441 sec/iter\n",
      "Epoch: 473 | Batch: 005 / 011 | Total loss: 0.943 | Reg loss: 0.025 | Tree loss: 0.943 | Accuracy: 0.669500 | 1.441 sec/iter\n",
      "Epoch: 473 | Batch: 006 / 011 | Total loss: 0.899 | Reg loss: 0.025 | Tree loss: 0.899 | Accuracy: 0.687500 | 1.44 sec/iter\n",
      "Epoch: 473 | Batch: 007 / 011 | Total loss: 0.936 | Reg loss: 0.025 | Tree loss: 0.936 | Accuracy: 0.662500 | 1.44 sec/iter\n",
      "Epoch: 473 | Batch: 008 / 011 | Total loss: 0.908 | Reg loss: 0.025 | Tree loss: 0.908 | Accuracy: 0.675000 | 1.44 sec/iter\n",
      "Epoch: 473 | Batch: 009 / 011 | Total loss: 0.893 | Reg loss: 0.025 | Tree loss: 0.893 | Accuracy: 0.677000 | 1.44 sec/iter\n",
      "Epoch: 473 | Batch: 010 / 011 | Total loss: 0.905 | Reg loss: 0.025 | Tree loss: 0.905 | Accuracy: 0.706485 | 1.44 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 474 | Batch: 000 / 011 | Total loss: 1.001 | Reg loss: 0.025 | Tree loss: 1.001 | Accuracy: 0.662000 | 1.441 sec/iter\n",
      "Epoch: 474 | Batch: 001 / 011 | Total loss: 0.999 | Reg loss: 0.025 | Tree loss: 0.999 | Accuracy: 0.680000 | 1.441 sec/iter\n",
      "Epoch: 474 | Batch: 002 / 011 | Total loss: 0.995 | Reg loss: 0.025 | Tree loss: 0.995 | Accuracy: 0.666500 | 1.441 sec/iter\n",
      "Epoch: 474 | Batch: 003 / 011 | Total loss: 0.960 | Reg loss: 0.025 | Tree loss: 0.960 | Accuracy: 0.673500 | 1.441 sec/iter\n",
      "Epoch: 474 | Batch: 004 / 011 | Total loss: 0.943 | Reg loss: 0.025 | Tree loss: 0.943 | Accuracy: 0.685500 | 1.441 sec/iter\n",
      "Epoch: 474 | Batch: 005 / 011 | Total loss: 0.927 | Reg loss: 0.025 | Tree loss: 0.927 | Accuracy: 0.690500 | 1.441 sec/iter\n",
      "Epoch: 474 | Batch: 006 / 011 | Total loss: 0.923 | Reg loss: 0.025 | Tree loss: 0.923 | Accuracy: 0.675000 | 1.441 sec/iter\n",
      "Epoch: 474 | Batch: 007 / 011 | Total loss: 0.920 | Reg loss: 0.025 | Tree loss: 0.920 | Accuracy: 0.685000 | 1.441 sec/iter\n",
      "Epoch: 474 | Batch: 008 / 011 | Total loss: 0.894 | Reg loss: 0.025 | Tree loss: 0.894 | Accuracy: 0.671000 | 1.441 sec/iter\n",
      "Epoch: 474 | Batch: 009 / 011 | Total loss: 0.899 | Reg loss: 0.025 | Tree loss: 0.899 | Accuracy: 0.668000 | 1.441 sec/iter\n",
      "Epoch: 474 | Batch: 010 / 011 | Total loss: 0.962 | Reg loss: 0.025 | Tree loss: 0.962 | Accuracy: 0.648464 | 1.44 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 475 | Batch: 000 / 011 | Total loss: 1.007 | Reg loss: 0.025 | Tree loss: 1.007 | Accuracy: 0.675500 | 1.441 sec/iter\n",
      "Epoch: 475 | Batch: 001 / 011 | Total loss: 0.998 | Reg loss: 0.025 | Tree loss: 0.998 | Accuracy: 0.652500 | 1.441 sec/iter\n",
      "Epoch: 475 | Batch: 002 / 011 | Total loss: 0.963 | Reg loss: 0.025 | Tree loss: 0.963 | Accuracy: 0.680500 | 1.441 sec/iter\n",
      "Epoch: 475 | Batch: 003 / 011 | Total loss: 0.960 | Reg loss: 0.025 | Tree loss: 0.960 | Accuracy: 0.683000 | 1.441 sec/iter\n",
      "Epoch: 475 | Batch: 004 / 011 | Total loss: 0.927 | Reg loss: 0.025 | Tree loss: 0.927 | Accuracy: 0.695000 | 1.441 sec/iter\n",
      "Epoch: 475 | Batch: 005 / 011 | Total loss: 0.944 | Reg loss: 0.025 | Tree loss: 0.944 | Accuracy: 0.679500 | 1.441 sec/iter\n",
      "Epoch: 475 | Batch: 006 / 011 | Total loss: 0.922 | Reg loss: 0.025 | Tree loss: 0.922 | Accuracy: 0.669500 | 1.441 sec/iter\n",
      "Epoch: 475 | Batch: 007 / 011 | Total loss: 0.939 | Reg loss: 0.025 | Tree loss: 0.939 | Accuracy: 0.657000 | 1.441 sec/iter\n",
      "Epoch: 475 | Batch: 008 / 011 | Total loss: 0.915 | Reg loss: 0.025 | Tree loss: 0.915 | Accuracy: 0.665500 | 1.441 sec/iter\n",
      "Epoch: 475 | Batch: 009 / 011 | Total loss: 0.917 | Reg loss: 0.025 | Tree loss: 0.917 | Accuracy: 0.676000 | 1.441 sec/iter\n",
      "Epoch: 475 | Batch: 010 / 011 | Total loss: 0.849 | Reg loss: 0.025 | Tree loss: 0.849 | Accuracy: 0.706485 | 1.44 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 476 | Batch: 000 / 011 | Total loss: 1.033 | Reg loss: 0.025 | Tree loss: 1.033 | Accuracy: 0.649000 | 1.441 sec/iter\n",
      "Epoch: 476 | Batch: 001 / 011 | Total loss: 0.968 | Reg loss: 0.025 | Tree loss: 0.968 | Accuracy: 0.692500 | 1.441 sec/iter\n",
      "Epoch: 476 | Batch: 002 / 011 | Total loss: 0.955 | Reg loss: 0.025 | Tree loss: 0.955 | Accuracy: 0.684500 | 1.441 sec/iter\n",
      "Epoch: 476 | Batch: 003 / 011 | Total loss: 0.945 | Reg loss: 0.025 | Tree loss: 0.945 | Accuracy: 0.682000 | 1.44 sec/iter\n",
      "Epoch: 476 | Batch: 004 / 011 | Total loss: 0.927 | Reg loss: 0.025 | Tree loss: 0.927 | Accuracy: 0.700000 | 1.44 sec/iter\n",
      "Epoch: 476 | Batch: 005 / 011 | Total loss: 0.929 | Reg loss: 0.025 | Tree loss: 0.929 | Accuracy: 0.689500 | 1.44 sec/iter\n",
      "Epoch: 476 | Batch: 006 / 011 | Total loss: 0.933 | Reg loss: 0.025 | Tree loss: 0.933 | Accuracy: 0.672500 | 1.44 sec/iter\n",
      "Epoch: 476 | Batch: 007 / 011 | Total loss: 0.916 | Reg loss: 0.025 | Tree loss: 0.916 | Accuracy: 0.673500 | 1.44 sec/iter\n",
      "Epoch: 476 | Batch: 008 / 011 | Total loss: 0.948 | Reg loss: 0.025 | Tree loss: 0.948 | Accuracy: 0.665000 | 1.44 sec/iter\n",
      "Epoch: 476 | Batch: 009 / 011 | Total loss: 0.914 | Reg loss: 0.025 | Tree loss: 0.914 | Accuracy: 0.668000 | 1.44 sec/iter\n",
      "Epoch: 476 | Batch: 010 / 011 | Total loss: 0.938 | Reg loss: 0.025 | Tree loss: 0.938 | Accuracy: 0.658703 | 1.44 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 477 | Batch: 000 / 011 | Total loss: 0.989 | Reg loss: 0.025 | Tree loss: 0.989 | Accuracy: 0.670000 | 1.441 sec/iter\n",
      "Epoch: 477 | Batch: 001 / 011 | Total loss: 1.006 | Reg loss: 0.025 | Tree loss: 1.006 | Accuracy: 0.666000 | 1.441 sec/iter\n",
      "Epoch: 477 | Batch: 002 / 011 | Total loss: 0.965 | Reg loss: 0.025 | Tree loss: 0.965 | Accuracy: 0.677500 | 1.441 sec/iter\n",
      "Epoch: 477 | Batch: 003 / 011 | Total loss: 0.929 | Reg loss: 0.025 | Tree loss: 0.929 | Accuracy: 0.695500 | 1.441 sec/iter\n",
      "Epoch: 477 | Batch: 004 / 011 | Total loss: 0.935 | Reg loss: 0.025 | Tree loss: 0.935 | Accuracy: 0.691000 | 1.441 sec/iter\n",
      "Epoch: 477 | Batch: 005 / 011 | Total loss: 0.933 | Reg loss: 0.025 | Tree loss: 0.933 | Accuracy: 0.681500 | 1.441 sec/iter\n",
      "Epoch: 477 | Batch: 006 / 011 | Total loss: 0.932 | Reg loss: 0.025 | Tree loss: 0.932 | Accuracy: 0.689000 | 1.441 sec/iter\n",
      "Epoch: 477 | Batch: 007 / 011 | Total loss: 0.928 | Reg loss: 0.025 | Tree loss: 0.928 | Accuracy: 0.673500 | 1.441 sec/iter\n",
      "Epoch: 477 | Batch: 008 / 011 | Total loss: 0.929 | Reg loss: 0.025 | Tree loss: 0.929 | Accuracy: 0.669500 | 1.441 sec/iter\n",
      "Epoch: 477 | Batch: 009 / 011 | Total loss: 0.917 | Reg loss: 0.025 | Tree loss: 0.917 | Accuracy: 0.672000 | 1.44 sec/iter\n",
      "Epoch: 477 | Batch: 010 / 011 | Total loss: 0.949 | Reg loss: 0.025 | Tree loss: 0.949 | Accuracy: 0.621160 | 1.44 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 478 | Batch: 000 / 011 | Total loss: 1.013 | Reg loss: 0.025 | Tree loss: 1.013 | Accuracy: 0.657000 | 1.441 sec/iter\n",
      "Epoch: 478 | Batch: 001 / 011 | Total loss: 0.984 | Reg loss: 0.025 | Tree loss: 0.984 | Accuracy: 0.668000 | 1.441 sec/iter\n",
      "Epoch: 478 | Batch: 002 / 011 | Total loss: 0.966 | Reg loss: 0.025 | Tree loss: 0.966 | Accuracy: 0.699000 | 1.441 sec/iter\n",
      "Epoch: 478 | Batch: 003 / 011 | Total loss: 0.959 | Reg loss: 0.025 | Tree loss: 0.959 | Accuracy: 0.690500 | 1.441 sec/iter\n",
      "Epoch: 478 | Batch: 004 / 011 | Total loss: 0.935 | Reg loss: 0.025 | Tree loss: 0.935 | Accuracy: 0.690000 | 1.441 sec/iter\n",
      "Epoch: 478 | Batch: 005 / 011 | Total loss: 0.928 | Reg loss: 0.025 | Tree loss: 0.928 | Accuracy: 0.710000 | 1.441 sec/iter\n",
      "Epoch: 478 | Batch: 006 / 011 | Total loss: 0.916 | Reg loss: 0.025 | Tree loss: 0.916 | Accuracy: 0.676000 | 1.441 sec/iter\n",
      "Epoch: 478 | Batch: 007 / 011 | Total loss: 0.924 | Reg loss: 0.025 | Tree loss: 0.924 | Accuracy: 0.671000 | 1.441 sec/iter\n",
      "Epoch: 478 | Batch: 008 / 011 | Total loss: 0.912 | Reg loss: 0.025 | Tree loss: 0.912 | Accuracy: 0.666000 | 1.441 sec/iter\n",
      "Epoch: 478 | Batch: 009 / 011 | Total loss: 0.921 | Reg loss: 0.025 | Tree loss: 0.921 | Accuracy: 0.670500 | 1.441 sec/iter\n",
      "Epoch: 478 | Batch: 010 / 011 | Total loss: 0.887 | Reg loss: 0.025 | Tree loss: 0.887 | Accuracy: 0.658703 | 1.441 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 479 | Batch: 000 / 011 | Total loss: 1.011 | Reg loss: 0.025 | Tree loss: 1.011 | Accuracy: 0.666500 | 1.442 sec/iter\n",
      "Epoch: 479 | Batch: 001 / 011 | Total loss: 0.989 | Reg loss: 0.025 | Tree loss: 0.989 | Accuracy: 0.676000 | 1.442 sec/iter\n",
      "Epoch: 479 | Batch: 002 / 011 | Total loss: 0.962 | Reg loss: 0.025 | Tree loss: 0.962 | Accuracy: 0.694500 | 1.441 sec/iter\n",
      "Epoch: 479 | Batch: 003 / 011 | Total loss: 0.941 | Reg loss: 0.025 | Tree loss: 0.941 | Accuracy: 0.691500 | 1.441 sec/iter\n",
      "Epoch: 479 | Batch: 004 / 011 | Total loss: 0.929 | Reg loss: 0.025 | Tree loss: 0.929 | Accuracy: 0.671000 | 1.441 sec/iter\n",
      "Epoch: 479 | Batch: 005 / 011 | Total loss: 0.955 | Reg loss: 0.025 | Tree loss: 0.955 | Accuracy: 0.658500 | 1.441 sec/iter\n",
      "Epoch: 479 | Batch: 006 / 011 | Total loss: 0.939 | Reg loss: 0.025 | Tree loss: 0.939 | Accuracy: 0.672500 | 1.441 sec/iter\n",
      "Epoch: 479 | Batch: 007 / 011 | Total loss: 0.921 | Reg loss: 0.025 | Tree loss: 0.921 | Accuracy: 0.676000 | 1.441 sec/iter\n",
      "Epoch: 479 | Batch: 008 / 011 | Total loss: 0.913 | Reg loss: 0.025 | Tree loss: 0.913 | Accuracy: 0.686500 | 1.441 sec/iter\n",
      "Epoch: 479 | Batch: 009 / 011 | Total loss: 0.918 | Reg loss: 0.025 | Tree loss: 0.918 | Accuracy: 0.670000 | 1.441 sec/iter\n",
      "Epoch: 479 | Batch: 010 / 011 | Total loss: 0.874 | Reg loss: 0.025 | Tree loss: 0.874 | Accuracy: 0.686007 | 1.441 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 480 | Batch: 000 / 011 | Total loss: 1.002 | Reg loss: 0.025 | Tree loss: 1.002 | Accuracy: 0.682000 | 1.442 sec/iter\n",
      "Epoch: 480 | Batch: 001 / 011 | Total loss: 1.003 | Reg loss: 0.025 | Tree loss: 1.003 | Accuracy: 0.661500 | 1.442 sec/iter\n",
      "Epoch: 480 | Batch: 002 / 011 | Total loss: 0.964 | Reg loss: 0.025 | Tree loss: 0.964 | Accuracy: 0.678000 | 1.442 sec/iter\n",
      "Epoch: 480 | Batch: 003 / 011 | Total loss: 0.946 | Reg loss: 0.025 | Tree loss: 0.946 | Accuracy: 0.672500 | 1.442 sec/iter\n",
      "Epoch: 480 | Batch: 004 / 011 | Total loss: 0.942 | Reg loss: 0.025 | Tree loss: 0.942 | Accuracy: 0.682000 | 1.442 sec/iter\n",
      "Epoch: 480 | Batch: 005 / 011 | Total loss: 0.938 | Reg loss: 0.025 | Tree loss: 0.938 | Accuracy: 0.684000 | 1.441 sec/iter\n",
      "Epoch: 480 | Batch: 006 / 011 | Total loss: 0.923 | Reg loss: 0.025 | Tree loss: 0.923 | Accuracy: 0.687000 | 1.441 sec/iter\n",
      "Epoch: 480 | Batch: 007 / 011 | Total loss: 0.916 | Reg loss: 0.025 | Tree loss: 0.916 | Accuracy: 0.667000 | 1.441 sec/iter\n",
      "Epoch: 480 | Batch: 008 / 011 | Total loss: 0.908 | Reg loss: 0.025 | Tree loss: 0.908 | Accuracy: 0.677000 | 1.441 sec/iter\n",
      "Epoch: 480 | Batch: 009 / 011 | Total loss: 0.934 | Reg loss: 0.025 | Tree loss: 0.934 | Accuracy: 0.659000 | 1.441 sec/iter\n",
      "Epoch: 480 | Batch: 010 / 011 | Total loss: 0.955 | Reg loss: 0.025 | Tree loss: 0.955 | Accuracy: 0.648464 | 1.441 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 481 | Batch: 000 / 011 | Total loss: 0.986 | Reg loss: 0.025 | Tree loss: 0.986 | Accuracy: 0.676500 | 1.442 sec/iter\n",
      "Epoch: 481 | Batch: 001 / 011 | Total loss: 0.987 | Reg loss: 0.025 | Tree loss: 0.987 | Accuracy: 0.679500 | 1.442 sec/iter\n",
      "Epoch: 481 | Batch: 002 / 011 | Total loss: 0.987 | Reg loss: 0.025 | Tree loss: 0.987 | Accuracy: 0.665000 | 1.442 sec/iter\n",
      "Epoch: 481 | Batch: 003 / 011 | Total loss: 0.963 | Reg loss: 0.025 | Tree loss: 0.963 | Accuracy: 0.681500 | 1.442 sec/iter\n",
      "Epoch: 481 | Batch: 004 / 011 | Total loss: 0.912 | Reg loss: 0.025 | Tree loss: 0.912 | Accuracy: 0.700500 | 1.442 sec/iter\n",
      "Epoch: 481 | Batch: 005 / 011 | Total loss: 0.935 | Reg loss: 0.025 | Tree loss: 0.935 | Accuracy: 0.665000 | 1.442 sec/iter\n",
      "Epoch: 481 | Batch: 006 / 011 | Total loss: 0.945 | Reg loss: 0.025 | Tree loss: 0.945 | Accuracy: 0.659500 | 1.442 sec/iter\n",
      "Epoch: 481 | Batch: 007 / 011 | Total loss: 0.924 | Reg loss: 0.025 | Tree loss: 0.924 | Accuracy: 0.666500 | 1.441 sec/iter\n",
      "Epoch: 481 | Batch: 008 / 011 | Total loss: 0.922 | Reg loss: 0.025 | Tree loss: 0.922 | Accuracy: 0.662000 | 1.441 sec/iter\n",
      "Epoch: 481 | Batch: 009 / 011 | Total loss: 0.915 | Reg loss: 0.025 | Tree loss: 0.915 | Accuracy: 0.676000 | 1.441 sec/iter\n",
      "Epoch: 481 | Batch: 010 / 011 | Total loss: 0.875 | Reg loss: 0.025 | Tree loss: 0.875 | Accuracy: 0.675768 | 1.441 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 482 | Batch: 000 / 011 | Total loss: 0.992 | Reg loss: 0.025 | Tree loss: 0.992 | Accuracy: 0.678000 | 1.441 sec/iter\n",
      "Epoch: 482 | Batch: 001 / 011 | Total loss: 0.986 | Reg loss: 0.025 | Tree loss: 0.986 | Accuracy: 0.683000 | 1.441 sec/iter\n",
      "Epoch: 482 | Batch: 002 / 011 | Total loss: 0.971 | Reg loss: 0.025 | Tree loss: 0.971 | Accuracy: 0.685500 | 1.441 sec/iter\n",
      "Epoch: 482 | Batch: 003 / 011 | Total loss: 0.950 | Reg loss: 0.025 | Tree loss: 0.950 | Accuracy: 0.687000 | 1.441 sec/iter\n",
      "Epoch: 482 | Batch: 004 / 011 | Total loss: 0.930 | Reg loss: 0.025 | Tree loss: 0.930 | Accuracy: 0.696500 | 1.441 sec/iter\n",
      "Epoch: 482 | Batch: 005 / 011 | Total loss: 0.922 | Reg loss: 0.025 | Tree loss: 0.922 | Accuracy: 0.687500 | 1.441 sec/iter\n",
      "Epoch: 482 | Batch: 006 / 011 | Total loss: 0.913 | Reg loss: 0.025 | Tree loss: 0.913 | Accuracy: 0.682500 | 1.441 sec/iter\n",
      "Epoch: 482 | Batch: 007 / 011 | Total loss: 0.953 | Reg loss: 0.025 | Tree loss: 0.953 | Accuracy: 0.640000 | 1.441 sec/iter\n",
      "Epoch: 482 | Batch: 008 / 011 | Total loss: 0.910 | Reg loss: 0.025 | Tree loss: 0.910 | Accuracy: 0.683500 | 1.441 sec/iter\n",
      "Epoch: 482 | Batch: 009 / 011 | Total loss: 0.940 | Reg loss: 0.025 | Tree loss: 0.940 | Accuracy: 0.655000 | 1.441 sec/iter\n",
      "Epoch: 482 | Batch: 010 / 011 | Total loss: 0.928 | Reg loss: 0.025 | Tree loss: 0.928 | Accuracy: 0.675768 | 1.441 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 483 | Batch: 000 / 011 | Total loss: 1.026 | Reg loss: 0.025 | Tree loss: 1.026 | Accuracy: 0.669500 | 1.442 sec/iter\n",
      "Epoch: 483 | Batch: 001 / 011 | Total loss: 0.981 | Reg loss: 0.025 | Tree loss: 0.981 | Accuracy: 0.674000 | 1.442 sec/iter\n",
      "Epoch: 483 | Batch: 002 / 011 | Total loss: 0.936 | Reg loss: 0.025 | Tree loss: 0.936 | Accuracy: 0.694500 | 1.442 sec/iter\n",
      "Epoch: 483 | Batch: 003 / 011 | Total loss: 0.961 | Reg loss: 0.025 | Tree loss: 0.961 | Accuracy: 0.676000 | 1.442 sec/iter\n",
      "Epoch: 483 | Batch: 004 / 011 | Total loss: 0.928 | Reg loss: 0.025 | Tree loss: 0.928 | Accuracy: 0.683000 | 1.441 sec/iter\n",
      "Epoch: 483 | Batch: 005 / 011 | Total loss: 0.947 | Reg loss: 0.025 | Tree loss: 0.947 | Accuracy: 0.665500 | 1.441 sec/iter\n",
      "Epoch: 483 | Batch: 006 / 011 | Total loss: 0.910 | Reg loss: 0.025 | Tree loss: 0.910 | Accuracy: 0.685000 | 1.441 sec/iter\n",
      "Epoch: 483 | Batch: 007 / 011 | Total loss: 0.948 | Reg loss: 0.025 | Tree loss: 0.948 | Accuracy: 0.647500 | 1.441 sec/iter\n",
      "Epoch: 483 | Batch: 008 / 011 | Total loss: 0.921 | Reg loss: 0.025 | Tree loss: 0.921 | Accuracy: 0.664500 | 1.441 sec/iter\n",
      "Epoch: 483 | Batch: 009 / 011 | Total loss: 0.927 | Reg loss: 0.025 | Tree loss: 0.927 | Accuracy: 0.654500 | 1.441 sec/iter\n",
      "Epoch: 483 | Batch: 010 / 011 | Total loss: 0.959 | Reg loss: 0.025 | Tree loss: 0.959 | Accuracy: 0.655290 | 1.441 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 8: 0.9723756906077345\n",
      "Epoch: 484 | Batch: 000 / 011 | Total loss: 1.023 | Reg loss: 0.025 | Tree loss: 1.023 | Accuracy: 0.659000 | 1.442 sec/iter\n",
      "Epoch: 484 | Batch: 001 / 011 | Total loss: 0.972 | Reg loss: 0.025 | Tree loss: 0.972 | Accuracy: 0.677500 | 1.442 sec/iter\n",
      "Epoch: 484 | Batch: 002 / 011 | Total loss: 0.982 | Reg loss: 0.025 | Tree loss: 0.982 | Accuracy: 0.687000 | 1.442 sec/iter\n",
      "Epoch: 484 | Batch: 003 / 011 | Total loss: 0.948 | Reg loss: 0.025 | Tree loss: 0.948 | Accuracy: 0.695500 | 1.442 sec/iter\n",
      "Epoch: 484 | Batch: 004 / 011 | Total loss: 0.939 | Reg loss: 0.025 | Tree loss: 0.939 | Accuracy: 0.690000 | 1.442 sec/iter\n",
      "Epoch: 484 | Batch: 005 / 011 | Total loss: 0.939 | Reg loss: 0.025 | Tree loss: 0.939 | Accuracy: 0.683500 | 1.442 sec/iter\n",
      "Epoch: 484 | Batch: 006 / 011 | Total loss: 0.912 | Reg loss: 0.025 | Tree loss: 0.912 | Accuracy: 0.686000 | 1.442 sec/iter\n",
      "Epoch: 484 | Batch: 007 / 011 | Total loss: 0.936 | Reg loss: 0.025 | Tree loss: 0.936 | Accuracy: 0.669500 | 1.441 sec/iter\n",
      "Epoch: 484 | Batch: 008 / 011 | Total loss: 0.916 | Reg loss: 0.025 | Tree loss: 0.916 | Accuracy: 0.671000 | 1.441 sec/iter\n",
      "Epoch: 484 | Batch: 009 / 011 | Total loss: 0.897 | Reg loss: 0.025 | Tree loss: 0.897 | Accuracy: 0.678000 | 1.441 sec/iter\n",
      "Epoch: 484 | Batch: 010 / 011 | Total loss: 0.920 | Reg loss: 0.025 | Tree loss: 0.920 | Accuracy: 0.658703 | 1.441 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 485 | Batch: 000 / 011 | Total loss: 1.009 | Reg loss: 0.025 | Tree loss: 1.009 | Accuracy: 0.663500 | 1.442 sec/iter\n",
      "Epoch: 485 | Batch: 001 / 011 | Total loss: 0.990 | Reg loss: 0.025 | Tree loss: 0.990 | Accuracy: 0.671000 | 1.442 sec/iter\n",
      "Epoch: 485 | Batch: 002 / 011 | Total loss: 0.952 | Reg loss: 0.025 | Tree loss: 0.952 | Accuracy: 0.684500 | 1.442 sec/iter\n",
      "Epoch: 485 | Batch: 003 / 011 | Total loss: 0.931 | Reg loss: 0.025 | Tree loss: 0.931 | Accuracy: 0.698500 | 1.442 sec/iter\n",
      "Epoch: 485 | Batch: 004 / 011 | Total loss: 0.940 | Reg loss: 0.025 | Tree loss: 0.940 | Accuracy: 0.681500 | 1.442 sec/iter\n",
      "Epoch: 485 | Batch: 005 / 011 | Total loss: 0.921 | Reg loss: 0.025 | Tree loss: 0.921 | Accuracy: 0.689500 | 1.442 sec/iter\n",
      "Epoch: 485 | Batch: 006 / 011 | Total loss: 0.938 | Reg loss: 0.025 | Tree loss: 0.938 | Accuracy: 0.662000 | 1.442 sec/iter\n",
      "Epoch: 485 | Batch: 007 / 011 | Total loss: 0.941 | Reg loss: 0.025 | Tree loss: 0.941 | Accuracy: 0.660500 | 1.442 sec/iter\n",
      "Epoch: 485 | Batch: 008 / 011 | Total loss: 0.930 | Reg loss: 0.025 | Tree loss: 0.930 | Accuracy: 0.671500 | 1.442 sec/iter\n",
      "Epoch: 485 | Batch: 009 / 011 | Total loss: 0.898 | Reg loss: 0.025 | Tree loss: 0.898 | Accuracy: 0.691500 | 1.442 sec/iter\n",
      "Epoch: 485 | Batch: 010 / 011 | Total loss: 1.004 | Reg loss: 0.025 | Tree loss: 1.004 | Accuracy: 0.638225 | 1.442 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 486 | Batch: 000 / 011 | Total loss: 0.987 | Reg loss: 0.025 | Tree loss: 0.987 | Accuracy: 0.691500 | 1.442 sec/iter\n",
      "Epoch: 486 | Batch: 001 / 011 | Total loss: 0.991 | Reg loss: 0.025 | Tree loss: 0.991 | Accuracy: 0.674500 | 1.442 sec/iter\n",
      "Epoch: 486 | Batch: 002 / 011 | Total loss: 0.977 | Reg loss: 0.025 | Tree loss: 0.977 | Accuracy: 0.691000 | 1.442 sec/iter\n",
      "Epoch: 486 | Batch: 003 / 011 | Total loss: 0.965 | Reg loss: 0.025 | Tree loss: 0.965 | Accuracy: 0.668500 | 1.442 sec/iter\n",
      "Epoch: 486 | Batch: 004 / 011 | Total loss: 0.919 | Reg loss: 0.025 | Tree loss: 0.919 | Accuracy: 0.694000 | 1.442 sec/iter\n",
      "Epoch: 486 | Batch: 005 / 011 | Total loss: 0.929 | Reg loss: 0.025 | Tree loss: 0.929 | Accuracy: 0.682000 | 1.442 sec/iter\n",
      "Epoch: 486 | Batch: 006 / 011 | Total loss: 0.924 | Reg loss: 0.025 | Tree loss: 0.924 | Accuracy: 0.691500 | 1.442 sec/iter\n",
      "Epoch: 486 | Batch: 007 / 011 | Total loss: 0.938 | Reg loss: 0.025 | Tree loss: 0.938 | Accuracy: 0.665500 | 1.442 sec/iter\n",
      "Epoch: 486 | Batch: 008 / 011 | Total loss: 0.951 | Reg loss: 0.025 | Tree loss: 0.951 | Accuracy: 0.645500 | 1.442 sec/iter\n",
      "Epoch: 486 | Batch: 009 / 011 | Total loss: 0.925 | Reg loss: 0.025 | Tree loss: 0.925 | Accuracy: 0.670000 | 1.442 sec/iter\n",
      "Epoch: 486 | Batch: 010 / 011 | Total loss: 0.920 | Reg loss: 0.025 | Tree loss: 0.920 | Accuracy: 0.651877 | 1.442 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 487 | Batch: 000 / 011 | Total loss: 0.993 | Reg loss: 0.025 | Tree loss: 0.993 | Accuracy: 0.676500 | 1.443 sec/iter\n",
      "Epoch: 487 | Batch: 001 / 011 | Total loss: 0.992 | Reg loss: 0.025 | Tree loss: 0.992 | Accuracy: 0.664500 | 1.443 sec/iter\n",
      "Epoch: 487 | Batch: 002 / 011 | Total loss: 0.985 | Reg loss: 0.025 | Tree loss: 0.985 | Accuracy: 0.659500 | 1.442 sec/iter\n",
      "Epoch: 487 | Batch: 003 / 011 | Total loss: 0.966 | Reg loss: 0.025 | Tree loss: 0.966 | Accuracy: 0.678000 | 1.442 sec/iter\n",
      "Epoch: 487 | Batch: 004 / 011 | Total loss: 0.955 | Reg loss: 0.025 | Tree loss: 0.955 | Accuracy: 0.670500 | 1.442 sec/iter\n",
      "Epoch: 487 | Batch: 005 / 011 | Total loss: 0.932 | Reg loss: 0.025 | Tree loss: 0.932 | Accuracy: 0.679500 | 1.442 sec/iter\n",
      "Epoch: 487 | Batch: 006 / 011 | Total loss: 0.933 | Reg loss: 0.025 | Tree loss: 0.933 | Accuracy: 0.689500 | 1.442 sec/iter\n",
      "Epoch: 487 | Batch: 007 / 011 | Total loss: 0.911 | Reg loss: 0.025 | Tree loss: 0.911 | Accuracy: 0.666500 | 1.442 sec/iter\n",
      "Epoch: 487 | Batch: 008 / 011 | Total loss: 0.920 | Reg loss: 0.025 | Tree loss: 0.920 | Accuracy: 0.660500 | 1.442 sec/iter\n",
      "Epoch: 487 | Batch: 009 / 011 | Total loss: 0.914 | Reg loss: 0.025 | Tree loss: 0.914 | Accuracy: 0.669000 | 1.442 sec/iter\n",
      "Epoch: 487 | Batch: 010 / 011 | Total loss: 0.907 | Reg loss: 0.025 | Tree loss: 0.907 | Accuracy: 0.696246 | 1.442 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 488 | Batch: 000 / 011 | Total loss: 1.027 | Reg loss: 0.025 | Tree loss: 1.027 | Accuracy: 0.661500 | 1.442 sec/iter\n",
      "Epoch: 488 | Batch: 001 / 011 | Total loss: 0.992 | Reg loss: 0.025 | Tree loss: 0.992 | Accuracy: 0.661500 | 1.442 sec/iter\n",
      "Epoch: 488 | Batch: 002 / 011 | Total loss: 0.983 | Reg loss: 0.025 | Tree loss: 0.983 | Accuracy: 0.679000 | 1.442 sec/iter\n",
      "Epoch: 488 | Batch: 003 / 011 | Total loss: 0.956 | Reg loss: 0.025 | Tree loss: 0.956 | Accuracy: 0.684500 | 1.442 sec/iter\n",
      "Epoch: 488 | Batch: 004 / 011 | Total loss: 0.914 | Reg loss: 0.025 | Tree loss: 0.914 | Accuracy: 0.705000 | 1.442 sec/iter\n",
      "Epoch: 488 | Batch: 005 / 011 | Total loss: 0.926 | Reg loss: 0.025 | Tree loss: 0.926 | Accuracy: 0.688500 | 1.442 sec/iter\n",
      "Epoch: 488 | Batch: 006 / 011 | Total loss: 0.917 | Reg loss: 0.025 | Tree loss: 0.917 | Accuracy: 0.673500 | 1.442 sec/iter\n",
      "Epoch: 488 | Batch: 007 / 011 | Total loss: 0.895 | Reg loss: 0.025 | Tree loss: 0.895 | Accuracy: 0.675500 | 1.441 sec/iter\n",
      "Epoch: 488 | Batch: 008 / 011 | Total loss: 0.918 | Reg loss: 0.025 | Tree loss: 0.918 | Accuracy: 0.667500 | 1.441 sec/iter\n",
      "Epoch: 488 | Batch: 009 / 011 | Total loss: 0.941 | Reg loss: 0.025 | Tree loss: 0.941 | Accuracy: 0.658500 | 1.441 sec/iter\n",
      "Epoch: 488 | Batch: 010 / 011 | Total loss: 0.975 | Reg loss: 0.025 | Tree loss: 0.975 | Accuracy: 0.641638 | 1.441 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 8: 0.9723756906077345\n",
      "Epoch: 489 | Batch: 000 / 011 | Total loss: 1.009 | Reg loss: 0.025 | Tree loss: 1.009 | Accuracy: 0.657500 | 1.442 sec/iter\n",
      "Epoch: 489 | Batch: 001 / 011 | Total loss: 1.005 | Reg loss: 0.025 | Tree loss: 1.005 | Accuracy: 0.662000 | 1.442 sec/iter\n",
      "Epoch: 489 | Batch: 002 / 011 | Total loss: 0.991 | Reg loss: 0.025 | Tree loss: 0.991 | Accuracy: 0.680000 | 1.442 sec/iter\n",
      "Epoch: 489 | Batch: 003 / 011 | Total loss: 0.946 | Reg loss: 0.025 | Tree loss: 0.946 | Accuracy: 0.701500 | 1.442 sec/iter\n",
      "Epoch: 489 | Batch: 004 / 011 | Total loss: 0.917 | Reg loss: 0.025 | Tree loss: 0.917 | Accuracy: 0.711000 | 1.442 sec/iter\n",
      "Epoch: 489 | Batch: 005 / 011 | Total loss: 0.917 | Reg loss: 0.025 | Tree loss: 0.917 | Accuracy: 0.692000 | 1.442 sec/iter\n",
      "Epoch: 489 | Batch: 006 / 011 | Total loss: 0.923 | Reg loss: 0.025 | Tree loss: 0.923 | Accuracy: 0.668000 | 1.442 sec/iter\n",
      "Epoch: 489 | Batch: 007 / 011 | Total loss: 0.932 | Reg loss: 0.025 | Tree loss: 0.932 | Accuracy: 0.659500 | 1.442 sec/iter\n",
      "Epoch: 489 | Batch: 008 / 011 | Total loss: 0.915 | Reg loss: 0.025 | Tree loss: 0.915 | Accuracy: 0.678500 | 1.442 sec/iter\n",
      "Epoch: 489 | Batch: 009 / 011 | Total loss: 0.942 | Reg loss: 0.025 | Tree loss: 0.942 | Accuracy: 0.652500 | 1.442 sec/iter\n",
      "Epoch: 489 | Batch: 010 / 011 | Total loss: 0.871 | Reg loss: 0.025 | Tree loss: 0.871 | Accuracy: 0.703072 | 1.442 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 490 | Batch: 000 / 011 | Total loss: 1.014 | Reg loss: 0.025 | Tree loss: 1.014 | Accuracy: 0.656000 | 1.442 sec/iter\n",
      "Epoch: 490 | Batch: 001 / 011 | Total loss: 0.985 | Reg loss: 0.025 | Tree loss: 0.985 | Accuracy: 0.670500 | 1.442 sec/iter\n",
      "Epoch: 490 | Batch: 002 / 011 | Total loss: 0.986 | Reg loss: 0.025 | Tree loss: 0.986 | Accuracy: 0.671500 | 1.442 sec/iter\n",
      "Epoch: 490 | Batch: 003 / 011 | Total loss: 0.953 | Reg loss: 0.025 | Tree loss: 0.953 | Accuracy: 0.690500 | 1.442 sec/iter\n",
      "Epoch: 490 | Batch: 004 / 011 | Total loss: 0.953 | Reg loss: 0.025 | Tree loss: 0.953 | Accuracy: 0.682000 | 1.442 sec/iter\n",
      "Epoch: 490 | Batch: 005 / 011 | Total loss: 0.920 | Reg loss: 0.025 | Tree loss: 0.920 | Accuracy: 0.691000 | 1.442 sec/iter\n",
      "Epoch: 490 | Batch: 006 / 011 | Total loss: 0.923 | Reg loss: 0.025 | Tree loss: 0.923 | Accuracy: 0.688000 | 1.442 sec/iter\n",
      "Epoch: 490 | Batch: 007 / 011 | Total loss: 0.927 | Reg loss: 0.025 | Tree loss: 0.927 | Accuracy: 0.671000 | 1.442 sec/iter\n",
      "Epoch: 490 | Batch: 008 / 011 | Total loss: 0.900 | Reg loss: 0.025 | Tree loss: 0.900 | Accuracy: 0.680500 | 1.442 sec/iter\n",
      "Epoch: 490 | Batch: 009 / 011 | Total loss: 0.910 | Reg loss: 0.025 | Tree loss: 0.910 | Accuracy: 0.663000 | 1.442 sec/iter\n",
      "Epoch: 490 | Batch: 010 / 011 | Total loss: 0.905 | Reg loss: 0.025 | Tree loss: 0.905 | Accuracy: 0.665529 | 1.442 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 491 | Batch: 000 / 011 | Total loss: 0.995 | Reg loss: 0.025 | Tree loss: 0.995 | Accuracy: 0.676500 | 1.443 sec/iter\n",
      "Epoch: 491 | Batch: 001 / 011 | Total loss: 0.988 | Reg loss: 0.025 | Tree loss: 0.988 | Accuracy: 0.676000 | 1.443 sec/iter\n",
      "Epoch: 491 | Batch: 002 / 011 | Total loss: 0.960 | Reg loss: 0.025 | Tree loss: 0.960 | Accuracy: 0.688500 | 1.443 sec/iter\n",
      "Epoch: 491 | Batch: 003 / 011 | Total loss: 0.967 | Reg loss: 0.025 | Tree loss: 0.967 | Accuracy: 0.670000 | 1.443 sec/iter\n",
      "Epoch: 491 | Batch: 004 / 011 | Total loss: 0.929 | Reg loss: 0.025 | Tree loss: 0.929 | Accuracy: 0.696000 | 1.442 sec/iter\n",
      "Epoch: 491 | Batch: 005 / 011 | Total loss: 0.925 | Reg loss: 0.025 | Tree loss: 0.925 | Accuracy: 0.702500 | 1.442 sec/iter\n",
      "Epoch: 491 | Batch: 006 / 011 | Total loss: 0.926 | Reg loss: 0.025 | Tree loss: 0.926 | Accuracy: 0.678000 | 1.442 sec/iter\n",
      "Epoch: 491 | Batch: 007 / 011 | Total loss: 0.918 | Reg loss: 0.025 | Tree loss: 0.918 | Accuracy: 0.670000 | 1.442 sec/iter\n",
      "Epoch: 491 | Batch: 008 / 011 | Total loss: 0.938 | Reg loss: 0.025 | Tree loss: 0.938 | Accuracy: 0.661000 | 1.442 sec/iter\n",
      "Epoch: 491 | Batch: 009 / 011 | Total loss: 0.911 | Reg loss: 0.025 | Tree loss: 0.911 | Accuracy: 0.663500 | 1.442 sec/iter\n",
      "Epoch: 491 | Batch: 010 / 011 | Total loss: 0.918 | Reg loss: 0.025 | Tree loss: 0.918 | Accuracy: 0.696246 | 1.442 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 492 | Batch: 000 / 011 | Total loss: 1.019 | Reg loss: 0.025 | Tree loss: 1.019 | Accuracy: 0.661000 | 1.443 sec/iter\n",
      "Epoch: 492 | Batch: 001 / 011 | Total loss: 0.995 | Reg loss: 0.025 | Tree loss: 0.995 | Accuracy: 0.660000 | 1.443 sec/iter\n",
      "Epoch: 492 | Batch: 002 / 011 | Total loss: 0.945 | Reg loss: 0.025 | Tree loss: 0.945 | Accuracy: 0.694500 | 1.443 sec/iter\n",
      "Epoch: 492 | Batch: 003 / 011 | Total loss: 0.965 | Reg loss: 0.025 | Tree loss: 0.965 | Accuracy: 0.672000 | 1.443 sec/iter\n",
      "Epoch: 492 | Batch: 004 / 011 | Total loss: 0.946 | Reg loss: 0.025 | Tree loss: 0.946 | Accuracy: 0.684000 | 1.443 sec/iter\n",
      "Epoch: 492 | Batch: 005 / 011 | Total loss: 0.956 | Reg loss: 0.025 | Tree loss: 0.956 | Accuracy: 0.675000 | 1.443 sec/iter\n",
      "Epoch: 492 | Batch: 006 / 011 | Total loss: 0.929 | Reg loss: 0.025 | Tree loss: 0.929 | Accuracy: 0.675000 | 1.442 sec/iter\n",
      "Epoch: 492 | Batch: 007 / 011 | Total loss: 0.892 | Reg loss: 0.025 | Tree loss: 0.892 | Accuracy: 0.688500 | 1.442 sec/iter\n",
      "Epoch: 492 | Batch: 008 / 011 | Total loss: 0.904 | Reg loss: 0.025 | Tree loss: 0.904 | Accuracy: 0.676000 | 1.442 sec/iter\n",
      "Epoch: 492 | Batch: 009 / 011 | Total loss: 0.905 | Reg loss: 0.025 | Tree loss: 0.905 | Accuracy: 0.679000 | 1.442 sec/iter\n",
      "Epoch: 492 | Batch: 010 / 011 | Total loss: 0.866 | Reg loss: 0.025 | Tree loss: 0.866 | Accuracy: 0.720137 | 1.442 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 493 | Batch: 000 / 011 | Total loss: 1.008 | Reg loss: 0.025 | Tree loss: 1.008 | Accuracy: 0.658000 | 1.443 sec/iter\n",
      "Epoch: 493 | Batch: 001 / 011 | Total loss: 0.962 | Reg loss: 0.025 | Tree loss: 0.962 | Accuracy: 0.694500 | 1.443 sec/iter\n",
      "Epoch: 493 | Batch: 002 / 011 | Total loss: 0.994 | Reg loss: 0.025 | Tree loss: 0.994 | Accuracy: 0.670500 | 1.443 sec/iter\n",
      "Epoch: 493 | Batch: 003 / 011 | Total loss: 0.950 | Reg loss: 0.025 | Tree loss: 0.950 | Accuracy: 0.698500 | 1.443 sec/iter\n",
      "Epoch: 493 | Batch: 004 / 011 | Total loss: 0.922 | Reg loss: 0.025 | Tree loss: 0.922 | Accuracy: 0.691000 | 1.443 sec/iter\n",
      "Epoch: 493 | Batch: 005 / 011 | Total loss: 0.956 | Reg loss: 0.025 | Tree loss: 0.956 | Accuracy: 0.658000 | 1.443 sec/iter\n",
      "Epoch: 493 | Batch: 006 / 011 | Total loss: 0.948 | Reg loss: 0.025 | Tree loss: 0.948 | Accuracy: 0.664500 | 1.443 sec/iter\n",
      "Epoch: 493 | Batch: 007 / 011 | Total loss: 0.913 | Reg loss: 0.025 | Tree loss: 0.913 | Accuracy: 0.677000 | 1.443 sec/iter\n",
      "Epoch: 493 | Batch: 008 / 011 | Total loss: 0.904 | Reg loss: 0.025 | Tree loss: 0.904 | Accuracy: 0.688000 | 1.442 sec/iter\n",
      "Epoch: 493 | Batch: 009 / 011 | Total loss: 0.921 | Reg loss: 0.025 | Tree loss: 0.921 | Accuracy: 0.666000 | 1.442 sec/iter\n",
      "Epoch: 493 | Batch: 010 / 011 | Total loss: 0.920 | Reg loss: 0.025 | Tree loss: 0.920 | Accuracy: 0.662116 | 1.442 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 8: 0.9723756906077345\n",
      "Epoch: 494 | Batch: 000 / 011 | Total loss: 1.016 | Reg loss: 0.025 | Tree loss: 1.016 | Accuracy: 0.655000 | 1.443 sec/iter\n",
      "Epoch: 494 | Batch: 001 / 011 | Total loss: 0.980 | Reg loss: 0.025 | Tree loss: 0.980 | Accuracy: 0.676000 | 1.443 sec/iter\n",
      "Epoch: 494 | Batch: 002 / 011 | Total loss: 0.988 | Reg loss: 0.025 | Tree loss: 0.988 | Accuracy: 0.669500 | 1.443 sec/iter\n",
      "Epoch: 494 | Batch: 003 / 011 | Total loss: 0.951 | Reg loss: 0.025 | Tree loss: 0.951 | Accuracy: 0.691500 | 1.442 sec/iter\n",
      "Epoch: 494 | Batch: 004 / 011 | Total loss: 0.953 | Reg loss: 0.025 | Tree loss: 0.953 | Accuracy: 0.685500 | 1.442 sec/iter\n",
      "Epoch: 494 | Batch: 005 / 011 | Total loss: 0.929 | Reg loss: 0.025 | Tree loss: 0.929 | Accuracy: 0.685500 | 1.442 sec/iter\n",
      "Epoch: 494 | Batch: 006 / 011 | Total loss: 0.920 | Reg loss: 0.025 | Tree loss: 0.920 | Accuracy: 0.675500 | 1.442 sec/iter\n",
      "Epoch: 494 | Batch: 007 / 011 | Total loss: 0.910 | Reg loss: 0.025 | Tree loss: 0.910 | Accuracy: 0.675000 | 1.442 sec/iter\n",
      "Epoch: 494 | Batch: 008 / 011 | Total loss: 0.888 | Reg loss: 0.025 | Tree loss: 0.888 | Accuracy: 0.697500 | 1.442 sec/iter\n",
      "Epoch: 494 | Batch: 009 / 011 | Total loss: 0.943 | Reg loss: 0.025 | Tree loss: 0.943 | Accuracy: 0.651000 | 1.442 sec/iter\n",
      "Epoch: 494 | Batch: 010 / 011 | Total loss: 0.838 | Reg loss: 0.025 | Tree loss: 0.838 | Accuracy: 0.730375 | 1.442 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 495 | Batch: 000 / 011 | Total loss: 1.006 | Reg loss: 0.025 | Tree loss: 1.006 | Accuracy: 0.677500 | 1.443 sec/iter\n",
      "Epoch: 495 | Batch: 001 / 011 | Total loss: 0.984 | Reg loss: 0.025 | Tree loss: 0.984 | Accuracy: 0.673500 | 1.443 sec/iter\n",
      "Epoch: 495 | Batch: 002 / 011 | Total loss: 0.955 | Reg loss: 0.025 | Tree loss: 0.955 | Accuracy: 0.695500 | 1.443 sec/iter\n",
      "Epoch: 495 | Batch: 003 / 011 | Total loss: 0.965 | Reg loss: 0.025 | Tree loss: 0.965 | Accuracy: 0.675000 | 1.443 sec/iter\n",
      "Epoch: 495 | Batch: 004 / 011 | Total loss: 0.935 | Reg loss: 0.025 | Tree loss: 0.935 | Accuracy: 0.682000 | 1.443 sec/iter\n",
      "Epoch: 495 | Batch: 005 / 011 | Total loss: 0.946 | Reg loss: 0.025 | Tree loss: 0.946 | Accuracy: 0.674000 | 1.443 sec/iter\n",
      "Epoch: 495 | Batch: 006 / 011 | Total loss: 0.927 | Reg loss: 0.025 | Tree loss: 0.927 | Accuracy: 0.666000 | 1.442 sec/iter\n",
      "Epoch: 495 | Batch: 007 / 011 | Total loss: 0.927 | Reg loss: 0.025 | Tree loss: 0.927 | Accuracy: 0.659500 | 1.442 sec/iter\n",
      "Epoch: 495 | Batch: 008 / 011 | Total loss: 0.927 | Reg loss: 0.025 | Tree loss: 0.927 | Accuracy: 0.675500 | 1.442 sec/iter\n",
      "Epoch: 495 | Batch: 009 / 011 | Total loss: 0.897 | Reg loss: 0.025 | Tree loss: 0.897 | Accuracy: 0.668000 | 1.442 sec/iter\n",
      "Epoch: 495 | Batch: 010 / 011 | Total loss: 0.920 | Reg loss: 0.025 | Tree loss: 0.920 | Accuracy: 0.655290 | 1.442 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 496 | Batch: 000 / 011 | Total loss: 0.989 | Reg loss: 0.025 | Tree loss: 0.989 | Accuracy: 0.674000 | 1.443 sec/iter\n",
      "Epoch: 496 | Batch: 001 / 011 | Total loss: 0.992 | Reg loss: 0.025 | Tree loss: 0.992 | Accuracy: 0.665000 | 1.443 sec/iter\n",
      "Epoch: 496 | Batch: 002 / 011 | Total loss: 0.962 | Reg loss: 0.025 | Tree loss: 0.962 | Accuracy: 0.682000 | 1.443 sec/iter\n",
      "Epoch: 496 | Batch: 003 / 011 | Total loss: 0.963 | Reg loss: 0.025 | Tree loss: 0.963 | Accuracy: 0.681500 | 1.443 sec/iter\n",
      "Epoch: 496 | Batch: 004 / 011 | Total loss: 0.942 | Reg loss: 0.025 | Tree loss: 0.942 | Accuracy: 0.689500 | 1.443 sec/iter\n",
      "Epoch: 496 | Batch: 005 / 011 | Total loss: 0.921 | Reg loss: 0.025 | Tree loss: 0.921 | Accuracy: 0.694500 | 1.443 sec/iter\n",
      "Epoch: 496 | Batch: 006 / 011 | Total loss: 0.941 | Reg loss: 0.025 | Tree loss: 0.941 | Accuracy: 0.672500 | 1.443 sec/iter\n",
      "Epoch: 496 | Batch: 007 / 011 | Total loss: 0.929 | Reg loss: 0.025 | Tree loss: 0.929 | Accuracy: 0.661000 | 1.443 sec/iter\n",
      "Epoch: 496 | Batch: 008 / 011 | Total loss: 0.926 | Reg loss: 0.025 | Tree loss: 0.926 | Accuracy: 0.655000 | 1.443 sec/iter\n",
      "Epoch: 496 | Batch: 009 / 011 | Total loss: 0.896 | Reg loss: 0.025 | Tree loss: 0.896 | Accuracy: 0.679500 | 1.442 sec/iter\n",
      "Epoch: 496 | Batch: 010 / 011 | Total loss: 0.925 | Reg loss: 0.025 | Tree loss: 0.925 | Accuracy: 0.655290 | 1.442 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 497 | Batch: 000 / 011 | Total loss: 1.019 | Reg loss: 0.025 | Tree loss: 1.019 | Accuracy: 0.663500 | 1.443 sec/iter\n",
      "Epoch: 497 | Batch: 001 / 011 | Total loss: 0.975 | Reg loss: 0.025 | Tree loss: 0.975 | Accuracy: 0.675000 | 1.443 sec/iter\n",
      "Epoch: 497 | Batch: 002 / 011 | Total loss: 0.966 | Reg loss: 0.025 | Tree loss: 0.966 | Accuracy: 0.672500 | 1.443 sec/iter\n",
      "Epoch: 497 | Batch: 003 / 011 | Total loss: 0.941 | Reg loss: 0.025 | Tree loss: 0.941 | Accuracy: 0.691000 | 1.443 sec/iter\n",
      "Epoch: 497 | Batch: 004 / 011 | Total loss: 0.924 | Reg loss: 0.025 | Tree loss: 0.924 | Accuracy: 0.698000 | 1.443 sec/iter\n",
      "Epoch: 497 | Batch: 005 / 011 | Total loss: 0.927 | Reg loss: 0.025 | Tree loss: 0.927 | Accuracy: 0.691000 | 1.443 sec/iter\n",
      "Epoch: 497 | Batch: 006 / 011 | Total loss: 0.923 | Reg loss: 0.025 | Tree loss: 0.923 | Accuracy: 0.684000 | 1.443 sec/iter\n",
      "Epoch: 497 | Batch: 007 / 011 | Total loss: 0.933 | Reg loss: 0.025 | Tree loss: 0.933 | Accuracy: 0.662500 | 1.443 sec/iter\n",
      "Epoch: 497 | Batch: 008 / 011 | Total loss: 0.912 | Reg loss: 0.025 | Tree loss: 0.912 | Accuracy: 0.671500 | 1.443 sec/iter\n",
      "Epoch: 497 | Batch: 009 / 011 | Total loss: 0.926 | Reg loss: 0.025 | Tree loss: 0.926 | Accuracy: 0.665000 | 1.443 sec/iter\n",
      "Epoch: 497 | Batch: 010 / 011 | Total loss: 0.940 | Reg loss: 0.025 | Tree loss: 0.940 | Accuracy: 0.634812 | 1.443 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 498 | Batch: 000 / 011 | Total loss: 0.993 | Reg loss: 0.025 | Tree loss: 0.993 | Accuracy: 0.685500 | 1.444 sec/iter\n",
      "Epoch: 498 | Batch: 001 / 011 | Total loss: 1.003 | Reg loss: 0.025 | Tree loss: 1.003 | Accuracy: 0.655000 | 1.443 sec/iter\n",
      "Epoch: 498 | Batch: 002 / 011 | Total loss: 0.977 | Reg loss: 0.025 | Tree loss: 0.977 | Accuracy: 0.682000 | 1.443 sec/iter\n",
      "Epoch: 498 | Batch: 003 / 011 | Total loss: 0.959 | Reg loss: 0.025 | Tree loss: 0.959 | Accuracy: 0.691000 | 1.443 sec/iter\n",
      "Epoch: 498 | Batch: 004 / 011 | Total loss: 0.926 | Reg loss: 0.025 | Tree loss: 0.926 | Accuracy: 0.692500 | 1.443 sec/iter\n",
      "Epoch: 498 | Batch: 005 / 011 | Total loss: 0.922 | Reg loss: 0.025 | Tree loss: 0.922 | Accuracy: 0.670500 | 1.443 sec/iter\n",
      "Epoch: 498 | Batch: 006 / 011 | Total loss: 0.921 | Reg loss: 0.025 | Tree loss: 0.921 | Accuracy: 0.680000 | 1.443 sec/iter\n",
      "Epoch: 498 | Batch: 007 / 011 | Total loss: 0.922 | Reg loss: 0.025 | Tree loss: 0.922 | Accuracy: 0.674000 | 1.443 sec/iter\n",
      "Epoch: 498 | Batch: 008 / 011 | Total loss: 0.929 | Reg loss: 0.025 | Tree loss: 0.929 | Accuracy: 0.652500 | 1.443 sec/iter\n",
      "Epoch: 498 | Batch: 009 / 011 | Total loss: 0.911 | Reg loss: 0.025 | Tree loss: 0.911 | Accuracy: 0.677500 | 1.443 sec/iter\n",
      "Epoch: 498 | Batch: 010 / 011 | Total loss: 0.911 | Reg loss: 0.025 | Tree loss: 0.911 | Accuracy: 0.696246 | 1.443 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 8: 0.9723756906077345\n",
      "Epoch: 499 | Batch: 000 / 011 | Total loss: 0.997 | Reg loss: 0.025 | Tree loss: 0.997 | Accuracy: 0.680500 | 1.444 sec/iter\n",
      "Epoch: 499 | Batch: 001 / 011 | Total loss: 0.981 | Reg loss: 0.025 | Tree loss: 0.981 | Accuracy: 0.672500 | 1.444 sec/iter\n",
      "Epoch: 499 | Batch: 002 / 011 | Total loss: 0.956 | Reg loss: 0.025 | Tree loss: 0.956 | Accuracy: 0.687000 | 1.444 sec/iter\n",
      "Epoch: 499 | Batch: 003 / 011 | Total loss: 0.948 | Reg loss: 0.025 | Tree loss: 0.948 | Accuracy: 0.697000 | 1.443 sec/iter\n",
      "Epoch: 499 | Batch: 004 / 011 | Total loss: 0.956 | Reg loss: 0.025 | Tree loss: 0.956 | Accuracy: 0.669000 | 1.443 sec/iter\n",
      "Epoch: 499 | Batch: 005 / 011 | Total loss: 0.939 | Reg loss: 0.025 | Tree loss: 0.939 | Accuracy: 0.665000 | 1.443 sec/iter\n",
      "Epoch: 499 | Batch: 006 / 011 | Total loss: 0.928 | Reg loss: 0.025 | Tree loss: 0.928 | Accuracy: 0.666500 | 1.443 sec/iter\n",
      "Epoch: 499 | Batch: 007 / 011 | Total loss: 0.922 | Reg loss: 0.025 | Tree loss: 0.922 | Accuracy: 0.674500 | 1.443 sec/iter\n",
      "Epoch: 499 | Batch: 008 / 011 | Total loss: 0.891 | Reg loss: 0.025 | Tree loss: 0.891 | Accuracy: 0.700500 | 1.443 sec/iter\n",
      "Epoch: 499 | Batch: 009 / 011 | Total loss: 0.917 | Reg loss: 0.025 | Tree loss: 0.917 | Accuracy: 0.673000 | 1.443 sec/iter\n",
      "Epoch: 499 | Batch: 010 / 011 | Total loss: 0.842 | Reg loss: 0.025 | Tree loss: 0.842 | Accuracy: 0.699659 | 1.443 sec/iter\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "iteration = 0\n",
    "for epoch in range(epochs):\n",
    "    # Training\n",
    "    avg_sp = show_sparseness(tree)\n",
    "    sparsity.append(avg_sp)\n",
    "    iteration = do_epoch(tree, tree_loader, device, log_interval, losses, accs, epoch, iteration)\n",
    "    \n",
    "    if epoch % 1 == 0:\n",
    "        prune_tree(tree, factor=5)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel('Iteration')\n",
    "plt.plot(accs, label='Accuracy vs iteration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel('Iteration')\n",
    "plt.plot(losses, label='Loss vs iteration')\n",
    "plt.yscale(\"log\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "weights = tree.inner_nodes.weight.cpu().detach().numpy().flatten()\n",
    "plt.hist(weights, bins=500)\n",
    "weights_std = np.std(weights)\n",
    "weights_mean = np.mean(weights)\n",
    "plt.axvline(weights_mean + weights_std, color='r')\n",
    "plt.axvline(weights_mean - weights_std, color='r')\n",
    "plt.title(f\"Mean: {weights_mean}   |   STD: {weights_std}\")\n",
    "plt.yscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average height: 7.579710144927536\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1200x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(15, 10), dpi=80)\n",
    "avg_height, root = tree.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accumulate samples in the leaves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of patterns: 69\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of patterns: {len(root.get_leaves())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = 'MLE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eitan.k/miniconda3/envs/rambo/lib/python3.8/site-packages/torch/nn/functional.py:1709: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    }
   ],
   "source": [
    "root.clear_leaves_samples()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (data, target) in enumerate(tree_loader):\n",
    "        root.accumulate_samples(data, method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tighten boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eitan.k/EntangledExplainableClustering/soft_decision_tree/sdt_model.py:14: RuntimeWarning: divide by zero encountered in log\n",
      "  return np.log(1 / (1 - x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Pattern 1 ==============\n",
      "20293\n",
      "============== Pattern 2 ==============\n",
      "============== Pattern 3 ==============\n",
      "============== Pattern 4 ==============\n",
      "============== Pattern 5 ==============\n",
      "============== Pattern 6 ==============\n",
      "============== Pattern 7 ==============\n",
      "============== Pattern 8 ==============\n",
      "============== Pattern 9 ==============\n",
      "============== Pattern 10 ==============\n",
      "============== Pattern 11 ==============\n",
      "============== Pattern 12 ==============\n",
      "============== Pattern 13 ==============\n",
      "============== Pattern 14 ==============\n",
      "============== Pattern 15 ==============\n",
      "============== Pattern 16 ==============\n",
      "============== Pattern 17 ==============\n",
      "============== Pattern 18 ==============\n",
      "============== Pattern 19 ==============\n",
      "============== Pattern 20 ==============\n",
      "============== Pattern 21 ==============\n",
      "============== Pattern 22 ==============\n",
      "============== Pattern 23 ==============\n",
      "============== Pattern 24 ==============\n",
      "============== Pattern 25 ==============\n",
      "============== Pattern 26 ==============\n",
      "============== Pattern 27 ==============\n",
      "============== Pattern 28 ==============\n",
      "============== Pattern 29 ==============\n",
      "============== Pattern 30 ==============\n",
      "============== Pattern 31 ==============\n",
      "============== Pattern 32 ==============\n",
      "============== Pattern 33 ==============\n",
      "============== Pattern 34 ==============\n",
      "============== Pattern 35 ==============\n",
      "============== Pattern 36 ==============\n",
      "============== Pattern 37 ==============\n",
      "============== Pattern 38 ==============\n",
      "============== Pattern 39 ==============\n",
      "============== Pattern 40 ==============\n",
      "============== Pattern 41 ==============\n",
      "============== Pattern 42 ==============\n",
      "============== Pattern 43 ==============\n",
      "============== Pattern 44 ==============\n",
      "============== Pattern 45 ==============\n",
      "============== Pattern 46 ==============\n",
      "============== Pattern 47 ==============\n",
      "============== Pattern 48 ==============\n",
      "============== Pattern 49 ==============\n",
      "============== Pattern 50 ==============\n",
      "============== Pattern 51 ==============\n",
      "============== Pattern 52 ==============\n",
      "============== Pattern 53 ==============\n",
      "============== Pattern 54 ==============\n",
      "============== Pattern 55 ==============\n",
      "============== Pattern 56 ==============\n",
      "============== Pattern 57 ==============\n",
      "============== Pattern 58 ==============\n",
      "============== Pattern 59 ==============\n",
      "============== Pattern 60 ==============\n",
      "============== Pattern 61 ==============\n",
      "============== Pattern 62 ==============\n",
      "============== Pattern 63 ==============\n",
      "============== Pattern 64 ==============\n",
      "============== Pattern 65 ==============\n",
      "============== Pattern 66 ==============\n",
      "============== Pattern 67 ==============\n",
      "============== Pattern 68 ==============\n",
      "============== Pattern 69 ==============\n",
      "Average comprehensibility: 68.78260869565217\n",
      "std comprehensibility: 18.011968295840386\n",
      "var comprehensibility: 324.43100189035925\n",
      "minimum comprehensibility: 26\n",
      "maximum comprehensibility: 96\n"
     ]
    }
   ],
   "source": [
    "signal_names = dataset.dataset.all_signals\n",
    "normalizers = torch.tensor([])\n",
    "attr_names = []\n",
    "for signal_name in signal_names:\n",
    "    attr_names += [f\"T{i}.{signal_name}\" for i in range(sampled.shape[-1])]\n",
    "    sensor_norm = torch.tensor([torch.tensor(dataset.dataset.sensor_maxs[signal_name]) for _ in range(sampled.shape[-1])])\n",
    "    normalizers = torch.cat([normalizers, sensor_norm])\n",
    "    \n",
    "\n",
    "# print(attr_names)\n",
    "leaves = root.get_leaves()\n",
    "sum_comprehensibility = 0\n",
    "comprehensibilities = []\n",
    "for pattern_counter, leaf in enumerate(leaves):\n",
    "    leaf.reset_path()\n",
    "    leaf.tighten_with_accumulated_samples()\n",
    "    conds = leaf.get_path_conditions(attr_names)\n",
    "    for cond in conds:\n",
    "        cond.weights = cond.weights / normalizers\n",
    "    print(f\"============== Pattern {pattern_counter + 1} ==============\")\n",
    "    comprehensibilities.append(sum([cond.comprehensibility for cond in conds]))\n",
    "    \n",
    "print(f\"Average comprehensibility: {np.mean(comprehensibilities)}\")\n",
    "print(f\"std comprehensibility: {np.std(comprehensibilities)}\")\n",
    "print(f\"var comprehensibility: {np.var(comprehensibilities)}\")\n",
    "print(f\"minimum comprehensibility: {np.min(comprehensibilities)}\")\n",
    "print(f\"maximum comprehensibility: {np.max(comprehensibilities)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
