{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib widget\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import torch.nn as nn\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from stream_generators.market_basket_dataset import MarketBasketDataset, BinaryEncodingTransform, RemoveItemsTransform\n",
    "from utils.MatplotlibUtils import reduce_dims_and_plot\n",
    "from network.auto_encoder import AutoEncoder\n",
    "from losses.knn_loss import KNNLoss\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from soft_decision_tree.sdt_model import SDT\n",
    "from sklearn.metrics import davies_bouldin_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 256\n",
    "tree_depth = 12\n",
    "device = 'cuda'\n",
    "dataset_path = r\"/mnt/qnap/ekosman/Groceries_dataset.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the market basket dataset and use one-hot encoding for items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MarketBasketDataset(dataset_path=dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoEncoder(dataset.n_items, 50, 4).train().to(device)\n",
    "epochs = 500\n",
    "lr = 5e-3\n",
    "batch_size = 512\n",
    "log_every = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.transform = torchvision.transforms.Compose([\n",
    "    RemoveItemsTransform(p=0.5),\n",
    "    BinaryEncodingTransform(mapping=dataset.items_to_idx),\n",
    "]\n",
    ")\n",
    "dataset.target_transform = torchvision.transforms.Compose([\n",
    "    BinaryEncodingTransform(mapping=dataset.items_to_idx),\n",
    "]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 / 500 | iteration 0 / 30 | Total Loss: 8.182477951049805 | KNN Loss: 6.232606410980225 | BCE Loss: 1.94987154006958\n",
      "Epoch 0 / 500 | iteration 5 / 30 | Total Loss: 8.211150169372559 | KNN Loss: 6.232600688934326 | BCE Loss: 1.9785493612289429\n",
      "Epoch 0 / 500 | iteration 10 / 30 | Total Loss: 8.215576171875 | KNN Loss: 6.232440948486328 | BCE Loss: 1.98313570022583\n",
      "Epoch 0 / 500 | iteration 15 / 30 | Total Loss: 8.188455581665039 | KNN Loss: 6.232435703277588 | BCE Loss: 1.9560197591781616\n",
      "Epoch 0 / 500 | iteration 20 / 30 | Total Loss: 8.175670623779297 | KNN Loss: 6.232313632965088 | BCE Loss: 1.9433574676513672\n",
      "Epoch 0 / 500 | iteration 25 / 30 | Total Loss: 8.12234878540039 | KNN Loss: 6.232375621795654 | BCE Loss: 1.8899729251861572\n",
      "Epoch 1 / 500 | iteration 0 / 30 | Total Loss: 8.198297500610352 | KNN Loss: 6.232083797454834 | BCE Loss: 1.9662132263183594\n",
      "Epoch 1 / 500 | iteration 5 / 30 | Total Loss: 8.153447151184082 | KNN Loss: 6.232396602630615 | BCE Loss: 1.9210506677627563\n",
      "Epoch 1 / 500 | iteration 10 / 30 | Total Loss: 8.152872085571289 | KNN Loss: 6.232117176055908 | BCE Loss: 1.9207546710968018\n",
      "Epoch 1 / 500 | iteration 15 / 30 | Total Loss: 8.11858081817627 | KNN Loss: 6.232038974761963 | BCE Loss: 1.8865418434143066\n",
      "Epoch 1 / 500 | iteration 20 / 30 | Total Loss: 8.129182815551758 | KNN Loss: 6.231962203979492 | BCE Loss: 1.8972210884094238\n",
      "Epoch 1 / 500 | iteration 25 / 30 | Total Loss: 8.113350868225098 | KNN Loss: 6.231910228729248 | BCE Loss: 1.8814409971237183\n",
      "Epoch 2 / 500 | iteration 0 / 30 | Total Loss: 8.130870819091797 | KNN Loss: 6.231736660003662 | BCE Loss: 1.899134635925293\n",
      "Epoch 2 / 500 | iteration 5 / 30 | Total Loss: 8.137304306030273 | KNN Loss: 6.231465816497803 | BCE Loss: 1.9058380126953125\n",
      "Epoch 2 / 500 | iteration 10 / 30 | Total Loss: 8.09416389465332 | KNN Loss: 6.231373310089111 | BCE Loss: 1.862790584564209\n",
      "Epoch 2 / 500 | iteration 15 / 30 | Total Loss: 8.094829559326172 | KNN Loss: 6.23147439956665 | BCE Loss: 1.8633546829223633\n",
      "Epoch 2 / 500 | iteration 20 / 30 | Total Loss: 8.047341346740723 | KNN Loss: 6.2314558029174805 | BCE Loss: 1.8158857822418213\n",
      "Epoch 2 / 500 | iteration 25 / 30 | Total Loss: 8.10768985748291 | KNN Loss: 6.231472492218018 | BCE Loss: 1.8762174844741821\n",
      "Epoch 3 / 500 | iteration 0 / 30 | Total Loss: 8.085882186889648 | KNN Loss: 6.231099605560303 | BCE Loss: 1.8547825813293457\n",
      "Epoch 3 / 500 | iteration 5 / 30 | Total Loss: 8.081788063049316 | KNN Loss: 6.230716705322266 | BCE Loss: 1.8510711193084717\n",
      "Epoch 3 / 500 | iteration 10 / 30 | Total Loss: 8.05538272857666 | KNN Loss: 6.230935096740723 | BCE Loss: 1.8244476318359375\n",
      "Epoch 3 / 500 | iteration 15 / 30 | Total Loss: 8.079280853271484 | KNN Loss: 6.230608940124512 | BCE Loss: 1.8486720323562622\n",
      "Epoch 3 / 500 | iteration 20 / 30 | Total Loss: 8.020886421203613 | KNN Loss: 6.230827331542969 | BCE Loss: 1.7900594472885132\n",
      "Epoch 3 / 500 | iteration 25 / 30 | Total Loss: 8.015729904174805 | KNN Loss: 6.230547904968262 | BCE Loss: 1.7851815223693848\n",
      "Epoch 4 / 500 | iteration 0 / 30 | Total Loss: 7.999063491821289 | KNN Loss: 6.230437278747559 | BCE Loss: 1.768626093864441\n",
      "Epoch 4 / 500 | iteration 5 / 30 | Total Loss: 8.006586074829102 | KNN Loss: 6.230181694030762 | BCE Loss: 1.7764041423797607\n",
      "Epoch 4 / 500 | iteration 10 / 30 | Total Loss: 8.014457702636719 | KNN Loss: 6.2297773361206055 | BCE Loss: 1.7846803665161133\n",
      "Epoch 4 / 500 | iteration 15 / 30 | Total Loss: 7.994426250457764 | KNN Loss: 6.229816913604736 | BCE Loss: 1.7646092176437378\n",
      "Epoch 4 / 500 | iteration 20 / 30 | Total Loss: 8.01950454711914 | KNN Loss: 6.229437828063965 | BCE Loss: 1.7900662422180176\n",
      "Epoch 4 / 500 | iteration 25 / 30 | Total Loss: 7.971489906311035 | KNN Loss: 6.229278087615967 | BCE Loss: 1.7422120571136475\n",
      "Epoch 5 / 500 | iteration 0 / 30 | Total Loss: 7.949039936065674 | KNN Loss: 6.22915506362915 | BCE Loss: 1.719884991645813\n",
      "Epoch 5 / 500 | iteration 5 / 30 | Total Loss: 7.931997299194336 | KNN Loss: 6.22869348526001 | BCE Loss: 1.703303575515747\n",
      "Epoch 5 / 500 | iteration 10 / 30 | Total Loss: 7.950750827789307 | KNN Loss: 6.228760719299316 | BCE Loss: 1.7219901084899902\n",
      "Epoch 5 / 500 | iteration 15 / 30 | Total Loss: 7.928262233734131 | KNN Loss: 6.228191375732422 | BCE Loss: 1.700070858001709\n",
      "Epoch 5 / 500 | iteration 20 / 30 | Total Loss: 7.946836471557617 | KNN Loss: 6.227737903594971 | BCE Loss: 1.7190983295440674\n",
      "Epoch 5 / 500 | iteration 25 / 30 | Total Loss: 7.953780651092529 | KNN Loss: 6.228092193603516 | BCE Loss: 1.7256884574890137\n",
      "Epoch 6 / 500 | iteration 0 / 30 | Total Loss: 7.9103546142578125 | KNN Loss: 6.227712154388428 | BCE Loss: 1.6826424598693848\n",
      "Epoch 6 / 500 | iteration 5 / 30 | Total Loss: 7.89092493057251 | KNN Loss: 6.227362155914307 | BCE Loss: 1.6635628938674927\n",
      "Epoch 6 / 500 | iteration 10 / 30 | Total Loss: 7.905919075012207 | KNN Loss: 6.226827621459961 | BCE Loss: 1.6790916919708252\n",
      "Epoch 6 / 500 | iteration 15 / 30 | Total Loss: 7.84354829788208 | KNN Loss: 6.226786136627197 | BCE Loss: 1.6167621612548828\n",
      "Epoch 6 / 500 | iteration 20 / 30 | Total Loss: 7.864727973937988 | KNN Loss: 6.226190567016602 | BCE Loss: 1.6385371685028076\n",
      "Epoch 6 / 500 | iteration 25 / 30 | Total Loss: 7.800202369689941 | KNN Loss: 6.224667549133301 | BCE Loss: 1.5755349397659302\n",
      "Epoch 7 / 500 | iteration 0 / 30 | Total Loss: 7.80620813369751 | KNN Loss: 6.223307132720947 | BCE Loss: 1.582900881767273\n",
      "Epoch 7 / 500 | iteration 5 / 30 | Total Loss: 7.805437088012695 | KNN Loss: 6.223092079162598 | BCE Loss: 1.5823450088500977\n",
      "Epoch 7 / 500 | iteration 10 / 30 | Total Loss: 7.784966468811035 | KNN Loss: 6.223057270050049 | BCE Loss: 1.5619093179702759\n",
      "Epoch 7 / 500 | iteration 15 / 30 | Total Loss: 7.748075008392334 | KNN Loss: 6.223316192626953 | BCE Loss: 1.5247588157653809\n",
      "Epoch 7 / 500 | iteration 20 / 30 | Total Loss: 7.7325239181518555 | KNN Loss: 6.2222676277160645 | BCE Loss: 1.5102561712265015\n",
      "Epoch 7 / 500 | iteration 25 / 30 | Total Loss: 7.72570276260376 | KNN Loss: 6.2219414710998535 | BCE Loss: 1.5037612915039062\n",
      "Epoch 8 / 500 | iteration 0 / 30 | Total Loss: 7.7949066162109375 | KNN Loss: 6.221786975860596 | BCE Loss: 1.5731194019317627\n",
      "Epoch 8 / 500 | iteration 5 / 30 | Total Loss: 7.704623699188232 | KNN Loss: 6.221218585968018 | BCE Loss: 1.4834051132202148\n",
      "Epoch 8 / 500 | iteration 10 / 30 | Total Loss: 7.635645866394043 | KNN Loss: 6.218652725219727 | BCE Loss: 1.4169929027557373\n",
      "Epoch 8 / 500 | iteration 15 / 30 | Total Loss: 7.660077095031738 | KNN Loss: 6.2182087898254395 | BCE Loss: 1.4418680667877197\n",
      "Epoch 8 / 500 | iteration 20 / 30 | Total Loss: 7.622496604919434 | KNN Loss: 6.216193199157715 | BCE Loss: 1.4063032865524292\n",
      "Epoch 8 / 500 | iteration 25 / 30 | Total Loss: 7.562137126922607 | KNN Loss: 6.216214656829834 | BCE Loss: 1.3459224700927734\n",
      "Epoch 9 / 500 | iteration 0 / 30 | Total Loss: 7.59712028503418 | KNN Loss: 6.216407775878906 | BCE Loss: 1.3807127475738525\n",
      "Epoch 9 / 500 | iteration 5 / 30 | Total Loss: 7.534738063812256 | KNN Loss: 6.213868618011475 | BCE Loss: 1.3208694458007812\n",
      "Epoch 9 / 500 | iteration 10 / 30 | Total Loss: 7.520125389099121 | KNN Loss: 6.210822105407715 | BCE Loss: 1.3093031644821167\n",
      "Epoch 9 / 500 | iteration 15 / 30 | Total Loss: 7.505119800567627 | KNN Loss: 6.2110748291015625 | BCE Loss: 1.2940449714660645\n",
      "Epoch 9 / 500 | iteration 20 / 30 | Total Loss: 7.472906112670898 | KNN Loss: 6.20896053314209 | BCE Loss: 1.2639458179473877\n",
      "Epoch 9 / 500 | iteration 25 / 30 | Total Loss: 7.432900905609131 | KNN Loss: 6.20332670211792 | BCE Loss: 1.2295743227005005\n",
      "Epoch 10 / 500 | iteration 0 / 30 | Total Loss: 7.431233882904053 | KNN Loss: 6.203474044799805 | BCE Loss: 1.227759838104248\n",
      "Epoch 10 / 500 | iteration 5 / 30 | Total Loss: 7.43013334274292 | KNN Loss: 6.202047824859619 | BCE Loss: 1.2280855178833008\n",
      "Epoch 10 / 500 | iteration 10 / 30 | Total Loss: 7.419128894805908 | KNN Loss: 6.200311660766602 | BCE Loss: 1.2188172340393066\n",
      "Epoch 10 / 500 | iteration 15 / 30 | Total Loss: 7.37098503112793 | KNN Loss: 6.194252014160156 | BCE Loss: 1.1767332553863525\n",
      "Epoch 10 / 500 | iteration 20 / 30 | Total Loss: 7.376909255981445 | KNN Loss: 6.195506572723389 | BCE Loss: 1.1814029216766357\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 / 500 | iteration 25 / 30 | Total Loss: 7.318770408630371 | KNN Loss: 6.189958572387695 | BCE Loss: 1.1288115978240967\n",
      "Epoch 11 / 500 | iteration 0 / 30 | Total Loss: 7.308002471923828 | KNN Loss: 6.188998222351074 | BCE Loss: 1.119004487991333\n",
      "Epoch 11 / 500 | iteration 5 / 30 | Total Loss: 7.333943843841553 | KNN Loss: 6.183051586151123 | BCE Loss: 1.1508922576904297\n",
      "Epoch 11 / 500 | iteration 10 / 30 | Total Loss: 7.269381523132324 | KNN Loss: 6.177389144897461 | BCE Loss: 1.0919926166534424\n",
      "Epoch 11 / 500 | iteration 15 / 30 | Total Loss: 7.314318656921387 | KNN Loss: 6.177396774291992 | BCE Loss: 1.136922001838684\n",
      "Epoch 11 / 500 | iteration 20 / 30 | Total Loss: 7.283210754394531 | KNN Loss: 6.16886043548584 | BCE Loss: 1.1143503189086914\n",
      "Epoch 11 / 500 | iteration 25 / 30 | Total Loss: 7.2974090576171875 | KNN Loss: 6.164889812469482 | BCE Loss: 1.1325193643569946\n",
      "Epoch 12 / 500 | iteration 0 / 30 | Total Loss: 7.250821590423584 | KNN Loss: 6.15731143951416 | BCE Loss: 1.0935100317001343\n",
      "Epoch 12 / 500 | iteration 5 / 30 | Total Loss: 7.239255428314209 | KNN Loss: 6.148444652557373 | BCE Loss: 1.090810775756836\n",
      "Epoch 12 / 500 | iteration 10 / 30 | Total Loss: 7.239650726318359 | KNN Loss: 6.150568962097168 | BCE Loss: 1.0890820026397705\n",
      "Epoch 12 / 500 | iteration 15 / 30 | Total Loss: 7.227732181549072 | KNN Loss: 6.135636806488037 | BCE Loss: 1.0920953750610352\n",
      "Epoch 12 / 500 | iteration 20 / 30 | Total Loss: 7.218563556671143 | KNN Loss: 6.1341753005981445 | BCE Loss: 1.084388256072998\n",
      "Epoch 12 / 500 | iteration 25 / 30 | Total Loss: 7.209576606750488 | KNN Loss: 6.128977298736572 | BCE Loss: 1.080599308013916\n",
      "Epoch 13 / 500 | iteration 0 / 30 | Total Loss: 7.190094470977783 | KNN Loss: 6.116319179534912 | BCE Loss: 1.0737754106521606\n",
      "Epoch 13 / 500 | iteration 5 / 30 | Total Loss: 7.191582679748535 | KNN Loss: 6.10011100769043 | BCE Loss: 1.091471791267395\n",
      "Epoch 13 / 500 | iteration 10 / 30 | Total Loss: 7.160939693450928 | KNN Loss: 6.089992523193359 | BCE Loss: 1.0709470510482788\n",
      "Epoch 13 / 500 | iteration 15 / 30 | Total Loss: 7.1944427490234375 | KNN Loss: 6.095180988311768 | BCE Loss: 1.0992616415023804\n",
      "Epoch 13 / 500 | iteration 20 / 30 | Total Loss: 7.135457992553711 | KNN Loss: 6.079648494720459 | BCE Loss: 1.055809497833252\n",
      "Epoch 13 / 500 | iteration 25 / 30 | Total Loss: 7.135558605194092 | KNN Loss: 6.071539878845215 | BCE Loss: 1.0640186071395874\n",
      "Epoch 14 / 500 | iteration 0 / 30 | Total Loss: 7.148900032043457 | KNN Loss: 6.066015243530273 | BCE Loss: 1.0828850269317627\n",
      "Epoch 14 / 500 | iteration 5 / 30 | Total Loss: 7.116142272949219 | KNN Loss: 6.051738262176514 | BCE Loss: 1.0644042491912842\n",
      "Epoch 14 / 500 | iteration 10 / 30 | Total Loss: 7.141363143920898 | KNN Loss: 6.035412311553955 | BCE Loss: 1.105950951576233\n",
      "Epoch 14 / 500 | iteration 15 / 30 | Total Loss: 7.074599266052246 | KNN Loss: 6.026549339294434 | BCE Loss: 1.0480501651763916\n",
      "Epoch 14 / 500 | iteration 20 / 30 | Total Loss: 7.088118553161621 | KNN Loss: 6.008723735809326 | BCE Loss: 1.079395055770874\n",
      "Epoch 14 / 500 | iteration 25 / 30 | Total Loss: 7.081079483032227 | KNN Loss: 5.995334148406982 | BCE Loss: 1.0857454538345337\n",
      "Epoch 15 / 500 | iteration 0 / 30 | Total Loss: 7.057511329650879 | KNN Loss: 5.981086254119873 | BCE Loss: 1.0764250755310059\n",
      "Epoch 15 / 500 | iteration 5 / 30 | Total Loss: 7.028700828552246 | KNN Loss: 5.972690105438232 | BCE Loss: 1.0560107231140137\n",
      "Epoch 15 / 500 | iteration 10 / 30 | Total Loss: 7.0151166915893555 | KNN Loss: 5.951308727264404 | BCE Loss: 1.063807725906372\n",
      "Epoch 15 / 500 | iteration 15 / 30 | Total Loss: 7.009943962097168 | KNN Loss: 5.929684638977051 | BCE Loss: 1.0802593231201172\n",
      "Epoch 15 / 500 | iteration 20 / 30 | Total Loss: 7.008872032165527 | KNN Loss: 5.920382022857666 | BCE Loss: 1.0884902477264404\n",
      "Epoch 15 / 500 | iteration 25 / 30 | Total Loss: 6.999199390411377 | KNN Loss: 5.918403148651123 | BCE Loss: 1.080796241760254\n",
      "Epoch 16 / 500 | iteration 0 / 30 | Total Loss: 6.988884925842285 | KNN Loss: 5.910648345947266 | BCE Loss: 1.0782365798950195\n",
      "Epoch 16 / 500 | iteration 5 / 30 | Total Loss: 6.9723005294799805 | KNN Loss: 5.889485836029053 | BCE Loss: 1.0828149318695068\n",
      "Epoch 16 / 500 | iteration 10 / 30 | Total Loss: 6.996262073516846 | KNN Loss: 5.895754337310791 | BCE Loss: 1.1005077362060547\n",
      "Epoch 16 / 500 | iteration 15 / 30 | Total Loss: 6.938256740570068 | KNN Loss: 5.87354850769043 | BCE Loss: 1.0647081136703491\n",
      "Epoch 16 / 500 | iteration 20 / 30 | Total Loss: 6.918251991271973 | KNN Loss: 5.865562438964844 | BCE Loss: 1.0526893138885498\n",
      "Epoch 16 / 500 | iteration 25 / 30 | Total Loss: 6.920966625213623 | KNN Loss: 5.863646030426025 | BCE Loss: 1.0573205947875977\n",
      "Epoch 17 / 500 | iteration 0 / 30 | Total Loss: 6.938809394836426 | KNN Loss: 5.858999729156494 | BCE Loss: 1.0798096656799316\n",
      "Epoch 17 / 500 | iteration 5 / 30 | Total Loss: 6.931985855102539 | KNN Loss: 5.860350131988525 | BCE Loss: 1.0716357231140137\n",
      "Epoch 17 / 500 | iteration 10 / 30 | Total Loss: 6.927100658416748 | KNN Loss: 5.854887008666992 | BCE Loss: 1.0722135305404663\n",
      "Epoch 17 / 500 | iteration 15 / 30 | Total Loss: 6.923849582672119 | KNN Loss: 5.8506083488464355 | BCE Loss: 1.0732412338256836\n",
      "Epoch 17 / 500 | iteration 20 / 30 | Total Loss: 6.90286111831665 | KNN Loss: 5.836735725402832 | BCE Loss: 1.0661253929138184\n",
      "Epoch 17 / 500 | iteration 25 / 30 | Total Loss: 6.868363857269287 | KNN Loss: 5.829248428344727 | BCE Loss: 1.0391154289245605\n",
      "Epoch 18 / 500 | iteration 0 / 30 | Total Loss: 6.911011219024658 | KNN Loss: 5.827739715576172 | BCE Loss: 1.0832715034484863\n",
      "Epoch 18 / 500 | iteration 5 / 30 | Total Loss: 6.87650203704834 | KNN Loss: 5.830789089202881 | BCE Loss: 1.045712947845459\n",
      "Epoch 18 / 500 | iteration 10 / 30 | Total Loss: 6.852912902832031 | KNN Loss: 5.815763473510742 | BCE Loss: 1.0371496677398682\n",
      "Epoch 18 / 500 | iteration 15 / 30 | Total Loss: 6.877630233764648 | KNN Loss: 5.812650680541992 | BCE Loss: 1.0649794340133667\n",
      "Epoch 18 / 500 | iteration 20 / 30 | Total Loss: 6.883225917816162 | KNN Loss: 5.805841445922852 | BCE Loss: 1.077384352684021\n",
      "Epoch 18 / 500 | iteration 25 / 30 | Total Loss: 6.864853858947754 | KNN Loss: 5.811270236968994 | BCE Loss: 1.0535835027694702\n",
      "Epoch 19 / 500 | iteration 0 / 30 | Total Loss: 6.853476524353027 | KNN Loss: 5.808823108673096 | BCE Loss: 1.0446536540985107\n",
      "Epoch 19 / 500 | iteration 5 / 30 | Total Loss: 6.883774280548096 | KNN Loss: 5.825264930725098 | BCE Loss: 1.058509349822998\n",
      "Epoch 19 / 500 | iteration 10 / 30 | Total Loss: 6.856686592102051 | KNN Loss: 5.798791408538818 | BCE Loss: 1.0578951835632324\n",
      "Epoch 19 / 500 | iteration 15 / 30 | Total Loss: 6.869840621948242 | KNN Loss: 5.781528949737549 | BCE Loss: 1.0883115530014038\n",
      "Epoch 19 / 500 | iteration 20 / 30 | Total Loss: 6.848135948181152 | KNN Loss: 5.794270038604736 | BCE Loss: 1.053865909576416\n",
      "Epoch 19 / 500 | iteration 25 / 30 | Total Loss: 6.848264217376709 | KNN Loss: 5.791060924530029 | BCE Loss: 1.0572032928466797\n",
      "Epoch 20 / 500 | iteration 0 / 30 | Total Loss: 6.81777286529541 | KNN Loss: 5.763371467590332 | BCE Loss: 1.0544013977050781\n",
      "Epoch 20 / 500 | iteration 5 / 30 | Total Loss: 6.830311298370361 | KNN Loss: 5.780725002288818 | BCE Loss: 1.049586296081543\n",
      "Epoch 20 / 500 | iteration 10 / 30 | Total Loss: 6.852977752685547 | KNN Loss: 5.779660224914551 | BCE Loss: 1.0733177661895752\n",
      "Epoch 20 / 500 | iteration 15 / 30 | Total Loss: 6.807913780212402 | KNN Loss: 5.770057201385498 | BCE Loss: 1.0378565788269043\n",
      "Epoch 20 / 500 | iteration 20 / 30 | Total Loss: 6.8291192054748535 | KNN Loss: 5.768237590789795 | BCE Loss: 1.0608817338943481\n",
      "Epoch 20 / 500 | iteration 25 / 30 | Total Loss: 6.792386054992676 | KNN Loss: 5.746301651000977 | BCE Loss: 1.0460846424102783\n",
      "Epoch 21 / 500 | iteration 0 / 30 | Total Loss: 6.827337741851807 | KNN Loss: 5.758364677429199 | BCE Loss: 1.068973183631897\n",
      "Epoch 21 / 500 | iteration 5 / 30 | Total Loss: 6.806154727935791 | KNN Loss: 5.742937088012695 | BCE Loss: 1.0632176399230957\n",
      "Epoch 21 / 500 | iteration 10 / 30 | Total Loss: 6.78502082824707 | KNN Loss: 5.733818054199219 | BCE Loss: 1.0512027740478516\n",
      "Epoch 21 / 500 | iteration 15 / 30 | Total Loss: 6.816183090209961 | KNN Loss: 5.759969711303711 | BCE Loss: 1.056213617324829\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 / 500 | iteration 20 / 30 | Total Loss: 6.800716876983643 | KNN Loss: 5.751204490661621 | BCE Loss: 1.0495123863220215\n",
      "Epoch 21 / 500 | iteration 25 / 30 | Total Loss: 6.809625625610352 | KNN Loss: 5.739058017730713 | BCE Loss: 1.0705674886703491\n",
      "Epoch 22 / 500 | iteration 0 / 30 | Total Loss: 6.784581184387207 | KNN Loss: 5.72567081451416 | BCE Loss: 1.0589104890823364\n",
      "Epoch 22 / 500 | iteration 5 / 30 | Total Loss: 6.7950334548950195 | KNN Loss: 5.731474876403809 | BCE Loss: 1.0635583400726318\n",
      "Epoch 22 / 500 | iteration 10 / 30 | Total Loss: 6.8036208152771 | KNN Loss: 5.738285064697266 | BCE Loss: 1.0653358697891235\n",
      "Epoch 22 / 500 | iteration 15 / 30 | Total Loss: 6.782561779022217 | KNN Loss: 5.733485221862793 | BCE Loss: 1.0490766763687134\n",
      "Epoch 22 / 500 | iteration 20 / 30 | Total Loss: 6.798057556152344 | KNN Loss: 5.735341548919678 | BCE Loss: 1.062716007232666\n",
      "Epoch 22 / 500 | iteration 25 / 30 | Total Loss: 6.756974697113037 | KNN Loss: 5.7236127853393555 | BCE Loss: 1.0333620309829712\n",
      "Epoch 23 / 500 | iteration 0 / 30 | Total Loss: 6.747217178344727 | KNN Loss: 5.710287570953369 | BCE Loss: 1.0369293689727783\n",
      "Epoch 23 / 500 | iteration 5 / 30 | Total Loss: 6.760729789733887 | KNN Loss: 5.714742660522461 | BCE Loss: 1.0459871292114258\n",
      "Epoch 23 / 500 | iteration 10 / 30 | Total Loss: 6.742637634277344 | KNN Loss: 5.6998982429504395 | BCE Loss: 1.0427391529083252\n",
      "Epoch 23 / 500 | iteration 15 / 30 | Total Loss: 6.841575622558594 | KNN Loss: 5.738624095916748 | BCE Loss: 1.1029515266418457\n",
      "Epoch 23 / 500 | iteration 20 / 30 | Total Loss: 6.795319080352783 | KNN Loss: 5.727146625518799 | BCE Loss: 1.0681724548339844\n",
      "Epoch 23 / 500 | iteration 25 / 30 | Total Loss: 6.7687859535217285 | KNN Loss: 5.726351737976074 | BCE Loss: 1.0424343347549438\n",
      "Epoch 24 / 500 | iteration 0 / 30 | Total Loss: 6.769206523895264 | KNN Loss: 5.700115203857422 | BCE Loss: 1.0690913200378418\n",
      "Epoch 24 / 500 | iteration 5 / 30 | Total Loss: 6.7366204261779785 | KNN Loss: 5.7047271728515625 | BCE Loss: 1.0318931341171265\n",
      "Epoch 24 / 500 | iteration 10 / 30 | Total Loss: 6.784605026245117 | KNN Loss: 5.711345195770264 | BCE Loss: 1.0732595920562744\n",
      "Epoch 24 / 500 | iteration 15 / 30 | Total Loss: 6.7535834312438965 | KNN Loss: 5.705451488494873 | BCE Loss: 1.0481319427490234\n",
      "Epoch 24 / 500 | iteration 20 / 30 | Total Loss: 6.766683578491211 | KNN Loss: 5.702203273773193 | BCE Loss: 1.0644804239273071\n",
      "Epoch 24 / 500 | iteration 25 / 30 | Total Loss: 6.744718074798584 | KNN Loss: 5.692936420440674 | BCE Loss: 1.0517816543579102\n",
      "Epoch 25 / 500 | iteration 0 / 30 | Total Loss: 6.716497421264648 | KNN Loss: 5.68544864654541 | BCE Loss: 1.0310486555099487\n",
      "Epoch 25 / 500 | iteration 5 / 30 | Total Loss: 6.760808944702148 | KNN Loss: 5.7008376121521 | BCE Loss: 1.0599714517593384\n",
      "Epoch 25 / 500 | iteration 10 / 30 | Total Loss: 6.756351470947266 | KNN Loss: 5.703009128570557 | BCE Loss: 1.053342580795288\n",
      "Epoch 25 / 500 | iteration 15 / 30 | Total Loss: 6.748024940490723 | KNN Loss: 5.6954545974731445 | BCE Loss: 1.0525705814361572\n",
      "Epoch 25 / 500 | iteration 20 / 30 | Total Loss: 6.726481914520264 | KNN Loss: 5.682712554931641 | BCE Loss: 1.0437692403793335\n",
      "Epoch 25 / 500 | iteration 25 / 30 | Total Loss: 6.76664924621582 | KNN Loss: 5.693570137023926 | BCE Loss: 1.0730793476104736\n",
      "Epoch 26 / 500 | iteration 0 / 30 | Total Loss: 6.756479263305664 | KNN Loss: 5.698574066162109 | BCE Loss: 1.0579049587249756\n",
      "Epoch 26 / 500 | iteration 5 / 30 | Total Loss: 6.740745544433594 | KNN Loss: 5.6829376220703125 | BCE Loss: 1.0578079223632812\n",
      "Epoch 26 / 500 | iteration 10 / 30 | Total Loss: 6.832592964172363 | KNN Loss: 5.777488708496094 | BCE Loss: 1.0551040172576904\n",
      "Epoch 26 / 500 | iteration 15 / 30 | Total Loss: 6.741976261138916 | KNN Loss: 5.673658847808838 | BCE Loss: 1.0683174133300781\n",
      "Epoch 26 / 500 | iteration 20 / 30 | Total Loss: 6.77079963684082 | KNN Loss: 5.709309101104736 | BCE Loss: 1.061490535736084\n",
      "Epoch 26 / 500 | iteration 25 / 30 | Total Loss: 6.7440266609191895 | KNN Loss: 5.682314395904541 | BCE Loss: 1.0617122650146484\n",
      "Epoch 27 / 500 | iteration 0 / 30 | Total Loss: 6.733516693115234 | KNN Loss: 5.6750168800354 | BCE Loss: 1.058500051498413\n",
      "Epoch 27 / 500 | iteration 5 / 30 | Total Loss: 6.742747783660889 | KNN Loss: 5.699391841888428 | BCE Loss: 1.043355941772461\n",
      "Epoch 27 / 500 | iteration 10 / 30 | Total Loss: 6.720746040344238 | KNN Loss: 5.684235095977783 | BCE Loss: 1.036510705947876\n",
      "Epoch 27 / 500 | iteration 15 / 30 | Total Loss: 6.737699031829834 | KNN Loss: 5.672571182250977 | BCE Loss: 1.0651278495788574\n",
      "Epoch 27 / 500 | iteration 20 / 30 | Total Loss: 6.731688976287842 | KNN Loss: 5.6740851402282715 | BCE Loss: 1.0576038360595703\n",
      "Epoch 27 / 500 | iteration 25 / 30 | Total Loss: 6.723968029022217 | KNN Loss: 5.666059494018555 | BCE Loss: 1.0579084157943726\n",
      "Epoch 28 / 500 | iteration 0 / 30 | Total Loss: 6.716708183288574 | KNN Loss: 5.675633430480957 | BCE Loss: 1.041074514389038\n",
      "Epoch 28 / 500 | iteration 5 / 30 | Total Loss: 6.71599817276001 | KNN Loss: 5.665658473968506 | BCE Loss: 1.050339698791504\n",
      "Epoch 28 / 500 | iteration 10 / 30 | Total Loss: 6.740729331970215 | KNN Loss: 5.677791595458984 | BCE Loss: 1.0629377365112305\n",
      "Epoch 28 / 500 | iteration 15 / 30 | Total Loss: 6.724411487579346 | KNN Loss: 5.677403450012207 | BCE Loss: 1.0470081567764282\n",
      "Epoch 28 / 500 | iteration 20 / 30 | Total Loss: 6.749275207519531 | KNN Loss: 5.691681861877441 | BCE Loss: 1.0575931072235107\n",
      "Epoch 28 / 500 | iteration 25 / 30 | Total Loss: 6.764280796051025 | KNN Loss: 5.712778568267822 | BCE Loss: 1.0515021085739136\n",
      "Epoch 29 / 500 | iteration 0 / 30 | Total Loss: 6.779500484466553 | KNN Loss: 5.701693534851074 | BCE Loss: 1.077806830406189\n",
      "Epoch 29 / 500 | iteration 5 / 30 | Total Loss: 6.717813968658447 | KNN Loss: 5.667287826538086 | BCE Loss: 1.0505261421203613\n",
      "Epoch 29 / 500 | iteration 10 / 30 | Total Loss: 6.68890380859375 | KNN Loss: 5.65493631362915 | BCE Loss: 1.0339674949645996\n",
      "Epoch 29 / 500 | iteration 15 / 30 | Total Loss: 6.759966850280762 | KNN Loss: 5.694817543029785 | BCE Loss: 1.0651490688323975\n",
      "Epoch 29 / 500 | iteration 20 / 30 | Total Loss: 6.73006010055542 | KNN Loss: 5.6980881690979 | BCE Loss: 1.0319719314575195\n",
      "Epoch 29 / 500 | iteration 25 / 30 | Total Loss: 6.728818893432617 | KNN Loss: 5.658986568450928 | BCE Loss: 1.0698325634002686\n",
      "Epoch 30 / 500 | iteration 0 / 30 | Total Loss: 6.756241798400879 | KNN Loss: 5.6978068351745605 | BCE Loss: 1.0584348440170288\n",
      "Epoch 30 / 500 | iteration 5 / 30 | Total Loss: 6.783492088317871 | KNN Loss: 5.701217174530029 | BCE Loss: 1.0822750329971313\n",
      "Epoch 30 / 500 | iteration 10 / 30 | Total Loss: 6.70184850692749 | KNN Loss: 5.660994052886963 | BCE Loss: 1.0408543348312378\n",
      "Epoch 30 / 500 | iteration 15 / 30 | Total Loss: 6.735182762145996 | KNN Loss: 5.667290687561035 | BCE Loss: 1.06789231300354\n",
      "Epoch 30 / 500 | iteration 20 / 30 | Total Loss: 6.729091167449951 | KNN Loss: 5.658274173736572 | BCE Loss: 1.070816993713379\n",
      "Epoch 30 / 500 | iteration 25 / 30 | Total Loss: 6.688813209533691 | KNN Loss: 5.654160976409912 | BCE Loss: 1.0346521139144897\n",
      "Epoch 31 / 500 | iteration 0 / 30 | Total Loss: 6.719604969024658 | KNN Loss: 5.680520534515381 | BCE Loss: 1.039084553718567\n",
      "Epoch 31 / 500 | iteration 5 / 30 | Total Loss: 6.71076774597168 | KNN Loss: 5.6546950340271 | BCE Loss: 1.0560729503631592\n",
      "Epoch 31 / 500 | iteration 10 / 30 | Total Loss: 6.676331520080566 | KNN Loss: 5.6462836265563965 | BCE Loss: 1.030048131942749\n",
      "Epoch 31 / 500 | iteration 15 / 30 | Total Loss: 6.739919185638428 | KNN Loss: 5.674473762512207 | BCE Loss: 1.0654454231262207\n",
      "Epoch 31 / 500 | iteration 20 / 30 | Total Loss: 6.684823036193848 | KNN Loss: 5.636870861053467 | BCE Loss: 1.04795241355896\n",
      "Epoch 31 / 500 | iteration 25 / 30 | Total Loss: 6.708946228027344 | KNN Loss: 5.638557434082031 | BCE Loss: 1.0703890323638916\n",
      "Epoch 32 / 500 | iteration 0 / 30 | Total Loss: 6.724876880645752 | KNN Loss: 5.652121543884277 | BCE Loss: 1.0727554559707642\n",
      "Epoch 32 / 500 | iteration 5 / 30 | Total Loss: 6.719254970550537 | KNN Loss: 5.658341407775879 | BCE Loss: 1.0609135627746582\n",
      "Epoch 32 / 500 | iteration 10 / 30 | Total Loss: 6.70418119430542 | KNN Loss: 5.657352447509766 | BCE Loss: 1.0468287467956543\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32 / 500 | iteration 15 / 30 | Total Loss: 6.715782642364502 | KNN Loss: 5.660604476928711 | BCE Loss: 1.0551780462265015\n",
      "Epoch 32 / 500 | iteration 20 / 30 | Total Loss: 6.747676849365234 | KNN Loss: 5.711723327636719 | BCE Loss: 1.0359532833099365\n",
      "Epoch 32 / 500 | iteration 25 / 30 | Total Loss: 6.675665378570557 | KNN Loss: 5.6262688636779785 | BCE Loss: 1.0493966341018677\n",
      "Epoch 33 / 500 | iteration 0 / 30 | Total Loss: 6.724010467529297 | KNN Loss: 5.652440547943115 | BCE Loss: 1.0715696811676025\n",
      "Epoch 33 / 500 | iteration 5 / 30 | Total Loss: 6.698491096496582 | KNN Loss: 5.65304708480835 | BCE Loss: 1.0454440116882324\n",
      "Epoch 33 / 500 | iteration 10 / 30 | Total Loss: 6.679124355316162 | KNN Loss: 5.627102851867676 | BCE Loss: 1.0520216226577759\n",
      "Epoch 33 / 500 | iteration 15 / 30 | Total Loss: 6.677981853485107 | KNN Loss: 5.628269672393799 | BCE Loss: 1.0497121810913086\n",
      "Epoch 33 / 500 | iteration 20 / 30 | Total Loss: 6.7041120529174805 | KNN Loss: 5.648830890655518 | BCE Loss: 1.055281162261963\n",
      "Epoch 33 / 500 | iteration 25 / 30 | Total Loss: 6.700533866882324 | KNN Loss: 5.65041971206665 | BCE Loss: 1.050114393234253\n",
      "Epoch 34 / 500 | iteration 0 / 30 | Total Loss: 6.722499370574951 | KNN Loss: 5.66532039642334 | BCE Loss: 1.0571790933609009\n",
      "Epoch 34 / 500 | iteration 5 / 30 | Total Loss: 6.7098565101623535 | KNN Loss: 5.635809898376465 | BCE Loss: 1.0740464925765991\n",
      "Epoch 34 / 500 | iteration 10 / 30 | Total Loss: 6.684755325317383 | KNN Loss: 5.628509044647217 | BCE Loss: 1.056246042251587\n",
      "Epoch 34 / 500 | iteration 15 / 30 | Total Loss: 6.669703960418701 | KNN Loss: 5.6255059242248535 | BCE Loss: 1.0441980361938477\n",
      "Epoch 34 / 500 | iteration 20 / 30 | Total Loss: 6.712924957275391 | KNN Loss: 5.705422878265381 | BCE Loss: 1.0075023174285889\n",
      "Epoch 34 / 500 | iteration 25 / 30 | Total Loss: 6.715019226074219 | KNN Loss: 5.6454644203186035 | BCE Loss: 1.0695548057556152\n",
      "Epoch 35 / 500 | iteration 0 / 30 | Total Loss: 6.692282676696777 | KNN Loss: 5.642867088317871 | BCE Loss: 1.0494155883789062\n",
      "Epoch 35 / 500 | iteration 5 / 30 | Total Loss: 6.720993995666504 | KNN Loss: 5.697450160980225 | BCE Loss: 1.0235440731048584\n",
      "Epoch 35 / 500 | iteration 10 / 30 | Total Loss: 6.6842360496521 | KNN Loss: 5.644593238830566 | BCE Loss: 1.0396429300308228\n",
      "Epoch 35 / 500 | iteration 15 / 30 | Total Loss: 6.6872453689575195 | KNN Loss: 5.627240180969238 | BCE Loss: 1.0600054264068604\n",
      "Epoch 35 / 500 | iteration 20 / 30 | Total Loss: 6.683114528656006 | KNN Loss: 5.622034549713135 | BCE Loss: 1.061079978942871\n",
      "Epoch 35 / 500 | iteration 25 / 30 | Total Loss: 6.686408519744873 | KNN Loss: 5.65070915222168 | BCE Loss: 1.0356993675231934\n",
      "Epoch 36 / 500 | iteration 0 / 30 | Total Loss: 6.74812650680542 | KNN Loss: 5.654562950134277 | BCE Loss: 1.0935635566711426\n",
      "Epoch 36 / 500 | iteration 5 / 30 | Total Loss: 6.802298545837402 | KNN Loss: 5.731348991394043 | BCE Loss: 1.0709497928619385\n",
      "Epoch 36 / 500 | iteration 10 / 30 | Total Loss: 6.699678897857666 | KNN Loss: 5.6206135749816895 | BCE Loss: 1.0790653228759766\n",
      "Epoch 36 / 500 | iteration 15 / 30 | Total Loss: 6.719084739685059 | KNN Loss: 5.685425281524658 | BCE Loss: 1.0336592197418213\n",
      "Epoch 36 / 500 | iteration 20 / 30 | Total Loss: 6.738086700439453 | KNN Loss: 5.70073127746582 | BCE Loss: 1.0373551845550537\n",
      "Epoch 36 / 500 | iteration 25 / 30 | Total Loss: 6.674351215362549 | KNN Loss: 5.631216526031494 | BCE Loss: 1.0431346893310547\n",
      "Epoch 37 / 500 | iteration 0 / 30 | Total Loss: 6.661655426025391 | KNN Loss: 5.611518383026123 | BCE Loss: 1.0501371622085571\n",
      "Epoch 37 / 500 | iteration 5 / 30 | Total Loss: 6.685230731964111 | KNN Loss: 5.642717361450195 | BCE Loss: 1.0425132513046265\n",
      "Epoch 37 / 500 | iteration 10 / 30 | Total Loss: 6.689644813537598 | KNN Loss: 5.645351886749268 | BCE Loss: 1.0442928075790405\n",
      "Epoch 37 / 500 | iteration 15 / 30 | Total Loss: 6.708299160003662 | KNN Loss: 5.644826412200928 | BCE Loss: 1.0634727478027344\n",
      "Epoch 37 / 500 | iteration 20 / 30 | Total Loss: 6.697946071624756 | KNN Loss: 5.650193691253662 | BCE Loss: 1.0477522611618042\n",
      "Epoch 37 / 500 | iteration 25 / 30 | Total Loss: 6.713843822479248 | KNN Loss: 5.672452449798584 | BCE Loss: 1.0413914918899536\n",
      "Epoch 38 / 500 | iteration 0 / 30 | Total Loss: 6.685689926147461 | KNN Loss: 5.625112533569336 | BCE Loss: 1.060577392578125\n",
      "Epoch 38 / 500 | iteration 5 / 30 | Total Loss: 6.7226715087890625 | KNN Loss: 5.652084827423096 | BCE Loss: 1.0705865621566772\n",
      "Epoch 38 / 500 | iteration 10 / 30 | Total Loss: 6.723642826080322 | KNN Loss: 5.6977105140686035 | BCE Loss: 1.0259324312210083\n",
      "Epoch 38 / 500 | iteration 15 / 30 | Total Loss: 6.664551258087158 | KNN Loss: 5.613403797149658 | BCE Loss: 1.0511475801467896\n",
      "Epoch 38 / 500 | iteration 20 / 30 | Total Loss: 6.63620662689209 | KNN Loss: 5.6191887855529785 | BCE Loss: 1.0170178413391113\n",
      "Epoch 38 / 500 | iteration 25 / 30 | Total Loss: 6.672521591186523 | KNN Loss: 5.617550849914551 | BCE Loss: 1.0549707412719727\n",
      "Epoch 39 / 500 | iteration 0 / 30 | Total Loss: 6.680974006652832 | KNN Loss: 5.630259037017822 | BCE Loss: 1.0507149696350098\n",
      "Epoch 39 / 500 | iteration 5 / 30 | Total Loss: 6.706510543823242 | KNN Loss: 5.6843671798706055 | BCE Loss: 1.0221433639526367\n",
      "Epoch 39 / 500 | iteration 10 / 30 | Total Loss: 6.76078987121582 | KNN Loss: 5.680174827575684 | BCE Loss: 1.0806150436401367\n",
      "Epoch 39 / 500 | iteration 15 / 30 | Total Loss: 6.710663795471191 | KNN Loss: 5.619968414306641 | BCE Loss: 1.0906956195831299\n",
      "Epoch 39 / 500 | iteration 20 / 30 | Total Loss: 6.704207897186279 | KNN Loss: 5.679292678833008 | BCE Loss: 1.024915337562561\n",
      "Epoch 39 / 500 | iteration 25 / 30 | Total Loss: 6.698657989501953 | KNN Loss: 5.630959510803223 | BCE Loss: 1.0676982402801514\n",
      "Epoch 40 / 500 | iteration 0 / 30 | Total Loss: 6.670464515686035 | KNN Loss: 5.620186805725098 | BCE Loss: 1.0502774715423584\n",
      "Epoch 40 / 500 | iteration 5 / 30 | Total Loss: 6.722723007202148 | KNN Loss: 5.671139240264893 | BCE Loss: 1.0515836477279663\n",
      "Epoch 40 / 500 | iteration 10 / 30 | Total Loss: 6.687656402587891 | KNN Loss: 5.629064083099365 | BCE Loss: 1.058592438697815\n",
      "Epoch 40 / 500 | iteration 15 / 30 | Total Loss: 6.653898239135742 | KNN Loss: 5.614456653594971 | BCE Loss: 1.0394413471221924\n",
      "Epoch 40 / 500 | iteration 20 / 30 | Total Loss: 6.696573734283447 | KNN Loss: 5.654452800750732 | BCE Loss: 1.0421209335327148\n",
      "Epoch 40 / 500 | iteration 25 / 30 | Total Loss: 6.756667137145996 | KNN Loss: 5.690750598907471 | BCE Loss: 1.0659167766571045\n",
      "Epoch 41 / 500 | iteration 0 / 30 | Total Loss: 6.676388740539551 | KNN Loss: 5.6246747970581055 | BCE Loss: 1.0517139434814453\n",
      "Epoch 41 / 500 | iteration 5 / 30 | Total Loss: 6.663841724395752 | KNN Loss: 5.616746425628662 | BCE Loss: 1.0470952987670898\n",
      "Epoch 41 / 500 | iteration 10 / 30 | Total Loss: 6.786032676696777 | KNN Loss: 5.732092380523682 | BCE Loss: 1.0539400577545166\n",
      "Epoch 41 / 500 | iteration 15 / 30 | Total Loss: 6.682435035705566 | KNN Loss: 5.609078884124756 | BCE Loss: 1.0733559131622314\n",
      "Epoch 41 / 500 | iteration 20 / 30 | Total Loss: 6.719142913818359 | KNN Loss: 5.661947250366211 | BCE Loss: 1.0571956634521484\n",
      "Epoch 41 / 500 | iteration 25 / 30 | Total Loss: 6.703041076660156 | KNN Loss: 5.647324562072754 | BCE Loss: 1.0557163953781128\n",
      "Epoch 42 / 500 | iteration 0 / 30 | Total Loss: 6.678867340087891 | KNN Loss: 5.614012241363525 | BCE Loss: 1.0648553371429443\n",
      "Epoch 42 / 500 | iteration 5 / 30 | Total Loss: 6.742221832275391 | KNN Loss: 5.696681022644043 | BCE Loss: 1.045540690422058\n",
      "Epoch 42 / 500 | iteration 10 / 30 | Total Loss: 6.658344268798828 | KNN Loss: 5.622501373291016 | BCE Loss: 1.035842776298523\n",
      "Epoch 42 / 500 | iteration 15 / 30 | Total Loss: 6.709288120269775 | KNN Loss: 5.657265663146973 | BCE Loss: 1.0520225763320923\n",
      "Epoch 42 / 500 | iteration 20 / 30 | Total Loss: 6.692842483520508 | KNN Loss: 5.655256748199463 | BCE Loss: 1.0375854969024658\n",
      "Epoch 42 / 500 | iteration 25 / 30 | Total Loss: 6.724137306213379 | KNN Loss: 5.669906139373779 | BCE Loss: 1.05423104763031\n",
      "Epoch 43 / 500 | iteration 0 / 30 | Total Loss: 6.715092182159424 | KNN Loss: 5.647237300872803 | BCE Loss: 1.0678550004959106\n",
      "Epoch 43 / 500 | iteration 5 / 30 | Total Loss: 6.689347267150879 | KNN Loss: 5.640155792236328 | BCE Loss: 1.0491914749145508\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43 / 500 | iteration 10 / 30 | Total Loss: 6.689226150512695 | KNN Loss: 5.645828723907471 | BCE Loss: 1.0433974266052246\n",
      "Epoch 43 / 500 | iteration 15 / 30 | Total Loss: 6.715989589691162 | KNN Loss: 5.688199520111084 | BCE Loss: 1.0277901887893677\n",
      "Epoch 43 / 500 | iteration 20 / 30 | Total Loss: 6.751047134399414 | KNN Loss: 5.682516098022461 | BCE Loss: 1.0685312747955322\n",
      "Epoch 43 / 500 | iteration 25 / 30 | Total Loss: 6.694040775299072 | KNN Loss: 5.638171195983887 | BCE Loss: 1.0558695793151855\n",
      "Epoch 44 / 500 | iteration 0 / 30 | Total Loss: 6.681375503540039 | KNN Loss: 5.631553649902344 | BCE Loss: 1.0498217344284058\n",
      "Epoch 44 / 500 | iteration 5 / 30 | Total Loss: 6.676664352416992 | KNN Loss: 5.613317966461182 | BCE Loss: 1.0633466243743896\n",
      "Epoch 44 / 500 | iteration 10 / 30 | Total Loss: 6.730112075805664 | KNN Loss: 5.698916435241699 | BCE Loss: 1.0311956405639648\n",
      "Epoch 44 / 500 | iteration 15 / 30 | Total Loss: 6.651851654052734 | KNN Loss: 5.611672878265381 | BCE Loss: 1.0401790142059326\n",
      "Epoch 44 / 500 | iteration 20 / 30 | Total Loss: 6.6621551513671875 | KNN Loss: 5.6336774826049805 | BCE Loss: 1.0284775495529175\n",
      "Epoch 44 / 500 | iteration 25 / 30 | Total Loss: 6.678088188171387 | KNN Loss: 5.616863250732422 | BCE Loss: 1.0612250566482544\n",
      "Epoch 45 / 500 | iteration 0 / 30 | Total Loss: 6.715817451477051 | KNN Loss: 5.672726631164551 | BCE Loss: 1.0430909395217896\n",
      "Epoch 45 / 500 | iteration 5 / 30 | Total Loss: 6.674997329711914 | KNN Loss: 5.6223955154418945 | BCE Loss: 1.0526018142700195\n",
      "Epoch 45 / 500 | iteration 10 / 30 | Total Loss: 6.670601844787598 | KNN Loss: 5.6222429275512695 | BCE Loss: 1.0483589172363281\n",
      "Epoch 45 / 500 | iteration 15 / 30 | Total Loss: 6.685434341430664 | KNN Loss: 5.666870594024658 | BCE Loss: 1.0185635089874268\n",
      "Epoch 45 / 500 | iteration 20 / 30 | Total Loss: 6.6889753341674805 | KNN Loss: 5.626917362213135 | BCE Loss: 1.0620579719543457\n",
      "Epoch 45 / 500 | iteration 25 / 30 | Total Loss: 6.656247138977051 | KNN Loss: 5.623140335083008 | BCE Loss: 1.033106803894043\n",
      "Epoch 46 / 500 | iteration 0 / 30 | Total Loss: 6.683732032775879 | KNN Loss: 5.620595932006836 | BCE Loss: 1.063136100769043\n",
      "Epoch 46 / 500 | iteration 5 / 30 | Total Loss: 6.764758110046387 | KNN Loss: 5.7273945808410645 | BCE Loss: 1.0373635292053223\n",
      "Epoch 46 / 500 | iteration 10 / 30 | Total Loss: 6.642035007476807 | KNN Loss: 5.625926494598389 | BCE Loss: 1.0161083936691284\n",
      "Epoch 46 / 500 | iteration 15 / 30 | Total Loss: 6.677341461181641 | KNN Loss: 5.65491247177124 | BCE Loss: 1.0224288702011108\n",
      "Epoch 46 / 500 | iteration 20 / 30 | Total Loss: 6.6905412673950195 | KNN Loss: 5.6613922119140625 | BCE Loss: 1.029149055480957\n",
      "Epoch 46 / 500 | iteration 25 / 30 | Total Loss: 6.705109119415283 | KNN Loss: 5.65568208694458 | BCE Loss: 1.0494269132614136\n",
      "Epoch 47 / 500 | iteration 0 / 30 | Total Loss: 6.656323432922363 | KNN Loss: 5.618182182312012 | BCE Loss: 1.0381410121917725\n",
      "Epoch 47 / 500 | iteration 5 / 30 | Total Loss: 6.680290699005127 | KNN Loss: 5.6384992599487305 | BCE Loss: 1.041791319847107\n",
      "Epoch 47 / 500 | iteration 10 / 30 | Total Loss: 6.684985160827637 | KNN Loss: 5.622089385986328 | BCE Loss: 1.0628958940505981\n",
      "Epoch 47 / 500 | iteration 15 / 30 | Total Loss: 6.716300010681152 | KNN Loss: 5.664990425109863 | BCE Loss: 1.0513098239898682\n",
      "Epoch 47 / 500 | iteration 20 / 30 | Total Loss: 6.67912483215332 | KNN Loss: 5.615043640136719 | BCE Loss: 1.0640811920166016\n",
      "Epoch 47 / 500 | iteration 25 / 30 | Total Loss: 6.667153358459473 | KNN Loss: 5.626382350921631 | BCE Loss: 1.0407711267471313\n",
      "Epoch 48 / 500 | iteration 0 / 30 | Total Loss: 6.663792610168457 | KNN Loss: 5.612454891204834 | BCE Loss: 1.051337480545044\n",
      "Epoch 48 / 500 | iteration 5 / 30 | Total Loss: 6.670557498931885 | KNN Loss: 5.630109786987305 | BCE Loss: 1.0404475927352905\n",
      "Epoch 48 / 500 | iteration 10 / 30 | Total Loss: 6.740625381469727 | KNN Loss: 5.666659832000732 | BCE Loss: 1.0739657878875732\n",
      "Epoch 48 / 500 | iteration 15 / 30 | Total Loss: 6.736302375793457 | KNN Loss: 5.67221736907959 | BCE Loss: 1.0640852451324463\n",
      "Epoch 48 / 500 | iteration 20 / 30 | Total Loss: 6.690958023071289 | KNN Loss: 5.636719703674316 | BCE Loss: 1.054238200187683\n",
      "Epoch 48 / 500 | iteration 25 / 30 | Total Loss: 6.7529449462890625 | KNN Loss: 5.7045464515686035 | BCE Loss: 1.0483983755111694\n",
      "Epoch 49 / 500 | iteration 0 / 30 | Total Loss: 6.692876815795898 | KNN Loss: 5.616010665893555 | BCE Loss: 1.0768663883209229\n",
      "Epoch 49 / 500 | iteration 5 / 30 | Total Loss: 6.652833938598633 | KNN Loss: 5.603826999664307 | BCE Loss: 1.0490069389343262\n",
      "Epoch 49 / 500 | iteration 10 / 30 | Total Loss: 6.710846900939941 | KNN Loss: 5.656117916107178 | BCE Loss: 1.0547287464141846\n",
      "Epoch 49 / 500 | iteration 15 / 30 | Total Loss: 6.674602031707764 | KNN Loss: 5.629867076873779 | BCE Loss: 1.0447349548339844\n",
      "Epoch 49 / 500 | iteration 20 / 30 | Total Loss: 6.699049949645996 | KNN Loss: 5.635191917419434 | BCE Loss: 1.0638580322265625\n",
      "Epoch 49 / 500 | iteration 25 / 30 | Total Loss: 6.684451580047607 | KNN Loss: 5.663260459899902 | BCE Loss: 1.021191120147705\n",
      "Epoch 50 / 500 | iteration 0 / 30 | Total Loss: 6.656055450439453 | KNN Loss: 5.614502429962158 | BCE Loss: 1.041553258895874\n",
      "Epoch 50 / 500 | iteration 5 / 30 | Total Loss: 6.673547744750977 | KNN Loss: 5.6216864585876465 | BCE Loss: 1.0518615245819092\n",
      "Epoch 50 / 500 | iteration 10 / 30 | Total Loss: 6.655181407928467 | KNN Loss: 5.617145538330078 | BCE Loss: 1.0380357503890991\n",
      "Epoch 50 / 500 | iteration 15 / 30 | Total Loss: 6.738568305969238 | KNN Loss: 5.701786518096924 | BCE Loss: 1.0367820262908936\n",
      "Epoch 50 / 500 | iteration 20 / 30 | Total Loss: 6.6490912437438965 | KNN Loss: 5.617884159088135 | BCE Loss: 1.0312070846557617\n",
      "Epoch 50 / 500 | iteration 25 / 30 | Total Loss: 6.714697360992432 | KNN Loss: 5.639764308929443 | BCE Loss: 1.0749330520629883\n",
      "Epoch 51 / 500 | iteration 0 / 30 | Total Loss: 6.668331146240234 | KNN Loss: 5.6131911277771 | BCE Loss: 1.0551397800445557\n",
      "Epoch 51 / 500 | iteration 5 / 30 | Total Loss: 6.6979780197143555 | KNN Loss: 5.678783416748047 | BCE Loss: 1.0191948413848877\n",
      "Epoch 51 / 500 | iteration 10 / 30 | Total Loss: 6.649309158325195 | KNN Loss: 5.6125946044921875 | BCE Loss: 1.0367143154144287\n",
      "Epoch 51 / 500 | iteration 15 / 30 | Total Loss: 6.672426223754883 | KNN Loss: 5.623781204223633 | BCE Loss: 1.048644781112671\n",
      "Epoch 51 / 500 | iteration 20 / 30 | Total Loss: 6.678140640258789 | KNN Loss: 5.62606143951416 | BCE Loss: 1.0520793199539185\n",
      "Epoch 51 / 500 | iteration 25 / 30 | Total Loss: 6.642126083374023 | KNN Loss: 5.600303649902344 | BCE Loss: 1.0418224334716797\n",
      "Epoch 52 / 500 | iteration 0 / 30 | Total Loss: 6.668168544769287 | KNN Loss: 5.609569072723389 | BCE Loss: 1.0585994720458984\n",
      "Epoch 52 / 500 | iteration 5 / 30 | Total Loss: 6.67729377746582 | KNN Loss: 5.609339714050293 | BCE Loss: 1.067954182624817\n",
      "Epoch 52 / 500 | iteration 10 / 30 | Total Loss: 6.691803932189941 | KNN Loss: 5.623476028442383 | BCE Loss: 1.0683280229568481\n",
      "Epoch 52 / 500 | iteration 15 / 30 | Total Loss: 6.682607650756836 | KNN Loss: 5.642022132873535 | BCE Loss: 1.0405857563018799\n",
      "Epoch 52 / 500 | iteration 20 / 30 | Total Loss: 6.665379524230957 | KNN Loss: 5.615695953369141 | BCE Loss: 1.049683690071106\n",
      "Epoch 52 / 500 | iteration 25 / 30 | Total Loss: 6.638016700744629 | KNN Loss: 5.608108997344971 | BCE Loss: 1.0299077033996582\n",
      "Epoch 53 / 500 | iteration 0 / 30 | Total Loss: 6.668642997741699 | KNN Loss: 5.615748405456543 | BCE Loss: 1.0528945922851562\n",
      "Epoch 53 / 500 | iteration 5 / 30 | Total Loss: 6.663453102111816 | KNN Loss: 5.656673908233643 | BCE Loss: 1.0067789554595947\n",
      "Epoch 53 / 500 | iteration 10 / 30 | Total Loss: 6.639267921447754 | KNN Loss: 5.609749794006348 | BCE Loss: 1.0295181274414062\n",
      "Epoch 53 / 500 | iteration 15 / 30 | Total Loss: 6.709259033203125 | KNN Loss: 5.662583827972412 | BCE Loss: 1.0466750860214233\n",
      "Epoch 53 / 500 | iteration 20 / 30 | Total Loss: 6.731982231140137 | KNN Loss: 5.676746845245361 | BCE Loss: 1.0552353858947754\n",
      "Epoch 53 / 500 | iteration 25 / 30 | Total Loss: 6.706532001495361 | KNN Loss: 5.684896469116211 | BCE Loss: 1.0216355323791504\n",
      "Epoch 54 / 500 | iteration 0 / 30 | Total Loss: 6.7177324295043945 | KNN Loss: 5.648348808288574 | BCE Loss: 1.0693833827972412\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54 / 500 | iteration 5 / 30 | Total Loss: 6.7017822265625 | KNN Loss: 5.63620662689209 | BCE Loss: 1.0655755996704102\n",
      "Epoch 54 / 500 | iteration 10 / 30 | Total Loss: 6.650291442871094 | KNN Loss: 5.609005451202393 | BCE Loss: 1.0412859916687012\n",
      "Epoch 54 / 500 | iteration 15 / 30 | Total Loss: 6.724653720855713 | KNN Loss: 5.651760101318359 | BCE Loss: 1.072893738746643\n",
      "Epoch 54 / 500 | iteration 20 / 30 | Total Loss: 6.634978294372559 | KNN Loss: 5.607069969177246 | BCE Loss: 1.0279083251953125\n",
      "Epoch 54 / 500 | iteration 25 / 30 | Total Loss: 6.656762599945068 | KNN Loss: 5.621818542480469 | BCE Loss: 1.0349440574645996\n",
      "Epoch 55 / 500 | iteration 0 / 30 | Total Loss: 6.708695411682129 | KNN Loss: 5.6456146240234375 | BCE Loss: 1.0630805492401123\n",
      "Epoch 55 / 500 | iteration 5 / 30 | Total Loss: 6.669973373413086 | KNN Loss: 5.634293556213379 | BCE Loss: 1.0356799364089966\n",
      "Epoch 55 / 500 | iteration 10 / 30 | Total Loss: 6.646852493286133 | KNN Loss: 5.611717224121094 | BCE Loss: 1.0351355075836182\n",
      "Epoch 55 / 500 | iteration 15 / 30 | Total Loss: 6.773627758026123 | KNN Loss: 5.686547756195068 | BCE Loss: 1.0870798826217651\n",
      "Epoch 55 / 500 | iteration 20 / 30 | Total Loss: 6.687054634094238 | KNN Loss: 5.623520374298096 | BCE Loss: 1.0635342597961426\n",
      "Epoch 55 / 500 | iteration 25 / 30 | Total Loss: 6.678112030029297 | KNN Loss: 5.614709854125977 | BCE Loss: 1.0634019374847412\n",
      "Epoch 56 / 500 | iteration 0 / 30 | Total Loss: 6.640612602233887 | KNN Loss: 5.61774206161499 | BCE Loss: 1.0228707790374756\n",
      "Epoch 56 / 500 | iteration 5 / 30 | Total Loss: 6.633088111877441 | KNN Loss: 5.607877254486084 | BCE Loss: 1.0252110958099365\n",
      "Epoch 56 / 500 | iteration 10 / 30 | Total Loss: 6.64985466003418 | KNN Loss: 5.602989673614502 | BCE Loss: 1.0468647480010986\n",
      "Epoch 56 / 500 | iteration 15 / 30 | Total Loss: 6.656630039215088 | KNN Loss: 5.617842674255371 | BCE Loss: 1.0387873649597168\n",
      "Epoch 56 / 500 | iteration 20 / 30 | Total Loss: 6.696846008300781 | KNN Loss: 5.633437633514404 | BCE Loss: 1.0634081363677979\n",
      "Epoch 56 / 500 | iteration 25 / 30 | Total Loss: 6.65157413482666 | KNN Loss: 5.607194423675537 | BCE Loss: 1.0443795919418335\n",
      "Epoch 57 / 500 | iteration 0 / 30 | Total Loss: 6.621910095214844 | KNN Loss: 5.606674671173096 | BCE Loss: 1.0152356624603271\n",
      "Epoch 57 / 500 | iteration 5 / 30 | Total Loss: 6.689671039581299 | KNN Loss: 5.642354488372803 | BCE Loss: 1.047316551208496\n",
      "Epoch 57 / 500 | iteration 10 / 30 | Total Loss: 6.700515270233154 | KNN Loss: 5.63673210144043 | BCE Loss: 1.0637831687927246\n",
      "Epoch 57 / 500 | iteration 15 / 30 | Total Loss: 6.6729607582092285 | KNN Loss: 5.6243577003479 | BCE Loss: 1.0486030578613281\n",
      "Epoch 57 / 500 | iteration 20 / 30 | Total Loss: 6.672738075256348 | KNN Loss: 5.597123622894287 | BCE Loss: 1.0756142139434814\n",
      "Epoch 57 / 500 | iteration 25 / 30 | Total Loss: 6.685857772827148 | KNN Loss: 5.60899543762207 | BCE Loss: 1.0768625736236572\n",
      "Epoch 58 / 500 | iteration 0 / 30 | Total Loss: 6.727103233337402 | KNN Loss: 5.698419570922852 | BCE Loss: 1.0286836624145508\n",
      "Epoch 58 / 500 | iteration 5 / 30 | Total Loss: 6.690188407897949 | KNN Loss: 5.65107536315918 | BCE Loss: 1.039113163948059\n",
      "Epoch 58 / 500 | iteration 10 / 30 | Total Loss: 6.636709213256836 | KNN Loss: 5.600347518920898 | BCE Loss: 1.0363614559173584\n",
      "Epoch 58 / 500 | iteration 15 / 30 | Total Loss: 6.704155921936035 | KNN Loss: 5.645003795623779 | BCE Loss: 1.0591521263122559\n",
      "Epoch 58 / 500 | iteration 20 / 30 | Total Loss: 6.688654899597168 | KNN Loss: 5.60141658782959 | BCE Loss: 1.0872383117675781\n",
      "Epoch 58 / 500 | iteration 25 / 30 | Total Loss: 6.791440486907959 | KNN Loss: 5.755487442016602 | BCE Loss: 1.0359529256820679\n",
      "Epoch 59 / 500 | iteration 0 / 30 | Total Loss: 6.65877103805542 | KNN Loss: 5.615679740905762 | BCE Loss: 1.0430912971496582\n",
      "Epoch 59 / 500 | iteration 5 / 30 | Total Loss: 6.687877178192139 | KNN Loss: 5.616097927093506 | BCE Loss: 1.0717793703079224\n",
      "Epoch 59 / 500 | iteration 10 / 30 | Total Loss: 6.729644775390625 | KNN Loss: 5.6632843017578125 | BCE Loss: 1.0663604736328125\n",
      "Epoch 59 / 500 | iteration 15 / 30 | Total Loss: 6.724964141845703 | KNN Loss: 5.702108383178711 | BCE Loss: 1.022855520248413\n",
      "Epoch 59 / 500 | iteration 20 / 30 | Total Loss: 6.655711650848389 | KNN Loss: 5.613338947296143 | BCE Loss: 1.042372703552246\n",
      "Epoch 59 / 500 | iteration 25 / 30 | Total Loss: 6.727774620056152 | KNN Loss: 5.664475440979004 | BCE Loss: 1.0632991790771484\n",
      "Epoch 60 / 500 | iteration 0 / 30 | Total Loss: 6.678157806396484 | KNN Loss: 5.611912250518799 | BCE Loss: 1.0662455558776855\n",
      "Epoch 60 / 500 | iteration 5 / 30 | Total Loss: 6.631145477294922 | KNN Loss: 5.602290630340576 | BCE Loss: 1.0288548469543457\n",
      "Epoch 60 / 500 | iteration 10 / 30 | Total Loss: 6.720292568206787 | KNN Loss: 5.650064945220947 | BCE Loss: 1.0702276229858398\n",
      "Epoch 60 / 500 | iteration 15 / 30 | Total Loss: 6.659054756164551 | KNN Loss: 5.608929634094238 | BCE Loss: 1.050125002861023\n",
      "Epoch 60 / 500 | iteration 20 / 30 | Total Loss: 6.728451251983643 | KNN Loss: 5.68778133392334 | BCE Loss: 1.0406699180603027\n",
      "Epoch 60 / 500 | iteration 25 / 30 | Total Loss: 6.6615190505981445 | KNN Loss: 5.6139044761657715 | BCE Loss: 1.0476146936416626\n",
      "Epoch 61 / 500 | iteration 0 / 30 | Total Loss: 6.6569414138793945 | KNN Loss: 5.606762409210205 | BCE Loss: 1.0501792430877686\n",
      "Epoch 61 / 500 | iteration 5 / 30 | Total Loss: 6.670064926147461 | KNN Loss: 5.6090407371521 | BCE Loss: 1.0610244274139404\n",
      "Epoch 61 / 500 | iteration 10 / 30 | Total Loss: 6.687631607055664 | KNN Loss: 5.649338722229004 | BCE Loss: 1.038292646408081\n",
      "Epoch 61 / 500 | iteration 15 / 30 | Total Loss: 6.656941890716553 | KNN Loss: 5.612944602966309 | BCE Loss: 1.0439972877502441\n",
      "Epoch 61 / 500 | iteration 20 / 30 | Total Loss: 6.660041332244873 | KNN Loss: 5.610685348510742 | BCE Loss: 1.0493559837341309\n",
      "Epoch 61 / 500 | iteration 25 / 30 | Total Loss: 6.674258708953857 | KNN Loss: 5.6526007652282715 | BCE Loss: 1.0216578245162964\n",
      "Epoch 62 / 500 | iteration 0 / 30 | Total Loss: 6.6482720375061035 | KNN Loss: 5.616896152496338 | BCE Loss: 1.0313760042190552\n",
      "Epoch 62 / 500 | iteration 5 / 30 | Total Loss: 6.7633490562438965 | KNN Loss: 5.693777084350586 | BCE Loss: 1.0695719718933105\n",
      "Epoch 62 / 500 | iteration 10 / 30 | Total Loss: 6.7478461265563965 | KNN Loss: 5.709827423095703 | BCE Loss: 1.038018822669983\n",
      "Epoch 62 / 500 | iteration 15 / 30 | Total Loss: 6.668491840362549 | KNN Loss: 5.612650394439697 | BCE Loss: 1.055841326713562\n",
      "Epoch 62 / 500 | iteration 20 / 30 | Total Loss: 6.689565658569336 | KNN Loss: 5.637396812438965 | BCE Loss: 1.0521690845489502\n",
      "Epoch 62 / 500 | iteration 25 / 30 | Total Loss: 6.678857803344727 | KNN Loss: 5.618236064910889 | BCE Loss: 1.060621738433838\n",
      "Epoch 63 / 500 | iteration 0 / 30 | Total Loss: 6.677223205566406 | KNN Loss: 5.638894557952881 | BCE Loss: 1.0383285284042358\n",
      "Epoch 63 / 500 | iteration 5 / 30 | Total Loss: 6.672618865966797 | KNN Loss: 5.623444080352783 | BCE Loss: 1.0491747856140137\n",
      "Epoch 63 / 500 | iteration 10 / 30 | Total Loss: 6.690925598144531 | KNN Loss: 5.630688667297363 | BCE Loss: 1.060237169265747\n",
      "Epoch 63 / 500 | iteration 15 / 30 | Total Loss: 6.682024955749512 | KNN Loss: 5.629355430603027 | BCE Loss: 1.0526697635650635\n",
      "Epoch 63 / 500 | iteration 20 / 30 | Total Loss: 6.68510627746582 | KNN Loss: 5.63518762588501 | BCE Loss: 1.0499184131622314\n",
      "Epoch 63 / 500 | iteration 25 / 30 | Total Loss: 6.6999711990356445 | KNN Loss: 5.642121315002441 | BCE Loss: 1.0578498840332031\n",
      "Epoch 64 / 500 | iteration 0 / 30 | Total Loss: 6.736085891723633 | KNN Loss: 5.663289546966553 | BCE Loss: 1.072796106338501\n",
      "Epoch 64 / 500 | iteration 5 / 30 | Total Loss: 6.674338340759277 | KNN Loss: 5.625223636627197 | BCE Loss: 1.0491145849227905\n",
      "Epoch 64 / 500 | iteration 10 / 30 | Total Loss: 6.6952667236328125 | KNN Loss: 5.669665336608887 | BCE Loss: 1.0256011486053467\n",
      "Epoch 64 / 500 | iteration 15 / 30 | Total Loss: 6.669024467468262 | KNN Loss: 5.616941928863525 | BCE Loss: 1.0520825386047363\n",
      "Epoch 64 / 500 | iteration 20 / 30 | Total Loss: 6.688965320587158 | KNN Loss: 5.638629913330078 | BCE Loss: 1.05033540725708\n",
      "Epoch 64 / 500 | iteration 25 / 30 | Total Loss: 6.690296173095703 | KNN Loss: 5.658356189727783 | BCE Loss: 1.0319401025772095\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65 / 500 | iteration 0 / 30 | Total Loss: 6.736957550048828 | KNN Loss: 5.690012454986572 | BCE Loss: 1.046945333480835\n",
      "Epoch 65 / 500 | iteration 5 / 30 | Total Loss: 6.6618971824646 | KNN Loss: 5.604918003082275 | BCE Loss: 1.0569792985916138\n",
      "Epoch 65 / 500 | iteration 10 / 30 | Total Loss: 6.634238243103027 | KNN Loss: 5.608509540557861 | BCE Loss: 1.0257289409637451\n",
      "Epoch 65 / 500 | iteration 15 / 30 | Total Loss: 6.633997440338135 | KNN Loss: 5.595698356628418 | BCE Loss: 1.0382990837097168\n",
      "Epoch 65 / 500 | iteration 20 / 30 | Total Loss: 6.66232967376709 | KNN Loss: 5.597160816192627 | BCE Loss: 1.0651686191558838\n",
      "Epoch 65 / 500 | iteration 25 / 30 | Total Loss: 6.634922504425049 | KNN Loss: 5.600239276885986 | BCE Loss: 1.0346832275390625\n",
      "Epoch 66 / 500 | iteration 0 / 30 | Total Loss: 6.635773658752441 | KNN Loss: 5.593874931335449 | BCE Loss: 1.0418987274169922\n",
      "Epoch 66 / 500 | iteration 5 / 30 | Total Loss: 6.655123710632324 | KNN Loss: 5.616445541381836 | BCE Loss: 1.0386781692504883\n",
      "Epoch 66 / 500 | iteration 10 / 30 | Total Loss: 6.652120590209961 | KNN Loss: 5.602291107177734 | BCE Loss: 1.049829363822937\n",
      "Epoch 66 / 500 | iteration 15 / 30 | Total Loss: 6.686343193054199 | KNN Loss: 5.605831623077393 | BCE Loss: 1.0805115699768066\n",
      "Epoch 66 / 500 | iteration 20 / 30 | Total Loss: 6.68973970413208 | KNN Loss: 5.6464033126831055 | BCE Loss: 1.043336272239685\n",
      "Epoch 66 / 500 | iteration 25 / 30 | Total Loss: 6.6654953956604 | KNN Loss: 5.60230016708374 | BCE Loss: 1.0631952285766602\n",
      "Epoch 67 / 500 | iteration 0 / 30 | Total Loss: 6.696432113647461 | KNN Loss: 5.650796890258789 | BCE Loss: 1.045635461807251\n",
      "Epoch 67 / 500 | iteration 5 / 30 | Total Loss: 6.660672187805176 | KNN Loss: 5.61083984375 | BCE Loss: 1.0498321056365967\n",
      "Epoch 67 / 500 | iteration 10 / 30 | Total Loss: 6.688971996307373 | KNN Loss: 5.648765563964844 | BCE Loss: 1.0402063131332397\n",
      "Epoch 67 / 500 | iteration 15 / 30 | Total Loss: 6.695136070251465 | KNN Loss: 5.61010217666626 | BCE Loss: 1.085033893585205\n",
      "Epoch 67 / 500 | iteration 20 / 30 | Total Loss: 6.653876304626465 | KNN Loss: 5.6125946044921875 | BCE Loss: 1.0412819385528564\n",
      "Epoch 67 / 500 | iteration 25 / 30 | Total Loss: 6.672883987426758 | KNN Loss: 5.6302876472473145 | BCE Loss: 1.0425963401794434\n",
      "Epoch 68 / 500 | iteration 0 / 30 | Total Loss: 6.702425003051758 | KNN Loss: 5.637526988983154 | BCE Loss: 1.064898133277893\n",
      "Epoch 68 / 500 | iteration 5 / 30 | Total Loss: 6.663052082061768 | KNN Loss: 5.612385272979736 | BCE Loss: 1.0506666898727417\n",
      "Epoch 68 / 500 | iteration 10 / 30 | Total Loss: 6.651309967041016 | KNN Loss: 5.607838153839111 | BCE Loss: 1.0434718132019043\n",
      "Epoch 68 / 500 | iteration 15 / 30 | Total Loss: 6.651468276977539 | KNN Loss: 5.602649211883545 | BCE Loss: 1.0488190650939941\n",
      "Epoch 68 / 500 | iteration 20 / 30 | Total Loss: 6.662164211273193 | KNN Loss: 5.612570285797119 | BCE Loss: 1.0495940446853638\n",
      "Epoch 68 / 500 | iteration 25 / 30 | Total Loss: 6.715978145599365 | KNN Loss: 5.676827430725098 | BCE Loss: 1.0391507148742676\n",
      "Epoch    69: reducing learning rate of group 0 to 3.5000e-03.\n",
      "Epoch 69 / 500 | iteration 0 / 30 | Total Loss: 6.75860595703125 | KNN Loss: 5.697692394256592 | BCE Loss: 1.0609138011932373\n",
      "Epoch 69 / 500 | iteration 5 / 30 | Total Loss: 6.669630527496338 | KNN Loss: 5.649661540985107 | BCE Loss: 1.01996910572052\n",
      "Epoch 69 / 500 | iteration 10 / 30 | Total Loss: 6.7082133293151855 | KNN Loss: 5.665321350097656 | BCE Loss: 1.0428918600082397\n",
      "Epoch 69 / 500 | iteration 15 / 30 | Total Loss: 6.761153221130371 | KNN Loss: 5.666987419128418 | BCE Loss: 1.0941660404205322\n",
      "Epoch 69 / 500 | iteration 20 / 30 | Total Loss: 6.670528411865234 | KNN Loss: 5.618206977844238 | BCE Loss: 1.052321434020996\n",
      "Epoch 69 / 500 | iteration 25 / 30 | Total Loss: 6.707660675048828 | KNN Loss: 5.65501070022583 | BCE Loss: 1.0526502132415771\n",
      "Epoch 70 / 500 | iteration 0 / 30 | Total Loss: 6.679474830627441 | KNN Loss: 5.638181209564209 | BCE Loss: 1.0412938594818115\n",
      "Epoch 70 / 500 | iteration 5 / 30 | Total Loss: 6.69936990737915 | KNN Loss: 5.656397342681885 | BCE Loss: 1.0429725646972656\n",
      "Epoch 70 / 500 | iteration 10 / 30 | Total Loss: 6.660993576049805 | KNN Loss: 5.60409688949585 | BCE Loss: 1.0568968057632446\n",
      "Epoch 70 / 500 | iteration 15 / 30 | Total Loss: 6.651247978210449 | KNN Loss: 5.618271827697754 | BCE Loss: 1.0329760313034058\n",
      "Epoch 70 / 500 | iteration 20 / 30 | Total Loss: 6.667569637298584 | KNN Loss: 5.638711929321289 | BCE Loss: 1.028857707977295\n",
      "Epoch 70 / 500 | iteration 25 / 30 | Total Loss: 6.638771057128906 | KNN Loss: 5.596398830413818 | BCE Loss: 1.0423719882965088\n",
      "Epoch 71 / 500 | iteration 0 / 30 | Total Loss: 6.712662696838379 | KNN Loss: 5.664087295532227 | BCE Loss: 1.0485756397247314\n",
      "Epoch 71 / 500 | iteration 5 / 30 | Total Loss: 6.673589706420898 | KNN Loss: 5.631641864776611 | BCE Loss: 1.041947603225708\n",
      "Epoch 71 / 500 | iteration 10 / 30 | Total Loss: 6.700516700744629 | KNN Loss: 5.666996002197266 | BCE Loss: 1.0335206985473633\n",
      "Epoch 71 / 500 | iteration 15 / 30 | Total Loss: 6.738299369812012 | KNN Loss: 5.701698303222656 | BCE Loss: 1.0366008281707764\n",
      "Epoch 71 / 500 | iteration 20 / 30 | Total Loss: 6.723304748535156 | KNN Loss: 5.6540141105651855 | BCE Loss: 1.0692903995513916\n",
      "Epoch 71 / 500 | iteration 25 / 30 | Total Loss: 6.702244758605957 | KNN Loss: 5.630340576171875 | BCE Loss: 1.071904182434082\n",
      "Epoch 72 / 500 | iteration 0 / 30 | Total Loss: 6.645475387573242 | KNN Loss: 5.593621253967285 | BCE Loss: 1.051854133605957\n",
      "Epoch 72 / 500 | iteration 5 / 30 | Total Loss: 6.660148620605469 | KNN Loss: 5.60693883895874 | BCE Loss: 1.0532097816467285\n",
      "Epoch 72 / 500 | iteration 10 / 30 | Total Loss: 6.617988109588623 | KNN Loss: 5.6085028648376465 | BCE Loss: 1.0094852447509766\n",
      "Epoch 72 / 500 | iteration 15 / 30 | Total Loss: 6.708442687988281 | KNN Loss: 5.670896530151367 | BCE Loss: 1.037546157836914\n",
      "Epoch 72 / 500 | iteration 20 / 30 | Total Loss: 6.703983783721924 | KNN Loss: 5.643089294433594 | BCE Loss: 1.06089448928833\n",
      "Epoch 72 / 500 | iteration 25 / 30 | Total Loss: 6.702268600463867 | KNN Loss: 5.61899471282959 | BCE Loss: 1.0832737684249878\n",
      "Epoch 73 / 500 | iteration 0 / 30 | Total Loss: 6.684292793273926 | KNN Loss: 5.649109363555908 | BCE Loss: 1.0351836681365967\n",
      "Epoch 73 / 500 | iteration 5 / 30 | Total Loss: 6.675930976867676 | KNN Loss: 5.609583854675293 | BCE Loss: 1.0663470029830933\n",
      "Epoch 73 / 500 | iteration 10 / 30 | Total Loss: 6.6834330558776855 | KNN Loss: 5.641721725463867 | BCE Loss: 1.041711449623108\n",
      "Epoch 73 / 500 | iteration 15 / 30 | Total Loss: 6.670258522033691 | KNN Loss: 5.612658977508545 | BCE Loss: 1.0575997829437256\n",
      "Epoch 73 / 500 | iteration 20 / 30 | Total Loss: 6.643405914306641 | KNN Loss: 5.606078147888184 | BCE Loss: 1.0373280048370361\n",
      "Epoch 73 / 500 | iteration 25 / 30 | Total Loss: 6.653184413909912 | KNN Loss: 5.623844623565674 | BCE Loss: 1.0293399095535278\n",
      "Epoch 74 / 500 | iteration 0 / 30 | Total Loss: 6.686282157897949 | KNN Loss: 5.640117168426514 | BCE Loss: 1.0461652278900146\n",
      "Epoch 74 / 500 | iteration 5 / 30 | Total Loss: 6.703235149383545 | KNN Loss: 5.666257381439209 | BCE Loss: 1.0369778871536255\n",
      "Epoch 74 / 500 | iteration 10 / 30 | Total Loss: 6.719070911407471 | KNN Loss: 5.69480562210083 | BCE Loss: 1.0242652893066406\n",
      "Epoch 74 / 500 | iteration 15 / 30 | Total Loss: 6.6971435546875 | KNN Loss: 5.61590576171875 | BCE Loss: 1.0812379121780396\n",
      "Epoch 74 / 500 | iteration 20 / 30 | Total Loss: 6.658053398132324 | KNN Loss: 5.605117321014404 | BCE Loss: 1.0529358386993408\n",
      "Epoch 74 / 500 | iteration 25 / 30 | Total Loss: 6.685481548309326 | KNN Loss: 5.666743278503418 | BCE Loss: 1.0187383890151978\n",
      "Epoch 75 / 500 | iteration 0 / 30 | Total Loss: 6.667527198791504 | KNN Loss: 5.604214668273926 | BCE Loss: 1.063312292098999\n",
      "Epoch 75 / 500 | iteration 5 / 30 | Total Loss: 6.662744522094727 | KNN Loss: 5.597705364227295 | BCE Loss: 1.065039038658142\n",
      "Epoch 75 / 500 | iteration 10 / 30 | Total Loss: 6.678262233734131 | KNN Loss: 5.622873783111572 | BCE Loss: 1.0553885698318481\n",
      "Epoch 75 / 500 | iteration 15 / 30 | Total Loss: 6.649163246154785 | KNN Loss: 5.602844715118408 | BCE Loss: 1.0463184118270874\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75 / 500 | iteration 20 / 30 | Total Loss: 6.667660713195801 | KNN Loss: 5.6298017501831055 | BCE Loss: 1.0378589630126953\n",
      "Epoch 75 / 500 | iteration 25 / 30 | Total Loss: 6.658428192138672 | KNN Loss: 5.603091716766357 | BCE Loss: 1.0553362369537354\n",
      "Epoch 76 / 500 | iteration 0 / 30 | Total Loss: 6.694732666015625 | KNN Loss: 5.659072399139404 | BCE Loss: 1.0356600284576416\n",
      "Epoch 76 / 500 | iteration 5 / 30 | Total Loss: 6.6935133934021 | KNN Loss: 5.621359825134277 | BCE Loss: 1.0721535682678223\n",
      "Epoch 76 / 500 | iteration 10 / 30 | Total Loss: 6.727113723754883 | KNN Loss: 5.693207740783691 | BCE Loss: 1.0339057445526123\n",
      "Epoch 76 / 500 | iteration 15 / 30 | Total Loss: 6.6981401443481445 | KNN Loss: 5.662759780883789 | BCE Loss: 1.0353806018829346\n",
      "Epoch 76 / 500 | iteration 20 / 30 | Total Loss: 6.658168315887451 | KNN Loss: 5.62257194519043 | BCE Loss: 1.035596489906311\n",
      "Epoch 76 / 500 | iteration 25 / 30 | Total Loss: 6.688642978668213 | KNN Loss: 5.671849727630615 | BCE Loss: 1.0167932510375977\n",
      "Epoch 77 / 500 | iteration 0 / 30 | Total Loss: 6.750384330749512 | KNN Loss: 5.692246913909912 | BCE Loss: 1.0581371784210205\n",
      "Epoch 77 / 500 | iteration 5 / 30 | Total Loss: 6.668617248535156 | KNN Loss: 5.620663642883301 | BCE Loss: 1.0479536056518555\n",
      "Epoch 77 / 500 | iteration 10 / 30 | Total Loss: 6.723141670227051 | KNN Loss: 5.661481857299805 | BCE Loss: 1.0616600513458252\n",
      "Epoch 77 / 500 | iteration 15 / 30 | Total Loss: 6.69132137298584 | KNN Loss: 5.594601631164551 | BCE Loss: 1.09671950340271\n",
      "Epoch 77 / 500 | iteration 20 / 30 | Total Loss: 6.674005508422852 | KNN Loss: 5.613577842712402 | BCE Loss: 1.0604274272918701\n",
      "Epoch 77 / 500 | iteration 25 / 30 | Total Loss: 6.684618949890137 | KNN Loss: 5.604994773864746 | BCE Loss: 1.0796244144439697\n",
      "Epoch 78 / 500 | iteration 0 / 30 | Total Loss: 6.702398300170898 | KNN Loss: 5.63313627243042 | BCE Loss: 1.069262146949768\n",
      "Epoch 78 / 500 | iteration 5 / 30 | Total Loss: 6.652073860168457 | KNN Loss: 5.606168270111084 | BCE Loss: 1.045905590057373\n",
      "Epoch 78 / 500 | iteration 10 / 30 | Total Loss: 6.694021224975586 | KNN Loss: 5.634852886199951 | BCE Loss: 1.0591681003570557\n",
      "Epoch 78 / 500 | iteration 15 / 30 | Total Loss: 6.687864303588867 | KNN Loss: 5.594873905181885 | BCE Loss: 1.0929906368255615\n",
      "Epoch 78 / 500 | iteration 20 / 30 | Total Loss: 6.7101569175720215 | KNN Loss: 5.654949188232422 | BCE Loss: 1.0552077293395996\n",
      "Epoch 78 / 500 | iteration 25 / 30 | Total Loss: 6.660810947418213 | KNN Loss: 5.602663040161133 | BCE Loss: 1.05814790725708\n",
      "Epoch 79 / 500 | iteration 0 / 30 | Total Loss: 6.634167194366455 | KNN Loss: 5.60558557510376 | BCE Loss: 1.0285817384719849\n",
      "Epoch 79 / 500 | iteration 5 / 30 | Total Loss: 6.65792179107666 | KNN Loss: 5.606305122375488 | BCE Loss: 1.0516166687011719\n",
      "Epoch 79 / 500 | iteration 10 / 30 | Total Loss: 6.659348011016846 | KNN Loss: 5.593010425567627 | BCE Loss: 1.0663375854492188\n",
      "Epoch 79 / 500 | iteration 15 / 30 | Total Loss: 6.6801276206970215 | KNN Loss: 5.63110876083374 | BCE Loss: 1.0490189790725708\n",
      "Epoch 79 / 500 | iteration 20 / 30 | Total Loss: 6.679404258728027 | KNN Loss: 5.613218784332275 | BCE Loss: 1.0661855936050415\n",
      "Epoch 79 / 500 | iteration 25 / 30 | Total Loss: 6.6799726486206055 | KNN Loss: 5.61563777923584 | BCE Loss: 1.064334750175476\n",
      "Epoch 80 / 500 | iteration 0 / 30 | Total Loss: 6.679985046386719 | KNN Loss: 5.62438440322876 | BCE Loss: 1.0556004047393799\n",
      "Epoch 80 / 500 | iteration 5 / 30 | Total Loss: 6.671238899230957 | KNN Loss: 5.6344428062438965 | BCE Loss: 1.0367960929870605\n",
      "Epoch 80 / 500 | iteration 10 / 30 | Total Loss: 6.632183074951172 | KNN Loss: 5.595696449279785 | BCE Loss: 1.0364863872528076\n",
      "Epoch 80 / 500 | iteration 15 / 30 | Total Loss: 6.656185150146484 | KNN Loss: 5.611237049102783 | BCE Loss: 1.0449483394622803\n",
      "Epoch 80 / 500 | iteration 20 / 30 | Total Loss: 6.655812740325928 | KNN Loss: 5.618686199188232 | BCE Loss: 1.0371264219284058\n",
      "Epoch 80 / 500 | iteration 25 / 30 | Total Loss: 6.6817779541015625 | KNN Loss: 5.601602077484131 | BCE Loss: 1.0801761150360107\n",
      "Epoch 81 / 500 | iteration 0 / 30 | Total Loss: 6.723267078399658 | KNN Loss: 5.656976699829102 | BCE Loss: 1.0662903785705566\n",
      "Epoch 81 / 500 | iteration 5 / 30 | Total Loss: 6.669702529907227 | KNN Loss: 5.614542007446289 | BCE Loss: 1.0551605224609375\n",
      "Epoch 81 / 500 | iteration 10 / 30 | Total Loss: 6.657905578613281 | KNN Loss: 5.604276657104492 | BCE Loss: 1.0536290407180786\n",
      "Epoch 81 / 500 | iteration 15 / 30 | Total Loss: 6.670932769775391 | KNN Loss: 5.622595310211182 | BCE Loss: 1.048337697982788\n",
      "Epoch 81 / 500 | iteration 20 / 30 | Total Loss: 6.658039093017578 | KNN Loss: 5.629054069519043 | BCE Loss: 1.028984785079956\n",
      "Epoch 81 / 500 | iteration 25 / 30 | Total Loss: 6.708725929260254 | KNN Loss: 5.641231536865234 | BCE Loss: 1.06749427318573\n",
      "Epoch 82 / 500 | iteration 0 / 30 | Total Loss: 6.668791770935059 | KNN Loss: 5.639626979827881 | BCE Loss: 1.0291646718978882\n",
      "Epoch 82 / 500 | iteration 5 / 30 | Total Loss: 6.767150402069092 | KNN Loss: 5.721197605133057 | BCE Loss: 1.0459526777267456\n",
      "Epoch 82 / 500 | iteration 10 / 30 | Total Loss: 6.642794609069824 | KNN Loss: 5.633073806762695 | BCE Loss: 1.0097205638885498\n",
      "Epoch 82 / 500 | iteration 15 / 30 | Total Loss: 6.739712715148926 | KNN Loss: 5.653365135192871 | BCE Loss: 1.0863476991653442\n",
      "Epoch 82 / 500 | iteration 20 / 30 | Total Loss: 6.74603271484375 | KNN Loss: 5.669891357421875 | BCE Loss: 1.076141595840454\n",
      "Epoch 82 / 500 | iteration 25 / 30 | Total Loss: 6.640805721282959 | KNN Loss: 5.604610443115234 | BCE Loss: 1.0361952781677246\n",
      "Epoch 83 / 500 | iteration 0 / 30 | Total Loss: 6.6503705978393555 | KNN Loss: 5.601080417633057 | BCE Loss: 1.049290418624878\n",
      "Epoch 83 / 500 | iteration 5 / 30 | Total Loss: 6.6765241622924805 | KNN Loss: 5.640936374664307 | BCE Loss: 1.0355875492095947\n",
      "Epoch 83 / 500 | iteration 10 / 30 | Total Loss: 6.670754432678223 | KNN Loss: 5.6088433265686035 | BCE Loss: 1.0619112253189087\n",
      "Epoch 83 / 500 | iteration 15 / 30 | Total Loss: 6.699547290802002 | KNN Loss: 5.617129325866699 | BCE Loss: 1.0824179649353027\n",
      "Epoch 83 / 500 | iteration 20 / 30 | Total Loss: 6.6293840408325195 | KNN Loss: 5.600003719329834 | BCE Loss: 1.0293800830841064\n",
      "Epoch 83 / 500 | iteration 25 / 30 | Total Loss: 6.681153297424316 | KNN Loss: 5.603238582611084 | BCE Loss: 1.0779149532318115\n",
      "Epoch    84: reducing learning rate of group 0 to 2.4500e-03.\n",
      "Epoch 84 / 500 | iteration 0 / 30 | Total Loss: 6.685017108917236 | KNN Loss: 5.639232158660889 | BCE Loss: 1.045784831047058\n",
      "Epoch 84 / 500 | iteration 5 / 30 | Total Loss: 6.653017044067383 | KNN Loss: 5.622314929962158 | BCE Loss: 1.0307018756866455\n",
      "Epoch 84 / 500 | iteration 10 / 30 | Total Loss: 6.647749900817871 | KNN Loss: 5.606812000274658 | BCE Loss: 1.040938138961792\n",
      "Epoch 84 / 500 | iteration 15 / 30 | Total Loss: 6.670991897583008 | KNN Loss: 5.610518932342529 | BCE Loss: 1.0604727268218994\n",
      "Epoch 84 / 500 | iteration 20 / 30 | Total Loss: 6.63201904296875 | KNN Loss: 5.598359107971191 | BCE Loss: 1.0336601734161377\n",
      "Epoch 84 / 500 | iteration 25 / 30 | Total Loss: 6.648414134979248 | KNN Loss: 5.598886966705322 | BCE Loss: 1.0495272874832153\n",
      "Epoch 85 / 500 | iteration 0 / 30 | Total Loss: 6.700716972351074 | KNN Loss: 5.623950004577637 | BCE Loss: 1.0767669677734375\n",
      "Epoch 85 / 500 | iteration 5 / 30 | Total Loss: 6.684232711791992 | KNN Loss: 5.642930030822754 | BCE Loss: 1.0413026809692383\n",
      "Epoch 85 / 500 | iteration 10 / 30 | Total Loss: 6.679052352905273 | KNN Loss: 5.603839874267578 | BCE Loss: 1.0752122402191162\n",
      "Epoch 85 / 500 | iteration 15 / 30 | Total Loss: 6.709357261657715 | KNN Loss: 5.691205978393555 | BCE Loss: 1.018151044845581\n",
      "Epoch 85 / 500 | iteration 20 / 30 | Total Loss: 6.630282402038574 | KNN Loss: 5.597883224487305 | BCE Loss: 1.0323994159698486\n",
      "Epoch 85 / 500 | iteration 25 / 30 | Total Loss: 6.671529293060303 | KNN Loss: 5.605776786804199 | BCE Loss: 1.0657525062561035\n",
      "Epoch 86 / 500 | iteration 0 / 30 | Total Loss: 6.634572982788086 | KNN Loss: 5.597868919372559 | BCE Loss: 1.0367043018341064\n",
      "Epoch 86 / 500 | iteration 5 / 30 | Total Loss: 6.684478282928467 | KNN Loss: 5.6098222732543945 | BCE Loss: 1.0746558904647827\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86 / 500 | iteration 10 / 30 | Total Loss: 6.74531364440918 | KNN Loss: 5.696653366088867 | BCE Loss: 1.0486605167388916\n",
      "Epoch 86 / 500 | iteration 15 / 30 | Total Loss: 6.667490482330322 | KNN Loss: 5.6081719398498535 | BCE Loss: 1.0593184232711792\n",
      "Epoch 86 / 500 | iteration 20 / 30 | Total Loss: 6.663773536682129 | KNN Loss: 5.611473083496094 | BCE Loss: 1.052300214767456\n",
      "Epoch 86 / 500 | iteration 25 / 30 | Total Loss: 6.668682098388672 | KNN Loss: 5.627532958984375 | BCE Loss: 1.041149377822876\n",
      "Epoch 87 / 500 | iteration 0 / 30 | Total Loss: 6.7231645584106445 | KNN Loss: 5.671037673950195 | BCE Loss: 1.0521271228790283\n",
      "Epoch 87 / 500 | iteration 5 / 30 | Total Loss: 6.650536060333252 | KNN Loss: 5.609241008758545 | BCE Loss: 1.0412949323654175\n",
      "Epoch 87 / 500 | iteration 10 / 30 | Total Loss: 6.635078430175781 | KNN Loss: 5.594808101654053 | BCE Loss: 1.0402705669403076\n",
      "Epoch 87 / 500 | iteration 15 / 30 | Total Loss: 6.7622246742248535 | KNN Loss: 5.687489032745361 | BCE Loss: 1.0747356414794922\n",
      "Epoch 87 / 500 | iteration 20 / 30 | Total Loss: 6.685400009155273 | KNN Loss: 5.621253490447998 | BCE Loss: 1.0641463994979858\n",
      "Epoch 87 / 500 | iteration 25 / 30 | Total Loss: 6.651364326477051 | KNN Loss: 5.594886302947998 | BCE Loss: 1.0564782619476318\n",
      "Epoch 88 / 500 | iteration 0 / 30 | Total Loss: 6.649666786193848 | KNN Loss: 5.609385967254639 | BCE Loss: 1.040280818939209\n",
      "Epoch 88 / 500 | iteration 5 / 30 | Total Loss: 6.802809715270996 | KNN Loss: 5.704178810119629 | BCE Loss: 1.098630666732788\n",
      "Epoch 88 / 500 | iteration 10 / 30 | Total Loss: 6.742800235748291 | KNN Loss: 5.724433422088623 | BCE Loss: 1.0183669328689575\n",
      "Epoch 88 / 500 | iteration 15 / 30 | Total Loss: 6.675666809082031 | KNN Loss: 5.6218953132629395 | BCE Loss: 1.0537714958190918\n",
      "Epoch 88 / 500 | iteration 20 / 30 | Total Loss: 6.683267593383789 | KNN Loss: 5.643876075744629 | BCE Loss: 1.0393917560577393\n",
      "Epoch 88 / 500 | iteration 25 / 30 | Total Loss: 6.640326976776123 | KNN Loss: 5.593832969665527 | BCE Loss: 1.0464940071105957\n",
      "Epoch 89 / 500 | iteration 0 / 30 | Total Loss: 6.714554309844971 | KNN Loss: 5.6140522956848145 | BCE Loss: 1.1005020141601562\n",
      "Epoch 89 / 500 | iteration 5 / 30 | Total Loss: 6.672247886657715 | KNN Loss: 5.643590927124023 | BCE Loss: 1.0286567211151123\n",
      "Epoch 89 / 500 | iteration 10 / 30 | Total Loss: 6.680427551269531 | KNN Loss: 5.653134346008301 | BCE Loss: 1.0272929668426514\n",
      "Epoch 89 / 500 | iteration 15 / 30 | Total Loss: 6.661818981170654 | KNN Loss: 5.60272216796875 | BCE Loss: 1.0590969324111938\n",
      "Epoch 89 / 500 | iteration 20 / 30 | Total Loss: 6.667679786682129 | KNN Loss: 5.607101917266846 | BCE Loss: 1.0605778694152832\n",
      "Epoch 89 / 500 | iteration 25 / 30 | Total Loss: 6.661991119384766 | KNN Loss: 5.5978922843933105 | BCE Loss: 1.0640990734100342\n",
      "Epoch 90 / 500 | iteration 0 / 30 | Total Loss: 6.615716934204102 | KNN Loss: 5.595050811767578 | BCE Loss: 1.0206658840179443\n",
      "Epoch 90 / 500 | iteration 5 / 30 | Total Loss: 6.678959369659424 | KNN Loss: 5.6455583572387695 | BCE Loss: 1.0334010124206543\n",
      "Epoch 90 / 500 | iteration 10 / 30 | Total Loss: 6.667006492614746 | KNN Loss: 5.633913040161133 | BCE Loss: 1.0330936908721924\n",
      "Epoch 90 / 500 | iteration 15 / 30 | Total Loss: 6.636732578277588 | KNN Loss: 5.599542617797852 | BCE Loss: 1.0371900796890259\n",
      "Epoch 90 / 500 | iteration 20 / 30 | Total Loss: 6.720857620239258 | KNN Loss: 5.612832546234131 | BCE Loss: 1.108025074005127\n",
      "Epoch 90 / 500 | iteration 25 / 30 | Total Loss: 6.66431188583374 | KNN Loss: 5.623040199279785 | BCE Loss: 1.0412718057632446\n",
      "Epoch 91 / 500 | iteration 0 / 30 | Total Loss: 6.702473163604736 | KNN Loss: 5.654684543609619 | BCE Loss: 1.0477885007858276\n",
      "Epoch 91 / 500 | iteration 5 / 30 | Total Loss: 6.646631240844727 | KNN Loss: 5.601433277130127 | BCE Loss: 1.0451982021331787\n",
      "Epoch 91 / 500 | iteration 10 / 30 | Total Loss: 6.672138690948486 | KNN Loss: 5.596821308135986 | BCE Loss: 1.0753173828125\n",
      "Epoch 91 / 500 | iteration 15 / 30 | Total Loss: 6.672564506530762 | KNN Loss: 5.651567459106445 | BCE Loss: 1.0209972858428955\n",
      "Epoch 91 / 500 | iteration 20 / 30 | Total Loss: 6.656647682189941 | KNN Loss: 5.6063456535339355 | BCE Loss: 1.0503019094467163\n",
      "Epoch 91 / 500 | iteration 25 / 30 | Total Loss: 6.647282600402832 | KNN Loss: 5.606065273284912 | BCE Loss: 1.0412174463272095\n",
      "Epoch 92 / 500 | iteration 0 / 30 | Total Loss: 6.676820755004883 | KNN Loss: 5.641913890838623 | BCE Loss: 1.0349071025848389\n",
      "Epoch 92 / 500 | iteration 5 / 30 | Total Loss: 6.649482727050781 | KNN Loss: 5.596202373504639 | BCE Loss: 1.0532801151275635\n",
      "Epoch 92 / 500 | iteration 10 / 30 | Total Loss: 6.7216386795043945 | KNN Loss: 5.693138599395752 | BCE Loss: 1.0285003185272217\n",
      "Epoch 92 / 500 | iteration 15 / 30 | Total Loss: 6.676387786865234 | KNN Loss: 5.59979248046875 | BCE Loss: 1.0765953063964844\n",
      "Epoch 92 / 500 | iteration 20 / 30 | Total Loss: 6.731636047363281 | KNN Loss: 5.657626152038574 | BCE Loss: 1.074009656906128\n",
      "Epoch 92 / 500 | iteration 25 / 30 | Total Loss: 6.679350852966309 | KNN Loss: 5.613170623779297 | BCE Loss: 1.0661799907684326\n",
      "Epoch 93 / 500 | iteration 0 / 30 | Total Loss: 6.768725872039795 | KNN Loss: 5.725228786468506 | BCE Loss: 1.0434969663619995\n",
      "Epoch 93 / 500 | iteration 5 / 30 | Total Loss: 6.649639129638672 | KNN Loss: 5.607463836669922 | BCE Loss: 1.042175531387329\n",
      "Epoch 93 / 500 | iteration 10 / 30 | Total Loss: 6.631686210632324 | KNN Loss: 5.611639499664307 | BCE Loss: 1.0200469493865967\n",
      "Epoch 93 / 500 | iteration 15 / 30 | Total Loss: 6.730710506439209 | KNN Loss: 5.684576034545898 | BCE Loss: 1.046134352684021\n",
      "Epoch 93 / 500 | iteration 20 / 30 | Total Loss: 6.659808158874512 | KNN Loss: 5.604211330413818 | BCE Loss: 1.0555968284606934\n",
      "Epoch 93 / 500 | iteration 25 / 30 | Total Loss: 6.721746921539307 | KNN Loss: 5.674752712249756 | BCE Loss: 1.0469942092895508\n",
      "Epoch 94 / 500 | iteration 0 / 30 | Total Loss: 6.640027046203613 | KNN Loss: 5.600220203399658 | BCE Loss: 1.0398069620132446\n",
      "Epoch 94 / 500 | iteration 5 / 30 | Total Loss: 6.662591934204102 | KNN Loss: 5.626983165740967 | BCE Loss: 1.0356085300445557\n",
      "Epoch 94 / 500 | iteration 10 / 30 | Total Loss: 6.701756477355957 | KNN Loss: 5.627580165863037 | BCE Loss: 1.07417631149292\n",
      "Epoch 94 / 500 | iteration 15 / 30 | Total Loss: 6.649733543395996 | KNN Loss: 5.601362705230713 | BCE Loss: 1.0483710765838623\n",
      "Epoch 94 / 500 | iteration 20 / 30 | Total Loss: 6.648663520812988 | KNN Loss: 5.605428218841553 | BCE Loss: 1.043235421180725\n",
      "Epoch 94 / 500 | iteration 25 / 30 | Total Loss: 6.672168731689453 | KNN Loss: 5.6071553230285645 | BCE Loss: 1.0650134086608887\n",
      "Epoch    95: reducing learning rate of group 0 to 1.7150e-03.\n",
      "Epoch 95 / 500 | iteration 0 / 30 | Total Loss: 6.698575973510742 | KNN Loss: 5.628859996795654 | BCE Loss: 1.0697157382965088\n",
      "Epoch 95 / 500 | iteration 5 / 30 | Total Loss: 6.651965141296387 | KNN Loss: 5.598424911499023 | BCE Loss: 1.0535399913787842\n",
      "Epoch 95 / 500 | iteration 10 / 30 | Total Loss: 6.629210472106934 | KNN Loss: 5.592526435852051 | BCE Loss: 1.0366839170455933\n",
      "Epoch 95 / 500 | iteration 15 / 30 | Total Loss: 6.6550374031066895 | KNN Loss: 5.610916614532471 | BCE Loss: 1.0441206693649292\n",
      "Epoch 95 / 500 | iteration 20 / 30 | Total Loss: 6.636044025421143 | KNN Loss: 5.6130266189575195 | BCE Loss: 1.0230175256729126\n",
      "Epoch 95 / 500 | iteration 25 / 30 | Total Loss: 6.68865966796875 | KNN Loss: 5.613225936889648 | BCE Loss: 1.0754339694976807\n",
      "Epoch 96 / 500 | iteration 0 / 30 | Total Loss: 6.663922309875488 | KNN Loss: 5.615105628967285 | BCE Loss: 1.0488165616989136\n",
      "Epoch 96 / 500 | iteration 5 / 30 | Total Loss: 6.6731858253479 | KNN Loss: 5.605554580688477 | BCE Loss: 1.0676311254501343\n",
      "Epoch 96 / 500 | iteration 10 / 30 | Total Loss: 6.657373428344727 | KNN Loss: 5.636203765869141 | BCE Loss: 1.0211694240570068\n",
      "Epoch 96 / 500 | iteration 15 / 30 | Total Loss: 6.74057149887085 | KNN Loss: 5.688114166259766 | BCE Loss: 1.052457332611084\n",
      "Epoch 96 / 500 | iteration 20 / 30 | Total Loss: 6.701828956604004 | KNN Loss: 5.649261951446533 | BCE Loss: 1.0525672435760498\n",
      "Epoch 96 / 500 | iteration 25 / 30 | Total Loss: 6.700676441192627 | KNN Loss: 5.644386291503906 | BCE Loss: 1.0562900304794312\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97 / 500 | iteration 0 / 30 | Total Loss: 6.735601902008057 | KNN Loss: 5.686628818511963 | BCE Loss: 1.0489730834960938\n",
      "Epoch 97 / 500 | iteration 5 / 30 | Total Loss: 6.700606346130371 | KNN Loss: 5.67059326171875 | BCE Loss: 1.0300133228302002\n",
      "Epoch 97 / 500 | iteration 10 / 30 | Total Loss: 6.711550235748291 | KNN Loss: 5.680934906005859 | BCE Loss: 1.0306153297424316\n",
      "Epoch 97 / 500 | iteration 15 / 30 | Total Loss: 6.705007076263428 | KNN Loss: 5.681201457977295 | BCE Loss: 1.0238054990768433\n",
      "Epoch 97 / 500 | iteration 20 / 30 | Total Loss: 6.7050275802612305 | KNN Loss: 5.662910461425781 | BCE Loss: 1.0421173572540283\n",
      "Epoch 97 / 500 | iteration 25 / 30 | Total Loss: 6.66266393661499 | KNN Loss: 5.610222816467285 | BCE Loss: 1.052441120147705\n",
      "Epoch 98 / 500 | iteration 0 / 30 | Total Loss: 6.671745300292969 | KNN Loss: 5.601093292236328 | BCE Loss: 1.0706522464752197\n",
      "Epoch 98 / 500 | iteration 5 / 30 | Total Loss: 6.631798267364502 | KNN Loss: 5.594435691833496 | BCE Loss: 1.0373625755310059\n",
      "Epoch 98 / 500 | iteration 10 / 30 | Total Loss: 6.656471252441406 | KNN Loss: 5.6226887702941895 | BCE Loss: 1.0337824821472168\n",
      "Epoch 98 / 500 | iteration 15 / 30 | Total Loss: 6.681123733520508 | KNN Loss: 5.62704610824585 | BCE Loss: 1.0540776252746582\n",
      "Epoch 98 / 500 | iteration 20 / 30 | Total Loss: 6.712466239929199 | KNN Loss: 5.645667552947998 | BCE Loss: 1.0667989253997803\n",
      "Epoch 98 / 500 | iteration 25 / 30 | Total Loss: 6.658115386962891 | KNN Loss: 5.606791019439697 | BCE Loss: 1.0513241291046143\n",
      "Epoch 99 / 500 | iteration 0 / 30 | Total Loss: 6.648794174194336 | KNN Loss: 5.612051486968994 | BCE Loss: 1.0367424488067627\n",
      "Epoch 99 / 500 | iteration 5 / 30 | Total Loss: 6.773447036743164 | KNN Loss: 5.7101640701293945 | BCE Loss: 1.0632827281951904\n",
      "Epoch 99 / 500 | iteration 10 / 30 | Total Loss: 6.649845123291016 | KNN Loss: 5.601258277893066 | BCE Loss: 1.0485867261886597\n",
      "Epoch 99 / 500 | iteration 15 / 30 | Total Loss: 6.726353168487549 | KNN Loss: 5.696527481079102 | BCE Loss: 1.0298256874084473\n",
      "Epoch 99 / 500 | iteration 20 / 30 | Total Loss: 6.633548736572266 | KNN Loss: 5.601265907287598 | BCE Loss: 1.032282829284668\n",
      "Epoch 99 / 500 | iteration 25 / 30 | Total Loss: 6.675336837768555 | KNN Loss: 5.629008769989014 | BCE Loss: 1.046328067779541\n",
      "Epoch 100 / 500 | iteration 0 / 30 | Total Loss: 6.678070545196533 | KNN Loss: 5.627086639404297 | BCE Loss: 1.0509839057922363\n",
      "Epoch 100 / 500 | iteration 5 / 30 | Total Loss: 6.627228736877441 | KNN Loss: 5.610009670257568 | BCE Loss: 1.017219066619873\n",
      "Epoch 100 / 500 | iteration 10 / 30 | Total Loss: 6.640842914581299 | KNN Loss: 5.5954484939575195 | BCE Loss: 1.0453944206237793\n",
      "Epoch 100 / 500 | iteration 15 / 30 | Total Loss: 6.696208953857422 | KNN Loss: 5.640126705169678 | BCE Loss: 1.0560823678970337\n",
      "Epoch 100 / 500 | iteration 20 / 30 | Total Loss: 6.702059745788574 | KNN Loss: 5.665362358093262 | BCE Loss: 1.0366976261138916\n",
      "Epoch 100 / 500 | iteration 25 / 30 | Total Loss: 6.771856307983398 | KNN Loss: 5.700894355773926 | BCE Loss: 1.0709620714187622\n",
      "Epoch 101 / 500 | iteration 0 / 30 | Total Loss: 6.659514427185059 | KNN Loss: 5.640677452087402 | BCE Loss: 1.0188370943069458\n",
      "Epoch 101 / 500 | iteration 5 / 30 | Total Loss: 6.713932991027832 | KNN Loss: 5.651194095611572 | BCE Loss: 1.0627386569976807\n",
      "Epoch 101 / 500 | iteration 10 / 30 | Total Loss: 6.696863174438477 | KNN Loss: 5.651706218719482 | BCE Loss: 1.045156717300415\n",
      "Epoch 101 / 500 | iteration 15 / 30 | Total Loss: 6.702812194824219 | KNN Loss: 5.642270088195801 | BCE Loss: 1.0605422258377075\n",
      "Epoch 101 / 500 | iteration 20 / 30 | Total Loss: 6.669232368469238 | KNN Loss: 5.620419025421143 | BCE Loss: 1.0488132238388062\n",
      "Epoch 101 / 500 | iteration 25 / 30 | Total Loss: 6.709313869476318 | KNN Loss: 5.678735256195068 | BCE Loss: 1.03057861328125\n",
      "Epoch 102 / 500 | iteration 0 / 30 | Total Loss: 6.671626567840576 | KNN Loss: 5.622696876525879 | BCE Loss: 1.0489296913146973\n",
      "Epoch 102 / 500 | iteration 5 / 30 | Total Loss: 6.657515525817871 | KNN Loss: 5.6023335456848145 | BCE Loss: 1.0551820993423462\n",
      "Epoch 102 / 500 | iteration 10 / 30 | Total Loss: 6.708975791931152 | KNN Loss: 5.6433563232421875 | BCE Loss: 1.0656194686889648\n",
      "Epoch 102 / 500 | iteration 15 / 30 | Total Loss: 6.694689750671387 | KNN Loss: 5.623462200164795 | BCE Loss: 1.0712273120880127\n",
      "Epoch 102 / 500 | iteration 20 / 30 | Total Loss: 6.6322736740112305 | KNN Loss: 5.595886707305908 | BCE Loss: 1.0363869667053223\n",
      "Epoch 102 / 500 | iteration 25 / 30 | Total Loss: 6.717376708984375 | KNN Loss: 5.629166126251221 | BCE Loss: 1.0882107019424438\n",
      "Epoch 103 / 500 | iteration 0 / 30 | Total Loss: 6.631493091583252 | KNN Loss: 5.605413913726807 | BCE Loss: 1.0260790586471558\n",
      "Epoch 103 / 500 | iteration 5 / 30 | Total Loss: 6.705227375030518 | KNN Loss: 5.663613319396973 | BCE Loss: 1.041614055633545\n",
      "Epoch 103 / 500 | iteration 10 / 30 | Total Loss: 6.646899700164795 | KNN Loss: 5.605262756347656 | BCE Loss: 1.0416368246078491\n",
      "Epoch 103 / 500 | iteration 15 / 30 | Total Loss: 6.733300685882568 | KNN Loss: 5.698531150817871 | BCE Loss: 1.0347696542739868\n",
      "Epoch 103 / 500 | iteration 20 / 30 | Total Loss: 6.6457014083862305 | KNN Loss: 5.622809410095215 | BCE Loss: 1.0228919982910156\n",
      "Epoch 103 / 500 | iteration 25 / 30 | Total Loss: 6.662935256958008 | KNN Loss: 5.633491516113281 | BCE Loss: 1.0294439792633057\n",
      "Epoch 104 / 500 | iteration 0 / 30 | Total Loss: 6.67338752746582 | KNN Loss: 5.630344867706299 | BCE Loss: 1.043042778968811\n",
      "Epoch 104 / 500 | iteration 5 / 30 | Total Loss: 6.726573944091797 | KNN Loss: 5.652276039123535 | BCE Loss: 1.0742979049682617\n",
      "Epoch 104 / 500 | iteration 10 / 30 | Total Loss: 6.642472743988037 | KNN Loss: 5.604130744934082 | BCE Loss: 1.0383421182632446\n",
      "Epoch 104 / 500 | iteration 15 / 30 | Total Loss: 6.702147006988525 | KNN Loss: 5.657870769500732 | BCE Loss: 1.044276237487793\n",
      "Epoch 104 / 500 | iteration 20 / 30 | Total Loss: 6.726385593414307 | KNN Loss: 5.656153202056885 | BCE Loss: 1.0702323913574219\n",
      "Epoch 104 / 500 | iteration 25 / 30 | Total Loss: 6.665809631347656 | KNN Loss: 5.6423258781433105 | BCE Loss: 1.0234835147857666\n",
      "Epoch 105 / 500 | iteration 0 / 30 | Total Loss: 6.803573131561279 | KNN Loss: 5.7167229652404785 | BCE Loss: 1.0868502855300903\n",
      "Epoch 105 / 500 | iteration 5 / 30 | Total Loss: 6.6375041007995605 | KNN Loss: 5.6022868156433105 | BCE Loss: 1.0352174043655396\n",
      "Epoch 105 / 500 | iteration 10 / 30 | Total Loss: 6.705340385437012 | KNN Loss: 5.657510757446289 | BCE Loss: 1.0478297472000122\n",
      "Epoch 105 / 500 | iteration 15 / 30 | Total Loss: 6.668402671813965 | KNN Loss: 5.611624717712402 | BCE Loss: 1.0567781925201416\n",
      "Epoch 105 / 500 | iteration 20 / 30 | Total Loss: 6.6768598556518555 | KNN Loss: 5.6101226806640625 | BCE Loss: 1.066737174987793\n",
      "Epoch 105 / 500 | iteration 25 / 30 | Total Loss: 6.654056549072266 | KNN Loss: 5.621824741363525 | BCE Loss: 1.0322319269180298\n",
      "Epoch   106: reducing learning rate of group 0 to 1.2005e-03.\n",
      "Epoch 106 / 500 | iteration 0 / 30 | Total Loss: 6.642303943634033 | KNN Loss: 5.61234712600708 | BCE Loss: 1.0299566984176636\n",
      "Epoch 106 / 500 | iteration 5 / 30 | Total Loss: 6.660885810852051 | KNN Loss: 5.606398105621338 | BCE Loss: 1.054487705230713\n",
      "Epoch 106 / 500 | iteration 10 / 30 | Total Loss: 6.606541633605957 | KNN Loss: 5.603132247924805 | BCE Loss: 1.003409504890442\n",
      "Epoch 106 / 500 | iteration 15 / 30 | Total Loss: 6.622082233428955 | KNN Loss: 5.5967888832092285 | BCE Loss: 1.0252934694290161\n",
      "Epoch 106 / 500 | iteration 20 / 30 | Total Loss: 6.636466026306152 | KNN Loss: 5.599634170532227 | BCE Loss: 1.0368317365646362\n",
      "Epoch 106 / 500 | iteration 25 / 30 | Total Loss: 6.734574317932129 | KNN Loss: 5.668741226196289 | BCE Loss: 1.0658328533172607\n",
      "Epoch 107 / 500 | iteration 0 / 30 | Total Loss: 6.6734724044799805 | KNN Loss: 5.601969242095947 | BCE Loss: 1.0715034008026123\n",
      "Epoch 107 / 500 | iteration 5 / 30 | Total Loss: 6.697513103485107 | KNN Loss: 5.634451866149902 | BCE Loss: 1.0630611181259155\n",
      "Epoch 107 / 500 | iteration 10 / 30 | Total Loss: 6.689561367034912 | KNN Loss: 5.632945537567139 | BCE Loss: 1.0566158294677734\n",
      "Epoch 107 / 500 | iteration 15 / 30 | Total Loss: 6.645410537719727 | KNN Loss: 5.619353294372559 | BCE Loss: 1.026057243347168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 107 / 500 | iteration 20 / 30 | Total Loss: 6.642368316650391 | KNN Loss: 5.614650249481201 | BCE Loss: 1.0277180671691895\n",
      "Epoch 107 / 500 | iteration 25 / 30 | Total Loss: 6.661529541015625 | KNN Loss: 5.622446060180664 | BCE Loss: 1.03908371925354\n",
      "Epoch 108 / 500 | iteration 0 / 30 | Total Loss: 6.723221778869629 | KNN Loss: 5.6963300704956055 | BCE Loss: 1.0268919467926025\n",
      "Epoch 108 / 500 | iteration 5 / 30 | Total Loss: 6.674414157867432 | KNN Loss: 5.621864318847656 | BCE Loss: 1.0525498390197754\n",
      "Epoch 108 / 500 | iteration 10 / 30 | Total Loss: 6.699146747589111 | KNN Loss: 5.639377593994141 | BCE Loss: 1.0597691535949707\n",
      "Epoch 108 / 500 | iteration 15 / 30 | Total Loss: 6.672486305236816 | KNN Loss: 5.6222944259643555 | BCE Loss: 1.05019211769104\n",
      "Epoch 108 / 500 | iteration 20 / 30 | Total Loss: 6.6783671379089355 | KNN Loss: 5.62606143951416 | BCE Loss: 1.0523056983947754\n",
      "Epoch 108 / 500 | iteration 25 / 30 | Total Loss: 6.655538082122803 | KNN Loss: 5.595443248748779 | BCE Loss: 1.0600947141647339\n",
      "Epoch 109 / 500 | iteration 0 / 30 | Total Loss: 6.685178756713867 | KNN Loss: 5.629512786865234 | BCE Loss: 1.0556657314300537\n",
      "Epoch 109 / 500 | iteration 5 / 30 | Total Loss: 6.68035888671875 | KNN Loss: 5.615182399749756 | BCE Loss: 1.0651767253875732\n",
      "Epoch 109 / 500 | iteration 10 / 30 | Total Loss: 6.680089950561523 | KNN Loss: 5.621287822723389 | BCE Loss: 1.0588021278381348\n",
      "Epoch 109 / 500 | iteration 15 / 30 | Total Loss: 6.645676612854004 | KNN Loss: 5.600641250610352 | BCE Loss: 1.0450353622436523\n",
      "Epoch 109 / 500 | iteration 20 / 30 | Total Loss: 6.6746015548706055 | KNN Loss: 5.633960247039795 | BCE Loss: 1.0406410694122314\n",
      "Epoch 109 / 500 | iteration 25 / 30 | Total Loss: 6.626729965209961 | KNN Loss: 5.5993852615356445 | BCE Loss: 1.0273449420928955\n",
      "Epoch 110 / 500 | iteration 0 / 30 | Total Loss: 6.638323783874512 | KNN Loss: 5.600231647491455 | BCE Loss: 1.0380923748016357\n",
      "Epoch 110 / 500 | iteration 5 / 30 | Total Loss: 6.624201774597168 | KNN Loss: 5.602458477020264 | BCE Loss: 1.0217430591583252\n",
      "Epoch 110 / 500 | iteration 10 / 30 | Total Loss: 6.669800758361816 | KNN Loss: 5.617917060852051 | BCE Loss: 1.051883578300476\n",
      "Epoch 110 / 500 | iteration 15 / 30 | Total Loss: 6.675069808959961 | KNN Loss: 5.627232551574707 | BCE Loss: 1.047837495803833\n",
      "Epoch 110 / 500 | iteration 20 / 30 | Total Loss: 6.66192102432251 | KNN Loss: 5.612545967102051 | BCE Loss: 1.049375057220459\n",
      "Epoch 110 / 500 | iteration 25 / 30 | Total Loss: 6.673588275909424 | KNN Loss: 5.635259628295898 | BCE Loss: 1.0383286476135254\n",
      "Epoch 111 / 500 | iteration 0 / 30 | Total Loss: 6.606902122497559 | KNN Loss: 5.593027114868164 | BCE Loss: 1.0138747692108154\n",
      "Epoch 111 / 500 | iteration 5 / 30 | Total Loss: 6.680006980895996 | KNN Loss: 5.620060443878174 | BCE Loss: 1.0599462985992432\n",
      "Epoch 111 / 500 | iteration 10 / 30 | Total Loss: 6.729866981506348 | KNN Loss: 5.640632629394531 | BCE Loss: 1.0892343521118164\n",
      "Epoch 111 / 500 | iteration 15 / 30 | Total Loss: 6.67494010925293 | KNN Loss: 5.61960506439209 | BCE Loss: 1.0553348064422607\n",
      "Epoch 111 / 500 | iteration 20 / 30 | Total Loss: 6.658407211303711 | KNN Loss: 5.59813117980957 | BCE Loss: 1.0602757930755615\n",
      "Epoch 111 / 500 | iteration 25 / 30 | Total Loss: 6.628316879272461 | KNN Loss: 5.620328903198242 | BCE Loss: 1.0079882144927979\n",
      "Epoch 112 / 500 | iteration 0 / 30 | Total Loss: 6.771947383880615 | KNN Loss: 5.724567890167236 | BCE Loss: 1.047379493713379\n",
      "Epoch 112 / 500 | iteration 5 / 30 | Total Loss: 6.709833145141602 | KNN Loss: 5.661664962768555 | BCE Loss: 1.0481679439544678\n",
      "Epoch 112 / 500 | iteration 10 / 30 | Total Loss: 6.709961891174316 | KNN Loss: 5.644504547119141 | BCE Loss: 1.0654572248458862\n",
      "Epoch 112 / 500 | iteration 15 / 30 | Total Loss: 6.639479637145996 | KNN Loss: 5.638404369354248 | BCE Loss: 1.0010755062103271\n",
      "Epoch 112 / 500 | iteration 20 / 30 | Total Loss: 6.721047401428223 | KNN Loss: 5.646272659301758 | BCE Loss: 1.074774980545044\n",
      "Epoch 112 / 500 | iteration 25 / 30 | Total Loss: 6.665377140045166 | KNN Loss: 5.625607013702393 | BCE Loss: 1.0397701263427734\n",
      "Epoch 113 / 500 | iteration 0 / 30 | Total Loss: 6.682079792022705 | KNN Loss: 5.612382888793945 | BCE Loss: 1.0696970224380493\n",
      "Epoch 113 / 500 | iteration 5 / 30 | Total Loss: 6.66970157623291 | KNN Loss: 5.630810260772705 | BCE Loss: 1.0388914346694946\n",
      "Epoch 113 / 500 | iteration 10 / 30 | Total Loss: 6.640538215637207 | KNN Loss: 5.617737770080566 | BCE Loss: 1.0228004455566406\n",
      "Epoch 113 / 500 | iteration 15 / 30 | Total Loss: 6.783851623535156 | KNN Loss: 5.713255882263184 | BCE Loss: 1.0705958604812622\n",
      "Epoch 113 / 500 | iteration 20 / 30 | Total Loss: 6.654533863067627 | KNN Loss: 5.608861923217773 | BCE Loss: 1.0456719398498535\n",
      "Epoch 113 / 500 | iteration 25 / 30 | Total Loss: 6.690829753875732 | KNN Loss: 5.599590301513672 | BCE Loss: 1.09123957157135\n",
      "Epoch 114 / 500 | iteration 0 / 30 | Total Loss: 6.657622337341309 | KNN Loss: 5.62013053894043 | BCE Loss: 1.037491798400879\n",
      "Epoch 114 / 500 | iteration 5 / 30 | Total Loss: 6.6461029052734375 | KNN Loss: 5.623958587646484 | BCE Loss: 1.0221444368362427\n",
      "Epoch 114 / 500 | iteration 10 / 30 | Total Loss: 6.65757942199707 | KNN Loss: 5.6286187171936035 | BCE Loss: 1.028960943222046\n",
      "Epoch 114 / 500 | iteration 15 / 30 | Total Loss: 6.656552314758301 | KNN Loss: 5.6115899085998535 | BCE Loss: 1.0449624061584473\n",
      "Epoch 114 / 500 | iteration 20 / 30 | Total Loss: 6.724102020263672 | KNN Loss: 5.659292697906494 | BCE Loss: 1.0648090839385986\n",
      "Epoch 114 / 500 | iteration 25 / 30 | Total Loss: 6.622841835021973 | KNN Loss: 5.5967631340026855 | BCE Loss: 1.0260789394378662\n",
      "Epoch 115 / 500 | iteration 0 / 30 | Total Loss: 6.687580108642578 | KNN Loss: 5.6409220695495605 | BCE Loss: 1.0466578006744385\n",
      "Epoch 115 / 500 | iteration 5 / 30 | Total Loss: 6.676980972290039 | KNN Loss: 5.631969928741455 | BCE Loss: 1.0450109243392944\n",
      "Epoch 115 / 500 | iteration 10 / 30 | Total Loss: 6.669440746307373 | KNN Loss: 5.609776020050049 | BCE Loss: 1.0596648454666138\n",
      "Epoch 115 / 500 | iteration 15 / 30 | Total Loss: 6.6248250007629395 | KNN Loss: 5.601276397705078 | BCE Loss: 1.0235487222671509\n",
      "Epoch 115 / 500 | iteration 20 / 30 | Total Loss: 6.686733245849609 | KNN Loss: 5.642265319824219 | BCE Loss: 1.0444681644439697\n",
      "Epoch 115 / 500 | iteration 25 / 30 | Total Loss: 6.667570114135742 | KNN Loss: 5.611714839935303 | BCE Loss: 1.055855393409729\n",
      "Epoch 116 / 500 | iteration 0 / 30 | Total Loss: 6.672534942626953 | KNN Loss: 5.615179061889648 | BCE Loss: 1.0573558807373047\n",
      "Epoch 116 / 500 | iteration 5 / 30 | Total Loss: 6.634487628936768 | KNN Loss: 5.593304634094238 | BCE Loss: 1.0411829948425293\n",
      "Epoch 116 / 500 | iteration 10 / 30 | Total Loss: 6.687726974487305 | KNN Loss: 5.629931926727295 | BCE Loss: 1.0577950477600098\n",
      "Epoch 116 / 500 | iteration 15 / 30 | Total Loss: 6.640153884887695 | KNN Loss: 5.60418176651001 | BCE Loss: 1.0359723567962646\n",
      "Epoch 116 / 500 | iteration 20 / 30 | Total Loss: 6.646450042724609 | KNN Loss: 5.600022315979004 | BCE Loss: 1.0464274883270264\n",
      "Epoch 116 / 500 | iteration 25 / 30 | Total Loss: 6.761687278747559 | KNN Loss: 5.688174247741699 | BCE Loss: 1.073513150215149\n",
      "Epoch 117 / 500 | iteration 0 / 30 | Total Loss: 6.709134101867676 | KNN Loss: 5.672500133514404 | BCE Loss: 1.0366339683532715\n",
      "Epoch 117 / 500 | iteration 5 / 30 | Total Loss: 6.637130260467529 | KNN Loss: 5.607690811157227 | BCE Loss: 1.0294395685195923\n",
      "Epoch 117 / 500 | iteration 10 / 30 | Total Loss: 6.66363525390625 | KNN Loss: 5.616858959197998 | BCE Loss: 1.046776533126831\n",
      "Epoch 117 / 500 | iteration 15 / 30 | Total Loss: 6.684296607971191 | KNN Loss: 5.6245598793029785 | BCE Loss: 1.059736728668213\n",
      "Epoch 117 / 500 | iteration 20 / 30 | Total Loss: 6.6355743408203125 | KNN Loss: 5.600987911224365 | BCE Loss: 1.0345866680145264\n",
      "Epoch 117 / 500 | iteration 25 / 30 | Total Loss: 6.672975540161133 | KNN Loss: 5.622113227844238 | BCE Loss: 1.0508623123168945\n",
      "Epoch 118 / 500 | iteration 0 / 30 | Total Loss: 6.642358303070068 | KNN Loss: 5.59832763671875 | BCE Loss: 1.0440305471420288\n",
      "Epoch 118 / 500 | iteration 5 / 30 | Total Loss: 6.69105339050293 | KNN Loss: 5.640806198120117 | BCE Loss: 1.0502469539642334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118 / 500 | iteration 10 / 30 | Total Loss: 6.654528617858887 | KNN Loss: 5.597446441650391 | BCE Loss: 1.057082176208496\n",
      "Epoch 118 / 500 | iteration 15 / 30 | Total Loss: 6.6501688957214355 | KNN Loss: 5.618995189666748 | BCE Loss: 1.0311737060546875\n",
      "Epoch 118 / 500 | iteration 20 / 30 | Total Loss: 6.6280622482299805 | KNN Loss: 5.590504169464111 | BCE Loss: 1.0375579595565796\n",
      "Epoch 118 / 500 | iteration 25 / 30 | Total Loss: 6.690496921539307 | KNN Loss: 5.634491443634033 | BCE Loss: 1.0560054779052734\n",
      "Epoch 119 / 500 | iteration 0 / 30 | Total Loss: 6.752155303955078 | KNN Loss: 5.720042705535889 | BCE Loss: 1.0321123600006104\n",
      "Epoch 119 / 500 | iteration 5 / 30 | Total Loss: 6.664290428161621 | KNN Loss: 5.6135969161987305 | BCE Loss: 1.0506936311721802\n",
      "Epoch 119 / 500 | iteration 10 / 30 | Total Loss: 6.708821773529053 | KNN Loss: 5.636144161224365 | BCE Loss: 1.072677493095398\n",
      "Epoch 119 / 500 | iteration 15 / 30 | Total Loss: 6.670483589172363 | KNN Loss: 5.6188225746154785 | BCE Loss: 1.0516612529754639\n",
      "Epoch 119 / 500 | iteration 20 / 30 | Total Loss: 6.678617000579834 | KNN Loss: 5.632582664489746 | BCE Loss: 1.0460344552993774\n",
      "Epoch 119 / 500 | iteration 25 / 30 | Total Loss: 6.658674240112305 | KNN Loss: 5.630201816558838 | BCE Loss: 1.0284723043441772\n",
      "Epoch 120 / 500 | iteration 0 / 30 | Total Loss: 6.698003768920898 | KNN Loss: 5.666311740875244 | BCE Loss: 1.0316922664642334\n",
      "Epoch 120 / 500 | iteration 5 / 30 | Total Loss: 6.698155879974365 | KNN Loss: 5.627995491027832 | BCE Loss: 1.0701602697372437\n",
      "Epoch 120 / 500 | iteration 10 / 30 | Total Loss: 6.685749053955078 | KNN Loss: 5.650328159332275 | BCE Loss: 1.0354206562042236\n",
      "Epoch 120 / 500 | iteration 15 / 30 | Total Loss: 6.727211952209473 | KNN Loss: 5.6866607666015625 | BCE Loss: 1.0405514240264893\n",
      "Epoch 120 / 500 | iteration 20 / 30 | Total Loss: 6.766130447387695 | KNN Loss: 5.696486949920654 | BCE Loss: 1.0696437358856201\n",
      "Epoch 120 / 500 | iteration 25 / 30 | Total Loss: 6.666543960571289 | KNN Loss: 5.616622447967529 | BCE Loss: 1.0499215126037598\n",
      "Epoch 121 / 500 | iteration 0 / 30 | Total Loss: 6.684866905212402 | KNN Loss: 5.617181777954102 | BCE Loss: 1.0676850080490112\n",
      "Epoch 121 / 500 | iteration 5 / 30 | Total Loss: 6.768745422363281 | KNN Loss: 5.739222049713135 | BCE Loss: 1.029523491859436\n",
      "Epoch 121 / 500 | iteration 10 / 30 | Total Loss: 6.641256332397461 | KNN Loss: 5.6082940101623535 | BCE Loss: 1.0329625606536865\n",
      "Epoch 121 / 500 | iteration 15 / 30 | Total Loss: 6.6662797927856445 | KNN Loss: 5.640213489532471 | BCE Loss: 1.0260663032531738\n",
      "Epoch 121 / 500 | iteration 20 / 30 | Total Loss: 6.671252250671387 | KNN Loss: 5.616968631744385 | BCE Loss: 1.054283618927002\n",
      "Epoch 121 / 500 | iteration 25 / 30 | Total Loss: 6.678045272827148 | KNN Loss: 5.616680145263672 | BCE Loss: 1.0613648891448975\n",
      "Epoch 122 / 500 | iteration 0 / 30 | Total Loss: 6.644537925720215 | KNN Loss: 5.6099677085876465 | BCE Loss: 1.0345702171325684\n",
      "Epoch 122 / 500 | iteration 5 / 30 | Total Loss: 6.646973609924316 | KNN Loss: 5.5962324142456055 | BCE Loss: 1.050741195678711\n",
      "Epoch 122 / 500 | iteration 10 / 30 | Total Loss: 6.631340026855469 | KNN Loss: 5.599889755249023 | BCE Loss: 1.0314505100250244\n",
      "Epoch 122 / 500 | iteration 15 / 30 | Total Loss: 6.687000751495361 | KNN Loss: 5.601191520690918 | BCE Loss: 1.0858092308044434\n",
      "Epoch 122 / 500 | iteration 20 / 30 | Total Loss: 6.636747360229492 | KNN Loss: 5.602532386779785 | BCE Loss: 1.0342150926589966\n",
      "Epoch 122 / 500 | iteration 25 / 30 | Total Loss: 6.655577659606934 | KNN Loss: 5.622992515563965 | BCE Loss: 1.0325853824615479\n",
      "Epoch 123 / 500 | iteration 0 / 30 | Total Loss: 6.679752826690674 | KNN Loss: 5.616322994232178 | BCE Loss: 1.063429832458496\n",
      "Epoch 123 / 500 | iteration 5 / 30 | Total Loss: 6.686369895935059 | KNN Loss: 5.6476521492004395 | BCE Loss: 1.0387177467346191\n",
      "Epoch 123 / 500 | iteration 10 / 30 | Total Loss: 6.670804500579834 | KNN Loss: 5.603710651397705 | BCE Loss: 1.0670937299728394\n",
      "Epoch 123 / 500 | iteration 15 / 30 | Total Loss: 6.644810199737549 | KNN Loss: 5.599525451660156 | BCE Loss: 1.0452848672866821\n",
      "Epoch 123 / 500 | iteration 20 / 30 | Total Loss: 6.663957595825195 | KNN Loss: 5.629295825958252 | BCE Loss: 1.0346615314483643\n",
      "Epoch 123 / 500 | iteration 25 / 30 | Total Loss: 6.668476104736328 | KNN Loss: 5.6468048095703125 | BCE Loss: 1.021671175956726\n",
      "Epoch 124 / 500 | iteration 0 / 30 | Total Loss: 6.626053810119629 | KNN Loss: 5.602283000946045 | BCE Loss: 1.023770809173584\n",
      "Epoch 124 / 500 | iteration 5 / 30 | Total Loss: 6.659173965454102 | KNN Loss: 5.608130931854248 | BCE Loss: 1.0510430335998535\n",
      "Epoch 124 / 500 | iteration 10 / 30 | Total Loss: 6.657871246337891 | KNN Loss: 5.6109209060668945 | BCE Loss: 1.046950340270996\n",
      "Epoch 124 / 500 | iteration 15 / 30 | Total Loss: 6.653383255004883 | KNN Loss: 5.605896949768066 | BCE Loss: 1.0474865436553955\n",
      "Epoch 124 / 500 | iteration 20 / 30 | Total Loss: 6.6598920822143555 | KNN Loss: 5.616001129150391 | BCE Loss: 1.0438909530639648\n",
      "Epoch 124 / 500 | iteration 25 / 30 | Total Loss: 6.655087471008301 | KNN Loss: 5.610037803649902 | BCE Loss: 1.0450494289398193\n",
      "Epoch 125 / 500 | iteration 0 / 30 | Total Loss: 6.665042877197266 | KNN Loss: 5.6531219482421875 | BCE Loss: 1.0119209289550781\n",
      "Epoch 125 / 500 | iteration 5 / 30 | Total Loss: 6.665328502655029 | KNN Loss: 5.60835075378418 | BCE Loss: 1.0569778680801392\n",
      "Epoch 125 / 500 | iteration 10 / 30 | Total Loss: 6.711039066314697 | KNN Loss: 5.671021938323975 | BCE Loss: 1.0400171279907227\n",
      "Epoch 125 / 500 | iteration 15 / 30 | Total Loss: 6.650557041168213 | KNN Loss: 5.607119560241699 | BCE Loss: 1.0434374809265137\n",
      "Epoch 125 / 500 | iteration 20 / 30 | Total Loss: 6.716743469238281 | KNN Loss: 5.636140823364258 | BCE Loss: 1.0806028842926025\n",
      "Epoch 125 / 500 | iteration 25 / 30 | Total Loss: 6.692784786224365 | KNN Loss: 5.625783443450928 | BCE Loss: 1.067001223564148\n",
      "Epoch 126 / 500 | iteration 0 / 30 | Total Loss: 6.702300071716309 | KNN Loss: 5.678134918212891 | BCE Loss: 1.0241649150848389\n",
      "Epoch 126 / 500 | iteration 5 / 30 | Total Loss: 6.683438301086426 | KNN Loss: 5.6365861892700195 | BCE Loss: 1.0468523502349854\n",
      "Epoch 126 / 500 | iteration 10 / 30 | Total Loss: 6.583868026733398 | KNN Loss: 5.592630863189697 | BCE Loss: 0.9912370443344116\n",
      "Epoch 126 / 500 | iteration 15 / 30 | Total Loss: 6.618412017822266 | KNN Loss: 5.590945720672607 | BCE Loss: 1.0274661779403687\n",
      "Epoch 126 / 500 | iteration 20 / 30 | Total Loss: 6.6510396003723145 | KNN Loss: 5.613378047943115 | BCE Loss: 1.0376616716384888\n",
      "Epoch 126 / 500 | iteration 25 / 30 | Total Loss: 6.72752571105957 | KNN Loss: 5.666533946990967 | BCE Loss: 1.0609920024871826\n",
      "Epoch 127 / 500 | iteration 0 / 30 | Total Loss: 6.71588134765625 | KNN Loss: 5.644731521606445 | BCE Loss: 1.0711500644683838\n",
      "Epoch 127 / 500 | iteration 5 / 30 | Total Loss: 6.646697521209717 | KNN Loss: 5.609084606170654 | BCE Loss: 1.0376129150390625\n",
      "Epoch 127 / 500 | iteration 10 / 30 | Total Loss: 6.700948715209961 | KNN Loss: 5.640372276306152 | BCE Loss: 1.0605762004852295\n",
      "Epoch 127 / 500 | iteration 15 / 30 | Total Loss: 6.632541179656982 | KNN Loss: 5.594664573669434 | BCE Loss: 1.0378766059875488\n",
      "Epoch 127 / 500 | iteration 20 / 30 | Total Loss: 6.634227275848389 | KNN Loss: 5.597347736358643 | BCE Loss: 1.036879539489746\n",
      "Epoch 127 / 500 | iteration 25 / 30 | Total Loss: 6.634884834289551 | KNN Loss: 5.630692481994629 | BCE Loss: 1.0041924715042114\n",
      "Epoch 128 / 500 | iteration 0 / 30 | Total Loss: 6.791403770446777 | KNN Loss: 5.696559429168701 | BCE Loss: 1.094844102859497\n",
      "Epoch 128 / 500 | iteration 5 / 30 | Total Loss: 6.67053747177124 | KNN Loss: 5.647657871246338 | BCE Loss: 1.022879719734192\n",
      "Epoch 128 / 500 | iteration 10 / 30 | Total Loss: 6.65950870513916 | KNN Loss: 5.601780891418457 | BCE Loss: 1.0577280521392822\n",
      "Epoch 128 / 500 | iteration 15 / 30 | Total Loss: 6.67148494720459 | KNN Loss: 5.619742393493652 | BCE Loss: 1.0517423152923584\n",
      "Epoch 128 / 500 | iteration 20 / 30 | Total Loss: 6.653592586517334 | KNN Loss: 5.633014678955078 | BCE Loss: 1.0205779075622559\n",
      "Epoch 128 / 500 | iteration 25 / 30 | Total Loss: 6.676031589508057 | KNN Loss: 5.636785507202148 | BCE Loss: 1.0392459630966187\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 129 / 500 | iteration 0 / 30 | Total Loss: 6.710764408111572 | KNN Loss: 5.639047622680664 | BCE Loss: 1.0717167854309082\n",
      "Epoch 129 / 500 | iteration 5 / 30 | Total Loss: 6.645365238189697 | KNN Loss: 5.604659080505371 | BCE Loss: 1.0407062768936157\n",
      "Epoch 129 / 500 | iteration 10 / 30 | Total Loss: 6.652410507202148 | KNN Loss: 5.593637466430664 | BCE Loss: 1.0587730407714844\n",
      "Epoch 129 / 500 | iteration 15 / 30 | Total Loss: 6.699736595153809 | KNN Loss: 5.66482400894165 | BCE Loss: 1.0349125862121582\n",
      "Epoch 129 / 500 | iteration 20 / 30 | Total Loss: 6.66864013671875 | KNN Loss: 5.603036880493164 | BCE Loss: 1.065603256225586\n",
      "Epoch 129 / 500 | iteration 25 / 30 | Total Loss: 6.642287731170654 | KNN Loss: 5.59523868560791 | BCE Loss: 1.0470490455627441\n",
      "Epoch 130 / 500 | iteration 0 / 30 | Total Loss: 6.662049770355225 | KNN Loss: 5.635184288024902 | BCE Loss: 1.0268653631210327\n",
      "Epoch 130 / 500 | iteration 5 / 30 | Total Loss: 6.7150373458862305 | KNN Loss: 5.647828102111816 | BCE Loss: 1.0672094821929932\n",
      "Epoch 130 / 500 | iteration 10 / 30 | Total Loss: 6.656769752502441 | KNN Loss: 5.604568958282471 | BCE Loss: 1.0522005558013916\n",
      "Epoch 130 / 500 | iteration 15 / 30 | Total Loss: 6.709687232971191 | KNN Loss: 5.6495561599731445 | BCE Loss: 1.0601309537887573\n",
      "Epoch 130 / 500 | iteration 20 / 30 | Total Loss: 6.695630073547363 | KNN Loss: 5.640564441680908 | BCE Loss: 1.0550658702850342\n",
      "Epoch 130 / 500 | iteration 25 / 30 | Total Loss: 6.703157901763916 | KNN Loss: 5.619692802429199 | BCE Loss: 1.0834650993347168\n",
      "Epoch 131 / 500 | iteration 0 / 30 | Total Loss: 6.6528096199035645 | KNN Loss: 5.6201629638671875 | BCE Loss: 1.032646656036377\n",
      "Epoch 131 / 500 | iteration 5 / 30 | Total Loss: 6.656680107116699 | KNN Loss: 5.643134117126465 | BCE Loss: 1.013546109199524\n",
      "Epoch 131 / 500 | iteration 10 / 30 | Total Loss: 6.657401084899902 | KNN Loss: 5.600741863250732 | BCE Loss: 1.0566589832305908\n",
      "Epoch 131 / 500 | iteration 15 / 30 | Total Loss: 6.630022048950195 | KNN Loss: 5.587031364440918 | BCE Loss: 1.0429909229278564\n",
      "Epoch 131 / 500 | iteration 20 / 30 | Total Loss: 6.639039993286133 | KNN Loss: 5.603761196136475 | BCE Loss: 1.035278558731079\n",
      "Epoch 131 / 500 | iteration 25 / 30 | Total Loss: 6.668298244476318 | KNN Loss: 5.623844146728516 | BCE Loss: 1.0444542169570923\n",
      "Epoch 132 / 500 | iteration 0 / 30 | Total Loss: 6.7026824951171875 | KNN Loss: 5.640401363372803 | BCE Loss: 1.0622808933258057\n",
      "Epoch 132 / 500 | iteration 5 / 30 | Total Loss: 6.645022869110107 | KNN Loss: 5.599172115325928 | BCE Loss: 1.0458507537841797\n",
      "Epoch 132 / 500 | iteration 10 / 30 | Total Loss: 6.724344253540039 | KNN Loss: 5.679319858551025 | BCE Loss: 1.0450241565704346\n",
      "Epoch 132 / 500 | iteration 15 / 30 | Total Loss: 6.639003753662109 | KNN Loss: 5.612213134765625 | BCE Loss: 1.0267906188964844\n",
      "Epoch 132 / 500 | iteration 20 / 30 | Total Loss: 6.650989532470703 | KNN Loss: 5.609593391418457 | BCE Loss: 1.0413960218429565\n",
      "Epoch 132 / 500 | iteration 25 / 30 | Total Loss: 6.625786304473877 | KNN Loss: 5.595105171203613 | BCE Loss: 1.0306811332702637\n",
      "Epoch 133 / 500 | iteration 0 / 30 | Total Loss: 6.637402534484863 | KNN Loss: 5.590151786804199 | BCE Loss: 1.0472508668899536\n",
      "Epoch 133 / 500 | iteration 5 / 30 | Total Loss: 6.668902397155762 | KNN Loss: 5.596789836883545 | BCE Loss: 1.0721125602722168\n",
      "Epoch 133 / 500 | iteration 10 / 30 | Total Loss: 6.667141914367676 | KNN Loss: 5.606046676635742 | BCE Loss: 1.0610949993133545\n",
      "Epoch 133 / 500 | iteration 15 / 30 | Total Loss: 6.652705192565918 | KNN Loss: 5.613478183746338 | BCE Loss: 1.03922700881958\n",
      "Epoch 133 / 500 | iteration 20 / 30 | Total Loss: 6.640036106109619 | KNN Loss: 5.59371280670166 | BCE Loss: 1.046323299407959\n",
      "Epoch 133 / 500 | iteration 25 / 30 | Total Loss: 6.646852493286133 | KNN Loss: 5.592829704284668 | BCE Loss: 1.0540227890014648\n",
      "Epoch 134 / 500 | iteration 0 / 30 | Total Loss: 6.692071437835693 | KNN Loss: 5.628378868103027 | BCE Loss: 1.0636926889419556\n",
      "Epoch 134 / 500 | iteration 5 / 30 | Total Loss: 6.642124176025391 | KNN Loss: 5.621224403381348 | BCE Loss: 1.0208998918533325\n",
      "Epoch 134 / 500 | iteration 10 / 30 | Total Loss: 6.719824314117432 | KNN Loss: 5.657322883605957 | BCE Loss: 1.0625015497207642\n",
      "Epoch 134 / 500 | iteration 15 / 30 | Total Loss: 6.661823272705078 | KNN Loss: 5.6127095222473145 | BCE Loss: 1.0491137504577637\n",
      "Epoch 134 / 500 | iteration 20 / 30 | Total Loss: 6.647308349609375 | KNN Loss: 5.594021797180176 | BCE Loss: 1.0532867908477783\n",
      "Epoch 134 / 500 | iteration 25 / 30 | Total Loss: 6.680312633514404 | KNN Loss: 5.612431049346924 | BCE Loss: 1.06788170337677\n",
      "Epoch 135 / 500 | iteration 0 / 30 | Total Loss: 6.680102348327637 | KNN Loss: 5.6314802169799805 | BCE Loss: 1.0486223697662354\n",
      "Epoch 135 / 500 | iteration 5 / 30 | Total Loss: 6.661337375640869 | KNN Loss: 5.652271747589111 | BCE Loss: 1.0090655088424683\n",
      "Epoch 135 / 500 | iteration 10 / 30 | Total Loss: 6.6557297706604 | KNN Loss: 5.593176364898682 | BCE Loss: 1.0625534057617188\n",
      "Epoch 135 / 500 | iteration 15 / 30 | Total Loss: 6.681664943695068 | KNN Loss: 5.624597549438477 | BCE Loss: 1.0570672750473022\n",
      "Epoch 135 / 500 | iteration 20 / 30 | Total Loss: 6.817008018493652 | KNN Loss: 5.786620140075684 | BCE Loss: 1.0303877592086792\n",
      "Epoch 135 / 500 | iteration 25 / 30 | Total Loss: 6.660638332366943 | KNN Loss: 5.621978759765625 | BCE Loss: 1.0386595726013184\n",
      "Epoch   136: reducing learning rate of group 0 to 8.4035e-04.\n",
      "Epoch 136 / 500 | iteration 0 / 30 | Total Loss: 6.670871734619141 | KNN Loss: 5.615419387817383 | BCE Loss: 1.0554524660110474\n",
      "Epoch 136 / 500 | iteration 5 / 30 | Total Loss: 6.729616165161133 | KNN Loss: 5.692112922668457 | BCE Loss: 1.0375032424926758\n",
      "Epoch 136 / 500 | iteration 10 / 30 | Total Loss: 6.685232162475586 | KNN Loss: 5.639137268066406 | BCE Loss: 1.0460950136184692\n",
      "Epoch 136 / 500 | iteration 15 / 30 | Total Loss: 6.63979959487915 | KNN Loss: 5.611746788024902 | BCE Loss: 1.028052806854248\n",
      "Epoch 136 / 500 | iteration 20 / 30 | Total Loss: 6.7094316482543945 | KNN Loss: 5.654969692230225 | BCE Loss: 1.0544617176055908\n",
      "Epoch 136 / 500 | iteration 25 / 30 | Total Loss: 6.674273490905762 | KNN Loss: 5.616044998168945 | BCE Loss: 1.0582284927368164\n",
      "Epoch 137 / 500 | iteration 0 / 30 | Total Loss: 6.683403491973877 | KNN Loss: 5.634794235229492 | BCE Loss: 1.0486091375350952\n",
      "Epoch 137 / 500 | iteration 5 / 30 | Total Loss: 6.663732528686523 | KNN Loss: 5.606112480163574 | BCE Loss: 1.0576198101043701\n",
      "Epoch 137 / 500 | iteration 10 / 30 | Total Loss: 6.679429054260254 | KNN Loss: 5.620680332183838 | BCE Loss: 1.058748483657837\n",
      "Epoch 137 / 500 | iteration 15 / 30 | Total Loss: 6.653572082519531 | KNN Loss: 5.622751235961914 | BCE Loss: 1.030820608139038\n",
      "Epoch 137 / 500 | iteration 20 / 30 | Total Loss: 6.63235330581665 | KNN Loss: 5.607328414916992 | BCE Loss: 1.0250248908996582\n",
      "Epoch 137 / 500 | iteration 25 / 30 | Total Loss: 6.719072341918945 | KNN Loss: 5.6457390785217285 | BCE Loss: 1.0733330249786377\n",
      "Epoch 138 / 500 | iteration 0 / 30 | Total Loss: 6.647604465484619 | KNN Loss: 5.5901103019714355 | BCE Loss: 1.0574942827224731\n",
      "Epoch 138 / 500 | iteration 5 / 30 | Total Loss: 6.665175437927246 | KNN Loss: 5.616852760314941 | BCE Loss: 1.0483224391937256\n",
      "Epoch 138 / 500 | iteration 10 / 30 | Total Loss: 6.6738786697387695 | KNN Loss: 5.6198248863220215 | BCE Loss: 1.0540540218353271\n",
      "Epoch 138 / 500 | iteration 15 / 30 | Total Loss: 6.638492107391357 | KNN Loss: 5.598554611206055 | BCE Loss: 1.0399373769760132\n",
      "Epoch 138 / 500 | iteration 20 / 30 | Total Loss: 6.677754878997803 | KNN Loss: 5.630431175231934 | BCE Loss: 1.0473238229751587\n",
      "Epoch 138 / 500 | iteration 25 / 30 | Total Loss: 6.61544132232666 | KNN Loss: 5.601519584655762 | BCE Loss: 1.0139214992523193\n",
      "Epoch 139 / 500 | iteration 0 / 30 | Total Loss: 6.67703914642334 | KNN Loss: 5.6313652992248535 | BCE Loss: 1.0456738471984863\n",
      "Epoch 139 / 500 | iteration 5 / 30 | Total Loss: 6.652902603149414 | KNN Loss: 5.597467422485352 | BCE Loss: 1.0554349422454834\n",
      "Epoch 139 / 500 | iteration 10 / 30 | Total Loss: 6.675262451171875 | KNN Loss: 5.6152873039245605 | BCE Loss: 1.0599753856658936\n",
      "Epoch 139 / 500 | iteration 15 / 30 | Total Loss: 6.645068168640137 | KNN Loss: 5.597754955291748 | BCE Loss: 1.0473129749298096\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 139 / 500 | iteration 20 / 30 | Total Loss: 6.655292510986328 | KNN Loss: 5.612277984619141 | BCE Loss: 1.0430145263671875\n",
      "Epoch 139 / 500 | iteration 25 / 30 | Total Loss: 6.682567596435547 | KNN Loss: 5.64408016204834 | BCE Loss: 1.038487434387207\n",
      "Epoch 140 / 500 | iteration 0 / 30 | Total Loss: 6.677860260009766 | KNN Loss: 5.640701770782471 | BCE Loss: 1.0371583700180054\n",
      "Epoch 140 / 500 | iteration 5 / 30 | Total Loss: 6.692222595214844 | KNN Loss: 5.628222942352295 | BCE Loss: 1.0639994144439697\n",
      "Epoch 140 / 500 | iteration 10 / 30 | Total Loss: 6.6839470863342285 | KNN Loss: 5.628842830657959 | BCE Loss: 1.0551042556762695\n",
      "Epoch 140 / 500 | iteration 15 / 30 | Total Loss: 6.660314083099365 | KNN Loss: 5.618414878845215 | BCE Loss: 1.0418992042541504\n",
      "Epoch 140 / 500 | iteration 20 / 30 | Total Loss: 6.633325576782227 | KNN Loss: 5.598674774169922 | BCE Loss: 1.0346505641937256\n",
      "Epoch 140 / 500 | iteration 25 / 30 | Total Loss: 6.625510215759277 | KNN Loss: 5.595191955566406 | BCE Loss: 1.0303184986114502\n",
      "Epoch 141 / 500 | iteration 0 / 30 | Total Loss: 6.7251715660095215 | KNN Loss: 5.671869277954102 | BCE Loss: 1.05330228805542\n",
      "Epoch 141 / 500 | iteration 5 / 30 | Total Loss: 6.6501970291137695 | KNN Loss: 5.600900650024414 | BCE Loss: 1.0492961406707764\n",
      "Epoch 141 / 500 | iteration 10 / 30 | Total Loss: 6.68276309967041 | KNN Loss: 5.64457368850708 | BCE Loss: 1.038189172744751\n",
      "Epoch 141 / 500 | iteration 15 / 30 | Total Loss: 6.66066837310791 | KNN Loss: 5.610129356384277 | BCE Loss: 1.0505387783050537\n",
      "Epoch 141 / 500 | iteration 20 / 30 | Total Loss: 6.650307655334473 | KNN Loss: 5.605663776397705 | BCE Loss: 1.0446439981460571\n",
      "Epoch 141 / 500 | iteration 25 / 30 | Total Loss: 6.6835036277771 | KNN Loss: 5.601457118988037 | BCE Loss: 1.082046389579773\n",
      "Epoch 142 / 500 | iteration 0 / 30 | Total Loss: 6.656600475311279 | KNN Loss: 5.625176429748535 | BCE Loss: 1.0314240455627441\n",
      "Epoch 142 / 500 | iteration 5 / 30 | Total Loss: 6.7008819580078125 | KNN Loss: 5.634364128112793 | BCE Loss: 1.0665180683135986\n",
      "Epoch 142 / 500 | iteration 10 / 30 | Total Loss: 6.656728744506836 | KNN Loss: 5.625112533569336 | BCE Loss: 1.0316162109375\n",
      "Epoch 142 / 500 | iteration 15 / 30 | Total Loss: 6.67782735824585 | KNN Loss: 5.608706951141357 | BCE Loss: 1.0691204071044922\n",
      "Epoch 142 / 500 | iteration 20 / 30 | Total Loss: 6.656888484954834 | KNN Loss: 5.639108657836914 | BCE Loss: 1.01777982711792\n",
      "Epoch 142 / 500 | iteration 25 / 30 | Total Loss: 6.669931411743164 | KNN Loss: 5.601253509521484 | BCE Loss: 1.0686781406402588\n",
      "Epoch 143 / 500 | iteration 0 / 30 | Total Loss: 6.630560874938965 | KNN Loss: 5.614114284515381 | BCE Loss: 1.016446590423584\n",
      "Epoch 143 / 500 | iteration 5 / 30 | Total Loss: 6.667152404785156 | KNN Loss: 5.62330436706543 | BCE Loss: 1.0438477993011475\n",
      "Epoch 143 / 500 | iteration 10 / 30 | Total Loss: 6.701230049133301 | KNN Loss: 5.649403095245361 | BCE Loss: 1.0518271923065186\n",
      "Epoch 143 / 500 | iteration 15 / 30 | Total Loss: 6.662069320678711 | KNN Loss: 5.597873687744141 | BCE Loss: 1.0641956329345703\n",
      "Epoch 143 / 500 | iteration 20 / 30 | Total Loss: 6.665640354156494 | KNN Loss: 5.600311756134033 | BCE Loss: 1.065328598022461\n",
      "Epoch 143 / 500 | iteration 25 / 30 | Total Loss: 6.62966775894165 | KNN Loss: 5.594151020050049 | BCE Loss: 1.0355167388916016\n",
      "Epoch 144 / 500 | iteration 0 / 30 | Total Loss: 6.612297534942627 | KNN Loss: 5.616006374359131 | BCE Loss: 0.9962912797927856\n",
      "Epoch 144 / 500 | iteration 5 / 30 | Total Loss: 6.655841827392578 | KNN Loss: 5.592226982116699 | BCE Loss: 1.0636146068572998\n",
      "Epoch 144 / 500 | iteration 10 / 30 | Total Loss: 6.7019758224487305 | KNN Loss: 5.616123199462891 | BCE Loss: 1.0858523845672607\n",
      "Epoch 144 / 500 | iteration 15 / 30 | Total Loss: 6.708340644836426 | KNN Loss: 5.6508684158325195 | BCE Loss: 1.0574722290039062\n",
      "Epoch 144 / 500 | iteration 20 / 30 | Total Loss: 6.747768878936768 | KNN Loss: 5.693157196044922 | BCE Loss: 1.0546116828918457\n",
      "Epoch 144 / 500 | iteration 25 / 30 | Total Loss: 6.665657997131348 | KNN Loss: 5.6326003074646 | BCE Loss: 1.033057689666748\n",
      "Epoch 145 / 500 | iteration 0 / 30 | Total Loss: 6.680131435394287 | KNN Loss: 5.625578880310059 | BCE Loss: 1.054552674293518\n",
      "Epoch 145 / 500 | iteration 5 / 30 | Total Loss: 6.725005626678467 | KNN Loss: 5.682662010192871 | BCE Loss: 1.0423434972763062\n",
      "Epoch 145 / 500 | iteration 10 / 30 | Total Loss: 6.651046276092529 | KNN Loss: 5.604490756988525 | BCE Loss: 1.0465556383132935\n",
      "Epoch 145 / 500 | iteration 15 / 30 | Total Loss: 6.631896018981934 | KNN Loss: 5.596782207489014 | BCE Loss: 1.03511381149292\n",
      "Epoch 145 / 500 | iteration 20 / 30 | Total Loss: 6.6834845542907715 | KNN Loss: 5.6198344230651855 | BCE Loss: 1.0636500120162964\n",
      "Epoch 145 / 500 | iteration 25 / 30 | Total Loss: 6.663017749786377 | KNN Loss: 5.601932048797607 | BCE Loss: 1.0610857009887695\n",
      "Epoch 146 / 500 | iteration 0 / 30 | Total Loss: 6.662033557891846 | KNN Loss: 5.597517490386963 | BCE Loss: 1.0645160675048828\n",
      "Epoch 146 / 500 | iteration 5 / 30 | Total Loss: 6.682900428771973 | KNN Loss: 5.644993782043457 | BCE Loss: 1.0379064083099365\n",
      "Epoch 146 / 500 | iteration 10 / 30 | Total Loss: 6.677247524261475 | KNN Loss: 5.611011028289795 | BCE Loss: 1.0662364959716797\n",
      "Epoch 146 / 500 | iteration 15 / 30 | Total Loss: 6.70502233505249 | KNN Loss: 5.64771032333374 | BCE Loss: 1.0573118925094604\n",
      "Epoch 146 / 500 | iteration 20 / 30 | Total Loss: 6.6778340339660645 | KNN Loss: 5.6440911293029785 | BCE Loss: 1.033742904663086\n",
      "Epoch 146 / 500 | iteration 25 / 30 | Total Loss: 6.645755767822266 | KNN Loss: 5.588319778442383 | BCE Loss: 1.057436227798462\n",
      "Epoch   147: reducing learning rate of group 0 to 5.8824e-04.\n",
      "Epoch 147 / 500 | iteration 0 / 30 | Total Loss: 6.671116828918457 | KNN Loss: 5.610788822174072 | BCE Loss: 1.0603277683258057\n",
      "Epoch 147 / 500 | iteration 5 / 30 | Total Loss: 6.6539130210876465 | KNN Loss: 5.604025840759277 | BCE Loss: 1.0498872995376587\n",
      "Epoch 147 / 500 | iteration 10 / 30 | Total Loss: 6.664567947387695 | KNN Loss: 5.594483375549316 | BCE Loss: 1.070084571838379\n",
      "Epoch 147 / 500 | iteration 15 / 30 | Total Loss: 6.69222354888916 | KNN Loss: 5.654627323150635 | BCE Loss: 1.0375962257385254\n",
      "Epoch 147 / 500 | iteration 20 / 30 | Total Loss: 6.685734272003174 | KNN Loss: 5.634273052215576 | BCE Loss: 1.0514612197875977\n",
      "Epoch 147 / 500 | iteration 25 / 30 | Total Loss: 6.650668144226074 | KNN Loss: 5.613204002380371 | BCE Loss: 1.037463903427124\n",
      "Epoch 148 / 500 | iteration 0 / 30 | Total Loss: 6.639729976654053 | KNN Loss: 5.590726375579834 | BCE Loss: 1.0490034818649292\n",
      "Epoch 148 / 500 | iteration 5 / 30 | Total Loss: 6.651206970214844 | KNN Loss: 5.600246429443359 | BCE Loss: 1.050960659980774\n",
      "Epoch 148 / 500 | iteration 10 / 30 | Total Loss: 6.6897687911987305 | KNN Loss: 5.634637832641602 | BCE Loss: 1.0551308393478394\n",
      "Epoch 148 / 500 | iteration 15 / 30 | Total Loss: 6.634438991546631 | KNN Loss: 5.61034631729126 | BCE Loss: 1.024092674255371\n",
      "Epoch 148 / 500 | iteration 20 / 30 | Total Loss: 6.770636081695557 | KNN Loss: 5.720566272735596 | BCE Loss: 1.0500696897506714\n",
      "Epoch 148 / 500 | iteration 25 / 30 | Total Loss: 6.6639838218688965 | KNN Loss: 5.617892265319824 | BCE Loss: 1.0460914373397827\n",
      "Epoch 149 / 500 | iteration 0 / 30 | Total Loss: 6.636658668518066 | KNN Loss: 5.59404993057251 | BCE Loss: 1.0426088571548462\n",
      "Epoch 149 / 500 | iteration 5 / 30 | Total Loss: 6.6512956619262695 | KNN Loss: 5.600903511047363 | BCE Loss: 1.0503920316696167\n",
      "Epoch 149 / 500 | iteration 10 / 30 | Total Loss: 6.676267147064209 | KNN Loss: 5.601576328277588 | BCE Loss: 1.074690818786621\n",
      "Epoch 149 / 500 | iteration 15 / 30 | Total Loss: 6.728445529937744 | KNN Loss: 5.667447566986084 | BCE Loss: 1.0609979629516602\n",
      "Epoch 149 / 500 | iteration 20 / 30 | Total Loss: 6.681098461151123 | KNN Loss: 5.611822128295898 | BCE Loss: 1.069276213645935\n",
      "Epoch 149 / 500 | iteration 25 / 30 | Total Loss: 6.68800687789917 | KNN Loss: 5.620102405548096 | BCE Loss: 1.0679044723510742\n",
      "Epoch 150 / 500 | iteration 0 / 30 | Total Loss: 6.674300670623779 | KNN Loss: 5.625746726989746 | BCE Loss: 1.0485539436340332\n",
      "Epoch 150 / 500 | iteration 5 / 30 | Total Loss: 6.642329692840576 | KNN Loss: 5.6354875564575195 | BCE Loss: 1.0068421363830566\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 150 / 500 | iteration 10 / 30 | Total Loss: 6.6834821701049805 | KNN Loss: 5.620075702667236 | BCE Loss: 1.0634067058563232\n",
      "Epoch 150 / 500 | iteration 15 / 30 | Total Loss: 6.673224925994873 | KNN Loss: 5.6281538009643555 | BCE Loss: 1.0450712442398071\n",
      "Epoch 150 / 500 | iteration 20 / 30 | Total Loss: 6.62900447845459 | KNN Loss: 5.6066203117370605 | BCE Loss: 1.0223839282989502\n",
      "Epoch 150 / 500 | iteration 25 / 30 | Total Loss: 6.703327655792236 | KNN Loss: 5.621386528015137 | BCE Loss: 1.08194100856781\n",
      "Epoch 151 / 500 | iteration 0 / 30 | Total Loss: 6.762264728546143 | KNN Loss: 5.725063800811768 | BCE Loss: 1.0372010469436646\n",
      "Epoch 151 / 500 | iteration 5 / 30 | Total Loss: 6.696443557739258 | KNN Loss: 5.658609390258789 | BCE Loss: 1.0378340482711792\n",
      "Epoch 151 / 500 | iteration 10 / 30 | Total Loss: 6.6345367431640625 | KNN Loss: 5.602720260620117 | BCE Loss: 1.0318162441253662\n",
      "Epoch 151 / 500 | iteration 15 / 30 | Total Loss: 6.686225891113281 | KNN Loss: 5.617949962615967 | BCE Loss: 1.0682756900787354\n",
      "Epoch 151 / 500 | iteration 20 / 30 | Total Loss: 6.631627559661865 | KNN Loss: 5.5960869789123535 | BCE Loss: 1.0355405807495117\n",
      "Epoch 151 / 500 | iteration 25 / 30 | Total Loss: 6.675532341003418 | KNN Loss: 5.631188869476318 | BCE Loss: 1.0443434715270996\n",
      "Epoch 152 / 500 | iteration 0 / 30 | Total Loss: 6.731888771057129 | KNN Loss: 5.685689926147461 | BCE Loss: 1.0461987257003784\n",
      "Epoch 152 / 500 | iteration 5 / 30 | Total Loss: 6.660980224609375 | KNN Loss: 5.606026649475098 | BCE Loss: 1.0549535751342773\n",
      "Epoch 152 / 500 | iteration 10 / 30 | Total Loss: 6.690542221069336 | KNN Loss: 5.6409101486206055 | BCE Loss: 1.049631953239441\n",
      "Epoch 152 / 500 | iteration 15 / 30 | Total Loss: 6.685077667236328 | KNN Loss: 5.608730792999268 | BCE Loss: 1.0763468742370605\n",
      "Epoch 152 / 500 | iteration 20 / 30 | Total Loss: 6.628156661987305 | KNN Loss: 5.599031925201416 | BCE Loss: 1.0291248559951782\n",
      "Epoch 152 / 500 | iteration 25 / 30 | Total Loss: 6.655117988586426 | KNN Loss: 5.618795871734619 | BCE Loss: 1.0363223552703857\n",
      "Epoch 153 / 500 | iteration 0 / 30 | Total Loss: 6.776993274688721 | KNN Loss: 5.705480098724365 | BCE Loss: 1.0715131759643555\n",
      "Epoch 153 / 500 | iteration 5 / 30 | Total Loss: 6.6879377365112305 | KNN Loss: 5.63559627532959 | BCE Loss: 1.0523416996002197\n",
      "Epoch 153 / 500 | iteration 10 / 30 | Total Loss: 6.685610771179199 | KNN Loss: 5.609460353851318 | BCE Loss: 1.07615065574646\n",
      "Epoch 153 / 500 | iteration 15 / 30 | Total Loss: 6.650683403015137 | KNN Loss: 5.594622611999512 | BCE Loss: 1.056060552597046\n",
      "Epoch 153 / 500 | iteration 20 / 30 | Total Loss: 6.646176815032959 | KNN Loss: 5.595767974853516 | BCE Loss: 1.0504088401794434\n",
      "Epoch 153 / 500 | iteration 25 / 30 | Total Loss: 6.7599029541015625 | KNN Loss: 5.722446441650391 | BCE Loss: 1.037456750869751\n",
      "Epoch 154 / 500 | iteration 0 / 30 | Total Loss: 6.71638298034668 | KNN Loss: 5.687026500701904 | BCE Loss: 1.0293564796447754\n",
      "Epoch 154 / 500 | iteration 5 / 30 | Total Loss: 6.697296142578125 | KNN Loss: 5.625824451446533 | BCE Loss: 1.0714714527130127\n",
      "Epoch 154 / 500 | iteration 10 / 30 | Total Loss: 6.645832061767578 | KNN Loss: 5.617469787597656 | BCE Loss: 1.0283620357513428\n",
      "Epoch 154 / 500 | iteration 15 / 30 | Total Loss: 6.640204906463623 | KNN Loss: 5.594723701477051 | BCE Loss: 1.0454810857772827\n",
      "Epoch 154 / 500 | iteration 20 / 30 | Total Loss: 6.618435859680176 | KNN Loss: 5.6035237312316895 | BCE Loss: 1.0149121284484863\n",
      "Epoch 154 / 500 | iteration 25 / 30 | Total Loss: 6.66038179397583 | KNN Loss: 5.5958099365234375 | BCE Loss: 1.064571738243103\n",
      "Epoch 155 / 500 | iteration 0 / 30 | Total Loss: 6.697244644165039 | KNN Loss: 5.652910232543945 | BCE Loss: 1.0443341732025146\n",
      "Epoch 155 / 500 | iteration 5 / 30 | Total Loss: 6.689311504364014 | KNN Loss: 5.6417555809021 | BCE Loss: 1.047555923461914\n",
      "Epoch 155 / 500 | iteration 10 / 30 | Total Loss: 6.728009223937988 | KNN Loss: 5.676809310913086 | BCE Loss: 1.0512001514434814\n",
      "Epoch 155 / 500 | iteration 15 / 30 | Total Loss: 6.726354122161865 | KNN Loss: 5.690304279327393 | BCE Loss: 1.036049723625183\n",
      "Epoch 155 / 500 | iteration 20 / 30 | Total Loss: 6.724948883056641 | KNN Loss: 5.664370536804199 | BCE Loss: 1.0605781078338623\n",
      "Epoch 155 / 500 | iteration 25 / 30 | Total Loss: 6.707083702087402 | KNN Loss: 5.669929027557373 | BCE Loss: 1.0371549129486084\n",
      "Epoch 156 / 500 | iteration 0 / 30 | Total Loss: 6.663641929626465 | KNN Loss: 5.6132636070251465 | BCE Loss: 1.0503780841827393\n",
      "Epoch 156 / 500 | iteration 5 / 30 | Total Loss: 6.631348609924316 | KNN Loss: 5.5905632972717285 | BCE Loss: 1.040785551071167\n",
      "Epoch 156 / 500 | iteration 10 / 30 | Total Loss: 6.643152236938477 | KNN Loss: 5.602664947509766 | BCE Loss: 1.040487289428711\n",
      "Epoch 156 / 500 | iteration 15 / 30 | Total Loss: 6.6623101234436035 | KNN Loss: 5.630728721618652 | BCE Loss: 1.0315815210342407\n",
      "Epoch 156 / 500 | iteration 20 / 30 | Total Loss: 6.663300514221191 | KNN Loss: 5.589201927185059 | BCE Loss: 1.0740983486175537\n",
      "Epoch 156 / 500 | iteration 25 / 30 | Total Loss: 6.660879135131836 | KNN Loss: 5.622775077819824 | BCE Loss: 1.0381038188934326\n",
      "Epoch 157 / 500 | iteration 0 / 30 | Total Loss: 6.711334705352783 | KNN Loss: 5.670234680175781 | BCE Loss: 1.0410999059677124\n",
      "Epoch 157 / 500 | iteration 5 / 30 | Total Loss: 6.632901191711426 | KNN Loss: 5.599630832672119 | BCE Loss: 1.0332701206207275\n",
      "Epoch 157 / 500 | iteration 10 / 30 | Total Loss: 6.695136070251465 | KNN Loss: 5.627673625946045 | BCE Loss: 1.0674625635147095\n",
      "Epoch 157 / 500 | iteration 15 / 30 | Total Loss: 6.699462890625 | KNN Loss: 5.657942771911621 | BCE Loss: 1.041520118713379\n",
      "Epoch 157 / 500 | iteration 20 / 30 | Total Loss: 6.660931587219238 | KNN Loss: 5.636303901672363 | BCE Loss: 1.0246275663375854\n",
      "Epoch 157 / 500 | iteration 25 / 30 | Total Loss: 6.691269397735596 | KNN Loss: 5.604280471801758 | BCE Loss: 1.086988925933838\n",
      "Epoch   158: reducing learning rate of group 0 to 4.1177e-04.\n",
      "Epoch 158 / 500 | iteration 0 / 30 | Total Loss: 6.665496349334717 | KNN Loss: 5.605574607849121 | BCE Loss: 1.0599217414855957\n",
      "Epoch 158 / 500 | iteration 5 / 30 | Total Loss: 6.689793109893799 | KNN Loss: 5.599093914031982 | BCE Loss: 1.0906991958618164\n",
      "Epoch 158 / 500 | iteration 10 / 30 | Total Loss: 6.639214038848877 | KNN Loss: 5.599081516265869 | BCE Loss: 1.0401324033737183\n",
      "Epoch 158 / 500 | iteration 15 / 30 | Total Loss: 6.7292070388793945 | KNN Loss: 5.702544689178467 | BCE Loss: 1.0266623497009277\n",
      "Epoch 158 / 500 | iteration 20 / 30 | Total Loss: 6.644253730773926 | KNN Loss: 5.61065673828125 | BCE Loss: 1.0335967540740967\n",
      "Epoch 158 / 500 | iteration 25 / 30 | Total Loss: 6.680811882019043 | KNN Loss: 5.6400957107543945 | BCE Loss: 1.0407159328460693\n",
      "Epoch 159 / 500 | iteration 0 / 30 | Total Loss: 6.620606422424316 | KNN Loss: 5.597113132476807 | BCE Loss: 1.0234932899475098\n",
      "Epoch 159 / 500 | iteration 5 / 30 | Total Loss: 6.686134338378906 | KNN Loss: 5.629108905792236 | BCE Loss: 1.05702543258667\n",
      "Epoch 159 / 500 | iteration 10 / 30 | Total Loss: 6.693303108215332 | KNN Loss: 5.611964702606201 | BCE Loss: 1.0813381671905518\n",
      "Epoch 159 / 500 | iteration 15 / 30 | Total Loss: 6.6748223304748535 | KNN Loss: 5.6318864822387695 | BCE Loss: 1.042935848236084\n",
      "Epoch 159 / 500 | iteration 20 / 30 | Total Loss: 6.628149509429932 | KNN Loss: 5.593231201171875 | BCE Loss: 1.0349183082580566\n",
      "Epoch 159 / 500 | iteration 25 / 30 | Total Loss: 6.665599822998047 | KNN Loss: 5.612952709197998 | BCE Loss: 1.052647352218628\n",
      "Epoch 160 / 500 | iteration 0 / 30 | Total Loss: 6.730400085449219 | KNN Loss: 5.684182167053223 | BCE Loss: 1.046217679977417\n",
      "Epoch 160 / 500 | iteration 5 / 30 | Total Loss: 6.746989727020264 | KNN Loss: 5.684059143066406 | BCE Loss: 1.062930703163147\n",
      "Epoch 160 / 500 | iteration 10 / 30 | Total Loss: 6.731013774871826 | KNN Loss: 5.679035663604736 | BCE Loss: 1.0519779920578003\n",
      "Epoch 160 / 500 | iteration 15 / 30 | Total Loss: 6.703473091125488 | KNN Loss: 5.655407428741455 | BCE Loss: 1.0480656623840332\n",
      "Epoch 160 / 500 | iteration 20 / 30 | Total Loss: 6.735836505889893 | KNN Loss: 5.669721603393555 | BCE Loss: 1.0661147832870483\n",
      "Epoch 160 / 500 | iteration 25 / 30 | Total Loss: 6.685501575469971 | KNN Loss: 5.634076118469238 | BCE Loss: 1.0514254570007324\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 161 / 500 | iteration 0 / 30 | Total Loss: 6.643012046813965 | KNN Loss: 5.593273162841797 | BCE Loss: 1.049739122390747\n",
      "Epoch 161 / 500 | iteration 5 / 30 | Total Loss: 6.7042155265808105 | KNN Loss: 5.6444549560546875 | BCE Loss: 1.059760570526123\n",
      "Epoch 161 / 500 | iteration 10 / 30 | Total Loss: 6.652129173278809 | KNN Loss: 5.60851526260376 | BCE Loss: 1.043614149093628\n",
      "Epoch 161 / 500 | iteration 15 / 30 | Total Loss: 6.691746234893799 | KNN Loss: 5.631418704986572 | BCE Loss: 1.0603275299072266\n",
      "Epoch 161 / 500 | iteration 20 / 30 | Total Loss: 6.6456475257873535 | KNN Loss: 5.615178108215332 | BCE Loss: 1.0304694175720215\n",
      "Epoch 161 / 500 | iteration 25 / 30 | Total Loss: 6.675307273864746 | KNN Loss: 5.622501850128174 | BCE Loss: 1.0528051853179932\n",
      "Epoch 162 / 500 | iteration 0 / 30 | Total Loss: 6.678328514099121 | KNN Loss: 5.60626220703125 | BCE Loss: 1.072066068649292\n",
      "Epoch 162 / 500 | iteration 5 / 30 | Total Loss: 6.654244422912598 | KNN Loss: 5.621358394622803 | BCE Loss: 1.0328857898712158\n",
      "Epoch 162 / 500 | iteration 10 / 30 | Total Loss: 6.642892837524414 | KNN Loss: 5.59113883972168 | BCE Loss: 1.0517542362213135\n",
      "Epoch 162 / 500 | iteration 15 / 30 | Total Loss: 6.667868614196777 | KNN Loss: 5.605215072631836 | BCE Loss: 1.0626533031463623\n",
      "Epoch 162 / 500 | iteration 20 / 30 | Total Loss: 6.628695011138916 | KNN Loss: 5.591458320617676 | BCE Loss: 1.0372366905212402\n",
      "Epoch 162 / 500 | iteration 25 / 30 | Total Loss: 6.646246910095215 | KNN Loss: 5.593034744262695 | BCE Loss: 1.0532121658325195\n",
      "Epoch 163 / 500 | iteration 0 / 30 | Total Loss: 6.74124002456665 | KNN Loss: 5.709510326385498 | BCE Loss: 1.0317296981811523\n",
      "Epoch 163 / 500 | iteration 5 / 30 | Total Loss: 6.650251865386963 | KNN Loss: 5.62980842590332 | BCE Loss: 1.0204434394836426\n",
      "Epoch 163 / 500 | iteration 10 / 30 | Total Loss: 6.645323753356934 | KNN Loss: 5.622134685516357 | BCE Loss: 1.0231893062591553\n",
      "Epoch 163 / 500 | iteration 15 / 30 | Total Loss: 6.651410102844238 | KNN Loss: 5.634302616119385 | BCE Loss: 1.0171077251434326\n",
      "Epoch 163 / 500 | iteration 20 / 30 | Total Loss: 6.764984130859375 | KNN Loss: 5.69343900680542 | BCE Loss: 1.071545124053955\n",
      "Epoch 163 / 500 | iteration 25 / 30 | Total Loss: 6.73082160949707 | KNN Loss: 5.706313610076904 | BCE Loss: 1.0245082378387451\n",
      "Epoch 164 / 500 | iteration 0 / 30 | Total Loss: 6.6881890296936035 | KNN Loss: 5.650301933288574 | BCE Loss: 1.0378869771957397\n",
      "Epoch 164 / 500 | iteration 5 / 30 | Total Loss: 6.665011882781982 | KNN Loss: 5.618566989898682 | BCE Loss: 1.0464448928833008\n",
      "Epoch 164 / 500 | iteration 10 / 30 | Total Loss: 6.652020454406738 | KNN Loss: 5.611184597015381 | BCE Loss: 1.0408360958099365\n",
      "Epoch 164 / 500 | iteration 15 / 30 | Total Loss: 6.67173957824707 | KNN Loss: 5.628514766693115 | BCE Loss: 1.0432249307632446\n",
      "Epoch 164 / 500 | iteration 20 / 30 | Total Loss: 6.636945724487305 | KNN Loss: 5.61240291595459 | BCE Loss: 1.0245428085327148\n",
      "Epoch 164 / 500 | iteration 25 / 30 | Total Loss: 6.702533721923828 | KNN Loss: 5.626843452453613 | BCE Loss: 1.0756900310516357\n",
      "Epoch 165 / 500 | iteration 0 / 30 | Total Loss: 6.6428303718566895 | KNN Loss: 5.621859550476074 | BCE Loss: 1.0209708213806152\n",
      "Epoch 165 / 500 | iteration 5 / 30 | Total Loss: 6.652419090270996 | KNN Loss: 5.599092960357666 | BCE Loss: 1.0533263683319092\n",
      "Epoch 165 / 500 | iteration 10 / 30 | Total Loss: 6.635144233703613 | KNN Loss: 5.599869251251221 | BCE Loss: 1.0352752208709717\n",
      "Epoch 165 / 500 | iteration 15 / 30 | Total Loss: 6.5851640701293945 | KNN Loss: 5.589784145355225 | BCE Loss: 0.9953798055648804\n",
      "Epoch 165 / 500 | iteration 20 / 30 | Total Loss: 6.655608654022217 | KNN Loss: 5.5988078117370605 | BCE Loss: 1.0568008422851562\n",
      "Epoch 165 / 500 | iteration 25 / 30 | Total Loss: 6.697381496429443 | KNN Loss: 5.617908477783203 | BCE Loss: 1.0794730186462402\n",
      "Epoch 166 / 500 | iteration 0 / 30 | Total Loss: 6.668152332305908 | KNN Loss: 5.650410175323486 | BCE Loss: 1.0177421569824219\n",
      "Epoch 166 / 500 | iteration 5 / 30 | Total Loss: 6.641848564147949 | KNN Loss: 5.596497535705566 | BCE Loss: 1.0453507900238037\n",
      "Epoch 166 / 500 | iteration 10 / 30 | Total Loss: 6.747715950012207 | KNN Loss: 5.692286968231201 | BCE Loss: 1.0554288625717163\n",
      "Epoch 166 / 500 | iteration 15 / 30 | Total Loss: 6.650498390197754 | KNN Loss: 5.590666770935059 | BCE Loss: 1.0598313808441162\n",
      "Epoch 166 / 500 | iteration 20 / 30 | Total Loss: 6.667546272277832 | KNN Loss: 5.608230113983154 | BCE Loss: 1.0593159198760986\n",
      "Epoch 166 / 500 | iteration 25 / 30 | Total Loss: 6.670530319213867 | KNN Loss: 5.649172782897949 | BCE Loss: 1.0213576555252075\n",
      "Epoch 167 / 500 | iteration 0 / 30 | Total Loss: 6.6835761070251465 | KNN Loss: 5.613149166107178 | BCE Loss: 1.0704268217086792\n",
      "Epoch 167 / 500 | iteration 5 / 30 | Total Loss: 6.6854400634765625 | KNN Loss: 5.623536109924316 | BCE Loss: 1.0619040727615356\n",
      "Epoch 167 / 500 | iteration 10 / 30 | Total Loss: 6.6361188888549805 | KNN Loss: 5.590056419372559 | BCE Loss: 1.0460622310638428\n",
      "Epoch 167 / 500 | iteration 15 / 30 | Total Loss: 6.669723033905029 | KNN Loss: 5.639618873596191 | BCE Loss: 1.030104160308838\n",
      "Epoch 167 / 500 | iteration 20 / 30 | Total Loss: 6.6871442794799805 | KNN Loss: 5.6396002769470215 | BCE Loss: 1.0475437641143799\n",
      "Epoch 167 / 500 | iteration 25 / 30 | Total Loss: 6.706051826477051 | KNN Loss: 5.6651740074157715 | BCE Loss: 1.0408776998519897\n",
      "Epoch 168 / 500 | iteration 0 / 30 | Total Loss: 6.736861705780029 | KNN Loss: 5.669862747192383 | BCE Loss: 1.066999077796936\n",
      "Epoch 168 / 500 | iteration 5 / 30 | Total Loss: 6.713793754577637 | KNN Loss: 5.653383731842041 | BCE Loss: 1.0604100227355957\n",
      "Epoch 168 / 500 | iteration 10 / 30 | Total Loss: 6.653134822845459 | KNN Loss: 5.620113372802734 | BCE Loss: 1.0330214500427246\n",
      "Epoch 168 / 500 | iteration 15 / 30 | Total Loss: 6.6904497146606445 | KNN Loss: 5.604172229766846 | BCE Loss: 1.0862772464752197\n",
      "Epoch 168 / 500 | iteration 20 / 30 | Total Loss: 6.644883155822754 | KNN Loss: 5.595106601715088 | BCE Loss: 1.049776315689087\n",
      "Epoch 168 / 500 | iteration 25 / 30 | Total Loss: 6.633763313293457 | KNN Loss: 5.605465888977051 | BCE Loss: 1.0282973051071167\n",
      "Epoch   169: reducing learning rate of group 0 to 2.8824e-04.\n",
      "Epoch 169 / 500 | iteration 0 / 30 | Total Loss: 6.746106147766113 | KNN Loss: 5.712385177612305 | BCE Loss: 1.0337209701538086\n",
      "Epoch 169 / 500 | iteration 5 / 30 | Total Loss: 6.693085193634033 | KNN Loss: 5.620480537414551 | BCE Loss: 1.072604775428772\n",
      "Epoch 169 / 500 | iteration 10 / 30 | Total Loss: 6.624707221984863 | KNN Loss: 5.591172218322754 | BCE Loss: 1.0335352420806885\n",
      "Epoch 169 / 500 | iteration 15 / 30 | Total Loss: 6.659203052520752 | KNN Loss: 5.607458114624023 | BCE Loss: 1.051745057106018\n",
      "Epoch 169 / 500 | iteration 20 / 30 | Total Loss: 6.633901596069336 | KNN Loss: 5.6027913093566895 | BCE Loss: 1.031110167503357\n",
      "Epoch 169 / 500 | iteration 25 / 30 | Total Loss: 6.770982265472412 | KNN Loss: 5.711790561676025 | BCE Loss: 1.0591917037963867\n",
      "Epoch 170 / 500 | iteration 0 / 30 | Total Loss: 6.664377212524414 | KNN Loss: 5.613487720489502 | BCE Loss: 1.050889253616333\n",
      "Epoch 170 / 500 | iteration 5 / 30 | Total Loss: 6.709027290344238 | KNN Loss: 5.6792120933532715 | BCE Loss: 1.0298151969909668\n",
      "Epoch 170 / 500 | iteration 10 / 30 | Total Loss: 6.669968605041504 | KNN Loss: 5.625985145568848 | BCE Loss: 1.0439835786819458\n",
      "Epoch 170 / 500 | iteration 15 / 30 | Total Loss: 6.630309581756592 | KNN Loss: 5.609586715698242 | BCE Loss: 1.0207228660583496\n",
      "Epoch 170 / 500 | iteration 20 / 30 | Total Loss: 6.618450164794922 | KNN Loss: 5.594903469085693 | BCE Loss: 1.023546576499939\n",
      "Epoch 170 / 500 | iteration 25 / 30 | Total Loss: 6.655254364013672 | KNN Loss: 5.599183559417725 | BCE Loss: 1.0560708045959473\n",
      "Epoch 171 / 500 | iteration 0 / 30 | Total Loss: 6.676059722900391 | KNN Loss: 5.616728782653809 | BCE Loss: 1.059330701828003\n",
      "Epoch 171 / 500 | iteration 5 / 30 | Total Loss: 6.644442558288574 | KNN Loss: 5.600185871124268 | BCE Loss: 1.0442564487457275\n",
      "Epoch 171 / 500 | iteration 10 / 30 | Total Loss: 6.633213996887207 | KNN Loss: 5.596888542175293 | BCE Loss: 1.0363256931304932\n",
      "Epoch 171 / 500 | iteration 15 / 30 | Total Loss: 6.641535758972168 | KNN Loss: 5.596739292144775 | BCE Loss: 1.0447965860366821\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 171 / 500 | iteration 20 / 30 | Total Loss: 6.627845287322998 | KNN Loss: 5.598957538604736 | BCE Loss: 1.0288878679275513\n",
      "Epoch 171 / 500 | iteration 25 / 30 | Total Loss: 6.625782012939453 | KNN Loss: 5.590404510498047 | BCE Loss: 1.0353777408599854\n",
      "Epoch 172 / 500 | iteration 0 / 30 | Total Loss: 6.681704998016357 | KNN Loss: 5.623116493225098 | BCE Loss: 1.0585883855819702\n",
      "Epoch 172 / 500 | iteration 5 / 30 | Total Loss: 6.660345554351807 | KNN Loss: 5.609485626220703 | BCE Loss: 1.0508599281311035\n",
      "Epoch 172 / 500 | iteration 10 / 30 | Total Loss: 6.68867826461792 | KNN Loss: 5.630383491516113 | BCE Loss: 1.0582947731018066\n",
      "Epoch 172 / 500 | iteration 15 / 30 | Total Loss: 6.661768913269043 | KNN Loss: 5.612947463989258 | BCE Loss: 1.0488216876983643\n",
      "Epoch 172 / 500 | iteration 20 / 30 | Total Loss: 6.63248872756958 | KNN Loss: 5.606969833374023 | BCE Loss: 1.0255188941955566\n",
      "Epoch 172 / 500 | iteration 25 / 30 | Total Loss: 6.677586078643799 | KNN Loss: 5.62206506729126 | BCE Loss: 1.055521011352539\n",
      "Epoch 173 / 500 | iteration 0 / 30 | Total Loss: 6.724685192108154 | KNN Loss: 5.679728031158447 | BCE Loss: 1.044957160949707\n",
      "Epoch 173 / 500 | iteration 5 / 30 | Total Loss: 6.657754898071289 | KNN Loss: 5.620269298553467 | BCE Loss: 1.0374853610992432\n",
      "Epoch 173 / 500 | iteration 10 / 30 | Total Loss: 6.682931423187256 | KNN Loss: 5.603671550750732 | BCE Loss: 1.0792597532272339\n",
      "Epoch 173 / 500 | iteration 15 / 30 | Total Loss: 6.6681928634643555 | KNN Loss: 5.607542037963867 | BCE Loss: 1.0606510639190674\n",
      "Epoch 173 / 500 | iteration 20 / 30 | Total Loss: 6.645867347717285 | KNN Loss: 5.597070693969727 | BCE Loss: 1.048796534538269\n",
      "Epoch 173 / 500 | iteration 25 / 30 | Total Loss: 6.6280670166015625 | KNN Loss: 5.603816509246826 | BCE Loss: 1.0242502689361572\n",
      "Epoch 174 / 500 | iteration 0 / 30 | Total Loss: 6.653851509094238 | KNN Loss: 5.614903450012207 | BCE Loss: 1.0389482975006104\n",
      "Epoch 174 / 500 | iteration 5 / 30 | Total Loss: 6.739278316497803 | KNN Loss: 5.678822040557861 | BCE Loss: 1.060456395149231\n",
      "Epoch 174 / 500 | iteration 10 / 30 | Total Loss: 6.696220397949219 | KNN Loss: 5.657566070556641 | BCE Loss: 1.0386545658111572\n",
      "Epoch 174 / 500 | iteration 15 / 30 | Total Loss: 6.667943000793457 | KNN Loss: 5.599273681640625 | BCE Loss: 1.068669319152832\n",
      "Epoch 174 / 500 | iteration 20 / 30 | Total Loss: 6.657115459442139 | KNN Loss: 5.592465877532959 | BCE Loss: 1.0646494626998901\n",
      "Epoch 174 / 500 | iteration 25 / 30 | Total Loss: 6.642393112182617 | KNN Loss: 5.592727184295654 | BCE Loss: 1.049665927886963\n",
      "Epoch 175 / 500 | iteration 0 / 30 | Total Loss: 6.647060394287109 | KNN Loss: 5.61051607131958 | BCE Loss: 1.0365445613861084\n",
      "Epoch 175 / 500 | iteration 5 / 30 | Total Loss: 6.683109283447266 | KNN Loss: 5.628785133361816 | BCE Loss: 1.0543243885040283\n",
      "Epoch 175 / 500 | iteration 10 / 30 | Total Loss: 6.619995594024658 | KNN Loss: 5.603646278381348 | BCE Loss: 1.0163493156433105\n",
      "Epoch 175 / 500 | iteration 15 / 30 | Total Loss: 6.676135540008545 | KNN Loss: 5.608992576599121 | BCE Loss: 1.0671429634094238\n",
      "Epoch 175 / 500 | iteration 20 / 30 | Total Loss: 6.723099231719971 | KNN Loss: 5.6947407722473145 | BCE Loss: 1.0283583402633667\n",
      "Epoch 175 / 500 | iteration 25 / 30 | Total Loss: 6.6876420974731445 | KNN Loss: 5.60732364654541 | BCE Loss: 1.0803182125091553\n",
      "Epoch 176 / 500 | iteration 0 / 30 | Total Loss: 6.655799388885498 | KNN Loss: 5.5968499183654785 | BCE Loss: 1.0589494705200195\n",
      "Epoch 176 / 500 | iteration 5 / 30 | Total Loss: 6.74750280380249 | KNN Loss: 5.70125150680542 | BCE Loss: 1.0462512969970703\n",
      "Epoch 176 / 500 | iteration 10 / 30 | Total Loss: 6.716553688049316 | KNN Loss: 5.658053398132324 | BCE Loss: 1.0585005283355713\n",
      "Epoch 176 / 500 | iteration 15 / 30 | Total Loss: 6.719442367553711 | KNN Loss: 5.660215854644775 | BCE Loss: 1.0592265129089355\n",
      "Epoch 176 / 500 | iteration 20 / 30 | Total Loss: 6.699428081512451 | KNN Loss: 5.647857189178467 | BCE Loss: 1.0515708923339844\n",
      "Epoch 176 / 500 | iteration 25 / 30 | Total Loss: 6.628076076507568 | KNN Loss: 5.605835914611816 | BCE Loss: 1.022240161895752\n",
      "Epoch 177 / 500 | iteration 0 / 30 | Total Loss: 6.693835735321045 | KNN Loss: 5.6442036628723145 | BCE Loss: 1.0496320724487305\n",
      "Epoch 177 / 500 | iteration 5 / 30 | Total Loss: 6.734786033630371 | KNN Loss: 5.710813045501709 | BCE Loss: 1.023972988128662\n",
      "Epoch 177 / 500 | iteration 10 / 30 | Total Loss: 6.621720314025879 | KNN Loss: 5.595119476318359 | BCE Loss: 1.0266010761260986\n",
      "Epoch 177 / 500 | iteration 15 / 30 | Total Loss: 6.670291423797607 | KNN Loss: 5.61246395111084 | BCE Loss: 1.0578274726867676\n",
      "Epoch 177 / 500 | iteration 20 / 30 | Total Loss: 6.646323204040527 | KNN Loss: 5.593300819396973 | BCE Loss: 1.0530221462249756\n",
      "Epoch 177 / 500 | iteration 25 / 30 | Total Loss: 6.652057647705078 | KNN Loss: 5.60446310043335 | BCE Loss: 1.0475945472717285\n",
      "Epoch 178 / 500 | iteration 0 / 30 | Total Loss: 6.647632122039795 | KNN Loss: 5.602339744567871 | BCE Loss: 1.0452924966812134\n",
      "Epoch 178 / 500 | iteration 5 / 30 | Total Loss: 6.6459550857543945 | KNN Loss: 5.618402481079102 | BCE Loss: 1.0275523662567139\n",
      "Epoch 178 / 500 | iteration 10 / 30 | Total Loss: 6.694954872131348 | KNN Loss: 5.6519951820373535 | BCE Loss: 1.0429599285125732\n",
      "Epoch 178 / 500 | iteration 15 / 30 | Total Loss: 6.666913032531738 | KNN Loss: 5.5948100090026855 | BCE Loss: 1.0721027851104736\n",
      "Epoch 178 / 500 | iteration 20 / 30 | Total Loss: 6.671092987060547 | KNN Loss: 5.623763561248779 | BCE Loss: 1.047329306602478\n",
      "Epoch 178 / 500 | iteration 25 / 30 | Total Loss: 6.691111087799072 | KNN Loss: 5.630378246307373 | BCE Loss: 1.0607328414916992\n",
      "Epoch 179 / 500 | iteration 0 / 30 | Total Loss: 6.659987449645996 | KNN Loss: 5.613354682922363 | BCE Loss: 1.0466326475143433\n",
      "Epoch 179 / 500 | iteration 5 / 30 | Total Loss: 6.6815032958984375 | KNN Loss: 5.633634567260742 | BCE Loss: 1.0478689670562744\n",
      "Epoch 179 / 500 | iteration 10 / 30 | Total Loss: 6.634978771209717 | KNN Loss: 5.595021724700928 | BCE Loss: 1.039957046508789\n",
      "Epoch 179 / 500 | iteration 15 / 30 | Total Loss: 6.701970100402832 | KNN Loss: 5.635282039642334 | BCE Loss: 1.066687822341919\n",
      "Epoch 179 / 500 | iteration 20 / 30 | Total Loss: 6.644536972045898 | KNN Loss: 5.603814125061035 | BCE Loss: 1.0407226085662842\n",
      "Epoch 179 / 500 | iteration 25 / 30 | Total Loss: 6.7729644775390625 | KNN Loss: 5.707178115844727 | BCE Loss: 1.065786600112915\n",
      "Epoch 180 / 500 | iteration 0 / 30 | Total Loss: 6.646559238433838 | KNN Loss: 5.591325759887695 | BCE Loss: 1.055233359336853\n",
      "Epoch 180 / 500 | iteration 5 / 30 | Total Loss: 6.677034854888916 | KNN Loss: 5.627527236938477 | BCE Loss: 1.049507737159729\n",
      "Epoch 180 / 500 | iteration 10 / 30 | Total Loss: 6.718001842498779 | KNN Loss: 5.65191650390625 | BCE Loss: 1.0660853385925293\n",
      "Epoch 180 / 500 | iteration 15 / 30 | Total Loss: 6.605061054229736 | KNN Loss: 5.599348545074463 | BCE Loss: 1.005712628364563\n",
      "Epoch 180 / 500 | iteration 20 / 30 | Total Loss: 6.676973342895508 | KNN Loss: 5.625086784362793 | BCE Loss: 1.0518864393234253\n",
      "Epoch 180 / 500 | iteration 25 / 30 | Total Loss: 6.691340446472168 | KNN Loss: 5.643604755401611 | BCE Loss: 1.0477354526519775\n",
      "Epoch 181 / 500 | iteration 0 / 30 | Total Loss: 6.651296615600586 | KNN Loss: 5.611184597015381 | BCE Loss: 1.040111780166626\n",
      "Epoch 181 / 500 | iteration 5 / 30 | Total Loss: 6.744510650634766 | KNN Loss: 5.677032470703125 | BCE Loss: 1.0674781799316406\n",
      "Epoch 181 / 500 | iteration 10 / 30 | Total Loss: 6.666831970214844 | KNN Loss: 5.603333950042725 | BCE Loss: 1.0634980201721191\n",
      "Epoch 181 / 500 | iteration 15 / 30 | Total Loss: 6.636508464813232 | KNN Loss: 5.592864036560059 | BCE Loss: 1.0436444282531738\n",
      "Epoch 181 / 500 | iteration 20 / 30 | Total Loss: 6.723258972167969 | KNN Loss: 5.68787145614624 | BCE Loss: 1.0353877544403076\n",
      "Epoch 181 / 500 | iteration 25 / 30 | Total Loss: 6.658890724182129 | KNN Loss: 5.618210792541504 | BCE Loss: 1.040680170059204\n",
      "Epoch 182 / 500 | iteration 0 / 30 | Total Loss: 6.692039966583252 | KNN Loss: 5.6362738609313965 | BCE Loss: 1.055766224861145\n",
      "Epoch 182 / 500 | iteration 5 / 30 | Total Loss: 6.658616065979004 | KNN Loss: 5.610738277435303 | BCE Loss: 1.0478780269622803\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 182 / 500 | iteration 10 / 30 | Total Loss: 6.681608200073242 | KNN Loss: 5.62089204788208 | BCE Loss: 1.0607163906097412\n",
      "Epoch 182 / 500 | iteration 15 / 30 | Total Loss: 6.642932891845703 | KNN Loss: 5.600748538970947 | BCE Loss: 1.0421844720840454\n",
      "Epoch 182 / 500 | iteration 20 / 30 | Total Loss: 6.660069465637207 | KNN Loss: 5.605281829833984 | BCE Loss: 1.0547873973846436\n",
      "Epoch 182 / 500 | iteration 25 / 30 | Total Loss: 6.716601848602295 | KNN Loss: 5.672891616821289 | BCE Loss: 1.0437101125717163\n",
      "Epoch   183: reducing learning rate of group 0 to 2.0177e-04.\n",
      "Epoch 183 / 500 | iteration 0 / 30 | Total Loss: 6.65536642074585 | KNN Loss: 5.608720779418945 | BCE Loss: 1.0466456413269043\n",
      "Epoch 183 / 500 | iteration 5 / 30 | Total Loss: 6.672023773193359 | KNN Loss: 5.629927635192871 | BCE Loss: 1.0420962572097778\n",
      "Epoch 183 / 500 | iteration 10 / 30 | Total Loss: 6.65839147567749 | KNN Loss: 5.632142066955566 | BCE Loss: 1.0262494087219238\n",
      "Epoch 183 / 500 | iteration 15 / 30 | Total Loss: 6.666905403137207 | KNN Loss: 5.614073276519775 | BCE Loss: 1.0528318881988525\n",
      "Epoch 183 / 500 | iteration 20 / 30 | Total Loss: 6.668004035949707 | KNN Loss: 5.601507186889648 | BCE Loss: 1.0664968490600586\n",
      "Epoch 183 / 500 | iteration 25 / 30 | Total Loss: 6.725011825561523 | KNN Loss: 5.684009552001953 | BCE Loss: 1.0410025119781494\n",
      "Epoch 184 / 500 | iteration 0 / 30 | Total Loss: 6.646395206451416 | KNN Loss: 5.599056720733643 | BCE Loss: 1.047338604927063\n",
      "Epoch 184 / 500 | iteration 5 / 30 | Total Loss: 6.633810997009277 | KNN Loss: 5.596721172332764 | BCE Loss: 1.0370895862579346\n",
      "Epoch 184 / 500 | iteration 10 / 30 | Total Loss: 6.708090782165527 | KNN Loss: 5.660064697265625 | BCE Loss: 1.0480258464813232\n",
      "Epoch 184 / 500 | iteration 15 / 30 | Total Loss: 6.61874532699585 | KNN Loss: 5.595874309539795 | BCE Loss: 1.0228710174560547\n",
      "Epoch 184 / 500 | iteration 20 / 30 | Total Loss: 6.724802017211914 | KNN Loss: 5.667451858520508 | BCE Loss: 1.0573502779006958\n",
      "Epoch 184 / 500 | iteration 25 / 30 | Total Loss: 6.7130961418151855 | KNN Loss: 5.66977596282959 | BCE Loss: 1.0433202981948853\n",
      "Epoch 185 / 500 | iteration 0 / 30 | Total Loss: 6.674508094787598 | KNN Loss: 5.608395576477051 | BCE Loss: 1.0661126375198364\n",
      "Epoch 185 / 500 | iteration 5 / 30 | Total Loss: 6.695196628570557 | KNN Loss: 5.602004528045654 | BCE Loss: 1.0931919813156128\n",
      "Epoch 185 / 500 | iteration 10 / 30 | Total Loss: 6.668417453765869 | KNN Loss: 5.618579864501953 | BCE Loss: 1.049837589263916\n",
      "Epoch 185 / 500 | iteration 15 / 30 | Total Loss: 6.686020374298096 | KNN Loss: 5.635056018829346 | BCE Loss: 1.0509642362594604\n",
      "Epoch 185 / 500 | iteration 20 / 30 | Total Loss: 6.710979461669922 | KNN Loss: 5.660163402557373 | BCE Loss: 1.0508159399032593\n",
      "Epoch 185 / 500 | iteration 25 / 30 | Total Loss: 6.684874534606934 | KNN Loss: 5.636537075042725 | BCE Loss: 1.0483372211456299\n",
      "Epoch 186 / 500 | iteration 0 / 30 | Total Loss: 6.67038631439209 | KNN Loss: 5.60015869140625 | BCE Loss: 1.0702275037765503\n",
      "Epoch 186 / 500 | iteration 5 / 30 | Total Loss: 6.645467758178711 | KNN Loss: 5.598083019256592 | BCE Loss: 1.04738450050354\n",
      "Epoch 186 / 500 | iteration 10 / 30 | Total Loss: 6.704063892364502 | KNN Loss: 5.645023345947266 | BCE Loss: 1.0590405464172363\n",
      "Epoch 186 / 500 | iteration 15 / 30 | Total Loss: 6.7009077072143555 | KNN Loss: 5.64135217666626 | BCE Loss: 1.0595557689666748\n",
      "Epoch 186 / 500 | iteration 20 / 30 | Total Loss: 6.641443252563477 | KNN Loss: 5.618525505065918 | BCE Loss: 1.022917628288269\n",
      "Epoch 186 / 500 | iteration 25 / 30 | Total Loss: 6.691280364990234 | KNN Loss: 5.6194539070129395 | BCE Loss: 1.071826696395874\n",
      "Epoch 187 / 500 | iteration 0 / 30 | Total Loss: 6.65903377532959 | KNN Loss: 5.630007743835449 | BCE Loss: 1.0290261507034302\n",
      "Epoch 187 / 500 | iteration 5 / 30 | Total Loss: 6.698008060455322 | KNN Loss: 5.6079840660095215 | BCE Loss: 1.0900241136550903\n",
      "Epoch 187 / 500 | iteration 10 / 30 | Total Loss: 6.671710014343262 | KNN Loss: 5.612249851226807 | BCE Loss: 1.059459924697876\n",
      "Epoch 187 / 500 | iteration 15 / 30 | Total Loss: 6.618381500244141 | KNN Loss: 5.596420764923096 | BCE Loss: 1.0219606161117554\n",
      "Epoch 187 / 500 | iteration 20 / 30 | Total Loss: 6.656611919403076 | KNN Loss: 5.607624530792236 | BCE Loss: 1.0489872694015503\n",
      "Epoch 187 / 500 | iteration 25 / 30 | Total Loss: 6.667028903961182 | KNN Loss: 5.6089887619018555 | BCE Loss: 1.0580401420593262\n",
      "Epoch 188 / 500 | iteration 0 / 30 | Total Loss: 6.663527965545654 | KNN Loss: 5.59840726852417 | BCE Loss: 1.0651206970214844\n",
      "Epoch 188 / 500 | iteration 5 / 30 | Total Loss: 6.6975555419921875 | KNN Loss: 5.604735374450684 | BCE Loss: 1.092820405960083\n",
      "Epoch 188 / 500 | iteration 10 / 30 | Total Loss: 6.685811519622803 | KNN Loss: 5.649761199951172 | BCE Loss: 1.0360503196716309\n",
      "Epoch 188 / 500 | iteration 15 / 30 | Total Loss: 6.662328243255615 | KNN Loss: 5.616079330444336 | BCE Loss: 1.0462490320205688\n",
      "Epoch 188 / 500 | iteration 20 / 30 | Total Loss: 6.718512058258057 | KNN Loss: 5.676178932189941 | BCE Loss: 1.0423332452774048\n",
      "Epoch 188 / 500 | iteration 25 / 30 | Total Loss: 6.638031005859375 | KNN Loss: 5.596220970153809 | BCE Loss: 1.0418097972869873\n",
      "Epoch 189 / 500 | iteration 0 / 30 | Total Loss: 6.662822723388672 | KNN Loss: 5.62047004699707 | BCE Loss: 1.0423527956008911\n",
      "Epoch 189 / 500 | iteration 5 / 30 | Total Loss: 6.673209190368652 | KNN Loss: 5.6121368408203125 | BCE Loss: 1.0610721111297607\n",
      "Epoch 189 / 500 | iteration 10 / 30 | Total Loss: 6.706331253051758 | KNN Loss: 5.696988582611084 | BCE Loss: 1.0093424320220947\n",
      "Epoch 189 / 500 | iteration 15 / 30 | Total Loss: 6.654708385467529 | KNN Loss: 5.597795486450195 | BCE Loss: 1.0569130182266235\n",
      "Epoch 189 / 500 | iteration 20 / 30 | Total Loss: 6.654247760772705 | KNN Loss: 5.611428737640381 | BCE Loss: 1.0428190231323242\n",
      "Epoch 189 / 500 | iteration 25 / 30 | Total Loss: 6.750672340393066 | KNN Loss: 5.674803733825684 | BCE Loss: 1.0758686065673828\n",
      "Epoch 190 / 500 | iteration 0 / 30 | Total Loss: 6.654107570648193 | KNN Loss: 5.592996120452881 | BCE Loss: 1.061111330986023\n",
      "Epoch 190 / 500 | iteration 5 / 30 | Total Loss: 6.669986724853516 | KNN Loss: 5.596354007720947 | BCE Loss: 1.0736325979232788\n",
      "Epoch 190 / 500 | iteration 10 / 30 | Total Loss: 6.689109802246094 | KNN Loss: 5.605874061584473 | BCE Loss: 1.083235502243042\n",
      "Epoch 190 / 500 | iteration 15 / 30 | Total Loss: 6.677047252655029 | KNN Loss: 5.636961460113525 | BCE Loss: 1.040085792541504\n",
      "Epoch 190 / 500 | iteration 20 / 30 | Total Loss: 6.652428150177002 | KNN Loss: 5.59113883972168 | BCE Loss: 1.0612891912460327\n",
      "Epoch 190 / 500 | iteration 25 / 30 | Total Loss: 6.66075325012207 | KNN Loss: 5.592891216278076 | BCE Loss: 1.0678620338439941\n",
      "Epoch 191 / 500 | iteration 0 / 30 | Total Loss: 6.664467811584473 | KNN Loss: 5.631616592407227 | BCE Loss: 1.032850980758667\n",
      "Epoch 191 / 500 | iteration 5 / 30 | Total Loss: 6.658988952636719 | KNN Loss: 5.610655784606934 | BCE Loss: 1.0483331680297852\n",
      "Epoch 191 / 500 | iteration 10 / 30 | Total Loss: 6.75318717956543 | KNN Loss: 5.704865455627441 | BCE Loss: 1.0483214855194092\n",
      "Epoch 191 / 500 | iteration 15 / 30 | Total Loss: 6.666826248168945 | KNN Loss: 5.592790126800537 | BCE Loss: 1.0740363597869873\n",
      "Epoch 191 / 500 | iteration 20 / 30 | Total Loss: 6.672538757324219 | KNN Loss: 5.6138811111450195 | BCE Loss: 1.0586576461791992\n",
      "Epoch 191 / 500 | iteration 25 / 30 | Total Loss: 6.694876670837402 | KNN Loss: 5.660358905792236 | BCE Loss: 1.0345178842544556\n",
      "Epoch 192 / 500 | iteration 0 / 30 | Total Loss: 6.7628350257873535 | KNN Loss: 5.681640148162842 | BCE Loss: 1.0811949968338013\n",
      "Epoch 192 / 500 | iteration 5 / 30 | Total Loss: 6.710958003997803 | KNN Loss: 5.658599376678467 | BCE Loss: 1.052358627319336\n",
      "Epoch 192 / 500 | iteration 10 / 30 | Total Loss: 6.644371032714844 | KNN Loss: 5.5967817306518555 | BCE Loss: 1.0475895404815674\n",
      "Epoch 192 / 500 | iteration 15 / 30 | Total Loss: 6.698006629943848 | KNN Loss: 5.630311012268066 | BCE Loss: 1.0676956176757812\n",
      "Epoch 192 / 500 | iteration 20 / 30 | Total Loss: 6.651876926422119 | KNN Loss: 5.607968330383301 | BCE Loss: 1.0439085960388184\n",
      "Epoch 192 / 500 | iteration 25 / 30 | Total Loss: 6.654902935028076 | KNN Loss: 5.603973388671875 | BCE Loss: 1.0509296655654907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 193 / 500 | iteration 0 / 30 | Total Loss: 6.706310749053955 | KNN Loss: 5.662971496582031 | BCE Loss: 1.0433393716812134\n",
      "Epoch 193 / 500 | iteration 5 / 30 | Total Loss: 6.690129280090332 | KNN Loss: 5.653651714324951 | BCE Loss: 1.0364773273468018\n",
      "Epoch 193 / 500 | iteration 10 / 30 | Total Loss: 6.638817310333252 | KNN Loss: 5.598745822906494 | BCE Loss: 1.0400714874267578\n",
      "Epoch 193 / 500 | iteration 15 / 30 | Total Loss: 6.653109073638916 | KNN Loss: 5.6159491539001465 | BCE Loss: 1.03715980052948\n",
      "Epoch 193 / 500 | iteration 20 / 30 | Total Loss: 6.654963493347168 | KNN Loss: 5.621153831481934 | BCE Loss: 1.0338096618652344\n",
      "Epoch 193 / 500 | iteration 25 / 30 | Total Loss: 6.737525939941406 | KNN Loss: 5.679196357727051 | BCE Loss: 1.0583298206329346\n",
      "Epoch   194: reducing learning rate of group 0 to 1.4124e-04.\n",
      "Epoch 194 / 500 | iteration 0 / 30 | Total Loss: 6.645843505859375 | KNN Loss: 5.597964286804199 | BCE Loss: 1.0478792190551758\n",
      "Epoch 194 / 500 | iteration 5 / 30 | Total Loss: 6.64759635925293 | KNN Loss: 5.600374698638916 | BCE Loss: 1.0472218990325928\n",
      "Epoch 194 / 500 | iteration 10 / 30 | Total Loss: 6.658398628234863 | KNN Loss: 5.614762783050537 | BCE Loss: 1.0436357259750366\n",
      "Epoch 194 / 500 | iteration 15 / 30 | Total Loss: 6.656547546386719 | KNN Loss: 5.607597351074219 | BCE Loss: 1.048950433731079\n",
      "Epoch 194 / 500 | iteration 20 / 30 | Total Loss: 6.72170352935791 | KNN Loss: 5.7219557762146 | BCE Loss: 0.9997478723526001\n",
      "Epoch 194 / 500 | iteration 25 / 30 | Total Loss: 6.642610549926758 | KNN Loss: 5.621502876281738 | BCE Loss: 1.02110755443573\n",
      "Epoch 195 / 500 | iteration 0 / 30 | Total Loss: 6.634986400604248 | KNN Loss: 5.597994804382324 | BCE Loss: 1.0369914770126343\n",
      "Epoch 195 / 500 | iteration 5 / 30 | Total Loss: 6.727898120880127 | KNN Loss: 5.675142765045166 | BCE Loss: 1.052755355834961\n",
      "Epoch 195 / 500 | iteration 10 / 30 | Total Loss: 6.647482395172119 | KNN Loss: 5.600194931030273 | BCE Loss: 1.0472874641418457\n",
      "Epoch 195 / 500 | iteration 15 / 30 | Total Loss: 6.623398780822754 | KNN Loss: 5.588739395141602 | BCE Loss: 1.0346593856811523\n",
      "Epoch 195 / 500 | iteration 20 / 30 | Total Loss: 6.6606764793396 | KNN Loss: 5.599552631378174 | BCE Loss: 1.0611238479614258\n",
      "Epoch 195 / 500 | iteration 25 / 30 | Total Loss: 6.686063766479492 | KNN Loss: 5.609176158905029 | BCE Loss: 1.0768877267837524\n",
      "Epoch 196 / 500 | iteration 0 / 30 | Total Loss: 6.6499786376953125 | KNN Loss: 5.61182165145874 | BCE Loss: 1.0381572246551514\n",
      "Epoch 196 / 500 | iteration 5 / 30 | Total Loss: 6.7289276123046875 | KNN Loss: 5.673087120056152 | BCE Loss: 1.0558406114578247\n",
      "Epoch 196 / 500 | iteration 10 / 30 | Total Loss: 6.680085182189941 | KNN Loss: 5.630549907684326 | BCE Loss: 1.0495353937149048\n",
      "Epoch 196 / 500 | iteration 15 / 30 | Total Loss: 6.710942268371582 | KNN Loss: 5.63765287399292 | BCE Loss: 1.073289394378662\n",
      "Epoch 196 / 500 | iteration 20 / 30 | Total Loss: 6.623600959777832 | KNN Loss: 5.601134300231934 | BCE Loss: 1.0224665403366089\n",
      "Epoch 196 / 500 | iteration 25 / 30 | Total Loss: 6.605397701263428 | KNN Loss: 5.595909118652344 | BCE Loss: 1.0094887018203735\n",
      "Epoch 197 / 500 | iteration 0 / 30 | Total Loss: 6.686077117919922 | KNN Loss: 5.648333549499512 | BCE Loss: 1.0377434492111206\n",
      "Epoch 197 / 500 | iteration 5 / 30 | Total Loss: 6.627777099609375 | KNN Loss: 5.5911478996276855 | BCE Loss: 1.0366291999816895\n",
      "Epoch 197 / 500 | iteration 10 / 30 | Total Loss: 6.643735885620117 | KNN Loss: 5.62431001663208 | BCE Loss: 1.019425868988037\n",
      "Epoch 197 / 500 | iteration 15 / 30 | Total Loss: 6.635904312133789 | KNN Loss: 5.597878932952881 | BCE Loss: 1.0380253791809082\n",
      "Epoch 197 / 500 | iteration 20 / 30 | Total Loss: 6.680290222167969 | KNN Loss: 5.634075164794922 | BCE Loss: 1.0462151765823364\n",
      "Epoch 197 / 500 | iteration 25 / 30 | Total Loss: 6.67000675201416 | KNN Loss: 5.602541446685791 | BCE Loss: 1.0674655437469482\n",
      "Epoch 198 / 500 | iteration 0 / 30 | Total Loss: 6.698031425476074 | KNN Loss: 5.648065090179443 | BCE Loss: 1.04996657371521\n",
      "Epoch 198 / 500 | iteration 5 / 30 | Total Loss: 6.657853126525879 | KNN Loss: 5.606091499328613 | BCE Loss: 1.0517618656158447\n",
      "Epoch 198 / 500 | iteration 10 / 30 | Total Loss: 6.6808271408081055 | KNN Loss: 5.617247104644775 | BCE Loss: 1.0635799169540405\n",
      "Epoch 198 / 500 | iteration 15 / 30 | Total Loss: 6.679322242736816 | KNN Loss: 5.6481242179870605 | BCE Loss: 1.0311981439590454\n",
      "Epoch 198 / 500 | iteration 20 / 30 | Total Loss: 6.687315940856934 | KNN Loss: 5.624762535095215 | BCE Loss: 1.0625532865524292\n",
      "Epoch 198 / 500 | iteration 25 / 30 | Total Loss: 6.657383918762207 | KNN Loss: 5.601344585418701 | BCE Loss: 1.056039571762085\n",
      "Epoch 199 / 500 | iteration 0 / 30 | Total Loss: 6.697518825531006 | KNN Loss: 5.645452499389648 | BCE Loss: 1.0520663261413574\n",
      "Epoch 199 / 500 | iteration 5 / 30 | Total Loss: 6.660747528076172 | KNN Loss: 5.603923320770264 | BCE Loss: 1.0568243265151978\n",
      "Epoch 199 / 500 | iteration 10 / 30 | Total Loss: 6.677700996398926 | KNN Loss: 5.605876922607422 | BCE Loss: 1.0718238353729248\n",
      "Epoch 199 / 500 | iteration 15 / 30 | Total Loss: 6.704909324645996 | KNN Loss: 5.6598310470581055 | BCE Loss: 1.0450785160064697\n",
      "Epoch 199 / 500 | iteration 20 / 30 | Total Loss: 6.69999885559082 | KNN Loss: 5.650603771209717 | BCE Loss: 1.0493948459625244\n",
      "Epoch 199 / 500 | iteration 25 / 30 | Total Loss: 6.68472957611084 | KNN Loss: 5.6319804191589355 | BCE Loss: 1.0527491569519043\n",
      "Epoch 200 / 500 | iteration 0 / 30 | Total Loss: 6.680536270141602 | KNN Loss: 5.591740131378174 | BCE Loss: 1.0887960195541382\n",
      "Epoch 200 / 500 | iteration 5 / 30 | Total Loss: 6.7041826248168945 | KNN Loss: 5.664237022399902 | BCE Loss: 1.039945363998413\n",
      "Epoch 200 / 500 | iteration 10 / 30 | Total Loss: 6.673612594604492 | KNN Loss: 5.614467620849609 | BCE Loss: 1.0591447353363037\n",
      "Epoch 200 / 500 | iteration 15 / 30 | Total Loss: 6.629847526550293 | KNN Loss: 5.596837520599365 | BCE Loss: 1.0330102443695068\n",
      "Epoch 200 / 500 | iteration 20 / 30 | Total Loss: 6.735111236572266 | KNN Loss: 5.671276092529297 | BCE Loss: 1.0638351440429688\n",
      "Epoch 200 / 500 | iteration 25 / 30 | Total Loss: 6.71120548248291 | KNN Loss: 5.652383327484131 | BCE Loss: 1.0588222742080688\n",
      "Epoch 201 / 500 | iteration 0 / 30 | Total Loss: 6.659469127655029 | KNN Loss: 5.6323161125183105 | BCE Loss: 1.0271528959274292\n",
      "Epoch 201 / 500 | iteration 5 / 30 | Total Loss: 6.651086330413818 | KNN Loss: 5.614119529724121 | BCE Loss: 1.0369666814804077\n",
      "Epoch 201 / 500 | iteration 10 / 30 | Total Loss: 6.706459999084473 | KNN Loss: 5.642699718475342 | BCE Loss: 1.0637602806091309\n",
      "Epoch 201 / 500 | iteration 15 / 30 | Total Loss: 6.665828704833984 | KNN Loss: 5.629245758056641 | BCE Loss: 1.0365829467773438\n",
      "Epoch 201 / 500 | iteration 20 / 30 | Total Loss: 6.650048732757568 | KNN Loss: 5.613818645477295 | BCE Loss: 1.0362299680709839\n",
      "Epoch 201 / 500 | iteration 25 / 30 | Total Loss: 6.638301849365234 | KNN Loss: 5.611960411071777 | BCE Loss: 1.0263413190841675\n",
      "Epoch 202 / 500 | iteration 0 / 30 | Total Loss: 6.717341899871826 | KNN Loss: 5.6776204109191895 | BCE Loss: 1.0397214889526367\n",
      "Epoch 202 / 500 | iteration 5 / 30 | Total Loss: 6.7127532958984375 | KNN Loss: 5.654225826263428 | BCE Loss: 1.0585272312164307\n",
      "Epoch 202 / 500 | iteration 10 / 30 | Total Loss: 6.639336109161377 | KNN Loss: 5.614497661590576 | BCE Loss: 1.0248384475708008\n",
      "Epoch 202 / 500 | iteration 15 / 30 | Total Loss: 6.689007759094238 | KNN Loss: 5.633420944213867 | BCE Loss: 1.055586814880371\n",
      "Epoch 202 / 500 | iteration 20 / 30 | Total Loss: 6.620163917541504 | KNN Loss: 5.60599422454834 | BCE Loss: 1.0141699314117432\n",
      "Epoch 202 / 500 | iteration 25 / 30 | Total Loss: 6.686193943023682 | KNN Loss: 5.623458385467529 | BCE Loss: 1.062735676765442\n",
      "Epoch 203 / 500 | iteration 0 / 30 | Total Loss: 6.664806365966797 | KNN Loss: 5.6095709800720215 | BCE Loss: 1.0552351474761963\n",
      "Epoch 203 / 500 | iteration 5 / 30 | Total Loss: 6.706542015075684 | KNN Loss: 5.650608062744141 | BCE Loss: 1.055934190750122\n",
      "Epoch 203 / 500 | iteration 10 / 30 | Total Loss: 6.65714693069458 | KNN Loss: 5.638382911682129 | BCE Loss: 1.0187640190124512\n",
      "Epoch 203 / 500 | iteration 15 / 30 | Total Loss: 6.665000915527344 | KNN Loss: 5.615769386291504 | BCE Loss: 1.049231767654419\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 203 / 500 | iteration 20 / 30 | Total Loss: 6.63314151763916 | KNN Loss: 5.5923285484313965 | BCE Loss: 1.0408128499984741\n",
      "Epoch 203 / 500 | iteration 25 / 30 | Total Loss: 6.744845390319824 | KNN Loss: 5.714748382568359 | BCE Loss: 1.030097246170044\n",
      "Epoch 204 / 500 | iteration 0 / 30 | Total Loss: 6.656296253204346 | KNN Loss: 5.6369709968566895 | BCE Loss: 1.0193252563476562\n",
      "Epoch 204 / 500 | iteration 5 / 30 | Total Loss: 6.642267227172852 | KNN Loss: 5.602746486663818 | BCE Loss: 1.0395206212997437\n",
      "Epoch 204 / 500 | iteration 10 / 30 | Total Loss: 6.686194896697998 | KNN Loss: 5.618226528167725 | BCE Loss: 1.0679683685302734\n",
      "Epoch 204 / 500 | iteration 15 / 30 | Total Loss: 6.702664375305176 | KNN Loss: 5.674350738525391 | BCE Loss: 1.028313398361206\n",
      "Epoch 204 / 500 | iteration 20 / 30 | Total Loss: 6.676589012145996 | KNN Loss: 5.624922275543213 | BCE Loss: 1.0516666173934937\n",
      "Epoch 204 / 500 | iteration 25 / 30 | Total Loss: 6.745023727416992 | KNN Loss: 5.671672344207764 | BCE Loss: 1.0733513832092285\n",
      "Epoch   205: reducing learning rate of group 0 to 9.8866e-05.\n",
      "Epoch 205 / 500 | iteration 0 / 30 | Total Loss: 6.630253791809082 | KNN Loss: 5.595029830932617 | BCE Loss: 1.0352237224578857\n",
      "Epoch 205 / 500 | iteration 5 / 30 | Total Loss: 6.656225681304932 | KNN Loss: 5.596327304840088 | BCE Loss: 1.0598983764648438\n",
      "Epoch 205 / 500 | iteration 10 / 30 | Total Loss: 6.660232067108154 | KNN Loss: 5.6062541007995605 | BCE Loss: 1.0539779663085938\n",
      "Epoch 205 / 500 | iteration 15 / 30 | Total Loss: 6.661128044128418 | KNN Loss: 5.590025424957275 | BCE Loss: 1.0711026191711426\n",
      "Epoch 205 / 500 | iteration 20 / 30 | Total Loss: 6.719085216522217 | KNN Loss: 5.663576602935791 | BCE Loss: 1.0555086135864258\n",
      "Epoch 205 / 500 | iteration 25 / 30 | Total Loss: 6.6428985595703125 | KNN Loss: 5.596269607543945 | BCE Loss: 1.0466288328170776\n",
      "Epoch 206 / 500 | iteration 0 / 30 | Total Loss: 6.654404640197754 | KNN Loss: 5.5982136726379395 | BCE Loss: 1.0561907291412354\n",
      "Epoch 206 / 500 | iteration 5 / 30 | Total Loss: 6.711467742919922 | KNN Loss: 5.656359672546387 | BCE Loss: 1.0551083087921143\n",
      "Epoch 206 / 500 | iteration 10 / 30 | Total Loss: 6.75405216217041 | KNN Loss: 5.679692268371582 | BCE Loss: 1.074359655380249\n",
      "Epoch 206 / 500 | iteration 15 / 30 | Total Loss: 6.662268161773682 | KNN Loss: 5.634392738342285 | BCE Loss: 1.027875542640686\n",
      "Epoch 206 / 500 | iteration 20 / 30 | Total Loss: 6.625041961669922 | KNN Loss: 5.606947422027588 | BCE Loss: 1.018094778060913\n",
      "Epoch 206 / 500 | iteration 25 / 30 | Total Loss: 6.688830375671387 | KNN Loss: 5.6656694412231445 | BCE Loss: 1.0231609344482422\n",
      "Epoch 207 / 500 | iteration 0 / 30 | Total Loss: 6.694803237915039 | KNN Loss: 5.662322998046875 | BCE Loss: 1.032480001449585\n",
      "Epoch 207 / 500 | iteration 5 / 30 | Total Loss: 6.638209342956543 | KNN Loss: 5.593498229980469 | BCE Loss: 1.0447113513946533\n",
      "Epoch 207 / 500 | iteration 10 / 30 | Total Loss: 6.643420219421387 | KNN Loss: 5.595144748687744 | BCE Loss: 1.048275351524353\n",
      "Epoch 207 / 500 | iteration 15 / 30 | Total Loss: 6.643453598022461 | KNN Loss: 5.602468967437744 | BCE Loss: 1.0409846305847168\n",
      "Epoch 207 / 500 | iteration 20 / 30 | Total Loss: 6.713284492492676 | KNN Loss: 5.641216278076172 | BCE Loss: 1.072068452835083\n",
      "Epoch 207 / 500 | iteration 25 / 30 | Total Loss: 6.637874603271484 | KNN Loss: 5.599449634552002 | BCE Loss: 1.0384247303009033\n",
      "Epoch 208 / 500 | iteration 0 / 30 | Total Loss: 6.649237632751465 | KNN Loss: 5.629006862640381 | BCE Loss: 1.0202305316925049\n",
      "Epoch 208 / 500 | iteration 5 / 30 | Total Loss: 6.643505573272705 | KNN Loss: 5.59268045425415 | BCE Loss: 1.0508252382278442\n",
      "Epoch 208 / 500 | iteration 10 / 30 | Total Loss: 6.750462055206299 | KNN Loss: 5.674417972564697 | BCE Loss: 1.076043963432312\n",
      "Epoch 208 / 500 | iteration 15 / 30 | Total Loss: 6.646347999572754 | KNN Loss: 5.591383934020996 | BCE Loss: 1.0549638271331787\n",
      "Epoch 208 / 500 | iteration 20 / 30 | Total Loss: 6.7204999923706055 | KNN Loss: 5.656984329223633 | BCE Loss: 1.0635154247283936\n",
      "Epoch 208 / 500 | iteration 25 / 30 | Total Loss: 6.715473175048828 | KNN Loss: 5.68995475769043 | BCE Loss: 1.0255181789398193\n",
      "Epoch 209 / 500 | iteration 0 / 30 | Total Loss: 6.660349369049072 | KNN Loss: 5.61765718460083 | BCE Loss: 1.0426923036575317\n",
      "Epoch 209 / 500 | iteration 5 / 30 | Total Loss: 6.686103820800781 | KNN Loss: 5.616272926330566 | BCE Loss: 1.0698306560516357\n",
      "Epoch 209 / 500 | iteration 10 / 30 | Total Loss: 6.634634971618652 | KNN Loss: 5.598806858062744 | BCE Loss: 1.0358281135559082\n",
      "Epoch 209 / 500 | iteration 15 / 30 | Total Loss: 6.679462432861328 | KNN Loss: 5.648080348968506 | BCE Loss: 1.0313823223114014\n",
      "Epoch 209 / 500 | iteration 20 / 30 | Total Loss: 6.71366548538208 | KNN Loss: 5.6551833152771 | BCE Loss: 1.0584821701049805\n",
      "Epoch 209 / 500 | iteration 25 / 30 | Total Loss: 6.686706066131592 | KNN Loss: 5.630455493927002 | BCE Loss: 1.0562505722045898\n",
      "Epoch 210 / 500 | iteration 0 / 30 | Total Loss: 6.650015830993652 | KNN Loss: 5.6173295974731445 | BCE Loss: 1.032686471939087\n",
      "Epoch 210 / 500 | iteration 5 / 30 | Total Loss: 6.687714576721191 | KNN Loss: 5.640621662139893 | BCE Loss: 1.0470929145812988\n",
      "Epoch 210 / 500 | iteration 10 / 30 | Total Loss: 6.64797306060791 | KNN Loss: 5.61120080947876 | BCE Loss: 1.03677237033844\n",
      "Epoch 210 / 500 | iteration 15 / 30 | Total Loss: 6.663089275360107 | KNN Loss: 5.6197075843811035 | BCE Loss: 1.043381690979004\n",
      "Epoch 210 / 500 | iteration 20 / 30 | Total Loss: 6.674104690551758 | KNN Loss: 5.618436336517334 | BCE Loss: 1.0556683540344238\n",
      "Epoch 210 / 500 | iteration 25 / 30 | Total Loss: 6.747546195983887 | KNN Loss: 5.714688777923584 | BCE Loss: 1.0328576564788818\n",
      "Epoch 211 / 500 | iteration 0 / 30 | Total Loss: 6.680530548095703 | KNN Loss: 5.653204917907715 | BCE Loss: 1.0273255109786987\n",
      "Epoch 211 / 500 | iteration 5 / 30 | Total Loss: 6.698019027709961 | KNN Loss: 5.6544413566589355 | BCE Loss: 1.0435775518417358\n",
      "Epoch 211 / 500 | iteration 10 / 30 | Total Loss: 6.67591667175293 | KNN Loss: 5.597797870635986 | BCE Loss: 1.0781185626983643\n",
      "Epoch 211 / 500 | iteration 15 / 30 | Total Loss: 6.743095397949219 | KNN Loss: 5.662654399871826 | BCE Loss: 1.0804409980773926\n",
      "Epoch 211 / 500 | iteration 20 / 30 | Total Loss: 6.73080587387085 | KNN Loss: 5.6845550537109375 | BCE Loss: 1.0462507009506226\n",
      "Epoch 211 / 500 | iteration 25 / 30 | Total Loss: 6.751028537750244 | KNN Loss: 5.714681625366211 | BCE Loss: 1.0363470315933228\n",
      "Epoch 212 / 500 | iteration 0 / 30 | Total Loss: 6.690242290496826 | KNN Loss: 5.62712287902832 | BCE Loss: 1.0631194114685059\n",
      "Epoch 212 / 500 | iteration 5 / 30 | Total Loss: 6.657783508300781 | KNN Loss: 5.610110759735107 | BCE Loss: 1.0476725101470947\n",
      "Epoch 212 / 500 | iteration 10 / 30 | Total Loss: 6.660984992980957 | KNN Loss: 5.634472370147705 | BCE Loss: 1.026512622833252\n",
      "Epoch 212 / 500 | iteration 15 / 30 | Total Loss: 6.665500640869141 | KNN Loss: 5.597912788391113 | BCE Loss: 1.0675880908966064\n",
      "Epoch 212 / 500 | iteration 20 / 30 | Total Loss: 6.690081596374512 | KNN Loss: 5.633862495422363 | BCE Loss: 1.0562188625335693\n",
      "Epoch 212 / 500 | iteration 25 / 30 | Total Loss: 6.637983798980713 | KNN Loss: 5.602349758148193 | BCE Loss: 1.0356340408325195\n",
      "Epoch 213 / 500 | iteration 0 / 30 | Total Loss: 6.6389055252075195 | KNN Loss: 5.592027187347412 | BCE Loss: 1.0468780994415283\n",
      "Epoch 213 / 500 | iteration 5 / 30 | Total Loss: 6.712536811828613 | KNN Loss: 5.673141002655029 | BCE Loss: 1.0393955707550049\n",
      "Epoch 213 / 500 | iteration 10 / 30 | Total Loss: 6.6304473876953125 | KNN Loss: 5.594262599945068 | BCE Loss: 1.0361849069595337\n",
      "Epoch 213 / 500 | iteration 15 / 30 | Total Loss: 6.651554584503174 | KNN Loss: 5.60740327835083 | BCE Loss: 1.0441513061523438\n",
      "Epoch 213 / 500 | iteration 20 / 30 | Total Loss: 6.663443088531494 | KNN Loss: 5.608107089996338 | BCE Loss: 1.0553359985351562\n",
      "Epoch 213 / 500 | iteration 25 / 30 | Total Loss: 6.709843158721924 | KNN Loss: 5.649235725402832 | BCE Loss: 1.0606075525283813\n",
      "Epoch 214 / 500 | iteration 0 / 30 | Total Loss: 6.637206077575684 | KNN Loss: 5.60819673538208 | BCE Loss: 1.0290091037750244\n",
      "Epoch 214 / 500 | iteration 5 / 30 | Total Loss: 6.729824066162109 | KNN Loss: 5.681450366973877 | BCE Loss: 1.0483734607696533\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 214 / 500 | iteration 10 / 30 | Total Loss: 6.65761661529541 | KNN Loss: 5.591122627258301 | BCE Loss: 1.0664937496185303\n",
      "Epoch 214 / 500 | iteration 15 / 30 | Total Loss: 6.626102447509766 | KNN Loss: 5.5947699546813965 | BCE Loss: 1.0313327312469482\n",
      "Epoch 214 / 500 | iteration 20 / 30 | Total Loss: 6.708847522735596 | KNN Loss: 5.641964912414551 | BCE Loss: 1.0668824911117554\n",
      "Epoch 214 / 500 | iteration 25 / 30 | Total Loss: 6.6160125732421875 | KNN Loss: 5.5957465171813965 | BCE Loss: 1.020265817642212\n",
      "Epoch 215 / 500 | iteration 0 / 30 | Total Loss: 6.690831661224365 | KNN Loss: 5.642663478851318 | BCE Loss: 1.0481681823730469\n",
      "Epoch 215 / 500 | iteration 5 / 30 | Total Loss: 6.671355247497559 | KNN Loss: 5.611227512359619 | BCE Loss: 1.0601274967193604\n",
      "Epoch 215 / 500 | iteration 10 / 30 | Total Loss: 6.667659282684326 | KNN Loss: 5.60070276260376 | BCE Loss: 1.0669564008712769\n",
      "Epoch 215 / 500 | iteration 15 / 30 | Total Loss: 6.693853378295898 | KNN Loss: 5.625725269317627 | BCE Loss: 1.068128228187561\n",
      "Epoch 215 / 500 | iteration 20 / 30 | Total Loss: 6.631296634674072 | KNN Loss: 5.599094390869141 | BCE Loss: 1.032202124595642\n",
      "Epoch 215 / 500 | iteration 25 / 30 | Total Loss: 6.675968647003174 | KNN Loss: 5.629276752471924 | BCE Loss: 1.04669189453125\n",
      "Epoch   216: reducing learning rate of group 0 to 6.9206e-05.\n",
      "Epoch 216 / 500 | iteration 0 / 30 | Total Loss: 6.6419172286987305 | KNN Loss: 5.594081878662109 | BCE Loss: 1.047835350036621\n",
      "Epoch 216 / 500 | iteration 5 / 30 | Total Loss: 6.683056354522705 | KNN Loss: 5.621006488800049 | BCE Loss: 1.0620498657226562\n",
      "Epoch 216 / 500 | iteration 10 / 30 | Total Loss: 6.676712989807129 | KNN Loss: 5.628371715545654 | BCE Loss: 1.0483410358428955\n",
      "Epoch 216 / 500 | iteration 15 / 30 | Total Loss: 6.699509620666504 | KNN Loss: 5.624265670776367 | BCE Loss: 1.0752439498901367\n",
      "Epoch 216 / 500 | iteration 20 / 30 | Total Loss: 6.6760711669921875 | KNN Loss: 5.619076728820801 | BCE Loss: 1.0569944381713867\n",
      "Epoch 216 / 500 | iteration 25 / 30 | Total Loss: 6.737447261810303 | KNN Loss: 5.673661708831787 | BCE Loss: 1.0637856721878052\n",
      "Epoch 217 / 500 | iteration 0 / 30 | Total Loss: 6.69147253036499 | KNN Loss: 5.689364433288574 | BCE Loss: 1.0021082162857056\n",
      "Epoch 217 / 500 | iteration 5 / 30 | Total Loss: 6.664651870727539 | KNN Loss: 5.61210298538208 | BCE Loss: 1.052549123764038\n",
      "Epoch 217 / 500 | iteration 10 / 30 | Total Loss: 6.643099784851074 | KNN Loss: 5.609956741333008 | BCE Loss: 1.0331428050994873\n",
      "Epoch 217 / 500 | iteration 15 / 30 | Total Loss: 6.666377544403076 | KNN Loss: 5.610382556915283 | BCE Loss: 1.055994987487793\n",
      "Epoch 217 / 500 | iteration 20 / 30 | Total Loss: 6.664478302001953 | KNN Loss: 5.641820907592773 | BCE Loss: 1.0226573944091797\n",
      "Epoch 217 / 500 | iteration 25 / 30 | Total Loss: 6.658269882202148 | KNN Loss: 5.626928806304932 | BCE Loss: 1.0313410758972168\n",
      "Epoch 218 / 500 | iteration 0 / 30 | Total Loss: 6.776325225830078 | KNN Loss: 5.718716144561768 | BCE Loss: 1.0576090812683105\n",
      "Epoch 218 / 500 | iteration 5 / 30 | Total Loss: 6.628383159637451 | KNN Loss: 5.5918684005737305 | BCE Loss: 1.0365147590637207\n",
      "Epoch 218 / 500 | iteration 10 / 30 | Total Loss: 6.637133598327637 | KNN Loss: 5.590879917144775 | BCE Loss: 1.0462534427642822\n",
      "Epoch 218 / 500 | iteration 15 / 30 | Total Loss: 6.620477676391602 | KNN Loss: 5.6012773513793945 | BCE Loss: 1.019200325012207\n",
      "Epoch 218 / 500 | iteration 20 / 30 | Total Loss: 6.626053333282471 | KNN Loss: 5.599299430847168 | BCE Loss: 1.0267539024353027\n",
      "Epoch 218 / 500 | iteration 25 / 30 | Total Loss: 6.727696418762207 | KNN Loss: 5.681715488433838 | BCE Loss: 1.0459810495376587\n",
      "Epoch 219 / 500 | iteration 0 / 30 | Total Loss: 6.626051902770996 | KNN Loss: 5.604608058929443 | BCE Loss: 1.0214439630508423\n",
      "Epoch 219 / 500 | iteration 5 / 30 | Total Loss: 6.639570713043213 | KNN Loss: 5.6013922691345215 | BCE Loss: 1.0381784439086914\n",
      "Epoch 219 / 500 | iteration 10 / 30 | Total Loss: 6.686010360717773 | KNN Loss: 5.618915557861328 | BCE Loss: 1.0670945644378662\n",
      "Epoch 219 / 500 | iteration 15 / 30 | Total Loss: 6.663645267486572 | KNN Loss: 5.641114234924316 | BCE Loss: 1.0225311517715454\n",
      "Epoch 219 / 500 | iteration 20 / 30 | Total Loss: 6.702960968017578 | KNN Loss: 5.641106128692627 | BCE Loss: 1.0618548393249512\n",
      "Epoch 219 / 500 | iteration 25 / 30 | Total Loss: 6.694677352905273 | KNN Loss: 5.627038955688477 | BCE Loss: 1.0676381587982178\n",
      "Epoch 220 / 500 | iteration 0 / 30 | Total Loss: 6.639270305633545 | KNN Loss: 5.602057456970215 | BCE Loss: 1.03721284866333\n",
      "Epoch 220 / 500 | iteration 5 / 30 | Total Loss: 6.748976230621338 | KNN Loss: 5.664194107055664 | BCE Loss: 1.0847820043563843\n",
      "Epoch 220 / 500 | iteration 10 / 30 | Total Loss: 6.684195518493652 | KNN Loss: 5.614665985107422 | BCE Loss: 1.06952965259552\n",
      "Epoch 220 / 500 | iteration 15 / 30 | Total Loss: 6.686272621154785 | KNN Loss: 5.614711761474609 | BCE Loss: 1.0715606212615967\n",
      "Epoch 220 / 500 | iteration 20 / 30 | Total Loss: 6.702150344848633 | KNN Loss: 5.6564483642578125 | BCE Loss: 1.0457019805908203\n",
      "Epoch 220 / 500 | iteration 25 / 30 | Total Loss: 6.670736789703369 | KNN Loss: 5.614522457122803 | BCE Loss: 1.0562142133712769\n",
      "Epoch 221 / 500 | iteration 0 / 30 | Total Loss: 6.717040061950684 | KNN Loss: 5.638003826141357 | BCE Loss: 1.0790364742279053\n",
      "Epoch 221 / 500 | iteration 5 / 30 | Total Loss: 6.643002510070801 | KNN Loss: 5.603098392486572 | BCE Loss: 1.0399043560028076\n",
      "Epoch 221 / 500 | iteration 10 / 30 | Total Loss: 6.687112808227539 | KNN Loss: 5.640661716461182 | BCE Loss: 1.0464508533477783\n",
      "Epoch 221 / 500 | iteration 15 / 30 | Total Loss: 6.789607048034668 | KNN Loss: 5.73077392578125 | BCE Loss: 1.058833122253418\n",
      "Epoch 221 / 500 | iteration 20 / 30 | Total Loss: 6.68144416809082 | KNN Loss: 5.630679607391357 | BCE Loss: 1.050764799118042\n",
      "Epoch 221 / 500 | iteration 25 / 30 | Total Loss: 6.69251012802124 | KNN Loss: 5.603316307067871 | BCE Loss: 1.0891938209533691\n",
      "Epoch 222 / 500 | iteration 0 / 30 | Total Loss: 6.676246166229248 | KNN Loss: 5.613650798797607 | BCE Loss: 1.0625953674316406\n",
      "Epoch 222 / 500 | iteration 5 / 30 | Total Loss: 6.643123149871826 | KNN Loss: 5.595747947692871 | BCE Loss: 1.047375202178955\n",
      "Epoch 222 / 500 | iteration 10 / 30 | Total Loss: 6.639228343963623 | KNN Loss: 5.5976409912109375 | BCE Loss: 1.041587233543396\n",
      "Epoch 222 / 500 | iteration 15 / 30 | Total Loss: 6.640881538391113 | KNN Loss: 5.58925199508667 | BCE Loss: 1.0516295433044434\n",
      "Epoch 222 / 500 | iteration 20 / 30 | Total Loss: 6.701618194580078 | KNN Loss: 5.663638114929199 | BCE Loss: 1.037980079650879\n",
      "Epoch 222 / 500 | iteration 25 / 30 | Total Loss: 6.694390773773193 | KNN Loss: 5.647214889526367 | BCE Loss: 1.0471760034561157\n",
      "Epoch 223 / 500 | iteration 0 / 30 | Total Loss: 6.688473224639893 | KNN Loss: 5.652355194091797 | BCE Loss: 1.0361180305480957\n",
      "Epoch 223 / 500 | iteration 5 / 30 | Total Loss: 6.653914451599121 | KNN Loss: 5.607605934143066 | BCE Loss: 1.0463087558746338\n",
      "Epoch 223 / 500 | iteration 10 / 30 | Total Loss: 6.712721824645996 | KNN Loss: 5.634298324584961 | BCE Loss: 1.0784235000610352\n",
      "Epoch 223 / 500 | iteration 15 / 30 | Total Loss: 6.6793341636657715 | KNN Loss: 5.626143932342529 | BCE Loss: 1.0531903505325317\n",
      "Epoch 223 / 500 | iteration 20 / 30 | Total Loss: 6.643703460693359 | KNN Loss: 5.593364715576172 | BCE Loss: 1.0503387451171875\n",
      "Epoch 223 / 500 | iteration 25 / 30 | Total Loss: 6.654009819030762 | KNN Loss: 5.609828472137451 | BCE Loss: 1.044181227684021\n",
      "Epoch 224 / 500 | iteration 0 / 30 | Total Loss: 6.6751389503479 | KNN Loss: 5.628636360168457 | BCE Loss: 1.0465025901794434\n",
      "Epoch 224 / 500 | iteration 5 / 30 | Total Loss: 6.644015312194824 | KNN Loss: 5.609703063964844 | BCE Loss: 1.0343124866485596\n",
      "Epoch 224 / 500 | iteration 10 / 30 | Total Loss: 6.666928291320801 | KNN Loss: 5.61221170425415 | BCE Loss: 1.0547163486480713\n",
      "Epoch 224 / 500 | iteration 15 / 30 | Total Loss: 6.684764385223389 | KNN Loss: 5.624859809875488 | BCE Loss: 1.0599044561386108\n",
      "Epoch 224 / 500 | iteration 20 / 30 | Total Loss: 6.640705585479736 | KNN Loss: 5.617156982421875 | BCE Loss: 1.0235484838485718\n",
      "Epoch 224 / 500 | iteration 25 / 30 | Total Loss: 6.797733783721924 | KNN Loss: 5.748520851135254 | BCE Loss: 1.0492130517959595\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 225 / 500 | iteration 0 / 30 | Total Loss: 6.7488203048706055 | KNN Loss: 5.6866888999938965 | BCE Loss: 1.0621312856674194\n",
      "Epoch 225 / 500 | iteration 5 / 30 | Total Loss: 6.63664436340332 | KNN Loss: 5.601637363433838 | BCE Loss: 1.0350072383880615\n",
      "Epoch 225 / 500 | iteration 10 / 30 | Total Loss: 6.680408477783203 | KNN Loss: 5.6106696128845215 | BCE Loss: 1.069738745689392\n",
      "Epoch 225 / 500 | iteration 15 / 30 | Total Loss: 6.670209884643555 | KNN Loss: 5.620833396911621 | BCE Loss: 1.049376368522644\n",
      "Epoch 225 / 500 | iteration 20 / 30 | Total Loss: 6.681789875030518 | KNN Loss: 5.611025333404541 | BCE Loss: 1.0707645416259766\n",
      "Epoch 225 / 500 | iteration 25 / 30 | Total Loss: 6.673892974853516 | KNN Loss: 5.638190269470215 | BCE Loss: 1.0357025861740112\n",
      "Epoch 226 / 500 | iteration 0 / 30 | Total Loss: 6.705818176269531 | KNN Loss: 5.663251876831055 | BCE Loss: 1.0425662994384766\n",
      "Epoch 226 / 500 | iteration 5 / 30 | Total Loss: 6.725537300109863 | KNN Loss: 5.6648664474487305 | BCE Loss: 1.060671091079712\n",
      "Epoch 226 / 500 | iteration 10 / 30 | Total Loss: 6.669407367706299 | KNN Loss: 5.636852741241455 | BCE Loss: 1.0325546264648438\n",
      "Epoch 226 / 500 | iteration 15 / 30 | Total Loss: 6.695367813110352 | KNN Loss: 5.654139041900635 | BCE Loss: 1.0412285327911377\n",
      "Epoch 226 / 500 | iteration 20 / 30 | Total Loss: 6.66156530380249 | KNN Loss: 5.635282516479492 | BCE Loss: 1.0262829065322876\n",
      "Epoch 226 / 500 | iteration 25 / 30 | Total Loss: 6.703930854797363 | KNN Loss: 5.6394219398498535 | BCE Loss: 1.0645091533660889\n",
      "Epoch   227: reducing learning rate of group 0 to 4.8445e-05.\n",
      "Epoch 227 / 500 | iteration 0 / 30 | Total Loss: 6.609521865844727 | KNN Loss: 5.610162734985352 | BCE Loss: 0.999359130859375\n",
      "Epoch 227 / 500 | iteration 5 / 30 | Total Loss: 6.743462085723877 | KNN Loss: 5.699018478393555 | BCE Loss: 1.0444437265396118\n",
      "Epoch 227 / 500 | iteration 10 / 30 | Total Loss: 6.696658134460449 | KNN Loss: 5.621109962463379 | BCE Loss: 1.0755481719970703\n",
      "Epoch 227 / 500 | iteration 15 / 30 | Total Loss: 6.748631954193115 | KNN Loss: 5.668852806091309 | BCE Loss: 1.079779028892517\n",
      "Epoch 227 / 500 | iteration 20 / 30 | Total Loss: 6.681066513061523 | KNN Loss: 5.625576972961426 | BCE Loss: 1.0554893016815186\n",
      "Epoch 227 / 500 | iteration 25 / 30 | Total Loss: 6.681635856628418 | KNN Loss: 5.617157936096191 | BCE Loss: 1.0644781589508057\n",
      "Epoch 228 / 500 | iteration 0 / 30 | Total Loss: 6.635092735290527 | KNN Loss: 5.593717098236084 | BCE Loss: 1.0413758754730225\n",
      "Epoch 228 / 500 | iteration 5 / 30 | Total Loss: 6.646883487701416 | KNN Loss: 5.616311550140381 | BCE Loss: 1.0305720567703247\n",
      "Epoch 228 / 500 | iteration 10 / 30 | Total Loss: 6.702282905578613 | KNN Loss: 5.670412540435791 | BCE Loss: 1.0318703651428223\n",
      "Epoch 228 / 500 | iteration 15 / 30 | Total Loss: 6.700606822967529 | KNN Loss: 5.653913974761963 | BCE Loss: 1.0466928482055664\n",
      "Epoch 228 / 500 | iteration 20 / 30 | Total Loss: 6.677770137786865 | KNN Loss: 5.625186920166016 | BCE Loss: 1.05258309841156\n",
      "Epoch 228 / 500 | iteration 25 / 30 | Total Loss: 6.685482978820801 | KNN Loss: 5.628574848175049 | BCE Loss: 1.056908369064331\n",
      "Epoch 229 / 500 | iteration 0 / 30 | Total Loss: 6.686516761779785 | KNN Loss: 5.631874084472656 | BCE Loss: 1.0546424388885498\n",
      "Epoch 229 / 500 | iteration 5 / 30 | Total Loss: 6.661408424377441 | KNN Loss: 5.625090599060059 | BCE Loss: 1.0363178253173828\n",
      "Epoch 229 / 500 | iteration 10 / 30 | Total Loss: 6.6589436531066895 | KNN Loss: 5.59604549407959 | BCE Loss: 1.0628981590270996\n",
      "Epoch 229 / 500 | iteration 15 / 30 | Total Loss: 6.6284942626953125 | KNN Loss: 5.613133907318115 | BCE Loss: 1.0153604745864868\n",
      "Epoch 229 / 500 | iteration 20 / 30 | Total Loss: 6.671482086181641 | KNN Loss: 5.595563888549805 | BCE Loss: 1.075918197631836\n",
      "Epoch 229 / 500 | iteration 25 / 30 | Total Loss: 6.630791664123535 | KNN Loss: 5.593087673187256 | BCE Loss: 1.0377039909362793\n",
      "Epoch 230 / 500 | iteration 0 / 30 | Total Loss: 6.672106742858887 | KNN Loss: 5.623345375061035 | BCE Loss: 1.0487613677978516\n",
      "Epoch 230 / 500 | iteration 5 / 30 | Total Loss: 6.648222923278809 | KNN Loss: 5.627768516540527 | BCE Loss: 1.0204541683197021\n",
      "Epoch 230 / 500 | iteration 10 / 30 | Total Loss: 6.686513423919678 | KNN Loss: 5.632663726806641 | BCE Loss: 1.053849697113037\n",
      "Epoch 230 / 500 | iteration 15 / 30 | Total Loss: 6.700072288513184 | KNN Loss: 5.667763710021973 | BCE Loss: 1.03230881690979\n",
      "Epoch 230 / 500 | iteration 20 / 30 | Total Loss: 6.642604827880859 | KNN Loss: 5.6056694984436035 | BCE Loss: 1.0369352102279663\n",
      "Epoch 230 / 500 | iteration 25 / 30 | Total Loss: 6.627663612365723 | KNN Loss: 5.596898078918457 | BCE Loss: 1.0307652950286865\n",
      "Epoch 231 / 500 | iteration 0 / 30 | Total Loss: 6.730682373046875 | KNN Loss: 5.689974784851074 | BCE Loss: 1.0407078266143799\n",
      "Epoch 231 / 500 | iteration 5 / 30 | Total Loss: 6.626945972442627 | KNN Loss: 5.5967912673950195 | BCE Loss: 1.030154824256897\n",
      "Epoch 231 / 500 | iteration 10 / 30 | Total Loss: 6.65900993347168 | KNN Loss: 5.594725608825684 | BCE Loss: 1.0642845630645752\n",
      "Epoch 231 / 500 | iteration 15 / 30 | Total Loss: 6.686504364013672 | KNN Loss: 5.649399280548096 | BCE Loss: 1.0371049642562866\n",
      "Epoch 231 / 500 | iteration 20 / 30 | Total Loss: 6.760102272033691 | KNN Loss: 5.70541524887085 | BCE Loss: 1.0546871423721313\n",
      "Epoch 231 / 500 | iteration 25 / 30 | Total Loss: 6.660393714904785 | KNN Loss: 5.606578826904297 | BCE Loss: 1.0538146495819092\n",
      "Epoch 232 / 500 | iteration 0 / 30 | Total Loss: 6.695859432220459 | KNN Loss: 5.6266560554504395 | BCE Loss: 1.0692033767700195\n",
      "Epoch 232 / 500 | iteration 5 / 30 | Total Loss: 6.696950435638428 | KNN Loss: 5.6394243240356445 | BCE Loss: 1.0575259923934937\n",
      "Epoch 232 / 500 | iteration 10 / 30 | Total Loss: 6.704239368438721 | KNN Loss: 5.652027130126953 | BCE Loss: 1.0522123575210571\n",
      "Epoch 232 / 500 | iteration 15 / 30 | Total Loss: 6.649186611175537 | KNN Loss: 5.611218452453613 | BCE Loss: 1.0379680395126343\n",
      "Epoch 232 / 500 | iteration 20 / 30 | Total Loss: 6.706099987030029 | KNN Loss: 5.624458312988281 | BCE Loss: 1.081641674041748\n",
      "Epoch 232 / 500 | iteration 25 / 30 | Total Loss: 6.699954032897949 | KNN Loss: 5.6395745277404785 | BCE Loss: 1.0603792667388916\n",
      "Epoch 233 / 500 | iteration 0 / 30 | Total Loss: 6.618697643280029 | KNN Loss: 5.592780590057373 | BCE Loss: 1.0259169340133667\n",
      "Epoch 233 / 500 | iteration 5 / 30 | Total Loss: 6.654404640197754 | KNN Loss: 5.625896453857422 | BCE Loss: 1.0285084247589111\n",
      "Epoch 233 / 500 | iteration 10 / 30 | Total Loss: 6.72105598449707 | KNN Loss: 5.650296688079834 | BCE Loss: 1.0707592964172363\n",
      "Epoch 233 / 500 | iteration 15 / 30 | Total Loss: 6.730286598205566 | KNN Loss: 5.678086280822754 | BCE Loss: 1.052200198173523\n",
      "Epoch 233 / 500 | iteration 20 / 30 | Total Loss: 6.642003059387207 | KNN Loss: 5.600213527679443 | BCE Loss: 1.0417895317077637\n",
      "Epoch 233 / 500 | iteration 25 / 30 | Total Loss: 6.655411720275879 | KNN Loss: 5.604759693145752 | BCE Loss: 1.0506521463394165\n",
      "Epoch 234 / 500 | iteration 0 / 30 | Total Loss: 6.665209770202637 | KNN Loss: 5.598996162414551 | BCE Loss: 1.0662137269973755\n",
      "Epoch 234 / 500 | iteration 5 / 30 | Total Loss: 6.662960052490234 | KNN Loss: 5.594869613647461 | BCE Loss: 1.068090558052063\n",
      "Epoch 234 / 500 | iteration 10 / 30 | Total Loss: 6.718128204345703 | KNN Loss: 5.667877674102783 | BCE Loss: 1.0502506494522095\n",
      "Epoch 234 / 500 | iteration 15 / 30 | Total Loss: 6.728301048278809 | KNN Loss: 5.6771559715271 | BCE Loss: 1.051145076751709\n",
      "Epoch 234 / 500 | iteration 20 / 30 | Total Loss: 6.627869606018066 | KNN Loss: 5.590991020202637 | BCE Loss: 1.0368784666061401\n",
      "Epoch 234 / 500 | iteration 25 / 30 | Total Loss: 6.690069198608398 | KNN Loss: 5.649669170379639 | BCE Loss: 1.0404002666473389\n",
      "Epoch 235 / 500 | iteration 0 / 30 | Total Loss: 6.7112040519714355 | KNN Loss: 5.673550605773926 | BCE Loss: 1.0376534461975098\n",
      "Epoch 235 / 500 | iteration 5 / 30 | Total Loss: 6.679447174072266 | KNN Loss: 5.635737895965576 | BCE Loss: 1.0437091588974\n",
      "Epoch 235 / 500 | iteration 10 / 30 | Total Loss: 6.663660526275635 | KNN Loss: 5.592218399047852 | BCE Loss: 1.0714421272277832\n",
      "Epoch 235 / 500 | iteration 15 / 30 | Total Loss: 6.636416912078857 | KNN Loss: 5.599611759185791 | BCE Loss: 1.0368050336837769\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 235 / 500 | iteration 20 / 30 | Total Loss: 6.676481246948242 | KNN Loss: 5.637205123901367 | BCE Loss: 1.039275884628296\n",
      "Epoch 235 / 500 | iteration 25 / 30 | Total Loss: 6.6517229080200195 | KNN Loss: 5.590928554534912 | BCE Loss: 1.0607943534851074\n",
      "Epoch 236 / 500 | iteration 0 / 30 | Total Loss: 6.660060405731201 | KNN Loss: 5.597643852233887 | BCE Loss: 1.0624165534973145\n",
      "Epoch 236 / 500 | iteration 5 / 30 | Total Loss: 6.682954788208008 | KNN Loss: 5.612449645996094 | BCE Loss: 1.0705053806304932\n",
      "Epoch 236 / 500 | iteration 10 / 30 | Total Loss: 6.718941688537598 | KNN Loss: 5.681159019470215 | BCE Loss: 1.037782907485962\n",
      "Epoch 236 / 500 | iteration 15 / 30 | Total Loss: 6.616495132446289 | KNN Loss: 5.595297336578369 | BCE Loss: 1.0211975574493408\n",
      "Epoch 236 / 500 | iteration 20 / 30 | Total Loss: 6.6557536125183105 | KNN Loss: 5.62200403213501 | BCE Loss: 1.0337495803833008\n",
      "Epoch 236 / 500 | iteration 25 / 30 | Total Loss: 6.655881881713867 | KNN Loss: 5.6051788330078125 | BCE Loss: 1.0507030487060547\n",
      "Epoch 237 / 500 | iteration 0 / 30 | Total Loss: 6.690800666809082 | KNN Loss: 5.650309085845947 | BCE Loss: 1.0404915809631348\n",
      "Epoch 237 / 500 | iteration 5 / 30 | Total Loss: 6.707526206970215 | KNN Loss: 5.680172443389893 | BCE Loss: 1.0273537635803223\n",
      "Epoch 237 / 500 | iteration 10 / 30 | Total Loss: 6.75144100189209 | KNN Loss: 5.68324089050293 | BCE Loss: 1.0682001113891602\n",
      "Epoch 237 / 500 | iteration 15 / 30 | Total Loss: 6.754725456237793 | KNN Loss: 5.689330577850342 | BCE Loss: 1.0653951168060303\n",
      "Epoch 237 / 500 | iteration 20 / 30 | Total Loss: 6.667031764984131 | KNN Loss: 5.612592697143555 | BCE Loss: 1.0544389486312866\n",
      "Epoch 237 / 500 | iteration 25 / 30 | Total Loss: 6.662611961364746 | KNN Loss: 5.622534275054932 | BCE Loss: 1.0400779247283936\n",
      "Epoch   238: reducing learning rate of group 0 to 3.3911e-05.\n",
      "Epoch 238 / 500 | iteration 0 / 30 | Total Loss: 6.675020217895508 | KNN Loss: 5.617042064666748 | BCE Loss: 1.0579781532287598\n",
      "Epoch 238 / 500 | iteration 5 / 30 | Total Loss: 6.6800737380981445 | KNN Loss: 5.648770809173584 | BCE Loss: 1.03130304813385\n",
      "Epoch 238 / 500 | iteration 10 / 30 | Total Loss: 6.728934288024902 | KNN Loss: 5.66822624206543 | BCE Loss: 1.0607080459594727\n",
      "Epoch 238 / 500 | iteration 15 / 30 | Total Loss: 6.655823707580566 | KNN Loss: 5.610342502593994 | BCE Loss: 1.0454814434051514\n",
      "Epoch 238 / 500 | iteration 20 / 30 | Total Loss: 6.736775875091553 | KNN Loss: 5.695159435272217 | BCE Loss: 1.0416165590286255\n",
      "Epoch 238 / 500 | iteration 25 / 30 | Total Loss: 6.755094051361084 | KNN Loss: 5.700796127319336 | BCE Loss: 1.054297924041748\n",
      "Epoch 239 / 500 | iteration 0 / 30 | Total Loss: 6.675553321838379 | KNN Loss: 5.612865924835205 | BCE Loss: 1.0626872777938843\n",
      "Epoch 239 / 500 | iteration 5 / 30 | Total Loss: 6.631137847900391 | KNN Loss: 5.616313457489014 | BCE Loss: 1.014824390411377\n",
      "Epoch 239 / 500 | iteration 10 / 30 | Total Loss: 6.622865676879883 | KNN Loss: 5.59862756729126 | BCE Loss: 1.024237871170044\n",
      "Epoch 239 / 500 | iteration 15 / 30 | Total Loss: 6.678123474121094 | KNN Loss: 5.625932216644287 | BCE Loss: 1.0521914958953857\n",
      "Epoch 239 / 500 | iteration 20 / 30 | Total Loss: 6.656030654907227 | KNN Loss: 5.5945024490356445 | BCE Loss: 1.0615284442901611\n",
      "Epoch 239 / 500 | iteration 25 / 30 | Total Loss: 6.6841044425964355 | KNN Loss: 5.641329765319824 | BCE Loss: 1.0427746772766113\n",
      "Epoch 240 / 500 | iteration 0 / 30 | Total Loss: 6.685183525085449 | KNN Loss: 5.620065212249756 | BCE Loss: 1.0651181936264038\n",
      "Epoch 240 / 500 | iteration 5 / 30 | Total Loss: 6.681173324584961 | KNN Loss: 5.62868595123291 | BCE Loss: 1.0524876117706299\n",
      "Epoch 240 / 500 | iteration 10 / 30 | Total Loss: 6.719509124755859 | KNN Loss: 5.650062084197998 | BCE Loss: 1.0694472789764404\n",
      "Epoch 240 / 500 | iteration 15 / 30 | Total Loss: 6.632275581359863 | KNN Loss: 5.5980634689331055 | BCE Loss: 1.034212350845337\n",
      "Epoch 240 / 500 | iteration 20 / 30 | Total Loss: 6.743875980377197 | KNN Loss: 5.69961404800415 | BCE Loss: 1.0442619323730469\n",
      "Epoch 240 / 500 | iteration 25 / 30 | Total Loss: 6.73647403717041 | KNN Loss: 5.6740899085998535 | BCE Loss: 1.062384009361267\n",
      "Epoch 241 / 500 | iteration 0 / 30 | Total Loss: 6.665226936340332 | KNN Loss: 5.613136291503906 | BCE Loss: 1.0520904064178467\n",
      "Epoch 241 / 500 | iteration 5 / 30 | Total Loss: 6.691891670227051 | KNN Loss: 5.685594081878662 | BCE Loss: 1.0062974691390991\n",
      "Epoch 241 / 500 | iteration 10 / 30 | Total Loss: 6.666790962219238 | KNN Loss: 5.622169494628906 | BCE Loss: 1.0446217060089111\n",
      "Epoch 241 / 500 | iteration 15 / 30 | Total Loss: 6.6419453620910645 | KNN Loss: 5.611793041229248 | BCE Loss: 1.0301523208618164\n",
      "Epoch 241 / 500 | iteration 20 / 30 | Total Loss: 6.672358989715576 | KNN Loss: 5.634102821350098 | BCE Loss: 1.038256049156189\n",
      "Epoch 241 / 500 | iteration 25 / 30 | Total Loss: 6.649040699005127 | KNN Loss: 5.594597816467285 | BCE Loss: 1.0544428825378418\n",
      "Epoch 242 / 500 | iteration 0 / 30 | Total Loss: 6.659000873565674 | KNN Loss: 5.621641635894775 | BCE Loss: 1.0373591184616089\n",
      "Epoch 242 / 500 | iteration 5 / 30 | Total Loss: 6.736717700958252 | KNN Loss: 5.6775360107421875 | BCE Loss: 1.0591816902160645\n",
      "Epoch 242 / 500 | iteration 10 / 30 | Total Loss: 6.785954475402832 | KNN Loss: 5.729569911956787 | BCE Loss: 1.056384801864624\n",
      "Epoch 242 / 500 | iteration 15 / 30 | Total Loss: 6.635522842407227 | KNN Loss: 5.596479415893555 | BCE Loss: 1.0390433073043823\n",
      "Epoch 242 / 500 | iteration 20 / 30 | Total Loss: 6.629317283630371 | KNN Loss: 5.5927276611328125 | BCE Loss: 1.0365898609161377\n",
      "Epoch 242 / 500 | iteration 25 / 30 | Total Loss: 6.725125312805176 | KNN Loss: 5.65366268157959 | BCE Loss: 1.0714623928070068\n",
      "Epoch 243 / 500 | iteration 0 / 30 | Total Loss: 6.726667881011963 | KNN Loss: 5.660551071166992 | BCE Loss: 1.0661166906356812\n",
      "Epoch 243 / 500 | iteration 5 / 30 | Total Loss: 6.723114013671875 | KNN Loss: 5.692882061004639 | BCE Loss: 1.0302319526672363\n",
      "Epoch 243 / 500 | iteration 10 / 30 | Total Loss: 6.667919158935547 | KNN Loss: 5.627379417419434 | BCE Loss: 1.0405399799346924\n",
      "Epoch 243 / 500 | iteration 15 / 30 | Total Loss: 6.674862861633301 | KNN Loss: 5.601154804229736 | BCE Loss: 1.0737080574035645\n",
      "Epoch 243 / 500 | iteration 20 / 30 | Total Loss: 6.66423225402832 | KNN Loss: 5.59469747543335 | BCE Loss: 1.0695350170135498\n",
      "Epoch 243 / 500 | iteration 25 / 30 | Total Loss: 6.653862953186035 | KNN Loss: 5.604197025299072 | BCE Loss: 1.049666166305542\n",
      "Epoch 244 / 500 | iteration 0 / 30 | Total Loss: 6.685164451599121 | KNN Loss: 5.65278959274292 | BCE Loss: 1.0323748588562012\n",
      "Epoch 244 / 500 | iteration 5 / 30 | Total Loss: 6.678964614868164 | KNN Loss: 5.619460105895996 | BCE Loss: 1.059504747390747\n",
      "Epoch 244 / 500 | iteration 10 / 30 | Total Loss: 6.721393585205078 | KNN Loss: 5.678930282592773 | BCE Loss: 1.0424635410308838\n",
      "Epoch 244 / 500 | iteration 15 / 30 | Total Loss: 6.761167526245117 | KNN Loss: 5.720004081726074 | BCE Loss: 1.041163682937622\n",
      "Epoch 244 / 500 | iteration 20 / 30 | Total Loss: 6.659505367279053 | KNN Loss: 5.618325710296631 | BCE Loss: 1.0411796569824219\n",
      "Epoch 244 / 500 | iteration 25 / 30 | Total Loss: 6.6468377113342285 | KNN Loss: 5.616840839385986 | BCE Loss: 1.0299969911575317\n",
      "Epoch 245 / 500 | iteration 0 / 30 | Total Loss: 6.68190336227417 | KNN Loss: 5.642004013061523 | BCE Loss: 1.0398993492126465\n",
      "Epoch 245 / 500 | iteration 5 / 30 | Total Loss: 6.659108638763428 | KNN Loss: 5.63525390625 | BCE Loss: 1.0238547325134277\n",
      "Epoch 245 / 500 | iteration 10 / 30 | Total Loss: 6.649111747741699 | KNN Loss: 5.617080211639404 | BCE Loss: 1.032031774520874\n",
      "Epoch 245 / 500 | iteration 15 / 30 | Total Loss: 6.669662952423096 | KNN Loss: 5.6297712326049805 | BCE Loss: 1.0398917198181152\n",
      "Epoch 245 / 500 | iteration 20 / 30 | Total Loss: 6.62865686416626 | KNN Loss: 5.598099708557129 | BCE Loss: 1.0305572748184204\n",
      "Epoch 245 / 500 | iteration 25 / 30 | Total Loss: 6.655595302581787 | KNN Loss: 5.590259075164795 | BCE Loss: 1.0653362274169922\n",
      "Epoch 246 / 500 | iteration 0 / 30 | Total Loss: 6.652685165405273 | KNN Loss: 5.62684965133667 | BCE Loss: 1.0258352756500244\n",
      "Epoch 246 / 500 | iteration 5 / 30 | Total Loss: 6.680974006652832 | KNN Loss: 5.649647235870361 | BCE Loss: 1.0313266515731812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 246 / 500 | iteration 10 / 30 | Total Loss: 6.679064750671387 | KNN Loss: 5.6255292892456055 | BCE Loss: 1.0535354614257812\n",
      "Epoch 246 / 500 | iteration 15 / 30 | Total Loss: 6.640125751495361 | KNN Loss: 5.611093521118164 | BCE Loss: 1.0290322303771973\n",
      "Epoch 246 / 500 | iteration 20 / 30 | Total Loss: 6.654121398925781 | KNN Loss: 5.597908973693848 | BCE Loss: 1.056212306022644\n",
      "Epoch 246 / 500 | iteration 25 / 30 | Total Loss: 6.663610935211182 | KNN Loss: 5.62062931060791 | BCE Loss: 1.042981505393982\n",
      "Epoch 247 / 500 | iteration 0 / 30 | Total Loss: 6.671507358551025 | KNN Loss: 5.643348693847656 | BCE Loss: 1.0281585454940796\n",
      "Epoch 247 / 500 | iteration 5 / 30 | Total Loss: 6.617877006530762 | KNN Loss: 5.598084926605225 | BCE Loss: 1.019791841506958\n",
      "Epoch 247 / 500 | iteration 10 / 30 | Total Loss: 6.672032356262207 | KNN Loss: 5.607692241668701 | BCE Loss: 1.0643399953842163\n",
      "Epoch 247 / 500 | iteration 15 / 30 | Total Loss: 6.6544013023376465 | KNN Loss: 5.597583770751953 | BCE Loss: 1.056817650794983\n",
      "Epoch 247 / 500 | iteration 20 / 30 | Total Loss: 6.660573959350586 | KNN Loss: 5.611176490783691 | BCE Loss: 1.0493977069854736\n",
      "Epoch 247 / 500 | iteration 25 / 30 | Total Loss: 6.6801066398620605 | KNN Loss: 5.615401744842529 | BCE Loss: 1.0647050142288208\n",
      "Epoch 248 / 500 | iteration 0 / 30 | Total Loss: 6.756280422210693 | KNN Loss: 5.691987037658691 | BCE Loss: 1.064293384552002\n",
      "Epoch 248 / 500 | iteration 5 / 30 | Total Loss: 6.642332553863525 | KNN Loss: 5.595761299133301 | BCE Loss: 1.0465712547302246\n",
      "Epoch 248 / 500 | iteration 10 / 30 | Total Loss: 6.628500938415527 | KNN Loss: 5.602507591247559 | BCE Loss: 1.0259931087493896\n",
      "Epoch 248 / 500 | iteration 15 / 30 | Total Loss: 6.668921947479248 | KNN Loss: 5.612905502319336 | BCE Loss: 1.056016445159912\n",
      "Epoch 248 / 500 | iteration 20 / 30 | Total Loss: 6.652214527130127 | KNN Loss: 5.618686676025391 | BCE Loss: 1.0335277318954468\n",
      "Epoch 248 / 500 | iteration 25 / 30 | Total Loss: 6.669139385223389 | KNN Loss: 5.609445095062256 | BCE Loss: 1.0596944093704224\n",
      "Epoch   249: reducing learning rate of group 0 to 2.3738e-05.\n",
      "Epoch 249 / 500 | iteration 0 / 30 | Total Loss: 6.713423252105713 | KNN Loss: 5.6634626388549805 | BCE Loss: 1.0499606132507324\n",
      "Epoch 249 / 500 | iteration 5 / 30 | Total Loss: 6.71000862121582 | KNN Loss: 5.6524834632873535 | BCE Loss: 1.0575249195098877\n",
      "Epoch 249 / 500 | iteration 10 / 30 | Total Loss: 6.644580364227295 | KNN Loss: 5.606250286102295 | BCE Loss: 1.038330078125\n",
      "Epoch 249 / 500 | iteration 15 / 30 | Total Loss: 6.668497085571289 | KNN Loss: 5.614282131195068 | BCE Loss: 1.0542147159576416\n",
      "Epoch 249 / 500 | iteration 20 / 30 | Total Loss: 6.652953147888184 | KNN Loss: 5.59407901763916 | BCE Loss: 1.0588741302490234\n",
      "Epoch 249 / 500 | iteration 25 / 30 | Total Loss: 6.634270668029785 | KNN Loss: 5.601640701293945 | BCE Loss: 1.032630205154419\n",
      "Epoch 250 / 500 | iteration 0 / 30 | Total Loss: 6.713435649871826 | KNN Loss: 5.654726505279541 | BCE Loss: 1.0587091445922852\n",
      "Epoch 250 / 500 | iteration 5 / 30 | Total Loss: 6.720009803771973 | KNN Loss: 5.644402027130127 | BCE Loss: 1.0756076574325562\n",
      "Epoch 250 / 500 | iteration 10 / 30 | Total Loss: 6.63770866394043 | KNN Loss: 5.60068416595459 | BCE Loss: 1.0370242595672607\n",
      "Epoch 250 / 500 | iteration 15 / 30 | Total Loss: 6.6398468017578125 | KNN Loss: 5.595211029052734 | BCE Loss: 1.0446360111236572\n",
      "Epoch 250 / 500 | iteration 20 / 30 | Total Loss: 6.672718048095703 | KNN Loss: 5.615816593170166 | BCE Loss: 1.0569015741348267\n",
      "Epoch 250 / 500 | iteration 25 / 30 | Total Loss: 6.620192050933838 | KNN Loss: 5.58938455581665 | BCE Loss: 1.030807375907898\n",
      "Epoch 251 / 500 | iteration 0 / 30 | Total Loss: 6.771873950958252 | KNN Loss: 5.711006164550781 | BCE Loss: 1.0608679056167603\n",
      "Epoch 251 / 500 | iteration 5 / 30 | Total Loss: 6.616791725158691 | KNN Loss: 5.590130805969238 | BCE Loss: 1.026660680770874\n",
      "Epoch 251 / 500 | iteration 10 / 30 | Total Loss: 6.651168346405029 | KNN Loss: 5.610929012298584 | BCE Loss: 1.0402393341064453\n",
      "Epoch 251 / 500 | iteration 15 / 30 | Total Loss: 6.738668441772461 | KNN Loss: 5.67644739151001 | BCE Loss: 1.0622209310531616\n",
      "Epoch 251 / 500 | iteration 20 / 30 | Total Loss: 6.638242721557617 | KNN Loss: 5.602153778076172 | BCE Loss: 1.0360888242721558\n",
      "Epoch 251 / 500 | iteration 25 / 30 | Total Loss: 6.705427169799805 | KNN Loss: 5.659820079803467 | BCE Loss: 1.0456068515777588\n",
      "Epoch 252 / 500 | iteration 0 / 30 | Total Loss: 6.6447248458862305 | KNN Loss: 5.6079864501953125 | BCE Loss: 1.0367381572723389\n",
      "Epoch 252 / 500 | iteration 5 / 30 | Total Loss: 6.761446475982666 | KNN Loss: 5.695343971252441 | BCE Loss: 1.066102385520935\n",
      "Epoch 252 / 500 | iteration 10 / 30 | Total Loss: 6.693690299987793 | KNN Loss: 5.641729354858398 | BCE Loss: 1.0519607067108154\n",
      "Epoch 252 / 500 | iteration 15 / 30 | Total Loss: 6.651671409606934 | KNN Loss: 5.641880035400391 | BCE Loss: 1.0097911357879639\n",
      "Epoch 252 / 500 | iteration 20 / 30 | Total Loss: 6.642170429229736 | KNN Loss: 5.602740287780762 | BCE Loss: 1.039430022239685\n",
      "Epoch 252 / 500 | iteration 25 / 30 | Total Loss: 6.64223051071167 | KNN Loss: 5.597718715667725 | BCE Loss: 1.0445116758346558\n",
      "Epoch 253 / 500 | iteration 0 / 30 | Total Loss: 6.733615875244141 | KNN Loss: 5.678677558898926 | BCE Loss: 1.0549380779266357\n",
      "Epoch 253 / 500 | iteration 5 / 30 | Total Loss: 6.670601844787598 | KNN Loss: 5.618644714355469 | BCE Loss: 1.0519572496414185\n",
      "Epoch 253 / 500 | iteration 10 / 30 | Total Loss: 6.6410698890686035 | KNN Loss: 5.595144271850586 | BCE Loss: 1.0459256172180176\n",
      "Epoch 253 / 500 | iteration 15 / 30 | Total Loss: 6.664763450622559 | KNN Loss: 5.639604091644287 | BCE Loss: 1.025159478187561\n",
      "Epoch 253 / 500 | iteration 20 / 30 | Total Loss: 6.638384819030762 | KNN Loss: 5.601047039031982 | BCE Loss: 1.0373375415802002\n",
      "Epoch 253 / 500 | iteration 25 / 30 | Total Loss: 6.663108825683594 | KNN Loss: 5.602559566497803 | BCE Loss: 1.0605493783950806\n",
      "Epoch 254 / 500 | iteration 0 / 30 | Total Loss: 6.620987892150879 | KNN Loss: 5.593336582183838 | BCE Loss: 1.027651309967041\n",
      "Epoch 254 / 500 | iteration 5 / 30 | Total Loss: 6.651021957397461 | KNN Loss: 5.608607769012451 | BCE Loss: 1.0424141883850098\n",
      "Epoch 254 / 500 | iteration 10 / 30 | Total Loss: 6.659592628479004 | KNN Loss: 5.615087032318115 | BCE Loss: 1.0445055961608887\n",
      "Epoch 254 / 500 | iteration 15 / 30 | Total Loss: 6.628256797790527 | KNN Loss: 5.592126369476318 | BCE Loss: 1.036130428314209\n",
      "Epoch 254 / 500 | iteration 20 / 30 | Total Loss: 6.681125164031982 | KNN Loss: 5.630533218383789 | BCE Loss: 1.0505919456481934\n",
      "Epoch 254 / 500 | iteration 25 / 30 | Total Loss: 6.679084777832031 | KNN Loss: 5.648006916046143 | BCE Loss: 1.0310778617858887\n",
      "Epoch 255 / 500 | iteration 0 / 30 | Total Loss: 6.673406600952148 | KNN Loss: 5.626038551330566 | BCE Loss: 1.0473682880401611\n",
      "Epoch 255 / 500 | iteration 5 / 30 | Total Loss: 6.640052318572998 | KNN Loss: 5.599689960479736 | BCE Loss: 1.0403624773025513\n",
      "Epoch 255 / 500 | iteration 10 / 30 | Total Loss: 6.713113784790039 | KNN Loss: 5.6704912185668945 | BCE Loss: 1.0426223278045654\n",
      "Epoch 255 / 500 | iteration 15 / 30 | Total Loss: 6.6443986892700195 | KNN Loss: 5.596837520599365 | BCE Loss: 1.0475614070892334\n",
      "Epoch 255 / 500 | iteration 20 / 30 | Total Loss: 6.6257853507995605 | KNN Loss: 5.599698066711426 | BCE Loss: 1.0260872840881348\n",
      "Epoch 255 / 500 | iteration 25 / 30 | Total Loss: 6.686845779418945 | KNN Loss: 5.6287312507629395 | BCE Loss: 1.0581144094467163\n",
      "Epoch 256 / 500 | iteration 0 / 30 | Total Loss: 6.729698657989502 | KNN Loss: 5.637894630432129 | BCE Loss: 1.091804027557373\n",
      "Epoch 256 / 500 | iteration 5 / 30 | Total Loss: 6.658965110778809 | KNN Loss: 5.643406867980957 | BCE Loss: 1.0155582427978516\n",
      "Epoch 256 / 500 | iteration 10 / 30 | Total Loss: 6.697498321533203 | KNN Loss: 5.662558555603027 | BCE Loss: 1.0349400043487549\n",
      "Epoch 256 / 500 | iteration 15 / 30 | Total Loss: 6.638967990875244 | KNN Loss: 5.61187744140625 | BCE Loss: 1.0270905494689941\n",
      "Epoch 256 / 500 | iteration 20 / 30 | Total Loss: 6.655472278594971 | KNN Loss: 5.600247383117676 | BCE Loss: 1.0552250146865845\n",
      "Epoch 256 / 500 | iteration 25 / 30 | Total Loss: 6.705252170562744 | KNN Loss: 5.687188625335693 | BCE Loss: 1.0180636644363403\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 257 / 500 | iteration 0 / 30 | Total Loss: 6.647035598754883 | KNN Loss: 5.5970282554626465 | BCE Loss: 1.0500075817108154\n",
      "Epoch 257 / 500 | iteration 5 / 30 | Total Loss: 6.68233585357666 | KNN Loss: 5.617194175720215 | BCE Loss: 1.0651416778564453\n",
      "Epoch 257 / 500 | iteration 10 / 30 | Total Loss: 6.690984725952148 | KNN Loss: 5.653266906738281 | BCE Loss: 1.0377180576324463\n",
      "Epoch 257 / 500 | iteration 15 / 30 | Total Loss: 6.6489577293396 | KNN Loss: 5.59602689743042 | BCE Loss: 1.0529308319091797\n",
      "Epoch 257 / 500 | iteration 20 / 30 | Total Loss: 6.697073936462402 | KNN Loss: 5.661931037902832 | BCE Loss: 1.0351431369781494\n",
      "Epoch 257 / 500 | iteration 25 / 30 | Total Loss: 6.632165908813477 | KNN Loss: 5.593078136444092 | BCE Loss: 1.0390878915786743\n",
      "Epoch 258 / 500 | iteration 0 / 30 | Total Loss: 6.637179851531982 | KNN Loss: 5.594545364379883 | BCE Loss: 1.0426344871520996\n",
      "Epoch 258 / 500 | iteration 5 / 30 | Total Loss: 6.656878471374512 | KNN Loss: 5.594600677490234 | BCE Loss: 1.062277913093567\n",
      "Epoch 258 / 500 | iteration 10 / 30 | Total Loss: 6.620212554931641 | KNN Loss: 5.5976881980896 | BCE Loss: 1.022524356842041\n",
      "Epoch 258 / 500 | iteration 15 / 30 | Total Loss: 6.681710243225098 | KNN Loss: 5.662243366241455 | BCE Loss: 1.0194666385650635\n",
      "Epoch 258 / 500 | iteration 20 / 30 | Total Loss: 6.670140266418457 | KNN Loss: 5.604428768157959 | BCE Loss: 1.0657117366790771\n",
      "Epoch 258 / 500 | iteration 25 / 30 | Total Loss: 6.74535608291626 | KNN Loss: 5.691340923309326 | BCE Loss: 1.0540151596069336\n",
      "Epoch 259 / 500 | iteration 0 / 30 | Total Loss: 6.68607234954834 | KNN Loss: 5.592965602874756 | BCE Loss: 1.093106985092163\n",
      "Epoch 259 / 500 | iteration 5 / 30 | Total Loss: 6.694845199584961 | KNN Loss: 5.626096248626709 | BCE Loss: 1.0687488317489624\n",
      "Epoch 259 / 500 | iteration 10 / 30 | Total Loss: 6.662720203399658 | KNN Loss: 5.622846603393555 | BCE Loss: 1.039873480796814\n",
      "Epoch 259 / 500 | iteration 15 / 30 | Total Loss: 6.7109055519104 | KNN Loss: 5.673076629638672 | BCE Loss: 1.037828803062439\n",
      "Epoch 259 / 500 | iteration 20 / 30 | Total Loss: 6.70994234085083 | KNN Loss: 5.673424243927002 | BCE Loss: 1.0365180969238281\n",
      "Epoch 259 / 500 | iteration 25 / 30 | Total Loss: 6.727102279663086 | KNN Loss: 5.6594767570495605 | BCE Loss: 1.0676257610321045\n",
      "Epoch   260: reducing learning rate of group 0 to 1.6616e-05.\n",
      "Epoch 260 / 500 | iteration 0 / 30 | Total Loss: 6.662996292114258 | KNN Loss: 5.6372971534729 | BCE Loss: 1.025699257850647\n",
      "Epoch 260 / 500 | iteration 5 / 30 | Total Loss: 6.694623947143555 | KNN Loss: 5.627828598022461 | BCE Loss: 1.0667952299118042\n",
      "Epoch 260 / 500 | iteration 10 / 30 | Total Loss: 6.710300445556641 | KNN Loss: 5.664309978485107 | BCE Loss: 1.045990228652954\n",
      "Epoch 260 / 500 | iteration 15 / 30 | Total Loss: 6.776524543762207 | KNN Loss: 5.713458061218262 | BCE Loss: 1.0630662441253662\n",
      "Epoch 260 / 500 | iteration 20 / 30 | Total Loss: 6.676218509674072 | KNN Loss: 5.616190433502197 | BCE Loss: 1.060028076171875\n",
      "Epoch 260 / 500 | iteration 25 / 30 | Total Loss: 6.657893657684326 | KNN Loss: 5.630332946777344 | BCE Loss: 1.0275605916976929\n",
      "Epoch 261 / 500 | iteration 0 / 30 | Total Loss: 6.778234958648682 | KNN Loss: 5.714840888977051 | BCE Loss: 1.0633939504623413\n",
      "Epoch 261 / 500 | iteration 5 / 30 | Total Loss: 6.770844459533691 | KNN Loss: 5.69150972366333 | BCE Loss: 1.0793349742889404\n",
      "Epoch 261 / 500 | iteration 10 / 30 | Total Loss: 6.629960060119629 | KNN Loss: 5.594615459442139 | BCE Loss: 1.0353444814682007\n",
      "Epoch 261 / 500 | iteration 15 / 30 | Total Loss: 6.707562446594238 | KNN Loss: 5.659953594207764 | BCE Loss: 1.0476086139678955\n",
      "Epoch 261 / 500 | iteration 20 / 30 | Total Loss: 6.649425029754639 | KNN Loss: 5.620667457580566 | BCE Loss: 1.0287574529647827\n",
      "Epoch 261 / 500 | iteration 25 / 30 | Total Loss: 6.626794338226318 | KNN Loss: 5.610313892364502 | BCE Loss: 1.0164804458618164\n",
      "Epoch 262 / 500 | iteration 0 / 30 | Total Loss: 6.644418716430664 | KNN Loss: 5.6075544357299805 | BCE Loss: 1.0368645191192627\n",
      "Epoch 262 / 500 | iteration 5 / 30 | Total Loss: 6.668962478637695 | KNN Loss: 5.648913860321045 | BCE Loss: 1.0200483798980713\n",
      "Epoch 262 / 500 | iteration 10 / 30 | Total Loss: 6.669973373413086 | KNN Loss: 5.623983383178711 | BCE Loss: 1.045989990234375\n",
      "Epoch 262 / 500 | iteration 15 / 30 | Total Loss: 6.669625282287598 | KNN Loss: 5.618608474731445 | BCE Loss: 1.0510168075561523\n",
      "Epoch 262 / 500 | iteration 20 / 30 | Total Loss: 6.692265510559082 | KNN Loss: 5.598316669464111 | BCE Loss: 1.0939486026763916\n",
      "Epoch 262 / 500 | iteration 25 / 30 | Total Loss: 6.676621913909912 | KNN Loss: 5.607158660888672 | BCE Loss: 1.0694631338119507\n",
      "Epoch 263 / 500 | iteration 0 / 30 | Total Loss: 6.699469089508057 | KNN Loss: 5.61559534072876 | BCE Loss: 1.0838737487792969\n",
      "Epoch 263 / 500 | iteration 5 / 30 | Total Loss: 6.729043960571289 | KNN Loss: 5.67287540435791 | BCE Loss: 1.056168794631958\n",
      "Epoch 263 / 500 | iteration 10 / 30 | Total Loss: 6.609099388122559 | KNN Loss: 5.595784664154053 | BCE Loss: 1.013314962387085\n",
      "Epoch 263 / 500 | iteration 15 / 30 | Total Loss: 6.689908981323242 | KNN Loss: 5.63347053527832 | BCE Loss: 1.0564384460449219\n",
      "Epoch 263 / 500 | iteration 20 / 30 | Total Loss: 6.681870460510254 | KNN Loss: 5.640913486480713 | BCE Loss: 1.040956735610962\n",
      "Epoch 263 / 500 | iteration 25 / 30 | Total Loss: 6.627620697021484 | KNN Loss: 5.60038948059082 | BCE Loss: 1.0272314548492432\n",
      "Epoch 264 / 500 | iteration 0 / 30 | Total Loss: 6.654302597045898 | KNN Loss: 5.595626354217529 | BCE Loss: 1.0586761236190796\n",
      "Epoch 264 / 500 | iteration 5 / 30 | Total Loss: 6.679730415344238 | KNN Loss: 5.650350093841553 | BCE Loss: 1.0293800830841064\n",
      "Epoch 264 / 500 | iteration 10 / 30 | Total Loss: 6.676057815551758 | KNN Loss: 5.631862640380859 | BCE Loss: 1.0441951751708984\n",
      "Epoch 264 / 500 | iteration 15 / 30 | Total Loss: 6.6609392166137695 | KNN Loss: 5.6184401512146 | BCE Loss: 1.04249906539917\n",
      "Epoch 264 / 500 | iteration 20 / 30 | Total Loss: 6.632832050323486 | KNN Loss: 5.602939605712891 | BCE Loss: 1.0298924446105957\n",
      "Epoch 264 / 500 | iteration 25 / 30 | Total Loss: 6.7080979347229 | KNN Loss: 5.641098499298096 | BCE Loss: 1.0669994354248047\n",
      "Epoch 265 / 500 | iteration 0 / 30 | Total Loss: 6.733220100402832 | KNN Loss: 5.684699535369873 | BCE Loss: 1.048520565032959\n",
      "Epoch 265 / 500 | iteration 5 / 30 | Total Loss: 6.714219093322754 | KNN Loss: 5.704568386077881 | BCE Loss: 1.0096509456634521\n",
      "Epoch 265 / 500 | iteration 10 / 30 | Total Loss: 6.681849956512451 | KNN Loss: 5.650558948516846 | BCE Loss: 1.031291127204895\n",
      "Epoch 265 / 500 | iteration 15 / 30 | Total Loss: 6.632246971130371 | KNN Loss: 5.595253944396973 | BCE Loss: 1.0369932651519775\n",
      "Epoch 265 / 500 | iteration 20 / 30 | Total Loss: 6.752257823944092 | KNN Loss: 5.647892475128174 | BCE Loss: 1.1043652296066284\n",
      "Epoch 265 / 500 | iteration 25 / 30 | Total Loss: 6.672739028930664 | KNN Loss: 5.595245838165283 | BCE Loss: 1.0774929523468018\n",
      "Epoch 266 / 500 | iteration 0 / 30 | Total Loss: 6.6508893966674805 | KNN Loss: 5.617393970489502 | BCE Loss: 1.033495545387268\n",
      "Epoch 266 / 500 | iteration 5 / 30 | Total Loss: 6.662257194519043 | KNN Loss: 5.610130786895752 | BCE Loss: 1.052126169204712\n",
      "Epoch 266 / 500 | iteration 10 / 30 | Total Loss: 6.663123607635498 | KNN Loss: 5.591253280639648 | BCE Loss: 1.07187020778656\n",
      "Epoch 266 / 500 | iteration 15 / 30 | Total Loss: 6.638513565063477 | KNN Loss: 5.592451095581055 | BCE Loss: 1.0460625886917114\n",
      "Epoch 266 / 500 | iteration 20 / 30 | Total Loss: 6.648331165313721 | KNN Loss: 5.596257209777832 | BCE Loss: 1.0520739555358887\n",
      "Epoch 266 / 500 | iteration 25 / 30 | Total Loss: 6.663793563842773 | KNN Loss: 5.600473403930664 | BCE Loss: 1.063320279121399\n",
      "Epoch 267 / 500 | iteration 0 / 30 | Total Loss: 6.674521446228027 | KNN Loss: 5.60811710357666 | BCE Loss: 1.0664043426513672\n",
      "Epoch 267 / 500 | iteration 5 / 30 | Total Loss: 6.648778915405273 | KNN Loss: 5.591544151306152 | BCE Loss: 1.057234525680542\n",
      "Epoch 267 / 500 | iteration 10 / 30 | Total Loss: 6.693901062011719 | KNN Loss: 5.660431385040283 | BCE Loss: 1.0334696769714355\n",
      "Epoch 267 / 500 | iteration 15 / 30 | Total Loss: 6.652384281158447 | KNN Loss: 5.608578205108643 | BCE Loss: 1.0438059568405151\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 267 / 500 | iteration 20 / 30 | Total Loss: 6.689516544342041 | KNN Loss: 5.6289472579956055 | BCE Loss: 1.0605692863464355\n",
      "Epoch 267 / 500 | iteration 25 / 30 | Total Loss: 6.746791362762451 | KNN Loss: 5.7048773765563965 | BCE Loss: 1.0419139862060547\n",
      "Epoch 268 / 500 | iteration 0 / 30 | Total Loss: 6.665771007537842 | KNN Loss: 5.611189842224121 | BCE Loss: 1.0545812845230103\n",
      "Epoch 268 / 500 | iteration 5 / 30 | Total Loss: 6.712515830993652 | KNN Loss: 5.6839375495910645 | BCE Loss: 1.0285780429840088\n",
      "Epoch 268 / 500 | iteration 10 / 30 | Total Loss: 6.671202182769775 | KNN Loss: 5.594386100769043 | BCE Loss: 1.0768160820007324\n",
      "Epoch 268 / 500 | iteration 15 / 30 | Total Loss: 6.713964462280273 | KNN Loss: 5.637327194213867 | BCE Loss: 1.0766371488571167\n",
      "Epoch 268 / 500 | iteration 20 / 30 | Total Loss: 6.661250114440918 | KNN Loss: 5.639242172241211 | BCE Loss: 1.022007703781128\n",
      "Epoch 268 / 500 | iteration 25 / 30 | Total Loss: 6.665184020996094 | KNN Loss: 5.611840724945068 | BCE Loss: 1.0533435344696045\n",
      "Epoch 269 / 500 | iteration 0 / 30 | Total Loss: 6.642988681793213 | KNN Loss: 5.61428165435791 | BCE Loss: 1.0287070274353027\n",
      "Epoch 269 / 500 | iteration 5 / 30 | Total Loss: 6.686151504516602 | KNN Loss: 5.655270099639893 | BCE Loss: 1.0308811664581299\n",
      "Epoch 269 / 500 | iteration 10 / 30 | Total Loss: 6.630762100219727 | KNN Loss: 5.621175289154053 | BCE Loss: 1.0095869302749634\n",
      "Epoch 269 / 500 | iteration 15 / 30 | Total Loss: 6.643088340759277 | KNN Loss: 5.600002288818359 | BCE Loss: 1.0430861711502075\n",
      "Epoch 269 / 500 | iteration 20 / 30 | Total Loss: 6.732490539550781 | KNN Loss: 5.657613754272461 | BCE Loss: 1.0748767852783203\n",
      "Epoch 269 / 500 | iteration 25 / 30 | Total Loss: 6.649620056152344 | KNN Loss: 5.612741947174072 | BCE Loss: 1.036878228187561\n",
      "Epoch 270 / 500 | iteration 0 / 30 | Total Loss: 6.705868244171143 | KNN Loss: 5.652204990386963 | BCE Loss: 1.0536633729934692\n",
      "Epoch 270 / 500 | iteration 5 / 30 | Total Loss: 6.730423927307129 | KNN Loss: 5.68986701965332 | BCE Loss: 1.0405566692352295\n",
      "Epoch 270 / 500 | iteration 10 / 30 | Total Loss: 6.646444797515869 | KNN Loss: 5.615635871887207 | BCE Loss: 1.0308090448379517\n",
      "Epoch 270 / 500 | iteration 15 / 30 | Total Loss: 6.697903156280518 | KNN Loss: 5.657134532928467 | BCE Loss: 1.0407687425613403\n",
      "Epoch 270 / 500 | iteration 20 / 30 | Total Loss: 6.737706184387207 | KNN Loss: 5.659917831420898 | BCE Loss: 1.0777881145477295\n",
      "Epoch 270 / 500 | iteration 25 / 30 | Total Loss: 6.673078536987305 | KNN Loss: 5.602992534637451 | BCE Loss: 1.070085883140564\n",
      "Epoch   271: reducing learning rate of group 0 to 1.1632e-05.\n",
      "Epoch 271 / 500 | iteration 0 / 30 | Total Loss: 6.667365074157715 | KNN Loss: 5.622975826263428 | BCE Loss: 1.044389009475708\n",
      "Epoch 271 / 500 | iteration 5 / 30 | Total Loss: 6.685244560241699 | KNN Loss: 5.6518378257751465 | BCE Loss: 1.0334068536758423\n",
      "Epoch 271 / 500 | iteration 10 / 30 | Total Loss: 6.618424415588379 | KNN Loss: 5.595402240753174 | BCE Loss: 1.023021936416626\n",
      "Epoch 271 / 500 | iteration 15 / 30 | Total Loss: 6.704104423522949 | KNN Loss: 5.6319580078125 | BCE Loss: 1.0721462965011597\n",
      "Epoch 271 / 500 | iteration 20 / 30 | Total Loss: 6.708665370941162 | KNN Loss: 5.646948337554932 | BCE Loss: 1.061716914176941\n",
      "Epoch 271 / 500 | iteration 25 / 30 | Total Loss: 6.664319038391113 | KNN Loss: 5.599967956542969 | BCE Loss: 1.0643510818481445\n",
      "Epoch 272 / 500 | iteration 0 / 30 | Total Loss: 6.7299065589904785 | KNN Loss: 5.641117095947266 | BCE Loss: 1.088789463043213\n",
      "Epoch 272 / 500 | iteration 5 / 30 | Total Loss: 6.637687683105469 | KNN Loss: 5.601256847381592 | BCE Loss: 1.036430835723877\n",
      "Epoch 272 / 500 | iteration 10 / 30 | Total Loss: 6.704868793487549 | KNN Loss: 5.652252674102783 | BCE Loss: 1.0526161193847656\n",
      "Epoch 272 / 500 | iteration 15 / 30 | Total Loss: 6.691318988800049 | KNN Loss: 5.602464199066162 | BCE Loss: 1.0888546705245972\n",
      "Epoch 272 / 500 | iteration 20 / 30 | Total Loss: 6.632205486297607 | KNN Loss: 5.593477249145508 | BCE Loss: 1.0387282371520996\n",
      "Epoch 272 / 500 | iteration 25 / 30 | Total Loss: 6.692591667175293 | KNN Loss: 5.6540985107421875 | BCE Loss: 1.0384933948516846\n",
      "Epoch 273 / 500 | iteration 0 / 30 | Total Loss: 6.648126602172852 | KNN Loss: 5.612936019897461 | BCE Loss: 1.0351905822753906\n",
      "Epoch 273 / 500 | iteration 5 / 30 | Total Loss: 6.662707328796387 | KNN Loss: 5.625164031982422 | BCE Loss: 1.037543535232544\n",
      "Epoch 273 / 500 | iteration 10 / 30 | Total Loss: 6.694994926452637 | KNN Loss: 5.638120651245117 | BCE Loss: 1.05687415599823\n",
      "Epoch 273 / 500 | iteration 15 / 30 | Total Loss: 6.681482315063477 | KNN Loss: 5.6106438636779785 | BCE Loss: 1.0708386898040771\n",
      "Epoch 273 / 500 | iteration 20 / 30 | Total Loss: 6.741198539733887 | KNN Loss: 5.679980278015137 | BCE Loss: 1.0612183809280396\n",
      "Epoch 273 / 500 | iteration 25 / 30 | Total Loss: 6.688777923583984 | KNN Loss: 5.624030590057373 | BCE Loss: 1.0647475719451904\n",
      "Epoch 274 / 500 | iteration 0 / 30 | Total Loss: 6.6416120529174805 | KNN Loss: 5.607177257537842 | BCE Loss: 1.0344350337982178\n",
      "Epoch 274 / 500 | iteration 5 / 30 | Total Loss: 6.626902103424072 | KNN Loss: 5.6191864013671875 | BCE Loss: 1.0077157020568848\n",
      "Epoch 274 / 500 | iteration 10 / 30 | Total Loss: 6.673288822174072 | KNN Loss: 5.621440887451172 | BCE Loss: 1.0518479347229004\n",
      "Epoch 274 / 500 | iteration 15 / 30 | Total Loss: 6.638491630554199 | KNN Loss: 5.612168312072754 | BCE Loss: 1.0263234376907349\n",
      "Epoch 274 / 500 | iteration 20 / 30 | Total Loss: 6.671469688415527 | KNN Loss: 5.655849456787109 | BCE Loss: 1.015620231628418\n",
      "Epoch 274 / 500 | iteration 25 / 30 | Total Loss: 6.665249824523926 | KNN Loss: 5.6318745613098145 | BCE Loss: 1.0333755016326904\n",
      "Epoch 275 / 500 | iteration 0 / 30 | Total Loss: 6.67336368560791 | KNN Loss: 5.622701644897461 | BCE Loss: 1.0506620407104492\n",
      "Epoch 275 / 500 | iteration 5 / 30 | Total Loss: 6.650946617126465 | KNN Loss: 5.603908061981201 | BCE Loss: 1.0470385551452637\n",
      "Epoch 275 / 500 | iteration 10 / 30 | Total Loss: 6.751750469207764 | KNN Loss: 5.669731616973877 | BCE Loss: 1.0820188522338867\n",
      "Epoch 275 / 500 | iteration 15 / 30 | Total Loss: 6.655522346496582 | KNN Loss: 5.619610786437988 | BCE Loss: 1.0359115600585938\n",
      "Epoch 275 / 500 | iteration 20 / 30 | Total Loss: 6.679191589355469 | KNN Loss: 5.61820125579834 | BCE Loss: 1.060990571975708\n",
      "Epoch 275 / 500 | iteration 25 / 30 | Total Loss: 6.656137466430664 | KNN Loss: 5.634672164916992 | BCE Loss: 1.0214653015136719\n",
      "Epoch 276 / 500 | iteration 0 / 30 | Total Loss: 6.707374572753906 | KNN Loss: 5.6772236824035645 | BCE Loss: 1.0301507711410522\n",
      "Epoch 276 / 500 | iteration 5 / 30 | Total Loss: 6.625836372375488 | KNN Loss: 5.612431526184082 | BCE Loss: 1.0134048461914062\n",
      "Epoch 276 / 500 | iteration 10 / 30 | Total Loss: 6.698329925537109 | KNN Loss: 5.65248441696167 | BCE Loss: 1.0458457469940186\n",
      "Epoch 276 / 500 | iteration 15 / 30 | Total Loss: 6.692563056945801 | KNN Loss: 5.65686559677124 | BCE Loss: 1.03569757938385\n",
      "Epoch 276 / 500 | iteration 20 / 30 | Total Loss: 6.68443489074707 | KNN Loss: 5.635599613189697 | BCE Loss: 1.0488353967666626\n",
      "Epoch 276 / 500 | iteration 25 / 30 | Total Loss: 6.693948745727539 | KNN Loss: 5.665815830230713 | BCE Loss: 1.0281331539154053\n",
      "Epoch 277 / 500 | iteration 0 / 30 | Total Loss: 6.636161804199219 | KNN Loss: 5.597195625305176 | BCE Loss: 1.038966417312622\n",
      "Epoch 277 / 500 | iteration 5 / 30 | Total Loss: 6.665359973907471 | KNN Loss: 5.613151550292969 | BCE Loss: 1.052208423614502\n",
      "Epoch 277 / 500 | iteration 10 / 30 | Total Loss: 6.690288543701172 | KNN Loss: 5.667551517486572 | BCE Loss: 1.0227370262145996\n",
      "Epoch 277 / 500 | iteration 15 / 30 | Total Loss: 6.665936470031738 | KNN Loss: 5.63932991027832 | BCE Loss: 1.0266063213348389\n",
      "Epoch 277 / 500 | iteration 20 / 30 | Total Loss: 6.653416633605957 | KNN Loss: 5.606876850128174 | BCE Loss: 1.046539545059204\n",
      "Epoch 277 / 500 | iteration 25 / 30 | Total Loss: 6.642537593841553 | KNN Loss: 5.608288288116455 | BCE Loss: 1.034249186515808\n",
      "Epoch 278 / 500 | iteration 0 / 30 | Total Loss: 6.646739482879639 | KNN Loss: 5.610017776489258 | BCE Loss: 1.0367218255996704\n",
      "Epoch 278 / 500 | iteration 5 / 30 | Total Loss: 6.704894542694092 | KNN Loss: 5.628546237945557 | BCE Loss: 1.0763483047485352\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 278 / 500 | iteration 10 / 30 | Total Loss: 6.702492713928223 | KNN Loss: 5.662189960479736 | BCE Loss: 1.0403025150299072\n",
      "Epoch 278 / 500 | iteration 15 / 30 | Total Loss: 6.716117858886719 | KNN Loss: 5.6305389404296875 | BCE Loss: 1.0855786800384521\n",
      "Epoch 278 / 500 | iteration 20 / 30 | Total Loss: 6.6608076095581055 | KNN Loss: 5.601461887359619 | BCE Loss: 1.0593459606170654\n",
      "Epoch 278 / 500 | iteration 25 / 30 | Total Loss: 6.646828651428223 | KNN Loss: 5.595900058746338 | BCE Loss: 1.0509283542633057\n",
      "Epoch 279 / 500 | iteration 0 / 30 | Total Loss: 6.735372543334961 | KNN Loss: 5.673603534698486 | BCE Loss: 1.0617690086364746\n",
      "Epoch 279 / 500 | iteration 5 / 30 | Total Loss: 6.704068660736084 | KNN Loss: 5.630214214324951 | BCE Loss: 1.0738544464111328\n",
      "Epoch 279 / 500 | iteration 10 / 30 | Total Loss: 6.738284111022949 | KNN Loss: 5.676184177398682 | BCE Loss: 1.062099814414978\n",
      "Epoch 279 / 500 | iteration 15 / 30 | Total Loss: 6.7389068603515625 | KNN Loss: 5.672243595123291 | BCE Loss: 1.0666635036468506\n",
      "Epoch 279 / 500 | iteration 20 / 30 | Total Loss: 6.712773323059082 | KNN Loss: 5.637783527374268 | BCE Loss: 1.074989914894104\n",
      "Epoch 279 / 500 | iteration 25 / 30 | Total Loss: 6.649591445922852 | KNN Loss: 5.622944355010986 | BCE Loss: 1.0266469717025757\n",
      "Epoch 280 / 500 | iteration 0 / 30 | Total Loss: 6.64761209487915 | KNN Loss: 5.601161956787109 | BCE Loss: 1.046450138092041\n",
      "Epoch 280 / 500 | iteration 5 / 30 | Total Loss: 6.753974437713623 | KNN Loss: 5.700298309326172 | BCE Loss: 1.0536761283874512\n",
      "Epoch 280 / 500 | iteration 10 / 30 | Total Loss: 6.691292762756348 | KNN Loss: 5.627679347991943 | BCE Loss: 1.0636134147644043\n",
      "Epoch 280 / 500 | iteration 15 / 30 | Total Loss: 6.70378303527832 | KNN Loss: 5.6559247970581055 | BCE Loss: 1.047858476638794\n",
      "Epoch 280 / 500 | iteration 20 / 30 | Total Loss: 6.691140174865723 | KNN Loss: 5.634782791137695 | BCE Loss: 1.0563572645187378\n",
      "Epoch 280 / 500 | iteration 25 / 30 | Total Loss: 6.673441410064697 | KNN Loss: 5.595828533172607 | BCE Loss: 1.0776128768920898\n",
      "Epoch 281 / 500 | iteration 0 / 30 | Total Loss: 6.6733269691467285 | KNN Loss: 5.634700298309326 | BCE Loss: 1.0386265516281128\n",
      "Epoch 281 / 500 | iteration 5 / 30 | Total Loss: 6.775927543640137 | KNN Loss: 5.730454921722412 | BCE Loss: 1.0454727411270142\n",
      "Epoch 281 / 500 | iteration 10 / 30 | Total Loss: 6.704188823699951 | KNN Loss: 5.644742488861084 | BCE Loss: 1.0594463348388672\n",
      "Epoch 281 / 500 | iteration 15 / 30 | Total Loss: 6.634523868560791 | KNN Loss: 5.613897323608398 | BCE Loss: 1.0206265449523926\n",
      "Epoch 281 / 500 | iteration 20 / 30 | Total Loss: 6.666292667388916 | KNN Loss: 5.606198787689209 | BCE Loss: 1.060093879699707\n",
      "Epoch 281 / 500 | iteration 25 / 30 | Total Loss: 6.660487174987793 | KNN Loss: 5.606830596923828 | BCE Loss: 1.0536563396453857\n",
      "Epoch   282: reducing learning rate of group 0 to 8.1421e-06.\n",
      "Epoch 282 / 500 | iteration 0 / 30 | Total Loss: 6.683711051940918 | KNN Loss: 5.6545186042785645 | BCE Loss: 1.0291922092437744\n",
      "Epoch 282 / 500 | iteration 5 / 30 | Total Loss: 6.6523637771606445 | KNN Loss: 5.615711212158203 | BCE Loss: 1.0366525650024414\n",
      "Epoch 282 / 500 | iteration 10 / 30 | Total Loss: 6.7740068435668945 | KNN Loss: 5.7315754890441895 | BCE Loss: 1.042431116104126\n",
      "Epoch 282 / 500 | iteration 15 / 30 | Total Loss: 6.732887268066406 | KNN Loss: 5.690482139587402 | BCE Loss: 1.042405366897583\n",
      "Epoch 282 / 500 | iteration 20 / 30 | Total Loss: 6.709023475646973 | KNN Loss: 5.651455402374268 | BCE Loss: 1.0575679540634155\n",
      "Epoch 282 / 500 | iteration 25 / 30 | Total Loss: 6.718576431274414 | KNN Loss: 5.636704444885254 | BCE Loss: 1.0818718671798706\n",
      "Epoch 283 / 500 | iteration 0 / 30 | Total Loss: 6.683585166931152 | KNN Loss: 5.6194000244140625 | BCE Loss: 1.0641849040985107\n",
      "Epoch 283 / 500 | iteration 5 / 30 | Total Loss: 6.686305999755859 | KNN Loss: 5.64277458190918 | BCE Loss: 1.0435311794281006\n",
      "Epoch 283 / 500 | iteration 10 / 30 | Total Loss: 6.6915483474731445 | KNN Loss: 5.658924579620361 | BCE Loss: 1.032623529434204\n",
      "Epoch 283 / 500 | iteration 15 / 30 | Total Loss: 6.748492240905762 | KNN Loss: 5.721386432647705 | BCE Loss: 1.0271059274673462\n",
      "Epoch 283 / 500 | iteration 20 / 30 | Total Loss: 6.698744773864746 | KNN Loss: 5.636622905731201 | BCE Loss: 1.062122106552124\n",
      "Epoch 283 / 500 | iteration 25 / 30 | Total Loss: 6.663802146911621 | KNN Loss: 5.609565734863281 | BCE Loss: 1.0542364120483398\n",
      "Epoch 284 / 500 | iteration 0 / 30 | Total Loss: 6.638404369354248 | KNN Loss: 5.595469951629639 | BCE Loss: 1.0429344177246094\n",
      "Epoch 284 / 500 | iteration 5 / 30 | Total Loss: 6.761176109313965 | KNN Loss: 5.707171440124512 | BCE Loss: 1.054004430770874\n",
      "Epoch 284 / 500 | iteration 10 / 30 | Total Loss: 6.691381454467773 | KNN Loss: 5.624600887298584 | BCE Loss: 1.0667808055877686\n",
      "Epoch 284 / 500 | iteration 15 / 30 | Total Loss: 6.7338714599609375 | KNN Loss: 5.676924705505371 | BCE Loss: 1.0569469928741455\n",
      "Epoch 284 / 500 | iteration 20 / 30 | Total Loss: 6.656726360321045 | KNN Loss: 5.600186824798584 | BCE Loss: 1.056539535522461\n",
      "Epoch 284 / 500 | iteration 25 / 30 | Total Loss: 6.644108772277832 | KNN Loss: 5.59486722946167 | BCE Loss: 1.049241304397583\n",
      "Epoch 285 / 500 | iteration 0 / 30 | Total Loss: 6.6044020652771 | KNN Loss: 5.59791898727417 | BCE Loss: 1.0064830780029297\n",
      "Epoch 285 / 500 | iteration 5 / 30 | Total Loss: 6.686399936676025 | KNN Loss: 5.631379127502441 | BCE Loss: 1.055020809173584\n",
      "Epoch 285 / 500 | iteration 10 / 30 | Total Loss: 6.670652389526367 | KNN Loss: 5.601275444030762 | BCE Loss: 1.069376826286316\n",
      "Epoch 285 / 500 | iteration 15 / 30 | Total Loss: 6.672476768493652 | KNN Loss: 5.630068778991699 | BCE Loss: 1.042407751083374\n",
      "Epoch 285 / 500 | iteration 20 / 30 | Total Loss: 6.666633605957031 | KNN Loss: 5.62666130065918 | BCE Loss: 1.0399724245071411\n",
      "Epoch 285 / 500 | iteration 25 / 30 | Total Loss: 6.701954364776611 | KNN Loss: 5.627295017242432 | BCE Loss: 1.0746593475341797\n",
      "Epoch 286 / 500 | iteration 0 / 30 | Total Loss: 6.655674934387207 | KNN Loss: 5.634957790374756 | BCE Loss: 1.0207173824310303\n",
      "Epoch 286 / 500 | iteration 5 / 30 | Total Loss: 6.657609462738037 | KNN Loss: 5.604888439178467 | BCE Loss: 1.0527210235595703\n",
      "Epoch 286 / 500 | iteration 10 / 30 | Total Loss: 6.663246154785156 | KNN Loss: 5.634620189666748 | BCE Loss: 1.028625726699829\n",
      "Epoch 286 / 500 | iteration 15 / 30 | Total Loss: 6.691723346710205 | KNN Loss: 5.676686763763428 | BCE Loss: 1.0150365829467773\n",
      "Epoch 286 / 500 | iteration 20 / 30 | Total Loss: 6.670227527618408 | KNN Loss: 5.597410678863525 | BCE Loss: 1.0728169679641724\n",
      "Epoch 286 / 500 | iteration 25 / 30 | Total Loss: 6.787370204925537 | KNN Loss: 5.758755207061768 | BCE Loss: 1.028615117073059\n",
      "Epoch 287 / 500 | iteration 0 / 30 | Total Loss: 6.787611484527588 | KNN Loss: 5.770031929016113 | BCE Loss: 1.0175795555114746\n",
      "Epoch 287 / 500 | iteration 5 / 30 | Total Loss: 6.732561111450195 | KNN Loss: 5.6341400146484375 | BCE Loss: 1.098421335220337\n",
      "Epoch 287 / 500 | iteration 10 / 30 | Total Loss: 6.676698684692383 | KNN Loss: 5.635921955108643 | BCE Loss: 1.0407769680023193\n",
      "Epoch 287 / 500 | iteration 15 / 30 | Total Loss: 6.635490417480469 | KNN Loss: 5.591809272766113 | BCE Loss: 1.0436809062957764\n",
      "Epoch 287 / 500 | iteration 20 / 30 | Total Loss: 6.743247032165527 | KNN Loss: 5.689176559448242 | BCE Loss: 1.0540704727172852\n",
      "Epoch 287 / 500 | iteration 25 / 30 | Total Loss: 6.63165283203125 | KNN Loss: 5.6028828620910645 | BCE Loss: 1.0287697315216064\n",
      "Epoch 288 / 500 | iteration 0 / 30 | Total Loss: 6.704998016357422 | KNN Loss: 5.654382705688477 | BCE Loss: 1.0506153106689453\n",
      "Epoch 288 / 500 | iteration 5 / 30 | Total Loss: 6.647895336151123 | KNN Loss: 5.5984625816345215 | BCE Loss: 1.0494327545166016\n",
      "Epoch 288 / 500 | iteration 10 / 30 | Total Loss: 6.649752140045166 | KNN Loss: 5.596160411834717 | BCE Loss: 1.0535917282104492\n",
      "Epoch 288 / 500 | iteration 15 / 30 | Total Loss: 6.647103309631348 | KNN Loss: 5.613157749176025 | BCE Loss: 1.0339457988739014\n",
      "Epoch 288 / 500 | iteration 20 / 30 | Total Loss: 6.708004474639893 | KNN Loss: 5.6525421142578125 | BCE Loss: 1.0554622411727905\n",
      "Epoch 288 / 500 | iteration 25 / 30 | Total Loss: 6.66670036315918 | KNN Loss: 5.598450183868408 | BCE Loss: 1.0682504177093506\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 289 / 500 | iteration 0 / 30 | Total Loss: 6.693600177764893 | KNN Loss: 5.630528450012207 | BCE Loss: 1.063071608543396\n",
      "Epoch 289 / 500 | iteration 5 / 30 | Total Loss: 6.691220283508301 | KNN Loss: 5.634036064147949 | BCE Loss: 1.0571839809417725\n",
      "Epoch 289 / 500 | iteration 10 / 30 | Total Loss: 6.711946964263916 | KNN Loss: 5.664770126342773 | BCE Loss: 1.047176718711853\n",
      "Epoch 289 / 500 | iteration 15 / 30 | Total Loss: 6.671706199645996 | KNN Loss: 5.615844249725342 | BCE Loss: 1.0558621883392334\n",
      "Epoch 289 / 500 | iteration 20 / 30 | Total Loss: 6.6622633934021 | KNN Loss: 5.617457389831543 | BCE Loss: 1.044805884361267\n",
      "Epoch 289 / 500 | iteration 25 / 30 | Total Loss: 6.689056396484375 | KNN Loss: 5.633315563201904 | BCE Loss: 1.0557408332824707\n",
      "Epoch 290 / 500 | iteration 0 / 30 | Total Loss: 6.659138202667236 | KNN Loss: 5.609602928161621 | BCE Loss: 1.0495352745056152\n",
      "Epoch 290 / 500 | iteration 5 / 30 | Total Loss: 6.652096748352051 | KNN Loss: 5.59266471862793 | BCE Loss: 1.059431791305542\n",
      "Epoch 290 / 500 | iteration 10 / 30 | Total Loss: 6.668692588806152 | KNN Loss: 5.628993511199951 | BCE Loss: 1.039698839187622\n",
      "Epoch 290 / 500 | iteration 15 / 30 | Total Loss: 6.687466621398926 | KNN Loss: 5.64524507522583 | BCE Loss: 1.0422214269638062\n",
      "Epoch 290 / 500 | iteration 20 / 30 | Total Loss: 6.656000137329102 | KNN Loss: 5.60972261428833 | BCE Loss: 1.0462772846221924\n",
      "Epoch 290 / 500 | iteration 25 / 30 | Total Loss: 6.63870906829834 | KNN Loss: 5.609744071960449 | BCE Loss: 1.0289649963378906\n",
      "Epoch 291 / 500 | iteration 0 / 30 | Total Loss: 6.647591590881348 | KNN Loss: 5.5943522453308105 | BCE Loss: 1.053239345550537\n",
      "Epoch 291 / 500 | iteration 5 / 30 | Total Loss: 6.750557899475098 | KNN Loss: 5.67120885848999 | BCE Loss: 1.0793489217758179\n",
      "Epoch 291 / 500 | iteration 10 / 30 | Total Loss: 6.686291694641113 | KNN Loss: 5.644414901733398 | BCE Loss: 1.0418767929077148\n",
      "Epoch 291 / 500 | iteration 15 / 30 | Total Loss: 6.670724868774414 | KNN Loss: 5.606146335601807 | BCE Loss: 1.0645785331726074\n",
      "Epoch 291 / 500 | iteration 20 / 30 | Total Loss: 6.700345993041992 | KNN Loss: 5.653060436248779 | BCE Loss: 1.0472854375839233\n",
      "Epoch 291 / 500 | iteration 25 / 30 | Total Loss: 6.624444484710693 | KNN Loss: 5.591102600097656 | BCE Loss: 1.0333420038223267\n",
      "Epoch 292 / 500 | iteration 0 / 30 | Total Loss: 6.645923614501953 | KNN Loss: 5.608310699462891 | BCE Loss: 1.0376129150390625\n",
      "Epoch 292 / 500 | iteration 5 / 30 | Total Loss: 6.645129203796387 | KNN Loss: 5.592127799987793 | BCE Loss: 1.0530014038085938\n",
      "Epoch 292 / 500 | iteration 10 / 30 | Total Loss: 6.665752410888672 | KNN Loss: 5.605328559875488 | BCE Loss: 1.0604239702224731\n",
      "Epoch 292 / 500 | iteration 15 / 30 | Total Loss: 6.696569919586182 | KNN Loss: 5.651519775390625 | BCE Loss: 1.045050024986267\n",
      "Epoch 292 / 500 | iteration 20 / 30 | Total Loss: 6.652423858642578 | KNN Loss: 5.620281219482422 | BCE Loss: 1.0321424007415771\n",
      "Epoch 292 / 500 | iteration 25 / 30 | Total Loss: 6.636715888977051 | KNN Loss: 5.608391761779785 | BCE Loss: 1.0283238887786865\n",
      "Epoch   293: reducing learning rate of group 0 to 5.6994e-06.\n",
      "Epoch 293 / 500 | iteration 0 / 30 | Total Loss: 6.783384323120117 | KNN Loss: 5.703962326049805 | BCE Loss: 1.079421877861023\n",
      "Epoch 293 / 500 | iteration 5 / 30 | Total Loss: 6.6888017654418945 | KNN Loss: 5.631778240203857 | BCE Loss: 1.057023525238037\n",
      "Epoch 293 / 500 | iteration 10 / 30 | Total Loss: 6.693405628204346 | KNN Loss: 5.674777984619141 | BCE Loss: 1.0186275243759155\n",
      "Epoch 293 / 500 | iteration 15 / 30 | Total Loss: 6.6796674728393555 | KNN Loss: 5.644882678985596 | BCE Loss: 1.0347850322723389\n",
      "Epoch 293 / 500 | iteration 20 / 30 | Total Loss: 6.635899066925049 | KNN Loss: 5.614065647125244 | BCE Loss: 1.0218334197998047\n",
      "Epoch 293 / 500 | iteration 25 / 30 | Total Loss: 6.635148048400879 | KNN Loss: 5.602222442626953 | BCE Loss: 1.0329258441925049\n",
      "Epoch 294 / 500 | iteration 0 / 30 | Total Loss: 6.673440456390381 | KNN Loss: 5.6218414306640625 | BCE Loss: 1.051599144935608\n",
      "Epoch 294 / 500 | iteration 5 / 30 | Total Loss: 6.71966028213501 | KNN Loss: 5.665719985961914 | BCE Loss: 1.0539402961730957\n",
      "Epoch 294 / 500 | iteration 10 / 30 | Total Loss: 6.689404010772705 | KNN Loss: 5.6600189208984375 | BCE Loss: 1.0293852090835571\n",
      "Epoch 294 / 500 | iteration 15 / 30 | Total Loss: 6.73470401763916 | KNN Loss: 5.676289081573486 | BCE Loss: 1.058415174484253\n",
      "Epoch 294 / 500 | iteration 20 / 30 | Total Loss: 6.642141819000244 | KNN Loss: 5.599937438964844 | BCE Loss: 1.0422043800354004\n",
      "Epoch 294 / 500 | iteration 25 / 30 | Total Loss: 6.675309181213379 | KNN Loss: 5.6475324630737305 | BCE Loss: 1.0277767181396484\n",
      "Epoch 295 / 500 | iteration 0 / 30 | Total Loss: 6.704575061798096 | KNN Loss: 5.635618686676025 | BCE Loss: 1.0689562559127808\n",
      "Epoch 295 / 500 | iteration 5 / 30 | Total Loss: 6.610013008117676 | KNN Loss: 5.605236530303955 | BCE Loss: 1.0047762393951416\n",
      "Epoch 295 / 500 | iteration 10 / 30 | Total Loss: 6.672516822814941 | KNN Loss: 5.6333231925964355 | BCE Loss: 1.039193868637085\n",
      "Epoch 295 / 500 | iteration 15 / 30 | Total Loss: 6.719053268432617 | KNN Loss: 5.671583652496338 | BCE Loss: 1.0474698543548584\n",
      "Epoch 295 / 500 | iteration 20 / 30 | Total Loss: 6.725642681121826 | KNN Loss: 5.674816608428955 | BCE Loss: 1.0508259534835815\n",
      "Epoch 295 / 500 | iteration 25 / 30 | Total Loss: 6.6546711921691895 | KNN Loss: 5.607046127319336 | BCE Loss: 1.0476250648498535\n",
      "Epoch 296 / 500 | iteration 0 / 30 | Total Loss: 6.665339469909668 | KNN Loss: 5.615383625030518 | BCE Loss: 1.0499558448791504\n",
      "Epoch 296 / 500 | iteration 5 / 30 | Total Loss: 6.6618499755859375 | KNN Loss: 5.596879482269287 | BCE Loss: 1.0649707317352295\n",
      "Epoch 296 / 500 | iteration 10 / 30 | Total Loss: 6.655839443206787 | KNN Loss: 5.6010236740112305 | BCE Loss: 1.0548157691955566\n",
      "Epoch 296 / 500 | iteration 15 / 30 | Total Loss: 6.684652805328369 | KNN Loss: 5.606875896453857 | BCE Loss: 1.0777769088745117\n",
      "Epoch 296 / 500 | iteration 20 / 30 | Total Loss: 6.661637306213379 | KNN Loss: 5.6439924240112305 | BCE Loss: 1.0176451206207275\n",
      "Epoch 296 / 500 | iteration 25 / 30 | Total Loss: 6.633787155151367 | KNN Loss: 5.603687763214111 | BCE Loss: 1.0300991535186768\n",
      "Epoch 297 / 500 | iteration 0 / 30 | Total Loss: 6.66598653793335 | KNN Loss: 5.593420028686523 | BCE Loss: 1.0725665092468262\n",
      "Epoch 297 / 500 | iteration 5 / 30 | Total Loss: 6.67721700668335 | KNN Loss: 5.637646198272705 | BCE Loss: 1.039570927619934\n",
      "Epoch 297 / 500 | iteration 10 / 30 | Total Loss: 6.70734977722168 | KNN Loss: 5.640692234039307 | BCE Loss: 1.066657304763794\n",
      "Epoch 297 / 500 | iteration 15 / 30 | Total Loss: 6.709902763366699 | KNN Loss: 5.647815227508545 | BCE Loss: 1.0620877742767334\n",
      "Epoch 297 / 500 | iteration 20 / 30 | Total Loss: 6.623734474182129 | KNN Loss: 5.6069488525390625 | BCE Loss: 1.0167858600616455\n",
      "Epoch 297 / 500 | iteration 25 / 30 | Total Loss: 6.670685768127441 | KNN Loss: 5.6264801025390625 | BCE Loss: 1.044205904006958\n",
      "Epoch 298 / 500 | iteration 0 / 30 | Total Loss: 6.7564873695373535 | KNN Loss: 5.715689659118652 | BCE Loss: 1.0407977104187012\n",
      "Epoch 298 / 500 | iteration 5 / 30 | Total Loss: 6.735172748565674 | KNN Loss: 5.6796135902404785 | BCE Loss: 1.0555592775344849\n",
      "Epoch 298 / 500 | iteration 10 / 30 | Total Loss: 6.7506513595581055 | KNN Loss: 5.72796106338501 | BCE Loss: 1.0226902961730957\n",
      "Epoch 298 / 500 | iteration 15 / 30 | Total Loss: 6.666027069091797 | KNN Loss: 5.633950233459473 | BCE Loss: 1.0320770740509033\n",
      "Epoch 298 / 500 | iteration 20 / 30 | Total Loss: 6.77915096282959 | KNN Loss: 5.704277515411377 | BCE Loss: 1.0748733282089233\n",
      "Epoch 298 / 500 | iteration 25 / 30 | Total Loss: 6.759349822998047 | KNN Loss: 5.697719573974609 | BCE Loss: 1.0616304874420166\n",
      "Epoch 299 / 500 | iteration 0 / 30 | Total Loss: 6.618253231048584 | KNN Loss: 5.593639850616455 | BCE Loss: 1.024613380432129\n",
      "Epoch 299 / 500 | iteration 5 / 30 | Total Loss: 6.67803955078125 | KNN Loss: 5.634161949157715 | BCE Loss: 1.0438777208328247\n",
      "Epoch 299 / 500 | iteration 10 / 30 | Total Loss: 6.729004383087158 | KNN Loss: 5.653300762176514 | BCE Loss: 1.0757036209106445\n",
      "Epoch 299 / 500 | iteration 15 / 30 | Total Loss: 6.661844253540039 | KNN Loss: 5.601158618927002 | BCE Loss: 1.060685396194458\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 299 / 500 | iteration 20 / 30 | Total Loss: 6.613533020019531 | KNN Loss: 5.592226028442383 | BCE Loss: 1.0213069915771484\n",
      "Epoch 299 / 500 | iteration 25 / 30 | Total Loss: 6.691777229309082 | KNN Loss: 5.679767608642578 | BCE Loss: 1.012009859085083\n",
      "Epoch 300 / 500 | iteration 0 / 30 | Total Loss: 6.648251533508301 | KNN Loss: 5.597432613372803 | BCE Loss: 1.0508191585540771\n",
      "Epoch 300 / 500 | iteration 5 / 30 | Total Loss: 6.65736198425293 | KNN Loss: 5.602949142456055 | BCE Loss: 1.054412841796875\n",
      "Epoch 300 / 500 | iteration 10 / 30 | Total Loss: 6.666622638702393 | KNN Loss: 5.631566524505615 | BCE Loss: 1.035056233406067\n",
      "Epoch 300 / 500 | iteration 15 / 30 | Total Loss: 6.679257869720459 | KNN Loss: 5.633421421051025 | BCE Loss: 1.0458364486694336\n",
      "Epoch 300 / 500 | iteration 20 / 30 | Total Loss: 6.6732048988342285 | KNN Loss: 5.608818531036377 | BCE Loss: 1.0643863677978516\n",
      "Epoch 300 / 500 | iteration 25 / 30 | Total Loss: 6.686565399169922 | KNN Loss: 5.632523059844971 | BCE Loss: 1.0540424585342407\n",
      "Epoch 301 / 500 | iteration 0 / 30 | Total Loss: 6.6978759765625 | KNN Loss: 5.638589382171631 | BCE Loss: 1.0592868328094482\n",
      "Epoch 301 / 500 | iteration 5 / 30 | Total Loss: 6.666857719421387 | KNN Loss: 5.613185405731201 | BCE Loss: 1.0536720752716064\n",
      "Epoch 301 / 500 | iteration 10 / 30 | Total Loss: 6.679974555969238 | KNN Loss: 5.605318069458008 | BCE Loss: 1.0746562480926514\n",
      "Epoch 301 / 500 | iteration 15 / 30 | Total Loss: 6.731040954589844 | KNN Loss: 5.66414737701416 | BCE Loss: 1.0668938159942627\n",
      "Epoch 301 / 500 | iteration 20 / 30 | Total Loss: 6.673455238342285 | KNN Loss: 5.604551792144775 | BCE Loss: 1.0689032077789307\n",
      "Epoch 301 / 500 | iteration 25 / 30 | Total Loss: 6.671454429626465 | KNN Loss: 5.6206231117248535 | BCE Loss: 1.0508311986923218\n",
      "Epoch 302 / 500 | iteration 0 / 30 | Total Loss: 6.72150182723999 | KNN Loss: 5.672004699707031 | BCE Loss: 1.049497127532959\n",
      "Epoch 302 / 500 | iteration 5 / 30 | Total Loss: 6.671262741088867 | KNN Loss: 5.615787506103516 | BCE Loss: 1.0554754734039307\n",
      "Epoch 302 / 500 | iteration 10 / 30 | Total Loss: 6.66657829284668 | KNN Loss: 5.615593433380127 | BCE Loss: 1.0509850978851318\n",
      "Epoch 302 / 500 | iteration 15 / 30 | Total Loss: 6.648759841918945 | KNN Loss: 5.640578269958496 | BCE Loss: 1.0081818103790283\n",
      "Epoch 302 / 500 | iteration 20 / 30 | Total Loss: 6.685136795043945 | KNN Loss: 5.618946552276611 | BCE Loss: 1.0661901235580444\n",
      "Epoch 302 / 500 | iteration 25 / 30 | Total Loss: 6.66450309753418 | KNN Loss: 5.608129501342773 | BCE Loss: 1.0563735961914062\n",
      "Epoch 303 / 500 | iteration 0 / 30 | Total Loss: 6.6824822425842285 | KNN Loss: 5.628355503082275 | BCE Loss: 1.0541268587112427\n",
      "Epoch 303 / 500 | iteration 5 / 30 | Total Loss: 6.643131256103516 | KNN Loss: 5.592655658721924 | BCE Loss: 1.0504757165908813\n",
      "Epoch 303 / 500 | iteration 10 / 30 | Total Loss: 6.698960304260254 | KNN Loss: 5.631226539611816 | BCE Loss: 1.0677337646484375\n",
      "Epoch 303 / 500 | iteration 15 / 30 | Total Loss: 6.658210754394531 | KNN Loss: 5.624965667724609 | BCE Loss: 1.0332449674606323\n",
      "Epoch 303 / 500 | iteration 20 / 30 | Total Loss: 6.6546125411987305 | KNN Loss: 5.60256290435791 | BCE Loss: 1.0520496368408203\n",
      "Epoch 303 / 500 | iteration 25 / 30 | Total Loss: 6.744805812835693 | KNN Loss: 5.6883344650268555 | BCE Loss: 1.056471347808838\n",
      "Epoch   304: reducing learning rate of group 0 to 3.9896e-06.\n",
      "Epoch 304 / 500 | iteration 0 / 30 | Total Loss: 6.739062309265137 | KNN Loss: 5.716638565063477 | BCE Loss: 1.0224238634109497\n",
      "Epoch 304 / 500 | iteration 5 / 30 | Total Loss: 6.7090983390808105 | KNN Loss: 5.680019378662109 | BCE Loss: 1.0290788412094116\n",
      "Epoch 304 / 500 | iteration 10 / 30 | Total Loss: 6.6556620597839355 | KNN Loss: 5.614911079406738 | BCE Loss: 1.0407509803771973\n",
      "Epoch 304 / 500 | iteration 15 / 30 | Total Loss: 6.652027130126953 | KNN Loss: 5.593709468841553 | BCE Loss: 1.05831778049469\n",
      "Epoch 304 / 500 | iteration 20 / 30 | Total Loss: 6.686266899108887 | KNN Loss: 5.599912643432617 | BCE Loss: 1.0863544940948486\n",
      "Epoch 304 / 500 | iteration 25 / 30 | Total Loss: 6.658319473266602 | KNN Loss: 5.602421760559082 | BCE Loss: 1.055897831916809\n",
      "Epoch 305 / 500 | iteration 0 / 30 | Total Loss: 6.682723045349121 | KNN Loss: 5.658279895782471 | BCE Loss: 1.0244429111480713\n",
      "Epoch 305 / 500 | iteration 5 / 30 | Total Loss: 6.66281270980835 | KNN Loss: 5.626141548156738 | BCE Loss: 1.0366712808609009\n",
      "Epoch 305 / 500 | iteration 10 / 30 | Total Loss: 6.635410308837891 | KNN Loss: 5.61663818359375 | BCE Loss: 1.0187718868255615\n",
      "Epoch 305 / 500 | iteration 15 / 30 | Total Loss: 6.671921730041504 | KNN Loss: 5.6201090812683105 | BCE Loss: 1.0518125295639038\n",
      "Epoch 305 / 500 | iteration 20 / 30 | Total Loss: 6.744016170501709 | KNN Loss: 5.680584907531738 | BCE Loss: 1.0634312629699707\n",
      "Epoch 305 / 500 | iteration 25 / 30 | Total Loss: 6.7369818687438965 | KNN Loss: 5.6833720207214355 | BCE Loss: 1.053609848022461\n",
      "Epoch 306 / 500 | iteration 0 / 30 | Total Loss: 6.715415954589844 | KNN Loss: 5.692595481872559 | BCE Loss: 1.022820234298706\n",
      "Epoch 306 / 500 | iteration 5 / 30 | Total Loss: 6.645785331726074 | KNN Loss: 5.6034674644470215 | BCE Loss: 1.0423181056976318\n",
      "Epoch 306 / 500 | iteration 10 / 30 | Total Loss: 6.641140937805176 | KNN Loss: 5.6113810539245605 | BCE Loss: 1.0297601222991943\n",
      "Epoch 306 / 500 | iteration 15 / 30 | Total Loss: 6.6554951667785645 | KNN Loss: 5.602879047393799 | BCE Loss: 1.052616000175476\n",
      "Epoch 306 / 500 | iteration 20 / 30 | Total Loss: 6.665952682495117 | KNN Loss: 5.602416038513184 | BCE Loss: 1.0635364055633545\n",
      "Epoch 306 / 500 | iteration 25 / 30 | Total Loss: 6.707527160644531 | KNN Loss: 5.637511253356934 | BCE Loss: 1.0700156688690186\n",
      "Epoch 307 / 500 | iteration 0 / 30 | Total Loss: 6.668548583984375 | KNN Loss: 5.646903038024902 | BCE Loss: 1.0216453075408936\n",
      "Epoch 307 / 500 | iteration 5 / 30 | Total Loss: 6.750787258148193 | KNN Loss: 5.704841613769531 | BCE Loss: 1.0459455251693726\n",
      "Epoch 307 / 500 | iteration 10 / 30 | Total Loss: 6.645756721496582 | KNN Loss: 5.598458290100098 | BCE Loss: 1.0472981929779053\n",
      "Epoch 307 / 500 | iteration 15 / 30 | Total Loss: 6.697115898132324 | KNN Loss: 5.613279342651367 | BCE Loss: 1.0838364362716675\n",
      "Epoch 307 / 500 | iteration 20 / 30 | Total Loss: 6.652307033538818 | KNN Loss: 5.621730327606201 | BCE Loss: 1.0305767059326172\n",
      "Epoch 307 / 500 | iteration 25 / 30 | Total Loss: 6.688860893249512 | KNN Loss: 5.597297191619873 | BCE Loss: 1.0915634632110596\n",
      "Epoch 308 / 500 | iteration 0 / 30 | Total Loss: 6.738702774047852 | KNN Loss: 5.715268135070801 | BCE Loss: 1.0234348773956299\n",
      "Epoch 308 / 500 | iteration 5 / 30 | Total Loss: 6.645089149475098 | KNN Loss: 5.613349914550781 | BCE Loss: 1.0317394733428955\n",
      "Epoch 308 / 500 | iteration 10 / 30 | Total Loss: 6.665127754211426 | KNN Loss: 5.615932464599609 | BCE Loss: 1.0491950511932373\n",
      "Epoch 308 / 500 | iteration 15 / 30 | Total Loss: 6.716814041137695 | KNN Loss: 5.669407844543457 | BCE Loss: 1.0474061965942383\n",
      "Epoch 308 / 500 | iteration 20 / 30 | Total Loss: 6.7107625007629395 | KNN Loss: 5.6509881019592285 | BCE Loss: 1.0597742795944214\n",
      "Epoch 308 / 500 | iteration 25 / 30 | Total Loss: 6.726543426513672 | KNN Loss: 5.658491134643555 | BCE Loss: 1.068052053451538\n",
      "Epoch 309 / 500 | iteration 0 / 30 | Total Loss: 6.682682037353516 | KNN Loss: 5.612199783325195 | BCE Loss: 1.0704820156097412\n",
      "Epoch 309 / 500 | iteration 5 / 30 | Total Loss: 6.683324813842773 | KNN Loss: 5.6020636558532715 | BCE Loss: 1.081261157989502\n",
      "Epoch 309 / 500 | iteration 10 / 30 | Total Loss: 6.682445526123047 | KNN Loss: 5.619740009307861 | BCE Loss: 1.0627055168151855\n",
      "Epoch 309 / 500 | iteration 15 / 30 | Total Loss: 6.679965972900391 | KNN Loss: 5.6192755699157715 | BCE Loss: 1.0606906414031982\n",
      "Epoch 309 / 500 | iteration 20 / 30 | Total Loss: 6.670097827911377 | KNN Loss: 5.62754487991333 | BCE Loss: 1.0425529479980469\n",
      "Epoch 309 / 500 | iteration 25 / 30 | Total Loss: 6.686799049377441 | KNN Loss: 5.635137557983398 | BCE Loss: 1.0516616106033325\n",
      "Epoch 310 / 500 | iteration 0 / 30 | Total Loss: 6.638148307800293 | KNN Loss: 5.5932207107543945 | BCE Loss: 1.044927716255188\n",
      "Epoch 310 / 500 | iteration 5 / 30 | Total Loss: 6.666685104370117 | KNN Loss: 5.611176013946533 | BCE Loss: 1.055509090423584\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 310 / 500 | iteration 10 / 30 | Total Loss: 6.648674964904785 | KNN Loss: 5.6248626708984375 | BCE Loss: 1.023812174797058\n",
      "Epoch 310 / 500 | iteration 15 / 30 | Total Loss: 6.650419235229492 | KNN Loss: 5.613167762756348 | BCE Loss: 1.0372512340545654\n",
      "Epoch 310 / 500 | iteration 20 / 30 | Total Loss: 6.65997314453125 | KNN Loss: 5.597779273986816 | BCE Loss: 1.0621936321258545\n",
      "Epoch 310 / 500 | iteration 25 / 30 | Total Loss: 6.744662761688232 | KNN Loss: 5.690520763397217 | BCE Loss: 1.0541419982910156\n",
      "Epoch 311 / 500 | iteration 0 / 30 | Total Loss: 6.638860702514648 | KNN Loss: 5.593068599700928 | BCE Loss: 1.0457919836044312\n",
      "Epoch 311 / 500 | iteration 5 / 30 | Total Loss: 6.660667419433594 | KNN Loss: 5.603341102600098 | BCE Loss: 1.057326316833496\n",
      "Epoch 311 / 500 | iteration 10 / 30 | Total Loss: 6.643486022949219 | KNN Loss: 5.606847763061523 | BCE Loss: 1.0366381406784058\n",
      "Epoch 311 / 500 | iteration 15 / 30 | Total Loss: 6.693835258483887 | KNN Loss: 5.638180255889893 | BCE Loss: 1.0556551218032837\n",
      "Epoch 311 / 500 | iteration 20 / 30 | Total Loss: 6.644608497619629 | KNN Loss: 5.600149631500244 | BCE Loss: 1.0444591045379639\n",
      "Epoch 311 / 500 | iteration 25 / 30 | Total Loss: 6.73684024810791 | KNN Loss: 5.6848530769348145 | BCE Loss: 1.0519872903823853\n",
      "Epoch 312 / 500 | iteration 0 / 30 | Total Loss: 6.663234710693359 | KNN Loss: 5.606814384460449 | BCE Loss: 1.0564205646514893\n",
      "Epoch 312 / 500 | iteration 5 / 30 | Total Loss: 6.692930221557617 | KNN Loss: 5.595569610595703 | BCE Loss: 1.0973608493804932\n",
      "Epoch 312 / 500 | iteration 10 / 30 | Total Loss: 6.652531623840332 | KNN Loss: 5.6158881187438965 | BCE Loss: 1.036643385887146\n",
      "Epoch 312 / 500 | iteration 15 / 30 | Total Loss: 6.6504082679748535 | KNN Loss: 5.649184703826904 | BCE Loss: 1.0012234449386597\n",
      "Epoch 312 / 500 | iteration 20 / 30 | Total Loss: 6.644132614135742 | KNN Loss: 5.597475051879883 | BCE Loss: 1.0466573238372803\n",
      "Epoch 312 / 500 | iteration 25 / 30 | Total Loss: 6.6906232833862305 | KNN Loss: 5.62144660949707 | BCE Loss: 1.0691767930984497\n",
      "Epoch 313 / 500 | iteration 0 / 30 | Total Loss: 6.713656902313232 | KNN Loss: 5.654269695281982 | BCE Loss: 1.05938720703125\n",
      "Epoch 313 / 500 | iteration 5 / 30 | Total Loss: 6.713028430938721 | KNN Loss: 5.65224552154541 | BCE Loss: 1.0607829093933105\n",
      "Epoch 313 / 500 | iteration 10 / 30 | Total Loss: 6.678442001342773 | KNN Loss: 5.635131359100342 | BCE Loss: 1.0433104038238525\n",
      "Epoch 313 / 500 | iteration 15 / 30 | Total Loss: 6.698774337768555 | KNN Loss: 5.653223037719727 | BCE Loss: 1.0455515384674072\n",
      "Epoch 313 / 500 | iteration 20 / 30 | Total Loss: 6.7026495933532715 | KNN Loss: 5.64541482925415 | BCE Loss: 1.0572348833084106\n",
      "Epoch 313 / 500 | iteration 25 / 30 | Total Loss: 6.62892484664917 | KNN Loss: 5.592898845672607 | BCE Loss: 1.0360260009765625\n",
      "Epoch 314 / 500 | iteration 0 / 30 | Total Loss: 6.650801658630371 | KNN Loss: 5.612136363983154 | BCE Loss: 1.0386654138565063\n",
      "Epoch 314 / 500 | iteration 5 / 30 | Total Loss: 6.6218156814575195 | KNN Loss: 5.611242771148682 | BCE Loss: 1.0105730295181274\n",
      "Epoch 314 / 500 | iteration 10 / 30 | Total Loss: 6.609336853027344 | KNN Loss: 5.596470355987549 | BCE Loss: 1.012866735458374\n",
      "Epoch 314 / 500 | iteration 15 / 30 | Total Loss: 6.692113876342773 | KNN Loss: 5.638347148895264 | BCE Loss: 1.0537664890289307\n",
      "Epoch 314 / 500 | iteration 20 / 30 | Total Loss: 6.63265323638916 | KNN Loss: 5.599269866943359 | BCE Loss: 1.0333833694458008\n",
      "Epoch 314 / 500 | iteration 25 / 30 | Total Loss: 6.623964309692383 | KNN Loss: 5.592072486877441 | BCE Loss: 1.031891942024231\n",
      "Epoch   315: reducing learning rate of group 0 to 2.7927e-06.\n",
      "Epoch 315 / 500 | iteration 0 / 30 | Total Loss: 6.635648727416992 | KNN Loss: 5.593936443328857 | BCE Loss: 1.0417124032974243\n",
      "Epoch 315 / 500 | iteration 5 / 30 | Total Loss: 6.649184703826904 | KNN Loss: 5.593973636627197 | BCE Loss: 1.055211067199707\n",
      "Epoch 315 / 500 | iteration 10 / 30 | Total Loss: 6.662284851074219 | KNN Loss: 5.615333557128906 | BCE Loss: 1.046951413154602\n",
      "Epoch 315 / 500 | iteration 15 / 30 | Total Loss: 6.6585516929626465 | KNN Loss: 5.619870185852051 | BCE Loss: 1.0386815071105957\n",
      "Epoch 315 / 500 | iteration 20 / 30 | Total Loss: 6.732073783874512 | KNN Loss: 5.688567161560059 | BCE Loss: 1.0435068607330322\n",
      "Epoch 315 / 500 | iteration 25 / 30 | Total Loss: 6.62777853012085 | KNN Loss: 5.605016708374023 | BCE Loss: 1.0227618217468262\n",
      "Epoch 316 / 500 | iteration 0 / 30 | Total Loss: 6.656852722167969 | KNN Loss: 5.593080997467041 | BCE Loss: 1.0637719631195068\n",
      "Epoch 316 / 500 | iteration 5 / 30 | Total Loss: 6.715540885925293 | KNN Loss: 5.672531604766846 | BCE Loss: 1.0430095195770264\n",
      "Epoch 316 / 500 | iteration 10 / 30 | Total Loss: 6.685230731964111 | KNN Loss: 5.633167266845703 | BCE Loss: 1.0520634651184082\n",
      "Epoch 316 / 500 | iteration 15 / 30 | Total Loss: 6.665889739990234 | KNN Loss: 5.609326362609863 | BCE Loss: 1.056563138961792\n",
      "Epoch 316 / 500 | iteration 20 / 30 | Total Loss: 6.650491237640381 | KNN Loss: 5.610753059387207 | BCE Loss: 1.0397381782531738\n",
      "Epoch 316 / 500 | iteration 25 / 30 | Total Loss: 6.657135009765625 | KNN Loss: 5.592202663421631 | BCE Loss: 1.0649323463439941\n",
      "Epoch 317 / 500 | iteration 0 / 30 | Total Loss: 6.66831636428833 | KNN Loss: 5.603950500488281 | BCE Loss: 1.0643658638000488\n",
      "Epoch 317 / 500 | iteration 5 / 30 | Total Loss: 6.713324546813965 | KNN Loss: 5.6422014236450195 | BCE Loss: 1.0711228847503662\n",
      "Epoch 317 / 500 | iteration 10 / 30 | Total Loss: 6.706113338470459 | KNN Loss: 5.631638050079346 | BCE Loss: 1.0744754076004028\n",
      "Epoch 317 / 500 | iteration 15 / 30 | Total Loss: 6.725976943969727 | KNN Loss: 5.685383319854736 | BCE Loss: 1.0405933856964111\n",
      "Epoch 317 / 500 | iteration 20 / 30 | Total Loss: 6.688382148742676 | KNN Loss: 5.648225784301758 | BCE Loss: 1.0401561260223389\n",
      "Epoch 317 / 500 | iteration 25 / 30 | Total Loss: 6.646893501281738 | KNN Loss: 5.604145526885986 | BCE Loss: 1.042747974395752\n",
      "Epoch 318 / 500 | iteration 0 / 30 | Total Loss: 6.648601531982422 | KNN Loss: 5.62065315246582 | BCE Loss: 1.0279486179351807\n",
      "Epoch 318 / 500 | iteration 5 / 30 | Total Loss: 6.696388244628906 | KNN Loss: 5.621732711791992 | BCE Loss: 1.0746557712554932\n",
      "Epoch 318 / 500 | iteration 10 / 30 | Total Loss: 6.622448921203613 | KNN Loss: 5.59005880355835 | BCE Loss: 1.0323901176452637\n",
      "Epoch 318 / 500 | iteration 15 / 30 | Total Loss: 6.667037010192871 | KNN Loss: 5.628370761871338 | BCE Loss: 1.038666009902954\n",
      "Epoch 318 / 500 | iteration 20 / 30 | Total Loss: 6.72930908203125 | KNN Loss: 5.664209842681885 | BCE Loss: 1.0650994777679443\n",
      "Epoch 318 / 500 | iteration 25 / 30 | Total Loss: 6.647027492523193 | KNN Loss: 5.626339435577393 | BCE Loss: 1.0206880569458008\n",
      "Epoch 319 / 500 | iteration 0 / 30 | Total Loss: 6.662880897521973 | KNN Loss: 5.612067699432373 | BCE Loss: 1.0508129596710205\n",
      "Epoch 319 / 500 | iteration 5 / 30 | Total Loss: 6.7027506828308105 | KNN Loss: 5.6870198249816895 | BCE Loss: 1.015730857849121\n",
      "Epoch 319 / 500 | iteration 10 / 30 | Total Loss: 6.733495712280273 | KNN Loss: 5.659402370452881 | BCE Loss: 1.0740935802459717\n",
      "Epoch 319 / 500 | iteration 15 / 30 | Total Loss: 6.637028694152832 | KNN Loss: 5.593345642089844 | BCE Loss: 1.0436830520629883\n",
      "Epoch 319 / 500 | iteration 20 / 30 | Total Loss: 6.645312309265137 | KNN Loss: 5.6268696784973145 | BCE Loss: 1.0184425115585327\n",
      "Epoch 319 / 500 | iteration 25 / 30 | Total Loss: 6.726515293121338 | KNN Loss: 5.649528503417969 | BCE Loss: 1.0769866704940796\n",
      "Epoch 320 / 500 | iteration 0 / 30 | Total Loss: 6.863018035888672 | KNN Loss: 5.81200647354126 | BCE Loss: 1.051011562347412\n",
      "Epoch 320 / 500 | iteration 5 / 30 | Total Loss: 6.704354286193848 | KNN Loss: 5.659513473510742 | BCE Loss: 1.044840693473816\n",
      "Epoch 320 / 500 | iteration 10 / 30 | Total Loss: 6.718240737915039 | KNN Loss: 5.6401262283325195 | BCE Loss: 1.0781147480010986\n",
      "Epoch 320 / 500 | iteration 15 / 30 | Total Loss: 6.675572872161865 | KNN Loss: 5.603755474090576 | BCE Loss: 1.0718172788619995\n",
      "Epoch 320 / 500 | iteration 20 / 30 | Total Loss: 6.72684383392334 | KNN Loss: 5.663658142089844 | BCE Loss: 1.063185691833496\n",
      "Epoch 320 / 500 | iteration 25 / 30 | Total Loss: 6.632549285888672 | KNN Loss: 5.602787017822266 | BCE Loss: 1.0297621488571167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 321 / 500 | iteration 0 / 30 | Total Loss: 6.6820268630981445 | KNN Loss: 5.620161056518555 | BCE Loss: 1.0618658065795898\n",
      "Epoch 321 / 500 | iteration 5 / 30 | Total Loss: 6.637747764587402 | KNN Loss: 5.607336521148682 | BCE Loss: 1.0304112434387207\n",
      "Epoch 321 / 500 | iteration 10 / 30 | Total Loss: 6.674618721008301 | KNN Loss: 5.632928848266602 | BCE Loss: 1.0416898727416992\n",
      "Epoch 321 / 500 | iteration 15 / 30 | Total Loss: 6.654331684112549 | KNN Loss: 5.59250020980835 | BCE Loss: 1.0618314743041992\n",
      "Epoch 321 / 500 | iteration 20 / 30 | Total Loss: 6.641572952270508 | KNN Loss: 5.595075607299805 | BCE Loss: 1.0464974641799927\n",
      "Epoch 321 / 500 | iteration 25 / 30 | Total Loss: 6.684658050537109 | KNN Loss: 5.6262006759643555 | BCE Loss: 1.058457374572754\n",
      "Epoch 322 / 500 | iteration 0 / 30 | Total Loss: 6.677923679351807 | KNN Loss: 5.6006622314453125 | BCE Loss: 1.0772615671157837\n",
      "Epoch 322 / 500 | iteration 5 / 30 | Total Loss: 6.7279953956604 | KNN Loss: 5.667718410491943 | BCE Loss: 1.060276985168457\n",
      "Epoch 322 / 500 | iteration 10 / 30 | Total Loss: 6.66750431060791 | KNN Loss: 5.621527671813965 | BCE Loss: 1.0459764003753662\n",
      "Epoch 322 / 500 | iteration 15 / 30 | Total Loss: 6.67705774307251 | KNN Loss: 5.622434616088867 | BCE Loss: 1.0546231269836426\n",
      "Epoch 322 / 500 | iteration 20 / 30 | Total Loss: 6.63370943069458 | KNN Loss: 5.596291542053223 | BCE Loss: 1.0374178886413574\n",
      "Epoch 322 / 500 | iteration 25 / 30 | Total Loss: 6.728458404541016 | KNN Loss: 5.64579439163208 | BCE Loss: 1.0826637744903564\n",
      "Epoch 323 / 500 | iteration 0 / 30 | Total Loss: 6.641507148742676 | KNN Loss: 5.615444660186768 | BCE Loss: 1.026062250137329\n",
      "Epoch 323 / 500 | iteration 5 / 30 | Total Loss: 6.721563816070557 | KNN Loss: 5.662902355194092 | BCE Loss: 1.0586614608764648\n",
      "Epoch 323 / 500 | iteration 10 / 30 | Total Loss: 6.702691555023193 | KNN Loss: 5.666098117828369 | BCE Loss: 1.0365934371948242\n",
      "Epoch 323 / 500 | iteration 15 / 30 | Total Loss: 6.649177551269531 | KNN Loss: 5.596192359924316 | BCE Loss: 1.052985429763794\n",
      "Epoch 323 / 500 | iteration 20 / 30 | Total Loss: 6.69370174407959 | KNN Loss: 5.634597301483154 | BCE Loss: 1.0591042041778564\n",
      "Epoch 323 / 500 | iteration 25 / 30 | Total Loss: 6.670492649078369 | KNN Loss: 5.611686706542969 | BCE Loss: 1.0588058233261108\n",
      "Epoch 324 / 500 | iteration 0 / 30 | Total Loss: 6.69566011428833 | KNN Loss: 5.630903720855713 | BCE Loss: 1.0647563934326172\n",
      "Epoch 324 / 500 | iteration 5 / 30 | Total Loss: 6.693661689758301 | KNN Loss: 5.620415687561035 | BCE Loss: 1.0732462406158447\n",
      "Epoch 324 / 500 | iteration 10 / 30 | Total Loss: 6.665453910827637 | KNN Loss: 5.623741626739502 | BCE Loss: 1.0417125225067139\n",
      "Epoch 324 / 500 | iteration 15 / 30 | Total Loss: 6.645542144775391 | KNN Loss: 5.610698699951172 | BCE Loss: 1.0348435640335083\n",
      "Epoch 324 / 500 | iteration 20 / 30 | Total Loss: 6.707699775695801 | KNN Loss: 5.668930530548096 | BCE Loss: 1.0387691259384155\n",
      "Epoch 324 / 500 | iteration 25 / 30 | Total Loss: 6.737178802490234 | KNN Loss: 5.6840739250183105 | BCE Loss: 1.0531046390533447\n",
      "Epoch 325 / 500 | iteration 0 / 30 | Total Loss: 6.637392044067383 | KNN Loss: 5.592974662780762 | BCE Loss: 1.044417381286621\n",
      "Epoch 325 / 500 | iteration 5 / 30 | Total Loss: 6.7031049728393555 | KNN Loss: 5.610622406005859 | BCE Loss: 1.092482328414917\n",
      "Epoch 325 / 500 | iteration 10 / 30 | Total Loss: 6.6714935302734375 | KNN Loss: 5.603000164031982 | BCE Loss: 1.0684934854507446\n",
      "Epoch 325 / 500 | iteration 15 / 30 | Total Loss: 6.775394916534424 | KNN Loss: 5.712530612945557 | BCE Loss: 1.0628643035888672\n",
      "Epoch 325 / 500 | iteration 20 / 30 | Total Loss: 6.6558613777160645 | KNN Loss: 5.597521781921387 | BCE Loss: 1.0583395957946777\n",
      "Epoch 325 / 500 | iteration 25 / 30 | Total Loss: 6.669720649719238 | KNN Loss: 5.598321437835693 | BCE Loss: 1.0713990926742554\n",
      "Epoch   326: reducing learning rate of group 0 to 1.9549e-06.\n",
      "Epoch 326 / 500 | iteration 0 / 30 | Total Loss: 6.686857223510742 | KNN Loss: 5.666172981262207 | BCE Loss: 1.0206841230392456\n",
      "Epoch 326 / 500 | iteration 5 / 30 | Total Loss: 6.728031158447266 | KNN Loss: 5.6681013107299805 | BCE Loss: 1.0599300861358643\n",
      "Epoch 326 / 500 | iteration 10 / 30 | Total Loss: 6.713228225708008 | KNN Loss: 5.6549835205078125 | BCE Loss: 1.0582444667816162\n",
      "Epoch 326 / 500 | iteration 15 / 30 | Total Loss: 6.660299301147461 | KNN Loss: 5.6121826171875 | BCE Loss: 1.0481168031692505\n",
      "Epoch 326 / 500 | iteration 20 / 30 | Total Loss: 6.641037464141846 | KNN Loss: 5.61334753036499 | BCE Loss: 1.0276899337768555\n",
      "Epoch 326 / 500 | iteration 25 / 30 | Total Loss: 6.7129011154174805 | KNN Loss: 5.65604829788208 | BCE Loss: 1.05685293674469\n",
      "Epoch 327 / 500 | iteration 0 / 30 | Total Loss: 6.670461177825928 | KNN Loss: 5.612126350402832 | BCE Loss: 1.0583348274230957\n",
      "Epoch 327 / 500 | iteration 5 / 30 | Total Loss: 6.71192741394043 | KNN Loss: 5.653087139129639 | BCE Loss: 1.0588405132293701\n",
      "Epoch 327 / 500 | iteration 10 / 30 | Total Loss: 6.65898323059082 | KNN Loss: 5.624791145324707 | BCE Loss: 1.0341918468475342\n",
      "Epoch 327 / 500 | iteration 15 / 30 | Total Loss: 6.692339897155762 | KNN Loss: 5.639476776123047 | BCE Loss: 1.052863359451294\n",
      "Epoch 327 / 500 | iteration 20 / 30 | Total Loss: 6.6827850341796875 | KNN Loss: 5.623379230499268 | BCE Loss: 1.0594059228897095\n",
      "Epoch 327 / 500 | iteration 25 / 30 | Total Loss: 6.6378278732299805 | KNN Loss: 5.617149353027344 | BCE Loss: 1.0206785202026367\n",
      "Epoch 328 / 500 | iteration 0 / 30 | Total Loss: 6.647372245788574 | KNN Loss: 5.606101036071777 | BCE Loss: 1.041271448135376\n",
      "Epoch 328 / 500 | iteration 5 / 30 | Total Loss: 6.724743843078613 | KNN Loss: 5.665605545043945 | BCE Loss: 1.059138298034668\n",
      "Epoch 328 / 500 | iteration 10 / 30 | Total Loss: 6.668951034545898 | KNN Loss: 5.635680675506592 | BCE Loss: 1.0332703590393066\n",
      "Epoch 328 / 500 | iteration 15 / 30 | Total Loss: 6.65866756439209 | KNN Loss: 5.593504905700684 | BCE Loss: 1.0651628971099854\n",
      "Epoch 328 / 500 | iteration 20 / 30 | Total Loss: 6.661199569702148 | KNN Loss: 5.6124091148376465 | BCE Loss: 1.048790693283081\n",
      "Epoch 328 / 500 | iteration 25 / 30 | Total Loss: 6.67169189453125 | KNN Loss: 5.618307590484619 | BCE Loss: 1.05338454246521\n",
      "Epoch 329 / 500 | iteration 0 / 30 | Total Loss: 6.699940204620361 | KNN Loss: 5.654464244842529 | BCE Loss: 1.0454758405685425\n",
      "Epoch 329 / 500 | iteration 5 / 30 | Total Loss: 6.658420562744141 | KNN Loss: 5.598931312561035 | BCE Loss: 1.0594892501831055\n",
      "Epoch 329 / 500 | iteration 10 / 30 | Total Loss: 6.64325475692749 | KNN Loss: 5.5926642417907715 | BCE Loss: 1.0505905151367188\n",
      "Epoch 329 / 500 | iteration 15 / 30 | Total Loss: 6.697051525115967 | KNN Loss: 5.659590244293213 | BCE Loss: 1.037461280822754\n",
      "Epoch 329 / 500 | iteration 20 / 30 | Total Loss: 6.722787857055664 | KNN Loss: 5.668364524841309 | BCE Loss: 1.0544230937957764\n",
      "Epoch 329 / 500 | iteration 25 / 30 | Total Loss: 6.6622700691223145 | KNN Loss: 5.61631441116333 | BCE Loss: 1.045955777168274\n",
      "Epoch 330 / 500 | iteration 0 / 30 | Total Loss: 6.692713260650635 | KNN Loss: 5.661344051361084 | BCE Loss: 1.0313692092895508\n",
      "Epoch 330 / 500 | iteration 5 / 30 | Total Loss: 6.690021514892578 | KNN Loss: 5.6161088943481445 | BCE Loss: 1.0739128589630127\n",
      "Epoch 330 / 500 | iteration 10 / 30 | Total Loss: 6.653149127960205 | KNN Loss: 5.612132549285889 | BCE Loss: 1.0410165786743164\n",
      "Epoch 330 / 500 | iteration 15 / 30 | Total Loss: 6.662996292114258 | KNN Loss: 5.612537384033203 | BCE Loss: 1.0504589080810547\n",
      "Epoch 330 / 500 | iteration 20 / 30 | Total Loss: 6.673280715942383 | KNN Loss: 5.603339672088623 | BCE Loss: 1.0699408054351807\n",
      "Epoch 330 / 500 | iteration 25 / 30 | Total Loss: 6.62869930267334 | KNN Loss: 5.591922283172607 | BCE Loss: 1.0367767810821533\n",
      "Epoch 331 / 500 | iteration 0 / 30 | Total Loss: 6.650126934051514 | KNN Loss: 5.6220173835754395 | BCE Loss: 1.0281095504760742\n",
      "Epoch 331 / 500 | iteration 5 / 30 | Total Loss: 6.720311641693115 | KNN Loss: 5.650713920593262 | BCE Loss: 1.069597601890564\n",
      "Epoch 331 / 500 | iteration 10 / 30 | Total Loss: 6.680507183074951 | KNN Loss: 5.637365818023682 | BCE Loss: 1.0431413650512695\n",
      "Epoch 331 / 500 | iteration 15 / 30 | Total Loss: 6.617579460144043 | KNN Loss: 5.59621524810791 | BCE Loss: 1.0213643312454224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 331 / 500 | iteration 20 / 30 | Total Loss: 6.7066330909729 | KNN Loss: 5.652093887329102 | BCE Loss: 1.0545392036437988\n",
      "Epoch 331 / 500 | iteration 25 / 30 | Total Loss: 6.701410293579102 | KNN Loss: 5.661324501037598 | BCE Loss: 1.0400855541229248\n",
      "Epoch 332 / 500 | iteration 0 / 30 | Total Loss: 6.634898662567139 | KNN Loss: 5.609177589416504 | BCE Loss: 1.0257210731506348\n",
      "Epoch 332 / 500 | iteration 5 / 30 | Total Loss: 6.735024929046631 | KNN Loss: 5.679218292236328 | BCE Loss: 1.0558066368103027\n",
      "Epoch 332 / 500 | iteration 10 / 30 | Total Loss: 6.6408891677856445 | KNN Loss: 5.616164207458496 | BCE Loss: 1.0247249603271484\n",
      "Epoch 332 / 500 | iteration 15 / 30 | Total Loss: 6.727060794830322 | KNN Loss: 5.638340950012207 | BCE Loss: 1.0887197256088257\n",
      "Epoch 332 / 500 | iteration 20 / 30 | Total Loss: 6.675785064697266 | KNN Loss: 5.621368885040283 | BCE Loss: 1.054416298866272\n",
      "Epoch 332 / 500 | iteration 25 / 30 | Total Loss: 6.689411640167236 | KNN Loss: 5.639071941375732 | BCE Loss: 1.050339698791504\n",
      "Epoch 333 / 500 | iteration 0 / 30 | Total Loss: 6.691391468048096 | KNN Loss: 5.663269996643066 | BCE Loss: 1.0281215906143188\n",
      "Epoch 333 / 500 | iteration 5 / 30 | Total Loss: 6.676107883453369 | KNN Loss: 5.626132488250732 | BCE Loss: 1.0499755144119263\n",
      "Epoch 333 / 500 | iteration 10 / 30 | Total Loss: 6.6397833824157715 | KNN Loss: 5.610456466674805 | BCE Loss: 1.0293269157409668\n",
      "Epoch 333 / 500 | iteration 15 / 30 | Total Loss: 6.6384477615356445 | KNN Loss: 5.624128341674805 | BCE Loss: 1.0143194198608398\n",
      "Epoch 333 / 500 | iteration 20 / 30 | Total Loss: 6.671165466308594 | KNN Loss: 5.628484725952148 | BCE Loss: 1.0426807403564453\n",
      "Epoch 333 / 500 | iteration 25 / 30 | Total Loss: 6.668635368347168 | KNN Loss: 5.625527381896973 | BCE Loss: 1.0431082248687744\n",
      "Epoch 334 / 500 | iteration 0 / 30 | Total Loss: 6.658329963684082 | KNN Loss: 5.595639705657959 | BCE Loss: 1.0626904964447021\n",
      "Epoch 334 / 500 | iteration 5 / 30 | Total Loss: 6.63637113571167 | KNN Loss: 5.591458797454834 | BCE Loss: 1.0449124574661255\n",
      "Epoch 334 / 500 | iteration 10 / 30 | Total Loss: 6.662082672119141 | KNN Loss: 5.596467971801758 | BCE Loss: 1.0656148195266724\n",
      "Epoch 334 / 500 | iteration 15 / 30 | Total Loss: 6.667377471923828 | KNN Loss: 5.623606204986572 | BCE Loss: 1.0437710285186768\n",
      "Epoch 334 / 500 | iteration 20 / 30 | Total Loss: 6.699653625488281 | KNN Loss: 5.642314910888672 | BCE Loss: 1.0573384761810303\n",
      "Epoch 334 / 500 | iteration 25 / 30 | Total Loss: 6.749894142150879 | KNN Loss: 5.6761393547058105 | BCE Loss: 1.073754906654358\n",
      "Epoch 335 / 500 | iteration 0 / 30 | Total Loss: 6.634910583496094 | KNN Loss: 5.602176666259766 | BCE Loss: 1.0327337980270386\n",
      "Epoch 335 / 500 | iteration 5 / 30 | Total Loss: 6.664381980895996 | KNN Loss: 5.6376800537109375 | BCE Loss: 1.0267021656036377\n",
      "Epoch 335 / 500 | iteration 10 / 30 | Total Loss: 6.650710105895996 | KNN Loss: 5.599366664886475 | BCE Loss: 1.051343321800232\n",
      "Epoch 335 / 500 | iteration 15 / 30 | Total Loss: 6.751636981964111 | KNN Loss: 5.683441162109375 | BCE Loss: 1.0681958198547363\n",
      "Epoch 335 / 500 | iteration 20 / 30 | Total Loss: 6.724388122558594 | KNN Loss: 5.65835428237915 | BCE Loss: 1.0660340785980225\n",
      "Epoch 335 / 500 | iteration 25 / 30 | Total Loss: 6.6530442237854 | KNN Loss: 5.604757308959961 | BCE Loss: 1.04828679561615\n",
      "Epoch 336 / 500 | iteration 0 / 30 | Total Loss: 6.7506022453308105 | KNN Loss: 5.692786693572998 | BCE Loss: 1.0578155517578125\n",
      "Epoch 336 / 500 | iteration 5 / 30 | Total Loss: 6.7540602684021 | KNN Loss: 5.701096534729004 | BCE Loss: 1.0529637336730957\n",
      "Epoch 336 / 500 | iteration 10 / 30 | Total Loss: 6.710177421569824 | KNN Loss: 5.676395893096924 | BCE Loss: 1.0337812900543213\n",
      "Epoch 336 / 500 | iteration 15 / 30 | Total Loss: 6.668187141418457 | KNN Loss: 5.623666286468506 | BCE Loss: 1.0445208549499512\n",
      "Epoch 336 / 500 | iteration 20 / 30 | Total Loss: 6.728229522705078 | KNN Loss: 5.702981948852539 | BCE Loss: 1.025247573852539\n",
      "Epoch 336 / 500 | iteration 25 / 30 | Total Loss: 6.696691513061523 | KNN Loss: 5.641147613525391 | BCE Loss: 1.0555440187454224\n",
      "Epoch   337: reducing learning rate of group 0 to 1.3684e-06.\n",
      "Epoch 337 / 500 | iteration 0 / 30 | Total Loss: 6.7007927894592285 | KNN Loss: 5.642009735107422 | BCE Loss: 1.058782935142517\n",
      "Epoch 337 / 500 | iteration 5 / 30 | Total Loss: 6.663209915161133 | KNN Loss: 5.610287666320801 | BCE Loss: 1.0529224872589111\n",
      "Epoch 337 / 500 | iteration 10 / 30 | Total Loss: 6.612147331237793 | KNN Loss: 5.5965399742126465 | BCE Loss: 1.0156075954437256\n",
      "Epoch 337 / 500 | iteration 15 / 30 | Total Loss: 6.656655311584473 | KNN Loss: 5.6166911125183105 | BCE Loss: 1.0399640798568726\n",
      "Epoch 337 / 500 | iteration 20 / 30 | Total Loss: 6.679129600524902 | KNN Loss: 5.631550312042236 | BCE Loss: 1.0475794076919556\n",
      "Epoch 337 / 500 | iteration 25 / 30 | Total Loss: 6.736298561096191 | KNN Loss: 5.693903923034668 | BCE Loss: 1.0423946380615234\n",
      "Epoch 338 / 500 | iteration 0 / 30 | Total Loss: 6.668783664703369 | KNN Loss: 5.613531112670898 | BCE Loss: 1.0552525520324707\n",
      "Epoch 338 / 500 | iteration 5 / 30 | Total Loss: 6.690560340881348 | KNN Loss: 5.611354827880859 | BCE Loss: 1.0792052745819092\n",
      "Epoch 338 / 500 | iteration 10 / 30 | Total Loss: 6.651612281799316 | KNN Loss: 5.602391719818115 | BCE Loss: 1.049220323562622\n",
      "Epoch 338 / 500 | iteration 15 / 30 | Total Loss: 6.674644947052002 | KNN Loss: 5.641299724578857 | BCE Loss: 1.0333452224731445\n",
      "Epoch 338 / 500 | iteration 20 / 30 | Total Loss: 6.642508029937744 | KNN Loss: 5.613255500793457 | BCE Loss: 1.0292526483535767\n",
      "Epoch 338 / 500 | iteration 25 / 30 | Total Loss: 6.641139507293701 | KNN Loss: 5.63015604019165 | BCE Loss: 1.0109833478927612\n",
      "Epoch 339 / 500 | iteration 0 / 30 | Total Loss: 6.714711666107178 | KNN Loss: 5.67281436920166 | BCE Loss: 1.0418972969055176\n",
      "Epoch 339 / 500 | iteration 5 / 30 | Total Loss: 6.68762731552124 | KNN Loss: 5.618960857391357 | BCE Loss: 1.0686664581298828\n",
      "Epoch 339 / 500 | iteration 10 / 30 | Total Loss: 6.650785446166992 | KNN Loss: 5.6015095710754395 | BCE Loss: 1.0492761135101318\n",
      "Epoch 339 / 500 | iteration 15 / 30 | Total Loss: 6.667572498321533 | KNN Loss: 5.608917236328125 | BCE Loss: 1.0586552619934082\n",
      "Epoch 339 / 500 | iteration 20 / 30 | Total Loss: 6.652589797973633 | KNN Loss: 5.598114490509033 | BCE Loss: 1.0544750690460205\n",
      "Epoch 339 / 500 | iteration 25 / 30 | Total Loss: 6.695120811462402 | KNN Loss: 5.609864711761475 | BCE Loss: 1.0852560997009277\n",
      "Epoch 340 / 500 | iteration 0 / 30 | Total Loss: 6.644676685333252 | KNN Loss: 5.595665454864502 | BCE Loss: 1.0490113496780396\n",
      "Epoch 340 / 500 | iteration 5 / 30 | Total Loss: 6.636882781982422 | KNN Loss: 5.6089372634887695 | BCE Loss: 1.027945637702942\n",
      "Epoch 340 / 500 | iteration 10 / 30 | Total Loss: 6.683891296386719 | KNN Loss: 5.6477837562561035 | BCE Loss: 1.0361073017120361\n",
      "Epoch 340 / 500 | iteration 15 / 30 | Total Loss: 6.66872501373291 | KNN Loss: 5.626296520233154 | BCE Loss: 1.0424282550811768\n",
      "Epoch 340 / 500 | iteration 20 / 30 | Total Loss: 6.71813440322876 | KNN Loss: 5.656874656677246 | BCE Loss: 1.0612597465515137\n",
      "Epoch 340 / 500 | iteration 25 / 30 | Total Loss: 6.696352005004883 | KNN Loss: 5.637986660003662 | BCE Loss: 1.0583653450012207\n",
      "Epoch 341 / 500 | iteration 0 / 30 | Total Loss: 6.645221710205078 | KNN Loss: 5.608065605163574 | BCE Loss: 1.037156105041504\n",
      "Epoch 341 / 500 | iteration 5 / 30 | Total Loss: 6.641779899597168 | KNN Loss: 5.606778621673584 | BCE Loss: 1.035001277923584\n",
      "Epoch 341 / 500 | iteration 10 / 30 | Total Loss: 6.704346656799316 | KNN Loss: 5.634414196014404 | BCE Loss: 1.0699323415756226\n",
      "Epoch 341 / 500 | iteration 15 / 30 | Total Loss: 6.663504600524902 | KNN Loss: 5.597024440765381 | BCE Loss: 1.0664801597595215\n",
      "Epoch 341 / 500 | iteration 20 / 30 | Total Loss: 6.669590950012207 | KNN Loss: 5.643187999725342 | BCE Loss: 1.0264027118682861\n",
      "Epoch 341 / 500 | iteration 25 / 30 | Total Loss: 6.6391072273254395 | KNN Loss: 5.6076979637146 | BCE Loss: 1.0314091444015503\n",
      "Epoch 342 / 500 | iteration 0 / 30 | Total Loss: 6.722785949707031 | KNN Loss: 5.684962749481201 | BCE Loss: 1.037822961807251\n",
      "Epoch 342 / 500 | iteration 5 / 30 | Total Loss: 6.635152816772461 | KNN Loss: 5.60914421081543 | BCE Loss: 1.0260088443756104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 342 / 500 | iteration 10 / 30 | Total Loss: 6.669587135314941 | KNN Loss: 5.617282867431641 | BCE Loss: 1.0523042678833008\n",
      "Epoch 342 / 500 | iteration 15 / 30 | Total Loss: 6.65382194519043 | KNN Loss: 5.600876331329346 | BCE Loss: 1.052945613861084\n",
      "Epoch 342 / 500 | iteration 20 / 30 | Total Loss: 6.653424263000488 | KNN Loss: 5.621708869934082 | BCE Loss: 1.0317153930664062\n",
      "Epoch 342 / 500 | iteration 25 / 30 | Total Loss: 6.665713310241699 | KNN Loss: 5.6117658615112305 | BCE Loss: 1.0539476871490479\n",
      "Epoch 343 / 500 | iteration 0 / 30 | Total Loss: 6.65416955947876 | KNN Loss: 5.605318546295166 | BCE Loss: 1.0488508939743042\n",
      "Epoch 343 / 500 | iteration 5 / 30 | Total Loss: 6.677487373352051 | KNN Loss: 5.637402057647705 | BCE Loss: 1.0400854349136353\n",
      "Epoch 343 / 500 | iteration 10 / 30 | Total Loss: 6.69460916519165 | KNN Loss: 5.632408618927002 | BCE Loss: 1.0622005462646484\n",
      "Epoch 343 / 500 | iteration 15 / 30 | Total Loss: 6.640981674194336 | KNN Loss: 5.60848331451416 | BCE Loss: 1.0324981212615967\n",
      "Epoch 343 / 500 | iteration 20 / 30 | Total Loss: 6.686207294464111 | KNN Loss: 5.64254093170166 | BCE Loss: 1.0436662435531616\n",
      "Epoch 343 / 500 | iteration 25 / 30 | Total Loss: 6.756628036499023 | KNN Loss: 5.7124528884887695 | BCE Loss: 1.044175386428833\n",
      "Epoch 344 / 500 | iteration 0 / 30 | Total Loss: 6.637002468109131 | KNN Loss: 5.605554103851318 | BCE Loss: 1.031448245048523\n",
      "Epoch 344 / 500 | iteration 5 / 30 | Total Loss: 6.664879322052002 | KNN Loss: 5.618409156799316 | BCE Loss: 1.0464701652526855\n",
      "Epoch 344 / 500 | iteration 10 / 30 | Total Loss: 6.669632911682129 | KNN Loss: 5.62406587600708 | BCE Loss: 1.0455667972564697\n",
      "Epoch 344 / 500 | iteration 15 / 30 | Total Loss: 6.667401313781738 | KNN Loss: 5.61278772354126 | BCE Loss: 1.0546133518218994\n",
      "Epoch 344 / 500 | iteration 20 / 30 | Total Loss: 6.7363691329956055 | KNN Loss: 5.672115802764893 | BCE Loss: 1.0642530918121338\n",
      "Epoch 344 / 500 | iteration 25 / 30 | Total Loss: 6.781105041503906 | KNN Loss: 5.701732635498047 | BCE Loss: 1.079372525215149\n",
      "Epoch 345 / 500 | iteration 0 / 30 | Total Loss: 6.660499572753906 | KNN Loss: 5.627886772155762 | BCE Loss: 1.0326128005981445\n",
      "Epoch 345 / 500 | iteration 5 / 30 | Total Loss: 6.736544132232666 | KNN Loss: 5.64874792098999 | BCE Loss: 1.0877963304519653\n",
      "Epoch 345 / 500 | iteration 10 / 30 | Total Loss: 6.645444393157959 | KNN Loss: 5.599043369293213 | BCE Loss: 1.046401023864746\n",
      "Epoch 345 / 500 | iteration 15 / 30 | Total Loss: 6.75637674331665 | KNN Loss: 5.6789679527282715 | BCE Loss: 1.077408790588379\n",
      "Epoch 345 / 500 | iteration 20 / 30 | Total Loss: 6.705268383026123 | KNN Loss: 5.66650390625 | BCE Loss: 1.038764476776123\n",
      "Epoch 345 / 500 | iteration 25 / 30 | Total Loss: 6.713679313659668 | KNN Loss: 5.67672061920166 | BCE Loss: 1.0369586944580078\n",
      "Epoch 346 / 500 | iteration 0 / 30 | Total Loss: 6.674339294433594 | KNN Loss: 5.622875690460205 | BCE Loss: 1.0514638423919678\n",
      "Epoch 346 / 500 | iteration 5 / 30 | Total Loss: 6.670255184173584 | KNN Loss: 5.601712226867676 | BCE Loss: 1.0685429573059082\n",
      "Epoch 346 / 500 | iteration 10 / 30 | Total Loss: 6.627565383911133 | KNN Loss: 5.602234840393066 | BCE Loss: 1.0253303050994873\n",
      "Epoch 346 / 500 | iteration 15 / 30 | Total Loss: 6.667837142944336 | KNN Loss: 5.623230457305908 | BCE Loss: 1.0446069240570068\n",
      "Epoch 346 / 500 | iteration 20 / 30 | Total Loss: 6.672402381896973 | KNN Loss: 5.619625568389893 | BCE Loss: 1.052776575088501\n",
      "Epoch 346 / 500 | iteration 25 / 30 | Total Loss: 6.665185451507568 | KNN Loss: 5.637170314788818 | BCE Loss: 1.0280152559280396\n",
      "Epoch 347 / 500 | iteration 0 / 30 | Total Loss: 6.641849040985107 | KNN Loss: 5.607975006103516 | BCE Loss: 1.0338740348815918\n",
      "Epoch 347 / 500 | iteration 5 / 30 | Total Loss: 6.647181034088135 | KNN Loss: 5.595295429229736 | BCE Loss: 1.051885724067688\n",
      "Epoch 347 / 500 | iteration 10 / 30 | Total Loss: 6.650980472564697 | KNN Loss: 5.613777160644531 | BCE Loss: 1.0372031927108765\n",
      "Epoch 347 / 500 | iteration 15 / 30 | Total Loss: 6.72211217880249 | KNN Loss: 5.671553134918213 | BCE Loss: 1.0505590438842773\n",
      "Epoch 347 / 500 | iteration 20 / 30 | Total Loss: 6.667148590087891 | KNN Loss: 5.623532295227051 | BCE Loss: 1.0436162948608398\n",
      "Epoch 347 / 500 | iteration 25 / 30 | Total Loss: 6.711464881896973 | KNN Loss: 5.668402194976807 | BCE Loss: 1.0430629253387451\n",
      "Epoch   348: reducing learning rate of group 0 to 9.5791e-07.\n",
      "Epoch 348 / 500 | iteration 0 / 30 | Total Loss: 6.654268264770508 | KNN Loss: 5.594891548156738 | BCE Loss: 1.0593769550323486\n",
      "Epoch 348 / 500 | iteration 5 / 30 | Total Loss: 6.65522575378418 | KNN Loss: 5.6207275390625 | BCE Loss: 1.0344984531402588\n",
      "Epoch 348 / 500 | iteration 10 / 30 | Total Loss: 6.7726311683654785 | KNN Loss: 5.7162885665893555 | BCE Loss: 1.056342601776123\n",
      "Epoch 348 / 500 | iteration 15 / 30 | Total Loss: 6.659345626831055 | KNN Loss: 5.597370147705078 | BCE Loss: 1.0619752407073975\n",
      "Epoch 348 / 500 | iteration 20 / 30 | Total Loss: 6.667855262756348 | KNN Loss: 5.619458198547363 | BCE Loss: 1.0483968257904053\n",
      "Epoch 348 / 500 | iteration 25 / 30 | Total Loss: 6.668512344360352 | KNN Loss: 5.621908187866211 | BCE Loss: 1.0466042757034302\n",
      "Epoch 349 / 500 | iteration 0 / 30 | Total Loss: 6.747129917144775 | KNN Loss: 5.662765026092529 | BCE Loss: 1.0843650102615356\n",
      "Epoch 349 / 500 | iteration 5 / 30 | Total Loss: 6.635738849639893 | KNN Loss: 5.602632522583008 | BCE Loss: 1.0331064462661743\n",
      "Epoch 349 / 500 | iteration 10 / 30 | Total Loss: 6.781876564025879 | KNN Loss: 5.720218181610107 | BCE Loss: 1.061658263206482\n",
      "Epoch 349 / 500 | iteration 15 / 30 | Total Loss: 6.74418830871582 | KNN Loss: 5.720609188079834 | BCE Loss: 1.0235793590545654\n",
      "Epoch 349 / 500 | iteration 20 / 30 | Total Loss: 6.676634311676025 | KNN Loss: 5.613799571990967 | BCE Loss: 1.0628348588943481\n",
      "Epoch 349 / 500 | iteration 25 / 30 | Total Loss: 6.657536029815674 | KNN Loss: 5.618276119232178 | BCE Loss: 1.039259910583496\n",
      "Epoch 350 / 500 | iteration 0 / 30 | Total Loss: 6.686790466308594 | KNN Loss: 5.630345821380615 | BCE Loss: 1.0564444065093994\n",
      "Epoch 350 / 500 | iteration 5 / 30 | Total Loss: 6.6167402267456055 | KNN Loss: 5.595998764038086 | BCE Loss: 1.02074134349823\n",
      "Epoch 350 / 500 | iteration 10 / 30 | Total Loss: 6.67697811126709 | KNN Loss: 5.622150897979736 | BCE Loss: 1.0548272132873535\n",
      "Epoch 350 / 500 | iteration 15 / 30 | Total Loss: 6.712609767913818 | KNN Loss: 5.631906986236572 | BCE Loss: 1.080702781677246\n",
      "Epoch 350 / 500 | iteration 20 / 30 | Total Loss: 6.6221466064453125 | KNN Loss: 5.593892574310303 | BCE Loss: 1.0282539129257202\n",
      "Epoch 350 / 500 | iteration 25 / 30 | Total Loss: 6.66643762588501 | KNN Loss: 5.634387493133545 | BCE Loss: 1.0320500135421753\n",
      "Epoch 351 / 500 | iteration 0 / 30 | Total Loss: 6.649482727050781 | KNN Loss: 5.614956378936768 | BCE Loss: 1.0345265865325928\n",
      "Epoch 351 / 500 | iteration 5 / 30 | Total Loss: 6.6628313064575195 | KNN Loss: 5.618945598602295 | BCE Loss: 1.0438858270645142\n",
      "Epoch 351 / 500 | iteration 10 / 30 | Total Loss: 6.721406936645508 | KNN Loss: 5.666802883148193 | BCE Loss: 1.0546040534973145\n",
      "Epoch 351 / 500 | iteration 15 / 30 | Total Loss: 6.637569427490234 | KNN Loss: 5.608298301696777 | BCE Loss: 1.029270887374878\n",
      "Epoch 351 / 500 | iteration 20 / 30 | Total Loss: 6.726510047912598 | KNN Loss: 5.668087005615234 | BCE Loss: 1.0584229230880737\n",
      "Epoch 351 / 500 | iteration 25 / 30 | Total Loss: 6.681445598602295 | KNN Loss: 5.626485824584961 | BCE Loss: 1.054959774017334\n",
      "Epoch 352 / 500 | iteration 0 / 30 | Total Loss: 6.6751813888549805 | KNN Loss: 5.622941017150879 | BCE Loss: 1.0522404909133911\n",
      "Epoch 352 / 500 | iteration 5 / 30 | Total Loss: 6.681666374206543 | KNN Loss: 5.629719257354736 | BCE Loss: 1.0519471168518066\n",
      "Epoch 352 / 500 | iteration 10 / 30 | Total Loss: 6.625046253204346 | KNN Loss: 5.590028762817383 | BCE Loss: 1.0350173711776733\n",
      "Epoch 352 / 500 | iteration 15 / 30 | Total Loss: 6.667241096496582 | KNN Loss: 5.605091571807861 | BCE Loss: 1.0621492862701416\n",
      "Epoch 352 / 500 | iteration 20 / 30 | Total Loss: 6.7920098304748535 | KNN Loss: 5.722270965576172 | BCE Loss: 1.0697389841079712\n",
      "Epoch 352 / 500 | iteration 25 / 30 | Total Loss: 6.703763961791992 | KNN Loss: 5.656909465789795 | BCE Loss: 1.0468546152114868\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 353 / 500 | iteration 0 / 30 | Total Loss: 6.652334213256836 | KNN Loss: 5.625709056854248 | BCE Loss: 1.0266252756118774\n",
      "Epoch 353 / 500 | iteration 5 / 30 | Total Loss: 6.793625831604004 | KNN Loss: 5.735267162322998 | BCE Loss: 1.0583585500717163\n",
      "Epoch 353 / 500 | iteration 10 / 30 | Total Loss: 6.6278252601623535 | KNN Loss: 5.595922946929932 | BCE Loss: 1.0319023132324219\n",
      "Epoch 353 / 500 | iteration 15 / 30 | Total Loss: 6.75868034362793 | KNN Loss: 5.689879417419434 | BCE Loss: 1.0688011646270752\n",
      "Epoch 353 / 500 | iteration 20 / 30 | Total Loss: 6.699810981750488 | KNN Loss: 5.596123695373535 | BCE Loss: 1.1036872863769531\n",
      "Epoch 353 / 500 | iteration 25 / 30 | Total Loss: 6.683258056640625 | KNN Loss: 5.651554584503174 | BCE Loss: 1.0317037105560303\n",
      "Epoch 354 / 500 | iteration 0 / 30 | Total Loss: 6.620163917541504 | KNN Loss: 5.601367473602295 | BCE Loss: 1.018796682357788\n",
      "Epoch 354 / 500 | iteration 5 / 30 | Total Loss: 6.659585952758789 | KNN Loss: 5.61619758605957 | BCE Loss: 1.0433886051177979\n",
      "Epoch 354 / 500 | iteration 10 / 30 | Total Loss: 6.634241104125977 | KNN Loss: 5.593659400939941 | BCE Loss: 1.040581464767456\n",
      "Epoch 354 / 500 | iteration 15 / 30 | Total Loss: 6.679412841796875 | KNN Loss: 5.642387866973877 | BCE Loss: 1.0370252132415771\n",
      "Epoch 354 / 500 | iteration 20 / 30 | Total Loss: 6.658697128295898 | KNN Loss: 5.612438678741455 | BCE Loss: 1.0462584495544434\n",
      "Epoch 354 / 500 | iteration 25 / 30 | Total Loss: 6.6879682540893555 | KNN Loss: 5.624541282653809 | BCE Loss: 1.0634269714355469\n",
      "Epoch 355 / 500 | iteration 0 / 30 | Total Loss: 6.636310577392578 | KNN Loss: 5.597890377044678 | BCE Loss: 1.0384202003479004\n",
      "Epoch 355 / 500 | iteration 5 / 30 | Total Loss: 6.66513204574585 | KNN Loss: 5.598891735076904 | BCE Loss: 1.0662403106689453\n",
      "Epoch 355 / 500 | iteration 10 / 30 | Total Loss: 6.625837326049805 | KNN Loss: 5.588820457458496 | BCE Loss: 1.0370169878005981\n",
      "Epoch 355 / 500 | iteration 15 / 30 | Total Loss: 6.694613456726074 | KNN Loss: 5.62986946105957 | BCE Loss: 1.064743995666504\n",
      "Epoch 355 / 500 | iteration 20 / 30 | Total Loss: 6.6788835525512695 | KNN Loss: 5.638744354248047 | BCE Loss: 1.0401394367218018\n",
      "Epoch 355 / 500 | iteration 25 / 30 | Total Loss: 6.664391994476318 | KNN Loss: 5.631771564483643 | BCE Loss: 1.0326204299926758\n",
      "Epoch 356 / 500 | iteration 0 / 30 | Total Loss: 6.69054651260376 | KNN Loss: 5.633499622344971 | BCE Loss: 1.0570470094680786\n",
      "Epoch 356 / 500 | iteration 5 / 30 | Total Loss: 6.735393524169922 | KNN Loss: 5.679874897003174 | BCE Loss: 1.055518388748169\n",
      "Epoch 356 / 500 | iteration 10 / 30 | Total Loss: 6.622674942016602 | KNN Loss: 5.5891265869140625 | BCE Loss: 1.0335484743118286\n",
      "Epoch 356 / 500 | iteration 15 / 30 | Total Loss: 6.631045818328857 | KNN Loss: 5.590878486633301 | BCE Loss: 1.0401673316955566\n",
      "Epoch 356 / 500 | iteration 20 / 30 | Total Loss: 6.731417179107666 | KNN Loss: 5.654843330383301 | BCE Loss: 1.0765738487243652\n",
      "Epoch 356 / 500 | iteration 25 / 30 | Total Loss: 6.646152973175049 | KNN Loss: 5.613877296447754 | BCE Loss: 1.032275676727295\n",
      "Epoch 357 / 500 | iteration 0 / 30 | Total Loss: 6.705131530761719 | KNN Loss: 5.646112442016602 | BCE Loss: 1.0590190887451172\n",
      "Epoch 357 / 500 | iteration 5 / 30 | Total Loss: 6.659228324890137 | KNN Loss: 5.639925479888916 | BCE Loss: 1.0193028450012207\n",
      "Epoch 357 / 500 | iteration 10 / 30 | Total Loss: 6.688755512237549 | KNN Loss: 5.630605697631836 | BCE Loss: 1.058149814605713\n",
      "Epoch 357 / 500 | iteration 15 / 30 | Total Loss: 6.682981491088867 | KNN Loss: 5.620629787445068 | BCE Loss: 1.0623518228530884\n",
      "Epoch 357 / 500 | iteration 20 / 30 | Total Loss: 6.6478271484375 | KNN Loss: 5.611979007720947 | BCE Loss: 1.0358480215072632\n",
      "Epoch 357 / 500 | iteration 25 / 30 | Total Loss: 6.708292484283447 | KNN Loss: 5.636113166809082 | BCE Loss: 1.0721793174743652\n",
      "Epoch 358 / 500 | iteration 0 / 30 | Total Loss: 6.74828577041626 | KNN Loss: 5.700551986694336 | BCE Loss: 1.0477337837219238\n",
      "Epoch 358 / 500 | iteration 5 / 30 | Total Loss: 6.659002780914307 | KNN Loss: 5.598495960235596 | BCE Loss: 1.0605067014694214\n",
      "Epoch 358 / 500 | iteration 10 / 30 | Total Loss: 6.647538185119629 | KNN Loss: 5.593305587768555 | BCE Loss: 1.0542324781417847\n",
      "Epoch 358 / 500 | iteration 15 / 30 | Total Loss: 6.654021263122559 | KNN Loss: 5.6066460609436035 | BCE Loss: 1.047375202178955\n",
      "Epoch 358 / 500 | iteration 20 / 30 | Total Loss: 6.677585601806641 | KNN Loss: 5.631524085998535 | BCE Loss: 1.0460612773895264\n",
      "Epoch 358 / 500 | iteration 25 / 30 | Total Loss: 6.689093589782715 | KNN Loss: 5.614676475524902 | BCE Loss: 1.074416995048523\n",
      "Epoch   359: reducing learning rate of group 0 to 6.7053e-07.\n",
      "Epoch 359 / 500 | iteration 0 / 30 | Total Loss: 6.719918251037598 | KNN Loss: 5.644214630126953 | BCE Loss: 1.0757033824920654\n",
      "Epoch 359 / 500 | iteration 5 / 30 | Total Loss: 6.769979953765869 | KNN Loss: 5.721485137939453 | BCE Loss: 1.0484946966171265\n",
      "Epoch 359 / 500 | iteration 10 / 30 | Total Loss: 6.685256481170654 | KNN Loss: 5.632442951202393 | BCE Loss: 1.0528136491775513\n",
      "Epoch 359 / 500 | iteration 15 / 30 | Total Loss: 6.6556291580200195 | KNN Loss: 5.601420879364014 | BCE Loss: 1.0542083978652954\n",
      "Epoch 359 / 500 | iteration 20 / 30 | Total Loss: 6.683813095092773 | KNN Loss: 5.605584144592285 | BCE Loss: 1.0782287120819092\n",
      "Epoch 359 / 500 | iteration 25 / 30 | Total Loss: 6.696208477020264 | KNN Loss: 5.642289638519287 | BCE Loss: 1.0539189577102661\n",
      "Epoch 360 / 500 | iteration 0 / 30 | Total Loss: 6.656896591186523 | KNN Loss: 5.5959978103637695 | BCE Loss: 1.060898780822754\n",
      "Epoch 360 / 500 | iteration 5 / 30 | Total Loss: 6.711284160614014 | KNN Loss: 5.629755973815918 | BCE Loss: 1.0815283060073853\n",
      "Epoch 360 / 500 | iteration 10 / 30 | Total Loss: 6.645113945007324 | KNN Loss: 5.60935115814209 | BCE Loss: 1.0357626676559448\n",
      "Epoch 360 / 500 | iteration 15 / 30 | Total Loss: 6.6523332595825195 | KNN Loss: 5.617582321166992 | BCE Loss: 1.0347509384155273\n",
      "Epoch 360 / 500 | iteration 20 / 30 | Total Loss: 6.633976936340332 | KNN Loss: 5.60298490524292 | BCE Loss: 1.0309921503067017\n",
      "Epoch 360 / 500 | iteration 25 / 30 | Total Loss: 6.722902774810791 | KNN Loss: 5.672247409820557 | BCE Loss: 1.050655484199524\n",
      "Epoch 361 / 500 | iteration 0 / 30 | Total Loss: 6.687856197357178 | KNN Loss: 5.624492168426514 | BCE Loss: 1.063364028930664\n",
      "Epoch 361 / 500 | iteration 5 / 30 | Total Loss: 6.695734024047852 | KNN Loss: 5.653181552886963 | BCE Loss: 1.0425523519515991\n",
      "Epoch 361 / 500 | iteration 10 / 30 | Total Loss: 6.687677383422852 | KNN Loss: 5.664859294891357 | BCE Loss: 1.022817850112915\n",
      "Epoch 361 / 500 | iteration 15 / 30 | Total Loss: 6.648745059967041 | KNN Loss: 5.592858791351318 | BCE Loss: 1.055886149406433\n",
      "Epoch 361 / 500 | iteration 20 / 30 | Total Loss: 6.7416791915893555 | KNN Loss: 5.699610710144043 | BCE Loss: 1.0420687198638916\n",
      "Epoch 361 / 500 | iteration 25 / 30 | Total Loss: 6.679027557373047 | KNN Loss: 5.609851837158203 | BCE Loss: 1.0691757202148438\n",
      "Epoch 362 / 500 | iteration 0 / 30 | Total Loss: 6.681629657745361 | KNN Loss: 5.607982635498047 | BCE Loss: 1.0736470222473145\n",
      "Epoch 362 / 500 | iteration 5 / 30 | Total Loss: 6.66585636138916 | KNN Loss: 5.6135783195495605 | BCE Loss: 1.0522780418395996\n",
      "Epoch 362 / 500 | iteration 10 / 30 | Total Loss: 6.652789115905762 | KNN Loss: 5.625487804412842 | BCE Loss: 1.02730131149292\n",
      "Epoch 362 / 500 | iteration 15 / 30 | Total Loss: 6.618999481201172 | KNN Loss: 5.592590808868408 | BCE Loss: 1.0264087915420532\n",
      "Epoch 362 / 500 | iteration 20 / 30 | Total Loss: 6.66703987121582 | KNN Loss: 5.612021446228027 | BCE Loss: 1.0550181865692139\n",
      "Epoch 362 / 500 | iteration 25 / 30 | Total Loss: 6.7053985595703125 | KNN Loss: 5.62575101852417 | BCE Loss: 1.079647421836853\n",
      "Epoch 363 / 500 | iteration 0 / 30 | Total Loss: 6.80290412902832 | KNN Loss: 5.742935657501221 | BCE Loss: 1.0599687099456787\n",
      "Epoch 363 / 500 | iteration 5 / 30 | Total Loss: 6.615631103515625 | KNN Loss: 5.600255966186523 | BCE Loss: 1.0153753757476807\n",
      "Epoch 363 / 500 | iteration 10 / 30 | Total Loss: 6.667434215545654 | KNN Loss: 5.612005710601807 | BCE Loss: 1.0554285049438477\n",
      "Epoch 363 / 500 | iteration 15 / 30 | Total Loss: 6.63889217376709 | KNN Loss: 5.600218296051025 | BCE Loss: 1.0386741161346436\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 363 / 500 | iteration 20 / 30 | Total Loss: 6.652896881103516 | KNN Loss: 5.610687732696533 | BCE Loss: 1.0422090291976929\n",
      "Epoch 363 / 500 | iteration 25 / 30 | Total Loss: 6.658353805541992 | KNN Loss: 5.626745223999023 | BCE Loss: 1.0316088199615479\n",
      "Epoch 364 / 500 | iteration 0 / 30 | Total Loss: 6.647310733795166 | KNN Loss: 5.6042399406433105 | BCE Loss: 1.043070912361145\n",
      "Epoch 364 / 500 | iteration 5 / 30 | Total Loss: 6.6379876136779785 | KNN Loss: 5.599636077880859 | BCE Loss: 1.0383514165878296\n",
      "Epoch 364 / 500 | iteration 10 / 30 | Total Loss: 6.7761430740356445 | KNN Loss: 5.715860366821289 | BCE Loss: 1.060282588005066\n",
      "Epoch 364 / 500 | iteration 15 / 30 | Total Loss: 6.662109375 | KNN Loss: 5.622045040130615 | BCE Loss: 1.0400643348693848\n",
      "Epoch 364 / 500 | iteration 20 / 30 | Total Loss: 6.733790874481201 | KNN Loss: 5.675692558288574 | BCE Loss: 1.0580984354019165\n",
      "Epoch 364 / 500 | iteration 25 / 30 | Total Loss: 6.651797294616699 | KNN Loss: 5.597598075866699 | BCE Loss: 1.05419921875\n",
      "Epoch 365 / 500 | iteration 0 / 30 | Total Loss: 6.625120162963867 | KNN Loss: 5.604773044586182 | BCE Loss: 1.020346999168396\n",
      "Epoch 365 / 500 | iteration 5 / 30 | Total Loss: 6.698119640350342 | KNN Loss: 5.639201641082764 | BCE Loss: 1.0589179992675781\n",
      "Epoch 365 / 500 | iteration 10 / 30 | Total Loss: 6.654442310333252 | KNN Loss: 5.597641468048096 | BCE Loss: 1.0568008422851562\n",
      "Epoch 365 / 500 | iteration 15 / 30 | Total Loss: 6.671288013458252 | KNN Loss: 5.6240057945251465 | BCE Loss: 1.047282099723816\n",
      "Epoch 365 / 500 | iteration 20 / 30 | Total Loss: 6.685140132904053 | KNN Loss: 5.654627799987793 | BCE Loss: 1.0305124521255493\n",
      "Epoch 365 / 500 | iteration 25 / 30 | Total Loss: 6.76583194732666 | KNN Loss: 5.681817054748535 | BCE Loss: 1.084015130996704\n",
      "Epoch 366 / 500 | iteration 0 / 30 | Total Loss: 6.639861106872559 | KNN Loss: 5.590458869934082 | BCE Loss: 1.0494024753570557\n",
      "Epoch 366 / 500 | iteration 5 / 30 | Total Loss: 6.682796001434326 | KNN Loss: 5.615839958190918 | BCE Loss: 1.0669559240341187\n",
      "Epoch 366 / 500 | iteration 10 / 30 | Total Loss: 6.700031280517578 | KNN Loss: 5.652464389801025 | BCE Loss: 1.0475668907165527\n",
      "Epoch 366 / 500 | iteration 15 / 30 | Total Loss: 6.618434906005859 | KNN Loss: 5.5961384773254395 | BCE Loss: 1.0222961902618408\n",
      "Epoch 366 / 500 | iteration 20 / 30 | Total Loss: 6.704112529754639 | KNN Loss: 5.650084972381592 | BCE Loss: 1.0540275573730469\n",
      "Epoch 366 / 500 | iteration 25 / 30 | Total Loss: 6.661059856414795 | KNN Loss: 5.61115026473999 | BCE Loss: 1.0499095916748047\n",
      "Epoch 367 / 500 | iteration 0 / 30 | Total Loss: 6.657189846038818 | KNN Loss: 5.609735012054443 | BCE Loss: 1.047454833984375\n",
      "Epoch 367 / 500 | iteration 5 / 30 | Total Loss: 6.672188758850098 | KNN Loss: 5.6416449546813965 | BCE Loss: 1.0305440425872803\n",
      "Epoch 367 / 500 | iteration 10 / 30 | Total Loss: 6.6795334815979 | KNN Loss: 5.6200852394104 | BCE Loss: 1.0594482421875\n",
      "Epoch 367 / 500 | iteration 15 / 30 | Total Loss: 6.67990255355835 | KNN Loss: 5.609749794006348 | BCE Loss: 1.0701528787612915\n",
      "Epoch 367 / 500 | iteration 20 / 30 | Total Loss: 6.687456130981445 | KNN Loss: 5.618570327758789 | BCE Loss: 1.0688856840133667\n",
      "Epoch 367 / 500 | iteration 25 / 30 | Total Loss: 6.654008388519287 | KNN Loss: 5.595267295837402 | BCE Loss: 1.0587410926818848\n",
      "Epoch 368 / 500 | iteration 0 / 30 | Total Loss: 6.625133037567139 | KNN Loss: 5.601304054260254 | BCE Loss: 1.0238289833068848\n",
      "Epoch 368 / 500 | iteration 5 / 30 | Total Loss: 6.7354736328125 | KNN Loss: 5.682303428649902 | BCE Loss: 1.0531699657440186\n",
      "Epoch 368 / 500 | iteration 10 / 30 | Total Loss: 6.661042213439941 | KNN Loss: 5.628345489501953 | BCE Loss: 1.0326968431472778\n",
      "Epoch 368 / 500 | iteration 15 / 30 | Total Loss: 6.712957382202148 | KNN Loss: 5.668872356414795 | BCE Loss: 1.0440852642059326\n",
      "Epoch 368 / 500 | iteration 20 / 30 | Total Loss: 6.645226955413818 | KNN Loss: 5.599355220794678 | BCE Loss: 1.0458717346191406\n",
      "Epoch 368 / 500 | iteration 25 / 30 | Total Loss: 6.646147727966309 | KNN Loss: 5.600649356842041 | BCE Loss: 1.045498251914978\n",
      "Epoch 369 / 500 | iteration 0 / 30 | Total Loss: 6.643146991729736 | KNN Loss: 5.61374044418335 | BCE Loss: 1.0294065475463867\n",
      "Epoch 369 / 500 | iteration 5 / 30 | Total Loss: 6.653787612915039 | KNN Loss: 5.607346057891846 | BCE Loss: 1.0464413166046143\n",
      "Epoch 369 / 500 | iteration 10 / 30 | Total Loss: 6.706832408905029 | KNN Loss: 5.662587642669678 | BCE Loss: 1.0442447662353516\n",
      "Epoch 369 / 500 | iteration 15 / 30 | Total Loss: 6.665009498596191 | KNN Loss: 5.635746002197266 | BCE Loss: 1.0292637348175049\n",
      "Epoch 369 / 500 | iteration 20 / 30 | Total Loss: 6.710569381713867 | KNN Loss: 5.642422676086426 | BCE Loss: 1.068146824836731\n",
      "Epoch 369 / 500 | iteration 25 / 30 | Total Loss: 6.623084545135498 | KNN Loss: 5.594596862792969 | BCE Loss: 1.0284876823425293\n",
      "Epoch   370: reducing learning rate of group 0 to 4.6937e-07.\n",
      "Epoch 370 / 500 | iteration 0 / 30 | Total Loss: 6.662199974060059 | KNN Loss: 5.6017255783081055 | BCE Loss: 1.0604743957519531\n",
      "Epoch 370 / 500 | iteration 5 / 30 | Total Loss: 6.689817905426025 | KNN Loss: 5.617407321929932 | BCE Loss: 1.0724107027053833\n",
      "Epoch 370 / 500 | iteration 10 / 30 | Total Loss: 6.738667964935303 | KNN Loss: 5.697908878326416 | BCE Loss: 1.0407590866088867\n",
      "Epoch 370 / 500 | iteration 15 / 30 | Total Loss: 6.673526763916016 | KNN Loss: 5.645136833190918 | BCE Loss: 1.0283896923065186\n",
      "Epoch 370 / 500 | iteration 20 / 30 | Total Loss: 6.653027534484863 | KNN Loss: 5.603051662445068 | BCE Loss: 1.049975872039795\n",
      "Epoch 370 / 500 | iteration 25 / 30 | Total Loss: 6.725354194641113 | KNN Loss: 5.680517673492432 | BCE Loss: 1.0448366403579712\n",
      "Epoch 371 / 500 | iteration 0 / 30 | Total Loss: 6.654253959655762 | KNN Loss: 5.605147838592529 | BCE Loss: 1.0491060018539429\n",
      "Epoch 371 / 500 | iteration 5 / 30 | Total Loss: 6.699972152709961 | KNN Loss: 5.663997650146484 | BCE Loss: 1.0359747409820557\n",
      "Epoch 371 / 500 | iteration 10 / 30 | Total Loss: 6.661208629608154 | KNN Loss: 5.645254611968994 | BCE Loss: 1.0159538984298706\n",
      "Epoch 371 / 500 | iteration 15 / 30 | Total Loss: 6.632214069366455 | KNN Loss: 5.602764129638672 | BCE Loss: 1.0294500589370728\n",
      "Epoch 371 / 500 | iteration 20 / 30 | Total Loss: 6.681427955627441 | KNN Loss: 5.6407599449157715 | BCE Loss: 1.0406677722930908\n",
      "Epoch 371 / 500 | iteration 25 / 30 | Total Loss: 6.660393714904785 | KNN Loss: 5.595752239227295 | BCE Loss: 1.0646414756774902\n",
      "Epoch 372 / 500 | iteration 0 / 30 | Total Loss: 6.684979438781738 | KNN Loss: 5.650017261505127 | BCE Loss: 1.0349619388580322\n",
      "Epoch 372 / 500 | iteration 5 / 30 | Total Loss: 6.637369155883789 | KNN Loss: 5.597994804382324 | BCE Loss: 1.0393743515014648\n",
      "Epoch 372 / 500 | iteration 10 / 30 | Total Loss: 6.676959037780762 | KNN Loss: 5.599965572357178 | BCE Loss: 1.076993465423584\n",
      "Epoch 372 / 500 | iteration 15 / 30 | Total Loss: 6.727481842041016 | KNN Loss: 5.697722434997559 | BCE Loss: 1.0297596454620361\n",
      "Epoch 372 / 500 | iteration 20 / 30 | Total Loss: 6.765307426452637 | KNN Loss: 5.719388008117676 | BCE Loss: 1.0459191799163818\n",
      "Epoch 372 / 500 | iteration 25 / 30 | Total Loss: 6.650801658630371 | KNN Loss: 5.626238822937012 | BCE Loss: 1.0245630741119385\n",
      "Epoch 373 / 500 | iteration 0 / 30 | Total Loss: 6.6746625900268555 | KNN Loss: 5.642596244812012 | BCE Loss: 1.0320665836334229\n",
      "Epoch 373 / 500 | iteration 5 / 30 | Total Loss: 6.641593933105469 | KNN Loss: 5.594938278198242 | BCE Loss: 1.0466554164886475\n",
      "Epoch 373 / 500 | iteration 10 / 30 | Total Loss: 6.649139881134033 | KNN Loss: 5.593069076538086 | BCE Loss: 1.0560708045959473\n",
      "Epoch 373 / 500 | iteration 15 / 30 | Total Loss: 6.627286434173584 | KNN Loss: 5.604557037353516 | BCE Loss: 1.0227293968200684\n",
      "Epoch 373 / 500 | iteration 20 / 30 | Total Loss: 6.672344207763672 | KNN Loss: 5.631543159484863 | BCE Loss: 1.040800929069519\n",
      "Epoch 373 / 500 | iteration 25 / 30 | Total Loss: 6.657773494720459 | KNN Loss: 5.611659526824951 | BCE Loss: 1.0461139678955078\n",
      "Epoch 374 / 500 | iteration 0 / 30 | Total Loss: 6.721494674682617 | KNN Loss: 5.655317783355713 | BCE Loss: 1.0661770105361938\n",
      "Epoch 374 / 500 | iteration 5 / 30 | Total Loss: 6.752508163452148 | KNN Loss: 5.707004547119141 | BCE Loss: 1.045503854751587\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 374 / 500 | iteration 10 / 30 | Total Loss: 6.644322395324707 | KNN Loss: 5.604955673217773 | BCE Loss: 1.0393668413162231\n",
      "Epoch 374 / 500 | iteration 15 / 30 | Total Loss: 6.648681163787842 | KNN Loss: 5.599183559417725 | BCE Loss: 1.0494976043701172\n",
      "Epoch 374 / 500 | iteration 20 / 30 | Total Loss: 6.654484272003174 | KNN Loss: 5.609583377838135 | BCE Loss: 1.0449007749557495\n",
      "Epoch 374 / 500 | iteration 25 / 30 | Total Loss: 6.679830551147461 | KNN Loss: 5.626730918884277 | BCE Loss: 1.0530993938446045\n",
      "Epoch 375 / 500 | iteration 0 / 30 | Total Loss: 6.65690803527832 | KNN Loss: 5.647232532501221 | BCE Loss: 1.0096756219863892\n",
      "Epoch 375 / 500 | iteration 5 / 30 | Total Loss: 6.722081661224365 | KNN Loss: 5.663464069366455 | BCE Loss: 1.0586174726486206\n",
      "Epoch 375 / 500 | iteration 10 / 30 | Total Loss: 6.637582778930664 | KNN Loss: 5.607883930206299 | BCE Loss: 1.0296987295150757\n",
      "Epoch 375 / 500 | iteration 15 / 30 | Total Loss: 6.772745132446289 | KNN Loss: 5.732587814331055 | BCE Loss: 1.0401573181152344\n",
      "Epoch 375 / 500 | iteration 20 / 30 | Total Loss: 6.7464399337768555 | KNN Loss: 5.668005466461182 | BCE Loss: 1.0784343481063843\n",
      "Epoch 375 / 500 | iteration 25 / 30 | Total Loss: 6.655881404876709 | KNN Loss: 5.58988618850708 | BCE Loss: 1.0659953355789185\n",
      "Epoch 376 / 500 | iteration 0 / 30 | Total Loss: 6.658030986785889 | KNN Loss: 5.610962867736816 | BCE Loss: 1.0470681190490723\n",
      "Epoch 376 / 500 | iteration 5 / 30 | Total Loss: 6.664299964904785 | KNN Loss: 5.5987396240234375 | BCE Loss: 1.0655601024627686\n",
      "Epoch 376 / 500 | iteration 10 / 30 | Total Loss: 6.66860818862915 | KNN Loss: 5.624146938323975 | BCE Loss: 1.0444612503051758\n",
      "Epoch 376 / 500 | iteration 15 / 30 | Total Loss: 6.680675506591797 | KNN Loss: 5.628310203552246 | BCE Loss: 1.0523651838302612\n",
      "Epoch 376 / 500 | iteration 20 / 30 | Total Loss: 6.680349349975586 | KNN Loss: 5.597431182861328 | BCE Loss: 1.0829182863235474\n",
      "Epoch 376 / 500 | iteration 25 / 30 | Total Loss: 6.678406238555908 | KNN Loss: 5.615210056304932 | BCE Loss: 1.063196063041687\n",
      "Epoch 377 / 500 | iteration 0 / 30 | Total Loss: 6.6776885986328125 | KNN Loss: 5.616494655609131 | BCE Loss: 1.0611937046051025\n",
      "Epoch 377 / 500 | iteration 5 / 30 | Total Loss: 6.692803859710693 | KNN Loss: 5.622114181518555 | BCE Loss: 1.0706896781921387\n",
      "Epoch 377 / 500 | iteration 10 / 30 | Total Loss: 6.694267272949219 | KNN Loss: 5.664096832275391 | BCE Loss: 1.0301706790924072\n",
      "Epoch 377 / 500 | iteration 15 / 30 | Total Loss: 6.702243804931641 | KNN Loss: 5.658575057983398 | BCE Loss: 1.0436687469482422\n",
      "Epoch 377 / 500 | iteration 20 / 30 | Total Loss: 6.588618755340576 | KNN Loss: 5.590587615966797 | BCE Loss: 0.998030960559845\n",
      "Epoch 377 / 500 | iteration 25 / 30 | Total Loss: 6.671905994415283 | KNN Loss: 5.607361793518066 | BCE Loss: 1.0645442008972168\n",
      "Epoch 378 / 500 | iteration 0 / 30 | Total Loss: 6.657275199890137 | KNN Loss: 5.603521823883057 | BCE Loss: 1.053753137588501\n",
      "Epoch 378 / 500 | iteration 5 / 30 | Total Loss: 6.717219352722168 | KNN Loss: 5.670222282409668 | BCE Loss: 1.046997308731079\n",
      "Epoch 378 / 500 | iteration 10 / 30 | Total Loss: 6.715259552001953 | KNN Loss: 5.634042263031006 | BCE Loss: 1.0812171697616577\n",
      "Epoch 378 / 500 | iteration 15 / 30 | Total Loss: 6.6386566162109375 | KNN Loss: 5.604196071624756 | BCE Loss: 1.0344603061676025\n",
      "Epoch 378 / 500 | iteration 20 / 30 | Total Loss: 6.689115524291992 | KNN Loss: 5.647276401519775 | BCE Loss: 1.0418388843536377\n",
      "Epoch 378 / 500 | iteration 25 / 30 | Total Loss: 6.66156530380249 | KNN Loss: 5.626180171966553 | BCE Loss: 1.035385251045227\n",
      "Epoch 379 / 500 | iteration 0 / 30 | Total Loss: 6.614012718200684 | KNN Loss: 5.604418754577637 | BCE Loss: 1.009594202041626\n",
      "Epoch 379 / 500 | iteration 5 / 30 | Total Loss: 6.752736568450928 | KNN Loss: 5.69227933883667 | BCE Loss: 1.0604572296142578\n",
      "Epoch 379 / 500 | iteration 10 / 30 | Total Loss: 6.645105361938477 | KNN Loss: 5.602193355560303 | BCE Loss: 1.0429118871688843\n",
      "Epoch 379 / 500 | iteration 15 / 30 | Total Loss: 6.742188453674316 | KNN Loss: 5.7121076583862305 | BCE Loss: 1.0300805568695068\n",
      "Epoch 379 / 500 | iteration 20 / 30 | Total Loss: 6.7231903076171875 | KNN Loss: 5.6605424880981445 | BCE Loss: 1.062648057937622\n",
      "Epoch 379 / 500 | iteration 25 / 30 | Total Loss: 6.7952470779418945 | KNN Loss: 5.7445526123046875 | BCE Loss: 1.0506947040557861\n",
      "Epoch 380 / 500 | iteration 0 / 30 | Total Loss: 6.624102592468262 | KNN Loss: 5.598847389221191 | BCE Loss: 1.0252554416656494\n",
      "Epoch 380 / 500 | iteration 5 / 30 | Total Loss: 6.700902462005615 | KNN Loss: 5.631205081939697 | BCE Loss: 1.069697380065918\n",
      "Epoch 380 / 500 | iteration 10 / 30 | Total Loss: 6.622211933135986 | KNN Loss: 5.605454921722412 | BCE Loss: 1.0167568922042847\n",
      "Epoch 380 / 500 | iteration 15 / 30 | Total Loss: 6.651630401611328 | KNN Loss: 5.609745502471924 | BCE Loss: 1.0418851375579834\n",
      "Epoch 380 / 500 | iteration 20 / 30 | Total Loss: 6.669549942016602 | KNN Loss: 5.633456707000732 | BCE Loss: 1.03609299659729\n",
      "Epoch 380 / 500 | iteration 25 / 30 | Total Loss: 6.7648468017578125 | KNN Loss: 5.697256088256836 | BCE Loss: 1.0675909519195557\n",
      "Epoch   381: reducing learning rate of group 0 to 3.2856e-07.\n",
      "Epoch 381 / 500 | iteration 0 / 30 | Total Loss: 6.677557945251465 | KNN Loss: 5.628305912017822 | BCE Loss: 1.0492520332336426\n",
      "Epoch 381 / 500 | iteration 5 / 30 | Total Loss: 6.702877998352051 | KNN Loss: 5.627551078796387 | BCE Loss: 1.0753270387649536\n",
      "Epoch 381 / 500 | iteration 10 / 30 | Total Loss: 6.678016662597656 | KNN Loss: 5.620736122131348 | BCE Loss: 1.057280421257019\n",
      "Epoch 381 / 500 | iteration 15 / 30 | Total Loss: 6.670802116394043 | KNN Loss: 5.644750595092773 | BCE Loss: 1.0260517597198486\n",
      "Epoch 381 / 500 | iteration 20 / 30 | Total Loss: 6.649081230163574 | KNN Loss: 5.609437942504883 | BCE Loss: 1.0396430492401123\n",
      "Epoch 381 / 500 | iteration 25 / 30 | Total Loss: 6.716064453125 | KNN Loss: 5.683187961578369 | BCE Loss: 1.0328764915466309\n",
      "Epoch 382 / 500 | iteration 0 / 30 | Total Loss: 6.67204475402832 | KNN Loss: 5.642693519592285 | BCE Loss: 1.0293512344360352\n",
      "Epoch 382 / 500 | iteration 5 / 30 | Total Loss: 6.663975715637207 | KNN Loss: 5.62016487121582 | BCE Loss: 1.0438110828399658\n",
      "Epoch 382 / 500 | iteration 10 / 30 | Total Loss: 6.665956497192383 | KNN Loss: 5.638936519622803 | BCE Loss: 1.0270202159881592\n",
      "Epoch 382 / 500 | iteration 15 / 30 | Total Loss: 6.627621173858643 | KNN Loss: 5.605152130126953 | BCE Loss: 1.022469162940979\n",
      "Epoch 382 / 500 | iteration 20 / 30 | Total Loss: 6.694371223449707 | KNN Loss: 5.598184108734131 | BCE Loss: 1.096186876296997\n",
      "Epoch 382 / 500 | iteration 25 / 30 | Total Loss: 6.657266616821289 | KNN Loss: 5.618955135345459 | BCE Loss: 1.0383113622665405\n",
      "Epoch 383 / 500 | iteration 0 / 30 | Total Loss: 6.698703289031982 | KNN Loss: 5.685029983520508 | BCE Loss: 1.0136733055114746\n",
      "Epoch 383 / 500 | iteration 5 / 30 | Total Loss: 6.639964580535889 | KNN Loss: 5.601443290710449 | BCE Loss: 1.0385212898254395\n",
      "Epoch 383 / 500 | iteration 10 / 30 | Total Loss: 6.742337226867676 | KNN Loss: 5.6615495681762695 | BCE Loss: 1.0807878971099854\n",
      "Epoch 383 / 500 | iteration 15 / 30 | Total Loss: 6.653885841369629 | KNN Loss: 5.622832298278809 | BCE Loss: 1.0310537815093994\n",
      "Epoch 383 / 500 | iteration 20 / 30 | Total Loss: 6.644729137420654 | KNN Loss: 5.603519916534424 | BCE Loss: 1.0412092208862305\n",
      "Epoch 383 / 500 | iteration 25 / 30 | Total Loss: 6.6917405128479 | KNN Loss: 5.6216654777526855 | BCE Loss: 1.0700750350952148\n",
      "Epoch 384 / 500 | iteration 0 / 30 | Total Loss: 6.669345378875732 | KNN Loss: 5.625531196594238 | BCE Loss: 1.0438141822814941\n",
      "Epoch 384 / 500 | iteration 5 / 30 | Total Loss: 6.680775165557861 | KNN Loss: 5.644705295562744 | BCE Loss: 1.0360697507858276\n",
      "Epoch 384 / 500 | iteration 10 / 30 | Total Loss: 6.726217269897461 | KNN Loss: 5.66343879699707 | BCE Loss: 1.0627787113189697\n",
      "Epoch 384 / 500 | iteration 15 / 30 | Total Loss: 6.749330520629883 | KNN Loss: 5.70689058303833 | BCE Loss: 1.0424401760101318\n",
      "Epoch 384 / 500 | iteration 20 / 30 | Total Loss: 6.734327793121338 | KNN Loss: 5.66454553604126 | BCE Loss: 1.0697822570800781\n",
      "Epoch 384 / 500 | iteration 25 / 30 | Total Loss: 6.638300895690918 | KNN Loss: 5.601075172424316 | BCE Loss: 1.0372254848480225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 385 / 500 | iteration 0 / 30 | Total Loss: 6.785067558288574 | KNN Loss: 5.706675052642822 | BCE Loss: 1.0783923864364624\n",
      "Epoch 385 / 500 | iteration 5 / 30 | Total Loss: 6.8033294677734375 | KNN Loss: 5.736708164215088 | BCE Loss: 1.06662118434906\n",
      "Epoch 385 / 500 | iteration 10 / 30 | Total Loss: 6.664425849914551 | KNN Loss: 5.626609802246094 | BCE Loss: 1.0378159284591675\n",
      "Epoch 385 / 500 | iteration 15 / 30 | Total Loss: 6.6142425537109375 | KNN Loss: 5.601129055023193 | BCE Loss: 1.0131137371063232\n",
      "Epoch 385 / 500 | iteration 20 / 30 | Total Loss: 6.698152542114258 | KNN Loss: 5.66974401473999 | BCE Loss: 1.0284085273742676\n",
      "Epoch 385 / 500 | iteration 25 / 30 | Total Loss: 6.649499416351318 | KNN Loss: 5.591833591461182 | BCE Loss: 1.0576657056808472\n",
      "Epoch 386 / 500 | iteration 0 / 30 | Total Loss: 6.650694847106934 | KNN Loss: 5.595646381378174 | BCE Loss: 1.0550482273101807\n",
      "Epoch 386 / 500 | iteration 5 / 30 | Total Loss: 6.6485915184021 | KNN Loss: 5.59659481048584 | BCE Loss: 1.0519968271255493\n",
      "Epoch 386 / 500 | iteration 10 / 30 | Total Loss: 6.715629577636719 | KNN Loss: 5.666504859924316 | BCE Loss: 1.0491244792938232\n",
      "Epoch 386 / 500 | iteration 15 / 30 | Total Loss: 6.700986385345459 | KNN Loss: 5.650789260864258 | BCE Loss: 1.0501972436904907\n",
      "Epoch 386 / 500 | iteration 20 / 30 | Total Loss: 6.636476516723633 | KNN Loss: 5.599270343780518 | BCE Loss: 1.0372061729431152\n",
      "Epoch 386 / 500 | iteration 25 / 30 | Total Loss: 6.673923015594482 | KNN Loss: 5.633397102355957 | BCE Loss: 1.040526032447815\n",
      "Epoch 387 / 500 | iteration 0 / 30 | Total Loss: 6.645984172821045 | KNN Loss: 5.596599578857422 | BCE Loss: 1.0493844747543335\n",
      "Epoch 387 / 500 | iteration 5 / 30 | Total Loss: 6.641129493713379 | KNN Loss: 5.59988260269165 | BCE Loss: 1.0412466526031494\n",
      "Epoch 387 / 500 | iteration 10 / 30 | Total Loss: 6.662964344024658 | KNN Loss: 5.610514163970947 | BCE Loss: 1.052450180053711\n",
      "Epoch 387 / 500 | iteration 15 / 30 | Total Loss: 6.656989097595215 | KNN Loss: 5.598609447479248 | BCE Loss: 1.058379888534546\n",
      "Epoch 387 / 500 | iteration 20 / 30 | Total Loss: 6.684549331665039 | KNN Loss: 5.63912296295166 | BCE Loss: 1.045426368713379\n",
      "Epoch 387 / 500 | iteration 25 / 30 | Total Loss: 6.68458890914917 | KNN Loss: 5.6622209548950195 | BCE Loss: 1.0223678350448608\n",
      "Epoch 388 / 500 | iteration 0 / 30 | Total Loss: 6.735095500946045 | KNN Loss: 5.694783687591553 | BCE Loss: 1.0403119325637817\n",
      "Epoch 388 / 500 | iteration 5 / 30 | Total Loss: 6.623536109924316 | KNN Loss: 5.607134819030762 | BCE Loss: 1.0164012908935547\n",
      "Epoch 388 / 500 | iteration 10 / 30 | Total Loss: 6.638211250305176 | KNN Loss: 5.588342189788818 | BCE Loss: 1.049869179725647\n",
      "Epoch 388 / 500 | iteration 15 / 30 | Total Loss: 6.631046295166016 | KNN Loss: 5.614039421081543 | BCE Loss: 1.0170071125030518\n",
      "Epoch 388 / 500 | iteration 20 / 30 | Total Loss: 6.689957141876221 | KNN Loss: 5.651477813720703 | BCE Loss: 1.038479208946228\n",
      "Epoch 388 / 500 | iteration 25 / 30 | Total Loss: 6.699529647827148 | KNN Loss: 5.65472936630249 | BCE Loss: 1.0448002815246582\n",
      "Epoch 389 / 500 | iteration 0 / 30 | Total Loss: 6.694818496704102 | KNN Loss: 5.633912563323975 | BCE Loss: 1.0609060525894165\n",
      "Epoch 389 / 500 | iteration 5 / 30 | Total Loss: 6.629222869873047 | KNN Loss: 5.597295761108398 | BCE Loss: 1.0319271087646484\n",
      "Epoch 389 / 500 | iteration 10 / 30 | Total Loss: 6.659117698669434 | KNN Loss: 5.638014316558838 | BCE Loss: 1.0211031436920166\n",
      "Epoch 389 / 500 | iteration 15 / 30 | Total Loss: 6.676725387573242 | KNN Loss: 5.628364562988281 | BCE Loss: 1.0483609437942505\n",
      "Epoch 389 / 500 | iteration 20 / 30 | Total Loss: 6.733574867248535 | KNN Loss: 5.6871843338012695 | BCE Loss: 1.046390414237976\n",
      "Epoch 389 / 500 | iteration 25 / 30 | Total Loss: 6.652745246887207 | KNN Loss: 5.605915546417236 | BCE Loss: 1.0468294620513916\n",
      "Epoch 390 / 500 | iteration 0 / 30 | Total Loss: 6.657073020935059 | KNN Loss: 5.608253479003906 | BCE Loss: 1.0488197803497314\n",
      "Epoch 390 / 500 | iteration 5 / 30 | Total Loss: 6.644937515258789 | KNN Loss: 5.615942478179932 | BCE Loss: 1.0289952754974365\n",
      "Epoch 390 / 500 | iteration 10 / 30 | Total Loss: 6.607803821563721 | KNN Loss: 5.596774101257324 | BCE Loss: 1.0110297203063965\n",
      "Epoch 390 / 500 | iteration 15 / 30 | Total Loss: 6.637692451477051 | KNN Loss: 5.5969014167785645 | BCE Loss: 1.0407907962799072\n",
      "Epoch 390 / 500 | iteration 20 / 30 | Total Loss: 6.733430862426758 | KNN Loss: 5.665172576904297 | BCE Loss: 1.06825852394104\n",
      "Epoch 390 / 500 | iteration 25 / 30 | Total Loss: 6.624926567077637 | KNN Loss: 5.606431007385254 | BCE Loss: 1.0184953212738037\n",
      "Epoch 391 / 500 | iteration 0 / 30 | Total Loss: 6.682165622711182 | KNN Loss: 5.597151279449463 | BCE Loss: 1.0850143432617188\n",
      "Epoch 391 / 500 | iteration 5 / 30 | Total Loss: 6.669770240783691 | KNN Loss: 5.6360955238342285 | BCE Loss: 1.033674716949463\n",
      "Epoch 391 / 500 | iteration 10 / 30 | Total Loss: 6.655139923095703 | KNN Loss: 5.612566947937012 | BCE Loss: 1.0425727367401123\n",
      "Epoch 391 / 500 | iteration 15 / 30 | Total Loss: 6.648014068603516 | KNN Loss: 5.60603666305542 | BCE Loss: 1.0419774055480957\n",
      "Epoch 391 / 500 | iteration 20 / 30 | Total Loss: 6.6406378746032715 | KNN Loss: 5.59945821762085 | BCE Loss: 1.0411796569824219\n",
      "Epoch 391 / 500 | iteration 25 / 30 | Total Loss: 6.679184913635254 | KNN Loss: 5.634451389312744 | BCE Loss: 1.0447336435317993\n",
      "Epoch   392: reducing learning rate of group 0 to 2.2999e-07.\n",
      "Epoch 392 / 500 | iteration 0 / 30 | Total Loss: 6.674524307250977 | KNN Loss: 5.635880947113037 | BCE Loss: 1.0386433601379395\n",
      "Epoch 392 / 500 | iteration 5 / 30 | Total Loss: 6.7048139572143555 | KNN Loss: 5.674339771270752 | BCE Loss: 1.0304739475250244\n",
      "Epoch 392 / 500 | iteration 10 / 30 | Total Loss: 6.690467834472656 | KNN Loss: 5.625382423400879 | BCE Loss: 1.0650851726531982\n",
      "Epoch 392 / 500 | iteration 15 / 30 | Total Loss: 6.652508735656738 | KNN Loss: 5.592543125152588 | BCE Loss: 1.0599656105041504\n",
      "Epoch 392 / 500 | iteration 20 / 30 | Total Loss: 6.626323699951172 | KNN Loss: 5.59217643737793 | BCE Loss: 1.0341475009918213\n",
      "Epoch 392 / 500 | iteration 25 / 30 | Total Loss: 6.708767890930176 | KNN Loss: 5.64476203918457 | BCE Loss: 1.0640056133270264\n",
      "Epoch 393 / 500 | iteration 0 / 30 | Total Loss: 6.630112648010254 | KNN Loss: 5.589648246765137 | BCE Loss: 1.0404642820358276\n",
      "Epoch 393 / 500 | iteration 5 / 30 | Total Loss: 6.660765647888184 | KNN Loss: 5.616186618804932 | BCE Loss: 1.0445791482925415\n",
      "Epoch 393 / 500 | iteration 10 / 30 | Total Loss: 6.677257537841797 | KNN Loss: 5.598230361938477 | BCE Loss: 1.0790269374847412\n",
      "Epoch 393 / 500 | iteration 15 / 30 | Total Loss: 6.731832504272461 | KNN Loss: 5.672530651092529 | BCE Loss: 1.0593016147613525\n",
      "Epoch 393 / 500 | iteration 20 / 30 | Total Loss: 6.667546272277832 | KNN Loss: 5.631291389465332 | BCE Loss: 1.036255121231079\n",
      "Epoch 393 / 500 | iteration 25 / 30 | Total Loss: 6.634892463684082 | KNN Loss: 5.620522975921631 | BCE Loss: 1.0143694877624512\n",
      "Epoch 394 / 500 | iteration 0 / 30 | Total Loss: 6.638763904571533 | KNN Loss: 5.616997241973877 | BCE Loss: 1.0217667818069458\n",
      "Epoch 394 / 500 | iteration 5 / 30 | Total Loss: 6.661678314208984 | KNN Loss: 5.602936744689941 | BCE Loss: 1.0587413311004639\n",
      "Epoch 394 / 500 | iteration 10 / 30 | Total Loss: 6.614073276519775 | KNN Loss: 5.589926719665527 | BCE Loss: 1.0241464376449585\n",
      "Epoch 394 / 500 | iteration 15 / 30 | Total Loss: 6.706604957580566 | KNN Loss: 5.669549942016602 | BCE Loss: 1.0370547771453857\n",
      "Epoch 394 / 500 | iteration 20 / 30 | Total Loss: 6.637970924377441 | KNN Loss: 5.608673572540283 | BCE Loss: 1.0292973518371582\n",
      "Epoch 394 / 500 | iteration 25 / 30 | Total Loss: 6.695185661315918 | KNN Loss: 5.643400192260742 | BCE Loss: 1.0517853498458862\n",
      "Epoch 395 / 500 | iteration 0 / 30 | Total Loss: 6.645872592926025 | KNN Loss: 5.597192764282227 | BCE Loss: 1.0486798286437988\n",
      "Epoch 395 / 500 | iteration 5 / 30 | Total Loss: 6.644110679626465 | KNN Loss: 5.595139980316162 | BCE Loss: 1.0489705801010132\n",
      "Epoch 395 / 500 | iteration 10 / 30 | Total Loss: 6.638535499572754 | KNN Loss: 5.602152347564697 | BCE Loss: 1.0363833904266357\n",
      "Epoch 395 / 500 | iteration 15 / 30 | Total Loss: 6.700135231018066 | KNN Loss: 5.633333206176758 | BCE Loss: 1.0668020248413086\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 395 / 500 | iteration 20 / 30 | Total Loss: 6.674430847167969 | KNN Loss: 5.632808208465576 | BCE Loss: 1.0416227579116821\n",
      "Epoch 395 / 500 | iteration 25 / 30 | Total Loss: 6.726423263549805 | KNN Loss: 5.688623905181885 | BCE Loss: 1.037799596786499\n",
      "Epoch 396 / 500 | iteration 0 / 30 | Total Loss: 6.682547569274902 | KNN Loss: 5.6457085609436035 | BCE Loss: 1.0368390083312988\n",
      "Epoch 396 / 500 | iteration 5 / 30 | Total Loss: 6.681992530822754 | KNN Loss: 5.64169454574585 | BCE Loss: 1.0402981042861938\n",
      "Epoch 396 / 500 | iteration 10 / 30 | Total Loss: 6.680999755859375 | KNN Loss: 5.64704704284668 | BCE Loss: 1.0339527130126953\n",
      "Epoch 396 / 500 | iteration 15 / 30 | Total Loss: 6.698088645935059 | KNN Loss: 5.653799533843994 | BCE Loss: 1.0442888736724854\n",
      "Epoch 396 / 500 | iteration 20 / 30 | Total Loss: 6.665309429168701 | KNN Loss: 5.602972030639648 | BCE Loss: 1.0623373985290527\n",
      "Epoch 396 / 500 | iteration 25 / 30 | Total Loss: 6.674775123596191 | KNN Loss: 5.61165714263916 | BCE Loss: 1.0631177425384521\n",
      "Epoch 397 / 500 | iteration 0 / 30 | Total Loss: 6.658874988555908 | KNN Loss: 5.5964765548706055 | BCE Loss: 1.0623984336853027\n",
      "Epoch 397 / 500 | iteration 5 / 30 | Total Loss: 6.698357582092285 | KNN Loss: 5.658011436462402 | BCE Loss: 1.040346384048462\n",
      "Epoch 397 / 500 | iteration 10 / 30 | Total Loss: 6.625057697296143 | KNN Loss: 5.626030445098877 | BCE Loss: 0.9990273118019104\n",
      "Epoch 397 / 500 | iteration 15 / 30 | Total Loss: 6.678561210632324 | KNN Loss: 5.63338041305542 | BCE Loss: 1.0451807975769043\n",
      "Epoch 397 / 500 | iteration 20 / 30 | Total Loss: 6.688315391540527 | KNN Loss: 5.633852958679199 | BCE Loss: 1.0544624328613281\n",
      "Epoch 397 / 500 | iteration 25 / 30 | Total Loss: 6.618863105773926 | KNN Loss: 5.595290184020996 | BCE Loss: 1.0235729217529297\n",
      "Epoch 398 / 500 | iteration 0 / 30 | Total Loss: 6.656067848205566 | KNN Loss: 5.610567092895508 | BCE Loss: 1.0455009937286377\n",
      "Epoch 398 / 500 | iteration 5 / 30 | Total Loss: 6.692751884460449 | KNN Loss: 5.649925231933594 | BCE Loss: 1.0428264141082764\n",
      "Epoch 398 / 500 | iteration 10 / 30 | Total Loss: 6.608494758605957 | KNN Loss: 5.595622539520264 | BCE Loss: 1.012872338294983\n",
      "Epoch 398 / 500 | iteration 15 / 30 | Total Loss: 6.684957981109619 | KNN Loss: 5.630345821380615 | BCE Loss: 1.054612159729004\n",
      "Epoch 398 / 500 | iteration 20 / 30 | Total Loss: 6.7228617668151855 | KNN Loss: 5.655089855194092 | BCE Loss: 1.0677719116210938\n",
      "Epoch 398 / 500 | iteration 25 / 30 | Total Loss: 6.747265815734863 | KNN Loss: 5.704227447509766 | BCE Loss: 1.0430384874343872\n",
      "Epoch 399 / 500 | iteration 0 / 30 | Total Loss: 6.699635982513428 | KNN Loss: 5.64926290512085 | BCE Loss: 1.0503729581832886\n",
      "Epoch 399 / 500 | iteration 5 / 30 | Total Loss: 6.656247615814209 | KNN Loss: 5.621631622314453 | BCE Loss: 1.0346159934997559\n",
      "Epoch 399 / 500 | iteration 10 / 30 | Total Loss: 6.6708269119262695 | KNN Loss: 5.636009216308594 | BCE Loss: 1.0348174571990967\n",
      "Epoch 399 / 500 | iteration 15 / 30 | Total Loss: 6.673958778381348 | KNN Loss: 5.602365493774414 | BCE Loss: 1.071593165397644\n",
      "Epoch 399 / 500 | iteration 20 / 30 | Total Loss: 6.667349815368652 | KNN Loss: 5.614789962768555 | BCE Loss: 1.0525599718093872\n",
      "Epoch 399 / 500 | iteration 25 / 30 | Total Loss: 6.754741668701172 | KNN Loss: 5.68478536605835 | BCE Loss: 1.0699561834335327\n",
      "Epoch 400 / 500 | iteration 0 / 30 | Total Loss: 6.653251647949219 | KNN Loss: 5.600098609924316 | BCE Loss: 1.0531532764434814\n",
      "Epoch 400 / 500 | iteration 5 / 30 | Total Loss: 6.652771949768066 | KNN Loss: 5.608129978179932 | BCE Loss: 1.0446417331695557\n",
      "Epoch 400 / 500 | iteration 10 / 30 | Total Loss: 6.706790924072266 | KNN Loss: 5.668924808502197 | BCE Loss: 1.0378661155700684\n",
      "Epoch 400 / 500 | iteration 15 / 30 | Total Loss: 6.695202827453613 | KNN Loss: 5.655635833740234 | BCE Loss: 1.039567232131958\n",
      "Epoch 400 / 500 | iteration 20 / 30 | Total Loss: 6.659815311431885 | KNN Loss: 5.603009223937988 | BCE Loss: 1.056805968284607\n",
      "Epoch 400 / 500 | iteration 25 / 30 | Total Loss: 6.693271636962891 | KNN Loss: 5.634139060974121 | BCE Loss: 1.0591328144073486\n",
      "Epoch 401 / 500 | iteration 0 / 30 | Total Loss: 6.643393516540527 | KNN Loss: 5.598133087158203 | BCE Loss: 1.0452604293823242\n",
      "Epoch 401 / 500 | iteration 5 / 30 | Total Loss: 6.660080432891846 | KNN Loss: 5.60774564743042 | BCE Loss: 1.0523347854614258\n",
      "Epoch 401 / 500 | iteration 10 / 30 | Total Loss: 6.695294380187988 | KNN Loss: 5.662836074829102 | BCE Loss: 1.0324580669403076\n",
      "Epoch 401 / 500 | iteration 15 / 30 | Total Loss: 6.7043137550354 | KNN Loss: 5.674983501434326 | BCE Loss: 1.0293303728103638\n",
      "Epoch 401 / 500 | iteration 20 / 30 | Total Loss: 6.64860725402832 | KNN Loss: 5.600727558135986 | BCE Loss: 1.0478795766830444\n",
      "Epoch 401 / 500 | iteration 25 / 30 | Total Loss: 6.6688456535339355 | KNN Loss: 5.624051094055176 | BCE Loss: 1.0447945594787598\n",
      "Epoch 402 / 500 | iteration 0 / 30 | Total Loss: 6.739633083343506 | KNN Loss: 5.684399127960205 | BCE Loss: 1.0552340745925903\n",
      "Epoch 402 / 500 | iteration 5 / 30 | Total Loss: 6.673220634460449 | KNN Loss: 5.6549763679504395 | BCE Loss: 1.0182442665100098\n",
      "Epoch 402 / 500 | iteration 10 / 30 | Total Loss: 6.732302188873291 | KNN Loss: 5.682305812835693 | BCE Loss: 1.049996256828308\n",
      "Epoch 402 / 500 | iteration 15 / 30 | Total Loss: 6.685384750366211 | KNN Loss: 5.647139072418213 | BCE Loss: 1.0382455587387085\n",
      "Epoch 402 / 500 | iteration 20 / 30 | Total Loss: 6.654409408569336 | KNN Loss: 5.619345664978027 | BCE Loss: 1.0350635051727295\n",
      "Epoch 402 / 500 | iteration 25 / 30 | Total Loss: 6.66309118270874 | KNN Loss: 5.59511661529541 | BCE Loss: 1.06797456741333\n",
      "Epoch   403: reducing learning rate of group 0 to 1.6100e-07.\n",
      "Epoch 403 / 500 | iteration 0 / 30 | Total Loss: 6.679980754852295 | KNN Loss: 5.619335651397705 | BCE Loss: 1.0606451034545898\n",
      "Epoch 403 / 500 | iteration 5 / 30 | Total Loss: 6.611512184143066 | KNN Loss: 5.599740505218506 | BCE Loss: 1.0117716789245605\n",
      "Epoch 403 / 500 | iteration 10 / 30 | Total Loss: 6.669552326202393 | KNN Loss: 5.611648082733154 | BCE Loss: 1.0579042434692383\n",
      "Epoch 403 / 500 | iteration 15 / 30 | Total Loss: 6.691164970397949 | KNN Loss: 5.637595176696777 | BCE Loss: 1.0535695552825928\n",
      "Epoch 403 / 500 | iteration 20 / 30 | Total Loss: 6.645434379577637 | KNN Loss: 5.606667518615723 | BCE Loss: 1.038766622543335\n",
      "Epoch 403 / 500 | iteration 25 / 30 | Total Loss: 6.671939373016357 | KNN Loss: 5.600241661071777 | BCE Loss: 1.07169771194458\n",
      "Epoch 404 / 500 | iteration 0 / 30 | Total Loss: 6.668040752410889 | KNN Loss: 5.609766960144043 | BCE Loss: 1.0582737922668457\n",
      "Epoch 404 / 500 | iteration 5 / 30 | Total Loss: 6.695613861083984 | KNN Loss: 5.63466739654541 | BCE Loss: 1.0609467029571533\n",
      "Epoch 404 / 500 | iteration 10 / 30 | Total Loss: 6.635127067565918 | KNN Loss: 5.631772041320801 | BCE Loss: 1.0033549070358276\n",
      "Epoch 404 / 500 | iteration 15 / 30 | Total Loss: 6.669462203979492 | KNN Loss: 5.612006187438965 | BCE Loss: 1.0574560165405273\n",
      "Epoch 404 / 500 | iteration 20 / 30 | Total Loss: 6.6582441329956055 | KNN Loss: 5.633254528045654 | BCE Loss: 1.024989366531372\n",
      "Epoch 404 / 500 | iteration 25 / 30 | Total Loss: 6.639753818511963 | KNN Loss: 5.600913047790527 | BCE Loss: 1.038840889930725\n",
      "Epoch 405 / 500 | iteration 0 / 30 | Total Loss: 6.677520751953125 | KNN Loss: 5.594282150268555 | BCE Loss: 1.0832383632659912\n",
      "Epoch 405 / 500 | iteration 5 / 30 | Total Loss: 6.626945495605469 | KNN Loss: 5.596007347106934 | BCE Loss: 1.030937910079956\n",
      "Epoch 405 / 500 | iteration 10 / 30 | Total Loss: 6.65521240234375 | KNN Loss: 5.596585750579834 | BCE Loss: 1.058626413345337\n",
      "Epoch 405 / 500 | iteration 15 / 30 | Total Loss: 6.68137264251709 | KNN Loss: 5.619289875030518 | BCE Loss: 1.0620828866958618\n",
      "Epoch 405 / 500 | iteration 20 / 30 | Total Loss: 6.706415176391602 | KNN Loss: 5.6722517013549805 | BCE Loss: 1.0341633558273315\n",
      "Epoch 405 / 500 | iteration 25 / 30 | Total Loss: 6.729989528656006 | KNN Loss: 5.6663665771484375 | BCE Loss: 1.0636229515075684\n",
      "Epoch 406 / 500 | iteration 0 / 30 | Total Loss: 6.689577579498291 | KNN Loss: 5.605737209320068 | BCE Loss: 1.0838403701782227\n",
      "Epoch 406 / 500 | iteration 5 / 30 | Total Loss: 6.68005895614624 | KNN Loss: 5.630754470825195 | BCE Loss: 1.049304485321045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 406 / 500 | iteration 10 / 30 | Total Loss: 6.667544364929199 | KNN Loss: 5.611945629119873 | BCE Loss: 1.055598497390747\n",
      "Epoch 406 / 500 | iteration 15 / 30 | Total Loss: 6.681788444519043 | KNN Loss: 5.6136794090271 | BCE Loss: 1.0681092739105225\n",
      "Epoch 406 / 500 | iteration 20 / 30 | Total Loss: 6.682497978210449 | KNN Loss: 5.637839317321777 | BCE Loss: 1.0446586608886719\n",
      "Epoch 406 / 500 | iteration 25 / 30 | Total Loss: 6.687449932098389 | KNN Loss: 5.612906455993652 | BCE Loss: 1.0745434761047363\n",
      "Epoch 407 / 500 | iteration 0 / 30 | Total Loss: 6.674480438232422 | KNN Loss: 5.640607833862305 | BCE Loss: 1.0338727235794067\n",
      "Epoch 407 / 500 | iteration 5 / 30 | Total Loss: 6.705484390258789 | KNN Loss: 5.678563594818115 | BCE Loss: 1.026921033859253\n",
      "Epoch 407 / 500 | iteration 10 / 30 | Total Loss: 6.649173736572266 | KNN Loss: 5.591495513916016 | BCE Loss: 1.057677984237671\n",
      "Epoch 407 / 500 | iteration 15 / 30 | Total Loss: 6.700611114501953 | KNN Loss: 5.653326034545898 | BCE Loss: 1.0472848415374756\n",
      "Epoch 407 / 500 | iteration 20 / 30 | Total Loss: 6.630822658538818 | KNN Loss: 5.591883182525635 | BCE Loss: 1.0389394760131836\n",
      "Epoch 407 / 500 | iteration 25 / 30 | Total Loss: 6.687323570251465 | KNN Loss: 5.6340556144714355 | BCE Loss: 1.0532677173614502\n",
      "Epoch 408 / 500 | iteration 0 / 30 | Total Loss: 6.638826370239258 | KNN Loss: 5.5945916175842285 | BCE Loss: 1.0442349910736084\n",
      "Epoch 408 / 500 | iteration 5 / 30 | Total Loss: 6.722409248352051 | KNN Loss: 5.623144626617432 | BCE Loss: 1.0992647409439087\n",
      "Epoch 408 / 500 | iteration 10 / 30 | Total Loss: 6.644124507904053 | KNN Loss: 5.597416877746582 | BCE Loss: 1.0467075109481812\n",
      "Epoch 408 / 500 | iteration 15 / 30 | Total Loss: 6.685356140136719 | KNN Loss: 5.61794900894165 | BCE Loss: 1.0674071311950684\n",
      "Epoch 408 / 500 | iteration 20 / 30 | Total Loss: 6.6853508949279785 | KNN Loss: 5.628207206726074 | BCE Loss: 1.0571435689926147\n",
      "Epoch 408 / 500 | iteration 25 / 30 | Total Loss: 6.616239070892334 | KNN Loss: 5.595339775085449 | BCE Loss: 1.0208992958068848\n",
      "Epoch 409 / 500 | iteration 0 / 30 | Total Loss: 6.691484451293945 | KNN Loss: 5.638844013214111 | BCE Loss: 1.0526403188705444\n",
      "Epoch 409 / 500 | iteration 5 / 30 | Total Loss: 6.617031097412109 | KNN Loss: 5.5952558517456055 | BCE Loss: 1.021775484085083\n",
      "Epoch 409 / 500 | iteration 10 / 30 | Total Loss: 6.652204990386963 | KNN Loss: 5.601655006408691 | BCE Loss: 1.0505499839782715\n",
      "Epoch 409 / 500 | iteration 15 / 30 | Total Loss: 6.676131248474121 | KNN Loss: 5.612689971923828 | BCE Loss: 1.0634410381317139\n",
      "Epoch 409 / 500 | iteration 20 / 30 | Total Loss: 6.661398887634277 | KNN Loss: 5.598492622375488 | BCE Loss: 1.0629065036773682\n",
      "Epoch 409 / 500 | iteration 25 / 30 | Total Loss: 6.662247657775879 | KNN Loss: 5.60028076171875 | BCE Loss: 1.0619670152664185\n",
      "Epoch 410 / 500 | iteration 0 / 30 | Total Loss: 6.647619247436523 | KNN Loss: 5.602712631225586 | BCE Loss: 1.0449068546295166\n",
      "Epoch 410 / 500 | iteration 5 / 30 | Total Loss: 6.650816917419434 | KNN Loss: 5.593593597412109 | BCE Loss: 1.0572234392166138\n",
      "Epoch 410 / 500 | iteration 10 / 30 | Total Loss: 6.728217124938965 | KNN Loss: 5.656314373016357 | BCE Loss: 1.0719025135040283\n",
      "Epoch 410 / 500 | iteration 15 / 30 | Total Loss: 6.692107200622559 | KNN Loss: 5.645340442657471 | BCE Loss: 1.046766996383667\n",
      "Epoch 410 / 500 | iteration 20 / 30 | Total Loss: 6.642757892608643 | KNN Loss: 5.603082656860352 | BCE Loss: 1.0396753549575806\n",
      "Epoch 410 / 500 | iteration 25 / 30 | Total Loss: 6.6557416915893555 | KNN Loss: 5.596939563751221 | BCE Loss: 1.0588018894195557\n",
      "Epoch 411 / 500 | iteration 0 / 30 | Total Loss: 6.640382289886475 | KNN Loss: 5.610201358795166 | BCE Loss: 1.0301809310913086\n",
      "Epoch 411 / 500 | iteration 5 / 30 | Total Loss: 6.640617370605469 | KNN Loss: 5.61379861831665 | BCE Loss: 1.0268189907073975\n",
      "Epoch 411 / 500 | iteration 10 / 30 | Total Loss: 6.696343898773193 | KNN Loss: 5.640429973602295 | BCE Loss: 1.0559139251708984\n",
      "Epoch 411 / 500 | iteration 15 / 30 | Total Loss: 6.601005554199219 | KNN Loss: 5.593541622161865 | BCE Loss: 1.007464051246643\n",
      "Epoch 411 / 500 | iteration 20 / 30 | Total Loss: 6.659416198730469 | KNN Loss: 5.612207412719727 | BCE Loss: 1.047208547592163\n",
      "Epoch 411 / 500 | iteration 25 / 30 | Total Loss: 6.671835899353027 | KNN Loss: 5.5966291427612305 | BCE Loss: 1.075206995010376\n",
      "Epoch 412 / 500 | iteration 0 / 30 | Total Loss: 6.681595802307129 | KNN Loss: 5.641933917999268 | BCE Loss: 1.0396621227264404\n",
      "Epoch 412 / 500 | iteration 5 / 30 | Total Loss: 6.715742588043213 | KNN Loss: 5.644803524017334 | BCE Loss: 1.0709391832351685\n",
      "Epoch 412 / 500 | iteration 10 / 30 | Total Loss: 6.693805694580078 | KNN Loss: 5.639633655548096 | BCE Loss: 1.0541718006134033\n",
      "Epoch 412 / 500 | iteration 15 / 30 | Total Loss: 6.656839847564697 | KNN Loss: 5.61154842376709 | BCE Loss: 1.0452914237976074\n",
      "Epoch 412 / 500 | iteration 20 / 30 | Total Loss: 6.664698600769043 | KNN Loss: 5.620156764984131 | BCE Loss: 1.044541597366333\n",
      "Epoch 412 / 500 | iteration 25 / 30 | Total Loss: 6.662247657775879 | KNN Loss: 5.604736804962158 | BCE Loss: 1.0575110912322998\n",
      "Epoch 413 / 500 | iteration 0 / 30 | Total Loss: 6.6663737297058105 | KNN Loss: 5.629064559936523 | BCE Loss: 1.037309169769287\n",
      "Epoch 413 / 500 | iteration 5 / 30 | Total Loss: 6.758575439453125 | KNN Loss: 5.728776454925537 | BCE Loss: 1.0297991037368774\n",
      "Epoch 413 / 500 | iteration 10 / 30 | Total Loss: 6.65888786315918 | KNN Loss: 5.600430488586426 | BCE Loss: 1.058457374572754\n",
      "Epoch 413 / 500 | iteration 15 / 30 | Total Loss: 6.691524505615234 | KNN Loss: 5.620453357696533 | BCE Loss: 1.0710713863372803\n",
      "Epoch 413 / 500 | iteration 20 / 30 | Total Loss: 6.705402851104736 | KNN Loss: 5.626749515533447 | BCE Loss: 1.0786532163619995\n",
      "Epoch 413 / 500 | iteration 25 / 30 | Total Loss: 6.6406168937683105 | KNN Loss: 5.590557098388672 | BCE Loss: 1.0500599145889282\n",
      "Epoch   414: reducing learning rate of group 0 to 1.1270e-07.\n",
      "Epoch 414 / 500 | iteration 0 / 30 | Total Loss: 6.73974609375 | KNN Loss: 5.688144207000732 | BCE Loss: 1.051601767539978\n",
      "Epoch 414 / 500 | iteration 5 / 30 | Total Loss: 6.661736488342285 | KNN Loss: 5.602973937988281 | BCE Loss: 1.058762550354004\n",
      "Epoch 414 / 500 | iteration 10 / 30 | Total Loss: 6.666608810424805 | KNN Loss: 5.643822193145752 | BCE Loss: 1.0227868556976318\n",
      "Epoch 414 / 500 | iteration 15 / 30 | Total Loss: 6.642084121704102 | KNN Loss: 5.628889083862305 | BCE Loss: 1.0131947994232178\n",
      "Epoch 414 / 500 | iteration 20 / 30 | Total Loss: 6.679263591766357 | KNN Loss: 5.663204669952393 | BCE Loss: 1.0160589218139648\n",
      "Epoch 414 / 500 | iteration 25 / 30 | Total Loss: 6.655010223388672 | KNN Loss: 5.610727787017822 | BCE Loss: 1.0442824363708496\n",
      "Epoch 415 / 500 | iteration 0 / 30 | Total Loss: 6.703445911407471 | KNN Loss: 5.645792484283447 | BCE Loss: 1.0576533079147339\n",
      "Epoch 415 / 500 | iteration 5 / 30 | Total Loss: 6.690753936767578 | KNN Loss: 5.620452404022217 | BCE Loss: 1.0703017711639404\n",
      "Epoch 415 / 500 | iteration 10 / 30 | Total Loss: 6.623014450073242 | KNN Loss: 5.601790904998779 | BCE Loss: 1.021223545074463\n",
      "Epoch 415 / 500 | iteration 15 / 30 | Total Loss: 6.6722917556762695 | KNN Loss: 5.617660045623779 | BCE Loss: 1.0546314716339111\n",
      "Epoch 415 / 500 | iteration 20 / 30 | Total Loss: 6.6979522705078125 | KNN Loss: 5.640023231506348 | BCE Loss: 1.0579288005828857\n",
      "Epoch 415 / 500 | iteration 25 / 30 | Total Loss: 6.665412425994873 | KNN Loss: 5.6305999755859375 | BCE Loss: 1.034812569618225\n",
      "Epoch 416 / 500 | iteration 0 / 30 | Total Loss: 6.711512565612793 | KNN Loss: 5.650016784667969 | BCE Loss: 1.0614960193634033\n",
      "Epoch 416 / 500 | iteration 5 / 30 | Total Loss: 6.645484447479248 | KNN Loss: 5.617029190063477 | BCE Loss: 1.0284552574157715\n",
      "Epoch 416 / 500 | iteration 10 / 30 | Total Loss: 6.649692535400391 | KNN Loss: 5.616462707519531 | BCE Loss: 1.0332295894622803\n",
      "Epoch 416 / 500 | iteration 15 / 30 | Total Loss: 6.79692268371582 | KNN Loss: 5.718747138977051 | BCE Loss: 1.0781753063201904\n",
      "Epoch 416 / 500 | iteration 20 / 30 | Total Loss: 6.677425384521484 | KNN Loss: 5.630691051483154 | BCE Loss: 1.046734094619751\n",
      "Epoch 416 / 500 | iteration 25 / 30 | Total Loss: 6.7348504066467285 | KNN Loss: 5.6833720207214355 | BCE Loss: 1.051478385925293\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 417 / 500 | iteration 0 / 30 | Total Loss: 6.725886344909668 | KNN Loss: 5.654155731201172 | BCE Loss: 1.071730375289917\n",
      "Epoch 417 / 500 | iteration 5 / 30 | Total Loss: 6.694731712341309 | KNN Loss: 5.669042110443115 | BCE Loss: 1.0256898403167725\n",
      "Epoch 417 / 500 | iteration 10 / 30 | Total Loss: 6.68310546875 | KNN Loss: 5.6468682289123535 | BCE Loss: 1.0362374782562256\n",
      "Epoch 417 / 500 | iteration 15 / 30 | Total Loss: 6.662860870361328 | KNN Loss: 5.625968933105469 | BCE Loss: 1.036892056465149\n",
      "Epoch 417 / 500 | iteration 20 / 30 | Total Loss: 6.661364555358887 | KNN Loss: 5.608191013336182 | BCE Loss: 1.0531737804412842\n",
      "Epoch 417 / 500 | iteration 25 / 30 | Total Loss: 6.633981227874756 | KNN Loss: 5.59464168548584 | BCE Loss: 1.0393396615982056\n",
      "Epoch 418 / 500 | iteration 0 / 30 | Total Loss: 6.658901214599609 | KNN Loss: 5.598267078399658 | BCE Loss: 1.060633897781372\n",
      "Epoch 418 / 500 | iteration 5 / 30 | Total Loss: 6.655983924865723 | KNN Loss: 5.6329345703125 | BCE Loss: 1.0230493545532227\n",
      "Epoch 418 / 500 | iteration 10 / 30 | Total Loss: 6.686010360717773 | KNN Loss: 5.636658668518066 | BCE Loss: 1.049351692199707\n",
      "Epoch 418 / 500 | iteration 15 / 30 | Total Loss: 6.684405326843262 | KNN Loss: 5.612300395965576 | BCE Loss: 1.072105050086975\n",
      "Epoch 418 / 500 | iteration 20 / 30 | Total Loss: 6.771697998046875 | KNN Loss: 5.719651222229004 | BCE Loss: 1.052046775817871\n",
      "Epoch 418 / 500 | iteration 25 / 30 | Total Loss: 6.70206356048584 | KNN Loss: 5.645205497741699 | BCE Loss: 1.0568583011627197\n",
      "Epoch 419 / 500 | iteration 0 / 30 | Total Loss: 6.653143882751465 | KNN Loss: 5.602519989013672 | BCE Loss: 1.0506236553192139\n",
      "Epoch 419 / 500 | iteration 5 / 30 | Total Loss: 6.660665988922119 | KNN Loss: 5.604764461517334 | BCE Loss: 1.0559016466140747\n",
      "Epoch 419 / 500 | iteration 10 / 30 | Total Loss: 6.727136611938477 | KNN Loss: 5.649982929229736 | BCE Loss: 1.0771534442901611\n",
      "Epoch 419 / 500 | iteration 15 / 30 | Total Loss: 6.661837577819824 | KNN Loss: 5.620619773864746 | BCE Loss: 1.0412176847457886\n",
      "Epoch 419 / 500 | iteration 20 / 30 | Total Loss: 6.658801078796387 | KNN Loss: 5.608423233032227 | BCE Loss: 1.0503778457641602\n",
      "Epoch 419 / 500 | iteration 25 / 30 | Total Loss: 6.695781707763672 | KNN Loss: 5.638963222503662 | BCE Loss: 1.0568187236785889\n",
      "Epoch 420 / 500 | iteration 0 / 30 | Total Loss: 6.69357442855835 | KNN Loss: 5.6371965408325195 | BCE Loss: 1.0563777685165405\n",
      "Epoch 420 / 500 | iteration 5 / 30 | Total Loss: 6.641067028045654 | KNN Loss: 5.600494384765625 | BCE Loss: 1.0405726432800293\n",
      "Epoch 420 / 500 | iteration 10 / 30 | Total Loss: 6.6469035148620605 | KNN Loss: 5.60152530670166 | BCE Loss: 1.04537832736969\n",
      "Epoch 420 / 500 | iteration 15 / 30 | Total Loss: 6.659347057342529 | KNN Loss: 5.614802837371826 | BCE Loss: 1.0445441007614136\n",
      "Epoch 420 / 500 | iteration 20 / 30 | Total Loss: 6.638087272644043 | KNN Loss: 5.5913896560668945 | BCE Loss: 1.046697735786438\n",
      "Epoch 420 / 500 | iteration 25 / 30 | Total Loss: 6.63702392578125 | KNN Loss: 5.617716312408447 | BCE Loss: 1.0193073749542236\n",
      "Epoch 421 / 500 | iteration 0 / 30 | Total Loss: 6.806313991546631 | KNN Loss: 5.719841480255127 | BCE Loss: 1.0864726305007935\n",
      "Epoch 421 / 500 | iteration 5 / 30 | Total Loss: 6.669677734375 | KNN Loss: 5.631453990936279 | BCE Loss: 1.0382237434387207\n",
      "Epoch 421 / 500 | iteration 10 / 30 | Total Loss: 6.713083267211914 | KNN Loss: 5.643024921417236 | BCE Loss: 1.0700581073760986\n",
      "Epoch 421 / 500 | iteration 15 / 30 | Total Loss: 6.696433067321777 | KNN Loss: 5.634438514709473 | BCE Loss: 1.0619943141937256\n",
      "Epoch 421 / 500 | iteration 20 / 30 | Total Loss: 6.651429176330566 | KNN Loss: 5.606380939483643 | BCE Loss: 1.0450482368469238\n",
      "Epoch 421 / 500 | iteration 25 / 30 | Total Loss: 6.709228038787842 | KNN Loss: 5.630388259887695 | BCE Loss: 1.078839898109436\n",
      "Epoch 422 / 500 | iteration 0 / 30 | Total Loss: 6.633743762969971 | KNN Loss: 5.594369888305664 | BCE Loss: 1.039373755455017\n",
      "Epoch 422 / 500 | iteration 5 / 30 | Total Loss: 6.675384521484375 | KNN Loss: 5.6063432693481445 | BCE Loss: 1.0690412521362305\n",
      "Epoch 422 / 500 | iteration 10 / 30 | Total Loss: 6.712165832519531 | KNN Loss: 5.649720191955566 | BCE Loss: 1.062445878982544\n",
      "Epoch 422 / 500 | iteration 15 / 30 | Total Loss: 6.670041084289551 | KNN Loss: 5.625046253204346 | BCE Loss: 1.044994592666626\n",
      "Epoch 422 / 500 | iteration 20 / 30 | Total Loss: 6.679040908813477 | KNN Loss: 5.603165149688721 | BCE Loss: 1.0758756399154663\n",
      "Epoch 422 / 500 | iteration 25 / 30 | Total Loss: 6.607285976409912 | KNN Loss: 5.596261501312256 | BCE Loss: 1.0110245943069458\n",
      "Epoch 423 / 500 | iteration 0 / 30 | Total Loss: 6.665647506713867 | KNN Loss: 5.618459701538086 | BCE Loss: 1.0471880435943604\n",
      "Epoch 423 / 500 | iteration 5 / 30 | Total Loss: 6.698463439941406 | KNN Loss: 5.651010990142822 | BCE Loss: 1.0474523305892944\n",
      "Epoch 423 / 500 | iteration 10 / 30 | Total Loss: 6.7017059326171875 | KNN Loss: 5.648073196411133 | BCE Loss: 1.0536327362060547\n",
      "Epoch 423 / 500 | iteration 15 / 30 | Total Loss: 6.619931221008301 | KNN Loss: 5.607097625732422 | BCE Loss: 1.012833595275879\n",
      "Epoch 423 / 500 | iteration 20 / 30 | Total Loss: 6.659263610839844 | KNN Loss: 5.6168212890625 | BCE Loss: 1.0424425601959229\n",
      "Epoch 423 / 500 | iteration 25 / 30 | Total Loss: 6.653443336486816 | KNN Loss: 5.59727144241333 | BCE Loss: 1.0561721324920654\n",
      "Epoch 424 / 500 | iteration 0 / 30 | Total Loss: 6.6684041023254395 | KNN Loss: 5.613635540008545 | BCE Loss: 1.0547685623168945\n",
      "Epoch 424 / 500 | iteration 5 / 30 | Total Loss: 6.675048351287842 | KNN Loss: 5.606258392333984 | BCE Loss: 1.0687899589538574\n",
      "Epoch 424 / 500 | iteration 10 / 30 | Total Loss: 6.641549587249756 | KNN Loss: 5.595270156860352 | BCE Loss: 1.0462794303894043\n",
      "Epoch 424 / 500 | iteration 15 / 30 | Total Loss: 6.653841018676758 | KNN Loss: 5.605169773101807 | BCE Loss: 1.0486712455749512\n",
      "Epoch 424 / 500 | iteration 20 / 30 | Total Loss: 6.674482822418213 | KNN Loss: 5.6138787269592285 | BCE Loss: 1.060604214668274\n",
      "Epoch 424 / 500 | iteration 25 / 30 | Total Loss: 6.675111293792725 | KNN Loss: 5.640266418457031 | BCE Loss: 1.0348448753356934\n",
      "Epoch 425 / 500 | iteration 0 / 30 | Total Loss: 6.667768478393555 | KNN Loss: 5.594059944152832 | BCE Loss: 1.0737085342407227\n",
      "Epoch 425 / 500 | iteration 5 / 30 | Total Loss: 6.681840896606445 | KNN Loss: 5.645096778869629 | BCE Loss: 1.0367443561553955\n",
      "Epoch 425 / 500 | iteration 10 / 30 | Total Loss: 6.6744184494018555 | KNN Loss: 5.656050205230713 | BCE Loss: 1.0183680057525635\n",
      "Epoch 425 / 500 | iteration 15 / 30 | Total Loss: 6.699723243713379 | KNN Loss: 5.641577243804932 | BCE Loss: 1.0581462383270264\n",
      "Epoch 425 / 500 | iteration 20 / 30 | Total Loss: 6.641911506652832 | KNN Loss: 5.6119279861450195 | BCE Loss: 1.0299837589263916\n",
      "Epoch 425 / 500 | iteration 25 / 30 | Total Loss: 6.624764442443848 | KNN Loss: 5.596229553222656 | BCE Loss: 1.0285348892211914\n",
      "Epoch 426 / 500 | iteration 0 / 30 | Total Loss: 6.7327423095703125 | KNN Loss: 5.72248649597168 | BCE Loss: 1.0102558135986328\n",
      "Epoch 426 / 500 | iteration 5 / 30 | Total Loss: 6.779562473297119 | KNN Loss: 5.727536201477051 | BCE Loss: 1.0520262718200684\n",
      "Epoch 426 / 500 | iteration 10 / 30 | Total Loss: 6.685129165649414 | KNN Loss: 5.637025356292725 | BCE Loss: 1.0481035709381104\n",
      "Epoch 426 / 500 | iteration 15 / 30 | Total Loss: 6.656691074371338 | KNN Loss: 5.615732669830322 | BCE Loss: 1.0409585237503052\n",
      "Epoch 426 / 500 | iteration 20 / 30 | Total Loss: 6.726966857910156 | KNN Loss: 5.691005706787109 | BCE Loss: 1.0359612703323364\n",
      "Epoch 426 / 500 | iteration 25 / 30 | Total Loss: 6.716222763061523 | KNN Loss: 5.69625997543335 | BCE Loss: 1.019963026046753\n",
      "Epoch 427 / 500 | iteration 0 / 30 | Total Loss: 6.64085054397583 | KNN Loss: 5.6111650466918945 | BCE Loss: 1.029685378074646\n",
      "Epoch 427 / 500 | iteration 5 / 30 | Total Loss: 6.683024883270264 | KNN Loss: 5.664966106414795 | BCE Loss: 1.0180588960647583\n",
      "Epoch 427 / 500 | iteration 10 / 30 | Total Loss: 6.739751815795898 | KNN Loss: 5.678133487701416 | BCE Loss: 1.0616185665130615\n",
      "Epoch 427 / 500 | iteration 15 / 30 | Total Loss: 6.667624473571777 | KNN Loss: 5.619301795959473 | BCE Loss: 1.0483226776123047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 427 / 500 | iteration 20 / 30 | Total Loss: 6.676930904388428 | KNN Loss: 5.6487202644348145 | BCE Loss: 1.0282105207443237\n",
      "Epoch 427 / 500 | iteration 25 / 30 | Total Loss: 6.688145637512207 | KNN Loss: 5.639692306518555 | BCE Loss: 1.0484533309936523\n",
      "Epoch 428 / 500 | iteration 0 / 30 | Total Loss: 6.681848526000977 | KNN Loss: 5.622967720031738 | BCE Loss: 1.0588806867599487\n",
      "Epoch 428 / 500 | iteration 5 / 30 | Total Loss: 6.642606735229492 | KNN Loss: 5.616644382476807 | BCE Loss: 1.0259621143341064\n",
      "Epoch 428 / 500 | iteration 10 / 30 | Total Loss: 6.659778594970703 | KNN Loss: 5.6185407638549805 | BCE Loss: 1.0412378311157227\n",
      "Epoch 428 / 500 | iteration 15 / 30 | Total Loss: 6.622081756591797 | KNN Loss: 5.602856636047363 | BCE Loss: 1.0192253589630127\n",
      "Epoch 428 / 500 | iteration 20 / 30 | Total Loss: 6.61286735534668 | KNN Loss: 5.591809272766113 | BCE Loss: 1.0210579633712769\n",
      "Epoch 428 / 500 | iteration 25 / 30 | Total Loss: 6.680312633514404 | KNN Loss: 5.598023414611816 | BCE Loss: 1.0822890996932983\n",
      "Epoch 429 / 500 | iteration 0 / 30 | Total Loss: 6.640903472900391 | KNN Loss: 5.6089887619018555 | BCE Loss: 1.0319145917892456\n",
      "Epoch 429 / 500 | iteration 5 / 30 | Total Loss: 6.665745735168457 | KNN Loss: 5.598801612854004 | BCE Loss: 1.0669442415237427\n",
      "Epoch 429 / 500 | iteration 10 / 30 | Total Loss: 6.656258583068848 | KNN Loss: 5.60769510269165 | BCE Loss: 1.0485632419586182\n",
      "Epoch 429 / 500 | iteration 15 / 30 | Total Loss: 6.653735160827637 | KNN Loss: 5.608781337738037 | BCE Loss: 1.0449535846710205\n",
      "Epoch 429 / 500 | iteration 20 / 30 | Total Loss: 6.769094467163086 | KNN Loss: 5.7111639976501465 | BCE Loss: 1.0579307079315186\n",
      "Epoch 429 / 500 | iteration 25 / 30 | Total Loss: 6.640564918518066 | KNN Loss: 5.6129326820373535 | BCE Loss: 1.0276323556900024\n",
      "Epoch 430 / 500 | iteration 0 / 30 | Total Loss: 6.644587993621826 | KNN Loss: 5.607422351837158 | BCE Loss: 1.0371657609939575\n",
      "Epoch 430 / 500 | iteration 5 / 30 | Total Loss: 6.713366985321045 | KNN Loss: 5.650614261627197 | BCE Loss: 1.0627527236938477\n",
      "Epoch 430 / 500 | iteration 10 / 30 | Total Loss: 6.688157558441162 | KNN Loss: 5.643563747406006 | BCE Loss: 1.0445939302444458\n",
      "Epoch 430 / 500 | iteration 15 / 30 | Total Loss: 6.626526832580566 | KNN Loss: 5.593427658081055 | BCE Loss: 1.0330992937088013\n",
      "Epoch 430 / 500 | iteration 20 / 30 | Total Loss: 6.643084526062012 | KNN Loss: 5.5955681800842285 | BCE Loss: 1.0475164651870728\n",
      "Epoch 430 / 500 | iteration 25 / 30 | Total Loss: 6.67453145980835 | KNN Loss: 5.601149082183838 | BCE Loss: 1.0733823776245117\n",
      "Epoch 431 / 500 | iteration 0 / 30 | Total Loss: 6.673521518707275 | KNN Loss: 5.6117424964904785 | BCE Loss: 1.0617791414260864\n",
      "Epoch 431 / 500 | iteration 5 / 30 | Total Loss: 6.695497512817383 | KNN Loss: 5.651968479156494 | BCE Loss: 1.0435292720794678\n",
      "Epoch 431 / 500 | iteration 10 / 30 | Total Loss: 6.642590045928955 | KNN Loss: 5.602833271026611 | BCE Loss: 1.0397567749023438\n",
      "Epoch 431 / 500 | iteration 15 / 30 | Total Loss: 6.722603797912598 | KNN Loss: 5.637670993804932 | BCE Loss: 1.084932804107666\n",
      "Epoch 431 / 500 | iteration 20 / 30 | Total Loss: 6.692216396331787 | KNN Loss: 5.632593631744385 | BCE Loss: 1.0596227645874023\n",
      "Epoch 431 / 500 | iteration 25 / 30 | Total Loss: 6.758841037750244 | KNN Loss: 5.684048652648926 | BCE Loss: 1.074792504310608\n",
      "Epoch 432 / 500 | iteration 0 / 30 | Total Loss: 6.693051338195801 | KNN Loss: 5.635947227478027 | BCE Loss: 1.0571038722991943\n",
      "Epoch 432 / 500 | iteration 5 / 30 | Total Loss: 6.666261672973633 | KNN Loss: 5.595703601837158 | BCE Loss: 1.0705580711364746\n",
      "Epoch 432 / 500 | iteration 10 / 30 | Total Loss: 6.688342094421387 | KNN Loss: 5.656659126281738 | BCE Loss: 1.0316827297210693\n",
      "Epoch 432 / 500 | iteration 15 / 30 | Total Loss: 6.668021202087402 | KNN Loss: 5.600166320800781 | BCE Loss: 1.0678551197052002\n",
      "Epoch 432 / 500 | iteration 20 / 30 | Total Loss: 6.664173126220703 | KNN Loss: 5.608323574066162 | BCE Loss: 1.0558497905731201\n",
      "Epoch 432 / 500 | iteration 25 / 30 | Total Loss: 6.698222637176514 | KNN Loss: 5.653855323791504 | BCE Loss: 1.0443673133850098\n",
      "Epoch 433 / 500 | iteration 0 / 30 | Total Loss: 6.689898490905762 | KNN Loss: 5.6247453689575195 | BCE Loss: 1.0651533603668213\n",
      "Epoch 433 / 500 | iteration 5 / 30 | Total Loss: 6.6508469581604 | KNN Loss: 5.606565475463867 | BCE Loss: 1.0442814826965332\n",
      "Epoch 433 / 500 | iteration 10 / 30 | Total Loss: 6.661995887756348 | KNN Loss: 5.591250896453857 | BCE Loss: 1.0707447528839111\n",
      "Epoch 433 / 500 | iteration 15 / 30 | Total Loss: 6.683172225952148 | KNN Loss: 5.62943172454834 | BCE Loss: 1.0537407398223877\n",
      "Epoch 433 / 500 | iteration 20 / 30 | Total Loss: 6.642366409301758 | KNN Loss: 5.625825881958008 | BCE Loss: 1.016540765762329\n",
      "Epoch 433 / 500 | iteration 25 / 30 | Total Loss: 6.673584461212158 | KNN Loss: 5.627265453338623 | BCE Loss: 1.0463190078735352\n",
      "Epoch   434: reducing learning rate of group 0 to 7.8888e-08.\n",
      "Epoch 434 / 500 | iteration 0 / 30 | Total Loss: 6.628183364868164 | KNN Loss: 5.596935272216797 | BCE Loss: 1.031247854232788\n",
      "Epoch 434 / 500 | iteration 5 / 30 | Total Loss: 6.671334266662598 | KNN Loss: 5.611165523529053 | BCE Loss: 1.0601685047149658\n",
      "Epoch 434 / 500 | iteration 10 / 30 | Total Loss: 6.677217483520508 | KNN Loss: 5.594216346740723 | BCE Loss: 1.0830013751983643\n",
      "Epoch 434 / 500 | iteration 15 / 30 | Total Loss: 6.677206993103027 | KNN Loss: 5.621353626251221 | BCE Loss: 1.055853247642517\n",
      "Epoch 434 / 500 | iteration 20 / 30 | Total Loss: 6.645619869232178 | KNN Loss: 5.6043291091918945 | BCE Loss: 1.0412907600402832\n",
      "Epoch 434 / 500 | iteration 25 / 30 | Total Loss: 6.718418121337891 | KNN Loss: 5.641941070556641 | BCE Loss: 1.07647705078125\n",
      "Epoch 435 / 500 | iteration 0 / 30 | Total Loss: 6.642417907714844 | KNN Loss: 5.597465515136719 | BCE Loss: 1.044952154159546\n",
      "Epoch 435 / 500 | iteration 5 / 30 | Total Loss: 6.683472633361816 | KNN Loss: 5.62393856048584 | BCE Loss: 1.059533953666687\n",
      "Epoch 435 / 500 | iteration 10 / 30 | Total Loss: 6.685878753662109 | KNN Loss: 5.654099464416504 | BCE Loss: 1.0317792892456055\n",
      "Epoch 435 / 500 | iteration 15 / 30 | Total Loss: 6.645041465759277 | KNN Loss: 5.604864597320557 | BCE Loss: 1.0401766300201416\n",
      "Epoch 435 / 500 | iteration 20 / 30 | Total Loss: 6.666245937347412 | KNN Loss: 5.606417179107666 | BCE Loss: 1.0598286390304565\n",
      "Epoch 435 / 500 | iteration 25 / 30 | Total Loss: 6.692744255065918 | KNN Loss: 5.626675128936768 | BCE Loss: 1.0660693645477295\n",
      "Epoch 436 / 500 | iteration 0 / 30 | Total Loss: 6.751951217651367 | KNN Loss: 5.7300286293029785 | BCE Loss: 1.0219228267669678\n",
      "Epoch 436 / 500 | iteration 5 / 30 | Total Loss: 6.684767246246338 | KNN Loss: 5.634138107299805 | BCE Loss: 1.0506291389465332\n",
      "Epoch 436 / 500 | iteration 10 / 30 | Total Loss: 6.632869243621826 | KNN Loss: 5.600384712219238 | BCE Loss: 1.0324846506118774\n",
      "Epoch 436 / 500 | iteration 15 / 30 | Total Loss: 6.684145927429199 | KNN Loss: 5.6589508056640625 | BCE Loss: 1.0251948833465576\n",
      "Epoch 436 / 500 | iteration 20 / 30 | Total Loss: 6.708446502685547 | KNN Loss: 5.6432695388793945 | BCE Loss: 1.0651772022247314\n",
      "Epoch 436 / 500 | iteration 25 / 30 | Total Loss: 6.677596569061279 | KNN Loss: 5.63949728012085 | BCE Loss: 1.0380994081497192\n",
      "Epoch 437 / 500 | iteration 0 / 30 | Total Loss: 6.747100830078125 | KNN Loss: 5.719000816345215 | BCE Loss: 1.028099775314331\n",
      "Epoch 437 / 500 | iteration 5 / 30 | Total Loss: 6.719839572906494 | KNN Loss: 5.674178123474121 | BCE Loss: 1.045661449432373\n",
      "Epoch 437 / 500 | iteration 10 / 30 | Total Loss: 6.714149475097656 | KNN Loss: 5.690969944000244 | BCE Loss: 1.0231797695159912\n",
      "Epoch 437 / 500 | iteration 15 / 30 | Total Loss: 6.646170139312744 | KNN Loss: 5.598458766937256 | BCE Loss: 1.0477113723754883\n",
      "Epoch 437 / 500 | iteration 20 / 30 | Total Loss: 6.688459396362305 | KNN Loss: 5.667551517486572 | BCE Loss: 1.0209076404571533\n",
      "Epoch 437 / 500 | iteration 25 / 30 | Total Loss: 6.633211135864258 | KNN Loss: 5.589598178863525 | BCE Loss: 1.0436131954193115\n",
      "Epoch 438 / 500 | iteration 0 / 30 | Total Loss: 6.7015886306762695 | KNN Loss: 5.669559955596924 | BCE Loss: 1.0320286750793457\n",
      "Epoch 438 / 500 | iteration 5 / 30 | Total Loss: 6.695866584777832 | KNN Loss: 5.616184711456299 | BCE Loss: 1.079681634902954\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 438 / 500 | iteration 10 / 30 | Total Loss: 6.641079902648926 | KNN Loss: 5.6287102699279785 | BCE Loss: 1.0123693943023682\n",
      "Epoch 438 / 500 | iteration 15 / 30 | Total Loss: 6.644201278686523 | KNN Loss: 5.606711387634277 | BCE Loss: 1.037489652633667\n",
      "Epoch 438 / 500 | iteration 20 / 30 | Total Loss: 6.706498622894287 | KNN Loss: 5.660494804382324 | BCE Loss: 1.046003818511963\n",
      "Epoch 438 / 500 | iteration 25 / 30 | Total Loss: 6.643157958984375 | KNN Loss: 5.606011390686035 | BCE Loss: 1.0371466875076294\n",
      "Epoch 439 / 500 | iteration 0 / 30 | Total Loss: 6.699958801269531 | KNN Loss: 5.663671970367432 | BCE Loss: 1.0362865924835205\n",
      "Epoch 439 / 500 | iteration 5 / 30 | Total Loss: 6.73884391784668 | KNN Loss: 5.681560039520264 | BCE Loss: 1.0572839975357056\n",
      "Epoch 439 / 500 | iteration 10 / 30 | Total Loss: 6.711134910583496 | KNN Loss: 5.643615245819092 | BCE Loss: 1.0675196647644043\n",
      "Epoch 439 / 500 | iteration 15 / 30 | Total Loss: 6.686648845672607 | KNN Loss: 5.633330345153809 | BCE Loss: 1.0533185005187988\n",
      "Epoch 439 / 500 | iteration 20 / 30 | Total Loss: 6.62522029876709 | KNN Loss: 5.594218730926514 | BCE Loss: 1.0310016870498657\n",
      "Epoch 439 / 500 | iteration 25 / 30 | Total Loss: 6.668301105499268 | KNN Loss: 5.621800422668457 | BCE Loss: 1.0465006828308105\n",
      "Epoch 440 / 500 | iteration 0 / 30 | Total Loss: 6.686928749084473 | KNN Loss: 5.654242515563965 | BCE Loss: 1.0326859951019287\n",
      "Epoch 440 / 500 | iteration 5 / 30 | Total Loss: 6.666900634765625 | KNN Loss: 5.591170310974121 | BCE Loss: 1.075730562210083\n",
      "Epoch 440 / 500 | iteration 10 / 30 | Total Loss: 6.663438320159912 | KNN Loss: 5.638760089874268 | BCE Loss: 1.024678111076355\n",
      "Epoch 440 / 500 | iteration 15 / 30 | Total Loss: 6.676388740539551 | KNN Loss: 5.6175007820129395 | BCE Loss: 1.0588881969451904\n",
      "Epoch 440 / 500 | iteration 20 / 30 | Total Loss: 6.664230823516846 | KNN Loss: 5.619655132293701 | BCE Loss: 1.0445756912231445\n",
      "Epoch 440 / 500 | iteration 25 / 30 | Total Loss: 6.746410369873047 | KNN Loss: 5.681324481964111 | BCE Loss: 1.0650861263275146\n",
      "Epoch 441 / 500 | iteration 0 / 30 | Total Loss: 6.650491714477539 | KNN Loss: 5.590866565704346 | BCE Loss: 1.0596253871917725\n",
      "Epoch 441 / 500 | iteration 5 / 30 | Total Loss: 6.661538124084473 | KNN Loss: 5.612437725067139 | BCE Loss: 1.049100637435913\n",
      "Epoch 441 / 500 | iteration 10 / 30 | Total Loss: 6.756717681884766 | KNN Loss: 5.693795680999756 | BCE Loss: 1.0629220008850098\n",
      "Epoch 441 / 500 | iteration 15 / 30 | Total Loss: 6.690402984619141 | KNN Loss: 5.651298999786377 | BCE Loss: 1.0391041040420532\n",
      "Epoch 441 / 500 | iteration 20 / 30 | Total Loss: 6.70485782623291 | KNN Loss: 5.6315412521362305 | BCE Loss: 1.0733163356781006\n",
      "Epoch 441 / 500 | iteration 25 / 30 | Total Loss: 6.727980136871338 | KNN Loss: 5.679460525512695 | BCE Loss: 1.0485196113586426\n",
      "Epoch 442 / 500 | iteration 0 / 30 | Total Loss: 6.7127838134765625 | KNN Loss: 5.659949779510498 | BCE Loss: 1.052833914756775\n",
      "Epoch 442 / 500 | iteration 5 / 30 | Total Loss: 6.644221305847168 | KNN Loss: 5.593245983123779 | BCE Loss: 1.0509755611419678\n",
      "Epoch 442 / 500 | iteration 10 / 30 | Total Loss: 6.654323101043701 | KNN Loss: 5.6017279624938965 | BCE Loss: 1.0525950193405151\n",
      "Epoch 442 / 500 | iteration 15 / 30 | Total Loss: 6.692504405975342 | KNN Loss: 5.640978813171387 | BCE Loss: 1.051525592803955\n",
      "Epoch 442 / 500 | iteration 20 / 30 | Total Loss: 6.669373512268066 | KNN Loss: 5.653273105621338 | BCE Loss: 1.0161006450653076\n",
      "Epoch 442 / 500 | iteration 25 / 30 | Total Loss: 6.638139724731445 | KNN Loss: 5.60527229309082 | BCE Loss: 1.032867431640625\n",
      "Epoch 443 / 500 | iteration 0 / 30 | Total Loss: 6.658140182495117 | KNN Loss: 5.611087799072266 | BCE Loss: 1.0470525026321411\n",
      "Epoch 443 / 500 | iteration 5 / 30 | Total Loss: 6.669039726257324 | KNN Loss: 5.6014790534973145 | BCE Loss: 1.0675606727600098\n",
      "Epoch 443 / 500 | iteration 10 / 30 | Total Loss: 6.658982276916504 | KNN Loss: 5.600419044494629 | BCE Loss: 1.058563232421875\n",
      "Epoch 443 / 500 | iteration 15 / 30 | Total Loss: 6.648327827453613 | KNN Loss: 5.5979156494140625 | BCE Loss: 1.0504124164581299\n",
      "Epoch 443 / 500 | iteration 20 / 30 | Total Loss: 6.668483734130859 | KNN Loss: 5.608687400817871 | BCE Loss: 1.0597960948944092\n",
      "Epoch 443 / 500 | iteration 25 / 30 | Total Loss: 6.658284664154053 | KNN Loss: 5.616313457489014 | BCE Loss: 1.041971206665039\n",
      "Epoch 444 / 500 | iteration 0 / 30 | Total Loss: 6.708752632141113 | KNN Loss: 5.621679782867432 | BCE Loss: 1.0870728492736816\n",
      "Epoch 444 / 500 | iteration 5 / 30 | Total Loss: 6.670954704284668 | KNN Loss: 5.596810817718506 | BCE Loss: 1.0741441249847412\n",
      "Epoch 444 / 500 | iteration 10 / 30 | Total Loss: 6.670784950256348 | KNN Loss: 5.644041061401367 | BCE Loss: 1.02674400806427\n",
      "Epoch 444 / 500 | iteration 15 / 30 | Total Loss: 6.717102527618408 | KNN Loss: 5.651028156280518 | BCE Loss: 1.066074252128601\n",
      "Epoch 444 / 500 | iteration 20 / 30 | Total Loss: 6.712643623352051 | KNN Loss: 5.687625885009766 | BCE Loss: 1.0250177383422852\n",
      "Epoch 444 / 500 | iteration 25 / 30 | Total Loss: 6.647214889526367 | KNN Loss: 5.594635009765625 | BCE Loss: 1.0525798797607422\n",
      "Epoch   445: reducing learning rate of group 0 to 5.5221e-08.\n",
      "Epoch 445 / 500 | iteration 0 / 30 | Total Loss: 6.626856327056885 | KNN Loss: 5.610580921173096 | BCE Loss: 1.016275405883789\n",
      "Epoch 445 / 500 | iteration 5 / 30 | Total Loss: 6.663327693939209 | KNN Loss: 5.612901210784912 | BCE Loss: 1.0504263639450073\n",
      "Epoch 445 / 500 | iteration 10 / 30 | Total Loss: 6.610908031463623 | KNN Loss: 5.59809684753418 | BCE Loss: 1.0128111839294434\n",
      "Epoch 445 / 500 | iteration 15 / 30 | Total Loss: 6.650555610656738 | KNN Loss: 5.59672737121582 | BCE Loss: 1.053828239440918\n",
      "Epoch 445 / 500 | iteration 20 / 30 | Total Loss: 6.656815528869629 | KNN Loss: 5.593813419342041 | BCE Loss: 1.063002109527588\n",
      "Epoch 445 / 500 | iteration 25 / 30 | Total Loss: 6.6480817794799805 | KNN Loss: 5.588442325592041 | BCE Loss: 1.059639573097229\n",
      "Epoch 446 / 500 | iteration 0 / 30 | Total Loss: 6.692603588104248 | KNN Loss: 5.6598310470581055 | BCE Loss: 1.032772421836853\n",
      "Epoch 446 / 500 | iteration 5 / 30 | Total Loss: 6.638998031616211 | KNN Loss: 5.59182596206665 | BCE Loss: 1.047171950340271\n",
      "Epoch 446 / 500 | iteration 10 / 30 | Total Loss: 6.683414459228516 | KNN Loss: 5.633409023284912 | BCE Loss: 1.0500054359436035\n",
      "Epoch 446 / 500 | iteration 15 / 30 | Total Loss: 6.654130458831787 | KNN Loss: 5.6302409172058105 | BCE Loss: 1.023889422416687\n",
      "Epoch 446 / 500 | iteration 20 / 30 | Total Loss: 6.6628947257995605 | KNN Loss: 5.602309703826904 | BCE Loss: 1.0605850219726562\n",
      "Epoch 446 / 500 | iteration 25 / 30 | Total Loss: 6.691208362579346 | KNN Loss: 5.632577896118164 | BCE Loss: 1.0586305856704712\n",
      "Epoch 447 / 500 | iteration 0 / 30 | Total Loss: 6.645766258239746 | KNN Loss: 5.594183921813965 | BCE Loss: 1.0515823364257812\n",
      "Epoch 447 / 500 | iteration 5 / 30 | Total Loss: 6.643731117248535 | KNN Loss: 5.600279808044434 | BCE Loss: 1.0434510707855225\n",
      "Epoch 447 / 500 | iteration 10 / 30 | Total Loss: 6.648948669433594 | KNN Loss: 5.610320568084717 | BCE Loss: 1.038628339767456\n",
      "Epoch 447 / 500 | iteration 15 / 30 | Total Loss: 6.696661949157715 | KNN Loss: 5.645763397216797 | BCE Loss: 1.050898551940918\n",
      "Epoch 447 / 500 | iteration 20 / 30 | Total Loss: 6.692295074462891 | KNN Loss: 5.63679838180542 | BCE Loss: 1.0554965734481812\n",
      "Epoch 447 / 500 | iteration 25 / 30 | Total Loss: 6.647327423095703 | KNN Loss: 5.600253582000732 | BCE Loss: 1.0470740795135498\n",
      "Epoch 448 / 500 | iteration 0 / 30 | Total Loss: 6.668950080871582 | KNN Loss: 5.642589092254639 | BCE Loss: 1.0263612270355225\n",
      "Epoch 448 / 500 | iteration 5 / 30 | Total Loss: 6.678431510925293 | KNN Loss: 5.6197357177734375 | BCE Loss: 1.058695673942566\n",
      "Epoch 448 / 500 | iteration 10 / 30 | Total Loss: 6.637436389923096 | KNN Loss: 5.594178199768066 | BCE Loss: 1.0432581901550293\n",
      "Epoch 448 / 500 | iteration 15 / 30 | Total Loss: 6.690670013427734 | KNN Loss: 5.611446380615234 | BCE Loss: 1.0792236328125\n",
      "Epoch 448 / 500 | iteration 20 / 30 | Total Loss: 6.630911827087402 | KNN Loss: 5.632009506225586 | BCE Loss: 0.9989020824432373\n",
      "Epoch 448 / 500 | iteration 25 / 30 | Total Loss: 6.683446884155273 | KNN Loss: 5.609489440917969 | BCE Loss: 1.0739572048187256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 449 / 500 | iteration 0 / 30 | Total Loss: 6.671750068664551 | KNN Loss: 5.6568074226379395 | BCE Loss: 1.0149428844451904\n",
      "Epoch 449 / 500 | iteration 5 / 30 | Total Loss: 6.816885471343994 | KNN Loss: 5.7464599609375 | BCE Loss: 1.0704253911972046\n",
      "Epoch 449 / 500 | iteration 10 / 30 | Total Loss: 6.6697187423706055 | KNN Loss: 5.591679573059082 | BCE Loss: 1.0780394077301025\n",
      "Epoch 449 / 500 | iteration 15 / 30 | Total Loss: 6.688450813293457 | KNN Loss: 5.608777046203613 | BCE Loss: 1.0796736478805542\n",
      "Epoch 449 / 500 | iteration 20 / 30 | Total Loss: 6.66083288192749 | KNN Loss: 5.606150150299072 | BCE Loss: 1.054682731628418\n",
      "Epoch 449 / 500 | iteration 25 / 30 | Total Loss: 6.629759311676025 | KNN Loss: 5.604778289794922 | BCE Loss: 1.0249810218811035\n",
      "Epoch 450 / 500 | iteration 0 / 30 | Total Loss: 6.653360366821289 | KNN Loss: 5.59627628326416 | BCE Loss: 1.057084083557129\n",
      "Epoch 450 / 500 | iteration 5 / 30 | Total Loss: 6.643348693847656 | KNN Loss: 5.606680870056152 | BCE Loss: 1.0366675853729248\n",
      "Epoch 450 / 500 | iteration 10 / 30 | Total Loss: 6.639786720275879 | KNN Loss: 5.593515872955322 | BCE Loss: 1.0462708473205566\n",
      "Epoch 450 / 500 | iteration 15 / 30 | Total Loss: 6.668574333190918 | KNN Loss: 5.62915563583374 | BCE Loss: 1.0394189357757568\n",
      "Epoch 450 / 500 | iteration 20 / 30 | Total Loss: 6.674525260925293 | KNN Loss: 5.62481164932251 | BCE Loss: 1.0497136116027832\n",
      "Epoch 450 / 500 | iteration 25 / 30 | Total Loss: 6.630611419677734 | KNN Loss: 5.616335391998291 | BCE Loss: 1.0142759084701538\n",
      "Epoch 451 / 500 | iteration 0 / 30 | Total Loss: 6.680915832519531 | KNN Loss: 5.602286338806152 | BCE Loss: 1.078629732131958\n",
      "Epoch 451 / 500 | iteration 5 / 30 | Total Loss: 6.655039310455322 | KNN Loss: 5.607084274291992 | BCE Loss: 1.0479551553726196\n",
      "Epoch 451 / 500 | iteration 10 / 30 | Total Loss: 6.640102386474609 | KNN Loss: 5.610539436340332 | BCE Loss: 1.0295627117156982\n",
      "Epoch 451 / 500 | iteration 15 / 30 | Total Loss: 6.635218620300293 | KNN Loss: 5.609926700592041 | BCE Loss: 1.0252916812896729\n",
      "Epoch 451 / 500 | iteration 20 / 30 | Total Loss: 6.628952503204346 | KNN Loss: 5.597586631774902 | BCE Loss: 1.0313658714294434\n",
      "Epoch 451 / 500 | iteration 25 / 30 | Total Loss: 6.712762832641602 | KNN Loss: 5.668117046356201 | BCE Loss: 1.0446457862854004\n",
      "Epoch 452 / 500 | iteration 0 / 30 | Total Loss: 6.667374610900879 | KNN Loss: 5.611569404602051 | BCE Loss: 1.055804967880249\n",
      "Epoch 452 / 500 | iteration 5 / 30 | Total Loss: 6.642056465148926 | KNN Loss: 5.59684944152832 | BCE Loss: 1.0452070236206055\n",
      "Epoch 452 / 500 | iteration 10 / 30 | Total Loss: 6.615450859069824 | KNN Loss: 5.596280574798584 | BCE Loss: 1.0191700458526611\n",
      "Epoch 452 / 500 | iteration 15 / 30 | Total Loss: 6.6850385665893555 | KNN Loss: 5.622926712036133 | BCE Loss: 1.0621120929718018\n",
      "Epoch 452 / 500 | iteration 20 / 30 | Total Loss: 6.711845397949219 | KNN Loss: 5.657082557678223 | BCE Loss: 1.054762601852417\n",
      "Epoch 452 / 500 | iteration 25 / 30 | Total Loss: 6.730045318603516 | KNN Loss: 5.68357515335083 | BCE Loss: 1.0464701652526855\n",
      "Epoch 453 / 500 | iteration 0 / 30 | Total Loss: 6.693118572235107 | KNN Loss: 5.649325847625732 | BCE Loss: 1.0437926054000854\n",
      "Epoch 453 / 500 | iteration 5 / 30 | Total Loss: 6.629899501800537 | KNN Loss: 5.595731258392334 | BCE Loss: 1.0341682434082031\n",
      "Epoch 453 / 500 | iteration 10 / 30 | Total Loss: 6.675332069396973 | KNN Loss: 5.649805545806885 | BCE Loss: 1.025526523590088\n",
      "Epoch 453 / 500 | iteration 15 / 30 | Total Loss: 6.865065574645996 | KNN Loss: 5.80523681640625 | BCE Loss: 1.0598286390304565\n",
      "Epoch 453 / 500 | iteration 20 / 30 | Total Loss: 6.621311187744141 | KNN Loss: 5.594881057739258 | BCE Loss: 1.026430368423462\n",
      "Epoch 453 / 500 | iteration 25 / 30 | Total Loss: 6.676908493041992 | KNN Loss: 5.62253999710083 | BCE Loss: 1.054368257522583\n",
      "Epoch 454 / 500 | iteration 0 / 30 | Total Loss: 6.705770492553711 | KNN Loss: 5.6236891746521 | BCE Loss: 1.0820810794830322\n",
      "Epoch 454 / 500 | iteration 5 / 30 | Total Loss: 6.634482383728027 | KNN Loss: 5.593283176422119 | BCE Loss: 1.0411993265151978\n",
      "Epoch 454 / 500 | iteration 10 / 30 | Total Loss: 6.701066970825195 | KNN Loss: 5.634673118591309 | BCE Loss: 1.0663936138153076\n",
      "Epoch 454 / 500 | iteration 15 / 30 | Total Loss: 6.659007549285889 | KNN Loss: 5.630899906158447 | BCE Loss: 1.0281076431274414\n",
      "Epoch 454 / 500 | iteration 20 / 30 | Total Loss: 6.63559627532959 | KNN Loss: 5.6011786460876465 | BCE Loss: 1.0344175100326538\n",
      "Epoch 454 / 500 | iteration 25 / 30 | Total Loss: 6.703214645385742 | KNN Loss: 5.667891502380371 | BCE Loss: 1.035323143005371\n",
      "Epoch 455 / 500 | iteration 0 / 30 | Total Loss: 6.658350467681885 | KNN Loss: 5.634063720703125 | BCE Loss: 1.0242866277694702\n",
      "Epoch 455 / 500 | iteration 5 / 30 | Total Loss: 6.686851501464844 | KNN Loss: 5.629136562347412 | BCE Loss: 1.0577151775360107\n",
      "Epoch 455 / 500 | iteration 10 / 30 | Total Loss: 6.6438140869140625 | KNN Loss: 5.595146656036377 | BCE Loss: 1.0486674308776855\n",
      "Epoch 455 / 500 | iteration 15 / 30 | Total Loss: 6.636606693267822 | KNN Loss: 5.603476524353027 | BCE Loss: 1.0331302881240845\n",
      "Epoch 455 / 500 | iteration 20 / 30 | Total Loss: 6.6809163093566895 | KNN Loss: 5.650355339050293 | BCE Loss: 1.030560851097107\n",
      "Epoch 455 / 500 | iteration 25 / 30 | Total Loss: 6.659887790679932 | KNN Loss: 5.592749118804932 | BCE Loss: 1.067138671875\n",
      "Epoch   456: reducing learning rate of group 0 to 3.8655e-08.\n",
      "Epoch 456 / 500 | iteration 0 / 30 | Total Loss: 6.6585822105407715 | KNN Loss: 5.594791889190674 | BCE Loss: 1.063790202140808\n",
      "Epoch 456 / 500 | iteration 5 / 30 | Total Loss: 6.709706783294678 | KNN Loss: 5.659773826599121 | BCE Loss: 1.0499330759048462\n",
      "Epoch 456 / 500 | iteration 10 / 30 | Total Loss: 6.664339065551758 | KNN Loss: 5.61923885345459 | BCE Loss: 1.0451003313064575\n",
      "Epoch 456 / 500 | iteration 15 / 30 | Total Loss: 6.636831283569336 | KNN Loss: 5.610456466674805 | BCE Loss: 1.0263748168945312\n",
      "Epoch 456 / 500 | iteration 20 / 30 | Total Loss: 6.689787864685059 | KNN Loss: 5.653268814086914 | BCE Loss: 1.0365188121795654\n",
      "Epoch 456 / 500 | iteration 25 / 30 | Total Loss: 6.706392288208008 | KNN Loss: 5.672179222106934 | BCE Loss: 1.0342128276824951\n",
      "Epoch 457 / 500 | iteration 0 / 30 | Total Loss: 6.649855613708496 | KNN Loss: 5.624831676483154 | BCE Loss: 1.0250238180160522\n",
      "Epoch 457 / 500 | iteration 5 / 30 | Total Loss: 6.694394111633301 | KNN Loss: 5.669485092163086 | BCE Loss: 1.024909257888794\n",
      "Epoch 457 / 500 | iteration 10 / 30 | Total Loss: 6.635881423950195 | KNN Loss: 5.60481071472168 | BCE Loss: 1.0310709476470947\n",
      "Epoch 457 / 500 | iteration 15 / 30 | Total Loss: 6.631679058074951 | KNN Loss: 5.595465660095215 | BCE Loss: 1.0362132787704468\n",
      "Epoch 457 / 500 | iteration 20 / 30 | Total Loss: 6.6817474365234375 | KNN Loss: 5.608436584472656 | BCE Loss: 1.0733106136322021\n",
      "Epoch 457 / 500 | iteration 25 / 30 | Total Loss: 6.6806640625 | KNN Loss: 5.642078399658203 | BCE Loss: 1.0385854244232178\n",
      "Epoch 458 / 500 | iteration 0 / 30 | Total Loss: 6.625638961791992 | KNN Loss: 5.60617208480835 | BCE Loss: 1.0194666385650635\n",
      "Epoch 458 / 500 | iteration 5 / 30 | Total Loss: 6.649636745452881 | KNN Loss: 5.620128631591797 | BCE Loss: 1.029508113861084\n",
      "Epoch 458 / 500 | iteration 10 / 30 | Total Loss: 6.68856143951416 | KNN Loss: 5.6338934898376465 | BCE Loss: 1.0546681880950928\n",
      "Epoch 458 / 500 | iteration 15 / 30 | Total Loss: 6.677016735076904 | KNN Loss: 5.63803243637085 | BCE Loss: 1.0389842987060547\n",
      "Epoch 458 / 500 | iteration 20 / 30 | Total Loss: 6.658997535705566 | KNN Loss: 5.602842807769775 | BCE Loss: 1.0561549663543701\n",
      "Epoch 458 / 500 | iteration 25 / 30 | Total Loss: 6.61043643951416 | KNN Loss: 5.594139575958252 | BCE Loss: 1.0162971019744873\n",
      "Epoch 459 / 500 | iteration 0 / 30 | Total Loss: 6.69921875 | KNN Loss: 5.626955032348633 | BCE Loss: 1.0722637176513672\n",
      "Epoch 459 / 500 | iteration 5 / 30 | Total Loss: 6.703373432159424 | KNN Loss: 5.659104347229004 | BCE Loss: 1.0442689657211304\n",
      "Epoch 459 / 500 | iteration 10 / 30 | Total Loss: 6.732687950134277 | KNN Loss: 5.69561243057251 | BCE Loss: 1.0370755195617676\n",
      "Epoch 459 / 500 | iteration 15 / 30 | Total Loss: 6.660306453704834 | KNN Loss: 5.5920796394348145 | BCE Loss: 1.0682268142700195\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 459 / 500 | iteration 20 / 30 | Total Loss: 6.697253227233887 | KNN Loss: 5.628466606140137 | BCE Loss: 1.068786859512329\n",
      "Epoch 459 / 500 | iteration 25 / 30 | Total Loss: 6.63754940032959 | KNN Loss: 5.617853164672852 | BCE Loss: 1.0196962356567383\n",
      "Epoch 460 / 500 | iteration 0 / 30 | Total Loss: 6.67661190032959 | KNN Loss: 5.646628379821777 | BCE Loss: 1.0299835205078125\n",
      "Epoch 460 / 500 | iteration 5 / 30 | Total Loss: 6.713959693908691 | KNN Loss: 5.652726650238037 | BCE Loss: 1.0612328052520752\n",
      "Epoch 460 / 500 | iteration 10 / 30 | Total Loss: 6.686912536621094 | KNN Loss: 5.632821559906006 | BCE Loss: 1.054091215133667\n",
      "Epoch 460 / 500 | iteration 15 / 30 | Total Loss: 6.7007598876953125 | KNN Loss: 5.641837120056152 | BCE Loss: 1.0589227676391602\n",
      "Epoch 460 / 500 | iteration 20 / 30 | Total Loss: 6.736947059631348 | KNN Loss: 5.665217399597168 | BCE Loss: 1.0717295408248901\n",
      "Epoch 460 / 500 | iteration 25 / 30 | Total Loss: 6.636678695678711 | KNN Loss: 5.590761661529541 | BCE Loss: 1.0459169149398804\n",
      "Epoch 461 / 500 | iteration 0 / 30 | Total Loss: 6.677114486694336 | KNN Loss: 5.614314079284668 | BCE Loss: 1.062800407409668\n",
      "Epoch 461 / 500 | iteration 5 / 30 | Total Loss: 6.6332902908325195 | KNN Loss: 5.593380928039551 | BCE Loss: 1.0399091243743896\n",
      "Epoch 461 / 500 | iteration 10 / 30 | Total Loss: 6.624537467956543 | KNN Loss: 5.591850757598877 | BCE Loss: 1.032686471939087\n",
      "Epoch 461 / 500 | iteration 15 / 30 | Total Loss: 6.652275085449219 | KNN Loss: 5.623443603515625 | BCE Loss: 1.0288313627243042\n",
      "Epoch 461 / 500 | iteration 20 / 30 | Total Loss: 6.691098213195801 | KNN Loss: 5.651517391204834 | BCE Loss: 1.0395805835723877\n",
      "Epoch 461 / 500 | iteration 25 / 30 | Total Loss: 6.6188201904296875 | KNN Loss: 5.590932846069336 | BCE Loss: 1.0278874635696411\n",
      "Epoch 462 / 500 | iteration 0 / 30 | Total Loss: 6.663818359375 | KNN Loss: 5.620694160461426 | BCE Loss: 1.0431243181228638\n",
      "Epoch 462 / 500 | iteration 5 / 30 | Total Loss: 6.7277727127075195 | KNN Loss: 5.706189155578613 | BCE Loss: 1.0215833187103271\n",
      "Epoch 462 / 500 | iteration 10 / 30 | Total Loss: 6.66378927230835 | KNN Loss: 5.613986492156982 | BCE Loss: 1.0498027801513672\n",
      "Epoch 462 / 500 | iteration 15 / 30 | Total Loss: 6.666101932525635 | KNN Loss: 5.623562812805176 | BCE Loss: 1.0425390005111694\n",
      "Epoch 462 / 500 | iteration 20 / 30 | Total Loss: 6.637292861938477 | KNN Loss: 5.59310245513916 | BCE Loss: 1.0441902875900269\n",
      "Epoch 462 / 500 | iteration 25 / 30 | Total Loss: 6.649229049682617 | KNN Loss: 5.606323719024658 | BCE Loss: 1.042905569076538\n",
      "Epoch 463 / 500 | iteration 0 / 30 | Total Loss: 6.695456504821777 | KNN Loss: 5.622856140136719 | BCE Loss: 1.0726001262664795\n",
      "Epoch 463 / 500 | iteration 5 / 30 | Total Loss: 6.686540603637695 | KNN Loss: 5.648176193237305 | BCE Loss: 1.0383646488189697\n",
      "Epoch 463 / 500 | iteration 10 / 30 | Total Loss: 6.835675239562988 | KNN Loss: 5.767567157745361 | BCE Loss: 1.0681079626083374\n",
      "Epoch 463 / 500 | iteration 15 / 30 | Total Loss: 6.671622276306152 | KNN Loss: 5.627357006072998 | BCE Loss: 1.0442650318145752\n",
      "Epoch 463 / 500 | iteration 20 / 30 | Total Loss: 6.651832103729248 | KNN Loss: 5.620387554168701 | BCE Loss: 1.0314446687698364\n",
      "Epoch 463 / 500 | iteration 25 / 30 | Total Loss: 6.710948944091797 | KNN Loss: 5.661564350128174 | BCE Loss: 1.0493848323822021\n",
      "Epoch 464 / 500 | iteration 0 / 30 | Total Loss: 6.666510105133057 | KNN Loss: 5.634138584136963 | BCE Loss: 1.0323716402053833\n",
      "Epoch 464 / 500 | iteration 5 / 30 | Total Loss: 6.695354461669922 | KNN Loss: 5.647330284118652 | BCE Loss: 1.048024296760559\n",
      "Epoch 464 / 500 | iteration 10 / 30 | Total Loss: 6.661196708679199 | KNN Loss: 5.620306015014648 | BCE Loss: 1.0408904552459717\n",
      "Epoch 464 / 500 | iteration 15 / 30 | Total Loss: 6.708382606506348 | KNN Loss: 5.639429092407227 | BCE Loss: 1.0689536333084106\n",
      "Epoch 464 / 500 | iteration 20 / 30 | Total Loss: 6.644974231719971 | KNN Loss: 5.6019673347473145 | BCE Loss: 1.0430068969726562\n",
      "Epoch 464 / 500 | iteration 25 / 30 | Total Loss: 6.667854309082031 | KNN Loss: 5.601113796234131 | BCE Loss: 1.0667407512664795\n",
      "Epoch 465 / 500 | iteration 0 / 30 | Total Loss: 6.703593730926514 | KNN Loss: 5.646061420440674 | BCE Loss: 1.0575323104858398\n",
      "Epoch 465 / 500 | iteration 5 / 30 | Total Loss: 6.708126068115234 | KNN Loss: 5.6601409912109375 | BCE Loss: 1.0479850769042969\n",
      "Epoch 465 / 500 | iteration 10 / 30 | Total Loss: 6.631234169006348 | KNN Loss: 5.600072383880615 | BCE Loss: 1.0311615467071533\n",
      "Epoch 465 / 500 | iteration 15 / 30 | Total Loss: 6.658730506896973 | KNN Loss: 5.606210708618164 | BCE Loss: 1.0525199174880981\n",
      "Epoch 465 / 500 | iteration 20 / 30 | Total Loss: 6.652063369750977 | KNN Loss: 5.598579406738281 | BCE Loss: 1.0534840822219849\n",
      "Epoch 465 / 500 | iteration 25 / 30 | Total Loss: 6.68811559677124 | KNN Loss: 5.659245014190674 | BCE Loss: 1.0288705825805664\n",
      "Epoch 466 / 500 | iteration 0 / 30 | Total Loss: 6.657035827636719 | KNN Loss: 5.592350959777832 | BCE Loss: 1.0646848678588867\n",
      "Epoch 466 / 500 | iteration 5 / 30 | Total Loss: 6.633921146392822 | KNN Loss: 5.593825340270996 | BCE Loss: 1.0400956869125366\n",
      "Epoch 466 / 500 | iteration 10 / 30 | Total Loss: 6.67141580581665 | KNN Loss: 5.625741004943848 | BCE Loss: 1.0456746816635132\n",
      "Epoch 466 / 500 | iteration 15 / 30 | Total Loss: 6.647294998168945 | KNN Loss: 5.613941192626953 | BCE Loss: 1.0333540439605713\n",
      "Epoch 466 / 500 | iteration 20 / 30 | Total Loss: 6.700717449188232 | KNN Loss: 5.629103183746338 | BCE Loss: 1.071614384651184\n",
      "Epoch 466 / 500 | iteration 25 / 30 | Total Loss: 6.650792121887207 | KNN Loss: 5.590303421020508 | BCE Loss: 1.0604887008666992\n",
      "Epoch   467: reducing learning rate of group 0 to 2.7058e-08.\n",
      "Epoch 467 / 500 | iteration 0 / 30 | Total Loss: 6.635610103607178 | KNN Loss: 5.594394207000732 | BCE Loss: 1.0412158966064453\n",
      "Epoch 467 / 500 | iteration 5 / 30 | Total Loss: 6.65860652923584 | KNN Loss: 5.599766254425049 | BCE Loss: 1.058840274810791\n",
      "Epoch 467 / 500 | iteration 10 / 30 | Total Loss: 6.627880096435547 | KNN Loss: 5.610812187194824 | BCE Loss: 1.0170681476593018\n",
      "Epoch 467 / 500 | iteration 15 / 30 | Total Loss: 6.654298782348633 | KNN Loss: 5.602325439453125 | BCE Loss: 1.051973581314087\n",
      "Epoch 467 / 500 | iteration 20 / 30 | Total Loss: 6.662981033325195 | KNN Loss: 5.632394790649414 | BCE Loss: 1.0305862426757812\n",
      "Epoch 467 / 500 | iteration 25 / 30 | Total Loss: 6.640526294708252 | KNN Loss: 5.603357315063477 | BCE Loss: 1.0371689796447754\n",
      "Epoch 468 / 500 | iteration 0 / 30 | Total Loss: 6.659419536590576 | KNN Loss: 5.6100029945373535 | BCE Loss: 1.049416422843933\n",
      "Epoch 468 / 500 | iteration 5 / 30 | Total Loss: 6.7165985107421875 | KNN Loss: 5.676851272583008 | BCE Loss: 1.0397469997406006\n",
      "Epoch 468 / 500 | iteration 10 / 30 | Total Loss: 6.660558700561523 | KNN Loss: 5.6024980545043945 | BCE Loss: 1.0580604076385498\n",
      "Epoch 468 / 500 | iteration 15 / 30 | Total Loss: 6.645686626434326 | KNN Loss: 5.596852779388428 | BCE Loss: 1.0488338470458984\n",
      "Epoch 468 / 500 | iteration 20 / 30 | Total Loss: 6.828744888305664 | KNN Loss: 5.771107196807861 | BCE Loss: 1.0576376914978027\n",
      "Epoch 468 / 500 | iteration 25 / 30 | Total Loss: 6.681257247924805 | KNN Loss: 5.6283464431762695 | BCE Loss: 1.0529106855392456\n",
      "Epoch 469 / 500 | iteration 0 / 30 | Total Loss: 6.672022819519043 | KNN Loss: 5.606632232666016 | BCE Loss: 1.0653903484344482\n",
      "Epoch 469 / 500 | iteration 5 / 30 | Total Loss: 6.628302574157715 | KNN Loss: 5.6023736000061035 | BCE Loss: 1.0259289741516113\n",
      "Epoch 469 / 500 | iteration 10 / 30 | Total Loss: 6.65685510635376 | KNN Loss: 5.626217842102051 | BCE Loss: 1.0306373834609985\n",
      "Epoch 469 / 500 | iteration 15 / 30 | Total Loss: 6.648314476013184 | KNN Loss: 5.615671157836914 | BCE Loss: 1.032643437385559\n",
      "Epoch 469 / 500 | iteration 20 / 30 | Total Loss: 6.657715797424316 | KNN Loss: 5.602575778961182 | BCE Loss: 1.0551397800445557\n",
      "Epoch 469 / 500 | iteration 25 / 30 | Total Loss: 6.662472724914551 | KNN Loss: 5.615722179412842 | BCE Loss: 1.0467503070831299\n",
      "Epoch 470 / 500 | iteration 0 / 30 | Total Loss: 6.611817836761475 | KNN Loss: 5.596131801605225 | BCE Loss: 1.01568603515625\n",
      "Epoch 470 / 500 | iteration 5 / 30 | Total Loss: 6.6457109451293945 | KNN Loss: 5.594868183135986 | BCE Loss: 1.0508428812026978\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 470 / 500 | iteration 10 / 30 | Total Loss: 6.671650409698486 | KNN Loss: 5.602357387542725 | BCE Loss: 1.0692931413650513\n",
      "Epoch 470 / 500 | iteration 15 / 30 | Total Loss: 6.641423225402832 | KNN Loss: 5.590216159820557 | BCE Loss: 1.0512068271636963\n",
      "Epoch 470 / 500 | iteration 20 / 30 | Total Loss: 6.626206874847412 | KNN Loss: 5.591987609863281 | BCE Loss: 1.0342191457748413\n",
      "Epoch 470 / 500 | iteration 25 / 30 | Total Loss: 6.725613594055176 | KNN Loss: 5.67105770111084 | BCE Loss: 1.0545557737350464\n",
      "Epoch 471 / 500 | iteration 0 / 30 | Total Loss: 6.614880561828613 | KNN Loss: 5.602985858917236 | BCE Loss: 1.0118944644927979\n",
      "Epoch 471 / 500 | iteration 5 / 30 | Total Loss: 6.7131028175354 | KNN Loss: 5.665530681610107 | BCE Loss: 1.0475720167160034\n",
      "Epoch 471 / 500 | iteration 10 / 30 | Total Loss: 6.661980628967285 | KNN Loss: 5.599809646606445 | BCE Loss: 1.0621707439422607\n",
      "Epoch 471 / 500 | iteration 15 / 30 | Total Loss: 6.632842063903809 | KNN Loss: 5.598044395446777 | BCE Loss: 1.0347976684570312\n",
      "Epoch 471 / 500 | iteration 20 / 30 | Total Loss: 6.697455406188965 | KNN Loss: 5.631283760070801 | BCE Loss: 1.0661718845367432\n",
      "Epoch 471 / 500 | iteration 25 / 30 | Total Loss: 6.671502113342285 | KNN Loss: 5.617968559265137 | BCE Loss: 1.053533673286438\n",
      "Epoch 472 / 500 | iteration 0 / 30 | Total Loss: 6.786750316619873 | KNN Loss: 5.739738464355469 | BCE Loss: 1.0470118522644043\n",
      "Epoch 472 / 500 | iteration 5 / 30 | Total Loss: 6.644711971282959 | KNN Loss: 5.605326175689697 | BCE Loss: 1.0393859148025513\n",
      "Epoch 472 / 500 | iteration 10 / 30 | Total Loss: 6.700691223144531 | KNN Loss: 5.650285243988037 | BCE Loss: 1.050405740737915\n",
      "Epoch 472 / 500 | iteration 15 / 30 | Total Loss: 6.70969295501709 | KNN Loss: 5.65438175201416 | BCE Loss: 1.0553114414215088\n",
      "Epoch 472 / 500 | iteration 20 / 30 | Total Loss: 6.664968013763428 | KNN Loss: 5.622863292694092 | BCE Loss: 1.042104721069336\n",
      "Epoch 472 / 500 | iteration 25 / 30 | Total Loss: 6.624074935913086 | KNN Loss: 5.604096412658691 | BCE Loss: 1.019978642463684\n",
      "Epoch 473 / 500 | iteration 0 / 30 | Total Loss: 6.656542778015137 | KNN Loss: 5.603546619415283 | BCE Loss: 1.0529961585998535\n",
      "Epoch 473 / 500 | iteration 5 / 30 | Total Loss: 6.671494007110596 | KNN Loss: 5.635675430297852 | BCE Loss: 1.0358185768127441\n",
      "Epoch 473 / 500 | iteration 10 / 30 | Total Loss: 6.6970295906066895 | KNN Loss: 5.657033443450928 | BCE Loss: 1.0399962663650513\n",
      "Epoch 473 / 500 | iteration 15 / 30 | Total Loss: 6.636717796325684 | KNN Loss: 5.596650123596191 | BCE Loss: 1.040067434310913\n",
      "Epoch 473 / 500 | iteration 20 / 30 | Total Loss: 6.735536575317383 | KNN Loss: 5.662002086639404 | BCE Loss: 1.0735342502593994\n",
      "Epoch 473 / 500 | iteration 25 / 30 | Total Loss: 6.638903617858887 | KNN Loss: 5.607648849487305 | BCE Loss: 1.0312546491622925\n",
      "Epoch 474 / 500 | iteration 0 / 30 | Total Loss: 6.668422698974609 | KNN Loss: 5.593609809875488 | BCE Loss: 1.074812889099121\n",
      "Epoch 474 / 500 | iteration 5 / 30 | Total Loss: 6.655647277832031 | KNN Loss: 5.622034549713135 | BCE Loss: 1.0336127281188965\n",
      "Epoch 474 / 500 | iteration 10 / 30 | Total Loss: 6.706780433654785 | KNN Loss: 5.622635841369629 | BCE Loss: 1.0841443538665771\n",
      "Epoch 474 / 500 | iteration 15 / 30 | Total Loss: 6.629279136657715 | KNN Loss: 5.601223468780518 | BCE Loss: 1.0280554294586182\n",
      "Epoch 474 / 500 | iteration 20 / 30 | Total Loss: 6.657182693481445 | KNN Loss: 5.599693775177002 | BCE Loss: 1.0574889183044434\n",
      "Epoch 474 / 500 | iteration 25 / 30 | Total Loss: 6.764141082763672 | KNN Loss: 5.739230632781982 | BCE Loss: 1.0249104499816895\n",
      "Epoch 475 / 500 | iteration 0 / 30 | Total Loss: 6.671723365783691 | KNN Loss: 5.621039867401123 | BCE Loss: 1.0506837368011475\n",
      "Epoch 475 / 500 | iteration 5 / 30 | Total Loss: 6.683187961578369 | KNN Loss: 5.612431049346924 | BCE Loss: 1.0707570314407349\n",
      "Epoch 475 / 500 | iteration 10 / 30 | Total Loss: 6.668653964996338 | KNN Loss: 5.625270843505859 | BCE Loss: 1.0433831214904785\n",
      "Epoch 475 / 500 | iteration 15 / 30 | Total Loss: 6.715084552764893 | KNN Loss: 5.632452487945557 | BCE Loss: 1.0826319456100464\n",
      "Epoch 475 / 500 | iteration 20 / 30 | Total Loss: 6.702395915985107 | KNN Loss: 5.6901092529296875 | BCE Loss: 1.0122865438461304\n",
      "Epoch 475 / 500 | iteration 25 / 30 | Total Loss: 6.639344215393066 | KNN Loss: 5.605886936187744 | BCE Loss: 1.0334570407867432\n",
      "Epoch 476 / 500 | iteration 0 / 30 | Total Loss: 6.657833099365234 | KNN Loss: 5.63447380065918 | BCE Loss: 1.0233591794967651\n",
      "Epoch 476 / 500 | iteration 5 / 30 | Total Loss: 6.672843933105469 | KNN Loss: 5.653522491455078 | BCE Loss: 1.0193216800689697\n",
      "Epoch 476 / 500 | iteration 10 / 30 | Total Loss: 6.709379196166992 | KNN Loss: 5.643721103668213 | BCE Loss: 1.0656583309173584\n",
      "Epoch 476 / 500 | iteration 15 / 30 | Total Loss: 6.731276035308838 | KNN Loss: 5.675576686859131 | BCE Loss: 1.055699348449707\n",
      "Epoch 476 / 500 | iteration 20 / 30 | Total Loss: 6.745924949645996 | KNN Loss: 5.690122127532959 | BCE Loss: 1.055802822113037\n",
      "Epoch 476 / 500 | iteration 25 / 30 | Total Loss: 6.652397155761719 | KNN Loss: 5.596209526062012 | BCE Loss: 1.056187391281128\n",
      "Epoch 477 / 500 | iteration 0 / 30 | Total Loss: 6.651756286621094 | KNN Loss: 5.596445560455322 | BCE Loss: 1.0553107261657715\n",
      "Epoch 477 / 500 | iteration 5 / 30 | Total Loss: 6.708632469177246 | KNN Loss: 5.640379905700684 | BCE Loss: 1.0682528018951416\n",
      "Epoch 477 / 500 | iteration 10 / 30 | Total Loss: 6.749972343444824 | KNN Loss: 5.718407154083252 | BCE Loss: 1.0315651893615723\n",
      "Epoch 477 / 500 | iteration 15 / 30 | Total Loss: 6.664884090423584 | KNN Loss: 5.630313873291016 | BCE Loss: 1.0345702171325684\n",
      "Epoch 477 / 500 | iteration 20 / 30 | Total Loss: 6.732027053833008 | KNN Loss: 5.644902229309082 | BCE Loss: 1.0871250629425049\n",
      "Epoch 477 / 500 | iteration 25 / 30 | Total Loss: 6.707304000854492 | KNN Loss: 5.6086225509643555 | BCE Loss: 1.0986812114715576\n",
      "Epoch 478 / 500 | iteration 0 / 30 | Total Loss: 6.644250869750977 | KNN Loss: 5.597128391265869 | BCE Loss: 1.0471223592758179\n",
      "Epoch 478 / 500 | iteration 5 / 30 | Total Loss: 6.631933212280273 | KNN Loss: 5.597845077514648 | BCE Loss: 1.034087896347046\n",
      "Epoch 478 / 500 | iteration 10 / 30 | Total Loss: 6.651737213134766 | KNN Loss: 5.595826625823975 | BCE Loss: 1.055910587310791\n",
      "Epoch 478 / 500 | iteration 15 / 30 | Total Loss: 6.661711692810059 | KNN Loss: 5.597841262817383 | BCE Loss: 1.0638701915740967\n",
      "Epoch 478 / 500 | iteration 20 / 30 | Total Loss: 6.697137832641602 | KNN Loss: 5.629348278045654 | BCE Loss: 1.0677895545959473\n",
      "Epoch 478 / 500 | iteration 25 / 30 | Total Loss: 6.6569013595581055 | KNN Loss: 5.608860015869141 | BCE Loss: 1.0480414628982544\n",
      "Epoch 479 / 500 | iteration 0 / 30 | Total Loss: 6.737966537475586 | KNN Loss: 5.661859512329102 | BCE Loss: 1.076107144355774\n",
      "Epoch 479 / 500 | iteration 5 / 30 | Total Loss: 6.646790504455566 | KNN Loss: 5.598339080810547 | BCE Loss: 1.0484511852264404\n",
      "Epoch 479 / 500 | iteration 10 / 30 | Total Loss: 6.654753684997559 | KNN Loss: 5.634100914001465 | BCE Loss: 1.0206525325775146\n",
      "Epoch 479 / 500 | iteration 15 / 30 | Total Loss: 6.689157485961914 | KNN Loss: 5.621412754058838 | BCE Loss: 1.0677447319030762\n",
      "Epoch 479 / 500 | iteration 20 / 30 | Total Loss: 6.660571098327637 | KNN Loss: 5.607388019561768 | BCE Loss: 1.0531830787658691\n",
      "Epoch 479 / 500 | iteration 25 / 30 | Total Loss: 6.680423736572266 | KNN Loss: 5.649385929107666 | BCE Loss: 1.0310380458831787\n",
      "Epoch 480 / 500 | iteration 0 / 30 | Total Loss: 6.732090473175049 | KNN Loss: 5.677434921264648 | BCE Loss: 1.0546554327011108\n",
      "Epoch 480 / 500 | iteration 5 / 30 | Total Loss: 6.655858039855957 | KNN Loss: 5.59453010559082 | BCE Loss: 1.0613281726837158\n",
      "Epoch 480 / 500 | iteration 10 / 30 | Total Loss: 6.663243293762207 | KNN Loss: 5.606107234954834 | BCE Loss: 1.0571361780166626\n",
      "Epoch 480 / 500 | iteration 15 / 30 | Total Loss: 6.6844048500061035 | KNN Loss: 5.6080403327941895 | BCE Loss: 1.0763646364212036\n",
      "Epoch 480 / 500 | iteration 20 / 30 | Total Loss: 6.6962385177612305 | KNN Loss: 5.638218879699707 | BCE Loss: 1.0580196380615234\n",
      "Epoch 480 / 500 | iteration 25 / 30 | Total Loss: 6.706181526184082 | KNN Loss: 5.6468892097473145 | BCE Loss: 1.0592925548553467\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 481 / 500 | iteration 0 / 30 | Total Loss: 6.684351921081543 | KNN Loss: 5.649083137512207 | BCE Loss: 1.0352685451507568\n",
      "Epoch 481 / 500 | iteration 5 / 30 | Total Loss: 6.659478664398193 | KNN Loss: 5.603353977203369 | BCE Loss: 1.0561246871948242\n",
      "Epoch 481 / 500 | iteration 10 / 30 | Total Loss: 6.66899299621582 | KNN Loss: 5.6213250160217285 | BCE Loss: 1.0476679801940918\n",
      "Epoch 481 / 500 | iteration 15 / 30 | Total Loss: 6.660287857055664 | KNN Loss: 5.603072643280029 | BCE Loss: 1.0572152137756348\n",
      "Epoch 481 / 500 | iteration 20 / 30 | Total Loss: 6.61958646774292 | KNN Loss: 5.595616340637207 | BCE Loss: 1.0239700078964233\n",
      "Epoch 481 / 500 | iteration 25 / 30 | Total Loss: 6.686114311218262 | KNN Loss: 5.6205549240112305 | BCE Loss: 1.0655591487884521\n",
      "Epoch 482 / 500 | iteration 0 / 30 | Total Loss: 6.647724628448486 | KNN Loss: 5.606989860534668 | BCE Loss: 1.0407347679138184\n",
      "Epoch 482 / 500 | iteration 5 / 30 | Total Loss: 6.663987159729004 | KNN Loss: 5.633882522583008 | BCE Loss: 1.0301045179367065\n",
      "Epoch 482 / 500 | iteration 10 / 30 | Total Loss: 6.636953353881836 | KNN Loss: 5.603877544403076 | BCE Loss: 1.0330755710601807\n",
      "Epoch 482 / 500 | iteration 15 / 30 | Total Loss: 6.631171226501465 | KNN Loss: 5.604785919189453 | BCE Loss: 1.0263850688934326\n",
      "Epoch 482 / 500 | iteration 20 / 30 | Total Loss: 6.634167194366455 | KNN Loss: 5.592219829559326 | BCE Loss: 1.041947364807129\n",
      "Epoch 482 / 500 | iteration 25 / 30 | Total Loss: 6.709928512573242 | KNN Loss: 5.641051292419434 | BCE Loss: 1.0688774585723877\n",
      "Epoch 483 / 500 | iteration 0 / 30 | Total Loss: 6.666953086853027 | KNN Loss: 5.6131768226623535 | BCE Loss: 1.0537760257720947\n",
      "Epoch 483 / 500 | iteration 5 / 30 | Total Loss: 6.695215225219727 | KNN Loss: 5.638844966888428 | BCE Loss: 1.0563700199127197\n",
      "Epoch 483 / 500 | iteration 10 / 30 | Total Loss: 6.713131427764893 | KNN Loss: 5.648301601409912 | BCE Loss: 1.0648298263549805\n",
      "Epoch 483 / 500 | iteration 15 / 30 | Total Loss: 6.719201564788818 | KNN Loss: 5.636606216430664 | BCE Loss: 1.0825953483581543\n",
      "Epoch 483 / 500 | iteration 20 / 30 | Total Loss: 6.643943786621094 | KNN Loss: 5.608856201171875 | BCE Loss: 1.0350875854492188\n",
      "Epoch 483 / 500 | iteration 25 / 30 | Total Loss: 6.640958309173584 | KNN Loss: 5.595824241638184 | BCE Loss: 1.0451340675354004\n",
      "Epoch 484 / 500 | iteration 0 / 30 | Total Loss: 6.711197376251221 | KNN Loss: 5.6673455238342285 | BCE Loss: 1.0438518524169922\n",
      "Epoch 484 / 500 | iteration 5 / 30 | Total Loss: 6.660672664642334 | KNN Loss: 5.62589168548584 | BCE Loss: 1.0347809791564941\n",
      "Epoch 484 / 500 | iteration 10 / 30 | Total Loss: 6.651103496551514 | KNN Loss: 5.595857620239258 | BCE Loss: 1.0552458763122559\n",
      "Epoch 484 / 500 | iteration 15 / 30 | Total Loss: 6.681107521057129 | KNN Loss: 5.629489898681641 | BCE Loss: 1.0516173839569092\n",
      "Epoch 484 / 500 | iteration 20 / 30 | Total Loss: 6.6913533210754395 | KNN Loss: 5.64788818359375 | BCE Loss: 1.0434651374816895\n",
      "Epoch 484 / 500 | iteration 25 / 30 | Total Loss: 6.685980796813965 | KNN Loss: 5.648401260375977 | BCE Loss: 1.0375795364379883\n",
      "Epoch 485 / 500 | iteration 0 / 30 | Total Loss: 6.707114219665527 | KNN Loss: 5.674695014953613 | BCE Loss: 1.0324194431304932\n",
      "Epoch 485 / 500 | iteration 5 / 30 | Total Loss: 6.6966400146484375 | KNN Loss: 5.666736125946045 | BCE Loss: 1.0299038887023926\n",
      "Epoch 485 / 500 | iteration 10 / 30 | Total Loss: 6.697023391723633 | KNN Loss: 5.655851364135742 | BCE Loss: 1.0411717891693115\n",
      "Epoch 485 / 500 | iteration 15 / 30 | Total Loss: 6.6618146896362305 | KNN Loss: 5.61251974105835 | BCE Loss: 1.0492947101593018\n",
      "Epoch 485 / 500 | iteration 20 / 30 | Total Loss: 6.660337924957275 | KNN Loss: 5.594305992126465 | BCE Loss: 1.066031813621521\n",
      "Epoch 485 / 500 | iteration 25 / 30 | Total Loss: 6.656766891479492 | KNN Loss: 5.596798419952393 | BCE Loss: 1.0599685907363892\n",
      "Epoch 486 / 500 | iteration 0 / 30 | Total Loss: 6.809230804443359 | KNN Loss: 5.734617710113525 | BCE Loss: 1.074613332748413\n",
      "Epoch 486 / 500 | iteration 5 / 30 | Total Loss: 6.65949821472168 | KNN Loss: 5.607233047485352 | BCE Loss: 1.052264928817749\n",
      "Epoch 486 / 500 | iteration 10 / 30 | Total Loss: 6.71157169342041 | KNN Loss: 5.652188777923584 | BCE Loss: 1.0593830347061157\n",
      "Epoch 486 / 500 | iteration 15 / 30 | Total Loss: 6.7419819831848145 | KNN Loss: 5.662761211395264 | BCE Loss: 1.0792207717895508\n",
      "Epoch 486 / 500 | iteration 20 / 30 | Total Loss: 6.670104503631592 | KNN Loss: 5.628698348999023 | BCE Loss: 1.0414061546325684\n",
      "Epoch 486 / 500 | iteration 25 / 30 | Total Loss: 6.62300443649292 | KNN Loss: 5.60046911239624 | BCE Loss: 1.0225353240966797\n",
      "Epoch 487 / 500 | iteration 0 / 30 | Total Loss: 6.763788223266602 | KNN Loss: 5.727575302124023 | BCE Loss: 1.0362130403518677\n",
      "Epoch 487 / 500 | iteration 5 / 30 | Total Loss: 6.801290512084961 | KNN Loss: 5.771454811096191 | BCE Loss: 1.0298359394073486\n",
      "Epoch 487 / 500 | iteration 10 / 30 | Total Loss: 6.661341667175293 | KNN Loss: 5.605081081390381 | BCE Loss: 1.056260347366333\n",
      "Epoch 487 / 500 | iteration 15 / 30 | Total Loss: 6.684408187866211 | KNN Loss: 5.614691257476807 | BCE Loss: 1.0697166919708252\n",
      "Epoch 487 / 500 | iteration 20 / 30 | Total Loss: 6.6897125244140625 | KNN Loss: 5.63320779800415 | BCE Loss: 1.056504487991333\n",
      "Epoch 487 / 500 | iteration 25 / 30 | Total Loss: 6.663852691650391 | KNN Loss: 5.6615095138549805 | BCE Loss: 1.0023432970046997\n",
      "Epoch 488 / 500 | iteration 0 / 30 | Total Loss: 6.663081645965576 | KNN Loss: 5.6091694831848145 | BCE Loss: 1.0539122819900513\n",
      "Epoch 488 / 500 | iteration 5 / 30 | Total Loss: 6.653298377990723 | KNN Loss: 5.614912986755371 | BCE Loss: 1.0383856296539307\n",
      "Epoch 488 / 500 | iteration 10 / 30 | Total Loss: 6.675356864929199 | KNN Loss: 5.621940612792969 | BCE Loss: 1.0534162521362305\n",
      "Epoch 488 / 500 | iteration 15 / 30 | Total Loss: 6.652952671051025 | KNN Loss: 5.599674701690674 | BCE Loss: 1.0532779693603516\n",
      "Epoch 488 / 500 | iteration 20 / 30 | Total Loss: 6.692938804626465 | KNN Loss: 5.664819240570068 | BCE Loss: 1.0281193256378174\n",
      "Epoch 488 / 500 | iteration 25 / 30 | Total Loss: 6.677836894989014 | KNN Loss: 5.625181674957275 | BCE Loss: 1.0526553392410278\n",
      "Epoch 489 / 500 | iteration 0 / 30 | Total Loss: 6.641971588134766 | KNN Loss: 5.595290184020996 | BCE Loss: 1.0466816425323486\n",
      "Epoch 489 / 500 | iteration 5 / 30 | Total Loss: 6.687981605529785 | KNN Loss: 5.638979911804199 | BCE Loss: 1.049001693725586\n",
      "Epoch 489 / 500 | iteration 10 / 30 | Total Loss: 6.683460712432861 | KNN Loss: 5.637959957122803 | BCE Loss: 1.045500636100769\n",
      "Epoch 489 / 500 | iteration 15 / 30 | Total Loss: 6.670029640197754 | KNN Loss: 5.636886119842529 | BCE Loss: 1.0331432819366455\n",
      "Epoch 489 / 500 | iteration 20 / 30 | Total Loss: 6.633500099182129 | KNN Loss: 5.598089218139648 | BCE Loss: 1.0354108810424805\n",
      "Epoch 489 / 500 | iteration 25 / 30 | Total Loss: 6.690796375274658 | KNN Loss: 5.633678913116455 | BCE Loss: 1.0571174621582031\n",
      "Epoch 490 / 500 | iteration 0 / 30 | Total Loss: 6.722019672393799 | KNN Loss: 5.6493072509765625 | BCE Loss: 1.0727124214172363\n",
      "Epoch 490 / 500 | iteration 5 / 30 | Total Loss: 6.656150817871094 | KNN Loss: 5.615610122680664 | BCE Loss: 1.0405404567718506\n",
      "Epoch 490 / 500 | iteration 10 / 30 | Total Loss: 6.6693220138549805 | KNN Loss: 5.607950210571289 | BCE Loss: 1.0613720417022705\n",
      "Epoch 490 / 500 | iteration 15 / 30 | Total Loss: 6.693173408508301 | KNN Loss: 5.6165242195129395 | BCE Loss: 1.0766491889953613\n",
      "Epoch 490 / 500 | iteration 20 / 30 | Total Loss: 6.6662797927856445 | KNN Loss: 5.626965045928955 | BCE Loss: 1.0393149852752686\n",
      "Epoch 490 / 500 | iteration 25 / 30 | Total Loss: 6.701884746551514 | KNN Loss: 5.63477897644043 | BCE Loss: 1.0671058893203735\n",
      "Epoch 491 / 500 | iteration 0 / 30 | Total Loss: 6.676283836364746 | KNN Loss: 5.641241550445557 | BCE Loss: 1.0350422859191895\n",
      "Epoch 491 / 500 | iteration 5 / 30 | Total Loss: 6.693363189697266 | KNN Loss: 5.640787124633789 | BCE Loss: 1.052575945854187\n",
      "Epoch 491 / 500 | iteration 10 / 30 | Total Loss: 6.634184837341309 | KNN Loss: 5.593795299530029 | BCE Loss: 1.0403897762298584\n",
      "Epoch 491 / 500 | iteration 15 / 30 | Total Loss: 6.698298454284668 | KNN Loss: 5.657973766326904 | BCE Loss: 1.0403248071670532\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 491 / 500 | iteration 20 / 30 | Total Loss: 6.70100212097168 | KNN Loss: 5.629997253417969 | BCE Loss: 1.0710049867630005\n",
      "Epoch 491 / 500 | iteration 25 / 30 | Total Loss: 6.667936325073242 | KNN Loss: 5.608964920043945 | BCE Loss: 1.0589715242385864\n",
      "Epoch 492 / 500 | iteration 0 / 30 | Total Loss: 6.705996513366699 | KNN Loss: 5.622093200683594 | BCE Loss: 1.0839033126831055\n",
      "Epoch 492 / 500 | iteration 5 / 30 | Total Loss: 6.620012283325195 | KNN Loss: 5.596725940704346 | BCE Loss: 1.0232864618301392\n",
      "Epoch 492 / 500 | iteration 10 / 30 | Total Loss: 6.694492816925049 | KNN Loss: 5.644543170928955 | BCE Loss: 1.0499496459960938\n",
      "Epoch 492 / 500 | iteration 15 / 30 | Total Loss: 6.641535758972168 | KNN Loss: 5.601391792297363 | BCE Loss: 1.0401442050933838\n",
      "Epoch 492 / 500 | iteration 20 / 30 | Total Loss: 6.67543363571167 | KNN Loss: 5.627509117126465 | BCE Loss: 1.0479243993759155\n",
      "Epoch 492 / 500 | iteration 25 / 30 | Total Loss: 6.679417133331299 | KNN Loss: 5.596860408782959 | BCE Loss: 1.0825567245483398\n",
      "Epoch 493 / 500 | iteration 0 / 30 | Total Loss: 6.63382625579834 | KNN Loss: 5.614053726196289 | BCE Loss: 1.0197722911834717\n",
      "Epoch 493 / 500 | iteration 5 / 30 | Total Loss: 6.653841495513916 | KNN Loss: 5.631899356842041 | BCE Loss: 1.021942138671875\n",
      "Epoch 493 / 500 | iteration 10 / 30 | Total Loss: 6.651038646697998 | KNN Loss: 5.615721225738525 | BCE Loss: 1.035317301750183\n",
      "Epoch 493 / 500 | iteration 15 / 30 | Total Loss: 6.678149223327637 | KNN Loss: 5.6388983726501465 | BCE Loss: 1.0392507314682007\n",
      "Epoch 493 / 500 | iteration 20 / 30 | Total Loss: 6.666783809661865 | KNN Loss: 5.630500316619873 | BCE Loss: 1.0362834930419922\n",
      "Epoch 493 / 500 | iteration 25 / 30 | Total Loss: 6.720990180969238 | KNN Loss: 5.666237831115723 | BCE Loss: 1.0547525882720947\n",
      "Epoch 494 / 500 | iteration 0 / 30 | Total Loss: 6.629031181335449 | KNN Loss: 5.602931022644043 | BCE Loss: 1.0261003971099854\n",
      "Epoch 494 / 500 | iteration 5 / 30 | Total Loss: 6.707292556762695 | KNN Loss: 5.641931056976318 | BCE Loss: 1.0653612613677979\n",
      "Epoch 494 / 500 | iteration 10 / 30 | Total Loss: 6.733416557312012 | KNN Loss: 5.688399314880371 | BCE Loss: 1.0450172424316406\n",
      "Epoch 494 / 500 | iteration 15 / 30 | Total Loss: 6.6758198738098145 | KNN Loss: 5.6104254722595215 | BCE Loss: 1.065394401550293\n",
      "Epoch 494 / 500 | iteration 20 / 30 | Total Loss: 6.669253349304199 | KNN Loss: 5.626490116119385 | BCE Loss: 1.0427629947662354\n",
      "Epoch 494 / 500 | iteration 25 / 30 | Total Loss: 6.781844139099121 | KNN Loss: 5.735387325286865 | BCE Loss: 1.0464569330215454\n",
      "Epoch 495 / 500 | iteration 0 / 30 | Total Loss: 6.735865592956543 | KNN Loss: 5.668088436126709 | BCE Loss: 1.067777156829834\n",
      "Epoch 495 / 500 | iteration 5 / 30 | Total Loss: 6.672396659851074 | KNN Loss: 5.592857360839844 | BCE Loss: 1.0795390605926514\n",
      "Epoch 495 / 500 | iteration 10 / 30 | Total Loss: 6.629253387451172 | KNN Loss: 5.596874237060547 | BCE Loss: 1.032379150390625\n",
      "Epoch 495 / 500 | iteration 15 / 30 | Total Loss: 6.723840713500977 | KNN Loss: 5.667701244354248 | BCE Loss: 1.0561392307281494\n",
      "Epoch 495 / 500 | iteration 20 / 30 | Total Loss: 6.652331352233887 | KNN Loss: 5.614377975463867 | BCE Loss: 1.0379536151885986\n",
      "Epoch 495 / 500 | iteration 25 / 30 | Total Loss: 6.746912002563477 | KNN Loss: 5.702730655670166 | BCE Loss: 1.0441815853118896\n",
      "Epoch 496 / 500 | iteration 0 / 30 | Total Loss: 6.6756367683410645 | KNN Loss: 5.619561672210693 | BCE Loss: 1.0560749769210815\n",
      "Epoch 496 / 500 | iteration 5 / 30 | Total Loss: 6.651679992675781 | KNN Loss: 5.619797706604004 | BCE Loss: 1.0318825244903564\n",
      "Epoch 496 / 500 | iteration 10 / 30 | Total Loss: 6.739840507507324 | KNN Loss: 5.674345016479492 | BCE Loss: 1.0654953718185425\n",
      "Epoch 496 / 500 | iteration 15 / 30 | Total Loss: 6.711582183837891 | KNN Loss: 5.6791510581970215 | BCE Loss: 1.0324313640594482\n",
      "Epoch 496 / 500 | iteration 20 / 30 | Total Loss: 6.648066520690918 | KNN Loss: 5.599922180175781 | BCE Loss: 1.0481444597244263\n",
      "Epoch 496 / 500 | iteration 25 / 30 | Total Loss: 6.662622451782227 | KNN Loss: 5.6044135093688965 | BCE Loss: 1.058208703994751\n",
      "Epoch 497 / 500 | iteration 0 / 30 | Total Loss: 6.641495704650879 | KNN Loss: 5.614746570587158 | BCE Loss: 1.0267488956451416\n",
      "Epoch 497 / 500 | iteration 5 / 30 | Total Loss: 6.6986894607543945 | KNN Loss: 5.669676303863525 | BCE Loss: 1.02901291847229\n",
      "Epoch 497 / 500 | iteration 10 / 30 | Total Loss: 6.702517509460449 | KNN Loss: 5.669247150421143 | BCE Loss: 1.033270239830017\n",
      "Epoch 497 / 500 | iteration 15 / 30 | Total Loss: 6.672414302825928 | KNN Loss: 5.621455192565918 | BCE Loss: 1.0509589910507202\n",
      "Epoch 497 / 500 | iteration 20 / 30 | Total Loss: 6.6732306480407715 | KNN Loss: 5.617509841918945 | BCE Loss: 1.0557208061218262\n",
      "Epoch 497 / 500 | iteration 25 / 30 | Total Loss: 6.701870918273926 | KNN Loss: 5.648374080657959 | BCE Loss: 1.0534968376159668\n",
      "Epoch 498 / 500 | iteration 0 / 30 | Total Loss: 6.74109411239624 | KNN Loss: 5.690699577331543 | BCE Loss: 1.0503946542739868\n",
      "Epoch 498 / 500 | iteration 5 / 30 | Total Loss: 6.640669822692871 | KNN Loss: 5.605031490325928 | BCE Loss: 1.0356380939483643\n",
      "Epoch 498 / 500 | iteration 10 / 30 | Total Loss: 6.668704986572266 | KNN Loss: 5.610973834991455 | BCE Loss: 1.0577309131622314\n",
      "Epoch 498 / 500 | iteration 15 / 30 | Total Loss: 6.63421630859375 | KNN Loss: 5.593430042266846 | BCE Loss: 1.0407862663269043\n",
      "Epoch 498 / 500 | iteration 20 / 30 | Total Loss: 6.684368133544922 | KNN Loss: 5.636662483215332 | BCE Loss: 1.0477056503295898\n",
      "Epoch 498 / 500 | iteration 25 / 30 | Total Loss: 6.733532428741455 | KNN Loss: 5.669450283050537 | BCE Loss: 1.0640820264816284\n",
      "Epoch 499 / 500 | iteration 0 / 30 | Total Loss: 6.636775970458984 | KNN Loss: 5.614938735961914 | BCE Loss: 1.0218371152877808\n",
      "Epoch 499 / 500 | iteration 5 / 30 | Total Loss: 6.708998203277588 | KNN Loss: 5.6546711921691895 | BCE Loss: 1.0543268918991089\n",
      "Epoch 499 / 500 | iteration 10 / 30 | Total Loss: 6.745306968688965 | KNN Loss: 5.7336249351501465 | BCE Loss: 1.0116822719573975\n",
      "Epoch 499 / 500 | iteration 15 / 30 | Total Loss: 6.635302543640137 | KNN Loss: 5.608983993530273 | BCE Loss: 1.0263185501098633\n",
      "Epoch 499 / 500 | iteration 20 / 30 | Total Loss: 6.705867767333984 | KNN Loss: 5.63930082321167 | BCE Loss: 1.0665671825408936\n",
      "Epoch 499 / 500 | iteration 25 / 30 | Total Loss: 6.648991584777832 | KNN Loss: 5.596693515777588 | BCE Loss: 1.0522979497909546\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "data_iter = torch.utils.data.DataLoader(dataset,\n",
    "                                     batch_size=batch_size,\n",
    "                                     shuffle=True,\n",
    "                                     num_workers=1,\n",
    "                                     pin_memory=True)\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', verbose=True, factor=0.7, threshold=1e-4)\n",
    "knn_crt = KNNLoss(k=k).to(device)\n",
    "losses = []\n",
    "alpha = 10/170\n",
    "gamma = 2\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for iteration, (batch, target) in enumerate(data_iter):\n",
    "        batch = batch.to(device)\n",
    "        target = target.to(device)\n",
    "        outputs, iterm = model(batch, return_intermidiate=True)\n",
    "        mse_loss = F.binary_cross_entropy_with_logits(outputs, target, reduction='none')\n",
    "        mask = torch.ones_like(mse_loss)\n",
    "        mask[target == 0] = alpha ** gamma\n",
    "        mask[target == 1] = (1 - alpha) ** gamma\n",
    "        mse_loss = (mse_loss * mask).sum(dim=-1).mean()\n",
    "        try:\n",
    "            knn_loss = knn_crt(iterm)\n",
    "            if torch.isinf(knn_loss):\n",
    "                knn_loss = 0\n",
    "        except ValueError:\n",
    "            knn_loss = torch.tensor(0)\n",
    "        loss = mse_loss + knn_loss\n",
    "        total_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if iteration % log_every == 0:\n",
    "            print(f\"Epoch {epoch} / {epochs} | iteration {iteration} / {len(data_iter)} | Total Loss: {loss.item()} | KNN Loss: {knn_loss.item()} | BCE Loss: {mse_loss.item()}\")\n",
    "    \n",
    "    scheduler.step(total_loss / (iteration + 1))\n",
    "    losses.append(total_loss / (iteration + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.8217,  3.6125,  2.5152,  3.3619,  3.3702,  0.6665,  2.4555,  1.9713,\n",
      "          2.2067,  1.9203,  2.0551,  2.0604,  0.7355,  1.7611,  1.2391,  1.4528,\n",
      "          2.7113,  3.0813,  2.5865,  2.2356,  1.6618,  2.8783,  2.2024,  2.5269,\n",
      "          2.3406,  1.6601,  2.0477,  1.3680,  1.4390,  0.3112, -0.2259,  0.9774,\n",
      "          0.2020,  0.8970,  1.4721,  1.3503,  0.9529,  3.0087,  0.7396,  1.2688,\n",
      "          0.9522, -0.7081, -0.2568,  2.2572,  2.0168,  0.6907, -0.2034,  0.0830,\n",
      "          1.2786,  2.3213,  1.7738,  0.1577,  1.3278,  0.5104, -0.6213,  1.0817,\n",
      "          1.4182,  1.2694,  1.2766,  1.7575,  0.5297,  0.8090,  0.1186,  1.6593,\n",
      "          1.2432,  1.6040, -1.7948,  0.2945,  2.2321,  2.0777,  2.4627,  0.3653,\n",
      "          1.2580,  2.2580,  1.9304,  1.2560,  0.2079,  0.7257,  0.2168,  1.5396,\n",
      "          0.0038,  0.3593,  1.7438, -0.3736,  0.2129, -1.0330, -2.3198, -0.2471,\n",
      "          0.5458, -1.7872,  0.4219, -0.1311, -0.5523, -0.8504,  0.5359,  1.2067,\n",
      "         -0.6807, -0.6634,  0.3122,  1.0736,  0.6439, -1.1940,  0.8380,  1.0500,\n",
      "         -1.1855, -1.0914, -0.1231,  0.0330, -0.9769, -1.6218, -0.4181, -2.6274,\n",
      "         -0.3424,  1.6940,  1.5277, -0.2486, -0.6089,  0.0232,  1.4846, -2.4922,\n",
      "          0.1818, -0.1849,  0.4845, -0.6472,  0.0043, -0.7248, -0.9232,  0.9429,\n",
      "          0.2457, -0.5491,  0.3305, -0.6204, -1.2236, -0.3226, -0.4981,  0.8267,\n",
      "         -0.4599,  0.1447, -1.8843, -0.9591, -1.3106,  0.5994, -1.8267, -0.9446,\n",
      "         -0.9703, -0.6136, -1.5139, -1.0264, -2.2783, -0.9544, -1.2372, -0.3495,\n",
      "         -1.7077,  0.4769, -1.4198, -0.5122, -3.1203,  0.1576, -0.1619, -0.7365,\n",
      "         -2.1528, -1.6325, -1.1411, -1.2745, -2.2833, -2.3001, -3.1078]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor(-3.1203, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "tensor(3.6125, device='cuda:0', grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "outputs, iterm = model(dataset[67][0].unsqueeze(0).to(device), return_intermidiate=True)\n",
    "print(outputs)\n",
    "print(outputs.min())\n",
    "print(outputs.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "727e4559ea9047f98f6010d4fb6e93e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.transform = torchvision.transforms.Compose([\n",
    "    BinaryEncodingTransform(mapping=dataset.items_to_idx),\n",
    "]\n",
    ")\n",
    "dataset.target_transform = torchvision.transforms.Compose([\n",
    "    BinaryEncodingTransform(mapping=dataset.items_to_idx),\n",
    "]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_ = [d[0].cpu() for d in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:00<00:00, 79.20it/s]\n"
     ]
    }
   ],
   "source": [
    "model = model.eval().cpu()\n",
    "projections = model.calculate_intermidiate(dataset_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "288d07b558da4d679b143d7624e26669",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72a364a9e6824ff6b62c7354b9957310",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "distances = pairwise_distances(projections)\n",
    "# distances = np.triu(distances)\n",
    "distances_f = distances.flatten()\n",
    "\n",
    "plt.matshow(distances)\n",
    "plt.colorbar()\n",
    "plt.figure()\n",
    "plt.hist(distances_f[distances_f > 0], bins=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit DBSCAN and calculate indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = DBSCAN(eps=0.2, min_samples=80).fit_predict(projections)\n",
    "# scores = []\n",
    "# best_score = float('inf')\n",
    "# clusters = None\n",
    "# range_ = list(range(5, 20))\n",
    "# for k in tqdm(range_):\n",
    "#     y = GaussianMixture(n_components=k).fit_predict(projections)\n",
    "#     cur_score = davies_bouldin_score(projections, y)\n",
    "#     scores.append(cur_score)\n",
    "    \n",
    "#     if cur_score < best_score:\n",
    "#         best_score = cur_score\n",
    "#         clusters = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "379d8ee7c41748888c387a00576b1c14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "perplexity = 100\n",
    "\n",
    "p = reduce_dims_and_plot(projections[clusters != -1],\n",
    "                         y=clusters[clusters != -1],\n",
    "                         title=f'perplexity: {perplexity}',\n",
    "                         file_name=None,\n",
    "                         perplexity=perplexity,\n",
    "                         library='Multicore-TSNE',\n",
    "                         perform_PCA=False,\n",
    "                         projected=None,\n",
    "                         figure_type='2d',\n",
    "                         show_figure=True,\n",
    "                         close_figure=False,\n",
    "                         text=None)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn import tree\n",
    "# from sklearn.tree import _tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_dataset = torch.stack(dataset_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = DecisionTreeClassifier(max_depth=200, min_samples_leaf=5)\n",
    "# clf = clf.fit(tensor_dataset[clusters!=-1], clusters[clusters != -1])\n",
    "# print(clf.score(tensor_dataset[clusters!=-1], clusters[clusters != -1]))\n",
    "# print(clf.get_depth())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores = []\n",
    "# for min_samples in range(1,50, 1):\n",
    "#     clf = DecisionTreeClassifier(max_depth=200, min_samples_leaf=min_samples)\n",
    "#     clf = clf.fit(tensor_dataset[clusters!=-1], clusters[clusters != -1])\n",
    "#     scores.append(clf.score(tensor_dataset[clusters!=-1], clusters[clusters != -1]))\n",
    "    \n",
    "# plt.figure()\n",
    "# plt.plot(list(range(1,50, 1)), scores)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rules(tree, feature_names, class_names):\n",
    "    tree_ = tree.tree_\n",
    "    feature_name = [\n",
    "        feature_names[i] if i != _tree.TREE_UNDEFINED else \"undefined!\"\n",
    "        for i in tree_.feature\n",
    "    ]\n",
    "\n",
    "    paths = []\n",
    "    path = []\n",
    "    \n",
    "    def recurse(node, path, paths):\n",
    "        if tree_.feature[node] != _tree.TREE_UNDEFINED:\n",
    "            name = feature_name[node]\n",
    "            threshold = tree_.threshold[node]\n",
    "            p1, p2 = list(path), list(path)\n",
    "#             p1 += [f\"({name} <= {np.round(threshold, 3)})\"]\n",
    "            p1 += [(name, '<=', np.round(threshold, 3))]\n",
    "            recurse(tree_.children_left[node], p1, paths)\n",
    "            p2 += [(name, '>', np.round(threshold, 3))]\n",
    "            recurse(tree_.children_right[node], p2, paths)\n",
    "        else:\n",
    "            path += [(tree_.value[node], tree_.n_node_samples[node])]\n",
    "            paths += [path]\n",
    "            \n",
    "    recurse(0, path, paths)\n",
    "\n",
    "    # sort by samples count\n",
    "    samples_count = [p[-1][1] for p in paths]\n",
    "    ii = list(np.argsort(samples_count))\n",
    "    paths = [paths[i] for i in reversed(ii)]\n",
    "    \n",
    "    rules = []\n",
    "    for path in paths:\n",
    "        rule = []\n",
    "        \n",
    "        for p in path[:-1]:\n",
    "            rule += [p]\n",
    "        target = \" then \"\n",
    "        if class_names is None:\n",
    "            target += \"response: \"+str(np.round(path[-1][0][0][0],3))\n",
    "        else:\n",
    "            classes = path[-1][0][0]\n",
    "            l = np.argmax(classes)\n",
    "            target += f\"class: {class_names[l]} (proba: {np.round(100.0*classes[l]/np.sum(classes),2)}%)\"\n",
    "           \n",
    "        proba = np.round(100.0*classes[l]/np.sum(classes),2)\n",
    "        target += f\" | based on {path[-1][1]:,} samples\"\n",
    "        rule_wrapper = {'target': target, 'rule': rule, 'proba': proba}\n",
    "        rules += [rule_wrapper]\n",
    "        \n",
    "    return rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rules = get_rules(clf, dataset.items, clusters[clusters != -1])\n",
    "\n",
    "# for rule in rules:\n",
    "#     n_pos = 0\n",
    "#     for c,p,v in rule['rule']:\n",
    "#         if p == '>':\n",
    "#             n_pos += 1\n",
    "#     rule['pos'] = n_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure()\n",
    "# probs = [r['proba'] for r in rules]\n",
    "# plt.hist(probs, bins = 100)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rules = sorted(rules, key=lambda x:x['pos'])\n",
    "# rules = [r for r in rules if r['proba'] > 50]\n",
    "# print(len(rules))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(17):\n",
    "#     r_i = rules[i]\n",
    "#     print(f\"------------- rule {i} length {len(r_i)} -------------\")\n",
    "#     print(r_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Soft-Decision-Tree given the self-labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_dataset = list(zip(tensor_dataset[clusters!=-1], clusters[clusters != -1]))\n",
    "batch_size = 512\n",
    "tree_loader = torch.utils.data.DataLoader(tree_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define how we prune the weights of a node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_node(node_weights, factor=1):\n",
    "    w = node_weights.cpu().detach().numpy()\n",
    "    mean_ = np.mean(w)\n",
    "    std_ = np.std(w)\n",
    "    node_weights[((mean_ - std_ * factor) < node_weights) & (node_weights < (mean_ + std_ * factor))] = 0\n",
    "    return node_weights\n",
    "\n",
    "def prune_node_keep(node_weights, keep=4):\n",
    "    w = node_weights.cpu().detach().numpy()\n",
    "    throw_idx = np.argsort(abs(w))[:-keep]\n",
    "    node_weights[throw_idx] = 0\n",
    "    return node_weights\n",
    "\n",
    "def prune_tree(tree_, factor):\n",
    "    new_weights = tree_.inner_nodes.weight.clone()\n",
    "    for i in range(new_weights.shape[0]):\n",
    "        res = prune_node_keep(new_weights[i, :], factor)\n",
    "        new_weights[i, :] = res\n",
    "\n",
    "    with torch.no_grad():\n",
    "        tree_.inner_nodes.weight.copy_(new_weights)\n",
    "        \n",
    "def sparseness(x):\n",
    "    s = []\n",
    "    for i in range(x.shape[0]):\n",
    "        x_ = x[i, :]\n",
    "        sp = (len(x_) - torch.norm(x_, 0).item()) / len(x_)\n",
    "        s.append(sp)\n",
    "    return np.mean(s)\n",
    "\n",
    "def compute_regularization_by_level(tree):\n",
    "    total_reg = 0\n",
    "    for i in range(tree.inner_nodes.weight.shape[0]):\n",
    "        cur_level = np.floor(np.log2(i+1))\n",
    "        node_reg = torch.norm(tree.inner_nodes.weight[i].view(-1), 2)\n",
    "        total_reg += 2**(-cur_level) * node_reg\n",
    "    return total_reg\n",
    "\n",
    "def show_sparseness(tree):\n",
    "    avg_sp = sparseness(tree.inner_nodes.weight)\n",
    "    print(f\"Average sparseness: {avg_sp}\")\n",
    "    layer = 0\n",
    "    sps = []\n",
    "    for i in range(tree.inner_nodes.weight.shape[0]):\n",
    "        cur_layer = int(np.floor(np.log2(i+1)))\n",
    "        if cur_layer != layer:\n",
    "            print(f\"layer {layer}: {np.mean(sps)}\")\n",
    "            sps = []\n",
    "            layer = cur_layer\n",
    "\n",
    "        x_ = tree.inner_nodes.weight[i, :]\n",
    "        sp = (len(x_) - torch.norm(x_, 0).item()) / len(x_)\n",
    "        sps.append(sp)\n",
    "        \n",
    "    return avg_sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_epoch(model, loader, device, log_interval, losses, accs, epoch, iteration):\n",
    "    model = model.train()\n",
    "    for batch_idx, (data, target) in enumerate(loader):\n",
    "        iteration += 1\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        output, penalty = tree.forward(data)\n",
    "\n",
    "        # Loss\n",
    "        loss_tree = criterion(output, target.view(-1))\n",
    "\n",
    "        # Penalty\n",
    "        loss_tree += penalty\n",
    "\n",
    "        # Sparse regularization\n",
    "#         fc_params = torch.cat([x.view(-1) for x in tree.inner_nodes.parameters()])\n",
    "#         regularization = sparsity_lamda * torch.norm(fc_params, 2)\n",
    "        regularization = sparsity_lamda * compute_regularization_by_level(tree)\n",
    "        loss = loss_tree\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        pred = output.data.max(1)[1]\n",
    "        correct = pred.eq(target.view(-1).data).sum()\n",
    "        accs.append(correct.item() / data.size()[0])\n",
    "\n",
    "        # Print training status\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(f\"Epoch: {epoch:02d} | Batch: {batch_idx:03d} / {len(loader):03d} | Total loss: {loss.item():.3f} | Reg loss: {regularization.item():.3f} | Tree loss: {loss_tree.item():.3f} | Accuracy: {correct.item() / data.size()[0]:03f} | {round((time.time() - start_time) / iteration, 3)} sec/iter\")\n",
    "            \n",
    "    return iteration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 5e-3\n",
    "weight_decay = 5e-4\n",
    "sparsity_lamda = 2e-3\n",
    "epochs = 100\n",
    "output_dim = len(set(clusters))\n",
    "log_interval = 1\n",
    "use_cuda = device != 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = SDT(input_dim=tensor_dataset.shape[1], output_dim=len(clusters - 1), depth=tree_depth, lamda=1e-3, use_cuda=use_cuda)\n",
    "optimizer = torch.optim.Adam(tree.parameters(),\n",
    "                                 lr=lr,\n",
    "                                 weight_decay=weight_decay)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "tree = tree.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "accs = []\n",
    "sparsity = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.0\n",
      "layer 0: 0.0\n",
      "layer 1: 0.0\n",
      "layer 2: 0.0\n",
      "layer 3: 0.0\n",
      "layer 4: 0.0\n",
      "layer 5: 0.0\n",
      "layer 6: 0.0\n",
      "layer 7: 0.0\n",
      "layer 8: 0.0\n",
      "layer 9: 0.0\n",
      "layer 10: 0.0\n",
      "Epoch: 00 | Batch: 000 / 030 | Total loss: 9.630 | Reg loss: 0.014 | Tree loss: 9.630 | Accuracy: 0.000000 | 12.32 sec/iter\n",
      "Epoch: 00 | Batch: 001 / 030 | Total loss: 9.622 | Reg loss: 0.013 | Tree loss: 9.622 | Accuracy: 0.000000 | 9.791 sec/iter\n",
      "Epoch: 00 | Batch: 002 / 030 | Total loss: 9.615 | Reg loss: 0.012 | Tree loss: 9.615 | Accuracy: 0.000000 | 8.947 sec/iter\n",
      "Epoch: 00 | Batch: 003 / 030 | Total loss: 9.607 | Reg loss: 0.011 | Tree loss: 9.607 | Accuracy: 0.000000 | 8.54 sec/iter\n",
      "Epoch: 00 | Batch: 004 / 030 | Total loss: 9.600 | Reg loss: 0.010 | Tree loss: 9.600 | Accuracy: 0.005859 | 8.291 sec/iter\n",
      "Epoch: 00 | Batch: 005 / 030 | Total loss: 9.592 | Reg loss: 0.010 | Tree loss: 9.592 | Accuracy: 0.019531 | 8.133 sec/iter\n",
      "Epoch: 00 | Batch: 006 / 030 | Total loss: 9.584 | Reg loss: 0.009 | Tree loss: 9.584 | Accuracy: 0.113281 | 7.973 sec/iter\n",
      "Epoch: 00 | Batch: 007 / 030 | Total loss: 9.577 | Reg loss: 0.009 | Tree loss: 9.577 | Accuracy: 0.210938 | 7.83 sec/iter\n",
      "Epoch: 00 | Batch: 008 / 030 | Total loss: 9.570 | Reg loss: 0.008 | Tree loss: 9.570 | Accuracy: 0.373047 | 7.699 sec/iter\n",
      "Epoch: 00 | Batch: 009 / 030 | Total loss: 9.564 | Reg loss: 0.008 | Tree loss: 9.564 | Accuracy: 0.490234 | 7.589 sec/iter\n",
      "Epoch: 00 | Batch: 010 / 030 | Total loss: 9.556 | Reg loss: 0.008 | Tree loss: 9.556 | Accuracy: 0.550781 | 7.5 sec/iter\n",
      "Epoch: 00 | Batch: 011 / 030 | Total loss: 9.549 | Reg loss: 0.008 | Tree loss: 9.549 | Accuracy: 0.554688 | 7.432 sec/iter\n",
      "Epoch: 00 | Batch: 012 / 030 | Total loss: 9.542 | Reg loss: 0.008 | Tree loss: 9.542 | Accuracy: 0.529297 | 7.377 sec/iter\n",
      "Epoch: 00 | Batch: 013 / 030 | Total loss: 9.536 | Reg loss: 0.008 | Tree loss: 9.536 | Accuracy: 0.570312 | 7.329 sec/iter\n",
      "Epoch: 00 | Batch: 014 / 030 | Total loss: 9.529 | Reg loss: 0.009 | Tree loss: 9.529 | Accuracy: 0.585938 | 7.294 sec/iter\n",
      "Epoch: 00 | Batch: 015 / 030 | Total loss: 9.525 | Reg loss: 0.009 | Tree loss: 9.525 | Accuracy: 0.566406 | 7.271 sec/iter\n",
      "Epoch: 00 | Batch: 016 / 030 | Total loss: 9.516 | Reg loss: 0.009 | Tree loss: 9.516 | Accuracy: 0.628906 | 7.261 sec/iter\n",
      "Epoch: 00 | Batch: 017 / 030 | Total loss: 9.513 | Reg loss: 0.010 | Tree loss: 9.513 | Accuracy: 0.603516 | 7.255 sec/iter\n",
      "Epoch: 00 | Batch: 018 / 030 | Total loss: 9.504 | Reg loss: 0.010 | Tree loss: 9.504 | Accuracy: 0.593750 | 7.249 sec/iter\n",
      "Epoch: 00 | Batch: 019 / 030 | Total loss: 9.498 | Reg loss: 0.010 | Tree loss: 9.498 | Accuracy: 0.603516 | 7.238 sec/iter\n",
      "Epoch: 00 | Batch: 020 / 030 | Total loss: 9.491 | Reg loss: 0.011 | Tree loss: 9.491 | Accuracy: 0.591797 | 7.204 sec/iter\n",
      "Epoch: 00 | Batch: 021 / 030 | Total loss: 9.486 | Reg loss: 0.011 | Tree loss: 9.486 | Accuracy: 0.564453 | 7.116 sec/iter\n",
      "Epoch: 00 | Batch: 022 / 030 | Total loss: 9.482 | Reg loss: 0.012 | Tree loss: 9.482 | Accuracy: 0.578125 | 7.034 sec/iter\n",
      "Epoch: 00 | Batch: 023 / 030 | Total loss: 9.476 | Reg loss: 0.012 | Tree loss: 9.476 | Accuracy: 0.552734 | 6.956 sec/iter\n",
      "Epoch: 00 | Batch: 024 / 030 | Total loss: 9.468 | Reg loss: 0.012 | Tree loss: 9.468 | Accuracy: 0.578125 | 6.961 sec/iter\n",
      "Epoch: 00 | Batch: 025 / 030 | Total loss: 9.462 | Reg loss: 0.013 | Tree loss: 9.462 | Accuracy: 0.595703 | 6.909 sec/iter\n",
      "Epoch: 00 | Batch: 026 / 030 | Total loss: 9.459 | Reg loss: 0.013 | Tree loss: 9.459 | Accuracy: 0.607422 | 6.925 sec/iter\n",
      "Epoch: 00 | Batch: 027 / 030 | Total loss: 9.451 | Reg loss: 0.013 | Tree loss: 9.451 | Accuracy: 0.576172 | 6.936 sec/iter\n",
      "Epoch: 00 | Batch: 028 / 030 | Total loss: 9.452 | Reg loss: 0.013 | Tree loss: 9.452 | Accuracy: 0.535156 | 6.947 sec/iter\n",
      "Epoch: 00 | Batch: 029 / 030 | Total loss: 9.437 | Reg loss: 0.014 | Tree loss: 9.437 | Accuracy: 0.650485 | 6.824 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 01 | Batch: 000 / 030 | Total loss: 9.511 | Reg loss: 0.006 | Tree loss: 9.511 | Accuracy: 0.591797 | 8.221 sec/iter\n",
      "Epoch: 01 | Batch: 001 / 030 | Total loss: 9.502 | Reg loss: 0.006 | Tree loss: 9.502 | Accuracy: 0.626953 | 8.181 sec/iter\n",
      "Epoch: 01 | Batch: 002 / 030 | Total loss: 9.502 | Reg loss: 0.006 | Tree loss: 9.502 | Accuracy: 0.564453 | 8.141 sec/iter\n",
      "Epoch: 01 | Batch: 003 / 030 | Total loss: 9.497 | Reg loss: 0.006 | Tree loss: 9.497 | Accuracy: 0.566406 | 8.105 sec/iter\n",
      "Epoch: 01 | Batch: 004 / 030 | Total loss: 9.487 | Reg loss: 0.007 | Tree loss: 9.487 | Accuracy: 0.587891 | 8.069 sec/iter\n",
      "Epoch: 01 | Batch: 005 / 030 | Total loss: 9.479 | Reg loss: 0.007 | Tree loss: 9.479 | Accuracy: 0.609375 | 8.036 sec/iter\n",
      "Epoch: 01 | Batch: 006 / 030 | Total loss: 9.476 | Reg loss: 0.007 | Tree loss: 9.476 | Accuracy: 0.566406 | 8.007 sec/iter\n",
      "Epoch: 01 | Batch: 007 / 030 | Total loss: 9.467 | Reg loss: 0.008 | Tree loss: 9.467 | Accuracy: 0.572266 | 7.975 sec/iter\n",
      "Epoch: 01 | Batch: 008 / 030 | Total loss: 9.462 | Reg loss: 0.008 | Tree loss: 9.462 | Accuracy: 0.562500 | 7.944 sec/iter\n",
      "Epoch: 01 | Batch: 009 / 030 | Total loss: 9.451 | Reg loss: 0.009 | Tree loss: 9.451 | Accuracy: 0.609375 | 7.918 sec/iter\n",
      "Epoch: 01 | Batch: 010 / 030 | Total loss: 9.445 | Reg loss: 0.009 | Tree loss: 9.445 | Accuracy: 0.582031 | 7.894 sec/iter\n",
      "Epoch: 01 | Batch: 011 / 030 | Total loss: 9.445 | Reg loss: 0.009 | Tree loss: 9.445 | Accuracy: 0.562500 | 7.871 sec/iter\n",
      "Epoch: 01 | Batch: 012 / 030 | Total loss: 9.435 | Reg loss: 0.010 | Tree loss: 9.435 | Accuracy: 0.613281 | 7.84 sec/iter\n",
      "Epoch: 01 | Batch: 013 / 030 | Total loss: 9.429 | Reg loss: 0.010 | Tree loss: 9.429 | Accuracy: 0.589844 | 7.779 sec/iter\n",
      "Epoch: 01 | Batch: 014 / 030 | Total loss: 9.425 | Reg loss: 0.011 | Tree loss: 9.425 | Accuracy: 0.603516 | 7.719 sec/iter\n",
      "Epoch: 01 | Batch: 015 / 030 | Total loss: 9.418 | Reg loss: 0.011 | Tree loss: 9.418 | Accuracy: 0.599609 | 7.661 sec/iter\n",
      "Epoch: 01 | Batch: 016 / 030 | Total loss: 9.415 | Reg loss: 0.012 | Tree loss: 9.415 | Accuracy: 0.560547 | 7.598 sec/iter\n",
      "Epoch: 01 | Batch: 017 / 030 | Total loss: 9.407 | Reg loss: 0.012 | Tree loss: 9.407 | Accuracy: 0.562500 | 7.554 sec/iter\n",
      "Epoch: 01 | Batch: 018 / 030 | Total loss: 9.403 | Reg loss: 0.012 | Tree loss: 9.403 | Accuracy: 0.583984 | 7.509 sec/iter\n",
      "Epoch: 01 | Batch: 019 / 030 | Total loss: 9.404 | Reg loss: 0.013 | Tree loss: 9.404 | Accuracy: 0.570312 | 7.506 sec/iter\n",
      "Epoch: 01 | Batch: 020 / 030 | Total loss: 9.388 | Reg loss: 0.013 | Tree loss: 9.388 | Accuracy: 0.621094 | 7.503 sec/iter\n",
      "Epoch: 01 | Batch: 021 / 030 | Total loss: 9.390 | Reg loss: 0.014 | Tree loss: 9.390 | Accuracy: 0.578125 | 7.497 sec/iter\n",
      "Epoch: 01 | Batch: 022 / 030 | Total loss: 9.382 | Reg loss: 0.014 | Tree loss: 9.382 | Accuracy: 0.576172 | 7.492 sec/iter\n",
      "Epoch: 01 | Batch: 023 / 030 | Total loss: 9.374 | Reg loss: 0.014 | Tree loss: 9.374 | Accuracy: 0.587891 | 7.488 sec/iter\n",
      "Epoch: 01 | Batch: 024 / 030 | Total loss: 9.379 | Reg loss: 0.015 | Tree loss: 9.379 | Accuracy: 0.554688 | 7.482 sec/iter\n",
      "Epoch: 01 | Batch: 025 / 030 | Total loss: 9.367 | Reg loss: 0.015 | Tree loss: 9.367 | Accuracy: 0.560547 | 7.477 sec/iter\n",
      "Epoch: 01 | Batch: 026 / 030 | Total loss: 9.360 | Reg loss: 0.015 | Tree loss: 9.360 | Accuracy: 0.576172 | 7.471 sec/iter\n",
      "Epoch: 01 | Batch: 027 / 030 | Total loss: 9.358 | Reg loss: 0.016 | Tree loss: 9.358 | Accuracy: 0.593750 | 7.466 sec/iter\n",
      "Epoch: 01 | Batch: 028 / 030 | Total loss: 9.355 | Reg loss: 0.016 | Tree loss: 9.355 | Accuracy: 0.580078 | 7.462 sec/iter\n",
      "Epoch: 01 | Batch: 029 / 030 | Total loss: 9.349 | Reg loss: 0.016 | Tree loss: 9.349 | Accuracy: 0.514563 | 7.391 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 02 | Batch: 000 / 030 | Total loss: 9.430 | Reg loss: 0.009 | Tree loss: 9.430 | Accuracy: 0.578125 | 8.098 sec/iter\n",
      "Epoch: 02 | Batch: 001 / 030 | Total loss: 9.427 | Reg loss: 0.009 | Tree loss: 9.427 | Accuracy: 0.576172 | 8.081 sec/iter\n",
      "Epoch: 02 | Batch: 002 / 030 | Total loss: 9.422 | Reg loss: 0.009 | Tree loss: 9.422 | Accuracy: 0.552734 | 8.064 sec/iter\n",
      "Epoch: 02 | Batch: 003 / 030 | Total loss: 9.416 | Reg loss: 0.010 | Tree loss: 9.416 | Accuracy: 0.593750 | 8.047 sec/iter\n",
      "Epoch: 02 | Batch: 004 / 030 | Total loss: 9.408 | Reg loss: 0.010 | Tree loss: 9.408 | Accuracy: 0.601562 | 8.031 sec/iter\n",
      "Epoch: 02 | Batch: 005 / 030 | Total loss: 9.401 | Reg loss: 0.010 | Tree loss: 9.401 | Accuracy: 0.576172 | 7.986 sec/iter\n",
      "Epoch: 02 | Batch: 006 / 030 | Total loss: 9.396 | Reg loss: 0.010 | Tree loss: 9.396 | Accuracy: 0.585938 | 7.94 sec/iter\n",
      "Epoch: 02 | Batch: 007 / 030 | Total loss: 9.392 | Reg loss: 0.011 | Tree loss: 9.392 | Accuracy: 0.576172 | 7.893 sec/iter\n",
      "Epoch: 02 | Batch: 008 / 030 | Total loss: 9.385 | Reg loss: 0.011 | Tree loss: 9.385 | Accuracy: 0.556641 | 7.847 sec/iter\n",
      "Epoch: 02 | Batch: 009 / 030 | Total loss: 9.375 | Reg loss: 0.011 | Tree loss: 9.375 | Accuracy: 0.564453 | 7.791 sec/iter\n",
      "Epoch: 02 | Batch: 010 / 030 | Total loss: 9.370 | Reg loss: 0.012 | Tree loss: 9.370 | Accuracy: 0.583984 | 7.758 sec/iter\n",
      "Epoch: 02 | Batch: 011 / 030 | Total loss: 9.362 | Reg loss: 0.012 | Tree loss: 9.362 | Accuracy: 0.582031 | 7.723 sec/iter\n",
      "Epoch: 02 | Batch: 012 / 030 | Total loss: 9.357 | Reg loss: 0.013 | Tree loss: 9.357 | Accuracy: 0.570312 | 7.718 sec/iter\n",
      "Epoch: 02 | Batch: 013 / 030 | Total loss: 9.347 | Reg loss: 0.013 | Tree loss: 9.347 | Accuracy: 0.599609 | 7.712 sec/iter\n",
      "Epoch: 02 | Batch: 014 / 030 | Total loss: 9.343 | Reg loss: 0.014 | Tree loss: 9.343 | Accuracy: 0.582031 | 7.707 sec/iter\n",
      "Epoch: 02 | Batch: 015 / 030 | Total loss: 9.335 | Reg loss: 0.014 | Tree loss: 9.335 | Accuracy: 0.570312 | 7.701 sec/iter\n",
      "Epoch: 02 | Batch: 016 / 030 | Total loss: 9.330 | Reg loss: 0.015 | Tree loss: 9.330 | Accuracy: 0.582031 | 7.696 sec/iter\n",
      "Epoch: 02 | Batch: 017 / 030 | Total loss: 9.316 | Reg loss: 0.015 | Tree loss: 9.316 | Accuracy: 0.597656 | 7.69 sec/iter\n",
      "Epoch: 02 | Batch: 018 / 030 | Total loss: 9.315 | Reg loss: 0.015 | Tree loss: 9.315 | Accuracy: 0.578125 | 7.684 sec/iter\n",
      "Epoch: 02 | Batch: 019 / 030 | Total loss: 9.304 | Reg loss: 0.016 | Tree loss: 9.304 | Accuracy: 0.599609 | 7.677 sec/iter\n",
      "Epoch: 02 | Batch: 020 / 030 | Total loss: 9.300 | Reg loss: 0.016 | Tree loss: 9.300 | Accuracy: 0.619141 | 7.67 sec/iter\n",
      "Epoch: 02 | Batch: 021 / 030 | Total loss: 9.291 | Reg loss: 0.017 | Tree loss: 9.291 | Accuracy: 0.605469 | 7.662 sec/iter\n",
      "Epoch: 02 | Batch: 022 / 030 | Total loss: 9.281 | Reg loss: 0.017 | Tree loss: 9.281 | Accuracy: 0.609375 | 7.655 sec/iter\n",
      "Epoch: 02 | Batch: 023 / 030 | Total loss: 9.277 | Reg loss: 0.017 | Tree loss: 9.277 | Accuracy: 0.576172 | 7.648 sec/iter\n",
      "Epoch: 02 | Batch: 024 / 030 | Total loss: 9.271 | Reg loss: 0.018 | Tree loss: 9.271 | Accuracy: 0.566406 | 7.641 sec/iter\n",
      "Epoch: 02 | Batch: 025 / 030 | Total loss: 9.255 | Reg loss: 0.018 | Tree loss: 9.255 | Accuracy: 0.582031 | 7.633 sec/iter\n",
      "Epoch: 02 | Batch: 026 / 030 | Total loss: 9.257 | Reg loss: 0.019 | Tree loss: 9.257 | Accuracy: 0.568359 | 7.625 sec/iter\n",
      "Epoch: 02 | Batch: 027 / 030 | Total loss: 9.243 | Reg loss: 0.019 | Tree loss: 9.243 | Accuracy: 0.572266 | 7.617 sec/iter\n",
      "Epoch: 02 | Batch: 028 / 030 | Total loss: 9.235 | Reg loss: 0.019 | Tree loss: 9.235 | Accuracy: 0.580078 | 7.61 sec/iter\n",
      "Epoch: 02 | Batch: 029 / 030 | Total loss: 9.196 | Reg loss: 0.020 | Tree loss: 9.196 | Accuracy: 0.650485 | 7.562 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 03 | Batch: 000 / 030 | Total loss: 9.357 | Reg loss: 0.012 | Tree loss: 9.357 | Accuracy: 0.558594 | 7.915 sec/iter\n",
      "Epoch: 03 | Batch: 001 / 030 | Total loss: 9.348 | Reg loss: 0.012 | Tree loss: 9.348 | Accuracy: 0.578125 | 7.885 sec/iter\n",
      "Epoch: 03 | Batch: 002 / 030 | Total loss: 9.344 | Reg loss: 0.012 | Tree loss: 9.344 | Accuracy: 0.574219 | 7.853 sec/iter\n",
      "Epoch: 03 | Batch: 003 / 030 | Total loss: 9.336 | Reg loss: 0.013 | Tree loss: 9.336 | Accuracy: 0.576172 | 7.844 sec/iter\n",
      "Epoch: 03 | Batch: 004 / 030 | Total loss: 9.329 | Reg loss: 0.013 | Tree loss: 9.329 | Accuracy: 0.552734 | 7.816 sec/iter\n",
      "Epoch: 03 | Batch: 005 / 030 | Total loss: 9.313 | Reg loss: 0.013 | Tree loss: 9.313 | Accuracy: 0.609375 | 7.787 sec/iter\n",
      "Epoch: 03 | Batch: 006 / 030 | Total loss: 9.313 | Reg loss: 0.013 | Tree loss: 9.313 | Accuracy: 0.535156 | 7.758 sec/iter\n",
      "Epoch: 03 | Batch: 007 / 030 | Total loss: 9.295 | Reg loss: 0.014 | Tree loss: 9.295 | Accuracy: 0.601562 | 7.749 sec/iter\n",
      "Epoch: 03 | Batch: 008 / 030 | Total loss: 9.284 | Reg loss: 0.014 | Tree loss: 9.284 | Accuracy: 0.597656 | 7.738 sec/iter\n",
      "Epoch: 03 | Batch: 009 / 030 | Total loss: 9.280 | Reg loss: 0.014 | Tree loss: 9.280 | Accuracy: 0.560547 | 7.727 sec/iter\n",
      "Epoch: 03 | Batch: 010 / 030 | Total loss: 9.266 | Reg loss: 0.015 | Tree loss: 9.266 | Accuracy: 0.591797 | 7.716 sec/iter\n",
      "Epoch: 03 | Batch: 011 / 030 | Total loss: 9.257 | Reg loss: 0.015 | Tree loss: 9.257 | Accuracy: 0.582031 | 7.706 sec/iter\n",
      "Epoch: 03 | Batch: 012 / 030 | Total loss: 9.239 | Reg loss: 0.016 | Tree loss: 9.239 | Accuracy: 0.644531 | 7.695 sec/iter\n",
      "Epoch: 03 | Batch: 013 / 030 | Total loss: 9.237 | Reg loss: 0.016 | Tree loss: 9.237 | Accuracy: 0.603516 | 7.687 sec/iter\n",
      "Epoch: 03 | Batch: 014 / 030 | Total loss: 9.220 | Reg loss: 0.017 | Tree loss: 9.220 | Accuracy: 0.607422 | 7.679 sec/iter\n",
      "Epoch: 03 | Batch: 015 / 030 | Total loss: 9.219 | Reg loss: 0.017 | Tree loss: 9.219 | Accuracy: 0.564453 | 7.673 sec/iter\n",
      "Epoch: 03 | Batch: 016 / 030 | Total loss: 9.198 | Reg loss: 0.017 | Tree loss: 9.198 | Accuracy: 0.593750 | 7.667 sec/iter\n",
      "Epoch: 03 | Batch: 017 / 030 | Total loss: 9.189 | Reg loss: 0.018 | Tree loss: 9.189 | Accuracy: 0.613281 | 7.662 sec/iter\n",
      "Epoch: 03 | Batch: 018 / 030 | Total loss: 9.185 | Reg loss: 0.018 | Tree loss: 9.185 | Accuracy: 0.583984 | 7.657 sec/iter\n",
      "Epoch: 03 | Batch: 019 / 030 | Total loss: 9.175 | Reg loss: 0.019 | Tree loss: 9.175 | Accuracy: 0.550781 | 7.653 sec/iter\n",
      "Epoch: 03 | Batch: 020 / 030 | Total loss: 9.157 | Reg loss: 0.019 | Tree loss: 9.157 | Accuracy: 0.564453 | 7.648 sec/iter\n",
      "Epoch: 03 | Batch: 021 / 030 | Total loss: 9.149 | Reg loss: 0.020 | Tree loss: 9.149 | Accuracy: 0.583984 | 7.644 sec/iter\n",
      "Epoch: 03 | Batch: 022 / 030 | Total loss: 9.139 | Reg loss: 0.020 | Tree loss: 9.139 | Accuracy: 0.552734 | 7.639 sec/iter\n",
      "Epoch: 03 | Batch: 023 / 030 | Total loss: 9.116 | Reg loss: 0.020 | Tree loss: 9.116 | Accuracy: 0.595703 | 7.635 sec/iter\n",
      "Epoch: 03 | Batch: 024 / 030 | Total loss: 9.109 | Reg loss: 0.021 | Tree loss: 9.109 | Accuracy: 0.585938 | 7.631 sec/iter\n",
      "Epoch: 03 | Batch: 025 / 030 | Total loss: 9.105 | Reg loss: 0.021 | Tree loss: 9.105 | Accuracy: 0.583984 | 7.627 sec/iter\n",
      "Epoch: 03 | Batch: 026 / 030 | Total loss: 9.075 | Reg loss: 0.021 | Tree loss: 9.075 | Accuracy: 0.568359 | 7.622 sec/iter\n",
      "Epoch: 03 | Batch: 027 / 030 | Total loss: 9.068 | Reg loss: 0.022 | Tree loss: 9.068 | Accuracy: 0.587891 | 7.618 sec/iter\n",
      "Epoch: 03 | Batch: 028 / 030 | Total loss: 9.059 | Reg loss: 0.022 | Tree loss: 9.059 | Accuracy: 0.601562 | 7.613 sec/iter\n",
      "Epoch: 03 | Batch: 029 / 030 | Total loss: 9.049 | Reg loss: 0.022 | Tree loss: 9.049 | Accuracy: 0.563107 | 7.577 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 04 | Batch: 000 / 030 | Total loss: 9.251 | Reg loss: 0.015 | Tree loss: 9.251 | Accuracy: 0.560547 | 7.648 sec/iter\n",
      "Epoch: 04 | Batch: 001 / 030 | Total loss: 9.238 | Reg loss: 0.015 | Tree loss: 9.238 | Accuracy: 0.585938 | 7.626 sec/iter\n",
      "Epoch: 04 | Batch: 002 / 030 | Total loss: 9.225 | Reg loss: 0.015 | Tree loss: 9.225 | Accuracy: 0.605469 | 7.621 sec/iter\n",
      "Epoch: 04 | Batch: 003 / 030 | Total loss: 9.219 | Reg loss: 0.015 | Tree loss: 9.219 | Accuracy: 0.570312 | 7.614 sec/iter\n",
      "Epoch: 04 | Batch: 004 / 030 | Total loss: 9.201 | Reg loss: 0.015 | Tree loss: 9.201 | Accuracy: 0.589844 | 7.595 sec/iter\n",
      "Epoch: 04 | Batch: 005 / 030 | Total loss: 9.186 | Reg loss: 0.016 | Tree loss: 9.186 | Accuracy: 0.595703 | 7.574 sec/iter\n",
      "Epoch: 04 | Batch: 006 / 030 | Total loss: 9.179 | Reg loss: 0.016 | Tree loss: 9.179 | Accuracy: 0.564453 | 7.552 sec/iter\n",
      "Epoch: 04 | Batch: 007 / 030 | Total loss: 9.150 | Reg loss: 0.016 | Tree loss: 9.150 | Accuracy: 0.603516 | 7.53 sec/iter\n",
      "Epoch: 04 | Batch: 008 / 030 | Total loss: 9.134 | Reg loss: 0.016 | Tree loss: 9.134 | Accuracy: 0.580078 | 7.508 sec/iter\n",
      "Epoch: 04 | Batch: 009 / 030 | Total loss: 9.123 | Reg loss: 0.017 | Tree loss: 9.123 | Accuracy: 0.595703 | 7.487 sec/iter\n",
      "Epoch: 04 | Batch: 010 / 030 | Total loss: 9.098 | Reg loss: 0.017 | Tree loss: 9.098 | Accuracy: 0.634766 | 7.486 sec/iter\n",
      "Epoch: 04 | Batch: 011 / 030 | Total loss: 9.096 | Reg loss: 0.018 | Tree loss: 9.096 | Accuracy: 0.582031 | 7.485 sec/iter\n",
      "Epoch: 04 | Batch: 012 / 030 | Total loss: 9.072 | Reg loss: 0.018 | Tree loss: 9.072 | Accuracy: 0.580078 | 7.485 sec/iter\n",
      "Epoch: 04 | Batch: 013 / 030 | Total loss: 9.047 | Reg loss: 0.018 | Tree loss: 9.047 | Accuracy: 0.607422 | 7.484 sec/iter\n",
      "Epoch: 04 | Batch: 014 / 030 | Total loss: 9.036 | Reg loss: 0.019 | Tree loss: 9.036 | Accuracy: 0.587891 | 7.483 sec/iter\n",
      "Epoch: 04 | Batch: 015 / 030 | Total loss: 9.017 | Reg loss: 0.019 | Tree loss: 9.017 | Accuracy: 0.556641 | 7.482 sec/iter\n",
      "Epoch: 04 | Batch: 016 / 030 | Total loss: 8.995 | Reg loss: 0.020 | Tree loss: 8.995 | Accuracy: 0.578125 | 7.479 sec/iter\n",
      "Epoch: 04 | Batch: 017 / 030 | Total loss: 8.982 | Reg loss: 0.020 | Tree loss: 8.982 | Accuracy: 0.568359 | 7.477 sec/iter\n",
      "Epoch: 04 | Batch: 018 / 030 | Total loss: 8.966 | Reg loss: 0.021 | Tree loss: 8.966 | Accuracy: 0.570312 | 7.475 sec/iter\n",
      "Epoch: 04 | Batch: 019 / 030 | Total loss: 8.944 | Reg loss: 0.022 | Tree loss: 8.944 | Accuracy: 0.570312 | 7.472 sec/iter\n",
      "Epoch: 04 | Batch: 020 / 030 | Total loss: 8.925 | Reg loss: 0.022 | Tree loss: 8.925 | Accuracy: 0.562500 | 7.47 sec/iter\n",
      "Epoch: 04 | Batch: 021 / 030 | Total loss: 8.904 | Reg loss: 0.023 | Tree loss: 8.904 | Accuracy: 0.568359 | 7.467 sec/iter\n",
      "Epoch: 04 | Batch: 022 / 030 | Total loss: 8.887 | Reg loss: 0.023 | Tree loss: 8.887 | Accuracy: 0.597656 | 7.464 sec/iter\n",
      "Epoch: 04 | Batch: 023 / 030 | Total loss: 8.857 | Reg loss: 0.024 | Tree loss: 8.857 | Accuracy: 0.583984 | 7.459 sec/iter\n",
      "Epoch: 04 | Batch: 024 / 030 | Total loss: 8.823 | Reg loss: 0.025 | Tree loss: 8.823 | Accuracy: 0.595703 | 7.454 sec/iter\n",
      "Epoch: 04 | Batch: 025 / 030 | Total loss: 8.802 | Reg loss: 0.025 | Tree loss: 8.802 | Accuracy: 0.570312 | 7.448 sec/iter\n",
      "Epoch: 04 | Batch: 026 / 030 | Total loss: 8.757 | Reg loss: 0.026 | Tree loss: 8.757 | Accuracy: 0.593750 | 7.443 sec/iter\n",
      "Epoch: 04 | Batch: 027 / 030 | Total loss: 8.759 | Reg loss: 0.026 | Tree loss: 8.759 | Accuracy: 0.564453 | 7.439 sec/iter\n",
      "Epoch: 04 | Batch: 028 / 030 | Total loss: 8.741 | Reg loss: 0.027 | Tree loss: 8.741 | Accuracy: 0.582031 | 7.434 sec/iter\n",
      "Epoch: 04 | Batch: 029 / 030 | Total loss: 8.726 | Reg loss: 0.027 | Tree loss: 8.726 | Accuracy: 0.553398 | 7.407 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 05 | Batch: 000 / 030 | Total loss: 9.088 | Reg loss: 0.017 | Tree loss: 9.088 | Accuracy: 0.591797 | 7.427 sec/iter\n",
      "Epoch: 05 | Batch: 001 / 030 | Total loss: 9.052 | Reg loss: 0.017 | Tree loss: 9.052 | Accuracy: 0.621094 | 7.415 sec/iter\n",
      "Epoch: 05 | Batch: 002 / 030 | Total loss: 9.041 | Reg loss: 0.018 | Tree loss: 9.041 | Accuracy: 0.593750 | 7.412 sec/iter\n",
      "Epoch: 05 | Batch: 003 / 030 | Total loss: 9.020 | Reg loss: 0.018 | Tree loss: 9.020 | Accuracy: 0.580078 | 7.409 sec/iter\n",
      "Epoch: 05 | Batch: 004 / 030 | Total loss: 8.999 | Reg loss: 0.018 | Tree loss: 8.999 | Accuracy: 0.599609 | 7.407 sec/iter\n",
      "Epoch: 05 | Batch: 005 / 030 | Total loss: 8.994 | Reg loss: 0.019 | Tree loss: 8.994 | Accuracy: 0.535156 | 7.404 sec/iter\n",
      "Epoch: 05 | Batch: 006 / 030 | Total loss: 8.951 | Reg loss: 0.019 | Tree loss: 8.951 | Accuracy: 0.617188 | 7.401 sec/iter\n",
      "Epoch: 05 | Batch: 007 / 030 | Total loss: 8.936 | Reg loss: 0.019 | Tree loss: 8.936 | Accuracy: 0.558594 | 7.399 sec/iter\n",
      "Epoch: 05 | Batch: 008 / 030 | Total loss: 8.924 | Reg loss: 0.020 | Tree loss: 8.924 | Accuracy: 0.546875 | 7.386 sec/iter\n",
      "Epoch: 05 | Batch: 009 / 030 | Total loss: 8.904 | Reg loss: 0.020 | Tree loss: 8.904 | Accuracy: 0.582031 | 7.371 sec/iter\n",
      "Epoch: 05 | Batch: 010 / 030 | Total loss: 8.836 | Reg loss: 0.021 | Tree loss: 8.836 | Accuracy: 0.617188 | 7.356 sec/iter\n",
      "Epoch: 05 | Batch: 011 / 030 | Total loss: 8.820 | Reg loss: 0.021 | Tree loss: 8.820 | Accuracy: 0.580078 | 7.341 sec/iter\n",
      "Epoch: 05 | Batch: 012 / 030 | Total loss: 8.808 | Reg loss: 0.022 | Tree loss: 8.808 | Accuracy: 0.570312 | 7.326 sec/iter\n",
      "Epoch: 05 | Batch: 013 / 030 | Total loss: 8.768 | Reg loss: 0.023 | Tree loss: 8.768 | Accuracy: 0.583984 | 7.325 sec/iter\n",
      "Epoch: 05 | Batch: 014 / 030 | Total loss: 8.732 | Reg loss: 0.023 | Tree loss: 8.732 | Accuracy: 0.623047 | 7.323 sec/iter\n",
      "Epoch: 05 | Batch: 015 / 030 | Total loss: 8.715 | Reg loss: 0.024 | Tree loss: 8.715 | Accuracy: 0.587891 | 7.323 sec/iter\n",
      "Epoch: 05 | Batch: 016 / 030 | Total loss: 8.700 | Reg loss: 0.024 | Tree loss: 8.700 | Accuracy: 0.585938 | 7.322 sec/iter\n",
      "Epoch: 05 | Batch: 017 / 030 | Total loss: 8.651 | Reg loss: 0.025 | Tree loss: 8.651 | Accuracy: 0.572266 | 7.322 sec/iter\n",
      "Epoch: 05 | Batch: 018 / 030 | Total loss: 8.604 | Reg loss: 0.026 | Tree loss: 8.604 | Accuracy: 0.632812 | 7.321 sec/iter\n",
      "Epoch: 05 | Batch: 019 / 030 | Total loss: 8.571 | Reg loss: 0.026 | Tree loss: 8.571 | Accuracy: 0.609375 | 7.32 sec/iter\n",
      "Epoch: 05 | Batch: 020 / 030 | Total loss: 8.591 | Reg loss: 0.027 | Tree loss: 8.591 | Accuracy: 0.589844 | 7.319 sec/iter\n",
      "Epoch: 05 | Batch: 021 / 030 | Total loss: 8.551 | Reg loss: 0.027 | Tree loss: 8.551 | Accuracy: 0.541016 | 7.318 sec/iter\n",
      "Epoch: 05 | Batch: 022 / 030 | Total loss: 8.520 | Reg loss: 0.028 | Tree loss: 8.520 | Accuracy: 0.572266 | 7.317 sec/iter\n",
      "Epoch: 05 | Batch: 023 / 030 | Total loss: 8.445 | Reg loss: 0.029 | Tree loss: 8.445 | Accuracy: 0.611328 | 7.316 sec/iter\n",
      "Epoch: 05 | Batch: 024 / 030 | Total loss: 8.464 | Reg loss: 0.029 | Tree loss: 8.464 | Accuracy: 0.570312 | 7.315 sec/iter\n",
      "Epoch: 05 | Batch: 025 / 030 | Total loss: 8.429 | Reg loss: 0.030 | Tree loss: 8.429 | Accuracy: 0.568359 | 7.314 sec/iter\n",
      "Epoch: 05 | Batch: 026 / 030 | Total loss: 8.393 | Reg loss: 0.030 | Tree loss: 8.393 | Accuracy: 0.546875 | 7.313 sec/iter\n",
      "Epoch: 05 | Batch: 027 / 030 | Total loss: 8.359 | Reg loss: 0.031 | Tree loss: 8.359 | Accuracy: 0.572266 | 7.311 sec/iter\n",
      "Epoch: 05 | Batch: 028 / 030 | Total loss: 8.366 | Reg loss: 0.031 | Tree loss: 8.366 | Accuracy: 0.542969 | 7.303 sec/iter\n",
      "Epoch: 05 | Batch: 029 / 030 | Total loss: 8.272 | Reg loss: 0.032 | Tree loss: 8.272 | Accuracy: 0.563107 | 7.279 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 06 | Batch: 000 / 030 | Total loss: 8.869 | Reg loss: 0.021 | Tree loss: 8.869 | Accuracy: 0.537109 | 7.315 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 06 | Batch: 001 / 030 | Total loss: 8.816 | Reg loss: 0.021 | Tree loss: 8.816 | Accuracy: 0.578125 | 7.315 sec/iter\n",
      "Epoch: 06 | Batch: 002 / 030 | Total loss: 8.819 | Reg loss: 0.021 | Tree loss: 8.819 | Accuracy: 0.568359 | 7.314 sec/iter\n",
      "Epoch: 06 | Batch: 003 / 030 | Total loss: 8.783 | Reg loss: 0.021 | Tree loss: 8.783 | Accuracy: 0.574219 | 7.313 sec/iter\n",
      "Epoch: 06 | Batch: 004 / 030 | Total loss: 8.783 | Reg loss: 0.021 | Tree loss: 8.783 | Accuracy: 0.572266 | 7.312 sec/iter\n",
      "Epoch: 06 | Batch: 005 / 030 | Total loss: 8.733 | Reg loss: 0.021 | Tree loss: 8.733 | Accuracy: 0.593750 | 7.312 sec/iter\n",
      "Epoch: 06 | Batch: 006 / 030 | Total loss: 8.719 | Reg loss: 0.022 | Tree loss: 8.719 | Accuracy: 0.583984 | 7.311 sec/iter\n",
      "Epoch: 06 | Batch: 007 / 030 | Total loss: 8.683 | Reg loss: 0.022 | Tree loss: 8.683 | Accuracy: 0.580078 | 7.31 sec/iter\n",
      "Epoch: 06 | Batch: 008 / 030 | Total loss: 8.636 | Reg loss: 0.023 | Tree loss: 8.636 | Accuracy: 0.601562 | 7.308 sec/iter\n",
      "Epoch: 06 | Batch: 009 / 030 | Total loss: 8.619 | Reg loss: 0.023 | Tree loss: 8.619 | Accuracy: 0.597656 | 7.306 sec/iter\n",
      "Epoch: 06 | Batch: 010 / 030 | Total loss: 8.567 | Reg loss: 0.023 | Tree loss: 8.567 | Accuracy: 0.619141 | 7.296 sec/iter\n",
      "Epoch: 06 | Batch: 011 / 030 | Total loss: 8.549 | Reg loss: 0.024 | Tree loss: 8.549 | Accuracy: 0.593750 | 7.284 sec/iter\n",
      "Epoch: 06 | Batch: 012 / 030 | Total loss: 8.512 | Reg loss: 0.024 | Tree loss: 8.512 | Accuracy: 0.574219 | 7.271 sec/iter\n",
      "Epoch: 06 | Batch: 013 / 030 | Total loss: 8.491 | Reg loss: 0.025 | Tree loss: 8.491 | Accuracy: 0.550781 | 7.258 sec/iter\n",
      "Epoch: 06 | Batch: 014 / 030 | Total loss: 8.420 | Reg loss: 0.025 | Tree loss: 8.420 | Accuracy: 0.609375 | 7.245 sec/iter\n",
      "Epoch: 06 | Batch: 015 / 030 | Total loss: 8.434 | Reg loss: 0.026 | Tree loss: 8.434 | Accuracy: 0.564453 | 7.233 sec/iter\n",
      "Epoch: 06 | Batch: 016 / 030 | Total loss: 8.372 | Reg loss: 0.026 | Tree loss: 8.372 | Accuracy: 0.619141 | 7.232 sec/iter\n",
      "Epoch: 06 | Batch: 017 / 030 | Total loss: 8.322 | Reg loss: 0.027 | Tree loss: 8.322 | Accuracy: 0.611328 | 7.232 sec/iter\n",
      "Epoch: 06 | Batch: 018 / 030 | Total loss: 8.331 | Reg loss: 0.027 | Tree loss: 8.331 | Accuracy: 0.580078 | 7.231 sec/iter\n",
      "Epoch: 06 | Batch: 019 / 030 | Total loss: 8.325 | Reg loss: 0.028 | Tree loss: 8.325 | Accuracy: 0.587891 | 7.23 sec/iter\n",
      "Epoch: 06 | Batch: 020 / 030 | Total loss: 8.256 | Reg loss: 0.028 | Tree loss: 8.256 | Accuracy: 0.572266 | 7.229 sec/iter\n",
      "Epoch: 06 | Batch: 021 / 030 | Total loss: 8.257 | Reg loss: 0.029 | Tree loss: 8.257 | Accuracy: 0.529297 | 7.228 sec/iter\n",
      "Epoch: 06 | Batch: 022 / 030 | Total loss: 8.184 | Reg loss: 0.029 | Tree loss: 8.184 | Accuracy: 0.570312 | 7.227 sec/iter\n",
      "Epoch: 06 | Batch: 023 / 030 | Total loss: 8.170 | Reg loss: 0.030 | Tree loss: 8.170 | Accuracy: 0.564453 | 7.226 sec/iter\n",
      "Epoch: 06 | Batch: 024 / 030 | Total loss: 8.133 | Reg loss: 0.030 | Tree loss: 8.133 | Accuracy: 0.601562 | 7.224 sec/iter\n",
      "Epoch: 06 | Batch: 025 / 030 | Total loss: 8.084 | Reg loss: 0.031 | Tree loss: 8.084 | Accuracy: 0.613281 | 7.224 sec/iter\n",
      "Epoch: 06 | Batch: 026 / 030 | Total loss: 8.064 | Reg loss: 0.031 | Tree loss: 8.064 | Accuracy: 0.597656 | 7.22 sec/iter\n",
      "Epoch: 06 | Batch: 027 / 030 | Total loss: 8.042 | Reg loss: 0.032 | Tree loss: 8.042 | Accuracy: 0.603516 | 7.21 sec/iter\n",
      "Epoch: 06 | Batch: 028 / 030 | Total loss: 8.006 | Reg loss: 0.032 | Tree loss: 8.006 | Accuracy: 0.560547 | 7.199 sec/iter\n",
      "Epoch: 06 | Batch: 029 / 030 | Total loss: 8.001 | Reg loss: 0.033 | Tree loss: 8.001 | Accuracy: 0.533981 | 7.18 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 07 | Batch: 000 / 030 | Total loss: 8.618 | Reg loss: 0.023 | Tree loss: 8.618 | Accuracy: 0.546875 | 7.211 sec/iter\n",
      "Epoch: 07 | Batch: 001 / 030 | Total loss: 8.571 | Reg loss: 0.023 | Tree loss: 8.571 | Accuracy: 0.628906 | 7.211 sec/iter\n",
      "Epoch: 07 | Batch: 002 / 030 | Total loss: 8.543 | Reg loss: 0.023 | Tree loss: 8.543 | Accuracy: 0.607422 | 7.211 sec/iter\n",
      "Epoch: 07 | Batch: 003 / 030 | Total loss: 8.523 | Reg loss: 0.023 | Tree loss: 8.523 | Accuracy: 0.619141 | 7.211 sec/iter\n",
      "Epoch: 07 | Batch: 004 / 030 | Total loss: 8.507 | Reg loss: 0.024 | Tree loss: 8.507 | Accuracy: 0.568359 | 7.211 sec/iter\n",
      "Epoch: 07 | Batch: 005 / 030 | Total loss: 8.460 | Reg loss: 0.024 | Tree loss: 8.460 | Accuracy: 0.591797 | 7.212 sec/iter\n",
      "Epoch: 07 | Batch: 006 / 030 | Total loss: 8.442 | Reg loss: 0.024 | Tree loss: 8.442 | Accuracy: 0.595703 | 7.212 sec/iter\n",
      "Epoch: 07 | Batch: 007 / 030 | Total loss: 8.400 | Reg loss: 0.024 | Tree loss: 8.400 | Accuracy: 0.591797 | 7.213 sec/iter\n",
      "Epoch: 07 | Batch: 008 / 030 | Total loss: 8.350 | Reg loss: 0.025 | Tree loss: 8.350 | Accuracy: 0.603516 | 7.214 sec/iter\n",
      "Epoch: 07 | Batch: 009 / 030 | Total loss: 8.327 | Reg loss: 0.025 | Tree loss: 8.327 | Accuracy: 0.595703 | 7.215 sec/iter\n",
      "Epoch: 07 | Batch: 010 / 030 | Total loss: 8.336 | Reg loss: 0.025 | Tree loss: 8.336 | Accuracy: 0.556641 | 7.216 sec/iter\n",
      "Epoch: 07 | Batch: 011 / 030 | Total loss: 8.258 | Reg loss: 0.026 | Tree loss: 8.258 | Accuracy: 0.566406 | 7.216 sec/iter\n",
      "Epoch: 07 | Batch: 012 / 030 | Total loss: 8.204 | Reg loss: 0.026 | Tree loss: 8.204 | Accuracy: 0.603516 | 7.216 sec/iter\n",
      "Epoch: 07 | Batch: 013 / 030 | Total loss: 8.208 | Reg loss: 0.027 | Tree loss: 8.208 | Accuracy: 0.595703 | 7.206 sec/iter\n",
      "Epoch: 07 | Batch: 014 / 030 | Total loss: 8.135 | Reg loss: 0.027 | Tree loss: 8.135 | Accuracy: 0.554688 | 7.196 sec/iter\n",
      "Epoch: 07 | Batch: 015 / 030 | Total loss: 8.123 | Reg loss: 0.027 | Tree loss: 8.123 | Accuracy: 0.568359 | 7.187 sec/iter\n",
      "Epoch: 07 | Batch: 016 / 030 | Total loss: 8.053 | Reg loss: 0.028 | Tree loss: 8.053 | Accuracy: 0.597656 | 7.177 sec/iter\n",
      "Epoch: 07 | Batch: 017 / 030 | Total loss: 8.059 | Reg loss: 0.028 | Tree loss: 8.059 | Accuracy: 0.564453 | 7.166 sec/iter\n",
      "Epoch: 07 | Batch: 018 / 030 | Total loss: 8.032 | Reg loss: 0.029 | Tree loss: 8.032 | Accuracy: 0.585938 | 7.167 sec/iter\n",
      "Epoch: 07 | Batch: 019 / 030 | Total loss: 7.998 | Reg loss: 0.029 | Tree loss: 7.998 | Accuracy: 0.558594 | 7.168 sec/iter\n",
      "Epoch: 07 | Batch: 020 / 030 | Total loss: 7.979 | Reg loss: 0.030 | Tree loss: 7.979 | Accuracy: 0.572266 | 7.169 sec/iter\n",
      "Epoch: 07 | Batch: 021 / 030 | Total loss: 7.933 | Reg loss: 0.030 | Tree loss: 7.933 | Accuracy: 0.582031 | 7.169 sec/iter\n",
      "Epoch: 07 | Batch: 022 / 030 | Total loss: 7.882 | Reg loss: 0.031 | Tree loss: 7.882 | Accuracy: 0.583984 | 7.169 sec/iter\n",
      "Epoch: 07 | Batch: 023 / 030 | Total loss: 7.859 | Reg loss: 0.031 | Tree loss: 7.859 | Accuracy: 0.558594 | 7.17 sec/iter\n",
      "Epoch: 07 | Batch: 024 / 030 | Total loss: 7.818 | Reg loss: 0.031 | Tree loss: 7.818 | Accuracy: 0.572266 | 7.17 sec/iter\n",
      "Epoch: 07 | Batch: 025 / 030 | Total loss: 7.786 | Reg loss: 0.032 | Tree loss: 7.786 | Accuracy: 0.585938 | 7.164 sec/iter\n",
      "Epoch: 07 | Batch: 026 / 030 | Total loss: 7.774 | Reg loss: 0.032 | Tree loss: 7.774 | Accuracy: 0.564453 | 7.156 sec/iter\n",
      "Epoch: 07 | Batch: 027 / 030 | Total loss: 7.739 | Reg loss: 0.033 | Tree loss: 7.739 | Accuracy: 0.578125 | 7.148 sec/iter\n",
      "Epoch: 07 | Batch: 028 / 030 | Total loss: 7.722 | Reg loss: 0.033 | Tree loss: 7.722 | Accuracy: 0.603516 | 7.149 sec/iter\n",
      "Epoch: 07 | Batch: 029 / 030 | Total loss: 7.684 | Reg loss: 0.033 | Tree loss: 7.684 | Accuracy: 0.572816 | 7.133 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 08 | Batch: 000 / 030 | Total loss: 8.330 | Reg loss: 0.025 | Tree loss: 8.330 | Accuracy: 0.591797 | 7.185 sec/iter\n",
      "Epoch: 08 | Batch: 001 / 030 | Total loss: 8.292 | Reg loss: 0.025 | Tree loss: 8.292 | Accuracy: 0.582031 | 7.184 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 08 | Batch: 002 / 030 | Total loss: 8.262 | Reg loss: 0.025 | Tree loss: 8.262 | Accuracy: 0.570312 | 7.184 sec/iter\n",
      "Epoch: 08 | Batch: 003 / 030 | Total loss: 8.246 | Reg loss: 0.026 | Tree loss: 8.246 | Accuracy: 0.574219 | 7.183 sec/iter\n",
      "Epoch: 08 | Batch: 004 / 030 | Total loss: 8.200 | Reg loss: 0.026 | Tree loss: 8.200 | Accuracy: 0.566406 | 7.183 sec/iter\n",
      "Epoch: 08 | Batch: 005 / 030 | Total loss: 8.150 | Reg loss: 0.026 | Tree loss: 8.150 | Accuracy: 0.593750 | 7.182 sec/iter\n",
      "Epoch: 08 | Batch: 006 / 030 | Total loss: 8.137 | Reg loss: 0.026 | Tree loss: 8.137 | Accuracy: 0.593750 | 7.182 sec/iter\n",
      "Epoch: 08 | Batch: 007 / 030 | Total loss: 8.100 | Reg loss: 0.026 | Tree loss: 8.100 | Accuracy: 0.595703 | 7.181 sec/iter\n",
      "Epoch: 08 | Batch: 008 / 030 | Total loss: 8.053 | Reg loss: 0.027 | Tree loss: 8.053 | Accuracy: 0.560547 | 7.181 sec/iter\n",
      "Epoch: 08 | Batch: 009 / 030 | Total loss: 8.004 | Reg loss: 0.027 | Tree loss: 8.004 | Accuracy: 0.597656 | 7.18 sec/iter\n",
      "Epoch: 08 | Batch: 010 / 030 | Total loss: 7.983 | Reg loss: 0.027 | Tree loss: 7.983 | Accuracy: 0.580078 | 7.179 sec/iter\n",
      "Epoch: 08 | Batch: 011 / 030 | Total loss: 7.933 | Reg loss: 0.028 | Tree loss: 7.933 | Accuracy: 0.595703 | 7.179 sec/iter\n",
      "Epoch: 08 | Batch: 012 / 030 | Total loss: 7.908 | Reg loss: 0.028 | Tree loss: 7.908 | Accuracy: 0.556641 | 7.178 sec/iter\n",
      "Epoch: 08 | Batch: 013 / 030 | Total loss: 7.894 | Reg loss: 0.028 | Tree loss: 7.894 | Accuracy: 0.591797 | 7.177 sec/iter\n",
      "Epoch: 08 | Batch: 014 / 030 | Total loss: 7.862 | Reg loss: 0.029 | Tree loss: 7.862 | Accuracy: 0.576172 | 7.174 sec/iter\n",
      "Epoch: 08 | Batch: 015 / 030 | Total loss: 7.842 | Reg loss: 0.029 | Tree loss: 7.842 | Accuracy: 0.568359 | 7.165 sec/iter\n",
      "Epoch: 08 | Batch: 016 / 030 | Total loss: 7.764 | Reg loss: 0.030 | Tree loss: 7.764 | Accuracy: 0.582031 | 7.156 sec/iter\n",
      "Epoch: 08 | Batch: 017 / 030 | Total loss: 7.759 | Reg loss: 0.030 | Tree loss: 7.759 | Accuracy: 0.593750 | 7.147 sec/iter\n",
      "Epoch: 08 | Batch: 018 / 030 | Total loss: 7.688 | Reg loss: 0.030 | Tree loss: 7.688 | Accuracy: 0.589844 | 7.148 sec/iter\n",
      "Epoch: 08 | Batch: 019 / 030 | Total loss: 7.682 | Reg loss: 0.031 | Tree loss: 7.682 | Accuracy: 0.623047 | 7.148 sec/iter\n",
      "Epoch: 08 | Batch: 020 / 030 | Total loss: 7.637 | Reg loss: 0.031 | Tree loss: 7.637 | Accuracy: 0.589844 | 7.148 sec/iter\n",
      "Epoch: 08 | Batch: 021 / 030 | Total loss: 7.603 | Reg loss: 0.032 | Tree loss: 7.603 | Accuracy: 0.542969 | 7.148 sec/iter\n",
      "Epoch: 08 | Batch: 022 / 030 | Total loss: 7.564 | Reg loss: 0.032 | Tree loss: 7.564 | Accuracy: 0.611328 | 7.148 sec/iter\n",
      "Epoch: 08 | Batch: 023 / 030 | Total loss: 7.560 | Reg loss: 0.033 | Tree loss: 7.560 | Accuracy: 0.570312 | 7.143 sec/iter\n",
      "Epoch: 08 | Batch: 024 / 030 | Total loss: 7.524 | Reg loss: 0.033 | Tree loss: 7.524 | Accuracy: 0.582031 | 7.136 sec/iter\n",
      "Epoch: 08 | Batch: 025 / 030 | Total loss: 7.504 | Reg loss: 0.033 | Tree loss: 7.504 | Accuracy: 0.580078 | 7.129 sec/iter\n",
      "Epoch: 08 | Batch: 026 / 030 | Total loss: 7.469 | Reg loss: 0.034 | Tree loss: 7.469 | Accuracy: 0.576172 | 7.13 sec/iter\n",
      "Epoch: 08 | Batch: 027 / 030 | Total loss: 7.411 | Reg loss: 0.034 | Tree loss: 7.411 | Accuracy: 0.585938 | 7.13 sec/iter\n",
      "Epoch: 08 | Batch: 028 / 030 | Total loss: 7.381 | Reg loss: 0.035 | Tree loss: 7.381 | Accuracy: 0.589844 | 7.131 sec/iter\n",
      "Epoch: 08 | Batch: 029 / 030 | Total loss: 7.457 | Reg loss: 0.035 | Tree loss: 7.457 | Accuracy: 0.524272 | 7.117 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 09 | Batch: 000 / 030 | Total loss: 8.026 | Reg loss: 0.027 | Tree loss: 8.026 | Accuracy: 0.558594 | 7.262 sec/iter\n",
      "Epoch: 09 | Batch: 001 / 030 | Total loss: 7.979 | Reg loss: 0.027 | Tree loss: 7.979 | Accuracy: 0.564453 | 7.261 sec/iter\n",
      "Epoch: 09 | Batch: 002 / 030 | Total loss: 7.972 | Reg loss: 0.027 | Tree loss: 7.972 | Accuracy: 0.562500 | 7.261 sec/iter\n",
      "Epoch: 09 | Batch: 003 / 030 | Total loss: 7.928 | Reg loss: 0.027 | Tree loss: 7.928 | Accuracy: 0.597656 | 7.26 sec/iter\n",
      "Epoch: 09 | Batch: 004 / 030 | Total loss: 7.899 | Reg loss: 0.028 | Tree loss: 7.899 | Accuracy: 0.572266 | 7.259 sec/iter\n",
      "Epoch: 09 | Batch: 005 / 030 | Total loss: 7.875 | Reg loss: 0.028 | Tree loss: 7.875 | Accuracy: 0.595703 | 7.258 sec/iter\n",
      "Epoch: 09 | Batch: 006 / 030 | Total loss: 7.832 | Reg loss: 0.028 | Tree loss: 7.832 | Accuracy: 0.576172 | 7.257 sec/iter\n",
      "Epoch: 09 | Batch: 007 / 030 | Total loss: 7.776 | Reg loss: 0.028 | Tree loss: 7.776 | Accuracy: 0.589844 | 7.256 sec/iter\n",
      "Epoch: 09 | Batch: 008 / 030 | Total loss: 7.775 | Reg loss: 0.029 | Tree loss: 7.775 | Accuracy: 0.535156 | 7.254 sec/iter\n",
      "Epoch: 09 | Batch: 009 / 030 | Total loss: 7.689 | Reg loss: 0.029 | Tree loss: 7.689 | Accuracy: 0.589844 | 7.246 sec/iter\n",
      "Epoch: 09 | Batch: 010 / 030 | Total loss: 7.706 | Reg loss: 0.029 | Tree loss: 7.706 | Accuracy: 0.580078 | 7.237 sec/iter\n",
      "Epoch: 09 | Batch: 011 / 030 | Total loss: 7.626 | Reg loss: 0.030 | Tree loss: 7.626 | Accuracy: 0.605469 | 7.228 sec/iter\n",
      "Epoch: 09 | Batch: 012 / 030 | Total loss: 7.564 | Reg loss: 0.030 | Tree loss: 7.564 | Accuracy: 0.597656 | 7.219 sec/iter\n",
      "Epoch: 09 | Batch: 013 / 030 | Total loss: 7.581 | Reg loss: 0.030 | Tree loss: 7.581 | Accuracy: 0.550781 | 7.216 sec/iter\n",
      "Epoch: 09 | Batch: 014 / 030 | Total loss: 7.506 | Reg loss: 0.031 | Tree loss: 7.506 | Accuracy: 0.626953 | 7.214 sec/iter\n",
      "Epoch: 09 | Batch: 015 / 030 | Total loss: 7.482 | Reg loss: 0.031 | Tree loss: 7.482 | Accuracy: 0.580078 | 7.211 sec/iter\n",
      "Epoch: 09 | Batch: 016 / 030 | Total loss: 7.461 | Reg loss: 0.032 | Tree loss: 7.461 | Accuracy: 0.597656 | 7.202 sec/iter\n",
      "Epoch: 09 | Batch: 017 / 030 | Total loss: 7.414 | Reg loss: 0.032 | Tree loss: 7.414 | Accuracy: 0.572266 | 7.194 sec/iter\n",
      "Epoch: 09 | Batch: 018 / 030 | Total loss: 7.429 | Reg loss: 0.032 | Tree loss: 7.429 | Accuracy: 0.570312 | 7.185 sec/iter\n",
      "Epoch: 09 | Batch: 019 / 030 | Total loss: 7.351 | Reg loss: 0.033 | Tree loss: 7.351 | Accuracy: 0.562500 | 7.177 sec/iter\n",
      "Epoch: 09 | Batch: 020 / 030 | Total loss: 7.313 | Reg loss: 0.033 | Tree loss: 7.313 | Accuracy: 0.591797 | 7.169 sec/iter\n",
      "Epoch: 09 | Batch: 021 / 030 | Total loss: 7.334 | Reg loss: 0.034 | Tree loss: 7.334 | Accuracy: 0.556641 | 7.16 sec/iter\n",
      "Epoch: 09 | Batch: 022 / 030 | Total loss: 7.234 | Reg loss: 0.034 | Tree loss: 7.234 | Accuracy: 0.609375 | 7.152 sec/iter\n",
      "Epoch: 09 | Batch: 023 / 030 | Total loss: 7.252 | Reg loss: 0.034 | Tree loss: 7.252 | Accuracy: 0.558594 | 7.144 sec/iter\n",
      "Epoch: 09 | Batch: 024 / 030 | Total loss: 7.193 | Reg loss: 0.035 | Tree loss: 7.193 | Accuracy: 0.599609 | 7.136 sec/iter\n",
      "Epoch: 09 | Batch: 025 / 030 | Total loss: 7.171 | Reg loss: 0.035 | Tree loss: 7.171 | Accuracy: 0.589844 | 7.127 sec/iter\n",
      "Epoch: 09 | Batch: 026 / 030 | Total loss: 7.110 | Reg loss: 0.036 | Tree loss: 7.110 | Accuracy: 0.613281 | 7.125 sec/iter\n",
      "Epoch: 09 | Batch: 027 / 030 | Total loss: 7.122 | Reg loss: 0.036 | Tree loss: 7.122 | Accuracy: 0.595703 | 7.124 sec/iter\n",
      "Epoch: 09 | Batch: 028 / 030 | Total loss: 7.049 | Reg loss: 0.036 | Tree loss: 7.049 | Accuracy: 0.599609 | 7.123 sec/iter\n",
      "Epoch: 09 | Batch: 029 / 030 | Total loss: 7.034 | Reg loss: 0.037 | Tree loss: 7.034 | Accuracy: 0.582524 | 7.11 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 10 | Batch: 000 / 030 | Total loss: 7.680 | Reg loss: 0.029 | Tree loss: 7.680 | Accuracy: 0.582031 | 7.158 sec/iter\n",
      "Epoch: 10 | Batch: 001 / 030 | Total loss: 7.665 | Reg loss: 0.029 | Tree loss: 7.665 | Accuracy: 0.597656 | 7.157 sec/iter\n",
      "Epoch: 10 | Batch: 002 / 030 | Total loss: 7.661 | Reg loss: 0.029 | Tree loss: 7.661 | Accuracy: 0.539062 | 7.156 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 | Batch: 003 / 030 | Total loss: 7.610 | Reg loss: 0.029 | Tree loss: 7.610 | Accuracy: 0.589844 | 7.154 sec/iter\n",
      "Epoch: 10 | Batch: 004 / 030 | Total loss: 7.615 | Reg loss: 0.030 | Tree loss: 7.615 | Accuracy: 0.558594 | 7.152 sec/iter\n",
      "Epoch: 10 | Batch: 005 / 030 | Total loss: 7.568 | Reg loss: 0.030 | Tree loss: 7.568 | Accuracy: 0.595703 | 7.15 sec/iter\n",
      "Epoch: 10 | Batch: 006 / 030 | Total loss: 7.514 | Reg loss: 0.030 | Tree loss: 7.514 | Accuracy: 0.597656 | 7.148 sec/iter\n",
      "Epoch: 10 | Batch: 007 / 030 | Total loss: 7.484 | Reg loss: 0.030 | Tree loss: 7.484 | Accuracy: 0.583984 | 7.147 sec/iter\n",
      "Epoch: 10 | Batch: 008 / 030 | Total loss: 7.433 | Reg loss: 0.030 | Tree loss: 7.433 | Accuracy: 0.568359 | 7.146 sec/iter\n",
      "Epoch: 10 | Batch: 009 / 030 | Total loss: 7.397 | Reg loss: 0.031 | Tree loss: 7.397 | Accuracy: 0.578125 | 7.145 sec/iter\n",
      "Epoch: 10 | Batch: 010 / 030 | Total loss: 7.343 | Reg loss: 0.031 | Tree loss: 7.343 | Accuracy: 0.630859 | 7.144 sec/iter\n",
      "Epoch: 10 | Batch: 011 / 030 | Total loss: 7.311 | Reg loss: 0.031 | Tree loss: 7.311 | Accuracy: 0.582031 | 7.137 sec/iter\n",
      "Epoch: 10 | Batch: 012 / 030 | Total loss: 7.262 | Reg loss: 0.032 | Tree loss: 7.262 | Accuracy: 0.609375 | 7.129 sec/iter\n",
      "Epoch: 10 | Batch: 013 / 030 | Total loss: 7.225 | Reg loss: 0.032 | Tree loss: 7.225 | Accuracy: 0.615234 | 7.121 sec/iter\n",
      "Epoch: 10 | Batch: 014 / 030 | Total loss: 7.167 | Reg loss: 0.032 | Tree loss: 7.167 | Accuracy: 0.642578 | 7.114 sec/iter\n",
      "Epoch: 10 | Batch: 015 / 030 | Total loss: 7.196 | Reg loss: 0.033 | Tree loss: 7.196 | Accuracy: 0.546875 | 7.106 sec/iter\n",
      "Epoch: 10 | Batch: 016 / 030 | Total loss: 7.124 | Reg loss: 0.033 | Tree loss: 7.124 | Accuracy: 0.570312 | 7.099 sec/iter\n",
      "Epoch: 10 | Batch: 017 / 030 | Total loss: 7.109 | Reg loss: 0.034 | Tree loss: 7.109 | Accuracy: 0.544922 | 7.092 sec/iter\n",
      "Epoch: 10 | Batch: 018 / 030 | Total loss: 7.071 | Reg loss: 0.034 | Tree loss: 7.071 | Accuracy: 0.574219 | 7.091 sec/iter\n",
      "Epoch: 10 | Batch: 019 / 030 | Total loss: 7.046 | Reg loss: 0.034 | Tree loss: 7.046 | Accuracy: 0.578125 | 7.09 sec/iter\n",
      "Epoch: 10 | Batch: 020 / 030 | Total loss: 7.041 | Reg loss: 0.035 | Tree loss: 7.041 | Accuracy: 0.556641 | 7.089 sec/iter\n",
      "Epoch: 10 | Batch: 021 / 030 | Total loss: 6.964 | Reg loss: 0.035 | Tree loss: 6.964 | Accuracy: 0.601562 | 7.089 sec/iter\n",
      "Epoch: 10 | Batch: 022 / 030 | Total loss: 6.947 | Reg loss: 0.036 | Tree loss: 6.947 | Accuracy: 0.603516 | 7.081 sec/iter\n",
      "Epoch: 10 | Batch: 023 / 030 | Total loss: 6.927 | Reg loss: 0.036 | Tree loss: 6.927 | Accuracy: 0.574219 | 7.074 sec/iter\n",
      "Epoch: 10 | Batch: 024 / 030 | Total loss: 6.860 | Reg loss: 0.036 | Tree loss: 6.860 | Accuracy: 0.597656 | 7.067 sec/iter\n",
      "Epoch: 10 | Batch: 025 / 030 | Total loss: 6.900 | Reg loss: 0.037 | Tree loss: 6.900 | Accuracy: 0.591797 | 7.06 sec/iter\n",
      "Epoch: 10 | Batch: 026 / 030 | Total loss: 6.830 | Reg loss: 0.037 | Tree loss: 6.830 | Accuracy: 0.601562 | 7.053 sec/iter\n",
      "Epoch: 10 | Batch: 027 / 030 | Total loss: 6.844 | Reg loss: 0.037 | Tree loss: 6.844 | Accuracy: 0.550781 | 7.046 sec/iter\n",
      "Epoch: 10 | Batch: 028 / 030 | Total loss: 6.823 | Reg loss: 0.038 | Tree loss: 6.823 | Accuracy: 0.552734 | 7.039 sec/iter\n",
      "Epoch: 10 | Batch: 029 / 030 | Total loss: 6.797 | Reg loss: 0.038 | Tree loss: 6.797 | Accuracy: 0.504854 | 7.028 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 11 | Batch: 000 / 030 | Total loss: 7.357 | Reg loss: 0.031 | Tree loss: 7.357 | Accuracy: 0.615234 | 7.085 sec/iter\n",
      "Epoch: 11 | Batch: 001 / 030 | Total loss: 7.392 | Reg loss: 0.031 | Tree loss: 7.392 | Accuracy: 0.580078 | 7.085 sec/iter\n",
      "Epoch: 11 | Batch: 002 / 030 | Total loss: 7.367 | Reg loss: 0.031 | Tree loss: 7.367 | Accuracy: 0.554688 | 7.085 sec/iter\n",
      "Epoch: 11 | Batch: 003 / 030 | Total loss: 7.282 | Reg loss: 0.031 | Tree loss: 7.282 | Accuracy: 0.576172 | 7.085 sec/iter\n",
      "Epoch: 11 | Batch: 004 / 030 | Total loss: 7.276 | Reg loss: 0.031 | Tree loss: 7.276 | Accuracy: 0.556641 | 7.084 sec/iter\n",
      "Epoch: 11 | Batch: 005 / 030 | Total loss: 7.225 | Reg loss: 0.032 | Tree loss: 7.225 | Accuracy: 0.597656 | 7.084 sec/iter\n",
      "Epoch: 11 | Batch: 006 / 030 | Total loss: 7.188 | Reg loss: 0.032 | Tree loss: 7.188 | Accuracy: 0.601562 | 7.084 sec/iter\n",
      "Epoch: 11 | Batch: 007 / 030 | Total loss: 7.194 | Reg loss: 0.032 | Tree loss: 7.194 | Accuracy: 0.574219 | 7.084 sec/iter\n",
      "Epoch: 11 | Batch: 008 / 030 | Total loss: 7.107 | Reg loss: 0.032 | Tree loss: 7.107 | Accuracy: 0.564453 | 7.083 sec/iter\n",
      "Epoch: 11 | Batch: 009 / 030 | Total loss: 7.064 | Reg loss: 0.033 | Tree loss: 7.064 | Accuracy: 0.623047 | 7.083 sec/iter\n",
      "Epoch: 11 | Batch: 010 / 030 | Total loss: 7.050 | Reg loss: 0.033 | Tree loss: 7.050 | Accuracy: 0.583984 | 7.083 sec/iter\n",
      "Epoch: 11 | Batch: 011 / 030 | Total loss: 7.049 | Reg loss: 0.033 | Tree loss: 7.049 | Accuracy: 0.564453 | 7.082 sec/iter\n",
      "Epoch: 11 | Batch: 012 / 030 | Total loss: 6.951 | Reg loss: 0.033 | Tree loss: 6.951 | Accuracy: 0.585938 | 7.082 sec/iter\n",
      "Epoch: 11 | Batch: 013 / 030 | Total loss: 6.939 | Reg loss: 0.034 | Tree loss: 6.939 | Accuracy: 0.537109 | 7.082 sec/iter\n",
      "Epoch: 11 | Batch: 014 / 030 | Total loss: 6.926 | Reg loss: 0.034 | Tree loss: 6.926 | Accuracy: 0.554688 | 7.077 sec/iter\n",
      "Epoch: 11 | Batch: 015 / 030 | Total loss: 6.886 | Reg loss: 0.034 | Tree loss: 6.886 | Accuracy: 0.556641 | 7.072 sec/iter\n",
      "Epoch: 11 | Batch: 016 / 030 | Total loss: 6.852 | Reg loss: 0.035 | Tree loss: 6.852 | Accuracy: 0.597656 | 7.067 sec/iter\n",
      "Epoch: 11 | Batch: 017 / 030 | Total loss: 6.786 | Reg loss: 0.035 | Tree loss: 6.786 | Accuracy: 0.582031 | 7.068 sec/iter\n",
      "Epoch: 11 | Batch: 018 / 030 | Total loss: 6.790 | Reg loss: 0.035 | Tree loss: 6.790 | Accuracy: 0.595703 | 7.068 sec/iter\n",
      "Epoch: 11 | Batch: 019 / 030 | Total loss: 6.766 | Reg loss: 0.036 | Tree loss: 6.766 | Accuracy: 0.576172 | 7.069 sec/iter\n",
      "Epoch: 11 | Batch: 020 / 030 | Total loss: 6.711 | Reg loss: 0.036 | Tree loss: 6.711 | Accuracy: 0.601562 | 7.069 sec/iter\n",
      "Epoch: 11 | Batch: 021 / 030 | Total loss: 6.696 | Reg loss: 0.036 | Tree loss: 6.696 | Accuracy: 0.587891 | 7.07 sec/iter\n",
      "Epoch: 11 | Batch: 022 / 030 | Total loss: 6.672 | Reg loss: 0.037 | Tree loss: 6.672 | Accuracy: 0.589844 | 7.069 sec/iter\n",
      "Epoch: 11 | Batch: 023 / 030 | Total loss: 6.621 | Reg loss: 0.037 | Tree loss: 6.621 | Accuracy: 0.623047 | 7.064 sec/iter\n",
      "Epoch: 11 | Batch: 024 / 030 | Total loss: 6.626 | Reg loss: 0.037 | Tree loss: 6.626 | Accuracy: 0.585938 | 7.06 sec/iter\n",
      "Epoch: 11 | Batch: 025 / 030 | Total loss: 6.559 | Reg loss: 0.038 | Tree loss: 6.559 | Accuracy: 0.595703 | 7.061 sec/iter\n",
      "Epoch: 11 | Batch: 026 / 030 | Total loss: 6.568 | Reg loss: 0.038 | Tree loss: 6.568 | Accuracy: 0.546875 | 7.062 sec/iter\n",
      "Epoch: 11 | Batch: 027 / 030 | Total loss: 6.544 | Reg loss: 0.038 | Tree loss: 6.544 | Accuracy: 0.564453 | 7.063 sec/iter\n",
      "Epoch: 11 | Batch: 028 / 030 | Total loss: 6.485 | Reg loss: 0.038 | Tree loss: 6.485 | Accuracy: 0.623047 | 7.063 sec/iter\n",
      "Epoch: 11 | Batch: 029 / 030 | Total loss: 6.402 | Reg loss: 0.039 | Tree loss: 6.402 | Accuracy: 0.601942 | 7.053 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 12 | Batch: 000 / 030 | Total loss: 7.116 | Reg loss: 0.033 | Tree loss: 7.116 | Accuracy: 0.583984 | 7.122 sec/iter\n",
      "Epoch: 12 | Batch: 001 / 030 | Total loss: 7.033 | Reg loss: 0.033 | Tree loss: 7.033 | Accuracy: 0.568359 | 7.122 sec/iter\n",
      "Epoch: 12 | Batch: 002 / 030 | Total loss: 7.029 | Reg loss: 0.033 | Tree loss: 7.029 | Accuracy: 0.597656 | 7.121 sec/iter\n",
      "Epoch: 12 | Batch: 003 / 030 | Total loss: 6.980 | Reg loss: 0.033 | Tree loss: 6.980 | Accuracy: 0.582031 | 7.12 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12 | Batch: 004 / 030 | Total loss: 6.945 | Reg loss: 0.033 | Tree loss: 6.945 | Accuracy: 0.599609 | 7.119 sec/iter\n",
      "Epoch: 12 | Batch: 005 / 030 | Total loss: 6.885 | Reg loss: 0.033 | Tree loss: 6.885 | Accuracy: 0.585938 | 7.117 sec/iter\n",
      "Epoch: 12 | Batch: 006 / 030 | Total loss: 6.905 | Reg loss: 0.033 | Tree loss: 6.905 | Accuracy: 0.585938 | 7.116 sec/iter\n",
      "Epoch: 12 | Batch: 007 / 030 | Total loss: 6.851 | Reg loss: 0.034 | Tree loss: 6.851 | Accuracy: 0.576172 | 7.115 sec/iter\n",
      "Epoch: 12 | Batch: 008 / 030 | Total loss: 6.796 | Reg loss: 0.034 | Tree loss: 6.796 | Accuracy: 0.580078 | 7.114 sec/iter\n",
      "Epoch: 12 | Batch: 009 / 030 | Total loss: 6.774 | Reg loss: 0.034 | Tree loss: 6.774 | Accuracy: 0.615234 | 7.114 sec/iter\n",
      "Epoch: 12 | Batch: 010 / 030 | Total loss: 6.765 | Reg loss: 0.034 | Tree loss: 6.765 | Accuracy: 0.554688 | 7.112 sec/iter\n",
      "Epoch: 12 | Batch: 011 / 030 | Total loss: 6.708 | Reg loss: 0.034 | Tree loss: 6.708 | Accuracy: 0.595703 | 7.111 sec/iter\n",
      "Epoch: 12 | Batch: 012 / 030 | Total loss: 6.704 | Reg loss: 0.035 | Tree loss: 6.704 | Accuracy: 0.533203 | 7.105 sec/iter\n",
      "Epoch: 12 | Batch: 013 / 030 | Total loss: 6.666 | Reg loss: 0.035 | Tree loss: 6.666 | Accuracy: 0.597656 | 7.098 sec/iter\n",
      "Epoch: 12 | Batch: 014 / 030 | Total loss: 6.596 | Reg loss: 0.035 | Tree loss: 6.596 | Accuracy: 0.578125 | 7.092 sec/iter\n",
      "Epoch: 12 | Batch: 015 / 030 | Total loss: 6.543 | Reg loss: 0.035 | Tree loss: 6.543 | Accuracy: 0.595703 | 7.086 sec/iter\n",
      "Epoch: 12 | Batch: 016 / 030 | Total loss: 6.554 | Reg loss: 0.036 | Tree loss: 6.554 | Accuracy: 0.603516 | 7.08 sec/iter\n",
      "Epoch: 12 | Batch: 017 / 030 | Total loss: 6.527 | Reg loss: 0.036 | Tree loss: 6.527 | Accuracy: 0.570312 | 7.074 sec/iter\n",
      "Epoch: 12 | Batch: 018 / 030 | Total loss: 6.493 | Reg loss: 0.036 | Tree loss: 6.493 | Accuracy: 0.580078 | 7.071 sec/iter\n",
      "Epoch: 12 | Batch: 019 / 030 | Total loss: 6.490 | Reg loss: 0.037 | Tree loss: 6.490 | Accuracy: 0.578125 | 7.067 sec/iter\n",
      "Epoch: 12 | Batch: 020 / 030 | Total loss: 6.426 | Reg loss: 0.037 | Tree loss: 6.426 | Accuracy: 0.591797 | 7.068 sec/iter\n",
      "Epoch: 12 | Batch: 021 / 030 | Total loss: 6.408 | Reg loss: 0.037 | Tree loss: 6.408 | Accuracy: 0.605469 | 7.068 sec/iter\n",
      "Epoch: 12 | Batch: 022 / 030 | Total loss: 6.414 | Reg loss: 0.037 | Tree loss: 6.414 | Accuracy: 0.591797 | 7.069 sec/iter\n",
      "Epoch: 12 | Batch: 023 / 030 | Total loss: 6.353 | Reg loss: 0.038 | Tree loss: 6.353 | Accuracy: 0.582031 | 7.07 sec/iter\n",
      "Epoch: 12 | Batch: 024 / 030 | Total loss: 6.340 | Reg loss: 0.038 | Tree loss: 6.340 | Accuracy: 0.609375 | 7.071 sec/iter\n",
      "Epoch: 12 | Batch: 025 / 030 | Total loss: 6.303 | Reg loss: 0.038 | Tree loss: 6.303 | Accuracy: 0.578125 | 7.071 sec/iter\n",
      "Epoch: 12 | Batch: 026 / 030 | Total loss: 6.311 | Reg loss: 0.039 | Tree loss: 6.311 | Accuracy: 0.552734 | 7.071 sec/iter\n",
      "Epoch: 12 | Batch: 027 / 030 | Total loss: 6.274 | Reg loss: 0.039 | Tree loss: 6.274 | Accuracy: 0.599609 | 7.071 sec/iter\n",
      "Epoch: 12 | Batch: 028 / 030 | Total loss: 6.278 | Reg loss: 0.039 | Tree loss: 6.278 | Accuracy: 0.539062 | 7.07 sec/iter\n",
      "Epoch: 12 | Batch: 029 / 030 | Total loss: 6.235 | Reg loss: 0.039 | Tree loss: 6.235 | Accuracy: 0.524272 | 7.06 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 13 | Batch: 000 / 030 | Total loss: 6.764 | Reg loss: 0.034 | Tree loss: 6.764 | Accuracy: 0.589844 | 7.088 sec/iter\n",
      "Epoch: 13 | Batch: 001 / 030 | Total loss: 6.745 | Reg loss: 0.034 | Tree loss: 6.745 | Accuracy: 0.568359 | 7.088 sec/iter\n",
      "Epoch: 13 | Batch: 002 / 030 | Total loss: 6.719 | Reg loss: 0.034 | Tree loss: 6.719 | Accuracy: 0.574219 | 7.088 sec/iter\n",
      "Epoch: 13 | Batch: 003 / 030 | Total loss: 6.673 | Reg loss: 0.034 | Tree loss: 6.673 | Accuracy: 0.621094 | 7.088 sec/iter\n",
      "Epoch: 13 | Batch: 004 / 030 | Total loss: 6.593 | Reg loss: 0.034 | Tree loss: 6.593 | Accuracy: 0.621094 | 7.088 sec/iter\n",
      "Epoch: 13 | Batch: 005 / 030 | Total loss: 6.627 | Reg loss: 0.035 | Tree loss: 6.627 | Accuracy: 0.593750 | 7.089 sec/iter\n",
      "Epoch: 13 | Batch: 006 / 030 | Total loss: 6.569 | Reg loss: 0.035 | Tree loss: 6.569 | Accuracy: 0.587891 | 7.089 sec/iter\n",
      "Epoch: 13 | Batch: 007 / 030 | Total loss: 6.567 | Reg loss: 0.035 | Tree loss: 6.567 | Accuracy: 0.576172 | 7.089 sec/iter\n",
      "Epoch: 13 | Batch: 008 / 030 | Total loss: 6.523 | Reg loss: 0.035 | Tree loss: 6.523 | Accuracy: 0.589844 | 7.089 sec/iter\n",
      "Epoch: 13 | Batch: 009 / 030 | Total loss: 6.502 | Reg loss: 0.035 | Tree loss: 6.502 | Accuracy: 0.574219 | 7.088 sec/iter\n",
      "Epoch: 13 | Batch: 010 / 030 | Total loss: 6.442 | Reg loss: 0.035 | Tree loss: 6.442 | Accuracy: 0.591797 | 7.088 sec/iter\n",
      "Epoch: 13 | Batch: 011 / 030 | Total loss: 6.443 | Reg loss: 0.036 | Tree loss: 6.443 | Accuracy: 0.568359 | 7.087 sec/iter\n",
      "Epoch: 13 | Batch: 012 / 030 | Total loss: 6.407 | Reg loss: 0.036 | Tree loss: 6.407 | Accuracy: 0.533203 | 7.086 sec/iter\n",
      "Epoch: 13 | Batch: 013 / 030 | Total loss: 6.390 | Reg loss: 0.036 | Tree loss: 6.390 | Accuracy: 0.587891 | 7.085 sec/iter\n",
      "Epoch: 13 | Batch: 014 / 030 | Total loss: 6.357 | Reg loss: 0.036 | Tree loss: 6.357 | Accuracy: 0.566406 | 7.084 sec/iter\n",
      "Epoch: 13 | Batch: 015 / 030 | Total loss: 6.288 | Reg loss: 0.037 | Tree loss: 6.288 | Accuracy: 0.611328 | 7.081 sec/iter\n",
      "Epoch: 13 | Batch: 016 / 030 | Total loss: 6.246 | Reg loss: 0.037 | Tree loss: 6.246 | Accuracy: 0.603516 | 7.073 sec/iter\n",
      "Epoch: 13 | Batch: 017 / 030 | Total loss: 6.259 | Reg loss: 0.037 | Tree loss: 6.259 | Accuracy: 0.576172 | 7.069 sec/iter\n",
      "Epoch: 13 | Batch: 018 / 030 | Total loss: 6.241 | Reg loss: 0.037 | Tree loss: 6.241 | Accuracy: 0.552734 | 7.07 sec/iter\n",
      "Epoch: 13 | Batch: 019 / 030 | Total loss: 6.216 | Reg loss: 0.037 | Tree loss: 6.216 | Accuracy: 0.580078 | 7.071 sec/iter\n",
      "Epoch: 13 | Batch: 020 / 030 | Total loss: 6.140 | Reg loss: 0.038 | Tree loss: 6.140 | Accuracy: 0.585938 | 7.072 sec/iter\n",
      "Epoch: 13 | Batch: 021 / 030 | Total loss: 6.155 | Reg loss: 0.038 | Tree loss: 6.155 | Accuracy: 0.566406 | 7.073 sec/iter\n",
      "Epoch: 13 | Batch: 022 / 030 | Total loss: 6.105 | Reg loss: 0.038 | Tree loss: 6.105 | Accuracy: 0.589844 | 7.073 sec/iter\n",
      "Epoch: 13 | Batch: 023 / 030 | Total loss: 6.082 | Reg loss: 0.038 | Tree loss: 6.082 | Accuracy: 0.603516 | 7.074 sec/iter\n",
      "Epoch: 13 | Batch: 024 / 030 | Total loss: 6.080 | Reg loss: 0.039 | Tree loss: 6.080 | Accuracy: 0.572266 | 7.075 sec/iter\n",
      "Epoch: 13 | Batch: 025 / 030 | Total loss: 6.050 | Reg loss: 0.039 | Tree loss: 6.050 | Accuracy: 0.578125 | 7.074 sec/iter\n",
      "Epoch: 13 | Batch: 026 / 030 | Total loss: 6.007 | Reg loss: 0.039 | Tree loss: 6.007 | Accuracy: 0.574219 | 7.073 sec/iter\n",
      "Epoch: 13 | Batch: 027 / 030 | Total loss: 6.008 | Reg loss: 0.039 | Tree loss: 6.008 | Accuracy: 0.605469 | 7.072 sec/iter\n",
      "Epoch: 13 | Batch: 028 / 030 | Total loss: 6.022 | Reg loss: 0.040 | Tree loss: 6.022 | Accuracy: 0.552734 | 7.071 sec/iter\n",
      "Epoch: 13 | Batch: 029 / 030 | Total loss: 5.973 | Reg loss: 0.040 | Tree loss: 5.973 | Accuracy: 0.601942 | 7.062 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 14 | Batch: 000 / 030 | Total loss: 6.442 | Reg loss: 0.035 | Tree loss: 6.442 | Accuracy: 0.576172 | 7.087 sec/iter\n",
      "Epoch: 14 | Batch: 001 / 030 | Total loss: 6.436 | Reg loss: 0.035 | Tree loss: 6.436 | Accuracy: 0.556641 | 7.087 sec/iter\n",
      "Epoch: 14 | Batch: 002 / 030 | Total loss: 6.380 | Reg loss: 0.035 | Tree loss: 6.380 | Accuracy: 0.583984 | 7.086 sec/iter\n",
      "Epoch: 14 | Batch: 003 / 030 | Total loss: 6.371 | Reg loss: 0.036 | Tree loss: 6.371 | Accuracy: 0.583984 | 7.084 sec/iter\n",
      "Epoch: 14 | Batch: 004 / 030 | Total loss: 6.341 | Reg loss: 0.036 | Tree loss: 6.341 | Accuracy: 0.570312 | 7.083 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14 | Batch: 005 / 030 | Total loss: 6.333 | Reg loss: 0.036 | Tree loss: 6.333 | Accuracy: 0.576172 | 7.081 sec/iter\n",
      "Epoch: 14 | Batch: 006 / 030 | Total loss: 6.268 | Reg loss: 0.036 | Tree loss: 6.268 | Accuracy: 0.628906 | 7.08 sec/iter\n",
      "Epoch: 14 | Batch: 007 / 030 | Total loss: 6.264 | Reg loss: 0.036 | Tree loss: 6.264 | Accuracy: 0.550781 | 7.079 sec/iter\n",
      "Epoch: 14 | Batch: 008 / 030 | Total loss: 6.236 | Reg loss: 0.036 | Tree loss: 6.236 | Accuracy: 0.580078 | 7.079 sec/iter\n",
      "Epoch: 14 | Batch: 009 / 030 | Total loss: 6.190 | Reg loss: 0.036 | Tree loss: 6.190 | Accuracy: 0.628906 | 7.079 sec/iter\n",
      "Epoch: 14 | Batch: 010 / 030 | Total loss: 6.211 | Reg loss: 0.036 | Tree loss: 6.211 | Accuracy: 0.582031 | 7.079 sec/iter\n",
      "Epoch: 14 | Batch: 011 / 030 | Total loss: 6.171 | Reg loss: 0.037 | Tree loss: 6.171 | Accuracy: 0.603516 | 7.08 sec/iter\n",
      "Epoch: 14 | Batch: 012 / 030 | Total loss: 6.097 | Reg loss: 0.037 | Tree loss: 6.097 | Accuracy: 0.578125 | 7.08 sec/iter\n",
      "Epoch: 14 | Batch: 013 / 030 | Total loss: 6.083 | Reg loss: 0.037 | Tree loss: 6.083 | Accuracy: 0.582031 | 7.081 sec/iter\n",
      "Epoch: 14 | Batch: 014 / 030 | Total loss: 6.050 | Reg loss: 0.037 | Tree loss: 6.050 | Accuracy: 0.583984 | 7.079 sec/iter\n",
      "Epoch: 14 | Batch: 015 / 030 | Total loss: 6.039 | Reg loss: 0.037 | Tree loss: 6.039 | Accuracy: 0.580078 | 7.071 sec/iter\n",
      "Epoch: 14 | Batch: 016 / 030 | Total loss: 5.997 | Reg loss: 0.038 | Tree loss: 5.997 | Accuracy: 0.587891 | 7.072 sec/iter\n",
      "Epoch: 14 | Batch: 017 / 030 | Total loss: 5.989 | Reg loss: 0.038 | Tree loss: 5.989 | Accuracy: 0.585938 | 7.073 sec/iter\n",
      "Epoch: 14 | Batch: 018 / 030 | Total loss: 5.942 | Reg loss: 0.038 | Tree loss: 5.942 | Accuracy: 0.597656 | 7.074 sec/iter\n",
      "Epoch: 14 | Batch: 019 / 030 | Total loss: 5.930 | Reg loss: 0.038 | Tree loss: 5.930 | Accuracy: 0.550781 | 7.075 sec/iter\n",
      "Epoch: 14 | Batch: 020 / 030 | Total loss: 5.878 | Reg loss: 0.038 | Tree loss: 5.878 | Accuracy: 0.619141 | 7.075 sec/iter\n",
      "Epoch: 14 | Batch: 021 / 030 | Total loss: 5.880 | Reg loss: 0.039 | Tree loss: 5.880 | Accuracy: 0.564453 | 7.075 sec/iter\n",
      "Epoch: 14 | Batch: 022 / 030 | Total loss: 5.887 | Reg loss: 0.039 | Tree loss: 5.887 | Accuracy: 0.548828 | 7.075 sec/iter\n",
      "Epoch: 14 | Batch: 023 / 030 | Total loss: 5.819 | Reg loss: 0.039 | Tree loss: 5.819 | Accuracy: 0.599609 | 7.076 sec/iter\n",
      "Epoch: 14 | Batch: 024 / 030 | Total loss: 5.828 | Reg loss: 0.039 | Tree loss: 5.828 | Accuracy: 0.599609 | 7.076 sec/iter\n",
      "Epoch: 14 | Batch: 025 / 030 | Total loss: 5.787 | Reg loss: 0.039 | Tree loss: 5.787 | Accuracy: 0.582031 | 7.076 sec/iter\n",
      "Epoch: 14 | Batch: 026 / 030 | Total loss: 5.767 | Reg loss: 0.040 | Tree loss: 5.767 | Accuracy: 0.574219 | 7.077 sec/iter\n",
      "Epoch: 14 | Batch: 027 / 030 | Total loss: 5.760 | Reg loss: 0.040 | Tree loss: 5.760 | Accuracy: 0.570312 | 7.077 sec/iter\n",
      "Epoch: 14 | Batch: 028 / 030 | Total loss: 5.720 | Reg loss: 0.040 | Tree loss: 5.720 | Accuracy: 0.582031 | 7.077 sec/iter\n",
      "Epoch: 14 | Batch: 029 / 030 | Total loss: 5.776 | Reg loss: 0.040 | Tree loss: 5.776 | Accuracy: 0.543689 | 7.068 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 15 | Batch: 000 / 030 | Total loss: 6.163 | Reg loss: 0.036 | Tree loss: 6.163 | Accuracy: 0.587891 | 7.169 sec/iter\n",
      "Epoch: 15 | Batch: 001 / 030 | Total loss: 6.117 | Reg loss: 0.036 | Tree loss: 6.117 | Accuracy: 0.650391 | 7.168 sec/iter\n",
      "Epoch: 15 | Batch: 002 / 030 | Total loss: 6.123 | Reg loss: 0.036 | Tree loss: 6.123 | Accuracy: 0.601562 | 7.168 sec/iter\n",
      "Epoch: 15 | Batch: 003 / 030 | Total loss: 6.121 | Reg loss: 0.036 | Tree loss: 6.121 | Accuracy: 0.583984 | 7.165 sec/iter\n",
      "Epoch: 15 | Batch: 004 / 030 | Total loss: 6.080 | Reg loss: 0.037 | Tree loss: 6.080 | Accuracy: 0.595703 | 7.16 sec/iter\n",
      "Epoch: 15 | Batch: 005 / 030 | Total loss: 6.059 | Reg loss: 0.037 | Tree loss: 6.059 | Accuracy: 0.562500 | 7.155 sec/iter\n",
      "Epoch: 15 | Batch: 006 / 030 | Total loss: 6.012 | Reg loss: 0.037 | Tree loss: 6.012 | Accuracy: 0.568359 | 7.152 sec/iter\n",
      "Epoch: 15 | Batch: 007 / 030 | Total loss: 6.010 | Reg loss: 0.037 | Tree loss: 6.010 | Accuracy: 0.529297 | 7.153 sec/iter\n",
      "Epoch: 15 | Batch: 008 / 030 | Total loss: 5.957 | Reg loss: 0.037 | Tree loss: 5.957 | Accuracy: 0.583984 | 7.154 sec/iter\n",
      "Epoch: 15 | Batch: 009 / 030 | Total loss: 5.916 | Reg loss: 0.037 | Tree loss: 5.916 | Accuracy: 0.576172 | 7.155 sec/iter\n",
      "Epoch: 15 | Batch: 010 / 030 | Total loss: 5.896 | Reg loss: 0.037 | Tree loss: 5.896 | Accuracy: 0.570312 | 7.156 sec/iter\n",
      "Epoch: 15 | Batch: 011 / 030 | Total loss: 5.857 | Reg loss: 0.037 | Tree loss: 5.857 | Accuracy: 0.566406 | 7.157 sec/iter\n",
      "Epoch: 15 | Batch: 012 / 030 | Total loss: 5.859 | Reg loss: 0.038 | Tree loss: 5.859 | Accuracy: 0.537109 | 7.157 sec/iter\n",
      "Epoch: 15 | Batch: 013 / 030 | Total loss: 5.820 | Reg loss: 0.038 | Tree loss: 5.820 | Accuracy: 0.582031 | 7.158 sec/iter\n",
      "Epoch: 15 | Batch: 014 / 030 | Total loss: 5.786 | Reg loss: 0.038 | Tree loss: 5.786 | Accuracy: 0.568359 | 7.158 sec/iter\n",
      "Epoch: 15 | Batch: 015 / 030 | Total loss: 5.736 | Reg loss: 0.038 | Tree loss: 5.736 | Accuracy: 0.605469 | 7.159 sec/iter\n",
      "Epoch: 15 | Batch: 016 / 030 | Total loss: 5.733 | Reg loss: 0.038 | Tree loss: 5.733 | Accuracy: 0.546875 | 7.159 sec/iter\n",
      "Epoch: 15 | Batch: 017 / 030 | Total loss: 5.700 | Reg loss: 0.038 | Tree loss: 5.700 | Accuracy: 0.599609 | 7.159 sec/iter\n",
      "Epoch: 15 | Batch: 018 / 030 | Total loss: 5.672 | Reg loss: 0.039 | Tree loss: 5.672 | Accuracy: 0.587891 | 7.16 sec/iter\n",
      "Epoch: 15 | Batch: 019 / 030 | Total loss: 5.608 | Reg loss: 0.039 | Tree loss: 5.608 | Accuracy: 0.625000 | 7.161 sec/iter\n",
      "Epoch: 15 | Batch: 020 / 030 | Total loss: 5.624 | Reg loss: 0.039 | Tree loss: 5.624 | Accuracy: 0.617188 | 7.161 sec/iter\n",
      "Epoch: 15 | Batch: 021 / 030 | Total loss: 5.622 | Reg loss: 0.039 | Tree loss: 5.622 | Accuracy: 0.548828 | 7.162 sec/iter\n",
      "Epoch: 15 | Batch: 022 / 030 | Total loss: 5.560 | Reg loss: 0.039 | Tree loss: 5.560 | Accuracy: 0.630859 | 7.162 sec/iter\n",
      "Epoch: 15 | Batch: 023 / 030 | Total loss: 5.546 | Reg loss: 0.040 | Tree loss: 5.546 | Accuracy: 0.615234 | 7.162 sec/iter\n",
      "Epoch: 15 | Batch: 024 / 030 | Total loss: 5.581 | Reg loss: 0.040 | Tree loss: 5.581 | Accuracy: 0.550781 | 7.162 sec/iter\n",
      "Epoch: 15 | Batch: 025 / 030 | Total loss: 5.501 | Reg loss: 0.040 | Tree loss: 5.501 | Accuracy: 0.591797 | 7.162 sec/iter\n",
      "Epoch: 15 | Batch: 026 / 030 | Total loss: 5.512 | Reg loss: 0.040 | Tree loss: 5.512 | Accuracy: 0.564453 | 7.162 sec/iter\n",
      "Epoch: 15 | Batch: 027 / 030 | Total loss: 5.470 | Reg loss: 0.040 | Tree loss: 5.470 | Accuracy: 0.578125 | 7.162 sec/iter\n",
      "Epoch: 15 | Batch: 028 / 030 | Total loss: 5.490 | Reg loss: 0.041 | Tree loss: 5.490 | Accuracy: 0.570312 | 7.162 sec/iter\n",
      "Epoch: 15 | Batch: 029 / 030 | Total loss: 5.418 | Reg loss: 0.041 | Tree loss: 5.418 | Accuracy: 0.601942 | 7.153 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 16 | Batch: 000 / 030 | Total loss: 5.890 | Reg loss: 0.037 | Tree loss: 5.890 | Accuracy: 0.605469 | 7.181 sec/iter\n",
      "Epoch: 16 | Batch: 001 / 030 | Total loss: 5.841 | Reg loss: 0.037 | Tree loss: 5.841 | Accuracy: 0.541016 | 7.177 sec/iter\n",
      "Epoch: 16 | Batch: 002 / 030 | Total loss: 5.818 | Reg loss: 0.037 | Tree loss: 5.818 | Accuracy: 0.626953 | 7.174 sec/iter\n",
      "Epoch: 16 | Batch: 003 / 030 | Total loss: 5.830 | Reg loss: 0.037 | Tree loss: 5.830 | Accuracy: 0.552734 | 7.169 sec/iter\n",
      "Epoch: 16 | Batch: 004 / 030 | Total loss: 5.813 | Reg loss: 0.037 | Tree loss: 5.813 | Accuracy: 0.564453 | 7.164 sec/iter\n",
      "Epoch: 16 | Batch: 005 / 030 | Total loss: 5.739 | Reg loss: 0.037 | Tree loss: 5.739 | Accuracy: 0.585938 | 7.159 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16 | Batch: 006 / 030 | Total loss: 5.722 | Reg loss: 0.037 | Tree loss: 5.722 | Accuracy: 0.609375 | 7.154 sec/iter\n",
      "Epoch: 16 | Batch: 007 / 030 | Total loss: 5.691 | Reg loss: 0.037 | Tree loss: 5.691 | Accuracy: 0.578125 | 7.149 sec/iter\n",
      "Epoch: 16 | Batch: 008 / 030 | Total loss: 5.689 | Reg loss: 0.038 | Tree loss: 5.689 | Accuracy: 0.560547 | 7.145 sec/iter\n",
      "Epoch: 16 | Batch: 009 / 030 | Total loss: 5.631 | Reg loss: 0.038 | Tree loss: 5.631 | Accuracy: 0.615234 | 7.14 sec/iter\n",
      "Epoch: 16 | Batch: 010 / 030 | Total loss: 5.648 | Reg loss: 0.038 | Tree loss: 5.648 | Accuracy: 0.554688 | 7.135 sec/iter\n",
      "Epoch: 16 | Batch: 011 / 030 | Total loss: 5.585 | Reg loss: 0.038 | Tree loss: 5.585 | Accuracy: 0.603516 | 7.135 sec/iter\n",
      "Epoch: 16 | Batch: 012 / 030 | Total loss: 5.566 | Reg loss: 0.038 | Tree loss: 5.566 | Accuracy: 0.576172 | 7.135 sec/iter\n",
      "Epoch: 16 | Batch: 013 / 030 | Total loss: 5.549 | Reg loss: 0.038 | Tree loss: 5.549 | Accuracy: 0.583984 | 7.135 sec/iter\n",
      "Epoch: 16 | Batch: 014 / 030 | Total loss: 5.491 | Reg loss: 0.038 | Tree loss: 5.491 | Accuracy: 0.615234 | 7.135 sec/iter\n",
      "Epoch: 16 | Batch: 015 / 030 | Total loss: 5.499 | Reg loss: 0.039 | Tree loss: 5.499 | Accuracy: 0.541016 | 7.135 sec/iter\n",
      "Epoch: 16 | Batch: 016 / 030 | Total loss: 5.449 | Reg loss: 0.039 | Tree loss: 5.449 | Accuracy: 0.589844 | 7.135 sec/iter\n",
      "Epoch: 16 | Batch: 017 / 030 | Total loss: 5.432 | Reg loss: 0.039 | Tree loss: 5.432 | Accuracy: 0.593750 | 7.135 sec/iter\n",
      "Epoch: 16 | Batch: 018 / 030 | Total loss: 5.402 | Reg loss: 0.039 | Tree loss: 5.402 | Accuracy: 0.599609 | 7.135 sec/iter\n",
      "Epoch: 16 | Batch: 019 / 030 | Total loss: 5.357 | Reg loss: 0.039 | Tree loss: 5.357 | Accuracy: 0.607422 | 7.135 sec/iter\n",
      "Epoch: 16 | Batch: 020 / 030 | Total loss: 5.366 | Reg loss: 0.040 | Tree loss: 5.366 | Accuracy: 0.576172 | 7.135 sec/iter\n",
      "Epoch: 16 | Batch: 021 / 030 | Total loss: 5.373 | Reg loss: 0.040 | Tree loss: 5.373 | Accuracy: 0.529297 | 7.135 sec/iter\n",
      "Epoch: 16 | Batch: 022 / 030 | Total loss: 5.324 | Reg loss: 0.040 | Tree loss: 5.324 | Accuracy: 0.585938 | 7.136 sec/iter\n",
      "Epoch: 16 | Batch: 023 / 030 | Total loss: 5.311 | Reg loss: 0.040 | Tree loss: 5.311 | Accuracy: 0.599609 | 7.136 sec/iter\n",
      "Epoch: 16 | Batch: 024 / 030 | Total loss: 5.273 | Reg loss: 0.040 | Tree loss: 5.273 | Accuracy: 0.585938 | 7.135 sec/iter\n",
      "Epoch: 16 | Batch: 025 / 030 | Total loss: 5.243 | Reg loss: 0.040 | Tree loss: 5.243 | Accuracy: 0.585938 | 7.135 sec/iter\n",
      "Epoch: 16 | Batch: 026 / 030 | Total loss: 5.251 | Reg loss: 0.041 | Tree loss: 5.251 | Accuracy: 0.570312 | 7.135 sec/iter\n",
      "Epoch: 16 | Batch: 027 / 030 | Total loss: 5.216 | Reg loss: 0.041 | Tree loss: 5.216 | Accuracy: 0.570312 | 7.135 sec/iter\n",
      "Epoch: 16 | Batch: 028 / 030 | Total loss: 5.191 | Reg loss: 0.041 | Tree loss: 5.191 | Accuracy: 0.587891 | 7.135 sec/iter\n",
      "Epoch: 16 | Batch: 029 / 030 | Total loss: 5.119 | Reg loss: 0.041 | Tree loss: 5.119 | Accuracy: 0.601942 | 7.127 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 17 | Batch: 000 / 030 | Total loss: 5.621 | Reg loss: 0.038 | Tree loss: 5.621 | Accuracy: 0.564453 | 7.132 sec/iter\n",
      "Epoch: 17 | Batch: 001 / 030 | Total loss: 5.551 | Reg loss: 0.038 | Tree loss: 5.551 | Accuracy: 0.597656 | 7.132 sec/iter\n",
      "Epoch: 17 | Batch: 002 / 030 | Total loss: 5.560 | Reg loss: 0.038 | Tree loss: 5.560 | Accuracy: 0.576172 | 7.132 sec/iter\n",
      "Epoch: 17 | Batch: 003 / 030 | Total loss: 5.525 | Reg loss: 0.038 | Tree loss: 5.525 | Accuracy: 0.591797 | 7.132 sec/iter\n",
      "Epoch: 17 | Batch: 004 / 030 | Total loss: 5.510 | Reg loss: 0.038 | Tree loss: 5.510 | Accuracy: 0.601562 | 7.131 sec/iter\n",
      "Epoch: 17 | Batch: 005 / 030 | Total loss: 5.483 | Reg loss: 0.038 | Tree loss: 5.483 | Accuracy: 0.574219 | 7.13 sec/iter\n",
      "Epoch: 17 | Batch: 006 / 030 | Total loss: 5.445 | Reg loss: 0.038 | Tree loss: 5.445 | Accuracy: 0.580078 | 7.129 sec/iter\n",
      "Epoch: 17 | Batch: 007 / 030 | Total loss: 5.431 | Reg loss: 0.038 | Tree loss: 5.431 | Accuracy: 0.552734 | 7.128 sec/iter\n",
      "Epoch: 17 | Batch: 008 / 030 | Total loss: 5.399 | Reg loss: 0.038 | Tree loss: 5.399 | Accuracy: 0.570312 | 7.127 sec/iter\n",
      "Epoch: 17 | Batch: 009 / 030 | Total loss: 5.353 | Reg loss: 0.038 | Tree loss: 5.353 | Accuracy: 0.599609 | 7.125 sec/iter\n",
      "Epoch: 17 | Batch: 010 / 030 | Total loss: 5.346 | Reg loss: 0.038 | Tree loss: 5.346 | Accuracy: 0.574219 | 7.121 sec/iter\n",
      "Epoch: 17 | Batch: 011 / 030 | Total loss: 5.301 | Reg loss: 0.039 | Tree loss: 5.301 | Accuracy: 0.623047 | 7.116 sec/iter\n",
      "Epoch: 17 | Batch: 012 / 030 | Total loss: 5.283 | Reg loss: 0.039 | Tree loss: 5.283 | Accuracy: 0.599609 | 7.112 sec/iter\n",
      "Epoch: 17 | Batch: 013 / 030 | Total loss: 5.260 | Reg loss: 0.039 | Tree loss: 5.260 | Accuracy: 0.568359 | 7.107 sec/iter\n",
      "Epoch: 17 | Batch: 014 / 030 | Total loss: 5.233 | Reg loss: 0.039 | Tree loss: 5.233 | Accuracy: 0.619141 | 7.102 sec/iter\n",
      "Epoch: 17 | Batch: 015 / 030 | Total loss: 5.208 | Reg loss: 0.039 | Tree loss: 5.208 | Accuracy: 0.574219 | 7.098 sec/iter\n",
      "Epoch: 17 | Batch: 016 / 030 | Total loss: 5.203 | Reg loss: 0.039 | Tree loss: 5.203 | Accuracy: 0.570312 | 7.094 sec/iter\n",
      "Epoch: 17 | Batch: 017 / 030 | Total loss: 5.192 | Reg loss: 0.040 | Tree loss: 5.192 | Accuracy: 0.556641 | 7.089 sec/iter\n",
      "Epoch: 17 | Batch: 018 / 030 | Total loss: 5.163 | Reg loss: 0.040 | Tree loss: 5.163 | Accuracy: 0.572266 | 7.09 sec/iter\n",
      "Epoch: 17 | Batch: 019 / 030 | Total loss: 5.115 | Reg loss: 0.040 | Tree loss: 5.115 | Accuracy: 0.578125 | 7.09 sec/iter\n",
      "Epoch: 17 | Batch: 020 / 030 | Total loss: 5.067 | Reg loss: 0.040 | Tree loss: 5.067 | Accuracy: 0.607422 | 7.09 sec/iter\n",
      "Epoch: 17 | Batch: 021 / 030 | Total loss: 5.065 | Reg loss: 0.040 | Tree loss: 5.065 | Accuracy: 0.591797 | 7.09 sec/iter\n",
      "Epoch: 17 | Batch: 022 / 030 | Total loss: 5.043 | Reg loss: 0.041 | Tree loss: 5.043 | Accuracy: 0.568359 | 7.09 sec/iter\n",
      "Epoch: 17 | Batch: 023 / 030 | Total loss: 5.008 | Reg loss: 0.041 | Tree loss: 5.008 | Accuracy: 0.591797 | 7.09 sec/iter\n",
      "Epoch: 17 | Batch: 024 / 030 | Total loss: 5.007 | Reg loss: 0.041 | Tree loss: 5.007 | Accuracy: 0.589844 | 7.089 sec/iter\n",
      "Epoch: 17 | Batch: 025 / 030 | Total loss: 4.996 | Reg loss: 0.041 | Tree loss: 4.996 | Accuracy: 0.564453 | 7.089 sec/iter\n",
      "Epoch: 17 | Batch: 026 / 030 | Total loss: 4.962 | Reg loss: 0.041 | Tree loss: 4.962 | Accuracy: 0.562500 | 7.088 sec/iter\n",
      "Epoch: 17 | Batch: 027 / 030 | Total loss: 4.953 | Reg loss: 0.041 | Tree loss: 4.953 | Accuracy: 0.574219 | 7.087 sec/iter\n",
      "Epoch: 17 | Batch: 028 / 030 | Total loss: 4.907 | Reg loss: 0.042 | Tree loss: 4.907 | Accuracy: 0.591797 | 7.086 sec/iter\n",
      "Epoch: 17 | Batch: 029 / 030 | Total loss: 4.839 | Reg loss: 0.042 | Tree loss: 4.839 | Accuracy: 0.650485 | 7.079 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 18 | Batch: 000 / 030 | Total loss: 5.347 | Reg loss: 0.038 | Tree loss: 5.347 | Accuracy: 0.580078 | 7.095 sec/iter\n",
      "Epoch: 18 | Batch: 001 / 030 | Total loss: 5.296 | Reg loss: 0.038 | Tree loss: 5.296 | Accuracy: 0.599609 | 7.095 sec/iter\n",
      "Epoch: 18 | Batch: 002 / 030 | Total loss: 5.291 | Reg loss: 0.038 | Tree loss: 5.291 | Accuracy: 0.572266 | 7.096 sec/iter\n",
      "Epoch: 18 | Batch: 003 / 030 | Total loss: 5.248 | Reg loss: 0.038 | Tree loss: 5.248 | Accuracy: 0.597656 | 7.096 sec/iter\n",
      "Epoch: 18 | Batch: 004 / 030 | Total loss: 5.242 | Reg loss: 0.038 | Tree loss: 5.242 | Accuracy: 0.582031 | 7.096 sec/iter\n",
      "Epoch: 18 | Batch: 005 / 030 | Total loss: 5.209 | Reg loss: 0.038 | Tree loss: 5.209 | Accuracy: 0.599609 | 7.096 sec/iter\n",
      "Epoch: 18 | Batch: 006 / 030 | Total loss: 5.204 | Reg loss: 0.038 | Tree loss: 5.204 | Accuracy: 0.589844 | 7.096 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18 | Batch: 007 / 030 | Total loss: 5.157 | Reg loss: 0.039 | Tree loss: 5.157 | Accuracy: 0.593750 | 7.096 sec/iter\n",
      "Epoch: 18 | Batch: 008 / 030 | Total loss: 5.148 | Reg loss: 0.039 | Tree loss: 5.148 | Accuracy: 0.572266 | 7.096 sec/iter\n",
      "Epoch: 18 | Batch: 009 / 030 | Total loss: 5.091 | Reg loss: 0.039 | Tree loss: 5.091 | Accuracy: 0.597656 | 7.096 sec/iter\n",
      "Epoch: 18 | Batch: 010 / 030 | Total loss: 5.097 | Reg loss: 0.039 | Tree loss: 5.097 | Accuracy: 0.564453 | 7.096 sec/iter\n",
      "Epoch: 18 | Batch: 011 / 030 | Total loss: 5.032 | Reg loss: 0.039 | Tree loss: 5.032 | Accuracy: 0.582031 | 7.096 sec/iter\n",
      "Epoch: 18 | Batch: 012 / 030 | Total loss: 5.001 | Reg loss: 0.039 | Tree loss: 5.001 | Accuracy: 0.587891 | 7.096 sec/iter\n",
      "Epoch: 18 | Batch: 013 / 030 | Total loss: 5.034 | Reg loss: 0.039 | Tree loss: 5.034 | Accuracy: 0.535156 | 7.096 sec/iter\n",
      "Epoch: 18 | Batch: 014 / 030 | Total loss: 4.948 | Reg loss: 0.040 | Tree loss: 4.948 | Accuracy: 0.617188 | 7.095 sec/iter\n",
      "Epoch: 18 | Batch: 015 / 030 | Total loss: 4.925 | Reg loss: 0.040 | Tree loss: 4.925 | Accuracy: 0.615234 | 7.095 sec/iter\n",
      "Epoch: 18 | Batch: 016 / 030 | Total loss: 4.900 | Reg loss: 0.040 | Tree loss: 4.900 | Accuracy: 0.580078 | 7.091 sec/iter\n",
      "Epoch: 18 | Batch: 017 / 030 | Total loss: 4.908 | Reg loss: 0.040 | Tree loss: 4.908 | Accuracy: 0.558594 | 7.087 sec/iter\n",
      "Epoch: 18 | Batch: 018 / 030 | Total loss: 4.885 | Reg loss: 0.040 | Tree loss: 4.885 | Accuracy: 0.585938 | 7.083 sec/iter\n",
      "Epoch: 18 | Batch: 019 / 030 | Total loss: 4.830 | Reg loss: 0.041 | Tree loss: 4.830 | Accuracy: 0.580078 | 7.079 sec/iter\n",
      "Epoch: 18 | Batch: 020 / 030 | Total loss: 4.823 | Reg loss: 0.041 | Tree loss: 4.823 | Accuracy: 0.597656 | 7.075 sec/iter\n",
      "Epoch: 18 | Batch: 021 / 030 | Total loss: 4.807 | Reg loss: 0.041 | Tree loss: 4.807 | Accuracy: 0.572266 | 7.071 sec/iter\n",
      "Epoch: 18 | Batch: 022 / 030 | Total loss: 4.776 | Reg loss: 0.041 | Tree loss: 4.776 | Accuracy: 0.566406 | 7.067 sec/iter\n",
      "Epoch: 18 | Batch: 023 / 030 | Total loss: 4.766 | Reg loss: 0.041 | Tree loss: 4.766 | Accuracy: 0.552734 | 7.067 sec/iter\n",
      "Epoch: 18 | Batch: 024 / 030 | Total loss: 4.749 | Reg loss: 0.041 | Tree loss: 4.749 | Accuracy: 0.572266 | 7.067 sec/iter\n",
      "Epoch: 18 | Batch: 025 / 030 | Total loss: 4.732 | Reg loss: 0.042 | Tree loss: 4.732 | Accuracy: 0.585938 | 7.067 sec/iter\n",
      "Epoch: 18 | Batch: 026 / 030 | Total loss: 4.682 | Reg loss: 0.042 | Tree loss: 4.682 | Accuracy: 0.625000 | 7.064 sec/iter\n",
      "Epoch: 18 | Batch: 027 / 030 | Total loss: 4.664 | Reg loss: 0.042 | Tree loss: 4.664 | Accuracy: 0.583984 | 7.061 sec/iter\n",
      "Epoch: 18 | Batch: 028 / 030 | Total loss: 4.661 | Reg loss: 0.042 | Tree loss: 4.661 | Accuracy: 0.564453 | 7.058 sec/iter\n",
      "Epoch: 18 | Batch: 029 / 030 | Total loss: 4.587 | Reg loss: 0.042 | Tree loss: 4.587 | Accuracy: 0.524272 | 7.052 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 19 | Batch: 000 / 030 | Total loss: 5.107 | Reg loss: 0.039 | Tree loss: 5.107 | Accuracy: 0.574219 | 7.068 sec/iter\n",
      "Epoch: 19 | Batch: 001 / 030 | Total loss: 5.056 | Reg loss: 0.039 | Tree loss: 5.056 | Accuracy: 0.603516 | 7.067 sec/iter\n",
      "Epoch: 19 | Batch: 002 / 030 | Total loss: 5.051 | Reg loss: 0.039 | Tree loss: 5.051 | Accuracy: 0.585938 | 7.067 sec/iter\n",
      "Epoch: 19 | Batch: 003 / 030 | Total loss: 5.032 | Reg loss: 0.039 | Tree loss: 5.032 | Accuracy: 0.537109 | 7.067 sec/iter\n",
      "Epoch: 19 | Batch: 004 / 030 | Total loss: 4.985 | Reg loss: 0.039 | Tree loss: 4.985 | Accuracy: 0.568359 | 7.067 sec/iter\n",
      "Epoch: 19 | Batch: 005 / 030 | Total loss: 4.955 | Reg loss: 0.039 | Tree loss: 4.955 | Accuracy: 0.587891 | 7.068 sec/iter\n",
      "Epoch: 19 | Batch: 006 / 030 | Total loss: 4.932 | Reg loss: 0.039 | Tree loss: 4.932 | Accuracy: 0.582031 | 7.068 sec/iter\n",
      "Epoch: 19 | Batch: 007 / 030 | Total loss: 4.908 | Reg loss: 0.039 | Tree loss: 4.908 | Accuracy: 0.558594 | 7.069 sec/iter\n",
      "Epoch: 19 | Batch: 008 / 030 | Total loss: 4.845 | Reg loss: 0.039 | Tree loss: 4.845 | Accuracy: 0.607422 | 7.069 sec/iter\n",
      "Epoch: 19 | Batch: 009 / 030 | Total loss: 4.861 | Reg loss: 0.039 | Tree loss: 4.861 | Accuracy: 0.562500 | 7.069 sec/iter\n",
      "Epoch: 19 | Batch: 010 / 030 | Total loss: 4.833 | Reg loss: 0.039 | Tree loss: 4.833 | Accuracy: 0.587891 | 7.07 sec/iter\n",
      "Epoch: 19 | Batch: 011 / 030 | Total loss: 4.776 | Reg loss: 0.040 | Tree loss: 4.776 | Accuracy: 0.607422 | 7.07 sec/iter\n",
      "Epoch: 19 | Batch: 012 / 030 | Total loss: 4.781 | Reg loss: 0.040 | Tree loss: 4.781 | Accuracy: 0.599609 | 7.07 sec/iter\n",
      "Epoch: 19 | Batch: 013 / 030 | Total loss: 4.698 | Reg loss: 0.040 | Tree loss: 4.698 | Accuracy: 0.611328 | 7.07 sec/iter\n",
      "Epoch: 19 | Batch: 014 / 030 | Total loss: 4.718 | Reg loss: 0.040 | Tree loss: 4.718 | Accuracy: 0.591797 | 7.07 sec/iter\n",
      "Epoch: 19 | Batch: 015 / 030 | Total loss: 4.688 | Reg loss: 0.040 | Tree loss: 4.688 | Accuracy: 0.585938 | 7.07 sec/iter\n",
      "Epoch: 19 | Batch: 016 / 030 | Total loss: 4.661 | Reg loss: 0.040 | Tree loss: 4.661 | Accuracy: 0.580078 | 7.07 sec/iter\n",
      "Epoch: 19 | Batch: 017 / 030 | Total loss: 4.621 | Reg loss: 0.041 | Tree loss: 4.621 | Accuracy: 0.576172 | 7.07 sec/iter\n",
      "Epoch: 19 | Batch: 018 / 030 | Total loss: 4.616 | Reg loss: 0.041 | Tree loss: 4.616 | Accuracy: 0.580078 | 7.07 sec/iter\n",
      "Epoch: 19 | Batch: 019 / 030 | Total loss: 4.584 | Reg loss: 0.041 | Tree loss: 4.584 | Accuracy: 0.580078 | 7.07 sec/iter\n",
      "Epoch: 19 | Batch: 020 / 030 | Total loss: 4.554 | Reg loss: 0.041 | Tree loss: 4.554 | Accuracy: 0.585938 | 7.066 sec/iter\n",
      "Epoch: 19 | Batch: 021 / 030 | Total loss: 4.529 | Reg loss: 0.041 | Tree loss: 4.529 | Accuracy: 0.595703 | 7.062 sec/iter\n",
      "Epoch: 19 | Batch: 022 / 030 | Total loss: 4.487 | Reg loss: 0.041 | Tree loss: 4.487 | Accuracy: 0.621094 | 7.058 sec/iter\n",
      "Epoch: 19 | Batch: 023 / 030 | Total loss: 4.521 | Reg loss: 0.042 | Tree loss: 4.521 | Accuracy: 0.548828 | 7.055 sec/iter\n",
      "Epoch: 19 | Batch: 024 / 030 | Total loss: 4.478 | Reg loss: 0.042 | Tree loss: 4.478 | Accuracy: 0.578125 | 7.051 sec/iter\n",
      "Epoch: 19 | Batch: 025 / 030 | Total loss: 4.454 | Reg loss: 0.042 | Tree loss: 4.454 | Accuracy: 0.572266 | 7.048 sec/iter\n",
      "Epoch: 19 | Batch: 026 / 030 | Total loss: 4.445 | Reg loss: 0.042 | Tree loss: 4.445 | Accuracy: 0.568359 | 7.045 sec/iter\n",
      "Epoch: 19 | Batch: 027 / 030 | Total loss: 4.420 | Reg loss: 0.042 | Tree loss: 4.420 | Accuracy: 0.580078 | 7.042 sec/iter\n",
      "Epoch: 19 | Batch: 028 / 030 | Total loss: 4.401 | Reg loss: 0.043 | Tree loss: 4.401 | Accuracy: 0.582031 | 7.042 sec/iter\n",
      "Epoch: 19 | Batch: 029 / 030 | Total loss: 4.391 | Reg loss: 0.043 | Tree loss: 4.391 | Accuracy: 0.582524 | 7.036 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 20 | Batch: 000 / 030 | Total loss: 4.852 | Reg loss: 0.039 | Tree loss: 4.852 | Accuracy: 0.582031 | 7.052 sec/iter\n",
      "Epoch: 20 | Batch: 001 / 030 | Total loss: 4.824 | Reg loss: 0.039 | Tree loss: 4.824 | Accuracy: 0.599609 | 7.052 sec/iter\n",
      "Epoch: 20 | Batch: 002 / 030 | Total loss: 4.787 | Reg loss: 0.039 | Tree loss: 4.787 | Accuracy: 0.564453 | 7.053 sec/iter\n",
      "Epoch: 20 | Batch: 003 / 030 | Total loss: 4.765 | Reg loss: 0.039 | Tree loss: 4.765 | Accuracy: 0.541016 | 7.053 sec/iter\n",
      "Epoch: 20 | Batch: 004 / 030 | Total loss: 4.726 | Reg loss: 0.039 | Tree loss: 4.726 | Accuracy: 0.587891 | 7.053 sec/iter\n",
      "Epoch: 20 | Batch: 005 / 030 | Total loss: 4.678 | Reg loss: 0.039 | Tree loss: 4.678 | Accuracy: 0.591797 | 7.053 sec/iter\n",
      "Epoch: 20 | Batch: 006 / 030 | Total loss: 4.703 | Reg loss: 0.039 | Tree loss: 4.703 | Accuracy: 0.576172 | 7.053 sec/iter\n",
      "Epoch: 20 | Batch: 007 / 030 | Total loss: 4.633 | Reg loss: 0.040 | Tree loss: 4.633 | Accuracy: 0.570312 | 7.053 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20 | Batch: 008 / 030 | Total loss: 4.626 | Reg loss: 0.040 | Tree loss: 4.626 | Accuracy: 0.587891 | 7.053 sec/iter\n",
      "Epoch: 20 | Batch: 009 / 030 | Total loss: 4.627 | Reg loss: 0.040 | Tree loss: 4.627 | Accuracy: 0.564453 | 7.053 sec/iter\n",
      "Epoch: 20 | Batch: 010 / 030 | Total loss: 4.542 | Reg loss: 0.040 | Tree loss: 4.542 | Accuracy: 0.621094 | 7.052 sec/iter\n",
      "Epoch: 20 | Batch: 011 / 030 | Total loss: 4.518 | Reg loss: 0.040 | Tree loss: 4.518 | Accuracy: 0.609375 | 7.052 sec/iter\n",
      "Epoch: 20 | Batch: 012 / 030 | Total loss: 4.541 | Reg loss: 0.040 | Tree loss: 4.541 | Accuracy: 0.556641 | 7.051 sec/iter\n",
      "Epoch: 20 | Batch: 013 / 030 | Total loss: 4.448 | Reg loss: 0.040 | Tree loss: 4.448 | Accuracy: 0.634766 | 7.051 sec/iter\n",
      "Epoch: 20 | Batch: 014 / 030 | Total loss: 4.466 | Reg loss: 0.041 | Tree loss: 4.466 | Accuracy: 0.583984 | 7.051 sec/iter\n",
      "Epoch: 20 | Batch: 015 / 030 | Total loss: 4.440 | Reg loss: 0.041 | Tree loss: 4.440 | Accuracy: 0.583984 | 7.05 sec/iter\n",
      "Epoch: 20 | Batch: 016 / 030 | Total loss: 4.387 | Reg loss: 0.041 | Tree loss: 4.387 | Accuracy: 0.611328 | 7.05 sec/iter\n",
      "Epoch: 20 | Batch: 017 / 030 | Total loss: 4.388 | Reg loss: 0.041 | Tree loss: 4.388 | Accuracy: 0.574219 | 7.05 sec/iter\n",
      "Epoch: 20 | Batch: 018 / 030 | Total loss: 4.350 | Reg loss: 0.041 | Tree loss: 4.350 | Accuracy: 0.568359 | 7.05 sec/iter\n",
      "Epoch: 20 | Batch: 019 / 030 | Total loss: 4.325 | Reg loss: 0.041 | Tree loss: 4.325 | Accuracy: 0.580078 | 7.05 sec/iter\n",
      "Epoch: 20 | Batch: 020 / 030 | Total loss: 4.294 | Reg loss: 0.042 | Tree loss: 4.294 | Accuracy: 0.615234 | 7.05 sec/iter\n",
      "Epoch: 20 | Batch: 021 / 030 | Total loss: 4.320 | Reg loss: 0.042 | Tree loss: 4.320 | Accuracy: 0.550781 | 7.05 sec/iter\n",
      "Epoch: 20 | Batch: 022 / 030 | Total loss: 4.263 | Reg loss: 0.042 | Tree loss: 4.263 | Accuracy: 0.593750 | 7.05 sec/iter\n",
      "Epoch: 20 | Batch: 023 / 030 | Total loss: 4.202 | Reg loss: 0.042 | Tree loss: 4.202 | Accuracy: 0.609375 | 7.048 sec/iter\n",
      "Epoch: 20 | Batch: 024 / 030 | Total loss: 4.187 | Reg loss: 0.042 | Tree loss: 4.187 | Accuracy: 0.587891 | 7.043 sec/iter\n",
      "Epoch: 20 | Batch: 025 / 030 | Total loss: 4.232 | Reg loss: 0.042 | Tree loss: 4.232 | Accuracy: 0.550781 | 7.041 sec/iter\n",
      "Epoch: 20 | Batch: 026 / 030 | Total loss: 4.178 | Reg loss: 0.043 | Tree loss: 4.178 | Accuracy: 0.593750 | 7.042 sec/iter\n",
      "Epoch: 20 | Batch: 027 / 030 | Total loss: 4.177 | Reg loss: 0.043 | Tree loss: 4.177 | Accuracy: 0.531250 | 7.042 sec/iter\n",
      "Epoch: 20 | Batch: 028 / 030 | Total loss: 4.143 | Reg loss: 0.043 | Tree loss: 4.143 | Accuracy: 0.566406 | 7.043 sec/iter\n",
      "Epoch: 20 | Batch: 029 / 030 | Total loss: 4.029 | Reg loss: 0.043 | Tree loss: 4.029 | Accuracy: 0.640777 | 7.037 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 21 | Batch: 000 / 030 | Total loss: 4.612 | Reg loss: 0.040 | Tree loss: 4.612 | Accuracy: 0.562500 | 7.105 sec/iter\n",
      "Epoch: 21 | Batch: 001 / 030 | Total loss: 4.591 | Reg loss: 0.040 | Tree loss: 4.591 | Accuracy: 0.574219 | 7.105 sec/iter\n",
      "Epoch: 21 | Batch: 002 / 030 | Total loss: 4.541 | Reg loss: 0.040 | Tree loss: 4.541 | Accuracy: 0.580078 | 7.105 sec/iter\n",
      "Epoch: 21 | Batch: 003 / 030 | Total loss: 4.541 | Reg loss: 0.040 | Tree loss: 4.541 | Accuracy: 0.574219 | 7.105 sec/iter\n",
      "Epoch: 21 | Batch: 004 / 030 | Total loss: 4.502 | Reg loss: 0.040 | Tree loss: 4.502 | Accuracy: 0.574219 | 7.105 sec/iter\n",
      "Epoch: 21 | Batch: 005 / 030 | Total loss: 4.476 | Reg loss: 0.040 | Tree loss: 4.476 | Accuracy: 0.574219 | 7.104 sec/iter\n",
      "Epoch: 21 | Batch: 006 / 030 | Total loss: 4.441 | Reg loss: 0.040 | Tree loss: 4.441 | Accuracy: 0.605469 | 7.104 sec/iter\n",
      "Epoch: 21 | Batch: 007 / 030 | Total loss: 4.400 | Reg loss: 0.040 | Tree loss: 4.400 | Accuracy: 0.593750 | 7.104 sec/iter\n",
      "Epoch: 21 | Batch: 008 / 030 | Total loss: 4.398 | Reg loss: 0.040 | Tree loss: 4.398 | Accuracy: 0.556641 | 7.103 sec/iter\n",
      "Epoch: 21 | Batch: 009 / 030 | Total loss: 4.376 | Reg loss: 0.040 | Tree loss: 4.376 | Accuracy: 0.568359 | 7.103 sec/iter\n",
      "Epoch: 21 | Batch: 010 / 030 | Total loss: 4.317 | Reg loss: 0.040 | Tree loss: 4.317 | Accuracy: 0.601562 | 7.102 sec/iter\n",
      "Epoch: 21 | Batch: 011 / 030 | Total loss: 4.291 | Reg loss: 0.040 | Tree loss: 4.291 | Accuracy: 0.603516 | 7.102 sec/iter\n",
      "Epoch: 21 | Batch: 012 / 030 | Total loss: 4.274 | Reg loss: 0.041 | Tree loss: 4.274 | Accuracy: 0.607422 | 7.102 sec/iter\n",
      "Epoch: 21 | Batch: 013 / 030 | Total loss: 4.248 | Reg loss: 0.041 | Tree loss: 4.248 | Accuracy: 0.570312 | 7.102 sec/iter\n",
      "Epoch: 21 | Batch: 014 / 030 | Total loss: 4.214 | Reg loss: 0.041 | Tree loss: 4.214 | Accuracy: 0.582031 | 7.101 sec/iter\n",
      "Epoch: 21 | Batch: 015 / 030 | Total loss: 4.196 | Reg loss: 0.041 | Tree loss: 4.196 | Accuracy: 0.554688 | 7.096 sec/iter\n",
      "Epoch: 21 | Batch: 016 / 030 | Total loss: 4.160 | Reg loss: 0.041 | Tree loss: 4.160 | Accuracy: 0.578125 | 7.091 sec/iter\n",
      "Epoch: 21 | Batch: 017 / 030 | Total loss: 4.113 | Reg loss: 0.041 | Tree loss: 4.113 | Accuracy: 0.621094 | 7.092 sec/iter\n",
      "Epoch: 21 | Batch: 018 / 030 | Total loss: 4.104 | Reg loss: 0.042 | Tree loss: 4.104 | Accuracy: 0.601562 | 7.092 sec/iter\n",
      "Epoch: 21 | Batch: 019 / 030 | Total loss: 4.078 | Reg loss: 0.042 | Tree loss: 4.078 | Accuracy: 0.593750 | 7.093 sec/iter\n",
      "Epoch: 21 | Batch: 020 / 030 | Total loss: 4.045 | Reg loss: 0.042 | Tree loss: 4.045 | Accuracy: 0.585938 | 7.093 sec/iter\n",
      "Epoch: 21 | Batch: 021 / 030 | Total loss: 4.017 | Reg loss: 0.042 | Tree loss: 4.017 | Accuracy: 0.628906 | 7.094 sec/iter\n",
      "Epoch: 21 | Batch: 022 / 030 | Total loss: 4.028 | Reg loss: 0.042 | Tree loss: 4.028 | Accuracy: 0.587891 | 7.094 sec/iter\n",
      "Epoch: 21 | Batch: 023 / 030 | Total loss: 3.975 | Reg loss: 0.042 | Tree loss: 3.975 | Accuracy: 0.597656 | 7.094 sec/iter\n",
      "Epoch: 21 | Batch: 024 / 030 | Total loss: 3.981 | Reg loss: 0.043 | Tree loss: 3.981 | Accuracy: 0.566406 | 7.094 sec/iter\n",
      "Epoch: 21 | Batch: 025 / 030 | Total loss: 3.962 | Reg loss: 0.043 | Tree loss: 3.962 | Accuracy: 0.539062 | 7.095 sec/iter\n",
      "Epoch: 21 | Batch: 026 / 030 | Total loss: 3.923 | Reg loss: 0.043 | Tree loss: 3.923 | Accuracy: 0.541016 | 7.095 sec/iter\n",
      "Epoch: 21 | Batch: 027 / 030 | Total loss: 3.911 | Reg loss: 0.043 | Tree loss: 3.911 | Accuracy: 0.580078 | 7.095 sec/iter\n",
      "Epoch: 21 | Batch: 028 / 030 | Total loss: 3.860 | Reg loss: 0.043 | Tree loss: 3.860 | Accuracy: 0.591797 | 7.095 sec/iter\n",
      "Epoch: 21 | Batch: 029 / 030 | Total loss: 3.801 | Reg loss: 0.043 | Tree loss: 3.801 | Accuracy: 0.601942 | 7.09 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 22 | Batch: 000 / 030 | Total loss: 4.371 | Reg loss: 0.040 | Tree loss: 4.371 | Accuracy: 0.576172 | 7.153 sec/iter\n",
      "Epoch: 22 | Batch: 001 / 030 | Total loss: 4.319 | Reg loss: 0.040 | Tree loss: 4.319 | Accuracy: 0.589844 | 7.153 sec/iter\n",
      "Epoch: 22 | Batch: 002 / 030 | Total loss: 4.330 | Reg loss: 0.040 | Tree loss: 4.330 | Accuracy: 0.585938 | 7.153 sec/iter\n",
      "Epoch: 22 | Batch: 003 / 030 | Total loss: 4.301 | Reg loss: 0.040 | Tree loss: 4.301 | Accuracy: 0.597656 | 7.153 sec/iter\n",
      "Epoch: 22 | Batch: 004 / 030 | Total loss: 4.244 | Reg loss: 0.040 | Tree loss: 4.244 | Accuracy: 0.564453 | 7.152 sec/iter\n",
      "Epoch: 22 | Batch: 005 / 030 | Total loss: 4.253 | Reg loss: 0.040 | Tree loss: 4.253 | Accuracy: 0.560547 | 7.151 sec/iter\n",
      "Epoch: 22 | Batch: 006 / 030 | Total loss: 4.179 | Reg loss: 0.040 | Tree loss: 4.179 | Accuracy: 0.609375 | 7.148 sec/iter\n",
      "Epoch: 22 | Batch: 007 / 030 | Total loss: 4.167 | Reg loss: 0.040 | Tree loss: 4.167 | Accuracy: 0.595703 | 7.143 sec/iter\n",
      "Epoch: 22 | Batch: 008 / 030 | Total loss: 4.176 | Reg loss: 0.041 | Tree loss: 4.176 | Accuracy: 0.558594 | 7.14 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22 | Batch: 009 / 030 | Total loss: 4.097 | Reg loss: 0.041 | Tree loss: 4.097 | Accuracy: 0.585938 | 7.137 sec/iter\n",
      "Epoch: 22 | Batch: 010 / 030 | Total loss: 4.058 | Reg loss: 0.041 | Tree loss: 4.058 | Accuracy: 0.576172 | 7.137 sec/iter\n",
      "Epoch: 22 | Batch: 011 / 030 | Total loss: 4.059 | Reg loss: 0.041 | Tree loss: 4.059 | Accuracy: 0.582031 | 7.137 sec/iter\n",
      "Epoch: 22 | Batch: 012 / 030 | Total loss: 4.048 | Reg loss: 0.041 | Tree loss: 4.048 | Accuracy: 0.544922 | 7.137 sec/iter\n",
      "Epoch: 22 | Batch: 013 / 030 | Total loss: 3.995 | Reg loss: 0.041 | Tree loss: 3.995 | Accuracy: 0.587891 | 7.138 sec/iter\n",
      "Epoch: 22 | Batch: 014 / 030 | Total loss: 3.959 | Reg loss: 0.041 | Tree loss: 3.959 | Accuracy: 0.626953 | 7.138 sec/iter\n",
      "Epoch: 22 | Batch: 015 / 030 | Total loss: 3.938 | Reg loss: 0.042 | Tree loss: 3.938 | Accuracy: 0.599609 | 7.138 sec/iter\n",
      "Epoch: 22 | Batch: 016 / 030 | Total loss: 3.924 | Reg loss: 0.042 | Tree loss: 3.924 | Accuracy: 0.603516 | 7.138 sec/iter\n",
      "Epoch: 22 | Batch: 017 / 030 | Total loss: 3.884 | Reg loss: 0.042 | Tree loss: 3.884 | Accuracy: 0.576172 | 7.138 sec/iter\n",
      "Epoch: 22 | Batch: 018 / 030 | Total loss: 3.850 | Reg loss: 0.042 | Tree loss: 3.850 | Accuracy: 0.587891 | 7.138 sec/iter\n",
      "Epoch: 22 | Batch: 019 / 030 | Total loss: 3.812 | Reg loss: 0.042 | Tree loss: 3.812 | Accuracy: 0.576172 | 7.138 sec/iter\n",
      "Epoch: 22 | Batch: 020 / 030 | Total loss: 3.829 | Reg loss: 0.042 | Tree loss: 3.829 | Accuracy: 0.572266 | 7.138 sec/iter\n",
      "Epoch: 22 | Batch: 021 / 030 | Total loss: 3.786 | Reg loss: 0.043 | Tree loss: 3.786 | Accuracy: 0.568359 | 7.138 sec/iter\n",
      "Epoch: 22 | Batch: 022 / 030 | Total loss: 3.742 | Reg loss: 0.043 | Tree loss: 3.742 | Accuracy: 0.623047 | 7.137 sec/iter\n",
      "Epoch: 22 | Batch: 023 / 030 | Total loss: 3.733 | Reg loss: 0.043 | Tree loss: 3.733 | Accuracy: 0.582031 | 7.137 sec/iter\n",
      "Epoch: 22 | Batch: 024 / 030 | Total loss: 3.716 | Reg loss: 0.043 | Tree loss: 3.716 | Accuracy: 0.564453 | 7.137 sec/iter\n",
      "Epoch: 22 | Batch: 025 / 030 | Total loss: 3.682 | Reg loss: 0.043 | Tree loss: 3.682 | Accuracy: 0.578125 | 7.137 sec/iter\n",
      "Epoch: 22 | Batch: 026 / 030 | Total loss: 3.705 | Reg loss: 0.043 | Tree loss: 3.705 | Accuracy: 0.531250 | 7.137 sec/iter\n",
      "Epoch: 22 | Batch: 027 / 030 | Total loss: 3.619 | Reg loss: 0.043 | Tree loss: 3.619 | Accuracy: 0.589844 | 7.136 sec/iter\n",
      "Epoch: 22 | Batch: 028 / 030 | Total loss: 3.615 | Reg loss: 0.044 | Tree loss: 3.615 | Accuracy: 0.587891 | 7.136 sec/iter\n",
      "Epoch: 22 | Batch: 029 / 030 | Total loss: 3.554 | Reg loss: 0.044 | Tree loss: 3.554 | Accuracy: 0.669903 | 7.131 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 23 | Batch: 000 / 030 | Total loss: 4.135 | Reg loss: 0.041 | Tree loss: 4.135 | Accuracy: 0.591797 | 7.165 sec/iter\n",
      "Epoch: 23 | Batch: 001 / 030 | Total loss: 4.116 | Reg loss: 0.041 | Tree loss: 4.116 | Accuracy: 0.609375 | 7.162 sec/iter\n",
      "Epoch: 23 | Batch: 002 / 030 | Total loss: 4.089 | Reg loss: 0.041 | Tree loss: 4.089 | Accuracy: 0.587891 | 7.16 sec/iter\n",
      "Epoch: 23 | Batch: 003 / 030 | Total loss: 4.048 | Reg loss: 0.041 | Tree loss: 4.048 | Accuracy: 0.574219 | 7.16 sec/iter\n",
      "Epoch: 23 | Batch: 004 / 030 | Total loss: 4.080 | Reg loss: 0.041 | Tree loss: 4.080 | Accuracy: 0.533203 | 7.156 sec/iter\n",
      "Epoch: 23 | Batch: 005 / 030 | Total loss: 4.026 | Reg loss: 0.041 | Tree loss: 4.026 | Accuracy: 0.568359 | 7.153 sec/iter\n",
      "Epoch: 23 | Batch: 006 / 030 | Total loss: 3.969 | Reg loss: 0.041 | Tree loss: 3.969 | Accuracy: 0.583984 | 7.15 sec/iter\n",
      "Epoch: 23 | Batch: 007 / 030 | Total loss: 3.922 | Reg loss: 0.041 | Tree loss: 3.922 | Accuracy: 0.595703 | 7.146 sec/iter\n",
      "Epoch: 23 | Batch: 008 / 030 | Total loss: 3.920 | Reg loss: 0.041 | Tree loss: 3.920 | Accuracy: 0.580078 | 7.143 sec/iter\n",
      "Epoch: 23 | Batch: 009 / 030 | Total loss: 3.895 | Reg loss: 0.041 | Tree loss: 3.895 | Accuracy: 0.572266 | 7.14 sec/iter\n",
      "Epoch: 23 | Batch: 010 / 030 | Total loss: 3.848 | Reg loss: 0.041 | Tree loss: 3.848 | Accuracy: 0.562500 | 7.139 sec/iter\n",
      "Epoch: 23 | Batch: 011 / 030 | Total loss: 3.825 | Reg loss: 0.042 | Tree loss: 3.825 | Accuracy: 0.566406 | 7.139 sec/iter\n",
      "Epoch: 23 | Batch: 012 / 030 | Total loss: 3.818 | Reg loss: 0.042 | Tree loss: 3.818 | Accuracy: 0.560547 | 7.139 sec/iter\n",
      "Epoch: 23 | Batch: 013 / 030 | Total loss: 3.757 | Reg loss: 0.042 | Tree loss: 3.757 | Accuracy: 0.599609 | 7.139 sec/iter\n",
      "Epoch: 23 | Batch: 014 / 030 | Total loss: 3.714 | Reg loss: 0.042 | Tree loss: 3.714 | Accuracy: 0.601562 | 7.139 sec/iter\n",
      "Epoch: 23 | Batch: 015 / 030 | Total loss: 3.714 | Reg loss: 0.042 | Tree loss: 3.714 | Accuracy: 0.582031 | 7.139 sec/iter\n",
      "Epoch: 23 | Batch: 016 / 030 | Total loss: 3.672 | Reg loss: 0.042 | Tree loss: 3.672 | Accuracy: 0.576172 | 7.138 sec/iter\n",
      "Epoch: 23 | Batch: 017 / 030 | Total loss: 3.603 | Reg loss: 0.042 | Tree loss: 3.603 | Accuracy: 0.628906 | 7.138 sec/iter\n",
      "Epoch: 23 | Batch: 018 / 030 | Total loss: 3.624 | Reg loss: 0.043 | Tree loss: 3.624 | Accuracy: 0.583984 | 7.138 sec/iter\n",
      "Epoch: 23 | Batch: 019 / 030 | Total loss: 3.587 | Reg loss: 0.043 | Tree loss: 3.587 | Accuracy: 0.615234 | 7.138 sec/iter\n",
      "Epoch: 23 | Batch: 020 / 030 | Total loss: 3.606 | Reg loss: 0.043 | Tree loss: 3.606 | Accuracy: 0.564453 | 7.137 sec/iter\n",
      "Epoch: 23 | Batch: 021 / 030 | Total loss: 3.562 | Reg loss: 0.043 | Tree loss: 3.562 | Accuracy: 0.574219 | 7.137 sec/iter\n",
      "Epoch: 23 | Batch: 022 / 030 | Total loss: 3.528 | Reg loss: 0.043 | Tree loss: 3.528 | Accuracy: 0.582031 | 7.137 sec/iter\n",
      "Epoch: 23 | Batch: 023 / 030 | Total loss: 3.496 | Reg loss: 0.043 | Tree loss: 3.496 | Accuracy: 0.574219 | 7.137 sec/iter\n",
      "Epoch: 23 | Batch: 024 / 030 | Total loss: 3.485 | Reg loss: 0.044 | Tree loss: 3.485 | Accuracy: 0.580078 | 7.137 sec/iter\n",
      "Epoch: 23 | Batch: 025 / 030 | Total loss: 3.464 | Reg loss: 0.044 | Tree loss: 3.464 | Accuracy: 0.539062 | 7.136 sec/iter\n",
      "Epoch: 23 | Batch: 026 / 030 | Total loss: 3.435 | Reg loss: 0.044 | Tree loss: 3.435 | Accuracy: 0.582031 | 7.136 sec/iter\n",
      "Epoch: 23 | Batch: 027 / 030 | Total loss: 3.383 | Reg loss: 0.044 | Tree loss: 3.383 | Accuracy: 0.628906 | 7.136 sec/iter\n",
      "Epoch: 23 | Batch: 028 / 030 | Total loss: 3.350 | Reg loss: 0.044 | Tree loss: 3.350 | Accuracy: 0.599609 | 7.136 sec/iter\n",
      "Epoch: 23 | Batch: 029 / 030 | Total loss: 3.351 | Reg loss: 0.044 | Tree loss: 3.351 | Accuracy: 0.592233 | 7.13 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 24 | Batch: 000 / 030 | Total loss: 3.940 | Reg loss: 0.041 | Tree loss: 3.940 | Accuracy: 0.613281 | 7.137 sec/iter\n",
      "Epoch: 24 | Batch: 001 / 030 | Total loss: 3.927 | Reg loss: 0.041 | Tree loss: 3.927 | Accuracy: 0.548828 | 7.136 sec/iter\n",
      "Epoch: 24 | Batch: 002 / 030 | Total loss: 3.853 | Reg loss: 0.041 | Tree loss: 3.853 | Accuracy: 0.572266 | 7.135 sec/iter\n",
      "Epoch: 24 | Batch: 003 / 030 | Total loss: 3.819 | Reg loss: 0.041 | Tree loss: 3.819 | Accuracy: 0.605469 | 7.134 sec/iter\n",
      "Epoch: 24 | Batch: 004 / 030 | Total loss: 3.819 | Reg loss: 0.042 | Tree loss: 3.819 | Accuracy: 0.599609 | 7.133 sec/iter\n",
      "Epoch: 24 | Batch: 005 / 030 | Total loss: 3.785 | Reg loss: 0.042 | Tree loss: 3.785 | Accuracy: 0.562500 | 7.133 sec/iter\n",
      "Epoch: 24 | Batch: 006 / 030 | Total loss: 3.755 | Reg loss: 0.042 | Tree loss: 3.755 | Accuracy: 0.552734 | 7.132 sec/iter\n",
      "Epoch: 24 | Batch: 007 / 030 | Total loss: 3.745 | Reg loss: 0.042 | Tree loss: 3.745 | Accuracy: 0.548828 | 7.132 sec/iter\n",
      "Epoch: 24 | Batch: 008 / 030 | Total loss: 3.699 | Reg loss: 0.042 | Tree loss: 3.699 | Accuracy: 0.595703 | 7.131 sec/iter\n",
      "Epoch: 24 | Batch: 009 / 030 | Total loss: 3.663 | Reg loss: 0.042 | Tree loss: 3.663 | Accuracy: 0.582031 | 7.128 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24 | Batch: 010 / 030 | Total loss: 3.671 | Reg loss: 0.042 | Tree loss: 3.671 | Accuracy: 0.578125 | 7.125 sec/iter\n",
      "Epoch: 24 | Batch: 011 / 030 | Total loss: 3.594 | Reg loss: 0.042 | Tree loss: 3.594 | Accuracy: 0.597656 | 7.121 sec/iter\n",
      "Epoch: 24 | Batch: 012 / 030 | Total loss: 3.597 | Reg loss: 0.042 | Tree loss: 3.597 | Accuracy: 0.583984 | 7.118 sec/iter\n",
      "Epoch: 24 | Batch: 013 / 030 | Total loss: 3.526 | Reg loss: 0.042 | Tree loss: 3.526 | Accuracy: 0.605469 | 7.115 sec/iter\n",
      "Epoch: 24 | Batch: 014 / 030 | Total loss: 3.523 | Reg loss: 0.043 | Tree loss: 3.523 | Accuracy: 0.603516 | 7.112 sec/iter\n",
      "Epoch: 24 | Batch: 015 / 030 | Total loss: 3.504 | Reg loss: 0.043 | Tree loss: 3.504 | Accuracy: 0.574219 | 7.109 sec/iter\n",
      "Epoch: 24 | Batch: 016 / 030 | Total loss: 3.514 | Reg loss: 0.043 | Tree loss: 3.514 | Accuracy: 0.552734 | 7.109 sec/iter\n",
      "Epoch: 24 | Batch: 017 / 030 | Total loss: 3.459 | Reg loss: 0.043 | Tree loss: 3.459 | Accuracy: 0.566406 | 7.109 sec/iter\n",
      "Epoch: 24 | Batch: 018 / 030 | Total loss: 3.434 | Reg loss: 0.043 | Tree loss: 3.434 | Accuracy: 0.574219 | 7.109 sec/iter\n",
      "Epoch: 24 | Batch: 019 / 030 | Total loss: 3.380 | Reg loss: 0.043 | Tree loss: 3.380 | Accuracy: 0.593750 | 7.109 sec/iter\n",
      "Epoch: 24 | Batch: 020 / 030 | Total loss: 3.399 | Reg loss: 0.043 | Tree loss: 3.399 | Accuracy: 0.558594 | 7.109 sec/iter\n",
      "Epoch: 24 | Batch: 021 / 030 | Total loss: 3.363 | Reg loss: 0.044 | Tree loss: 3.363 | Accuracy: 0.562500 | 7.109 sec/iter\n",
      "Epoch: 24 | Batch: 022 / 030 | Total loss: 3.316 | Reg loss: 0.044 | Tree loss: 3.316 | Accuracy: 0.582031 | 7.109 sec/iter\n",
      "Epoch: 24 | Batch: 023 / 030 | Total loss: 3.276 | Reg loss: 0.044 | Tree loss: 3.276 | Accuracy: 0.593750 | 7.108 sec/iter\n",
      "Epoch: 24 | Batch: 024 / 030 | Total loss: 3.266 | Reg loss: 0.044 | Tree loss: 3.266 | Accuracy: 0.568359 | 7.108 sec/iter\n",
      "Epoch: 24 | Batch: 025 / 030 | Total loss: 3.241 | Reg loss: 0.044 | Tree loss: 3.241 | Accuracy: 0.589844 | 7.107 sec/iter\n",
      "Epoch: 24 | Batch: 026 / 030 | Total loss: 3.217 | Reg loss: 0.044 | Tree loss: 3.217 | Accuracy: 0.595703 | 7.106 sec/iter\n",
      "Epoch: 24 | Batch: 027 / 030 | Total loss: 3.168 | Reg loss: 0.044 | Tree loss: 3.168 | Accuracy: 0.601562 | 7.105 sec/iter\n",
      "Epoch: 24 | Batch: 028 / 030 | Total loss: 3.151 | Reg loss: 0.045 | Tree loss: 3.151 | Accuracy: 0.625000 | 7.105 sec/iter\n",
      "Epoch: 24 | Batch: 029 / 030 | Total loss: 3.116 | Reg loss: 0.045 | Tree loss: 3.116 | Accuracy: 0.640777 | 7.1 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 25 | Batch: 000 / 030 | Total loss: 3.742 | Reg loss: 0.042 | Tree loss: 3.742 | Accuracy: 0.552734 | 7.106 sec/iter\n",
      "Epoch: 25 | Batch: 001 / 030 | Total loss: 3.739 | Reg loss: 0.042 | Tree loss: 3.739 | Accuracy: 0.558594 | 7.106 sec/iter\n",
      "Epoch: 25 | Batch: 002 / 030 | Total loss: 3.666 | Reg loss: 0.042 | Tree loss: 3.666 | Accuracy: 0.572266 | 7.106 sec/iter\n",
      "Epoch: 25 | Batch: 003 / 030 | Total loss: 3.636 | Reg loss: 0.042 | Tree loss: 3.636 | Accuracy: 0.576172 | 7.106 sec/iter\n",
      "Epoch: 25 | Batch: 004 / 030 | Total loss: 3.641 | Reg loss: 0.042 | Tree loss: 3.641 | Accuracy: 0.576172 | 7.107 sec/iter\n",
      "Epoch: 25 | Batch: 005 / 030 | Total loss: 3.539 | Reg loss: 0.042 | Tree loss: 3.539 | Accuracy: 0.640625 | 7.107 sec/iter\n",
      "Epoch: 25 | Batch: 006 / 030 | Total loss: 3.504 | Reg loss: 0.042 | Tree loss: 3.504 | Accuracy: 0.613281 | 7.106 sec/iter\n",
      "Epoch: 25 | Batch: 007 / 030 | Total loss: 3.534 | Reg loss: 0.042 | Tree loss: 3.534 | Accuracy: 0.599609 | 7.106 sec/iter\n",
      "Epoch: 25 | Batch: 008 / 030 | Total loss: 3.497 | Reg loss: 0.042 | Tree loss: 3.497 | Accuracy: 0.576172 | 7.105 sec/iter\n",
      "Epoch: 25 | Batch: 009 / 030 | Total loss: 3.475 | Reg loss: 0.043 | Tree loss: 3.475 | Accuracy: 0.566406 | 7.105 sec/iter\n",
      "Epoch: 25 | Batch: 010 / 030 | Total loss: 3.441 | Reg loss: 0.043 | Tree loss: 3.441 | Accuracy: 0.580078 | 7.104 sec/iter\n",
      "Epoch: 25 | Batch: 011 / 030 | Total loss: 3.422 | Reg loss: 0.043 | Tree loss: 3.422 | Accuracy: 0.597656 | 7.104 sec/iter\n",
      "Epoch: 25 | Batch: 012 / 030 | Total loss: 3.411 | Reg loss: 0.043 | Tree loss: 3.411 | Accuracy: 0.576172 | 7.103 sec/iter\n",
      "Epoch: 25 | Batch: 013 / 030 | Total loss: 3.355 | Reg loss: 0.043 | Tree loss: 3.355 | Accuracy: 0.578125 | 7.103 sec/iter\n",
      "Epoch: 25 | Batch: 014 / 030 | Total loss: 3.342 | Reg loss: 0.043 | Tree loss: 3.342 | Accuracy: 0.597656 | 7.102 sec/iter\n",
      "Epoch: 25 | Batch: 015 / 030 | Total loss: 3.291 | Reg loss: 0.043 | Tree loss: 3.291 | Accuracy: 0.597656 | 7.101 sec/iter\n",
      "Epoch: 25 | Batch: 016 / 030 | Total loss: 3.284 | Reg loss: 0.043 | Tree loss: 3.284 | Accuracy: 0.572266 | 7.099 sec/iter\n",
      "Epoch: 25 | Batch: 017 / 030 | Total loss: 3.258 | Reg loss: 0.043 | Tree loss: 3.258 | Accuracy: 0.601562 | 7.097 sec/iter\n",
      "Epoch: 25 | Batch: 018 / 030 | Total loss: 3.211 | Reg loss: 0.044 | Tree loss: 3.211 | Accuracy: 0.605469 | 7.097 sec/iter\n",
      "Epoch: 25 | Batch: 019 / 030 | Total loss: 3.232 | Reg loss: 0.044 | Tree loss: 3.232 | Accuracy: 0.572266 | 7.097 sec/iter\n",
      "Epoch: 25 | Batch: 020 / 030 | Total loss: 3.158 | Reg loss: 0.044 | Tree loss: 3.158 | Accuracy: 0.601562 | 7.098 sec/iter\n",
      "Epoch: 25 | Batch: 021 / 030 | Total loss: 3.148 | Reg loss: 0.044 | Tree loss: 3.148 | Accuracy: 0.576172 | 7.098 sec/iter\n",
      "Epoch: 25 | Batch: 022 / 030 | Total loss: 3.149 | Reg loss: 0.044 | Tree loss: 3.149 | Accuracy: 0.574219 | 7.098 sec/iter\n",
      "Epoch: 25 | Batch: 023 / 030 | Total loss: 3.112 | Reg loss: 0.044 | Tree loss: 3.112 | Accuracy: 0.570312 | 7.098 sec/iter\n",
      "Epoch: 25 | Batch: 024 / 030 | Total loss: 3.125 | Reg loss: 0.044 | Tree loss: 3.125 | Accuracy: 0.556641 | 7.098 sec/iter\n",
      "Epoch: 25 | Batch: 025 / 030 | Total loss: 3.070 | Reg loss: 0.045 | Tree loss: 3.070 | Accuracy: 0.539062 | 7.098 sec/iter\n",
      "Epoch: 25 | Batch: 026 / 030 | Total loss: 3.030 | Reg loss: 0.045 | Tree loss: 3.030 | Accuracy: 0.603516 | 7.097 sec/iter\n",
      "Epoch: 25 | Batch: 027 / 030 | Total loss: 2.998 | Reg loss: 0.045 | Tree loss: 2.998 | Accuracy: 0.599609 | 7.095 sec/iter\n",
      "Epoch: 25 | Batch: 028 / 030 | Total loss: 2.986 | Reg loss: 0.045 | Tree loss: 2.986 | Accuracy: 0.562500 | 7.096 sec/iter\n",
      "Epoch: 25 | Batch: 029 / 030 | Total loss: 2.991 | Reg loss: 0.045 | Tree loss: 2.991 | Accuracy: 0.611650 | 7.091 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 26 | Batch: 000 / 030 | Total loss: 3.532 | Reg loss: 0.043 | Tree loss: 3.532 | Accuracy: 0.609375 | 7.108 sec/iter\n",
      "Epoch: 26 | Batch: 001 / 030 | Total loss: 3.499 | Reg loss: 0.043 | Tree loss: 3.499 | Accuracy: 0.591797 | 7.108 sec/iter\n",
      "Epoch: 26 | Batch: 002 / 030 | Total loss: 3.483 | Reg loss: 0.043 | Tree loss: 3.483 | Accuracy: 0.554688 | 7.108 sec/iter\n",
      "Epoch: 26 | Batch: 003 / 030 | Total loss: 3.481 | Reg loss: 0.043 | Tree loss: 3.481 | Accuracy: 0.578125 | 7.108 sec/iter\n",
      "Epoch: 26 | Batch: 004 / 030 | Total loss: 3.458 | Reg loss: 0.043 | Tree loss: 3.458 | Accuracy: 0.595703 | 7.108 sec/iter\n",
      "Epoch: 26 | Batch: 005 / 030 | Total loss: 3.411 | Reg loss: 0.043 | Tree loss: 3.411 | Accuracy: 0.587891 | 7.108 sec/iter\n",
      "Epoch: 26 | Batch: 006 / 030 | Total loss: 3.397 | Reg loss: 0.043 | Tree loss: 3.397 | Accuracy: 0.548828 | 7.108 sec/iter\n",
      "Epoch: 26 | Batch: 007 / 030 | Total loss: 3.320 | Reg loss: 0.043 | Tree loss: 3.320 | Accuracy: 0.591797 | 7.108 sec/iter\n",
      "Epoch: 26 | Batch: 008 / 030 | Total loss: 3.337 | Reg loss: 0.043 | Tree loss: 3.337 | Accuracy: 0.564453 | 7.107 sec/iter\n",
      "Epoch: 26 | Batch: 009 / 030 | Total loss: 3.269 | Reg loss: 0.043 | Tree loss: 3.269 | Accuracy: 0.609375 | 7.107 sec/iter\n",
      "Epoch: 26 | Batch: 010 / 030 | Total loss: 3.249 | Reg loss: 0.043 | Tree loss: 3.249 | Accuracy: 0.583984 | 7.107 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26 | Batch: 011 / 030 | Total loss: 3.247 | Reg loss: 0.043 | Tree loss: 3.247 | Accuracy: 0.574219 | 7.107 sec/iter\n",
      "Epoch: 26 | Batch: 012 / 030 | Total loss: 3.225 | Reg loss: 0.043 | Tree loss: 3.225 | Accuracy: 0.568359 | 7.107 sec/iter\n",
      "Epoch: 26 | Batch: 013 / 030 | Total loss: 3.182 | Reg loss: 0.043 | Tree loss: 3.182 | Accuracy: 0.576172 | 7.106 sec/iter\n",
      "Epoch: 26 | Batch: 014 / 030 | Total loss: 3.160 | Reg loss: 0.044 | Tree loss: 3.160 | Accuracy: 0.574219 | 7.105 sec/iter\n",
      "Epoch: 26 | Batch: 015 / 030 | Total loss: 3.165 | Reg loss: 0.044 | Tree loss: 3.165 | Accuracy: 0.564453 | 7.102 sec/iter\n",
      "Epoch: 26 | Batch: 016 / 030 | Total loss: 3.111 | Reg loss: 0.044 | Tree loss: 3.111 | Accuracy: 0.574219 | 7.099 sec/iter\n",
      "Epoch: 26 | Batch: 017 / 030 | Total loss: 3.085 | Reg loss: 0.044 | Tree loss: 3.085 | Accuracy: 0.562500 | 7.096 sec/iter\n",
      "Epoch: 26 | Batch: 018 / 030 | Total loss: 3.063 | Reg loss: 0.044 | Tree loss: 3.063 | Accuracy: 0.566406 | 7.093 sec/iter\n",
      "Epoch: 26 | Batch: 019 / 030 | Total loss: 3.044 | Reg loss: 0.044 | Tree loss: 3.044 | Accuracy: 0.574219 | 7.09 sec/iter\n",
      "Epoch: 26 | Batch: 020 / 030 | Total loss: 3.013 | Reg loss: 0.044 | Tree loss: 3.013 | Accuracy: 0.603516 | 7.087 sec/iter\n",
      "Epoch: 26 | Batch: 021 / 030 | Total loss: 2.973 | Reg loss: 0.044 | Tree loss: 2.973 | Accuracy: 0.583984 | 7.084 sec/iter\n",
      "Epoch: 26 | Batch: 022 / 030 | Total loss: 2.919 | Reg loss: 0.045 | Tree loss: 2.919 | Accuracy: 0.605469 | 7.082 sec/iter\n",
      "Epoch: 26 | Batch: 023 / 030 | Total loss: 2.938 | Reg loss: 0.045 | Tree loss: 2.938 | Accuracy: 0.585938 | 7.079 sec/iter\n",
      "Epoch: 26 | Batch: 024 / 030 | Total loss: 2.914 | Reg loss: 0.045 | Tree loss: 2.914 | Accuracy: 0.578125 | 7.076 sec/iter\n",
      "Epoch: 26 | Batch: 025 / 030 | Total loss: 2.886 | Reg loss: 0.045 | Tree loss: 2.886 | Accuracy: 0.564453 | 7.073 sec/iter\n",
      "Epoch: 26 | Batch: 026 / 030 | Total loss: 2.861 | Reg loss: 0.045 | Tree loss: 2.861 | Accuracy: 0.585938 | 7.071 sec/iter\n",
      "Epoch: 26 | Batch: 027 / 030 | Total loss: 2.853 | Reg loss: 0.045 | Tree loss: 2.853 | Accuracy: 0.613281 | 7.068 sec/iter\n",
      "Epoch: 26 | Batch: 028 / 030 | Total loss: 2.804 | Reg loss: 0.045 | Tree loss: 2.804 | Accuracy: 0.609375 | 7.065 sec/iter\n",
      "Epoch: 26 | Batch: 029 / 030 | Total loss: 2.658 | Reg loss: 0.045 | Tree loss: 2.658 | Accuracy: 0.679612 | 7.06 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 27 | Batch: 000 / 030 | Total loss: 3.340 | Reg loss: 0.043 | Tree loss: 3.340 | Accuracy: 0.603516 | 7.073 sec/iter\n",
      "Epoch: 27 | Batch: 001 / 030 | Total loss: 3.332 | Reg loss: 0.043 | Tree loss: 3.332 | Accuracy: 0.552734 | 7.074 sec/iter\n",
      "Epoch: 27 | Batch: 002 / 030 | Total loss: 3.315 | Reg loss: 0.043 | Tree loss: 3.315 | Accuracy: 0.613281 | 7.074 sec/iter\n",
      "Epoch: 27 | Batch: 003 / 030 | Total loss: 3.323 | Reg loss: 0.043 | Tree loss: 3.323 | Accuracy: 0.582031 | 7.074 sec/iter\n",
      "Epoch: 27 | Batch: 004 / 030 | Total loss: 3.240 | Reg loss: 0.043 | Tree loss: 3.240 | Accuracy: 0.601562 | 7.074 sec/iter\n",
      "Epoch: 27 | Batch: 005 / 030 | Total loss: 3.228 | Reg loss: 0.043 | Tree loss: 3.228 | Accuracy: 0.587891 | 7.074 sec/iter\n",
      "Epoch: 27 | Batch: 006 / 030 | Total loss: 3.212 | Reg loss: 0.043 | Tree loss: 3.212 | Accuracy: 0.576172 | 7.074 sec/iter\n",
      "Epoch: 27 | Batch: 007 / 030 | Total loss: 3.176 | Reg loss: 0.043 | Tree loss: 3.176 | Accuracy: 0.574219 | 7.074 sec/iter\n",
      "Epoch: 27 | Batch: 008 / 030 | Total loss: 3.125 | Reg loss: 0.043 | Tree loss: 3.125 | Accuracy: 0.587891 | 7.073 sec/iter\n",
      "Epoch: 27 | Batch: 009 / 030 | Total loss: 3.115 | Reg loss: 0.043 | Tree loss: 3.115 | Accuracy: 0.617188 | 7.073 sec/iter\n",
      "Epoch: 27 | Batch: 010 / 030 | Total loss: 3.133 | Reg loss: 0.044 | Tree loss: 3.133 | Accuracy: 0.597656 | 7.072 sec/iter\n",
      "Epoch: 27 | Batch: 011 / 030 | Total loss: 3.078 | Reg loss: 0.044 | Tree loss: 3.078 | Accuracy: 0.562500 | 7.072 sec/iter\n",
      "Epoch: 27 | Batch: 012 / 030 | Total loss: 3.072 | Reg loss: 0.044 | Tree loss: 3.072 | Accuracy: 0.552734 | 7.072 sec/iter\n",
      "Epoch: 27 | Batch: 013 / 030 | Total loss: 3.031 | Reg loss: 0.044 | Tree loss: 3.031 | Accuracy: 0.564453 | 7.071 sec/iter\n",
      "Epoch: 27 | Batch: 014 / 030 | Total loss: 3.005 | Reg loss: 0.044 | Tree loss: 3.005 | Accuracy: 0.542969 | 7.071 sec/iter\n",
      "Epoch: 27 | Batch: 015 / 030 | Total loss: 2.976 | Reg loss: 0.044 | Tree loss: 2.976 | Accuracy: 0.546875 | 7.071 sec/iter\n",
      "Epoch: 27 | Batch: 016 / 030 | Total loss: 2.930 | Reg loss: 0.044 | Tree loss: 2.930 | Accuracy: 0.595703 | 7.071 sec/iter\n",
      "Epoch: 27 | Batch: 017 / 030 | Total loss: 2.928 | Reg loss: 0.044 | Tree loss: 2.928 | Accuracy: 0.587891 | 7.071 sec/iter\n",
      "Epoch: 27 | Batch: 018 / 030 | Total loss: 2.886 | Reg loss: 0.044 | Tree loss: 2.886 | Accuracy: 0.583984 | 7.071 sec/iter\n",
      "Epoch: 27 | Batch: 019 / 030 | Total loss: 2.862 | Reg loss: 0.045 | Tree loss: 2.862 | Accuracy: 0.595703 | 7.071 sec/iter\n",
      "Epoch: 27 | Batch: 020 / 030 | Total loss: 2.829 | Reg loss: 0.045 | Tree loss: 2.829 | Accuracy: 0.583984 | 7.071 sec/iter\n",
      "Epoch: 27 | Batch: 021 / 030 | Total loss: 2.824 | Reg loss: 0.045 | Tree loss: 2.824 | Accuracy: 0.556641 | 7.068 sec/iter\n",
      "Epoch: 27 | Batch: 022 / 030 | Total loss: 2.776 | Reg loss: 0.045 | Tree loss: 2.776 | Accuracy: 0.593750 | 7.065 sec/iter\n",
      "Epoch: 27 | Batch: 023 / 030 | Total loss: 2.756 | Reg loss: 0.045 | Tree loss: 2.756 | Accuracy: 0.591797 | 7.062 sec/iter\n",
      "Epoch: 27 | Batch: 024 / 030 | Total loss: 2.766 | Reg loss: 0.045 | Tree loss: 2.766 | Accuracy: 0.605469 | 7.059 sec/iter\n",
      "Epoch: 27 | Batch: 025 / 030 | Total loss: 2.723 | Reg loss: 0.045 | Tree loss: 2.723 | Accuracy: 0.589844 | 7.056 sec/iter\n",
      "Epoch: 27 | Batch: 026 / 030 | Total loss: 2.702 | Reg loss: 0.045 | Tree loss: 2.702 | Accuracy: 0.578125 | 7.054 sec/iter\n",
      "Epoch: 27 | Batch: 027 / 030 | Total loss: 2.678 | Reg loss: 0.045 | Tree loss: 2.678 | Accuracy: 0.583984 | 7.052 sec/iter\n",
      "Epoch: 27 | Batch: 028 / 030 | Total loss: 2.663 | Reg loss: 0.046 | Tree loss: 2.663 | Accuracy: 0.574219 | 7.05 sec/iter\n",
      "Epoch: 27 | Batch: 029 / 030 | Total loss: 2.673 | Reg loss: 0.046 | Tree loss: 2.673 | Accuracy: 0.660194 | 7.045 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 28 | Batch: 000 / 030 | Total loss: 3.184 | Reg loss: 0.044 | Tree loss: 3.184 | Accuracy: 0.585938 | 7.057 sec/iter\n",
      "Epoch: 28 | Batch: 001 / 030 | Total loss: 3.222 | Reg loss: 0.044 | Tree loss: 3.222 | Accuracy: 0.568359 | 7.057 sec/iter\n",
      "Epoch: 28 | Batch: 002 / 030 | Total loss: 3.160 | Reg loss: 0.044 | Tree loss: 3.160 | Accuracy: 0.570312 | 7.057 sec/iter\n",
      "Epoch: 28 | Batch: 003 / 030 | Total loss: 3.112 | Reg loss: 0.044 | Tree loss: 3.112 | Accuracy: 0.580078 | 7.058 sec/iter\n",
      "Epoch: 28 | Batch: 004 / 030 | Total loss: 3.082 | Reg loss: 0.044 | Tree loss: 3.082 | Accuracy: 0.570312 | 7.058 sec/iter\n",
      "Epoch: 28 | Batch: 005 / 030 | Total loss: 3.079 | Reg loss: 0.044 | Tree loss: 3.079 | Accuracy: 0.597656 | 7.058 sec/iter\n",
      "Epoch: 28 | Batch: 006 / 030 | Total loss: 3.043 | Reg loss: 0.044 | Tree loss: 3.043 | Accuracy: 0.611328 | 7.058 sec/iter\n",
      "Epoch: 28 | Batch: 007 / 030 | Total loss: 2.992 | Reg loss: 0.044 | Tree loss: 2.992 | Accuracy: 0.574219 | 7.058 sec/iter\n",
      "Epoch: 28 | Batch: 008 / 030 | Total loss: 2.956 | Reg loss: 0.044 | Tree loss: 2.956 | Accuracy: 0.611328 | 7.058 sec/iter\n",
      "Epoch: 28 | Batch: 009 / 030 | Total loss: 2.950 | Reg loss: 0.044 | Tree loss: 2.950 | Accuracy: 0.582031 | 7.058 sec/iter\n",
      "Epoch: 28 | Batch: 010 / 030 | Total loss: 2.925 | Reg loss: 0.044 | Tree loss: 2.925 | Accuracy: 0.607422 | 7.057 sec/iter\n",
      "Epoch: 28 | Batch: 011 / 030 | Total loss: 2.894 | Reg loss: 0.044 | Tree loss: 2.894 | Accuracy: 0.574219 | 7.057 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 28 | Batch: 012 / 030 | Total loss: 2.888 | Reg loss: 0.044 | Tree loss: 2.888 | Accuracy: 0.617188 | 7.056 sec/iter\n",
      "Epoch: 28 | Batch: 013 / 030 | Total loss: 2.885 | Reg loss: 0.044 | Tree loss: 2.885 | Accuracy: 0.585938 | 7.056 sec/iter\n",
      "Epoch: 28 | Batch: 014 / 030 | Total loss: 2.882 | Reg loss: 0.044 | Tree loss: 2.882 | Accuracy: 0.542969 | 7.056 sec/iter\n",
      "Epoch: 28 | Batch: 015 / 030 | Total loss: 2.826 | Reg loss: 0.044 | Tree loss: 2.826 | Accuracy: 0.583984 | 7.056 sec/iter\n",
      "Epoch: 28 | Batch: 016 / 030 | Total loss: 2.812 | Reg loss: 0.045 | Tree loss: 2.812 | Accuracy: 0.560547 | 7.056 sec/iter\n",
      "Epoch: 28 | Batch: 017 / 030 | Total loss: 2.748 | Reg loss: 0.045 | Tree loss: 2.748 | Accuracy: 0.568359 | 7.055 sec/iter\n",
      "Epoch: 28 | Batch: 018 / 030 | Total loss: 2.731 | Reg loss: 0.045 | Tree loss: 2.731 | Accuracy: 0.582031 | 7.055 sec/iter\n",
      "Epoch: 28 | Batch: 019 / 030 | Total loss: 2.685 | Reg loss: 0.045 | Tree loss: 2.685 | Accuracy: 0.625000 | 7.055 sec/iter\n",
      "Epoch: 28 | Batch: 020 / 030 | Total loss: 2.680 | Reg loss: 0.045 | Tree loss: 2.680 | Accuracy: 0.609375 | 7.055 sec/iter\n",
      "Epoch: 28 | Batch: 021 / 030 | Total loss: 2.679 | Reg loss: 0.045 | Tree loss: 2.679 | Accuracy: 0.570312 | 7.055 sec/iter\n",
      "Epoch: 28 | Batch: 022 / 030 | Total loss: 2.652 | Reg loss: 0.045 | Tree loss: 2.652 | Accuracy: 0.593750 | 7.055 sec/iter\n",
      "Epoch: 28 | Batch: 023 / 030 | Total loss: 2.642 | Reg loss: 0.045 | Tree loss: 2.642 | Accuracy: 0.541016 | 7.056 sec/iter\n",
      "Epoch: 28 | Batch: 024 / 030 | Total loss: 2.645 | Reg loss: 0.045 | Tree loss: 2.645 | Accuracy: 0.562500 | 7.054 sec/iter\n",
      "Epoch: 28 | Batch: 025 / 030 | Total loss: 2.620 | Reg loss: 0.045 | Tree loss: 2.620 | Accuracy: 0.558594 | 7.05 sec/iter\n",
      "Epoch: 28 | Batch: 026 / 030 | Total loss: 2.550 | Reg loss: 0.046 | Tree loss: 2.550 | Accuracy: 0.603516 | 7.049 sec/iter\n",
      "Epoch: 28 | Batch: 027 / 030 | Total loss: 2.543 | Reg loss: 0.046 | Tree loss: 2.543 | Accuracy: 0.589844 | 7.049 sec/iter\n",
      "Epoch: 28 | Batch: 028 / 030 | Total loss: 2.551 | Reg loss: 0.046 | Tree loss: 2.551 | Accuracy: 0.562500 | 7.05 sec/iter\n",
      "Epoch: 28 | Batch: 029 / 030 | Total loss: 2.417 | Reg loss: 0.046 | Tree loss: 2.417 | Accuracy: 0.631068 | 7.045 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 29 | Batch: 000 / 030 | Total loss: 2.995 | Reg loss: 0.044 | Tree loss: 2.995 | Accuracy: 0.580078 | 7.082 sec/iter\n",
      "Epoch: 29 | Batch: 001 / 030 | Total loss: 2.988 | Reg loss: 0.044 | Tree loss: 2.988 | Accuracy: 0.605469 | 7.083 sec/iter\n",
      "Epoch: 29 | Batch: 002 / 030 | Total loss: 2.988 | Reg loss: 0.044 | Tree loss: 2.988 | Accuracy: 0.572266 | 7.083 sec/iter\n",
      "Epoch: 29 | Batch: 003 / 030 | Total loss: 2.948 | Reg loss: 0.044 | Tree loss: 2.948 | Accuracy: 0.591797 | 7.082 sec/iter\n",
      "Epoch: 29 | Batch: 004 / 030 | Total loss: 2.960 | Reg loss: 0.044 | Tree loss: 2.960 | Accuracy: 0.583984 | 7.082 sec/iter\n",
      "Epoch: 29 | Batch: 005 / 030 | Total loss: 2.897 | Reg loss: 0.044 | Tree loss: 2.897 | Accuracy: 0.582031 | 7.082 sec/iter\n",
      "Epoch: 29 | Batch: 006 / 030 | Total loss: 2.966 | Reg loss: 0.044 | Tree loss: 2.966 | Accuracy: 0.539062 | 7.081 sec/iter\n",
      "Epoch: 29 | Batch: 007 / 030 | Total loss: 2.893 | Reg loss: 0.044 | Tree loss: 2.893 | Accuracy: 0.611328 | 7.08 sec/iter\n",
      "Epoch: 29 | Batch: 008 / 030 | Total loss: 2.844 | Reg loss: 0.044 | Tree loss: 2.844 | Accuracy: 0.576172 | 7.08 sec/iter\n",
      "Epoch: 29 | Batch: 009 / 030 | Total loss: 2.862 | Reg loss: 0.044 | Tree loss: 2.862 | Accuracy: 0.554688 | 7.079 sec/iter\n",
      "Epoch: 29 | Batch: 010 / 030 | Total loss: 2.816 | Reg loss: 0.044 | Tree loss: 2.816 | Accuracy: 0.587891 | 7.079 sec/iter\n",
      "Epoch: 29 | Batch: 011 / 030 | Total loss: 2.783 | Reg loss: 0.044 | Tree loss: 2.783 | Accuracy: 0.564453 | 7.079 sec/iter\n",
      "Epoch: 29 | Batch: 012 / 030 | Total loss: 2.789 | Reg loss: 0.044 | Tree loss: 2.789 | Accuracy: 0.574219 | 7.079 sec/iter\n",
      "Epoch: 29 | Batch: 013 / 030 | Total loss: 2.729 | Reg loss: 0.044 | Tree loss: 2.729 | Accuracy: 0.585938 | 7.079 sec/iter\n",
      "Epoch: 29 | Batch: 014 / 030 | Total loss: 2.707 | Reg loss: 0.045 | Tree loss: 2.707 | Accuracy: 0.572266 | 7.079 sec/iter\n",
      "Epoch: 29 | Batch: 015 / 030 | Total loss: 2.687 | Reg loss: 0.045 | Tree loss: 2.687 | Accuracy: 0.578125 | 7.079 sec/iter\n",
      "Epoch: 29 | Batch: 016 / 030 | Total loss: 2.667 | Reg loss: 0.045 | Tree loss: 2.667 | Accuracy: 0.556641 | 7.079 sec/iter\n",
      "Epoch: 29 | Batch: 017 / 030 | Total loss: 2.632 | Reg loss: 0.045 | Tree loss: 2.632 | Accuracy: 0.578125 | 7.079 sec/iter\n",
      "Epoch: 29 | Batch: 018 / 030 | Total loss: 2.589 | Reg loss: 0.045 | Tree loss: 2.589 | Accuracy: 0.609375 | 7.079 sec/iter\n",
      "Epoch: 29 | Batch: 019 / 030 | Total loss: 2.609 | Reg loss: 0.045 | Tree loss: 2.609 | Accuracy: 0.574219 | 7.075 sec/iter\n",
      "Epoch: 29 | Batch: 020 / 030 | Total loss: 2.526 | Reg loss: 0.045 | Tree loss: 2.526 | Accuracy: 0.593750 | 7.075 sec/iter\n",
      "Epoch: 29 | Batch: 021 / 030 | Total loss: 2.570 | Reg loss: 0.045 | Tree loss: 2.570 | Accuracy: 0.548828 | 7.076 sec/iter\n",
      "Epoch: 29 | Batch: 022 / 030 | Total loss: 2.490 | Reg loss: 0.045 | Tree loss: 2.490 | Accuracy: 0.597656 | 7.076 sec/iter\n",
      "Epoch: 29 | Batch: 023 / 030 | Total loss: 2.488 | Reg loss: 0.045 | Tree loss: 2.488 | Accuracy: 0.630859 | 7.077 sec/iter\n",
      "Epoch: 29 | Batch: 024 / 030 | Total loss: 2.499 | Reg loss: 0.046 | Tree loss: 2.499 | Accuracy: 0.576172 | 7.077 sec/iter\n",
      "Epoch: 29 | Batch: 025 / 030 | Total loss: 2.445 | Reg loss: 0.046 | Tree loss: 2.445 | Accuracy: 0.615234 | 7.077 sec/iter\n",
      "Epoch: 29 | Batch: 026 / 030 | Total loss: 2.420 | Reg loss: 0.046 | Tree loss: 2.420 | Accuracy: 0.578125 | 7.077 sec/iter\n",
      "Epoch: 29 | Batch: 027 / 030 | Total loss: 2.389 | Reg loss: 0.046 | Tree loss: 2.389 | Accuracy: 0.591797 | 7.078 sec/iter\n",
      "Epoch: 29 | Batch: 028 / 030 | Total loss: 2.370 | Reg loss: 0.046 | Tree loss: 2.370 | Accuracy: 0.593750 | 7.078 sec/iter\n",
      "Epoch: 29 | Batch: 029 / 030 | Total loss: 2.434 | Reg loss: 0.046 | Tree loss: 2.434 | Accuracy: 0.563107 | 7.074 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 30 | Batch: 000 / 030 | Total loss: 2.876 | Reg loss: 0.044 | Tree loss: 2.876 | Accuracy: 0.591797 | 7.093 sec/iter\n",
      "Epoch: 30 | Batch: 001 / 030 | Total loss: 2.890 | Reg loss: 0.044 | Tree loss: 2.890 | Accuracy: 0.603516 | 7.093 sec/iter\n",
      "Epoch: 30 | Batch: 002 / 030 | Total loss: 2.857 | Reg loss: 0.044 | Tree loss: 2.857 | Accuracy: 0.607422 | 7.094 sec/iter\n",
      "Epoch: 30 | Batch: 003 / 030 | Total loss: 2.850 | Reg loss: 0.044 | Tree loss: 2.850 | Accuracy: 0.572266 | 7.093 sec/iter\n",
      "Epoch: 30 | Batch: 004 / 030 | Total loss: 2.774 | Reg loss: 0.044 | Tree loss: 2.774 | Accuracy: 0.613281 | 7.093 sec/iter\n",
      "Epoch: 30 | Batch: 005 / 030 | Total loss: 2.822 | Reg loss: 0.044 | Tree loss: 2.822 | Accuracy: 0.535156 | 7.092 sec/iter\n",
      "Epoch: 30 | Batch: 006 / 030 | Total loss: 2.793 | Reg loss: 0.044 | Tree loss: 2.793 | Accuracy: 0.601562 | 7.091 sec/iter\n",
      "Epoch: 30 | Batch: 007 / 030 | Total loss: 2.723 | Reg loss: 0.044 | Tree loss: 2.723 | Accuracy: 0.582031 | 7.091 sec/iter\n",
      "Epoch: 30 | Batch: 008 / 030 | Total loss: 2.722 | Reg loss: 0.044 | Tree loss: 2.722 | Accuracy: 0.574219 | 7.09 sec/iter\n",
      "Epoch: 30 | Batch: 009 / 030 | Total loss: 2.707 | Reg loss: 0.044 | Tree loss: 2.707 | Accuracy: 0.564453 | 7.09 sec/iter\n",
      "Epoch: 30 | Batch: 010 / 030 | Total loss: 2.673 | Reg loss: 0.044 | Tree loss: 2.673 | Accuracy: 0.572266 | 7.09 sec/iter\n",
      "Epoch: 30 | Batch: 011 / 030 | Total loss: 2.621 | Reg loss: 0.045 | Tree loss: 2.621 | Accuracy: 0.601562 | 7.089 sec/iter\n",
      "Epoch: 30 | Batch: 012 / 030 | Total loss: 2.635 | Reg loss: 0.045 | Tree loss: 2.635 | Accuracy: 0.552734 | 7.09 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30 | Batch: 013 / 030 | Total loss: 2.593 | Reg loss: 0.045 | Tree loss: 2.593 | Accuracy: 0.611328 | 7.09 sec/iter\n",
      "Epoch: 30 | Batch: 014 / 030 | Total loss: 2.590 | Reg loss: 0.045 | Tree loss: 2.590 | Accuracy: 0.515625 | 7.089 sec/iter\n",
      "Epoch: 30 | Batch: 015 / 030 | Total loss: 2.514 | Reg loss: 0.045 | Tree loss: 2.514 | Accuracy: 0.619141 | 7.088 sec/iter\n",
      "Epoch: 30 | Batch: 016 / 030 | Total loss: 2.503 | Reg loss: 0.045 | Tree loss: 2.503 | Accuracy: 0.576172 | 7.085 sec/iter\n",
      "Epoch: 30 | Batch: 017 / 030 | Total loss: 2.488 | Reg loss: 0.045 | Tree loss: 2.488 | Accuracy: 0.597656 | 7.085 sec/iter\n",
      "Epoch: 30 | Batch: 018 / 030 | Total loss: 2.451 | Reg loss: 0.045 | Tree loss: 2.451 | Accuracy: 0.607422 | 7.085 sec/iter\n",
      "Epoch: 30 | Batch: 019 / 030 | Total loss: 2.489 | Reg loss: 0.045 | Tree loss: 2.489 | Accuracy: 0.544922 | 7.085 sec/iter\n",
      "Epoch: 30 | Batch: 020 / 030 | Total loss: 2.427 | Reg loss: 0.045 | Tree loss: 2.427 | Accuracy: 0.578125 | 7.084 sec/iter\n",
      "Epoch: 30 | Batch: 021 / 030 | Total loss: 2.418 | Reg loss: 0.045 | Tree loss: 2.418 | Accuracy: 0.548828 | 7.084 sec/iter\n",
      "Epoch: 30 | Batch: 022 / 030 | Total loss: 2.388 | Reg loss: 0.045 | Tree loss: 2.388 | Accuracy: 0.593750 | 7.083 sec/iter\n",
      "Epoch: 30 | Batch: 023 / 030 | Total loss: 2.390 | Reg loss: 0.046 | Tree loss: 2.390 | Accuracy: 0.591797 | 7.082 sec/iter\n",
      "Epoch: 30 | Batch: 024 / 030 | Total loss: 2.374 | Reg loss: 0.046 | Tree loss: 2.374 | Accuracy: 0.591797 | 7.082 sec/iter\n",
      "Epoch: 30 | Batch: 025 / 030 | Total loss: 2.309 | Reg loss: 0.046 | Tree loss: 2.309 | Accuracy: 0.623047 | 7.082 sec/iter\n",
      "Epoch: 30 | Batch: 026 / 030 | Total loss: 2.324 | Reg loss: 0.046 | Tree loss: 2.324 | Accuracy: 0.582031 | 7.082 sec/iter\n",
      "Epoch: 30 | Batch: 027 / 030 | Total loss: 2.323 | Reg loss: 0.046 | Tree loss: 2.323 | Accuracy: 0.572266 | 7.082 sec/iter\n",
      "Epoch: 30 | Batch: 028 / 030 | Total loss: 2.290 | Reg loss: 0.046 | Tree loss: 2.290 | Accuracy: 0.572266 | 7.082 sec/iter\n",
      "Epoch: 30 | Batch: 029 / 030 | Total loss: 2.223 | Reg loss: 0.046 | Tree loss: 2.223 | Accuracy: 0.592233 | 7.078 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 31 | Batch: 000 / 030 | Total loss: 2.726 | Reg loss: 0.044 | Tree loss: 2.726 | Accuracy: 0.587891 | 7.118 sec/iter\n",
      "Epoch: 31 | Batch: 001 / 030 | Total loss: 2.739 | Reg loss: 0.044 | Tree loss: 2.739 | Accuracy: 0.572266 | 7.118 sec/iter\n",
      "Epoch: 31 | Batch: 002 / 030 | Total loss: 2.701 | Reg loss: 0.044 | Tree loss: 2.701 | Accuracy: 0.580078 | 7.117 sec/iter\n",
      "Epoch: 31 | Batch: 003 / 030 | Total loss: 2.720 | Reg loss: 0.044 | Tree loss: 2.720 | Accuracy: 0.562500 | 7.117 sec/iter\n",
      "Epoch: 31 | Batch: 004 / 030 | Total loss: 2.671 | Reg loss: 0.044 | Tree loss: 2.671 | Accuracy: 0.593750 | 7.117 sec/iter\n",
      "Epoch: 31 | Batch: 005 / 030 | Total loss: 2.678 | Reg loss: 0.044 | Tree loss: 2.678 | Accuracy: 0.564453 | 7.117 sec/iter\n",
      "Epoch: 31 | Batch: 006 / 030 | Total loss: 2.638 | Reg loss: 0.044 | Tree loss: 2.638 | Accuracy: 0.601562 | 7.117 sec/iter\n",
      "Epoch: 31 | Batch: 007 / 030 | Total loss: 2.620 | Reg loss: 0.045 | Tree loss: 2.620 | Accuracy: 0.576172 | 7.115 sec/iter\n",
      "Epoch: 31 | Batch: 008 / 030 | Total loss: 2.601 | Reg loss: 0.045 | Tree loss: 2.601 | Accuracy: 0.613281 | 7.113 sec/iter\n",
      "Epoch: 31 | Batch: 009 / 030 | Total loss: 2.563 | Reg loss: 0.045 | Tree loss: 2.563 | Accuracy: 0.603516 | 7.109 sec/iter\n",
      "Epoch: 31 | Batch: 010 / 030 | Total loss: 2.558 | Reg loss: 0.045 | Tree loss: 2.558 | Accuracy: 0.560547 | 7.108 sec/iter\n",
      "Epoch: 31 | Batch: 011 / 030 | Total loss: 2.513 | Reg loss: 0.045 | Tree loss: 2.513 | Accuracy: 0.587891 | 7.108 sec/iter\n",
      "Epoch: 31 | Batch: 012 / 030 | Total loss: 2.471 | Reg loss: 0.045 | Tree loss: 2.471 | Accuracy: 0.611328 | 7.109 sec/iter\n",
      "Epoch: 31 | Batch: 013 / 030 | Total loss: 2.476 | Reg loss: 0.045 | Tree loss: 2.476 | Accuracy: 0.609375 | 7.109 sec/iter\n",
      "Epoch: 31 | Batch: 014 / 030 | Total loss: 2.460 | Reg loss: 0.045 | Tree loss: 2.460 | Accuracy: 0.554688 | 7.109 sec/iter\n",
      "Epoch: 31 | Batch: 015 / 030 | Total loss: 2.428 | Reg loss: 0.045 | Tree loss: 2.428 | Accuracy: 0.539062 | 7.109 sec/iter\n",
      "Epoch: 31 | Batch: 016 / 030 | Total loss: 2.416 | Reg loss: 0.045 | Tree loss: 2.416 | Accuracy: 0.564453 | 7.109 sec/iter\n",
      "Epoch: 31 | Batch: 017 / 030 | Total loss: 2.415 | Reg loss: 0.045 | Tree loss: 2.415 | Accuracy: 0.550781 | 7.11 sec/iter\n",
      "Epoch: 31 | Batch: 018 / 030 | Total loss: 2.356 | Reg loss: 0.045 | Tree loss: 2.356 | Accuracy: 0.585938 | 7.11 sec/iter\n",
      "Epoch: 31 | Batch: 019 / 030 | Total loss: 2.348 | Reg loss: 0.045 | Tree loss: 2.348 | Accuracy: 0.597656 | 7.11 sec/iter\n",
      "Epoch: 31 | Batch: 020 / 030 | Total loss: 2.355 | Reg loss: 0.045 | Tree loss: 2.355 | Accuracy: 0.560547 | 7.11 sec/iter\n",
      "Epoch: 31 | Batch: 021 / 030 | Total loss: 2.302 | Reg loss: 0.046 | Tree loss: 2.302 | Accuracy: 0.593750 | 7.11 sec/iter\n",
      "Epoch: 31 | Batch: 022 / 030 | Total loss: 2.248 | Reg loss: 0.046 | Tree loss: 2.248 | Accuracy: 0.609375 | 7.11 sec/iter\n",
      "Epoch: 31 | Batch: 023 / 030 | Total loss: 2.292 | Reg loss: 0.046 | Tree loss: 2.292 | Accuracy: 0.572266 | 7.11 sec/iter\n",
      "Epoch: 31 | Batch: 024 / 030 | Total loss: 2.239 | Reg loss: 0.046 | Tree loss: 2.239 | Accuracy: 0.572266 | 7.11 sec/iter\n",
      "Epoch: 31 | Batch: 025 / 030 | Total loss: 2.227 | Reg loss: 0.046 | Tree loss: 2.227 | Accuracy: 0.597656 | 7.11 sec/iter\n",
      "Epoch: 31 | Batch: 026 / 030 | Total loss: 2.212 | Reg loss: 0.046 | Tree loss: 2.212 | Accuracy: 0.603516 | 7.11 sec/iter\n",
      "Epoch: 31 | Batch: 027 / 030 | Total loss: 2.169 | Reg loss: 0.046 | Tree loss: 2.169 | Accuracy: 0.585938 | 7.11 sec/iter\n",
      "Epoch: 31 | Batch: 028 / 030 | Total loss: 2.168 | Reg loss: 0.046 | Tree loss: 2.168 | Accuracy: 0.583984 | 7.11 sec/iter\n",
      "Epoch: 31 | Batch: 029 / 030 | Total loss: 2.175 | Reg loss: 0.046 | Tree loss: 2.175 | Accuracy: 0.601942 | 7.106 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 32 | Batch: 000 / 030 | Total loss: 2.633 | Reg loss: 0.045 | Tree loss: 2.633 | Accuracy: 0.582031 | 7.139 sec/iter\n",
      "Epoch: 32 | Batch: 001 / 030 | Total loss: 2.613 | Reg loss: 0.045 | Tree loss: 2.613 | Accuracy: 0.585938 | 7.137 sec/iter\n",
      "Epoch: 32 | Batch: 002 / 030 | Total loss: 2.642 | Reg loss: 0.045 | Tree loss: 2.642 | Accuracy: 0.548828 | 7.135 sec/iter\n",
      "Epoch: 32 | Batch: 003 / 030 | Total loss: 2.561 | Reg loss: 0.045 | Tree loss: 2.561 | Accuracy: 0.587891 | 7.133 sec/iter\n",
      "Epoch: 32 | Batch: 004 / 030 | Total loss: 2.573 | Reg loss: 0.045 | Tree loss: 2.573 | Accuracy: 0.574219 | 7.132 sec/iter\n",
      "Epoch: 32 | Batch: 005 / 030 | Total loss: 2.489 | Reg loss: 0.045 | Tree loss: 2.489 | Accuracy: 0.630859 | 7.13 sec/iter\n",
      "Epoch: 32 | Batch: 006 / 030 | Total loss: 2.509 | Reg loss: 0.045 | Tree loss: 2.509 | Accuracy: 0.587891 | 7.13 sec/iter\n",
      "Epoch: 32 | Batch: 007 / 030 | Total loss: 2.436 | Reg loss: 0.045 | Tree loss: 2.436 | Accuracy: 0.583984 | 7.13 sec/iter\n",
      "Epoch: 32 | Batch: 008 / 030 | Total loss: 2.487 | Reg loss: 0.045 | Tree loss: 2.487 | Accuracy: 0.585938 | 7.13 sec/iter\n",
      "Epoch: 32 | Batch: 009 / 030 | Total loss: 2.457 | Reg loss: 0.045 | Tree loss: 2.457 | Accuracy: 0.591797 | 7.13 sec/iter\n",
      "Epoch: 32 | Batch: 010 / 030 | Total loss: 2.405 | Reg loss: 0.045 | Tree loss: 2.405 | Accuracy: 0.603516 | 7.13 sec/iter\n",
      "Epoch: 32 | Batch: 011 / 030 | Total loss: 2.397 | Reg loss: 0.045 | Tree loss: 2.397 | Accuracy: 0.591797 | 7.13 sec/iter\n",
      "Epoch: 32 | Batch: 012 / 030 | Total loss: 2.370 | Reg loss: 0.045 | Tree loss: 2.370 | Accuracy: 0.628906 | 7.131 sec/iter\n",
      "Epoch: 32 | Batch: 013 / 030 | Total loss: 2.363 | Reg loss: 0.045 | Tree loss: 2.363 | Accuracy: 0.572266 | 7.131 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 32 | Batch: 014 / 030 | Total loss: 2.334 | Reg loss: 0.045 | Tree loss: 2.334 | Accuracy: 0.593750 | 7.131 sec/iter\n",
      "Epoch: 32 | Batch: 015 / 030 | Total loss: 2.317 | Reg loss: 0.045 | Tree loss: 2.317 | Accuracy: 0.593750 | 7.131 sec/iter\n",
      "Epoch: 32 | Batch: 016 / 030 | Total loss: 2.322 | Reg loss: 0.045 | Tree loss: 2.322 | Accuracy: 0.574219 | 7.131 sec/iter\n",
      "Epoch: 32 | Batch: 017 / 030 | Total loss: 2.276 | Reg loss: 0.045 | Tree loss: 2.276 | Accuracy: 0.558594 | 7.131 sec/iter\n",
      "Epoch: 32 | Batch: 018 / 030 | Total loss: 2.226 | Reg loss: 0.045 | Tree loss: 2.226 | Accuracy: 0.621094 | 7.131 sec/iter\n",
      "Epoch: 32 | Batch: 019 / 030 | Total loss: 2.227 | Reg loss: 0.045 | Tree loss: 2.227 | Accuracy: 0.621094 | 7.131 sec/iter\n",
      "Epoch: 32 | Batch: 020 / 030 | Total loss: 2.198 | Reg loss: 0.046 | Tree loss: 2.198 | Accuracy: 0.574219 | 7.131 sec/iter\n",
      "Epoch: 32 | Batch: 021 / 030 | Total loss: 2.226 | Reg loss: 0.046 | Tree loss: 2.226 | Accuracy: 0.566406 | 7.13 sec/iter\n",
      "Epoch: 32 | Batch: 022 / 030 | Total loss: 2.181 | Reg loss: 0.046 | Tree loss: 2.181 | Accuracy: 0.587891 | 7.13 sec/iter\n",
      "Epoch: 32 | Batch: 023 / 030 | Total loss: 2.179 | Reg loss: 0.046 | Tree loss: 2.179 | Accuracy: 0.552734 | 7.13 sec/iter\n",
      "Epoch: 32 | Batch: 024 / 030 | Total loss: 2.182 | Reg loss: 0.046 | Tree loss: 2.182 | Accuracy: 0.554688 | 7.13 sec/iter\n",
      "Epoch: 32 | Batch: 025 / 030 | Total loss: 2.127 | Reg loss: 0.046 | Tree loss: 2.127 | Accuracy: 0.562500 | 7.13 sec/iter\n",
      "Epoch: 32 | Batch: 026 / 030 | Total loss: 2.144 | Reg loss: 0.046 | Tree loss: 2.144 | Accuracy: 0.554688 | 7.13 sec/iter\n",
      "Epoch: 32 | Batch: 027 / 030 | Total loss: 2.113 | Reg loss: 0.046 | Tree loss: 2.113 | Accuracy: 0.556641 | 7.129 sec/iter\n",
      "Epoch: 32 | Batch: 028 / 030 | Total loss: 2.080 | Reg loss: 0.046 | Tree loss: 2.080 | Accuracy: 0.558594 | 7.129 sec/iter\n",
      "Epoch: 32 | Batch: 029 / 030 | Total loss: 2.047 | Reg loss: 0.046 | Tree loss: 2.047 | Accuracy: 0.650485 | 7.126 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 33 | Batch: 000 / 030 | Total loss: 2.552 | Reg loss: 0.045 | Tree loss: 2.552 | Accuracy: 0.548828 | 7.129 sec/iter\n",
      "Epoch: 33 | Batch: 001 / 030 | Total loss: 2.505 | Reg loss: 0.045 | Tree loss: 2.505 | Accuracy: 0.593750 | 7.128 sec/iter\n",
      "Epoch: 33 | Batch: 002 / 030 | Total loss: 2.506 | Reg loss: 0.045 | Tree loss: 2.506 | Accuracy: 0.578125 | 7.128 sec/iter\n",
      "Epoch: 33 | Batch: 003 / 030 | Total loss: 2.493 | Reg loss: 0.045 | Tree loss: 2.493 | Accuracy: 0.558594 | 7.127 sec/iter\n",
      "Epoch: 33 | Batch: 004 / 030 | Total loss: 2.462 | Reg loss: 0.045 | Tree loss: 2.462 | Accuracy: 0.564453 | 7.125 sec/iter\n",
      "Epoch: 33 | Batch: 005 / 030 | Total loss: 2.411 | Reg loss: 0.045 | Tree loss: 2.411 | Accuracy: 0.609375 | 7.123 sec/iter\n",
      "Epoch: 33 | Batch: 006 / 030 | Total loss: 2.409 | Reg loss: 0.045 | Tree loss: 2.409 | Accuracy: 0.570312 | 7.121 sec/iter\n",
      "Epoch: 33 | Batch: 007 / 030 | Total loss: 2.404 | Reg loss: 0.045 | Tree loss: 2.404 | Accuracy: 0.541016 | 7.118 sec/iter\n",
      "Epoch: 33 | Batch: 008 / 030 | Total loss: 2.345 | Reg loss: 0.045 | Tree loss: 2.345 | Accuracy: 0.607422 | 7.116 sec/iter\n",
      "Epoch: 33 | Batch: 009 / 030 | Total loss: 2.325 | Reg loss: 0.045 | Tree loss: 2.325 | Accuracy: 0.582031 | 7.113 sec/iter\n",
      "Epoch: 33 | Batch: 010 / 030 | Total loss: 2.326 | Reg loss: 0.045 | Tree loss: 2.326 | Accuracy: 0.554688 | 7.111 sec/iter\n",
      "Epoch: 33 | Batch: 011 / 030 | Total loss: 2.275 | Reg loss: 0.045 | Tree loss: 2.275 | Accuracy: 0.585938 | 7.109 sec/iter\n",
      "Epoch: 33 | Batch: 012 / 030 | Total loss: 2.257 | Reg loss: 0.045 | Tree loss: 2.257 | Accuracy: 0.611328 | 7.106 sec/iter\n",
      "Epoch: 33 | Batch: 013 / 030 | Total loss: 2.267 | Reg loss: 0.045 | Tree loss: 2.267 | Accuracy: 0.572266 | 7.104 sec/iter\n",
      "Epoch: 33 | Batch: 014 / 030 | Total loss: 2.239 | Reg loss: 0.045 | Tree loss: 2.239 | Accuracy: 0.574219 | 7.101 sec/iter\n",
      "Epoch: 33 | Batch: 015 / 030 | Total loss: 2.198 | Reg loss: 0.045 | Tree loss: 2.198 | Accuracy: 0.574219 | 7.101 sec/iter\n",
      "Epoch: 33 | Batch: 016 / 030 | Total loss: 2.206 | Reg loss: 0.045 | Tree loss: 2.206 | Accuracy: 0.580078 | 7.101 sec/iter\n",
      "Epoch: 33 | Batch: 017 / 030 | Total loss: 2.151 | Reg loss: 0.045 | Tree loss: 2.151 | Accuracy: 0.589844 | 7.101 sec/iter\n",
      "Epoch: 33 | Batch: 018 / 030 | Total loss: 2.142 | Reg loss: 0.045 | Tree loss: 2.142 | Accuracy: 0.582031 | 7.101 sec/iter\n",
      "Epoch: 33 | Batch: 019 / 030 | Total loss: 2.142 | Reg loss: 0.045 | Tree loss: 2.142 | Accuracy: 0.589844 | 7.102 sec/iter\n",
      "Epoch: 33 | Batch: 020 / 030 | Total loss: 2.121 | Reg loss: 0.046 | Tree loss: 2.121 | Accuracy: 0.560547 | 7.102 sec/iter\n",
      "Epoch: 33 | Batch: 021 / 030 | Total loss: 2.094 | Reg loss: 0.046 | Tree loss: 2.094 | Accuracy: 0.609375 | 7.102 sec/iter\n",
      "Epoch: 33 | Batch: 022 / 030 | Total loss: 2.109 | Reg loss: 0.046 | Tree loss: 2.109 | Accuracy: 0.599609 | 7.102 sec/iter\n",
      "Epoch: 33 | Batch: 023 / 030 | Total loss: 2.096 | Reg loss: 0.046 | Tree loss: 2.096 | Accuracy: 0.544922 | 7.102 sec/iter\n",
      "Epoch: 33 | Batch: 024 / 030 | Total loss: 2.049 | Reg loss: 0.046 | Tree loss: 2.049 | Accuracy: 0.587891 | 7.102 sec/iter\n",
      "Epoch: 33 | Batch: 025 / 030 | Total loss: 2.004 | Reg loss: 0.046 | Tree loss: 2.004 | Accuracy: 0.599609 | 7.102 sec/iter\n",
      "Epoch: 33 | Batch: 026 / 030 | Total loss: 1.996 | Reg loss: 0.046 | Tree loss: 1.996 | Accuracy: 0.609375 | 7.102 sec/iter\n",
      "Epoch: 33 | Batch: 027 / 030 | Total loss: 2.018 | Reg loss: 0.046 | Tree loss: 2.018 | Accuracy: 0.578125 | 7.102 sec/iter\n",
      "Epoch: 33 | Batch: 028 / 030 | Total loss: 1.969 | Reg loss: 0.046 | Tree loss: 1.969 | Accuracy: 0.644531 | 7.101 sec/iter\n",
      "Epoch: 33 | Batch: 029 / 030 | Total loss: 2.000 | Reg loss: 0.046 | Tree loss: 2.000 | Accuracy: 0.572816 | 7.097 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 34 | Batch: 000 / 030 | Total loss: 2.417 | Reg loss: 0.045 | Tree loss: 2.417 | Accuracy: 0.609375 | 7.104 sec/iter\n",
      "Epoch: 34 | Batch: 001 / 030 | Total loss: 2.399 | Reg loss: 0.045 | Tree loss: 2.399 | Accuracy: 0.589844 | 7.104 sec/iter\n",
      "Epoch: 34 | Batch: 002 / 030 | Total loss: 2.404 | Reg loss: 0.045 | Tree loss: 2.404 | Accuracy: 0.589844 | 7.104 sec/iter\n",
      "Epoch: 34 | Batch: 003 / 030 | Total loss: 2.312 | Reg loss: 0.045 | Tree loss: 2.312 | Accuracy: 0.642578 | 7.104 sec/iter\n",
      "Epoch: 34 | Batch: 004 / 030 | Total loss: 2.367 | Reg loss: 0.045 | Tree loss: 2.367 | Accuracy: 0.541016 | 7.105 sec/iter\n",
      "Epoch: 34 | Batch: 005 / 030 | Total loss: 2.338 | Reg loss: 0.045 | Tree loss: 2.338 | Accuracy: 0.585938 | 7.105 sec/iter\n",
      "Epoch: 34 | Batch: 006 / 030 | Total loss: 2.288 | Reg loss: 0.045 | Tree loss: 2.288 | Accuracy: 0.593750 | 7.105 sec/iter\n",
      "Epoch: 34 | Batch: 007 / 030 | Total loss: 2.295 | Reg loss: 0.045 | Tree loss: 2.295 | Accuracy: 0.578125 | 7.105 sec/iter\n",
      "Epoch: 34 | Batch: 008 / 030 | Total loss: 2.232 | Reg loss: 0.045 | Tree loss: 2.232 | Accuracy: 0.603516 | 7.105 sec/iter\n",
      "Epoch: 34 | Batch: 009 / 030 | Total loss: 2.233 | Reg loss: 0.045 | Tree loss: 2.233 | Accuracy: 0.556641 | 7.104 sec/iter\n",
      "Epoch: 34 | Batch: 010 / 030 | Total loss: 2.213 | Reg loss: 0.045 | Tree loss: 2.213 | Accuracy: 0.603516 | 7.104 sec/iter\n",
      "Epoch: 34 | Batch: 011 / 030 | Total loss: 2.178 | Reg loss: 0.045 | Tree loss: 2.178 | Accuracy: 0.578125 | 7.103 sec/iter\n",
      "Epoch: 34 | Batch: 012 / 030 | Total loss: 2.205 | Reg loss: 0.045 | Tree loss: 2.205 | Accuracy: 0.566406 | 7.102 sec/iter\n",
      "Epoch: 34 | Batch: 013 / 030 | Total loss: 2.180 | Reg loss: 0.045 | Tree loss: 2.180 | Accuracy: 0.566406 | 7.1 sec/iter\n",
      "Epoch: 34 | Batch: 014 / 030 | Total loss: 2.122 | Reg loss: 0.045 | Tree loss: 2.122 | Accuracy: 0.582031 | 7.098 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 34 | Batch: 015 / 030 | Total loss: 2.111 | Reg loss: 0.045 | Tree loss: 2.111 | Accuracy: 0.585938 | 7.095 sec/iter\n",
      "Epoch: 34 | Batch: 016 / 030 | Total loss: 2.105 | Reg loss: 0.045 | Tree loss: 2.105 | Accuracy: 0.558594 | 7.093 sec/iter\n",
      "Epoch: 34 | Batch: 017 / 030 | Total loss: 2.067 | Reg loss: 0.045 | Tree loss: 2.067 | Accuracy: 0.597656 | 7.091 sec/iter\n",
      "Epoch: 34 | Batch: 018 / 030 | Total loss: 2.081 | Reg loss: 0.045 | Tree loss: 2.081 | Accuracy: 0.546875 | 7.088 sec/iter\n",
      "Epoch: 34 | Batch: 019 / 030 | Total loss: 2.031 | Reg loss: 0.045 | Tree loss: 2.031 | Accuracy: 0.574219 | 7.086 sec/iter\n",
      "Epoch: 34 | Batch: 020 / 030 | Total loss: 2.042 | Reg loss: 0.046 | Tree loss: 2.042 | Accuracy: 0.566406 | 7.084 sec/iter\n",
      "Epoch: 34 | Batch: 021 / 030 | Total loss: 1.996 | Reg loss: 0.046 | Tree loss: 1.996 | Accuracy: 0.611328 | 7.082 sec/iter\n",
      "Epoch: 34 | Batch: 022 / 030 | Total loss: 1.993 | Reg loss: 0.046 | Tree loss: 1.993 | Accuracy: 0.580078 | 7.082 sec/iter\n",
      "Epoch: 34 | Batch: 023 / 030 | Total loss: 1.996 | Reg loss: 0.046 | Tree loss: 1.996 | Accuracy: 0.582031 | 7.082 sec/iter\n",
      "Epoch: 34 | Batch: 024 / 030 | Total loss: 1.984 | Reg loss: 0.046 | Tree loss: 1.984 | Accuracy: 0.554688 | 7.082 sec/iter\n",
      "Epoch: 34 | Batch: 025 / 030 | Total loss: 1.949 | Reg loss: 0.046 | Tree loss: 1.949 | Accuracy: 0.595703 | 7.083 sec/iter\n",
      "Epoch: 34 | Batch: 026 / 030 | Total loss: 1.952 | Reg loss: 0.046 | Tree loss: 1.952 | Accuracy: 0.578125 | 7.082 sec/iter\n",
      "Epoch: 34 | Batch: 027 / 030 | Total loss: 1.916 | Reg loss: 0.046 | Tree loss: 1.916 | Accuracy: 0.572266 | 7.08 sec/iter\n",
      "Epoch: 34 | Batch: 028 / 030 | Total loss: 1.896 | Reg loss: 0.046 | Tree loss: 1.896 | Accuracy: 0.601562 | 7.081 sec/iter\n",
      "Epoch: 34 | Batch: 029 / 030 | Total loss: 1.894 | Reg loss: 0.046 | Tree loss: 1.894 | Accuracy: 0.621359 | 7.077 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 35 | Batch: 000 / 030 | Total loss: 2.322 | Reg loss: 0.045 | Tree loss: 2.322 | Accuracy: 0.576172 | 7.09 sec/iter\n",
      "Epoch: 35 | Batch: 001 / 030 | Total loss: 2.304 | Reg loss: 0.045 | Tree loss: 2.304 | Accuracy: 0.589844 | 7.09 sec/iter\n",
      "Epoch: 35 | Batch: 002 / 030 | Total loss: 2.300 | Reg loss: 0.045 | Tree loss: 2.300 | Accuracy: 0.562500 | 7.09 sec/iter\n",
      "Epoch: 35 | Batch: 003 / 030 | Total loss: 2.254 | Reg loss: 0.045 | Tree loss: 2.254 | Accuracy: 0.570312 | 7.09 sec/iter\n",
      "Epoch: 35 | Batch: 004 / 030 | Total loss: 2.215 | Reg loss: 0.045 | Tree loss: 2.215 | Accuracy: 0.597656 | 7.09 sec/iter\n",
      "Epoch: 35 | Batch: 005 / 030 | Total loss: 2.212 | Reg loss: 0.045 | Tree loss: 2.212 | Accuracy: 0.623047 | 7.09 sec/iter\n",
      "Epoch: 35 | Batch: 006 / 030 | Total loss: 2.214 | Reg loss: 0.045 | Tree loss: 2.214 | Accuracy: 0.580078 | 7.09 sec/iter\n",
      "Epoch: 35 | Batch: 007 / 030 | Total loss: 2.183 | Reg loss: 0.045 | Tree loss: 2.183 | Accuracy: 0.560547 | 7.089 sec/iter\n",
      "Epoch: 35 | Batch: 008 / 030 | Total loss: 2.175 | Reg loss: 0.045 | Tree loss: 2.175 | Accuracy: 0.541016 | 7.089 sec/iter\n",
      "Epoch: 35 | Batch: 009 / 030 | Total loss: 2.140 | Reg loss: 0.045 | Tree loss: 2.140 | Accuracy: 0.595703 | 7.089 sec/iter\n",
      "Epoch: 35 | Batch: 010 / 030 | Total loss: 2.106 | Reg loss: 0.045 | Tree loss: 2.106 | Accuracy: 0.572266 | 7.089 sec/iter\n",
      "Epoch: 35 | Batch: 011 / 030 | Total loss: 2.082 | Reg loss: 0.045 | Tree loss: 2.082 | Accuracy: 0.601562 | 7.089 sec/iter\n",
      "Epoch: 35 | Batch: 012 / 030 | Total loss: 2.066 | Reg loss: 0.045 | Tree loss: 2.066 | Accuracy: 0.597656 | 7.089 sec/iter\n",
      "Epoch: 35 | Batch: 013 / 030 | Total loss: 2.098 | Reg loss: 0.045 | Tree loss: 2.098 | Accuracy: 0.578125 | 7.089 sec/iter\n",
      "Epoch: 35 | Batch: 014 / 030 | Total loss: 2.056 | Reg loss: 0.045 | Tree loss: 2.056 | Accuracy: 0.578125 | 7.089 sec/iter\n",
      "Epoch: 35 | Batch: 015 / 030 | Total loss: 2.034 | Reg loss: 0.045 | Tree loss: 2.034 | Accuracy: 0.623047 | 7.089 sec/iter\n",
      "Epoch: 35 | Batch: 016 / 030 | Total loss: 2.011 | Reg loss: 0.045 | Tree loss: 2.011 | Accuracy: 0.576172 | 7.089 sec/iter\n",
      "Epoch: 35 | Batch: 017 / 030 | Total loss: 1.989 | Reg loss: 0.045 | Tree loss: 1.989 | Accuracy: 0.593750 | 7.089 sec/iter\n",
      "Epoch: 35 | Batch: 018 / 030 | Total loss: 1.989 | Reg loss: 0.045 | Tree loss: 1.989 | Accuracy: 0.525391 | 7.088 sec/iter\n",
      "Epoch: 35 | Batch: 019 / 030 | Total loss: 1.955 | Reg loss: 0.045 | Tree loss: 1.955 | Accuracy: 0.589844 | 7.086 sec/iter\n",
      "Epoch: 35 | Batch: 020 / 030 | Total loss: 1.940 | Reg loss: 0.045 | Tree loss: 1.940 | Accuracy: 0.601562 | 7.083 sec/iter\n",
      "Epoch: 35 | Batch: 021 / 030 | Total loss: 1.932 | Reg loss: 0.046 | Tree loss: 1.932 | Accuracy: 0.585938 | 7.081 sec/iter\n",
      "Epoch: 35 | Batch: 022 / 030 | Total loss: 1.934 | Reg loss: 0.046 | Tree loss: 1.934 | Accuracy: 0.564453 | 7.079 sec/iter\n",
      "Epoch: 35 | Batch: 023 / 030 | Total loss: 1.902 | Reg loss: 0.046 | Tree loss: 1.902 | Accuracy: 0.552734 | 7.077 sec/iter\n",
      "Epoch: 35 | Batch: 024 / 030 | Total loss: 1.908 | Reg loss: 0.046 | Tree loss: 1.908 | Accuracy: 0.570312 | 7.075 sec/iter\n",
      "Epoch: 35 | Batch: 025 / 030 | Total loss: 1.878 | Reg loss: 0.046 | Tree loss: 1.878 | Accuracy: 0.583984 | 7.074 sec/iter\n",
      "Epoch: 35 | Batch: 026 / 030 | Total loss: 1.859 | Reg loss: 0.046 | Tree loss: 1.859 | Accuracy: 0.615234 | 7.072 sec/iter\n",
      "Epoch: 35 | Batch: 027 / 030 | Total loss: 1.854 | Reg loss: 0.046 | Tree loss: 1.854 | Accuracy: 0.583984 | 7.072 sec/iter\n",
      "Epoch: 35 | Batch: 028 / 030 | Total loss: 1.811 | Reg loss: 0.046 | Tree loss: 1.811 | Accuracy: 0.601562 | 7.073 sec/iter\n",
      "Epoch: 35 | Batch: 029 / 030 | Total loss: 1.785 | Reg loss: 0.046 | Tree loss: 1.785 | Accuracy: 0.621359 | 7.069 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 36 | Batch: 000 / 030 | Total loss: 2.193 | Reg loss: 0.045 | Tree loss: 2.193 | Accuracy: 0.623047 | 7.093 sec/iter\n",
      "Epoch: 36 | Batch: 001 / 030 | Total loss: 2.207 | Reg loss: 0.045 | Tree loss: 2.207 | Accuracy: 0.566406 | 7.093 sec/iter\n",
      "Epoch: 36 | Batch: 002 / 030 | Total loss: 2.200 | Reg loss: 0.045 | Tree loss: 2.200 | Accuracy: 0.619141 | 7.093 sec/iter\n",
      "Epoch: 36 | Batch: 003 / 030 | Total loss: 2.137 | Reg loss: 0.045 | Tree loss: 2.137 | Accuracy: 0.560547 | 7.093 sec/iter\n",
      "Epoch: 36 | Batch: 004 / 030 | Total loss: 2.122 | Reg loss: 0.045 | Tree loss: 2.122 | Accuracy: 0.558594 | 7.093 sec/iter\n",
      "Epoch: 36 | Batch: 005 / 030 | Total loss: 2.169 | Reg loss: 0.045 | Tree loss: 2.169 | Accuracy: 0.591797 | 7.093 sec/iter\n",
      "Epoch: 36 | Batch: 006 / 030 | Total loss: 2.127 | Reg loss: 0.045 | Tree loss: 2.127 | Accuracy: 0.580078 | 7.093 sec/iter\n",
      "Epoch: 36 | Batch: 007 / 030 | Total loss: 2.084 | Reg loss: 0.045 | Tree loss: 2.084 | Accuracy: 0.595703 | 7.093 sec/iter\n",
      "Epoch: 36 | Batch: 008 / 030 | Total loss: 2.103 | Reg loss: 0.045 | Tree loss: 2.103 | Accuracy: 0.556641 | 7.092 sec/iter\n",
      "Epoch: 36 | Batch: 009 / 030 | Total loss: 2.075 | Reg loss: 0.045 | Tree loss: 2.075 | Accuracy: 0.529297 | 7.092 sec/iter\n",
      "Epoch: 36 | Batch: 010 / 030 | Total loss: 2.035 | Reg loss: 0.045 | Tree loss: 2.035 | Accuracy: 0.589844 | 7.092 sec/iter\n",
      "Epoch: 36 | Batch: 011 / 030 | Total loss: 2.041 | Reg loss: 0.045 | Tree loss: 2.041 | Accuracy: 0.578125 | 7.092 sec/iter\n",
      "Epoch: 36 | Batch: 012 / 030 | Total loss: 2.001 | Reg loss: 0.045 | Tree loss: 2.001 | Accuracy: 0.572266 | 7.092 sec/iter\n",
      "Epoch: 36 | Batch: 013 / 030 | Total loss: 1.966 | Reg loss: 0.045 | Tree loss: 1.966 | Accuracy: 0.595703 | 7.092 sec/iter\n",
      "Epoch: 36 | Batch: 014 / 030 | Total loss: 1.973 | Reg loss: 0.045 | Tree loss: 1.973 | Accuracy: 0.597656 | 7.092 sec/iter\n",
      "Epoch: 36 | Batch: 015 / 030 | Total loss: 1.958 | Reg loss: 0.045 | Tree loss: 1.958 | Accuracy: 0.583984 | 7.092 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 36 | Batch: 016 / 030 | Total loss: 1.930 | Reg loss: 0.045 | Tree loss: 1.930 | Accuracy: 0.583984 | 7.092 sec/iter\n",
      "Epoch: 36 | Batch: 017 / 030 | Total loss: 1.905 | Reg loss: 0.045 | Tree loss: 1.905 | Accuracy: 0.568359 | 7.091 sec/iter\n",
      "Epoch: 36 | Batch: 018 / 030 | Total loss: 1.900 | Reg loss: 0.045 | Tree loss: 1.900 | Accuracy: 0.576172 | 7.089 sec/iter\n",
      "Epoch: 36 | Batch: 019 / 030 | Total loss: 1.884 | Reg loss: 0.045 | Tree loss: 1.884 | Accuracy: 0.599609 | 7.086 sec/iter\n",
      "Epoch: 36 | Batch: 020 / 030 | Total loss: 1.842 | Reg loss: 0.045 | Tree loss: 1.842 | Accuracy: 0.585938 | 7.085 sec/iter\n",
      "Epoch: 36 | Batch: 021 / 030 | Total loss: 1.875 | Reg loss: 0.045 | Tree loss: 1.875 | Accuracy: 0.580078 | 7.083 sec/iter\n",
      "Epoch: 36 | Batch: 022 / 030 | Total loss: 1.843 | Reg loss: 0.046 | Tree loss: 1.843 | Accuracy: 0.587891 | 7.083 sec/iter\n",
      "Epoch: 36 | Batch: 023 / 030 | Total loss: 1.814 | Reg loss: 0.046 | Tree loss: 1.814 | Accuracy: 0.572266 | 7.082 sec/iter\n",
      "Epoch: 36 | Batch: 024 / 030 | Total loss: 1.796 | Reg loss: 0.046 | Tree loss: 1.796 | Accuracy: 0.609375 | 7.082 sec/iter\n",
      "Epoch: 36 | Batch: 025 / 030 | Total loss: 1.794 | Reg loss: 0.046 | Tree loss: 1.794 | Accuracy: 0.574219 | 7.081 sec/iter\n",
      "Epoch: 36 | Batch: 026 / 030 | Total loss: 1.791 | Reg loss: 0.046 | Tree loss: 1.791 | Accuracy: 0.578125 | 7.08 sec/iter\n",
      "Epoch: 36 | Batch: 027 / 030 | Total loss: 1.777 | Reg loss: 0.046 | Tree loss: 1.777 | Accuracy: 0.580078 | 7.08 sec/iter\n",
      "Epoch: 36 | Batch: 028 / 030 | Total loss: 1.745 | Reg loss: 0.046 | Tree loss: 1.745 | Accuracy: 0.605469 | 7.08 sec/iter\n",
      "Epoch: 36 | Batch: 029 / 030 | Total loss: 1.799 | Reg loss: 0.046 | Tree loss: 1.799 | Accuracy: 0.582524 | 7.077 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 37 | Batch: 000 / 030 | Total loss: 2.177 | Reg loss: 0.045 | Tree loss: 2.177 | Accuracy: 0.542969 | 7.084 sec/iter\n",
      "Epoch: 37 | Batch: 001 / 030 | Total loss: 2.114 | Reg loss: 0.045 | Tree loss: 2.114 | Accuracy: 0.582031 | 7.084 sec/iter\n",
      "Epoch: 37 | Batch: 002 / 030 | Total loss: 2.123 | Reg loss: 0.045 | Tree loss: 2.123 | Accuracy: 0.617188 | 7.084 sec/iter\n",
      "Epoch: 37 | Batch: 003 / 030 | Total loss: 2.103 | Reg loss: 0.045 | Tree loss: 2.103 | Accuracy: 0.585938 | 7.084 sec/iter\n",
      "Epoch: 37 | Batch: 004 / 030 | Total loss: 2.076 | Reg loss: 0.045 | Tree loss: 2.076 | Accuracy: 0.593750 | 7.084 sec/iter\n",
      "Epoch: 37 | Batch: 005 / 030 | Total loss: 2.017 | Reg loss: 0.045 | Tree loss: 2.017 | Accuracy: 0.599609 | 7.085 sec/iter\n",
      "Epoch: 37 | Batch: 006 / 030 | Total loss: 2.012 | Reg loss: 0.045 | Tree loss: 2.012 | Accuracy: 0.613281 | 7.085 sec/iter\n",
      "Epoch: 37 | Batch: 007 / 030 | Total loss: 2.035 | Reg loss: 0.045 | Tree loss: 2.035 | Accuracy: 0.576172 | 7.084 sec/iter\n",
      "Epoch: 37 | Batch: 008 / 030 | Total loss: 1.974 | Reg loss: 0.045 | Tree loss: 1.974 | Accuracy: 0.580078 | 7.084 sec/iter\n",
      "Epoch: 37 | Batch: 009 / 030 | Total loss: 1.950 | Reg loss: 0.045 | Tree loss: 1.950 | Accuracy: 0.568359 | 7.084 sec/iter\n",
      "Epoch: 37 | Batch: 010 / 030 | Total loss: 1.982 | Reg loss: 0.045 | Tree loss: 1.982 | Accuracy: 0.585938 | 7.083 sec/iter\n",
      "Epoch: 37 | Batch: 011 / 030 | Total loss: 1.932 | Reg loss: 0.045 | Tree loss: 1.932 | Accuracy: 0.597656 | 7.083 sec/iter\n",
      "Epoch: 37 | Batch: 012 / 030 | Total loss: 1.935 | Reg loss: 0.045 | Tree loss: 1.935 | Accuracy: 0.570312 | 7.082 sec/iter\n",
      "Epoch: 37 | Batch: 013 / 030 | Total loss: 1.910 | Reg loss: 0.045 | Tree loss: 1.910 | Accuracy: 0.576172 | 7.082 sec/iter\n",
      "Epoch: 37 | Batch: 014 / 030 | Total loss: 1.918 | Reg loss: 0.045 | Tree loss: 1.918 | Accuracy: 0.550781 | 7.082 sec/iter\n",
      "Epoch: 37 | Batch: 015 / 030 | Total loss: 1.888 | Reg loss: 0.045 | Tree loss: 1.888 | Accuracy: 0.550781 | 7.082 sec/iter\n",
      "Epoch: 37 | Batch: 016 / 030 | Total loss: 1.863 | Reg loss: 0.045 | Tree loss: 1.863 | Accuracy: 0.578125 | 7.082 sec/iter\n",
      "Epoch: 37 | Batch: 017 / 030 | Total loss: 1.832 | Reg loss: 0.045 | Tree loss: 1.832 | Accuracy: 0.601562 | 7.082 sec/iter\n",
      "Epoch: 37 | Batch: 018 / 030 | Total loss: 1.830 | Reg loss: 0.045 | Tree loss: 1.830 | Accuracy: 0.566406 | 7.082 sec/iter\n",
      "Epoch: 37 | Batch: 019 / 030 | Total loss: 1.798 | Reg loss: 0.045 | Tree loss: 1.798 | Accuracy: 0.576172 | 7.079 sec/iter\n",
      "Epoch: 37 | Batch: 020 / 030 | Total loss: 1.799 | Reg loss: 0.045 | Tree loss: 1.799 | Accuracy: 0.611328 | 7.076 sec/iter\n",
      "Epoch: 37 | Batch: 021 / 030 | Total loss: 1.775 | Reg loss: 0.045 | Tree loss: 1.775 | Accuracy: 0.550781 | 7.075 sec/iter\n",
      "Epoch: 37 | Batch: 022 / 030 | Total loss: 1.789 | Reg loss: 0.045 | Tree loss: 1.789 | Accuracy: 0.570312 | 7.075 sec/iter\n",
      "Epoch: 37 | Batch: 023 / 030 | Total loss: 1.746 | Reg loss: 0.046 | Tree loss: 1.746 | Accuracy: 0.574219 | 7.076 sec/iter\n",
      "Epoch: 37 | Batch: 024 / 030 | Total loss: 1.740 | Reg loss: 0.046 | Tree loss: 1.740 | Accuracy: 0.611328 | 7.076 sec/iter\n",
      "Epoch: 37 | Batch: 025 / 030 | Total loss: 1.722 | Reg loss: 0.046 | Tree loss: 1.722 | Accuracy: 0.595703 | 7.076 sec/iter\n",
      "Epoch: 37 | Batch: 026 / 030 | Total loss: 1.683 | Reg loss: 0.046 | Tree loss: 1.683 | Accuracy: 0.603516 | 7.076 sec/iter\n",
      "Epoch: 37 | Batch: 027 / 030 | Total loss: 1.690 | Reg loss: 0.046 | Tree loss: 1.690 | Accuracy: 0.564453 | 7.077 sec/iter\n",
      "Epoch: 37 | Batch: 028 / 030 | Total loss: 1.663 | Reg loss: 0.046 | Tree loss: 1.663 | Accuracy: 0.593750 | 7.077 sec/iter\n",
      "Epoch: 37 | Batch: 029 / 030 | Total loss: 1.668 | Reg loss: 0.046 | Tree loss: 1.668 | Accuracy: 0.640777 | 7.073 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 38 | Batch: 000 / 030 | Total loss: 2.049 | Reg loss: 0.045 | Tree loss: 2.049 | Accuracy: 0.589844 | 7.093 sec/iter\n",
      "Epoch: 38 | Batch: 001 / 030 | Total loss: 2.078 | Reg loss: 0.045 | Tree loss: 2.078 | Accuracy: 0.554688 | 7.093 sec/iter\n",
      "Epoch: 38 | Batch: 002 / 030 | Total loss: 1.974 | Reg loss: 0.045 | Tree loss: 1.974 | Accuracy: 0.601562 | 7.093 sec/iter\n",
      "Epoch: 38 | Batch: 003 / 030 | Total loss: 2.004 | Reg loss: 0.045 | Tree loss: 2.004 | Accuracy: 0.597656 | 7.093 sec/iter\n",
      "Epoch: 38 | Batch: 004 / 030 | Total loss: 1.998 | Reg loss: 0.045 | Tree loss: 1.998 | Accuracy: 0.576172 | 7.093 sec/iter\n",
      "Epoch: 38 | Batch: 005 / 030 | Total loss: 1.957 | Reg loss: 0.045 | Tree loss: 1.957 | Accuracy: 0.570312 | 7.093 sec/iter\n",
      "Epoch: 38 | Batch: 006 / 030 | Total loss: 1.999 | Reg loss: 0.045 | Tree loss: 1.999 | Accuracy: 0.556641 | 7.093 sec/iter\n",
      "Epoch: 38 | Batch: 007 / 030 | Total loss: 1.934 | Reg loss: 0.045 | Tree loss: 1.934 | Accuracy: 0.572266 | 7.093 sec/iter\n",
      "Epoch: 38 | Batch: 008 / 030 | Total loss: 1.940 | Reg loss: 0.045 | Tree loss: 1.940 | Accuracy: 0.582031 | 7.093 sec/iter\n",
      "Epoch: 38 | Batch: 009 / 030 | Total loss: 1.899 | Reg loss: 0.045 | Tree loss: 1.899 | Accuracy: 0.570312 | 7.093 sec/iter\n",
      "Epoch: 38 | Batch: 010 / 030 | Total loss: 1.834 | Reg loss: 0.045 | Tree loss: 1.834 | Accuracy: 0.599609 | 7.092 sec/iter\n",
      "Epoch: 38 | Batch: 011 / 030 | Total loss: 1.857 | Reg loss: 0.045 | Tree loss: 1.857 | Accuracy: 0.611328 | 7.092 sec/iter\n",
      "Epoch: 38 | Batch: 012 / 030 | Total loss: 1.825 | Reg loss: 0.045 | Tree loss: 1.825 | Accuracy: 0.589844 | 7.092 sec/iter\n",
      "Epoch: 38 | Batch: 013 / 030 | Total loss: 1.845 | Reg loss: 0.045 | Tree loss: 1.845 | Accuracy: 0.566406 | 7.092 sec/iter\n",
      "Epoch: 38 | Batch: 014 / 030 | Total loss: 1.836 | Reg loss: 0.045 | Tree loss: 1.836 | Accuracy: 0.556641 | 7.092 sec/iter\n",
      "Epoch: 38 | Batch: 015 / 030 | Total loss: 1.800 | Reg loss: 0.045 | Tree loss: 1.800 | Accuracy: 0.548828 | 7.089 sec/iter\n",
      "Epoch: 38 | Batch: 016 / 030 | Total loss: 1.766 | Reg loss: 0.045 | Tree loss: 1.766 | Accuracy: 0.615234 | 7.086 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 38 | Batch: 017 / 030 | Total loss: 1.776 | Reg loss: 0.045 | Tree loss: 1.776 | Accuracy: 0.552734 | 7.087 sec/iter\n",
      "Epoch: 38 | Batch: 018 / 030 | Total loss: 1.756 | Reg loss: 0.045 | Tree loss: 1.756 | Accuracy: 0.601562 | 7.087 sec/iter\n",
      "Epoch: 38 | Batch: 019 / 030 | Total loss: 1.762 | Reg loss: 0.045 | Tree loss: 1.762 | Accuracy: 0.578125 | 7.087 sec/iter\n",
      "Epoch: 38 | Batch: 020 / 030 | Total loss: 1.717 | Reg loss: 0.045 | Tree loss: 1.717 | Accuracy: 0.595703 | 7.087 sec/iter\n",
      "Epoch: 38 | Batch: 021 / 030 | Total loss: 1.688 | Reg loss: 0.045 | Tree loss: 1.688 | Accuracy: 0.599609 | 7.088 sec/iter\n",
      "Epoch: 38 | Batch: 022 / 030 | Total loss: 1.716 | Reg loss: 0.045 | Tree loss: 1.716 | Accuracy: 0.580078 | 7.088 sec/iter\n",
      "Epoch: 38 | Batch: 023 / 030 | Total loss: 1.667 | Reg loss: 0.045 | Tree loss: 1.667 | Accuracy: 0.591797 | 7.088 sec/iter\n",
      "Epoch: 38 | Batch: 024 / 030 | Total loss: 1.667 | Reg loss: 0.046 | Tree loss: 1.667 | Accuracy: 0.595703 | 7.088 sec/iter\n",
      "Epoch: 38 | Batch: 025 / 030 | Total loss: 1.655 | Reg loss: 0.046 | Tree loss: 1.655 | Accuracy: 0.578125 | 7.088 sec/iter\n",
      "Epoch: 38 | Batch: 026 / 030 | Total loss: 1.672 | Reg loss: 0.046 | Tree loss: 1.672 | Accuracy: 0.578125 | 7.088 sec/iter\n",
      "Epoch: 38 | Batch: 027 / 030 | Total loss: 1.665 | Reg loss: 0.046 | Tree loss: 1.665 | Accuracy: 0.597656 | 7.088 sec/iter\n",
      "Epoch: 38 | Batch: 028 / 030 | Total loss: 1.636 | Reg loss: 0.046 | Tree loss: 1.636 | Accuracy: 0.589844 | 7.088 sec/iter\n",
      "Epoch: 38 | Batch: 029 / 030 | Total loss: 1.618 | Reg loss: 0.046 | Tree loss: 1.618 | Accuracy: 0.592233 | 7.085 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 39 | Batch: 000 / 030 | Total loss: 2.021 | Reg loss: 0.045 | Tree loss: 2.021 | Accuracy: 0.554688 | 7.12 sec/iter\n",
      "Epoch: 39 | Batch: 001 / 030 | Total loss: 1.951 | Reg loss: 0.045 | Tree loss: 1.951 | Accuracy: 0.613281 | 7.121 sec/iter\n",
      "Epoch: 39 | Batch: 002 / 030 | Total loss: 1.964 | Reg loss: 0.045 | Tree loss: 1.964 | Accuracy: 0.597656 | 7.121 sec/iter\n",
      "Epoch: 39 | Batch: 003 / 030 | Total loss: 1.913 | Reg loss: 0.045 | Tree loss: 1.913 | Accuracy: 0.572266 | 7.121 sec/iter\n",
      "Epoch: 39 | Batch: 004 / 030 | Total loss: 1.927 | Reg loss: 0.045 | Tree loss: 1.927 | Accuracy: 0.595703 | 7.121 sec/iter\n",
      "Epoch: 39 | Batch: 005 / 030 | Total loss: 1.895 | Reg loss: 0.045 | Tree loss: 1.895 | Accuracy: 0.558594 | 7.12 sec/iter\n",
      "Epoch: 39 | Batch: 006 / 030 | Total loss: 1.856 | Reg loss: 0.045 | Tree loss: 1.856 | Accuracy: 0.640625 | 7.119 sec/iter\n",
      "Epoch: 39 | Batch: 007 / 030 | Total loss: 1.896 | Reg loss: 0.045 | Tree loss: 1.896 | Accuracy: 0.529297 | 7.118 sec/iter\n",
      "Epoch: 39 | Batch: 008 / 030 | Total loss: 1.798 | Reg loss: 0.045 | Tree loss: 1.798 | Accuracy: 0.644531 | 7.118 sec/iter\n",
      "Epoch: 39 | Batch: 009 / 030 | Total loss: 1.824 | Reg loss: 0.045 | Tree loss: 1.824 | Accuracy: 0.611328 | 7.119 sec/iter\n",
      "Epoch: 39 | Batch: 010 / 030 | Total loss: 1.846 | Reg loss: 0.045 | Tree loss: 1.846 | Accuracy: 0.554688 | 7.119 sec/iter\n",
      "Epoch: 39 | Batch: 011 / 030 | Total loss: 1.778 | Reg loss: 0.045 | Tree loss: 1.778 | Accuracy: 0.583984 | 7.119 sec/iter\n",
      "Epoch: 39 | Batch: 012 / 030 | Total loss: 1.777 | Reg loss: 0.045 | Tree loss: 1.777 | Accuracy: 0.589844 | 7.12 sec/iter\n",
      "Epoch: 39 | Batch: 013 / 030 | Total loss: 1.747 | Reg loss: 0.045 | Tree loss: 1.747 | Accuracy: 0.615234 | 7.12 sec/iter\n",
      "Epoch: 39 | Batch: 014 / 030 | Total loss: 1.734 | Reg loss: 0.045 | Tree loss: 1.734 | Accuracy: 0.595703 | 7.12 sec/iter\n",
      "Epoch: 39 | Batch: 015 / 030 | Total loss: 1.724 | Reg loss: 0.045 | Tree loss: 1.724 | Accuracy: 0.572266 | 7.12 sec/iter\n",
      "Epoch: 39 | Batch: 016 / 030 | Total loss: 1.721 | Reg loss: 0.045 | Tree loss: 1.721 | Accuracy: 0.585938 | 7.12 sec/iter\n",
      "Epoch: 39 | Batch: 017 / 030 | Total loss: 1.712 | Reg loss: 0.045 | Tree loss: 1.712 | Accuracy: 0.566406 | 7.12 sec/iter\n",
      "Epoch: 39 | Batch: 018 / 030 | Total loss: 1.680 | Reg loss: 0.045 | Tree loss: 1.680 | Accuracy: 0.585938 | 7.12 sec/iter\n",
      "Epoch: 39 | Batch: 019 / 030 | Total loss: 1.691 | Reg loss: 0.045 | Tree loss: 1.691 | Accuracy: 0.578125 | 7.12 sec/iter\n",
      "Epoch: 39 | Batch: 020 / 030 | Total loss: 1.665 | Reg loss: 0.045 | Tree loss: 1.665 | Accuracy: 0.552734 | 7.12 sec/iter\n",
      "Epoch: 39 | Batch: 021 / 030 | Total loss: 1.650 | Reg loss: 0.045 | Tree loss: 1.650 | Accuracy: 0.589844 | 7.119 sec/iter\n",
      "Epoch: 39 | Batch: 022 / 030 | Total loss: 1.607 | Reg loss: 0.045 | Tree loss: 1.607 | Accuracy: 0.617188 | 7.119 sec/iter\n",
      "Epoch: 39 | Batch: 023 / 030 | Total loss: 1.621 | Reg loss: 0.045 | Tree loss: 1.621 | Accuracy: 0.562500 | 7.118 sec/iter\n",
      "Epoch: 39 | Batch: 024 / 030 | Total loss: 1.625 | Reg loss: 0.045 | Tree loss: 1.625 | Accuracy: 0.546875 | 7.118 sec/iter\n",
      "Epoch: 39 | Batch: 025 / 030 | Total loss: 1.612 | Reg loss: 0.046 | Tree loss: 1.612 | Accuracy: 0.566406 | 7.118 sec/iter\n",
      "Epoch: 39 | Batch: 026 / 030 | Total loss: 1.581 | Reg loss: 0.046 | Tree loss: 1.581 | Accuracy: 0.585938 | 7.117 sec/iter\n",
      "Epoch: 39 | Batch: 027 / 030 | Total loss: 1.579 | Reg loss: 0.046 | Tree loss: 1.579 | Accuracy: 0.550781 | 7.117 sec/iter\n",
      "Epoch: 39 | Batch: 028 / 030 | Total loss: 1.572 | Reg loss: 0.046 | Tree loss: 1.572 | Accuracy: 0.589844 | 7.117 sec/iter\n",
      "Epoch: 39 | Batch: 029 / 030 | Total loss: 1.536 | Reg loss: 0.046 | Tree loss: 1.536 | Accuracy: 0.543689 | 7.114 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 40 | Batch: 000 / 030 | Total loss: 1.903 | Reg loss: 0.044 | Tree loss: 1.903 | Accuracy: 0.539062 | 7.128 sec/iter\n",
      "Epoch: 40 | Batch: 001 / 030 | Total loss: 1.927 | Reg loss: 0.044 | Tree loss: 1.927 | Accuracy: 0.591797 | 7.127 sec/iter\n",
      "Epoch: 40 | Batch: 002 / 030 | Total loss: 1.881 | Reg loss: 0.044 | Tree loss: 1.881 | Accuracy: 0.578125 | 7.125 sec/iter\n",
      "Epoch: 40 | Batch: 003 / 030 | Total loss: 1.836 | Reg loss: 0.044 | Tree loss: 1.836 | Accuracy: 0.587891 | 7.124 sec/iter\n",
      "Epoch: 40 | Batch: 004 / 030 | Total loss: 1.864 | Reg loss: 0.044 | Tree loss: 1.864 | Accuracy: 0.582031 | 7.124 sec/iter\n",
      "Epoch: 40 | Batch: 005 / 030 | Total loss: 1.877 | Reg loss: 0.044 | Tree loss: 1.877 | Accuracy: 0.552734 | 7.122 sec/iter\n",
      "Epoch: 40 | Batch: 006 / 030 | Total loss: 1.783 | Reg loss: 0.044 | Tree loss: 1.783 | Accuracy: 0.566406 | 7.123 sec/iter\n",
      "Epoch: 40 | Batch: 007 / 030 | Total loss: 1.805 | Reg loss: 0.045 | Tree loss: 1.805 | Accuracy: 0.554688 | 7.123 sec/iter\n",
      "Epoch: 40 | Batch: 008 / 030 | Total loss: 1.788 | Reg loss: 0.045 | Tree loss: 1.788 | Accuracy: 0.564453 | 7.123 sec/iter\n",
      "Epoch: 40 | Batch: 009 / 030 | Total loss: 1.758 | Reg loss: 0.045 | Tree loss: 1.758 | Accuracy: 0.589844 | 7.123 sec/iter\n",
      "Epoch: 40 | Batch: 010 / 030 | Total loss: 1.755 | Reg loss: 0.045 | Tree loss: 1.755 | Accuracy: 0.607422 | 7.123 sec/iter\n",
      "Epoch: 40 | Batch: 011 / 030 | Total loss: 1.740 | Reg loss: 0.045 | Tree loss: 1.740 | Accuracy: 0.640625 | 7.123 sec/iter\n",
      "Epoch: 40 | Batch: 012 / 030 | Total loss: 1.728 | Reg loss: 0.045 | Tree loss: 1.728 | Accuracy: 0.576172 | 7.123 sec/iter\n",
      "Epoch: 40 | Batch: 013 / 030 | Total loss: 1.661 | Reg loss: 0.045 | Tree loss: 1.661 | Accuracy: 0.609375 | 7.123 sec/iter\n",
      "Epoch: 40 | Batch: 014 / 030 | Total loss: 1.670 | Reg loss: 0.045 | Tree loss: 1.670 | Accuracy: 0.587891 | 7.123 sec/iter\n",
      "Epoch: 40 | Batch: 015 / 030 | Total loss: 1.674 | Reg loss: 0.045 | Tree loss: 1.674 | Accuracy: 0.599609 | 7.123 sec/iter\n",
      "Epoch: 40 | Batch: 016 / 030 | Total loss: 1.625 | Reg loss: 0.045 | Tree loss: 1.625 | Accuracy: 0.589844 | 7.123 sec/iter\n",
      "Epoch: 40 | Batch: 017 / 030 | Total loss: 1.613 | Reg loss: 0.045 | Tree loss: 1.613 | Accuracy: 0.611328 | 7.122 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 40 | Batch: 018 / 030 | Total loss: 1.614 | Reg loss: 0.045 | Tree loss: 1.614 | Accuracy: 0.603516 | 7.122 sec/iter\n",
      "Epoch: 40 | Batch: 019 / 030 | Total loss: 1.587 | Reg loss: 0.045 | Tree loss: 1.587 | Accuracy: 0.595703 | 7.122 sec/iter\n",
      "Epoch: 40 | Batch: 020 / 030 | Total loss: 1.621 | Reg loss: 0.045 | Tree loss: 1.621 | Accuracy: 0.541016 | 7.122 sec/iter\n",
      "Epoch: 40 | Batch: 021 / 030 | Total loss: 1.608 | Reg loss: 0.045 | Tree loss: 1.608 | Accuracy: 0.552734 | 7.121 sec/iter\n",
      "Epoch: 40 | Batch: 022 / 030 | Total loss: 1.590 | Reg loss: 0.045 | Tree loss: 1.590 | Accuracy: 0.568359 | 7.121 sec/iter\n",
      "Epoch: 40 | Batch: 023 / 030 | Total loss: 1.555 | Reg loss: 0.045 | Tree loss: 1.555 | Accuracy: 0.583984 | 7.121 sec/iter\n",
      "Epoch: 40 | Batch: 024 / 030 | Total loss: 1.580 | Reg loss: 0.045 | Tree loss: 1.580 | Accuracy: 0.574219 | 7.121 sec/iter\n",
      "Epoch: 40 | Batch: 025 / 030 | Total loss: 1.522 | Reg loss: 0.045 | Tree loss: 1.522 | Accuracy: 0.621094 | 7.121 sec/iter\n",
      "Epoch: 40 | Batch: 026 / 030 | Total loss: 1.530 | Reg loss: 0.046 | Tree loss: 1.530 | Accuracy: 0.595703 | 7.121 sec/iter\n",
      "Epoch: 40 | Batch: 027 / 030 | Total loss: 1.513 | Reg loss: 0.046 | Tree loss: 1.513 | Accuracy: 0.580078 | 7.121 sec/iter\n",
      "Epoch: 40 | Batch: 028 / 030 | Total loss: 1.517 | Reg loss: 0.046 | Tree loss: 1.517 | Accuracy: 0.576172 | 7.121 sec/iter\n",
      "Epoch: 40 | Batch: 029 / 030 | Total loss: 1.525 | Reg loss: 0.046 | Tree loss: 1.525 | Accuracy: 0.475728 | 7.118 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 41 | Batch: 000 / 030 | Total loss: 1.827 | Reg loss: 0.044 | Tree loss: 1.827 | Accuracy: 0.568359 | 7.124 sec/iter\n",
      "Epoch: 41 | Batch: 001 / 030 | Total loss: 1.854 | Reg loss: 0.044 | Tree loss: 1.854 | Accuracy: 0.527344 | 7.122 sec/iter\n",
      "Epoch: 41 | Batch: 002 / 030 | Total loss: 1.797 | Reg loss: 0.044 | Tree loss: 1.797 | Accuracy: 0.605469 | 7.12 sec/iter\n",
      "Epoch: 41 | Batch: 003 / 030 | Total loss: 1.794 | Reg loss: 0.044 | Tree loss: 1.794 | Accuracy: 0.580078 | 7.12 sec/iter\n",
      "Epoch: 41 | Batch: 004 / 030 | Total loss: 1.768 | Reg loss: 0.044 | Tree loss: 1.768 | Accuracy: 0.615234 | 7.119 sec/iter\n",
      "Epoch: 41 | Batch: 005 / 030 | Total loss: 1.757 | Reg loss: 0.044 | Tree loss: 1.757 | Accuracy: 0.568359 | 7.12 sec/iter\n",
      "Epoch: 41 | Batch: 006 / 030 | Total loss: 1.766 | Reg loss: 0.044 | Tree loss: 1.766 | Accuracy: 0.580078 | 7.12 sec/iter\n",
      "Epoch: 41 | Batch: 007 / 030 | Total loss: 1.738 | Reg loss: 0.044 | Tree loss: 1.738 | Accuracy: 0.568359 | 7.12 sec/iter\n",
      "Epoch: 41 | Batch: 008 / 030 | Total loss: 1.719 | Reg loss: 0.045 | Tree loss: 1.719 | Accuracy: 0.560547 | 7.121 sec/iter\n",
      "Epoch: 41 | Batch: 009 / 030 | Total loss: 1.693 | Reg loss: 0.045 | Tree loss: 1.693 | Accuracy: 0.578125 | 7.121 sec/iter\n",
      "Epoch: 41 | Batch: 010 / 030 | Total loss: 1.687 | Reg loss: 0.045 | Tree loss: 1.687 | Accuracy: 0.593750 | 7.121 sec/iter\n",
      "Epoch: 41 | Batch: 011 / 030 | Total loss: 1.686 | Reg loss: 0.045 | Tree loss: 1.686 | Accuracy: 0.589844 | 7.121 sec/iter\n",
      "Epoch: 41 | Batch: 012 / 030 | Total loss: 1.665 | Reg loss: 0.045 | Tree loss: 1.665 | Accuracy: 0.574219 | 7.122 sec/iter\n",
      "Epoch: 41 | Batch: 013 / 030 | Total loss: 1.653 | Reg loss: 0.045 | Tree loss: 1.653 | Accuracy: 0.580078 | 7.122 sec/iter\n",
      "Epoch: 41 | Batch: 014 / 030 | Total loss: 1.597 | Reg loss: 0.045 | Tree loss: 1.597 | Accuracy: 0.591797 | 7.122 sec/iter\n",
      "Epoch: 41 | Batch: 015 / 030 | Total loss: 1.607 | Reg loss: 0.045 | Tree loss: 1.607 | Accuracy: 0.585938 | 7.122 sec/iter\n",
      "Epoch: 41 | Batch: 016 / 030 | Total loss: 1.603 | Reg loss: 0.045 | Tree loss: 1.603 | Accuracy: 0.576172 | 7.122 sec/iter\n",
      "Epoch: 41 | Batch: 017 / 030 | Total loss: 1.581 | Reg loss: 0.045 | Tree loss: 1.581 | Accuracy: 0.597656 | 7.122 sec/iter\n",
      "Epoch: 41 | Batch: 018 / 030 | Total loss: 1.552 | Reg loss: 0.045 | Tree loss: 1.552 | Accuracy: 0.572266 | 7.122 sec/iter\n",
      "Epoch: 41 | Batch: 019 / 030 | Total loss: 1.547 | Reg loss: 0.045 | Tree loss: 1.547 | Accuracy: 0.572266 | 7.122 sec/iter\n",
      "Epoch: 41 | Batch: 020 / 030 | Total loss: 1.526 | Reg loss: 0.045 | Tree loss: 1.526 | Accuracy: 0.609375 | 7.122 sec/iter\n",
      "Epoch: 41 | Batch: 021 / 030 | Total loss: 1.538 | Reg loss: 0.045 | Tree loss: 1.538 | Accuracy: 0.572266 | 7.122 sec/iter\n",
      "Epoch: 41 | Batch: 022 / 030 | Total loss: 1.535 | Reg loss: 0.045 | Tree loss: 1.535 | Accuracy: 0.572266 | 7.122 sec/iter\n",
      "Epoch: 41 | Batch: 023 / 030 | Total loss: 1.508 | Reg loss: 0.045 | Tree loss: 1.508 | Accuracy: 0.583984 | 7.122 sec/iter\n",
      "Epoch: 41 | Batch: 024 / 030 | Total loss: 1.489 | Reg loss: 0.045 | Tree loss: 1.489 | Accuracy: 0.626953 | 7.122 sec/iter\n",
      "Epoch: 41 | Batch: 025 / 030 | Total loss: 1.480 | Reg loss: 0.046 | Tree loss: 1.480 | Accuracy: 0.576172 | 7.122 sec/iter\n",
      "Epoch: 41 | Batch: 026 / 030 | Total loss: 1.469 | Reg loss: 0.046 | Tree loss: 1.469 | Accuracy: 0.589844 | 7.122 sec/iter\n",
      "Epoch: 41 | Batch: 027 / 030 | Total loss: 1.453 | Reg loss: 0.046 | Tree loss: 1.453 | Accuracy: 0.617188 | 7.122 sec/iter\n",
      "Epoch: 41 | Batch: 028 / 030 | Total loss: 1.450 | Reg loss: 0.046 | Tree loss: 1.450 | Accuracy: 0.570312 | 7.122 sec/iter\n",
      "Epoch: 41 | Batch: 029 / 030 | Total loss: 1.469 | Reg loss: 0.046 | Tree loss: 1.469 | Accuracy: 0.563107 | 7.119 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 42 | Batch: 000 / 030 | Total loss: 1.798 | Reg loss: 0.044 | Tree loss: 1.798 | Accuracy: 0.566406 | 7.121 sec/iter\n",
      "Epoch: 42 | Batch: 001 / 030 | Total loss: 1.796 | Reg loss: 0.044 | Tree loss: 1.796 | Accuracy: 0.550781 | 7.121 sec/iter\n",
      "Epoch: 42 | Batch: 002 / 030 | Total loss: 1.750 | Reg loss: 0.044 | Tree loss: 1.750 | Accuracy: 0.560547 | 7.122 sec/iter\n",
      "Epoch: 42 | Batch: 003 / 030 | Total loss: 1.752 | Reg loss: 0.044 | Tree loss: 1.752 | Accuracy: 0.550781 | 7.12 sec/iter\n",
      "Epoch: 42 | Batch: 004 / 030 | Total loss: 1.730 | Reg loss: 0.044 | Tree loss: 1.730 | Accuracy: 0.580078 | 7.121 sec/iter\n",
      "Epoch: 42 | Batch: 005 / 030 | Total loss: 1.710 | Reg loss: 0.044 | Tree loss: 1.710 | Accuracy: 0.562500 | 7.121 sec/iter\n",
      "Epoch: 42 | Batch: 006 / 030 | Total loss: 1.684 | Reg loss: 0.044 | Tree loss: 1.684 | Accuracy: 0.615234 | 7.121 sec/iter\n",
      "Epoch: 42 | Batch: 007 / 030 | Total loss: 1.691 | Reg loss: 0.044 | Tree loss: 1.691 | Accuracy: 0.572266 | 7.12 sec/iter\n",
      "Epoch: 42 | Batch: 008 / 030 | Total loss: 1.650 | Reg loss: 0.045 | Tree loss: 1.650 | Accuracy: 0.589844 | 7.12 sec/iter\n",
      "Epoch: 42 | Batch: 009 / 030 | Total loss: 1.636 | Reg loss: 0.045 | Tree loss: 1.636 | Accuracy: 0.589844 | 7.119 sec/iter\n",
      "Epoch: 42 | Batch: 010 / 030 | Total loss: 1.613 | Reg loss: 0.045 | Tree loss: 1.613 | Accuracy: 0.619141 | 7.119 sec/iter\n",
      "Epoch: 42 | Batch: 011 / 030 | Total loss: 1.550 | Reg loss: 0.045 | Tree loss: 1.550 | Accuracy: 0.626953 | 7.118 sec/iter\n",
      "Epoch: 42 | Batch: 012 / 030 | Total loss: 1.601 | Reg loss: 0.045 | Tree loss: 1.601 | Accuracy: 0.578125 | 7.118 sec/iter\n",
      "Epoch: 42 | Batch: 013 / 030 | Total loss: 1.597 | Reg loss: 0.045 | Tree loss: 1.597 | Accuracy: 0.548828 | 7.118 sec/iter\n",
      "Epoch: 42 | Batch: 014 / 030 | Total loss: 1.579 | Reg loss: 0.045 | Tree loss: 1.579 | Accuracy: 0.580078 | 7.118 sec/iter\n",
      "Epoch: 42 | Batch: 015 / 030 | Total loss: 1.530 | Reg loss: 0.045 | Tree loss: 1.530 | Accuracy: 0.583984 | 7.118 sec/iter\n",
      "Epoch: 42 | Batch: 016 / 030 | Total loss: 1.486 | Reg loss: 0.045 | Tree loss: 1.486 | Accuracy: 0.623047 | 7.118 sec/iter\n",
      "Epoch: 42 | Batch: 017 / 030 | Total loss: 1.521 | Reg loss: 0.045 | Tree loss: 1.521 | Accuracy: 0.593750 | 7.118 sec/iter\n",
      "Epoch: 42 | Batch: 018 / 030 | Total loss: 1.487 | Reg loss: 0.045 | Tree loss: 1.487 | Accuracy: 0.589844 | 7.118 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 42 | Batch: 019 / 030 | Total loss: 1.502 | Reg loss: 0.045 | Tree loss: 1.502 | Accuracy: 0.568359 | 7.118 sec/iter\n",
      "Epoch: 42 | Batch: 020 / 030 | Total loss: 1.475 | Reg loss: 0.045 | Tree loss: 1.475 | Accuracy: 0.623047 | 7.119 sec/iter\n",
      "Epoch: 42 | Batch: 021 / 030 | Total loss: 1.465 | Reg loss: 0.045 | Tree loss: 1.465 | Accuracy: 0.585938 | 7.119 sec/iter\n",
      "Epoch: 42 | Batch: 022 / 030 | Total loss: 1.485 | Reg loss: 0.045 | Tree loss: 1.485 | Accuracy: 0.566406 | 7.119 sec/iter\n",
      "Epoch: 42 | Batch: 023 / 030 | Total loss: 1.462 | Reg loss: 0.045 | Tree loss: 1.462 | Accuracy: 0.585938 | 7.119 sec/iter\n",
      "Epoch: 42 | Batch: 024 / 030 | Total loss: 1.433 | Reg loss: 0.045 | Tree loss: 1.433 | Accuracy: 0.560547 | 7.119 sec/iter\n",
      "Epoch: 42 | Batch: 025 / 030 | Total loss: 1.461 | Reg loss: 0.046 | Tree loss: 1.461 | Accuracy: 0.525391 | 7.119 sec/iter\n",
      "Epoch: 42 | Batch: 026 / 030 | Total loss: 1.425 | Reg loss: 0.046 | Tree loss: 1.425 | Accuracy: 0.585938 | 7.119 sec/iter\n",
      "Epoch: 42 | Batch: 027 / 030 | Total loss: 1.420 | Reg loss: 0.046 | Tree loss: 1.420 | Accuracy: 0.576172 | 7.119 sec/iter\n",
      "Epoch: 42 | Batch: 028 / 030 | Total loss: 1.388 | Reg loss: 0.046 | Tree loss: 1.388 | Accuracy: 0.630859 | 7.119 sec/iter\n",
      "Epoch: 42 | Batch: 029 / 030 | Total loss: 1.373 | Reg loss: 0.046 | Tree loss: 1.373 | Accuracy: 0.631068 | 7.115 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 43 | Batch: 000 / 030 | Total loss: 1.752 | Reg loss: 0.044 | Tree loss: 1.752 | Accuracy: 0.562500 | 7.121 sec/iter\n",
      "Epoch: 43 | Batch: 001 / 030 | Total loss: 1.718 | Reg loss: 0.044 | Tree loss: 1.718 | Accuracy: 0.574219 | 7.122 sec/iter\n",
      "Epoch: 43 | Batch: 002 / 030 | Total loss: 1.649 | Reg loss: 0.044 | Tree loss: 1.649 | Accuracy: 0.580078 | 7.12 sec/iter\n",
      "Epoch: 43 | Batch: 003 / 030 | Total loss: 1.668 | Reg loss: 0.044 | Tree loss: 1.668 | Accuracy: 0.605469 | 7.119 sec/iter\n",
      "Epoch: 43 | Batch: 004 / 030 | Total loss: 1.648 | Reg loss: 0.044 | Tree loss: 1.648 | Accuracy: 0.576172 | 7.119 sec/iter\n",
      "Epoch: 43 | Batch: 005 / 030 | Total loss: 1.669 | Reg loss: 0.044 | Tree loss: 1.669 | Accuracy: 0.535156 | 7.12 sec/iter\n",
      "Epoch: 43 | Batch: 006 / 030 | Total loss: 1.662 | Reg loss: 0.044 | Tree loss: 1.662 | Accuracy: 0.566406 | 7.12 sec/iter\n",
      "Epoch: 43 | Batch: 007 / 030 | Total loss: 1.633 | Reg loss: 0.044 | Tree loss: 1.633 | Accuracy: 0.578125 | 7.12 sec/iter\n",
      "Epoch: 43 | Batch: 008 / 030 | Total loss: 1.593 | Reg loss: 0.044 | Tree loss: 1.593 | Accuracy: 0.615234 | 7.12 sec/iter\n",
      "Epoch: 43 | Batch: 009 / 030 | Total loss: 1.574 | Reg loss: 0.045 | Tree loss: 1.574 | Accuracy: 0.619141 | 7.12 sec/iter\n",
      "Epoch: 43 | Batch: 010 / 030 | Total loss: 1.584 | Reg loss: 0.045 | Tree loss: 1.584 | Accuracy: 0.605469 | 7.12 sec/iter\n",
      "Epoch: 43 | Batch: 011 / 030 | Total loss: 1.569 | Reg loss: 0.045 | Tree loss: 1.569 | Accuracy: 0.568359 | 7.12 sec/iter\n",
      "Epoch: 43 | Batch: 012 / 030 | Total loss: 1.542 | Reg loss: 0.045 | Tree loss: 1.542 | Accuracy: 0.566406 | 7.12 sec/iter\n",
      "Epoch: 43 | Batch: 013 / 030 | Total loss: 1.531 | Reg loss: 0.045 | Tree loss: 1.531 | Accuracy: 0.578125 | 7.121 sec/iter\n",
      "Epoch: 43 | Batch: 014 / 030 | Total loss: 1.508 | Reg loss: 0.045 | Tree loss: 1.508 | Accuracy: 0.591797 | 7.121 sec/iter\n",
      "Epoch: 43 | Batch: 015 / 030 | Total loss: 1.492 | Reg loss: 0.045 | Tree loss: 1.492 | Accuracy: 0.605469 | 7.121 sec/iter\n",
      "Epoch: 43 | Batch: 016 / 030 | Total loss: 1.500 | Reg loss: 0.045 | Tree loss: 1.500 | Accuracy: 0.560547 | 7.121 sec/iter\n",
      "Epoch: 43 | Batch: 017 / 030 | Total loss: 1.495 | Reg loss: 0.045 | Tree loss: 1.495 | Accuracy: 0.591797 | 7.121 sec/iter\n",
      "Epoch: 43 | Batch: 018 / 030 | Total loss: 1.472 | Reg loss: 0.045 | Tree loss: 1.472 | Accuracy: 0.550781 | 7.121 sec/iter\n",
      "Epoch: 43 | Batch: 019 / 030 | Total loss: 1.426 | Reg loss: 0.045 | Tree loss: 1.426 | Accuracy: 0.615234 | 7.12 sec/iter\n",
      "Epoch: 43 | Batch: 020 / 030 | Total loss: 1.426 | Reg loss: 0.045 | Tree loss: 1.426 | Accuracy: 0.572266 | 7.12 sec/iter\n",
      "Epoch: 43 | Batch: 021 / 030 | Total loss: 1.429 | Reg loss: 0.045 | Tree loss: 1.429 | Accuracy: 0.570312 | 7.12 sec/iter\n",
      "Epoch: 43 | Batch: 022 / 030 | Total loss: 1.418 | Reg loss: 0.045 | Tree loss: 1.418 | Accuracy: 0.576172 | 7.12 sec/iter\n",
      "Epoch: 43 | Batch: 023 / 030 | Total loss: 1.396 | Reg loss: 0.045 | Tree loss: 1.396 | Accuracy: 0.568359 | 7.12 sec/iter\n",
      "Epoch: 43 | Batch: 024 / 030 | Total loss: 1.389 | Reg loss: 0.045 | Tree loss: 1.389 | Accuracy: 0.605469 | 7.119 sec/iter\n",
      "Epoch: 43 | Batch: 025 / 030 | Total loss: 1.396 | Reg loss: 0.046 | Tree loss: 1.396 | Accuracy: 0.572266 | 7.119 sec/iter\n",
      "Epoch: 43 | Batch: 026 / 030 | Total loss: 1.354 | Reg loss: 0.046 | Tree loss: 1.354 | Accuracy: 0.585938 | 7.117 sec/iter\n",
      "Epoch: 43 | Batch: 027 / 030 | Total loss: 1.351 | Reg loss: 0.046 | Tree loss: 1.351 | Accuracy: 0.599609 | 7.115 sec/iter\n",
      "Epoch: 43 | Batch: 028 / 030 | Total loss: 1.347 | Reg loss: 0.046 | Tree loss: 1.347 | Accuracy: 0.599609 | 7.114 sec/iter\n",
      "Epoch: 43 | Batch: 029 / 030 | Total loss: 1.322 | Reg loss: 0.046 | Tree loss: 1.322 | Accuracy: 0.601942 | 7.111 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 44 | Batch: 000 / 030 | Total loss: 1.669 | Reg loss: 0.044 | Tree loss: 1.669 | Accuracy: 0.597656 | 7.119 sec/iter\n",
      "Epoch: 44 | Batch: 001 / 030 | Total loss: 1.695 | Reg loss: 0.044 | Tree loss: 1.695 | Accuracy: 0.589844 | 7.117 sec/iter\n",
      "Epoch: 44 | Batch: 002 / 030 | Total loss: 1.651 | Reg loss: 0.044 | Tree loss: 1.651 | Accuracy: 0.613281 | 7.116 sec/iter\n",
      "Epoch: 44 | Batch: 003 / 030 | Total loss: 1.625 | Reg loss: 0.044 | Tree loss: 1.625 | Accuracy: 0.607422 | 7.115 sec/iter\n",
      "Epoch: 44 | Batch: 004 / 030 | Total loss: 1.629 | Reg loss: 0.044 | Tree loss: 1.629 | Accuracy: 0.587891 | 7.115 sec/iter\n",
      "Epoch: 44 | Batch: 005 / 030 | Total loss: 1.625 | Reg loss: 0.044 | Tree loss: 1.625 | Accuracy: 0.578125 | 7.114 sec/iter\n",
      "Epoch: 44 | Batch: 006 / 030 | Total loss: 1.607 | Reg loss: 0.044 | Tree loss: 1.607 | Accuracy: 0.560547 | 7.114 sec/iter\n",
      "Epoch: 44 | Batch: 007 / 030 | Total loss: 1.568 | Reg loss: 0.044 | Tree loss: 1.568 | Accuracy: 0.576172 | 7.113 sec/iter\n",
      "Epoch: 44 | Batch: 008 / 030 | Total loss: 1.554 | Reg loss: 0.044 | Tree loss: 1.554 | Accuracy: 0.558594 | 7.113 sec/iter\n",
      "Epoch: 44 | Batch: 009 / 030 | Total loss: 1.534 | Reg loss: 0.044 | Tree loss: 1.534 | Accuracy: 0.605469 | 7.113 sec/iter\n",
      "Epoch: 44 | Batch: 010 / 030 | Total loss: 1.532 | Reg loss: 0.044 | Tree loss: 1.532 | Accuracy: 0.566406 | 7.113 sec/iter\n",
      "Epoch: 44 | Batch: 011 / 030 | Total loss: 1.480 | Reg loss: 0.045 | Tree loss: 1.480 | Accuracy: 0.625000 | 7.113 sec/iter\n",
      "Epoch: 44 | Batch: 012 / 030 | Total loss: 1.476 | Reg loss: 0.045 | Tree loss: 1.476 | Accuracy: 0.583984 | 7.113 sec/iter\n",
      "Epoch: 44 | Batch: 013 / 030 | Total loss: 1.485 | Reg loss: 0.045 | Tree loss: 1.485 | Accuracy: 0.587891 | 7.113 sec/iter\n",
      "Epoch: 44 | Batch: 014 / 030 | Total loss: 1.478 | Reg loss: 0.045 | Tree loss: 1.478 | Accuracy: 0.583984 | 7.113 sec/iter\n",
      "Epoch: 44 | Batch: 015 / 030 | Total loss: 1.428 | Reg loss: 0.045 | Tree loss: 1.428 | Accuracy: 0.636719 | 7.114 sec/iter\n",
      "Epoch: 44 | Batch: 016 / 030 | Total loss: 1.439 | Reg loss: 0.045 | Tree loss: 1.439 | Accuracy: 0.576172 | 7.114 sec/iter\n",
      "Epoch: 44 | Batch: 017 / 030 | Total loss: 1.402 | Reg loss: 0.045 | Tree loss: 1.402 | Accuracy: 0.607422 | 7.114 sec/iter\n",
      "Epoch: 44 | Batch: 018 / 030 | Total loss: 1.399 | Reg loss: 0.045 | Tree loss: 1.399 | Accuracy: 0.583984 | 7.114 sec/iter\n",
      "Epoch: 44 | Batch: 019 / 030 | Total loss: 1.427 | Reg loss: 0.045 | Tree loss: 1.427 | Accuracy: 0.554688 | 7.114 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 44 | Batch: 020 / 030 | Total loss: 1.406 | Reg loss: 0.045 | Tree loss: 1.406 | Accuracy: 0.562500 | 7.114 sec/iter\n",
      "Epoch: 44 | Batch: 021 / 030 | Total loss: 1.378 | Reg loss: 0.045 | Tree loss: 1.378 | Accuracy: 0.582031 | 7.114 sec/iter\n",
      "Epoch: 44 | Batch: 022 / 030 | Total loss: 1.365 | Reg loss: 0.045 | Tree loss: 1.365 | Accuracy: 0.568359 | 7.114 sec/iter\n",
      "Epoch: 44 | Batch: 023 / 030 | Total loss: 1.332 | Reg loss: 0.045 | Tree loss: 1.332 | Accuracy: 0.630859 | 7.114 sec/iter\n",
      "Epoch: 44 | Batch: 024 / 030 | Total loss: 1.347 | Reg loss: 0.045 | Tree loss: 1.347 | Accuracy: 0.566406 | 7.113 sec/iter\n",
      "Epoch: 44 | Batch: 025 / 030 | Total loss: 1.333 | Reg loss: 0.045 | Tree loss: 1.333 | Accuracy: 0.582031 | 7.113 sec/iter\n",
      "Epoch: 44 | Batch: 026 / 030 | Total loss: 1.340 | Reg loss: 0.045 | Tree loss: 1.340 | Accuracy: 0.566406 | 7.112 sec/iter\n",
      "Epoch: 44 | Batch: 027 / 030 | Total loss: 1.328 | Reg loss: 0.046 | Tree loss: 1.328 | Accuracy: 0.537109 | 7.11 sec/iter\n",
      "Epoch: 44 | Batch: 028 / 030 | Total loss: 1.312 | Reg loss: 0.046 | Tree loss: 1.312 | Accuracy: 0.539062 | 7.108 sec/iter\n",
      "Epoch: 44 | Batch: 029 / 030 | Total loss: 1.367 | Reg loss: 0.046 | Tree loss: 1.367 | Accuracy: 0.504854 | 7.105 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 45 | Batch: 000 / 030 | Total loss: 1.609 | Reg loss: 0.044 | Tree loss: 1.609 | Accuracy: 0.613281 | 7.113 sec/iter\n",
      "Epoch: 45 | Batch: 001 / 030 | Total loss: 1.640 | Reg loss: 0.044 | Tree loss: 1.640 | Accuracy: 0.578125 | 7.112 sec/iter\n",
      "Epoch: 45 | Batch: 002 / 030 | Total loss: 1.596 | Reg loss: 0.044 | Tree loss: 1.596 | Accuracy: 0.582031 | 7.112 sec/iter\n",
      "Epoch: 45 | Batch: 003 / 030 | Total loss: 1.600 | Reg loss: 0.044 | Tree loss: 1.600 | Accuracy: 0.556641 | 7.112 sec/iter\n",
      "Epoch: 45 | Batch: 004 / 030 | Total loss: 1.564 | Reg loss: 0.044 | Tree loss: 1.564 | Accuracy: 0.607422 | 7.112 sec/iter\n",
      "Epoch: 45 | Batch: 005 / 030 | Total loss: 1.556 | Reg loss: 0.044 | Tree loss: 1.556 | Accuracy: 0.578125 | 7.111 sec/iter\n",
      "Epoch: 45 | Batch: 006 / 030 | Total loss: 1.575 | Reg loss: 0.044 | Tree loss: 1.575 | Accuracy: 0.562500 | 7.111 sec/iter\n",
      "Epoch: 45 | Batch: 007 / 030 | Total loss: 1.514 | Reg loss: 0.044 | Tree loss: 1.514 | Accuracy: 0.570312 | 7.11 sec/iter\n",
      "Epoch: 45 | Batch: 008 / 030 | Total loss: 1.509 | Reg loss: 0.044 | Tree loss: 1.509 | Accuracy: 0.597656 | 7.11 sec/iter\n",
      "Epoch: 45 | Batch: 009 / 030 | Total loss: 1.505 | Reg loss: 0.044 | Tree loss: 1.505 | Accuracy: 0.607422 | 7.11 sec/iter\n",
      "Epoch: 45 | Batch: 010 / 030 | Total loss: 1.481 | Reg loss: 0.044 | Tree loss: 1.481 | Accuracy: 0.582031 | 7.109 sec/iter\n",
      "Epoch: 45 | Batch: 011 / 030 | Total loss: 1.482 | Reg loss: 0.044 | Tree loss: 1.482 | Accuracy: 0.601562 | 7.109 sec/iter\n",
      "Epoch: 45 | Batch: 012 / 030 | Total loss: 1.468 | Reg loss: 0.044 | Tree loss: 1.468 | Accuracy: 0.607422 | 7.11 sec/iter\n",
      "Epoch: 45 | Batch: 013 / 030 | Total loss: 1.458 | Reg loss: 0.045 | Tree loss: 1.458 | Accuracy: 0.556641 | 7.11 sec/iter\n",
      "Epoch: 45 | Batch: 014 / 030 | Total loss: 1.403 | Reg loss: 0.045 | Tree loss: 1.403 | Accuracy: 0.597656 | 7.11 sec/iter\n",
      "Epoch: 45 | Batch: 015 / 030 | Total loss: 1.386 | Reg loss: 0.045 | Tree loss: 1.386 | Accuracy: 0.628906 | 7.11 sec/iter\n",
      "Epoch: 45 | Batch: 016 / 030 | Total loss: 1.406 | Reg loss: 0.045 | Tree loss: 1.406 | Accuracy: 0.572266 | 7.11 sec/iter\n",
      "Epoch: 45 | Batch: 017 / 030 | Total loss: 1.420 | Reg loss: 0.045 | Tree loss: 1.420 | Accuracy: 0.546875 | 7.11 sec/iter\n",
      "Epoch: 45 | Batch: 018 / 030 | Total loss: 1.387 | Reg loss: 0.045 | Tree loss: 1.387 | Accuracy: 0.560547 | 7.11 sec/iter\n",
      "Epoch: 45 | Batch: 019 / 030 | Total loss: 1.367 | Reg loss: 0.045 | Tree loss: 1.367 | Accuracy: 0.574219 | 7.11 sec/iter\n",
      "Epoch: 45 | Batch: 020 / 030 | Total loss: 1.356 | Reg loss: 0.045 | Tree loss: 1.356 | Accuracy: 0.564453 | 7.11 sec/iter\n",
      "Epoch: 45 | Batch: 021 / 030 | Total loss: 1.359 | Reg loss: 0.045 | Tree loss: 1.359 | Accuracy: 0.562500 | 7.11 sec/iter\n",
      "Epoch: 45 | Batch: 022 / 030 | Total loss: 1.340 | Reg loss: 0.045 | Tree loss: 1.340 | Accuracy: 0.572266 | 7.11 sec/iter\n",
      "Epoch: 45 | Batch: 023 / 030 | Total loss: 1.287 | Reg loss: 0.045 | Tree loss: 1.287 | Accuracy: 0.595703 | 7.11 sec/iter\n",
      "Epoch: 45 | Batch: 024 / 030 | Total loss: 1.285 | Reg loss: 0.045 | Tree loss: 1.285 | Accuracy: 0.636719 | 7.11 sec/iter\n",
      "Epoch: 45 | Batch: 025 / 030 | Total loss: 1.330 | Reg loss: 0.045 | Tree loss: 1.330 | Accuracy: 0.503906 | 7.11 sec/iter\n",
      "Epoch: 45 | Batch: 026 / 030 | Total loss: 1.281 | Reg loss: 0.045 | Tree loss: 1.281 | Accuracy: 0.574219 | 7.109 sec/iter\n",
      "Epoch: 45 | Batch: 027 / 030 | Total loss: 1.270 | Reg loss: 0.045 | Tree loss: 1.270 | Accuracy: 0.597656 | 7.107 sec/iter\n",
      "Epoch: 45 | Batch: 028 / 030 | Total loss: 1.247 | Reg loss: 0.045 | Tree loss: 1.247 | Accuracy: 0.595703 | 7.107 sec/iter\n",
      "Epoch: 45 | Batch: 029 / 030 | Total loss: 1.233 | Reg loss: 0.046 | Tree loss: 1.233 | Accuracy: 0.660194 | 7.104 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 46 | Batch: 000 / 030 | Total loss: 1.625 | Reg loss: 0.044 | Tree loss: 1.625 | Accuracy: 0.558594 | 7.11 sec/iter\n",
      "Epoch: 46 | Batch: 001 / 030 | Total loss: 1.602 | Reg loss: 0.044 | Tree loss: 1.602 | Accuracy: 0.580078 | 7.109 sec/iter\n",
      "Epoch: 46 | Batch: 002 / 030 | Total loss: 1.545 | Reg loss: 0.044 | Tree loss: 1.545 | Accuracy: 0.613281 | 7.109 sec/iter\n",
      "Epoch: 46 | Batch: 003 / 030 | Total loss: 1.569 | Reg loss: 0.044 | Tree loss: 1.569 | Accuracy: 0.597656 | 7.109 sec/iter\n",
      "Epoch: 46 | Batch: 004 / 030 | Total loss: 1.536 | Reg loss: 0.044 | Tree loss: 1.536 | Accuracy: 0.546875 | 7.109 sec/iter\n",
      "Epoch: 46 | Batch: 005 / 030 | Total loss: 1.542 | Reg loss: 0.044 | Tree loss: 1.542 | Accuracy: 0.560547 | 7.109 sec/iter\n",
      "Epoch: 46 | Batch: 006 / 030 | Total loss: 1.522 | Reg loss: 0.044 | Tree loss: 1.522 | Accuracy: 0.587891 | 7.109 sec/iter\n",
      "Epoch: 46 | Batch: 007 / 030 | Total loss: 1.480 | Reg loss: 0.044 | Tree loss: 1.480 | Accuracy: 0.585938 | 7.11 sec/iter\n",
      "Epoch: 46 | Batch: 008 / 030 | Total loss: 1.454 | Reg loss: 0.044 | Tree loss: 1.454 | Accuracy: 0.601562 | 7.11 sec/iter\n",
      "Epoch: 46 | Batch: 009 / 030 | Total loss: 1.450 | Reg loss: 0.044 | Tree loss: 1.450 | Accuracy: 0.609375 | 7.11 sec/iter\n",
      "Epoch: 46 | Batch: 010 / 030 | Total loss: 1.443 | Reg loss: 0.044 | Tree loss: 1.443 | Accuracy: 0.593750 | 7.11 sec/iter\n",
      "Epoch: 46 | Batch: 011 / 030 | Total loss: 1.443 | Reg loss: 0.044 | Tree loss: 1.443 | Accuracy: 0.568359 | 7.11 sec/iter\n",
      "Epoch: 46 | Batch: 012 / 030 | Total loss: 1.443 | Reg loss: 0.044 | Tree loss: 1.443 | Accuracy: 0.554688 | 7.111 sec/iter\n",
      "Epoch: 46 | Batch: 013 / 030 | Total loss: 1.396 | Reg loss: 0.044 | Tree loss: 1.396 | Accuracy: 0.580078 | 7.111 sec/iter\n",
      "Epoch: 46 | Batch: 014 / 030 | Total loss: 1.380 | Reg loss: 0.044 | Tree loss: 1.380 | Accuracy: 0.593750 | 7.111 sec/iter\n",
      "Epoch: 46 | Batch: 015 / 030 | Total loss: 1.368 | Reg loss: 0.044 | Tree loss: 1.368 | Accuracy: 0.599609 | 7.111 sec/iter\n",
      "Epoch: 46 | Batch: 016 / 030 | Total loss: 1.339 | Reg loss: 0.045 | Tree loss: 1.339 | Accuracy: 0.605469 | 7.111 sec/iter\n",
      "Epoch: 46 | Batch: 017 / 030 | Total loss: 1.370 | Reg loss: 0.045 | Tree loss: 1.370 | Accuracy: 0.599609 | 7.111 sec/iter\n",
      "Epoch: 46 | Batch: 018 / 030 | Total loss: 1.362 | Reg loss: 0.045 | Tree loss: 1.362 | Accuracy: 0.525391 | 7.111 sec/iter\n",
      "Epoch: 46 | Batch: 019 / 030 | Total loss: 1.315 | Reg loss: 0.045 | Tree loss: 1.315 | Accuracy: 0.617188 | 7.11 sec/iter\n",
      "Epoch: 46 | Batch: 020 / 030 | Total loss: 1.320 | Reg loss: 0.045 | Tree loss: 1.320 | Accuracy: 0.583984 | 7.11 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 46 | Batch: 021 / 030 | Total loss: 1.285 | Reg loss: 0.045 | Tree loss: 1.285 | Accuracy: 0.589844 | 7.11 sec/iter\n",
      "Epoch: 46 | Batch: 022 / 030 | Total loss: 1.287 | Reg loss: 0.045 | Tree loss: 1.287 | Accuracy: 0.568359 | 7.11 sec/iter\n",
      "Epoch: 46 | Batch: 023 / 030 | Total loss: 1.288 | Reg loss: 0.045 | Tree loss: 1.288 | Accuracy: 0.583984 | 7.109 sec/iter\n",
      "Epoch: 46 | Batch: 024 / 030 | Total loss: 1.266 | Reg loss: 0.045 | Tree loss: 1.266 | Accuracy: 0.564453 | 7.107 sec/iter\n",
      "Epoch: 46 | Batch: 025 / 030 | Total loss: 1.262 | Reg loss: 0.045 | Tree loss: 1.262 | Accuracy: 0.580078 | 7.106 sec/iter\n",
      "Epoch: 46 | Batch: 026 / 030 | Total loss: 1.251 | Reg loss: 0.045 | Tree loss: 1.251 | Accuracy: 0.585938 | 7.104 sec/iter\n",
      "Epoch: 46 | Batch: 027 / 030 | Total loss: 1.246 | Reg loss: 0.045 | Tree loss: 1.246 | Accuracy: 0.572266 | 7.102 sec/iter\n",
      "Epoch: 46 | Batch: 028 / 030 | Total loss: 1.248 | Reg loss: 0.045 | Tree loss: 1.248 | Accuracy: 0.578125 | 7.101 sec/iter\n",
      "Epoch: 46 | Batch: 029 / 030 | Total loss: 1.189 | Reg loss: 0.045 | Tree loss: 1.189 | Accuracy: 0.650485 | 7.098 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 47 | Batch: 000 / 030 | Total loss: 1.576 | Reg loss: 0.044 | Tree loss: 1.576 | Accuracy: 0.562500 | 7.101 sec/iter\n",
      "Epoch: 47 | Batch: 001 / 030 | Total loss: 1.524 | Reg loss: 0.044 | Tree loss: 1.524 | Accuracy: 0.589844 | 7.1 sec/iter\n",
      "Epoch: 47 | Batch: 002 / 030 | Total loss: 1.539 | Reg loss: 0.044 | Tree loss: 1.539 | Accuracy: 0.582031 | 7.1 sec/iter\n",
      "Epoch: 47 | Batch: 003 / 030 | Total loss: 1.567 | Reg loss: 0.044 | Tree loss: 1.567 | Accuracy: 0.537109 | 7.1 sec/iter\n",
      "Epoch: 47 | Batch: 004 / 030 | Total loss: 1.538 | Reg loss: 0.044 | Tree loss: 1.538 | Accuracy: 0.546875 | 7.1 sec/iter\n",
      "Epoch: 47 | Batch: 005 / 030 | Total loss: 1.484 | Reg loss: 0.044 | Tree loss: 1.484 | Accuracy: 0.591797 | 7.1 sec/iter\n",
      "Epoch: 47 | Batch: 006 / 030 | Total loss: 1.491 | Reg loss: 0.044 | Tree loss: 1.491 | Accuracy: 0.560547 | 7.1 sec/iter\n",
      "Epoch: 47 | Batch: 007 / 030 | Total loss: 1.463 | Reg loss: 0.044 | Tree loss: 1.463 | Accuracy: 0.583984 | 7.1 sec/iter\n",
      "Epoch: 47 | Batch: 008 / 030 | Total loss: 1.460 | Reg loss: 0.044 | Tree loss: 1.460 | Accuracy: 0.587891 | 7.101 sec/iter\n",
      "Epoch: 47 | Batch: 009 / 030 | Total loss: 1.448 | Reg loss: 0.044 | Tree loss: 1.448 | Accuracy: 0.576172 | 7.101 sec/iter\n",
      "Epoch: 47 | Batch: 010 / 030 | Total loss: 1.420 | Reg loss: 0.044 | Tree loss: 1.420 | Accuracy: 0.607422 | 7.101 sec/iter\n",
      "Epoch: 47 | Batch: 011 / 030 | Total loss: 1.430 | Reg loss: 0.044 | Tree loss: 1.430 | Accuracy: 0.574219 | 7.101 sec/iter\n",
      "Epoch: 47 | Batch: 012 / 030 | Total loss: 1.399 | Reg loss: 0.044 | Tree loss: 1.399 | Accuracy: 0.558594 | 7.101 sec/iter\n",
      "Epoch: 47 | Batch: 013 / 030 | Total loss: 1.370 | Reg loss: 0.044 | Tree loss: 1.370 | Accuracy: 0.591797 | 7.101 sec/iter\n",
      "Epoch: 47 | Batch: 014 / 030 | Total loss: 1.347 | Reg loss: 0.044 | Tree loss: 1.347 | Accuracy: 0.595703 | 7.101 sec/iter\n",
      "Epoch: 47 | Batch: 015 / 030 | Total loss: 1.306 | Reg loss: 0.044 | Tree loss: 1.306 | Accuracy: 0.593750 | 7.101 sec/iter\n",
      "Epoch: 47 | Batch: 016 / 030 | Total loss: 1.336 | Reg loss: 0.044 | Tree loss: 1.336 | Accuracy: 0.568359 | 7.101 sec/iter\n",
      "Epoch: 47 | Batch: 017 / 030 | Total loss: 1.304 | Reg loss: 0.044 | Tree loss: 1.304 | Accuracy: 0.621094 | 7.101 sec/iter\n",
      "Epoch: 47 | Batch: 018 / 030 | Total loss: 1.281 | Reg loss: 0.044 | Tree loss: 1.281 | Accuracy: 0.599609 | 7.101 sec/iter\n",
      "Epoch: 47 | Batch: 019 / 030 | Total loss: 1.275 | Reg loss: 0.045 | Tree loss: 1.275 | Accuracy: 0.585938 | 7.101 sec/iter\n",
      "Epoch: 47 | Batch: 020 / 030 | Total loss: 1.274 | Reg loss: 0.045 | Tree loss: 1.274 | Accuracy: 0.607422 | 7.101 sec/iter\n",
      "Epoch: 47 | Batch: 021 / 030 | Total loss: 1.289 | Reg loss: 0.045 | Tree loss: 1.289 | Accuracy: 0.568359 | 7.101 sec/iter\n",
      "Epoch: 47 | Batch: 022 / 030 | Total loss: 1.247 | Reg loss: 0.045 | Tree loss: 1.247 | Accuracy: 0.591797 | 7.101 sec/iter\n",
      "Epoch: 47 | Batch: 023 / 030 | Total loss: 1.254 | Reg loss: 0.045 | Tree loss: 1.254 | Accuracy: 0.582031 | 7.101 sec/iter\n",
      "Epoch: 47 | Batch: 024 / 030 | Total loss: 1.235 | Reg loss: 0.045 | Tree loss: 1.235 | Accuracy: 0.572266 | 7.101 sec/iter\n",
      "Epoch: 47 | Batch: 025 / 030 | Total loss: 1.224 | Reg loss: 0.045 | Tree loss: 1.224 | Accuracy: 0.603516 | 7.1 sec/iter\n",
      "Epoch: 47 | Batch: 026 / 030 | Total loss: 1.227 | Reg loss: 0.045 | Tree loss: 1.227 | Accuracy: 0.560547 | 7.098 sec/iter\n",
      "Epoch: 47 | Batch: 027 / 030 | Total loss: 1.184 | Reg loss: 0.045 | Tree loss: 1.184 | Accuracy: 0.625000 | 7.097 sec/iter\n",
      "Epoch: 47 | Batch: 028 / 030 | Total loss: 1.213 | Reg loss: 0.045 | Tree loss: 1.213 | Accuracy: 0.587891 | 7.095 sec/iter\n",
      "Epoch: 47 | Batch: 029 / 030 | Total loss: 1.183 | Reg loss: 0.045 | Tree loss: 1.183 | Accuracy: 0.514563 | 7.092 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 48 | Batch: 000 / 030 | Total loss: 1.536 | Reg loss: 0.044 | Tree loss: 1.536 | Accuracy: 0.587891 | 7.095 sec/iter\n",
      "Epoch: 48 | Batch: 001 / 030 | Total loss: 1.512 | Reg loss: 0.044 | Tree loss: 1.512 | Accuracy: 0.585938 | 7.096 sec/iter\n",
      "Epoch: 48 | Batch: 002 / 030 | Total loss: 1.525 | Reg loss: 0.044 | Tree loss: 1.525 | Accuracy: 0.583984 | 7.096 sec/iter\n",
      "Epoch: 48 | Batch: 003 / 030 | Total loss: 1.495 | Reg loss: 0.044 | Tree loss: 1.495 | Accuracy: 0.583984 | 7.096 sec/iter\n",
      "Epoch: 48 | Batch: 004 / 030 | Total loss: 1.501 | Reg loss: 0.044 | Tree loss: 1.501 | Accuracy: 0.599609 | 7.096 sec/iter\n",
      "Epoch: 48 | Batch: 005 / 030 | Total loss: 1.485 | Reg loss: 0.044 | Tree loss: 1.485 | Accuracy: 0.554688 | 7.096 sec/iter\n",
      "Epoch: 48 | Batch: 006 / 030 | Total loss: 1.449 | Reg loss: 0.044 | Tree loss: 1.449 | Accuracy: 0.560547 | 7.096 sec/iter\n",
      "Epoch: 48 | Batch: 007 / 030 | Total loss: 1.422 | Reg loss: 0.044 | Tree loss: 1.422 | Accuracy: 0.597656 | 7.096 sec/iter\n",
      "Epoch: 48 | Batch: 008 / 030 | Total loss: 1.411 | Reg loss: 0.044 | Tree loss: 1.411 | Accuracy: 0.576172 | 7.096 sec/iter\n",
      "Epoch: 48 | Batch: 009 / 030 | Total loss: 1.392 | Reg loss: 0.044 | Tree loss: 1.392 | Accuracy: 0.605469 | 7.096 sec/iter\n",
      "Epoch: 48 | Batch: 010 / 030 | Total loss: 1.386 | Reg loss: 0.044 | Tree loss: 1.386 | Accuracy: 0.593750 | 7.096 sec/iter\n",
      "Epoch: 48 | Batch: 011 / 030 | Total loss: 1.370 | Reg loss: 0.044 | Tree loss: 1.370 | Accuracy: 0.585938 | 7.096 sec/iter\n",
      "Epoch: 48 | Batch: 012 / 030 | Total loss: 1.356 | Reg loss: 0.044 | Tree loss: 1.356 | Accuracy: 0.582031 | 7.095 sec/iter\n",
      "Epoch: 48 | Batch: 013 / 030 | Total loss: 1.358 | Reg loss: 0.044 | Tree loss: 1.358 | Accuracy: 0.580078 | 7.095 sec/iter\n",
      "Epoch: 48 | Batch: 014 / 030 | Total loss: 1.327 | Reg loss: 0.044 | Tree loss: 1.327 | Accuracy: 0.570312 | 7.095 sec/iter\n",
      "Epoch: 48 | Batch: 015 / 030 | Total loss: 1.328 | Reg loss: 0.044 | Tree loss: 1.328 | Accuracy: 0.583984 | 7.094 sec/iter\n",
      "Epoch: 48 | Batch: 016 / 030 | Total loss: 1.303 | Reg loss: 0.044 | Tree loss: 1.303 | Accuracy: 0.566406 | 7.094 sec/iter\n",
      "Epoch: 48 | Batch: 017 / 030 | Total loss: 1.288 | Reg loss: 0.044 | Tree loss: 1.288 | Accuracy: 0.578125 | 7.094 sec/iter\n",
      "Epoch: 48 | Batch: 018 / 030 | Total loss: 1.270 | Reg loss: 0.044 | Tree loss: 1.270 | Accuracy: 0.595703 | 7.094 sec/iter\n",
      "Epoch: 48 | Batch: 019 / 030 | Total loss: 1.267 | Reg loss: 0.044 | Tree loss: 1.267 | Accuracy: 0.576172 | 7.094 sec/iter\n",
      "Epoch: 48 | Batch: 020 / 030 | Total loss: 1.236 | Reg loss: 0.044 | Tree loss: 1.236 | Accuracy: 0.578125 | 7.094 sec/iter\n",
      "Epoch: 48 | Batch: 021 / 030 | Total loss: 1.260 | Reg loss: 0.044 | Tree loss: 1.260 | Accuracy: 0.539062 | 7.094 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 48 | Batch: 022 / 030 | Total loss: 1.250 | Reg loss: 0.045 | Tree loss: 1.250 | Accuracy: 0.562500 | 7.094 sec/iter\n",
      "Epoch: 48 | Batch: 023 / 030 | Total loss: 1.191 | Reg loss: 0.045 | Tree loss: 1.191 | Accuracy: 0.632812 | 7.094 sec/iter\n",
      "Epoch: 48 | Batch: 024 / 030 | Total loss: 1.201 | Reg loss: 0.045 | Tree loss: 1.201 | Accuracy: 0.605469 | 7.094 sec/iter\n",
      "Epoch: 48 | Batch: 025 / 030 | Total loss: 1.225 | Reg loss: 0.045 | Tree loss: 1.225 | Accuracy: 0.574219 | 7.094 sec/iter\n",
      "Epoch: 48 | Batch: 026 / 030 | Total loss: 1.205 | Reg loss: 0.045 | Tree loss: 1.205 | Accuracy: 0.570312 | 7.093 sec/iter\n",
      "Epoch: 48 | Batch: 027 / 030 | Total loss: 1.175 | Reg loss: 0.045 | Tree loss: 1.175 | Accuracy: 0.591797 | 7.092 sec/iter\n",
      "Epoch: 48 | Batch: 028 / 030 | Total loss: 1.169 | Reg loss: 0.045 | Tree loss: 1.169 | Accuracy: 0.587891 | 7.09 sec/iter\n",
      "Epoch: 48 | Batch: 029 / 030 | Total loss: 1.119 | Reg loss: 0.045 | Tree loss: 1.119 | Accuracy: 0.631068 | 7.087 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 49 | Batch: 000 / 030 | Total loss: 1.494 | Reg loss: 0.043 | Tree loss: 1.494 | Accuracy: 0.568359 | 7.093 sec/iter\n",
      "Epoch: 49 | Batch: 001 / 030 | Total loss: 1.494 | Reg loss: 0.043 | Tree loss: 1.494 | Accuracy: 0.585938 | 7.093 sec/iter\n",
      "Epoch: 49 | Batch: 002 / 030 | Total loss: 1.464 | Reg loss: 0.043 | Tree loss: 1.464 | Accuracy: 0.597656 | 7.093 sec/iter\n",
      "Epoch: 49 | Batch: 003 / 030 | Total loss: 1.479 | Reg loss: 0.043 | Tree loss: 1.479 | Accuracy: 0.585938 | 7.093 sec/iter\n",
      "Epoch: 49 | Batch: 004 / 030 | Total loss: 1.485 | Reg loss: 0.043 | Tree loss: 1.485 | Accuracy: 0.550781 | 7.092 sec/iter\n",
      "Epoch: 49 | Batch: 005 / 030 | Total loss: 1.442 | Reg loss: 0.043 | Tree loss: 1.442 | Accuracy: 0.585938 | 7.092 sec/iter\n",
      "Epoch: 49 | Batch: 006 / 030 | Total loss: 1.409 | Reg loss: 0.043 | Tree loss: 1.409 | Accuracy: 0.613281 | 7.091 sec/iter\n",
      "Epoch: 49 | Batch: 007 / 030 | Total loss: 1.426 | Reg loss: 0.043 | Tree loss: 1.426 | Accuracy: 0.562500 | 7.091 sec/iter\n",
      "Epoch: 49 | Batch: 008 / 030 | Total loss: 1.400 | Reg loss: 0.043 | Tree loss: 1.400 | Accuracy: 0.570312 | 7.091 sec/iter\n",
      "Epoch: 49 | Batch: 009 / 030 | Total loss: 1.379 | Reg loss: 0.043 | Tree loss: 1.379 | Accuracy: 0.585938 | 7.091 sec/iter\n",
      "Epoch: 49 | Batch: 010 / 030 | Total loss: 1.375 | Reg loss: 0.044 | Tree loss: 1.375 | Accuracy: 0.599609 | 7.091 sec/iter\n",
      "Epoch: 49 | Batch: 011 / 030 | Total loss: 1.347 | Reg loss: 0.044 | Tree loss: 1.347 | Accuracy: 0.558594 | 7.091 sec/iter\n",
      "Epoch: 49 | Batch: 012 / 030 | Total loss: 1.311 | Reg loss: 0.044 | Tree loss: 1.311 | Accuracy: 0.589844 | 7.091 sec/iter\n",
      "Epoch: 49 | Batch: 013 / 030 | Total loss: 1.329 | Reg loss: 0.044 | Tree loss: 1.329 | Accuracy: 0.589844 | 7.091 sec/iter\n",
      "Epoch: 49 | Batch: 014 / 030 | Total loss: 1.305 | Reg loss: 0.044 | Tree loss: 1.305 | Accuracy: 0.607422 | 7.091 sec/iter\n",
      "Epoch: 49 | Batch: 015 / 030 | Total loss: 1.312 | Reg loss: 0.044 | Tree loss: 1.312 | Accuracy: 0.533203 | 7.091 sec/iter\n",
      "Epoch: 49 | Batch: 016 / 030 | Total loss: 1.290 | Reg loss: 0.044 | Tree loss: 1.290 | Accuracy: 0.576172 | 7.091 sec/iter\n",
      "Epoch: 49 | Batch: 017 / 030 | Total loss: 1.248 | Reg loss: 0.044 | Tree loss: 1.248 | Accuracy: 0.605469 | 7.091 sec/iter\n",
      "Epoch: 49 | Batch: 018 / 030 | Total loss: 1.257 | Reg loss: 0.044 | Tree loss: 1.257 | Accuracy: 0.562500 | 7.091 sec/iter\n",
      "Epoch: 49 | Batch: 019 / 030 | Total loss: 1.215 | Reg loss: 0.044 | Tree loss: 1.215 | Accuracy: 0.583984 | 7.091 sec/iter\n",
      "Epoch: 49 | Batch: 020 / 030 | Total loss: 1.211 | Reg loss: 0.044 | Tree loss: 1.211 | Accuracy: 0.582031 | 7.091 sec/iter\n",
      "Epoch: 49 | Batch: 021 / 030 | Total loss: 1.231 | Reg loss: 0.044 | Tree loss: 1.231 | Accuracy: 0.585938 | 7.091 sec/iter\n",
      "Epoch: 49 | Batch: 022 / 030 | Total loss: 1.194 | Reg loss: 0.044 | Tree loss: 1.194 | Accuracy: 0.589844 | 7.091 sec/iter\n",
      "Epoch: 49 | Batch: 023 / 030 | Total loss: 1.186 | Reg loss: 0.044 | Tree loss: 1.186 | Accuracy: 0.601562 | 7.091 sec/iter\n",
      "Epoch: 49 | Batch: 024 / 030 | Total loss: 1.207 | Reg loss: 0.044 | Tree loss: 1.207 | Accuracy: 0.572266 | 7.091 sec/iter\n",
      "Epoch: 49 | Batch: 025 / 030 | Total loss: 1.179 | Reg loss: 0.045 | Tree loss: 1.179 | Accuracy: 0.580078 | 7.091 sec/iter\n",
      "Epoch: 49 | Batch: 026 / 030 | Total loss: 1.159 | Reg loss: 0.045 | Tree loss: 1.159 | Accuracy: 0.593750 | 7.09 sec/iter\n",
      "Epoch: 49 | Batch: 027 / 030 | Total loss: 1.136 | Reg loss: 0.045 | Tree loss: 1.136 | Accuracy: 0.607422 | 7.089 sec/iter\n",
      "Epoch: 49 | Batch: 028 / 030 | Total loss: 1.164 | Reg loss: 0.045 | Tree loss: 1.164 | Accuracy: 0.568359 | 7.088 sec/iter\n",
      "Epoch: 49 | Batch: 029 / 030 | Total loss: 1.136 | Reg loss: 0.045 | Tree loss: 1.136 | Accuracy: 0.611650 | 7.086 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 50 | Batch: 000 / 030 | Total loss: 1.436 | Reg loss: 0.043 | Tree loss: 1.436 | Accuracy: 0.593750 | 7.091 sec/iter\n",
      "Epoch: 50 | Batch: 001 / 030 | Total loss: 1.464 | Reg loss: 0.043 | Tree loss: 1.464 | Accuracy: 0.601562 | 7.091 sec/iter\n",
      "Epoch: 50 | Batch: 002 / 030 | Total loss: 1.461 | Reg loss: 0.043 | Tree loss: 1.461 | Accuracy: 0.587891 | 7.092 sec/iter\n",
      "Epoch: 50 | Batch: 003 / 030 | Total loss: 1.466 | Reg loss: 0.043 | Tree loss: 1.466 | Accuracy: 0.558594 | 7.092 sec/iter\n",
      "Epoch: 50 | Batch: 004 / 030 | Total loss: 1.450 | Reg loss: 0.043 | Tree loss: 1.450 | Accuracy: 0.568359 | 7.092 sec/iter\n",
      "Epoch: 50 | Batch: 005 / 030 | Total loss: 1.428 | Reg loss: 0.043 | Tree loss: 1.428 | Accuracy: 0.595703 | 7.092 sec/iter\n",
      "Epoch: 50 | Batch: 006 / 030 | Total loss: 1.393 | Reg loss: 0.043 | Tree loss: 1.393 | Accuracy: 0.560547 | 7.092 sec/iter\n",
      "Epoch: 50 | Batch: 007 / 030 | Total loss: 1.357 | Reg loss: 0.043 | Tree loss: 1.357 | Accuracy: 0.617188 | 7.092 sec/iter\n",
      "Epoch: 50 | Batch: 008 / 030 | Total loss: 1.389 | Reg loss: 0.043 | Tree loss: 1.389 | Accuracy: 0.570312 | 7.092 sec/iter\n",
      "Epoch: 50 | Batch: 009 / 030 | Total loss: 1.334 | Reg loss: 0.043 | Tree loss: 1.334 | Accuracy: 0.585938 | 7.091 sec/iter\n",
      "Epoch: 50 | Batch: 010 / 030 | Total loss: 1.331 | Reg loss: 0.043 | Tree loss: 1.331 | Accuracy: 0.597656 | 7.091 sec/iter\n",
      "Epoch: 50 | Batch: 011 / 030 | Total loss: 1.328 | Reg loss: 0.043 | Tree loss: 1.328 | Accuracy: 0.597656 | 7.091 sec/iter\n",
      "Epoch: 50 | Batch: 012 / 030 | Total loss: 1.335 | Reg loss: 0.043 | Tree loss: 1.335 | Accuracy: 0.544922 | 7.091 sec/iter\n",
      "Epoch: 50 | Batch: 013 / 030 | Total loss: 1.300 | Reg loss: 0.043 | Tree loss: 1.300 | Accuracy: 0.556641 | 7.091 sec/iter\n",
      "Epoch: 50 | Batch: 014 / 030 | Total loss: 1.272 | Reg loss: 0.044 | Tree loss: 1.272 | Accuracy: 0.597656 | 7.091 sec/iter\n",
      "Epoch: 50 | Batch: 015 / 030 | Total loss: 1.273 | Reg loss: 0.044 | Tree loss: 1.273 | Accuracy: 0.599609 | 7.09 sec/iter\n",
      "Epoch: 50 | Batch: 016 / 030 | Total loss: 1.224 | Reg loss: 0.044 | Tree loss: 1.224 | Accuracy: 0.611328 | 7.09 sec/iter\n",
      "Epoch: 50 | Batch: 017 / 030 | Total loss: 1.236 | Reg loss: 0.044 | Tree loss: 1.236 | Accuracy: 0.595703 | 7.09 sec/iter\n",
      "Epoch: 50 | Batch: 018 / 030 | Total loss: 1.207 | Reg loss: 0.044 | Tree loss: 1.207 | Accuracy: 0.617188 | 7.09 sec/iter\n",
      "Epoch: 50 | Batch: 019 / 030 | Total loss: 1.191 | Reg loss: 0.044 | Tree loss: 1.191 | Accuracy: 0.613281 | 7.09 sec/iter\n",
      "Epoch: 50 | Batch: 020 / 030 | Total loss: 1.209 | Reg loss: 0.044 | Tree loss: 1.209 | Accuracy: 0.570312 | 7.09 sec/iter\n",
      "Epoch: 50 | Batch: 021 / 030 | Total loss: 1.189 | Reg loss: 0.044 | Tree loss: 1.189 | Accuracy: 0.585938 | 7.09 sec/iter\n",
      "Epoch: 50 | Batch: 022 / 030 | Total loss: 1.188 | Reg loss: 0.044 | Tree loss: 1.188 | Accuracy: 0.566406 | 7.09 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50 | Batch: 023 / 030 | Total loss: 1.192 | Reg loss: 0.044 | Tree loss: 1.192 | Accuracy: 0.554688 | 7.09 sec/iter\n",
      "Epoch: 50 | Batch: 024 / 030 | Total loss: 1.188 | Reg loss: 0.044 | Tree loss: 1.188 | Accuracy: 0.539062 | 7.088 sec/iter\n",
      "Epoch: 50 | Batch: 025 / 030 | Total loss: 1.176 | Reg loss: 0.044 | Tree loss: 1.176 | Accuracy: 0.548828 | 7.087 sec/iter\n",
      "Epoch: 50 | Batch: 026 / 030 | Total loss: 1.143 | Reg loss: 0.044 | Tree loss: 1.143 | Accuracy: 0.593750 | 7.085 sec/iter\n",
      "Epoch: 50 | Batch: 027 / 030 | Total loss: 1.130 | Reg loss: 0.044 | Tree loss: 1.130 | Accuracy: 0.597656 | 7.084 sec/iter\n",
      "Epoch: 50 | Batch: 028 / 030 | Total loss: 1.133 | Reg loss: 0.044 | Tree loss: 1.133 | Accuracy: 0.570312 | 7.082 sec/iter\n",
      "Epoch: 50 | Batch: 029 / 030 | Total loss: 1.095 | Reg loss: 0.045 | Tree loss: 1.095 | Accuracy: 0.592233 | 7.079 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 51 | Batch: 000 / 030 | Total loss: 1.529 | Reg loss: 0.043 | Tree loss: 1.529 | Accuracy: 0.574219 | 7.091 sec/iter\n",
      "Epoch: 51 | Batch: 001 / 030 | Total loss: 1.481 | Reg loss: 0.043 | Tree loss: 1.481 | Accuracy: 0.560547 | 7.091 sec/iter\n",
      "Epoch: 51 | Batch: 002 / 030 | Total loss: 1.457 | Reg loss: 0.043 | Tree loss: 1.457 | Accuracy: 0.546875 | 7.091 sec/iter\n",
      "Epoch: 51 | Batch: 003 / 030 | Total loss: 1.405 | Reg loss: 0.043 | Tree loss: 1.405 | Accuracy: 0.607422 | 7.091 sec/iter\n",
      "Epoch: 51 | Batch: 004 / 030 | Total loss: 1.418 | Reg loss: 0.043 | Tree loss: 1.418 | Accuracy: 0.615234 | 7.091 sec/iter\n",
      "Epoch: 51 | Batch: 005 / 030 | Total loss: 1.412 | Reg loss: 0.043 | Tree loss: 1.412 | Accuracy: 0.554688 | 7.091 sec/iter\n",
      "Epoch: 51 | Batch: 006 / 030 | Total loss: 1.411 | Reg loss: 0.043 | Tree loss: 1.411 | Accuracy: 0.593750 | 7.091 sec/iter\n",
      "Epoch: 51 | Batch: 007 / 030 | Total loss: 1.350 | Reg loss: 0.043 | Tree loss: 1.350 | Accuracy: 0.597656 | 7.091 sec/iter\n",
      "Epoch: 51 | Batch: 008 / 030 | Total loss: 1.341 | Reg loss: 0.043 | Tree loss: 1.341 | Accuracy: 0.576172 | 7.091 sec/iter\n",
      "Epoch: 51 | Batch: 009 / 030 | Total loss: 1.315 | Reg loss: 0.043 | Tree loss: 1.315 | Accuracy: 0.607422 | 7.091 sec/iter\n",
      "Epoch: 51 | Batch: 010 / 030 | Total loss: 1.288 | Reg loss: 0.043 | Tree loss: 1.288 | Accuracy: 0.625000 | 7.091 sec/iter\n",
      "Epoch: 51 | Batch: 011 / 030 | Total loss: 1.270 | Reg loss: 0.043 | Tree loss: 1.270 | Accuracy: 0.580078 | 7.091 sec/iter\n",
      "Epoch: 51 | Batch: 012 / 030 | Total loss: 1.282 | Reg loss: 0.043 | Tree loss: 1.282 | Accuracy: 0.566406 | 7.091 sec/iter\n",
      "Epoch: 51 | Batch: 013 / 030 | Total loss: 1.292 | Reg loss: 0.043 | Tree loss: 1.292 | Accuracy: 0.539062 | 7.091 sec/iter\n",
      "Epoch: 51 | Batch: 014 / 030 | Total loss: 1.287 | Reg loss: 0.043 | Tree loss: 1.287 | Accuracy: 0.542969 | 7.091 sec/iter\n",
      "Epoch: 51 | Batch: 015 / 030 | Total loss: 1.251 | Reg loss: 0.043 | Tree loss: 1.251 | Accuracy: 0.550781 | 7.091 sec/iter\n",
      "Epoch: 51 | Batch: 016 / 030 | Total loss: 1.225 | Reg loss: 0.043 | Tree loss: 1.225 | Accuracy: 0.601562 | 7.091 sec/iter\n",
      "Epoch: 51 | Batch: 017 / 030 | Total loss: 1.232 | Reg loss: 0.044 | Tree loss: 1.232 | Accuracy: 0.558594 | 7.091 sec/iter\n",
      "Epoch: 51 | Batch: 018 / 030 | Total loss: 1.184 | Reg loss: 0.044 | Tree loss: 1.184 | Accuracy: 0.609375 | 7.091 sec/iter\n",
      "Epoch: 51 | Batch: 019 / 030 | Total loss: 1.195 | Reg loss: 0.044 | Tree loss: 1.195 | Accuracy: 0.564453 | 7.091 sec/iter\n",
      "Epoch: 51 | Batch: 020 / 030 | Total loss: 1.149 | Reg loss: 0.044 | Tree loss: 1.149 | Accuracy: 0.601562 | 7.091 sec/iter\n",
      "Epoch: 51 | Batch: 021 / 030 | Total loss: 1.180 | Reg loss: 0.044 | Tree loss: 1.180 | Accuracy: 0.585938 | 7.09 sec/iter\n",
      "Epoch: 51 | Batch: 022 / 030 | Total loss: 1.158 | Reg loss: 0.044 | Tree loss: 1.158 | Accuracy: 0.570312 | 7.09 sec/iter\n",
      "Epoch: 51 | Batch: 023 / 030 | Total loss: 1.136 | Reg loss: 0.044 | Tree loss: 1.136 | Accuracy: 0.607422 | 7.089 sec/iter\n",
      "Epoch: 51 | Batch: 024 / 030 | Total loss: 1.137 | Reg loss: 0.044 | Tree loss: 1.137 | Accuracy: 0.595703 | 7.088 sec/iter\n",
      "Epoch: 51 | Batch: 025 / 030 | Total loss: 1.111 | Reg loss: 0.044 | Tree loss: 1.111 | Accuracy: 0.597656 | 7.086 sec/iter\n",
      "Epoch: 51 | Batch: 026 / 030 | Total loss: 1.111 | Reg loss: 0.044 | Tree loss: 1.111 | Accuracy: 0.582031 | 7.085 sec/iter\n",
      "Epoch: 51 | Batch: 027 / 030 | Total loss: 1.088 | Reg loss: 0.044 | Tree loss: 1.088 | Accuracy: 0.601562 | 7.085 sec/iter\n",
      "Epoch: 51 | Batch: 028 / 030 | Total loss: 1.096 | Reg loss: 0.044 | Tree loss: 1.096 | Accuracy: 0.574219 | 7.085 sec/iter\n",
      "Epoch: 51 | Batch: 029 / 030 | Total loss: 1.094 | Reg loss: 0.044 | Tree loss: 1.094 | Accuracy: 0.640777 | 7.082 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 52 | Batch: 000 / 030 | Total loss: 1.470 | Reg loss: 0.043 | Tree loss: 1.470 | Accuracy: 0.587891 | 7.088 sec/iter\n",
      "Epoch: 52 | Batch: 001 / 030 | Total loss: 1.425 | Reg loss: 0.043 | Tree loss: 1.425 | Accuracy: 0.589844 | 7.088 sec/iter\n",
      "Epoch: 52 | Batch: 002 / 030 | Total loss: 1.434 | Reg loss: 0.043 | Tree loss: 1.434 | Accuracy: 0.566406 | 7.087 sec/iter\n",
      "Epoch: 52 | Batch: 003 / 030 | Total loss: 1.415 | Reg loss: 0.043 | Tree loss: 1.415 | Accuracy: 0.589844 | 7.087 sec/iter\n",
      "Epoch: 52 | Batch: 004 / 030 | Total loss: 1.399 | Reg loss: 0.043 | Tree loss: 1.399 | Accuracy: 0.564453 | 7.086 sec/iter\n",
      "Epoch: 52 | Batch: 005 / 030 | Total loss: 1.381 | Reg loss: 0.043 | Tree loss: 1.381 | Accuracy: 0.587891 | 7.086 sec/iter\n",
      "Epoch: 52 | Batch: 006 / 030 | Total loss: 1.383 | Reg loss: 0.043 | Tree loss: 1.383 | Accuracy: 0.572266 | 7.086 sec/iter\n",
      "Epoch: 52 | Batch: 007 / 030 | Total loss: 1.328 | Reg loss: 0.043 | Tree loss: 1.328 | Accuracy: 0.619141 | 7.086 sec/iter\n",
      "Epoch: 52 | Batch: 008 / 030 | Total loss: 1.298 | Reg loss: 0.043 | Tree loss: 1.298 | Accuracy: 0.613281 | 7.086 sec/iter\n",
      "Epoch: 52 | Batch: 009 / 030 | Total loss: 1.332 | Reg loss: 0.043 | Tree loss: 1.332 | Accuracy: 0.589844 | 7.086 sec/iter\n",
      "Epoch: 52 | Batch: 010 / 030 | Total loss: 1.297 | Reg loss: 0.043 | Tree loss: 1.297 | Accuracy: 0.576172 | 7.086 sec/iter\n",
      "Epoch: 52 | Batch: 011 / 030 | Total loss: 1.268 | Reg loss: 0.043 | Tree loss: 1.268 | Accuracy: 0.611328 | 7.086 sec/iter\n",
      "Epoch: 52 | Batch: 012 / 030 | Total loss: 1.276 | Reg loss: 0.043 | Tree loss: 1.276 | Accuracy: 0.556641 | 7.086 sec/iter\n",
      "Epoch: 52 | Batch: 013 / 030 | Total loss: 1.263 | Reg loss: 0.043 | Tree loss: 1.263 | Accuracy: 0.568359 | 7.087 sec/iter\n",
      "Epoch: 52 | Batch: 014 / 030 | Total loss: 1.202 | Reg loss: 0.043 | Tree loss: 1.202 | Accuracy: 0.599609 | 7.087 sec/iter\n",
      "Epoch: 52 | Batch: 015 / 030 | Total loss: 1.216 | Reg loss: 0.043 | Tree loss: 1.216 | Accuracy: 0.578125 | 7.087 sec/iter\n",
      "Epoch: 52 | Batch: 016 / 030 | Total loss: 1.208 | Reg loss: 0.043 | Tree loss: 1.208 | Accuracy: 0.568359 | 7.087 sec/iter\n",
      "Epoch: 52 | Batch: 017 / 030 | Total loss: 1.192 | Reg loss: 0.043 | Tree loss: 1.192 | Accuracy: 0.560547 | 7.087 sec/iter\n",
      "Epoch: 52 | Batch: 018 / 030 | Total loss: 1.193 | Reg loss: 0.043 | Tree loss: 1.193 | Accuracy: 0.568359 | 7.087 sec/iter\n",
      "Epoch: 52 | Batch: 019 / 030 | Total loss: 1.175 | Reg loss: 0.044 | Tree loss: 1.175 | Accuracy: 0.568359 | 7.087 sec/iter\n",
      "Epoch: 52 | Batch: 020 / 030 | Total loss: 1.142 | Reg loss: 0.044 | Tree loss: 1.142 | Accuracy: 0.574219 | 7.087 sec/iter\n",
      "Epoch: 52 | Batch: 021 / 030 | Total loss: 1.143 | Reg loss: 0.044 | Tree loss: 1.143 | Accuracy: 0.597656 | 7.087 sec/iter\n",
      "Epoch: 52 | Batch: 022 / 030 | Total loss: 1.136 | Reg loss: 0.044 | Tree loss: 1.136 | Accuracy: 0.601562 | 7.087 sec/iter\n",
      "Epoch: 52 | Batch: 023 / 030 | Total loss: 1.134 | Reg loss: 0.044 | Tree loss: 1.134 | Accuracy: 0.560547 | 7.086 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 52 | Batch: 024 / 030 | Total loss: 1.121 | Reg loss: 0.044 | Tree loss: 1.121 | Accuracy: 0.568359 | 7.084 sec/iter\n",
      "Epoch: 52 | Batch: 025 / 030 | Total loss: 1.118 | Reg loss: 0.044 | Tree loss: 1.118 | Accuracy: 0.589844 | 7.083 sec/iter\n",
      "Epoch: 52 | Batch: 026 / 030 | Total loss: 1.091 | Reg loss: 0.044 | Tree loss: 1.091 | Accuracy: 0.597656 | 7.082 sec/iter\n",
      "Epoch: 52 | Batch: 027 / 030 | Total loss: 1.098 | Reg loss: 0.044 | Tree loss: 1.098 | Accuracy: 0.560547 | 7.082 sec/iter\n",
      "Epoch: 52 | Batch: 028 / 030 | Total loss: 1.074 | Reg loss: 0.044 | Tree loss: 1.074 | Accuracy: 0.603516 | 7.082 sec/iter\n",
      "Epoch: 52 | Batch: 029 / 030 | Total loss: 1.050 | Reg loss: 0.044 | Tree loss: 1.050 | Accuracy: 0.631068 | 7.08 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 53 | Batch: 000 / 030 | Total loss: 1.421 | Reg loss: 0.043 | Tree loss: 1.421 | Accuracy: 0.574219 | 7.084 sec/iter\n",
      "Epoch: 53 | Batch: 001 / 030 | Total loss: 1.440 | Reg loss: 0.043 | Tree loss: 1.440 | Accuracy: 0.556641 | 7.083 sec/iter\n",
      "Epoch: 53 | Batch: 002 / 030 | Total loss: 1.405 | Reg loss: 0.043 | Tree loss: 1.405 | Accuracy: 0.613281 | 7.083 sec/iter\n",
      "Epoch: 53 | Batch: 003 / 030 | Total loss: 1.391 | Reg loss: 0.043 | Tree loss: 1.391 | Accuracy: 0.597656 | 7.083 sec/iter\n",
      "Epoch: 53 | Batch: 004 / 030 | Total loss: 1.365 | Reg loss: 0.043 | Tree loss: 1.365 | Accuracy: 0.625000 | 7.083 sec/iter\n",
      "Epoch: 53 | Batch: 005 / 030 | Total loss: 1.354 | Reg loss: 0.043 | Tree loss: 1.354 | Accuracy: 0.589844 | 7.083 sec/iter\n",
      "Epoch: 53 | Batch: 006 / 030 | Total loss: 1.383 | Reg loss: 0.043 | Tree loss: 1.383 | Accuracy: 0.558594 | 7.083 sec/iter\n",
      "Epoch: 53 | Batch: 007 / 030 | Total loss: 1.315 | Reg loss: 0.043 | Tree loss: 1.315 | Accuracy: 0.595703 | 7.084 sec/iter\n",
      "Epoch: 53 | Batch: 008 / 030 | Total loss: 1.348 | Reg loss: 0.043 | Tree loss: 1.348 | Accuracy: 0.541016 | 7.084 sec/iter\n",
      "Epoch: 53 | Batch: 009 / 030 | Total loss: 1.292 | Reg loss: 0.043 | Tree loss: 1.292 | Accuracy: 0.558594 | 7.084 sec/iter\n",
      "Epoch: 53 | Batch: 010 / 030 | Total loss: 1.266 | Reg loss: 0.043 | Tree loss: 1.266 | Accuracy: 0.601562 | 7.084 sec/iter\n",
      "Epoch: 53 | Batch: 011 / 030 | Total loss: 1.248 | Reg loss: 0.043 | Tree loss: 1.248 | Accuracy: 0.580078 | 7.084 sec/iter\n",
      "Epoch: 53 | Batch: 012 / 030 | Total loss: 1.245 | Reg loss: 0.043 | Tree loss: 1.245 | Accuracy: 0.572266 | 7.084 sec/iter\n",
      "Epoch: 53 | Batch: 013 / 030 | Total loss: 1.228 | Reg loss: 0.043 | Tree loss: 1.228 | Accuracy: 0.593750 | 7.084 sec/iter\n",
      "Epoch: 53 | Batch: 014 / 030 | Total loss: 1.217 | Reg loss: 0.043 | Tree loss: 1.217 | Accuracy: 0.570312 | 7.084 sec/iter\n",
      "Epoch: 53 | Batch: 015 / 030 | Total loss: 1.185 | Reg loss: 0.043 | Tree loss: 1.185 | Accuracy: 0.572266 | 7.084 sec/iter\n",
      "Epoch: 53 | Batch: 016 / 030 | Total loss: 1.190 | Reg loss: 0.043 | Tree loss: 1.190 | Accuracy: 0.564453 | 7.085 sec/iter\n",
      "Epoch: 53 | Batch: 017 / 030 | Total loss: 1.174 | Reg loss: 0.043 | Tree loss: 1.174 | Accuracy: 0.570312 | 7.085 sec/iter\n",
      "Epoch: 53 | Batch: 018 / 030 | Total loss: 1.172 | Reg loss: 0.043 | Tree loss: 1.172 | Accuracy: 0.603516 | 7.085 sec/iter\n",
      "Epoch: 53 | Batch: 019 / 030 | Total loss: 1.146 | Reg loss: 0.043 | Tree loss: 1.146 | Accuracy: 0.595703 | 7.084 sec/iter\n",
      "Epoch: 53 | Batch: 020 / 030 | Total loss: 1.145 | Reg loss: 0.043 | Tree loss: 1.145 | Accuracy: 0.597656 | 7.084 sec/iter\n",
      "Epoch: 53 | Batch: 021 / 030 | Total loss: 1.121 | Reg loss: 0.044 | Tree loss: 1.121 | Accuracy: 0.593750 | 7.084 sec/iter\n",
      "Epoch: 53 | Batch: 022 / 030 | Total loss: 1.100 | Reg loss: 0.044 | Tree loss: 1.100 | Accuracy: 0.611328 | 7.083 sec/iter\n",
      "Epoch: 53 | Batch: 023 / 030 | Total loss: 1.106 | Reg loss: 0.044 | Tree loss: 1.106 | Accuracy: 0.583984 | 7.081 sec/iter\n",
      "Epoch: 53 | Batch: 024 / 030 | Total loss: 1.080 | Reg loss: 0.044 | Tree loss: 1.080 | Accuracy: 0.589844 | 7.08 sec/iter\n",
      "Epoch: 53 | Batch: 025 / 030 | Total loss: 1.106 | Reg loss: 0.044 | Tree loss: 1.106 | Accuracy: 0.560547 | 7.078 sec/iter\n",
      "Epoch: 53 | Batch: 026 / 030 | Total loss: 1.075 | Reg loss: 0.044 | Tree loss: 1.075 | Accuracy: 0.562500 | 7.077 sec/iter\n",
      "Epoch: 53 | Batch: 027 / 030 | Total loss: 1.068 | Reg loss: 0.044 | Tree loss: 1.068 | Accuracy: 0.580078 | 7.076 sec/iter\n",
      "Epoch: 53 | Batch: 028 / 030 | Total loss: 1.059 | Reg loss: 0.044 | Tree loss: 1.059 | Accuracy: 0.582031 | 7.076 sec/iter\n",
      "Epoch: 53 | Batch: 029 / 030 | Total loss: 1.058 | Reg loss: 0.044 | Tree loss: 1.058 | Accuracy: 0.601942 | 7.074 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 54 | Batch: 000 / 030 | Total loss: 1.418 | Reg loss: 0.042 | Tree loss: 1.418 | Accuracy: 0.583984 | 7.082 sec/iter\n",
      "Epoch: 54 | Batch: 001 / 030 | Total loss: 1.431 | Reg loss: 0.042 | Tree loss: 1.431 | Accuracy: 0.539062 | 7.082 sec/iter\n",
      "Epoch: 54 | Batch: 002 / 030 | Total loss: 1.363 | Reg loss: 0.042 | Tree loss: 1.363 | Accuracy: 0.613281 | 7.082 sec/iter\n",
      "Epoch: 54 | Batch: 003 / 030 | Total loss: 1.386 | Reg loss: 0.042 | Tree loss: 1.386 | Accuracy: 0.585938 | 7.082 sec/iter\n",
      "Epoch: 54 | Batch: 004 / 030 | Total loss: 1.361 | Reg loss: 0.042 | Tree loss: 1.361 | Accuracy: 0.593750 | 7.082 sec/iter\n",
      "Epoch: 54 | Batch: 005 / 030 | Total loss: 1.382 | Reg loss: 0.042 | Tree loss: 1.382 | Accuracy: 0.580078 | 7.082 sec/iter\n",
      "Epoch: 54 | Batch: 006 / 030 | Total loss: 1.322 | Reg loss: 0.042 | Tree loss: 1.322 | Accuracy: 0.587891 | 7.082 sec/iter\n",
      "Epoch: 54 | Batch: 007 / 030 | Total loss: 1.288 | Reg loss: 0.042 | Tree loss: 1.288 | Accuracy: 0.589844 | 7.082 sec/iter\n",
      "Epoch: 54 | Batch: 008 / 030 | Total loss: 1.278 | Reg loss: 0.043 | Tree loss: 1.278 | Accuracy: 0.574219 | 7.082 sec/iter\n",
      "Epoch: 54 | Batch: 009 / 030 | Total loss: 1.295 | Reg loss: 0.043 | Tree loss: 1.295 | Accuracy: 0.554688 | 7.082 sec/iter\n",
      "Epoch: 54 | Batch: 010 / 030 | Total loss: 1.253 | Reg loss: 0.043 | Tree loss: 1.253 | Accuracy: 0.585938 | 7.082 sec/iter\n",
      "Epoch: 54 | Batch: 011 / 030 | Total loss: 1.245 | Reg loss: 0.043 | Tree loss: 1.245 | Accuracy: 0.568359 | 7.082 sec/iter\n",
      "Epoch: 54 | Batch: 012 / 030 | Total loss: 1.225 | Reg loss: 0.043 | Tree loss: 1.225 | Accuracy: 0.580078 | 7.082 sec/iter\n",
      "Epoch: 54 | Batch: 013 / 030 | Total loss: 1.211 | Reg loss: 0.043 | Tree loss: 1.211 | Accuracy: 0.578125 | 7.082 sec/iter\n",
      "Epoch: 54 | Batch: 014 / 030 | Total loss: 1.185 | Reg loss: 0.043 | Tree loss: 1.185 | Accuracy: 0.599609 | 7.082 sec/iter\n",
      "Epoch: 54 | Batch: 015 / 030 | Total loss: 1.172 | Reg loss: 0.043 | Tree loss: 1.172 | Accuracy: 0.560547 | 7.082 sec/iter\n",
      "Epoch: 54 | Batch: 016 / 030 | Total loss: 1.169 | Reg loss: 0.043 | Tree loss: 1.169 | Accuracy: 0.580078 | 7.082 sec/iter\n",
      "Epoch: 54 | Batch: 017 / 030 | Total loss: 1.172 | Reg loss: 0.043 | Tree loss: 1.172 | Accuracy: 0.576172 | 7.081 sec/iter\n",
      "Epoch: 54 | Batch: 018 / 030 | Total loss: 1.137 | Reg loss: 0.043 | Tree loss: 1.137 | Accuracy: 0.601562 | 7.081 sec/iter\n",
      "Epoch: 54 | Batch: 019 / 030 | Total loss: 1.126 | Reg loss: 0.043 | Tree loss: 1.126 | Accuracy: 0.609375 | 7.081 sec/iter\n",
      "Epoch: 54 | Batch: 020 / 030 | Total loss: 1.138 | Reg loss: 0.043 | Tree loss: 1.138 | Accuracy: 0.564453 | 7.081 sec/iter\n",
      "Epoch: 54 | Batch: 021 / 030 | Total loss: 1.113 | Reg loss: 0.043 | Tree loss: 1.113 | Accuracy: 0.580078 | 7.08 sec/iter\n",
      "Epoch: 54 | Batch: 022 / 030 | Total loss: 1.117 | Reg loss: 0.044 | Tree loss: 1.117 | Accuracy: 0.564453 | 7.078 sec/iter\n",
      "Epoch: 54 | Batch: 023 / 030 | Total loss: 1.069 | Reg loss: 0.044 | Tree loss: 1.069 | Accuracy: 0.617188 | 7.077 sec/iter\n",
      "Epoch: 54 | Batch: 024 / 030 | Total loss: 1.073 | Reg loss: 0.044 | Tree loss: 1.073 | Accuracy: 0.617188 | 7.075 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 54 | Batch: 025 / 030 | Total loss: 1.073 | Reg loss: 0.044 | Tree loss: 1.073 | Accuracy: 0.583984 | 7.074 sec/iter\n",
      "Epoch: 54 | Batch: 026 / 030 | Total loss: 1.042 | Reg loss: 0.044 | Tree loss: 1.042 | Accuracy: 0.613281 | 7.073 sec/iter\n",
      "Epoch: 54 | Batch: 027 / 030 | Total loss: 1.072 | Reg loss: 0.044 | Tree loss: 1.072 | Accuracy: 0.544922 | 7.073 sec/iter\n",
      "Epoch: 54 | Batch: 028 / 030 | Total loss: 1.053 | Reg loss: 0.044 | Tree loss: 1.053 | Accuracy: 0.566406 | 7.073 sec/iter\n",
      "Epoch: 54 | Batch: 029 / 030 | Total loss: 1.053 | Reg loss: 0.044 | Tree loss: 1.053 | Accuracy: 0.611650 | 7.071 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 55 | Batch: 000 / 030 | Total loss: 1.400 | Reg loss: 0.042 | Tree loss: 1.400 | Accuracy: 0.578125 | 7.077 sec/iter\n",
      "Epoch: 55 | Batch: 001 / 030 | Total loss: 1.412 | Reg loss: 0.042 | Tree loss: 1.412 | Accuracy: 0.578125 | 7.077 sec/iter\n",
      "Epoch: 55 | Batch: 002 / 030 | Total loss: 1.357 | Reg loss: 0.042 | Tree loss: 1.357 | Accuracy: 0.560547 | 7.077 sec/iter\n",
      "Epoch: 55 | Batch: 003 / 030 | Total loss: 1.357 | Reg loss: 0.042 | Tree loss: 1.357 | Accuracy: 0.574219 | 7.077 sec/iter\n",
      "Epoch: 55 | Batch: 004 / 030 | Total loss: 1.321 | Reg loss: 0.042 | Tree loss: 1.321 | Accuracy: 0.585938 | 7.077 sec/iter\n",
      "Epoch: 55 | Batch: 005 / 030 | Total loss: 1.343 | Reg loss: 0.042 | Tree loss: 1.343 | Accuracy: 0.595703 | 7.077 sec/iter\n",
      "Epoch: 55 | Batch: 006 / 030 | Total loss: 1.310 | Reg loss: 0.042 | Tree loss: 1.310 | Accuracy: 0.568359 | 7.077 sec/iter\n",
      "Epoch: 55 | Batch: 007 / 030 | Total loss: 1.289 | Reg loss: 0.042 | Tree loss: 1.289 | Accuracy: 0.623047 | 7.077 sec/iter\n",
      "Epoch: 55 | Batch: 008 / 030 | Total loss: 1.282 | Reg loss: 0.042 | Tree loss: 1.282 | Accuracy: 0.572266 | 7.077 sec/iter\n",
      "Epoch: 55 | Batch: 009 / 030 | Total loss: 1.279 | Reg loss: 0.042 | Tree loss: 1.279 | Accuracy: 0.560547 | 7.077 sec/iter\n",
      "Epoch: 55 | Batch: 010 / 030 | Total loss: 1.259 | Reg loss: 0.043 | Tree loss: 1.259 | Accuracy: 0.560547 | 7.077 sec/iter\n",
      "Epoch: 55 | Batch: 011 / 030 | Total loss: 1.244 | Reg loss: 0.043 | Tree loss: 1.244 | Accuracy: 0.578125 | 7.076 sec/iter\n",
      "Epoch: 55 | Batch: 012 / 030 | Total loss: 1.230 | Reg loss: 0.043 | Tree loss: 1.230 | Accuracy: 0.599609 | 7.076 sec/iter\n",
      "Epoch: 55 | Batch: 013 / 030 | Total loss: 1.199 | Reg loss: 0.043 | Tree loss: 1.199 | Accuracy: 0.601562 | 7.076 sec/iter\n",
      "Epoch: 55 | Batch: 014 / 030 | Total loss: 1.173 | Reg loss: 0.043 | Tree loss: 1.173 | Accuracy: 0.572266 | 7.076 sec/iter\n",
      "Epoch: 55 | Batch: 015 / 030 | Total loss: 1.182 | Reg loss: 0.043 | Tree loss: 1.182 | Accuracy: 0.560547 | 7.076 sec/iter\n",
      "Epoch: 55 | Batch: 016 / 030 | Total loss: 1.146 | Reg loss: 0.043 | Tree loss: 1.146 | Accuracy: 0.578125 | 7.076 sec/iter\n",
      "Epoch: 55 | Batch: 017 / 030 | Total loss: 1.140 | Reg loss: 0.043 | Tree loss: 1.140 | Accuracy: 0.576172 | 7.076 sec/iter\n",
      "Epoch: 55 | Batch: 018 / 030 | Total loss: 1.112 | Reg loss: 0.043 | Tree loss: 1.112 | Accuracy: 0.609375 | 7.076 sec/iter\n",
      "Epoch: 55 | Batch: 019 / 030 | Total loss: 1.116 | Reg loss: 0.043 | Tree loss: 1.116 | Accuracy: 0.550781 | 7.076 sec/iter\n",
      "Epoch: 55 | Batch: 020 / 030 | Total loss: 1.121 | Reg loss: 0.043 | Tree loss: 1.121 | Accuracy: 0.542969 | 7.076 sec/iter\n",
      "Epoch: 55 | Batch: 021 / 030 | Total loss: 1.091 | Reg loss: 0.043 | Tree loss: 1.091 | Accuracy: 0.574219 | 7.076 sec/iter\n",
      "Epoch: 55 | Batch: 022 / 030 | Total loss: 1.072 | Reg loss: 0.043 | Tree loss: 1.072 | Accuracy: 0.603516 | 7.075 sec/iter\n",
      "Epoch: 55 | Batch: 023 / 030 | Total loss: 1.082 | Reg loss: 0.044 | Tree loss: 1.082 | Accuracy: 0.591797 | 7.074 sec/iter\n",
      "Epoch: 55 | Batch: 024 / 030 | Total loss: 1.074 | Reg loss: 0.044 | Tree loss: 1.074 | Accuracy: 0.562500 | 7.073 sec/iter\n",
      "Epoch: 55 | Batch: 025 / 030 | Total loss: 1.045 | Reg loss: 0.044 | Tree loss: 1.045 | Accuracy: 0.609375 | 7.072 sec/iter\n",
      "Epoch: 55 | Batch: 026 / 030 | Total loss: 1.063 | Reg loss: 0.044 | Tree loss: 1.063 | Accuracy: 0.574219 | 7.072 sec/iter\n",
      "Epoch: 55 | Batch: 027 / 030 | Total loss: 1.023 | Reg loss: 0.044 | Tree loss: 1.023 | Accuracy: 0.621094 | 7.072 sec/iter\n",
      "Epoch: 55 | Batch: 028 / 030 | Total loss: 1.015 | Reg loss: 0.044 | Tree loss: 1.015 | Accuracy: 0.638672 | 7.071 sec/iter\n",
      "Epoch: 55 | Batch: 029 / 030 | Total loss: 1.030 | Reg loss: 0.044 | Tree loss: 1.030 | Accuracy: 0.572816 | 7.069 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 56 | Batch: 000 / 030 | Total loss: 1.429 | Reg loss: 0.042 | Tree loss: 1.429 | Accuracy: 0.554688 | 7.076 sec/iter\n",
      "Epoch: 56 | Batch: 001 / 030 | Total loss: 1.376 | Reg loss: 0.042 | Tree loss: 1.376 | Accuracy: 0.570312 | 7.076 sec/iter\n",
      "Epoch: 56 | Batch: 002 / 030 | Total loss: 1.373 | Reg loss: 0.042 | Tree loss: 1.373 | Accuracy: 0.546875 | 7.076 sec/iter\n",
      "Epoch: 56 | Batch: 003 / 030 | Total loss: 1.317 | Reg loss: 0.042 | Tree loss: 1.317 | Accuracy: 0.591797 | 7.076 sec/iter\n",
      "Epoch: 56 | Batch: 004 / 030 | Total loss: 1.315 | Reg loss: 0.042 | Tree loss: 1.315 | Accuracy: 0.603516 | 7.076 sec/iter\n",
      "Epoch: 56 | Batch: 005 / 030 | Total loss: 1.292 | Reg loss: 0.042 | Tree loss: 1.292 | Accuracy: 0.593750 | 7.076 sec/iter\n",
      "Epoch: 56 | Batch: 006 / 030 | Total loss: 1.293 | Reg loss: 0.042 | Tree loss: 1.293 | Accuracy: 0.560547 | 7.076 sec/iter\n",
      "Epoch: 56 | Batch: 007 / 030 | Total loss: 1.271 | Reg loss: 0.042 | Tree loss: 1.271 | Accuracy: 0.609375 | 7.076 sec/iter\n",
      "Epoch: 56 | Batch: 008 / 030 | Total loss: 1.293 | Reg loss: 0.042 | Tree loss: 1.293 | Accuracy: 0.558594 | 7.076 sec/iter\n",
      "Epoch: 56 | Batch: 009 / 030 | Total loss: 1.254 | Reg loss: 0.042 | Tree loss: 1.254 | Accuracy: 0.591797 | 7.075 sec/iter\n",
      "Epoch: 56 | Batch: 010 / 030 | Total loss: 1.235 | Reg loss: 0.042 | Tree loss: 1.235 | Accuracy: 0.585938 | 7.075 sec/iter\n",
      "Epoch: 56 | Batch: 011 / 030 | Total loss: 1.210 | Reg loss: 0.042 | Tree loss: 1.210 | Accuracy: 0.593750 | 7.075 sec/iter\n",
      "Epoch: 56 | Batch: 012 / 030 | Total loss: 1.180 | Reg loss: 0.043 | Tree loss: 1.180 | Accuracy: 0.613281 | 7.075 sec/iter\n",
      "Epoch: 56 | Batch: 013 / 030 | Total loss: 1.182 | Reg loss: 0.043 | Tree loss: 1.182 | Accuracy: 0.564453 | 7.075 sec/iter\n",
      "Epoch: 56 | Batch: 014 / 030 | Total loss: 1.168 | Reg loss: 0.043 | Tree loss: 1.168 | Accuracy: 0.585938 | 7.074 sec/iter\n",
      "Epoch: 56 | Batch: 015 / 030 | Total loss: 1.131 | Reg loss: 0.043 | Tree loss: 1.131 | Accuracy: 0.611328 | 7.074 sec/iter\n",
      "Epoch: 56 | Batch: 016 / 030 | Total loss: 1.154 | Reg loss: 0.043 | Tree loss: 1.154 | Accuracy: 0.580078 | 7.074 sec/iter\n",
      "Epoch: 56 | Batch: 017 / 030 | Total loss: 1.116 | Reg loss: 0.043 | Tree loss: 1.116 | Accuracy: 0.595703 | 7.074 sec/iter\n",
      "Epoch: 56 | Batch: 018 / 030 | Total loss: 1.122 | Reg loss: 0.043 | Tree loss: 1.122 | Accuracy: 0.572266 | 7.074 sec/iter\n",
      "Epoch: 56 | Batch: 019 / 030 | Total loss: 1.088 | Reg loss: 0.043 | Tree loss: 1.088 | Accuracy: 0.589844 | 7.074 sec/iter\n",
      "Epoch: 56 | Batch: 020 / 030 | Total loss: 1.108 | Reg loss: 0.043 | Tree loss: 1.108 | Accuracy: 0.554688 | 7.074 sec/iter\n",
      "Epoch: 56 | Batch: 021 / 030 | Total loss: 1.086 | Reg loss: 0.043 | Tree loss: 1.086 | Accuracy: 0.574219 | 7.074 sec/iter\n",
      "Epoch: 56 | Batch: 022 / 030 | Total loss: 1.076 | Reg loss: 0.043 | Tree loss: 1.076 | Accuracy: 0.609375 | 7.073 sec/iter\n",
      "Epoch: 56 | Batch: 023 / 030 | Total loss: 1.069 | Reg loss: 0.043 | Tree loss: 1.069 | Accuracy: 0.582031 | 7.072 sec/iter\n",
      "Epoch: 56 | Batch: 024 / 030 | Total loss: 1.069 | Reg loss: 0.043 | Tree loss: 1.069 | Accuracy: 0.585938 | 7.071 sec/iter\n",
      "Epoch: 56 | Batch: 025 / 030 | Total loss: 1.033 | Reg loss: 0.044 | Tree loss: 1.033 | Accuracy: 0.591797 | 7.071 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 56 | Batch: 026 / 030 | Total loss: 1.030 | Reg loss: 0.044 | Tree loss: 1.030 | Accuracy: 0.626953 | 7.072 sec/iter\n",
      "Epoch: 56 | Batch: 027 / 030 | Total loss: 1.043 | Reg loss: 0.044 | Tree loss: 1.043 | Accuracy: 0.558594 | 7.072 sec/iter\n",
      "Epoch: 56 | Batch: 028 / 030 | Total loss: 1.038 | Reg loss: 0.044 | Tree loss: 1.038 | Accuracy: 0.550781 | 7.072 sec/iter\n",
      "Epoch: 56 | Batch: 029 / 030 | Total loss: 1.037 | Reg loss: 0.044 | Tree loss: 1.037 | Accuracy: 0.543689 | 7.07 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 57 | Batch: 000 / 030 | Total loss: 1.377 | Reg loss: 0.042 | Tree loss: 1.377 | Accuracy: 0.597656 | 7.087 sec/iter\n",
      "Epoch: 57 | Batch: 001 / 030 | Total loss: 1.384 | Reg loss: 0.042 | Tree loss: 1.384 | Accuracy: 0.576172 | 7.087 sec/iter\n",
      "Epoch: 57 | Batch: 002 / 030 | Total loss: 1.365 | Reg loss: 0.042 | Tree loss: 1.365 | Accuracy: 0.595703 | 7.087 sec/iter\n",
      "Epoch: 57 | Batch: 003 / 030 | Total loss: 1.318 | Reg loss: 0.042 | Tree loss: 1.318 | Accuracy: 0.582031 | 7.087 sec/iter\n",
      "Epoch: 57 | Batch: 004 / 030 | Total loss: 1.289 | Reg loss: 0.042 | Tree loss: 1.289 | Accuracy: 0.576172 | 7.087 sec/iter\n",
      "Epoch: 57 | Batch: 005 / 030 | Total loss: 1.285 | Reg loss: 0.042 | Tree loss: 1.285 | Accuracy: 0.595703 | 7.087 sec/iter\n",
      "Epoch: 57 | Batch: 006 / 030 | Total loss: 1.290 | Reg loss: 0.042 | Tree loss: 1.290 | Accuracy: 0.591797 | 7.087 sec/iter\n",
      "Epoch: 57 | Batch: 007 / 030 | Total loss: 1.265 | Reg loss: 0.042 | Tree loss: 1.265 | Accuracy: 0.580078 | 7.087 sec/iter\n",
      "Epoch: 57 | Batch: 008 / 030 | Total loss: 1.260 | Reg loss: 0.042 | Tree loss: 1.260 | Accuracy: 0.550781 | 7.087 sec/iter\n",
      "Epoch: 57 | Batch: 009 / 030 | Total loss: 1.241 | Reg loss: 0.042 | Tree loss: 1.241 | Accuracy: 0.597656 | 7.087 sec/iter\n",
      "Epoch: 57 | Batch: 010 / 030 | Total loss: 1.217 | Reg loss: 0.042 | Tree loss: 1.217 | Accuracy: 0.568359 | 7.087 sec/iter\n",
      "Epoch: 57 | Batch: 011 / 030 | Total loss: 1.202 | Reg loss: 0.042 | Tree loss: 1.202 | Accuracy: 0.568359 | 7.087 sec/iter\n",
      "Epoch: 57 | Batch: 012 / 030 | Total loss: 1.177 | Reg loss: 0.042 | Tree loss: 1.177 | Accuracy: 0.578125 | 7.087 sec/iter\n",
      "Epoch: 57 | Batch: 013 / 030 | Total loss: 1.191 | Reg loss: 0.043 | Tree loss: 1.191 | Accuracy: 0.568359 | 7.087 sec/iter\n",
      "Epoch: 57 | Batch: 014 / 030 | Total loss: 1.135 | Reg loss: 0.043 | Tree loss: 1.135 | Accuracy: 0.601562 | 7.087 sec/iter\n",
      "Epoch: 57 | Batch: 015 / 030 | Total loss: 1.134 | Reg loss: 0.043 | Tree loss: 1.134 | Accuracy: 0.589844 | 7.086 sec/iter\n",
      "Epoch: 57 | Batch: 016 / 030 | Total loss: 1.149 | Reg loss: 0.043 | Tree loss: 1.149 | Accuracy: 0.542969 | 7.084 sec/iter\n",
      "Epoch: 57 | Batch: 017 / 030 | Total loss: 1.108 | Reg loss: 0.043 | Tree loss: 1.108 | Accuracy: 0.564453 | 7.083 sec/iter\n",
      "Epoch: 57 | Batch: 018 / 030 | Total loss: 1.119 | Reg loss: 0.043 | Tree loss: 1.119 | Accuracy: 0.556641 | 7.081 sec/iter\n",
      "Epoch: 57 | Batch: 019 / 030 | Total loss: 1.109 | Reg loss: 0.043 | Tree loss: 1.109 | Accuracy: 0.574219 | 7.08 sec/iter\n",
      "Epoch: 57 | Batch: 020 / 030 | Total loss: 1.085 | Reg loss: 0.043 | Tree loss: 1.085 | Accuracy: 0.619141 | 7.08 sec/iter\n",
      "Epoch: 57 | Batch: 021 / 030 | Total loss: 1.080 | Reg loss: 0.043 | Tree loss: 1.080 | Accuracy: 0.583984 | 7.081 sec/iter\n",
      "Epoch: 57 | Batch: 022 / 030 | Total loss: 1.064 | Reg loss: 0.043 | Tree loss: 1.064 | Accuracy: 0.621094 | 7.081 sec/iter\n",
      "Epoch: 57 | Batch: 023 / 030 | Total loss: 1.051 | Reg loss: 0.043 | Tree loss: 1.051 | Accuracy: 0.580078 | 7.081 sec/iter\n",
      "Epoch: 57 | Batch: 024 / 030 | Total loss: 1.032 | Reg loss: 0.043 | Tree loss: 1.032 | Accuracy: 0.574219 | 7.081 sec/iter\n",
      "Epoch: 57 | Batch: 025 / 030 | Total loss: 1.043 | Reg loss: 0.043 | Tree loss: 1.043 | Accuracy: 0.582031 | 7.08 sec/iter\n",
      "Epoch: 57 | Batch: 026 / 030 | Total loss: 1.010 | Reg loss: 0.044 | Tree loss: 1.010 | Accuracy: 0.611328 | 7.08 sec/iter\n",
      "Epoch: 57 | Batch: 027 / 030 | Total loss: 1.013 | Reg loss: 0.044 | Tree loss: 1.013 | Accuracy: 0.568359 | 7.08 sec/iter\n",
      "Epoch: 57 | Batch: 028 / 030 | Total loss: 1.005 | Reg loss: 0.044 | Tree loss: 1.005 | Accuracy: 0.603516 | 7.079 sec/iter\n",
      "Epoch: 57 | Batch: 029 / 030 | Total loss: 1.008 | Reg loss: 0.044 | Tree loss: 1.008 | Accuracy: 0.582524 | 7.077 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 58 | Batch: 000 / 030 | Total loss: 1.359 | Reg loss: 0.042 | Tree loss: 1.359 | Accuracy: 0.625000 | 7.082 sec/iter\n",
      "Epoch: 58 | Batch: 001 / 030 | Total loss: 1.355 | Reg loss: 0.042 | Tree loss: 1.355 | Accuracy: 0.591797 | 7.082 sec/iter\n",
      "Epoch: 58 | Batch: 002 / 030 | Total loss: 1.347 | Reg loss: 0.042 | Tree loss: 1.347 | Accuracy: 0.558594 | 7.082 sec/iter\n",
      "Epoch: 58 | Batch: 003 / 030 | Total loss: 1.300 | Reg loss: 0.042 | Tree loss: 1.300 | Accuracy: 0.578125 | 7.082 sec/iter\n",
      "Epoch: 58 | Batch: 004 / 030 | Total loss: 1.288 | Reg loss: 0.042 | Tree loss: 1.288 | Accuracy: 0.591797 | 7.082 sec/iter\n",
      "Epoch: 58 | Batch: 005 / 030 | Total loss: 1.296 | Reg loss: 0.042 | Tree loss: 1.296 | Accuracy: 0.583984 | 7.082 sec/iter\n",
      "Epoch: 58 | Batch: 006 / 030 | Total loss: 1.282 | Reg loss: 0.042 | Tree loss: 1.282 | Accuracy: 0.574219 | 7.082 sec/iter\n",
      "Epoch: 58 | Batch: 007 / 030 | Total loss: 1.271 | Reg loss: 0.042 | Tree loss: 1.271 | Accuracy: 0.585938 | 7.082 sec/iter\n",
      "Epoch: 58 | Batch: 008 / 030 | Total loss: 1.246 | Reg loss: 0.042 | Tree loss: 1.246 | Accuracy: 0.572266 | 7.082 sec/iter\n",
      "Epoch: 58 | Batch: 009 / 030 | Total loss: 1.220 | Reg loss: 0.042 | Tree loss: 1.220 | Accuracy: 0.587891 | 7.082 sec/iter\n",
      "Epoch: 58 | Batch: 010 / 030 | Total loss: 1.218 | Reg loss: 0.042 | Tree loss: 1.218 | Accuracy: 0.556641 | 7.082 sec/iter\n",
      "Epoch: 58 | Batch: 011 / 030 | Total loss: 1.203 | Reg loss: 0.042 | Tree loss: 1.203 | Accuracy: 0.574219 | 7.082 sec/iter\n",
      "Epoch: 58 | Batch: 012 / 030 | Total loss: 1.176 | Reg loss: 0.042 | Tree loss: 1.176 | Accuracy: 0.582031 | 7.082 sec/iter\n",
      "Epoch: 58 | Batch: 013 / 030 | Total loss: 1.152 | Reg loss: 0.042 | Tree loss: 1.152 | Accuracy: 0.583984 | 7.082 sec/iter\n",
      "Epoch: 58 | Batch: 014 / 030 | Total loss: 1.172 | Reg loss: 0.042 | Tree loss: 1.172 | Accuracy: 0.546875 | 7.082 sec/iter\n",
      "Epoch: 58 | Batch: 015 / 030 | Total loss: 1.106 | Reg loss: 0.043 | Tree loss: 1.106 | Accuracy: 0.589844 | 7.082 sec/iter\n",
      "Epoch: 58 | Batch: 016 / 030 | Total loss: 1.120 | Reg loss: 0.043 | Tree loss: 1.120 | Accuracy: 0.580078 | 7.081 sec/iter\n",
      "Epoch: 58 | Batch: 017 / 030 | Total loss: 1.103 | Reg loss: 0.043 | Tree loss: 1.103 | Accuracy: 0.582031 | 7.079 sec/iter\n",
      "Epoch: 58 | Batch: 018 / 030 | Total loss: 1.090 | Reg loss: 0.043 | Tree loss: 1.090 | Accuracy: 0.605469 | 7.078 sec/iter\n",
      "Epoch: 58 | Batch: 019 / 030 | Total loss: 1.076 | Reg loss: 0.043 | Tree loss: 1.076 | Accuracy: 0.593750 | 7.077 sec/iter\n",
      "Epoch: 58 | Batch: 020 / 030 | Total loss: 1.064 | Reg loss: 0.043 | Tree loss: 1.064 | Accuracy: 0.595703 | 7.077 sec/iter\n",
      "Epoch: 58 | Batch: 021 / 030 | Total loss: 1.067 | Reg loss: 0.043 | Tree loss: 1.067 | Accuracy: 0.580078 | 7.077 sec/iter\n",
      "Epoch: 58 | Batch: 022 / 030 | Total loss: 1.056 | Reg loss: 0.043 | Tree loss: 1.056 | Accuracy: 0.550781 | 7.078 sec/iter\n",
      "Epoch: 58 | Batch: 023 / 030 | Total loss: 1.027 | Reg loss: 0.043 | Tree loss: 1.027 | Accuracy: 0.611328 | 7.078 sec/iter\n",
      "Epoch: 58 | Batch: 024 / 030 | Total loss: 1.049 | Reg loss: 0.043 | Tree loss: 1.049 | Accuracy: 0.515625 | 7.078 sec/iter\n",
      "Epoch: 58 | Batch: 025 / 030 | Total loss: 1.026 | Reg loss: 0.043 | Tree loss: 1.026 | Accuracy: 0.589844 | 7.078 sec/iter\n",
      "Epoch: 58 | Batch: 026 / 030 | Total loss: 1.006 | Reg loss: 0.043 | Tree loss: 1.006 | Accuracy: 0.595703 | 7.078 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 58 | Batch: 027 / 030 | Total loss: 1.015 | Reg loss: 0.043 | Tree loss: 1.015 | Accuracy: 0.601562 | 7.078 sec/iter\n",
      "Epoch: 58 | Batch: 028 / 030 | Total loss: 0.988 | Reg loss: 0.044 | Tree loss: 0.988 | Accuracy: 0.621094 | 7.078 sec/iter\n",
      "Epoch: 58 | Batch: 029 / 030 | Total loss: 1.039 | Reg loss: 0.044 | Tree loss: 1.039 | Accuracy: 0.553398 | 7.076 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 59 | Batch: 000 / 030 | Total loss: 1.342 | Reg loss: 0.042 | Tree loss: 1.342 | Accuracy: 0.560547 | 7.101 sec/iter\n",
      "Epoch: 59 | Batch: 001 / 030 | Total loss: 1.339 | Reg loss: 0.042 | Tree loss: 1.339 | Accuracy: 0.578125 | 7.101 sec/iter\n",
      "Epoch: 59 | Batch: 002 / 030 | Total loss: 1.316 | Reg loss: 0.042 | Tree loss: 1.316 | Accuracy: 0.589844 | 7.101 sec/iter\n",
      "Epoch: 59 | Batch: 003 / 030 | Total loss: 1.338 | Reg loss: 0.042 | Tree loss: 1.338 | Accuracy: 0.574219 | 7.101 sec/iter\n",
      "Epoch: 59 | Batch: 004 / 030 | Total loss: 1.318 | Reg loss: 0.042 | Tree loss: 1.318 | Accuracy: 0.576172 | 7.102 sec/iter\n",
      "Epoch: 59 | Batch: 005 / 030 | Total loss: 1.278 | Reg loss: 0.042 | Tree loss: 1.278 | Accuracy: 0.570312 | 7.102 sec/iter\n",
      "Epoch: 59 | Batch: 006 / 030 | Total loss: 1.292 | Reg loss: 0.042 | Tree loss: 1.292 | Accuracy: 0.558594 | 7.101 sec/iter\n",
      "Epoch: 59 | Batch: 007 / 030 | Total loss: 1.250 | Reg loss: 0.042 | Tree loss: 1.250 | Accuracy: 0.613281 | 7.1 sec/iter\n",
      "Epoch: 59 | Batch: 008 / 030 | Total loss: 1.235 | Reg loss: 0.042 | Tree loss: 1.235 | Accuracy: 0.599609 | 7.099 sec/iter\n",
      "Epoch: 59 | Batch: 009 / 030 | Total loss: 1.204 | Reg loss: 0.042 | Tree loss: 1.204 | Accuracy: 0.611328 | 7.099 sec/iter\n",
      "Epoch: 59 | Batch: 010 / 030 | Total loss: 1.191 | Reg loss: 0.042 | Tree loss: 1.191 | Accuracy: 0.591797 | 7.098 sec/iter\n",
      "Epoch: 59 | Batch: 011 / 030 | Total loss: 1.178 | Reg loss: 0.042 | Tree loss: 1.178 | Accuracy: 0.591797 | 7.098 sec/iter\n",
      "Epoch: 59 | Batch: 012 / 030 | Total loss: 1.142 | Reg loss: 0.042 | Tree loss: 1.142 | Accuracy: 0.605469 | 7.098 sec/iter\n",
      "Epoch: 59 | Batch: 013 / 030 | Total loss: 1.165 | Reg loss: 0.042 | Tree loss: 1.165 | Accuracy: 0.548828 | 7.098 sec/iter\n",
      "Epoch: 59 | Batch: 014 / 030 | Total loss: 1.116 | Reg loss: 0.042 | Tree loss: 1.116 | Accuracy: 0.601562 | 7.099 sec/iter\n",
      "Epoch: 59 | Batch: 015 / 030 | Total loss: 1.137 | Reg loss: 0.042 | Tree loss: 1.137 | Accuracy: 0.560547 | 7.099 sec/iter\n",
      "Epoch: 59 | Batch: 016 / 030 | Total loss: 1.098 | Reg loss: 0.043 | Tree loss: 1.098 | Accuracy: 0.580078 | 7.099 sec/iter\n",
      "Epoch: 59 | Batch: 017 / 030 | Total loss: 1.109 | Reg loss: 0.043 | Tree loss: 1.109 | Accuracy: 0.562500 | 7.099 sec/iter\n",
      "Epoch: 59 | Batch: 018 / 030 | Total loss: 1.068 | Reg loss: 0.043 | Tree loss: 1.068 | Accuracy: 0.597656 | 7.099 sec/iter\n",
      "Epoch: 59 | Batch: 019 / 030 | Total loss: 1.074 | Reg loss: 0.043 | Tree loss: 1.074 | Accuracy: 0.580078 | 7.099 sec/iter\n",
      "Epoch: 59 | Batch: 020 / 030 | Total loss: 1.047 | Reg loss: 0.043 | Tree loss: 1.047 | Accuracy: 0.595703 | 7.098 sec/iter\n",
      "Epoch: 59 | Batch: 021 / 030 | Total loss: 1.059 | Reg loss: 0.043 | Tree loss: 1.059 | Accuracy: 0.550781 | 7.098 sec/iter\n",
      "Epoch: 59 | Batch: 022 / 030 | Total loss: 1.009 | Reg loss: 0.043 | Tree loss: 1.009 | Accuracy: 0.636719 | 7.098 sec/iter\n",
      "Epoch: 59 | Batch: 023 / 030 | Total loss: 1.031 | Reg loss: 0.043 | Tree loss: 1.031 | Accuracy: 0.582031 | 7.098 sec/iter\n",
      "Epoch: 59 | Batch: 024 / 030 | Total loss: 1.024 | Reg loss: 0.043 | Tree loss: 1.024 | Accuracy: 0.578125 | 7.098 sec/iter\n",
      "Epoch: 59 | Batch: 025 / 030 | Total loss: 1.016 | Reg loss: 0.043 | Tree loss: 1.016 | Accuracy: 0.570312 | 7.098 sec/iter\n",
      "Epoch: 59 | Batch: 026 / 030 | Total loss: 1.008 | Reg loss: 0.043 | Tree loss: 1.008 | Accuracy: 0.585938 | 7.097 sec/iter\n",
      "Epoch: 59 | Batch: 027 / 030 | Total loss: 0.998 | Reg loss: 0.043 | Tree loss: 0.998 | Accuracy: 0.570312 | 7.097 sec/iter\n",
      "Epoch: 59 | Batch: 028 / 030 | Total loss: 0.991 | Reg loss: 0.043 | Tree loss: 0.991 | Accuracy: 0.585938 | 7.097 sec/iter\n",
      "Epoch: 59 | Batch: 029 / 030 | Total loss: 1.003 | Reg loss: 0.044 | Tree loss: 1.003 | Accuracy: 0.543689 | 7.095 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 60 | Batch: 000 / 030 | Total loss: 1.356 | Reg loss: 0.042 | Tree loss: 1.356 | Accuracy: 0.599609 | 7.104 sec/iter\n",
      "Epoch: 60 | Batch: 001 / 030 | Total loss: 1.313 | Reg loss: 0.042 | Tree loss: 1.313 | Accuracy: 0.611328 | 7.104 sec/iter\n",
      "Epoch: 60 | Batch: 002 / 030 | Total loss: 1.380 | Reg loss: 0.042 | Tree loss: 1.380 | Accuracy: 0.527344 | 7.104 sec/iter\n",
      "Epoch: 60 | Batch: 003 / 030 | Total loss: 1.299 | Reg loss: 0.042 | Tree loss: 1.299 | Accuracy: 0.578125 | 7.103 sec/iter\n",
      "Epoch: 60 | Batch: 004 / 030 | Total loss: 1.298 | Reg loss: 0.042 | Tree loss: 1.298 | Accuracy: 0.578125 | 7.102 sec/iter\n",
      "Epoch: 60 | Batch: 005 / 030 | Total loss: 1.209 | Reg loss: 0.042 | Tree loss: 1.209 | Accuracy: 0.615234 | 7.101 sec/iter\n",
      "Epoch: 60 | Batch: 006 / 030 | Total loss: 1.234 | Reg loss: 0.042 | Tree loss: 1.234 | Accuracy: 0.591797 | 7.099 sec/iter\n",
      "Epoch: 60 | Batch: 007 / 030 | Total loss: 1.239 | Reg loss: 0.042 | Tree loss: 1.239 | Accuracy: 0.617188 | 7.098 sec/iter\n",
      "Epoch: 60 | Batch: 008 / 030 | Total loss: 1.197 | Reg loss: 0.042 | Tree loss: 1.197 | Accuracy: 0.623047 | 7.097 sec/iter\n",
      "Epoch: 60 | Batch: 009 / 030 | Total loss: 1.204 | Reg loss: 0.042 | Tree loss: 1.204 | Accuracy: 0.578125 | 7.097 sec/iter\n",
      "Epoch: 60 | Batch: 010 / 030 | Total loss: 1.176 | Reg loss: 0.042 | Tree loss: 1.176 | Accuracy: 0.580078 | 7.097 sec/iter\n",
      "Epoch: 60 | Batch: 011 / 030 | Total loss: 1.181 | Reg loss: 0.042 | Tree loss: 1.181 | Accuracy: 0.583984 | 7.097 sec/iter\n",
      "Epoch: 60 | Batch: 012 / 030 | Total loss: 1.156 | Reg loss: 0.042 | Tree loss: 1.156 | Accuracy: 0.587891 | 7.097 sec/iter\n",
      "Epoch: 60 | Batch: 013 / 030 | Total loss: 1.098 | Reg loss: 0.042 | Tree loss: 1.098 | Accuracy: 0.619141 | 7.097 sec/iter\n",
      "Epoch: 60 | Batch: 014 / 030 | Total loss: 1.120 | Reg loss: 0.042 | Tree loss: 1.120 | Accuracy: 0.578125 | 7.096 sec/iter\n",
      "Epoch: 60 | Batch: 015 / 030 | Total loss: 1.119 | Reg loss: 0.042 | Tree loss: 1.119 | Accuracy: 0.578125 | 7.096 sec/iter\n",
      "Epoch: 60 | Batch: 016 / 030 | Total loss: 1.100 | Reg loss: 0.042 | Tree loss: 1.100 | Accuracy: 0.619141 | 7.096 sec/iter\n",
      "Epoch: 60 | Batch: 017 / 030 | Total loss: 1.088 | Reg loss: 0.043 | Tree loss: 1.088 | Accuracy: 0.578125 | 7.096 sec/iter\n",
      "Epoch: 60 | Batch: 018 / 030 | Total loss: 1.076 | Reg loss: 0.043 | Tree loss: 1.076 | Accuracy: 0.585938 | 7.096 sec/iter\n",
      "Epoch: 60 | Batch: 019 / 030 | Total loss: 1.076 | Reg loss: 0.043 | Tree loss: 1.076 | Accuracy: 0.576172 | 7.095 sec/iter\n",
      "Epoch: 60 | Batch: 020 / 030 | Total loss: 1.034 | Reg loss: 0.043 | Tree loss: 1.034 | Accuracy: 0.615234 | 7.096 sec/iter\n",
      "Epoch: 60 | Batch: 021 / 030 | Total loss: 1.062 | Reg loss: 0.043 | Tree loss: 1.062 | Accuracy: 0.558594 | 7.096 sec/iter\n",
      "Epoch: 60 | Batch: 022 / 030 | Total loss: 1.021 | Reg loss: 0.043 | Tree loss: 1.021 | Accuracy: 0.582031 | 7.096 sec/iter\n",
      "Epoch: 60 | Batch: 023 / 030 | Total loss: 1.024 | Reg loss: 0.043 | Tree loss: 1.024 | Accuracy: 0.593750 | 7.096 sec/iter\n",
      "Epoch: 60 | Batch: 024 / 030 | Total loss: 1.045 | Reg loss: 0.043 | Tree loss: 1.045 | Accuracy: 0.509766 | 7.096 sec/iter\n",
      "Epoch: 60 | Batch: 025 / 030 | Total loss: 1.025 | Reg loss: 0.043 | Tree loss: 1.025 | Accuracy: 0.556641 | 7.096 sec/iter\n",
      "Epoch: 60 | Batch: 026 / 030 | Total loss: 0.992 | Reg loss: 0.043 | Tree loss: 0.992 | Accuracy: 0.574219 | 7.096 sec/iter\n",
      "Epoch: 60 | Batch: 027 / 030 | Total loss: 1.009 | Reg loss: 0.043 | Tree loss: 1.009 | Accuracy: 0.552734 | 7.096 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 60 | Batch: 028 / 030 | Total loss: 0.989 | Reg loss: 0.043 | Tree loss: 0.989 | Accuracy: 0.548828 | 7.096 sec/iter\n",
      "Epoch: 60 | Batch: 029 / 030 | Total loss: 0.973 | Reg loss: 0.043 | Tree loss: 0.973 | Accuracy: 0.592233 | 7.094 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 61 | Batch: 000 / 030 | Total loss: 1.333 | Reg loss: 0.042 | Tree loss: 1.333 | Accuracy: 0.570312 | 7.11 sec/iter\n",
      "Epoch: 61 | Batch: 001 / 030 | Total loss: 1.350 | Reg loss: 0.042 | Tree loss: 1.350 | Accuracy: 0.550781 | 7.109 sec/iter\n",
      "Epoch: 61 | Batch: 002 / 030 | Total loss: 1.343 | Reg loss: 0.042 | Tree loss: 1.343 | Accuracy: 0.558594 | 7.107 sec/iter\n",
      "Epoch: 61 | Batch: 003 / 030 | Total loss: 1.293 | Reg loss: 0.042 | Tree loss: 1.293 | Accuracy: 0.574219 | 7.106 sec/iter\n",
      "Epoch: 61 | Batch: 004 / 030 | Total loss: 1.297 | Reg loss: 0.042 | Tree loss: 1.297 | Accuracy: 0.572266 | 7.107 sec/iter\n",
      "Epoch: 61 | Batch: 005 / 030 | Total loss: 1.271 | Reg loss: 0.042 | Tree loss: 1.271 | Accuracy: 0.628906 | 7.107 sec/iter\n",
      "Epoch: 61 | Batch: 006 / 030 | Total loss: 1.264 | Reg loss: 0.042 | Tree loss: 1.264 | Accuracy: 0.552734 | 7.107 sec/iter\n",
      "Epoch: 61 | Batch: 007 / 030 | Total loss: 1.238 | Reg loss: 0.042 | Tree loss: 1.238 | Accuracy: 0.537109 | 7.107 sec/iter\n",
      "Epoch: 61 | Batch: 008 / 030 | Total loss: 1.221 | Reg loss: 0.042 | Tree loss: 1.221 | Accuracy: 0.556641 | 7.107 sec/iter\n",
      "Epoch: 61 | Batch: 009 / 030 | Total loss: 1.180 | Reg loss: 0.042 | Tree loss: 1.180 | Accuracy: 0.601562 | 7.107 sec/iter\n",
      "Epoch: 61 | Batch: 010 / 030 | Total loss: 1.162 | Reg loss: 0.042 | Tree loss: 1.162 | Accuracy: 0.605469 | 7.107 sec/iter\n",
      "Epoch: 61 | Batch: 011 / 030 | Total loss: 1.160 | Reg loss: 0.042 | Tree loss: 1.160 | Accuracy: 0.552734 | 7.107 sec/iter\n",
      "Epoch: 61 | Batch: 012 / 030 | Total loss: 1.149 | Reg loss: 0.042 | Tree loss: 1.149 | Accuracy: 0.605469 | 7.107 sec/iter\n",
      "Epoch: 61 | Batch: 013 / 030 | Total loss: 1.117 | Reg loss: 0.042 | Tree loss: 1.117 | Accuracy: 0.601562 | 7.107 sec/iter\n",
      "Epoch: 61 | Batch: 014 / 030 | Total loss: 1.123 | Reg loss: 0.042 | Tree loss: 1.123 | Accuracy: 0.578125 | 7.107 sec/iter\n",
      "Epoch: 61 | Batch: 015 / 030 | Total loss: 1.103 | Reg loss: 0.042 | Tree loss: 1.103 | Accuracy: 0.595703 | 7.107 sec/iter\n",
      "Epoch: 61 | Batch: 016 / 030 | Total loss: 1.095 | Reg loss: 0.042 | Tree loss: 1.095 | Accuracy: 0.583984 | 7.107 sec/iter\n",
      "Epoch: 61 | Batch: 017 / 030 | Total loss: 1.090 | Reg loss: 0.042 | Tree loss: 1.090 | Accuracy: 0.576172 | 7.107 sec/iter\n",
      "Epoch: 61 | Batch: 018 / 030 | Total loss: 1.067 | Reg loss: 0.042 | Tree loss: 1.067 | Accuracy: 0.593750 | 7.107 sec/iter\n",
      "Epoch: 61 | Batch: 019 / 030 | Total loss: 1.030 | Reg loss: 0.043 | Tree loss: 1.030 | Accuracy: 0.619141 | 7.107 sec/iter\n",
      "Epoch: 61 | Batch: 020 / 030 | Total loss: 1.049 | Reg loss: 0.043 | Tree loss: 1.049 | Accuracy: 0.548828 | 7.107 sec/iter\n",
      "Epoch: 61 | Batch: 021 / 030 | Total loss: 1.040 | Reg loss: 0.043 | Tree loss: 1.040 | Accuracy: 0.566406 | 7.107 sec/iter\n",
      "Epoch: 61 | Batch: 022 / 030 | Total loss: 1.019 | Reg loss: 0.043 | Tree loss: 1.019 | Accuracy: 0.585938 | 7.107 sec/iter\n",
      "Epoch: 61 | Batch: 023 / 030 | Total loss: 0.996 | Reg loss: 0.043 | Tree loss: 0.996 | Accuracy: 0.593750 | 7.107 sec/iter\n",
      "Epoch: 61 | Batch: 024 / 030 | Total loss: 0.980 | Reg loss: 0.043 | Tree loss: 0.980 | Accuracy: 0.619141 | 7.107 sec/iter\n",
      "Epoch: 61 | Batch: 025 / 030 | Total loss: 0.976 | Reg loss: 0.043 | Tree loss: 0.976 | Accuracy: 0.617188 | 7.107 sec/iter\n",
      "Epoch: 61 | Batch: 026 / 030 | Total loss: 1.001 | Reg loss: 0.043 | Tree loss: 1.001 | Accuracy: 0.566406 | 7.107 sec/iter\n",
      "Epoch: 61 | Batch: 027 / 030 | Total loss: 0.969 | Reg loss: 0.043 | Tree loss: 0.969 | Accuracy: 0.599609 | 7.107 sec/iter\n",
      "Epoch: 61 | Batch: 028 / 030 | Total loss: 0.964 | Reg loss: 0.043 | Tree loss: 0.964 | Accuracy: 0.591797 | 7.107 sec/iter\n",
      "Epoch: 61 | Batch: 029 / 030 | Total loss: 0.953 | Reg loss: 0.043 | Tree loss: 0.953 | Accuracy: 0.563107 | 7.105 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 62 | Batch: 000 / 030 | Total loss: 1.329 | Reg loss: 0.042 | Tree loss: 1.329 | Accuracy: 0.583984 | 7.106 sec/iter\n",
      "Epoch: 62 | Batch: 001 / 030 | Total loss: 1.326 | Reg loss: 0.041 | Tree loss: 1.326 | Accuracy: 0.558594 | 7.106 sec/iter\n",
      "Epoch: 62 | Batch: 002 / 030 | Total loss: 1.294 | Reg loss: 0.041 | Tree loss: 1.294 | Accuracy: 0.599609 | 7.106 sec/iter\n",
      "Epoch: 62 | Batch: 003 / 030 | Total loss: 1.321 | Reg loss: 0.041 | Tree loss: 1.321 | Accuracy: 0.544922 | 7.106 sec/iter\n",
      "Epoch: 62 | Batch: 004 / 030 | Total loss: 1.281 | Reg loss: 0.042 | Tree loss: 1.281 | Accuracy: 0.587891 | 7.106 sec/iter\n",
      "Epoch: 62 | Batch: 005 / 030 | Total loss: 1.259 | Reg loss: 0.042 | Tree loss: 1.259 | Accuracy: 0.578125 | 7.106 sec/iter\n",
      "Epoch: 62 | Batch: 006 / 030 | Total loss: 1.235 | Reg loss: 0.042 | Tree loss: 1.235 | Accuracy: 0.560547 | 7.106 sec/iter\n",
      "Epoch: 62 | Batch: 007 / 030 | Total loss: 1.213 | Reg loss: 0.042 | Tree loss: 1.213 | Accuracy: 0.613281 | 7.105 sec/iter\n",
      "Epoch: 62 | Batch: 008 / 030 | Total loss: 1.203 | Reg loss: 0.042 | Tree loss: 1.203 | Accuracy: 0.603516 | 7.105 sec/iter\n",
      "Epoch: 62 | Batch: 009 / 030 | Total loss: 1.211 | Reg loss: 0.042 | Tree loss: 1.211 | Accuracy: 0.556641 | 7.105 sec/iter\n",
      "Epoch: 62 | Batch: 010 / 030 | Total loss: 1.136 | Reg loss: 0.042 | Tree loss: 1.136 | Accuracy: 0.587891 | 7.104 sec/iter\n",
      "Epoch: 62 | Batch: 011 / 030 | Total loss: 1.140 | Reg loss: 0.042 | Tree loss: 1.140 | Accuracy: 0.625000 | 7.104 sec/iter\n",
      "Epoch: 62 | Batch: 012 / 030 | Total loss: 1.160 | Reg loss: 0.042 | Tree loss: 1.160 | Accuracy: 0.566406 | 7.104 sec/iter\n",
      "Epoch: 62 | Batch: 013 / 030 | Total loss: 1.105 | Reg loss: 0.042 | Tree loss: 1.105 | Accuracy: 0.613281 | 7.104 sec/iter\n",
      "Epoch: 62 | Batch: 014 / 030 | Total loss: 1.124 | Reg loss: 0.042 | Tree loss: 1.124 | Accuracy: 0.546875 | 7.104 sec/iter\n",
      "Epoch: 62 | Batch: 015 / 030 | Total loss: 1.096 | Reg loss: 0.042 | Tree loss: 1.096 | Accuracy: 0.574219 | 7.104 sec/iter\n",
      "Epoch: 62 | Batch: 016 / 030 | Total loss: 1.069 | Reg loss: 0.042 | Tree loss: 1.069 | Accuracy: 0.595703 | 7.104 sec/iter\n",
      "Epoch: 62 | Batch: 017 / 030 | Total loss: 1.058 | Reg loss: 0.042 | Tree loss: 1.058 | Accuracy: 0.585938 | 7.104 sec/iter\n",
      "Epoch: 62 | Batch: 018 / 030 | Total loss: 1.037 | Reg loss: 0.042 | Tree loss: 1.037 | Accuracy: 0.619141 | 7.104 sec/iter\n",
      "Epoch: 62 | Batch: 019 / 030 | Total loss: 1.024 | Reg loss: 0.042 | Tree loss: 1.024 | Accuracy: 0.599609 | 7.105 sec/iter\n",
      "Epoch: 62 | Batch: 020 / 030 | Total loss: 1.030 | Reg loss: 0.043 | Tree loss: 1.030 | Accuracy: 0.564453 | 7.105 sec/iter\n",
      "Epoch: 62 | Batch: 021 / 030 | Total loss: 1.037 | Reg loss: 0.043 | Tree loss: 1.037 | Accuracy: 0.574219 | 7.105 sec/iter\n",
      "Epoch: 62 | Batch: 022 / 030 | Total loss: 1.017 | Reg loss: 0.043 | Tree loss: 1.017 | Accuracy: 0.601562 | 7.105 sec/iter\n",
      "Epoch: 62 | Batch: 023 / 030 | Total loss: 1.019 | Reg loss: 0.043 | Tree loss: 1.019 | Accuracy: 0.562500 | 7.105 sec/iter\n",
      "Epoch: 62 | Batch: 024 / 030 | Total loss: 1.014 | Reg loss: 0.043 | Tree loss: 1.014 | Accuracy: 0.560547 | 7.105 sec/iter\n",
      "Epoch: 62 | Batch: 025 / 030 | Total loss: 1.008 | Reg loss: 0.043 | Tree loss: 1.008 | Accuracy: 0.566406 | 7.105 sec/iter\n",
      "Epoch: 62 | Batch: 026 / 030 | Total loss: 0.974 | Reg loss: 0.043 | Tree loss: 0.974 | Accuracy: 0.587891 | 7.105 sec/iter\n",
      "Epoch: 62 | Batch: 027 / 030 | Total loss: 0.972 | Reg loss: 0.043 | Tree loss: 0.972 | Accuracy: 0.587891 | 7.105 sec/iter\n",
      "Epoch: 62 | Batch: 028 / 030 | Total loss: 0.968 | Reg loss: 0.043 | Tree loss: 0.968 | Accuracy: 0.597656 | 7.104 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 62 | Batch: 029 / 030 | Total loss: 0.971 | Reg loss: 0.043 | Tree loss: 0.971 | Accuracy: 0.563107 | 7.102 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 63 | Batch: 000 / 030 | Total loss: 1.366 | Reg loss: 0.041 | Tree loss: 1.366 | Accuracy: 0.566406 | 7.107 sec/iter\n",
      "Epoch: 63 | Batch: 001 / 030 | Total loss: 1.279 | Reg loss: 0.041 | Tree loss: 1.279 | Accuracy: 0.601562 | 7.106 sec/iter\n",
      "Epoch: 63 | Batch: 002 / 030 | Total loss: 1.273 | Reg loss: 0.041 | Tree loss: 1.273 | Accuracy: 0.580078 | 7.106 sec/iter\n",
      "Epoch: 63 | Batch: 003 / 030 | Total loss: 1.292 | Reg loss: 0.041 | Tree loss: 1.292 | Accuracy: 0.578125 | 7.106 sec/iter\n",
      "Epoch: 63 | Batch: 004 / 030 | Total loss: 1.246 | Reg loss: 0.041 | Tree loss: 1.246 | Accuracy: 0.605469 | 7.106 sec/iter\n",
      "Epoch: 63 | Batch: 005 / 030 | Total loss: 1.255 | Reg loss: 0.041 | Tree loss: 1.255 | Accuracy: 0.556641 | 7.107 sec/iter\n",
      "Epoch: 63 | Batch: 006 / 030 | Total loss: 1.262 | Reg loss: 0.041 | Tree loss: 1.262 | Accuracy: 0.597656 | 7.107 sec/iter\n",
      "Epoch: 63 | Batch: 007 / 030 | Total loss: 1.193 | Reg loss: 0.041 | Tree loss: 1.193 | Accuracy: 0.605469 | 7.106 sec/iter\n",
      "Epoch: 63 | Batch: 008 / 030 | Total loss: 1.202 | Reg loss: 0.042 | Tree loss: 1.202 | Accuracy: 0.566406 | 7.106 sec/iter\n",
      "Epoch: 63 | Batch: 009 / 030 | Total loss: 1.188 | Reg loss: 0.042 | Tree loss: 1.188 | Accuracy: 0.593750 | 7.106 sec/iter\n",
      "Epoch: 63 | Batch: 010 / 030 | Total loss: 1.176 | Reg loss: 0.042 | Tree loss: 1.176 | Accuracy: 0.580078 | 7.105 sec/iter\n",
      "Epoch: 63 | Batch: 011 / 030 | Total loss: 1.148 | Reg loss: 0.042 | Tree loss: 1.148 | Accuracy: 0.585938 | 7.105 sec/iter\n",
      "Epoch: 63 | Batch: 012 / 030 | Total loss: 1.115 | Reg loss: 0.042 | Tree loss: 1.115 | Accuracy: 0.591797 | 7.105 sec/iter\n",
      "Epoch: 63 | Batch: 013 / 030 | Total loss: 1.134 | Reg loss: 0.042 | Tree loss: 1.134 | Accuracy: 0.560547 | 7.105 sec/iter\n",
      "Epoch: 63 | Batch: 014 / 030 | Total loss: 1.102 | Reg loss: 0.042 | Tree loss: 1.102 | Accuracy: 0.580078 | 7.105 sec/iter\n",
      "Epoch: 63 | Batch: 015 / 030 | Total loss: 1.077 | Reg loss: 0.042 | Tree loss: 1.077 | Accuracy: 0.585938 | 7.105 sec/iter\n",
      "Epoch: 63 | Batch: 016 / 030 | Total loss: 1.071 | Reg loss: 0.042 | Tree loss: 1.071 | Accuracy: 0.587891 | 7.105 sec/iter\n",
      "Epoch: 63 | Batch: 017 / 030 | Total loss: 1.062 | Reg loss: 0.042 | Tree loss: 1.062 | Accuracy: 0.587891 | 7.105 sec/iter\n",
      "Epoch: 63 | Batch: 018 / 030 | Total loss: 1.025 | Reg loss: 0.042 | Tree loss: 1.025 | Accuracy: 0.628906 | 7.105 sec/iter\n",
      "Epoch: 63 | Batch: 019 / 030 | Total loss: 1.039 | Reg loss: 0.042 | Tree loss: 1.039 | Accuracy: 0.570312 | 7.105 sec/iter\n",
      "Epoch: 63 | Batch: 020 / 030 | Total loss: 1.016 | Reg loss: 0.042 | Tree loss: 1.016 | Accuracy: 0.585938 | 7.105 sec/iter\n",
      "Epoch: 63 | Batch: 021 / 030 | Total loss: 1.019 | Reg loss: 0.043 | Tree loss: 1.019 | Accuracy: 0.560547 | 7.105 sec/iter\n",
      "Epoch: 63 | Batch: 022 / 030 | Total loss: 1.001 | Reg loss: 0.043 | Tree loss: 1.001 | Accuracy: 0.585938 | 7.105 sec/iter\n",
      "Epoch: 63 | Batch: 023 / 030 | Total loss: 1.012 | Reg loss: 0.043 | Tree loss: 1.012 | Accuracy: 0.560547 | 7.105 sec/iter\n",
      "Epoch: 63 | Batch: 024 / 030 | Total loss: 0.968 | Reg loss: 0.043 | Tree loss: 0.968 | Accuracy: 0.613281 | 7.105 sec/iter\n",
      "Epoch: 63 | Batch: 025 / 030 | Total loss: 0.984 | Reg loss: 0.043 | Tree loss: 0.984 | Accuracy: 0.583984 | 7.106 sec/iter\n",
      "Epoch: 63 | Batch: 026 / 030 | Total loss: 0.985 | Reg loss: 0.043 | Tree loss: 0.985 | Accuracy: 0.568359 | 7.105 sec/iter\n",
      "Epoch: 63 | Batch: 027 / 030 | Total loss: 0.971 | Reg loss: 0.043 | Tree loss: 0.971 | Accuracy: 0.560547 | 7.104 sec/iter\n",
      "Epoch: 63 | Batch: 028 / 030 | Total loss: 0.968 | Reg loss: 0.043 | Tree loss: 0.968 | Accuracy: 0.564453 | 7.103 sec/iter\n",
      "Epoch: 63 | Batch: 029 / 030 | Total loss: 0.954 | Reg loss: 0.043 | Tree loss: 0.954 | Accuracy: 0.611650 | 7.101 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 64 | Batch: 000 / 030 | Total loss: 1.314 | Reg loss: 0.041 | Tree loss: 1.314 | Accuracy: 0.589844 | 7.106 sec/iter\n",
      "Epoch: 64 | Batch: 001 / 030 | Total loss: 1.298 | Reg loss: 0.041 | Tree loss: 1.298 | Accuracy: 0.591797 | 7.105 sec/iter\n",
      "Epoch: 64 | Batch: 002 / 030 | Total loss: 1.294 | Reg loss: 0.041 | Tree loss: 1.294 | Accuracy: 0.576172 | 7.105 sec/iter\n",
      "Epoch: 64 | Batch: 003 / 030 | Total loss: 1.292 | Reg loss: 0.041 | Tree loss: 1.292 | Accuracy: 0.570312 | 7.105 sec/iter\n",
      "Epoch: 64 | Batch: 004 / 030 | Total loss: 1.277 | Reg loss: 0.041 | Tree loss: 1.277 | Accuracy: 0.583984 | 7.105 sec/iter\n",
      "Epoch: 64 | Batch: 005 / 030 | Total loss: 1.240 | Reg loss: 0.041 | Tree loss: 1.240 | Accuracy: 0.589844 | 7.105 sec/iter\n",
      "Epoch: 64 | Batch: 006 / 030 | Total loss: 1.223 | Reg loss: 0.041 | Tree loss: 1.223 | Accuracy: 0.566406 | 7.105 sec/iter\n",
      "Epoch: 64 | Batch: 007 / 030 | Total loss: 1.187 | Reg loss: 0.041 | Tree loss: 1.187 | Accuracy: 0.593750 | 7.105 sec/iter\n",
      "Epoch: 64 | Batch: 008 / 030 | Total loss: 1.162 | Reg loss: 0.041 | Tree loss: 1.162 | Accuracy: 0.585938 | 7.105 sec/iter\n",
      "Epoch: 64 | Batch: 009 / 030 | Total loss: 1.144 | Reg loss: 0.041 | Tree loss: 1.144 | Accuracy: 0.587891 | 7.105 sec/iter\n",
      "Epoch: 64 | Batch: 010 / 030 | Total loss: 1.163 | Reg loss: 0.042 | Tree loss: 1.163 | Accuracy: 0.591797 | 7.105 sec/iter\n",
      "Epoch: 64 | Batch: 011 / 030 | Total loss: 1.106 | Reg loss: 0.042 | Tree loss: 1.106 | Accuracy: 0.603516 | 7.105 sec/iter\n",
      "Epoch: 64 | Batch: 012 / 030 | Total loss: 1.132 | Reg loss: 0.042 | Tree loss: 1.132 | Accuracy: 0.564453 | 7.105 sec/iter\n",
      "Epoch: 64 | Batch: 013 / 030 | Total loss: 1.102 | Reg loss: 0.042 | Tree loss: 1.102 | Accuracy: 0.623047 | 7.104 sec/iter\n",
      "Epoch: 64 | Batch: 014 / 030 | Total loss: 1.062 | Reg loss: 0.042 | Tree loss: 1.062 | Accuracy: 0.609375 | 7.104 sec/iter\n",
      "Epoch: 64 | Batch: 015 / 030 | Total loss: 1.076 | Reg loss: 0.042 | Tree loss: 1.076 | Accuracy: 0.578125 | 7.104 sec/iter\n",
      "Epoch: 64 | Batch: 016 / 030 | Total loss: 1.060 | Reg loss: 0.042 | Tree loss: 1.060 | Accuracy: 0.611328 | 7.104 sec/iter\n",
      "Epoch: 64 | Batch: 017 / 030 | Total loss: 1.049 | Reg loss: 0.042 | Tree loss: 1.049 | Accuracy: 0.580078 | 7.104 sec/iter\n",
      "Epoch: 64 | Batch: 018 / 030 | Total loss: 1.061 | Reg loss: 0.042 | Tree loss: 1.061 | Accuracy: 0.542969 | 7.104 sec/iter\n",
      "Epoch: 64 | Batch: 019 / 030 | Total loss: 1.036 | Reg loss: 0.042 | Tree loss: 1.036 | Accuracy: 0.607422 | 7.104 sec/iter\n",
      "Epoch: 64 | Batch: 020 / 030 | Total loss: 1.045 | Reg loss: 0.042 | Tree loss: 1.045 | Accuracy: 0.546875 | 7.104 sec/iter\n",
      "Epoch: 64 | Batch: 021 / 030 | Total loss: 1.014 | Reg loss: 0.042 | Tree loss: 1.014 | Accuracy: 0.556641 | 7.104 sec/iter\n",
      "Epoch: 64 | Batch: 022 / 030 | Total loss: 1.007 | Reg loss: 0.043 | Tree loss: 1.007 | Accuracy: 0.593750 | 7.104 sec/iter\n",
      "Epoch: 64 | Batch: 023 / 030 | Total loss: 0.994 | Reg loss: 0.043 | Tree loss: 0.994 | Accuracy: 0.576172 | 7.104 sec/iter\n",
      "Epoch: 64 | Batch: 024 / 030 | Total loss: 0.997 | Reg loss: 0.043 | Tree loss: 0.997 | Accuracy: 0.568359 | 7.104 sec/iter\n",
      "Epoch: 64 | Batch: 025 / 030 | Total loss: 0.999 | Reg loss: 0.043 | Tree loss: 0.999 | Accuracy: 0.562500 | 7.103 sec/iter\n",
      "Epoch: 64 | Batch: 026 / 030 | Total loss: 0.960 | Reg loss: 0.043 | Tree loss: 0.960 | Accuracy: 0.591797 | 7.102 sec/iter\n",
      "Epoch: 64 | Batch: 027 / 030 | Total loss: 0.964 | Reg loss: 0.043 | Tree loss: 0.964 | Accuracy: 0.572266 | 7.101 sec/iter\n",
      "Epoch: 64 | Batch: 028 / 030 | Total loss: 0.961 | Reg loss: 0.043 | Tree loss: 0.961 | Accuracy: 0.583984 | 7.101 sec/iter\n",
      "Epoch: 64 | Batch: 029 / 030 | Total loss: 0.940 | Reg loss: 0.043 | Tree loss: 0.940 | Accuracy: 0.582524 | 7.099 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 65 | Batch: 000 / 030 | Total loss: 1.309 | Reg loss: 0.041 | Tree loss: 1.309 | Accuracy: 0.585938 | 7.102 sec/iter\n",
      "Epoch: 65 | Batch: 001 / 030 | Total loss: 1.315 | Reg loss: 0.041 | Tree loss: 1.315 | Accuracy: 0.595703 | 7.102 sec/iter\n",
      "Epoch: 65 | Batch: 002 / 030 | Total loss: 1.295 | Reg loss: 0.041 | Tree loss: 1.295 | Accuracy: 0.613281 | 7.103 sec/iter\n",
      "Epoch: 65 | Batch: 003 / 030 | Total loss: 1.263 | Reg loss: 0.041 | Tree loss: 1.263 | Accuracy: 0.601562 | 7.103 sec/iter\n",
      "Epoch: 65 | Batch: 004 / 030 | Total loss: 1.264 | Reg loss: 0.041 | Tree loss: 1.264 | Accuracy: 0.585938 | 7.103 sec/iter\n",
      "Epoch: 65 | Batch: 005 / 030 | Total loss: 1.236 | Reg loss: 0.041 | Tree loss: 1.236 | Accuracy: 0.591797 | 7.103 sec/iter\n",
      "Epoch: 65 | Batch: 006 / 030 | Total loss: 1.194 | Reg loss: 0.041 | Tree loss: 1.194 | Accuracy: 0.572266 | 7.103 sec/iter\n",
      "Epoch: 65 | Batch: 007 / 030 | Total loss: 1.202 | Reg loss: 0.041 | Tree loss: 1.202 | Accuracy: 0.582031 | 7.103 sec/iter\n",
      "Epoch: 65 | Batch: 008 / 030 | Total loss: 1.187 | Reg loss: 0.041 | Tree loss: 1.187 | Accuracy: 0.554688 | 7.103 sec/iter\n",
      "Epoch: 65 | Batch: 009 / 030 | Total loss: 1.183 | Reg loss: 0.041 | Tree loss: 1.183 | Accuracy: 0.574219 | 7.103 sec/iter\n",
      "Epoch: 65 | Batch: 010 / 030 | Total loss: 1.149 | Reg loss: 0.041 | Tree loss: 1.149 | Accuracy: 0.570312 | 7.103 sec/iter\n",
      "Epoch: 65 | Batch: 011 / 030 | Total loss: 1.123 | Reg loss: 0.042 | Tree loss: 1.123 | Accuracy: 0.593750 | 7.103 sec/iter\n",
      "Epoch: 65 | Batch: 012 / 030 | Total loss: 1.089 | Reg loss: 0.042 | Tree loss: 1.089 | Accuracy: 0.613281 | 7.103 sec/iter\n",
      "Epoch: 65 | Batch: 013 / 030 | Total loss: 1.116 | Reg loss: 0.042 | Tree loss: 1.116 | Accuracy: 0.550781 | 7.102 sec/iter\n",
      "Epoch: 65 | Batch: 014 / 030 | Total loss: 1.095 | Reg loss: 0.042 | Tree loss: 1.095 | Accuracy: 0.583984 | 7.102 sec/iter\n",
      "Epoch: 65 | Batch: 015 / 030 | Total loss: 1.077 | Reg loss: 0.042 | Tree loss: 1.077 | Accuracy: 0.580078 | 7.102 sec/iter\n",
      "Epoch: 65 | Batch: 016 / 030 | Total loss: 1.040 | Reg loss: 0.042 | Tree loss: 1.040 | Accuracy: 0.615234 | 7.102 sec/iter\n",
      "Epoch: 65 | Batch: 017 / 030 | Total loss: 1.061 | Reg loss: 0.042 | Tree loss: 1.061 | Accuracy: 0.560547 | 7.102 sec/iter\n",
      "Epoch: 65 | Batch: 018 / 030 | Total loss: 1.036 | Reg loss: 0.042 | Tree loss: 1.036 | Accuracy: 0.578125 | 7.102 sec/iter\n",
      "Epoch: 65 | Batch: 019 / 030 | Total loss: 1.031 | Reg loss: 0.042 | Tree loss: 1.031 | Accuracy: 0.578125 | 7.102 sec/iter\n",
      "Epoch: 65 | Batch: 020 / 030 | Total loss: 1.006 | Reg loss: 0.042 | Tree loss: 1.006 | Accuracy: 0.591797 | 7.102 sec/iter\n",
      "Epoch: 65 | Batch: 021 / 030 | Total loss: 1.019 | Reg loss: 0.042 | Tree loss: 1.019 | Accuracy: 0.560547 | 7.102 sec/iter\n",
      "Epoch: 65 | Batch: 022 / 030 | Total loss: 0.987 | Reg loss: 0.042 | Tree loss: 0.987 | Accuracy: 0.583984 | 7.102 sec/iter\n",
      "Epoch: 65 | Batch: 023 / 030 | Total loss: 0.997 | Reg loss: 0.042 | Tree loss: 0.997 | Accuracy: 0.533203 | 7.102 sec/iter\n",
      "Epoch: 65 | Batch: 024 / 030 | Total loss: 0.977 | Reg loss: 0.043 | Tree loss: 0.977 | Accuracy: 0.578125 | 7.101 sec/iter\n",
      "Epoch: 65 | Batch: 025 / 030 | Total loss: 0.944 | Reg loss: 0.043 | Tree loss: 0.944 | Accuracy: 0.625000 | 7.099 sec/iter\n",
      "Epoch: 65 | Batch: 026 / 030 | Total loss: 0.952 | Reg loss: 0.043 | Tree loss: 0.952 | Accuracy: 0.582031 | 7.098 sec/iter\n",
      "Epoch: 65 | Batch: 027 / 030 | Total loss: 0.946 | Reg loss: 0.043 | Tree loss: 0.946 | Accuracy: 0.585938 | 7.097 sec/iter\n",
      "Epoch: 65 | Batch: 028 / 030 | Total loss: 0.951 | Reg loss: 0.043 | Tree loss: 0.951 | Accuracy: 0.570312 | 7.096 sec/iter\n",
      "Epoch: 65 | Batch: 029 / 030 | Total loss: 0.909 | Reg loss: 0.043 | Tree loss: 0.909 | Accuracy: 0.621359 | 7.094 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 66 | Batch: 000 / 030 | Total loss: 1.297 | Reg loss: 0.041 | Tree loss: 1.297 | Accuracy: 0.599609 | 7.098 sec/iter\n",
      "Epoch: 66 | Batch: 001 / 030 | Total loss: 1.274 | Reg loss: 0.041 | Tree loss: 1.274 | Accuracy: 0.583984 | 7.098 sec/iter\n",
      "Epoch: 66 | Batch: 002 / 030 | Total loss: 1.318 | Reg loss: 0.041 | Tree loss: 1.318 | Accuracy: 0.587891 | 7.098 sec/iter\n",
      "Epoch: 66 | Batch: 003 / 030 | Total loss: 1.252 | Reg loss: 0.041 | Tree loss: 1.252 | Accuracy: 0.570312 | 7.097 sec/iter\n",
      "Epoch: 66 | Batch: 004 / 030 | Total loss: 1.290 | Reg loss: 0.041 | Tree loss: 1.290 | Accuracy: 0.578125 | 7.097 sec/iter\n",
      "Epoch: 66 | Batch: 005 / 030 | Total loss: 1.220 | Reg loss: 0.041 | Tree loss: 1.220 | Accuracy: 0.576172 | 7.097 sec/iter\n",
      "Epoch: 66 | Batch: 006 / 030 | Total loss: 1.219 | Reg loss: 0.041 | Tree loss: 1.219 | Accuracy: 0.568359 | 7.097 sec/iter\n",
      "Epoch: 66 | Batch: 007 / 030 | Total loss: 1.193 | Reg loss: 0.041 | Tree loss: 1.193 | Accuracy: 0.597656 | 7.096 sec/iter\n",
      "Epoch: 66 | Batch: 008 / 030 | Total loss: 1.190 | Reg loss: 0.041 | Tree loss: 1.190 | Accuracy: 0.570312 | 7.096 sec/iter\n",
      "Epoch: 66 | Batch: 009 / 030 | Total loss: 1.193 | Reg loss: 0.041 | Tree loss: 1.193 | Accuracy: 0.558594 | 7.096 sec/iter\n",
      "Epoch: 66 | Batch: 010 / 030 | Total loss: 1.144 | Reg loss: 0.041 | Tree loss: 1.144 | Accuracy: 0.585938 | 7.096 sec/iter\n",
      "Epoch: 66 | Batch: 011 / 030 | Total loss: 1.130 | Reg loss: 0.041 | Tree loss: 1.130 | Accuracy: 0.601562 | 7.096 sec/iter\n",
      "Epoch: 66 | Batch: 012 / 030 | Total loss: 1.090 | Reg loss: 0.041 | Tree loss: 1.090 | Accuracy: 0.562500 | 7.096 sec/iter\n",
      "Epoch: 66 | Batch: 013 / 030 | Total loss: 1.111 | Reg loss: 0.042 | Tree loss: 1.111 | Accuracy: 0.568359 | 7.096 sec/iter\n",
      "Epoch: 66 | Batch: 014 / 030 | Total loss: 1.054 | Reg loss: 0.042 | Tree loss: 1.054 | Accuracy: 0.599609 | 7.097 sec/iter\n",
      "Epoch: 66 | Batch: 015 / 030 | Total loss: 1.053 | Reg loss: 0.042 | Tree loss: 1.053 | Accuracy: 0.580078 | 7.097 sec/iter\n",
      "Epoch: 66 | Batch: 016 / 030 | Total loss: 1.038 | Reg loss: 0.042 | Tree loss: 1.038 | Accuracy: 0.572266 | 7.097 sec/iter\n",
      "Epoch: 66 | Batch: 017 / 030 | Total loss: 1.037 | Reg loss: 0.042 | Tree loss: 1.037 | Accuracy: 0.597656 | 7.097 sec/iter\n",
      "Epoch: 66 | Batch: 018 / 030 | Total loss: 1.034 | Reg loss: 0.042 | Tree loss: 1.034 | Accuracy: 0.578125 | 7.097 sec/iter\n",
      "Epoch: 66 | Batch: 019 / 030 | Total loss: 1.019 | Reg loss: 0.042 | Tree loss: 1.019 | Accuracy: 0.566406 | 7.097 sec/iter\n",
      "Epoch: 66 | Batch: 020 / 030 | Total loss: 1.011 | Reg loss: 0.042 | Tree loss: 1.011 | Accuracy: 0.582031 | 7.097 sec/iter\n",
      "Epoch: 66 | Batch: 021 / 030 | Total loss: 0.993 | Reg loss: 0.042 | Tree loss: 0.993 | Accuracy: 0.603516 | 7.097 sec/iter\n",
      "Epoch: 66 | Batch: 022 / 030 | Total loss: 0.991 | Reg loss: 0.042 | Tree loss: 0.991 | Accuracy: 0.583984 | 7.097 sec/iter\n",
      "Epoch: 66 | Batch: 023 / 030 | Total loss: 0.961 | Reg loss: 0.042 | Tree loss: 0.961 | Accuracy: 0.615234 | 7.097 sec/iter\n",
      "Epoch: 66 | Batch: 024 / 030 | Total loss: 0.960 | Reg loss: 0.042 | Tree loss: 0.960 | Accuracy: 0.623047 | 7.097 sec/iter\n",
      "Epoch: 66 | Batch: 025 / 030 | Total loss: 0.960 | Reg loss: 0.043 | Tree loss: 0.960 | Accuracy: 0.585938 | 7.097 sec/iter\n",
      "Epoch: 66 | Batch: 026 / 030 | Total loss: 0.960 | Reg loss: 0.043 | Tree loss: 0.960 | Accuracy: 0.587891 | 7.096 sec/iter\n",
      "Epoch: 66 | Batch: 027 / 030 | Total loss: 0.940 | Reg loss: 0.043 | Tree loss: 0.940 | Accuracy: 0.587891 | 7.095 sec/iter\n",
      "Epoch: 66 | Batch: 028 / 030 | Total loss: 0.964 | Reg loss: 0.043 | Tree loss: 0.964 | Accuracy: 0.529297 | 7.094 sec/iter\n",
      "Epoch: 66 | Batch: 029 / 030 | Total loss: 0.920 | Reg loss: 0.043 | Tree loss: 0.920 | Accuracy: 0.572816 | 7.092 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 10: 0.9821428571428573\n",
      "Epoch: 67 | Batch: 000 / 030 | Total loss: 1.325 | Reg loss: 0.041 | Tree loss: 1.325 | Accuracy: 0.548828 | 7.097 sec/iter\n",
      "Epoch: 67 | Batch: 001 / 030 | Total loss: 1.304 | Reg loss: 0.041 | Tree loss: 1.304 | Accuracy: 0.574219 | 7.097 sec/iter\n",
      "Epoch: 67 | Batch: 002 / 030 | Total loss: 1.279 | Reg loss: 0.041 | Tree loss: 1.279 | Accuracy: 0.556641 | 7.097 sec/iter\n",
      "Epoch: 67 | Batch: 003 / 030 | Total loss: 1.284 | Reg loss: 0.041 | Tree loss: 1.284 | Accuracy: 0.599609 | 7.097 sec/iter\n",
      "Epoch: 67 | Batch: 004 / 030 | Total loss: 1.249 | Reg loss: 0.041 | Tree loss: 1.249 | Accuracy: 0.589844 | 7.097 sec/iter\n",
      "Epoch: 67 | Batch: 005 / 030 | Total loss: 1.227 | Reg loss: 0.041 | Tree loss: 1.227 | Accuracy: 0.583984 | 7.097 sec/iter\n",
      "Epoch: 67 | Batch: 006 / 030 | Total loss: 1.220 | Reg loss: 0.041 | Tree loss: 1.220 | Accuracy: 0.564453 | 7.097 sec/iter\n",
      "Epoch: 67 | Batch: 007 / 030 | Total loss: 1.194 | Reg loss: 0.041 | Tree loss: 1.194 | Accuracy: 0.566406 | 7.097 sec/iter\n",
      "Epoch: 67 | Batch: 008 / 030 | Total loss: 1.178 | Reg loss: 0.041 | Tree loss: 1.178 | Accuracy: 0.582031 | 7.097 sec/iter\n",
      "Epoch: 67 | Batch: 009 / 030 | Total loss: 1.151 | Reg loss: 0.041 | Tree loss: 1.151 | Accuracy: 0.583984 | 7.097 sec/iter\n",
      "Epoch: 67 | Batch: 010 / 030 | Total loss: 1.160 | Reg loss: 0.041 | Tree loss: 1.160 | Accuracy: 0.535156 | 7.097 sec/iter\n",
      "Epoch: 67 | Batch: 011 / 030 | Total loss: 1.124 | Reg loss: 0.041 | Tree loss: 1.124 | Accuracy: 0.580078 | 7.097 sec/iter\n",
      "Epoch: 67 | Batch: 012 / 030 | Total loss: 1.110 | Reg loss: 0.041 | Tree loss: 1.110 | Accuracy: 0.582031 | 7.097 sec/iter\n",
      "Epoch: 67 | Batch: 013 / 030 | Total loss: 1.083 | Reg loss: 0.041 | Tree loss: 1.083 | Accuracy: 0.593750 | 7.096 sec/iter\n",
      "Epoch: 67 | Batch: 014 / 030 | Total loss: 1.060 | Reg loss: 0.042 | Tree loss: 1.060 | Accuracy: 0.572266 | 7.096 sec/iter\n",
      "Epoch: 67 | Batch: 015 / 030 | Total loss: 1.046 | Reg loss: 0.042 | Tree loss: 1.046 | Accuracy: 0.603516 | 7.096 sec/iter\n",
      "Epoch: 67 | Batch: 016 / 030 | Total loss: 1.023 | Reg loss: 0.042 | Tree loss: 1.023 | Accuracy: 0.642578 | 7.096 sec/iter\n",
      "Epoch: 67 | Batch: 017 / 030 | Total loss: 1.047 | Reg loss: 0.042 | Tree loss: 1.047 | Accuracy: 0.556641 | 7.096 sec/iter\n",
      "Epoch: 67 | Batch: 018 / 030 | Total loss: 1.001 | Reg loss: 0.042 | Tree loss: 1.001 | Accuracy: 0.623047 | 7.096 sec/iter\n",
      "Epoch: 67 | Batch: 019 / 030 | Total loss: 1.010 | Reg loss: 0.042 | Tree loss: 1.010 | Accuracy: 0.560547 | 7.096 sec/iter\n",
      "Epoch: 67 | Batch: 020 / 030 | Total loss: 1.022 | Reg loss: 0.042 | Tree loss: 1.022 | Accuracy: 0.529297 | 7.096 sec/iter\n",
      "Epoch: 67 | Batch: 021 / 030 | Total loss: 0.964 | Reg loss: 0.042 | Tree loss: 0.964 | Accuracy: 0.603516 | 7.096 sec/iter\n",
      "Epoch: 67 | Batch: 022 / 030 | Total loss: 0.988 | Reg loss: 0.042 | Tree loss: 0.988 | Accuracy: 0.556641 | 7.096 sec/iter\n",
      "Epoch: 67 | Batch: 023 / 030 | Total loss: 0.979 | Reg loss: 0.042 | Tree loss: 0.979 | Accuracy: 0.597656 | 7.096 sec/iter\n",
      "Epoch: 67 | Batch: 024 / 030 | Total loss: 0.967 | Reg loss: 0.042 | Tree loss: 0.967 | Accuracy: 0.583984 | 7.094 sec/iter\n",
      "Epoch: 67 | Batch: 025 / 030 | Total loss: 0.944 | Reg loss: 0.042 | Tree loss: 0.944 | Accuracy: 0.589844 | 7.093 sec/iter\n",
      "Epoch: 67 | Batch: 026 / 030 | Total loss: 0.927 | Reg loss: 0.043 | Tree loss: 0.927 | Accuracy: 0.632812 | 7.092 sec/iter\n",
      "Epoch: 67 | Batch: 027 / 030 | Total loss: 0.927 | Reg loss: 0.043 | Tree loss: 0.927 | Accuracy: 0.607422 | 7.091 sec/iter\n",
      "Epoch: 67 | Batch: 028 / 030 | Total loss: 0.937 | Reg loss: 0.043 | Tree loss: 0.937 | Accuracy: 0.587891 | 7.09 sec/iter\n",
      "Epoch: 67 | Batch: 029 / 030 | Total loss: 0.928 | Reg loss: 0.043 | Tree loss: 0.928 | Accuracy: 0.640777 | 7.088 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 68 | Batch: 000 / 030 | Total loss: 1.319 | Reg loss: 0.041 | Tree loss: 1.319 | Accuracy: 0.568359 | 7.094 sec/iter\n",
      "Epoch: 68 | Batch: 001 / 030 | Total loss: 1.289 | Reg loss: 0.041 | Tree loss: 1.289 | Accuracy: 0.574219 | 7.094 sec/iter\n",
      "Epoch: 68 | Batch: 002 / 030 | Total loss: 1.269 | Reg loss: 0.041 | Tree loss: 1.269 | Accuracy: 0.599609 | 7.094 sec/iter\n",
      "Epoch: 68 | Batch: 003 / 030 | Total loss: 1.246 | Reg loss: 0.041 | Tree loss: 1.246 | Accuracy: 0.603516 | 7.094 sec/iter\n",
      "Epoch: 68 | Batch: 004 / 030 | Total loss: 1.215 | Reg loss: 0.041 | Tree loss: 1.215 | Accuracy: 0.625000 | 7.094 sec/iter\n",
      "Epoch: 68 | Batch: 005 / 030 | Total loss: 1.192 | Reg loss: 0.041 | Tree loss: 1.192 | Accuracy: 0.570312 | 7.094 sec/iter\n",
      "Epoch: 68 | Batch: 006 / 030 | Total loss: 1.197 | Reg loss: 0.041 | Tree loss: 1.197 | Accuracy: 0.558594 | 7.094 sec/iter\n",
      "Epoch: 68 | Batch: 007 / 030 | Total loss: 1.168 | Reg loss: 0.041 | Tree loss: 1.168 | Accuracy: 0.568359 | 7.094 sec/iter\n",
      "Epoch: 68 | Batch: 008 / 030 | Total loss: 1.166 | Reg loss: 0.041 | Tree loss: 1.166 | Accuracy: 0.580078 | 7.094 sec/iter\n",
      "Epoch: 68 | Batch: 009 / 030 | Total loss: 1.157 | Reg loss: 0.041 | Tree loss: 1.157 | Accuracy: 0.613281 | 7.093 sec/iter\n",
      "Epoch: 68 | Batch: 010 / 030 | Total loss: 1.170 | Reg loss: 0.041 | Tree loss: 1.170 | Accuracy: 0.529297 | 7.093 sec/iter\n",
      "Epoch: 68 | Batch: 011 / 030 | Total loss: 1.147 | Reg loss: 0.041 | Tree loss: 1.147 | Accuracy: 0.533203 | 7.093 sec/iter\n",
      "Epoch: 68 | Batch: 012 / 030 | Total loss: 1.092 | Reg loss: 0.041 | Tree loss: 1.092 | Accuracy: 0.595703 | 7.093 sec/iter\n",
      "Epoch: 68 | Batch: 013 / 030 | Total loss: 1.079 | Reg loss: 0.041 | Tree loss: 1.079 | Accuracy: 0.580078 | 7.093 sec/iter\n",
      "Epoch: 68 | Batch: 014 / 030 | Total loss: 1.048 | Reg loss: 0.041 | Tree loss: 1.048 | Accuracy: 0.603516 | 7.093 sec/iter\n",
      "Epoch: 68 | Batch: 015 / 030 | Total loss: 1.050 | Reg loss: 0.041 | Tree loss: 1.050 | Accuracy: 0.593750 | 7.092 sec/iter\n",
      "Epoch: 68 | Batch: 016 / 030 | Total loss: 1.054 | Reg loss: 0.042 | Tree loss: 1.054 | Accuracy: 0.554688 | 7.092 sec/iter\n",
      "Epoch: 68 | Batch: 017 / 030 | Total loss: 1.033 | Reg loss: 0.042 | Tree loss: 1.033 | Accuracy: 0.609375 | 7.092 sec/iter\n",
      "Epoch: 68 | Batch: 018 / 030 | Total loss: 1.033 | Reg loss: 0.042 | Tree loss: 1.033 | Accuracy: 0.556641 | 7.093 sec/iter\n",
      "Epoch: 68 | Batch: 019 / 030 | Total loss: 1.006 | Reg loss: 0.042 | Tree loss: 1.006 | Accuracy: 0.593750 | 7.092 sec/iter\n",
      "Epoch: 68 | Batch: 020 / 030 | Total loss: 0.992 | Reg loss: 0.042 | Tree loss: 0.992 | Accuracy: 0.601562 | 7.093 sec/iter\n",
      "Epoch: 68 | Batch: 021 / 030 | Total loss: 0.969 | Reg loss: 0.042 | Tree loss: 0.969 | Accuracy: 0.613281 | 7.093 sec/iter\n",
      "Epoch: 68 | Batch: 022 / 030 | Total loss: 0.971 | Reg loss: 0.042 | Tree loss: 0.971 | Accuracy: 0.593750 | 7.093 sec/iter\n",
      "Epoch: 68 | Batch: 023 / 030 | Total loss: 0.988 | Reg loss: 0.042 | Tree loss: 0.988 | Accuracy: 0.539062 | 7.093 sec/iter\n",
      "Epoch: 68 | Batch: 024 / 030 | Total loss: 0.961 | Reg loss: 0.042 | Tree loss: 0.961 | Accuracy: 0.558594 | 7.092 sec/iter\n",
      "Epoch: 68 | Batch: 025 / 030 | Total loss: 0.943 | Reg loss: 0.042 | Tree loss: 0.943 | Accuracy: 0.578125 | 7.091 sec/iter\n",
      "Epoch: 68 | Batch: 026 / 030 | Total loss: 0.932 | Reg loss: 0.042 | Tree loss: 0.932 | Accuracy: 0.609375 | 7.09 sec/iter\n",
      "Epoch: 68 | Batch: 027 / 030 | Total loss: 0.929 | Reg loss: 0.042 | Tree loss: 0.929 | Accuracy: 0.617188 | 7.09 sec/iter\n",
      "Epoch: 68 | Batch: 028 / 030 | Total loss: 0.926 | Reg loss: 0.043 | Tree loss: 0.926 | Accuracy: 0.593750 | 7.09 sec/iter\n",
      "Epoch: 68 | Batch: 029 / 030 | Total loss: 0.974 | Reg loss: 0.043 | Tree loss: 0.974 | Accuracy: 0.504854 | 7.088 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 69 | Batch: 000 / 030 | Total loss: 1.348 | Reg loss: 0.041 | Tree loss: 1.348 | Accuracy: 0.560547 | 7.094 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 69 | Batch: 001 / 030 | Total loss: 1.275 | Reg loss: 0.041 | Tree loss: 1.275 | Accuracy: 0.591797 | 7.094 sec/iter\n",
      "Epoch: 69 | Batch: 002 / 030 | Total loss: 1.262 | Reg loss: 0.041 | Tree loss: 1.262 | Accuracy: 0.597656 | 7.094 sec/iter\n",
      "Epoch: 69 | Batch: 003 / 030 | Total loss: 1.245 | Reg loss: 0.041 | Tree loss: 1.245 | Accuracy: 0.611328 | 7.094 sec/iter\n",
      "Epoch: 69 | Batch: 004 / 030 | Total loss: 1.262 | Reg loss: 0.041 | Tree loss: 1.262 | Accuracy: 0.531250 | 7.094 sec/iter\n",
      "Epoch: 69 | Batch: 005 / 030 | Total loss: 1.204 | Reg loss: 0.041 | Tree loss: 1.204 | Accuracy: 0.580078 | 7.094 sec/iter\n",
      "Epoch: 69 | Batch: 006 / 030 | Total loss: 1.248 | Reg loss: 0.041 | Tree loss: 1.248 | Accuracy: 0.525391 | 7.094 sec/iter\n",
      "Epoch: 69 | Batch: 007 / 030 | Total loss: 1.155 | Reg loss: 0.041 | Tree loss: 1.155 | Accuracy: 0.591797 | 7.094 sec/iter\n",
      "Epoch: 69 | Batch: 008 / 030 | Total loss: 1.170 | Reg loss: 0.041 | Tree loss: 1.170 | Accuracy: 0.562500 | 7.094 sec/iter\n",
      "Epoch: 69 | Batch: 009 / 030 | Total loss: 1.103 | Reg loss: 0.041 | Tree loss: 1.103 | Accuracy: 0.597656 | 7.094 sec/iter\n",
      "Epoch: 69 | Batch: 010 / 030 | Total loss: 1.125 | Reg loss: 0.041 | Tree loss: 1.125 | Accuracy: 0.578125 | 7.094 sec/iter\n",
      "Epoch: 69 | Batch: 011 / 030 | Total loss: 1.102 | Reg loss: 0.041 | Tree loss: 1.102 | Accuracy: 0.593750 | 7.094 sec/iter\n",
      "Epoch: 69 | Batch: 012 / 030 | Total loss: 1.089 | Reg loss: 0.041 | Tree loss: 1.089 | Accuracy: 0.580078 | 7.094 sec/iter\n",
      "Epoch: 69 | Batch: 013 / 030 | Total loss: 1.080 | Reg loss: 0.041 | Tree loss: 1.080 | Accuracy: 0.599609 | 7.094 sec/iter\n",
      "Epoch: 69 | Batch: 014 / 030 | Total loss: 1.064 | Reg loss: 0.041 | Tree loss: 1.064 | Accuracy: 0.572266 | 7.094 sec/iter\n",
      "Epoch: 69 | Batch: 015 / 030 | Total loss: 1.049 | Reg loss: 0.041 | Tree loss: 1.049 | Accuracy: 0.576172 | 7.094 sec/iter\n",
      "Epoch: 69 | Batch: 016 / 030 | Total loss: 1.020 | Reg loss: 0.041 | Tree loss: 1.020 | Accuracy: 0.595703 | 7.094 sec/iter\n",
      "Epoch: 69 | Batch: 017 / 030 | Total loss: 1.027 | Reg loss: 0.042 | Tree loss: 1.027 | Accuracy: 0.607422 | 7.093 sec/iter\n",
      "Epoch: 69 | Batch: 018 / 030 | Total loss: 1.015 | Reg loss: 0.042 | Tree loss: 1.015 | Accuracy: 0.558594 | 7.093 sec/iter\n",
      "Epoch: 69 | Batch: 019 / 030 | Total loss: 0.995 | Reg loss: 0.042 | Tree loss: 0.995 | Accuracy: 0.568359 | 7.093 sec/iter\n",
      "Epoch: 69 | Batch: 020 / 030 | Total loss: 0.991 | Reg loss: 0.042 | Tree loss: 0.991 | Accuracy: 0.585938 | 7.093 sec/iter\n",
      "Epoch: 69 | Batch: 021 / 030 | Total loss: 0.971 | Reg loss: 0.042 | Tree loss: 0.971 | Accuracy: 0.595703 | 7.093 sec/iter\n",
      "Epoch: 69 | Batch: 022 / 030 | Total loss: 0.963 | Reg loss: 0.042 | Tree loss: 0.963 | Accuracy: 0.597656 | 7.092 sec/iter\n",
      "Epoch: 69 | Batch: 023 / 030 | Total loss: 0.944 | Reg loss: 0.042 | Tree loss: 0.944 | Accuracy: 0.587891 | 7.091 sec/iter\n",
      "Epoch: 69 | Batch: 024 / 030 | Total loss: 0.958 | Reg loss: 0.042 | Tree loss: 0.958 | Accuracy: 0.587891 | 7.09 sec/iter\n",
      "Epoch: 69 | Batch: 025 / 030 | Total loss: 0.943 | Reg loss: 0.042 | Tree loss: 0.943 | Accuracy: 0.585938 | 7.088 sec/iter\n",
      "Epoch: 69 | Batch: 026 / 030 | Total loss: 0.926 | Reg loss: 0.042 | Tree loss: 0.926 | Accuracy: 0.603516 | 7.088 sec/iter\n",
      "Epoch: 69 | Batch: 027 / 030 | Total loss: 0.945 | Reg loss: 0.042 | Tree loss: 0.945 | Accuracy: 0.574219 | 7.088 sec/iter\n",
      "Epoch: 69 | Batch: 028 / 030 | Total loss: 0.919 | Reg loss: 0.042 | Tree loss: 0.919 | Accuracy: 0.609375 | 7.087 sec/iter\n",
      "Epoch: 69 | Batch: 029 / 030 | Total loss: 0.938 | Reg loss: 0.043 | Tree loss: 0.938 | Accuracy: 0.543689 | 7.086 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 70 | Batch: 000 / 030 | Total loss: 1.287 | Reg loss: 0.041 | Tree loss: 1.287 | Accuracy: 0.589844 | 7.091 sec/iter\n",
      "Epoch: 70 | Batch: 001 / 030 | Total loss: 1.303 | Reg loss: 0.041 | Tree loss: 1.303 | Accuracy: 0.582031 | 7.091 sec/iter\n",
      "Epoch: 70 | Batch: 002 / 030 | Total loss: 1.254 | Reg loss: 0.041 | Tree loss: 1.254 | Accuracy: 0.601562 | 7.091 sec/iter\n",
      "Epoch: 70 | Batch: 003 / 030 | Total loss: 1.243 | Reg loss: 0.041 | Tree loss: 1.243 | Accuracy: 0.589844 | 7.091 sec/iter\n",
      "Epoch: 70 | Batch: 004 / 030 | Total loss: 1.226 | Reg loss: 0.041 | Tree loss: 1.226 | Accuracy: 0.568359 | 7.091 sec/iter\n",
      "Epoch: 70 | Batch: 005 / 030 | Total loss: 1.208 | Reg loss: 0.041 | Tree loss: 1.208 | Accuracy: 0.601562 | 7.091 sec/iter\n",
      "Epoch: 70 | Batch: 006 / 030 | Total loss: 1.188 | Reg loss: 0.041 | Tree loss: 1.188 | Accuracy: 0.552734 | 7.091 sec/iter\n",
      "Epoch: 70 | Batch: 007 / 030 | Total loss: 1.150 | Reg loss: 0.041 | Tree loss: 1.150 | Accuracy: 0.593750 | 7.091 sec/iter\n",
      "Epoch: 70 | Batch: 008 / 030 | Total loss: 1.163 | Reg loss: 0.041 | Tree loss: 1.163 | Accuracy: 0.583984 | 7.09 sec/iter\n",
      "Epoch: 70 | Batch: 009 / 030 | Total loss: 1.156 | Reg loss: 0.041 | Tree loss: 1.156 | Accuracy: 0.546875 | 7.09 sec/iter\n",
      "Epoch: 70 | Batch: 010 / 030 | Total loss: 1.116 | Reg loss: 0.041 | Tree loss: 1.116 | Accuracy: 0.574219 | 7.09 sec/iter\n",
      "Epoch: 70 | Batch: 011 / 030 | Total loss: 1.105 | Reg loss: 0.041 | Tree loss: 1.105 | Accuracy: 0.593750 | 7.09 sec/iter\n",
      "Epoch: 70 | Batch: 012 / 030 | Total loss: 1.091 | Reg loss: 0.041 | Tree loss: 1.091 | Accuracy: 0.566406 | 7.09 sec/iter\n",
      "Epoch: 70 | Batch: 013 / 030 | Total loss: 1.073 | Reg loss: 0.041 | Tree loss: 1.073 | Accuracy: 0.599609 | 7.09 sec/iter\n",
      "Epoch: 70 | Batch: 014 / 030 | Total loss: 1.058 | Reg loss: 0.041 | Tree loss: 1.058 | Accuracy: 0.603516 | 7.089 sec/iter\n",
      "Epoch: 70 | Batch: 015 / 030 | Total loss: 1.036 | Reg loss: 0.041 | Tree loss: 1.036 | Accuracy: 0.601562 | 7.089 sec/iter\n",
      "Epoch: 70 | Batch: 016 / 030 | Total loss: 1.032 | Reg loss: 0.041 | Tree loss: 1.032 | Accuracy: 0.587891 | 7.089 sec/iter\n",
      "Epoch: 70 | Batch: 017 / 030 | Total loss: 1.021 | Reg loss: 0.041 | Tree loss: 1.021 | Accuracy: 0.566406 | 7.089 sec/iter\n",
      "Epoch: 70 | Batch: 018 / 030 | Total loss: 1.010 | Reg loss: 0.042 | Tree loss: 1.010 | Accuracy: 0.562500 | 7.089 sec/iter\n",
      "Epoch: 70 | Batch: 019 / 030 | Total loss: 0.998 | Reg loss: 0.042 | Tree loss: 0.998 | Accuracy: 0.564453 | 7.089 sec/iter\n",
      "Epoch: 70 | Batch: 020 / 030 | Total loss: 0.962 | Reg loss: 0.042 | Tree loss: 0.962 | Accuracy: 0.619141 | 7.089 sec/iter\n",
      "Epoch: 70 | Batch: 021 / 030 | Total loss: 0.972 | Reg loss: 0.042 | Tree loss: 0.972 | Accuracy: 0.572266 | 7.089 sec/iter\n",
      "Epoch: 70 | Batch: 022 / 030 | Total loss: 0.980 | Reg loss: 0.042 | Tree loss: 0.980 | Accuracy: 0.568359 | 7.089 sec/iter\n",
      "Epoch: 70 | Batch: 023 / 030 | Total loss: 0.969 | Reg loss: 0.042 | Tree loss: 0.969 | Accuracy: 0.585938 | 7.088 sec/iter\n",
      "Epoch: 70 | Batch: 024 / 030 | Total loss: 0.957 | Reg loss: 0.042 | Tree loss: 0.957 | Accuracy: 0.556641 | 7.087 sec/iter\n",
      "Epoch: 70 | Batch: 025 / 030 | Total loss: 0.925 | Reg loss: 0.042 | Tree loss: 0.925 | Accuracy: 0.617188 | 7.086 sec/iter\n",
      "Epoch: 70 | Batch: 026 / 030 | Total loss: 0.948 | Reg loss: 0.042 | Tree loss: 0.948 | Accuracy: 0.562500 | 7.087 sec/iter\n",
      "Epoch: 70 | Batch: 027 / 030 | Total loss: 0.914 | Reg loss: 0.042 | Tree loss: 0.914 | Accuracy: 0.605469 | 7.087 sec/iter\n",
      "Epoch: 70 | Batch: 028 / 030 | Total loss: 0.929 | Reg loss: 0.042 | Tree loss: 0.929 | Accuracy: 0.568359 | 7.086 sec/iter\n",
      "Epoch: 70 | Batch: 029 / 030 | Total loss: 0.880 | Reg loss: 0.042 | Tree loss: 0.880 | Accuracy: 0.650485 | 7.085 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 71 | Batch: 000 / 030 | Total loss: 1.287 | Reg loss: 0.041 | Tree loss: 1.287 | Accuracy: 0.585938 | 7.09 sec/iter\n",
      "Epoch: 71 | Batch: 001 / 030 | Total loss: 1.299 | Reg loss: 0.041 | Tree loss: 1.299 | Accuracy: 0.580078 | 7.09 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 71 | Batch: 002 / 030 | Total loss: 1.268 | Reg loss: 0.041 | Tree loss: 1.268 | Accuracy: 0.591797 | 7.09 sec/iter\n",
      "Epoch: 71 | Batch: 003 / 030 | Total loss: 1.242 | Reg loss: 0.041 | Tree loss: 1.242 | Accuracy: 0.582031 | 7.09 sec/iter\n",
      "Epoch: 71 | Batch: 004 / 030 | Total loss: 1.230 | Reg loss: 0.041 | Tree loss: 1.230 | Accuracy: 0.572266 | 7.09 sec/iter\n",
      "Epoch: 71 | Batch: 005 / 030 | Total loss: 1.205 | Reg loss: 0.041 | Tree loss: 1.205 | Accuracy: 0.585938 | 7.09 sec/iter\n",
      "Epoch: 71 | Batch: 006 / 030 | Total loss: 1.189 | Reg loss: 0.041 | Tree loss: 1.189 | Accuracy: 0.580078 | 7.09 sec/iter\n",
      "Epoch: 71 | Batch: 007 / 030 | Total loss: 1.185 | Reg loss: 0.041 | Tree loss: 1.185 | Accuracy: 0.576172 | 7.09 sec/iter\n",
      "Epoch: 71 | Batch: 008 / 030 | Total loss: 1.154 | Reg loss: 0.041 | Tree loss: 1.154 | Accuracy: 0.574219 | 7.09 sec/iter\n",
      "Epoch: 71 | Batch: 009 / 030 | Total loss: 1.134 | Reg loss: 0.041 | Tree loss: 1.134 | Accuracy: 0.572266 | 7.089 sec/iter\n",
      "Epoch: 71 | Batch: 010 / 030 | Total loss: 1.115 | Reg loss: 0.041 | Tree loss: 1.115 | Accuracy: 0.552734 | 7.089 sec/iter\n",
      "Epoch: 71 | Batch: 011 / 030 | Total loss: 1.107 | Reg loss: 0.041 | Tree loss: 1.107 | Accuracy: 0.556641 | 7.089 sec/iter\n",
      "Epoch: 71 | Batch: 012 / 030 | Total loss: 1.071 | Reg loss: 0.041 | Tree loss: 1.071 | Accuracy: 0.578125 | 7.089 sec/iter\n",
      "Epoch: 71 | Batch: 013 / 030 | Total loss: 1.070 | Reg loss: 0.041 | Tree loss: 1.070 | Accuracy: 0.583984 | 7.089 sec/iter\n",
      "Epoch: 71 | Batch: 014 / 030 | Total loss: 1.064 | Reg loss: 0.041 | Tree loss: 1.064 | Accuracy: 0.564453 | 7.089 sec/iter\n",
      "Epoch: 71 | Batch: 015 / 030 | Total loss: 1.017 | Reg loss: 0.041 | Tree loss: 1.017 | Accuracy: 0.589844 | 7.089 sec/iter\n",
      "Epoch: 71 | Batch: 016 / 030 | Total loss: 1.021 | Reg loss: 0.041 | Tree loss: 1.021 | Accuracy: 0.585938 | 7.088 sec/iter\n",
      "Epoch: 71 | Batch: 017 / 030 | Total loss: 0.998 | Reg loss: 0.041 | Tree loss: 0.998 | Accuracy: 0.628906 | 7.088 sec/iter\n",
      "Epoch: 71 | Batch: 018 / 030 | Total loss: 0.998 | Reg loss: 0.041 | Tree loss: 0.998 | Accuracy: 0.587891 | 7.088 sec/iter\n",
      "Epoch: 71 | Batch: 019 / 030 | Total loss: 0.992 | Reg loss: 0.042 | Tree loss: 0.992 | Accuracy: 0.593750 | 7.088 sec/iter\n",
      "Epoch: 71 | Batch: 020 / 030 | Total loss: 0.979 | Reg loss: 0.042 | Tree loss: 0.979 | Accuracy: 0.568359 | 7.088 sec/iter\n",
      "Epoch: 71 | Batch: 021 / 030 | Total loss: 0.965 | Reg loss: 0.042 | Tree loss: 0.965 | Accuracy: 0.582031 | 7.088 sec/iter\n",
      "Epoch: 71 | Batch: 022 / 030 | Total loss: 0.971 | Reg loss: 0.042 | Tree loss: 0.971 | Accuracy: 0.578125 | 7.087 sec/iter\n",
      "Epoch: 71 | Batch: 023 / 030 | Total loss: 0.951 | Reg loss: 0.042 | Tree loss: 0.951 | Accuracy: 0.603516 | 7.086 sec/iter\n",
      "Epoch: 71 | Batch: 024 / 030 | Total loss: 0.930 | Reg loss: 0.042 | Tree loss: 0.930 | Accuracy: 0.625000 | 7.085 sec/iter\n",
      "Epoch: 71 | Batch: 025 / 030 | Total loss: 0.949 | Reg loss: 0.042 | Tree loss: 0.949 | Accuracy: 0.556641 | 7.086 sec/iter\n",
      "Epoch: 71 | Batch: 026 / 030 | Total loss: 0.930 | Reg loss: 0.042 | Tree loss: 0.930 | Accuracy: 0.582031 | 7.085 sec/iter\n",
      "Epoch: 71 | Batch: 027 / 030 | Total loss: 0.944 | Reg loss: 0.042 | Tree loss: 0.944 | Accuracy: 0.542969 | 7.085 sec/iter\n",
      "Epoch: 71 | Batch: 028 / 030 | Total loss: 0.885 | Reg loss: 0.042 | Tree loss: 0.885 | Accuracy: 0.642578 | 7.085 sec/iter\n",
      "Epoch: 71 | Batch: 029 / 030 | Total loss: 0.922 | Reg loss: 0.042 | Tree loss: 0.922 | Accuracy: 0.563107 | 7.083 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 72 | Batch: 000 / 030 | Total loss: 1.272 | Reg loss: 0.040 | Tree loss: 1.272 | Accuracy: 0.601562 | 7.086 sec/iter\n",
      "Epoch: 72 | Batch: 001 / 030 | Total loss: 1.269 | Reg loss: 0.040 | Tree loss: 1.269 | Accuracy: 0.623047 | 7.086 sec/iter\n",
      "Epoch: 72 | Batch: 002 / 030 | Total loss: 1.274 | Reg loss: 0.040 | Tree loss: 1.274 | Accuracy: 0.585938 | 7.086 sec/iter\n",
      "Epoch: 72 | Batch: 003 / 030 | Total loss: 1.257 | Reg loss: 0.040 | Tree loss: 1.257 | Accuracy: 0.556641 | 7.086 sec/iter\n",
      "Epoch: 72 | Batch: 004 / 030 | Total loss: 1.250 | Reg loss: 0.040 | Tree loss: 1.250 | Accuracy: 0.533203 | 7.086 sec/iter\n",
      "Epoch: 72 | Batch: 005 / 030 | Total loss: 1.216 | Reg loss: 0.040 | Tree loss: 1.216 | Accuracy: 0.556641 | 7.086 sec/iter\n",
      "Epoch: 72 | Batch: 006 / 030 | Total loss: 1.180 | Reg loss: 0.040 | Tree loss: 1.180 | Accuracy: 0.601562 | 7.087 sec/iter\n",
      "Epoch: 72 | Batch: 007 / 030 | Total loss: 1.156 | Reg loss: 0.041 | Tree loss: 1.156 | Accuracy: 0.589844 | 7.087 sec/iter\n",
      "Epoch: 72 | Batch: 008 / 030 | Total loss: 1.145 | Reg loss: 0.041 | Tree loss: 1.145 | Accuracy: 0.558594 | 7.087 sec/iter\n",
      "Epoch: 72 | Batch: 009 / 030 | Total loss: 1.150 | Reg loss: 0.041 | Tree loss: 1.150 | Accuracy: 0.505859 | 7.087 sec/iter\n",
      "Epoch: 72 | Batch: 010 / 030 | Total loss: 1.125 | Reg loss: 0.041 | Tree loss: 1.125 | Accuracy: 0.560547 | 7.087 sec/iter\n",
      "Epoch: 72 | Batch: 011 / 030 | Total loss: 1.097 | Reg loss: 0.041 | Tree loss: 1.097 | Accuracy: 0.560547 | 7.087 sec/iter\n",
      "Epoch: 72 | Batch: 012 / 030 | Total loss: 1.098 | Reg loss: 0.041 | Tree loss: 1.098 | Accuracy: 0.583984 | 7.087 sec/iter\n",
      "Epoch: 72 | Batch: 013 / 030 | Total loss: 1.066 | Reg loss: 0.041 | Tree loss: 1.066 | Accuracy: 0.589844 | 7.087 sec/iter\n",
      "Epoch: 72 | Batch: 014 / 030 | Total loss: 1.030 | Reg loss: 0.041 | Tree loss: 1.030 | Accuracy: 0.591797 | 7.087 sec/iter\n",
      "Epoch: 72 | Batch: 015 / 030 | Total loss: 1.043 | Reg loss: 0.041 | Tree loss: 1.043 | Accuracy: 0.578125 | 7.087 sec/iter\n",
      "Epoch: 72 | Batch: 016 / 030 | Total loss: 0.987 | Reg loss: 0.041 | Tree loss: 0.987 | Accuracy: 0.603516 | 7.087 sec/iter\n",
      "Epoch: 72 | Batch: 017 / 030 | Total loss: 0.995 | Reg loss: 0.041 | Tree loss: 0.995 | Accuracy: 0.595703 | 7.087 sec/iter\n",
      "Epoch: 72 | Batch: 018 / 030 | Total loss: 0.992 | Reg loss: 0.041 | Tree loss: 0.992 | Accuracy: 0.603516 | 7.087 sec/iter\n",
      "Epoch: 72 | Batch: 019 / 030 | Total loss: 0.991 | Reg loss: 0.041 | Tree loss: 0.991 | Accuracy: 0.580078 | 7.087 sec/iter\n",
      "Epoch: 72 | Batch: 020 / 030 | Total loss: 0.973 | Reg loss: 0.042 | Tree loss: 0.973 | Accuracy: 0.556641 | 7.087 sec/iter\n",
      "Epoch: 72 | Batch: 021 / 030 | Total loss: 0.959 | Reg loss: 0.042 | Tree loss: 0.959 | Accuracy: 0.593750 | 7.086 sec/iter\n",
      "Epoch: 72 | Batch: 022 / 030 | Total loss: 0.948 | Reg loss: 0.042 | Tree loss: 0.948 | Accuracy: 0.611328 | 7.085 sec/iter\n",
      "Epoch: 72 | Batch: 023 / 030 | Total loss: 0.972 | Reg loss: 0.042 | Tree loss: 0.972 | Accuracy: 0.539062 | 7.084 sec/iter\n",
      "Epoch: 72 | Batch: 024 / 030 | Total loss: 0.935 | Reg loss: 0.042 | Tree loss: 0.935 | Accuracy: 0.611328 | 7.083 sec/iter\n",
      "Epoch: 72 | Batch: 025 / 030 | Total loss: 0.930 | Reg loss: 0.042 | Tree loss: 0.930 | Accuracy: 0.572266 | 7.081 sec/iter\n",
      "Epoch: 72 | Batch: 026 / 030 | Total loss: 0.911 | Reg loss: 0.042 | Tree loss: 0.911 | Accuracy: 0.632812 | 7.08 sec/iter\n",
      "Epoch: 72 | Batch: 027 / 030 | Total loss: 0.895 | Reg loss: 0.042 | Tree loss: 0.895 | Accuracy: 0.625000 | 7.079 sec/iter\n",
      "Epoch: 72 | Batch: 028 / 030 | Total loss: 0.912 | Reg loss: 0.042 | Tree loss: 0.912 | Accuracy: 0.593750 | 7.078 sec/iter\n",
      "Epoch: 72 | Batch: 029 / 030 | Total loss: 0.897 | Reg loss: 0.042 | Tree loss: 0.897 | Accuracy: 0.601942 | 7.077 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 73 | Batch: 000 / 030 | Total loss: 1.274 | Reg loss: 0.040 | Tree loss: 1.274 | Accuracy: 0.583984 | 7.084 sec/iter\n",
      "Epoch: 73 | Batch: 001 / 030 | Total loss: 1.239 | Reg loss: 0.040 | Tree loss: 1.239 | Accuracy: 0.605469 | 7.084 sec/iter\n",
      "Epoch: 73 | Batch: 002 / 030 | Total loss: 1.279 | Reg loss: 0.040 | Tree loss: 1.279 | Accuracy: 0.570312 | 7.084 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 73 | Batch: 003 / 030 | Total loss: 1.271 | Reg loss: 0.040 | Tree loss: 1.271 | Accuracy: 0.570312 | 7.084 sec/iter\n",
      "Epoch: 73 | Batch: 004 / 030 | Total loss: 1.266 | Reg loss: 0.040 | Tree loss: 1.266 | Accuracy: 0.568359 | 7.083 sec/iter\n",
      "Epoch: 73 | Batch: 005 / 030 | Total loss: 1.212 | Reg loss: 0.040 | Tree loss: 1.212 | Accuracy: 0.582031 | 7.083 sec/iter\n",
      "Epoch: 73 | Batch: 006 / 030 | Total loss: 1.201 | Reg loss: 0.040 | Tree loss: 1.201 | Accuracy: 0.566406 | 7.083 sec/iter\n",
      "Epoch: 73 | Batch: 007 / 030 | Total loss: 1.154 | Reg loss: 0.040 | Tree loss: 1.154 | Accuracy: 0.582031 | 7.083 sec/iter\n",
      "Epoch: 73 | Batch: 008 / 030 | Total loss: 1.127 | Reg loss: 0.040 | Tree loss: 1.127 | Accuracy: 0.597656 | 7.082 sec/iter\n",
      "Epoch: 73 | Batch: 009 / 030 | Total loss: 1.144 | Reg loss: 0.041 | Tree loss: 1.144 | Accuracy: 0.539062 | 7.082 sec/iter\n",
      "Epoch: 73 | Batch: 010 / 030 | Total loss: 1.106 | Reg loss: 0.041 | Tree loss: 1.106 | Accuracy: 0.593750 | 7.082 sec/iter\n",
      "Epoch: 73 | Batch: 011 / 030 | Total loss: 1.097 | Reg loss: 0.041 | Tree loss: 1.097 | Accuracy: 0.585938 | 7.082 sec/iter\n",
      "Epoch: 73 | Batch: 012 / 030 | Total loss: 1.076 | Reg loss: 0.041 | Tree loss: 1.076 | Accuracy: 0.576172 | 7.082 sec/iter\n",
      "Epoch: 73 | Batch: 013 / 030 | Total loss: 1.034 | Reg loss: 0.041 | Tree loss: 1.034 | Accuracy: 0.601562 | 7.082 sec/iter\n",
      "Epoch: 73 | Batch: 014 / 030 | Total loss: 1.029 | Reg loss: 0.041 | Tree loss: 1.029 | Accuracy: 0.574219 | 7.082 sec/iter\n",
      "Epoch: 73 | Batch: 015 / 030 | Total loss: 1.021 | Reg loss: 0.041 | Tree loss: 1.021 | Accuracy: 0.601562 | 7.083 sec/iter\n",
      "Epoch: 73 | Batch: 016 / 030 | Total loss: 1.014 | Reg loss: 0.041 | Tree loss: 1.014 | Accuracy: 0.582031 | 7.083 sec/iter\n",
      "Epoch: 73 | Batch: 017 / 030 | Total loss: 0.990 | Reg loss: 0.041 | Tree loss: 0.990 | Accuracy: 0.583984 | 7.083 sec/iter\n",
      "Epoch: 73 | Batch: 018 / 030 | Total loss: 0.984 | Reg loss: 0.041 | Tree loss: 0.984 | Accuracy: 0.607422 | 7.082 sec/iter\n",
      "Epoch: 73 | Batch: 019 / 030 | Total loss: 0.968 | Reg loss: 0.041 | Tree loss: 0.968 | Accuracy: 0.570312 | 7.082 sec/iter\n",
      "Epoch: 73 | Batch: 020 / 030 | Total loss: 0.953 | Reg loss: 0.041 | Tree loss: 0.953 | Accuracy: 0.605469 | 7.082 sec/iter\n",
      "Epoch: 73 | Batch: 021 / 030 | Total loss: 0.962 | Reg loss: 0.042 | Tree loss: 0.962 | Accuracy: 0.580078 | 7.082 sec/iter\n",
      "Epoch: 73 | Batch: 022 / 030 | Total loss: 0.941 | Reg loss: 0.042 | Tree loss: 0.941 | Accuracy: 0.615234 | 7.082 sec/iter\n",
      "Epoch: 73 | Batch: 023 / 030 | Total loss: 0.947 | Reg loss: 0.042 | Tree loss: 0.947 | Accuracy: 0.556641 | 7.082 sec/iter\n",
      "Epoch: 73 | Batch: 024 / 030 | Total loss: 0.950 | Reg loss: 0.042 | Tree loss: 0.950 | Accuracy: 0.560547 | 7.082 sec/iter\n",
      "Epoch: 73 | Batch: 025 / 030 | Total loss: 0.914 | Reg loss: 0.042 | Tree loss: 0.914 | Accuracy: 0.593750 | 7.082 sec/iter\n",
      "Epoch: 73 | Batch: 026 / 030 | Total loss: 0.927 | Reg loss: 0.042 | Tree loss: 0.927 | Accuracy: 0.564453 | 7.081 sec/iter\n",
      "Epoch: 73 | Batch: 027 / 030 | Total loss: 0.931 | Reg loss: 0.042 | Tree loss: 0.931 | Accuracy: 0.587891 | 7.081 sec/iter\n",
      "Epoch: 73 | Batch: 028 / 030 | Total loss: 0.907 | Reg loss: 0.042 | Tree loss: 0.907 | Accuracy: 0.595703 | 7.08 sec/iter\n",
      "Epoch: 73 | Batch: 029 / 030 | Total loss: 0.914 | Reg loss: 0.042 | Tree loss: 0.914 | Accuracy: 0.572816 | 7.079 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 74 | Batch: 000 / 030 | Total loss: 1.284 | Reg loss: 0.040 | Tree loss: 1.284 | Accuracy: 0.583984 | 7.085 sec/iter\n",
      "Epoch: 74 | Batch: 001 / 030 | Total loss: 1.279 | Reg loss: 0.040 | Tree loss: 1.279 | Accuracy: 0.562500 | 7.085 sec/iter\n",
      "Epoch: 74 | Batch: 002 / 030 | Total loss: 1.237 | Reg loss: 0.040 | Tree loss: 1.237 | Accuracy: 0.566406 | 7.086 sec/iter\n",
      "Epoch: 74 | Batch: 003 / 030 | Total loss: 1.222 | Reg loss: 0.040 | Tree loss: 1.222 | Accuracy: 0.568359 | 7.086 sec/iter\n",
      "Epoch: 74 | Batch: 004 / 030 | Total loss: 1.256 | Reg loss: 0.040 | Tree loss: 1.256 | Accuracy: 0.552734 | 7.086 sec/iter\n",
      "Epoch: 74 | Batch: 005 / 030 | Total loss: 1.178 | Reg loss: 0.040 | Tree loss: 1.178 | Accuracy: 0.613281 | 7.086 sec/iter\n",
      "Epoch: 74 | Batch: 006 / 030 | Total loss: 1.181 | Reg loss: 0.040 | Tree loss: 1.181 | Accuracy: 0.599609 | 7.086 sec/iter\n",
      "Epoch: 74 | Batch: 007 / 030 | Total loss: 1.154 | Reg loss: 0.040 | Tree loss: 1.154 | Accuracy: 0.605469 | 7.086 sec/iter\n",
      "Epoch: 74 | Batch: 008 / 030 | Total loss: 1.149 | Reg loss: 0.040 | Tree loss: 1.149 | Accuracy: 0.560547 | 7.086 sec/iter\n",
      "Epoch: 74 | Batch: 009 / 030 | Total loss: 1.127 | Reg loss: 0.040 | Tree loss: 1.127 | Accuracy: 0.552734 | 7.086 sec/iter\n",
      "Epoch: 74 | Batch: 010 / 030 | Total loss: 1.129 | Reg loss: 0.040 | Tree loss: 1.129 | Accuracy: 0.546875 | 7.086 sec/iter\n",
      "Epoch: 74 | Batch: 011 / 030 | Total loss: 1.090 | Reg loss: 0.041 | Tree loss: 1.090 | Accuracy: 0.570312 | 7.086 sec/iter\n",
      "Epoch: 74 | Batch: 012 / 030 | Total loss: 1.067 | Reg loss: 0.041 | Tree loss: 1.067 | Accuracy: 0.578125 | 7.086 sec/iter\n",
      "Epoch: 74 | Batch: 013 / 030 | Total loss: 1.019 | Reg loss: 0.041 | Tree loss: 1.019 | Accuracy: 0.626953 | 7.086 sec/iter\n",
      "Epoch: 74 | Batch: 014 / 030 | Total loss: 1.045 | Reg loss: 0.041 | Tree loss: 1.045 | Accuracy: 0.570312 | 7.085 sec/iter\n",
      "Epoch: 74 | Batch: 015 / 030 | Total loss: 1.015 | Reg loss: 0.041 | Tree loss: 1.015 | Accuracy: 0.601562 | 7.084 sec/iter\n",
      "Epoch: 74 | Batch: 016 / 030 | Total loss: 1.033 | Reg loss: 0.041 | Tree loss: 1.033 | Accuracy: 0.550781 | 7.083 sec/iter\n",
      "Epoch: 74 | Batch: 017 / 030 | Total loss: 0.988 | Reg loss: 0.041 | Tree loss: 0.988 | Accuracy: 0.595703 | 7.082 sec/iter\n",
      "Epoch: 74 | Batch: 018 / 030 | Total loss: 0.993 | Reg loss: 0.041 | Tree loss: 0.993 | Accuracy: 0.558594 | 7.081 sec/iter\n",
      "Epoch: 74 | Batch: 019 / 030 | Total loss: 0.975 | Reg loss: 0.041 | Tree loss: 0.975 | Accuracy: 0.597656 | 7.08 sec/iter\n",
      "Epoch: 74 | Batch: 020 / 030 | Total loss: 0.949 | Reg loss: 0.041 | Tree loss: 0.949 | Accuracy: 0.609375 | 7.079 sec/iter\n",
      "Epoch: 74 | Batch: 021 / 030 | Total loss: 0.943 | Reg loss: 0.041 | Tree loss: 0.943 | Accuracy: 0.636719 | 7.078 sec/iter\n",
      "Epoch: 74 | Batch: 022 / 030 | Total loss: 0.950 | Reg loss: 0.042 | Tree loss: 0.950 | Accuracy: 0.580078 | 7.078 sec/iter\n",
      "Epoch: 74 | Batch: 023 / 030 | Total loss: 0.939 | Reg loss: 0.042 | Tree loss: 0.939 | Accuracy: 0.562500 | 7.078 sec/iter\n",
      "Epoch: 74 | Batch: 024 / 030 | Total loss: 0.931 | Reg loss: 0.042 | Tree loss: 0.931 | Accuracy: 0.568359 | 7.078 sec/iter\n",
      "Epoch: 74 | Batch: 025 / 030 | Total loss: 0.917 | Reg loss: 0.042 | Tree loss: 0.917 | Accuracy: 0.625000 | 7.078 sec/iter\n",
      "Epoch: 74 | Batch: 026 / 030 | Total loss: 0.914 | Reg loss: 0.042 | Tree loss: 0.914 | Accuracy: 0.595703 | 7.077 sec/iter\n",
      "Epoch: 74 | Batch: 027 / 030 | Total loss: 0.917 | Reg loss: 0.042 | Tree loss: 0.917 | Accuracy: 0.570312 | 7.076 sec/iter\n",
      "Epoch: 74 | Batch: 028 / 030 | Total loss: 0.917 | Reg loss: 0.042 | Tree loss: 0.917 | Accuracy: 0.580078 | 7.076 sec/iter\n",
      "Epoch: 74 | Batch: 029 / 030 | Total loss: 0.881 | Reg loss: 0.042 | Tree loss: 0.881 | Accuracy: 0.631068 | 7.075 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 75 | Batch: 000 / 030 | Total loss: 1.298 | Reg loss: 0.040 | Tree loss: 1.298 | Accuracy: 0.591797 | 7.08 sec/iter\n",
      "Epoch: 75 | Batch: 001 / 030 | Total loss: 1.284 | Reg loss: 0.040 | Tree loss: 1.284 | Accuracy: 0.583984 | 7.08 sec/iter\n",
      "Epoch: 75 | Batch: 002 / 030 | Total loss: 1.267 | Reg loss: 0.040 | Tree loss: 1.267 | Accuracy: 0.568359 | 7.08 sec/iter\n",
      "Epoch: 75 | Batch: 003 / 030 | Total loss: 1.252 | Reg loss: 0.040 | Tree loss: 1.252 | Accuracy: 0.570312 | 7.081 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 75 | Batch: 004 / 030 | Total loss: 1.191 | Reg loss: 0.040 | Tree loss: 1.191 | Accuracy: 0.589844 | 7.081 sec/iter\n",
      "Epoch: 75 | Batch: 005 / 030 | Total loss: 1.176 | Reg loss: 0.040 | Tree loss: 1.176 | Accuracy: 0.601562 | 7.081 sec/iter\n",
      "Epoch: 75 | Batch: 006 / 030 | Total loss: 1.184 | Reg loss: 0.040 | Tree loss: 1.184 | Accuracy: 0.568359 | 7.081 sec/iter\n",
      "Epoch: 75 | Batch: 007 / 030 | Total loss: 1.163 | Reg loss: 0.040 | Tree loss: 1.163 | Accuracy: 0.572266 | 7.081 sec/iter\n",
      "Epoch: 75 | Batch: 008 / 030 | Total loss: 1.114 | Reg loss: 0.040 | Tree loss: 1.114 | Accuracy: 0.603516 | 7.081 sec/iter\n",
      "Epoch: 75 | Batch: 009 / 030 | Total loss: 1.117 | Reg loss: 0.040 | Tree loss: 1.117 | Accuracy: 0.582031 | 7.08 sec/iter\n",
      "Epoch: 75 | Batch: 010 / 030 | Total loss: 1.128 | Reg loss: 0.040 | Tree loss: 1.128 | Accuracy: 0.560547 | 7.08 sec/iter\n",
      "Epoch: 75 | Batch: 011 / 030 | Total loss: 1.072 | Reg loss: 0.040 | Tree loss: 1.072 | Accuracy: 0.564453 | 7.08 sec/iter\n",
      "Epoch: 75 | Batch: 012 / 030 | Total loss: 1.063 | Reg loss: 0.041 | Tree loss: 1.063 | Accuracy: 0.580078 | 7.08 sec/iter\n",
      "Epoch: 75 | Batch: 013 / 030 | Total loss: 1.043 | Reg loss: 0.041 | Tree loss: 1.043 | Accuracy: 0.582031 | 7.08 sec/iter\n",
      "Epoch: 75 | Batch: 014 / 030 | Total loss: 1.004 | Reg loss: 0.041 | Tree loss: 1.004 | Accuracy: 0.619141 | 7.08 sec/iter\n",
      "Epoch: 75 | Batch: 015 / 030 | Total loss: 1.010 | Reg loss: 0.041 | Tree loss: 1.010 | Accuracy: 0.611328 | 7.08 sec/iter\n",
      "Epoch: 75 | Batch: 016 / 030 | Total loss: 1.017 | Reg loss: 0.041 | Tree loss: 1.017 | Accuracy: 0.552734 | 7.08 sec/iter\n",
      "Epoch: 75 | Batch: 017 / 030 | Total loss: 0.991 | Reg loss: 0.041 | Tree loss: 0.991 | Accuracy: 0.605469 | 7.08 sec/iter\n",
      "Epoch: 75 | Batch: 018 / 030 | Total loss: 1.000 | Reg loss: 0.041 | Tree loss: 1.000 | Accuracy: 0.548828 | 7.079 sec/iter\n",
      "Epoch: 75 | Batch: 019 / 030 | Total loss: 0.962 | Reg loss: 0.041 | Tree loss: 0.962 | Accuracy: 0.601562 | 7.078 sec/iter\n",
      "Epoch: 75 | Batch: 020 / 030 | Total loss: 0.960 | Reg loss: 0.041 | Tree loss: 0.960 | Accuracy: 0.587891 | 7.078 sec/iter\n",
      "Epoch: 75 | Batch: 021 / 030 | Total loss: 0.956 | Reg loss: 0.041 | Tree loss: 0.956 | Accuracy: 0.587891 | 7.078 sec/iter\n",
      "Epoch: 75 | Batch: 022 / 030 | Total loss: 0.941 | Reg loss: 0.041 | Tree loss: 0.941 | Accuracy: 0.595703 | 7.078 sec/iter\n",
      "Epoch: 75 | Batch: 023 / 030 | Total loss: 0.941 | Reg loss: 0.041 | Tree loss: 0.941 | Accuracy: 0.570312 | 7.078 sec/iter\n",
      "Epoch: 75 | Batch: 024 / 030 | Total loss: 0.903 | Reg loss: 0.042 | Tree loss: 0.903 | Accuracy: 0.609375 | 7.078 sec/iter\n",
      "Epoch: 75 | Batch: 025 / 030 | Total loss: 0.937 | Reg loss: 0.042 | Tree loss: 0.937 | Accuracy: 0.558594 | 7.078 sec/iter\n",
      "Epoch: 75 | Batch: 026 / 030 | Total loss: 0.890 | Reg loss: 0.042 | Tree loss: 0.890 | Accuracy: 0.605469 | 7.077 sec/iter\n",
      "Epoch: 75 | Batch: 027 / 030 | Total loss: 0.917 | Reg loss: 0.042 | Tree loss: 0.917 | Accuracy: 0.564453 | 7.076 sec/iter\n",
      "Epoch: 75 | Batch: 028 / 030 | Total loss: 0.914 | Reg loss: 0.042 | Tree loss: 0.914 | Accuracy: 0.568359 | 7.075 sec/iter\n",
      "Epoch: 75 | Batch: 029 / 030 | Total loss: 0.905 | Reg loss: 0.042 | Tree loss: 0.905 | Accuracy: 0.553398 | 7.073 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 76 | Batch: 000 / 030 | Total loss: 1.288 | Reg loss: 0.040 | Tree loss: 1.288 | Accuracy: 0.578125 | 7.078 sec/iter\n",
      "Epoch: 76 | Batch: 001 / 030 | Total loss: 1.270 | Reg loss: 0.040 | Tree loss: 1.270 | Accuracy: 0.564453 | 7.078 sec/iter\n",
      "Epoch: 76 | Batch: 002 / 030 | Total loss: 1.239 | Reg loss: 0.040 | Tree loss: 1.239 | Accuracy: 0.562500 | 7.078 sec/iter\n",
      "Epoch: 76 | Batch: 003 / 030 | Total loss: 1.236 | Reg loss: 0.040 | Tree loss: 1.236 | Accuracy: 0.568359 | 7.078 sec/iter\n",
      "Epoch: 76 | Batch: 004 / 030 | Total loss: 1.198 | Reg loss: 0.040 | Tree loss: 1.198 | Accuracy: 0.580078 | 7.078 sec/iter\n",
      "Epoch: 76 | Batch: 005 / 030 | Total loss: 1.182 | Reg loss: 0.040 | Tree loss: 1.182 | Accuracy: 0.564453 | 7.078 sec/iter\n",
      "Epoch: 76 | Batch: 006 / 030 | Total loss: 1.166 | Reg loss: 0.040 | Tree loss: 1.166 | Accuracy: 0.566406 | 7.079 sec/iter\n",
      "Epoch: 76 | Batch: 007 / 030 | Total loss: 1.152 | Reg loss: 0.040 | Tree loss: 1.152 | Accuracy: 0.583984 | 7.079 sec/iter\n",
      "Epoch: 76 | Batch: 008 / 030 | Total loss: 1.137 | Reg loss: 0.040 | Tree loss: 1.137 | Accuracy: 0.572266 | 7.079 sec/iter\n",
      "Epoch: 76 | Batch: 009 / 030 | Total loss: 1.121 | Reg loss: 0.040 | Tree loss: 1.121 | Accuracy: 0.585938 | 7.079 sec/iter\n",
      "Epoch: 76 | Batch: 010 / 030 | Total loss: 1.107 | Reg loss: 0.040 | Tree loss: 1.107 | Accuracy: 0.574219 | 7.079 sec/iter\n",
      "Epoch: 76 | Batch: 011 / 030 | Total loss: 1.081 | Reg loss: 0.040 | Tree loss: 1.081 | Accuracy: 0.583984 | 7.079 sec/iter\n",
      "Epoch: 76 | Batch: 012 / 030 | Total loss: 1.073 | Reg loss: 0.040 | Tree loss: 1.073 | Accuracy: 0.570312 | 7.079 sec/iter\n",
      "Epoch: 76 | Batch: 013 / 030 | Total loss: 1.055 | Reg loss: 0.041 | Tree loss: 1.055 | Accuracy: 0.582031 | 7.079 sec/iter\n",
      "Epoch: 76 | Batch: 014 / 030 | Total loss: 1.031 | Reg loss: 0.041 | Tree loss: 1.031 | Accuracy: 0.587891 | 7.079 sec/iter\n",
      "Epoch: 76 | Batch: 015 / 030 | Total loss: 1.007 | Reg loss: 0.041 | Tree loss: 1.007 | Accuracy: 0.582031 | 7.078 sec/iter\n",
      "Epoch: 76 | Batch: 016 / 030 | Total loss: 1.000 | Reg loss: 0.041 | Tree loss: 1.000 | Accuracy: 0.617188 | 7.077 sec/iter\n",
      "Epoch: 76 | Batch: 017 / 030 | Total loss: 1.000 | Reg loss: 0.041 | Tree loss: 1.000 | Accuracy: 0.546875 | 7.076 sec/iter\n",
      "Epoch: 76 | Batch: 018 / 030 | Total loss: 0.969 | Reg loss: 0.041 | Tree loss: 0.969 | Accuracy: 0.599609 | 7.075 sec/iter\n",
      "Epoch: 76 | Batch: 019 / 030 | Total loss: 0.962 | Reg loss: 0.041 | Tree loss: 0.962 | Accuracy: 0.585938 | 7.074 sec/iter\n",
      "Epoch: 76 | Batch: 020 / 030 | Total loss: 0.965 | Reg loss: 0.041 | Tree loss: 0.965 | Accuracy: 0.556641 | 7.073 sec/iter\n",
      "Epoch: 76 | Batch: 021 / 030 | Total loss: 0.935 | Reg loss: 0.041 | Tree loss: 0.935 | Accuracy: 0.658203 | 7.072 sec/iter\n",
      "Epoch: 76 | Batch: 022 / 030 | Total loss: 0.934 | Reg loss: 0.041 | Tree loss: 0.934 | Accuracy: 0.607422 | 7.072 sec/iter\n",
      "Epoch: 76 | Batch: 023 / 030 | Total loss: 0.957 | Reg loss: 0.041 | Tree loss: 0.957 | Accuracy: 0.554688 | 7.072 sec/iter\n",
      "Epoch: 76 | Batch: 024 / 030 | Total loss: 0.920 | Reg loss: 0.041 | Tree loss: 0.920 | Accuracy: 0.587891 | 7.072 sec/iter\n",
      "Epoch: 76 | Batch: 025 / 030 | Total loss: 0.888 | Reg loss: 0.042 | Tree loss: 0.888 | Accuracy: 0.636719 | 7.072 sec/iter\n",
      "Epoch: 76 | Batch: 026 / 030 | Total loss: 0.911 | Reg loss: 0.042 | Tree loss: 0.911 | Accuracy: 0.585938 | 7.071 sec/iter\n",
      "Epoch: 76 | Batch: 027 / 030 | Total loss: 0.921 | Reg loss: 0.042 | Tree loss: 0.921 | Accuracy: 0.556641 | 7.071 sec/iter\n",
      "Epoch: 76 | Batch: 028 / 030 | Total loss: 0.901 | Reg loss: 0.042 | Tree loss: 0.901 | Accuracy: 0.593750 | 7.07 sec/iter\n",
      "Epoch: 76 | Batch: 029 / 030 | Total loss: 0.882 | Reg loss: 0.042 | Tree loss: 0.882 | Accuracy: 0.611650 | 7.069 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 77 | Batch: 000 / 030 | Total loss: 1.265 | Reg loss: 0.040 | Tree loss: 1.265 | Accuracy: 0.621094 | 7.083 sec/iter\n",
      "Epoch: 77 | Batch: 001 / 030 | Total loss: 1.247 | Reg loss: 0.040 | Tree loss: 1.247 | Accuracy: 0.607422 | 7.083 sec/iter\n",
      "Epoch: 77 | Batch: 002 / 030 | Total loss: 1.267 | Reg loss: 0.040 | Tree loss: 1.267 | Accuracy: 0.564453 | 7.083 sec/iter\n",
      "Epoch: 77 | Batch: 003 / 030 | Total loss: 1.202 | Reg loss: 0.040 | Tree loss: 1.202 | Accuracy: 0.601562 | 7.083 sec/iter\n",
      "Epoch: 77 | Batch: 004 / 030 | Total loss: 1.231 | Reg loss: 0.040 | Tree loss: 1.231 | Accuracy: 0.585938 | 7.083 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 77 | Batch: 005 / 030 | Total loss: 1.196 | Reg loss: 0.040 | Tree loss: 1.196 | Accuracy: 0.582031 | 7.084 sec/iter\n",
      "Epoch: 77 | Batch: 006 / 030 | Total loss: 1.148 | Reg loss: 0.040 | Tree loss: 1.148 | Accuracy: 0.582031 | 7.084 sec/iter\n",
      "Epoch: 77 | Batch: 007 / 030 | Total loss: 1.146 | Reg loss: 0.040 | Tree loss: 1.146 | Accuracy: 0.607422 | 7.084 sec/iter\n",
      "Epoch: 77 | Batch: 008 / 030 | Total loss: 1.125 | Reg loss: 0.040 | Tree loss: 1.125 | Accuracy: 0.580078 | 7.084 sec/iter\n",
      "Epoch: 77 | Batch: 009 / 030 | Total loss: 1.085 | Reg loss: 0.040 | Tree loss: 1.085 | Accuracy: 0.605469 | 7.084 sec/iter\n",
      "Epoch: 77 | Batch: 010 / 030 | Total loss: 1.096 | Reg loss: 0.040 | Tree loss: 1.096 | Accuracy: 0.572266 | 7.084 sec/iter\n",
      "Epoch: 77 | Batch: 011 / 030 | Total loss: 1.092 | Reg loss: 0.040 | Tree loss: 1.092 | Accuracy: 0.513672 | 7.084 sec/iter\n",
      "Epoch: 77 | Batch: 012 / 030 | Total loss: 1.058 | Reg loss: 0.040 | Tree loss: 1.058 | Accuracy: 0.589844 | 7.084 sec/iter\n",
      "Epoch: 77 | Batch: 013 / 030 | Total loss: 1.034 | Reg loss: 0.040 | Tree loss: 1.034 | Accuracy: 0.591797 | 7.083 sec/iter\n",
      "Epoch: 77 | Batch: 014 / 030 | Total loss: 1.047 | Reg loss: 0.041 | Tree loss: 1.047 | Accuracy: 0.564453 | 7.082 sec/iter\n",
      "Epoch: 77 | Batch: 015 / 030 | Total loss: 1.017 | Reg loss: 0.041 | Tree loss: 1.017 | Accuracy: 0.570312 | 7.081 sec/iter\n",
      "Epoch: 77 | Batch: 016 / 030 | Total loss: 1.002 | Reg loss: 0.041 | Tree loss: 1.002 | Accuracy: 0.580078 | 7.08 sec/iter\n",
      "Epoch: 77 | Batch: 017 / 030 | Total loss: 0.994 | Reg loss: 0.041 | Tree loss: 0.994 | Accuracy: 0.585938 | 7.079 sec/iter\n",
      "Epoch: 77 | Batch: 018 / 030 | Total loss: 1.001 | Reg loss: 0.041 | Tree loss: 1.001 | Accuracy: 0.554688 | 7.078 sec/iter\n",
      "Epoch: 77 | Batch: 019 / 030 | Total loss: 0.957 | Reg loss: 0.041 | Tree loss: 0.957 | Accuracy: 0.593750 | 7.077 sec/iter\n",
      "Epoch: 77 | Batch: 020 / 030 | Total loss: 0.961 | Reg loss: 0.041 | Tree loss: 0.961 | Accuracy: 0.558594 | 7.076 sec/iter\n",
      "Epoch: 77 | Batch: 021 / 030 | Total loss: 0.924 | Reg loss: 0.041 | Tree loss: 0.924 | Accuracy: 0.619141 | 7.075 sec/iter\n",
      "Epoch: 77 | Batch: 022 / 030 | Total loss: 0.927 | Reg loss: 0.041 | Tree loss: 0.927 | Accuracy: 0.582031 | 7.076 sec/iter\n",
      "Epoch: 77 | Batch: 023 / 030 | Total loss: 0.930 | Reg loss: 0.041 | Tree loss: 0.930 | Accuracy: 0.580078 | 7.076 sec/iter\n",
      "Epoch: 77 | Batch: 024 / 030 | Total loss: 0.922 | Reg loss: 0.041 | Tree loss: 0.922 | Accuracy: 0.589844 | 7.076 sec/iter\n",
      "Epoch: 77 | Batch: 025 / 030 | Total loss: 0.928 | Reg loss: 0.041 | Tree loss: 0.928 | Accuracy: 0.537109 | 7.076 sec/iter\n",
      "Epoch: 77 | Batch: 026 / 030 | Total loss: 0.905 | Reg loss: 0.042 | Tree loss: 0.905 | Accuracy: 0.605469 | 7.076 sec/iter\n",
      "Epoch: 77 | Batch: 027 / 030 | Total loss: 0.905 | Reg loss: 0.042 | Tree loss: 0.905 | Accuracy: 0.580078 | 7.076 sec/iter\n",
      "Epoch: 77 | Batch: 028 / 030 | Total loss: 0.894 | Reg loss: 0.042 | Tree loss: 0.894 | Accuracy: 0.591797 | 7.077 sec/iter\n",
      "Epoch: 77 | Batch: 029 / 030 | Total loss: 0.900 | Reg loss: 0.042 | Tree loss: 0.900 | Accuracy: 0.592233 | 7.075 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 78 | Batch: 000 / 030 | Total loss: 1.284 | Reg loss: 0.040 | Tree loss: 1.284 | Accuracy: 0.578125 | 7.094 sec/iter\n",
      "Epoch: 78 | Batch: 001 / 030 | Total loss: 1.244 | Reg loss: 0.040 | Tree loss: 1.244 | Accuracy: 0.605469 | 7.094 sec/iter\n",
      "Epoch: 78 | Batch: 002 / 030 | Total loss: 1.218 | Reg loss: 0.040 | Tree loss: 1.218 | Accuracy: 0.593750 | 7.094 sec/iter\n",
      "Epoch: 78 | Batch: 003 / 030 | Total loss: 1.225 | Reg loss: 0.040 | Tree loss: 1.225 | Accuracy: 0.603516 | 7.094 sec/iter\n",
      "Epoch: 78 | Batch: 004 / 030 | Total loss: 1.211 | Reg loss: 0.040 | Tree loss: 1.211 | Accuracy: 0.558594 | 7.094 sec/iter\n",
      "Epoch: 78 | Batch: 005 / 030 | Total loss: 1.215 | Reg loss: 0.040 | Tree loss: 1.215 | Accuracy: 0.591797 | 7.094 sec/iter\n",
      "Epoch: 78 | Batch: 006 / 030 | Total loss: 1.147 | Reg loss: 0.040 | Tree loss: 1.147 | Accuracy: 0.623047 | 7.094 sec/iter\n",
      "Epoch: 78 | Batch: 007 / 030 | Total loss: 1.143 | Reg loss: 0.040 | Tree loss: 1.143 | Accuracy: 0.578125 | 7.093 sec/iter\n",
      "Epoch: 78 | Batch: 008 / 030 | Total loss: 1.113 | Reg loss: 0.040 | Tree loss: 1.113 | Accuracy: 0.562500 | 7.093 sec/iter\n",
      "Epoch: 78 | Batch: 009 / 030 | Total loss: 1.110 | Reg loss: 0.040 | Tree loss: 1.110 | Accuracy: 0.578125 | 7.093 sec/iter\n",
      "Epoch: 78 | Batch: 010 / 030 | Total loss: 1.112 | Reg loss: 0.040 | Tree loss: 1.112 | Accuracy: 0.562500 | 7.092 sec/iter\n",
      "Epoch: 78 | Batch: 011 / 030 | Total loss: 1.080 | Reg loss: 0.040 | Tree loss: 1.080 | Accuracy: 0.562500 | 7.091 sec/iter\n",
      "Epoch: 78 | Batch: 012 / 030 | Total loss: 1.062 | Reg loss: 0.040 | Tree loss: 1.062 | Accuracy: 0.582031 | 7.09 sec/iter\n",
      "Epoch: 78 | Batch: 013 / 030 | Total loss: 1.058 | Reg loss: 0.040 | Tree loss: 1.058 | Accuracy: 0.554688 | 7.09 sec/iter\n",
      "Epoch: 78 | Batch: 014 / 030 | Total loss: 1.035 | Reg loss: 0.040 | Tree loss: 1.035 | Accuracy: 0.546875 | 7.09 sec/iter\n",
      "Epoch: 78 | Batch: 015 / 030 | Total loss: 1.010 | Reg loss: 0.041 | Tree loss: 1.010 | Accuracy: 0.566406 | 7.09 sec/iter\n",
      "Epoch: 78 | Batch: 016 / 030 | Total loss: 1.007 | Reg loss: 0.041 | Tree loss: 1.007 | Accuracy: 0.570312 | 7.09 sec/iter\n",
      "Epoch: 78 | Batch: 017 / 030 | Total loss: 0.989 | Reg loss: 0.041 | Tree loss: 0.989 | Accuracy: 0.576172 | 7.09 sec/iter\n",
      "Epoch: 78 | Batch: 018 / 030 | Total loss: 0.956 | Reg loss: 0.041 | Tree loss: 0.956 | Accuracy: 0.605469 | 7.09 sec/iter\n",
      "Epoch: 78 | Batch: 019 / 030 | Total loss: 0.954 | Reg loss: 0.041 | Tree loss: 0.954 | Accuracy: 0.603516 | 7.089 sec/iter\n",
      "Epoch: 78 | Batch: 020 / 030 | Total loss: 0.949 | Reg loss: 0.041 | Tree loss: 0.949 | Accuracy: 0.566406 | 7.089 sec/iter\n",
      "Epoch: 78 | Batch: 021 / 030 | Total loss: 0.935 | Reg loss: 0.041 | Tree loss: 0.935 | Accuracy: 0.621094 | 7.089 sec/iter\n",
      "Epoch: 78 | Batch: 022 / 030 | Total loss: 0.938 | Reg loss: 0.041 | Tree loss: 0.938 | Accuracy: 0.566406 | 7.089 sec/iter\n",
      "Epoch: 78 | Batch: 023 / 030 | Total loss: 0.904 | Reg loss: 0.041 | Tree loss: 0.904 | Accuracy: 0.597656 | 7.089 sec/iter\n",
      "Epoch: 78 | Batch: 024 / 030 | Total loss: 0.902 | Reg loss: 0.041 | Tree loss: 0.902 | Accuracy: 0.615234 | 7.089 sec/iter\n",
      "Epoch: 78 | Batch: 025 / 030 | Total loss: 0.913 | Reg loss: 0.041 | Tree loss: 0.913 | Accuracy: 0.566406 | 7.089 sec/iter\n",
      "Epoch: 78 | Batch: 026 / 030 | Total loss: 0.901 | Reg loss: 0.041 | Tree loss: 0.901 | Accuracy: 0.576172 | 7.089 sec/iter\n",
      "Epoch: 78 | Batch: 027 / 030 | Total loss: 0.894 | Reg loss: 0.042 | Tree loss: 0.894 | Accuracy: 0.580078 | 7.089 sec/iter\n",
      "Epoch: 78 | Batch: 028 / 030 | Total loss: 0.892 | Reg loss: 0.042 | Tree loss: 0.892 | Accuracy: 0.609375 | 7.089 sec/iter\n",
      "Epoch: 78 | Batch: 029 / 030 | Total loss: 0.893 | Reg loss: 0.042 | Tree loss: 0.893 | Accuracy: 0.572816 | 7.088 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 79 | Batch: 000 / 030 | Total loss: 1.281 | Reg loss: 0.040 | Tree loss: 1.281 | Accuracy: 0.556641 | 7.103 sec/iter\n",
      "Epoch: 79 | Batch: 001 / 030 | Total loss: 1.232 | Reg loss: 0.040 | Tree loss: 1.232 | Accuracy: 0.574219 | 7.103 sec/iter\n",
      "Epoch: 79 | Batch: 002 / 030 | Total loss: 1.238 | Reg loss: 0.040 | Tree loss: 1.238 | Accuracy: 0.564453 | 7.103 sec/iter\n",
      "Epoch: 79 | Batch: 003 / 030 | Total loss: 1.198 | Reg loss: 0.040 | Tree loss: 1.198 | Accuracy: 0.585938 | 7.102 sec/iter\n",
      "Epoch: 79 | Batch: 004 / 030 | Total loss: 1.211 | Reg loss: 0.040 | Tree loss: 1.211 | Accuracy: 0.599609 | 7.101 sec/iter\n",
      "Epoch: 79 | Batch: 005 / 030 | Total loss: 1.209 | Reg loss: 0.040 | Tree loss: 1.209 | Accuracy: 0.578125 | 7.1 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 79 | Batch: 006 / 030 | Total loss: 1.155 | Reg loss: 0.040 | Tree loss: 1.155 | Accuracy: 0.585938 | 7.099 sec/iter\n",
      "Epoch: 79 | Batch: 007 / 030 | Total loss: 1.168 | Reg loss: 0.040 | Tree loss: 1.168 | Accuracy: 0.570312 | 7.099 sec/iter\n",
      "Epoch: 79 | Batch: 008 / 030 | Total loss: 1.131 | Reg loss: 0.040 | Tree loss: 1.131 | Accuracy: 0.578125 | 7.1 sec/iter\n",
      "Epoch: 79 | Batch: 009 / 030 | Total loss: 1.100 | Reg loss: 0.040 | Tree loss: 1.100 | Accuracy: 0.582031 | 7.1 sec/iter\n",
      "Epoch: 79 | Batch: 010 / 030 | Total loss: 1.067 | Reg loss: 0.040 | Tree loss: 1.067 | Accuracy: 0.595703 | 7.1 sec/iter\n",
      "Epoch: 79 | Batch: 011 / 030 | Total loss: 1.061 | Reg loss: 0.040 | Tree loss: 1.061 | Accuracy: 0.583984 | 7.1 sec/iter\n",
      "Epoch: 79 | Batch: 012 / 030 | Total loss: 1.052 | Reg loss: 0.040 | Tree loss: 1.052 | Accuracy: 0.585938 | 7.1 sec/iter\n",
      "Epoch: 79 | Batch: 013 / 030 | Total loss: 1.010 | Reg loss: 0.040 | Tree loss: 1.010 | Accuracy: 0.591797 | 7.1 sec/iter\n",
      "Epoch: 79 | Batch: 014 / 030 | Total loss: 1.001 | Reg loss: 0.040 | Tree loss: 1.001 | Accuracy: 0.603516 | 7.1 sec/iter\n",
      "Epoch: 79 | Batch: 015 / 030 | Total loss: 1.017 | Reg loss: 0.040 | Tree loss: 1.017 | Accuracy: 0.574219 | 7.1 sec/iter\n",
      "Epoch: 79 | Batch: 016 / 030 | Total loss: 1.005 | Reg loss: 0.040 | Tree loss: 1.005 | Accuracy: 0.580078 | 7.1 sec/iter\n",
      "Epoch: 79 | Batch: 017 / 030 | Total loss: 0.984 | Reg loss: 0.041 | Tree loss: 0.984 | Accuracy: 0.572266 | 7.1 sec/iter\n",
      "Epoch: 79 | Batch: 018 / 030 | Total loss: 0.965 | Reg loss: 0.041 | Tree loss: 0.965 | Accuracy: 0.583984 | 7.1 sec/iter\n",
      "Epoch: 79 | Batch: 019 / 030 | Total loss: 0.966 | Reg loss: 0.041 | Tree loss: 0.966 | Accuracy: 0.587891 | 7.1 sec/iter\n",
      "Epoch: 79 | Batch: 020 / 030 | Total loss: 0.961 | Reg loss: 0.041 | Tree loss: 0.961 | Accuracy: 0.583984 | 7.1 sec/iter\n",
      "Epoch: 79 | Batch: 021 / 030 | Total loss: 0.931 | Reg loss: 0.041 | Tree loss: 0.931 | Accuracy: 0.613281 | 7.1 sec/iter\n",
      "Epoch: 79 | Batch: 022 / 030 | Total loss: 0.938 | Reg loss: 0.041 | Tree loss: 0.938 | Accuracy: 0.568359 | 7.1 sec/iter\n",
      "Epoch: 79 | Batch: 023 / 030 | Total loss: 0.934 | Reg loss: 0.041 | Tree loss: 0.934 | Accuracy: 0.572266 | 7.1 sec/iter\n",
      "Epoch: 79 | Batch: 024 / 030 | Total loss: 0.911 | Reg loss: 0.041 | Tree loss: 0.911 | Accuracy: 0.609375 | 7.1 sec/iter\n",
      "Epoch: 79 | Batch: 025 / 030 | Total loss: 0.916 | Reg loss: 0.041 | Tree loss: 0.916 | Accuracy: 0.564453 | 7.1 sec/iter\n",
      "Epoch: 79 | Batch: 026 / 030 | Total loss: 0.892 | Reg loss: 0.041 | Tree loss: 0.892 | Accuracy: 0.587891 | 7.1 sec/iter\n",
      "Epoch: 79 | Batch: 027 / 030 | Total loss: 0.911 | Reg loss: 0.041 | Tree loss: 0.911 | Accuracy: 0.558594 | 7.1 sec/iter\n",
      "Epoch: 79 | Batch: 028 / 030 | Total loss: 0.895 | Reg loss: 0.041 | Tree loss: 0.895 | Accuracy: 0.591797 | 7.1 sec/iter\n",
      "Epoch: 79 | Batch: 029 / 030 | Total loss: 0.868 | Reg loss: 0.042 | Tree loss: 0.868 | Accuracy: 0.660194 | 7.099 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 80 | Batch: 000 / 030 | Total loss: 1.306 | Reg loss: 0.040 | Tree loss: 1.306 | Accuracy: 0.546875 | 7.104 sec/iter\n",
      "Epoch: 80 | Batch: 001 / 030 | Total loss: 1.239 | Reg loss: 0.040 | Tree loss: 1.239 | Accuracy: 0.617188 | 7.104 sec/iter\n",
      "Epoch: 80 | Batch: 002 / 030 | Total loss: 1.233 | Reg loss: 0.040 | Tree loss: 1.233 | Accuracy: 0.582031 | 7.103 sec/iter\n",
      "Epoch: 80 | Batch: 003 / 030 | Total loss: 1.218 | Reg loss: 0.040 | Tree loss: 1.218 | Accuracy: 0.550781 | 7.102 sec/iter\n",
      "Epoch: 80 | Batch: 004 / 030 | Total loss: 1.207 | Reg loss: 0.040 | Tree loss: 1.207 | Accuracy: 0.562500 | 7.102 sec/iter\n",
      "Epoch: 80 | Batch: 005 / 030 | Total loss: 1.181 | Reg loss: 0.040 | Tree loss: 1.181 | Accuracy: 0.587891 | 7.102 sec/iter\n",
      "Epoch: 80 | Batch: 006 / 030 | Total loss: 1.174 | Reg loss: 0.040 | Tree loss: 1.174 | Accuracy: 0.593750 | 7.102 sec/iter\n",
      "Epoch: 80 | Batch: 007 / 030 | Total loss: 1.120 | Reg loss: 0.040 | Tree loss: 1.120 | Accuracy: 0.583984 | 7.102 sec/iter\n",
      "Epoch: 80 | Batch: 008 / 030 | Total loss: 1.122 | Reg loss: 0.040 | Tree loss: 1.122 | Accuracy: 0.558594 | 7.103 sec/iter\n",
      "Epoch: 80 | Batch: 009 / 030 | Total loss: 1.115 | Reg loss: 0.040 | Tree loss: 1.115 | Accuracy: 0.587891 | 7.103 sec/iter\n",
      "Epoch: 80 | Batch: 010 / 030 | Total loss: 1.090 | Reg loss: 0.040 | Tree loss: 1.090 | Accuracy: 0.541016 | 7.103 sec/iter\n",
      "Epoch: 80 | Batch: 011 / 030 | Total loss: 1.084 | Reg loss: 0.040 | Tree loss: 1.084 | Accuracy: 0.554688 | 7.103 sec/iter\n",
      "Epoch: 80 | Batch: 012 / 030 | Total loss: 1.029 | Reg loss: 0.040 | Tree loss: 1.029 | Accuracy: 0.599609 | 7.103 sec/iter\n",
      "Epoch: 80 | Batch: 013 / 030 | Total loss: 1.040 | Reg loss: 0.040 | Tree loss: 1.040 | Accuracy: 0.566406 | 7.102 sec/iter\n",
      "Epoch: 80 | Batch: 014 / 030 | Total loss: 0.993 | Reg loss: 0.040 | Tree loss: 0.993 | Accuracy: 0.597656 | 7.102 sec/iter\n",
      "Epoch: 80 | Batch: 015 / 030 | Total loss: 0.983 | Reg loss: 0.040 | Tree loss: 0.983 | Accuracy: 0.603516 | 7.102 sec/iter\n",
      "Epoch: 80 | Batch: 016 / 030 | Total loss: 0.958 | Reg loss: 0.040 | Tree loss: 0.958 | Accuracy: 0.626953 | 7.102 sec/iter\n",
      "Epoch: 80 | Batch: 017 / 030 | Total loss: 0.984 | Reg loss: 0.041 | Tree loss: 0.984 | Accuracy: 0.576172 | 7.102 sec/iter\n",
      "Epoch: 80 | Batch: 018 / 030 | Total loss: 0.967 | Reg loss: 0.041 | Tree loss: 0.967 | Accuracy: 0.589844 | 7.102 sec/iter\n",
      "Epoch: 80 | Batch: 019 / 030 | Total loss: 0.966 | Reg loss: 0.041 | Tree loss: 0.966 | Accuracy: 0.570312 | 7.101 sec/iter\n",
      "Epoch: 80 | Batch: 020 / 030 | Total loss: 0.943 | Reg loss: 0.041 | Tree loss: 0.943 | Accuracy: 0.601562 | 7.101 sec/iter\n",
      "Epoch: 80 | Batch: 021 / 030 | Total loss: 0.933 | Reg loss: 0.041 | Tree loss: 0.933 | Accuracy: 0.605469 | 7.101 sec/iter\n",
      "Epoch: 80 | Batch: 022 / 030 | Total loss: 0.918 | Reg loss: 0.041 | Tree loss: 0.918 | Accuracy: 0.576172 | 7.101 sec/iter\n",
      "Epoch: 80 | Batch: 023 / 030 | Total loss: 0.926 | Reg loss: 0.041 | Tree loss: 0.926 | Accuracy: 0.597656 | 7.101 sec/iter\n",
      "Epoch: 80 | Batch: 024 / 030 | Total loss: 0.917 | Reg loss: 0.041 | Tree loss: 0.917 | Accuracy: 0.552734 | 7.101 sec/iter\n",
      "Epoch: 80 | Batch: 025 / 030 | Total loss: 0.892 | Reg loss: 0.041 | Tree loss: 0.892 | Accuracy: 0.595703 | 7.101 sec/iter\n",
      "Epoch: 80 | Batch: 026 / 030 | Total loss: 0.886 | Reg loss: 0.041 | Tree loss: 0.886 | Accuracy: 0.615234 | 7.101 sec/iter\n",
      "Epoch: 80 | Batch: 027 / 030 | Total loss: 0.894 | Reg loss: 0.041 | Tree loss: 0.894 | Accuracy: 0.585938 | 7.101 sec/iter\n",
      "Epoch: 80 | Batch: 028 / 030 | Total loss: 0.895 | Reg loss: 0.041 | Tree loss: 0.895 | Accuracy: 0.570312 | 7.101 sec/iter\n",
      "Epoch: 80 | Batch: 029 / 030 | Total loss: 0.872 | Reg loss: 0.041 | Tree loss: 0.872 | Accuracy: 0.592233 | 7.1 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 81 | Batch: 000 / 030 | Total loss: 1.270 | Reg loss: 0.040 | Tree loss: 1.270 | Accuracy: 0.597656 | 7.102 sec/iter\n",
      "Epoch: 81 | Batch: 001 / 030 | Total loss: 1.242 | Reg loss: 0.040 | Tree loss: 1.242 | Accuracy: 0.582031 | 7.102 sec/iter\n",
      "Epoch: 81 | Batch: 002 / 030 | Total loss: 1.214 | Reg loss: 0.040 | Tree loss: 1.214 | Accuracy: 0.605469 | 7.101 sec/iter\n",
      "Epoch: 81 | Batch: 003 / 030 | Total loss: 1.230 | Reg loss: 0.040 | Tree loss: 1.230 | Accuracy: 0.587891 | 7.1 sec/iter\n",
      "Epoch: 81 | Batch: 004 / 030 | Total loss: 1.195 | Reg loss: 0.040 | Tree loss: 1.195 | Accuracy: 0.587891 | 7.099 sec/iter\n",
      "Epoch: 81 | Batch: 005 / 030 | Total loss: 1.178 | Reg loss: 0.040 | Tree loss: 1.178 | Accuracy: 0.615234 | 7.098 sec/iter\n",
      "Epoch: 81 | Batch: 006 / 030 | Total loss: 1.131 | Reg loss: 0.040 | Tree loss: 1.131 | Accuracy: 0.626953 | 7.097 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 81 | Batch: 007 / 030 | Total loss: 1.133 | Reg loss: 0.040 | Tree loss: 1.133 | Accuracy: 0.574219 | 7.097 sec/iter\n",
      "Epoch: 81 | Batch: 008 / 030 | Total loss: 1.125 | Reg loss: 0.040 | Tree loss: 1.125 | Accuracy: 0.572266 | 7.097 sec/iter\n",
      "Epoch: 81 | Batch: 009 / 030 | Total loss: 1.065 | Reg loss: 0.040 | Tree loss: 1.065 | Accuracy: 0.611328 | 7.097 sec/iter\n",
      "Epoch: 81 | Batch: 010 / 030 | Total loss: 1.061 | Reg loss: 0.040 | Tree loss: 1.061 | Accuracy: 0.570312 | 7.097 sec/iter\n",
      "Epoch: 81 | Batch: 011 / 030 | Total loss: 1.068 | Reg loss: 0.040 | Tree loss: 1.068 | Accuracy: 0.578125 | 7.097 sec/iter\n",
      "Epoch: 81 | Batch: 012 / 030 | Total loss: 1.060 | Reg loss: 0.040 | Tree loss: 1.060 | Accuracy: 0.597656 | 7.097 sec/iter\n",
      "Epoch: 81 | Batch: 013 / 030 | Total loss: 1.059 | Reg loss: 0.040 | Tree loss: 1.059 | Accuracy: 0.556641 | 7.097 sec/iter\n",
      "Epoch: 81 | Batch: 014 / 030 | Total loss: 1.020 | Reg loss: 0.040 | Tree loss: 1.020 | Accuracy: 0.558594 | 7.097 sec/iter\n",
      "Epoch: 81 | Batch: 015 / 030 | Total loss: 1.000 | Reg loss: 0.040 | Tree loss: 1.000 | Accuracy: 0.587891 | 7.097 sec/iter\n",
      "Epoch: 81 | Batch: 016 / 030 | Total loss: 0.989 | Reg loss: 0.040 | Tree loss: 0.989 | Accuracy: 0.578125 | 7.097 sec/iter\n",
      "Epoch: 81 | Batch: 017 / 030 | Total loss: 0.971 | Reg loss: 0.040 | Tree loss: 0.971 | Accuracy: 0.587891 | 7.096 sec/iter\n",
      "Epoch: 81 | Batch: 018 / 030 | Total loss: 0.969 | Reg loss: 0.041 | Tree loss: 0.969 | Accuracy: 0.576172 | 7.096 sec/iter\n",
      "Epoch: 81 | Batch: 019 / 030 | Total loss: 0.947 | Reg loss: 0.041 | Tree loss: 0.947 | Accuracy: 0.550781 | 7.096 sec/iter\n",
      "Epoch: 81 | Batch: 020 / 030 | Total loss: 0.959 | Reg loss: 0.041 | Tree loss: 0.959 | Accuracy: 0.562500 | 7.096 sec/iter\n",
      "Epoch: 81 | Batch: 021 / 030 | Total loss: 0.949 | Reg loss: 0.041 | Tree loss: 0.949 | Accuracy: 0.574219 | 7.096 sec/iter\n",
      "Epoch: 81 | Batch: 022 / 030 | Total loss: 0.918 | Reg loss: 0.041 | Tree loss: 0.918 | Accuracy: 0.587891 | 7.096 sec/iter\n",
      "Epoch: 81 | Batch: 023 / 030 | Total loss: 0.921 | Reg loss: 0.041 | Tree loss: 0.921 | Accuracy: 0.554688 | 7.096 sec/iter\n",
      "Epoch: 81 | Batch: 024 / 030 | Total loss: 0.911 | Reg loss: 0.041 | Tree loss: 0.911 | Accuracy: 0.570312 | 7.096 sec/iter\n",
      "Epoch: 81 | Batch: 025 / 030 | Total loss: 0.897 | Reg loss: 0.041 | Tree loss: 0.897 | Accuracy: 0.572266 | 7.096 sec/iter\n",
      "Epoch: 81 | Batch: 026 / 030 | Total loss: 0.887 | Reg loss: 0.041 | Tree loss: 0.887 | Accuracy: 0.603516 | 7.096 sec/iter\n",
      "Epoch: 81 | Batch: 027 / 030 | Total loss: 0.876 | Reg loss: 0.041 | Tree loss: 0.876 | Accuracy: 0.613281 | 7.096 sec/iter\n",
      "Epoch: 81 | Batch: 028 / 030 | Total loss: 0.898 | Reg loss: 0.041 | Tree loss: 0.898 | Accuracy: 0.578125 | 7.096 sec/iter\n",
      "Epoch: 81 | Batch: 029 / 030 | Total loss: 0.923 | Reg loss: 0.041 | Tree loss: 0.923 | Accuracy: 0.485437 | 7.094 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 82 | Batch: 000 / 030 | Total loss: 1.260 | Reg loss: 0.039 | Tree loss: 1.260 | Accuracy: 0.556641 | 7.098 sec/iter\n",
      "Epoch: 82 | Batch: 001 / 030 | Total loss: 1.243 | Reg loss: 0.039 | Tree loss: 1.243 | Accuracy: 0.597656 | 7.098 sec/iter\n",
      "Epoch: 82 | Batch: 002 / 030 | Total loss: 1.211 | Reg loss: 0.039 | Tree loss: 1.211 | Accuracy: 0.583984 | 7.098 sec/iter\n",
      "Epoch: 82 | Batch: 003 / 030 | Total loss: 1.211 | Reg loss: 0.039 | Tree loss: 1.211 | Accuracy: 0.576172 | 7.098 sec/iter\n",
      "Epoch: 82 | Batch: 004 / 030 | Total loss: 1.206 | Reg loss: 0.039 | Tree loss: 1.206 | Accuracy: 0.591797 | 7.098 sec/iter\n",
      "Epoch: 82 | Batch: 005 / 030 | Total loss: 1.167 | Reg loss: 0.040 | Tree loss: 1.167 | Accuracy: 0.566406 | 7.097 sec/iter\n",
      "Epoch: 82 | Batch: 006 / 030 | Total loss: 1.175 | Reg loss: 0.040 | Tree loss: 1.175 | Accuracy: 0.564453 | 7.097 sec/iter\n",
      "Epoch: 82 | Batch: 007 / 030 | Total loss: 1.132 | Reg loss: 0.040 | Tree loss: 1.132 | Accuracy: 0.566406 | 7.097 sec/iter\n",
      "Epoch: 82 | Batch: 008 / 030 | Total loss: 1.123 | Reg loss: 0.040 | Tree loss: 1.123 | Accuracy: 0.593750 | 7.097 sec/iter\n",
      "Epoch: 82 | Batch: 009 / 030 | Total loss: 1.091 | Reg loss: 0.040 | Tree loss: 1.091 | Accuracy: 0.615234 | 7.096 sec/iter\n",
      "Epoch: 82 | Batch: 010 / 030 | Total loss: 1.054 | Reg loss: 0.040 | Tree loss: 1.054 | Accuracy: 0.597656 | 7.096 sec/iter\n",
      "Epoch: 82 | Batch: 011 / 030 | Total loss: 1.053 | Reg loss: 0.040 | Tree loss: 1.053 | Accuracy: 0.605469 | 7.096 sec/iter\n",
      "Epoch: 82 | Batch: 012 / 030 | Total loss: 1.061 | Reg loss: 0.040 | Tree loss: 1.061 | Accuracy: 0.544922 | 7.096 sec/iter\n",
      "Epoch: 82 | Batch: 013 / 030 | Total loss: 1.033 | Reg loss: 0.040 | Tree loss: 1.033 | Accuracy: 0.576172 | 7.096 sec/iter\n",
      "Epoch: 82 | Batch: 014 / 030 | Total loss: 1.009 | Reg loss: 0.040 | Tree loss: 1.009 | Accuracy: 0.597656 | 7.096 sec/iter\n",
      "Epoch: 82 | Batch: 015 / 030 | Total loss: 0.988 | Reg loss: 0.040 | Tree loss: 0.988 | Accuracy: 0.595703 | 7.096 sec/iter\n",
      "Epoch: 82 | Batch: 016 / 030 | Total loss: 0.958 | Reg loss: 0.040 | Tree loss: 0.958 | Accuracy: 0.619141 | 7.096 sec/iter\n",
      "Epoch: 82 | Batch: 017 / 030 | Total loss: 0.955 | Reg loss: 0.040 | Tree loss: 0.955 | Accuracy: 0.593750 | 7.096 sec/iter\n",
      "Epoch: 82 | Batch: 018 / 030 | Total loss: 0.970 | Reg loss: 0.040 | Tree loss: 0.970 | Accuracy: 0.570312 | 7.096 sec/iter\n",
      "Epoch: 82 | Batch: 019 / 030 | Total loss: 0.944 | Reg loss: 0.041 | Tree loss: 0.944 | Accuracy: 0.576172 | 7.096 sec/iter\n",
      "Epoch: 82 | Batch: 020 / 030 | Total loss: 0.930 | Reg loss: 0.041 | Tree loss: 0.930 | Accuracy: 0.589844 | 7.096 sec/iter\n",
      "Epoch: 82 | Batch: 021 / 030 | Total loss: 0.951 | Reg loss: 0.041 | Tree loss: 0.951 | Accuracy: 0.537109 | 7.096 sec/iter\n",
      "Epoch: 82 | Batch: 022 / 030 | Total loss: 0.920 | Reg loss: 0.041 | Tree loss: 0.920 | Accuracy: 0.580078 | 7.096 sec/iter\n",
      "Epoch: 82 | Batch: 023 / 030 | Total loss: 0.942 | Reg loss: 0.041 | Tree loss: 0.942 | Accuracy: 0.544922 | 7.096 sec/iter\n",
      "Epoch: 82 | Batch: 024 / 030 | Total loss: 0.905 | Reg loss: 0.041 | Tree loss: 0.905 | Accuracy: 0.605469 | 7.096 sec/iter\n",
      "Epoch: 82 | Batch: 025 / 030 | Total loss: 0.896 | Reg loss: 0.041 | Tree loss: 0.896 | Accuracy: 0.599609 | 7.096 sec/iter\n",
      "Epoch: 82 | Batch: 026 / 030 | Total loss: 0.887 | Reg loss: 0.041 | Tree loss: 0.887 | Accuracy: 0.585938 | 7.096 sec/iter\n",
      "Epoch: 82 | Batch: 027 / 030 | Total loss: 0.897 | Reg loss: 0.041 | Tree loss: 0.897 | Accuracy: 0.568359 | 7.096 sec/iter\n",
      "Epoch: 82 | Batch: 028 / 030 | Total loss: 0.883 | Reg loss: 0.041 | Tree loss: 0.883 | Accuracy: 0.597656 | 7.095 sec/iter\n",
      "Epoch: 82 | Batch: 029 / 030 | Total loss: 0.877 | Reg loss: 0.041 | Tree loss: 0.877 | Accuracy: 0.592233 | 7.093 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 83 | Batch: 000 / 030 | Total loss: 1.300 | Reg loss: 0.039 | Tree loss: 1.300 | Accuracy: 0.568359 | 7.099 sec/iter\n",
      "Epoch: 83 | Batch: 001 / 030 | Total loss: 1.294 | Reg loss: 0.039 | Tree loss: 1.294 | Accuracy: 0.552734 | 7.099 sec/iter\n",
      "Epoch: 83 | Batch: 002 / 030 | Total loss: 1.235 | Reg loss: 0.039 | Tree loss: 1.235 | Accuracy: 0.548828 | 7.099 sec/iter\n",
      "Epoch: 83 | Batch: 003 / 030 | Total loss: 1.205 | Reg loss: 0.039 | Tree loss: 1.205 | Accuracy: 0.589844 | 7.098 sec/iter\n",
      "Epoch: 83 | Batch: 004 / 030 | Total loss: 1.181 | Reg loss: 0.039 | Tree loss: 1.181 | Accuracy: 0.599609 | 7.097 sec/iter\n",
      "Epoch: 83 | Batch: 005 / 030 | Total loss: 1.178 | Reg loss: 0.039 | Tree loss: 1.178 | Accuracy: 0.589844 | 7.097 sec/iter\n",
      "Epoch: 83 | Batch: 006 / 030 | Total loss: 1.159 | Reg loss: 0.039 | Tree loss: 1.159 | Accuracy: 0.574219 | 7.097 sec/iter\n",
      "Epoch: 83 | Batch: 007 / 030 | Total loss: 1.133 | Reg loss: 0.040 | Tree loss: 1.133 | Accuracy: 0.558594 | 7.097 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 83 | Batch: 008 / 030 | Total loss: 1.072 | Reg loss: 0.040 | Tree loss: 1.072 | Accuracy: 0.605469 | 7.097 sec/iter\n",
      "Epoch: 83 | Batch: 009 / 030 | Total loss: 1.093 | Reg loss: 0.040 | Tree loss: 1.093 | Accuracy: 0.541016 | 7.097 sec/iter\n",
      "Epoch: 83 | Batch: 010 / 030 | Total loss: 1.059 | Reg loss: 0.040 | Tree loss: 1.059 | Accuracy: 0.582031 | 7.097 sec/iter\n",
      "Epoch: 83 | Batch: 011 / 030 | Total loss: 1.046 | Reg loss: 0.040 | Tree loss: 1.046 | Accuracy: 0.601562 | 7.097 sec/iter\n",
      "Epoch: 83 | Batch: 012 / 030 | Total loss: 1.011 | Reg loss: 0.040 | Tree loss: 1.011 | Accuracy: 0.585938 | 7.097 sec/iter\n",
      "Epoch: 83 | Batch: 013 / 030 | Total loss: 1.020 | Reg loss: 0.040 | Tree loss: 1.020 | Accuracy: 0.595703 | 7.097 sec/iter\n",
      "Epoch: 83 | Batch: 014 / 030 | Total loss: 1.012 | Reg loss: 0.040 | Tree loss: 1.012 | Accuracy: 0.597656 | 7.097 sec/iter\n",
      "Epoch: 83 | Batch: 015 / 030 | Total loss: 0.995 | Reg loss: 0.040 | Tree loss: 0.995 | Accuracy: 0.578125 | 7.097 sec/iter\n",
      "Epoch: 83 | Batch: 016 / 030 | Total loss: 0.981 | Reg loss: 0.040 | Tree loss: 0.981 | Accuracy: 0.583984 | 7.097 sec/iter\n",
      "Epoch: 83 | Batch: 017 / 030 | Total loss: 0.956 | Reg loss: 0.040 | Tree loss: 0.956 | Accuracy: 0.640625 | 7.097 sec/iter\n",
      "Epoch: 83 | Batch: 018 / 030 | Total loss: 0.939 | Reg loss: 0.040 | Tree loss: 0.939 | Accuracy: 0.595703 | 7.097 sec/iter\n",
      "Epoch: 83 | Batch: 019 / 030 | Total loss: 0.918 | Reg loss: 0.040 | Tree loss: 0.918 | Accuracy: 0.599609 | 7.097 sec/iter\n",
      "Epoch: 83 | Batch: 020 / 030 | Total loss: 0.956 | Reg loss: 0.041 | Tree loss: 0.956 | Accuracy: 0.537109 | 7.097 sec/iter\n",
      "Epoch: 83 | Batch: 021 / 030 | Total loss: 0.946 | Reg loss: 0.041 | Tree loss: 0.946 | Accuracy: 0.548828 | 7.097 sec/iter\n",
      "Epoch: 83 | Batch: 022 / 030 | Total loss: 0.917 | Reg loss: 0.041 | Tree loss: 0.917 | Accuracy: 0.605469 | 7.097 sec/iter\n",
      "Epoch: 83 | Batch: 023 / 030 | Total loss: 0.902 | Reg loss: 0.041 | Tree loss: 0.902 | Accuracy: 0.597656 | 7.097 sec/iter\n",
      "Epoch: 83 | Batch: 024 / 030 | Total loss: 0.912 | Reg loss: 0.041 | Tree loss: 0.912 | Accuracy: 0.582031 | 7.096 sec/iter\n",
      "Epoch: 83 | Batch: 025 / 030 | Total loss: 0.908 | Reg loss: 0.041 | Tree loss: 0.908 | Accuracy: 0.568359 | 7.095 sec/iter\n",
      "Epoch: 83 | Batch: 026 / 030 | Total loss: 0.893 | Reg loss: 0.041 | Tree loss: 0.893 | Accuracy: 0.595703 | 7.094 sec/iter\n",
      "Epoch: 83 | Batch: 027 / 030 | Total loss: 0.867 | Reg loss: 0.041 | Tree loss: 0.867 | Accuracy: 0.599609 | 7.093 sec/iter\n",
      "Epoch: 83 | Batch: 028 / 030 | Total loss: 0.891 | Reg loss: 0.041 | Tree loss: 0.891 | Accuracy: 0.568359 | 7.093 sec/iter\n",
      "Epoch: 83 | Batch: 029 / 030 | Total loss: 0.847 | Reg loss: 0.041 | Tree loss: 0.847 | Accuracy: 0.621359 | 7.092 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 84 | Batch: 000 / 030 | Total loss: 1.229 | Reg loss: 0.039 | Tree loss: 1.229 | Accuracy: 0.601562 | 7.097 sec/iter\n",
      "Epoch: 84 | Batch: 001 / 030 | Total loss: 1.245 | Reg loss: 0.039 | Tree loss: 1.245 | Accuracy: 0.576172 | 7.097 sec/iter\n",
      "Epoch: 84 | Batch: 002 / 030 | Total loss: 1.237 | Reg loss: 0.039 | Tree loss: 1.237 | Accuracy: 0.593750 | 7.097 sec/iter\n",
      "Epoch: 84 | Batch: 003 / 030 | Total loss: 1.209 | Reg loss: 0.039 | Tree loss: 1.209 | Accuracy: 0.580078 | 7.096 sec/iter\n",
      "Epoch: 84 | Batch: 004 / 030 | Total loss: 1.236 | Reg loss: 0.039 | Tree loss: 1.236 | Accuracy: 0.558594 | 7.095 sec/iter\n",
      "Epoch: 84 | Batch: 005 / 030 | Total loss: 1.195 | Reg loss: 0.039 | Tree loss: 1.195 | Accuracy: 0.576172 | 7.095 sec/iter\n",
      "Epoch: 84 | Batch: 006 / 030 | Total loss: 1.140 | Reg loss: 0.039 | Tree loss: 1.140 | Accuracy: 0.605469 | 7.095 sec/iter\n",
      "Epoch: 84 | Batch: 007 / 030 | Total loss: 1.134 | Reg loss: 0.039 | Tree loss: 1.134 | Accuracy: 0.585938 | 7.096 sec/iter\n",
      "Epoch: 84 | Batch: 008 / 030 | Total loss: 1.110 | Reg loss: 0.039 | Tree loss: 1.110 | Accuracy: 0.595703 | 7.096 sec/iter\n",
      "Epoch: 84 | Batch: 009 / 030 | Total loss: 1.096 | Reg loss: 0.040 | Tree loss: 1.096 | Accuracy: 0.578125 | 7.095 sec/iter\n",
      "Epoch: 84 | Batch: 010 / 030 | Total loss: 1.064 | Reg loss: 0.040 | Tree loss: 1.064 | Accuracy: 0.568359 | 7.095 sec/iter\n",
      "Epoch: 84 | Batch: 011 / 030 | Total loss: 1.067 | Reg loss: 0.040 | Tree loss: 1.067 | Accuracy: 0.566406 | 7.095 sec/iter\n",
      "Epoch: 84 | Batch: 012 / 030 | Total loss: 1.030 | Reg loss: 0.040 | Tree loss: 1.030 | Accuracy: 0.599609 | 7.095 sec/iter\n",
      "Epoch: 84 | Batch: 013 / 030 | Total loss: 1.009 | Reg loss: 0.040 | Tree loss: 1.009 | Accuracy: 0.601562 | 7.095 sec/iter\n",
      "Epoch: 84 | Batch: 014 / 030 | Total loss: 0.999 | Reg loss: 0.040 | Tree loss: 0.999 | Accuracy: 0.580078 | 7.095 sec/iter\n",
      "Epoch: 84 | Batch: 015 / 030 | Total loss: 0.986 | Reg loss: 0.040 | Tree loss: 0.986 | Accuracy: 0.576172 | 7.094 sec/iter\n",
      "Epoch: 84 | Batch: 016 / 030 | Total loss: 0.989 | Reg loss: 0.040 | Tree loss: 0.989 | Accuracy: 0.566406 | 7.094 sec/iter\n",
      "Epoch: 84 | Batch: 017 / 030 | Total loss: 0.981 | Reg loss: 0.040 | Tree loss: 0.981 | Accuracy: 0.529297 | 7.094 sec/iter\n",
      "Epoch: 84 | Batch: 018 / 030 | Total loss: 0.943 | Reg loss: 0.040 | Tree loss: 0.943 | Accuracy: 0.605469 | 7.094 sec/iter\n",
      "Epoch: 84 | Batch: 019 / 030 | Total loss: 0.952 | Reg loss: 0.040 | Tree loss: 0.952 | Accuracy: 0.574219 | 7.094 sec/iter\n",
      "Epoch: 84 | Batch: 020 / 030 | Total loss: 0.939 | Reg loss: 0.040 | Tree loss: 0.939 | Accuracy: 0.589844 | 7.094 sec/iter\n",
      "Epoch: 84 | Batch: 021 / 030 | Total loss: 0.915 | Reg loss: 0.041 | Tree loss: 0.915 | Accuracy: 0.599609 | 7.094 sec/iter\n",
      "Epoch: 84 | Batch: 022 / 030 | Total loss: 0.906 | Reg loss: 0.041 | Tree loss: 0.906 | Accuracy: 0.601562 | 7.094 sec/iter\n",
      "Epoch: 84 | Batch: 023 / 030 | Total loss: 0.895 | Reg loss: 0.041 | Tree loss: 0.895 | Accuracy: 0.603516 | 7.094 sec/iter\n",
      "Epoch: 84 | Batch: 024 / 030 | Total loss: 0.911 | Reg loss: 0.041 | Tree loss: 0.911 | Accuracy: 0.574219 | 7.094 sec/iter\n",
      "Epoch: 84 | Batch: 025 / 030 | Total loss: 0.902 | Reg loss: 0.041 | Tree loss: 0.902 | Accuracy: 0.564453 | 7.093 sec/iter\n",
      "Epoch: 84 | Batch: 026 / 030 | Total loss: 0.873 | Reg loss: 0.041 | Tree loss: 0.873 | Accuracy: 0.607422 | 7.092 sec/iter\n",
      "Epoch: 84 | Batch: 027 / 030 | Total loss: 0.882 | Reg loss: 0.041 | Tree loss: 0.882 | Accuracy: 0.568359 | 7.091 sec/iter\n",
      "Epoch: 84 | Batch: 028 / 030 | Total loss: 0.869 | Reg loss: 0.041 | Tree loss: 0.869 | Accuracy: 0.585938 | 7.09 sec/iter\n",
      "Epoch: 84 | Batch: 029 / 030 | Total loss: 0.891 | Reg loss: 0.041 | Tree loss: 0.891 | Accuracy: 0.514563 | 7.088 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 85 | Batch: 000 / 030 | Total loss: 1.250 | Reg loss: 0.039 | Tree loss: 1.250 | Accuracy: 0.593750 | 7.094 sec/iter\n",
      "Epoch: 85 | Batch: 001 / 030 | Total loss: 1.216 | Reg loss: 0.039 | Tree loss: 1.216 | Accuracy: 0.613281 | 7.094 sec/iter\n",
      "Epoch: 85 | Batch: 002 / 030 | Total loss: 1.221 | Reg loss: 0.039 | Tree loss: 1.221 | Accuracy: 0.570312 | 7.094 sec/iter\n",
      "Epoch: 85 | Batch: 003 / 030 | Total loss: 1.231 | Reg loss: 0.039 | Tree loss: 1.231 | Accuracy: 0.582031 | 7.093 sec/iter\n",
      "Epoch: 85 | Batch: 004 / 030 | Total loss: 1.186 | Reg loss: 0.039 | Tree loss: 1.186 | Accuracy: 0.599609 | 7.093 sec/iter\n",
      "Epoch: 85 | Batch: 005 / 030 | Total loss: 1.143 | Reg loss: 0.039 | Tree loss: 1.143 | Accuracy: 0.554688 | 7.093 sec/iter\n",
      "Epoch: 85 | Batch: 006 / 030 | Total loss: 1.158 | Reg loss: 0.039 | Tree loss: 1.158 | Accuracy: 0.574219 | 7.093 sec/iter\n",
      "Epoch: 85 | Batch: 007 / 030 | Total loss: 1.111 | Reg loss: 0.039 | Tree loss: 1.111 | Accuracy: 0.613281 | 7.092 sec/iter\n",
      "Epoch: 85 | Batch: 008 / 030 | Total loss: 1.120 | Reg loss: 0.039 | Tree loss: 1.120 | Accuracy: 0.583984 | 7.092 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 85 | Batch: 009 / 030 | Total loss: 1.098 | Reg loss: 0.039 | Tree loss: 1.098 | Accuracy: 0.591797 | 7.092 sec/iter\n",
      "Epoch: 85 | Batch: 010 / 030 | Total loss: 1.085 | Reg loss: 0.040 | Tree loss: 1.085 | Accuracy: 0.546875 | 7.092 sec/iter\n",
      "Epoch: 85 | Batch: 011 / 030 | Total loss: 1.050 | Reg loss: 0.040 | Tree loss: 1.050 | Accuracy: 0.583984 | 7.092 sec/iter\n",
      "Epoch: 85 | Batch: 012 / 030 | Total loss: 1.037 | Reg loss: 0.040 | Tree loss: 1.037 | Accuracy: 0.552734 | 7.092 sec/iter\n",
      "Epoch: 85 | Batch: 013 / 030 | Total loss: 1.010 | Reg loss: 0.040 | Tree loss: 1.010 | Accuracy: 0.566406 | 7.092 sec/iter\n",
      "Epoch: 85 | Batch: 014 / 030 | Total loss: 1.005 | Reg loss: 0.040 | Tree loss: 1.005 | Accuracy: 0.595703 | 7.092 sec/iter\n",
      "Epoch: 85 | Batch: 015 / 030 | Total loss: 1.014 | Reg loss: 0.040 | Tree loss: 1.014 | Accuracy: 0.558594 | 7.092 sec/iter\n",
      "Epoch: 85 | Batch: 016 / 030 | Total loss: 0.944 | Reg loss: 0.040 | Tree loss: 0.944 | Accuracy: 0.636719 | 7.092 sec/iter\n",
      "Epoch: 85 | Batch: 017 / 030 | Total loss: 0.964 | Reg loss: 0.040 | Tree loss: 0.964 | Accuracy: 0.568359 | 7.092 sec/iter\n",
      "Epoch: 85 | Batch: 018 / 030 | Total loss: 0.951 | Reg loss: 0.040 | Tree loss: 0.951 | Accuracy: 0.603516 | 7.092 sec/iter\n",
      "Epoch: 85 | Batch: 019 / 030 | Total loss: 0.931 | Reg loss: 0.040 | Tree loss: 0.931 | Accuracy: 0.595703 | 7.092 sec/iter\n",
      "Epoch: 85 | Batch: 020 / 030 | Total loss: 0.927 | Reg loss: 0.040 | Tree loss: 0.927 | Accuracy: 0.623047 | 7.092 sec/iter\n",
      "Epoch: 85 | Batch: 021 / 030 | Total loss: 0.919 | Reg loss: 0.040 | Tree loss: 0.919 | Accuracy: 0.582031 | 7.092 sec/iter\n",
      "Epoch: 85 | Batch: 022 / 030 | Total loss: 0.931 | Reg loss: 0.041 | Tree loss: 0.931 | Accuracy: 0.556641 | 7.092 sec/iter\n",
      "Epoch: 85 | Batch: 023 / 030 | Total loss: 0.900 | Reg loss: 0.041 | Tree loss: 0.900 | Accuracy: 0.593750 | 7.092 sec/iter\n",
      "Epoch: 85 | Batch: 024 / 030 | Total loss: 0.898 | Reg loss: 0.041 | Tree loss: 0.898 | Accuracy: 0.558594 | 7.092 sec/iter\n",
      "Epoch: 85 | Batch: 025 / 030 | Total loss: 0.919 | Reg loss: 0.041 | Tree loss: 0.919 | Accuracy: 0.542969 | 7.091 sec/iter\n",
      "Epoch: 85 | Batch: 026 / 030 | Total loss: 0.885 | Reg loss: 0.041 | Tree loss: 0.885 | Accuracy: 0.576172 | 7.09 sec/iter\n",
      "Epoch: 85 | Batch: 027 / 030 | Total loss: 0.877 | Reg loss: 0.041 | Tree loss: 0.877 | Accuracy: 0.601562 | 7.089 sec/iter\n",
      "Epoch: 85 | Batch: 028 / 030 | Total loss: 0.883 | Reg loss: 0.041 | Tree loss: 0.883 | Accuracy: 0.582031 | 7.088 sec/iter\n",
      "Epoch: 85 | Batch: 029 / 030 | Total loss: 0.889 | Reg loss: 0.041 | Tree loss: 0.889 | Accuracy: 0.572816 | 7.086 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 86 | Batch: 000 / 030 | Total loss: 1.243 | Reg loss: 0.039 | Tree loss: 1.243 | Accuracy: 0.607422 | 7.092 sec/iter\n",
      "Epoch: 86 | Batch: 001 / 030 | Total loss: 1.254 | Reg loss: 0.039 | Tree loss: 1.254 | Accuracy: 0.585938 | 7.092 sec/iter\n",
      "Epoch: 86 | Batch: 002 / 030 | Total loss: 1.250 | Reg loss: 0.039 | Tree loss: 1.250 | Accuracy: 0.597656 | 7.092 sec/iter\n",
      "Epoch: 86 | Batch: 003 / 030 | Total loss: 1.240 | Reg loss: 0.039 | Tree loss: 1.240 | Accuracy: 0.544922 | 7.092 sec/iter\n",
      "Epoch: 86 | Batch: 004 / 030 | Total loss: 1.170 | Reg loss: 0.039 | Tree loss: 1.170 | Accuracy: 0.587891 | 7.092 sec/iter\n",
      "Epoch: 86 | Batch: 005 / 030 | Total loss: 1.165 | Reg loss: 0.039 | Tree loss: 1.165 | Accuracy: 0.574219 | 7.091 sec/iter\n",
      "Epoch: 86 | Batch: 006 / 030 | Total loss: 1.134 | Reg loss: 0.039 | Tree loss: 1.134 | Accuracy: 0.617188 | 7.091 sec/iter\n",
      "Epoch: 86 | Batch: 007 / 030 | Total loss: 1.094 | Reg loss: 0.039 | Tree loss: 1.094 | Accuracy: 0.609375 | 7.091 sec/iter\n",
      "Epoch: 86 | Batch: 008 / 030 | Total loss: 1.092 | Reg loss: 0.039 | Tree loss: 1.092 | Accuracy: 0.601562 | 7.091 sec/iter\n",
      "Epoch: 86 | Batch: 009 / 030 | Total loss: 1.055 | Reg loss: 0.039 | Tree loss: 1.055 | Accuracy: 0.580078 | 7.09 sec/iter\n",
      "Epoch: 86 | Batch: 010 / 030 | Total loss: 1.086 | Reg loss: 0.039 | Tree loss: 1.086 | Accuracy: 0.564453 | 7.09 sec/iter\n",
      "Epoch: 86 | Batch: 011 / 030 | Total loss: 1.054 | Reg loss: 0.040 | Tree loss: 1.054 | Accuracy: 0.582031 | 7.09 sec/iter\n",
      "Epoch: 86 | Batch: 012 / 030 | Total loss: 1.018 | Reg loss: 0.040 | Tree loss: 1.018 | Accuracy: 0.572266 | 7.091 sec/iter\n",
      "Epoch: 86 | Batch: 013 / 030 | Total loss: 1.022 | Reg loss: 0.040 | Tree loss: 1.022 | Accuracy: 0.568359 | 7.091 sec/iter\n",
      "Epoch: 86 | Batch: 014 / 030 | Total loss: 1.028 | Reg loss: 0.040 | Tree loss: 1.028 | Accuracy: 0.550781 | 7.091 sec/iter\n",
      "Epoch: 86 | Batch: 015 / 030 | Total loss: 0.995 | Reg loss: 0.040 | Tree loss: 0.995 | Accuracy: 0.560547 | 7.091 sec/iter\n",
      "Epoch: 86 | Batch: 016 / 030 | Total loss: 0.952 | Reg loss: 0.040 | Tree loss: 0.952 | Accuracy: 0.607422 | 7.091 sec/iter\n",
      "Epoch: 86 | Batch: 017 / 030 | Total loss: 0.949 | Reg loss: 0.040 | Tree loss: 0.949 | Accuracy: 0.599609 | 7.091 sec/iter\n",
      "Epoch: 86 | Batch: 018 / 030 | Total loss: 0.961 | Reg loss: 0.040 | Tree loss: 0.961 | Accuracy: 0.560547 | 7.091 sec/iter\n",
      "Epoch: 86 | Batch: 019 / 030 | Total loss: 0.955 | Reg loss: 0.040 | Tree loss: 0.955 | Accuracy: 0.546875 | 7.091 sec/iter\n",
      "Epoch: 86 | Batch: 020 / 030 | Total loss: 0.931 | Reg loss: 0.040 | Tree loss: 0.931 | Accuracy: 0.582031 | 7.091 sec/iter\n",
      "Epoch: 86 | Batch: 021 / 030 | Total loss: 0.893 | Reg loss: 0.040 | Tree loss: 0.893 | Accuracy: 0.599609 | 7.091 sec/iter\n",
      "Epoch: 86 | Batch: 022 / 030 | Total loss: 0.892 | Reg loss: 0.040 | Tree loss: 0.892 | Accuracy: 0.617188 | 7.091 sec/iter\n",
      "Epoch: 86 | Batch: 023 / 030 | Total loss: 0.888 | Reg loss: 0.041 | Tree loss: 0.888 | Accuracy: 0.613281 | 7.091 sec/iter\n",
      "Epoch: 86 | Batch: 024 / 030 | Total loss: 0.924 | Reg loss: 0.041 | Tree loss: 0.924 | Accuracy: 0.548828 | 7.091 sec/iter\n",
      "Epoch: 86 | Batch: 025 / 030 | Total loss: 0.907 | Reg loss: 0.041 | Tree loss: 0.907 | Accuracy: 0.552734 | 7.09 sec/iter\n",
      "Epoch: 86 | Batch: 026 / 030 | Total loss: 0.873 | Reg loss: 0.041 | Tree loss: 0.873 | Accuracy: 0.609375 | 7.089 sec/iter\n",
      "Epoch: 86 | Batch: 027 / 030 | Total loss: 0.878 | Reg loss: 0.041 | Tree loss: 0.878 | Accuracy: 0.583984 | 7.089 sec/iter\n",
      "Epoch: 86 | Batch: 028 / 030 | Total loss: 0.882 | Reg loss: 0.041 | Tree loss: 0.882 | Accuracy: 0.566406 | 7.089 sec/iter\n",
      "Epoch: 86 | Batch: 029 / 030 | Total loss: 0.854 | Reg loss: 0.041 | Tree loss: 0.854 | Accuracy: 0.621359 | 7.088 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 87 | Batch: 000 / 030 | Total loss: 1.256 | Reg loss: 0.039 | Tree loss: 1.256 | Accuracy: 0.582031 | 7.091 sec/iter\n",
      "Epoch: 87 | Batch: 001 / 030 | Total loss: 1.231 | Reg loss: 0.039 | Tree loss: 1.231 | Accuracy: 0.609375 | 7.09 sec/iter\n",
      "Epoch: 87 | Batch: 002 / 030 | Total loss: 1.241 | Reg loss: 0.039 | Tree loss: 1.241 | Accuracy: 0.539062 | 7.09 sec/iter\n",
      "Epoch: 87 | Batch: 003 / 030 | Total loss: 1.190 | Reg loss: 0.039 | Tree loss: 1.190 | Accuracy: 0.542969 | 7.09 sec/iter\n",
      "Epoch: 87 | Batch: 004 / 030 | Total loss: 1.169 | Reg loss: 0.039 | Tree loss: 1.169 | Accuracy: 0.607422 | 7.09 sec/iter\n",
      "Epoch: 87 | Batch: 005 / 030 | Total loss: 1.168 | Reg loss: 0.039 | Tree loss: 1.168 | Accuracy: 0.587891 | 7.091 sec/iter\n",
      "Epoch: 87 | Batch: 006 / 030 | Total loss: 1.094 | Reg loss: 0.039 | Tree loss: 1.094 | Accuracy: 0.628906 | 7.091 sec/iter\n",
      "Epoch: 87 | Batch: 007 / 030 | Total loss: 1.115 | Reg loss: 0.039 | Tree loss: 1.115 | Accuracy: 0.595703 | 7.091 sec/iter\n",
      "Epoch: 87 | Batch: 008 / 030 | Total loss: 1.125 | Reg loss: 0.039 | Tree loss: 1.125 | Accuracy: 0.529297 | 7.091 sec/iter\n",
      "Epoch: 87 | Batch: 009 / 030 | Total loss: 1.107 | Reg loss: 0.039 | Tree loss: 1.107 | Accuracy: 0.574219 | 7.091 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 87 | Batch: 010 / 030 | Total loss: 1.064 | Reg loss: 0.039 | Tree loss: 1.064 | Accuracy: 0.568359 | 7.091 sec/iter\n",
      "Epoch: 87 | Batch: 011 / 030 | Total loss: 1.050 | Reg loss: 0.039 | Tree loss: 1.050 | Accuracy: 0.556641 | 7.091 sec/iter\n",
      "Epoch: 87 | Batch: 012 / 030 | Total loss: 1.047 | Reg loss: 0.039 | Tree loss: 1.047 | Accuracy: 0.566406 | 7.091 sec/iter\n",
      "Epoch: 87 | Batch: 013 / 030 | Total loss: 1.005 | Reg loss: 0.040 | Tree loss: 1.005 | Accuracy: 0.625000 | 7.091 sec/iter\n",
      "Epoch: 87 | Batch: 014 / 030 | Total loss: 0.993 | Reg loss: 0.040 | Tree loss: 0.993 | Accuracy: 0.582031 | 7.091 sec/iter\n",
      "Epoch: 87 | Batch: 015 / 030 | Total loss: 0.976 | Reg loss: 0.040 | Tree loss: 0.976 | Accuracy: 0.593750 | 7.091 sec/iter\n",
      "Epoch: 87 | Batch: 016 / 030 | Total loss: 0.970 | Reg loss: 0.040 | Tree loss: 0.970 | Accuracy: 0.585938 | 7.091 sec/iter\n",
      "Epoch: 87 | Batch: 017 / 030 | Total loss: 0.954 | Reg loss: 0.040 | Tree loss: 0.954 | Accuracy: 0.621094 | 7.091 sec/iter\n",
      "Epoch: 87 | Batch: 018 / 030 | Total loss: 0.960 | Reg loss: 0.040 | Tree loss: 0.960 | Accuracy: 0.576172 | 7.091 sec/iter\n",
      "Epoch: 87 | Batch: 019 / 030 | Total loss: 0.953 | Reg loss: 0.040 | Tree loss: 0.953 | Accuracy: 0.562500 | 7.091 sec/iter\n",
      "Epoch: 87 | Batch: 020 / 030 | Total loss: 0.919 | Reg loss: 0.040 | Tree loss: 0.919 | Accuracy: 0.587891 | 7.091 sec/iter\n",
      "Epoch: 87 | Batch: 021 / 030 | Total loss: 0.903 | Reg loss: 0.040 | Tree loss: 0.903 | Accuracy: 0.601562 | 7.091 sec/iter\n",
      "Epoch: 87 | Batch: 022 / 030 | Total loss: 0.921 | Reg loss: 0.040 | Tree loss: 0.921 | Accuracy: 0.568359 | 7.09 sec/iter\n",
      "Epoch: 87 | Batch: 023 / 030 | Total loss: 0.889 | Reg loss: 0.040 | Tree loss: 0.889 | Accuracy: 0.593750 | 7.089 sec/iter\n",
      "Epoch: 87 | Batch: 024 / 030 | Total loss: 0.917 | Reg loss: 0.041 | Tree loss: 0.917 | Accuracy: 0.544922 | 7.089 sec/iter\n",
      "Epoch: 87 | Batch: 025 / 030 | Total loss: 0.869 | Reg loss: 0.041 | Tree loss: 0.869 | Accuracy: 0.615234 | 7.088 sec/iter\n",
      "Epoch: 87 | Batch: 026 / 030 | Total loss: 0.887 | Reg loss: 0.041 | Tree loss: 0.887 | Accuracy: 0.589844 | 7.087 sec/iter\n",
      "Epoch: 87 | Batch: 027 / 030 | Total loss: 0.884 | Reg loss: 0.041 | Tree loss: 0.884 | Accuracy: 0.582031 | 7.086 sec/iter\n",
      "Epoch: 87 | Batch: 028 / 030 | Total loss: 0.868 | Reg loss: 0.041 | Tree loss: 0.868 | Accuracy: 0.601562 | 7.085 sec/iter\n",
      "Epoch: 87 | Batch: 029 / 030 | Total loss: 0.905 | Reg loss: 0.041 | Tree loss: 0.905 | Accuracy: 0.485437 | 7.084 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 88 | Batch: 000 / 030 | Total loss: 1.250 | Reg loss: 0.039 | Tree loss: 1.250 | Accuracy: 0.572266 | 7.085 sec/iter\n",
      "Epoch: 88 | Batch: 001 / 030 | Total loss: 1.216 | Reg loss: 0.039 | Tree loss: 1.216 | Accuracy: 0.609375 | 7.084 sec/iter\n",
      "Epoch: 88 | Batch: 002 / 030 | Total loss: 1.234 | Reg loss: 0.039 | Tree loss: 1.234 | Accuracy: 0.566406 | 7.085 sec/iter\n",
      "Epoch: 88 | Batch: 003 / 030 | Total loss: 1.203 | Reg loss: 0.039 | Tree loss: 1.203 | Accuracy: 0.599609 | 7.085 sec/iter\n",
      "Epoch: 88 | Batch: 004 / 030 | Total loss: 1.168 | Reg loss: 0.039 | Tree loss: 1.168 | Accuracy: 0.617188 | 7.085 sec/iter\n",
      "Epoch: 88 | Batch: 005 / 030 | Total loss: 1.156 | Reg loss: 0.039 | Tree loss: 1.156 | Accuracy: 0.613281 | 7.085 sec/iter\n",
      "Epoch: 88 | Batch: 006 / 030 | Total loss: 1.150 | Reg loss: 0.039 | Tree loss: 1.150 | Accuracy: 0.570312 | 7.085 sec/iter\n",
      "Epoch: 88 | Batch: 007 / 030 | Total loss: 1.148 | Reg loss: 0.039 | Tree loss: 1.148 | Accuracy: 0.560547 | 7.085 sec/iter\n",
      "Epoch: 88 | Batch: 008 / 030 | Total loss: 1.100 | Reg loss: 0.039 | Tree loss: 1.100 | Accuracy: 0.591797 | 7.085 sec/iter\n",
      "Epoch: 88 | Batch: 009 / 030 | Total loss: 1.103 | Reg loss: 0.039 | Tree loss: 1.103 | Accuracy: 0.552734 | 7.085 sec/iter\n",
      "Epoch: 88 | Batch: 010 / 030 | Total loss: 1.087 | Reg loss: 0.039 | Tree loss: 1.087 | Accuracy: 0.568359 | 7.085 sec/iter\n",
      "Epoch: 88 | Batch: 011 / 030 | Total loss: 1.042 | Reg loss: 0.039 | Tree loss: 1.042 | Accuracy: 0.568359 | 7.085 sec/iter\n",
      "Epoch: 88 | Batch: 012 / 030 | Total loss: 1.032 | Reg loss: 0.039 | Tree loss: 1.032 | Accuracy: 0.546875 | 7.085 sec/iter\n",
      "Epoch: 88 | Batch: 013 / 030 | Total loss: 1.024 | Reg loss: 0.040 | Tree loss: 1.024 | Accuracy: 0.568359 | 7.085 sec/iter\n",
      "Epoch: 88 | Batch: 014 / 030 | Total loss: 1.003 | Reg loss: 0.040 | Tree loss: 1.003 | Accuracy: 0.560547 | 7.085 sec/iter\n",
      "Epoch: 88 | Batch: 015 / 030 | Total loss: 0.939 | Reg loss: 0.040 | Tree loss: 0.939 | Accuracy: 0.638672 | 7.085 sec/iter\n",
      "Epoch: 88 | Batch: 016 / 030 | Total loss: 0.957 | Reg loss: 0.040 | Tree loss: 0.957 | Accuracy: 0.580078 | 7.085 sec/iter\n",
      "Epoch: 88 | Batch: 017 / 030 | Total loss: 0.946 | Reg loss: 0.040 | Tree loss: 0.946 | Accuracy: 0.585938 | 7.085 sec/iter\n",
      "Epoch: 88 | Batch: 018 / 030 | Total loss: 0.928 | Reg loss: 0.040 | Tree loss: 0.928 | Accuracy: 0.582031 | 7.085 sec/iter\n",
      "Epoch: 88 | Batch: 019 / 030 | Total loss: 0.950 | Reg loss: 0.040 | Tree loss: 0.950 | Accuracy: 0.539062 | 7.085 sec/iter\n",
      "Epoch: 88 | Batch: 020 / 030 | Total loss: 0.952 | Reg loss: 0.040 | Tree loss: 0.952 | Accuracy: 0.554688 | 7.085 sec/iter\n",
      "Epoch: 88 | Batch: 021 / 030 | Total loss: 0.912 | Reg loss: 0.040 | Tree loss: 0.912 | Accuracy: 0.582031 | 7.085 sec/iter\n",
      "Epoch: 88 | Batch: 022 / 030 | Total loss: 0.910 | Reg loss: 0.040 | Tree loss: 0.910 | Accuracy: 0.585938 | 7.085 sec/iter\n",
      "Epoch: 88 | Batch: 023 / 030 | Total loss: 0.879 | Reg loss: 0.040 | Tree loss: 0.879 | Accuracy: 0.613281 | 7.085 sec/iter\n",
      "Epoch: 88 | Batch: 024 / 030 | Total loss: 0.884 | Reg loss: 0.040 | Tree loss: 0.884 | Accuracy: 0.619141 | 7.085 sec/iter\n",
      "Epoch: 88 | Batch: 025 / 030 | Total loss: 0.878 | Reg loss: 0.041 | Tree loss: 0.878 | Accuracy: 0.599609 | 7.085 sec/iter\n",
      "Epoch: 88 | Batch: 026 / 030 | Total loss: 0.894 | Reg loss: 0.041 | Tree loss: 0.894 | Accuracy: 0.570312 | 7.084 sec/iter\n",
      "Epoch: 88 | Batch: 027 / 030 | Total loss: 0.865 | Reg loss: 0.041 | Tree loss: 0.865 | Accuracy: 0.607422 | 7.083 sec/iter\n",
      "Epoch: 88 | Batch: 028 / 030 | Total loss: 0.870 | Reg loss: 0.041 | Tree loss: 0.870 | Accuracy: 0.570312 | 7.082 sec/iter\n",
      "Epoch: 88 | Batch: 029 / 030 | Total loss: 0.828 | Reg loss: 0.041 | Tree loss: 0.828 | Accuracy: 0.611650 | 7.081 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 89 | Batch: 000 / 030 | Total loss: 1.260 | Reg loss: 0.039 | Tree loss: 1.260 | Accuracy: 0.574219 | 7.083 sec/iter\n",
      "Epoch: 89 | Batch: 001 / 030 | Total loss: 1.224 | Reg loss: 0.039 | Tree loss: 1.224 | Accuracy: 0.611328 | 7.083 sec/iter\n",
      "Epoch: 89 | Batch: 002 / 030 | Total loss: 1.245 | Reg loss: 0.039 | Tree loss: 1.245 | Accuracy: 0.601562 | 7.083 sec/iter\n",
      "Epoch: 89 | Batch: 003 / 030 | Total loss: 1.196 | Reg loss: 0.039 | Tree loss: 1.196 | Accuracy: 0.593750 | 7.083 sec/iter\n",
      "Epoch: 89 | Batch: 004 / 030 | Total loss: 1.219 | Reg loss: 0.039 | Tree loss: 1.219 | Accuracy: 0.609375 | 7.084 sec/iter\n",
      "Epoch: 89 | Batch: 005 / 030 | Total loss: 1.154 | Reg loss: 0.039 | Tree loss: 1.154 | Accuracy: 0.597656 | 7.084 sec/iter\n",
      "Epoch: 89 | Batch: 006 / 030 | Total loss: 1.113 | Reg loss: 0.039 | Tree loss: 1.113 | Accuracy: 0.583984 | 7.084 sec/iter\n",
      "Epoch: 89 | Batch: 007 / 030 | Total loss: 1.121 | Reg loss: 0.039 | Tree loss: 1.121 | Accuracy: 0.593750 | 7.084 sec/iter\n",
      "Epoch: 89 | Batch: 008 / 030 | Total loss: 1.095 | Reg loss: 0.039 | Tree loss: 1.095 | Accuracy: 0.595703 | 7.084 sec/iter\n",
      "Epoch: 89 | Batch: 009 / 030 | Total loss: 1.071 | Reg loss: 0.039 | Tree loss: 1.071 | Accuracy: 0.607422 | 7.084 sec/iter\n",
      "Epoch: 89 | Batch: 010 / 030 | Total loss: 1.075 | Reg loss: 0.039 | Tree loss: 1.075 | Accuracy: 0.568359 | 7.084 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 89 | Batch: 011 / 030 | Total loss: 1.053 | Reg loss: 0.039 | Tree loss: 1.053 | Accuracy: 0.583984 | 7.084 sec/iter\n",
      "Epoch: 89 | Batch: 012 / 030 | Total loss: 1.040 | Reg loss: 0.039 | Tree loss: 1.040 | Accuracy: 0.546875 | 7.084 sec/iter\n",
      "Epoch: 89 | Batch: 013 / 030 | Total loss: 0.992 | Reg loss: 0.039 | Tree loss: 0.992 | Accuracy: 0.591797 | 7.084 sec/iter\n",
      "Epoch: 89 | Batch: 014 / 030 | Total loss: 0.991 | Reg loss: 0.040 | Tree loss: 0.991 | Accuracy: 0.583984 | 7.084 sec/iter\n",
      "Epoch: 89 | Batch: 015 / 030 | Total loss: 0.982 | Reg loss: 0.040 | Tree loss: 0.982 | Accuracy: 0.550781 | 7.084 sec/iter\n",
      "Epoch: 89 | Batch: 016 / 030 | Total loss: 0.954 | Reg loss: 0.040 | Tree loss: 0.954 | Accuracy: 0.587891 | 7.084 sec/iter\n",
      "Epoch: 89 | Batch: 017 / 030 | Total loss: 0.948 | Reg loss: 0.040 | Tree loss: 0.948 | Accuracy: 0.589844 | 7.084 sec/iter\n",
      "Epoch: 89 | Batch: 018 / 030 | Total loss: 0.962 | Reg loss: 0.040 | Tree loss: 0.962 | Accuracy: 0.562500 | 7.084 sec/iter\n",
      "Epoch: 89 | Batch: 019 / 030 | Total loss: 0.929 | Reg loss: 0.040 | Tree loss: 0.929 | Accuracy: 0.593750 | 7.084 sec/iter\n",
      "Epoch: 89 | Batch: 020 / 030 | Total loss: 0.925 | Reg loss: 0.040 | Tree loss: 0.925 | Accuracy: 0.578125 | 7.084 sec/iter\n",
      "Epoch: 89 | Batch: 021 / 030 | Total loss: 0.913 | Reg loss: 0.040 | Tree loss: 0.913 | Accuracy: 0.568359 | 7.084 sec/iter\n",
      "Epoch: 89 | Batch: 022 / 030 | Total loss: 0.900 | Reg loss: 0.040 | Tree loss: 0.900 | Accuracy: 0.591797 | 7.084 sec/iter\n",
      "Epoch: 89 | Batch: 023 / 030 | Total loss: 0.909 | Reg loss: 0.040 | Tree loss: 0.909 | Accuracy: 0.554688 | 7.084 sec/iter\n",
      "Epoch: 89 | Batch: 024 / 030 | Total loss: 0.897 | Reg loss: 0.040 | Tree loss: 0.897 | Accuracy: 0.548828 | 7.084 sec/iter\n",
      "Epoch: 89 | Batch: 025 / 030 | Total loss: 0.876 | Reg loss: 0.040 | Tree loss: 0.876 | Accuracy: 0.593750 | 7.083 sec/iter\n",
      "Epoch: 89 | Batch: 026 / 030 | Total loss: 0.865 | Reg loss: 0.040 | Tree loss: 0.865 | Accuracy: 0.617188 | 7.082 sec/iter\n",
      "Epoch: 89 | Batch: 027 / 030 | Total loss: 0.877 | Reg loss: 0.041 | Tree loss: 0.877 | Accuracy: 0.564453 | 7.081 sec/iter\n",
      "Epoch: 89 | Batch: 028 / 030 | Total loss: 0.872 | Reg loss: 0.041 | Tree loss: 0.872 | Accuracy: 0.566406 | 7.08 sec/iter\n",
      "Epoch: 89 | Batch: 029 / 030 | Total loss: 0.887 | Reg loss: 0.041 | Tree loss: 0.887 | Accuracy: 0.524272 | 7.079 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 90 | Batch: 000 / 030 | Total loss: 1.276 | Reg loss: 0.039 | Tree loss: 1.276 | Accuracy: 0.582031 | 7.081 sec/iter\n",
      "Epoch: 90 | Batch: 001 / 030 | Total loss: 1.202 | Reg loss: 0.039 | Tree loss: 1.202 | Accuracy: 0.636719 | 7.081 sec/iter\n",
      "Epoch: 90 | Batch: 002 / 030 | Total loss: 1.240 | Reg loss: 0.039 | Tree loss: 1.240 | Accuracy: 0.568359 | 7.081 sec/iter\n",
      "Epoch: 90 | Batch: 003 / 030 | Total loss: 1.213 | Reg loss: 0.039 | Tree loss: 1.213 | Accuracy: 0.562500 | 7.081 sec/iter\n",
      "Epoch: 90 | Batch: 004 / 030 | Total loss: 1.175 | Reg loss: 0.039 | Tree loss: 1.175 | Accuracy: 0.599609 | 7.081 sec/iter\n",
      "Epoch: 90 | Batch: 005 / 030 | Total loss: 1.127 | Reg loss: 0.039 | Tree loss: 1.127 | Accuracy: 0.589844 | 7.081 sec/iter\n",
      "Epoch: 90 | Batch: 006 / 030 | Total loss: 1.138 | Reg loss: 0.039 | Tree loss: 1.138 | Accuracy: 0.560547 | 7.081 sec/iter\n",
      "Epoch: 90 | Batch: 007 / 030 | Total loss: 1.100 | Reg loss: 0.039 | Tree loss: 1.100 | Accuracy: 0.595703 | 7.081 sec/iter\n",
      "Epoch: 90 | Batch: 008 / 030 | Total loss: 1.114 | Reg loss: 0.039 | Tree loss: 1.114 | Accuracy: 0.564453 | 7.081 sec/iter\n",
      "Epoch: 90 | Batch: 009 / 030 | Total loss: 1.080 | Reg loss: 0.039 | Tree loss: 1.080 | Accuracy: 0.568359 | 7.081 sec/iter\n",
      "Epoch: 90 | Batch: 010 / 030 | Total loss: 1.086 | Reg loss: 0.039 | Tree loss: 1.086 | Accuracy: 0.542969 | 7.081 sec/iter\n",
      "Epoch: 90 | Batch: 011 / 030 | Total loss: 1.010 | Reg loss: 0.039 | Tree loss: 1.010 | Accuracy: 0.603516 | 7.081 sec/iter\n",
      "Epoch: 90 | Batch: 012 / 030 | Total loss: 1.027 | Reg loss: 0.039 | Tree loss: 1.027 | Accuracy: 0.582031 | 7.081 sec/iter\n",
      "Epoch: 90 | Batch: 013 / 030 | Total loss: 1.002 | Reg loss: 0.039 | Tree loss: 1.002 | Accuracy: 0.623047 | 7.081 sec/iter\n",
      "Epoch: 90 | Batch: 014 / 030 | Total loss: 1.005 | Reg loss: 0.039 | Tree loss: 1.005 | Accuracy: 0.570312 | 7.081 sec/iter\n",
      "Epoch: 90 | Batch: 015 / 030 | Total loss: 1.014 | Reg loss: 0.040 | Tree loss: 1.014 | Accuracy: 0.546875 | 7.081 sec/iter\n",
      "Epoch: 90 | Batch: 016 / 030 | Total loss: 0.952 | Reg loss: 0.040 | Tree loss: 0.952 | Accuracy: 0.605469 | 7.081 sec/iter\n",
      "Epoch: 90 | Batch: 017 / 030 | Total loss: 0.932 | Reg loss: 0.040 | Tree loss: 0.932 | Accuracy: 0.603516 | 7.081 sec/iter\n",
      "Epoch: 90 | Batch: 018 / 030 | Total loss: 0.927 | Reg loss: 0.040 | Tree loss: 0.927 | Accuracy: 0.597656 | 7.081 sec/iter\n",
      "Epoch: 90 | Batch: 019 / 030 | Total loss: 0.932 | Reg loss: 0.040 | Tree loss: 0.932 | Accuracy: 0.585938 | 7.081 sec/iter\n",
      "Epoch: 90 | Batch: 020 / 030 | Total loss: 0.928 | Reg loss: 0.040 | Tree loss: 0.928 | Accuracy: 0.572266 | 7.081 sec/iter\n",
      "Epoch: 90 | Batch: 021 / 030 | Total loss: 0.917 | Reg loss: 0.040 | Tree loss: 0.917 | Accuracy: 0.562500 | 7.081 sec/iter\n",
      "Epoch: 90 | Batch: 022 / 030 | Total loss: 0.881 | Reg loss: 0.040 | Tree loss: 0.881 | Accuracy: 0.611328 | 7.081 sec/iter\n",
      "Epoch: 90 | Batch: 023 / 030 | Total loss: 0.903 | Reg loss: 0.040 | Tree loss: 0.903 | Accuracy: 0.570312 | 7.081 sec/iter\n",
      "Epoch: 90 | Batch: 024 / 030 | Total loss: 0.878 | Reg loss: 0.040 | Tree loss: 0.878 | Accuracy: 0.613281 | 7.081 sec/iter\n",
      "Epoch: 90 | Batch: 025 / 030 | Total loss: 0.890 | Reg loss: 0.040 | Tree loss: 0.890 | Accuracy: 0.570312 | 7.081 sec/iter\n",
      "Epoch: 90 | Batch: 026 / 030 | Total loss: 0.889 | Reg loss: 0.040 | Tree loss: 0.889 | Accuracy: 0.556641 | 7.08 sec/iter\n",
      "Epoch: 90 | Batch: 027 / 030 | Total loss: 0.881 | Reg loss: 0.040 | Tree loss: 0.881 | Accuracy: 0.580078 | 7.079 sec/iter\n",
      "Epoch: 90 | Batch: 028 / 030 | Total loss: 0.871 | Reg loss: 0.041 | Tree loss: 0.871 | Accuracy: 0.564453 | 7.078 sec/iter\n",
      "Epoch: 90 | Batch: 029 / 030 | Total loss: 0.846 | Reg loss: 0.041 | Tree loss: 0.846 | Accuracy: 0.631068 | 7.076 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 91 | Batch: 000 / 030 | Total loss: 1.232 | Reg loss: 0.039 | Tree loss: 1.232 | Accuracy: 0.617188 | 7.081 sec/iter\n",
      "Epoch: 91 | Batch: 001 / 030 | Total loss: 1.230 | Reg loss: 0.039 | Tree loss: 1.230 | Accuracy: 0.609375 | 7.081 sec/iter\n",
      "Epoch: 91 | Batch: 002 / 030 | Total loss: 1.225 | Reg loss: 0.039 | Tree loss: 1.225 | Accuracy: 0.578125 | 7.081 sec/iter\n",
      "Epoch: 91 | Batch: 003 / 030 | Total loss: 1.229 | Reg loss: 0.039 | Tree loss: 1.229 | Accuracy: 0.564453 | 7.081 sec/iter\n",
      "Epoch: 91 | Batch: 004 / 030 | Total loss: 1.197 | Reg loss: 0.039 | Tree loss: 1.197 | Accuracy: 0.568359 | 7.081 sec/iter\n",
      "Epoch: 91 | Batch: 005 / 030 | Total loss: 1.138 | Reg loss: 0.039 | Tree loss: 1.138 | Accuracy: 0.587891 | 7.081 sec/iter\n",
      "Epoch: 91 | Batch: 006 / 030 | Total loss: 1.123 | Reg loss: 0.039 | Tree loss: 1.123 | Accuracy: 0.597656 | 7.081 sec/iter\n",
      "Epoch: 91 | Batch: 007 / 030 | Total loss: 1.113 | Reg loss: 0.039 | Tree loss: 1.113 | Accuracy: 0.605469 | 7.081 sec/iter\n",
      "Epoch: 91 | Batch: 008 / 030 | Total loss: 1.099 | Reg loss: 0.039 | Tree loss: 1.099 | Accuracy: 0.560547 | 7.081 sec/iter\n",
      "Epoch: 91 | Batch: 009 / 030 | Total loss: 1.056 | Reg loss: 0.039 | Tree loss: 1.056 | Accuracy: 0.611328 | 7.081 sec/iter\n",
      "Epoch: 91 | Batch: 010 / 030 | Total loss: 1.069 | Reg loss: 0.039 | Tree loss: 1.069 | Accuracy: 0.542969 | 7.081 sec/iter\n",
      "Epoch: 91 | Batch: 011 / 030 | Total loss: 1.023 | Reg loss: 0.039 | Tree loss: 1.023 | Accuracy: 0.580078 | 7.08 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 91 | Batch: 012 / 030 | Total loss: 1.026 | Reg loss: 0.039 | Tree loss: 1.026 | Accuracy: 0.580078 | 7.08 sec/iter\n",
      "Epoch: 91 | Batch: 013 / 030 | Total loss: 1.000 | Reg loss: 0.039 | Tree loss: 1.000 | Accuracy: 0.591797 | 7.08 sec/iter\n",
      "Epoch: 91 | Batch: 014 / 030 | Total loss: 0.981 | Reg loss: 0.039 | Tree loss: 0.981 | Accuracy: 0.593750 | 7.08 sec/iter\n",
      "Epoch: 91 | Batch: 015 / 030 | Total loss: 0.983 | Reg loss: 0.039 | Tree loss: 0.983 | Accuracy: 0.576172 | 7.08 sec/iter\n",
      "Epoch: 91 | Batch: 016 / 030 | Total loss: 0.959 | Reg loss: 0.040 | Tree loss: 0.959 | Accuracy: 0.591797 | 7.08 sec/iter\n",
      "Epoch: 91 | Batch: 017 / 030 | Total loss: 0.983 | Reg loss: 0.040 | Tree loss: 0.983 | Accuracy: 0.541016 | 7.08 sec/iter\n",
      "Epoch: 91 | Batch: 018 / 030 | Total loss: 0.920 | Reg loss: 0.040 | Tree loss: 0.920 | Accuracy: 0.630859 | 7.08 sec/iter\n",
      "Epoch: 91 | Batch: 019 / 030 | Total loss: 0.937 | Reg loss: 0.040 | Tree loss: 0.937 | Accuracy: 0.560547 | 7.08 sec/iter\n",
      "Epoch: 91 | Batch: 020 / 030 | Total loss: 0.941 | Reg loss: 0.040 | Tree loss: 0.941 | Accuracy: 0.564453 | 7.08 sec/iter\n",
      "Epoch: 91 | Batch: 021 / 030 | Total loss: 0.915 | Reg loss: 0.040 | Tree loss: 0.915 | Accuracy: 0.576172 | 7.08 sec/iter\n",
      "Epoch: 91 | Batch: 022 / 030 | Total loss: 0.899 | Reg loss: 0.040 | Tree loss: 0.899 | Accuracy: 0.574219 | 7.08 sec/iter\n",
      "Epoch: 91 | Batch: 023 / 030 | Total loss: 0.896 | Reg loss: 0.040 | Tree loss: 0.896 | Accuracy: 0.576172 | 7.08 sec/iter\n",
      "Epoch: 91 | Batch: 024 / 030 | Total loss: 0.896 | Reg loss: 0.040 | Tree loss: 0.896 | Accuracy: 0.578125 | 7.08 sec/iter\n",
      "Epoch: 91 | Batch: 025 / 030 | Total loss: 0.867 | Reg loss: 0.040 | Tree loss: 0.867 | Accuracy: 0.615234 | 7.079 sec/iter\n",
      "Epoch: 91 | Batch: 026 / 030 | Total loss: 0.876 | Reg loss: 0.040 | Tree loss: 0.876 | Accuracy: 0.578125 | 7.079 sec/iter\n",
      "Epoch: 91 | Batch: 027 / 030 | Total loss: 0.864 | Reg loss: 0.040 | Tree loss: 0.864 | Accuracy: 0.576172 | 7.078 sec/iter\n",
      "Epoch: 91 | Batch: 028 / 030 | Total loss: 0.853 | Reg loss: 0.040 | Tree loss: 0.853 | Accuracy: 0.582031 | 7.077 sec/iter\n",
      "Epoch: 91 | Batch: 029 / 030 | Total loss: 0.891 | Reg loss: 0.041 | Tree loss: 0.891 | Accuracy: 0.533981 | 7.076 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 92 | Batch: 000 / 030 | Total loss: 1.215 | Reg loss: 0.039 | Tree loss: 1.215 | Accuracy: 0.603516 | 7.079 sec/iter\n",
      "Epoch: 92 | Batch: 001 / 030 | Total loss: 1.223 | Reg loss: 0.039 | Tree loss: 1.223 | Accuracy: 0.611328 | 7.079 sec/iter\n",
      "Epoch: 92 | Batch: 002 / 030 | Total loss: 1.189 | Reg loss: 0.039 | Tree loss: 1.189 | Accuracy: 0.582031 | 7.079 sec/iter\n",
      "Epoch: 92 | Batch: 003 / 030 | Total loss: 1.196 | Reg loss: 0.039 | Tree loss: 1.196 | Accuracy: 0.568359 | 7.079 sec/iter\n",
      "Epoch: 92 | Batch: 004 / 030 | Total loss: 1.157 | Reg loss: 0.039 | Tree loss: 1.157 | Accuracy: 0.580078 | 7.079 sec/iter\n",
      "Epoch: 92 | Batch: 005 / 030 | Total loss: 1.176 | Reg loss: 0.039 | Tree loss: 1.176 | Accuracy: 0.566406 | 7.079 sec/iter\n",
      "Epoch: 92 | Batch: 006 / 030 | Total loss: 1.138 | Reg loss: 0.039 | Tree loss: 1.138 | Accuracy: 0.564453 | 7.079 sec/iter\n",
      "Epoch: 92 | Batch: 007 / 030 | Total loss: 1.119 | Reg loss: 0.039 | Tree loss: 1.119 | Accuracy: 0.576172 | 7.079 sec/iter\n",
      "Epoch: 92 | Batch: 008 / 030 | Total loss: 1.114 | Reg loss: 0.039 | Tree loss: 1.114 | Accuracy: 0.597656 | 7.079 sec/iter\n",
      "Epoch: 92 | Batch: 009 / 030 | Total loss: 1.088 | Reg loss: 0.039 | Tree loss: 1.088 | Accuracy: 0.548828 | 7.08 sec/iter\n",
      "Epoch: 92 | Batch: 010 / 030 | Total loss: 1.057 | Reg loss: 0.039 | Tree loss: 1.057 | Accuracy: 0.566406 | 7.08 sec/iter\n",
      "Epoch: 92 | Batch: 011 / 030 | Total loss: 1.027 | Reg loss: 0.039 | Tree loss: 1.027 | Accuracy: 0.617188 | 7.08 sec/iter\n",
      "Epoch: 92 | Batch: 012 / 030 | Total loss: 1.028 | Reg loss: 0.039 | Tree loss: 1.028 | Accuracy: 0.582031 | 7.08 sec/iter\n",
      "Epoch: 92 | Batch: 013 / 030 | Total loss: 0.996 | Reg loss: 0.039 | Tree loss: 0.996 | Accuracy: 0.578125 | 7.08 sec/iter\n",
      "Epoch: 92 | Batch: 014 / 030 | Total loss: 0.987 | Reg loss: 0.039 | Tree loss: 0.987 | Accuracy: 0.578125 | 7.08 sec/iter\n",
      "Epoch: 92 | Batch: 015 / 030 | Total loss: 0.964 | Reg loss: 0.039 | Tree loss: 0.964 | Accuracy: 0.587891 | 7.08 sec/iter\n",
      "Epoch: 92 | Batch: 016 / 030 | Total loss: 0.954 | Reg loss: 0.039 | Tree loss: 0.954 | Accuracy: 0.597656 | 7.08 sec/iter\n",
      "Epoch: 92 | Batch: 017 / 030 | Total loss: 0.964 | Reg loss: 0.040 | Tree loss: 0.964 | Accuracy: 0.576172 | 7.08 sec/iter\n",
      "Epoch: 92 | Batch: 018 / 030 | Total loss: 0.951 | Reg loss: 0.040 | Tree loss: 0.951 | Accuracy: 0.568359 | 7.08 sec/iter\n",
      "Epoch: 92 | Batch: 019 / 030 | Total loss: 0.933 | Reg loss: 0.040 | Tree loss: 0.933 | Accuracy: 0.593750 | 7.079 sec/iter\n",
      "Epoch: 92 | Batch: 020 / 030 | Total loss: 0.918 | Reg loss: 0.040 | Tree loss: 0.918 | Accuracy: 0.570312 | 7.079 sec/iter\n",
      "Epoch: 92 | Batch: 021 / 030 | Total loss: 0.909 | Reg loss: 0.040 | Tree loss: 0.909 | Accuracy: 0.568359 | 7.079 sec/iter\n",
      "Epoch: 92 | Batch: 022 / 030 | Total loss: 0.890 | Reg loss: 0.040 | Tree loss: 0.890 | Accuracy: 0.585938 | 7.079 sec/iter\n",
      "Epoch: 92 | Batch: 023 / 030 | Total loss: 0.914 | Reg loss: 0.040 | Tree loss: 0.914 | Accuracy: 0.560547 | 7.078 sec/iter\n",
      "Epoch: 92 | Batch: 024 / 030 | Total loss: 0.865 | Reg loss: 0.040 | Tree loss: 0.865 | Accuracy: 0.605469 | 7.077 sec/iter\n",
      "Epoch: 92 | Batch: 025 / 030 | Total loss: 0.882 | Reg loss: 0.040 | Tree loss: 0.882 | Accuracy: 0.566406 | 7.077 sec/iter\n",
      "Epoch: 92 | Batch: 026 / 030 | Total loss: 0.876 | Reg loss: 0.040 | Tree loss: 0.876 | Accuracy: 0.603516 | 7.076 sec/iter\n",
      "Epoch: 92 | Batch: 027 / 030 | Total loss: 0.857 | Reg loss: 0.040 | Tree loss: 0.857 | Accuracy: 0.605469 | 7.075 sec/iter\n",
      "Epoch: 92 | Batch: 028 / 030 | Total loss: 0.882 | Reg loss: 0.040 | Tree loss: 0.882 | Accuracy: 0.583984 | 7.074 sec/iter\n",
      "Epoch: 92 | Batch: 029 / 030 | Total loss: 0.871 | Reg loss: 0.040 | Tree loss: 0.871 | Accuracy: 0.611650 | 7.073 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 93 | Batch: 000 / 030 | Total loss: 1.261 | Reg loss: 0.039 | Tree loss: 1.261 | Accuracy: 0.613281 | 7.073 sec/iter\n",
      "Epoch: 93 | Batch: 001 / 030 | Total loss: 1.267 | Reg loss: 0.039 | Tree loss: 1.267 | Accuracy: 0.560547 | 7.072 sec/iter\n",
      "Epoch: 93 | Batch: 002 / 030 | Total loss: 1.225 | Reg loss: 0.039 | Tree loss: 1.225 | Accuracy: 0.593750 | 7.072 sec/iter\n",
      "Epoch: 93 | Batch: 003 / 030 | Total loss: 1.198 | Reg loss: 0.039 | Tree loss: 1.198 | Accuracy: 0.613281 | 7.071 sec/iter\n",
      "Epoch: 93 | Batch: 004 / 030 | Total loss: 1.178 | Reg loss: 0.039 | Tree loss: 1.178 | Accuracy: 0.603516 | 7.07 sec/iter\n",
      "Epoch: 93 | Batch: 005 / 030 | Total loss: 1.128 | Reg loss: 0.039 | Tree loss: 1.128 | Accuracy: 0.570312 | 7.07 sec/iter\n",
      "Epoch: 93 | Batch: 006 / 030 | Total loss: 1.110 | Reg loss: 0.039 | Tree loss: 1.110 | Accuracy: 0.595703 | 7.069 sec/iter\n",
      "Epoch: 93 | Batch: 007 / 030 | Total loss: 1.093 | Reg loss: 0.039 | Tree loss: 1.093 | Accuracy: 0.599609 | 7.068 sec/iter\n",
      "Epoch: 93 | Batch: 008 / 030 | Total loss: 1.083 | Reg loss: 0.039 | Tree loss: 1.083 | Accuracy: 0.585938 | 7.067 sec/iter\n",
      "Epoch: 93 | Batch: 009 / 030 | Total loss: 1.042 | Reg loss: 0.039 | Tree loss: 1.042 | Accuracy: 0.595703 | 7.066 sec/iter\n",
      "Epoch: 93 | Batch: 010 / 030 | Total loss: 1.050 | Reg loss: 0.039 | Tree loss: 1.050 | Accuracy: 0.570312 | 7.066 sec/iter\n",
      "Epoch: 93 | Batch: 011 / 030 | Total loss: 1.010 | Reg loss: 0.039 | Tree loss: 1.010 | Accuracy: 0.591797 | 7.065 sec/iter\n",
      "Epoch: 93 | Batch: 012 / 030 | Total loss: 1.008 | Reg loss: 0.039 | Tree loss: 1.008 | Accuracy: 0.572266 | 7.064 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 93 | Batch: 013 / 030 | Total loss: 0.997 | Reg loss: 0.039 | Tree loss: 0.997 | Accuracy: 0.583984 | 7.063 sec/iter\n",
      "Epoch: 93 | Batch: 014 / 030 | Total loss: 0.971 | Reg loss: 0.039 | Tree loss: 0.971 | Accuracy: 0.580078 | 7.062 sec/iter\n",
      "Epoch: 93 | Batch: 015 / 030 | Total loss: 0.986 | Reg loss: 0.039 | Tree loss: 0.986 | Accuracy: 0.546875 | 7.061 sec/iter\n",
      "Epoch: 93 | Batch: 016 / 030 | Total loss: 0.964 | Reg loss: 0.039 | Tree loss: 0.964 | Accuracy: 0.580078 | 7.061 sec/iter\n",
      "Epoch: 93 | Batch: 017 / 030 | Total loss: 0.928 | Reg loss: 0.039 | Tree loss: 0.928 | Accuracy: 0.591797 | 7.06 sec/iter\n",
      "Epoch: 93 | Batch: 018 / 030 | Total loss: 0.927 | Reg loss: 0.040 | Tree loss: 0.927 | Accuracy: 0.593750 | 7.059 sec/iter\n",
      "Epoch: 93 | Batch: 019 / 030 | Total loss: 0.925 | Reg loss: 0.040 | Tree loss: 0.925 | Accuracy: 0.582031 | 7.058 sec/iter\n",
      "Epoch: 93 | Batch: 020 / 030 | Total loss: 0.911 | Reg loss: 0.040 | Tree loss: 0.911 | Accuracy: 0.589844 | 7.057 sec/iter\n",
      "Epoch: 93 | Batch: 021 / 030 | Total loss: 0.926 | Reg loss: 0.040 | Tree loss: 0.926 | Accuracy: 0.544922 | 7.056 sec/iter\n",
      "Epoch: 93 | Batch: 022 / 030 | Total loss: 0.924 | Reg loss: 0.040 | Tree loss: 0.924 | Accuracy: 0.556641 | 7.056 sec/iter\n",
      "Epoch: 93 | Batch: 023 / 030 | Total loss: 0.900 | Reg loss: 0.040 | Tree loss: 0.900 | Accuracy: 0.580078 | 7.055 sec/iter\n",
      "Epoch: 93 | Batch: 024 / 030 | Total loss: 0.885 | Reg loss: 0.040 | Tree loss: 0.885 | Accuracy: 0.578125 | 7.054 sec/iter\n",
      "Epoch: 93 | Batch: 025 / 030 | Total loss: 0.878 | Reg loss: 0.040 | Tree loss: 0.878 | Accuracy: 0.585938 | 7.053 sec/iter\n",
      "Epoch: 93 | Batch: 026 / 030 | Total loss: 0.865 | Reg loss: 0.040 | Tree loss: 0.865 | Accuracy: 0.611328 | 7.052 sec/iter\n",
      "Epoch: 93 | Batch: 027 / 030 | Total loss: 0.882 | Reg loss: 0.040 | Tree loss: 0.882 | Accuracy: 0.568359 | 7.052 sec/iter\n",
      "Epoch: 93 | Batch: 028 / 030 | Total loss: 0.871 | Reg loss: 0.040 | Tree loss: 0.871 | Accuracy: 0.562500 | 7.051 sec/iter\n",
      "Epoch: 93 | Batch: 029 / 030 | Total loss: 0.856 | Reg loss: 0.040 | Tree loss: 0.856 | Accuracy: 0.572816 | 7.049 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 94 | Batch: 000 / 030 | Total loss: 1.277 | Reg loss: 0.038 | Tree loss: 1.277 | Accuracy: 0.601562 | 7.05 sec/iter\n",
      "Epoch: 94 | Batch: 001 / 030 | Total loss: 1.174 | Reg loss: 0.038 | Tree loss: 1.174 | Accuracy: 0.609375 | 7.049 sec/iter\n",
      "Epoch: 94 | Batch: 002 / 030 | Total loss: 1.237 | Reg loss: 0.038 | Tree loss: 1.237 | Accuracy: 0.580078 | 7.048 sec/iter\n",
      "Epoch: 94 | Batch: 003 / 030 | Total loss: 1.195 | Reg loss: 0.038 | Tree loss: 1.195 | Accuracy: 0.558594 | 7.048 sec/iter\n",
      "Epoch: 94 | Batch: 004 / 030 | Total loss: 1.172 | Reg loss: 0.038 | Tree loss: 1.172 | Accuracy: 0.591797 | 7.047 sec/iter\n",
      "Epoch: 94 | Batch: 005 / 030 | Total loss: 1.137 | Reg loss: 0.039 | Tree loss: 1.137 | Accuracy: 0.587891 | 7.046 sec/iter\n",
      "Epoch: 94 | Batch: 006 / 030 | Total loss: 1.127 | Reg loss: 0.039 | Tree loss: 1.127 | Accuracy: 0.626953 | 7.046 sec/iter\n",
      "Epoch: 94 | Batch: 007 / 030 | Total loss: 1.124 | Reg loss: 0.039 | Tree loss: 1.124 | Accuracy: 0.519531 | 7.045 sec/iter\n",
      "Epoch: 94 | Batch: 008 / 030 | Total loss: 1.072 | Reg loss: 0.039 | Tree loss: 1.072 | Accuracy: 0.632812 | 7.044 sec/iter\n",
      "Epoch: 94 | Batch: 009 / 030 | Total loss: 1.047 | Reg loss: 0.039 | Tree loss: 1.047 | Accuracy: 0.607422 | 7.044 sec/iter\n",
      "Epoch: 94 | Batch: 010 / 030 | Total loss: 1.060 | Reg loss: 0.039 | Tree loss: 1.060 | Accuracy: 0.582031 | 7.043 sec/iter\n",
      "Epoch: 94 | Batch: 011 / 030 | Total loss: 1.017 | Reg loss: 0.039 | Tree loss: 1.017 | Accuracy: 0.583984 | 7.042 sec/iter\n",
      "Epoch: 94 | Batch: 012 / 030 | Total loss: 1.022 | Reg loss: 0.039 | Tree loss: 1.022 | Accuracy: 0.558594 | 7.041 sec/iter\n",
      "Epoch: 94 | Batch: 013 / 030 | Total loss: 0.974 | Reg loss: 0.039 | Tree loss: 0.974 | Accuracy: 0.605469 | 7.04 sec/iter\n",
      "Epoch: 94 | Batch: 014 / 030 | Total loss: 0.990 | Reg loss: 0.039 | Tree loss: 0.990 | Accuracy: 0.570312 | 7.04 sec/iter\n",
      "Epoch: 94 | Batch: 015 / 030 | Total loss: 0.983 | Reg loss: 0.039 | Tree loss: 0.983 | Accuracy: 0.605469 | 7.039 sec/iter\n",
      "Epoch: 94 | Batch: 016 / 030 | Total loss: 0.986 | Reg loss: 0.039 | Tree loss: 0.986 | Accuracy: 0.550781 | 7.038 sec/iter\n",
      "Epoch: 94 | Batch: 017 / 030 | Total loss: 0.953 | Reg loss: 0.039 | Tree loss: 0.953 | Accuracy: 0.560547 | 7.037 sec/iter\n",
      "Epoch: 94 | Batch: 018 / 030 | Total loss: 0.934 | Reg loss: 0.039 | Tree loss: 0.934 | Accuracy: 0.580078 | 7.036 sec/iter\n",
      "Epoch: 94 | Batch: 019 / 030 | Total loss: 0.949 | Reg loss: 0.040 | Tree loss: 0.949 | Accuracy: 0.564453 | 7.035 sec/iter\n",
      "Epoch: 94 | Batch: 020 / 030 | Total loss: 0.924 | Reg loss: 0.040 | Tree loss: 0.924 | Accuracy: 0.568359 | 7.035 sec/iter\n",
      "Epoch: 94 | Batch: 021 / 030 | Total loss: 0.927 | Reg loss: 0.040 | Tree loss: 0.927 | Accuracy: 0.552734 | 7.034 sec/iter\n",
      "Epoch: 94 | Batch: 022 / 030 | Total loss: 0.908 | Reg loss: 0.040 | Tree loss: 0.908 | Accuracy: 0.580078 | 7.033 sec/iter\n",
      "Epoch: 94 | Batch: 023 / 030 | Total loss: 0.872 | Reg loss: 0.040 | Tree loss: 0.872 | Accuracy: 0.605469 | 7.032 sec/iter\n",
      "Epoch: 94 | Batch: 024 / 030 | Total loss: 0.874 | Reg loss: 0.040 | Tree loss: 0.874 | Accuracy: 0.578125 | 7.031 sec/iter\n",
      "Epoch: 94 | Batch: 025 / 030 | Total loss: 0.876 | Reg loss: 0.040 | Tree loss: 0.876 | Accuracy: 0.589844 | 7.031 sec/iter\n",
      "Epoch: 94 | Batch: 026 / 030 | Total loss: 0.857 | Reg loss: 0.040 | Tree loss: 0.857 | Accuracy: 0.607422 | 7.03 sec/iter\n",
      "Epoch: 94 | Batch: 027 / 030 | Total loss: 0.868 | Reg loss: 0.040 | Tree loss: 0.868 | Accuracy: 0.574219 | 7.029 sec/iter\n",
      "Epoch: 94 | Batch: 028 / 030 | Total loss: 0.863 | Reg loss: 0.040 | Tree loss: 0.863 | Accuracy: 0.566406 | 7.029 sec/iter\n",
      "Epoch: 94 | Batch: 029 / 030 | Total loss: 0.838 | Reg loss: 0.040 | Tree loss: 0.838 | Accuracy: 0.582524 | 7.027 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 95 | Batch: 000 / 030 | Total loss: 1.238 | Reg loss: 0.038 | Tree loss: 1.238 | Accuracy: 0.564453 | 7.027 sec/iter\n",
      "Epoch: 95 | Batch: 001 / 030 | Total loss: 1.280 | Reg loss: 0.038 | Tree loss: 1.280 | Accuracy: 0.574219 | 7.027 sec/iter\n",
      "Epoch: 95 | Batch: 002 / 030 | Total loss: 1.204 | Reg loss: 0.038 | Tree loss: 1.204 | Accuracy: 0.599609 | 7.026 sec/iter\n",
      "Epoch: 95 | Batch: 003 / 030 | Total loss: 1.224 | Reg loss: 0.038 | Tree loss: 1.224 | Accuracy: 0.583984 | 7.026 sec/iter\n",
      "Epoch: 95 | Batch: 004 / 030 | Total loss: 1.155 | Reg loss: 0.038 | Tree loss: 1.155 | Accuracy: 0.611328 | 7.025 sec/iter\n",
      "Epoch: 95 | Batch: 005 / 030 | Total loss: 1.141 | Reg loss: 0.038 | Tree loss: 1.141 | Accuracy: 0.617188 | 7.024 sec/iter\n",
      "Epoch: 95 | Batch: 006 / 030 | Total loss: 1.126 | Reg loss: 0.038 | Tree loss: 1.126 | Accuracy: 0.589844 | 7.024 sec/iter\n",
      "Epoch: 95 | Batch: 007 / 030 | Total loss: 1.116 | Reg loss: 0.039 | Tree loss: 1.116 | Accuracy: 0.572266 | 7.023 sec/iter\n",
      "Epoch: 95 | Batch: 008 / 030 | Total loss: 1.090 | Reg loss: 0.039 | Tree loss: 1.090 | Accuracy: 0.591797 | 7.022 sec/iter\n",
      "Epoch: 95 | Batch: 009 / 030 | Total loss: 1.058 | Reg loss: 0.039 | Tree loss: 1.058 | Accuracy: 0.587891 | 7.022 sec/iter\n",
      "Epoch: 95 | Batch: 010 / 030 | Total loss: 1.055 | Reg loss: 0.039 | Tree loss: 1.055 | Accuracy: 0.572266 | 7.021 sec/iter\n",
      "Epoch: 95 | Batch: 011 / 030 | Total loss: 1.054 | Reg loss: 0.039 | Tree loss: 1.054 | Accuracy: 0.572266 | 7.02 sec/iter\n",
      "Epoch: 95 | Batch: 012 / 030 | Total loss: 1.004 | Reg loss: 0.039 | Tree loss: 1.004 | Accuracy: 0.607422 | 7.019 sec/iter\n",
      "Epoch: 95 | Batch: 013 / 030 | Total loss: 1.001 | Reg loss: 0.039 | Tree loss: 1.001 | Accuracy: 0.597656 | 7.019 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 95 | Batch: 014 / 030 | Total loss: 0.965 | Reg loss: 0.039 | Tree loss: 0.965 | Accuracy: 0.630859 | 7.018 sec/iter\n",
      "Epoch: 95 | Batch: 015 / 030 | Total loss: 0.962 | Reg loss: 0.039 | Tree loss: 0.962 | Accuracy: 0.574219 | 7.017 sec/iter\n",
      "Epoch: 95 | Batch: 016 / 030 | Total loss: 0.932 | Reg loss: 0.039 | Tree loss: 0.932 | Accuracy: 0.601562 | 7.016 sec/iter\n",
      "Epoch: 95 | Batch: 017 / 030 | Total loss: 0.933 | Reg loss: 0.039 | Tree loss: 0.933 | Accuracy: 0.583984 | 7.015 sec/iter\n",
      "Epoch: 95 | Batch: 018 / 030 | Total loss: 0.935 | Reg loss: 0.039 | Tree loss: 0.935 | Accuracy: 0.599609 | 7.015 sec/iter\n",
      "Epoch: 95 | Batch: 019 / 030 | Total loss: 0.950 | Reg loss: 0.039 | Tree loss: 0.950 | Accuracy: 0.509766 | 7.014 sec/iter\n",
      "Epoch: 95 | Batch: 020 / 030 | Total loss: 0.908 | Reg loss: 0.040 | Tree loss: 0.908 | Accuracy: 0.585938 | 7.013 sec/iter\n",
      "Epoch: 95 | Batch: 021 / 030 | Total loss: 0.885 | Reg loss: 0.040 | Tree loss: 0.885 | Accuracy: 0.607422 | 7.012 sec/iter\n",
      "Epoch: 95 | Batch: 022 / 030 | Total loss: 0.921 | Reg loss: 0.040 | Tree loss: 0.921 | Accuracy: 0.541016 | 7.011 sec/iter\n",
      "Epoch: 95 | Batch: 023 / 030 | Total loss: 0.881 | Reg loss: 0.040 | Tree loss: 0.881 | Accuracy: 0.576172 | 7.011 sec/iter\n",
      "Epoch: 95 | Batch: 024 / 030 | Total loss: 0.892 | Reg loss: 0.040 | Tree loss: 0.892 | Accuracy: 0.562500 | 7.01 sec/iter\n",
      "Epoch: 95 | Batch: 025 / 030 | Total loss: 0.871 | Reg loss: 0.040 | Tree loss: 0.871 | Accuracy: 0.578125 | 7.009 sec/iter\n",
      "Epoch: 95 | Batch: 026 / 030 | Total loss: 0.870 | Reg loss: 0.040 | Tree loss: 0.870 | Accuracy: 0.570312 | 7.009 sec/iter\n",
      "Epoch: 95 | Batch: 027 / 030 | Total loss: 0.876 | Reg loss: 0.040 | Tree loss: 0.876 | Accuracy: 0.542969 | 7.008 sec/iter\n",
      "Epoch: 95 | Batch: 028 / 030 | Total loss: 0.857 | Reg loss: 0.040 | Tree loss: 0.857 | Accuracy: 0.589844 | 7.007 sec/iter\n",
      "Epoch: 95 | Batch: 029 / 030 | Total loss: 0.846 | Reg loss: 0.040 | Tree loss: 0.846 | Accuracy: 0.601942 | 7.006 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 96 | Batch: 000 / 030 | Total loss: 1.255 | Reg loss: 0.038 | Tree loss: 1.255 | Accuracy: 0.578125 | 7.006 sec/iter\n",
      "Epoch: 96 | Batch: 001 / 030 | Total loss: 1.230 | Reg loss: 0.038 | Tree loss: 1.230 | Accuracy: 0.564453 | 7.006 sec/iter\n",
      "Epoch: 96 | Batch: 002 / 030 | Total loss: 1.200 | Reg loss: 0.038 | Tree loss: 1.200 | Accuracy: 0.595703 | 7.005 sec/iter\n",
      "Epoch: 96 | Batch: 003 / 030 | Total loss: 1.155 | Reg loss: 0.038 | Tree loss: 1.155 | Accuracy: 0.591797 | 7.004 sec/iter\n",
      "Epoch: 96 | Batch: 004 / 030 | Total loss: 1.181 | Reg loss: 0.038 | Tree loss: 1.181 | Accuracy: 0.560547 | 7.004 sec/iter\n",
      "Epoch: 96 | Batch: 005 / 030 | Total loss: 1.157 | Reg loss: 0.038 | Tree loss: 1.157 | Accuracy: 0.576172 | 7.003 sec/iter\n",
      "Epoch: 96 | Batch: 006 / 030 | Total loss: 1.110 | Reg loss: 0.038 | Tree loss: 1.110 | Accuracy: 0.587891 | 7.003 sec/iter\n",
      "Epoch: 96 | Batch: 007 / 030 | Total loss: 1.104 | Reg loss: 0.038 | Tree loss: 1.104 | Accuracy: 0.605469 | 7.002 sec/iter\n",
      "Epoch: 96 | Batch: 008 / 030 | Total loss: 1.086 | Reg loss: 0.038 | Tree loss: 1.086 | Accuracy: 0.580078 | 7.001 sec/iter\n",
      "Epoch: 96 | Batch: 009 / 030 | Total loss: 1.088 | Reg loss: 0.039 | Tree loss: 1.088 | Accuracy: 0.572266 | 7.001 sec/iter\n",
      "Epoch: 96 | Batch: 010 / 030 | Total loss: 1.072 | Reg loss: 0.039 | Tree loss: 1.072 | Accuracy: 0.564453 | 7.0 sec/iter\n",
      "Epoch: 96 | Batch: 011 / 030 | Total loss: 1.036 | Reg loss: 0.039 | Tree loss: 1.036 | Accuracy: 0.599609 | 6.999 sec/iter\n",
      "Epoch: 96 | Batch: 012 / 030 | Total loss: 1.025 | Reg loss: 0.039 | Tree loss: 1.025 | Accuracy: 0.611328 | 6.999 sec/iter\n",
      "Epoch: 96 | Batch: 013 / 030 | Total loss: 0.977 | Reg loss: 0.039 | Tree loss: 0.977 | Accuracy: 0.609375 | 6.998 sec/iter\n",
      "Epoch: 96 | Batch: 014 / 030 | Total loss: 0.947 | Reg loss: 0.039 | Tree loss: 0.947 | Accuracy: 0.634766 | 6.997 sec/iter\n",
      "Epoch: 96 | Batch: 015 / 030 | Total loss: 0.996 | Reg loss: 0.039 | Tree loss: 0.996 | Accuracy: 0.580078 | 6.996 sec/iter\n",
      "Epoch: 96 | Batch: 016 / 030 | Total loss: 0.941 | Reg loss: 0.039 | Tree loss: 0.941 | Accuracy: 0.572266 | 6.995 sec/iter\n",
      "Epoch: 96 | Batch: 017 / 030 | Total loss: 0.946 | Reg loss: 0.039 | Tree loss: 0.946 | Accuracy: 0.578125 | 6.995 sec/iter\n",
      "Epoch: 96 | Batch: 018 / 030 | Total loss: 0.949 | Reg loss: 0.039 | Tree loss: 0.949 | Accuracy: 0.568359 | 6.994 sec/iter\n",
      "Epoch: 96 | Batch: 019 / 030 | Total loss: 0.898 | Reg loss: 0.039 | Tree loss: 0.898 | Accuracy: 0.613281 | 6.993 sec/iter\n",
      "Epoch: 96 | Batch: 020 / 030 | Total loss: 0.921 | Reg loss: 0.039 | Tree loss: 0.921 | Accuracy: 0.572266 | 6.992 sec/iter\n",
      "Epoch: 96 | Batch: 021 / 030 | Total loss: 0.893 | Reg loss: 0.040 | Tree loss: 0.893 | Accuracy: 0.583984 | 6.992 sec/iter\n",
      "Epoch: 96 | Batch: 022 / 030 | Total loss: 0.901 | Reg loss: 0.040 | Tree loss: 0.901 | Accuracy: 0.585938 | 6.991 sec/iter\n",
      "Epoch: 96 | Batch: 023 / 030 | Total loss: 0.899 | Reg loss: 0.040 | Tree loss: 0.899 | Accuracy: 0.554688 | 6.99 sec/iter\n",
      "Epoch: 96 | Batch: 024 / 030 | Total loss: 0.888 | Reg loss: 0.040 | Tree loss: 0.888 | Accuracy: 0.552734 | 6.988 sec/iter\n",
      "Epoch: 96 | Batch: 025 / 030 | Total loss: 0.887 | Reg loss: 0.040 | Tree loss: 0.887 | Accuracy: 0.576172 | 6.987 sec/iter\n",
      "Epoch: 96 | Batch: 026 / 030 | Total loss: 0.866 | Reg loss: 0.040 | Tree loss: 0.866 | Accuracy: 0.566406 | 6.986 sec/iter\n",
      "Epoch: 96 | Batch: 027 / 030 | Total loss: 0.851 | Reg loss: 0.040 | Tree loss: 0.851 | Accuracy: 0.593750 | 6.985 sec/iter\n",
      "Epoch: 96 | Batch: 028 / 030 | Total loss: 0.863 | Reg loss: 0.040 | Tree loss: 0.863 | Accuracy: 0.578125 | 6.984 sec/iter\n",
      "Epoch: 96 | Batch: 029 / 030 | Total loss: 0.871 | Reg loss: 0.040 | Tree loss: 0.871 | Accuracy: 0.543689 | 6.983 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 97 | Batch: 000 / 030 | Total loss: 1.272 | Reg loss: 0.038 | Tree loss: 1.272 | Accuracy: 0.566406 | 6.982 sec/iter\n",
      "Epoch: 97 | Batch: 001 / 030 | Total loss: 1.215 | Reg loss: 0.038 | Tree loss: 1.215 | Accuracy: 0.593750 | 6.98 sec/iter\n",
      "Epoch: 97 | Batch: 002 / 030 | Total loss: 1.176 | Reg loss: 0.038 | Tree loss: 1.176 | Accuracy: 0.587891 | 6.979 sec/iter\n",
      "Epoch: 97 | Batch: 003 / 030 | Total loss: 1.217 | Reg loss: 0.038 | Tree loss: 1.217 | Accuracy: 0.591797 | 6.978 sec/iter\n",
      "Epoch: 97 | Batch: 004 / 030 | Total loss: 1.199 | Reg loss: 0.038 | Tree loss: 1.199 | Accuracy: 0.544922 | 6.977 sec/iter\n",
      "Epoch: 97 | Batch: 005 / 030 | Total loss: 1.132 | Reg loss: 0.038 | Tree loss: 1.132 | Accuracy: 0.576172 | 6.976 sec/iter\n",
      "Epoch: 97 | Batch: 006 / 030 | Total loss: 1.108 | Reg loss: 0.038 | Tree loss: 1.108 | Accuracy: 0.583984 | 6.975 sec/iter\n",
      "Epoch: 97 | Batch: 007 / 030 | Total loss: 1.119 | Reg loss: 0.038 | Tree loss: 1.119 | Accuracy: 0.601562 | 6.973 sec/iter\n",
      "Epoch: 97 | Batch: 008 / 030 | Total loss: 1.107 | Reg loss: 0.038 | Tree loss: 1.107 | Accuracy: 0.576172 | 6.972 sec/iter\n",
      "Epoch: 97 | Batch: 009 / 030 | Total loss: 1.054 | Reg loss: 0.038 | Tree loss: 1.054 | Accuracy: 0.583984 | 6.971 sec/iter\n",
      "Epoch: 97 | Batch: 010 / 030 | Total loss: 1.032 | Reg loss: 0.039 | Tree loss: 1.032 | Accuracy: 0.607422 | 6.97 sec/iter\n",
      "Epoch: 97 | Batch: 011 / 030 | Total loss: 1.044 | Reg loss: 0.039 | Tree loss: 1.044 | Accuracy: 0.564453 | 6.969 sec/iter\n",
      "Epoch: 97 | Batch: 012 / 030 | Total loss: 1.005 | Reg loss: 0.039 | Tree loss: 1.005 | Accuracy: 0.603516 | 6.968 sec/iter\n",
      "Epoch: 97 | Batch: 013 / 030 | Total loss: 0.980 | Reg loss: 0.039 | Tree loss: 0.980 | Accuracy: 0.568359 | 6.967 sec/iter\n",
      "Epoch: 97 | Batch: 014 / 030 | Total loss: 1.007 | Reg loss: 0.039 | Tree loss: 1.007 | Accuracy: 0.556641 | 6.965 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 97 | Batch: 015 / 030 | Total loss: 0.979 | Reg loss: 0.039 | Tree loss: 0.979 | Accuracy: 0.582031 | 6.964 sec/iter\n",
      "Epoch: 97 | Batch: 016 / 030 | Total loss: 0.941 | Reg loss: 0.039 | Tree loss: 0.941 | Accuracy: 0.587891 | 6.963 sec/iter\n",
      "Epoch: 97 | Batch: 017 / 030 | Total loss: 0.961 | Reg loss: 0.039 | Tree loss: 0.961 | Accuracy: 0.560547 | 6.962 sec/iter\n",
      "Epoch: 97 | Batch: 018 / 030 | Total loss: 0.907 | Reg loss: 0.039 | Tree loss: 0.907 | Accuracy: 0.628906 | 6.961 sec/iter\n",
      "Epoch: 97 | Batch: 019 / 030 | Total loss: 0.923 | Reg loss: 0.039 | Tree loss: 0.923 | Accuracy: 0.580078 | 6.96 sec/iter\n",
      "Epoch: 97 | Batch: 020 / 030 | Total loss: 0.918 | Reg loss: 0.039 | Tree loss: 0.918 | Accuracy: 0.546875 | 6.959 sec/iter\n",
      "Epoch: 97 | Batch: 021 / 030 | Total loss: 0.893 | Reg loss: 0.040 | Tree loss: 0.893 | Accuracy: 0.615234 | 6.958 sec/iter\n",
      "Epoch: 97 | Batch: 022 / 030 | Total loss: 0.882 | Reg loss: 0.040 | Tree loss: 0.882 | Accuracy: 0.591797 | 6.956 sec/iter\n",
      "Epoch: 97 | Batch: 023 / 030 | Total loss: 0.895 | Reg loss: 0.040 | Tree loss: 0.895 | Accuracy: 0.570312 | 6.955 sec/iter\n",
      "Epoch: 97 | Batch: 024 / 030 | Total loss: 0.868 | Reg loss: 0.040 | Tree loss: 0.868 | Accuracy: 0.587891 | 6.954 sec/iter\n",
      "Epoch: 97 | Batch: 025 / 030 | Total loss: 0.892 | Reg loss: 0.040 | Tree loss: 0.892 | Accuracy: 0.570312 | 6.953 sec/iter\n",
      "Epoch: 97 | Batch: 026 / 030 | Total loss: 0.850 | Reg loss: 0.040 | Tree loss: 0.850 | Accuracy: 0.593750 | 6.952 sec/iter\n",
      "Epoch: 97 | Batch: 027 / 030 | Total loss: 0.851 | Reg loss: 0.040 | Tree loss: 0.851 | Accuracy: 0.591797 | 6.951 sec/iter\n",
      "Epoch: 97 | Batch: 028 / 030 | Total loss: 0.853 | Reg loss: 0.040 | Tree loss: 0.853 | Accuracy: 0.587891 | 6.949 sec/iter\n",
      "Epoch: 97 | Batch: 029 / 030 | Total loss: 0.865 | Reg loss: 0.040 | Tree loss: 0.865 | Accuracy: 0.572816 | 6.948 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 98 | Batch: 000 / 030 | Total loss: 1.254 | Reg loss: 0.038 | Tree loss: 1.254 | Accuracy: 0.583984 | 6.947 sec/iter\n",
      "Epoch: 98 | Batch: 001 / 030 | Total loss: 1.262 | Reg loss: 0.038 | Tree loss: 1.262 | Accuracy: 0.564453 | 6.946 sec/iter\n",
      "Epoch: 98 | Batch: 002 / 030 | Total loss: 1.201 | Reg loss: 0.038 | Tree loss: 1.201 | Accuracy: 0.580078 | 6.945 sec/iter\n",
      "Epoch: 98 | Batch: 003 / 030 | Total loss: 1.209 | Reg loss: 0.038 | Tree loss: 1.209 | Accuracy: 0.583984 | 6.943 sec/iter\n",
      "Epoch: 98 | Batch: 004 / 030 | Total loss: 1.158 | Reg loss: 0.038 | Tree loss: 1.158 | Accuracy: 0.619141 | 6.942 sec/iter\n",
      "Epoch: 98 | Batch: 005 / 030 | Total loss: 1.139 | Reg loss: 0.038 | Tree loss: 1.139 | Accuracy: 0.585938 | 6.941 sec/iter\n",
      "Epoch: 98 | Batch: 006 / 030 | Total loss: 1.122 | Reg loss: 0.038 | Tree loss: 1.122 | Accuracy: 0.562500 | 6.94 sec/iter\n",
      "Epoch: 98 | Batch: 007 / 030 | Total loss: 1.095 | Reg loss: 0.038 | Tree loss: 1.095 | Accuracy: 0.578125 | 6.939 sec/iter\n",
      "Epoch: 98 | Batch: 008 / 030 | Total loss: 1.077 | Reg loss: 0.038 | Tree loss: 1.077 | Accuracy: 0.595703 | 6.938 sec/iter\n",
      "Epoch: 98 | Batch: 009 / 030 | Total loss: 1.059 | Reg loss: 0.038 | Tree loss: 1.059 | Accuracy: 0.603516 | 6.936 sec/iter\n",
      "Epoch: 98 | Batch: 010 / 030 | Total loss: 1.033 | Reg loss: 0.038 | Tree loss: 1.033 | Accuracy: 0.585938 | 6.935 sec/iter\n",
      "Epoch: 98 | Batch: 011 / 030 | Total loss: 1.018 | Reg loss: 0.039 | Tree loss: 1.018 | Accuracy: 0.609375 | 6.934 sec/iter\n",
      "Epoch: 98 | Batch: 012 / 030 | Total loss: 1.004 | Reg loss: 0.039 | Tree loss: 1.004 | Accuracy: 0.574219 | 6.933 sec/iter\n",
      "Epoch: 98 | Batch: 013 / 030 | Total loss: 0.973 | Reg loss: 0.039 | Tree loss: 0.973 | Accuracy: 0.607422 | 6.932 sec/iter\n",
      "Epoch: 98 | Batch: 014 / 030 | Total loss: 0.993 | Reg loss: 0.039 | Tree loss: 0.993 | Accuracy: 0.582031 | 6.931 sec/iter\n",
      "Epoch: 98 | Batch: 015 / 030 | Total loss: 0.971 | Reg loss: 0.039 | Tree loss: 0.971 | Accuracy: 0.550781 | 6.93 sec/iter\n",
      "Epoch: 98 | Batch: 016 / 030 | Total loss: 0.944 | Reg loss: 0.039 | Tree loss: 0.944 | Accuracy: 0.601562 | 6.928 sec/iter\n",
      "Epoch: 98 | Batch: 017 / 030 | Total loss: 0.955 | Reg loss: 0.039 | Tree loss: 0.955 | Accuracy: 0.544922 | 6.927 sec/iter\n",
      "Epoch: 98 | Batch: 018 / 030 | Total loss: 0.953 | Reg loss: 0.039 | Tree loss: 0.953 | Accuracy: 0.544922 | 6.926 sec/iter\n",
      "Epoch: 98 | Batch: 019 / 030 | Total loss: 0.920 | Reg loss: 0.039 | Tree loss: 0.920 | Accuracy: 0.580078 | 6.925 sec/iter\n",
      "Epoch: 98 | Batch: 020 / 030 | Total loss: 0.909 | Reg loss: 0.039 | Tree loss: 0.909 | Accuracy: 0.595703 | 6.924 sec/iter\n",
      "Epoch: 98 | Batch: 021 / 030 | Total loss: 0.896 | Reg loss: 0.039 | Tree loss: 0.896 | Accuracy: 0.572266 | 6.923 sec/iter\n",
      "Epoch: 98 | Batch: 022 / 030 | Total loss: 0.885 | Reg loss: 0.039 | Tree loss: 0.885 | Accuracy: 0.580078 | 6.922 sec/iter\n",
      "Epoch: 98 | Batch: 023 / 030 | Total loss: 0.875 | Reg loss: 0.040 | Tree loss: 0.875 | Accuracy: 0.605469 | 6.92 sec/iter\n",
      "Epoch: 98 | Batch: 024 / 030 | Total loss: 0.873 | Reg loss: 0.040 | Tree loss: 0.873 | Accuracy: 0.587891 | 6.919 sec/iter\n",
      "Epoch: 98 | Batch: 025 / 030 | Total loss: 0.886 | Reg loss: 0.040 | Tree loss: 0.886 | Accuracy: 0.566406 | 6.918 sec/iter\n",
      "Epoch: 98 | Batch: 026 / 030 | Total loss: 0.872 | Reg loss: 0.040 | Tree loss: 0.872 | Accuracy: 0.568359 | 6.917 sec/iter\n",
      "Epoch: 98 | Batch: 027 / 030 | Total loss: 0.856 | Reg loss: 0.040 | Tree loss: 0.856 | Accuracy: 0.597656 | 6.916 sec/iter\n",
      "Epoch: 98 | Batch: 028 / 030 | Total loss: 0.852 | Reg loss: 0.040 | Tree loss: 0.852 | Accuracy: 0.585938 | 6.915 sec/iter\n",
      "Epoch: 98 | Batch: 029 / 030 | Total loss: 0.845 | Reg loss: 0.040 | Tree loss: 0.845 | Accuracy: 0.592233 | 6.913 sec/iter\n",
      "Average sparseness: 0.9821428571428572\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "layer 9: 0.9821428571428573\n",
      "layer 10: 0.9821428571428573\n",
      "Epoch: 99 | Batch: 000 / 030 | Total loss: 1.244 | Reg loss: 0.038 | Tree loss: 1.244 | Accuracy: 0.570312 | 6.912 sec/iter\n",
      "Epoch: 99 | Batch: 001 / 030 | Total loss: 1.235 | Reg loss: 0.038 | Tree loss: 1.235 | Accuracy: 0.572266 | 6.911 sec/iter\n",
      "Epoch: 99 | Batch: 002 / 030 | Total loss: 1.197 | Reg loss: 0.038 | Tree loss: 1.197 | Accuracy: 0.605469 | 6.91 sec/iter\n",
      "Epoch: 99 | Batch: 003 / 030 | Total loss: 1.211 | Reg loss: 0.038 | Tree loss: 1.211 | Accuracy: 0.566406 | 6.909 sec/iter\n",
      "Epoch: 99 | Batch: 004 / 030 | Total loss: 1.190 | Reg loss: 0.038 | Tree loss: 1.190 | Accuracy: 0.578125 | 6.908 sec/iter\n",
      "Epoch: 99 | Batch: 005 / 030 | Total loss: 1.165 | Reg loss: 0.038 | Tree loss: 1.165 | Accuracy: 0.552734 | 6.907 sec/iter\n",
      "Epoch: 99 | Batch: 006 / 030 | Total loss: 1.106 | Reg loss: 0.038 | Tree loss: 1.106 | Accuracy: 0.619141 | 6.905 sec/iter\n",
      "Epoch: 99 | Batch: 007 / 030 | Total loss: 1.114 | Reg loss: 0.038 | Tree loss: 1.114 | Accuracy: 0.578125 | 6.904 sec/iter\n",
      "Epoch: 99 | Batch: 008 / 030 | Total loss: 1.090 | Reg loss: 0.038 | Tree loss: 1.090 | Accuracy: 0.607422 | 6.903 sec/iter\n",
      "Epoch: 99 | Batch: 009 / 030 | Total loss: 1.067 | Reg loss: 0.038 | Tree loss: 1.067 | Accuracy: 0.562500 | 6.902 sec/iter\n",
      "Epoch: 99 | Batch: 010 / 030 | Total loss: 1.026 | Reg loss: 0.038 | Tree loss: 1.026 | Accuracy: 0.589844 | 6.901 sec/iter\n",
      "Epoch: 99 | Batch: 011 / 030 | Total loss: 1.015 | Reg loss: 0.039 | Tree loss: 1.015 | Accuracy: 0.597656 | 6.9 sec/iter\n",
      "Epoch: 99 | Batch: 012 / 030 | Total loss: 1.019 | Reg loss: 0.039 | Tree loss: 1.019 | Accuracy: 0.552734 | 6.899 sec/iter\n",
      "Epoch: 99 | Batch: 013 / 030 | Total loss: 0.984 | Reg loss: 0.039 | Tree loss: 0.984 | Accuracy: 0.568359 | 6.897 sec/iter\n",
      "Epoch: 99 | Batch: 014 / 030 | Total loss: 0.983 | Reg loss: 0.039 | Tree loss: 0.983 | Accuracy: 0.560547 | 6.896 sec/iter\n",
      "Epoch: 99 | Batch: 015 / 030 | Total loss: 0.949 | Reg loss: 0.039 | Tree loss: 0.949 | Accuracy: 0.585938 | 6.895 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 99 | Batch: 016 / 030 | Total loss: 0.961 | Reg loss: 0.039 | Tree loss: 0.961 | Accuracy: 0.548828 | 6.894 sec/iter\n",
      "Epoch: 99 | Batch: 017 / 030 | Total loss: 0.945 | Reg loss: 0.039 | Tree loss: 0.945 | Accuracy: 0.578125 | 6.893 sec/iter\n",
      "Epoch: 99 | Batch: 018 / 030 | Total loss: 0.923 | Reg loss: 0.039 | Tree loss: 0.923 | Accuracy: 0.601562 | 6.892 sec/iter\n",
      "Epoch: 99 | Batch: 019 / 030 | Total loss: 0.914 | Reg loss: 0.039 | Tree loss: 0.914 | Accuracy: 0.580078 | 6.891 sec/iter\n",
      "Epoch: 99 | Batch: 020 / 030 | Total loss: 0.918 | Reg loss: 0.039 | Tree loss: 0.918 | Accuracy: 0.570312 | 6.89 sec/iter\n",
      "Epoch: 99 | Batch: 021 / 030 | Total loss: 0.878 | Reg loss: 0.039 | Tree loss: 0.878 | Accuracy: 0.597656 | 6.889 sec/iter\n",
      "Epoch: 99 | Batch: 022 / 030 | Total loss: 0.889 | Reg loss: 0.039 | Tree loss: 0.889 | Accuracy: 0.585938 | 6.887 sec/iter\n",
      "Epoch: 99 | Batch: 023 / 030 | Total loss: 0.874 | Reg loss: 0.040 | Tree loss: 0.874 | Accuracy: 0.595703 | 6.886 sec/iter\n",
      "Epoch: 99 | Batch: 024 / 030 | Total loss: 0.870 | Reg loss: 0.040 | Tree loss: 0.870 | Accuracy: 0.617188 | 6.885 sec/iter\n",
      "Epoch: 99 | Batch: 025 / 030 | Total loss: 0.873 | Reg loss: 0.040 | Tree loss: 0.873 | Accuracy: 0.574219 | 6.884 sec/iter\n",
      "Epoch: 99 | Batch: 026 / 030 | Total loss: 0.851 | Reg loss: 0.040 | Tree loss: 0.851 | Accuracy: 0.617188 | 6.883 sec/iter\n",
      "Epoch: 99 | Batch: 027 / 030 | Total loss: 0.860 | Reg loss: 0.040 | Tree loss: 0.860 | Accuracy: 0.605469 | 6.882 sec/iter\n",
      "Epoch: 99 | Batch: 028 / 030 | Total loss: 0.853 | Reg loss: 0.040 | Tree loss: 0.853 | Accuracy: 0.578125 | 6.881 sec/iter\n",
      "Epoch: 99 | Batch: 029 / 030 | Total loss: 0.885 | Reg loss: 0.040 | Tree loss: 0.885 | Accuracy: 0.495146 | 6.879 sec/iter\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "iteration = 0\n",
    "for epoch in range(epochs):\n",
    "    # Training\n",
    "    avg_sp = show_sparseness(tree)\n",
    "    sparsity.append(avg_sp)\n",
    "    iteration = do_epoch(tree, tree_loader, device, log_interval, losses, accs, epoch, iteration)\n",
    "    \n",
    "    if epoch % 1 == 0:\n",
    "        prune_tree(tree, factor=3)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36604defb66f4a2aa4b786a7b973ea66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel('Iteration')\n",
    "plt.plot(accs, label='Accuracy vs iteration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f699b2d901314e2997c11dc8933a253f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f87c15ff5c974541ac7ddb2a6df2e637",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel('Iteration')\n",
    "plt.plot(losses, label='Loss vs iteration')\n",
    "plt.yscale(\"log\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "weights = tree.inner_nodes.weight.cpu().detach().numpy().flatten()\n",
    "plt.hist(weights, bins=500)\n",
    "weights_std = np.std(weights)\n",
    "weights_mean = np.mean(weights)\n",
    "plt.axvline(weights_mean + weights_std, color='r')\n",
    "plt.axvline(weights_mean - weights_std, color='r')\n",
    "plt.title(f\"Mean: {weights_mean}   |   STD: {weights_std}\")\n",
    "plt.yscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da90998c99e049869ddff51561f4b84d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average height: 11.736542962219993\n"
     ]
    }
   ],
   "source": [
    "plt.figure(figsize=(15, 10), dpi=80)\n",
    "avg_height, root = tree.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accumulate samples in the leaves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of patterns: 2991\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of patterns: {len(root.get_leaves())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = 'greedy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eitan.k/.local/lib/python3.6/site-packages/torch/nn/functional.py:1709: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    }
   ],
   "source": [
    "root.clear_leaves_samples()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (data, target) in enumerate(tree_loader):\n",
    "        root.accumulate_samples(data, method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tighten boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eitan.k/EntangledExplainableClustering/soft_decision_tree/sdt_model.py:14: RuntimeWarning: divide by zero encountered in log\n",
      "  return np.log(1 / (1 - x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Pattern 1 ==============\n",
      "============== Pattern 2 ==============\n",
      "============== Pattern 3 ==============\n",
      "============== Pattern 4 ==============\n",
      "============== Pattern 5 ==============\n",
      "============== Pattern 6 ==============\n",
      "============== Pattern 7 ==============\n",
      "============== Pattern 8 ==============\n",
      "============== Pattern 9 ==============\n",
      "============== Pattern 10 ==============\n",
      "============== Pattern 11 ==============\n",
      "============== Pattern 12 ==============\n",
      "============== Pattern 13 ==============\n",
      "============== Pattern 14 ==============\n",
      "============== Pattern 15 ==============\n",
      "============== Pattern 16 ==============\n",
      "============== Pattern 17 ==============\n",
      "============== Pattern 18 ==============\n",
      "============== Pattern 19 ==============\n",
      "============== Pattern 20 ==============\n",
      "============== Pattern 21 ==============\n",
      "============== Pattern 22 ==============\n",
      "============== Pattern 23 ==============\n",
      "============== Pattern 24 ==============\n",
      "============== Pattern 25 ==============\n",
      "============== Pattern 26 ==============\n",
      "============== Pattern 27 ==============\n",
      "============== Pattern 28 ==============\n",
      "============== Pattern 29 ==============\n",
      "============== Pattern 30 ==============\n",
      "============== Pattern 31 ==============\n",
      "============== Pattern 32 ==============\n",
      "============== Pattern 33 ==============\n",
      "============== Pattern 34 ==============\n",
      "============== Pattern 35 ==============\n",
      "============== Pattern 36 ==============\n",
      "============== Pattern 37 ==============\n",
      "14951\n",
      "============== Pattern 38 ==============\n",
      "============== Pattern 39 ==============\n",
      "============== Pattern 40 ==============\n",
      "============== Pattern 41 ==============\n",
      "============== Pattern 42 ==============\n",
      "============== Pattern 43 ==============\n",
      "============== Pattern 44 ==============\n",
      "============== Pattern 45 ==============\n",
      "============== Pattern 46 ==============\n",
      "============== Pattern 47 ==============\n",
      "============== Pattern 48 ==============\n",
      "============== Pattern 49 ==============\n",
      "============== Pattern 50 ==============\n",
      "============== Pattern 51 ==============\n",
      "============== Pattern 52 ==============\n",
      "============== Pattern 53 ==============\n",
      "============== Pattern 54 ==============\n",
      "============== Pattern 55 ==============\n",
      "============== Pattern 56 ==============\n",
      "============== Pattern 57 ==============\n",
      "============== Pattern 58 ==============\n",
      "============== Pattern 59 ==============\n",
      "============== Pattern 60 ==============\n",
      "============== Pattern 61 ==============\n",
      "============== Pattern 62 ==============\n",
      "============== Pattern 63 ==============\n",
      "============== Pattern 64 ==============\n",
      "============== Pattern 65 ==============\n",
      "============== Pattern 66 ==============\n",
      "============== Pattern 67 ==============\n",
      "============== Pattern 68 ==============\n",
      "============== Pattern 69 ==============\n",
      "============== Pattern 70 ==============\n",
      "============== Pattern 71 ==============\n",
      "============== Pattern 72 ==============\n",
      "============== Pattern 73 ==============\n",
      "============== Pattern 74 ==============\n",
      "============== Pattern 75 ==============\n",
      "============== Pattern 76 ==============\n",
      "============== Pattern 77 ==============\n",
      "============== Pattern 78 ==============\n",
      "============== Pattern 79 ==============\n",
      "============== Pattern 80 ==============\n",
      "============== Pattern 81 ==============\n",
      "============== Pattern 82 ==============\n",
      "============== Pattern 83 ==============\n",
      "============== Pattern 84 ==============\n",
      "============== Pattern 85 ==============\n",
      "============== Pattern 86 ==============\n",
      "============== Pattern 87 ==============\n",
      "============== Pattern 88 ==============\n",
      "============== Pattern 89 ==============\n",
      "============== Pattern 90 ==============\n",
      "============== Pattern 91 ==============\n",
      "============== Pattern 92 ==============\n",
      "============== Pattern 93 ==============\n",
      "============== Pattern 94 ==============\n",
      "============== Pattern 95 ==============\n",
      "============== Pattern 96 ==============\n",
      "============== Pattern 97 ==============\n",
      "============== Pattern 98 ==============\n",
      "============== Pattern 99 ==============\n",
      "============== Pattern 100 ==============\n",
      "============== Pattern 101 ==============\n",
      "============== Pattern 102 ==============\n",
      "============== Pattern 103 ==============\n",
      "============== Pattern 104 ==============\n",
      "============== Pattern 105 ==============\n",
      "============== Pattern 106 ==============\n",
      "============== Pattern 107 ==============\n",
      "============== Pattern 108 ==============\n",
      "============== Pattern 109 ==============\n",
      "============== Pattern 110 ==============\n",
      "============== Pattern 111 ==============\n",
      "============== Pattern 112 ==============\n",
      "============== Pattern 113 ==============\n",
      "============== Pattern 114 ==============\n",
      "============== Pattern 115 ==============\n",
      "============== Pattern 116 ==============\n",
      "============== Pattern 117 ==============\n",
      "============== Pattern 118 ==============\n",
      "============== Pattern 119 ==============\n",
      "============== Pattern 120 ==============\n",
      "============== Pattern 121 ==============\n",
      "============== Pattern 122 ==============\n",
      "============== Pattern 123 ==============\n",
      "============== Pattern 124 ==============\n",
      "============== Pattern 125 ==============\n",
      "============== Pattern 126 ==============\n",
      "============== Pattern 127 ==============\n",
      "============== Pattern 128 ==============\n",
      "============== Pattern 129 ==============\n",
      "============== Pattern 130 ==============\n",
      "============== Pattern 131 ==============\n",
      "============== Pattern 132 ==============\n",
      "============== Pattern 133 ==============\n",
      "============== Pattern 134 ==============\n",
      "============== Pattern 135 ==============\n",
      "============== Pattern 136 ==============\n",
      "============== Pattern 137 ==============\n",
      "============== Pattern 138 ==============\n",
      "============== Pattern 139 ==============\n",
      "============== Pattern 140 ==============\n",
      "============== Pattern 141 ==============\n",
      "============== Pattern 142 ==============\n",
      "============== Pattern 143 ==============\n",
      "============== Pattern 144 ==============\n",
      "============== Pattern 145 ==============\n",
      "============== Pattern 146 ==============\n",
      "============== Pattern 147 ==============\n",
      "============== Pattern 148 ==============\n",
      "============== Pattern 149 ==============\n",
      "============== Pattern 150 ==============\n",
      "============== Pattern 151 ==============\n",
      "============== Pattern 152 ==============\n",
      "============== Pattern 153 ==============\n",
      "============== Pattern 154 ==============\n",
      "============== Pattern 155 ==============\n",
      "============== Pattern 156 ==============\n",
      "============== Pattern 157 ==============\n",
      "============== Pattern 158 ==============\n",
      "============== Pattern 159 ==============\n",
      "============== Pattern 160 ==============\n",
      "============== Pattern 161 ==============\n",
      "============== Pattern 162 ==============\n",
      "============== Pattern 163 ==============\n",
      "============== Pattern 164 ==============\n",
      "============== Pattern 165 ==============\n",
      "============== Pattern 166 ==============\n",
      "============== Pattern 167 ==============\n",
      "============== Pattern 168 ==============\n",
      "============== Pattern 169 ==============\n",
      "============== Pattern 170 ==============\n",
      "============== Pattern 171 ==============\n",
      "============== Pattern 172 ==============\n",
      "============== Pattern 173 ==============\n",
      "============== Pattern 174 ==============\n",
      "============== Pattern 175 ==============\n",
      "============== Pattern 176 ==============\n",
      "============== Pattern 177 ==============\n",
      "============== Pattern 178 ==============\n",
      "============== Pattern 179 ==============\n",
      "============== Pattern 180 ==============\n",
      "============== Pattern 181 ==============\n",
      "============== Pattern 182 ==============\n",
      "============== Pattern 183 ==============\n",
      "============== Pattern 184 ==============\n",
      "============== Pattern 185 ==============\n",
      "============== Pattern 186 ==============\n",
      "============== Pattern 187 ==============\n",
      "============== Pattern 188 ==============\n",
      "============== Pattern 189 ==============\n",
      "============== Pattern 190 ==============\n",
      "============== Pattern 191 ==============\n",
      "============== Pattern 192 ==============\n",
      "============== Pattern 193 ==============\n",
      "============== Pattern 194 ==============\n",
      "============== Pattern 195 ==============\n",
      "============== Pattern 196 ==============\n",
      "============== Pattern 197 ==============\n",
      "============== Pattern 198 ==============\n",
      "============== Pattern 199 ==============\n",
      "============== Pattern 200 ==============\n",
      "============== Pattern 201 ==============\n",
      "============== Pattern 202 ==============\n",
      "============== Pattern 203 ==============\n",
      "============== Pattern 204 ==============\n",
      "============== Pattern 205 ==============\n",
      "============== Pattern 206 ==============\n",
      "============== Pattern 207 ==============\n",
      "============== Pattern 208 ==============\n",
      "============== Pattern 209 ==============\n",
      "============== Pattern 210 ==============\n",
      "============== Pattern 211 ==============\n",
      "============== Pattern 212 ==============\n",
      "============== Pattern 213 ==============\n",
      "============== Pattern 214 ==============\n",
      "============== Pattern 215 ==============\n",
      "============== Pattern 216 ==============\n",
      "============== Pattern 217 ==============\n",
      "============== Pattern 218 ==============\n",
      "============== Pattern 219 ==============\n",
      "============== Pattern 220 ==============\n",
      "============== Pattern 221 ==============\n",
      "============== Pattern 222 ==============\n",
      "============== Pattern 223 ==============\n",
      "============== Pattern 224 ==============\n",
      "============== Pattern 225 ==============\n",
      "============== Pattern 226 ==============\n",
      "============== Pattern 227 ==============\n",
      "============== Pattern 228 ==============\n",
      "============== Pattern 229 ==============\n",
      "============== Pattern 230 ==============\n",
      "============== Pattern 231 ==============\n",
      "============== Pattern 232 ==============\n",
      "============== Pattern 233 ==============\n",
      "============== Pattern 234 ==============\n",
      "============== Pattern 235 ==============\n",
      "============== Pattern 236 ==============\n",
      "============== Pattern 237 ==============\n",
      "============== Pattern 238 ==============\n",
      "============== Pattern 239 ==============\n",
      "============== Pattern 240 ==============\n",
      "============== Pattern 241 ==============\n",
      "============== Pattern 242 ==============\n",
      "============== Pattern 243 ==============\n",
      "============== Pattern 244 ==============\n",
      "============== Pattern 245 ==============\n",
      "============== Pattern 246 ==============\n",
      "============== Pattern 247 ==============\n",
      "============== Pattern 248 ==============\n",
      "============== Pattern 249 ==============\n",
      "============== Pattern 250 ==============\n",
      "============== Pattern 251 ==============\n",
      "============== Pattern 252 ==============\n",
      "============== Pattern 253 ==============\n",
      "============== Pattern 254 ==============\n",
      "============== Pattern 255 ==============\n",
      "============== Pattern 256 ==============\n",
      "============== Pattern 257 ==============\n",
      "============== Pattern 258 ==============\n",
      "============== Pattern 259 ==============\n",
      "============== Pattern 260 ==============\n",
      "============== Pattern 261 ==============\n",
      "============== Pattern 262 ==============\n",
      "============== Pattern 263 ==============\n",
      "============== Pattern 264 ==============\n",
      "============== Pattern 265 ==============\n",
      "============== Pattern 266 ==============\n",
      "============== Pattern 267 ==============\n",
      "============== Pattern 268 ==============\n",
      "============== Pattern 269 ==============\n",
      "============== Pattern 270 ==============\n",
      "============== Pattern 271 ==============\n",
      "============== Pattern 272 ==============\n",
      "============== Pattern 273 ==============\n",
      "============== Pattern 274 ==============\n",
      "============== Pattern 275 ==============\n",
      "============== Pattern 276 ==============\n",
      "============== Pattern 277 ==============\n",
      "============== Pattern 278 ==============\n",
      "============== Pattern 279 ==============\n",
      "============== Pattern 280 ==============\n",
      "============== Pattern 281 ==============\n",
      "============== Pattern 282 ==============\n",
      "============== Pattern 283 ==============\n",
      "============== Pattern 284 ==============\n",
      "============== Pattern 285 ==============\n",
      "============== Pattern 286 ==============\n",
      "============== Pattern 287 ==============\n",
      "============== Pattern 288 ==============\n",
      "============== Pattern 289 ==============\n",
      "============== Pattern 290 ==============\n",
      "============== Pattern 291 ==============\n",
      "============== Pattern 292 ==============\n",
      "============== Pattern 293 ==============\n",
      "============== Pattern 294 ==============\n",
      "============== Pattern 295 ==============\n",
      "============== Pattern 296 ==============\n",
      "============== Pattern 297 ==============\n",
      "============== Pattern 298 ==============\n",
      "============== Pattern 299 ==============\n",
      "============== Pattern 300 ==============\n",
      "============== Pattern 301 ==============\n",
      "============== Pattern 302 ==============\n",
      "============== Pattern 303 ==============\n",
      "============== Pattern 304 ==============\n",
      "============== Pattern 305 ==============\n",
      "============== Pattern 306 ==============\n",
      "============== Pattern 307 ==============\n",
      "============== Pattern 308 ==============\n",
      "============== Pattern 309 ==============\n",
      "============== Pattern 310 ==============\n",
      "============== Pattern 311 ==============\n",
      "============== Pattern 312 ==============\n",
      "============== Pattern 313 ==============\n",
      "============== Pattern 314 ==============\n",
      "============== Pattern 315 ==============\n",
      "============== Pattern 316 ==============\n",
      "============== Pattern 317 ==============\n",
      "============== Pattern 318 ==============\n",
      "============== Pattern 319 ==============\n",
      "============== Pattern 320 ==============\n",
      "============== Pattern 321 ==============\n",
      "============== Pattern 322 ==============\n",
      "============== Pattern 323 ==============\n",
      "============== Pattern 324 ==============\n",
      "============== Pattern 325 ==============\n",
      "============== Pattern 326 ==============\n",
      "============== Pattern 327 ==============\n",
      "============== Pattern 328 ==============\n",
      "============== Pattern 329 ==============\n",
      "============== Pattern 330 ==============\n",
      "============== Pattern 331 ==============\n",
      "============== Pattern 332 ==============\n",
      "============== Pattern 333 ==============\n",
      "============== Pattern 334 ==============\n",
      "============== Pattern 335 ==============\n",
      "============== Pattern 336 ==============\n",
      "============== Pattern 337 ==============\n",
      "============== Pattern 338 ==============\n",
      "============== Pattern 339 ==============\n",
      "============== Pattern 340 ==============\n",
      "============== Pattern 341 ==============\n",
      "============== Pattern 342 ==============\n",
      "============== Pattern 343 ==============\n",
      "============== Pattern 344 ==============\n",
      "============== Pattern 345 ==============\n",
      "============== Pattern 346 ==============\n",
      "============== Pattern 347 ==============\n",
      "============== Pattern 348 ==============\n",
      "============== Pattern 349 ==============\n",
      "============== Pattern 350 ==============\n",
      "============== Pattern 351 ==============\n",
      "============== Pattern 352 ==============\n",
      "============== Pattern 353 ==============\n",
      "============== Pattern 354 ==============\n",
      "============== Pattern 355 ==============\n",
      "============== Pattern 356 ==============\n",
      "============== Pattern 357 ==============\n",
      "============== Pattern 358 ==============\n",
      "============== Pattern 359 ==============\n",
      "============== Pattern 360 ==============\n",
      "============== Pattern 361 ==============\n",
      "============== Pattern 362 ==============\n",
      "============== Pattern 363 ==============\n",
      "============== Pattern 364 ==============\n",
      "============== Pattern 365 ==============\n",
      "============== Pattern 366 ==============\n",
      "============== Pattern 367 ==============\n",
      "============== Pattern 368 ==============\n",
      "============== Pattern 369 ==============\n",
      "============== Pattern 370 ==============\n",
      "============== Pattern 371 ==============\n",
      "============== Pattern 372 ==============\n",
      "============== Pattern 373 ==============\n",
      "============== Pattern 374 ==============\n",
      "============== Pattern 375 ==============\n",
      "============== Pattern 376 ==============\n",
      "============== Pattern 377 ==============\n",
      "============== Pattern 378 ==============\n",
      "============== Pattern 379 ==============\n",
      "============== Pattern 380 ==============\n",
      "============== Pattern 381 ==============\n",
      "============== Pattern 382 ==============\n",
      "============== Pattern 383 ==============\n",
      "============== Pattern 384 ==============\n",
      "============== Pattern 385 ==============\n",
      "============== Pattern 386 ==============\n",
      "============== Pattern 387 ==============\n",
      "============== Pattern 388 ==============\n",
      "============== Pattern 389 ==============\n",
      "============== Pattern 390 ==============\n",
      "============== Pattern 391 ==============\n",
      "============== Pattern 392 ==============\n",
      "============== Pattern 393 ==============\n",
      "============== Pattern 394 ==============\n",
      "============== Pattern 395 ==============\n",
      "============== Pattern 396 ==============\n",
      "============== Pattern 397 ==============\n",
      "============== Pattern 398 ==============\n",
      "============== Pattern 399 ==============\n",
      "============== Pattern 400 ==============\n",
      "============== Pattern 401 ==============\n",
      "============== Pattern 402 ==============\n",
      "============== Pattern 403 ==============\n",
      "============== Pattern 404 ==============\n",
      "============== Pattern 405 ==============\n",
      "============== Pattern 406 ==============\n",
      "============== Pattern 407 ==============\n",
      "============== Pattern 408 ==============\n",
      "============== Pattern 409 ==============\n",
      "============== Pattern 410 ==============\n",
      "============== Pattern 411 ==============\n",
      "============== Pattern 412 ==============\n",
      "============== Pattern 413 ==============\n",
      "============== Pattern 414 ==============\n",
      "============== Pattern 415 ==============\n",
      "============== Pattern 416 ==============\n",
      "============== Pattern 417 ==============\n",
      "============== Pattern 418 ==============\n",
      "============== Pattern 419 ==============\n",
      "============== Pattern 420 ==============\n",
      "============== Pattern 421 ==============\n",
      "============== Pattern 422 ==============\n",
      "============== Pattern 423 ==============\n",
      "============== Pattern 424 ==============\n",
      "============== Pattern 425 ==============\n",
      "============== Pattern 426 ==============\n",
      "============== Pattern 427 ==============\n",
      "============== Pattern 428 ==============\n",
      "============== Pattern 429 ==============\n",
      "============== Pattern 430 ==============\n",
      "============== Pattern 431 ==============\n",
      "============== Pattern 432 ==============\n",
      "============== Pattern 433 ==============\n",
      "============== Pattern 434 ==============\n",
      "============== Pattern 435 ==============\n",
      "============== Pattern 436 ==============\n",
      "============== Pattern 437 ==============\n",
      "============== Pattern 438 ==============\n",
      "============== Pattern 439 ==============\n",
      "============== Pattern 440 ==============\n",
      "============== Pattern 441 ==============\n",
      "============== Pattern 442 ==============\n",
      "============== Pattern 443 ==============\n",
      "============== Pattern 444 ==============\n",
      "============== Pattern 445 ==============\n",
      "============== Pattern 446 ==============\n",
      "============== Pattern 447 ==============\n",
      "============== Pattern 448 ==============\n",
      "============== Pattern 449 ==============\n",
      "============== Pattern 450 ==============\n",
      "============== Pattern 451 ==============\n",
      "============== Pattern 452 ==============\n",
      "============== Pattern 453 ==============\n",
      "============== Pattern 454 ==============\n",
      "============== Pattern 455 ==============\n",
      "============== Pattern 456 ==============\n",
      "============== Pattern 457 ==============\n",
      "============== Pattern 458 ==============\n",
      "============== Pattern 459 ==============\n",
      "============== Pattern 460 ==============\n",
      "============== Pattern 461 ==============\n",
      "============== Pattern 462 ==============\n",
      "============== Pattern 463 ==============\n",
      "============== Pattern 464 ==============\n",
      "============== Pattern 465 ==============\n",
      "============== Pattern 466 ==============\n",
      "============== Pattern 467 ==============\n",
      "============== Pattern 468 ==============\n",
      "============== Pattern 469 ==============\n",
      "============== Pattern 470 ==============\n",
      "============== Pattern 471 ==============\n",
      "============== Pattern 472 ==============\n",
      "============== Pattern 473 ==============\n",
      "============== Pattern 474 ==============\n",
      "============== Pattern 475 ==============\n",
      "============== Pattern 476 ==============\n",
      "============== Pattern 477 ==============\n",
      "============== Pattern 478 ==============\n",
      "============== Pattern 479 ==============\n",
      "============== Pattern 480 ==============\n",
      "============== Pattern 481 ==============\n",
      "============== Pattern 482 ==============\n",
      "============== Pattern 483 ==============\n",
      "============== Pattern 484 ==============\n",
      "============== Pattern 485 ==============\n",
      "============== Pattern 486 ==============\n",
      "============== Pattern 487 ==============\n",
      "============== Pattern 488 ==============\n",
      "============== Pattern 489 ==============\n",
      "============== Pattern 490 ==============\n",
      "============== Pattern 491 ==============\n",
      "============== Pattern 492 ==============\n",
      "============== Pattern 493 ==============\n",
      "============== Pattern 494 ==============\n",
      "============== Pattern 495 ==============\n",
      "============== Pattern 496 ==============\n",
      "============== Pattern 497 ==============\n",
      "============== Pattern 498 ==============\n",
      "============== Pattern 499 ==============\n",
      "============== Pattern 500 ==============\n",
      "============== Pattern 501 ==============\n",
      "============== Pattern 502 ==============\n",
      "============== Pattern 503 ==============\n",
      "============== Pattern 504 ==============\n",
      "============== Pattern 505 ==============\n",
      "============== Pattern 506 ==============\n",
      "============== Pattern 507 ==============\n",
      "============== Pattern 508 ==============\n",
      "============== Pattern 509 ==============\n",
      "============== Pattern 510 ==============\n",
      "============== Pattern 511 ==============\n",
      "============== Pattern 512 ==============\n",
      "============== Pattern 513 ==============\n",
      "============== Pattern 514 ==============\n",
      "============== Pattern 515 ==============\n",
      "============== Pattern 516 ==============\n",
      "============== Pattern 517 ==============\n",
      "============== Pattern 518 ==============\n",
      "============== Pattern 519 ==============\n",
      "============== Pattern 520 ==============\n",
      "============== Pattern 521 ==============\n",
      "============== Pattern 522 ==============\n",
      "============== Pattern 523 ==============\n",
      "============== Pattern 524 ==============\n",
      "============== Pattern 525 ==============\n",
      "============== Pattern 526 ==============\n",
      "============== Pattern 527 ==============\n",
      "============== Pattern 528 ==============\n",
      "============== Pattern 529 ==============\n",
      "============== Pattern 530 ==============\n",
      "============== Pattern 531 ==============\n",
      "============== Pattern 532 ==============\n",
      "============== Pattern 533 ==============\n",
      "============== Pattern 534 ==============\n",
      "============== Pattern 535 ==============\n",
      "============== Pattern 536 ==============\n",
      "============== Pattern 537 ==============\n",
      "============== Pattern 538 ==============\n",
      "============== Pattern 539 ==============\n",
      "============== Pattern 540 ==============\n",
      "============== Pattern 541 ==============\n",
      "============== Pattern 542 ==============\n",
      "============== Pattern 543 ==============\n",
      "============== Pattern 544 ==============\n",
      "============== Pattern 545 ==============\n",
      "============== Pattern 546 ==============\n",
      "============== Pattern 547 ==============\n",
      "============== Pattern 548 ==============\n",
      "============== Pattern 549 ==============\n",
      "============== Pattern 550 ==============\n",
      "============== Pattern 551 ==============\n",
      "============== Pattern 552 ==============\n",
      "============== Pattern 553 ==============\n",
      "============== Pattern 554 ==============\n",
      "============== Pattern 555 ==============\n",
      "============== Pattern 556 ==============\n",
      "============== Pattern 557 ==============\n",
      "============== Pattern 558 ==============\n",
      "============== Pattern 559 ==============\n",
      "============== Pattern 560 ==============\n",
      "============== Pattern 561 ==============\n",
      "============== Pattern 562 ==============\n",
      "============== Pattern 563 ==============\n",
      "============== Pattern 564 ==============\n",
      "============== Pattern 565 ==============\n",
      "============== Pattern 566 ==============\n",
      "============== Pattern 567 ==============\n",
      "============== Pattern 568 ==============\n",
      "============== Pattern 569 ==============\n",
      "============== Pattern 570 ==============\n",
      "============== Pattern 571 ==============\n",
      "============== Pattern 572 ==============\n",
      "============== Pattern 573 ==============\n",
      "============== Pattern 574 ==============\n",
      "============== Pattern 575 ==============\n",
      "============== Pattern 576 ==============\n",
      "============== Pattern 577 ==============\n",
      "============== Pattern 578 ==============\n",
      "============== Pattern 579 ==============\n",
      "============== Pattern 580 ==============\n",
      "============== Pattern 581 ==============\n",
      "============== Pattern 582 ==============\n",
      "============== Pattern 583 ==============\n",
      "============== Pattern 584 ==============\n",
      "============== Pattern 585 ==============\n",
      "============== Pattern 586 ==============\n",
      "============== Pattern 587 ==============\n",
      "============== Pattern 588 ==============\n",
      "============== Pattern 589 ==============\n",
      "============== Pattern 590 ==============\n",
      "============== Pattern 591 ==============\n",
      "============== Pattern 592 ==============\n",
      "============== Pattern 593 ==============\n",
      "============== Pattern 594 ==============\n",
      "============== Pattern 595 ==============\n",
      "============== Pattern 596 ==============\n",
      "============== Pattern 597 ==============\n",
      "============== Pattern 598 ==============\n",
      "============== Pattern 599 ==============\n",
      "============== Pattern 600 ==============\n",
      "============== Pattern 601 ==============\n",
      "============== Pattern 602 ==============\n",
      "============== Pattern 603 ==============\n",
      "============== Pattern 604 ==============\n",
      "============== Pattern 605 ==============\n",
      "============== Pattern 606 ==============\n",
      "============== Pattern 607 ==============\n",
      "============== Pattern 608 ==============\n",
      "============== Pattern 609 ==============\n",
      "============== Pattern 610 ==============\n",
      "============== Pattern 611 ==============\n",
      "============== Pattern 612 ==============\n",
      "============== Pattern 613 ==============\n",
      "============== Pattern 614 ==============\n",
      "============== Pattern 615 ==============\n",
      "============== Pattern 616 ==============\n",
      "============== Pattern 617 ==============\n",
      "============== Pattern 618 ==============\n",
      "============== Pattern 619 ==============\n",
      "============== Pattern 620 ==============\n",
      "============== Pattern 621 ==============\n",
      "============== Pattern 622 ==============\n",
      "============== Pattern 623 ==============\n",
      "============== Pattern 624 ==============\n",
      "============== Pattern 625 ==============\n",
      "============== Pattern 626 ==============\n",
      "============== Pattern 627 ==============\n",
      "============== Pattern 628 ==============\n",
      "============== Pattern 629 ==============\n",
      "============== Pattern 630 ==============\n",
      "============== Pattern 631 ==============\n",
      "============== Pattern 632 ==============\n",
      "============== Pattern 633 ==============\n",
      "============== Pattern 634 ==============\n",
      "============== Pattern 635 ==============\n",
      "============== Pattern 636 ==============\n",
      "============== Pattern 637 ==============\n",
      "============== Pattern 638 ==============\n",
      "============== Pattern 639 ==============\n",
      "============== Pattern 640 ==============\n",
      "============== Pattern 641 ==============\n",
      "============== Pattern 642 ==============\n",
      "============== Pattern 643 ==============\n",
      "============== Pattern 644 ==============\n",
      "============== Pattern 645 ==============\n",
      "============== Pattern 646 ==============\n",
      "============== Pattern 647 ==============\n",
      "============== Pattern 648 ==============\n",
      "============== Pattern 649 ==============\n",
      "============== Pattern 650 ==============\n",
      "============== Pattern 651 ==============\n",
      "============== Pattern 652 ==============\n",
      "============== Pattern 653 ==============\n",
      "============== Pattern 654 ==============\n",
      "============== Pattern 655 ==============\n",
      "============== Pattern 656 ==============\n",
      "============== Pattern 657 ==============\n",
      "============== Pattern 658 ==============\n",
      "============== Pattern 659 ==============\n",
      "============== Pattern 660 ==============\n",
      "============== Pattern 661 ==============\n",
      "============== Pattern 662 ==============\n",
      "============== Pattern 663 ==============\n",
      "============== Pattern 664 ==============\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Pattern 665 ==============\n",
      "============== Pattern 666 ==============\n",
      "============== Pattern 667 ==============\n",
      "============== Pattern 668 ==============\n",
      "============== Pattern 669 ==============\n",
      "============== Pattern 670 ==============\n",
      "============== Pattern 671 ==============\n",
      "============== Pattern 672 ==============\n",
      "============== Pattern 673 ==============\n",
      "============== Pattern 674 ==============\n",
      "============== Pattern 675 ==============\n",
      "============== Pattern 676 ==============\n",
      "============== Pattern 677 ==============\n",
      "============== Pattern 678 ==============\n",
      "============== Pattern 679 ==============\n",
      "============== Pattern 680 ==============\n",
      "============== Pattern 681 ==============\n",
      "============== Pattern 682 ==============\n",
      "============== Pattern 683 ==============\n",
      "============== Pattern 684 ==============\n",
      "============== Pattern 685 ==============\n",
      "============== Pattern 686 ==============\n",
      "============== Pattern 687 ==============\n",
      "============== Pattern 688 ==============\n",
      "============== Pattern 689 ==============\n",
      "============== Pattern 690 ==============\n",
      "============== Pattern 691 ==============\n",
      "============== Pattern 692 ==============\n",
      "============== Pattern 693 ==============\n",
      "============== Pattern 694 ==============\n",
      "============== Pattern 695 ==============\n",
      "============== Pattern 696 ==============\n",
      "============== Pattern 697 ==============\n",
      "============== Pattern 698 ==============\n",
      "============== Pattern 699 ==============\n",
      "============== Pattern 700 ==============\n",
      "============== Pattern 701 ==============\n",
      "============== Pattern 702 ==============\n",
      "============== Pattern 703 ==============\n",
      "============== Pattern 704 ==============\n",
      "============== Pattern 705 ==============\n",
      "============== Pattern 706 ==============\n",
      "============== Pattern 707 ==============\n",
      "============== Pattern 708 ==============\n",
      "============== Pattern 709 ==============\n",
      "============== Pattern 710 ==============\n",
      "============== Pattern 711 ==============\n",
      "============== Pattern 712 ==============\n",
      "============== Pattern 713 ==============\n",
      "============== Pattern 714 ==============\n",
      "============== Pattern 715 ==============\n",
      "============== Pattern 716 ==============\n",
      "============== Pattern 717 ==============\n",
      "============== Pattern 718 ==============\n",
      "============== Pattern 719 ==============\n",
      "============== Pattern 720 ==============\n",
      "============== Pattern 721 ==============\n",
      "============== Pattern 722 ==============\n",
      "============== Pattern 723 ==============\n",
      "============== Pattern 724 ==============\n",
      "============== Pattern 725 ==============\n",
      "============== Pattern 726 ==============\n",
      "============== Pattern 727 ==============\n",
      "============== Pattern 728 ==============\n",
      "============== Pattern 729 ==============\n",
      "============== Pattern 730 ==============\n",
      "============== Pattern 731 ==============\n",
      "============== Pattern 732 ==============\n",
      "============== Pattern 733 ==============\n",
      "============== Pattern 734 ==============\n",
      "============== Pattern 735 ==============\n",
      "============== Pattern 736 ==============\n",
      "============== Pattern 737 ==============\n",
      "============== Pattern 738 ==============\n",
      "============== Pattern 739 ==============\n",
      "============== Pattern 740 ==============\n",
      "============== Pattern 741 ==============\n",
      "============== Pattern 742 ==============\n",
      "============== Pattern 743 ==============\n",
      "============== Pattern 744 ==============\n",
      "============== Pattern 745 ==============\n",
      "============== Pattern 746 ==============\n",
      "============== Pattern 747 ==============\n",
      "============== Pattern 748 ==============\n",
      "============== Pattern 749 ==============\n",
      "============== Pattern 750 ==============\n",
      "============== Pattern 751 ==============\n",
      "============== Pattern 752 ==============\n",
      "============== Pattern 753 ==============\n",
      "============== Pattern 754 ==============\n",
      "============== Pattern 755 ==============\n",
      "============== Pattern 756 ==============\n",
      "============== Pattern 757 ==============\n",
      "============== Pattern 758 ==============\n",
      "============== Pattern 759 ==============\n",
      "============== Pattern 760 ==============\n",
      "============== Pattern 761 ==============\n",
      "============== Pattern 762 ==============\n",
      "============== Pattern 763 ==============\n",
      "============== Pattern 764 ==============\n",
      "============== Pattern 765 ==============\n",
      "============== Pattern 766 ==============\n",
      "============== Pattern 767 ==============\n",
      "============== Pattern 768 ==============\n",
      "============== Pattern 769 ==============\n",
      "============== Pattern 770 ==============\n",
      "============== Pattern 771 ==============\n",
      "============== Pattern 772 ==============\n",
      "============== Pattern 773 ==============\n",
      "============== Pattern 774 ==============\n",
      "============== Pattern 775 ==============\n",
      "============== Pattern 776 ==============\n",
      "============== Pattern 777 ==============\n",
      "============== Pattern 778 ==============\n",
      "============== Pattern 779 ==============\n",
      "============== Pattern 780 ==============\n",
      "============== Pattern 781 ==============\n",
      "============== Pattern 782 ==============\n",
      "============== Pattern 783 ==============\n",
      "============== Pattern 784 ==============\n",
      "============== Pattern 785 ==============\n",
      "============== Pattern 786 ==============\n",
      "============== Pattern 787 ==============\n",
      "============== Pattern 788 ==============\n",
      "============== Pattern 789 ==============\n",
      "============== Pattern 790 ==============\n",
      "============== Pattern 791 ==============\n",
      "============== Pattern 792 ==============\n",
      "============== Pattern 793 ==============\n",
      "============== Pattern 794 ==============\n",
      "============== Pattern 795 ==============\n",
      "============== Pattern 796 ==============\n",
      "============== Pattern 797 ==============\n",
      "============== Pattern 798 ==============\n",
      "============== Pattern 799 ==============\n",
      "============== Pattern 800 ==============\n",
      "============== Pattern 801 ==============\n",
      "============== Pattern 802 ==============\n",
      "============== Pattern 803 ==============\n",
      "============== Pattern 804 ==============\n",
      "============== Pattern 805 ==============\n",
      "============== Pattern 806 ==============\n",
      "============== Pattern 807 ==============\n",
      "============== Pattern 808 ==============\n",
      "============== Pattern 809 ==============\n",
      "============== Pattern 810 ==============\n",
      "============== Pattern 811 ==============\n",
      "============== Pattern 812 ==============\n",
      "============== Pattern 813 ==============\n",
      "============== Pattern 814 ==============\n",
      "============== Pattern 815 ==============\n",
      "============== Pattern 816 ==============\n",
      "============== Pattern 817 ==============\n",
      "============== Pattern 818 ==============\n",
      "============== Pattern 819 ==============\n",
      "============== Pattern 820 ==============\n",
      "============== Pattern 821 ==============\n",
      "============== Pattern 822 ==============\n",
      "============== Pattern 823 ==============\n",
      "============== Pattern 824 ==============\n",
      "============== Pattern 825 ==============\n",
      "============== Pattern 826 ==============\n",
      "============== Pattern 827 ==============\n",
      "============== Pattern 828 ==============\n",
      "============== Pattern 829 ==============\n",
      "============== Pattern 830 ==============\n",
      "============== Pattern 831 ==============\n",
      "============== Pattern 832 ==============\n",
      "============== Pattern 833 ==============\n",
      "============== Pattern 834 ==============\n",
      "============== Pattern 835 ==============\n",
      "============== Pattern 836 ==============\n",
      "============== Pattern 837 ==============\n",
      "============== Pattern 838 ==============\n",
      "============== Pattern 839 ==============\n",
      "============== Pattern 840 ==============\n",
      "============== Pattern 841 ==============\n",
      "============== Pattern 842 ==============\n",
      "============== Pattern 843 ==============\n",
      "============== Pattern 844 ==============\n",
      "============== Pattern 845 ==============\n",
      "============== Pattern 846 ==============\n",
      "============== Pattern 847 ==============\n",
      "============== Pattern 848 ==============\n",
      "============== Pattern 849 ==============\n",
      "============== Pattern 850 ==============\n",
      "============== Pattern 851 ==============\n",
      "============== Pattern 852 ==============\n",
      "============== Pattern 853 ==============\n",
      "============== Pattern 854 ==============\n",
      "============== Pattern 855 ==============\n",
      "============== Pattern 856 ==============\n",
      "============== Pattern 857 ==============\n",
      "============== Pattern 858 ==============\n",
      "============== Pattern 859 ==============\n",
      "============== Pattern 860 ==============\n",
      "============== Pattern 861 ==============\n",
      "============== Pattern 862 ==============\n",
      "============== Pattern 863 ==============\n",
      "============== Pattern 864 ==============\n",
      "============== Pattern 865 ==============\n",
      "============== Pattern 866 ==============\n",
      "============== Pattern 867 ==============\n",
      "============== Pattern 868 ==============\n",
      "============== Pattern 869 ==============\n",
      "============== Pattern 870 ==============\n",
      "============== Pattern 871 ==============\n",
      "============== Pattern 872 ==============\n",
      "============== Pattern 873 ==============\n",
      "============== Pattern 874 ==============\n",
      "============== Pattern 875 ==============\n",
      "============== Pattern 876 ==============\n",
      "============== Pattern 877 ==============\n",
      "============== Pattern 878 ==============\n",
      "============== Pattern 879 ==============\n",
      "============== Pattern 880 ==============\n",
      "============== Pattern 881 ==============\n",
      "============== Pattern 882 ==============\n",
      "============== Pattern 883 ==============\n",
      "============== Pattern 884 ==============\n",
      "============== Pattern 885 ==============\n",
      "============== Pattern 886 ==============\n",
      "============== Pattern 887 ==============\n",
      "============== Pattern 888 ==============\n",
      "============== Pattern 889 ==============\n",
      "============== Pattern 890 ==============\n",
      "============== Pattern 891 ==============\n",
      "============== Pattern 892 ==============\n",
      "============== Pattern 893 ==============\n",
      "============== Pattern 894 ==============\n",
      "============== Pattern 895 ==============\n",
      "============== Pattern 896 ==============\n",
      "============== Pattern 897 ==============\n",
      "============== Pattern 898 ==============\n",
      "============== Pattern 899 ==============\n",
      "============== Pattern 900 ==============\n",
      "============== Pattern 901 ==============\n",
      "============== Pattern 902 ==============\n",
      "============== Pattern 903 ==============\n",
      "============== Pattern 904 ==============\n",
      "============== Pattern 905 ==============\n",
      "============== Pattern 906 ==============\n",
      "============== Pattern 907 ==============\n",
      "============== Pattern 908 ==============\n",
      "============== Pattern 909 ==============\n",
      "============== Pattern 910 ==============\n",
      "============== Pattern 911 ==============\n",
      "============== Pattern 912 ==============\n",
      "============== Pattern 913 ==============\n",
      "============== Pattern 914 ==============\n",
      "============== Pattern 915 ==============\n",
      "============== Pattern 916 ==============\n",
      "============== Pattern 917 ==============\n",
      "============== Pattern 918 ==============\n",
      "============== Pattern 919 ==============\n",
      "============== Pattern 920 ==============\n",
      "============== Pattern 921 ==============\n",
      "============== Pattern 922 ==============\n",
      "============== Pattern 923 ==============\n",
      "============== Pattern 924 ==============\n",
      "============== Pattern 925 ==============\n",
      "============== Pattern 926 ==============\n",
      "============== Pattern 927 ==============\n",
      "============== Pattern 928 ==============\n",
      "============== Pattern 929 ==============\n",
      "============== Pattern 930 ==============\n",
      "============== Pattern 931 ==============\n",
      "============== Pattern 932 ==============\n",
      "============== Pattern 933 ==============\n",
      "============== Pattern 934 ==============\n",
      "============== Pattern 935 ==============\n",
      "============== Pattern 936 ==============\n",
      "============== Pattern 937 ==============\n",
      "============== Pattern 938 ==============\n",
      "============== Pattern 939 ==============\n",
      "============== Pattern 940 ==============\n",
      "============== Pattern 941 ==============\n",
      "============== Pattern 942 ==============\n",
      "============== Pattern 943 ==============\n",
      "============== Pattern 944 ==============\n",
      "============== Pattern 945 ==============\n",
      "============== Pattern 946 ==============\n",
      "============== Pattern 947 ==============\n",
      "============== Pattern 948 ==============\n",
      "============== Pattern 949 ==============\n",
      "============== Pattern 950 ==============\n",
      "============== Pattern 951 ==============\n",
      "============== Pattern 952 ==============\n",
      "============== Pattern 953 ==============\n",
      "============== Pattern 954 ==============\n",
      "============== Pattern 955 ==============\n",
      "============== Pattern 956 ==============\n",
      "============== Pattern 957 ==============\n",
      "============== Pattern 958 ==============\n",
      "============== Pattern 959 ==============\n",
      "============== Pattern 960 ==============\n",
      "============== Pattern 961 ==============\n",
      "============== Pattern 962 ==============\n",
      "============== Pattern 963 ==============\n",
      "============== Pattern 964 ==============\n",
      "============== Pattern 965 ==============\n",
      "============== Pattern 966 ==============\n",
      "============== Pattern 967 ==============\n",
      "============== Pattern 968 ==============\n",
      "============== Pattern 969 ==============\n",
      "============== Pattern 970 ==============\n",
      "============== Pattern 971 ==============\n",
      "============== Pattern 972 ==============\n",
      "============== Pattern 973 ==============\n",
      "============== Pattern 974 ==============\n",
      "============== Pattern 975 ==============\n",
      "============== Pattern 976 ==============\n",
      "============== Pattern 977 ==============\n",
      "============== Pattern 978 ==============\n",
      "============== Pattern 979 ==============\n",
      "============== Pattern 980 ==============\n",
      "============== Pattern 981 ==============\n",
      "============== Pattern 982 ==============\n",
      "============== Pattern 983 ==============\n",
      "============== Pattern 984 ==============\n",
      "============== Pattern 985 ==============\n",
      "============== Pattern 986 ==============\n",
      "============== Pattern 987 ==============\n",
      "============== Pattern 988 ==============\n",
      "============== Pattern 989 ==============\n",
      "============== Pattern 990 ==============\n",
      "============== Pattern 991 ==============\n",
      "============== Pattern 992 ==============\n",
      "============== Pattern 993 ==============\n",
      "============== Pattern 994 ==============\n",
      "============== Pattern 995 ==============\n",
      "============== Pattern 996 ==============\n",
      "============== Pattern 997 ==============\n",
      "============== Pattern 998 ==============\n",
      "============== Pattern 999 ==============\n",
      "============== Pattern 1000 ==============\n",
      "============== Pattern 1001 ==============\n",
      "============== Pattern 1002 ==============\n",
      "============== Pattern 1003 ==============\n",
      "============== Pattern 1004 ==============\n",
      "============== Pattern 1005 ==============\n",
      "============== Pattern 1006 ==============\n",
      "============== Pattern 1007 ==============\n",
      "============== Pattern 1008 ==============\n",
      "============== Pattern 1009 ==============\n",
      "============== Pattern 1010 ==============\n",
      "============== Pattern 1011 ==============\n",
      "============== Pattern 1012 ==============\n",
      "============== Pattern 1013 ==============\n",
      "============== Pattern 1014 ==============\n",
      "============== Pattern 1015 ==============\n",
      "============== Pattern 1016 ==============\n",
      "============== Pattern 1017 ==============\n",
      "============== Pattern 1018 ==============\n",
      "============== Pattern 1019 ==============\n",
      "============== Pattern 1020 ==============\n",
      "============== Pattern 1021 ==============\n",
      "============== Pattern 1022 ==============\n",
      "============== Pattern 1023 ==============\n",
      "============== Pattern 1024 ==============\n",
      "============== Pattern 1025 ==============\n",
      "============== Pattern 1026 ==============\n",
      "============== Pattern 1027 ==============\n",
      "============== Pattern 1028 ==============\n",
      "============== Pattern 1029 ==============\n",
      "============== Pattern 1030 ==============\n",
      "============== Pattern 1031 ==============\n",
      "============== Pattern 1032 ==============\n",
      "============== Pattern 1033 ==============\n",
      "============== Pattern 1034 ==============\n",
      "============== Pattern 1035 ==============\n",
      "============== Pattern 1036 ==============\n",
      "============== Pattern 1037 ==============\n",
      "============== Pattern 1038 ==============\n",
      "============== Pattern 1039 ==============\n",
      "============== Pattern 1040 ==============\n",
      "============== Pattern 1041 ==============\n",
      "============== Pattern 1042 ==============\n",
      "============== Pattern 1043 ==============\n",
      "============== Pattern 1044 ==============\n",
      "============== Pattern 1045 ==============\n",
      "============== Pattern 1046 ==============\n",
      "============== Pattern 1047 ==============\n",
      "============== Pattern 1048 ==============\n",
      "============== Pattern 1049 ==============\n",
      "============== Pattern 1050 ==============\n",
      "============== Pattern 1051 ==============\n",
      "============== Pattern 1052 ==============\n",
      "============== Pattern 1053 ==============\n",
      "============== Pattern 1054 ==============\n",
      "============== Pattern 1055 ==============\n",
      "============== Pattern 1056 ==============\n",
      "============== Pattern 1057 ==============\n",
      "============== Pattern 1058 ==============\n",
      "============== Pattern 1059 ==============\n",
      "============== Pattern 1060 ==============\n",
      "============== Pattern 1061 ==============\n",
      "============== Pattern 1062 ==============\n",
      "============== Pattern 1063 ==============\n",
      "============== Pattern 1064 ==============\n",
      "============== Pattern 1065 ==============\n",
      "============== Pattern 1066 ==============\n",
      "============== Pattern 1067 ==============\n",
      "============== Pattern 1068 ==============\n",
      "============== Pattern 1069 ==============\n",
      "============== Pattern 1070 ==============\n",
      "============== Pattern 1071 ==============\n",
      "============== Pattern 1072 ==============\n",
      "============== Pattern 1073 ==============\n",
      "============== Pattern 1074 ==============\n",
      "============== Pattern 1075 ==============\n",
      "============== Pattern 1076 ==============\n",
      "============== Pattern 1077 ==============\n",
      "============== Pattern 1078 ==============\n",
      "============== Pattern 1079 ==============\n",
      "============== Pattern 1080 ==============\n",
      "============== Pattern 1081 ==============\n",
      "============== Pattern 1082 ==============\n",
      "============== Pattern 1083 ==============\n",
      "============== Pattern 1084 ==============\n",
      "============== Pattern 1085 ==============\n",
      "============== Pattern 1086 ==============\n",
      "============== Pattern 1087 ==============\n",
      "============== Pattern 1088 ==============\n",
      "============== Pattern 1089 ==============\n",
      "============== Pattern 1090 ==============\n",
      "============== Pattern 1091 ==============\n",
      "============== Pattern 1092 ==============\n",
      "============== Pattern 1093 ==============\n",
      "============== Pattern 1094 ==============\n",
      "============== Pattern 1095 ==============\n",
      "============== Pattern 1096 ==============\n",
      "============== Pattern 1097 ==============\n",
      "============== Pattern 1098 ==============\n",
      "============== Pattern 1099 ==============\n",
      "============== Pattern 1100 ==============\n",
      "============== Pattern 1101 ==============\n",
      "============== Pattern 1102 ==============\n",
      "============== Pattern 1103 ==============\n",
      "============== Pattern 1104 ==============\n",
      "============== Pattern 1105 ==============\n",
      "============== Pattern 1106 ==============\n",
      "============== Pattern 1107 ==============\n",
      "============== Pattern 1108 ==============\n",
      "============== Pattern 1109 ==============\n",
      "============== Pattern 1110 ==============\n",
      "============== Pattern 1111 ==============\n",
      "============== Pattern 1112 ==============\n",
      "============== Pattern 1113 ==============\n",
      "============== Pattern 1114 ==============\n",
      "============== Pattern 1115 ==============\n",
      "============== Pattern 1116 ==============\n",
      "============== Pattern 1117 ==============\n",
      "============== Pattern 1118 ==============\n",
      "============== Pattern 1119 ==============\n",
      "============== Pattern 1120 ==============\n",
      "============== Pattern 1121 ==============\n",
      "============== Pattern 1122 ==============\n",
      "============== Pattern 1123 ==============\n",
      "============== Pattern 1124 ==============\n",
      "============== Pattern 1125 ==============\n",
      "============== Pattern 1126 ==============\n",
      "============== Pattern 1127 ==============\n",
      "============== Pattern 1128 ==============\n",
      "============== Pattern 1129 ==============\n",
      "============== Pattern 1130 ==============\n",
      "============== Pattern 1131 ==============\n",
      "============== Pattern 1132 ==============\n",
      "============== Pattern 1133 ==============\n",
      "============== Pattern 1134 ==============\n",
      "============== Pattern 1135 ==============\n",
      "============== Pattern 1136 ==============\n",
      "============== Pattern 1137 ==============\n",
      "============== Pattern 1138 ==============\n",
      "============== Pattern 1139 ==============\n",
      "============== Pattern 1140 ==============\n",
      "============== Pattern 1141 ==============\n",
      "============== Pattern 1142 ==============\n",
      "============== Pattern 1143 ==============\n",
      "============== Pattern 1144 ==============\n",
      "============== Pattern 1145 ==============\n",
      "============== Pattern 1146 ==============\n",
      "============== Pattern 1147 ==============\n",
      "============== Pattern 1148 ==============\n",
      "============== Pattern 1149 ==============\n",
      "============== Pattern 1150 ==============\n",
      "============== Pattern 1151 ==============\n",
      "============== Pattern 1152 ==============\n",
      "============== Pattern 1153 ==============\n",
      "============== Pattern 1154 ==============\n",
      "============== Pattern 1155 ==============\n",
      "============== Pattern 1156 ==============\n",
      "============== Pattern 1157 ==============\n",
      "============== Pattern 1158 ==============\n",
      "============== Pattern 1159 ==============\n",
      "============== Pattern 1160 ==============\n",
      "============== Pattern 1161 ==============\n",
      "============== Pattern 1162 ==============\n",
      "============== Pattern 1163 ==============\n",
      "============== Pattern 1164 ==============\n",
      "============== Pattern 1165 ==============\n",
      "============== Pattern 1166 ==============\n",
      "============== Pattern 1167 ==============\n",
      "============== Pattern 1168 ==============\n",
      "============== Pattern 1169 ==============\n",
      "============== Pattern 1170 ==============\n",
      "============== Pattern 1171 ==============\n",
      "============== Pattern 1172 ==============\n",
      "============== Pattern 1173 ==============\n",
      "============== Pattern 1174 ==============\n",
      "============== Pattern 1175 ==============\n",
      "============== Pattern 1176 ==============\n",
      "============== Pattern 1177 ==============\n",
      "============== Pattern 1178 ==============\n",
      "============== Pattern 1179 ==============\n",
      "============== Pattern 1180 ==============\n",
      "============== Pattern 1181 ==============\n",
      "============== Pattern 1182 ==============\n",
      "============== Pattern 1183 ==============\n",
      "============== Pattern 1184 ==============\n",
      "============== Pattern 1185 ==============\n",
      "============== Pattern 1186 ==============\n",
      "============== Pattern 1187 ==============\n",
      "============== Pattern 1188 ==============\n",
      "============== Pattern 1189 ==============\n",
      "============== Pattern 1190 ==============\n",
      "============== Pattern 1191 ==============\n",
      "============== Pattern 1192 ==============\n",
      "============== Pattern 1193 ==============\n",
      "============== Pattern 1194 ==============\n",
      "============== Pattern 1195 ==============\n",
      "============== Pattern 1196 ==============\n",
      "============== Pattern 1197 ==============\n",
      "============== Pattern 1198 ==============\n",
      "============== Pattern 1199 ==============\n",
      "============== Pattern 1200 ==============\n",
      "============== Pattern 1201 ==============\n",
      "============== Pattern 1202 ==============\n",
      "============== Pattern 1203 ==============\n",
      "============== Pattern 1204 ==============\n",
      "============== Pattern 1205 ==============\n",
      "============== Pattern 1206 ==============\n",
      "============== Pattern 1207 ==============\n",
      "============== Pattern 1208 ==============\n",
      "============== Pattern 1209 ==============\n",
      "============== Pattern 1210 ==============\n",
      "============== Pattern 1211 ==============\n",
      "============== Pattern 1212 ==============\n",
      "============== Pattern 1213 ==============\n",
      "============== Pattern 1214 ==============\n",
      "============== Pattern 1215 ==============\n",
      "============== Pattern 1216 ==============\n",
      "============== Pattern 1217 ==============\n",
      "============== Pattern 1218 ==============\n",
      "============== Pattern 1219 ==============\n",
      "============== Pattern 1220 ==============\n",
      "============== Pattern 1221 ==============\n",
      "============== Pattern 1222 ==============\n",
      "============== Pattern 1223 ==============\n",
      "============== Pattern 1224 ==============\n",
      "============== Pattern 1225 ==============\n",
      "============== Pattern 1226 ==============\n",
      "============== Pattern 1227 ==============\n",
      "============== Pattern 1228 ==============\n",
      "============== Pattern 1229 ==============\n",
      "============== Pattern 1230 ==============\n",
      "============== Pattern 1231 ==============\n",
      "============== Pattern 1232 ==============\n",
      "============== Pattern 1233 ==============\n",
      "============== Pattern 1234 ==============\n",
      "============== Pattern 1235 ==============\n",
      "============== Pattern 1236 ==============\n",
      "============== Pattern 1237 ==============\n",
      "============== Pattern 1238 ==============\n",
      "============== Pattern 1239 ==============\n",
      "============== Pattern 1240 ==============\n",
      "============== Pattern 1241 ==============\n",
      "============== Pattern 1242 ==============\n",
      "============== Pattern 1243 ==============\n",
      "============== Pattern 1244 ==============\n",
      "============== Pattern 1245 ==============\n",
      "============== Pattern 1246 ==============\n",
      "============== Pattern 1247 ==============\n",
      "============== Pattern 1248 ==============\n",
      "============== Pattern 1249 ==============\n",
      "============== Pattern 1250 ==============\n",
      "============== Pattern 1251 ==============\n",
      "============== Pattern 1252 ==============\n",
      "============== Pattern 1253 ==============\n",
      "============== Pattern 1254 ==============\n",
      "============== Pattern 1255 ==============\n",
      "============== Pattern 1256 ==============\n",
      "============== Pattern 1257 ==============\n",
      "============== Pattern 1258 ==============\n",
      "============== Pattern 1259 ==============\n",
      "============== Pattern 1260 ==============\n",
      "============== Pattern 1261 ==============\n",
      "============== Pattern 1262 ==============\n",
      "============== Pattern 1263 ==============\n",
      "============== Pattern 1264 ==============\n",
      "============== Pattern 1265 ==============\n",
      "============== Pattern 1266 ==============\n",
      "============== Pattern 1267 ==============\n",
      "============== Pattern 1268 ==============\n",
      "============== Pattern 1269 ==============\n",
      "============== Pattern 1270 ==============\n",
      "============== Pattern 1271 ==============\n",
      "============== Pattern 1272 ==============\n",
      "============== Pattern 1273 ==============\n",
      "============== Pattern 1274 ==============\n",
      "============== Pattern 1275 ==============\n",
      "============== Pattern 1276 ==============\n",
      "============== Pattern 1277 ==============\n",
      "============== Pattern 1278 ==============\n",
      "============== Pattern 1279 ==============\n",
      "============== Pattern 1280 ==============\n",
      "============== Pattern 1281 ==============\n",
      "============== Pattern 1282 ==============\n",
      "============== Pattern 1283 ==============\n",
      "============== Pattern 1284 ==============\n",
      "============== Pattern 1285 ==============\n",
      "============== Pattern 1286 ==============\n",
      "============== Pattern 1287 ==============\n",
      "============== Pattern 1288 ==============\n",
      "============== Pattern 1289 ==============\n",
      "============== Pattern 1290 ==============\n",
      "============== Pattern 1291 ==============\n",
      "============== Pattern 1292 ==============\n",
      "============== Pattern 1293 ==============\n",
      "============== Pattern 1294 ==============\n",
      "============== Pattern 1295 ==============\n",
      "============== Pattern 1296 ==============\n",
      "============== Pattern 1297 ==============\n",
      "============== Pattern 1298 ==============\n",
      "============== Pattern 1299 ==============\n",
      "============== Pattern 1300 ==============\n",
      "============== Pattern 1301 ==============\n",
      "============== Pattern 1302 ==============\n",
      "============== Pattern 1303 ==============\n",
      "============== Pattern 1304 ==============\n",
      "============== Pattern 1305 ==============\n",
      "============== Pattern 1306 ==============\n",
      "============== Pattern 1307 ==============\n",
      "============== Pattern 1308 ==============\n",
      "============== Pattern 1309 ==============\n",
      "============== Pattern 1310 ==============\n",
      "============== Pattern 1311 ==============\n",
      "============== Pattern 1312 ==============\n",
      "============== Pattern 1313 ==============\n",
      "============== Pattern 1314 ==============\n",
      "============== Pattern 1315 ==============\n",
      "============== Pattern 1316 ==============\n",
      "============== Pattern 1317 ==============\n",
      "============== Pattern 1318 ==============\n",
      "============== Pattern 1319 ==============\n",
      "============== Pattern 1320 ==============\n",
      "============== Pattern 1321 ==============\n",
      "============== Pattern 1322 ==============\n",
      "============== Pattern 1323 ==============\n",
      "============== Pattern 1324 ==============\n",
      "============== Pattern 1325 ==============\n",
      "============== Pattern 1326 ==============\n",
      "============== Pattern 1327 ==============\n",
      "============== Pattern 1328 ==============\n",
      "============== Pattern 1329 ==============\n",
      "============== Pattern 1330 ==============\n",
      "============== Pattern 1331 ==============\n",
      "============== Pattern 1332 ==============\n",
      "============== Pattern 1333 ==============\n",
      "============== Pattern 1334 ==============\n",
      "============== Pattern 1335 ==============\n",
      "============== Pattern 1336 ==============\n",
      "============== Pattern 1337 ==============\n",
      "============== Pattern 1338 ==============\n",
      "============== Pattern 1339 ==============\n",
      "============== Pattern 1340 ==============\n",
      "============== Pattern 1341 ==============\n",
      "============== Pattern 1342 ==============\n",
      "============== Pattern 1343 ==============\n",
      "============== Pattern 1344 ==============\n",
      "============== Pattern 1345 ==============\n",
      "============== Pattern 1346 ==============\n",
      "============== Pattern 1347 ==============\n",
      "============== Pattern 1348 ==============\n",
      "============== Pattern 1349 ==============\n",
      "============== Pattern 1350 ==============\n",
      "============== Pattern 1351 ==============\n",
      "============== Pattern 1352 ==============\n",
      "============== Pattern 1353 ==============\n",
      "============== Pattern 1354 ==============\n",
      "============== Pattern 1355 ==============\n",
      "============== Pattern 1356 ==============\n",
      "============== Pattern 1357 ==============\n",
      "============== Pattern 1358 ==============\n",
      "============== Pattern 1359 ==============\n",
      "============== Pattern 1360 ==============\n",
      "============== Pattern 1361 ==============\n",
      "============== Pattern 1362 ==============\n",
      "============== Pattern 1363 ==============\n",
      "============== Pattern 1364 ==============\n",
      "============== Pattern 1365 ==============\n",
      "============== Pattern 1366 ==============\n",
      "============== Pattern 1367 ==============\n",
      "============== Pattern 1368 ==============\n",
      "============== Pattern 1369 ==============\n",
      "============== Pattern 1370 ==============\n",
      "============== Pattern 1371 ==============\n",
      "============== Pattern 1372 ==============\n",
      "============== Pattern 1373 ==============\n",
      "============== Pattern 1374 ==============\n",
      "============== Pattern 1375 ==============\n",
      "============== Pattern 1376 ==============\n",
      "============== Pattern 1377 ==============\n",
      "============== Pattern 1378 ==============\n",
      "============== Pattern 1379 ==============\n",
      "============== Pattern 1380 ==============\n",
      "============== Pattern 1381 ==============\n",
      "============== Pattern 1382 ==============\n",
      "============== Pattern 1383 ==============\n",
      "============== Pattern 1384 ==============\n",
      "============== Pattern 1385 ==============\n",
      "============== Pattern 1386 ==============\n",
      "============== Pattern 1387 ==============\n",
      "============== Pattern 1388 ==============\n",
      "============== Pattern 1389 ==============\n",
      "============== Pattern 1390 ==============\n",
      "============== Pattern 1391 ==============\n",
      "============== Pattern 1392 ==============\n",
      "============== Pattern 1393 ==============\n",
      "============== Pattern 1394 ==============\n",
      "============== Pattern 1395 ==============\n",
      "============== Pattern 1396 ==============\n",
      "============== Pattern 1397 ==============\n",
      "============== Pattern 1398 ==============\n",
      "============== Pattern 1399 ==============\n",
      "============== Pattern 1400 ==============\n",
      "============== Pattern 1401 ==============\n",
      "============== Pattern 1402 ==============\n",
      "============== Pattern 1403 ==============\n",
      "============== Pattern 1404 ==============\n",
      "============== Pattern 1405 ==============\n",
      "============== Pattern 1406 ==============\n",
      "============== Pattern 1407 ==============\n",
      "============== Pattern 1408 ==============\n",
      "============== Pattern 1409 ==============\n",
      "============== Pattern 1410 ==============\n",
      "============== Pattern 1411 ==============\n",
      "============== Pattern 1412 ==============\n",
      "============== Pattern 1413 ==============\n",
      "============== Pattern 1414 ==============\n",
      "============== Pattern 1415 ==============\n",
      "============== Pattern 1416 ==============\n",
      "============== Pattern 1417 ==============\n",
      "============== Pattern 1418 ==============\n",
      "============== Pattern 1419 ==============\n",
      "============== Pattern 1420 ==============\n",
      "============== Pattern 1421 ==============\n",
      "============== Pattern 1422 ==============\n",
      "============== Pattern 1423 ==============\n",
      "============== Pattern 1424 ==============\n",
      "============== Pattern 1425 ==============\n",
      "============== Pattern 1426 ==============\n",
      "============== Pattern 1427 ==============\n",
      "============== Pattern 1428 ==============\n",
      "============== Pattern 1429 ==============\n",
      "============== Pattern 1430 ==============\n",
      "============== Pattern 1431 ==============\n",
      "============== Pattern 1432 ==============\n",
      "============== Pattern 1433 ==============\n",
      "============== Pattern 1434 ==============\n",
      "============== Pattern 1435 ==============\n",
      "============== Pattern 1436 ==============\n",
      "============== Pattern 1437 ==============\n",
      "============== Pattern 1438 ==============\n",
      "============== Pattern 1439 ==============\n",
      "============== Pattern 1440 ==============\n",
      "============== Pattern 1441 ==============\n",
      "============== Pattern 1442 ==============\n",
      "============== Pattern 1443 ==============\n",
      "============== Pattern 1444 ==============\n",
      "============== Pattern 1445 ==============\n",
      "============== Pattern 1446 ==============\n",
      "============== Pattern 1447 ==============\n",
      "============== Pattern 1448 ==============\n",
      "============== Pattern 1449 ==============\n",
      "============== Pattern 1450 ==============\n",
      "============== Pattern 1451 ==============\n",
      "============== Pattern 1452 ==============\n",
      "============== Pattern 1453 ==============\n",
      "============== Pattern 1454 ==============\n",
      "============== Pattern 1455 ==============\n",
      "============== Pattern 1456 ==============\n",
      "============== Pattern 1457 ==============\n",
      "============== Pattern 1458 ==============\n",
      "============== Pattern 1459 ==============\n",
      "============== Pattern 1460 ==============\n",
      "============== Pattern 1461 ==============\n",
      "============== Pattern 1462 ==============\n",
      "============== Pattern 1463 ==============\n",
      "============== Pattern 1464 ==============\n",
      "============== Pattern 1465 ==============\n",
      "============== Pattern 1466 ==============\n",
      "============== Pattern 1467 ==============\n",
      "============== Pattern 1468 ==============\n",
      "============== Pattern 1469 ==============\n",
      "============== Pattern 1470 ==============\n",
      "============== Pattern 1471 ==============\n",
      "============== Pattern 1472 ==============\n",
      "============== Pattern 1473 ==============\n",
      "============== Pattern 1474 ==============\n",
      "============== Pattern 1475 ==============\n",
      "============== Pattern 1476 ==============\n",
      "============== Pattern 1477 ==============\n",
      "============== Pattern 1478 ==============\n",
      "============== Pattern 1479 ==============\n",
      "============== Pattern 1480 ==============\n",
      "============== Pattern 1481 ==============\n",
      "============== Pattern 1482 ==============\n",
      "============== Pattern 1483 ==============\n",
      "============== Pattern 1484 ==============\n",
      "============== Pattern 1485 ==============\n",
      "============== Pattern 1486 ==============\n",
      "============== Pattern 1487 ==============\n",
      "============== Pattern 1488 ==============\n",
      "============== Pattern 1489 ==============\n",
      "============== Pattern 1490 ==============\n",
      "============== Pattern 1491 ==============\n",
      "============== Pattern 1492 ==============\n",
      "============== Pattern 1493 ==============\n",
      "============== Pattern 1494 ==============\n",
      "============== Pattern 1495 ==============\n",
      "============== Pattern 1496 ==============\n",
      "============== Pattern 1497 ==============\n",
      "============== Pattern 1498 ==============\n",
      "============== Pattern 1499 ==============\n",
      "============== Pattern 1500 ==============\n",
      "============== Pattern 1501 ==============\n",
      "============== Pattern 1502 ==============\n",
      "============== Pattern 1503 ==============\n",
      "============== Pattern 1504 ==============\n",
      "============== Pattern 1505 ==============\n",
      "============== Pattern 1506 ==============\n",
      "============== Pattern 1507 ==============\n",
      "============== Pattern 1508 ==============\n",
      "============== Pattern 1509 ==============\n",
      "============== Pattern 1510 ==============\n",
      "============== Pattern 1511 ==============\n",
      "============== Pattern 1512 ==============\n",
      "============== Pattern 1513 ==============\n",
      "============== Pattern 1514 ==============\n",
      "============== Pattern 1515 ==============\n",
      "============== Pattern 1516 ==============\n",
      "============== Pattern 1517 ==============\n",
      "============== Pattern 1518 ==============\n",
      "============== Pattern 1519 ==============\n",
      "============== Pattern 1520 ==============\n",
      "============== Pattern 1521 ==============\n",
      "============== Pattern 1522 ==============\n",
      "============== Pattern 1523 ==============\n",
      "============== Pattern 1524 ==============\n",
      "============== Pattern 1525 ==============\n",
      "============== Pattern 1526 ==============\n",
      "============== Pattern 1527 ==============\n",
      "============== Pattern 1528 ==============\n",
      "============== Pattern 1529 ==============\n",
      "============== Pattern 1530 ==============\n",
      "============== Pattern 1531 ==============\n",
      "============== Pattern 1532 ==============\n",
      "============== Pattern 1533 ==============\n",
      "============== Pattern 1534 ==============\n",
      "============== Pattern 1535 ==============\n",
      "============== Pattern 1536 ==============\n",
      "============== Pattern 1537 ==============\n",
      "============== Pattern 1538 ==============\n",
      "============== Pattern 1539 ==============\n",
      "============== Pattern 1540 ==============\n",
      "============== Pattern 1541 ==============\n",
      "============== Pattern 1542 ==============\n",
      "============== Pattern 1543 ==============\n",
      "============== Pattern 1544 ==============\n",
      "============== Pattern 1545 ==============\n",
      "============== Pattern 1546 ==============\n",
      "============== Pattern 1547 ==============\n",
      "============== Pattern 1548 ==============\n",
      "============== Pattern 1549 ==============\n",
      "============== Pattern 1550 ==============\n",
      "============== Pattern 1551 ==============\n",
      "============== Pattern 1552 ==============\n",
      "============== Pattern 1553 ==============\n",
      "============== Pattern 1554 ==============\n",
      "============== Pattern 1555 ==============\n",
      "============== Pattern 1556 ==============\n",
      "============== Pattern 1557 ==============\n",
      "============== Pattern 1558 ==============\n",
      "============== Pattern 1559 ==============\n",
      "============== Pattern 1560 ==============\n",
      "============== Pattern 1561 ==============\n",
      "============== Pattern 1562 ==============\n",
      "============== Pattern 1563 ==============\n",
      "============== Pattern 1564 ==============\n",
      "============== Pattern 1565 ==============\n",
      "============== Pattern 1566 ==============\n",
      "============== Pattern 1567 ==============\n",
      "============== Pattern 1568 ==============\n",
      "============== Pattern 1569 ==============\n",
      "============== Pattern 1570 ==============\n",
      "============== Pattern 1571 ==============\n",
      "============== Pattern 1572 ==============\n",
      "============== Pattern 1573 ==============\n",
      "============== Pattern 1574 ==============\n",
      "============== Pattern 1575 ==============\n",
      "============== Pattern 1576 ==============\n",
      "============== Pattern 1577 ==============\n",
      "============== Pattern 1578 ==============\n",
      "============== Pattern 1579 ==============\n",
      "============== Pattern 1580 ==============\n",
      "============== Pattern 1581 ==============\n",
      "============== Pattern 1582 ==============\n",
      "============== Pattern 1583 ==============\n",
      "============== Pattern 1584 ==============\n",
      "============== Pattern 1585 ==============\n",
      "============== Pattern 1586 ==============\n",
      "============== Pattern 1587 ==============\n",
      "============== Pattern 1588 ==============\n",
      "============== Pattern 1589 ==============\n",
      "============== Pattern 1590 ==============\n",
      "============== Pattern 1591 ==============\n",
      "============== Pattern 1592 ==============\n",
      "============== Pattern 1593 ==============\n",
      "============== Pattern 1594 ==============\n",
      "============== Pattern 1595 ==============\n",
      "============== Pattern 1596 ==============\n",
      "============== Pattern 1597 ==============\n",
      "============== Pattern 1598 ==============\n",
      "============== Pattern 1599 ==============\n",
      "============== Pattern 1600 ==============\n",
      "============== Pattern 1601 ==============\n",
      "============== Pattern 1602 ==============\n",
      "============== Pattern 1603 ==============\n",
      "============== Pattern 1604 ==============\n",
      "============== Pattern 1605 ==============\n",
      "============== Pattern 1606 ==============\n",
      "============== Pattern 1607 ==============\n",
      "============== Pattern 1608 ==============\n",
      "============== Pattern 1609 ==============\n",
      "============== Pattern 1610 ==============\n",
      "============== Pattern 1611 ==============\n",
      "============== Pattern 1612 ==============\n",
      "============== Pattern 1613 ==============\n",
      "============== Pattern 1614 ==============\n",
      "============== Pattern 1615 ==============\n",
      "============== Pattern 1616 ==============\n",
      "============== Pattern 1617 ==============\n",
      "============== Pattern 1618 ==============\n",
      "============== Pattern 1619 ==============\n",
      "============== Pattern 1620 ==============\n",
      "============== Pattern 1621 ==============\n",
      "============== Pattern 1622 ==============\n",
      "============== Pattern 1623 ==============\n",
      "============== Pattern 1624 ==============\n",
      "============== Pattern 1625 ==============\n",
      "============== Pattern 1626 ==============\n",
      "============== Pattern 1627 ==============\n",
      "============== Pattern 1628 ==============\n",
      "============== Pattern 1629 ==============\n",
      "============== Pattern 1630 ==============\n",
      "============== Pattern 1631 ==============\n",
      "============== Pattern 1632 ==============\n",
      "============== Pattern 1633 ==============\n",
      "============== Pattern 1634 ==============\n",
      "============== Pattern 1635 ==============\n",
      "============== Pattern 1636 ==============\n",
      "============== Pattern 1637 ==============\n",
      "============== Pattern 1638 ==============\n",
      "============== Pattern 1639 ==============\n",
      "============== Pattern 1640 ==============\n",
      "============== Pattern 1641 ==============\n",
      "============== Pattern 1642 ==============\n",
      "============== Pattern 1643 ==============\n",
      "============== Pattern 1644 ==============\n",
      "============== Pattern 1645 ==============\n",
      "============== Pattern 1646 ==============\n",
      "============== Pattern 1647 ==============\n",
      "============== Pattern 1648 ==============\n",
      "============== Pattern 1649 ==============\n",
      "============== Pattern 1650 ==============\n",
      "============== Pattern 1651 ==============\n",
      "============== Pattern 1652 ==============\n",
      "============== Pattern 1653 ==============\n",
      "============== Pattern 1654 ==============\n",
      "============== Pattern 1655 ==============\n",
      "============== Pattern 1656 ==============\n",
      "============== Pattern 1657 ==============\n",
      "============== Pattern 1658 ==============\n",
      "============== Pattern 1659 ==============\n",
      "============== Pattern 1660 ==============\n",
      "============== Pattern 1661 ==============\n",
      "============== Pattern 1662 ==============\n",
      "============== Pattern 1663 ==============\n",
      "============== Pattern 1664 ==============\n",
      "============== Pattern 1665 ==============\n",
      "============== Pattern 1666 ==============\n",
      "============== Pattern 1667 ==============\n",
      "============== Pattern 1668 ==============\n",
      "============== Pattern 1669 ==============\n",
      "============== Pattern 1670 ==============\n",
      "============== Pattern 1671 ==============\n",
      "============== Pattern 1672 ==============\n",
      "============== Pattern 1673 ==============\n",
      "============== Pattern 1674 ==============\n",
      "============== Pattern 1675 ==============\n",
      "============== Pattern 1676 ==============\n",
      "============== Pattern 1677 ==============\n",
      "============== Pattern 1678 ==============\n",
      "============== Pattern 1679 ==============\n",
      "============== Pattern 1680 ==============\n",
      "============== Pattern 1681 ==============\n",
      "============== Pattern 1682 ==============\n",
      "============== Pattern 1683 ==============\n",
      "============== Pattern 1684 ==============\n",
      "============== Pattern 1685 ==============\n",
      "============== Pattern 1686 ==============\n",
      "============== Pattern 1687 ==============\n",
      "============== Pattern 1688 ==============\n",
      "============== Pattern 1689 ==============\n",
      "============== Pattern 1690 ==============\n",
      "============== Pattern 1691 ==============\n",
      "============== Pattern 1692 ==============\n",
      "============== Pattern 1693 ==============\n",
      "============== Pattern 1694 ==============\n",
      "============== Pattern 1695 ==============\n",
      "============== Pattern 1696 ==============\n",
      "============== Pattern 1697 ==============\n",
      "============== Pattern 1698 ==============\n",
      "============== Pattern 1699 ==============\n",
      "============== Pattern 1700 ==============\n",
      "============== Pattern 1701 ==============\n",
      "============== Pattern 1702 ==============\n",
      "============== Pattern 1703 ==============\n",
      "============== Pattern 1704 ==============\n",
      "============== Pattern 1705 ==============\n",
      "============== Pattern 1706 ==============\n",
      "============== Pattern 1707 ==============\n",
      "============== Pattern 1708 ==============\n",
      "============== Pattern 1709 ==============\n",
      "============== Pattern 1710 ==============\n",
      "============== Pattern 1711 ==============\n",
      "============== Pattern 1712 ==============\n",
      "============== Pattern 1713 ==============\n",
      "============== Pattern 1714 ==============\n",
      "============== Pattern 1715 ==============\n",
      "============== Pattern 1716 ==============\n",
      "============== Pattern 1717 ==============\n",
      "============== Pattern 1718 ==============\n",
      "============== Pattern 1719 ==============\n",
      "============== Pattern 1720 ==============\n",
      "============== Pattern 1721 ==============\n",
      "============== Pattern 1722 ==============\n",
      "============== Pattern 1723 ==============\n",
      "============== Pattern 1724 ==============\n",
      "============== Pattern 1725 ==============\n",
      "============== Pattern 1726 ==============\n",
      "============== Pattern 1727 ==============\n",
      "============== Pattern 1728 ==============\n",
      "============== Pattern 1729 ==============\n",
      "============== Pattern 1730 ==============\n",
      "============== Pattern 1731 ==============\n",
      "============== Pattern 1732 ==============\n",
      "============== Pattern 1733 ==============\n",
      "============== Pattern 1734 ==============\n",
      "============== Pattern 1735 ==============\n",
      "============== Pattern 1736 ==============\n",
      "============== Pattern 1737 ==============\n",
      "============== Pattern 1738 ==============\n",
      "============== Pattern 1739 ==============\n",
      "============== Pattern 1740 ==============\n",
      "============== Pattern 1741 ==============\n",
      "============== Pattern 1742 ==============\n",
      "============== Pattern 1743 ==============\n",
      "============== Pattern 1744 ==============\n",
      "============== Pattern 1745 ==============\n",
      "============== Pattern 1746 ==============\n",
      "============== Pattern 1747 ==============\n",
      "============== Pattern 1748 ==============\n",
      "============== Pattern 1749 ==============\n",
      "============== Pattern 1750 ==============\n",
      "============== Pattern 1751 ==============\n",
      "============== Pattern 1752 ==============\n",
      "============== Pattern 1753 ==============\n",
      "============== Pattern 1754 ==============\n",
      "============== Pattern 1755 ==============\n",
      "============== Pattern 1756 ==============\n",
      "============== Pattern 1757 ==============\n",
      "============== Pattern 1758 ==============\n",
      "============== Pattern 1759 ==============\n",
      "============== Pattern 1760 ==============\n",
      "============== Pattern 1761 ==============\n",
      "============== Pattern 1762 ==============\n",
      "============== Pattern 1763 ==============\n",
      "============== Pattern 1764 ==============\n",
      "============== Pattern 1765 ==============\n",
      "============== Pattern 1766 ==============\n",
      "============== Pattern 1767 ==============\n",
      "============== Pattern 1768 ==============\n",
      "============== Pattern 1769 ==============\n",
      "============== Pattern 1770 ==============\n",
      "============== Pattern 1771 ==============\n",
      "============== Pattern 1772 ==============\n",
      "============== Pattern 1773 ==============\n",
      "============== Pattern 1774 ==============\n",
      "============== Pattern 1775 ==============\n",
      "============== Pattern 1776 ==============\n",
      "============== Pattern 1777 ==============\n",
      "============== Pattern 1778 ==============\n",
      "============== Pattern 1779 ==============\n",
      "============== Pattern 1780 ==============\n",
      "============== Pattern 1781 ==============\n",
      "============== Pattern 1782 ==============\n",
      "============== Pattern 1783 ==============\n",
      "============== Pattern 1784 ==============\n",
      "============== Pattern 1785 ==============\n",
      "============== Pattern 1786 ==============\n",
      "============== Pattern 1787 ==============\n",
      "============== Pattern 1788 ==============\n",
      "============== Pattern 1789 ==============\n",
      "============== Pattern 1790 ==============\n",
      "============== Pattern 1791 ==============\n",
      "============== Pattern 1792 ==============\n",
      "============== Pattern 1793 ==============\n",
      "============== Pattern 1794 ==============\n",
      "============== Pattern 1795 ==============\n",
      "============== Pattern 1796 ==============\n",
      "============== Pattern 1797 ==============\n",
      "============== Pattern 1798 ==============\n",
      "============== Pattern 1799 ==============\n",
      "============== Pattern 1800 ==============\n",
      "============== Pattern 1801 ==============\n",
      "============== Pattern 1802 ==============\n",
      "============== Pattern 1803 ==============\n",
      "============== Pattern 1804 ==============\n",
      "============== Pattern 1805 ==============\n",
      "============== Pattern 1806 ==============\n",
      "============== Pattern 1807 ==============\n",
      "============== Pattern 1808 ==============\n",
      "============== Pattern 1809 ==============\n",
      "============== Pattern 1810 ==============\n",
      "============== Pattern 1811 ==============\n",
      "============== Pattern 1812 ==============\n",
      "============== Pattern 1813 ==============\n",
      "============== Pattern 1814 ==============\n",
      "============== Pattern 1815 ==============\n",
      "============== Pattern 1816 ==============\n",
      "============== Pattern 1817 ==============\n",
      "============== Pattern 1818 ==============\n",
      "============== Pattern 1819 ==============\n",
      "============== Pattern 1820 ==============\n",
      "============== Pattern 1821 ==============\n",
      "============== Pattern 1822 ==============\n",
      "============== Pattern 1823 ==============\n",
      "============== Pattern 1824 ==============\n",
      "============== Pattern 1825 ==============\n",
      "============== Pattern 1826 ==============\n",
      "============== Pattern 1827 ==============\n",
      "============== Pattern 1828 ==============\n",
      "============== Pattern 1829 ==============\n",
      "============== Pattern 1830 ==============\n",
      "============== Pattern 1831 ==============\n",
      "============== Pattern 1832 ==============\n",
      "============== Pattern 1833 ==============\n",
      "============== Pattern 1834 ==============\n",
      "============== Pattern 1835 ==============\n",
      "============== Pattern 1836 ==============\n",
      "============== Pattern 1837 ==============\n",
      "============== Pattern 1838 ==============\n",
      "============== Pattern 1839 ==============\n",
      "============== Pattern 1840 ==============\n",
      "============== Pattern 1841 ==============\n",
      "============== Pattern 1842 ==============\n",
      "============== Pattern 1843 ==============\n",
      "============== Pattern 1844 ==============\n",
      "============== Pattern 1845 ==============\n",
      "============== Pattern 1846 ==============\n",
      "============== Pattern 1847 ==============\n",
      "============== Pattern 1848 ==============\n",
      "============== Pattern 1849 ==============\n",
      "============== Pattern 1850 ==============\n",
      "============== Pattern 1851 ==============\n",
      "============== Pattern 1852 ==============\n",
      "============== Pattern 1853 ==============\n",
      "============== Pattern 1854 ==============\n",
      "============== Pattern 1855 ==============\n",
      "============== Pattern 1856 ==============\n",
      "============== Pattern 1857 ==============\n",
      "============== Pattern 1858 ==============\n",
      "============== Pattern 1859 ==============\n",
      "============== Pattern 1860 ==============\n",
      "============== Pattern 1861 ==============\n",
      "============== Pattern 1862 ==============\n",
      "============== Pattern 1863 ==============\n",
      "============== Pattern 1864 ==============\n",
      "============== Pattern 1865 ==============\n",
      "============== Pattern 1866 ==============\n",
      "============== Pattern 1867 ==============\n",
      "============== Pattern 1868 ==============\n",
      "============== Pattern 1869 ==============\n",
      "============== Pattern 1870 ==============\n",
      "============== Pattern 1871 ==============\n",
      "============== Pattern 1872 ==============\n",
      "============== Pattern 1873 ==============\n",
      "============== Pattern 1874 ==============\n",
      "============== Pattern 1875 ==============\n",
      "============== Pattern 1876 ==============\n",
      "============== Pattern 1877 ==============\n",
      "============== Pattern 1878 ==============\n",
      "============== Pattern 1879 ==============\n",
      "============== Pattern 1880 ==============\n",
      "============== Pattern 1881 ==============\n",
      "============== Pattern 1882 ==============\n",
      "============== Pattern 1883 ==============\n",
      "============== Pattern 1884 ==============\n",
      "============== Pattern 1885 ==============\n",
      "============== Pattern 1886 ==============\n",
      "============== Pattern 1887 ==============\n",
      "============== Pattern 1888 ==============\n",
      "============== Pattern 1889 ==============\n",
      "============== Pattern 1890 ==============\n",
      "============== Pattern 1891 ==============\n",
      "============== Pattern 1892 ==============\n",
      "============== Pattern 1893 ==============\n",
      "============== Pattern 1894 ==============\n",
      "============== Pattern 1895 ==============\n",
      "============== Pattern 1896 ==============\n",
      "============== Pattern 1897 ==============\n",
      "============== Pattern 1898 ==============\n",
      "============== Pattern 1899 ==============\n",
      "============== Pattern 1900 ==============\n",
      "============== Pattern 1901 ==============\n",
      "============== Pattern 1902 ==============\n",
      "============== Pattern 1903 ==============\n",
      "============== Pattern 1904 ==============\n",
      "============== Pattern 1905 ==============\n",
      "============== Pattern 1906 ==============\n",
      "============== Pattern 1907 ==============\n",
      "============== Pattern 1908 ==============\n",
      "============== Pattern 1909 ==============\n",
      "============== Pattern 1910 ==============\n",
      "============== Pattern 1911 ==============\n",
      "============== Pattern 1912 ==============\n",
      "============== Pattern 1913 ==============\n",
      "============== Pattern 1914 ==============\n",
      "============== Pattern 1915 ==============\n",
      "============== Pattern 1916 ==============\n",
      "============== Pattern 1917 ==============\n",
      "============== Pattern 1918 ==============\n",
      "============== Pattern 1919 ==============\n",
      "============== Pattern 1920 ==============\n",
      "============== Pattern 1921 ==============\n",
      "============== Pattern 1922 ==============\n",
      "============== Pattern 1923 ==============\n",
      "============== Pattern 1924 ==============\n",
      "============== Pattern 1925 ==============\n",
      "============== Pattern 1926 ==============\n",
      "============== Pattern 1927 ==============\n",
      "============== Pattern 1928 ==============\n",
      "============== Pattern 1929 ==============\n",
      "============== Pattern 1930 ==============\n",
      "============== Pattern 1931 ==============\n",
      "============== Pattern 1932 ==============\n",
      "============== Pattern 1933 ==============\n",
      "============== Pattern 1934 ==============\n",
      "============== Pattern 1935 ==============\n",
      "============== Pattern 1936 ==============\n",
      "============== Pattern 1937 ==============\n",
      "============== Pattern 1938 ==============\n",
      "============== Pattern 1939 ==============\n",
      "============== Pattern 1940 ==============\n",
      "============== Pattern 1941 ==============\n",
      "============== Pattern 1942 ==============\n",
      "============== Pattern 1943 ==============\n",
      "============== Pattern 1944 ==============\n",
      "============== Pattern 1945 ==============\n",
      "============== Pattern 1946 ==============\n",
      "============== Pattern 1947 ==============\n",
      "============== Pattern 1948 ==============\n",
      "============== Pattern 1949 ==============\n",
      "============== Pattern 1950 ==============\n",
      "============== Pattern 1951 ==============\n",
      "============== Pattern 1952 ==============\n",
      "============== Pattern 1953 ==============\n",
      "============== Pattern 1954 ==============\n",
      "============== Pattern 1955 ==============\n",
      "============== Pattern 1956 ==============\n",
      "============== Pattern 1957 ==============\n",
      "============== Pattern 1958 ==============\n",
      "============== Pattern 1959 ==============\n",
      "============== Pattern 1960 ==============\n",
      "============== Pattern 1961 ==============\n",
      "============== Pattern 1962 ==============\n",
      "============== Pattern 1963 ==============\n",
      "============== Pattern 1964 ==============\n",
      "============== Pattern 1965 ==============\n",
      "============== Pattern 1966 ==============\n",
      "============== Pattern 1967 ==============\n",
      "============== Pattern 1968 ==============\n",
      "============== Pattern 1969 ==============\n",
      "============== Pattern 1970 ==============\n",
      "============== Pattern 1971 ==============\n",
      "============== Pattern 1972 ==============\n",
      "============== Pattern 1973 ==============\n",
      "============== Pattern 1974 ==============\n",
      "============== Pattern 1975 ==============\n",
      "============== Pattern 1976 ==============\n",
      "============== Pattern 1977 ==============\n",
      "============== Pattern 1978 ==============\n",
      "============== Pattern 1979 ==============\n",
      "============== Pattern 1980 ==============\n",
      "============== Pattern 1981 ==============\n",
      "============== Pattern 1982 ==============\n",
      "============== Pattern 1983 ==============\n",
      "============== Pattern 1984 ==============\n",
      "============== Pattern 1985 ==============\n",
      "============== Pattern 1986 ==============\n",
      "============== Pattern 1987 ==============\n",
      "============== Pattern 1988 ==============\n",
      "============== Pattern 1989 ==============\n",
      "============== Pattern 1990 ==============\n",
      "============== Pattern 1991 ==============\n",
      "============== Pattern 1992 ==============\n",
      "============== Pattern 1993 ==============\n",
      "============== Pattern 1994 ==============\n",
      "============== Pattern 1995 ==============\n",
      "============== Pattern 1996 ==============\n",
      "============== Pattern 1997 ==============\n",
      "============== Pattern 1998 ==============\n",
      "============== Pattern 1999 ==============\n",
      "============== Pattern 2000 ==============\n",
      "============== Pattern 2001 ==============\n",
      "============== Pattern 2002 ==============\n",
      "============== Pattern 2003 ==============\n",
      "============== Pattern 2004 ==============\n",
      "============== Pattern 2005 ==============\n",
      "============== Pattern 2006 ==============\n",
      "============== Pattern 2007 ==============\n",
      "============== Pattern 2008 ==============\n",
      "============== Pattern 2009 ==============\n",
      "============== Pattern 2010 ==============\n",
      "============== Pattern 2011 ==============\n",
      "============== Pattern 2012 ==============\n",
      "============== Pattern 2013 ==============\n",
      "============== Pattern 2014 ==============\n",
      "============== Pattern 2015 ==============\n",
      "============== Pattern 2016 ==============\n",
      "============== Pattern 2017 ==============\n",
      "============== Pattern 2018 ==============\n",
      "============== Pattern 2019 ==============\n",
      "============== Pattern 2020 ==============\n",
      "============== Pattern 2021 ==============\n",
      "============== Pattern 2022 ==============\n",
      "============== Pattern 2023 ==============\n",
      "============== Pattern 2024 ==============\n",
      "============== Pattern 2025 ==============\n",
      "============== Pattern 2026 ==============\n",
      "============== Pattern 2027 ==============\n",
      "============== Pattern 2028 ==============\n",
      "============== Pattern 2029 ==============\n",
      "============== Pattern 2030 ==============\n",
      "============== Pattern 2031 ==============\n",
      "============== Pattern 2032 ==============\n",
      "============== Pattern 2033 ==============\n",
      "============== Pattern 2034 ==============\n",
      "============== Pattern 2035 ==============\n",
      "============== Pattern 2036 ==============\n",
      "============== Pattern 2037 ==============\n",
      "============== Pattern 2038 ==============\n",
      "============== Pattern 2039 ==============\n",
      "============== Pattern 2040 ==============\n",
      "============== Pattern 2041 ==============\n",
      "============== Pattern 2042 ==============\n",
      "============== Pattern 2043 ==============\n",
      "============== Pattern 2044 ==============\n",
      "============== Pattern 2045 ==============\n",
      "============== Pattern 2046 ==============\n",
      "============== Pattern 2047 ==============\n",
      "============== Pattern 2048 ==============\n",
      "============== Pattern 2049 ==============\n",
      "============== Pattern 2050 ==============\n",
      "============== Pattern 2051 ==============\n",
      "============== Pattern 2052 ==============\n",
      "============== Pattern 2053 ==============\n",
      "============== Pattern 2054 ==============\n",
      "============== Pattern 2055 ==============\n",
      "============== Pattern 2056 ==============\n",
      "============== Pattern 2057 ==============\n",
      "============== Pattern 2058 ==============\n",
      "============== Pattern 2059 ==============\n",
      "============== Pattern 2060 ==============\n",
      "============== Pattern 2061 ==============\n",
      "============== Pattern 2062 ==============\n",
      "============== Pattern 2063 ==============\n",
      "============== Pattern 2064 ==============\n",
      "============== Pattern 2065 ==============\n",
      "============== Pattern 2066 ==============\n",
      "============== Pattern 2067 ==============\n",
      "============== Pattern 2068 ==============\n",
      "============== Pattern 2069 ==============\n",
      "============== Pattern 2070 ==============\n",
      "============== Pattern 2071 ==============\n",
      "============== Pattern 2072 ==============\n",
      "============== Pattern 2073 ==============\n",
      "============== Pattern 2074 ==============\n",
      "============== Pattern 2075 ==============\n",
      "============== Pattern 2076 ==============\n",
      "============== Pattern 2077 ==============\n",
      "============== Pattern 2078 ==============\n",
      "============== Pattern 2079 ==============\n",
      "============== Pattern 2080 ==============\n",
      "============== Pattern 2081 ==============\n",
      "============== Pattern 2082 ==============\n",
      "============== Pattern 2083 ==============\n",
      "============== Pattern 2084 ==============\n",
      "============== Pattern 2085 ==============\n",
      "============== Pattern 2086 ==============\n",
      "============== Pattern 2087 ==============\n",
      "============== Pattern 2088 ==============\n",
      "============== Pattern 2089 ==============\n",
      "============== Pattern 2090 ==============\n",
      "============== Pattern 2091 ==============\n",
      "============== Pattern 2092 ==============\n",
      "============== Pattern 2093 ==============\n",
      "============== Pattern 2094 ==============\n",
      "============== Pattern 2095 ==============\n",
      "============== Pattern 2096 ==============\n",
      "============== Pattern 2097 ==============\n",
      "============== Pattern 2098 ==============\n",
      "============== Pattern 2099 ==============\n",
      "============== Pattern 2100 ==============\n",
      "============== Pattern 2101 ==============\n",
      "============== Pattern 2102 ==============\n",
      "============== Pattern 2103 ==============\n",
      "============== Pattern 2104 ==============\n",
      "============== Pattern 2105 ==============\n",
      "============== Pattern 2106 ==============\n",
      "============== Pattern 2107 ==============\n",
      "============== Pattern 2108 ==============\n",
      "============== Pattern 2109 ==============\n",
      "============== Pattern 2110 ==============\n",
      "============== Pattern 2111 ==============\n",
      "============== Pattern 2112 ==============\n",
      "============== Pattern 2113 ==============\n",
      "============== Pattern 2114 ==============\n",
      "============== Pattern 2115 ==============\n",
      "============== Pattern 2116 ==============\n",
      "============== Pattern 2117 ==============\n",
      "============== Pattern 2118 ==============\n",
      "============== Pattern 2119 ==============\n",
      "============== Pattern 2120 ==============\n",
      "============== Pattern 2121 ==============\n",
      "============== Pattern 2122 ==============\n",
      "============== Pattern 2123 ==============\n",
      "============== Pattern 2124 ==============\n",
      "============== Pattern 2125 ==============\n",
      "============== Pattern 2126 ==============\n",
      "============== Pattern 2127 ==============\n",
      "============== Pattern 2128 ==============\n",
      "============== Pattern 2129 ==============\n",
      "============== Pattern 2130 ==============\n",
      "============== Pattern 2131 ==============\n",
      "============== Pattern 2132 ==============\n",
      "============== Pattern 2133 ==============\n",
      "============== Pattern 2134 ==============\n",
      "============== Pattern 2135 ==============\n",
      "============== Pattern 2136 ==============\n",
      "============== Pattern 2137 ==============\n",
      "============== Pattern 2138 ==============\n",
      "============== Pattern 2139 ==============\n",
      "============== Pattern 2140 ==============\n",
      "============== Pattern 2141 ==============\n",
      "============== Pattern 2142 ==============\n",
      "============== Pattern 2143 ==============\n",
      "============== Pattern 2144 ==============\n",
      "============== Pattern 2145 ==============\n",
      "============== Pattern 2146 ==============\n",
      "============== Pattern 2147 ==============\n",
      "============== Pattern 2148 ==============\n",
      "============== Pattern 2149 ==============\n",
      "============== Pattern 2150 ==============\n",
      "============== Pattern 2151 ==============\n",
      "============== Pattern 2152 ==============\n",
      "============== Pattern 2153 ==============\n",
      "============== Pattern 2154 ==============\n",
      "============== Pattern 2155 ==============\n",
      "============== Pattern 2156 ==============\n",
      "============== Pattern 2157 ==============\n",
      "============== Pattern 2158 ==============\n",
      "============== Pattern 2159 ==============\n",
      "============== Pattern 2160 ==============\n",
      "============== Pattern 2161 ==============\n",
      "============== Pattern 2162 ==============\n",
      "============== Pattern 2163 ==============\n",
      "============== Pattern 2164 ==============\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Pattern 2165 ==============\n",
      "============== Pattern 2166 ==============\n",
      "============== Pattern 2167 ==============\n",
      "============== Pattern 2168 ==============\n",
      "============== Pattern 2169 ==============\n",
      "============== Pattern 2170 ==============\n",
      "============== Pattern 2171 ==============\n",
      "============== Pattern 2172 ==============\n",
      "============== Pattern 2173 ==============\n",
      "============== Pattern 2174 ==============\n",
      "============== Pattern 2175 ==============\n",
      "============== Pattern 2176 ==============\n",
      "============== Pattern 2177 ==============\n",
      "============== Pattern 2178 ==============\n",
      "============== Pattern 2179 ==============\n",
      "============== Pattern 2180 ==============\n",
      "============== Pattern 2181 ==============\n",
      "============== Pattern 2182 ==============\n",
      "============== Pattern 2183 ==============\n",
      "============== Pattern 2184 ==============\n",
      "============== Pattern 2185 ==============\n",
      "============== Pattern 2186 ==============\n",
      "============== Pattern 2187 ==============\n",
      "============== Pattern 2188 ==============\n",
      "============== Pattern 2189 ==============\n",
      "============== Pattern 2190 ==============\n",
      "============== Pattern 2191 ==============\n",
      "============== Pattern 2192 ==============\n",
      "============== Pattern 2193 ==============\n",
      "============== Pattern 2194 ==============\n",
      "============== Pattern 2195 ==============\n",
      "============== Pattern 2196 ==============\n",
      "============== Pattern 2197 ==============\n",
      "============== Pattern 2198 ==============\n",
      "============== Pattern 2199 ==============\n",
      "============== Pattern 2200 ==============\n",
      "============== Pattern 2201 ==============\n",
      "============== Pattern 2202 ==============\n",
      "============== Pattern 2203 ==============\n",
      "============== Pattern 2204 ==============\n",
      "============== Pattern 2205 ==============\n",
      "============== Pattern 2206 ==============\n",
      "============== Pattern 2207 ==============\n",
      "============== Pattern 2208 ==============\n",
      "============== Pattern 2209 ==============\n",
      "============== Pattern 2210 ==============\n",
      "============== Pattern 2211 ==============\n",
      "============== Pattern 2212 ==============\n",
      "============== Pattern 2213 ==============\n",
      "============== Pattern 2214 ==============\n",
      "============== Pattern 2215 ==============\n",
      "============== Pattern 2216 ==============\n",
      "============== Pattern 2217 ==============\n",
      "============== Pattern 2218 ==============\n",
      "============== Pattern 2219 ==============\n",
      "============== Pattern 2220 ==============\n",
      "============== Pattern 2221 ==============\n",
      "============== Pattern 2222 ==============\n",
      "============== Pattern 2223 ==============\n",
      "============== Pattern 2224 ==============\n",
      "============== Pattern 2225 ==============\n",
      "============== Pattern 2226 ==============\n",
      "============== Pattern 2227 ==============\n",
      "============== Pattern 2228 ==============\n",
      "============== Pattern 2229 ==============\n",
      "============== Pattern 2230 ==============\n",
      "============== Pattern 2231 ==============\n",
      "============== Pattern 2232 ==============\n",
      "============== Pattern 2233 ==============\n",
      "============== Pattern 2234 ==============\n",
      "============== Pattern 2235 ==============\n",
      "============== Pattern 2236 ==============\n",
      "============== Pattern 2237 ==============\n",
      "============== Pattern 2238 ==============\n",
      "============== Pattern 2239 ==============\n",
      "============== Pattern 2240 ==============\n",
      "============== Pattern 2241 ==============\n",
      "============== Pattern 2242 ==============\n",
      "============== Pattern 2243 ==============\n",
      "============== Pattern 2244 ==============\n",
      "============== Pattern 2245 ==============\n",
      "============== Pattern 2246 ==============\n",
      "============== Pattern 2247 ==============\n",
      "============== Pattern 2248 ==============\n",
      "============== Pattern 2249 ==============\n",
      "============== Pattern 2250 ==============\n",
      "============== Pattern 2251 ==============\n",
      "============== Pattern 2252 ==============\n",
      "============== Pattern 2253 ==============\n",
      "============== Pattern 2254 ==============\n",
      "============== Pattern 2255 ==============\n",
      "============== Pattern 2256 ==============\n",
      "============== Pattern 2257 ==============\n",
      "============== Pattern 2258 ==============\n",
      "============== Pattern 2259 ==============\n",
      "============== Pattern 2260 ==============\n",
      "============== Pattern 2261 ==============\n",
      "============== Pattern 2262 ==============\n",
      "============== Pattern 2263 ==============\n",
      "============== Pattern 2264 ==============\n",
      "============== Pattern 2265 ==============\n",
      "============== Pattern 2266 ==============\n",
      "============== Pattern 2267 ==============\n",
      "============== Pattern 2268 ==============\n",
      "============== Pattern 2269 ==============\n",
      "============== Pattern 2270 ==============\n",
      "============== Pattern 2271 ==============\n",
      "============== Pattern 2272 ==============\n",
      "============== Pattern 2273 ==============\n",
      "============== Pattern 2274 ==============\n",
      "============== Pattern 2275 ==============\n",
      "============== Pattern 2276 ==============\n",
      "============== Pattern 2277 ==============\n",
      "============== Pattern 2278 ==============\n",
      "============== Pattern 2279 ==============\n",
      "============== Pattern 2280 ==============\n",
      "============== Pattern 2281 ==============\n",
      "============== Pattern 2282 ==============\n",
      "============== Pattern 2283 ==============\n",
      "============== Pattern 2284 ==============\n",
      "============== Pattern 2285 ==============\n",
      "============== Pattern 2286 ==============\n",
      "============== Pattern 2287 ==============\n",
      "============== Pattern 2288 ==============\n",
      "============== Pattern 2289 ==============\n",
      "============== Pattern 2290 ==============\n",
      "============== Pattern 2291 ==============\n",
      "============== Pattern 2292 ==============\n",
      "============== Pattern 2293 ==============\n",
      "============== Pattern 2294 ==============\n",
      "============== Pattern 2295 ==============\n",
      "============== Pattern 2296 ==============\n",
      "============== Pattern 2297 ==============\n",
      "============== Pattern 2298 ==============\n",
      "============== Pattern 2299 ==============\n",
      "============== Pattern 2300 ==============\n",
      "============== Pattern 2301 ==============\n",
      "============== Pattern 2302 ==============\n",
      "============== Pattern 2303 ==============\n",
      "============== Pattern 2304 ==============\n",
      "============== Pattern 2305 ==============\n",
      "============== Pattern 2306 ==============\n",
      "============== Pattern 2307 ==============\n",
      "============== Pattern 2308 ==============\n",
      "============== Pattern 2309 ==============\n",
      "============== Pattern 2310 ==============\n",
      "============== Pattern 2311 ==============\n",
      "============== Pattern 2312 ==============\n",
      "============== Pattern 2313 ==============\n",
      "============== Pattern 2314 ==============\n",
      "============== Pattern 2315 ==============\n",
      "============== Pattern 2316 ==============\n",
      "============== Pattern 2317 ==============\n",
      "============== Pattern 2318 ==============\n",
      "============== Pattern 2319 ==============\n",
      "============== Pattern 2320 ==============\n",
      "============== Pattern 2321 ==============\n",
      "============== Pattern 2322 ==============\n",
      "============== Pattern 2323 ==============\n",
      "============== Pattern 2324 ==============\n",
      "============== Pattern 2325 ==============\n",
      "============== Pattern 2326 ==============\n",
      "============== Pattern 2327 ==============\n",
      "============== Pattern 2328 ==============\n",
      "============== Pattern 2329 ==============\n",
      "============== Pattern 2330 ==============\n",
      "============== Pattern 2331 ==============\n",
      "============== Pattern 2332 ==============\n",
      "============== Pattern 2333 ==============\n",
      "============== Pattern 2334 ==============\n",
      "============== Pattern 2335 ==============\n",
      "============== Pattern 2336 ==============\n",
      "============== Pattern 2337 ==============\n",
      "============== Pattern 2338 ==============\n",
      "============== Pattern 2339 ==============\n",
      "============== Pattern 2340 ==============\n",
      "============== Pattern 2341 ==============\n",
      "============== Pattern 2342 ==============\n",
      "============== Pattern 2343 ==============\n",
      "============== Pattern 2344 ==============\n",
      "============== Pattern 2345 ==============\n",
      "============== Pattern 2346 ==============\n",
      "============== Pattern 2347 ==============\n",
      "============== Pattern 2348 ==============\n",
      "============== Pattern 2349 ==============\n",
      "============== Pattern 2350 ==============\n",
      "============== Pattern 2351 ==============\n",
      "============== Pattern 2352 ==============\n",
      "============== Pattern 2353 ==============\n",
      "============== Pattern 2354 ==============\n",
      "============== Pattern 2355 ==============\n",
      "============== Pattern 2356 ==============\n",
      "============== Pattern 2357 ==============\n",
      "============== Pattern 2358 ==============\n",
      "============== Pattern 2359 ==============\n",
      "============== Pattern 2360 ==============\n",
      "============== Pattern 2361 ==============\n",
      "============== Pattern 2362 ==============\n",
      "============== Pattern 2363 ==============\n",
      "============== Pattern 2364 ==============\n",
      "============== Pattern 2365 ==============\n",
      "============== Pattern 2366 ==============\n",
      "============== Pattern 2367 ==============\n",
      "============== Pattern 2368 ==============\n",
      "============== Pattern 2369 ==============\n",
      "============== Pattern 2370 ==============\n",
      "============== Pattern 2371 ==============\n",
      "============== Pattern 2372 ==============\n",
      "============== Pattern 2373 ==============\n",
      "============== Pattern 2374 ==============\n",
      "============== Pattern 2375 ==============\n",
      "============== Pattern 2376 ==============\n",
      "============== Pattern 2377 ==============\n",
      "============== Pattern 2378 ==============\n",
      "============== Pattern 2379 ==============\n",
      "============== Pattern 2380 ==============\n",
      "============== Pattern 2381 ==============\n",
      "============== Pattern 2382 ==============\n",
      "============== Pattern 2383 ==============\n",
      "============== Pattern 2384 ==============\n",
      "============== Pattern 2385 ==============\n",
      "============== Pattern 2386 ==============\n",
      "============== Pattern 2387 ==============\n",
      "============== Pattern 2388 ==============\n",
      "============== Pattern 2389 ==============\n",
      "============== Pattern 2390 ==============\n",
      "============== Pattern 2391 ==============\n",
      "============== Pattern 2392 ==============\n",
      "============== Pattern 2393 ==============\n",
      "============== Pattern 2394 ==============\n",
      "============== Pattern 2395 ==============\n",
      "============== Pattern 2396 ==============\n",
      "============== Pattern 2397 ==============\n",
      "============== Pattern 2398 ==============\n",
      "============== Pattern 2399 ==============\n",
      "============== Pattern 2400 ==============\n",
      "============== Pattern 2401 ==============\n",
      "============== Pattern 2402 ==============\n",
      "============== Pattern 2403 ==============\n",
      "============== Pattern 2404 ==============\n",
      "============== Pattern 2405 ==============\n",
      "============== Pattern 2406 ==============\n",
      "============== Pattern 2407 ==============\n",
      "============== Pattern 2408 ==============\n",
      "============== Pattern 2409 ==============\n",
      "============== Pattern 2410 ==============\n",
      "============== Pattern 2411 ==============\n",
      "============== Pattern 2412 ==============\n",
      "============== Pattern 2413 ==============\n",
      "============== Pattern 2414 ==============\n",
      "============== Pattern 2415 ==============\n",
      "============== Pattern 2416 ==============\n",
      "============== Pattern 2417 ==============\n",
      "============== Pattern 2418 ==============\n",
      "============== Pattern 2419 ==============\n",
      "============== Pattern 2420 ==============\n",
      "============== Pattern 2421 ==============\n",
      "============== Pattern 2422 ==============\n",
      "============== Pattern 2423 ==============\n",
      "============== Pattern 2424 ==============\n",
      "============== Pattern 2425 ==============\n",
      "============== Pattern 2426 ==============\n",
      "============== Pattern 2427 ==============\n",
      "============== Pattern 2428 ==============\n",
      "============== Pattern 2429 ==============\n",
      "============== Pattern 2430 ==============\n",
      "============== Pattern 2431 ==============\n",
      "============== Pattern 2432 ==============\n",
      "============== Pattern 2433 ==============\n",
      "============== Pattern 2434 ==============\n",
      "============== Pattern 2435 ==============\n",
      "============== Pattern 2436 ==============\n",
      "============== Pattern 2437 ==============\n",
      "============== Pattern 2438 ==============\n",
      "============== Pattern 2439 ==============\n",
      "============== Pattern 2440 ==============\n",
      "============== Pattern 2441 ==============\n",
      "============== Pattern 2442 ==============\n",
      "============== Pattern 2443 ==============\n",
      "============== Pattern 2444 ==============\n",
      "============== Pattern 2445 ==============\n",
      "============== Pattern 2446 ==============\n",
      "============== Pattern 2447 ==============\n",
      "============== Pattern 2448 ==============\n",
      "============== Pattern 2449 ==============\n",
      "============== Pattern 2450 ==============\n",
      "============== Pattern 2451 ==============\n",
      "============== Pattern 2452 ==============\n",
      "============== Pattern 2453 ==============\n",
      "============== Pattern 2454 ==============\n",
      "============== Pattern 2455 ==============\n",
      "============== Pattern 2456 ==============\n",
      "============== Pattern 2457 ==============\n",
      "============== Pattern 2458 ==============\n",
      "============== Pattern 2459 ==============\n",
      "============== Pattern 2460 ==============\n",
      "============== Pattern 2461 ==============\n",
      "============== Pattern 2462 ==============\n",
      "============== Pattern 2463 ==============\n",
      "============== Pattern 2464 ==============\n",
      "============== Pattern 2465 ==============\n",
      "============== Pattern 2466 ==============\n",
      "============== Pattern 2467 ==============\n",
      "============== Pattern 2468 ==============\n",
      "============== Pattern 2469 ==============\n",
      "============== Pattern 2470 ==============\n",
      "============== Pattern 2471 ==============\n",
      "============== Pattern 2472 ==============\n",
      "============== Pattern 2473 ==============\n",
      "============== Pattern 2474 ==============\n",
      "============== Pattern 2475 ==============\n",
      "============== Pattern 2476 ==============\n",
      "============== Pattern 2477 ==============\n",
      "============== Pattern 2478 ==============\n",
      "============== Pattern 2479 ==============\n",
      "============== Pattern 2480 ==============\n",
      "============== Pattern 2481 ==============\n",
      "============== Pattern 2482 ==============\n",
      "============== Pattern 2483 ==============\n",
      "============== Pattern 2484 ==============\n",
      "============== Pattern 2485 ==============\n",
      "============== Pattern 2486 ==============\n",
      "============== Pattern 2487 ==============\n",
      "============== Pattern 2488 ==============\n",
      "============== Pattern 2489 ==============\n",
      "============== Pattern 2490 ==============\n",
      "============== Pattern 2491 ==============\n",
      "============== Pattern 2492 ==============\n",
      "============== Pattern 2493 ==============\n",
      "============== Pattern 2494 ==============\n",
      "============== Pattern 2495 ==============\n",
      "============== Pattern 2496 ==============\n",
      "============== Pattern 2497 ==============\n",
      "============== Pattern 2498 ==============\n",
      "============== Pattern 2499 ==============\n",
      "============== Pattern 2500 ==============\n",
      "============== Pattern 2501 ==============\n",
      "============== Pattern 2502 ==============\n",
      "============== Pattern 2503 ==============\n",
      "============== Pattern 2504 ==============\n",
      "============== Pattern 2505 ==============\n",
      "============== Pattern 2506 ==============\n",
      "============== Pattern 2507 ==============\n",
      "============== Pattern 2508 ==============\n",
      "============== Pattern 2509 ==============\n",
      "============== Pattern 2510 ==============\n",
      "============== Pattern 2511 ==============\n",
      "============== Pattern 2512 ==============\n",
      "============== Pattern 2513 ==============\n",
      "============== Pattern 2514 ==============\n",
      "============== Pattern 2515 ==============\n",
      "============== Pattern 2516 ==============\n",
      "============== Pattern 2517 ==============\n",
      "============== Pattern 2518 ==============\n",
      "============== Pattern 2519 ==============\n",
      "============== Pattern 2520 ==============\n",
      "============== Pattern 2521 ==============\n",
      "============== Pattern 2522 ==============\n",
      "============== Pattern 2523 ==============\n",
      "============== Pattern 2524 ==============\n",
      "============== Pattern 2525 ==============\n",
      "============== Pattern 2526 ==============\n",
      "============== Pattern 2527 ==============\n",
      "============== Pattern 2528 ==============\n",
      "============== Pattern 2529 ==============\n",
      "============== Pattern 2530 ==============\n",
      "============== Pattern 2531 ==============\n",
      "============== Pattern 2532 ==============\n",
      "============== Pattern 2533 ==============\n",
      "============== Pattern 2534 ==============\n",
      "============== Pattern 2535 ==============\n",
      "============== Pattern 2536 ==============\n",
      "============== Pattern 2537 ==============\n",
      "============== Pattern 2538 ==============\n",
      "============== Pattern 2539 ==============\n",
      "============== Pattern 2540 ==============\n",
      "============== Pattern 2541 ==============\n",
      "============== Pattern 2542 ==============\n",
      "============== Pattern 2543 ==============\n",
      "============== Pattern 2544 ==============\n",
      "============== Pattern 2545 ==============\n",
      "============== Pattern 2546 ==============\n",
      "============== Pattern 2547 ==============\n",
      "============== Pattern 2548 ==============\n",
      "============== Pattern 2549 ==============\n",
      "============== Pattern 2550 ==============\n",
      "============== Pattern 2551 ==============\n",
      "============== Pattern 2552 ==============\n",
      "============== Pattern 2553 ==============\n",
      "============== Pattern 2554 ==============\n",
      "============== Pattern 2555 ==============\n",
      "============== Pattern 2556 ==============\n",
      "============== Pattern 2557 ==============\n",
      "============== Pattern 2558 ==============\n",
      "============== Pattern 2559 ==============\n",
      "============== Pattern 2560 ==============\n",
      "============== Pattern 2561 ==============\n",
      "============== Pattern 2562 ==============\n",
      "============== Pattern 2563 ==============\n",
      "============== Pattern 2564 ==============\n",
      "============== Pattern 2565 ==============\n",
      "============== Pattern 2566 ==============\n",
      "============== Pattern 2567 ==============\n",
      "============== Pattern 2568 ==============\n",
      "============== Pattern 2569 ==============\n",
      "============== Pattern 2570 ==============\n",
      "============== Pattern 2571 ==============\n",
      "============== Pattern 2572 ==============\n",
      "============== Pattern 2573 ==============\n",
      "============== Pattern 2574 ==============\n",
      "============== Pattern 2575 ==============\n",
      "============== Pattern 2576 ==============\n",
      "============== Pattern 2577 ==============\n",
      "============== Pattern 2578 ==============\n",
      "============== Pattern 2579 ==============\n",
      "============== Pattern 2580 ==============\n",
      "============== Pattern 2581 ==============\n",
      "============== Pattern 2582 ==============\n",
      "============== Pattern 2583 ==============\n",
      "============== Pattern 2584 ==============\n",
      "============== Pattern 2585 ==============\n",
      "============== Pattern 2586 ==============\n",
      "============== Pattern 2587 ==============\n",
      "============== Pattern 2588 ==============\n",
      "============== Pattern 2589 ==============\n",
      "============== Pattern 2590 ==============\n",
      "============== Pattern 2591 ==============\n",
      "============== Pattern 2592 ==============\n",
      "============== Pattern 2593 ==============\n",
      "============== Pattern 2594 ==============\n",
      "============== Pattern 2595 ==============\n",
      "============== Pattern 2596 ==============\n",
      "============== Pattern 2597 ==============\n",
      "============== Pattern 2598 ==============\n",
      "============== Pattern 2599 ==============\n",
      "============== Pattern 2600 ==============\n",
      "============== Pattern 2601 ==============\n",
      "============== Pattern 2602 ==============\n",
      "============== Pattern 2603 ==============\n",
      "============== Pattern 2604 ==============\n",
      "============== Pattern 2605 ==============\n",
      "============== Pattern 2606 ==============\n",
      "============== Pattern 2607 ==============\n",
      "============== Pattern 2608 ==============\n",
      "============== Pattern 2609 ==============\n",
      "============== Pattern 2610 ==============\n",
      "============== Pattern 2611 ==============\n",
      "============== Pattern 2612 ==============\n",
      "============== Pattern 2613 ==============\n",
      "============== Pattern 2614 ==============\n",
      "============== Pattern 2615 ==============\n",
      "============== Pattern 2616 ==============\n",
      "============== Pattern 2617 ==============\n",
      "============== Pattern 2618 ==============\n",
      "============== Pattern 2619 ==============\n",
      "============== Pattern 2620 ==============\n",
      "============== Pattern 2621 ==============\n",
      "============== Pattern 2622 ==============\n",
      "============== Pattern 2623 ==============\n",
      "============== Pattern 2624 ==============\n",
      "============== Pattern 2625 ==============\n",
      "============== Pattern 2626 ==============\n",
      "============== Pattern 2627 ==============\n",
      "============== Pattern 2628 ==============\n",
      "============== Pattern 2629 ==============\n",
      "============== Pattern 2630 ==============\n",
      "============== Pattern 2631 ==============\n",
      "============== Pattern 2632 ==============\n",
      "============== Pattern 2633 ==============\n",
      "============== Pattern 2634 ==============\n",
      "============== Pattern 2635 ==============\n",
      "============== Pattern 2636 ==============\n",
      "============== Pattern 2637 ==============\n",
      "============== Pattern 2638 ==============\n",
      "============== Pattern 2639 ==============\n",
      "============== Pattern 2640 ==============\n",
      "============== Pattern 2641 ==============\n",
      "============== Pattern 2642 ==============\n",
      "============== Pattern 2643 ==============\n",
      "============== Pattern 2644 ==============\n",
      "============== Pattern 2645 ==============\n",
      "============== Pattern 2646 ==============\n",
      "============== Pattern 2647 ==============\n",
      "============== Pattern 2648 ==============\n",
      "============== Pattern 2649 ==============\n",
      "============== Pattern 2650 ==============\n",
      "============== Pattern 2651 ==============\n",
      "============== Pattern 2652 ==============\n",
      "============== Pattern 2653 ==============\n",
      "============== Pattern 2654 ==============\n",
      "============== Pattern 2655 ==============\n",
      "============== Pattern 2656 ==============\n",
      "============== Pattern 2657 ==============\n",
      "============== Pattern 2658 ==============\n",
      "============== Pattern 2659 ==============\n",
      "============== Pattern 2660 ==============\n",
      "============== Pattern 2661 ==============\n",
      "============== Pattern 2662 ==============\n",
      "============== Pattern 2663 ==============\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Pattern 2664 ==============\n",
      "============== Pattern 2665 ==============\n",
      "============== Pattern 2666 ==============\n",
      "============== Pattern 2667 ==============\n",
      "============== Pattern 2668 ==============\n",
      "============== Pattern 2669 ==============\n",
      "============== Pattern 2670 ==============\n",
      "============== Pattern 2671 ==============\n",
      "============== Pattern 2672 ==============\n",
      "============== Pattern 2673 ==============\n",
      "============== Pattern 2674 ==============\n",
      "============== Pattern 2675 ==============\n",
      "============== Pattern 2676 ==============\n",
      "============== Pattern 2677 ==============\n",
      "============== Pattern 2678 ==============\n",
      "============== Pattern 2679 ==============\n",
      "============== Pattern 2680 ==============\n",
      "============== Pattern 2681 ==============\n",
      "============== Pattern 2682 ==============\n",
      "============== Pattern 2683 ==============\n",
      "============== Pattern 2684 ==============\n",
      "============== Pattern 2685 ==============\n",
      "============== Pattern 2686 ==============\n",
      "============== Pattern 2687 ==============\n",
      "============== Pattern 2688 ==============\n",
      "============== Pattern 2689 ==============\n",
      "============== Pattern 2690 ==============\n",
      "============== Pattern 2691 ==============\n",
      "============== Pattern 2692 ==============\n",
      "============== Pattern 2693 ==============\n",
      "============== Pattern 2694 ==============\n",
      "============== Pattern 2695 ==============\n",
      "============== Pattern 2696 ==============\n",
      "============== Pattern 2697 ==============\n",
      "============== Pattern 2698 ==============\n",
      "============== Pattern 2699 ==============\n",
      "============== Pattern 2700 ==============\n",
      "============== Pattern 2701 ==============\n",
      "============== Pattern 2702 ==============\n",
      "============== Pattern 2703 ==============\n",
      "============== Pattern 2704 ==============\n",
      "============== Pattern 2705 ==============\n",
      "============== Pattern 2706 ==============\n",
      "============== Pattern 2707 ==============\n",
      "============== Pattern 2708 ==============\n",
      "============== Pattern 2709 ==============\n",
      "============== Pattern 2710 ==============\n",
      "============== Pattern 2711 ==============\n",
      "============== Pattern 2712 ==============\n",
      "============== Pattern 2713 ==============\n",
      "============== Pattern 2714 ==============\n",
      "============== Pattern 2715 ==============\n",
      "============== Pattern 2716 ==============\n",
      "============== Pattern 2717 ==============\n",
      "============== Pattern 2718 ==============\n",
      "============== Pattern 2719 ==============\n",
      "============== Pattern 2720 ==============\n",
      "============== Pattern 2721 ==============\n",
      "============== Pattern 2722 ==============\n",
      "============== Pattern 2723 ==============\n",
      "============== Pattern 2724 ==============\n",
      "============== Pattern 2725 ==============\n",
      "============== Pattern 2726 ==============\n",
      "============== Pattern 2727 ==============\n",
      "============== Pattern 2728 ==============\n",
      "============== Pattern 2729 ==============\n",
      "============== Pattern 2730 ==============\n",
      "============== Pattern 2731 ==============\n",
      "============== Pattern 2732 ==============\n",
      "============== Pattern 2733 ==============\n",
      "============== Pattern 2734 ==============\n",
      "============== Pattern 2735 ==============\n",
      "============== Pattern 2736 ==============\n",
      "============== Pattern 2737 ==============\n",
      "============== Pattern 2738 ==============\n",
      "============== Pattern 2739 ==============\n",
      "============== Pattern 2740 ==============\n",
      "============== Pattern 2741 ==============\n",
      "============== Pattern 2742 ==============\n",
      "============== Pattern 2743 ==============\n",
      "============== Pattern 2744 ==============\n",
      "============== Pattern 2745 ==============\n",
      "============== Pattern 2746 ==============\n",
      "============== Pattern 2747 ==============\n",
      "============== Pattern 2748 ==============\n",
      "============== Pattern 2749 ==============\n",
      "============== Pattern 2750 ==============\n",
      "============== Pattern 2751 ==============\n",
      "============== Pattern 2752 ==============\n",
      "============== Pattern 2753 ==============\n",
      "============== Pattern 2754 ==============\n",
      "============== Pattern 2755 ==============\n",
      "============== Pattern 2756 ==============\n",
      "============== Pattern 2757 ==============\n",
      "============== Pattern 2758 ==============\n",
      "============== Pattern 2759 ==============\n",
      "============== Pattern 2760 ==============\n",
      "============== Pattern 2761 ==============\n",
      "============== Pattern 2762 ==============\n",
      "============== Pattern 2763 ==============\n",
      "============== Pattern 2764 ==============\n",
      "============== Pattern 2765 ==============\n",
      "============== Pattern 2766 ==============\n",
      "============== Pattern 2767 ==============\n",
      "============== Pattern 2768 ==============\n",
      "============== Pattern 2769 ==============\n",
      "============== Pattern 2770 ==============\n",
      "============== Pattern 2771 ==============\n",
      "============== Pattern 2772 ==============\n",
      "============== Pattern 2773 ==============\n",
      "============== Pattern 2774 ==============\n",
      "============== Pattern 2775 ==============\n",
      "============== Pattern 2776 ==============\n",
      "============== Pattern 2777 ==============\n",
      "============== Pattern 2778 ==============\n",
      "============== Pattern 2779 ==============\n",
      "============== Pattern 2780 ==============\n",
      "============== Pattern 2781 ==============\n",
      "============== Pattern 2782 ==============\n",
      "============== Pattern 2783 ==============\n",
      "============== Pattern 2784 ==============\n",
      "============== Pattern 2785 ==============\n",
      "============== Pattern 2786 ==============\n",
      "============== Pattern 2787 ==============\n",
      "============== Pattern 2788 ==============\n",
      "============== Pattern 2789 ==============\n",
      "============== Pattern 2790 ==============\n",
      "============== Pattern 2791 ==============\n",
      "============== Pattern 2792 ==============\n",
      "============== Pattern 2793 ==============\n",
      "============== Pattern 2794 ==============\n",
      "============== Pattern 2795 ==============\n",
      "============== Pattern 2796 ==============\n",
      "============== Pattern 2797 ==============\n",
      "============== Pattern 2798 ==============\n",
      "============== Pattern 2799 ==============\n",
      "============== Pattern 2800 ==============\n",
      "============== Pattern 2801 ==============\n",
      "============== Pattern 2802 ==============\n",
      "============== Pattern 2803 ==============\n",
      "============== Pattern 2804 ==============\n",
      "============== Pattern 2805 ==============\n",
      "============== Pattern 2806 ==============\n",
      "============== Pattern 2807 ==============\n",
      "============== Pattern 2808 ==============\n",
      "============== Pattern 2809 ==============\n",
      "============== Pattern 2810 ==============\n",
      "============== Pattern 2811 ==============\n",
      "============== Pattern 2812 ==============\n",
      "============== Pattern 2813 ==============\n",
      "============== Pattern 2814 ==============\n",
      "============== Pattern 2815 ==============\n",
      "============== Pattern 2816 ==============\n",
      "============== Pattern 2817 ==============\n",
      "============== Pattern 2818 ==============\n",
      "============== Pattern 2819 ==============\n",
      "============== Pattern 2820 ==============\n",
      "============== Pattern 2821 ==============\n",
      "============== Pattern 2822 ==============\n",
      "============== Pattern 2823 ==============\n",
      "============== Pattern 2824 ==============\n",
      "============== Pattern 2825 ==============\n",
      "============== Pattern 2826 ==============\n",
      "============== Pattern 2827 ==============\n",
      "============== Pattern 2828 ==============\n",
      "============== Pattern 2829 ==============\n",
      "============== Pattern 2830 ==============\n",
      "============== Pattern 2831 ==============\n",
      "============== Pattern 2832 ==============\n",
      "============== Pattern 2833 ==============\n",
      "============== Pattern 2834 ==============\n",
      "============== Pattern 2835 ==============\n",
      "============== Pattern 2836 ==============\n",
      "============== Pattern 2837 ==============\n",
      "============== Pattern 2838 ==============\n",
      "============== Pattern 2839 ==============\n",
      "============== Pattern 2840 ==============\n",
      "============== Pattern 2841 ==============\n",
      "============== Pattern 2842 ==============\n",
      "============== Pattern 2843 ==============\n",
      "============== Pattern 2844 ==============\n",
      "============== Pattern 2845 ==============\n",
      "============== Pattern 2846 ==============\n",
      "============== Pattern 2847 ==============\n",
      "============== Pattern 2848 ==============\n",
      "============== Pattern 2849 ==============\n",
      "============== Pattern 2850 ==============\n",
      "============== Pattern 2851 ==============\n",
      "============== Pattern 2852 ==============\n",
      "============== Pattern 2853 ==============\n",
      "============== Pattern 2854 ==============\n",
      "============== Pattern 2855 ==============\n",
      "============== Pattern 2856 ==============\n",
      "============== Pattern 2857 ==============\n",
      "============== Pattern 2858 ==============\n",
      "============== Pattern 2859 ==============\n",
      "============== Pattern 2860 ==============\n",
      "============== Pattern 2861 ==============\n",
      "============== Pattern 2862 ==============\n",
      "============== Pattern 2863 ==============\n",
      "============== Pattern 2864 ==============\n",
      "============== Pattern 2865 ==============\n",
      "============== Pattern 2866 ==============\n",
      "============== Pattern 2867 ==============\n",
      "============== Pattern 2868 ==============\n",
      "============== Pattern 2869 ==============\n",
      "============== Pattern 2870 ==============\n",
      "============== Pattern 2871 ==============\n",
      "============== Pattern 2872 ==============\n",
      "============== Pattern 2873 ==============\n",
      "============== Pattern 2874 ==============\n",
      "============== Pattern 2875 ==============\n",
      "============== Pattern 2876 ==============\n",
      "============== Pattern 2877 ==============\n",
      "============== Pattern 2878 ==============\n",
      "============== Pattern 2879 ==============\n",
      "============== Pattern 2880 ==============\n",
      "============== Pattern 2881 ==============\n",
      "============== Pattern 2882 ==============\n",
      "============== Pattern 2883 ==============\n",
      "============== Pattern 2884 ==============\n",
      "============== Pattern 2885 ==============\n",
      "============== Pattern 2886 ==============\n",
      "============== Pattern 2887 ==============\n",
      "============== Pattern 2888 ==============\n",
      "============== Pattern 2889 ==============\n",
      "============== Pattern 2890 ==============\n",
      "============== Pattern 2891 ==============\n",
      "============== Pattern 2892 ==============\n",
      "============== Pattern 2893 ==============\n",
      "============== Pattern 2894 ==============\n",
      "============== Pattern 2895 ==============\n",
      "============== Pattern 2896 ==============\n",
      "============== Pattern 2897 ==============\n",
      "============== Pattern 2898 ==============\n",
      "============== Pattern 2899 ==============\n",
      "============== Pattern 2900 ==============\n",
      "============== Pattern 2901 ==============\n",
      "============== Pattern 2902 ==============\n",
      "============== Pattern 2903 ==============\n",
      "============== Pattern 2904 ==============\n",
      "============== Pattern 2905 ==============\n",
      "============== Pattern 2906 ==============\n",
      "============== Pattern 2907 ==============\n",
      "============== Pattern 2908 ==============\n",
      "============== Pattern 2909 ==============\n",
      "============== Pattern 2910 ==============\n",
      "============== Pattern 2911 ==============\n",
      "============== Pattern 2912 ==============\n",
      "============== Pattern 2913 ==============\n",
      "============== Pattern 2914 ==============\n",
      "============== Pattern 2915 ==============\n",
      "============== Pattern 2916 ==============\n",
      "============== Pattern 2917 ==============\n",
      "============== Pattern 2918 ==============\n",
      "============== Pattern 2919 ==============\n",
      "============== Pattern 2920 ==============\n",
      "============== Pattern 2921 ==============\n",
      "============== Pattern 2922 ==============\n",
      "============== Pattern 2923 ==============\n",
      "============== Pattern 2924 ==============\n",
      "============== Pattern 2925 ==============\n",
      "============== Pattern 2926 ==============\n",
      "============== Pattern 2927 ==============\n",
      "============== Pattern 2928 ==============\n",
      "============== Pattern 2929 ==============\n",
      "============== Pattern 2930 ==============\n",
      "============== Pattern 2931 ==============\n",
      "============== Pattern 2932 ==============\n",
      "============== Pattern 2933 ==============\n",
      "============== Pattern 2934 ==============\n",
      "============== Pattern 2935 ==============\n",
      "============== Pattern 2936 ==============\n",
      "============== Pattern 2937 ==============\n",
      "============== Pattern 2938 ==============\n",
      "============== Pattern 2939 ==============\n",
      "============== Pattern 2940 ==============\n",
      "============== Pattern 2941 ==============\n",
      "============== Pattern 2942 ==============\n",
      "============== Pattern 2943 ==============\n",
      "============== Pattern 2944 ==============\n",
      "============== Pattern 2945 ==============\n",
      "============== Pattern 2946 ==============\n",
      "============== Pattern 2947 ==============\n",
      "============== Pattern 2948 ==============\n",
      "============== Pattern 2949 ==============\n",
      "============== Pattern 2950 ==============\n",
      "============== Pattern 2951 ==============\n",
      "============== Pattern 2952 ==============\n",
      "============== Pattern 2953 ==============\n",
      "============== Pattern 2954 ==============\n",
      "============== Pattern 2955 ==============\n",
      "============== Pattern 2956 ==============\n",
      "============== Pattern 2957 ==============\n",
      "============== Pattern 2958 ==============\n",
      "============== Pattern 2959 ==============\n",
      "============== Pattern 2960 ==============\n",
      "============== Pattern 2961 ==============\n",
      "============== Pattern 2962 ==============\n",
      "============== Pattern 2963 ==============\n",
      "============== Pattern 2964 ==============\n",
      "============== Pattern 2965 ==============\n",
      "============== Pattern 2966 ==============\n",
      "============== Pattern 2967 ==============\n",
      "============== Pattern 2968 ==============\n",
      "============== Pattern 2969 ==============\n",
      "============== Pattern 2970 ==============\n",
      "============== Pattern 2971 ==============\n",
      "============== Pattern 2972 ==============\n",
      "============== Pattern 2973 ==============\n",
      "============== Pattern 2974 ==============\n",
      "============== Pattern 2975 ==============\n",
      "============== Pattern 2976 ==============\n",
      "============== Pattern 2977 ==============\n",
      "============== Pattern 2978 ==============\n",
      "============== Pattern 2979 ==============\n",
      "============== Pattern 2980 ==============\n",
      "============== Pattern 2981 ==============\n",
      "============== Pattern 2982 ==============\n",
      "============== Pattern 2983 ==============\n",
      "============== Pattern 2984 ==============\n",
      "============== Pattern 2985 ==============\n",
      "============== Pattern 2986 ==============\n",
      "============== Pattern 2987 ==============\n",
      "============== Pattern 2988 ==============\n",
      "============== Pattern 2989 ==============\n",
      "============== Pattern 2990 ==============\n",
      "============== Pattern 2991 ==============\n",
      "Average comprehensibility: 60.04948177866934\n",
      "std comprehensibility: 4.502708301128793\n",
      "var comprehensibility: 20.274382045054143\n",
      "minimum comprehensibility: 38\n",
      "maximum comprehensibility: 70\n"
     ]
    }
   ],
   "source": [
    "attr_names = dataset.items\n",
    "\n",
    "# print(attr_names)\n",
    "leaves = root.get_leaves()\n",
    "sum_comprehensibility = 0\n",
    "comprehensibilities = []\n",
    "for pattern_counter, leaf in enumerate(leaves):\n",
    "    leaf.reset_path()\n",
    "    leaf.tighten_with_accumulated_samples()\n",
    "    conds = leaf.get_path_conditions(attr_names)\n",
    "    print(f\"============== Pattern {pattern_counter + 1} ==============\")\n",
    "    comprehensibilities.append(sum([cond.comprehensibility for cond in conds]))\n",
    "    \n",
    "print(f\"Average comprehensibility: {np.mean(comprehensibilities)}\")\n",
    "print(f\"std comprehensibility: {np.std(comprehensibilities)}\")\n",
    "print(f\"var comprehensibility: {np.var(comprehensibilities)}\")\n",
    "print(f\"minimum comprehensibility: {np.min(comprehensibilities)}\")\n",
    "print(f\"maximum comprehensibility: {np.max(comprehensibilities)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
