{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib widget\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import torch.nn as nn\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from stream_generators.market_basket_dataset import MarketBasketDataset, BinaryEncodingTransform, RemoveItemsTransform\n",
    "from utils.MatplotlibUtils import reduce_dims_and_plot\n",
    "from network.auto_encoder import AutoEncoder\n",
    "from losses.knn_loss import KNNLoss\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from soft_decision_tree.sdt_model import SDT\n",
    "from sklearn.metrics import davies_bouldin_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 64\n",
    "tree_depth = 6\n",
    "device = 'cuda'\n",
    "dataset_path = r\"/mnt/qnap/ekosman/Groceries_dataset.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the market basket dataset and use one-hot encoding for items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MarketBasketDataset(dataset_path=dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoEncoder(dataset.n_items, 50, 4).train().to(device)\n",
    "epochs = 500\n",
    "lr = 5e-3\n",
    "batch_size = 512\n",
    "log_every = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.transform = torchvision.transforms.Compose([\n",
    "    RemoveItemsTransform(p=0.5),\n",
    "    BinaryEncodingTransform(mapping=dataset.items_to_idx),\n",
    "]\n",
    ")\n",
    "dataset.target_transform = torchvision.transforms.Compose([\n",
    "    BinaryEncodingTransform(mapping=dataset.items_to_idx),\n",
    "]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 / 500 | iteration 0 / 30 | Total Loss: 8.20648193359375 | KNN Loss: 6.229707717895508 | BCE Loss: 1.9767743349075317\n",
      "Epoch 0 / 500 | iteration 5 / 30 | Total Loss: 8.153791427612305 | KNN Loss: 6.229312896728516 | BCE Loss: 1.9244780540466309\n",
      "Epoch 0 / 500 | iteration 10 / 30 | Total Loss: 8.150104522705078 | KNN Loss: 6.22951078414917 | BCE Loss: 1.9205937385559082\n",
      "Epoch 0 / 500 | iteration 15 / 30 | Total Loss: 8.113521575927734 | KNN Loss: 6.229531288146973 | BCE Loss: 1.8839905261993408\n",
      "Epoch 0 / 500 | iteration 20 / 30 | Total Loss: 8.16419506072998 | KNN Loss: 6.228993892669678 | BCE Loss: 1.9352010488510132\n",
      "Epoch 0 / 500 | iteration 25 / 30 | Total Loss: 8.130905151367188 | KNN Loss: 6.228990077972412 | BCE Loss: 1.9019153118133545\n",
      "Epoch 1 / 500 | iteration 0 / 30 | Total Loss: 8.154356956481934 | KNN Loss: 6.228734970092773 | BCE Loss: 1.9256222248077393\n",
      "Epoch 1 / 500 | iteration 5 / 30 | Total Loss: 8.132513046264648 | KNN Loss: 6.228709697723389 | BCE Loss: 1.9038028717041016\n",
      "Epoch 1 / 500 | iteration 10 / 30 | Total Loss: 8.130053520202637 | KNN Loss: 6.228667736053467 | BCE Loss: 1.90138578414917\n",
      "Epoch 1 / 500 | iteration 15 / 30 | Total Loss: 8.1417236328125 | KNN Loss: 6.228212356567383 | BCE Loss: 1.9135113954544067\n",
      "Epoch 1 / 500 | iteration 20 / 30 | Total Loss: 8.141115188598633 | KNN Loss: 6.2280497550964355 | BCE Loss: 1.9130651950836182\n",
      "Epoch 1 / 500 | iteration 25 / 30 | Total Loss: 8.118173599243164 | KNN Loss: 6.227808952331543 | BCE Loss: 1.890364408493042\n",
      "Epoch 2 / 500 | iteration 0 / 30 | Total Loss: 8.13221549987793 | KNN Loss: 6.227659225463867 | BCE Loss: 1.9045562744140625\n",
      "Epoch 2 / 500 | iteration 5 / 30 | Total Loss: 8.114484786987305 | KNN Loss: 6.227724552154541 | BCE Loss: 1.8867597579956055\n",
      "Epoch 2 / 500 | iteration 10 / 30 | Total Loss: 8.1080322265625 | KNN Loss: 6.227458477020264 | BCE Loss: 1.8805742263793945\n",
      "Epoch 2 / 500 | iteration 15 / 30 | Total Loss: 8.092626571655273 | KNN Loss: 6.227215766906738 | BCE Loss: 1.8654111623764038\n",
      "Epoch 2 / 500 | iteration 20 / 30 | Total Loss: 8.092147827148438 | KNN Loss: 6.226921558380127 | BCE Loss: 1.8652260303497314\n",
      "Epoch 2 / 500 | iteration 25 / 30 | Total Loss: 8.065191268920898 | KNN Loss: 6.226383209228516 | BCE Loss: 1.8388079404830933\n",
      "Epoch 3 / 500 | iteration 0 / 30 | Total Loss: 8.03689956665039 | KNN Loss: 6.226396560668945 | BCE Loss: 1.8105027675628662\n",
      "Epoch 3 / 500 | iteration 5 / 30 | Total Loss: 8.064845085144043 | KNN Loss: 6.225793838500977 | BCE Loss: 1.839051604270935\n",
      "Epoch 3 / 500 | iteration 10 / 30 | Total Loss: 8.08012866973877 | KNN Loss: 6.225614547729492 | BCE Loss: 1.8545143604278564\n",
      "Epoch 3 / 500 | iteration 15 / 30 | Total Loss: 8.025519371032715 | KNN Loss: 6.225387096405029 | BCE Loss: 1.8001320362091064\n",
      "Epoch 3 / 500 | iteration 20 / 30 | Total Loss: 8.05484390258789 | KNN Loss: 6.224757194519043 | BCE Loss: 1.8300871849060059\n",
      "Epoch 3 / 500 | iteration 25 / 30 | Total Loss: 8.054713249206543 | KNN Loss: 6.225485801696777 | BCE Loss: 1.8292278051376343\n",
      "Epoch 4 / 500 | iteration 0 / 30 | Total Loss: 8.017685890197754 | KNN Loss: 6.223945140838623 | BCE Loss: 1.7937411069869995\n",
      "Epoch 4 / 500 | iteration 5 / 30 | Total Loss: 8.013472557067871 | KNN Loss: 6.223458766937256 | BCE Loss: 1.7900140285491943\n",
      "Epoch 4 / 500 | iteration 10 / 30 | Total Loss: 8.052870750427246 | KNN Loss: 6.2235565185546875 | BCE Loss: 1.8293139934539795\n",
      "Epoch 4 / 500 | iteration 15 / 30 | Total Loss: 8.01340103149414 | KNN Loss: 6.223618984222412 | BCE Loss: 1.7897820472717285\n",
      "Epoch 4 / 500 | iteration 20 / 30 | Total Loss: 7.983856678009033 | KNN Loss: 6.222249984741211 | BCE Loss: 1.7616066932678223\n",
      "Epoch 4 / 500 | iteration 25 / 30 | Total Loss: 8.018510818481445 | KNN Loss: 6.221158027648926 | BCE Loss: 1.7973527908325195\n",
      "Epoch 5 / 500 | iteration 0 / 30 | Total Loss: 7.917315483093262 | KNN Loss: 6.221251487731934 | BCE Loss: 1.6960642337799072\n",
      "Epoch 5 / 500 | iteration 5 / 30 | Total Loss: 7.951231479644775 | KNN Loss: 6.219966411590576 | BCE Loss: 1.7312651872634888\n",
      "Epoch 5 / 500 | iteration 10 / 30 | Total Loss: 7.9705400466918945 | KNN Loss: 6.220553874969482 | BCE Loss: 1.749986171722412\n",
      "Epoch 5 / 500 | iteration 15 / 30 | Total Loss: 7.93900728225708 | KNN Loss: 6.219863414764404 | BCE Loss: 1.7191438674926758\n",
      "Epoch 5 / 500 | iteration 20 / 30 | Total Loss: 7.942994117736816 | KNN Loss: 6.217997074127197 | BCE Loss: 1.7249970436096191\n",
      "Epoch 5 / 500 | iteration 25 / 30 | Total Loss: 7.928135395050049 | KNN Loss: 6.219007968902588 | BCE Loss: 1.709127426147461\n",
      "Epoch 6 / 500 | iteration 0 / 30 | Total Loss: 7.9293928146362305 | KNN Loss: 6.217809677124023 | BCE Loss: 1.7115833759307861\n",
      "Epoch 6 / 500 | iteration 5 / 30 | Total Loss: 7.874726295471191 | KNN Loss: 6.215137958526611 | BCE Loss: 1.6595885753631592\n",
      "Epoch 6 / 500 | iteration 10 / 30 | Total Loss: 7.876467227935791 | KNN Loss: 6.2137861251831055 | BCE Loss: 1.6626811027526855\n",
      "Epoch 6 / 500 | iteration 15 / 30 | Total Loss: 7.871196746826172 | KNN Loss: 6.21321439743042 | BCE Loss: 1.6579824686050415\n",
      "Epoch 6 / 500 | iteration 20 / 30 | Total Loss: 7.887169361114502 | KNN Loss: 6.211975574493408 | BCE Loss: 1.6751936674118042\n",
      "Epoch 6 / 500 | iteration 25 / 30 | Total Loss: 7.860769748687744 | KNN Loss: 6.212186336517334 | BCE Loss: 1.6485834121704102\n",
      "Epoch 7 / 500 | iteration 0 / 30 | Total Loss: 7.855623245239258 | KNN Loss: 6.2089409828186035 | BCE Loss: 1.6466823816299438\n",
      "Epoch 7 / 500 | iteration 5 / 30 | Total Loss: 7.839492321014404 | KNN Loss: 6.20495080947876 | BCE Loss: 1.6345415115356445\n",
      "Epoch 7 / 500 | iteration 10 / 30 | Total Loss: 7.803741455078125 | KNN Loss: 6.203617572784424 | BCE Loss: 1.6001237630844116\n",
      "Epoch 7 / 500 | iteration 15 / 30 | Total Loss: 7.784850120544434 | KNN Loss: 6.2054362297058105 | BCE Loss: 1.579413652420044\n",
      "Epoch 7 / 500 | iteration 20 / 30 | Total Loss: 7.750797748565674 | KNN Loss: 6.202013969421387 | BCE Loss: 1.5487836599349976\n",
      "Epoch 7 / 500 | iteration 25 / 30 | Total Loss: 7.76231575012207 | KNN Loss: 6.1976494789123535 | BCE Loss: 1.5646661520004272\n",
      "Epoch 8 / 500 | iteration 0 / 30 | Total Loss: 7.771114349365234 | KNN Loss: 6.192774295806885 | BCE Loss: 1.5783398151397705\n",
      "Epoch 8 / 500 | iteration 5 / 30 | Total Loss: 7.741286754608154 | KNN Loss: 6.190091609954834 | BCE Loss: 1.5511951446533203\n",
      "Epoch 8 / 500 | iteration 10 / 30 | Total Loss: 7.716796875 | KNN Loss: 6.183695316314697 | BCE Loss: 1.5331015586853027\n",
      "Epoch 8 / 500 | iteration 15 / 30 | Total Loss: 7.708998680114746 | KNN Loss: 6.180912017822266 | BCE Loss: 1.5280864238739014\n",
      "Epoch 8 / 500 | iteration 20 / 30 | Total Loss: 7.710935592651367 | KNN Loss: 6.171230316162109 | BCE Loss: 1.5397052764892578\n",
      "Epoch 8 / 500 | iteration 25 / 30 | Total Loss: 7.639604568481445 | KNN Loss: 6.165853023529053 | BCE Loss: 1.4737513065338135\n",
      "Epoch 9 / 500 | iteration 0 / 30 | Total Loss: 7.64137601852417 | KNN Loss: 6.155241012573242 | BCE Loss: 1.4861350059509277\n",
      "Epoch 9 / 500 | iteration 5 / 30 | Total Loss: 7.6081037521362305 | KNN Loss: 6.152209281921387 | BCE Loss: 1.4558947086334229\n",
      "Epoch 9 / 500 | iteration 10 / 30 | Total Loss: 7.55560827255249 | KNN Loss: 6.133655071258545 | BCE Loss: 1.4219532012939453\n",
      "Epoch 9 / 500 | iteration 15 / 30 | Total Loss: 7.558263778686523 | KNN Loss: 6.126798152923584 | BCE Loss: 1.43146550655365\n",
      "Epoch 9 / 500 | iteration 20 / 30 | Total Loss: 7.4864091873168945 | KNN Loss: 6.11170768737793 | BCE Loss: 1.3747012615203857\n",
      "Epoch 9 / 500 | iteration 25 / 30 | Total Loss: 7.489190578460693 | KNN Loss: 6.106590747833252 | BCE Loss: 1.382599949836731\n",
      "Epoch 10 / 500 | iteration 0 / 30 | Total Loss: 7.420659065246582 | KNN Loss: 6.071640491485596 | BCE Loss: 1.3490185737609863\n",
      "Epoch 10 / 500 | iteration 5 / 30 | Total Loss: 7.402422904968262 | KNN Loss: 6.058656692504883 | BCE Loss: 1.3437660932540894\n",
      "Epoch 10 / 500 | iteration 10 / 30 | Total Loss: 7.395337104797363 | KNN Loss: 6.047069072723389 | BCE Loss: 1.3482680320739746\n",
      "Epoch 10 / 500 | iteration 15 / 30 | Total Loss: 7.3041157722473145 | KNN Loss: 6.003385066986084 | BCE Loss: 1.3007307052612305\n",
      "Epoch 10 / 500 | iteration 20 / 30 | Total Loss: 7.251194000244141 | KNN Loss: 5.958312034606934 | BCE Loss: 1.2928822040557861\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 / 500 | iteration 25 / 30 | Total Loss: 7.164376258850098 | KNN Loss: 5.898101329803467 | BCE Loss: 1.2662748098373413\n",
      "Epoch 11 / 500 | iteration 0 / 30 | Total Loss: 7.144925117492676 | KNN Loss: 5.868739604949951 | BCE Loss: 1.2761855125427246\n",
      "Epoch 11 / 500 | iteration 5 / 30 | Total Loss: 7.037556171417236 | KNN Loss: 5.836838245391846 | BCE Loss: 1.2007179260253906\n",
      "Epoch 11 / 500 | iteration 10 / 30 | Total Loss: 6.977527618408203 | KNN Loss: 5.7611799240112305 | BCE Loss: 1.2163479328155518\n",
      "Epoch 11 / 500 | iteration 15 / 30 | Total Loss: 6.82755184173584 | KNN Loss: 5.637224197387695 | BCE Loss: 1.1903276443481445\n",
      "Epoch 11 / 500 | iteration 20 / 30 | Total Loss: 6.773477554321289 | KNN Loss: 5.586361885070801 | BCE Loss: 1.1871156692504883\n",
      "Epoch 11 / 500 | iteration 25 / 30 | Total Loss: 6.676501274108887 | KNN Loss: 5.483994007110596 | BCE Loss: 1.192507028579712\n",
      "Epoch 12 / 500 | iteration 0 / 30 | Total Loss: 6.55367374420166 | KNN Loss: 5.393153667449951 | BCE Loss: 1.160520076751709\n",
      "Epoch 12 / 500 | iteration 5 / 30 | Total Loss: 6.4104132652282715 | KNN Loss: 5.246469020843506 | BCE Loss: 1.163944125175476\n",
      "Epoch 12 / 500 | iteration 10 / 30 | Total Loss: 6.286515712738037 | KNN Loss: 5.16939115524292 | BCE Loss: 1.1171245574951172\n",
      "Epoch 12 / 500 | iteration 15 / 30 | Total Loss: 6.18094539642334 | KNN Loss: 5.063691139221191 | BCE Loss: 1.1172542572021484\n",
      "Epoch 12 / 500 | iteration 20 / 30 | Total Loss: 6.084870338439941 | KNN Loss: 4.980307579040527 | BCE Loss: 1.104562520980835\n",
      "Epoch 12 / 500 | iteration 25 / 30 | Total Loss: 6.066459655761719 | KNN Loss: 4.920255184173584 | BCE Loss: 1.1462044715881348\n",
      "Epoch 13 / 500 | iteration 0 / 30 | Total Loss: 5.944694519042969 | KNN Loss: 4.820591926574707 | BCE Loss: 1.1241025924682617\n",
      "Epoch 13 / 500 | iteration 5 / 30 | Total Loss: 5.867527484893799 | KNN Loss: 4.764720439910889 | BCE Loss: 1.1028071641921997\n",
      "Epoch 13 / 500 | iteration 10 / 30 | Total Loss: 5.806433200836182 | KNN Loss: 4.721075057983398 | BCE Loss: 1.0853581428527832\n",
      "Epoch 13 / 500 | iteration 15 / 30 | Total Loss: 5.862550735473633 | KNN Loss: 4.736057281494141 | BCE Loss: 1.126493215560913\n",
      "Epoch 13 / 500 | iteration 20 / 30 | Total Loss: 5.7870192527771 | KNN Loss: 4.6866455078125 | BCE Loss: 1.1003737449645996\n",
      "Epoch 13 / 500 | iteration 25 / 30 | Total Loss: 5.754183769226074 | KNN Loss: 4.655423641204834 | BCE Loss: 1.0987602472305298\n",
      "Epoch 14 / 500 | iteration 0 / 30 | Total Loss: 5.719767093658447 | KNN Loss: 4.621303081512451 | BCE Loss: 1.098464012145996\n",
      "Epoch 14 / 500 | iteration 5 / 30 | Total Loss: 5.705781936645508 | KNN Loss: 4.622513294219971 | BCE Loss: 1.083268642425537\n",
      "Epoch 14 / 500 | iteration 10 / 30 | Total Loss: 5.666697025299072 | KNN Loss: 4.576972484588623 | BCE Loss: 1.0897245407104492\n",
      "Epoch 14 / 500 | iteration 15 / 30 | Total Loss: 5.638641357421875 | KNN Loss: 4.572317600250244 | BCE Loss: 1.0663237571716309\n",
      "Epoch 14 / 500 | iteration 20 / 30 | Total Loss: 5.672654628753662 | KNN Loss: 4.589347839355469 | BCE Loss: 1.083306908607483\n",
      "Epoch 14 / 500 | iteration 25 / 30 | Total Loss: 5.694535255432129 | KNN Loss: 4.605539798736572 | BCE Loss: 1.0889954566955566\n",
      "Epoch 15 / 500 | iteration 0 / 30 | Total Loss: 5.685735702514648 | KNN Loss: 4.58095121383667 | BCE Loss: 1.104784607887268\n",
      "Epoch 15 / 500 | iteration 5 / 30 | Total Loss: 5.66133975982666 | KNN Loss: 4.588793754577637 | BCE Loss: 1.0725457668304443\n",
      "Epoch 15 / 500 | iteration 10 / 30 | Total Loss: 5.688186168670654 | KNN Loss: 4.598855018615723 | BCE Loss: 1.0893311500549316\n",
      "Epoch 15 / 500 | iteration 15 / 30 | Total Loss: 5.709615707397461 | KNN Loss: 4.598489761352539 | BCE Loss: 1.1111257076263428\n",
      "Epoch 15 / 500 | iteration 20 / 30 | Total Loss: 5.627493858337402 | KNN Loss: 4.554380893707275 | BCE Loss: 1.073113203048706\n",
      "Epoch 15 / 500 | iteration 25 / 30 | Total Loss: 5.615450382232666 | KNN Loss: 4.5370941162109375 | BCE Loss: 1.0783562660217285\n",
      "Epoch 16 / 500 | iteration 0 / 30 | Total Loss: 5.6150665283203125 | KNN Loss: 4.554572582244873 | BCE Loss: 1.0604937076568604\n",
      "Epoch 16 / 500 | iteration 5 / 30 | Total Loss: 5.594371318817139 | KNN Loss: 4.537527084350586 | BCE Loss: 1.0568443536758423\n",
      "Epoch 16 / 500 | iteration 10 / 30 | Total Loss: 5.638767719268799 | KNN Loss: 4.551734447479248 | BCE Loss: 1.0870333909988403\n",
      "Epoch 16 / 500 | iteration 15 / 30 | Total Loss: 5.677886962890625 | KNN Loss: 4.574287414550781 | BCE Loss: 1.1035994291305542\n",
      "Epoch 16 / 500 | iteration 20 / 30 | Total Loss: 5.62253999710083 | KNN Loss: 4.545627593994141 | BCE Loss: 1.076912522315979\n",
      "Epoch 16 / 500 | iteration 25 / 30 | Total Loss: 5.620509624481201 | KNN Loss: 4.545123100280762 | BCE Loss: 1.0753865242004395\n",
      "Epoch 17 / 500 | iteration 0 / 30 | Total Loss: 5.689332008361816 | KNN Loss: 4.596221923828125 | BCE Loss: 1.0931098461151123\n",
      "Epoch 17 / 500 | iteration 5 / 30 | Total Loss: 5.624690532684326 | KNN Loss: 4.542619228363037 | BCE Loss: 1.082071304321289\n",
      "Epoch 17 / 500 | iteration 10 / 30 | Total Loss: 5.629563331604004 | KNN Loss: 4.547640800476074 | BCE Loss: 1.0819227695465088\n",
      "Epoch 17 / 500 | iteration 15 / 30 | Total Loss: 5.620672225952148 | KNN Loss: 4.526434898376465 | BCE Loss: 1.0942373275756836\n",
      "Epoch 17 / 500 | iteration 20 / 30 | Total Loss: 5.5588483810424805 | KNN Loss: 4.487802982330322 | BCE Loss: 1.0710452795028687\n",
      "Epoch 17 / 500 | iteration 25 / 30 | Total Loss: 5.589040756225586 | KNN Loss: 4.506860733032227 | BCE Loss: 1.0821797847747803\n",
      "Epoch 18 / 500 | iteration 0 / 30 | Total Loss: 5.563922882080078 | KNN Loss: 4.488667011260986 | BCE Loss: 1.0752556324005127\n",
      "Epoch 18 / 500 | iteration 5 / 30 | Total Loss: 5.551668643951416 | KNN Loss: 4.514287948608398 | BCE Loss: 1.0373806953430176\n",
      "Epoch 18 / 500 | iteration 10 / 30 | Total Loss: 5.587559700012207 | KNN Loss: 4.502690315246582 | BCE Loss: 1.0848695039749146\n",
      "Epoch 18 / 500 | iteration 15 / 30 | Total Loss: 5.60273551940918 | KNN Loss: 4.525026321411133 | BCE Loss: 1.0777089595794678\n",
      "Epoch 18 / 500 | iteration 20 / 30 | Total Loss: 5.543216705322266 | KNN Loss: 4.488561153411865 | BCE Loss: 1.0546553134918213\n",
      "Epoch 18 / 500 | iteration 25 / 30 | Total Loss: 5.600076198577881 | KNN Loss: 4.512548446655273 | BCE Loss: 1.0875276327133179\n",
      "Epoch 19 / 500 | iteration 0 / 30 | Total Loss: 5.57629919052124 | KNN Loss: 4.513040065765381 | BCE Loss: 1.0632591247558594\n",
      "Epoch 19 / 500 | iteration 5 / 30 | Total Loss: 5.5890398025512695 | KNN Loss: 4.4924516677856445 | BCE Loss: 1.096587896347046\n",
      "Epoch 19 / 500 | iteration 10 / 30 | Total Loss: 5.5824666023254395 | KNN Loss: 4.504300594329834 | BCE Loss: 1.078166127204895\n",
      "Epoch 19 / 500 | iteration 15 / 30 | Total Loss: 5.58327054977417 | KNN Loss: 4.515773296356201 | BCE Loss: 1.0674973726272583\n",
      "Epoch 19 / 500 | iteration 20 / 30 | Total Loss: 5.55869722366333 | KNN Loss: 4.493695259094238 | BCE Loss: 1.0650020837783813\n",
      "Epoch 19 / 500 | iteration 25 / 30 | Total Loss: 5.545123100280762 | KNN Loss: 4.497470378875732 | BCE Loss: 1.0476528406143188\n",
      "Epoch 20 / 500 | iteration 0 / 30 | Total Loss: 5.544656753540039 | KNN Loss: 4.497458457946777 | BCE Loss: 1.0471981763839722\n",
      "Epoch 20 / 500 | iteration 5 / 30 | Total Loss: 5.551680564880371 | KNN Loss: 4.473318576812744 | BCE Loss: 1.078362226486206\n",
      "Epoch 20 / 500 | iteration 10 / 30 | Total Loss: 5.530566215515137 | KNN Loss: 4.484960556030273 | BCE Loss: 1.0456056594848633\n",
      "Epoch 20 / 500 | iteration 15 / 30 | Total Loss: 5.581347942352295 | KNN Loss: 4.510611534118652 | BCE Loss: 1.0707364082336426\n",
      "Epoch 20 / 500 | iteration 20 / 30 | Total Loss: 5.499983787536621 | KNN Loss: 4.474111557006836 | BCE Loss: 1.0258724689483643\n",
      "Epoch 20 / 500 | iteration 25 / 30 | Total Loss: 5.564836025238037 | KNN Loss: 4.4979424476623535 | BCE Loss: 1.0668935775756836\n",
      "Epoch 21 / 500 | iteration 0 / 30 | Total Loss: 5.559642791748047 | KNN Loss: 4.492273807525635 | BCE Loss: 1.0673691034317017\n",
      "Epoch 21 / 500 | iteration 5 / 30 | Total Loss: 5.570772647857666 | KNN Loss: 4.51032829284668 | BCE Loss: 1.0604443550109863\n",
      "Epoch 21 / 500 | iteration 10 / 30 | Total Loss: 5.543495178222656 | KNN Loss: 4.509365558624268 | BCE Loss: 1.0341298580169678\n",
      "Epoch 21 / 500 | iteration 15 / 30 | Total Loss: 5.554365158081055 | KNN Loss: 4.478518009185791 | BCE Loss: 1.0758471488952637\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 / 500 | iteration 20 / 30 | Total Loss: 5.5549540519714355 | KNN Loss: 4.488445281982422 | BCE Loss: 1.0665087699890137\n",
      "Epoch 21 / 500 | iteration 25 / 30 | Total Loss: 5.548538684844971 | KNN Loss: 4.491568088531494 | BCE Loss: 1.0569705963134766\n",
      "Epoch 22 / 500 | iteration 0 / 30 | Total Loss: 5.554873466491699 | KNN Loss: 4.491104602813721 | BCE Loss: 1.0637686252593994\n",
      "Epoch 22 / 500 | iteration 5 / 30 | Total Loss: 5.548341751098633 | KNN Loss: 4.495208740234375 | BCE Loss: 1.053133249282837\n",
      "Epoch 22 / 500 | iteration 10 / 30 | Total Loss: 5.5611186027526855 | KNN Loss: 4.513321399688721 | BCE Loss: 1.0477972030639648\n",
      "Epoch 22 / 500 | iteration 15 / 30 | Total Loss: 5.552997589111328 | KNN Loss: 4.482912063598633 | BCE Loss: 1.0700852870941162\n",
      "Epoch 22 / 500 | iteration 20 / 30 | Total Loss: 5.53004264831543 | KNN Loss: 4.47650146484375 | BCE Loss: 1.0535414218902588\n",
      "Epoch 22 / 500 | iteration 25 / 30 | Total Loss: 5.532318592071533 | KNN Loss: 4.472842216491699 | BCE Loss: 1.0594764947891235\n",
      "Epoch 23 / 500 | iteration 0 / 30 | Total Loss: 5.5575666427612305 | KNN Loss: 4.503304958343506 | BCE Loss: 1.0542616844177246\n",
      "Epoch 23 / 500 | iteration 5 / 30 | Total Loss: 5.512808322906494 | KNN Loss: 4.461904048919678 | BCE Loss: 1.0509042739868164\n",
      "Epoch 23 / 500 | iteration 10 / 30 | Total Loss: 5.562824249267578 | KNN Loss: 4.491197109222412 | BCE Loss: 1.071626901626587\n",
      "Epoch 23 / 500 | iteration 15 / 30 | Total Loss: 5.542550086975098 | KNN Loss: 4.482062339782715 | BCE Loss: 1.0604875087738037\n",
      "Epoch 23 / 500 | iteration 20 / 30 | Total Loss: 5.483558654785156 | KNN Loss: 4.443387985229492 | BCE Loss: 1.0401707887649536\n",
      "Epoch 23 / 500 | iteration 25 / 30 | Total Loss: 5.520173072814941 | KNN Loss: 4.464380264282227 | BCE Loss: 1.0557926893234253\n",
      "Epoch 24 / 500 | iteration 0 / 30 | Total Loss: 5.569047927856445 | KNN Loss: 4.480517387390137 | BCE Loss: 1.0885307788848877\n",
      "Epoch 24 / 500 | iteration 5 / 30 | Total Loss: 5.502216339111328 | KNN Loss: 4.461221694946289 | BCE Loss: 1.0409945249557495\n",
      "Epoch 24 / 500 | iteration 10 / 30 | Total Loss: 5.513665199279785 | KNN Loss: 4.4645771980285645 | BCE Loss: 1.0490882396697998\n",
      "Epoch 24 / 500 | iteration 15 / 30 | Total Loss: 5.545014381408691 | KNN Loss: 4.474843978881836 | BCE Loss: 1.0701701641082764\n",
      "Epoch 24 / 500 | iteration 20 / 30 | Total Loss: 5.497584819793701 | KNN Loss: 4.4680562019348145 | BCE Loss: 1.0295286178588867\n",
      "Epoch 24 / 500 | iteration 25 / 30 | Total Loss: 5.537583827972412 | KNN Loss: 4.478082656860352 | BCE Loss: 1.05950129032135\n",
      "Epoch 25 / 500 | iteration 0 / 30 | Total Loss: 5.494113922119141 | KNN Loss: 4.480354309082031 | BCE Loss: 1.0137598514556885\n",
      "Epoch 25 / 500 | iteration 5 / 30 | Total Loss: 5.520191192626953 | KNN Loss: 4.458108901977539 | BCE Loss: 1.062082290649414\n",
      "Epoch 25 / 500 | iteration 10 / 30 | Total Loss: 5.496609210968018 | KNN Loss: 4.458718776702881 | BCE Loss: 1.0378905534744263\n",
      "Epoch 25 / 500 | iteration 15 / 30 | Total Loss: 5.52519416809082 | KNN Loss: 4.448797702789307 | BCE Loss: 1.0763962268829346\n",
      "Epoch 25 / 500 | iteration 20 / 30 | Total Loss: 5.523558616638184 | KNN Loss: 4.455033302307129 | BCE Loss: 1.0685250759124756\n",
      "Epoch 25 / 500 | iteration 25 / 30 | Total Loss: 5.479193210601807 | KNN Loss: 4.464779376983643 | BCE Loss: 1.0144139528274536\n",
      "Epoch 26 / 500 | iteration 0 / 30 | Total Loss: 5.501204013824463 | KNN Loss: 4.464722156524658 | BCE Loss: 1.0364818572998047\n",
      "Epoch 26 / 500 | iteration 5 / 30 | Total Loss: 5.487081527709961 | KNN Loss: 4.455506801605225 | BCE Loss: 1.0315747261047363\n",
      "Epoch 26 / 500 | iteration 10 / 30 | Total Loss: 5.497064590454102 | KNN Loss: 4.470160007476807 | BCE Loss: 1.026904582977295\n",
      "Epoch 26 / 500 | iteration 15 / 30 | Total Loss: 5.510987758636475 | KNN Loss: 4.445531368255615 | BCE Loss: 1.0654562711715698\n",
      "Epoch 26 / 500 | iteration 20 / 30 | Total Loss: 5.522573471069336 | KNN Loss: 4.4640302658081055 | BCE Loss: 1.0585434436798096\n",
      "Epoch 26 / 500 | iteration 25 / 30 | Total Loss: 5.507562160491943 | KNN Loss: 4.451565742492676 | BCE Loss: 1.0559964179992676\n",
      "Epoch 27 / 500 | iteration 0 / 30 | Total Loss: 5.504199028015137 | KNN Loss: 4.438068866729736 | BCE Loss: 1.06613028049469\n",
      "Epoch 27 / 500 | iteration 5 / 30 | Total Loss: 5.4971747398376465 | KNN Loss: 4.459051132202148 | BCE Loss: 1.038123607635498\n",
      "Epoch 27 / 500 | iteration 10 / 30 | Total Loss: 5.528700828552246 | KNN Loss: 4.4635748863220215 | BCE Loss: 1.0651257038116455\n",
      "Epoch 27 / 500 | iteration 15 / 30 | Total Loss: 5.4691877365112305 | KNN Loss: 4.41503381729126 | BCE Loss: 1.0541539192199707\n",
      "Epoch 27 / 500 | iteration 20 / 30 | Total Loss: 5.488893985748291 | KNN Loss: 4.459452152252197 | BCE Loss: 1.0294418334960938\n",
      "Epoch 27 / 500 | iteration 25 / 30 | Total Loss: 5.509237289428711 | KNN Loss: 4.481327533721924 | BCE Loss: 1.027909517288208\n",
      "Epoch 28 / 500 | iteration 0 / 30 | Total Loss: 5.511961936950684 | KNN Loss: 4.451351642608643 | BCE Loss: 1.0606104135513306\n",
      "Epoch 28 / 500 | iteration 5 / 30 | Total Loss: 5.481492042541504 | KNN Loss: 4.44536018371582 | BCE Loss: 1.0361316204071045\n",
      "Epoch 28 / 500 | iteration 10 / 30 | Total Loss: 5.475542068481445 | KNN Loss: 4.4238667488098145 | BCE Loss: 1.0516750812530518\n",
      "Epoch 28 / 500 | iteration 15 / 30 | Total Loss: 5.510094165802002 | KNN Loss: 4.425998687744141 | BCE Loss: 1.0840955972671509\n",
      "Epoch 28 / 500 | iteration 20 / 30 | Total Loss: 5.496927261352539 | KNN Loss: 4.428435325622559 | BCE Loss: 1.0684919357299805\n",
      "Epoch 28 / 500 | iteration 25 / 30 | Total Loss: 5.5201311111450195 | KNN Loss: 4.459758758544922 | BCE Loss: 1.0603721141815186\n",
      "Epoch 29 / 500 | iteration 0 / 30 | Total Loss: 5.51690673828125 | KNN Loss: 4.4540486335754395 | BCE Loss: 1.0628578662872314\n",
      "Epoch 29 / 500 | iteration 5 / 30 | Total Loss: 5.509917259216309 | KNN Loss: 4.459522247314453 | BCE Loss: 1.0503950119018555\n",
      "Epoch 29 / 500 | iteration 10 / 30 | Total Loss: 5.483304023742676 | KNN Loss: 4.423426151275635 | BCE Loss: 1.059877872467041\n",
      "Epoch 29 / 500 | iteration 15 / 30 | Total Loss: 5.448487281799316 | KNN Loss: 4.412908554077148 | BCE Loss: 1.035578966140747\n",
      "Epoch 29 / 500 | iteration 20 / 30 | Total Loss: 5.52114200592041 | KNN Loss: 4.4474358558654785 | BCE Loss: 1.0737062692642212\n",
      "Epoch 29 / 500 | iteration 25 / 30 | Total Loss: 5.5099334716796875 | KNN Loss: 4.456448078155518 | BCE Loss: 1.0534851551055908\n",
      "Epoch 30 / 500 | iteration 0 / 30 | Total Loss: 5.499137878417969 | KNN Loss: 4.437195777893066 | BCE Loss: 1.0619423389434814\n",
      "Epoch 30 / 500 | iteration 5 / 30 | Total Loss: 5.466160774230957 | KNN Loss: 4.434533596038818 | BCE Loss: 1.0316274166107178\n",
      "Epoch 30 / 500 | iteration 10 / 30 | Total Loss: 5.465198516845703 | KNN Loss: 4.413295269012451 | BCE Loss: 1.0519033670425415\n",
      "Epoch 30 / 500 | iteration 15 / 30 | Total Loss: 5.426775932312012 | KNN Loss: 4.397994518280029 | BCE Loss: 1.0287812948226929\n",
      "Epoch 30 / 500 | iteration 20 / 30 | Total Loss: 5.466965675354004 | KNN Loss: 4.436323165893555 | BCE Loss: 1.0306425094604492\n",
      "Epoch 30 / 500 | iteration 25 / 30 | Total Loss: 5.500229358673096 | KNN Loss: 4.438622951507568 | BCE Loss: 1.0616064071655273\n",
      "Epoch 31 / 500 | iteration 0 / 30 | Total Loss: 5.4910149574279785 | KNN Loss: 4.445396900177002 | BCE Loss: 1.0456180572509766\n",
      "Epoch 31 / 500 | iteration 5 / 30 | Total Loss: 5.459510803222656 | KNN Loss: 4.419571399688721 | BCE Loss: 1.0399394035339355\n",
      "Epoch 31 / 500 | iteration 10 / 30 | Total Loss: 5.468924522399902 | KNN Loss: 4.43502140045166 | BCE Loss: 1.033902883529663\n",
      "Epoch 31 / 500 | iteration 15 / 30 | Total Loss: 5.4599995613098145 | KNN Loss: 4.4195990562438965 | BCE Loss: 1.040400505065918\n",
      "Epoch 31 / 500 | iteration 20 / 30 | Total Loss: 5.457972049713135 | KNN Loss: 4.3968377113342285 | BCE Loss: 1.0611343383789062\n",
      "Epoch 31 / 500 | iteration 25 / 30 | Total Loss: 5.518101215362549 | KNN Loss: 4.434550762176514 | BCE Loss: 1.0835505723953247\n",
      "Epoch 32 / 500 | iteration 0 / 30 | Total Loss: 5.47326135635376 | KNN Loss: 4.440469741821289 | BCE Loss: 1.0327916145324707\n",
      "Epoch 32 / 500 | iteration 5 / 30 | Total Loss: 5.510684013366699 | KNN Loss: 4.427999019622803 | BCE Loss: 1.082685112953186\n",
      "Epoch 32 / 500 | iteration 10 / 30 | Total Loss: 5.430150032043457 | KNN Loss: 4.405461311340332 | BCE Loss: 1.0246886014938354\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32 / 500 | iteration 15 / 30 | Total Loss: 5.465092658996582 | KNN Loss: 4.412795543670654 | BCE Loss: 1.0522971153259277\n",
      "Epoch 32 / 500 | iteration 20 / 30 | Total Loss: 5.471210479736328 | KNN Loss: 4.431530475616455 | BCE Loss: 1.039679765701294\n",
      "Epoch 32 / 500 | iteration 25 / 30 | Total Loss: 5.5225958824157715 | KNN Loss: 4.42623233795166 | BCE Loss: 1.0963634252548218\n",
      "Epoch 33 / 500 | iteration 0 / 30 | Total Loss: 5.483162879943848 | KNN Loss: 4.425594329833984 | BCE Loss: 1.0575687885284424\n",
      "Epoch 33 / 500 | iteration 5 / 30 | Total Loss: 5.465487480163574 | KNN Loss: 4.40500020980835 | BCE Loss: 1.0604875087738037\n",
      "Epoch 33 / 500 | iteration 10 / 30 | Total Loss: 5.452884197235107 | KNN Loss: 4.401498317718506 | BCE Loss: 1.0513858795166016\n",
      "Epoch 33 / 500 | iteration 15 / 30 | Total Loss: 5.537181854248047 | KNN Loss: 4.447823524475098 | BCE Loss: 1.0893582105636597\n",
      "Epoch 33 / 500 | iteration 20 / 30 | Total Loss: 5.439001083374023 | KNN Loss: 4.420112609863281 | BCE Loss: 1.0188885927200317\n",
      "Epoch 33 / 500 | iteration 25 / 30 | Total Loss: 5.444067001342773 | KNN Loss: 4.398664474487305 | BCE Loss: 1.0454026460647583\n",
      "Epoch 34 / 500 | iteration 0 / 30 | Total Loss: 5.434345245361328 | KNN Loss: 4.388364315032959 | BCE Loss: 1.0459809303283691\n",
      "Epoch 34 / 500 | iteration 5 / 30 | Total Loss: 5.432413578033447 | KNN Loss: 4.392235279083252 | BCE Loss: 1.0401782989501953\n",
      "Epoch 34 / 500 | iteration 10 / 30 | Total Loss: 5.438814640045166 | KNN Loss: 4.43229866027832 | BCE Loss: 1.0065159797668457\n",
      "Epoch 34 / 500 | iteration 15 / 30 | Total Loss: 5.4699907302856445 | KNN Loss: 4.4359235763549805 | BCE Loss: 1.034066915512085\n",
      "Epoch 34 / 500 | iteration 20 / 30 | Total Loss: 5.483405113220215 | KNN Loss: 4.4373555183410645 | BCE Loss: 1.0460493564605713\n",
      "Epoch 34 / 500 | iteration 25 / 30 | Total Loss: 5.4300384521484375 | KNN Loss: 4.394839286804199 | BCE Loss: 1.0351991653442383\n",
      "Epoch 35 / 500 | iteration 0 / 30 | Total Loss: 5.462638854980469 | KNN Loss: 4.416069030761719 | BCE Loss: 1.046570062637329\n",
      "Epoch 35 / 500 | iteration 5 / 30 | Total Loss: 5.483264923095703 | KNN Loss: 4.420086860656738 | BCE Loss: 1.0631778240203857\n",
      "Epoch 35 / 500 | iteration 10 / 30 | Total Loss: 5.4308929443359375 | KNN Loss: 4.39790678024292 | BCE Loss: 1.0329861640930176\n",
      "Epoch 35 / 500 | iteration 15 / 30 | Total Loss: 5.463813781738281 | KNN Loss: 4.435093402862549 | BCE Loss: 1.0287201404571533\n",
      "Epoch 35 / 500 | iteration 20 / 30 | Total Loss: 5.458250999450684 | KNN Loss: 4.401991844177246 | BCE Loss: 1.0562589168548584\n",
      "Epoch 35 / 500 | iteration 25 / 30 | Total Loss: 5.502748012542725 | KNN Loss: 4.439233779907227 | BCE Loss: 1.0635141134262085\n",
      "Epoch 36 / 500 | iteration 0 / 30 | Total Loss: 5.446374893188477 | KNN Loss: 4.399142742156982 | BCE Loss: 1.0472321510314941\n",
      "Epoch 36 / 500 | iteration 5 / 30 | Total Loss: 5.432999610900879 | KNN Loss: 4.399890899658203 | BCE Loss: 1.0331087112426758\n",
      "Epoch 36 / 500 | iteration 10 / 30 | Total Loss: 5.410686492919922 | KNN Loss: 4.355747222900391 | BCE Loss: 1.0549391508102417\n",
      "Epoch 36 / 500 | iteration 15 / 30 | Total Loss: 5.512669563293457 | KNN Loss: 4.469719886779785 | BCE Loss: 1.0429495573043823\n",
      "Epoch 36 / 500 | iteration 20 / 30 | Total Loss: 5.460320472717285 | KNN Loss: 4.438605308532715 | BCE Loss: 1.0217154026031494\n",
      "Epoch 36 / 500 | iteration 25 / 30 | Total Loss: 5.453856468200684 | KNN Loss: 4.394052505493164 | BCE Loss: 1.0598039627075195\n",
      "Epoch 37 / 500 | iteration 0 / 30 | Total Loss: 5.4630889892578125 | KNN Loss: 4.424572944641113 | BCE Loss: 1.0385158061981201\n",
      "Epoch 37 / 500 | iteration 5 / 30 | Total Loss: 5.480895519256592 | KNN Loss: 4.442571640014648 | BCE Loss: 1.0383237600326538\n",
      "Epoch 37 / 500 | iteration 10 / 30 | Total Loss: 5.457006454467773 | KNN Loss: 4.411884784698486 | BCE Loss: 1.045121669769287\n",
      "Epoch 37 / 500 | iteration 15 / 30 | Total Loss: 5.461166858673096 | KNN Loss: 4.39306640625 | BCE Loss: 1.0681004524230957\n",
      "Epoch 37 / 500 | iteration 20 / 30 | Total Loss: 5.432033061981201 | KNN Loss: 4.383591175079346 | BCE Loss: 1.0484418869018555\n",
      "Epoch 37 / 500 | iteration 25 / 30 | Total Loss: 5.442534446716309 | KNN Loss: 4.41917610168457 | BCE Loss: 1.0233585834503174\n",
      "Epoch 38 / 500 | iteration 0 / 30 | Total Loss: 5.417557716369629 | KNN Loss: 4.410280704498291 | BCE Loss: 1.007277250289917\n",
      "Epoch 38 / 500 | iteration 5 / 30 | Total Loss: 5.440692901611328 | KNN Loss: 4.375678539276123 | BCE Loss: 1.0650146007537842\n",
      "Epoch 38 / 500 | iteration 10 / 30 | Total Loss: 5.436819076538086 | KNN Loss: 4.4189653396606445 | BCE Loss: 1.0178537368774414\n",
      "Epoch 38 / 500 | iteration 15 / 30 | Total Loss: 5.44202995300293 | KNN Loss: 4.381418228149414 | BCE Loss: 1.0606114864349365\n",
      "Epoch 38 / 500 | iteration 20 / 30 | Total Loss: 5.44809627532959 | KNN Loss: 4.3860673904418945 | BCE Loss: 1.0620286464691162\n",
      "Epoch 38 / 500 | iteration 25 / 30 | Total Loss: 5.504837512969971 | KNN Loss: 4.445532321929932 | BCE Loss: 1.0593050718307495\n",
      "Epoch 39 / 500 | iteration 0 / 30 | Total Loss: 5.494167327880859 | KNN Loss: 4.432409286499023 | BCE Loss: 1.061758279800415\n",
      "Epoch 39 / 500 | iteration 5 / 30 | Total Loss: 5.439290523529053 | KNN Loss: 4.379537582397461 | BCE Loss: 1.0597528219223022\n",
      "Epoch 39 / 500 | iteration 10 / 30 | Total Loss: 5.4199604988098145 | KNN Loss: 4.388058662414551 | BCE Loss: 1.0319018363952637\n",
      "Epoch 39 / 500 | iteration 15 / 30 | Total Loss: 5.456178665161133 | KNN Loss: 4.428929328918457 | BCE Loss: 1.0272493362426758\n",
      "Epoch 39 / 500 | iteration 20 / 30 | Total Loss: 5.487547874450684 | KNN Loss: 4.426578521728516 | BCE Loss: 1.060969591140747\n",
      "Epoch 39 / 500 | iteration 25 / 30 | Total Loss: 5.43053674697876 | KNN Loss: 4.371084213256836 | BCE Loss: 1.0594525337219238\n",
      "Epoch 40 / 500 | iteration 0 / 30 | Total Loss: 5.450367450714111 | KNN Loss: 4.403247833251953 | BCE Loss: 1.0471196174621582\n",
      "Epoch 40 / 500 | iteration 5 / 30 | Total Loss: 5.486377716064453 | KNN Loss: 4.437326908111572 | BCE Loss: 1.0490508079528809\n",
      "Epoch 40 / 500 | iteration 10 / 30 | Total Loss: 5.5243706703186035 | KNN Loss: 4.450742721557617 | BCE Loss: 1.0736279487609863\n",
      "Epoch 40 / 500 | iteration 15 / 30 | Total Loss: 5.452278137207031 | KNN Loss: 4.444037437438965 | BCE Loss: 1.0082404613494873\n",
      "Epoch 40 / 500 | iteration 20 / 30 | Total Loss: 5.445175647735596 | KNN Loss: 4.404943466186523 | BCE Loss: 1.0402321815490723\n",
      "Epoch 40 / 500 | iteration 25 / 30 | Total Loss: 5.383834362030029 | KNN Loss: 4.352988243103027 | BCE Loss: 1.030846118927002\n",
      "Epoch 41 / 500 | iteration 0 / 30 | Total Loss: 5.410675525665283 | KNN Loss: 4.369315147399902 | BCE Loss: 1.0413602590560913\n",
      "Epoch 41 / 500 | iteration 5 / 30 | Total Loss: 5.414916038513184 | KNN Loss: 4.374516010284424 | BCE Loss: 1.0403999090194702\n",
      "Epoch 41 / 500 | iteration 10 / 30 | Total Loss: 5.449540615081787 | KNN Loss: 4.3874030113220215 | BCE Loss: 1.0621376037597656\n",
      "Epoch 41 / 500 | iteration 15 / 30 | Total Loss: 5.506773471832275 | KNN Loss: 4.442601203918457 | BCE Loss: 1.0641722679138184\n",
      "Epoch 41 / 500 | iteration 20 / 30 | Total Loss: 5.46135139465332 | KNN Loss: 4.422318458557129 | BCE Loss: 1.0390328168869019\n",
      "Epoch 41 / 500 | iteration 25 / 30 | Total Loss: 5.406974792480469 | KNN Loss: 4.367307186126709 | BCE Loss: 1.0396673679351807\n",
      "Epoch 42 / 500 | iteration 0 / 30 | Total Loss: 5.439409255981445 | KNN Loss: 4.394521713256836 | BCE Loss: 1.0448875427246094\n",
      "Epoch 42 / 500 | iteration 5 / 30 | Total Loss: 5.466169357299805 | KNN Loss: 4.416100978851318 | BCE Loss: 1.0500681400299072\n",
      "Epoch 42 / 500 | iteration 10 / 30 | Total Loss: 5.442313194274902 | KNN Loss: 4.414153575897217 | BCE Loss: 1.0281596183776855\n",
      "Epoch 42 / 500 | iteration 15 / 30 | Total Loss: 5.413845539093018 | KNN Loss: 4.375110149383545 | BCE Loss: 1.038735270500183\n",
      "Epoch 42 / 500 | iteration 20 / 30 | Total Loss: 5.458888530731201 | KNN Loss: 4.429960250854492 | BCE Loss: 1.028928279876709\n",
      "Epoch 42 / 500 | iteration 25 / 30 | Total Loss: 5.454944610595703 | KNN Loss: 4.4031147956848145 | BCE Loss: 1.0518298149108887\n",
      "Epoch 43 / 500 | iteration 0 / 30 | Total Loss: 5.388408660888672 | KNN Loss: 4.35597038269043 | BCE Loss: 1.032438039779663\n",
      "Epoch 43 / 500 | iteration 5 / 30 | Total Loss: 5.406235694885254 | KNN Loss: 4.3930463790893555 | BCE Loss: 1.0131890773773193\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43 / 500 | iteration 10 / 30 | Total Loss: 5.449646949768066 | KNN Loss: 4.367004871368408 | BCE Loss: 1.0826420783996582\n",
      "Epoch 43 / 500 | iteration 15 / 30 | Total Loss: 5.438197135925293 | KNN Loss: 4.406385898590088 | BCE Loss: 1.031810998916626\n",
      "Epoch 43 / 500 | iteration 20 / 30 | Total Loss: 5.438326358795166 | KNN Loss: 4.400182723999023 | BCE Loss: 1.038143515586853\n",
      "Epoch 43 / 500 | iteration 25 / 30 | Total Loss: 5.430689811706543 | KNN Loss: 4.380826473236084 | BCE Loss: 1.0498634576797485\n",
      "Epoch 44 / 500 | iteration 0 / 30 | Total Loss: 5.429266929626465 | KNN Loss: 4.39487361907959 | BCE Loss: 1.034393072128296\n",
      "Epoch 44 / 500 | iteration 5 / 30 | Total Loss: 5.439969062805176 | KNN Loss: 4.393905162811279 | BCE Loss: 1.0460641384124756\n",
      "Epoch 44 / 500 | iteration 10 / 30 | Total Loss: 5.422880172729492 | KNN Loss: 4.380152225494385 | BCE Loss: 1.0427279472351074\n",
      "Epoch 44 / 500 | iteration 15 / 30 | Total Loss: 5.443521499633789 | KNN Loss: 4.396193027496338 | BCE Loss: 1.0473285913467407\n",
      "Epoch 44 / 500 | iteration 20 / 30 | Total Loss: 5.412588119506836 | KNN Loss: 4.3905253410339355 | BCE Loss: 1.0220630168914795\n",
      "Epoch 44 / 500 | iteration 25 / 30 | Total Loss: 5.453335285186768 | KNN Loss: 4.413064002990723 | BCE Loss: 1.0402711629867554\n",
      "Epoch 45 / 500 | iteration 0 / 30 | Total Loss: 5.379915237426758 | KNN Loss: 4.349828720092773 | BCE Loss: 1.0300862789154053\n",
      "Epoch 45 / 500 | iteration 5 / 30 | Total Loss: 5.436446666717529 | KNN Loss: 4.402831554412842 | BCE Loss: 1.033615231513977\n",
      "Epoch 45 / 500 | iteration 10 / 30 | Total Loss: 5.437548637390137 | KNN Loss: 4.413991451263428 | BCE Loss: 1.023557186126709\n",
      "Epoch 45 / 500 | iteration 15 / 30 | Total Loss: 5.475391864776611 | KNN Loss: 4.405949592590332 | BCE Loss: 1.0694422721862793\n",
      "Epoch 45 / 500 | iteration 20 / 30 | Total Loss: 5.434632301330566 | KNN Loss: 4.404597282409668 | BCE Loss: 1.0300352573394775\n",
      "Epoch 45 / 500 | iteration 25 / 30 | Total Loss: 5.530728340148926 | KNN Loss: 4.475914001464844 | BCE Loss: 1.0548145771026611\n",
      "Epoch 46 / 500 | iteration 0 / 30 | Total Loss: 5.4836554527282715 | KNN Loss: 4.437543869018555 | BCE Loss: 1.0461117029190063\n",
      "Epoch 46 / 500 | iteration 5 / 30 | Total Loss: 5.440567970275879 | KNN Loss: 4.40885066986084 | BCE Loss: 1.031717300415039\n",
      "Epoch 46 / 500 | iteration 10 / 30 | Total Loss: 5.441969871520996 | KNN Loss: 4.371810436248779 | BCE Loss: 1.0701594352722168\n",
      "Epoch 46 / 500 | iteration 15 / 30 | Total Loss: 5.451572895050049 | KNN Loss: 4.447200298309326 | BCE Loss: 1.0043725967407227\n",
      "Epoch 46 / 500 | iteration 20 / 30 | Total Loss: 5.46722936630249 | KNN Loss: 4.450328826904297 | BCE Loss: 1.0169005393981934\n",
      "Epoch 46 / 500 | iteration 25 / 30 | Total Loss: 5.407576084136963 | KNN Loss: 4.363743782043457 | BCE Loss: 1.0438323020935059\n",
      "Epoch 47 / 500 | iteration 0 / 30 | Total Loss: 5.446823596954346 | KNN Loss: 4.40451192855835 | BCE Loss: 1.042311668395996\n",
      "Epoch 47 / 500 | iteration 5 / 30 | Total Loss: 5.434894561767578 | KNN Loss: 4.3867034912109375 | BCE Loss: 1.0481913089752197\n",
      "Epoch 47 / 500 | iteration 10 / 30 | Total Loss: 5.448665618896484 | KNN Loss: 4.4008612632751465 | BCE Loss: 1.047804355621338\n",
      "Epoch 47 / 500 | iteration 15 / 30 | Total Loss: 5.41935920715332 | KNN Loss: 4.382431983947754 | BCE Loss: 1.036927342414856\n",
      "Epoch 47 / 500 | iteration 20 / 30 | Total Loss: 5.410595893859863 | KNN Loss: 4.383451461791992 | BCE Loss: 1.0271445512771606\n",
      "Epoch 47 / 500 | iteration 25 / 30 | Total Loss: 5.474902153015137 | KNN Loss: 4.415809631347656 | BCE Loss: 1.0590922832489014\n",
      "Epoch 48 / 500 | iteration 0 / 30 | Total Loss: 5.395890235900879 | KNN Loss: 4.3744215965271 | BCE Loss: 1.0214685201644897\n",
      "Epoch 48 / 500 | iteration 5 / 30 | Total Loss: 5.481747150421143 | KNN Loss: 4.433318138122559 | BCE Loss: 1.048429012298584\n",
      "Epoch 48 / 500 | iteration 10 / 30 | Total Loss: 5.483325481414795 | KNN Loss: 4.412317276000977 | BCE Loss: 1.0710082054138184\n",
      "Epoch 48 / 500 | iteration 15 / 30 | Total Loss: 5.471324443817139 | KNN Loss: 4.420447826385498 | BCE Loss: 1.0508766174316406\n",
      "Epoch 48 / 500 | iteration 20 / 30 | Total Loss: 5.415593147277832 | KNN Loss: 4.358936786651611 | BCE Loss: 1.0566562414169312\n",
      "Epoch 48 / 500 | iteration 25 / 30 | Total Loss: 5.402967929840088 | KNN Loss: 4.377046585083008 | BCE Loss: 1.02592134475708\n",
      "Epoch 49 / 500 | iteration 0 / 30 | Total Loss: 5.44107723236084 | KNN Loss: 4.389427185058594 | BCE Loss: 1.0516499280929565\n",
      "Epoch 49 / 500 | iteration 5 / 30 | Total Loss: 5.388576984405518 | KNN Loss: 4.366049766540527 | BCE Loss: 1.0225272178649902\n",
      "Epoch 49 / 500 | iteration 10 / 30 | Total Loss: 5.39636754989624 | KNN Loss: 4.364090442657471 | BCE Loss: 1.0322771072387695\n",
      "Epoch 49 / 500 | iteration 15 / 30 | Total Loss: 5.436282157897949 | KNN Loss: 4.387103080749512 | BCE Loss: 1.0491788387298584\n",
      "Epoch 49 / 500 | iteration 20 / 30 | Total Loss: 5.423655986785889 | KNN Loss: 4.387472152709961 | BCE Loss: 1.0361838340759277\n",
      "Epoch 49 / 500 | iteration 25 / 30 | Total Loss: 5.413783073425293 | KNN Loss: 4.370218753814697 | BCE Loss: 1.0435645580291748\n",
      "Epoch 50 / 500 | iteration 0 / 30 | Total Loss: 5.398069858551025 | KNN Loss: 4.365652561187744 | BCE Loss: 1.0324172973632812\n",
      "Epoch 50 / 500 | iteration 5 / 30 | Total Loss: 5.400483131408691 | KNN Loss: 4.353903770446777 | BCE Loss: 1.046579122543335\n",
      "Epoch 50 / 500 | iteration 10 / 30 | Total Loss: 5.425743579864502 | KNN Loss: 4.403824806213379 | BCE Loss: 1.0219188928604126\n",
      "Epoch 50 / 500 | iteration 15 / 30 | Total Loss: 5.4205827713012695 | KNN Loss: 4.395394325256348 | BCE Loss: 1.0251884460449219\n",
      "Epoch 50 / 500 | iteration 20 / 30 | Total Loss: 5.414727210998535 | KNN Loss: 4.371762752532959 | BCE Loss: 1.0429645776748657\n",
      "Epoch 50 / 500 | iteration 25 / 30 | Total Loss: 5.395205974578857 | KNN Loss: 4.354883670806885 | BCE Loss: 1.0403223037719727\n",
      "Epoch 51 / 500 | iteration 0 / 30 | Total Loss: 5.440184593200684 | KNN Loss: 4.394311904907227 | BCE Loss: 1.0458729267120361\n",
      "Epoch 51 / 500 | iteration 5 / 30 | Total Loss: 5.405655384063721 | KNN Loss: 4.358616352081299 | BCE Loss: 1.0470389127731323\n",
      "Epoch 51 / 500 | iteration 10 / 30 | Total Loss: 5.447072982788086 | KNN Loss: 4.408512592315674 | BCE Loss: 1.0385606288909912\n",
      "Epoch 51 / 500 | iteration 15 / 30 | Total Loss: 5.435193061828613 | KNN Loss: 4.415864944458008 | BCE Loss: 1.0193283557891846\n",
      "Epoch 51 / 500 | iteration 20 / 30 | Total Loss: 5.387630462646484 | KNN Loss: 4.34501838684082 | BCE Loss: 1.042612075805664\n",
      "Epoch 51 / 500 | iteration 25 / 30 | Total Loss: 5.404973030090332 | KNN Loss: 4.3852219581604 | BCE Loss: 1.0197510719299316\n",
      "Epoch 52 / 500 | iteration 0 / 30 | Total Loss: 5.366508483886719 | KNN Loss: 4.357024192810059 | BCE Loss: 1.0094841718673706\n",
      "Epoch 52 / 500 | iteration 5 / 30 | Total Loss: 5.444090366363525 | KNN Loss: 4.393730640411377 | BCE Loss: 1.0503597259521484\n",
      "Epoch 52 / 500 | iteration 10 / 30 | Total Loss: 5.423216342926025 | KNN Loss: 4.381137371063232 | BCE Loss: 1.042078971862793\n",
      "Epoch 52 / 500 | iteration 15 / 30 | Total Loss: 5.4081244468688965 | KNN Loss: 4.370139122009277 | BCE Loss: 1.0379853248596191\n",
      "Epoch 52 / 500 | iteration 20 / 30 | Total Loss: 5.441499710083008 | KNN Loss: 4.403850555419922 | BCE Loss: 1.037649154663086\n",
      "Epoch 52 / 500 | iteration 25 / 30 | Total Loss: 5.422708511352539 | KNN Loss: 4.373182773590088 | BCE Loss: 1.0495256185531616\n",
      "Epoch 53 / 500 | iteration 0 / 30 | Total Loss: 5.4378156661987305 | KNN Loss: 4.392444133758545 | BCE Loss: 1.0453712940216064\n",
      "Epoch 53 / 500 | iteration 5 / 30 | Total Loss: 5.389794826507568 | KNN Loss: 4.364875793457031 | BCE Loss: 1.024919033050537\n",
      "Epoch 53 / 500 | iteration 10 / 30 | Total Loss: 5.4228715896606445 | KNN Loss: 4.376680374145508 | BCE Loss: 1.0461912155151367\n",
      "Epoch 53 / 500 | iteration 15 / 30 | Total Loss: 5.408182144165039 | KNN Loss: 4.4154839515686035 | BCE Loss: 0.9926982522010803\n",
      "Epoch 53 / 500 | iteration 20 / 30 | Total Loss: 5.428563594818115 | KNN Loss: 4.376416206359863 | BCE Loss: 1.052147388458252\n",
      "Epoch 53 / 500 | iteration 25 / 30 | Total Loss: 5.396663665771484 | KNN Loss: 4.349066257476807 | BCE Loss: 1.0475974082946777\n",
      "Epoch 54 / 500 | iteration 0 / 30 | Total Loss: 5.406235218048096 | KNN Loss: 4.394098281860352 | BCE Loss: 1.0121369361877441\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54 / 500 | iteration 5 / 30 | Total Loss: 5.449002265930176 | KNN Loss: 4.384759426116943 | BCE Loss: 1.064242959022522\n",
      "Epoch 54 / 500 | iteration 10 / 30 | Total Loss: 5.4345502853393555 | KNN Loss: 4.393093109130859 | BCE Loss: 1.041456937789917\n",
      "Epoch 54 / 500 | iteration 15 / 30 | Total Loss: 5.435512542724609 | KNN Loss: 4.39929723739624 | BCE Loss: 1.03621506690979\n",
      "Epoch 54 / 500 | iteration 20 / 30 | Total Loss: 5.455670356750488 | KNN Loss: 4.377146244049072 | BCE Loss: 1.078524112701416\n",
      "Epoch 54 / 500 | iteration 25 / 30 | Total Loss: 5.399627208709717 | KNN Loss: 4.3628764152526855 | BCE Loss: 1.0367507934570312\n",
      "Epoch 55 / 500 | iteration 0 / 30 | Total Loss: 5.409934997558594 | KNN Loss: 4.399448871612549 | BCE Loss: 1.0104858875274658\n",
      "Epoch 55 / 500 | iteration 5 / 30 | Total Loss: 5.371906280517578 | KNN Loss: 4.363832950592041 | BCE Loss: 1.0080735683441162\n",
      "Epoch 55 / 500 | iteration 10 / 30 | Total Loss: 5.45555305480957 | KNN Loss: 4.432048797607422 | BCE Loss: 1.0235044956207275\n",
      "Epoch 55 / 500 | iteration 15 / 30 | Total Loss: 5.406797885894775 | KNN Loss: 4.371520042419434 | BCE Loss: 1.0352778434753418\n",
      "Epoch 55 / 500 | iteration 20 / 30 | Total Loss: 5.448080062866211 | KNN Loss: 4.391940116882324 | BCE Loss: 1.0561397075653076\n",
      "Epoch 55 / 500 | iteration 25 / 30 | Total Loss: 5.385607719421387 | KNN Loss: 4.359148025512695 | BCE Loss: 1.0264595746994019\n",
      "Epoch 56 / 500 | iteration 0 / 30 | Total Loss: 5.388307094573975 | KNN Loss: 4.338322162628174 | BCE Loss: 1.0499849319458008\n",
      "Epoch 56 / 500 | iteration 5 / 30 | Total Loss: 5.41115665435791 | KNN Loss: 4.3748087882995605 | BCE Loss: 1.0363481044769287\n",
      "Epoch 56 / 500 | iteration 10 / 30 | Total Loss: 5.418886184692383 | KNN Loss: 4.388396263122559 | BCE Loss: 1.0304896831512451\n",
      "Epoch 56 / 500 | iteration 15 / 30 | Total Loss: 5.408212661743164 | KNN Loss: 4.367094993591309 | BCE Loss: 1.0411179065704346\n",
      "Epoch 56 / 500 | iteration 20 / 30 | Total Loss: 5.416546821594238 | KNN Loss: 4.362126350402832 | BCE Loss: 1.0544207096099854\n",
      "Epoch 56 / 500 | iteration 25 / 30 | Total Loss: 5.400557518005371 | KNN Loss: 4.38469123840332 | BCE Loss: 1.0158665180206299\n",
      "Epoch 57 / 500 | iteration 0 / 30 | Total Loss: 5.408740520477295 | KNN Loss: 4.373198986053467 | BCE Loss: 1.0355415344238281\n",
      "Epoch 57 / 500 | iteration 5 / 30 | Total Loss: 5.405075550079346 | KNN Loss: 4.3662519454956055 | BCE Loss: 1.0388236045837402\n",
      "Epoch 57 / 500 | iteration 10 / 30 | Total Loss: 5.463981628417969 | KNN Loss: 4.4227800369262695 | BCE Loss: 1.0412017107009888\n",
      "Epoch 57 / 500 | iteration 15 / 30 | Total Loss: 5.367404937744141 | KNN Loss: 4.349797248840332 | BCE Loss: 1.0176074504852295\n",
      "Epoch 57 / 500 | iteration 20 / 30 | Total Loss: 5.380917549133301 | KNN Loss: 4.381295204162598 | BCE Loss: 0.999622106552124\n",
      "Epoch 57 / 500 | iteration 25 / 30 | Total Loss: 5.349778175354004 | KNN Loss: 4.346541881561279 | BCE Loss: 1.0032362937927246\n",
      "Epoch 58 / 500 | iteration 0 / 30 | Total Loss: 5.430614471435547 | KNN Loss: 4.363688945770264 | BCE Loss: 1.0669255256652832\n",
      "Epoch 58 / 500 | iteration 5 / 30 | Total Loss: 5.390855312347412 | KNN Loss: 4.365479946136475 | BCE Loss: 1.0253753662109375\n",
      "Epoch 58 / 500 | iteration 10 / 30 | Total Loss: 5.374443054199219 | KNN Loss: 4.363540172576904 | BCE Loss: 1.0109031200408936\n",
      "Epoch 58 / 500 | iteration 15 / 30 | Total Loss: 5.421691417694092 | KNN Loss: 4.4023847579956055 | BCE Loss: 1.0193066596984863\n",
      "Epoch 58 / 500 | iteration 20 / 30 | Total Loss: 5.39314603805542 | KNN Loss: 4.360533714294434 | BCE Loss: 1.0326123237609863\n",
      "Epoch 58 / 500 | iteration 25 / 30 | Total Loss: 5.3994340896606445 | KNN Loss: 4.348440170288086 | BCE Loss: 1.0509941577911377\n",
      "Epoch 59 / 500 | iteration 0 / 30 | Total Loss: 5.367969989776611 | KNN Loss: 4.351137638092041 | BCE Loss: 1.0168324708938599\n",
      "Epoch 59 / 500 | iteration 5 / 30 | Total Loss: 5.367802619934082 | KNN Loss: 4.350238800048828 | BCE Loss: 1.017563819885254\n",
      "Epoch 59 / 500 | iteration 10 / 30 | Total Loss: 5.416420936584473 | KNN Loss: 4.389313220977783 | BCE Loss: 1.0271077156066895\n",
      "Epoch 59 / 500 | iteration 15 / 30 | Total Loss: 5.423338890075684 | KNN Loss: 4.37170934677124 | BCE Loss: 1.0516297817230225\n",
      "Epoch 59 / 500 | iteration 20 / 30 | Total Loss: 5.429568767547607 | KNN Loss: 4.373367786407471 | BCE Loss: 1.0562008619308472\n",
      "Epoch 59 / 500 | iteration 25 / 30 | Total Loss: 5.455906867980957 | KNN Loss: 4.436147212982178 | BCE Loss: 1.0197594165802002\n",
      "Epoch 60 / 500 | iteration 0 / 30 | Total Loss: 5.3847126960754395 | KNN Loss: 4.360602855682373 | BCE Loss: 1.0241098403930664\n",
      "Epoch 60 / 500 | iteration 5 / 30 | Total Loss: 5.440230846405029 | KNN Loss: 4.395781517028809 | BCE Loss: 1.0444494485855103\n",
      "Epoch 60 / 500 | iteration 10 / 30 | Total Loss: 5.391924858093262 | KNN Loss: 4.372215747833252 | BCE Loss: 1.0197092294692993\n",
      "Epoch 60 / 500 | iteration 15 / 30 | Total Loss: 5.454691410064697 | KNN Loss: 4.435194492340088 | BCE Loss: 1.0194969177246094\n",
      "Epoch 60 / 500 | iteration 20 / 30 | Total Loss: 5.367445945739746 | KNN Loss: 4.368897438049316 | BCE Loss: 0.998548686504364\n",
      "Epoch 60 / 500 | iteration 25 / 30 | Total Loss: 5.387725353240967 | KNN Loss: 4.362695693969727 | BCE Loss: 1.0250296592712402\n",
      "Epoch 61 / 500 | iteration 0 / 30 | Total Loss: 5.419384002685547 | KNN Loss: 4.3904948234558105 | BCE Loss: 1.0288889408111572\n",
      "Epoch 61 / 500 | iteration 5 / 30 | Total Loss: 5.374441623687744 | KNN Loss: 4.341531753540039 | BCE Loss: 1.032909870147705\n",
      "Epoch 61 / 500 | iteration 10 / 30 | Total Loss: 5.367671489715576 | KNN Loss: 4.349256992340088 | BCE Loss: 1.0184143781661987\n",
      "Epoch 61 / 500 | iteration 15 / 30 | Total Loss: 5.358716011047363 | KNN Loss: 4.33439302444458 | BCE Loss: 1.0243232250213623\n",
      "Epoch 61 / 500 | iteration 20 / 30 | Total Loss: 5.35597562789917 | KNN Loss: 4.340019226074219 | BCE Loss: 1.0159564018249512\n",
      "Epoch 61 / 500 | iteration 25 / 30 | Total Loss: 5.3663411140441895 | KNN Loss: 4.325162410736084 | BCE Loss: 1.041178822517395\n",
      "Epoch 62 / 500 | iteration 0 / 30 | Total Loss: 5.3587188720703125 | KNN Loss: 4.351605415344238 | BCE Loss: 1.0071132183074951\n",
      "Epoch 62 / 500 | iteration 5 / 30 | Total Loss: 5.406048774719238 | KNN Loss: 4.395566463470459 | BCE Loss: 1.0104823112487793\n",
      "Epoch 62 / 500 | iteration 10 / 30 | Total Loss: 5.452615737915039 | KNN Loss: 4.421954154968262 | BCE Loss: 1.0306613445281982\n",
      "Epoch 62 / 500 | iteration 15 / 30 | Total Loss: 5.382629871368408 | KNN Loss: 4.357568740844727 | BCE Loss: 1.0250611305236816\n",
      "Epoch 62 / 500 | iteration 20 / 30 | Total Loss: 5.416059494018555 | KNN Loss: 4.3868889808654785 | BCE Loss: 1.0291705131530762\n",
      "Epoch 62 / 500 | iteration 25 / 30 | Total Loss: 5.41148567199707 | KNN Loss: 4.381632328033447 | BCE Loss: 1.0298532247543335\n",
      "Epoch 63 / 500 | iteration 0 / 30 | Total Loss: 5.3852620124816895 | KNN Loss: 4.353483200073242 | BCE Loss: 1.0317788124084473\n",
      "Epoch 63 / 500 | iteration 5 / 30 | Total Loss: 5.357490539550781 | KNN Loss: 4.357767105102539 | BCE Loss: 0.9997236132621765\n",
      "Epoch 63 / 500 | iteration 10 / 30 | Total Loss: 5.402055740356445 | KNN Loss: 4.375126361846924 | BCE Loss: 1.0269291400909424\n",
      "Epoch 63 / 500 | iteration 15 / 30 | Total Loss: 5.380546569824219 | KNN Loss: 4.363646507263184 | BCE Loss: 1.016899824142456\n",
      "Epoch 63 / 500 | iteration 20 / 30 | Total Loss: 5.394704818725586 | KNN Loss: 4.350874900817871 | BCE Loss: 1.043830156326294\n",
      "Epoch 63 / 500 | iteration 25 / 30 | Total Loss: 5.346158027648926 | KNN Loss: 4.34224271774292 | BCE Loss: 1.0039154291152954\n",
      "Epoch 64 / 500 | iteration 0 / 30 | Total Loss: 5.391196250915527 | KNN Loss: 4.332167625427246 | BCE Loss: 1.0590288639068604\n",
      "Epoch 64 / 500 | iteration 5 / 30 | Total Loss: 5.446938514709473 | KNN Loss: 4.384058475494385 | BCE Loss: 1.0628799200057983\n",
      "Epoch 64 / 500 | iteration 10 / 30 | Total Loss: 5.427858352661133 | KNN Loss: 4.406826019287109 | BCE Loss: 1.0210322141647339\n",
      "Epoch 64 / 500 | iteration 15 / 30 | Total Loss: 5.449990749359131 | KNN Loss: 4.388915061950684 | BCE Loss: 1.0610756874084473\n",
      "Epoch 64 / 500 | iteration 20 / 30 | Total Loss: 5.412281036376953 | KNN Loss: 4.360388278961182 | BCE Loss: 1.0518927574157715\n",
      "Epoch 64 / 500 | iteration 25 / 30 | Total Loss: 5.339052677154541 | KNN Loss: 4.321866512298584 | BCE Loss: 1.0171860456466675\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65 / 500 | iteration 0 / 30 | Total Loss: 5.391382217407227 | KNN Loss: 4.372014999389648 | BCE Loss: 1.019366979598999\n",
      "Epoch 65 / 500 | iteration 5 / 30 | Total Loss: 5.449484825134277 | KNN Loss: 4.38981819152832 | BCE Loss: 1.0596665143966675\n",
      "Epoch 65 / 500 | iteration 10 / 30 | Total Loss: 5.417436599731445 | KNN Loss: 4.3928608894348145 | BCE Loss: 1.0245754718780518\n",
      "Epoch 65 / 500 | iteration 15 / 30 | Total Loss: 5.410167694091797 | KNN Loss: 4.370477199554443 | BCE Loss: 1.0396902561187744\n",
      "Epoch 65 / 500 | iteration 20 / 30 | Total Loss: 5.371586799621582 | KNN Loss: 4.362719535827637 | BCE Loss: 1.0088670253753662\n",
      "Epoch 65 / 500 | iteration 25 / 30 | Total Loss: 5.4390363693237305 | KNN Loss: 4.396903991699219 | BCE Loss: 1.0421321392059326\n",
      "Epoch 66 / 500 | iteration 0 / 30 | Total Loss: 5.366359710693359 | KNN Loss: 4.362687587738037 | BCE Loss: 1.0036718845367432\n",
      "Epoch 66 / 500 | iteration 5 / 30 | Total Loss: 5.437038898468018 | KNN Loss: 4.4082255363464355 | BCE Loss: 1.028813362121582\n",
      "Epoch 66 / 500 | iteration 10 / 30 | Total Loss: 5.390510559082031 | KNN Loss: 4.3714823722839355 | BCE Loss: 1.0190284252166748\n",
      "Epoch 66 / 500 | iteration 15 / 30 | Total Loss: 5.431207656860352 | KNN Loss: 4.390146732330322 | BCE Loss: 1.0410608053207397\n",
      "Epoch 66 / 500 | iteration 20 / 30 | Total Loss: 5.387655735015869 | KNN Loss: 4.35425329208374 | BCE Loss: 1.0334025621414185\n",
      "Epoch 66 / 500 | iteration 25 / 30 | Total Loss: 5.4050612449646 | KNN Loss: 4.364720344543457 | BCE Loss: 1.0403409004211426\n",
      "Epoch 67 / 500 | iteration 0 / 30 | Total Loss: 5.3633222579956055 | KNN Loss: 4.3211140632629395 | BCE Loss: 1.042208194732666\n",
      "Epoch 67 / 500 | iteration 5 / 30 | Total Loss: 5.424328804016113 | KNN Loss: 4.358514785766602 | BCE Loss: 1.0658138990402222\n",
      "Epoch 67 / 500 | iteration 10 / 30 | Total Loss: 5.404283046722412 | KNN Loss: 4.3777618408203125 | BCE Loss: 1.0265212059020996\n",
      "Epoch 67 / 500 | iteration 15 / 30 | Total Loss: 5.423835277557373 | KNN Loss: 4.373168468475342 | BCE Loss: 1.0506668090820312\n",
      "Epoch 67 / 500 | iteration 20 / 30 | Total Loss: 5.415895462036133 | KNN Loss: 4.3960137367248535 | BCE Loss: 1.0198816061019897\n",
      "Epoch 67 / 500 | iteration 25 / 30 | Total Loss: 5.396212100982666 | KNN Loss: 4.355141639709473 | BCE Loss: 1.041070580482483\n",
      "Epoch 68 / 500 | iteration 0 / 30 | Total Loss: 5.435503959655762 | KNN Loss: 4.365298271179199 | BCE Loss: 1.070205569267273\n",
      "Epoch 68 / 500 | iteration 5 / 30 | Total Loss: 5.386167526245117 | KNN Loss: 4.345717430114746 | BCE Loss: 1.0404503345489502\n",
      "Epoch 68 / 500 | iteration 10 / 30 | Total Loss: 5.394805908203125 | KNN Loss: 4.342829704284668 | BCE Loss: 1.051976203918457\n",
      "Epoch 68 / 500 | iteration 15 / 30 | Total Loss: 5.367334365844727 | KNN Loss: 4.350171089172363 | BCE Loss: 1.0171635150909424\n",
      "Epoch 68 / 500 | iteration 20 / 30 | Total Loss: 5.407523155212402 | KNN Loss: 4.367863655090332 | BCE Loss: 1.0396595001220703\n",
      "Epoch 68 / 500 | iteration 25 / 30 | Total Loss: 5.395446300506592 | KNN Loss: 4.363863945007324 | BCE Loss: 1.0315823554992676\n",
      "Epoch 69 / 500 | iteration 0 / 30 | Total Loss: 5.345477104187012 | KNN Loss: 4.319098472595215 | BCE Loss: 1.0263786315917969\n",
      "Epoch 69 / 500 | iteration 5 / 30 | Total Loss: 5.399831771850586 | KNN Loss: 4.353115558624268 | BCE Loss: 1.046716332435608\n",
      "Epoch 69 / 500 | iteration 10 / 30 | Total Loss: 5.456917762756348 | KNN Loss: 4.424172401428223 | BCE Loss: 1.032745361328125\n",
      "Epoch 69 / 500 | iteration 15 / 30 | Total Loss: 5.416932582855225 | KNN Loss: 4.359374523162842 | BCE Loss: 1.0575580596923828\n",
      "Epoch 69 / 500 | iteration 20 / 30 | Total Loss: 5.361947059631348 | KNN Loss: 4.363081932067871 | BCE Loss: 0.9988651275634766\n",
      "Epoch 69 / 500 | iteration 25 / 30 | Total Loss: 5.437409400939941 | KNN Loss: 4.413651466369629 | BCE Loss: 1.023757815361023\n",
      "Epoch 70 / 500 | iteration 0 / 30 | Total Loss: 5.404778480529785 | KNN Loss: 4.3759765625 | BCE Loss: 1.0288021564483643\n",
      "Epoch 70 / 500 | iteration 5 / 30 | Total Loss: 5.378125190734863 | KNN Loss: 4.353652000427246 | BCE Loss: 1.0244734287261963\n",
      "Epoch 70 / 500 | iteration 10 / 30 | Total Loss: 5.395613670349121 | KNN Loss: 4.352489948272705 | BCE Loss: 1.043123722076416\n",
      "Epoch 70 / 500 | iteration 15 / 30 | Total Loss: 5.387442588806152 | KNN Loss: 4.350774765014648 | BCE Loss: 1.036667823791504\n",
      "Epoch 70 / 500 | iteration 20 / 30 | Total Loss: 5.335548400878906 | KNN Loss: 4.333988666534424 | BCE Loss: 1.0015594959259033\n",
      "Epoch 70 / 500 | iteration 25 / 30 | Total Loss: 5.3557024002075195 | KNN Loss: 4.331252098083496 | BCE Loss: 1.0244505405426025\n",
      "Epoch 71 / 500 | iteration 0 / 30 | Total Loss: 5.37974214553833 | KNN Loss: 4.376910209655762 | BCE Loss: 1.002832055091858\n",
      "Epoch 71 / 500 | iteration 5 / 30 | Total Loss: 5.407976150512695 | KNN Loss: 4.36602783203125 | BCE Loss: 1.0419481992721558\n",
      "Epoch 71 / 500 | iteration 10 / 30 | Total Loss: 5.3522725105285645 | KNN Loss: 4.341785430908203 | BCE Loss: 1.0104869604110718\n",
      "Epoch 71 / 500 | iteration 15 / 30 | Total Loss: 5.3655900955200195 | KNN Loss: 4.335733890533447 | BCE Loss: 1.0298562049865723\n",
      "Epoch 71 / 500 | iteration 20 / 30 | Total Loss: 5.45839786529541 | KNN Loss: 4.426494121551514 | BCE Loss: 1.0319035053253174\n",
      "Epoch 71 / 500 | iteration 25 / 30 | Total Loss: 5.414244651794434 | KNN Loss: 4.384288311004639 | BCE Loss: 1.029956579208374\n",
      "Epoch 72 / 500 | iteration 0 / 30 | Total Loss: 5.328866958618164 | KNN Loss: 4.335522651672363 | BCE Loss: 0.9933445453643799\n",
      "Epoch 72 / 500 | iteration 5 / 30 | Total Loss: 5.414572715759277 | KNN Loss: 4.372965335845947 | BCE Loss: 1.041607141494751\n",
      "Epoch 72 / 500 | iteration 10 / 30 | Total Loss: 5.420860290527344 | KNN Loss: 4.392657279968262 | BCE Loss: 1.028202772140503\n",
      "Epoch 72 / 500 | iteration 15 / 30 | Total Loss: 5.414689540863037 | KNN Loss: 4.386598110198975 | BCE Loss: 1.0280914306640625\n",
      "Epoch 72 / 500 | iteration 20 / 30 | Total Loss: 5.472897052764893 | KNN Loss: 4.42043399810791 | BCE Loss: 1.0524630546569824\n",
      "Epoch 72 / 500 | iteration 25 / 30 | Total Loss: 5.3983283042907715 | KNN Loss: 4.358725547790527 | BCE Loss: 1.0396027565002441\n",
      "Epoch 73 / 500 | iteration 0 / 30 | Total Loss: 5.33921480178833 | KNN Loss: 4.325951099395752 | BCE Loss: 1.0132637023925781\n",
      "Epoch 73 / 500 | iteration 5 / 30 | Total Loss: 5.392702102661133 | KNN Loss: 4.346252918243408 | BCE Loss: 1.0464491844177246\n",
      "Epoch 73 / 500 | iteration 10 / 30 | Total Loss: 5.406346797943115 | KNN Loss: 4.363827228546143 | BCE Loss: 1.0425195693969727\n",
      "Epoch 73 / 500 | iteration 15 / 30 | Total Loss: 5.3487420082092285 | KNN Loss: 4.337866306304932 | BCE Loss: 1.0108757019042969\n",
      "Epoch 73 / 500 | iteration 20 / 30 | Total Loss: 5.407231330871582 | KNN Loss: 4.381599426269531 | BCE Loss: 1.0256319046020508\n",
      "Epoch 73 / 500 | iteration 25 / 30 | Total Loss: 5.4418134689331055 | KNN Loss: 4.395627021789551 | BCE Loss: 1.0461864471435547\n",
      "Epoch 74 / 500 | iteration 0 / 30 | Total Loss: 5.349703788757324 | KNN Loss: 4.349468231201172 | BCE Loss: 1.0002353191375732\n",
      "Epoch 74 / 500 | iteration 5 / 30 | Total Loss: 5.443155288696289 | KNN Loss: 4.399865627288818 | BCE Loss: 1.0432897806167603\n",
      "Epoch 74 / 500 | iteration 10 / 30 | Total Loss: 5.407406330108643 | KNN Loss: 4.376768589019775 | BCE Loss: 1.0306376218795776\n",
      "Epoch 74 / 500 | iteration 15 / 30 | Total Loss: 5.406500339508057 | KNN Loss: 4.358451843261719 | BCE Loss: 1.048048496246338\n",
      "Epoch 74 / 500 | iteration 20 / 30 | Total Loss: 5.4041428565979 | KNN Loss: 4.394718647003174 | BCE Loss: 1.009424090385437\n",
      "Epoch 74 / 500 | iteration 25 / 30 | Total Loss: 5.400533676147461 | KNN Loss: 4.347176551818848 | BCE Loss: 1.0533571243286133\n",
      "Epoch 75 / 500 | iteration 0 / 30 | Total Loss: 5.400338649749756 | KNN Loss: 4.348257064819336 | BCE Loss: 1.05208158493042\n",
      "Epoch 75 / 500 | iteration 5 / 30 | Total Loss: 5.371382713317871 | KNN Loss: 4.354428768157959 | BCE Loss: 1.016953706741333\n",
      "Epoch 75 / 500 | iteration 10 / 30 | Total Loss: 5.389561176300049 | KNN Loss: 4.335210800170898 | BCE Loss: 1.0543503761291504\n",
      "Epoch 75 / 500 | iteration 15 / 30 | Total Loss: 5.384143352508545 | KNN Loss: 4.382198333740234 | BCE Loss: 1.0019451379776\n",
      "Epoch 75 / 500 | iteration 20 / 30 | Total Loss: 5.396594524383545 | KNN Loss: 4.381267070770264 | BCE Loss: 1.0153274536132812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75 / 500 | iteration 25 / 30 | Total Loss: 5.38702392578125 | KNN Loss: 4.331572532653809 | BCE Loss: 1.0554511547088623\n",
      "Epoch 76 / 500 | iteration 0 / 30 | Total Loss: 5.380053520202637 | KNN Loss: 4.339132785797119 | BCE Loss: 1.0409208536148071\n",
      "Epoch 76 / 500 | iteration 5 / 30 | Total Loss: 5.417142391204834 | KNN Loss: 4.351643085479736 | BCE Loss: 1.0654993057250977\n",
      "Epoch 76 / 500 | iteration 10 / 30 | Total Loss: 5.421656608581543 | KNN Loss: 4.388874053955078 | BCE Loss: 1.032782793045044\n",
      "Epoch 76 / 500 | iteration 15 / 30 | Total Loss: 5.431421279907227 | KNN Loss: 4.381196022033691 | BCE Loss: 1.0502252578735352\n",
      "Epoch 76 / 500 | iteration 20 / 30 | Total Loss: 5.425710678100586 | KNN Loss: 4.402492523193359 | BCE Loss: 1.0232183933258057\n",
      "Epoch 76 / 500 | iteration 25 / 30 | Total Loss: 5.429288387298584 | KNN Loss: 4.405551910400391 | BCE Loss: 1.023736596107483\n",
      "Epoch 77 / 500 | iteration 0 / 30 | Total Loss: 5.424142837524414 | KNN Loss: 4.382375240325928 | BCE Loss: 1.0417675971984863\n",
      "Epoch 77 / 500 | iteration 5 / 30 | Total Loss: 5.432647705078125 | KNN Loss: 4.410935878753662 | BCE Loss: 1.0217115879058838\n",
      "Epoch 77 / 500 | iteration 10 / 30 | Total Loss: 5.399127960205078 | KNN Loss: 4.356610298156738 | BCE Loss: 1.0425176620483398\n",
      "Epoch 77 / 500 | iteration 15 / 30 | Total Loss: 5.385327339172363 | KNN Loss: 4.353760242462158 | BCE Loss: 1.031567096710205\n",
      "Epoch 77 / 500 | iteration 20 / 30 | Total Loss: 5.378969192504883 | KNN Loss: 4.352123260498047 | BCE Loss: 1.026845932006836\n",
      "Epoch 77 / 500 | iteration 25 / 30 | Total Loss: 5.3787617683410645 | KNN Loss: 4.351288795471191 | BCE Loss: 1.027472972869873\n",
      "Epoch 78 / 500 | iteration 0 / 30 | Total Loss: 5.498626708984375 | KNN Loss: 4.447398662567139 | BCE Loss: 1.0512282848358154\n",
      "Epoch 78 / 500 | iteration 5 / 30 | Total Loss: 5.365049362182617 | KNN Loss: 4.348336219787598 | BCE Loss: 1.0167131423950195\n",
      "Epoch 78 / 500 | iteration 10 / 30 | Total Loss: 5.419032096862793 | KNN Loss: 4.379693508148193 | BCE Loss: 1.0393385887145996\n",
      "Epoch 78 / 500 | iteration 15 / 30 | Total Loss: 5.387146949768066 | KNN Loss: 4.362088680267334 | BCE Loss: 1.0250580310821533\n",
      "Epoch 78 / 500 | iteration 20 / 30 | Total Loss: 5.444967269897461 | KNN Loss: 4.401325225830078 | BCE Loss: 1.0436418056488037\n",
      "Epoch 78 / 500 | iteration 25 / 30 | Total Loss: 5.3472113609313965 | KNN Loss: 4.32682466506958 | BCE Loss: 1.0203866958618164\n",
      "Epoch 79 / 500 | iteration 0 / 30 | Total Loss: 5.364439964294434 | KNN Loss: 4.368042945861816 | BCE Loss: 0.9963967800140381\n",
      "Epoch 79 / 500 | iteration 5 / 30 | Total Loss: 5.347860336303711 | KNN Loss: 4.325518608093262 | BCE Loss: 1.0223419666290283\n",
      "Epoch 79 / 500 | iteration 10 / 30 | Total Loss: 5.356732368469238 | KNN Loss: 4.339383125305176 | BCE Loss: 1.0173492431640625\n",
      "Epoch 79 / 500 | iteration 15 / 30 | Total Loss: 5.3744940757751465 | KNN Loss: 4.335255146026611 | BCE Loss: 1.0392390489578247\n",
      "Epoch 79 / 500 | iteration 20 / 30 | Total Loss: 5.433567523956299 | KNN Loss: 4.403099060058594 | BCE Loss: 1.030468463897705\n",
      "Epoch 79 / 500 | iteration 25 / 30 | Total Loss: 5.403924942016602 | KNN Loss: 4.356705665588379 | BCE Loss: 1.0472193956375122\n",
      "Epoch 80 / 500 | iteration 0 / 30 | Total Loss: 5.335781097412109 | KNN Loss: 4.309667587280273 | BCE Loss: 1.026113748550415\n",
      "Epoch 80 / 500 | iteration 5 / 30 | Total Loss: 5.398950576782227 | KNN Loss: 4.381190776824951 | BCE Loss: 1.0177596807479858\n",
      "Epoch 80 / 500 | iteration 10 / 30 | Total Loss: 5.406384468078613 | KNN Loss: 4.372838497161865 | BCE Loss: 1.033545970916748\n",
      "Epoch 80 / 500 | iteration 15 / 30 | Total Loss: 5.404858589172363 | KNN Loss: 4.372502326965332 | BCE Loss: 1.0323561429977417\n",
      "Epoch 80 / 500 | iteration 20 / 30 | Total Loss: 5.3968658447265625 | KNN Loss: 4.373979091644287 | BCE Loss: 1.0228869915008545\n",
      "Epoch 80 / 500 | iteration 25 / 30 | Total Loss: 5.433380126953125 | KNN Loss: 4.384567737579346 | BCE Loss: 1.0488121509552002\n",
      "Epoch 81 / 500 | iteration 0 / 30 | Total Loss: 5.3798508644104 | KNN Loss: 4.367833614349365 | BCE Loss: 1.0120172500610352\n",
      "Epoch 81 / 500 | iteration 5 / 30 | Total Loss: 5.388559818267822 | KNN Loss: 4.357779026031494 | BCE Loss: 1.0307807922363281\n",
      "Epoch 81 / 500 | iteration 10 / 30 | Total Loss: 5.421480655670166 | KNN Loss: 4.3757548332214355 | BCE Loss: 1.045725703239441\n",
      "Epoch 81 / 500 | iteration 15 / 30 | Total Loss: 5.447293758392334 | KNN Loss: 4.409214496612549 | BCE Loss: 1.0380793809890747\n",
      "Epoch 81 / 500 | iteration 20 / 30 | Total Loss: 5.355550765991211 | KNN Loss: 4.330255508422852 | BCE Loss: 1.0252951383590698\n",
      "Epoch 81 / 500 | iteration 25 / 30 | Total Loss: 5.398357391357422 | KNN Loss: 4.3750152587890625 | BCE Loss: 1.0233418941497803\n",
      "Epoch 82 / 500 | iteration 0 / 30 | Total Loss: 5.358615875244141 | KNN Loss: 4.350133895874023 | BCE Loss: 1.0084822177886963\n",
      "Epoch 82 / 500 | iteration 5 / 30 | Total Loss: 5.367371082305908 | KNN Loss: 4.354371547698975 | BCE Loss: 1.0129995346069336\n",
      "Epoch 82 / 500 | iteration 10 / 30 | Total Loss: 5.4172563552856445 | KNN Loss: 4.392528057098389 | BCE Loss: 1.0247280597686768\n",
      "Epoch 82 / 500 | iteration 15 / 30 | Total Loss: 5.427476406097412 | KNN Loss: 4.366486072540283 | BCE Loss: 1.0609904527664185\n",
      "Epoch 82 / 500 | iteration 20 / 30 | Total Loss: 5.337389945983887 | KNN Loss: 4.338383197784424 | BCE Loss: 0.999006986618042\n",
      "Epoch 82 / 500 | iteration 25 / 30 | Total Loss: 5.4056172370910645 | KNN Loss: 4.37650728225708 | BCE Loss: 1.0291099548339844\n",
      "Epoch 83 / 500 | iteration 0 / 30 | Total Loss: 5.343201637268066 | KNN Loss: 4.320858001708984 | BCE Loss: 1.0223437547683716\n",
      "Epoch 83 / 500 | iteration 5 / 30 | Total Loss: 5.416566848754883 | KNN Loss: 4.366563320159912 | BCE Loss: 1.0500036478042603\n",
      "Epoch 83 / 500 | iteration 10 / 30 | Total Loss: 5.404324054718018 | KNN Loss: 4.353784084320068 | BCE Loss: 1.0505400896072388\n",
      "Epoch 83 / 500 | iteration 15 / 30 | Total Loss: 5.3316168785095215 | KNN Loss: 4.326319217681885 | BCE Loss: 1.0052977800369263\n",
      "Epoch 83 / 500 | iteration 20 / 30 | Total Loss: 5.392298698425293 | KNN Loss: 4.363309383392334 | BCE Loss: 1.028989315032959\n",
      "Epoch 83 / 500 | iteration 25 / 30 | Total Loss: 5.393084526062012 | KNN Loss: 4.346665859222412 | BCE Loss: 1.0464184284210205\n",
      "Epoch 84 / 500 | iteration 0 / 30 | Total Loss: 5.354094982147217 | KNN Loss: 4.368739128112793 | BCE Loss: 0.9853559732437134\n",
      "Epoch 84 / 500 | iteration 5 / 30 | Total Loss: 5.329568862915039 | KNN Loss: 4.322207450866699 | BCE Loss: 1.0073612928390503\n",
      "Epoch 84 / 500 | iteration 10 / 30 | Total Loss: 5.371342658996582 | KNN Loss: 4.354076862335205 | BCE Loss: 1.017265796661377\n",
      "Epoch 84 / 500 | iteration 15 / 30 | Total Loss: 5.411529541015625 | KNN Loss: 4.366356372833252 | BCE Loss: 1.0451734066009521\n",
      "Epoch 84 / 500 | iteration 20 / 30 | Total Loss: 5.399876594543457 | KNN Loss: 4.365618705749512 | BCE Loss: 1.0342577695846558\n",
      "Epoch 84 / 500 | iteration 25 / 30 | Total Loss: 5.506479263305664 | KNN Loss: 4.415494441986084 | BCE Loss: 1.09098482131958\n",
      "Epoch 85 / 500 | iteration 0 / 30 | Total Loss: 5.4026103019714355 | KNN Loss: 4.341302394866943 | BCE Loss: 1.0613079071044922\n",
      "Epoch 85 / 500 | iteration 5 / 30 | Total Loss: 5.390292644500732 | KNN Loss: 4.363487243652344 | BCE Loss: 1.0268055200576782\n",
      "Epoch 85 / 500 | iteration 10 / 30 | Total Loss: 5.398641586303711 | KNN Loss: 4.355161190032959 | BCE Loss: 1.043480634689331\n",
      "Epoch 85 / 500 | iteration 15 / 30 | Total Loss: 5.404537677764893 | KNN Loss: 4.391475677490234 | BCE Loss: 1.0130620002746582\n",
      "Epoch 85 / 500 | iteration 20 / 30 | Total Loss: 5.3381500244140625 | KNN Loss: 4.3260297775268555 | BCE Loss: 1.012120008468628\n",
      "Epoch 85 / 500 | iteration 25 / 30 | Total Loss: 5.339287281036377 | KNN Loss: 4.332030296325684 | BCE Loss: 1.0072569847106934\n",
      "Epoch    86: reducing learning rate of group 0 to 3.5000e-03.\n",
      "Epoch 86 / 500 | iteration 0 / 30 | Total Loss: 5.360888481140137 | KNN Loss: 4.334649085998535 | BCE Loss: 1.0262391567230225\n",
      "Epoch 86 / 500 | iteration 5 / 30 | Total Loss: 5.402112007141113 | KNN Loss: 4.363065242767334 | BCE Loss: 1.0390467643737793\n",
      "Epoch 86 / 500 | iteration 10 / 30 | Total Loss: 5.389768600463867 | KNN Loss: 4.383200168609619 | BCE Loss: 1.0065683126449585\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86 / 500 | iteration 15 / 30 | Total Loss: 5.451026916503906 | KNN Loss: 4.4105448722839355 | BCE Loss: 1.0404820442199707\n",
      "Epoch 86 / 500 | iteration 20 / 30 | Total Loss: 5.399460315704346 | KNN Loss: 4.376109600067139 | BCE Loss: 1.023350715637207\n",
      "Epoch 86 / 500 | iteration 25 / 30 | Total Loss: 5.343379020690918 | KNN Loss: 4.310970783233643 | BCE Loss: 1.0324079990386963\n",
      "Epoch 87 / 500 | iteration 0 / 30 | Total Loss: 5.369244575500488 | KNN Loss: 4.3475751876831055 | BCE Loss: 1.021669626235962\n",
      "Epoch 87 / 500 | iteration 5 / 30 | Total Loss: 5.364074230194092 | KNN Loss: 4.318551063537598 | BCE Loss: 1.0455231666564941\n",
      "Epoch 87 / 500 | iteration 10 / 30 | Total Loss: 5.330653190612793 | KNN Loss: 4.3121795654296875 | BCE Loss: 1.0184736251831055\n",
      "Epoch 87 / 500 | iteration 15 / 30 | Total Loss: 5.37527322769165 | KNN Loss: 4.33676290512085 | BCE Loss: 1.0385102033615112\n",
      "Epoch 87 / 500 | iteration 20 / 30 | Total Loss: 5.41039514541626 | KNN Loss: 4.382853984832764 | BCE Loss: 1.0275410413742065\n",
      "Epoch 87 / 500 | iteration 25 / 30 | Total Loss: 5.3929901123046875 | KNN Loss: 4.344823837280273 | BCE Loss: 1.0481663942337036\n",
      "Epoch 88 / 500 | iteration 0 / 30 | Total Loss: 5.364996910095215 | KNN Loss: 4.331914901733398 | BCE Loss: 1.0330822467803955\n",
      "Epoch 88 / 500 | iteration 5 / 30 | Total Loss: 5.338996887207031 | KNN Loss: 4.327619552612305 | BCE Loss: 1.0113773345947266\n",
      "Epoch 88 / 500 | iteration 10 / 30 | Total Loss: 5.353695869445801 | KNN Loss: 4.345881938934326 | BCE Loss: 1.0078139305114746\n",
      "Epoch 88 / 500 | iteration 15 / 30 | Total Loss: 5.3573527336120605 | KNN Loss: 4.326435565948486 | BCE Loss: 1.0309172868728638\n",
      "Epoch 88 / 500 | iteration 20 / 30 | Total Loss: 5.414266586303711 | KNN Loss: 4.378242492675781 | BCE Loss: 1.0360240936279297\n",
      "Epoch 88 / 500 | iteration 25 / 30 | Total Loss: 5.333432674407959 | KNN Loss: 4.323117733001709 | BCE Loss: 1.0103148221969604\n",
      "Epoch 89 / 500 | iteration 0 / 30 | Total Loss: 5.372501850128174 | KNN Loss: 4.3611578941345215 | BCE Loss: 1.0113439559936523\n",
      "Epoch 89 / 500 | iteration 5 / 30 | Total Loss: 5.383189678192139 | KNN Loss: 4.364331245422363 | BCE Loss: 1.0188584327697754\n",
      "Epoch 89 / 500 | iteration 10 / 30 | Total Loss: 5.3887152671813965 | KNN Loss: 4.366524696350098 | BCE Loss: 1.0221905708312988\n",
      "Epoch 89 / 500 | iteration 15 / 30 | Total Loss: 5.449998378753662 | KNN Loss: 4.377656936645508 | BCE Loss: 1.0723414421081543\n",
      "Epoch 89 / 500 | iteration 20 / 30 | Total Loss: 5.459949016571045 | KNN Loss: 4.4106316566467285 | BCE Loss: 1.0493173599243164\n",
      "Epoch 89 / 500 | iteration 25 / 30 | Total Loss: 5.353034019470215 | KNN Loss: 4.354329586029053 | BCE Loss: 0.9987042546272278\n",
      "Epoch 90 / 500 | iteration 0 / 30 | Total Loss: 5.363666534423828 | KNN Loss: 4.3322553634643555 | BCE Loss: 1.0314114093780518\n",
      "Epoch 90 / 500 | iteration 5 / 30 | Total Loss: 5.430520057678223 | KNN Loss: 4.38355016708374 | BCE Loss: 1.0469698905944824\n",
      "Epoch 90 / 500 | iteration 10 / 30 | Total Loss: 5.392111778259277 | KNN Loss: 4.382742404937744 | BCE Loss: 1.0093692541122437\n",
      "Epoch 90 / 500 | iteration 15 / 30 | Total Loss: 5.370580673217773 | KNN Loss: 4.362847328186035 | BCE Loss: 1.0077333450317383\n",
      "Epoch 90 / 500 | iteration 20 / 30 | Total Loss: 5.3795671463012695 | KNN Loss: 4.366241931915283 | BCE Loss: 1.0133252143859863\n",
      "Epoch 90 / 500 | iteration 25 / 30 | Total Loss: 5.401477813720703 | KNN Loss: 4.357802867889404 | BCE Loss: 1.043675184249878\n",
      "Epoch 91 / 500 | iteration 0 / 30 | Total Loss: 5.3643646240234375 | KNN Loss: 4.351747035980225 | BCE Loss: 1.012617588043213\n",
      "Epoch 91 / 500 | iteration 5 / 30 | Total Loss: 5.415760040283203 | KNN Loss: 4.378117084503174 | BCE Loss: 1.0376427173614502\n",
      "Epoch 91 / 500 | iteration 10 / 30 | Total Loss: 5.441371917724609 | KNN Loss: 4.379136562347412 | BCE Loss: 1.0622355937957764\n",
      "Epoch 91 / 500 | iteration 15 / 30 | Total Loss: 5.412331581115723 | KNN Loss: 4.3631720542907715 | BCE Loss: 1.0491595268249512\n",
      "Epoch 91 / 500 | iteration 20 / 30 | Total Loss: 5.3974833488464355 | KNN Loss: 4.367190361022949 | BCE Loss: 1.0302928686141968\n",
      "Epoch 91 / 500 | iteration 25 / 30 | Total Loss: 5.378393650054932 | KNN Loss: 4.3773980140686035 | BCE Loss: 1.0009955167770386\n",
      "Epoch 92 / 500 | iteration 0 / 30 | Total Loss: 5.403257369995117 | KNN Loss: 4.386870384216309 | BCE Loss: 1.0163867473602295\n",
      "Epoch 92 / 500 | iteration 5 / 30 | Total Loss: 5.376989364624023 | KNN Loss: 4.343262672424316 | BCE Loss: 1.0337268114089966\n",
      "Epoch 92 / 500 | iteration 10 / 30 | Total Loss: 5.329606533050537 | KNN Loss: 4.323965549468994 | BCE Loss: 1.005640983581543\n",
      "Epoch 92 / 500 | iteration 15 / 30 | Total Loss: 5.428187370300293 | KNN Loss: 4.404230117797852 | BCE Loss: 1.0239572525024414\n",
      "Epoch 92 / 500 | iteration 20 / 30 | Total Loss: 5.365094184875488 | KNN Loss: 4.3327741622924805 | BCE Loss: 1.0323197841644287\n",
      "Epoch 92 / 500 | iteration 25 / 30 | Total Loss: 5.386865615844727 | KNN Loss: 4.360114097595215 | BCE Loss: 1.0267516374588013\n",
      "Epoch 93 / 500 | iteration 0 / 30 | Total Loss: 5.3402557373046875 | KNN Loss: 4.327902317047119 | BCE Loss: 1.0123534202575684\n",
      "Epoch 93 / 500 | iteration 5 / 30 | Total Loss: 5.397153377532959 | KNN Loss: 4.382278919219971 | BCE Loss: 1.0148744583129883\n",
      "Epoch 93 / 500 | iteration 10 / 30 | Total Loss: 5.38226842880249 | KNN Loss: 4.357650279998779 | BCE Loss: 1.0246180295944214\n",
      "Epoch 93 / 500 | iteration 15 / 30 | Total Loss: 5.412684440612793 | KNN Loss: 4.3703765869140625 | BCE Loss: 1.04230797290802\n",
      "Epoch 93 / 500 | iteration 20 / 30 | Total Loss: 5.417633056640625 | KNN Loss: 4.365971565246582 | BCE Loss: 1.0516612529754639\n",
      "Epoch 93 / 500 | iteration 25 / 30 | Total Loss: 5.364757537841797 | KNN Loss: 4.346199989318848 | BCE Loss: 1.0185575485229492\n",
      "Epoch 94 / 500 | iteration 0 / 30 | Total Loss: 5.378252983093262 | KNN Loss: 4.359691143035889 | BCE Loss: 1.018561840057373\n",
      "Epoch 94 / 500 | iteration 5 / 30 | Total Loss: 5.381832122802734 | KNN Loss: 4.354519844055176 | BCE Loss: 1.0273123979568481\n",
      "Epoch 94 / 500 | iteration 10 / 30 | Total Loss: 5.46594762802124 | KNN Loss: 4.410332202911377 | BCE Loss: 1.0556155443191528\n",
      "Epoch 94 / 500 | iteration 15 / 30 | Total Loss: 5.425967216491699 | KNN Loss: 4.3919830322265625 | BCE Loss: 1.0339840650558472\n",
      "Epoch 94 / 500 | iteration 20 / 30 | Total Loss: 5.390658378601074 | KNN Loss: 4.346062183380127 | BCE Loss: 1.0445964336395264\n",
      "Epoch 94 / 500 | iteration 25 / 30 | Total Loss: 5.4104156494140625 | KNN Loss: 4.391561985015869 | BCE Loss: 1.0188535451889038\n",
      "Epoch 95 / 500 | iteration 0 / 30 | Total Loss: 5.358733654022217 | KNN Loss: 4.336881637573242 | BCE Loss: 1.0218520164489746\n",
      "Epoch 95 / 500 | iteration 5 / 30 | Total Loss: 5.3569464683532715 | KNN Loss: 4.347363471984863 | BCE Loss: 1.0095828771591187\n",
      "Epoch 95 / 500 | iteration 10 / 30 | Total Loss: 5.380200386047363 | KNN Loss: 4.337381839752197 | BCE Loss: 1.0428186655044556\n",
      "Epoch 95 / 500 | iteration 15 / 30 | Total Loss: 5.364375114440918 | KNN Loss: 4.330053329467773 | BCE Loss: 1.0343217849731445\n",
      "Epoch 95 / 500 | iteration 20 / 30 | Total Loss: 5.362922668457031 | KNN Loss: 4.316112518310547 | BCE Loss: 1.0468100309371948\n",
      "Epoch 95 / 500 | iteration 25 / 30 | Total Loss: 5.373742580413818 | KNN Loss: 4.357114315032959 | BCE Loss: 1.0166282653808594\n",
      "Epoch 96 / 500 | iteration 0 / 30 | Total Loss: 5.370630264282227 | KNN Loss: 4.360326290130615 | BCE Loss: 1.0103040933609009\n",
      "Epoch 96 / 500 | iteration 5 / 30 | Total Loss: 5.362436294555664 | KNN Loss: 4.329478740692139 | BCE Loss: 1.0329573154449463\n",
      "Epoch 96 / 500 | iteration 10 / 30 | Total Loss: 5.3278703689575195 | KNN Loss: 4.3209123611450195 | BCE Loss: 1.006958246231079\n",
      "Epoch 96 / 500 | iteration 15 / 30 | Total Loss: 5.396004676818848 | KNN Loss: 4.364125728607178 | BCE Loss: 1.0318787097930908\n",
      "Epoch 96 / 500 | iteration 20 / 30 | Total Loss: 5.393657207489014 | KNN Loss: 4.3685102462768555 | BCE Loss: 1.0251470804214478\n",
      "Epoch 96 / 500 | iteration 25 / 30 | Total Loss: 5.379507064819336 | KNN Loss: 4.339178562164307 | BCE Loss: 1.0403287410736084\n",
      "Epoch 97 / 500 | iteration 0 / 30 | Total Loss: 5.3941450119018555 | KNN Loss: 4.381479740142822 | BCE Loss: 1.0126652717590332\n",
      "Epoch 97 / 500 | iteration 5 / 30 | Total Loss: 5.357079982757568 | KNN Loss: 4.335455417633057 | BCE Loss: 1.0216244459152222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97 / 500 | iteration 10 / 30 | Total Loss: 5.375739097595215 | KNN Loss: 4.346031188964844 | BCE Loss: 1.0297081470489502\n",
      "Epoch 97 / 500 | iteration 15 / 30 | Total Loss: 5.378602504730225 | KNN Loss: 4.352100849151611 | BCE Loss: 1.0265016555786133\n",
      "Epoch 97 / 500 | iteration 20 / 30 | Total Loss: 5.336757183074951 | KNN Loss: 4.333893299102783 | BCE Loss: 1.002863883972168\n",
      "Epoch 97 / 500 | iteration 25 / 30 | Total Loss: 5.354245185852051 | KNN Loss: 4.33836555480957 | BCE Loss: 1.0158798694610596\n",
      "Epoch 98 / 500 | iteration 0 / 30 | Total Loss: 5.350261211395264 | KNN Loss: 4.34181022644043 | BCE Loss: 1.0084511041641235\n",
      "Epoch 98 / 500 | iteration 5 / 30 | Total Loss: 5.39853572845459 | KNN Loss: 4.36923885345459 | BCE Loss: 1.029296636581421\n",
      "Epoch 98 / 500 | iteration 10 / 30 | Total Loss: 5.356836318969727 | KNN Loss: 4.332324504852295 | BCE Loss: 1.0245115756988525\n",
      "Epoch 98 / 500 | iteration 15 / 30 | Total Loss: 5.3523383140563965 | KNN Loss: 4.3183674812316895 | BCE Loss: 1.033970832824707\n",
      "Epoch 98 / 500 | iteration 20 / 30 | Total Loss: 5.370089054107666 | KNN Loss: 4.328390121459961 | BCE Loss: 1.0416988134384155\n",
      "Epoch 98 / 500 | iteration 25 / 30 | Total Loss: 5.404635429382324 | KNN Loss: 4.363155841827393 | BCE Loss: 1.0414795875549316\n",
      "Epoch 99 / 500 | iteration 0 / 30 | Total Loss: 5.370909690856934 | KNN Loss: 4.350699424743652 | BCE Loss: 1.0202100276947021\n",
      "Epoch 99 / 500 | iteration 5 / 30 | Total Loss: 5.381470203399658 | KNN Loss: 4.358234405517578 | BCE Loss: 1.02323579788208\n",
      "Epoch 99 / 500 | iteration 10 / 30 | Total Loss: 5.358036994934082 | KNN Loss: 4.3465704917907715 | BCE Loss: 1.0114662647247314\n",
      "Epoch 99 / 500 | iteration 15 / 30 | Total Loss: 5.417701244354248 | KNN Loss: 4.3961567878723145 | BCE Loss: 1.0215444564819336\n",
      "Epoch 99 / 500 | iteration 20 / 30 | Total Loss: 5.384181022644043 | KNN Loss: 4.368943691253662 | BCE Loss: 1.0152373313903809\n",
      "Epoch 99 / 500 | iteration 25 / 30 | Total Loss: 5.430235862731934 | KNN Loss: 4.395623207092285 | BCE Loss: 1.0346128940582275\n",
      "Epoch 100 / 500 | iteration 0 / 30 | Total Loss: 5.411805152893066 | KNN Loss: 4.375511646270752 | BCE Loss: 1.0362937450408936\n",
      "Epoch 100 / 500 | iteration 5 / 30 | Total Loss: 5.402145862579346 | KNN Loss: 4.345461845397949 | BCE Loss: 1.0566840171813965\n",
      "Epoch 100 / 500 | iteration 10 / 30 | Total Loss: 5.376169204711914 | KNN Loss: 4.3483405113220215 | BCE Loss: 1.0278289318084717\n",
      "Epoch 100 / 500 | iteration 15 / 30 | Total Loss: 5.378042221069336 | KNN Loss: 4.339518070220947 | BCE Loss: 1.0385243892669678\n",
      "Epoch 100 / 500 | iteration 20 / 30 | Total Loss: 5.369256019592285 | KNN Loss: 4.3598809242248535 | BCE Loss: 1.0093748569488525\n",
      "Epoch 100 / 500 | iteration 25 / 30 | Total Loss: 5.375833034515381 | KNN Loss: 4.337072849273682 | BCE Loss: 1.0387603044509888\n",
      "Epoch 101 / 500 | iteration 0 / 30 | Total Loss: 5.407156944274902 | KNN Loss: 4.363164901733398 | BCE Loss: 1.0439919233322144\n",
      "Epoch 101 / 500 | iteration 5 / 30 | Total Loss: 5.424691200256348 | KNN Loss: 4.3866143226623535 | BCE Loss: 1.0380768775939941\n",
      "Epoch 101 / 500 | iteration 10 / 30 | Total Loss: 5.397549629211426 | KNN Loss: 4.37337064743042 | BCE Loss: 1.024179220199585\n",
      "Epoch 101 / 500 | iteration 15 / 30 | Total Loss: 5.33980131149292 | KNN Loss: 4.336905002593994 | BCE Loss: 1.0028963088989258\n",
      "Epoch 101 / 500 | iteration 20 / 30 | Total Loss: 5.394282341003418 | KNN Loss: 4.351710319519043 | BCE Loss: 1.042572259902954\n",
      "Epoch 101 / 500 | iteration 25 / 30 | Total Loss: 5.430619239807129 | KNN Loss: 4.386087894439697 | BCE Loss: 1.044531226158142\n",
      "Epoch 102 / 500 | iteration 0 / 30 | Total Loss: 5.409189224243164 | KNN Loss: 4.3850836753845215 | BCE Loss: 1.0241057872772217\n",
      "Epoch 102 / 500 | iteration 5 / 30 | Total Loss: 5.387864112854004 | KNN Loss: 4.374300956726074 | BCE Loss: 1.0135633945465088\n",
      "Epoch 102 / 500 | iteration 10 / 30 | Total Loss: 5.358852386474609 | KNN Loss: 4.341238498687744 | BCE Loss: 1.0176141262054443\n",
      "Epoch 102 / 500 | iteration 15 / 30 | Total Loss: 5.389176368713379 | KNN Loss: 4.36580228805542 | BCE Loss: 1.023374319076538\n",
      "Epoch 102 / 500 | iteration 20 / 30 | Total Loss: 5.36882209777832 | KNN Loss: 4.338598728179932 | BCE Loss: 1.0302231311798096\n",
      "Epoch 102 / 500 | iteration 25 / 30 | Total Loss: 5.401355266571045 | KNN Loss: 4.365108013153076 | BCE Loss: 1.0362472534179688\n",
      "Epoch 103 / 500 | iteration 0 / 30 | Total Loss: 5.401246070861816 | KNN Loss: 4.366008281707764 | BCE Loss: 1.0352380275726318\n",
      "Epoch 103 / 500 | iteration 5 / 30 | Total Loss: 5.419212341308594 | KNN Loss: 4.380843639373779 | BCE Loss: 1.0383687019348145\n",
      "Epoch 103 / 500 | iteration 10 / 30 | Total Loss: 5.426427364349365 | KNN Loss: 4.393645763397217 | BCE Loss: 1.0327816009521484\n",
      "Epoch 103 / 500 | iteration 15 / 30 | Total Loss: 5.364559173583984 | KNN Loss: 4.341662883758545 | BCE Loss: 1.0228965282440186\n",
      "Epoch 103 / 500 | iteration 20 / 30 | Total Loss: 5.364571571350098 | KNN Loss: 4.339917182922363 | BCE Loss: 1.024654507637024\n",
      "Epoch 103 / 500 | iteration 25 / 30 | Total Loss: 5.380915641784668 | KNN Loss: 4.366504192352295 | BCE Loss: 1.014411449432373\n",
      "Epoch 104 / 500 | iteration 0 / 30 | Total Loss: 5.372267723083496 | KNN Loss: 4.368661880493164 | BCE Loss: 1.0036060810089111\n",
      "Epoch 104 / 500 | iteration 5 / 30 | Total Loss: 5.479206085205078 | KNN Loss: 4.436290264129639 | BCE Loss: 1.0429155826568604\n",
      "Epoch 104 / 500 | iteration 10 / 30 | Total Loss: 5.3763837814331055 | KNN Loss: 4.3554368019104 | BCE Loss: 1.020946741104126\n",
      "Epoch 104 / 500 | iteration 15 / 30 | Total Loss: 5.383382797241211 | KNN Loss: 4.3365583419799805 | BCE Loss: 1.0468246936798096\n",
      "Epoch 104 / 500 | iteration 20 / 30 | Total Loss: 5.437736511230469 | KNN Loss: 4.367499351501465 | BCE Loss: 1.0702369213104248\n",
      "Epoch 104 / 500 | iteration 25 / 30 | Total Loss: 5.342041969299316 | KNN Loss: 4.332562446594238 | BCE Loss: 1.0094797611236572\n",
      "Epoch 105 / 500 | iteration 0 / 30 | Total Loss: 5.385307312011719 | KNN Loss: 4.354515075683594 | BCE Loss: 1.030792474746704\n",
      "Epoch 105 / 500 | iteration 5 / 30 | Total Loss: 5.386780261993408 | KNN Loss: 4.374969959259033 | BCE Loss: 1.0118101835250854\n",
      "Epoch 105 / 500 | iteration 10 / 30 | Total Loss: 5.396492958068848 | KNN Loss: 4.35167121887207 | BCE Loss: 1.0448217391967773\n",
      "Epoch 105 / 500 | iteration 15 / 30 | Total Loss: 5.388241767883301 | KNN Loss: 4.370403289794922 | BCE Loss: 1.017838478088379\n",
      "Epoch 105 / 500 | iteration 20 / 30 | Total Loss: 5.339957237243652 | KNN Loss: 4.336272239685059 | BCE Loss: 1.0036847591400146\n",
      "Epoch 105 / 500 | iteration 25 / 30 | Total Loss: 5.37799072265625 | KNN Loss: 4.362174987792969 | BCE Loss: 1.0158159732818604\n",
      "Epoch 106 / 500 | iteration 0 / 30 | Total Loss: 5.348074913024902 | KNN Loss: 4.336953163146973 | BCE Loss: 1.0111219882965088\n",
      "Epoch 106 / 500 | iteration 5 / 30 | Total Loss: 5.371959686279297 | KNN Loss: 4.328034400939941 | BCE Loss: 1.0439252853393555\n",
      "Epoch 106 / 500 | iteration 10 / 30 | Total Loss: 5.386573791503906 | KNN Loss: 4.3451738357543945 | BCE Loss: 1.0413998365402222\n",
      "Epoch 106 / 500 | iteration 15 / 30 | Total Loss: 5.409738540649414 | KNN Loss: 4.350289344787598 | BCE Loss: 1.059449315071106\n",
      "Epoch 106 / 500 | iteration 20 / 30 | Total Loss: 5.381113529205322 | KNN Loss: 4.3531575202941895 | BCE Loss: 1.0279560089111328\n",
      "Epoch 106 / 500 | iteration 25 / 30 | Total Loss: 5.356038570404053 | KNN Loss: 4.330941200256348 | BCE Loss: 1.0250972509384155\n",
      "Epoch   107: reducing learning rate of group 0 to 2.4500e-03.\n",
      "Epoch 107 / 500 | iteration 0 / 30 | Total Loss: 5.356054782867432 | KNN Loss: 4.356255531311035 | BCE Loss: 0.9997992515563965\n",
      "Epoch 107 / 500 | iteration 5 / 30 | Total Loss: 5.364297389984131 | KNN Loss: 4.329671382904053 | BCE Loss: 1.0346260070800781\n",
      "Epoch 107 / 500 | iteration 10 / 30 | Total Loss: 5.400266647338867 | KNN Loss: 4.349144458770752 | BCE Loss: 1.0511219501495361\n",
      "Epoch 107 / 500 | iteration 15 / 30 | Total Loss: 5.311537265777588 | KNN Loss: 4.322882175445557 | BCE Loss: 0.9886549711227417\n",
      "Epoch 107 / 500 | iteration 20 / 30 | Total Loss: 5.353231906890869 | KNN Loss: 4.334769248962402 | BCE Loss: 1.0184626579284668\n",
      "Epoch 107 / 500 | iteration 25 / 30 | Total Loss: 5.345574378967285 | KNN Loss: 4.313660144805908 | BCE Loss: 1.0319139957427979\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 108 / 500 | iteration 0 / 30 | Total Loss: 5.446849822998047 | KNN Loss: 4.411737442016602 | BCE Loss: 1.0351126194000244\n",
      "Epoch 108 / 500 | iteration 5 / 30 | Total Loss: 5.4523444175720215 | KNN Loss: 4.451930046081543 | BCE Loss: 1.0004143714904785\n",
      "Epoch 108 / 500 | iteration 10 / 30 | Total Loss: 5.371026039123535 | KNN Loss: 4.361382961273193 | BCE Loss: 1.009643316268921\n",
      "Epoch 108 / 500 | iteration 15 / 30 | Total Loss: 5.370291233062744 | KNN Loss: 4.333562850952148 | BCE Loss: 1.0367285013198853\n",
      "Epoch 108 / 500 | iteration 20 / 30 | Total Loss: 5.396492004394531 | KNN Loss: 4.350986480712891 | BCE Loss: 1.0455056428909302\n",
      "Epoch 108 / 500 | iteration 25 / 30 | Total Loss: 5.3906168937683105 | KNN Loss: 4.359808921813965 | BCE Loss: 1.0308078527450562\n",
      "Epoch 109 / 500 | iteration 0 / 30 | Total Loss: 5.367439270019531 | KNN Loss: 4.340241432189941 | BCE Loss: 1.0271977186203003\n",
      "Epoch 109 / 500 | iteration 5 / 30 | Total Loss: 5.378763675689697 | KNN Loss: 4.3424153327941895 | BCE Loss: 1.0363484621047974\n",
      "Epoch 109 / 500 | iteration 10 / 30 | Total Loss: 5.366499423980713 | KNN Loss: 4.337764739990234 | BCE Loss: 1.028734564781189\n",
      "Epoch 109 / 500 | iteration 15 / 30 | Total Loss: 5.320638179779053 | KNN Loss: 4.331887245178223 | BCE Loss: 0.9887508153915405\n",
      "Epoch 109 / 500 | iteration 20 / 30 | Total Loss: 5.398401260375977 | KNN Loss: 4.372557640075684 | BCE Loss: 1.0258433818817139\n",
      "Epoch 109 / 500 | iteration 25 / 30 | Total Loss: 5.352110862731934 | KNN Loss: 4.337267875671387 | BCE Loss: 1.0148427486419678\n",
      "Epoch 110 / 500 | iteration 0 / 30 | Total Loss: 5.358035087585449 | KNN Loss: 4.33019495010376 | BCE Loss: 1.0278401374816895\n",
      "Epoch 110 / 500 | iteration 5 / 30 | Total Loss: 5.349217414855957 | KNN Loss: 4.335593223571777 | BCE Loss: 1.0136244297027588\n",
      "Epoch 110 / 500 | iteration 10 / 30 | Total Loss: 5.426401138305664 | KNN Loss: 4.402524471282959 | BCE Loss: 1.023876428604126\n",
      "Epoch 110 / 500 | iteration 15 / 30 | Total Loss: 5.380581855773926 | KNN Loss: 4.361563682556152 | BCE Loss: 1.0190184116363525\n",
      "Epoch 110 / 500 | iteration 20 / 30 | Total Loss: 5.336804389953613 | KNN Loss: 4.336189270019531 | BCE Loss: 1.0006153583526611\n",
      "Epoch 110 / 500 | iteration 25 / 30 | Total Loss: 5.41481351852417 | KNN Loss: 4.355601787567139 | BCE Loss: 1.0592117309570312\n",
      "Epoch 111 / 500 | iteration 0 / 30 | Total Loss: 5.435020923614502 | KNN Loss: 4.370753765106201 | BCE Loss: 1.0642671585083008\n",
      "Epoch 111 / 500 | iteration 5 / 30 | Total Loss: 5.3643646240234375 | KNN Loss: 4.348168849945068 | BCE Loss: 1.0161960124969482\n",
      "Epoch 111 / 500 | iteration 10 / 30 | Total Loss: 5.381452560424805 | KNN Loss: 4.385998249053955 | BCE Loss: 0.9954541921615601\n",
      "Epoch 111 / 500 | iteration 15 / 30 | Total Loss: 5.369890213012695 | KNN Loss: 4.322221755981445 | BCE Loss: 1.0476683378219604\n",
      "Epoch 111 / 500 | iteration 20 / 30 | Total Loss: 5.413747787475586 | KNN Loss: 4.393063545227051 | BCE Loss: 1.020684003829956\n",
      "Epoch 111 / 500 | iteration 25 / 30 | Total Loss: 5.4321465492248535 | KNN Loss: 4.381148815155029 | BCE Loss: 1.0509976148605347\n",
      "Epoch 112 / 500 | iteration 0 / 30 | Total Loss: 5.4484076499938965 | KNN Loss: 4.396852493286133 | BCE Loss: 1.0515550374984741\n",
      "Epoch 112 / 500 | iteration 5 / 30 | Total Loss: 5.345970630645752 | KNN Loss: 4.329539775848389 | BCE Loss: 1.0164308547973633\n",
      "Epoch 112 / 500 | iteration 10 / 30 | Total Loss: 5.410023212432861 | KNN Loss: 4.388219833374023 | BCE Loss: 1.021803379058838\n",
      "Epoch 112 / 500 | iteration 15 / 30 | Total Loss: 5.376999855041504 | KNN Loss: 4.350618839263916 | BCE Loss: 1.026381254196167\n",
      "Epoch 112 / 500 | iteration 20 / 30 | Total Loss: 5.347689151763916 | KNN Loss: 4.329714298248291 | BCE Loss: 1.0179749727249146\n",
      "Epoch 112 / 500 | iteration 25 / 30 | Total Loss: 5.382511615753174 | KNN Loss: 4.363231658935547 | BCE Loss: 1.0192800760269165\n",
      "Epoch 113 / 500 | iteration 0 / 30 | Total Loss: 5.322193145751953 | KNN Loss: 4.323990821838379 | BCE Loss: 0.9982023239135742\n",
      "Epoch 113 / 500 | iteration 5 / 30 | Total Loss: 5.386725902557373 | KNN Loss: 4.343958377838135 | BCE Loss: 1.0427675247192383\n",
      "Epoch 113 / 500 | iteration 10 / 30 | Total Loss: 5.398483753204346 | KNN Loss: 4.403615951538086 | BCE Loss: 0.9948679208755493\n",
      "Epoch 113 / 500 | iteration 15 / 30 | Total Loss: 5.387951374053955 | KNN Loss: 4.361321449279785 | BCE Loss: 1.02662992477417\n",
      "Epoch 113 / 500 | iteration 20 / 30 | Total Loss: 5.428657531738281 | KNN Loss: 4.355440616607666 | BCE Loss: 1.0732169151306152\n",
      "Epoch 113 / 500 | iteration 25 / 30 | Total Loss: 5.395112991333008 | KNN Loss: 4.3582682609558105 | BCE Loss: 1.0368449687957764\n",
      "Epoch 114 / 500 | iteration 0 / 30 | Total Loss: 5.381776332855225 | KNN Loss: 4.352977275848389 | BCE Loss: 1.028799057006836\n",
      "Epoch 114 / 500 | iteration 5 / 30 | Total Loss: 5.374359607696533 | KNN Loss: 4.332859516143799 | BCE Loss: 1.0415000915527344\n",
      "Epoch 114 / 500 | iteration 10 / 30 | Total Loss: 5.378363132476807 | KNN Loss: 4.355619430541992 | BCE Loss: 1.022743821144104\n",
      "Epoch 114 / 500 | iteration 15 / 30 | Total Loss: 5.3372273445129395 | KNN Loss: 4.337740898132324 | BCE Loss: 0.9994862675666809\n",
      "Epoch 114 / 500 | iteration 20 / 30 | Total Loss: 5.400766372680664 | KNN Loss: 4.368308067321777 | BCE Loss: 1.0324585437774658\n",
      "Epoch 114 / 500 | iteration 25 / 30 | Total Loss: 5.419266700744629 | KNN Loss: 4.36885404586792 | BCE Loss: 1.050412654876709\n",
      "Epoch 115 / 500 | iteration 0 / 30 | Total Loss: 5.400341033935547 | KNN Loss: 4.352519512176514 | BCE Loss: 1.0478217601776123\n",
      "Epoch 115 / 500 | iteration 5 / 30 | Total Loss: 5.393068313598633 | KNN Loss: 4.377884864807129 | BCE Loss: 1.015183687210083\n",
      "Epoch 115 / 500 | iteration 10 / 30 | Total Loss: 5.354424476623535 | KNN Loss: 4.334917068481445 | BCE Loss: 1.0195075273513794\n",
      "Epoch 115 / 500 | iteration 15 / 30 | Total Loss: 5.38320255279541 | KNN Loss: 4.365375995635986 | BCE Loss: 1.017826795578003\n",
      "Epoch 115 / 500 | iteration 20 / 30 | Total Loss: 5.32029914855957 | KNN Loss: 4.322492599487305 | BCE Loss: 0.9978064298629761\n",
      "Epoch 115 / 500 | iteration 25 / 30 | Total Loss: 5.369240760803223 | KNN Loss: 4.330704212188721 | BCE Loss: 1.0385363101959229\n",
      "Epoch 116 / 500 | iteration 0 / 30 | Total Loss: 5.393502712249756 | KNN Loss: 4.355404376983643 | BCE Loss: 1.0380982160568237\n",
      "Epoch 116 / 500 | iteration 5 / 30 | Total Loss: 5.396427631378174 | KNN Loss: 4.360898971557617 | BCE Loss: 1.0355286598205566\n",
      "Epoch 116 / 500 | iteration 10 / 30 | Total Loss: 5.4091315269470215 | KNN Loss: 4.386453628540039 | BCE Loss: 1.022678017616272\n",
      "Epoch 116 / 500 | iteration 15 / 30 | Total Loss: 5.415640830993652 | KNN Loss: 4.375933647155762 | BCE Loss: 1.0397071838378906\n",
      "Epoch 116 / 500 | iteration 20 / 30 | Total Loss: 5.379541397094727 | KNN Loss: 4.364505767822266 | BCE Loss: 1.0150353908538818\n",
      "Epoch 116 / 500 | iteration 25 / 30 | Total Loss: 5.423746109008789 | KNN Loss: 4.370137691497803 | BCE Loss: 1.0536084175109863\n",
      "Epoch 117 / 500 | iteration 0 / 30 | Total Loss: 5.352049350738525 | KNN Loss: 4.346707820892334 | BCE Loss: 1.0053414106369019\n",
      "Epoch 117 / 500 | iteration 5 / 30 | Total Loss: 5.383370399475098 | KNN Loss: 4.361345291137695 | BCE Loss: 1.0220253467559814\n",
      "Epoch 117 / 500 | iteration 10 / 30 | Total Loss: 5.329389572143555 | KNN Loss: 4.303791522979736 | BCE Loss: 1.0255982875823975\n",
      "Epoch 117 / 500 | iteration 15 / 30 | Total Loss: 5.334442138671875 | KNN Loss: 4.335968017578125 | BCE Loss: 0.9984740614891052\n",
      "Epoch 117 / 500 | iteration 20 / 30 | Total Loss: 5.354250907897949 | KNN Loss: 4.343901634216309 | BCE Loss: 1.0103490352630615\n",
      "Epoch 117 / 500 | iteration 25 / 30 | Total Loss: 5.389244079589844 | KNN Loss: 4.335070610046387 | BCE Loss: 1.054173231124878\n",
      "Epoch 118 / 500 | iteration 0 / 30 | Total Loss: 5.367644786834717 | KNN Loss: 4.3423237800598145 | BCE Loss: 1.0253208875656128\n",
      "Epoch 118 / 500 | iteration 5 / 30 | Total Loss: 5.403477191925049 | KNN Loss: 4.381474494934082 | BCE Loss: 1.0220026969909668\n",
      "Epoch 118 / 500 | iteration 10 / 30 | Total Loss: 5.363481521606445 | KNN Loss: 4.33501672744751 | BCE Loss: 1.0284650325775146\n",
      "Epoch 118 / 500 | iteration 15 / 30 | Total Loss: 5.341633319854736 | KNN Loss: 4.330392837524414 | BCE Loss: 1.0112406015396118\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118 / 500 | iteration 20 / 30 | Total Loss: 5.362718105316162 | KNN Loss: 4.340510368347168 | BCE Loss: 1.0222077369689941\n",
      "Epoch 118 / 500 | iteration 25 / 30 | Total Loss: 5.405222415924072 | KNN Loss: 4.36403751373291 | BCE Loss: 1.0411847829818726\n",
      "Epoch 119 / 500 | iteration 0 / 30 | Total Loss: 5.386414527893066 | KNN Loss: 4.36256217956543 | BCE Loss: 1.0238525867462158\n",
      "Epoch 119 / 500 | iteration 5 / 30 | Total Loss: 5.413398265838623 | KNN Loss: 4.3725104331970215 | BCE Loss: 1.0408879518508911\n",
      "Epoch 119 / 500 | iteration 10 / 30 | Total Loss: 5.375942230224609 | KNN Loss: 4.380314350128174 | BCE Loss: 0.9956281185150146\n",
      "Epoch 119 / 500 | iteration 15 / 30 | Total Loss: 5.398951530456543 | KNN Loss: 4.359055995941162 | BCE Loss: 1.0398955345153809\n",
      "Epoch 119 / 500 | iteration 20 / 30 | Total Loss: 5.4364776611328125 | KNN Loss: 4.378838539123535 | BCE Loss: 1.0576390027999878\n",
      "Epoch 119 / 500 | iteration 25 / 30 | Total Loss: 5.4148054122924805 | KNN Loss: 4.36395788192749 | BCE Loss: 1.0508477687835693\n",
      "Epoch 120 / 500 | iteration 0 / 30 | Total Loss: 5.414779186248779 | KNN Loss: 4.366964817047119 | BCE Loss: 1.0478144884109497\n",
      "Epoch 120 / 500 | iteration 5 / 30 | Total Loss: 5.3900837898254395 | KNN Loss: 4.344348907470703 | BCE Loss: 1.0457348823547363\n",
      "Epoch 120 / 500 | iteration 10 / 30 | Total Loss: 5.356813907623291 | KNN Loss: 4.355445861816406 | BCE Loss: 1.0013680458068848\n",
      "Epoch 120 / 500 | iteration 15 / 30 | Total Loss: 5.335840225219727 | KNN Loss: 4.333407402038574 | BCE Loss: 1.0024325847625732\n",
      "Epoch 120 / 500 | iteration 20 / 30 | Total Loss: 5.379839897155762 | KNN Loss: 4.357884407043457 | BCE Loss: 1.0219557285308838\n",
      "Epoch 120 / 500 | iteration 25 / 30 | Total Loss: 5.404848098754883 | KNN Loss: 4.372044563293457 | BCE Loss: 1.0328035354614258\n",
      "Epoch 121 / 500 | iteration 0 / 30 | Total Loss: 5.413597106933594 | KNN Loss: 4.383233547210693 | BCE Loss: 1.0303634405136108\n",
      "Epoch 121 / 500 | iteration 5 / 30 | Total Loss: 5.44875955581665 | KNN Loss: 4.388319969177246 | BCE Loss: 1.0604394674301147\n",
      "Epoch 121 / 500 | iteration 10 / 30 | Total Loss: 5.391798973083496 | KNN Loss: 4.3423919677734375 | BCE Loss: 1.049406886100769\n",
      "Epoch 121 / 500 | iteration 15 / 30 | Total Loss: 5.385356903076172 | KNN Loss: 4.34944486618042 | BCE Loss: 1.035912275314331\n",
      "Epoch 121 / 500 | iteration 20 / 30 | Total Loss: 5.363144874572754 | KNN Loss: 4.330073833465576 | BCE Loss: 1.0330710411071777\n",
      "Epoch 121 / 500 | iteration 25 / 30 | Total Loss: 5.359151840209961 | KNN Loss: 4.365965843200684 | BCE Loss: 0.9931861758232117\n",
      "Epoch 122 / 500 | iteration 0 / 30 | Total Loss: 5.354438304901123 | KNN Loss: 4.3584208488464355 | BCE Loss: 0.9960173964500427\n",
      "Epoch 122 / 500 | iteration 5 / 30 | Total Loss: 5.354948043823242 | KNN Loss: 4.337486267089844 | BCE Loss: 1.0174616575241089\n",
      "Epoch 122 / 500 | iteration 10 / 30 | Total Loss: 5.388130187988281 | KNN Loss: 4.373006820678711 | BCE Loss: 1.0151231288909912\n",
      "Epoch 122 / 500 | iteration 15 / 30 | Total Loss: 5.356777191162109 | KNN Loss: 4.349512577056885 | BCE Loss: 1.0072648525238037\n",
      "Epoch 122 / 500 | iteration 20 / 30 | Total Loss: 5.375481605529785 | KNN Loss: 4.317906856536865 | BCE Loss: 1.05757474899292\n",
      "Epoch 122 / 500 | iteration 25 / 30 | Total Loss: 5.369077205657959 | KNN Loss: 4.344937324523926 | BCE Loss: 1.0241397619247437\n",
      "Epoch 123 / 500 | iteration 0 / 30 | Total Loss: 5.37911319732666 | KNN Loss: 4.357589244842529 | BCE Loss: 1.0215237140655518\n",
      "Epoch 123 / 500 | iteration 5 / 30 | Total Loss: 5.363802433013916 | KNN Loss: 4.333168983459473 | BCE Loss: 1.0306334495544434\n",
      "Epoch 123 / 500 | iteration 10 / 30 | Total Loss: 5.445196151733398 | KNN Loss: 4.39056921005249 | BCE Loss: 1.0546271800994873\n",
      "Epoch 123 / 500 | iteration 15 / 30 | Total Loss: 5.437474250793457 | KNN Loss: 4.404964923858643 | BCE Loss: 1.032509207725525\n",
      "Epoch 123 / 500 | iteration 20 / 30 | Total Loss: 5.382856369018555 | KNN Loss: 4.376084804534912 | BCE Loss: 1.0067716836929321\n",
      "Epoch 123 / 500 | iteration 25 / 30 | Total Loss: 5.385895252227783 | KNN Loss: 4.352204322814941 | BCE Loss: 1.0336910486221313\n",
      "Epoch 124 / 500 | iteration 0 / 30 | Total Loss: 5.40242338180542 | KNN Loss: 4.39348030090332 | BCE Loss: 1.00894296169281\n",
      "Epoch 124 / 500 | iteration 5 / 30 | Total Loss: 5.405116081237793 | KNN Loss: 4.384364128112793 | BCE Loss: 1.020752191543579\n",
      "Epoch 124 / 500 | iteration 10 / 30 | Total Loss: 5.423257350921631 | KNN Loss: 4.37373685836792 | BCE Loss: 1.049520492553711\n",
      "Epoch 124 / 500 | iteration 15 / 30 | Total Loss: 5.397439002990723 | KNN Loss: 4.366864204406738 | BCE Loss: 1.0305746793746948\n",
      "Epoch 124 / 500 | iteration 20 / 30 | Total Loss: 5.4182305335998535 | KNN Loss: 4.364310264587402 | BCE Loss: 1.0539202690124512\n",
      "Epoch 124 / 500 | iteration 25 / 30 | Total Loss: 5.326731204986572 | KNN Loss: 4.347401142120361 | BCE Loss: 0.9793300628662109\n",
      "Epoch 125 / 500 | iteration 0 / 30 | Total Loss: 5.388335704803467 | KNN Loss: 4.343343257904053 | BCE Loss: 1.044992446899414\n",
      "Epoch 125 / 500 | iteration 5 / 30 | Total Loss: 5.434625625610352 | KNN Loss: 4.376410484313965 | BCE Loss: 1.0582153797149658\n",
      "Epoch 125 / 500 | iteration 10 / 30 | Total Loss: 5.361968040466309 | KNN Loss: 4.335872173309326 | BCE Loss: 1.0260957479476929\n",
      "Epoch 125 / 500 | iteration 15 / 30 | Total Loss: 5.356520652770996 | KNN Loss: 4.323925971984863 | BCE Loss: 1.0325945615768433\n",
      "Epoch 125 / 500 | iteration 20 / 30 | Total Loss: 5.405937194824219 | KNN Loss: 4.3896379470825195 | BCE Loss: 1.0162992477416992\n",
      "Epoch 125 / 500 | iteration 25 / 30 | Total Loss: 5.362795829772949 | KNN Loss: 4.349277496337891 | BCE Loss: 1.0135185718536377\n",
      "Epoch 126 / 500 | iteration 0 / 30 | Total Loss: 5.3737616539001465 | KNN Loss: 4.362251281738281 | BCE Loss: 1.0115102529525757\n",
      "Epoch 126 / 500 | iteration 5 / 30 | Total Loss: 5.416621208190918 | KNN Loss: 4.352146625518799 | BCE Loss: 1.06447434425354\n",
      "Epoch 126 / 500 | iteration 10 / 30 | Total Loss: 5.357870578765869 | KNN Loss: 4.340917110443115 | BCE Loss: 1.016953468322754\n",
      "Epoch 126 / 500 | iteration 15 / 30 | Total Loss: 5.40457820892334 | KNN Loss: 4.375881195068359 | BCE Loss: 1.0286972522735596\n",
      "Epoch 126 / 500 | iteration 20 / 30 | Total Loss: 5.396111965179443 | KNN Loss: 4.348843574523926 | BCE Loss: 1.0472683906555176\n",
      "Epoch 126 / 500 | iteration 25 / 30 | Total Loss: 5.388162612915039 | KNN Loss: 4.352756500244141 | BCE Loss: 1.0354063510894775\n",
      "Epoch 127 / 500 | iteration 0 / 30 | Total Loss: 5.335168838500977 | KNN Loss: 4.329215049743652 | BCE Loss: 1.0059537887573242\n",
      "Epoch 127 / 500 | iteration 5 / 30 | Total Loss: 5.3931379318237305 | KNN Loss: 4.364795684814453 | BCE Loss: 1.0283420085906982\n",
      "Epoch 127 / 500 | iteration 10 / 30 | Total Loss: 5.356651306152344 | KNN Loss: 4.332951545715332 | BCE Loss: 1.0236999988555908\n",
      "Epoch 127 / 500 | iteration 15 / 30 | Total Loss: 5.38681173324585 | KNN Loss: 4.358679294586182 | BCE Loss: 1.0281323194503784\n",
      "Epoch 127 / 500 | iteration 20 / 30 | Total Loss: 5.373447418212891 | KNN Loss: 4.354408264160156 | BCE Loss: 1.0190390348434448\n",
      "Epoch 127 / 500 | iteration 25 / 30 | Total Loss: 5.404256820678711 | KNN Loss: 4.379665851593018 | BCE Loss: 1.0245912075042725\n",
      "Epoch   128: reducing learning rate of group 0 to 1.7150e-03.\n",
      "Epoch 128 / 500 | iteration 0 / 30 | Total Loss: 5.372872352600098 | KNN Loss: 4.342337608337402 | BCE Loss: 1.0305347442626953\n",
      "Epoch 128 / 500 | iteration 5 / 30 | Total Loss: 5.366842269897461 | KNN Loss: 4.33016300201416 | BCE Loss: 1.0366791486740112\n",
      "Epoch 128 / 500 | iteration 10 / 30 | Total Loss: 5.4304656982421875 | KNN Loss: 4.404951572418213 | BCE Loss: 1.0255138874053955\n",
      "Epoch 128 / 500 | iteration 15 / 30 | Total Loss: 5.351369857788086 | KNN Loss: 4.3397698402404785 | BCE Loss: 1.0116000175476074\n",
      "Epoch 128 / 500 | iteration 20 / 30 | Total Loss: 5.369318008422852 | KNN Loss: 4.343093395233154 | BCE Loss: 1.0262246131896973\n",
      "Epoch 128 / 500 | iteration 25 / 30 | Total Loss: 5.398507118225098 | KNN Loss: 4.373143196105957 | BCE Loss: 1.0253641605377197\n",
      "Epoch 129 / 500 | iteration 0 / 30 | Total Loss: 5.444252967834473 | KNN Loss: 4.406264781951904 | BCE Loss: 1.0379879474639893\n",
      "Epoch 129 / 500 | iteration 5 / 30 | Total Loss: 5.416585922241211 | KNN Loss: 4.382122039794922 | BCE Loss: 1.0344640016555786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 129 / 500 | iteration 10 / 30 | Total Loss: 5.376575946807861 | KNN Loss: 4.338107109069824 | BCE Loss: 1.038468837738037\n",
      "Epoch 129 / 500 | iteration 15 / 30 | Total Loss: 5.3478922843933105 | KNN Loss: 4.3298821449279785 | BCE Loss: 1.018010139465332\n",
      "Epoch 129 / 500 | iteration 20 / 30 | Total Loss: 5.397371292114258 | KNN Loss: 4.370826244354248 | BCE Loss: 1.0265452861785889\n",
      "Epoch 129 / 500 | iteration 25 / 30 | Total Loss: 5.37315559387207 | KNN Loss: 4.348732948303223 | BCE Loss: 1.0244224071502686\n",
      "Epoch 130 / 500 | iteration 0 / 30 | Total Loss: 5.414874076843262 | KNN Loss: 4.368021488189697 | BCE Loss: 1.046852707862854\n",
      "Epoch 130 / 500 | iteration 5 / 30 | Total Loss: 5.34259557723999 | KNN Loss: 4.333748817443848 | BCE Loss: 1.0088468790054321\n",
      "Epoch 130 / 500 | iteration 10 / 30 | Total Loss: 5.333478927612305 | KNN Loss: 4.347184658050537 | BCE Loss: 0.9862940311431885\n",
      "Epoch 130 / 500 | iteration 15 / 30 | Total Loss: 5.400967597961426 | KNN Loss: 4.388238430023193 | BCE Loss: 1.0127294063568115\n",
      "Epoch 130 / 500 | iteration 20 / 30 | Total Loss: 5.389112949371338 | KNN Loss: 4.367767333984375 | BCE Loss: 1.021345615386963\n",
      "Epoch 130 / 500 | iteration 25 / 30 | Total Loss: 5.401711940765381 | KNN Loss: 4.360616207122803 | BCE Loss: 1.0410957336425781\n",
      "Epoch 131 / 500 | iteration 0 / 30 | Total Loss: 5.3975419998168945 | KNN Loss: 4.373349666595459 | BCE Loss: 1.0241920948028564\n",
      "Epoch 131 / 500 | iteration 5 / 30 | Total Loss: 5.352359771728516 | KNN Loss: 4.338039398193359 | BCE Loss: 1.0143203735351562\n",
      "Epoch 131 / 500 | iteration 10 / 30 | Total Loss: 5.385887145996094 | KNN Loss: 4.350322723388672 | BCE Loss: 1.0355641841888428\n",
      "Epoch 131 / 500 | iteration 15 / 30 | Total Loss: 5.330531120300293 | KNN Loss: 4.332118034362793 | BCE Loss: 0.9984130859375\n",
      "Epoch 131 / 500 | iteration 20 / 30 | Total Loss: 5.342045783996582 | KNN Loss: 4.314816474914551 | BCE Loss: 1.0272295475006104\n",
      "Epoch 131 / 500 | iteration 25 / 30 | Total Loss: 5.396590232849121 | KNN Loss: 4.358164310455322 | BCE Loss: 1.038426160812378\n",
      "Epoch 132 / 500 | iteration 0 / 30 | Total Loss: 5.38916015625 | KNN Loss: 4.351783752441406 | BCE Loss: 1.0373766422271729\n",
      "Epoch 132 / 500 | iteration 5 / 30 | Total Loss: 5.393633842468262 | KNN Loss: 4.356319427490234 | BCE Loss: 1.0373146533966064\n",
      "Epoch 132 / 500 | iteration 10 / 30 | Total Loss: 5.376310348510742 | KNN Loss: 4.372891902923584 | BCE Loss: 1.0034186840057373\n",
      "Epoch 132 / 500 | iteration 15 / 30 | Total Loss: 5.34830379486084 | KNN Loss: 4.343699932098389 | BCE Loss: 1.0046037435531616\n",
      "Epoch 132 / 500 | iteration 20 / 30 | Total Loss: 5.375724792480469 | KNN Loss: 4.366257667541504 | BCE Loss: 1.009467363357544\n",
      "Epoch 132 / 500 | iteration 25 / 30 | Total Loss: 5.408049583435059 | KNN Loss: 4.363504886627197 | BCE Loss: 1.0445446968078613\n",
      "Epoch 133 / 500 | iteration 0 / 30 | Total Loss: 5.34780740737915 | KNN Loss: 4.339996337890625 | BCE Loss: 1.007811188697815\n",
      "Epoch 133 / 500 | iteration 5 / 30 | Total Loss: 5.351022720336914 | KNN Loss: 4.32975435256958 | BCE Loss: 1.021268606185913\n",
      "Epoch 133 / 500 | iteration 10 / 30 | Total Loss: 5.375923156738281 | KNN Loss: 4.3598127365112305 | BCE Loss: 1.0161101818084717\n",
      "Epoch 133 / 500 | iteration 15 / 30 | Total Loss: 5.39164924621582 | KNN Loss: 4.38806676864624 | BCE Loss: 1.0035825967788696\n",
      "Epoch 133 / 500 | iteration 20 / 30 | Total Loss: 5.470895767211914 | KNN Loss: 4.414058208465576 | BCE Loss: 1.0568376779556274\n",
      "Epoch 133 / 500 | iteration 25 / 30 | Total Loss: 5.429452896118164 | KNN Loss: 4.389524936676025 | BCE Loss: 1.0399281978607178\n",
      "Epoch 134 / 500 | iteration 0 / 30 | Total Loss: 5.383779525756836 | KNN Loss: 4.388384819030762 | BCE Loss: 0.9953949451446533\n",
      "Epoch 134 / 500 | iteration 5 / 30 | Total Loss: 5.346678733825684 | KNN Loss: 4.3272385597229 | BCE Loss: 1.0194402933120728\n",
      "Epoch 134 / 500 | iteration 10 / 30 | Total Loss: 5.372267246246338 | KNN Loss: 4.367799758911133 | BCE Loss: 1.0044673681259155\n",
      "Epoch 134 / 500 | iteration 15 / 30 | Total Loss: 5.384657859802246 | KNN Loss: 4.373547077178955 | BCE Loss: 1.0111106634140015\n",
      "Epoch 134 / 500 | iteration 20 / 30 | Total Loss: 5.335892677307129 | KNN Loss: 4.319465637207031 | BCE Loss: 1.0164268016815186\n",
      "Epoch 134 / 500 | iteration 25 / 30 | Total Loss: 5.391753673553467 | KNN Loss: 4.350833415985107 | BCE Loss: 1.0409202575683594\n",
      "Epoch 135 / 500 | iteration 0 / 30 | Total Loss: 5.412437438964844 | KNN Loss: 4.397459506988525 | BCE Loss: 1.0149779319763184\n",
      "Epoch 135 / 500 | iteration 5 / 30 | Total Loss: 5.395238876342773 | KNN Loss: 4.3545355796813965 | BCE Loss: 1.0407034158706665\n",
      "Epoch 135 / 500 | iteration 10 / 30 | Total Loss: 5.405348777770996 | KNN Loss: 4.358860492706299 | BCE Loss: 1.0464885234832764\n",
      "Epoch 135 / 500 | iteration 15 / 30 | Total Loss: 5.327300071716309 | KNN Loss: 4.333629608154297 | BCE Loss: 0.9936702251434326\n",
      "Epoch 135 / 500 | iteration 20 / 30 | Total Loss: 5.42934513092041 | KNN Loss: 4.391235828399658 | BCE Loss: 1.0381091833114624\n",
      "Epoch 135 / 500 | iteration 25 / 30 | Total Loss: 5.373414993286133 | KNN Loss: 4.359814167022705 | BCE Loss: 1.0136005878448486\n",
      "Epoch 136 / 500 | iteration 0 / 30 | Total Loss: 5.419159889221191 | KNN Loss: 4.391164779663086 | BCE Loss: 1.0279953479766846\n",
      "Epoch 136 / 500 | iteration 5 / 30 | Total Loss: 5.364884376525879 | KNN Loss: 4.336308002471924 | BCE Loss: 1.028576135635376\n",
      "Epoch 136 / 500 | iteration 10 / 30 | Total Loss: 5.381041526794434 | KNN Loss: 4.343138694763184 | BCE Loss: 1.0379029512405396\n",
      "Epoch 136 / 500 | iteration 15 / 30 | Total Loss: 5.392876625061035 | KNN Loss: 4.376835346221924 | BCE Loss: 1.0160410404205322\n",
      "Epoch 136 / 500 | iteration 20 / 30 | Total Loss: 5.376082897186279 | KNN Loss: 4.346920013427734 | BCE Loss: 1.0291630029678345\n",
      "Epoch 136 / 500 | iteration 25 / 30 | Total Loss: 5.367030620574951 | KNN Loss: 4.348522186279297 | BCE Loss: 1.0185085535049438\n",
      "Epoch 137 / 500 | iteration 0 / 30 | Total Loss: 5.365113258361816 | KNN Loss: 4.342407703399658 | BCE Loss: 1.022705316543579\n",
      "Epoch 137 / 500 | iteration 5 / 30 | Total Loss: 5.3669514656066895 | KNN Loss: 4.3293914794921875 | BCE Loss: 1.037559986114502\n",
      "Epoch 137 / 500 | iteration 10 / 30 | Total Loss: 5.336023330688477 | KNN Loss: 4.3306450843811035 | BCE Loss: 1.0053784847259521\n",
      "Epoch 137 / 500 | iteration 15 / 30 | Total Loss: 5.349514961242676 | KNN Loss: 4.312266826629639 | BCE Loss: 1.0372482538223267\n",
      "Epoch 137 / 500 | iteration 20 / 30 | Total Loss: 5.353257179260254 | KNN Loss: 4.313457489013672 | BCE Loss: 1.0397995710372925\n",
      "Epoch 137 / 500 | iteration 25 / 30 | Total Loss: 5.396806716918945 | KNN Loss: 4.348502159118652 | BCE Loss: 1.048304557800293\n",
      "Epoch 138 / 500 | iteration 0 / 30 | Total Loss: 5.423773288726807 | KNN Loss: 4.370855808258057 | BCE Loss: 1.0529173612594604\n",
      "Epoch 138 / 500 | iteration 5 / 30 | Total Loss: 5.3627729415893555 | KNN Loss: 4.355292797088623 | BCE Loss: 1.0074800252914429\n",
      "Epoch 138 / 500 | iteration 10 / 30 | Total Loss: 5.386381149291992 | KNN Loss: 4.357322692871094 | BCE Loss: 1.0290584564208984\n",
      "Epoch 138 / 500 | iteration 15 / 30 | Total Loss: 5.428433895111084 | KNN Loss: 4.372374534606934 | BCE Loss: 1.05605947971344\n",
      "Epoch 138 / 500 | iteration 20 / 30 | Total Loss: 5.435183525085449 | KNN Loss: 4.389418601989746 | BCE Loss: 1.045764684677124\n",
      "Epoch 138 / 500 | iteration 25 / 30 | Total Loss: 5.428982257843018 | KNN Loss: 4.398956298828125 | BCE Loss: 1.030025839805603\n",
      "Epoch 139 / 500 | iteration 0 / 30 | Total Loss: 5.404499053955078 | KNN Loss: 4.350241661071777 | BCE Loss: 1.0542572736740112\n",
      "Epoch 139 / 500 | iteration 5 / 30 | Total Loss: 5.371461391448975 | KNN Loss: 4.358274459838867 | BCE Loss: 1.0131868124008179\n",
      "Epoch 139 / 500 | iteration 10 / 30 | Total Loss: 5.382918834686279 | KNN Loss: 4.341068267822266 | BCE Loss: 1.0418505668640137\n",
      "Epoch 139 / 500 | iteration 15 / 30 | Total Loss: 5.341561317443848 | KNN Loss: 4.333301067352295 | BCE Loss: 1.0082604885101318\n",
      "Epoch 139 / 500 | iteration 20 / 30 | Total Loss: 5.328383922576904 | KNN Loss: 4.31757926940918 | BCE Loss: 1.0108047723770142\n",
      "Epoch 139 / 500 | iteration 25 / 30 | Total Loss: 5.394870281219482 | KNN Loss: 4.34866189956665 | BCE Loss: 1.0462082624435425\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 140 / 500 | iteration 0 / 30 | Total Loss: 5.3428754806518555 | KNN Loss: 4.329902172088623 | BCE Loss: 1.0129730701446533\n",
      "Epoch 140 / 500 | iteration 5 / 30 | Total Loss: 5.370046138763428 | KNN Loss: 4.342462062835693 | BCE Loss: 1.0275839567184448\n",
      "Epoch 140 / 500 | iteration 10 / 30 | Total Loss: 5.386135101318359 | KNN Loss: 4.365938186645508 | BCE Loss: 1.0201969146728516\n",
      "Epoch 140 / 500 | iteration 15 / 30 | Total Loss: 5.3488945960998535 | KNN Loss: 4.322516441345215 | BCE Loss: 1.0263781547546387\n",
      "Epoch 140 / 500 | iteration 20 / 30 | Total Loss: 5.366454601287842 | KNN Loss: 4.33605432510376 | BCE Loss: 1.0304001569747925\n",
      "Epoch 140 / 500 | iteration 25 / 30 | Total Loss: 5.373658657073975 | KNN Loss: 4.337007522583008 | BCE Loss: 1.0366511344909668\n",
      "Epoch 141 / 500 | iteration 0 / 30 | Total Loss: 5.374581336975098 | KNN Loss: 4.349555492401123 | BCE Loss: 1.0250260829925537\n",
      "Epoch 141 / 500 | iteration 5 / 30 | Total Loss: 5.349714756011963 | KNN Loss: 4.31928014755249 | BCE Loss: 1.0304346084594727\n",
      "Epoch 141 / 500 | iteration 10 / 30 | Total Loss: 5.418790817260742 | KNN Loss: 4.382073402404785 | BCE Loss: 1.036717176437378\n",
      "Epoch 141 / 500 | iteration 15 / 30 | Total Loss: 5.45467472076416 | KNN Loss: 4.424595355987549 | BCE Loss: 1.0300791263580322\n",
      "Epoch 141 / 500 | iteration 20 / 30 | Total Loss: 5.37477445602417 | KNN Loss: 4.354068756103516 | BCE Loss: 1.0207055807113647\n",
      "Epoch 141 / 500 | iteration 25 / 30 | Total Loss: 5.342819690704346 | KNN Loss: 4.317683219909668 | BCE Loss: 1.0251364707946777\n",
      "Epoch 142 / 500 | iteration 0 / 30 | Total Loss: 5.380608081817627 | KNN Loss: 4.35047721862793 | BCE Loss: 1.0301308631896973\n",
      "Epoch 142 / 500 | iteration 5 / 30 | Total Loss: 5.396553993225098 | KNN Loss: 4.391460418701172 | BCE Loss: 1.0050935745239258\n",
      "Epoch 142 / 500 | iteration 10 / 30 | Total Loss: 5.392924785614014 | KNN Loss: 4.338833808898926 | BCE Loss: 1.0540908575057983\n",
      "Epoch 142 / 500 | iteration 15 / 30 | Total Loss: 5.3508782386779785 | KNN Loss: 4.323094367980957 | BCE Loss: 1.0277838706970215\n",
      "Epoch 142 / 500 | iteration 20 / 30 | Total Loss: 5.3733015060424805 | KNN Loss: 4.35923957824707 | BCE Loss: 1.0140621662139893\n",
      "Epoch 142 / 500 | iteration 25 / 30 | Total Loss: 5.41001558303833 | KNN Loss: 4.369190216064453 | BCE Loss: 1.040825366973877\n",
      "Epoch 143 / 500 | iteration 0 / 30 | Total Loss: 5.3614606857299805 | KNN Loss: 4.375462532043457 | BCE Loss: 0.9859983921051025\n",
      "Epoch 143 / 500 | iteration 5 / 30 | Total Loss: 5.394778251647949 | KNN Loss: 4.364229202270508 | BCE Loss: 1.0305488109588623\n",
      "Epoch 143 / 500 | iteration 10 / 30 | Total Loss: 5.397652626037598 | KNN Loss: 4.379741191864014 | BCE Loss: 1.017911672592163\n",
      "Epoch 143 / 500 | iteration 15 / 30 | Total Loss: 5.380989074707031 | KNN Loss: 4.3582611083984375 | BCE Loss: 1.0227277278900146\n",
      "Epoch 143 / 500 | iteration 20 / 30 | Total Loss: 5.3708038330078125 | KNN Loss: 4.3455810546875 | BCE Loss: 1.0252230167388916\n",
      "Epoch 143 / 500 | iteration 25 / 30 | Total Loss: 5.3423004150390625 | KNN Loss: 4.328489303588867 | BCE Loss: 1.0138113498687744\n",
      "Epoch 144 / 500 | iteration 0 / 30 | Total Loss: 5.379495143890381 | KNN Loss: 4.347816467285156 | BCE Loss: 1.0316786766052246\n",
      "Epoch 144 / 500 | iteration 5 / 30 | Total Loss: 5.36570405960083 | KNN Loss: 4.324093341827393 | BCE Loss: 1.0416107177734375\n",
      "Epoch 144 / 500 | iteration 10 / 30 | Total Loss: 5.4059038162231445 | KNN Loss: 4.36029052734375 | BCE Loss: 1.045613169670105\n",
      "Epoch 144 / 500 | iteration 15 / 30 | Total Loss: 5.337715148925781 | KNN Loss: 4.325965881347656 | BCE Loss: 1.0117493867874146\n",
      "Epoch 144 / 500 | iteration 20 / 30 | Total Loss: 5.3048834800720215 | KNN Loss: 4.312384128570557 | BCE Loss: 0.9924991726875305\n",
      "Epoch 144 / 500 | iteration 25 / 30 | Total Loss: 5.376239776611328 | KNN Loss: 4.3332624435424805 | BCE Loss: 1.0429770946502686\n",
      "Epoch 145 / 500 | iteration 0 / 30 | Total Loss: 5.325137138366699 | KNN Loss: 4.325838565826416 | BCE Loss: 0.9992984533309937\n",
      "Epoch 145 / 500 | iteration 5 / 30 | Total Loss: 5.33643913269043 | KNN Loss: 4.345455646514893 | BCE Loss: 0.9909836053848267\n",
      "Epoch 145 / 500 | iteration 10 / 30 | Total Loss: 5.346004486083984 | KNN Loss: 4.3292155265808105 | BCE Loss: 1.0167887210845947\n",
      "Epoch 145 / 500 | iteration 15 / 30 | Total Loss: 5.3832597732543945 | KNN Loss: 4.343809604644775 | BCE Loss: 1.03944993019104\n",
      "Epoch 145 / 500 | iteration 20 / 30 | Total Loss: 5.375771522521973 | KNN Loss: 4.3610382080078125 | BCE Loss: 1.0147335529327393\n",
      "Epoch 145 / 500 | iteration 25 / 30 | Total Loss: 5.360240459442139 | KNN Loss: 4.315832614898682 | BCE Loss: 1.0444079637527466\n",
      "Epoch 146 / 500 | iteration 0 / 30 | Total Loss: 5.366535186767578 | KNN Loss: 4.335695743560791 | BCE Loss: 1.0308393239974976\n",
      "Epoch 146 / 500 | iteration 5 / 30 | Total Loss: 5.403203010559082 | KNN Loss: 4.364500045776367 | BCE Loss: 1.038703203201294\n",
      "Epoch 146 / 500 | iteration 10 / 30 | Total Loss: 5.420347213745117 | KNN Loss: 4.39584493637085 | BCE Loss: 1.0245022773742676\n",
      "Epoch 146 / 500 | iteration 15 / 30 | Total Loss: 5.423647880554199 | KNN Loss: 4.393223285675049 | BCE Loss: 1.03042471408844\n",
      "Epoch 146 / 500 | iteration 20 / 30 | Total Loss: 5.382350444793701 | KNN Loss: 4.3802995681762695 | BCE Loss: 1.0020508766174316\n",
      "Epoch 146 / 500 | iteration 25 / 30 | Total Loss: 5.407018661499023 | KNN Loss: 4.348321437835693 | BCE Loss: 1.05869722366333\n",
      "Epoch 147 / 500 | iteration 0 / 30 | Total Loss: 5.372905254364014 | KNN Loss: 4.369979381561279 | BCE Loss: 1.0029257535934448\n",
      "Epoch 147 / 500 | iteration 5 / 30 | Total Loss: 5.357974529266357 | KNN Loss: 4.337051868438721 | BCE Loss: 1.0209226608276367\n",
      "Epoch 147 / 500 | iteration 10 / 30 | Total Loss: 5.374665260314941 | KNN Loss: 4.361940383911133 | BCE Loss: 1.0127246379852295\n",
      "Epoch 147 / 500 | iteration 15 / 30 | Total Loss: 5.333271026611328 | KNN Loss: 4.312837600708008 | BCE Loss: 1.0204331874847412\n",
      "Epoch 147 / 500 | iteration 20 / 30 | Total Loss: 5.383828163146973 | KNN Loss: 4.3477654457092285 | BCE Loss: 1.0360628366470337\n",
      "Epoch 147 / 500 | iteration 25 / 30 | Total Loss: 5.381134510040283 | KNN Loss: 4.37637186050415 | BCE Loss: 1.0047626495361328\n",
      "Epoch 148 / 500 | iteration 0 / 30 | Total Loss: 5.426066875457764 | KNN Loss: 4.378119468688965 | BCE Loss: 1.0479472875595093\n",
      "Epoch 148 / 500 | iteration 5 / 30 | Total Loss: 5.408864974975586 | KNN Loss: 4.362846374511719 | BCE Loss: 1.0460186004638672\n",
      "Epoch 148 / 500 | iteration 10 / 30 | Total Loss: 5.356058120727539 | KNN Loss: 4.32898473739624 | BCE Loss: 1.0270733833312988\n",
      "Epoch 148 / 500 | iteration 15 / 30 | Total Loss: 5.394473552703857 | KNN Loss: 4.382548809051514 | BCE Loss: 1.0119247436523438\n",
      "Epoch 148 / 500 | iteration 20 / 30 | Total Loss: 5.309957504272461 | KNN Loss: 4.312913417816162 | BCE Loss: 0.9970441460609436\n",
      "Epoch 148 / 500 | iteration 25 / 30 | Total Loss: 5.390131950378418 | KNN Loss: 4.366763114929199 | BCE Loss: 1.0233685970306396\n",
      "Epoch   149: reducing learning rate of group 0 to 1.2005e-03.\n",
      "Epoch 149 / 500 | iteration 0 / 30 | Total Loss: 5.3293914794921875 | KNN Loss: 4.313504219055176 | BCE Loss: 1.0158874988555908\n",
      "Epoch 149 / 500 | iteration 5 / 30 | Total Loss: 5.358346939086914 | KNN Loss: 4.31797981262207 | BCE Loss: 1.0403671264648438\n",
      "Epoch 149 / 500 | iteration 10 / 30 | Total Loss: 5.3697662353515625 | KNN Loss: 4.345062255859375 | BCE Loss: 1.0247039794921875\n",
      "Epoch 149 / 500 | iteration 15 / 30 | Total Loss: 5.365447044372559 | KNN Loss: 4.342387676239014 | BCE Loss: 1.0230594873428345\n",
      "Epoch 149 / 500 | iteration 20 / 30 | Total Loss: 5.384934425354004 | KNN Loss: 4.369306564331055 | BCE Loss: 1.0156276226043701\n",
      "Epoch 149 / 500 | iteration 25 / 30 | Total Loss: 5.3716936111450195 | KNN Loss: 4.350369930267334 | BCE Loss: 1.0213236808776855\n",
      "Epoch 150 / 500 | iteration 0 / 30 | Total Loss: 5.413212776184082 | KNN Loss: 4.375572681427002 | BCE Loss: 1.037639856338501\n",
      "Epoch 150 / 500 | iteration 5 / 30 | Total Loss: 5.315609455108643 | KNN Loss: 4.345012187957764 | BCE Loss: 0.9705971479415894\n",
      "Epoch 150 / 500 | iteration 10 / 30 | Total Loss: 5.451773643493652 | KNN Loss: 4.416555881500244 | BCE Loss: 1.035217523574829\n",
      "Epoch 150 / 500 | iteration 15 / 30 | Total Loss: 5.412178993225098 | KNN Loss: 4.398141384124756 | BCE Loss: 1.0140376091003418\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 150 / 500 | iteration 20 / 30 | Total Loss: 5.320639610290527 | KNN Loss: 4.330503940582275 | BCE Loss: 0.990135908126831\n",
      "Epoch 150 / 500 | iteration 25 / 30 | Total Loss: 5.372696876525879 | KNN Loss: 4.336907386779785 | BCE Loss: 1.0357896089553833\n",
      "Epoch 151 / 500 | iteration 0 / 30 | Total Loss: 5.386903762817383 | KNN Loss: 4.376443386077881 | BCE Loss: 1.010460376739502\n",
      "Epoch 151 / 500 | iteration 5 / 30 | Total Loss: 5.39193058013916 | KNN Loss: 4.3518829345703125 | BCE Loss: 1.0400474071502686\n",
      "Epoch 151 / 500 | iteration 10 / 30 | Total Loss: 5.398073196411133 | KNN Loss: 4.340515613555908 | BCE Loss: 1.0575578212738037\n",
      "Epoch 151 / 500 | iteration 15 / 30 | Total Loss: 5.389028072357178 | KNN Loss: 4.367123603820801 | BCE Loss: 1.0219045877456665\n",
      "Epoch 151 / 500 | iteration 20 / 30 | Total Loss: 5.4321208000183105 | KNN Loss: 4.403107166290283 | BCE Loss: 1.0290136337280273\n",
      "Epoch 151 / 500 | iteration 25 / 30 | Total Loss: 5.408133506774902 | KNN Loss: 4.38257360458374 | BCE Loss: 1.025559663772583\n",
      "Epoch 152 / 500 | iteration 0 / 30 | Total Loss: 5.373282432556152 | KNN Loss: 4.348916530609131 | BCE Loss: 1.0243656635284424\n",
      "Epoch 152 / 500 | iteration 5 / 30 | Total Loss: 5.402102470397949 | KNN Loss: 4.368760585784912 | BCE Loss: 1.0333421230316162\n",
      "Epoch 152 / 500 | iteration 10 / 30 | Total Loss: 5.37445068359375 | KNN Loss: 4.344561576843262 | BCE Loss: 1.0298893451690674\n",
      "Epoch 152 / 500 | iteration 15 / 30 | Total Loss: 5.402704238891602 | KNN Loss: 4.368381500244141 | BCE Loss: 1.03432297706604\n",
      "Epoch 152 / 500 | iteration 20 / 30 | Total Loss: 5.367201328277588 | KNN Loss: 4.354588031768799 | BCE Loss: 1.0126134157180786\n",
      "Epoch 152 / 500 | iteration 25 / 30 | Total Loss: 5.391424179077148 | KNN Loss: 4.379893779754639 | BCE Loss: 1.0115303993225098\n",
      "Epoch 153 / 500 | iteration 0 / 30 | Total Loss: 5.394464492797852 | KNN Loss: 4.352270603179932 | BCE Loss: 1.04219388961792\n",
      "Epoch 153 / 500 | iteration 5 / 30 | Total Loss: 5.348339080810547 | KNN Loss: 4.330745220184326 | BCE Loss: 1.0175938606262207\n",
      "Epoch 153 / 500 | iteration 10 / 30 | Total Loss: 5.354781150817871 | KNN Loss: 4.3383708000183105 | BCE Loss: 1.0164105892181396\n",
      "Epoch 153 / 500 | iteration 15 / 30 | Total Loss: 5.409919738769531 | KNN Loss: 4.3398332595825195 | BCE Loss: 1.0700867176055908\n",
      "Epoch 153 / 500 | iteration 20 / 30 | Total Loss: 5.3324127197265625 | KNN Loss: 4.310841083526611 | BCE Loss: 1.0215718746185303\n",
      "Epoch 153 / 500 | iteration 25 / 30 | Total Loss: 5.3778557777404785 | KNN Loss: 4.361343860626221 | BCE Loss: 1.0165117979049683\n",
      "Epoch 154 / 500 | iteration 0 / 30 | Total Loss: 5.420505523681641 | KNN Loss: 4.364037036895752 | BCE Loss: 1.0564687252044678\n",
      "Epoch 154 / 500 | iteration 5 / 30 | Total Loss: 5.375637054443359 | KNN Loss: 4.345952987670898 | BCE Loss: 1.029684066772461\n",
      "Epoch 154 / 500 | iteration 10 / 30 | Total Loss: 5.384067535400391 | KNN Loss: 4.361266613006592 | BCE Loss: 1.022801160812378\n",
      "Epoch 154 / 500 | iteration 15 / 30 | Total Loss: 5.430315017700195 | KNN Loss: 4.398353576660156 | BCE Loss: 1.0319616794586182\n",
      "Epoch 154 / 500 | iteration 20 / 30 | Total Loss: 5.3674750328063965 | KNN Loss: 4.313958644866943 | BCE Loss: 1.0535163879394531\n",
      "Epoch 154 / 500 | iteration 25 / 30 | Total Loss: 5.382631301879883 | KNN Loss: 4.358081817626953 | BCE Loss: 1.0245496034622192\n",
      "Epoch 155 / 500 | iteration 0 / 30 | Total Loss: 5.360254764556885 | KNN Loss: 4.33366060256958 | BCE Loss: 1.0265942811965942\n",
      "Epoch 155 / 500 | iteration 5 / 30 | Total Loss: 5.335003852844238 | KNN Loss: 4.310409069061279 | BCE Loss: 1.024595022201538\n",
      "Epoch 155 / 500 | iteration 10 / 30 | Total Loss: 5.377763748168945 | KNN Loss: 4.343904972076416 | BCE Loss: 1.0338588953018188\n",
      "Epoch 155 / 500 | iteration 15 / 30 | Total Loss: 5.398468017578125 | KNN Loss: 4.379726886749268 | BCE Loss: 1.0187413692474365\n",
      "Epoch 155 / 500 | iteration 20 / 30 | Total Loss: 5.4136457443237305 | KNN Loss: 4.370558738708496 | BCE Loss: 1.0430872440338135\n",
      "Epoch 155 / 500 | iteration 25 / 30 | Total Loss: 5.3713154792785645 | KNN Loss: 4.350950717926025 | BCE Loss: 1.020364761352539\n",
      "Epoch 156 / 500 | iteration 0 / 30 | Total Loss: 5.395899772644043 | KNN Loss: 4.379059314727783 | BCE Loss: 1.0168405771255493\n",
      "Epoch 156 / 500 | iteration 5 / 30 | Total Loss: 5.369604110717773 | KNN Loss: 4.34372615814209 | BCE Loss: 1.0258780717849731\n",
      "Epoch 156 / 500 | iteration 10 / 30 | Total Loss: 5.357394695281982 | KNN Loss: 4.347955703735352 | BCE Loss: 1.0094389915466309\n",
      "Epoch 156 / 500 | iteration 15 / 30 | Total Loss: 5.3552374839782715 | KNN Loss: 4.324294090270996 | BCE Loss: 1.0309433937072754\n",
      "Epoch 156 / 500 | iteration 20 / 30 | Total Loss: 5.390946388244629 | KNN Loss: 4.358067989349365 | BCE Loss: 1.0328786373138428\n",
      "Epoch 156 / 500 | iteration 25 / 30 | Total Loss: 5.339649677276611 | KNN Loss: 4.337926387786865 | BCE Loss: 1.0017231702804565\n",
      "Epoch 157 / 500 | iteration 0 / 30 | Total Loss: 5.3246355056762695 | KNN Loss: 4.32036828994751 | BCE Loss: 1.0042669773101807\n",
      "Epoch 157 / 500 | iteration 5 / 30 | Total Loss: 5.368040084838867 | KNN Loss: 4.338764667510986 | BCE Loss: 1.0292752981185913\n",
      "Epoch 157 / 500 | iteration 10 / 30 | Total Loss: 5.344888687133789 | KNN Loss: 4.341213703155518 | BCE Loss: 1.0036749839782715\n",
      "Epoch 157 / 500 | iteration 15 / 30 | Total Loss: 5.389777183532715 | KNN Loss: 4.341047763824463 | BCE Loss: 1.0487295389175415\n",
      "Epoch 157 / 500 | iteration 20 / 30 | Total Loss: 5.349055290222168 | KNN Loss: 4.351105690002441 | BCE Loss: 0.9979493618011475\n",
      "Epoch 157 / 500 | iteration 25 / 30 | Total Loss: 5.4343438148498535 | KNN Loss: 4.392978668212891 | BCE Loss: 1.0413650274276733\n",
      "Epoch 158 / 500 | iteration 0 / 30 | Total Loss: 5.38828182220459 | KNN Loss: 4.3688554763793945 | BCE Loss: 1.0194262266159058\n",
      "Epoch 158 / 500 | iteration 5 / 30 | Total Loss: 5.427184104919434 | KNN Loss: 4.4066972732543945 | BCE Loss: 1.02048659324646\n",
      "Epoch 158 / 500 | iteration 10 / 30 | Total Loss: 5.406427383422852 | KNN Loss: 4.397039890289307 | BCE Loss: 1.0093872547149658\n",
      "Epoch 158 / 500 | iteration 15 / 30 | Total Loss: 5.348184108734131 | KNN Loss: 4.320949077606201 | BCE Loss: 1.0272350311279297\n",
      "Epoch 158 / 500 | iteration 20 / 30 | Total Loss: 5.385665416717529 | KNN Loss: 4.358822345733643 | BCE Loss: 1.0268430709838867\n",
      "Epoch 158 / 500 | iteration 25 / 30 | Total Loss: 5.358955383300781 | KNN Loss: 4.343835830688477 | BCE Loss: 1.0151196718215942\n",
      "Epoch 159 / 500 | iteration 0 / 30 | Total Loss: 5.336762428283691 | KNN Loss: 4.318007469177246 | BCE Loss: 1.0187550783157349\n",
      "Epoch 159 / 500 | iteration 5 / 30 | Total Loss: 5.3907575607299805 | KNN Loss: 4.378620147705078 | BCE Loss: 1.0121376514434814\n",
      "Epoch 159 / 500 | iteration 10 / 30 | Total Loss: 5.315984725952148 | KNN Loss: 4.316549301147461 | BCE Loss: 0.9994352459907532\n",
      "Epoch 159 / 500 | iteration 15 / 30 | Total Loss: 5.369341850280762 | KNN Loss: 4.34705924987793 | BCE Loss: 1.022282361984253\n",
      "Epoch 159 / 500 | iteration 20 / 30 | Total Loss: 5.357077121734619 | KNN Loss: 4.346887111663818 | BCE Loss: 1.0101900100708008\n",
      "Epoch 159 / 500 | iteration 25 / 30 | Total Loss: 5.377275466918945 | KNN Loss: 4.367635726928711 | BCE Loss: 1.0096396207809448\n",
      "Epoch   160: reducing learning rate of group 0 to 8.4035e-04.\n",
      "Epoch 160 / 500 | iteration 0 / 30 | Total Loss: 5.367260932922363 | KNN Loss: 4.336331367492676 | BCE Loss: 1.0309293270111084\n",
      "Epoch 160 / 500 | iteration 5 / 30 | Total Loss: 5.4221391677856445 | KNN Loss: 4.391551971435547 | BCE Loss: 1.0305874347686768\n",
      "Epoch 160 / 500 | iteration 10 / 30 | Total Loss: 5.372758865356445 | KNN Loss: 4.33277702331543 | BCE Loss: 1.0399818420410156\n",
      "Epoch 160 / 500 | iteration 15 / 30 | Total Loss: 5.367780685424805 | KNN Loss: 4.341312408447266 | BCE Loss: 1.0264685153961182\n",
      "Epoch 160 / 500 | iteration 20 / 30 | Total Loss: 5.390368461608887 | KNN Loss: 4.355967044830322 | BCE Loss: 1.0344016551971436\n",
      "Epoch 160 / 500 | iteration 25 / 30 | Total Loss: 5.378971099853516 | KNN Loss: 4.336722373962402 | BCE Loss: 1.0422484874725342\n",
      "Epoch 161 / 500 | iteration 0 / 30 | Total Loss: 5.383878231048584 | KNN Loss: 4.383729457855225 | BCE Loss: 1.0001487731933594\n",
      "Epoch 161 / 500 | iteration 5 / 30 | Total Loss: 5.394722938537598 | KNN Loss: 4.375427722930908 | BCE Loss: 1.0192954540252686\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 161 / 500 | iteration 10 / 30 | Total Loss: 5.352229118347168 | KNN Loss: 4.324623107910156 | BCE Loss: 1.0276062488555908\n",
      "Epoch 161 / 500 | iteration 15 / 30 | Total Loss: 5.341828346252441 | KNN Loss: 4.319684028625488 | BCE Loss: 1.022144079208374\n",
      "Epoch 161 / 500 | iteration 20 / 30 | Total Loss: 5.400810241699219 | KNN Loss: 4.381416320800781 | BCE Loss: 1.0193936824798584\n",
      "Epoch 161 / 500 | iteration 25 / 30 | Total Loss: 5.377937316894531 | KNN Loss: 4.355761528015137 | BCE Loss: 1.022175669670105\n",
      "Epoch 162 / 500 | iteration 0 / 30 | Total Loss: 5.390307903289795 | KNN Loss: 4.3600592613220215 | BCE Loss: 1.0302486419677734\n",
      "Epoch 162 / 500 | iteration 5 / 30 | Total Loss: 5.381618499755859 | KNN Loss: 4.345142841339111 | BCE Loss: 1.036475658416748\n",
      "Epoch 162 / 500 | iteration 10 / 30 | Total Loss: 5.416598320007324 | KNN Loss: 4.419052600860596 | BCE Loss: 0.9975454807281494\n",
      "Epoch 162 / 500 | iteration 15 / 30 | Total Loss: 5.362450122833252 | KNN Loss: 4.339688301086426 | BCE Loss: 1.0227618217468262\n",
      "Epoch 162 / 500 | iteration 20 / 30 | Total Loss: 5.406055450439453 | KNN Loss: 4.385771751403809 | BCE Loss: 1.0202834606170654\n",
      "Epoch 162 / 500 | iteration 25 / 30 | Total Loss: 5.427860736846924 | KNN Loss: 4.389345645904541 | BCE Loss: 1.0385150909423828\n",
      "Epoch 163 / 500 | iteration 0 / 30 | Total Loss: 5.359478950500488 | KNN Loss: 4.34113883972168 | BCE Loss: 1.0183403491973877\n",
      "Epoch 163 / 500 | iteration 5 / 30 | Total Loss: 5.479290008544922 | KNN Loss: 4.454448223114014 | BCE Loss: 1.024841547012329\n",
      "Epoch 163 / 500 | iteration 10 / 30 | Total Loss: 5.350515365600586 | KNN Loss: 4.360147476196289 | BCE Loss: 0.9903680086135864\n",
      "Epoch 163 / 500 | iteration 15 / 30 | Total Loss: 5.3791351318359375 | KNN Loss: 4.3581671714782715 | BCE Loss: 1.020967721939087\n",
      "Epoch 163 / 500 | iteration 20 / 30 | Total Loss: 5.33974552154541 | KNN Loss: 4.341796398162842 | BCE Loss: 0.9979492425918579\n",
      "Epoch 163 / 500 | iteration 25 / 30 | Total Loss: 5.362888336181641 | KNN Loss: 4.319525241851807 | BCE Loss: 1.043363094329834\n",
      "Epoch 164 / 500 | iteration 0 / 30 | Total Loss: 5.367210388183594 | KNN Loss: 4.361245632171631 | BCE Loss: 1.005964756011963\n",
      "Epoch 164 / 500 | iteration 5 / 30 | Total Loss: 5.409493446350098 | KNN Loss: 4.358129024505615 | BCE Loss: 1.0513646602630615\n",
      "Epoch 164 / 500 | iteration 10 / 30 | Total Loss: 5.343600273132324 | KNN Loss: 4.3220744132995605 | BCE Loss: 1.0215259790420532\n",
      "Epoch 164 / 500 | iteration 15 / 30 | Total Loss: 5.433727264404297 | KNN Loss: 4.371716499328613 | BCE Loss: 1.0620105266571045\n",
      "Epoch 164 / 500 | iteration 20 / 30 | Total Loss: 5.345244407653809 | KNN Loss: 4.3420305252075195 | BCE Loss: 1.0032137632369995\n",
      "Epoch 164 / 500 | iteration 25 / 30 | Total Loss: 5.38163948059082 | KNN Loss: 4.344789028167725 | BCE Loss: 1.0368503332138062\n",
      "Epoch 165 / 500 | iteration 0 / 30 | Total Loss: 5.384516716003418 | KNN Loss: 4.3289971351623535 | BCE Loss: 1.0555198192596436\n",
      "Epoch 165 / 500 | iteration 5 / 30 | Total Loss: 5.370935440063477 | KNN Loss: 4.344882965087891 | BCE Loss: 1.026052713394165\n",
      "Epoch 165 / 500 | iteration 10 / 30 | Total Loss: 5.3817138671875 | KNN Loss: 4.3495988845825195 | BCE Loss: 1.0321152210235596\n",
      "Epoch 165 / 500 | iteration 15 / 30 | Total Loss: 5.425506591796875 | KNN Loss: 4.385435581207275 | BCE Loss: 1.0400710105895996\n",
      "Epoch 165 / 500 | iteration 20 / 30 | Total Loss: 5.368885517120361 | KNN Loss: 4.345069408416748 | BCE Loss: 1.0238161087036133\n",
      "Epoch 165 / 500 | iteration 25 / 30 | Total Loss: 5.3857421875 | KNN Loss: 4.359567642211914 | BCE Loss: 1.0261744260787964\n",
      "Epoch 166 / 500 | iteration 0 / 30 | Total Loss: 5.38325834274292 | KNN Loss: 4.352302551269531 | BCE Loss: 1.0309559106826782\n",
      "Epoch 166 / 500 | iteration 5 / 30 | Total Loss: 5.358578681945801 | KNN Loss: 4.3528547286987305 | BCE Loss: 1.0057237148284912\n",
      "Epoch 166 / 500 | iteration 10 / 30 | Total Loss: 5.363012313842773 | KNN Loss: 4.348349094390869 | BCE Loss: 1.0146629810333252\n",
      "Epoch 166 / 500 | iteration 15 / 30 | Total Loss: 5.3719940185546875 | KNN Loss: 4.359975337982178 | BCE Loss: 1.0120184421539307\n",
      "Epoch 166 / 500 | iteration 20 / 30 | Total Loss: 5.394596099853516 | KNN Loss: 4.3761420249938965 | BCE Loss: 1.0184539556503296\n",
      "Epoch 166 / 500 | iteration 25 / 30 | Total Loss: 5.397541046142578 | KNN Loss: 4.351646423339844 | BCE Loss: 1.0458943843841553\n",
      "Epoch 167 / 500 | iteration 0 / 30 | Total Loss: 5.356895446777344 | KNN Loss: 4.33338737487793 | BCE Loss: 1.023507833480835\n",
      "Epoch 167 / 500 | iteration 5 / 30 | Total Loss: 5.342498779296875 | KNN Loss: 4.320619106292725 | BCE Loss: 1.0218795537948608\n",
      "Epoch 167 / 500 | iteration 10 / 30 | Total Loss: 5.447601318359375 | KNN Loss: 4.389071941375732 | BCE Loss: 1.0585296154022217\n",
      "Epoch 167 / 500 | iteration 15 / 30 | Total Loss: 5.359095573425293 | KNN Loss: 4.366805076599121 | BCE Loss: 0.9922904968261719\n",
      "Epoch 167 / 500 | iteration 20 / 30 | Total Loss: 5.431077003479004 | KNN Loss: 4.395641326904297 | BCE Loss: 1.035435676574707\n",
      "Epoch 167 / 500 | iteration 25 / 30 | Total Loss: 5.352428436279297 | KNN Loss: 4.3493852615356445 | BCE Loss: 1.0030431747436523\n",
      "Epoch 168 / 500 | iteration 0 / 30 | Total Loss: 5.377375602722168 | KNN Loss: 4.354457378387451 | BCE Loss: 1.0229183435440063\n",
      "Epoch 168 / 500 | iteration 5 / 30 | Total Loss: 5.3133344650268555 | KNN Loss: 4.319441795349121 | BCE Loss: 0.9938925504684448\n",
      "Epoch 168 / 500 | iteration 10 / 30 | Total Loss: 5.37675666809082 | KNN Loss: 4.351008415222168 | BCE Loss: 1.0257480144500732\n",
      "Epoch 168 / 500 | iteration 15 / 30 | Total Loss: 5.36407470703125 | KNN Loss: 4.351526737213135 | BCE Loss: 1.0125479698181152\n",
      "Epoch 168 / 500 | iteration 20 / 30 | Total Loss: 5.354206562042236 | KNN Loss: 4.316986560821533 | BCE Loss: 1.0372200012207031\n",
      "Epoch 168 / 500 | iteration 25 / 30 | Total Loss: 5.4306640625 | KNN Loss: 4.367293357849121 | BCE Loss: 1.063370704650879\n",
      "Epoch 169 / 500 | iteration 0 / 30 | Total Loss: 5.340721130371094 | KNN Loss: 4.320718288421631 | BCE Loss: 1.0200026035308838\n",
      "Epoch 169 / 500 | iteration 5 / 30 | Total Loss: 5.406507968902588 | KNN Loss: 4.377200126647949 | BCE Loss: 1.0293078422546387\n",
      "Epoch 169 / 500 | iteration 10 / 30 | Total Loss: 5.396669387817383 | KNN Loss: 4.374153137207031 | BCE Loss: 1.0225160121917725\n",
      "Epoch 169 / 500 | iteration 15 / 30 | Total Loss: 5.340694427490234 | KNN Loss: 4.339745998382568 | BCE Loss: 1.000948190689087\n",
      "Epoch 169 / 500 | iteration 20 / 30 | Total Loss: 5.392486572265625 | KNN Loss: 4.356777191162109 | BCE Loss: 1.0357095003128052\n",
      "Epoch 169 / 500 | iteration 25 / 30 | Total Loss: 5.396398544311523 | KNN Loss: 4.38734769821167 | BCE Loss: 1.009050726890564\n",
      "Epoch 170 / 500 | iteration 0 / 30 | Total Loss: 5.363124370574951 | KNN Loss: 4.352341651916504 | BCE Loss: 1.0107827186584473\n",
      "Epoch 170 / 500 | iteration 5 / 30 | Total Loss: 5.403256893157959 | KNN Loss: 4.342529773712158 | BCE Loss: 1.0607271194458008\n",
      "Epoch 170 / 500 | iteration 10 / 30 | Total Loss: 5.31409215927124 | KNN Loss: 4.309414386749268 | BCE Loss: 1.0046778917312622\n",
      "Epoch 170 / 500 | iteration 15 / 30 | Total Loss: 5.395323753356934 | KNN Loss: 4.382725715637207 | BCE Loss: 1.0125977993011475\n",
      "Epoch 170 / 500 | iteration 20 / 30 | Total Loss: 5.408893585205078 | KNN Loss: 4.368288516998291 | BCE Loss: 1.040604829788208\n",
      "Epoch 170 / 500 | iteration 25 / 30 | Total Loss: 5.382050514221191 | KNN Loss: 4.333552837371826 | BCE Loss: 1.0484977960586548\n",
      "Epoch   171: reducing learning rate of group 0 to 5.8824e-04.\n",
      "Epoch 171 / 500 | iteration 0 / 30 | Total Loss: 5.384695053100586 | KNN Loss: 4.317162036895752 | BCE Loss: 1.0675327777862549\n",
      "Epoch 171 / 500 | iteration 5 / 30 | Total Loss: 5.354893684387207 | KNN Loss: 4.351482391357422 | BCE Loss: 1.0034114122390747\n",
      "Epoch 171 / 500 | iteration 10 / 30 | Total Loss: 5.331897735595703 | KNN Loss: 4.337489128112793 | BCE Loss: 0.994408369064331\n",
      "Epoch 171 / 500 | iteration 15 / 30 | Total Loss: 5.342728137969971 | KNN Loss: 4.327571392059326 | BCE Loss: 1.015156865119934\n",
      "Epoch 171 / 500 | iteration 20 / 30 | Total Loss: 5.335791110992432 | KNN Loss: 4.322115421295166 | BCE Loss: 1.0136758089065552\n",
      "Epoch 171 / 500 | iteration 25 / 30 | Total Loss: 5.374978065490723 | KNN Loss: 4.336721897125244 | BCE Loss: 1.0382564067840576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 172 / 500 | iteration 0 / 30 | Total Loss: 5.34600830078125 | KNN Loss: 4.309240818023682 | BCE Loss: 1.0367674827575684\n",
      "Epoch 172 / 500 | iteration 5 / 30 | Total Loss: 5.418545722961426 | KNN Loss: 4.388669967651367 | BCE Loss: 1.0298758745193481\n",
      "Epoch 172 / 500 | iteration 10 / 30 | Total Loss: 5.389286518096924 | KNN Loss: 4.359756946563721 | BCE Loss: 1.0295295715332031\n",
      "Epoch 172 / 500 | iteration 15 / 30 | Total Loss: 5.428611755371094 | KNN Loss: 4.395956516265869 | BCE Loss: 1.0326553583145142\n",
      "Epoch 172 / 500 | iteration 20 / 30 | Total Loss: 5.3387908935546875 | KNN Loss: 4.326706886291504 | BCE Loss: 1.0120837688446045\n",
      "Epoch 172 / 500 | iteration 25 / 30 | Total Loss: 5.347718238830566 | KNN Loss: 4.327335357666016 | BCE Loss: 1.0203826427459717\n",
      "Epoch 173 / 500 | iteration 0 / 30 | Total Loss: 5.366621017456055 | KNN Loss: 4.3409552574157715 | BCE Loss: 1.0256658792495728\n",
      "Epoch 173 / 500 | iteration 5 / 30 | Total Loss: 5.402732849121094 | KNN Loss: 4.374973297119141 | BCE Loss: 1.0277595520019531\n",
      "Epoch 173 / 500 | iteration 10 / 30 | Total Loss: 5.365739345550537 | KNN Loss: 4.322265148162842 | BCE Loss: 1.0434740781784058\n",
      "Epoch 173 / 500 | iteration 15 / 30 | Total Loss: 5.440918445587158 | KNN Loss: 4.388141632080078 | BCE Loss: 1.05277681350708\n",
      "Epoch 173 / 500 | iteration 20 / 30 | Total Loss: 5.403351783752441 | KNN Loss: 4.371618270874023 | BCE Loss: 1.031733512878418\n",
      "Epoch 173 / 500 | iteration 25 / 30 | Total Loss: 5.334558963775635 | KNN Loss: 4.319307327270508 | BCE Loss: 1.0152517557144165\n",
      "Epoch 174 / 500 | iteration 0 / 30 | Total Loss: 5.337191581726074 | KNN Loss: 4.329601287841797 | BCE Loss: 1.0075902938842773\n",
      "Epoch 174 / 500 | iteration 5 / 30 | Total Loss: 5.379296779632568 | KNN Loss: 4.356122970581055 | BCE Loss: 1.0231739282608032\n",
      "Epoch 174 / 500 | iteration 10 / 30 | Total Loss: 5.411734580993652 | KNN Loss: 4.3937249183654785 | BCE Loss: 1.018009901046753\n",
      "Epoch 174 / 500 | iteration 15 / 30 | Total Loss: 5.3844895362854 | KNN Loss: 4.338734149932861 | BCE Loss: 1.045755386352539\n",
      "Epoch 174 / 500 | iteration 20 / 30 | Total Loss: 5.358176231384277 | KNN Loss: 4.324080944061279 | BCE Loss: 1.034095287322998\n",
      "Epoch 174 / 500 | iteration 25 / 30 | Total Loss: 5.421598434448242 | KNN Loss: 4.368594646453857 | BCE Loss: 1.0530035495758057\n",
      "Epoch 175 / 500 | iteration 0 / 30 | Total Loss: 5.405828952789307 | KNN Loss: 4.3466267585754395 | BCE Loss: 1.0592023134231567\n",
      "Epoch 175 / 500 | iteration 5 / 30 | Total Loss: 5.3276519775390625 | KNN Loss: 4.315651893615723 | BCE Loss: 1.0119998455047607\n",
      "Epoch 175 / 500 | iteration 10 / 30 | Total Loss: 5.312217712402344 | KNN Loss: 4.319296360015869 | BCE Loss: 0.9929213523864746\n",
      "Epoch 175 / 500 | iteration 15 / 30 | Total Loss: 5.366024017333984 | KNN Loss: 4.362056732177734 | BCE Loss: 1.00396728515625\n",
      "Epoch 175 / 500 | iteration 20 / 30 | Total Loss: 5.414847373962402 | KNN Loss: 4.362793445587158 | BCE Loss: 1.052053689956665\n",
      "Epoch 175 / 500 | iteration 25 / 30 | Total Loss: 5.414629936218262 | KNN Loss: 4.395658493041992 | BCE Loss: 1.0189712047576904\n",
      "Epoch 176 / 500 | iteration 0 / 30 | Total Loss: 5.4176130294799805 | KNN Loss: 4.398386001586914 | BCE Loss: 1.0192272663116455\n",
      "Epoch 176 / 500 | iteration 5 / 30 | Total Loss: 5.291728973388672 | KNN Loss: 4.2997541427612305 | BCE Loss: 0.9919745922088623\n",
      "Epoch 176 / 500 | iteration 10 / 30 | Total Loss: 5.362078666687012 | KNN Loss: 4.345283031463623 | BCE Loss: 1.0167957544326782\n",
      "Epoch 176 / 500 | iteration 15 / 30 | Total Loss: 5.402626037597656 | KNN Loss: 4.382073879241943 | BCE Loss: 1.020552158355713\n",
      "Epoch 176 / 500 | iteration 20 / 30 | Total Loss: 5.47051477432251 | KNN Loss: 4.408377647399902 | BCE Loss: 1.062137246131897\n",
      "Epoch 176 / 500 | iteration 25 / 30 | Total Loss: 5.334293365478516 | KNN Loss: 4.343968868255615 | BCE Loss: 0.9903244972229004\n",
      "Epoch 177 / 500 | iteration 0 / 30 | Total Loss: 5.350849151611328 | KNN Loss: 4.338658332824707 | BCE Loss: 1.0121910572052002\n",
      "Epoch 177 / 500 | iteration 5 / 30 | Total Loss: 5.380021572113037 | KNN Loss: 4.31729793548584 | BCE Loss: 1.0627237558364868\n",
      "Epoch 177 / 500 | iteration 10 / 30 | Total Loss: 5.387192726135254 | KNN Loss: 4.343667984008789 | BCE Loss: 1.043524980545044\n",
      "Epoch 177 / 500 | iteration 15 / 30 | Total Loss: 5.4324164390563965 | KNN Loss: 4.390405178070068 | BCE Loss: 1.0420113801956177\n",
      "Epoch 177 / 500 | iteration 20 / 30 | Total Loss: 5.40131950378418 | KNN Loss: 4.3737335205078125 | BCE Loss: 1.0275862216949463\n",
      "Epoch 177 / 500 | iteration 25 / 30 | Total Loss: 5.355900764465332 | KNN Loss: 4.346130847930908 | BCE Loss: 1.0097696781158447\n",
      "Epoch 178 / 500 | iteration 0 / 30 | Total Loss: 5.392179489135742 | KNN Loss: 4.355690956115723 | BCE Loss: 1.0364887714385986\n",
      "Epoch 178 / 500 | iteration 5 / 30 | Total Loss: 5.3539252281188965 | KNN Loss: 4.339832782745361 | BCE Loss: 1.0140924453735352\n",
      "Epoch 178 / 500 | iteration 10 / 30 | Total Loss: 5.341612815856934 | KNN Loss: 4.3270368576049805 | BCE Loss: 1.0145759582519531\n",
      "Epoch 178 / 500 | iteration 15 / 30 | Total Loss: 5.375476837158203 | KNN Loss: 4.354695796966553 | BCE Loss: 1.0207810401916504\n",
      "Epoch 178 / 500 | iteration 20 / 30 | Total Loss: 5.411951541900635 | KNN Loss: 4.384515285491943 | BCE Loss: 1.0274362564086914\n",
      "Epoch 178 / 500 | iteration 25 / 30 | Total Loss: 5.345125675201416 | KNN Loss: 4.351342678070068 | BCE Loss: 0.9937828779220581\n",
      "Epoch 179 / 500 | iteration 0 / 30 | Total Loss: 5.3455095291137695 | KNN Loss: 4.331367015838623 | BCE Loss: 1.0141425132751465\n",
      "Epoch 179 / 500 | iteration 5 / 30 | Total Loss: 5.352585792541504 | KNN Loss: 4.331878185272217 | BCE Loss: 1.020707607269287\n",
      "Epoch 179 / 500 | iteration 10 / 30 | Total Loss: 5.3711838722229 | KNN Loss: 4.344132423400879 | BCE Loss: 1.027051568031311\n",
      "Epoch 179 / 500 | iteration 15 / 30 | Total Loss: 5.483181476593018 | KNN Loss: 4.445141315460205 | BCE Loss: 1.038040041923523\n",
      "Epoch 179 / 500 | iteration 20 / 30 | Total Loss: 5.339923858642578 | KNN Loss: 4.3484296798706055 | BCE Loss: 0.9914941191673279\n",
      "Epoch 179 / 500 | iteration 25 / 30 | Total Loss: 5.412747383117676 | KNN Loss: 4.389255523681641 | BCE Loss: 1.0234918594360352\n",
      "Epoch 180 / 500 | iteration 0 / 30 | Total Loss: 5.359095573425293 | KNN Loss: 4.328901290893555 | BCE Loss: 1.0301945209503174\n",
      "Epoch 180 / 500 | iteration 5 / 30 | Total Loss: 5.387040615081787 | KNN Loss: 4.357330799102783 | BCE Loss: 1.0297099351882935\n",
      "Epoch 180 / 500 | iteration 10 / 30 | Total Loss: 5.43279504776001 | KNN Loss: 4.388451099395752 | BCE Loss: 1.0443439483642578\n",
      "Epoch 180 / 500 | iteration 15 / 30 | Total Loss: 5.383491039276123 | KNN Loss: 4.354231834411621 | BCE Loss: 1.0292590856552124\n",
      "Epoch 180 / 500 | iteration 20 / 30 | Total Loss: 5.313205718994141 | KNN Loss: 4.321864128112793 | BCE Loss: 0.9913417100906372\n",
      "Epoch 180 / 500 | iteration 25 / 30 | Total Loss: 5.394715309143066 | KNN Loss: 4.354032039642334 | BCE Loss: 1.0406835079193115\n",
      "Epoch 181 / 500 | iteration 0 / 30 | Total Loss: 5.370579242706299 | KNN Loss: 4.335268974304199 | BCE Loss: 1.03531014919281\n",
      "Epoch 181 / 500 | iteration 5 / 30 | Total Loss: 5.356751441955566 | KNN Loss: 4.345540523529053 | BCE Loss: 1.0112111568450928\n",
      "Epoch 181 / 500 | iteration 10 / 30 | Total Loss: 5.334394454956055 | KNN Loss: 4.320497512817383 | BCE Loss: 1.0138970613479614\n",
      "Epoch 181 / 500 | iteration 15 / 30 | Total Loss: 5.393696308135986 | KNN Loss: 4.365484237670898 | BCE Loss: 1.028212070465088\n",
      "Epoch 181 / 500 | iteration 20 / 30 | Total Loss: 5.404573440551758 | KNN Loss: 4.362386703491211 | BCE Loss: 1.042186975479126\n",
      "Epoch 181 / 500 | iteration 25 / 30 | Total Loss: 5.36665153503418 | KNN Loss: 4.334689617156982 | BCE Loss: 1.0319619178771973\n",
      "Epoch   182: reducing learning rate of group 0 to 4.1177e-04.\n",
      "Epoch 182 / 500 | iteration 0 / 30 | Total Loss: 5.4118242263793945 | KNN Loss: 4.358661651611328 | BCE Loss: 1.0531625747680664\n",
      "Epoch 182 / 500 | iteration 5 / 30 | Total Loss: 5.386925220489502 | KNN Loss: 4.346718788146973 | BCE Loss: 1.0402064323425293\n",
      "Epoch 182 / 500 | iteration 10 / 30 | Total Loss: 5.37921142578125 | KNN Loss: 4.339116096496582 | BCE Loss: 1.040095329284668\n",
      "Epoch 182 / 500 | iteration 15 / 30 | Total Loss: 5.361865043640137 | KNN Loss: 4.365505695343018 | BCE Loss: 0.9963593482971191\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 182 / 500 | iteration 20 / 30 | Total Loss: 5.363295555114746 | KNN Loss: 4.338576316833496 | BCE Loss: 1.024718999862671\n",
      "Epoch 182 / 500 | iteration 25 / 30 | Total Loss: 5.350245475769043 | KNN Loss: 4.35152530670166 | BCE Loss: 0.9987201690673828\n",
      "Epoch 183 / 500 | iteration 0 / 30 | Total Loss: 5.352194786071777 | KNN Loss: 4.339862823486328 | BCE Loss: 1.0123322010040283\n",
      "Epoch 183 / 500 | iteration 5 / 30 | Total Loss: 5.396963119506836 | KNN Loss: 4.372692584991455 | BCE Loss: 1.02427077293396\n",
      "Epoch 183 / 500 | iteration 10 / 30 | Total Loss: 5.404500961303711 | KNN Loss: 4.373344421386719 | BCE Loss: 1.0311565399169922\n",
      "Epoch 183 / 500 | iteration 15 / 30 | Total Loss: 5.339503765106201 | KNN Loss: 4.3305277824401855 | BCE Loss: 1.008975863456726\n",
      "Epoch 183 / 500 | iteration 20 / 30 | Total Loss: 5.3696746826171875 | KNN Loss: 4.339744567871094 | BCE Loss: 1.0299301147460938\n",
      "Epoch 183 / 500 | iteration 25 / 30 | Total Loss: 5.399806022644043 | KNN Loss: 4.373886585235596 | BCE Loss: 1.0259196758270264\n",
      "Epoch 184 / 500 | iteration 0 / 30 | Total Loss: 5.348723411560059 | KNN Loss: 4.33711576461792 | BCE Loss: 1.0116077661514282\n",
      "Epoch 184 / 500 | iteration 5 / 30 | Total Loss: 5.391443252563477 | KNN Loss: 4.349060535430908 | BCE Loss: 1.0423824787139893\n",
      "Epoch 184 / 500 | iteration 10 / 30 | Total Loss: 5.408103942871094 | KNN Loss: 4.375583648681641 | BCE Loss: 1.0325205326080322\n",
      "Epoch 184 / 500 | iteration 15 / 30 | Total Loss: 5.3446502685546875 | KNN Loss: 4.331053733825684 | BCE Loss: 1.013596773147583\n",
      "Epoch 184 / 500 | iteration 20 / 30 | Total Loss: 5.329930782318115 | KNN Loss: 4.31191349029541 | BCE Loss: 1.018017292022705\n",
      "Epoch 184 / 500 | iteration 25 / 30 | Total Loss: 5.348265647888184 | KNN Loss: 4.313453674316406 | BCE Loss: 1.0348119735717773\n",
      "Epoch 185 / 500 | iteration 0 / 30 | Total Loss: 5.334957122802734 | KNN Loss: 4.337078094482422 | BCE Loss: 0.9978790283203125\n",
      "Epoch 185 / 500 | iteration 5 / 30 | Total Loss: 5.3973588943481445 | KNN Loss: 4.347815036773682 | BCE Loss: 1.049543857574463\n",
      "Epoch 185 / 500 | iteration 10 / 30 | Total Loss: 5.403589248657227 | KNN Loss: 4.396380424499512 | BCE Loss: 1.0072089433670044\n",
      "Epoch 185 / 500 | iteration 15 / 30 | Total Loss: 5.391109466552734 | KNN Loss: 4.360274314880371 | BCE Loss: 1.0308353900909424\n",
      "Epoch 185 / 500 | iteration 20 / 30 | Total Loss: 5.406691551208496 | KNN Loss: 4.3627400398254395 | BCE Loss: 1.0439517498016357\n",
      "Epoch 185 / 500 | iteration 25 / 30 | Total Loss: 5.422662734985352 | KNN Loss: 4.410265922546387 | BCE Loss: 1.0123968124389648\n",
      "Epoch 186 / 500 | iteration 0 / 30 | Total Loss: 5.416492462158203 | KNN Loss: 4.394046306610107 | BCE Loss: 1.0224461555480957\n",
      "Epoch 186 / 500 | iteration 5 / 30 | Total Loss: 5.4700608253479 | KNN Loss: 4.385286808013916 | BCE Loss: 1.0847740173339844\n",
      "Epoch 186 / 500 | iteration 10 / 30 | Total Loss: 5.371491432189941 | KNN Loss: 4.340663433074951 | BCE Loss: 1.0308281183242798\n",
      "Epoch 186 / 500 | iteration 15 / 30 | Total Loss: 5.389595031738281 | KNN Loss: 4.385722637176514 | BCE Loss: 1.0038721561431885\n",
      "Epoch 186 / 500 | iteration 20 / 30 | Total Loss: 5.355310440063477 | KNN Loss: 4.361210346221924 | BCE Loss: 0.9940998554229736\n",
      "Epoch 186 / 500 | iteration 25 / 30 | Total Loss: 5.320131301879883 | KNN Loss: 4.307577610015869 | BCE Loss: 1.0125538110733032\n",
      "Epoch 187 / 500 | iteration 0 / 30 | Total Loss: 5.331535339355469 | KNN Loss: 4.325644493103027 | BCE Loss: 1.0058910846710205\n",
      "Epoch 187 / 500 | iteration 5 / 30 | Total Loss: 5.3848652839660645 | KNN Loss: 4.342393398284912 | BCE Loss: 1.0424718856811523\n",
      "Epoch 187 / 500 | iteration 10 / 30 | Total Loss: 5.341310024261475 | KNN Loss: 4.301458358764648 | BCE Loss: 1.0398517847061157\n",
      "Epoch 187 / 500 | iteration 15 / 30 | Total Loss: 5.348620414733887 | KNN Loss: 4.322120189666748 | BCE Loss: 1.0265003442764282\n",
      "Epoch 187 / 500 | iteration 20 / 30 | Total Loss: 5.369524955749512 | KNN Loss: 4.313368320465088 | BCE Loss: 1.0561566352844238\n",
      "Epoch 187 / 500 | iteration 25 / 30 | Total Loss: 5.417704105377197 | KNN Loss: 4.406951904296875 | BCE Loss: 1.0107520818710327\n",
      "Epoch 188 / 500 | iteration 0 / 30 | Total Loss: 5.357840061187744 | KNN Loss: 4.322943210601807 | BCE Loss: 1.0348968505859375\n",
      "Epoch 188 / 500 | iteration 5 / 30 | Total Loss: 5.372544765472412 | KNN Loss: 4.361513614654541 | BCE Loss: 1.0110312700271606\n",
      "Epoch 188 / 500 | iteration 10 / 30 | Total Loss: 5.344640731811523 | KNN Loss: 4.351602554321289 | BCE Loss: 0.9930380582809448\n",
      "Epoch 188 / 500 | iteration 15 / 30 | Total Loss: 5.372498035430908 | KNN Loss: 4.357003688812256 | BCE Loss: 1.015494465827942\n",
      "Epoch 188 / 500 | iteration 20 / 30 | Total Loss: 5.397458553314209 | KNN Loss: 4.374425411224365 | BCE Loss: 1.0230330228805542\n",
      "Epoch 188 / 500 | iteration 25 / 30 | Total Loss: 5.350830078125 | KNN Loss: 4.3162360191345215 | BCE Loss: 1.0345942974090576\n",
      "Epoch 189 / 500 | iteration 0 / 30 | Total Loss: 5.353352069854736 | KNN Loss: 4.335999965667725 | BCE Loss: 1.0173521041870117\n",
      "Epoch 189 / 500 | iteration 5 / 30 | Total Loss: 5.3575849533081055 | KNN Loss: 4.316676616668701 | BCE Loss: 1.0409083366394043\n",
      "Epoch 189 / 500 | iteration 10 / 30 | Total Loss: 5.399889945983887 | KNN Loss: 4.379955768585205 | BCE Loss: 1.0199339389801025\n",
      "Epoch 189 / 500 | iteration 15 / 30 | Total Loss: 5.392961025238037 | KNN Loss: 4.380003452301025 | BCE Loss: 1.0129575729370117\n",
      "Epoch 189 / 500 | iteration 20 / 30 | Total Loss: 5.3587141036987305 | KNN Loss: 4.33804988861084 | BCE Loss: 1.0206642150878906\n",
      "Epoch 189 / 500 | iteration 25 / 30 | Total Loss: 5.422999382019043 | KNN Loss: 4.361804962158203 | BCE Loss: 1.061194658279419\n",
      "Epoch 190 / 500 | iteration 0 / 30 | Total Loss: 5.368168830871582 | KNN Loss: 4.32495641708374 | BCE Loss: 1.0432122945785522\n",
      "Epoch 190 / 500 | iteration 5 / 30 | Total Loss: 5.313878059387207 | KNN Loss: 4.323272705078125 | BCE Loss: 0.9906051158905029\n",
      "Epoch 190 / 500 | iteration 10 / 30 | Total Loss: 5.358053207397461 | KNN Loss: 4.32509708404541 | BCE Loss: 1.0329558849334717\n",
      "Epoch 190 / 500 | iteration 15 / 30 | Total Loss: 5.349085330963135 | KNN Loss: 4.350209712982178 | BCE Loss: 0.9988755583763123\n",
      "Epoch 190 / 500 | iteration 20 / 30 | Total Loss: 5.333657264709473 | KNN Loss: 4.3211822509765625 | BCE Loss: 1.0124752521514893\n",
      "Epoch 190 / 500 | iteration 25 / 30 | Total Loss: 5.403273582458496 | KNN Loss: 4.3940253257751465 | BCE Loss: 1.0092483758926392\n",
      "Epoch 191 / 500 | iteration 0 / 30 | Total Loss: 5.383371353149414 | KNN Loss: 4.362170219421387 | BCE Loss: 1.021201252937317\n",
      "Epoch 191 / 500 | iteration 5 / 30 | Total Loss: 5.322664737701416 | KNN Loss: 4.3305463790893555 | BCE Loss: 0.9921185374259949\n",
      "Epoch 191 / 500 | iteration 10 / 30 | Total Loss: 5.381486415863037 | KNN Loss: 4.349699974060059 | BCE Loss: 1.0317864418029785\n",
      "Epoch 191 / 500 | iteration 15 / 30 | Total Loss: 5.401076316833496 | KNN Loss: 4.375204086303711 | BCE Loss: 1.025871992111206\n",
      "Epoch 191 / 500 | iteration 20 / 30 | Total Loss: 5.397747993469238 | KNN Loss: 4.349950790405273 | BCE Loss: 1.047797441482544\n",
      "Epoch 191 / 500 | iteration 25 / 30 | Total Loss: 5.312248229980469 | KNN Loss: 4.316790580749512 | BCE Loss: 0.9954574108123779\n",
      "Epoch 192 / 500 | iteration 0 / 30 | Total Loss: 5.358733177185059 | KNN Loss: 4.33369255065918 | BCE Loss: 1.0250407457351685\n",
      "Epoch 192 / 500 | iteration 5 / 30 | Total Loss: 5.400312900543213 | KNN Loss: 4.378596782684326 | BCE Loss: 1.0217161178588867\n",
      "Epoch 192 / 500 | iteration 10 / 30 | Total Loss: 5.363011360168457 | KNN Loss: 4.349703311920166 | BCE Loss: 1.013307809829712\n",
      "Epoch 192 / 500 | iteration 15 / 30 | Total Loss: 5.418366432189941 | KNN Loss: 4.410168647766113 | BCE Loss: 1.0081977844238281\n",
      "Epoch 192 / 500 | iteration 20 / 30 | Total Loss: 5.408295154571533 | KNN Loss: 4.35948371887207 | BCE Loss: 1.048811435699463\n",
      "Epoch 192 / 500 | iteration 25 / 30 | Total Loss: 5.436102867126465 | KNN Loss: 4.374495029449463 | BCE Loss: 1.061607837677002\n",
      "Epoch 193 / 500 | iteration 0 / 30 | Total Loss: 5.391411781311035 | KNN Loss: 4.362286567687988 | BCE Loss: 1.0291249752044678\n",
      "Epoch 193 / 500 | iteration 5 / 30 | Total Loss: 5.386319160461426 | KNN Loss: 4.360692977905273 | BCE Loss: 1.0256259441375732\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 193 / 500 | iteration 10 / 30 | Total Loss: 5.422121047973633 | KNN Loss: 4.388824939727783 | BCE Loss: 1.0332963466644287\n",
      "Epoch 193 / 500 | iteration 15 / 30 | Total Loss: 5.366623401641846 | KNN Loss: 4.3213419914245605 | BCE Loss: 1.0452812910079956\n",
      "Epoch 193 / 500 | iteration 20 / 30 | Total Loss: 5.3987135887146 | KNN Loss: 4.381022930145264 | BCE Loss: 1.0176905393600464\n",
      "Epoch 193 / 500 | iteration 25 / 30 | Total Loss: 5.38063907623291 | KNN Loss: 4.34373140335083 | BCE Loss: 1.036907434463501\n",
      "Epoch 194 / 500 | iteration 0 / 30 | Total Loss: 5.374362468719482 | KNN Loss: 4.344920635223389 | BCE Loss: 1.0294419527053833\n",
      "Epoch 194 / 500 | iteration 5 / 30 | Total Loss: 5.408993721008301 | KNN Loss: 4.383683204650879 | BCE Loss: 1.025310754776001\n",
      "Epoch 194 / 500 | iteration 10 / 30 | Total Loss: 5.3824968338012695 | KNN Loss: 4.349276065826416 | BCE Loss: 1.0332205295562744\n",
      "Epoch 194 / 500 | iteration 15 / 30 | Total Loss: 5.442544937133789 | KNN Loss: 4.394824504852295 | BCE Loss: 1.0477203130722046\n",
      "Epoch 194 / 500 | iteration 20 / 30 | Total Loss: 5.380170822143555 | KNN Loss: 4.3339128494262695 | BCE Loss: 1.0462580919265747\n",
      "Epoch 194 / 500 | iteration 25 / 30 | Total Loss: 5.395277976989746 | KNN Loss: 4.355030536651611 | BCE Loss: 1.0402474403381348\n",
      "Epoch 195 / 500 | iteration 0 / 30 | Total Loss: 5.380937576293945 | KNN Loss: 4.340805530548096 | BCE Loss: 1.0401320457458496\n",
      "Epoch 195 / 500 | iteration 5 / 30 | Total Loss: 5.392827033996582 | KNN Loss: 4.350283622741699 | BCE Loss: 1.0425432920455933\n",
      "Epoch 195 / 500 | iteration 10 / 30 | Total Loss: 5.415011882781982 | KNN Loss: 4.3817925453186035 | BCE Loss: 1.0332194566726685\n",
      "Epoch 195 / 500 | iteration 15 / 30 | Total Loss: 5.368208885192871 | KNN Loss: 4.333365440368652 | BCE Loss: 1.0348432064056396\n",
      "Epoch 195 / 500 | iteration 20 / 30 | Total Loss: 5.393459320068359 | KNN Loss: 4.379656791687012 | BCE Loss: 1.013802409172058\n",
      "Epoch 195 / 500 | iteration 25 / 30 | Total Loss: 5.372771263122559 | KNN Loss: 4.343522548675537 | BCE Loss: 1.029248595237732\n",
      "Epoch 196 / 500 | iteration 0 / 30 | Total Loss: 5.299181938171387 | KNN Loss: 4.298295974731445 | BCE Loss: 1.0008862018585205\n",
      "Epoch 196 / 500 | iteration 5 / 30 | Total Loss: 5.378812789916992 | KNN Loss: 4.376687049865723 | BCE Loss: 1.0021255016326904\n",
      "Epoch 196 / 500 | iteration 10 / 30 | Total Loss: 5.368710994720459 | KNN Loss: 4.343856334686279 | BCE Loss: 1.0248546600341797\n",
      "Epoch 196 / 500 | iteration 15 / 30 | Total Loss: 5.399667739868164 | KNN Loss: 4.37235164642334 | BCE Loss: 1.0273158550262451\n",
      "Epoch 196 / 500 | iteration 20 / 30 | Total Loss: 5.383821487426758 | KNN Loss: 4.364582538604736 | BCE Loss: 1.019238829612732\n",
      "Epoch 196 / 500 | iteration 25 / 30 | Total Loss: 5.36858606338501 | KNN Loss: 4.32415771484375 | BCE Loss: 1.0444282293319702\n",
      "Epoch 197 / 500 | iteration 0 / 30 | Total Loss: 5.351869106292725 | KNN Loss: 4.340030670166016 | BCE Loss: 1.011838436126709\n",
      "Epoch 197 / 500 | iteration 5 / 30 | Total Loss: 5.414782524108887 | KNN Loss: 4.385997295379639 | BCE Loss: 1.028784990310669\n",
      "Epoch 197 / 500 | iteration 10 / 30 | Total Loss: 5.363576412200928 | KNN Loss: 4.348094940185547 | BCE Loss: 1.0154814720153809\n",
      "Epoch 197 / 500 | iteration 15 / 30 | Total Loss: 5.389017581939697 | KNN Loss: 4.347686767578125 | BCE Loss: 1.0413309335708618\n",
      "Epoch 197 / 500 | iteration 20 / 30 | Total Loss: 5.331673622131348 | KNN Loss: 4.335772514343262 | BCE Loss: 0.9959012269973755\n",
      "Epoch 197 / 500 | iteration 25 / 30 | Total Loss: 5.379692554473877 | KNN Loss: 4.382294654846191 | BCE Loss: 0.9973978996276855\n",
      "Epoch 198 / 500 | iteration 0 / 30 | Total Loss: 5.330732822418213 | KNN Loss: 4.299324989318848 | BCE Loss: 1.0314078330993652\n",
      "Epoch 198 / 500 | iteration 5 / 30 | Total Loss: 5.368349075317383 | KNN Loss: 4.340550899505615 | BCE Loss: 1.0277981758117676\n",
      "Epoch 198 / 500 | iteration 10 / 30 | Total Loss: 5.437269687652588 | KNN Loss: 4.395097255706787 | BCE Loss: 1.0421723127365112\n",
      "Epoch 198 / 500 | iteration 15 / 30 | Total Loss: 5.410734176635742 | KNN Loss: 4.39075231552124 | BCE Loss: 1.0199819803237915\n",
      "Epoch 198 / 500 | iteration 20 / 30 | Total Loss: 5.3786821365356445 | KNN Loss: 4.341766357421875 | BCE Loss: 1.0369155406951904\n",
      "Epoch 198 / 500 | iteration 25 / 30 | Total Loss: 5.408795356750488 | KNN Loss: 4.3748579025268555 | BCE Loss: 1.0339374542236328\n",
      "Epoch 199 / 500 | iteration 0 / 30 | Total Loss: 5.378715991973877 | KNN Loss: 4.342306613922119 | BCE Loss: 1.0364092588424683\n",
      "Epoch 199 / 500 | iteration 5 / 30 | Total Loss: 5.409839630126953 | KNN Loss: 4.365462779998779 | BCE Loss: 1.0443767309188843\n",
      "Epoch 199 / 500 | iteration 10 / 30 | Total Loss: 5.41867208480835 | KNN Loss: 4.376984596252441 | BCE Loss: 1.0416874885559082\n",
      "Epoch 199 / 500 | iteration 15 / 30 | Total Loss: 5.3484930992126465 | KNN Loss: 4.3285064697265625 | BCE Loss: 1.0199867486953735\n",
      "Epoch 199 / 500 | iteration 20 / 30 | Total Loss: 5.387579441070557 | KNN Loss: 4.375402927398682 | BCE Loss: 1.0121766328811646\n",
      "Epoch 199 / 500 | iteration 25 / 30 | Total Loss: 5.387571334838867 | KNN Loss: 4.347540855407715 | BCE Loss: 1.0400304794311523\n",
      "Epoch 200 / 500 | iteration 0 / 30 | Total Loss: 5.36936092376709 | KNN Loss: 4.34140157699585 | BCE Loss: 1.0279593467712402\n",
      "Epoch 200 / 500 | iteration 5 / 30 | Total Loss: 5.378835678100586 | KNN Loss: 4.351767063140869 | BCE Loss: 1.0270686149597168\n",
      "Epoch 200 / 500 | iteration 10 / 30 | Total Loss: 5.404430389404297 | KNN Loss: 4.3642706871032715 | BCE Loss: 1.0401597023010254\n",
      "Epoch 200 / 500 | iteration 15 / 30 | Total Loss: 5.372947692871094 | KNN Loss: 4.341432571411133 | BCE Loss: 1.0315150022506714\n",
      "Epoch 200 / 500 | iteration 20 / 30 | Total Loss: 5.331171989440918 | KNN Loss: 4.325460910797119 | BCE Loss: 1.0057108402252197\n",
      "Epoch 200 / 500 | iteration 25 / 30 | Total Loss: 5.359380722045898 | KNN Loss: 4.337763786315918 | BCE Loss: 1.0216171741485596\n",
      "Epoch 201 / 500 | iteration 0 / 30 | Total Loss: 5.393626689910889 | KNN Loss: 4.359172821044922 | BCE Loss: 1.0344538688659668\n",
      "Epoch 201 / 500 | iteration 5 / 30 | Total Loss: 5.436252593994141 | KNN Loss: 4.3837571144104 | BCE Loss: 1.0524953603744507\n",
      "Epoch 201 / 500 | iteration 10 / 30 | Total Loss: 5.323097229003906 | KNN Loss: 4.323451042175293 | BCE Loss: 0.999646008014679\n",
      "Epoch 201 / 500 | iteration 15 / 30 | Total Loss: 5.374497413635254 | KNN Loss: 4.362098217010498 | BCE Loss: 1.0123989582061768\n",
      "Epoch 201 / 500 | iteration 20 / 30 | Total Loss: 5.400540351867676 | KNN Loss: 4.403325080871582 | BCE Loss: 0.9972154498100281\n",
      "Epoch 201 / 500 | iteration 25 / 30 | Total Loss: 5.420640468597412 | KNN Loss: 4.363590240478516 | BCE Loss: 1.0570502281188965\n",
      "Epoch 202 / 500 | iteration 0 / 30 | Total Loss: 5.354413986206055 | KNN Loss: 4.348093509674072 | BCE Loss: 1.0063203573226929\n",
      "Epoch 202 / 500 | iteration 5 / 30 | Total Loss: 5.42928409576416 | KNN Loss: 4.352791786193848 | BCE Loss: 1.0764925479888916\n",
      "Epoch 202 / 500 | iteration 10 / 30 | Total Loss: 5.367475986480713 | KNN Loss: 4.345170974731445 | BCE Loss: 1.0223050117492676\n",
      "Epoch 202 / 500 | iteration 15 / 30 | Total Loss: 5.361572265625 | KNN Loss: 4.3431172370910645 | BCE Loss: 1.0184552669525146\n",
      "Epoch 202 / 500 | iteration 20 / 30 | Total Loss: 5.404263496398926 | KNN Loss: 4.366538047790527 | BCE Loss: 1.0377254486083984\n",
      "Epoch 202 / 500 | iteration 25 / 30 | Total Loss: 5.43858528137207 | KNN Loss: 4.377411842346191 | BCE Loss: 1.061173439025879\n",
      "Epoch 203 / 500 | iteration 0 / 30 | Total Loss: 5.391161918640137 | KNN Loss: 4.354140281677246 | BCE Loss: 1.0370217561721802\n",
      "Epoch 203 / 500 | iteration 5 / 30 | Total Loss: 5.349843978881836 | KNN Loss: 4.349992275238037 | BCE Loss: 0.9998515844345093\n",
      "Epoch 203 / 500 | iteration 10 / 30 | Total Loss: 5.4187188148498535 | KNN Loss: 4.3892083168029785 | BCE Loss: 1.0295103788375854\n",
      "Epoch 203 / 500 | iteration 15 / 30 | Total Loss: 5.388949871063232 | KNN Loss: 4.361537456512451 | BCE Loss: 1.0274124145507812\n",
      "Epoch 203 / 500 | iteration 20 / 30 | Total Loss: 5.335057258605957 | KNN Loss: 4.330289840698242 | BCE Loss: 1.0047674179077148\n",
      "Epoch 203 / 500 | iteration 25 / 30 | Total Loss: 5.386730670928955 | KNN Loss: 4.3424296379089355 | BCE Loss: 1.04430091381073\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 204 / 500 | iteration 0 / 30 | Total Loss: 5.348715782165527 | KNN Loss: 4.320167064666748 | BCE Loss: 1.0285487174987793\n",
      "Epoch 204 / 500 | iteration 5 / 30 | Total Loss: 5.4134016036987305 | KNN Loss: 4.3543009757995605 | BCE Loss: 1.059100866317749\n",
      "Epoch 204 / 500 | iteration 10 / 30 | Total Loss: 5.359466552734375 | KNN Loss: 4.353501319885254 | BCE Loss: 1.0059654712677002\n",
      "Epoch 204 / 500 | iteration 15 / 30 | Total Loss: 5.384145736694336 | KNN Loss: 4.339105129241943 | BCE Loss: 1.0450408458709717\n",
      "Epoch 204 / 500 | iteration 20 / 30 | Total Loss: 5.352299213409424 | KNN Loss: 4.337447643280029 | BCE Loss: 1.0148515701293945\n",
      "Epoch 204 / 500 | iteration 25 / 30 | Total Loss: 5.327912330627441 | KNN Loss: 4.311300754547119 | BCE Loss: 1.0166113376617432\n",
      "Epoch 205 / 500 | iteration 0 / 30 | Total Loss: 5.382155418395996 | KNN Loss: 4.333001136779785 | BCE Loss: 1.049154281616211\n",
      "Epoch 205 / 500 | iteration 5 / 30 | Total Loss: 5.397663116455078 | KNN Loss: 4.369076728820801 | BCE Loss: 1.028586506843567\n",
      "Epoch 205 / 500 | iteration 10 / 30 | Total Loss: 5.351713180541992 | KNN Loss: 4.3218817710876465 | BCE Loss: 1.0298312902450562\n",
      "Epoch 205 / 500 | iteration 15 / 30 | Total Loss: 5.367273330688477 | KNN Loss: 4.373199462890625 | BCE Loss: 0.9940738677978516\n",
      "Epoch 205 / 500 | iteration 20 / 30 | Total Loss: 5.383154392242432 | KNN Loss: 4.36443567276001 | BCE Loss: 1.0187187194824219\n",
      "Epoch 205 / 500 | iteration 25 / 30 | Total Loss: 5.364006996154785 | KNN Loss: 4.34384822845459 | BCE Loss: 1.0201585292816162\n",
      "Epoch 206 / 500 | iteration 0 / 30 | Total Loss: 5.433445453643799 | KNN Loss: 4.384812355041504 | BCE Loss: 1.0486329793930054\n",
      "Epoch 206 / 500 | iteration 5 / 30 | Total Loss: 5.423569679260254 | KNN Loss: 4.388638973236084 | BCE Loss: 1.0349304676055908\n",
      "Epoch 206 / 500 | iteration 10 / 30 | Total Loss: 5.380887031555176 | KNN Loss: 4.354402542114258 | BCE Loss: 1.0264843702316284\n",
      "Epoch 206 / 500 | iteration 15 / 30 | Total Loss: 5.349030494689941 | KNN Loss: 4.331036567687988 | BCE Loss: 1.0179939270019531\n",
      "Epoch 206 / 500 | iteration 20 / 30 | Total Loss: 5.370681285858154 | KNN Loss: 4.344667434692383 | BCE Loss: 1.0260138511657715\n",
      "Epoch 206 / 500 | iteration 25 / 30 | Total Loss: 5.385836601257324 | KNN Loss: 4.358253479003906 | BCE Loss: 1.027583122253418\n",
      "Epoch 207 / 500 | iteration 0 / 30 | Total Loss: 5.328508377075195 | KNN Loss: 4.32920503616333 | BCE Loss: 0.9993034601211548\n",
      "Epoch 207 / 500 | iteration 5 / 30 | Total Loss: 5.443754196166992 | KNN Loss: 4.395195484161377 | BCE Loss: 1.0485589504241943\n",
      "Epoch 207 / 500 | iteration 10 / 30 | Total Loss: 5.39780855178833 | KNN Loss: 4.354082107543945 | BCE Loss: 1.0437264442443848\n",
      "Epoch 207 / 500 | iteration 15 / 30 | Total Loss: 5.358538627624512 | KNN Loss: 4.347181797027588 | BCE Loss: 1.011357069015503\n",
      "Epoch 207 / 500 | iteration 20 / 30 | Total Loss: 5.361703395843506 | KNN Loss: 4.350147724151611 | BCE Loss: 1.0115556716918945\n",
      "Epoch 207 / 500 | iteration 25 / 30 | Total Loss: 5.431123733520508 | KNN Loss: 4.405920505523682 | BCE Loss: 1.0252034664154053\n",
      "Epoch 208 / 500 | iteration 0 / 30 | Total Loss: 5.367280960083008 | KNN Loss: 4.349330902099609 | BCE Loss: 1.0179499387741089\n",
      "Epoch 208 / 500 | iteration 5 / 30 | Total Loss: 5.375594615936279 | KNN Loss: 4.341030597686768 | BCE Loss: 1.0345640182495117\n",
      "Epoch 208 / 500 | iteration 10 / 30 | Total Loss: 5.350826263427734 | KNN Loss: 4.3330607414245605 | BCE Loss: 1.017765760421753\n",
      "Epoch 208 / 500 | iteration 15 / 30 | Total Loss: 5.400975227355957 | KNN Loss: 4.368609428405762 | BCE Loss: 1.0323660373687744\n",
      "Epoch 208 / 500 | iteration 20 / 30 | Total Loss: 5.425146102905273 | KNN Loss: 4.385735034942627 | BCE Loss: 1.039410948753357\n",
      "Epoch 208 / 500 | iteration 25 / 30 | Total Loss: 5.407090187072754 | KNN Loss: 4.351680278778076 | BCE Loss: 1.0554101467132568\n",
      "Epoch   209: reducing learning rate of group 0 to 2.8824e-04.\n",
      "Epoch 209 / 500 | iteration 0 / 30 | Total Loss: 5.404376029968262 | KNN Loss: 4.397120952606201 | BCE Loss: 1.007254958152771\n",
      "Epoch 209 / 500 | iteration 5 / 30 | Total Loss: 5.4083662033081055 | KNN Loss: 4.36858606338501 | BCE Loss: 1.0397799015045166\n",
      "Epoch 209 / 500 | iteration 10 / 30 | Total Loss: 5.342004776000977 | KNN Loss: 4.314192771911621 | BCE Loss: 1.0278120040893555\n",
      "Epoch 209 / 500 | iteration 15 / 30 | Total Loss: 5.349688529968262 | KNN Loss: 4.309046268463135 | BCE Loss: 1.0406420230865479\n",
      "Epoch 209 / 500 | iteration 20 / 30 | Total Loss: 5.339413166046143 | KNN Loss: 4.329618453979492 | BCE Loss: 1.0097945928573608\n",
      "Epoch 209 / 500 | iteration 25 / 30 | Total Loss: 5.3803205490112305 | KNN Loss: 4.367630481719971 | BCE Loss: 1.0126903057098389\n",
      "Epoch 210 / 500 | iteration 0 / 30 | Total Loss: 5.3876471519470215 | KNN Loss: 4.366802215576172 | BCE Loss: 1.0208449363708496\n",
      "Epoch 210 / 500 | iteration 5 / 30 | Total Loss: 5.412388801574707 | KNN Loss: 4.383324146270752 | BCE Loss: 1.029064655303955\n",
      "Epoch 210 / 500 | iteration 10 / 30 | Total Loss: 5.355569839477539 | KNN Loss: 4.342929363250732 | BCE Loss: 1.0126402378082275\n",
      "Epoch 210 / 500 | iteration 15 / 30 | Total Loss: 5.391798973083496 | KNN Loss: 4.366135120391846 | BCE Loss: 1.0256638526916504\n",
      "Epoch 210 / 500 | iteration 20 / 30 | Total Loss: 5.426638603210449 | KNN Loss: 4.354720115661621 | BCE Loss: 1.0719184875488281\n",
      "Epoch 210 / 500 | iteration 25 / 30 | Total Loss: 5.340282440185547 | KNN Loss: 4.335945129394531 | BCE Loss: 1.0043374300003052\n",
      "Epoch 211 / 500 | iteration 0 / 30 | Total Loss: 5.360815525054932 | KNN Loss: 4.304507732391357 | BCE Loss: 1.0563079118728638\n",
      "Epoch 211 / 500 | iteration 5 / 30 | Total Loss: 5.313229084014893 | KNN Loss: 4.316773891448975 | BCE Loss: 0.9964553117752075\n",
      "Epoch 211 / 500 | iteration 10 / 30 | Total Loss: 5.341015815734863 | KNN Loss: 4.323491096496582 | BCE Loss: 1.0175249576568604\n",
      "Epoch 211 / 500 | iteration 15 / 30 | Total Loss: 5.365353584289551 | KNN Loss: 4.338954448699951 | BCE Loss: 1.0263988971710205\n",
      "Epoch 211 / 500 | iteration 20 / 30 | Total Loss: 5.350460529327393 | KNN Loss: 4.337813854217529 | BCE Loss: 1.0126466751098633\n",
      "Epoch 211 / 500 | iteration 25 / 30 | Total Loss: 5.392147541046143 | KNN Loss: 4.354964256286621 | BCE Loss: 1.0371832847595215\n",
      "Epoch 212 / 500 | iteration 0 / 30 | Total Loss: 5.362823009490967 | KNN Loss: 4.384139537811279 | BCE Loss: 0.9786834716796875\n",
      "Epoch 212 / 500 | iteration 5 / 30 | Total Loss: 5.37247896194458 | KNN Loss: 4.319390296936035 | BCE Loss: 1.0530885457992554\n",
      "Epoch 212 / 500 | iteration 10 / 30 | Total Loss: 5.362656116485596 | KNN Loss: 4.345537185668945 | BCE Loss: 1.01711905002594\n",
      "Epoch 212 / 500 | iteration 15 / 30 | Total Loss: 5.419571399688721 | KNN Loss: 4.3760786056518555 | BCE Loss: 1.0434927940368652\n",
      "Epoch 212 / 500 | iteration 20 / 30 | Total Loss: 5.350402355194092 | KNN Loss: 4.345696449279785 | BCE Loss: 1.0047059059143066\n",
      "Epoch 212 / 500 | iteration 25 / 30 | Total Loss: 5.361528396606445 | KNN Loss: 4.352147102355957 | BCE Loss: 1.0093810558319092\n",
      "Epoch 213 / 500 | iteration 0 / 30 | Total Loss: 5.436594009399414 | KNN Loss: 4.387135982513428 | BCE Loss: 1.0494582653045654\n",
      "Epoch 213 / 500 | iteration 5 / 30 | Total Loss: 5.385769367218018 | KNN Loss: 4.367830753326416 | BCE Loss: 1.0179386138916016\n",
      "Epoch 213 / 500 | iteration 10 / 30 | Total Loss: 5.369327068328857 | KNN Loss: 4.361103534698486 | BCE Loss: 1.008223533630371\n",
      "Epoch 213 / 500 | iteration 15 / 30 | Total Loss: 5.428171634674072 | KNN Loss: 4.384272575378418 | BCE Loss: 1.0438990592956543\n",
      "Epoch 213 / 500 | iteration 20 / 30 | Total Loss: 5.334143161773682 | KNN Loss: 4.343358993530273 | BCE Loss: 0.9907843470573425\n",
      "Epoch 213 / 500 | iteration 25 / 30 | Total Loss: 5.474295139312744 | KNN Loss: 4.427435874938965 | BCE Loss: 1.0468591451644897\n",
      "Epoch 214 / 500 | iteration 0 / 30 | Total Loss: 5.350829601287842 | KNN Loss: 4.353059768676758 | BCE Loss: 0.9977697730064392\n",
      "Epoch 214 / 500 | iteration 5 / 30 | Total Loss: 5.366631507873535 | KNN Loss: 4.328237533569336 | BCE Loss: 1.0383938550949097\n",
      "Epoch 214 / 500 | iteration 10 / 30 | Total Loss: 5.456910133361816 | KNN Loss: 4.380386829376221 | BCE Loss: 1.0765233039855957\n",
      "Epoch 214 / 500 | iteration 15 / 30 | Total Loss: 5.4241132736206055 | KNN Loss: 4.405419826507568 | BCE Loss: 1.0186936855316162\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 214 / 500 | iteration 20 / 30 | Total Loss: 5.411269664764404 | KNN Loss: 4.3769097328186035 | BCE Loss: 1.0343598127365112\n",
      "Epoch 214 / 500 | iteration 25 / 30 | Total Loss: 5.409047603607178 | KNN Loss: 4.370877742767334 | BCE Loss: 1.0381697416305542\n",
      "Epoch 215 / 500 | iteration 0 / 30 | Total Loss: 5.363781929016113 | KNN Loss: 4.362726211547852 | BCE Loss: 1.0010559558868408\n",
      "Epoch 215 / 500 | iteration 5 / 30 | Total Loss: 5.374637603759766 | KNN Loss: 4.3541364669799805 | BCE Loss: 1.020500898361206\n",
      "Epoch 215 / 500 | iteration 10 / 30 | Total Loss: 5.38748836517334 | KNN Loss: 4.374902725219727 | BCE Loss: 1.0125858783721924\n",
      "Epoch 215 / 500 | iteration 15 / 30 | Total Loss: 5.422930717468262 | KNN Loss: 4.409876823425293 | BCE Loss: 1.0130541324615479\n",
      "Epoch 215 / 500 | iteration 20 / 30 | Total Loss: 5.412209510803223 | KNN Loss: 4.361316680908203 | BCE Loss: 1.0508928298950195\n",
      "Epoch 215 / 500 | iteration 25 / 30 | Total Loss: 5.416443824768066 | KNN Loss: 4.371646881103516 | BCE Loss: 1.0447967052459717\n",
      "Epoch 216 / 500 | iteration 0 / 30 | Total Loss: 5.391617298126221 | KNN Loss: 4.372523307800293 | BCE Loss: 1.0190938711166382\n",
      "Epoch 216 / 500 | iteration 5 / 30 | Total Loss: 5.392520427703857 | KNN Loss: 4.333678245544434 | BCE Loss: 1.0588423013687134\n",
      "Epoch 216 / 500 | iteration 10 / 30 | Total Loss: 5.379169464111328 | KNN Loss: 4.371109962463379 | BCE Loss: 1.0080595016479492\n",
      "Epoch 216 / 500 | iteration 15 / 30 | Total Loss: 5.397082805633545 | KNN Loss: 4.352461814880371 | BCE Loss: 1.0446211099624634\n",
      "Epoch 216 / 500 | iteration 20 / 30 | Total Loss: 5.321218967437744 | KNN Loss: 4.319356918334961 | BCE Loss: 1.0018620491027832\n",
      "Epoch 216 / 500 | iteration 25 / 30 | Total Loss: 5.41731595993042 | KNN Loss: 4.398825645446777 | BCE Loss: 1.018490195274353\n",
      "Epoch 217 / 500 | iteration 0 / 30 | Total Loss: 5.356274127960205 | KNN Loss: 4.322322845458984 | BCE Loss: 1.0339512825012207\n",
      "Epoch 217 / 500 | iteration 5 / 30 | Total Loss: 5.379642009735107 | KNN Loss: 4.340609550476074 | BCE Loss: 1.0390324592590332\n",
      "Epoch 217 / 500 | iteration 10 / 30 | Total Loss: 5.3794660568237305 | KNN Loss: 4.358963489532471 | BCE Loss: 1.0205024480819702\n",
      "Epoch 217 / 500 | iteration 15 / 30 | Total Loss: 5.344570159912109 | KNN Loss: 4.30544376373291 | BCE Loss: 1.0391263961791992\n",
      "Epoch 217 / 500 | iteration 20 / 30 | Total Loss: 5.388774871826172 | KNN Loss: 4.365768909454346 | BCE Loss: 1.0230059623718262\n",
      "Epoch 217 / 500 | iteration 25 / 30 | Total Loss: 5.342602252960205 | KNN Loss: 4.335274696350098 | BCE Loss: 1.0073275566101074\n",
      "Epoch 218 / 500 | iteration 0 / 30 | Total Loss: 5.400461673736572 | KNN Loss: 4.393889904022217 | BCE Loss: 1.006571650505066\n",
      "Epoch 218 / 500 | iteration 5 / 30 | Total Loss: 5.36857795715332 | KNN Loss: 4.3121442794799805 | BCE Loss: 1.0564334392547607\n",
      "Epoch 218 / 500 | iteration 10 / 30 | Total Loss: 5.400535583496094 | KNN Loss: 4.357606410980225 | BCE Loss: 1.0429290533065796\n",
      "Epoch 218 / 500 | iteration 15 / 30 | Total Loss: 5.333600997924805 | KNN Loss: 4.343161582946777 | BCE Loss: 0.9904395341873169\n",
      "Epoch 218 / 500 | iteration 20 / 30 | Total Loss: 5.367334365844727 | KNN Loss: 4.329349040985107 | BCE Loss: 1.03798508644104\n",
      "Epoch 218 / 500 | iteration 25 / 30 | Total Loss: 5.392467498779297 | KNN Loss: 4.378300189971924 | BCE Loss: 1.014167070388794\n",
      "Epoch 219 / 500 | iteration 0 / 30 | Total Loss: 5.393710613250732 | KNN Loss: 4.389677047729492 | BCE Loss: 1.0040336847305298\n",
      "Epoch 219 / 500 | iteration 5 / 30 | Total Loss: 5.384029388427734 | KNN Loss: 4.362934589385986 | BCE Loss: 1.021094799041748\n",
      "Epoch 219 / 500 | iteration 10 / 30 | Total Loss: 5.390099048614502 | KNN Loss: 4.338080406188965 | BCE Loss: 1.052018642425537\n",
      "Epoch 219 / 500 | iteration 15 / 30 | Total Loss: 5.384294509887695 | KNN Loss: 4.368162631988525 | BCE Loss: 1.0161317586898804\n",
      "Epoch 219 / 500 | iteration 20 / 30 | Total Loss: 5.390178203582764 | KNN Loss: 4.363360404968262 | BCE Loss: 1.026817798614502\n",
      "Epoch 219 / 500 | iteration 25 / 30 | Total Loss: 5.381531238555908 | KNN Loss: 4.34503698348999 | BCE Loss: 1.0364941358566284\n",
      "Epoch   220: reducing learning rate of group 0 to 2.0177e-04.\n",
      "Epoch 220 / 500 | iteration 0 / 30 | Total Loss: 5.404226303100586 | KNN Loss: 4.354783058166504 | BCE Loss: 1.049443244934082\n",
      "Epoch 220 / 500 | iteration 5 / 30 | Total Loss: 5.451224327087402 | KNN Loss: 4.389471054077148 | BCE Loss: 1.061753511428833\n",
      "Epoch 220 / 500 | iteration 10 / 30 | Total Loss: 5.405402660369873 | KNN Loss: 4.376898288726807 | BCE Loss: 1.028504490852356\n",
      "Epoch 220 / 500 | iteration 15 / 30 | Total Loss: 5.408783912658691 | KNN Loss: 4.380709171295166 | BCE Loss: 1.0280746221542358\n",
      "Epoch 220 / 500 | iteration 20 / 30 | Total Loss: 5.391711711883545 | KNN Loss: 4.344230651855469 | BCE Loss: 1.0474810600280762\n",
      "Epoch 220 / 500 | iteration 25 / 30 | Total Loss: 5.345841884613037 | KNN Loss: 4.351424694061279 | BCE Loss: 0.9944170117378235\n",
      "Epoch 221 / 500 | iteration 0 / 30 | Total Loss: 5.382087230682373 | KNN Loss: 4.353724479675293 | BCE Loss: 1.0283626317977905\n",
      "Epoch 221 / 500 | iteration 5 / 30 | Total Loss: 5.433663845062256 | KNN Loss: 4.390722274780273 | BCE Loss: 1.042941689491272\n",
      "Epoch 221 / 500 | iteration 10 / 30 | Total Loss: 5.395442008972168 | KNN Loss: 4.375678539276123 | BCE Loss: 1.019763708114624\n",
      "Epoch 221 / 500 | iteration 15 / 30 | Total Loss: 5.379696369171143 | KNN Loss: 4.3510284423828125 | BCE Loss: 1.02866792678833\n",
      "Epoch 221 / 500 | iteration 20 / 30 | Total Loss: 5.358133792877197 | KNN Loss: 4.350461483001709 | BCE Loss: 1.0076724290847778\n",
      "Epoch 221 / 500 | iteration 25 / 30 | Total Loss: 5.3889641761779785 | KNN Loss: 4.345860958099365 | BCE Loss: 1.0431030988693237\n",
      "Epoch 222 / 500 | iteration 0 / 30 | Total Loss: 5.3558244705200195 | KNN Loss: 4.341090202331543 | BCE Loss: 1.0147340297698975\n",
      "Epoch 222 / 500 | iteration 5 / 30 | Total Loss: 5.4160943031311035 | KNN Loss: 4.376521587371826 | BCE Loss: 1.0395727157592773\n",
      "Epoch 222 / 500 | iteration 10 / 30 | Total Loss: 5.371201992034912 | KNN Loss: 4.347853183746338 | BCE Loss: 1.0233488082885742\n",
      "Epoch 222 / 500 | iteration 15 / 30 | Total Loss: 5.360626220703125 | KNN Loss: 4.348968029022217 | BCE Loss: 1.0116581916809082\n",
      "Epoch 222 / 500 | iteration 20 / 30 | Total Loss: 5.385586738586426 | KNN Loss: 4.371110916137695 | BCE Loss: 1.0144758224487305\n",
      "Epoch 222 / 500 | iteration 25 / 30 | Total Loss: 5.344686985015869 | KNN Loss: 4.340094089508057 | BCE Loss: 1.004592776298523\n",
      "Epoch 223 / 500 | iteration 0 / 30 | Total Loss: 5.386096954345703 | KNN Loss: 4.363804340362549 | BCE Loss: 1.0222928524017334\n",
      "Epoch 223 / 500 | iteration 5 / 30 | Total Loss: 5.438915252685547 | KNN Loss: 4.400020599365234 | BCE Loss: 1.0388946533203125\n",
      "Epoch 223 / 500 | iteration 10 / 30 | Total Loss: 5.348407745361328 | KNN Loss: 4.33268928527832 | BCE Loss: 1.0157184600830078\n",
      "Epoch 223 / 500 | iteration 15 / 30 | Total Loss: 5.395968914031982 | KNN Loss: 4.369083404541016 | BCE Loss: 1.0268853902816772\n",
      "Epoch 223 / 500 | iteration 20 / 30 | Total Loss: 5.348869323730469 | KNN Loss: 4.329097270965576 | BCE Loss: 1.019771933555603\n",
      "Epoch 223 / 500 | iteration 25 / 30 | Total Loss: 5.333327293395996 | KNN Loss: 4.305236339569092 | BCE Loss: 1.0280907154083252\n",
      "Epoch 224 / 500 | iteration 0 / 30 | Total Loss: 5.384067058563232 | KNN Loss: 4.345277309417725 | BCE Loss: 1.0387896299362183\n",
      "Epoch 224 / 500 | iteration 5 / 30 | Total Loss: 5.336603164672852 | KNN Loss: 4.308100700378418 | BCE Loss: 1.0285022258758545\n",
      "Epoch 224 / 500 | iteration 10 / 30 | Total Loss: 5.363916873931885 | KNN Loss: 4.339568614959717 | BCE Loss: 1.024348258972168\n",
      "Epoch 224 / 500 | iteration 15 / 30 | Total Loss: 5.3607354164123535 | KNN Loss: 4.363770008087158 | BCE Loss: 0.9969652891159058\n",
      "Epoch 224 / 500 | iteration 20 / 30 | Total Loss: 5.441618919372559 | KNN Loss: 4.403019905090332 | BCE Loss: 1.0385990142822266\n",
      "Epoch 224 / 500 | iteration 25 / 30 | Total Loss: 5.355229377746582 | KNN Loss: 4.344507694244385 | BCE Loss: 1.0107219219207764\n",
      "Epoch 225 / 500 | iteration 0 / 30 | Total Loss: 5.371330261230469 | KNN Loss: 4.3351826667785645 | BCE Loss: 1.0361473560333252\n",
      "Epoch 225 / 500 | iteration 5 / 30 | Total Loss: 5.442253589630127 | KNN Loss: 4.419745922088623 | BCE Loss: 1.022507667541504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 225 / 500 | iteration 10 / 30 | Total Loss: 5.378992557525635 | KNN Loss: 4.326544761657715 | BCE Loss: 1.0524479150772095\n",
      "Epoch 225 / 500 | iteration 15 / 30 | Total Loss: 5.3792924880981445 | KNN Loss: 4.349289417266846 | BCE Loss: 1.0300031900405884\n",
      "Epoch 225 / 500 | iteration 20 / 30 | Total Loss: 5.3625359535217285 | KNN Loss: 4.318331718444824 | BCE Loss: 1.0442043542861938\n",
      "Epoch 225 / 500 | iteration 25 / 30 | Total Loss: 5.359462261199951 | KNN Loss: 4.3393378257751465 | BCE Loss: 1.0201244354248047\n",
      "Epoch 226 / 500 | iteration 0 / 30 | Total Loss: 5.340794563293457 | KNN Loss: 4.312992095947266 | BCE Loss: 1.027802586555481\n",
      "Epoch 226 / 500 | iteration 5 / 30 | Total Loss: 5.435834884643555 | KNN Loss: 4.394080638885498 | BCE Loss: 1.0417542457580566\n",
      "Epoch 226 / 500 | iteration 10 / 30 | Total Loss: 5.371147632598877 | KNN Loss: 4.328165531158447 | BCE Loss: 1.0429822206497192\n",
      "Epoch 226 / 500 | iteration 15 / 30 | Total Loss: 5.376134395599365 | KNN Loss: 4.35329532623291 | BCE Loss: 1.022839069366455\n",
      "Epoch 226 / 500 | iteration 20 / 30 | Total Loss: 5.371401786804199 | KNN Loss: 4.337770938873291 | BCE Loss: 1.033630609512329\n",
      "Epoch 226 / 500 | iteration 25 / 30 | Total Loss: 5.346706390380859 | KNN Loss: 4.334840297698975 | BCE Loss: 1.0118662118911743\n",
      "Epoch 227 / 500 | iteration 0 / 30 | Total Loss: 5.356536865234375 | KNN Loss: 4.348750591278076 | BCE Loss: 1.0077863931655884\n",
      "Epoch 227 / 500 | iteration 5 / 30 | Total Loss: 5.4007415771484375 | KNN Loss: 4.368901252746582 | BCE Loss: 1.0318405628204346\n",
      "Epoch 227 / 500 | iteration 10 / 30 | Total Loss: 5.390899181365967 | KNN Loss: 4.351990699768066 | BCE Loss: 1.0389084815979004\n",
      "Epoch 227 / 500 | iteration 15 / 30 | Total Loss: 5.3784499168396 | KNN Loss: 4.359612941741943 | BCE Loss: 1.0188370943069458\n",
      "Epoch 227 / 500 | iteration 20 / 30 | Total Loss: 5.33597469329834 | KNN Loss: 4.329866886138916 | BCE Loss: 1.006108045578003\n",
      "Epoch 227 / 500 | iteration 25 / 30 | Total Loss: 5.4562883377075195 | KNN Loss: 4.417484760284424 | BCE Loss: 1.0388034582138062\n",
      "Epoch 228 / 500 | iteration 0 / 30 | Total Loss: 5.399516582489014 | KNN Loss: 4.363259315490723 | BCE Loss: 1.036257266998291\n",
      "Epoch 228 / 500 | iteration 5 / 30 | Total Loss: 5.391166687011719 | KNN Loss: 4.350427627563477 | BCE Loss: 1.0407390594482422\n",
      "Epoch 228 / 500 | iteration 10 / 30 | Total Loss: 5.345053195953369 | KNN Loss: 4.326231479644775 | BCE Loss: 1.0188218355178833\n",
      "Epoch 228 / 500 | iteration 15 / 30 | Total Loss: 5.482648849487305 | KNN Loss: 4.435418128967285 | BCE Loss: 1.0472304821014404\n",
      "Epoch 228 / 500 | iteration 20 / 30 | Total Loss: 5.349946022033691 | KNN Loss: 4.305075645446777 | BCE Loss: 1.0448706150054932\n",
      "Epoch 228 / 500 | iteration 25 / 30 | Total Loss: 5.333268165588379 | KNN Loss: 4.323750972747803 | BCE Loss: 1.0095174312591553\n",
      "Epoch 229 / 500 | iteration 0 / 30 | Total Loss: 5.4568939208984375 | KNN Loss: 4.427073955535889 | BCE Loss: 1.0298199653625488\n",
      "Epoch 229 / 500 | iteration 5 / 30 | Total Loss: 5.338620662689209 | KNN Loss: 4.32172966003418 | BCE Loss: 1.0168910026550293\n",
      "Epoch 229 / 500 | iteration 10 / 30 | Total Loss: 5.37282657623291 | KNN Loss: 4.324865341186523 | BCE Loss: 1.0479612350463867\n",
      "Epoch 229 / 500 | iteration 15 / 30 | Total Loss: 5.3618316650390625 | KNN Loss: 4.345383644104004 | BCE Loss: 1.0164477825164795\n",
      "Epoch 229 / 500 | iteration 20 / 30 | Total Loss: 5.427085876464844 | KNN Loss: 4.386419296264648 | BCE Loss: 1.0406668186187744\n",
      "Epoch 229 / 500 | iteration 25 / 30 | Total Loss: 5.3632588386535645 | KNN Loss: 4.333279609680176 | BCE Loss: 1.0299792289733887\n",
      "Epoch 230 / 500 | iteration 0 / 30 | Total Loss: 5.428050994873047 | KNN Loss: 4.3960981369018555 | BCE Loss: 1.0319526195526123\n",
      "Epoch 230 / 500 | iteration 5 / 30 | Total Loss: 5.324600696563721 | KNN Loss: 4.3256449699401855 | BCE Loss: 0.9989558458328247\n",
      "Epoch 230 / 500 | iteration 10 / 30 | Total Loss: 5.339108467102051 | KNN Loss: 4.318021297454834 | BCE Loss: 1.0210869312286377\n",
      "Epoch 230 / 500 | iteration 15 / 30 | Total Loss: 5.398349761962891 | KNN Loss: 4.372533321380615 | BCE Loss: 1.0258166790008545\n",
      "Epoch 230 / 500 | iteration 20 / 30 | Total Loss: 5.409839630126953 | KNN Loss: 4.374634742736816 | BCE Loss: 1.0352051258087158\n",
      "Epoch 230 / 500 | iteration 25 / 30 | Total Loss: 5.359282493591309 | KNN Loss: 4.333249568939209 | BCE Loss: 1.0260329246520996\n",
      "Epoch   231: reducing learning rate of group 0 to 1.4124e-04.\n",
      "Epoch 231 / 500 | iteration 0 / 30 | Total Loss: 5.353969097137451 | KNN Loss: 4.330410480499268 | BCE Loss: 1.0235586166381836\n",
      "Epoch 231 / 500 | iteration 5 / 30 | Total Loss: 5.361473560333252 | KNN Loss: 4.307326793670654 | BCE Loss: 1.0541467666625977\n",
      "Epoch 231 / 500 | iteration 10 / 30 | Total Loss: 5.3955278396606445 | KNN Loss: 4.356134414672852 | BCE Loss: 1.0393933057785034\n",
      "Epoch 231 / 500 | iteration 15 / 30 | Total Loss: 5.32870626449585 | KNN Loss: 4.3293561935424805 | BCE Loss: 0.9993501901626587\n",
      "Epoch 231 / 500 | iteration 20 / 30 | Total Loss: 5.418649673461914 | KNN Loss: 4.378030776977539 | BCE Loss: 1.040618658065796\n",
      "Epoch 231 / 500 | iteration 25 / 30 | Total Loss: 5.3839240074157715 | KNN Loss: 4.3493523597717285 | BCE Loss: 1.0345715284347534\n",
      "Epoch 232 / 500 | iteration 0 / 30 | Total Loss: 5.370273590087891 | KNN Loss: 4.352526664733887 | BCE Loss: 1.0177468061447144\n",
      "Epoch 232 / 500 | iteration 5 / 30 | Total Loss: 5.352738380432129 | KNN Loss: 4.326234340667725 | BCE Loss: 1.0265040397644043\n",
      "Epoch 232 / 500 | iteration 10 / 30 | Total Loss: 5.382238388061523 | KNN Loss: 4.347023010253906 | BCE Loss: 1.035215139389038\n",
      "Epoch 232 / 500 | iteration 15 / 30 | Total Loss: 5.384876251220703 | KNN Loss: 4.368947505950928 | BCE Loss: 1.015928864479065\n",
      "Epoch 232 / 500 | iteration 20 / 30 | Total Loss: 5.429328441619873 | KNN Loss: 4.405598163604736 | BCE Loss: 1.0237302780151367\n",
      "Epoch 232 / 500 | iteration 25 / 30 | Total Loss: 5.403074741363525 | KNN Loss: 4.373953819274902 | BCE Loss: 1.029120922088623\n",
      "Epoch 233 / 500 | iteration 0 / 30 | Total Loss: 5.437509536743164 | KNN Loss: 4.387994289398193 | BCE Loss: 1.0495153665542603\n",
      "Epoch 233 / 500 | iteration 5 / 30 | Total Loss: 5.442879676818848 | KNN Loss: 4.38625431060791 | BCE Loss: 1.0566251277923584\n",
      "Epoch 233 / 500 | iteration 10 / 30 | Total Loss: 5.426000595092773 | KNN Loss: 4.383636474609375 | BCE Loss: 1.0423638820648193\n",
      "Epoch 233 / 500 | iteration 15 / 30 | Total Loss: 5.361050605773926 | KNN Loss: 4.351578235626221 | BCE Loss: 1.0094726085662842\n",
      "Epoch 233 / 500 | iteration 20 / 30 | Total Loss: 5.334760665893555 | KNN Loss: 4.323885917663574 | BCE Loss: 1.010874629020691\n",
      "Epoch 233 / 500 | iteration 25 / 30 | Total Loss: 5.40192985534668 | KNN Loss: 4.387897491455078 | BCE Loss: 1.0140321254730225\n",
      "Epoch 234 / 500 | iteration 0 / 30 | Total Loss: 5.371293067932129 | KNN Loss: 4.328575134277344 | BCE Loss: 1.042717695236206\n",
      "Epoch 234 / 500 | iteration 5 / 30 | Total Loss: 5.385784149169922 | KNN Loss: 4.383231163024902 | BCE Loss: 1.00255286693573\n",
      "Epoch 234 / 500 | iteration 10 / 30 | Total Loss: 5.343756675720215 | KNN Loss: 4.333349704742432 | BCE Loss: 1.0104069709777832\n",
      "Epoch 234 / 500 | iteration 15 / 30 | Total Loss: 5.339034557342529 | KNN Loss: 4.325744152069092 | BCE Loss: 1.013290524482727\n",
      "Epoch 234 / 500 | iteration 20 / 30 | Total Loss: 5.3816938400268555 | KNN Loss: 4.378046989440918 | BCE Loss: 1.003646969795227\n",
      "Epoch 234 / 500 | iteration 25 / 30 | Total Loss: 5.398303031921387 | KNN Loss: 4.355151176452637 | BCE Loss: 1.0431517362594604\n",
      "Epoch 235 / 500 | iteration 0 / 30 | Total Loss: 5.347347736358643 | KNN Loss: 4.327576637268066 | BCE Loss: 1.0197710990905762\n",
      "Epoch 235 / 500 | iteration 5 / 30 | Total Loss: 5.343685150146484 | KNN Loss: 4.323127746582031 | BCE Loss: 1.0205576419830322\n",
      "Epoch 235 / 500 | iteration 10 / 30 | Total Loss: 5.3444318771362305 | KNN Loss: 4.316233158111572 | BCE Loss: 1.0281987190246582\n",
      "Epoch 235 / 500 | iteration 15 / 30 | Total Loss: 5.35958194732666 | KNN Loss: 4.352808475494385 | BCE Loss: 1.0067732334136963\n",
      "Epoch 235 / 500 | iteration 20 / 30 | Total Loss: 5.380978584289551 | KNN Loss: 4.365772247314453 | BCE Loss: 1.0152064561843872\n",
      "Epoch 235 / 500 | iteration 25 / 30 | Total Loss: 5.349981307983398 | KNN Loss: 4.313515663146973 | BCE Loss: 1.0364654064178467\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 236 / 500 | iteration 0 / 30 | Total Loss: 5.339364528656006 | KNN Loss: 4.337825775146484 | BCE Loss: 1.001538634300232\n",
      "Epoch 236 / 500 | iteration 5 / 30 | Total Loss: 5.416865348815918 | KNN Loss: 4.333803653717041 | BCE Loss: 1.0830614566802979\n",
      "Epoch 236 / 500 | iteration 10 / 30 | Total Loss: 5.37565803527832 | KNN Loss: 4.3523759841918945 | BCE Loss: 1.0232818126678467\n",
      "Epoch 236 / 500 | iteration 15 / 30 | Total Loss: 5.332500457763672 | KNN Loss: 4.334814071655273 | BCE Loss: 0.9976861476898193\n",
      "Epoch 236 / 500 | iteration 20 / 30 | Total Loss: 5.3318281173706055 | KNN Loss: 4.311900615692139 | BCE Loss: 1.019927740097046\n",
      "Epoch 236 / 500 | iteration 25 / 30 | Total Loss: 5.401526927947998 | KNN Loss: 4.381556510925293 | BCE Loss: 1.019970417022705\n",
      "Epoch 237 / 500 | iteration 0 / 30 | Total Loss: 5.421878337860107 | KNN Loss: 4.350667953491211 | BCE Loss: 1.071210503578186\n",
      "Epoch 237 / 500 | iteration 5 / 30 | Total Loss: 5.396313667297363 | KNN Loss: 4.38993501663208 | BCE Loss: 1.0063786506652832\n",
      "Epoch 237 / 500 | iteration 10 / 30 | Total Loss: 5.3682451248168945 | KNN Loss: 4.337047576904297 | BCE Loss: 1.0311973094940186\n",
      "Epoch 237 / 500 | iteration 15 / 30 | Total Loss: 5.4228386878967285 | KNN Loss: 4.3964385986328125 | BCE Loss: 1.0263999700546265\n",
      "Epoch 237 / 500 | iteration 20 / 30 | Total Loss: 5.361523628234863 | KNN Loss: 4.3238396644592285 | BCE Loss: 1.0376840829849243\n",
      "Epoch 237 / 500 | iteration 25 / 30 | Total Loss: 5.365270137786865 | KNN Loss: 4.3343119621276855 | BCE Loss: 1.0309581756591797\n",
      "Epoch 238 / 500 | iteration 0 / 30 | Total Loss: 5.402887344360352 | KNN Loss: 4.3678388595581055 | BCE Loss: 1.0350486040115356\n",
      "Epoch 238 / 500 | iteration 5 / 30 | Total Loss: 5.401335716247559 | KNN Loss: 4.351831436157227 | BCE Loss: 1.0495041608810425\n",
      "Epoch 238 / 500 | iteration 10 / 30 | Total Loss: 5.396720886230469 | KNN Loss: 4.35164737701416 | BCE Loss: 1.0450732707977295\n",
      "Epoch 238 / 500 | iteration 15 / 30 | Total Loss: 5.4375319480896 | KNN Loss: 4.390410423278809 | BCE Loss: 1.0471216440200806\n",
      "Epoch 238 / 500 | iteration 20 / 30 | Total Loss: 5.336928367614746 | KNN Loss: 4.320071697235107 | BCE Loss: 1.0168564319610596\n",
      "Epoch 238 / 500 | iteration 25 / 30 | Total Loss: 5.403914451599121 | KNN Loss: 4.356314659118652 | BCE Loss: 1.0475997924804688\n",
      "Epoch 239 / 500 | iteration 0 / 30 | Total Loss: 5.3787431716918945 | KNN Loss: 4.357219696044922 | BCE Loss: 1.0215232372283936\n",
      "Epoch 239 / 500 | iteration 5 / 30 | Total Loss: 5.384614944458008 | KNN Loss: 4.3516364097595215 | BCE Loss: 1.0329785346984863\n",
      "Epoch 239 / 500 | iteration 10 / 30 | Total Loss: 5.445923805236816 | KNN Loss: 4.397007465362549 | BCE Loss: 1.0489163398742676\n",
      "Epoch 239 / 500 | iteration 15 / 30 | Total Loss: 5.4610185623168945 | KNN Loss: 4.4118828773498535 | BCE Loss: 1.049135446548462\n",
      "Epoch 239 / 500 | iteration 20 / 30 | Total Loss: 5.31442928314209 | KNN Loss: 4.3128790855407715 | BCE Loss: 1.0015500783920288\n",
      "Epoch 239 / 500 | iteration 25 / 30 | Total Loss: 5.351077079772949 | KNN Loss: 4.3148956298828125 | BCE Loss: 1.0361813306808472\n",
      "Epoch 240 / 500 | iteration 0 / 30 | Total Loss: 5.381031036376953 | KNN Loss: 4.338265895843506 | BCE Loss: 1.0427653789520264\n",
      "Epoch 240 / 500 | iteration 5 / 30 | Total Loss: 5.408134460449219 | KNN Loss: 4.399756908416748 | BCE Loss: 1.0083774328231812\n",
      "Epoch 240 / 500 | iteration 10 / 30 | Total Loss: 5.35675573348999 | KNN Loss: 4.3479838371276855 | BCE Loss: 1.0087718963623047\n",
      "Epoch 240 / 500 | iteration 15 / 30 | Total Loss: 5.425242900848389 | KNN Loss: 4.385260581970215 | BCE Loss: 1.0399824380874634\n",
      "Epoch 240 / 500 | iteration 20 / 30 | Total Loss: 5.359597206115723 | KNN Loss: 4.329944133758545 | BCE Loss: 1.0296533107757568\n",
      "Epoch 240 / 500 | iteration 25 / 30 | Total Loss: 5.352207183837891 | KNN Loss: 4.321249485015869 | BCE Loss: 1.0309576988220215\n",
      "Epoch 241 / 500 | iteration 0 / 30 | Total Loss: 5.382817268371582 | KNN Loss: 4.359050750732422 | BCE Loss: 1.0237667560577393\n",
      "Epoch 241 / 500 | iteration 5 / 30 | Total Loss: 5.336568355560303 | KNN Loss: 4.335440635681152 | BCE Loss: 1.0011276006698608\n",
      "Epoch 241 / 500 | iteration 10 / 30 | Total Loss: 5.40726375579834 | KNN Loss: 4.384820461273193 | BCE Loss: 1.0224435329437256\n",
      "Epoch 241 / 500 | iteration 15 / 30 | Total Loss: 5.419922828674316 | KNN Loss: 4.350186347961426 | BCE Loss: 1.0697364807128906\n",
      "Epoch 241 / 500 | iteration 20 / 30 | Total Loss: 5.389841079711914 | KNN Loss: 4.356602668762207 | BCE Loss: 1.0332385301589966\n",
      "Epoch 241 / 500 | iteration 25 / 30 | Total Loss: 5.397785186767578 | KNN Loss: 4.368635177612305 | BCE Loss: 1.0291497707366943\n",
      "Epoch 242 / 500 | iteration 0 / 30 | Total Loss: 5.340106964111328 | KNN Loss: 4.338521480560303 | BCE Loss: 1.0015854835510254\n",
      "Epoch 242 / 500 | iteration 5 / 30 | Total Loss: 5.3713836669921875 | KNN Loss: 4.338342189788818 | BCE Loss: 1.0330417156219482\n",
      "Epoch 242 / 500 | iteration 10 / 30 | Total Loss: 5.356698989868164 | KNN Loss: 4.348292827606201 | BCE Loss: 1.0084060430526733\n",
      "Epoch 242 / 500 | iteration 15 / 30 | Total Loss: 5.410086631774902 | KNN Loss: 4.39048957824707 | BCE Loss: 1.0195972919464111\n",
      "Epoch 242 / 500 | iteration 20 / 30 | Total Loss: 5.396335601806641 | KNN Loss: 4.357339859008789 | BCE Loss: 1.0389957427978516\n",
      "Epoch 242 / 500 | iteration 25 / 30 | Total Loss: 5.38658332824707 | KNN Loss: 4.353703498840332 | BCE Loss: 1.0328800678253174\n",
      "Epoch 243 / 500 | iteration 0 / 30 | Total Loss: 5.36002254486084 | KNN Loss: 4.322786331176758 | BCE Loss: 1.037236213684082\n",
      "Epoch 243 / 500 | iteration 5 / 30 | Total Loss: 5.344759941101074 | KNN Loss: 4.339365005493164 | BCE Loss: 1.0053949356079102\n",
      "Epoch 243 / 500 | iteration 10 / 30 | Total Loss: 5.449201583862305 | KNN Loss: 4.41096830368042 | BCE Loss: 1.0382330417633057\n",
      "Epoch 243 / 500 | iteration 15 / 30 | Total Loss: 5.393697738647461 | KNN Loss: 4.349961280822754 | BCE Loss: 1.0437363386154175\n",
      "Epoch 243 / 500 | iteration 20 / 30 | Total Loss: 5.405616283416748 | KNN Loss: 4.3702073097229 | BCE Loss: 1.0354089736938477\n",
      "Epoch 243 / 500 | iteration 25 / 30 | Total Loss: 5.379678726196289 | KNN Loss: 4.344942092895508 | BCE Loss: 1.0347363948822021\n",
      "Epoch 244 / 500 | iteration 0 / 30 | Total Loss: 5.341744422912598 | KNN Loss: 4.323398590087891 | BCE Loss: 1.018345594406128\n",
      "Epoch 244 / 500 | iteration 5 / 30 | Total Loss: 5.319939136505127 | KNN Loss: 4.319676876068115 | BCE Loss: 1.0002622604370117\n",
      "Epoch 244 / 500 | iteration 10 / 30 | Total Loss: 5.339756965637207 | KNN Loss: 4.323009490966797 | BCE Loss: 1.016747236251831\n",
      "Epoch 244 / 500 | iteration 15 / 30 | Total Loss: 5.425091743469238 | KNN Loss: 4.388641357421875 | BCE Loss: 1.0364502668380737\n",
      "Epoch 244 / 500 | iteration 20 / 30 | Total Loss: 5.336272239685059 | KNN Loss: 4.321079254150391 | BCE Loss: 1.015192985534668\n",
      "Epoch 244 / 500 | iteration 25 / 30 | Total Loss: 5.36021089553833 | KNN Loss: 4.330191612243652 | BCE Loss: 1.0300192832946777\n",
      "Epoch 245 / 500 | iteration 0 / 30 | Total Loss: 5.358736991882324 | KNN Loss: 4.346095085144043 | BCE Loss: 1.0126416683197021\n",
      "Epoch 245 / 500 | iteration 5 / 30 | Total Loss: 5.37823486328125 | KNN Loss: 4.372078895568848 | BCE Loss: 1.0061557292938232\n",
      "Epoch 245 / 500 | iteration 10 / 30 | Total Loss: 5.4330854415893555 | KNN Loss: 4.412444114685059 | BCE Loss: 1.0206413269042969\n",
      "Epoch 245 / 500 | iteration 15 / 30 | Total Loss: 5.318495750427246 | KNN Loss: 4.306462287902832 | BCE Loss: 1.0120337009429932\n",
      "Epoch 245 / 500 | iteration 20 / 30 | Total Loss: 5.370432376861572 | KNN Loss: 4.368140697479248 | BCE Loss: 1.0022916793823242\n",
      "Epoch 245 / 500 | iteration 25 / 30 | Total Loss: 5.370243072509766 | KNN Loss: 4.329987049102783 | BCE Loss: 1.0402557849884033\n",
      "Epoch 246 / 500 | iteration 0 / 30 | Total Loss: 5.37694787979126 | KNN Loss: 4.346829891204834 | BCE Loss: 1.0301179885864258\n",
      "Epoch 246 / 500 | iteration 5 / 30 | Total Loss: 5.393014430999756 | KNN Loss: 4.373769283294678 | BCE Loss: 1.0192450284957886\n",
      "Epoch 246 / 500 | iteration 10 / 30 | Total Loss: 5.3473663330078125 | KNN Loss: 4.332260608673096 | BCE Loss: 1.0151057243347168\n",
      "Epoch 246 / 500 | iteration 15 / 30 | Total Loss: 5.434907913208008 | KNN Loss: 4.399109363555908 | BCE Loss: 1.0357983112335205\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 246 / 500 | iteration 20 / 30 | Total Loss: 5.348838806152344 | KNN Loss: 4.314168930053711 | BCE Loss: 1.0346699953079224\n",
      "Epoch 246 / 500 | iteration 25 / 30 | Total Loss: 5.391701698303223 | KNN Loss: 4.327085494995117 | BCE Loss: 1.0646159648895264\n",
      "Epoch 247 / 500 | iteration 0 / 30 | Total Loss: 5.3645524978637695 | KNN Loss: 4.341398239135742 | BCE Loss: 1.0231542587280273\n",
      "Epoch 247 / 500 | iteration 5 / 30 | Total Loss: 5.436985015869141 | KNN Loss: 4.397541046142578 | BCE Loss: 1.0394442081451416\n",
      "Epoch 247 / 500 | iteration 10 / 30 | Total Loss: 5.332683086395264 | KNN Loss: 4.321549415588379 | BCE Loss: 1.0111335515975952\n",
      "Epoch 247 / 500 | iteration 15 / 30 | Total Loss: 5.445439338684082 | KNN Loss: 4.404435157775879 | BCE Loss: 1.0410044193267822\n",
      "Epoch 247 / 500 | iteration 20 / 30 | Total Loss: 5.358083724975586 | KNN Loss: 4.334383964538574 | BCE Loss: 1.0236995220184326\n",
      "Epoch 247 / 500 | iteration 25 / 30 | Total Loss: 5.383734226226807 | KNN Loss: 4.3540239334106445 | BCE Loss: 1.029710292816162\n",
      "Epoch 248 / 500 | iteration 0 / 30 | Total Loss: 5.4129180908203125 | KNN Loss: 4.35688591003418 | BCE Loss: 1.056032419204712\n",
      "Epoch 248 / 500 | iteration 5 / 30 | Total Loss: 5.401212692260742 | KNN Loss: 4.373865127563477 | BCE Loss: 1.0273476839065552\n",
      "Epoch 248 / 500 | iteration 10 / 30 | Total Loss: 5.3346099853515625 | KNN Loss: 4.316671371459961 | BCE Loss: 1.017938494682312\n",
      "Epoch 248 / 500 | iteration 15 / 30 | Total Loss: 5.355832099914551 | KNN Loss: 4.321404457092285 | BCE Loss: 1.0344276428222656\n",
      "Epoch 248 / 500 | iteration 20 / 30 | Total Loss: 5.397071361541748 | KNN Loss: 4.4039106369018555 | BCE Loss: 0.9931608438491821\n",
      "Epoch 248 / 500 | iteration 25 / 30 | Total Loss: 5.344017028808594 | KNN Loss: 4.345261096954346 | BCE Loss: 0.9987557530403137\n",
      "Epoch 249 / 500 | iteration 0 / 30 | Total Loss: 5.431062698364258 | KNN Loss: 4.385410785675049 | BCE Loss: 1.045652151107788\n",
      "Epoch 249 / 500 | iteration 5 / 30 | Total Loss: 5.372564315795898 | KNN Loss: 4.32256555557251 | BCE Loss: 1.0499987602233887\n",
      "Epoch 249 / 500 | iteration 10 / 30 | Total Loss: 5.373779296875 | KNN Loss: 4.328869342803955 | BCE Loss: 1.044910192489624\n",
      "Epoch 249 / 500 | iteration 15 / 30 | Total Loss: 5.394268989562988 | KNN Loss: 4.350919246673584 | BCE Loss: 1.0433496236801147\n",
      "Epoch 249 / 500 | iteration 20 / 30 | Total Loss: 5.405576229095459 | KNN Loss: 4.381038188934326 | BCE Loss: 1.0245380401611328\n",
      "Epoch 249 / 500 | iteration 25 / 30 | Total Loss: 5.400219917297363 | KNN Loss: 4.350420951843262 | BCE Loss: 1.0497987270355225\n",
      "Epoch 250 / 500 | iteration 0 / 30 | Total Loss: 5.329817771911621 | KNN Loss: 4.326299667358398 | BCE Loss: 1.0035181045532227\n",
      "Epoch 250 / 500 | iteration 5 / 30 | Total Loss: 5.3499555587768555 | KNN Loss: 4.335167407989502 | BCE Loss: 1.0147883892059326\n",
      "Epoch 250 / 500 | iteration 10 / 30 | Total Loss: 5.525519371032715 | KNN Loss: 4.4694390296936035 | BCE Loss: 1.0560801029205322\n",
      "Epoch 250 / 500 | iteration 15 / 30 | Total Loss: 5.401490688323975 | KNN Loss: 4.39174222946167 | BCE Loss: 1.0097484588623047\n",
      "Epoch 250 / 500 | iteration 20 / 30 | Total Loss: 5.3817033767700195 | KNN Loss: 4.3669562339782715 | BCE Loss: 1.0147472620010376\n",
      "Epoch 250 / 500 | iteration 25 / 30 | Total Loss: 5.387718200683594 | KNN Loss: 4.343921184539795 | BCE Loss: 1.0437970161437988\n",
      "Epoch 251 / 500 | iteration 0 / 30 | Total Loss: 5.372236728668213 | KNN Loss: 4.3502278327941895 | BCE Loss: 1.0220087766647339\n",
      "Epoch 251 / 500 | iteration 5 / 30 | Total Loss: 5.405706405639648 | KNN Loss: 4.379527568817139 | BCE Loss: 1.0261787176132202\n",
      "Epoch 251 / 500 | iteration 10 / 30 | Total Loss: 5.343149185180664 | KNN Loss: 4.326011657714844 | BCE Loss: 1.0171377658843994\n",
      "Epoch 251 / 500 | iteration 15 / 30 | Total Loss: 5.376028060913086 | KNN Loss: 4.338848114013672 | BCE Loss: 1.0371801853179932\n",
      "Epoch 251 / 500 | iteration 20 / 30 | Total Loss: 5.372046947479248 | KNN Loss: 4.344048500061035 | BCE Loss: 1.0279985666275024\n",
      "Epoch 251 / 500 | iteration 25 / 30 | Total Loss: 5.416476249694824 | KNN Loss: 4.369230270385742 | BCE Loss: 1.0472462177276611\n",
      "Epoch   252: reducing learning rate of group 0 to 9.8866e-05.\n",
      "Epoch 252 / 500 | iteration 0 / 30 | Total Loss: 5.39536190032959 | KNN Loss: 4.35759973526001 | BCE Loss: 1.0377624034881592\n",
      "Epoch 252 / 500 | iteration 5 / 30 | Total Loss: 5.328555107116699 | KNN Loss: 4.31738805770874 | BCE Loss: 1.0111668109893799\n",
      "Epoch 252 / 500 | iteration 10 / 30 | Total Loss: 5.382952690124512 | KNN Loss: 4.36378288269043 | BCE Loss: 1.019169807434082\n",
      "Epoch 252 / 500 | iteration 15 / 30 | Total Loss: 5.34990119934082 | KNN Loss: 4.330163955688477 | BCE Loss: 1.0197373628616333\n",
      "Epoch 252 / 500 | iteration 20 / 30 | Total Loss: 5.407501220703125 | KNN Loss: 4.374789714813232 | BCE Loss: 1.0327117443084717\n",
      "Epoch 252 / 500 | iteration 25 / 30 | Total Loss: 5.4160590171813965 | KNN Loss: 4.427504539489746 | BCE Loss: 0.9885546565055847\n",
      "Epoch 253 / 500 | iteration 0 / 30 | Total Loss: 5.368424892425537 | KNN Loss: 4.340677738189697 | BCE Loss: 1.0277470350265503\n",
      "Epoch 253 / 500 | iteration 5 / 30 | Total Loss: 5.35824728012085 | KNN Loss: 4.350358963012695 | BCE Loss: 1.0078883171081543\n",
      "Epoch 253 / 500 | iteration 10 / 30 | Total Loss: 5.399377822875977 | KNN Loss: 4.3486552238464355 | BCE Loss: 1.050722599029541\n",
      "Epoch 253 / 500 | iteration 15 / 30 | Total Loss: 5.367019176483154 | KNN Loss: 4.328603267669678 | BCE Loss: 1.0384159088134766\n",
      "Epoch 253 / 500 | iteration 20 / 30 | Total Loss: 5.372871398925781 | KNN Loss: 4.3619608879089355 | BCE Loss: 1.0109107494354248\n",
      "Epoch 253 / 500 | iteration 25 / 30 | Total Loss: 5.385616779327393 | KNN Loss: 4.356531143188477 | BCE Loss: 1.0290857553482056\n",
      "Epoch 254 / 500 | iteration 0 / 30 | Total Loss: 5.372236251831055 | KNN Loss: 4.354114532470703 | BCE Loss: 1.0181219577789307\n",
      "Epoch 254 / 500 | iteration 5 / 30 | Total Loss: 5.375894069671631 | KNN Loss: 4.362699031829834 | BCE Loss: 1.0131949186325073\n",
      "Epoch 254 / 500 | iteration 10 / 30 | Total Loss: 5.357436180114746 | KNN Loss: 4.327075481414795 | BCE Loss: 1.0303606986999512\n",
      "Epoch 254 / 500 | iteration 15 / 30 | Total Loss: 5.413959503173828 | KNN Loss: 4.390608787536621 | BCE Loss: 1.0233505964279175\n",
      "Epoch 254 / 500 | iteration 20 / 30 | Total Loss: 5.335216522216797 | KNN Loss: 4.311828136444092 | BCE Loss: 1.023388147354126\n",
      "Epoch 254 / 500 | iteration 25 / 30 | Total Loss: 5.416971683502197 | KNN Loss: 4.401974678039551 | BCE Loss: 1.0149970054626465\n",
      "Epoch 255 / 500 | iteration 0 / 30 | Total Loss: 5.446308135986328 | KNN Loss: 4.377485275268555 | BCE Loss: 1.0688230991363525\n",
      "Epoch 255 / 500 | iteration 5 / 30 | Total Loss: 5.370535850524902 | KNN Loss: 4.362135410308838 | BCE Loss: 1.0084004402160645\n",
      "Epoch 255 / 500 | iteration 10 / 30 | Total Loss: 5.397603988647461 | KNN Loss: 4.414339065551758 | BCE Loss: 0.9832649230957031\n",
      "Epoch 255 / 500 | iteration 15 / 30 | Total Loss: 5.431078910827637 | KNN Loss: 4.416942596435547 | BCE Loss: 1.0141360759735107\n",
      "Epoch 255 / 500 | iteration 20 / 30 | Total Loss: 5.405738830566406 | KNN Loss: 4.3825178146362305 | BCE Loss: 1.0232207775115967\n",
      "Epoch 255 / 500 | iteration 25 / 30 | Total Loss: 5.429957389831543 | KNN Loss: 4.377180576324463 | BCE Loss: 1.052776575088501\n",
      "Epoch 256 / 500 | iteration 0 / 30 | Total Loss: 5.39060640335083 | KNN Loss: 4.36078405380249 | BCE Loss: 1.0298224687576294\n",
      "Epoch 256 / 500 | iteration 5 / 30 | Total Loss: 5.403016090393066 | KNN Loss: 4.351647853851318 | BCE Loss: 1.0513684749603271\n",
      "Epoch 256 / 500 | iteration 10 / 30 | Total Loss: 5.365105628967285 | KNN Loss: 4.3153157234191895 | BCE Loss: 1.0497899055480957\n",
      "Epoch 256 / 500 | iteration 15 / 30 | Total Loss: 5.376396179199219 | KNN Loss: 4.351585388183594 | BCE Loss: 1.024810791015625\n",
      "Epoch 256 / 500 | iteration 20 / 30 | Total Loss: 5.386244297027588 | KNN Loss: 4.358972072601318 | BCE Loss: 1.02727210521698\n",
      "Epoch 256 / 500 | iteration 25 / 30 | Total Loss: 5.381655693054199 | KNN Loss: 4.366279125213623 | BCE Loss: 1.015376329421997\n",
      "Epoch 257 / 500 | iteration 0 / 30 | Total Loss: 5.398453712463379 | KNN Loss: 4.391093730926514 | BCE Loss: 1.0073597431182861\n",
      "Epoch 257 / 500 | iteration 5 / 30 | Total Loss: 5.435783386230469 | KNN Loss: 4.384026050567627 | BCE Loss: 1.051757574081421\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 257 / 500 | iteration 10 / 30 | Total Loss: 5.371140003204346 | KNN Loss: 4.33725118637085 | BCE Loss: 1.033888816833496\n",
      "Epoch 257 / 500 | iteration 15 / 30 | Total Loss: 5.382960319519043 | KNN Loss: 4.357584476470947 | BCE Loss: 1.0253756046295166\n",
      "Epoch 257 / 500 | iteration 20 / 30 | Total Loss: 5.3643364906311035 | KNN Loss: 4.336507797241211 | BCE Loss: 1.027828574180603\n",
      "Epoch 257 / 500 | iteration 25 / 30 | Total Loss: 5.354681968688965 | KNN Loss: 4.333842754364014 | BCE Loss: 1.020838975906372\n",
      "Epoch 258 / 500 | iteration 0 / 30 | Total Loss: 5.365898132324219 | KNN Loss: 4.344149112701416 | BCE Loss: 1.0217489004135132\n",
      "Epoch 258 / 500 | iteration 5 / 30 | Total Loss: 5.329577445983887 | KNN Loss: 4.335717678070068 | BCE Loss: 0.9938599467277527\n",
      "Epoch 258 / 500 | iteration 10 / 30 | Total Loss: 5.380795478820801 | KNN Loss: 4.344150543212891 | BCE Loss: 1.0366451740264893\n",
      "Epoch 258 / 500 | iteration 15 / 30 | Total Loss: 5.432803153991699 | KNN Loss: 4.409760475158691 | BCE Loss: 1.023042917251587\n",
      "Epoch 258 / 500 | iteration 20 / 30 | Total Loss: 5.408875465393066 | KNN Loss: 4.3752641677856445 | BCE Loss: 1.0336114168167114\n",
      "Epoch 258 / 500 | iteration 25 / 30 | Total Loss: 5.334139823913574 | KNN Loss: 4.333198070526123 | BCE Loss: 1.0009419918060303\n",
      "Epoch 259 / 500 | iteration 0 / 30 | Total Loss: 5.418558120727539 | KNN Loss: 4.3837151527404785 | BCE Loss: 1.0348429679870605\n",
      "Epoch 259 / 500 | iteration 5 / 30 | Total Loss: 5.378903388977051 | KNN Loss: 4.369379043579102 | BCE Loss: 1.0095244646072388\n",
      "Epoch 259 / 500 | iteration 10 / 30 | Total Loss: 5.40310001373291 | KNN Loss: 4.361389636993408 | BCE Loss: 1.041710615158081\n",
      "Epoch 259 / 500 | iteration 15 / 30 | Total Loss: 5.409331321716309 | KNN Loss: 4.346006870269775 | BCE Loss: 1.0633244514465332\n",
      "Epoch 259 / 500 | iteration 20 / 30 | Total Loss: 5.380610466003418 | KNN Loss: 4.34334135055542 | BCE Loss: 1.037268877029419\n",
      "Epoch 259 / 500 | iteration 25 / 30 | Total Loss: 5.469759464263916 | KNN Loss: 4.412318229675293 | BCE Loss: 1.0574411153793335\n",
      "Epoch 260 / 500 | iteration 0 / 30 | Total Loss: 5.389276027679443 | KNN Loss: 4.349599361419678 | BCE Loss: 1.039676547050476\n",
      "Epoch 260 / 500 | iteration 5 / 30 | Total Loss: 5.429305076599121 | KNN Loss: 4.353663921356201 | BCE Loss: 1.07564115524292\n",
      "Epoch 260 / 500 | iteration 10 / 30 | Total Loss: 5.390692234039307 | KNN Loss: 4.372193336486816 | BCE Loss: 1.0184987783432007\n",
      "Epoch 260 / 500 | iteration 15 / 30 | Total Loss: 5.345091819763184 | KNN Loss: 4.335539817810059 | BCE Loss: 1.0095518827438354\n",
      "Epoch 260 / 500 | iteration 20 / 30 | Total Loss: 5.34242582321167 | KNN Loss: 4.326849937438965 | BCE Loss: 1.015575885772705\n",
      "Epoch 260 / 500 | iteration 25 / 30 | Total Loss: 5.360266208648682 | KNN Loss: 4.322498798370361 | BCE Loss: 1.0377674102783203\n",
      "Epoch 261 / 500 | iteration 0 / 30 | Total Loss: 5.3964691162109375 | KNN Loss: 4.362897872924805 | BCE Loss: 1.0335712432861328\n",
      "Epoch 261 / 500 | iteration 5 / 30 | Total Loss: 5.358736991882324 | KNN Loss: 4.329724311828613 | BCE Loss: 1.029012680053711\n",
      "Epoch 261 / 500 | iteration 10 / 30 | Total Loss: 5.385497093200684 | KNN Loss: 4.350798606872559 | BCE Loss: 1.034698486328125\n",
      "Epoch 261 / 500 | iteration 15 / 30 | Total Loss: 5.36405611038208 | KNN Loss: 4.345864295959473 | BCE Loss: 1.0181918144226074\n",
      "Epoch 261 / 500 | iteration 20 / 30 | Total Loss: 5.353087425231934 | KNN Loss: 4.337106227874756 | BCE Loss: 1.0159814357757568\n",
      "Epoch 261 / 500 | iteration 25 / 30 | Total Loss: 5.4027838706970215 | KNN Loss: 4.412870407104492 | BCE Loss: 0.9899134039878845\n",
      "Epoch 262 / 500 | iteration 0 / 30 | Total Loss: 5.4011383056640625 | KNN Loss: 4.365204811096191 | BCE Loss: 1.0359337329864502\n",
      "Epoch 262 / 500 | iteration 5 / 30 | Total Loss: 5.382253646850586 | KNN Loss: 4.34438419342041 | BCE Loss: 1.0378694534301758\n",
      "Epoch 262 / 500 | iteration 10 / 30 | Total Loss: 5.37580680847168 | KNN Loss: 4.344588756561279 | BCE Loss: 1.0312182903289795\n",
      "Epoch 262 / 500 | iteration 15 / 30 | Total Loss: 5.401494026184082 | KNN Loss: 4.359210968017578 | BCE Loss: 1.042283058166504\n",
      "Epoch 262 / 500 | iteration 20 / 30 | Total Loss: 5.343713283538818 | KNN Loss: 4.3231096267700195 | BCE Loss: 1.0206036567687988\n",
      "Epoch 262 / 500 | iteration 25 / 30 | Total Loss: 5.339620590209961 | KNN Loss: 4.302301406860352 | BCE Loss: 1.0373189449310303\n",
      "Epoch   263: reducing learning rate of group 0 to 6.9206e-05.\n",
      "Epoch 263 / 500 | iteration 0 / 30 | Total Loss: 5.383194923400879 | KNN Loss: 4.351539134979248 | BCE Loss: 1.03165602684021\n",
      "Epoch 263 / 500 | iteration 5 / 30 | Total Loss: 5.37175178527832 | KNN Loss: 4.335965633392334 | BCE Loss: 1.0357861518859863\n",
      "Epoch 263 / 500 | iteration 10 / 30 | Total Loss: 5.386306285858154 | KNN Loss: 4.386646270751953 | BCE Loss: 0.9996601939201355\n",
      "Epoch 263 / 500 | iteration 15 / 30 | Total Loss: 5.3502960205078125 | KNN Loss: 4.337055683135986 | BCE Loss: 1.013240098953247\n",
      "Epoch 263 / 500 | iteration 20 / 30 | Total Loss: 5.441370010375977 | KNN Loss: 4.412577152252197 | BCE Loss: 1.0287929773330688\n",
      "Epoch 263 / 500 | iteration 25 / 30 | Total Loss: 5.351001739501953 | KNN Loss: 4.351945400238037 | BCE Loss: 0.999056339263916\n",
      "Epoch 264 / 500 | iteration 0 / 30 | Total Loss: 5.413836479187012 | KNN Loss: 4.367449760437012 | BCE Loss: 1.046386480331421\n",
      "Epoch 264 / 500 | iteration 5 / 30 | Total Loss: 5.350179672241211 | KNN Loss: 4.317423343658447 | BCE Loss: 1.0327560901641846\n",
      "Epoch 264 / 500 | iteration 10 / 30 | Total Loss: 5.402759552001953 | KNN Loss: 4.394619941711426 | BCE Loss: 1.0081393718719482\n",
      "Epoch 264 / 500 | iteration 15 / 30 | Total Loss: 5.311335563659668 | KNN Loss: 4.327240467071533 | BCE Loss: 0.9840949773788452\n",
      "Epoch 264 / 500 | iteration 20 / 30 | Total Loss: 5.394981861114502 | KNN Loss: 4.359941005706787 | BCE Loss: 1.0350409746170044\n",
      "Epoch 264 / 500 | iteration 25 / 30 | Total Loss: 5.379715919494629 | KNN Loss: 4.379636287689209 | BCE Loss: 1.00007963180542\n",
      "Epoch 265 / 500 | iteration 0 / 30 | Total Loss: 5.349179267883301 | KNN Loss: 4.340646266937256 | BCE Loss: 1.0085327625274658\n",
      "Epoch 265 / 500 | iteration 5 / 30 | Total Loss: 5.432689666748047 | KNN Loss: 4.364773750305176 | BCE Loss: 1.0679157972335815\n",
      "Epoch 265 / 500 | iteration 10 / 30 | Total Loss: 5.371120452880859 | KNN Loss: 4.351166248321533 | BCE Loss: 1.0199544429779053\n",
      "Epoch 265 / 500 | iteration 15 / 30 | Total Loss: 5.387048721313477 | KNN Loss: 4.348060607910156 | BCE Loss: 1.0389878749847412\n",
      "Epoch 265 / 500 | iteration 20 / 30 | Total Loss: 5.478479385375977 | KNN Loss: 4.4437384605407715 | BCE Loss: 1.0347411632537842\n",
      "Epoch 265 / 500 | iteration 25 / 30 | Total Loss: 5.359389781951904 | KNN Loss: 4.356747150421143 | BCE Loss: 1.0026427507400513\n",
      "Epoch 266 / 500 | iteration 0 / 30 | Total Loss: 5.4228105545043945 | KNN Loss: 4.392367362976074 | BCE Loss: 1.0304429531097412\n",
      "Epoch 266 / 500 | iteration 5 / 30 | Total Loss: 5.3596367835998535 | KNN Loss: 4.3315749168396 | BCE Loss: 1.028061866760254\n",
      "Epoch 266 / 500 | iteration 10 / 30 | Total Loss: 5.34506368637085 | KNN Loss: 4.330268859863281 | BCE Loss: 1.0147947072982788\n",
      "Epoch 266 / 500 | iteration 15 / 30 | Total Loss: 5.397819995880127 | KNN Loss: 4.369391441345215 | BCE Loss: 1.028428554534912\n",
      "Epoch 266 / 500 | iteration 20 / 30 | Total Loss: 5.375787734985352 | KNN Loss: 4.35530424118042 | BCE Loss: 1.0204836130142212\n",
      "Epoch 266 / 500 | iteration 25 / 30 | Total Loss: 5.386147499084473 | KNN Loss: 4.365963459014893 | BCE Loss: 1.0201841592788696\n",
      "Epoch 267 / 500 | iteration 0 / 30 | Total Loss: 5.358948707580566 | KNN Loss: 4.333973407745361 | BCE Loss: 1.0249755382537842\n",
      "Epoch 267 / 500 | iteration 5 / 30 | Total Loss: 5.400866508483887 | KNN Loss: 4.355368614196777 | BCE Loss: 1.0454978942871094\n",
      "Epoch 267 / 500 | iteration 10 / 30 | Total Loss: 5.369668483734131 | KNN Loss: 4.339475631713867 | BCE Loss: 1.0301928520202637\n",
      "Epoch 267 / 500 | iteration 15 / 30 | Total Loss: 5.3943634033203125 | KNN Loss: 4.388021945953369 | BCE Loss: 1.0063414573669434\n",
      "Epoch 267 / 500 | iteration 20 / 30 | Total Loss: 5.377253532409668 | KNN Loss: 4.379228591918945 | BCE Loss: 0.9980250000953674\n",
      "Epoch 267 / 500 | iteration 25 / 30 | Total Loss: 5.399313449859619 | KNN Loss: 4.3388495445251465 | BCE Loss: 1.0604640245437622\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 268 / 500 | iteration 0 / 30 | Total Loss: 5.369577407836914 | KNN Loss: 4.339098930358887 | BCE Loss: 1.0304783582687378\n",
      "Epoch 268 / 500 | iteration 5 / 30 | Total Loss: 5.427004337310791 | KNN Loss: 4.39735746383667 | BCE Loss: 1.0296469926834106\n",
      "Epoch 268 / 500 | iteration 10 / 30 | Total Loss: 5.390960693359375 | KNN Loss: 4.351238250732422 | BCE Loss: 1.039722204208374\n",
      "Epoch 268 / 500 | iteration 15 / 30 | Total Loss: 5.336404800415039 | KNN Loss: 4.338916301727295 | BCE Loss: 0.9974887371063232\n",
      "Epoch 268 / 500 | iteration 20 / 30 | Total Loss: 5.385190486907959 | KNN Loss: 4.336864471435547 | BCE Loss: 1.0483261346817017\n",
      "Epoch 268 / 500 | iteration 25 / 30 | Total Loss: 5.360833168029785 | KNN Loss: 4.339285850524902 | BCE Loss: 1.0215471982955933\n",
      "Epoch 269 / 500 | iteration 0 / 30 | Total Loss: 5.384693622589111 | KNN Loss: 4.354675769805908 | BCE Loss: 1.0300178527832031\n",
      "Epoch 269 / 500 | iteration 5 / 30 | Total Loss: 5.376913547515869 | KNN Loss: 4.348777770996094 | BCE Loss: 1.028135895729065\n",
      "Epoch 269 / 500 | iteration 10 / 30 | Total Loss: 5.319830894470215 | KNN Loss: 4.309284210205078 | BCE Loss: 1.0105469226837158\n",
      "Epoch 269 / 500 | iteration 15 / 30 | Total Loss: 5.395606994628906 | KNN Loss: 4.397167205810547 | BCE Loss: 0.9984398484230042\n",
      "Epoch 269 / 500 | iteration 20 / 30 | Total Loss: 5.337450981140137 | KNN Loss: 4.331340789794922 | BCE Loss: 1.0061103105545044\n",
      "Epoch 269 / 500 | iteration 25 / 30 | Total Loss: 5.434322357177734 | KNN Loss: 4.378194332122803 | BCE Loss: 1.0561280250549316\n",
      "Epoch 270 / 500 | iteration 0 / 30 | Total Loss: 5.3433518409729 | KNN Loss: 4.322969913482666 | BCE Loss: 1.0203819274902344\n",
      "Epoch 270 / 500 | iteration 5 / 30 | Total Loss: 5.346827983856201 | KNN Loss: 4.31233024597168 | BCE Loss: 1.034497857093811\n",
      "Epoch 270 / 500 | iteration 10 / 30 | Total Loss: 5.369476318359375 | KNN Loss: 4.373021125793457 | BCE Loss: 0.9964554309844971\n",
      "Epoch 270 / 500 | iteration 15 / 30 | Total Loss: 5.439110279083252 | KNN Loss: 4.418522834777832 | BCE Loss: 1.02058744430542\n",
      "Epoch 270 / 500 | iteration 20 / 30 | Total Loss: 5.366366863250732 | KNN Loss: 4.341245174407959 | BCE Loss: 1.0251216888427734\n",
      "Epoch 270 / 500 | iteration 25 / 30 | Total Loss: 5.379570960998535 | KNN Loss: 4.358595848083496 | BCE Loss: 1.0209753513336182\n",
      "Epoch 271 / 500 | iteration 0 / 30 | Total Loss: 5.397005081176758 | KNN Loss: 4.357558250427246 | BCE Loss: 1.0394470691680908\n",
      "Epoch 271 / 500 | iteration 5 / 30 | Total Loss: 5.365067958831787 | KNN Loss: 4.3057708740234375 | BCE Loss: 1.0592972040176392\n",
      "Epoch 271 / 500 | iteration 10 / 30 | Total Loss: 5.352940082550049 | KNN Loss: 4.34122371673584 | BCE Loss: 1.011716365814209\n",
      "Epoch 271 / 500 | iteration 15 / 30 | Total Loss: 5.4330949783325195 | KNN Loss: 4.3901896476745605 | BCE Loss: 1.0429050922393799\n",
      "Epoch 271 / 500 | iteration 20 / 30 | Total Loss: 5.429042816162109 | KNN Loss: 4.387202739715576 | BCE Loss: 1.041839838027954\n",
      "Epoch 271 / 500 | iteration 25 / 30 | Total Loss: 5.375354766845703 | KNN Loss: 4.343583106994629 | BCE Loss: 1.0317714214324951\n",
      "Epoch 272 / 500 | iteration 0 / 30 | Total Loss: 5.307949066162109 | KNN Loss: 4.335008144378662 | BCE Loss: 0.9729408025741577\n",
      "Epoch 272 / 500 | iteration 5 / 30 | Total Loss: 5.368858337402344 | KNN Loss: 4.36372184753418 | BCE Loss: 1.0051366090774536\n",
      "Epoch 272 / 500 | iteration 10 / 30 | Total Loss: 5.41625452041626 | KNN Loss: 4.368930816650391 | BCE Loss: 1.0473238229751587\n",
      "Epoch 272 / 500 | iteration 15 / 30 | Total Loss: 5.40034294128418 | KNN Loss: 4.368076324462891 | BCE Loss: 1.0322667360305786\n",
      "Epoch 272 / 500 | iteration 20 / 30 | Total Loss: 5.36434268951416 | KNN Loss: 4.365207195281982 | BCE Loss: 0.9991357326507568\n",
      "Epoch 272 / 500 | iteration 25 / 30 | Total Loss: 5.366000175476074 | KNN Loss: 4.346036434173584 | BCE Loss: 1.0199636220932007\n",
      "Epoch 273 / 500 | iteration 0 / 30 | Total Loss: 5.327636241912842 | KNN Loss: 4.322958946228027 | BCE Loss: 1.0046772956848145\n",
      "Epoch 273 / 500 | iteration 5 / 30 | Total Loss: 5.3736419677734375 | KNN Loss: 4.3386549949646 | BCE Loss: 1.034987211227417\n",
      "Epoch 273 / 500 | iteration 10 / 30 | Total Loss: 5.4013776779174805 | KNN Loss: 4.358573913574219 | BCE Loss: 1.0428040027618408\n",
      "Epoch 273 / 500 | iteration 15 / 30 | Total Loss: 5.347785949707031 | KNN Loss: 4.334623336791992 | BCE Loss: 1.01316237449646\n",
      "Epoch 273 / 500 | iteration 20 / 30 | Total Loss: 5.376729488372803 | KNN Loss: 4.332446098327637 | BCE Loss: 1.044283390045166\n",
      "Epoch 273 / 500 | iteration 25 / 30 | Total Loss: 5.379756927490234 | KNN Loss: 4.348246097564697 | BCE Loss: 1.031510829925537\n",
      "Epoch   274: reducing learning rate of group 0 to 4.8445e-05.\n",
      "Epoch 274 / 500 | iteration 0 / 30 | Total Loss: 5.372109413146973 | KNN Loss: 4.343695640563965 | BCE Loss: 1.028414011001587\n",
      "Epoch 274 / 500 | iteration 5 / 30 | Total Loss: 5.357323169708252 | KNN Loss: 4.342400550842285 | BCE Loss: 1.0149226188659668\n",
      "Epoch 274 / 500 | iteration 10 / 30 | Total Loss: 5.362056255340576 | KNN Loss: 4.375308036804199 | BCE Loss: 0.9867483377456665\n",
      "Epoch 274 / 500 | iteration 15 / 30 | Total Loss: 5.406656265258789 | KNN Loss: 4.379331588745117 | BCE Loss: 1.0273246765136719\n",
      "Epoch 274 / 500 | iteration 20 / 30 | Total Loss: 5.408398628234863 | KNN Loss: 4.351479530334473 | BCE Loss: 1.056918978691101\n",
      "Epoch 274 / 500 | iteration 25 / 30 | Total Loss: 5.431358337402344 | KNN Loss: 4.408640384674072 | BCE Loss: 1.0227179527282715\n",
      "Epoch 275 / 500 | iteration 0 / 30 | Total Loss: 5.387107849121094 | KNN Loss: 4.3731689453125 | BCE Loss: 1.0139390230178833\n",
      "Epoch 275 / 500 | iteration 5 / 30 | Total Loss: 5.440457344055176 | KNN Loss: 4.3947978019714355 | BCE Loss: 1.0456593036651611\n",
      "Epoch 275 / 500 | iteration 10 / 30 | Total Loss: 5.343016624450684 | KNN Loss: 4.350961208343506 | BCE Loss: 0.9920556545257568\n",
      "Epoch 275 / 500 | iteration 15 / 30 | Total Loss: 5.468624591827393 | KNN Loss: 4.423165798187256 | BCE Loss: 1.0454586744308472\n",
      "Epoch 275 / 500 | iteration 20 / 30 | Total Loss: 5.393628120422363 | KNN Loss: 4.337220668792725 | BCE Loss: 1.0564074516296387\n",
      "Epoch 275 / 500 | iteration 25 / 30 | Total Loss: 5.390830039978027 | KNN Loss: 4.359880447387695 | BCE Loss: 1.030949354171753\n",
      "Epoch 276 / 500 | iteration 0 / 30 | Total Loss: 5.341030597686768 | KNN Loss: 4.329300880432129 | BCE Loss: 1.0117298364639282\n",
      "Epoch 276 / 500 | iteration 5 / 30 | Total Loss: 5.396772384643555 | KNN Loss: 4.368219375610352 | BCE Loss: 1.0285531282424927\n",
      "Epoch 276 / 500 | iteration 10 / 30 | Total Loss: 5.387625694274902 | KNN Loss: 4.369778633117676 | BCE Loss: 1.0178472995758057\n",
      "Epoch 276 / 500 | iteration 15 / 30 | Total Loss: 5.3706488609313965 | KNN Loss: 4.371273994445801 | BCE Loss: 0.9993746876716614\n",
      "Epoch 276 / 500 | iteration 20 / 30 | Total Loss: 5.324178695678711 | KNN Loss: 4.311038017272949 | BCE Loss: 1.0131407976150513\n",
      "Epoch 276 / 500 | iteration 25 / 30 | Total Loss: 5.383278846740723 | KNN Loss: 4.365338325500488 | BCE Loss: 1.0179405212402344\n",
      "Epoch 277 / 500 | iteration 0 / 30 | Total Loss: 5.375128746032715 | KNN Loss: 4.337284564971924 | BCE Loss: 1.0378444194793701\n",
      "Epoch 277 / 500 | iteration 5 / 30 | Total Loss: 5.356450080871582 | KNN Loss: 4.319373607635498 | BCE Loss: 1.037076473236084\n",
      "Epoch 277 / 500 | iteration 10 / 30 | Total Loss: 5.325735092163086 | KNN Loss: 4.318129539489746 | BCE Loss: 1.0076053142547607\n",
      "Epoch 277 / 500 | iteration 15 / 30 | Total Loss: 5.422806739807129 | KNN Loss: 4.361461639404297 | BCE Loss: 1.061344861984253\n",
      "Epoch 277 / 500 | iteration 20 / 30 | Total Loss: 5.364951133728027 | KNN Loss: 4.346883773803711 | BCE Loss: 1.0180675983428955\n",
      "Epoch 277 / 500 | iteration 25 / 30 | Total Loss: 5.3251237869262695 | KNN Loss: 4.320603847503662 | BCE Loss: 1.0045197010040283\n",
      "Epoch 278 / 500 | iteration 0 / 30 | Total Loss: 5.366398811340332 | KNN Loss: 4.348307132720947 | BCE Loss: 1.0180916786193848\n",
      "Epoch 278 / 500 | iteration 5 / 30 | Total Loss: 5.3765482902526855 | KNN Loss: 4.362481117248535 | BCE Loss: 1.0140671730041504\n",
      "Epoch 278 / 500 | iteration 10 / 30 | Total Loss: 5.357319355010986 | KNN Loss: 4.356561183929443 | BCE Loss: 1.0007580518722534\n",
      "Epoch 278 / 500 | iteration 15 / 30 | Total Loss: 5.365650177001953 | KNN Loss: 4.329502582550049 | BCE Loss: 1.0361475944519043\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 278 / 500 | iteration 20 / 30 | Total Loss: 5.315619468688965 | KNN Loss: 4.301762580871582 | BCE Loss: 1.0138567686080933\n",
      "Epoch 278 / 500 | iteration 25 / 30 | Total Loss: 5.373989582061768 | KNN Loss: 4.359185218811035 | BCE Loss: 1.0148043632507324\n",
      "Epoch 279 / 500 | iteration 0 / 30 | Total Loss: 5.34412956237793 | KNN Loss: 4.317481994628906 | BCE Loss: 1.0266475677490234\n",
      "Epoch 279 / 500 | iteration 5 / 30 | Total Loss: 5.348244667053223 | KNN Loss: 4.3188042640686035 | BCE Loss: 1.02944016456604\n",
      "Epoch 279 / 500 | iteration 10 / 30 | Total Loss: 5.349989891052246 | KNN Loss: 4.342126846313477 | BCE Loss: 1.007863163948059\n",
      "Epoch 279 / 500 | iteration 15 / 30 | Total Loss: 5.356930255889893 | KNN Loss: 4.337635040283203 | BCE Loss: 1.0192952156066895\n",
      "Epoch 279 / 500 | iteration 20 / 30 | Total Loss: 5.4054460525512695 | KNN Loss: 4.350876331329346 | BCE Loss: 1.0545694828033447\n",
      "Epoch 279 / 500 | iteration 25 / 30 | Total Loss: 5.433729648590088 | KNN Loss: 4.394647121429443 | BCE Loss: 1.0390825271606445\n",
      "Epoch 280 / 500 | iteration 0 / 30 | Total Loss: 5.357064247131348 | KNN Loss: 4.368927955627441 | BCE Loss: 0.9881362915039062\n",
      "Epoch 280 / 500 | iteration 5 / 30 | Total Loss: 5.418707847595215 | KNN Loss: 4.364584445953369 | BCE Loss: 1.0541234016418457\n",
      "Epoch 280 / 500 | iteration 10 / 30 | Total Loss: 5.371096611022949 | KNN Loss: 4.364856719970703 | BCE Loss: 1.006239652633667\n",
      "Epoch 280 / 500 | iteration 15 / 30 | Total Loss: 5.405840873718262 | KNN Loss: 4.395834922790527 | BCE Loss: 1.0100061893463135\n",
      "Epoch 280 / 500 | iteration 20 / 30 | Total Loss: 5.406357765197754 | KNN Loss: 4.358739852905273 | BCE Loss: 1.04761803150177\n",
      "Epoch 280 / 500 | iteration 25 / 30 | Total Loss: 5.36735200881958 | KNN Loss: 4.346449375152588 | BCE Loss: 1.0209027528762817\n",
      "Epoch 281 / 500 | iteration 0 / 30 | Total Loss: 5.388442516326904 | KNN Loss: 4.357827663421631 | BCE Loss: 1.0306147336959839\n",
      "Epoch 281 / 500 | iteration 5 / 30 | Total Loss: 5.340592861175537 | KNN Loss: 4.314165115356445 | BCE Loss: 1.0264276266098022\n",
      "Epoch 281 / 500 | iteration 10 / 30 | Total Loss: 5.380418300628662 | KNN Loss: 4.341699123382568 | BCE Loss: 1.0387191772460938\n",
      "Epoch 281 / 500 | iteration 15 / 30 | Total Loss: 5.33421516418457 | KNN Loss: 4.319015026092529 | BCE Loss: 1.015199899673462\n",
      "Epoch 281 / 500 | iteration 20 / 30 | Total Loss: 5.393978118896484 | KNN Loss: 4.367407321929932 | BCE Loss: 1.0265706777572632\n",
      "Epoch 281 / 500 | iteration 25 / 30 | Total Loss: 5.377583026885986 | KNN Loss: 4.36821174621582 | BCE Loss: 1.009371280670166\n",
      "Epoch 282 / 500 | iteration 0 / 30 | Total Loss: 5.374310493469238 | KNN Loss: 4.336991310119629 | BCE Loss: 1.0373194217681885\n",
      "Epoch 282 / 500 | iteration 5 / 30 | Total Loss: 5.382563591003418 | KNN Loss: 4.379257678985596 | BCE Loss: 1.0033056735992432\n",
      "Epoch 282 / 500 | iteration 10 / 30 | Total Loss: 5.395611763000488 | KNN Loss: 4.374505996704102 | BCE Loss: 1.0211058855056763\n",
      "Epoch 282 / 500 | iteration 15 / 30 | Total Loss: 5.458992004394531 | KNN Loss: 4.422386646270752 | BCE Loss: 1.0366053581237793\n",
      "Epoch 282 / 500 | iteration 20 / 30 | Total Loss: 5.429262161254883 | KNN Loss: 4.3886542320251465 | BCE Loss: 1.0406076908111572\n",
      "Epoch 282 / 500 | iteration 25 / 30 | Total Loss: 5.432231426239014 | KNN Loss: 4.409108638763428 | BCE Loss: 1.023122787475586\n",
      "Epoch 283 / 500 | iteration 0 / 30 | Total Loss: 5.380248546600342 | KNN Loss: 4.321364879608154 | BCE Loss: 1.058883547782898\n",
      "Epoch 283 / 500 | iteration 5 / 30 | Total Loss: 5.391325950622559 | KNN Loss: 4.376241207122803 | BCE Loss: 1.015084981918335\n",
      "Epoch 283 / 500 | iteration 10 / 30 | Total Loss: 5.339947700500488 | KNN Loss: 4.3430962562561035 | BCE Loss: 0.9968514442443848\n",
      "Epoch 283 / 500 | iteration 15 / 30 | Total Loss: 5.375581741333008 | KNN Loss: 4.351843357086182 | BCE Loss: 1.0237386226654053\n",
      "Epoch 283 / 500 | iteration 20 / 30 | Total Loss: 5.352872848510742 | KNN Loss: 4.326308727264404 | BCE Loss: 1.026564359664917\n",
      "Epoch 283 / 500 | iteration 25 / 30 | Total Loss: 5.35770320892334 | KNN Loss: 4.3371429443359375 | BCE Loss: 1.0205605030059814\n",
      "Epoch 284 / 500 | iteration 0 / 30 | Total Loss: 5.340527534484863 | KNN Loss: 4.319990634918213 | BCE Loss: 1.0205371379852295\n",
      "Epoch 284 / 500 | iteration 5 / 30 | Total Loss: 5.397032737731934 | KNN Loss: 4.357791423797607 | BCE Loss: 1.0392411947250366\n",
      "Epoch 284 / 500 | iteration 10 / 30 | Total Loss: 5.394946098327637 | KNN Loss: 4.346207141876221 | BCE Loss: 1.048738718032837\n",
      "Epoch 284 / 500 | iteration 15 / 30 | Total Loss: 5.394382476806641 | KNN Loss: 4.409587860107422 | BCE Loss: 0.9847946166992188\n",
      "Epoch 284 / 500 | iteration 20 / 30 | Total Loss: 5.390321731567383 | KNN Loss: 4.370201110839844 | BCE Loss: 1.02012038230896\n",
      "Epoch 284 / 500 | iteration 25 / 30 | Total Loss: 5.426961421966553 | KNN Loss: 4.388041019439697 | BCE Loss: 1.038920521736145\n",
      "Epoch   285: reducing learning rate of group 0 to 3.3911e-05.\n",
      "Epoch 285 / 500 | iteration 0 / 30 | Total Loss: 5.418985366821289 | KNN Loss: 4.387305736541748 | BCE Loss: 1.031679391860962\n",
      "Epoch 285 / 500 | iteration 5 / 30 | Total Loss: 5.440883159637451 | KNN Loss: 4.369892120361328 | BCE Loss: 1.070991039276123\n",
      "Epoch 285 / 500 | iteration 10 / 30 | Total Loss: 5.426277160644531 | KNN Loss: 4.38508415222168 | BCE Loss: 1.0411927700042725\n",
      "Epoch 285 / 500 | iteration 15 / 30 | Total Loss: 5.429312705993652 | KNN Loss: 4.382133483886719 | BCE Loss: 1.0471792221069336\n",
      "Epoch 285 / 500 | iteration 20 / 30 | Total Loss: 5.365185260772705 | KNN Loss: 4.3607096672058105 | BCE Loss: 1.0044755935668945\n",
      "Epoch 285 / 500 | iteration 25 / 30 | Total Loss: 5.335063934326172 | KNN Loss: 4.344515323638916 | BCE Loss: 0.990548849105835\n",
      "Epoch 286 / 500 | iteration 0 / 30 | Total Loss: 5.333430290222168 | KNN Loss: 4.324605941772461 | BCE Loss: 1.0088245868682861\n",
      "Epoch 286 / 500 | iteration 5 / 30 | Total Loss: 5.351674556732178 | KNN Loss: 4.315623760223389 | BCE Loss: 1.0360509157180786\n",
      "Epoch 286 / 500 | iteration 10 / 30 | Total Loss: 5.34611177444458 | KNN Loss: 4.3462700843811035 | BCE Loss: 0.9998416900634766\n",
      "Epoch 286 / 500 | iteration 15 / 30 | Total Loss: 5.317887306213379 | KNN Loss: 4.3238911628723145 | BCE Loss: 0.9939960241317749\n",
      "Epoch 286 / 500 | iteration 20 / 30 | Total Loss: 5.402101516723633 | KNN Loss: 4.3616862297058105 | BCE Loss: 1.0404152870178223\n",
      "Epoch 286 / 500 | iteration 25 / 30 | Total Loss: 5.35996675491333 | KNN Loss: 4.345555305480957 | BCE Loss: 1.014411449432373\n",
      "Epoch 287 / 500 | iteration 0 / 30 | Total Loss: 5.396105766296387 | KNN Loss: 4.33979606628418 | BCE Loss: 1.0563098192214966\n",
      "Epoch 287 / 500 | iteration 5 / 30 | Total Loss: 5.404357433319092 | KNN Loss: 4.374150276184082 | BCE Loss: 1.0302070379257202\n",
      "Epoch 287 / 500 | iteration 10 / 30 | Total Loss: 5.383766174316406 | KNN Loss: 4.354248046875 | BCE Loss: 1.0295178890228271\n",
      "Epoch 287 / 500 | iteration 15 / 30 | Total Loss: 5.403857707977295 | KNN Loss: 4.363150596618652 | BCE Loss: 1.040706992149353\n",
      "Epoch 287 / 500 | iteration 20 / 30 | Total Loss: 5.435606479644775 | KNN Loss: 4.402082920074463 | BCE Loss: 1.0335235595703125\n",
      "Epoch 287 / 500 | iteration 25 / 30 | Total Loss: 5.352512836456299 | KNN Loss: 4.315431594848633 | BCE Loss: 1.037081241607666\n",
      "Epoch 288 / 500 | iteration 0 / 30 | Total Loss: 5.393671035766602 | KNN Loss: 4.36899471282959 | BCE Loss: 1.0246765613555908\n",
      "Epoch 288 / 500 | iteration 5 / 30 | Total Loss: 5.396382808685303 | KNN Loss: 4.341876983642578 | BCE Loss: 1.0545058250427246\n",
      "Epoch 288 / 500 | iteration 10 / 30 | Total Loss: 5.399827003479004 | KNN Loss: 4.360076904296875 | BCE Loss: 1.039750337600708\n",
      "Epoch 288 / 500 | iteration 15 / 30 | Total Loss: 5.3891072273254395 | KNN Loss: 4.365504264831543 | BCE Loss: 1.0236029624938965\n",
      "Epoch 288 / 500 | iteration 20 / 30 | Total Loss: 5.4393630027771 | KNN Loss: 4.393525123596191 | BCE Loss: 1.0458378791809082\n",
      "Epoch 288 / 500 | iteration 25 / 30 | Total Loss: 5.398687839508057 | KNN Loss: 4.346203804016113 | BCE Loss: 1.0524840354919434\n",
      "Epoch 289 / 500 | iteration 0 / 30 | Total Loss: 5.387788772583008 | KNN Loss: 4.3557233810424805 | BCE Loss: 1.0320651531219482\n",
      "Epoch 289 / 500 | iteration 5 / 30 | Total Loss: 5.366044521331787 | KNN Loss: 4.326933860778809 | BCE Loss: 1.039110541343689\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 289 / 500 | iteration 10 / 30 | Total Loss: 5.368314266204834 | KNN Loss: 4.356307029724121 | BCE Loss: 1.012007236480713\n",
      "Epoch 289 / 500 | iteration 15 / 30 | Total Loss: 5.421264171600342 | KNN Loss: 4.370146751403809 | BCE Loss: 1.0511174201965332\n",
      "Epoch 289 / 500 | iteration 20 / 30 | Total Loss: 5.426236152648926 | KNN Loss: 4.3680925369262695 | BCE Loss: 1.0581433773040771\n",
      "Epoch 289 / 500 | iteration 25 / 30 | Total Loss: 5.383167266845703 | KNN Loss: 4.391207695007324 | BCE Loss: 0.9919596314430237\n",
      "Epoch 290 / 500 | iteration 0 / 30 | Total Loss: 5.386585235595703 | KNN Loss: 4.347933292388916 | BCE Loss: 1.0386518239974976\n",
      "Epoch 290 / 500 | iteration 5 / 30 | Total Loss: 5.3985466957092285 | KNN Loss: 4.370584487915039 | BCE Loss: 1.0279620885849\n",
      "Epoch 290 / 500 | iteration 10 / 30 | Total Loss: 5.38459587097168 | KNN Loss: 4.3313727378845215 | BCE Loss: 1.053222894668579\n",
      "Epoch 290 / 500 | iteration 15 / 30 | Total Loss: 5.326060771942139 | KNN Loss: 4.335299968719482 | BCE Loss: 0.9907607436180115\n",
      "Epoch 290 / 500 | iteration 20 / 30 | Total Loss: 5.440710067749023 | KNN Loss: 4.377078056335449 | BCE Loss: 1.0636320114135742\n",
      "Epoch 290 / 500 | iteration 25 / 30 | Total Loss: 5.372512340545654 | KNN Loss: 4.3479743003845215 | BCE Loss: 1.0245381593704224\n",
      "Epoch 291 / 500 | iteration 0 / 30 | Total Loss: 5.3821892738342285 | KNN Loss: 4.332695484161377 | BCE Loss: 1.0494937896728516\n",
      "Epoch 291 / 500 | iteration 5 / 30 | Total Loss: 5.452530860900879 | KNN Loss: 4.414853572845459 | BCE Loss: 1.0376771688461304\n",
      "Epoch 291 / 500 | iteration 10 / 30 | Total Loss: 5.318528652191162 | KNN Loss: 4.333459854125977 | BCE Loss: 0.9850689172744751\n",
      "Epoch 291 / 500 | iteration 15 / 30 | Total Loss: 5.353181838989258 | KNN Loss: 4.330733776092529 | BCE Loss: 1.0224483013153076\n",
      "Epoch 291 / 500 | iteration 20 / 30 | Total Loss: 5.379367828369141 | KNN Loss: 4.364333629608154 | BCE Loss: 1.0150341987609863\n",
      "Epoch 291 / 500 | iteration 25 / 30 | Total Loss: 5.386698246002197 | KNN Loss: 4.3467116355896 | BCE Loss: 1.0399866104125977\n",
      "Epoch 292 / 500 | iteration 0 / 30 | Total Loss: 5.424603462219238 | KNN Loss: 4.387368679046631 | BCE Loss: 1.0372345447540283\n",
      "Epoch 292 / 500 | iteration 5 / 30 | Total Loss: 5.406594276428223 | KNN Loss: 4.365459442138672 | BCE Loss: 1.0411349534988403\n",
      "Epoch 292 / 500 | iteration 10 / 30 | Total Loss: 5.383878707885742 | KNN Loss: 4.34499979019165 | BCE Loss: 1.0388790369033813\n",
      "Epoch 292 / 500 | iteration 15 / 30 | Total Loss: 5.443778991699219 | KNN Loss: 4.397372245788574 | BCE Loss: 1.0464067459106445\n",
      "Epoch 292 / 500 | iteration 20 / 30 | Total Loss: 5.369321823120117 | KNN Loss: 4.367528438568115 | BCE Loss: 1.0017932653427124\n",
      "Epoch 292 / 500 | iteration 25 / 30 | Total Loss: 5.352908134460449 | KNN Loss: 4.333408355712891 | BCE Loss: 1.0194997787475586\n",
      "Epoch 293 / 500 | iteration 0 / 30 | Total Loss: 5.392214298248291 | KNN Loss: 4.372835636138916 | BCE Loss: 1.019378662109375\n",
      "Epoch 293 / 500 | iteration 5 / 30 | Total Loss: 5.347585678100586 | KNN Loss: 4.342061519622803 | BCE Loss: 1.0055243968963623\n",
      "Epoch 293 / 500 | iteration 10 / 30 | Total Loss: 5.369989395141602 | KNN Loss: 4.324840545654297 | BCE Loss: 1.0451488494873047\n",
      "Epoch 293 / 500 | iteration 15 / 30 | Total Loss: 5.35890007019043 | KNN Loss: 4.334503650665283 | BCE Loss: 1.0243964195251465\n",
      "Epoch 293 / 500 | iteration 20 / 30 | Total Loss: 5.4076924324035645 | KNN Loss: 4.364969253540039 | BCE Loss: 1.0427230596542358\n",
      "Epoch 293 / 500 | iteration 25 / 30 | Total Loss: 5.37920618057251 | KNN Loss: 4.352723121643066 | BCE Loss: 1.026483178138733\n",
      "Epoch 294 / 500 | iteration 0 / 30 | Total Loss: 5.413871765136719 | KNN Loss: 4.374917030334473 | BCE Loss: 1.038954734802246\n",
      "Epoch 294 / 500 | iteration 5 / 30 | Total Loss: 5.323535919189453 | KNN Loss: 4.31749963760376 | BCE Loss: 1.006036400794983\n",
      "Epoch 294 / 500 | iteration 10 / 30 | Total Loss: 5.399114608764648 | KNN Loss: 4.386233806610107 | BCE Loss: 1.0128810405731201\n",
      "Epoch 294 / 500 | iteration 15 / 30 | Total Loss: 5.325048923492432 | KNN Loss: 4.304198265075684 | BCE Loss: 1.0208507776260376\n",
      "Epoch 294 / 500 | iteration 20 / 30 | Total Loss: 5.4025397300720215 | KNN Loss: 4.3740081787109375 | BCE Loss: 1.0285316705703735\n",
      "Epoch 294 / 500 | iteration 25 / 30 | Total Loss: 5.35465145111084 | KNN Loss: 4.317202568054199 | BCE Loss: 1.0374488830566406\n",
      "Epoch 295 / 500 | iteration 0 / 30 | Total Loss: 5.3590312004089355 | KNN Loss: 4.3454766273498535 | BCE Loss: 1.0135546922683716\n",
      "Epoch 295 / 500 | iteration 5 / 30 | Total Loss: 5.360705375671387 | KNN Loss: 4.34467887878418 | BCE Loss: 1.016026496887207\n",
      "Epoch 295 / 500 | iteration 10 / 30 | Total Loss: 5.378012657165527 | KNN Loss: 4.365675449371338 | BCE Loss: 1.0123372077941895\n",
      "Epoch 295 / 500 | iteration 15 / 30 | Total Loss: 5.367954254150391 | KNN Loss: 4.33689546585083 | BCE Loss: 1.0310590267181396\n",
      "Epoch 295 / 500 | iteration 20 / 30 | Total Loss: 5.354148864746094 | KNN Loss: 4.335931301116943 | BCE Loss: 1.0182178020477295\n",
      "Epoch 295 / 500 | iteration 25 / 30 | Total Loss: 5.4714765548706055 | KNN Loss: 4.39718770980835 | BCE Loss: 1.0742888450622559\n",
      "Epoch   296: reducing learning rate of group 0 to 2.3738e-05.\n",
      "Epoch 296 / 500 | iteration 0 / 30 | Total Loss: 5.415471076965332 | KNN Loss: 4.369297027587891 | BCE Loss: 1.0461738109588623\n",
      "Epoch 296 / 500 | iteration 5 / 30 | Total Loss: 5.372258186340332 | KNN Loss: 4.352715015411377 | BCE Loss: 1.0195434093475342\n",
      "Epoch 296 / 500 | iteration 10 / 30 | Total Loss: 5.37553596496582 | KNN Loss: 4.366959571838379 | BCE Loss: 1.0085761547088623\n",
      "Epoch 296 / 500 | iteration 15 / 30 | Total Loss: 5.384662628173828 | KNN Loss: 4.359210968017578 | BCE Loss: 1.025451421737671\n",
      "Epoch 296 / 500 | iteration 20 / 30 | Total Loss: 5.346304893493652 | KNN Loss: 4.315415859222412 | BCE Loss: 1.0308892726898193\n",
      "Epoch 296 / 500 | iteration 25 / 30 | Total Loss: 5.3613739013671875 | KNN Loss: 4.3403401374816895 | BCE Loss: 1.0210340023040771\n",
      "Epoch 297 / 500 | iteration 0 / 30 | Total Loss: 5.419965744018555 | KNN Loss: 4.4018235206604 | BCE Loss: 1.0181424617767334\n",
      "Epoch 297 / 500 | iteration 5 / 30 | Total Loss: 5.403960227966309 | KNN Loss: 4.3372368812561035 | BCE Loss: 1.066723108291626\n",
      "Epoch 297 / 500 | iteration 10 / 30 | Total Loss: 5.370153427124023 | KNN Loss: 4.363940238952637 | BCE Loss: 1.0062134265899658\n",
      "Epoch 297 / 500 | iteration 15 / 30 | Total Loss: 5.387383937835693 | KNN Loss: 4.354418754577637 | BCE Loss: 1.0329653024673462\n",
      "Epoch 297 / 500 | iteration 20 / 30 | Total Loss: 5.340816974639893 | KNN Loss: 4.313445568084717 | BCE Loss: 1.0273714065551758\n",
      "Epoch 297 / 500 | iteration 25 / 30 | Total Loss: 5.36852502822876 | KNN Loss: 4.346167087554932 | BCE Loss: 1.0223578214645386\n",
      "Epoch 298 / 500 | iteration 0 / 30 | Total Loss: 5.331360816955566 | KNN Loss: 4.343547344207764 | BCE Loss: 0.9878134727478027\n",
      "Epoch 298 / 500 | iteration 5 / 30 | Total Loss: 5.390260219573975 | KNN Loss: 4.352808475494385 | BCE Loss: 1.0374516248703003\n",
      "Epoch 298 / 500 | iteration 10 / 30 | Total Loss: 5.4231157302856445 | KNN Loss: 4.385772228240967 | BCE Loss: 1.0373437404632568\n",
      "Epoch 298 / 500 | iteration 15 / 30 | Total Loss: 5.4255452156066895 | KNN Loss: 4.405593395233154 | BCE Loss: 1.0199518203735352\n",
      "Epoch 298 / 500 | iteration 20 / 30 | Total Loss: 5.37795877456665 | KNN Loss: 4.336678981781006 | BCE Loss: 1.0412797927856445\n",
      "Epoch 298 / 500 | iteration 25 / 30 | Total Loss: 5.40985107421875 | KNN Loss: 4.357947826385498 | BCE Loss: 1.0519031286239624\n",
      "Epoch 299 / 500 | iteration 0 / 30 | Total Loss: 5.405149459838867 | KNN Loss: 4.3651652336120605 | BCE Loss: 1.0399844646453857\n",
      "Epoch 299 / 500 | iteration 5 / 30 | Total Loss: 5.381588935852051 | KNN Loss: 4.327568054199219 | BCE Loss: 1.054020643234253\n",
      "Epoch 299 / 500 | iteration 10 / 30 | Total Loss: 5.413232803344727 | KNN Loss: 4.392909049987793 | BCE Loss: 1.0203239917755127\n",
      "Epoch 299 / 500 | iteration 15 / 30 | Total Loss: 5.378459930419922 | KNN Loss: 4.360041618347168 | BCE Loss: 1.0184180736541748\n",
      "Epoch 299 / 500 | iteration 20 / 30 | Total Loss: 5.403984069824219 | KNN Loss: 4.362154960632324 | BCE Loss: 1.0418293476104736\n",
      "Epoch 299 / 500 | iteration 25 / 30 | Total Loss: 5.363580226898193 | KNN Loss: 4.336323261260986 | BCE Loss: 1.027256965637207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 300 / 500 | iteration 0 / 30 | Total Loss: 5.389990329742432 | KNN Loss: 4.355979919433594 | BCE Loss: 1.034010410308838\n",
      "Epoch 300 / 500 | iteration 5 / 30 | Total Loss: 5.348206520080566 | KNN Loss: 4.313880443572998 | BCE Loss: 1.0343259572982788\n",
      "Epoch 300 / 500 | iteration 10 / 30 | Total Loss: 5.396929740905762 | KNN Loss: 4.364871978759766 | BCE Loss: 1.0320580005645752\n",
      "Epoch 300 / 500 | iteration 15 / 30 | Total Loss: 5.388006210327148 | KNN Loss: 4.359795093536377 | BCE Loss: 1.0282108783721924\n",
      "Epoch 300 / 500 | iteration 20 / 30 | Total Loss: 5.370822906494141 | KNN Loss: 4.342879295349121 | BCE Loss: 1.0279433727264404\n",
      "Epoch 300 / 500 | iteration 25 / 30 | Total Loss: 5.386289119720459 | KNN Loss: 4.3584465980529785 | BCE Loss: 1.0278425216674805\n",
      "Epoch 301 / 500 | iteration 0 / 30 | Total Loss: 5.387313365936279 | KNN Loss: 4.361653804779053 | BCE Loss: 1.025659441947937\n",
      "Epoch 301 / 500 | iteration 5 / 30 | Total Loss: 5.371978759765625 | KNN Loss: 4.334102153778076 | BCE Loss: 1.0378764867782593\n",
      "Epoch 301 / 500 | iteration 10 / 30 | Total Loss: 5.34514856338501 | KNN Loss: 4.332671165466309 | BCE Loss: 1.0124775171279907\n",
      "Epoch 301 / 500 | iteration 15 / 30 | Total Loss: 5.390554428100586 | KNN Loss: 4.362805366516113 | BCE Loss: 1.0277490615844727\n",
      "Epoch 301 / 500 | iteration 20 / 30 | Total Loss: 5.360152721405029 | KNN Loss: 4.353982448577881 | BCE Loss: 1.006170392036438\n",
      "Epoch 301 / 500 | iteration 25 / 30 | Total Loss: 5.371289253234863 | KNN Loss: 4.339190483093262 | BCE Loss: 1.0320987701416016\n",
      "Epoch 302 / 500 | iteration 0 / 30 | Total Loss: 5.3702616691589355 | KNN Loss: 4.340038299560547 | BCE Loss: 1.0302233695983887\n",
      "Epoch 302 / 500 | iteration 5 / 30 | Total Loss: 5.337823867797852 | KNN Loss: 4.325052261352539 | BCE Loss: 1.0127718448638916\n",
      "Epoch 302 / 500 | iteration 10 / 30 | Total Loss: 5.455217361450195 | KNN Loss: 4.405252456665039 | BCE Loss: 1.0499649047851562\n",
      "Epoch 302 / 500 | iteration 15 / 30 | Total Loss: 5.324102401733398 | KNN Loss: 4.304792404174805 | BCE Loss: 1.0193099975585938\n",
      "Epoch 302 / 500 | iteration 20 / 30 | Total Loss: 5.436811923980713 | KNN Loss: 4.422758102416992 | BCE Loss: 1.0140539407730103\n",
      "Epoch 302 / 500 | iteration 25 / 30 | Total Loss: 5.388473987579346 | KNN Loss: 4.315211296081543 | BCE Loss: 1.0732626914978027\n",
      "Epoch 303 / 500 | iteration 0 / 30 | Total Loss: 5.360328674316406 | KNN Loss: 4.351844310760498 | BCE Loss: 1.0084843635559082\n",
      "Epoch 303 / 500 | iteration 5 / 30 | Total Loss: 5.324948310852051 | KNN Loss: 4.315479755401611 | BCE Loss: 1.0094687938690186\n",
      "Epoch 303 / 500 | iteration 10 / 30 | Total Loss: 5.364843368530273 | KNN Loss: 4.322597980499268 | BCE Loss: 1.0422453880310059\n",
      "Epoch 303 / 500 | iteration 15 / 30 | Total Loss: 5.397176742553711 | KNN Loss: 4.383077621459961 | BCE Loss: 1.014098882675171\n",
      "Epoch 303 / 500 | iteration 20 / 30 | Total Loss: 5.405462741851807 | KNN Loss: 4.356205463409424 | BCE Loss: 1.0492572784423828\n",
      "Epoch 303 / 500 | iteration 25 / 30 | Total Loss: 5.444975852966309 | KNN Loss: 4.403661727905273 | BCE Loss: 1.0413141250610352\n",
      "Epoch 304 / 500 | iteration 0 / 30 | Total Loss: 5.395815849304199 | KNN Loss: 4.351034641265869 | BCE Loss: 1.0447814464569092\n",
      "Epoch 304 / 500 | iteration 5 / 30 | Total Loss: 5.422749042510986 | KNN Loss: 4.384696960449219 | BCE Loss: 1.0380520820617676\n",
      "Epoch 304 / 500 | iteration 10 / 30 | Total Loss: 5.3672966957092285 | KNN Loss: 4.358734130859375 | BCE Loss: 1.008562684059143\n",
      "Epoch 304 / 500 | iteration 15 / 30 | Total Loss: 5.348733901977539 | KNN Loss: 4.305619239807129 | BCE Loss: 1.0431149005889893\n",
      "Epoch 304 / 500 | iteration 20 / 30 | Total Loss: 5.348541736602783 | KNN Loss: 4.304599285125732 | BCE Loss: 1.0439424514770508\n",
      "Epoch 304 / 500 | iteration 25 / 30 | Total Loss: 5.314436912536621 | KNN Loss: 4.3182501792907715 | BCE Loss: 0.9961866140365601\n",
      "Epoch 305 / 500 | iteration 0 / 30 | Total Loss: 5.350542068481445 | KNN Loss: 4.324349403381348 | BCE Loss: 1.0261924266815186\n",
      "Epoch 305 / 500 | iteration 5 / 30 | Total Loss: 5.457620620727539 | KNN Loss: 4.39823055267334 | BCE Loss: 1.0593903064727783\n",
      "Epoch 305 / 500 | iteration 10 / 30 | Total Loss: 5.416905879974365 | KNN Loss: 4.388967037200928 | BCE Loss: 1.027938723564148\n",
      "Epoch 305 / 500 | iteration 15 / 30 | Total Loss: 5.378464698791504 | KNN Loss: 4.3231635093688965 | BCE Loss: 1.0553009510040283\n",
      "Epoch 305 / 500 | iteration 20 / 30 | Total Loss: 5.395843505859375 | KNN Loss: 4.370634078979492 | BCE Loss: 1.025209665298462\n",
      "Epoch 305 / 500 | iteration 25 / 30 | Total Loss: 5.415687084197998 | KNN Loss: 4.412163257598877 | BCE Loss: 1.0035239458084106\n",
      "Epoch 306 / 500 | iteration 0 / 30 | Total Loss: 5.376901626586914 | KNN Loss: 4.364367961883545 | BCE Loss: 1.0125339031219482\n",
      "Epoch 306 / 500 | iteration 5 / 30 | Total Loss: 5.363246917724609 | KNN Loss: 4.3568315505981445 | BCE Loss: 1.0064154863357544\n",
      "Epoch 306 / 500 | iteration 10 / 30 | Total Loss: 5.418048858642578 | KNN Loss: 4.357233047485352 | BCE Loss: 1.0608158111572266\n",
      "Epoch 306 / 500 | iteration 15 / 30 | Total Loss: 5.393855094909668 | KNN Loss: 4.341773509979248 | BCE Loss: 1.052081823348999\n",
      "Epoch 306 / 500 | iteration 20 / 30 | Total Loss: 5.349806785583496 | KNN Loss: 4.321337699890137 | BCE Loss: 1.0284690856933594\n",
      "Epoch 306 / 500 | iteration 25 / 30 | Total Loss: 5.357365131378174 | KNN Loss: 4.341588020324707 | BCE Loss: 1.0157772302627563\n",
      "Epoch   307: reducing learning rate of group 0 to 1.6616e-05.\n",
      "Epoch 307 / 500 | iteration 0 / 30 | Total Loss: 5.330109119415283 | KNN Loss: 4.345288276672363 | BCE Loss: 0.9848210215568542\n",
      "Epoch 307 / 500 | iteration 5 / 30 | Total Loss: 5.408085823059082 | KNN Loss: 4.3911333084106445 | BCE Loss: 1.0169527530670166\n",
      "Epoch 307 / 500 | iteration 10 / 30 | Total Loss: 5.3737030029296875 | KNN Loss: 4.335383892059326 | BCE Loss: 1.0383191108703613\n",
      "Epoch 307 / 500 | iteration 15 / 30 | Total Loss: 5.416014671325684 | KNN Loss: 4.378117561340332 | BCE Loss: 1.0378968715667725\n",
      "Epoch 307 / 500 | iteration 20 / 30 | Total Loss: 5.346365928649902 | KNN Loss: 4.351872444152832 | BCE Loss: 0.9944934248924255\n",
      "Epoch 307 / 500 | iteration 25 / 30 | Total Loss: 5.3668012619018555 | KNN Loss: 4.342604160308838 | BCE Loss: 1.0241968631744385\n",
      "Epoch 308 / 500 | iteration 0 / 30 | Total Loss: 5.355116844177246 | KNN Loss: 4.337245941162109 | BCE Loss: 1.0178709030151367\n",
      "Epoch 308 / 500 | iteration 5 / 30 | Total Loss: 5.435235500335693 | KNN Loss: 4.392354488372803 | BCE Loss: 1.042880892753601\n",
      "Epoch 308 / 500 | iteration 10 / 30 | Total Loss: 5.350622177124023 | KNN Loss: 4.337014198303223 | BCE Loss: 1.0136079788208008\n",
      "Epoch 308 / 500 | iteration 15 / 30 | Total Loss: 5.343301773071289 | KNN Loss: 4.340412616729736 | BCE Loss: 1.0028891563415527\n",
      "Epoch 308 / 500 | iteration 20 / 30 | Total Loss: 5.379262924194336 | KNN Loss: 4.382203102111816 | BCE Loss: 0.9970600605010986\n",
      "Epoch 308 / 500 | iteration 25 / 30 | Total Loss: 5.352041244506836 | KNN Loss: 4.333775043487549 | BCE Loss: 1.018266201019287\n",
      "Epoch 309 / 500 | iteration 0 / 30 | Total Loss: 5.364167213439941 | KNN Loss: 4.322142124176025 | BCE Loss: 1.0420253276824951\n",
      "Epoch 309 / 500 | iteration 5 / 30 | Total Loss: 5.356112480163574 | KNN Loss: 4.3564934730529785 | BCE Loss: 0.9996188879013062\n",
      "Epoch 309 / 500 | iteration 10 / 30 | Total Loss: 5.34422492980957 | KNN Loss: 4.356947422027588 | BCE Loss: 0.9872773885726929\n",
      "Epoch 309 / 500 | iteration 15 / 30 | Total Loss: 5.323356628417969 | KNN Loss: 4.3133039474487305 | BCE Loss: 1.0100526809692383\n",
      "Epoch 309 / 500 | iteration 20 / 30 | Total Loss: 5.403404235839844 | KNN Loss: 4.394020080566406 | BCE Loss: 1.009384274482727\n",
      "Epoch 309 / 500 | iteration 25 / 30 | Total Loss: 5.405255317687988 | KNN Loss: 4.370946884155273 | BCE Loss: 1.0343084335327148\n",
      "Epoch 310 / 500 | iteration 0 / 30 | Total Loss: 5.419109344482422 | KNN Loss: 4.391449451446533 | BCE Loss: 1.0276598930358887\n",
      "Epoch 310 / 500 | iteration 5 / 30 | Total Loss: 5.349345684051514 | KNN Loss: 4.3137593269348145 | BCE Loss: 1.0355862379074097\n",
      "Epoch 310 / 500 | iteration 10 / 30 | Total Loss: 5.411859512329102 | KNN Loss: 4.389963150024414 | BCE Loss: 1.021896243095398\n",
      "Epoch 310 / 500 | iteration 15 / 30 | Total Loss: 5.384322166442871 | KNN Loss: 4.350366115570068 | BCE Loss: 1.0339558124542236\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 310 / 500 | iteration 20 / 30 | Total Loss: 5.410435676574707 | KNN Loss: 4.364666938781738 | BCE Loss: 1.0457689762115479\n",
      "Epoch 310 / 500 | iteration 25 / 30 | Total Loss: 5.3740153312683105 | KNN Loss: 4.344790935516357 | BCE Loss: 1.0292242765426636\n",
      "Epoch 311 / 500 | iteration 0 / 30 | Total Loss: 5.342670440673828 | KNN Loss: 4.315915107727051 | BCE Loss: 1.0267555713653564\n",
      "Epoch 311 / 500 | iteration 5 / 30 | Total Loss: 5.338126182556152 | KNN Loss: 4.32811164855957 | BCE Loss: 1.0100146532058716\n",
      "Epoch 311 / 500 | iteration 10 / 30 | Total Loss: 5.386592864990234 | KNN Loss: 4.353062629699707 | BCE Loss: 1.0335302352905273\n",
      "Epoch 311 / 500 | iteration 15 / 30 | Total Loss: 5.349861145019531 | KNN Loss: 4.331956386566162 | BCE Loss: 1.0179049968719482\n",
      "Epoch 311 / 500 | iteration 20 / 30 | Total Loss: 5.402221202850342 | KNN Loss: 4.38935661315918 | BCE Loss: 1.012864589691162\n",
      "Epoch 311 / 500 | iteration 25 / 30 | Total Loss: 5.369329452514648 | KNN Loss: 4.338865280151367 | BCE Loss: 1.0304641723632812\n",
      "Epoch 312 / 500 | iteration 0 / 30 | Total Loss: 5.363755226135254 | KNN Loss: 4.359851360321045 | BCE Loss: 1.003904104232788\n",
      "Epoch 312 / 500 | iteration 5 / 30 | Total Loss: 5.4016618728637695 | KNN Loss: 4.370127201080322 | BCE Loss: 1.0315349102020264\n",
      "Epoch 312 / 500 | iteration 10 / 30 | Total Loss: 5.416475296020508 | KNN Loss: 4.408841609954834 | BCE Loss: 1.0076338052749634\n",
      "Epoch 312 / 500 | iteration 15 / 30 | Total Loss: 5.383050918579102 | KNN Loss: 4.345858097076416 | BCE Loss: 1.0371925830841064\n",
      "Epoch 312 / 500 | iteration 20 / 30 | Total Loss: 5.448737144470215 | KNN Loss: 4.409860610961914 | BCE Loss: 1.0388764142990112\n",
      "Epoch 312 / 500 | iteration 25 / 30 | Total Loss: 5.321236610412598 | KNN Loss: 4.299912929534912 | BCE Loss: 1.0213236808776855\n",
      "Epoch 313 / 500 | iteration 0 / 30 | Total Loss: 5.299093246459961 | KNN Loss: 4.311582088470459 | BCE Loss: 0.9875112771987915\n",
      "Epoch 313 / 500 | iteration 5 / 30 | Total Loss: 5.326626777648926 | KNN Loss: 4.311677932739258 | BCE Loss: 1.014948844909668\n",
      "Epoch 313 / 500 | iteration 10 / 30 | Total Loss: 5.338901996612549 | KNN Loss: 4.353421688079834 | BCE Loss: 0.9854803681373596\n",
      "Epoch 313 / 500 | iteration 15 / 30 | Total Loss: 5.424139976501465 | KNN Loss: 4.382699966430664 | BCE Loss: 1.0414402484893799\n",
      "Epoch 313 / 500 | iteration 20 / 30 | Total Loss: 5.347692966461182 | KNN Loss: 4.321484088897705 | BCE Loss: 1.0262088775634766\n",
      "Epoch 313 / 500 | iteration 25 / 30 | Total Loss: 5.366243839263916 | KNN Loss: 4.341741561889648 | BCE Loss: 1.024502158164978\n",
      "Epoch 314 / 500 | iteration 0 / 30 | Total Loss: 5.453533172607422 | KNN Loss: 4.413554668426514 | BCE Loss: 1.0399787425994873\n",
      "Epoch 314 / 500 | iteration 5 / 30 | Total Loss: 5.423731803894043 | KNN Loss: 4.363722801208496 | BCE Loss: 1.060009241104126\n",
      "Epoch 314 / 500 | iteration 10 / 30 | Total Loss: 5.361271381378174 | KNN Loss: 4.337101459503174 | BCE Loss: 1.0241698026657104\n",
      "Epoch 314 / 500 | iteration 15 / 30 | Total Loss: 5.366130828857422 | KNN Loss: 4.327524662017822 | BCE Loss: 1.0386064052581787\n",
      "Epoch 314 / 500 | iteration 20 / 30 | Total Loss: 5.388897895812988 | KNN Loss: 4.352163791656494 | BCE Loss: 1.036733865737915\n",
      "Epoch 314 / 500 | iteration 25 / 30 | Total Loss: 5.356629371643066 | KNN Loss: 4.328658103942871 | BCE Loss: 1.0279712677001953\n",
      "Epoch 315 / 500 | iteration 0 / 30 | Total Loss: 5.419939994812012 | KNN Loss: 4.385400772094727 | BCE Loss: 1.0345394611358643\n",
      "Epoch 315 / 500 | iteration 5 / 30 | Total Loss: 5.381408214569092 | KNN Loss: 4.346933364868164 | BCE Loss: 1.0344747304916382\n",
      "Epoch 315 / 500 | iteration 10 / 30 | Total Loss: 5.3605804443359375 | KNN Loss: 4.3362507820129395 | BCE Loss: 1.024329662322998\n",
      "Epoch 315 / 500 | iteration 15 / 30 | Total Loss: 5.348483085632324 | KNN Loss: 4.330877304077148 | BCE Loss: 1.0176060199737549\n",
      "Epoch 315 / 500 | iteration 20 / 30 | Total Loss: 5.388123989105225 | KNN Loss: 4.364158630371094 | BCE Loss: 1.0239652395248413\n",
      "Epoch 315 / 500 | iteration 25 / 30 | Total Loss: 5.373386859893799 | KNN Loss: 4.372009754180908 | BCE Loss: 1.001376986503601\n",
      "Epoch 316 / 500 | iteration 0 / 30 | Total Loss: 5.418202877044678 | KNN Loss: 4.369354248046875 | BCE Loss: 1.0488486289978027\n",
      "Epoch 316 / 500 | iteration 5 / 30 | Total Loss: 5.4011335372924805 | KNN Loss: 4.386069297790527 | BCE Loss: 1.0150644779205322\n",
      "Epoch 316 / 500 | iteration 10 / 30 | Total Loss: 5.3468337059021 | KNN Loss: 4.346492767333984 | BCE Loss: 1.0003409385681152\n",
      "Epoch 316 / 500 | iteration 15 / 30 | Total Loss: 5.359067916870117 | KNN Loss: 4.335386753082275 | BCE Loss: 1.0236810445785522\n",
      "Epoch 316 / 500 | iteration 20 / 30 | Total Loss: 5.349902153015137 | KNN Loss: 4.333998203277588 | BCE Loss: 1.0159039497375488\n",
      "Epoch 316 / 500 | iteration 25 / 30 | Total Loss: 5.398937225341797 | KNN Loss: 4.3704681396484375 | BCE Loss: 1.0284690856933594\n",
      "Epoch 317 / 500 | iteration 0 / 30 | Total Loss: 5.322383880615234 | KNN Loss: 4.313287734985352 | BCE Loss: 1.0090959072113037\n",
      "Epoch 317 / 500 | iteration 5 / 30 | Total Loss: 5.394858360290527 | KNN Loss: 4.359864711761475 | BCE Loss: 1.0349934101104736\n",
      "Epoch 317 / 500 | iteration 10 / 30 | Total Loss: 5.3863019943237305 | KNN Loss: 4.352806568145752 | BCE Loss: 1.0334954261779785\n",
      "Epoch 317 / 500 | iteration 15 / 30 | Total Loss: 5.387508392333984 | KNN Loss: 4.364559173583984 | BCE Loss: 1.02294921875\n",
      "Epoch 317 / 500 | iteration 20 / 30 | Total Loss: 5.368971824645996 | KNN Loss: 4.338446140289307 | BCE Loss: 1.0305255651474\n",
      "Epoch 317 / 500 | iteration 25 / 30 | Total Loss: 5.379937171936035 | KNN Loss: 4.36513614654541 | BCE Loss: 1.014801263809204\n",
      "Epoch 318 / 500 | iteration 0 / 30 | Total Loss: 5.34075403213501 | KNN Loss: 4.323848724365234 | BCE Loss: 1.0169053077697754\n",
      "Epoch 318 / 500 | iteration 5 / 30 | Total Loss: 5.359314441680908 | KNN Loss: 4.358487129211426 | BCE Loss: 1.0008273124694824\n",
      "Epoch 318 / 500 | iteration 10 / 30 | Total Loss: 5.364232063293457 | KNN Loss: 4.333416938781738 | BCE Loss: 1.0308148860931396\n",
      "Epoch 318 / 500 | iteration 15 / 30 | Total Loss: 5.360488414764404 | KNN Loss: 4.356393337249756 | BCE Loss: 1.0040950775146484\n",
      "Epoch 318 / 500 | iteration 20 / 30 | Total Loss: 5.369068145751953 | KNN Loss: 4.360377788543701 | BCE Loss: 1.0086904764175415\n",
      "Epoch 318 / 500 | iteration 25 / 30 | Total Loss: 5.395205497741699 | KNN Loss: 4.333110332489014 | BCE Loss: 1.0620954036712646\n",
      "Epoch 319 / 500 | iteration 0 / 30 | Total Loss: 5.356631278991699 | KNN Loss: 4.360567569732666 | BCE Loss: 0.9960638284683228\n",
      "Epoch 319 / 500 | iteration 5 / 30 | Total Loss: 5.337420463562012 | KNN Loss: 4.335794448852539 | BCE Loss: 1.0016262531280518\n",
      "Epoch 319 / 500 | iteration 10 / 30 | Total Loss: 5.353472709655762 | KNN Loss: 4.34771203994751 | BCE Loss: 1.005760908126831\n",
      "Epoch 319 / 500 | iteration 15 / 30 | Total Loss: 5.351304054260254 | KNN Loss: 4.3332390785217285 | BCE Loss: 1.0180649757385254\n",
      "Epoch 319 / 500 | iteration 20 / 30 | Total Loss: 5.394847869873047 | KNN Loss: 4.344422340393066 | BCE Loss: 1.050425410270691\n",
      "Epoch 319 / 500 | iteration 25 / 30 | Total Loss: 5.3275909423828125 | KNN Loss: 4.333131313323975 | BCE Loss: 0.9944596290588379\n",
      "Epoch 320 / 500 | iteration 0 / 30 | Total Loss: 5.3959245681762695 | KNN Loss: 4.35770845413208 | BCE Loss: 1.0382158756256104\n",
      "Epoch 320 / 500 | iteration 5 / 30 | Total Loss: 5.33491849899292 | KNN Loss: 4.316467761993408 | BCE Loss: 1.0184508562088013\n",
      "Epoch 320 / 500 | iteration 10 / 30 | Total Loss: 5.401086330413818 | KNN Loss: 4.358633995056152 | BCE Loss: 1.0424522161483765\n",
      "Epoch 320 / 500 | iteration 15 / 30 | Total Loss: 5.381769180297852 | KNN Loss: 4.346092700958252 | BCE Loss: 1.0356767177581787\n",
      "Epoch 320 / 500 | iteration 20 / 30 | Total Loss: 5.3878912925720215 | KNN Loss: 4.361645698547363 | BCE Loss: 1.0262454748153687\n",
      "Epoch 320 / 500 | iteration 25 / 30 | Total Loss: 5.4018683433532715 | KNN Loss: 4.3613409996032715 | BCE Loss: 1.0405274629592896\n",
      "Epoch 321 / 500 | iteration 0 / 30 | Total Loss: 5.359464645385742 | KNN Loss: 4.336658477783203 | BCE Loss: 1.022806167602539\n",
      "Epoch 321 / 500 | iteration 5 / 30 | Total Loss: 5.362302780151367 | KNN Loss: 4.339195728302002 | BCE Loss: 1.0231068134307861\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 321 / 500 | iteration 10 / 30 | Total Loss: 5.34492301940918 | KNN Loss: 4.324484348297119 | BCE Loss: 1.0204386711120605\n",
      "Epoch 321 / 500 | iteration 15 / 30 | Total Loss: 5.350344657897949 | KNN Loss: 4.334980010986328 | BCE Loss: 1.0153645277023315\n",
      "Epoch 321 / 500 | iteration 20 / 30 | Total Loss: 5.345221042633057 | KNN Loss: 4.309000492095947 | BCE Loss: 1.036220669746399\n",
      "Epoch 321 / 500 | iteration 25 / 30 | Total Loss: 5.355144500732422 | KNN Loss: 4.345033645629883 | BCE Loss: 1.0101110935211182\n",
      "Epoch 322 / 500 | iteration 0 / 30 | Total Loss: 5.343729496002197 | KNN Loss: 4.363171100616455 | BCE Loss: 0.9805582165718079\n",
      "Epoch 322 / 500 | iteration 5 / 30 | Total Loss: 5.39039421081543 | KNN Loss: 4.363812446594238 | BCE Loss: 1.026581883430481\n",
      "Epoch 322 / 500 | iteration 10 / 30 | Total Loss: 5.392969131469727 | KNN Loss: 4.34699821472168 | BCE Loss: 1.045971155166626\n",
      "Epoch 322 / 500 | iteration 15 / 30 | Total Loss: 5.393810272216797 | KNN Loss: 4.371192932128906 | BCE Loss: 1.0226174592971802\n",
      "Epoch 322 / 500 | iteration 20 / 30 | Total Loss: 5.349726676940918 | KNN Loss: 4.339375019073486 | BCE Loss: 1.0103514194488525\n",
      "Epoch 322 / 500 | iteration 25 / 30 | Total Loss: 5.385914325714111 | KNN Loss: 4.342109203338623 | BCE Loss: 1.0438050031661987\n",
      "Epoch   323: reducing learning rate of group 0 to 1.1632e-05.\n",
      "Epoch 323 / 500 | iteration 0 / 30 | Total Loss: 5.41787052154541 | KNN Loss: 4.386619567871094 | BCE Loss: 1.0312509536743164\n",
      "Epoch 323 / 500 | iteration 5 / 30 | Total Loss: 5.394047737121582 | KNN Loss: 4.356234073638916 | BCE Loss: 1.0378137826919556\n",
      "Epoch 323 / 500 | iteration 10 / 30 | Total Loss: 5.392192840576172 | KNN Loss: 4.345855712890625 | BCE Loss: 1.0463372468948364\n",
      "Epoch 323 / 500 | iteration 15 / 30 | Total Loss: 5.355627059936523 | KNN Loss: 4.343221187591553 | BCE Loss: 1.0124058723449707\n",
      "Epoch 323 / 500 | iteration 20 / 30 | Total Loss: 5.359994888305664 | KNN Loss: 4.343179702758789 | BCE Loss: 1.0168150663375854\n",
      "Epoch 323 / 500 | iteration 25 / 30 | Total Loss: 5.408807277679443 | KNN Loss: 4.344233989715576 | BCE Loss: 1.0645732879638672\n",
      "Epoch 324 / 500 | iteration 0 / 30 | Total Loss: 5.358613014221191 | KNN Loss: 4.341314315795898 | BCE Loss: 1.017298936843872\n",
      "Epoch 324 / 500 | iteration 5 / 30 | Total Loss: 5.383937358856201 | KNN Loss: 4.356502532958984 | BCE Loss: 1.0274348258972168\n",
      "Epoch 324 / 500 | iteration 10 / 30 | Total Loss: 5.361530303955078 | KNN Loss: 4.329775333404541 | BCE Loss: 1.031754732131958\n",
      "Epoch 324 / 500 | iteration 15 / 30 | Total Loss: 5.358694553375244 | KNN Loss: 4.347879409790039 | BCE Loss: 1.010815143585205\n",
      "Epoch 324 / 500 | iteration 20 / 30 | Total Loss: 5.403861045837402 | KNN Loss: 4.356949329376221 | BCE Loss: 1.0469114780426025\n",
      "Epoch 324 / 500 | iteration 25 / 30 | Total Loss: 5.376313209533691 | KNN Loss: 4.365391254425049 | BCE Loss: 1.0109219551086426\n",
      "Epoch 325 / 500 | iteration 0 / 30 | Total Loss: 5.414097785949707 | KNN Loss: 4.389159202575684 | BCE Loss: 1.0249383449554443\n",
      "Epoch 325 / 500 | iteration 5 / 30 | Total Loss: 5.4074602127075195 | KNN Loss: 4.382762432098389 | BCE Loss: 1.0246977806091309\n",
      "Epoch 325 / 500 | iteration 10 / 30 | Total Loss: 5.403809547424316 | KNN Loss: 4.39419412612915 | BCE Loss: 1.0096155405044556\n",
      "Epoch 325 / 500 | iteration 15 / 30 | Total Loss: 5.336236953735352 | KNN Loss: 4.341041088104248 | BCE Loss: 0.9951958656311035\n",
      "Epoch 325 / 500 | iteration 20 / 30 | Total Loss: 5.356768608093262 | KNN Loss: 4.322930812835693 | BCE Loss: 1.033837914466858\n",
      "Epoch 325 / 500 | iteration 25 / 30 | Total Loss: 5.3837175369262695 | KNN Loss: 4.346089839935303 | BCE Loss: 1.0376276969909668\n",
      "Epoch 326 / 500 | iteration 0 / 30 | Total Loss: 5.35347318649292 | KNN Loss: 4.320528030395508 | BCE Loss: 1.032945156097412\n",
      "Epoch 326 / 500 | iteration 5 / 30 | Total Loss: 5.331676483154297 | KNN Loss: 4.333831310272217 | BCE Loss: 0.997844934463501\n",
      "Epoch 326 / 500 | iteration 10 / 30 | Total Loss: 5.370976448059082 | KNN Loss: 4.329180717468262 | BCE Loss: 1.0417959690093994\n",
      "Epoch 326 / 500 | iteration 15 / 30 | Total Loss: 5.33056640625 | KNN Loss: 4.305939674377441 | BCE Loss: 1.0246269702911377\n",
      "Epoch 326 / 500 | iteration 20 / 30 | Total Loss: 5.349784851074219 | KNN Loss: 4.343667984008789 | BCE Loss: 1.0061171054840088\n",
      "Epoch 326 / 500 | iteration 25 / 30 | Total Loss: 5.353184700012207 | KNN Loss: 4.343557834625244 | BCE Loss: 1.009626865386963\n",
      "Epoch 327 / 500 | iteration 0 / 30 | Total Loss: 5.370665550231934 | KNN Loss: 4.3574395179748535 | BCE Loss: 1.01322603225708\n",
      "Epoch 327 / 500 | iteration 5 / 30 | Total Loss: 5.412140846252441 | KNN Loss: 4.378155708312988 | BCE Loss: 1.0339853763580322\n",
      "Epoch 327 / 500 | iteration 10 / 30 | Total Loss: 5.361697196960449 | KNN Loss: 4.344423770904541 | BCE Loss: 1.0172734260559082\n",
      "Epoch 327 / 500 | iteration 15 / 30 | Total Loss: 5.416518211364746 | KNN Loss: 4.401410102844238 | BCE Loss: 1.0151079893112183\n",
      "Epoch 327 / 500 | iteration 20 / 30 | Total Loss: 5.431380748748779 | KNN Loss: 4.378347873687744 | BCE Loss: 1.0530327558517456\n",
      "Epoch 327 / 500 | iteration 25 / 30 | Total Loss: 5.337721824645996 | KNN Loss: 4.317199230194092 | BCE Loss: 1.0205227136611938\n",
      "Epoch 328 / 500 | iteration 0 / 30 | Total Loss: 5.397597312927246 | KNN Loss: 4.356270790100098 | BCE Loss: 1.0413262844085693\n",
      "Epoch 328 / 500 | iteration 5 / 30 | Total Loss: 5.391498565673828 | KNN Loss: 4.359457015991211 | BCE Loss: 1.0320415496826172\n",
      "Epoch 328 / 500 | iteration 10 / 30 | Total Loss: 5.407777786254883 | KNN Loss: 4.407566070556641 | BCE Loss: 1.0002117156982422\n",
      "Epoch 328 / 500 | iteration 15 / 30 | Total Loss: 5.39817476272583 | KNN Loss: 4.389673233032227 | BCE Loss: 1.0085015296936035\n",
      "Epoch 328 / 500 | iteration 20 / 30 | Total Loss: 5.3814592361450195 | KNN Loss: 4.387246131896973 | BCE Loss: 0.9942131638526917\n",
      "Epoch 328 / 500 | iteration 25 / 30 | Total Loss: 5.375387191772461 | KNN Loss: 4.339815139770508 | BCE Loss: 1.0355720520019531\n",
      "Epoch 329 / 500 | iteration 0 / 30 | Total Loss: 5.409494876861572 | KNN Loss: 4.343990325927734 | BCE Loss: 1.065504550933838\n",
      "Epoch 329 / 500 | iteration 5 / 30 | Total Loss: 5.398349761962891 | KNN Loss: 4.349245071411133 | BCE Loss: 1.0491046905517578\n",
      "Epoch 329 / 500 | iteration 10 / 30 | Total Loss: 5.394440650939941 | KNN Loss: 4.370796203613281 | BCE Loss: 1.0236446857452393\n",
      "Epoch 329 / 500 | iteration 15 / 30 | Total Loss: 5.348138332366943 | KNN Loss: 4.303658485412598 | BCE Loss: 1.0444798469543457\n",
      "Epoch 329 / 500 | iteration 20 / 30 | Total Loss: 5.383498668670654 | KNN Loss: 4.34110689163208 | BCE Loss: 1.0423916578292847\n",
      "Epoch 329 / 500 | iteration 25 / 30 | Total Loss: 5.416461944580078 | KNN Loss: 4.366325378417969 | BCE Loss: 1.0501368045806885\n",
      "Epoch 330 / 500 | iteration 0 / 30 | Total Loss: 5.405296802520752 | KNN Loss: 4.392124652862549 | BCE Loss: 1.0131721496582031\n",
      "Epoch 330 / 500 | iteration 5 / 30 | Total Loss: 5.381285667419434 | KNN Loss: 4.311773300170898 | BCE Loss: 1.069512128829956\n",
      "Epoch 330 / 500 | iteration 10 / 30 | Total Loss: 5.364918231964111 | KNN Loss: 4.346477508544922 | BCE Loss: 1.018440842628479\n",
      "Epoch 330 / 500 | iteration 15 / 30 | Total Loss: 5.422984600067139 | KNN Loss: 4.402471542358398 | BCE Loss: 1.0205130577087402\n",
      "Epoch 330 / 500 | iteration 20 / 30 | Total Loss: 5.351563453674316 | KNN Loss: 4.364680290222168 | BCE Loss: 0.986883282661438\n",
      "Epoch 330 / 500 | iteration 25 / 30 | Total Loss: 5.374528884887695 | KNN Loss: 4.367898941040039 | BCE Loss: 1.0066301822662354\n",
      "Epoch 331 / 500 | iteration 0 / 30 | Total Loss: 5.345522880554199 | KNN Loss: 4.337174892425537 | BCE Loss: 1.0083482265472412\n",
      "Epoch 331 / 500 | iteration 5 / 30 | Total Loss: 5.384437084197998 | KNN Loss: 4.337319374084473 | BCE Loss: 1.0471175909042358\n",
      "Epoch 331 / 500 | iteration 10 / 30 | Total Loss: 5.366629600524902 | KNN Loss: 4.332739353179932 | BCE Loss: 1.0338904857635498\n",
      "Epoch 331 / 500 | iteration 15 / 30 | Total Loss: 5.347479343414307 | KNN Loss: 4.315058708190918 | BCE Loss: 1.0324206352233887\n",
      "Epoch 331 / 500 | iteration 20 / 30 | Total Loss: 5.376716613769531 | KNN Loss: 4.355826377868652 | BCE Loss: 1.020890235900879\n",
      "Epoch 331 / 500 | iteration 25 / 30 | Total Loss: 5.411820411682129 | KNN Loss: 4.38922643661499 | BCE Loss: 1.0225939750671387\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 332 / 500 | iteration 0 / 30 | Total Loss: 5.411312103271484 | KNN Loss: 4.355129241943359 | BCE Loss: 1.056182622909546\n",
      "Epoch 332 / 500 | iteration 5 / 30 | Total Loss: 5.346630096435547 | KNN Loss: 4.33907413482666 | BCE Loss: 1.0075558423995972\n",
      "Epoch 332 / 500 | iteration 10 / 30 | Total Loss: 5.349249362945557 | KNN Loss: 4.351034641265869 | BCE Loss: 0.9982146620750427\n",
      "Epoch 332 / 500 | iteration 15 / 30 | Total Loss: 5.372572898864746 | KNN Loss: 4.352540969848633 | BCE Loss: 1.0200319290161133\n",
      "Epoch 332 / 500 | iteration 20 / 30 | Total Loss: 5.382381439208984 | KNN Loss: 4.331045627593994 | BCE Loss: 1.0513358116149902\n",
      "Epoch 332 / 500 | iteration 25 / 30 | Total Loss: 5.355215072631836 | KNN Loss: 4.350736141204834 | BCE Loss: 1.0044786930084229\n",
      "Epoch 333 / 500 | iteration 0 / 30 | Total Loss: 5.344328880310059 | KNN Loss: 4.329905033111572 | BCE Loss: 1.0144238471984863\n",
      "Epoch 333 / 500 | iteration 5 / 30 | Total Loss: 5.351188659667969 | KNN Loss: 4.339887619018555 | BCE Loss: 1.011301040649414\n",
      "Epoch 333 / 500 | iteration 10 / 30 | Total Loss: 5.402032375335693 | KNN Loss: 4.3610453605651855 | BCE Loss: 1.0409868955612183\n",
      "Epoch 333 / 500 | iteration 15 / 30 | Total Loss: 5.351428985595703 | KNN Loss: 4.328965187072754 | BCE Loss: 1.0224637985229492\n",
      "Epoch 333 / 500 | iteration 20 / 30 | Total Loss: 5.392441272735596 | KNN Loss: 4.359508037567139 | BCE Loss: 1.032933235168457\n",
      "Epoch 333 / 500 | iteration 25 / 30 | Total Loss: 5.368236064910889 | KNN Loss: 4.327124118804932 | BCE Loss: 1.041111946105957\n",
      "Epoch   334: reducing learning rate of group 0 to 8.1421e-06.\n",
      "Epoch 334 / 500 | iteration 0 / 30 | Total Loss: 5.412186622619629 | KNN Loss: 4.389708042144775 | BCE Loss: 1.0224785804748535\n",
      "Epoch 334 / 500 | iteration 5 / 30 | Total Loss: 5.347943305969238 | KNN Loss: 4.3223981857299805 | BCE Loss: 1.0255448818206787\n",
      "Epoch 334 / 500 | iteration 10 / 30 | Total Loss: 5.349201679229736 | KNN Loss: 4.331561088562012 | BCE Loss: 1.0176405906677246\n",
      "Epoch 334 / 500 | iteration 15 / 30 | Total Loss: 5.397812843322754 | KNN Loss: 4.364248752593994 | BCE Loss: 1.0335640907287598\n",
      "Epoch 334 / 500 | iteration 20 / 30 | Total Loss: 5.3409624099731445 | KNN Loss: 4.325650691986084 | BCE Loss: 1.0153119564056396\n",
      "Epoch 334 / 500 | iteration 25 / 30 | Total Loss: 5.362913131713867 | KNN Loss: 4.338003158569336 | BCE Loss: 1.0249098539352417\n",
      "Epoch 335 / 500 | iteration 0 / 30 | Total Loss: 5.326393127441406 | KNN Loss: 4.310969829559326 | BCE Loss: 1.01542329788208\n",
      "Epoch 335 / 500 | iteration 5 / 30 | Total Loss: 5.367093563079834 | KNN Loss: 4.353531360626221 | BCE Loss: 1.0135622024536133\n",
      "Epoch 335 / 500 | iteration 10 / 30 | Total Loss: 5.406252861022949 | KNN Loss: 4.387441635131836 | BCE Loss: 1.0188109874725342\n",
      "Epoch 335 / 500 | iteration 15 / 30 | Total Loss: 5.295538902282715 | KNN Loss: 4.326560020446777 | BCE Loss: 0.9689788818359375\n",
      "Epoch 335 / 500 | iteration 20 / 30 | Total Loss: 5.386569023132324 | KNN Loss: 4.345993995666504 | BCE Loss: 1.0405752658843994\n",
      "Epoch 335 / 500 | iteration 25 / 30 | Total Loss: 5.382331848144531 | KNN Loss: 4.350358963012695 | BCE Loss: 1.031972885131836\n",
      "Epoch 336 / 500 | iteration 0 / 30 | Total Loss: 5.423097133636475 | KNN Loss: 4.3904829025268555 | BCE Loss: 1.0326142311096191\n",
      "Epoch 336 / 500 | iteration 5 / 30 | Total Loss: 5.395604133605957 | KNN Loss: 4.327340126037598 | BCE Loss: 1.0682637691497803\n",
      "Epoch 336 / 500 | iteration 10 / 30 | Total Loss: 5.38245964050293 | KNN Loss: 4.345351219177246 | BCE Loss: 1.0371085405349731\n",
      "Epoch 336 / 500 | iteration 15 / 30 | Total Loss: 5.422598838806152 | KNN Loss: 4.38431978225708 | BCE Loss: 1.0382788181304932\n",
      "Epoch 336 / 500 | iteration 20 / 30 | Total Loss: 5.3096537590026855 | KNN Loss: 4.307248115539551 | BCE Loss: 1.0024057626724243\n",
      "Epoch 336 / 500 | iteration 25 / 30 | Total Loss: 5.351424217224121 | KNN Loss: 4.346875190734863 | BCE Loss: 1.0045491456985474\n",
      "Epoch 337 / 500 | iteration 0 / 30 | Total Loss: 5.390377044677734 | KNN Loss: 4.363821983337402 | BCE Loss: 1.0265552997589111\n",
      "Epoch 337 / 500 | iteration 5 / 30 | Total Loss: 5.389898300170898 | KNN Loss: 4.381372451782227 | BCE Loss: 1.008526086807251\n",
      "Epoch 337 / 500 | iteration 10 / 30 | Total Loss: 5.40949010848999 | KNN Loss: 4.36881685256958 | BCE Loss: 1.0406732559204102\n",
      "Epoch 337 / 500 | iteration 15 / 30 | Total Loss: 5.352217197418213 | KNN Loss: 4.32118558883667 | BCE Loss: 1.031031608581543\n",
      "Epoch 337 / 500 | iteration 20 / 30 | Total Loss: 5.383588790893555 | KNN Loss: 4.354324817657471 | BCE Loss: 1.0292638540267944\n",
      "Epoch 337 / 500 | iteration 25 / 30 | Total Loss: 5.377289295196533 | KNN Loss: 4.340987682342529 | BCE Loss: 1.036301612854004\n",
      "Epoch 338 / 500 | iteration 0 / 30 | Total Loss: 5.336622714996338 | KNN Loss: 4.306599140167236 | BCE Loss: 1.0300235748291016\n",
      "Epoch 338 / 500 | iteration 5 / 30 | Total Loss: 5.327260494232178 | KNN Loss: 4.2951436042785645 | BCE Loss: 1.0321168899536133\n",
      "Epoch 338 / 500 | iteration 10 / 30 | Total Loss: 5.397928714752197 | KNN Loss: 4.351654052734375 | BCE Loss: 1.0462746620178223\n",
      "Epoch 338 / 500 | iteration 15 / 30 | Total Loss: 5.3629631996154785 | KNN Loss: 4.344232082366943 | BCE Loss: 1.0187311172485352\n",
      "Epoch 338 / 500 | iteration 20 / 30 | Total Loss: 5.338659763336182 | KNN Loss: 4.337949275970459 | BCE Loss: 1.0007104873657227\n",
      "Epoch 338 / 500 | iteration 25 / 30 | Total Loss: 5.33950662612915 | KNN Loss: 4.334536075592041 | BCE Loss: 1.004970669746399\n",
      "Epoch 339 / 500 | iteration 0 / 30 | Total Loss: 5.396951675415039 | KNN Loss: 4.3441033363342285 | BCE Loss: 1.0528485774993896\n",
      "Epoch 339 / 500 | iteration 5 / 30 | Total Loss: 5.351640224456787 | KNN Loss: 4.334516525268555 | BCE Loss: 1.0171236991882324\n",
      "Epoch 339 / 500 | iteration 10 / 30 | Total Loss: 5.376989364624023 | KNN Loss: 4.341094493865967 | BCE Loss: 1.0358948707580566\n",
      "Epoch 339 / 500 | iteration 15 / 30 | Total Loss: 5.379562854766846 | KNN Loss: 4.372989177703857 | BCE Loss: 1.0065735578536987\n",
      "Epoch 339 / 500 | iteration 20 / 30 | Total Loss: 5.43964958190918 | KNN Loss: 4.3821187019348145 | BCE Loss: 1.0575307607650757\n",
      "Epoch 339 / 500 | iteration 25 / 30 | Total Loss: 5.345165252685547 | KNN Loss: 4.323326587677002 | BCE Loss: 1.021838903427124\n",
      "Epoch 340 / 500 | iteration 0 / 30 | Total Loss: 5.340063095092773 | KNN Loss: 4.317050933837891 | BCE Loss: 1.0230120420455933\n",
      "Epoch 340 / 500 | iteration 5 / 30 | Total Loss: 5.388030529022217 | KNN Loss: 4.3737311363220215 | BCE Loss: 1.0142993927001953\n",
      "Epoch 340 / 500 | iteration 10 / 30 | Total Loss: 5.40873908996582 | KNN Loss: 4.368905544281006 | BCE Loss: 1.039833664894104\n",
      "Epoch 340 / 500 | iteration 15 / 30 | Total Loss: 5.3645429611206055 | KNN Loss: 4.354218482971191 | BCE Loss: 1.010324239730835\n",
      "Epoch 340 / 500 | iteration 20 / 30 | Total Loss: 5.328044891357422 | KNN Loss: 4.311882972717285 | BCE Loss: 1.0161619186401367\n",
      "Epoch 340 / 500 | iteration 25 / 30 | Total Loss: 5.335366249084473 | KNN Loss: 4.341775894165039 | BCE Loss: 0.993590235710144\n",
      "Epoch 341 / 500 | iteration 0 / 30 | Total Loss: 5.351271152496338 | KNN Loss: 4.314843654632568 | BCE Loss: 1.036427617073059\n",
      "Epoch 341 / 500 | iteration 5 / 30 | Total Loss: 5.442672252655029 | KNN Loss: 4.405493259429932 | BCE Loss: 1.0371789932250977\n",
      "Epoch 341 / 500 | iteration 10 / 30 | Total Loss: 5.3807783126831055 | KNN Loss: 4.3357625007629395 | BCE Loss: 1.0450160503387451\n",
      "Epoch 341 / 500 | iteration 15 / 30 | Total Loss: 5.318947792053223 | KNN Loss: 4.301187992095947 | BCE Loss: 1.0177597999572754\n",
      "Epoch 341 / 500 | iteration 20 / 30 | Total Loss: 5.362710952758789 | KNN Loss: 4.320314407348633 | BCE Loss: 1.0423963069915771\n",
      "Epoch 341 / 500 | iteration 25 / 30 | Total Loss: 5.285854339599609 | KNN Loss: 4.2970781326293945 | BCE Loss: 0.988776445388794\n",
      "Epoch 342 / 500 | iteration 0 / 30 | Total Loss: 5.372860908508301 | KNN Loss: 4.3399128913879395 | BCE Loss: 1.0329480171203613\n",
      "Epoch 342 / 500 | iteration 5 / 30 | Total Loss: 5.389163970947266 | KNN Loss: 4.362297058105469 | BCE Loss: 1.0268669128417969\n",
      "Epoch 342 / 500 | iteration 10 / 30 | Total Loss: 5.364424705505371 | KNN Loss: 4.332459926605225 | BCE Loss: 1.0319647789001465\n",
      "Epoch 342 / 500 | iteration 15 / 30 | Total Loss: 5.412357330322266 | KNN Loss: 4.398085594177246 | BCE Loss: 1.01427161693573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 342 / 500 | iteration 20 / 30 | Total Loss: 5.451674461364746 | KNN Loss: 4.418697357177734 | BCE Loss: 1.0329771041870117\n",
      "Epoch 342 / 500 | iteration 25 / 30 | Total Loss: 5.389314651489258 | KNN Loss: 4.3579511642456055 | BCE Loss: 1.031363606452942\n",
      "Epoch 343 / 500 | iteration 0 / 30 | Total Loss: 5.3628411293029785 | KNN Loss: 4.350743293762207 | BCE Loss: 1.0120978355407715\n",
      "Epoch 343 / 500 | iteration 5 / 30 | Total Loss: 5.405381202697754 | KNN Loss: 4.340137958526611 | BCE Loss: 1.0652433633804321\n",
      "Epoch 343 / 500 | iteration 10 / 30 | Total Loss: 5.397469520568848 | KNN Loss: 4.361158847808838 | BCE Loss: 1.0363106727600098\n",
      "Epoch 343 / 500 | iteration 15 / 30 | Total Loss: 5.3735671043396 | KNN Loss: 4.334228992462158 | BCE Loss: 1.039338231086731\n",
      "Epoch 343 / 500 | iteration 20 / 30 | Total Loss: 5.379185676574707 | KNN Loss: 4.369607448577881 | BCE Loss: 1.0095784664154053\n",
      "Epoch 343 / 500 | iteration 25 / 30 | Total Loss: 5.327301979064941 | KNN Loss: 4.30942964553833 | BCE Loss: 1.0178723335266113\n",
      "Epoch 344 / 500 | iteration 0 / 30 | Total Loss: 5.388546943664551 | KNN Loss: 4.3641839027404785 | BCE Loss: 1.0243630409240723\n",
      "Epoch 344 / 500 | iteration 5 / 30 | Total Loss: 5.325196266174316 | KNN Loss: 4.322630882263184 | BCE Loss: 1.0025651454925537\n",
      "Epoch 344 / 500 | iteration 10 / 30 | Total Loss: 5.417393684387207 | KNN Loss: 4.393235683441162 | BCE Loss: 1.024158000946045\n",
      "Epoch 344 / 500 | iteration 15 / 30 | Total Loss: 5.364750862121582 | KNN Loss: 4.376968860626221 | BCE Loss: 0.9877822399139404\n",
      "Epoch 344 / 500 | iteration 20 / 30 | Total Loss: 5.385443687438965 | KNN Loss: 4.343628883361816 | BCE Loss: 1.0418145656585693\n",
      "Epoch 344 / 500 | iteration 25 / 30 | Total Loss: 5.372829914093018 | KNN Loss: 4.343266010284424 | BCE Loss: 1.0295637845993042\n",
      "Epoch   345: reducing learning rate of group 0 to 5.6994e-06.\n",
      "Epoch 345 / 500 | iteration 0 / 30 | Total Loss: 5.414390563964844 | KNN Loss: 4.3901190757751465 | BCE Loss: 1.0242716073989868\n",
      "Epoch 345 / 500 | iteration 5 / 30 | Total Loss: 5.341761589050293 | KNN Loss: 4.3441619873046875 | BCE Loss: 0.9975996017456055\n",
      "Epoch 345 / 500 | iteration 10 / 30 | Total Loss: 5.428542613983154 | KNN Loss: 4.394204139709473 | BCE Loss: 1.0343384742736816\n",
      "Epoch 345 / 500 | iteration 15 / 30 | Total Loss: 5.373117446899414 | KNN Loss: 4.3481879234313965 | BCE Loss: 1.0249292850494385\n",
      "Epoch 345 / 500 | iteration 20 / 30 | Total Loss: 5.409101486206055 | KNN Loss: 4.388962745666504 | BCE Loss: 1.0201385021209717\n",
      "Epoch 345 / 500 | iteration 25 / 30 | Total Loss: 5.368207931518555 | KNN Loss: 4.31875467300415 | BCE Loss: 1.0494534969329834\n",
      "Epoch 346 / 500 | iteration 0 / 30 | Total Loss: 5.348219871520996 | KNN Loss: 4.321212291717529 | BCE Loss: 1.027007818222046\n",
      "Epoch 346 / 500 | iteration 5 / 30 | Total Loss: 5.377212047576904 | KNN Loss: 4.332123756408691 | BCE Loss: 1.0450881719589233\n",
      "Epoch 346 / 500 | iteration 10 / 30 | Total Loss: 5.362512588500977 | KNN Loss: 4.331180095672607 | BCE Loss: 1.03133225440979\n",
      "Epoch 346 / 500 | iteration 15 / 30 | Total Loss: 5.331284046173096 | KNN Loss: 4.318543434143066 | BCE Loss: 1.0127404928207397\n",
      "Epoch 346 / 500 | iteration 20 / 30 | Total Loss: 5.39262056350708 | KNN Loss: 4.367537021636963 | BCE Loss: 1.0250835418701172\n",
      "Epoch 346 / 500 | iteration 25 / 30 | Total Loss: 5.357022285461426 | KNN Loss: 4.331055164337158 | BCE Loss: 1.0259673595428467\n",
      "Epoch 347 / 500 | iteration 0 / 30 | Total Loss: 5.376071453094482 | KNN Loss: 4.327872276306152 | BCE Loss: 1.04819917678833\n",
      "Epoch 347 / 500 | iteration 5 / 30 | Total Loss: 5.3628058433532715 | KNN Loss: 4.3400559425354 | BCE Loss: 1.022749900817871\n",
      "Epoch 347 / 500 | iteration 10 / 30 | Total Loss: 5.387866973876953 | KNN Loss: 4.354140758514404 | BCE Loss: 1.0337259769439697\n",
      "Epoch 347 / 500 | iteration 15 / 30 | Total Loss: 5.354619026184082 | KNN Loss: 4.345620632171631 | BCE Loss: 1.008998155593872\n",
      "Epoch 347 / 500 | iteration 20 / 30 | Total Loss: 5.375211715698242 | KNN Loss: 4.350051403045654 | BCE Loss: 1.025160551071167\n",
      "Epoch 347 / 500 | iteration 25 / 30 | Total Loss: 5.341770648956299 | KNN Loss: 4.338451385498047 | BCE Loss: 1.003319263458252\n",
      "Epoch 348 / 500 | iteration 0 / 30 | Total Loss: 5.380801200866699 | KNN Loss: 4.3619866371154785 | BCE Loss: 1.0188148021697998\n",
      "Epoch 348 / 500 | iteration 5 / 30 | Total Loss: 5.370197296142578 | KNN Loss: 4.352170944213867 | BCE Loss: 1.0180261135101318\n",
      "Epoch 348 / 500 | iteration 10 / 30 | Total Loss: 5.381410598754883 | KNN Loss: 4.364897727966309 | BCE Loss: 1.0165126323699951\n",
      "Epoch 348 / 500 | iteration 15 / 30 | Total Loss: 5.368018627166748 | KNN Loss: 4.3432698249816895 | BCE Loss: 1.024748682975769\n",
      "Epoch 348 / 500 | iteration 20 / 30 | Total Loss: 5.394089698791504 | KNN Loss: 4.364145755767822 | BCE Loss: 1.0299441814422607\n",
      "Epoch 348 / 500 | iteration 25 / 30 | Total Loss: 5.419988632202148 | KNN Loss: 4.369194030761719 | BCE Loss: 1.0507944822311401\n",
      "Epoch 349 / 500 | iteration 0 / 30 | Total Loss: 5.369700908660889 | KNN Loss: 4.348579406738281 | BCE Loss: 1.0211215019226074\n",
      "Epoch 349 / 500 | iteration 5 / 30 | Total Loss: 5.374330520629883 | KNN Loss: 4.35675048828125 | BCE Loss: 1.0175797939300537\n",
      "Epoch 349 / 500 | iteration 10 / 30 | Total Loss: 5.386323928833008 | KNN Loss: 4.369387626647949 | BCE Loss: 1.0169363021850586\n",
      "Epoch 349 / 500 | iteration 15 / 30 | Total Loss: 5.362248420715332 | KNN Loss: 4.331105709075928 | BCE Loss: 1.0311424732208252\n",
      "Epoch 349 / 500 | iteration 20 / 30 | Total Loss: 5.425340175628662 | KNN Loss: 4.3949737548828125 | BCE Loss: 1.0303664207458496\n",
      "Epoch 349 / 500 | iteration 25 / 30 | Total Loss: 5.3532843589782715 | KNN Loss: 4.35493803024292 | BCE Loss: 0.9983463883399963\n",
      "Epoch 350 / 500 | iteration 0 / 30 | Total Loss: 5.399209976196289 | KNN Loss: 4.385263442993164 | BCE Loss: 1.013946294784546\n",
      "Epoch 350 / 500 | iteration 5 / 30 | Total Loss: 5.387186527252197 | KNN Loss: 4.355582237243652 | BCE Loss: 1.0316044092178345\n",
      "Epoch 350 / 500 | iteration 10 / 30 | Total Loss: 5.383164882659912 | KNN Loss: 4.363389015197754 | BCE Loss: 1.0197758674621582\n",
      "Epoch 350 / 500 | iteration 15 / 30 | Total Loss: 5.36820650100708 | KNN Loss: 4.330493927001953 | BCE Loss: 1.0377126932144165\n",
      "Epoch 350 / 500 | iteration 20 / 30 | Total Loss: 5.358791351318359 | KNN Loss: 4.353598594665527 | BCE Loss: 1.0051929950714111\n",
      "Epoch 350 / 500 | iteration 25 / 30 | Total Loss: 5.386690139770508 | KNN Loss: 4.381828308105469 | BCE Loss: 1.0048617124557495\n",
      "Epoch 351 / 500 | iteration 0 / 30 | Total Loss: 5.314205646514893 | KNN Loss: 4.329306602478027 | BCE Loss: 0.9848992228507996\n",
      "Epoch 351 / 500 | iteration 5 / 30 | Total Loss: 5.419626235961914 | KNN Loss: 4.409523010253906 | BCE Loss: 1.010103464126587\n",
      "Epoch 351 / 500 | iteration 10 / 30 | Total Loss: 5.459604263305664 | KNN Loss: 4.414829254150391 | BCE Loss: 1.044775128364563\n",
      "Epoch 351 / 500 | iteration 15 / 30 | Total Loss: 5.353933811187744 | KNN Loss: 4.342758655548096 | BCE Loss: 1.0111751556396484\n",
      "Epoch 351 / 500 | iteration 20 / 30 | Total Loss: 5.3813958168029785 | KNN Loss: 4.3514227867126465 | BCE Loss: 1.0299729108810425\n",
      "Epoch 351 / 500 | iteration 25 / 30 | Total Loss: 5.404817581176758 | KNN Loss: 4.367129325866699 | BCE Loss: 1.0376880168914795\n",
      "Epoch 352 / 500 | iteration 0 / 30 | Total Loss: 5.375786781311035 | KNN Loss: 4.355591773986816 | BCE Loss: 1.0201951265335083\n",
      "Epoch 352 / 500 | iteration 5 / 30 | Total Loss: 5.350984573364258 | KNN Loss: 4.3467206954956055 | BCE Loss: 1.0042641162872314\n",
      "Epoch 352 / 500 | iteration 10 / 30 | Total Loss: 5.394660472869873 | KNN Loss: 4.380847454071045 | BCE Loss: 1.0138128995895386\n",
      "Epoch 352 / 500 | iteration 15 / 30 | Total Loss: 5.383662223815918 | KNN Loss: 4.369022846221924 | BCE Loss: 1.014639139175415\n",
      "Epoch 352 / 500 | iteration 20 / 30 | Total Loss: 5.38542366027832 | KNN Loss: 4.338283061981201 | BCE Loss: 1.0471407175064087\n",
      "Epoch 352 / 500 | iteration 25 / 30 | Total Loss: 5.332505226135254 | KNN Loss: 4.3283843994140625 | BCE Loss: 1.0041208267211914\n",
      "Epoch 353 / 500 | iteration 0 / 30 | Total Loss: 5.368508815765381 | KNN Loss: 4.348977565765381 | BCE Loss: 1.0195311307907104\n",
      "Epoch 353 / 500 | iteration 5 / 30 | Total Loss: 5.3566694259643555 | KNN Loss: 4.317057132720947 | BCE Loss: 1.0396122932434082\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 353 / 500 | iteration 10 / 30 | Total Loss: 5.3717942237854 | KNN Loss: 4.3435750007629395 | BCE Loss: 1.028219223022461\n",
      "Epoch 353 / 500 | iteration 15 / 30 | Total Loss: 5.387276649475098 | KNN Loss: 4.355654239654541 | BCE Loss: 1.0316225290298462\n",
      "Epoch 353 / 500 | iteration 20 / 30 | Total Loss: 5.374594688415527 | KNN Loss: 4.328200340270996 | BCE Loss: 1.0463945865631104\n",
      "Epoch 353 / 500 | iteration 25 / 30 | Total Loss: 5.344537734985352 | KNN Loss: 4.313028335571289 | BCE Loss: 1.0315093994140625\n",
      "Epoch 354 / 500 | iteration 0 / 30 | Total Loss: 5.407098293304443 | KNN Loss: 4.381347179412842 | BCE Loss: 1.0257512331008911\n",
      "Epoch 354 / 500 | iteration 5 / 30 | Total Loss: 5.389979362487793 | KNN Loss: 4.335143566131592 | BCE Loss: 1.0548357963562012\n",
      "Epoch 354 / 500 | iteration 10 / 30 | Total Loss: 5.4219841957092285 | KNN Loss: 4.376189231872559 | BCE Loss: 1.04579496383667\n",
      "Epoch 354 / 500 | iteration 15 / 30 | Total Loss: 5.379724025726318 | KNN Loss: 4.372725963592529 | BCE Loss: 1.006998062133789\n",
      "Epoch 354 / 500 | iteration 20 / 30 | Total Loss: 5.366891384124756 | KNN Loss: 4.346275806427002 | BCE Loss: 1.020615577697754\n",
      "Epoch 354 / 500 | iteration 25 / 30 | Total Loss: 5.381387710571289 | KNN Loss: 4.341817855834961 | BCE Loss: 1.0395698547363281\n",
      "Epoch 355 / 500 | iteration 0 / 30 | Total Loss: 5.36772346496582 | KNN Loss: 4.34928560256958 | BCE Loss: 1.0184378623962402\n",
      "Epoch 355 / 500 | iteration 5 / 30 | Total Loss: 5.427089691162109 | KNN Loss: 4.392804145812988 | BCE Loss: 1.0342857837677002\n",
      "Epoch 355 / 500 | iteration 10 / 30 | Total Loss: 5.400080680847168 | KNN Loss: 4.3647284507751465 | BCE Loss: 1.035352110862732\n",
      "Epoch 355 / 500 | iteration 15 / 30 | Total Loss: 5.353009223937988 | KNN Loss: 4.3499979972839355 | BCE Loss: 1.0030111074447632\n",
      "Epoch 355 / 500 | iteration 20 / 30 | Total Loss: 5.438844680786133 | KNN Loss: 4.385084629058838 | BCE Loss: 1.053760051727295\n",
      "Epoch 355 / 500 | iteration 25 / 30 | Total Loss: 5.33128547668457 | KNN Loss: 4.322388648986816 | BCE Loss: 1.0088967084884644\n",
      "Epoch   356: reducing learning rate of group 0 to 3.9896e-06.\n",
      "Epoch 356 / 500 | iteration 0 / 30 | Total Loss: 5.400036811828613 | KNN Loss: 4.383490085601807 | BCE Loss: 1.0165469646453857\n",
      "Epoch 356 / 500 | iteration 5 / 30 | Total Loss: 5.361990928649902 | KNN Loss: 4.343539714813232 | BCE Loss: 1.0184509754180908\n",
      "Epoch 356 / 500 | iteration 10 / 30 | Total Loss: 5.384840965270996 | KNN Loss: 4.333939075469971 | BCE Loss: 1.050902009010315\n",
      "Epoch 356 / 500 | iteration 15 / 30 | Total Loss: 5.342879295349121 | KNN Loss: 4.316298484802246 | BCE Loss: 1.0265809297561646\n",
      "Epoch 356 / 500 | iteration 20 / 30 | Total Loss: 5.366044521331787 | KNN Loss: 4.350528240203857 | BCE Loss: 1.0155161619186401\n",
      "Epoch 356 / 500 | iteration 25 / 30 | Total Loss: 5.382293224334717 | KNN Loss: 4.355989456176758 | BCE Loss: 1.0263036489486694\n",
      "Epoch 357 / 500 | iteration 0 / 30 | Total Loss: 5.349359512329102 | KNN Loss: 4.32459831237793 | BCE Loss: 1.0247609615325928\n",
      "Epoch 357 / 500 | iteration 5 / 30 | Total Loss: 5.427422046661377 | KNN Loss: 4.407705783843994 | BCE Loss: 1.0197162628173828\n",
      "Epoch 357 / 500 | iteration 10 / 30 | Total Loss: 5.409947872161865 | KNN Loss: 4.375820159912109 | BCE Loss: 1.0341278314590454\n",
      "Epoch 357 / 500 | iteration 15 / 30 | Total Loss: 5.362054824829102 | KNN Loss: 4.339526653289795 | BCE Loss: 1.0225281715393066\n",
      "Epoch 357 / 500 | iteration 20 / 30 | Total Loss: 5.369393825531006 | KNN Loss: 4.356154918670654 | BCE Loss: 1.0132389068603516\n",
      "Epoch 357 / 500 | iteration 25 / 30 | Total Loss: 5.408992290496826 | KNN Loss: 4.363197326660156 | BCE Loss: 1.0457948446273804\n",
      "Epoch 358 / 500 | iteration 0 / 30 | Total Loss: 5.404062271118164 | KNN Loss: 4.376180171966553 | BCE Loss: 1.0278823375701904\n",
      "Epoch 358 / 500 | iteration 5 / 30 | Total Loss: 5.363880157470703 | KNN Loss: 4.314043998718262 | BCE Loss: 1.0498361587524414\n",
      "Epoch 358 / 500 | iteration 10 / 30 | Total Loss: 5.407238960266113 | KNN Loss: 4.3558831214904785 | BCE Loss: 1.0513557195663452\n",
      "Epoch 358 / 500 | iteration 15 / 30 | Total Loss: 5.429054260253906 | KNN Loss: 4.40659236907959 | BCE Loss: 1.0224618911743164\n",
      "Epoch 358 / 500 | iteration 20 / 30 | Total Loss: 5.412534236907959 | KNN Loss: 4.393332481384277 | BCE Loss: 1.0192017555236816\n",
      "Epoch 358 / 500 | iteration 25 / 30 | Total Loss: 5.414791107177734 | KNN Loss: 4.367230415344238 | BCE Loss: 1.047560453414917\n",
      "Epoch 359 / 500 | iteration 0 / 30 | Total Loss: 5.346135139465332 | KNN Loss: 4.341195106506348 | BCE Loss: 1.0049399137496948\n",
      "Epoch 359 / 500 | iteration 5 / 30 | Total Loss: 5.407623291015625 | KNN Loss: 4.377171039581299 | BCE Loss: 1.030452013015747\n",
      "Epoch 359 / 500 | iteration 10 / 30 | Total Loss: 5.39443302154541 | KNN Loss: 4.362390041351318 | BCE Loss: 1.0320427417755127\n",
      "Epoch 359 / 500 | iteration 15 / 30 | Total Loss: 5.385620594024658 | KNN Loss: 4.369277477264404 | BCE Loss: 1.016343116760254\n",
      "Epoch 359 / 500 | iteration 20 / 30 | Total Loss: 5.355138778686523 | KNN Loss: 4.328097343444824 | BCE Loss: 1.0270416736602783\n",
      "Epoch 359 / 500 | iteration 25 / 30 | Total Loss: 5.384568214416504 | KNN Loss: 4.373144149780273 | BCE Loss: 1.011423945426941\n",
      "Epoch 360 / 500 | iteration 0 / 30 | Total Loss: 5.332056999206543 | KNN Loss: 4.334896087646484 | BCE Loss: 0.9971607327461243\n",
      "Epoch 360 / 500 | iteration 5 / 30 | Total Loss: 5.399187088012695 | KNN Loss: 4.3658647537231445 | BCE Loss: 1.0333223342895508\n",
      "Epoch 360 / 500 | iteration 10 / 30 | Total Loss: 5.3955397605896 | KNN Loss: 4.343132972717285 | BCE Loss: 1.0524067878723145\n",
      "Epoch 360 / 500 | iteration 15 / 30 | Total Loss: 5.420341968536377 | KNN Loss: 4.371982574462891 | BCE Loss: 1.0483593940734863\n",
      "Epoch 360 / 500 | iteration 20 / 30 | Total Loss: 5.397150993347168 | KNN Loss: 4.360246181488037 | BCE Loss: 1.0369045734405518\n",
      "Epoch 360 / 500 | iteration 25 / 30 | Total Loss: 5.370415687561035 | KNN Loss: 4.337518215179443 | BCE Loss: 1.0328972339630127\n",
      "Epoch 361 / 500 | iteration 0 / 30 | Total Loss: 5.340603828430176 | KNN Loss: 4.343276023864746 | BCE Loss: 0.9973275661468506\n",
      "Epoch 361 / 500 | iteration 5 / 30 | Total Loss: 5.385115623474121 | KNN Loss: 4.353760242462158 | BCE Loss: 1.031355619430542\n",
      "Epoch 361 / 500 | iteration 10 / 30 | Total Loss: 5.3919854164123535 | KNN Loss: 4.354526042938232 | BCE Loss: 1.0374592542648315\n",
      "Epoch 361 / 500 | iteration 15 / 30 | Total Loss: 5.396097660064697 | KNN Loss: 4.357686996459961 | BCE Loss: 1.0384106636047363\n",
      "Epoch 361 / 500 | iteration 20 / 30 | Total Loss: 5.371129035949707 | KNN Loss: 4.342929363250732 | BCE Loss: 1.0281999111175537\n",
      "Epoch 361 / 500 | iteration 25 / 30 | Total Loss: 5.354406356811523 | KNN Loss: 4.320282936096191 | BCE Loss: 1.0341236591339111\n",
      "Epoch 362 / 500 | iteration 0 / 30 | Total Loss: 5.407122611999512 | KNN Loss: 4.364746570587158 | BCE Loss: 1.042376160621643\n",
      "Epoch 362 / 500 | iteration 5 / 30 | Total Loss: 5.351450443267822 | KNN Loss: 4.3218464851379395 | BCE Loss: 1.0296039581298828\n",
      "Epoch 362 / 500 | iteration 10 / 30 | Total Loss: 5.400331497192383 | KNN Loss: 4.391815662384033 | BCE Loss: 1.0085159540176392\n",
      "Epoch 362 / 500 | iteration 15 / 30 | Total Loss: 5.3509416580200195 | KNN Loss: 4.352644443511963 | BCE Loss: 0.9982969760894775\n",
      "Epoch 362 / 500 | iteration 20 / 30 | Total Loss: 5.348443508148193 | KNN Loss: 4.354848384857178 | BCE Loss: 0.9935950636863708\n",
      "Epoch 362 / 500 | iteration 25 / 30 | Total Loss: 5.468665599822998 | KNN Loss: 4.420341491699219 | BCE Loss: 1.0483242273330688\n",
      "Epoch 363 / 500 | iteration 0 / 30 | Total Loss: 5.3540802001953125 | KNN Loss: 4.339211463928223 | BCE Loss: 1.0148687362670898\n",
      "Epoch 363 / 500 | iteration 5 / 30 | Total Loss: 5.3041863441467285 | KNN Loss: 4.319927215576172 | BCE Loss: 0.9842591881752014\n",
      "Epoch 363 / 500 | iteration 10 / 30 | Total Loss: 5.380925178527832 | KNN Loss: 4.354126930236816 | BCE Loss: 1.0267980098724365\n",
      "Epoch 363 / 500 | iteration 15 / 30 | Total Loss: 5.376227855682373 | KNN Loss: 4.327949047088623 | BCE Loss: 1.04827880859375\n",
      "Epoch 363 / 500 | iteration 20 / 30 | Total Loss: 5.395369529724121 | KNN Loss: 4.379123210906982 | BCE Loss: 1.0162460803985596\n",
      "Epoch 363 / 500 | iteration 25 / 30 | Total Loss: 5.405770301818848 | KNN Loss: 4.365780353546143 | BCE Loss: 1.039989948272705\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 364 / 500 | iteration 0 / 30 | Total Loss: 5.372774124145508 | KNN Loss: 4.351986885070801 | BCE Loss: 1.0207874774932861\n",
      "Epoch 364 / 500 | iteration 5 / 30 | Total Loss: 5.461465835571289 | KNN Loss: 4.410369873046875 | BCE Loss: 1.0510962009429932\n",
      "Epoch 364 / 500 | iteration 10 / 30 | Total Loss: 5.372284889221191 | KNN Loss: 4.3514723777771 | BCE Loss: 1.020812749862671\n",
      "Epoch 364 / 500 | iteration 15 / 30 | Total Loss: 5.372580528259277 | KNN Loss: 4.3545823097229 | BCE Loss: 1.0179980993270874\n",
      "Epoch 364 / 500 | iteration 20 / 30 | Total Loss: 5.368582248687744 | KNN Loss: 4.327709674835205 | BCE Loss: 1.040872573852539\n",
      "Epoch 364 / 500 | iteration 25 / 30 | Total Loss: 5.31877326965332 | KNN Loss: 4.322635173797607 | BCE Loss: 0.996138334274292\n",
      "Epoch 365 / 500 | iteration 0 / 30 | Total Loss: 5.4661865234375 | KNN Loss: 4.416163921356201 | BCE Loss: 1.0500223636627197\n",
      "Epoch 365 / 500 | iteration 5 / 30 | Total Loss: 5.442360877990723 | KNN Loss: 4.391425132751465 | BCE Loss: 1.050935983657837\n",
      "Epoch 365 / 500 | iteration 10 / 30 | Total Loss: 5.352675437927246 | KNN Loss: 4.327789306640625 | BCE Loss: 1.0248860120773315\n",
      "Epoch 365 / 500 | iteration 15 / 30 | Total Loss: 5.339865684509277 | KNN Loss: 4.326903343200684 | BCE Loss: 1.0129621028900146\n",
      "Epoch 365 / 500 | iteration 20 / 30 | Total Loss: 5.408537864685059 | KNN Loss: 4.368483543395996 | BCE Loss: 1.0400545597076416\n",
      "Epoch 365 / 500 | iteration 25 / 30 | Total Loss: 5.396767616271973 | KNN Loss: 4.372943878173828 | BCE Loss: 1.0238239765167236\n",
      "Epoch 366 / 500 | iteration 0 / 30 | Total Loss: 5.3592939376831055 | KNN Loss: 4.330106735229492 | BCE Loss: 1.0291874408721924\n",
      "Epoch 366 / 500 | iteration 5 / 30 | Total Loss: 5.346651554107666 | KNN Loss: 4.3647074699401855 | BCE Loss: 0.9819441437721252\n",
      "Epoch 366 / 500 | iteration 10 / 30 | Total Loss: 5.422102928161621 | KNN Loss: 4.359942436218262 | BCE Loss: 1.0621602535247803\n",
      "Epoch 366 / 500 | iteration 15 / 30 | Total Loss: 5.370980739593506 | KNN Loss: 4.3446245193481445 | BCE Loss: 1.0263562202453613\n",
      "Epoch 366 / 500 | iteration 20 / 30 | Total Loss: 5.368986129760742 | KNN Loss: 4.344690322875977 | BCE Loss: 1.0242958068847656\n",
      "Epoch 366 / 500 | iteration 25 / 30 | Total Loss: 5.382675647735596 | KNN Loss: 4.328925132751465 | BCE Loss: 1.0537505149841309\n",
      "Epoch   367: reducing learning rate of group 0 to 2.7927e-06.\n",
      "Epoch 367 / 500 | iteration 0 / 30 | Total Loss: 5.357151985168457 | KNN Loss: 4.353857517242432 | BCE Loss: 1.0032944679260254\n",
      "Epoch 367 / 500 | iteration 5 / 30 | Total Loss: 5.340620517730713 | KNN Loss: 4.329900741577148 | BCE Loss: 1.010719895362854\n",
      "Epoch 367 / 500 | iteration 10 / 30 | Total Loss: 5.460408687591553 | KNN Loss: 4.395825386047363 | BCE Loss: 1.0645831823349\n",
      "Epoch 367 / 500 | iteration 15 / 30 | Total Loss: 5.385451793670654 | KNN Loss: 4.336613178253174 | BCE Loss: 1.0488386154174805\n",
      "Epoch 367 / 500 | iteration 20 / 30 | Total Loss: 5.3489089012146 | KNN Loss: 4.328319072723389 | BCE Loss: 1.0205897092819214\n",
      "Epoch 367 / 500 | iteration 25 / 30 | Total Loss: 5.393810749053955 | KNN Loss: 4.402968883514404 | BCE Loss: 0.9908420443534851\n",
      "Epoch 368 / 500 | iteration 0 / 30 | Total Loss: 5.386518955230713 | KNN Loss: 4.347894668579102 | BCE Loss: 1.0386241674423218\n",
      "Epoch 368 / 500 | iteration 5 / 30 | Total Loss: 5.372110366821289 | KNN Loss: 4.366612911224365 | BCE Loss: 1.005497694015503\n",
      "Epoch 368 / 500 | iteration 10 / 30 | Total Loss: 5.360548496246338 | KNN Loss: 4.35060977935791 | BCE Loss: 1.0099387168884277\n",
      "Epoch 368 / 500 | iteration 15 / 30 | Total Loss: 5.368860244750977 | KNN Loss: 4.322310924530029 | BCE Loss: 1.0465490818023682\n",
      "Epoch 368 / 500 | iteration 20 / 30 | Total Loss: 5.415876388549805 | KNN Loss: 4.380277156829834 | BCE Loss: 1.0355994701385498\n",
      "Epoch 368 / 500 | iteration 25 / 30 | Total Loss: 5.35176944732666 | KNN Loss: 4.325745105743408 | BCE Loss: 1.026024341583252\n",
      "Epoch 369 / 500 | iteration 0 / 30 | Total Loss: 5.454326152801514 | KNN Loss: 4.420321941375732 | BCE Loss: 1.0340040922164917\n",
      "Epoch 369 / 500 | iteration 5 / 30 | Total Loss: 5.411374568939209 | KNN Loss: 4.341902732849121 | BCE Loss: 1.0694719552993774\n",
      "Epoch 369 / 500 | iteration 10 / 30 | Total Loss: 5.3839497566223145 | KNN Loss: 4.354280948638916 | BCE Loss: 1.0296688079833984\n",
      "Epoch 369 / 500 | iteration 15 / 30 | Total Loss: 5.333381175994873 | KNN Loss: 4.308213233947754 | BCE Loss: 1.0251680612564087\n",
      "Epoch 369 / 500 | iteration 20 / 30 | Total Loss: 5.364676475524902 | KNN Loss: 4.354722499847412 | BCE Loss: 1.0099538564682007\n",
      "Epoch 369 / 500 | iteration 25 / 30 | Total Loss: 5.419328212738037 | KNN Loss: 4.364351749420166 | BCE Loss: 1.054976463317871\n",
      "Epoch 370 / 500 | iteration 0 / 30 | Total Loss: 5.376444339752197 | KNN Loss: 4.373830795288086 | BCE Loss: 1.0026134252548218\n",
      "Epoch 370 / 500 | iteration 5 / 30 | Total Loss: 5.374911308288574 | KNN Loss: 4.345363616943359 | BCE Loss: 1.029547929763794\n",
      "Epoch 370 / 500 | iteration 10 / 30 | Total Loss: 5.4066691398620605 | KNN Loss: 4.383464813232422 | BCE Loss: 1.0232043266296387\n",
      "Epoch 370 / 500 | iteration 15 / 30 | Total Loss: 5.3574323654174805 | KNN Loss: 4.316892147064209 | BCE Loss: 1.0405399799346924\n",
      "Epoch 370 / 500 | iteration 20 / 30 | Total Loss: 5.430144786834717 | KNN Loss: 4.376835346221924 | BCE Loss: 1.053309440612793\n",
      "Epoch 370 / 500 | iteration 25 / 30 | Total Loss: 5.379436016082764 | KNN Loss: 4.356240749359131 | BCE Loss: 1.0231952667236328\n",
      "Epoch 371 / 500 | iteration 0 / 30 | Total Loss: 5.324930667877197 | KNN Loss: 4.3173604011535645 | BCE Loss: 1.0075702667236328\n",
      "Epoch 371 / 500 | iteration 5 / 30 | Total Loss: 5.362638473510742 | KNN Loss: 4.331627368927002 | BCE Loss: 1.0310111045837402\n",
      "Epoch 371 / 500 | iteration 10 / 30 | Total Loss: 5.42924165725708 | KNN Loss: 4.408990383148193 | BCE Loss: 1.0202512741088867\n",
      "Epoch 371 / 500 | iteration 15 / 30 | Total Loss: 5.376278877258301 | KNN Loss: 4.340226650238037 | BCE Loss: 1.0360522270202637\n",
      "Epoch 371 / 500 | iteration 20 / 30 | Total Loss: 5.42962646484375 | KNN Loss: 4.419948577880859 | BCE Loss: 1.0096781253814697\n",
      "Epoch 371 / 500 | iteration 25 / 30 | Total Loss: 5.359747886657715 | KNN Loss: 4.315815448760986 | BCE Loss: 1.0439326763153076\n",
      "Epoch 372 / 500 | iteration 0 / 30 | Total Loss: 5.354658126831055 | KNN Loss: 4.350912570953369 | BCE Loss: 1.0037455558776855\n",
      "Epoch 372 / 500 | iteration 5 / 30 | Total Loss: 5.402126789093018 | KNN Loss: 4.385573863983154 | BCE Loss: 1.0165529251098633\n",
      "Epoch 372 / 500 | iteration 10 / 30 | Total Loss: 5.392292022705078 | KNN Loss: 4.365976810455322 | BCE Loss: 1.0263152122497559\n",
      "Epoch 372 / 500 | iteration 15 / 30 | Total Loss: 5.396246433258057 | KNN Loss: 4.329243183135986 | BCE Loss: 1.0670032501220703\n",
      "Epoch 372 / 500 | iteration 20 / 30 | Total Loss: 5.392709255218506 | KNN Loss: 4.367806911468506 | BCE Loss: 1.02490234375\n",
      "Epoch 372 / 500 | iteration 25 / 30 | Total Loss: 5.396340370178223 | KNN Loss: 4.359842300415039 | BCE Loss: 1.0364978313446045\n",
      "Epoch 373 / 500 | iteration 0 / 30 | Total Loss: 5.386700630187988 | KNN Loss: 4.342768669128418 | BCE Loss: 1.0439320802688599\n",
      "Epoch 373 / 500 | iteration 5 / 30 | Total Loss: 5.403819561004639 | KNN Loss: 4.344773292541504 | BCE Loss: 1.0590461492538452\n",
      "Epoch 373 / 500 | iteration 10 / 30 | Total Loss: 5.387933254241943 | KNN Loss: 4.324866771697998 | BCE Loss: 1.0630663633346558\n",
      "Epoch 373 / 500 | iteration 15 / 30 | Total Loss: 5.378285884857178 | KNN Loss: 4.375273704528809 | BCE Loss: 1.0030121803283691\n",
      "Epoch 373 / 500 | iteration 20 / 30 | Total Loss: 5.3489179611206055 | KNN Loss: 4.350427627563477 | BCE Loss: 0.9984902739524841\n",
      "Epoch 373 / 500 | iteration 25 / 30 | Total Loss: 5.379270553588867 | KNN Loss: 4.371073246002197 | BCE Loss: 1.0081970691680908\n",
      "Epoch 374 / 500 | iteration 0 / 30 | Total Loss: 5.377894401550293 | KNN Loss: 4.360644340515137 | BCE Loss: 1.0172498226165771\n",
      "Epoch 374 / 500 | iteration 5 / 30 | Total Loss: 5.389827251434326 | KNN Loss: 4.362930774688721 | BCE Loss: 1.0268964767456055\n",
      "Epoch 374 / 500 | iteration 10 / 30 | Total Loss: 5.311488151550293 | KNN Loss: 4.338866710662842 | BCE Loss: 0.9726215600967407\n",
      "Epoch 374 / 500 | iteration 15 / 30 | Total Loss: 5.428043365478516 | KNN Loss: 4.382027626037598 | BCE Loss: 1.046015977859497\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 374 / 500 | iteration 20 / 30 | Total Loss: 5.353998184204102 | KNN Loss: 4.3004374504089355 | BCE Loss: 1.053560495376587\n",
      "Epoch 374 / 500 | iteration 25 / 30 | Total Loss: 5.361170768737793 | KNN Loss: 4.346921920776367 | BCE Loss: 1.0142486095428467\n",
      "Epoch 375 / 500 | iteration 0 / 30 | Total Loss: 5.425289154052734 | KNN Loss: 4.3696112632751465 | BCE Loss: 1.055677890777588\n",
      "Epoch 375 / 500 | iteration 5 / 30 | Total Loss: 5.354892730712891 | KNN Loss: 4.347204685211182 | BCE Loss: 1.0076879262924194\n",
      "Epoch 375 / 500 | iteration 10 / 30 | Total Loss: 5.319975852966309 | KNN Loss: 4.3120598793029785 | BCE Loss: 1.0079158544540405\n",
      "Epoch 375 / 500 | iteration 15 / 30 | Total Loss: 5.350371837615967 | KNN Loss: 4.323099136352539 | BCE Loss: 1.0272728204727173\n",
      "Epoch 375 / 500 | iteration 20 / 30 | Total Loss: 5.418903827667236 | KNN Loss: 4.399919509887695 | BCE Loss: 1.018984317779541\n",
      "Epoch 375 / 500 | iteration 25 / 30 | Total Loss: 5.390944004058838 | KNN Loss: 4.364144325256348 | BCE Loss: 1.0267997980117798\n",
      "Epoch 376 / 500 | iteration 0 / 30 | Total Loss: 5.397905349731445 | KNN Loss: 4.368358135223389 | BCE Loss: 1.0295469760894775\n",
      "Epoch 376 / 500 | iteration 5 / 30 | Total Loss: 5.348231315612793 | KNN Loss: 4.3240556716918945 | BCE Loss: 1.0241754055023193\n",
      "Epoch 376 / 500 | iteration 10 / 30 | Total Loss: 5.402303218841553 | KNN Loss: 4.3662495613098145 | BCE Loss: 1.0360536575317383\n",
      "Epoch 376 / 500 | iteration 15 / 30 | Total Loss: 5.372743606567383 | KNN Loss: 4.354681015014648 | BCE Loss: 1.0180624723434448\n",
      "Epoch 376 / 500 | iteration 20 / 30 | Total Loss: 5.407383441925049 | KNN Loss: 4.390103340148926 | BCE Loss: 1.017280101776123\n",
      "Epoch 376 / 500 | iteration 25 / 30 | Total Loss: 5.391791343688965 | KNN Loss: 4.36787748336792 | BCE Loss: 1.023914098739624\n",
      "Epoch 377 / 500 | iteration 0 / 30 | Total Loss: 5.347249984741211 | KNN Loss: 4.334798336029053 | BCE Loss: 1.0124518871307373\n",
      "Epoch 377 / 500 | iteration 5 / 30 | Total Loss: 5.393973350524902 | KNN Loss: 4.3412017822265625 | BCE Loss: 1.052771806716919\n",
      "Epoch 377 / 500 | iteration 10 / 30 | Total Loss: 5.330361843109131 | KNN Loss: 4.320181369781494 | BCE Loss: 1.0101805925369263\n",
      "Epoch 377 / 500 | iteration 15 / 30 | Total Loss: 5.393237590789795 | KNN Loss: 4.329446792602539 | BCE Loss: 1.0637909173965454\n",
      "Epoch 377 / 500 | iteration 20 / 30 | Total Loss: 5.375183582305908 | KNN Loss: 4.337055206298828 | BCE Loss: 1.03812837600708\n",
      "Epoch 377 / 500 | iteration 25 / 30 | Total Loss: 5.360492706298828 | KNN Loss: 4.314291477203369 | BCE Loss: 1.046201467514038\n",
      "Epoch   378: reducing learning rate of group 0 to 1.9549e-06.\n",
      "Epoch 378 / 500 | iteration 0 / 30 | Total Loss: 5.3737874031066895 | KNN Loss: 4.356210708618164 | BCE Loss: 1.0175766944885254\n",
      "Epoch 378 / 500 | iteration 5 / 30 | Total Loss: 5.406258583068848 | KNN Loss: 4.399434566497803 | BCE Loss: 1.0068237781524658\n",
      "Epoch 378 / 500 | iteration 10 / 30 | Total Loss: 5.433722496032715 | KNN Loss: 4.400513648986816 | BCE Loss: 1.033208966255188\n",
      "Epoch 378 / 500 | iteration 15 / 30 | Total Loss: 5.403359413146973 | KNN Loss: 4.384646892547607 | BCE Loss: 1.0187122821807861\n",
      "Epoch 378 / 500 | iteration 20 / 30 | Total Loss: 5.3515095710754395 | KNN Loss: 4.325839996337891 | BCE Loss: 1.0256695747375488\n",
      "Epoch 378 / 500 | iteration 25 / 30 | Total Loss: 5.4043169021606445 | KNN Loss: 4.368814945220947 | BCE Loss: 1.0355019569396973\n",
      "Epoch 379 / 500 | iteration 0 / 30 | Total Loss: 5.398625373840332 | KNN Loss: 4.350396156311035 | BCE Loss: 1.048229455947876\n",
      "Epoch 379 / 500 | iteration 5 / 30 | Total Loss: 5.451869487762451 | KNN Loss: 4.417956352233887 | BCE Loss: 1.033913254737854\n",
      "Epoch 379 / 500 | iteration 10 / 30 | Total Loss: 5.387433052062988 | KNN Loss: 4.338112831115723 | BCE Loss: 1.0493202209472656\n",
      "Epoch 379 / 500 | iteration 15 / 30 | Total Loss: 5.353120803833008 | KNN Loss: 4.319930076599121 | BCE Loss: 1.0331909656524658\n",
      "Epoch 379 / 500 | iteration 20 / 30 | Total Loss: 5.417218208312988 | KNN Loss: 4.380311489105225 | BCE Loss: 1.0369067192077637\n",
      "Epoch 379 / 500 | iteration 25 / 30 | Total Loss: 5.354269981384277 | KNN Loss: 4.360560417175293 | BCE Loss: 0.9937096238136292\n",
      "Epoch 380 / 500 | iteration 0 / 30 | Total Loss: 5.317452430725098 | KNN Loss: 4.328979015350342 | BCE Loss: 0.9884735345840454\n",
      "Epoch 380 / 500 | iteration 5 / 30 | Total Loss: 5.390711784362793 | KNN Loss: 4.371661186218262 | BCE Loss: 1.0190508365631104\n",
      "Epoch 380 / 500 | iteration 10 / 30 | Total Loss: 5.380645751953125 | KNN Loss: 4.35938024520874 | BCE Loss: 1.0212655067443848\n",
      "Epoch 380 / 500 | iteration 15 / 30 | Total Loss: 5.363755226135254 | KNN Loss: 4.360261917114258 | BCE Loss: 1.003493070602417\n",
      "Epoch 380 / 500 | iteration 20 / 30 | Total Loss: 5.425442695617676 | KNN Loss: 4.394562721252441 | BCE Loss: 1.0308802127838135\n",
      "Epoch 380 / 500 | iteration 25 / 30 | Total Loss: 5.3595147132873535 | KNN Loss: 4.345561504364014 | BCE Loss: 1.0139530897140503\n",
      "Epoch 381 / 500 | iteration 0 / 30 | Total Loss: 5.339494228363037 | KNN Loss: 4.326517105102539 | BCE Loss: 1.0129770040512085\n",
      "Epoch 381 / 500 | iteration 5 / 30 | Total Loss: 5.399672031402588 | KNN Loss: 4.346673965454102 | BCE Loss: 1.0529980659484863\n",
      "Epoch 381 / 500 | iteration 10 / 30 | Total Loss: 5.392918586730957 | KNN Loss: 4.345429420471191 | BCE Loss: 1.0474889278411865\n",
      "Epoch 381 / 500 | iteration 15 / 30 | Total Loss: 5.3600263595581055 | KNN Loss: 4.372855186462402 | BCE Loss: 0.9871711134910583\n",
      "Epoch 381 / 500 | iteration 20 / 30 | Total Loss: 5.374034404754639 | KNN Loss: 4.342187881469727 | BCE Loss: 1.0318464040756226\n",
      "Epoch 381 / 500 | iteration 25 / 30 | Total Loss: 5.413334846496582 | KNN Loss: 4.389380931854248 | BCE Loss: 1.0239536762237549\n",
      "Epoch 382 / 500 | iteration 0 / 30 | Total Loss: 5.363581657409668 | KNN Loss: 4.347198963165283 | BCE Loss: 1.0163824558258057\n",
      "Epoch 382 / 500 | iteration 5 / 30 | Total Loss: 5.34799861907959 | KNN Loss: 4.346345901489258 | BCE Loss: 1.001652479171753\n",
      "Epoch 382 / 500 | iteration 10 / 30 | Total Loss: 5.385573387145996 | KNN Loss: 4.323400020599365 | BCE Loss: 1.06217360496521\n",
      "Epoch 382 / 500 | iteration 15 / 30 | Total Loss: 5.365108013153076 | KNN Loss: 4.336392879486084 | BCE Loss: 1.0287150144577026\n",
      "Epoch 382 / 500 | iteration 20 / 30 | Total Loss: 5.395920276641846 | KNN Loss: 4.364992618560791 | BCE Loss: 1.0309276580810547\n",
      "Epoch 382 / 500 | iteration 25 / 30 | Total Loss: 5.415264129638672 | KNN Loss: 4.4017815589904785 | BCE Loss: 1.013482689857483\n",
      "Epoch 383 / 500 | iteration 0 / 30 | Total Loss: 5.402169227600098 | KNN Loss: 4.389363765716553 | BCE Loss: 1.0128053426742554\n",
      "Epoch 383 / 500 | iteration 5 / 30 | Total Loss: 5.407713890075684 | KNN Loss: 4.396914958953857 | BCE Loss: 1.0107989311218262\n",
      "Epoch 383 / 500 | iteration 10 / 30 | Total Loss: 5.420699119567871 | KNN Loss: 4.382031440734863 | BCE Loss: 1.0386674404144287\n",
      "Epoch 383 / 500 | iteration 15 / 30 | Total Loss: 5.348113059997559 | KNN Loss: 4.333831310272217 | BCE Loss: 1.0142817497253418\n",
      "Epoch 383 / 500 | iteration 20 / 30 | Total Loss: 5.366016387939453 | KNN Loss: 4.348269462585449 | BCE Loss: 1.017746925354004\n",
      "Epoch 383 / 500 | iteration 25 / 30 | Total Loss: 5.397976398468018 | KNN Loss: 4.376266956329346 | BCE Loss: 1.0217094421386719\n",
      "Epoch 384 / 500 | iteration 0 / 30 | Total Loss: 5.469181060791016 | KNN Loss: 4.442479610443115 | BCE Loss: 1.02670156955719\n",
      "Epoch 384 / 500 | iteration 5 / 30 | Total Loss: 5.326740264892578 | KNN Loss: 4.3335723876953125 | BCE Loss: 0.9931678175926208\n",
      "Epoch 384 / 500 | iteration 10 / 30 | Total Loss: 5.399558067321777 | KNN Loss: 4.352496147155762 | BCE Loss: 1.0470620393753052\n",
      "Epoch 384 / 500 | iteration 15 / 30 | Total Loss: 5.453551769256592 | KNN Loss: 4.405296325683594 | BCE Loss: 1.048255443572998\n",
      "Epoch 384 / 500 | iteration 20 / 30 | Total Loss: 5.375079154968262 | KNN Loss: 4.357996463775635 | BCE Loss: 1.017082929611206\n",
      "Epoch 384 / 500 | iteration 25 / 30 | Total Loss: 5.377658843994141 | KNN Loss: 4.350364685058594 | BCE Loss: 1.0272940397262573\n",
      "Epoch 385 / 500 | iteration 0 / 30 | Total Loss: 5.364258766174316 | KNN Loss: 4.362991809844971 | BCE Loss: 1.0012671947479248\n",
      "Epoch 385 / 500 | iteration 5 / 30 | Total Loss: 5.348291397094727 | KNN Loss: 4.337709426879883 | BCE Loss: 1.0105817317962646\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 385 / 500 | iteration 10 / 30 | Total Loss: 5.3813276290893555 | KNN Loss: 4.34381103515625 | BCE Loss: 1.0375168323516846\n",
      "Epoch 385 / 500 | iteration 15 / 30 | Total Loss: 5.357524871826172 | KNN Loss: 4.324836254119873 | BCE Loss: 1.032688856124878\n",
      "Epoch 385 / 500 | iteration 20 / 30 | Total Loss: 5.404170989990234 | KNN Loss: 4.361038684844971 | BCE Loss: 1.0431320667266846\n",
      "Epoch 385 / 500 | iteration 25 / 30 | Total Loss: 5.346055030822754 | KNN Loss: 4.334372520446777 | BCE Loss: 1.0116827487945557\n",
      "Epoch 386 / 500 | iteration 0 / 30 | Total Loss: 5.354611396789551 | KNN Loss: 4.330931186676025 | BCE Loss: 1.0236800909042358\n",
      "Epoch 386 / 500 | iteration 5 / 30 | Total Loss: 5.421602249145508 | KNN Loss: 4.379310607910156 | BCE Loss: 1.0422918796539307\n",
      "Epoch 386 / 500 | iteration 10 / 30 | Total Loss: 5.409458160400391 | KNN Loss: 4.35283088684082 | BCE Loss: 1.0566273927688599\n",
      "Epoch 386 / 500 | iteration 15 / 30 | Total Loss: 5.345491886138916 | KNN Loss: 4.327455520629883 | BCE Loss: 1.0180364847183228\n",
      "Epoch 386 / 500 | iteration 20 / 30 | Total Loss: 5.408489227294922 | KNN Loss: 4.377689838409424 | BCE Loss: 1.030799388885498\n",
      "Epoch 386 / 500 | iteration 25 / 30 | Total Loss: 5.3287858963012695 | KNN Loss: 4.337329864501953 | BCE Loss: 0.9914557933807373\n",
      "Epoch 387 / 500 | iteration 0 / 30 | Total Loss: 5.399643898010254 | KNN Loss: 4.356263160705566 | BCE Loss: 1.0433809757232666\n",
      "Epoch 387 / 500 | iteration 5 / 30 | Total Loss: 5.369809150695801 | KNN Loss: 4.324773788452148 | BCE Loss: 1.0450352430343628\n",
      "Epoch 387 / 500 | iteration 10 / 30 | Total Loss: 5.364563941955566 | KNN Loss: 4.36755895614624 | BCE Loss: 0.9970049858093262\n",
      "Epoch 387 / 500 | iteration 15 / 30 | Total Loss: 5.367603778839111 | KNN Loss: 4.3307108879089355 | BCE Loss: 1.0368928909301758\n",
      "Epoch 387 / 500 | iteration 20 / 30 | Total Loss: 5.42814826965332 | KNN Loss: 4.395357608795166 | BCE Loss: 1.0327907800674438\n",
      "Epoch 387 / 500 | iteration 25 / 30 | Total Loss: 5.358641624450684 | KNN Loss: 4.353014945983887 | BCE Loss: 1.005626916885376\n",
      "Epoch 388 / 500 | iteration 0 / 30 | Total Loss: 5.399271011352539 | KNN Loss: 4.364007472991943 | BCE Loss: 1.0352637767791748\n",
      "Epoch 388 / 500 | iteration 5 / 30 | Total Loss: 5.390840530395508 | KNN Loss: 4.3569111824035645 | BCE Loss: 1.0339295864105225\n",
      "Epoch 388 / 500 | iteration 10 / 30 | Total Loss: 5.344912052154541 | KNN Loss: 4.319727897644043 | BCE Loss: 1.025184154510498\n",
      "Epoch 388 / 500 | iteration 15 / 30 | Total Loss: 5.393951416015625 | KNN Loss: 4.366055488586426 | BCE Loss: 1.0278956890106201\n",
      "Epoch 388 / 500 | iteration 20 / 30 | Total Loss: 5.3509979248046875 | KNN Loss: 4.32258415222168 | BCE Loss: 1.028414011001587\n",
      "Epoch 388 / 500 | iteration 25 / 30 | Total Loss: 5.395524024963379 | KNN Loss: 4.384678363800049 | BCE Loss: 1.0108455419540405\n",
      "Epoch   389: reducing learning rate of group 0 to 1.3684e-06.\n",
      "Epoch 389 / 500 | iteration 0 / 30 | Total Loss: 5.363272666931152 | KNN Loss: 4.3578104972839355 | BCE Loss: 1.0054622888565063\n",
      "Epoch 389 / 500 | iteration 5 / 30 | Total Loss: 5.388022422790527 | KNN Loss: 4.3243279457092285 | BCE Loss: 1.0636944770812988\n",
      "Epoch 389 / 500 | iteration 10 / 30 | Total Loss: 5.370930194854736 | KNN Loss: 4.332484722137451 | BCE Loss: 1.0384453535079956\n",
      "Epoch 389 / 500 | iteration 15 / 30 | Total Loss: 5.3838887214660645 | KNN Loss: 4.347522735595703 | BCE Loss: 1.0363659858703613\n",
      "Epoch 389 / 500 | iteration 20 / 30 | Total Loss: 5.3668928146362305 | KNN Loss: 4.334871292114258 | BCE Loss: 1.032021403312683\n",
      "Epoch 389 / 500 | iteration 25 / 30 | Total Loss: 5.36077880859375 | KNN Loss: 4.3323445320129395 | BCE Loss: 1.0284340381622314\n",
      "Epoch 390 / 500 | iteration 0 / 30 | Total Loss: 5.387930393218994 | KNN Loss: 4.3795294761657715 | BCE Loss: 1.0084009170532227\n",
      "Epoch 390 / 500 | iteration 5 / 30 | Total Loss: 5.325726509094238 | KNN Loss: 4.320239543914795 | BCE Loss: 1.0054869651794434\n",
      "Epoch 390 / 500 | iteration 10 / 30 | Total Loss: 5.388787269592285 | KNN Loss: 4.390066146850586 | BCE Loss: 0.9987211227416992\n",
      "Epoch 390 / 500 | iteration 15 / 30 | Total Loss: 5.354349136352539 | KNN Loss: 4.328969478607178 | BCE Loss: 1.0253794193267822\n",
      "Epoch 390 / 500 | iteration 20 / 30 | Total Loss: 5.347369194030762 | KNN Loss: 4.320553302764893 | BCE Loss: 1.0268158912658691\n",
      "Epoch 390 / 500 | iteration 25 / 30 | Total Loss: 5.3254194259643555 | KNN Loss: 4.327938079833984 | BCE Loss: 0.9974814653396606\n",
      "Epoch 391 / 500 | iteration 0 / 30 | Total Loss: 5.35142707824707 | KNN Loss: 4.331817150115967 | BCE Loss: 1.0196099281311035\n",
      "Epoch 391 / 500 | iteration 5 / 30 | Total Loss: 5.348782062530518 | KNN Loss: 4.360352993011475 | BCE Loss: 0.9884288907051086\n",
      "Epoch 391 / 500 | iteration 10 / 30 | Total Loss: 5.332789897918701 | KNN Loss: 4.315564155578613 | BCE Loss: 1.017225742340088\n",
      "Epoch 391 / 500 | iteration 15 / 30 | Total Loss: 5.324108600616455 | KNN Loss: 4.322214603424072 | BCE Loss: 1.0018939971923828\n",
      "Epoch 391 / 500 | iteration 20 / 30 | Total Loss: 5.386298179626465 | KNN Loss: 4.3529582023620605 | BCE Loss: 1.0333402156829834\n",
      "Epoch 391 / 500 | iteration 25 / 30 | Total Loss: 5.400922775268555 | KNN Loss: 4.3690619468688965 | BCE Loss: 1.0318610668182373\n",
      "Epoch 392 / 500 | iteration 0 / 30 | Total Loss: 5.376513957977295 | KNN Loss: 4.352721214294434 | BCE Loss: 1.0237927436828613\n",
      "Epoch 392 / 500 | iteration 5 / 30 | Total Loss: 5.314291954040527 | KNN Loss: 4.309340953826904 | BCE Loss: 1.004950761795044\n",
      "Epoch 392 / 500 | iteration 10 / 30 | Total Loss: 5.378171443939209 | KNN Loss: 4.338521480560303 | BCE Loss: 1.0396499633789062\n",
      "Epoch 392 / 500 | iteration 15 / 30 | Total Loss: 5.403409957885742 | KNN Loss: 4.371012210845947 | BCE Loss: 1.032397985458374\n",
      "Epoch 392 / 500 | iteration 20 / 30 | Total Loss: 5.398103713989258 | KNN Loss: 4.370092391967773 | BCE Loss: 1.0280110836029053\n",
      "Epoch 392 / 500 | iteration 25 / 30 | Total Loss: 5.3021087646484375 | KNN Loss: 4.304056167602539 | BCE Loss: 0.9980527758598328\n",
      "Epoch 393 / 500 | iteration 0 / 30 | Total Loss: 5.338127613067627 | KNN Loss: 4.3192219734191895 | BCE Loss: 1.018905520439148\n",
      "Epoch 393 / 500 | iteration 5 / 30 | Total Loss: 5.430848121643066 | KNN Loss: 4.419353485107422 | BCE Loss: 1.0114946365356445\n",
      "Epoch 393 / 500 | iteration 10 / 30 | Total Loss: 5.366421222686768 | KNN Loss: 4.338597297668457 | BCE Loss: 1.027823805809021\n",
      "Epoch 393 / 500 | iteration 15 / 30 | Total Loss: 5.382135391235352 | KNN Loss: 4.3554606437683105 | BCE Loss: 1.026674747467041\n",
      "Epoch 393 / 500 | iteration 20 / 30 | Total Loss: 5.39104700088501 | KNN Loss: 4.346410274505615 | BCE Loss: 1.0446367263793945\n",
      "Epoch 393 / 500 | iteration 25 / 30 | Total Loss: 5.332927227020264 | KNN Loss: 4.319647789001465 | BCE Loss: 1.0132794380187988\n",
      "Epoch 394 / 500 | iteration 0 / 30 | Total Loss: 5.349506855010986 | KNN Loss: 4.32394552230835 | BCE Loss: 1.0255614519119263\n",
      "Epoch 394 / 500 | iteration 5 / 30 | Total Loss: 5.373088836669922 | KNN Loss: 4.333319664001465 | BCE Loss: 1.0397694110870361\n",
      "Epoch 394 / 500 | iteration 10 / 30 | Total Loss: 5.310483932495117 | KNN Loss: 4.306551933288574 | BCE Loss: 1.0039318799972534\n",
      "Epoch 394 / 500 | iteration 15 / 30 | Total Loss: 5.372529029846191 | KNN Loss: 4.349647521972656 | BCE Loss: 1.0228817462921143\n",
      "Epoch 394 / 500 | iteration 20 / 30 | Total Loss: 5.377491474151611 | KNN Loss: 4.34390926361084 | BCE Loss: 1.0335822105407715\n",
      "Epoch 394 / 500 | iteration 25 / 30 | Total Loss: 5.374783992767334 | KNN Loss: 4.344080448150635 | BCE Loss: 1.0307035446166992\n",
      "Epoch 395 / 500 | iteration 0 / 30 | Total Loss: 5.385933876037598 | KNN Loss: 4.3512725830078125 | BCE Loss: 1.0346614122390747\n",
      "Epoch 395 / 500 | iteration 5 / 30 | Total Loss: 5.302034854888916 | KNN Loss: 4.312938690185547 | BCE Loss: 0.9890961647033691\n",
      "Epoch 395 / 500 | iteration 10 / 30 | Total Loss: 5.334025859832764 | KNN Loss: 4.319710731506348 | BCE Loss: 1.0143152475357056\n",
      "Epoch 395 / 500 | iteration 15 / 30 | Total Loss: 5.364306926727295 | KNN Loss: 4.35407018661499 | BCE Loss: 1.0102366209030151\n",
      "Epoch 395 / 500 | iteration 20 / 30 | Total Loss: 5.39614200592041 | KNN Loss: 4.358458042144775 | BCE Loss: 1.0376842021942139\n",
      "Epoch 395 / 500 | iteration 25 / 30 | Total Loss: 5.332789421081543 | KNN Loss: 4.307199001312256 | BCE Loss: 1.025590419769287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 396 / 500 | iteration 0 / 30 | Total Loss: 5.479053020477295 | KNN Loss: 4.424442768096924 | BCE Loss: 1.054610252380371\n",
      "Epoch 396 / 500 | iteration 5 / 30 | Total Loss: 5.4039812088012695 | KNN Loss: 4.381107330322266 | BCE Loss: 1.022873878479004\n",
      "Epoch 396 / 500 | iteration 10 / 30 | Total Loss: 5.393434524536133 | KNN Loss: 4.372860431671143 | BCE Loss: 1.0205740928649902\n",
      "Epoch 396 / 500 | iteration 15 / 30 | Total Loss: 5.357295513153076 | KNN Loss: 4.340730667114258 | BCE Loss: 1.016564965248108\n",
      "Epoch 396 / 500 | iteration 20 / 30 | Total Loss: 5.375602722167969 | KNN Loss: 4.356148719787598 | BCE Loss: 1.019453763961792\n",
      "Epoch 396 / 500 | iteration 25 / 30 | Total Loss: 5.38551139831543 | KNN Loss: 4.362536430358887 | BCE Loss: 1.022975206375122\n",
      "Epoch 397 / 500 | iteration 0 / 30 | Total Loss: 5.35002326965332 | KNN Loss: 4.322474956512451 | BCE Loss: 1.0275481939315796\n",
      "Epoch 397 / 500 | iteration 5 / 30 | Total Loss: 5.322835445404053 | KNN Loss: 4.341619491577148 | BCE Loss: 0.9812160730361938\n",
      "Epoch 397 / 500 | iteration 10 / 30 | Total Loss: 5.393923759460449 | KNN Loss: 4.365708351135254 | BCE Loss: 1.0282151699066162\n",
      "Epoch 397 / 500 | iteration 15 / 30 | Total Loss: 5.3652729988098145 | KNN Loss: 4.337670803070068 | BCE Loss: 1.0276023149490356\n",
      "Epoch 397 / 500 | iteration 20 / 30 | Total Loss: 5.390600204467773 | KNN Loss: 4.36410665512085 | BCE Loss: 1.0264933109283447\n",
      "Epoch 397 / 500 | iteration 25 / 30 | Total Loss: 5.3684868812561035 | KNN Loss: 4.336898326873779 | BCE Loss: 1.0315884351730347\n",
      "Epoch 398 / 500 | iteration 0 / 30 | Total Loss: 5.337582588195801 | KNN Loss: 4.3228349685668945 | BCE Loss: 1.0147476196289062\n",
      "Epoch 398 / 500 | iteration 5 / 30 | Total Loss: 5.354879379272461 | KNN Loss: 4.353277206420898 | BCE Loss: 1.0016024112701416\n",
      "Epoch 398 / 500 | iteration 10 / 30 | Total Loss: 5.416443824768066 | KNN Loss: 4.357109069824219 | BCE Loss: 1.0593349933624268\n",
      "Epoch 398 / 500 | iteration 15 / 30 | Total Loss: 5.333409786224365 | KNN Loss: 4.307488918304443 | BCE Loss: 1.0259207487106323\n",
      "Epoch 398 / 500 | iteration 20 / 30 | Total Loss: 5.40889835357666 | KNN Loss: 4.376986980438232 | BCE Loss: 1.0319112539291382\n",
      "Epoch 398 / 500 | iteration 25 / 30 | Total Loss: 5.39299201965332 | KNN Loss: 4.358652114868164 | BCE Loss: 1.0343401432037354\n",
      "Epoch 399 / 500 | iteration 0 / 30 | Total Loss: 5.378032684326172 | KNN Loss: 4.330574035644531 | BCE Loss: 1.047458529472351\n",
      "Epoch 399 / 500 | iteration 5 / 30 | Total Loss: 5.4239702224731445 | KNN Loss: 4.398852348327637 | BCE Loss: 1.025118112564087\n",
      "Epoch 399 / 500 | iteration 10 / 30 | Total Loss: 5.3844404220581055 | KNN Loss: 4.352932453155518 | BCE Loss: 1.031507968902588\n",
      "Epoch 399 / 500 | iteration 15 / 30 | Total Loss: 5.368242263793945 | KNN Loss: 4.336854934692383 | BCE Loss: 1.0313873291015625\n",
      "Epoch 399 / 500 | iteration 20 / 30 | Total Loss: 5.380802154541016 | KNN Loss: 4.342113018035889 | BCE Loss: 1.038689136505127\n",
      "Epoch 399 / 500 | iteration 25 / 30 | Total Loss: 5.35268497467041 | KNN Loss: 4.3335161209106445 | BCE Loss: 1.0191690921783447\n",
      "Epoch   400: reducing learning rate of group 0 to 9.5791e-07.\n",
      "Epoch 400 / 500 | iteration 0 / 30 | Total Loss: 5.378312110900879 | KNN Loss: 4.344683647155762 | BCE Loss: 1.033628225326538\n",
      "Epoch 400 / 500 | iteration 5 / 30 | Total Loss: 5.375208854675293 | KNN Loss: 4.360495567321777 | BCE Loss: 1.0147135257720947\n",
      "Epoch 400 / 500 | iteration 10 / 30 | Total Loss: 5.409614086151123 | KNN Loss: 4.375697612762451 | BCE Loss: 1.0339163541793823\n",
      "Epoch 400 / 500 | iteration 15 / 30 | Total Loss: 5.356748580932617 | KNN Loss: 4.3408403396606445 | BCE Loss: 1.0159080028533936\n",
      "Epoch 400 / 500 | iteration 20 / 30 | Total Loss: 5.354196548461914 | KNN Loss: 4.345629692077637 | BCE Loss: 1.0085670948028564\n",
      "Epoch 400 / 500 | iteration 25 / 30 | Total Loss: 5.363959312438965 | KNN Loss: 4.339066505432129 | BCE Loss: 1.024893045425415\n",
      "Epoch 401 / 500 | iteration 0 / 30 | Total Loss: 5.386665344238281 | KNN Loss: 4.350764751434326 | BCE Loss: 1.035900592803955\n",
      "Epoch 401 / 500 | iteration 5 / 30 | Total Loss: 5.376119136810303 | KNN Loss: 4.372997283935547 | BCE Loss: 1.0031218528747559\n",
      "Epoch 401 / 500 | iteration 10 / 30 | Total Loss: 5.350765228271484 | KNN Loss: 4.321343421936035 | BCE Loss: 1.0294216871261597\n",
      "Epoch 401 / 500 | iteration 15 / 30 | Total Loss: 5.354117393493652 | KNN Loss: 4.365161418914795 | BCE Loss: 0.9889562129974365\n",
      "Epoch 401 / 500 | iteration 20 / 30 | Total Loss: 5.363198280334473 | KNN Loss: 4.339104652404785 | BCE Loss: 1.0240936279296875\n",
      "Epoch 401 / 500 | iteration 25 / 30 | Total Loss: 5.37215518951416 | KNN Loss: 4.319839000701904 | BCE Loss: 1.0523163080215454\n",
      "Epoch 402 / 500 | iteration 0 / 30 | Total Loss: 5.3567399978637695 | KNN Loss: 4.341180801391602 | BCE Loss: 1.015559196472168\n",
      "Epoch 402 / 500 | iteration 5 / 30 | Total Loss: 5.366006374359131 | KNN Loss: 4.338473796844482 | BCE Loss: 1.0275325775146484\n",
      "Epoch 402 / 500 | iteration 10 / 30 | Total Loss: 5.329298973083496 | KNN Loss: 4.316481590270996 | BCE Loss: 1.012817144393921\n",
      "Epoch 402 / 500 | iteration 15 / 30 | Total Loss: 5.348394870758057 | KNN Loss: 4.352883338928223 | BCE Loss: 0.9955114126205444\n",
      "Epoch 402 / 500 | iteration 20 / 30 | Total Loss: 5.384767532348633 | KNN Loss: 4.3444437980651855 | BCE Loss: 1.0403237342834473\n",
      "Epoch 402 / 500 | iteration 25 / 30 | Total Loss: 5.37274169921875 | KNN Loss: 4.3092803955078125 | BCE Loss: 1.063461422920227\n",
      "Epoch 403 / 500 | iteration 0 / 30 | Total Loss: 5.362889289855957 | KNN Loss: 4.320435047149658 | BCE Loss: 1.0424543619155884\n",
      "Epoch 403 / 500 | iteration 5 / 30 | Total Loss: 5.386017799377441 | KNN Loss: 4.361534118652344 | BCE Loss: 1.0244834423065186\n",
      "Epoch 403 / 500 | iteration 10 / 30 | Total Loss: 5.399965286254883 | KNN Loss: 4.346774101257324 | BCE Loss: 1.0531913042068481\n",
      "Epoch 403 / 500 | iteration 15 / 30 | Total Loss: 5.401761054992676 | KNN Loss: 4.352238178253174 | BCE Loss: 1.0495226383209229\n",
      "Epoch 403 / 500 | iteration 20 / 30 | Total Loss: 5.413409233093262 | KNN Loss: 4.385307788848877 | BCE Loss: 1.0281012058258057\n",
      "Epoch 403 / 500 | iteration 25 / 30 | Total Loss: 5.379043102264404 | KNN Loss: 4.352147579193115 | BCE Loss: 1.0268956422805786\n",
      "Epoch 404 / 500 | iteration 0 / 30 | Total Loss: 5.380145072937012 | KNN Loss: 4.35723352432251 | BCE Loss: 1.022911548614502\n",
      "Epoch 404 / 500 | iteration 5 / 30 | Total Loss: 5.356325149536133 | KNN Loss: 4.335064888000488 | BCE Loss: 1.021260142326355\n",
      "Epoch 404 / 500 | iteration 10 / 30 | Total Loss: 5.418316841125488 | KNN Loss: 4.385988712310791 | BCE Loss: 1.0323282480239868\n",
      "Epoch 404 / 500 | iteration 15 / 30 | Total Loss: 5.359952449798584 | KNN Loss: 4.364365100860596 | BCE Loss: 0.9955874085426331\n",
      "Epoch 404 / 500 | iteration 20 / 30 | Total Loss: 5.317850112915039 | KNN Loss: 4.324880123138428 | BCE Loss: 0.9929701685905457\n",
      "Epoch 404 / 500 | iteration 25 / 30 | Total Loss: 5.399868011474609 | KNN Loss: 4.367187023162842 | BCE Loss: 1.032680869102478\n",
      "Epoch 405 / 500 | iteration 0 / 30 | Total Loss: 5.38001823425293 | KNN Loss: 4.348283290863037 | BCE Loss: 1.0317347049713135\n",
      "Epoch 405 / 500 | iteration 5 / 30 | Total Loss: 5.404478549957275 | KNN Loss: 4.365062236785889 | BCE Loss: 1.0394163131713867\n",
      "Epoch 405 / 500 | iteration 10 / 30 | Total Loss: 5.39244270324707 | KNN Loss: 4.368950843811035 | BCE Loss: 1.0234917402267456\n",
      "Epoch 405 / 500 | iteration 15 / 30 | Total Loss: 5.414113998413086 | KNN Loss: 4.3481597900390625 | BCE Loss: 1.0659540891647339\n",
      "Epoch 405 / 500 | iteration 20 / 30 | Total Loss: 5.353418827056885 | KNN Loss: 4.31515645980835 | BCE Loss: 1.0382623672485352\n",
      "Epoch 405 / 500 | iteration 25 / 30 | Total Loss: 5.389200210571289 | KNN Loss: 4.337115287780762 | BCE Loss: 1.0520851612091064\n",
      "Epoch 406 / 500 | iteration 0 / 30 | Total Loss: 5.346421718597412 | KNN Loss: 4.317256450653076 | BCE Loss: 1.029165267944336\n",
      "Epoch 406 / 500 | iteration 5 / 30 | Total Loss: 5.358325958251953 | KNN Loss: 4.35474967956543 | BCE Loss: 1.0035765171051025\n",
      "Epoch 406 / 500 | iteration 10 / 30 | Total Loss: 5.35129451751709 | KNN Loss: 4.313503742218018 | BCE Loss: 1.0377905368804932\n",
      "Epoch 406 / 500 | iteration 15 / 30 | Total Loss: 5.320223808288574 | KNN Loss: 4.303933620452881 | BCE Loss: 1.0162899494171143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 406 / 500 | iteration 20 / 30 | Total Loss: 5.38449239730835 | KNN Loss: 4.349279880523682 | BCE Loss: 1.035212516784668\n",
      "Epoch 406 / 500 | iteration 25 / 30 | Total Loss: 5.40925407409668 | KNN Loss: 4.359587669372559 | BCE Loss: 1.049666166305542\n",
      "Epoch 407 / 500 | iteration 0 / 30 | Total Loss: 5.45833158493042 | KNN Loss: 4.370003700256348 | BCE Loss: 1.0883278846740723\n",
      "Epoch 407 / 500 | iteration 5 / 30 | Total Loss: 5.374407768249512 | KNN Loss: 4.3740925788879395 | BCE Loss: 1.0003154277801514\n",
      "Epoch 407 / 500 | iteration 10 / 30 | Total Loss: 5.381504058837891 | KNN Loss: 4.365171432495117 | BCE Loss: 1.0163328647613525\n",
      "Epoch 407 / 500 | iteration 15 / 30 | Total Loss: 5.40058708190918 | KNN Loss: 4.402865886688232 | BCE Loss: 0.9977211952209473\n",
      "Epoch 407 / 500 | iteration 20 / 30 | Total Loss: 5.348052978515625 | KNN Loss: 4.322151184082031 | BCE Loss: 1.0259015560150146\n",
      "Epoch 407 / 500 | iteration 25 / 30 | Total Loss: 5.398111343383789 | KNN Loss: 4.368443489074707 | BCE Loss: 1.0296680927276611\n",
      "Epoch 408 / 500 | iteration 0 / 30 | Total Loss: 5.374032020568848 | KNN Loss: 4.3330254554748535 | BCE Loss: 1.0410068035125732\n",
      "Epoch 408 / 500 | iteration 5 / 30 | Total Loss: 5.337864875793457 | KNN Loss: 4.335483074188232 | BCE Loss: 1.0023820400238037\n",
      "Epoch 408 / 500 | iteration 10 / 30 | Total Loss: 5.369976043701172 | KNN Loss: 4.356657028198242 | BCE Loss: 1.0133188962936401\n",
      "Epoch 408 / 500 | iteration 15 / 30 | Total Loss: 5.367834568023682 | KNN Loss: 4.354910850524902 | BCE Loss: 1.0129237174987793\n",
      "Epoch 408 / 500 | iteration 20 / 30 | Total Loss: 5.372872352600098 | KNN Loss: 4.342343330383301 | BCE Loss: 1.0305290222167969\n",
      "Epoch 408 / 500 | iteration 25 / 30 | Total Loss: 5.34587287902832 | KNN Loss: 4.338510990142822 | BCE Loss: 1.0073617696762085\n",
      "Epoch 409 / 500 | iteration 0 / 30 | Total Loss: 5.367586135864258 | KNN Loss: 4.3438310623168945 | BCE Loss: 1.0237548351287842\n",
      "Epoch 409 / 500 | iteration 5 / 30 | Total Loss: 5.357990264892578 | KNN Loss: 4.326085567474365 | BCE Loss: 1.0319044589996338\n",
      "Epoch 409 / 500 | iteration 10 / 30 | Total Loss: 5.395467758178711 | KNN Loss: 4.377830982208252 | BCE Loss: 1.017636775970459\n",
      "Epoch 409 / 500 | iteration 15 / 30 | Total Loss: 5.391705513000488 | KNN Loss: 4.376587867736816 | BCE Loss: 1.0151176452636719\n",
      "Epoch 409 / 500 | iteration 20 / 30 | Total Loss: 5.398558616638184 | KNN Loss: 4.344177722930908 | BCE Loss: 1.0543807744979858\n",
      "Epoch 409 / 500 | iteration 25 / 30 | Total Loss: 5.412234306335449 | KNN Loss: 4.385745525360107 | BCE Loss: 1.026489019393921\n",
      "Epoch 410 / 500 | iteration 0 / 30 | Total Loss: 5.385024070739746 | KNN Loss: 4.333962917327881 | BCE Loss: 1.0510613918304443\n",
      "Epoch 410 / 500 | iteration 5 / 30 | Total Loss: 5.388404369354248 | KNN Loss: 4.333613395690918 | BCE Loss: 1.05479097366333\n",
      "Epoch 410 / 500 | iteration 10 / 30 | Total Loss: 5.345674514770508 | KNN Loss: 4.327432155609131 | BCE Loss: 1.018242359161377\n",
      "Epoch 410 / 500 | iteration 15 / 30 | Total Loss: 5.360498428344727 | KNN Loss: 4.3324666023254395 | BCE Loss: 1.028031826019287\n",
      "Epoch 410 / 500 | iteration 20 / 30 | Total Loss: 5.384341239929199 | KNN Loss: 4.360703468322754 | BCE Loss: 1.0236377716064453\n",
      "Epoch 410 / 500 | iteration 25 / 30 | Total Loss: 5.379640102386475 | KNN Loss: 4.352194786071777 | BCE Loss: 1.0274453163146973\n",
      "Epoch   411: reducing learning rate of group 0 to 6.7053e-07.\n",
      "Epoch 411 / 500 | iteration 0 / 30 | Total Loss: 5.354886531829834 | KNN Loss: 4.342711925506592 | BCE Loss: 1.0121746063232422\n",
      "Epoch 411 / 500 | iteration 5 / 30 | Total Loss: 5.3806681632995605 | KNN Loss: 4.350434303283691 | BCE Loss: 1.0302338600158691\n",
      "Epoch 411 / 500 | iteration 10 / 30 | Total Loss: 5.341649055480957 | KNN Loss: 4.313292026519775 | BCE Loss: 1.0283567905426025\n",
      "Epoch 411 / 500 | iteration 15 / 30 | Total Loss: 5.335565090179443 | KNN Loss: 4.3100810050964355 | BCE Loss: 1.0254840850830078\n",
      "Epoch 411 / 500 | iteration 20 / 30 | Total Loss: 5.379652500152588 | KNN Loss: 4.373193264007568 | BCE Loss: 1.0064592361450195\n",
      "Epoch 411 / 500 | iteration 25 / 30 | Total Loss: 5.363251686096191 | KNN Loss: 4.333549976348877 | BCE Loss: 1.0297019481658936\n",
      "Epoch 412 / 500 | iteration 0 / 30 | Total Loss: 5.365910053253174 | KNN Loss: 4.336740493774414 | BCE Loss: 1.0291696786880493\n",
      "Epoch 412 / 500 | iteration 5 / 30 | Total Loss: 5.410576343536377 | KNN Loss: 4.37639856338501 | BCE Loss: 1.0341777801513672\n",
      "Epoch 412 / 500 | iteration 10 / 30 | Total Loss: 5.355990886688232 | KNN Loss: 4.357048511505127 | BCE Loss: 0.998942494392395\n",
      "Epoch 412 / 500 | iteration 15 / 30 | Total Loss: 5.428882598876953 | KNN Loss: 4.353147029876709 | BCE Loss: 1.0757354497909546\n",
      "Epoch 412 / 500 | iteration 20 / 30 | Total Loss: 5.406373977661133 | KNN Loss: 4.36184549331665 | BCE Loss: 1.0445284843444824\n",
      "Epoch 412 / 500 | iteration 25 / 30 | Total Loss: 5.428008079528809 | KNN Loss: 4.409664154052734 | BCE Loss: 1.0183439254760742\n",
      "Epoch 413 / 500 | iteration 0 / 30 | Total Loss: 5.356956958770752 | KNN Loss: 4.343369483947754 | BCE Loss: 1.0135875940322876\n",
      "Epoch 413 / 500 | iteration 5 / 30 | Total Loss: 5.368475914001465 | KNN Loss: 4.326333522796631 | BCE Loss: 1.042142629623413\n",
      "Epoch 413 / 500 | iteration 10 / 30 | Total Loss: 5.421095848083496 | KNN Loss: 4.345097064971924 | BCE Loss: 1.0759987831115723\n",
      "Epoch 413 / 500 | iteration 15 / 30 | Total Loss: 5.396284103393555 | KNN Loss: 4.348240852355957 | BCE Loss: 1.0480434894561768\n",
      "Epoch 413 / 500 | iteration 20 / 30 | Total Loss: 5.427667617797852 | KNN Loss: 4.381204128265381 | BCE Loss: 1.0464637279510498\n",
      "Epoch 413 / 500 | iteration 25 / 30 | Total Loss: 5.399195671081543 | KNN Loss: 4.3774919509887695 | BCE Loss: 1.0217034816741943\n",
      "Epoch 414 / 500 | iteration 0 / 30 | Total Loss: 5.357815742492676 | KNN Loss: 4.365730285644531 | BCE Loss: 0.9920854568481445\n",
      "Epoch 414 / 500 | iteration 5 / 30 | Total Loss: 5.3653154373168945 | KNN Loss: 4.351316452026367 | BCE Loss: 1.0139988660812378\n",
      "Epoch 414 / 500 | iteration 10 / 30 | Total Loss: 5.393527984619141 | KNN Loss: 4.369682312011719 | BCE Loss: 1.023845911026001\n",
      "Epoch 414 / 500 | iteration 15 / 30 | Total Loss: 5.344298839569092 | KNN Loss: 4.3215413093566895 | BCE Loss: 1.0227575302124023\n",
      "Epoch 414 / 500 | iteration 20 / 30 | Total Loss: 5.387003421783447 | KNN Loss: 4.364505767822266 | BCE Loss: 1.022497534751892\n",
      "Epoch 414 / 500 | iteration 25 / 30 | Total Loss: 5.407658576965332 | KNN Loss: 4.366851329803467 | BCE Loss: 1.0408073663711548\n",
      "Epoch 415 / 500 | iteration 0 / 30 | Total Loss: 5.357355117797852 | KNN Loss: 4.34696626663208 | BCE Loss: 1.0103886127471924\n",
      "Epoch 415 / 500 | iteration 5 / 30 | Total Loss: 5.379132270812988 | KNN Loss: 4.3650617599487305 | BCE Loss: 1.0140702724456787\n",
      "Epoch 415 / 500 | iteration 10 / 30 | Total Loss: 5.306144714355469 | KNN Loss: 4.303138256072998 | BCE Loss: 1.0030062198638916\n",
      "Epoch 415 / 500 | iteration 15 / 30 | Total Loss: 5.397116661071777 | KNN Loss: 4.363765239715576 | BCE Loss: 1.0333513021469116\n",
      "Epoch 415 / 500 | iteration 20 / 30 | Total Loss: 5.387113094329834 | KNN Loss: 4.364516735076904 | BCE Loss: 1.0225964784622192\n",
      "Epoch 415 / 500 | iteration 25 / 30 | Total Loss: 5.368167877197266 | KNN Loss: 4.336028099060059 | BCE Loss: 1.032139539718628\n",
      "Epoch 416 / 500 | iteration 0 / 30 | Total Loss: 5.394561767578125 | KNN Loss: 4.34623908996582 | BCE Loss: 1.0483229160308838\n",
      "Epoch 416 / 500 | iteration 5 / 30 | Total Loss: 5.322046756744385 | KNN Loss: 4.3276872634887695 | BCE Loss: 0.9943593740463257\n",
      "Epoch 416 / 500 | iteration 10 / 30 | Total Loss: 5.377567768096924 | KNN Loss: 4.35247802734375 | BCE Loss: 1.0250897407531738\n",
      "Epoch 416 / 500 | iteration 15 / 30 | Total Loss: 5.398764610290527 | KNN Loss: 4.380390644073486 | BCE Loss: 1.018373966217041\n",
      "Epoch 416 / 500 | iteration 20 / 30 | Total Loss: 5.339441776275635 | KNN Loss: 4.306850433349609 | BCE Loss: 1.0325913429260254\n",
      "Epoch 416 / 500 | iteration 25 / 30 | Total Loss: 5.356960296630859 | KNN Loss: 4.325162410736084 | BCE Loss: 1.0317976474761963\n",
      "Epoch 417 / 500 | iteration 0 / 30 | Total Loss: 5.321125030517578 | KNN Loss: 4.308087348937988 | BCE Loss: 1.0130374431610107\n",
      "Epoch 417 / 500 | iteration 5 / 30 | Total Loss: 5.407382011413574 | KNN Loss: 4.369776248931885 | BCE Loss: 1.0376056432724\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 417 / 500 | iteration 10 / 30 | Total Loss: 5.352338790893555 | KNN Loss: 4.3151655197143555 | BCE Loss: 1.0371730327606201\n",
      "Epoch 417 / 500 | iteration 15 / 30 | Total Loss: 5.377419471740723 | KNN Loss: 4.3636345863342285 | BCE Loss: 1.0137851238250732\n",
      "Epoch 417 / 500 | iteration 20 / 30 | Total Loss: 5.376640319824219 | KNN Loss: 4.3538713455200195 | BCE Loss: 1.0227687358856201\n",
      "Epoch 417 / 500 | iteration 25 / 30 | Total Loss: 5.358232498168945 | KNN Loss: 4.3286237716674805 | BCE Loss: 1.0296088457107544\n",
      "Epoch 418 / 500 | iteration 0 / 30 | Total Loss: 5.374948501586914 | KNN Loss: 4.335185527801514 | BCE Loss: 1.0397629737854004\n",
      "Epoch 418 / 500 | iteration 5 / 30 | Total Loss: 5.433009624481201 | KNN Loss: 4.401450157165527 | BCE Loss: 1.0315595865249634\n",
      "Epoch 418 / 500 | iteration 10 / 30 | Total Loss: 5.339648723602295 | KNN Loss: 4.330431938171387 | BCE Loss: 1.0092166662216187\n",
      "Epoch 418 / 500 | iteration 15 / 30 | Total Loss: 5.378896713256836 | KNN Loss: 4.346991062164307 | BCE Loss: 1.0319056510925293\n",
      "Epoch 418 / 500 | iteration 20 / 30 | Total Loss: 5.350137710571289 | KNN Loss: 4.325233459472656 | BCE Loss: 1.0249042510986328\n",
      "Epoch 418 / 500 | iteration 25 / 30 | Total Loss: 5.404064178466797 | KNN Loss: 4.361944198608398 | BCE Loss: 1.0421202182769775\n",
      "Epoch 419 / 500 | iteration 0 / 30 | Total Loss: 5.392614364624023 | KNN Loss: 4.356140613555908 | BCE Loss: 1.0364739894866943\n",
      "Epoch 419 / 500 | iteration 5 / 30 | Total Loss: 5.368772029876709 | KNN Loss: 4.361096382141113 | BCE Loss: 1.0076755285263062\n",
      "Epoch 419 / 500 | iteration 10 / 30 | Total Loss: 5.351452827453613 | KNN Loss: 4.343472957611084 | BCE Loss: 1.0079796314239502\n",
      "Epoch 419 / 500 | iteration 15 / 30 | Total Loss: 5.377947807312012 | KNN Loss: 4.349413871765137 | BCE Loss: 1.028533935546875\n",
      "Epoch 419 / 500 | iteration 20 / 30 | Total Loss: 5.37593936920166 | KNN Loss: 4.354198932647705 | BCE Loss: 1.021740198135376\n",
      "Epoch 419 / 500 | iteration 25 / 30 | Total Loss: 5.44509744644165 | KNN Loss: 4.403096675872803 | BCE Loss: 1.0420007705688477\n",
      "Epoch 420 / 500 | iteration 0 / 30 | Total Loss: 5.402416706085205 | KNN Loss: 4.3522162437438965 | BCE Loss: 1.050200343132019\n",
      "Epoch 420 / 500 | iteration 5 / 30 | Total Loss: 5.386707782745361 | KNN Loss: 4.361530303955078 | BCE Loss: 1.0251773595809937\n",
      "Epoch 420 / 500 | iteration 10 / 30 | Total Loss: 5.331233024597168 | KNN Loss: 4.318457126617432 | BCE Loss: 1.0127756595611572\n",
      "Epoch 420 / 500 | iteration 15 / 30 | Total Loss: 5.383094787597656 | KNN Loss: 4.338549613952637 | BCE Loss: 1.0445449352264404\n",
      "Epoch 420 / 500 | iteration 20 / 30 | Total Loss: 5.388077259063721 | KNN Loss: 4.380172252655029 | BCE Loss: 1.007905125617981\n",
      "Epoch 420 / 500 | iteration 25 / 30 | Total Loss: 5.349820137023926 | KNN Loss: 4.33308744430542 | BCE Loss: 1.0167324542999268\n",
      "Epoch 421 / 500 | iteration 0 / 30 | Total Loss: 5.348586082458496 | KNN Loss: 4.3512701988220215 | BCE Loss: 0.9973158836364746\n",
      "Epoch 421 / 500 | iteration 5 / 30 | Total Loss: 5.399949073791504 | KNN Loss: 4.365727424621582 | BCE Loss: 1.0342214107513428\n",
      "Epoch 421 / 500 | iteration 10 / 30 | Total Loss: 5.3726091384887695 | KNN Loss: 4.365572452545166 | BCE Loss: 1.0070366859436035\n",
      "Epoch 421 / 500 | iteration 15 / 30 | Total Loss: 5.4007768630981445 | KNN Loss: 4.344205379486084 | BCE Loss: 1.0565712451934814\n",
      "Epoch 421 / 500 | iteration 20 / 30 | Total Loss: 5.366590976715088 | KNN Loss: 4.364998817443848 | BCE Loss: 1.0015921592712402\n",
      "Epoch 421 / 500 | iteration 25 / 30 | Total Loss: 5.362293720245361 | KNN Loss: 4.326679229736328 | BCE Loss: 1.0356146097183228\n",
      "Epoch   422: reducing learning rate of group 0 to 4.6937e-07.\n",
      "Epoch 422 / 500 | iteration 0 / 30 | Total Loss: 5.410977363586426 | KNN Loss: 4.396434783935547 | BCE Loss: 1.014542579650879\n",
      "Epoch 422 / 500 | iteration 5 / 30 | Total Loss: 5.380764961242676 | KNN Loss: 4.361198902130127 | BCE Loss: 1.019566297531128\n",
      "Epoch 422 / 500 | iteration 10 / 30 | Total Loss: 5.387465000152588 | KNN Loss: 4.384317398071289 | BCE Loss: 1.0031474828720093\n",
      "Epoch 422 / 500 | iteration 15 / 30 | Total Loss: 5.401797771453857 | KNN Loss: 4.390126705169678 | BCE Loss: 1.0116710662841797\n",
      "Epoch 422 / 500 | iteration 20 / 30 | Total Loss: 5.337172508239746 | KNN Loss: 4.322062969207764 | BCE Loss: 1.0151097774505615\n",
      "Epoch 422 / 500 | iteration 25 / 30 | Total Loss: 5.4189910888671875 | KNN Loss: 4.368149757385254 | BCE Loss: 1.0508415699005127\n",
      "Epoch 423 / 500 | iteration 0 / 30 | Total Loss: 5.343852996826172 | KNN Loss: 4.347330093383789 | BCE Loss: 0.9965227246284485\n",
      "Epoch 423 / 500 | iteration 5 / 30 | Total Loss: 5.38004207611084 | KNN Loss: 4.347218036651611 | BCE Loss: 1.0328240394592285\n",
      "Epoch 423 / 500 | iteration 10 / 30 | Total Loss: 5.367786407470703 | KNN Loss: 4.3643317222595215 | BCE Loss: 1.003454566001892\n",
      "Epoch 423 / 500 | iteration 15 / 30 | Total Loss: 5.375139236450195 | KNN Loss: 4.348177433013916 | BCE Loss: 1.0269618034362793\n",
      "Epoch 423 / 500 | iteration 20 / 30 | Total Loss: 5.421202659606934 | KNN Loss: 4.37938928604126 | BCE Loss: 1.0418134927749634\n",
      "Epoch 423 / 500 | iteration 25 / 30 | Total Loss: 5.342842102050781 | KNN Loss: 4.308351039886475 | BCE Loss: 1.0344910621643066\n",
      "Epoch 424 / 500 | iteration 0 / 30 | Total Loss: 5.420697212219238 | KNN Loss: 4.385666847229004 | BCE Loss: 1.0350303649902344\n",
      "Epoch 424 / 500 | iteration 5 / 30 | Total Loss: 5.414914131164551 | KNN Loss: 4.369443893432617 | BCE Loss: 1.0454702377319336\n",
      "Epoch 424 / 500 | iteration 10 / 30 | Total Loss: 5.453691482543945 | KNN Loss: 4.411795616149902 | BCE Loss: 1.0418959856033325\n",
      "Epoch 424 / 500 | iteration 15 / 30 | Total Loss: 5.372640609741211 | KNN Loss: 4.325332164764404 | BCE Loss: 1.0473082065582275\n",
      "Epoch 424 / 500 | iteration 20 / 30 | Total Loss: 5.363032817840576 | KNN Loss: 4.338907718658447 | BCE Loss: 1.0241249799728394\n",
      "Epoch 424 / 500 | iteration 25 / 30 | Total Loss: 5.37883996963501 | KNN Loss: 4.386412620544434 | BCE Loss: 0.9924273490905762\n",
      "Epoch 425 / 500 | iteration 0 / 30 | Total Loss: 5.355083465576172 | KNN Loss: 4.327655792236328 | BCE Loss: 1.0274276733398438\n",
      "Epoch 425 / 500 | iteration 5 / 30 | Total Loss: 5.399473667144775 | KNN Loss: 4.357889175415039 | BCE Loss: 1.0415846109390259\n",
      "Epoch 425 / 500 | iteration 10 / 30 | Total Loss: 5.420214653015137 | KNN Loss: 4.393289566040039 | BCE Loss: 1.0269248485565186\n",
      "Epoch 425 / 500 | iteration 15 / 30 | Total Loss: 5.400406837463379 | KNN Loss: 4.361759662628174 | BCE Loss: 1.0386472940444946\n",
      "Epoch 425 / 500 | iteration 20 / 30 | Total Loss: 5.354804992675781 | KNN Loss: 4.3197340965271 | BCE Loss: 1.0350711345672607\n",
      "Epoch 425 / 500 | iteration 25 / 30 | Total Loss: 5.339869499206543 | KNN Loss: 4.343214988708496 | BCE Loss: 0.9966545701026917\n",
      "Epoch 426 / 500 | iteration 0 / 30 | Total Loss: 5.389472961425781 | KNN Loss: 4.355571746826172 | BCE Loss: 1.0339012145996094\n",
      "Epoch 426 / 500 | iteration 5 / 30 | Total Loss: 5.355290412902832 | KNN Loss: 4.324960231781006 | BCE Loss: 1.030329942703247\n",
      "Epoch 426 / 500 | iteration 10 / 30 | Total Loss: 5.417342185974121 | KNN Loss: 4.386839866638184 | BCE Loss: 1.030502438545227\n",
      "Epoch 426 / 500 | iteration 15 / 30 | Total Loss: 5.360702991485596 | KNN Loss: 4.3399434089660645 | BCE Loss: 1.0207595825195312\n",
      "Epoch 426 / 500 | iteration 20 / 30 | Total Loss: 5.3889570236206055 | KNN Loss: 4.3577656745910645 | BCE Loss: 1.031191110610962\n",
      "Epoch 426 / 500 | iteration 25 / 30 | Total Loss: 5.436010837554932 | KNN Loss: 4.376755237579346 | BCE Loss: 1.0592554807662964\n",
      "Epoch 427 / 500 | iteration 0 / 30 | Total Loss: 5.360649108886719 | KNN Loss: 4.3350443840026855 | BCE Loss: 1.025604486465454\n",
      "Epoch 427 / 500 | iteration 5 / 30 | Total Loss: 5.3476033210754395 | KNN Loss: 4.333322525024414 | BCE Loss: 1.0142807960510254\n",
      "Epoch 427 / 500 | iteration 10 / 30 | Total Loss: 5.441184997558594 | KNN Loss: 4.37308406829834 | BCE Loss: 1.0681010484695435\n",
      "Epoch 427 / 500 | iteration 15 / 30 | Total Loss: 5.34643030166626 | KNN Loss: 4.323779582977295 | BCE Loss: 1.0226507186889648\n",
      "Epoch 427 / 500 | iteration 20 / 30 | Total Loss: 5.332411766052246 | KNN Loss: 4.30849027633667 | BCE Loss: 1.0239213705062866\n",
      "Epoch 427 / 500 | iteration 25 / 30 | Total Loss: 5.360768795013428 | KNN Loss: 4.354423999786377 | BCE Loss: 1.0063449144363403\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 428 / 500 | iteration 0 / 30 | Total Loss: 5.421289920806885 | KNN Loss: 4.422662734985352 | BCE Loss: 0.9986270070075989\n",
      "Epoch 428 / 500 | iteration 5 / 30 | Total Loss: 5.395833969116211 | KNN Loss: 4.362049102783203 | BCE Loss: 1.0337848663330078\n",
      "Epoch 428 / 500 | iteration 10 / 30 | Total Loss: 5.390296936035156 | KNN Loss: 4.336329936981201 | BCE Loss: 1.0539672374725342\n",
      "Epoch 428 / 500 | iteration 15 / 30 | Total Loss: 5.3510026931762695 | KNN Loss: 4.337388038635254 | BCE Loss: 1.0136144161224365\n",
      "Epoch 428 / 500 | iteration 20 / 30 | Total Loss: 5.383535385131836 | KNN Loss: 4.361435413360596 | BCE Loss: 1.0220997333526611\n",
      "Epoch 428 / 500 | iteration 25 / 30 | Total Loss: 5.352415561676025 | KNN Loss: 4.311427116394043 | BCE Loss: 1.040988564491272\n",
      "Epoch 429 / 500 | iteration 0 / 30 | Total Loss: 5.419510364532471 | KNN Loss: 4.354089736938477 | BCE Loss: 1.0654206275939941\n",
      "Epoch 429 / 500 | iteration 5 / 30 | Total Loss: 5.428005695343018 | KNN Loss: 4.3959856033325195 | BCE Loss: 1.0320202112197876\n",
      "Epoch 429 / 500 | iteration 10 / 30 | Total Loss: 5.404989719390869 | KNN Loss: 4.364975452423096 | BCE Loss: 1.0400141477584839\n",
      "Epoch 429 / 500 | iteration 15 / 30 | Total Loss: 5.362691879272461 | KNN Loss: 4.3283772468566895 | BCE Loss: 1.0343143939971924\n",
      "Epoch 429 / 500 | iteration 20 / 30 | Total Loss: 5.377292633056641 | KNN Loss: 4.348876476287842 | BCE Loss: 1.0284161567687988\n",
      "Epoch 429 / 500 | iteration 25 / 30 | Total Loss: 5.337376594543457 | KNN Loss: 4.3149895668029785 | BCE Loss: 1.0223867893218994\n",
      "Epoch 430 / 500 | iteration 0 / 30 | Total Loss: 5.386552810668945 | KNN Loss: 4.369936466217041 | BCE Loss: 1.0166165828704834\n",
      "Epoch 430 / 500 | iteration 5 / 30 | Total Loss: 5.326011657714844 | KNN Loss: 4.30593729019165 | BCE Loss: 1.0200746059417725\n",
      "Epoch 430 / 500 | iteration 10 / 30 | Total Loss: 5.392514705657959 | KNN Loss: 4.345338821411133 | BCE Loss: 1.0471757650375366\n",
      "Epoch 430 / 500 | iteration 15 / 30 | Total Loss: 5.399138927459717 | KNN Loss: 4.396786689758301 | BCE Loss: 1.0023523569107056\n",
      "Epoch 430 / 500 | iteration 20 / 30 | Total Loss: 5.440546989440918 | KNN Loss: 4.385032653808594 | BCE Loss: 1.0555142164230347\n",
      "Epoch 430 / 500 | iteration 25 / 30 | Total Loss: 5.359280109405518 | KNN Loss: 4.333826541900635 | BCE Loss: 1.0254534482955933\n",
      "Epoch 431 / 500 | iteration 0 / 30 | Total Loss: 5.358837604522705 | KNN Loss: 4.335214138031006 | BCE Loss: 1.0236234664916992\n",
      "Epoch 431 / 500 | iteration 5 / 30 | Total Loss: 5.4091267585754395 | KNN Loss: 4.404508590698242 | BCE Loss: 1.0046181678771973\n",
      "Epoch 431 / 500 | iteration 10 / 30 | Total Loss: 5.373415946960449 | KNN Loss: 4.340490818023682 | BCE Loss: 1.0329251289367676\n",
      "Epoch 431 / 500 | iteration 15 / 30 | Total Loss: 5.3834028244018555 | KNN Loss: 4.335638046264648 | BCE Loss: 1.0477650165557861\n",
      "Epoch 431 / 500 | iteration 20 / 30 | Total Loss: 5.367765426635742 | KNN Loss: 4.328503608703613 | BCE Loss: 1.0392615795135498\n",
      "Epoch 431 / 500 | iteration 25 / 30 | Total Loss: 5.394509315490723 | KNN Loss: 4.36602783203125 | BCE Loss: 1.0284812450408936\n",
      "Epoch 432 / 500 | iteration 0 / 30 | Total Loss: 5.341259479522705 | KNN Loss: 4.308093547821045 | BCE Loss: 1.0331659317016602\n",
      "Epoch 432 / 500 | iteration 5 / 30 | Total Loss: 5.372199535369873 | KNN Loss: 4.369251251220703 | BCE Loss: 1.00294828414917\n",
      "Epoch 432 / 500 | iteration 10 / 30 | Total Loss: 5.419529914855957 | KNN Loss: 4.376403331756592 | BCE Loss: 1.0431265830993652\n",
      "Epoch 432 / 500 | iteration 15 / 30 | Total Loss: 5.374224662780762 | KNN Loss: 4.314876556396484 | BCE Loss: 1.0593483448028564\n",
      "Epoch 432 / 500 | iteration 20 / 30 | Total Loss: 5.358870506286621 | KNN Loss: 4.3230061531066895 | BCE Loss: 1.0358643531799316\n",
      "Epoch 432 / 500 | iteration 25 / 30 | Total Loss: 5.445730209350586 | KNN Loss: 4.430615425109863 | BCE Loss: 1.0151150226593018\n",
      "Epoch   433: reducing learning rate of group 0 to 3.2856e-07.\n",
      "Epoch 433 / 500 | iteration 0 / 30 | Total Loss: 5.381498336791992 | KNN Loss: 4.365181922912598 | BCE Loss: 1.0163166522979736\n",
      "Epoch 433 / 500 | iteration 5 / 30 | Total Loss: 5.397226810455322 | KNN Loss: 4.337021350860596 | BCE Loss: 1.0602054595947266\n",
      "Epoch 433 / 500 | iteration 10 / 30 | Total Loss: 5.378864288330078 | KNN Loss: 4.353560924530029 | BCE Loss: 1.025303602218628\n",
      "Epoch 433 / 500 | iteration 15 / 30 | Total Loss: 5.360833644866943 | KNN Loss: 4.316805362701416 | BCE Loss: 1.0440281629562378\n",
      "Epoch 433 / 500 | iteration 20 / 30 | Total Loss: 5.319716930389404 | KNN Loss: 4.313100814819336 | BCE Loss: 1.0066161155700684\n",
      "Epoch 433 / 500 | iteration 25 / 30 | Total Loss: 5.434330463409424 | KNN Loss: 4.379951000213623 | BCE Loss: 1.0543795824050903\n",
      "Epoch 434 / 500 | iteration 0 / 30 | Total Loss: 5.384090423583984 | KNN Loss: 4.367495536804199 | BCE Loss: 1.016594648361206\n",
      "Epoch 434 / 500 | iteration 5 / 30 | Total Loss: 5.426750183105469 | KNN Loss: 4.408782005310059 | BCE Loss: 1.0179682970046997\n",
      "Epoch 434 / 500 | iteration 10 / 30 | Total Loss: 5.352680206298828 | KNN Loss: 4.323706150054932 | BCE Loss: 1.0289740562438965\n",
      "Epoch 434 / 500 | iteration 15 / 30 | Total Loss: 5.359626770019531 | KNN Loss: 4.325253486633301 | BCE Loss: 1.0343730449676514\n",
      "Epoch 434 / 500 | iteration 20 / 30 | Total Loss: 5.40397310256958 | KNN Loss: 4.37030029296875 | BCE Loss: 1.0336729288101196\n",
      "Epoch 434 / 500 | iteration 25 / 30 | Total Loss: 5.376798152923584 | KNN Loss: 4.34897518157959 | BCE Loss: 1.0278229713439941\n",
      "Epoch 435 / 500 | iteration 0 / 30 | Total Loss: 5.357166290283203 | KNN Loss: 4.3511457443237305 | BCE Loss: 1.0060207843780518\n",
      "Epoch 435 / 500 | iteration 5 / 30 | Total Loss: 5.3323845863342285 | KNN Loss: 4.320838928222656 | BCE Loss: 1.0115456581115723\n",
      "Epoch 435 / 500 | iteration 10 / 30 | Total Loss: 5.401281833648682 | KNN Loss: 4.362729072570801 | BCE Loss: 1.0385527610778809\n",
      "Epoch 435 / 500 | iteration 15 / 30 | Total Loss: 5.369644641876221 | KNN Loss: 4.37369441986084 | BCE Loss: 0.9959500432014465\n",
      "Epoch 435 / 500 | iteration 20 / 30 | Total Loss: 5.38731575012207 | KNN Loss: 4.338647842407227 | BCE Loss: 1.0486679077148438\n",
      "Epoch 435 / 500 | iteration 25 / 30 | Total Loss: 5.364042282104492 | KNN Loss: 4.343063831329346 | BCE Loss: 1.0209782123565674\n",
      "Epoch 436 / 500 | iteration 0 / 30 | Total Loss: 5.321464538574219 | KNN Loss: 4.3070268630981445 | BCE Loss: 1.0144376754760742\n",
      "Epoch 436 / 500 | iteration 5 / 30 | Total Loss: 5.344273567199707 | KNN Loss: 4.317174434661865 | BCE Loss: 1.0270992517471313\n",
      "Epoch 436 / 500 | iteration 10 / 30 | Total Loss: 5.431999206542969 | KNN Loss: 4.3893141746521 | BCE Loss: 1.0426850318908691\n",
      "Epoch 436 / 500 | iteration 15 / 30 | Total Loss: 5.376285552978516 | KNN Loss: 4.35073709487915 | BCE Loss: 1.0255486965179443\n",
      "Epoch 436 / 500 | iteration 20 / 30 | Total Loss: 5.434352874755859 | KNN Loss: 4.377567768096924 | BCE Loss: 1.0567853450775146\n",
      "Epoch 436 / 500 | iteration 25 / 30 | Total Loss: 5.387862205505371 | KNN Loss: 4.360624313354492 | BCE Loss: 1.027238130569458\n",
      "Epoch 437 / 500 | iteration 0 / 30 | Total Loss: 5.372689247131348 | KNN Loss: 4.376439094543457 | BCE Loss: 0.9962501525878906\n",
      "Epoch 437 / 500 | iteration 5 / 30 | Total Loss: 5.370521545410156 | KNN Loss: 4.332686424255371 | BCE Loss: 1.0378351211547852\n",
      "Epoch 437 / 500 | iteration 10 / 30 | Total Loss: 5.359770774841309 | KNN Loss: 4.3450493812561035 | BCE Loss: 1.0147216320037842\n",
      "Epoch 437 / 500 | iteration 15 / 30 | Total Loss: 5.452350616455078 | KNN Loss: 4.383338451385498 | BCE Loss: 1.06901216506958\n",
      "Epoch 437 / 500 | iteration 20 / 30 | Total Loss: 5.342038154602051 | KNN Loss: 4.33612060546875 | BCE Loss: 1.0059173107147217\n",
      "Epoch 437 / 500 | iteration 25 / 30 | Total Loss: 5.369680881500244 | KNN Loss: 4.33912992477417 | BCE Loss: 1.0305509567260742\n",
      "Epoch 438 / 500 | iteration 0 / 30 | Total Loss: 5.395700931549072 | KNN Loss: 4.38585901260376 | BCE Loss: 1.009842038154602\n",
      "Epoch 438 / 500 | iteration 5 / 30 | Total Loss: 5.413577079772949 | KNN Loss: 4.361459732055664 | BCE Loss: 1.052117109298706\n",
      "Epoch 438 / 500 | iteration 10 / 30 | Total Loss: 5.349684715270996 | KNN Loss: 4.328301429748535 | BCE Loss: 1.0213831663131714\n",
      "Epoch 438 / 500 | iteration 15 / 30 | Total Loss: 5.355838775634766 | KNN Loss: 4.336956977844238 | BCE Loss: 1.0188816785812378\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 438 / 500 | iteration 20 / 30 | Total Loss: 5.4169511795043945 | KNN Loss: 4.378000259399414 | BCE Loss: 1.0389506816864014\n",
      "Epoch 438 / 500 | iteration 25 / 30 | Total Loss: 5.374704360961914 | KNN Loss: 4.360998630523682 | BCE Loss: 1.0137059688568115\n",
      "Epoch 439 / 500 | iteration 0 / 30 | Total Loss: 5.400293350219727 | KNN Loss: 4.336090564727783 | BCE Loss: 1.0642027854919434\n",
      "Epoch 439 / 500 | iteration 5 / 30 | Total Loss: 5.392294883728027 | KNN Loss: 4.387814998626709 | BCE Loss: 1.0044798851013184\n",
      "Epoch 439 / 500 | iteration 10 / 30 | Total Loss: 5.423892498016357 | KNN Loss: 4.417397499084473 | BCE Loss: 1.0064949989318848\n",
      "Epoch 439 / 500 | iteration 15 / 30 | Total Loss: 5.377165794372559 | KNN Loss: 4.338637828826904 | BCE Loss: 1.0385277271270752\n",
      "Epoch 439 / 500 | iteration 20 / 30 | Total Loss: 5.317832946777344 | KNN Loss: 4.313119411468506 | BCE Loss: 1.004713773727417\n",
      "Epoch 439 / 500 | iteration 25 / 30 | Total Loss: 5.35314416885376 | KNN Loss: 4.353750705718994 | BCE Loss: 0.9993933439254761\n",
      "Epoch 440 / 500 | iteration 0 / 30 | Total Loss: 5.38178825378418 | KNN Loss: 4.330704689025879 | BCE Loss: 1.0510836839675903\n",
      "Epoch 440 / 500 | iteration 5 / 30 | Total Loss: 5.345036029815674 | KNN Loss: 4.336232662200928 | BCE Loss: 1.0088032484054565\n",
      "Epoch 440 / 500 | iteration 10 / 30 | Total Loss: 5.40798282623291 | KNN Loss: 4.36683988571167 | BCE Loss: 1.0411431789398193\n",
      "Epoch 440 / 500 | iteration 15 / 30 | Total Loss: 5.4301557540893555 | KNN Loss: 4.390525817871094 | BCE Loss: 1.0396299362182617\n",
      "Epoch 440 / 500 | iteration 20 / 30 | Total Loss: 5.365048885345459 | KNN Loss: 4.35089111328125 | BCE Loss: 1.014157772064209\n",
      "Epoch 440 / 500 | iteration 25 / 30 | Total Loss: 5.394463062286377 | KNN Loss: 4.360750198364258 | BCE Loss: 1.0337127447128296\n",
      "Epoch 441 / 500 | iteration 0 / 30 | Total Loss: 5.359313488006592 | KNN Loss: 4.337487697601318 | BCE Loss: 1.0218257904052734\n",
      "Epoch 441 / 500 | iteration 5 / 30 | Total Loss: 5.331463813781738 | KNN Loss: 4.333735466003418 | BCE Loss: 0.9977283477783203\n",
      "Epoch 441 / 500 | iteration 10 / 30 | Total Loss: 5.370628356933594 | KNN Loss: 4.330386638641357 | BCE Loss: 1.0402417182922363\n",
      "Epoch 441 / 500 | iteration 15 / 30 | Total Loss: 5.344050407409668 | KNN Loss: 4.313983917236328 | BCE Loss: 1.0300663709640503\n",
      "Epoch 441 / 500 | iteration 20 / 30 | Total Loss: 5.403806686401367 | KNN Loss: 4.389609336853027 | BCE Loss: 1.0141973495483398\n",
      "Epoch 441 / 500 | iteration 25 / 30 | Total Loss: 5.370475769042969 | KNN Loss: 4.347404479980469 | BCE Loss: 1.023071050643921\n",
      "Epoch 442 / 500 | iteration 0 / 30 | Total Loss: 5.418374061584473 | KNN Loss: 4.3634843826293945 | BCE Loss: 1.0548896789550781\n",
      "Epoch 442 / 500 | iteration 5 / 30 | Total Loss: 5.383171558380127 | KNN Loss: 4.353439807891846 | BCE Loss: 1.0297318696975708\n",
      "Epoch 442 / 500 | iteration 10 / 30 | Total Loss: 5.36181640625 | KNN Loss: 4.329824924468994 | BCE Loss: 1.0319916009902954\n",
      "Epoch 442 / 500 | iteration 15 / 30 | Total Loss: 5.395397663116455 | KNN Loss: 4.370067596435547 | BCE Loss: 1.0253300666809082\n",
      "Epoch 442 / 500 | iteration 20 / 30 | Total Loss: 5.381556510925293 | KNN Loss: 4.34359884262085 | BCE Loss: 1.0379579067230225\n",
      "Epoch 442 / 500 | iteration 25 / 30 | Total Loss: 5.388394355773926 | KNN Loss: 4.369861125946045 | BCE Loss: 1.0185332298278809\n",
      "Epoch 443 / 500 | iteration 0 / 30 | Total Loss: 5.359604835510254 | KNN Loss: 4.337245464324951 | BCE Loss: 1.0223594903945923\n",
      "Epoch 443 / 500 | iteration 5 / 30 | Total Loss: 5.363419532775879 | KNN Loss: 4.3262410163879395 | BCE Loss: 1.0371782779693604\n",
      "Epoch 443 / 500 | iteration 10 / 30 | Total Loss: 5.396225929260254 | KNN Loss: 4.37274694442749 | BCE Loss: 1.0234789848327637\n",
      "Epoch 443 / 500 | iteration 15 / 30 | Total Loss: 5.420168399810791 | KNN Loss: 4.373101711273193 | BCE Loss: 1.047066569328308\n",
      "Epoch 443 / 500 | iteration 20 / 30 | Total Loss: 5.3587493896484375 | KNN Loss: 4.363403797149658 | BCE Loss: 0.9953455328941345\n",
      "Epoch 443 / 500 | iteration 25 / 30 | Total Loss: 5.397106647491455 | KNN Loss: 4.382660865783691 | BCE Loss: 1.0144457817077637\n",
      "Epoch   444: reducing learning rate of group 0 to 2.2999e-07.\n",
      "Epoch 444 / 500 | iteration 0 / 30 | Total Loss: 5.326465129852295 | KNN Loss: 4.3200883865356445 | BCE Loss: 1.00637686252594\n",
      "Epoch 444 / 500 | iteration 5 / 30 | Total Loss: 5.356195449829102 | KNN Loss: 4.343513011932373 | BCE Loss: 1.0126826763153076\n",
      "Epoch 444 / 500 | iteration 10 / 30 | Total Loss: 5.4131760597229 | KNN Loss: 4.411115646362305 | BCE Loss: 1.0020604133605957\n",
      "Epoch 444 / 500 | iteration 15 / 30 | Total Loss: 5.33270788192749 | KNN Loss: 4.32355260848999 | BCE Loss: 1.0091553926467896\n",
      "Epoch 444 / 500 | iteration 20 / 30 | Total Loss: 5.34853458404541 | KNN Loss: 4.308176517486572 | BCE Loss: 1.0403581857681274\n",
      "Epoch 444 / 500 | iteration 25 / 30 | Total Loss: 5.385744094848633 | KNN Loss: 4.338748931884766 | BCE Loss: 1.0469954013824463\n",
      "Epoch 445 / 500 | iteration 0 / 30 | Total Loss: 5.430379390716553 | KNN Loss: 4.3838276863098145 | BCE Loss: 1.0465515851974487\n",
      "Epoch 445 / 500 | iteration 5 / 30 | Total Loss: 5.35498046875 | KNN Loss: 4.340020656585693 | BCE Loss: 1.0149599313735962\n",
      "Epoch 445 / 500 | iteration 10 / 30 | Total Loss: 5.371870517730713 | KNN Loss: 4.36018180847168 | BCE Loss: 1.0116887092590332\n",
      "Epoch 445 / 500 | iteration 15 / 30 | Total Loss: 5.429363250732422 | KNN Loss: 4.393415451049805 | BCE Loss: 1.0359477996826172\n",
      "Epoch 445 / 500 | iteration 20 / 30 | Total Loss: 5.425634860992432 | KNN Loss: 4.4055867195129395 | BCE Loss: 1.0200481414794922\n",
      "Epoch 445 / 500 | iteration 25 / 30 | Total Loss: 5.392343521118164 | KNN Loss: 4.391648292541504 | BCE Loss: 1.000694990158081\n",
      "Epoch 446 / 500 | iteration 0 / 30 | Total Loss: 5.3571858406066895 | KNN Loss: 4.340641498565674 | BCE Loss: 1.0165444612503052\n",
      "Epoch 446 / 500 | iteration 5 / 30 | Total Loss: 5.330625534057617 | KNN Loss: 4.311758518218994 | BCE Loss: 1.018867015838623\n",
      "Epoch 446 / 500 | iteration 10 / 30 | Total Loss: 5.36691951751709 | KNN Loss: 4.342828273773193 | BCE Loss: 1.0240910053253174\n",
      "Epoch 446 / 500 | iteration 15 / 30 | Total Loss: 5.3396430015563965 | KNN Loss: 4.311519622802734 | BCE Loss: 1.028123378753662\n",
      "Epoch 446 / 500 | iteration 20 / 30 | Total Loss: 5.39755916595459 | KNN Loss: 4.37233304977417 | BCE Loss: 1.0252258777618408\n",
      "Epoch 446 / 500 | iteration 25 / 30 | Total Loss: 5.410617351531982 | KNN Loss: 4.371858596801758 | BCE Loss: 1.0387587547302246\n",
      "Epoch 447 / 500 | iteration 0 / 30 | Total Loss: 5.35978889465332 | KNN Loss: 4.345944404602051 | BCE Loss: 1.01384437084198\n",
      "Epoch 447 / 500 | iteration 5 / 30 | Total Loss: 5.327310562133789 | KNN Loss: 4.312441349029541 | BCE Loss: 1.0148690938949585\n",
      "Epoch 447 / 500 | iteration 10 / 30 | Total Loss: 5.30623722076416 | KNN Loss: 4.294806957244873 | BCE Loss: 1.011430263519287\n",
      "Epoch 447 / 500 | iteration 15 / 30 | Total Loss: 5.390120506286621 | KNN Loss: 4.350071907043457 | BCE Loss: 1.040048599243164\n",
      "Epoch 447 / 500 | iteration 20 / 30 | Total Loss: 5.361275672912598 | KNN Loss: 4.357789039611816 | BCE Loss: 1.0034865140914917\n",
      "Epoch 447 / 500 | iteration 25 / 30 | Total Loss: 5.448953628540039 | KNN Loss: 4.41800594329834 | BCE Loss: 1.0309476852416992\n",
      "Epoch 448 / 500 | iteration 0 / 30 | Total Loss: 5.409490585327148 | KNN Loss: 4.374682426452637 | BCE Loss: 1.0348080396652222\n",
      "Epoch 448 / 500 | iteration 5 / 30 | Total Loss: 5.368525505065918 | KNN Loss: 4.3343729972839355 | BCE Loss: 1.0341523885726929\n",
      "Epoch 448 / 500 | iteration 10 / 30 | Total Loss: 5.377184867858887 | KNN Loss: 4.363465785980225 | BCE Loss: 1.013719081878662\n",
      "Epoch 448 / 500 | iteration 15 / 30 | Total Loss: 5.346257209777832 | KNN Loss: 4.325450420379639 | BCE Loss: 1.0208065509796143\n",
      "Epoch 448 / 500 | iteration 20 / 30 | Total Loss: 5.434630393981934 | KNN Loss: 4.390064239501953 | BCE Loss: 1.04456627368927\n",
      "Epoch 448 / 500 | iteration 25 / 30 | Total Loss: 5.410877704620361 | KNN Loss: 4.383070468902588 | BCE Loss: 1.0278072357177734\n",
      "Epoch 449 / 500 | iteration 0 / 30 | Total Loss: 5.384572982788086 | KNN Loss: 4.380979061126709 | BCE Loss: 1.003593921661377\n",
      "Epoch 449 / 500 | iteration 5 / 30 | Total Loss: 5.3598103523254395 | KNN Loss: 4.353435039520264 | BCE Loss: 1.0063753128051758\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 449 / 500 | iteration 10 / 30 | Total Loss: 5.423148155212402 | KNN Loss: 4.35068416595459 | BCE Loss: 1.0724639892578125\n",
      "Epoch 449 / 500 | iteration 15 / 30 | Total Loss: 5.461158275604248 | KNN Loss: 4.419189453125 | BCE Loss: 1.0419687032699585\n",
      "Epoch 449 / 500 | iteration 20 / 30 | Total Loss: 5.389246463775635 | KNN Loss: 4.370909690856934 | BCE Loss: 1.0183367729187012\n",
      "Epoch 449 / 500 | iteration 25 / 30 | Total Loss: 5.395580768585205 | KNN Loss: 4.372767448425293 | BCE Loss: 1.0228132009506226\n",
      "Epoch 450 / 500 | iteration 0 / 30 | Total Loss: 5.377960205078125 | KNN Loss: 4.34993839263916 | BCE Loss: 1.0280218124389648\n",
      "Epoch 450 / 500 | iteration 5 / 30 | Total Loss: 5.393111228942871 | KNN Loss: 4.352309703826904 | BCE Loss: 1.0408015251159668\n",
      "Epoch 450 / 500 | iteration 10 / 30 | Total Loss: 5.331720352172852 | KNN Loss: 4.331087589263916 | BCE Loss: 1.0006327629089355\n",
      "Epoch 450 / 500 | iteration 15 / 30 | Total Loss: 5.380775451660156 | KNN Loss: 4.344493389129639 | BCE Loss: 1.0362818241119385\n",
      "Epoch 450 / 500 | iteration 20 / 30 | Total Loss: 5.435630798339844 | KNN Loss: 4.392556190490723 | BCE Loss: 1.0430747270584106\n",
      "Epoch 450 / 500 | iteration 25 / 30 | Total Loss: 5.3497514724731445 | KNN Loss: 4.32857084274292 | BCE Loss: 1.0211808681488037\n",
      "Epoch 451 / 500 | iteration 0 / 30 | Total Loss: 5.402265548706055 | KNN Loss: 4.371221542358398 | BCE Loss: 1.0310442447662354\n",
      "Epoch 451 / 500 | iteration 5 / 30 | Total Loss: 5.367636680603027 | KNN Loss: 4.345853328704834 | BCE Loss: 1.0217835903167725\n",
      "Epoch 451 / 500 | iteration 10 / 30 | Total Loss: 5.374967098236084 | KNN Loss: 4.346855640411377 | BCE Loss: 1.0281113386154175\n",
      "Epoch 451 / 500 | iteration 15 / 30 | Total Loss: 5.417573928833008 | KNN Loss: 4.391923427581787 | BCE Loss: 1.0256502628326416\n",
      "Epoch 451 / 500 | iteration 20 / 30 | Total Loss: 5.391570091247559 | KNN Loss: 4.3545098304748535 | BCE Loss: 1.037060260772705\n",
      "Epoch 451 / 500 | iteration 25 / 30 | Total Loss: 5.411282539367676 | KNN Loss: 4.377974987030029 | BCE Loss: 1.0333075523376465\n",
      "Epoch 452 / 500 | iteration 0 / 30 | Total Loss: 5.353815078735352 | KNN Loss: 4.325195789337158 | BCE Loss: 1.0286195278167725\n",
      "Epoch 452 / 500 | iteration 5 / 30 | Total Loss: 5.415977954864502 | KNN Loss: 4.370410919189453 | BCE Loss: 1.0455671548843384\n",
      "Epoch 452 / 500 | iteration 10 / 30 | Total Loss: 5.393102169036865 | KNN Loss: 4.352722644805908 | BCE Loss: 1.040379524230957\n",
      "Epoch 452 / 500 | iteration 15 / 30 | Total Loss: 5.3513078689575195 | KNN Loss: 4.339317321777344 | BCE Loss: 1.0119905471801758\n",
      "Epoch 452 / 500 | iteration 20 / 30 | Total Loss: 5.450948715209961 | KNN Loss: 4.398467540740967 | BCE Loss: 1.0524812936782837\n",
      "Epoch 452 / 500 | iteration 25 / 30 | Total Loss: 5.39641809463501 | KNN Loss: 4.356016159057617 | BCE Loss: 1.0404019355773926\n",
      "Epoch 453 / 500 | iteration 0 / 30 | Total Loss: 5.403326034545898 | KNN Loss: 4.367246627807617 | BCE Loss: 1.0360791683197021\n",
      "Epoch 453 / 500 | iteration 5 / 30 | Total Loss: 5.399759769439697 | KNN Loss: 4.378666877746582 | BCE Loss: 1.0210928916931152\n",
      "Epoch 453 / 500 | iteration 10 / 30 | Total Loss: 5.355263710021973 | KNN Loss: 4.351233959197998 | BCE Loss: 1.0040295124053955\n",
      "Epoch 453 / 500 | iteration 15 / 30 | Total Loss: 5.360222339630127 | KNN Loss: 4.338516712188721 | BCE Loss: 1.0217055082321167\n",
      "Epoch 453 / 500 | iteration 20 / 30 | Total Loss: 5.376971244812012 | KNN Loss: 4.348100662231445 | BCE Loss: 1.028870701789856\n",
      "Epoch 453 / 500 | iteration 25 / 30 | Total Loss: 5.385580062866211 | KNN Loss: 4.375985145568848 | BCE Loss: 1.0095951557159424\n",
      "Epoch 454 / 500 | iteration 0 / 30 | Total Loss: 5.412257194519043 | KNN Loss: 4.358740329742432 | BCE Loss: 1.0535171031951904\n",
      "Epoch 454 / 500 | iteration 5 / 30 | Total Loss: 5.354816913604736 | KNN Loss: 4.340790271759033 | BCE Loss: 1.0140266418457031\n",
      "Epoch 454 / 500 | iteration 10 / 30 | Total Loss: 5.355701923370361 | KNN Loss: 4.354413032531738 | BCE Loss: 1.001288890838623\n",
      "Epoch 454 / 500 | iteration 15 / 30 | Total Loss: 5.379406929016113 | KNN Loss: 4.337745189666748 | BCE Loss: 1.0416615009307861\n",
      "Epoch 454 / 500 | iteration 20 / 30 | Total Loss: 5.349503993988037 | KNN Loss: 4.326888561248779 | BCE Loss: 1.0226153135299683\n",
      "Epoch 454 / 500 | iteration 25 / 30 | Total Loss: 5.38175630569458 | KNN Loss: 4.357329845428467 | BCE Loss: 1.0244264602661133\n",
      "Epoch   455: reducing learning rate of group 0 to 1.6100e-07.\n",
      "Epoch 455 / 500 | iteration 0 / 30 | Total Loss: 5.413802146911621 | KNN Loss: 4.356362342834473 | BCE Loss: 1.0574395656585693\n",
      "Epoch 455 / 500 | iteration 5 / 30 | Total Loss: 5.327927589416504 | KNN Loss: 4.302964210510254 | BCE Loss: 1.024963617324829\n",
      "Epoch 455 / 500 | iteration 10 / 30 | Total Loss: 5.344505310058594 | KNN Loss: 4.3314738273620605 | BCE Loss: 1.0130314826965332\n",
      "Epoch 455 / 500 | iteration 15 / 30 | Total Loss: 5.40065336227417 | KNN Loss: 4.366183280944824 | BCE Loss: 1.0344700813293457\n",
      "Epoch 455 / 500 | iteration 20 / 30 | Total Loss: 5.395492076873779 | KNN Loss: 4.3737568855285645 | BCE Loss: 1.0217351913452148\n",
      "Epoch 455 / 500 | iteration 25 / 30 | Total Loss: 5.426490783691406 | KNN Loss: 4.374688625335693 | BCE Loss: 1.051802158355713\n",
      "Epoch 456 / 500 | iteration 0 / 30 | Total Loss: 5.415474891662598 | KNN Loss: 4.362328052520752 | BCE Loss: 1.0531470775604248\n",
      "Epoch 456 / 500 | iteration 5 / 30 | Total Loss: 5.360596656799316 | KNN Loss: 4.335083961486816 | BCE Loss: 1.025512456893921\n",
      "Epoch 456 / 500 | iteration 10 / 30 | Total Loss: 5.433633804321289 | KNN Loss: 4.391151428222656 | BCE Loss: 1.042482614517212\n",
      "Epoch 456 / 500 | iteration 15 / 30 | Total Loss: 5.4263715744018555 | KNN Loss: 4.364335060119629 | BCE Loss: 1.0620367527008057\n",
      "Epoch 456 / 500 | iteration 20 / 30 | Total Loss: 5.4141459465026855 | KNN Loss: 4.391897678375244 | BCE Loss: 1.0222482681274414\n",
      "Epoch 456 / 500 | iteration 25 / 30 | Total Loss: 5.3618292808532715 | KNN Loss: 4.362360000610352 | BCE Loss: 0.9994692802429199\n",
      "Epoch 457 / 500 | iteration 0 / 30 | Total Loss: 5.392523765563965 | KNN Loss: 4.370777606964111 | BCE Loss: 1.021746039390564\n",
      "Epoch 457 / 500 | iteration 5 / 30 | Total Loss: 5.362525939941406 | KNN Loss: 4.3539018630981445 | BCE Loss: 1.0086238384246826\n",
      "Epoch 457 / 500 | iteration 10 / 30 | Total Loss: 5.332849502563477 | KNN Loss: 4.330092906951904 | BCE Loss: 1.0027565956115723\n",
      "Epoch 457 / 500 | iteration 15 / 30 | Total Loss: 5.362632751464844 | KNN Loss: 4.344433784484863 | BCE Loss: 1.0181987285614014\n",
      "Epoch 457 / 500 | iteration 20 / 30 | Total Loss: 5.3947625160217285 | KNN Loss: 4.355950832366943 | BCE Loss: 1.0388118028640747\n",
      "Epoch 457 / 500 | iteration 25 / 30 | Total Loss: 5.355661392211914 | KNN Loss: 4.329406261444092 | BCE Loss: 1.0262552499771118\n",
      "Epoch 458 / 500 | iteration 0 / 30 | Total Loss: 5.300012588500977 | KNN Loss: 4.299222469329834 | BCE Loss: 1.000789999961853\n",
      "Epoch 458 / 500 | iteration 5 / 30 | Total Loss: 5.408927917480469 | KNN Loss: 4.368223190307617 | BCE Loss: 1.0407048463821411\n",
      "Epoch 458 / 500 | iteration 10 / 30 | Total Loss: 5.332283973693848 | KNN Loss: 4.342062950134277 | BCE Loss: 0.990220844745636\n",
      "Epoch 458 / 500 | iteration 15 / 30 | Total Loss: 5.404561996459961 | KNN Loss: 4.366972923278809 | BCE Loss: 1.0375893115997314\n",
      "Epoch 458 / 500 | iteration 20 / 30 | Total Loss: 5.377070903778076 | KNN Loss: 4.368214130401611 | BCE Loss: 1.0088567733764648\n",
      "Epoch 458 / 500 | iteration 25 / 30 | Total Loss: 5.369984149932861 | KNN Loss: 4.349734306335449 | BCE Loss: 1.020249843597412\n",
      "Epoch 459 / 500 | iteration 0 / 30 | Total Loss: 5.418244361877441 | KNN Loss: 4.371448516845703 | BCE Loss: 1.0467956066131592\n",
      "Epoch 459 / 500 | iteration 5 / 30 | Total Loss: 5.354631423950195 | KNN Loss: 4.328057289123535 | BCE Loss: 1.0265740156173706\n",
      "Epoch 459 / 500 | iteration 10 / 30 | Total Loss: 5.371773719787598 | KNN Loss: 4.3709940910339355 | BCE Loss: 1.000779628753662\n",
      "Epoch 459 / 500 | iteration 15 / 30 | Total Loss: 5.380401134490967 | KNN Loss: 4.324671268463135 | BCE Loss: 1.055729866027832\n",
      "Epoch 459 / 500 | iteration 20 / 30 | Total Loss: 5.38028621673584 | KNN Loss: 4.360862731933594 | BCE Loss: 1.0194236040115356\n",
      "Epoch 459 / 500 | iteration 25 / 30 | Total Loss: 5.369575500488281 | KNN Loss: 4.325686931610107 | BCE Loss: 1.0438885688781738\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 460 / 500 | iteration 0 / 30 | Total Loss: 5.386999607086182 | KNN Loss: 4.380929946899414 | BCE Loss: 1.0060697793960571\n",
      "Epoch 460 / 500 | iteration 5 / 30 | Total Loss: 5.334714889526367 | KNN Loss: 4.324290752410889 | BCE Loss: 1.0104238986968994\n",
      "Epoch 460 / 500 | iteration 10 / 30 | Total Loss: 5.385002613067627 | KNN Loss: 4.346275806427002 | BCE Loss: 1.0387266874313354\n",
      "Epoch 460 / 500 | iteration 15 / 30 | Total Loss: 5.4249653816223145 | KNN Loss: 4.3769330978393555 | BCE Loss: 1.0480321645736694\n",
      "Epoch 460 / 500 | iteration 20 / 30 | Total Loss: 5.290914535522461 | KNN Loss: 4.295884132385254 | BCE Loss: 0.9950305819511414\n",
      "Epoch 460 / 500 | iteration 25 / 30 | Total Loss: 5.386981010437012 | KNN Loss: 4.3875532150268555 | BCE Loss: 0.9994277954101562\n",
      "Epoch 461 / 500 | iteration 0 / 30 | Total Loss: 5.370757579803467 | KNN Loss: 4.334097385406494 | BCE Loss: 1.0366601943969727\n",
      "Epoch 461 / 500 | iteration 5 / 30 | Total Loss: 5.3638505935668945 | KNN Loss: 4.335745811462402 | BCE Loss: 1.028104543685913\n",
      "Epoch 461 / 500 | iteration 10 / 30 | Total Loss: 5.362905502319336 | KNN Loss: 4.32277250289917 | BCE Loss: 1.040132761001587\n",
      "Epoch 461 / 500 | iteration 15 / 30 | Total Loss: 5.3452887535095215 | KNN Loss: 4.327888488769531 | BCE Loss: 1.0174002647399902\n",
      "Epoch 461 / 500 | iteration 20 / 30 | Total Loss: 5.405063629150391 | KNN Loss: 4.371913433074951 | BCE Loss: 1.0331504344940186\n",
      "Epoch 461 / 500 | iteration 25 / 30 | Total Loss: 5.335080146789551 | KNN Loss: 4.316658020019531 | BCE Loss: 1.0184221267700195\n",
      "Epoch 462 / 500 | iteration 0 / 30 | Total Loss: 5.4354424476623535 | KNN Loss: 4.396437168121338 | BCE Loss: 1.0390053987503052\n",
      "Epoch 462 / 500 | iteration 5 / 30 | Total Loss: 5.417137145996094 | KNN Loss: 4.352532386779785 | BCE Loss: 1.0646045207977295\n",
      "Epoch 462 / 500 | iteration 10 / 30 | Total Loss: 5.348008632659912 | KNN Loss: 4.3223137855529785 | BCE Loss: 1.0256949663162231\n",
      "Epoch 462 / 500 | iteration 15 / 30 | Total Loss: 5.361069202423096 | KNN Loss: 4.344191074371338 | BCE Loss: 1.0168782472610474\n",
      "Epoch 462 / 500 | iteration 20 / 30 | Total Loss: 5.369550704956055 | KNN Loss: 4.3343658447265625 | BCE Loss: 1.035184621810913\n",
      "Epoch 462 / 500 | iteration 25 / 30 | Total Loss: 5.394195556640625 | KNN Loss: 4.373548984527588 | BCE Loss: 1.020646333694458\n",
      "Epoch 463 / 500 | iteration 0 / 30 | Total Loss: 5.356017112731934 | KNN Loss: 4.341794967651367 | BCE Loss: 1.0142223834991455\n",
      "Epoch 463 / 500 | iteration 5 / 30 | Total Loss: 5.351846694946289 | KNN Loss: 4.31804084777832 | BCE Loss: 1.0338058471679688\n",
      "Epoch 463 / 500 | iteration 10 / 30 | Total Loss: 5.390763282775879 | KNN Loss: 4.359679698944092 | BCE Loss: 1.0310837030410767\n",
      "Epoch 463 / 500 | iteration 15 / 30 | Total Loss: 5.380005359649658 | KNN Loss: 4.354602336883545 | BCE Loss: 1.0254031419754028\n",
      "Epoch 463 / 500 | iteration 20 / 30 | Total Loss: 5.358954429626465 | KNN Loss: 4.330729961395264 | BCE Loss: 1.0282244682312012\n",
      "Epoch 463 / 500 | iteration 25 / 30 | Total Loss: 5.3762407302856445 | KNN Loss: 4.360790252685547 | BCE Loss: 1.0154504776000977\n",
      "Epoch 464 / 500 | iteration 0 / 30 | Total Loss: 5.369827747344971 | KNN Loss: 4.368327617645264 | BCE Loss: 1.0015002489089966\n",
      "Epoch 464 / 500 | iteration 5 / 30 | Total Loss: 5.3293914794921875 | KNN Loss: 4.327436447143555 | BCE Loss: 1.001955270767212\n",
      "Epoch 464 / 500 | iteration 10 / 30 | Total Loss: 5.425433158874512 | KNN Loss: 4.375146389007568 | BCE Loss: 1.0502867698669434\n",
      "Epoch 464 / 500 | iteration 15 / 30 | Total Loss: 5.362620830535889 | KNN Loss: 4.326338768005371 | BCE Loss: 1.0362821817398071\n",
      "Epoch 464 / 500 | iteration 20 / 30 | Total Loss: 5.434146881103516 | KNN Loss: 4.382823467254639 | BCE Loss: 1.051323413848877\n",
      "Epoch 464 / 500 | iteration 25 / 30 | Total Loss: 5.407138347625732 | KNN Loss: 4.359851360321045 | BCE Loss: 1.0472869873046875\n",
      "Epoch 465 / 500 | iteration 0 / 30 | Total Loss: 5.410181999206543 | KNN Loss: 4.384875774383545 | BCE Loss: 1.025306224822998\n",
      "Epoch 465 / 500 | iteration 5 / 30 | Total Loss: 5.331265449523926 | KNN Loss: 4.300483703613281 | BCE Loss: 1.030781626701355\n",
      "Epoch 465 / 500 | iteration 10 / 30 | Total Loss: 5.43482780456543 | KNN Loss: 4.388961315155029 | BCE Loss: 1.0458662509918213\n",
      "Epoch 465 / 500 | iteration 15 / 30 | Total Loss: 5.36566686630249 | KNN Loss: 4.3347859382629395 | BCE Loss: 1.0308809280395508\n",
      "Epoch 465 / 500 | iteration 20 / 30 | Total Loss: 5.352590560913086 | KNN Loss: 4.336377143859863 | BCE Loss: 1.0162131786346436\n",
      "Epoch 465 / 500 | iteration 25 / 30 | Total Loss: 5.4011664390563965 | KNN Loss: 4.364410877227783 | BCE Loss: 1.0367556810379028\n",
      "Epoch   466: reducing learning rate of group 0 to 1.1270e-07.\n",
      "Epoch 466 / 500 | iteration 0 / 30 | Total Loss: 5.398702144622803 | KNN Loss: 4.353869438171387 | BCE Loss: 1.0448325872421265\n",
      "Epoch 466 / 500 | iteration 5 / 30 | Total Loss: 5.38342809677124 | KNN Loss: 4.340523719787598 | BCE Loss: 1.0429044961929321\n",
      "Epoch 466 / 500 | iteration 10 / 30 | Total Loss: 5.394559860229492 | KNN Loss: 4.344228744506836 | BCE Loss: 1.0503312349319458\n",
      "Epoch 466 / 500 | iteration 15 / 30 | Total Loss: 5.365419387817383 | KNN Loss: 4.33847188949585 | BCE Loss: 1.0269476175308228\n",
      "Epoch 466 / 500 | iteration 20 / 30 | Total Loss: 5.363495826721191 | KNN Loss: 4.353424549102783 | BCE Loss: 1.010071039199829\n",
      "Epoch 466 / 500 | iteration 25 / 30 | Total Loss: 5.345900535583496 | KNN Loss: 4.329555988311768 | BCE Loss: 1.0163445472717285\n",
      "Epoch 467 / 500 | iteration 0 / 30 | Total Loss: 5.452325344085693 | KNN Loss: 4.405325412750244 | BCE Loss: 1.0470000505447388\n",
      "Epoch 467 / 500 | iteration 5 / 30 | Total Loss: 5.391826629638672 | KNN Loss: 4.367768287658691 | BCE Loss: 1.0240581035614014\n",
      "Epoch 467 / 500 | iteration 10 / 30 | Total Loss: 5.346368312835693 | KNN Loss: 4.342288494110107 | BCE Loss: 1.0040796995162964\n",
      "Epoch 467 / 500 | iteration 15 / 30 | Total Loss: 5.378479480743408 | KNN Loss: 4.35509729385376 | BCE Loss: 1.0233820676803589\n",
      "Epoch 467 / 500 | iteration 20 / 30 | Total Loss: 5.348821640014648 | KNN Loss: 4.329029560089111 | BCE Loss: 1.019792079925537\n",
      "Epoch 467 / 500 | iteration 25 / 30 | Total Loss: 5.3546905517578125 | KNN Loss: 4.327182769775391 | BCE Loss: 1.027508020401001\n",
      "Epoch 468 / 500 | iteration 0 / 30 | Total Loss: 5.3957743644714355 | KNN Loss: 4.365108489990234 | BCE Loss: 1.0306658744812012\n",
      "Epoch 468 / 500 | iteration 5 / 30 | Total Loss: 5.360173225402832 | KNN Loss: 4.316748142242432 | BCE Loss: 1.0434249639511108\n",
      "Epoch 468 / 500 | iteration 10 / 30 | Total Loss: 5.379782676696777 | KNN Loss: 4.347975730895996 | BCE Loss: 1.0318071842193604\n",
      "Epoch 468 / 500 | iteration 15 / 30 | Total Loss: 5.400484085083008 | KNN Loss: 4.38616943359375 | BCE Loss: 1.014314889907837\n",
      "Epoch 468 / 500 | iteration 20 / 30 | Total Loss: 5.345896244049072 | KNN Loss: 4.3449578285217285 | BCE Loss: 1.0009382963180542\n",
      "Epoch 468 / 500 | iteration 25 / 30 | Total Loss: 5.329719066619873 | KNN Loss: 4.3274712562561035 | BCE Loss: 1.00224769115448\n",
      "Epoch 469 / 500 | iteration 0 / 30 | Total Loss: 5.305901527404785 | KNN Loss: 4.313867568969727 | BCE Loss: 0.9920337200164795\n",
      "Epoch 469 / 500 | iteration 5 / 30 | Total Loss: 5.379639148712158 | KNN Loss: 4.347382545471191 | BCE Loss: 1.0322567224502563\n",
      "Epoch 469 / 500 | iteration 10 / 30 | Total Loss: 5.380368232727051 | KNN Loss: 4.337583065032959 | BCE Loss: 1.042785406112671\n",
      "Epoch 469 / 500 | iteration 15 / 30 | Total Loss: 5.336278915405273 | KNN Loss: 4.330142974853516 | BCE Loss: 1.006136178970337\n",
      "Epoch 469 / 500 | iteration 20 / 30 | Total Loss: 5.396170616149902 | KNN Loss: 4.3710408210754395 | BCE Loss: 1.025129795074463\n",
      "Epoch 469 / 500 | iteration 25 / 30 | Total Loss: 5.356594085693359 | KNN Loss: 4.320498466491699 | BCE Loss: 1.0360956192016602\n",
      "Epoch 470 / 500 | iteration 0 / 30 | Total Loss: 5.359951019287109 | KNN Loss: 4.340485095977783 | BCE Loss: 1.0194661617279053\n",
      "Epoch 470 / 500 | iteration 5 / 30 | Total Loss: 5.41668701171875 | KNN Loss: 4.364564418792725 | BCE Loss: 1.0521224737167358\n",
      "Epoch 470 / 500 | iteration 10 / 30 | Total Loss: 5.308536529541016 | KNN Loss: 4.304110527038574 | BCE Loss: 1.0044262409210205\n",
      "Epoch 470 / 500 | iteration 15 / 30 | Total Loss: 5.407595634460449 | KNN Loss: 4.368615627288818 | BCE Loss: 1.0389801263809204\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 470 / 500 | iteration 20 / 30 | Total Loss: 5.423464298248291 | KNN Loss: 4.391591548919678 | BCE Loss: 1.0318726301193237\n",
      "Epoch 470 / 500 | iteration 25 / 30 | Total Loss: 5.368599891662598 | KNN Loss: 4.357300281524658 | BCE Loss: 1.01129949092865\n",
      "Epoch 471 / 500 | iteration 0 / 30 | Total Loss: 5.375969886779785 | KNN Loss: 4.3642730712890625 | BCE Loss: 1.0116968154907227\n",
      "Epoch 471 / 500 | iteration 5 / 30 | Total Loss: 5.359688758850098 | KNN Loss: 4.33807897567749 | BCE Loss: 1.0216097831726074\n",
      "Epoch 471 / 500 | iteration 10 / 30 | Total Loss: 5.340062141418457 | KNN Loss: 4.32853889465332 | BCE Loss: 1.0115234851837158\n",
      "Epoch 471 / 500 | iteration 15 / 30 | Total Loss: 5.390985488891602 | KNN Loss: 4.370636463165283 | BCE Loss: 1.0203490257263184\n",
      "Epoch 471 / 500 | iteration 20 / 30 | Total Loss: 5.374786853790283 | KNN Loss: 4.332377910614014 | BCE Loss: 1.0424089431762695\n",
      "Epoch 471 / 500 | iteration 25 / 30 | Total Loss: 5.377124309539795 | KNN Loss: 4.357393741607666 | BCE Loss: 1.0197304487228394\n",
      "Epoch 472 / 500 | iteration 0 / 30 | Total Loss: 5.332680702209473 | KNN Loss: 4.3283891677856445 | BCE Loss: 1.004291296005249\n",
      "Epoch 472 / 500 | iteration 5 / 30 | Total Loss: 5.392594337463379 | KNN Loss: 4.354731559753418 | BCE Loss: 1.03786301612854\n",
      "Epoch 472 / 500 | iteration 10 / 30 | Total Loss: 5.43466329574585 | KNN Loss: 4.3818511962890625 | BCE Loss: 1.052812099456787\n",
      "Epoch 472 / 500 | iteration 15 / 30 | Total Loss: 5.43690299987793 | KNN Loss: 4.398838996887207 | BCE Loss: 1.0380642414093018\n",
      "Epoch 472 / 500 | iteration 20 / 30 | Total Loss: 5.318134784698486 | KNN Loss: 4.322399139404297 | BCE Loss: 0.995735764503479\n",
      "Epoch 472 / 500 | iteration 25 / 30 | Total Loss: 5.423521041870117 | KNN Loss: 4.403223991394043 | BCE Loss: 1.0202972888946533\n",
      "Epoch 473 / 500 | iteration 0 / 30 | Total Loss: 5.416279315948486 | KNN Loss: 4.37654447555542 | BCE Loss: 1.039734959602356\n",
      "Epoch 473 / 500 | iteration 5 / 30 | Total Loss: 5.3500518798828125 | KNN Loss: 4.352865219116211 | BCE Loss: 0.9971866607666016\n",
      "Epoch 473 / 500 | iteration 10 / 30 | Total Loss: 5.409385681152344 | KNN Loss: 4.371284008026123 | BCE Loss: 1.0381014347076416\n",
      "Epoch 473 / 500 | iteration 15 / 30 | Total Loss: 5.408567905426025 | KNN Loss: 4.376312255859375 | BCE Loss: 1.03225576877594\n",
      "Epoch 473 / 500 | iteration 20 / 30 | Total Loss: 5.361144065856934 | KNN Loss: 4.347297191619873 | BCE Loss: 1.013846755027771\n",
      "Epoch 473 / 500 | iteration 25 / 30 | Total Loss: 5.348583221435547 | KNN Loss: 4.313408374786377 | BCE Loss: 1.0351747274398804\n",
      "Epoch 474 / 500 | iteration 0 / 30 | Total Loss: 5.381014347076416 | KNN Loss: 4.334707260131836 | BCE Loss: 1.04630708694458\n",
      "Epoch 474 / 500 | iteration 5 / 30 | Total Loss: 5.362923622131348 | KNN Loss: 4.357192039489746 | BCE Loss: 1.0057318210601807\n",
      "Epoch 474 / 500 | iteration 10 / 30 | Total Loss: 5.392876148223877 | KNN Loss: 4.353642463684082 | BCE Loss: 1.0392338037490845\n",
      "Epoch 474 / 500 | iteration 15 / 30 | Total Loss: 5.395317554473877 | KNN Loss: 4.3742876052856445 | BCE Loss: 1.0210299491882324\n",
      "Epoch 474 / 500 | iteration 20 / 30 | Total Loss: 5.35818338394165 | KNN Loss: 4.324869155883789 | BCE Loss: 1.0333142280578613\n",
      "Epoch 474 / 500 | iteration 25 / 30 | Total Loss: 5.365235328674316 | KNN Loss: 4.343565940856934 | BCE Loss: 1.0216691493988037\n",
      "Epoch 475 / 500 | iteration 0 / 30 | Total Loss: 5.357840538024902 | KNN Loss: 4.348057270050049 | BCE Loss: 1.009783387184143\n",
      "Epoch 475 / 500 | iteration 5 / 30 | Total Loss: 5.405524253845215 | KNN Loss: 4.374120235443115 | BCE Loss: 1.0314042568206787\n",
      "Epoch 475 / 500 | iteration 10 / 30 | Total Loss: 5.31783390045166 | KNN Loss: 4.323456764221191 | BCE Loss: 0.9943770170211792\n",
      "Epoch 475 / 500 | iteration 15 / 30 | Total Loss: 5.371227741241455 | KNN Loss: 4.353281497955322 | BCE Loss: 1.0179462432861328\n",
      "Epoch 475 / 500 | iteration 20 / 30 | Total Loss: 5.366884708404541 | KNN Loss: 4.3492279052734375 | BCE Loss: 1.017656922340393\n",
      "Epoch 475 / 500 | iteration 25 / 30 | Total Loss: 5.440131187438965 | KNN Loss: 4.411789894104004 | BCE Loss: 1.0283414125442505\n",
      "Epoch 476 / 500 | iteration 0 / 30 | Total Loss: 5.376530647277832 | KNN Loss: 4.362772464752197 | BCE Loss: 1.0137581825256348\n",
      "Epoch 476 / 500 | iteration 5 / 30 | Total Loss: 5.411406517028809 | KNN Loss: 4.366970062255859 | BCE Loss: 1.0444362163543701\n",
      "Epoch 476 / 500 | iteration 10 / 30 | Total Loss: 5.386631011962891 | KNN Loss: 4.336733818054199 | BCE Loss: 1.0498974323272705\n",
      "Epoch 476 / 500 | iteration 15 / 30 | Total Loss: 5.379621982574463 | KNN Loss: 4.356367111206055 | BCE Loss: 1.0232548713684082\n",
      "Epoch 476 / 500 | iteration 20 / 30 | Total Loss: 5.350432395935059 | KNN Loss: 4.31713342666626 | BCE Loss: 1.0332989692687988\n",
      "Epoch 476 / 500 | iteration 25 / 30 | Total Loss: 5.354333400726318 | KNN Loss: 4.329471588134766 | BCE Loss: 1.0248619318008423\n",
      "Epoch   477: reducing learning rate of group 0 to 7.8888e-08.\n",
      "Epoch 477 / 500 | iteration 0 / 30 | Total Loss: 5.368712902069092 | KNN Loss: 4.351759910583496 | BCE Loss: 1.0169529914855957\n",
      "Epoch 477 / 500 | iteration 5 / 30 | Total Loss: 5.380753993988037 | KNN Loss: 4.364237308502197 | BCE Loss: 1.0165165662765503\n",
      "Epoch 477 / 500 | iteration 10 / 30 | Total Loss: 5.340457916259766 | KNN Loss: 4.31663703918457 | BCE Loss: 1.0238206386566162\n",
      "Epoch 477 / 500 | iteration 15 / 30 | Total Loss: 5.409728050231934 | KNN Loss: 4.406216144561768 | BCE Loss: 1.003511905670166\n",
      "Epoch 477 / 500 | iteration 20 / 30 | Total Loss: 5.358222484588623 | KNN Loss: 4.327924728393555 | BCE Loss: 1.0302977561950684\n",
      "Epoch 477 / 500 | iteration 25 / 30 | Total Loss: 5.3729143142700195 | KNN Loss: 4.328058242797852 | BCE Loss: 1.044856309890747\n",
      "Epoch 478 / 500 | iteration 0 / 30 | Total Loss: 5.420079708099365 | KNN Loss: 4.3989715576171875 | BCE Loss: 1.0211082696914673\n",
      "Epoch 478 / 500 | iteration 5 / 30 | Total Loss: 5.374222755432129 | KNN Loss: 4.367677211761475 | BCE Loss: 1.0065457820892334\n",
      "Epoch 478 / 500 | iteration 10 / 30 | Total Loss: 5.3660407066345215 | KNN Loss: 4.345849990844727 | BCE Loss: 1.020190715789795\n",
      "Epoch 478 / 500 | iteration 15 / 30 | Total Loss: 5.365231037139893 | KNN Loss: 4.350540637969971 | BCE Loss: 1.0146903991699219\n",
      "Epoch 478 / 500 | iteration 20 / 30 | Total Loss: 5.396385669708252 | KNN Loss: 4.36386251449585 | BCE Loss: 1.032523274421692\n",
      "Epoch 478 / 500 | iteration 25 / 30 | Total Loss: 5.360886573791504 | KNN Loss: 4.328580856323242 | BCE Loss: 1.0323054790496826\n",
      "Epoch 479 / 500 | iteration 0 / 30 | Total Loss: 5.34584903717041 | KNN Loss: 4.356058597564697 | BCE Loss: 0.9897903203964233\n",
      "Epoch 479 / 500 | iteration 5 / 30 | Total Loss: 5.386068344116211 | KNN Loss: 4.383448600769043 | BCE Loss: 1.0026196241378784\n",
      "Epoch 479 / 500 | iteration 10 / 30 | Total Loss: 5.399127006530762 | KNN Loss: 4.380058288574219 | BCE Loss: 1.019068956375122\n",
      "Epoch 479 / 500 | iteration 15 / 30 | Total Loss: 5.330500602722168 | KNN Loss: 4.313889026641846 | BCE Loss: 1.0166113376617432\n",
      "Epoch 479 / 500 | iteration 20 / 30 | Total Loss: 5.357339859008789 | KNN Loss: 4.322962760925293 | BCE Loss: 1.034376859664917\n",
      "Epoch 479 / 500 | iteration 25 / 30 | Total Loss: 5.358457088470459 | KNN Loss: 4.31887674331665 | BCE Loss: 1.0395803451538086\n",
      "Epoch 480 / 500 | iteration 0 / 30 | Total Loss: 5.389015197753906 | KNN Loss: 4.345753192901611 | BCE Loss: 1.043262243270874\n",
      "Epoch 480 / 500 | iteration 5 / 30 | Total Loss: 5.392673492431641 | KNN Loss: 4.360508918762207 | BCE Loss: 1.0321648120880127\n",
      "Epoch 480 / 500 | iteration 10 / 30 | Total Loss: 5.376513481140137 | KNN Loss: 4.346389293670654 | BCE Loss: 1.0301244258880615\n",
      "Epoch 480 / 500 | iteration 15 / 30 | Total Loss: 5.393090724945068 | KNN Loss: 4.368348121643066 | BCE Loss: 1.024742603302002\n",
      "Epoch 480 / 500 | iteration 20 / 30 | Total Loss: 5.341029644012451 | KNN Loss: 4.305181503295898 | BCE Loss: 1.0358481407165527\n",
      "Epoch 480 / 500 | iteration 25 / 30 | Total Loss: 5.395458698272705 | KNN Loss: 4.3474531173706055 | BCE Loss: 1.0480057001113892\n",
      "Epoch 481 / 500 | iteration 0 / 30 | Total Loss: 5.336817741394043 | KNN Loss: 4.337392807006836 | BCE Loss: 0.9994251728057861\n",
      "Epoch 481 / 500 | iteration 5 / 30 | Total Loss: 5.39156436920166 | KNN Loss: 4.361385822296143 | BCE Loss: 1.030178427696228\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 481 / 500 | iteration 10 / 30 | Total Loss: 5.449256420135498 | KNN Loss: 4.416590213775635 | BCE Loss: 1.0326663255691528\n",
      "Epoch 481 / 500 | iteration 15 / 30 | Total Loss: 5.333684921264648 | KNN Loss: 4.340879917144775 | BCE Loss: 0.9928052425384521\n",
      "Epoch 481 / 500 | iteration 20 / 30 | Total Loss: 5.391486167907715 | KNN Loss: 4.374935150146484 | BCE Loss: 1.0165507793426514\n",
      "Epoch 481 / 500 | iteration 25 / 30 | Total Loss: 5.351450443267822 | KNN Loss: 4.331624984741211 | BCE Loss: 1.0198253393173218\n",
      "Epoch 482 / 500 | iteration 0 / 30 | Total Loss: 5.394283294677734 | KNN Loss: 4.3771209716796875 | BCE Loss: 1.0171622037887573\n",
      "Epoch 482 / 500 | iteration 5 / 30 | Total Loss: 5.38389778137207 | KNN Loss: 4.361886024475098 | BCE Loss: 1.0220119953155518\n",
      "Epoch 482 / 500 | iteration 10 / 30 | Total Loss: 5.368283271789551 | KNN Loss: 4.354225158691406 | BCE Loss: 1.0140578746795654\n",
      "Epoch 482 / 500 | iteration 15 / 30 | Total Loss: 5.428168296813965 | KNN Loss: 4.3803253173828125 | BCE Loss: 1.047843098640442\n",
      "Epoch 482 / 500 | iteration 20 / 30 | Total Loss: 5.405454635620117 | KNN Loss: 4.386525630950928 | BCE Loss: 1.0189287662506104\n",
      "Epoch 482 / 500 | iteration 25 / 30 | Total Loss: 5.34298849105835 | KNN Loss: 4.330031871795654 | BCE Loss: 1.0129567384719849\n",
      "Epoch 483 / 500 | iteration 0 / 30 | Total Loss: 5.401026725769043 | KNN Loss: 4.347812652587891 | BCE Loss: 1.053214192390442\n",
      "Epoch 483 / 500 | iteration 5 / 30 | Total Loss: 5.40850830078125 | KNN Loss: 4.346839904785156 | BCE Loss: 1.0616685152053833\n",
      "Epoch 483 / 500 | iteration 10 / 30 | Total Loss: 5.333517074584961 | KNN Loss: 4.329733848571777 | BCE Loss: 1.0037832260131836\n",
      "Epoch 483 / 500 | iteration 15 / 30 | Total Loss: 5.385899543762207 | KNN Loss: 4.364555358886719 | BCE Loss: 1.0213441848754883\n",
      "Epoch 483 / 500 | iteration 20 / 30 | Total Loss: 5.339447021484375 | KNN Loss: 4.328898906707764 | BCE Loss: 1.0105479955673218\n",
      "Epoch 483 / 500 | iteration 25 / 30 | Total Loss: 5.3685479164123535 | KNN Loss: 4.336739540100098 | BCE Loss: 1.0318084955215454\n",
      "Epoch 484 / 500 | iteration 0 / 30 | Total Loss: 5.349403381347656 | KNN Loss: 4.342727184295654 | BCE Loss: 1.0066763162612915\n",
      "Epoch 484 / 500 | iteration 5 / 30 | Total Loss: 5.375242233276367 | KNN Loss: 4.335838794708252 | BCE Loss: 1.0394035577774048\n",
      "Epoch 484 / 500 | iteration 10 / 30 | Total Loss: 5.421072959899902 | KNN Loss: 4.393883228302002 | BCE Loss: 1.0271894931793213\n",
      "Epoch 484 / 500 | iteration 15 / 30 | Total Loss: 5.407586097717285 | KNN Loss: 4.378561973571777 | BCE Loss: 1.029024362564087\n",
      "Epoch 484 / 500 | iteration 20 / 30 | Total Loss: 5.335200786590576 | KNN Loss: 4.329163551330566 | BCE Loss: 1.0060372352600098\n",
      "Epoch 484 / 500 | iteration 25 / 30 | Total Loss: 5.4055657386779785 | KNN Loss: 4.35516357421875 | BCE Loss: 1.050402045249939\n",
      "Epoch 485 / 500 | iteration 0 / 30 | Total Loss: 5.352773666381836 | KNN Loss: 4.338926315307617 | BCE Loss: 1.0138473510742188\n",
      "Epoch 485 / 500 | iteration 5 / 30 | Total Loss: 5.398882865905762 | KNN Loss: 4.357202053070068 | BCE Loss: 1.0416805744171143\n",
      "Epoch 485 / 500 | iteration 10 / 30 | Total Loss: 5.354986190795898 | KNN Loss: 4.34152364730835 | BCE Loss: 1.0134625434875488\n",
      "Epoch 485 / 500 | iteration 15 / 30 | Total Loss: 5.381325721740723 | KNN Loss: 4.362107276916504 | BCE Loss: 1.0192186832427979\n",
      "Epoch 485 / 500 | iteration 20 / 30 | Total Loss: 5.372828483581543 | KNN Loss: 4.335641860961914 | BCE Loss: 1.0371863842010498\n",
      "Epoch 485 / 500 | iteration 25 / 30 | Total Loss: 5.348433971405029 | KNN Loss: 4.311112880706787 | BCE Loss: 1.0373209714889526\n",
      "Epoch 486 / 500 | iteration 0 / 30 | Total Loss: 5.356863975524902 | KNN Loss: 4.3268537521362305 | BCE Loss: 1.0300103425979614\n",
      "Epoch 486 / 500 | iteration 5 / 30 | Total Loss: 5.363628387451172 | KNN Loss: 4.360928535461426 | BCE Loss: 1.0027000904083252\n",
      "Epoch 486 / 500 | iteration 10 / 30 | Total Loss: 5.401479721069336 | KNN Loss: 4.373104572296143 | BCE Loss: 1.0283753871917725\n",
      "Epoch 486 / 500 | iteration 15 / 30 | Total Loss: 5.357452392578125 | KNN Loss: 4.346968173980713 | BCE Loss: 1.010483980178833\n",
      "Epoch 486 / 500 | iteration 20 / 30 | Total Loss: 5.350583076477051 | KNN Loss: 4.342452049255371 | BCE Loss: 1.0081307888031006\n",
      "Epoch 486 / 500 | iteration 25 / 30 | Total Loss: 5.345603942871094 | KNN Loss: 4.346659183502197 | BCE Loss: 0.9989447593688965\n",
      "Epoch 487 / 500 | iteration 0 / 30 | Total Loss: 5.32967472076416 | KNN Loss: 4.326968193054199 | BCE Loss: 1.00270676612854\n",
      "Epoch 487 / 500 | iteration 5 / 30 | Total Loss: 5.348360061645508 | KNN Loss: 4.324617385864258 | BCE Loss: 1.0237427949905396\n",
      "Epoch 487 / 500 | iteration 10 / 30 | Total Loss: 5.34940242767334 | KNN Loss: 4.346729755401611 | BCE Loss: 1.002672791481018\n",
      "Epoch 487 / 500 | iteration 15 / 30 | Total Loss: 5.390972137451172 | KNN Loss: 4.352468967437744 | BCE Loss: 1.0385031700134277\n",
      "Epoch 487 / 500 | iteration 20 / 30 | Total Loss: 5.356817245483398 | KNN Loss: 4.333924293518066 | BCE Loss: 1.022892713546753\n",
      "Epoch 487 / 500 | iteration 25 / 30 | Total Loss: 5.392337799072266 | KNN Loss: 4.353504657745361 | BCE Loss: 1.0388329029083252\n",
      "Epoch   488: reducing learning rate of group 0 to 5.5221e-08.\n",
      "Epoch 488 / 500 | iteration 0 / 30 | Total Loss: 5.40287446975708 | KNN Loss: 4.363007068634033 | BCE Loss: 1.0398674011230469\n",
      "Epoch 488 / 500 | iteration 5 / 30 | Total Loss: 5.340040683746338 | KNN Loss: 4.342051982879639 | BCE Loss: 0.997988760471344\n",
      "Epoch 488 / 500 | iteration 10 / 30 | Total Loss: 5.3914923667907715 | KNN Loss: 4.370413780212402 | BCE Loss: 1.0210787057876587\n",
      "Epoch 488 / 500 | iteration 15 / 30 | Total Loss: 5.402381420135498 | KNN Loss: 4.390314102172852 | BCE Loss: 1.012067198753357\n",
      "Epoch 488 / 500 | iteration 20 / 30 | Total Loss: 5.442370414733887 | KNN Loss: 4.389829635620117 | BCE Loss: 1.0525407791137695\n",
      "Epoch 488 / 500 | iteration 25 / 30 | Total Loss: 5.346017360687256 | KNN Loss: 4.330649375915527 | BCE Loss: 1.015368103981018\n",
      "Epoch 489 / 500 | iteration 0 / 30 | Total Loss: 5.414502143859863 | KNN Loss: 4.366964817047119 | BCE Loss: 1.0475373268127441\n",
      "Epoch 489 / 500 | iteration 5 / 30 | Total Loss: 5.371887683868408 | KNN Loss: 4.348333358764648 | BCE Loss: 1.0235543251037598\n",
      "Epoch 489 / 500 | iteration 10 / 30 | Total Loss: 5.39492130279541 | KNN Loss: 4.375187873840332 | BCE Loss: 1.0197333097457886\n",
      "Epoch 489 / 500 | iteration 15 / 30 | Total Loss: 5.385829925537109 | KNN Loss: 4.3608622550964355 | BCE Loss: 1.024967908859253\n",
      "Epoch 489 / 500 | iteration 20 / 30 | Total Loss: 5.396223068237305 | KNN Loss: 4.365412712097168 | BCE Loss: 1.0308102369308472\n",
      "Epoch 489 / 500 | iteration 25 / 30 | Total Loss: 5.354068756103516 | KNN Loss: 4.323281288146973 | BCE Loss: 1.0307873487472534\n",
      "Epoch 490 / 500 | iteration 0 / 30 | Total Loss: 5.4119110107421875 | KNN Loss: 4.386603832244873 | BCE Loss: 1.0253071784973145\n",
      "Epoch 490 / 500 | iteration 5 / 30 | Total Loss: 5.339424133300781 | KNN Loss: 4.325253009796143 | BCE Loss: 1.0141708850860596\n",
      "Epoch 490 / 500 | iteration 10 / 30 | Total Loss: 5.407506942749023 | KNN Loss: 4.394042015075684 | BCE Loss: 1.0134649276733398\n",
      "Epoch 490 / 500 | iteration 15 / 30 | Total Loss: 5.393504619598389 | KNN Loss: 4.363752841949463 | BCE Loss: 1.0297517776489258\n",
      "Epoch 490 / 500 | iteration 20 / 30 | Total Loss: 5.356389045715332 | KNN Loss: 4.360840797424316 | BCE Loss: 0.9955481290817261\n",
      "Epoch 490 / 500 | iteration 25 / 30 | Total Loss: 5.369414329528809 | KNN Loss: 4.33820915222168 | BCE Loss: 1.0312052965164185\n",
      "Epoch 491 / 500 | iteration 0 / 30 | Total Loss: 5.421055793762207 | KNN Loss: 4.372767448425293 | BCE Loss: 1.0482884645462036\n",
      "Epoch 491 / 500 | iteration 5 / 30 | Total Loss: 5.382462978363037 | KNN Loss: 4.3346147537231445 | BCE Loss: 1.0478482246398926\n",
      "Epoch 491 / 500 | iteration 10 / 30 | Total Loss: 5.374478340148926 | KNN Loss: 4.348267555236816 | BCE Loss: 1.0262107849121094\n",
      "Epoch 491 / 500 | iteration 15 / 30 | Total Loss: 5.355401039123535 | KNN Loss: 4.3403449058532715 | BCE Loss: 1.0150560140609741\n",
      "Epoch 491 / 500 | iteration 20 / 30 | Total Loss: 5.361627578735352 | KNN Loss: 4.348870277404785 | BCE Loss: 1.0127573013305664\n",
      "Epoch 491 / 500 | iteration 25 / 30 | Total Loss: 5.405476093292236 | KNN Loss: 4.394975185394287 | BCE Loss: 1.0105009078979492\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 492 / 500 | iteration 0 / 30 | Total Loss: 5.382512092590332 | KNN Loss: 4.365140438079834 | BCE Loss: 1.0173718929290771\n",
      "Epoch 492 / 500 | iteration 5 / 30 | Total Loss: 5.305492401123047 | KNN Loss: 4.301912784576416 | BCE Loss: 1.0035796165466309\n",
      "Epoch 492 / 500 | iteration 10 / 30 | Total Loss: 5.386650085449219 | KNN Loss: 4.372052192687988 | BCE Loss: 1.0145981311798096\n",
      "Epoch 492 / 500 | iteration 15 / 30 | Total Loss: 5.3594818115234375 | KNN Loss: 4.342615127563477 | BCE Loss: 1.016866683959961\n",
      "Epoch 492 / 500 | iteration 20 / 30 | Total Loss: 5.398952960968018 | KNN Loss: 4.367936611175537 | BCE Loss: 1.0310163497924805\n",
      "Epoch 492 / 500 | iteration 25 / 30 | Total Loss: 5.428266525268555 | KNN Loss: 4.396023750305176 | BCE Loss: 1.032242774963379\n",
      "Epoch 493 / 500 | iteration 0 / 30 | Total Loss: 5.367700099945068 | KNN Loss: 4.355287075042725 | BCE Loss: 1.0124129056930542\n",
      "Epoch 493 / 500 | iteration 5 / 30 | Total Loss: 5.358769416809082 | KNN Loss: 4.3501362800598145 | BCE Loss: 1.0086333751678467\n",
      "Epoch 493 / 500 | iteration 10 / 30 | Total Loss: 5.367423057556152 | KNN Loss: 4.348212718963623 | BCE Loss: 1.0192101001739502\n",
      "Epoch 493 / 500 | iteration 15 / 30 | Total Loss: 5.396651268005371 | KNN Loss: 4.374935626983643 | BCE Loss: 1.021715760231018\n",
      "Epoch 493 / 500 | iteration 20 / 30 | Total Loss: 5.531546115875244 | KNN Loss: 4.499734401702881 | BCE Loss: 1.0318118333816528\n",
      "Epoch 493 / 500 | iteration 25 / 30 | Total Loss: 5.350244045257568 | KNN Loss: 4.321750640869141 | BCE Loss: 1.0284934043884277\n",
      "Epoch 494 / 500 | iteration 0 / 30 | Total Loss: 5.347935199737549 | KNN Loss: 4.31965446472168 | BCE Loss: 1.0282808542251587\n",
      "Epoch 494 / 500 | iteration 5 / 30 | Total Loss: 5.320758819580078 | KNN Loss: 4.312058448791504 | BCE Loss: 1.0087003707885742\n",
      "Epoch 494 / 500 | iteration 10 / 30 | Total Loss: 5.366294860839844 | KNN Loss: 4.362391471862793 | BCE Loss: 1.0039031505584717\n",
      "Epoch 494 / 500 | iteration 15 / 30 | Total Loss: 5.340489387512207 | KNN Loss: 4.339677333831787 | BCE Loss: 1.000812292098999\n",
      "Epoch 494 / 500 | iteration 20 / 30 | Total Loss: 5.410745620727539 | KNN Loss: 4.394031047821045 | BCE Loss: 1.0167145729064941\n",
      "Epoch 494 / 500 | iteration 25 / 30 | Total Loss: 5.417849540710449 | KNN Loss: 4.373946189880371 | BCE Loss: 1.043903112411499\n",
      "Epoch 495 / 500 | iteration 0 / 30 | Total Loss: 5.445106029510498 | KNN Loss: 4.3856329917907715 | BCE Loss: 1.0594731569290161\n",
      "Epoch 495 / 500 | iteration 5 / 30 | Total Loss: 5.332296371459961 | KNN Loss: 4.3097453117370605 | BCE Loss: 1.0225512981414795\n",
      "Epoch 495 / 500 | iteration 10 / 30 | Total Loss: 5.3884100914001465 | KNN Loss: 4.360901355743408 | BCE Loss: 1.0275087356567383\n",
      "Epoch 495 / 500 | iteration 15 / 30 | Total Loss: 5.396493911743164 | KNN Loss: 4.355056285858154 | BCE Loss: 1.0414373874664307\n",
      "Epoch 495 / 500 | iteration 20 / 30 | Total Loss: 5.370230674743652 | KNN Loss: 4.339723110198975 | BCE Loss: 1.0305073261260986\n",
      "Epoch 495 / 500 | iteration 25 / 30 | Total Loss: 5.374844074249268 | KNN Loss: 4.355814456939697 | BCE Loss: 1.0190294981002808\n",
      "Epoch 496 / 500 | iteration 0 / 30 | Total Loss: 5.429040908813477 | KNN Loss: 4.423072814941406 | BCE Loss: 1.0059682130813599\n",
      "Epoch 496 / 500 | iteration 5 / 30 | Total Loss: 5.350415229797363 | KNN Loss: 4.333635330200195 | BCE Loss: 1.0167800188064575\n",
      "Epoch 496 / 500 | iteration 10 / 30 | Total Loss: 5.38604211807251 | KNN Loss: 4.347292423248291 | BCE Loss: 1.0387496948242188\n",
      "Epoch 496 / 500 | iteration 15 / 30 | Total Loss: 5.4205732345581055 | KNN Loss: 4.389178276062012 | BCE Loss: 1.0313951969146729\n",
      "Epoch 496 / 500 | iteration 20 / 30 | Total Loss: 5.4056596755981445 | KNN Loss: 4.344254016876221 | BCE Loss: 1.0614056587219238\n",
      "Epoch 496 / 500 | iteration 25 / 30 | Total Loss: 5.341673374176025 | KNN Loss: 4.310994625091553 | BCE Loss: 1.0306787490844727\n",
      "Epoch 497 / 500 | iteration 0 / 30 | Total Loss: 5.36455774307251 | KNN Loss: 4.355687141418457 | BCE Loss: 1.0088706016540527\n",
      "Epoch 497 / 500 | iteration 5 / 30 | Total Loss: 5.357687950134277 | KNN Loss: 4.334846496582031 | BCE Loss: 1.0228416919708252\n",
      "Epoch 497 / 500 | iteration 10 / 30 | Total Loss: 5.395748138427734 | KNN Loss: 4.356932640075684 | BCE Loss: 1.0388152599334717\n",
      "Epoch 497 / 500 | iteration 15 / 30 | Total Loss: 5.325693130493164 | KNN Loss: 4.336828231811523 | BCE Loss: 0.9888648986816406\n",
      "Epoch 497 / 500 | iteration 20 / 30 | Total Loss: 5.45479154586792 | KNN Loss: 4.397751331329346 | BCE Loss: 1.0570402145385742\n",
      "Epoch 497 / 500 | iteration 25 / 30 | Total Loss: 5.358620643615723 | KNN Loss: 4.323529243469238 | BCE Loss: 1.0350911617279053\n",
      "Epoch 498 / 500 | iteration 0 / 30 | Total Loss: 5.397365570068359 | KNN Loss: 4.394538879394531 | BCE Loss: 1.0028265714645386\n",
      "Epoch 498 / 500 | iteration 5 / 30 | Total Loss: 5.357061386108398 | KNN Loss: 4.336093902587891 | BCE Loss: 1.0209676027297974\n",
      "Epoch 498 / 500 | iteration 10 / 30 | Total Loss: 5.39525842666626 | KNN Loss: 4.373663902282715 | BCE Loss: 1.021594524383545\n",
      "Epoch 498 / 500 | iteration 15 / 30 | Total Loss: 5.4084014892578125 | KNN Loss: 4.376528739929199 | BCE Loss: 1.0318729877471924\n",
      "Epoch 498 / 500 | iteration 20 / 30 | Total Loss: 5.415214538574219 | KNN Loss: 4.39973258972168 | BCE Loss: 1.0154818296432495\n",
      "Epoch 498 / 500 | iteration 25 / 30 | Total Loss: 5.3872270584106445 | KNN Loss: 4.3557305335998535 | BCE Loss: 1.0314966440200806\n",
      "Epoch   499: reducing learning rate of group 0 to 3.8655e-08.\n",
      "Epoch 499 / 500 | iteration 0 / 30 | Total Loss: 5.40395450592041 | KNN Loss: 4.350770473480225 | BCE Loss: 1.053183913230896\n",
      "Epoch 499 / 500 | iteration 5 / 30 | Total Loss: 5.3729681968688965 | KNN Loss: 4.341774940490723 | BCE Loss: 1.0311932563781738\n",
      "Epoch 499 / 500 | iteration 10 / 30 | Total Loss: 5.399859428405762 | KNN Loss: 4.36484432220459 | BCE Loss: 1.0350151062011719\n",
      "Epoch 499 / 500 | iteration 15 / 30 | Total Loss: 5.3616943359375 | KNN Loss: 4.356085300445557 | BCE Loss: 1.0056090354919434\n",
      "Epoch 499 / 500 | iteration 20 / 30 | Total Loss: 5.395147800445557 | KNN Loss: 4.369136333465576 | BCE Loss: 1.0260114669799805\n",
      "Epoch 499 / 500 | iteration 25 / 30 | Total Loss: 5.373416900634766 | KNN Loss: 4.313572406768799 | BCE Loss: 1.059844732284546\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "data_iter = torch.utils.data.DataLoader(dataset,\n",
    "                                     batch_size=batch_size,\n",
    "                                     shuffle=True,\n",
    "                                     num_workers=1,\n",
    "                                     pin_memory=True)\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', verbose=True, factor=0.7, threshold=1e-4)\n",
    "knn_crt = KNNLoss(k=k).to(device)\n",
    "losses = []\n",
    "alpha = 10/170\n",
    "gamma = 2\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for iteration, (batch, target) in enumerate(data_iter):\n",
    "        batch = batch.to(device)\n",
    "        target = target.to(device)\n",
    "        outputs, iterm = model(batch, return_intermidiate=True)\n",
    "        mse_loss = F.binary_cross_entropy_with_logits(outputs, target, reduction='none')\n",
    "        mask = torch.ones_like(mse_loss)\n",
    "        mask[target == 0] = alpha ** gamma\n",
    "        mask[target == 1] = (1 - alpha) ** gamma\n",
    "        mse_loss = (mse_loss * mask).sum(dim=-1).mean()\n",
    "        try:\n",
    "            knn_loss = knn_crt(iterm)\n",
    "            if torch.isinf(knn_loss):\n",
    "                knn_loss = 0\n",
    "        except ValueError:\n",
    "            knn_loss = torch.tensor(0)\n",
    "        loss = mse_loss + knn_loss\n",
    "        total_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if iteration % log_every == 0:\n",
    "            print(f\"Epoch {epoch} / {epochs} | iteration {iteration} / {len(data_iter)} | Total Loss: {loss.item()} | KNN Loss: {knn_loss.item()} | BCE Loss: {mse_loss.item()}\")\n",
    "    \n",
    "    scheduler.step(total_loss / (iteration + 1))\n",
    "    losses.append(total_loss / (iteration + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.7441,  2.6732,  3.0950,  4.1396,  4.0467,  0.6395,  1.9095,  2.7127,\n",
      "          1.7671,  1.6543,  2.7099,  1.4446,  1.1443,  2.1694,  1.2966,  2.0321,\n",
      "          2.4437,  1.9823,  1.9205,  2.8463,  1.7464,  3.4970,  1.5933,  3.1552,\n",
      "          2.5494,  1.8534,  1.7290,  1.4775,  1.4493,  0.4399,  0.1224,  1.0770,\n",
      "          0.2949,  1.1134,  1.3717,  1.1682,  1.3365,  3.8645,  0.7598,  1.3859,\n",
      "          1.3416, -0.6291, -0.3325,  2.8607,  2.1404,  0.6125, -0.1257, -0.0143,\n",
      "          1.4580,  2.1000,  2.1628,  0.0753,  1.7593,  0.5025, -0.3609,  1.1956,\n",
      "          1.8183,  1.4060,  1.6863,  1.7672,  0.6736,  0.8092,  0.1594,  1.3707,\n",
      "          1.1880,  1.9930, -1.7575,  0.3536,  1.8959,  1.8832,  3.0835,  0.2219,\n",
      "          1.3353,  1.6725,  1.6955,  1.6607,  0.1663,  0.6747,  0.0909,  1.3627,\n",
      "         -0.0319,  0.7510,  2.2128, -0.3403,  0.3755, -1.0097, -2.2813, -0.5493,\n",
      "          0.6126, -1.6691,  0.3744, -0.0092, -0.4956, -0.7589,  0.5502,  1.3111,\n",
      "         -0.6540, -1.2032,  0.4960,  1.4372,  0.2312, -1.0984,  0.7811,  1.3549,\n",
      "         -1.1902, -1.1187, -0.1515,  0.4045, -0.8830, -1.7696, -0.5705, -2.6737,\n",
      "         -0.3242,  2.0915,  1.2111, -0.4856, -0.5280, -0.0800,  1.1391, -2.4220,\n",
      "          0.0050, -0.1264,  0.3450, -0.5659,  0.3459, -0.6310, -0.9076,  0.9535,\n",
      "          0.4354, -0.4160,  0.3941, -0.6790, -1.4527,  0.0667, -0.5224,  0.9331,\n",
      "         -0.0556,  0.0688, -1.7443, -0.9292, -1.2989,  0.7445, -1.8146, -0.9469,\n",
      "         -1.0210, -0.7652, -1.4913, -0.9389, -2.2472, -0.9625, -1.3736, -0.5249,\n",
      "         -1.6054,  0.4412, -1.8672, -0.6123, -3.2076, -0.2399, -0.1036, -0.9087,\n",
      "         -2.1281, -1.9123, -1.0614, -1.3131, -2.1434, -2.2897, -3.2513]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor(-3.2513, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "tensor(4.1396, device='cuda:0', grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "outputs, iterm = model(dataset[67][0].unsqueeze(0).to(device), return_intermidiate=True)\n",
    "print(outputs)\n",
    "print(outputs.min())\n",
    "print(outputs.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c090660288214b91a357d03d8902f685",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.transform = torchvision.transforms.Compose([\n",
    "    BinaryEncodingTransform(mapping=dataset.items_to_idx),\n",
    "]\n",
    ")\n",
    "dataset.target_transform = torchvision.transforms.Compose([\n",
    "    BinaryEncodingTransform(mapping=dataset.items_to_idx),\n",
    "]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_ = [d[0].to('cpu') for d in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:00<00:00, 92.22it/s]\n"
     ]
    }
   ],
   "source": [
    "model = model.eval().to('cpu')\n",
    "projections = model.calculate_intermidiate(dataset_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5069e8c5f6b74d92b136490416312ddb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b396fb5fdbc4ae2bcda6c060b3c5744",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "distances = pairwise_distances(projections)\n",
    "# distances = np.triu(distances)\n",
    "distances_f = distances.flatten()\n",
    "\n",
    "plt.matshow(distances)\n",
    "plt.colorbar()\n",
    "plt.figure()\n",
    "plt.hist(distances_f[distances_f > 0], bins=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit DBSCAN and calculate indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = DBSCAN(eps=0.1, min_samples=80).fit_predict(projections)\n",
    "# scores = []\n",
    "# best_score = float('inf')\n",
    "# clusters = None\n",
    "# range_ = list(range(5, 20))\n",
    "# for k in tqdm(range_):\n",
    "#     y = GaussianMixture(n_components=k).fit_predict(projections)\n",
    "#     cur_score = davies_bouldin_score(projections, y)\n",
    "#     scores.append(cur_score)\n",
    "    \n",
    "#     if cur_score < best_score:\n",
    "#         best_score = cur_score\n",
    "#         clusters = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66e6a780077042d1aaef7939eef9517d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "perplexity = 100\n",
    "\n",
    "p = reduce_dims_and_plot(projections[clusters != -1],\n",
    "                         y=clusters[clusters != -1],\n",
    "                         title=f'perplexity: {perplexity}',\n",
    "                         file_name=None,\n",
    "                         perplexity=perplexity,\n",
    "                         library='Multicore-TSNE',\n",
    "                         perform_PCA=False,\n",
    "                         projected=None,\n",
    "                         figure_type='2d',\n",
    "                         show_figure=True,\n",
    "                         close_figure=False,\n",
    "                         text=None)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn import tree\n",
    "# from sklearn.tree import _tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = DecisionTreeClassifier(max_depth=200, min_samples_leaf=5)\n",
    "# clf = clf.fit(tensor_dataset[clusters!=-1], clusters[clusters != -1])\n",
    "# print(clf.score(tensor_dataset[clusters!=-1], clusters[clusters != -1]))\n",
    "# print(clf.get_depth())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores = []\n",
    "# for min_samples in range(1,50, 1):\n",
    "#     clf = DecisionTreeClassifier(max_depth=200, min_samples_leaf=min_samples)\n",
    "#     clf = clf.fit(tensor_dataset[clusters!=-1], clusters[clusters != -1])\n",
    "#     scores.append(clf.score(tensor_dataset[clusters!=-1], clusters[clusters != -1]))\n",
    "    \n",
    "# plt.figure()\n",
    "# plt.plot(list(range(1,50, 1)), scores)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rules(tree, feature_names, class_names):\n",
    "    tree_ = tree.tree_\n",
    "    feature_name = [\n",
    "        feature_names[i] if i != _tree.TREE_UNDEFINED else \"undefined!\"\n",
    "        for i in tree_.feature\n",
    "    ]\n",
    "\n",
    "    paths = []\n",
    "    path = []\n",
    "    \n",
    "    def recurse(node, path, paths):\n",
    "        if tree_.feature[node] != _tree.TREE_UNDEFINED:\n",
    "            name = feature_name[node]\n",
    "            threshold = tree_.threshold[node]\n",
    "            p1, p2 = list(path), list(path)\n",
    "#             p1 += [f\"({name} <= {np.round(threshold, 3)})\"]\n",
    "            p1 += [(name, '<=', np.round(threshold, 3))]\n",
    "            recurse(tree_.children_left[node], p1, paths)\n",
    "            p2 += [(name, '>', np.round(threshold, 3))]\n",
    "            recurse(tree_.children_right[node], p2, paths)\n",
    "        else:\n",
    "            path += [(tree_.value[node], tree_.n_node_samples[node])]\n",
    "            paths += [path]\n",
    "            \n",
    "    recurse(0, path, paths)\n",
    "\n",
    "    # sort by samples count\n",
    "    samples_count = [p[-1][1] for p in paths]\n",
    "    ii = list(np.argsort(samples_count))\n",
    "    paths = [paths[i] for i in reversed(ii)]\n",
    "    \n",
    "    rules = []\n",
    "    for path in paths:\n",
    "        rule = []\n",
    "        \n",
    "        for p in path[:-1]:\n",
    "            rule += [p]\n",
    "        target = \" then \"\n",
    "        if class_names is None:\n",
    "            target += \"response: \"+str(np.round(path[-1][0][0][0],3))\n",
    "        else:\n",
    "            classes = path[-1][0][0]\n",
    "            l = np.argmax(classes)\n",
    "            target += f\"class: {class_names[l]} (proba: {np.round(100.0*classes[l]/np.sum(classes),2)}%)\"\n",
    "           \n",
    "        proba = np.round(100.0*classes[l]/np.sum(classes),2)\n",
    "        target += f\" | based on {path[-1][1]:,} samples\"\n",
    "        rule_wrapper = {'target': target, 'rule': rule, 'proba': proba}\n",
    "        rules += [rule_wrapper]\n",
    "        \n",
    "    return rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rules = get_rules(clf, dataset.items, clusters[clusters != -1])\n",
    "\n",
    "# for rule in rules:\n",
    "#     n_pos = 0\n",
    "#     for c,p,v in rule['rule']:\n",
    "#         if p == '>':\n",
    "#             n_pos += 1\n",
    "#     rule['pos'] = n_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure()\n",
    "# probs = [r['proba'] for r in rules]\n",
    "# plt.hist(probs, bins = 100)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rules = sorted(rules, key=lambda x:x['pos'])\n",
    "# rules = [r for r in rules if r['proba'] > 50]\n",
    "# print(len(rules))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(17):\n",
    "#     r_i = rules[i]\n",
    "#     print(f\"------------- rule {i} length {len(r_i)} -------------\")\n",
    "#     print(r_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Soft-Decision-Tree given the self-labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_dataset = torch.stack(dataset_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_dataset = list(zip(tensor_dataset[clusters!=-1], clusters[clusters != -1]))\n",
    "batch_size = 512\n",
    "tree_loader = torch.utils.data.DataLoader(tree_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define how we prune the weights of a node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_node(node_weights, factor=1):\n",
    "    w = node_weights.cpu().detach().numpy()\n",
    "    mean_ = np.mean(w)\n",
    "    std_ = np.std(w)\n",
    "    node_weights[((mean_ - std_ * factor) < node_weights) & (node_weights < (mean_ + std_ * factor))] = 0\n",
    "    return node_weights\n",
    "\n",
    "def prune_node_keep(node_weights, keep=4):\n",
    "    w = node_weights.cpu().detach().numpy()\n",
    "    throw_idx = np.argsort(abs(w))[:-keep]\n",
    "    node_weights[throw_idx] = 0\n",
    "    return node_weights\n",
    "\n",
    "def prune_tree(tree_, factor):\n",
    "    new_weights = tree_.inner_nodes.weight.clone()\n",
    "    for i in range(new_weights.shape[0]):\n",
    "        res = prune_node_keep(new_weights[i, :], factor)\n",
    "        new_weights[i, :] = res\n",
    "\n",
    "    with torch.no_grad():\n",
    "        tree_.inner_nodes.weight.copy_(new_weights)\n",
    "        \n",
    "def sparseness(x):\n",
    "    s = []\n",
    "    for i in range(x.shape[0]):\n",
    "        x_ = x[i, :]\n",
    "        sp = (len(x_) - torch.norm(x_, 0).item()) / len(x_)\n",
    "        s.append(sp)\n",
    "    return np.mean(s)\n",
    "\n",
    "def compute_regularization_by_level(tree):\n",
    "    total_reg = 0\n",
    "    for i in range(tree.inner_nodes.weight.shape[0]):\n",
    "        cur_level = np.floor(np.log2(i+1))\n",
    "        node_reg = torch.norm(tree.inner_nodes.weight[i].view(-1), 2)\n",
    "        total_reg += 2**(-cur_level) * node_reg\n",
    "    return total_reg\n",
    "\n",
    "def show_sparseness(tree):\n",
    "    avg_sp = sparseness(tree.inner_nodes.weight)\n",
    "    print(f\"Average sparseness: {avg_sp}\")\n",
    "    layer = 0\n",
    "    sps = []\n",
    "    for i in range(tree.inner_nodes.weight.shape[0]):\n",
    "        cur_layer = int(np.floor(np.log2(i+1)))\n",
    "        if cur_layer != layer:\n",
    "            print(f\"layer {layer}: {np.mean(sps)}\")\n",
    "            sps = []\n",
    "            layer = cur_layer\n",
    "\n",
    "        x_ = tree.inner_nodes.weight[i, :]\n",
    "        sp = (len(x_) - torch.norm(x_, 0).item()) / len(x_)\n",
    "        sps.append(sp)\n",
    "        \n",
    "    return avg_sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_epoch(model, loader, device, log_interval, losses, accs, epoch, iteration):\n",
    "    model = model.train()\n",
    "    for batch_idx, (data, target) in enumerate(loader):\n",
    "        iteration += 1\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        output, penalty = tree.forward(data)\n",
    "\n",
    "        # Loss\n",
    "        loss_tree = criterion(output, target.view(-1))\n",
    "\n",
    "        # Penalty\n",
    "        loss_tree += penalty\n",
    "\n",
    "        # Sparse regularization\n",
    "#         fc_params = torch.cat([x.view(-1) for x in tree.inner_nodes.parameters()])\n",
    "#         regularization = sparsity_lamda * torch.norm(fc_params, 2)\n",
    "        regularization = sparsity_lamda * compute_regularization_by_level(tree)\n",
    "        loss = loss_tree\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        pred = output.data.max(1)[1]\n",
    "        correct = pred.eq(target.view(-1).data).sum()\n",
    "        accs.append(correct.item() / data.size()[0])\n",
    "\n",
    "        # Print training status\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(f\"Epoch: {epoch:02d} | Batch: {batch_idx:03d} / {len(loader):03d} | Total loss: {loss.item():.3f} | Reg loss: {regularization.item():.3f} | Tree loss: {loss_tree.item():.3f} | Accuracy: {correct.item() / data.size()[0]:03f} | {round((time.time() - start_time) / iteration, 3)} sec/iter\")\n",
    "            \n",
    "    return iteration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 5e-3\n",
    "weight_decay = 5e-4\n",
    "sparsity_lamda = 2e-3\n",
    "epochs = 100\n",
    "output_dim = len(set(clusters))\n",
    "log_interval = 1\n",
    "use_cuda = device != 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = SDT(input_dim=tensor_dataset.shape[1], output_dim=len(clusters - 1), depth=tree_depth, lamda=1e-3, use_cuda=use_cuda)\n",
    "optimizer = torch.optim.Adam(tree.parameters(),\n",
    "                                 lr=lr,\n",
    "                                 weight_decay=weight_decay)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "tree = tree.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "accs = []\n",
    "sparsity = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.0\n",
      "layer 0: 0.0\n",
      "layer 1: 0.0\n",
      "layer 2: 0.0\n",
      "layer 3: 0.0\n",
      "layer 4: 0.0\n",
      "Epoch: 00 | Batch: 000 / 025 | Total loss: 9.590 | Reg loss: 0.007 | Tree loss: 9.590 | Accuracy: 0.000000 | 0.097 sec/iter\n",
      "Epoch: 00 | Batch: 001 / 025 | Total loss: 9.587 | Reg loss: 0.007 | Tree loss: 9.587 | Accuracy: 0.000000 | 0.086 sec/iter\n",
      "Epoch: 00 | Batch: 002 / 025 | Total loss: 9.571 | Reg loss: 0.007 | Tree loss: 9.571 | Accuracy: 0.000000 | 0.079 sec/iter\n",
      "Epoch: 00 | Batch: 003 / 025 | Total loss: 9.558 | Reg loss: 0.007 | Tree loss: 9.558 | Accuracy: 0.000000 | 0.075 sec/iter\n",
      "Epoch: 00 | Batch: 004 / 025 | Total loss: 9.561 | Reg loss: 0.006 | Tree loss: 9.561 | Accuracy: 0.000000 | 0.072 sec/iter\n",
      "Epoch: 00 | Batch: 005 / 025 | Total loss: 9.550 | Reg loss: 0.006 | Tree loss: 9.550 | Accuracy: 0.000000 | 0.07 sec/iter\n",
      "Epoch: 00 | Batch: 006 / 025 | Total loss: 9.535 | Reg loss: 0.006 | Tree loss: 9.535 | Accuracy: 0.000000 | 0.068 sec/iter\n",
      "Epoch: 00 | Batch: 007 / 025 | Total loss: 9.523 | Reg loss: 0.006 | Tree loss: 9.523 | Accuracy: 0.000000 | 0.068 sec/iter\n",
      "Epoch: 00 | Batch: 008 / 025 | Total loss: 9.522 | Reg loss: 0.006 | Tree loss: 9.522 | Accuracy: 0.000000 | 0.068 sec/iter\n",
      "Epoch: 00 | Batch: 009 / 025 | Total loss: 9.502 | Reg loss: 0.007 | Tree loss: 9.502 | Accuracy: 0.000000 | 0.068 sec/iter\n",
      "Epoch: 00 | Batch: 010 / 025 | Total loss: 9.497 | Reg loss: 0.007 | Tree loss: 9.497 | Accuracy: 0.000000 | 0.068 sec/iter\n",
      "Epoch: 00 | Batch: 011 / 025 | Total loss: 9.491 | Reg loss: 0.007 | Tree loss: 9.491 | Accuracy: 0.000000 | 0.068 sec/iter\n",
      "Epoch: 00 | Batch: 012 / 025 | Total loss: 9.490 | Reg loss: 0.007 | Tree loss: 9.490 | Accuracy: 0.000000 | 0.069 sec/iter\n",
      "Epoch: 00 | Batch: 013 / 025 | Total loss: 9.469 | Reg loss: 0.007 | Tree loss: 9.469 | Accuracy: 0.000000 | 0.069 sec/iter\n",
      "Epoch: 00 | Batch: 014 / 025 | Total loss: 9.466 | Reg loss: 0.007 | Tree loss: 9.466 | Accuracy: 0.000000 | 0.069 sec/iter\n",
      "Epoch: 00 | Batch: 015 / 025 | Total loss: 9.455 | Reg loss: 0.007 | Tree loss: 9.455 | Accuracy: 0.000000 | 0.069 sec/iter\n",
      "Epoch: 00 | Batch: 016 / 025 | Total loss: 9.447 | Reg loss: 0.007 | Tree loss: 9.447 | Accuracy: 0.000000 | 0.069 sec/iter\n",
      "Epoch: 00 | Batch: 017 / 025 | Total loss: 9.437 | Reg loss: 0.008 | Tree loss: 9.437 | Accuracy: 0.000000 | 0.069 sec/iter\n",
      "Epoch: 00 | Batch: 018 / 025 | Total loss: 9.424 | Reg loss: 0.008 | Tree loss: 9.424 | Accuracy: 0.000000 | 0.069 sec/iter\n",
      "Epoch: 00 | Batch: 019 / 025 | Total loss: 9.414 | Reg loss: 0.008 | Tree loss: 9.414 | Accuracy: 0.000000 | 0.069 sec/iter\n",
      "Epoch: 00 | Batch: 020 / 025 | Total loss: 9.407 | Reg loss: 0.008 | Tree loss: 9.407 | Accuracy: 0.000000 | 0.069 sec/iter\n",
      "Epoch: 00 | Batch: 021 / 025 | Total loss: 9.398 | Reg loss: 0.008 | Tree loss: 9.398 | Accuracy: 0.000000 | 0.068 sec/iter\n",
      "Epoch: 00 | Batch: 022 / 025 | Total loss: 9.396 | Reg loss: 0.009 | Tree loss: 9.396 | Accuracy: 0.000000 | 0.068 sec/iter\n",
      "Epoch: 00 | Batch: 023 / 025 | Total loss: 9.380 | Reg loss: 0.009 | Tree loss: 9.380 | Accuracy: 0.000000 | 0.068 sec/iter\n",
      "Epoch: 00 | Batch: 024 / 025 | Total loss: 9.380 | Reg loss: 0.009 | Tree loss: 9.380 | Accuracy: 0.000000 | 0.068 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 01 | Batch: 000 / 025 | Total loss: 9.452 | Reg loss: 0.003 | Tree loss: 9.452 | Accuracy: 0.000000 | 0.069 sec/iter\n",
      "Epoch: 01 | Batch: 001 / 025 | Total loss: 9.440 | Reg loss: 0.003 | Tree loss: 9.440 | Accuracy: 0.000000 | 0.069 sec/iter\n",
      "Epoch: 01 | Batch: 002 / 025 | Total loss: 9.432 | Reg loss: 0.003 | Tree loss: 9.432 | Accuracy: 0.000000 | 0.069 sec/iter\n",
      "Epoch: 01 | Batch: 003 / 025 | Total loss: 9.421 | Reg loss: 0.004 | Tree loss: 9.421 | Accuracy: 0.000000 | 0.068 sec/iter\n",
      "Epoch: 01 | Batch: 004 / 025 | Total loss: 9.417 | Reg loss: 0.004 | Tree loss: 9.417 | Accuracy: 0.000000 | 0.068 sec/iter\n",
      "Epoch: 01 | Batch: 005 / 025 | Total loss: 9.405 | Reg loss: 0.004 | Tree loss: 9.405 | Accuracy: 0.000000 | 0.068 sec/iter\n",
      "Epoch: 01 | Batch: 006 / 025 | Total loss: 9.396 | Reg loss: 0.004 | Tree loss: 9.396 | Accuracy: 0.000000 | 0.068 sec/iter\n",
      "Epoch: 01 | Batch: 007 / 025 | Total loss: 9.383 | Reg loss: 0.004 | Tree loss: 9.383 | Accuracy: 0.000000 | 0.068 sec/iter\n",
      "Epoch: 01 | Batch: 008 / 025 | Total loss: 9.378 | Reg loss: 0.005 | Tree loss: 9.378 | Accuracy: 0.000000 | 0.068 sec/iter\n",
      "Epoch: 01 | Batch: 009 / 025 | Total loss: 9.373 | Reg loss: 0.005 | Tree loss: 9.373 | Accuracy: 0.000000 | 0.068 sec/iter\n",
      "Epoch: 01 | Batch: 010 / 025 | Total loss: 9.360 | Reg loss: 0.005 | Tree loss: 9.360 | Accuracy: 0.000000 | 0.068 sec/iter\n",
      "Epoch: 01 | Batch: 011 / 025 | Total loss: 9.354 | Reg loss: 0.005 | Tree loss: 9.354 | Accuracy: 0.001953 | 0.068 sec/iter\n",
      "Epoch: 01 | Batch: 012 / 025 | Total loss: 9.343 | Reg loss: 0.006 | Tree loss: 9.343 | Accuracy: 0.001953 | 0.068 sec/iter\n",
      "Epoch: 01 | Batch: 013 / 025 | Total loss: 9.336 | Reg loss: 0.006 | Tree loss: 9.336 | Accuracy: 0.001953 | 0.068 sec/iter\n",
      "Epoch: 01 | Batch: 014 / 025 | Total loss: 9.324 | Reg loss: 0.006 | Tree loss: 9.324 | Accuracy: 0.009766 | 0.069 sec/iter\n",
      "Epoch: 01 | Batch: 015 / 025 | Total loss: 9.320 | Reg loss: 0.007 | Tree loss: 9.320 | Accuracy: 0.019531 | 0.069 sec/iter\n",
      "Epoch: 01 | Batch: 016 / 025 | Total loss: 9.304 | Reg loss: 0.007 | Tree loss: 9.304 | Accuracy: 0.025391 | 0.068 sec/iter\n",
      "Epoch: 01 | Batch: 017 / 025 | Total loss: 9.300 | Reg loss: 0.007 | Tree loss: 9.300 | Accuracy: 0.039062 | 0.068 sec/iter\n",
      "Epoch: 01 | Batch: 018 / 025 | Total loss: 9.286 | Reg loss: 0.007 | Tree loss: 9.286 | Accuracy: 0.056641 | 0.068 sec/iter\n",
      "Epoch: 01 | Batch: 019 / 025 | Total loss: 9.285 | Reg loss: 0.008 | Tree loss: 9.285 | Accuracy: 0.060547 | 0.068 sec/iter\n",
      "Epoch: 01 | Batch: 020 / 025 | Total loss: 9.272 | Reg loss: 0.008 | Tree loss: 9.272 | Accuracy: 0.083984 | 0.068 sec/iter\n",
      "Epoch: 01 | Batch: 021 / 025 | Total loss: 9.262 | Reg loss: 0.008 | Tree loss: 9.262 | Accuracy: 0.093750 | 0.069 sec/iter\n",
      "Epoch: 01 | Batch: 022 / 025 | Total loss: 9.254 | Reg loss: 0.009 | Tree loss: 9.254 | Accuracy: 0.119141 | 0.069 sec/iter\n",
      "Epoch: 01 | Batch: 023 / 025 | Total loss: 9.246 | Reg loss: 0.009 | Tree loss: 9.246 | Accuracy: 0.132812 | 0.069 sec/iter\n",
      "Epoch: 01 | Batch: 024 / 025 | Total loss: 9.239 | Reg loss: 0.009 | Tree loss: 9.239 | Accuracy: 0.103125 | 0.068 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 02 | Batch: 000 / 025 | Total loss: 9.324 | Reg loss: 0.005 | Tree loss: 9.324 | Accuracy: 0.027344 | 0.069 sec/iter\n",
      "Epoch: 02 | Batch: 001 / 025 | Total loss: 9.310 | Reg loss: 0.005 | Tree loss: 9.310 | Accuracy: 0.041016 | 0.069 sec/iter\n",
      "Epoch: 02 | Batch: 002 / 025 | Total loss: 9.306 | Reg loss: 0.005 | Tree loss: 9.306 | Accuracy: 0.039062 | 0.069 sec/iter\n",
      "Epoch: 02 | Batch: 003 / 025 | Total loss: 9.288 | Reg loss: 0.005 | Tree loss: 9.288 | Accuracy: 0.074219 | 0.068 sec/iter\n",
      "Epoch: 02 | Batch: 004 / 025 | Total loss: 9.279 | Reg loss: 0.005 | Tree loss: 9.279 | Accuracy: 0.068359 | 0.068 sec/iter\n",
      "Epoch: 02 | Batch: 005 / 025 | Total loss: 9.276 | Reg loss: 0.006 | Tree loss: 9.276 | Accuracy: 0.085938 | 0.068 sec/iter\n",
      "Epoch: 02 | Batch: 006 / 025 | Total loss: 9.266 | Reg loss: 0.006 | Tree loss: 9.266 | Accuracy: 0.093750 | 0.068 sec/iter\n",
      "Epoch: 02 | Batch: 007 / 025 | Total loss: 9.251 | Reg loss: 0.006 | Tree loss: 9.251 | Accuracy: 0.111328 | 0.068 sec/iter\n",
      "Epoch: 02 | Batch: 008 / 025 | Total loss: 9.242 | Reg loss: 0.006 | Tree loss: 9.242 | Accuracy: 0.117188 | 0.068 sec/iter\n",
      "Epoch: 02 | Batch: 009 / 025 | Total loss: 9.239 | Reg loss: 0.006 | Tree loss: 9.239 | Accuracy: 0.119141 | 0.068 sec/iter\n",
      "Epoch: 02 | Batch: 010 / 025 | Total loss: 9.226 | Reg loss: 0.007 | Tree loss: 9.226 | Accuracy: 0.119141 | 0.068 sec/iter\n",
      "Epoch: 02 | Batch: 011 / 025 | Total loss: 9.218 | Reg loss: 0.007 | Tree loss: 9.218 | Accuracy: 0.111328 | 0.068 sec/iter\n",
      "Epoch: 02 | Batch: 012 / 025 | Total loss: 9.214 | Reg loss: 0.007 | Tree loss: 9.214 | Accuracy: 0.095703 | 0.068 sec/iter\n",
      "Epoch: 02 | Batch: 013 / 025 | Total loss: 9.206 | Reg loss: 0.007 | Tree loss: 9.206 | Accuracy: 0.083984 | 0.069 sec/iter\n",
      "Epoch: 02 | Batch: 014 / 025 | Total loss: 9.187 | Reg loss: 0.008 | Tree loss: 9.187 | Accuracy: 0.130859 | 0.069 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02 | Batch: 015 / 025 | Total loss: 9.186 | Reg loss: 0.008 | Tree loss: 9.186 | Accuracy: 0.101562 | 0.069 sec/iter\n",
      "Epoch: 02 | Batch: 016 / 025 | Total loss: 9.175 | Reg loss: 0.008 | Tree loss: 9.175 | Accuracy: 0.111328 | 0.069 sec/iter\n",
      "Epoch: 02 | Batch: 017 / 025 | Total loss: 9.168 | Reg loss: 0.008 | Tree loss: 9.168 | Accuracy: 0.093750 | 0.069 sec/iter\n",
      "Epoch: 02 | Batch: 018 / 025 | Total loss: 9.159 | Reg loss: 0.009 | Tree loss: 9.159 | Accuracy: 0.109375 | 0.069 sec/iter\n",
      "Epoch: 02 | Batch: 019 / 025 | Total loss: 9.142 | Reg loss: 0.009 | Tree loss: 9.142 | Accuracy: 0.109375 | 0.069 sec/iter\n",
      "Epoch: 02 | Batch: 020 / 025 | Total loss: 9.141 | Reg loss: 0.009 | Tree loss: 9.141 | Accuracy: 0.099609 | 0.069 sec/iter\n",
      "Epoch: 02 | Batch: 021 / 025 | Total loss: 9.129 | Reg loss: 0.010 | Tree loss: 9.129 | Accuracy: 0.089844 | 0.069 sec/iter\n",
      "Epoch: 02 | Batch: 022 / 025 | Total loss: 9.128 | Reg loss: 0.010 | Tree loss: 9.128 | Accuracy: 0.115234 | 0.069 sec/iter\n",
      "Epoch: 02 | Batch: 023 / 025 | Total loss: 9.104 | Reg loss: 0.010 | Tree loss: 9.104 | Accuracy: 0.111328 | 0.069 sec/iter\n",
      "Epoch: 02 | Batch: 024 / 025 | Total loss: 9.094 | Reg loss: 0.010 | Tree loss: 9.094 | Accuracy: 0.137500 | 0.069 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 03 | Batch: 000 / 025 | Total loss: 9.184 | Reg loss: 0.007 | Tree loss: 9.184 | Accuracy: 0.117188 | 0.07 sec/iter\n",
      "Epoch: 03 | Batch: 001 / 025 | Total loss: 9.174 | Reg loss: 0.007 | Tree loss: 9.174 | Accuracy: 0.103516 | 0.07 sec/iter\n",
      "Epoch: 03 | Batch: 002 / 025 | Total loss: 9.175 | Reg loss: 0.007 | Tree loss: 9.175 | Accuracy: 0.089844 | 0.07 sec/iter\n",
      "Epoch: 03 | Batch: 003 / 025 | Total loss: 9.163 | Reg loss: 0.007 | Tree loss: 9.163 | Accuracy: 0.109375 | 0.07 sec/iter\n",
      "Epoch: 03 | Batch: 004 / 025 | Total loss: 9.153 | Reg loss: 0.007 | Tree loss: 9.153 | Accuracy: 0.111328 | 0.07 sec/iter\n",
      "Epoch: 03 | Batch: 005 / 025 | Total loss: 9.144 | Reg loss: 0.007 | Tree loss: 9.144 | Accuracy: 0.091797 | 0.07 sec/iter\n",
      "Epoch: 03 | Batch: 006 / 025 | Total loss: 9.132 | Reg loss: 0.007 | Tree loss: 9.132 | Accuracy: 0.111328 | 0.07 sec/iter\n",
      "Epoch: 03 | Batch: 007 / 025 | Total loss: 9.130 | Reg loss: 0.007 | Tree loss: 9.130 | Accuracy: 0.103516 | 0.069 sec/iter\n",
      "Epoch: 03 | Batch: 008 / 025 | Total loss: 9.120 | Reg loss: 0.008 | Tree loss: 9.120 | Accuracy: 0.083984 | 0.069 sec/iter\n",
      "Epoch: 03 | Batch: 009 / 025 | Total loss: 9.100 | Reg loss: 0.008 | Tree loss: 9.100 | Accuracy: 0.117188 | 0.069 sec/iter\n",
      "Epoch: 03 | Batch: 010 / 025 | Total loss: 9.100 | Reg loss: 0.008 | Tree loss: 9.100 | Accuracy: 0.126953 | 0.069 sec/iter\n",
      "Epoch: 03 | Batch: 011 / 025 | Total loss: 9.083 | Reg loss: 0.008 | Tree loss: 9.083 | Accuracy: 0.099609 | 0.069 sec/iter\n",
      "Epoch: 03 | Batch: 012 / 025 | Total loss: 9.077 | Reg loss: 0.009 | Tree loss: 9.077 | Accuracy: 0.105469 | 0.069 sec/iter\n",
      "Epoch: 03 | Batch: 013 / 025 | Total loss: 9.064 | Reg loss: 0.009 | Tree loss: 9.064 | Accuracy: 0.117188 | 0.07 sec/iter\n",
      "Epoch: 03 | Batch: 014 / 025 | Total loss: 9.064 | Reg loss: 0.009 | Tree loss: 9.064 | Accuracy: 0.105469 | 0.07 sec/iter\n",
      "Epoch: 03 | Batch: 015 / 025 | Total loss: 9.058 | Reg loss: 0.009 | Tree loss: 9.058 | Accuracy: 0.080078 | 0.07 sec/iter\n",
      "Epoch: 03 | Batch: 016 / 025 | Total loss: 9.040 | Reg loss: 0.010 | Tree loss: 9.040 | Accuracy: 0.107422 | 0.07 sec/iter\n",
      "Epoch: 03 | Batch: 017 / 025 | Total loss: 9.028 | Reg loss: 0.010 | Tree loss: 9.028 | Accuracy: 0.093750 | 0.07 sec/iter\n",
      "Epoch: 03 | Batch: 018 / 025 | Total loss: 9.022 | Reg loss: 0.010 | Tree loss: 9.022 | Accuracy: 0.099609 | 0.07 sec/iter\n",
      "Epoch: 03 | Batch: 019 / 025 | Total loss: 9.018 | Reg loss: 0.010 | Tree loss: 9.018 | Accuracy: 0.101562 | 0.07 sec/iter\n",
      "Epoch: 03 | Batch: 020 / 025 | Total loss: 9.003 | Reg loss: 0.011 | Tree loss: 9.003 | Accuracy: 0.125000 | 0.07 sec/iter\n",
      "Epoch: 03 | Batch: 021 / 025 | Total loss: 8.992 | Reg loss: 0.011 | Tree loss: 8.992 | Accuracy: 0.109375 | 0.07 sec/iter\n",
      "Epoch: 03 | Batch: 022 / 025 | Total loss: 8.987 | Reg loss: 0.011 | Tree loss: 8.987 | Accuracy: 0.113281 | 0.07 sec/iter\n",
      "Epoch: 03 | Batch: 023 / 025 | Total loss: 8.983 | Reg loss: 0.011 | Tree loss: 8.983 | Accuracy: 0.115234 | 0.07 sec/iter\n",
      "Epoch: 03 | Batch: 024 / 025 | Total loss: 8.980 | Reg loss: 0.012 | Tree loss: 8.980 | Accuracy: 0.096875 | 0.07 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 04 | Batch: 000 / 025 | Total loss: 9.065 | Reg loss: 0.008 | Tree loss: 9.065 | Accuracy: 0.093750 | 0.07 sec/iter\n",
      "Epoch: 04 | Batch: 001 / 025 | Total loss: 9.055 | Reg loss: 0.008 | Tree loss: 9.055 | Accuracy: 0.123047 | 0.07 sec/iter\n",
      "Epoch: 04 | Batch: 002 / 025 | Total loss: 9.052 | Reg loss: 0.008 | Tree loss: 9.052 | Accuracy: 0.095703 | 0.07 sec/iter\n",
      "Epoch: 04 | Batch: 003 / 025 | Total loss: 9.025 | Reg loss: 0.008 | Tree loss: 9.025 | Accuracy: 0.128906 | 0.07 sec/iter\n",
      "Epoch: 04 | Batch: 004 / 025 | Total loss: 9.024 | Reg loss: 0.009 | Tree loss: 9.024 | Accuracy: 0.103516 | 0.07 sec/iter\n",
      "Epoch: 04 | Batch: 005 / 025 | Total loss: 9.015 | Reg loss: 0.009 | Tree loss: 9.015 | Accuracy: 0.111328 | 0.07 sec/iter\n",
      "Epoch: 04 | Batch: 006 / 025 | Total loss: 9.000 | Reg loss: 0.009 | Tree loss: 9.000 | Accuracy: 0.099609 | 0.07 sec/iter\n",
      "Epoch: 04 | Batch: 007 / 025 | Total loss: 8.998 | Reg loss: 0.009 | Tree loss: 8.998 | Accuracy: 0.091797 | 0.07 sec/iter\n",
      "Epoch: 04 | Batch: 008 / 025 | Total loss: 8.983 | Reg loss: 0.009 | Tree loss: 8.983 | Accuracy: 0.078125 | 0.07 sec/iter\n",
      "Epoch: 04 | Batch: 009 / 025 | Total loss: 8.973 | Reg loss: 0.009 | Tree loss: 8.973 | Accuracy: 0.128906 | 0.07 sec/iter\n",
      "Epoch: 04 | Batch: 010 / 025 | Total loss: 8.974 | Reg loss: 0.010 | Tree loss: 8.974 | Accuracy: 0.078125 | 0.07 sec/iter\n",
      "Epoch: 04 | Batch: 011 / 025 | Total loss: 8.951 | Reg loss: 0.010 | Tree loss: 8.951 | Accuracy: 0.125000 | 0.07 sec/iter\n",
      "Epoch: 04 | Batch: 012 / 025 | Total loss: 8.946 | Reg loss: 0.010 | Tree loss: 8.946 | Accuracy: 0.117188 | 0.07 sec/iter\n",
      "Epoch: 04 | Batch: 013 / 025 | Total loss: 8.926 | Reg loss: 0.010 | Tree loss: 8.926 | Accuracy: 0.109375 | 0.07 sec/iter\n",
      "Epoch: 04 | Batch: 014 / 025 | Total loss: 8.921 | Reg loss: 0.010 | Tree loss: 8.921 | Accuracy: 0.103516 | 0.07 sec/iter\n",
      "Epoch: 04 | Batch: 015 / 025 | Total loss: 8.914 | Reg loss: 0.011 | Tree loss: 8.914 | Accuracy: 0.140625 | 0.07 sec/iter\n",
      "Epoch: 04 | Batch: 016 / 025 | Total loss: 8.904 | Reg loss: 0.011 | Tree loss: 8.904 | Accuracy: 0.103516 | 0.07 sec/iter\n",
      "Epoch: 04 | Batch: 017 / 025 | Total loss: 8.896 | Reg loss: 0.011 | Tree loss: 8.896 | Accuracy: 0.117188 | 0.07 sec/iter\n",
      "Epoch: 04 | Batch: 018 / 025 | Total loss: 8.890 | Reg loss: 0.011 | Tree loss: 8.890 | Accuracy: 0.093750 | 0.07 sec/iter\n",
      "Epoch: 04 | Batch: 019 / 025 | Total loss: 8.873 | Reg loss: 0.012 | Tree loss: 8.873 | Accuracy: 0.097656 | 0.07 sec/iter\n",
      "Epoch: 04 | Batch: 020 / 025 | Total loss: 8.878 | Reg loss: 0.012 | Tree loss: 8.878 | Accuracy: 0.076172 | 0.07 sec/iter\n",
      "Epoch: 04 | Batch: 021 / 025 | Total loss: 8.870 | Reg loss: 0.012 | Tree loss: 8.870 | Accuracy: 0.097656 | 0.07 sec/iter\n",
      "Epoch: 04 | Batch: 022 / 025 | Total loss: 8.852 | Reg loss: 0.013 | Tree loss: 8.852 | Accuracy: 0.103516 | 0.07 sec/iter\n",
      "Epoch: 04 | Batch: 023 / 025 | Total loss: 8.838 | Reg loss: 0.013 | Tree loss: 8.838 | Accuracy: 0.093750 | 0.07 sec/iter\n",
      "Epoch: 04 | Batch: 024 / 025 | Total loss: 8.837 | Reg loss: 0.013 | Tree loss: 8.837 | Accuracy: 0.106250 | 0.07 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 05 | Batch: 000 / 025 | Total loss: 8.931 | Reg loss: 0.010 | Tree loss: 8.931 | Accuracy: 0.103516 | 0.07 sec/iter\n",
      "Epoch: 05 | Batch: 001 / 025 | Total loss: 8.929 | Reg loss: 0.010 | Tree loss: 8.929 | Accuracy: 0.087891 | 0.07 sec/iter\n",
      "Epoch: 05 | Batch: 002 / 025 | Total loss: 8.914 | Reg loss: 0.010 | Tree loss: 8.914 | Accuracy: 0.103516 | 0.07 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 05 | Batch: 003 / 025 | Total loss: 8.896 | Reg loss: 0.010 | Tree loss: 8.896 | Accuracy: 0.117188 | 0.07 sec/iter\n",
      "Epoch: 05 | Batch: 004 / 025 | Total loss: 8.892 | Reg loss: 0.010 | Tree loss: 8.892 | Accuracy: 0.078125 | 0.07 sec/iter\n",
      "Epoch: 05 | Batch: 005 / 025 | Total loss: 8.877 | Reg loss: 0.010 | Tree loss: 8.877 | Accuracy: 0.111328 | 0.07 sec/iter\n",
      "Epoch: 05 | Batch: 006 / 025 | Total loss: 8.872 | Reg loss: 0.010 | Tree loss: 8.872 | Accuracy: 0.111328 | 0.07 sec/iter\n",
      "Epoch: 05 | Batch: 007 / 025 | Total loss: 8.862 | Reg loss: 0.010 | Tree loss: 8.862 | Accuracy: 0.105469 | 0.07 sec/iter\n",
      "Epoch: 05 | Batch: 008 / 025 | Total loss: 8.851 | Reg loss: 0.011 | Tree loss: 8.851 | Accuracy: 0.130859 | 0.07 sec/iter\n",
      "Epoch: 05 | Batch: 009 / 025 | Total loss: 8.839 | Reg loss: 0.011 | Tree loss: 8.839 | Accuracy: 0.107422 | 0.07 sec/iter\n",
      "Epoch: 05 | Batch: 010 / 025 | Total loss: 8.832 | Reg loss: 0.011 | Tree loss: 8.832 | Accuracy: 0.101562 | 0.07 sec/iter\n",
      "Epoch: 05 | Batch: 011 / 025 | Total loss: 8.826 | Reg loss: 0.011 | Tree loss: 8.826 | Accuracy: 0.087891 | 0.07 sec/iter\n",
      "Epoch: 05 | Batch: 012 / 025 | Total loss: 8.813 | Reg loss: 0.012 | Tree loss: 8.813 | Accuracy: 0.093750 | 0.07 sec/iter\n",
      "Epoch: 05 | Batch: 013 / 025 | Total loss: 8.803 | Reg loss: 0.012 | Tree loss: 8.803 | Accuracy: 0.099609 | 0.07 sec/iter\n",
      "Epoch: 05 | Batch: 014 / 025 | Total loss: 8.789 | Reg loss: 0.012 | Tree loss: 8.789 | Accuracy: 0.117188 | 0.07 sec/iter\n",
      "Epoch: 05 | Batch: 015 / 025 | Total loss: 8.784 | Reg loss: 0.012 | Tree loss: 8.784 | Accuracy: 0.087891 | 0.07 sec/iter\n",
      "Epoch: 05 | Batch: 016 / 025 | Total loss: 8.766 | Reg loss: 0.013 | Tree loss: 8.766 | Accuracy: 0.115234 | 0.07 sec/iter\n",
      "Epoch: 05 | Batch: 017 / 025 | Total loss: 8.753 | Reg loss: 0.013 | Tree loss: 8.753 | Accuracy: 0.130859 | 0.07 sec/iter\n",
      "Epoch: 05 | Batch: 018 / 025 | Total loss: 8.756 | Reg loss: 0.013 | Tree loss: 8.756 | Accuracy: 0.109375 | 0.07 sec/iter\n",
      "Epoch: 05 | Batch: 019 / 025 | Total loss: 8.742 | Reg loss: 0.013 | Tree loss: 8.742 | Accuracy: 0.113281 | 0.07 sec/iter\n",
      "Epoch: 05 | Batch: 020 / 025 | Total loss: 8.725 | Reg loss: 0.014 | Tree loss: 8.725 | Accuracy: 0.121094 | 0.07 sec/iter\n",
      "Epoch: 05 | Batch: 021 / 025 | Total loss: 8.707 | Reg loss: 0.014 | Tree loss: 8.707 | Accuracy: 0.134766 | 0.07 sec/iter\n",
      "Epoch: 05 | Batch: 022 / 025 | Total loss: 8.700 | Reg loss: 0.014 | Tree loss: 8.700 | Accuracy: 0.095703 | 0.07 sec/iter\n",
      "Epoch: 05 | Batch: 023 / 025 | Total loss: 8.690 | Reg loss: 0.014 | Tree loss: 8.690 | Accuracy: 0.101562 | 0.07 sec/iter\n",
      "Epoch: 05 | Batch: 024 / 025 | Total loss: 8.698 | Reg loss: 0.015 | Tree loss: 8.698 | Accuracy: 0.075000 | 0.07 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 06 | Batch: 000 / 025 | Total loss: 8.795 | Reg loss: 0.011 | Tree loss: 8.795 | Accuracy: 0.123047 | 0.07 sec/iter\n",
      "Epoch: 06 | Batch: 001 / 025 | Total loss: 8.787 | Reg loss: 0.011 | Tree loss: 8.787 | Accuracy: 0.095703 | 0.07 sec/iter\n",
      "Epoch: 06 | Batch: 002 / 025 | Total loss: 8.780 | Reg loss: 0.011 | Tree loss: 8.780 | Accuracy: 0.095703 | 0.07 sec/iter\n",
      "Epoch: 06 | Batch: 003 / 025 | Total loss: 8.754 | Reg loss: 0.011 | Tree loss: 8.754 | Accuracy: 0.130859 | 0.07 sec/iter\n",
      "Epoch: 06 | Batch: 004 / 025 | Total loss: 8.763 | Reg loss: 0.012 | Tree loss: 8.763 | Accuracy: 0.089844 | 0.07 sec/iter\n",
      "Epoch: 06 | Batch: 005 / 025 | Total loss: 8.740 | Reg loss: 0.012 | Tree loss: 8.740 | Accuracy: 0.095703 | 0.07 sec/iter\n",
      "Epoch: 06 | Batch: 006 / 025 | Total loss: 8.738 | Reg loss: 0.012 | Tree loss: 8.738 | Accuracy: 0.101562 | 0.07 sec/iter\n",
      "Epoch: 06 | Batch: 007 / 025 | Total loss: 8.724 | Reg loss: 0.012 | Tree loss: 8.724 | Accuracy: 0.074219 | 0.07 sec/iter\n",
      "Epoch: 06 | Batch: 008 / 025 | Total loss: 8.705 | Reg loss: 0.012 | Tree loss: 8.705 | Accuracy: 0.107422 | 0.07 sec/iter\n",
      "Epoch: 06 | Batch: 009 / 025 | Total loss: 8.693 | Reg loss: 0.013 | Tree loss: 8.693 | Accuracy: 0.125000 | 0.07 sec/iter\n",
      "Epoch: 06 | Batch: 010 / 025 | Total loss: 8.677 | Reg loss: 0.013 | Tree loss: 8.677 | Accuracy: 0.113281 | 0.07 sec/iter\n",
      "Epoch: 06 | Batch: 011 / 025 | Total loss: 8.681 | Reg loss: 0.013 | Tree loss: 8.681 | Accuracy: 0.082031 | 0.07 sec/iter\n",
      "Epoch: 06 | Batch: 012 / 025 | Total loss: 8.665 | Reg loss: 0.013 | Tree loss: 8.665 | Accuracy: 0.109375 | 0.07 sec/iter\n",
      "Epoch: 06 | Batch: 013 / 025 | Total loss: 8.653 | Reg loss: 0.014 | Tree loss: 8.653 | Accuracy: 0.099609 | 0.07 sec/iter\n",
      "Epoch: 06 | Batch: 014 / 025 | Total loss: 8.633 | Reg loss: 0.014 | Tree loss: 8.633 | Accuracy: 0.091797 | 0.07 sec/iter\n",
      "Epoch: 06 | Batch: 015 / 025 | Total loss: 8.636 | Reg loss: 0.014 | Tree loss: 8.636 | Accuracy: 0.085938 | 0.07 sec/iter\n",
      "Epoch: 06 | Batch: 016 / 025 | Total loss: 8.626 | Reg loss: 0.014 | Tree loss: 8.626 | Accuracy: 0.107422 | 0.069 sec/iter\n",
      "Epoch: 06 | Batch: 017 / 025 | Total loss: 8.603 | Reg loss: 0.015 | Tree loss: 8.603 | Accuracy: 0.111328 | 0.069 sec/iter\n",
      "Epoch: 06 | Batch: 018 / 025 | Total loss: 8.598 | Reg loss: 0.015 | Tree loss: 8.598 | Accuracy: 0.097656 | 0.069 sec/iter\n",
      "Epoch: 06 | Batch: 019 / 025 | Total loss: 8.580 | Reg loss: 0.015 | Tree loss: 8.580 | Accuracy: 0.111328 | 0.069 sec/iter\n",
      "Epoch: 06 | Batch: 020 / 025 | Total loss: 8.557 | Reg loss: 0.016 | Tree loss: 8.557 | Accuracy: 0.121094 | 0.069 sec/iter\n",
      "Epoch: 06 | Batch: 021 / 025 | Total loss: 8.562 | Reg loss: 0.016 | Tree loss: 8.562 | Accuracy: 0.105469 | 0.069 sec/iter\n",
      "Epoch: 06 | Batch: 022 / 025 | Total loss: 8.551 | Reg loss: 0.016 | Tree loss: 8.551 | Accuracy: 0.101562 | 0.069 sec/iter\n",
      "Epoch: 06 | Batch: 023 / 025 | Total loss: 8.527 | Reg loss: 0.016 | Tree loss: 8.527 | Accuracy: 0.101562 | 0.069 sec/iter\n",
      "Epoch: 06 | Batch: 024 / 025 | Total loss: 8.530 | Reg loss: 0.017 | Tree loss: 8.530 | Accuracy: 0.121875 | 0.069 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 07 | Batch: 000 / 025 | Total loss: 8.655 | Reg loss: 0.013 | Tree loss: 8.655 | Accuracy: 0.111328 | 0.069 sec/iter\n",
      "Epoch: 07 | Batch: 001 / 025 | Total loss: 8.637 | Reg loss: 0.013 | Tree loss: 8.637 | Accuracy: 0.105469 | 0.069 sec/iter\n",
      "Epoch: 07 | Batch: 002 / 025 | Total loss: 8.625 | Reg loss: 0.013 | Tree loss: 8.625 | Accuracy: 0.097656 | 0.069 sec/iter\n",
      "Epoch: 07 | Batch: 003 / 025 | Total loss: 8.613 | Reg loss: 0.013 | Tree loss: 8.613 | Accuracy: 0.101562 | 0.069 sec/iter\n",
      "Epoch: 07 | Batch: 004 / 025 | Total loss: 8.606 | Reg loss: 0.013 | Tree loss: 8.606 | Accuracy: 0.123047 | 0.069 sec/iter\n",
      "Epoch: 07 | Batch: 005 / 025 | Total loss: 8.586 | Reg loss: 0.014 | Tree loss: 8.586 | Accuracy: 0.119141 | 0.069 sec/iter\n",
      "Epoch: 07 | Batch: 006 / 025 | Total loss: 8.577 | Reg loss: 0.014 | Tree loss: 8.577 | Accuracy: 0.107422 | 0.069 sec/iter\n",
      "Epoch: 07 | Batch: 007 / 025 | Total loss: 8.560 | Reg loss: 0.014 | Tree loss: 8.560 | Accuracy: 0.095703 | 0.069 sec/iter\n",
      "Epoch: 07 | Batch: 008 / 025 | Total loss: 8.566 | Reg loss: 0.014 | Tree loss: 8.566 | Accuracy: 0.119141 | 0.069 sec/iter\n",
      "Epoch: 07 | Batch: 009 / 025 | Total loss: 8.548 | Reg loss: 0.014 | Tree loss: 8.548 | Accuracy: 0.095703 | 0.069 sec/iter\n",
      "Epoch: 07 | Batch: 010 / 025 | Total loss: 8.532 | Reg loss: 0.015 | Tree loss: 8.532 | Accuracy: 0.082031 | 0.069 sec/iter\n",
      "Epoch: 07 | Batch: 011 / 025 | Total loss: 8.518 | Reg loss: 0.015 | Tree loss: 8.518 | Accuracy: 0.105469 | 0.069 sec/iter\n",
      "Epoch: 07 | Batch: 012 / 025 | Total loss: 8.485 | Reg loss: 0.015 | Tree loss: 8.485 | Accuracy: 0.128906 | 0.069 sec/iter\n",
      "Epoch: 07 | Batch: 013 / 025 | Total loss: 8.471 | Reg loss: 0.016 | Tree loss: 8.471 | Accuracy: 0.134766 | 0.069 sec/iter\n",
      "Epoch: 07 | Batch: 014 / 025 | Total loss: 8.460 | Reg loss: 0.016 | Tree loss: 8.460 | Accuracy: 0.093750 | 0.069 sec/iter\n",
      "Epoch: 07 | Batch: 015 / 025 | Total loss: 8.465 | Reg loss: 0.016 | Tree loss: 8.465 | Accuracy: 0.085938 | 0.069 sec/iter\n",
      "Epoch: 07 | Batch: 016 / 025 | Total loss: 8.428 | Reg loss: 0.016 | Tree loss: 8.428 | Accuracy: 0.099609 | 0.069 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 07 | Batch: 017 / 025 | Total loss: 8.425 | Reg loss: 0.017 | Tree loss: 8.425 | Accuracy: 0.101562 | 0.069 sec/iter\n",
      "Epoch: 07 | Batch: 018 / 025 | Total loss: 8.410 | Reg loss: 0.017 | Tree loss: 8.410 | Accuracy: 0.119141 | 0.069 sec/iter\n",
      "Epoch: 07 | Batch: 019 / 025 | Total loss: 8.395 | Reg loss: 0.017 | Tree loss: 8.395 | Accuracy: 0.115234 | 0.069 sec/iter\n",
      "Epoch: 07 | Batch: 020 / 025 | Total loss: 8.386 | Reg loss: 0.018 | Tree loss: 8.386 | Accuracy: 0.093750 | 0.069 sec/iter\n",
      "Epoch: 07 | Batch: 021 / 025 | Total loss: 8.383 | Reg loss: 0.018 | Tree loss: 8.383 | Accuracy: 0.076172 | 0.069 sec/iter\n",
      "Epoch: 07 | Batch: 022 / 025 | Total loss: 8.365 | Reg loss: 0.018 | Tree loss: 8.365 | Accuracy: 0.099609 | 0.069 sec/iter\n",
      "Epoch: 07 | Batch: 023 / 025 | Total loss: 8.344 | Reg loss: 0.019 | Tree loss: 8.344 | Accuracy: 0.107422 | 0.069 sec/iter\n",
      "Epoch: 07 | Batch: 024 / 025 | Total loss: 8.332 | Reg loss: 0.019 | Tree loss: 8.332 | Accuracy: 0.103125 | 0.069 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 08 | Batch: 000 / 025 | Total loss: 8.485 | Reg loss: 0.015 | Tree loss: 8.485 | Accuracy: 0.125000 | 0.069 sec/iter\n",
      "Epoch: 08 | Batch: 001 / 025 | Total loss: 8.488 | Reg loss: 0.015 | Tree loss: 8.488 | Accuracy: 0.099609 | 0.069 sec/iter\n",
      "Epoch: 08 | Batch: 002 / 025 | Total loss: 8.470 | Reg loss: 0.015 | Tree loss: 8.470 | Accuracy: 0.105469 | 0.069 sec/iter\n",
      "Epoch: 08 | Batch: 003 / 025 | Total loss: 8.468 | Reg loss: 0.015 | Tree loss: 8.468 | Accuracy: 0.089844 | 0.069 sec/iter\n",
      "Epoch: 08 | Batch: 004 / 025 | Total loss: 8.430 | Reg loss: 0.015 | Tree loss: 8.430 | Accuracy: 0.099609 | 0.069 sec/iter\n",
      "Epoch: 08 | Batch: 005 / 025 | Total loss: 8.431 | Reg loss: 0.016 | Tree loss: 8.431 | Accuracy: 0.101562 | 0.069 sec/iter\n",
      "Epoch: 08 | Batch: 006 / 025 | Total loss: 8.417 | Reg loss: 0.016 | Tree loss: 8.417 | Accuracy: 0.085938 | 0.069 sec/iter\n",
      "Epoch: 08 | Batch: 007 / 025 | Total loss: 8.376 | Reg loss: 0.016 | Tree loss: 8.376 | Accuracy: 0.121094 | 0.069 sec/iter\n",
      "Epoch: 08 | Batch: 008 / 025 | Total loss: 8.367 | Reg loss: 0.016 | Tree loss: 8.367 | Accuracy: 0.134766 | 0.069 sec/iter\n",
      "Epoch: 08 | Batch: 009 / 025 | Total loss: 8.358 | Reg loss: 0.016 | Tree loss: 8.358 | Accuracy: 0.103516 | 0.069 sec/iter\n",
      "Epoch: 08 | Batch: 010 / 025 | Total loss: 8.328 | Reg loss: 0.017 | Tree loss: 8.328 | Accuracy: 0.119141 | 0.069 sec/iter\n",
      "Epoch: 08 | Batch: 011 / 025 | Total loss: 8.324 | Reg loss: 0.017 | Tree loss: 8.324 | Accuracy: 0.107422 | 0.069 sec/iter\n",
      "Epoch: 08 | Batch: 012 / 025 | Total loss: 8.313 | Reg loss: 0.017 | Tree loss: 8.313 | Accuracy: 0.115234 | 0.069 sec/iter\n",
      "Epoch: 08 | Batch: 013 / 025 | Total loss: 8.302 | Reg loss: 0.018 | Tree loss: 8.302 | Accuracy: 0.093750 | 0.069 sec/iter\n",
      "Epoch: 08 | Batch: 014 / 025 | Total loss: 8.267 | Reg loss: 0.018 | Tree loss: 8.267 | Accuracy: 0.121094 | 0.069 sec/iter\n",
      "Epoch: 08 | Batch: 015 / 025 | Total loss: 8.266 | Reg loss: 0.018 | Tree loss: 8.266 | Accuracy: 0.107422 | 0.069 sec/iter\n",
      "Epoch: 08 | Batch: 016 / 025 | Total loss: 8.242 | Reg loss: 0.019 | Tree loss: 8.242 | Accuracy: 0.109375 | 0.069 sec/iter\n",
      "Epoch: 08 | Batch: 017 / 025 | Total loss: 8.242 | Reg loss: 0.019 | Tree loss: 8.242 | Accuracy: 0.091797 | 0.069 sec/iter\n",
      "Epoch: 08 | Batch: 018 / 025 | Total loss: 8.209 | Reg loss: 0.019 | Tree loss: 8.209 | Accuracy: 0.095703 | 0.069 sec/iter\n",
      "Epoch: 08 | Batch: 019 / 025 | Total loss: 8.211 | Reg loss: 0.020 | Tree loss: 8.211 | Accuracy: 0.091797 | 0.069 sec/iter\n",
      "Epoch: 08 | Batch: 020 / 025 | Total loss: 8.180 | Reg loss: 0.020 | Tree loss: 8.180 | Accuracy: 0.097656 | 0.069 sec/iter\n",
      "Epoch: 08 | Batch: 021 / 025 | Total loss: 8.157 | Reg loss: 0.020 | Tree loss: 8.157 | Accuracy: 0.119141 | 0.069 sec/iter\n",
      "Epoch: 08 | Batch: 022 / 025 | Total loss: 8.153 | Reg loss: 0.021 | Tree loss: 8.153 | Accuracy: 0.095703 | 0.069 sec/iter\n",
      "Epoch: 08 | Batch: 023 / 025 | Total loss: 8.132 | Reg loss: 0.021 | Tree loss: 8.132 | Accuracy: 0.091797 | 0.069 sec/iter\n",
      "Epoch: 08 | Batch: 024 / 025 | Total loss: 8.125 | Reg loss: 0.021 | Tree loss: 8.125 | Accuracy: 0.100000 | 0.069 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 09 | Batch: 000 / 025 | Total loss: 8.335 | Reg loss: 0.017 | Tree loss: 8.335 | Accuracy: 0.097656 | 0.069 sec/iter\n",
      "Epoch: 09 | Batch: 001 / 025 | Total loss: 8.300 | Reg loss: 0.017 | Tree loss: 8.300 | Accuracy: 0.119141 | 0.069 sec/iter\n",
      "Epoch: 09 | Batch: 002 / 025 | Total loss: 8.298 | Reg loss: 0.017 | Tree loss: 8.298 | Accuracy: 0.103516 | 0.069 sec/iter\n",
      "Epoch: 09 | Batch: 003 / 025 | Total loss: 8.274 | Reg loss: 0.017 | Tree loss: 8.274 | Accuracy: 0.117188 | 0.069 sec/iter\n",
      "Epoch: 09 | Batch: 004 / 025 | Total loss: 8.241 | Reg loss: 0.017 | Tree loss: 8.241 | Accuracy: 0.095703 | 0.069 sec/iter\n",
      "Epoch: 09 | Batch: 005 / 025 | Total loss: 8.244 | Reg loss: 0.017 | Tree loss: 8.244 | Accuracy: 0.107422 | 0.069 sec/iter\n",
      "Epoch: 09 | Batch: 006 / 025 | Total loss: 8.229 | Reg loss: 0.018 | Tree loss: 8.229 | Accuracy: 0.074219 | 0.069 sec/iter\n",
      "Epoch: 09 | Batch: 007 / 025 | Total loss: 8.214 | Reg loss: 0.018 | Tree loss: 8.214 | Accuracy: 0.105469 | 0.069 sec/iter\n",
      "Epoch: 09 | Batch: 008 / 025 | Total loss: 8.192 | Reg loss: 0.018 | Tree loss: 8.192 | Accuracy: 0.130859 | 0.069 sec/iter\n",
      "Epoch: 09 | Batch: 009 / 025 | Total loss: 8.167 | Reg loss: 0.018 | Tree loss: 8.167 | Accuracy: 0.087891 | 0.069 sec/iter\n",
      "Epoch: 09 | Batch: 010 / 025 | Total loss: 8.135 | Reg loss: 0.019 | Tree loss: 8.135 | Accuracy: 0.119141 | 0.069 sec/iter\n",
      "Epoch: 09 | Batch: 011 / 025 | Total loss: 8.124 | Reg loss: 0.019 | Tree loss: 8.124 | Accuracy: 0.105469 | 0.069 sec/iter\n",
      "Epoch: 09 | Batch: 012 / 025 | Total loss: 8.130 | Reg loss: 0.019 | Tree loss: 8.130 | Accuracy: 0.089844 | 0.069 sec/iter\n",
      "Epoch: 09 | Batch: 013 / 025 | Total loss: 8.091 | Reg loss: 0.019 | Tree loss: 8.091 | Accuracy: 0.113281 | 0.068 sec/iter\n",
      "Epoch: 09 | Batch: 014 / 025 | Total loss: 8.076 | Reg loss: 0.020 | Tree loss: 8.076 | Accuracy: 0.109375 | 0.068 sec/iter\n",
      "Epoch: 09 | Batch: 015 / 025 | Total loss: 8.053 | Reg loss: 0.020 | Tree loss: 8.053 | Accuracy: 0.101562 | 0.068 sec/iter\n",
      "Epoch: 09 | Batch: 016 / 025 | Total loss: 8.064 | Reg loss: 0.020 | Tree loss: 8.064 | Accuracy: 0.111328 | 0.068 sec/iter\n",
      "Epoch: 09 | Batch: 017 / 025 | Total loss: 8.029 | Reg loss: 0.021 | Tree loss: 8.029 | Accuracy: 0.107422 | 0.068 sec/iter\n",
      "Epoch: 09 | Batch: 018 / 025 | Total loss: 7.990 | Reg loss: 0.021 | Tree loss: 7.990 | Accuracy: 0.105469 | 0.068 sec/iter\n",
      "Epoch: 09 | Batch: 019 / 025 | Total loss: 7.988 | Reg loss: 0.021 | Tree loss: 7.988 | Accuracy: 0.115234 | 0.068 sec/iter\n",
      "Epoch: 09 | Batch: 020 / 025 | Total loss: 7.961 | Reg loss: 0.022 | Tree loss: 7.961 | Accuracy: 0.126953 | 0.068 sec/iter\n",
      "Epoch: 09 | Batch: 021 / 025 | Total loss: 7.940 | Reg loss: 0.022 | Tree loss: 7.940 | Accuracy: 0.125000 | 0.068 sec/iter\n",
      "Epoch: 09 | Batch: 022 / 025 | Total loss: 7.940 | Reg loss: 0.022 | Tree loss: 7.940 | Accuracy: 0.093750 | 0.068 sec/iter\n",
      "Epoch: 09 | Batch: 023 / 025 | Total loss: 7.933 | Reg loss: 0.023 | Tree loss: 7.933 | Accuracy: 0.119141 | 0.068 sec/iter\n",
      "Epoch: 09 | Batch: 024 / 025 | Total loss: 7.925 | Reg loss: 0.023 | Tree loss: 7.925 | Accuracy: 0.071875 | 0.068 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 10 | Batch: 000 / 025 | Total loss: 8.146 | Reg loss: 0.019 | Tree loss: 8.146 | Accuracy: 0.125000 | 0.068 sec/iter\n",
      "Epoch: 10 | Batch: 001 / 025 | Total loss: 8.122 | Reg loss: 0.019 | Tree loss: 8.122 | Accuracy: 0.128906 | 0.068 sec/iter\n",
      "Epoch: 10 | Batch: 002 / 025 | Total loss: 8.124 | Reg loss: 0.019 | Tree loss: 8.124 | Accuracy: 0.101562 | 0.068 sec/iter\n",
      "Epoch: 10 | Batch: 003 / 025 | Total loss: 8.095 | Reg loss: 0.019 | Tree loss: 8.095 | Accuracy: 0.121094 | 0.068 sec/iter\n",
      "Epoch: 10 | Batch: 004 / 025 | Total loss: 8.058 | Reg loss: 0.019 | Tree loss: 8.058 | Accuracy: 0.128906 | 0.068 sec/iter\n",
      "Epoch: 10 | Batch: 005 / 025 | Total loss: 8.051 | Reg loss: 0.019 | Tree loss: 8.051 | Accuracy: 0.105469 | 0.068 sec/iter\n",
      "Epoch: 10 | Batch: 006 / 025 | Total loss: 8.042 | Reg loss: 0.019 | Tree loss: 8.042 | Accuracy: 0.095703 | 0.068 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 | Batch: 007 / 025 | Total loss: 8.009 | Reg loss: 0.020 | Tree loss: 8.009 | Accuracy: 0.121094 | 0.068 sec/iter\n",
      "Epoch: 10 | Batch: 008 / 025 | Total loss: 8.001 | Reg loss: 0.020 | Tree loss: 8.001 | Accuracy: 0.109375 | 0.068 sec/iter\n",
      "Epoch: 10 | Batch: 009 / 025 | Total loss: 7.980 | Reg loss: 0.020 | Tree loss: 7.980 | Accuracy: 0.123047 | 0.068 sec/iter\n",
      "Epoch: 10 | Batch: 010 / 025 | Total loss: 7.955 | Reg loss: 0.020 | Tree loss: 7.955 | Accuracy: 0.113281 | 0.068 sec/iter\n",
      "Epoch: 10 | Batch: 011 / 025 | Total loss: 7.922 | Reg loss: 0.021 | Tree loss: 7.922 | Accuracy: 0.101562 | 0.068 sec/iter\n",
      "Epoch: 10 | Batch: 012 / 025 | Total loss: 7.910 | Reg loss: 0.021 | Tree loss: 7.910 | Accuracy: 0.121094 | 0.068 sec/iter\n",
      "Epoch: 10 | Batch: 013 / 025 | Total loss: 7.901 | Reg loss: 0.021 | Tree loss: 7.901 | Accuracy: 0.099609 | 0.068 sec/iter\n",
      "Epoch: 10 | Batch: 014 / 025 | Total loss: 7.861 | Reg loss: 0.021 | Tree loss: 7.861 | Accuracy: 0.101562 | 0.068 sec/iter\n",
      "Epoch: 10 | Batch: 015 / 025 | Total loss: 7.870 | Reg loss: 0.022 | Tree loss: 7.870 | Accuracy: 0.087891 | 0.068 sec/iter\n",
      "Epoch: 10 | Batch: 016 / 025 | Total loss: 7.837 | Reg loss: 0.022 | Tree loss: 7.837 | Accuracy: 0.130859 | 0.068 sec/iter\n",
      "Epoch: 10 | Batch: 017 / 025 | Total loss: 7.826 | Reg loss: 0.022 | Tree loss: 7.826 | Accuracy: 0.099609 | 0.069 sec/iter\n",
      "Epoch: 10 | Batch: 018 / 025 | Total loss: 7.801 | Reg loss: 0.023 | Tree loss: 7.801 | Accuracy: 0.087891 | 0.069 sec/iter\n",
      "Epoch: 10 | Batch: 019 / 025 | Total loss: 7.799 | Reg loss: 0.023 | Tree loss: 7.799 | Accuracy: 0.105469 | 0.068 sec/iter\n",
      "Epoch: 10 | Batch: 020 / 025 | Total loss: 7.740 | Reg loss: 0.023 | Tree loss: 7.740 | Accuracy: 0.107422 | 0.068 sec/iter\n",
      "Epoch: 10 | Batch: 021 / 025 | Total loss: 7.742 | Reg loss: 0.024 | Tree loss: 7.742 | Accuracy: 0.103516 | 0.068 sec/iter\n",
      "Epoch: 10 | Batch: 022 / 025 | Total loss: 7.707 | Reg loss: 0.024 | Tree loss: 7.707 | Accuracy: 0.109375 | 0.068 sec/iter\n",
      "Epoch: 10 | Batch: 023 / 025 | Total loss: 7.701 | Reg loss: 0.024 | Tree loss: 7.701 | Accuracy: 0.107422 | 0.068 sec/iter\n",
      "Epoch: 10 | Batch: 024 / 025 | Total loss: 7.692 | Reg loss: 0.025 | Tree loss: 7.692 | Accuracy: 0.056250 | 0.068 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 11 | Batch: 000 / 025 | Total loss: 7.966 | Reg loss: 0.020 | Tree loss: 7.966 | Accuracy: 0.099609 | 0.068 sec/iter\n",
      "Epoch: 11 | Batch: 001 / 025 | Total loss: 7.949 | Reg loss: 0.020 | Tree loss: 7.949 | Accuracy: 0.080078 | 0.068 sec/iter\n",
      "Epoch: 11 | Batch: 002 / 025 | Total loss: 7.910 | Reg loss: 0.021 | Tree loss: 7.910 | Accuracy: 0.111328 | 0.068 sec/iter\n",
      "Epoch: 11 | Batch: 003 / 025 | Total loss: 7.900 | Reg loss: 0.021 | Tree loss: 7.900 | Accuracy: 0.117188 | 0.068 sec/iter\n",
      "Epoch: 11 | Batch: 004 / 025 | Total loss: 7.894 | Reg loss: 0.021 | Tree loss: 7.894 | Accuracy: 0.111328 | 0.068 sec/iter\n",
      "Epoch: 11 | Batch: 005 / 025 | Total loss: 7.845 | Reg loss: 0.021 | Tree loss: 7.845 | Accuracy: 0.144531 | 0.068 sec/iter\n",
      "Epoch: 11 | Batch: 006 / 025 | Total loss: 7.837 | Reg loss: 0.021 | Tree loss: 7.837 | Accuracy: 0.103516 | 0.068 sec/iter\n",
      "Epoch: 11 | Batch: 007 / 025 | Total loss: 7.804 | Reg loss: 0.021 | Tree loss: 7.804 | Accuracy: 0.146484 | 0.068 sec/iter\n",
      "Epoch: 11 | Batch: 008 / 025 | Total loss: 7.788 | Reg loss: 0.021 | Tree loss: 7.788 | Accuracy: 0.132812 | 0.068 sec/iter\n",
      "Epoch: 11 | Batch: 009 / 025 | Total loss: 7.793 | Reg loss: 0.022 | Tree loss: 7.793 | Accuracy: 0.121094 | 0.068 sec/iter\n",
      "Epoch: 11 | Batch: 010 / 025 | Total loss: 7.770 | Reg loss: 0.022 | Tree loss: 7.770 | Accuracy: 0.101562 | 0.068 sec/iter\n",
      "Epoch: 11 | Batch: 011 / 025 | Total loss: 7.733 | Reg loss: 0.022 | Tree loss: 7.733 | Accuracy: 0.107422 | 0.068 sec/iter\n",
      "Epoch: 11 | Batch: 012 / 025 | Total loss: 7.724 | Reg loss: 0.022 | Tree loss: 7.724 | Accuracy: 0.091797 | 0.068 sec/iter\n",
      "Epoch: 11 | Batch: 013 / 025 | Total loss: 7.678 | Reg loss: 0.023 | Tree loss: 7.678 | Accuracy: 0.103516 | 0.068 sec/iter\n",
      "Epoch: 11 | Batch: 014 / 025 | Total loss: 7.694 | Reg loss: 0.023 | Tree loss: 7.694 | Accuracy: 0.083984 | 0.068 sec/iter\n",
      "Epoch: 11 | Batch: 015 / 025 | Total loss: 7.630 | Reg loss: 0.023 | Tree loss: 7.630 | Accuracy: 0.132812 | 0.068 sec/iter\n",
      "Epoch: 11 | Batch: 016 / 025 | Total loss: 7.611 | Reg loss: 0.023 | Tree loss: 7.611 | Accuracy: 0.111328 | 0.068 sec/iter\n",
      "Epoch: 11 | Batch: 017 / 025 | Total loss: 7.615 | Reg loss: 0.024 | Tree loss: 7.615 | Accuracy: 0.095703 | 0.068 sec/iter\n",
      "Epoch: 11 | Batch: 018 / 025 | Total loss: 7.629 | Reg loss: 0.024 | Tree loss: 7.629 | Accuracy: 0.083984 | 0.068 sec/iter\n",
      "Epoch: 11 | Batch: 019 / 025 | Total loss: 7.573 | Reg loss: 0.024 | Tree loss: 7.573 | Accuracy: 0.121094 | 0.068 sec/iter\n",
      "Epoch: 11 | Batch: 020 / 025 | Total loss: 7.553 | Reg loss: 0.025 | Tree loss: 7.553 | Accuracy: 0.099609 | 0.068 sec/iter\n",
      "Epoch: 11 | Batch: 021 / 025 | Total loss: 7.548 | Reg loss: 0.025 | Tree loss: 7.548 | Accuracy: 0.091797 | 0.068 sec/iter\n",
      "Epoch: 11 | Batch: 022 / 025 | Total loss: 7.502 | Reg loss: 0.025 | Tree loss: 7.502 | Accuracy: 0.083984 | 0.068 sec/iter\n",
      "Epoch: 11 | Batch: 023 / 025 | Total loss: 7.488 | Reg loss: 0.026 | Tree loss: 7.488 | Accuracy: 0.117188 | 0.068 sec/iter\n",
      "Epoch: 11 | Batch: 024 / 025 | Total loss: 7.475 | Reg loss: 0.026 | Tree loss: 7.475 | Accuracy: 0.100000 | 0.068 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 12 | Batch: 000 / 025 | Total loss: 7.790 | Reg loss: 0.022 | Tree loss: 7.790 | Accuracy: 0.109375 | 0.068 sec/iter\n",
      "Epoch: 12 | Batch: 001 / 025 | Total loss: 7.733 | Reg loss: 0.022 | Tree loss: 7.733 | Accuracy: 0.117188 | 0.068 sec/iter\n",
      "Epoch: 12 | Batch: 002 / 025 | Total loss: 7.739 | Reg loss: 0.022 | Tree loss: 7.739 | Accuracy: 0.123047 | 0.068 sec/iter\n",
      "Epoch: 12 | Batch: 003 / 025 | Total loss: 7.726 | Reg loss: 0.022 | Tree loss: 7.726 | Accuracy: 0.121094 | 0.068 sec/iter\n",
      "Epoch: 12 | Batch: 004 / 025 | Total loss: 7.682 | Reg loss: 0.022 | Tree loss: 7.682 | Accuracy: 0.113281 | 0.068 sec/iter\n",
      "Epoch: 12 | Batch: 005 / 025 | Total loss: 7.660 | Reg loss: 0.022 | Tree loss: 7.660 | Accuracy: 0.103516 | 0.068 sec/iter\n",
      "Epoch: 12 | Batch: 006 / 025 | Total loss: 7.644 | Reg loss: 0.023 | Tree loss: 7.644 | Accuracy: 0.128906 | 0.068 sec/iter\n",
      "Epoch: 12 | Batch: 007 / 025 | Total loss: 7.635 | Reg loss: 0.023 | Tree loss: 7.635 | Accuracy: 0.103516 | 0.068 sec/iter\n",
      "Epoch: 12 | Batch: 008 / 025 | Total loss: 7.580 | Reg loss: 0.023 | Tree loss: 7.580 | Accuracy: 0.115234 | 0.068 sec/iter\n",
      "Epoch: 12 | Batch: 009 / 025 | Total loss: 7.584 | Reg loss: 0.023 | Tree loss: 7.584 | Accuracy: 0.132812 | 0.068 sec/iter\n",
      "Epoch: 12 | Batch: 010 / 025 | Total loss: 7.563 | Reg loss: 0.023 | Tree loss: 7.563 | Accuracy: 0.107422 | 0.068 sec/iter\n",
      "Epoch: 12 | Batch: 011 / 025 | Total loss: 7.538 | Reg loss: 0.023 | Tree loss: 7.538 | Accuracy: 0.109375 | 0.068 sec/iter\n",
      "Epoch: 12 | Batch: 012 / 025 | Total loss: 7.484 | Reg loss: 0.024 | Tree loss: 7.484 | Accuracy: 0.136719 | 0.068 sec/iter\n",
      "Epoch: 12 | Batch: 013 / 025 | Total loss: 7.488 | Reg loss: 0.024 | Tree loss: 7.488 | Accuracy: 0.103516 | 0.068 sec/iter\n",
      "Epoch: 12 | Batch: 014 / 025 | Total loss: 7.458 | Reg loss: 0.024 | Tree loss: 7.458 | Accuracy: 0.093750 | 0.068 sec/iter\n",
      "Epoch: 12 | Batch: 015 / 025 | Total loss: 7.462 | Reg loss: 0.024 | Tree loss: 7.462 | Accuracy: 0.099609 | 0.068 sec/iter\n",
      "Epoch: 12 | Batch: 016 / 025 | Total loss: 7.446 | Reg loss: 0.025 | Tree loss: 7.446 | Accuracy: 0.089844 | 0.068 sec/iter\n",
      "Epoch: 12 | Batch: 017 / 025 | Total loss: 7.414 | Reg loss: 0.025 | Tree loss: 7.414 | Accuracy: 0.095703 | 0.068 sec/iter\n",
      "Epoch: 12 | Batch: 018 / 025 | Total loss: 7.396 | Reg loss: 0.025 | Tree loss: 7.396 | Accuracy: 0.117188 | 0.068 sec/iter\n",
      "Epoch: 12 | Batch: 019 / 025 | Total loss: 7.394 | Reg loss: 0.026 | Tree loss: 7.394 | Accuracy: 0.105469 | 0.068 sec/iter\n",
      "Epoch: 12 | Batch: 020 / 025 | Total loss: 7.369 | Reg loss: 0.026 | Tree loss: 7.369 | Accuracy: 0.099609 | 0.069 sec/iter\n",
      "Epoch: 12 | Batch: 021 / 025 | Total loss: 7.337 | Reg loss: 0.026 | Tree loss: 7.337 | Accuracy: 0.119141 | 0.069 sec/iter\n",
      "Epoch: 12 | Batch: 022 / 025 | Total loss: 7.313 | Reg loss: 0.026 | Tree loss: 7.313 | Accuracy: 0.097656 | 0.069 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12 | Batch: 023 / 025 | Total loss: 7.269 | Reg loss: 0.027 | Tree loss: 7.269 | Accuracy: 0.113281 | 0.069 sec/iter\n",
      "Epoch: 12 | Batch: 024 / 025 | Total loss: 7.244 | Reg loss: 0.027 | Tree loss: 7.244 | Accuracy: 0.140625 | 0.068 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 13 | Batch: 000 / 025 | Total loss: 7.575 | Reg loss: 0.023 | Tree loss: 7.575 | Accuracy: 0.136719 | 0.069 sec/iter\n",
      "Epoch: 13 | Batch: 001 / 025 | Total loss: 7.567 | Reg loss: 0.023 | Tree loss: 7.567 | Accuracy: 0.138672 | 0.069 sec/iter\n",
      "Epoch: 13 | Batch: 002 / 025 | Total loss: 7.520 | Reg loss: 0.023 | Tree loss: 7.520 | Accuracy: 0.126953 | 0.069 sec/iter\n",
      "Epoch: 13 | Batch: 003 / 025 | Total loss: 7.503 | Reg loss: 0.024 | Tree loss: 7.503 | Accuracy: 0.123047 | 0.069 sec/iter\n",
      "Epoch: 13 | Batch: 004 / 025 | Total loss: 7.511 | Reg loss: 0.024 | Tree loss: 7.511 | Accuracy: 0.097656 | 0.069 sec/iter\n",
      "Epoch: 13 | Batch: 005 / 025 | Total loss: 7.467 | Reg loss: 0.024 | Tree loss: 7.467 | Accuracy: 0.126953 | 0.069 sec/iter\n",
      "Epoch: 13 | Batch: 006 / 025 | Total loss: 7.451 | Reg loss: 0.024 | Tree loss: 7.451 | Accuracy: 0.109375 | 0.069 sec/iter\n",
      "Epoch: 13 | Batch: 007 / 025 | Total loss: 7.406 | Reg loss: 0.024 | Tree loss: 7.406 | Accuracy: 0.117188 | 0.069 sec/iter\n",
      "Epoch: 13 | Batch: 008 / 025 | Total loss: 7.412 | Reg loss: 0.024 | Tree loss: 7.412 | Accuracy: 0.119141 | 0.069 sec/iter\n",
      "Epoch: 13 | Batch: 009 / 025 | Total loss: 7.395 | Reg loss: 0.024 | Tree loss: 7.395 | Accuracy: 0.095703 | 0.069 sec/iter\n",
      "Epoch: 13 | Batch: 010 / 025 | Total loss: 7.358 | Reg loss: 0.025 | Tree loss: 7.358 | Accuracy: 0.126953 | 0.069 sec/iter\n",
      "Epoch: 13 | Batch: 011 / 025 | Total loss: 7.349 | Reg loss: 0.025 | Tree loss: 7.349 | Accuracy: 0.113281 | 0.069 sec/iter\n",
      "Epoch: 13 | Batch: 012 / 025 | Total loss: 7.316 | Reg loss: 0.025 | Tree loss: 7.316 | Accuracy: 0.111328 | 0.069 sec/iter\n",
      "Epoch: 13 | Batch: 013 / 025 | Total loss: 7.341 | Reg loss: 0.025 | Tree loss: 7.341 | Accuracy: 0.089844 | 0.069 sec/iter\n",
      "Epoch: 13 | Batch: 014 / 025 | Total loss: 7.245 | Reg loss: 0.025 | Tree loss: 7.245 | Accuracy: 0.125000 | 0.069 sec/iter\n",
      "Epoch: 13 | Batch: 015 / 025 | Total loss: 7.257 | Reg loss: 0.026 | Tree loss: 7.257 | Accuracy: 0.115234 | 0.069 sec/iter\n",
      "Epoch: 13 | Batch: 016 / 025 | Total loss: 7.218 | Reg loss: 0.026 | Tree loss: 7.218 | Accuracy: 0.121094 | 0.069 sec/iter\n",
      "Epoch: 13 | Batch: 017 / 025 | Total loss: 7.214 | Reg loss: 0.026 | Tree loss: 7.214 | Accuracy: 0.121094 | 0.069 sec/iter\n",
      "Epoch: 13 | Batch: 018 / 025 | Total loss: 7.219 | Reg loss: 0.026 | Tree loss: 7.219 | Accuracy: 0.091797 | 0.069 sec/iter\n",
      "Epoch: 13 | Batch: 019 / 025 | Total loss: 7.178 | Reg loss: 0.027 | Tree loss: 7.178 | Accuracy: 0.111328 | 0.069 sec/iter\n",
      "Epoch: 13 | Batch: 020 / 025 | Total loss: 7.161 | Reg loss: 0.027 | Tree loss: 7.161 | Accuracy: 0.130859 | 0.069 sec/iter\n",
      "Epoch: 13 | Batch: 021 / 025 | Total loss: 7.144 | Reg loss: 0.027 | Tree loss: 7.144 | Accuracy: 0.089844 | 0.069 sec/iter\n",
      "Epoch: 13 | Batch: 022 / 025 | Total loss: 7.118 | Reg loss: 0.027 | Tree loss: 7.118 | Accuracy: 0.115234 | 0.069 sec/iter\n",
      "Epoch: 13 | Batch: 023 / 025 | Total loss: 7.095 | Reg loss: 0.028 | Tree loss: 7.095 | Accuracy: 0.101562 | 0.069 sec/iter\n",
      "Epoch: 13 | Batch: 024 / 025 | Total loss: 7.049 | Reg loss: 0.028 | Tree loss: 7.049 | Accuracy: 0.112500 | 0.069 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 14 | Batch: 000 / 025 | Total loss: 7.383 | Reg loss: 0.025 | Tree loss: 7.383 | Accuracy: 0.103516 | 0.069 sec/iter\n",
      "Epoch: 14 | Batch: 001 / 025 | Total loss: 7.374 | Reg loss: 0.025 | Tree loss: 7.374 | Accuracy: 0.105469 | 0.069 sec/iter\n",
      "Epoch: 14 | Batch: 002 / 025 | Total loss: 7.328 | Reg loss: 0.025 | Tree loss: 7.328 | Accuracy: 0.148438 | 0.069 sec/iter\n",
      "Epoch: 14 | Batch: 003 / 025 | Total loss: 7.335 | Reg loss: 0.025 | Tree loss: 7.335 | Accuracy: 0.111328 | 0.069 sec/iter\n",
      "Epoch: 14 | Batch: 004 / 025 | Total loss: 7.304 | Reg loss: 0.025 | Tree loss: 7.304 | Accuracy: 0.134766 | 0.069 sec/iter\n",
      "Epoch: 14 | Batch: 005 / 025 | Total loss: 7.275 | Reg loss: 0.025 | Tree loss: 7.275 | Accuracy: 0.126953 | 0.069 sec/iter\n",
      "Epoch: 14 | Batch: 006 / 025 | Total loss: 7.245 | Reg loss: 0.025 | Tree loss: 7.245 | Accuracy: 0.128906 | 0.069 sec/iter\n",
      "Epoch: 14 | Batch: 007 / 025 | Total loss: 7.245 | Reg loss: 0.025 | Tree loss: 7.245 | Accuracy: 0.105469 | 0.069 sec/iter\n",
      "Epoch: 14 | Batch: 008 / 025 | Total loss: 7.209 | Reg loss: 0.025 | Tree loss: 7.209 | Accuracy: 0.101562 | 0.069 sec/iter\n",
      "Epoch: 14 | Batch: 009 / 025 | Total loss: 7.202 | Reg loss: 0.025 | Tree loss: 7.202 | Accuracy: 0.128906 | 0.069 sec/iter\n",
      "Epoch: 14 | Batch: 010 / 025 | Total loss: 7.160 | Reg loss: 0.026 | Tree loss: 7.160 | Accuracy: 0.111328 | 0.069 sec/iter\n",
      "Epoch: 14 | Batch: 011 / 025 | Total loss: 7.118 | Reg loss: 0.026 | Tree loss: 7.118 | Accuracy: 0.130859 | 0.069 sec/iter\n",
      "Epoch: 14 | Batch: 012 / 025 | Total loss: 7.139 | Reg loss: 0.026 | Tree loss: 7.139 | Accuracy: 0.111328 | 0.069 sec/iter\n",
      "Epoch: 14 | Batch: 013 / 025 | Total loss: 7.120 | Reg loss: 0.026 | Tree loss: 7.120 | Accuracy: 0.105469 | 0.069 sec/iter\n",
      "Epoch: 14 | Batch: 014 / 025 | Total loss: 7.081 | Reg loss: 0.026 | Tree loss: 7.081 | Accuracy: 0.144531 | 0.069 sec/iter\n",
      "Epoch: 14 | Batch: 015 / 025 | Total loss: 7.066 | Reg loss: 0.027 | Tree loss: 7.066 | Accuracy: 0.093750 | 0.069 sec/iter\n",
      "Epoch: 14 | Batch: 016 / 025 | Total loss: 7.087 | Reg loss: 0.027 | Tree loss: 7.087 | Accuracy: 0.080078 | 0.069 sec/iter\n",
      "Epoch: 14 | Batch: 017 / 025 | Total loss: 7.004 | Reg loss: 0.027 | Tree loss: 7.004 | Accuracy: 0.140625 | 0.069 sec/iter\n",
      "Epoch: 14 | Batch: 018 / 025 | Total loss: 7.007 | Reg loss: 0.027 | Tree loss: 7.007 | Accuracy: 0.113281 | 0.069 sec/iter\n",
      "Epoch: 14 | Batch: 019 / 025 | Total loss: 6.975 | Reg loss: 0.027 | Tree loss: 6.975 | Accuracy: 0.125000 | 0.069 sec/iter\n",
      "Epoch: 14 | Batch: 020 / 025 | Total loss: 6.971 | Reg loss: 0.028 | Tree loss: 6.971 | Accuracy: 0.097656 | 0.069 sec/iter\n",
      "Epoch: 14 | Batch: 021 / 025 | Total loss: 6.946 | Reg loss: 0.028 | Tree loss: 6.946 | Accuracy: 0.107422 | 0.069 sec/iter\n",
      "Epoch: 14 | Batch: 022 / 025 | Total loss: 6.928 | Reg loss: 0.028 | Tree loss: 6.928 | Accuracy: 0.107422 | 0.069 sec/iter\n",
      "Epoch: 14 | Batch: 023 / 025 | Total loss: 6.911 | Reg loss: 0.028 | Tree loss: 6.911 | Accuracy: 0.136719 | 0.069 sec/iter\n",
      "Epoch: 14 | Batch: 024 / 025 | Total loss: 6.862 | Reg loss: 0.029 | Tree loss: 6.862 | Accuracy: 0.121875 | 0.069 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 15 | Batch: 000 / 025 | Total loss: 7.212 | Reg loss: 0.026 | Tree loss: 7.212 | Accuracy: 0.099609 | 0.069 sec/iter\n",
      "Epoch: 15 | Batch: 001 / 025 | Total loss: 7.181 | Reg loss: 0.026 | Tree loss: 7.181 | Accuracy: 0.103516 | 0.069 sec/iter\n",
      "Epoch: 15 | Batch: 002 / 025 | Total loss: 7.175 | Reg loss: 0.026 | Tree loss: 7.175 | Accuracy: 0.126953 | 0.069 sec/iter\n",
      "Epoch: 15 | Batch: 003 / 025 | Total loss: 7.153 | Reg loss: 0.026 | Tree loss: 7.153 | Accuracy: 0.125000 | 0.069 sec/iter\n",
      "Epoch: 15 | Batch: 004 / 025 | Total loss: 7.133 | Reg loss: 0.026 | Tree loss: 7.133 | Accuracy: 0.117188 | 0.069 sec/iter\n",
      "Epoch: 15 | Batch: 005 / 025 | Total loss: 7.092 | Reg loss: 0.026 | Tree loss: 7.092 | Accuracy: 0.126953 | 0.069 sec/iter\n",
      "Epoch: 15 | Batch: 006 / 025 | Total loss: 7.058 | Reg loss: 0.026 | Tree loss: 7.058 | Accuracy: 0.132812 | 0.069 sec/iter\n",
      "Epoch: 15 | Batch: 007 / 025 | Total loss: 7.030 | Reg loss: 0.026 | Tree loss: 7.030 | Accuracy: 0.128906 | 0.069 sec/iter\n",
      "Epoch: 15 | Batch: 008 / 025 | Total loss: 7.011 | Reg loss: 0.026 | Tree loss: 7.011 | Accuracy: 0.109375 | 0.069 sec/iter\n",
      "Epoch: 15 | Batch: 009 / 025 | Total loss: 6.972 | Reg loss: 0.026 | Tree loss: 6.972 | Accuracy: 0.125000 | 0.069 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15 | Batch: 010 / 025 | Total loss: 6.974 | Reg loss: 0.027 | Tree loss: 6.974 | Accuracy: 0.113281 | 0.069 sec/iter\n",
      "Epoch: 15 | Batch: 011 / 025 | Total loss: 6.956 | Reg loss: 0.027 | Tree loss: 6.956 | Accuracy: 0.134766 | 0.069 sec/iter\n",
      "Epoch: 15 | Batch: 012 / 025 | Total loss: 6.920 | Reg loss: 0.027 | Tree loss: 6.920 | Accuracy: 0.115234 | 0.069 sec/iter\n",
      "Epoch: 15 | Batch: 013 / 025 | Total loss: 6.948 | Reg loss: 0.027 | Tree loss: 6.948 | Accuracy: 0.083984 | 0.069 sec/iter\n",
      "Epoch: 15 | Batch: 014 / 025 | Total loss: 6.914 | Reg loss: 0.027 | Tree loss: 6.914 | Accuracy: 0.132812 | 0.069 sec/iter\n",
      "Epoch: 15 | Batch: 015 / 025 | Total loss: 6.880 | Reg loss: 0.027 | Tree loss: 6.880 | Accuracy: 0.107422 | 0.069 sec/iter\n",
      "Epoch: 15 | Batch: 016 / 025 | Total loss: 6.859 | Reg loss: 0.028 | Tree loss: 6.859 | Accuracy: 0.109375 | 0.069 sec/iter\n",
      "Epoch: 15 | Batch: 017 / 025 | Total loss: 6.821 | Reg loss: 0.028 | Tree loss: 6.821 | Accuracy: 0.121094 | 0.069 sec/iter\n",
      "Epoch: 15 | Batch: 018 / 025 | Total loss: 6.806 | Reg loss: 0.028 | Tree loss: 6.806 | Accuracy: 0.117188 | 0.069 sec/iter\n",
      "Epoch: 15 | Batch: 019 / 025 | Total loss: 6.792 | Reg loss: 0.028 | Tree loss: 6.792 | Accuracy: 0.119141 | 0.069 sec/iter\n",
      "Epoch: 15 | Batch: 020 / 025 | Total loss: 6.770 | Reg loss: 0.028 | Tree loss: 6.770 | Accuracy: 0.119141 | 0.069 sec/iter\n",
      "Epoch: 15 | Batch: 021 / 025 | Total loss: 6.741 | Reg loss: 0.029 | Tree loss: 6.741 | Accuracy: 0.113281 | 0.069 sec/iter\n",
      "Epoch: 15 | Batch: 022 / 025 | Total loss: 6.739 | Reg loss: 0.029 | Tree loss: 6.739 | Accuracy: 0.125000 | 0.069 sec/iter\n",
      "Epoch: 15 | Batch: 023 / 025 | Total loss: 6.717 | Reg loss: 0.029 | Tree loss: 6.717 | Accuracy: 0.126953 | 0.069 sec/iter\n",
      "Epoch: 15 | Batch: 024 / 025 | Total loss: 6.734 | Reg loss: 0.029 | Tree loss: 6.734 | Accuracy: 0.100000 | 0.069 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 16 | Batch: 000 / 025 | Total loss: 7.030 | Reg loss: 0.027 | Tree loss: 7.030 | Accuracy: 0.125000 | 0.069 sec/iter\n",
      "Epoch: 16 | Batch: 001 / 025 | Total loss: 7.005 | Reg loss: 0.027 | Tree loss: 7.005 | Accuracy: 0.121094 | 0.069 sec/iter\n",
      "Epoch: 16 | Batch: 002 / 025 | Total loss: 7.000 | Reg loss: 0.027 | Tree loss: 7.000 | Accuracy: 0.117188 | 0.069 sec/iter\n",
      "Epoch: 16 | Batch: 003 / 025 | Total loss: 6.955 | Reg loss: 0.027 | Tree loss: 6.955 | Accuracy: 0.115234 | 0.069 sec/iter\n",
      "Epoch: 16 | Batch: 004 / 025 | Total loss: 6.920 | Reg loss: 0.027 | Tree loss: 6.920 | Accuracy: 0.107422 | 0.069 sec/iter\n",
      "Epoch: 16 | Batch: 005 / 025 | Total loss: 6.903 | Reg loss: 0.027 | Tree loss: 6.903 | Accuracy: 0.150391 | 0.069 sec/iter\n",
      "Epoch: 16 | Batch: 006 / 025 | Total loss: 6.886 | Reg loss: 0.027 | Tree loss: 6.886 | Accuracy: 0.123047 | 0.069 sec/iter\n",
      "Epoch: 16 | Batch: 007 / 025 | Total loss: 6.836 | Reg loss: 0.027 | Tree loss: 6.836 | Accuracy: 0.126953 | 0.069 sec/iter\n",
      "Epoch: 16 | Batch: 008 / 025 | Total loss: 6.854 | Reg loss: 0.027 | Tree loss: 6.854 | Accuracy: 0.113281 | 0.069 sec/iter\n",
      "Epoch: 16 | Batch: 009 / 025 | Total loss: 6.828 | Reg loss: 0.027 | Tree loss: 6.828 | Accuracy: 0.140625 | 0.069 sec/iter\n",
      "Epoch: 16 | Batch: 010 / 025 | Total loss: 6.780 | Reg loss: 0.027 | Tree loss: 6.780 | Accuracy: 0.134766 | 0.069 sec/iter\n",
      "Epoch: 16 | Batch: 011 / 025 | Total loss: 6.772 | Reg loss: 0.027 | Tree loss: 6.772 | Accuracy: 0.119141 | 0.069 sec/iter\n",
      "Epoch: 16 | Batch: 012 / 025 | Total loss: 6.736 | Reg loss: 0.028 | Tree loss: 6.736 | Accuracy: 0.140625 | 0.069 sec/iter\n",
      "Epoch: 16 | Batch: 013 / 025 | Total loss: 6.749 | Reg loss: 0.028 | Tree loss: 6.749 | Accuracy: 0.115234 | 0.069 sec/iter\n",
      "Epoch: 16 | Batch: 014 / 025 | Total loss: 6.706 | Reg loss: 0.028 | Tree loss: 6.706 | Accuracy: 0.142578 | 0.069 sec/iter\n",
      "Epoch: 16 | Batch: 015 / 025 | Total loss: 6.693 | Reg loss: 0.028 | Tree loss: 6.693 | Accuracy: 0.128906 | 0.069 sec/iter\n",
      "Epoch: 16 | Batch: 016 / 025 | Total loss: 6.670 | Reg loss: 0.028 | Tree loss: 6.670 | Accuracy: 0.119141 | 0.069 sec/iter\n",
      "Epoch: 16 | Batch: 017 / 025 | Total loss: 6.649 | Reg loss: 0.028 | Tree loss: 6.649 | Accuracy: 0.144531 | 0.069 sec/iter\n",
      "Epoch: 16 | Batch: 018 / 025 | Total loss: 6.648 | Reg loss: 0.029 | Tree loss: 6.648 | Accuracy: 0.107422 | 0.069 sec/iter\n",
      "Epoch: 16 | Batch: 019 / 025 | Total loss: 6.572 | Reg loss: 0.029 | Tree loss: 6.572 | Accuracy: 0.140625 | 0.069 sec/iter\n",
      "Epoch: 16 | Batch: 020 / 025 | Total loss: 6.558 | Reg loss: 0.029 | Tree loss: 6.558 | Accuracy: 0.099609 | 0.069 sec/iter\n",
      "Epoch: 16 | Batch: 021 / 025 | Total loss: 6.593 | Reg loss: 0.029 | Tree loss: 6.593 | Accuracy: 0.117188 | 0.069 sec/iter\n",
      "Epoch: 16 | Batch: 022 / 025 | Total loss: 6.587 | Reg loss: 0.029 | Tree loss: 6.587 | Accuracy: 0.097656 | 0.069 sec/iter\n",
      "Epoch: 16 | Batch: 023 / 025 | Total loss: 6.509 | Reg loss: 0.030 | Tree loss: 6.509 | Accuracy: 0.115234 | 0.069 sec/iter\n",
      "Epoch: 16 | Batch: 024 / 025 | Total loss: 6.535 | Reg loss: 0.030 | Tree loss: 6.535 | Accuracy: 0.084375 | 0.069 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 17 | Batch: 000 / 025 | Total loss: 6.844 | Reg loss: 0.027 | Tree loss: 6.844 | Accuracy: 0.136719 | 0.069 sec/iter\n",
      "Epoch: 17 | Batch: 001 / 025 | Total loss: 6.798 | Reg loss: 0.027 | Tree loss: 6.798 | Accuracy: 0.142578 | 0.069 sec/iter\n",
      "Epoch: 17 | Batch: 002 / 025 | Total loss: 6.770 | Reg loss: 0.027 | Tree loss: 6.770 | Accuracy: 0.125000 | 0.069 sec/iter\n",
      "Epoch: 17 | Batch: 003 / 025 | Total loss: 6.787 | Reg loss: 0.027 | Tree loss: 6.787 | Accuracy: 0.095703 | 0.069 sec/iter\n",
      "Epoch: 17 | Batch: 004 / 025 | Total loss: 6.756 | Reg loss: 0.027 | Tree loss: 6.756 | Accuracy: 0.130859 | 0.069 sec/iter\n",
      "Epoch: 17 | Batch: 005 / 025 | Total loss: 6.737 | Reg loss: 0.027 | Tree loss: 6.737 | Accuracy: 0.115234 | 0.069 sec/iter\n",
      "Epoch: 17 | Batch: 006 / 025 | Total loss: 6.654 | Reg loss: 0.028 | Tree loss: 6.654 | Accuracy: 0.152344 | 0.069 sec/iter\n",
      "Epoch: 17 | Batch: 007 / 025 | Total loss: 6.696 | Reg loss: 0.028 | Tree loss: 6.696 | Accuracy: 0.113281 | 0.069 sec/iter\n",
      "Epoch: 17 | Batch: 008 / 025 | Total loss: 6.666 | Reg loss: 0.028 | Tree loss: 6.666 | Accuracy: 0.128906 | 0.069 sec/iter\n",
      "Epoch: 17 | Batch: 009 / 025 | Total loss: 6.644 | Reg loss: 0.028 | Tree loss: 6.644 | Accuracy: 0.128906 | 0.069 sec/iter\n",
      "Epoch: 17 | Batch: 010 / 025 | Total loss: 6.608 | Reg loss: 0.028 | Tree loss: 6.608 | Accuracy: 0.126953 | 0.069 sec/iter\n",
      "Epoch: 17 | Batch: 011 / 025 | Total loss: 6.601 | Reg loss: 0.028 | Tree loss: 6.601 | Accuracy: 0.121094 | 0.069 sec/iter\n",
      "Epoch: 17 | Batch: 012 / 025 | Total loss: 6.548 | Reg loss: 0.028 | Tree loss: 6.548 | Accuracy: 0.142578 | 0.069 sec/iter\n",
      "Epoch: 17 | Batch: 013 / 025 | Total loss: 6.521 | Reg loss: 0.028 | Tree loss: 6.521 | Accuracy: 0.142578 | 0.069 sec/iter\n",
      "Epoch: 17 | Batch: 014 / 025 | Total loss: 6.543 | Reg loss: 0.028 | Tree loss: 6.543 | Accuracy: 0.138672 | 0.069 sec/iter\n",
      "Epoch: 17 | Batch: 015 / 025 | Total loss: 6.446 | Reg loss: 0.029 | Tree loss: 6.446 | Accuracy: 0.146484 | 0.069 sec/iter\n",
      "Epoch: 17 | Batch: 016 / 025 | Total loss: 6.494 | Reg loss: 0.029 | Tree loss: 6.494 | Accuracy: 0.140625 | 0.069 sec/iter\n",
      "Epoch: 17 | Batch: 017 / 025 | Total loss: 6.483 | Reg loss: 0.029 | Tree loss: 6.483 | Accuracy: 0.101562 | 0.069 sec/iter\n",
      "Epoch: 17 | Batch: 018 / 025 | Total loss: 6.437 | Reg loss: 0.029 | Tree loss: 6.437 | Accuracy: 0.126953 | 0.069 sec/iter\n",
      "Epoch: 17 | Batch: 019 / 025 | Total loss: 6.459 | Reg loss: 0.029 | Tree loss: 6.459 | Accuracy: 0.123047 | 0.069 sec/iter\n",
      "Epoch: 17 | Batch: 020 / 025 | Total loss: 6.428 | Reg loss: 0.029 | Tree loss: 6.428 | Accuracy: 0.101562 | 0.069 sec/iter\n",
      "Epoch: 17 | Batch: 021 / 025 | Total loss: 6.428 | Reg loss: 0.030 | Tree loss: 6.428 | Accuracy: 0.099609 | 0.069 sec/iter\n",
      "Epoch: 17 | Batch: 022 / 025 | Total loss: 6.390 | Reg loss: 0.030 | Tree loss: 6.390 | Accuracy: 0.107422 | 0.069 sec/iter\n",
      "Epoch: 17 | Batch: 023 / 025 | Total loss: 6.355 | Reg loss: 0.030 | Tree loss: 6.355 | Accuracy: 0.103516 | 0.069 sec/iter\n",
      "Epoch: 17 | Batch: 024 / 025 | Total loss: 6.354 | Reg loss: 0.030 | Tree loss: 6.354 | Accuracy: 0.121875 | 0.069 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18 | Batch: 000 / 025 | Total loss: 6.668 | Reg loss: 0.028 | Tree loss: 6.668 | Accuracy: 0.138672 | 0.069 sec/iter\n",
      "Epoch: 18 | Batch: 001 / 025 | Total loss: 6.595 | Reg loss: 0.028 | Tree loss: 6.595 | Accuracy: 0.142578 | 0.069 sec/iter\n",
      "Epoch: 18 | Batch: 002 / 025 | Total loss: 6.622 | Reg loss: 0.028 | Tree loss: 6.622 | Accuracy: 0.134766 | 0.069 sec/iter\n",
      "Epoch: 18 | Batch: 003 / 025 | Total loss: 6.564 | Reg loss: 0.028 | Tree loss: 6.564 | Accuracy: 0.150391 | 0.069 sec/iter\n",
      "Epoch: 18 | Batch: 004 / 025 | Total loss: 6.543 | Reg loss: 0.028 | Tree loss: 6.543 | Accuracy: 0.134766 | 0.069 sec/iter\n",
      "Epoch: 18 | Batch: 005 / 025 | Total loss: 6.567 | Reg loss: 0.028 | Tree loss: 6.567 | Accuracy: 0.126953 | 0.069 sec/iter\n",
      "Epoch: 18 | Batch: 006 / 025 | Total loss: 6.522 | Reg loss: 0.028 | Tree loss: 6.522 | Accuracy: 0.132812 | 0.069 sec/iter\n",
      "Epoch: 18 | Batch: 007 / 025 | Total loss: 6.510 | Reg loss: 0.028 | Tree loss: 6.510 | Accuracy: 0.107422 | 0.069 sec/iter\n",
      "Epoch: 18 | Batch: 008 / 025 | Total loss: 6.480 | Reg loss: 0.028 | Tree loss: 6.480 | Accuracy: 0.132812 | 0.069 sec/iter\n",
      "Epoch: 18 | Batch: 009 / 025 | Total loss: 6.453 | Reg loss: 0.028 | Tree loss: 6.453 | Accuracy: 0.146484 | 0.069 sec/iter\n",
      "Epoch: 18 | Batch: 010 / 025 | Total loss: 6.460 | Reg loss: 0.028 | Tree loss: 6.460 | Accuracy: 0.105469 | 0.069 sec/iter\n",
      "Epoch: 18 | Batch: 011 / 025 | Total loss: 6.423 | Reg loss: 0.029 | Tree loss: 6.423 | Accuracy: 0.138672 | 0.069 sec/iter\n",
      "Epoch: 18 | Batch: 012 / 025 | Total loss: 6.420 | Reg loss: 0.029 | Tree loss: 6.420 | Accuracy: 0.117188 | 0.069 sec/iter\n",
      "Epoch: 18 | Batch: 013 / 025 | Total loss: 6.380 | Reg loss: 0.029 | Tree loss: 6.380 | Accuracy: 0.128906 | 0.069 sec/iter\n",
      "Epoch: 18 | Batch: 014 / 025 | Total loss: 6.355 | Reg loss: 0.029 | Tree loss: 6.355 | Accuracy: 0.125000 | 0.069 sec/iter\n",
      "Epoch: 18 | Batch: 015 / 025 | Total loss: 6.339 | Reg loss: 0.029 | Tree loss: 6.339 | Accuracy: 0.101562 | 0.069 sec/iter\n",
      "Epoch: 18 | Batch: 016 / 025 | Total loss: 6.319 | Reg loss: 0.029 | Tree loss: 6.319 | Accuracy: 0.117188 | 0.069 sec/iter\n",
      "Epoch: 18 | Batch: 017 / 025 | Total loss: 6.309 | Reg loss: 0.029 | Tree loss: 6.309 | Accuracy: 0.105469 | 0.069 sec/iter\n",
      "Epoch: 18 | Batch: 018 / 025 | Total loss: 6.266 | Reg loss: 0.030 | Tree loss: 6.266 | Accuracy: 0.138672 | 0.069 sec/iter\n",
      "Epoch: 18 | Batch: 019 / 025 | Total loss: 6.206 | Reg loss: 0.030 | Tree loss: 6.206 | Accuracy: 0.126953 | 0.069 sec/iter\n",
      "Epoch: 18 | Batch: 020 / 025 | Total loss: 6.227 | Reg loss: 0.030 | Tree loss: 6.227 | Accuracy: 0.138672 | 0.069 sec/iter\n",
      "Epoch: 18 | Batch: 021 / 025 | Total loss: 6.214 | Reg loss: 0.030 | Tree loss: 6.214 | Accuracy: 0.138672 | 0.069 sec/iter\n",
      "Epoch: 18 | Batch: 022 / 025 | Total loss: 6.201 | Reg loss: 0.030 | Tree loss: 6.201 | Accuracy: 0.091797 | 0.069 sec/iter\n",
      "Epoch: 18 | Batch: 023 / 025 | Total loss: 6.183 | Reg loss: 0.030 | Tree loss: 6.183 | Accuracy: 0.109375 | 0.069 sec/iter\n",
      "Epoch: 18 | Batch: 024 / 025 | Total loss: 6.141 | Reg loss: 0.031 | Tree loss: 6.141 | Accuracy: 0.143750 | 0.069 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 19 | Batch: 000 / 025 | Total loss: 6.449 | Reg loss: 0.028 | Tree loss: 6.449 | Accuracy: 0.109375 | 0.069 sec/iter\n",
      "Epoch: 19 | Batch: 001 / 025 | Total loss: 6.471 | Reg loss: 0.028 | Tree loss: 6.471 | Accuracy: 0.128906 | 0.069 sec/iter\n",
      "Epoch: 19 | Batch: 002 / 025 | Total loss: 6.393 | Reg loss: 0.028 | Tree loss: 6.393 | Accuracy: 0.138672 | 0.069 sec/iter\n",
      "Epoch: 19 | Batch: 003 / 025 | Total loss: 6.402 | Reg loss: 0.028 | Tree loss: 6.402 | Accuracy: 0.115234 | 0.069 sec/iter\n",
      "Epoch: 19 | Batch: 004 / 025 | Total loss: 6.363 | Reg loss: 0.028 | Tree loss: 6.363 | Accuracy: 0.160156 | 0.069 sec/iter\n",
      "Epoch: 19 | Batch: 005 / 025 | Total loss: 6.345 | Reg loss: 0.028 | Tree loss: 6.345 | Accuracy: 0.136719 | 0.069 sec/iter\n",
      "Epoch: 19 | Batch: 006 / 025 | Total loss: 6.342 | Reg loss: 0.028 | Tree loss: 6.342 | Accuracy: 0.128906 | 0.069 sec/iter\n",
      "Epoch: 19 | Batch: 007 / 025 | Total loss: 6.343 | Reg loss: 0.028 | Tree loss: 6.343 | Accuracy: 0.117188 | 0.069 sec/iter\n",
      "Epoch: 19 | Batch: 008 / 025 | Total loss: 6.295 | Reg loss: 0.029 | Tree loss: 6.295 | Accuracy: 0.144531 | 0.069 sec/iter\n",
      "Epoch: 19 | Batch: 009 / 025 | Total loss: 6.286 | Reg loss: 0.029 | Tree loss: 6.286 | Accuracy: 0.126953 | 0.069 sec/iter\n",
      "Epoch: 19 | Batch: 010 / 025 | Total loss: 6.253 | Reg loss: 0.029 | Tree loss: 6.253 | Accuracy: 0.130859 | 0.069 sec/iter\n",
      "Epoch: 19 | Batch: 011 / 025 | Total loss: 6.248 | Reg loss: 0.029 | Tree loss: 6.248 | Accuracy: 0.123047 | 0.069 sec/iter\n",
      "Epoch: 19 | Batch: 012 / 025 | Total loss: 6.245 | Reg loss: 0.029 | Tree loss: 6.245 | Accuracy: 0.125000 | 0.069 sec/iter\n",
      "Epoch: 19 | Batch: 013 / 025 | Total loss: 6.206 | Reg loss: 0.029 | Tree loss: 6.206 | Accuracy: 0.095703 | 0.069 sec/iter\n",
      "Epoch: 19 | Batch: 014 / 025 | Total loss: 6.169 | Reg loss: 0.029 | Tree loss: 6.169 | Accuracy: 0.128906 | 0.069 sec/iter\n",
      "Epoch: 19 | Batch: 015 / 025 | Total loss: 6.177 | Reg loss: 0.030 | Tree loss: 6.177 | Accuracy: 0.115234 | 0.069 sec/iter\n",
      "Epoch: 19 | Batch: 016 / 025 | Total loss: 6.168 | Reg loss: 0.030 | Tree loss: 6.168 | Accuracy: 0.097656 | 0.069 sec/iter\n",
      "Epoch: 19 | Batch: 017 / 025 | Total loss: 6.116 | Reg loss: 0.030 | Tree loss: 6.116 | Accuracy: 0.117188 | 0.069 sec/iter\n",
      "Epoch: 19 | Batch: 018 / 025 | Total loss: 6.110 | Reg loss: 0.030 | Tree loss: 6.110 | Accuracy: 0.097656 | 0.069 sec/iter\n",
      "Epoch: 19 | Batch: 019 / 025 | Total loss: 6.035 | Reg loss: 0.030 | Tree loss: 6.035 | Accuracy: 0.140625 | 0.069 sec/iter\n",
      "Epoch: 19 | Batch: 020 / 025 | Total loss: 6.089 | Reg loss: 0.030 | Tree loss: 6.089 | Accuracy: 0.121094 | 0.069 sec/iter\n",
      "Epoch: 19 | Batch: 021 / 025 | Total loss: 6.045 | Reg loss: 0.031 | Tree loss: 6.045 | Accuracy: 0.142578 | 0.069 sec/iter\n",
      "Epoch: 19 | Batch: 022 / 025 | Total loss: 6.045 | Reg loss: 0.031 | Tree loss: 6.045 | Accuracy: 0.144531 | 0.069 sec/iter\n",
      "Epoch: 19 | Batch: 023 / 025 | Total loss: 5.989 | Reg loss: 0.031 | Tree loss: 5.989 | Accuracy: 0.132812 | 0.069 sec/iter\n",
      "Epoch: 19 | Batch: 024 / 025 | Total loss: 6.019 | Reg loss: 0.031 | Tree loss: 6.019 | Accuracy: 0.125000 | 0.069 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 20 | Batch: 000 / 025 | Total loss: 6.259 | Reg loss: 0.028 | Tree loss: 6.259 | Accuracy: 0.119141 | 0.069 sec/iter\n",
      "Epoch: 20 | Batch: 001 / 025 | Total loss: 6.278 | Reg loss: 0.028 | Tree loss: 6.278 | Accuracy: 0.107422 | 0.069 sec/iter\n",
      "Epoch: 20 | Batch: 002 / 025 | Total loss: 6.228 | Reg loss: 0.028 | Tree loss: 6.228 | Accuracy: 0.142578 | 0.069 sec/iter\n",
      "Epoch: 20 | Batch: 003 / 025 | Total loss: 6.224 | Reg loss: 0.028 | Tree loss: 6.224 | Accuracy: 0.128906 | 0.069 sec/iter\n",
      "Epoch: 20 | Batch: 004 / 025 | Total loss: 6.239 | Reg loss: 0.029 | Tree loss: 6.239 | Accuracy: 0.123047 | 0.069 sec/iter\n",
      "Epoch: 20 | Batch: 005 / 025 | Total loss: 6.192 | Reg loss: 0.029 | Tree loss: 6.192 | Accuracy: 0.103516 | 0.069 sec/iter\n",
      "Epoch: 20 | Batch: 006 / 025 | Total loss: 6.155 | Reg loss: 0.029 | Tree loss: 6.155 | Accuracy: 0.123047 | 0.069 sec/iter\n",
      "Epoch: 20 | Batch: 007 / 025 | Total loss: 6.190 | Reg loss: 0.029 | Tree loss: 6.190 | Accuracy: 0.119141 | 0.069 sec/iter\n",
      "Epoch: 20 | Batch: 008 / 025 | Total loss: 6.136 | Reg loss: 0.029 | Tree loss: 6.136 | Accuracy: 0.138672 | 0.069 sec/iter\n",
      "Epoch: 20 | Batch: 009 / 025 | Total loss: 6.113 | Reg loss: 0.029 | Tree loss: 6.113 | Accuracy: 0.119141 | 0.069 sec/iter\n",
      "Epoch: 20 | Batch: 010 / 025 | Total loss: 6.059 | Reg loss: 0.029 | Tree loss: 6.059 | Accuracy: 0.134766 | 0.069 sec/iter\n",
      "Epoch: 20 | Batch: 011 / 025 | Total loss: 6.004 | Reg loss: 0.029 | Tree loss: 6.004 | Accuracy: 0.144531 | 0.069 sec/iter\n",
      "Epoch: 20 | Batch: 012 / 025 | Total loss: 6.050 | Reg loss: 0.029 | Tree loss: 6.050 | Accuracy: 0.105469 | 0.069 sec/iter\n",
      "Epoch: 20 | Batch: 013 / 025 | Total loss: 5.978 | Reg loss: 0.030 | Tree loss: 5.978 | Accuracy: 0.134766 | 0.069 sec/iter\n",
      "Epoch: 20 | Batch: 014 / 025 | Total loss: 6.019 | Reg loss: 0.030 | Tree loss: 6.019 | Accuracy: 0.115234 | 0.069 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20 | Batch: 015 / 025 | Total loss: 5.977 | Reg loss: 0.030 | Tree loss: 5.977 | Accuracy: 0.134766 | 0.069 sec/iter\n",
      "Epoch: 20 | Batch: 016 / 025 | Total loss: 5.974 | Reg loss: 0.030 | Tree loss: 5.974 | Accuracy: 0.126953 | 0.069 sec/iter\n",
      "Epoch: 20 | Batch: 017 / 025 | Total loss: 5.944 | Reg loss: 0.030 | Tree loss: 5.944 | Accuracy: 0.134766 | 0.069 sec/iter\n",
      "Epoch: 20 | Batch: 018 / 025 | Total loss: 5.894 | Reg loss: 0.030 | Tree loss: 5.894 | Accuracy: 0.121094 | 0.069 sec/iter\n",
      "Epoch: 20 | Batch: 019 / 025 | Total loss: 5.861 | Reg loss: 0.031 | Tree loss: 5.861 | Accuracy: 0.132812 | 0.069 sec/iter\n",
      "Epoch: 20 | Batch: 020 / 025 | Total loss: 5.819 | Reg loss: 0.031 | Tree loss: 5.819 | Accuracy: 0.162109 | 0.069 sec/iter\n",
      "Epoch: 20 | Batch: 021 / 025 | Total loss: 5.849 | Reg loss: 0.031 | Tree loss: 5.849 | Accuracy: 0.111328 | 0.069 sec/iter\n",
      "Epoch: 20 | Batch: 022 / 025 | Total loss: 5.837 | Reg loss: 0.031 | Tree loss: 5.837 | Accuracy: 0.111328 | 0.069 sec/iter\n",
      "Epoch: 20 | Batch: 023 / 025 | Total loss: 5.790 | Reg loss: 0.031 | Tree loss: 5.790 | Accuracy: 0.126953 | 0.069 sec/iter\n",
      "Epoch: 20 | Batch: 024 / 025 | Total loss: 5.822 | Reg loss: 0.031 | Tree loss: 5.822 | Accuracy: 0.125000 | 0.069 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 21 | Batch: 000 / 025 | Total loss: 6.097 | Reg loss: 0.029 | Tree loss: 6.097 | Accuracy: 0.126953 | 0.069 sec/iter\n",
      "Epoch: 21 | Batch: 001 / 025 | Total loss: 6.064 | Reg loss: 0.029 | Tree loss: 6.064 | Accuracy: 0.119141 | 0.069 sec/iter\n",
      "Epoch: 21 | Batch: 002 / 025 | Total loss: 6.077 | Reg loss: 0.029 | Tree loss: 6.077 | Accuracy: 0.144531 | 0.069 sec/iter\n",
      "Epoch: 21 | Batch: 003 / 025 | Total loss: 6.022 | Reg loss: 0.029 | Tree loss: 6.022 | Accuracy: 0.123047 | 0.069 sec/iter\n",
      "Epoch: 21 | Batch: 004 / 025 | Total loss: 6.014 | Reg loss: 0.029 | Tree loss: 6.014 | Accuracy: 0.128906 | 0.069 sec/iter\n",
      "Epoch: 21 | Batch: 005 / 025 | Total loss: 5.981 | Reg loss: 0.029 | Tree loss: 5.981 | Accuracy: 0.132812 | 0.069 sec/iter\n",
      "Epoch: 21 | Batch: 006 / 025 | Total loss: 5.955 | Reg loss: 0.029 | Tree loss: 5.955 | Accuracy: 0.117188 | 0.069 sec/iter\n",
      "Epoch: 21 | Batch: 007 / 025 | Total loss: 5.981 | Reg loss: 0.029 | Tree loss: 5.981 | Accuracy: 0.121094 | 0.069 sec/iter\n",
      "Epoch: 21 | Batch: 008 / 025 | Total loss: 5.963 | Reg loss: 0.029 | Tree loss: 5.963 | Accuracy: 0.101562 | 0.069 sec/iter\n",
      "Epoch: 21 | Batch: 009 / 025 | Total loss: 5.914 | Reg loss: 0.029 | Tree loss: 5.914 | Accuracy: 0.115234 | 0.069 sec/iter\n",
      "Epoch: 21 | Batch: 010 / 025 | Total loss: 5.866 | Reg loss: 0.030 | Tree loss: 5.866 | Accuracy: 0.128906 | 0.069 sec/iter\n",
      "Epoch: 21 | Batch: 011 / 025 | Total loss: 5.843 | Reg loss: 0.030 | Tree loss: 5.843 | Accuracy: 0.119141 | 0.069 sec/iter\n",
      "Epoch: 21 | Batch: 012 / 025 | Total loss: 5.851 | Reg loss: 0.030 | Tree loss: 5.851 | Accuracy: 0.113281 | 0.069 sec/iter\n",
      "Epoch: 21 | Batch: 013 / 025 | Total loss: 5.788 | Reg loss: 0.030 | Tree loss: 5.788 | Accuracy: 0.136719 | 0.069 sec/iter\n",
      "Epoch: 21 | Batch: 014 / 025 | Total loss: 5.789 | Reg loss: 0.030 | Tree loss: 5.789 | Accuracy: 0.138672 | 0.069 sec/iter\n",
      "Epoch: 21 | Batch: 015 / 025 | Total loss: 5.743 | Reg loss: 0.030 | Tree loss: 5.743 | Accuracy: 0.123047 | 0.069 sec/iter\n",
      "Epoch: 21 | Batch: 016 / 025 | Total loss: 5.693 | Reg loss: 0.030 | Tree loss: 5.693 | Accuracy: 0.140625 | 0.069 sec/iter\n",
      "Epoch: 21 | Batch: 017 / 025 | Total loss: 5.748 | Reg loss: 0.031 | Tree loss: 5.748 | Accuracy: 0.119141 | 0.069 sec/iter\n",
      "Epoch: 21 | Batch: 018 / 025 | Total loss: 5.709 | Reg loss: 0.031 | Tree loss: 5.709 | Accuracy: 0.134766 | 0.069 sec/iter\n",
      "Epoch: 21 | Batch: 019 / 025 | Total loss: 5.707 | Reg loss: 0.031 | Tree loss: 5.707 | Accuracy: 0.119141 | 0.069 sec/iter\n",
      "Epoch: 21 | Batch: 020 / 025 | Total loss: 5.667 | Reg loss: 0.031 | Tree loss: 5.667 | Accuracy: 0.113281 | 0.069 sec/iter\n",
      "Epoch: 21 | Batch: 021 / 025 | Total loss: 5.635 | Reg loss: 0.031 | Tree loss: 5.635 | Accuracy: 0.142578 | 0.069 sec/iter\n",
      "Epoch: 21 | Batch: 022 / 025 | Total loss: 5.646 | Reg loss: 0.031 | Tree loss: 5.646 | Accuracy: 0.134766 | 0.07 sec/iter\n",
      "Epoch: 21 | Batch: 023 / 025 | Total loss: 5.636 | Reg loss: 0.032 | Tree loss: 5.636 | Accuracy: 0.136719 | 0.069 sec/iter\n",
      "Epoch: 21 | Batch: 024 / 025 | Total loss: 5.594 | Reg loss: 0.032 | Tree loss: 5.594 | Accuracy: 0.118750 | 0.069 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 22 | Batch: 000 / 025 | Total loss: 5.912 | Reg loss: 0.029 | Tree loss: 5.912 | Accuracy: 0.111328 | 0.07 sec/iter\n",
      "Epoch: 22 | Batch: 001 / 025 | Total loss: 5.827 | Reg loss: 0.029 | Tree loss: 5.827 | Accuracy: 0.138672 | 0.07 sec/iter\n",
      "Epoch: 22 | Batch: 002 / 025 | Total loss: 5.838 | Reg loss: 0.029 | Tree loss: 5.838 | Accuracy: 0.119141 | 0.07 sec/iter\n",
      "Epoch: 22 | Batch: 003 / 025 | Total loss: 5.826 | Reg loss: 0.029 | Tree loss: 5.826 | Accuracy: 0.125000 | 0.07 sec/iter\n",
      "Epoch: 22 | Batch: 004 / 025 | Total loss: 5.811 | Reg loss: 0.030 | Tree loss: 5.811 | Accuracy: 0.117188 | 0.07 sec/iter\n",
      "Epoch: 22 | Batch: 005 / 025 | Total loss: 5.776 | Reg loss: 0.030 | Tree loss: 5.776 | Accuracy: 0.138672 | 0.07 sec/iter\n",
      "Epoch: 22 | Batch: 006 / 025 | Total loss: 5.818 | Reg loss: 0.030 | Tree loss: 5.818 | Accuracy: 0.128906 | 0.07 sec/iter\n",
      "Epoch: 22 | Batch: 007 / 025 | Total loss: 5.711 | Reg loss: 0.030 | Tree loss: 5.711 | Accuracy: 0.140625 | 0.07 sec/iter\n",
      "Epoch: 22 | Batch: 008 / 025 | Total loss: 5.674 | Reg loss: 0.030 | Tree loss: 5.674 | Accuracy: 0.119141 | 0.07 sec/iter\n",
      "Epoch: 22 | Batch: 009 / 025 | Total loss: 5.645 | Reg loss: 0.030 | Tree loss: 5.645 | Accuracy: 0.136719 | 0.07 sec/iter\n",
      "Epoch: 22 | Batch: 010 / 025 | Total loss: 5.699 | Reg loss: 0.030 | Tree loss: 5.699 | Accuracy: 0.105469 | 0.07 sec/iter\n",
      "Epoch: 22 | Batch: 011 / 025 | Total loss: 5.657 | Reg loss: 0.030 | Tree loss: 5.657 | Accuracy: 0.126953 | 0.07 sec/iter\n",
      "Epoch: 22 | Batch: 012 / 025 | Total loss: 5.619 | Reg loss: 0.030 | Tree loss: 5.619 | Accuracy: 0.121094 | 0.07 sec/iter\n",
      "Epoch: 22 | Batch: 013 / 025 | Total loss: 5.643 | Reg loss: 0.030 | Tree loss: 5.643 | Accuracy: 0.128906 | 0.07 sec/iter\n",
      "Epoch: 22 | Batch: 014 / 025 | Total loss: 5.641 | Reg loss: 0.031 | Tree loss: 5.641 | Accuracy: 0.103516 | 0.07 sec/iter\n",
      "Epoch: 22 | Batch: 015 / 025 | Total loss: 5.575 | Reg loss: 0.031 | Tree loss: 5.575 | Accuracy: 0.126953 | 0.07 sec/iter\n",
      "Epoch: 22 | Batch: 016 / 025 | Total loss: 5.564 | Reg loss: 0.031 | Tree loss: 5.564 | Accuracy: 0.126953 | 0.07 sec/iter\n",
      "Epoch: 22 | Batch: 017 / 025 | Total loss: 5.519 | Reg loss: 0.031 | Tree loss: 5.519 | Accuracy: 0.142578 | 0.07 sec/iter\n",
      "Epoch: 22 | Batch: 018 / 025 | Total loss: 5.480 | Reg loss: 0.031 | Tree loss: 5.480 | Accuracy: 0.154297 | 0.07 sec/iter\n",
      "Epoch: 22 | Batch: 019 / 025 | Total loss: 5.543 | Reg loss: 0.031 | Tree loss: 5.543 | Accuracy: 0.109375 | 0.07 sec/iter\n",
      "Epoch: 22 | Batch: 020 / 025 | Total loss: 5.472 | Reg loss: 0.031 | Tree loss: 5.472 | Accuracy: 0.105469 | 0.07 sec/iter\n",
      "Epoch: 22 | Batch: 021 / 025 | Total loss: 5.434 | Reg loss: 0.032 | Tree loss: 5.434 | Accuracy: 0.125000 | 0.07 sec/iter\n",
      "Epoch: 22 | Batch: 022 / 025 | Total loss: 5.463 | Reg loss: 0.032 | Tree loss: 5.463 | Accuracy: 0.121094 | 0.07 sec/iter\n",
      "Epoch: 22 | Batch: 023 / 025 | Total loss: 5.379 | Reg loss: 0.032 | Tree loss: 5.379 | Accuracy: 0.148438 | 0.07 sec/iter\n",
      "Epoch: 22 | Batch: 024 / 025 | Total loss: 5.444 | Reg loss: 0.032 | Tree loss: 5.444 | Accuracy: 0.118750 | 0.07 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 23 | Batch: 000 / 025 | Total loss: 5.653 | Reg loss: 0.030 | Tree loss: 5.653 | Accuracy: 0.128906 | 0.07 sec/iter\n",
      "Epoch: 23 | Batch: 001 / 025 | Total loss: 5.661 | Reg loss: 0.030 | Tree loss: 5.661 | Accuracy: 0.140625 | 0.07 sec/iter\n",
      "Epoch: 23 | Batch: 002 / 025 | Total loss: 5.677 | Reg loss: 0.030 | Tree loss: 5.677 | Accuracy: 0.119141 | 0.07 sec/iter\n",
      "Epoch: 23 | Batch: 003 / 025 | Total loss: 5.643 | Reg loss: 0.030 | Tree loss: 5.643 | Accuracy: 0.111328 | 0.07 sec/iter\n",
      "Epoch: 23 | Batch: 004 / 025 | Total loss: 5.618 | Reg loss: 0.030 | Tree loss: 5.618 | Accuracy: 0.121094 | 0.07 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23 | Batch: 005 / 025 | Total loss: 5.599 | Reg loss: 0.030 | Tree loss: 5.599 | Accuracy: 0.130859 | 0.07 sec/iter\n",
      "Epoch: 23 | Batch: 006 / 025 | Total loss: 5.559 | Reg loss: 0.030 | Tree loss: 5.559 | Accuracy: 0.132812 | 0.07 sec/iter\n",
      "Epoch: 23 | Batch: 007 / 025 | Total loss: 5.530 | Reg loss: 0.030 | Tree loss: 5.530 | Accuracy: 0.136719 | 0.07 sec/iter\n",
      "Epoch: 23 | Batch: 008 / 025 | Total loss: 5.492 | Reg loss: 0.030 | Tree loss: 5.492 | Accuracy: 0.132812 | 0.07 sec/iter\n",
      "Epoch: 23 | Batch: 009 / 025 | Total loss: 5.471 | Reg loss: 0.030 | Tree loss: 5.471 | Accuracy: 0.121094 | 0.069 sec/iter\n",
      "Epoch: 23 | Batch: 010 / 025 | Total loss: 5.484 | Reg loss: 0.031 | Tree loss: 5.484 | Accuracy: 0.123047 | 0.069 sec/iter\n",
      "Epoch: 23 | Batch: 011 / 025 | Total loss: 5.460 | Reg loss: 0.031 | Tree loss: 5.460 | Accuracy: 0.144531 | 0.069 sec/iter\n",
      "Epoch: 23 | Batch: 012 / 025 | Total loss: 5.398 | Reg loss: 0.031 | Tree loss: 5.398 | Accuracy: 0.115234 | 0.069 sec/iter\n",
      "Epoch: 23 | Batch: 013 / 025 | Total loss: 5.461 | Reg loss: 0.031 | Tree loss: 5.461 | Accuracy: 0.117188 | 0.069 sec/iter\n",
      "Epoch: 23 | Batch: 014 / 025 | Total loss: 5.376 | Reg loss: 0.031 | Tree loss: 5.376 | Accuracy: 0.123047 | 0.069 sec/iter\n",
      "Epoch: 23 | Batch: 015 / 025 | Total loss: 5.379 | Reg loss: 0.031 | Tree loss: 5.379 | Accuracy: 0.128906 | 0.069 sec/iter\n",
      "Epoch: 23 | Batch: 016 / 025 | Total loss: 5.363 | Reg loss: 0.031 | Tree loss: 5.363 | Accuracy: 0.119141 | 0.069 sec/iter\n",
      "Epoch: 23 | Batch: 017 / 025 | Total loss: 5.385 | Reg loss: 0.031 | Tree loss: 5.385 | Accuracy: 0.121094 | 0.069 sec/iter\n",
      "Epoch: 23 | Batch: 018 / 025 | Total loss: 5.296 | Reg loss: 0.031 | Tree loss: 5.296 | Accuracy: 0.125000 | 0.069 sec/iter\n",
      "Epoch: 23 | Batch: 019 / 025 | Total loss: 5.323 | Reg loss: 0.031 | Tree loss: 5.323 | Accuracy: 0.132812 | 0.069 sec/iter\n",
      "Epoch: 23 | Batch: 020 / 025 | Total loss: 5.304 | Reg loss: 0.032 | Tree loss: 5.304 | Accuracy: 0.128906 | 0.069 sec/iter\n",
      "Epoch: 23 | Batch: 021 / 025 | Total loss: 5.262 | Reg loss: 0.032 | Tree loss: 5.262 | Accuracy: 0.144531 | 0.069 sec/iter\n",
      "Epoch: 23 | Batch: 022 / 025 | Total loss: 5.277 | Reg loss: 0.032 | Tree loss: 5.277 | Accuracy: 0.126953 | 0.069 sec/iter\n",
      "Epoch: 23 | Batch: 023 / 025 | Total loss: 5.214 | Reg loss: 0.032 | Tree loss: 5.214 | Accuracy: 0.126953 | 0.069 sec/iter\n",
      "Epoch: 23 | Batch: 024 / 025 | Total loss: 5.229 | Reg loss: 0.032 | Tree loss: 5.229 | Accuracy: 0.103125 | 0.069 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 24 | Batch: 000 / 025 | Total loss: 5.422 | Reg loss: 0.030 | Tree loss: 5.422 | Accuracy: 0.132812 | 0.069 sec/iter\n",
      "Epoch: 24 | Batch: 001 / 025 | Total loss: 5.422 | Reg loss: 0.030 | Tree loss: 5.422 | Accuracy: 0.109375 | 0.069 sec/iter\n",
      "Epoch: 24 | Batch: 002 / 025 | Total loss: 5.406 | Reg loss: 0.030 | Tree loss: 5.406 | Accuracy: 0.134766 | 0.069 sec/iter\n",
      "Epoch: 24 | Batch: 003 / 025 | Total loss: 5.410 | Reg loss: 0.030 | Tree loss: 5.410 | Accuracy: 0.125000 | 0.069 sec/iter\n",
      "Epoch: 24 | Batch: 004 / 025 | Total loss: 5.394 | Reg loss: 0.030 | Tree loss: 5.394 | Accuracy: 0.132812 | 0.069 sec/iter\n",
      "Epoch: 24 | Batch: 005 / 025 | Total loss: 5.382 | Reg loss: 0.030 | Tree loss: 5.382 | Accuracy: 0.142578 | 0.069 sec/iter\n",
      "Epoch: 24 | Batch: 006 / 025 | Total loss: 5.437 | Reg loss: 0.031 | Tree loss: 5.437 | Accuracy: 0.109375 | 0.069 sec/iter\n",
      "Epoch: 24 | Batch: 007 / 025 | Total loss: 5.345 | Reg loss: 0.031 | Tree loss: 5.345 | Accuracy: 0.128906 | 0.069 sec/iter\n",
      "Epoch: 24 | Batch: 008 / 025 | Total loss: 5.351 | Reg loss: 0.031 | Tree loss: 5.351 | Accuracy: 0.128906 | 0.069 sec/iter\n",
      "Epoch: 24 | Batch: 009 / 025 | Total loss: 5.289 | Reg loss: 0.031 | Tree loss: 5.289 | Accuracy: 0.136719 | 0.069 sec/iter\n",
      "Epoch: 24 | Batch: 010 / 025 | Total loss: 5.283 | Reg loss: 0.031 | Tree loss: 5.283 | Accuracy: 0.123047 | 0.069 sec/iter\n",
      "Epoch: 24 | Batch: 011 / 025 | Total loss: 5.285 | Reg loss: 0.031 | Tree loss: 5.285 | Accuracy: 0.128906 | 0.069 sec/iter\n",
      "Epoch: 24 | Batch: 012 / 025 | Total loss: 5.269 | Reg loss: 0.031 | Tree loss: 5.269 | Accuracy: 0.134766 | 0.069 sec/iter\n",
      "Epoch: 24 | Batch: 013 / 025 | Total loss: 5.212 | Reg loss: 0.031 | Tree loss: 5.212 | Accuracy: 0.150391 | 0.069 sec/iter\n",
      "Epoch: 24 | Batch: 014 / 025 | Total loss: 5.266 | Reg loss: 0.031 | Tree loss: 5.266 | Accuracy: 0.126953 | 0.069 sec/iter\n",
      "Epoch: 24 | Batch: 015 / 025 | Total loss: 5.243 | Reg loss: 0.031 | Tree loss: 5.243 | Accuracy: 0.111328 | 0.069 sec/iter\n",
      "Epoch: 24 | Batch: 016 / 025 | Total loss: 5.139 | Reg loss: 0.031 | Tree loss: 5.139 | Accuracy: 0.138672 | 0.069 sec/iter\n",
      "Epoch: 24 | Batch: 017 / 025 | Total loss: 5.147 | Reg loss: 0.031 | Tree loss: 5.147 | Accuracy: 0.125000 | 0.069 sec/iter\n",
      "Epoch: 24 | Batch: 018 / 025 | Total loss: 5.084 | Reg loss: 0.031 | Tree loss: 5.084 | Accuracy: 0.136719 | 0.069 sec/iter\n",
      "Epoch: 24 | Batch: 019 / 025 | Total loss: 5.184 | Reg loss: 0.032 | Tree loss: 5.184 | Accuracy: 0.119141 | 0.069 sec/iter\n",
      "Epoch: 24 | Batch: 020 / 025 | Total loss: 5.123 | Reg loss: 0.032 | Tree loss: 5.123 | Accuracy: 0.128906 | 0.069 sec/iter\n",
      "Epoch: 24 | Batch: 021 / 025 | Total loss: 5.101 | Reg loss: 0.032 | Tree loss: 5.101 | Accuracy: 0.115234 | 0.069 sec/iter\n",
      "Epoch: 24 | Batch: 022 / 025 | Total loss: 5.094 | Reg loss: 0.032 | Tree loss: 5.094 | Accuracy: 0.117188 | 0.069 sec/iter\n",
      "Epoch: 24 | Batch: 023 / 025 | Total loss: 5.096 | Reg loss: 0.032 | Tree loss: 5.096 | Accuracy: 0.121094 | 0.069 sec/iter\n",
      "Epoch: 24 | Batch: 024 / 025 | Total loss: 5.025 | Reg loss: 0.032 | Tree loss: 5.025 | Accuracy: 0.134375 | 0.069 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 25 | Batch: 000 / 025 | Total loss: 5.290 | Reg loss: 0.031 | Tree loss: 5.290 | Accuracy: 0.121094 | 0.069 sec/iter\n",
      "Epoch: 25 | Batch: 001 / 025 | Total loss: 5.278 | Reg loss: 0.031 | Tree loss: 5.278 | Accuracy: 0.111328 | 0.069 sec/iter\n",
      "Epoch: 25 | Batch: 002 / 025 | Total loss: 5.245 | Reg loss: 0.031 | Tree loss: 5.245 | Accuracy: 0.126953 | 0.069 sec/iter\n",
      "Epoch: 25 | Batch: 003 / 025 | Total loss: 5.264 | Reg loss: 0.031 | Tree loss: 5.264 | Accuracy: 0.105469 | 0.069 sec/iter\n",
      "Epoch: 25 | Batch: 004 / 025 | Total loss: 5.220 | Reg loss: 0.031 | Tree loss: 5.220 | Accuracy: 0.136719 | 0.069 sec/iter\n",
      "Epoch: 25 | Batch: 005 / 025 | Total loss: 5.179 | Reg loss: 0.031 | Tree loss: 5.179 | Accuracy: 0.140625 | 0.069 sec/iter\n",
      "Epoch: 25 | Batch: 006 / 025 | Total loss: 5.191 | Reg loss: 0.031 | Tree loss: 5.191 | Accuracy: 0.125000 | 0.069 sec/iter\n",
      "Epoch: 25 | Batch: 007 / 025 | Total loss: 5.160 | Reg loss: 0.031 | Tree loss: 5.160 | Accuracy: 0.134766 | 0.069 sec/iter\n",
      "Epoch: 25 | Batch: 008 / 025 | Total loss: 5.149 | Reg loss: 0.031 | Tree loss: 5.149 | Accuracy: 0.121094 | 0.069 sec/iter\n",
      "Epoch: 25 | Batch: 009 / 025 | Total loss: 5.136 | Reg loss: 0.031 | Tree loss: 5.136 | Accuracy: 0.132812 | 0.069 sec/iter\n",
      "Epoch: 25 | Batch: 010 / 025 | Total loss: 5.138 | Reg loss: 0.031 | Tree loss: 5.138 | Accuracy: 0.125000 | 0.069 sec/iter\n",
      "Epoch: 25 | Batch: 011 / 025 | Total loss: 5.126 | Reg loss: 0.031 | Tree loss: 5.126 | Accuracy: 0.119141 | 0.069 sec/iter\n",
      "Epoch: 25 | Batch: 012 / 025 | Total loss: 5.074 | Reg loss: 0.031 | Tree loss: 5.074 | Accuracy: 0.142578 | 0.069 sec/iter\n",
      "Epoch: 25 | Batch: 013 / 025 | Total loss: 5.089 | Reg loss: 0.031 | Tree loss: 5.089 | Accuracy: 0.117188 | 0.069 sec/iter\n",
      "Epoch: 25 | Batch: 014 / 025 | Total loss: 5.014 | Reg loss: 0.031 | Tree loss: 5.014 | Accuracy: 0.113281 | 0.069 sec/iter\n",
      "Epoch: 25 | Batch: 015 / 025 | Total loss: 4.974 | Reg loss: 0.031 | Tree loss: 4.974 | Accuracy: 0.140625 | 0.069 sec/iter\n",
      "Epoch: 25 | Batch: 016 / 025 | Total loss: 4.960 | Reg loss: 0.031 | Tree loss: 4.960 | Accuracy: 0.128906 | 0.069 sec/iter\n",
      "Epoch: 25 | Batch: 017 / 025 | Total loss: 4.987 | Reg loss: 0.031 | Tree loss: 4.987 | Accuracy: 0.140625 | 0.069 sec/iter\n",
      "Epoch: 25 | Batch: 018 / 025 | Total loss: 4.985 | Reg loss: 0.031 | Tree loss: 4.985 | Accuracy: 0.115234 | 0.069 sec/iter\n",
      "Epoch: 25 | Batch: 019 / 025 | Total loss: 4.937 | Reg loss: 0.031 | Tree loss: 4.937 | Accuracy: 0.140625 | 0.069 sec/iter\n",
      "Epoch: 25 | Batch: 020 / 025 | Total loss: 4.897 | Reg loss: 0.032 | Tree loss: 4.897 | Accuracy: 0.142578 | 0.069 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25 | Batch: 021 / 025 | Total loss: 4.964 | Reg loss: 0.032 | Tree loss: 4.964 | Accuracy: 0.132812 | 0.069 sec/iter\n",
      "Epoch: 25 | Batch: 022 / 025 | Total loss: 4.905 | Reg loss: 0.032 | Tree loss: 4.905 | Accuracy: 0.121094 | 0.069 sec/iter\n",
      "Epoch: 25 | Batch: 023 / 025 | Total loss: 4.858 | Reg loss: 0.032 | Tree loss: 4.858 | Accuracy: 0.136719 | 0.069 sec/iter\n",
      "Epoch: 25 | Batch: 024 / 025 | Total loss: 4.848 | Reg loss: 0.032 | Tree loss: 4.848 | Accuracy: 0.115625 | 0.069 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 26 | Batch: 000 / 025 | Total loss: 5.117 | Reg loss: 0.031 | Tree loss: 5.117 | Accuracy: 0.148438 | 0.069 sec/iter\n",
      "Epoch: 26 | Batch: 001 / 025 | Total loss: 5.094 | Reg loss: 0.031 | Tree loss: 5.094 | Accuracy: 0.142578 | 0.069 sec/iter\n",
      "Epoch: 26 | Batch: 002 / 025 | Total loss: 5.134 | Reg loss: 0.031 | Tree loss: 5.134 | Accuracy: 0.111328 | 0.069 sec/iter\n",
      "Epoch: 26 | Batch: 003 / 025 | Total loss: 5.092 | Reg loss: 0.031 | Tree loss: 5.092 | Accuracy: 0.117188 | 0.069 sec/iter\n",
      "Epoch: 26 | Batch: 004 / 025 | Total loss: 5.033 | Reg loss: 0.031 | Tree loss: 5.033 | Accuracy: 0.130859 | 0.069 sec/iter\n",
      "Epoch: 26 | Batch: 005 / 025 | Total loss: 4.983 | Reg loss: 0.031 | Tree loss: 4.983 | Accuracy: 0.152344 | 0.069 sec/iter\n",
      "Epoch: 26 | Batch: 006 / 025 | Total loss: 4.980 | Reg loss: 0.031 | Tree loss: 4.980 | Accuracy: 0.134766 | 0.069 sec/iter\n",
      "Epoch: 26 | Batch: 007 / 025 | Total loss: 5.018 | Reg loss: 0.031 | Tree loss: 5.018 | Accuracy: 0.113281 | 0.069 sec/iter\n",
      "Epoch: 26 | Batch: 008 / 025 | Total loss: 4.973 | Reg loss: 0.031 | Tree loss: 4.973 | Accuracy: 0.099609 | 0.069 sec/iter\n",
      "Epoch: 26 | Batch: 009 / 025 | Total loss: 4.919 | Reg loss: 0.031 | Tree loss: 4.919 | Accuracy: 0.136719 | 0.069 sec/iter\n",
      "Epoch: 26 | Batch: 010 / 025 | Total loss: 4.942 | Reg loss: 0.031 | Tree loss: 4.942 | Accuracy: 0.125000 | 0.069 sec/iter\n",
      "Epoch: 26 | Batch: 011 / 025 | Total loss: 4.905 | Reg loss: 0.031 | Tree loss: 4.905 | Accuracy: 0.121094 | 0.069 sec/iter\n",
      "Epoch: 26 | Batch: 012 / 025 | Total loss: 4.899 | Reg loss: 0.031 | Tree loss: 4.899 | Accuracy: 0.128906 | 0.069 sec/iter\n",
      "Epoch: 26 | Batch: 013 / 025 | Total loss: 4.885 | Reg loss: 0.031 | Tree loss: 4.885 | Accuracy: 0.132812 | 0.069 sec/iter\n",
      "Epoch: 26 | Batch: 014 / 025 | Total loss: 4.880 | Reg loss: 0.031 | Tree loss: 4.880 | Accuracy: 0.115234 | 0.069 sec/iter\n",
      "Epoch: 26 | Batch: 015 / 025 | Total loss: 4.818 | Reg loss: 0.031 | Tree loss: 4.818 | Accuracy: 0.144531 | 0.069 sec/iter\n",
      "Epoch: 26 | Batch: 016 / 025 | Total loss: 4.809 | Reg loss: 0.031 | Tree loss: 4.809 | Accuracy: 0.119141 | 0.07 sec/iter\n",
      "Epoch: 26 | Batch: 017 / 025 | Total loss: 4.840 | Reg loss: 0.031 | Tree loss: 4.840 | Accuracy: 0.142578 | 0.07 sec/iter\n",
      "Epoch: 26 | Batch: 018 / 025 | Total loss: 4.814 | Reg loss: 0.031 | Tree loss: 4.814 | Accuracy: 0.123047 | 0.07 sec/iter\n",
      "Epoch: 26 | Batch: 019 / 025 | Total loss: 4.791 | Reg loss: 0.031 | Tree loss: 4.791 | Accuracy: 0.113281 | 0.07 sec/iter\n",
      "Epoch: 26 | Batch: 020 / 025 | Total loss: 4.741 | Reg loss: 0.031 | Tree loss: 4.741 | Accuracy: 0.134766 | 0.07 sec/iter\n",
      "Epoch: 26 | Batch: 021 / 025 | Total loss: 4.723 | Reg loss: 0.031 | Tree loss: 4.723 | Accuracy: 0.119141 | 0.07 sec/iter\n",
      "Epoch: 26 | Batch: 022 / 025 | Total loss: 4.695 | Reg loss: 0.031 | Tree loss: 4.695 | Accuracy: 0.158203 | 0.07 sec/iter\n",
      "Epoch: 26 | Batch: 023 / 025 | Total loss: 4.753 | Reg loss: 0.032 | Tree loss: 4.753 | Accuracy: 0.093750 | 0.07 sec/iter\n",
      "Epoch: 26 | Batch: 024 / 025 | Total loss: 4.659 | Reg loss: 0.032 | Tree loss: 4.659 | Accuracy: 0.134375 | 0.07 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 27 | Batch: 000 / 025 | Total loss: 4.935 | Reg loss: 0.030 | Tree loss: 4.935 | Accuracy: 0.119141 | 0.07 sec/iter\n",
      "Epoch: 27 | Batch: 001 / 025 | Total loss: 4.928 | Reg loss: 0.030 | Tree loss: 4.928 | Accuracy: 0.146484 | 0.07 sec/iter\n",
      "Epoch: 27 | Batch: 002 / 025 | Total loss: 4.933 | Reg loss: 0.030 | Tree loss: 4.933 | Accuracy: 0.105469 | 0.07 sec/iter\n",
      "Epoch: 27 | Batch: 003 / 025 | Total loss: 4.889 | Reg loss: 0.030 | Tree loss: 4.889 | Accuracy: 0.117188 | 0.07 sec/iter\n",
      "Epoch: 27 | Batch: 004 / 025 | Total loss: 4.864 | Reg loss: 0.030 | Tree loss: 4.864 | Accuracy: 0.123047 | 0.07 sec/iter\n",
      "Epoch: 27 | Batch: 005 / 025 | Total loss: 4.884 | Reg loss: 0.030 | Tree loss: 4.884 | Accuracy: 0.117188 | 0.07 sec/iter\n",
      "Epoch: 27 | Batch: 006 / 025 | Total loss: 4.841 | Reg loss: 0.030 | Tree loss: 4.841 | Accuracy: 0.117188 | 0.07 sec/iter\n",
      "Epoch: 27 | Batch: 007 / 025 | Total loss: 4.833 | Reg loss: 0.031 | Tree loss: 4.833 | Accuracy: 0.166016 | 0.07 sec/iter\n",
      "Epoch: 27 | Batch: 008 / 025 | Total loss: 4.783 | Reg loss: 0.031 | Tree loss: 4.783 | Accuracy: 0.126953 | 0.07 sec/iter\n",
      "Epoch: 27 | Batch: 009 / 025 | Total loss: 4.765 | Reg loss: 0.031 | Tree loss: 4.765 | Accuracy: 0.113281 | 0.07 sec/iter\n",
      "Epoch: 27 | Batch: 010 / 025 | Total loss: 4.785 | Reg loss: 0.031 | Tree loss: 4.785 | Accuracy: 0.130859 | 0.07 sec/iter\n",
      "Epoch: 27 | Batch: 011 / 025 | Total loss: 4.758 | Reg loss: 0.031 | Tree loss: 4.758 | Accuracy: 0.117188 | 0.07 sec/iter\n",
      "Epoch: 27 | Batch: 012 / 025 | Total loss: 4.748 | Reg loss: 0.031 | Tree loss: 4.748 | Accuracy: 0.136719 | 0.07 sec/iter\n",
      "Epoch: 27 | Batch: 013 / 025 | Total loss: 4.646 | Reg loss: 0.031 | Tree loss: 4.646 | Accuracy: 0.156250 | 0.07 sec/iter\n",
      "Epoch: 27 | Batch: 014 / 025 | Total loss: 4.715 | Reg loss: 0.031 | Tree loss: 4.715 | Accuracy: 0.136719 | 0.07 sec/iter\n",
      "Epoch: 27 | Batch: 015 / 025 | Total loss: 4.708 | Reg loss: 0.031 | Tree loss: 4.708 | Accuracy: 0.107422 | 0.07 sec/iter\n",
      "Epoch: 27 | Batch: 016 / 025 | Total loss: 4.690 | Reg loss: 0.031 | Tree loss: 4.690 | Accuracy: 0.119141 | 0.07 sec/iter\n",
      "Epoch: 27 | Batch: 017 / 025 | Total loss: 4.652 | Reg loss: 0.031 | Tree loss: 4.652 | Accuracy: 0.136719 | 0.07 sec/iter\n",
      "Epoch: 27 | Batch: 018 / 025 | Total loss: 4.653 | Reg loss: 0.031 | Tree loss: 4.653 | Accuracy: 0.132812 | 0.07 sec/iter\n",
      "Epoch: 27 | Batch: 019 / 025 | Total loss: 4.597 | Reg loss: 0.031 | Tree loss: 4.597 | Accuracy: 0.128906 | 0.07 sec/iter\n",
      "Epoch: 27 | Batch: 020 / 025 | Total loss: 4.601 | Reg loss: 0.031 | Tree loss: 4.601 | Accuracy: 0.130859 | 0.07 sec/iter\n",
      "Epoch: 27 | Batch: 021 / 025 | Total loss: 4.502 | Reg loss: 0.031 | Tree loss: 4.502 | Accuracy: 0.125000 | 0.07 sec/iter\n",
      "Epoch: 27 | Batch: 022 / 025 | Total loss: 4.527 | Reg loss: 0.031 | Tree loss: 4.527 | Accuracy: 0.136719 | 0.07 sec/iter\n",
      "Epoch: 27 | Batch: 023 / 025 | Total loss: 4.583 | Reg loss: 0.031 | Tree loss: 4.583 | Accuracy: 0.130859 | 0.07 sec/iter\n",
      "Epoch: 27 | Batch: 024 / 025 | Total loss: 4.527 | Reg loss: 0.031 | Tree loss: 4.527 | Accuracy: 0.109375 | 0.07 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 28 | Batch: 000 / 025 | Total loss: 4.752 | Reg loss: 0.030 | Tree loss: 4.752 | Accuracy: 0.132812 | 0.07 sec/iter\n",
      "Epoch: 28 | Batch: 001 / 025 | Total loss: 4.831 | Reg loss: 0.030 | Tree loss: 4.831 | Accuracy: 0.117188 | 0.07 sec/iter\n",
      "Epoch: 28 | Batch: 002 / 025 | Total loss: 4.743 | Reg loss: 0.030 | Tree loss: 4.743 | Accuracy: 0.140625 | 0.07 sec/iter\n",
      "Epoch: 28 | Batch: 003 / 025 | Total loss: 4.747 | Reg loss: 0.030 | Tree loss: 4.747 | Accuracy: 0.146484 | 0.07 sec/iter\n",
      "Epoch: 28 | Batch: 004 / 025 | Total loss: 4.680 | Reg loss: 0.030 | Tree loss: 4.680 | Accuracy: 0.142578 | 0.07 sec/iter\n",
      "Epoch: 28 | Batch: 005 / 025 | Total loss: 4.684 | Reg loss: 0.030 | Tree loss: 4.684 | Accuracy: 0.142578 | 0.07 sec/iter\n",
      "Epoch: 28 | Batch: 006 / 025 | Total loss: 4.702 | Reg loss: 0.030 | Tree loss: 4.702 | Accuracy: 0.126953 | 0.07 sec/iter\n",
      "Epoch: 28 | Batch: 007 / 025 | Total loss: 4.657 | Reg loss: 0.030 | Tree loss: 4.657 | Accuracy: 0.134766 | 0.07 sec/iter\n",
      "Epoch: 28 | Batch: 008 / 025 | Total loss: 4.621 | Reg loss: 0.030 | Tree loss: 4.621 | Accuracy: 0.142578 | 0.07 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 28 | Batch: 009 / 025 | Total loss: 4.537 | Reg loss: 0.030 | Tree loss: 4.537 | Accuracy: 0.134766 | 0.07 sec/iter\n",
      "Epoch: 28 | Batch: 010 / 025 | Total loss: 4.600 | Reg loss: 0.030 | Tree loss: 4.600 | Accuracy: 0.113281 | 0.07 sec/iter\n",
      "Epoch: 28 | Batch: 011 / 025 | Total loss: 4.535 | Reg loss: 0.030 | Tree loss: 4.535 | Accuracy: 0.117188 | 0.07 sec/iter\n",
      "Epoch: 28 | Batch: 012 / 025 | Total loss: 4.623 | Reg loss: 0.030 | Tree loss: 4.623 | Accuracy: 0.134766 | 0.07 sec/iter\n",
      "Epoch: 28 | Batch: 013 / 025 | Total loss: 4.526 | Reg loss: 0.030 | Tree loss: 4.526 | Accuracy: 0.128906 | 0.07 sec/iter\n",
      "Epoch: 28 | Batch: 014 / 025 | Total loss: 4.562 | Reg loss: 0.030 | Tree loss: 4.562 | Accuracy: 0.089844 | 0.07 sec/iter\n",
      "Epoch: 28 | Batch: 015 / 025 | Total loss: 4.523 | Reg loss: 0.030 | Tree loss: 4.523 | Accuracy: 0.123047 | 0.07 sec/iter\n",
      "Epoch: 28 | Batch: 016 / 025 | Total loss: 4.590 | Reg loss: 0.030 | Tree loss: 4.590 | Accuracy: 0.119141 | 0.07 sec/iter\n",
      "Epoch: 28 | Batch: 017 / 025 | Total loss: 4.467 | Reg loss: 0.030 | Tree loss: 4.467 | Accuracy: 0.125000 | 0.07 sec/iter\n",
      "Epoch: 28 | Batch: 018 / 025 | Total loss: 4.524 | Reg loss: 0.030 | Tree loss: 4.524 | Accuracy: 0.128906 | 0.07 sec/iter\n",
      "Epoch: 28 | Batch: 019 / 025 | Total loss: 4.439 | Reg loss: 0.031 | Tree loss: 4.439 | Accuracy: 0.123047 | 0.07 sec/iter\n",
      "Epoch: 28 | Batch: 020 / 025 | Total loss: 4.426 | Reg loss: 0.031 | Tree loss: 4.426 | Accuracy: 0.105469 | 0.07 sec/iter\n",
      "Epoch: 28 | Batch: 021 / 025 | Total loss: 4.451 | Reg loss: 0.031 | Tree loss: 4.451 | Accuracy: 0.128906 | 0.07 sec/iter\n",
      "Epoch: 28 | Batch: 022 / 025 | Total loss: 4.427 | Reg loss: 0.031 | Tree loss: 4.427 | Accuracy: 0.123047 | 0.07 sec/iter\n",
      "Epoch: 28 | Batch: 023 / 025 | Total loss: 4.338 | Reg loss: 0.031 | Tree loss: 4.338 | Accuracy: 0.148438 | 0.07 sec/iter\n",
      "Epoch: 28 | Batch: 024 / 025 | Total loss: 4.354 | Reg loss: 0.031 | Tree loss: 4.354 | Accuracy: 0.125000 | 0.07 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 29 | Batch: 000 / 025 | Total loss: 4.650 | Reg loss: 0.030 | Tree loss: 4.650 | Accuracy: 0.126953 | 0.07 sec/iter\n",
      "Epoch: 29 | Batch: 001 / 025 | Total loss: 4.554 | Reg loss: 0.030 | Tree loss: 4.554 | Accuracy: 0.158203 | 0.07 sec/iter\n",
      "Epoch: 29 | Batch: 002 / 025 | Total loss: 4.558 | Reg loss: 0.030 | Tree loss: 4.558 | Accuracy: 0.156250 | 0.07 sec/iter\n",
      "Epoch: 29 | Batch: 003 / 025 | Total loss: 4.585 | Reg loss: 0.030 | Tree loss: 4.585 | Accuracy: 0.130859 | 0.07 sec/iter\n",
      "Epoch: 29 | Batch: 004 / 025 | Total loss: 4.587 | Reg loss: 0.030 | Tree loss: 4.587 | Accuracy: 0.128906 | 0.07 sec/iter\n",
      "Epoch: 29 | Batch: 005 / 025 | Total loss: 4.511 | Reg loss: 0.030 | Tree loss: 4.511 | Accuracy: 0.126953 | 0.07 sec/iter\n",
      "Epoch: 29 | Batch: 006 / 025 | Total loss: 4.497 | Reg loss: 0.030 | Tree loss: 4.497 | Accuracy: 0.150391 | 0.07 sec/iter\n",
      "Epoch: 29 | Batch: 007 / 025 | Total loss: 4.507 | Reg loss: 0.030 | Tree loss: 4.507 | Accuracy: 0.107422 | 0.07 sec/iter\n",
      "Epoch: 29 | Batch: 008 / 025 | Total loss: 4.495 | Reg loss: 0.030 | Tree loss: 4.495 | Accuracy: 0.107422 | 0.07 sec/iter\n",
      "Epoch: 29 | Batch: 009 / 025 | Total loss: 4.410 | Reg loss: 0.030 | Tree loss: 4.410 | Accuracy: 0.154297 | 0.07 sec/iter\n",
      "Epoch: 29 | Batch: 010 / 025 | Total loss: 4.433 | Reg loss: 0.030 | Tree loss: 4.433 | Accuracy: 0.138672 | 0.07 sec/iter\n",
      "Epoch: 29 | Batch: 011 / 025 | Total loss: 4.452 | Reg loss: 0.030 | Tree loss: 4.452 | Accuracy: 0.123047 | 0.07 sec/iter\n",
      "Epoch: 29 | Batch: 012 / 025 | Total loss: 4.419 | Reg loss: 0.030 | Tree loss: 4.419 | Accuracy: 0.123047 | 0.07 sec/iter\n",
      "Epoch: 29 | Batch: 013 / 025 | Total loss: 4.416 | Reg loss: 0.030 | Tree loss: 4.416 | Accuracy: 0.117188 | 0.07 sec/iter\n",
      "Epoch: 29 | Batch: 014 / 025 | Total loss: 4.406 | Reg loss: 0.030 | Tree loss: 4.406 | Accuracy: 0.123047 | 0.07 sec/iter\n",
      "Epoch: 29 | Batch: 015 / 025 | Total loss: 4.401 | Reg loss: 0.030 | Tree loss: 4.401 | Accuracy: 0.111328 | 0.07 sec/iter\n",
      "Epoch: 29 | Batch: 016 / 025 | Total loss: 4.364 | Reg loss: 0.030 | Tree loss: 4.364 | Accuracy: 0.115234 | 0.07 sec/iter\n",
      "Epoch: 29 | Batch: 017 / 025 | Total loss: 4.342 | Reg loss: 0.030 | Tree loss: 4.342 | Accuracy: 0.115234 | 0.07 sec/iter\n",
      "Epoch: 29 | Batch: 018 / 025 | Total loss: 4.337 | Reg loss: 0.030 | Tree loss: 4.337 | Accuracy: 0.130859 | 0.07 sec/iter\n",
      "Epoch: 29 | Batch: 019 / 025 | Total loss: 4.314 | Reg loss: 0.030 | Tree loss: 4.314 | Accuracy: 0.117188 | 0.07 sec/iter\n",
      "Epoch: 29 | Batch: 020 / 025 | Total loss: 4.309 | Reg loss: 0.030 | Tree loss: 4.309 | Accuracy: 0.123047 | 0.07 sec/iter\n",
      "Epoch: 29 | Batch: 021 / 025 | Total loss: 4.264 | Reg loss: 0.030 | Tree loss: 4.264 | Accuracy: 0.142578 | 0.07 sec/iter\n",
      "Epoch: 29 | Batch: 022 / 025 | Total loss: 4.276 | Reg loss: 0.030 | Tree loss: 4.276 | Accuracy: 0.119141 | 0.07 sec/iter\n",
      "Epoch: 29 | Batch: 023 / 025 | Total loss: 4.295 | Reg loss: 0.030 | Tree loss: 4.295 | Accuracy: 0.119141 | 0.07 sec/iter\n",
      "Epoch: 29 | Batch: 024 / 025 | Total loss: 4.153 | Reg loss: 0.030 | Tree loss: 4.153 | Accuracy: 0.131250 | 0.07 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 30 | Batch: 000 / 025 | Total loss: 4.485 | Reg loss: 0.029 | Tree loss: 4.485 | Accuracy: 0.123047 | 0.07 sec/iter\n",
      "Epoch: 30 | Batch: 001 / 025 | Total loss: 4.451 | Reg loss: 0.029 | Tree loss: 4.451 | Accuracy: 0.150391 | 0.07 sec/iter\n",
      "Epoch: 30 | Batch: 002 / 025 | Total loss: 4.444 | Reg loss: 0.029 | Tree loss: 4.444 | Accuracy: 0.115234 | 0.07 sec/iter\n",
      "Epoch: 30 | Batch: 003 / 025 | Total loss: 4.410 | Reg loss: 0.029 | Tree loss: 4.410 | Accuracy: 0.125000 | 0.07 sec/iter\n",
      "Epoch: 30 | Batch: 004 / 025 | Total loss: 4.405 | Reg loss: 0.029 | Tree loss: 4.405 | Accuracy: 0.128906 | 0.07 sec/iter\n",
      "Epoch: 30 | Batch: 005 / 025 | Total loss: 4.404 | Reg loss: 0.029 | Tree loss: 4.404 | Accuracy: 0.166016 | 0.07 sec/iter\n",
      "Epoch: 30 | Batch: 006 / 025 | Total loss: 4.363 | Reg loss: 0.029 | Tree loss: 4.363 | Accuracy: 0.130859 | 0.07 sec/iter\n",
      "Epoch: 30 | Batch: 007 / 025 | Total loss: 4.356 | Reg loss: 0.029 | Tree loss: 4.356 | Accuracy: 0.111328 | 0.07 sec/iter\n",
      "Epoch: 30 | Batch: 008 / 025 | Total loss: 4.360 | Reg loss: 0.029 | Tree loss: 4.360 | Accuracy: 0.109375 | 0.07 sec/iter\n",
      "Epoch: 30 | Batch: 009 / 025 | Total loss: 4.270 | Reg loss: 0.029 | Tree loss: 4.270 | Accuracy: 0.150391 | 0.07 sec/iter\n",
      "Epoch: 30 | Batch: 010 / 025 | Total loss: 4.282 | Reg loss: 0.029 | Tree loss: 4.282 | Accuracy: 0.125000 | 0.07 sec/iter\n",
      "Epoch: 30 | Batch: 011 / 025 | Total loss: 4.299 | Reg loss: 0.029 | Tree loss: 4.299 | Accuracy: 0.123047 | 0.07 sec/iter\n",
      "Epoch: 30 | Batch: 012 / 025 | Total loss: 4.255 | Reg loss: 0.029 | Tree loss: 4.255 | Accuracy: 0.130859 | 0.07 sec/iter\n",
      "Epoch: 30 | Batch: 013 / 025 | Total loss: 4.261 | Reg loss: 0.029 | Tree loss: 4.261 | Accuracy: 0.130859 | 0.07 sec/iter\n",
      "Epoch: 30 | Batch: 014 / 025 | Total loss: 4.285 | Reg loss: 0.029 | Tree loss: 4.285 | Accuracy: 0.125000 | 0.07 sec/iter\n",
      "Epoch: 30 | Batch: 015 / 025 | Total loss: 4.219 | Reg loss: 0.029 | Tree loss: 4.219 | Accuracy: 0.105469 | 0.07 sec/iter\n",
      "Epoch: 30 | Batch: 016 / 025 | Total loss: 4.246 | Reg loss: 0.029 | Tree loss: 4.246 | Accuracy: 0.154297 | 0.07 sec/iter\n",
      "Epoch: 30 | Batch: 017 / 025 | Total loss: 4.170 | Reg loss: 0.029 | Tree loss: 4.170 | Accuracy: 0.126953 | 0.07 sec/iter\n",
      "Epoch: 30 | Batch: 018 / 025 | Total loss: 4.187 | Reg loss: 0.029 | Tree loss: 4.187 | Accuracy: 0.134766 | 0.07 sec/iter\n",
      "Epoch: 30 | Batch: 019 / 025 | Total loss: 4.136 | Reg loss: 0.029 | Tree loss: 4.136 | Accuracy: 0.138672 | 0.07 sec/iter\n",
      "Epoch: 30 | Batch: 020 / 025 | Total loss: 4.138 | Reg loss: 0.029 | Tree loss: 4.138 | Accuracy: 0.117188 | 0.07 sec/iter\n",
      "Epoch: 30 | Batch: 021 / 025 | Total loss: 4.139 | Reg loss: 0.029 | Tree loss: 4.139 | Accuracy: 0.123047 | 0.07 sec/iter\n",
      "Epoch: 30 | Batch: 022 / 025 | Total loss: 4.162 | Reg loss: 0.029 | Tree loss: 4.162 | Accuracy: 0.109375 | 0.07 sec/iter\n",
      "Epoch: 30 | Batch: 023 / 025 | Total loss: 4.072 | Reg loss: 0.030 | Tree loss: 4.072 | Accuracy: 0.119141 | 0.07 sec/iter\n",
      "Epoch: 30 | Batch: 024 / 025 | Total loss: 4.159 | Reg loss: 0.030 | Tree loss: 4.159 | Accuracy: 0.118750 | 0.07 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 31 | Batch: 000 / 025 | Total loss: 4.310 | Reg loss: 0.028 | Tree loss: 4.310 | Accuracy: 0.138672 | 0.07 sec/iter\n",
      "Epoch: 31 | Batch: 001 / 025 | Total loss: 4.349 | Reg loss: 0.028 | Tree loss: 4.349 | Accuracy: 0.095703 | 0.07 sec/iter\n",
      "Epoch: 31 | Batch: 002 / 025 | Total loss: 4.324 | Reg loss: 0.028 | Tree loss: 4.324 | Accuracy: 0.125000 | 0.07 sec/iter\n",
      "Epoch: 31 | Batch: 003 / 025 | Total loss: 4.305 | Reg loss: 0.028 | Tree loss: 4.305 | Accuracy: 0.121094 | 0.07 sec/iter\n",
      "Epoch: 31 | Batch: 004 / 025 | Total loss: 4.312 | Reg loss: 0.028 | Tree loss: 4.312 | Accuracy: 0.138672 | 0.07 sec/iter\n",
      "Epoch: 31 | Batch: 005 / 025 | Total loss: 4.243 | Reg loss: 0.028 | Tree loss: 4.243 | Accuracy: 0.119141 | 0.07 sec/iter\n",
      "Epoch: 31 | Batch: 006 / 025 | Total loss: 4.216 | Reg loss: 0.028 | Tree loss: 4.216 | Accuracy: 0.148438 | 0.07 sec/iter\n",
      "Epoch: 31 | Batch: 007 / 025 | Total loss: 4.206 | Reg loss: 0.028 | Tree loss: 4.206 | Accuracy: 0.132812 | 0.07 sec/iter\n",
      "Epoch: 31 | Batch: 008 / 025 | Total loss: 4.198 | Reg loss: 0.028 | Tree loss: 4.198 | Accuracy: 0.138672 | 0.07 sec/iter\n",
      "Epoch: 31 | Batch: 009 / 025 | Total loss: 4.209 | Reg loss: 0.028 | Tree loss: 4.209 | Accuracy: 0.134766 | 0.07 sec/iter\n",
      "Epoch: 31 | Batch: 010 / 025 | Total loss: 4.158 | Reg loss: 0.028 | Tree loss: 4.158 | Accuracy: 0.125000 | 0.07 sec/iter\n",
      "Epoch: 31 | Batch: 011 / 025 | Total loss: 4.101 | Reg loss: 0.028 | Tree loss: 4.101 | Accuracy: 0.152344 | 0.07 sec/iter\n",
      "Epoch: 31 | Batch: 012 / 025 | Total loss: 4.122 | Reg loss: 0.028 | Tree loss: 4.122 | Accuracy: 0.121094 | 0.07 sec/iter\n",
      "Epoch: 31 | Batch: 013 / 025 | Total loss: 4.071 | Reg loss: 0.028 | Tree loss: 4.071 | Accuracy: 0.146484 | 0.07 sec/iter\n",
      "Epoch: 31 | Batch: 014 / 025 | Total loss: 4.122 | Reg loss: 0.028 | Tree loss: 4.122 | Accuracy: 0.134766 | 0.07 sec/iter\n",
      "Epoch: 31 | Batch: 015 / 025 | Total loss: 4.053 | Reg loss: 0.028 | Tree loss: 4.053 | Accuracy: 0.115234 | 0.07 sec/iter\n",
      "Epoch: 31 | Batch: 016 / 025 | Total loss: 4.105 | Reg loss: 0.028 | Tree loss: 4.105 | Accuracy: 0.117188 | 0.07 sec/iter\n",
      "Epoch: 31 | Batch: 017 / 025 | Total loss: 4.005 | Reg loss: 0.028 | Tree loss: 4.005 | Accuracy: 0.128906 | 0.07 sec/iter\n",
      "Epoch: 31 | Batch: 018 / 025 | Total loss: 4.041 | Reg loss: 0.029 | Tree loss: 4.041 | Accuracy: 0.142578 | 0.07 sec/iter\n",
      "Epoch: 31 | Batch: 019 / 025 | Total loss: 3.984 | Reg loss: 0.029 | Tree loss: 3.984 | Accuracy: 0.132812 | 0.07 sec/iter\n",
      "Epoch: 31 | Batch: 020 / 025 | Total loss: 4.037 | Reg loss: 0.029 | Tree loss: 4.037 | Accuracy: 0.125000 | 0.07 sec/iter\n",
      "Epoch: 31 | Batch: 021 / 025 | Total loss: 3.956 | Reg loss: 0.029 | Tree loss: 3.956 | Accuracy: 0.123047 | 0.07 sec/iter\n",
      "Epoch: 31 | Batch: 022 / 025 | Total loss: 4.012 | Reg loss: 0.029 | Tree loss: 4.012 | Accuracy: 0.123047 | 0.07 sec/iter\n",
      "Epoch: 31 | Batch: 023 / 025 | Total loss: 4.013 | Reg loss: 0.029 | Tree loss: 4.013 | Accuracy: 0.111328 | 0.07 sec/iter\n",
      "Epoch: 31 | Batch: 024 / 025 | Total loss: 3.951 | Reg loss: 0.029 | Tree loss: 3.951 | Accuracy: 0.125000 | 0.07 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 32 | Batch: 000 / 025 | Total loss: 4.222 | Reg loss: 0.027 | Tree loss: 4.222 | Accuracy: 0.125000 | 0.07 sec/iter\n",
      "Epoch: 32 | Batch: 001 / 025 | Total loss: 4.215 | Reg loss: 0.027 | Tree loss: 4.215 | Accuracy: 0.140625 | 0.07 sec/iter\n",
      "Epoch: 32 | Batch: 002 / 025 | Total loss: 4.207 | Reg loss: 0.027 | Tree loss: 4.207 | Accuracy: 0.128906 | 0.07 sec/iter\n",
      "Epoch: 32 | Batch: 003 / 025 | Total loss: 4.169 | Reg loss: 0.027 | Tree loss: 4.169 | Accuracy: 0.105469 | 0.07 sec/iter\n",
      "Epoch: 32 | Batch: 004 / 025 | Total loss: 4.131 | Reg loss: 0.027 | Tree loss: 4.131 | Accuracy: 0.097656 | 0.07 sec/iter\n",
      "Epoch: 32 | Batch: 005 / 025 | Total loss: 4.142 | Reg loss: 0.027 | Tree loss: 4.142 | Accuracy: 0.107422 | 0.07 sec/iter\n",
      "Epoch: 32 | Batch: 006 / 025 | Total loss: 4.103 | Reg loss: 0.027 | Tree loss: 4.103 | Accuracy: 0.144531 | 0.07 sec/iter\n",
      "Epoch: 32 | Batch: 007 / 025 | Total loss: 4.045 | Reg loss: 0.027 | Tree loss: 4.045 | Accuracy: 0.130859 | 0.07 sec/iter\n",
      "Epoch: 32 | Batch: 008 / 025 | Total loss: 4.036 | Reg loss: 0.027 | Tree loss: 4.036 | Accuracy: 0.152344 | 0.07 sec/iter\n",
      "Epoch: 32 | Batch: 009 / 025 | Total loss: 4.044 | Reg loss: 0.027 | Tree loss: 4.044 | Accuracy: 0.136719 | 0.07 sec/iter\n",
      "Epoch: 32 | Batch: 010 / 025 | Total loss: 4.005 | Reg loss: 0.027 | Tree loss: 4.005 | Accuracy: 0.115234 | 0.07 sec/iter\n",
      "Epoch: 32 | Batch: 011 / 025 | Total loss: 3.969 | Reg loss: 0.027 | Tree loss: 3.969 | Accuracy: 0.140625 | 0.07 sec/iter\n",
      "Epoch: 32 | Batch: 012 / 025 | Total loss: 4.011 | Reg loss: 0.028 | Tree loss: 4.011 | Accuracy: 0.134766 | 0.07 sec/iter\n",
      "Epoch: 32 | Batch: 013 / 025 | Total loss: 3.970 | Reg loss: 0.028 | Tree loss: 3.970 | Accuracy: 0.128906 | 0.07 sec/iter\n",
      "Epoch: 32 | Batch: 014 / 025 | Total loss: 3.936 | Reg loss: 0.028 | Tree loss: 3.936 | Accuracy: 0.109375 | 0.07 sec/iter\n",
      "Epoch: 32 | Batch: 015 / 025 | Total loss: 3.972 | Reg loss: 0.028 | Tree loss: 3.972 | Accuracy: 0.134766 | 0.07 sec/iter\n",
      "Epoch: 32 | Batch: 016 / 025 | Total loss: 3.917 | Reg loss: 0.028 | Tree loss: 3.917 | Accuracy: 0.162109 | 0.07 sec/iter\n",
      "Epoch: 32 | Batch: 017 / 025 | Total loss: 3.891 | Reg loss: 0.028 | Tree loss: 3.891 | Accuracy: 0.101562 | 0.07 sec/iter\n",
      "Epoch: 32 | Batch: 018 / 025 | Total loss: 3.914 | Reg loss: 0.028 | Tree loss: 3.914 | Accuracy: 0.113281 | 0.07 sec/iter\n",
      "Epoch: 32 | Batch: 019 / 025 | Total loss: 3.840 | Reg loss: 0.028 | Tree loss: 3.840 | Accuracy: 0.142578 | 0.07 sec/iter\n",
      "Epoch: 32 | Batch: 020 / 025 | Total loss: 3.833 | Reg loss: 0.028 | Tree loss: 3.833 | Accuracy: 0.154297 | 0.07 sec/iter\n",
      "Epoch: 32 | Batch: 021 / 025 | Total loss: 3.794 | Reg loss: 0.028 | Tree loss: 3.794 | Accuracy: 0.156250 | 0.07 sec/iter\n",
      "Epoch: 32 | Batch: 022 / 025 | Total loss: 3.831 | Reg loss: 0.028 | Tree loss: 3.831 | Accuracy: 0.136719 | 0.07 sec/iter\n",
      "Epoch: 32 | Batch: 023 / 025 | Total loss: 3.865 | Reg loss: 0.028 | Tree loss: 3.865 | Accuracy: 0.105469 | 0.07 sec/iter\n",
      "Epoch: 32 | Batch: 024 / 025 | Total loss: 3.820 | Reg loss: 0.029 | Tree loss: 3.820 | Accuracy: 0.143750 | 0.071 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 33 | Batch: 000 / 025 | Total loss: 4.077 | Reg loss: 0.027 | Tree loss: 4.077 | Accuracy: 0.126953 | 0.071 sec/iter\n",
      "Epoch: 33 | Batch: 001 / 025 | Total loss: 4.050 | Reg loss: 0.026 | Tree loss: 4.050 | Accuracy: 0.125000 | 0.071 sec/iter\n",
      "Epoch: 33 | Batch: 002 / 025 | Total loss: 3.977 | Reg loss: 0.026 | Tree loss: 3.977 | Accuracy: 0.125000 | 0.071 sec/iter\n",
      "Epoch: 33 | Batch: 003 / 025 | Total loss: 4.067 | Reg loss: 0.026 | Tree loss: 4.067 | Accuracy: 0.115234 | 0.071 sec/iter\n",
      "Epoch: 33 | Batch: 004 / 025 | Total loss: 4.026 | Reg loss: 0.026 | Tree loss: 4.026 | Accuracy: 0.111328 | 0.071 sec/iter\n",
      "Epoch: 33 | Batch: 005 / 025 | Total loss: 3.975 | Reg loss: 0.026 | Tree loss: 3.975 | Accuracy: 0.126953 | 0.071 sec/iter\n",
      "Epoch: 33 | Batch: 006 / 025 | Total loss: 4.002 | Reg loss: 0.026 | Tree loss: 4.002 | Accuracy: 0.119141 | 0.071 sec/iter\n",
      "Epoch: 33 | Batch: 007 / 025 | Total loss: 3.930 | Reg loss: 0.027 | Tree loss: 3.930 | Accuracy: 0.140625 | 0.071 sec/iter\n",
      "Epoch: 33 | Batch: 008 / 025 | Total loss: 3.946 | Reg loss: 0.027 | Tree loss: 3.946 | Accuracy: 0.134766 | 0.071 sec/iter\n",
      "Epoch: 33 | Batch: 009 / 025 | Total loss: 3.941 | Reg loss: 0.027 | Tree loss: 3.941 | Accuracy: 0.140625 | 0.071 sec/iter\n",
      "Epoch: 33 | Batch: 010 / 025 | Total loss: 3.896 | Reg loss: 0.027 | Tree loss: 3.896 | Accuracy: 0.125000 | 0.071 sec/iter\n",
      "Epoch: 33 | Batch: 011 / 025 | Total loss: 3.886 | Reg loss: 0.027 | Tree loss: 3.886 | Accuracy: 0.125000 | 0.071 sec/iter\n",
      "Epoch: 33 | Batch: 012 / 025 | Total loss: 3.889 | Reg loss: 0.027 | Tree loss: 3.889 | Accuracy: 0.109375 | 0.071 sec/iter\n",
      "Epoch: 33 | Batch: 013 / 025 | Total loss: 3.852 | Reg loss: 0.027 | Tree loss: 3.852 | Accuracy: 0.099609 | 0.071 sec/iter\n",
      "Epoch: 33 | Batch: 014 / 025 | Total loss: 3.809 | Reg loss: 0.027 | Tree loss: 3.809 | Accuracy: 0.142578 | 0.071 sec/iter\n",
      "Epoch: 33 | Batch: 015 / 025 | Total loss: 3.810 | Reg loss: 0.027 | Tree loss: 3.810 | Accuracy: 0.138672 | 0.071 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 33 | Batch: 016 / 025 | Total loss: 3.790 | Reg loss: 0.027 | Tree loss: 3.790 | Accuracy: 0.140625 | 0.071 sec/iter\n",
      "Epoch: 33 | Batch: 017 / 025 | Total loss: 3.770 | Reg loss: 0.027 | Tree loss: 3.770 | Accuracy: 0.144531 | 0.071 sec/iter\n",
      "Epoch: 33 | Batch: 018 / 025 | Total loss: 3.777 | Reg loss: 0.027 | Tree loss: 3.777 | Accuracy: 0.140625 | 0.071 sec/iter\n",
      "Epoch: 33 | Batch: 019 / 025 | Total loss: 3.777 | Reg loss: 0.028 | Tree loss: 3.777 | Accuracy: 0.107422 | 0.071 sec/iter\n",
      "Epoch: 33 | Batch: 020 / 025 | Total loss: 3.612 | Reg loss: 0.028 | Tree loss: 3.612 | Accuracy: 0.171875 | 0.071 sec/iter\n",
      "Epoch: 33 | Batch: 021 / 025 | Total loss: 3.677 | Reg loss: 0.028 | Tree loss: 3.677 | Accuracy: 0.132812 | 0.071 sec/iter\n",
      "Epoch: 33 | Batch: 022 / 025 | Total loss: 3.696 | Reg loss: 0.028 | Tree loss: 3.696 | Accuracy: 0.128906 | 0.071 sec/iter\n",
      "Epoch: 33 | Batch: 023 / 025 | Total loss: 3.643 | Reg loss: 0.028 | Tree loss: 3.643 | Accuracy: 0.160156 | 0.071 sec/iter\n",
      "Epoch: 33 | Batch: 024 / 025 | Total loss: 3.671 | Reg loss: 0.028 | Tree loss: 3.671 | Accuracy: 0.146875 | 0.071 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 34 | Batch: 000 / 025 | Total loss: 3.887 | Reg loss: 0.026 | Tree loss: 3.887 | Accuracy: 0.164062 | 0.071 sec/iter\n",
      "Epoch: 34 | Batch: 001 / 025 | Total loss: 3.942 | Reg loss: 0.026 | Tree loss: 3.942 | Accuracy: 0.123047 | 0.071 sec/iter\n",
      "Epoch: 34 | Batch: 002 / 025 | Total loss: 3.931 | Reg loss: 0.026 | Tree loss: 3.931 | Accuracy: 0.103516 | 0.071 sec/iter\n",
      "Epoch: 34 | Batch: 003 / 025 | Total loss: 3.905 | Reg loss: 0.026 | Tree loss: 3.905 | Accuracy: 0.128906 | 0.071 sec/iter\n",
      "Epoch: 34 | Batch: 004 / 025 | Total loss: 3.847 | Reg loss: 0.026 | Tree loss: 3.847 | Accuracy: 0.134766 | 0.071 sec/iter\n",
      "Epoch: 34 | Batch: 005 / 025 | Total loss: 3.865 | Reg loss: 0.026 | Tree loss: 3.865 | Accuracy: 0.126953 | 0.071 sec/iter\n",
      "Epoch: 34 | Batch: 006 / 025 | Total loss: 3.809 | Reg loss: 0.026 | Tree loss: 3.809 | Accuracy: 0.134766 | 0.071 sec/iter\n",
      "Epoch: 34 | Batch: 007 / 025 | Total loss: 3.747 | Reg loss: 0.026 | Tree loss: 3.747 | Accuracy: 0.154297 | 0.071 sec/iter\n",
      "Epoch: 34 | Batch: 008 / 025 | Total loss: 3.830 | Reg loss: 0.026 | Tree loss: 3.830 | Accuracy: 0.136719 | 0.071 sec/iter\n",
      "Epoch: 34 | Batch: 009 / 025 | Total loss: 3.769 | Reg loss: 0.026 | Tree loss: 3.769 | Accuracy: 0.130859 | 0.071 sec/iter\n",
      "Epoch: 34 | Batch: 010 / 025 | Total loss: 3.757 | Reg loss: 0.026 | Tree loss: 3.757 | Accuracy: 0.144531 | 0.071 sec/iter\n",
      "Epoch: 34 | Batch: 011 / 025 | Total loss: 3.748 | Reg loss: 0.026 | Tree loss: 3.748 | Accuracy: 0.152344 | 0.071 sec/iter\n",
      "Epoch: 34 | Batch: 012 / 025 | Total loss: 3.719 | Reg loss: 0.026 | Tree loss: 3.719 | Accuracy: 0.123047 | 0.071 sec/iter\n",
      "Epoch: 34 | Batch: 013 / 025 | Total loss: 3.761 | Reg loss: 0.026 | Tree loss: 3.761 | Accuracy: 0.130859 | 0.071 sec/iter\n",
      "Epoch: 34 | Batch: 014 / 025 | Total loss: 3.705 | Reg loss: 0.027 | Tree loss: 3.705 | Accuracy: 0.134766 | 0.071 sec/iter\n",
      "Epoch: 34 | Batch: 015 / 025 | Total loss: 3.669 | Reg loss: 0.027 | Tree loss: 3.669 | Accuracy: 0.130859 | 0.071 sec/iter\n",
      "Epoch: 34 | Batch: 016 / 025 | Total loss: 3.666 | Reg loss: 0.027 | Tree loss: 3.666 | Accuracy: 0.138672 | 0.071 sec/iter\n",
      "Epoch: 34 | Batch: 017 / 025 | Total loss: 3.605 | Reg loss: 0.027 | Tree loss: 3.605 | Accuracy: 0.136719 | 0.071 sec/iter\n",
      "Epoch: 34 | Batch: 018 / 025 | Total loss: 3.659 | Reg loss: 0.027 | Tree loss: 3.659 | Accuracy: 0.109375 | 0.071 sec/iter\n",
      "Epoch: 34 | Batch: 019 / 025 | Total loss: 3.674 | Reg loss: 0.027 | Tree loss: 3.674 | Accuracy: 0.113281 | 0.071 sec/iter\n",
      "Epoch: 34 | Batch: 020 / 025 | Total loss: 3.621 | Reg loss: 0.027 | Tree loss: 3.621 | Accuracy: 0.142578 | 0.071 sec/iter\n",
      "Epoch: 34 | Batch: 021 / 025 | Total loss: 3.602 | Reg loss: 0.028 | Tree loss: 3.602 | Accuracy: 0.115234 | 0.071 sec/iter\n",
      "Epoch: 34 | Batch: 022 / 025 | Total loss: 3.563 | Reg loss: 0.028 | Tree loss: 3.563 | Accuracy: 0.117188 | 0.071 sec/iter\n",
      "Epoch: 34 | Batch: 023 / 025 | Total loss: 3.556 | Reg loss: 0.028 | Tree loss: 3.556 | Accuracy: 0.111328 | 0.071 sec/iter\n",
      "Epoch: 34 | Batch: 024 / 025 | Total loss: 3.537 | Reg loss: 0.028 | Tree loss: 3.537 | Accuracy: 0.106250 | 0.071 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 35 | Batch: 000 / 025 | Total loss: 3.874 | Reg loss: 0.025 | Tree loss: 3.874 | Accuracy: 0.109375 | 0.071 sec/iter\n",
      "Epoch: 35 | Batch: 001 / 025 | Total loss: 3.801 | Reg loss: 0.025 | Tree loss: 3.801 | Accuracy: 0.123047 | 0.071 sec/iter\n",
      "Epoch: 35 | Batch: 002 / 025 | Total loss: 3.797 | Reg loss: 0.025 | Tree loss: 3.797 | Accuracy: 0.125000 | 0.071 sec/iter\n",
      "Epoch: 35 | Batch: 003 / 025 | Total loss: 3.750 | Reg loss: 0.025 | Tree loss: 3.750 | Accuracy: 0.152344 | 0.071 sec/iter\n",
      "Epoch: 35 | Batch: 004 / 025 | Total loss: 3.682 | Reg loss: 0.025 | Tree loss: 3.682 | Accuracy: 0.134766 | 0.071 sec/iter\n",
      "Epoch: 35 | Batch: 005 / 025 | Total loss: 3.727 | Reg loss: 0.025 | Tree loss: 3.727 | Accuracy: 0.117188 | 0.071 sec/iter\n",
      "Epoch: 35 | Batch: 006 / 025 | Total loss: 3.730 | Reg loss: 0.025 | Tree loss: 3.730 | Accuracy: 0.134766 | 0.071 sec/iter\n",
      "Epoch: 35 | Batch: 007 / 025 | Total loss: 3.636 | Reg loss: 0.025 | Tree loss: 3.636 | Accuracy: 0.166016 | 0.071 sec/iter\n",
      "Epoch: 35 | Batch: 008 / 025 | Total loss: 3.672 | Reg loss: 0.026 | Tree loss: 3.672 | Accuracy: 0.162109 | 0.071 sec/iter\n",
      "Epoch: 35 | Batch: 009 / 025 | Total loss: 3.692 | Reg loss: 0.026 | Tree loss: 3.692 | Accuracy: 0.123047 | 0.071 sec/iter\n",
      "Epoch: 35 | Batch: 010 / 025 | Total loss: 3.688 | Reg loss: 0.026 | Tree loss: 3.688 | Accuracy: 0.105469 | 0.071 sec/iter\n",
      "Epoch: 35 | Batch: 011 / 025 | Total loss: 3.602 | Reg loss: 0.026 | Tree loss: 3.602 | Accuracy: 0.130859 | 0.071 sec/iter\n",
      "Epoch: 35 | Batch: 012 / 025 | Total loss: 3.614 | Reg loss: 0.026 | Tree loss: 3.614 | Accuracy: 0.134766 | 0.071 sec/iter\n",
      "Epoch: 35 | Batch: 013 / 025 | Total loss: 3.590 | Reg loss: 0.026 | Tree loss: 3.590 | Accuracy: 0.123047 | 0.071 sec/iter\n",
      "Epoch: 35 | Batch: 014 / 025 | Total loss: 3.563 | Reg loss: 0.026 | Tree loss: 3.563 | Accuracy: 0.119141 | 0.071 sec/iter\n",
      "Epoch: 35 | Batch: 015 / 025 | Total loss: 3.511 | Reg loss: 0.026 | Tree loss: 3.511 | Accuracy: 0.158203 | 0.071 sec/iter\n",
      "Epoch: 35 | Batch: 016 / 025 | Total loss: 3.536 | Reg loss: 0.027 | Tree loss: 3.536 | Accuracy: 0.117188 | 0.071 sec/iter\n",
      "Epoch: 35 | Batch: 017 / 025 | Total loss: 3.501 | Reg loss: 0.027 | Tree loss: 3.501 | Accuracy: 0.138672 | 0.071 sec/iter\n",
      "Epoch: 35 | Batch: 018 / 025 | Total loss: 3.514 | Reg loss: 0.027 | Tree loss: 3.514 | Accuracy: 0.117188 | 0.071 sec/iter\n",
      "Epoch: 35 | Batch: 019 / 025 | Total loss: 3.523 | Reg loss: 0.027 | Tree loss: 3.523 | Accuracy: 0.130859 | 0.071 sec/iter\n",
      "Epoch: 35 | Batch: 020 / 025 | Total loss: 3.519 | Reg loss: 0.027 | Tree loss: 3.519 | Accuracy: 0.121094 | 0.071 sec/iter\n",
      "Epoch: 35 | Batch: 021 / 025 | Total loss: 3.464 | Reg loss: 0.027 | Tree loss: 3.464 | Accuracy: 0.138672 | 0.071 sec/iter\n",
      "Epoch: 35 | Batch: 022 / 025 | Total loss: 3.514 | Reg loss: 0.028 | Tree loss: 3.514 | Accuracy: 0.134766 | 0.071 sec/iter\n",
      "Epoch: 35 | Batch: 023 / 025 | Total loss: 3.456 | Reg loss: 0.028 | Tree loss: 3.456 | Accuracy: 0.134766 | 0.071 sec/iter\n",
      "Epoch: 35 | Batch: 024 / 025 | Total loss: 3.456 | Reg loss: 0.028 | Tree loss: 3.456 | Accuracy: 0.131250 | 0.071 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 36 | Batch: 000 / 025 | Total loss: 3.676 | Reg loss: 0.025 | Tree loss: 3.676 | Accuracy: 0.160156 | 0.071 sec/iter\n",
      "Epoch: 36 | Batch: 001 / 025 | Total loss: 3.660 | Reg loss: 0.025 | Tree loss: 3.660 | Accuracy: 0.156250 | 0.071 sec/iter\n",
      "Epoch: 36 | Batch: 002 / 025 | Total loss: 3.654 | Reg loss: 0.025 | Tree loss: 3.654 | Accuracy: 0.119141 | 0.071 sec/iter\n",
      "Epoch: 36 | Batch: 003 / 025 | Total loss: 3.627 | Reg loss: 0.025 | Tree loss: 3.627 | Accuracy: 0.138672 | 0.071 sec/iter\n",
      "Epoch: 36 | Batch: 004 / 025 | Total loss: 3.611 | Reg loss: 0.025 | Tree loss: 3.611 | Accuracy: 0.128906 | 0.071 sec/iter\n",
      "Epoch: 36 | Batch: 005 / 025 | Total loss: 3.616 | Reg loss: 0.025 | Tree loss: 3.616 | Accuracy: 0.117188 | 0.071 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 36 | Batch: 006 / 025 | Total loss: 3.647 | Reg loss: 0.025 | Tree loss: 3.647 | Accuracy: 0.123047 | 0.071 sec/iter\n",
      "Epoch: 36 | Batch: 007 / 025 | Total loss: 3.575 | Reg loss: 0.026 | Tree loss: 3.575 | Accuracy: 0.105469 | 0.071 sec/iter\n",
      "Epoch: 36 | Batch: 008 / 025 | Total loss: 3.577 | Reg loss: 0.026 | Tree loss: 3.577 | Accuracy: 0.138672 | 0.071 sec/iter\n",
      "Epoch: 36 | Batch: 009 / 025 | Total loss: 3.510 | Reg loss: 0.026 | Tree loss: 3.510 | Accuracy: 0.148438 | 0.071 sec/iter\n",
      "Epoch: 36 | Batch: 010 / 025 | Total loss: 3.548 | Reg loss: 0.026 | Tree loss: 3.548 | Accuracy: 0.152344 | 0.071 sec/iter\n",
      "Epoch: 36 | Batch: 011 / 025 | Total loss: 3.514 | Reg loss: 0.026 | Tree loss: 3.514 | Accuracy: 0.121094 | 0.071 sec/iter\n",
      "Epoch: 36 | Batch: 012 / 025 | Total loss: 3.458 | Reg loss: 0.026 | Tree loss: 3.458 | Accuracy: 0.132812 | 0.071 sec/iter\n",
      "Epoch: 36 | Batch: 013 / 025 | Total loss: 3.416 | Reg loss: 0.026 | Tree loss: 3.416 | Accuracy: 0.126953 | 0.071 sec/iter\n",
      "Epoch: 36 | Batch: 014 / 025 | Total loss: 3.460 | Reg loss: 0.027 | Tree loss: 3.460 | Accuracy: 0.126953 | 0.071 sec/iter\n",
      "Epoch: 36 | Batch: 015 / 025 | Total loss: 3.473 | Reg loss: 0.027 | Tree loss: 3.473 | Accuracy: 0.146484 | 0.071 sec/iter\n",
      "Epoch: 36 | Batch: 016 / 025 | Total loss: 3.430 | Reg loss: 0.027 | Tree loss: 3.430 | Accuracy: 0.123047 | 0.071 sec/iter\n",
      "Epoch: 36 | Batch: 017 / 025 | Total loss: 3.405 | Reg loss: 0.027 | Tree loss: 3.405 | Accuracy: 0.148438 | 0.071 sec/iter\n",
      "Epoch: 36 | Batch: 018 / 025 | Total loss: 3.383 | Reg loss: 0.027 | Tree loss: 3.383 | Accuracy: 0.125000 | 0.071 sec/iter\n",
      "Epoch: 36 | Batch: 019 / 025 | Total loss: 3.382 | Reg loss: 0.027 | Tree loss: 3.382 | Accuracy: 0.134766 | 0.071 sec/iter\n",
      "Epoch: 36 | Batch: 020 / 025 | Total loss: 3.432 | Reg loss: 0.028 | Tree loss: 3.432 | Accuracy: 0.128906 | 0.071 sec/iter\n",
      "Epoch: 36 | Batch: 021 / 025 | Total loss: 3.328 | Reg loss: 0.028 | Tree loss: 3.328 | Accuracy: 0.156250 | 0.071 sec/iter\n",
      "Epoch: 36 | Batch: 022 / 025 | Total loss: 3.351 | Reg loss: 0.028 | Tree loss: 3.351 | Accuracy: 0.119141 | 0.071 sec/iter\n",
      "Epoch: 36 | Batch: 023 / 025 | Total loss: 3.375 | Reg loss: 0.028 | Tree loss: 3.375 | Accuracy: 0.123047 | 0.071 sec/iter\n",
      "Epoch: 36 | Batch: 024 / 025 | Total loss: 3.362 | Reg loss: 0.028 | Tree loss: 3.362 | Accuracy: 0.081250 | 0.071 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 37 | Batch: 000 / 025 | Total loss: 3.650 | Reg loss: 0.026 | Tree loss: 3.650 | Accuracy: 0.123047 | 0.071 sec/iter\n",
      "Epoch: 37 | Batch: 001 / 025 | Total loss: 3.600 | Reg loss: 0.026 | Tree loss: 3.600 | Accuracy: 0.095703 | 0.071 sec/iter\n",
      "Epoch: 37 | Batch: 002 / 025 | Total loss: 3.554 | Reg loss: 0.026 | Tree loss: 3.554 | Accuracy: 0.142578 | 0.071 sec/iter\n",
      "Epoch: 37 | Batch: 003 / 025 | Total loss: 3.514 | Reg loss: 0.026 | Tree loss: 3.514 | Accuracy: 0.134766 | 0.071 sec/iter\n",
      "Epoch: 37 | Batch: 004 / 025 | Total loss: 3.469 | Reg loss: 0.026 | Tree loss: 3.469 | Accuracy: 0.140625 | 0.071 sec/iter\n",
      "Epoch: 37 | Batch: 005 / 025 | Total loss: 3.498 | Reg loss: 0.026 | Tree loss: 3.498 | Accuracy: 0.144531 | 0.071 sec/iter\n",
      "Epoch: 37 | Batch: 006 / 025 | Total loss: 3.454 | Reg loss: 0.026 | Tree loss: 3.454 | Accuracy: 0.138672 | 0.071 sec/iter\n",
      "Epoch: 37 | Batch: 007 / 025 | Total loss: 3.519 | Reg loss: 0.026 | Tree loss: 3.519 | Accuracy: 0.134766 | 0.071 sec/iter\n",
      "Epoch: 37 | Batch: 008 / 025 | Total loss: 3.489 | Reg loss: 0.026 | Tree loss: 3.489 | Accuracy: 0.107422 | 0.071 sec/iter\n",
      "Epoch: 37 | Batch: 009 / 025 | Total loss: 3.440 | Reg loss: 0.026 | Tree loss: 3.440 | Accuracy: 0.138672 | 0.071 sec/iter\n",
      "Epoch: 37 | Batch: 010 / 025 | Total loss: 3.404 | Reg loss: 0.026 | Tree loss: 3.404 | Accuracy: 0.136719 | 0.071 sec/iter\n",
      "Epoch: 37 | Batch: 011 / 025 | Total loss: 3.384 | Reg loss: 0.026 | Tree loss: 3.384 | Accuracy: 0.138672 | 0.071 sec/iter\n",
      "Epoch: 37 | Batch: 012 / 025 | Total loss: 3.418 | Reg loss: 0.027 | Tree loss: 3.418 | Accuracy: 0.128906 | 0.071 sec/iter\n",
      "Epoch: 37 | Batch: 013 / 025 | Total loss: 3.371 | Reg loss: 0.027 | Tree loss: 3.371 | Accuracy: 0.150391 | 0.071 sec/iter\n",
      "Epoch: 37 | Batch: 014 / 025 | Total loss: 3.407 | Reg loss: 0.027 | Tree loss: 3.407 | Accuracy: 0.111328 | 0.071 sec/iter\n",
      "Epoch: 37 | Batch: 015 / 025 | Total loss: 3.367 | Reg loss: 0.027 | Tree loss: 3.367 | Accuracy: 0.136719 | 0.071 sec/iter\n",
      "Epoch: 37 | Batch: 016 / 025 | Total loss: 3.384 | Reg loss: 0.027 | Tree loss: 3.384 | Accuracy: 0.150391 | 0.071 sec/iter\n",
      "Epoch: 37 | Batch: 017 / 025 | Total loss: 3.339 | Reg loss: 0.027 | Tree loss: 3.339 | Accuracy: 0.136719 | 0.071 sec/iter\n",
      "Epoch: 37 | Batch: 018 / 025 | Total loss: 3.302 | Reg loss: 0.027 | Tree loss: 3.302 | Accuracy: 0.160156 | 0.071 sec/iter\n",
      "Epoch: 37 | Batch: 019 / 025 | Total loss: 3.301 | Reg loss: 0.028 | Tree loss: 3.301 | Accuracy: 0.113281 | 0.071 sec/iter\n",
      "Epoch: 37 | Batch: 020 / 025 | Total loss: 3.239 | Reg loss: 0.028 | Tree loss: 3.239 | Accuracy: 0.146484 | 0.071 sec/iter\n",
      "Epoch: 37 | Batch: 021 / 025 | Total loss: 3.280 | Reg loss: 0.028 | Tree loss: 3.280 | Accuracy: 0.140625 | 0.071 sec/iter\n",
      "Epoch: 37 | Batch: 022 / 025 | Total loss: 3.272 | Reg loss: 0.028 | Tree loss: 3.272 | Accuracy: 0.113281 | 0.071 sec/iter\n",
      "Epoch: 37 | Batch: 023 / 025 | Total loss: 3.297 | Reg loss: 0.028 | Tree loss: 3.297 | Accuracy: 0.125000 | 0.071 sec/iter\n",
      "Epoch: 37 | Batch: 024 / 025 | Total loss: 3.301 | Reg loss: 0.028 | Tree loss: 3.301 | Accuracy: 0.131250 | 0.071 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 38 | Batch: 000 / 025 | Total loss: 3.496 | Reg loss: 0.026 | Tree loss: 3.496 | Accuracy: 0.144531 | 0.071 sec/iter\n",
      "Epoch: 38 | Batch: 001 / 025 | Total loss: 3.475 | Reg loss: 0.026 | Tree loss: 3.475 | Accuracy: 0.121094 | 0.071 sec/iter\n",
      "Epoch: 38 | Batch: 002 / 025 | Total loss: 3.518 | Reg loss: 0.026 | Tree loss: 3.518 | Accuracy: 0.130859 | 0.071 sec/iter\n",
      "Epoch: 38 | Batch: 003 / 025 | Total loss: 3.464 | Reg loss: 0.026 | Tree loss: 3.464 | Accuracy: 0.130859 | 0.071 sec/iter\n",
      "Epoch: 38 | Batch: 004 / 025 | Total loss: 3.402 | Reg loss: 0.026 | Tree loss: 3.402 | Accuracy: 0.119141 | 0.071 sec/iter\n",
      "Epoch: 38 | Batch: 005 / 025 | Total loss: 3.430 | Reg loss: 0.026 | Tree loss: 3.430 | Accuracy: 0.148438 | 0.071 sec/iter\n",
      "Epoch: 38 | Batch: 006 / 025 | Total loss: 3.403 | Reg loss: 0.026 | Tree loss: 3.403 | Accuracy: 0.136719 | 0.071 sec/iter\n",
      "Epoch: 38 | Batch: 007 / 025 | Total loss: 3.372 | Reg loss: 0.026 | Tree loss: 3.372 | Accuracy: 0.125000 | 0.071 sec/iter\n",
      "Epoch: 38 | Batch: 008 / 025 | Total loss: 3.377 | Reg loss: 0.026 | Tree loss: 3.377 | Accuracy: 0.111328 | 0.071 sec/iter\n",
      "Epoch: 38 | Batch: 009 / 025 | Total loss: 3.327 | Reg loss: 0.027 | Tree loss: 3.327 | Accuracy: 0.144531 | 0.071 sec/iter\n",
      "Epoch: 38 | Batch: 010 / 025 | Total loss: 3.353 | Reg loss: 0.027 | Tree loss: 3.353 | Accuracy: 0.132812 | 0.071 sec/iter\n",
      "Epoch: 38 | Batch: 011 / 025 | Total loss: 3.325 | Reg loss: 0.027 | Tree loss: 3.325 | Accuracy: 0.111328 | 0.071 sec/iter\n",
      "Epoch: 38 | Batch: 012 / 025 | Total loss: 3.324 | Reg loss: 0.027 | Tree loss: 3.324 | Accuracy: 0.123047 | 0.071 sec/iter\n",
      "Epoch: 38 | Batch: 013 / 025 | Total loss: 3.290 | Reg loss: 0.027 | Tree loss: 3.290 | Accuracy: 0.138672 | 0.071 sec/iter\n",
      "Epoch: 38 | Batch: 014 / 025 | Total loss: 3.330 | Reg loss: 0.027 | Tree loss: 3.330 | Accuracy: 0.132812 | 0.071 sec/iter\n",
      "Epoch: 38 | Batch: 015 / 025 | Total loss: 3.236 | Reg loss: 0.027 | Tree loss: 3.236 | Accuracy: 0.142578 | 0.071 sec/iter\n",
      "Epoch: 38 | Batch: 016 / 025 | Total loss: 3.222 | Reg loss: 0.027 | Tree loss: 3.222 | Accuracy: 0.160156 | 0.071 sec/iter\n",
      "Epoch: 38 | Batch: 017 / 025 | Total loss: 3.218 | Reg loss: 0.028 | Tree loss: 3.218 | Accuracy: 0.140625 | 0.071 sec/iter\n",
      "Epoch: 38 | Batch: 018 / 025 | Total loss: 3.255 | Reg loss: 0.028 | Tree loss: 3.255 | Accuracy: 0.150391 | 0.071 sec/iter\n",
      "Epoch: 38 | Batch: 019 / 025 | Total loss: 3.223 | Reg loss: 0.028 | Tree loss: 3.223 | Accuracy: 0.111328 | 0.071 sec/iter\n",
      "Epoch: 38 | Batch: 020 / 025 | Total loss: 3.260 | Reg loss: 0.028 | Tree loss: 3.260 | Accuracy: 0.142578 | 0.071 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 38 | Batch: 021 / 025 | Total loss: 3.222 | Reg loss: 0.028 | Tree loss: 3.222 | Accuracy: 0.109375 | 0.071 sec/iter\n",
      "Epoch: 38 | Batch: 022 / 025 | Total loss: 3.263 | Reg loss: 0.028 | Tree loss: 3.263 | Accuracy: 0.123047 | 0.071 sec/iter\n",
      "Epoch: 38 | Batch: 023 / 025 | Total loss: 3.218 | Reg loss: 0.028 | Tree loss: 3.218 | Accuracy: 0.150391 | 0.071 sec/iter\n",
      "Epoch: 38 | Batch: 024 / 025 | Total loss: 3.179 | Reg loss: 0.029 | Tree loss: 3.179 | Accuracy: 0.128125 | 0.071 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 39 | Batch: 000 / 025 | Total loss: 3.452 | Reg loss: 0.026 | Tree loss: 3.452 | Accuracy: 0.126953 | 0.071 sec/iter\n",
      "Epoch: 39 | Batch: 001 / 025 | Total loss: 3.416 | Reg loss: 0.026 | Tree loss: 3.416 | Accuracy: 0.144531 | 0.071 sec/iter\n",
      "Epoch: 39 | Batch: 002 / 025 | Total loss: 3.366 | Reg loss: 0.026 | Tree loss: 3.366 | Accuracy: 0.134766 | 0.071 sec/iter\n",
      "Epoch: 39 | Batch: 003 / 025 | Total loss: 3.388 | Reg loss: 0.027 | Tree loss: 3.388 | Accuracy: 0.119141 | 0.071 sec/iter\n",
      "Epoch: 39 | Batch: 004 / 025 | Total loss: 3.371 | Reg loss: 0.027 | Tree loss: 3.371 | Accuracy: 0.109375 | 0.071 sec/iter\n",
      "Epoch: 39 | Batch: 005 / 025 | Total loss: 3.398 | Reg loss: 0.027 | Tree loss: 3.398 | Accuracy: 0.138672 | 0.071 sec/iter\n",
      "Epoch: 39 | Batch: 006 / 025 | Total loss: 3.322 | Reg loss: 0.027 | Tree loss: 3.322 | Accuracy: 0.158203 | 0.071 sec/iter\n",
      "Epoch: 39 | Batch: 007 / 025 | Total loss: 3.291 | Reg loss: 0.027 | Tree loss: 3.291 | Accuracy: 0.138672 | 0.071 sec/iter\n",
      "Epoch: 39 | Batch: 008 / 025 | Total loss: 3.354 | Reg loss: 0.027 | Tree loss: 3.354 | Accuracy: 0.105469 | 0.071 sec/iter\n",
      "Epoch: 39 | Batch: 009 / 025 | Total loss: 3.299 | Reg loss: 0.027 | Tree loss: 3.299 | Accuracy: 0.132812 | 0.071 sec/iter\n",
      "Epoch: 39 | Batch: 010 / 025 | Total loss: 3.267 | Reg loss: 0.027 | Tree loss: 3.267 | Accuracy: 0.158203 | 0.071 sec/iter\n",
      "Epoch: 39 | Batch: 011 / 025 | Total loss: 3.265 | Reg loss: 0.027 | Tree loss: 3.265 | Accuracy: 0.144531 | 0.071 sec/iter\n",
      "Epoch: 39 | Batch: 012 / 025 | Total loss: 3.238 | Reg loss: 0.027 | Tree loss: 3.238 | Accuracy: 0.154297 | 0.071 sec/iter\n",
      "Epoch: 39 | Batch: 013 / 025 | Total loss: 3.224 | Reg loss: 0.027 | Tree loss: 3.224 | Accuracy: 0.140625 | 0.071 sec/iter\n",
      "Epoch: 39 | Batch: 014 / 025 | Total loss: 3.217 | Reg loss: 0.028 | Tree loss: 3.217 | Accuracy: 0.119141 | 0.071 sec/iter\n",
      "Epoch: 39 | Batch: 015 / 025 | Total loss: 3.209 | Reg loss: 0.028 | Tree loss: 3.209 | Accuracy: 0.115234 | 0.071 sec/iter\n",
      "Epoch: 39 | Batch: 016 / 025 | Total loss: 3.212 | Reg loss: 0.028 | Tree loss: 3.212 | Accuracy: 0.115234 | 0.071 sec/iter\n",
      "Epoch: 39 | Batch: 017 / 025 | Total loss: 3.160 | Reg loss: 0.028 | Tree loss: 3.160 | Accuracy: 0.134766 | 0.071 sec/iter\n",
      "Epoch: 39 | Batch: 018 / 025 | Total loss: 3.164 | Reg loss: 0.028 | Tree loss: 3.164 | Accuracy: 0.140625 | 0.071 sec/iter\n",
      "Epoch: 39 | Batch: 019 / 025 | Total loss: 3.158 | Reg loss: 0.028 | Tree loss: 3.158 | Accuracy: 0.125000 | 0.071 sec/iter\n",
      "Epoch: 39 | Batch: 020 / 025 | Total loss: 3.117 | Reg loss: 0.028 | Tree loss: 3.117 | Accuracy: 0.130859 | 0.071 sec/iter\n",
      "Epoch: 39 | Batch: 021 / 025 | Total loss: 3.146 | Reg loss: 0.028 | Tree loss: 3.146 | Accuracy: 0.138672 | 0.071 sec/iter\n",
      "Epoch: 39 | Batch: 022 / 025 | Total loss: 3.100 | Reg loss: 0.029 | Tree loss: 3.100 | Accuracy: 0.158203 | 0.071 sec/iter\n",
      "Epoch: 39 | Batch: 023 / 025 | Total loss: 3.126 | Reg loss: 0.029 | Tree loss: 3.126 | Accuracy: 0.113281 | 0.071 sec/iter\n",
      "Epoch: 39 | Batch: 024 / 025 | Total loss: 3.049 | Reg loss: 0.029 | Tree loss: 3.049 | Accuracy: 0.109375 | 0.07 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 40 | Batch: 000 / 025 | Total loss: 3.347 | Reg loss: 0.027 | Tree loss: 3.347 | Accuracy: 0.138672 | 0.071 sec/iter\n",
      "Epoch: 40 | Batch: 001 / 025 | Total loss: 3.345 | Reg loss: 0.027 | Tree loss: 3.345 | Accuracy: 0.152344 | 0.071 sec/iter\n",
      "Epoch: 40 | Batch: 002 / 025 | Total loss: 3.310 | Reg loss: 0.027 | Tree loss: 3.310 | Accuracy: 0.136719 | 0.071 sec/iter\n",
      "Epoch: 40 | Batch: 003 / 025 | Total loss: 3.303 | Reg loss: 0.027 | Tree loss: 3.303 | Accuracy: 0.128906 | 0.071 sec/iter\n",
      "Epoch: 40 | Batch: 004 / 025 | Total loss: 3.272 | Reg loss: 0.027 | Tree loss: 3.272 | Accuracy: 0.146484 | 0.071 sec/iter\n",
      "Epoch: 40 | Batch: 005 / 025 | Total loss: 3.276 | Reg loss: 0.027 | Tree loss: 3.276 | Accuracy: 0.111328 | 0.071 sec/iter\n",
      "Epoch: 40 | Batch: 006 / 025 | Total loss: 3.296 | Reg loss: 0.027 | Tree loss: 3.296 | Accuracy: 0.115234 | 0.071 sec/iter\n",
      "Epoch: 40 | Batch: 007 / 025 | Total loss: 3.244 | Reg loss: 0.027 | Tree loss: 3.244 | Accuracy: 0.125000 | 0.071 sec/iter\n",
      "Epoch: 40 | Batch: 008 / 025 | Total loss: 3.240 | Reg loss: 0.027 | Tree loss: 3.240 | Accuracy: 0.166016 | 0.071 sec/iter\n",
      "Epoch: 40 | Batch: 009 / 025 | Total loss: 3.222 | Reg loss: 0.027 | Tree loss: 3.222 | Accuracy: 0.134766 | 0.071 sec/iter\n",
      "Epoch: 40 | Batch: 010 / 025 | Total loss: 3.200 | Reg loss: 0.027 | Tree loss: 3.200 | Accuracy: 0.128906 | 0.071 sec/iter\n",
      "Epoch: 40 | Batch: 011 / 025 | Total loss: 3.224 | Reg loss: 0.027 | Tree loss: 3.224 | Accuracy: 0.123047 | 0.071 sec/iter\n",
      "Epoch: 40 | Batch: 012 / 025 | Total loss: 3.192 | Reg loss: 0.028 | Tree loss: 3.192 | Accuracy: 0.130859 | 0.071 sec/iter\n",
      "Epoch: 40 | Batch: 013 / 025 | Total loss: 3.133 | Reg loss: 0.028 | Tree loss: 3.133 | Accuracy: 0.134766 | 0.071 sec/iter\n",
      "Epoch: 40 | Batch: 014 / 025 | Total loss: 3.159 | Reg loss: 0.028 | Tree loss: 3.159 | Accuracy: 0.146484 | 0.071 sec/iter\n",
      "Epoch: 40 | Batch: 015 / 025 | Total loss: 3.114 | Reg loss: 0.028 | Tree loss: 3.114 | Accuracy: 0.132812 | 0.071 sec/iter\n",
      "Epoch: 40 | Batch: 016 / 025 | Total loss: 3.143 | Reg loss: 0.028 | Tree loss: 3.143 | Accuracy: 0.121094 | 0.07 sec/iter\n",
      "Epoch: 40 | Batch: 017 / 025 | Total loss: 3.129 | Reg loss: 0.028 | Tree loss: 3.129 | Accuracy: 0.154297 | 0.071 sec/iter\n",
      "Epoch: 40 | Batch: 018 / 025 | Total loss: 3.107 | Reg loss: 0.028 | Tree loss: 3.107 | Accuracy: 0.121094 | 0.071 sec/iter\n",
      "Epoch: 40 | Batch: 019 / 025 | Total loss: 3.086 | Reg loss: 0.028 | Tree loss: 3.086 | Accuracy: 0.117188 | 0.071 sec/iter\n",
      "Epoch: 40 | Batch: 020 / 025 | Total loss: 3.065 | Reg loss: 0.029 | Tree loss: 3.065 | Accuracy: 0.119141 | 0.07 sec/iter\n",
      "Epoch: 40 | Batch: 021 / 025 | Total loss: 3.090 | Reg loss: 0.029 | Tree loss: 3.090 | Accuracy: 0.126953 | 0.07 sec/iter\n",
      "Epoch: 40 | Batch: 022 / 025 | Total loss: 3.093 | Reg loss: 0.029 | Tree loss: 3.093 | Accuracy: 0.126953 | 0.07 sec/iter\n",
      "Epoch: 40 | Batch: 023 / 025 | Total loss: 2.997 | Reg loss: 0.029 | Tree loss: 2.997 | Accuracy: 0.177734 | 0.07 sec/iter\n",
      "Epoch: 40 | Batch: 024 / 025 | Total loss: 3.035 | Reg loss: 0.029 | Tree loss: 3.035 | Accuracy: 0.134375 | 0.07 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 41 | Batch: 000 / 025 | Total loss: 3.240 | Reg loss: 0.027 | Tree loss: 3.240 | Accuracy: 0.130859 | 0.07 sec/iter\n",
      "Epoch: 41 | Batch: 001 / 025 | Total loss: 3.332 | Reg loss: 0.027 | Tree loss: 3.332 | Accuracy: 0.123047 | 0.071 sec/iter\n",
      "Epoch: 41 | Batch: 002 / 025 | Total loss: 3.263 | Reg loss: 0.027 | Tree loss: 3.263 | Accuracy: 0.132812 | 0.071 sec/iter\n",
      "Epoch: 41 | Batch: 003 / 025 | Total loss: 3.252 | Reg loss: 0.027 | Tree loss: 3.252 | Accuracy: 0.093750 | 0.071 sec/iter\n",
      "Epoch: 41 | Batch: 004 / 025 | Total loss: 3.240 | Reg loss: 0.027 | Tree loss: 3.240 | Accuracy: 0.128906 | 0.071 sec/iter\n",
      "Epoch: 41 | Batch: 005 / 025 | Total loss: 3.222 | Reg loss: 0.027 | Tree loss: 3.222 | Accuracy: 0.166016 | 0.07 sec/iter\n",
      "Epoch: 41 | Batch: 006 / 025 | Total loss: 3.233 | Reg loss: 0.027 | Tree loss: 3.233 | Accuracy: 0.128906 | 0.07 sec/iter\n",
      "Epoch: 41 | Batch: 007 / 025 | Total loss: 3.204 | Reg loss: 0.027 | Tree loss: 3.204 | Accuracy: 0.175781 | 0.07 sec/iter\n",
      "Epoch: 41 | Batch: 008 / 025 | Total loss: 3.157 | Reg loss: 0.027 | Tree loss: 3.157 | Accuracy: 0.134766 | 0.07 sec/iter\n",
      "Epoch: 41 | Batch: 009 / 025 | Total loss: 3.159 | Reg loss: 0.028 | Tree loss: 3.159 | Accuracy: 0.113281 | 0.07 sec/iter\n",
      "Epoch: 41 | Batch: 010 / 025 | Total loss: 3.166 | Reg loss: 0.028 | Tree loss: 3.166 | Accuracy: 0.128906 | 0.07 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 41 | Batch: 011 / 025 | Total loss: 3.136 | Reg loss: 0.028 | Tree loss: 3.136 | Accuracy: 0.142578 | 0.07 sec/iter\n",
      "Epoch: 41 | Batch: 012 / 025 | Total loss: 3.071 | Reg loss: 0.028 | Tree loss: 3.071 | Accuracy: 0.121094 | 0.07 sec/iter\n",
      "Epoch: 41 | Batch: 013 / 025 | Total loss: 3.108 | Reg loss: 0.028 | Tree loss: 3.108 | Accuracy: 0.150391 | 0.07 sec/iter\n",
      "Epoch: 41 | Batch: 014 / 025 | Total loss: 3.112 | Reg loss: 0.028 | Tree loss: 3.112 | Accuracy: 0.146484 | 0.07 sec/iter\n",
      "Epoch: 41 | Batch: 015 / 025 | Total loss: 3.043 | Reg loss: 0.028 | Tree loss: 3.043 | Accuracy: 0.115234 | 0.07 sec/iter\n",
      "Epoch: 41 | Batch: 016 / 025 | Total loss: 3.045 | Reg loss: 0.028 | Tree loss: 3.045 | Accuracy: 0.119141 | 0.07 sec/iter\n",
      "Epoch: 41 | Batch: 017 / 025 | Total loss: 3.083 | Reg loss: 0.028 | Tree loss: 3.083 | Accuracy: 0.138672 | 0.07 sec/iter\n",
      "Epoch: 41 | Batch: 018 / 025 | Total loss: 2.985 | Reg loss: 0.028 | Tree loss: 2.985 | Accuracy: 0.123047 | 0.07 sec/iter\n",
      "Epoch: 41 | Batch: 019 / 025 | Total loss: 3.061 | Reg loss: 0.029 | Tree loss: 3.061 | Accuracy: 0.132812 | 0.07 sec/iter\n",
      "Epoch: 41 | Batch: 020 / 025 | Total loss: 2.991 | Reg loss: 0.029 | Tree loss: 2.991 | Accuracy: 0.167969 | 0.07 sec/iter\n",
      "Epoch: 41 | Batch: 021 / 025 | Total loss: 3.024 | Reg loss: 0.029 | Tree loss: 3.024 | Accuracy: 0.140625 | 0.07 sec/iter\n",
      "Epoch: 41 | Batch: 022 / 025 | Total loss: 2.998 | Reg loss: 0.029 | Tree loss: 2.998 | Accuracy: 0.123047 | 0.07 sec/iter\n",
      "Epoch: 41 | Batch: 023 / 025 | Total loss: 3.009 | Reg loss: 0.029 | Tree loss: 3.009 | Accuracy: 0.130859 | 0.07 sec/iter\n",
      "Epoch: 41 | Batch: 024 / 025 | Total loss: 2.939 | Reg loss: 0.029 | Tree loss: 2.939 | Accuracy: 0.156250 | 0.07 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 42 | Batch: 000 / 025 | Total loss: 3.196 | Reg loss: 0.027 | Tree loss: 3.196 | Accuracy: 0.126953 | 0.07 sec/iter\n",
      "Epoch: 42 | Batch: 001 / 025 | Total loss: 3.217 | Reg loss: 0.027 | Tree loss: 3.217 | Accuracy: 0.119141 | 0.07 sec/iter\n",
      "Epoch: 42 | Batch: 002 / 025 | Total loss: 3.259 | Reg loss: 0.027 | Tree loss: 3.259 | Accuracy: 0.109375 | 0.07 sec/iter\n",
      "Epoch: 42 | Batch: 003 / 025 | Total loss: 3.178 | Reg loss: 0.028 | Tree loss: 3.178 | Accuracy: 0.154297 | 0.07 sec/iter\n",
      "Epoch: 42 | Batch: 004 / 025 | Total loss: 3.196 | Reg loss: 0.028 | Tree loss: 3.196 | Accuracy: 0.107422 | 0.07 sec/iter\n",
      "Epoch: 42 | Batch: 005 / 025 | Total loss: 3.206 | Reg loss: 0.028 | Tree loss: 3.206 | Accuracy: 0.136719 | 0.07 sec/iter\n",
      "Epoch: 42 | Batch: 006 / 025 | Total loss: 3.163 | Reg loss: 0.028 | Tree loss: 3.163 | Accuracy: 0.115234 | 0.07 sec/iter\n",
      "Epoch: 42 | Batch: 007 / 025 | Total loss: 3.069 | Reg loss: 0.028 | Tree loss: 3.069 | Accuracy: 0.144531 | 0.07 sec/iter\n",
      "Epoch: 42 | Batch: 008 / 025 | Total loss: 3.159 | Reg loss: 0.028 | Tree loss: 3.159 | Accuracy: 0.115234 | 0.07 sec/iter\n",
      "Epoch: 42 | Batch: 009 / 025 | Total loss: 3.080 | Reg loss: 0.028 | Tree loss: 3.080 | Accuracy: 0.130859 | 0.07 sec/iter\n",
      "Epoch: 42 | Batch: 010 / 025 | Total loss: 3.108 | Reg loss: 0.028 | Tree loss: 3.108 | Accuracy: 0.154297 | 0.07 sec/iter\n",
      "Epoch: 42 | Batch: 011 / 025 | Total loss: 3.014 | Reg loss: 0.028 | Tree loss: 3.014 | Accuracy: 0.154297 | 0.07 sec/iter\n",
      "Epoch: 42 | Batch: 012 / 025 | Total loss: 3.045 | Reg loss: 0.028 | Tree loss: 3.045 | Accuracy: 0.140625 | 0.07 sec/iter\n",
      "Epoch: 42 | Batch: 013 / 025 | Total loss: 3.032 | Reg loss: 0.028 | Tree loss: 3.032 | Accuracy: 0.121094 | 0.07 sec/iter\n",
      "Epoch: 42 | Batch: 014 / 025 | Total loss: 3.003 | Reg loss: 0.028 | Tree loss: 3.003 | Accuracy: 0.144531 | 0.07 sec/iter\n",
      "Epoch: 42 | Batch: 015 / 025 | Total loss: 2.999 | Reg loss: 0.028 | Tree loss: 2.999 | Accuracy: 0.154297 | 0.07 sec/iter\n",
      "Epoch: 42 | Batch: 016 / 025 | Total loss: 2.977 | Reg loss: 0.028 | Tree loss: 2.977 | Accuracy: 0.144531 | 0.07 sec/iter\n",
      "Epoch: 42 | Batch: 017 / 025 | Total loss: 3.022 | Reg loss: 0.029 | Tree loss: 3.022 | Accuracy: 0.140625 | 0.07 sec/iter\n",
      "Epoch: 42 | Batch: 018 / 025 | Total loss: 3.013 | Reg loss: 0.029 | Tree loss: 3.013 | Accuracy: 0.126953 | 0.07 sec/iter\n",
      "Epoch: 42 | Batch: 019 / 025 | Total loss: 2.945 | Reg loss: 0.029 | Tree loss: 2.945 | Accuracy: 0.132812 | 0.07 sec/iter\n",
      "Epoch: 42 | Batch: 020 / 025 | Total loss: 2.994 | Reg loss: 0.029 | Tree loss: 2.994 | Accuracy: 0.150391 | 0.07 sec/iter\n",
      "Epoch: 42 | Batch: 021 / 025 | Total loss: 2.981 | Reg loss: 0.029 | Tree loss: 2.981 | Accuracy: 0.121094 | 0.07 sec/iter\n",
      "Epoch: 42 | Batch: 022 / 025 | Total loss: 2.953 | Reg loss: 0.029 | Tree loss: 2.953 | Accuracy: 0.140625 | 0.07 sec/iter\n",
      "Epoch: 42 | Batch: 023 / 025 | Total loss: 2.926 | Reg loss: 0.029 | Tree loss: 2.926 | Accuracy: 0.169922 | 0.07 sec/iter\n",
      "Epoch: 42 | Batch: 024 / 025 | Total loss: 2.932 | Reg loss: 0.029 | Tree loss: 2.932 | Accuracy: 0.140625 | 0.07 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 43 | Batch: 000 / 025 | Total loss: 3.173 | Reg loss: 0.028 | Tree loss: 3.173 | Accuracy: 0.142578 | 0.07 sec/iter\n",
      "Epoch: 43 | Batch: 001 / 025 | Total loss: 3.188 | Reg loss: 0.028 | Tree loss: 3.188 | Accuracy: 0.132812 | 0.07 sec/iter\n",
      "Epoch: 43 | Batch: 002 / 025 | Total loss: 3.157 | Reg loss: 0.028 | Tree loss: 3.157 | Accuracy: 0.115234 | 0.07 sec/iter\n",
      "Epoch: 43 | Batch: 003 / 025 | Total loss: 3.124 | Reg loss: 0.028 | Tree loss: 3.124 | Accuracy: 0.130859 | 0.07 sec/iter\n",
      "Epoch: 43 | Batch: 004 / 025 | Total loss: 3.130 | Reg loss: 0.028 | Tree loss: 3.130 | Accuracy: 0.123047 | 0.07 sec/iter\n",
      "Epoch: 43 | Batch: 005 / 025 | Total loss: 3.109 | Reg loss: 0.028 | Tree loss: 3.109 | Accuracy: 0.158203 | 0.07 sec/iter\n",
      "Epoch: 43 | Batch: 006 / 025 | Total loss: 3.094 | Reg loss: 0.028 | Tree loss: 3.094 | Accuracy: 0.119141 | 0.07 sec/iter\n",
      "Epoch: 43 | Batch: 007 / 025 | Total loss: 3.099 | Reg loss: 0.028 | Tree loss: 3.099 | Accuracy: 0.132812 | 0.07 sec/iter\n",
      "Epoch: 43 | Batch: 008 / 025 | Total loss: 3.079 | Reg loss: 0.028 | Tree loss: 3.079 | Accuracy: 0.126953 | 0.07 sec/iter\n",
      "Epoch: 43 | Batch: 009 / 025 | Total loss: 3.011 | Reg loss: 0.028 | Tree loss: 3.011 | Accuracy: 0.146484 | 0.07 sec/iter\n",
      "Epoch: 43 | Batch: 010 / 025 | Total loss: 3.026 | Reg loss: 0.028 | Tree loss: 3.026 | Accuracy: 0.164062 | 0.07 sec/iter\n",
      "Epoch: 43 | Batch: 011 / 025 | Total loss: 3.012 | Reg loss: 0.028 | Tree loss: 3.012 | Accuracy: 0.111328 | 0.07 sec/iter\n",
      "Epoch: 43 | Batch: 012 / 025 | Total loss: 3.036 | Reg loss: 0.028 | Tree loss: 3.036 | Accuracy: 0.148438 | 0.07 sec/iter\n",
      "Epoch: 43 | Batch: 013 / 025 | Total loss: 3.007 | Reg loss: 0.028 | Tree loss: 3.007 | Accuracy: 0.148438 | 0.07 sec/iter\n",
      "Epoch: 43 | Batch: 014 / 025 | Total loss: 2.975 | Reg loss: 0.028 | Tree loss: 2.975 | Accuracy: 0.144531 | 0.07 sec/iter\n",
      "Epoch: 43 | Batch: 015 / 025 | Total loss: 2.928 | Reg loss: 0.029 | Tree loss: 2.928 | Accuracy: 0.142578 | 0.07 sec/iter\n",
      "Epoch: 43 | Batch: 016 / 025 | Total loss: 2.978 | Reg loss: 0.029 | Tree loss: 2.978 | Accuracy: 0.121094 | 0.07 sec/iter\n",
      "Epoch: 43 | Batch: 017 / 025 | Total loss: 2.916 | Reg loss: 0.029 | Tree loss: 2.916 | Accuracy: 0.166016 | 0.07 sec/iter\n",
      "Epoch: 43 | Batch: 018 / 025 | Total loss: 2.904 | Reg loss: 0.029 | Tree loss: 2.904 | Accuracy: 0.132812 | 0.07 sec/iter\n",
      "Epoch: 43 | Batch: 019 / 025 | Total loss: 2.941 | Reg loss: 0.029 | Tree loss: 2.941 | Accuracy: 0.117188 | 0.07 sec/iter\n",
      "Epoch: 43 | Batch: 020 / 025 | Total loss: 2.884 | Reg loss: 0.029 | Tree loss: 2.884 | Accuracy: 0.146484 | 0.07 sec/iter\n",
      "Epoch: 43 | Batch: 021 / 025 | Total loss: 2.915 | Reg loss: 0.029 | Tree loss: 2.915 | Accuracy: 0.144531 | 0.07 sec/iter\n",
      "Epoch: 43 | Batch: 022 / 025 | Total loss: 2.896 | Reg loss: 0.029 | Tree loss: 2.896 | Accuracy: 0.121094 | 0.07 sec/iter\n",
      "Epoch: 43 | Batch: 023 / 025 | Total loss: 2.913 | Reg loss: 0.029 | Tree loss: 2.913 | Accuracy: 0.128906 | 0.07 sec/iter\n",
      "Epoch: 43 | Batch: 024 / 025 | Total loss: 2.842 | Reg loss: 0.029 | Tree loss: 2.842 | Accuracy: 0.156250 | 0.07 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 44 | Batch: 000 / 025 | Total loss: 3.120 | Reg loss: 0.028 | Tree loss: 3.120 | Accuracy: 0.115234 | 0.071 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 44 | Batch: 001 / 025 | Total loss: 3.083 | Reg loss: 0.028 | Tree loss: 3.083 | Accuracy: 0.119141 | 0.071 sec/iter\n",
      "Epoch: 44 | Batch: 002 / 025 | Total loss: 3.061 | Reg loss: 0.028 | Tree loss: 3.061 | Accuracy: 0.132812 | 0.071 sec/iter\n",
      "Epoch: 44 | Batch: 003 / 025 | Total loss: 3.055 | Reg loss: 0.028 | Tree loss: 3.055 | Accuracy: 0.142578 | 0.071 sec/iter\n",
      "Epoch: 44 | Batch: 004 / 025 | Total loss: 3.013 | Reg loss: 0.028 | Tree loss: 3.013 | Accuracy: 0.179688 | 0.071 sec/iter\n",
      "Epoch: 44 | Batch: 005 / 025 | Total loss: 3.054 | Reg loss: 0.028 | Tree loss: 3.054 | Accuracy: 0.167969 | 0.071 sec/iter\n",
      "Epoch: 44 | Batch: 006 / 025 | Total loss: 3.033 | Reg loss: 0.028 | Tree loss: 3.033 | Accuracy: 0.152344 | 0.071 sec/iter\n",
      "Epoch: 44 | Batch: 007 / 025 | Total loss: 3.022 | Reg loss: 0.028 | Tree loss: 3.022 | Accuracy: 0.144531 | 0.071 sec/iter\n",
      "Epoch: 44 | Batch: 008 / 025 | Total loss: 3.003 | Reg loss: 0.028 | Tree loss: 3.003 | Accuracy: 0.125000 | 0.071 sec/iter\n",
      "Epoch: 44 | Batch: 009 / 025 | Total loss: 2.946 | Reg loss: 0.028 | Tree loss: 2.946 | Accuracy: 0.140625 | 0.071 sec/iter\n",
      "Epoch: 44 | Batch: 010 / 025 | Total loss: 3.005 | Reg loss: 0.028 | Tree loss: 3.005 | Accuracy: 0.140625 | 0.071 sec/iter\n",
      "Epoch: 44 | Batch: 011 / 025 | Total loss: 2.975 | Reg loss: 0.028 | Tree loss: 2.975 | Accuracy: 0.103516 | 0.071 sec/iter\n",
      "Epoch: 44 | Batch: 012 / 025 | Total loss: 2.986 | Reg loss: 0.029 | Tree loss: 2.986 | Accuracy: 0.136719 | 0.071 sec/iter\n",
      "Epoch: 44 | Batch: 013 / 025 | Total loss: 2.973 | Reg loss: 0.029 | Tree loss: 2.973 | Accuracy: 0.136719 | 0.071 sec/iter\n",
      "Epoch: 44 | Batch: 014 / 025 | Total loss: 2.952 | Reg loss: 0.029 | Tree loss: 2.952 | Accuracy: 0.119141 | 0.071 sec/iter\n",
      "Epoch: 44 | Batch: 015 / 025 | Total loss: 2.932 | Reg loss: 0.029 | Tree loss: 2.932 | Accuracy: 0.146484 | 0.071 sec/iter\n",
      "Epoch: 44 | Batch: 016 / 025 | Total loss: 2.893 | Reg loss: 0.029 | Tree loss: 2.893 | Accuracy: 0.140625 | 0.071 sec/iter\n",
      "Epoch: 44 | Batch: 017 / 025 | Total loss: 2.870 | Reg loss: 0.029 | Tree loss: 2.870 | Accuracy: 0.166016 | 0.071 sec/iter\n",
      "Epoch: 44 | Batch: 018 / 025 | Total loss: 2.918 | Reg loss: 0.029 | Tree loss: 2.918 | Accuracy: 0.138672 | 0.071 sec/iter\n",
      "Epoch: 44 | Batch: 019 / 025 | Total loss: 2.924 | Reg loss: 0.029 | Tree loss: 2.924 | Accuracy: 0.109375 | 0.071 sec/iter\n",
      "Epoch: 44 | Batch: 020 / 025 | Total loss: 2.888 | Reg loss: 0.029 | Tree loss: 2.888 | Accuracy: 0.142578 | 0.071 sec/iter\n",
      "Epoch: 44 | Batch: 021 / 025 | Total loss: 2.876 | Reg loss: 0.029 | Tree loss: 2.876 | Accuracy: 0.107422 | 0.071 sec/iter\n",
      "Epoch: 44 | Batch: 022 / 025 | Total loss: 2.894 | Reg loss: 0.029 | Tree loss: 2.894 | Accuracy: 0.132812 | 0.071 sec/iter\n",
      "Epoch: 44 | Batch: 023 / 025 | Total loss: 2.852 | Reg loss: 0.029 | Tree loss: 2.852 | Accuracy: 0.140625 | 0.071 sec/iter\n",
      "Epoch: 44 | Batch: 024 / 025 | Total loss: 2.828 | Reg loss: 0.030 | Tree loss: 2.828 | Accuracy: 0.115625 | 0.071 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 45 | Batch: 000 / 025 | Total loss: 3.052 | Reg loss: 0.028 | Tree loss: 3.052 | Accuracy: 0.148438 | 0.071 sec/iter\n",
      "Epoch: 45 | Batch: 001 / 025 | Total loss: 3.038 | Reg loss: 0.028 | Tree loss: 3.038 | Accuracy: 0.152344 | 0.071 sec/iter\n",
      "Epoch: 45 | Batch: 002 / 025 | Total loss: 3.014 | Reg loss: 0.028 | Tree loss: 3.014 | Accuracy: 0.144531 | 0.071 sec/iter\n",
      "Epoch: 45 | Batch: 003 / 025 | Total loss: 3.059 | Reg loss: 0.028 | Tree loss: 3.059 | Accuracy: 0.115234 | 0.071 sec/iter\n",
      "Epoch: 45 | Batch: 004 / 025 | Total loss: 3.049 | Reg loss: 0.028 | Tree loss: 3.049 | Accuracy: 0.115234 | 0.071 sec/iter\n",
      "Epoch: 45 | Batch: 005 / 025 | Total loss: 3.008 | Reg loss: 0.028 | Tree loss: 3.008 | Accuracy: 0.126953 | 0.071 sec/iter\n",
      "Epoch: 45 | Batch: 006 / 025 | Total loss: 3.016 | Reg loss: 0.028 | Tree loss: 3.016 | Accuracy: 0.130859 | 0.071 sec/iter\n",
      "Epoch: 45 | Batch: 007 / 025 | Total loss: 2.938 | Reg loss: 0.028 | Tree loss: 2.938 | Accuracy: 0.173828 | 0.071 sec/iter\n",
      "Epoch: 45 | Batch: 008 / 025 | Total loss: 2.932 | Reg loss: 0.028 | Tree loss: 2.932 | Accuracy: 0.140625 | 0.071 sec/iter\n",
      "Epoch: 45 | Batch: 009 / 025 | Total loss: 2.944 | Reg loss: 0.028 | Tree loss: 2.944 | Accuracy: 0.150391 | 0.071 sec/iter\n",
      "Epoch: 45 | Batch: 010 / 025 | Total loss: 2.999 | Reg loss: 0.029 | Tree loss: 2.999 | Accuracy: 0.097656 | 0.071 sec/iter\n",
      "Epoch: 45 | Batch: 011 / 025 | Total loss: 2.969 | Reg loss: 0.029 | Tree loss: 2.969 | Accuracy: 0.123047 | 0.071 sec/iter\n",
      "Epoch: 45 | Batch: 012 / 025 | Total loss: 2.919 | Reg loss: 0.029 | Tree loss: 2.919 | Accuracy: 0.142578 | 0.071 sec/iter\n",
      "Epoch: 45 | Batch: 013 / 025 | Total loss: 2.899 | Reg loss: 0.029 | Tree loss: 2.899 | Accuracy: 0.140625 | 0.071 sec/iter\n",
      "Epoch: 45 | Batch: 014 / 025 | Total loss: 2.842 | Reg loss: 0.029 | Tree loss: 2.842 | Accuracy: 0.130859 | 0.071 sec/iter\n",
      "Epoch: 45 | Batch: 015 / 025 | Total loss: 2.886 | Reg loss: 0.029 | Tree loss: 2.886 | Accuracy: 0.119141 | 0.071 sec/iter\n",
      "Epoch: 45 | Batch: 016 / 025 | Total loss: 2.846 | Reg loss: 0.029 | Tree loss: 2.846 | Accuracy: 0.130859 | 0.071 sec/iter\n",
      "Epoch: 45 | Batch: 017 / 025 | Total loss: 2.921 | Reg loss: 0.029 | Tree loss: 2.921 | Accuracy: 0.125000 | 0.071 sec/iter\n",
      "Epoch: 45 | Batch: 018 / 025 | Total loss: 2.857 | Reg loss: 0.029 | Tree loss: 2.857 | Accuracy: 0.140625 | 0.071 sec/iter\n",
      "Epoch: 45 | Batch: 019 / 025 | Total loss: 2.806 | Reg loss: 0.029 | Tree loss: 2.806 | Accuracy: 0.146484 | 0.071 sec/iter\n",
      "Epoch: 45 | Batch: 020 / 025 | Total loss: 2.823 | Reg loss: 0.029 | Tree loss: 2.823 | Accuracy: 0.148438 | 0.071 sec/iter\n",
      "Epoch: 45 | Batch: 021 / 025 | Total loss: 2.862 | Reg loss: 0.029 | Tree loss: 2.862 | Accuracy: 0.140625 | 0.071 sec/iter\n",
      "Epoch: 45 | Batch: 022 / 025 | Total loss: 2.789 | Reg loss: 0.030 | Tree loss: 2.789 | Accuracy: 0.140625 | 0.071 sec/iter\n",
      "Epoch: 45 | Batch: 023 / 025 | Total loss: 2.808 | Reg loss: 0.030 | Tree loss: 2.808 | Accuracy: 0.146484 | 0.071 sec/iter\n",
      "Epoch: 45 | Batch: 024 / 025 | Total loss: 2.782 | Reg loss: 0.030 | Tree loss: 2.782 | Accuracy: 0.143750 | 0.071 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 46 | Batch: 000 / 025 | Total loss: 3.001 | Reg loss: 0.028 | Tree loss: 3.001 | Accuracy: 0.160156 | 0.071 sec/iter\n",
      "Epoch: 46 | Batch: 001 / 025 | Total loss: 3.001 | Reg loss: 0.028 | Tree loss: 3.001 | Accuracy: 0.142578 | 0.071 sec/iter\n",
      "Epoch: 46 | Batch: 002 / 025 | Total loss: 3.007 | Reg loss: 0.028 | Tree loss: 3.007 | Accuracy: 0.144531 | 0.071 sec/iter\n",
      "Epoch: 46 | Batch: 003 / 025 | Total loss: 3.009 | Reg loss: 0.028 | Tree loss: 3.009 | Accuracy: 0.140625 | 0.071 sec/iter\n",
      "Epoch: 46 | Batch: 004 / 025 | Total loss: 3.040 | Reg loss: 0.028 | Tree loss: 3.040 | Accuracy: 0.091797 | 0.071 sec/iter\n",
      "Epoch: 46 | Batch: 005 / 025 | Total loss: 2.946 | Reg loss: 0.029 | Tree loss: 2.946 | Accuracy: 0.128906 | 0.071 sec/iter\n",
      "Epoch: 46 | Batch: 006 / 025 | Total loss: 2.948 | Reg loss: 0.029 | Tree loss: 2.948 | Accuracy: 0.142578 | 0.071 sec/iter\n",
      "Epoch: 46 | Batch: 007 / 025 | Total loss: 2.936 | Reg loss: 0.029 | Tree loss: 2.936 | Accuracy: 0.138672 | 0.071 sec/iter\n",
      "Epoch: 46 | Batch: 008 / 025 | Total loss: 2.938 | Reg loss: 0.029 | Tree loss: 2.938 | Accuracy: 0.126953 | 0.071 sec/iter\n",
      "Epoch: 46 | Batch: 009 / 025 | Total loss: 2.913 | Reg loss: 0.029 | Tree loss: 2.913 | Accuracy: 0.146484 | 0.071 sec/iter\n",
      "Epoch: 46 | Batch: 010 / 025 | Total loss: 2.881 | Reg loss: 0.029 | Tree loss: 2.881 | Accuracy: 0.154297 | 0.071 sec/iter\n",
      "Epoch: 46 | Batch: 011 / 025 | Total loss: 2.930 | Reg loss: 0.029 | Tree loss: 2.930 | Accuracy: 0.123047 | 0.071 sec/iter\n",
      "Epoch: 46 | Batch: 012 / 025 | Total loss: 2.832 | Reg loss: 0.029 | Tree loss: 2.832 | Accuracy: 0.156250 | 0.071 sec/iter\n",
      "Epoch: 46 | Batch: 013 / 025 | Total loss: 2.840 | Reg loss: 0.029 | Tree loss: 2.840 | Accuracy: 0.134766 | 0.071 sec/iter\n",
      "Epoch: 46 | Batch: 014 / 025 | Total loss: 2.834 | Reg loss: 0.029 | Tree loss: 2.834 | Accuracy: 0.162109 | 0.071 sec/iter\n",
      "Epoch: 46 | Batch: 015 / 025 | Total loss: 2.873 | Reg loss: 0.029 | Tree loss: 2.873 | Accuracy: 0.117188 | 0.071 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 46 | Batch: 016 / 025 | Total loss: 2.798 | Reg loss: 0.029 | Tree loss: 2.798 | Accuracy: 0.138672 | 0.071 sec/iter\n",
      "Epoch: 46 | Batch: 017 / 025 | Total loss: 2.855 | Reg loss: 0.029 | Tree loss: 2.855 | Accuracy: 0.107422 | 0.071 sec/iter\n",
      "Epoch: 46 | Batch: 018 / 025 | Total loss: 2.830 | Reg loss: 0.029 | Tree loss: 2.830 | Accuracy: 0.109375 | 0.071 sec/iter\n",
      "Epoch: 46 | Batch: 019 / 025 | Total loss: 2.784 | Reg loss: 0.029 | Tree loss: 2.784 | Accuracy: 0.166016 | 0.071 sec/iter\n",
      "Epoch: 46 | Batch: 020 / 025 | Total loss: 2.818 | Reg loss: 0.029 | Tree loss: 2.818 | Accuracy: 0.121094 | 0.071 sec/iter\n",
      "Epoch: 46 | Batch: 021 / 025 | Total loss: 2.766 | Reg loss: 0.030 | Tree loss: 2.766 | Accuracy: 0.136719 | 0.071 sec/iter\n",
      "Epoch: 46 | Batch: 022 / 025 | Total loss: 2.739 | Reg loss: 0.030 | Tree loss: 2.739 | Accuracy: 0.154297 | 0.071 sec/iter\n",
      "Epoch: 46 | Batch: 023 / 025 | Total loss: 2.770 | Reg loss: 0.030 | Tree loss: 2.770 | Accuracy: 0.130859 | 0.071 sec/iter\n",
      "Epoch: 46 | Batch: 024 / 025 | Total loss: 2.744 | Reg loss: 0.030 | Tree loss: 2.744 | Accuracy: 0.150000 | 0.071 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 47 | Batch: 000 / 025 | Total loss: 2.955 | Reg loss: 0.029 | Tree loss: 2.955 | Accuracy: 0.109375 | 0.071 sec/iter\n",
      "Epoch: 47 | Batch: 001 / 025 | Total loss: 2.948 | Reg loss: 0.029 | Tree loss: 2.948 | Accuracy: 0.144531 | 0.071 sec/iter\n",
      "Epoch: 47 | Batch: 002 / 025 | Total loss: 2.983 | Reg loss: 0.029 | Tree loss: 2.983 | Accuracy: 0.136719 | 0.071 sec/iter\n",
      "Epoch: 47 | Batch: 003 / 025 | Total loss: 2.967 | Reg loss: 0.029 | Tree loss: 2.967 | Accuracy: 0.134766 | 0.071 sec/iter\n",
      "Epoch: 47 | Batch: 004 / 025 | Total loss: 2.928 | Reg loss: 0.029 | Tree loss: 2.928 | Accuracy: 0.113281 | 0.071 sec/iter\n",
      "Epoch: 47 | Batch: 005 / 025 | Total loss: 2.926 | Reg loss: 0.029 | Tree loss: 2.926 | Accuracy: 0.128906 | 0.071 sec/iter\n",
      "Epoch: 47 | Batch: 006 / 025 | Total loss: 2.953 | Reg loss: 0.029 | Tree loss: 2.953 | Accuracy: 0.134766 | 0.071 sec/iter\n",
      "Epoch: 47 | Batch: 007 / 025 | Total loss: 2.884 | Reg loss: 0.029 | Tree loss: 2.884 | Accuracy: 0.166016 | 0.071 sec/iter\n",
      "Epoch: 47 | Batch: 008 / 025 | Total loss: 2.901 | Reg loss: 0.029 | Tree loss: 2.901 | Accuracy: 0.132812 | 0.071 sec/iter\n",
      "Epoch: 47 | Batch: 009 / 025 | Total loss: 2.845 | Reg loss: 0.029 | Tree loss: 2.845 | Accuracy: 0.101562 | 0.071 sec/iter\n",
      "Epoch: 47 | Batch: 010 / 025 | Total loss: 2.846 | Reg loss: 0.029 | Tree loss: 2.846 | Accuracy: 0.130859 | 0.071 sec/iter\n",
      "Epoch: 47 | Batch: 011 / 025 | Total loss: 2.801 | Reg loss: 0.029 | Tree loss: 2.801 | Accuracy: 0.146484 | 0.071 sec/iter\n",
      "Epoch: 47 | Batch: 012 / 025 | Total loss: 2.865 | Reg loss: 0.029 | Tree loss: 2.865 | Accuracy: 0.125000 | 0.071 sec/iter\n",
      "Epoch: 47 | Batch: 013 / 025 | Total loss: 2.784 | Reg loss: 0.029 | Tree loss: 2.784 | Accuracy: 0.130859 | 0.071 sec/iter\n",
      "Epoch: 47 | Batch: 014 / 025 | Total loss: 2.801 | Reg loss: 0.029 | Tree loss: 2.801 | Accuracy: 0.144531 | 0.071 sec/iter\n",
      "Epoch: 47 | Batch: 015 / 025 | Total loss: 2.803 | Reg loss: 0.029 | Tree loss: 2.803 | Accuracy: 0.146484 | 0.071 sec/iter\n",
      "Epoch: 47 | Batch: 016 / 025 | Total loss: 2.794 | Reg loss: 0.029 | Tree loss: 2.794 | Accuracy: 0.126953 | 0.071 sec/iter\n",
      "Epoch: 47 | Batch: 017 / 025 | Total loss: 2.815 | Reg loss: 0.029 | Tree loss: 2.815 | Accuracy: 0.123047 | 0.071 sec/iter\n",
      "Epoch: 47 | Batch: 018 / 025 | Total loss: 2.813 | Reg loss: 0.029 | Tree loss: 2.813 | Accuracy: 0.140625 | 0.071 sec/iter\n",
      "Epoch: 47 | Batch: 019 / 025 | Total loss: 2.810 | Reg loss: 0.029 | Tree loss: 2.810 | Accuracy: 0.140625 | 0.071 sec/iter\n",
      "Epoch: 47 | Batch: 020 / 025 | Total loss: 2.768 | Reg loss: 0.030 | Tree loss: 2.768 | Accuracy: 0.142578 | 0.071 sec/iter\n",
      "Epoch: 47 | Batch: 021 / 025 | Total loss: 2.730 | Reg loss: 0.030 | Tree loss: 2.730 | Accuracy: 0.136719 | 0.071 sec/iter\n",
      "Epoch: 47 | Batch: 022 / 025 | Total loss: 2.717 | Reg loss: 0.030 | Tree loss: 2.717 | Accuracy: 0.146484 | 0.071 sec/iter\n",
      "Epoch: 47 | Batch: 023 / 025 | Total loss: 2.722 | Reg loss: 0.030 | Tree loss: 2.722 | Accuracy: 0.158203 | 0.071 sec/iter\n",
      "Epoch: 47 | Batch: 024 / 025 | Total loss: 2.732 | Reg loss: 0.030 | Tree loss: 2.732 | Accuracy: 0.153125 | 0.071 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 48 | Batch: 000 / 025 | Total loss: 2.932 | Reg loss: 0.029 | Tree loss: 2.932 | Accuracy: 0.138672 | 0.071 sec/iter\n",
      "Epoch: 48 | Batch: 001 / 025 | Total loss: 2.986 | Reg loss: 0.029 | Tree loss: 2.986 | Accuracy: 0.125000 | 0.071 sec/iter\n",
      "Epoch: 48 | Batch: 002 / 025 | Total loss: 2.909 | Reg loss: 0.029 | Tree loss: 2.909 | Accuracy: 0.128906 | 0.071 sec/iter\n",
      "Epoch: 48 | Batch: 003 / 025 | Total loss: 2.912 | Reg loss: 0.029 | Tree loss: 2.912 | Accuracy: 0.123047 | 0.071 sec/iter\n",
      "Epoch: 48 | Batch: 004 / 025 | Total loss: 2.874 | Reg loss: 0.029 | Tree loss: 2.874 | Accuracy: 0.117188 | 0.071 sec/iter\n",
      "Epoch: 48 | Batch: 005 / 025 | Total loss: 2.912 | Reg loss: 0.029 | Tree loss: 2.912 | Accuracy: 0.166016 | 0.071 sec/iter\n",
      "Epoch: 48 | Batch: 006 / 025 | Total loss: 2.886 | Reg loss: 0.029 | Tree loss: 2.886 | Accuracy: 0.140625 | 0.071 sec/iter\n",
      "Epoch: 48 | Batch: 007 / 025 | Total loss: 2.827 | Reg loss: 0.029 | Tree loss: 2.827 | Accuracy: 0.115234 | 0.071 sec/iter\n",
      "Epoch: 48 | Batch: 008 / 025 | Total loss: 2.820 | Reg loss: 0.029 | Tree loss: 2.820 | Accuracy: 0.150391 | 0.071 sec/iter\n",
      "Epoch: 48 | Batch: 009 / 025 | Total loss: 2.803 | Reg loss: 0.029 | Tree loss: 2.803 | Accuracy: 0.132812 | 0.071 sec/iter\n",
      "Epoch: 48 | Batch: 010 / 025 | Total loss: 2.804 | Reg loss: 0.029 | Tree loss: 2.804 | Accuracy: 0.134766 | 0.071 sec/iter\n",
      "Epoch: 48 | Batch: 011 / 025 | Total loss: 2.804 | Reg loss: 0.029 | Tree loss: 2.804 | Accuracy: 0.123047 | 0.071 sec/iter\n",
      "Epoch: 48 | Batch: 012 / 025 | Total loss: 2.834 | Reg loss: 0.029 | Tree loss: 2.834 | Accuracy: 0.144531 | 0.071 sec/iter\n",
      "Epoch: 48 | Batch: 013 / 025 | Total loss: 2.832 | Reg loss: 0.029 | Tree loss: 2.832 | Accuracy: 0.121094 | 0.071 sec/iter\n",
      "Epoch: 48 | Batch: 014 / 025 | Total loss: 2.792 | Reg loss: 0.029 | Tree loss: 2.792 | Accuracy: 0.134766 | 0.071 sec/iter\n",
      "Epoch: 48 | Batch: 015 / 025 | Total loss: 2.768 | Reg loss: 0.029 | Tree loss: 2.768 | Accuracy: 0.158203 | 0.071 sec/iter\n",
      "Epoch: 48 | Batch: 016 / 025 | Total loss: 2.776 | Reg loss: 0.029 | Tree loss: 2.776 | Accuracy: 0.146484 | 0.071 sec/iter\n",
      "Epoch: 48 | Batch: 017 / 025 | Total loss: 2.713 | Reg loss: 0.029 | Tree loss: 2.713 | Accuracy: 0.134766 | 0.071 sec/iter\n",
      "Epoch: 48 | Batch: 018 / 025 | Total loss: 2.730 | Reg loss: 0.030 | Tree loss: 2.730 | Accuracy: 0.142578 | 0.071 sec/iter\n",
      "Epoch: 48 | Batch: 019 / 025 | Total loss: 2.729 | Reg loss: 0.030 | Tree loss: 2.729 | Accuracy: 0.162109 | 0.071 sec/iter\n",
      "Epoch: 48 | Batch: 020 / 025 | Total loss: 2.704 | Reg loss: 0.030 | Tree loss: 2.704 | Accuracy: 0.121094 | 0.071 sec/iter\n",
      "Epoch: 48 | Batch: 021 / 025 | Total loss: 2.703 | Reg loss: 0.030 | Tree loss: 2.703 | Accuracy: 0.160156 | 0.071 sec/iter\n",
      "Epoch: 48 | Batch: 022 / 025 | Total loss: 2.709 | Reg loss: 0.030 | Tree loss: 2.709 | Accuracy: 0.107422 | 0.071 sec/iter\n",
      "Epoch: 48 | Batch: 023 / 025 | Total loss: 2.733 | Reg loss: 0.030 | Tree loss: 2.733 | Accuracy: 0.136719 | 0.071 sec/iter\n",
      "Epoch: 48 | Batch: 024 / 025 | Total loss: 2.763 | Reg loss: 0.030 | Tree loss: 2.763 | Accuracy: 0.109375 | 0.071 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 49 | Batch: 000 / 025 | Total loss: 2.905 | Reg loss: 0.029 | Tree loss: 2.905 | Accuracy: 0.142578 | 0.071 sec/iter\n",
      "Epoch: 49 | Batch: 001 / 025 | Total loss: 2.927 | Reg loss: 0.029 | Tree loss: 2.927 | Accuracy: 0.140625 | 0.071 sec/iter\n",
      "Epoch: 49 | Batch: 002 / 025 | Total loss: 2.882 | Reg loss: 0.029 | Tree loss: 2.882 | Accuracy: 0.140625 | 0.071 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 49 | Batch: 003 / 025 | Total loss: 2.851 | Reg loss: 0.029 | Tree loss: 2.851 | Accuracy: 0.134766 | 0.071 sec/iter\n",
      "Epoch: 49 | Batch: 004 / 025 | Total loss: 2.885 | Reg loss: 0.029 | Tree loss: 2.885 | Accuracy: 0.152344 | 0.071 sec/iter\n",
      "Epoch: 49 | Batch: 005 / 025 | Total loss: 2.817 | Reg loss: 0.029 | Tree loss: 2.817 | Accuracy: 0.144531 | 0.071 sec/iter\n",
      "Epoch: 49 | Batch: 006 / 025 | Total loss: 2.839 | Reg loss: 0.029 | Tree loss: 2.839 | Accuracy: 0.150391 | 0.071 sec/iter\n",
      "Epoch: 49 | Batch: 007 / 025 | Total loss: 2.817 | Reg loss: 0.029 | Tree loss: 2.817 | Accuracy: 0.126953 | 0.071 sec/iter\n",
      "Epoch: 49 | Batch: 008 / 025 | Total loss: 2.821 | Reg loss: 0.029 | Tree loss: 2.821 | Accuracy: 0.132812 | 0.071 sec/iter\n",
      "Epoch: 49 | Batch: 009 / 025 | Total loss: 2.813 | Reg loss: 0.029 | Tree loss: 2.813 | Accuracy: 0.132812 | 0.071 sec/iter\n",
      "Epoch: 49 | Batch: 010 / 025 | Total loss: 2.760 | Reg loss: 0.029 | Tree loss: 2.760 | Accuracy: 0.142578 | 0.071 sec/iter\n",
      "Epoch: 49 | Batch: 011 / 025 | Total loss: 2.756 | Reg loss: 0.029 | Tree loss: 2.756 | Accuracy: 0.123047 | 0.071 sec/iter\n",
      "Epoch: 49 | Batch: 012 / 025 | Total loss: 2.758 | Reg loss: 0.029 | Tree loss: 2.758 | Accuracy: 0.140625 | 0.071 sec/iter\n",
      "Epoch: 49 | Batch: 013 / 025 | Total loss: 2.823 | Reg loss: 0.029 | Tree loss: 2.823 | Accuracy: 0.126953 | 0.071 sec/iter\n",
      "Epoch: 49 | Batch: 014 / 025 | Total loss: 2.802 | Reg loss: 0.029 | Tree loss: 2.802 | Accuracy: 0.136719 | 0.071 sec/iter\n",
      "Epoch: 49 | Batch: 015 / 025 | Total loss: 2.747 | Reg loss: 0.029 | Tree loss: 2.747 | Accuracy: 0.119141 | 0.071 sec/iter\n",
      "Epoch: 49 | Batch: 016 / 025 | Total loss: 2.680 | Reg loss: 0.030 | Tree loss: 2.680 | Accuracy: 0.138672 | 0.071 sec/iter\n",
      "Epoch: 49 | Batch: 017 / 025 | Total loss: 2.714 | Reg loss: 0.030 | Tree loss: 2.714 | Accuracy: 0.121094 | 0.071 sec/iter\n",
      "Epoch: 49 | Batch: 018 / 025 | Total loss: 2.711 | Reg loss: 0.030 | Tree loss: 2.711 | Accuracy: 0.126953 | 0.071 sec/iter\n",
      "Epoch: 49 | Batch: 019 / 025 | Total loss: 2.698 | Reg loss: 0.030 | Tree loss: 2.698 | Accuracy: 0.119141 | 0.071 sec/iter\n",
      "Epoch: 49 | Batch: 020 / 025 | Total loss: 2.697 | Reg loss: 0.030 | Tree loss: 2.697 | Accuracy: 0.130859 | 0.071 sec/iter\n",
      "Epoch: 49 | Batch: 021 / 025 | Total loss: 2.692 | Reg loss: 0.030 | Tree loss: 2.692 | Accuracy: 0.152344 | 0.071 sec/iter\n",
      "Epoch: 49 | Batch: 022 / 025 | Total loss: 2.701 | Reg loss: 0.030 | Tree loss: 2.701 | Accuracy: 0.121094 | 0.071 sec/iter\n",
      "Epoch: 49 | Batch: 023 / 025 | Total loss: 2.671 | Reg loss: 0.030 | Tree loss: 2.671 | Accuracy: 0.126953 | 0.071 sec/iter\n",
      "Epoch: 49 | Batch: 024 / 025 | Total loss: 2.662 | Reg loss: 0.030 | Tree loss: 2.662 | Accuracy: 0.153125 | 0.071 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 50 | Batch: 000 / 025 | Total loss: 2.856 | Reg loss: 0.029 | Tree loss: 2.856 | Accuracy: 0.160156 | 0.071 sec/iter\n",
      "Epoch: 50 | Batch: 001 / 025 | Total loss: 2.855 | Reg loss: 0.029 | Tree loss: 2.855 | Accuracy: 0.130859 | 0.071 sec/iter\n",
      "Epoch: 50 | Batch: 002 / 025 | Total loss: 2.840 | Reg loss: 0.029 | Tree loss: 2.840 | Accuracy: 0.132812 | 0.071 sec/iter\n",
      "Epoch: 50 | Batch: 003 / 025 | Total loss: 2.841 | Reg loss: 0.029 | Tree loss: 2.841 | Accuracy: 0.119141 | 0.071 sec/iter\n",
      "Epoch: 50 | Batch: 004 / 025 | Total loss: 2.791 | Reg loss: 0.029 | Tree loss: 2.791 | Accuracy: 0.162109 | 0.071 sec/iter\n",
      "Epoch: 50 | Batch: 005 / 025 | Total loss: 2.827 | Reg loss: 0.029 | Tree loss: 2.827 | Accuracy: 0.130859 | 0.071 sec/iter\n",
      "Epoch: 50 | Batch: 006 / 025 | Total loss: 2.806 | Reg loss: 0.029 | Tree loss: 2.806 | Accuracy: 0.123047 | 0.071 sec/iter\n",
      "Epoch: 50 | Batch: 007 / 025 | Total loss: 2.749 | Reg loss: 0.029 | Tree loss: 2.749 | Accuracy: 0.181641 | 0.071 sec/iter\n",
      "Epoch: 50 | Batch: 008 / 025 | Total loss: 2.807 | Reg loss: 0.029 | Tree loss: 2.807 | Accuracy: 0.128906 | 0.071 sec/iter\n",
      "Epoch: 50 | Batch: 009 / 025 | Total loss: 2.751 | Reg loss: 0.029 | Tree loss: 2.751 | Accuracy: 0.115234 | 0.071 sec/iter\n",
      "Epoch: 50 | Batch: 010 / 025 | Total loss: 2.794 | Reg loss: 0.029 | Tree loss: 2.794 | Accuracy: 0.154297 | 0.071 sec/iter\n",
      "Epoch: 50 | Batch: 011 / 025 | Total loss: 2.776 | Reg loss: 0.029 | Tree loss: 2.776 | Accuracy: 0.101562 | 0.071 sec/iter\n",
      "Epoch: 50 | Batch: 012 / 025 | Total loss: 2.759 | Reg loss: 0.029 | Tree loss: 2.759 | Accuracy: 0.119141 | 0.071 sec/iter\n",
      "Epoch: 50 | Batch: 013 / 025 | Total loss: 2.726 | Reg loss: 0.029 | Tree loss: 2.726 | Accuracy: 0.160156 | 0.071 sec/iter\n",
      "Epoch: 50 | Batch: 014 / 025 | Total loss: 2.691 | Reg loss: 0.029 | Tree loss: 2.691 | Accuracy: 0.128906 | 0.071 sec/iter\n",
      "Epoch: 50 | Batch: 015 / 025 | Total loss: 2.709 | Reg loss: 0.030 | Tree loss: 2.709 | Accuracy: 0.144531 | 0.071 sec/iter\n",
      "Epoch: 50 | Batch: 016 / 025 | Total loss: 2.677 | Reg loss: 0.030 | Tree loss: 2.677 | Accuracy: 0.156250 | 0.071 sec/iter\n",
      "Epoch: 50 | Batch: 017 / 025 | Total loss: 2.676 | Reg loss: 0.030 | Tree loss: 2.676 | Accuracy: 0.156250 | 0.071 sec/iter\n",
      "Epoch: 50 | Batch: 018 / 025 | Total loss: 2.725 | Reg loss: 0.030 | Tree loss: 2.725 | Accuracy: 0.140625 | 0.071 sec/iter\n",
      "Epoch: 50 | Batch: 019 / 025 | Total loss: 2.669 | Reg loss: 0.030 | Tree loss: 2.669 | Accuracy: 0.138672 | 0.071 sec/iter\n",
      "Epoch: 50 | Batch: 020 / 025 | Total loss: 2.677 | Reg loss: 0.030 | Tree loss: 2.677 | Accuracy: 0.128906 | 0.071 sec/iter\n",
      "Epoch: 50 | Batch: 021 / 025 | Total loss: 2.685 | Reg loss: 0.030 | Tree loss: 2.685 | Accuracy: 0.089844 | 0.071 sec/iter\n",
      "Epoch: 50 | Batch: 022 / 025 | Total loss: 2.671 | Reg loss: 0.030 | Tree loss: 2.671 | Accuracy: 0.125000 | 0.071 sec/iter\n",
      "Epoch: 50 | Batch: 023 / 025 | Total loss: 2.674 | Reg loss: 0.030 | Tree loss: 2.674 | Accuracy: 0.140625 | 0.071 sec/iter\n",
      "Epoch: 50 | Batch: 024 / 025 | Total loss: 2.665 | Reg loss: 0.030 | Tree loss: 2.665 | Accuracy: 0.134375 | 0.071 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 51 | Batch: 000 / 025 | Total loss: 2.867 | Reg loss: 0.029 | Tree loss: 2.867 | Accuracy: 0.119141 | 0.071 sec/iter\n",
      "Epoch: 51 | Batch: 001 / 025 | Total loss: 2.881 | Reg loss: 0.029 | Tree loss: 2.881 | Accuracy: 0.126953 | 0.071 sec/iter\n",
      "Epoch: 51 | Batch: 002 / 025 | Total loss: 2.780 | Reg loss: 0.029 | Tree loss: 2.780 | Accuracy: 0.128906 | 0.071 sec/iter\n",
      "Epoch: 51 | Batch: 003 / 025 | Total loss: 2.749 | Reg loss: 0.029 | Tree loss: 2.749 | Accuracy: 0.160156 | 0.071 sec/iter\n",
      "Epoch: 51 | Batch: 004 / 025 | Total loss: 2.844 | Reg loss: 0.029 | Tree loss: 2.844 | Accuracy: 0.115234 | 0.071 sec/iter\n",
      "Epoch: 51 | Batch: 005 / 025 | Total loss: 2.781 | Reg loss: 0.029 | Tree loss: 2.781 | Accuracy: 0.130859 | 0.071 sec/iter\n",
      "Epoch: 51 | Batch: 006 / 025 | Total loss: 2.778 | Reg loss: 0.029 | Tree loss: 2.778 | Accuracy: 0.115234 | 0.071 sec/iter\n",
      "Epoch: 51 | Batch: 007 / 025 | Total loss: 2.758 | Reg loss: 0.029 | Tree loss: 2.758 | Accuracy: 0.148438 | 0.071 sec/iter\n",
      "Epoch: 51 | Batch: 008 / 025 | Total loss: 2.740 | Reg loss: 0.029 | Tree loss: 2.740 | Accuracy: 0.113281 | 0.071 sec/iter\n",
      "Epoch: 51 | Batch: 009 / 025 | Total loss: 2.747 | Reg loss: 0.029 | Tree loss: 2.747 | Accuracy: 0.140625 | 0.071 sec/iter\n",
      "Epoch: 51 | Batch: 010 / 025 | Total loss: 2.729 | Reg loss: 0.029 | Tree loss: 2.729 | Accuracy: 0.156250 | 0.071 sec/iter\n",
      "Epoch: 51 | Batch: 011 / 025 | Total loss: 2.716 | Reg loss: 0.029 | Tree loss: 2.716 | Accuracy: 0.169922 | 0.071 sec/iter\n",
      "Epoch: 51 | Batch: 012 / 025 | Total loss: 2.731 | Reg loss: 0.029 | Tree loss: 2.731 | Accuracy: 0.134766 | 0.071 sec/iter\n",
      "Epoch: 51 | Batch: 013 / 025 | Total loss: 2.766 | Reg loss: 0.030 | Tree loss: 2.766 | Accuracy: 0.136719 | 0.071 sec/iter\n",
      "Epoch: 51 | Batch: 014 / 025 | Total loss: 2.669 | Reg loss: 0.030 | Tree loss: 2.669 | Accuracy: 0.144531 | 0.071 sec/iter\n",
      "Epoch: 51 | Batch: 015 / 025 | Total loss: 2.649 | Reg loss: 0.030 | Tree loss: 2.649 | Accuracy: 0.150391 | 0.071 sec/iter\n",
      "Epoch: 51 | Batch: 016 / 025 | Total loss: 2.682 | Reg loss: 0.030 | Tree loss: 2.682 | Accuracy: 0.111328 | 0.071 sec/iter\n",
      "Epoch: 51 | Batch: 017 / 025 | Total loss: 2.658 | Reg loss: 0.030 | Tree loss: 2.658 | Accuracy: 0.167969 | 0.071 sec/iter\n",
      "Epoch: 51 | Batch: 018 / 025 | Total loss: 2.671 | Reg loss: 0.030 | Tree loss: 2.671 | Accuracy: 0.154297 | 0.071 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 51 | Batch: 019 / 025 | Total loss: 2.641 | Reg loss: 0.030 | Tree loss: 2.641 | Accuracy: 0.134766 | 0.071 sec/iter\n",
      "Epoch: 51 | Batch: 020 / 025 | Total loss: 2.659 | Reg loss: 0.030 | Tree loss: 2.659 | Accuracy: 0.132812 | 0.071 sec/iter\n",
      "Epoch: 51 | Batch: 021 / 025 | Total loss: 2.610 | Reg loss: 0.030 | Tree loss: 2.610 | Accuracy: 0.136719 | 0.071 sec/iter\n",
      "Epoch: 51 | Batch: 022 / 025 | Total loss: 2.642 | Reg loss: 0.030 | Tree loss: 2.642 | Accuracy: 0.154297 | 0.071 sec/iter\n",
      "Epoch: 51 | Batch: 023 / 025 | Total loss: 2.628 | Reg loss: 0.030 | Tree loss: 2.628 | Accuracy: 0.140625 | 0.071 sec/iter\n",
      "Epoch: 51 | Batch: 024 / 025 | Total loss: 2.605 | Reg loss: 0.030 | Tree loss: 2.605 | Accuracy: 0.109375 | 0.071 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 52 | Batch: 000 / 025 | Total loss: 2.798 | Reg loss: 0.029 | Tree loss: 2.798 | Accuracy: 0.154297 | 0.071 sec/iter\n",
      "Epoch: 52 | Batch: 001 / 025 | Total loss: 2.751 | Reg loss: 0.029 | Tree loss: 2.751 | Accuracy: 0.167969 | 0.071 sec/iter\n",
      "Epoch: 52 | Batch: 002 / 025 | Total loss: 2.775 | Reg loss: 0.029 | Tree loss: 2.775 | Accuracy: 0.162109 | 0.071 sec/iter\n",
      "Epoch: 52 | Batch: 003 / 025 | Total loss: 2.761 | Reg loss: 0.029 | Tree loss: 2.761 | Accuracy: 0.173828 | 0.071 sec/iter\n",
      "Epoch: 52 | Batch: 004 / 025 | Total loss: 2.777 | Reg loss: 0.029 | Tree loss: 2.777 | Accuracy: 0.154297 | 0.071 sec/iter\n",
      "Epoch: 52 | Batch: 005 / 025 | Total loss: 2.787 | Reg loss: 0.029 | Tree loss: 2.787 | Accuracy: 0.125000 | 0.071 sec/iter\n",
      "Epoch: 52 | Batch: 006 / 025 | Total loss: 2.763 | Reg loss: 0.029 | Tree loss: 2.763 | Accuracy: 0.111328 | 0.071 sec/iter\n",
      "Epoch: 52 | Batch: 007 / 025 | Total loss: 2.734 | Reg loss: 0.029 | Tree loss: 2.734 | Accuracy: 0.117188 | 0.071 sec/iter\n",
      "Epoch: 52 | Batch: 008 / 025 | Total loss: 2.798 | Reg loss: 0.029 | Tree loss: 2.798 | Accuracy: 0.119141 | 0.071 sec/iter\n",
      "Epoch: 52 | Batch: 009 / 025 | Total loss: 2.720 | Reg loss: 0.029 | Tree loss: 2.720 | Accuracy: 0.148438 | 0.071 sec/iter\n",
      "Epoch: 52 | Batch: 010 / 025 | Total loss: 2.724 | Reg loss: 0.029 | Tree loss: 2.724 | Accuracy: 0.107422 | 0.071 sec/iter\n",
      "Epoch: 52 | Batch: 011 / 025 | Total loss: 2.673 | Reg loss: 0.030 | Tree loss: 2.673 | Accuracy: 0.140625 | 0.071 sec/iter\n",
      "Epoch: 52 | Batch: 012 / 025 | Total loss: 2.726 | Reg loss: 0.030 | Tree loss: 2.726 | Accuracy: 0.109375 | 0.071 sec/iter\n",
      "Epoch: 52 | Batch: 013 / 025 | Total loss: 2.678 | Reg loss: 0.030 | Tree loss: 2.678 | Accuracy: 0.152344 | 0.071 sec/iter\n",
      "Epoch: 52 | Batch: 014 / 025 | Total loss: 2.652 | Reg loss: 0.030 | Tree loss: 2.652 | Accuracy: 0.142578 | 0.071 sec/iter\n",
      "Epoch: 52 | Batch: 015 / 025 | Total loss: 2.629 | Reg loss: 0.030 | Tree loss: 2.629 | Accuracy: 0.150391 | 0.071 sec/iter\n",
      "Epoch: 52 | Batch: 016 / 025 | Total loss: 2.658 | Reg loss: 0.030 | Tree loss: 2.658 | Accuracy: 0.175781 | 0.071 sec/iter\n",
      "Epoch: 52 | Batch: 017 / 025 | Total loss: 2.633 | Reg loss: 0.030 | Tree loss: 2.633 | Accuracy: 0.138672 | 0.071 sec/iter\n",
      "Epoch: 52 | Batch: 018 / 025 | Total loss: 2.639 | Reg loss: 0.030 | Tree loss: 2.639 | Accuracy: 0.117188 | 0.071 sec/iter\n",
      "Epoch: 52 | Batch: 019 / 025 | Total loss: 2.616 | Reg loss: 0.030 | Tree loss: 2.616 | Accuracy: 0.140625 | 0.071 sec/iter\n",
      "Epoch: 52 | Batch: 020 / 025 | Total loss: 2.644 | Reg loss: 0.030 | Tree loss: 2.644 | Accuracy: 0.123047 | 0.071 sec/iter\n",
      "Epoch: 52 | Batch: 021 / 025 | Total loss: 2.615 | Reg loss: 0.030 | Tree loss: 2.615 | Accuracy: 0.125000 | 0.071 sec/iter\n",
      "Epoch: 52 | Batch: 022 / 025 | Total loss: 2.635 | Reg loss: 0.030 | Tree loss: 2.635 | Accuracy: 0.113281 | 0.071 sec/iter\n",
      "Epoch: 52 | Batch: 023 / 025 | Total loss: 2.582 | Reg loss: 0.030 | Tree loss: 2.582 | Accuracy: 0.154297 | 0.071 sec/iter\n",
      "Epoch: 52 | Batch: 024 / 025 | Total loss: 2.571 | Reg loss: 0.030 | Tree loss: 2.571 | Accuracy: 0.140625 | 0.071 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 53 | Batch: 000 / 025 | Total loss: 2.773 | Reg loss: 0.029 | Tree loss: 2.773 | Accuracy: 0.154297 | 0.071 sec/iter\n",
      "Epoch: 53 | Batch: 001 / 025 | Total loss: 2.779 | Reg loss: 0.029 | Tree loss: 2.779 | Accuracy: 0.144531 | 0.071 sec/iter\n",
      "Epoch: 53 | Batch: 002 / 025 | Total loss: 2.742 | Reg loss: 0.029 | Tree loss: 2.742 | Accuracy: 0.158203 | 0.071 sec/iter\n",
      "Epoch: 53 | Batch: 003 / 025 | Total loss: 2.732 | Reg loss: 0.029 | Tree loss: 2.732 | Accuracy: 0.134766 | 0.071 sec/iter\n",
      "Epoch: 53 | Batch: 004 / 025 | Total loss: 2.748 | Reg loss: 0.029 | Tree loss: 2.748 | Accuracy: 0.123047 | 0.071 sec/iter\n",
      "Epoch: 53 | Batch: 005 / 025 | Total loss: 2.727 | Reg loss: 0.029 | Tree loss: 2.727 | Accuracy: 0.138672 | 0.071 sec/iter\n",
      "Epoch: 53 | Batch: 006 / 025 | Total loss: 2.731 | Reg loss: 0.029 | Tree loss: 2.731 | Accuracy: 0.119141 | 0.071 sec/iter\n",
      "Epoch: 53 | Batch: 007 / 025 | Total loss: 2.711 | Reg loss: 0.029 | Tree loss: 2.711 | Accuracy: 0.125000 | 0.071 sec/iter\n",
      "Epoch: 53 | Batch: 008 / 025 | Total loss: 2.689 | Reg loss: 0.029 | Tree loss: 2.689 | Accuracy: 0.121094 | 0.071 sec/iter\n",
      "Epoch: 53 | Batch: 009 / 025 | Total loss: 2.673 | Reg loss: 0.030 | Tree loss: 2.673 | Accuracy: 0.140625 | 0.071 sec/iter\n",
      "Epoch: 53 | Batch: 010 / 025 | Total loss: 2.674 | Reg loss: 0.030 | Tree loss: 2.674 | Accuracy: 0.150391 | 0.071 sec/iter\n",
      "Epoch: 53 | Batch: 011 / 025 | Total loss: 2.680 | Reg loss: 0.030 | Tree loss: 2.680 | Accuracy: 0.154297 | 0.071 sec/iter\n",
      "Epoch: 53 | Batch: 012 / 025 | Total loss: 2.654 | Reg loss: 0.030 | Tree loss: 2.654 | Accuracy: 0.128906 | 0.071 sec/iter\n",
      "Epoch: 53 | Batch: 013 / 025 | Total loss: 2.697 | Reg loss: 0.030 | Tree loss: 2.697 | Accuracy: 0.132812 | 0.071 sec/iter\n",
      "Epoch: 53 | Batch: 014 / 025 | Total loss: 2.668 | Reg loss: 0.030 | Tree loss: 2.668 | Accuracy: 0.158203 | 0.071 sec/iter\n",
      "Epoch: 53 | Batch: 015 / 025 | Total loss: 2.613 | Reg loss: 0.030 | Tree loss: 2.613 | Accuracy: 0.130859 | 0.071 sec/iter\n",
      "Epoch: 53 | Batch: 016 / 025 | Total loss: 2.614 | Reg loss: 0.030 | Tree loss: 2.614 | Accuracy: 0.173828 | 0.071 sec/iter\n",
      "Epoch: 53 | Batch: 017 / 025 | Total loss: 2.612 | Reg loss: 0.030 | Tree loss: 2.612 | Accuracy: 0.140625 | 0.071 sec/iter\n",
      "Epoch: 53 | Batch: 018 / 025 | Total loss: 2.618 | Reg loss: 0.030 | Tree loss: 2.618 | Accuracy: 0.148438 | 0.071 sec/iter\n",
      "Epoch: 53 | Batch: 019 / 025 | Total loss: 2.603 | Reg loss: 0.030 | Tree loss: 2.603 | Accuracy: 0.142578 | 0.071 sec/iter\n",
      "Epoch: 53 | Batch: 020 / 025 | Total loss: 2.615 | Reg loss: 0.030 | Tree loss: 2.615 | Accuracy: 0.134766 | 0.071 sec/iter\n",
      "Epoch: 53 | Batch: 021 / 025 | Total loss: 2.650 | Reg loss: 0.030 | Tree loss: 2.650 | Accuracy: 0.121094 | 0.071 sec/iter\n",
      "Epoch: 53 | Batch: 022 / 025 | Total loss: 2.601 | Reg loss: 0.030 | Tree loss: 2.601 | Accuracy: 0.156250 | 0.071 sec/iter\n",
      "Epoch: 53 | Batch: 023 / 025 | Total loss: 2.578 | Reg loss: 0.030 | Tree loss: 2.578 | Accuracy: 0.160156 | 0.071 sec/iter\n",
      "Epoch: 53 | Batch: 024 / 025 | Total loss: 2.585 | Reg loss: 0.030 | Tree loss: 2.585 | Accuracy: 0.128125 | 0.071 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 54 | Batch: 000 / 025 | Total loss: 2.800 | Reg loss: 0.029 | Tree loss: 2.800 | Accuracy: 0.138672 | 0.071 sec/iter\n",
      "Epoch: 54 | Batch: 001 / 025 | Total loss: 2.774 | Reg loss: 0.029 | Tree loss: 2.774 | Accuracy: 0.152344 | 0.071 sec/iter\n",
      "Epoch: 54 | Batch: 002 / 025 | Total loss: 2.746 | Reg loss: 0.029 | Tree loss: 2.746 | Accuracy: 0.150391 | 0.071 sec/iter\n",
      "Epoch: 54 | Batch: 003 / 025 | Total loss: 2.707 | Reg loss: 0.029 | Tree loss: 2.707 | Accuracy: 0.166016 | 0.071 sec/iter\n",
      "Epoch: 54 | Batch: 004 / 025 | Total loss: 2.780 | Reg loss: 0.029 | Tree loss: 2.780 | Accuracy: 0.136719 | 0.071 sec/iter\n",
      "Epoch: 54 | Batch: 005 / 025 | Total loss: 2.696 | Reg loss: 0.029 | Tree loss: 2.696 | Accuracy: 0.144531 | 0.071 sec/iter\n",
      "Epoch: 54 | Batch: 006 / 025 | Total loss: 2.669 | Reg loss: 0.029 | Tree loss: 2.669 | Accuracy: 0.138672 | 0.071 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 54 | Batch: 007 / 025 | Total loss: 2.684 | Reg loss: 0.030 | Tree loss: 2.684 | Accuracy: 0.142578 | 0.071 sec/iter\n",
      "Epoch: 54 | Batch: 008 / 025 | Total loss: 2.648 | Reg loss: 0.030 | Tree loss: 2.648 | Accuracy: 0.136719 | 0.071 sec/iter\n",
      "Epoch: 54 | Batch: 009 / 025 | Total loss: 2.694 | Reg loss: 0.030 | Tree loss: 2.694 | Accuracy: 0.103516 | 0.071 sec/iter\n",
      "Epoch: 54 | Batch: 010 / 025 | Total loss: 2.660 | Reg loss: 0.030 | Tree loss: 2.660 | Accuracy: 0.154297 | 0.071 sec/iter\n",
      "Epoch: 54 | Batch: 011 / 025 | Total loss: 2.680 | Reg loss: 0.030 | Tree loss: 2.680 | Accuracy: 0.148438 | 0.071 sec/iter\n",
      "Epoch: 54 | Batch: 012 / 025 | Total loss: 2.609 | Reg loss: 0.030 | Tree loss: 2.609 | Accuracy: 0.125000 | 0.071 sec/iter\n",
      "Epoch: 54 | Batch: 013 / 025 | Total loss: 2.609 | Reg loss: 0.030 | Tree loss: 2.609 | Accuracy: 0.154297 | 0.071 sec/iter\n",
      "Epoch: 54 | Batch: 014 / 025 | Total loss: 2.640 | Reg loss: 0.030 | Tree loss: 2.640 | Accuracy: 0.134766 | 0.071 sec/iter\n",
      "Epoch: 54 | Batch: 015 / 025 | Total loss: 2.572 | Reg loss: 0.030 | Tree loss: 2.572 | Accuracy: 0.144531 | 0.071 sec/iter\n",
      "Epoch: 54 | Batch: 016 / 025 | Total loss: 2.640 | Reg loss: 0.030 | Tree loss: 2.640 | Accuracy: 0.142578 | 0.071 sec/iter\n",
      "Epoch: 54 | Batch: 017 / 025 | Total loss: 2.610 | Reg loss: 0.030 | Tree loss: 2.610 | Accuracy: 0.121094 | 0.071 sec/iter\n",
      "Epoch: 54 | Batch: 018 / 025 | Total loss: 2.626 | Reg loss: 0.030 | Tree loss: 2.626 | Accuracy: 0.130859 | 0.071 sec/iter\n",
      "Epoch: 54 | Batch: 019 / 025 | Total loss: 2.552 | Reg loss: 0.030 | Tree loss: 2.552 | Accuracy: 0.160156 | 0.071 sec/iter\n",
      "Epoch: 54 | Batch: 020 / 025 | Total loss: 2.581 | Reg loss: 0.030 | Tree loss: 2.581 | Accuracy: 0.142578 | 0.071 sec/iter\n",
      "Epoch: 54 | Batch: 021 / 025 | Total loss: 2.600 | Reg loss: 0.030 | Tree loss: 2.600 | Accuracy: 0.117188 | 0.071 sec/iter\n",
      "Epoch: 54 | Batch: 022 / 025 | Total loss: 2.533 | Reg loss: 0.030 | Tree loss: 2.533 | Accuracy: 0.156250 | 0.071 sec/iter\n",
      "Epoch: 54 | Batch: 023 / 025 | Total loss: 2.551 | Reg loss: 0.030 | Tree loss: 2.551 | Accuracy: 0.126953 | 0.071 sec/iter\n",
      "Epoch: 54 | Batch: 024 / 025 | Total loss: 2.554 | Reg loss: 0.030 | Tree loss: 2.554 | Accuracy: 0.146875 | 0.071 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 55 | Batch: 000 / 025 | Total loss: 2.733 | Reg loss: 0.029 | Tree loss: 2.733 | Accuracy: 0.146484 | 0.071 sec/iter\n",
      "Epoch: 55 | Batch: 001 / 025 | Total loss: 2.723 | Reg loss: 0.029 | Tree loss: 2.723 | Accuracy: 0.142578 | 0.071 sec/iter\n",
      "Epoch: 55 | Batch: 002 / 025 | Total loss: 2.737 | Reg loss: 0.030 | Tree loss: 2.737 | Accuracy: 0.162109 | 0.071 sec/iter\n",
      "Epoch: 55 | Batch: 003 / 025 | Total loss: 2.696 | Reg loss: 0.030 | Tree loss: 2.696 | Accuracy: 0.152344 | 0.071 sec/iter\n",
      "Epoch: 55 | Batch: 004 / 025 | Total loss: 2.703 | Reg loss: 0.030 | Tree loss: 2.703 | Accuracy: 0.144531 | 0.071 sec/iter\n",
      "Epoch: 55 | Batch: 005 / 025 | Total loss: 2.655 | Reg loss: 0.030 | Tree loss: 2.655 | Accuracy: 0.150391 | 0.071 sec/iter\n",
      "Epoch: 55 | Batch: 006 / 025 | Total loss: 2.724 | Reg loss: 0.030 | Tree loss: 2.724 | Accuracy: 0.117188 | 0.071 sec/iter\n",
      "Epoch: 55 | Batch: 007 / 025 | Total loss: 2.730 | Reg loss: 0.030 | Tree loss: 2.730 | Accuracy: 0.117188 | 0.071 sec/iter\n",
      "Epoch: 55 | Batch: 008 / 025 | Total loss: 2.641 | Reg loss: 0.030 | Tree loss: 2.641 | Accuracy: 0.152344 | 0.071 sec/iter\n",
      "Epoch: 55 | Batch: 009 / 025 | Total loss: 2.696 | Reg loss: 0.030 | Tree loss: 2.696 | Accuracy: 0.134766 | 0.071 sec/iter\n",
      "Epoch: 55 | Batch: 010 / 025 | Total loss: 2.652 | Reg loss: 0.030 | Tree loss: 2.652 | Accuracy: 0.142578 | 0.071 sec/iter\n",
      "Epoch: 55 | Batch: 011 / 025 | Total loss: 2.625 | Reg loss: 0.030 | Tree loss: 2.625 | Accuracy: 0.156250 | 0.071 sec/iter\n",
      "Epoch: 55 | Batch: 012 / 025 | Total loss: 2.626 | Reg loss: 0.030 | Tree loss: 2.626 | Accuracy: 0.142578 | 0.071 sec/iter\n",
      "Epoch: 55 | Batch: 013 / 025 | Total loss: 2.628 | Reg loss: 0.030 | Tree loss: 2.628 | Accuracy: 0.160156 | 0.071 sec/iter\n",
      "Epoch: 55 | Batch: 014 / 025 | Total loss: 2.584 | Reg loss: 0.030 | Tree loss: 2.584 | Accuracy: 0.115234 | 0.071 sec/iter\n",
      "Epoch: 55 | Batch: 015 / 025 | Total loss: 2.591 | Reg loss: 0.030 | Tree loss: 2.591 | Accuracy: 0.113281 | 0.071 sec/iter\n",
      "Epoch: 55 | Batch: 016 / 025 | Total loss: 2.574 | Reg loss: 0.030 | Tree loss: 2.574 | Accuracy: 0.164062 | 0.071 sec/iter\n",
      "Epoch: 55 | Batch: 017 / 025 | Total loss: 2.561 | Reg loss: 0.030 | Tree loss: 2.561 | Accuracy: 0.125000 | 0.071 sec/iter\n",
      "Epoch: 55 | Batch: 018 / 025 | Total loss: 2.574 | Reg loss: 0.030 | Tree loss: 2.574 | Accuracy: 0.148438 | 0.071 sec/iter\n",
      "Epoch: 55 | Batch: 019 / 025 | Total loss: 2.539 | Reg loss: 0.030 | Tree loss: 2.539 | Accuracy: 0.140625 | 0.071 sec/iter\n",
      "Epoch: 55 | Batch: 020 / 025 | Total loss: 2.552 | Reg loss: 0.030 | Tree loss: 2.552 | Accuracy: 0.140625 | 0.071 sec/iter\n",
      "Epoch: 55 | Batch: 021 / 025 | Total loss: 2.531 | Reg loss: 0.030 | Tree loss: 2.531 | Accuracy: 0.166016 | 0.071 sec/iter\n",
      "Epoch: 55 | Batch: 022 / 025 | Total loss: 2.563 | Reg loss: 0.030 | Tree loss: 2.563 | Accuracy: 0.142578 | 0.071 sec/iter\n",
      "Epoch: 55 | Batch: 023 / 025 | Total loss: 2.541 | Reg loss: 0.030 | Tree loss: 2.541 | Accuracy: 0.171875 | 0.071 sec/iter\n",
      "Epoch: 55 | Batch: 024 / 025 | Total loss: 2.525 | Reg loss: 0.030 | Tree loss: 2.525 | Accuracy: 0.125000 | 0.071 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 56 | Batch: 000 / 025 | Total loss: 2.696 | Reg loss: 0.030 | Tree loss: 2.696 | Accuracy: 0.169922 | 0.071 sec/iter\n",
      "Epoch: 56 | Batch: 001 / 025 | Total loss: 2.705 | Reg loss: 0.030 | Tree loss: 2.705 | Accuracy: 0.150391 | 0.071 sec/iter\n",
      "Epoch: 56 | Batch: 002 / 025 | Total loss: 2.692 | Reg loss: 0.030 | Tree loss: 2.692 | Accuracy: 0.152344 | 0.071 sec/iter\n",
      "Epoch: 56 | Batch: 003 / 025 | Total loss: 2.711 | Reg loss: 0.030 | Tree loss: 2.711 | Accuracy: 0.144531 | 0.071 sec/iter\n",
      "Epoch: 56 | Batch: 004 / 025 | Total loss: 2.716 | Reg loss: 0.030 | Tree loss: 2.716 | Accuracy: 0.142578 | 0.071 sec/iter\n",
      "Epoch: 56 | Batch: 005 / 025 | Total loss: 2.657 | Reg loss: 0.030 | Tree loss: 2.657 | Accuracy: 0.146484 | 0.071 sec/iter\n",
      "Epoch: 56 | Batch: 006 / 025 | Total loss: 2.653 | Reg loss: 0.030 | Tree loss: 2.653 | Accuracy: 0.152344 | 0.071 sec/iter\n",
      "Epoch: 56 | Batch: 007 / 025 | Total loss: 2.662 | Reg loss: 0.030 | Tree loss: 2.662 | Accuracy: 0.126953 | 0.071 sec/iter\n",
      "Epoch: 56 | Batch: 008 / 025 | Total loss: 2.638 | Reg loss: 0.030 | Tree loss: 2.638 | Accuracy: 0.171875 | 0.071 sec/iter\n",
      "Epoch: 56 | Batch: 009 / 025 | Total loss: 2.620 | Reg loss: 0.030 | Tree loss: 2.620 | Accuracy: 0.167969 | 0.071 sec/iter\n",
      "Epoch: 56 | Batch: 010 / 025 | Total loss: 2.654 | Reg loss: 0.030 | Tree loss: 2.654 | Accuracy: 0.134766 | 0.071 sec/iter\n",
      "Epoch: 56 | Batch: 011 / 025 | Total loss: 2.623 | Reg loss: 0.030 | Tree loss: 2.623 | Accuracy: 0.130859 | 0.071 sec/iter\n",
      "Epoch: 56 | Batch: 012 / 025 | Total loss: 2.593 | Reg loss: 0.030 | Tree loss: 2.593 | Accuracy: 0.128906 | 0.071 sec/iter\n",
      "Epoch: 56 | Batch: 013 / 025 | Total loss: 2.573 | Reg loss: 0.030 | Tree loss: 2.573 | Accuracy: 0.152344 | 0.071 sec/iter\n",
      "Epoch: 56 | Batch: 014 / 025 | Total loss: 2.552 | Reg loss: 0.030 | Tree loss: 2.552 | Accuracy: 0.136719 | 0.071 sec/iter\n",
      "Epoch: 56 | Batch: 015 / 025 | Total loss: 2.602 | Reg loss: 0.030 | Tree loss: 2.602 | Accuracy: 0.115234 | 0.071 sec/iter\n",
      "Epoch: 56 | Batch: 016 / 025 | Total loss: 2.561 | Reg loss: 0.030 | Tree loss: 2.561 | Accuracy: 0.164062 | 0.071 sec/iter\n",
      "Epoch: 56 | Batch: 017 / 025 | Total loss: 2.601 | Reg loss: 0.030 | Tree loss: 2.601 | Accuracy: 0.136719 | 0.071 sec/iter\n",
      "Epoch: 56 | Batch: 018 / 025 | Total loss: 2.501 | Reg loss: 0.030 | Tree loss: 2.501 | Accuracy: 0.142578 | 0.071 sec/iter\n",
      "Epoch: 56 | Batch: 019 / 025 | Total loss: 2.536 | Reg loss: 0.030 | Tree loss: 2.536 | Accuracy: 0.142578 | 0.071 sec/iter\n",
      "Epoch: 56 | Batch: 020 / 025 | Total loss: 2.539 | Reg loss: 0.030 | Tree loss: 2.539 | Accuracy: 0.125000 | 0.071 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 56 | Batch: 021 / 025 | Total loss: 2.548 | Reg loss: 0.030 | Tree loss: 2.548 | Accuracy: 0.130859 | 0.071 sec/iter\n",
      "Epoch: 56 | Batch: 022 / 025 | Total loss: 2.517 | Reg loss: 0.030 | Tree loss: 2.517 | Accuracy: 0.154297 | 0.071 sec/iter\n",
      "Epoch: 56 | Batch: 023 / 025 | Total loss: 2.540 | Reg loss: 0.030 | Tree loss: 2.540 | Accuracy: 0.144531 | 0.071 sec/iter\n",
      "Epoch: 56 | Batch: 024 / 025 | Total loss: 2.571 | Reg loss: 0.030 | Tree loss: 2.571 | Accuracy: 0.125000 | 0.071 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 57 | Batch: 000 / 025 | Total loss: 2.654 | Reg loss: 0.030 | Tree loss: 2.654 | Accuracy: 0.173828 | 0.071 sec/iter\n",
      "Epoch: 57 | Batch: 001 / 025 | Total loss: 2.697 | Reg loss: 0.030 | Tree loss: 2.697 | Accuracy: 0.166016 | 0.071 sec/iter\n",
      "Epoch: 57 | Batch: 002 / 025 | Total loss: 2.673 | Reg loss: 0.030 | Tree loss: 2.673 | Accuracy: 0.146484 | 0.071 sec/iter\n",
      "Epoch: 57 | Batch: 003 / 025 | Total loss: 2.650 | Reg loss: 0.030 | Tree loss: 2.650 | Accuracy: 0.191406 | 0.071 sec/iter\n",
      "Epoch: 57 | Batch: 004 / 025 | Total loss: 2.649 | Reg loss: 0.030 | Tree loss: 2.649 | Accuracy: 0.148438 | 0.071 sec/iter\n",
      "Epoch: 57 | Batch: 005 / 025 | Total loss: 2.647 | Reg loss: 0.030 | Tree loss: 2.647 | Accuracy: 0.132812 | 0.071 sec/iter\n",
      "Epoch: 57 | Batch: 006 / 025 | Total loss: 2.618 | Reg loss: 0.030 | Tree loss: 2.618 | Accuracy: 0.171875 | 0.071 sec/iter\n",
      "Epoch: 57 | Batch: 007 / 025 | Total loss: 2.650 | Reg loss: 0.030 | Tree loss: 2.650 | Accuracy: 0.132812 | 0.071 sec/iter\n",
      "Epoch: 57 | Batch: 008 / 025 | Total loss: 2.613 | Reg loss: 0.030 | Tree loss: 2.613 | Accuracy: 0.166016 | 0.071 sec/iter\n",
      "Epoch: 57 | Batch: 009 / 025 | Total loss: 2.608 | Reg loss: 0.030 | Tree loss: 2.608 | Accuracy: 0.160156 | 0.071 sec/iter\n",
      "Epoch: 57 | Batch: 010 / 025 | Total loss: 2.602 | Reg loss: 0.030 | Tree loss: 2.602 | Accuracy: 0.128906 | 0.071 sec/iter\n",
      "Epoch: 57 | Batch: 011 / 025 | Total loss: 2.655 | Reg loss: 0.030 | Tree loss: 2.655 | Accuracy: 0.132812 | 0.071 sec/iter\n",
      "Epoch: 57 | Batch: 012 / 025 | Total loss: 2.602 | Reg loss: 0.030 | Tree loss: 2.602 | Accuracy: 0.138672 | 0.071 sec/iter\n",
      "Epoch: 57 | Batch: 013 / 025 | Total loss: 2.596 | Reg loss: 0.030 | Tree loss: 2.596 | Accuracy: 0.136719 | 0.071 sec/iter\n",
      "Epoch: 57 | Batch: 014 / 025 | Total loss: 2.567 | Reg loss: 0.030 | Tree loss: 2.567 | Accuracy: 0.146484 | 0.071 sec/iter\n",
      "Epoch: 57 | Batch: 015 / 025 | Total loss: 2.574 | Reg loss: 0.030 | Tree loss: 2.574 | Accuracy: 0.115234 | 0.071 sec/iter\n",
      "Epoch: 57 | Batch: 016 / 025 | Total loss: 2.564 | Reg loss: 0.030 | Tree loss: 2.564 | Accuracy: 0.154297 | 0.071 sec/iter\n",
      "Epoch: 57 | Batch: 017 / 025 | Total loss: 2.560 | Reg loss: 0.030 | Tree loss: 2.560 | Accuracy: 0.142578 | 0.071 sec/iter\n",
      "Epoch: 57 | Batch: 018 / 025 | Total loss: 2.576 | Reg loss: 0.030 | Tree loss: 2.576 | Accuracy: 0.160156 | 0.071 sec/iter\n",
      "Epoch: 57 | Batch: 019 / 025 | Total loss: 2.516 | Reg loss: 0.030 | Tree loss: 2.516 | Accuracy: 0.126953 | 0.071 sec/iter\n",
      "Epoch: 57 | Batch: 020 / 025 | Total loss: 2.502 | Reg loss: 0.030 | Tree loss: 2.502 | Accuracy: 0.164062 | 0.071 sec/iter\n",
      "Epoch: 57 | Batch: 021 / 025 | Total loss: 2.510 | Reg loss: 0.030 | Tree loss: 2.510 | Accuracy: 0.117188 | 0.071 sec/iter\n",
      "Epoch: 57 | Batch: 022 / 025 | Total loss: 2.549 | Reg loss: 0.030 | Tree loss: 2.549 | Accuracy: 0.140625 | 0.071 sec/iter\n",
      "Epoch: 57 | Batch: 023 / 025 | Total loss: 2.483 | Reg loss: 0.030 | Tree loss: 2.483 | Accuracy: 0.123047 | 0.071 sec/iter\n",
      "Epoch: 57 | Batch: 024 / 025 | Total loss: 2.491 | Reg loss: 0.030 | Tree loss: 2.491 | Accuracy: 0.150000 | 0.071 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 58 | Batch: 000 / 025 | Total loss: 2.669 | Reg loss: 0.030 | Tree loss: 2.669 | Accuracy: 0.164062 | 0.071 sec/iter\n",
      "Epoch: 58 | Batch: 001 / 025 | Total loss: 2.684 | Reg loss: 0.030 | Tree loss: 2.684 | Accuracy: 0.142578 | 0.071 sec/iter\n",
      "Epoch: 58 | Batch: 002 / 025 | Total loss: 2.649 | Reg loss: 0.030 | Tree loss: 2.649 | Accuracy: 0.167969 | 0.071 sec/iter\n",
      "Epoch: 58 | Batch: 003 / 025 | Total loss: 2.652 | Reg loss: 0.030 | Tree loss: 2.652 | Accuracy: 0.162109 | 0.071 sec/iter\n",
      "Epoch: 58 | Batch: 004 / 025 | Total loss: 2.625 | Reg loss: 0.030 | Tree loss: 2.625 | Accuracy: 0.162109 | 0.071 sec/iter\n",
      "Epoch: 58 | Batch: 005 / 025 | Total loss: 2.626 | Reg loss: 0.030 | Tree loss: 2.626 | Accuracy: 0.177734 | 0.071 sec/iter\n",
      "Epoch: 58 | Batch: 006 / 025 | Total loss: 2.649 | Reg loss: 0.030 | Tree loss: 2.649 | Accuracy: 0.148438 | 0.071 sec/iter\n",
      "Epoch: 58 | Batch: 007 / 025 | Total loss: 2.623 | Reg loss: 0.030 | Tree loss: 2.623 | Accuracy: 0.142578 | 0.071 sec/iter\n",
      "Epoch: 58 | Batch: 008 / 025 | Total loss: 2.614 | Reg loss: 0.030 | Tree loss: 2.614 | Accuracy: 0.121094 | 0.071 sec/iter\n",
      "Epoch: 58 | Batch: 009 / 025 | Total loss: 2.597 | Reg loss: 0.030 | Tree loss: 2.597 | Accuracy: 0.148438 | 0.071 sec/iter\n",
      "Epoch: 58 | Batch: 010 / 025 | Total loss: 2.600 | Reg loss: 0.030 | Tree loss: 2.600 | Accuracy: 0.134766 | 0.071 sec/iter\n",
      "Epoch: 58 | Batch: 011 / 025 | Total loss: 2.587 | Reg loss: 0.030 | Tree loss: 2.587 | Accuracy: 0.158203 | 0.071 sec/iter\n",
      "Epoch: 58 | Batch: 012 / 025 | Total loss: 2.550 | Reg loss: 0.030 | Tree loss: 2.550 | Accuracy: 0.158203 | 0.071 sec/iter\n",
      "Epoch: 58 | Batch: 013 / 025 | Total loss: 2.566 | Reg loss: 0.030 | Tree loss: 2.566 | Accuracy: 0.144531 | 0.071 sec/iter\n",
      "Epoch: 58 | Batch: 014 / 025 | Total loss: 2.563 | Reg loss: 0.030 | Tree loss: 2.563 | Accuracy: 0.158203 | 0.071 sec/iter\n",
      "Epoch: 58 | Batch: 015 / 025 | Total loss: 2.582 | Reg loss: 0.030 | Tree loss: 2.582 | Accuracy: 0.158203 | 0.071 sec/iter\n",
      "Epoch: 58 | Batch: 016 / 025 | Total loss: 2.572 | Reg loss: 0.030 | Tree loss: 2.572 | Accuracy: 0.132812 | 0.071 sec/iter\n",
      "Epoch: 58 | Batch: 017 / 025 | Total loss: 2.531 | Reg loss: 0.030 | Tree loss: 2.531 | Accuracy: 0.158203 | 0.071 sec/iter\n",
      "Epoch: 58 | Batch: 018 / 025 | Total loss: 2.504 | Reg loss: 0.030 | Tree loss: 2.504 | Accuracy: 0.138672 | 0.071 sec/iter\n",
      "Epoch: 58 | Batch: 019 / 025 | Total loss: 2.508 | Reg loss: 0.030 | Tree loss: 2.508 | Accuracy: 0.160156 | 0.071 sec/iter\n",
      "Epoch: 58 | Batch: 020 / 025 | Total loss: 2.499 | Reg loss: 0.030 | Tree loss: 2.499 | Accuracy: 0.130859 | 0.071 sec/iter\n",
      "Epoch: 58 | Batch: 021 / 025 | Total loss: 2.491 | Reg loss: 0.030 | Tree loss: 2.491 | Accuracy: 0.132812 | 0.071 sec/iter\n",
      "Epoch: 58 | Batch: 022 / 025 | Total loss: 2.471 | Reg loss: 0.030 | Tree loss: 2.471 | Accuracy: 0.152344 | 0.071 sec/iter\n",
      "Epoch: 58 | Batch: 023 / 025 | Total loss: 2.498 | Reg loss: 0.030 | Tree loss: 2.498 | Accuracy: 0.136719 | 0.071 sec/iter\n",
      "Epoch: 58 | Batch: 024 / 025 | Total loss: 2.483 | Reg loss: 0.030 | Tree loss: 2.483 | Accuracy: 0.128125 | 0.071 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 59 | Batch: 000 / 025 | Total loss: 2.676 | Reg loss: 0.030 | Tree loss: 2.676 | Accuracy: 0.140625 | 0.071 sec/iter\n",
      "Epoch: 59 | Batch: 001 / 025 | Total loss: 2.667 | Reg loss: 0.030 | Tree loss: 2.667 | Accuracy: 0.162109 | 0.071 sec/iter\n",
      "Epoch: 59 | Batch: 002 / 025 | Total loss: 2.645 | Reg loss: 0.030 | Tree loss: 2.645 | Accuracy: 0.171875 | 0.071 sec/iter\n",
      "Epoch: 59 | Batch: 003 / 025 | Total loss: 2.646 | Reg loss: 0.030 | Tree loss: 2.646 | Accuracy: 0.164062 | 0.071 sec/iter\n",
      "Epoch: 59 | Batch: 004 / 025 | Total loss: 2.632 | Reg loss: 0.030 | Tree loss: 2.632 | Accuracy: 0.158203 | 0.071 sec/iter\n",
      "Epoch: 59 | Batch: 005 / 025 | Total loss: 2.602 | Reg loss: 0.030 | Tree loss: 2.602 | Accuracy: 0.171875 | 0.071 sec/iter\n",
      "Epoch: 59 | Batch: 006 / 025 | Total loss: 2.588 | Reg loss: 0.030 | Tree loss: 2.588 | Accuracy: 0.164062 | 0.071 sec/iter\n",
      "Epoch: 59 | Batch: 007 / 025 | Total loss: 2.561 | Reg loss: 0.030 | Tree loss: 2.561 | Accuracy: 0.160156 | 0.071 sec/iter\n",
      "Epoch: 59 | Batch: 008 / 025 | Total loss: 2.550 | Reg loss: 0.030 | Tree loss: 2.550 | Accuracy: 0.167969 | 0.071 sec/iter\n",
      "Epoch: 59 | Batch: 009 / 025 | Total loss: 2.592 | Reg loss: 0.030 | Tree loss: 2.592 | Accuracy: 0.117188 | 0.071 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 59 | Batch: 010 / 025 | Total loss: 2.585 | Reg loss: 0.030 | Tree loss: 2.585 | Accuracy: 0.144531 | 0.071 sec/iter\n",
      "Epoch: 59 | Batch: 011 / 025 | Total loss: 2.554 | Reg loss: 0.030 | Tree loss: 2.554 | Accuracy: 0.166016 | 0.071 sec/iter\n",
      "Epoch: 59 | Batch: 012 / 025 | Total loss: 2.548 | Reg loss: 0.030 | Tree loss: 2.548 | Accuracy: 0.144531 | 0.071 sec/iter\n",
      "Epoch: 59 | Batch: 013 / 025 | Total loss: 2.582 | Reg loss: 0.030 | Tree loss: 2.582 | Accuracy: 0.177734 | 0.071 sec/iter\n",
      "Epoch: 59 | Batch: 014 / 025 | Total loss: 2.571 | Reg loss: 0.030 | Tree loss: 2.571 | Accuracy: 0.156250 | 0.071 sec/iter\n",
      "Epoch: 59 | Batch: 015 / 025 | Total loss: 2.533 | Reg loss: 0.030 | Tree loss: 2.533 | Accuracy: 0.156250 | 0.071 sec/iter\n",
      "Epoch: 59 | Batch: 016 / 025 | Total loss: 2.540 | Reg loss: 0.030 | Tree loss: 2.540 | Accuracy: 0.160156 | 0.071 sec/iter\n",
      "Epoch: 59 | Batch: 017 / 025 | Total loss: 2.515 | Reg loss: 0.030 | Tree loss: 2.515 | Accuracy: 0.154297 | 0.071 sec/iter\n",
      "Epoch: 59 | Batch: 018 / 025 | Total loss: 2.535 | Reg loss: 0.030 | Tree loss: 2.535 | Accuracy: 0.146484 | 0.071 sec/iter\n",
      "Epoch: 59 | Batch: 019 / 025 | Total loss: 2.516 | Reg loss: 0.030 | Tree loss: 2.516 | Accuracy: 0.146484 | 0.071 sec/iter\n",
      "Epoch: 59 | Batch: 020 / 025 | Total loss: 2.473 | Reg loss: 0.030 | Tree loss: 2.473 | Accuracy: 0.164062 | 0.071 sec/iter\n",
      "Epoch: 59 | Batch: 021 / 025 | Total loss: 2.496 | Reg loss: 0.030 | Tree loss: 2.496 | Accuracy: 0.140625 | 0.071 sec/iter\n",
      "Epoch: 59 | Batch: 022 / 025 | Total loss: 2.448 | Reg loss: 0.030 | Tree loss: 2.448 | Accuracy: 0.171875 | 0.071 sec/iter\n",
      "Epoch: 59 | Batch: 023 / 025 | Total loss: 2.484 | Reg loss: 0.030 | Tree loss: 2.484 | Accuracy: 0.142578 | 0.071 sec/iter\n",
      "Epoch: 59 | Batch: 024 / 025 | Total loss: 2.493 | Reg loss: 0.030 | Tree loss: 2.493 | Accuracy: 0.140625 | 0.071 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 60 | Batch: 000 / 025 | Total loss: 2.654 | Reg loss: 0.030 | Tree loss: 2.654 | Accuracy: 0.164062 | 0.071 sec/iter\n",
      "Epoch: 60 | Batch: 001 / 025 | Total loss: 2.677 | Reg loss: 0.030 | Tree loss: 2.677 | Accuracy: 0.156250 | 0.071 sec/iter\n",
      "Epoch: 60 | Batch: 002 / 025 | Total loss: 2.597 | Reg loss: 0.030 | Tree loss: 2.597 | Accuracy: 0.166016 | 0.071 sec/iter\n",
      "Epoch: 60 | Batch: 003 / 025 | Total loss: 2.604 | Reg loss: 0.030 | Tree loss: 2.604 | Accuracy: 0.156250 | 0.071 sec/iter\n",
      "Epoch: 60 | Batch: 004 / 025 | Total loss: 2.578 | Reg loss: 0.030 | Tree loss: 2.578 | Accuracy: 0.171875 | 0.071 sec/iter\n",
      "Epoch: 60 | Batch: 005 / 025 | Total loss: 2.567 | Reg loss: 0.030 | Tree loss: 2.567 | Accuracy: 0.166016 | 0.071 sec/iter\n",
      "Epoch: 60 | Batch: 006 / 025 | Total loss: 2.581 | Reg loss: 0.030 | Tree loss: 2.581 | Accuracy: 0.166016 | 0.071 sec/iter\n",
      "Epoch: 60 | Batch: 007 / 025 | Total loss: 2.606 | Reg loss: 0.030 | Tree loss: 2.606 | Accuracy: 0.142578 | 0.071 sec/iter\n",
      "Epoch: 60 | Batch: 008 / 025 | Total loss: 2.568 | Reg loss: 0.030 | Tree loss: 2.568 | Accuracy: 0.144531 | 0.071 sec/iter\n",
      "Epoch: 60 | Batch: 009 / 025 | Total loss: 2.529 | Reg loss: 0.030 | Tree loss: 2.529 | Accuracy: 0.191406 | 0.071 sec/iter\n",
      "Epoch: 60 | Batch: 010 / 025 | Total loss: 2.543 | Reg loss: 0.030 | Tree loss: 2.543 | Accuracy: 0.154297 | 0.071 sec/iter\n",
      "Epoch: 60 | Batch: 011 / 025 | Total loss: 2.525 | Reg loss: 0.030 | Tree loss: 2.525 | Accuracy: 0.160156 | 0.071 sec/iter\n",
      "Epoch: 60 | Batch: 012 / 025 | Total loss: 2.524 | Reg loss: 0.030 | Tree loss: 2.524 | Accuracy: 0.162109 | 0.071 sec/iter\n",
      "Epoch: 60 | Batch: 013 / 025 | Total loss: 2.525 | Reg loss: 0.030 | Tree loss: 2.525 | Accuracy: 0.130859 | 0.071 sec/iter\n",
      "Epoch: 60 | Batch: 014 / 025 | Total loss: 2.541 | Reg loss: 0.030 | Tree loss: 2.541 | Accuracy: 0.169922 | 0.071 sec/iter\n",
      "Epoch: 60 | Batch: 015 / 025 | Total loss: 2.500 | Reg loss: 0.030 | Tree loss: 2.500 | Accuracy: 0.156250 | 0.071 sec/iter\n",
      "Epoch: 60 | Batch: 016 / 025 | Total loss: 2.589 | Reg loss: 0.030 | Tree loss: 2.589 | Accuracy: 0.146484 | 0.071 sec/iter\n",
      "Epoch: 60 | Batch: 017 / 025 | Total loss: 2.515 | Reg loss: 0.030 | Tree loss: 2.515 | Accuracy: 0.121094 | 0.071 sec/iter\n",
      "Epoch: 60 | Batch: 018 / 025 | Total loss: 2.510 | Reg loss: 0.030 | Tree loss: 2.510 | Accuracy: 0.160156 | 0.071 sec/iter\n",
      "Epoch: 60 | Batch: 019 / 025 | Total loss: 2.537 | Reg loss: 0.030 | Tree loss: 2.537 | Accuracy: 0.125000 | 0.071 sec/iter\n",
      "Epoch: 60 | Batch: 020 / 025 | Total loss: 2.504 | Reg loss: 0.030 | Tree loss: 2.504 | Accuracy: 0.169922 | 0.071 sec/iter\n",
      "Epoch: 60 | Batch: 021 / 025 | Total loss: 2.498 | Reg loss: 0.030 | Tree loss: 2.498 | Accuracy: 0.148438 | 0.071 sec/iter\n",
      "Epoch: 60 | Batch: 022 / 025 | Total loss: 2.456 | Reg loss: 0.030 | Tree loss: 2.456 | Accuracy: 0.144531 | 0.071 sec/iter\n",
      "Epoch: 60 | Batch: 023 / 025 | Total loss: 2.479 | Reg loss: 0.030 | Tree loss: 2.479 | Accuracy: 0.142578 | 0.071 sec/iter\n",
      "Epoch: 60 | Batch: 024 / 025 | Total loss: 2.474 | Reg loss: 0.030 | Tree loss: 2.474 | Accuracy: 0.143750 | 0.071 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 61 | Batch: 000 / 025 | Total loss: 2.659 | Reg loss: 0.030 | Tree loss: 2.659 | Accuracy: 0.169922 | 0.071 sec/iter\n",
      "Epoch: 61 | Batch: 001 / 025 | Total loss: 2.613 | Reg loss: 0.030 | Tree loss: 2.613 | Accuracy: 0.177734 | 0.071 sec/iter\n",
      "Epoch: 61 | Batch: 002 / 025 | Total loss: 2.585 | Reg loss: 0.030 | Tree loss: 2.585 | Accuracy: 0.162109 | 0.071 sec/iter\n",
      "Epoch: 61 | Batch: 003 / 025 | Total loss: 2.597 | Reg loss: 0.030 | Tree loss: 2.597 | Accuracy: 0.160156 | 0.071 sec/iter\n",
      "Epoch: 61 | Batch: 004 / 025 | Total loss: 2.584 | Reg loss: 0.030 | Tree loss: 2.584 | Accuracy: 0.187500 | 0.071 sec/iter\n",
      "Epoch: 61 | Batch: 005 / 025 | Total loss: 2.609 | Reg loss: 0.030 | Tree loss: 2.609 | Accuracy: 0.154297 | 0.071 sec/iter\n",
      "Epoch: 61 | Batch: 006 / 025 | Total loss: 2.585 | Reg loss: 0.030 | Tree loss: 2.585 | Accuracy: 0.181641 | 0.071 sec/iter\n",
      "Epoch: 61 | Batch: 007 / 025 | Total loss: 2.569 | Reg loss: 0.030 | Tree loss: 2.569 | Accuracy: 0.166016 | 0.071 sec/iter\n",
      "Epoch: 61 | Batch: 008 / 025 | Total loss: 2.603 | Reg loss: 0.030 | Tree loss: 2.603 | Accuracy: 0.132812 | 0.071 sec/iter\n",
      "Epoch: 61 | Batch: 009 / 025 | Total loss: 2.565 | Reg loss: 0.030 | Tree loss: 2.565 | Accuracy: 0.171875 | 0.071 sec/iter\n",
      "Epoch: 61 | Batch: 010 / 025 | Total loss: 2.523 | Reg loss: 0.030 | Tree loss: 2.523 | Accuracy: 0.160156 | 0.071 sec/iter\n",
      "Epoch: 61 | Batch: 011 / 025 | Total loss: 2.537 | Reg loss: 0.030 | Tree loss: 2.537 | Accuracy: 0.156250 | 0.071 sec/iter\n",
      "Epoch: 61 | Batch: 012 / 025 | Total loss: 2.526 | Reg loss: 0.030 | Tree loss: 2.526 | Accuracy: 0.160156 | 0.071 sec/iter\n",
      "Epoch: 61 | Batch: 013 / 025 | Total loss: 2.492 | Reg loss: 0.030 | Tree loss: 2.492 | Accuracy: 0.177734 | 0.071 sec/iter\n",
      "Epoch: 61 | Batch: 014 / 025 | Total loss: 2.515 | Reg loss: 0.030 | Tree loss: 2.515 | Accuracy: 0.140625 | 0.071 sec/iter\n",
      "Epoch: 61 | Batch: 015 / 025 | Total loss: 2.488 | Reg loss: 0.030 | Tree loss: 2.488 | Accuracy: 0.150391 | 0.071 sec/iter\n",
      "Epoch: 61 | Batch: 016 / 025 | Total loss: 2.471 | Reg loss: 0.030 | Tree loss: 2.471 | Accuracy: 0.169922 | 0.071 sec/iter\n",
      "Epoch: 61 | Batch: 017 / 025 | Total loss: 2.476 | Reg loss: 0.030 | Tree loss: 2.476 | Accuracy: 0.125000 | 0.071 sec/iter\n",
      "Epoch: 61 | Batch: 018 / 025 | Total loss: 2.481 | Reg loss: 0.030 | Tree loss: 2.481 | Accuracy: 0.152344 | 0.071 sec/iter\n",
      "Epoch: 61 | Batch: 019 / 025 | Total loss: 2.462 | Reg loss: 0.030 | Tree loss: 2.462 | Accuracy: 0.171875 | 0.071 sec/iter\n",
      "Epoch: 61 | Batch: 020 / 025 | Total loss: 2.519 | Reg loss: 0.030 | Tree loss: 2.519 | Accuracy: 0.134766 | 0.071 sec/iter\n",
      "Epoch: 61 | Batch: 021 / 025 | Total loss: 2.476 | Reg loss: 0.030 | Tree loss: 2.476 | Accuracy: 0.171875 | 0.071 sec/iter\n",
      "Epoch: 61 | Batch: 022 / 025 | Total loss: 2.507 | Reg loss: 0.030 | Tree loss: 2.507 | Accuracy: 0.126953 | 0.071 sec/iter\n",
      "Epoch: 61 | Batch: 023 / 025 | Total loss: 2.470 | Reg loss: 0.030 | Tree loss: 2.470 | Accuracy: 0.134766 | 0.071 sec/iter\n",
      "Epoch: 61 | Batch: 024 / 025 | Total loss: 2.437 | Reg loss: 0.030 | Tree loss: 2.437 | Accuracy: 0.153125 | 0.071 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 62 | Batch: 000 / 025 | Total loss: 2.638 | Reg loss: 0.030 | Tree loss: 2.638 | Accuracy: 0.167969 | 0.071 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 62 | Batch: 001 / 025 | Total loss: 2.603 | Reg loss: 0.030 | Tree loss: 2.603 | Accuracy: 0.207031 | 0.071 sec/iter\n",
      "Epoch: 62 | Batch: 002 / 025 | Total loss: 2.592 | Reg loss: 0.030 | Tree loss: 2.592 | Accuracy: 0.148438 | 0.071 sec/iter\n",
      "Epoch: 62 | Batch: 003 / 025 | Total loss: 2.617 | Reg loss: 0.030 | Tree loss: 2.617 | Accuracy: 0.152344 | 0.071 sec/iter\n",
      "Epoch: 62 | Batch: 004 / 025 | Total loss: 2.597 | Reg loss: 0.030 | Tree loss: 2.597 | Accuracy: 0.167969 | 0.071 sec/iter\n",
      "Epoch: 62 | Batch: 005 / 025 | Total loss: 2.583 | Reg loss: 0.030 | Tree loss: 2.583 | Accuracy: 0.166016 | 0.071 sec/iter\n",
      "Epoch: 62 | Batch: 006 / 025 | Total loss: 2.544 | Reg loss: 0.030 | Tree loss: 2.544 | Accuracy: 0.175781 | 0.071 sec/iter\n",
      "Epoch: 62 | Batch: 007 / 025 | Total loss: 2.573 | Reg loss: 0.030 | Tree loss: 2.573 | Accuracy: 0.173828 | 0.071 sec/iter\n",
      "Epoch: 62 | Batch: 008 / 025 | Total loss: 2.555 | Reg loss: 0.030 | Tree loss: 2.555 | Accuracy: 0.164062 | 0.071 sec/iter\n",
      "Epoch: 62 | Batch: 009 / 025 | Total loss: 2.507 | Reg loss: 0.030 | Tree loss: 2.507 | Accuracy: 0.179688 | 0.071 sec/iter\n",
      "Epoch: 62 | Batch: 010 / 025 | Total loss: 2.581 | Reg loss: 0.030 | Tree loss: 2.581 | Accuracy: 0.154297 | 0.071 sec/iter\n",
      "Epoch: 62 | Batch: 011 / 025 | Total loss: 2.507 | Reg loss: 0.030 | Tree loss: 2.507 | Accuracy: 0.185547 | 0.071 sec/iter\n",
      "Epoch: 62 | Batch: 012 / 025 | Total loss: 2.521 | Reg loss: 0.030 | Tree loss: 2.521 | Accuracy: 0.142578 | 0.071 sec/iter\n",
      "Epoch: 62 | Batch: 013 / 025 | Total loss: 2.491 | Reg loss: 0.030 | Tree loss: 2.491 | Accuracy: 0.160156 | 0.071 sec/iter\n",
      "Epoch: 62 | Batch: 014 / 025 | Total loss: 2.506 | Reg loss: 0.030 | Tree loss: 2.506 | Accuracy: 0.144531 | 0.071 sec/iter\n",
      "Epoch: 62 | Batch: 015 / 025 | Total loss: 2.512 | Reg loss: 0.030 | Tree loss: 2.512 | Accuracy: 0.152344 | 0.071 sec/iter\n",
      "Epoch: 62 | Batch: 016 / 025 | Total loss: 2.497 | Reg loss: 0.030 | Tree loss: 2.497 | Accuracy: 0.142578 | 0.071 sec/iter\n",
      "Epoch: 62 | Batch: 017 / 025 | Total loss: 2.459 | Reg loss: 0.030 | Tree loss: 2.459 | Accuracy: 0.154297 | 0.071 sec/iter\n",
      "Epoch: 62 | Batch: 018 / 025 | Total loss: 2.461 | Reg loss: 0.030 | Tree loss: 2.461 | Accuracy: 0.160156 | 0.071 sec/iter\n",
      "Epoch: 62 | Batch: 019 / 025 | Total loss: 2.495 | Reg loss: 0.030 | Tree loss: 2.495 | Accuracy: 0.142578 | 0.071 sec/iter\n",
      "Epoch: 62 | Batch: 020 / 025 | Total loss: 2.438 | Reg loss: 0.030 | Tree loss: 2.438 | Accuracy: 0.158203 | 0.071 sec/iter\n",
      "Epoch: 62 | Batch: 021 / 025 | Total loss: 2.453 | Reg loss: 0.030 | Tree loss: 2.453 | Accuracy: 0.164062 | 0.071 sec/iter\n",
      "Epoch: 62 | Batch: 022 / 025 | Total loss: 2.462 | Reg loss: 0.030 | Tree loss: 2.462 | Accuracy: 0.128906 | 0.071 sec/iter\n",
      "Epoch: 62 | Batch: 023 / 025 | Total loss: 2.423 | Reg loss: 0.030 | Tree loss: 2.423 | Accuracy: 0.140625 | 0.071 sec/iter\n",
      "Epoch: 62 | Batch: 024 / 025 | Total loss: 2.428 | Reg loss: 0.030 | Tree loss: 2.428 | Accuracy: 0.131250 | 0.071 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 63 | Batch: 000 / 025 | Total loss: 2.580 | Reg loss: 0.030 | Tree loss: 2.580 | Accuracy: 0.166016 | 0.071 sec/iter\n",
      "Epoch: 63 | Batch: 001 / 025 | Total loss: 2.643 | Reg loss: 0.030 | Tree loss: 2.643 | Accuracy: 0.173828 | 0.071 sec/iter\n",
      "Epoch: 63 | Batch: 002 / 025 | Total loss: 2.596 | Reg loss: 0.030 | Tree loss: 2.596 | Accuracy: 0.177734 | 0.071 sec/iter\n",
      "Epoch: 63 | Batch: 003 / 025 | Total loss: 2.574 | Reg loss: 0.030 | Tree loss: 2.574 | Accuracy: 0.185547 | 0.071 sec/iter\n",
      "Epoch: 63 | Batch: 004 / 025 | Total loss: 2.550 | Reg loss: 0.030 | Tree loss: 2.550 | Accuracy: 0.150391 | 0.071 sec/iter\n",
      "Epoch: 63 | Batch: 005 / 025 | Total loss: 2.541 | Reg loss: 0.030 | Tree loss: 2.541 | Accuracy: 0.179688 | 0.071 sec/iter\n",
      "Epoch: 63 | Batch: 006 / 025 | Total loss: 2.549 | Reg loss: 0.030 | Tree loss: 2.549 | Accuracy: 0.173828 | 0.071 sec/iter\n",
      "Epoch: 63 | Batch: 007 / 025 | Total loss: 2.587 | Reg loss: 0.030 | Tree loss: 2.587 | Accuracy: 0.138672 | 0.071 sec/iter\n",
      "Epoch: 63 | Batch: 008 / 025 | Total loss: 2.584 | Reg loss: 0.030 | Tree loss: 2.584 | Accuracy: 0.166016 | 0.071 sec/iter\n",
      "Epoch: 63 | Batch: 009 / 025 | Total loss: 2.516 | Reg loss: 0.030 | Tree loss: 2.516 | Accuracy: 0.146484 | 0.071 sec/iter\n",
      "Epoch: 63 | Batch: 010 / 025 | Total loss: 2.499 | Reg loss: 0.030 | Tree loss: 2.499 | Accuracy: 0.156250 | 0.071 sec/iter\n",
      "Epoch: 63 | Batch: 011 / 025 | Total loss: 2.455 | Reg loss: 0.030 | Tree loss: 2.455 | Accuracy: 0.189453 | 0.071 sec/iter\n",
      "Epoch: 63 | Batch: 012 / 025 | Total loss: 2.495 | Reg loss: 0.030 | Tree loss: 2.495 | Accuracy: 0.154297 | 0.071 sec/iter\n",
      "Epoch: 63 | Batch: 013 / 025 | Total loss: 2.540 | Reg loss: 0.030 | Tree loss: 2.540 | Accuracy: 0.146484 | 0.071 sec/iter\n",
      "Epoch: 63 | Batch: 014 / 025 | Total loss: 2.463 | Reg loss: 0.030 | Tree loss: 2.463 | Accuracy: 0.167969 | 0.071 sec/iter\n",
      "Epoch: 63 | Batch: 015 / 025 | Total loss: 2.479 | Reg loss: 0.030 | Tree loss: 2.479 | Accuracy: 0.126953 | 0.071 sec/iter\n",
      "Epoch: 63 | Batch: 016 / 025 | Total loss: 2.472 | Reg loss: 0.030 | Tree loss: 2.472 | Accuracy: 0.179688 | 0.071 sec/iter\n",
      "Epoch: 63 | Batch: 017 / 025 | Total loss: 2.501 | Reg loss: 0.030 | Tree loss: 2.501 | Accuracy: 0.154297 | 0.071 sec/iter\n",
      "Epoch: 63 | Batch: 018 / 025 | Total loss: 2.485 | Reg loss: 0.030 | Tree loss: 2.485 | Accuracy: 0.171875 | 0.071 sec/iter\n",
      "Epoch: 63 | Batch: 019 / 025 | Total loss: 2.478 | Reg loss: 0.030 | Tree loss: 2.478 | Accuracy: 0.144531 | 0.071 sec/iter\n",
      "Epoch: 63 | Batch: 020 / 025 | Total loss: 2.450 | Reg loss: 0.030 | Tree loss: 2.450 | Accuracy: 0.138672 | 0.071 sec/iter\n",
      "Epoch: 63 | Batch: 021 / 025 | Total loss: 2.429 | Reg loss: 0.030 | Tree loss: 2.429 | Accuracy: 0.187500 | 0.071 sec/iter\n",
      "Epoch: 63 | Batch: 022 / 025 | Total loss: 2.421 | Reg loss: 0.030 | Tree loss: 2.421 | Accuracy: 0.146484 | 0.071 sec/iter\n",
      "Epoch: 63 | Batch: 023 / 025 | Total loss: 2.458 | Reg loss: 0.030 | Tree loss: 2.458 | Accuracy: 0.156250 | 0.071 sec/iter\n",
      "Epoch: 63 | Batch: 024 / 025 | Total loss: 2.424 | Reg loss: 0.030 | Tree loss: 2.424 | Accuracy: 0.128125 | 0.071 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 64 | Batch: 000 / 025 | Total loss: 2.616 | Reg loss: 0.030 | Tree loss: 2.616 | Accuracy: 0.167969 | 0.071 sec/iter\n",
      "Epoch: 64 | Batch: 001 / 025 | Total loss: 2.574 | Reg loss: 0.030 | Tree loss: 2.574 | Accuracy: 0.173828 | 0.071 sec/iter\n",
      "Epoch: 64 | Batch: 002 / 025 | Total loss: 2.569 | Reg loss: 0.030 | Tree loss: 2.569 | Accuracy: 0.152344 | 0.071 sec/iter\n",
      "Epoch: 64 | Batch: 003 / 025 | Total loss: 2.540 | Reg loss: 0.030 | Tree loss: 2.540 | Accuracy: 0.177734 | 0.071 sec/iter\n",
      "Epoch: 64 | Batch: 004 / 025 | Total loss: 2.563 | Reg loss: 0.030 | Tree loss: 2.563 | Accuracy: 0.179688 | 0.071 sec/iter\n",
      "Epoch: 64 | Batch: 005 / 025 | Total loss: 2.550 | Reg loss: 0.030 | Tree loss: 2.550 | Accuracy: 0.175781 | 0.071 sec/iter\n",
      "Epoch: 64 | Batch: 006 / 025 | Total loss: 2.530 | Reg loss: 0.030 | Tree loss: 2.530 | Accuracy: 0.193359 | 0.071 sec/iter\n",
      "Epoch: 64 | Batch: 007 / 025 | Total loss: 2.546 | Reg loss: 0.030 | Tree loss: 2.546 | Accuracy: 0.150391 | 0.071 sec/iter\n",
      "Epoch: 64 | Batch: 008 / 025 | Total loss: 2.516 | Reg loss: 0.030 | Tree loss: 2.516 | Accuracy: 0.162109 | 0.071 sec/iter\n",
      "Epoch: 64 | Batch: 009 / 025 | Total loss: 2.505 | Reg loss: 0.030 | Tree loss: 2.505 | Accuracy: 0.146484 | 0.071 sec/iter\n",
      "Epoch: 64 | Batch: 010 / 025 | Total loss: 2.498 | Reg loss: 0.030 | Tree loss: 2.498 | Accuracy: 0.173828 | 0.071 sec/iter\n",
      "Epoch: 64 | Batch: 011 / 025 | Total loss: 2.490 | Reg loss: 0.030 | Tree loss: 2.490 | Accuracy: 0.171875 | 0.071 sec/iter\n",
      "Epoch: 64 | Batch: 012 / 025 | Total loss: 2.512 | Reg loss: 0.030 | Tree loss: 2.512 | Accuracy: 0.169922 | 0.071 sec/iter\n",
      "Epoch: 64 | Batch: 013 / 025 | Total loss: 2.485 | Reg loss: 0.030 | Tree loss: 2.485 | Accuracy: 0.144531 | 0.071 sec/iter\n",
      "Epoch: 64 | Batch: 014 / 025 | Total loss: 2.487 | Reg loss: 0.030 | Tree loss: 2.487 | Accuracy: 0.164062 | 0.071 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 64 | Batch: 015 / 025 | Total loss: 2.484 | Reg loss: 0.030 | Tree loss: 2.484 | Accuracy: 0.177734 | 0.071 sec/iter\n",
      "Epoch: 64 | Batch: 016 / 025 | Total loss: 2.471 | Reg loss: 0.030 | Tree loss: 2.471 | Accuracy: 0.167969 | 0.071 sec/iter\n",
      "Epoch: 64 | Batch: 017 / 025 | Total loss: 2.469 | Reg loss: 0.030 | Tree loss: 2.469 | Accuracy: 0.144531 | 0.071 sec/iter\n",
      "Epoch: 64 | Batch: 018 / 025 | Total loss: 2.487 | Reg loss: 0.030 | Tree loss: 2.487 | Accuracy: 0.148438 | 0.071 sec/iter\n",
      "Epoch: 64 | Batch: 019 / 025 | Total loss: 2.443 | Reg loss: 0.030 | Tree loss: 2.443 | Accuracy: 0.166016 | 0.071 sec/iter\n",
      "Epoch: 64 | Batch: 020 / 025 | Total loss: 2.473 | Reg loss: 0.030 | Tree loss: 2.473 | Accuracy: 0.146484 | 0.071 sec/iter\n",
      "Epoch: 64 | Batch: 021 / 025 | Total loss: 2.433 | Reg loss: 0.030 | Tree loss: 2.433 | Accuracy: 0.181641 | 0.071 sec/iter\n",
      "Epoch: 64 | Batch: 022 / 025 | Total loss: 2.448 | Reg loss: 0.030 | Tree loss: 2.448 | Accuracy: 0.146484 | 0.071 sec/iter\n",
      "Epoch: 64 | Batch: 023 / 025 | Total loss: 2.395 | Reg loss: 0.030 | Tree loss: 2.395 | Accuracy: 0.154297 | 0.071 sec/iter\n",
      "Epoch: 64 | Batch: 024 / 025 | Total loss: 2.419 | Reg loss: 0.030 | Tree loss: 2.419 | Accuracy: 0.171875 | 0.071 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 65 | Batch: 000 / 025 | Total loss: 2.566 | Reg loss: 0.030 | Tree loss: 2.566 | Accuracy: 0.164062 | 0.071 sec/iter\n",
      "Epoch: 65 | Batch: 001 / 025 | Total loss: 2.577 | Reg loss: 0.030 | Tree loss: 2.577 | Accuracy: 0.132812 | 0.071 sec/iter\n",
      "Epoch: 65 | Batch: 002 / 025 | Total loss: 2.575 | Reg loss: 0.030 | Tree loss: 2.575 | Accuracy: 0.142578 | 0.071 sec/iter\n",
      "Epoch: 65 | Batch: 003 / 025 | Total loss: 2.541 | Reg loss: 0.030 | Tree loss: 2.541 | Accuracy: 0.167969 | 0.071 sec/iter\n",
      "Epoch: 65 | Batch: 004 / 025 | Total loss: 2.530 | Reg loss: 0.030 | Tree loss: 2.530 | Accuracy: 0.156250 | 0.071 sec/iter\n",
      "Epoch: 65 | Batch: 005 / 025 | Total loss: 2.541 | Reg loss: 0.030 | Tree loss: 2.541 | Accuracy: 0.181641 | 0.071 sec/iter\n",
      "Epoch: 65 | Batch: 006 / 025 | Total loss: 2.513 | Reg loss: 0.030 | Tree loss: 2.513 | Accuracy: 0.183594 | 0.071 sec/iter\n",
      "Epoch: 65 | Batch: 007 / 025 | Total loss: 2.498 | Reg loss: 0.030 | Tree loss: 2.498 | Accuracy: 0.162109 | 0.071 sec/iter\n",
      "Epoch: 65 | Batch: 008 / 025 | Total loss: 2.538 | Reg loss: 0.030 | Tree loss: 2.538 | Accuracy: 0.171875 | 0.071 sec/iter\n",
      "Epoch: 65 | Batch: 009 / 025 | Total loss: 2.491 | Reg loss: 0.030 | Tree loss: 2.491 | Accuracy: 0.156250 | 0.071 sec/iter\n",
      "Epoch: 65 | Batch: 010 / 025 | Total loss: 2.503 | Reg loss: 0.030 | Tree loss: 2.503 | Accuracy: 0.171875 | 0.071 sec/iter\n",
      "Epoch: 65 | Batch: 011 / 025 | Total loss: 2.467 | Reg loss: 0.030 | Tree loss: 2.467 | Accuracy: 0.187500 | 0.071 sec/iter\n",
      "Epoch: 65 | Batch: 012 / 025 | Total loss: 2.470 | Reg loss: 0.030 | Tree loss: 2.470 | Accuracy: 0.150391 | 0.071 sec/iter\n",
      "Epoch: 65 | Batch: 013 / 025 | Total loss: 2.479 | Reg loss: 0.030 | Tree loss: 2.479 | Accuracy: 0.177734 | 0.071 sec/iter\n",
      "Epoch: 65 | Batch: 014 / 025 | Total loss: 2.474 | Reg loss: 0.030 | Tree loss: 2.474 | Accuracy: 0.181641 | 0.071 sec/iter\n",
      "Epoch: 65 | Batch: 015 / 025 | Total loss: 2.453 | Reg loss: 0.030 | Tree loss: 2.453 | Accuracy: 0.138672 | 0.071 sec/iter\n",
      "Epoch: 65 | Batch: 016 / 025 | Total loss: 2.477 | Reg loss: 0.030 | Tree loss: 2.477 | Accuracy: 0.154297 | 0.071 sec/iter\n",
      "Epoch: 65 | Batch: 017 / 025 | Total loss: 2.485 | Reg loss: 0.030 | Tree loss: 2.485 | Accuracy: 0.169922 | 0.071 sec/iter\n",
      "Epoch: 65 | Batch: 018 / 025 | Total loss: 2.458 | Reg loss: 0.030 | Tree loss: 2.458 | Accuracy: 0.146484 | 0.071 sec/iter\n",
      "Epoch: 65 | Batch: 019 / 025 | Total loss: 2.409 | Reg loss: 0.030 | Tree loss: 2.409 | Accuracy: 0.128906 | 0.071 sec/iter\n",
      "Epoch: 65 | Batch: 020 / 025 | Total loss: 2.451 | Reg loss: 0.030 | Tree loss: 2.451 | Accuracy: 0.179688 | 0.071 sec/iter\n",
      "Epoch: 65 | Batch: 021 / 025 | Total loss: 2.442 | Reg loss: 0.030 | Tree loss: 2.442 | Accuracy: 0.183594 | 0.071 sec/iter\n",
      "Epoch: 65 | Batch: 022 / 025 | Total loss: 2.423 | Reg loss: 0.030 | Tree loss: 2.423 | Accuracy: 0.175781 | 0.071 sec/iter\n",
      "Epoch: 65 | Batch: 023 / 025 | Total loss: 2.472 | Reg loss: 0.030 | Tree loss: 2.472 | Accuracy: 0.150391 | 0.071 sec/iter\n",
      "Epoch: 65 | Batch: 024 / 025 | Total loss: 2.431 | Reg loss: 0.030 | Tree loss: 2.431 | Accuracy: 0.131250 | 0.071 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 66 | Batch: 000 / 025 | Total loss: 2.587 | Reg loss: 0.030 | Tree loss: 2.587 | Accuracy: 0.171875 | 0.071 sec/iter\n",
      "Epoch: 66 | Batch: 001 / 025 | Total loss: 2.568 | Reg loss: 0.030 | Tree loss: 2.568 | Accuracy: 0.144531 | 0.071 sec/iter\n",
      "Epoch: 66 | Batch: 002 / 025 | Total loss: 2.559 | Reg loss: 0.030 | Tree loss: 2.559 | Accuracy: 0.189453 | 0.071 sec/iter\n",
      "Epoch: 66 | Batch: 003 / 025 | Total loss: 2.569 | Reg loss: 0.030 | Tree loss: 2.569 | Accuracy: 0.152344 | 0.071 sec/iter\n",
      "Epoch: 66 | Batch: 004 / 025 | Total loss: 2.545 | Reg loss: 0.030 | Tree loss: 2.545 | Accuracy: 0.134766 | 0.071 sec/iter\n",
      "Epoch: 66 | Batch: 005 / 025 | Total loss: 2.503 | Reg loss: 0.030 | Tree loss: 2.503 | Accuracy: 0.195312 | 0.071 sec/iter\n",
      "Epoch: 66 | Batch: 006 / 025 | Total loss: 2.519 | Reg loss: 0.030 | Tree loss: 2.519 | Accuracy: 0.158203 | 0.071 sec/iter\n",
      "Epoch: 66 | Batch: 007 / 025 | Total loss: 2.509 | Reg loss: 0.030 | Tree loss: 2.509 | Accuracy: 0.171875 | 0.071 sec/iter\n",
      "Epoch: 66 | Batch: 008 / 025 | Total loss: 2.500 | Reg loss: 0.030 | Tree loss: 2.500 | Accuracy: 0.150391 | 0.071 sec/iter\n",
      "Epoch: 66 | Batch: 009 / 025 | Total loss: 2.476 | Reg loss: 0.030 | Tree loss: 2.476 | Accuracy: 0.185547 | 0.071 sec/iter\n",
      "Epoch: 66 | Batch: 010 / 025 | Total loss: 2.521 | Reg loss: 0.030 | Tree loss: 2.521 | Accuracy: 0.126953 | 0.071 sec/iter\n",
      "Epoch: 66 | Batch: 011 / 025 | Total loss: 2.466 | Reg loss: 0.030 | Tree loss: 2.466 | Accuracy: 0.195312 | 0.071 sec/iter\n",
      "Epoch: 66 | Batch: 012 / 025 | Total loss: 2.469 | Reg loss: 0.030 | Tree loss: 2.469 | Accuracy: 0.167969 | 0.071 sec/iter\n",
      "Epoch: 66 | Batch: 013 / 025 | Total loss: 2.435 | Reg loss: 0.030 | Tree loss: 2.435 | Accuracy: 0.179688 | 0.071 sec/iter\n",
      "Epoch: 66 | Batch: 014 / 025 | Total loss: 2.440 | Reg loss: 0.030 | Tree loss: 2.440 | Accuracy: 0.193359 | 0.071 sec/iter\n",
      "Epoch: 66 | Batch: 015 / 025 | Total loss: 2.502 | Reg loss: 0.030 | Tree loss: 2.502 | Accuracy: 0.154297 | 0.071 sec/iter\n",
      "Epoch: 66 | Batch: 016 / 025 | Total loss: 2.412 | Reg loss: 0.030 | Tree loss: 2.412 | Accuracy: 0.164062 | 0.071 sec/iter\n",
      "Epoch: 66 | Batch: 017 / 025 | Total loss: 2.444 | Reg loss: 0.030 | Tree loss: 2.444 | Accuracy: 0.150391 | 0.071 sec/iter\n",
      "Epoch: 66 | Batch: 018 / 025 | Total loss: 2.425 | Reg loss: 0.030 | Tree loss: 2.425 | Accuracy: 0.162109 | 0.071 sec/iter\n",
      "Epoch: 66 | Batch: 019 / 025 | Total loss: 2.419 | Reg loss: 0.030 | Tree loss: 2.419 | Accuracy: 0.175781 | 0.071 sec/iter\n",
      "Epoch: 66 | Batch: 020 / 025 | Total loss: 2.485 | Reg loss: 0.030 | Tree loss: 2.485 | Accuracy: 0.152344 | 0.071 sec/iter\n",
      "Epoch: 66 | Batch: 021 / 025 | Total loss: 2.455 | Reg loss: 0.030 | Tree loss: 2.455 | Accuracy: 0.154297 | 0.071 sec/iter\n",
      "Epoch: 66 | Batch: 022 / 025 | Total loss: 2.404 | Reg loss: 0.030 | Tree loss: 2.404 | Accuracy: 0.158203 | 0.071 sec/iter\n",
      "Epoch: 66 | Batch: 023 / 025 | Total loss: 2.399 | Reg loss: 0.030 | Tree loss: 2.399 | Accuracy: 0.154297 | 0.071 sec/iter\n",
      "Epoch: 66 | Batch: 024 / 025 | Total loss: 2.418 | Reg loss: 0.030 | Tree loss: 2.418 | Accuracy: 0.131250 | 0.071 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 67 | Batch: 000 / 025 | Total loss: 2.606 | Reg loss: 0.030 | Tree loss: 2.606 | Accuracy: 0.179688 | 0.071 sec/iter\n",
      "Epoch: 67 | Batch: 001 / 025 | Total loss: 2.546 | Reg loss: 0.030 | Tree loss: 2.546 | Accuracy: 0.187500 | 0.071 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 67 | Batch: 002 / 025 | Total loss: 2.524 | Reg loss: 0.030 | Tree loss: 2.524 | Accuracy: 0.160156 | 0.071 sec/iter\n",
      "Epoch: 67 | Batch: 003 / 025 | Total loss: 2.529 | Reg loss: 0.030 | Tree loss: 2.529 | Accuracy: 0.148438 | 0.071 sec/iter\n",
      "Epoch: 67 | Batch: 004 / 025 | Total loss: 2.557 | Reg loss: 0.030 | Tree loss: 2.557 | Accuracy: 0.132812 | 0.071 sec/iter\n",
      "Epoch: 67 | Batch: 005 / 025 | Total loss: 2.516 | Reg loss: 0.030 | Tree loss: 2.516 | Accuracy: 0.179688 | 0.071 sec/iter\n",
      "Epoch: 67 | Batch: 006 / 025 | Total loss: 2.503 | Reg loss: 0.030 | Tree loss: 2.503 | Accuracy: 0.173828 | 0.071 sec/iter\n",
      "Epoch: 67 | Batch: 007 / 025 | Total loss: 2.485 | Reg loss: 0.030 | Tree loss: 2.485 | Accuracy: 0.162109 | 0.071 sec/iter\n",
      "Epoch: 67 | Batch: 008 / 025 | Total loss: 2.444 | Reg loss: 0.030 | Tree loss: 2.444 | Accuracy: 0.173828 | 0.071 sec/iter\n",
      "Epoch: 67 | Batch: 009 / 025 | Total loss: 2.484 | Reg loss: 0.030 | Tree loss: 2.484 | Accuracy: 0.154297 | 0.071 sec/iter\n",
      "Epoch: 67 | Batch: 010 / 025 | Total loss: 2.495 | Reg loss: 0.030 | Tree loss: 2.495 | Accuracy: 0.173828 | 0.071 sec/iter\n",
      "Epoch: 67 | Batch: 011 / 025 | Total loss: 2.442 | Reg loss: 0.030 | Tree loss: 2.442 | Accuracy: 0.154297 | 0.071 sec/iter\n",
      "Epoch: 67 | Batch: 012 / 025 | Total loss: 2.451 | Reg loss: 0.030 | Tree loss: 2.451 | Accuracy: 0.187500 | 0.071 sec/iter\n",
      "Epoch: 67 | Batch: 013 / 025 | Total loss: 2.439 | Reg loss: 0.030 | Tree loss: 2.439 | Accuracy: 0.185547 | 0.071 sec/iter\n",
      "Epoch: 67 | Batch: 014 / 025 | Total loss: 2.470 | Reg loss: 0.030 | Tree loss: 2.470 | Accuracy: 0.160156 | 0.071 sec/iter\n",
      "Epoch: 67 | Batch: 015 / 025 | Total loss: 2.456 | Reg loss: 0.030 | Tree loss: 2.456 | Accuracy: 0.169922 | 0.071 sec/iter\n",
      "Epoch: 67 | Batch: 016 / 025 | Total loss: 2.473 | Reg loss: 0.030 | Tree loss: 2.473 | Accuracy: 0.171875 | 0.071 sec/iter\n",
      "Epoch: 67 | Batch: 017 / 025 | Total loss: 2.429 | Reg loss: 0.030 | Tree loss: 2.429 | Accuracy: 0.185547 | 0.071 sec/iter\n",
      "Epoch: 67 | Batch: 018 / 025 | Total loss: 2.412 | Reg loss: 0.030 | Tree loss: 2.412 | Accuracy: 0.156250 | 0.071 sec/iter\n",
      "Epoch: 67 | Batch: 019 / 025 | Total loss: 2.460 | Reg loss: 0.030 | Tree loss: 2.460 | Accuracy: 0.126953 | 0.071 sec/iter\n",
      "Epoch: 67 | Batch: 020 / 025 | Total loss: 2.427 | Reg loss: 0.030 | Tree loss: 2.427 | Accuracy: 0.158203 | 0.071 sec/iter\n",
      "Epoch: 67 | Batch: 021 / 025 | Total loss: 2.438 | Reg loss: 0.030 | Tree loss: 2.438 | Accuracy: 0.183594 | 0.071 sec/iter\n",
      "Epoch: 67 | Batch: 022 / 025 | Total loss: 2.445 | Reg loss: 0.030 | Tree loss: 2.445 | Accuracy: 0.140625 | 0.071 sec/iter\n",
      "Epoch: 67 | Batch: 023 / 025 | Total loss: 2.392 | Reg loss: 0.030 | Tree loss: 2.392 | Accuracy: 0.166016 | 0.071 sec/iter\n",
      "Epoch: 67 | Batch: 024 / 025 | Total loss: 2.375 | Reg loss: 0.030 | Tree loss: 2.375 | Accuracy: 0.165625 | 0.071 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 68 | Batch: 000 / 025 | Total loss: 2.541 | Reg loss: 0.030 | Tree loss: 2.541 | Accuracy: 0.177734 | 0.071 sec/iter\n",
      "Epoch: 68 | Batch: 001 / 025 | Total loss: 2.568 | Reg loss: 0.030 | Tree loss: 2.568 | Accuracy: 0.166016 | 0.071 sec/iter\n",
      "Epoch: 68 | Batch: 002 / 025 | Total loss: 2.518 | Reg loss: 0.030 | Tree loss: 2.518 | Accuracy: 0.191406 | 0.071 sec/iter\n",
      "Epoch: 68 | Batch: 003 / 025 | Total loss: 2.519 | Reg loss: 0.030 | Tree loss: 2.519 | Accuracy: 0.154297 | 0.071 sec/iter\n",
      "Epoch: 68 | Batch: 004 / 025 | Total loss: 2.504 | Reg loss: 0.030 | Tree loss: 2.504 | Accuracy: 0.164062 | 0.071 sec/iter\n",
      "Epoch: 68 | Batch: 005 / 025 | Total loss: 2.584 | Reg loss: 0.030 | Tree loss: 2.584 | Accuracy: 0.189453 | 0.071 sec/iter\n",
      "Epoch: 68 | Batch: 006 / 025 | Total loss: 2.491 | Reg loss: 0.030 | Tree loss: 2.491 | Accuracy: 0.162109 | 0.071 sec/iter\n",
      "Epoch: 68 | Batch: 007 / 025 | Total loss: 2.496 | Reg loss: 0.030 | Tree loss: 2.496 | Accuracy: 0.185547 | 0.071 sec/iter\n",
      "Epoch: 68 | Batch: 008 / 025 | Total loss: 2.491 | Reg loss: 0.030 | Tree loss: 2.491 | Accuracy: 0.156250 | 0.071 sec/iter\n",
      "Epoch: 68 | Batch: 009 / 025 | Total loss: 2.496 | Reg loss: 0.030 | Tree loss: 2.496 | Accuracy: 0.171875 | 0.071 sec/iter\n",
      "Epoch: 68 | Batch: 010 / 025 | Total loss: 2.450 | Reg loss: 0.030 | Tree loss: 2.450 | Accuracy: 0.181641 | 0.071 sec/iter\n",
      "Epoch: 68 | Batch: 011 / 025 | Total loss: 2.463 | Reg loss: 0.030 | Tree loss: 2.463 | Accuracy: 0.140625 | 0.071 sec/iter\n",
      "Epoch: 68 | Batch: 012 / 025 | Total loss: 2.414 | Reg loss: 0.030 | Tree loss: 2.414 | Accuracy: 0.158203 | 0.071 sec/iter\n",
      "Epoch: 68 | Batch: 013 / 025 | Total loss: 2.463 | Reg loss: 0.030 | Tree loss: 2.463 | Accuracy: 0.144531 | 0.071 sec/iter\n",
      "Epoch: 68 | Batch: 014 / 025 | Total loss: 2.438 | Reg loss: 0.030 | Tree loss: 2.438 | Accuracy: 0.173828 | 0.071 sec/iter\n",
      "Epoch: 68 | Batch: 015 / 025 | Total loss: 2.417 | Reg loss: 0.030 | Tree loss: 2.417 | Accuracy: 0.166016 | 0.071 sec/iter\n",
      "Epoch: 68 | Batch: 016 / 025 | Total loss: 2.442 | Reg loss: 0.030 | Tree loss: 2.442 | Accuracy: 0.173828 | 0.071 sec/iter\n",
      "Epoch: 68 | Batch: 017 / 025 | Total loss: 2.422 | Reg loss: 0.030 | Tree loss: 2.422 | Accuracy: 0.150391 | 0.071 sec/iter\n",
      "Epoch: 68 | Batch: 018 / 025 | Total loss: 2.427 | Reg loss: 0.030 | Tree loss: 2.427 | Accuracy: 0.146484 | 0.071 sec/iter\n",
      "Epoch: 68 | Batch: 019 / 025 | Total loss: 2.407 | Reg loss: 0.030 | Tree loss: 2.407 | Accuracy: 0.158203 | 0.071 sec/iter\n",
      "Epoch: 68 | Batch: 020 / 025 | Total loss: 2.431 | Reg loss: 0.030 | Tree loss: 2.431 | Accuracy: 0.140625 | 0.071 sec/iter\n",
      "Epoch: 68 | Batch: 021 / 025 | Total loss: 2.383 | Reg loss: 0.030 | Tree loss: 2.383 | Accuracy: 0.167969 | 0.071 sec/iter\n",
      "Epoch: 68 | Batch: 022 / 025 | Total loss: 2.435 | Reg loss: 0.030 | Tree loss: 2.435 | Accuracy: 0.146484 | 0.071 sec/iter\n",
      "Epoch: 68 | Batch: 023 / 025 | Total loss: 2.406 | Reg loss: 0.030 | Tree loss: 2.406 | Accuracy: 0.183594 | 0.071 sec/iter\n",
      "Epoch: 68 | Batch: 024 / 025 | Total loss: 2.391 | Reg loss: 0.030 | Tree loss: 2.391 | Accuracy: 0.165625 | 0.071 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 69 | Batch: 000 / 025 | Total loss: 2.552 | Reg loss: 0.030 | Tree loss: 2.552 | Accuracy: 0.187500 | 0.071 sec/iter\n",
      "Epoch: 69 | Batch: 001 / 025 | Total loss: 2.496 | Reg loss: 0.030 | Tree loss: 2.496 | Accuracy: 0.162109 | 0.071 sec/iter\n",
      "Epoch: 69 | Batch: 002 / 025 | Total loss: 2.554 | Reg loss: 0.030 | Tree loss: 2.554 | Accuracy: 0.142578 | 0.071 sec/iter\n",
      "Epoch: 69 | Batch: 003 / 025 | Total loss: 2.515 | Reg loss: 0.030 | Tree loss: 2.515 | Accuracy: 0.171875 | 0.071 sec/iter\n",
      "Epoch: 69 | Batch: 004 / 025 | Total loss: 2.524 | Reg loss: 0.030 | Tree loss: 2.524 | Accuracy: 0.132812 | 0.071 sec/iter\n",
      "Epoch: 69 | Batch: 005 / 025 | Total loss: 2.487 | Reg loss: 0.030 | Tree loss: 2.487 | Accuracy: 0.166016 | 0.071 sec/iter\n",
      "Epoch: 69 | Batch: 006 / 025 | Total loss: 2.473 | Reg loss: 0.030 | Tree loss: 2.473 | Accuracy: 0.166016 | 0.071 sec/iter\n",
      "Epoch: 69 | Batch: 007 / 025 | Total loss: 2.503 | Reg loss: 0.030 | Tree loss: 2.503 | Accuracy: 0.164062 | 0.071 sec/iter\n",
      "Epoch: 69 | Batch: 008 / 025 | Total loss: 2.483 | Reg loss: 0.030 | Tree loss: 2.483 | Accuracy: 0.164062 | 0.071 sec/iter\n",
      "Epoch: 69 | Batch: 009 / 025 | Total loss: 2.496 | Reg loss: 0.030 | Tree loss: 2.496 | Accuracy: 0.144531 | 0.071 sec/iter\n",
      "Epoch: 69 | Batch: 010 / 025 | Total loss: 2.436 | Reg loss: 0.030 | Tree loss: 2.436 | Accuracy: 0.181641 | 0.071 sec/iter\n",
      "Epoch: 69 | Batch: 011 / 025 | Total loss: 2.460 | Reg loss: 0.030 | Tree loss: 2.460 | Accuracy: 0.167969 | 0.071 sec/iter\n",
      "Epoch: 69 | Batch: 012 / 025 | Total loss: 2.479 | Reg loss: 0.030 | Tree loss: 2.479 | Accuracy: 0.123047 | 0.071 sec/iter\n",
      "Epoch: 69 | Batch: 013 / 025 | Total loss: 2.414 | Reg loss: 0.030 | Tree loss: 2.414 | Accuracy: 0.189453 | 0.071 sec/iter\n",
      "Epoch: 69 | Batch: 014 / 025 | Total loss: 2.425 | Reg loss: 0.030 | Tree loss: 2.425 | Accuracy: 0.166016 | 0.071 sec/iter\n",
      "Epoch: 69 | Batch: 015 / 025 | Total loss: 2.421 | Reg loss: 0.030 | Tree loss: 2.421 | Accuracy: 0.166016 | 0.071 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 69 | Batch: 016 / 025 | Total loss: 2.458 | Reg loss: 0.030 | Tree loss: 2.458 | Accuracy: 0.140625 | 0.071 sec/iter\n",
      "Epoch: 69 | Batch: 017 / 025 | Total loss: 2.433 | Reg loss: 0.030 | Tree loss: 2.433 | Accuracy: 0.187500 | 0.071 sec/iter\n",
      "Epoch: 69 | Batch: 018 / 025 | Total loss: 2.447 | Reg loss: 0.030 | Tree loss: 2.447 | Accuracy: 0.142578 | 0.071 sec/iter\n",
      "Epoch: 69 | Batch: 019 / 025 | Total loss: 2.457 | Reg loss: 0.030 | Tree loss: 2.457 | Accuracy: 0.169922 | 0.071 sec/iter\n",
      "Epoch: 69 | Batch: 020 / 025 | Total loss: 2.378 | Reg loss: 0.030 | Tree loss: 2.378 | Accuracy: 0.189453 | 0.071 sec/iter\n",
      "Epoch: 69 | Batch: 021 / 025 | Total loss: 2.367 | Reg loss: 0.030 | Tree loss: 2.367 | Accuracy: 0.164062 | 0.071 sec/iter\n",
      "Epoch: 69 | Batch: 022 / 025 | Total loss: 2.404 | Reg loss: 0.030 | Tree loss: 2.404 | Accuracy: 0.150391 | 0.071 sec/iter\n",
      "Epoch: 69 | Batch: 023 / 025 | Total loss: 2.383 | Reg loss: 0.030 | Tree loss: 2.383 | Accuracy: 0.167969 | 0.071 sec/iter\n",
      "Epoch: 69 | Batch: 024 / 025 | Total loss: 2.352 | Reg loss: 0.030 | Tree loss: 2.352 | Accuracy: 0.190625 | 0.071 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 70 | Batch: 000 / 025 | Total loss: 2.542 | Reg loss: 0.030 | Tree loss: 2.542 | Accuracy: 0.187500 | 0.071 sec/iter\n",
      "Epoch: 70 | Batch: 001 / 025 | Total loss: 2.529 | Reg loss: 0.030 | Tree loss: 2.529 | Accuracy: 0.144531 | 0.071 sec/iter\n",
      "Epoch: 70 | Batch: 002 / 025 | Total loss: 2.522 | Reg loss: 0.030 | Tree loss: 2.522 | Accuracy: 0.146484 | 0.071 sec/iter\n",
      "Epoch: 70 | Batch: 003 / 025 | Total loss: 2.496 | Reg loss: 0.030 | Tree loss: 2.496 | Accuracy: 0.169922 | 0.071 sec/iter\n",
      "Epoch: 70 | Batch: 004 / 025 | Total loss: 2.479 | Reg loss: 0.030 | Tree loss: 2.479 | Accuracy: 0.160156 | 0.071 sec/iter\n",
      "Epoch: 70 | Batch: 005 / 025 | Total loss: 2.483 | Reg loss: 0.030 | Tree loss: 2.483 | Accuracy: 0.144531 | 0.071 sec/iter\n",
      "Epoch: 70 | Batch: 006 / 025 | Total loss: 2.458 | Reg loss: 0.030 | Tree loss: 2.458 | Accuracy: 0.164062 | 0.071 sec/iter\n",
      "Epoch: 70 | Batch: 007 / 025 | Total loss: 2.455 | Reg loss: 0.030 | Tree loss: 2.455 | Accuracy: 0.173828 | 0.071 sec/iter\n",
      "Epoch: 70 | Batch: 008 / 025 | Total loss: 2.441 | Reg loss: 0.030 | Tree loss: 2.441 | Accuracy: 0.189453 | 0.071 sec/iter\n",
      "Epoch: 70 | Batch: 009 / 025 | Total loss: 2.466 | Reg loss: 0.030 | Tree loss: 2.466 | Accuracy: 0.171875 | 0.071 sec/iter\n",
      "Epoch: 70 | Batch: 010 / 025 | Total loss: 2.457 | Reg loss: 0.030 | Tree loss: 2.457 | Accuracy: 0.181641 | 0.071 sec/iter\n",
      "Epoch: 70 | Batch: 011 / 025 | Total loss: 2.448 | Reg loss: 0.030 | Tree loss: 2.448 | Accuracy: 0.171875 | 0.071 sec/iter\n",
      "Epoch: 70 | Batch: 012 / 025 | Total loss: 2.467 | Reg loss: 0.030 | Tree loss: 2.467 | Accuracy: 0.152344 | 0.071 sec/iter\n",
      "Epoch: 70 | Batch: 013 / 025 | Total loss: 2.441 | Reg loss: 0.030 | Tree loss: 2.441 | Accuracy: 0.167969 | 0.071 sec/iter\n",
      "Epoch: 70 | Batch: 014 / 025 | Total loss: 2.444 | Reg loss: 0.030 | Tree loss: 2.444 | Accuracy: 0.183594 | 0.071 sec/iter\n",
      "Epoch: 70 | Batch: 015 / 025 | Total loss: 2.412 | Reg loss: 0.030 | Tree loss: 2.412 | Accuracy: 0.171875 | 0.071 sec/iter\n",
      "Epoch: 70 | Batch: 016 / 025 | Total loss: 2.442 | Reg loss: 0.030 | Tree loss: 2.442 | Accuracy: 0.175781 | 0.071 sec/iter\n",
      "Epoch: 70 | Batch: 017 / 025 | Total loss: 2.397 | Reg loss: 0.030 | Tree loss: 2.397 | Accuracy: 0.189453 | 0.071 sec/iter\n",
      "Epoch: 70 | Batch: 018 / 025 | Total loss: 2.417 | Reg loss: 0.030 | Tree loss: 2.417 | Accuracy: 0.130859 | 0.071 sec/iter\n",
      "Epoch: 70 | Batch: 019 / 025 | Total loss: 2.430 | Reg loss: 0.030 | Tree loss: 2.430 | Accuracy: 0.136719 | 0.071 sec/iter\n",
      "Epoch: 70 | Batch: 020 / 025 | Total loss: 2.391 | Reg loss: 0.030 | Tree loss: 2.391 | Accuracy: 0.154297 | 0.071 sec/iter\n",
      "Epoch: 70 | Batch: 021 / 025 | Total loss: 2.406 | Reg loss: 0.030 | Tree loss: 2.406 | Accuracy: 0.156250 | 0.071 sec/iter\n",
      "Epoch: 70 | Batch: 022 / 025 | Total loss: 2.437 | Reg loss: 0.030 | Tree loss: 2.437 | Accuracy: 0.158203 | 0.071 sec/iter\n",
      "Epoch: 70 | Batch: 023 / 025 | Total loss: 2.421 | Reg loss: 0.030 | Tree loss: 2.421 | Accuracy: 0.171875 | 0.071 sec/iter\n",
      "Epoch: 70 | Batch: 024 / 025 | Total loss: 2.354 | Reg loss: 0.030 | Tree loss: 2.354 | Accuracy: 0.162500 | 0.071 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 71 | Batch: 000 / 025 | Total loss: 2.536 | Reg loss: 0.030 | Tree loss: 2.536 | Accuracy: 0.154297 | 0.071 sec/iter\n",
      "Epoch: 71 | Batch: 001 / 025 | Total loss: 2.502 | Reg loss: 0.030 | Tree loss: 2.502 | Accuracy: 0.179688 | 0.071 sec/iter\n",
      "Epoch: 71 | Batch: 002 / 025 | Total loss: 2.547 | Reg loss: 0.030 | Tree loss: 2.547 | Accuracy: 0.167969 | 0.071 sec/iter\n",
      "Epoch: 71 | Batch: 003 / 025 | Total loss: 2.475 | Reg loss: 0.030 | Tree loss: 2.475 | Accuracy: 0.173828 | 0.071 sec/iter\n",
      "Epoch: 71 | Batch: 004 / 025 | Total loss: 2.447 | Reg loss: 0.030 | Tree loss: 2.447 | Accuracy: 0.193359 | 0.071 sec/iter\n",
      "Epoch: 71 | Batch: 005 / 025 | Total loss: 2.471 | Reg loss: 0.030 | Tree loss: 2.471 | Accuracy: 0.179688 | 0.071 sec/iter\n",
      "Epoch: 71 | Batch: 006 / 025 | Total loss: 2.484 | Reg loss: 0.030 | Tree loss: 2.484 | Accuracy: 0.166016 | 0.071 sec/iter\n",
      "Epoch: 71 | Batch: 007 / 025 | Total loss: 2.461 | Reg loss: 0.030 | Tree loss: 2.461 | Accuracy: 0.197266 | 0.071 sec/iter\n",
      "Epoch: 71 | Batch: 008 / 025 | Total loss: 2.458 | Reg loss: 0.030 | Tree loss: 2.458 | Accuracy: 0.173828 | 0.071 sec/iter\n",
      "Epoch: 71 | Batch: 009 / 025 | Total loss: 2.431 | Reg loss: 0.030 | Tree loss: 2.431 | Accuracy: 0.150391 | 0.071 sec/iter\n",
      "Epoch: 71 | Batch: 010 / 025 | Total loss: 2.486 | Reg loss: 0.030 | Tree loss: 2.486 | Accuracy: 0.132812 | 0.071 sec/iter\n",
      "Epoch: 71 | Batch: 011 / 025 | Total loss: 2.429 | Reg loss: 0.030 | Tree loss: 2.429 | Accuracy: 0.169922 | 0.071 sec/iter\n",
      "Epoch: 71 | Batch: 012 / 025 | Total loss: 2.443 | Reg loss: 0.030 | Tree loss: 2.443 | Accuracy: 0.183594 | 0.071 sec/iter\n",
      "Epoch: 71 | Batch: 013 / 025 | Total loss: 2.409 | Reg loss: 0.030 | Tree loss: 2.409 | Accuracy: 0.173828 | 0.071 sec/iter\n",
      "Epoch: 71 | Batch: 014 / 025 | Total loss: 2.430 | Reg loss: 0.030 | Tree loss: 2.430 | Accuracy: 0.164062 | 0.071 sec/iter\n",
      "Epoch: 71 | Batch: 015 / 025 | Total loss: 2.421 | Reg loss: 0.030 | Tree loss: 2.421 | Accuracy: 0.171875 | 0.071 sec/iter\n",
      "Epoch: 71 | Batch: 016 / 025 | Total loss: 2.426 | Reg loss: 0.030 | Tree loss: 2.426 | Accuracy: 0.152344 | 0.071 sec/iter\n",
      "Epoch: 71 | Batch: 017 / 025 | Total loss: 2.408 | Reg loss: 0.030 | Tree loss: 2.408 | Accuracy: 0.123047 | 0.071 sec/iter\n",
      "Epoch: 71 | Batch: 018 / 025 | Total loss: 2.417 | Reg loss: 0.030 | Tree loss: 2.417 | Accuracy: 0.183594 | 0.071 sec/iter\n",
      "Epoch: 71 | Batch: 019 / 025 | Total loss: 2.463 | Reg loss: 0.030 | Tree loss: 2.463 | Accuracy: 0.146484 | 0.071 sec/iter\n",
      "Epoch: 71 | Batch: 020 / 025 | Total loss: 2.396 | Reg loss: 0.030 | Tree loss: 2.396 | Accuracy: 0.150391 | 0.071 sec/iter\n",
      "Epoch: 71 | Batch: 021 / 025 | Total loss: 2.384 | Reg loss: 0.030 | Tree loss: 2.384 | Accuracy: 0.142578 | 0.071 sec/iter\n",
      "Epoch: 71 | Batch: 022 / 025 | Total loss: 2.361 | Reg loss: 0.030 | Tree loss: 2.361 | Accuracy: 0.187500 | 0.071 sec/iter\n",
      "Epoch: 71 | Batch: 023 / 025 | Total loss: 2.400 | Reg loss: 0.030 | Tree loss: 2.400 | Accuracy: 0.132812 | 0.071 sec/iter\n",
      "Epoch: 71 | Batch: 024 / 025 | Total loss: 2.385 | Reg loss: 0.030 | Tree loss: 2.385 | Accuracy: 0.159375 | 0.071 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 72 | Batch: 000 / 025 | Total loss: 2.503 | Reg loss: 0.030 | Tree loss: 2.503 | Accuracy: 0.169922 | 0.071 sec/iter\n",
      "Epoch: 72 | Batch: 001 / 025 | Total loss: 2.511 | Reg loss: 0.030 | Tree loss: 2.511 | Accuracy: 0.144531 | 0.071 sec/iter\n",
      "Epoch: 72 | Batch: 002 / 025 | Total loss: 2.478 | Reg loss: 0.030 | Tree loss: 2.478 | Accuracy: 0.173828 | 0.071 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 72 | Batch: 003 / 025 | Total loss: 2.489 | Reg loss: 0.030 | Tree loss: 2.489 | Accuracy: 0.160156 | 0.071 sec/iter\n",
      "Epoch: 72 | Batch: 004 / 025 | Total loss: 2.477 | Reg loss: 0.030 | Tree loss: 2.477 | Accuracy: 0.208984 | 0.071 sec/iter\n",
      "Epoch: 72 | Batch: 005 / 025 | Total loss: 2.463 | Reg loss: 0.030 | Tree loss: 2.463 | Accuracy: 0.171875 | 0.071 sec/iter\n",
      "Epoch: 72 | Batch: 006 / 025 | Total loss: 2.492 | Reg loss: 0.030 | Tree loss: 2.492 | Accuracy: 0.150391 | 0.071 sec/iter\n",
      "Epoch: 72 | Batch: 007 / 025 | Total loss: 2.493 | Reg loss: 0.030 | Tree loss: 2.493 | Accuracy: 0.148438 | 0.071 sec/iter\n",
      "Epoch: 72 | Batch: 008 / 025 | Total loss: 2.468 | Reg loss: 0.030 | Tree loss: 2.468 | Accuracy: 0.156250 | 0.071 sec/iter\n",
      "Epoch: 72 | Batch: 009 / 025 | Total loss: 2.478 | Reg loss: 0.030 | Tree loss: 2.478 | Accuracy: 0.152344 | 0.071 sec/iter\n",
      "Epoch: 72 | Batch: 010 / 025 | Total loss: 2.405 | Reg loss: 0.030 | Tree loss: 2.405 | Accuracy: 0.158203 | 0.071 sec/iter\n",
      "Epoch: 72 | Batch: 011 / 025 | Total loss: 2.410 | Reg loss: 0.030 | Tree loss: 2.410 | Accuracy: 0.160156 | 0.071 sec/iter\n",
      "Epoch: 72 | Batch: 012 / 025 | Total loss: 2.421 | Reg loss: 0.030 | Tree loss: 2.421 | Accuracy: 0.169922 | 0.071 sec/iter\n",
      "Epoch: 72 | Batch: 013 / 025 | Total loss: 2.419 | Reg loss: 0.030 | Tree loss: 2.419 | Accuracy: 0.187500 | 0.071 sec/iter\n",
      "Epoch: 72 | Batch: 014 / 025 | Total loss: 2.417 | Reg loss: 0.030 | Tree loss: 2.417 | Accuracy: 0.164062 | 0.071 sec/iter\n",
      "Epoch: 72 | Batch: 015 / 025 | Total loss: 2.424 | Reg loss: 0.030 | Tree loss: 2.424 | Accuracy: 0.150391 | 0.071 sec/iter\n",
      "Epoch: 72 | Batch: 016 / 025 | Total loss: 2.408 | Reg loss: 0.030 | Tree loss: 2.408 | Accuracy: 0.162109 | 0.071 sec/iter\n",
      "Epoch: 72 | Batch: 017 / 025 | Total loss: 2.376 | Reg loss: 0.030 | Tree loss: 2.376 | Accuracy: 0.179688 | 0.071 sec/iter\n",
      "Epoch: 72 | Batch: 018 / 025 | Total loss: 2.425 | Reg loss: 0.030 | Tree loss: 2.425 | Accuracy: 0.167969 | 0.071 sec/iter\n",
      "Epoch: 72 | Batch: 019 / 025 | Total loss: 2.394 | Reg loss: 0.030 | Tree loss: 2.394 | Accuracy: 0.162109 | 0.071 sec/iter\n",
      "Epoch: 72 | Batch: 020 / 025 | Total loss: 2.376 | Reg loss: 0.030 | Tree loss: 2.376 | Accuracy: 0.181641 | 0.071 sec/iter\n",
      "Epoch: 72 | Batch: 021 / 025 | Total loss: 2.398 | Reg loss: 0.030 | Tree loss: 2.398 | Accuracy: 0.181641 | 0.071 sec/iter\n",
      "Epoch: 72 | Batch: 022 / 025 | Total loss: 2.427 | Reg loss: 0.030 | Tree loss: 2.427 | Accuracy: 0.136719 | 0.071 sec/iter\n",
      "Epoch: 72 | Batch: 023 / 025 | Total loss: 2.383 | Reg loss: 0.030 | Tree loss: 2.383 | Accuracy: 0.169922 | 0.071 sec/iter\n",
      "Epoch: 72 | Batch: 024 / 025 | Total loss: 2.383 | Reg loss: 0.030 | Tree loss: 2.383 | Accuracy: 0.146875 | 0.071 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 73 | Batch: 000 / 025 | Total loss: 2.523 | Reg loss: 0.030 | Tree loss: 2.523 | Accuracy: 0.166016 | 0.071 sec/iter\n",
      "Epoch: 73 | Batch: 001 / 025 | Total loss: 2.519 | Reg loss: 0.030 | Tree loss: 2.519 | Accuracy: 0.164062 | 0.071 sec/iter\n",
      "Epoch: 73 | Batch: 002 / 025 | Total loss: 2.503 | Reg loss: 0.030 | Tree loss: 2.503 | Accuracy: 0.173828 | 0.071 sec/iter\n",
      "Epoch: 73 | Batch: 003 / 025 | Total loss: 2.489 | Reg loss: 0.030 | Tree loss: 2.489 | Accuracy: 0.171875 | 0.071 sec/iter\n",
      "Epoch: 73 | Batch: 004 / 025 | Total loss: 2.497 | Reg loss: 0.030 | Tree loss: 2.497 | Accuracy: 0.160156 | 0.071 sec/iter\n",
      "Epoch: 73 | Batch: 005 / 025 | Total loss: 2.497 | Reg loss: 0.030 | Tree loss: 2.497 | Accuracy: 0.191406 | 0.071 sec/iter\n",
      "Epoch: 73 | Batch: 006 / 025 | Total loss: 2.438 | Reg loss: 0.030 | Tree loss: 2.438 | Accuracy: 0.160156 | 0.071 sec/iter\n",
      "Epoch: 73 | Batch: 007 / 025 | Total loss: 2.451 | Reg loss: 0.030 | Tree loss: 2.451 | Accuracy: 0.136719 | 0.071 sec/iter\n",
      "Epoch: 73 | Batch: 008 / 025 | Total loss: 2.452 | Reg loss: 0.030 | Tree loss: 2.452 | Accuracy: 0.162109 | 0.071 sec/iter\n",
      "Epoch: 73 | Batch: 009 / 025 | Total loss: 2.464 | Reg loss: 0.030 | Tree loss: 2.464 | Accuracy: 0.156250 | 0.071 sec/iter\n",
      "Epoch: 73 | Batch: 010 / 025 | Total loss: 2.477 | Reg loss: 0.030 | Tree loss: 2.477 | Accuracy: 0.166016 | 0.071 sec/iter\n",
      "Epoch: 73 | Batch: 011 / 025 | Total loss: 2.443 | Reg loss: 0.030 | Tree loss: 2.443 | Accuracy: 0.164062 | 0.071 sec/iter\n",
      "Epoch: 73 | Batch: 012 / 025 | Total loss: 2.449 | Reg loss: 0.030 | Tree loss: 2.449 | Accuracy: 0.171875 | 0.071 sec/iter\n",
      "Epoch: 73 | Batch: 013 / 025 | Total loss: 2.404 | Reg loss: 0.030 | Tree loss: 2.404 | Accuracy: 0.164062 | 0.071 sec/iter\n",
      "Epoch: 73 | Batch: 014 / 025 | Total loss: 2.426 | Reg loss: 0.030 | Tree loss: 2.426 | Accuracy: 0.162109 | 0.071 sec/iter\n",
      "Epoch: 73 | Batch: 015 / 025 | Total loss: 2.364 | Reg loss: 0.030 | Tree loss: 2.364 | Accuracy: 0.183594 | 0.071 sec/iter\n",
      "Epoch: 73 | Batch: 016 / 025 | Total loss: 2.397 | Reg loss: 0.030 | Tree loss: 2.397 | Accuracy: 0.189453 | 0.071 sec/iter\n",
      "Epoch: 73 | Batch: 017 / 025 | Total loss: 2.360 | Reg loss: 0.030 | Tree loss: 2.360 | Accuracy: 0.181641 | 0.071 sec/iter\n",
      "Epoch: 73 | Batch: 018 / 025 | Total loss: 2.397 | Reg loss: 0.030 | Tree loss: 2.397 | Accuracy: 0.154297 | 0.071 sec/iter\n",
      "Epoch: 73 | Batch: 019 / 025 | Total loss: 2.337 | Reg loss: 0.030 | Tree loss: 2.337 | Accuracy: 0.179688 | 0.071 sec/iter\n",
      "Epoch: 73 | Batch: 020 / 025 | Total loss: 2.372 | Reg loss: 0.030 | Tree loss: 2.372 | Accuracy: 0.144531 | 0.071 sec/iter\n",
      "Epoch: 73 | Batch: 021 / 025 | Total loss: 2.389 | Reg loss: 0.030 | Tree loss: 2.389 | Accuracy: 0.152344 | 0.071 sec/iter\n",
      "Epoch: 73 | Batch: 022 / 025 | Total loss: 2.389 | Reg loss: 0.030 | Tree loss: 2.389 | Accuracy: 0.152344 | 0.071 sec/iter\n",
      "Epoch: 73 | Batch: 023 / 025 | Total loss: 2.363 | Reg loss: 0.030 | Tree loss: 2.363 | Accuracy: 0.138672 | 0.071 sec/iter\n",
      "Epoch: 73 | Batch: 024 / 025 | Total loss: 2.359 | Reg loss: 0.030 | Tree loss: 2.359 | Accuracy: 0.159375 | 0.071 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 74 | Batch: 000 / 025 | Total loss: 2.472 | Reg loss: 0.030 | Tree loss: 2.472 | Accuracy: 0.167969 | 0.071 sec/iter\n",
      "Epoch: 74 | Batch: 001 / 025 | Total loss: 2.508 | Reg loss: 0.030 | Tree loss: 2.508 | Accuracy: 0.140625 | 0.071 sec/iter\n",
      "Epoch: 74 | Batch: 002 / 025 | Total loss: 2.442 | Reg loss: 0.030 | Tree loss: 2.442 | Accuracy: 0.167969 | 0.071 sec/iter\n",
      "Epoch: 74 | Batch: 003 / 025 | Total loss: 2.555 | Reg loss: 0.030 | Tree loss: 2.555 | Accuracy: 0.144531 | 0.071 sec/iter\n",
      "Epoch: 74 | Batch: 004 / 025 | Total loss: 2.512 | Reg loss: 0.030 | Tree loss: 2.512 | Accuracy: 0.169922 | 0.071 sec/iter\n",
      "Epoch: 74 | Batch: 005 / 025 | Total loss: 2.426 | Reg loss: 0.030 | Tree loss: 2.426 | Accuracy: 0.169922 | 0.071 sec/iter\n",
      "Epoch: 74 | Batch: 006 / 025 | Total loss: 2.444 | Reg loss: 0.030 | Tree loss: 2.444 | Accuracy: 0.154297 | 0.071 sec/iter\n",
      "Epoch: 74 | Batch: 007 / 025 | Total loss: 2.437 | Reg loss: 0.030 | Tree loss: 2.437 | Accuracy: 0.173828 | 0.071 sec/iter\n",
      "Epoch: 74 | Batch: 008 / 025 | Total loss: 2.423 | Reg loss: 0.030 | Tree loss: 2.423 | Accuracy: 0.185547 | 0.071 sec/iter\n",
      "Epoch: 74 | Batch: 009 / 025 | Total loss: 2.428 | Reg loss: 0.030 | Tree loss: 2.428 | Accuracy: 0.156250 | 0.071 sec/iter\n",
      "Epoch: 74 | Batch: 010 / 025 | Total loss: 2.477 | Reg loss: 0.030 | Tree loss: 2.477 | Accuracy: 0.150391 | 0.071 sec/iter\n",
      "Epoch: 74 | Batch: 011 / 025 | Total loss: 2.436 | Reg loss: 0.030 | Tree loss: 2.436 | Accuracy: 0.173828 | 0.071 sec/iter\n",
      "Epoch: 74 | Batch: 012 / 025 | Total loss: 2.419 | Reg loss: 0.030 | Tree loss: 2.419 | Accuracy: 0.130859 | 0.071 sec/iter\n",
      "Epoch: 74 | Batch: 013 / 025 | Total loss: 2.435 | Reg loss: 0.030 | Tree loss: 2.435 | Accuracy: 0.154297 | 0.071 sec/iter\n",
      "Epoch: 74 | Batch: 014 / 025 | Total loss: 2.381 | Reg loss: 0.030 | Tree loss: 2.381 | Accuracy: 0.167969 | 0.071 sec/iter\n",
      "Epoch: 74 | Batch: 015 / 025 | Total loss: 2.427 | Reg loss: 0.030 | Tree loss: 2.427 | Accuracy: 0.158203 | 0.071 sec/iter\n",
      "Epoch: 74 | Batch: 016 / 025 | Total loss: 2.379 | Reg loss: 0.030 | Tree loss: 2.379 | Accuracy: 0.185547 | 0.071 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 74 | Batch: 017 / 025 | Total loss: 2.464 | Reg loss: 0.030 | Tree loss: 2.464 | Accuracy: 0.154297 | 0.071 sec/iter\n",
      "Epoch: 74 | Batch: 018 / 025 | Total loss: 2.380 | Reg loss: 0.030 | Tree loss: 2.380 | Accuracy: 0.167969 | 0.071 sec/iter\n",
      "Epoch: 74 | Batch: 019 / 025 | Total loss: 2.397 | Reg loss: 0.030 | Tree loss: 2.397 | Accuracy: 0.156250 | 0.071 sec/iter\n",
      "Epoch: 74 | Batch: 020 / 025 | Total loss: 2.363 | Reg loss: 0.030 | Tree loss: 2.363 | Accuracy: 0.181641 | 0.071 sec/iter\n",
      "Epoch: 74 | Batch: 021 / 025 | Total loss: 2.355 | Reg loss: 0.030 | Tree loss: 2.355 | Accuracy: 0.207031 | 0.071 sec/iter\n",
      "Epoch: 74 | Batch: 022 / 025 | Total loss: 2.367 | Reg loss: 0.030 | Tree loss: 2.367 | Accuracy: 0.154297 | 0.071 sec/iter\n",
      "Epoch: 74 | Batch: 023 / 025 | Total loss: 2.330 | Reg loss: 0.030 | Tree loss: 2.330 | Accuracy: 0.187500 | 0.071 sec/iter\n",
      "Epoch: 74 | Batch: 024 / 025 | Total loss: 2.364 | Reg loss: 0.030 | Tree loss: 2.364 | Accuracy: 0.165625 | 0.071 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 75 | Batch: 000 / 025 | Total loss: 2.505 | Reg loss: 0.030 | Tree loss: 2.505 | Accuracy: 0.175781 | 0.071 sec/iter\n",
      "Epoch: 75 | Batch: 001 / 025 | Total loss: 2.519 | Reg loss: 0.030 | Tree loss: 2.519 | Accuracy: 0.169922 | 0.071 sec/iter\n",
      "Epoch: 75 | Batch: 002 / 025 | Total loss: 2.508 | Reg loss: 0.030 | Tree loss: 2.508 | Accuracy: 0.166016 | 0.071 sec/iter\n",
      "Epoch: 75 | Batch: 003 / 025 | Total loss: 2.515 | Reg loss: 0.030 | Tree loss: 2.515 | Accuracy: 0.162109 | 0.071 sec/iter\n",
      "Epoch: 75 | Batch: 004 / 025 | Total loss: 2.459 | Reg loss: 0.030 | Tree loss: 2.459 | Accuracy: 0.158203 | 0.071 sec/iter\n",
      "Epoch: 75 | Batch: 005 / 025 | Total loss: 2.442 | Reg loss: 0.030 | Tree loss: 2.442 | Accuracy: 0.169922 | 0.071 sec/iter\n",
      "Epoch: 75 | Batch: 006 / 025 | Total loss: 2.455 | Reg loss: 0.030 | Tree loss: 2.455 | Accuracy: 0.191406 | 0.071 sec/iter\n",
      "Epoch: 75 | Batch: 007 / 025 | Total loss: 2.410 | Reg loss: 0.030 | Tree loss: 2.410 | Accuracy: 0.216797 | 0.071 sec/iter\n",
      "Epoch: 75 | Batch: 008 / 025 | Total loss: 2.436 | Reg loss: 0.030 | Tree loss: 2.436 | Accuracy: 0.154297 | 0.071 sec/iter\n",
      "Epoch: 75 | Batch: 009 / 025 | Total loss: 2.429 | Reg loss: 0.030 | Tree loss: 2.429 | Accuracy: 0.166016 | 0.071 sec/iter\n",
      "Epoch: 75 | Batch: 010 / 025 | Total loss: 2.428 | Reg loss: 0.030 | Tree loss: 2.428 | Accuracy: 0.164062 | 0.071 sec/iter\n",
      "Epoch: 75 | Batch: 011 / 025 | Total loss: 2.434 | Reg loss: 0.030 | Tree loss: 2.434 | Accuracy: 0.158203 | 0.071 sec/iter\n",
      "Epoch: 75 | Batch: 012 / 025 | Total loss: 2.372 | Reg loss: 0.030 | Tree loss: 2.372 | Accuracy: 0.166016 | 0.071 sec/iter\n",
      "Epoch: 75 | Batch: 013 / 025 | Total loss: 2.415 | Reg loss: 0.030 | Tree loss: 2.415 | Accuracy: 0.175781 | 0.071 sec/iter\n",
      "Epoch: 75 | Batch: 014 / 025 | Total loss: 2.373 | Reg loss: 0.030 | Tree loss: 2.373 | Accuracy: 0.152344 | 0.071 sec/iter\n",
      "Epoch: 75 | Batch: 015 / 025 | Total loss: 2.426 | Reg loss: 0.030 | Tree loss: 2.426 | Accuracy: 0.164062 | 0.071 sec/iter\n",
      "Epoch: 75 | Batch: 016 / 025 | Total loss: 2.394 | Reg loss: 0.030 | Tree loss: 2.394 | Accuracy: 0.167969 | 0.071 sec/iter\n",
      "Epoch: 75 | Batch: 017 / 025 | Total loss: 2.372 | Reg loss: 0.030 | Tree loss: 2.372 | Accuracy: 0.146484 | 0.071 sec/iter\n",
      "Epoch: 75 | Batch: 018 / 025 | Total loss: 2.385 | Reg loss: 0.030 | Tree loss: 2.385 | Accuracy: 0.130859 | 0.071 sec/iter\n",
      "Epoch: 75 | Batch: 019 / 025 | Total loss: 2.371 | Reg loss: 0.030 | Tree loss: 2.371 | Accuracy: 0.148438 | 0.071 sec/iter\n",
      "Epoch: 75 | Batch: 020 / 025 | Total loss: 2.361 | Reg loss: 0.030 | Tree loss: 2.361 | Accuracy: 0.183594 | 0.071 sec/iter\n",
      "Epoch: 75 | Batch: 021 / 025 | Total loss: 2.379 | Reg loss: 0.030 | Tree loss: 2.379 | Accuracy: 0.183594 | 0.071 sec/iter\n",
      "Epoch: 75 | Batch: 022 / 025 | Total loss: 2.346 | Reg loss: 0.030 | Tree loss: 2.346 | Accuracy: 0.132812 | 0.071 sec/iter\n",
      "Epoch: 75 | Batch: 023 / 025 | Total loss: 2.374 | Reg loss: 0.030 | Tree loss: 2.374 | Accuracy: 0.152344 | 0.071 sec/iter\n",
      "Epoch: 75 | Batch: 024 / 025 | Total loss: 2.380 | Reg loss: 0.030 | Tree loss: 2.380 | Accuracy: 0.171875 | 0.071 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 76 | Batch: 000 / 025 | Total loss: 2.477 | Reg loss: 0.029 | Tree loss: 2.477 | Accuracy: 0.167969 | 0.071 sec/iter\n",
      "Epoch: 76 | Batch: 001 / 025 | Total loss: 2.508 | Reg loss: 0.029 | Tree loss: 2.508 | Accuracy: 0.167969 | 0.071 sec/iter\n",
      "Epoch: 76 | Batch: 002 / 025 | Total loss: 2.498 | Reg loss: 0.029 | Tree loss: 2.498 | Accuracy: 0.175781 | 0.071 sec/iter\n",
      "Epoch: 76 | Batch: 003 / 025 | Total loss: 2.480 | Reg loss: 0.029 | Tree loss: 2.480 | Accuracy: 0.164062 | 0.071 sec/iter\n",
      "Epoch: 76 | Batch: 004 / 025 | Total loss: 2.413 | Reg loss: 0.029 | Tree loss: 2.413 | Accuracy: 0.187500 | 0.071 sec/iter\n",
      "Epoch: 76 | Batch: 005 / 025 | Total loss: 2.448 | Reg loss: 0.029 | Tree loss: 2.448 | Accuracy: 0.167969 | 0.071 sec/iter\n",
      "Epoch: 76 | Batch: 006 / 025 | Total loss: 2.455 | Reg loss: 0.030 | Tree loss: 2.455 | Accuracy: 0.138672 | 0.071 sec/iter\n",
      "Epoch: 76 | Batch: 007 / 025 | Total loss: 2.480 | Reg loss: 0.030 | Tree loss: 2.480 | Accuracy: 0.146484 | 0.071 sec/iter\n",
      "Epoch: 76 | Batch: 008 / 025 | Total loss: 2.432 | Reg loss: 0.030 | Tree loss: 2.432 | Accuracy: 0.185547 | 0.071 sec/iter\n",
      "Epoch: 76 | Batch: 009 / 025 | Total loss: 2.430 | Reg loss: 0.030 | Tree loss: 2.430 | Accuracy: 0.179688 | 0.071 sec/iter\n",
      "Epoch: 76 | Batch: 010 / 025 | Total loss: 2.421 | Reg loss: 0.030 | Tree loss: 2.421 | Accuracy: 0.173828 | 0.071 sec/iter\n",
      "Epoch: 76 | Batch: 011 / 025 | Total loss: 2.449 | Reg loss: 0.030 | Tree loss: 2.449 | Accuracy: 0.158203 | 0.071 sec/iter\n",
      "Epoch: 76 | Batch: 012 / 025 | Total loss: 2.397 | Reg loss: 0.030 | Tree loss: 2.397 | Accuracy: 0.158203 | 0.071 sec/iter\n",
      "Epoch: 76 | Batch: 013 / 025 | Total loss: 2.420 | Reg loss: 0.030 | Tree loss: 2.420 | Accuracy: 0.164062 | 0.071 sec/iter\n",
      "Epoch: 76 | Batch: 014 / 025 | Total loss: 2.382 | Reg loss: 0.030 | Tree loss: 2.382 | Accuracy: 0.146484 | 0.071 sec/iter\n",
      "Epoch: 76 | Batch: 015 / 025 | Total loss: 2.397 | Reg loss: 0.030 | Tree loss: 2.397 | Accuracy: 0.177734 | 0.071 sec/iter\n",
      "Epoch: 76 | Batch: 016 / 025 | Total loss: 2.345 | Reg loss: 0.030 | Tree loss: 2.345 | Accuracy: 0.175781 | 0.071 sec/iter\n",
      "Epoch: 76 | Batch: 017 / 025 | Total loss: 2.402 | Reg loss: 0.030 | Tree loss: 2.402 | Accuracy: 0.162109 | 0.071 sec/iter\n",
      "Epoch: 76 | Batch: 018 / 025 | Total loss: 2.397 | Reg loss: 0.030 | Tree loss: 2.397 | Accuracy: 0.169922 | 0.071 sec/iter\n",
      "Epoch: 76 | Batch: 019 / 025 | Total loss: 2.380 | Reg loss: 0.030 | Tree loss: 2.380 | Accuracy: 0.150391 | 0.071 sec/iter\n",
      "Epoch: 76 | Batch: 020 / 025 | Total loss: 2.350 | Reg loss: 0.030 | Tree loss: 2.350 | Accuracy: 0.195312 | 0.071 sec/iter\n",
      "Epoch: 76 | Batch: 021 / 025 | Total loss: 2.354 | Reg loss: 0.030 | Tree loss: 2.354 | Accuracy: 0.146484 | 0.071 sec/iter\n",
      "Epoch: 76 | Batch: 022 / 025 | Total loss: 2.333 | Reg loss: 0.030 | Tree loss: 2.333 | Accuracy: 0.171875 | 0.071 sec/iter\n",
      "Epoch: 76 | Batch: 023 / 025 | Total loss: 2.355 | Reg loss: 0.030 | Tree loss: 2.355 | Accuracy: 0.146484 | 0.071 sec/iter\n",
      "Epoch: 76 | Batch: 024 / 025 | Total loss: 2.347 | Reg loss: 0.030 | Tree loss: 2.347 | Accuracy: 0.143750 | 0.071 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 77 | Batch: 000 / 025 | Total loss: 2.494 | Reg loss: 0.029 | Tree loss: 2.494 | Accuracy: 0.164062 | 0.071 sec/iter\n",
      "Epoch: 77 | Batch: 001 / 025 | Total loss: 2.446 | Reg loss: 0.029 | Tree loss: 2.446 | Accuracy: 0.171875 | 0.071 sec/iter\n",
      "Epoch: 77 | Batch: 002 / 025 | Total loss: 2.454 | Reg loss: 0.029 | Tree loss: 2.454 | Accuracy: 0.164062 | 0.071 sec/iter\n",
      "Epoch: 77 | Batch: 003 / 025 | Total loss: 2.501 | Reg loss: 0.029 | Tree loss: 2.501 | Accuracy: 0.166016 | 0.071 sec/iter\n",
      "Epoch: 77 | Batch: 004 / 025 | Total loss: 2.427 | Reg loss: 0.029 | Tree loss: 2.427 | Accuracy: 0.193359 | 0.071 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 77 | Batch: 005 / 025 | Total loss: 2.441 | Reg loss: 0.029 | Tree loss: 2.441 | Accuracy: 0.152344 | 0.071 sec/iter\n",
      "Epoch: 77 | Batch: 006 / 025 | Total loss: 2.433 | Reg loss: 0.029 | Tree loss: 2.433 | Accuracy: 0.162109 | 0.071 sec/iter\n",
      "Epoch: 77 | Batch: 007 / 025 | Total loss: 2.417 | Reg loss: 0.029 | Tree loss: 2.417 | Accuracy: 0.164062 | 0.071 sec/iter\n",
      "Epoch: 77 | Batch: 008 / 025 | Total loss: 2.465 | Reg loss: 0.029 | Tree loss: 2.465 | Accuracy: 0.193359 | 0.071 sec/iter\n",
      "Epoch: 77 | Batch: 009 / 025 | Total loss: 2.447 | Reg loss: 0.030 | Tree loss: 2.447 | Accuracy: 0.154297 | 0.071 sec/iter\n",
      "Epoch: 77 | Batch: 010 / 025 | Total loss: 2.435 | Reg loss: 0.030 | Tree loss: 2.435 | Accuracy: 0.193359 | 0.071 sec/iter\n",
      "Epoch: 77 | Batch: 011 / 025 | Total loss: 2.392 | Reg loss: 0.030 | Tree loss: 2.392 | Accuracy: 0.173828 | 0.071 sec/iter\n",
      "Epoch: 77 | Batch: 012 / 025 | Total loss: 2.402 | Reg loss: 0.030 | Tree loss: 2.402 | Accuracy: 0.156250 | 0.071 sec/iter\n",
      "Epoch: 77 | Batch: 013 / 025 | Total loss: 2.396 | Reg loss: 0.030 | Tree loss: 2.396 | Accuracy: 0.160156 | 0.071 sec/iter\n",
      "Epoch: 77 | Batch: 014 / 025 | Total loss: 2.363 | Reg loss: 0.030 | Tree loss: 2.363 | Accuracy: 0.187500 | 0.071 sec/iter\n",
      "Epoch: 77 | Batch: 015 / 025 | Total loss: 2.352 | Reg loss: 0.030 | Tree loss: 2.352 | Accuracy: 0.150391 | 0.071 sec/iter\n",
      "Epoch: 77 | Batch: 016 / 025 | Total loss: 2.401 | Reg loss: 0.030 | Tree loss: 2.401 | Accuracy: 0.175781 | 0.071 sec/iter\n",
      "Epoch: 77 | Batch: 017 / 025 | Total loss: 2.384 | Reg loss: 0.030 | Tree loss: 2.384 | Accuracy: 0.132812 | 0.071 sec/iter\n",
      "Epoch: 77 | Batch: 018 / 025 | Total loss: 2.400 | Reg loss: 0.030 | Tree loss: 2.400 | Accuracy: 0.158203 | 0.071 sec/iter\n",
      "Epoch: 77 | Batch: 019 / 025 | Total loss: 2.429 | Reg loss: 0.030 | Tree loss: 2.429 | Accuracy: 0.158203 | 0.071 sec/iter\n",
      "Epoch: 77 | Batch: 020 / 025 | Total loss: 2.354 | Reg loss: 0.030 | Tree loss: 2.354 | Accuracy: 0.154297 | 0.071 sec/iter\n",
      "Epoch: 77 | Batch: 021 / 025 | Total loss: 2.373 | Reg loss: 0.030 | Tree loss: 2.373 | Accuracy: 0.160156 | 0.071 sec/iter\n",
      "Epoch: 77 | Batch: 022 / 025 | Total loss: 2.325 | Reg loss: 0.030 | Tree loss: 2.325 | Accuracy: 0.162109 | 0.071 sec/iter\n",
      "Epoch: 77 | Batch: 023 / 025 | Total loss: 2.356 | Reg loss: 0.030 | Tree loss: 2.356 | Accuracy: 0.150391 | 0.071 sec/iter\n",
      "Epoch: 77 | Batch: 024 / 025 | Total loss: 2.346 | Reg loss: 0.030 | Tree loss: 2.346 | Accuracy: 0.184375 | 0.071 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 78 | Batch: 000 / 025 | Total loss: 2.454 | Reg loss: 0.029 | Tree loss: 2.454 | Accuracy: 0.164062 | 0.071 sec/iter\n",
      "Epoch: 78 | Batch: 001 / 025 | Total loss: 2.502 | Reg loss: 0.029 | Tree loss: 2.502 | Accuracy: 0.158203 | 0.071 sec/iter\n",
      "Epoch: 78 | Batch: 002 / 025 | Total loss: 2.509 | Reg loss: 0.029 | Tree loss: 2.509 | Accuracy: 0.156250 | 0.071 sec/iter\n",
      "Epoch: 78 | Batch: 003 / 025 | Total loss: 2.519 | Reg loss: 0.029 | Tree loss: 2.519 | Accuracy: 0.167969 | 0.071 sec/iter\n",
      "Epoch: 78 | Batch: 004 / 025 | Total loss: 2.467 | Reg loss: 0.029 | Tree loss: 2.467 | Accuracy: 0.148438 | 0.071 sec/iter\n",
      "Epoch: 78 | Batch: 005 / 025 | Total loss: 2.421 | Reg loss: 0.029 | Tree loss: 2.421 | Accuracy: 0.158203 | 0.071 sec/iter\n",
      "Epoch: 78 | Batch: 006 / 025 | Total loss: 2.407 | Reg loss: 0.029 | Tree loss: 2.407 | Accuracy: 0.177734 | 0.071 sec/iter\n",
      "Epoch: 78 | Batch: 007 / 025 | Total loss: 2.466 | Reg loss: 0.029 | Tree loss: 2.466 | Accuracy: 0.142578 | 0.071 sec/iter\n",
      "Epoch: 78 | Batch: 008 / 025 | Total loss: 2.398 | Reg loss: 0.029 | Tree loss: 2.398 | Accuracy: 0.175781 | 0.071 sec/iter\n",
      "Epoch: 78 | Batch: 009 / 025 | Total loss: 2.431 | Reg loss: 0.029 | Tree loss: 2.431 | Accuracy: 0.173828 | 0.071 sec/iter\n",
      "Epoch: 78 | Batch: 010 / 025 | Total loss: 2.390 | Reg loss: 0.029 | Tree loss: 2.390 | Accuracy: 0.138672 | 0.071 sec/iter\n",
      "Epoch: 78 | Batch: 011 / 025 | Total loss: 2.428 | Reg loss: 0.030 | Tree loss: 2.428 | Accuracy: 0.156250 | 0.071 sec/iter\n",
      "Epoch: 78 | Batch: 012 / 025 | Total loss: 2.439 | Reg loss: 0.030 | Tree loss: 2.439 | Accuracy: 0.132812 | 0.071 sec/iter\n",
      "Epoch: 78 | Batch: 013 / 025 | Total loss: 2.336 | Reg loss: 0.030 | Tree loss: 2.336 | Accuracy: 0.177734 | 0.071 sec/iter\n",
      "Epoch: 78 | Batch: 014 / 025 | Total loss: 2.398 | Reg loss: 0.030 | Tree loss: 2.398 | Accuracy: 0.142578 | 0.071 sec/iter\n",
      "Epoch: 78 | Batch: 015 / 025 | Total loss: 2.374 | Reg loss: 0.030 | Tree loss: 2.374 | Accuracy: 0.166016 | 0.071 sec/iter\n",
      "Epoch: 78 | Batch: 016 / 025 | Total loss: 2.387 | Reg loss: 0.030 | Tree loss: 2.387 | Accuracy: 0.175781 | 0.071 sec/iter\n",
      "Epoch: 78 | Batch: 017 / 025 | Total loss: 2.360 | Reg loss: 0.030 | Tree loss: 2.360 | Accuracy: 0.175781 | 0.071 sec/iter\n",
      "Epoch: 78 | Batch: 018 / 025 | Total loss: 2.344 | Reg loss: 0.030 | Tree loss: 2.344 | Accuracy: 0.205078 | 0.071 sec/iter\n",
      "Epoch: 78 | Batch: 019 / 025 | Total loss: 2.350 | Reg loss: 0.030 | Tree loss: 2.350 | Accuracy: 0.175781 | 0.071 sec/iter\n",
      "Epoch: 78 | Batch: 020 / 025 | Total loss: 2.362 | Reg loss: 0.030 | Tree loss: 2.362 | Accuracy: 0.167969 | 0.071 sec/iter\n",
      "Epoch: 78 | Batch: 021 / 025 | Total loss: 2.357 | Reg loss: 0.030 | Tree loss: 2.357 | Accuracy: 0.167969 | 0.071 sec/iter\n",
      "Epoch: 78 | Batch: 022 / 025 | Total loss: 2.316 | Reg loss: 0.030 | Tree loss: 2.316 | Accuracy: 0.191406 | 0.071 sec/iter\n",
      "Epoch: 78 | Batch: 023 / 025 | Total loss: 2.371 | Reg loss: 0.030 | Tree loss: 2.371 | Accuracy: 0.138672 | 0.071 sec/iter\n",
      "Epoch: 78 | Batch: 024 / 025 | Total loss: 2.353 | Reg loss: 0.030 | Tree loss: 2.353 | Accuracy: 0.181250 | 0.071 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 79 | Batch: 000 / 025 | Total loss: 2.481 | Reg loss: 0.029 | Tree loss: 2.481 | Accuracy: 0.181641 | 0.071 sec/iter\n",
      "Epoch: 79 | Batch: 001 / 025 | Total loss: 2.444 | Reg loss: 0.029 | Tree loss: 2.444 | Accuracy: 0.210938 | 0.071 sec/iter\n",
      "Epoch: 79 | Batch: 002 / 025 | Total loss: 2.430 | Reg loss: 0.029 | Tree loss: 2.430 | Accuracy: 0.185547 | 0.071 sec/iter\n",
      "Epoch: 79 | Batch: 003 / 025 | Total loss: 2.459 | Reg loss: 0.029 | Tree loss: 2.459 | Accuracy: 0.160156 | 0.071 sec/iter\n",
      "Epoch: 79 | Batch: 004 / 025 | Total loss: 2.425 | Reg loss: 0.029 | Tree loss: 2.425 | Accuracy: 0.136719 | 0.071 sec/iter\n",
      "Epoch: 79 | Batch: 005 / 025 | Total loss: 2.435 | Reg loss: 0.029 | Tree loss: 2.435 | Accuracy: 0.189453 | 0.071 sec/iter\n",
      "Epoch: 79 | Batch: 006 / 025 | Total loss: 2.443 | Reg loss: 0.029 | Tree loss: 2.443 | Accuracy: 0.138672 | 0.071 sec/iter\n",
      "Epoch: 79 | Batch: 007 / 025 | Total loss: 2.461 | Reg loss: 0.029 | Tree loss: 2.461 | Accuracy: 0.156250 | 0.071 sec/iter\n",
      "Epoch: 79 | Batch: 008 / 025 | Total loss: 2.369 | Reg loss: 0.029 | Tree loss: 2.369 | Accuracy: 0.171875 | 0.071 sec/iter\n",
      "Epoch: 79 | Batch: 009 / 025 | Total loss: 2.427 | Reg loss: 0.029 | Tree loss: 2.427 | Accuracy: 0.154297 | 0.071 sec/iter\n",
      "Epoch: 79 | Batch: 010 / 025 | Total loss: 2.439 | Reg loss: 0.029 | Tree loss: 2.439 | Accuracy: 0.164062 | 0.071 sec/iter\n",
      "Epoch: 79 | Batch: 011 / 025 | Total loss: 2.388 | Reg loss: 0.029 | Tree loss: 2.388 | Accuracy: 0.177734 | 0.071 sec/iter\n",
      "Epoch: 79 | Batch: 012 / 025 | Total loss: 2.404 | Reg loss: 0.029 | Tree loss: 2.404 | Accuracy: 0.162109 | 0.071 sec/iter\n",
      "Epoch: 79 | Batch: 013 / 025 | Total loss: 2.389 | Reg loss: 0.030 | Tree loss: 2.389 | Accuracy: 0.158203 | 0.071 sec/iter\n",
      "Epoch: 79 | Batch: 014 / 025 | Total loss: 2.338 | Reg loss: 0.030 | Tree loss: 2.338 | Accuracy: 0.173828 | 0.071 sec/iter\n",
      "Epoch: 79 | Batch: 015 / 025 | Total loss: 2.377 | Reg loss: 0.030 | Tree loss: 2.377 | Accuracy: 0.179688 | 0.071 sec/iter\n",
      "Epoch: 79 | Batch: 016 / 025 | Total loss: 2.366 | Reg loss: 0.030 | Tree loss: 2.366 | Accuracy: 0.154297 | 0.071 sec/iter\n",
      "Epoch: 79 | Batch: 017 / 025 | Total loss: 2.402 | Reg loss: 0.030 | Tree loss: 2.402 | Accuracy: 0.146484 | 0.071 sec/iter\n",
      "Epoch: 79 | Batch: 018 / 025 | Total loss: 2.365 | Reg loss: 0.030 | Tree loss: 2.365 | Accuracy: 0.138672 | 0.071 sec/iter\n",
      "Epoch: 79 | Batch: 019 / 025 | Total loss: 2.386 | Reg loss: 0.030 | Tree loss: 2.386 | Accuracy: 0.150391 | 0.071 sec/iter\n",
      "Epoch: 79 | Batch: 020 / 025 | Total loss: 2.335 | Reg loss: 0.030 | Tree loss: 2.335 | Accuracy: 0.179688 | 0.071 sec/iter\n",
      "Epoch: 79 | Batch: 021 / 025 | Total loss: 2.381 | Reg loss: 0.030 | Tree loss: 2.381 | Accuracy: 0.148438 | 0.071 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 79 | Batch: 022 / 025 | Total loss: 2.381 | Reg loss: 0.030 | Tree loss: 2.381 | Accuracy: 0.154297 | 0.071 sec/iter\n",
      "Epoch: 79 | Batch: 023 / 025 | Total loss: 2.346 | Reg loss: 0.030 | Tree loss: 2.346 | Accuracy: 0.181641 | 0.071 sec/iter\n",
      "Epoch: 79 | Batch: 024 / 025 | Total loss: 2.369 | Reg loss: 0.030 | Tree loss: 2.369 | Accuracy: 0.184375 | 0.071 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 80 | Batch: 000 / 025 | Total loss: 2.462 | Reg loss: 0.029 | Tree loss: 2.462 | Accuracy: 0.156250 | 0.071 sec/iter\n",
      "Epoch: 80 | Batch: 001 / 025 | Total loss: 2.458 | Reg loss: 0.029 | Tree loss: 2.458 | Accuracy: 0.191406 | 0.071 sec/iter\n",
      "Epoch: 80 | Batch: 002 / 025 | Total loss: 2.464 | Reg loss: 0.029 | Tree loss: 2.464 | Accuracy: 0.158203 | 0.071 sec/iter\n",
      "Epoch: 80 | Batch: 003 / 025 | Total loss: 2.509 | Reg loss: 0.029 | Tree loss: 2.509 | Accuracy: 0.154297 | 0.071 sec/iter\n",
      "Epoch: 80 | Batch: 004 / 025 | Total loss: 2.431 | Reg loss: 0.029 | Tree loss: 2.431 | Accuracy: 0.175781 | 0.071 sec/iter\n",
      "Epoch: 80 | Batch: 005 / 025 | Total loss: 2.446 | Reg loss: 0.029 | Tree loss: 2.446 | Accuracy: 0.146484 | 0.071 sec/iter\n",
      "Epoch: 80 | Batch: 006 / 025 | Total loss: 2.444 | Reg loss: 0.029 | Tree loss: 2.444 | Accuracy: 0.177734 | 0.071 sec/iter\n",
      "Epoch: 80 | Batch: 007 / 025 | Total loss: 2.405 | Reg loss: 0.029 | Tree loss: 2.405 | Accuracy: 0.181641 | 0.071 sec/iter\n",
      "Epoch: 80 | Batch: 008 / 025 | Total loss: 2.395 | Reg loss: 0.029 | Tree loss: 2.395 | Accuracy: 0.164062 | 0.071 sec/iter\n",
      "Epoch: 80 | Batch: 009 / 025 | Total loss: 2.440 | Reg loss: 0.029 | Tree loss: 2.440 | Accuracy: 0.156250 | 0.071 sec/iter\n",
      "Epoch: 80 | Batch: 010 / 025 | Total loss: 2.443 | Reg loss: 0.029 | Tree loss: 2.443 | Accuracy: 0.150391 | 0.071 sec/iter\n",
      "Epoch: 80 | Batch: 011 / 025 | Total loss: 2.402 | Reg loss: 0.029 | Tree loss: 2.402 | Accuracy: 0.177734 | 0.071 sec/iter\n",
      "Epoch: 80 | Batch: 012 / 025 | Total loss: 2.394 | Reg loss: 0.029 | Tree loss: 2.394 | Accuracy: 0.136719 | 0.071 sec/iter\n",
      "Epoch: 80 | Batch: 013 / 025 | Total loss: 2.427 | Reg loss: 0.029 | Tree loss: 2.427 | Accuracy: 0.189453 | 0.071 sec/iter\n",
      "Epoch: 80 | Batch: 014 / 025 | Total loss: 2.379 | Reg loss: 0.029 | Tree loss: 2.379 | Accuracy: 0.152344 | 0.071 sec/iter\n",
      "Epoch: 80 | Batch: 015 / 025 | Total loss: 2.367 | Reg loss: 0.030 | Tree loss: 2.367 | Accuracy: 0.138672 | 0.071 sec/iter\n",
      "Epoch: 80 | Batch: 016 / 025 | Total loss: 2.378 | Reg loss: 0.030 | Tree loss: 2.378 | Accuracy: 0.154297 | 0.071 sec/iter\n",
      "Epoch: 80 | Batch: 017 / 025 | Total loss: 2.370 | Reg loss: 0.030 | Tree loss: 2.370 | Accuracy: 0.175781 | 0.071 sec/iter\n",
      "Epoch: 80 | Batch: 018 / 025 | Total loss: 2.316 | Reg loss: 0.030 | Tree loss: 2.316 | Accuracy: 0.171875 | 0.071 sec/iter\n",
      "Epoch: 80 | Batch: 019 / 025 | Total loss: 2.303 | Reg loss: 0.030 | Tree loss: 2.303 | Accuracy: 0.193359 | 0.071 sec/iter\n",
      "Epoch: 80 | Batch: 020 / 025 | Total loss: 2.296 | Reg loss: 0.030 | Tree loss: 2.296 | Accuracy: 0.177734 | 0.071 sec/iter\n",
      "Epoch: 80 | Batch: 021 / 025 | Total loss: 2.376 | Reg loss: 0.030 | Tree loss: 2.376 | Accuracy: 0.126953 | 0.071 sec/iter\n",
      "Epoch: 80 | Batch: 022 / 025 | Total loss: 2.339 | Reg loss: 0.030 | Tree loss: 2.339 | Accuracy: 0.189453 | 0.071 sec/iter\n",
      "Epoch: 80 | Batch: 023 / 025 | Total loss: 2.343 | Reg loss: 0.030 | Tree loss: 2.343 | Accuracy: 0.171875 | 0.071 sec/iter\n",
      "Epoch: 80 | Batch: 024 / 025 | Total loss: 2.341 | Reg loss: 0.030 | Tree loss: 2.341 | Accuracy: 0.150000 | 0.071 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 81 | Batch: 000 / 025 | Total loss: 2.480 | Reg loss: 0.029 | Tree loss: 2.480 | Accuracy: 0.167969 | 0.071 sec/iter\n",
      "Epoch: 81 | Batch: 001 / 025 | Total loss: 2.436 | Reg loss: 0.029 | Tree loss: 2.436 | Accuracy: 0.187500 | 0.071 sec/iter\n",
      "Epoch: 81 | Batch: 002 / 025 | Total loss: 2.467 | Reg loss: 0.029 | Tree loss: 2.467 | Accuracy: 0.177734 | 0.071 sec/iter\n",
      "Epoch: 81 | Batch: 003 / 025 | Total loss: 2.460 | Reg loss: 0.029 | Tree loss: 2.460 | Accuracy: 0.146484 | 0.071 sec/iter\n",
      "Epoch: 81 | Batch: 004 / 025 | Total loss: 2.396 | Reg loss: 0.029 | Tree loss: 2.396 | Accuracy: 0.160156 | 0.071 sec/iter\n",
      "Epoch: 81 | Batch: 005 / 025 | Total loss: 2.417 | Reg loss: 0.029 | Tree loss: 2.417 | Accuracy: 0.154297 | 0.071 sec/iter\n",
      "Epoch: 81 | Batch: 006 / 025 | Total loss: 2.432 | Reg loss: 0.029 | Tree loss: 2.432 | Accuracy: 0.181641 | 0.071 sec/iter\n",
      "Epoch: 81 | Batch: 007 / 025 | Total loss: 2.425 | Reg loss: 0.029 | Tree loss: 2.425 | Accuracy: 0.158203 | 0.071 sec/iter\n",
      "Epoch: 81 | Batch: 008 / 025 | Total loss: 2.394 | Reg loss: 0.029 | Tree loss: 2.394 | Accuracy: 0.171875 | 0.071 sec/iter\n",
      "Epoch: 81 | Batch: 009 / 025 | Total loss: 2.397 | Reg loss: 0.029 | Tree loss: 2.397 | Accuracy: 0.160156 | 0.071 sec/iter\n",
      "Epoch: 81 | Batch: 010 / 025 | Total loss: 2.401 | Reg loss: 0.029 | Tree loss: 2.401 | Accuracy: 0.152344 | 0.071 sec/iter\n",
      "Epoch: 81 | Batch: 011 / 025 | Total loss: 2.403 | Reg loss: 0.029 | Tree loss: 2.403 | Accuracy: 0.152344 | 0.071 sec/iter\n",
      "Epoch: 81 | Batch: 012 / 025 | Total loss: 2.396 | Reg loss: 0.029 | Tree loss: 2.396 | Accuracy: 0.177734 | 0.071 sec/iter\n",
      "Epoch: 81 | Batch: 013 / 025 | Total loss: 2.400 | Reg loss: 0.029 | Tree loss: 2.400 | Accuracy: 0.152344 | 0.071 sec/iter\n",
      "Epoch: 81 | Batch: 014 / 025 | Total loss: 2.359 | Reg loss: 0.029 | Tree loss: 2.359 | Accuracy: 0.154297 | 0.071 sec/iter\n",
      "Epoch: 81 | Batch: 015 / 025 | Total loss: 2.362 | Reg loss: 0.029 | Tree loss: 2.362 | Accuracy: 0.142578 | 0.071 sec/iter\n",
      "Epoch: 81 | Batch: 016 / 025 | Total loss: 2.416 | Reg loss: 0.029 | Tree loss: 2.416 | Accuracy: 0.160156 | 0.071 sec/iter\n",
      "Epoch: 81 | Batch: 017 / 025 | Total loss: 2.346 | Reg loss: 0.030 | Tree loss: 2.346 | Accuracy: 0.150391 | 0.071 sec/iter\n",
      "Epoch: 81 | Batch: 018 / 025 | Total loss: 2.366 | Reg loss: 0.030 | Tree loss: 2.366 | Accuracy: 0.148438 | 0.071 sec/iter\n",
      "Epoch: 81 | Batch: 019 / 025 | Total loss: 2.334 | Reg loss: 0.030 | Tree loss: 2.334 | Accuracy: 0.156250 | 0.071 sec/iter\n",
      "Epoch: 81 | Batch: 020 / 025 | Total loss: 2.361 | Reg loss: 0.030 | Tree loss: 2.361 | Accuracy: 0.193359 | 0.071 sec/iter\n",
      "Epoch: 81 | Batch: 021 / 025 | Total loss: 2.320 | Reg loss: 0.030 | Tree loss: 2.320 | Accuracy: 0.173828 | 0.071 sec/iter\n",
      "Epoch: 81 | Batch: 022 / 025 | Total loss: 2.369 | Reg loss: 0.030 | Tree loss: 2.369 | Accuracy: 0.197266 | 0.071 sec/iter\n",
      "Epoch: 81 | Batch: 023 / 025 | Total loss: 2.362 | Reg loss: 0.030 | Tree loss: 2.362 | Accuracy: 0.195312 | 0.071 sec/iter\n",
      "Epoch: 81 | Batch: 024 / 025 | Total loss: 2.337 | Reg loss: 0.030 | Tree loss: 2.337 | Accuracy: 0.175000 | 0.071 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 82 | Batch: 000 / 025 | Total loss: 2.454 | Reg loss: 0.029 | Tree loss: 2.454 | Accuracy: 0.173828 | 0.071 sec/iter\n",
      "Epoch: 82 | Batch: 001 / 025 | Total loss: 2.469 | Reg loss: 0.029 | Tree loss: 2.469 | Accuracy: 0.164062 | 0.071 sec/iter\n",
      "Epoch: 82 | Batch: 002 / 025 | Total loss: 2.460 | Reg loss: 0.029 | Tree loss: 2.460 | Accuracy: 0.173828 | 0.071 sec/iter\n",
      "Epoch: 82 | Batch: 003 / 025 | Total loss: 2.408 | Reg loss: 0.029 | Tree loss: 2.408 | Accuracy: 0.179688 | 0.071 sec/iter\n",
      "Epoch: 82 | Batch: 004 / 025 | Total loss: 2.457 | Reg loss: 0.029 | Tree loss: 2.457 | Accuracy: 0.146484 | 0.071 sec/iter\n",
      "Epoch: 82 | Batch: 005 / 025 | Total loss: 2.421 | Reg loss: 0.029 | Tree loss: 2.421 | Accuracy: 0.148438 | 0.071 sec/iter\n",
      "Epoch: 82 | Batch: 006 / 025 | Total loss: 2.424 | Reg loss: 0.029 | Tree loss: 2.424 | Accuracy: 0.164062 | 0.071 sec/iter\n",
      "Epoch: 82 | Batch: 007 / 025 | Total loss: 2.416 | Reg loss: 0.029 | Tree loss: 2.416 | Accuracy: 0.158203 | 0.071 sec/iter\n",
      "Epoch: 82 | Batch: 008 / 025 | Total loss: 2.388 | Reg loss: 0.029 | Tree loss: 2.388 | Accuracy: 0.164062 | 0.071 sec/iter\n",
      "Epoch: 82 | Batch: 009 / 025 | Total loss: 2.405 | Reg loss: 0.029 | Tree loss: 2.405 | Accuracy: 0.152344 | 0.071 sec/iter\n",
      "Epoch: 82 | Batch: 010 / 025 | Total loss: 2.382 | Reg loss: 0.029 | Tree loss: 2.382 | Accuracy: 0.173828 | 0.071 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 82 | Batch: 011 / 025 | Total loss: 2.451 | Reg loss: 0.029 | Tree loss: 2.451 | Accuracy: 0.171875 | 0.071 sec/iter\n",
      "Epoch: 82 | Batch: 012 / 025 | Total loss: 2.383 | Reg loss: 0.029 | Tree loss: 2.383 | Accuracy: 0.152344 | 0.071 sec/iter\n",
      "Epoch: 82 | Batch: 013 / 025 | Total loss: 2.386 | Reg loss: 0.029 | Tree loss: 2.386 | Accuracy: 0.158203 | 0.071 sec/iter\n",
      "Epoch: 82 | Batch: 014 / 025 | Total loss: 2.406 | Reg loss: 0.029 | Tree loss: 2.406 | Accuracy: 0.134766 | 0.071 sec/iter\n",
      "Epoch: 82 | Batch: 015 / 025 | Total loss: 2.348 | Reg loss: 0.029 | Tree loss: 2.348 | Accuracy: 0.146484 | 0.071 sec/iter\n",
      "Epoch: 82 | Batch: 016 / 025 | Total loss: 2.357 | Reg loss: 0.029 | Tree loss: 2.357 | Accuracy: 0.191406 | 0.071 sec/iter\n",
      "Epoch: 82 | Batch: 017 / 025 | Total loss: 2.319 | Reg loss: 0.029 | Tree loss: 2.319 | Accuracy: 0.167969 | 0.071 sec/iter\n",
      "Epoch: 82 | Batch: 018 / 025 | Total loss: 2.344 | Reg loss: 0.029 | Tree loss: 2.344 | Accuracy: 0.183594 | 0.071 sec/iter\n",
      "Epoch: 82 | Batch: 019 / 025 | Total loss: 2.332 | Reg loss: 0.030 | Tree loss: 2.332 | Accuracy: 0.175781 | 0.071 sec/iter\n",
      "Epoch: 82 | Batch: 020 / 025 | Total loss: 2.341 | Reg loss: 0.030 | Tree loss: 2.341 | Accuracy: 0.152344 | 0.071 sec/iter\n",
      "Epoch: 82 | Batch: 021 / 025 | Total loss: 2.358 | Reg loss: 0.030 | Tree loss: 2.358 | Accuracy: 0.164062 | 0.071 sec/iter\n",
      "Epoch: 82 | Batch: 022 / 025 | Total loss: 2.336 | Reg loss: 0.030 | Tree loss: 2.336 | Accuracy: 0.185547 | 0.071 sec/iter\n",
      "Epoch: 82 | Batch: 023 / 025 | Total loss: 2.372 | Reg loss: 0.030 | Tree loss: 2.372 | Accuracy: 0.181641 | 0.071 sec/iter\n",
      "Epoch: 82 | Batch: 024 / 025 | Total loss: 2.318 | Reg loss: 0.030 | Tree loss: 2.318 | Accuracy: 0.165625 | 0.071 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 83 | Batch: 000 / 025 | Total loss: 2.483 | Reg loss: 0.029 | Tree loss: 2.483 | Accuracy: 0.154297 | 0.071 sec/iter\n",
      "Epoch: 83 | Batch: 001 / 025 | Total loss: 2.485 | Reg loss: 0.029 | Tree loss: 2.485 | Accuracy: 0.181641 | 0.071 sec/iter\n",
      "Epoch: 83 | Batch: 002 / 025 | Total loss: 2.409 | Reg loss: 0.029 | Tree loss: 2.409 | Accuracy: 0.179688 | 0.071 sec/iter\n",
      "Epoch: 83 | Batch: 003 / 025 | Total loss: 2.441 | Reg loss: 0.029 | Tree loss: 2.441 | Accuracy: 0.175781 | 0.071 sec/iter\n",
      "Epoch: 83 | Batch: 004 / 025 | Total loss: 2.423 | Reg loss: 0.029 | Tree loss: 2.423 | Accuracy: 0.181641 | 0.071 sec/iter\n",
      "Epoch: 83 | Batch: 005 / 025 | Total loss: 2.396 | Reg loss: 0.029 | Tree loss: 2.396 | Accuracy: 0.171875 | 0.071 sec/iter\n",
      "Epoch: 83 | Batch: 006 / 025 | Total loss: 2.410 | Reg loss: 0.029 | Tree loss: 2.410 | Accuracy: 0.162109 | 0.071 sec/iter\n",
      "Epoch: 83 | Batch: 007 / 025 | Total loss: 2.390 | Reg loss: 0.029 | Tree loss: 2.390 | Accuracy: 0.166016 | 0.071 sec/iter\n",
      "Epoch: 83 | Batch: 008 / 025 | Total loss: 2.463 | Reg loss: 0.029 | Tree loss: 2.463 | Accuracy: 0.142578 | 0.071 sec/iter\n",
      "Epoch: 83 | Batch: 009 / 025 | Total loss: 2.410 | Reg loss: 0.029 | Tree loss: 2.410 | Accuracy: 0.128906 | 0.071 sec/iter\n",
      "Epoch: 83 | Batch: 010 / 025 | Total loss: 2.348 | Reg loss: 0.029 | Tree loss: 2.348 | Accuracy: 0.162109 | 0.071 sec/iter\n",
      "Epoch: 83 | Batch: 011 / 025 | Total loss: 2.396 | Reg loss: 0.029 | Tree loss: 2.396 | Accuracy: 0.191406 | 0.071 sec/iter\n",
      "Epoch: 83 | Batch: 012 / 025 | Total loss: 2.368 | Reg loss: 0.029 | Tree loss: 2.368 | Accuracy: 0.164062 | 0.071 sec/iter\n",
      "Epoch: 83 | Batch: 013 / 025 | Total loss: 2.366 | Reg loss: 0.029 | Tree loss: 2.366 | Accuracy: 0.171875 | 0.071 sec/iter\n",
      "Epoch: 83 | Batch: 014 / 025 | Total loss: 2.368 | Reg loss: 0.029 | Tree loss: 2.368 | Accuracy: 0.185547 | 0.071 sec/iter\n",
      "Epoch: 83 | Batch: 015 / 025 | Total loss: 2.386 | Reg loss: 0.029 | Tree loss: 2.386 | Accuracy: 0.164062 | 0.071 sec/iter\n",
      "Epoch: 83 | Batch: 016 / 025 | Total loss: 2.359 | Reg loss: 0.029 | Tree loss: 2.359 | Accuracy: 0.171875 | 0.071 sec/iter\n",
      "Epoch: 83 | Batch: 017 / 025 | Total loss: 2.341 | Reg loss: 0.029 | Tree loss: 2.341 | Accuracy: 0.154297 | 0.071 sec/iter\n",
      "Epoch: 83 | Batch: 018 / 025 | Total loss: 2.341 | Reg loss: 0.029 | Tree loss: 2.341 | Accuracy: 0.173828 | 0.071 sec/iter\n",
      "Epoch: 83 | Batch: 019 / 025 | Total loss: 2.359 | Reg loss: 0.029 | Tree loss: 2.359 | Accuracy: 0.175781 | 0.071 sec/iter\n",
      "Epoch: 83 | Batch: 020 / 025 | Total loss: 2.374 | Reg loss: 0.030 | Tree loss: 2.374 | Accuracy: 0.150391 | 0.071 sec/iter\n",
      "Epoch: 83 | Batch: 021 / 025 | Total loss: 2.314 | Reg loss: 0.030 | Tree loss: 2.314 | Accuracy: 0.158203 | 0.071 sec/iter\n",
      "Epoch: 83 | Batch: 022 / 025 | Total loss: 2.377 | Reg loss: 0.030 | Tree loss: 2.377 | Accuracy: 0.156250 | 0.071 sec/iter\n",
      "Epoch: 83 | Batch: 023 / 025 | Total loss: 2.309 | Reg loss: 0.030 | Tree loss: 2.309 | Accuracy: 0.164062 | 0.071 sec/iter\n",
      "Epoch: 83 | Batch: 024 / 025 | Total loss: 2.339 | Reg loss: 0.030 | Tree loss: 2.339 | Accuracy: 0.140625 | 0.071 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 84 | Batch: 000 / 025 | Total loss: 2.426 | Reg loss: 0.029 | Tree loss: 2.426 | Accuracy: 0.167969 | 0.071 sec/iter\n",
      "Epoch: 84 | Batch: 001 / 025 | Total loss: 2.446 | Reg loss: 0.029 | Tree loss: 2.446 | Accuracy: 0.169922 | 0.071 sec/iter\n",
      "Epoch: 84 | Batch: 002 / 025 | Total loss: 2.460 | Reg loss: 0.029 | Tree loss: 2.460 | Accuracy: 0.164062 | 0.071 sec/iter\n",
      "Epoch: 84 | Batch: 003 / 025 | Total loss: 2.439 | Reg loss: 0.029 | Tree loss: 2.439 | Accuracy: 0.160156 | 0.071 sec/iter\n",
      "Epoch: 84 | Batch: 004 / 025 | Total loss: 2.447 | Reg loss: 0.029 | Tree loss: 2.447 | Accuracy: 0.138672 | 0.071 sec/iter\n",
      "Epoch: 84 | Batch: 005 / 025 | Total loss: 2.453 | Reg loss: 0.029 | Tree loss: 2.453 | Accuracy: 0.164062 | 0.071 sec/iter\n",
      "Epoch: 84 | Batch: 006 / 025 | Total loss: 2.418 | Reg loss: 0.029 | Tree loss: 2.418 | Accuracy: 0.150391 | 0.071 sec/iter\n",
      "Epoch: 84 | Batch: 007 / 025 | Total loss: 2.419 | Reg loss: 0.029 | Tree loss: 2.419 | Accuracy: 0.179688 | 0.071 sec/iter\n",
      "Epoch: 84 | Batch: 008 / 025 | Total loss: 2.385 | Reg loss: 0.029 | Tree loss: 2.385 | Accuracy: 0.167969 | 0.071 sec/iter\n",
      "Epoch: 84 | Batch: 009 / 025 | Total loss: 2.400 | Reg loss: 0.029 | Tree loss: 2.400 | Accuracy: 0.169922 | 0.071 sec/iter\n",
      "Epoch: 84 | Batch: 010 / 025 | Total loss: 2.393 | Reg loss: 0.029 | Tree loss: 2.393 | Accuracy: 0.181641 | 0.071 sec/iter\n",
      "Epoch: 84 | Batch: 011 / 025 | Total loss: 2.381 | Reg loss: 0.029 | Tree loss: 2.381 | Accuracy: 0.164062 | 0.071 sec/iter\n",
      "Epoch: 84 | Batch: 012 / 025 | Total loss: 2.377 | Reg loss: 0.029 | Tree loss: 2.377 | Accuracy: 0.166016 | 0.071 sec/iter\n",
      "Epoch: 84 | Batch: 013 / 025 | Total loss: 2.355 | Reg loss: 0.029 | Tree loss: 2.355 | Accuracy: 0.177734 | 0.071 sec/iter\n",
      "Epoch: 84 | Batch: 014 / 025 | Total loss: 2.365 | Reg loss: 0.029 | Tree loss: 2.365 | Accuracy: 0.150391 | 0.071 sec/iter\n",
      "Epoch: 84 | Batch: 015 / 025 | Total loss: 2.343 | Reg loss: 0.029 | Tree loss: 2.343 | Accuracy: 0.154297 | 0.071 sec/iter\n",
      "Epoch: 84 | Batch: 016 / 025 | Total loss: 2.351 | Reg loss: 0.029 | Tree loss: 2.351 | Accuracy: 0.185547 | 0.071 sec/iter\n",
      "Epoch: 84 | Batch: 017 / 025 | Total loss: 2.329 | Reg loss: 0.029 | Tree loss: 2.329 | Accuracy: 0.148438 | 0.071 sec/iter\n",
      "Epoch: 84 | Batch: 018 / 025 | Total loss: 2.381 | Reg loss: 0.029 | Tree loss: 2.381 | Accuracy: 0.173828 | 0.071 sec/iter\n",
      "Epoch: 84 | Batch: 019 / 025 | Total loss: 2.346 | Reg loss: 0.029 | Tree loss: 2.346 | Accuracy: 0.152344 | 0.071 sec/iter\n",
      "Epoch: 84 | Batch: 020 / 025 | Total loss: 2.372 | Reg loss: 0.029 | Tree loss: 2.372 | Accuracy: 0.160156 | 0.071 sec/iter\n",
      "Epoch: 84 | Batch: 021 / 025 | Total loss: 2.334 | Reg loss: 0.029 | Tree loss: 2.334 | Accuracy: 0.193359 | 0.071 sec/iter\n",
      "Epoch: 84 | Batch: 022 / 025 | Total loss: 2.325 | Reg loss: 0.030 | Tree loss: 2.325 | Accuracy: 0.166016 | 0.071 sec/iter\n",
      "Epoch: 84 | Batch: 023 / 025 | Total loss: 2.318 | Reg loss: 0.030 | Tree loss: 2.318 | Accuracy: 0.162109 | 0.071 sec/iter\n",
      "Epoch: 84 | Batch: 024 / 025 | Total loss: 2.300 | Reg loss: 0.030 | Tree loss: 2.300 | Accuracy: 0.153125 | 0.071 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 85 | Batch: 000 / 025 | Total loss: 2.457 | Reg loss: 0.029 | Tree loss: 2.457 | Accuracy: 0.164062 | 0.071 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 85 | Batch: 001 / 025 | Total loss: 2.448 | Reg loss: 0.029 | Tree loss: 2.448 | Accuracy: 0.166016 | 0.071 sec/iter\n",
      "Epoch: 85 | Batch: 002 / 025 | Total loss: 2.394 | Reg loss: 0.029 | Tree loss: 2.394 | Accuracy: 0.183594 | 0.071 sec/iter\n",
      "Epoch: 85 | Batch: 003 / 025 | Total loss: 2.450 | Reg loss: 0.029 | Tree loss: 2.450 | Accuracy: 0.138672 | 0.071 sec/iter\n",
      "Epoch: 85 | Batch: 004 / 025 | Total loss: 2.483 | Reg loss: 0.029 | Tree loss: 2.483 | Accuracy: 0.158203 | 0.071 sec/iter\n",
      "Epoch: 85 | Batch: 005 / 025 | Total loss: 2.396 | Reg loss: 0.029 | Tree loss: 2.396 | Accuracy: 0.171875 | 0.071 sec/iter\n",
      "Epoch: 85 | Batch: 006 / 025 | Total loss: 2.448 | Reg loss: 0.029 | Tree loss: 2.448 | Accuracy: 0.148438 | 0.071 sec/iter\n",
      "Epoch: 85 | Batch: 007 / 025 | Total loss: 2.394 | Reg loss: 0.029 | Tree loss: 2.394 | Accuracy: 0.169922 | 0.071 sec/iter\n",
      "Epoch: 85 | Batch: 008 / 025 | Total loss: 2.405 | Reg loss: 0.029 | Tree loss: 2.405 | Accuracy: 0.150391 | 0.071 sec/iter\n",
      "Epoch: 85 | Batch: 009 / 025 | Total loss: 2.363 | Reg loss: 0.029 | Tree loss: 2.363 | Accuracy: 0.199219 | 0.071 sec/iter\n",
      "Epoch: 85 | Batch: 010 / 025 | Total loss: 2.402 | Reg loss: 0.029 | Tree loss: 2.402 | Accuracy: 0.171875 | 0.071 sec/iter\n",
      "Epoch: 85 | Batch: 011 / 025 | Total loss: 2.365 | Reg loss: 0.029 | Tree loss: 2.365 | Accuracy: 0.167969 | 0.071 sec/iter\n",
      "Epoch: 85 | Batch: 012 / 025 | Total loss: 2.391 | Reg loss: 0.029 | Tree loss: 2.391 | Accuracy: 0.167969 | 0.071 sec/iter\n",
      "Epoch: 85 | Batch: 013 / 025 | Total loss: 2.350 | Reg loss: 0.029 | Tree loss: 2.350 | Accuracy: 0.167969 | 0.071 sec/iter\n",
      "Epoch: 85 | Batch: 014 / 025 | Total loss: 2.379 | Reg loss: 0.029 | Tree loss: 2.379 | Accuracy: 0.175781 | 0.071 sec/iter\n",
      "Epoch: 85 | Batch: 015 / 025 | Total loss: 2.386 | Reg loss: 0.029 | Tree loss: 2.386 | Accuracy: 0.150391 | 0.071 sec/iter\n",
      "Epoch: 85 | Batch: 016 / 025 | Total loss: 2.322 | Reg loss: 0.029 | Tree loss: 2.322 | Accuracy: 0.175781 | 0.071 sec/iter\n",
      "Epoch: 85 | Batch: 017 / 025 | Total loss: 2.329 | Reg loss: 0.029 | Tree loss: 2.329 | Accuracy: 0.158203 | 0.071 sec/iter\n",
      "Epoch: 85 | Batch: 018 / 025 | Total loss: 2.341 | Reg loss: 0.029 | Tree loss: 2.341 | Accuracy: 0.150391 | 0.071 sec/iter\n",
      "Epoch: 85 | Batch: 019 / 025 | Total loss: 2.343 | Reg loss: 0.029 | Tree loss: 2.343 | Accuracy: 0.156250 | 0.071 sec/iter\n",
      "Epoch: 85 | Batch: 020 / 025 | Total loss: 2.332 | Reg loss: 0.029 | Tree loss: 2.332 | Accuracy: 0.169922 | 0.071 sec/iter\n",
      "Epoch: 85 | Batch: 021 / 025 | Total loss: 2.316 | Reg loss: 0.029 | Tree loss: 2.316 | Accuracy: 0.173828 | 0.071 sec/iter\n",
      "Epoch: 85 | Batch: 022 / 025 | Total loss: 2.356 | Reg loss: 0.029 | Tree loss: 2.356 | Accuracy: 0.169922 | 0.071 sec/iter\n",
      "Epoch: 85 | Batch: 023 / 025 | Total loss: 2.317 | Reg loss: 0.029 | Tree loss: 2.317 | Accuracy: 0.191406 | 0.071 sec/iter\n",
      "Epoch: 85 | Batch: 024 / 025 | Total loss: 2.331 | Reg loss: 0.030 | Tree loss: 2.331 | Accuracy: 0.143750 | 0.071 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 86 | Batch: 000 / 025 | Total loss: 2.465 | Reg loss: 0.029 | Tree loss: 2.465 | Accuracy: 0.142578 | 0.071 sec/iter\n",
      "Epoch: 86 | Batch: 001 / 025 | Total loss: 2.440 | Reg loss: 0.029 | Tree loss: 2.440 | Accuracy: 0.195312 | 0.071 sec/iter\n",
      "Epoch: 86 | Batch: 002 / 025 | Total loss: 2.447 | Reg loss: 0.029 | Tree loss: 2.447 | Accuracy: 0.193359 | 0.071 sec/iter\n",
      "Epoch: 86 | Batch: 003 / 025 | Total loss: 2.465 | Reg loss: 0.029 | Tree loss: 2.465 | Accuracy: 0.140625 | 0.071 sec/iter\n",
      "Epoch: 86 | Batch: 004 / 025 | Total loss: 2.487 | Reg loss: 0.029 | Tree loss: 2.487 | Accuracy: 0.150391 | 0.071 sec/iter\n",
      "Epoch: 86 | Batch: 005 / 025 | Total loss: 2.414 | Reg loss: 0.029 | Tree loss: 2.414 | Accuracy: 0.175781 | 0.071 sec/iter\n",
      "Epoch: 86 | Batch: 006 / 025 | Total loss: 2.389 | Reg loss: 0.029 | Tree loss: 2.389 | Accuracy: 0.156250 | 0.071 sec/iter\n",
      "Epoch: 86 | Batch: 007 / 025 | Total loss: 2.395 | Reg loss: 0.029 | Tree loss: 2.395 | Accuracy: 0.154297 | 0.071 sec/iter\n",
      "Epoch: 86 | Batch: 008 / 025 | Total loss: 2.431 | Reg loss: 0.029 | Tree loss: 2.431 | Accuracy: 0.166016 | 0.071 sec/iter\n",
      "Epoch: 86 | Batch: 009 / 025 | Total loss: 2.375 | Reg loss: 0.029 | Tree loss: 2.375 | Accuracy: 0.177734 | 0.071 sec/iter\n",
      "Epoch: 86 | Batch: 010 / 025 | Total loss: 2.358 | Reg loss: 0.029 | Tree loss: 2.358 | Accuracy: 0.167969 | 0.071 sec/iter\n",
      "Epoch: 86 | Batch: 011 / 025 | Total loss: 2.357 | Reg loss: 0.029 | Tree loss: 2.357 | Accuracy: 0.185547 | 0.071 sec/iter\n",
      "Epoch: 86 | Batch: 012 / 025 | Total loss: 2.367 | Reg loss: 0.029 | Tree loss: 2.367 | Accuracy: 0.166016 | 0.071 sec/iter\n",
      "Epoch: 86 | Batch: 013 / 025 | Total loss: 2.338 | Reg loss: 0.029 | Tree loss: 2.338 | Accuracy: 0.195312 | 0.071 sec/iter\n",
      "Epoch: 86 | Batch: 014 / 025 | Total loss: 2.395 | Reg loss: 0.029 | Tree loss: 2.395 | Accuracy: 0.150391 | 0.071 sec/iter\n",
      "Epoch: 86 | Batch: 015 / 025 | Total loss: 2.371 | Reg loss: 0.029 | Tree loss: 2.371 | Accuracy: 0.156250 | 0.071 sec/iter\n",
      "Epoch: 86 | Batch: 016 / 025 | Total loss: 2.310 | Reg loss: 0.029 | Tree loss: 2.310 | Accuracy: 0.187500 | 0.071 sec/iter\n",
      "Epoch: 86 | Batch: 017 / 025 | Total loss: 2.296 | Reg loss: 0.029 | Tree loss: 2.296 | Accuracy: 0.164062 | 0.071 sec/iter\n",
      "Epoch: 86 | Batch: 018 / 025 | Total loss: 2.354 | Reg loss: 0.029 | Tree loss: 2.354 | Accuracy: 0.166016 | 0.071 sec/iter\n",
      "Epoch: 86 | Batch: 019 / 025 | Total loss: 2.318 | Reg loss: 0.029 | Tree loss: 2.318 | Accuracy: 0.177734 | 0.071 sec/iter\n",
      "Epoch: 86 | Batch: 020 / 025 | Total loss: 2.341 | Reg loss: 0.029 | Tree loss: 2.341 | Accuracy: 0.187500 | 0.071 sec/iter\n",
      "Epoch: 86 | Batch: 021 / 025 | Total loss: 2.358 | Reg loss: 0.029 | Tree loss: 2.358 | Accuracy: 0.150391 | 0.071 sec/iter\n",
      "Epoch: 86 | Batch: 022 / 025 | Total loss: 2.328 | Reg loss: 0.029 | Tree loss: 2.328 | Accuracy: 0.150391 | 0.071 sec/iter\n",
      "Epoch: 86 | Batch: 023 / 025 | Total loss: 2.291 | Reg loss: 0.029 | Tree loss: 2.291 | Accuracy: 0.144531 | 0.071 sec/iter\n",
      "Epoch: 86 | Batch: 024 / 025 | Total loss: 2.326 | Reg loss: 0.029 | Tree loss: 2.326 | Accuracy: 0.165625 | 0.071 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 87 | Batch: 000 / 025 | Total loss: 2.440 | Reg loss: 0.029 | Tree loss: 2.440 | Accuracy: 0.169922 | 0.071 sec/iter\n",
      "Epoch: 87 | Batch: 001 / 025 | Total loss: 2.447 | Reg loss: 0.029 | Tree loss: 2.447 | Accuracy: 0.156250 | 0.071 sec/iter\n",
      "Epoch: 87 | Batch: 002 / 025 | Total loss: 2.444 | Reg loss: 0.029 | Tree loss: 2.444 | Accuracy: 0.162109 | 0.071 sec/iter\n",
      "Epoch: 87 | Batch: 003 / 025 | Total loss: 2.413 | Reg loss: 0.029 | Tree loss: 2.413 | Accuracy: 0.189453 | 0.071 sec/iter\n",
      "Epoch: 87 | Batch: 004 / 025 | Total loss: 2.430 | Reg loss: 0.029 | Tree loss: 2.430 | Accuracy: 0.146484 | 0.071 sec/iter\n",
      "Epoch: 87 | Batch: 005 / 025 | Total loss: 2.390 | Reg loss: 0.029 | Tree loss: 2.390 | Accuracy: 0.156250 | 0.071 sec/iter\n",
      "Epoch: 87 | Batch: 006 / 025 | Total loss: 2.406 | Reg loss: 0.029 | Tree loss: 2.406 | Accuracy: 0.171875 | 0.071 sec/iter\n",
      "Epoch: 87 | Batch: 007 / 025 | Total loss: 2.412 | Reg loss: 0.029 | Tree loss: 2.412 | Accuracy: 0.175781 | 0.071 sec/iter\n",
      "Epoch: 87 | Batch: 008 / 025 | Total loss: 2.387 | Reg loss: 0.029 | Tree loss: 2.387 | Accuracy: 0.185547 | 0.071 sec/iter\n",
      "Epoch: 87 | Batch: 009 / 025 | Total loss: 2.397 | Reg loss: 0.029 | Tree loss: 2.397 | Accuracy: 0.171875 | 0.071 sec/iter\n",
      "Epoch: 87 | Batch: 010 / 025 | Total loss: 2.385 | Reg loss: 0.029 | Tree loss: 2.385 | Accuracy: 0.128906 | 0.071 sec/iter\n",
      "Epoch: 87 | Batch: 011 / 025 | Total loss: 2.336 | Reg loss: 0.029 | Tree loss: 2.336 | Accuracy: 0.167969 | 0.071 sec/iter\n",
      "Epoch: 87 | Batch: 012 / 025 | Total loss: 2.401 | Reg loss: 0.029 | Tree loss: 2.401 | Accuracy: 0.181641 | 0.071 sec/iter\n",
      "Epoch: 87 | Batch: 013 / 025 | Total loss: 2.358 | Reg loss: 0.029 | Tree loss: 2.358 | Accuracy: 0.154297 | 0.071 sec/iter\n",
      "Epoch: 87 | Batch: 014 / 025 | Total loss: 2.360 | Reg loss: 0.029 | Tree loss: 2.360 | Accuracy: 0.130859 | 0.071 sec/iter\n",
      "Epoch: 87 | Batch: 015 / 025 | Total loss: 2.359 | Reg loss: 0.029 | Tree loss: 2.359 | Accuracy: 0.162109 | 0.071 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 87 | Batch: 016 / 025 | Total loss: 2.355 | Reg loss: 0.029 | Tree loss: 2.355 | Accuracy: 0.175781 | 0.071 sec/iter\n",
      "Epoch: 87 | Batch: 017 / 025 | Total loss: 2.334 | Reg loss: 0.029 | Tree loss: 2.334 | Accuracy: 0.160156 | 0.071 sec/iter\n",
      "Epoch: 87 | Batch: 018 / 025 | Total loss: 2.317 | Reg loss: 0.029 | Tree loss: 2.317 | Accuracy: 0.173828 | 0.071 sec/iter\n",
      "Epoch: 87 | Batch: 019 / 025 | Total loss: 2.341 | Reg loss: 0.029 | Tree loss: 2.341 | Accuracy: 0.152344 | 0.071 sec/iter\n",
      "Epoch: 87 | Batch: 020 / 025 | Total loss: 2.353 | Reg loss: 0.029 | Tree loss: 2.353 | Accuracy: 0.177734 | 0.071 sec/iter\n",
      "Epoch: 87 | Batch: 021 / 025 | Total loss: 2.351 | Reg loss: 0.029 | Tree loss: 2.351 | Accuracy: 0.162109 | 0.071 sec/iter\n",
      "Epoch: 87 | Batch: 022 / 025 | Total loss: 2.337 | Reg loss: 0.029 | Tree loss: 2.337 | Accuracy: 0.150391 | 0.071 sec/iter\n",
      "Epoch: 87 | Batch: 023 / 025 | Total loss: 2.287 | Reg loss: 0.029 | Tree loss: 2.287 | Accuracy: 0.214844 | 0.071 sec/iter\n",
      "Epoch: 87 | Batch: 024 / 025 | Total loss: 2.285 | Reg loss: 0.029 | Tree loss: 2.285 | Accuracy: 0.171875 | 0.071 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 88 | Batch: 000 / 025 | Total loss: 2.441 | Reg loss: 0.029 | Tree loss: 2.441 | Accuracy: 0.185547 | 0.071 sec/iter\n",
      "Epoch: 88 | Batch: 001 / 025 | Total loss: 2.465 | Reg loss: 0.029 | Tree loss: 2.465 | Accuracy: 0.144531 | 0.071 sec/iter\n",
      "Epoch: 88 | Batch: 002 / 025 | Total loss: 2.459 | Reg loss: 0.029 | Tree loss: 2.459 | Accuracy: 0.148438 | 0.071 sec/iter\n",
      "Epoch: 88 | Batch: 003 / 025 | Total loss: 2.455 | Reg loss: 0.029 | Tree loss: 2.455 | Accuracy: 0.148438 | 0.071 sec/iter\n",
      "Epoch: 88 | Batch: 004 / 025 | Total loss: 2.388 | Reg loss: 0.029 | Tree loss: 2.388 | Accuracy: 0.164062 | 0.071 sec/iter\n",
      "Epoch: 88 | Batch: 005 / 025 | Total loss: 2.378 | Reg loss: 0.029 | Tree loss: 2.378 | Accuracy: 0.185547 | 0.071 sec/iter\n",
      "Epoch: 88 | Batch: 006 / 025 | Total loss: 2.438 | Reg loss: 0.029 | Tree loss: 2.438 | Accuracy: 0.150391 | 0.071 sec/iter\n",
      "Epoch: 88 | Batch: 007 / 025 | Total loss: 2.413 | Reg loss: 0.029 | Tree loss: 2.413 | Accuracy: 0.140625 | 0.071 sec/iter\n",
      "Epoch: 88 | Batch: 008 / 025 | Total loss: 2.434 | Reg loss: 0.029 | Tree loss: 2.434 | Accuracy: 0.177734 | 0.071 sec/iter\n",
      "Epoch: 88 | Batch: 009 / 025 | Total loss: 2.372 | Reg loss: 0.029 | Tree loss: 2.372 | Accuracy: 0.158203 | 0.071 sec/iter\n",
      "Epoch: 88 | Batch: 010 / 025 | Total loss: 2.333 | Reg loss: 0.029 | Tree loss: 2.333 | Accuracy: 0.207031 | 0.071 sec/iter\n",
      "Epoch: 88 | Batch: 011 / 025 | Total loss: 2.400 | Reg loss: 0.029 | Tree loss: 2.400 | Accuracy: 0.164062 | 0.071 sec/iter\n",
      "Epoch: 88 | Batch: 012 / 025 | Total loss: 2.355 | Reg loss: 0.029 | Tree loss: 2.355 | Accuracy: 0.162109 | 0.071 sec/iter\n",
      "Epoch: 88 | Batch: 013 / 025 | Total loss: 2.373 | Reg loss: 0.029 | Tree loss: 2.373 | Accuracy: 0.169922 | 0.071 sec/iter\n",
      "Epoch: 88 | Batch: 014 / 025 | Total loss: 2.347 | Reg loss: 0.029 | Tree loss: 2.347 | Accuracy: 0.160156 | 0.071 sec/iter\n",
      "Epoch: 88 | Batch: 015 / 025 | Total loss: 2.347 | Reg loss: 0.029 | Tree loss: 2.347 | Accuracy: 0.173828 | 0.071 sec/iter\n",
      "Epoch: 88 | Batch: 016 / 025 | Total loss: 2.342 | Reg loss: 0.029 | Tree loss: 2.342 | Accuracy: 0.164062 | 0.071 sec/iter\n",
      "Epoch: 88 | Batch: 017 / 025 | Total loss: 2.341 | Reg loss: 0.029 | Tree loss: 2.341 | Accuracy: 0.179688 | 0.071 sec/iter\n",
      "Epoch: 88 | Batch: 018 / 025 | Total loss: 2.342 | Reg loss: 0.029 | Tree loss: 2.342 | Accuracy: 0.166016 | 0.071 sec/iter\n",
      "Epoch: 88 | Batch: 019 / 025 | Total loss: 2.325 | Reg loss: 0.029 | Tree loss: 2.325 | Accuracy: 0.173828 | 0.071 sec/iter\n",
      "Epoch: 88 | Batch: 020 / 025 | Total loss: 2.302 | Reg loss: 0.029 | Tree loss: 2.302 | Accuracy: 0.142578 | 0.071 sec/iter\n",
      "Epoch: 88 | Batch: 021 / 025 | Total loss: 2.292 | Reg loss: 0.029 | Tree loss: 2.292 | Accuracy: 0.191406 | 0.071 sec/iter\n",
      "Epoch: 88 | Batch: 022 / 025 | Total loss: 2.332 | Reg loss: 0.029 | Tree loss: 2.332 | Accuracy: 0.156250 | 0.071 sec/iter\n",
      "Epoch: 88 | Batch: 023 / 025 | Total loss: 2.297 | Reg loss: 0.029 | Tree loss: 2.297 | Accuracy: 0.181641 | 0.071 sec/iter\n",
      "Epoch: 88 | Batch: 024 / 025 | Total loss: 2.295 | Reg loss: 0.029 | Tree loss: 2.295 | Accuracy: 0.134375 | 0.071 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 89 | Batch: 000 / 025 | Total loss: 2.461 | Reg loss: 0.029 | Tree loss: 2.461 | Accuracy: 0.150391 | 0.071 sec/iter\n",
      "Epoch: 89 | Batch: 001 / 025 | Total loss: 2.428 | Reg loss: 0.029 | Tree loss: 2.428 | Accuracy: 0.195312 | 0.071 sec/iter\n",
      "Epoch: 89 | Batch: 002 / 025 | Total loss: 2.414 | Reg loss: 0.029 | Tree loss: 2.414 | Accuracy: 0.158203 | 0.071 sec/iter\n",
      "Epoch: 89 | Batch: 003 / 025 | Total loss: 2.423 | Reg loss: 0.029 | Tree loss: 2.423 | Accuracy: 0.167969 | 0.071 sec/iter\n",
      "Epoch: 89 | Batch: 004 / 025 | Total loss: 2.403 | Reg loss: 0.029 | Tree loss: 2.403 | Accuracy: 0.138672 | 0.071 sec/iter\n",
      "Epoch: 89 | Batch: 005 / 025 | Total loss: 2.418 | Reg loss: 0.029 | Tree loss: 2.418 | Accuracy: 0.152344 | 0.071 sec/iter\n",
      "Epoch: 89 | Batch: 006 / 025 | Total loss: 2.399 | Reg loss: 0.029 | Tree loss: 2.399 | Accuracy: 0.179688 | 0.071 sec/iter\n",
      "Epoch: 89 | Batch: 007 / 025 | Total loss: 2.412 | Reg loss: 0.029 | Tree loss: 2.412 | Accuracy: 0.158203 | 0.071 sec/iter\n",
      "Epoch: 89 | Batch: 008 / 025 | Total loss: 2.393 | Reg loss: 0.029 | Tree loss: 2.393 | Accuracy: 0.164062 | 0.071 sec/iter\n",
      "Epoch: 89 | Batch: 009 / 025 | Total loss: 2.359 | Reg loss: 0.029 | Tree loss: 2.359 | Accuracy: 0.162109 | 0.071 sec/iter\n",
      "Epoch: 89 | Batch: 010 / 025 | Total loss: 2.362 | Reg loss: 0.029 | Tree loss: 2.362 | Accuracy: 0.162109 | 0.071 sec/iter\n",
      "Epoch: 89 | Batch: 011 / 025 | Total loss: 2.360 | Reg loss: 0.029 | Tree loss: 2.360 | Accuracy: 0.169922 | 0.071 sec/iter\n",
      "Epoch: 89 | Batch: 012 / 025 | Total loss: 2.375 | Reg loss: 0.029 | Tree loss: 2.375 | Accuracy: 0.136719 | 0.071 sec/iter\n",
      "Epoch: 89 | Batch: 013 / 025 | Total loss: 2.382 | Reg loss: 0.029 | Tree loss: 2.382 | Accuracy: 0.164062 | 0.071 sec/iter\n",
      "Epoch: 89 | Batch: 014 / 025 | Total loss: 2.340 | Reg loss: 0.029 | Tree loss: 2.340 | Accuracy: 0.150391 | 0.071 sec/iter\n",
      "Epoch: 89 | Batch: 015 / 025 | Total loss: 2.385 | Reg loss: 0.029 | Tree loss: 2.385 | Accuracy: 0.148438 | 0.071 sec/iter\n",
      "Epoch: 89 | Batch: 016 / 025 | Total loss: 2.336 | Reg loss: 0.029 | Tree loss: 2.336 | Accuracy: 0.160156 | 0.071 sec/iter\n",
      "Epoch: 89 | Batch: 017 / 025 | Total loss: 2.295 | Reg loss: 0.029 | Tree loss: 2.295 | Accuracy: 0.160156 | 0.071 sec/iter\n",
      "Epoch: 89 | Batch: 018 / 025 | Total loss: 2.320 | Reg loss: 0.029 | Tree loss: 2.320 | Accuracy: 0.191406 | 0.071 sec/iter\n",
      "Epoch: 89 | Batch: 019 / 025 | Total loss: 2.355 | Reg loss: 0.029 | Tree loss: 2.355 | Accuracy: 0.179688 | 0.071 sec/iter\n",
      "Epoch: 89 | Batch: 020 / 025 | Total loss: 2.316 | Reg loss: 0.029 | Tree loss: 2.316 | Accuracy: 0.185547 | 0.071 sec/iter\n",
      "Epoch: 89 | Batch: 021 / 025 | Total loss: 2.312 | Reg loss: 0.029 | Tree loss: 2.312 | Accuracy: 0.173828 | 0.071 sec/iter\n",
      "Epoch: 89 | Batch: 022 / 025 | Total loss: 2.328 | Reg loss: 0.029 | Tree loss: 2.328 | Accuracy: 0.193359 | 0.071 sec/iter\n",
      "Epoch: 89 | Batch: 023 / 025 | Total loss: 2.303 | Reg loss: 0.029 | Tree loss: 2.303 | Accuracy: 0.177734 | 0.071 sec/iter\n",
      "Epoch: 89 | Batch: 024 / 025 | Total loss: 2.355 | Reg loss: 0.029 | Tree loss: 2.355 | Accuracy: 0.162500 | 0.071 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 90 | Batch: 000 / 025 | Total loss: 2.395 | Reg loss: 0.029 | Tree loss: 2.395 | Accuracy: 0.199219 | 0.071 sec/iter\n",
      "Epoch: 90 | Batch: 001 / 025 | Total loss: 2.435 | Reg loss: 0.029 | Tree loss: 2.435 | Accuracy: 0.181641 | 0.071 sec/iter\n",
      "Epoch: 90 | Batch: 002 / 025 | Total loss: 2.437 | Reg loss: 0.029 | Tree loss: 2.437 | Accuracy: 0.146484 | 0.071 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 90 | Batch: 003 / 025 | Total loss: 2.386 | Reg loss: 0.029 | Tree loss: 2.386 | Accuracy: 0.175781 | 0.071 sec/iter\n",
      "Epoch: 90 | Batch: 004 / 025 | Total loss: 2.413 | Reg loss: 0.029 | Tree loss: 2.413 | Accuracy: 0.177734 | 0.071 sec/iter\n",
      "Epoch: 90 | Batch: 005 / 025 | Total loss: 2.438 | Reg loss: 0.029 | Tree loss: 2.438 | Accuracy: 0.140625 | 0.071 sec/iter\n",
      "Epoch: 90 | Batch: 006 / 025 | Total loss: 2.374 | Reg loss: 0.029 | Tree loss: 2.374 | Accuracy: 0.167969 | 0.071 sec/iter\n",
      "Epoch: 90 | Batch: 007 / 025 | Total loss: 2.356 | Reg loss: 0.029 | Tree loss: 2.356 | Accuracy: 0.175781 | 0.071 sec/iter\n",
      "Epoch: 90 | Batch: 008 / 025 | Total loss: 2.390 | Reg loss: 0.029 | Tree loss: 2.390 | Accuracy: 0.158203 | 0.071 sec/iter\n",
      "Epoch: 90 | Batch: 009 / 025 | Total loss: 2.401 | Reg loss: 0.029 | Tree loss: 2.401 | Accuracy: 0.175781 | 0.071 sec/iter\n",
      "Epoch: 90 | Batch: 010 / 025 | Total loss: 2.379 | Reg loss: 0.029 | Tree loss: 2.379 | Accuracy: 0.173828 | 0.071 sec/iter\n",
      "Epoch: 90 | Batch: 011 / 025 | Total loss: 2.371 | Reg loss: 0.029 | Tree loss: 2.371 | Accuracy: 0.146484 | 0.071 sec/iter\n",
      "Epoch: 90 | Batch: 012 / 025 | Total loss: 2.396 | Reg loss: 0.029 | Tree loss: 2.396 | Accuracy: 0.156250 | 0.071 sec/iter\n",
      "Epoch: 90 | Batch: 013 / 025 | Total loss: 2.337 | Reg loss: 0.029 | Tree loss: 2.337 | Accuracy: 0.167969 | 0.071 sec/iter\n",
      "Epoch: 90 | Batch: 014 / 025 | Total loss: 2.397 | Reg loss: 0.029 | Tree loss: 2.397 | Accuracy: 0.148438 | 0.071 sec/iter\n",
      "Epoch: 90 | Batch: 015 / 025 | Total loss: 2.306 | Reg loss: 0.029 | Tree loss: 2.306 | Accuracy: 0.177734 | 0.071 sec/iter\n",
      "Epoch: 90 | Batch: 016 / 025 | Total loss: 2.366 | Reg loss: 0.029 | Tree loss: 2.366 | Accuracy: 0.167969 | 0.071 sec/iter\n",
      "Epoch: 90 | Batch: 017 / 025 | Total loss: 2.373 | Reg loss: 0.029 | Tree loss: 2.373 | Accuracy: 0.166016 | 0.071 sec/iter\n",
      "Epoch: 90 | Batch: 018 / 025 | Total loss: 2.341 | Reg loss: 0.029 | Tree loss: 2.341 | Accuracy: 0.146484 | 0.071 sec/iter\n",
      "Epoch: 90 | Batch: 019 / 025 | Total loss: 2.308 | Reg loss: 0.029 | Tree loss: 2.308 | Accuracy: 0.175781 | 0.071 sec/iter\n",
      "Epoch: 90 | Batch: 020 / 025 | Total loss: 2.310 | Reg loss: 0.029 | Tree loss: 2.310 | Accuracy: 0.167969 | 0.071 sec/iter\n",
      "Epoch: 90 | Batch: 021 / 025 | Total loss: 2.307 | Reg loss: 0.029 | Tree loss: 2.307 | Accuracy: 0.201172 | 0.071 sec/iter\n",
      "Epoch: 90 | Batch: 022 / 025 | Total loss: 2.313 | Reg loss: 0.029 | Tree loss: 2.313 | Accuracy: 0.171875 | 0.071 sec/iter\n",
      "Epoch: 90 | Batch: 023 / 025 | Total loss: 2.315 | Reg loss: 0.029 | Tree loss: 2.315 | Accuracy: 0.167969 | 0.071 sec/iter\n",
      "Epoch: 90 | Batch: 024 / 025 | Total loss: 2.320 | Reg loss: 0.029 | Tree loss: 2.320 | Accuracy: 0.171875 | 0.071 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 91 | Batch: 000 / 025 | Total loss: 2.421 | Reg loss: 0.029 | Tree loss: 2.421 | Accuracy: 0.177734 | 0.071 sec/iter\n",
      "Epoch: 91 | Batch: 001 / 025 | Total loss: 2.466 | Reg loss: 0.029 | Tree loss: 2.466 | Accuracy: 0.177734 | 0.071 sec/iter\n",
      "Epoch: 91 | Batch: 002 / 025 | Total loss: 2.425 | Reg loss: 0.029 | Tree loss: 2.425 | Accuracy: 0.158203 | 0.071 sec/iter\n",
      "Epoch: 91 | Batch: 003 / 025 | Total loss: 2.426 | Reg loss: 0.029 | Tree loss: 2.426 | Accuracy: 0.160156 | 0.071 sec/iter\n",
      "Epoch: 91 | Batch: 004 / 025 | Total loss: 2.397 | Reg loss: 0.029 | Tree loss: 2.397 | Accuracy: 0.169922 | 0.071 sec/iter\n",
      "Epoch: 91 | Batch: 005 / 025 | Total loss: 2.368 | Reg loss: 0.029 | Tree loss: 2.368 | Accuracy: 0.164062 | 0.071 sec/iter\n",
      "Epoch: 91 | Batch: 006 / 025 | Total loss: 2.401 | Reg loss: 0.029 | Tree loss: 2.401 | Accuracy: 0.175781 | 0.071 sec/iter\n",
      "Epoch: 91 | Batch: 007 / 025 | Total loss: 2.401 | Reg loss: 0.029 | Tree loss: 2.401 | Accuracy: 0.173828 | 0.071 sec/iter\n",
      "Epoch: 91 | Batch: 008 / 025 | Total loss: 2.375 | Reg loss: 0.029 | Tree loss: 2.375 | Accuracy: 0.150391 | 0.071 sec/iter\n",
      "Epoch: 91 | Batch: 009 / 025 | Total loss: 2.394 | Reg loss: 0.029 | Tree loss: 2.394 | Accuracy: 0.160156 | 0.071 sec/iter\n",
      "Epoch: 91 | Batch: 010 / 025 | Total loss: 2.379 | Reg loss: 0.029 | Tree loss: 2.379 | Accuracy: 0.185547 | 0.071 sec/iter\n",
      "Epoch: 91 | Batch: 011 / 025 | Total loss: 2.349 | Reg loss: 0.029 | Tree loss: 2.349 | Accuracy: 0.179688 | 0.071 sec/iter\n",
      "Epoch: 91 | Batch: 012 / 025 | Total loss: 2.350 | Reg loss: 0.029 | Tree loss: 2.350 | Accuracy: 0.171875 | 0.071 sec/iter\n",
      "Epoch: 91 | Batch: 013 / 025 | Total loss: 2.337 | Reg loss: 0.029 | Tree loss: 2.337 | Accuracy: 0.152344 | 0.071 sec/iter\n",
      "Epoch: 91 | Batch: 014 / 025 | Total loss: 2.322 | Reg loss: 0.029 | Tree loss: 2.322 | Accuracy: 0.164062 | 0.071 sec/iter\n",
      "Epoch: 91 | Batch: 015 / 025 | Total loss: 2.302 | Reg loss: 0.029 | Tree loss: 2.302 | Accuracy: 0.171875 | 0.071 sec/iter\n",
      "Epoch: 91 | Batch: 016 / 025 | Total loss: 2.347 | Reg loss: 0.029 | Tree loss: 2.347 | Accuracy: 0.171875 | 0.071 sec/iter\n",
      "Epoch: 91 | Batch: 017 / 025 | Total loss: 2.361 | Reg loss: 0.029 | Tree loss: 2.361 | Accuracy: 0.136719 | 0.071 sec/iter\n",
      "Epoch: 91 | Batch: 018 / 025 | Total loss: 2.349 | Reg loss: 0.029 | Tree loss: 2.349 | Accuracy: 0.167969 | 0.071 sec/iter\n",
      "Epoch: 91 | Batch: 019 / 025 | Total loss: 2.376 | Reg loss: 0.029 | Tree loss: 2.376 | Accuracy: 0.166016 | 0.071 sec/iter\n",
      "Epoch: 91 | Batch: 020 / 025 | Total loss: 2.316 | Reg loss: 0.029 | Tree loss: 2.316 | Accuracy: 0.156250 | 0.071 sec/iter\n",
      "Epoch: 91 | Batch: 021 / 025 | Total loss: 2.303 | Reg loss: 0.029 | Tree loss: 2.303 | Accuracy: 0.171875 | 0.071 sec/iter\n",
      "Epoch: 91 | Batch: 022 / 025 | Total loss: 2.326 | Reg loss: 0.029 | Tree loss: 2.326 | Accuracy: 0.156250 | 0.071 sec/iter\n",
      "Epoch: 91 | Batch: 023 / 025 | Total loss: 2.312 | Reg loss: 0.029 | Tree loss: 2.312 | Accuracy: 0.189453 | 0.071 sec/iter\n",
      "Epoch: 91 | Batch: 024 / 025 | Total loss: 2.286 | Reg loss: 0.029 | Tree loss: 2.286 | Accuracy: 0.131250 | 0.071 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 92 | Batch: 000 / 025 | Total loss: 2.424 | Reg loss: 0.029 | Tree loss: 2.424 | Accuracy: 0.167969 | 0.071 sec/iter\n",
      "Epoch: 92 | Batch: 001 / 025 | Total loss: 2.468 | Reg loss: 0.029 | Tree loss: 2.468 | Accuracy: 0.160156 | 0.071 sec/iter\n",
      "Epoch: 92 | Batch: 002 / 025 | Total loss: 2.430 | Reg loss: 0.029 | Tree loss: 2.430 | Accuracy: 0.171875 | 0.071 sec/iter\n",
      "Epoch: 92 | Batch: 003 / 025 | Total loss: 2.394 | Reg loss: 0.029 | Tree loss: 2.394 | Accuracy: 0.171875 | 0.071 sec/iter\n",
      "Epoch: 92 | Batch: 004 / 025 | Total loss: 2.407 | Reg loss: 0.029 | Tree loss: 2.407 | Accuracy: 0.177734 | 0.071 sec/iter\n",
      "Epoch: 92 | Batch: 005 / 025 | Total loss: 2.424 | Reg loss: 0.029 | Tree loss: 2.424 | Accuracy: 0.171875 | 0.071 sec/iter\n",
      "Epoch: 92 | Batch: 006 / 025 | Total loss: 2.362 | Reg loss: 0.029 | Tree loss: 2.362 | Accuracy: 0.164062 | 0.071 sec/iter\n",
      "Epoch: 92 | Batch: 007 / 025 | Total loss: 2.440 | Reg loss: 0.029 | Tree loss: 2.440 | Accuracy: 0.160156 | 0.071 sec/iter\n",
      "Epoch: 92 | Batch: 008 / 025 | Total loss: 2.371 | Reg loss: 0.029 | Tree loss: 2.371 | Accuracy: 0.156250 | 0.071 sec/iter\n",
      "Epoch: 92 | Batch: 009 / 025 | Total loss: 2.361 | Reg loss: 0.029 | Tree loss: 2.361 | Accuracy: 0.181641 | 0.071 sec/iter\n",
      "Epoch: 92 | Batch: 010 / 025 | Total loss: 2.336 | Reg loss: 0.029 | Tree loss: 2.336 | Accuracy: 0.144531 | 0.071 sec/iter\n",
      "Epoch: 92 | Batch: 011 / 025 | Total loss: 2.357 | Reg loss: 0.029 | Tree loss: 2.357 | Accuracy: 0.191406 | 0.071 sec/iter\n",
      "Epoch: 92 | Batch: 012 / 025 | Total loss: 2.319 | Reg loss: 0.029 | Tree loss: 2.319 | Accuracy: 0.171875 | 0.071 sec/iter\n",
      "Epoch: 92 | Batch: 013 / 025 | Total loss: 2.333 | Reg loss: 0.029 | Tree loss: 2.333 | Accuracy: 0.181641 | 0.071 sec/iter\n",
      "Epoch: 92 | Batch: 014 / 025 | Total loss: 2.354 | Reg loss: 0.029 | Tree loss: 2.354 | Accuracy: 0.126953 | 0.071 sec/iter\n",
      "Epoch: 92 | Batch: 015 / 025 | Total loss: 2.346 | Reg loss: 0.029 | Tree loss: 2.346 | Accuracy: 0.187500 | 0.071 sec/iter\n",
      "Epoch: 92 | Batch: 016 / 025 | Total loss: 2.322 | Reg loss: 0.029 | Tree loss: 2.322 | Accuracy: 0.193359 | 0.071 sec/iter\n",
      "Epoch: 92 | Batch: 017 / 025 | Total loss: 2.332 | Reg loss: 0.029 | Tree loss: 2.332 | Accuracy: 0.177734 | 0.071 sec/iter\n",
      "Epoch: 92 | Batch: 018 / 025 | Total loss: 2.355 | Reg loss: 0.029 | Tree loss: 2.355 | Accuracy: 0.152344 | 0.071 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 92 | Batch: 019 / 025 | Total loss: 2.336 | Reg loss: 0.029 | Tree loss: 2.336 | Accuracy: 0.146484 | 0.071 sec/iter\n",
      "Epoch: 92 | Batch: 020 / 025 | Total loss: 2.363 | Reg loss: 0.029 | Tree loss: 2.363 | Accuracy: 0.152344 | 0.071 sec/iter\n",
      "Epoch: 92 | Batch: 021 / 025 | Total loss: 2.297 | Reg loss: 0.029 | Tree loss: 2.297 | Accuracy: 0.148438 | 0.071 sec/iter\n",
      "Epoch: 92 | Batch: 022 / 025 | Total loss: 2.311 | Reg loss: 0.029 | Tree loss: 2.311 | Accuracy: 0.183594 | 0.071 sec/iter\n",
      "Epoch: 92 | Batch: 023 / 025 | Total loss: 2.303 | Reg loss: 0.029 | Tree loss: 2.303 | Accuracy: 0.150391 | 0.071 sec/iter\n",
      "Epoch: 92 | Batch: 024 / 025 | Total loss: 2.285 | Reg loss: 0.029 | Tree loss: 2.285 | Accuracy: 0.193750 | 0.071 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 93 | Batch: 000 / 025 | Total loss: 2.425 | Reg loss: 0.029 | Tree loss: 2.425 | Accuracy: 0.160156 | 0.071 sec/iter\n",
      "Epoch: 93 | Batch: 001 / 025 | Total loss: 2.398 | Reg loss: 0.029 | Tree loss: 2.398 | Accuracy: 0.173828 | 0.071 sec/iter\n",
      "Epoch: 93 | Batch: 002 / 025 | Total loss: 2.424 | Reg loss: 0.029 | Tree loss: 2.424 | Accuracy: 0.171875 | 0.071 sec/iter\n",
      "Epoch: 93 | Batch: 003 / 025 | Total loss: 2.419 | Reg loss: 0.029 | Tree loss: 2.419 | Accuracy: 0.185547 | 0.071 sec/iter\n",
      "Epoch: 93 | Batch: 004 / 025 | Total loss: 2.390 | Reg loss: 0.029 | Tree loss: 2.390 | Accuracy: 0.166016 | 0.071 sec/iter\n",
      "Epoch: 93 | Batch: 005 / 025 | Total loss: 2.385 | Reg loss: 0.029 | Tree loss: 2.385 | Accuracy: 0.152344 | 0.071 sec/iter\n",
      "Epoch: 93 | Batch: 006 / 025 | Total loss: 2.378 | Reg loss: 0.029 | Tree loss: 2.378 | Accuracy: 0.146484 | 0.072 sec/iter\n",
      "Epoch: 93 | Batch: 007 / 025 | Total loss: 2.355 | Reg loss: 0.029 | Tree loss: 2.355 | Accuracy: 0.175781 | 0.072 sec/iter\n",
      "Epoch: 93 | Batch: 008 / 025 | Total loss: 2.411 | Reg loss: 0.029 | Tree loss: 2.411 | Accuracy: 0.169922 | 0.072 sec/iter\n",
      "Epoch: 93 | Batch: 009 / 025 | Total loss: 2.348 | Reg loss: 0.029 | Tree loss: 2.348 | Accuracy: 0.173828 | 0.072 sec/iter\n",
      "Epoch: 93 | Batch: 010 / 025 | Total loss: 2.407 | Reg loss: 0.029 | Tree loss: 2.407 | Accuracy: 0.175781 | 0.072 sec/iter\n",
      "Epoch: 93 | Batch: 011 / 025 | Total loss: 2.340 | Reg loss: 0.029 | Tree loss: 2.340 | Accuracy: 0.171875 | 0.072 sec/iter\n",
      "Epoch: 93 | Batch: 012 / 025 | Total loss: 2.359 | Reg loss: 0.029 | Tree loss: 2.359 | Accuracy: 0.158203 | 0.072 sec/iter\n",
      "Epoch: 93 | Batch: 013 / 025 | Total loss: 2.399 | Reg loss: 0.029 | Tree loss: 2.399 | Accuracy: 0.154297 | 0.072 sec/iter\n",
      "Epoch: 93 | Batch: 014 / 025 | Total loss: 2.326 | Reg loss: 0.029 | Tree loss: 2.326 | Accuracy: 0.169922 | 0.072 sec/iter\n",
      "Epoch: 93 | Batch: 015 / 025 | Total loss: 2.333 | Reg loss: 0.029 | Tree loss: 2.333 | Accuracy: 0.158203 | 0.072 sec/iter\n",
      "Epoch: 93 | Batch: 016 / 025 | Total loss: 2.364 | Reg loss: 0.029 | Tree loss: 2.364 | Accuracy: 0.162109 | 0.072 sec/iter\n",
      "Epoch: 93 | Batch: 017 / 025 | Total loss: 2.344 | Reg loss: 0.029 | Tree loss: 2.344 | Accuracy: 0.146484 | 0.072 sec/iter\n",
      "Epoch: 93 | Batch: 018 / 025 | Total loss: 2.342 | Reg loss: 0.029 | Tree loss: 2.342 | Accuracy: 0.156250 | 0.072 sec/iter\n",
      "Epoch: 93 | Batch: 019 / 025 | Total loss: 2.319 | Reg loss: 0.029 | Tree loss: 2.319 | Accuracy: 0.175781 | 0.072 sec/iter\n",
      "Epoch: 93 | Batch: 020 / 025 | Total loss: 2.327 | Reg loss: 0.029 | Tree loss: 2.327 | Accuracy: 0.183594 | 0.072 sec/iter\n",
      "Epoch: 93 | Batch: 021 / 025 | Total loss: 2.311 | Reg loss: 0.029 | Tree loss: 2.311 | Accuracy: 0.173828 | 0.072 sec/iter\n",
      "Epoch: 93 | Batch: 022 / 025 | Total loss: 2.280 | Reg loss: 0.029 | Tree loss: 2.280 | Accuracy: 0.195312 | 0.072 sec/iter\n",
      "Epoch: 93 | Batch: 023 / 025 | Total loss: 2.326 | Reg loss: 0.029 | Tree loss: 2.326 | Accuracy: 0.148438 | 0.072 sec/iter\n",
      "Epoch: 93 | Batch: 024 / 025 | Total loss: 2.263 | Reg loss: 0.029 | Tree loss: 2.263 | Accuracy: 0.181250 | 0.072 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 94 | Batch: 000 / 025 | Total loss: 2.407 | Reg loss: 0.029 | Tree loss: 2.407 | Accuracy: 0.162109 | 0.072 sec/iter\n",
      "Epoch: 94 | Batch: 001 / 025 | Total loss: 2.408 | Reg loss: 0.029 | Tree loss: 2.408 | Accuracy: 0.169922 | 0.072 sec/iter\n",
      "Epoch: 94 | Batch: 002 / 025 | Total loss: 2.419 | Reg loss: 0.029 | Tree loss: 2.419 | Accuracy: 0.150391 | 0.072 sec/iter\n",
      "Epoch: 94 | Batch: 003 / 025 | Total loss: 2.393 | Reg loss: 0.029 | Tree loss: 2.393 | Accuracy: 0.166016 | 0.072 sec/iter\n",
      "Epoch: 94 | Batch: 004 / 025 | Total loss: 2.394 | Reg loss: 0.029 | Tree loss: 2.394 | Accuracy: 0.167969 | 0.072 sec/iter\n",
      "Epoch: 94 | Batch: 005 / 025 | Total loss: 2.418 | Reg loss: 0.029 | Tree loss: 2.418 | Accuracy: 0.160156 | 0.072 sec/iter\n",
      "Epoch: 94 | Batch: 006 / 025 | Total loss: 2.424 | Reg loss: 0.029 | Tree loss: 2.424 | Accuracy: 0.187500 | 0.072 sec/iter\n",
      "Epoch: 94 | Batch: 007 / 025 | Total loss: 2.377 | Reg loss: 0.029 | Tree loss: 2.377 | Accuracy: 0.166016 | 0.072 sec/iter\n",
      "Epoch: 94 | Batch: 008 / 025 | Total loss: 2.358 | Reg loss: 0.029 | Tree loss: 2.358 | Accuracy: 0.166016 | 0.072 sec/iter\n",
      "Epoch: 94 | Batch: 009 / 025 | Total loss: 2.391 | Reg loss: 0.029 | Tree loss: 2.391 | Accuracy: 0.185547 | 0.072 sec/iter\n",
      "Epoch: 94 | Batch: 010 / 025 | Total loss: 2.364 | Reg loss: 0.029 | Tree loss: 2.364 | Accuracy: 0.152344 | 0.072 sec/iter\n",
      "Epoch: 94 | Batch: 011 / 025 | Total loss: 2.395 | Reg loss: 0.029 | Tree loss: 2.395 | Accuracy: 0.148438 | 0.072 sec/iter\n",
      "Epoch: 94 | Batch: 012 / 025 | Total loss: 2.354 | Reg loss: 0.029 | Tree loss: 2.354 | Accuracy: 0.167969 | 0.072 sec/iter\n",
      "Epoch: 94 | Batch: 013 / 025 | Total loss: 2.362 | Reg loss: 0.029 | Tree loss: 2.362 | Accuracy: 0.177734 | 0.072 sec/iter\n",
      "Epoch: 94 | Batch: 014 / 025 | Total loss: 2.310 | Reg loss: 0.029 | Tree loss: 2.310 | Accuracy: 0.173828 | 0.072 sec/iter\n",
      "Epoch: 94 | Batch: 015 / 025 | Total loss: 2.344 | Reg loss: 0.029 | Tree loss: 2.344 | Accuracy: 0.181641 | 0.072 sec/iter\n",
      "Epoch: 94 | Batch: 016 / 025 | Total loss: 2.334 | Reg loss: 0.029 | Tree loss: 2.334 | Accuracy: 0.181641 | 0.072 sec/iter\n",
      "Epoch: 94 | Batch: 017 / 025 | Total loss: 2.288 | Reg loss: 0.029 | Tree loss: 2.288 | Accuracy: 0.144531 | 0.072 sec/iter\n",
      "Epoch: 94 | Batch: 018 / 025 | Total loss: 2.317 | Reg loss: 0.029 | Tree loss: 2.317 | Accuracy: 0.199219 | 0.072 sec/iter\n",
      "Epoch: 94 | Batch: 019 / 025 | Total loss: 2.299 | Reg loss: 0.029 | Tree loss: 2.299 | Accuracy: 0.181641 | 0.072 sec/iter\n",
      "Epoch: 94 | Batch: 020 / 025 | Total loss: 2.297 | Reg loss: 0.029 | Tree loss: 2.297 | Accuracy: 0.160156 | 0.072 sec/iter\n",
      "Epoch: 94 | Batch: 021 / 025 | Total loss: 2.325 | Reg loss: 0.029 | Tree loss: 2.325 | Accuracy: 0.144531 | 0.072 sec/iter\n",
      "Epoch: 94 | Batch: 022 / 025 | Total loss: 2.304 | Reg loss: 0.029 | Tree loss: 2.304 | Accuracy: 0.177734 | 0.072 sec/iter\n",
      "Epoch: 94 | Batch: 023 / 025 | Total loss: 2.323 | Reg loss: 0.029 | Tree loss: 2.323 | Accuracy: 0.171875 | 0.072 sec/iter\n",
      "Epoch: 94 | Batch: 024 / 025 | Total loss: 2.340 | Reg loss: 0.029 | Tree loss: 2.340 | Accuracy: 0.118750 | 0.072 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 95 | Batch: 000 / 025 | Total loss: 2.419 | Reg loss: 0.029 | Tree loss: 2.419 | Accuracy: 0.150391 | 0.072 sec/iter\n",
      "Epoch: 95 | Batch: 001 / 025 | Total loss: 2.442 | Reg loss: 0.029 | Tree loss: 2.442 | Accuracy: 0.181641 | 0.072 sec/iter\n",
      "Epoch: 95 | Batch: 002 / 025 | Total loss: 2.401 | Reg loss: 0.029 | Tree loss: 2.401 | Accuracy: 0.144531 | 0.072 sec/iter\n",
      "Epoch: 95 | Batch: 003 / 025 | Total loss: 2.416 | Reg loss: 0.029 | Tree loss: 2.416 | Accuracy: 0.160156 | 0.072 sec/iter\n",
      "Epoch: 95 | Batch: 004 / 025 | Total loss: 2.442 | Reg loss: 0.029 | Tree loss: 2.442 | Accuracy: 0.164062 | 0.072 sec/iter\n",
      "Epoch: 95 | Batch: 005 / 025 | Total loss: 2.381 | Reg loss: 0.029 | Tree loss: 2.381 | Accuracy: 0.148438 | 0.072 sec/iter\n",
      "Epoch: 95 | Batch: 006 / 025 | Total loss: 2.359 | Reg loss: 0.029 | Tree loss: 2.359 | Accuracy: 0.167969 | 0.072 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 95 | Batch: 007 / 025 | Total loss: 2.368 | Reg loss: 0.029 | Tree loss: 2.368 | Accuracy: 0.189453 | 0.072 sec/iter\n",
      "Epoch: 95 | Batch: 008 / 025 | Total loss: 2.404 | Reg loss: 0.029 | Tree loss: 2.404 | Accuracy: 0.158203 | 0.072 sec/iter\n",
      "Epoch: 95 | Batch: 009 / 025 | Total loss: 2.338 | Reg loss: 0.029 | Tree loss: 2.338 | Accuracy: 0.193359 | 0.072 sec/iter\n",
      "Epoch: 95 | Batch: 010 / 025 | Total loss: 2.348 | Reg loss: 0.029 | Tree loss: 2.348 | Accuracy: 0.199219 | 0.072 sec/iter\n",
      "Epoch: 95 | Batch: 011 / 025 | Total loss: 2.421 | Reg loss: 0.029 | Tree loss: 2.421 | Accuracy: 0.160156 | 0.072 sec/iter\n",
      "Epoch: 95 | Batch: 012 / 025 | Total loss: 2.354 | Reg loss: 0.029 | Tree loss: 2.354 | Accuracy: 0.158203 | 0.072 sec/iter\n",
      "Epoch: 95 | Batch: 013 / 025 | Total loss: 2.334 | Reg loss: 0.029 | Tree loss: 2.334 | Accuracy: 0.160156 | 0.072 sec/iter\n",
      "Epoch: 95 | Batch: 014 / 025 | Total loss: 2.305 | Reg loss: 0.029 | Tree loss: 2.305 | Accuracy: 0.185547 | 0.072 sec/iter\n",
      "Epoch: 95 | Batch: 015 / 025 | Total loss: 2.365 | Reg loss: 0.029 | Tree loss: 2.365 | Accuracy: 0.185547 | 0.072 sec/iter\n",
      "Epoch: 95 | Batch: 016 / 025 | Total loss: 2.306 | Reg loss: 0.029 | Tree loss: 2.306 | Accuracy: 0.175781 | 0.072 sec/iter\n",
      "Epoch: 95 | Batch: 017 / 025 | Total loss: 2.304 | Reg loss: 0.029 | Tree loss: 2.304 | Accuracy: 0.185547 | 0.072 sec/iter\n",
      "Epoch: 95 | Batch: 018 / 025 | Total loss: 2.314 | Reg loss: 0.029 | Tree loss: 2.314 | Accuracy: 0.144531 | 0.072 sec/iter\n",
      "Epoch: 95 | Batch: 019 / 025 | Total loss: 2.338 | Reg loss: 0.029 | Tree loss: 2.338 | Accuracy: 0.179688 | 0.072 sec/iter\n",
      "Epoch: 95 | Batch: 020 / 025 | Total loss: 2.330 | Reg loss: 0.029 | Tree loss: 2.330 | Accuracy: 0.152344 | 0.072 sec/iter\n",
      "Epoch: 95 | Batch: 021 / 025 | Total loss: 2.288 | Reg loss: 0.029 | Tree loss: 2.288 | Accuracy: 0.179688 | 0.072 sec/iter\n",
      "Epoch: 95 | Batch: 022 / 025 | Total loss: 2.327 | Reg loss: 0.029 | Tree loss: 2.327 | Accuracy: 0.166016 | 0.072 sec/iter\n",
      "Epoch: 95 | Batch: 023 / 025 | Total loss: 2.302 | Reg loss: 0.029 | Tree loss: 2.302 | Accuracy: 0.132812 | 0.072 sec/iter\n",
      "Epoch: 95 | Batch: 024 / 025 | Total loss: 2.267 | Reg loss: 0.029 | Tree loss: 2.267 | Accuracy: 0.168750 | 0.072 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 96 | Batch: 000 / 025 | Total loss: 2.416 | Reg loss: 0.029 | Tree loss: 2.416 | Accuracy: 0.136719 | 0.072 sec/iter\n",
      "Epoch: 96 | Batch: 001 / 025 | Total loss: 2.415 | Reg loss: 0.029 | Tree loss: 2.415 | Accuracy: 0.160156 | 0.072 sec/iter\n",
      "Epoch: 96 | Batch: 002 / 025 | Total loss: 2.429 | Reg loss: 0.029 | Tree loss: 2.429 | Accuracy: 0.144531 | 0.072 sec/iter\n",
      "Epoch: 96 | Batch: 003 / 025 | Total loss: 2.401 | Reg loss: 0.029 | Tree loss: 2.401 | Accuracy: 0.173828 | 0.072 sec/iter\n",
      "Epoch: 96 | Batch: 004 / 025 | Total loss: 2.395 | Reg loss: 0.029 | Tree loss: 2.395 | Accuracy: 0.191406 | 0.072 sec/iter\n",
      "Epoch: 96 | Batch: 005 / 025 | Total loss: 2.409 | Reg loss: 0.029 | Tree loss: 2.409 | Accuracy: 0.185547 | 0.072 sec/iter\n",
      "Epoch: 96 | Batch: 006 / 025 | Total loss: 2.375 | Reg loss: 0.029 | Tree loss: 2.375 | Accuracy: 0.164062 | 0.072 sec/iter\n",
      "Epoch: 96 | Batch: 007 / 025 | Total loss: 2.410 | Reg loss: 0.029 | Tree loss: 2.410 | Accuracy: 0.166016 | 0.072 sec/iter\n",
      "Epoch: 96 | Batch: 008 / 025 | Total loss: 2.391 | Reg loss: 0.029 | Tree loss: 2.391 | Accuracy: 0.156250 | 0.072 sec/iter\n",
      "Epoch: 96 | Batch: 009 / 025 | Total loss: 2.349 | Reg loss: 0.029 | Tree loss: 2.349 | Accuracy: 0.146484 | 0.072 sec/iter\n",
      "Epoch: 96 | Batch: 010 / 025 | Total loss: 2.356 | Reg loss: 0.029 | Tree loss: 2.356 | Accuracy: 0.167969 | 0.072 sec/iter\n",
      "Epoch: 96 | Batch: 011 / 025 | Total loss: 2.391 | Reg loss: 0.029 | Tree loss: 2.391 | Accuracy: 0.171875 | 0.072 sec/iter\n",
      "Epoch: 96 | Batch: 012 / 025 | Total loss: 2.331 | Reg loss: 0.029 | Tree loss: 2.331 | Accuracy: 0.171875 | 0.072 sec/iter\n",
      "Epoch: 96 | Batch: 013 / 025 | Total loss: 2.314 | Reg loss: 0.029 | Tree loss: 2.314 | Accuracy: 0.171875 | 0.072 sec/iter\n",
      "Epoch: 96 | Batch: 014 / 025 | Total loss: 2.319 | Reg loss: 0.029 | Tree loss: 2.319 | Accuracy: 0.140625 | 0.072 sec/iter\n",
      "Epoch: 96 | Batch: 015 / 025 | Total loss: 2.371 | Reg loss: 0.029 | Tree loss: 2.371 | Accuracy: 0.183594 | 0.072 sec/iter\n",
      "Epoch: 96 | Batch: 016 / 025 | Total loss: 2.342 | Reg loss: 0.029 | Tree loss: 2.342 | Accuracy: 0.150391 | 0.072 sec/iter\n",
      "Epoch: 96 | Batch: 017 / 025 | Total loss: 2.312 | Reg loss: 0.029 | Tree loss: 2.312 | Accuracy: 0.195312 | 0.072 sec/iter\n",
      "Epoch: 96 | Batch: 018 / 025 | Total loss: 2.310 | Reg loss: 0.029 | Tree loss: 2.310 | Accuracy: 0.177734 | 0.072 sec/iter\n",
      "Epoch: 96 | Batch: 019 / 025 | Total loss: 2.306 | Reg loss: 0.029 | Tree loss: 2.306 | Accuracy: 0.181641 | 0.072 sec/iter\n",
      "Epoch: 96 | Batch: 020 / 025 | Total loss: 2.325 | Reg loss: 0.029 | Tree loss: 2.325 | Accuracy: 0.193359 | 0.072 sec/iter\n",
      "Epoch: 96 | Batch: 021 / 025 | Total loss: 2.305 | Reg loss: 0.029 | Tree loss: 2.305 | Accuracy: 0.158203 | 0.072 sec/iter\n",
      "Epoch: 96 | Batch: 022 / 025 | Total loss: 2.333 | Reg loss: 0.029 | Tree loss: 2.333 | Accuracy: 0.152344 | 0.072 sec/iter\n",
      "Epoch: 96 | Batch: 023 / 025 | Total loss: 2.274 | Reg loss: 0.029 | Tree loss: 2.274 | Accuracy: 0.179688 | 0.072 sec/iter\n",
      "Epoch: 96 | Batch: 024 / 025 | Total loss: 2.236 | Reg loss: 0.029 | Tree loss: 2.236 | Accuracy: 0.181250 | 0.072 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 97 | Batch: 000 / 025 | Total loss: 2.443 | Reg loss: 0.028 | Tree loss: 2.443 | Accuracy: 0.156250 | 0.072 sec/iter\n",
      "Epoch: 97 | Batch: 001 / 025 | Total loss: 2.424 | Reg loss: 0.028 | Tree loss: 2.424 | Accuracy: 0.175781 | 0.072 sec/iter\n",
      "Epoch: 97 | Batch: 002 / 025 | Total loss: 2.413 | Reg loss: 0.028 | Tree loss: 2.413 | Accuracy: 0.162109 | 0.072 sec/iter\n",
      "Epoch: 97 | Batch: 003 / 025 | Total loss: 2.404 | Reg loss: 0.028 | Tree loss: 2.404 | Accuracy: 0.179688 | 0.072 sec/iter\n",
      "Epoch: 97 | Batch: 004 / 025 | Total loss: 2.403 | Reg loss: 0.028 | Tree loss: 2.403 | Accuracy: 0.181641 | 0.072 sec/iter\n",
      "Epoch: 97 | Batch: 005 / 025 | Total loss: 2.406 | Reg loss: 0.028 | Tree loss: 2.406 | Accuracy: 0.173828 | 0.072 sec/iter\n",
      "Epoch: 97 | Batch: 006 / 025 | Total loss: 2.361 | Reg loss: 0.028 | Tree loss: 2.361 | Accuracy: 0.183594 | 0.072 sec/iter\n",
      "Epoch: 97 | Batch: 007 / 025 | Total loss: 2.390 | Reg loss: 0.028 | Tree loss: 2.390 | Accuracy: 0.156250 | 0.072 sec/iter\n",
      "Epoch: 97 | Batch: 008 / 025 | Total loss: 2.382 | Reg loss: 0.028 | Tree loss: 2.382 | Accuracy: 0.171875 | 0.072 sec/iter\n",
      "Epoch: 97 | Batch: 009 / 025 | Total loss: 2.399 | Reg loss: 0.029 | Tree loss: 2.399 | Accuracy: 0.148438 | 0.072 sec/iter\n",
      "Epoch: 97 | Batch: 010 / 025 | Total loss: 2.355 | Reg loss: 0.029 | Tree loss: 2.355 | Accuracy: 0.179688 | 0.072 sec/iter\n",
      "Epoch: 97 | Batch: 011 / 025 | Total loss: 2.376 | Reg loss: 0.029 | Tree loss: 2.376 | Accuracy: 0.166016 | 0.072 sec/iter\n",
      "Epoch: 97 | Batch: 012 / 025 | Total loss: 2.315 | Reg loss: 0.029 | Tree loss: 2.315 | Accuracy: 0.181641 | 0.072 sec/iter\n",
      "Epoch: 97 | Batch: 013 / 025 | Total loss: 2.376 | Reg loss: 0.029 | Tree loss: 2.376 | Accuracy: 0.160156 | 0.072 sec/iter\n",
      "Epoch: 97 | Batch: 014 / 025 | Total loss: 2.339 | Reg loss: 0.029 | Tree loss: 2.339 | Accuracy: 0.166016 | 0.072 sec/iter\n",
      "Epoch: 97 | Batch: 015 / 025 | Total loss: 2.380 | Reg loss: 0.029 | Tree loss: 2.380 | Accuracy: 0.179688 | 0.072 sec/iter\n",
      "Epoch: 97 | Batch: 016 / 025 | Total loss: 2.339 | Reg loss: 0.029 | Tree loss: 2.339 | Accuracy: 0.185547 | 0.072 sec/iter\n",
      "Epoch: 97 | Batch: 017 / 025 | Total loss: 2.323 | Reg loss: 0.029 | Tree loss: 2.323 | Accuracy: 0.177734 | 0.072 sec/iter\n",
      "Epoch: 97 | Batch: 018 / 025 | Total loss: 2.284 | Reg loss: 0.029 | Tree loss: 2.284 | Accuracy: 0.179688 | 0.072 sec/iter\n",
      "Epoch: 97 | Batch: 019 / 025 | Total loss: 2.290 | Reg loss: 0.029 | Tree loss: 2.290 | Accuracy: 0.154297 | 0.072 sec/iter\n",
      "Epoch: 97 | Batch: 020 / 025 | Total loss: 2.278 | Reg loss: 0.029 | Tree loss: 2.278 | Accuracy: 0.162109 | 0.072 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 97 | Batch: 021 / 025 | Total loss: 2.277 | Reg loss: 0.029 | Tree loss: 2.277 | Accuracy: 0.177734 | 0.072 sec/iter\n",
      "Epoch: 97 | Batch: 022 / 025 | Total loss: 2.266 | Reg loss: 0.029 | Tree loss: 2.266 | Accuracy: 0.189453 | 0.072 sec/iter\n",
      "Epoch: 97 | Batch: 023 / 025 | Total loss: 2.287 | Reg loss: 0.029 | Tree loss: 2.287 | Accuracy: 0.152344 | 0.072 sec/iter\n",
      "Epoch: 97 | Batch: 024 / 025 | Total loss: 2.272 | Reg loss: 0.029 | Tree loss: 2.272 | Accuracy: 0.181250 | 0.072 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 98 | Batch: 000 / 025 | Total loss: 2.456 | Reg loss: 0.028 | Tree loss: 2.456 | Accuracy: 0.164062 | 0.072 sec/iter\n",
      "Epoch: 98 | Batch: 001 / 025 | Total loss: 2.457 | Reg loss: 0.028 | Tree loss: 2.457 | Accuracy: 0.160156 | 0.072 sec/iter\n",
      "Epoch: 98 | Batch: 002 / 025 | Total loss: 2.412 | Reg loss: 0.028 | Tree loss: 2.412 | Accuracy: 0.197266 | 0.072 sec/iter\n",
      "Epoch: 98 | Batch: 003 / 025 | Total loss: 2.426 | Reg loss: 0.028 | Tree loss: 2.426 | Accuracy: 0.169922 | 0.072 sec/iter\n",
      "Epoch: 98 | Batch: 004 / 025 | Total loss: 2.412 | Reg loss: 0.028 | Tree loss: 2.412 | Accuracy: 0.164062 | 0.072 sec/iter\n",
      "Epoch: 98 | Batch: 005 / 025 | Total loss: 2.372 | Reg loss: 0.028 | Tree loss: 2.372 | Accuracy: 0.179688 | 0.072 sec/iter\n",
      "Epoch: 98 | Batch: 006 / 025 | Total loss: 2.398 | Reg loss: 0.028 | Tree loss: 2.398 | Accuracy: 0.173828 | 0.072 sec/iter\n",
      "Epoch: 98 | Batch: 007 / 025 | Total loss: 2.392 | Reg loss: 0.028 | Tree loss: 2.392 | Accuracy: 0.171875 | 0.072 sec/iter\n",
      "Epoch: 98 | Batch: 008 / 025 | Total loss: 2.368 | Reg loss: 0.028 | Tree loss: 2.368 | Accuracy: 0.167969 | 0.072 sec/iter\n",
      "Epoch: 98 | Batch: 009 / 025 | Total loss: 2.368 | Reg loss: 0.028 | Tree loss: 2.368 | Accuracy: 0.158203 | 0.072 sec/iter\n",
      "Epoch: 98 | Batch: 010 / 025 | Total loss: 2.363 | Reg loss: 0.028 | Tree loss: 2.363 | Accuracy: 0.169922 | 0.072 sec/iter\n",
      "Epoch: 98 | Batch: 011 / 025 | Total loss: 2.298 | Reg loss: 0.029 | Tree loss: 2.298 | Accuracy: 0.171875 | 0.072 sec/iter\n",
      "Epoch: 98 | Batch: 012 / 025 | Total loss: 2.340 | Reg loss: 0.029 | Tree loss: 2.340 | Accuracy: 0.146484 | 0.072 sec/iter\n",
      "Epoch: 98 | Batch: 013 / 025 | Total loss: 2.360 | Reg loss: 0.029 | Tree loss: 2.360 | Accuracy: 0.169922 | 0.072 sec/iter\n",
      "Epoch: 98 | Batch: 014 / 025 | Total loss: 2.320 | Reg loss: 0.029 | Tree loss: 2.320 | Accuracy: 0.171875 | 0.072 sec/iter\n",
      "Epoch: 98 | Batch: 015 / 025 | Total loss: 2.289 | Reg loss: 0.029 | Tree loss: 2.289 | Accuracy: 0.179688 | 0.072 sec/iter\n",
      "Epoch: 98 | Batch: 016 / 025 | Total loss: 2.291 | Reg loss: 0.029 | Tree loss: 2.291 | Accuracy: 0.166016 | 0.072 sec/iter\n",
      "Epoch: 98 | Batch: 017 / 025 | Total loss: 2.314 | Reg loss: 0.029 | Tree loss: 2.314 | Accuracy: 0.150391 | 0.072 sec/iter\n",
      "Epoch: 98 | Batch: 018 / 025 | Total loss: 2.319 | Reg loss: 0.029 | Tree loss: 2.319 | Accuracy: 0.177734 | 0.072 sec/iter\n",
      "Epoch: 98 | Batch: 019 / 025 | Total loss: 2.328 | Reg loss: 0.029 | Tree loss: 2.328 | Accuracy: 0.158203 | 0.072 sec/iter\n",
      "Epoch: 98 | Batch: 020 / 025 | Total loss: 2.265 | Reg loss: 0.029 | Tree loss: 2.265 | Accuracy: 0.193359 | 0.072 sec/iter\n",
      "Epoch: 98 | Batch: 021 / 025 | Total loss: 2.265 | Reg loss: 0.029 | Tree loss: 2.265 | Accuracy: 0.177734 | 0.072 sec/iter\n",
      "Epoch: 98 | Batch: 022 / 025 | Total loss: 2.333 | Reg loss: 0.029 | Tree loss: 2.333 | Accuracy: 0.162109 | 0.072 sec/iter\n",
      "Epoch: 98 | Batch: 023 / 025 | Total loss: 2.309 | Reg loss: 0.029 | Tree loss: 2.309 | Accuracy: 0.162109 | 0.072 sec/iter\n",
      "Epoch: 98 | Batch: 024 / 025 | Total loss: 2.291 | Reg loss: 0.029 | Tree loss: 2.291 | Accuracy: 0.156250 | 0.072 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 99 | Batch: 000 / 025 | Total loss: 2.441 | Reg loss: 0.028 | Tree loss: 2.441 | Accuracy: 0.173828 | 0.072 sec/iter\n",
      "Epoch: 99 | Batch: 001 / 025 | Total loss: 2.432 | Reg loss: 0.028 | Tree loss: 2.432 | Accuracy: 0.171875 | 0.072 sec/iter\n",
      "Epoch: 99 | Batch: 002 / 025 | Total loss: 2.397 | Reg loss: 0.028 | Tree loss: 2.397 | Accuracy: 0.169922 | 0.072 sec/iter\n",
      "Epoch: 99 | Batch: 003 / 025 | Total loss: 2.399 | Reg loss: 0.028 | Tree loss: 2.399 | Accuracy: 0.193359 | 0.072 sec/iter\n",
      "Epoch: 99 | Batch: 004 / 025 | Total loss: 2.427 | Reg loss: 0.028 | Tree loss: 2.427 | Accuracy: 0.148438 | 0.072 sec/iter\n",
      "Epoch: 99 | Batch: 005 / 025 | Total loss: 2.383 | Reg loss: 0.028 | Tree loss: 2.383 | Accuracy: 0.148438 | 0.072 sec/iter\n",
      "Epoch: 99 | Batch: 006 / 025 | Total loss: 2.409 | Reg loss: 0.028 | Tree loss: 2.409 | Accuracy: 0.183594 | 0.072 sec/iter\n",
      "Epoch: 99 | Batch: 007 / 025 | Total loss: 2.373 | Reg loss: 0.028 | Tree loss: 2.373 | Accuracy: 0.158203 | 0.072 sec/iter\n",
      "Epoch: 99 | Batch: 008 / 025 | Total loss: 2.378 | Reg loss: 0.028 | Tree loss: 2.378 | Accuracy: 0.162109 | 0.072 sec/iter\n",
      "Epoch: 99 | Batch: 009 / 025 | Total loss: 2.324 | Reg loss: 0.028 | Tree loss: 2.324 | Accuracy: 0.167969 | 0.072 sec/iter\n",
      "Epoch: 99 | Batch: 010 / 025 | Total loss: 2.374 | Reg loss: 0.028 | Tree loss: 2.374 | Accuracy: 0.162109 | 0.072 sec/iter\n",
      "Epoch: 99 | Batch: 011 / 025 | Total loss: 2.340 | Reg loss: 0.028 | Tree loss: 2.340 | Accuracy: 0.173828 | 0.072 sec/iter\n",
      "Epoch: 99 | Batch: 012 / 025 | Total loss: 2.356 | Reg loss: 0.028 | Tree loss: 2.356 | Accuracy: 0.169922 | 0.072 sec/iter\n",
      "Epoch: 99 | Batch: 013 / 025 | Total loss: 2.308 | Reg loss: 0.029 | Tree loss: 2.308 | Accuracy: 0.171875 | 0.072 sec/iter\n",
      "Epoch: 99 | Batch: 014 / 025 | Total loss: 2.297 | Reg loss: 0.029 | Tree loss: 2.297 | Accuracy: 0.205078 | 0.072 sec/iter\n",
      "Epoch: 99 | Batch: 015 / 025 | Total loss: 2.337 | Reg loss: 0.029 | Tree loss: 2.337 | Accuracy: 0.154297 | 0.072 sec/iter\n",
      "Epoch: 99 | Batch: 016 / 025 | Total loss: 2.298 | Reg loss: 0.029 | Tree loss: 2.298 | Accuracy: 0.158203 | 0.072 sec/iter\n",
      "Epoch: 99 | Batch: 017 / 025 | Total loss: 2.292 | Reg loss: 0.029 | Tree loss: 2.292 | Accuracy: 0.162109 | 0.072 sec/iter\n",
      "Epoch: 99 | Batch: 018 / 025 | Total loss: 2.315 | Reg loss: 0.029 | Tree loss: 2.315 | Accuracy: 0.138672 | 0.072 sec/iter\n",
      "Epoch: 99 | Batch: 019 / 025 | Total loss: 2.316 | Reg loss: 0.029 | Tree loss: 2.316 | Accuracy: 0.166016 | 0.072 sec/iter\n",
      "Epoch: 99 | Batch: 020 / 025 | Total loss: 2.307 | Reg loss: 0.029 | Tree loss: 2.307 | Accuracy: 0.177734 | 0.072 sec/iter\n",
      "Epoch: 99 | Batch: 021 / 025 | Total loss: 2.296 | Reg loss: 0.029 | Tree loss: 2.296 | Accuracy: 0.167969 | 0.072 sec/iter\n",
      "Epoch: 99 | Batch: 022 / 025 | Total loss: 2.338 | Reg loss: 0.029 | Tree loss: 2.338 | Accuracy: 0.175781 | 0.072 sec/iter\n",
      "Epoch: 99 | Batch: 023 / 025 | Total loss: 2.288 | Reg loss: 0.029 | Tree loss: 2.288 | Accuracy: 0.179688 | 0.072 sec/iter\n",
      "Epoch: 99 | Batch: 024 / 025 | Total loss: 2.278 | Reg loss: 0.029 | Tree loss: 2.278 | Accuracy: 0.168750 | 0.072 sec/iter\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "iteration = 0\n",
    "for epoch in range(epochs):\n",
    "    # Training\n",
    "    avg_sp = show_sparseness(tree)\n",
    "    sparsity.append(avg_sp)\n",
    "    iteration = do_epoch(tree, tree_loader, device, log_interval, losses, accs, epoch, iteration)\n",
    "    \n",
    "    if epoch % 1 == 0:\n",
    "        prune_tree(tree, factor=3)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcb185629e0e478283c8ff33c29a6f32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel('Iteration')\n",
    "plt.plot(accs, label='Accuracy vs iteration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "230f37aaa0c44aaba6e187cf7bab7712",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27087bea6fa248088ecdf5847bb1d295",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel('Iteration')\n",
    "plt.plot(losses, label='Loss vs iteration')\n",
    "plt.yscale(\"log\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "weights = tree.inner_nodes.weight.cpu().detach().numpy().flatten()\n",
    "plt.hist(weights, bins=500)\n",
    "weights_std = np.std(weights)\n",
    "weights_mean = np.mean(weights)\n",
    "plt.axvline(weights_mean + weights_std, color='r')\n",
    "plt.axvline(weights_mean - weights_std, color='r')\n",
    "plt.title(f\"Mean: {weights_mean}   |   STD: {weights_std}\")\n",
    "plt.yscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3762ffb2785b439cbdc4448dd79541f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average height: 4.333333333333333\n"
     ]
    }
   ],
   "source": [
    "plt.figure(figsize=(15, 10), dpi=80)\n",
    "avg_height, root = tree.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accumulate samples in the leaves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of patterns: 12\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of patterns: {len(root.get_leaves())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = 'greedy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eitan.k/.local/lib/python3.6/site-packages/torch/nn/functional.py:1709: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    }
   ],
   "source": [
    "root.clear_leaves_samples()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (data, target) in enumerate(tree_loader):\n",
    "        root.accumulate_samples(data, method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tighten boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11084\n",
      "============== Pattern 1 ==============\n",
      "============== Pattern 2 ==============\n",
      "============== Pattern 3 ==============\n",
      "============== Pattern 4 ==============\n",
      "============== Pattern 5 ==============\n",
      "============== Pattern 6 ==============\n",
      "============== Pattern 7 ==============\n",
      "============== Pattern 8 ==============\n",
      "1524\n",
      "============== Pattern 9 ==============\n",
      "============== Pattern 10 ==============\n",
      "============== Pattern 11 ==============\n",
      "============== Pattern 12 ==============\n",
      "Average comprehensibility: 21.0\n",
      "std comprehensibility: 7.14142842854285\n",
      "var comprehensibility: 51.0\n",
      "minimum comprehensibility: 10\n",
      "maximum comprehensibility: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eitan.k/EntangledExplainableClustering/soft_decision_tree/sdt_model.py:14: RuntimeWarning: divide by zero encountered in log\n",
      "  return np.log(1 / (1 - x))\n"
     ]
    }
   ],
   "source": [
    "attr_names = dataset.items\n",
    "\n",
    "# print(attr_names)\n",
    "leaves = root.get_leaves()\n",
    "sum_comprehensibility = 0\n",
    "comprehensibilities = []\n",
    "for pattern_counter, leaf in enumerate(leaves):\n",
    "    leaf.reset_path()\n",
    "    leaf.tighten_with_accumulated_samples()\n",
    "    conds = leaf.get_path_conditions(attr_names)\n",
    "    print(f\"============== Pattern {pattern_counter + 1} ==============\")\n",
    "    comprehensibilities.append(sum([cond.comprehensibility for cond in conds]))\n",
    "    \n",
    "print(f\"Average comprehensibility: {np.mean(comprehensibilities)}\")\n",
    "print(f\"std comprehensibility: {np.std(comprehensibilities)}\")\n",
    "print(f\"var comprehensibility: {np.var(comprehensibilities)}\")\n",
    "print(f\"minimum comprehensibility: {np.min(comprehensibilities)}\")\n",
    "print(f\"maximum comprehensibility: {np.max(comprehensibilities)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
