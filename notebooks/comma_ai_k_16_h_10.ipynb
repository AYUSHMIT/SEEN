{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "%matplotlib inline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "import time\n",
    "import copy\n",
    "import os\n",
    "from queue import LifoQueue\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "from scipy.stats import kurtosis\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import network.cpc\n",
    "from network.cpc import CDCK2\n",
    "from utils.MatplotlibUtils import reduce_dims_and_plot\n",
    "from utils.ClassificationUtiols import onehot_coding\n",
    "from soft_decision_tree.sdt_model import SDT\n",
    "from sklearn import tree as tt\n",
    "\n",
    "# IPython.OutputArea.auto_scroll_threshold = 9999;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the model and the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load the model from: /home/eitan.k/EntangledExplainableClustering/knn_loss_batch_512_k_16/models/epoch_28.pt\n",
      "sensor names: (18 total)\n",
      "- speed\n",
      "- steering_angle\n",
      "- wheel_speed_0\n",
      "- wheel_speed_1\n",
      "- wheel_speed_2\n",
      "- wheel_speed_3\n",
      "- accelerometer_0\n",
      "- accelerometer_1\n",
      "- accelerometer_2\n",
      "- gyro_0\n",
      "- gyro_1\n",
      "- gyro_2\n",
      "- gyro_bias_0\n",
      "- gyro_bias_1\n",
      "- gyro_bias_2\n",
      "- gyro_uncalibrated_0\n",
      "- gyro_uncalibrated_1\n",
      "- gyro_uncalibrated_2\n",
      "Multihorizon size of the model: 30\n",
      "Test split ratio: 0.2\n",
      "Total number of windows in the dataset (without splitting): 101465\n"
     ]
    }
   ],
   "source": [
    "model_path = r''  # Insert path of the cpc model\n",
    "dataset_path = r''  # Insert path of the test dataset that was created using the run_cpc.py script\n",
    "\n",
    "print(f\"Load the model from: {model_path}\")\n",
    "model = torch.load(model_path, map_location='cpu')\n",
    "\n",
    "with open(dataset_path, 'rb') as fp:\n",
    "    dataset = pickle.load(fp)\n",
    "\n",
    "all_sensors = dataset.dataset.all_signals    \n",
    "print(f\"sensor names: ({len(all_sensors)} total)\")\n",
    "\n",
    "for s in all_sensors:\n",
    "    print(f\"- {s}\")\n",
    "    \n",
    "print(f\"Multihorizon size of the model: {model.timestep}\")\n",
    "print(f\"Test split ratio: {len(dataset) / len(dataset.dataset)}\")\n",
    "print(f\"Total number of windows in the dataset (without splitting): {len(dataset.dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 500\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Extract representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c6020823ffe49e4acb01d555fa2b0c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20293 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "projections = torch.tensor([])\n",
    "samples = torch.tensor([])\n",
    "device = 'cuda'\n",
    "model = model.to(device).eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    bar = tqdm(total=len(loader.dataset))\n",
    "    for batch in loader:\n",
    "        hidden = CDCK2.init_hidden(len(batch))\n",
    "        batch = batch.to(device)\n",
    "        hidden = hidden.to(device)\n",
    "\n",
    "        y = model.predict(batch, hidden).detach().cpu()\n",
    "        projections = torch.cat([projections, y.detach().cpu()])\n",
    "        samples = torch.cat([samples, batch.detach().cpu()])\n",
    "        bar.update(y.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit GMM and calculate indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8276e301584d47b2af2a054858a9c190",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scores = []\n",
    "best_score = float('inf')\n",
    "clusters = None\n",
    "range_ = list(range(5, 80))\n",
    "for k in tqdm(range_):\n",
    "    y = GaussianMixture(n_components=k).fit_predict(projections)\n",
    "    cur_score = davies_bouldin_score(projections, y)\n",
    "    scores.append(cur_score)\n",
    "    \n",
    "    if cur_score < best_score:\n",
    "        best_score = cur_score\n",
    "        clusters = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19}\n"
     ]
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('DB Score')\n",
    "plt.plot(range_, scores)\n",
    "best_k = range_[np.argmin(scores)]\n",
    "plt.axvline(best_k, color='r')\n",
    "plt.show()\n",
    "\n",
    "labels = set(clusters)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "distances = pairwise_distances(projections)\n",
    "# distances = np.triu(distances)\n",
    "distances_f = distances.flatten()\n",
    "\n",
    "plt.matshow(distances)\n",
    "plt.colorbar()\n",
    "plt.figure()\n",
    "plt.hist(distances_f[distances_f > 0], bins=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize with T-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "perplexity = 200\n",
    "\n",
    "p = reduce_dims_and_plot(projections,\n",
    "                         y=clusters,\n",
    "                         title=f'perplexity: {perplexity}',\n",
    "                         file_name=None,\n",
    "                         perplexity=perplexity,\n",
    "                         library='Multicore-TSNE',\n",
    "                         perform_PCA=False,\n",
    "                         projected=None,\n",
    "                         figure_type='2d',\n",
    "                         show_figure=True,\n",
    "                         close_figure=False,\n",
    "                         text=None)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Soft-Decision-Tree given the self-labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unnormalized_samples = samples.clone()\n",
    "\n",
    "# for col, sensor in enumerate(tqdm(dataset.dataset.all_signals)):\n",
    "#     denormalizer = dataset.dataset.get_denormalization_for_sensor(sensor)\n",
    "#     unnormalized_samples[:, col, :] = denormalizer(unnormalized_samples[:, col, :])\n",
    "\n",
    "sampled = samples[..., range(0, samples.shape[-1], 200)]\n",
    "\n",
    "samples_f = sampled.flatten(1)\n",
    "tree_dataset = list(zip(samples_f, clusters))\n",
    "batch_size = 2000\n",
    "tree_loader = torch.utils.data.DataLoader(tree_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define how we prune the weights of a node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_node(node_weights, factor=1):\n",
    "    w = node_weights.cpu().detach().numpy()\n",
    "    mean_ = np.mean(w)\n",
    "    std_ = np.std(w)\n",
    "    node_weights[((mean_ - std_ * factor) < node_weights) & (node_weights < (mean_ + std_ * factor))] = 0\n",
    "    return node_weights\n",
    "\n",
    "def prune_node_keep(node_weights, keep=4):\n",
    "    w = node_weights.cpu().detach().numpy()\n",
    "    throw_idx = np.argsort(abs(w))[:-keep]\n",
    "    node_weights[throw_idx] = 0\n",
    "    return node_weights\n",
    "\n",
    "def prune_tree(tree_, factor):\n",
    "    new_weights = tree_.inner_nodes.weight.clone()\n",
    "    for i in range(new_weights.shape[0]):\n",
    "        res = prune_node_keep(new_weights[i, :], factor)\n",
    "        new_weights[i, :] = res\n",
    "\n",
    "    with torch.no_grad():\n",
    "        tree_.inner_nodes.weight.copy_(new_weights)\n",
    "        \n",
    "def sparseness(x):\n",
    "    s = []\n",
    "    for i in range(x.shape[0]):\n",
    "        x_ = x[i, :]\n",
    "        sp = (len(x_) - torch.norm(x_, 0).item()) / len(x_)\n",
    "        s.append(sp)\n",
    "    return np.mean(s)\n",
    "\n",
    "def compute_regularization_by_level(tree):\n",
    "    total_reg = 0\n",
    "    for i in range(tree.inner_nodes.weight.shape[0]):\n",
    "        cur_level = np.floor(np.log2(i+1))\n",
    "        node_reg = torch.norm(tree.inner_nodes.weight[i].view(-1), 2)\n",
    "        total_reg += 2**(-cur_level) * node_reg\n",
    "    return total_reg\n",
    "\n",
    "def show_sparseness(tree):\n",
    "    avg_sp = sparseness(tree.inner_nodes.weight)\n",
    "    print(f\"Average sparseness: {avg_sp}\")\n",
    "    layer = 0\n",
    "    sps = []\n",
    "    for i in range(tree.inner_nodes.weight.shape[0]):\n",
    "        cur_layer = int(np.floor(np.log2(i+1)))\n",
    "        if cur_layer != layer:\n",
    "            print(f\"layer {layer}: {np.mean(sps)}\")\n",
    "            sps = []\n",
    "            layer = cur_layer\n",
    "\n",
    "        x_ = tree.inner_nodes.weight[i, :]\n",
    "        sp = (len(x_) - torch.norm(x_, 0).item()) / len(x_)\n",
    "        sps.append(sp)\n",
    "        \n",
    "    return avg_sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_epoch(model, loader, device, log_interval, losses, accs, epoch, iteration):\n",
    "    model = model.train()\n",
    "    for batch_idx, (data, target) in enumerate(loader):\n",
    "        iteration += 1\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        output, penalty = tree.forward(data)\n",
    "\n",
    "        # Loss\n",
    "        loss_tree = criterion(output, target.view(-1))\n",
    "\n",
    "        # Penalty\n",
    "        loss_tree += penalty\n",
    "\n",
    "        # Sparse regularization\n",
    "#         fc_params = torch.cat([x.view(-1) for x in tree.inner_nodes.parameters()])\n",
    "#         regularization = sparsity_lamda * torch.norm(fc_params, 2)\n",
    "        regularization = sparsity_lamda * compute_regularization_by_level(tree)\n",
    "        loss = loss_tree\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        pred = output.data.max(1)[1]\n",
    "        correct = pred.eq(target.view(-1).data).sum()\n",
    "        accs.append(correct.item() / data.size()[0])\n",
    "\n",
    "        # Print training status\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(f\"Epoch: {epoch:02d} | Batch: {batch_idx:03d} / {len(loader):03d} | Total loss: {loss.item():.3f} | Reg loss: {regularization.item():.3f} | Tree loss: {loss_tree.item():.3f} | Accuracy: {correct.item() / data.size()[0]:03f} | {round((time.time() - start_time) / iteration, 3)} sec/iter\")\n",
    "            \n",
    "    return iteration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 5e-3\n",
    "weight_decay = 5e-4\n",
    "sparsity_lamda = 2e-3\n",
    "epochs = 500\n",
    "output_dim = len(set(clusters))\n",
    "log_interval = 1\n",
    "tree_depth = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT accuracy: 0.9837382348593111\n"
     ]
    }
   ],
   "source": [
    "tree = SDT(input_dim=samples_f.shape[1], output_dim=len(labels), depth=tree_depth, lamda=1e-3, use_cuda=True)\n",
    "optimizer = torch.optim.Adam(tree.parameters(),\n",
    "                                 lr=lr,\n",
    "                                 weight_decay=weight_decay)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "tree = tree.to(device)\n",
    "clf = DecisionTreeClassifier(max_depth=tree_depth).fit(samples_f, clusters)\n",
    "print(f\"DT accuracy: {clf.score(samples_f, clusters)}\")\n",
    "tree.initialize_from_decision_tree(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "accs = []\n",
    "sparsity = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.2851811647035315\n",
      "layer 0: 0.988950276243094\n",
      "layer 1: 0.988950276243094\n",
      "layer 2: 0.988950276243094\n",
      "layer 3: 0.988950276243094\n",
      "layer 4: 0.988950276243094\n",
      "layer 5: 0.7108080110497237\n",
      "layer 6: 0.6180939226519337\n",
      "layer 7: 0.448118093922652\n",
      "layer 8: 0.25882683011049723\n",
      "Epoch: 00 | Batch: 000 / 011 | Total loss: 2.997 | Reg loss: 0.019 | Tree loss: 2.997 | Accuracy: 0.062000 | 1.396 sec/iter\n",
      "Epoch: 00 | Batch: 001 / 011 | Total loss: 2.989 | Reg loss: 0.018 | Tree loss: 2.989 | Accuracy: 0.062500 | 1.204 sec/iter\n",
      "Epoch: 00 | Batch: 002 / 011 | Total loss: 2.982 | Reg loss: 0.018 | Tree loss: 2.982 | Accuracy: 0.066500 | 1.188 sec/iter\n",
      "Epoch: 00 | Batch: 003 / 011 | Total loss: 2.979 | Reg loss: 0.018 | Tree loss: 2.979 | Accuracy: 0.062500 | 1.269 sec/iter\n",
      "Epoch: 00 | Batch: 004 / 011 | Total loss: 2.975 | Reg loss: 0.018 | Tree loss: 2.975 | Accuracy: 0.083500 | 1.411 sec/iter\n",
      "Epoch: 00 | Batch: 005 / 011 | Total loss: 2.967 | Reg loss: 0.017 | Tree loss: 2.967 | Accuracy: 0.125000 | 1.536 sec/iter\n",
      "Epoch: 00 | Batch: 006 / 011 | Total loss: 2.962 | Reg loss: 0.017 | Tree loss: 2.962 | Accuracy: 0.166000 | 1.556 sec/iter\n",
      "Epoch: 00 | Batch: 007 / 011 | Total loss: 2.956 | Reg loss: 0.017 | Tree loss: 2.956 | Accuracy: 0.177000 | 1.516 sec/iter\n",
      "Epoch: 00 | Batch: 008 / 011 | Total loss: 2.952 | Reg loss: 0.017 | Tree loss: 2.952 | Accuracy: 0.164500 | 1.485 sec/iter\n",
      "Epoch: 00 | Batch: 009 / 011 | Total loss: 2.942 | Reg loss: 0.017 | Tree loss: 2.942 | Accuracy: 0.182000 | 1.465 sec/iter\n",
      "Epoch: 00 | Batch: 010 / 011 | Total loss: 2.936 | Reg loss: 0.017 | Tree loss: 2.936 | Accuracy: 0.197952 | 1.484 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 01 | Batch: 000 / 011 | Total loss: 2.985 | Reg loss: 0.015 | Tree loss: 2.985 | Accuracy: 0.062000 | 1.534 sec/iter\n",
      "Epoch: 01 | Batch: 001 / 011 | Total loss: 2.980 | Reg loss: 0.015 | Tree loss: 2.980 | Accuracy: 0.065500 | 1.54 sec/iter\n",
      "Epoch: 01 | Batch: 002 / 011 | Total loss: 2.975 | Reg loss: 0.016 | Tree loss: 2.975 | Accuracy: 0.073500 | 1.555 sec/iter\n",
      "Epoch: 01 | Batch: 003 / 011 | Total loss: 2.967 | Reg loss: 0.016 | Tree loss: 2.967 | Accuracy: 0.097000 | 1.543 sec/iter\n",
      "Epoch: 01 | Batch: 004 / 011 | Total loss: 2.960 | Reg loss: 0.016 | Tree loss: 2.960 | Accuracy: 0.116000 | 1.524 sec/iter\n",
      "Epoch: 01 | Batch: 005 / 011 | Total loss: 2.952 | Reg loss: 0.016 | Tree loss: 2.952 | Accuracy: 0.157500 | 1.509 sec/iter\n",
      "Epoch: 01 | Batch: 006 / 011 | Total loss: 2.943 | Reg loss: 0.016 | Tree loss: 2.943 | Accuracy: 0.184000 | 1.503 sec/iter\n",
      "Epoch: 01 | Batch: 007 / 011 | Total loss: 2.939 | Reg loss: 0.016 | Tree loss: 2.939 | Accuracy: 0.158000 | 1.502 sec/iter\n",
      "Epoch: 01 | Batch: 008 / 011 | Total loss: 2.937 | Reg loss: 0.017 | Tree loss: 2.937 | Accuracy: 0.160500 | 1.499 sec/iter\n",
      "Epoch: 01 | Batch: 009 / 011 | Total loss: 2.927 | Reg loss: 0.017 | Tree loss: 2.927 | Accuracy: 0.177500 | 1.506 sec/iter\n",
      "Epoch: 01 | Batch: 010 / 011 | Total loss: 2.928 | Reg loss: 0.017 | Tree loss: 2.928 | Accuracy: 0.163823 | 1.508 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 02 | Batch: 000 / 011 | Total loss: 2.975 | Reg loss: 0.015 | Tree loss: 2.975 | Accuracy: 0.086500 | 1.496 sec/iter\n",
      "Epoch: 02 | Batch: 001 / 011 | Total loss: 2.971 | Reg loss: 0.015 | Tree loss: 2.971 | Accuracy: 0.102000 | 1.48 sec/iter\n",
      "Epoch: 02 | Batch: 002 / 011 | Total loss: 2.966 | Reg loss: 0.015 | Tree loss: 2.966 | Accuracy: 0.187500 | 1.469 sec/iter\n",
      "Epoch: 02 | Batch: 003 / 011 | Total loss: 2.956 | Reg loss: 0.016 | Tree loss: 2.956 | Accuracy: 0.243500 | 1.468 sec/iter\n",
      "Epoch: 02 | Batch: 004 / 011 | Total loss: 2.946 | Reg loss: 0.016 | Tree loss: 2.946 | Accuracy: 0.236500 | 1.478 sec/iter\n",
      "Epoch: 02 | Batch: 005 / 011 | Total loss: 2.937 | Reg loss: 0.016 | Tree loss: 2.937 | Accuracy: 0.233500 | 1.483 sec/iter\n",
      "Epoch: 02 | Batch: 006 / 011 | Total loss: 2.930 | Reg loss: 0.016 | Tree loss: 2.930 | Accuracy: 0.230000 | 1.491 sec/iter\n",
      "Epoch: 02 | Batch: 007 / 011 | Total loss: 2.920 | Reg loss: 0.016 | Tree loss: 2.920 | Accuracy: 0.227500 | 1.503 sec/iter\n",
      "Epoch: 02 | Batch: 008 / 011 | Total loss: 2.916 | Reg loss: 0.016 | Tree loss: 2.916 | Accuracy: 0.220000 | 1.492 sec/iter\n",
      "Epoch: 02 | Batch: 009 / 011 | Total loss: 2.910 | Reg loss: 0.017 | Tree loss: 2.910 | Accuracy: 0.205500 | 1.484 sec/iter\n",
      "Epoch: 02 | Batch: 010 / 011 | Total loss: 2.897 | Reg loss: 0.017 | Tree loss: 2.897 | Accuracy: 0.232082 | 1.483 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 03 | Batch: 000 / 011 | Total loss: 2.968 | Reg loss: 0.015 | Tree loss: 2.968 | Accuracy: 0.121500 | 1.485 sec/iter\n",
      "Epoch: 03 | Batch: 001 / 011 | Total loss: 2.963 | Reg loss: 0.015 | Tree loss: 2.963 | Accuracy: 0.130000 | 1.483 sec/iter\n",
      "Epoch: 03 | Batch: 002 / 011 | Total loss: 2.954 | Reg loss: 0.016 | Tree loss: 2.954 | Accuracy: 0.193500 | 1.485 sec/iter\n",
      "Epoch: 03 | Batch: 003 / 011 | Total loss: 2.945 | Reg loss: 0.016 | Tree loss: 2.945 | Accuracy: 0.218500 | 1.477 sec/iter\n",
      "Epoch: 03 | Batch: 004 / 011 | Total loss: 2.933 | Reg loss: 0.016 | Tree loss: 2.933 | Accuracy: 0.220000 | 1.482 sec/iter\n",
      "Epoch: 03 | Batch: 005 / 011 | Total loss: 2.924 | Reg loss: 0.016 | Tree loss: 2.924 | Accuracy: 0.206500 | 1.478 sec/iter\n",
      "Epoch: 03 | Batch: 006 / 011 | Total loss: 2.912 | Reg loss: 0.016 | Tree loss: 2.912 | Accuracy: 0.240500 | 1.472 sec/iter\n",
      "Epoch: 03 | Batch: 007 / 011 | Total loss: 2.901 | Reg loss: 0.016 | Tree loss: 2.901 | Accuracy: 0.263000 | 1.473 sec/iter\n",
      "Epoch: 03 | Batch: 008 / 011 | Total loss: 2.890 | Reg loss: 0.017 | Tree loss: 2.890 | Accuracy: 0.232000 | 1.48 sec/iter\n",
      "Epoch: 03 | Batch: 009 / 011 | Total loss: 2.883 | Reg loss: 0.017 | Tree loss: 2.883 | Accuracy: 0.233500 | 1.487 sec/iter\n",
      "Epoch: 03 | Batch: 010 / 011 | Total loss: 2.865 | Reg loss: 0.017 | Tree loss: 2.865 | Accuracy: 0.252560 | 1.49 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 04 | Batch: 000 / 011 | Total loss: 2.962 | Reg loss: 0.016 | Tree loss: 2.962 | Accuracy: 0.145500 | 1.486 sec/iter\n",
      "Epoch: 04 | Batch: 001 / 011 | Total loss: 2.953 | Reg loss: 0.016 | Tree loss: 2.953 | Accuracy: 0.221500 | 1.475 sec/iter\n",
      "Epoch: 04 | Batch: 002 / 011 | Total loss: 2.939 | Reg loss: 0.016 | Tree loss: 2.939 | Accuracy: 0.246500 | 1.47 sec/iter\n",
      "Epoch: 04 | Batch: 003 / 011 | Total loss: 2.928 | Reg loss: 0.016 | Tree loss: 2.928 | Accuracy: 0.219500 | 1.471 sec/iter\n",
      "Epoch: 04 | Batch: 004 / 011 | Total loss: 2.910 | Reg loss: 0.016 | Tree loss: 2.910 | Accuracy: 0.223000 | 1.476 sec/iter\n",
      "Epoch: 04 | Batch: 005 / 011 | Total loss: 2.897 | Reg loss: 0.016 | Tree loss: 2.897 | Accuracy: 0.228500 | 1.485 sec/iter\n",
      "Epoch: 04 | Batch: 006 / 011 | Total loss: 2.885 | Reg loss: 0.016 | Tree loss: 2.885 | Accuracy: 0.225500 | 1.488 sec/iter\n",
      "Epoch: 04 | Batch: 007 / 011 | Total loss: 2.871 | Reg loss: 0.017 | Tree loss: 2.871 | Accuracy: 0.216000 | 1.492 sec/iter\n",
      "Epoch: 04 | Batch: 008 / 011 | Total loss: 2.858 | Reg loss: 0.017 | Tree loss: 2.858 | Accuracy: 0.237500 | 1.487 sec/iter\n",
      "Epoch: 04 | Batch: 009 / 011 | Total loss: 2.849 | Reg loss: 0.017 | Tree loss: 2.849 | Accuracy: 0.232000 | 1.483 sec/iter\n",
      "Epoch: 04 | Batch: 010 / 011 | Total loss: 2.836 | Reg loss: 0.018 | Tree loss: 2.836 | Accuracy: 0.211604 | 1.481 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 05 | Batch: 000 / 011 | Total loss: 2.953 | Reg loss: 0.016 | Tree loss: 2.953 | Accuracy: 0.191000 | 1.488 sec/iter\n",
      "Epoch: 05 | Batch: 001 / 011 | Total loss: 2.939 | Reg loss: 0.016 | Tree loss: 2.939 | Accuracy: 0.244500 | 1.49 sec/iter\n",
      "Epoch: 05 | Batch: 002 / 011 | Total loss: 2.926 | Reg loss: 0.016 | Tree loss: 2.926 | Accuracy: 0.226000 | 1.492 sec/iter\n",
      "Epoch: 05 | Batch: 003 / 011 | Total loss: 2.907 | Reg loss: 0.016 | Tree loss: 2.907 | Accuracy: 0.209500 | 1.489 sec/iter\n",
      "Epoch: 05 | Batch: 004 / 011 | Total loss: 2.886 | Reg loss: 0.016 | Tree loss: 2.886 | Accuracy: 0.214500 | 1.484 sec/iter\n",
      "Epoch: 05 | Batch: 005 / 011 | Total loss: 2.871 | Reg loss: 0.017 | Tree loss: 2.871 | Accuracy: 0.209500 | 1.478 sec/iter\n",
      "Epoch: 05 | Batch: 006 / 011 | Total loss: 2.856 | Reg loss: 0.017 | Tree loss: 2.856 | Accuracy: 0.212000 | 1.473 sec/iter\n",
      "Epoch: 05 | Batch: 007 / 011 | Total loss: 2.830 | Reg loss: 0.017 | Tree loss: 2.830 | Accuracy: 0.239000 | 1.476 sec/iter\n",
      "Epoch: 05 | Batch: 008 / 011 | Total loss: 2.815 | Reg loss: 0.017 | Tree loss: 2.815 | Accuracy: 0.214500 | 1.473 sec/iter\n",
      "Epoch: 05 | Batch: 009 / 011 | Total loss: 2.810 | Reg loss: 0.018 | Tree loss: 2.810 | Accuracy: 0.217500 | 1.479 sec/iter\n",
      "Epoch: 05 | Batch: 010 / 011 | Total loss: 2.803 | Reg loss: 0.018 | Tree loss: 2.803 | Accuracy: 0.204778 | 1.484 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 06 | Batch: 000 / 011 | Total loss: 2.942 | Reg loss: 0.016 | Tree loss: 2.942 | Accuracy: 0.220000 | 1.481 sec/iter\n",
      "Epoch: 06 | Batch: 001 / 011 | Total loss: 2.921 | Reg loss: 0.016 | Tree loss: 2.921 | Accuracy: 0.250500 | 1.477 sec/iter\n",
      "Epoch: 06 | Batch: 002 / 011 | Total loss: 2.899 | Reg loss: 0.017 | Tree loss: 2.899 | Accuracy: 0.247000 | 1.473 sec/iter\n",
      "Epoch: 06 | Batch: 003 / 011 | Total loss: 2.881 | Reg loss: 0.017 | Tree loss: 2.881 | Accuracy: 0.231500 | 1.472 sec/iter\n",
      "Epoch: 06 | Batch: 004 / 011 | Total loss: 2.855 | Reg loss: 0.017 | Tree loss: 2.855 | Accuracy: 0.231500 | 1.472 sec/iter\n",
      "Epoch: 06 | Batch: 005 / 011 | Total loss: 2.826 | Reg loss: 0.017 | Tree loss: 2.826 | Accuracy: 0.250000 | 1.475 sec/iter\n",
      "Epoch: 06 | Batch: 006 / 011 | Total loss: 2.806 | Reg loss: 0.017 | Tree loss: 2.806 | Accuracy: 0.228000 | 1.479 sec/iter\n",
      "Epoch: 06 | Batch: 007 / 011 | Total loss: 2.792 | Reg loss: 0.018 | Tree loss: 2.792 | Accuracy: 0.227500 | 1.484 sec/iter\n",
      "Epoch: 06 | Batch: 008 / 011 | Total loss: 2.776 | Reg loss: 0.018 | Tree loss: 2.776 | Accuracy: 0.223000 | 1.481 sec/iter\n",
      "Epoch: 06 | Batch: 009 / 011 | Total loss: 2.773 | Reg loss: 0.018 | Tree loss: 2.773 | Accuracy: 0.204000 | 1.477 sec/iter\n",
      "Epoch: 06 | Batch: 010 / 011 | Total loss: 2.755 | Reg loss: 0.018 | Tree loss: 2.755 | Accuracy: 0.163823 | 1.478 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 07 | Batch: 000 / 011 | Total loss: 2.924 | Reg loss: 0.017 | Tree loss: 2.924 | Accuracy: 0.235500 | 1.482 sec/iter\n",
      "Epoch: 07 | Batch: 001 / 011 | Total loss: 2.902 | Reg loss: 0.017 | Tree loss: 2.902 | Accuracy: 0.239000 | 1.487 sec/iter\n",
      "Epoch: 07 | Batch: 002 / 011 | Total loss: 2.877 | Reg loss: 0.017 | Tree loss: 2.877 | Accuracy: 0.204000 | 1.487 sec/iter\n",
      "Epoch: 07 | Batch: 003 / 011 | Total loss: 2.847 | Reg loss: 0.017 | Tree loss: 2.847 | Accuracy: 0.220000 | 1.488 sec/iter\n",
      "Epoch: 07 | Batch: 004 / 011 | Total loss: 2.817 | Reg loss: 0.017 | Tree loss: 2.817 | Accuracy: 0.238000 | 1.486 sec/iter\n",
      "Epoch: 07 | Batch: 005 / 011 | Total loss: 2.790 | Reg loss: 0.018 | Tree loss: 2.790 | Accuracy: 0.223500 | 1.484 sec/iter\n",
      "Epoch: 07 | Batch: 006 / 011 | Total loss: 2.763 | Reg loss: 0.018 | Tree loss: 2.763 | Accuracy: 0.230500 | 1.488 sec/iter\n",
      "Epoch: 07 | Batch: 007 / 011 | Total loss: 2.741 | Reg loss: 0.018 | Tree loss: 2.741 | Accuracy: 0.229000 | 1.491 sec/iter\n",
      "Epoch: 07 | Batch: 008 / 011 | Total loss: 2.727 | Reg loss: 0.018 | Tree loss: 2.727 | Accuracy: 0.217000 | 1.492 sec/iter\n",
      "Epoch: 07 | Batch: 009 / 011 | Total loss: 2.706 | Reg loss: 0.019 | Tree loss: 2.706 | Accuracy: 0.226500 | 1.492 sec/iter\n",
      "Epoch: 07 | Batch: 010 / 011 | Total loss: 2.701 | Reg loss: 0.019 | Tree loss: 2.701 | Accuracy: 0.177474 | 1.489 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 08 | Batch: 000 / 011 | Total loss: 2.907 | Reg loss: 0.017 | Tree loss: 2.907 | Accuracy: 0.218000 | 1.485 sec/iter\n",
      "Epoch: 08 | Batch: 001 / 011 | Total loss: 2.868 | Reg loss: 0.018 | Tree loss: 2.868 | Accuracy: 0.224000 | 1.481 sec/iter\n",
      "Epoch: 08 | Batch: 002 / 011 | Total loss: 2.842 | Reg loss: 0.018 | Tree loss: 2.842 | Accuracy: 0.224500 | 1.486 sec/iter\n",
      "Epoch: 08 | Batch: 003 / 011 | Total loss: 2.806 | Reg loss: 0.018 | Tree loss: 2.806 | Accuracy: 0.226000 | 1.489 sec/iter\n",
      "Epoch: 08 | Batch: 004 / 011 | Total loss: 2.770 | Reg loss: 0.018 | Tree loss: 2.770 | Accuracy: 0.228500 | 1.491 sec/iter\n",
      "Epoch: 08 | Batch: 005 / 011 | Total loss: 2.732 | Reg loss: 0.018 | Tree loss: 2.732 | Accuracy: 0.236500 | 1.491 sec/iter\n",
      "Epoch: 08 | Batch: 006 / 011 | Total loss: 2.713 | Reg loss: 0.018 | Tree loss: 2.713 | Accuracy: 0.227500 | 1.492 sec/iter\n",
      "Epoch: 08 | Batch: 007 / 011 | Total loss: 2.700 | Reg loss: 0.019 | Tree loss: 2.700 | Accuracy: 0.218000 | 1.487 sec/iter\n",
      "Epoch: 08 | Batch: 008 / 011 | Total loss: 2.662 | Reg loss: 0.019 | Tree loss: 2.662 | Accuracy: 0.222500 | 1.483 sec/iter\n",
      "Epoch: 08 | Batch: 009 / 011 | Total loss: 2.662 | Reg loss: 0.019 | Tree loss: 2.662 | Accuracy: 0.215000 | 1.481 sec/iter\n",
      "Epoch: 08 | Batch: 010 / 011 | Total loss: 2.642 | Reg loss: 0.020 | Tree loss: 2.642 | Accuracy: 0.215017 | 1.476 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 09 | Batch: 000 / 011 | Total loss: 2.872 | Reg loss: 0.018 | Tree loss: 2.872 | Accuracy: 0.226000 | 1.477 sec/iter\n",
      "Epoch: 09 | Batch: 001 / 011 | Total loss: 2.839 | Reg loss: 0.018 | Tree loss: 2.839 | Accuracy: 0.201000 | 1.476 sec/iter\n",
      "Epoch: 09 | Batch: 002 / 011 | Total loss: 2.800 | Reg loss: 0.018 | Tree loss: 2.800 | Accuracy: 0.211500 | 1.479 sec/iter\n",
      "Epoch: 09 | Batch: 003 / 011 | Total loss: 2.765 | Reg loss: 0.018 | Tree loss: 2.765 | Accuracy: 0.206500 | 1.479 sec/iter\n",
      "Epoch: 09 | Batch: 004 / 011 | Total loss: 2.734 | Reg loss: 0.019 | Tree loss: 2.734 | Accuracy: 0.215500 | 1.481 sec/iter\n",
      "Epoch: 09 | Batch: 005 / 011 | Total loss: 2.691 | Reg loss: 0.019 | Tree loss: 2.691 | Accuracy: 0.240500 | 1.477 sec/iter\n",
      "Epoch: 09 | Batch: 006 / 011 | Total loss: 2.661 | Reg loss: 0.019 | Tree loss: 2.661 | Accuracy: 0.229500 | 1.476 sec/iter\n",
      "Epoch: 09 | Batch: 007 / 011 | Total loss: 2.644 | Reg loss: 0.019 | Tree loss: 2.644 | Accuracy: 0.212000 | 1.474 sec/iter\n",
      "Epoch: 09 | Batch: 008 / 011 | Total loss: 2.631 | Reg loss: 0.020 | Tree loss: 2.631 | Accuracy: 0.198000 | 1.476 sec/iter\n",
      "Epoch: 09 | Batch: 009 / 011 | Total loss: 2.608 | Reg loss: 0.020 | Tree loss: 2.608 | Accuracy: 0.216000 | 1.479 sec/iter\n",
      "Epoch: 09 | Batch: 010 / 011 | Total loss: 2.598 | Reg loss: 0.020 | Tree loss: 2.598 | Accuracy: 0.187713 | 1.482 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 10 | Batch: 000 / 011 | Total loss: 2.840 | Reg loss: 0.019 | Tree loss: 2.840 | Accuracy: 0.216500 | 1.485 sec/iter\n",
      "Epoch: 10 | Batch: 001 / 011 | Total loss: 2.802 | Reg loss: 0.019 | Tree loss: 2.802 | Accuracy: 0.209500 | 1.484 sec/iter\n",
      "Epoch: 10 | Batch: 002 / 011 | Total loss: 2.762 | Reg loss: 0.019 | Tree loss: 2.762 | Accuracy: 0.212000 | 1.481 sec/iter\n",
      "Epoch: 10 | Batch: 003 / 011 | Total loss: 2.715 | Reg loss: 0.019 | Tree loss: 2.715 | Accuracy: 0.225500 | 1.48 sec/iter\n",
      "Epoch: 10 | Batch: 004 / 011 | Total loss: 2.682 | Reg loss: 0.019 | Tree loss: 2.682 | Accuracy: 0.221500 | 1.48 sec/iter\n",
      "Epoch: 10 | Batch: 005 / 011 | Total loss: 2.648 | Reg loss: 0.019 | Tree loss: 2.648 | Accuracy: 0.215500 | 1.484 sec/iter\n",
      "Epoch: 10 | Batch: 006 / 011 | Total loss: 2.617 | Reg loss: 0.020 | Tree loss: 2.617 | Accuracy: 0.213500 | 1.487 sec/iter\n",
      "Epoch: 10 | Batch: 007 / 011 | Total loss: 2.591 | Reg loss: 0.020 | Tree loss: 2.591 | Accuracy: 0.227500 | 1.488 sec/iter\n",
      "Epoch: 10 | Batch: 008 / 011 | Total loss: 2.568 | Reg loss: 0.020 | Tree loss: 2.568 | Accuracy: 0.211500 | 1.485 sec/iter\n",
      "Epoch: 10 | Batch: 009 / 011 | Total loss: 2.556 | Reg loss: 0.020 | Tree loss: 2.556 | Accuracy: 0.212500 | 1.481 sec/iter\n",
      "Epoch: 10 | Batch: 010 / 011 | Total loss: 2.541 | Reg loss: 0.021 | Tree loss: 2.541 | Accuracy: 0.228669 | 1.476 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 11 | Batch: 000 / 011 | Total loss: 2.805 | Reg loss: 0.019 | Tree loss: 2.805 | Accuracy: 0.211000 | 1.475 sec/iter\n",
      "Epoch: 11 | Batch: 001 / 011 | Total loss: 2.764 | Reg loss: 0.020 | Tree loss: 2.764 | Accuracy: 0.203500 | 1.475 sec/iter\n",
      "Epoch: 11 | Batch: 002 / 011 | Total loss: 2.719 | Reg loss: 0.020 | Tree loss: 2.719 | Accuracy: 0.201500 | 1.479 sec/iter\n",
      "Epoch: 11 | Batch: 003 / 011 | Total loss: 2.680 | Reg loss: 0.020 | Tree loss: 2.680 | Accuracy: 0.216500 | 1.477 sec/iter\n",
      "Epoch: 11 | Batch: 004 / 011 | Total loss: 2.643 | Reg loss: 0.020 | Tree loss: 2.643 | Accuracy: 0.215000 | 1.478 sec/iter\n",
      "Epoch: 11 | Batch: 005 / 011 | Total loss: 2.585 | Reg loss: 0.020 | Tree loss: 2.585 | Accuracy: 0.235500 | 1.481 sec/iter\n",
      "Epoch: 11 | Batch: 006 / 011 | Total loss: 2.560 | Reg loss: 0.020 | Tree loss: 2.560 | Accuracy: 0.232500 | 1.48 sec/iter\n",
      "Epoch: 11 | Batch: 007 / 011 | Total loss: 2.544 | Reg loss: 0.021 | Tree loss: 2.544 | Accuracy: 0.231500 | 1.479 sec/iter\n",
      "Epoch: 11 | Batch: 008 / 011 | Total loss: 2.515 | Reg loss: 0.021 | Tree loss: 2.515 | Accuracy: 0.210500 | 1.479 sec/iter\n",
      "Epoch: 11 | Batch: 009 / 011 | Total loss: 2.513 | Reg loss: 0.021 | Tree loss: 2.513 | Accuracy: 0.219500 | 1.481 sec/iter\n",
      "Epoch: 11 | Batch: 010 / 011 | Total loss: 2.523 | Reg loss: 0.021 | Tree loss: 2.523 | Accuracy: 0.211604 | 1.479 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 12 | Batch: 000 / 011 | Total loss: 2.766 | Reg loss: 0.020 | Tree loss: 2.766 | Accuracy: 0.238000 | 1.484 sec/iter\n",
      "Epoch: 12 | Batch: 001 / 011 | Total loss: 2.730 | Reg loss: 0.020 | Tree loss: 2.730 | Accuracy: 0.207500 | 1.485 sec/iter\n",
      "Epoch: 12 | Batch: 002 / 011 | Total loss: 2.681 | Reg loss: 0.020 | Tree loss: 2.681 | Accuracy: 0.231000 | 1.484 sec/iter\n",
      "Epoch: 12 | Batch: 003 / 011 | Total loss: 2.625 | Reg loss: 0.020 | Tree loss: 2.625 | Accuracy: 0.228000 | 1.483 sec/iter\n",
      "Epoch: 12 | Batch: 004 / 011 | Total loss: 2.580 | Reg loss: 0.021 | Tree loss: 2.580 | Accuracy: 0.227500 | 1.483 sec/iter\n",
      "Epoch: 12 | Batch: 005 / 011 | Total loss: 2.554 | Reg loss: 0.021 | Tree loss: 2.554 | Accuracy: 0.207000 | 1.481 sec/iter\n",
      "Epoch: 12 | Batch: 006 / 011 | Total loss: 2.510 | Reg loss: 0.021 | Tree loss: 2.510 | Accuracy: 0.233000 | 1.482 sec/iter\n",
      "Epoch: 12 | Batch: 007 / 011 | Total loss: 2.499 | Reg loss: 0.021 | Tree loss: 2.499 | Accuracy: 0.196000 | 1.482 sec/iter\n",
      "Epoch: 12 | Batch: 008 / 011 | Total loss: 2.472 | Reg loss: 0.021 | Tree loss: 2.472 | Accuracy: 0.223500 | 1.483 sec/iter\n",
      "Epoch: 12 | Batch: 009 / 011 | Total loss: 2.453 | Reg loss: 0.022 | Tree loss: 2.453 | Accuracy: 0.233500 | 1.481 sec/iter\n",
      "Epoch: 12 | Batch: 010 / 011 | Total loss: 2.445 | Reg loss: 0.022 | Tree loss: 2.445 | Accuracy: 0.218430 | 1.477 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 13 | Batch: 000 / 011 | Total loss: 2.727 | Reg loss: 0.021 | Tree loss: 2.727 | Accuracy: 0.227500 | 1.475 sec/iter\n",
      "Epoch: 13 | Batch: 001 / 011 | Total loss: 2.689 | Reg loss: 0.021 | Tree loss: 2.689 | Accuracy: 0.219500 | 1.473 sec/iter\n",
      "Epoch: 13 | Batch: 002 / 011 | Total loss: 2.635 | Reg loss: 0.021 | Tree loss: 2.635 | Accuracy: 0.218500 | 1.474 sec/iter\n",
      "Epoch: 13 | Batch: 003 / 011 | Total loss: 2.576 | Reg loss: 0.021 | Tree loss: 2.576 | Accuracy: 0.222500 | 1.474 sec/iter\n",
      "Epoch: 13 | Batch: 004 / 011 | Total loss: 2.534 | Reg loss: 0.021 | Tree loss: 2.534 | Accuracy: 0.252000 | 1.478 sec/iter\n",
      "Epoch: 13 | Batch: 005 / 011 | Total loss: 2.512 | Reg loss: 0.021 | Tree loss: 2.512 | Accuracy: 0.238500 | 1.478 sec/iter\n",
      "Epoch: 13 | Batch: 006 / 011 | Total loss: 2.466 | Reg loss: 0.022 | Tree loss: 2.466 | Accuracy: 0.247500 | 1.476 sec/iter\n",
      "Epoch: 13 | Batch: 007 / 011 | Total loss: 2.454 | Reg loss: 0.022 | Tree loss: 2.454 | Accuracy: 0.214500 | 1.473 sec/iter\n",
      "Epoch: 13 | Batch: 008 / 011 | Total loss: 2.426 | Reg loss: 0.022 | Tree loss: 2.426 | Accuracy: 0.224000 | 1.472 sec/iter\n",
      "Epoch: 13 | Batch: 009 / 011 | Total loss: 2.404 | Reg loss: 0.022 | Tree loss: 2.404 | Accuracy: 0.254500 | 1.47 sec/iter\n",
      "Epoch: 13 | Batch: 010 / 011 | Total loss: 2.395 | Reg loss: 0.022 | Tree loss: 2.395 | Accuracy: 0.269625 | 1.47 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 14 | Batch: 000 / 011 | Total loss: 2.691 | Reg loss: 0.021 | Tree loss: 2.691 | Accuracy: 0.209000 | 1.476 sec/iter\n",
      "Epoch: 14 | Batch: 001 / 011 | Total loss: 2.631 | Reg loss: 0.022 | Tree loss: 2.631 | Accuracy: 0.228000 | 1.479 sec/iter\n",
      "Epoch: 14 | Batch: 002 / 011 | Total loss: 2.584 | Reg loss: 0.022 | Tree loss: 2.584 | Accuracy: 0.226500 | 1.48 sec/iter\n",
      "Epoch: 14 | Batch: 003 / 011 | Total loss: 2.526 | Reg loss: 0.022 | Tree loss: 2.526 | Accuracy: 0.254000 | 1.478 sec/iter\n",
      "Epoch: 14 | Batch: 004 / 011 | Total loss: 2.511 | Reg loss: 0.022 | Tree loss: 2.511 | Accuracy: 0.228500 | 1.476 sec/iter\n",
      "Epoch: 14 | Batch: 005 / 011 | Total loss: 2.466 | Reg loss: 0.022 | Tree loss: 2.466 | Accuracy: 0.249500 | 1.476 sec/iter\n",
      "Epoch: 14 | Batch: 006 / 011 | Total loss: 2.426 | Reg loss: 0.022 | Tree loss: 2.426 | Accuracy: 0.239000 | 1.475 sec/iter\n",
      "Epoch: 14 | Batch: 007 / 011 | Total loss: 2.400 | Reg loss: 0.022 | Tree loss: 2.400 | Accuracy: 0.239000 | 1.474 sec/iter\n",
      "Epoch: 14 | Batch: 008 / 011 | Total loss: 2.393 | Reg loss: 0.023 | Tree loss: 2.393 | Accuracy: 0.262500 | 1.475 sec/iter\n",
      "Epoch: 14 | Batch: 009 / 011 | Total loss: 2.383 | Reg loss: 0.023 | Tree loss: 2.383 | Accuracy: 0.260500 | 1.476 sec/iter\n",
      "Epoch: 14 | Batch: 010 / 011 | Total loss: 2.363 | Reg loss: 0.023 | Tree loss: 2.363 | Accuracy: 0.269625 | 1.475 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 15 | Batch: 000 / 011 | Total loss: 2.638 | Reg loss: 0.022 | Tree loss: 2.638 | Accuracy: 0.223000 | 1.474 sec/iter\n",
      "Epoch: 15 | Batch: 001 / 011 | Total loss: 2.589 | Reg loss: 0.022 | Tree loss: 2.589 | Accuracy: 0.236000 | 1.471 sec/iter\n",
      "Epoch: 15 | Batch: 002 / 011 | Total loss: 2.557 | Reg loss: 0.022 | Tree loss: 2.557 | Accuracy: 0.226000 | 1.468 sec/iter\n",
      "Epoch: 15 | Batch: 003 / 011 | Total loss: 2.506 | Reg loss: 0.022 | Tree loss: 2.506 | Accuracy: 0.229500 | 1.466 sec/iter\n",
      "Epoch: 15 | Batch: 004 / 011 | Total loss: 2.468 | Reg loss: 0.022 | Tree loss: 2.468 | Accuracy: 0.234000 | 1.47 sec/iter\n",
      "Epoch: 15 | Batch: 005 / 011 | Total loss: 2.420 | Reg loss: 0.023 | Tree loss: 2.420 | Accuracy: 0.232000 | 1.472 sec/iter\n",
      "Epoch: 15 | Batch: 006 / 011 | Total loss: 2.389 | Reg loss: 0.023 | Tree loss: 2.389 | Accuracy: 0.252500 | 1.472 sec/iter\n",
      "Epoch: 15 | Batch: 007 / 011 | Total loss: 2.375 | Reg loss: 0.023 | Tree loss: 2.375 | Accuracy: 0.232500 | 1.472 sec/iter\n",
      "Epoch: 15 | Batch: 008 / 011 | Total loss: 2.354 | Reg loss: 0.023 | Tree loss: 2.354 | Accuracy: 0.243500 | 1.47 sec/iter\n",
      "Epoch: 15 | Batch: 009 / 011 | Total loss: 2.314 | Reg loss: 0.023 | Tree loss: 2.314 | Accuracy: 0.275500 | 1.467 sec/iter\n",
      "Epoch: 15 | Batch: 010 / 011 | Total loss: 2.300 | Reg loss: 0.024 | Tree loss: 2.300 | Accuracy: 0.276451 | 1.465 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 16 | Batch: 000 / 011 | Total loss: 2.590 | Reg loss: 0.023 | Tree loss: 2.590 | Accuracy: 0.227500 | 1.466 sec/iter\n",
      "Epoch: 16 | Batch: 001 / 011 | Total loss: 2.549 | Reg loss: 0.023 | Tree loss: 2.549 | Accuracy: 0.227000 | 1.466 sec/iter\n",
      "Epoch: 16 | Batch: 002 / 011 | Total loss: 2.511 | Reg loss: 0.023 | Tree loss: 2.511 | Accuracy: 0.237000 | 1.468 sec/iter\n",
      "Epoch: 16 | Batch: 003 / 011 | Total loss: 2.466 | Reg loss: 0.023 | Tree loss: 2.466 | Accuracy: 0.240000 | 1.467 sec/iter\n",
      "Epoch: 16 | Batch: 004 / 011 | Total loss: 2.429 | Reg loss: 0.023 | Tree loss: 2.429 | Accuracy: 0.241500 | 1.466 sec/iter\n",
      "Epoch: 16 | Batch: 005 / 011 | Total loss: 2.386 | Reg loss: 0.023 | Tree loss: 2.386 | Accuracy: 0.251000 | 1.463 sec/iter\n",
      "Epoch: 16 | Batch: 006 / 011 | Total loss: 2.356 | Reg loss: 0.023 | Tree loss: 2.356 | Accuracy: 0.232500 | 1.462 sec/iter\n",
      "Epoch: 16 | Batch: 007 / 011 | Total loss: 2.326 | Reg loss: 0.024 | Tree loss: 2.326 | Accuracy: 0.260000 | 1.461 sec/iter\n",
      "Epoch: 16 | Batch: 008 / 011 | Total loss: 2.296 | Reg loss: 0.024 | Tree loss: 2.296 | Accuracy: 0.272000 | 1.462 sec/iter\n",
      "Epoch: 16 | Batch: 009 / 011 | Total loss: 2.294 | Reg loss: 0.024 | Tree loss: 2.294 | Accuracy: 0.279500 | 1.463 sec/iter\n",
      "Epoch: 16 | Batch: 010 / 011 | Total loss: 2.272 | Reg loss: 0.024 | Tree loss: 2.272 | Accuracy: 0.266212 | 1.463 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 17 | Batch: 000 / 011 | Total loss: 2.563 | Reg loss: 0.023 | Tree loss: 2.563 | Accuracy: 0.223000 | 1.463 sec/iter\n",
      "Epoch: 17 | Batch: 001 / 011 | Total loss: 2.508 | Reg loss: 0.023 | Tree loss: 2.508 | Accuracy: 0.235000 | 1.463 sec/iter\n",
      "Epoch: 17 | Batch: 002 / 011 | Total loss: 2.473 | Reg loss: 0.023 | Tree loss: 2.473 | Accuracy: 0.231500 | 1.461 sec/iter\n",
      "Epoch: 17 | Batch: 003 / 011 | Total loss: 2.418 | Reg loss: 0.024 | Tree loss: 2.418 | Accuracy: 0.259500 | 1.458 sec/iter\n",
      "Epoch: 17 | Batch: 004 / 011 | Total loss: 2.389 | Reg loss: 0.024 | Tree loss: 2.389 | Accuracy: 0.247000 | 1.456 sec/iter\n",
      "Epoch: 17 | Batch: 005 / 011 | Total loss: 2.352 | Reg loss: 0.024 | Tree loss: 2.352 | Accuracy: 0.253500 | 1.455 sec/iter\n",
      "Epoch: 17 | Batch: 006 / 011 | Total loss: 2.305 | Reg loss: 0.024 | Tree loss: 2.305 | Accuracy: 0.259500 | 1.454 sec/iter\n",
      "Epoch: 17 | Batch: 007 / 011 | Total loss: 2.289 | Reg loss: 0.024 | Tree loss: 2.289 | Accuracy: 0.244000 | 1.452 sec/iter\n",
      "Epoch: 17 | Batch: 008 / 011 | Total loss: 2.259 | Reg loss: 0.024 | Tree loss: 2.259 | Accuracy: 0.273000 | 1.454 sec/iter\n",
      "Epoch: 17 | Batch: 009 / 011 | Total loss: 2.267 | Reg loss: 0.025 | Tree loss: 2.267 | Accuracy: 0.288000 | 1.455 sec/iter\n",
      "Epoch: 17 | Batch: 010 / 011 | Total loss: 2.251 | Reg loss: 0.025 | Tree loss: 2.251 | Accuracy: 0.266212 | 1.457 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 18 | Batch: 000 / 011 | Total loss: 2.519 | Reg loss: 0.024 | Tree loss: 2.519 | Accuracy: 0.237000 | 1.459 sec/iter\n",
      "Epoch: 18 | Batch: 001 / 011 | Total loss: 2.494 | Reg loss: 0.024 | Tree loss: 2.494 | Accuracy: 0.222000 | 1.457 sec/iter\n",
      "Epoch: 18 | Batch: 002 / 011 | Total loss: 2.429 | Reg loss: 0.024 | Tree loss: 2.429 | Accuracy: 0.239500 | 1.455 sec/iter\n",
      "Epoch: 18 | Batch: 003 / 011 | Total loss: 2.389 | Reg loss: 0.024 | Tree loss: 2.389 | Accuracy: 0.252500 | 1.453 sec/iter\n",
      "Epoch: 18 | Batch: 004 / 011 | Total loss: 2.335 | Reg loss: 0.024 | Tree loss: 2.335 | Accuracy: 0.267500 | 1.451 sec/iter\n",
      "Epoch: 18 | Batch: 005 / 011 | Total loss: 2.315 | Reg loss: 0.024 | Tree loss: 2.315 | Accuracy: 0.246000 | 1.451 sec/iter\n",
      "Epoch: 18 | Batch: 006 / 011 | Total loss: 2.282 | Reg loss: 0.025 | Tree loss: 2.282 | Accuracy: 0.260000 | 1.451 sec/iter\n",
      "Epoch: 18 | Batch: 007 / 011 | Total loss: 2.241 | Reg loss: 0.025 | Tree loss: 2.241 | Accuracy: 0.272500 | 1.452 sec/iter\n",
      "Epoch: 18 | Batch: 008 / 011 | Total loss: 2.233 | Reg loss: 0.025 | Tree loss: 2.233 | Accuracy: 0.278500 | 1.453 sec/iter\n",
      "Epoch: 18 | Batch: 009 / 011 | Total loss: 2.217 | Reg loss: 0.025 | Tree loss: 2.217 | Accuracy: 0.292500 | 1.453 sec/iter\n",
      "Epoch: 18 | Batch: 010 / 011 | Total loss: 2.189 | Reg loss: 0.025 | Tree loss: 2.189 | Accuracy: 0.283276 | 1.451 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 19 | Batch: 000 / 011 | Total loss: 2.489 | Reg loss: 0.024 | Tree loss: 2.489 | Accuracy: 0.231000 | 1.45 sec/iter\n",
      "Epoch: 19 | Batch: 001 / 011 | Total loss: 2.457 | Reg loss: 0.025 | Tree loss: 2.457 | Accuracy: 0.238000 | 1.453 sec/iter\n",
      "Epoch: 19 | Batch: 002 / 011 | Total loss: 2.413 | Reg loss: 0.025 | Tree loss: 2.413 | Accuracy: 0.243500 | 1.453 sec/iter\n",
      "Epoch: 19 | Batch: 003 / 011 | Total loss: 2.368 | Reg loss: 0.025 | Tree loss: 2.368 | Accuracy: 0.244500 | 1.455 sec/iter\n",
      "Epoch: 19 | Batch: 004 / 011 | Total loss: 2.307 | Reg loss: 0.025 | Tree loss: 2.307 | Accuracy: 0.254000 | 1.454 sec/iter\n",
      "Epoch: 19 | Batch: 005 / 011 | Total loss: 2.272 | Reg loss: 0.025 | Tree loss: 2.272 | Accuracy: 0.263000 | 1.455 sec/iter\n",
      "Epoch: 19 | Batch: 006 / 011 | Total loss: 2.218 | Reg loss: 0.025 | Tree loss: 2.218 | Accuracy: 0.286000 | 1.454 sec/iter\n",
      "Epoch: 19 | Batch: 007 / 011 | Total loss: 2.192 | Reg loss: 0.025 | Tree loss: 2.192 | Accuracy: 0.282000 | 1.452 sec/iter\n",
      "Epoch: 19 | Batch: 008 / 011 | Total loss: 2.180 | Reg loss: 0.025 | Tree loss: 2.180 | Accuracy: 0.292500 | 1.451 sec/iter\n",
      "Epoch: 19 | Batch: 009 / 011 | Total loss: 2.177 | Reg loss: 0.026 | Tree loss: 2.177 | Accuracy: 0.336500 | 1.449 sec/iter\n",
      "Epoch: 19 | Batch: 010 / 011 | Total loss: 2.195 | Reg loss: 0.026 | Tree loss: 2.195 | Accuracy: 0.290102 | 1.451 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 20 | Batch: 000 / 011 | Total loss: 2.456 | Reg loss: 0.025 | Tree loss: 2.456 | Accuracy: 0.231500 | 1.453 sec/iter\n",
      "Epoch: 20 | Batch: 001 / 011 | Total loss: 2.412 | Reg loss: 0.025 | Tree loss: 2.412 | Accuracy: 0.241500 | 1.454 sec/iter\n",
      "Epoch: 20 | Batch: 002 / 011 | Total loss: 2.361 | Reg loss: 0.025 | Tree loss: 2.361 | Accuracy: 0.238000 | 1.454 sec/iter\n",
      "Epoch: 20 | Batch: 003 / 011 | Total loss: 2.324 | Reg loss: 0.025 | Tree loss: 2.324 | Accuracy: 0.254500 | 1.452 sec/iter\n",
      "Epoch: 20 | Batch: 004 / 011 | Total loss: 2.285 | Reg loss: 0.025 | Tree loss: 2.285 | Accuracy: 0.259000 | 1.452 sec/iter\n",
      "Epoch: 20 | Batch: 005 / 011 | Total loss: 2.245 | Reg loss: 0.026 | Tree loss: 2.245 | Accuracy: 0.258500 | 1.45 sec/iter\n",
      "Epoch: 20 | Batch: 006 / 011 | Total loss: 2.187 | Reg loss: 0.026 | Tree loss: 2.187 | Accuracy: 0.307000 | 1.449 sec/iter\n",
      "Epoch: 20 | Batch: 007 / 011 | Total loss: 2.164 | Reg loss: 0.026 | Tree loss: 2.164 | Accuracy: 0.303500 | 1.448 sec/iter\n",
      "Epoch: 20 | Batch: 008 / 011 | Total loss: 2.162 | Reg loss: 0.026 | Tree loss: 2.162 | Accuracy: 0.317500 | 1.45 sec/iter\n",
      "Epoch: 20 | Batch: 009 / 011 | Total loss: 2.149 | Reg loss: 0.026 | Tree loss: 2.149 | Accuracy: 0.318000 | 1.451 sec/iter\n",
      "Epoch: 20 | Batch: 010 / 011 | Total loss: 2.105 | Reg loss: 0.026 | Tree loss: 2.105 | Accuracy: 0.354949 | 1.453 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 21 | Batch: 000 / 011 | Total loss: 2.432 | Reg loss: 0.026 | Tree loss: 2.432 | Accuracy: 0.225500 | 1.453 sec/iter\n",
      "Epoch: 21 | Batch: 001 / 011 | Total loss: 2.386 | Reg loss: 0.026 | Tree loss: 2.386 | Accuracy: 0.241000 | 1.452 sec/iter\n",
      "Epoch: 21 | Batch: 002 / 011 | Total loss: 2.343 | Reg loss: 0.026 | Tree loss: 2.343 | Accuracy: 0.242500 | 1.451 sec/iter\n",
      "Epoch: 21 | Batch: 003 / 011 | Total loss: 2.295 | Reg loss: 0.026 | Tree loss: 2.295 | Accuracy: 0.254000 | 1.451 sec/iter\n",
      "Epoch: 21 | Batch: 004 / 011 | Total loss: 2.243 | Reg loss: 0.026 | Tree loss: 2.243 | Accuracy: 0.261500 | 1.453 sec/iter\n",
      "Epoch: 21 | Batch: 005 / 011 | Total loss: 2.195 | Reg loss: 0.026 | Tree loss: 2.195 | Accuracy: 0.293000 | 1.453 sec/iter\n",
      "Epoch: 21 | Batch: 006 / 011 | Total loss: 2.147 | Reg loss: 0.026 | Tree loss: 2.147 | Accuracy: 0.335000 | 1.453 sec/iter\n",
      "Epoch: 21 | Batch: 007 / 011 | Total loss: 2.143 | Reg loss: 0.026 | Tree loss: 2.143 | Accuracy: 0.334500 | 1.452 sec/iter\n",
      "Epoch: 21 | Batch: 008 / 011 | Total loss: 2.109 | Reg loss: 0.027 | Tree loss: 2.109 | Accuracy: 0.355500 | 1.451 sec/iter\n",
      "Epoch: 21 | Batch: 009 / 011 | Total loss: 2.101 | Reg loss: 0.027 | Tree loss: 2.101 | Accuracy: 0.340000 | 1.451 sec/iter\n",
      "Epoch: 21 | Batch: 010 / 011 | Total loss: 2.088 | Reg loss: 0.027 | Tree loss: 2.088 | Accuracy: 0.358362 | 1.45 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 22 | Batch: 000 / 011 | Total loss: 2.390 | Reg loss: 0.026 | Tree loss: 2.390 | Accuracy: 0.247500 | 1.449 sec/iter\n",
      "Epoch: 22 | Batch: 001 / 011 | Total loss: 2.357 | Reg loss: 0.026 | Tree loss: 2.357 | Accuracy: 0.227500 | 1.448 sec/iter\n",
      "Epoch: 22 | Batch: 002 / 011 | Total loss: 2.319 | Reg loss: 0.026 | Tree loss: 2.319 | Accuracy: 0.235000 | 1.449 sec/iter\n",
      "Epoch: 22 | Batch: 003 / 011 | Total loss: 2.245 | Reg loss: 0.026 | Tree loss: 2.245 | Accuracy: 0.282000 | 1.449 sec/iter\n",
      "Epoch: 22 | Batch: 004 / 011 | Total loss: 2.211 | Reg loss: 0.026 | Tree loss: 2.211 | Accuracy: 0.281500 | 1.451 sec/iter\n",
      "Epoch: 22 | Batch: 005 / 011 | Total loss: 2.153 | Reg loss: 0.027 | Tree loss: 2.153 | Accuracy: 0.318000 | 1.449 sec/iter\n",
      "Epoch: 22 | Batch: 006 / 011 | Total loss: 2.138 | Reg loss: 0.027 | Tree loss: 2.138 | Accuracy: 0.316000 | 1.448 sec/iter\n",
      "Epoch: 22 | Batch: 007 / 011 | Total loss: 2.099 | Reg loss: 0.027 | Tree loss: 2.099 | Accuracy: 0.316500 | 1.446 sec/iter\n",
      "Epoch: 22 | Batch: 008 / 011 | Total loss: 2.090 | Reg loss: 0.027 | Tree loss: 2.090 | Accuracy: 0.324500 | 1.445 sec/iter\n",
      "Epoch: 22 | Batch: 009 / 011 | Total loss: 2.078 | Reg loss: 0.027 | Tree loss: 2.078 | Accuracy: 0.333500 | 1.446 sec/iter\n",
      "Epoch: 22 | Batch: 010 / 011 | Total loss: 2.044 | Reg loss: 0.027 | Tree loss: 2.044 | Accuracy: 0.378840 | 1.448 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 23 | Batch: 000 / 011 | Total loss: 2.367 | Reg loss: 0.027 | Tree loss: 2.367 | Accuracy: 0.237500 | 1.45 sec/iter\n",
      "Epoch: 23 | Batch: 001 / 011 | Total loss: 2.319 | Reg loss: 0.027 | Tree loss: 2.319 | Accuracy: 0.249000 | 1.449 sec/iter\n",
      "Epoch: 23 | Batch: 002 / 011 | Total loss: 2.272 | Reg loss: 0.027 | Tree loss: 2.272 | Accuracy: 0.254500 | 1.449 sec/iter\n",
      "Epoch: 23 | Batch: 003 / 011 | Total loss: 2.248 | Reg loss: 0.027 | Tree loss: 2.248 | Accuracy: 0.262500 | 1.448 sec/iter\n",
      "Epoch: 23 | Batch: 004 / 011 | Total loss: 2.170 | Reg loss: 0.027 | Tree loss: 2.170 | Accuracy: 0.284500 | 1.449 sec/iter\n",
      "Epoch: 23 | Batch: 005 / 011 | Total loss: 2.132 | Reg loss: 0.027 | Tree loss: 2.132 | Accuracy: 0.318000 | 1.449 sec/iter\n",
      "Epoch: 23 | Batch: 006 / 011 | Total loss: 2.097 | Reg loss: 0.027 | Tree loss: 2.097 | Accuracy: 0.346000 | 1.45 sec/iter\n",
      "Epoch: 23 | Batch: 007 / 011 | Total loss: 2.050 | Reg loss: 0.027 | Tree loss: 2.050 | Accuracy: 0.348500 | 1.45 sec/iter\n",
      "Epoch: 23 | Batch: 008 / 011 | Total loss: 2.060 | Reg loss: 0.028 | Tree loss: 2.060 | Accuracy: 0.340500 | 1.45 sec/iter\n",
      "Epoch: 23 | Batch: 009 / 011 | Total loss: 2.037 | Reg loss: 0.028 | Tree loss: 2.037 | Accuracy: 0.342500 | 1.449 sec/iter\n",
      "Epoch: 23 | Batch: 010 / 011 | Total loss: 2.037 | Reg loss: 0.028 | Tree loss: 2.037 | Accuracy: 0.344710 | 1.447 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 24 | Batch: 000 / 011 | Total loss: 2.340 | Reg loss: 0.027 | Tree loss: 2.340 | Accuracy: 0.242000 | 1.449 sec/iter\n",
      "Epoch: 24 | Batch: 001 / 011 | Total loss: 2.295 | Reg loss: 0.027 | Tree loss: 2.295 | Accuracy: 0.268000 | 1.45 sec/iter\n",
      "Epoch: 24 | Batch: 002 / 011 | Total loss: 2.235 | Reg loss: 0.027 | Tree loss: 2.235 | Accuracy: 0.258500 | 1.451 sec/iter\n",
      "Epoch: 24 | Batch: 003 / 011 | Total loss: 2.184 | Reg loss: 0.027 | Tree loss: 2.184 | Accuracy: 0.306000 | 1.452 sec/iter\n",
      "Epoch: 24 | Batch: 004 / 011 | Total loss: 2.135 | Reg loss: 0.028 | Tree loss: 2.135 | Accuracy: 0.308000 | 1.456 sec/iter\n",
      "Epoch: 24 | Batch: 005 / 011 | Total loss: 2.101 | Reg loss: 0.028 | Tree loss: 2.101 | Accuracy: 0.350000 | 1.457 sec/iter\n",
      "Epoch: 24 | Batch: 006 / 011 | Total loss: 2.070 | Reg loss: 0.028 | Tree loss: 2.070 | Accuracy: 0.355000 | 1.458 sec/iter\n",
      "Epoch: 24 | Batch: 007 / 011 | Total loss: 2.047 | Reg loss: 0.028 | Tree loss: 2.047 | Accuracy: 0.340000 | 1.458 sec/iter\n",
      "Epoch: 24 | Batch: 008 / 011 | Total loss: 2.009 | Reg loss: 0.028 | Tree loss: 2.009 | Accuracy: 0.351000 | 1.459 sec/iter\n",
      "Epoch: 24 | Batch: 009 / 011 | Total loss: 2.001 | Reg loss: 0.028 | Tree loss: 2.001 | Accuracy: 0.348000 | 1.462 sec/iter\n",
      "Epoch: 24 | Batch: 010 / 011 | Total loss: 1.999 | Reg loss: 0.028 | Tree loss: 1.999 | Accuracy: 0.372014 | 1.465 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 25 | Batch: 000 / 011 | Total loss: 2.320 | Reg loss: 0.028 | Tree loss: 2.320 | Accuracy: 0.230000 | 1.468 sec/iter\n",
      "Epoch: 25 | Batch: 001 / 011 | Total loss: 2.280 | Reg loss: 0.028 | Tree loss: 2.280 | Accuracy: 0.263500 | 1.468 sec/iter\n",
      "Epoch: 25 | Batch: 002 / 011 | Total loss: 2.196 | Reg loss: 0.028 | Tree loss: 2.196 | Accuracy: 0.287000 | 1.469 sec/iter\n",
      "Epoch: 25 | Batch: 003 / 011 | Total loss: 2.169 | Reg loss: 0.028 | Tree loss: 2.169 | Accuracy: 0.283000 | 1.467 sec/iter\n",
      "Epoch: 25 | Batch: 004 / 011 | Total loss: 2.101 | Reg loss: 0.028 | Tree loss: 2.101 | Accuracy: 0.324500 | 1.468 sec/iter\n",
      "Epoch: 25 | Batch: 005 / 011 | Total loss: 2.073 | Reg loss: 0.028 | Tree loss: 2.073 | Accuracy: 0.352000 | 1.471 sec/iter\n",
      "Epoch: 25 | Batch: 006 / 011 | Total loss: 2.028 | Reg loss: 0.028 | Tree loss: 2.028 | Accuracy: 0.365000 | 1.472 sec/iter\n",
      "Epoch: 25 | Batch: 007 / 011 | Total loss: 1.999 | Reg loss: 0.028 | Tree loss: 1.999 | Accuracy: 0.390500 | 1.475 sec/iter\n",
      "Epoch: 25 | Batch: 008 / 011 | Total loss: 1.995 | Reg loss: 0.028 | Tree loss: 1.995 | Accuracy: 0.353000 | 1.478 sec/iter\n",
      "Epoch: 25 | Batch: 009 / 011 | Total loss: 1.993 | Reg loss: 0.029 | Tree loss: 1.993 | Accuracy: 0.322500 | 1.478 sec/iter\n",
      "Epoch: 25 | Batch: 010 / 011 | Total loss: 1.937 | Reg loss: 0.029 | Tree loss: 1.937 | Accuracy: 0.392491 | 1.479 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 26 | Batch: 000 / 011 | Total loss: 2.271 | Reg loss: 0.028 | Tree loss: 2.271 | Accuracy: 0.249500 | 1.482 sec/iter\n",
      "Epoch: 26 | Batch: 001 / 011 | Total loss: 2.226 | Reg loss: 0.028 | Tree loss: 2.226 | Accuracy: 0.266500 | 1.483 sec/iter\n",
      "Epoch: 26 | Batch: 002 / 011 | Total loss: 2.175 | Reg loss: 0.028 | Tree loss: 2.175 | Accuracy: 0.288500 | 1.484 sec/iter\n",
      "Epoch: 26 | Batch: 003 / 011 | Total loss: 2.133 | Reg loss: 0.028 | Tree loss: 2.133 | Accuracy: 0.317500 | 1.486 sec/iter\n",
      "Epoch: 26 | Batch: 004 / 011 | Total loss: 2.093 | Reg loss: 0.028 | Tree loss: 2.093 | Accuracy: 0.336500 | 1.486 sec/iter\n",
      "Epoch: 26 | Batch: 005 / 011 | Total loss: 2.040 | Reg loss: 0.029 | Tree loss: 2.040 | Accuracy: 0.357500 | 1.486 sec/iter\n",
      "Epoch: 26 | Batch: 006 / 011 | Total loss: 2.016 | Reg loss: 0.029 | Tree loss: 2.016 | Accuracy: 0.356500 | 1.488 sec/iter\n",
      "Epoch: 26 | Batch: 007 / 011 | Total loss: 1.983 | Reg loss: 0.029 | Tree loss: 1.983 | Accuracy: 0.381500 | 1.488 sec/iter\n",
      "Epoch: 26 | Batch: 008 / 011 | Total loss: 1.953 | Reg loss: 0.029 | Tree loss: 1.953 | Accuracy: 0.358000 | 1.489 sec/iter\n",
      "Epoch: 26 | Batch: 009 / 011 | Total loss: 1.965 | Reg loss: 0.029 | Tree loss: 1.965 | Accuracy: 0.362000 | 1.492 sec/iter\n",
      "Epoch: 26 | Batch: 010 / 011 | Total loss: 1.950 | Reg loss: 0.029 | Tree loss: 1.950 | Accuracy: 0.324232 | 1.495 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 27 | Batch: 000 / 011 | Total loss: 2.244 | Reg loss: 0.029 | Tree loss: 2.244 | Accuracy: 0.252000 | 1.497 sec/iter\n",
      "Epoch: 27 | Batch: 001 / 011 | Total loss: 2.218 | Reg loss: 0.029 | Tree loss: 2.218 | Accuracy: 0.266000 | 1.497 sec/iter\n",
      "Epoch: 27 | Batch: 002 / 011 | Total loss: 2.172 | Reg loss: 0.029 | Tree loss: 2.172 | Accuracy: 0.284000 | 1.498 sec/iter\n",
      "Epoch: 27 | Batch: 003 / 011 | Total loss: 2.115 | Reg loss: 0.029 | Tree loss: 2.115 | Accuracy: 0.325500 | 1.497 sec/iter\n",
      "Epoch: 27 | Batch: 004 / 011 | Total loss: 2.062 | Reg loss: 0.029 | Tree loss: 2.062 | Accuracy: 0.341500 | 1.5 sec/iter\n",
      "Epoch: 27 | Batch: 005 / 011 | Total loss: 2.011 | Reg loss: 0.029 | Tree loss: 2.011 | Accuracy: 0.389000 | 1.504 sec/iter\n",
      "Epoch: 27 | Batch: 006 / 011 | Total loss: 1.974 | Reg loss: 0.029 | Tree loss: 1.974 | Accuracy: 0.363000 | 1.507 sec/iter\n",
      "Epoch: 27 | Batch: 007 / 011 | Total loss: 1.961 | Reg loss: 0.029 | Tree loss: 1.961 | Accuracy: 0.362000 | 1.507 sec/iter\n",
      "Epoch: 27 | Batch: 008 / 011 | Total loss: 1.935 | Reg loss: 0.029 | Tree loss: 1.935 | Accuracy: 0.359000 | 1.509 sec/iter\n",
      "Epoch: 27 | Batch: 009 / 011 | Total loss: 1.920 | Reg loss: 0.029 | Tree loss: 1.920 | Accuracy: 0.375000 | 1.511 sec/iter\n",
      "Epoch: 27 | Batch: 010 / 011 | Total loss: 1.946 | Reg loss: 0.030 | Tree loss: 1.946 | Accuracy: 0.337884 | 1.513 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 28 | Batch: 000 / 011 | Total loss: 2.230 | Reg loss: 0.029 | Tree loss: 2.230 | Accuracy: 0.264500 | 1.517 sec/iter\n",
      "Epoch: 28 | Batch: 001 / 011 | Total loss: 2.175 | Reg loss: 0.029 | Tree loss: 2.175 | Accuracy: 0.291000 | 1.518 sec/iter\n",
      "Epoch: 28 | Batch: 002 / 011 | Total loss: 2.158 | Reg loss: 0.029 | Tree loss: 2.158 | Accuracy: 0.300000 | 1.519 sec/iter\n",
      "Epoch: 28 | Batch: 003 / 011 | Total loss: 2.079 | Reg loss: 0.029 | Tree loss: 2.079 | Accuracy: 0.341500 | 1.52 sec/iter\n",
      "Epoch: 28 | Batch: 004 / 011 | Total loss: 2.037 | Reg loss: 0.029 | Tree loss: 2.037 | Accuracy: 0.337500 | 1.52 sec/iter\n",
      "Epoch: 28 | Batch: 005 / 011 | Total loss: 2.001 | Reg loss: 0.029 | Tree loss: 2.001 | Accuracy: 0.343500 | 1.52 sec/iter\n",
      "Epoch: 28 | Batch: 006 / 011 | Total loss: 1.953 | Reg loss: 0.029 | Tree loss: 1.953 | Accuracy: 0.395000 | 1.52 sec/iter\n",
      "Epoch: 28 | Batch: 007 / 011 | Total loss: 1.931 | Reg loss: 0.030 | Tree loss: 1.931 | Accuracy: 0.368500 | 1.521 sec/iter\n",
      "Epoch: 28 | Batch: 008 / 011 | Total loss: 1.921 | Reg loss: 0.030 | Tree loss: 1.921 | Accuracy: 0.379000 | 1.522 sec/iter\n",
      "Epoch: 28 | Batch: 009 / 011 | Total loss: 1.905 | Reg loss: 0.030 | Tree loss: 1.905 | Accuracy: 0.357500 | 1.523 sec/iter\n",
      "Epoch: 28 | Batch: 010 / 011 | Total loss: 1.880 | Reg loss: 0.030 | Tree loss: 1.880 | Accuracy: 0.385666 | 1.522 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 29 | Batch: 000 / 011 | Total loss: 2.205 | Reg loss: 0.029 | Tree loss: 2.205 | Accuracy: 0.288500 | 1.522 sec/iter\n",
      "Epoch: 29 | Batch: 001 / 011 | Total loss: 2.175 | Reg loss: 0.030 | Tree loss: 2.175 | Accuracy: 0.307500 | 1.521 sec/iter\n",
      "Epoch: 29 | Batch: 002 / 011 | Total loss: 2.118 | Reg loss: 0.030 | Tree loss: 2.118 | Accuracy: 0.312000 | 1.521 sec/iter\n",
      "Epoch: 29 | Batch: 003 / 011 | Total loss: 2.065 | Reg loss: 0.030 | Tree loss: 2.065 | Accuracy: 0.320500 | 1.521 sec/iter\n",
      "Epoch: 29 | Batch: 004 / 011 | Total loss: 2.002 | Reg loss: 0.030 | Tree loss: 2.002 | Accuracy: 0.354500 | 1.521 sec/iter\n",
      "Epoch: 29 | Batch: 005 / 011 | Total loss: 1.950 | Reg loss: 0.030 | Tree loss: 1.950 | Accuracy: 0.390000 | 1.522 sec/iter\n",
      "Epoch: 29 | Batch: 006 / 011 | Total loss: 1.932 | Reg loss: 0.030 | Tree loss: 1.932 | Accuracy: 0.404000 | 1.523 sec/iter\n",
      "Epoch: 29 | Batch: 007 / 011 | Total loss: 1.911 | Reg loss: 0.030 | Tree loss: 1.911 | Accuracy: 0.360500 | 1.523 sec/iter\n",
      "Epoch: 29 | Batch: 008 / 011 | Total loss: 1.899 | Reg loss: 0.030 | Tree loss: 1.899 | Accuracy: 0.361500 | 1.524 sec/iter\n",
      "Epoch: 29 | Batch: 009 / 011 | Total loss: 1.884 | Reg loss: 0.030 | Tree loss: 1.884 | Accuracy: 0.380500 | 1.523 sec/iter\n",
      "Epoch: 29 | Batch: 010 / 011 | Total loss: 1.898 | Reg loss: 0.030 | Tree loss: 1.898 | Accuracy: 0.351536 | 1.521 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 30 | Batch: 000 / 011 | Total loss: 2.193 | Reg loss: 0.030 | Tree loss: 2.193 | Accuracy: 0.311500 | 1.521 sec/iter\n",
      "Epoch: 30 | Batch: 001 / 011 | Total loss: 2.138 | Reg loss: 0.030 | Tree loss: 2.138 | Accuracy: 0.314000 | 1.52 sec/iter\n",
      "Epoch: 30 | Batch: 002 / 011 | Total loss: 2.089 | Reg loss: 0.030 | Tree loss: 2.089 | Accuracy: 0.318500 | 1.519 sec/iter\n",
      "Epoch: 30 | Batch: 003 / 011 | Total loss: 2.039 | Reg loss: 0.030 | Tree loss: 2.039 | Accuracy: 0.336500 | 1.518 sec/iter\n",
      "Epoch: 30 | Batch: 004 / 011 | Total loss: 2.003 | Reg loss: 0.030 | Tree loss: 2.003 | Accuracy: 0.350500 | 1.517 sec/iter\n",
      "Epoch: 30 | Batch: 005 / 011 | Total loss: 1.933 | Reg loss: 0.030 | Tree loss: 1.933 | Accuracy: 0.387000 | 1.516 sec/iter\n",
      "Epoch: 30 | Batch: 006 / 011 | Total loss: 1.915 | Reg loss: 0.030 | Tree loss: 1.915 | Accuracy: 0.390000 | 1.515 sec/iter\n",
      "Epoch: 30 | Batch: 007 / 011 | Total loss: 1.888 | Reg loss: 0.030 | Tree loss: 1.888 | Accuracy: 0.370500 | 1.514 sec/iter\n",
      "Epoch: 30 | Batch: 008 / 011 | Total loss: 1.888 | Reg loss: 0.030 | Tree loss: 1.888 | Accuracy: 0.367000 | 1.513 sec/iter\n",
      "Epoch: 30 | Batch: 009 / 011 | Total loss: 1.864 | Reg loss: 0.031 | Tree loss: 1.864 | Accuracy: 0.392500 | 1.513 sec/iter\n",
      "Epoch: 30 | Batch: 010 / 011 | Total loss: 1.864 | Reg loss: 0.031 | Tree loss: 1.864 | Accuracy: 0.375427 | 1.512 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 31 | Batch: 000 / 011 | Total loss: 2.172 | Reg loss: 0.030 | Tree loss: 2.172 | Accuracy: 0.315500 | 1.511 sec/iter\n",
      "Epoch: 31 | Batch: 001 / 011 | Total loss: 2.112 | Reg loss: 0.030 | Tree loss: 2.112 | Accuracy: 0.360500 | 1.51 sec/iter\n",
      "Epoch: 31 | Batch: 002 / 011 | Total loss: 2.088 | Reg loss: 0.030 | Tree loss: 2.088 | Accuracy: 0.331000 | 1.51 sec/iter\n",
      "Epoch: 31 | Batch: 003 / 011 | Total loss: 2.034 | Reg loss: 0.030 | Tree loss: 2.034 | Accuracy: 0.347500 | 1.51 sec/iter\n",
      "Epoch: 31 | Batch: 004 / 011 | Total loss: 1.962 | Reg loss: 0.030 | Tree loss: 1.962 | Accuracy: 0.372000 | 1.51 sec/iter\n",
      "Epoch: 31 | Batch: 005 / 011 | Total loss: 1.933 | Reg loss: 0.030 | Tree loss: 1.933 | Accuracy: 0.380500 | 1.509 sec/iter\n",
      "Epoch: 31 | Batch: 006 / 011 | Total loss: 1.882 | Reg loss: 0.031 | Tree loss: 1.882 | Accuracy: 0.419000 | 1.509 sec/iter\n",
      "Epoch: 31 | Batch: 007 / 011 | Total loss: 1.878 | Reg loss: 0.031 | Tree loss: 1.878 | Accuracy: 0.400000 | 1.508 sec/iter\n",
      "Epoch: 31 | Batch: 008 / 011 | Total loss: 1.852 | Reg loss: 0.031 | Tree loss: 1.852 | Accuracy: 0.383000 | 1.507 sec/iter\n",
      "Epoch: 31 | Batch: 009 / 011 | Total loss: 1.844 | Reg loss: 0.031 | Tree loss: 1.844 | Accuracy: 0.393000 | 1.507 sec/iter\n",
      "Epoch: 31 | Batch: 010 / 011 | Total loss: 1.824 | Reg loss: 0.031 | Tree loss: 1.824 | Accuracy: 0.385666 | 1.507 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 32 | Batch: 000 / 011 | Total loss: 2.150 | Reg loss: 0.031 | Tree loss: 2.150 | Accuracy: 0.325500 | 1.507 sec/iter\n",
      "Epoch: 32 | Batch: 001 / 011 | Total loss: 2.119 | Reg loss: 0.031 | Tree loss: 2.119 | Accuracy: 0.340000 | 1.507 sec/iter\n",
      "Epoch: 32 | Batch: 002 / 011 | Total loss: 2.037 | Reg loss: 0.031 | Tree loss: 2.037 | Accuracy: 0.377000 | 1.506 sec/iter\n",
      "Epoch: 32 | Batch: 003 / 011 | Total loss: 1.999 | Reg loss: 0.031 | Tree loss: 1.999 | Accuracy: 0.359500 | 1.506 sec/iter\n",
      "Epoch: 32 | Batch: 004 / 011 | Total loss: 1.948 | Reg loss: 0.031 | Tree loss: 1.948 | Accuracy: 0.378500 | 1.507 sec/iter\n",
      "Epoch: 32 | Batch: 005 / 011 | Total loss: 1.917 | Reg loss: 0.031 | Tree loss: 1.917 | Accuracy: 0.387000 | 1.507 sec/iter\n",
      "Epoch: 32 | Batch: 006 / 011 | Total loss: 1.879 | Reg loss: 0.031 | Tree loss: 1.879 | Accuracy: 0.404500 | 1.507 sec/iter\n",
      "Epoch: 32 | Batch: 007 / 011 | Total loss: 1.858 | Reg loss: 0.031 | Tree loss: 1.858 | Accuracy: 0.399000 | 1.507 sec/iter\n",
      "Epoch: 32 | Batch: 008 / 011 | Total loss: 1.830 | Reg loss: 0.031 | Tree loss: 1.830 | Accuracy: 0.385500 | 1.507 sec/iter\n",
      "Epoch: 32 | Batch: 009 / 011 | Total loss: 1.824 | Reg loss: 0.031 | Tree loss: 1.824 | Accuracy: 0.375500 | 1.507 sec/iter\n",
      "Epoch: 32 | Batch: 010 / 011 | Total loss: 1.850 | Reg loss: 0.031 | Tree loss: 1.850 | Accuracy: 0.372014 | 1.506 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 33 | Batch: 000 / 011 | Total loss: 2.136 | Reg loss: 0.031 | Tree loss: 2.136 | Accuracy: 0.344000 | 1.507 sec/iter\n",
      "Epoch: 33 | Batch: 001 / 011 | Total loss: 2.089 | Reg loss: 0.031 | Tree loss: 2.089 | Accuracy: 0.349000 | 1.507 sec/iter\n",
      "Epoch: 33 | Batch: 002 / 011 | Total loss: 2.035 | Reg loss: 0.031 | Tree loss: 2.035 | Accuracy: 0.381500 | 1.507 sec/iter\n",
      "Epoch: 33 | Batch: 003 / 011 | Total loss: 1.977 | Reg loss: 0.031 | Tree loss: 1.977 | Accuracy: 0.384000 | 1.507 sec/iter\n",
      "Epoch: 33 | Batch: 004 / 011 | Total loss: 1.927 | Reg loss: 0.031 | Tree loss: 1.927 | Accuracy: 0.367000 | 1.507 sec/iter\n",
      "Epoch: 33 | Batch: 005 / 011 | Total loss: 1.885 | Reg loss: 0.031 | Tree loss: 1.885 | Accuracy: 0.407000 | 1.506 sec/iter\n",
      "Epoch: 33 | Batch: 006 / 011 | Total loss: 1.856 | Reg loss: 0.031 | Tree loss: 1.856 | Accuracy: 0.398000 | 1.506 sec/iter\n",
      "Epoch: 33 | Batch: 007 / 011 | Total loss: 1.853 | Reg loss: 0.031 | Tree loss: 1.853 | Accuracy: 0.406500 | 1.506 sec/iter\n",
      "Epoch: 33 | Batch: 008 / 011 | Total loss: 1.828 | Reg loss: 0.031 | Tree loss: 1.828 | Accuracy: 0.386500 | 1.505 sec/iter\n",
      "Epoch: 33 | Batch: 009 / 011 | Total loss: 1.816 | Reg loss: 0.031 | Tree loss: 1.816 | Accuracy: 0.400000 | 1.505 sec/iter\n",
      "Epoch: 33 | Batch: 010 / 011 | Total loss: 1.832 | Reg loss: 0.032 | Tree loss: 1.832 | Accuracy: 0.358362 | 1.504 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 34 | Batch: 000 / 011 | Total loss: 2.114 | Reg loss: 0.031 | Tree loss: 2.114 | Accuracy: 0.348000 | 1.505 sec/iter\n",
      "Epoch: 34 | Batch: 001 / 011 | Total loss: 2.079 | Reg loss: 0.031 | Tree loss: 2.079 | Accuracy: 0.360500 | 1.506 sec/iter\n",
      "Epoch: 34 | Batch: 002 / 011 | Total loss: 2.014 | Reg loss: 0.031 | Tree loss: 2.014 | Accuracy: 0.363500 | 1.505 sec/iter\n",
      "Epoch: 34 | Batch: 003 / 011 | Total loss: 1.961 | Reg loss: 0.031 | Tree loss: 1.961 | Accuracy: 0.379000 | 1.505 sec/iter\n",
      "Epoch: 34 | Batch: 004 / 011 | Total loss: 1.924 | Reg loss: 0.031 | Tree loss: 1.924 | Accuracy: 0.353500 | 1.505 sec/iter\n",
      "Epoch: 34 | Batch: 005 / 011 | Total loss: 1.879 | Reg loss: 0.031 | Tree loss: 1.879 | Accuracy: 0.380000 | 1.505 sec/iter\n",
      "Epoch: 34 | Batch: 006 / 011 | Total loss: 1.858 | Reg loss: 0.031 | Tree loss: 1.858 | Accuracy: 0.407500 | 1.505 sec/iter\n",
      "Epoch: 34 | Batch: 007 / 011 | Total loss: 1.823 | Reg loss: 0.032 | Tree loss: 1.823 | Accuracy: 0.404000 | 1.505 sec/iter\n",
      "Epoch: 34 | Batch: 008 / 011 | Total loss: 1.794 | Reg loss: 0.032 | Tree loss: 1.794 | Accuracy: 0.412000 | 1.505 sec/iter\n",
      "Epoch: 34 | Batch: 009 / 011 | Total loss: 1.792 | Reg loss: 0.032 | Tree loss: 1.792 | Accuracy: 0.384500 | 1.505 sec/iter\n",
      "Epoch: 34 | Batch: 010 / 011 | Total loss: 1.783 | Reg loss: 0.032 | Tree loss: 1.783 | Accuracy: 0.402730 | 1.504 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 35 | Batch: 000 / 011 | Total loss: 2.107 | Reg loss: 0.031 | Tree loss: 2.107 | Accuracy: 0.339000 | 1.505 sec/iter\n",
      "Epoch: 35 | Batch: 001 / 011 | Total loss: 2.065 | Reg loss: 0.031 | Tree loss: 2.065 | Accuracy: 0.349500 | 1.504 sec/iter\n",
      "Epoch: 35 | Batch: 002 / 011 | Total loss: 2.004 | Reg loss: 0.032 | Tree loss: 2.004 | Accuracy: 0.378500 | 1.504 sec/iter\n",
      "Epoch: 35 | Batch: 003 / 011 | Total loss: 1.930 | Reg loss: 0.032 | Tree loss: 1.930 | Accuracy: 0.393500 | 1.504 sec/iter\n",
      "Epoch: 35 | Batch: 004 / 011 | Total loss: 1.875 | Reg loss: 0.032 | Tree loss: 1.875 | Accuracy: 0.401000 | 1.504 sec/iter\n",
      "Epoch: 35 | Batch: 005 / 011 | Total loss: 1.850 | Reg loss: 0.032 | Tree loss: 1.850 | Accuracy: 0.397500 | 1.504 sec/iter\n",
      "Epoch: 35 | Batch: 006 / 011 | Total loss: 1.840 | Reg loss: 0.032 | Tree loss: 1.840 | Accuracy: 0.423500 | 1.504 sec/iter\n",
      "Epoch: 35 | Batch: 007 / 011 | Total loss: 1.803 | Reg loss: 0.032 | Tree loss: 1.803 | Accuracy: 0.406500 | 1.504 sec/iter\n",
      "Epoch: 35 | Batch: 008 / 011 | Total loss: 1.800 | Reg loss: 0.032 | Tree loss: 1.800 | Accuracy: 0.393000 | 1.504 sec/iter\n",
      "Epoch: 35 | Batch: 009 / 011 | Total loss: 1.794 | Reg loss: 0.032 | Tree loss: 1.794 | Accuracy: 0.394500 | 1.504 sec/iter\n",
      "Epoch: 35 | Batch: 010 / 011 | Total loss: 1.841 | Reg loss: 0.032 | Tree loss: 1.841 | Accuracy: 0.358362 | 1.504 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 36 | Batch: 000 / 011 | Total loss: 2.098 | Reg loss: 0.032 | Tree loss: 2.098 | Accuracy: 0.341000 | 1.503 sec/iter\n",
      "Epoch: 36 | Batch: 001 / 011 | Total loss: 2.032 | Reg loss: 0.032 | Tree loss: 2.032 | Accuracy: 0.335500 | 1.503 sec/iter\n",
      "Epoch: 36 | Batch: 002 / 011 | Total loss: 1.956 | Reg loss: 0.032 | Tree loss: 1.956 | Accuracy: 0.412500 | 1.502 sec/iter\n",
      "Epoch: 36 | Batch: 003 / 011 | Total loss: 1.930 | Reg loss: 0.032 | Tree loss: 1.930 | Accuracy: 0.391000 | 1.502 sec/iter\n",
      "Epoch: 36 | Batch: 004 / 011 | Total loss: 1.897 | Reg loss: 0.032 | Tree loss: 1.897 | Accuracy: 0.374500 | 1.503 sec/iter\n",
      "Epoch: 36 | Batch: 005 / 011 | Total loss: 1.867 | Reg loss: 0.032 | Tree loss: 1.867 | Accuracy: 0.398500 | 1.502 sec/iter\n",
      "Epoch: 36 | Batch: 006 / 011 | Total loss: 1.801 | Reg loss: 0.032 | Tree loss: 1.801 | Accuracy: 0.423500 | 1.502 sec/iter\n",
      "Epoch: 36 | Batch: 007 / 011 | Total loss: 1.788 | Reg loss: 0.032 | Tree loss: 1.788 | Accuracy: 0.423500 | 1.502 sec/iter\n",
      "Epoch: 36 | Batch: 008 / 011 | Total loss: 1.779 | Reg loss: 0.032 | Tree loss: 1.779 | Accuracy: 0.405500 | 1.502 sec/iter\n",
      "Epoch: 36 | Batch: 009 / 011 | Total loss: 1.781 | Reg loss: 0.032 | Tree loss: 1.781 | Accuracy: 0.406500 | 1.502 sec/iter\n",
      "Epoch: 36 | Batch: 010 / 011 | Total loss: 1.758 | Reg loss: 0.032 | Tree loss: 1.758 | Accuracy: 0.402730 | 1.502 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 37 | Batch: 000 / 011 | Total loss: 2.091 | Reg loss: 0.032 | Tree loss: 2.091 | Accuracy: 0.328000 | 1.502 sec/iter\n",
      "Epoch: 37 | Batch: 001 / 011 | Total loss: 2.027 | Reg loss: 0.032 | Tree loss: 2.027 | Accuracy: 0.346000 | 1.501 sec/iter\n",
      "Epoch: 37 | Batch: 002 / 011 | Total loss: 1.982 | Reg loss: 0.032 | Tree loss: 1.982 | Accuracy: 0.362000 | 1.501 sec/iter\n",
      "Epoch: 37 | Batch: 003 / 011 | Total loss: 1.918 | Reg loss: 0.032 | Tree loss: 1.918 | Accuracy: 0.391000 | 1.501 sec/iter\n",
      "Epoch: 37 | Batch: 004 / 011 | Total loss: 1.855 | Reg loss: 0.032 | Tree loss: 1.855 | Accuracy: 0.395500 | 1.501 sec/iter\n",
      "Epoch: 37 | Batch: 005 / 011 | Total loss: 1.828 | Reg loss: 0.032 | Tree loss: 1.828 | Accuracy: 0.423000 | 1.501 sec/iter\n",
      "Epoch: 37 | Batch: 006 / 011 | Total loss: 1.800 | Reg loss: 0.032 | Tree loss: 1.800 | Accuracy: 0.452500 | 1.501 sec/iter\n",
      "Epoch: 37 | Batch: 007 / 011 | Total loss: 1.778 | Reg loss: 0.032 | Tree loss: 1.778 | Accuracy: 0.408000 | 1.501 sec/iter\n",
      "Epoch: 37 | Batch: 008 / 011 | Total loss: 1.747 | Reg loss: 0.032 | Tree loss: 1.747 | Accuracy: 0.440500 | 1.501 sec/iter\n",
      "Epoch: 37 | Batch: 009 / 011 | Total loss: 1.750 | Reg loss: 0.032 | Tree loss: 1.750 | Accuracy: 0.418500 | 1.501 sec/iter\n",
      "Epoch: 37 | Batch: 010 / 011 | Total loss: 1.742 | Reg loss: 0.032 | Tree loss: 1.742 | Accuracy: 0.416382 | 1.501 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 38 | Batch: 000 / 011 | Total loss: 2.076 | Reg loss: 0.032 | Tree loss: 2.076 | Accuracy: 0.305000 | 1.502 sec/iter\n",
      "Epoch: 38 | Batch: 001 / 011 | Total loss: 2.024 | Reg loss: 0.032 | Tree loss: 2.024 | Accuracy: 0.335500 | 1.501 sec/iter\n",
      "Epoch: 38 | Batch: 002 / 011 | Total loss: 1.971 | Reg loss: 0.032 | Tree loss: 1.971 | Accuracy: 0.374000 | 1.501 sec/iter\n",
      "Epoch: 38 | Batch: 003 / 011 | Total loss: 1.904 | Reg loss: 0.032 | Tree loss: 1.904 | Accuracy: 0.407500 | 1.501 sec/iter\n",
      "Epoch: 38 | Batch: 004 / 011 | Total loss: 1.865 | Reg loss: 0.032 | Tree loss: 1.865 | Accuracy: 0.405500 | 1.501 sec/iter\n",
      "Epoch: 38 | Batch: 005 / 011 | Total loss: 1.815 | Reg loss: 0.032 | Tree loss: 1.815 | Accuracy: 0.430500 | 1.501 sec/iter\n",
      "Epoch: 38 | Batch: 006 / 011 | Total loss: 1.771 | Reg loss: 0.032 | Tree loss: 1.771 | Accuracy: 0.416500 | 1.501 sec/iter\n",
      "Epoch: 38 | Batch: 007 / 011 | Total loss: 1.752 | Reg loss: 0.033 | Tree loss: 1.752 | Accuracy: 0.425000 | 1.5 sec/iter\n",
      "Epoch: 38 | Batch: 008 / 011 | Total loss: 1.768 | Reg loss: 0.033 | Tree loss: 1.768 | Accuracy: 0.413500 | 1.5 sec/iter\n",
      "Epoch: 38 | Batch: 009 / 011 | Total loss: 1.736 | Reg loss: 0.033 | Tree loss: 1.736 | Accuracy: 0.413500 | 1.5 sec/iter\n",
      "Epoch: 38 | Batch: 010 / 011 | Total loss: 1.708 | Reg loss: 0.033 | Tree loss: 1.708 | Accuracy: 0.440273 | 1.501 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 39 | Batch: 000 / 011 | Total loss: 2.039 | Reg loss: 0.032 | Tree loss: 2.039 | Accuracy: 0.349000 | 1.5 sec/iter\n",
      "Epoch: 39 | Batch: 001 / 011 | Total loss: 1.990 | Reg loss: 0.032 | Tree loss: 1.990 | Accuracy: 0.348000 | 1.5 sec/iter\n",
      "Epoch: 39 | Batch: 002 / 011 | Total loss: 1.923 | Reg loss: 0.032 | Tree loss: 1.923 | Accuracy: 0.398000 | 1.5 sec/iter\n",
      "Epoch: 39 | Batch: 003 / 011 | Total loss: 1.897 | Reg loss: 0.033 | Tree loss: 1.897 | Accuracy: 0.385500 | 1.499 sec/iter\n",
      "Epoch: 39 | Batch: 004 / 011 | Total loss: 1.847 | Reg loss: 0.033 | Tree loss: 1.847 | Accuracy: 0.407000 | 1.499 sec/iter\n",
      "Epoch: 39 | Batch: 005 / 011 | Total loss: 1.794 | Reg loss: 0.033 | Tree loss: 1.794 | Accuracy: 0.442000 | 1.499 sec/iter\n",
      "Epoch: 39 | Batch: 006 / 011 | Total loss: 1.784 | Reg loss: 0.033 | Tree loss: 1.784 | Accuracy: 0.431500 | 1.499 sec/iter\n",
      "Epoch: 39 | Batch: 007 / 011 | Total loss: 1.760 | Reg loss: 0.033 | Tree loss: 1.760 | Accuracy: 0.404500 | 1.499 sec/iter\n",
      "Epoch: 39 | Batch: 008 / 011 | Total loss: 1.743 | Reg loss: 0.033 | Tree loss: 1.743 | Accuracy: 0.432000 | 1.5 sec/iter\n",
      "Epoch: 39 | Batch: 009 / 011 | Total loss: 1.737 | Reg loss: 0.033 | Tree loss: 1.737 | Accuracy: 0.420000 | 1.499 sec/iter\n",
      "Epoch: 39 | Batch: 010 / 011 | Total loss: 1.717 | Reg loss: 0.033 | Tree loss: 1.717 | Accuracy: 0.344710 | 1.499 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 40 | Batch: 000 / 011 | Total loss: 2.046 | Reg loss: 0.033 | Tree loss: 2.046 | Accuracy: 0.326000 | 1.5 sec/iter\n",
      "Epoch: 40 | Batch: 001 / 011 | Total loss: 1.994 | Reg loss: 0.033 | Tree loss: 1.994 | Accuracy: 0.354000 | 1.499 sec/iter\n",
      "Epoch: 40 | Batch: 002 / 011 | Total loss: 1.926 | Reg loss: 0.033 | Tree loss: 1.926 | Accuracy: 0.382500 | 1.499 sec/iter\n",
      "Epoch: 40 | Batch: 003 / 011 | Total loss: 1.883 | Reg loss: 0.033 | Tree loss: 1.883 | Accuracy: 0.390500 | 1.498 sec/iter\n",
      "Epoch: 40 | Batch: 004 / 011 | Total loss: 1.829 | Reg loss: 0.033 | Tree loss: 1.829 | Accuracy: 0.429500 | 1.499 sec/iter\n",
      "Epoch: 40 | Batch: 005 / 011 | Total loss: 1.787 | Reg loss: 0.033 | Tree loss: 1.787 | Accuracy: 0.432000 | 1.499 sec/iter\n",
      "Epoch: 40 | Batch: 006 / 011 | Total loss: 1.755 | Reg loss: 0.033 | Tree loss: 1.755 | Accuracy: 0.436000 | 1.499 sec/iter\n",
      "Epoch: 40 | Batch: 007 / 011 | Total loss: 1.748 | Reg loss: 0.033 | Tree loss: 1.748 | Accuracy: 0.441500 | 1.498 sec/iter\n",
      "Epoch: 40 | Batch: 008 / 011 | Total loss: 1.747 | Reg loss: 0.033 | Tree loss: 1.747 | Accuracy: 0.413500 | 1.498 sec/iter\n",
      "Epoch: 40 | Batch: 009 / 011 | Total loss: 1.707 | Reg loss: 0.033 | Tree loss: 1.707 | Accuracy: 0.437500 | 1.498 sec/iter\n",
      "Epoch: 40 | Batch: 010 / 011 | Total loss: 1.657 | Reg loss: 0.033 | Tree loss: 1.657 | Accuracy: 0.508532 | 1.497 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 41 | Batch: 000 / 011 | Total loss: 2.015 | Reg loss: 0.033 | Tree loss: 2.015 | Accuracy: 0.347000 | 1.497 sec/iter\n",
      "Epoch: 41 | Batch: 001 / 011 | Total loss: 1.975 | Reg loss: 0.033 | Tree loss: 1.975 | Accuracy: 0.358500 | 1.497 sec/iter\n",
      "Epoch: 41 | Batch: 002 / 011 | Total loss: 1.927 | Reg loss: 0.033 | Tree loss: 1.927 | Accuracy: 0.376000 | 1.498 sec/iter\n",
      "Epoch: 41 | Batch: 003 / 011 | Total loss: 1.861 | Reg loss: 0.033 | Tree loss: 1.861 | Accuracy: 0.408500 | 1.498 sec/iter\n",
      "Epoch: 41 | Batch: 004 / 011 | Total loss: 1.804 | Reg loss: 0.033 | Tree loss: 1.804 | Accuracy: 0.433500 | 1.498 sec/iter\n",
      "Epoch: 41 | Batch: 005 / 011 | Total loss: 1.789 | Reg loss: 0.033 | Tree loss: 1.789 | Accuracy: 0.434500 | 1.498 sec/iter\n",
      "Epoch: 41 | Batch: 006 / 011 | Total loss: 1.750 | Reg loss: 0.033 | Tree loss: 1.750 | Accuracy: 0.427000 | 1.498 sec/iter\n",
      "Epoch: 41 | Batch: 007 / 011 | Total loss: 1.723 | Reg loss: 0.033 | Tree loss: 1.723 | Accuracy: 0.438000 | 1.498 sec/iter\n",
      "Epoch: 41 | Batch: 008 / 011 | Total loss: 1.729 | Reg loss: 0.033 | Tree loss: 1.729 | Accuracy: 0.435500 | 1.497 sec/iter\n",
      "Epoch: 41 | Batch: 009 / 011 | Total loss: 1.719 | Reg loss: 0.033 | Tree loss: 1.719 | Accuracy: 0.426000 | 1.497 sec/iter\n",
      "Epoch: 41 | Batch: 010 / 011 | Total loss: 1.710 | Reg loss: 0.033 | Tree loss: 1.710 | Accuracy: 0.416382 | 1.498 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 42 | Batch: 000 / 011 | Total loss: 1.990 | Reg loss: 0.033 | Tree loss: 1.990 | Accuracy: 0.345000 | 1.499 sec/iter\n",
      "Epoch: 42 | Batch: 001 / 011 | Total loss: 1.968 | Reg loss: 0.033 | Tree loss: 1.968 | Accuracy: 0.344500 | 1.498 sec/iter\n",
      "Epoch: 42 | Batch: 002 / 011 | Total loss: 1.909 | Reg loss: 0.033 | Tree loss: 1.909 | Accuracy: 0.373000 | 1.498 sec/iter\n",
      "Epoch: 42 | Batch: 003 / 011 | Total loss: 1.865 | Reg loss: 0.033 | Tree loss: 1.865 | Accuracy: 0.388500 | 1.498 sec/iter\n",
      "Epoch: 42 | Batch: 004 / 011 | Total loss: 1.825 | Reg loss: 0.033 | Tree loss: 1.825 | Accuracy: 0.425500 | 1.498 sec/iter\n",
      "Epoch: 42 | Batch: 005 / 011 | Total loss: 1.770 | Reg loss: 0.033 | Tree loss: 1.770 | Accuracy: 0.424000 | 1.499 sec/iter\n",
      "Epoch: 42 | Batch: 006 / 011 | Total loss: 1.725 | Reg loss: 0.033 | Tree loss: 1.725 | Accuracy: 0.447000 | 1.499 sec/iter\n",
      "Epoch: 42 | Batch: 007 / 011 | Total loss: 1.718 | Reg loss: 0.033 | Tree loss: 1.718 | Accuracy: 0.454000 | 1.499 sec/iter\n",
      "Epoch: 42 | Batch: 008 / 011 | Total loss: 1.705 | Reg loss: 0.033 | Tree loss: 1.705 | Accuracy: 0.450000 | 1.499 sec/iter\n",
      "Epoch: 42 | Batch: 009 / 011 | Total loss: 1.705 | Reg loss: 0.033 | Tree loss: 1.705 | Accuracy: 0.450000 | 1.499 sec/iter\n",
      "Epoch: 42 | Batch: 010 / 011 | Total loss: 1.698 | Reg loss: 0.033 | Tree loss: 1.698 | Accuracy: 0.436860 | 1.499 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 43 | Batch: 000 / 011 | Total loss: 1.995 | Reg loss: 0.033 | Tree loss: 1.995 | Accuracy: 0.346500 | 1.499 sec/iter\n",
      "Epoch: 43 | Batch: 001 / 011 | Total loss: 1.953 | Reg loss: 0.033 | Tree loss: 1.953 | Accuracy: 0.352500 | 1.499 sec/iter\n",
      "Epoch: 43 | Batch: 002 / 011 | Total loss: 1.899 | Reg loss: 0.033 | Tree loss: 1.899 | Accuracy: 0.400000 | 1.498 sec/iter\n",
      "Epoch: 43 | Batch: 003 / 011 | Total loss: 1.843 | Reg loss: 0.033 | Tree loss: 1.843 | Accuracy: 0.412000 | 1.498 sec/iter\n",
      "Epoch: 43 | Batch: 004 / 011 | Total loss: 1.792 | Reg loss: 0.033 | Tree loss: 1.792 | Accuracy: 0.423500 | 1.498 sec/iter\n",
      "Epoch: 43 | Batch: 005 / 011 | Total loss: 1.749 | Reg loss: 0.033 | Tree loss: 1.749 | Accuracy: 0.453500 | 1.498 sec/iter\n",
      "Epoch: 43 | Batch: 006 / 011 | Total loss: 1.723 | Reg loss: 0.033 | Tree loss: 1.723 | Accuracy: 0.434500 | 1.498 sec/iter\n",
      "Epoch: 43 | Batch: 007 / 011 | Total loss: 1.711 | Reg loss: 0.033 | Tree loss: 1.711 | Accuracy: 0.437000 | 1.498 sec/iter\n",
      "Epoch: 43 | Batch: 008 / 011 | Total loss: 1.706 | Reg loss: 0.034 | Tree loss: 1.706 | Accuracy: 0.433000 | 1.498 sec/iter\n",
      "Epoch: 43 | Batch: 009 / 011 | Total loss: 1.695 | Reg loss: 0.034 | Tree loss: 1.695 | Accuracy: 0.436500 | 1.498 sec/iter\n",
      "Epoch: 43 | Batch: 010 / 011 | Total loss: 1.695 | Reg loss: 0.034 | Tree loss: 1.695 | Accuracy: 0.436860 | 1.498 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 44 | Batch: 000 / 011 | Total loss: 1.977 | Reg loss: 0.033 | Tree loss: 1.977 | Accuracy: 0.334000 | 1.499 sec/iter\n",
      "Epoch: 44 | Batch: 001 / 011 | Total loss: 1.936 | Reg loss: 0.033 | Tree loss: 1.936 | Accuracy: 0.354000 | 1.499 sec/iter\n",
      "Epoch: 44 | Batch: 002 / 011 | Total loss: 1.900 | Reg loss: 0.033 | Tree loss: 1.900 | Accuracy: 0.391500 | 1.499 sec/iter\n",
      "Epoch: 44 | Batch: 003 / 011 | Total loss: 1.826 | Reg loss: 0.033 | Tree loss: 1.826 | Accuracy: 0.417000 | 1.499 sec/iter\n",
      "Epoch: 44 | Batch: 004 / 011 | Total loss: 1.792 | Reg loss: 0.034 | Tree loss: 1.792 | Accuracy: 0.422500 | 1.5 sec/iter\n",
      "Epoch: 44 | Batch: 005 / 011 | Total loss: 1.744 | Reg loss: 0.034 | Tree loss: 1.744 | Accuracy: 0.445500 | 1.501 sec/iter\n",
      "Epoch: 44 | Batch: 006 / 011 | Total loss: 1.732 | Reg loss: 0.034 | Tree loss: 1.732 | Accuracy: 0.457000 | 1.502 sec/iter\n",
      "Epoch: 44 | Batch: 007 / 011 | Total loss: 1.703 | Reg loss: 0.034 | Tree loss: 1.703 | Accuracy: 0.439500 | 1.503 sec/iter\n",
      "Epoch: 44 | Batch: 008 / 011 | Total loss: 1.698 | Reg loss: 0.034 | Tree loss: 1.698 | Accuracy: 0.432000 | 1.504 sec/iter\n",
      "Epoch: 44 | Batch: 009 / 011 | Total loss: 1.665 | Reg loss: 0.034 | Tree loss: 1.665 | Accuracy: 0.455000 | 1.504 sec/iter\n",
      "Epoch: 44 | Batch: 010 / 011 | Total loss: 1.665 | Reg loss: 0.034 | Tree loss: 1.665 | Accuracy: 0.426621 | 1.505 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 45 | Batch: 000 / 011 | Total loss: 1.979 | Reg loss: 0.034 | Tree loss: 1.979 | Accuracy: 0.323000 | 1.518 sec/iter\n",
      "Epoch: 45 | Batch: 001 / 011 | Total loss: 1.919 | Reg loss: 0.034 | Tree loss: 1.919 | Accuracy: 0.362500 | 1.518 sec/iter\n",
      "Epoch: 45 | Batch: 002 / 011 | Total loss: 1.888 | Reg loss: 0.034 | Tree loss: 1.888 | Accuracy: 0.376000 | 1.519 sec/iter\n",
      "Epoch: 45 | Batch: 003 / 011 | Total loss: 1.813 | Reg loss: 0.034 | Tree loss: 1.813 | Accuracy: 0.417500 | 1.519 sec/iter\n",
      "Epoch: 45 | Batch: 004 / 011 | Total loss: 1.782 | Reg loss: 0.034 | Tree loss: 1.782 | Accuracy: 0.436500 | 1.52 sec/iter\n",
      "Epoch: 45 | Batch: 005 / 011 | Total loss: 1.745 | Reg loss: 0.034 | Tree loss: 1.745 | Accuracy: 0.463000 | 1.521 sec/iter\n",
      "Epoch: 45 | Batch: 006 / 011 | Total loss: 1.713 | Reg loss: 0.034 | Tree loss: 1.713 | Accuracy: 0.425000 | 1.522 sec/iter\n",
      "Epoch: 45 | Batch: 007 / 011 | Total loss: 1.688 | Reg loss: 0.034 | Tree loss: 1.688 | Accuracy: 0.435500 | 1.523 sec/iter\n",
      "Epoch: 45 | Batch: 008 / 011 | Total loss: 1.668 | Reg loss: 0.034 | Tree loss: 1.668 | Accuracy: 0.465000 | 1.523 sec/iter\n",
      "Epoch: 45 | Batch: 009 / 011 | Total loss: 1.671 | Reg loss: 0.034 | Tree loss: 1.671 | Accuracy: 0.436000 | 1.524 sec/iter\n",
      "Epoch: 45 | Batch: 010 / 011 | Total loss: 1.615 | Reg loss: 0.034 | Tree loss: 1.615 | Accuracy: 0.484642 | 1.524 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 46 | Batch: 000 / 011 | Total loss: 1.939 | Reg loss: 0.034 | Tree loss: 1.939 | Accuracy: 0.342000 | 1.533 sec/iter\n",
      "Epoch: 46 | Batch: 001 / 011 | Total loss: 1.918 | Reg loss: 0.034 | Tree loss: 1.918 | Accuracy: 0.356000 | 1.533 sec/iter\n",
      "Epoch: 46 | Batch: 002 / 011 | Total loss: 1.871 | Reg loss: 0.034 | Tree loss: 1.871 | Accuracy: 0.377500 | 1.533 sec/iter\n",
      "Epoch: 46 | Batch: 003 / 011 | Total loss: 1.807 | Reg loss: 0.034 | Tree loss: 1.807 | Accuracy: 0.416000 | 1.532 sec/iter\n",
      "Epoch: 46 | Batch: 004 / 011 | Total loss: 1.769 | Reg loss: 0.034 | Tree loss: 1.769 | Accuracy: 0.421500 | 1.532 sec/iter\n",
      "Epoch: 46 | Batch: 005 / 011 | Total loss: 1.732 | Reg loss: 0.034 | Tree loss: 1.732 | Accuracy: 0.453000 | 1.532 sec/iter\n",
      "Epoch: 46 | Batch: 006 / 011 | Total loss: 1.708 | Reg loss: 0.034 | Tree loss: 1.708 | Accuracy: 0.445000 | 1.532 sec/iter\n",
      "Epoch: 46 | Batch: 007 / 011 | Total loss: 1.664 | Reg loss: 0.034 | Tree loss: 1.664 | Accuracy: 0.446000 | 1.533 sec/iter\n",
      "Epoch: 46 | Batch: 008 / 011 | Total loss: 1.681 | Reg loss: 0.034 | Tree loss: 1.681 | Accuracy: 0.434500 | 1.533 sec/iter\n",
      "Epoch: 46 | Batch: 009 / 011 | Total loss: 1.678 | Reg loss: 0.034 | Tree loss: 1.678 | Accuracy: 0.449000 | 1.532 sec/iter\n",
      "Epoch: 46 | Batch: 010 / 011 | Total loss: 1.600 | Reg loss: 0.034 | Tree loss: 1.600 | Accuracy: 0.508532 | 1.53 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 47 | Batch: 000 / 011 | Total loss: 1.953 | Reg loss: 0.034 | Tree loss: 1.953 | Accuracy: 0.346000 | 1.542 sec/iter\n",
      "Epoch: 47 | Batch: 001 / 011 | Total loss: 1.916 | Reg loss: 0.034 | Tree loss: 1.916 | Accuracy: 0.352500 | 1.542 sec/iter\n",
      "Epoch: 47 | Batch: 002 / 011 | Total loss: 1.861 | Reg loss: 0.034 | Tree loss: 1.861 | Accuracy: 0.386500 | 1.542 sec/iter\n",
      "Epoch: 47 | Batch: 003 / 011 | Total loss: 1.793 | Reg loss: 0.034 | Tree loss: 1.793 | Accuracy: 0.406500 | 1.541 sec/iter\n",
      "Epoch: 47 | Batch: 004 / 011 | Total loss: 1.749 | Reg loss: 0.034 | Tree loss: 1.749 | Accuracy: 0.446000 | 1.541 sec/iter\n",
      "Epoch: 47 | Batch: 005 / 011 | Total loss: 1.715 | Reg loss: 0.034 | Tree loss: 1.715 | Accuracy: 0.453000 | 1.541 sec/iter\n",
      "Epoch: 47 | Batch: 006 / 011 | Total loss: 1.691 | Reg loss: 0.034 | Tree loss: 1.691 | Accuracy: 0.453000 | 1.541 sec/iter\n",
      "Epoch: 47 | Batch: 007 / 011 | Total loss: 1.667 | Reg loss: 0.034 | Tree loss: 1.667 | Accuracy: 0.443000 | 1.54 sec/iter\n",
      "Epoch: 47 | Batch: 008 / 011 | Total loss: 1.662 | Reg loss: 0.034 | Tree loss: 1.662 | Accuracy: 0.465500 | 1.54 sec/iter\n",
      "Epoch: 47 | Batch: 009 / 011 | Total loss: 1.667 | Reg loss: 0.034 | Tree loss: 1.667 | Accuracy: 0.458500 | 1.54 sec/iter\n",
      "Epoch: 47 | Batch: 010 / 011 | Total loss: 1.653 | Reg loss: 0.034 | Tree loss: 1.653 | Accuracy: 0.457338 | 1.538 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 48 | Batch: 000 / 011 | Total loss: 1.928 | Reg loss: 0.034 | Tree loss: 1.928 | Accuracy: 0.335000 | 1.539 sec/iter\n",
      "Epoch: 48 | Batch: 001 / 011 | Total loss: 1.883 | Reg loss: 0.034 | Tree loss: 1.883 | Accuracy: 0.374000 | 1.538 sec/iter\n",
      "Epoch: 48 | Batch: 002 / 011 | Total loss: 1.848 | Reg loss: 0.034 | Tree loss: 1.848 | Accuracy: 0.394500 | 1.538 sec/iter\n",
      "Epoch: 48 | Batch: 003 / 011 | Total loss: 1.785 | Reg loss: 0.034 | Tree loss: 1.785 | Accuracy: 0.414000 | 1.538 sec/iter\n",
      "Epoch: 48 | Batch: 004 / 011 | Total loss: 1.762 | Reg loss: 0.034 | Tree loss: 1.762 | Accuracy: 0.417000 | 1.537 sec/iter\n",
      "Epoch: 48 | Batch: 005 / 011 | Total loss: 1.717 | Reg loss: 0.034 | Tree loss: 1.717 | Accuracy: 0.434000 | 1.537 sec/iter\n",
      "Epoch: 48 | Batch: 006 / 011 | Total loss: 1.684 | Reg loss: 0.034 | Tree loss: 1.684 | Accuracy: 0.455500 | 1.537 sec/iter\n",
      "Epoch: 48 | Batch: 007 / 011 | Total loss: 1.659 | Reg loss: 0.034 | Tree loss: 1.659 | Accuracy: 0.459000 | 1.537 sec/iter\n",
      "Epoch: 48 | Batch: 008 / 011 | Total loss: 1.674 | Reg loss: 0.034 | Tree loss: 1.674 | Accuracy: 0.440500 | 1.537 sec/iter\n",
      "Epoch: 48 | Batch: 009 / 011 | Total loss: 1.643 | Reg loss: 0.034 | Tree loss: 1.643 | Accuracy: 0.446500 | 1.536 sec/iter\n",
      "Epoch: 48 | Batch: 010 / 011 | Total loss: 1.598 | Reg loss: 0.034 | Tree loss: 1.598 | Accuracy: 0.447099 | 1.535 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 49 | Batch: 000 / 011 | Total loss: 1.933 | Reg loss: 0.034 | Tree loss: 1.933 | Accuracy: 0.349500 | 1.546 sec/iter\n",
      "Epoch: 49 | Batch: 001 / 011 | Total loss: 1.891 | Reg loss: 0.034 | Tree loss: 1.891 | Accuracy: 0.363500 | 1.546 sec/iter\n",
      "Epoch: 49 | Batch: 002 / 011 | Total loss: 1.816 | Reg loss: 0.034 | Tree loss: 1.816 | Accuracy: 0.399500 | 1.545 sec/iter\n",
      "Epoch: 49 | Batch: 003 / 011 | Total loss: 1.787 | Reg loss: 0.034 | Tree loss: 1.787 | Accuracy: 0.413000 | 1.545 sec/iter\n",
      "Epoch: 49 | Batch: 004 / 011 | Total loss: 1.755 | Reg loss: 0.034 | Tree loss: 1.755 | Accuracy: 0.439500 | 1.545 sec/iter\n",
      "Epoch: 49 | Batch: 005 / 011 | Total loss: 1.675 | Reg loss: 0.034 | Tree loss: 1.675 | Accuracy: 0.481000 | 1.545 sec/iter\n",
      "Epoch: 49 | Batch: 006 / 011 | Total loss: 1.685 | Reg loss: 0.034 | Tree loss: 1.685 | Accuracy: 0.464500 | 1.545 sec/iter\n",
      "Epoch: 49 | Batch: 007 / 011 | Total loss: 1.677 | Reg loss: 0.034 | Tree loss: 1.677 | Accuracy: 0.428000 | 1.544 sec/iter\n",
      "Epoch: 49 | Batch: 008 / 011 | Total loss: 1.662 | Reg loss: 0.035 | Tree loss: 1.662 | Accuracy: 0.440500 | 1.544 sec/iter\n",
      "Epoch: 49 | Batch: 009 / 011 | Total loss: 1.631 | Reg loss: 0.035 | Tree loss: 1.631 | Accuracy: 0.452000 | 1.544 sec/iter\n",
      "Epoch: 49 | Batch: 010 / 011 | Total loss: 1.676 | Reg loss: 0.035 | Tree loss: 1.676 | Accuracy: 0.460751 | 1.542 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50 | Batch: 000 / 011 | Total loss: 1.906 | Reg loss: 0.034 | Tree loss: 1.906 | Accuracy: 0.347000 | 1.542 sec/iter\n",
      "Epoch: 50 | Batch: 001 / 011 | Total loss: 1.862 | Reg loss: 0.034 | Tree loss: 1.862 | Accuracy: 0.378000 | 1.542 sec/iter\n",
      "Epoch: 50 | Batch: 002 / 011 | Total loss: 1.838 | Reg loss: 0.034 | Tree loss: 1.838 | Accuracy: 0.398000 | 1.541 sec/iter\n",
      "Epoch: 50 | Batch: 003 / 011 | Total loss: 1.790 | Reg loss: 0.034 | Tree loss: 1.790 | Accuracy: 0.398000 | 1.54 sec/iter\n",
      "Epoch: 50 | Batch: 004 / 011 | Total loss: 1.739 | Reg loss: 0.034 | Tree loss: 1.739 | Accuracy: 0.423000 | 1.539 sec/iter\n",
      "Epoch: 50 | Batch: 005 / 011 | Total loss: 1.697 | Reg loss: 0.035 | Tree loss: 1.697 | Accuracy: 0.448500 | 1.538 sec/iter\n",
      "Epoch: 50 | Batch: 006 / 011 | Total loss: 1.668 | Reg loss: 0.035 | Tree loss: 1.668 | Accuracy: 0.458000 | 1.537 sec/iter\n",
      "Epoch: 50 | Batch: 007 / 011 | Total loss: 1.661 | Reg loss: 0.035 | Tree loss: 1.661 | Accuracy: 0.431000 | 1.537 sec/iter\n",
      "Epoch: 50 | Batch: 008 / 011 | Total loss: 1.652 | Reg loss: 0.035 | Tree loss: 1.652 | Accuracy: 0.448500 | 1.537 sec/iter\n",
      "Epoch: 50 | Batch: 009 / 011 | Total loss: 1.636 | Reg loss: 0.035 | Tree loss: 1.636 | Accuracy: 0.455500 | 1.537 sec/iter\n",
      "Epoch: 50 | Batch: 010 / 011 | Total loss: 1.646 | Reg loss: 0.035 | Tree loss: 1.646 | Accuracy: 0.477816 | 1.536 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 51 | Batch: 000 / 011 | Total loss: 1.918 | Reg loss: 0.035 | Tree loss: 1.918 | Accuracy: 0.342000 | 1.541 sec/iter\n",
      "Epoch: 51 | Batch: 001 / 011 | Total loss: 1.844 | Reg loss: 0.035 | Tree loss: 1.844 | Accuracy: 0.368500 | 1.54 sec/iter\n",
      "Epoch: 51 | Batch: 002 / 011 | Total loss: 1.807 | Reg loss: 0.035 | Tree loss: 1.807 | Accuracy: 0.401500 | 1.54 sec/iter\n",
      "Epoch: 51 | Batch: 003 / 011 | Total loss: 1.763 | Reg loss: 0.035 | Tree loss: 1.763 | Accuracy: 0.426000 | 1.54 sec/iter\n",
      "Epoch: 51 | Batch: 004 / 011 | Total loss: 1.720 | Reg loss: 0.035 | Tree loss: 1.720 | Accuracy: 0.456000 | 1.54 sec/iter\n",
      "Epoch: 51 | Batch: 005 / 011 | Total loss: 1.687 | Reg loss: 0.035 | Tree loss: 1.687 | Accuracy: 0.445000 | 1.54 sec/iter\n",
      "Epoch: 51 | Batch: 006 / 011 | Total loss: 1.663 | Reg loss: 0.035 | Tree loss: 1.663 | Accuracy: 0.466000 | 1.539 sec/iter\n",
      "Epoch: 51 | Batch: 007 / 011 | Total loss: 1.640 | Reg loss: 0.035 | Tree loss: 1.640 | Accuracy: 0.447000 | 1.539 sec/iter\n",
      "Epoch: 51 | Batch: 008 / 011 | Total loss: 1.664 | Reg loss: 0.035 | Tree loss: 1.664 | Accuracy: 0.421000 | 1.539 sec/iter\n",
      "Epoch: 51 | Batch: 009 / 011 | Total loss: 1.636 | Reg loss: 0.035 | Tree loss: 1.636 | Accuracy: 0.448500 | 1.539 sec/iter\n",
      "Epoch: 51 | Batch: 010 / 011 | Total loss: 1.601 | Reg loss: 0.035 | Tree loss: 1.601 | Accuracy: 0.467577 | 1.537 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 52 | Batch: 000 / 011 | Total loss: 1.904 | Reg loss: 0.035 | Tree loss: 1.904 | Accuracy: 0.353500 | 1.538 sec/iter\n",
      "Epoch: 52 | Batch: 001 / 011 | Total loss: 1.848 | Reg loss: 0.035 | Tree loss: 1.848 | Accuracy: 0.371000 | 1.537 sec/iter\n",
      "Epoch: 52 | Batch: 002 / 011 | Total loss: 1.802 | Reg loss: 0.035 | Tree loss: 1.802 | Accuracy: 0.400500 | 1.537 sec/iter\n",
      "Epoch: 52 | Batch: 003 / 011 | Total loss: 1.757 | Reg loss: 0.035 | Tree loss: 1.757 | Accuracy: 0.419000 | 1.537 sec/iter\n",
      "Epoch: 52 | Batch: 004 / 011 | Total loss: 1.711 | Reg loss: 0.035 | Tree loss: 1.711 | Accuracy: 0.453000 | 1.537 sec/iter\n",
      "Epoch: 52 | Batch: 005 / 011 | Total loss: 1.685 | Reg loss: 0.035 | Tree loss: 1.685 | Accuracy: 0.453500 | 1.536 sec/iter\n",
      "Epoch: 52 | Batch: 006 / 011 | Total loss: 1.662 | Reg loss: 0.035 | Tree loss: 1.662 | Accuracy: 0.442500 | 1.536 sec/iter\n",
      "Epoch: 52 | Batch: 007 / 011 | Total loss: 1.640 | Reg loss: 0.035 | Tree loss: 1.640 | Accuracy: 0.443000 | 1.536 sec/iter\n",
      "Epoch: 52 | Batch: 008 / 011 | Total loss: 1.623 | Reg loss: 0.035 | Tree loss: 1.623 | Accuracy: 0.440500 | 1.536 sec/iter\n",
      "Epoch: 52 | Batch: 009 / 011 | Total loss: 1.617 | Reg loss: 0.035 | Tree loss: 1.617 | Accuracy: 0.463000 | 1.535 sec/iter\n",
      "Epoch: 52 | Batch: 010 / 011 | Total loss: 1.653 | Reg loss: 0.035 | Tree loss: 1.653 | Accuracy: 0.412969 | 1.533 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 53 | Batch: 000 / 011 | Total loss: 1.902 | Reg loss: 0.035 | Tree loss: 1.902 | Accuracy: 0.331000 | 1.544 sec/iter\n",
      "Epoch: 53 | Batch: 001 / 011 | Total loss: 1.863 | Reg loss: 0.035 | Tree loss: 1.863 | Accuracy: 0.350500 | 1.544 sec/iter\n",
      "Epoch: 53 | Batch: 002 / 011 | Total loss: 1.793 | Reg loss: 0.035 | Tree loss: 1.793 | Accuracy: 0.407000 | 1.544 sec/iter\n",
      "Epoch: 53 | Batch: 003 / 011 | Total loss: 1.742 | Reg loss: 0.035 | Tree loss: 1.742 | Accuracy: 0.423000 | 1.543 sec/iter\n",
      "Epoch: 53 | Batch: 004 / 011 | Total loss: 1.712 | Reg loss: 0.035 | Tree loss: 1.712 | Accuracy: 0.455000 | 1.543 sec/iter\n",
      "Epoch: 53 | Batch: 005 / 011 | Total loss: 1.672 | Reg loss: 0.035 | Tree loss: 1.672 | Accuracy: 0.465500 | 1.543 sec/iter\n",
      "Epoch: 53 | Batch: 006 / 011 | Total loss: 1.664 | Reg loss: 0.035 | Tree loss: 1.664 | Accuracy: 0.440500 | 1.543 sec/iter\n",
      "Epoch: 53 | Batch: 007 / 011 | Total loss: 1.639 | Reg loss: 0.035 | Tree loss: 1.639 | Accuracy: 0.447500 | 1.543 sec/iter\n",
      "Epoch: 53 | Batch: 008 / 011 | Total loss: 1.622 | Reg loss: 0.035 | Tree loss: 1.622 | Accuracy: 0.456000 | 1.543 sec/iter\n",
      "Epoch: 53 | Batch: 009 / 011 | Total loss: 1.602 | Reg loss: 0.035 | Tree loss: 1.602 | Accuracy: 0.455000 | 1.543 sec/iter\n",
      "Epoch: 53 | Batch: 010 / 011 | Total loss: 1.598 | Reg loss: 0.035 | Tree loss: 1.598 | Accuracy: 0.447099 | 1.541 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 54 | Batch: 000 / 011 | Total loss: 1.906 | Reg loss: 0.035 | Tree loss: 1.906 | Accuracy: 0.336000 | 1.543 sec/iter\n",
      "Epoch: 54 | Batch: 001 / 011 | Total loss: 1.822 | Reg loss: 0.035 | Tree loss: 1.822 | Accuracy: 0.375000 | 1.542 sec/iter\n",
      "Epoch: 54 | Batch: 002 / 011 | Total loss: 1.809 | Reg loss: 0.035 | Tree loss: 1.809 | Accuracy: 0.412500 | 1.542 sec/iter\n",
      "Epoch: 54 | Batch: 003 / 011 | Total loss: 1.763 | Reg loss: 0.035 | Tree loss: 1.763 | Accuracy: 0.425000 | 1.542 sec/iter\n",
      "Epoch: 54 | Batch: 004 / 011 | Total loss: 1.686 | Reg loss: 0.035 | Tree loss: 1.686 | Accuracy: 0.461500 | 1.542 sec/iter\n",
      "Epoch: 54 | Batch: 005 / 011 | Total loss: 1.669 | Reg loss: 0.035 | Tree loss: 1.669 | Accuracy: 0.455500 | 1.542 sec/iter\n",
      "Epoch: 54 | Batch: 006 / 011 | Total loss: 1.622 | Reg loss: 0.035 | Tree loss: 1.622 | Accuracy: 0.462000 | 1.542 sec/iter\n",
      "Epoch: 54 | Batch: 007 / 011 | Total loss: 1.595 | Reg loss: 0.035 | Tree loss: 1.595 | Accuracy: 0.459000 | 1.542 sec/iter\n",
      "Epoch: 54 | Batch: 008 / 011 | Total loss: 1.627 | Reg loss: 0.035 | Tree loss: 1.627 | Accuracy: 0.447500 | 1.542 sec/iter\n",
      "Epoch: 54 | Batch: 009 / 011 | Total loss: 1.608 | Reg loss: 0.035 | Tree loss: 1.608 | Accuracy: 0.453000 | 1.541 sec/iter\n",
      "Epoch: 54 | Batch: 010 / 011 | Total loss: 1.598 | Reg loss: 0.035 | Tree loss: 1.598 | Accuracy: 0.464164 | 1.54 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 55 | Batch: 000 / 011 | Total loss: 1.876 | Reg loss: 0.035 | Tree loss: 1.876 | Accuracy: 0.367000 | 1.553 sec/iter\n",
      "Epoch: 55 | Batch: 001 / 011 | Total loss: 1.845 | Reg loss: 0.035 | Tree loss: 1.845 | Accuracy: 0.361000 | 1.553 sec/iter\n",
      "Epoch: 55 | Batch: 002 / 011 | Total loss: 1.790 | Reg loss: 0.035 | Tree loss: 1.790 | Accuracy: 0.396500 | 1.553 sec/iter\n",
      "Epoch: 55 | Batch: 003 / 011 | Total loss: 1.748 | Reg loss: 0.035 | Tree loss: 1.748 | Accuracy: 0.431500 | 1.553 sec/iter\n",
      "Epoch: 55 | Batch: 004 / 011 | Total loss: 1.697 | Reg loss: 0.035 | Tree loss: 1.697 | Accuracy: 0.443000 | 1.553 sec/iter\n",
      "Epoch: 55 | Batch: 005 / 011 | Total loss: 1.677 | Reg loss: 0.035 | Tree loss: 1.677 | Accuracy: 0.445500 | 1.553 sec/iter\n",
      "Epoch: 55 | Batch: 006 / 011 | Total loss: 1.621 | Reg loss: 0.035 | Tree loss: 1.621 | Accuracy: 0.474000 | 1.553 sec/iter\n",
      "Epoch: 55 | Batch: 007 / 011 | Total loss: 1.607 | Reg loss: 0.035 | Tree loss: 1.607 | Accuracy: 0.465000 | 1.553 sec/iter\n",
      "Epoch: 55 | Batch: 008 / 011 | Total loss: 1.606 | Reg loss: 0.035 | Tree loss: 1.606 | Accuracy: 0.448000 | 1.552 sec/iter\n",
      "Epoch: 55 | Batch: 009 / 011 | Total loss: 1.618 | Reg loss: 0.035 | Tree loss: 1.618 | Accuracy: 0.442500 | 1.553 sec/iter\n",
      "Epoch: 55 | Batch: 010 / 011 | Total loss: 1.567 | Reg loss: 0.035 | Tree loss: 1.567 | Accuracy: 0.402730 | 1.551 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 56 | Batch: 000 / 011 | Total loss: 1.862 | Reg loss: 0.035 | Tree loss: 1.862 | Accuracy: 0.352500 | 1.553 sec/iter\n",
      "Epoch: 56 | Batch: 001 / 011 | Total loss: 1.818 | Reg loss: 0.035 | Tree loss: 1.818 | Accuracy: 0.364000 | 1.552 sec/iter\n",
      "Epoch: 56 | Batch: 002 / 011 | Total loss: 1.776 | Reg loss: 0.035 | Tree loss: 1.776 | Accuracy: 0.404000 | 1.552 sec/iter\n",
      "Epoch: 56 | Batch: 003 / 011 | Total loss: 1.721 | Reg loss: 0.035 | Tree loss: 1.721 | Accuracy: 0.439000 | 1.552 sec/iter\n",
      "Epoch: 56 | Batch: 004 / 011 | Total loss: 1.682 | Reg loss: 0.035 | Tree loss: 1.682 | Accuracy: 0.456000 | 1.552 sec/iter\n",
      "Epoch: 56 | Batch: 005 / 011 | Total loss: 1.675 | Reg loss: 0.035 | Tree loss: 1.675 | Accuracy: 0.446500 | 1.552 sec/iter\n",
      "Epoch: 56 | Batch: 006 / 011 | Total loss: 1.633 | Reg loss: 0.035 | Tree loss: 1.633 | Accuracy: 0.486500 | 1.552 sec/iter\n",
      "Epoch: 56 | Batch: 007 / 011 | Total loss: 1.617 | Reg loss: 0.035 | Tree loss: 1.617 | Accuracy: 0.454500 | 1.552 sec/iter\n",
      "Epoch: 56 | Batch: 008 / 011 | Total loss: 1.600 | Reg loss: 0.035 | Tree loss: 1.600 | Accuracy: 0.444500 | 1.552 sec/iter\n",
      "Epoch: 56 | Batch: 009 / 011 | Total loss: 1.599 | Reg loss: 0.035 | Tree loss: 1.599 | Accuracy: 0.452000 | 1.551 sec/iter\n",
      "Epoch: 56 | Batch: 010 / 011 | Total loss: 1.583 | Reg loss: 0.035 | Tree loss: 1.583 | Accuracy: 0.440273 | 1.55 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 57 | Batch: 000 / 011 | Total loss: 1.867 | Reg loss: 0.035 | Tree loss: 1.867 | Accuracy: 0.363000 | 1.563 sec/iter\n",
      "Epoch: 57 | Batch: 001 / 011 | Total loss: 1.818 | Reg loss: 0.035 | Tree loss: 1.818 | Accuracy: 0.366500 | 1.563 sec/iter\n",
      "Epoch: 57 | Batch: 002 / 011 | Total loss: 1.784 | Reg loss: 0.035 | Tree loss: 1.784 | Accuracy: 0.403000 | 1.563 sec/iter\n",
      "Epoch: 57 | Batch: 003 / 011 | Total loss: 1.728 | Reg loss: 0.035 | Tree loss: 1.728 | Accuracy: 0.417000 | 1.562 sec/iter\n",
      "Epoch: 57 | Batch: 004 / 011 | Total loss: 1.692 | Reg loss: 0.035 | Tree loss: 1.692 | Accuracy: 0.457000 | 1.561 sec/iter\n",
      "Epoch: 57 | Batch: 005 / 011 | Total loss: 1.641 | Reg loss: 0.035 | Tree loss: 1.641 | Accuracy: 0.462500 | 1.56 sec/iter\n",
      "Epoch: 57 | Batch: 006 / 011 | Total loss: 1.615 | Reg loss: 0.035 | Tree loss: 1.615 | Accuracy: 0.444000 | 1.559 sec/iter\n",
      "Epoch: 57 | Batch: 007 / 011 | Total loss: 1.603 | Reg loss: 0.035 | Tree loss: 1.603 | Accuracy: 0.436000 | 1.559 sec/iter\n",
      "Epoch: 57 | Batch: 008 / 011 | Total loss: 1.589 | Reg loss: 0.035 | Tree loss: 1.589 | Accuracy: 0.471500 | 1.558 sec/iter\n",
      "Epoch: 57 | Batch: 009 / 011 | Total loss: 1.590 | Reg loss: 0.035 | Tree loss: 1.590 | Accuracy: 0.456000 | 1.557 sec/iter\n",
      "Epoch: 57 | Batch: 010 / 011 | Total loss: 1.632 | Reg loss: 0.036 | Tree loss: 1.632 | Accuracy: 0.409556 | 1.556 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 58 | Batch: 000 / 011 | Total loss: 1.834 | Reg loss: 0.035 | Tree loss: 1.834 | Accuracy: 0.374500 | 1.564 sec/iter\n",
      "Epoch: 58 | Batch: 001 / 011 | Total loss: 1.818 | Reg loss: 0.035 | Tree loss: 1.818 | Accuracy: 0.381000 | 1.563 sec/iter\n",
      "Epoch: 58 | Batch: 002 / 011 | Total loss: 1.761 | Reg loss: 0.035 | Tree loss: 1.761 | Accuracy: 0.396000 | 1.563 sec/iter\n",
      "Epoch: 58 | Batch: 003 / 011 | Total loss: 1.708 | Reg loss: 0.035 | Tree loss: 1.708 | Accuracy: 0.428000 | 1.563 sec/iter\n",
      "Epoch: 58 | Batch: 004 / 011 | Total loss: 1.682 | Reg loss: 0.035 | Tree loss: 1.682 | Accuracy: 0.438500 | 1.563 sec/iter\n",
      "Epoch: 58 | Batch: 005 / 011 | Total loss: 1.650 | Reg loss: 0.035 | Tree loss: 1.650 | Accuracy: 0.464000 | 1.563 sec/iter\n",
      "Epoch: 58 | Batch: 006 / 011 | Total loss: 1.618 | Reg loss: 0.035 | Tree loss: 1.618 | Accuracy: 0.450000 | 1.562 sec/iter\n",
      "Epoch: 58 | Batch: 007 / 011 | Total loss: 1.614 | Reg loss: 0.036 | Tree loss: 1.614 | Accuracy: 0.441500 | 1.562 sec/iter\n",
      "Epoch: 58 | Batch: 008 / 011 | Total loss: 1.597 | Reg loss: 0.036 | Tree loss: 1.597 | Accuracy: 0.436000 | 1.562 sec/iter\n",
      "Epoch: 58 | Batch: 009 / 011 | Total loss: 1.580 | Reg loss: 0.036 | Tree loss: 1.580 | Accuracy: 0.475000 | 1.562 sec/iter\n",
      "Epoch: 58 | Batch: 010 / 011 | Total loss: 1.583 | Reg loss: 0.036 | Tree loss: 1.583 | Accuracy: 0.494881 | 1.561 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 59 | Batch: 000 / 011 | Total loss: 1.841 | Reg loss: 0.035 | Tree loss: 1.841 | Accuracy: 0.369500 | 1.566 sec/iter\n",
      "Epoch: 59 | Batch: 001 / 011 | Total loss: 1.802 | Reg loss: 0.035 | Tree loss: 1.802 | Accuracy: 0.379000 | 1.566 sec/iter\n",
      "Epoch: 59 | Batch: 002 / 011 | Total loss: 1.766 | Reg loss: 0.035 | Tree loss: 1.766 | Accuracy: 0.402500 | 1.566 sec/iter\n",
      "Epoch: 59 | Batch: 003 / 011 | Total loss: 1.701 | Reg loss: 0.035 | Tree loss: 1.701 | Accuracy: 0.445500 | 1.566 sec/iter\n",
      "Epoch: 59 | Batch: 004 / 011 | Total loss: 1.647 | Reg loss: 0.036 | Tree loss: 1.647 | Accuracy: 0.470500 | 1.566 sec/iter\n",
      "Epoch: 59 | Batch: 005 / 011 | Total loss: 1.649 | Reg loss: 0.036 | Tree loss: 1.649 | Accuracy: 0.469500 | 1.566 sec/iter\n",
      "Epoch: 59 | Batch: 006 / 011 | Total loss: 1.623 | Reg loss: 0.036 | Tree loss: 1.623 | Accuracy: 0.457000 | 1.566 sec/iter\n",
      "Epoch: 59 | Batch: 007 / 011 | Total loss: 1.612 | Reg loss: 0.036 | Tree loss: 1.612 | Accuracy: 0.438500 | 1.566 sec/iter\n",
      "Epoch: 59 | Batch: 008 / 011 | Total loss: 1.578 | Reg loss: 0.036 | Tree loss: 1.578 | Accuracy: 0.447500 | 1.565 sec/iter\n",
      "Epoch: 59 | Batch: 009 / 011 | Total loss: 1.577 | Reg loss: 0.036 | Tree loss: 1.577 | Accuracy: 0.453000 | 1.565 sec/iter\n",
      "Epoch: 59 | Batch: 010 / 011 | Total loss: 1.576 | Reg loss: 0.036 | Tree loss: 1.576 | Accuracy: 0.450512 | 1.564 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 8: 0.9723756906077345\n",
      "Epoch: 60 | Batch: 000 / 011 | Total loss: 1.844 | Reg loss: 0.036 | Tree loss: 1.844 | Accuracy: 0.350000 | 1.567 sec/iter\n",
      "Epoch: 60 | Batch: 001 / 011 | Total loss: 1.814 | Reg loss: 0.036 | Tree loss: 1.814 | Accuracy: 0.350000 | 1.567 sec/iter\n",
      "Epoch: 60 | Batch: 002 / 011 | Total loss: 1.743 | Reg loss: 0.036 | Tree loss: 1.743 | Accuracy: 0.418500 | 1.567 sec/iter\n",
      "Epoch: 60 | Batch: 003 / 011 | Total loss: 1.703 | Reg loss: 0.036 | Tree loss: 1.703 | Accuracy: 0.441500 | 1.567 sec/iter\n",
      "Epoch: 60 | Batch: 004 / 011 | Total loss: 1.650 | Reg loss: 0.036 | Tree loss: 1.650 | Accuracy: 0.470000 | 1.567 sec/iter\n",
      "Epoch: 60 | Batch: 005 / 011 | Total loss: 1.633 | Reg loss: 0.036 | Tree loss: 1.633 | Accuracy: 0.461000 | 1.566 sec/iter\n",
      "Epoch: 60 | Batch: 006 / 011 | Total loss: 1.611 | Reg loss: 0.036 | Tree loss: 1.611 | Accuracy: 0.460500 | 1.566 sec/iter\n",
      "Epoch: 60 | Batch: 007 / 011 | Total loss: 1.586 | Reg loss: 0.036 | Tree loss: 1.586 | Accuracy: 0.456000 | 1.566 sec/iter\n",
      "Epoch: 60 | Batch: 008 / 011 | Total loss: 1.592 | Reg loss: 0.036 | Tree loss: 1.592 | Accuracy: 0.452000 | 1.566 sec/iter\n",
      "Epoch: 60 | Batch: 009 / 011 | Total loss: 1.576 | Reg loss: 0.036 | Tree loss: 1.576 | Accuracy: 0.459500 | 1.566 sec/iter\n",
      "Epoch: 60 | Batch: 010 / 011 | Total loss: 1.582 | Reg loss: 0.036 | Tree loss: 1.582 | Accuracy: 0.426621 | 1.565 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 61 | Batch: 000 / 011 | Total loss: 1.819 | Reg loss: 0.036 | Tree loss: 1.819 | Accuracy: 0.377500 | 1.574 sec/iter\n",
      "Epoch: 61 | Batch: 001 / 011 | Total loss: 1.782 | Reg loss: 0.036 | Tree loss: 1.782 | Accuracy: 0.373500 | 1.574 sec/iter\n",
      "Epoch: 61 | Batch: 002 / 011 | Total loss: 1.757 | Reg loss: 0.036 | Tree loss: 1.757 | Accuracy: 0.383000 | 1.573 sec/iter\n",
      "Epoch: 61 | Batch: 003 / 011 | Total loss: 1.689 | Reg loss: 0.036 | Tree loss: 1.689 | Accuracy: 0.430000 | 1.573 sec/iter\n",
      "Epoch: 61 | Batch: 004 / 011 | Total loss: 1.676 | Reg loss: 0.036 | Tree loss: 1.676 | Accuracy: 0.469500 | 1.573 sec/iter\n",
      "Epoch: 61 | Batch: 005 / 011 | Total loss: 1.627 | Reg loss: 0.036 | Tree loss: 1.627 | Accuracy: 0.469500 | 1.573 sec/iter\n",
      "Epoch: 61 | Batch: 006 / 011 | Total loss: 1.602 | Reg loss: 0.036 | Tree loss: 1.602 | Accuracy: 0.471500 | 1.573 sec/iter\n",
      "Epoch: 61 | Batch: 007 / 011 | Total loss: 1.586 | Reg loss: 0.036 | Tree loss: 1.586 | Accuracy: 0.450000 | 1.573 sec/iter\n",
      "Epoch: 61 | Batch: 008 / 011 | Total loss: 1.593 | Reg loss: 0.036 | Tree loss: 1.593 | Accuracy: 0.457000 | 1.573 sec/iter\n",
      "Epoch: 61 | Batch: 009 / 011 | Total loss: 1.560 | Reg loss: 0.036 | Tree loss: 1.560 | Accuracy: 0.461000 | 1.573 sec/iter\n",
      "Epoch: 61 | Batch: 010 / 011 | Total loss: 1.541 | Reg loss: 0.036 | Tree loss: 1.541 | Accuracy: 0.460751 | 1.572 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 62 | Batch: 000 / 011 | Total loss: 1.826 | Reg loss: 0.036 | Tree loss: 1.826 | Accuracy: 0.365500 | 1.572 sec/iter\n",
      "Epoch: 62 | Batch: 001 / 011 | Total loss: 1.778 | Reg loss: 0.036 | Tree loss: 1.778 | Accuracy: 0.407000 | 1.572 sec/iter\n",
      "Epoch: 62 | Batch: 002 / 011 | Total loss: 1.748 | Reg loss: 0.036 | Tree loss: 1.748 | Accuracy: 0.419500 | 1.572 sec/iter\n",
      "Epoch: 62 | Batch: 003 / 011 | Total loss: 1.682 | Reg loss: 0.036 | Tree loss: 1.682 | Accuracy: 0.451000 | 1.572 sec/iter\n",
      "Epoch: 62 | Batch: 004 / 011 | Total loss: 1.651 | Reg loss: 0.036 | Tree loss: 1.651 | Accuracy: 0.450500 | 1.572 sec/iter\n",
      "Epoch: 62 | Batch: 005 / 011 | Total loss: 1.618 | Reg loss: 0.036 | Tree loss: 1.618 | Accuracy: 0.463500 | 1.572 sec/iter\n",
      "Epoch: 62 | Batch: 006 / 011 | Total loss: 1.595 | Reg loss: 0.036 | Tree loss: 1.595 | Accuracy: 0.451500 | 1.572 sec/iter\n",
      "Epoch: 62 | Batch: 007 / 011 | Total loss: 1.595 | Reg loss: 0.036 | Tree loss: 1.595 | Accuracy: 0.439500 | 1.572 sec/iter\n",
      "Epoch: 62 | Batch: 008 / 011 | Total loss: 1.587 | Reg loss: 0.036 | Tree loss: 1.587 | Accuracy: 0.448000 | 1.571 sec/iter\n",
      "Epoch: 62 | Batch: 009 / 011 | Total loss: 1.572 | Reg loss: 0.036 | Tree loss: 1.572 | Accuracy: 0.442000 | 1.571 sec/iter\n",
      "Epoch: 62 | Batch: 010 / 011 | Total loss: 1.530 | Reg loss: 0.036 | Tree loss: 1.530 | Accuracy: 0.447099 | 1.57 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 63 | Batch: 000 / 011 | Total loss: 1.825 | Reg loss: 0.036 | Tree loss: 1.825 | Accuracy: 0.355000 | 1.581 sec/iter\n",
      "Epoch: 63 | Batch: 001 / 011 | Total loss: 1.805 | Reg loss: 0.036 | Tree loss: 1.805 | Accuracy: 0.363500 | 1.581 sec/iter\n",
      "Epoch: 63 | Batch: 002 / 011 | Total loss: 1.731 | Reg loss: 0.036 | Tree loss: 1.731 | Accuracy: 0.416000 | 1.581 sec/iter\n",
      "Epoch: 63 | Batch: 003 / 011 | Total loss: 1.691 | Reg loss: 0.036 | Tree loss: 1.691 | Accuracy: 0.436000 | 1.58 sec/iter\n",
      "Epoch: 63 | Batch: 004 / 011 | Total loss: 1.661 | Reg loss: 0.036 | Tree loss: 1.661 | Accuracy: 0.439500 | 1.581 sec/iter\n",
      "Epoch: 63 | Batch: 005 / 011 | Total loss: 1.599 | Reg loss: 0.036 | Tree loss: 1.599 | Accuracy: 0.460000 | 1.58 sec/iter\n",
      "Epoch: 63 | Batch: 006 / 011 | Total loss: 1.594 | Reg loss: 0.036 | Tree loss: 1.594 | Accuracy: 0.460500 | 1.58 sec/iter\n",
      "Epoch: 63 | Batch: 007 / 011 | Total loss: 1.580 | Reg loss: 0.036 | Tree loss: 1.580 | Accuracy: 0.467000 | 1.58 sec/iter\n",
      "Epoch: 63 | Batch: 008 / 011 | Total loss: 1.556 | Reg loss: 0.036 | Tree loss: 1.556 | Accuracy: 0.448000 | 1.58 sec/iter\n",
      "Epoch: 63 | Batch: 009 / 011 | Total loss: 1.562 | Reg loss: 0.036 | Tree loss: 1.562 | Accuracy: 0.461500 | 1.58 sec/iter\n",
      "Epoch: 63 | Batch: 010 / 011 | Total loss: 1.590 | Reg loss: 0.036 | Tree loss: 1.590 | Accuracy: 0.416382 | 1.579 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 64 | Batch: 000 / 011 | Total loss: 1.791 | Reg loss: 0.036 | Tree loss: 1.791 | Accuracy: 0.371500 | 1.58 sec/iter\n",
      "Epoch: 64 | Batch: 001 / 011 | Total loss: 1.783 | Reg loss: 0.036 | Tree loss: 1.783 | Accuracy: 0.388000 | 1.58 sec/iter\n",
      "Epoch: 64 | Batch: 002 / 011 | Total loss: 1.719 | Reg loss: 0.036 | Tree loss: 1.719 | Accuracy: 0.431000 | 1.579 sec/iter\n",
      "Epoch: 64 | Batch: 003 / 011 | Total loss: 1.657 | Reg loss: 0.036 | Tree loss: 1.657 | Accuracy: 0.458500 | 1.579 sec/iter\n",
      "Epoch: 64 | Batch: 004 / 011 | Total loss: 1.651 | Reg loss: 0.036 | Tree loss: 1.651 | Accuracy: 0.450500 | 1.579 sec/iter\n",
      "Epoch: 64 | Batch: 005 / 011 | Total loss: 1.619 | Reg loss: 0.036 | Tree loss: 1.619 | Accuracy: 0.465000 | 1.579 sec/iter\n",
      "Epoch: 64 | Batch: 006 / 011 | Total loss: 1.602 | Reg loss: 0.036 | Tree loss: 1.602 | Accuracy: 0.457000 | 1.579 sec/iter\n",
      "Epoch: 64 | Batch: 007 / 011 | Total loss: 1.576 | Reg loss: 0.036 | Tree loss: 1.576 | Accuracy: 0.444500 | 1.579 sec/iter\n",
      "Epoch: 64 | Batch: 008 / 011 | Total loss: 1.564 | Reg loss: 0.036 | Tree loss: 1.564 | Accuracy: 0.445500 | 1.579 sec/iter\n",
      "Epoch: 64 | Batch: 009 / 011 | Total loss: 1.554 | Reg loss: 0.036 | Tree loss: 1.554 | Accuracy: 0.446500 | 1.578 sec/iter\n",
      "Epoch: 64 | Batch: 010 / 011 | Total loss: 1.623 | Reg loss: 0.036 | Tree loss: 1.623 | Accuracy: 0.416382 | 1.577 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 65 | Batch: 000 / 011 | Total loss: 1.812 | Reg loss: 0.036 | Tree loss: 1.812 | Accuracy: 0.376500 | 1.579 sec/iter\n",
      "Epoch: 65 | Batch: 001 / 011 | Total loss: 1.774 | Reg loss: 0.036 | Tree loss: 1.774 | Accuracy: 0.372500 | 1.579 sec/iter\n",
      "Epoch: 65 | Batch: 002 / 011 | Total loss: 1.729 | Reg loss: 0.036 | Tree loss: 1.729 | Accuracy: 0.422500 | 1.578 sec/iter\n",
      "Epoch: 65 | Batch: 003 / 011 | Total loss: 1.688 | Reg loss: 0.036 | Tree loss: 1.688 | Accuracy: 0.453000 | 1.577 sec/iter\n",
      "Epoch: 65 | Batch: 004 / 011 | Total loss: 1.647 | Reg loss: 0.036 | Tree loss: 1.647 | Accuracy: 0.452000 | 1.576 sec/iter\n",
      "Epoch: 65 | Batch: 005 / 011 | Total loss: 1.607 | Reg loss: 0.036 | Tree loss: 1.607 | Accuracy: 0.468000 | 1.576 sec/iter\n",
      "Epoch: 65 | Batch: 006 / 011 | Total loss: 1.583 | Reg loss: 0.036 | Tree loss: 1.583 | Accuracy: 0.467500 | 1.575 sec/iter\n",
      "Epoch: 65 | Batch: 007 / 011 | Total loss: 1.591 | Reg loss: 0.036 | Tree loss: 1.591 | Accuracy: 0.437000 | 1.575 sec/iter\n",
      "Epoch: 65 | Batch: 008 / 011 | Total loss: 1.543 | Reg loss: 0.036 | Tree loss: 1.543 | Accuracy: 0.472000 | 1.575 sec/iter\n",
      "Epoch: 65 | Batch: 009 / 011 | Total loss: 1.553 | Reg loss: 0.036 | Tree loss: 1.553 | Accuracy: 0.460500 | 1.574 sec/iter\n",
      "Epoch: 65 | Batch: 010 / 011 | Total loss: 1.522 | Reg loss: 0.036 | Tree loss: 1.522 | Accuracy: 0.481229 | 1.573 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 66 | Batch: 000 / 011 | Total loss: 1.773 | Reg loss: 0.036 | Tree loss: 1.773 | Accuracy: 0.373500 | 1.581 sec/iter\n",
      "Epoch: 66 | Batch: 001 / 011 | Total loss: 1.800 | Reg loss: 0.036 | Tree loss: 1.800 | Accuracy: 0.376000 | 1.581 sec/iter\n",
      "Epoch: 66 | Batch: 002 / 011 | Total loss: 1.707 | Reg loss: 0.036 | Tree loss: 1.707 | Accuracy: 0.425000 | 1.581 sec/iter\n",
      "Epoch: 66 | Batch: 003 / 011 | Total loss: 1.655 | Reg loss: 0.036 | Tree loss: 1.655 | Accuracy: 0.462000 | 1.581 sec/iter\n",
      "Epoch: 66 | Batch: 004 / 011 | Total loss: 1.617 | Reg loss: 0.036 | Tree loss: 1.617 | Accuracy: 0.471000 | 1.581 sec/iter\n",
      "Epoch: 66 | Batch: 005 / 011 | Total loss: 1.592 | Reg loss: 0.036 | Tree loss: 1.592 | Accuracy: 0.473000 | 1.581 sec/iter\n",
      "Epoch: 66 | Batch: 006 / 011 | Total loss: 1.595 | Reg loss: 0.036 | Tree loss: 1.595 | Accuracy: 0.464500 | 1.581 sec/iter\n",
      "Epoch: 66 | Batch: 007 / 011 | Total loss: 1.583 | Reg loss: 0.036 | Tree loss: 1.583 | Accuracy: 0.449000 | 1.58 sec/iter\n",
      "Epoch: 66 | Batch: 008 / 011 | Total loss: 1.569 | Reg loss: 0.036 | Tree loss: 1.569 | Accuracy: 0.442000 | 1.58 sec/iter\n",
      "Epoch: 66 | Batch: 009 / 011 | Total loss: 1.562 | Reg loss: 0.036 | Tree loss: 1.562 | Accuracy: 0.444500 | 1.58 sec/iter\n",
      "Epoch: 66 | Batch: 010 / 011 | Total loss: 1.536 | Reg loss: 0.036 | Tree loss: 1.536 | Accuracy: 0.447099 | 1.579 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 67 | Batch: 000 / 011 | Total loss: 1.788 | Reg loss: 0.036 | Tree loss: 1.788 | Accuracy: 0.379500 | 1.585 sec/iter\n",
      "Epoch: 67 | Batch: 001 / 011 | Total loss: 1.764 | Reg loss: 0.036 | Tree loss: 1.764 | Accuracy: 0.393000 | 1.585 sec/iter\n",
      "Epoch: 67 | Batch: 002 / 011 | Total loss: 1.704 | Reg loss: 0.036 | Tree loss: 1.704 | Accuracy: 0.440500 | 1.585 sec/iter\n",
      "Epoch: 67 | Batch: 003 / 011 | Total loss: 1.677 | Reg loss: 0.036 | Tree loss: 1.677 | Accuracy: 0.438500 | 1.584 sec/iter\n",
      "Epoch: 67 | Batch: 004 / 011 | Total loss: 1.619 | Reg loss: 0.036 | Tree loss: 1.619 | Accuracy: 0.461000 | 1.584 sec/iter\n",
      "Epoch: 67 | Batch: 005 / 011 | Total loss: 1.588 | Reg loss: 0.036 | Tree loss: 1.588 | Accuracy: 0.492000 | 1.584 sec/iter\n",
      "Epoch: 67 | Batch: 006 / 011 | Total loss: 1.575 | Reg loss: 0.036 | Tree loss: 1.575 | Accuracy: 0.462000 | 1.584 sec/iter\n",
      "Epoch: 67 | Batch: 007 / 011 | Total loss: 1.571 | Reg loss: 0.036 | Tree loss: 1.571 | Accuracy: 0.463000 | 1.583 sec/iter\n",
      "Epoch: 67 | Batch: 008 / 011 | Total loss: 1.549 | Reg loss: 0.036 | Tree loss: 1.549 | Accuracy: 0.446500 | 1.583 sec/iter\n",
      "Epoch: 67 | Batch: 009 / 011 | Total loss: 1.554 | Reg loss: 0.036 | Tree loss: 1.554 | Accuracy: 0.448500 | 1.583 sec/iter\n",
      "Epoch: 67 | Batch: 010 / 011 | Total loss: 1.577 | Reg loss: 0.036 | Tree loss: 1.577 | Accuracy: 0.433447 | 1.582 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 68 | Batch: 000 / 011 | Total loss: 1.772 | Reg loss: 0.036 | Tree loss: 1.772 | Accuracy: 0.381000 | 1.588 sec/iter\n",
      "Epoch: 68 | Batch: 001 / 011 | Total loss: 1.740 | Reg loss: 0.036 | Tree loss: 1.740 | Accuracy: 0.394500 | 1.587 sec/iter\n",
      "Epoch: 68 | Batch: 002 / 011 | Total loss: 1.697 | Reg loss: 0.036 | Tree loss: 1.697 | Accuracy: 0.437000 | 1.587 sec/iter\n",
      "Epoch: 68 | Batch: 003 / 011 | Total loss: 1.671 | Reg loss: 0.036 | Tree loss: 1.671 | Accuracy: 0.442500 | 1.587 sec/iter\n",
      "Epoch: 68 | Batch: 004 / 011 | Total loss: 1.621 | Reg loss: 0.036 | Tree loss: 1.621 | Accuracy: 0.455000 | 1.587 sec/iter\n",
      "Epoch: 68 | Batch: 005 / 011 | Total loss: 1.606 | Reg loss: 0.036 | Tree loss: 1.606 | Accuracy: 0.473000 | 1.587 sec/iter\n",
      "Epoch: 68 | Batch: 006 / 011 | Total loss: 1.573 | Reg loss: 0.036 | Tree loss: 1.573 | Accuracy: 0.451000 | 1.587 sec/iter\n",
      "Epoch: 68 | Batch: 007 / 011 | Total loss: 1.553 | Reg loss: 0.036 | Tree loss: 1.553 | Accuracy: 0.458500 | 1.586 sec/iter\n",
      "Epoch: 68 | Batch: 008 / 011 | Total loss: 1.550 | Reg loss: 0.036 | Tree loss: 1.550 | Accuracy: 0.459500 | 1.587 sec/iter\n",
      "Epoch: 68 | Batch: 009 / 011 | Total loss: 1.554 | Reg loss: 0.036 | Tree loss: 1.554 | Accuracy: 0.446500 | 1.587 sec/iter\n",
      "Epoch: 68 | Batch: 010 / 011 | Total loss: 1.515 | Reg loss: 0.036 | Tree loss: 1.515 | Accuracy: 0.511945 | 1.585 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 69 | Batch: 000 / 011 | Total loss: 1.782 | Reg loss: 0.036 | Tree loss: 1.782 | Accuracy: 0.387000 | 1.586 sec/iter\n",
      "Epoch: 69 | Batch: 001 / 011 | Total loss: 1.741 | Reg loss: 0.036 | Tree loss: 1.741 | Accuracy: 0.423000 | 1.586 sec/iter\n",
      "Epoch: 69 | Batch: 002 / 011 | Total loss: 1.697 | Reg loss: 0.036 | Tree loss: 1.697 | Accuracy: 0.435500 | 1.586 sec/iter\n",
      "Epoch: 69 | Batch: 003 / 011 | Total loss: 1.647 | Reg loss: 0.036 | Tree loss: 1.647 | Accuracy: 0.451000 | 1.585 sec/iter\n",
      "Epoch: 69 | Batch: 004 / 011 | Total loss: 1.622 | Reg loss: 0.036 | Tree loss: 1.622 | Accuracy: 0.451500 | 1.586 sec/iter\n",
      "Epoch: 69 | Batch: 005 / 011 | Total loss: 1.608 | Reg loss: 0.036 | Tree loss: 1.608 | Accuracy: 0.450500 | 1.585 sec/iter\n",
      "Epoch: 69 | Batch: 006 / 011 | Total loss: 1.565 | Reg loss: 0.036 | Tree loss: 1.565 | Accuracy: 0.460500 | 1.585 sec/iter\n",
      "Epoch: 69 | Batch: 007 / 011 | Total loss: 1.546 | Reg loss: 0.036 | Tree loss: 1.546 | Accuracy: 0.469500 | 1.585 sec/iter\n",
      "Epoch: 69 | Batch: 008 / 011 | Total loss: 1.552 | Reg loss: 0.036 | Tree loss: 1.552 | Accuracy: 0.456500 | 1.585 sec/iter\n",
      "Epoch: 69 | Batch: 009 / 011 | Total loss: 1.550 | Reg loss: 0.036 | Tree loss: 1.550 | Accuracy: 0.457500 | 1.585 sec/iter\n",
      "Epoch: 69 | Batch: 010 / 011 | Total loss: 1.550 | Reg loss: 0.036 | Tree loss: 1.550 | Accuracy: 0.450512 | 1.584 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 70 | Batch: 000 / 011 | Total loss: 1.758 | Reg loss: 0.036 | Tree loss: 1.758 | Accuracy: 0.409500 | 1.594 sec/iter\n",
      "Epoch: 70 | Batch: 001 / 011 | Total loss: 1.738 | Reg loss: 0.036 | Tree loss: 1.738 | Accuracy: 0.401500 | 1.594 sec/iter\n",
      "Epoch: 70 | Batch: 002 / 011 | Total loss: 1.690 | Reg loss: 0.036 | Tree loss: 1.690 | Accuracy: 0.444000 | 1.594 sec/iter\n",
      "Epoch: 70 | Batch: 003 / 011 | Total loss: 1.638 | Reg loss: 0.036 | Tree loss: 1.638 | Accuracy: 0.460000 | 1.593 sec/iter\n",
      "Epoch: 70 | Batch: 004 / 011 | Total loss: 1.616 | Reg loss: 0.036 | Tree loss: 1.616 | Accuracy: 0.477000 | 1.593 sec/iter\n",
      "Epoch: 70 | Batch: 005 / 011 | Total loss: 1.587 | Reg loss: 0.036 | Tree loss: 1.587 | Accuracy: 0.454500 | 1.593 sec/iter\n",
      "Epoch: 70 | Batch: 006 / 011 | Total loss: 1.581 | Reg loss: 0.036 | Tree loss: 1.581 | Accuracy: 0.448000 | 1.593 sec/iter\n",
      "Epoch: 70 | Batch: 007 / 011 | Total loss: 1.554 | Reg loss: 0.036 | Tree loss: 1.554 | Accuracy: 0.461000 | 1.593 sec/iter\n",
      "Epoch: 70 | Batch: 008 / 011 | Total loss: 1.549 | Reg loss: 0.036 | Tree loss: 1.549 | Accuracy: 0.458000 | 1.593 sec/iter\n",
      "Epoch: 70 | Batch: 009 / 011 | Total loss: 1.537 | Reg loss: 0.036 | Tree loss: 1.537 | Accuracy: 0.458500 | 1.593 sec/iter\n",
      "Epoch: 70 | Batch: 010 / 011 | Total loss: 1.511 | Reg loss: 0.036 | Tree loss: 1.511 | Accuracy: 0.535836 | 1.592 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 71 | Batch: 000 / 011 | Total loss: 1.759 | Reg loss: 0.036 | Tree loss: 1.759 | Accuracy: 0.387500 | 1.593 sec/iter\n",
      "Epoch: 71 | Batch: 001 / 011 | Total loss: 1.718 | Reg loss: 0.036 | Tree loss: 1.718 | Accuracy: 0.414500 | 1.593 sec/iter\n",
      "Epoch: 71 | Batch: 002 / 011 | Total loss: 1.691 | Reg loss: 0.036 | Tree loss: 1.691 | Accuracy: 0.412500 | 1.592 sec/iter\n",
      "Epoch: 71 | Batch: 003 / 011 | Total loss: 1.653 | Reg loss: 0.036 | Tree loss: 1.653 | Accuracy: 0.449500 | 1.592 sec/iter\n",
      "Epoch: 71 | Batch: 004 / 011 | Total loss: 1.603 | Reg loss: 0.036 | Tree loss: 1.603 | Accuracy: 0.467500 | 1.592 sec/iter\n",
      "Epoch: 71 | Batch: 005 / 011 | Total loss: 1.582 | Reg loss: 0.036 | Tree loss: 1.582 | Accuracy: 0.481500 | 1.592 sec/iter\n",
      "Epoch: 71 | Batch: 006 / 011 | Total loss: 1.567 | Reg loss: 0.036 | Tree loss: 1.567 | Accuracy: 0.466000 | 1.592 sec/iter\n",
      "Epoch: 71 | Batch: 007 / 011 | Total loss: 1.557 | Reg loss: 0.036 | Tree loss: 1.557 | Accuracy: 0.456500 | 1.592 sec/iter\n",
      "Epoch: 71 | Batch: 008 / 011 | Total loss: 1.537 | Reg loss: 0.037 | Tree loss: 1.537 | Accuracy: 0.458000 | 1.592 sec/iter\n",
      "Epoch: 71 | Batch: 009 / 011 | Total loss: 1.533 | Reg loss: 0.037 | Tree loss: 1.533 | Accuracy: 0.462500 | 1.591 sec/iter\n",
      "Epoch: 71 | Batch: 010 / 011 | Total loss: 1.585 | Reg loss: 0.037 | Tree loss: 1.585 | Accuracy: 0.419795 | 1.59 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 72 | Batch: 000 / 011 | Total loss: 1.747 | Reg loss: 0.036 | Tree loss: 1.747 | Accuracy: 0.404500 | 1.6 sec/iter\n",
      "Epoch: 72 | Batch: 001 / 011 | Total loss: 1.711 | Reg loss: 0.036 | Tree loss: 1.711 | Accuracy: 0.388000 | 1.6 sec/iter\n",
      "Epoch: 72 | Batch: 002 / 011 | Total loss: 1.663 | Reg loss: 0.036 | Tree loss: 1.663 | Accuracy: 0.448000 | 1.599 sec/iter\n",
      "Epoch: 72 | Batch: 003 / 011 | Total loss: 1.657 | Reg loss: 0.036 | Tree loss: 1.657 | Accuracy: 0.436500 | 1.599 sec/iter\n",
      "Epoch: 72 | Batch: 004 / 011 | Total loss: 1.625 | Reg loss: 0.036 | Tree loss: 1.625 | Accuracy: 0.458000 | 1.598 sec/iter\n",
      "Epoch: 72 | Batch: 005 / 011 | Total loss: 1.575 | Reg loss: 0.036 | Tree loss: 1.575 | Accuracy: 0.465500 | 1.597 sec/iter\n",
      "Epoch: 72 | Batch: 006 / 011 | Total loss: 1.566 | Reg loss: 0.037 | Tree loss: 1.566 | Accuracy: 0.456000 | 1.597 sec/iter\n",
      "Epoch: 72 | Batch: 007 / 011 | Total loss: 1.565 | Reg loss: 0.037 | Tree loss: 1.565 | Accuracy: 0.447500 | 1.596 sec/iter\n",
      "Epoch: 72 | Batch: 008 / 011 | Total loss: 1.518 | Reg loss: 0.037 | Tree loss: 1.518 | Accuracy: 0.460500 | 1.595 sec/iter\n",
      "Epoch: 72 | Batch: 009 / 011 | Total loss: 1.541 | Reg loss: 0.037 | Tree loss: 1.541 | Accuracy: 0.452500 | 1.595 sec/iter\n",
      "Epoch: 72 | Batch: 010 / 011 | Total loss: 1.542 | Reg loss: 0.037 | Tree loss: 1.542 | Accuracy: 0.385666 | 1.594 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 73 | Batch: 000 / 011 | Total loss: 1.757 | Reg loss: 0.036 | Tree loss: 1.757 | Accuracy: 0.380000 | 1.602 sec/iter\n",
      "Epoch: 73 | Batch: 001 / 011 | Total loss: 1.730 | Reg loss: 0.036 | Tree loss: 1.730 | Accuracy: 0.394500 | 1.602 sec/iter\n",
      "Epoch: 73 | Batch: 002 / 011 | Total loss: 1.678 | Reg loss: 0.036 | Tree loss: 1.678 | Accuracy: 0.432500 | 1.601 sec/iter\n",
      "Epoch: 73 | Batch: 003 / 011 | Total loss: 1.620 | Reg loss: 0.036 | Tree loss: 1.620 | Accuracy: 0.471500 | 1.601 sec/iter\n",
      "Epoch: 73 | Batch: 004 / 011 | Total loss: 1.599 | Reg loss: 0.037 | Tree loss: 1.599 | Accuracy: 0.476500 | 1.601 sec/iter\n",
      "Epoch: 73 | Batch: 005 / 011 | Total loss: 1.551 | Reg loss: 0.037 | Tree loss: 1.551 | Accuracy: 0.497500 | 1.601 sec/iter\n",
      "Epoch: 73 | Batch: 006 / 011 | Total loss: 1.560 | Reg loss: 0.037 | Tree loss: 1.560 | Accuracy: 0.450500 | 1.601 sec/iter\n",
      "Epoch: 73 | Batch: 007 / 011 | Total loss: 1.550 | Reg loss: 0.037 | Tree loss: 1.550 | Accuracy: 0.464000 | 1.601 sec/iter\n",
      "Epoch: 73 | Batch: 008 / 011 | Total loss: 1.542 | Reg loss: 0.037 | Tree loss: 1.542 | Accuracy: 0.456500 | 1.601 sec/iter\n",
      "Epoch: 73 | Batch: 009 / 011 | Total loss: 1.536 | Reg loss: 0.037 | Tree loss: 1.536 | Accuracy: 0.455500 | 1.601 sec/iter\n",
      "Epoch: 73 | Batch: 010 / 011 | Total loss: 1.507 | Reg loss: 0.037 | Tree loss: 1.507 | Accuracy: 0.464164 | 1.599 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 74 | Batch: 000 / 011 | Total loss: 1.745 | Reg loss: 0.037 | Tree loss: 1.745 | Accuracy: 0.407500 | 1.601 sec/iter\n",
      "Epoch: 74 | Batch: 001 / 011 | Total loss: 1.715 | Reg loss: 0.037 | Tree loss: 1.715 | Accuracy: 0.393000 | 1.601 sec/iter\n",
      "Epoch: 74 | Batch: 002 / 011 | Total loss: 1.667 | Reg loss: 0.037 | Tree loss: 1.667 | Accuracy: 0.440500 | 1.601 sec/iter\n",
      "Epoch: 74 | Batch: 003 / 011 | Total loss: 1.634 | Reg loss: 0.037 | Tree loss: 1.634 | Accuracy: 0.437500 | 1.601 sec/iter\n",
      "Epoch: 74 | Batch: 004 / 011 | Total loss: 1.612 | Reg loss: 0.037 | Tree loss: 1.612 | Accuracy: 0.479000 | 1.601 sec/iter\n",
      "Epoch: 74 | Batch: 005 / 011 | Total loss: 1.579 | Reg loss: 0.037 | Tree loss: 1.579 | Accuracy: 0.478000 | 1.601 sec/iter\n",
      "Epoch: 74 | Batch: 006 / 011 | Total loss: 1.546 | Reg loss: 0.037 | Tree loss: 1.546 | Accuracy: 0.464500 | 1.6 sec/iter\n",
      "Epoch: 74 | Batch: 007 / 011 | Total loss: 1.528 | Reg loss: 0.037 | Tree loss: 1.528 | Accuracy: 0.469500 | 1.6 sec/iter\n",
      "Epoch: 74 | Batch: 008 / 011 | Total loss: 1.539 | Reg loss: 0.037 | Tree loss: 1.539 | Accuracy: 0.434000 | 1.6 sec/iter\n",
      "Epoch: 74 | Batch: 009 / 011 | Total loss: 1.524 | Reg loss: 0.037 | Tree loss: 1.524 | Accuracy: 0.468500 | 1.6 sec/iter\n",
      "Epoch: 74 | Batch: 010 / 011 | Total loss: 1.491 | Reg loss: 0.037 | Tree loss: 1.491 | Accuracy: 0.494881 | 1.599 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 75 | Batch: 000 / 011 | Total loss: 1.765 | Reg loss: 0.037 | Tree loss: 1.765 | Accuracy: 0.382500 | 1.606 sec/iter\n",
      "Epoch: 75 | Batch: 001 / 011 | Total loss: 1.718 | Reg loss: 0.037 | Tree loss: 1.718 | Accuracy: 0.403500 | 1.606 sec/iter\n",
      "Epoch: 75 | Batch: 002 / 011 | Total loss: 1.663 | Reg loss: 0.037 | Tree loss: 1.663 | Accuracy: 0.424500 | 1.605 sec/iter\n",
      "Epoch: 75 | Batch: 003 / 011 | Total loss: 1.602 | Reg loss: 0.037 | Tree loss: 1.602 | Accuracy: 0.466500 | 1.605 sec/iter\n",
      "Epoch: 75 | Batch: 004 / 011 | Total loss: 1.589 | Reg loss: 0.037 | Tree loss: 1.589 | Accuracy: 0.473000 | 1.605 sec/iter\n",
      "Epoch: 75 | Batch: 005 / 011 | Total loss: 1.571 | Reg loss: 0.037 | Tree loss: 1.571 | Accuracy: 0.473500 | 1.605 sec/iter\n",
      "Epoch: 75 | Batch: 006 / 011 | Total loss: 1.521 | Reg loss: 0.037 | Tree loss: 1.521 | Accuracy: 0.489500 | 1.604 sec/iter\n",
      "Epoch: 75 | Batch: 007 / 011 | Total loss: 1.529 | Reg loss: 0.037 | Tree loss: 1.529 | Accuracy: 0.465500 | 1.604 sec/iter\n",
      "Epoch: 75 | Batch: 008 / 011 | Total loss: 1.538 | Reg loss: 0.037 | Tree loss: 1.538 | Accuracy: 0.465000 | 1.604 sec/iter\n",
      "Epoch: 75 | Batch: 009 / 011 | Total loss: 1.546 | Reg loss: 0.037 | Tree loss: 1.546 | Accuracy: 0.450500 | 1.604 sec/iter\n",
      "Epoch: 75 | Batch: 010 / 011 | Total loss: 1.553 | Reg loss: 0.037 | Tree loss: 1.553 | Accuracy: 0.440273 | 1.603 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 76 | Batch: 000 / 011 | Total loss: 1.751 | Reg loss: 0.037 | Tree loss: 1.751 | Accuracy: 0.387000 | 1.606 sec/iter\n",
      "Epoch: 76 | Batch: 001 / 011 | Total loss: 1.709 | Reg loss: 0.037 | Tree loss: 1.709 | Accuracy: 0.406500 | 1.606 sec/iter\n",
      "Epoch: 76 | Batch: 002 / 011 | Total loss: 1.676 | Reg loss: 0.037 | Tree loss: 1.676 | Accuracy: 0.427000 | 1.606 sec/iter\n",
      "Epoch: 76 | Batch: 003 / 011 | Total loss: 1.641 | Reg loss: 0.037 | Tree loss: 1.641 | Accuracy: 0.434000 | 1.606 sec/iter\n",
      "Epoch: 76 | Batch: 004 / 011 | Total loss: 1.580 | Reg loss: 0.037 | Tree loss: 1.580 | Accuracy: 0.452000 | 1.606 sec/iter\n",
      "Epoch: 76 | Batch: 005 / 011 | Total loss: 1.566 | Reg loss: 0.037 | Tree loss: 1.566 | Accuracy: 0.478000 | 1.606 sec/iter\n",
      "Epoch: 76 | Batch: 006 / 011 | Total loss: 1.563 | Reg loss: 0.037 | Tree loss: 1.563 | Accuracy: 0.466500 | 1.605 sec/iter\n",
      "Epoch: 76 | Batch: 007 / 011 | Total loss: 1.540 | Reg loss: 0.037 | Tree loss: 1.540 | Accuracy: 0.457500 | 1.605 sec/iter\n",
      "Epoch: 76 | Batch: 008 / 011 | Total loss: 1.528 | Reg loss: 0.037 | Tree loss: 1.528 | Accuracy: 0.455500 | 1.605 sec/iter\n",
      "Epoch: 76 | Batch: 009 / 011 | Total loss: 1.483 | Reg loss: 0.037 | Tree loss: 1.483 | Accuracy: 0.491000 | 1.605 sec/iter\n",
      "Epoch: 76 | Batch: 010 / 011 | Total loss: 1.526 | Reg loss: 0.037 | Tree loss: 1.526 | Accuracy: 0.433447 | 1.604 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 77 | Batch: 000 / 011 | Total loss: 1.721 | Reg loss: 0.037 | Tree loss: 1.721 | Accuracy: 0.391500 | 1.605 sec/iter\n",
      "Epoch: 77 | Batch: 001 / 011 | Total loss: 1.697 | Reg loss: 0.037 | Tree loss: 1.697 | Accuracy: 0.395000 | 1.604 sec/iter\n",
      "Epoch: 77 | Batch: 002 / 011 | Total loss: 1.676 | Reg loss: 0.037 | Tree loss: 1.676 | Accuracy: 0.420000 | 1.604 sec/iter\n",
      "Epoch: 77 | Batch: 003 / 011 | Total loss: 1.615 | Reg loss: 0.037 | Tree loss: 1.615 | Accuracy: 0.460000 | 1.604 sec/iter\n",
      "Epoch: 77 | Batch: 004 / 011 | Total loss: 1.592 | Reg loss: 0.037 | Tree loss: 1.592 | Accuracy: 0.467500 | 1.604 sec/iter\n",
      "Epoch: 77 | Batch: 005 / 011 | Total loss: 1.557 | Reg loss: 0.037 | Tree loss: 1.557 | Accuracy: 0.478500 | 1.604 sec/iter\n",
      "Epoch: 77 | Batch: 006 / 011 | Total loss: 1.548 | Reg loss: 0.037 | Tree loss: 1.548 | Accuracy: 0.477000 | 1.604 sec/iter\n",
      "Epoch: 77 | Batch: 007 / 011 | Total loss: 1.528 | Reg loss: 0.037 | Tree loss: 1.528 | Accuracy: 0.467000 | 1.603 sec/iter\n",
      "Epoch: 77 | Batch: 008 / 011 | Total loss: 1.539 | Reg loss: 0.037 | Tree loss: 1.539 | Accuracy: 0.465000 | 1.604 sec/iter\n",
      "Epoch: 77 | Batch: 009 / 011 | Total loss: 1.521 | Reg loss: 0.037 | Tree loss: 1.521 | Accuracy: 0.452500 | 1.603 sec/iter\n",
      "Epoch: 77 | Batch: 010 / 011 | Total loss: 1.525 | Reg loss: 0.037 | Tree loss: 1.525 | Accuracy: 0.481229 | 1.602 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 78 | Batch: 000 / 011 | Total loss: 1.755 | Reg loss: 0.037 | Tree loss: 1.755 | Accuracy: 0.361500 | 1.611 sec/iter\n",
      "Epoch: 78 | Batch: 001 / 011 | Total loss: 1.709 | Reg loss: 0.037 | Tree loss: 1.709 | Accuracy: 0.389500 | 1.611 sec/iter\n",
      "Epoch: 78 | Batch: 002 / 011 | Total loss: 1.662 | Reg loss: 0.037 | Tree loss: 1.662 | Accuracy: 0.422000 | 1.611 sec/iter\n",
      "Epoch: 78 | Batch: 003 / 011 | Total loss: 1.615 | Reg loss: 0.037 | Tree loss: 1.615 | Accuracy: 0.445000 | 1.611 sec/iter\n",
      "Epoch: 78 | Batch: 004 / 011 | Total loss: 1.585 | Reg loss: 0.037 | Tree loss: 1.585 | Accuracy: 0.483500 | 1.611 sec/iter\n",
      "Epoch: 78 | Batch: 005 / 011 | Total loss: 1.567 | Reg loss: 0.037 | Tree loss: 1.567 | Accuracy: 0.488500 | 1.611 sec/iter\n",
      "Epoch: 78 | Batch: 006 / 011 | Total loss: 1.540 | Reg loss: 0.037 | Tree loss: 1.540 | Accuracy: 0.468500 | 1.611 sec/iter\n",
      "Epoch: 78 | Batch: 007 / 011 | Total loss: 1.519 | Reg loss: 0.037 | Tree loss: 1.519 | Accuracy: 0.480500 | 1.61 sec/iter\n",
      "Epoch: 78 | Batch: 008 / 011 | Total loss: 1.519 | Reg loss: 0.037 | Tree loss: 1.519 | Accuracy: 0.472500 | 1.61 sec/iter\n",
      "Epoch: 78 | Batch: 009 / 011 | Total loss: 1.509 | Reg loss: 0.037 | Tree loss: 1.509 | Accuracy: 0.460000 | 1.61 sec/iter\n",
      "Epoch: 78 | Batch: 010 / 011 | Total loss: 1.487 | Reg loss: 0.037 | Tree loss: 1.487 | Accuracy: 0.501706 | 1.609 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 79 | Batch: 000 / 011 | Total loss: 1.733 | Reg loss: 0.037 | Tree loss: 1.733 | Accuracy: 0.362000 | 1.61 sec/iter\n",
      "Epoch: 79 | Batch: 001 / 011 | Total loss: 1.699 | Reg loss: 0.037 | Tree loss: 1.699 | Accuracy: 0.378000 | 1.61 sec/iter\n",
      "Epoch: 79 | Batch: 002 / 011 | Total loss: 1.642 | Reg loss: 0.037 | Tree loss: 1.642 | Accuracy: 0.436500 | 1.61 sec/iter\n",
      "Epoch: 79 | Batch: 003 / 011 | Total loss: 1.631 | Reg loss: 0.037 | Tree loss: 1.631 | Accuracy: 0.446500 | 1.609 sec/iter\n",
      "Epoch: 79 | Batch: 004 / 011 | Total loss: 1.591 | Reg loss: 0.037 | Tree loss: 1.591 | Accuracy: 0.459500 | 1.609 sec/iter\n",
      "Epoch: 79 | Batch: 005 / 011 | Total loss: 1.570 | Reg loss: 0.037 | Tree loss: 1.570 | Accuracy: 0.477500 | 1.61 sec/iter\n",
      "Epoch: 79 | Batch: 006 / 011 | Total loss: 1.547 | Reg loss: 0.037 | Tree loss: 1.547 | Accuracy: 0.478500 | 1.609 sec/iter\n",
      "Epoch: 79 | Batch: 007 / 011 | Total loss: 1.525 | Reg loss: 0.037 | Tree loss: 1.525 | Accuracy: 0.482500 | 1.609 sec/iter\n",
      "Epoch: 79 | Batch: 008 / 011 | Total loss: 1.505 | Reg loss: 0.037 | Tree loss: 1.505 | Accuracy: 0.468000 | 1.609 sec/iter\n",
      "Epoch: 79 | Batch: 009 / 011 | Total loss: 1.492 | Reg loss: 0.037 | Tree loss: 1.492 | Accuracy: 0.490000 | 1.609 sec/iter\n",
      "Epoch: 79 | Batch: 010 / 011 | Total loss: 1.470 | Reg loss: 0.037 | Tree loss: 1.470 | Accuracy: 0.467577 | 1.608 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 80 | Batch: 000 / 011 | Total loss: 1.753 | Reg loss: 0.037 | Tree loss: 1.753 | Accuracy: 0.372500 | 1.609 sec/iter\n",
      "Epoch: 80 | Batch: 001 / 011 | Total loss: 1.706 | Reg loss: 0.037 | Tree loss: 1.706 | Accuracy: 0.389000 | 1.608 sec/iter\n",
      "Epoch: 80 | Batch: 002 / 011 | Total loss: 1.634 | Reg loss: 0.037 | Tree loss: 1.634 | Accuracy: 0.439000 | 1.608 sec/iter\n",
      "Epoch: 80 | Batch: 003 / 011 | Total loss: 1.622 | Reg loss: 0.037 | Tree loss: 1.622 | Accuracy: 0.448500 | 1.607 sec/iter\n",
      "Epoch: 80 | Batch: 004 / 011 | Total loss: 1.583 | Reg loss: 0.037 | Tree loss: 1.583 | Accuracy: 0.469000 | 1.606 sec/iter\n",
      "Epoch: 80 | Batch: 005 / 011 | Total loss: 1.550 | Reg loss: 0.037 | Tree loss: 1.550 | Accuracy: 0.481500 | 1.606 sec/iter\n",
      "Epoch: 80 | Batch: 006 / 011 | Total loss: 1.516 | Reg loss: 0.037 | Tree loss: 1.516 | Accuracy: 0.490500 | 1.605 sec/iter\n",
      "Epoch: 80 | Batch: 007 / 011 | Total loss: 1.518 | Reg loss: 0.037 | Tree loss: 1.518 | Accuracy: 0.476000 | 1.605 sec/iter\n",
      "Epoch: 80 | Batch: 008 / 011 | Total loss: 1.495 | Reg loss: 0.037 | Tree loss: 1.495 | Accuracy: 0.490500 | 1.605 sec/iter\n",
      "Epoch: 80 | Batch: 009 / 011 | Total loss: 1.535 | Reg loss: 0.037 | Tree loss: 1.535 | Accuracy: 0.451000 | 1.604 sec/iter\n",
      "Epoch: 80 | Batch: 010 / 011 | Total loss: 1.500 | Reg loss: 0.037 | Tree loss: 1.500 | Accuracy: 0.494881 | 1.604 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 81 | Batch: 000 / 011 | Total loss: 1.742 | Reg loss: 0.037 | Tree loss: 1.742 | Accuracy: 0.381500 | 1.61 sec/iter\n",
      "Epoch: 81 | Batch: 001 / 011 | Total loss: 1.686 | Reg loss: 0.037 | Tree loss: 1.686 | Accuracy: 0.388500 | 1.61 sec/iter\n",
      "Epoch: 81 | Batch: 002 / 011 | Total loss: 1.658 | Reg loss: 0.037 | Tree loss: 1.658 | Accuracy: 0.405500 | 1.61 sec/iter\n",
      "Epoch: 81 | Batch: 003 / 011 | Total loss: 1.611 | Reg loss: 0.037 | Tree loss: 1.611 | Accuracy: 0.458500 | 1.609 sec/iter\n",
      "Epoch: 81 | Batch: 004 / 011 | Total loss: 1.580 | Reg loss: 0.037 | Tree loss: 1.580 | Accuracy: 0.460500 | 1.609 sec/iter\n",
      "Epoch: 81 | Batch: 005 / 011 | Total loss: 1.542 | Reg loss: 0.037 | Tree loss: 1.542 | Accuracy: 0.490500 | 1.609 sec/iter\n",
      "Epoch: 81 | Batch: 006 / 011 | Total loss: 1.530 | Reg loss: 0.037 | Tree loss: 1.530 | Accuracy: 0.474500 | 1.609 sec/iter\n",
      "Epoch: 81 | Batch: 007 / 011 | Total loss: 1.514 | Reg loss: 0.037 | Tree loss: 1.514 | Accuracy: 0.484000 | 1.609 sec/iter\n",
      "Epoch: 81 | Batch: 008 / 011 | Total loss: 1.513 | Reg loss: 0.037 | Tree loss: 1.513 | Accuracy: 0.459000 | 1.609 sec/iter\n",
      "Epoch: 81 | Batch: 009 / 011 | Total loss: 1.502 | Reg loss: 0.037 | Tree loss: 1.502 | Accuracy: 0.465000 | 1.609 sec/iter\n",
      "Epoch: 81 | Batch: 010 / 011 | Total loss: 1.507 | Reg loss: 0.037 | Tree loss: 1.507 | Accuracy: 0.470990 | 1.608 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 82 | Batch: 000 / 011 | Total loss: 1.743 | Reg loss: 0.037 | Tree loss: 1.743 | Accuracy: 0.371000 | 1.612 sec/iter\n",
      "Epoch: 82 | Batch: 001 / 011 | Total loss: 1.682 | Reg loss: 0.037 | Tree loss: 1.682 | Accuracy: 0.401000 | 1.612 sec/iter\n",
      "Epoch: 82 | Batch: 002 / 011 | Total loss: 1.660 | Reg loss: 0.037 | Tree loss: 1.660 | Accuracy: 0.405000 | 1.612 sec/iter\n",
      "Epoch: 82 | Batch: 003 / 011 | Total loss: 1.614 | Reg loss: 0.037 | Tree loss: 1.614 | Accuracy: 0.423500 | 1.611 sec/iter\n",
      "Epoch: 82 | Batch: 004 / 011 | Total loss: 1.579 | Reg loss: 0.037 | Tree loss: 1.579 | Accuracy: 0.478000 | 1.611 sec/iter\n",
      "Epoch: 82 | Batch: 005 / 011 | Total loss: 1.545 | Reg loss: 0.037 | Tree loss: 1.545 | Accuracy: 0.499000 | 1.611 sec/iter\n",
      "Epoch: 82 | Batch: 006 / 011 | Total loss: 1.518 | Reg loss: 0.037 | Tree loss: 1.518 | Accuracy: 0.499500 | 1.61 sec/iter\n",
      "Epoch: 82 | Batch: 007 / 011 | Total loss: 1.518 | Reg loss: 0.037 | Tree loss: 1.518 | Accuracy: 0.470000 | 1.61 sec/iter\n",
      "Epoch: 82 | Batch: 008 / 011 | Total loss: 1.499 | Reg loss: 0.037 | Tree loss: 1.499 | Accuracy: 0.471500 | 1.61 sec/iter\n",
      "Epoch: 82 | Batch: 009 / 011 | Total loss: 1.495 | Reg loss: 0.037 | Tree loss: 1.495 | Accuracy: 0.456000 | 1.61 sec/iter\n",
      "Epoch: 82 | Batch: 010 / 011 | Total loss: 1.486 | Reg loss: 0.037 | Tree loss: 1.486 | Accuracy: 0.470990 | 1.609 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 83 | Batch: 000 / 011 | Total loss: 1.714 | Reg loss: 0.037 | Tree loss: 1.714 | Accuracy: 0.388000 | 1.615 sec/iter\n",
      "Epoch: 83 | Batch: 001 / 011 | Total loss: 1.676 | Reg loss: 0.037 | Tree loss: 1.676 | Accuracy: 0.404500 | 1.614 sec/iter\n",
      "Epoch: 83 | Batch: 002 / 011 | Total loss: 1.656 | Reg loss: 0.037 | Tree loss: 1.656 | Accuracy: 0.434000 | 1.614 sec/iter\n",
      "Epoch: 83 | Batch: 003 / 011 | Total loss: 1.609 | Reg loss: 0.037 | Tree loss: 1.609 | Accuracy: 0.441000 | 1.614 sec/iter\n",
      "Epoch: 83 | Batch: 004 / 011 | Total loss: 1.589 | Reg loss: 0.037 | Tree loss: 1.589 | Accuracy: 0.463500 | 1.614 sec/iter\n",
      "Epoch: 83 | Batch: 005 / 011 | Total loss: 1.544 | Reg loss: 0.037 | Tree loss: 1.544 | Accuracy: 0.480000 | 1.614 sec/iter\n",
      "Epoch: 83 | Batch: 006 / 011 | Total loss: 1.521 | Reg loss: 0.037 | Tree loss: 1.521 | Accuracy: 0.484000 | 1.614 sec/iter\n",
      "Epoch: 83 | Batch: 007 / 011 | Total loss: 1.522 | Reg loss: 0.037 | Tree loss: 1.522 | Accuracy: 0.459500 | 1.613 sec/iter\n",
      "Epoch: 83 | Batch: 008 / 011 | Total loss: 1.499 | Reg loss: 0.037 | Tree loss: 1.499 | Accuracy: 0.482000 | 1.613 sec/iter\n",
      "Epoch: 83 | Batch: 009 / 011 | Total loss: 1.508 | Reg loss: 0.037 | Tree loss: 1.508 | Accuracy: 0.457000 | 1.613 sec/iter\n",
      "Epoch: 83 | Batch: 010 / 011 | Total loss: 1.483 | Reg loss: 0.037 | Tree loss: 1.483 | Accuracy: 0.518771 | 1.612 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 84 | Batch: 000 / 011 | Total loss: 1.718 | Reg loss: 0.037 | Tree loss: 1.718 | Accuracy: 0.387000 | 1.613 sec/iter\n",
      "Epoch: 84 | Batch: 001 / 011 | Total loss: 1.680 | Reg loss: 0.037 | Tree loss: 1.680 | Accuracy: 0.402000 | 1.613 sec/iter\n",
      "Epoch: 84 | Batch: 002 / 011 | Total loss: 1.638 | Reg loss: 0.037 | Tree loss: 1.638 | Accuracy: 0.424000 | 1.612 sec/iter\n",
      "Epoch: 84 | Batch: 003 / 011 | Total loss: 1.599 | Reg loss: 0.037 | Tree loss: 1.599 | Accuracy: 0.464500 | 1.612 sec/iter\n",
      "Epoch: 84 | Batch: 004 / 011 | Total loss: 1.563 | Reg loss: 0.037 | Tree loss: 1.563 | Accuracy: 0.458000 | 1.612 sec/iter\n",
      "Epoch: 84 | Batch: 005 / 011 | Total loss: 1.529 | Reg loss: 0.037 | Tree loss: 1.529 | Accuracy: 0.483000 | 1.612 sec/iter\n",
      "Epoch: 84 | Batch: 006 / 011 | Total loss: 1.511 | Reg loss: 0.037 | Tree loss: 1.511 | Accuracy: 0.479500 | 1.612 sec/iter\n",
      "Epoch: 84 | Batch: 007 / 011 | Total loss: 1.548 | Reg loss: 0.037 | Tree loss: 1.548 | Accuracy: 0.449000 | 1.612 sec/iter\n",
      "Epoch: 84 | Batch: 008 / 011 | Total loss: 1.507 | Reg loss: 0.037 | Tree loss: 1.507 | Accuracy: 0.462500 | 1.612 sec/iter\n",
      "Epoch: 84 | Batch: 009 / 011 | Total loss: 1.507 | Reg loss: 0.037 | Tree loss: 1.507 | Accuracy: 0.449000 | 1.612 sec/iter\n",
      "Epoch: 84 | Batch: 010 / 011 | Total loss: 1.516 | Reg loss: 0.037 | Tree loss: 1.516 | Accuracy: 0.436860 | 1.611 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 85 | Batch: 000 / 011 | Total loss: 1.696 | Reg loss: 0.037 | Tree loss: 1.696 | Accuracy: 0.396000 | 1.619 sec/iter\n",
      "Epoch: 85 | Batch: 001 / 011 | Total loss: 1.676 | Reg loss: 0.037 | Tree loss: 1.676 | Accuracy: 0.392500 | 1.619 sec/iter\n",
      "Epoch: 85 | Batch: 002 / 011 | Total loss: 1.634 | Reg loss: 0.037 | Tree loss: 1.634 | Accuracy: 0.427500 | 1.619 sec/iter\n",
      "Epoch: 85 | Batch: 003 / 011 | Total loss: 1.623 | Reg loss: 0.037 | Tree loss: 1.623 | Accuracy: 0.435000 | 1.618 sec/iter\n",
      "Epoch: 85 | Batch: 004 / 011 | Total loss: 1.558 | Reg loss: 0.037 | Tree loss: 1.558 | Accuracy: 0.485500 | 1.618 sec/iter\n",
      "Epoch: 85 | Batch: 005 / 011 | Total loss: 1.534 | Reg loss: 0.037 | Tree loss: 1.534 | Accuracy: 0.493000 | 1.618 sec/iter\n",
      "Epoch: 85 | Batch: 006 / 011 | Total loss: 1.526 | Reg loss: 0.037 | Tree loss: 1.526 | Accuracy: 0.469000 | 1.618 sec/iter\n",
      "Epoch: 85 | Batch: 007 / 011 | Total loss: 1.511 | Reg loss: 0.037 | Tree loss: 1.511 | Accuracy: 0.468500 | 1.618 sec/iter\n",
      "Epoch: 85 | Batch: 008 / 011 | Total loss: 1.522 | Reg loss: 0.037 | Tree loss: 1.522 | Accuracy: 0.463000 | 1.618 sec/iter\n",
      "Epoch: 85 | Batch: 009 / 011 | Total loss: 1.517 | Reg loss: 0.037 | Tree loss: 1.517 | Accuracy: 0.453500 | 1.618 sec/iter\n",
      "Epoch: 85 | Batch: 010 / 011 | Total loss: 1.473 | Reg loss: 0.037 | Tree loss: 1.473 | Accuracy: 0.453925 | 1.617 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 86 | Batch: 000 / 011 | Total loss: 1.721 | Reg loss: 0.037 | Tree loss: 1.721 | Accuracy: 0.396000 | 1.618 sec/iter\n",
      "Epoch: 86 | Batch: 001 / 011 | Total loss: 1.686 | Reg loss: 0.037 | Tree loss: 1.686 | Accuracy: 0.400500 | 1.618 sec/iter\n",
      "Epoch: 86 | Batch: 002 / 011 | Total loss: 1.657 | Reg loss: 0.037 | Tree loss: 1.657 | Accuracy: 0.420500 | 1.617 sec/iter\n",
      "Epoch: 86 | Batch: 003 / 011 | Total loss: 1.577 | Reg loss: 0.037 | Tree loss: 1.577 | Accuracy: 0.465500 | 1.617 sec/iter\n",
      "Epoch: 86 | Batch: 004 / 011 | Total loss: 1.566 | Reg loss: 0.037 | Tree loss: 1.566 | Accuracy: 0.470000 | 1.617 sec/iter\n",
      "Epoch: 86 | Batch: 005 / 011 | Total loss: 1.548 | Reg loss: 0.037 | Tree loss: 1.548 | Accuracy: 0.485000 | 1.617 sec/iter\n",
      "Epoch: 86 | Batch: 006 / 011 | Total loss: 1.491 | Reg loss: 0.037 | Tree loss: 1.491 | Accuracy: 0.497500 | 1.617 sec/iter\n",
      "Epoch: 86 | Batch: 007 / 011 | Total loss: 1.518 | Reg loss: 0.037 | Tree loss: 1.518 | Accuracy: 0.449000 | 1.617 sec/iter\n",
      "Epoch: 86 | Batch: 008 / 011 | Total loss: 1.513 | Reg loss: 0.037 | Tree loss: 1.513 | Accuracy: 0.463000 | 1.617 sec/iter\n",
      "Epoch: 86 | Batch: 009 / 011 | Total loss: 1.496 | Reg loss: 0.037 | Tree loss: 1.496 | Accuracy: 0.474500 | 1.616 sec/iter\n",
      "Epoch: 86 | Batch: 010 / 011 | Total loss: 1.480 | Reg loss: 0.037 | Tree loss: 1.480 | Accuracy: 0.474403 | 1.615 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 87 | Batch: 000 / 011 | Total loss: 1.712 | Reg loss: 0.037 | Tree loss: 1.712 | Accuracy: 0.386000 | 1.624 sec/iter\n",
      "Epoch: 87 | Batch: 001 / 011 | Total loss: 1.685 | Reg loss: 0.037 | Tree loss: 1.685 | Accuracy: 0.385000 | 1.624 sec/iter\n",
      "Epoch: 87 | Batch: 002 / 011 | Total loss: 1.637 | Reg loss: 0.037 | Tree loss: 1.637 | Accuracy: 0.425500 | 1.623 sec/iter\n",
      "Epoch: 87 | Batch: 003 / 011 | Total loss: 1.591 | Reg loss: 0.037 | Tree loss: 1.591 | Accuracy: 0.455000 | 1.623 sec/iter\n",
      "Epoch: 87 | Batch: 004 / 011 | Total loss: 1.570 | Reg loss: 0.037 | Tree loss: 1.570 | Accuracy: 0.464000 | 1.622 sec/iter\n",
      "Epoch: 87 | Batch: 005 / 011 | Total loss: 1.544 | Reg loss: 0.037 | Tree loss: 1.544 | Accuracy: 0.478500 | 1.622 sec/iter\n",
      "Epoch: 87 | Batch: 006 / 011 | Total loss: 1.530 | Reg loss: 0.037 | Tree loss: 1.530 | Accuracy: 0.471000 | 1.621 sec/iter\n",
      "Epoch: 87 | Batch: 007 / 011 | Total loss: 1.509 | Reg loss: 0.037 | Tree loss: 1.509 | Accuracy: 0.479000 | 1.62 sec/iter\n",
      "Epoch: 87 | Batch: 008 / 011 | Total loss: 1.487 | Reg loss: 0.037 | Tree loss: 1.487 | Accuracy: 0.470000 | 1.62 sec/iter\n",
      "Epoch: 87 | Batch: 009 / 011 | Total loss: 1.468 | Reg loss: 0.037 | Tree loss: 1.468 | Accuracy: 0.478000 | 1.619 sec/iter\n",
      "Epoch: 87 | Batch: 010 / 011 | Total loss: 1.479 | Reg loss: 0.037 | Tree loss: 1.479 | Accuracy: 0.419795 | 1.618 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 88 | Batch: 000 / 011 | Total loss: 1.691 | Reg loss: 0.037 | Tree loss: 1.691 | Accuracy: 0.388500 | 1.625 sec/iter\n",
      "Epoch: 88 | Batch: 001 / 011 | Total loss: 1.663 | Reg loss: 0.037 | Tree loss: 1.663 | Accuracy: 0.392500 | 1.625 sec/iter\n",
      "Epoch: 88 | Batch: 002 / 011 | Total loss: 1.648 | Reg loss: 0.037 | Tree loss: 1.648 | Accuracy: 0.416500 | 1.625 sec/iter\n",
      "Epoch: 88 | Batch: 003 / 011 | Total loss: 1.602 | Reg loss: 0.037 | Tree loss: 1.602 | Accuracy: 0.455000 | 1.625 sec/iter\n",
      "Epoch: 88 | Batch: 004 / 011 | Total loss: 1.573 | Reg loss: 0.037 | Tree loss: 1.573 | Accuracy: 0.464000 | 1.624 sec/iter\n",
      "Epoch: 88 | Batch: 005 / 011 | Total loss: 1.544 | Reg loss: 0.037 | Tree loss: 1.544 | Accuracy: 0.490500 | 1.624 sec/iter\n",
      "Epoch: 88 | Batch: 006 / 011 | Total loss: 1.513 | Reg loss: 0.037 | Tree loss: 1.513 | Accuracy: 0.479000 | 1.624 sec/iter\n",
      "Epoch: 88 | Batch: 007 / 011 | Total loss: 1.500 | Reg loss: 0.037 | Tree loss: 1.500 | Accuracy: 0.476000 | 1.624 sec/iter\n",
      "Epoch: 88 | Batch: 008 / 011 | Total loss: 1.499 | Reg loss: 0.037 | Tree loss: 1.499 | Accuracy: 0.471000 | 1.624 sec/iter\n",
      "Epoch: 88 | Batch: 009 / 011 | Total loss: 1.482 | Reg loss: 0.037 | Tree loss: 1.482 | Accuracy: 0.477000 | 1.624 sec/iter\n",
      "Epoch: 88 | Batch: 010 / 011 | Total loss: 1.465 | Reg loss: 0.037 | Tree loss: 1.465 | Accuracy: 0.515358 | 1.623 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 89 | Batch: 000 / 011 | Total loss: 1.716 | Reg loss: 0.037 | Tree loss: 1.716 | Accuracy: 0.373000 | 1.625 sec/iter\n",
      "Epoch: 89 | Batch: 001 / 011 | Total loss: 1.692 | Reg loss: 0.037 | Tree loss: 1.692 | Accuracy: 0.384000 | 1.624 sec/iter\n",
      "Epoch: 89 | Batch: 002 / 011 | Total loss: 1.635 | Reg loss: 0.037 | Tree loss: 1.635 | Accuracy: 0.429500 | 1.624 sec/iter\n",
      "Epoch: 89 | Batch: 003 / 011 | Total loss: 1.575 | Reg loss: 0.037 | Tree loss: 1.575 | Accuracy: 0.471500 | 1.624 sec/iter\n",
      "Epoch: 89 | Batch: 004 / 011 | Total loss: 1.545 | Reg loss: 0.037 | Tree loss: 1.545 | Accuracy: 0.478500 | 1.624 sec/iter\n",
      "Epoch: 89 | Batch: 005 / 011 | Total loss: 1.531 | Reg loss: 0.037 | Tree loss: 1.531 | Accuracy: 0.492000 | 1.624 sec/iter\n",
      "Epoch: 89 | Batch: 006 / 011 | Total loss: 1.501 | Reg loss: 0.037 | Tree loss: 1.501 | Accuracy: 0.470500 | 1.624 sec/iter\n",
      "Epoch: 89 | Batch: 007 / 011 | Total loss: 1.497 | Reg loss: 0.037 | Tree loss: 1.497 | Accuracy: 0.472500 | 1.624 sec/iter\n",
      "Epoch: 89 | Batch: 008 / 011 | Total loss: 1.505 | Reg loss: 0.037 | Tree loss: 1.505 | Accuracy: 0.474500 | 1.624 sec/iter\n",
      "Epoch: 89 | Batch: 009 / 011 | Total loss: 1.493 | Reg loss: 0.037 | Tree loss: 1.493 | Accuracy: 0.478000 | 1.623 sec/iter\n",
      "Epoch: 89 | Batch: 010 / 011 | Total loss: 1.544 | Reg loss: 0.037 | Tree loss: 1.544 | Accuracy: 0.470990 | 1.623 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 90 | Batch: 000 / 011 | Total loss: 1.693 | Reg loss: 0.037 | Tree loss: 1.693 | Accuracy: 0.385500 | 1.628 sec/iter\n",
      "Epoch: 90 | Batch: 001 / 011 | Total loss: 1.659 | Reg loss: 0.037 | Tree loss: 1.659 | Accuracy: 0.403000 | 1.627 sec/iter\n",
      "Epoch: 90 | Batch: 002 / 011 | Total loss: 1.642 | Reg loss: 0.037 | Tree loss: 1.642 | Accuracy: 0.406500 | 1.627 sec/iter\n",
      "Epoch: 90 | Batch: 003 / 011 | Total loss: 1.591 | Reg loss: 0.037 | Tree loss: 1.591 | Accuracy: 0.468500 | 1.627 sec/iter\n",
      "Epoch: 90 | Batch: 004 / 011 | Total loss: 1.542 | Reg loss: 0.037 | Tree loss: 1.542 | Accuracy: 0.458500 | 1.626 sec/iter\n",
      "Epoch: 90 | Batch: 005 / 011 | Total loss: 1.519 | Reg loss: 0.037 | Tree loss: 1.519 | Accuracy: 0.477000 | 1.626 sec/iter\n",
      "Epoch: 90 | Batch: 006 / 011 | Total loss: 1.542 | Reg loss: 0.037 | Tree loss: 1.542 | Accuracy: 0.466500 | 1.626 sec/iter\n",
      "Epoch: 90 | Batch: 007 / 011 | Total loss: 1.489 | Reg loss: 0.037 | Tree loss: 1.489 | Accuracy: 0.485500 | 1.626 sec/iter\n",
      "Epoch: 90 | Batch: 008 / 011 | Total loss: 1.523 | Reg loss: 0.037 | Tree loss: 1.523 | Accuracy: 0.450500 | 1.626 sec/iter\n",
      "Epoch: 90 | Batch: 009 / 011 | Total loss: 1.503 | Reg loss: 0.037 | Tree loss: 1.503 | Accuracy: 0.457000 | 1.626 sec/iter\n",
      "Epoch: 90 | Batch: 010 / 011 | Total loss: 1.466 | Reg loss: 0.037 | Tree loss: 1.466 | Accuracy: 0.501706 | 1.625 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 91 | Batch: 000 / 011 | Total loss: 1.683 | Reg loss: 0.037 | Tree loss: 1.683 | Accuracy: 0.383500 | 1.628 sec/iter\n",
      "Epoch: 91 | Batch: 001 / 011 | Total loss: 1.655 | Reg loss: 0.037 | Tree loss: 1.655 | Accuracy: 0.408500 | 1.628 sec/iter\n",
      "Epoch: 91 | Batch: 002 / 011 | Total loss: 1.624 | Reg loss: 0.037 | Tree loss: 1.624 | Accuracy: 0.448000 | 1.628 sec/iter\n",
      "Epoch: 91 | Batch: 003 / 011 | Total loss: 1.586 | Reg loss: 0.037 | Tree loss: 1.586 | Accuracy: 0.447500 | 1.628 sec/iter\n",
      "Epoch: 91 | Batch: 004 / 011 | Total loss: 1.540 | Reg loss: 0.037 | Tree loss: 1.540 | Accuracy: 0.483500 | 1.628 sec/iter\n",
      "Epoch: 91 | Batch: 005 / 011 | Total loss: 1.522 | Reg loss: 0.037 | Tree loss: 1.522 | Accuracy: 0.480500 | 1.628 sec/iter\n",
      "Epoch: 91 | Batch: 006 / 011 | Total loss: 1.539 | Reg loss: 0.037 | Tree loss: 1.539 | Accuracy: 0.441000 | 1.627 sec/iter\n",
      "Epoch: 91 | Batch: 007 / 011 | Total loss: 1.497 | Reg loss: 0.037 | Tree loss: 1.497 | Accuracy: 0.467500 | 1.627 sec/iter\n",
      "Epoch: 91 | Batch: 008 / 011 | Total loss: 1.519 | Reg loss: 0.037 | Tree loss: 1.519 | Accuracy: 0.447500 | 1.627 sec/iter\n",
      "Epoch: 91 | Batch: 009 / 011 | Total loss: 1.494 | Reg loss: 0.037 | Tree loss: 1.494 | Accuracy: 0.467500 | 1.627 sec/iter\n",
      "Epoch: 91 | Batch: 010 / 011 | Total loss: 1.494 | Reg loss: 0.037 | Tree loss: 1.494 | Accuracy: 0.484642 | 1.626 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 92 | Batch: 000 / 011 | Total loss: 1.671 | Reg loss: 0.037 | Tree loss: 1.671 | Accuracy: 0.415500 | 1.627 sec/iter\n",
      "Epoch: 92 | Batch: 001 / 011 | Total loss: 1.669 | Reg loss: 0.037 | Tree loss: 1.669 | Accuracy: 0.389500 | 1.626 sec/iter\n",
      "Epoch: 92 | Batch: 002 / 011 | Total loss: 1.659 | Reg loss: 0.037 | Tree loss: 1.659 | Accuracy: 0.405500 | 1.626 sec/iter\n",
      "Epoch: 92 | Batch: 003 / 011 | Total loss: 1.571 | Reg loss: 0.037 | Tree loss: 1.571 | Accuracy: 0.454500 | 1.626 sec/iter\n",
      "Epoch: 92 | Batch: 004 / 011 | Total loss: 1.560 | Reg loss: 0.037 | Tree loss: 1.560 | Accuracy: 0.456500 | 1.626 sec/iter\n",
      "Epoch: 92 | Batch: 005 / 011 | Total loss: 1.516 | Reg loss: 0.037 | Tree loss: 1.516 | Accuracy: 0.493500 | 1.626 sec/iter\n",
      "Epoch: 92 | Batch: 006 / 011 | Total loss: 1.506 | Reg loss: 0.037 | Tree loss: 1.506 | Accuracy: 0.486000 | 1.626 sec/iter\n",
      "Epoch: 92 | Batch: 007 / 011 | Total loss: 1.508 | Reg loss: 0.037 | Tree loss: 1.508 | Accuracy: 0.475500 | 1.625 sec/iter\n",
      "Epoch: 92 | Batch: 008 / 011 | Total loss: 1.489 | Reg loss: 0.037 | Tree loss: 1.489 | Accuracy: 0.457000 | 1.626 sec/iter\n",
      "Epoch: 92 | Batch: 009 / 011 | Total loss: 1.503 | Reg loss: 0.037 | Tree loss: 1.503 | Accuracy: 0.464000 | 1.625 sec/iter\n",
      "Epoch: 92 | Batch: 010 / 011 | Total loss: 1.501 | Reg loss: 0.037 | Tree loss: 1.501 | Accuracy: 0.470990 | 1.624 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 93 | Batch: 000 / 011 | Total loss: 1.713 | Reg loss: 0.037 | Tree loss: 1.713 | Accuracy: 0.368500 | 1.632 sec/iter\n",
      "Epoch: 93 | Batch: 001 / 011 | Total loss: 1.654 | Reg loss: 0.037 | Tree loss: 1.654 | Accuracy: 0.407000 | 1.632 sec/iter\n",
      "Epoch: 93 | Batch: 002 / 011 | Total loss: 1.616 | Reg loss: 0.037 | Tree loss: 1.616 | Accuracy: 0.436500 | 1.632 sec/iter\n",
      "Epoch: 93 | Batch: 003 / 011 | Total loss: 1.567 | Reg loss: 0.037 | Tree loss: 1.567 | Accuracy: 0.473500 | 1.632 sec/iter\n",
      "Epoch: 93 | Batch: 004 / 011 | Total loss: 1.560 | Reg loss: 0.037 | Tree loss: 1.560 | Accuracy: 0.458500 | 1.631 sec/iter\n",
      "Epoch: 93 | Batch: 005 / 011 | Total loss: 1.525 | Reg loss: 0.037 | Tree loss: 1.525 | Accuracy: 0.476500 | 1.632 sec/iter\n",
      "Epoch: 93 | Batch: 006 / 011 | Total loss: 1.505 | Reg loss: 0.037 | Tree loss: 1.505 | Accuracy: 0.469000 | 1.631 sec/iter\n",
      "Epoch: 93 | Batch: 007 / 011 | Total loss: 1.515 | Reg loss: 0.037 | Tree loss: 1.515 | Accuracy: 0.473000 | 1.631 sec/iter\n",
      "Epoch: 93 | Batch: 008 / 011 | Total loss: 1.477 | Reg loss: 0.037 | Tree loss: 1.477 | Accuracy: 0.464000 | 1.631 sec/iter\n",
      "Epoch: 93 | Batch: 009 / 011 | Total loss: 1.483 | Reg loss: 0.037 | Tree loss: 1.483 | Accuracy: 0.482500 | 1.631 sec/iter\n",
      "Epoch: 93 | Batch: 010 / 011 | Total loss: 1.554 | Reg loss: 0.037 | Tree loss: 1.554 | Accuracy: 0.426621 | 1.63 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 94 | Batch: 000 / 011 | Total loss: 1.694 | Reg loss: 0.037 | Tree loss: 1.694 | Accuracy: 0.390000 | 1.631 sec/iter\n",
      "Epoch: 94 | Batch: 001 / 011 | Total loss: 1.664 | Reg loss: 0.037 | Tree loss: 1.664 | Accuracy: 0.393500 | 1.631 sec/iter\n",
      "Epoch: 94 | Batch: 002 / 011 | Total loss: 1.618 | Reg loss: 0.037 | Tree loss: 1.618 | Accuracy: 0.430000 | 1.63 sec/iter\n",
      "Epoch: 94 | Batch: 003 / 011 | Total loss: 1.580 | Reg loss: 0.037 | Tree loss: 1.580 | Accuracy: 0.462500 | 1.63 sec/iter\n",
      "Epoch: 94 | Batch: 004 / 011 | Total loss: 1.520 | Reg loss: 0.037 | Tree loss: 1.520 | Accuracy: 0.477500 | 1.63 sec/iter\n",
      "Epoch: 94 | Batch: 005 / 011 | Total loss: 1.523 | Reg loss: 0.037 | Tree loss: 1.523 | Accuracy: 0.472000 | 1.63 sec/iter\n",
      "Epoch: 94 | Batch: 006 / 011 | Total loss: 1.504 | Reg loss: 0.037 | Tree loss: 1.504 | Accuracy: 0.475000 | 1.63 sec/iter\n",
      "Epoch: 94 | Batch: 007 / 011 | Total loss: 1.490 | Reg loss: 0.037 | Tree loss: 1.490 | Accuracy: 0.472000 | 1.63 sec/iter\n",
      "Epoch: 94 | Batch: 008 / 011 | Total loss: 1.490 | Reg loss: 0.037 | Tree loss: 1.490 | Accuracy: 0.476500 | 1.63 sec/iter\n",
      "Epoch: 94 | Batch: 009 / 011 | Total loss: 1.512 | Reg loss: 0.037 | Tree loss: 1.512 | Accuracy: 0.456500 | 1.629 sec/iter\n",
      "Epoch: 94 | Batch: 010 / 011 | Total loss: 1.478 | Reg loss: 0.037 | Tree loss: 1.478 | Accuracy: 0.484642 | 1.628 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 95 | Batch: 000 / 011 | Total loss: 1.712 | Reg loss: 0.037 | Tree loss: 1.712 | Accuracy: 0.378000 | 1.63 sec/iter\n",
      "Epoch: 95 | Batch: 001 / 011 | Total loss: 1.665 | Reg loss: 0.037 | Tree loss: 1.665 | Accuracy: 0.392000 | 1.629 sec/iter\n",
      "Epoch: 95 | Batch: 002 / 011 | Total loss: 1.627 | Reg loss: 0.037 | Tree loss: 1.627 | Accuracy: 0.430500 | 1.628 sec/iter\n",
      "Epoch: 95 | Batch: 003 / 011 | Total loss: 1.607 | Reg loss: 0.037 | Tree loss: 1.607 | Accuracy: 0.445000 | 1.628 sec/iter\n",
      "Epoch: 95 | Batch: 004 / 011 | Total loss: 1.553 | Reg loss: 0.037 | Tree loss: 1.553 | Accuracy: 0.483000 | 1.627 sec/iter\n",
      "Epoch: 95 | Batch: 005 / 011 | Total loss: 1.501 | Reg loss: 0.037 | Tree loss: 1.501 | Accuracy: 0.486500 | 1.627 sec/iter\n",
      "Epoch: 95 | Batch: 006 / 011 | Total loss: 1.485 | Reg loss: 0.037 | Tree loss: 1.485 | Accuracy: 0.488500 | 1.626 sec/iter\n",
      "Epoch: 95 | Batch: 007 / 011 | Total loss: 1.490 | Reg loss: 0.037 | Tree loss: 1.490 | Accuracy: 0.469000 | 1.626 sec/iter\n",
      "Epoch: 95 | Batch: 008 / 011 | Total loss: 1.484 | Reg loss: 0.037 | Tree loss: 1.484 | Accuracy: 0.478000 | 1.626 sec/iter\n",
      "Epoch: 95 | Batch: 009 / 011 | Total loss: 1.472 | Reg loss: 0.038 | Tree loss: 1.472 | Accuracy: 0.480500 | 1.626 sec/iter\n",
      "Epoch: 95 | Batch: 010 / 011 | Total loss: 1.427 | Reg loss: 0.038 | Tree loss: 1.427 | Accuracy: 0.440273 | 1.625 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 96 | Batch: 000 / 011 | Total loss: 1.685 | Reg loss: 0.037 | Tree loss: 1.685 | Accuracy: 0.392500 | 1.63 sec/iter\n",
      "Epoch: 96 | Batch: 001 / 011 | Total loss: 1.677 | Reg loss: 0.037 | Tree loss: 1.677 | Accuracy: 0.405500 | 1.63 sec/iter\n",
      "Epoch: 96 | Batch: 002 / 011 | Total loss: 1.626 | Reg loss: 0.037 | Tree loss: 1.626 | Accuracy: 0.422500 | 1.63 sec/iter\n",
      "Epoch: 96 | Batch: 003 / 011 | Total loss: 1.585 | Reg loss: 0.037 | Tree loss: 1.585 | Accuracy: 0.438500 | 1.63 sec/iter\n",
      "Epoch: 96 | Batch: 004 / 011 | Total loss: 1.542 | Reg loss: 0.037 | Tree loss: 1.542 | Accuracy: 0.476500 | 1.63 sec/iter\n",
      "Epoch: 96 | Batch: 005 / 011 | Total loss: 1.521 | Reg loss: 0.037 | Tree loss: 1.521 | Accuracy: 0.485000 | 1.63 sec/iter\n",
      "Epoch: 96 | Batch: 006 / 011 | Total loss: 1.498 | Reg loss: 0.037 | Tree loss: 1.498 | Accuracy: 0.472000 | 1.629 sec/iter\n",
      "Epoch: 96 | Batch: 007 / 011 | Total loss: 1.477 | Reg loss: 0.037 | Tree loss: 1.477 | Accuracy: 0.462500 | 1.629 sec/iter\n",
      "Epoch: 96 | Batch: 008 / 011 | Total loss: 1.465 | Reg loss: 0.038 | Tree loss: 1.465 | Accuracy: 0.483500 | 1.629 sec/iter\n",
      "Epoch: 96 | Batch: 009 / 011 | Total loss: 1.481 | Reg loss: 0.038 | Tree loss: 1.481 | Accuracy: 0.468000 | 1.629 sec/iter\n",
      "Epoch: 96 | Batch: 010 / 011 | Total loss: 1.479 | Reg loss: 0.038 | Tree loss: 1.479 | Accuracy: 0.467577 | 1.628 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 97 | Batch: 000 / 011 | Total loss: 1.675 | Reg loss: 0.037 | Tree loss: 1.675 | Accuracy: 0.386000 | 1.632 sec/iter\n",
      "Epoch: 97 | Batch: 001 / 011 | Total loss: 1.646 | Reg loss: 0.037 | Tree loss: 1.646 | Accuracy: 0.417000 | 1.632 sec/iter\n",
      "Epoch: 97 | Batch: 002 / 011 | Total loss: 1.613 | Reg loss: 0.037 | Tree loss: 1.613 | Accuracy: 0.437000 | 1.632 sec/iter\n",
      "Epoch: 97 | Batch: 003 / 011 | Total loss: 1.600 | Reg loss: 0.037 | Tree loss: 1.600 | Accuracy: 0.438500 | 1.631 sec/iter\n",
      "Epoch: 97 | Batch: 004 / 011 | Total loss: 1.538 | Reg loss: 0.037 | Tree loss: 1.538 | Accuracy: 0.471000 | 1.631 sec/iter\n",
      "Epoch: 97 | Batch: 005 / 011 | Total loss: 1.505 | Reg loss: 0.037 | Tree loss: 1.505 | Accuracy: 0.485500 | 1.631 sec/iter\n",
      "Epoch: 97 | Batch: 006 / 011 | Total loss: 1.503 | Reg loss: 0.037 | Tree loss: 1.503 | Accuracy: 0.497000 | 1.631 sec/iter\n",
      "Epoch: 97 | Batch: 007 / 011 | Total loss: 1.496 | Reg loss: 0.038 | Tree loss: 1.496 | Accuracy: 0.461000 | 1.631 sec/iter\n",
      "Epoch: 97 | Batch: 008 / 011 | Total loss: 1.490 | Reg loss: 0.038 | Tree loss: 1.490 | Accuracy: 0.449500 | 1.63 sec/iter\n",
      "Epoch: 97 | Batch: 009 / 011 | Total loss: 1.484 | Reg loss: 0.038 | Tree loss: 1.484 | Accuracy: 0.451500 | 1.63 sec/iter\n",
      "Epoch: 97 | Batch: 010 / 011 | Total loss: 1.499 | Reg loss: 0.038 | Tree loss: 1.499 | Accuracy: 0.457338 | 1.629 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 98 | Batch: 000 / 011 | Total loss: 1.717 | Reg loss: 0.037 | Tree loss: 1.717 | Accuracy: 0.366000 | 1.634 sec/iter\n",
      "Epoch: 98 | Batch: 001 / 011 | Total loss: 1.642 | Reg loss: 0.037 | Tree loss: 1.642 | Accuracy: 0.409500 | 1.633 sec/iter\n",
      "Epoch: 98 | Batch: 002 / 011 | Total loss: 1.618 | Reg loss: 0.037 | Tree loss: 1.618 | Accuracy: 0.438000 | 1.633 sec/iter\n",
      "Epoch: 98 | Batch: 003 / 011 | Total loss: 1.561 | Reg loss: 0.037 | Tree loss: 1.561 | Accuracy: 0.456000 | 1.633 sec/iter\n",
      "Epoch: 98 | Batch: 004 / 011 | Total loss: 1.527 | Reg loss: 0.037 | Tree loss: 1.527 | Accuracy: 0.487500 | 1.633 sec/iter\n",
      "Epoch: 98 | Batch: 005 / 011 | Total loss: 1.496 | Reg loss: 0.038 | Tree loss: 1.496 | Accuracy: 0.489000 | 1.633 sec/iter\n",
      "Epoch: 98 | Batch: 006 / 011 | Total loss: 1.513 | Reg loss: 0.038 | Tree loss: 1.513 | Accuracy: 0.477000 | 1.633 sec/iter\n",
      "Epoch: 98 | Batch: 007 / 011 | Total loss: 1.498 | Reg loss: 0.038 | Tree loss: 1.498 | Accuracy: 0.471500 | 1.632 sec/iter\n",
      "Epoch: 98 | Batch: 008 / 011 | Total loss: 1.482 | Reg loss: 0.038 | Tree loss: 1.482 | Accuracy: 0.471000 | 1.632 sec/iter\n",
      "Epoch: 98 | Batch: 009 / 011 | Total loss: 1.465 | Reg loss: 0.038 | Tree loss: 1.465 | Accuracy: 0.484500 | 1.633 sec/iter\n",
      "Epoch: 98 | Batch: 010 / 011 | Total loss: 1.534 | Reg loss: 0.038 | Tree loss: 1.534 | Accuracy: 0.443686 | 1.632 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 99 | Batch: 000 / 011 | Total loss: 1.693 | Reg loss: 0.037 | Tree loss: 1.693 | Accuracy: 0.376000 | 1.632 sec/iter\n",
      "Epoch: 99 | Batch: 001 / 011 | Total loss: 1.655 | Reg loss: 0.037 | Tree loss: 1.655 | Accuracy: 0.416000 | 1.632 sec/iter\n",
      "Epoch: 99 | Batch: 002 / 011 | Total loss: 1.602 | Reg loss: 0.037 | Tree loss: 1.602 | Accuracy: 0.426500 | 1.632 sec/iter\n",
      "Epoch: 99 | Batch: 003 / 011 | Total loss: 1.571 | Reg loss: 0.037 | Tree loss: 1.571 | Accuracy: 0.471000 | 1.632 sec/iter\n",
      "Epoch: 99 | Batch: 004 / 011 | Total loss: 1.533 | Reg loss: 0.038 | Tree loss: 1.533 | Accuracy: 0.478500 | 1.632 sec/iter\n",
      "Epoch: 99 | Batch: 005 / 011 | Total loss: 1.528 | Reg loss: 0.038 | Tree loss: 1.528 | Accuracy: 0.477000 | 1.631 sec/iter\n",
      "Epoch: 99 | Batch: 006 / 011 | Total loss: 1.500 | Reg loss: 0.038 | Tree loss: 1.500 | Accuracy: 0.464000 | 1.631 sec/iter\n",
      "Epoch: 99 | Batch: 007 / 011 | Total loss: 1.482 | Reg loss: 0.038 | Tree loss: 1.482 | Accuracy: 0.468000 | 1.631 sec/iter\n",
      "Epoch: 99 | Batch: 008 / 011 | Total loss: 1.490 | Reg loss: 0.038 | Tree loss: 1.490 | Accuracy: 0.469500 | 1.631 sec/iter\n",
      "Epoch: 99 | Batch: 009 / 011 | Total loss: 1.468 | Reg loss: 0.038 | Tree loss: 1.468 | Accuracy: 0.459500 | 1.631 sec/iter\n",
      "Epoch: 99 | Batch: 010 / 011 | Total loss: 1.503 | Reg loss: 0.038 | Tree loss: 1.503 | Accuracy: 0.481229 | 1.63 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100 | Batch: 000 / 011 | Total loss: 1.692 | Reg loss: 0.037 | Tree loss: 1.692 | Accuracy: 0.395000 | 1.637 sec/iter\n",
      "Epoch: 100 | Batch: 001 / 011 | Total loss: 1.661 | Reg loss: 0.037 | Tree loss: 1.661 | Accuracy: 0.398000 | 1.637 sec/iter\n",
      "Epoch: 100 | Batch: 002 / 011 | Total loss: 1.607 | Reg loss: 0.038 | Tree loss: 1.607 | Accuracy: 0.430500 | 1.637 sec/iter\n",
      "Epoch: 100 | Batch: 003 / 011 | Total loss: 1.579 | Reg loss: 0.038 | Tree loss: 1.579 | Accuracy: 0.437500 | 1.637 sec/iter\n",
      "Epoch: 100 | Batch: 004 / 011 | Total loss: 1.538 | Reg loss: 0.038 | Tree loss: 1.538 | Accuracy: 0.459000 | 1.637 sec/iter\n",
      "Epoch: 100 | Batch: 005 / 011 | Total loss: 1.540 | Reg loss: 0.038 | Tree loss: 1.540 | Accuracy: 0.465500 | 1.637 sec/iter\n",
      "Epoch: 100 | Batch: 006 / 011 | Total loss: 1.487 | Reg loss: 0.038 | Tree loss: 1.487 | Accuracy: 0.480000 | 1.636 sec/iter\n",
      "Epoch: 100 | Batch: 007 / 011 | Total loss: 1.471 | Reg loss: 0.038 | Tree loss: 1.471 | Accuracy: 0.477000 | 1.636 sec/iter\n",
      "Epoch: 100 | Batch: 008 / 011 | Total loss: 1.463 | Reg loss: 0.038 | Tree loss: 1.463 | Accuracy: 0.475000 | 1.636 sec/iter\n",
      "Epoch: 100 | Batch: 009 / 011 | Total loss: 1.484 | Reg loss: 0.038 | Tree loss: 1.484 | Accuracy: 0.471500 | 1.636 sec/iter\n",
      "Epoch: 100 | Batch: 010 / 011 | Total loss: 1.476 | Reg loss: 0.038 | Tree loss: 1.476 | Accuracy: 0.440273 | 1.635 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 101 | Batch: 000 / 011 | Total loss: 1.675 | Reg loss: 0.038 | Tree loss: 1.675 | Accuracy: 0.387000 | 1.636 sec/iter\n",
      "Epoch: 101 | Batch: 001 / 011 | Total loss: 1.649 | Reg loss: 0.038 | Tree loss: 1.649 | Accuracy: 0.407500 | 1.636 sec/iter\n",
      "Epoch: 101 | Batch: 002 / 011 | Total loss: 1.598 | Reg loss: 0.038 | Tree loss: 1.598 | Accuracy: 0.434500 | 1.635 sec/iter\n",
      "Epoch: 101 | Batch: 003 / 011 | Total loss: 1.594 | Reg loss: 0.038 | Tree loss: 1.594 | Accuracy: 0.444000 | 1.635 sec/iter\n",
      "Epoch: 101 | Batch: 004 / 011 | Total loss: 1.529 | Reg loss: 0.038 | Tree loss: 1.529 | Accuracy: 0.482500 | 1.635 sec/iter\n",
      "Epoch: 101 | Batch: 005 / 011 | Total loss: 1.518 | Reg loss: 0.038 | Tree loss: 1.518 | Accuracy: 0.466500 | 1.635 sec/iter\n",
      "Epoch: 101 | Batch: 006 / 011 | Total loss: 1.501 | Reg loss: 0.038 | Tree loss: 1.501 | Accuracy: 0.482500 | 1.635 sec/iter\n",
      "Epoch: 101 | Batch: 007 / 011 | Total loss: 1.477 | Reg loss: 0.038 | Tree loss: 1.477 | Accuracy: 0.471000 | 1.635 sec/iter\n",
      "Epoch: 101 | Batch: 008 / 011 | Total loss: 1.468 | Reg loss: 0.038 | Tree loss: 1.468 | Accuracy: 0.455000 | 1.635 sec/iter\n",
      "Epoch: 101 | Batch: 009 / 011 | Total loss: 1.470 | Reg loss: 0.038 | Tree loss: 1.470 | Accuracy: 0.472500 | 1.635 sec/iter\n",
      "Epoch: 101 | Batch: 010 / 011 | Total loss: 1.517 | Reg loss: 0.038 | Tree loss: 1.517 | Accuracy: 0.464164 | 1.634 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 102 | Batch: 000 / 011 | Total loss: 1.673 | Reg loss: 0.038 | Tree loss: 1.673 | Accuracy: 0.384500 | 1.641 sec/iter\n",
      "Epoch: 102 | Batch: 001 / 011 | Total loss: 1.657 | Reg loss: 0.038 | Tree loss: 1.657 | Accuracy: 0.414500 | 1.641 sec/iter\n",
      "Epoch: 102 | Batch: 002 / 011 | Total loss: 1.604 | Reg loss: 0.038 | Tree loss: 1.604 | Accuracy: 0.429500 | 1.641 sec/iter\n",
      "Epoch: 102 | Batch: 003 / 011 | Total loss: 1.579 | Reg loss: 0.038 | Tree loss: 1.579 | Accuracy: 0.442500 | 1.64 sec/iter\n",
      "Epoch: 102 | Batch: 004 / 011 | Total loss: 1.544 | Reg loss: 0.038 | Tree loss: 1.544 | Accuracy: 0.445500 | 1.64 sec/iter\n",
      "Epoch: 102 | Batch: 005 / 011 | Total loss: 1.513 | Reg loss: 0.038 | Tree loss: 1.513 | Accuracy: 0.477500 | 1.639 sec/iter\n",
      "Epoch: 102 | Batch: 006 / 011 | Total loss: 1.493 | Reg loss: 0.038 | Tree loss: 1.493 | Accuracy: 0.480500 | 1.638 sec/iter\n",
      "Epoch: 102 | Batch: 007 / 011 | Total loss: 1.467 | Reg loss: 0.038 | Tree loss: 1.467 | Accuracy: 0.478000 | 1.638 sec/iter\n",
      "Epoch: 102 | Batch: 008 / 011 | Total loss: 1.482 | Reg loss: 0.038 | Tree loss: 1.482 | Accuracy: 0.463000 | 1.637 sec/iter\n",
      "Epoch: 102 | Batch: 009 / 011 | Total loss: 1.472 | Reg loss: 0.038 | Tree loss: 1.472 | Accuracy: 0.467000 | 1.637 sec/iter\n",
      "Epoch: 102 | Batch: 010 / 011 | Total loss: 1.448 | Reg loss: 0.038 | Tree loss: 1.448 | Accuracy: 0.505119 | 1.636 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 103 | Batch: 000 / 011 | Total loss: 1.704 | Reg loss: 0.038 | Tree loss: 1.704 | Accuracy: 0.387500 | 1.64 sec/iter\n",
      "Epoch: 103 | Batch: 001 / 011 | Total loss: 1.631 | Reg loss: 0.038 | Tree loss: 1.631 | Accuracy: 0.401500 | 1.64 sec/iter\n",
      "Epoch: 103 | Batch: 002 / 011 | Total loss: 1.602 | Reg loss: 0.038 | Tree loss: 1.602 | Accuracy: 0.421000 | 1.64 sec/iter\n",
      "Epoch: 103 | Batch: 003 / 011 | Total loss: 1.546 | Reg loss: 0.038 | Tree loss: 1.546 | Accuracy: 0.464000 | 1.639 sec/iter\n",
      "Epoch: 103 | Batch: 004 / 011 | Total loss: 1.524 | Reg loss: 0.038 | Tree loss: 1.524 | Accuracy: 0.480000 | 1.639 sec/iter\n",
      "Epoch: 103 | Batch: 005 / 011 | Total loss: 1.517 | Reg loss: 0.038 | Tree loss: 1.517 | Accuracy: 0.459500 | 1.639 sec/iter\n",
      "Epoch: 103 | Batch: 006 / 011 | Total loss: 1.490 | Reg loss: 0.038 | Tree loss: 1.490 | Accuracy: 0.481000 | 1.639 sec/iter\n",
      "Epoch: 103 | Batch: 007 / 011 | Total loss: 1.479 | Reg loss: 0.038 | Tree loss: 1.479 | Accuracy: 0.464500 | 1.639 sec/iter\n",
      "Epoch: 103 | Batch: 008 / 011 | Total loss: 1.474 | Reg loss: 0.038 | Tree loss: 1.474 | Accuracy: 0.472000 | 1.638 sec/iter\n",
      "Epoch: 103 | Batch: 009 / 011 | Total loss: 1.491 | Reg loss: 0.038 | Tree loss: 1.491 | Accuracy: 0.456000 | 1.638 sec/iter\n",
      "Epoch: 103 | Batch: 010 / 011 | Total loss: 1.472 | Reg loss: 0.038 | Tree loss: 1.472 | Accuracy: 0.515358 | 1.638 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 104 | Batch: 000 / 011 | Total loss: 1.664 | Reg loss: 0.038 | Tree loss: 1.664 | Accuracy: 0.386500 | 1.641 sec/iter\n",
      "Epoch: 104 | Batch: 001 / 011 | Total loss: 1.649 | Reg loss: 0.038 | Tree loss: 1.649 | Accuracy: 0.410000 | 1.641 sec/iter\n",
      "Epoch: 104 | Batch: 002 / 011 | Total loss: 1.602 | Reg loss: 0.038 | Tree loss: 1.602 | Accuracy: 0.418000 | 1.641 sec/iter\n",
      "Epoch: 104 | Batch: 003 / 011 | Total loss: 1.571 | Reg loss: 0.038 | Tree loss: 1.571 | Accuracy: 0.438000 | 1.641 sec/iter\n",
      "Epoch: 104 | Batch: 004 / 011 | Total loss: 1.549 | Reg loss: 0.038 | Tree loss: 1.549 | Accuracy: 0.467000 | 1.641 sec/iter\n",
      "Epoch: 104 | Batch: 005 / 011 | Total loss: 1.511 | Reg loss: 0.038 | Tree loss: 1.511 | Accuracy: 0.480000 | 1.641 sec/iter\n",
      "Epoch: 104 | Batch: 006 / 011 | Total loss: 1.497 | Reg loss: 0.038 | Tree loss: 1.497 | Accuracy: 0.465000 | 1.64 sec/iter\n",
      "Epoch: 104 | Batch: 007 / 011 | Total loss: 1.462 | Reg loss: 0.038 | Tree loss: 1.462 | Accuracy: 0.486500 | 1.64 sec/iter\n",
      "Epoch: 104 | Batch: 008 / 011 | Total loss: 1.483 | Reg loss: 0.038 | Tree loss: 1.483 | Accuracy: 0.460000 | 1.64 sec/iter\n",
      "Epoch: 104 | Batch: 009 / 011 | Total loss: 1.462 | Reg loss: 0.038 | Tree loss: 1.462 | Accuracy: 0.466000 | 1.64 sec/iter\n",
      "Epoch: 104 | Batch: 010 / 011 | Total loss: 1.451 | Reg loss: 0.038 | Tree loss: 1.451 | Accuracy: 0.488055 | 1.639 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 105 | Batch: 000 / 011 | Total loss: 1.674 | Reg loss: 0.038 | Tree loss: 1.674 | Accuracy: 0.379500 | 1.642 sec/iter\n",
      "Epoch: 105 | Batch: 001 / 011 | Total loss: 1.627 | Reg loss: 0.038 | Tree loss: 1.627 | Accuracy: 0.407000 | 1.642 sec/iter\n",
      "Epoch: 105 | Batch: 002 / 011 | Total loss: 1.604 | Reg loss: 0.038 | Tree loss: 1.604 | Accuracy: 0.419500 | 1.642 sec/iter\n",
      "Epoch: 105 | Batch: 003 / 011 | Total loss: 1.546 | Reg loss: 0.038 | Tree loss: 1.546 | Accuracy: 0.458000 | 1.642 sec/iter\n",
      "Epoch: 105 | Batch: 004 / 011 | Total loss: 1.552 | Reg loss: 0.038 | Tree loss: 1.552 | Accuracy: 0.459500 | 1.641 sec/iter\n",
      "Epoch: 105 | Batch: 005 / 011 | Total loss: 1.503 | Reg loss: 0.038 | Tree loss: 1.503 | Accuracy: 0.474500 | 1.641 sec/iter\n",
      "Epoch: 105 | Batch: 006 / 011 | Total loss: 1.508 | Reg loss: 0.038 | Tree loss: 1.508 | Accuracy: 0.491000 | 1.641 sec/iter\n",
      "Epoch: 105 | Batch: 007 / 011 | Total loss: 1.465 | Reg loss: 0.038 | Tree loss: 1.465 | Accuracy: 0.485500 | 1.641 sec/iter\n",
      "Epoch: 105 | Batch: 008 / 011 | Total loss: 1.473 | Reg loss: 0.038 | Tree loss: 1.473 | Accuracy: 0.474500 | 1.641 sec/iter\n",
      "Epoch: 105 | Batch: 009 / 011 | Total loss: 1.468 | Reg loss: 0.038 | Tree loss: 1.468 | Accuracy: 0.460000 | 1.64 sec/iter\n",
      "Epoch: 105 | Batch: 010 / 011 | Total loss: 1.451 | Reg loss: 0.038 | Tree loss: 1.451 | Accuracy: 0.457338 | 1.64 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 106 | Batch: 000 / 011 | Total loss: 1.669 | Reg loss: 0.038 | Tree loss: 1.669 | Accuracy: 0.404000 | 1.644 sec/iter\n",
      "Epoch: 106 | Batch: 001 / 011 | Total loss: 1.641 | Reg loss: 0.038 | Tree loss: 1.641 | Accuracy: 0.399500 | 1.644 sec/iter\n",
      "Epoch: 106 | Batch: 002 / 011 | Total loss: 1.587 | Reg loss: 0.038 | Tree loss: 1.587 | Accuracy: 0.430500 | 1.644 sec/iter\n",
      "Epoch: 106 | Batch: 003 / 011 | Total loss: 1.566 | Reg loss: 0.038 | Tree loss: 1.566 | Accuracy: 0.449000 | 1.643 sec/iter\n",
      "Epoch: 106 | Batch: 004 / 011 | Total loss: 1.522 | Reg loss: 0.038 | Tree loss: 1.522 | Accuracy: 0.480000 | 1.644 sec/iter\n",
      "Epoch: 106 | Batch: 005 / 011 | Total loss: 1.502 | Reg loss: 0.038 | Tree loss: 1.502 | Accuracy: 0.470000 | 1.643 sec/iter\n",
      "Epoch: 106 | Batch: 006 / 011 | Total loss: 1.482 | Reg loss: 0.038 | Tree loss: 1.482 | Accuracy: 0.506000 | 1.643 sec/iter\n",
      "Epoch: 106 | Batch: 007 / 011 | Total loss: 1.490 | Reg loss: 0.038 | Tree loss: 1.490 | Accuracy: 0.467000 | 1.643 sec/iter\n",
      "Epoch: 106 | Batch: 008 / 011 | Total loss: 1.491 | Reg loss: 0.038 | Tree loss: 1.491 | Accuracy: 0.469500 | 1.643 sec/iter\n",
      "Epoch: 106 | Batch: 009 / 011 | Total loss: 1.464 | Reg loss: 0.038 | Tree loss: 1.464 | Accuracy: 0.475000 | 1.643 sec/iter\n",
      "Epoch: 106 | Batch: 010 / 011 | Total loss: 1.489 | Reg loss: 0.038 | Tree loss: 1.489 | Accuracy: 0.474403 | 1.642 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 107 | Batch: 000 / 011 | Total loss: 1.687 | Reg loss: 0.038 | Tree loss: 1.687 | Accuracy: 0.377500 | 1.643 sec/iter\n",
      "Epoch: 107 | Batch: 001 / 011 | Total loss: 1.664 | Reg loss: 0.038 | Tree loss: 1.664 | Accuracy: 0.387500 | 1.642 sec/iter\n",
      "Epoch: 107 | Batch: 002 / 011 | Total loss: 1.588 | Reg loss: 0.038 | Tree loss: 1.588 | Accuracy: 0.426000 | 1.642 sec/iter\n",
      "Epoch: 107 | Batch: 003 / 011 | Total loss: 1.553 | Reg loss: 0.038 | Tree loss: 1.553 | Accuracy: 0.460000 | 1.642 sec/iter\n",
      "Epoch: 107 | Batch: 004 / 011 | Total loss: 1.519 | Reg loss: 0.038 | Tree loss: 1.519 | Accuracy: 0.476500 | 1.642 sec/iter\n",
      "Epoch: 107 | Batch: 005 / 011 | Total loss: 1.506 | Reg loss: 0.038 | Tree loss: 1.506 | Accuracy: 0.487500 | 1.642 sec/iter\n",
      "Epoch: 107 | Batch: 006 / 011 | Total loss: 1.479 | Reg loss: 0.038 | Tree loss: 1.479 | Accuracy: 0.493000 | 1.642 sec/iter\n",
      "Epoch: 107 | Batch: 007 / 011 | Total loss: 1.480 | Reg loss: 0.038 | Tree loss: 1.480 | Accuracy: 0.472500 | 1.641 sec/iter\n",
      "Epoch: 107 | Batch: 008 / 011 | Total loss: 1.472 | Reg loss: 0.038 | Tree loss: 1.472 | Accuracy: 0.460500 | 1.642 sec/iter\n",
      "Epoch: 107 | Batch: 009 / 011 | Total loss: 1.477 | Reg loss: 0.038 | Tree loss: 1.477 | Accuracy: 0.474500 | 1.641 sec/iter\n",
      "Epoch: 107 | Batch: 010 / 011 | Total loss: 1.453 | Reg loss: 0.038 | Tree loss: 1.453 | Accuracy: 0.423208 | 1.64 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 108 | Batch: 000 / 011 | Total loss: 1.680 | Reg loss: 0.038 | Tree loss: 1.680 | Accuracy: 0.378000 | 1.647 sec/iter\n",
      "Epoch: 108 | Batch: 001 / 011 | Total loss: 1.641 | Reg loss: 0.038 | Tree loss: 1.641 | Accuracy: 0.400000 | 1.647 sec/iter\n",
      "Epoch: 108 | Batch: 002 / 011 | Total loss: 1.608 | Reg loss: 0.038 | Tree loss: 1.608 | Accuracy: 0.430500 | 1.647 sec/iter\n",
      "Epoch: 108 | Batch: 003 / 011 | Total loss: 1.575 | Reg loss: 0.038 | Tree loss: 1.575 | Accuracy: 0.436500 | 1.647 sec/iter\n",
      "Epoch: 108 | Batch: 004 / 011 | Total loss: 1.525 | Reg loss: 0.038 | Tree loss: 1.525 | Accuracy: 0.493000 | 1.647 sec/iter\n",
      "Epoch: 108 | Batch: 005 / 011 | Total loss: 1.507 | Reg loss: 0.038 | Tree loss: 1.507 | Accuracy: 0.472000 | 1.647 sec/iter\n",
      "Epoch: 108 | Batch: 006 / 011 | Total loss: 1.480 | Reg loss: 0.038 | Tree loss: 1.480 | Accuracy: 0.488500 | 1.646 sec/iter\n",
      "Epoch: 108 | Batch: 007 / 011 | Total loss: 1.471 | Reg loss: 0.038 | Tree loss: 1.471 | Accuracy: 0.483000 | 1.646 sec/iter\n",
      "Epoch: 108 | Batch: 008 / 011 | Total loss: 1.480 | Reg loss: 0.038 | Tree loss: 1.480 | Accuracy: 0.474500 | 1.646 sec/iter\n",
      "Epoch: 108 | Batch: 009 / 011 | Total loss: 1.462 | Reg loss: 0.038 | Tree loss: 1.462 | Accuracy: 0.467500 | 1.646 sec/iter\n",
      "Epoch: 108 | Batch: 010 / 011 | Total loss: 1.470 | Reg loss: 0.038 | Tree loss: 1.470 | Accuracy: 0.539249 | 1.645 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 109 | Batch: 000 / 011 | Total loss: 1.696 | Reg loss: 0.038 | Tree loss: 1.696 | Accuracy: 0.368500 | 1.646 sec/iter\n",
      "Epoch: 109 | Batch: 001 / 011 | Total loss: 1.605 | Reg loss: 0.038 | Tree loss: 1.605 | Accuracy: 0.431500 | 1.646 sec/iter\n",
      "Epoch: 109 | Batch: 002 / 011 | Total loss: 1.630 | Reg loss: 0.038 | Tree loss: 1.630 | Accuracy: 0.417000 | 1.646 sec/iter\n",
      "Epoch: 109 | Batch: 003 / 011 | Total loss: 1.557 | Reg loss: 0.038 | Tree loss: 1.557 | Accuracy: 0.433500 | 1.645 sec/iter\n",
      "Epoch: 109 | Batch: 004 / 011 | Total loss: 1.513 | Reg loss: 0.038 | Tree loss: 1.513 | Accuracy: 0.462500 | 1.645 sec/iter\n",
      "Epoch: 109 | Batch: 005 / 011 | Total loss: 1.495 | Reg loss: 0.038 | Tree loss: 1.495 | Accuracy: 0.493500 | 1.645 sec/iter\n",
      "Epoch: 109 | Batch: 006 / 011 | Total loss: 1.474 | Reg loss: 0.038 | Tree loss: 1.474 | Accuracy: 0.485500 | 1.645 sec/iter\n",
      "Epoch: 109 | Batch: 007 / 011 | Total loss: 1.458 | Reg loss: 0.038 | Tree loss: 1.458 | Accuracy: 0.476000 | 1.645 sec/iter\n",
      "Epoch: 109 | Batch: 008 / 011 | Total loss: 1.487 | Reg loss: 0.038 | Tree loss: 1.487 | Accuracy: 0.454000 | 1.645 sec/iter\n",
      "Epoch: 109 | Batch: 009 / 011 | Total loss: 1.482 | Reg loss: 0.038 | Tree loss: 1.482 | Accuracy: 0.474000 | 1.645 sec/iter\n",
      "Epoch: 109 | Batch: 010 / 011 | Total loss: 1.486 | Reg loss: 0.038 | Tree loss: 1.486 | Accuracy: 0.484642 | 1.644 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 110 | Batch: 000 / 011 | Total loss: 1.688 | Reg loss: 0.038 | Tree loss: 1.688 | Accuracy: 0.386500 | 1.645 sec/iter\n",
      "Epoch: 110 | Batch: 001 / 011 | Total loss: 1.635 | Reg loss: 0.038 | Tree loss: 1.635 | Accuracy: 0.386000 | 1.644 sec/iter\n",
      "Epoch: 110 | Batch: 002 / 011 | Total loss: 1.593 | Reg loss: 0.038 | Tree loss: 1.593 | Accuracy: 0.442000 | 1.644 sec/iter\n",
      "Epoch: 110 | Batch: 003 / 011 | Total loss: 1.544 | Reg loss: 0.038 | Tree loss: 1.544 | Accuracy: 0.462500 | 1.643 sec/iter\n",
      "Epoch: 110 | Batch: 004 / 011 | Total loss: 1.521 | Reg loss: 0.038 | Tree loss: 1.521 | Accuracy: 0.476500 | 1.643 sec/iter\n",
      "Epoch: 110 | Batch: 005 / 011 | Total loss: 1.498 | Reg loss: 0.038 | Tree loss: 1.498 | Accuracy: 0.487000 | 1.642 sec/iter\n",
      "Epoch: 110 | Batch: 006 / 011 | Total loss: 1.476 | Reg loss: 0.038 | Tree loss: 1.476 | Accuracy: 0.468000 | 1.642 sec/iter\n",
      "Epoch: 110 | Batch: 007 / 011 | Total loss: 1.470 | Reg loss: 0.038 | Tree loss: 1.470 | Accuracy: 0.487000 | 1.641 sec/iter\n",
      "Epoch: 110 | Batch: 008 / 011 | Total loss: 1.477 | Reg loss: 0.038 | Tree loss: 1.477 | Accuracy: 0.466500 | 1.641 sec/iter\n",
      "Epoch: 110 | Batch: 009 / 011 | Total loss: 1.478 | Reg loss: 0.038 | Tree loss: 1.478 | Accuracy: 0.454000 | 1.641 sec/iter\n",
      "Epoch: 110 | Batch: 010 / 011 | Total loss: 1.452 | Reg loss: 0.038 | Tree loss: 1.452 | Accuracy: 0.450512 | 1.64 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 111 | Batch: 000 / 011 | Total loss: 1.658 | Reg loss: 0.038 | Tree loss: 1.658 | Accuracy: 0.394000 | 1.645 sec/iter\n",
      "Epoch: 111 | Batch: 001 / 011 | Total loss: 1.640 | Reg loss: 0.038 | Tree loss: 1.640 | Accuracy: 0.416500 | 1.645 sec/iter\n",
      "Epoch: 111 | Batch: 002 / 011 | Total loss: 1.603 | Reg loss: 0.038 | Tree loss: 1.603 | Accuracy: 0.438000 | 1.645 sec/iter\n",
      "Epoch: 111 | Batch: 003 / 011 | Total loss: 1.562 | Reg loss: 0.038 | Tree loss: 1.562 | Accuracy: 0.471500 | 1.645 sec/iter\n",
      "Epoch: 111 | Batch: 004 / 011 | Total loss: 1.521 | Reg loss: 0.038 | Tree loss: 1.521 | Accuracy: 0.475000 | 1.645 sec/iter\n",
      "Epoch: 111 | Batch: 005 / 011 | Total loss: 1.512 | Reg loss: 0.038 | Tree loss: 1.512 | Accuracy: 0.459500 | 1.645 sec/iter\n",
      "Epoch: 111 | Batch: 006 / 011 | Total loss: 1.492 | Reg loss: 0.038 | Tree loss: 1.492 | Accuracy: 0.465500 | 1.644 sec/iter\n",
      "Epoch: 111 | Batch: 007 / 011 | Total loss: 1.484 | Reg loss: 0.038 | Tree loss: 1.484 | Accuracy: 0.470000 | 1.644 sec/iter\n",
      "Epoch: 111 | Batch: 008 / 011 | Total loss: 1.457 | Reg loss: 0.038 | Tree loss: 1.457 | Accuracy: 0.474500 | 1.644 sec/iter\n",
      "Epoch: 111 | Batch: 009 / 011 | Total loss: 1.445 | Reg loss: 0.038 | Tree loss: 1.445 | Accuracy: 0.484000 | 1.644 sec/iter\n",
      "Epoch: 111 | Batch: 010 / 011 | Total loss: 1.473 | Reg loss: 0.038 | Tree loss: 1.473 | Accuracy: 0.474403 | 1.643 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 112 | Batch: 000 / 011 | Total loss: 1.683 | Reg loss: 0.038 | Tree loss: 1.683 | Accuracy: 0.368000 | 1.646 sec/iter\n",
      "Epoch: 112 | Batch: 001 / 011 | Total loss: 1.644 | Reg loss: 0.038 | Tree loss: 1.644 | Accuracy: 0.402000 | 1.646 sec/iter\n",
      "Epoch: 112 | Batch: 002 / 011 | Total loss: 1.604 | Reg loss: 0.038 | Tree loss: 1.604 | Accuracy: 0.436500 | 1.646 sec/iter\n",
      "Epoch: 112 | Batch: 003 / 011 | Total loss: 1.566 | Reg loss: 0.038 | Tree loss: 1.566 | Accuracy: 0.452000 | 1.645 sec/iter\n",
      "Epoch: 112 | Batch: 004 / 011 | Total loss: 1.506 | Reg loss: 0.038 | Tree loss: 1.506 | Accuracy: 0.476000 | 1.645 sec/iter\n",
      "Epoch: 112 | Batch: 005 / 011 | Total loss: 1.503 | Reg loss: 0.038 | Tree loss: 1.503 | Accuracy: 0.478000 | 1.645 sec/iter\n",
      "Epoch: 112 | Batch: 006 / 011 | Total loss: 1.466 | Reg loss: 0.038 | Tree loss: 1.466 | Accuracy: 0.480500 | 1.645 sec/iter\n",
      "Epoch: 112 | Batch: 007 / 011 | Total loss: 1.462 | Reg loss: 0.038 | Tree loss: 1.462 | Accuracy: 0.467500 | 1.645 sec/iter\n",
      "Epoch: 112 | Batch: 008 / 011 | Total loss: 1.472 | Reg loss: 0.038 | Tree loss: 1.472 | Accuracy: 0.485500 | 1.644 sec/iter\n",
      "Epoch: 112 | Batch: 009 / 011 | Total loss: 1.464 | Reg loss: 0.038 | Tree loss: 1.464 | Accuracy: 0.468500 | 1.644 sec/iter\n",
      "Epoch: 112 | Batch: 010 / 011 | Total loss: 1.411 | Reg loss: 0.038 | Tree loss: 1.411 | Accuracy: 0.501706 | 1.644 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 113 | Batch: 000 / 011 | Total loss: 1.667 | Reg loss: 0.038 | Tree loss: 1.667 | Accuracy: 0.391000 | 1.648 sec/iter\n",
      "Epoch: 113 | Batch: 001 / 011 | Total loss: 1.639 | Reg loss: 0.038 | Tree loss: 1.639 | Accuracy: 0.404500 | 1.648 sec/iter\n",
      "Epoch: 113 | Batch: 002 / 011 | Total loss: 1.602 | Reg loss: 0.038 | Tree loss: 1.602 | Accuracy: 0.429500 | 1.647 sec/iter\n",
      "Epoch: 113 | Batch: 003 / 011 | Total loss: 1.555 | Reg loss: 0.038 | Tree loss: 1.555 | Accuracy: 0.465500 | 1.647 sec/iter\n",
      "Epoch: 113 | Batch: 004 / 011 | Total loss: 1.507 | Reg loss: 0.038 | Tree loss: 1.507 | Accuracy: 0.470500 | 1.647 sec/iter\n",
      "Epoch: 113 | Batch: 005 / 011 | Total loss: 1.472 | Reg loss: 0.038 | Tree loss: 1.472 | Accuracy: 0.485500 | 1.647 sec/iter\n",
      "Epoch: 113 | Batch: 006 / 011 | Total loss: 1.477 | Reg loss: 0.038 | Tree loss: 1.477 | Accuracy: 0.482500 | 1.647 sec/iter\n",
      "Epoch: 113 | Batch: 007 / 011 | Total loss: 1.477 | Reg loss: 0.038 | Tree loss: 1.477 | Accuracy: 0.481500 | 1.647 sec/iter\n",
      "Epoch: 113 | Batch: 008 / 011 | Total loss: 1.458 | Reg loss: 0.038 | Tree loss: 1.458 | Accuracy: 0.471000 | 1.647 sec/iter\n",
      "Epoch: 113 | Batch: 009 / 011 | Total loss: 1.476 | Reg loss: 0.038 | Tree loss: 1.476 | Accuracy: 0.451000 | 1.647 sec/iter\n",
      "Epoch: 113 | Batch: 010 / 011 | Total loss: 1.493 | Reg loss: 0.038 | Tree loss: 1.493 | Accuracy: 0.460751 | 1.646 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 114 | Batch: 000 / 011 | Total loss: 1.687 | Reg loss: 0.038 | Tree loss: 1.687 | Accuracy: 0.387500 | 1.646 sec/iter\n",
      "Epoch: 114 | Batch: 001 / 011 | Total loss: 1.644 | Reg loss: 0.038 | Tree loss: 1.644 | Accuracy: 0.410500 | 1.646 sec/iter\n",
      "Epoch: 114 | Batch: 002 / 011 | Total loss: 1.609 | Reg loss: 0.038 | Tree loss: 1.609 | Accuracy: 0.419500 | 1.646 sec/iter\n",
      "Epoch: 114 | Batch: 003 / 011 | Total loss: 1.560 | Reg loss: 0.038 | Tree loss: 1.560 | Accuracy: 0.447500 | 1.646 sec/iter\n",
      "Epoch: 114 | Batch: 004 / 011 | Total loss: 1.510 | Reg loss: 0.038 | Tree loss: 1.510 | Accuracy: 0.469000 | 1.646 sec/iter\n",
      "Epoch: 114 | Batch: 005 / 011 | Total loss: 1.501 | Reg loss: 0.038 | Tree loss: 1.501 | Accuracy: 0.476500 | 1.646 sec/iter\n",
      "Epoch: 114 | Batch: 006 / 011 | Total loss: 1.457 | Reg loss: 0.038 | Tree loss: 1.457 | Accuracy: 0.482500 | 1.646 sec/iter\n",
      "Epoch: 114 | Batch: 007 / 011 | Total loss: 1.466 | Reg loss: 0.038 | Tree loss: 1.466 | Accuracy: 0.482500 | 1.645 sec/iter\n",
      "Epoch: 114 | Batch: 008 / 011 | Total loss: 1.462 | Reg loss: 0.038 | Tree loss: 1.462 | Accuracy: 0.472500 | 1.645 sec/iter\n",
      "Epoch: 114 | Batch: 009 / 011 | Total loss: 1.446 | Reg loss: 0.038 | Tree loss: 1.446 | Accuracy: 0.479500 | 1.645 sec/iter\n",
      "Epoch: 114 | Batch: 010 / 011 | Total loss: 1.441 | Reg loss: 0.038 | Tree loss: 1.441 | Accuracy: 0.501706 | 1.644 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 115 | Batch: 000 / 011 | Total loss: 1.656 | Reg loss: 0.038 | Tree loss: 1.656 | Accuracy: 0.391000 | 1.651 sec/iter\n",
      "Epoch: 115 | Batch: 001 / 011 | Total loss: 1.635 | Reg loss: 0.038 | Tree loss: 1.635 | Accuracy: 0.415500 | 1.651 sec/iter\n",
      "Epoch: 115 | Batch: 002 / 011 | Total loss: 1.588 | Reg loss: 0.038 | Tree loss: 1.588 | Accuracy: 0.438000 | 1.65 sec/iter\n",
      "Epoch: 115 | Batch: 003 / 011 | Total loss: 1.556 | Reg loss: 0.038 | Tree loss: 1.556 | Accuracy: 0.453500 | 1.65 sec/iter\n",
      "Epoch: 115 | Batch: 004 / 011 | Total loss: 1.519 | Reg loss: 0.038 | Tree loss: 1.519 | Accuracy: 0.477500 | 1.65 sec/iter\n",
      "Epoch: 115 | Batch: 005 / 011 | Total loss: 1.501 | Reg loss: 0.038 | Tree loss: 1.501 | Accuracy: 0.489500 | 1.65 sec/iter\n",
      "Epoch: 115 | Batch: 006 / 011 | Total loss: 1.486 | Reg loss: 0.038 | Tree loss: 1.486 | Accuracy: 0.478500 | 1.65 sec/iter\n",
      "Epoch: 115 | Batch: 007 / 011 | Total loss: 1.483 | Reg loss: 0.038 | Tree loss: 1.483 | Accuracy: 0.464500 | 1.65 sec/iter\n",
      "Epoch: 115 | Batch: 008 / 011 | Total loss: 1.443 | Reg loss: 0.038 | Tree loss: 1.443 | Accuracy: 0.482500 | 1.65 sec/iter\n",
      "Epoch: 115 | Batch: 009 / 011 | Total loss: 1.464 | Reg loss: 0.038 | Tree loss: 1.464 | Accuracy: 0.469000 | 1.649 sec/iter\n",
      "Epoch: 115 | Batch: 010 / 011 | Total loss: 1.507 | Reg loss: 0.038 | Tree loss: 1.507 | Accuracy: 0.430034 | 1.649 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 116 | Batch: 000 / 011 | Total loss: 1.664 | Reg loss: 0.038 | Tree loss: 1.664 | Accuracy: 0.384000 | 1.649 sec/iter\n",
      "Epoch: 116 | Batch: 001 / 011 | Total loss: 1.631 | Reg loss: 0.038 | Tree loss: 1.631 | Accuracy: 0.383500 | 1.649 sec/iter\n",
      "Epoch: 116 | Batch: 002 / 011 | Total loss: 1.601 | Reg loss: 0.038 | Tree loss: 1.601 | Accuracy: 0.434500 | 1.649 sec/iter\n",
      "Epoch: 116 | Batch: 003 / 011 | Total loss: 1.575 | Reg loss: 0.038 | Tree loss: 1.575 | Accuracy: 0.441500 | 1.649 sec/iter\n",
      "Epoch: 116 | Batch: 004 / 011 | Total loss: 1.513 | Reg loss: 0.038 | Tree loss: 1.513 | Accuracy: 0.471500 | 1.649 sec/iter\n",
      "Epoch: 116 | Batch: 005 / 011 | Total loss: 1.486 | Reg loss: 0.038 | Tree loss: 1.486 | Accuracy: 0.481500 | 1.649 sec/iter\n",
      "Epoch: 116 | Batch: 006 / 011 | Total loss: 1.456 | Reg loss: 0.038 | Tree loss: 1.456 | Accuracy: 0.497500 | 1.648 sec/iter\n",
      "Epoch: 116 | Batch: 007 / 011 | Total loss: 1.494 | Reg loss: 0.038 | Tree loss: 1.494 | Accuracy: 0.489000 | 1.648 sec/iter\n",
      "Epoch: 116 | Batch: 008 / 011 | Total loss: 1.451 | Reg loss: 0.038 | Tree loss: 1.451 | Accuracy: 0.467500 | 1.648 sec/iter\n",
      "Epoch: 116 | Batch: 009 / 011 | Total loss: 1.454 | Reg loss: 0.038 | Tree loss: 1.454 | Accuracy: 0.471500 | 1.648 sec/iter\n",
      "Epoch: 116 | Batch: 010 / 011 | Total loss: 1.442 | Reg loss: 0.038 | Tree loss: 1.442 | Accuracy: 0.484642 | 1.647 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 117 | Batch: 000 / 011 | Total loss: 1.649 | Reg loss: 0.038 | Tree loss: 1.649 | Accuracy: 0.399000 | 1.653 sec/iter\n",
      "Epoch: 117 | Batch: 001 / 011 | Total loss: 1.634 | Reg loss: 0.038 | Tree loss: 1.634 | Accuracy: 0.404000 | 1.653 sec/iter\n",
      "Epoch: 117 | Batch: 002 / 011 | Total loss: 1.562 | Reg loss: 0.038 | Tree loss: 1.562 | Accuracy: 0.450500 | 1.653 sec/iter\n",
      "Epoch: 117 | Batch: 003 / 011 | Total loss: 1.550 | Reg loss: 0.038 | Tree loss: 1.550 | Accuracy: 0.444500 | 1.653 sec/iter\n",
      "Epoch: 117 | Batch: 004 / 011 | Total loss: 1.517 | Reg loss: 0.038 | Tree loss: 1.517 | Accuracy: 0.473000 | 1.652 sec/iter\n",
      "Epoch: 117 | Batch: 005 / 011 | Total loss: 1.490 | Reg loss: 0.038 | Tree loss: 1.490 | Accuracy: 0.478000 | 1.652 sec/iter\n",
      "Epoch: 117 | Batch: 006 / 011 | Total loss: 1.493 | Reg loss: 0.038 | Tree loss: 1.493 | Accuracy: 0.467000 | 1.651 sec/iter\n",
      "Epoch: 117 | Batch: 007 / 011 | Total loss: 1.479 | Reg loss: 0.038 | Tree loss: 1.479 | Accuracy: 0.482500 | 1.651 sec/iter\n",
      "Epoch: 117 | Batch: 008 / 011 | Total loss: 1.467 | Reg loss: 0.038 | Tree loss: 1.467 | Accuracy: 0.477500 | 1.65 sec/iter\n",
      "Epoch: 117 | Batch: 009 / 011 | Total loss: 1.453 | Reg loss: 0.038 | Tree loss: 1.453 | Accuracy: 0.474500 | 1.65 sec/iter\n",
      "Epoch: 117 | Batch: 010 / 011 | Total loss: 1.470 | Reg loss: 0.038 | Tree loss: 1.470 | Accuracy: 0.460751 | 1.649 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 118 | Batch: 000 / 011 | Total loss: 1.659 | Reg loss: 0.038 | Tree loss: 1.659 | Accuracy: 0.390500 | 1.654 sec/iter\n",
      "Epoch: 118 | Batch: 001 / 011 | Total loss: 1.651 | Reg loss: 0.038 | Tree loss: 1.651 | Accuracy: 0.384500 | 1.654 sec/iter\n",
      "Epoch: 118 | Batch: 002 / 011 | Total loss: 1.594 | Reg loss: 0.038 | Tree loss: 1.594 | Accuracy: 0.429000 | 1.653 sec/iter\n",
      "Epoch: 118 | Batch: 003 / 011 | Total loss: 1.542 | Reg loss: 0.038 | Tree loss: 1.542 | Accuracy: 0.465500 | 1.653 sec/iter\n",
      "Epoch: 118 | Batch: 004 / 011 | Total loss: 1.523 | Reg loss: 0.038 | Tree loss: 1.523 | Accuracy: 0.465500 | 1.653 sec/iter\n",
      "Epoch: 118 | Batch: 005 / 011 | Total loss: 1.492 | Reg loss: 0.038 | Tree loss: 1.492 | Accuracy: 0.464500 | 1.653 sec/iter\n",
      "Epoch: 118 | Batch: 006 / 011 | Total loss: 1.471 | Reg loss: 0.038 | Tree loss: 1.471 | Accuracy: 0.481500 | 1.653 sec/iter\n",
      "Epoch: 118 | Batch: 007 / 011 | Total loss: 1.445 | Reg loss: 0.038 | Tree loss: 1.445 | Accuracy: 0.492500 | 1.653 sec/iter\n",
      "Epoch: 118 | Batch: 008 / 011 | Total loss: 1.458 | Reg loss: 0.038 | Tree loss: 1.458 | Accuracy: 0.469000 | 1.653 sec/iter\n",
      "Epoch: 118 | Batch: 009 / 011 | Total loss: 1.461 | Reg loss: 0.038 | Tree loss: 1.461 | Accuracy: 0.467000 | 1.652 sec/iter\n",
      "Epoch: 118 | Batch: 010 / 011 | Total loss: 1.412 | Reg loss: 0.038 | Tree loss: 1.412 | Accuracy: 0.477816 | 1.652 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 119 | Batch: 000 / 011 | Total loss: 1.662 | Reg loss: 0.038 | Tree loss: 1.662 | Accuracy: 0.390000 | 1.653 sec/iter\n",
      "Epoch: 119 | Batch: 001 / 011 | Total loss: 1.629 | Reg loss: 0.038 | Tree loss: 1.629 | Accuracy: 0.412500 | 1.653 sec/iter\n",
      "Epoch: 119 | Batch: 002 / 011 | Total loss: 1.582 | Reg loss: 0.038 | Tree loss: 1.582 | Accuracy: 0.419000 | 1.653 sec/iter\n",
      "Epoch: 119 | Batch: 003 / 011 | Total loss: 1.546 | Reg loss: 0.038 | Tree loss: 1.546 | Accuracy: 0.459500 | 1.653 sec/iter\n",
      "Epoch: 119 | Batch: 004 / 011 | Total loss: 1.515 | Reg loss: 0.038 | Tree loss: 1.515 | Accuracy: 0.480500 | 1.653 sec/iter\n",
      "Epoch: 119 | Batch: 005 / 011 | Total loss: 1.495 | Reg loss: 0.038 | Tree loss: 1.495 | Accuracy: 0.477000 | 1.652 sec/iter\n",
      "Epoch: 119 | Batch: 006 / 011 | Total loss: 1.479 | Reg loss: 0.038 | Tree loss: 1.479 | Accuracy: 0.444000 | 1.652 sec/iter\n",
      "Epoch: 119 | Batch: 007 / 011 | Total loss: 1.449 | Reg loss: 0.038 | Tree loss: 1.449 | Accuracy: 0.486000 | 1.652 sec/iter\n",
      "Epoch: 119 | Batch: 008 / 011 | Total loss: 1.447 | Reg loss: 0.038 | Tree loss: 1.447 | Accuracy: 0.486000 | 1.652 sec/iter\n",
      "Epoch: 119 | Batch: 009 / 011 | Total loss: 1.468 | Reg loss: 0.038 | Tree loss: 1.468 | Accuracy: 0.487500 | 1.651 sec/iter\n",
      "Epoch: 119 | Batch: 010 / 011 | Total loss: 1.435 | Reg loss: 0.038 | Tree loss: 1.435 | Accuracy: 0.477816 | 1.651 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 8: 0.9723756906077345\n",
      "Epoch: 120 | Batch: 000 / 011 | Total loss: 1.688 | Reg loss: 0.038 | Tree loss: 1.688 | Accuracy: 0.368500 | 1.656 sec/iter\n",
      "Epoch: 120 | Batch: 001 / 011 | Total loss: 1.619 | Reg loss: 0.038 | Tree loss: 1.619 | Accuracy: 0.416000 | 1.655 sec/iter\n",
      "Epoch: 120 | Batch: 002 / 011 | Total loss: 1.589 | Reg loss: 0.038 | Tree loss: 1.589 | Accuracy: 0.425000 | 1.655 sec/iter\n",
      "Epoch: 120 | Batch: 003 / 011 | Total loss: 1.547 | Reg loss: 0.038 | Tree loss: 1.547 | Accuracy: 0.457000 | 1.655 sec/iter\n",
      "Epoch: 120 | Batch: 004 / 011 | Total loss: 1.513 | Reg loss: 0.038 | Tree loss: 1.513 | Accuracy: 0.463000 | 1.655 sec/iter\n",
      "Epoch: 120 | Batch: 005 / 011 | Total loss: 1.479 | Reg loss: 0.038 | Tree loss: 1.479 | Accuracy: 0.486000 | 1.655 sec/iter\n",
      "Epoch: 120 | Batch: 006 / 011 | Total loss: 1.447 | Reg loss: 0.038 | Tree loss: 1.447 | Accuracy: 0.498500 | 1.654 sec/iter\n",
      "Epoch: 120 | Batch: 007 / 011 | Total loss: 1.466 | Reg loss: 0.038 | Tree loss: 1.466 | Accuracy: 0.488000 | 1.654 sec/iter\n",
      "Epoch: 120 | Batch: 008 / 011 | Total loss: 1.469 | Reg loss: 0.038 | Tree loss: 1.469 | Accuracy: 0.446000 | 1.654 sec/iter\n",
      "Epoch: 120 | Batch: 009 / 011 | Total loss: 1.448 | Reg loss: 0.038 | Tree loss: 1.448 | Accuracy: 0.485500 | 1.654 sec/iter\n",
      "Epoch: 120 | Batch: 010 / 011 | Total loss: 1.500 | Reg loss: 0.038 | Tree loss: 1.500 | Accuracy: 0.477816 | 1.653 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 121 | Batch: 000 / 011 | Total loss: 1.673 | Reg loss: 0.038 | Tree loss: 1.673 | Accuracy: 0.387500 | 1.653 sec/iter\n",
      "Epoch: 121 | Batch: 001 / 011 | Total loss: 1.608 | Reg loss: 0.038 | Tree loss: 1.608 | Accuracy: 0.403000 | 1.653 sec/iter\n",
      "Epoch: 121 | Batch: 002 / 011 | Total loss: 1.562 | Reg loss: 0.038 | Tree loss: 1.562 | Accuracy: 0.425500 | 1.653 sec/iter\n",
      "Epoch: 121 | Batch: 003 / 011 | Total loss: 1.547 | Reg loss: 0.038 | Tree loss: 1.547 | Accuracy: 0.433500 | 1.653 sec/iter\n",
      "Epoch: 121 | Batch: 004 / 011 | Total loss: 1.541 | Reg loss: 0.038 | Tree loss: 1.541 | Accuracy: 0.469000 | 1.652 sec/iter\n",
      "Epoch: 121 | Batch: 005 / 011 | Total loss: 1.502 | Reg loss: 0.038 | Tree loss: 1.502 | Accuracy: 0.463500 | 1.652 sec/iter\n",
      "Epoch: 121 | Batch: 006 / 011 | Total loss: 1.478 | Reg loss: 0.038 | Tree loss: 1.478 | Accuracy: 0.485000 | 1.652 sec/iter\n",
      "Epoch: 121 | Batch: 007 / 011 | Total loss: 1.465 | Reg loss: 0.038 | Tree loss: 1.465 | Accuracy: 0.497000 | 1.652 sec/iter\n",
      "Epoch: 121 | Batch: 008 / 011 | Total loss: 1.455 | Reg loss: 0.038 | Tree loss: 1.455 | Accuracy: 0.466500 | 1.652 sec/iter\n",
      "Epoch: 121 | Batch: 009 / 011 | Total loss: 1.444 | Reg loss: 0.038 | Tree loss: 1.444 | Accuracy: 0.466500 | 1.651 sec/iter\n",
      "Epoch: 121 | Batch: 010 / 011 | Total loss: 1.397 | Reg loss: 0.038 | Tree loss: 1.397 | Accuracy: 0.522184 | 1.65 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 122 | Batch: 000 / 011 | Total loss: 1.685 | Reg loss: 0.038 | Tree loss: 1.685 | Accuracy: 0.391000 | 1.655 sec/iter\n",
      "Epoch: 122 | Batch: 001 / 011 | Total loss: 1.631 | Reg loss: 0.038 | Tree loss: 1.631 | Accuracy: 0.400000 | 1.655 sec/iter\n",
      "Epoch: 122 | Batch: 002 / 011 | Total loss: 1.578 | Reg loss: 0.038 | Tree loss: 1.578 | Accuracy: 0.430000 | 1.655 sec/iter\n",
      "Epoch: 122 | Batch: 003 / 011 | Total loss: 1.557 | Reg loss: 0.038 | Tree loss: 1.557 | Accuracy: 0.446500 | 1.655 sec/iter\n",
      "Epoch: 122 | Batch: 004 / 011 | Total loss: 1.477 | Reg loss: 0.038 | Tree loss: 1.477 | Accuracy: 0.484000 | 1.654 sec/iter\n",
      "Epoch: 122 | Batch: 005 / 011 | Total loss: 1.487 | Reg loss: 0.038 | Tree loss: 1.487 | Accuracy: 0.485500 | 1.654 sec/iter\n",
      "Epoch: 122 | Batch: 006 / 011 | Total loss: 1.442 | Reg loss: 0.038 | Tree loss: 1.442 | Accuracy: 0.505000 | 1.654 sec/iter\n",
      "Epoch: 122 | Batch: 007 / 011 | Total loss: 1.463 | Reg loss: 0.038 | Tree loss: 1.463 | Accuracy: 0.475000 | 1.654 sec/iter\n",
      "Epoch: 122 | Batch: 008 / 011 | Total loss: 1.467 | Reg loss: 0.038 | Tree loss: 1.467 | Accuracy: 0.462500 | 1.654 sec/iter\n",
      "Epoch: 122 | Batch: 009 / 011 | Total loss: 1.484 | Reg loss: 0.038 | Tree loss: 1.484 | Accuracy: 0.454000 | 1.653 sec/iter\n",
      "Epoch: 122 | Batch: 010 / 011 | Total loss: 1.445 | Reg loss: 0.038 | Tree loss: 1.445 | Accuracy: 0.474403 | 1.653 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 123 | Batch: 000 / 011 | Total loss: 1.644 | Reg loss: 0.038 | Tree loss: 1.644 | Accuracy: 0.389500 | 1.653 sec/iter\n",
      "Epoch: 123 | Batch: 001 / 011 | Total loss: 1.618 | Reg loss: 0.038 | Tree loss: 1.618 | Accuracy: 0.416500 | 1.652 sec/iter\n",
      "Epoch: 123 | Batch: 002 / 011 | Total loss: 1.599 | Reg loss: 0.038 | Tree loss: 1.599 | Accuracy: 0.432500 | 1.652 sec/iter\n",
      "Epoch: 123 | Batch: 003 / 011 | Total loss: 1.563 | Reg loss: 0.038 | Tree loss: 1.563 | Accuracy: 0.436500 | 1.652 sec/iter\n",
      "Epoch: 123 | Batch: 004 / 011 | Total loss: 1.529 | Reg loss: 0.038 | Tree loss: 1.529 | Accuracy: 0.473000 | 1.652 sec/iter\n",
      "Epoch: 123 | Batch: 005 / 011 | Total loss: 1.474 | Reg loss: 0.038 | Tree loss: 1.474 | Accuracy: 0.488000 | 1.652 sec/iter\n",
      "Epoch: 123 | Batch: 006 / 011 | Total loss: 1.482 | Reg loss: 0.038 | Tree loss: 1.482 | Accuracy: 0.480000 | 1.652 sec/iter\n",
      "Epoch: 123 | Batch: 007 / 011 | Total loss: 1.459 | Reg loss: 0.038 | Tree loss: 1.459 | Accuracy: 0.476000 | 1.651 sec/iter\n",
      "Epoch: 123 | Batch: 008 / 011 | Total loss: 1.448 | Reg loss: 0.038 | Tree loss: 1.448 | Accuracy: 0.494000 | 1.651 sec/iter\n",
      "Epoch: 123 | Batch: 009 / 011 | Total loss: 1.430 | Reg loss: 0.038 | Tree loss: 1.430 | Accuracy: 0.476500 | 1.651 sec/iter\n",
      "Epoch: 123 | Batch: 010 / 011 | Total loss: 1.434 | Reg loss: 0.038 | Tree loss: 1.434 | Accuracy: 0.484642 | 1.65 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 124 | Batch: 000 / 011 | Total loss: 1.665 | Reg loss: 0.038 | Tree loss: 1.665 | Accuracy: 0.387000 | 1.655 sec/iter\n",
      "Epoch: 124 | Batch: 001 / 011 | Total loss: 1.627 | Reg loss: 0.038 | Tree loss: 1.627 | Accuracy: 0.426500 | 1.654 sec/iter\n",
      "Epoch: 124 | Batch: 002 / 011 | Total loss: 1.573 | Reg loss: 0.038 | Tree loss: 1.573 | Accuracy: 0.429500 | 1.654 sec/iter\n",
      "Epoch: 124 | Batch: 003 / 011 | Total loss: 1.540 | Reg loss: 0.038 | Tree loss: 1.540 | Accuracy: 0.453500 | 1.654 sec/iter\n",
      "Epoch: 124 | Batch: 004 / 011 | Total loss: 1.520 | Reg loss: 0.038 | Tree loss: 1.520 | Accuracy: 0.480000 | 1.653 sec/iter\n",
      "Epoch: 124 | Batch: 005 / 011 | Total loss: 1.498 | Reg loss: 0.038 | Tree loss: 1.498 | Accuracy: 0.469000 | 1.653 sec/iter\n",
      "Epoch: 124 | Batch: 006 / 011 | Total loss: 1.455 | Reg loss: 0.038 | Tree loss: 1.455 | Accuracy: 0.493000 | 1.652 sec/iter\n",
      "Epoch: 124 | Batch: 007 / 011 | Total loss: 1.445 | Reg loss: 0.038 | Tree loss: 1.445 | Accuracy: 0.497000 | 1.652 sec/iter\n",
      "Epoch: 124 | Batch: 008 / 011 | Total loss: 1.476 | Reg loss: 0.038 | Tree loss: 1.476 | Accuracy: 0.464500 | 1.651 sec/iter\n",
      "Epoch: 124 | Batch: 009 / 011 | Total loss: 1.448 | Reg loss: 0.038 | Tree loss: 1.448 | Accuracy: 0.480000 | 1.651 sec/iter\n",
      "Epoch: 124 | Batch: 010 / 011 | Total loss: 1.412 | Reg loss: 0.038 | Tree loss: 1.412 | Accuracy: 0.501706 | 1.65 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 125 | Batch: 000 / 011 | Total loss: 1.662 | Reg loss: 0.038 | Tree loss: 1.662 | Accuracy: 0.378000 | 1.654 sec/iter\n",
      "Epoch: 125 | Batch: 001 / 011 | Total loss: 1.652 | Reg loss: 0.038 | Tree loss: 1.652 | Accuracy: 0.392000 | 1.653 sec/iter\n",
      "Epoch: 125 | Batch: 002 / 011 | Total loss: 1.601 | Reg loss: 0.038 | Tree loss: 1.601 | Accuracy: 0.415000 | 1.653 sec/iter\n",
      "Epoch: 125 | Batch: 003 / 011 | Total loss: 1.549 | Reg loss: 0.038 | Tree loss: 1.549 | Accuracy: 0.449500 | 1.653 sec/iter\n",
      "Epoch: 125 | Batch: 004 / 011 | Total loss: 1.496 | Reg loss: 0.038 | Tree loss: 1.496 | Accuracy: 0.486000 | 1.653 sec/iter\n",
      "Epoch: 125 | Batch: 005 / 011 | Total loss: 1.475 | Reg loss: 0.038 | Tree loss: 1.475 | Accuracy: 0.490500 | 1.653 sec/iter\n",
      "Epoch: 125 | Batch: 006 / 011 | Total loss: 1.448 | Reg loss: 0.038 | Tree loss: 1.448 | Accuracy: 0.483500 | 1.653 sec/iter\n",
      "Epoch: 125 | Batch: 007 / 011 | Total loss: 1.463 | Reg loss: 0.038 | Tree loss: 1.463 | Accuracy: 0.472000 | 1.652 sec/iter\n",
      "Epoch: 125 | Batch: 008 / 011 | Total loss: 1.425 | Reg loss: 0.038 | Tree loss: 1.425 | Accuracy: 0.486500 | 1.652 sec/iter\n",
      "Epoch: 125 | Batch: 009 / 011 | Total loss: 1.460 | Reg loss: 0.038 | Tree loss: 1.460 | Accuracy: 0.479000 | 1.652 sec/iter\n",
      "Epoch: 125 | Batch: 010 / 011 | Total loss: 1.443 | Reg loss: 0.038 | Tree loss: 1.443 | Accuracy: 0.474403 | 1.651 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 126 | Batch: 000 / 011 | Total loss: 1.615 | Reg loss: 0.038 | Tree loss: 1.615 | Accuracy: 0.406500 | 1.651 sec/iter\n",
      "Epoch: 126 | Batch: 001 / 011 | Total loss: 1.614 | Reg loss: 0.038 | Tree loss: 1.614 | Accuracy: 0.409500 | 1.651 sec/iter\n",
      "Epoch: 126 | Batch: 002 / 011 | Total loss: 1.593 | Reg loss: 0.038 | Tree loss: 1.593 | Accuracy: 0.437500 | 1.651 sec/iter\n",
      "Epoch: 126 | Batch: 003 / 011 | Total loss: 1.575 | Reg loss: 0.038 | Tree loss: 1.575 | Accuracy: 0.444000 | 1.651 sec/iter\n",
      "Epoch: 126 | Batch: 004 / 011 | Total loss: 1.502 | Reg loss: 0.038 | Tree loss: 1.502 | Accuracy: 0.474000 | 1.65 sec/iter\n",
      "Epoch: 126 | Batch: 005 / 011 | Total loss: 1.478 | Reg loss: 0.038 | Tree loss: 1.478 | Accuracy: 0.494500 | 1.65 sec/iter\n",
      "Epoch: 126 | Batch: 006 / 011 | Total loss: 1.467 | Reg loss: 0.038 | Tree loss: 1.467 | Accuracy: 0.485000 | 1.65 sec/iter\n",
      "Epoch: 126 | Batch: 007 / 011 | Total loss: 1.462 | Reg loss: 0.038 | Tree loss: 1.462 | Accuracy: 0.456500 | 1.65 sec/iter\n",
      "Epoch: 126 | Batch: 008 / 011 | Total loss: 1.461 | Reg loss: 0.038 | Tree loss: 1.461 | Accuracy: 0.463500 | 1.65 sec/iter\n",
      "Epoch: 126 | Batch: 009 / 011 | Total loss: 1.462 | Reg loss: 0.038 | Tree loss: 1.462 | Accuracy: 0.462000 | 1.649 sec/iter\n",
      "Epoch: 126 | Batch: 010 / 011 | Total loss: 1.502 | Reg loss: 0.038 | Tree loss: 1.502 | Accuracy: 0.477816 | 1.649 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 127 | Batch: 000 / 011 | Total loss: 1.633 | Reg loss: 0.038 | Tree loss: 1.633 | Accuracy: 0.391000 | 1.653 sec/iter\n",
      "Epoch: 127 | Batch: 001 / 011 | Total loss: 1.618 | Reg loss: 0.038 | Tree loss: 1.618 | Accuracy: 0.411000 | 1.653 sec/iter\n",
      "Epoch: 127 | Batch: 002 / 011 | Total loss: 1.594 | Reg loss: 0.038 | Tree loss: 1.594 | Accuracy: 0.435500 | 1.653 sec/iter\n",
      "Epoch: 127 | Batch: 003 / 011 | Total loss: 1.544 | Reg loss: 0.038 | Tree loss: 1.544 | Accuracy: 0.457000 | 1.652 sec/iter\n",
      "Epoch: 127 | Batch: 004 / 011 | Total loss: 1.526 | Reg loss: 0.038 | Tree loss: 1.526 | Accuracy: 0.458500 | 1.652 sec/iter\n",
      "Epoch: 127 | Batch: 005 / 011 | Total loss: 1.495 | Reg loss: 0.038 | Tree loss: 1.495 | Accuracy: 0.485000 | 1.652 sec/iter\n",
      "Epoch: 127 | Batch: 006 / 011 | Total loss: 1.450 | Reg loss: 0.038 | Tree loss: 1.450 | Accuracy: 0.486000 | 1.652 sec/iter\n",
      "Epoch: 127 | Batch: 007 / 011 | Total loss: 1.467 | Reg loss: 0.038 | Tree loss: 1.467 | Accuracy: 0.469000 | 1.652 sec/iter\n",
      "Epoch: 127 | Batch: 008 / 011 | Total loss: 1.443 | Reg loss: 0.038 | Tree loss: 1.443 | Accuracy: 0.482500 | 1.651 sec/iter\n",
      "Epoch: 127 | Batch: 009 / 011 | Total loss: 1.450 | Reg loss: 0.038 | Tree loss: 1.450 | Accuracy: 0.469000 | 1.651 sec/iter\n",
      "Epoch: 127 | Batch: 010 / 011 | Total loss: 1.445 | Reg loss: 0.038 | Tree loss: 1.445 | Accuracy: 0.559727 | 1.651 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 128 | Batch: 000 / 011 | Total loss: 1.653 | Reg loss: 0.038 | Tree loss: 1.653 | Accuracy: 0.393000 | 1.651 sec/iter\n",
      "Epoch: 128 | Batch: 001 / 011 | Total loss: 1.625 | Reg loss: 0.038 | Tree loss: 1.625 | Accuracy: 0.409000 | 1.65 sec/iter\n",
      "Epoch: 128 | Batch: 002 / 011 | Total loss: 1.576 | Reg loss: 0.038 | Tree loss: 1.576 | Accuracy: 0.427500 | 1.65 sec/iter\n",
      "Epoch: 128 | Batch: 003 / 011 | Total loss: 1.537 | Reg loss: 0.038 | Tree loss: 1.537 | Accuracy: 0.456500 | 1.65 sec/iter\n",
      "Epoch: 128 | Batch: 004 / 011 | Total loss: 1.494 | Reg loss: 0.038 | Tree loss: 1.494 | Accuracy: 0.482000 | 1.65 sec/iter\n",
      "Epoch: 128 | Batch: 005 / 011 | Total loss: 1.481 | Reg loss: 0.038 | Tree loss: 1.481 | Accuracy: 0.474000 | 1.65 sec/iter\n",
      "Epoch: 128 | Batch: 006 / 011 | Total loss: 1.467 | Reg loss: 0.038 | Tree loss: 1.467 | Accuracy: 0.484500 | 1.649 sec/iter\n",
      "Epoch: 128 | Batch: 007 / 011 | Total loss: 1.462 | Reg loss: 0.038 | Tree loss: 1.462 | Accuracy: 0.467000 | 1.649 sec/iter\n",
      "Epoch: 128 | Batch: 008 / 011 | Total loss: 1.456 | Reg loss: 0.038 | Tree loss: 1.456 | Accuracy: 0.463500 | 1.649 sec/iter\n",
      "Epoch: 128 | Batch: 009 / 011 | Total loss: 1.457 | Reg loss: 0.038 | Tree loss: 1.457 | Accuracy: 0.473500 | 1.649 sec/iter\n",
      "Epoch: 128 | Batch: 010 / 011 | Total loss: 1.471 | Reg loss: 0.038 | Tree loss: 1.471 | Accuracy: 0.412969 | 1.648 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 129 | Batch: 000 / 011 | Total loss: 1.640 | Reg loss: 0.038 | Tree loss: 1.640 | Accuracy: 0.404000 | 1.653 sec/iter\n",
      "Epoch: 129 | Batch: 001 / 011 | Total loss: 1.616 | Reg loss: 0.038 | Tree loss: 1.616 | Accuracy: 0.415000 | 1.652 sec/iter\n",
      "Epoch: 129 | Batch: 002 / 011 | Total loss: 1.582 | Reg loss: 0.038 | Tree loss: 1.582 | Accuracy: 0.437500 | 1.652 sec/iter\n",
      "Epoch: 129 | Batch: 003 / 011 | Total loss: 1.546 | Reg loss: 0.038 | Tree loss: 1.546 | Accuracy: 0.463500 | 1.652 sec/iter\n",
      "Epoch: 129 | Batch: 004 / 011 | Total loss: 1.529 | Reg loss: 0.038 | Tree loss: 1.529 | Accuracy: 0.469000 | 1.652 sec/iter\n",
      "Epoch: 129 | Batch: 005 / 011 | Total loss: 1.473 | Reg loss: 0.038 | Tree loss: 1.473 | Accuracy: 0.478000 | 1.652 sec/iter\n",
      "Epoch: 129 | Batch: 006 / 011 | Total loss: 1.471 | Reg loss: 0.038 | Tree loss: 1.471 | Accuracy: 0.471000 | 1.651 sec/iter\n",
      "Epoch: 129 | Batch: 007 / 011 | Total loss: 1.445 | Reg loss: 0.038 | Tree loss: 1.445 | Accuracy: 0.469500 | 1.651 sec/iter\n",
      "Epoch: 129 | Batch: 008 / 011 | Total loss: 1.449 | Reg loss: 0.038 | Tree loss: 1.449 | Accuracy: 0.482000 | 1.651 sec/iter\n",
      "Epoch: 129 | Batch: 009 / 011 | Total loss: 1.446 | Reg loss: 0.038 | Tree loss: 1.446 | Accuracy: 0.483500 | 1.651 sec/iter\n",
      "Epoch: 129 | Batch: 010 / 011 | Total loss: 1.483 | Reg loss: 0.038 | Tree loss: 1.483 | Accuracy: 0.457338 | 1.65 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 130 | Batch: 000 / 011 | Total loss: 1.630 | Reg loss: 0.038 | Tree loss: 1.630 | Accuracy: 0.399000 | 1.65 sec/iter\n",
      "Epoch: 130 | Batch: 001 / 011 | Total loss: 1.618 | Reg loss: 0.038 | Tree loss: 1.618 | Accuracy: 0.415500 | 1.65 sec/iter\n",
      "Epoch: 130 | Batch: 002 / 011 | Total loss: 1.589 | Reg loss: 0.038 | Tree loss: 1.589 | Accuracy: 0.414500 | 1.65 sec/iter\n",
      "Epoch: 130 | Batch: 003 / 011 | Total loss: 1.567 | Reg loss: 0.038 | Tree loss: 1.567 | Accuracy: 0.436500 | 1.65 sec/iter\n",
      "Epoch: 130 | Batch: 004 / 011 | Total loss: 1.475 | Reg loss: 0.038 | Tree loss: 1.475 | Accuracy: 0.488500 | 1.649 sec/iter\n",
      "Epoch: 130 | Batch: 005 / 011 | Total loss: 1.473 | Reg loss: 0.038 | Tree loss: 1.473 | Accuracy: 0.480000 | 1.649 sec/iter\n",
      "Epoch: 130 | Batch: 006 / 011 | Total loss: 1.471 | Reg loss: 0.038 | Tree loss: 1.471 | Accuracy: 0.476500 | 1.649 sec/iter\n",
      "Epoch: 130 | Batch: 007 / 011 | Total loss: 1.465 | Reg loss: 0.038 | Tree loss: 1.465 | Accuracy: 0.473500 | 1.649 sec/iter\n",
      "Epoch: 130 | Batch: 008 / 011 | Total loss: 1.452 | Reg loss: 0.038 | Tree loss: 1.452 | Accuracy: 0.482000 | 1.649 sec/iter\n",
      "Epoch: 130 | Batch: 009 / 011 | Total loss: 1.460 | Reg loss: 0.038 | Tree loss: 1.460 | Accuracy: 0.460000 | 1.648 sec/iter\n",
      "Epoch: 130 | Batch: 010 / 011 | Total loss: 1.468 | Reg loss: 0.038 | Tree loss: 1.468 | Accuracy: 0.481229 | 1.648 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 131 | Batch: 000 / 011 | Total loss: 1.653 | Reg loss: 0.038 | Tree loss: 1.653 | Accuracy: 0.378000 | 1.652 sec/iter\n",
      "Epoch: 131 | Batch: 001 / 011 | Total loss: 1.640 | Reg loss: 0.038 | Tree loss: 1.640 | Accuracy: 0.392500 | 1.652 sec/iter\n",
      "Epoch: 131 | Batch: 002 / 011 | Total loss: 1.582 | Reg loss: 0.038 | Tree loss: 1.582 | Accuracy: 0.426500 | 1.652 sec/iter\n",
      "Epoch: 131 | Batch: 003 / 011 | Total loss: 1.528 | Reg loss: 0.038 | Tree loss: 1.528 | Accuracy: 0.463000 | 1.651 sec/iter\n",
      "Epoch: 131 | Batch: 004 / 011 | Total loss: 1.507 | Reg loss: 0.038 | Tree loss: 1.507 | Accuracy: 0.467500 | 1.651 sec/iter\n",
      "Epoch: 131 | Batch: 005 / 011 | Total loss: 1.475 | Reg loss: 0.038 | Tree loss: 1.475 | Accuracy: 0.494500 | 1.65 sec/iter\n",
      "Epoch: 131 | Batch: 006 / 011 | Total loss: 1.495 | Reg loss: 0.038 | Tree loss: 1.495 | Accuracy: 0.484000 | 1.65 sec/iter\n",
      "Epoch: 131 | Batch: 007 / 011 | Total loss: 1.418 | Reg loss: 0.038 | Tree loss: 1.418 | Accuracy: 0.503000 | 1.649 sec/iter\n",
      "Epoch: 131 | Batch: 008 / 011 | Total loss: 1.460 | Reg loss: 0.038 | Tree loss: 1.460 | Accuracy: 0.478500 | 1.649 sec/iter\n",
      "Epoch: 131 | Batch: 009 / 011 | Total loss: 1.448 | Reg loss: 0.038 | Tree loss: 1.448 | Accuracy: 0.478500 | 1.649 sec/iter\n",
      "Epoch: 131 | Batch: 010 / 011 | Total loss: 1.416 | Reg loss: 0.038 | Tree loss: 1.416 | Accuracy: 0.481229 | 1.648 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 132 | Batch: 000 / 011 | Total loss: 1.638 | Reg loss: 0.038 | Tree loss: 1.638 | Accuracy: 0.390500 | 1.651 sec/iter\n",
      "Epoch: 132 | Batch: 001 / 011 | Total loss: 1.599 | Reg loss: 0.038 | Tree loss: 1.599 | Accuracy: 0.407000 | 1.651 sec/iter\n",
      "Epoch: 132 | Batch: 002 / 011 | Total loss: 1.604 | Reg loss: 0.038 | Tree loss: 1.604 | Accuracy: 0.426500 | 1.651 sec/iter\n",
      "Epoch: 132 | Batch: 003 / 011 | Total loss: 1.528 | Reg loss: 0.038 | Tree loss: 1.528 | Accuracy: 0.475500 | 1.651 sec/iter\n",
      "Epoch: 132 | Batch: 004 / 011 | Total loss: 1.527 | Reg loss: 0.038 | Tree loss: 1.527 | Accuracy: 0.473000 | 1.65 sec/iter\n",
      "Epoch: 132 | Batch: 005 / 011 | Total loss: 1.477 | Reg loss: 0.038 | Tree loss: 1.477 | Accuracy: 0.484000 | 1.65 sec/iter\n",
      "Epoch: 132 | Batch: 006 / 011 | Total loss: 1.465 | Reg loss: 0.038 | Tree loss: 1.465 | Accuracy: 0.481000 | 1.65 sec/iter\n",
      "Epoch: 132 | Batch: 007 / 011 | Total loss: 1.467 | Reg loss: 0.038 | Tree loss: 1.467 | Accuracy: 0.475000 | 1.65 sec/iter\n",
      "Epoch: 132 | Batch: 008 / 011 | Total loss: 1.435 | Reg loss: 0.038 | Tree loss: 1.435 | Accuracy: 0.496500 | 1.65 sec/iter\n",
      "Epoch: 132 | Batch: 009 / 011 | Total loss: 1.447 | Reg loss: 0.038 | Tree loss: 1.447 | Accuracy: 0.484500 | 1.65 sec/iter\n",
      "Epoch: 132 | Batch: 010 / 011 | Total loss: 1.504 | Reg loss: 0.038 | Tree loss: 1.504 | Accuracy: 0.477816 | 1.649 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 133 | Batch: 000 / 011 | Total loss: 1.649 | Reg loss: 0.038 | Tree loss: 1.649 | Accuracy: 0.386000 | 1.649 sec/iter\n",
      "Epoch: 133 | Batch: 001 / 011 | Total loss: 1.644 | Reg loss: 0.038 | Tree loss: 1.644 | Accuracy: 0.387500 | 1.649 sec/iter\n",
      "Epoch: 133 | Batch: 002 / 011 | Total loss: 1.593 | Reg loss: 0.038 | Tree loss: 1.593 | Accuracy: 0.419000 | 1.648 sec/iter\n",
      "Epoch: 133 | Batch: 003 / 011 | Total loss: 1.550 | Reg loss: 0.038 | Tree loss: 1.550 | Accuracy: 0.447500 | 1.648 sec/iter\n",
      "Epoch: 133 | Batch: 004 / 011 | Total loss: 1.492 | Reg loss: 0.038 | Tree loss: 1.492 | Accuracy: 0.474000 | 1.648 sec/iter\n",
      "Epoch: 133 | Batch: 005 / 011 | Total loss: 1.476 | Reg loss: 0.038 | Tree loss: 1.476 | Accuracy: 0.485000 | 1.648 sec/iter\n",
      "Epoch: 133 | Batch: 006 / 011 | Total loss: 1.458 | Reg loss: 0.038 | Tree loss: 1.458 | Accuracy: 0.496000 | 1.648 sec/iter\n",
      "Epoch: 133 | Batch: 007 / 011 | Total loss: 1.440 | Reg loss: 0.038 | Tree loss: 1.440 | Accuracy: 0.491500 | 1.648 sec/iter\n",
      "Epoch: 133 | Batch: 008 / 011 | Total loss: 1.469 | Reg loss: 0.038 | Tree loss: 1.469 | Accuracy: 0.475000 | 1.647 sec/iter\n",
      "Epoch: 133 | Batch: 009 / 011 | Total loss: 1.440 | Reg loss: 0.038 | Tree loss: 1.440 | Accuracy: 0.486500 | 1.647 sec/iter\n",
      "Epoch: 133 | Batch: 010 / 011 | Total loss: 1.403 | Reg loss: 0.038 | Tree loss: 1.403 | Accuracy: 0.501706 | 1.646 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 134 | Batch: 000 / 011 | Total loss: 1.645 | Reg loss: 0.038 | Tree loss: 1.645 | Accuracy: 0.395000 | 1.651 sec/iter\n",
      "Epoch: 134 | Batch: 001 / 011 | Total loss: 1.631 | Reg loss: 0.038 | Tree loss: 1.631 | Accuracy: 0.403000 | 1.65 sec/iter\n",
      "Epoch: 134 | Batch: 002 / 011 | Total loss: 1.600 | Reg loss: 0.038 | Tree loss: 1.600 | Accuracy: 0.434500 | 1.65 sec/iter\n",
      "Epoch: 134 | Batch: 003 / 011 | Total loss: 1.520 | Reg loss: 0.038 | Tree loss: 1.520 | Accuracy: 0.478000 | 1.65 sec/iter\n",
      "Epoch: 134 | Batch: 004 / 011 | Total loss: 1.496 | Reg loss: 0.038 | Tree loss: 1.496 | Accuracy: 0.489000 | 1.65 sec/iter\n",
      "Epoch: 134 | Batch: 005 / 011 | Total loss: 1.490 | Reg loss: 0.038 | Tree loss: 1.490 | Accuracy: 0.487500 | 1.65 sec/iter\n",
      "Epoch: 134 | Batch: 006 / 011 | Total loss: 1.447 | Reg loss: 0.038 | Tree loss: 1.447 | Accuracy: 0.479000 | 1.65 sec/iter\n",
      "Epoch: 134 | Batch: 007 / 011 | Total loss: 1.450 | Reg loss: 0.038 | Tree loss: 1.450 | Accuracy: 0.468500 | 1.649 sec/iter\n",
      "Epoch: 134 | Batch: 008 / 011 | Total loss: 1.435 | Reg loss: 0.038 | Tree loss: 1.435 | Accuracy: 0.499500 | 1.649 sec/iter\n",
      "Epoch: 134 | Batch: 009 / 011 | Total loss: 1.465 | Reg loss: 0.038 | Tree loss: 1.465 | Accuracy: 0.474500 | 1.649 sec/iter\n",
      "Epoch: 134 | Batch: 010 / 011 | Total loss: 1.503 | Reg loss: 0.038 | Tree loss: 1.503 | Accuracy: 0.440273 | 1.648 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 135 | Batch: 000 / 011 | Total loss: 1.649 | Reg loss: 0.038 | Tree loss: 1.649 | Accuracy: 0.393000 | 1.648 sec/iter\n",
      "Epoch: 135 | Batch: 001 / 011 | Total loss: 1.610 | Reg loss: 0.038 | Tree loss: 1.610 | Accuracy: 0.403500 | 1.648 sec/iter\n",
      "Epoch: 135 | Batch: 002 / 011 | Total loss: 1.591 | Reg loss: 0.038 | Tree loss: 1.591 | Accuracy: 0.422000 | 1.648 sec/iter\n",
      "Epoch: 135 | Batch: 003 / 011 | Total loss: 1.536 | Reg loss: 0.038 | Tree loss: 1.536 | Accuracy: 0.441500 | 1.648 sec/iter\n",
      "Epoch: 135 | Batch: 004 / 011 | Total loss: 1.509 | Reg loss: 0.038 | Tree loss: 1.509 | Accuracy: 0.475000 | 1.648 sec/iter\n",
      "Epoch: 135 | Batch: 005 / 011 | Total loss: 1.483 | Reg loss: 0.038 | Tree loss: 1.483 | Accuracy: 0.487500 | 1.648 sec/iter\n",
      "Epoch: 135 | Batch: 006 / 011 | Total loss: 1.454 | Reg loss: 0.038 | Tree loss: 1.454 | Accuracy: 0.495500 | 1.647 sec/iter\n",
      "Epoch: 135 | Batch: 007 / 011 | Total loss: 1.447 | Reg loss: 0.038 | Tree loss: 1.447 | Accuracy: 0.483500 | 1.647 sec/iter\n",
      "Epoch: 135 | Batch: 008 / 011 | Total loss: 1.458 | Reg loss: 0.038 | Tree loss: 1.458 | Accuracy: 0.482500 | 1.647 sec/iter\n",
      "Epoch: 135 | Batch: 009 / 011 | Total loss: 1.452 | Reg loss: 0.038 | Tree loss: 1.452 | Accuracy: 0.481000 | 1.647 sec/iter\n",
      "Epoch: 135 | Batch: 010 / 011 | Total loss: 1.421 | Reg loss: 0.038 | Tree loss: 1.421 | Accuracy: 0.501706 | 1.646 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 136 | Batch: 000 / 011 | Total loss: 1.649 | Reg loss: 0.038 | Tree loss: 1.649 | Accuracy: 0.395000 | 1.651 sec/iter\n",
      "Epoch: 136 | Batch: 001 / 011 | Total loss: 1.640 | Reg loss: 0.038 | Tree loss: 1.640 | Accuracy: 0.392000 | 1.65 sec/iter\n",
      "Epoch: 136 | Batch: 002 / 011 | Total loss: 1.565 | Reg loss: 0.038 | Tree loss: 1.565 | Accuracy: 0.454000 | 1.65 sec/iter\n",
      "Epoch: 136 | Batch: 003 / 011 | Total loss: 1.535 | Reg loss: 0.038 | Tree loss: 1.535 | Accuracy: 0.445500 | 1.65 sec/iter\n",
      "Epoch: 136 | Batch: 004 / 011 | Total loss: 1.507 | Reg loss: 0.038 | Tree loss: 1.507 | Accuracy: 0.483500 | 1.65 sec/iter\n",
      "Epoch: 136 | Batch: 005 / 011 | Total loss: 1.470 | Reg loss: 0.038 | Tree loss: 1.470 | Accuracy: 0.490000 | 1.649 sec/iter\n",
      "Epoch: 136 | Batch: 006 / 011 | Total loss: 1.457 | Reg loss: 0.038 | Tree loss: 1.457 | Accuracy: 0.492500 | 1.649 sec/iter\n",
      "Epoch: 136 | Batch: 007 / 011 | Total loss: 1.445 | Reg loss: 0.038 | Tree loss: 1.445 | Accuracy: 0.510000 | 1.649 sec/iter\n",
      "Epoch: 136 | Batch: 008 / 011 | Total loss: 1.444 | Reg loss: 0.038 | Tree loss: 1.444 | Accuracy: 0.486000 | 1.649 sec/iter\n",
      "Epoch: 136 | Batch: 009 / 011 | Total loss: 1.442 | Reg loss: 0.038 | Tree loss: 1.442 | Accuracy: 0.484500 | 1.649 sec/iter\n",
      "Epoch: 136 | Batch: 010 / 011 | Total loss: 1.480 | Reg loss: 0.038 | Tree loss: 1.480 | Accuracy: 0.470990 | 1.648 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 137 | Batch: 000 / 011 | Total loss: 1.657 | Reg loss: 0.038 | Tree loss: 1.657 | Accuracy: 0.388500 | 1.648 sec/iter\n",
      "Epoch: 137 | Batch: 001 / 011 | Total loss: 1.596 | Reg loss: 0.038 | Tree loss: 1.596 | Accuracy: 0.405000 | 1.648 sec/iter\n",
      "Epoch: 137 | Batch: 002 / 011 | Total loss: 1.589 | Reg loss: 0.038 | Tree loss: 1.589 | Accuracy: 0.429500 | 1.648 sec/iter\n",
      "Epoch: 137 | Batch: 003 / 011 | Total loss: 1.540 | Reg loss: 0.038 | Tree loss: 1.540 | Accuracy: 0.445000 | 1.647 sec/iter\n",
      "Epoch: 137 | Batch: 004 / 011 | Total loss: 1.483 | Reg loss: 0.038 | Tree loss: 1.483 | Accuracy: 0.487500 | 1.647 sec/iter\n",
      "Epoch: 137 | Batch: 005 / 011 | Total loss: 1.480 | Reg loss: 0.038 | Tree loss: 1.480 | Accuracy: 0.480500 | 1.647 sec/iter\n",
      "Epoch: 137 | Batch: 006 / 011 | Total loss: 1.463 | Reg loss: 0.038 | Tree loss: 1.463 | Accuracy: 0.477000 | 1.647 sec/iter\n",
      "Epoch: 137 | Batch: 007 / 011 | Total loss: 1.452 | Reg loss: 0.038 | Tree loss: 1.452 | Accuracy: 0.489500 | 1.647 sec/iter\n",
      "Epoch: 137 | Batch: 008 / 011 | Total loss: 1.456 | Reg loss: 0.038 | Tree loss: 1.456 | Accuracy: 0.484000 | 1.647 sec/iter\n",
      "Epoch: 137 | Batch: 009 / 011 | Total loss: 1.435 | Reg loss: 0.039 | Tree loss: 1.435 | Accuracy: 0.481000 | 1.646 sec/iter\n",
      "Epoch: 137 | Batch: 010 / 011 | Total loss: 1.456 | Reg loss: 0.039 | Tree loss: 1.456 | Accuracy: 0.457338 | 1.646 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 138 | Batch: 000 / 011 | Total loss: 1.625 | Reg loss: 0.038 | Tree loss: 1.625 | Accuracy: 0.418500 | 1.646 sec/iter\n",
      "Epoch: 138 | Batch: 001 / 011 | Total loss: 1.603 | Reg loss: 0.038 | Tree loss: 1.603 | Accuracy: 0.403500 | 1.646 sec/iter\n",
      "Epoch: 138 | Batch: 002 / 011 | Total loss: 1.581 | Reg loss: 0.038 | Tree loss: 1.581 | Accuracy: 0.442500 | 1.645 sec/iter\n",
      "Epoch: 138 | Batch: 003 / 011 | Total loss: 1.549 | Reg loss: 0.038 | Tree loss: 1.549 | Accuracy: 0.470500 | 1.645 sec/iter\n",
      "Epoch: 138 | Batch: 004 / 011 | Total loss: 1.511 | Reg loss: 0.038 | Tree loss: 1.511 | Accuracy: 0.478000 | 1.645 sec/iter\n",
      "Epoch: 138 | Batch: 005 / 011 | Total loss: 1.481 | Reg loss: 0.038 | Tree loss: 1.481 | Accuracy: 0.487000 | 1.645 sec/iter\n",
      "Epoch: 138 | Batch: 006 / 011 | Total loss: 1.467 | Reg loss: 0.038 | Tree loss: 1.467 | Accuracy: 0.488500 | 1.645 sec/iter\n",
      "Epoch: 138 | Batch: 007 / 011 | Total loss: 1.445 | Reg loss: 0.038 | Tree loss: 1.445 | Accuracy: 0.481000 | 1.645 sec/iter\n",
      "Epoch: 138 | Batch: 008 / 011 | Total loss: 1.458 | Reg loss: 0.039 | Tree loss: 1.458 | Accuracy: 0.476000 | 1.645 sec/iter\n",
      "Epoch: 138 | Batch: 009 / 011 | Total loss: 1.466 | Reg loss: 0.039 | Tree loss: 1.466 | Accuracy: 0.476000 | 1.644 sec/iter\n",
      "Epoch: 138 | Batch: 010 / 011 | Total loss: 1.413 | Reg loss: 0.039 | Tree loss: 1.413 | Accuracy: 0.532423 | 1.644 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 139 | Batch: 000 / 011 | Total loss: 1.635 | Reg loss: 0.038 | Tree loss: 1.635 | Accuracy: 0.405500 | 1.644 sec/iter\n",
      "Epoch: 139 | Batch: 001 / 011 | Total loss: 1.627 | Reg loss: 0.038 | Tree loss: 1.627 | Accuracy: 0.407500 | 1.643 sec/iter\n",
      "Epoch: 139 | Batch: 002 / 011 | Total loss: 1.577 | Reg loss: 0.038 | Tree loss: 1.577 | Accuracy: 0.419500 | 1.643 sec/iter\n",
      "Epoch: 139 | Batch: 003 / 011 | Total loss: 1.520 | Reg loss: 0.038 | Tree loss: 1.520 | Accuracy: 0.466000 | 1.643 sec/iter\n",
      "Epoch: 139 | Batch: 004 / 011 | Total loss: 1.502 | Reg loss: 0.038 | Tree loss: 1.502 | Accuracy: 0.481000 | 1.642 sec/iter\n",
      "Epoch: 139 | Batch: 005 / 011 | Total loss: 1.476 | Reg loss: 0.038 | Tree loss: 1.476 | Accuracy: 0.489000 | 1.642 sec/iter\n",
      "Epoch: 139 | Batch: 006 / 011 | Total loss: 1.461 | Reg loss: 0.038 | Tree loss: 1.461 | Accuracy: 0.469500 | 1.642 sec/iter\n",
      "Epoch: 139 | Batch: 007 / 011 | Total loss: 1.449 | Reg loss: 0.038 | Tree loss: 1.449 | Accuracy: 0.484000 | 1.642 sec/iter\n",
      "Epoch: 139 | Batch: 008 / 011 | Total loss: 1.433 | Reg loss: 0.039 | Tree loss: 1.433 | Accuracy: 0.478000 | 1.642 sec/iter\n",
      "Epoch: 139 | Batch: 009 / 011 | Total loss: 1.445 | Reg loss: 0.039 | Tree loss: 1.445 | Accuracy: 0.492000 | 1.641 sec/iter\n",
      "Epoch: 139 | Batch: 010 / 011 | Total loss: 1.477 | Reg loss: 0.039 | Tree loss: 1.477 | Accuracy: 0.515358 | 1.641 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 140 | Batch: 000 / 011 | Total loss: 1.629 | Reg loss: 0.038 | Tree loss: 1.629 | Accuracy: 0.414500 | 1.642 sec/iter\n",
      "Epoch: 140 | Batch: 001 / 011 | Total loss: 1.597 | Reg loss: 0.038 | Tree loss: 1.597 | Accuracy: 0.404500 | 1.642 sec/iter\n",
      "Epoch: 140 | Batch: 002 / 011 | Total loss: 1.593 | Reg loss: 0.038 | Tree loss: 1.593 | Accuracy: 0.429500 | 1.642 sec/iter\n",
      "Epoch: 140 | Batch: 003 / 011 | Total loss: 1.525 | Reg loss: 0.038 | Tree loss: 1.525 | Accuracy: 0.469000 | 1.642 sec/iter\n",
      "Epoch: 140 | Batch: 004 / 011 | Total loss: 1.517 | Reg loss: 0.038 | Tree loss: 1.517 | Accuracy: 0.456500 | 1.642 sec/iter\n",
      "Epoch: 140 | Batch: 005 / 011 | Total loss: 1.474 | Reg loss: 0.038 | Tree loss: 1.474 | Accuracy: 0.485500 | 1.641 sec/iter\n",
      "Epoch: 140 | Batch: 006 / 011 | Total loss: 1.461 | Reg loss: 0.038 | Tree loss: 1.461 | Accuracy: 0.486000 | 1.641 sec/iter\n",
      "Epoch: 140 | Batch: 007 / 011 | Total loss: 1.459 | Reg loss: 0.039 | Tree loss: 1.459 | Accuracy: 0.466000 | 1.641 sec/iter\n",
      "Epoch: 140 | Batch: 008 / 011 | Total loss: 1.421 | Reg loss: 0.039 | Tree loss: 1.421 | Accuracy: 0.491000 | 1.641 sec/iter\n",
      "Epoch: 140 | Batch: 009 / 011 | Total loss: 1.457 | Reg loss: 0.039 | Tree loss: 1.457 | Accuracy: 0.487000 | 1.641 sec/iter\n",
      "Epoch: 140 | Batch: 010 / 011 | Total loss: 1.437 | Reg loss: 0.039 | Tree loss: 1.437 | Accuracy: 0.488055 | 1.64 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 141 | Batch: 000 / 011 | Total loss: 1.643 | Reg loss: 0.038 | Tree loss: 1.643 | Accuracy: 0.394500 | 1.64 sec/iter\n",
      "Epoch: 141 | Batch: 001 / 011 | Total loss: 1.604 | Reg loss: 0.038 | Tree loss: 1.604 | Accuracy: 0.412000 | 1.64 sec/iter\n",
      "Epoch: 141 | Batch: 002 / 011 | Total loss: 1.575 | Reg loss: 0.038 | Tree loss: 1.575 | Accuracy: 0.422000 | 1.64 sec/iter\n",
      "Epoch: 141 | Batch: 003 / 011 | Total loss: 1.529 | Reg loss: 0.038 | Tree loss: 1.529 | Accuracy: 0.450000 | 1.64 sec/iter\n",
      "Epoch: 141 | Batch: 004 / 011 | Total loss: 1.499 | Reg loss: 0.038 | Tree loss: 1.499 | Accuracy: 0.482500 | 1.64 sec/iter\n",
      "Epoch: 141 | Batch: 005 / 011 | Total loss: 1.494 | Reg loss: 0.038 | Tree loss: 1.494 | Accuracy: 0.467500 | 1.64 sec/iter\n",
      "Epoch: 141 | Batch: 006 / 011 | Total loss: 1.434 | Reg loss: 0.039 | Tree loss: 1.434 | Accuracy: 0.497500 | 1.639 sec/iter\n",
      "Epoch: 141 | Batch: 007 / 011 | Total loss: 1.450 | Reg loss: 0.039 | Tree loss: 1.450 | Accuracy: 0.480500 | 1.639 sec/iter\n",
      "Epoch: 141 | Batch: 008 / 011 | Total loss: 1.451 | Reg loss: 0.039 | Tree loss: 1.451 | Accuracy: 0.467000 | 1.639 sec/iter\n",
      "Epoch: 141 | Batch: 009 / 011 | Total loss: 1.450 | Reg loss: 0.039 | Tree loss: 1.450 | Accuracy: 0.483000 | 1.639 sec/iter\n",
      "Epoch: 141 | Batch: 010 / 011 | Total loss: 1.469 | Reg loss: 0.039 | Tree loss: 1.469 | Accuracy: 0.529010 | 1.639 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 142 | Batch: 000 / 011 | Total loss: 1.650 | Reg loss: 0.038 | Tree loss: 1.650 | Accuracy: 0.403000 | 1.643 sec/iter\n",
      "Epoch: 142 | Batch: 001 / 011 | Total loss: 1.612 | Reg loss: 0.038 | Tree loss: 1.612 | Accuracy: 0.408500 | 1.643 sec/iter\n",
      "Epoch: 142 | Batch: 002 / 011 | Total loss: 1.576 | Reg loss: 0.038 | Tree loss: 1.576 | Accuracy: 0.441000 | 1.643 sec/iter\n",
      "Epoch: 142 | Batch: 003 / 011 | Total loss: 1.512 | Reg loss: 0.038 | Tree loss: 1.512 | Accuracy: 0.476500 | 1.643 sec/iter\n",
      "Epoch: 142 | Batch: 004 / 011 | Total loss: 1.499 | Reg loss: 0.038 | Tree loss: 1.499 | Accuracy: 0.477500 | 1.643 sec/iter\n",
      "Epoch: 142 | Batch: 005 / 011 | Total loss: 1.462 | Reg loss: 0.039 | Tree loss: 1.462 | Accuracy: 0.493500 | 1.643 sec/iter\n",
      "Epoch: 142 | Batch: 006 / 011 | Total loss: 1.457 | Reg loss: 0.039 | Tree loss: 1.457 | Accuracy: 0.477500 | 1.643 sec/iter\n",
      "Epoch: 142 | Batch: 007 / 011 | Total loss: 1.457 | Reg loss: 0.039 | Tree loss: 1.457 | Accuracy: 0.481000 | 1.643 sec/iter\n",
      "Epoch: 142 | Batch: 008 / 011 | Total loss: 1.444 | Reg loss: 0.039 | Tree loss: 1.444 | Accuracy: 0.485500 | 1.643 sec/iter\n",
      "Epoch: 142 | Batch: 009 / 011 | Total loss: 1.441 | Reg loss: 0.039 | Tree loss: 1.441 | Accuracy: 0.458500 | 1.643 sec/iter\n",
      "Epoch: 142 | Batch: 010 / 011 | Total loss: 1.416 | Reg loss: 0.039 | Tree loss: 1.416 | Accuracy: 0.477816 | 1.643 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 143 | Batch: 000 / 011 | Total loss: 1.651 | Reg loss: 0.038 | Tree loss: 1.651 | Accuracy: 0.383500 | 1.647 sec/iter\n",
      "Epoch: 143 | Batch: 001 / 011 | Total loss: 1.608 | Reg loss: 0.038 | Tree loss: 1.608 | Accuracy: 0.399500 | 1.647 sec/iter\n",
      "Epoch: 143 | Batch: 002 / 011 | Total loss: 1.585 | Reg loss: 0.038 | Tree loss: 1.585 | Accuracy: 0.426000 | 1.647 sec/iter\n",
      "Epoch: 143 | Batch: 003 / 011 | Total loss: 1.529 | Reg loss: 0.038 | Tree loss: 1.529 | Accuracy: 0.465000 | 1.647 sec/iter\n",
      "Epoch: 143 | Batch: 004 / 011 | Total loss: 1.497 | Reg loss: 0.039 | Tree loss: 1.497 | Accuracy: 0.475500 | 1.647 sec/iter\n",
      "Epoch: 143 | Batch: 005 / 011 | Total loss: 1.468 | Reg loss: 0.039 | Tree loss: 1.468 | Accuracy: 0.494000 | 1.647 sec/iter\n",
      "Epoch: 143 | Batch: 006 / 011 | Total loss: 1.449 | Reg loss: 0.039 | Tree loss: 1.449 | Accuracy: 0.482500 | 1.647 sec/iter\n",
      "Epoch: 143 | Batch: 007 / 011 | Total loss: 1.458 | Reg loss: 0.039 | Tree loss: 1.458 | Accuracy: 0.467000 | 1.647 sec/iter\n",
      "Epoch: 143 | Batch: 008 / 011 | Total loss: 1.449 | Reg loss: 0.039 | Tree loss: 1.449 | Accuracy: 0.479500 | 1.648 sec/iter\n",
      "Epoch: 143 | Batch: 009 / 011 | Total loss: 1.430 | Reg loss: 0.039 | Tree loss: 1.430 | Accuracy: 0.483000 | 1.648 sec/iter\n",
      "Epoch: 143 | Batch: 010 / 011 | Total loss: 1.409 | Reg loss: 0.039 | Tree loss: 1.409 | Accuracy: 0.440273 | 1.648 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 144 | Batch: 000 / 011 | Total loss: 1.651 | Reg loss: 0.038 | Tree loss: 1.651 | Accuracy: 0.391500 | 1.65 sec/iter\n",
      "Epoch: 144 | Batch: 001 / 011 | Total loss: 1.606 | Reg loss: 0.038 | Tree loss: 1.606 | Accuracy: 0.419000 | 1.65 sec/iter\n",
      "Epoch: 144 | Batch: 002 / 011 | Total loss: 1.555 | Reg loss: 0.039 | Tree loss: 1.555 | Accuracy: 0.444500 | 1.65 sec/iter\n",
      "Epoch: 144 | Batch: 003 / 011 | Total loss: 1.515 | Reg loss: 0.039 | Tree loss: 1.515 | Accuracy: 0.456000 | 1.651 sec/iter\n",
      "Epoch: 144 | Batch: 004 / 011 | Total loss: 1.522 | Reg loss: 0.039 | Tree loss: 1.522 | Accuracy: 0.461000 | 1.651 sec/iter\n",
      "Epoch: 144 | Batch: 005 / 011 | Total loss: 1.459 | Reg loss: 0.039 | Tree loss: 1.459 | Accuracy: 0.494000 | 1.651 sec/iter\n",
      "Epoch: 144 | Batch: 006 / 011 | Total loss: 1.450 | Reg loss: 0.039 | Tree loss: 1.450 | Accuracy: 0.480000 | 1.651 sec/iter\n",
      "Epoch: 144 | Batch: 007 / 011 | Total loss: 1.450 | Reg loss: 0.039 | Tree loss: 1.450 | Accuracy: 0.477500 | 1.651 sec/iter\n",
      "Epoch: 144 | Batch: 008 / 011 | Total loss: 1.454 | Reg loss: 0.039 | Tree loss: 1.454 | Accuracy: 0.457500 | 1.651 sec/iter\n",
      "Epoch: 144 | Batch: 009 / 011 | Total loss: 1.420 | Reg loss: 0.039 | Tree loss: 1.420 | Accuracy: 0.505500 | 1.651 sec/iter\n",
      "Epoch: 144 | Batch: 010 / 011 | Total loss: 1.451 | Reg loss: 0.039 | Tree loss: 1.451 | Accuracy: 0.481229 | 1.651 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 145 | Batch: 000 / 011 | Total loss: 1.646 | Reg loss: 0.039 | Tree loss: 1.646 | Accuracy: 0.394500 | 1.654 sec/iter\n",
      "Epoch: 145 | Batch: 001 / 011 | Total loss: 1.617 | Reg loss: 0.039 | Tree loss: 1.617 | Accuracy: 0.399500 | 1.654 sec/iter\n",
      "Epoch: 145 | Batch: 002 / 011 | Total loss: 1.574 | Reg loss: 0.039 | Tree loss: 1.574 | Accuracy: 0.403500 | 1.654 sec/iter\n",
      "Epoch: 145 | Batch: 003 / 011 | Total loss: 1.538 | Reg loss: 0.039 | Tree loss: 1.538 | Accuracy: 0.447500 | 1.654 sec/iter\n",
      "Epoch: 145 | Batch: 004 / 011 | Total loss: 1.495 | Reg loss: 0.039 | Tree loss: 1.495 | Accuracy: 0.467000 | 1.654 sec/iter\n",
      "Epoch: 145 | Batch: 005 / 011 | Total loss: 1.487 | Reg loss: 0.039 | Tree loss: 1.487 | Accuracy: 0.496000 | 1.654 sec/iter\n",
      "Epoch: 145 | Batch: 006 / 011 | Total loss: 1.447 | Reg loss: 0.039 | Tree loss: 1.447 | Accuracy: 0.488500 | 1.654 sec/iter\n",
      "Epoch: 145 | Batch: 007 / 011 | Total loss: 1.434 | Reg loss: 0.039 | Tree loss: 1.434 | Accuracy: 0.500500 | 1.654 sec/iter\n",
      "Epoch: 145 | Batch: 008 / 011 | Total loss: 1.432 | Reg loss: 0.039 | Tree loss: 1.432 | Accuracy: 0.476000 | 1.654 sec/iter\n",
      "Epoch: 145 | Batch: 009 / 011 | Total loss: 1.442 | Reg loss: 0.039 | Tree loss: 1.442 | Accuracy: 0.482500 | 1.654 sec/iter\n",
      "Epoch: 145 | Batch: 010 / 011 | Total loss: 1.460 | Reg loss: 0.039 | Tree loss: 1.460 | Accuracy: 0.481229 | 1.654 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 146 | Batch: 000 / 011 | Total loss: 1.640 | Reg loss: 0.039 | Tree loss: 1.640 | Accuracy: 0.388500 | 1.655 sec/iter\n",
      "Epoch: 146 | Batch: 001 / 011 | Total loss: 1.615 | Reg loss: 0.039 | Tree loss: 1.615 | Accuracy: 0.413000 | 1.655 sec/iter\n",
      "Epoch: 146 | Batch: 002 / 011 | Total loss: 1.585 | Reg loss: 0.039 | Tree loss: 1.585 | Accuracy: 0.416000 | 1.654 sec/iter\n",
      "Epoch: 146 | Batch: 003 / 011 | Total loss: 1.517 | Reg loss: 0.039 | Tree loss: 1.517 | Accuracy: 0.470500 | 1.655 sec/iter\n",
      "Epoch: 146 | Batch: 004 / 011 | Total loss: 1.499 | Reg loss: 0.039 | Tree loss: 1.499 | Accuracy: 0.479500 | 1.655 sec/iter\n",
      "Epoch: 146 | Batch: 005 / 011 | Total loss: 1.461 | Reg loss: 0.039 | Tree loss: 1.461 | Accuracy: 0.484500 | 1.655 sec/iter\n",
      "Epoch: 146 | Batch: 006 / 011 | Total loss: 1.454 | Reg loss: 0.039 | Tree loss: 1.454 | Accuracy: 0.497500 | 1.655 sec/iter\n",
      "Epoch: 146 | Batch: 007 / 011 | Total loss: 1.449 | Reg loss: 0.039 | Tree loss: 1.449 | Accuracy: 0.464500 | 1.655 sec/iter\n",
      "Epoch: 146 | Batch: 008 / 011 | Total loss: 1.451 | Reg loss: 0.039 | Tree loss: 1.451 | Accuracy: 0.476000 | 1.655 sec/iter\n",
      "Epoch: 146 | Batch: 009 / 011 | Total loss: 1.445 | Reg loss: 0.039 | Tree loss: 1.445 | Accuracy: 0.458500 | 1.655 sec/iter\n",
      "Epoch: 146 | Batch: 010 / 011 | Total loss: 1.470 | Reg loss: 0.039 | Tree loss: 1.470 | Accuracy: 0.467577 | 1.655 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 147 | Batch: 000 / 011 | Total loss: 1.632 | Reg loss: 0.039 | Tree loss: 1.632 | Accuracy: 0.399500 | 1.658 sec/iter\n",
      "Epoch: 147 | Batch: 001 / 011 | Total loss: 1.610 | Reg loss: 0.039 | Tree loss: 1.610 | Accuracy: 0.406000 | 1.658 sec/iter\n",
      "Epoch: 147 | Batch: 002 / 011 | Total loss: 1.560 | Reg loss: 0.039 | Tree loss: 1.560 | Accuracy: 0.436000 | 1.658 sec/iter\n",
      "Epoch: 147 | Batch: 003 / 011 | Total loss: 1.525 | Reg loss: 0.039 | Tree loss: 1.525 | Accuracy: 0.472500 | 1.658 sec/iter\n",
      "Epoch: 147 | Batch: 004 / 011 | Total loss: 1.505 | Reg loss: 0.039 | Tree loss: 1.505 | Accuracy: 0.464500 | 1.658 sec/iter\n",
      "Epoch: 147 | Batch: 005 / 011 | Total loss: 1.481 | Reg loss: 0.039 | Tree loss: 1.481 | Accuracy: 0.482500 | 1.658 sec/iter\n",
      "Epoch: 147 | Batch: 006 / 011 | Total loss: 1.442 | Reg loss: 0.039 | Tree loss: 1.442 | Accuracy: 0.501000 | 1.658 sec/iter\n",
      "Epoch: 147 | Batch: 007 / 011 | Total loss: 1.447 | Reg loss: 0.039 | Tree loss: 1.447 | Accuracy: 0.481500 | 1.658 sec/iter\n",
      "Epoch: 147 | Batch: 008 / 011 | Total loss: 1.441 | Reg loss: 0.039 | Tree loss: 1.441 | Accuracy: 0.478000 | 1.658 sec/iter\n",
      "Epoch: 147 | Batch: 009 / 011 | Total loss: 1.436 | Reg loss: 0.039 | Tree loss: 1.436 | Accuracy: 0.466500 | 1.658 sec/iter\n",
      "Epoch: 147 | Batch: 010 / 011 | Total loss: 1.492 | Reg loss: 0.039 | Tree loss: 1.492 | Accuracy: 0.453925 | 1.658 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 148 | Batch: 000 / 011 | Total loss: 1.630 | Reg loss: 0.039 | Tree loss: 1.630 | Accuracy: 0.397500 | 1.661 sec/iter\n",
      "Epoch: 148 | Batch: 001 / 011 | Total loss: 1.607 | Reg loss: 0.039 | Tree loss: 1.607 | Accuracy: 0.407000 | 1.661 sec/iter\n",
      "Epoch: 148 | Batch: 002 / 011 | Total loss: 1.567 | Reg loss: 0.039 | Tree loss: 1.567 | Accuracy: 0.427000 | 1.661 sec/iter\n",
      "Epoch: 148 | Batch: 003 / 011 | Total loss: 1.535 | Reg loss: 0.039 | Tree loss: 1.535 | Accuracy: 0.448000 | 1.661 sec/iter\n",
      "Epoch: 148 | Batch: 004 / 011 | Total loss: 1.485 | Reg loss: 0.039 | Tree loss: 1.485 | Accuracy: 0.467000 | 1.661 sec/iter\n",
      "Epoch: 148 | Batch: 005 / 011 | Total loss: 1.500 | Reg loss: 0.039 | Tree loss: 1.500 | Accuracy: 0.471000 | 1.661 sec/iter\n",
      "Epoch: 148 | Batch: 006 / 011 | Total loss: 1.446 | Reg loss: 0.039 | Tree loss: 1.446 | Accuracy: 0.490000 | 1.662 sec/iter\n",
      "Epoch: 148 | Batch: 007 / 011 | Total loss: 1.438 | Reg loss: 0.039 | Tree loss: 1.438 | Accuracy: 0.480500 | 1.661 sec/iter\n",
      "Epoch: 148 | Batch: 008 / 011 | Total loss: 1.452 | Reg loss: 0.039 | Tree loss: 1.452 | Accuracy: 0.473000 | 1.661 sec/iter\n",
      "Epoch: 148 | Batch: 009 / 011 | Total loss: 1.424 | Reg loss: 0.039 | Tree loss: 1.424 | Accuracy: 0.476500 | 1.661 sec/iter\n",
      "Epoch: 148 | Batch: 010 / 011 | Total loss: 1.465 | Reg loss: 0.039 | Tree loss: 1.465 | Accuracy: 0.525597 | 1.661 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 149 | Batch: 000 / 011 | Total loss: 1.650 | Reg loss: 0.039 | Tree loss: 1.650 | Accuracy: 0.389500 | 1.664 sec/iter\n",
      "Epoch: 149 | Batch: 001 / 011 | Total loss: 1.597 | Reg loss: 0.039 | Tree loss: 1.597 | Accuracy: 0.415500 | 1.664 sec/iter\n",
      "Epoch: 149 | Batch: 002 / 011 | Total loss: 1.563 | Reg loss: 0.039 | Tree loss: 1.563 | Accuracy: 0.444000 | 1.664 sec/iter\n",
      "Epoch: 149 | Batch: 003 / 011 | Total loss: 1.524 | Reg loss: 0.039 | Tree loss: 1.524 | Accuracy: 0.447000 | 1.664 sec/iter\n",
      "Epoch: 149 | Batch: 004 / 011 | Total loss: 1.481 | Reg loss: 0.039 | Tree loss: 1.481 | Accuracy: 0.478500 | 1.664 sec/iter\n",
      "Epoch: 149 | Batch: 005 / 011 | Total loss: 1.481 | Reg loss: 0.039 | Tree loss: 1.481 | Accuracy: 0.461000 | 1.664 sec/iter\n",
      "Epoch: 149 | Batch: 006 / 011 | Total loss: 1.452 | Reg loss: 0.039 | Tree loss: 1.452 | Accuracy: 0.480500 | 1.664 sec/iter\n",
      "Epoch: 149 | Batch: 007 / 011 | Total loss: 1.459 | Reg loss: 0.039 | Tree loss: 1.459 | Accuracy: 0.463500 | 1.664 sec/iter\n",
      "Epoch: 149 | Batch: 008 / 011 | Total loss: 1.447 | Reg loss: 0.039 | Tree loss: 1.447 | Accuracy: 0.482500 | 1.664 sec/iter\n",
      "Epoch: 149 | Batch: 009 / 011 | Total loss: 1.439 | Reg loss: 0.039 | Tree loss: 1.439 | Accuracy: 0.483500 | 1.664 sec/iter\n",
      "Epoch: 149 | Batch: 010 / 011 | Total loss: 1.368 | Reg loss: 0.039 | Tree loss: 1.368 | Accuracy: 0.508532 | 1.664 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 150 | Batch: 000 / 011 | Total loss: 1.631 | Reg loss: 0.039 | Tree loss: 1.631 | Accuracy: 0.397500 | 1.667 sec/iter\n",
      "Epoch: 150 | Batch: 001 / 011 | Total loss: 1.600 | Reg loss: 0.039 | Tree loss: 1.600 | Accuracy: 0.404000 | 1.667 sec/iter\n",
      "Epoch: 150 | Batch: 002 / 011 | Total loss: 1.565 | Reg loss: 0.039 | Tree loss: 1.565 | Accuracy: 0.414000 | 1.667 sec/iter\n",
      "Epoch: 150 | Batch: 003 / 011 | Total loss: 1.531 | Reg loss: 0.039 | Tree loss: 1.531 | Accuracy: 0.468000 | 1.667 sec/iter\n",
      "Epoch: 150 | Batch: 004 / 011 | Total loss: 1.491 | Reg loss: 0.039 | Tree loss: 1.491 | Accuracy: 0.471000 | 1.667 sec/iter\n",
      "Epoch: 150 | Batch: 005 / 011 | Total loss: 1.468 | Reg loss: 0.039 | Tree loss: 1.468 | Accuracy: 0.477500 | 1.667 sec/iter\n",
      "Epoch: 150 | Batch: 006 / 011 | Total loss: 1.463 | Reg loss: 0.039 | Tree loss: 1.463 | Accuracy: 0.473000 | 1.667 sec/iter\n",
      "Epoch: 150 | Batch: 007 / 011 | Total loss: 1.433 | Reg loss: 0.039 | Tree loss: 1.433 | Accuracy: 0.495000 | 1.668 sec/iter\n",
      "Epoch: 150 | Batch: 008 / 011 | Total loss: 1.454 | Reg loss: 0.039 | Tree loss: 1.454 | Accuracy: 0.471000 | 1.667 sec/iter\n",
      "Epoch: 150 | Batch: 009 / 011 | Total loss: 1.449 | Reg loss: 0.039 | Tree loss: 1.449 | Accuracy: 0.494500 | 1.668 sec/iter\n",
      "Epoch: 150 | Batch: 010 / 011 | Total loss: 1.370 | Reg loss: 0.039 | Tree loss: 1.370 | Accuracy: 0.494881 | 1.668 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 151 | Batch: 000 / 011 | Total loss: 1.621 | Reg loss: 0.039 | Tree loss: 1.621 | Accuracy: 0.394500 | 1.668 sec/iter\n",
      "Epoch: 151 | Batch: 001 / 011 | Total loss: 1.611 | Reg loss: 0.039 | Tree loss: 1.611 | Accuracy: 0.390500 | 1.668 sec/iter\n",
      "Epoch: 151 | Batch: 002 / 011 | Total loss: 1.558 | Reg loss: 0.039 | Tree loss: 1.558 | Accuracy: 0.428500 | 1.668 sec/iter\n",
      "Epoch: 151 | Batch: 003 / 011 | Total loss: 1.554 | Reg loss: 0.039 | Tree loss: 1.554 | Accuracy: 0.442000 | 1.668 sec/iter\n",
      "Epoch: 151 | Batch: 004 / 011 | Total loss: 1.498 | Reg loss: 0.039 | Tree loss: 1.498 | Accuracy: 0.468000 | 1.668 sec/iter\n",
      "Epoch: 151 | Batch: 005 / 011 | Total loss: 1.472 | Reg loss: 0.039 | Tree loss: 1.472 | Accuracy: 0.472500 | 1.668 sec/iter\n",
      "Epoch: 151 | Batch: 006 / 011 | Total loss: 1.480 | Reg loss: 0.039 | Tree loss: 1.480 | Accuracy: 0.470000 | 1.668 sec/iter\n",
      "Epoch: 151 | Batch: 007 / 011 | Total loss: 1.419 | Reg loss: 0.039 | Tree loss: 1.419 | Accuracy: 0.495000 | 1.668 sec/iter\n",
      "Epoch: 151 | Batch: 008 / 011 | Total loss: 1.423 | Reg loss: 0.039 | Tree loss: 1.423 | Accuracy: 0.482500 | 1.668 sec/iter\n",
      "Epoch: 151 | Batch: 009 / 011 | Total loss: 1.440 | Reg loss: 0.039 | Tree loss: 1.440 | Accuracy: 0.482500 | 1.669 sec/iter\n",
      "Epoch: 151 | Batch: 010 / 011 | Total loss: 1.421 | Reg loss: 0.039 | Tree loss: 1.421 | Accuracy: 0.511945 | 1.669 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 152 | Batch: 000 / 011 | Total loss: 1.638 | Reg loss: 0.039 | Tree loss: 1.638 | Accuracy: 0.400500 | 1.671 sec/iter\n",
      "Epoch: 152 | Batch: 001 / 011 | Total loss: 1.608 | Reg loss: 0.039 | Tree loss: 1.608 | Accuracy: 0.399500 | 1.671 sec/iter\n",
      "Epoch: 152 | Batch: 002 / 011 | Total loss: 1.570 | Reg loss: 0.039 | Tree loss: 1.570 | Accuracy: 0.424500 | 1.671 sec/iter\n",
      "Epoch: 152 | Batch: 003 / 011 | Total loss: 1.511 | Reg loss: 0.039 | Tree loss: 1.511 | Accuracy: 0.472000 | 1.671 sec/iter\n",
      "Epoch: 152 | Batch: 004 / 011 | Total loss: 1.503 | Reg loss: 0.039 | Tree loss: 1.503 | Accuracy: 0.470000 | 1.671 sec/iter\n",
      "Epoch: 152 | Batch: 005 / 011 | Total loss: 1.456 | Reg loss: 0.039 | Tree loss: 1.456 | Accuracy: 0.486500 | 1.671 sec/iter\n",
      "Epoch: 152 | Batch: 006 / 011 | Total loss: 1.464 | Reg loss: 0.039 | Tree loss: 1.464 | Accuracy: 0.491000 | 1.672 sec/iter\n",
      "Epoch: 152 | Batch: 007 / 011 | Total loss: 1.439 | Reg loss: 0.039 | Tree loss: 1.439 | Accuracy: 0.475500 | 1.672 sec/iter\n",
      "Epoch: 152 | Batch: 008 / 011 | Total loss: 1.438 | Reg loss: 0.039 | Tree loss: 1.438 | Accuracy: 0.485000 | 1.672 sec/iter\n",
      "Epoch: 152 | Batch: 009 / 011 | Total loss: 1.434 | Reg loss: 0.039 | Tree loss: 1.434 | Accuracy: 0.473500 | 1.671 sec/iter\n",
      "Epoch: 152 | Batch: 010 / 011 | Total loss: 1.440 | Reg loss: 0.039 | Tree loss: 1.440 | Accuracy: 0.450512 | 1.671 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 153 | Batch: 000 / 011 | Total loss: 1.631 | Reg loss: 0.039 | Tree loss: 1.631 | Accuracy: 0.390000 | 1.674 sec/iter\n",
      "Epoch: 153 | Batch: 001 / 011 | Total loss: 1.580 | Reg loss: 0.039 | Tree loss: 1.580 | Accuracy: 0.406000 | 1.674 sec/iter\n",
      "Epoch: 153 | Batch: 002 / 011 | Total loss: 1.566 | Reg loss: 0.039 | Tree loss: 1.566 | Accuracy: 0.447500 | 1.675 sec/iter\n",
      "Epoch: 153 | Batch: 003 / 011 | Total loss: 1.531 | Reg loss: 0.039 | Tree loss: 1.531 | Accuracy: 0.448000 | 1.675 sec/iter\n",
      "Epoch: 153 | Batch: 004 / 011 | Total loss: 1.468 | Reg loss: 0.039 | Tree loss: 1.468 | Accuracy: 0.495500 | 1.675 sec/iter\n",
      "Epoch: 153 | Batch: 005 / 011 | Total loss: 1.464 | Reg loss: 0.039 | Tree loss: 1.464 | Accuracy: 0.503000 | 1.675 sec/iter\n",
      "Epoch: 153 | Batch: 006 / 011 | Total loss: 1.451 | Reg loss: 0.039 | Tree loss: 1.451 | Accuracy: 0.508500 | 1.675 sec/iter\n",
      "Epoch: 153 | Batch: 007 / 011 | Total loss: 1.457 | Reg loss: 0.039 | Tree loss: 1.457 | Accuracy: 0.483000 | 1.675 sec/iter\n",
      "Epoch: 153 | Batch: 008 / 011 | Total loss: 1.446 | Reg loss: 0.039 | Tree loss: 1.446 | Accuracy: 0.483000 | 1.675 sec/iter\n",
      "Epoch: 153 | Batch: 009 / 011 | Total loss: 1.431 | Reg loss: 0.039 | Tree loss: 1.431 | Accuracy: 0.480000 | 1.675 sec/iter\n",
      "Epoch: 153 | Batch: 010 / 011 | Total loss: 1.499 | Reg loss: 0.039 | Tree loss: 1.499 | Accuracy: 0.460751 | 1.675 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 154 | Batch: 000 / 011 | Total loss: 1.647 | Reg loss: 0.039 | Tree loss: 1.647 | Accuracy: 0.406500 | 1.677 sec/iter\n",
      "Epoch: 154 | Batch: 001 / 011 | Total loss: 1.581 | Reg loss: 0.039 | Tree loss: 1.581 | Accuracy: 0.414000 | 1.677 sec/iter\n",
      "Epoch: 154 | Batch: 002 / 011 | Total loss: 1.576 | Reg loss: 0.039 | Tree loss: 1.576 | Accuracy: 0.427000 | 1.677 sec/iter\n",
      "Epoch: 154 | Batch: 003 / 011 | Total loss: 1.519 | Reg loss: 0.039 | Tree loss: 1.519 | Accuracy: 0.463000 | 1.678 sec/iter\n",
      "Epoch: 154 | Batch: 004 / 011 | Total loss: 1.492 | Reg loss: 0.039 | Tree loss: 1.492 | Accuracy: 0.469500 | 1.677 sec/iter\n",
      "Epoch: 154 | Batch: 005 / 011 | Total loss: 1.476 | Reg loss: 0.039 | Tree loss: 1.476 | Accuracy: 0.466000 | 1.677 sec/iter\n",
      "Epoch: 154 | Batch: 006 / 011 | Total loss: 1.443 | Reg loss: 0.039 | Tree loss: 1.443 | Accuracy: 0.492500 | 1.677 sec/iter\n",
      "Epoch: 154 | Batch: 007 / 011 | Total loss: 1.443 | Reg loss: 0.039 | Tree loss: 1.443 | Accuracy: 0.476500 | 1.677 sec/iter\n",
      "Epoch: 154 | Batch: 008 / 011 | Total loss: 1.431 | Reg loss: 0.039 | Tree loss: 1.431 | Accuracy: 0.477500 | 1.677 sec/iter\n",
      "Epoch: 154 | Batch: 009 / 011 | Total loss: 1.438 | Reg loss: 0.039 | Tree loss: 1.438 | Accuracy: 0.476000 | 1.677 sec/iter\n",
      "Epoch: 154 | Batch: 010 / 011 | Total loss: 1.441 | Reg loss: 0.039 | Tree loss: 1.441 | Accuracy: 0.484642 | 1.676 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 155 | Batch: 000 / 011 | Total loss: 1.654 | Reg loss: 0.039 | Tree loss: 1.654 | Accuracy: 0.374500 | 1.676 sec/iter\n",
      "Epoch: 155 | Batch: 001 / 011 | Total loss: 1.595 | Reg loss: 0.039 | Tree loss: 1.595 | Accuracy: 0.429000 | 1.676 sec/iter\n",
      "Epoch: 155 | Batch: 002 / 011 | Total loss: 1.552 | Reg loss: 0.039 | Tree loss: 1.552 | Accuracy: 0.456000 | 1.676 sec/iter\n",
      "Epoch: 155 | Batch: 003 / 011 | Total loss: 1.529 | Reg loss: 0.039 | Tree loss: 1.529 | Accuracy: 0.473000 | 1.676 sec/iter\n",
      "Epoch: 155 | Batch: 004 / 011 | Total loss: 1.517 | Reg loss: 0.039 | Tree loss: 1.517 | Accuracy: 0.481000 | 1.676 sec/iter\n",
      "Epoch: 155 | Batch: 005 / 011 | Total loss: 1.479 | Reg loss: 0.039 | Tree loss: 1.479 | Accuracy: 0.487500 | 1.675 sec/iter\n",
      "Epoch: 155 | Batch: 006 / 011 | Total loss: 1.424 | Reg loss: 0.039 | Tree loss: 1.424 | Accuracy: 0.503500 | 1.675 sec/iter\n",
      "Epoch: 155 | Batch: 007 / 011 | Total loss: 1.452 | Reg loss: 0.039 | Tree loss: 1.452 | Accuracy: 0.482000 | 1.675 sec/iter\n",
      "Epoch: 155 | Batch: 008 / 011 | Total loss: 1.435 | Reg loss: 0.039 | Tree loss: 1.435 | Accuracy: 0.475500 | 1.675 sec/iter\n",
      "Epoch: 155 | Batch: 009 / 011 | Total loss: 1.436 | Reg loss: 0.039 | Tree loss: 1.436 | Accuracy: 0.483000 | 1.675 sec/iter\n",
      "Epoch: 155 | Batch: 010 / 011 | Total loss: 1.427 | Reg loss: 0.039 | Tree loss: 1.427 | Accuracy: 0.508532 | 1.674 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 156 | Batch: 000 / 011 | Total loss: 1.666 | Reg loss: 0.039 | Tree loss: 1.666 | Accuracy: 0.388500 | 1.677 sec/iter\n",
      "Epoch: 156 | Batch: 001 / 011 | Total loss: 1.603 | Reg loss: 0.039 | Tree loss: 1.603 | Accuracy: 0.417500 | 1.677 sec/iter\n",
      "Epoch: 156 | Batch: 002 / 011 | Total loss: 1.570 | Reg loss: 0.039 | Tree loss: 1.570 | Accuracy: 0.428500 | 1.677 sec/iter\n",
      "Epoch: 156 | Batch: 003 / 011 | Total loss: 1.545 | Reg loss: 0.039 | Tree loss: 1.545 | Accuracy: 0.442500 | 1.677 sec/iter\n",
      "Epoch: 156 | Batch: 004 / 011 | Total loss: 1.475 | Reg loss: 0.039 | Tree loss: 1.475 | Accuracy: 0.488000 | 1.677 sec/iter\n",
      "Epoch: 156 | Batch: 005 / 011 | Total loss: 1.446 | Reg loss: 0.039 | Tree loss: 1.446 | Accuracy: 0.508500 | 1.677 sec/iter\n",
      "Epoch: 156 | Batch: 006 / 011 | Total loss: 1.447 | Reg loss: 0.039 | Tree loss: 1.447 | Accuracy: 0.486000 | 1.677 sec/iter\n",
      "Epoch: 156 | Batch: 007 / 011 | Total loss: 1.417 | Reg loss: 0.039 | Tree loss: 1.417 | Accuracy: 0.489500 | 1.676 sec/iter\n",
      "Epoch: 156 | Batch: 008 / 011 | Total loss: 1.466 | Reg loss: 0.039 | Tree loss: 1.466 | Accuracy: 0.461500 | 1.676 sec/iter\n",
      "Epoch: 156 | Batch: 009 / 011 | Total loss: 1.436 | Reg loss: 0.039 | Tree loss: 1.436 | Accuracy: 0.488000 | 1.676 sec/iter\n",
      "Epoch: 156 | Batch: 010 / 011 | Total loss: 1.399 | Reg loss: 0.039 | Tree loss: 1.399 | Accuracy: 0.505119 | 1.676 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 157 | Batch: 000 / 011 | Total loss: 1.638 | Reg loss: 0.039 | Tree loss: 1.638 | Accuracy: 0.402500 | 1.676 sec/iter\n",
      "Epoch: 157 | Batch: 001 / 011 | Total loss: 1.604 | Reg loss: 0.039 | Tree loss: 1.604 | Accuracy: 0.420000 | 1.675 sec/iter\n",
      "Epoch: 157 | Batch: 002 / 011 | Total loss: 1.564 | Reg loss: 0.039 | Tree loss: 1.564 | Accuracy: 0.435000 | 1.675 sec/iter\n",
      "Epoch: 157 | Batch: 003 / 011 | Total loss: 1.516 | Reg loss: 0.039 | Tree loss: 1.516 | Accuracy: 0.475500 | 1.674 sec/iter\n",
      "Epoch: 157 | Batch: 004 / 011 | Total loss: 1.489 | Reg loss: 0.039 | Tree loss: 1.489 | Accuracy: 0.484500 | 1.674 sec/iter\n",
      "Epoch: 157 | Batch: 005 / 011 | Total loss: 1.488 | Reg loss: 0.039 | Tree loss: 1.488 | Accuracy: 0.495000 | 1.674 sec/iter\n",
      "Epoch: 157 | Batch: 006 / 011 | Total loss: 1.458 | Reg loss: 0.039 | Tree loss: 1.458 | Accuracy: 0.493500 | 1.673 sec/iter\n",
      "Epoch: 157 | Batch: 007 / 011 | Total loss: 1.443 | Reg loss: 0.039 | Tree loss: 1.443 | Accuracy: 0.481500 | 1.673 sec/iter\n",
      "Epoch: 157 | Batch: 008 / 011 | Total loss: 1.407 | Reg loss: 0.039 | Tree loss: 1.407 | Accuracy: 0.486000 | 1.673 sec/iter\n",
      "Epoch: 157 | Batch: 009 / 011 | Total loss: 1.438 | Reg loss: 0.039 | Tree loss: 1.438 | Accuracy: 0.479000 | 1.673 sec/iter\n",
      "Epoch: 157 | Batch: 010 / 011 | Total loss: 1.436 | Reg loss: 0.039 | Tree loss: 1.436 | Accuracy: 0.498294 | 1.673 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 158 | Batch: 000 / 011 | Total loss: 1.608 | Reg loss: 0.039 | Tree loss: 1.608 | Accuracy: 0.424500 | 1.674 sec/iter\n",
      "Epoch: 158 | Batch: 001 / 011 | Total loss: 1.610 | Reg loss: 0.039 | Tree loss: 1.610 | Accuracy: 0.411500 | 1.674 sec/iter\n",
      "Epoch: 158 | Batch: 002 / 011 | Total loss: 1.580 | Reg loss: 0.039 | Tree loss: 1.580 | Accuracy: 0.419500 | 1.674 sec/iter\n",
      "Epoch: 158 | Batch: 003 / 011 | Total loss: 1.539 | Reg loss: 0.039 | Tree loss: 1.539 | Accuracy: 0.453000 | 1.674 sec/iter\n",
      "Epoch: 158 | Batch: 004 / 011 | Total loss: 1.483 | Reg loss: 0.039 | Tree loss: 1.483 | Accuracy: 0.475500 | 1.674 sec/iter\n",
      "Epoch: 158 | Batch: 005 / 011 | Total loss: 1.475 | Reg loss: 0.039 | Tree loss: 1.475 | Accuracy: 0.483000 | 1.673 sec/iter\n",
      "Epoch: 158 | Batch: 006 / 011 | Total loss: 1.452 | Reg loss: 0.039 | Tree loss: 1.452 | Accuracy: 0.497000 | 1.673 sec/iter\n",
      "Epoch: 158 | Batch: 007 / 011 | Total loss: 1.430 | Reg loss: 0.039 | Tree loss: 1.430 | Accuracy: 0.488500 | 1.673 sec/iter\n",
      "Epoch: 158 | Batch: 008 / 011 | Total loss: 1.408 | Reg loss: 0.039 | Tree loss: 1.408 | Accuracy: 0.485500 | 1.673 sec/iter\n",
      "Epoch: 158 | Batch: 009 / 011 | Total loss: 1.435 | Reg loss: 0.039 | Tree loss: 1.435 | Accuracy: 0.480500 | 1.673 sec/iter\n",
      "Epoch: 158 | Batch: 010 / 011 | Total loss: 1.419 | Reg loss: 0.039 | Tree loss: 1.419 | Accuracy: 0.467577 | 1.672 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 159 | Batch: 000 / 011 | Total loss: 1.645 | Reg loss: 0.039 | Tree loss: 1.645 | Accuracy: 0.394000 | 1.673 sec/iter\n",
      "Epoch: 159 | Batch: 001 / 011 | Total loss: 1.582 | Reg loss: 0.039 | Tree loss: 1.582 | Accuracy: 0.438500 | 1.673 sec/iter\n",
      "Epoch: 159 | Batch: 002 / 011 | Total loss: 1.564 | Reg loss: 0.039 | Tree loss: 1.564 | Accuracy: 0.437000 | 1.673 sec/iter\n",
      "Epoch: 159 | Batch: 003 / 011 | Total loss: 1.498 | Reg loss: 0.039 | Tree loss: 1.498 | Accuracy: 0.486500 | 1.672 sec/iter\n",
      "Epoch: 159 | Batch: 004 / 011 | Total loss: 1.470 | Reg loss: 0.039 | Tree loss: 1.470 | Accuracy: 0.499500 | 1.672 sec/iter\n",
      "Epoch: 159 | Batch: 005 / 011 | Total loss: 1.481 | Reg loss: 0.039 | Tree loss: 1.481 | Accuracy: 0.494000 | 1.672 sec/iter\n",
      "Epoch: 159 | Batch: 006 / 011 | Total loss: 1.474 | Reg loss: 0.039 | Tree loss: 1.474 | Accuracy: 0.483500 | 1.672 sec/iter\n",
      "Epoch: 159 | Batch: 007 / 011 | Total loss: 1.461 | Reg loss: 0.039 | Tree loss: 1.461 | Accuracy: 0.470500 | 1.672 sec/iter\n",
      "Epoch: 159 | Batch: 008 / 011 | Total loss: 1.424 | Reg loss: 0.039 | Tree loss: 1.424 | Accuracy: 0.481000 | 1.671 sec/iter\n",
      "Epoch: 159 | Batch: 009 / 011 | Total loss: 1.440 | Reg loss: 0.039 | Tree loss: 1.440 | Accuracy: 0.472000 | 1.671 sec/iter\n",
      "Epoch: 159 | Batch: 010 / 011 | Total loss: 1.401 | Reg loss: 0.039 | Tree loss: 1.401 | Accuracy: 0.477816 | 1.671 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 160 | Batch: 000 / 011 | Total loss: 1.632 | Reg loss: 0.039 | Tree loss: 1.632 | Accuracy: 0.412000 | 1.673 sec/iter\n",
      "Epoch: 160 | Batch: 001 / 011 | Total loss: 1.613 | Reg loss: 0.039 | Tree loss: 1.613 | Accuracy: 0.410500 | 1.673 sec/iter\n",
      "Epoch: 160 | Batch: 002 / 011 | Total loss: 1.596 | Reg loss: 0.039 | Tree loss: 1.596 | Accuracy: 0.420500 | 1.673 sec/iter\n",
      "Epoch: 160 | Batch: 003 / 011 | Total loss: 1.559 | Reg loss: 0.039 | Tree loss: 1.559 | Accuracy: 0.419500 | 1.673 sec/iter\n",
      "Epoch: 160 | Batch: 004 / 011 | Total loss: 1.465 | Reg loss: 0.039 | Tree loss: 1.465 | Accuracy: 0.499500 | 1.673 sec/iter\n",
      "Epoch: 160 | Batch: 005 / 011 | Total loss: 1.458 | Reg loss: 0.039 | Tree loss: 1.458 | Accuracy: 0.489000 | 1.673 sec/iter\n",
      "Epoch: 160 | Batch: 006 / 011 | Total loss: 1.433 | Reg loss: 0.039 | Tree loss: 1.433 | Accuracy: 0.505000 | 1.672 sec/iter\n",
      "Epoch: 160 | Batch: 007 / 011 | Total loss: 1.464 | Reg loss: 0.039 | Tree loss: 1.464 | Accuracy: 0.463500 | 1.672 sec/iter\n",
      "Epoch: 160 | Batch: 008 / 011 | Total loss: 1.428 | Reg loss: 0.039 | Tree loss: 1.428 | Accuracy: 0.493500 | 1.672 sec/iter\n",
      "Epoch: 160 | Batch: 009 / 011 | Total loss: 1.423 | Reg loss: 0.039 | Tree loss: 1.423 | Accuracy: 0.472500 | 1.672 sec/iter\n",
      "Epoch: 160 | Batch: 010 / 011 | Total loss: 1.451 | Reg loss: 0.039 | Tree loss: 1.451 | Accuracy: 0.488055 | 1.671 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 161 | Batch: 000 / 011 | Total loss: 1.633 | Reg loss: 0.039 | Tree loss: 1.633 | Accuracy: 0.404500 | 1.671 sec/iter\n",
      "Epoch: 161 | Batch: 001 / 011 | Total loss: 1.587 | Reg loss: 0.039 | Tree loss: 1.587 | Accuracy: 0.428000 | 1.671 sec/iter\n",
      "Epoch: 161 | Batch: 002 / 011 | Total loss: 1.563 | Reg loss: 0.039 | Tree loss: 1.563 | Accuracy: 0.442500 | 1.671 sec/iter\n",
      "Epoch: 161 | Batch: 003 / 011 | Total loss: 1.537 | Reg loss: 0.039 | Tree loss: 1.537 | Accuracy: 0.452000 | 1.671 sec/iter\n",
      "Epoch: 161 | Batch: 004 / 011 | Total loss: 1.481 | Reg loss: 0.039 | Tree loss: 1.481 | Accuracy: 0.485000 | 1.671 sec/iter\n",
      "Epoch: 161 | Batch: 005 / 011 | Total loss: 1.470 | Reg loss: 0.039 | Tree loss: 1.470 | Accuracy: 0.477500 | 1.671 sec/iter\n",
      "Epoch: 161 | Batch: 006 / 011 | Total loss: 1.451 | Reg loss: 0.039 | Tree loss: 1.451 | Accuracy: 0.486500 | 1.671 sec/iter\n",
      "Epoch: 161 | Batch: 007 / 011 | Total loss: 1.438 | Reg loss: 0.039 | Tree loss: 1.438 | Accuracy: 0.486500 | 1.671 sec/iter\n",
      "Epoch: 161 | Batch: 008 / 011 | Total loss: 1.421 | Reg loss: 0.039 | Tree loss: 1.421 | Accuracy: 0.501500 | 1.67 sec/iter\n",
      "Epoch: 161 | Batch: 009 / 011 | Total loss: 1.441 | Reg loss: 0.039 | Tree loss: 1.441 | Accuracy: 0.470500 | 1.67 sec/iter\n",
      "Epoch: 161 | Batch: 010 / 011 | Total loss: 1.488 | Reg loss: 0.039 | Tree loss: 1.488 | Accuracy: 0.426621 | 1.67 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 162 | Batch: 000 / 011 | Total loss: 1.651 | Reg loss: 0.039 | Tree loss: 1.651 | Accuracy: 0.397000 | 1.674 sec/iter\n",
      "Epoch: 162 | Batch: 001 / 011 | Total loss: 1.583 | Reg loss: 0.039 | Tree loss: 1.583 | Accuracy: 0.429000 | 1.674 sec/iter\n",
      "Epoch: 162 | Batch: 002 / 011 | Total loss: 1.565 | Reg loss: 0.039 | Tree loss: 1.565 | Accuracy: 0.427500 | 1.674 sec/iter\n",
      "Epoch: 162 | Batch: 003 / 011 | Total loss: 1.536 | Reg loss: 0.039 | Tree loss: 1.536 | Accuracy: 0.442000 | 1.674 sec/iter\n",
      "Epoch: 162 | Batch: 004 / 011 | Total loss: 1.461 | Reg loss: 0.039 | Tree loss: 1.461 | Accuracy: 0.482000 | 1.674 sec/iter\n",
      "Epoch: 162 | Batch: 005 / 011 | Total loss: 1.477 | Reg loss: 0.039 | Tree loss: 1.477 | Accuracy: 0.473000 | 1.674 sec/iter\n",
      "Epoch: 162 | Batch: 006 / 011 | Total loss: 1.470 | Reg loss: 0.039 | Tree loss: 1.470 | Accuracy: 0.495000 | 1.673 sec/iter\n",
      "Epoch: 162 | Batch: 007 / 011 | Total loss: 1.448 | Reg loss: 0.039 | Tree loss: 1.448 | Accuracy: 0.502500 | 1.673 sec/iter\n",
      "Epoch: 162 | Batch: 008 / 011 | Total loss: 1.416 | Reg loss: 0.039 | Tree loss: 1.416 | Accuracy: 0.494000 | 1.673 sec/iter\n",
      "Epoch: 162 | Batch: 009 / 011 | Total loss: 1.423 | Reg loss: 0.039 | Tree loss: 1.423 | Accuracy: 0.490500 | 1.673 sec/iter\n",
      "Epoch: 162 | Batch: 010 / 011 | Total loss: 1.416 | Reg loss: 0.039 | Tree loss: 1.416 | Accuracy: 0.467577 | 1.673 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 163 | Batch: 000 / 011 | Total loss: 1.614 | Reg loss: 0.039 | Tree loss: 1.614 | Accuracy: 0.402500 | 1.673 sec/iter\n",
      "Epoch: 163 | Batch: 001 / 011 | Total loss: 1.621 | Reg loss: 0.039 | Tree loss: 1.621 | Accuracy: 0.400500 | 1.673 sec/iter\n",
      "Epoch: 163 | Batch: 002 / 011 | Total loss: 1.548 | Reg loss: 0.039 | Tree loss: 1.548 | Accuracy: 0.450000 | 1.673 sec/iter\n",
      "Epoch: 163 | Batch: 003 / 011 | Total loss: 1.497 | Reg loss: 0.039 | Tree loss: 1.497 | Accuracy: 0.486500 | 1.673 sec/iter\n",
      "Epoch: 163 | Batch: 004 / 011 | Total loss: 1.513 | Reg loss: 0.039 | Tree loss: 1.513 | Accuracy: 0.485500 | 1.673 sec/iter\n",
      "Epoch: 163 | Batch: 005 / 011 | Total loss: 1.479 | Reg loss: 0.039 | Tree loss: 1.479 | Accuracy: 0.462500 | 1.673 sec/iter\n",
      "Epoch: 163 | Batch: 006 / 011 | Total loss: 1.451 | Reg loss: 0.039 | Tree loss: 1.451 | Accuracy: 0.496500 | 1.672 sec/iter\n",
      "Epoch: 163 | Batch: 007 / 011 | Total loss: 1.414 | Reg loss: 0.039 | Tree loss: 1.414 | Accuracy: 0.485500 | 1.672 sec/iter\n",
      "Epoch: 163 | Batch: 008 / 011 | Total loss: 1.445 | Reg loss: 0.039 | Tree loss: 1.445 | Accuracy: 0.479000 | 1.672 sec/iter\n",
      "Epoch: 163 | Batch: 009 / 011 | Total loss: 1.424 | Reg loss: 0.039 | Tree loss: 1.424 | Accuracy: 0.507500 | 1.672 sec/iter\n",
      "Epoch: 163 | Batch: 010 / 011 | Total loss: 1.460 | Reg loss: 0.039 | Tree loss: 1.460 | Accuracy: 0.515358 | 1.671 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 164 | Batch: 000 / 011 | Total loss: 1.608 | Reg loss: 0.039 | Tree loss: 1.608 | Accuracy: 0.400000 | 1.676 sec/iter\n",
      "Epoch: 164 | Batch: 001 / 011 | Total loss: 1.592 | Reg loss: 0.039 | Tree loss: 1.592 | Accuracy: 0.407000 | 1.676 sec/iter\n",
      "Epoch: 164 | Batch: 002 / 011 | Total loss: 1.562 | Reg loss: 0.039 | Tree loss: 1.562 | Accuracy: 0.436000 | 1.676 sec/iter\n",
      "Epoch: 164 | Batch: 003 / 011 | Total loss: 1.528 | Reg loss: 0.039 | Tree loss: 1.528 | Accuracy: 0.455000 | 1.675 sec/iter\n",
      "Epoch: 164 | Batch: 004 / 011 | Total loss: 1.478 | Reg loss: 0.039 | Tree loss: 1.478 | Accuracy: 0.486500 | 1.675 sec/iter\n",
      "Epoch: 164 | Batch: 005 / 011 | Total loss: 1.478 | Reg loss: 0.039 | Tree loss: 1.478 | Accuracy: 0.464000 | 1.675 sec/iter\n",
      "Epoch: 164 | Batch: 006 / 011 | Total loss: 1.426 | Reg loss: 0.039 | Tree loss: 1.426 | Accuracy: 0.521000 | 1.674 sec/iter\n",
      "Epoch: 164 | Batch: 007 / 011 | Total loss: 1.477 | Reg loss: 0.039 | Tree loss: 1.477 | Accuracy: 0.469500 | 1.674 sec/iter\n",
      "Epoch: 164 | Batch: 008 / 011 | Total loss: 1.427 | Reg loss: 0.039 | Tree loss: 1.427 | Accuracy: 0.495000 | 1.673 sec/iter\n",
      "Epoch: 164 | Batch: 009 / 011 | Total loss: 1.425 | Reg loss: 0.039 | Tree loss: 1.425 | Accuracy: 0.492000 | 1.673 sec/iter\n",
      "Epoch: 164 | Batch: 010 / 011 | Total loss: 1.466 | Reg loss: 0.039 | Tree loss: 1.466 | Accuracy: 0.474403 | 1.673 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 165 | Batch: 000 / 011 | Total loss: 1.616 | Reg loss: 0.039 | Tree loss: 1.616 | Accuracy: 0.397500 | 1.675 sec/iter\n",
      "Epoch: 165 | Batch: 001 / 011 | Total loss: 1.603 | Reg loss: 0.039 | Tree loss: 1.603 | Accuracy: 0.407500 | 1.675 sec/iter\n",
      "Epoch: 165 | Batch: 002 / 011 | Total loss: 1.573 | Reg loss: 0.039 | Tree loss: 1.573 | Accuracy: 0.422000 | 1.675 sec/iter\n",
      "Epoch: 165 | Batch: 003 / 011 | Total loss: 1.525 | Reg loss: 0.039 | Tree loss: 1.525 | Accuracy: 0.466000 | 1.675 sec/iter\n",
      "Epoch: 165 | Batch: 004 / 011 | Total loss: 1.480 | Reg loss: 0.039 | Tree loss: 1.480 | Accuracy: 0.490000 | 1.675 sec/iter\n",
      "Epoch: 165 | Batch: 005 / 011 | Total loss: 1.450 | Reg loss: 0.039 | Tree loss: 1.450 | Accuracy: 0.500500 | 1.675 sec/iter\n",
      "Epoch: 165 | Batch: 006 / 011 | Total loss: 1.468 | Reg loss: 0.039 | Tree loss: 1.468 | Accuracy: 0.491500 | 1.674 sec/iter\n",
      "Epoch: 165 | Batch: 007 / 011 | Total loss: 1.423 | Reg loss: 0.039 | Tree loss: 1.423 | Accuracy: 0.493500 | 1.674 sec/iter\n",
      "Epoch: 165 | Batch: 008 / 011 | Total loss: 1.441 | Reg loss: 0.039 | Tree loss: 1.441 | Accuracy: 0.480000 | 1.674 sec/iter\n",
      "Epoch: 165 | Batch: 009 / 011 | Total loss: 1.425 | Reg loss: 0.039 | Tree loss: 1.425 | Accuracy: 0.491500 | 1.674 sec/iter\n",
      "Epoch: 165 | Batch: 010 / 011 | Total loss: 1.438 | Reg loss: 0.039 | Tree loss: 1.438 | Accuracy: 0.505119 | 1.674 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 166 | Batch: 000 / 011 | Total loss: 1.647 | Reg loss: 0.039 | Tree loss: 1.647 | Accuracy: 0.378000 | 1.675 sec/iter\n",
      "Epoch: 166 | Batch: 001 / 011 | Total loss: 1.596 | Reg loss: 0.039 | Tree loss: 1.596 | Accuracy: 0.412500 | 1.675 sec/iter\n",
      "Epoch: 166 | Batch: 002 / 011 | Total loss: 1.548 | Reg loss: 0.039 | Tree loss: 1.548 | Accuracy: 0.429000 | 1.675 sec/iter\n",
      "Epoch: 166 | Batch: 003 / 011 | Total loss: 1.527 | Reg loss: 0.039 | Tree loss: 1.527 | Accuracy: 0.449500 | 1.675 sec/iter\n",
      "Epoch: 166 | Batch: 004 / 011 | Total loss: 1.487 | Reg loss: 0.039 | Tree loss: 1.487 | Accuracy: 0.457000 | 1.675 sec/iter\n",
      "Epoch: 166 | Batch: 005 / 011 | Total loss: 1.454 | Reg loss: 0.039 | Tree loss: 1.454 | Accuracy: 0.487500 | 1.675 sec/iter\n",
      "Epoch: 166 | Batch: 006 / 011 | Total loss: 1.457 | Reg loss: 0.039 | Tree loss: 1.457 | Accuracy: 0.494000 | 1.675 sec/iter\n",
      "Epoch: 166 | Batch: 007 / 011 | Total loss: 1.404 | Reg loss: 0.039 | Tree loss: 1.404 | Accuracy: 0.513000 | 1.675 sec/iter\n",
      "Epoch: 166 | Batch: 008 / 011 | Total loss: 1.441 | Reg loss: 0.039 | Tree loss: 1.441 | Accuracy: 0.501000 | 1.675 sec/iter\n",
      "Epoch: 166 | Batch: 009 / 011 | Total loss: 1.444 | Reg loss: 0.039 | Tree loss: 1.444 | Accuracy: 0.489000 | 1.675 sec/iter\n",
      "Epoch: 166 | Batch: 010 / 011 | Total loss: 1.462 | Reg loss: 0.039 | Tree loss: 1.462 | Accuracy: 0.464164 | 1.674 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 167 | Batch: 000 / 011 | Total loss: 1.608 | Reg loss: 0.039 | Tree loss: 1.608 | Accuracy: 0.420500 | 1.675 sec/iter\n",
      "Epoch: 167 | Batch: 001 / 011 | Total loss: 1.595 | Reg loss: 0.039 | Tree loss: 1.595 | Accuracy: 0.405000 | 1.675 sec/iter\n",
      "Epoch: 167 | Batch: 002 / 011 | Total loss: 1.563 | Reg loss: 0.039 | Tree loss: 1.563 | Accuracy: 0.425500 | 1.675 sec/iter\n",
      "Epoch: 167 | Batch: 003 / 011 | Total loss: 1.552 | Reg loss: 0.039 | Tree loss: 1.552 | Accuracy: 0.433500 | 1.675 sec/iter\n",
      "Epoch: 167 | Batch: 004 / 011 | Total loss: 1.490 | Reg loss: 0.039 | Tree loss: 1.490 | Accuracy: 0.486000 | 1.675 sec/iter\n",
      "Epoch: 167 | Batch: 005 / 011 | Total loss: 1.464 | Reg loss: 0.039 | Tree loss: 1.464 | Accuracy: 0.487000 | 1.675 sec/iter\n",
      "Epoch: 167 | Batch: 006 / 011 | Total loss: 1.432 | Reg loss: 0.039 | Tree loss: 1.432 | Accuracy: 0.511000 | 1.675 sec/iter\n",
      "Epoch: 167 | Batch: 007 / 011 | Total loss: 1.427 | Reg loss: 0.039 | Tree loss: 1.427 | Accuracy: 0.503500 | 1.675 sec/iter\n",
      "Epoch: 167 | Batch: 008 / 011 | Total loss: 1.440 | Reg loss: 0.039 | Tree loss: 1.440 | Accuracy: 0.492000 | 1.675 sec/iter\n",
      "Epoch: 167 | Batch: 009 / 011 | Total loss: 1.422 | Reg loss: 0.039 | Tree loss: 1.422 | Accuracy: 0.481500 | 1.674 sec/iter\n",
      "Epoch: 167 | Batch: 010 / 011 | Total loss: 1.433 | Reg loss: 0.039 | Tree loss: 1.433 | Accuracy: 0.494881 | 1.674 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 168 | Batch: 000 / 011 | Total loss: 1.625 | Reg loss: 0.039 | Tree loss: 1.625 | Accuracy: 0.398000 | 1.677 sec/iter\n",
      "Epoch: 168 | Batch: 001 / 011 | Total loss: 1.576 | Reg loss: 0.039 | Tree loss: 1.576 | Accuracy: 0.419500 | 1.677 sec/iter\n",
      "Epoch: 168 | Batch: 002 / 011 | Total loss: 1.555 | Reg loss: 0.039 | Tree loss: 1.555 | Accuracy: 0.435000 | 1.677 sec/iter\n",
      "Epoch: 168 | Batch: 003 / 011 | Total loss: 1.524 | Reg loss: 0.039 | Tree loss: 1.524 | Accuracy: 0.457500 | 1.676 sec/iter\n",
      "Epoch: 168 | Batch: 004 / 011 | Total loss: 1.461 | Reg loss: 0.039 | Tree loss: 1.461 | Accuracy: 0.473000 | 1.676 sec/iter\n",
      "Epoch: 168 | Batch: 005 / 011 | Total loss: 1.471 | Reg loss: 0.039 | Tree loss: 1.471 | Accuracy: 0.487000 | 1.676 sec/iter\n",
      "Epoch: 168 | Batch: 006 / 011 | Total loss: 1.449 | Reg loss: 0.039 | Tree loss: 1.449 | Accuracy: 0.505500 | 1.676 sec/iter\n",
      "Epoch: 168 | Batch: 007 / 011 | Total loss: 1.443 | Reg loss: 0.039 | Tree loss: 1.443 | Accuracy: 0.472000 | 1.676 sec/iter\n",
      "Epoch: 168 | Batch: 008 / 011 | Total loss: 1.421 | Reg loss: 0.039 | Tree loss: 1.421 | Accuracy: 0.485500 | 1.676 sec/iter\n",
      "Epoch: 168 | Batch: 009 / 011 | Total loss: 1.461 | Reg loss: 0.039 | Tree loss: 1.461 | Accuracy: 0.490500 | 1.676 sec/iter\n",
      "Epoch: 168 | Batch: 010 / 011 | Total loss: 1.387 | Reg loss: 0.039 | Tree loss: 1.387 | Accuracy: 0.508532 | 1.675 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 169 | Batch: 000 / 011 | Total loss: 1.612 | Reg loss: 0.039 | Tree loss: 1.612 | Accuracy: 0.404000 | 1.676 sec/iter\n",
      "Epoch: 169 | Batch: 001 / 011 | Total loss: 1.591 | Reg loss: 0.039 | Tree loss: 1.591 | Accuracy: 0.413500 | 1.676 sec/iter\n",
      "Epoch: 169 | Batch: 002 / 011 | Total loss: 1.573 | Reg loss: 0.039 | Tree loss: 1.573 | Accuracy: 0.424000 | 1.676 sec/iter\n",
      "Epoch: 169 | Batch: 003 / 011 | Total loss: 1.517 | Reg loss: 0.039 | Tree loss: 1.517 | Accuracy: 0.464500 | 1.675 sec/iter\n",
      "Epoch: 169 | Batch: 004 / 011 | Total loss: 1.492 | Reg loss: 0.039 | Tree loss: 1.492 | Accuracy: 0.478000 | 1.675 sec/iter\n",
      "Epoch: 169 | Batch: 005 / 011 | Total loss: 1.471 | Reg loss: 0.039 | Tree loss: 1.471 | Accuracy: 0.480500 | 1.675 sec/iter\n",
      "Epoch: 169 | Batch: 006 / 011 | Total loss: 1.435 | Reg loss: 0.039 | Tree loss: 1.435 | Accuracy: 0.484000 | 1.675 sec/iter\n",
      "Epoch: 169 | Batch: 007 / 011 | Total loss: 1.422 | Reg loss: 0.039 | Tree loss: 1.422 | Accuracy: 0.494500 | 1.675 sec/iter\n",
      "Epoch: 169 | Batch: 008 / 011 | Total loss: 1.440 | Reg loss: 0.039 | Tree loss: 1.440 | Accuracy: 0.477000 | 1.675 sec/iter\n",
      "Epoch: 169 | Batch: 009 / 011 | Total loss: 1.420 | Reg loss: 0.039 | Tree loss: 1.420 | Accuracy: 0.504000 | 1.675 sec/iter\n",
      "Epoch: 169 | Batch: 010 / 011 | Total loss: 1.480 | Reg loss: 0.039 | Tree loss: 1.480 | Accuracy: 0.474403 | 1.674 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 170 | Batch: 000 / 011 | Total loss: 1.651 | Reg loss: 0.039 | Tree loss: 1.651 | Accuracy: 0.400500 | 1.678 sec/iter\n",
      "Epoch: 170 | Batch: 001 / 011 | Total loss: 1.603 | Reg loss: 0.039 | Tree loss: 1.603 | Accuracy: 0.411500 | 1.678 sec/iter\n",
      "Epoch: 170 | Batch: 002 / 011 | Total loss: 1.543 | Reg loss: 0.039 | Tree loss: 1.543 | Accuracy: 0.450500 | 1.678 sec/iter\n",
      "Epoch: 170 | Batch: 003 / 011 | Total loss: 1.496 | Reg loss: 0.039 | Tree loss: 1.496 | Accuracy: 0.472500 | 1.678 sec/iter\n",
      "Epoch: 170 | Batch: 004 / 011 | Total loss: 1.470 | Reg loss: 0.039 | Tree loss: 1.470 | Accuracy: 0.483000 | 1.678 sec/iter\n",
      "Epoch: 170 | Batch: 005 / 011 | Total loss: 1.475 | Reg loss: 0.039 | Tree loss: 1.475 | Accuracy: 0.498000 | 1.678 sec/iter\n",
      "Epoch: 170 | Batch: 006 / 011 | Total loss: 1.452 | Reg loss: 0.039 | Tree loss: 1.452 | Accuracy: 0.493500 | 1.678 sec/iter\n",
      "Epoch: 170 | Batch: 007 / 011 | Total loss: 1.428 | Reg loss: 0.039 | Tree loss: 1.428 | Accuracy: 0.497500 | 1.678 sec/iter\n",
      "Epoch: 170 | Batch: 008 / 011 | Total loss: 1.437 | Reg loss: 0.039 | Tree loss: 1.437 | Accuracy: 0.485000 | 1.678 sec/iter\n",
      "Epoch: 170 | Batch: 009 / 011 | Total loss: 1.417 | Reg loss: 0.039 | Tree loss: 1.417 | Accuracy: 0.496000 | 1.678 sec/iter\n",
      "Epoch: 170 | Batch: 010 / 011 | Total loss: 1.510 | Reg loss: 0.039 | Tree loss: 1.510 | Accuracy: 0.470990 | 1.677 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 171 | Batch: 000 / 011 | Total loss: 1.637 | Reg loss: 0.039 | Tree loss: 1.637 | Accuracy: 0.403000 | 1.677 sec/iter\n",
      "Epoch: 171 | Batch: 001 / 011 | Total loss: 1.599 | Reg loss: 0.039 | Tree loss: 1.599 | Accuracy: 0.408000 | 1.677 sec/iter\n",
      "Epoch: 171 | Batch: 002 / 011 | Total loss: 1.582 | Reg loss: 0.039 | Tree loss: 1.582 | Accuracy: 0.423000 | 1.677 sec/iter\n",
      "Epoch: 171 | Batch: 003 / 011 | Total loss: 1.524 | Reg loss: 0.039 | Tree loss: 1.524 | Accuracy: 0.451000 | 1.677 sec/iter\n",
      "Epoch: 171 | Batch: 004 / 011 | Total loss: 1.478 | Reg loss: 0.039 | Tree loss: 1.478 | Accuracy: 0.492000 | 1.677 sec/iter\n",
      "Epoch: 171 | Batch: 005 / 011 | Total loss: 1.439 | Reg loss: 0.039 | Tree loss: 1.439 | Accuracy: 0.505000 | 1.677 sec/iter\n",
      "Epoch: 171 | Batch: 006 / 011 | Total loss: 1.457 | Reg loss: 0.039 | Tree loss: 1.457 | Accuracy: 0.480500 | 1.677 sec/iter\n",
      "Epoch: 171 | Batch: 007 / 011 | Total loss: 1.429 | Reg loss: 0.039 | Tree loss: 1.429 | Accuracy: 0.489500 | 1.677 sec/iter\n",
      "Epoch: 171 | Batch: 008 / 011 | Total loss: 1.416 | Reg loss: 0.039 | Tree loss: 1.416 | Accuracy: 0.500500 | 1.677 sec/iter\n",
      "Epoch: 171 | Batch: 009 / 011 | Total loss: 1.431 | Reg loss: 0.039 | Tree loss: 1.431 | Accuracy: 0.484500 | 1.676 sec/iter\n",
      "Epoch: 171 | Batch: 010 / 011 | Total loss: 1.408 | Reg loss: 0.039 | Tree loss: 1.408 | Accuracy: 0.484642 | 1.676 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 172 | Batch: 000 / 011 | Total loss: 1.628 | Reg loss: 0.039 | Tree loss: 1.628 | Accuracy: 0.400000 | 1.676 sec/iter\n",
      "Epoch: 172 | Batch: 001 / 011 | Total loss: 1.602 | Reg loss: 0.039 | Tree loss: 1.602 | Accuracy: 0.409500 | 1.676 sec/iter\n",
      "Epoch: 172 | Batch: 002 / 011 | Total loss: 1.572 | Reg loss: 0.039 | Tree loss: 1.572 | Accuracy: 0.426500 | 1.676 sec/iter\n",
      "Epoch: 172 | Batch: 003 / 011 | Total loss: 1.507 | Reg loss: 0.039 | Tree loss: 1.507 | Accuracy: 0.486000 | 1.675 sec/iter\n",
      "Epoch: 172 | Batch: 004 / 011 | Total loss: 1.486 | Reg loss: 0.039 | Tree loss: 1.486 | Accuracy: 0.479500 | 1.675 sec/iter\n",
      "Epoch: 172 | Batch: 005 / 011 | Total loss: 1.460 | Reg loss: 0.039 | Tree loss: 1.460 | Accuracy: 0.491500 | 1.675 sec/iter\n",
      "Epoch: 172 | Batch: 006 / 011 | Total loss: 1.435 | Reg loss: 0.039 | Tree loss: 1.435 | Accuracy: 0.495000 | 1.674 sec/iter\n",
      "Epoch: 172 | Batch: 007 / 011 | Total loss: 1.427 | Reg loss: 0.039 | Tree loss: 1.427 | Accuracy: 0.486000 | 1.674 sec/iter\n",
      "Epoch: 172 | Batch: 008 / 011 | Total loss: 1.414 | Reg loss: 0.039 | Tree loss: 1.414 | Accuracy: 0.489000 | 1.674 sec/iter\n",
      "Epoch: 172 | Batch: 009 / 011 | Total loss: 1.431 | Reg loss: 0.039 | Tree loss: 1.431 | Accuracy: 0.496000 | 1.674 sec/iter\n",
      "Epoch: 172 | Batch: 010 / 011 | Total loss: 1.442 | Reg loss: 0.039 | Tree loss: 1.442 | Accuracy: 0.474403 | 1.673 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 173 | Batch: 000 / 011 | Total loss: 1.636 | Reg loss: 0.039 | Tree loss: 1.636 | Accuracy: 0.385500 | 1.676 sec/iter\n",
      "Epoch: 173 | Batch: 001 / 011 | Total loss: 1.605 | Reg loss: 0.039 | Tree loss: 1.605 | Accuracy: 0.424000 | 1.676 sec/iter\n",
      "Epoch: 173 | Batch: 002 / 011 | Total loss: 1.542 | Reg loss: 0.039 | Tree loss: 1.542 | Accuracy: 0.458500 | 1.676 sec/iter\n",
      "Epoch: 173 | Batch: 003 / 011 | Total loss: 1.510 | Reg loss: 0.039 | Tree loss: 1.510 | Accuracy: 0.477000 | 1.676 sec/iter\n",
      "Epoch: 173 | Batch: 004 / 011 | Total loss: 1.478 | Reg loss: 0.039 | Tree loss: 1.478 | Accuracy: 0.474500 | 1.676 sec/iter\n",
      "Epoch: 173 | Batch: 005 / 011 | Total loss: 1.455 | Reg loss: 0.039 | Tree loss: 1.455 | Accuracy: 0.482500 | 1.676 sec/iter\n",
      "Epoch: 173 | Batch: 006 / 011 | Total loss: 1.446 | Reg loss: 0.039 | Tree loss: 1.446 | Accuracy: 0.494000 | 1.676 sec/iter\n",
      "Epoch: 173 | Batch: 007 / 011 | Total loss: 1.422 | Reg loss: 0.039 | Tree loss: 1.422 | Accuracy: 0.507000 | 1.676 sec/iter\n",
      "Epoch: 173 | Batch: 008 / 011 | Total loss: 1.443 | Reg loss: 0.039 | Tree loss: 1.443 | Accuracy: 0.482000 | 1.676 sec/iter\n",
      "Epoch: 173 | Batch: 009 / 011 | Total loss: 1.429 | Reg loss: 0.039 | Tree loss: 1.429 | Accuracy: 0.489000 | 1.676 sec/iter\n",
      "Epoch: 173 | Batch: 010 / 011 | Total loss: 1.440 | Reg loss: 0.039 | Tree loss: 1.440 | Accuracy: 0.484642 | 1.675 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 174 | Batch: 000 / 011 | Total loss: 1.624 | Reg loss: 0.039 | Tree loss: 1.624 | Accuracy: 0.398500 | 1.675 sec/iter\n",
      "Epoch: 174 | Batch: 001 / 011 | Total loss: 1.583 | Reg loss: 0.039 | Tree loss: 1.583 | Accuracy: 0.416000 | 1.675 sec/iter\n",
      "Epoch: 174 | Batch: 002 / 011 | Total loss: 1.576 | Reg loss: 0.039 | Tree loss: 1.576 | Accuracy: 0.424000 | 1.675 sec/iter\n",
      "Epoch: 174 | Batch: 003 / 011 | Total loss: 1.527 | Reg loss: 0.039 | Tree loss: 1.527 | Accuracy: 0.449000 | 1.675 sec/iter\n",
      "Epoch: 174 | Batch: 004 / 011 | Total loss: 1.487 | Reg loss: 0.039 | Tree loss: 1.487 | Accuracy: 0.475500 | 1.675 sec/iter\n",
      "Epoch: 174 | Batch: 005 / 011 | Total loss: 1.475 | Reg loss: 0.039 | Tree loss: 1.475 | Accuracy: 0.478500 | 1.675 sec/iter\n",
      "Epoch: 174 | Batch: 006 / 011 | Total loss: 1.427 | Reg loss: 0.039 | Tree loss: 1.427 | Accuracy: 0.502000 | 1.675 sec/iter\n",
      "Epoch: 174 | Batch: 007 / 011 | Total loss: 1.430 | Reg loss: 0.039 | Tree loss: 1.430 | Accuracy: 0.478500 | 1.675 sec/iter\n",
      "Epoch: 174 | Batch: 008 / 011 | Total loss: 1.440 | Reg loss: 0.039 | Tree loss: 1.440 | Accuracy: 0.473500 | 1.675 sec/iter\n",
      "Epoch: 174 | Batch: 009 / 011 | Total loss: 1.415 | Reg loss: 0.039 | Tree loss: 1.415 | Accuracy: 0.507500 | 1.674 sec/iter\n",
      "Epoch: 174 | Batch: 010 / 011 | Total loss: 1.411 | Reg loss: 0.039 | Tree loss: 1.411 | Accuracy: 0.477816 | 1.674 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 175 | Batch: 000 / 011 | Total loss: 1.644 | Reg loss: 0.039 | Tree loss: 1.644 | Accuracy: 0.382500 | 1.678 sec/iter\n",
      "Epoch: 175 | Batch: 001 / 011 | Total loss: 1.556 | Reg loss: 0.039 | Tree loss: 1.556 | Accuracy: 0.432000 | 1.678 sec/iter\n",
      "Epoch: 175 | Batch: 002 / 011 | Total loss: 1.549 | Reg loss: 0.039 | Tree loss: 1.549 | Accuracy: 0.439500 | 1.678 sec/iter\n",
      "Epoch: 175 | Batch: 003 / 011 | Total loss: 1.543 | Reg loss: 0.039 | Tree loss: 1.543 | Accuracy: 0.439000 | 1.677 sec/iter\n",
      "Epoch: 175 | Batch: 004 / 011 | Total loss: 1.472 | Reg loss: 0.039 | Tree loss: 1.472 | Accuracy: 0.487500 | 1.677 sec/iter\n",
      "Epoch: 175 | Batch: 005 / 011 | Total loss: 1.452 | Reg loss: 0.039 | Tree loss: 1.452 | Accuracy: 0.477000 | 1.677 sec/iter\n",
      "Epoch: 175 | Batch: 006 / 011 | Total loss: 1.446 | Reg loss: 0.039 | Tree loss: 1.446 | Accuracy: 0.489000 | 1.677 sec/iter\n",
      "Epoch: 175 | Batch: 007 / 011 | Total loss: 1.438 | Reg loss: 0.039 | Tree loss: 1.438 | Accuracy: 0.489500 | 1.677 sec/iter\n",
      "Epoch: 175 | Batch: 008 / 011 | Total loss: 1.447 | Reg loss: 0.039 | Tree loss: 1.447 | Accuracy: 0.491000 | 1.677 sec/iter\n",
      "Epoch: 175 | Batch: 009 / 011 | Total loss: 1.428 | Reg loss: 0.039 | Tree loss: 1.428 | Accuracy: 0.481500 | 1.677 sec/iter\n",
      "Epoch: 175 | Batch: 010 / 011 | Total loss: 1.400 | Reg loss: 0.039 | Tree loss: 1.400 | Accuracy: 0.494881 | 1.677 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 176 | Batch: 000 / 011 | Total loss: 1.622 | Reg loss: 0.039 | Tree loss: 1.622 | Accuracy: 0.392000 | 1.677 sec/iter\n",
      "Epoch: 176 | Batch: 001 / 011 | Total loss: 1.575 | Reg loss: 0.039 | Tree loss: 1.575 | Accuracy: 0.403000 | 1.677 sec/iter\n",
      "Epoch: 176 | Batch: 002 / 011 | Total loss: 1.578 | Reg loss: 0.039 | Tree loss: 1.578 | Accuracy: 0.423500 | 1.677 sec/iter\n",
      "Epoch: 176 | Batch: 003 / 011 | Total loss: 1.515 | Reg loss: 0.039 | Tree loss: 1.515 | Accuracy: 0.461000 | 1.676 sec/iter\n",
      "Epoch: 176 | Batch: 004 / 011 | Total loss: 1.484 | Reg loss: 0.039 | Tree loss: 1.484 | Accuracy: 0.465000 | 1.676 sec/iter\n",
      "Epoch: 176 | Batch: 005 / 011 | Total loss: 1.453 | Reg loss: 0.039 | Tree loss: 1.453 | Accuracy: 0.489000 | 1.676 sec/iter\n",
      "Epoch: 176 | Batch: 006 / 011 | Total loss: 1.431 | Reg loss: 0.039 | Tree loss: 1.431 | Accuracy: 0.516000 | 1.676 sec/iter\n",
      "Epoch: 176 | Batch: 007 / 011 | Total loss: 1.440 | Reg loss: 0.039 | Tree loss: 1.440 | Accuracy: 0.502500 | 1.676 sec/iter\n",
      "Epoch: 176 | Batch: 008 / 011 | Total loss: 1.421 | Reg loss: 0.039 | Tree loss: 1.421 | Accuracy: 0.495500 | 1.676 sec/iter\n",
      "Epoch: 176 | Batch: 009 / 011 | Total loss: 1.456 | Reg loss: 0.039 | Tree loss: 1.456 | Accuracy: 0.466000 | 1.676 sec/iter\n",
      "Epoch: 176 | Batch: 010 / 011 | Total loss: 1.408 | Reg loss: 0.039 | Tree loss: 1.408 | Accuracy: 0.522184 | 1.675 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 177 | Batch: 000 / 011 | Total loss: 1.634 | Reg loss: 0.039 | Tree loss: 1.634 | Accuracy: 0.386500 | 1.679 sec/iter\n",
      "Epoch: 177 | Batch: 001 / 011 | Total loss: 1.589 | Reg loss: 0.039 | Tree loss: 1.589 | Accuracy: 0.421500 | 1.679 sec/iter\n",
      "Epoch: 177 | Batch: 002 / 011 | Total loss: 1.551 | Reg loss: 0.039 | Tree loss: 1.551 | Accuracy: 0.449500 | 1.679 sec/iter\n",
      "Epoch: 177 | Batch: 003 / 011 | Total loss: 1.514 | Reg loss: 0.039 | Tree loss: 1.514 | Accuracy: 0.466000 | 1.679 sec/iter\n",
      "Epoch: 177 | Batch: 004 / 011 | Total loss: 1.465 | Reg loss: 0.039 | Tree loss: 1.465 | Accuracy: 0.486000 | 1.679 sec/iter\n",
      "Epoch: 177 | Batch: 005 / 011 | Total loss: 1.452 | Reg loss: 0.039 | Tree loss: 1.452 | Accuracy: 0.489500 | 1.679 sec/iter\n",
      "Epoch: 177 | Batch: 006 / 011 | Total loss: 1.459 | Reg loss: 0.039 | Tree loss: 1.459 | Accuracy: 0.486500 | 1.679 sec/iter\n",
      "Epoch: 177 | Batch: 007 / 011 | Total loss: 1.432 | Reg loss: 0.039 | Tree loss: 1.432 | Accuracy: 0.498500 | 1.679 sec/iter\n",
      "Epoch: 177 | Batch: 008 / 011 | Total loss: 1.410 | Reg loss: 0.039 | Tree loss: 1.410 | Accuracy: 0.494500 | 1.679 sec/iter\n",
      "Epoch: 177 | Batch: 009 / 011 | Total loss: 1.438 | Reg loss: 0.039 | Tree loss: 1.438 | Accuracy: 0.485500 | 1.679 sec/iter\n",
      "Epoch: 177 | Batch: 010 / 011 | Total loss: 1.405 | Reg loss: 0.039 | Tree loss: 1.405 | Accuracy: 0.488055 | 1.678 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 178 | Batch: 000 / 011 | Total loss: 1.609 | Reg loss: 0.039 | Tree loss: 1.609 | Accuracy: 0.410500 | 1.678 sec/iter\n",
      "Epoch: 178 | Batch: 001 / 011 | Total loss: 1.590 | Reg loss: 0.039 | Tree loss: 1.590 | Accuracy: 0.424000 | 1.678 sec/iter\n",
      "Epoch: 178 | Batch: 002 / 011 | Total loss: 1.553 | Reg loss: 0.039 | Tree loss: 1.553 | Accuracy: 0.430000 | 1.678 sec/iter\n",
      "Epoch: 178 | Batch: 003 / 011 | Total loss: 1.543 | Reg loss: 0.039 | Tree loss: 1.543 | Accuracy: 0.432500 | 1.678 sec/iter\n",
      "Epoch: 178 | Batch: 004 / 011 | Total loss: 1.486 | Reg loss: 0.039 | Tree loss: 1.486 | Accuracy: 0.465000 | 1.678 sec/iter\n",
      "Epoch: 178 | Batch: 005 / 011 | Total loss: 1.463 | Reg loss: 0.039 | Tree loss: 1.463 | Accuracy: 0.496500 | 1.678 sec/iter\n",
      "Epoch: 178 | Batch: 006 / 011 | Total loss: 1.431 | Reg loss: 0.039 | Tree loss: 1.431 | Accuracy: 0.506000 | 1.678 sec/iter\n",
      "Epoch: 178 | Batch: 007 / 011 | Total loss: 1.466 | Reg loss: 0.039 | Tree loss: 1.466 | Accuracy: 0.464500 | 1.678 sec/iter\n",
      "Epoch: 178 | Batch: 008 / 011 | Total loss: 1.413 | Reg loss: 0.039 | Tree loss: 1.413 | Accuracy: 0.506500 | 1.678 sec/iter\n",
      "Epoch: 178 | Batch: 009 / 011 | Total loss: 1.422 | Reg loss: 0.039 | Tree loss: 1.422 | Accuracy: 0.490500 | 1.677 sec/iter\n",
      "Epoch: 178 | Batch: 010 / 011 | Total loss: 1.451 | Reg loss: 0.039 | Tree loss: 1.451 | Accuracy: 0.501706 | 1.677 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 179 | Batch: 000 / 011 | Total loss: 1.625 | Reg loss: 0.039 | Tree loss: 1.625 | Accuracy: 0.409500 | 1.681 sec/iter\n",
      "Epoch: 179 | Batch: 001 / 011 | Total loss: 1.605 | Reg loss: 0.039 | Tree loss: 1.605 | Accuracy: 0.400000 | 1.681 sec/iter\n",
      "Epoch: 179 | Batch: 002 / 011 | Total loss: 1.570 | Reg loss: 0.039 | Tree loss: 1.570 | Accuracy: 0.435000 | 1.681 sec/iter\n",
      "Epoch: 179 | Batch: 003 / 011 | Total loss: 1.534 | Reg loss: 0.039 | Tree loss: 1.534 | Accuracy: 0.450500 | 1.68 sec/iter\n",
      "Epoch: 179 | Batch: 004 / 011 | Total loss: 1.488 | Reg loss: 0.039 | Tree loss: 1.488 | Accuracy: 0.472500 | 1.68 sec/iter\n",
      "Epoch: 179 | Batch: 005 / 011 | Total loss: 1.460 | Reg loss: 0.039 | Tree loss: 1.460 | Accuracy: 0.461500 | 1.68 sec/iter\n",
      "Epoch: 179 | Batch: 006 / 011 | Total loss: 1.420 | Reg loss: 0.039 | Tree loss: 1.420 | Accuracy: 0.509000 | 1.679 sec/iter\n",
      "Epoch: 179 | Batch: 007 / 011 | Total loss: 1.398 | Reg loss: 0.039 | Tree loss: 1.398 | Accuracy: 0.514000 | 1.679 sec/iter\n",
      "Epoch: 179 | Batch: 008 / 011 | Total loss: 1.438 | Reg loss: 0.039 | Tree loss: 1.438 | Accuracy: 0.474500 | 1.679 sec/iter\n",
      "Epoch: 179 | Batch: 009 / 011 | Total loss: 1.402 | Reg loss: 0.039 | Tree loss: 1.402 | Accuracy: 0.506000 | 1.678 sec/iter\n",
      "Epoch: 179 | Batch: 010 / 011 | Total loss: 1.441 | Reg loss: 0.039 | Tree loss: 1.441 | Accuracy: 0.474403 | 1.678 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 180 | Batch: 000 / 011 | Total loss: 1.648 | Reg loss: 0.039 | Tree loss: 1.648 | Accuracy: 0.392000 | 1.68 sec/iter\n",
      "Epoch: 180 | Batch: 001 / 011 | Total loss: 1.596 | Reg loss: 0.039 | Tree loss: 1.596 | Accuracy: 0.405500 | 1.68 sec/iter\n",
      "Epoch: 180 | Batch: 002 / 011 | Total loss: 1.544 | Reg loss: 0.039 | Tree loss: 1.544 | Accuracy: 0.457000 | 1.68 sec/iter\n",
      "Epoch: 180 | Batch: 003 / 011 | Total loss: 1.526 | Reg loss: 0.039 | Tree loss: 1.526 | Accuracy: 0.457500 | 1.68 sec/iter\n",
      "Epoch: 180 | Batch: 004 / 011 | Total loss: 1.465 | Reg loss: 0.039 | Tree loss: 1.465 | Accuracy: 0.506500 | 1.68 sec/iter\n",
      "Epoch: 180 | Batch: 005 / 011 | Total loss: 1.447 | Reg loss: 0.039 | Tree loss: 1.447 | Accuracy: 0.499500 | 1.68 sec/iter\n",
      "Epoch: 180 | Batch: 006 / 011 | Total loss: 1.465 | Reg loss: 0.039 | Tree loss: 1.465 | Accuracy: 0.482000 | 1.68 sec/iter\n",
      "Epoch: 180 | Batch: 007 / 011 | Total loss: 1.425 | Reg loss: 0.039 | Tree loss: 1.425 | Accuracy: 0.489000 | 1.68 sec/iter\n",
      "Epoch: 180 | Batch: 008 / 011 | Total loss: 1.413 | Reg loss: 0.039 | Tree loss: 1.413 | Accuracy: 0.508000 | 1.68 sec/iter\n",
      "Epoch: 180 | Batch: 009 / 011 | Total loss: 1.433 | Reg loss: 0.039 | Tree loss: 1.433 | Accuracy: 0.483500 | 1.68 sec/iter\n",
      "Epoch: 180 | Batch: 010 / 011 | Total loss: 1.281 | Reg loss: 0.039 | Tree loss: 1.281 | Accuracy: 0.566553 | 1.679 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 181 | Batch: 000 / 011 | Total loss: 1.638 | Reg loss: 0.039 | Tree loss: 1.638 | Accuracy: 0.379500 | 1.681 sec/iter\n",
      "Epoch: 181 | Batch: 001 / 011 | Total loss: 1.605 | Reg loss: 0.039 | Tree loss: 1.605 | Accuracy: 0.399000 | 1.68 sec/iter\n",
      "Epoch: 181 | Batch: 002 / 011 | Total loss: 1.534 | Reg loss: 0.039 | Tree loss: 1.534 | Accuracy: 0.463500 | 1.68 sec/iter\n",
      "Epoch: 181 | Batch: 003 / 011 | Total loss: 1.517 | Reg loss: 0.039 | Tree loss: 1.517 | Accuracy: 0.456500 | 1.68 sec/iter\n",
      "Epoch: 181 | Batch: 004 / 011 | Total loss: 1.481 | Reg loss: 0.039 | Tree loss: 1.481 | Accuracy: 0.482000 | 1.68 sec/iter\n",
      "Epoch: 181 | Batch: 005 / 011 | Total loss: 1.441 | Reg loss: 0.039 | Tree loss: 1.441 | Accuracy: 0.484500 | 1.68 sec/iter\n",
      "Epoch: 181 | Batch: 006 / 011 | Total loss: 1.442 | Reg loss: 0.039 | Tree loss: 1.442 | Accuracy: 0.493000 | 1.68 sec/iter\n",
      "Epoch: 181 | Batch: 007 / 011 | Total loss: 1.438 | Reg loss: 0.039 | Tree loss: 1.438 | Accuracy: 0.501000 | 1.68 sec/iter\n",
      "Epoch: 181 | Batch: 008 / 011 | Total loss: 1.424 | Reg loss: 0.039 | Tree loss: 1.424 | Accuracy: 0.504500 | 1.68 sec/iter\n",
      "Epoch: 181 | Batch: 009 / 011 | Total loss: 1.420 | Reg loss: 0.039 | Tree loss: 1.420 | Accuracy: 0.486000 | 1.68 sec/iter\n",
      "Epoch: 181 | Batch: 010 / 011 | Total loss: 1.406 | Reg loss: 0.039 | Tree loss: 1.406 | Accuracy: 0.484642 | 1.679 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 182 | Batch: 000 / 011 | Total loss: 1.622 | Reg loss: 0.039 | Tree loss: 1.622 | Accuracy: 0.401500 | 1.682 sec/iter\n",
      "Epoch: 182 | Batch: 001 / 011 | Total loss: 1.582 | Reg loss: 0.039 | Tree loss: 1.582 | Accuracy: 0.404000 | 1.682 sec/iter\n",
      "Epoch: 182 | Batch: 002 / 011 | Total loss: 1.549 | Reg loss: 0.039 | Tree loss: 1.549 | Accuracy: 0.439500 | 1.682 sec/iter\n",
      "Epoch: 182 | Batch: 003 / 011 | Total loss: 1.521 | Reg loss: 0.039 | Tree loss: 1.521 | Accuracy: 0.450000 | 1.682 sec/iter\n",
      "Epoch: 182 | Batch: 004 / 011 | Total loss: 1.484 | Reg loss: 0.039 | Tree loss: 1.484 | Accuracy: 0.483500 | 1.682 sec/iter\n",
      "Epoch: 182 | Batch: 005 / 011 | Total loss: 1.451 | Reg loss: 0.039 | Tree loss: 1.451 | Accuracy: 0.487000 | 1.682 sec/iter\n",
      "Epoch: 182 | Batch: 006 / 011 | Total loss: 1.443 | Reg loss: 0.039 | Tree loss: 1.443 | Accuracy: 0.495500 | 1.681 sec/iter\n",
      "Epoch: 182 | Batch: 007 / 011 | Total loss: 1.412 | Reg loss: 0.039 | Tree loss: 1.412 | Accuracy: 0.518000 | 1.681 sec/iter\n",
      "Epoch: 182 | Batch: 008 / 011 | Total loss: 1.429 | Reg loss: 0.039 | Tree loss: 1.429 | Accuracy: 0.502500 | 1.681 sec/iter\n",
      "Epoch: 182 | Batch: 009 / 011 | Total loss: 1.436 | Reg loss: 0.039 | Tree loss: 1.436 | Accuracy: 0.483500 | 1.681 sec/iter\n",
      "Epoch: 182 | Batch: 010 / 011 | Total loss: 1.464 | Reg loss: 0.039 | Tree loss: 1.464 | Accuracy: 0.443686 | 1.681 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 183 | Batch: 000 / 011 | Total loss: 1.619 | Reg loss: 0.039 | Tree loss: 1.619 | Accuracy: 0.401500 | 1.682 sec/iter\n",
      "Epoch: 183 | Batch: 001 / 011 | Total loss: 1.576 | Reg loss: 0.039 | Tree loss: 1.576 | Accuracy: 0.427500 | 1.682 sec/iter\n",
      "Epoch: 183 | Batch: 002 / 011 | Total loss: 1.538 | Reg loss: 0.039 | Tree loss: 1.538 | Accuracy: 0.456500 | 1.682 sec/iter\n",
      "Epoch: 183 | Batch: 003 / 011 | Total loss: 1.538 | Reg loss: 0.039 | Tree loss: 1.538 | Accuracy: 0.452000 | 1.682 sec/iter\n",
      "Epoch: 183 | Batch: 004 / 011 | Total loss: 1.514 | Reg loss: 0.039 | Tree loss: 1.514 | Accuracy: 0.460500 | 1.682 sec/iter\n",
      "Epoch: 183 | Batch: 005 / 011 | Total loss: 1.460 | Reg loss: 0.039 | Tree loss: 1.460 | Accuracy: 0.483500 | 1.681 sec/iter\n",
      "Epoch: 183 | Batch: 006 / 011 | Total loss: 1.418 | Reg loss: 0.039 | Tree loss: 1.418 | Accuracy: 0.508000 | 1.681 sec/iter\n",
      "Epoch: 183 | Batch: 007 / 011 | Total loss: 1.431 | Reg loss: 0.039 | Tree loss: 1.431 | Accuracy: 0.504000 | 1.681 sec/iter\n",
      "Epoch: 183 | Batch: 008 / 011 | Total loss: 1.437 | Reg loss: 0.039 | Tree loss: 1.437 | Accuracy: 0.476000 | 1.681 sec/iter\n",
      "Epoch: 183 | Batch: 009 / 011 | Total loss: 1.433 | Reg loss: 0.039 | Tree loss: 1.433 | Accuracy: 0.487500 | 1.681 sec/iter\n",
      "Epoch: 183 | Batch: 010 / 011 | Total loss: 1.391 | Reg loss: 0.039 | Tree loss: 1.391 | Accuracy: 0.549488 | 1.681 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 184 | Batch: 000 / 011 | Total loss: 1.630 | Reg loss: 0.039 | Tree loss: 1.630 | Accuracy: 0.401000 | 1.681 sec/iter\n",
      "Epoch: 184 | Batch: 001 / 011 | Total loss: 1.593 | Reg loss: 0.039 | Tree loss: 1.593 | Accuracy: 0.415000 | 1.681 sec/iter\n",
      "Epoch: 184 | Batch: 002 / 011 | Total loss: 1.578 | Reg loss: 0.039 | Tree loss: 1.578 | Accuracy: 0.407000 | 1.681 sec/iter\n",
      "Epoch: 184 | Batch: 003 / 011 | Total loss: 1.532 | Reg loss: 0.039 | Tree loss: 1.532 | Accuracy: 0.446500 | 1.68 sec/iter\n",
      "Epoch: 184 | Batch: 004 / 011 | Total loss: 1.451 | Reg loss: 0.039 | Tree loss: 1.451 | Accuracy: 0.484500 | 1.68 sec/iter\n",
      "Epoch: 184 | Batch: 005 / 011 | Total loss: 1.447 | Reg loss: 0.039 | Tree loss: 1.447 | Accuracy: 0.502500 | 1.68 sec/iter\n",
      "Epoch: 184 | Batch: 006 / 011 | Total loss: 1.439 | Reg loss: 0.039 | Tree loss: 1.439 | Accuracy: 0.498500 | 1.68 sec/iter\n",
      "Epoch: 184 | Batch: 007 / 011 | Total loss: 1.423 | Reg loss: 0.039 | Tree loss: 1.423 | Accuracy: 0.492000 | 1.68 sec/iter\n",
      "Epoch: 184 | Batch: 008 / 011 | Total loss: 1.428 | Reg loss: 0.039 | Tree loss: 1.428 | Accuracy: 0.493000 | 1.68 sec/iter\n",
      "Epoch: 184 | Batch: 009 / 011 | Total loss: 1.415 | Reg loss: 0.039 | Tree loss: 1.415 | Accuracy: 0.502500 | 1.68 sec/iter\n",
      "Epoch: 184 | Batch: 010 / 011 | Total loss: 1.436 | Reg loss: 0.039 | Tree loss: 1.436 | Accuracy: 0.494881 | 1.679 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 185 | Batch: 000 / 011 | Total loss: 1.608 | Reg loss: 0.039 | Tree loss: 1.608 | Accuracy: 0.411500 | 1.683 sec/iter\n",
      "Epoch: 185 | Batch: 001 / 011 | Total loss: 1.596 | Reg loss: 0.039 | Tree loss: 1.596 | Accuracy: 0.418500 | 1.683 sec/iter\n",
      "Epoch: 185 | Batch: 002 / 011 | Total loss: 1.558 | Reg loss: 0.039 | Tree loss: 1.558 | Accuracy: 0.435500 | 1.683 sec/iter\n",
      "Epoch: 185 | Batch: 003 / 011 | Total loss: 1.523 | Reg loss: 0.039 | Tree loss: 1.523 | Accuracy: 0.466000 | 1.683 sec/iter\n",
      "Epoch: 185 | Batch: 004 / 011 | Total loss: 1.470 | Reg loss: 0.039 | Tree loss: 1.470 | Accuracy: 0.482000 | 1.683 sec/iter\n",
      "Epoch: 185 | Batch: 005 / 011 | Total loss: 1.445 | Reg loss: 0.039 | Tree loss: 1.445 | Accuracy: 0.496000 | 1.683 sec/iter\n",
      "Epoch: 185 | Batch: 006 / 011 | Total loss: 1.437 | Reg loss: 0.039 | Tree loss: 1.437 | Accuracy: 0.490500 | 1.683 sec/iter\n",
      "Epoch: 185 | Batch: 007 / 011 | Total loss: 1.433 | Reg loss: 0.039 | Tree loss: 1.433 | Accuracy: 0.477500 | 1.683 sec/iter\n",
      "Epoch: 185 | Batch: 008 / 011 | Total loss: 1.423 | Reg loss: 0.039 | Tree loss: 1.423 | Accuracy: 0.479500 | 1.682 sec/iter\n",
      "Epoch: 185 | Batch: 009 / 011 | Total loss: 1.440 | Reg loss: 0.039 | Tree loss: 1.440 | Accuracy: 0.488500 | 1.682 sec/iter\n",
      "Epoch: 185 | Batch: 010 / 011 | Total loss: 1.445 | Reg loss: 0.039 | Tree loss: 1.445 | Accuracy: 0.464164 | 1.682 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 186 | Batch: 000 / 011 | Total loss: 1.624 | Reg loss: 0.039 | Tree loss: 1.624 | Accuracy: 0.396500 | 1.682 sec/iter\n",
      "Epoch: 186 | Batch: 001 / 011 | Total loss: 1.594 | Reg loss: 0.039 | Tree loss: 1.594 | Accuracy: 0.404500 | 1.682 sec/iter\n",
      "Epoch: 186 | Batch: 002 / 011 | Total loss: 1.557 | Reg loss: 0.039 | Tree loss: 1.557 | Accuracy: 0.435500 | 1.682 sec/iter\n",
      "Epoch: 186 | Batch: 003 / 011 | Total loss: 1.510 | Reg loss: 0.039 | Tree loss: 1.510 | Accuracy: 0.448000 | 1.682 sec/iter\n",
      "Epoch: 186 | Batch: 004 / 011 | Total loss: 1.495 | Reg loss: 0.039 | Tree loss: 1.495 | Accuracy: 0.457000 | 1.682 sec/iter\n",
      "Epoch: 186 | Batch: 005 / 011 | Total loss: 1.460 | Reg loss: 0.039 | Tree loss: 1.460 | Accuracy: 0.480000 | 1.682 sec/iter\n",
      "Epoch: 186 | Batch: 006 / 011 | Total loss: 1.438 | Reg loss: 0.039 | Tree loss: 1.438 | Accuracy: 0.488000 | 1.682 sec/iter\n",
      "Epoch: 186 | Batch: 007 / 011 | Total loss: 1.405 | Reg loss: 0.039 | Tree loss: 1.405 | Accuracy: 0.501000 | 1.682 sec/iter\n",
      "Epoch: 186 | Batch: 008 / 011 | Total loss: 1.436 | Reg loss: 0.039 | Tree loss: 1.436 | Accuracy: 0.483000 | 1.682 sec/iter\n",
      "Epoch: 186 | Batch: 009 / 011 | Total loss: 1.420 | Reg loss: 0.039 | Tree loss: 1.420 | Accuracy: 0.496000 | 1.681 sec/iter\n",
      "Epoch: 186 | Batch: 010 / 011 | Total loss: 1.442 | Reg loss: 0.039 | Tree loss: 1.442 | Accuracy: 0.488055 | 1.681 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 187 | Batch: 000 / 011 | Total loss: 1.621 | Reg loss: 0.039 | Tree loss: 1.621 | Accuracy: 0.404000 | 1.682 sec/iter\n",
      "Epoch: 187 | Batch: 001 / 011 | Total loss: 1.602 | Reg loss: 0.039 | Tree loss: 1.602 | Accuracy: 0.413000 | 1.681 sec/iter\n",
      "Epoch: 187 | Batch: 002 / 011 | Total loss: 1.571 | Reg loss: 0.039 | Tree loss: 1.571 | Accuracy: 0.427500 | 1.681 sec/iter\n",
      "Epoch: 187 | Batch: 003 / 011 | Total loss: 1.523 | Reg loss: 0.039 | Tree loss: 1.523 | Accuracy: 0.446500 | 1.681 sec/iter\n",
      "Epoch: 187 | Batch: 004 / 011 | Total loss: 1.467 | Reg loss: 0.039 | Tree loss: 1.467 | Accuracy: 0.477000 | 1.68 sec/iter\n",
      "Epoch: 187 | Batch: 005 / 011 | Total loss: 1.467 | Reg loss: 0.039 | Tree loss: 1.467 | Accuracy: 0.488000 | 1.68 sec/iter\n",
      "Epoch: 187 | Batch: 006 / 011 | Total loss: 1.436 | Reg loss: 0.039 | Tree loss: 1.436 | Accuracy: 0.500500 | 1.68 sec/iter\n",
      "Epoch: 187 | Batch: 007 / 011 | Total loss: 1.416 | Reg loss: 0.039 | Tree loss: 1.416 | Accuracy: 0.482000 | 1.679 sec/iter\n",
      "Epoch: 187 | Batch: 008 / 011 | Total loss: 1.401 | Reg loss: 0.039 | Tree loss: 1.401 | Accuracy: 0.507500 | 1.679 sec/iter\n",
      "Epoch: 187 | Batch: 009 / 011 | Total loss: 1.427 | Reg loss: 0.039 | Tree loss: 1.427 | Accuracy: 0.486000 | 1.679 sec/iter\n",
      "Epoch: 187 | Batch: 010 / 011 | Total loss: 1.414 | Reg loss: 0.039 | Tree loss: 1.414 | Accuracy: 0.481229 | 1.679 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 188 | Batch: 000 / 011 | Total loss: 1.628 | Reg loss: 0.039 | Tree loss: 1.628 | Accuracy: 0.389500 | 1.682 sec/iter\n",
      "Epoch: 188 | Batch: 001 / 011 | Total loss: 1.607 | Reg loss: 0.039 | Tree loss: 1.607 | Accuracy: 0.403000 | 1.681 sec/iter\n",
      "Epoch: 188 | Batch: 002 / 011 | Total loss: 1.579 | Reg loss: 0.039 | Tree loss: 1.579 | Accuracy: 0.401000 | 1.681 sec/iter\n",
      "Epoch: 188 | Batch: 003 / 011 | Total loss: 1.525 | Reg loss: 0.039 | Tree loss: 1.525 | Accuracy: 0.439000 | 1.681 sec/iter\n",
      "Epoch: 188 | Batch: 004 / 011 | Total loss: 1.478 | Reg loss: 0.039 | Tree loss: 1.478 | Accuracy: 0.472000 | 1.681 sec/iter\n",
      "Epoch: 188 | Batch: 005 / 011 | Total loss: 1.427 | Reg loss: 0.039 | Tree loss: 1.427 | Accuracy: 0.484500 | 1.681 sec/iter\n",
      "Epoch: 188 | Batch: 006 / 011 | Total loss: 1.412 | Reg loss: 0.039 | Tree loss: 1.412 | Accuracy: 0.508500 | 1.681 sec/iter\n",
      "Epoch: 188 | Batch: 007 / 011 | Total loss: 1.417 | Reg loss: 0.039 | Tree loss: 1.417 | Accuracy: 0.498500 | 1.681 sec/iter\n",
      "Epoch: 188 | Batch: 008 / 011 | Total loss: 1.431 | Reg loss: 0.039 | Tree loss: 1.431 | Accuracy: 0.478000 | 1.681 sec/iter\n",
      "Epoch: 188 | Batch: 009 / 011 | Total loss: 1.458 | Reg loss: 0.039 | Tree loss: 1.458 | Accuracy: 0.466000 | 1.681 sec/iter\n",
      "Epoch: 188 | Batch: 010 / 011 | Total loss: 1.408 | Reg loss: 0.039 | Tree loss: 1.408 | Accuracy: 0.467577 | 1.68 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 189 | Batch: 000 / 011 | Total loss: 1.619 | Reg loss: 0.039 | Tree loss: 1.619 | Accuracy: 0.399000 | 1.682 sec/iter\n",
      "Epoch: 189 | Batch: 001 / 011 | Total loss: 1.603 | Reg loss: 0.039 | Tree loss: 1.603 | Accuracy: 0.393000 | 1.682 sec/iter\n",
      "Epoch: 189 | Batch: 002 / 011 | Total loss: 1.537 | Reg loss: 0.039 | Tree loss: 1.537 | Accuracy: 0.426500 | 1.682 sec/iter\n",
      "Epoch: 189 | Batch: 003 / 011 | Total loss: 1.501 | Reg loss: 0.039 | Tree loss: 1.501 | Accuracy: 0.449500 | 1.682 sec/iter\n",
      "Epoch: 189 | Batch: 004 / 011 | Total loss: 1.493 | Reg loss: 0.039 | Tree loss: 1.493 | Accuracy: 0.481500 | 1.682 sec/iter\n",
      "Epoch: 189 | Batch: 005 / 011 | Total loss: 1.451 | Reg loss: 0.039 | Tree loss: 1.451 | Accuracy: 0.496500 | 1.682 sec/iter\n",
      "Epoch: 189 | Batch: 006 / 011 | Total loss: 1.469 | Reg loss: 0.039 | Tree loss: 1.469 | Accuracy: 0.476000 | 1.681 sec/iter\n",
      "Epoch: 189 | Batch: 007 / 011 | Total loss: 1.424 | Reg loss: 0.039 | Tree loss: 1.424 | Accuracy: 0.481000 | 1.681 sec/iter\n",
      "Epoch: 189 | Batch: 008 / 011 | Total loss: 1.426 | Reg loss: 0.039 | Tree loss: 1.426 | Accuracy: 0.489000 | 1.681 sec/iter\n",
      "Epoch: 189 | Batch: 009 / 011 | Total loss: 1.406 | Reg loss: 0.039 | Tree loss: 1.406 | Accuracy: 0.501500 | 1.681 sec/iter\n",
      "Epoch: 189 | Batch: 010 / 011 | Total loss: 1.340 | Reg loss: 0.039 | Tree loss: 1.340 | Accuracy: 0.559727 | 1.681 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 190 | Batch: 000 / 011 | Total loss: 1.632 | Reg loss: 0.039 | Tree loss: 1.632 | Accuracy: 0.391000 | 1.683 sec/iter\n",
      "Epoch: 190 | Batch: 001 / 011 | Total loss: 1.581 | Reg loss: 0.039 | Tree loss: 1.581 | Accuracy: 0.447500 | 1.683 sec/iter\n",
      "Epoch: 190 | Batch: 002 / 011 | Total loss: 1.517 | Reg loss: 0.039 | Tree loss: 1.517 | Accuracy: 0.438500 | 1.683 sec/iter\n",
      "Epoch: 190 | Batch: 003 / 011 | Total loss: 1.499 | Reg loss: 0.039 | Tree loss: 1.499 | Accuracy: 0.467000 | 1.682 sec/iter\n",
      "Epoch: 190 | Batch: 004 / 011 | Total loss: 1.447 | Reg loss: 0.039 | Tree loss: 1.447 | Accuracy: 0.492000 | 1.682 sec/iter\n",
      "Epoch: 190 | Batch: 005 / 011 | Total loss: 1.487 | Reg loss: 0.039 | Tree loss: 1.487 | Accuracy: 0.449000 | 1.682 sec/iter\n",
      "Epoch: 190 | Batch: 006 / 011 | Total loss: 1.441 | Reg loss: 0.039 | Tree loss: 1.441 | Accuracy: 0.480000 | 1.682 sec/iter\n",
      "Epoch: 190 | Batch: 007 / 011 | Total loss: 1.430 | Reg loss: 0.039 | Tree loss: 1.430 | Accuracy: 0.470500 | 1.682 sec/iter\n",
      "Epoch: 190 | Batch: 008 / 011 | Total loss: 1.433 | Reg loss: 0.039 | Tree loss: 1.433 | Accuracy: 0.480500 | 1.682 sec/iter\n",
      "Epoch: 190 | Batch: 009 / 011 | Total loss: 1.452 | Reg loss: 0.039 | Tree loss: 1.452 | Accuracy: 0.473000 | 1.682 sec/iter\n",
      "Epoch: 190 | Batch: 010 / 011 | Total loss: 1.390 | Reg loss: 0.039 | Tree loss: 1.390 | Accuracy: 0.525597 | 1.682 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 191 | Batch: 000 / 011 | Total loss: 1.626 | Reg loss: 0.039 | Tree loss: 1.626 | Accuracy: 0.407000 | 1.682 sec/iter\n",
      "Epoch: 191 | Batch: 001 / 011 | Total loss: 1.616 | Reg loss: 0.039 | Tree loss: 1.616 | Accuracy: 0.396000 | 1.682 sec/iter\n",
      "Epoch: 191 | Batch: 002 / 011 | Total loss: 1.541 | Reg loss: 0.039 | Tree loss: 1.541 | Accuracy: 0.458500 | 1.682 sec/iter\n",
      "Epoch: 191 | Batch: 003 / 011 | Total loss: 1.503 | Reg loss: 0.039 | Tree loss: 1.503 | Accuracy: 0.464000 | 1.682 sec/iter\n",
      "Epoch: 191 | Batch: 004 / 011 | Total loss: 1.461 | Reg loss: 0.039 | Tree loss: 1.461 | Accuracy: 0.475000 | 1.682 sec/iter\n",
      "Epoch: 191 | Batch: 005 / 011 | Total loss: 1.460 | Reg loss: 0.039 | Tree loss: 1.460 | Accuracy: 0.487000 | 1.682 sec/iter\n",
      "Epoch: 191 | Batch: 006 / 011 | Total loss: 1.430 | Reg loss: 0.039 | Tree loss: 1.430 | Accuracy: 0.495000 | 1.681 sec/iter\n",
      "Epoch: 191 | Batch: 007 / 011 | Total loss: 1.424 | Reg loss: 0.039 | Tree loss: 1.424 | Accuracy: 0.485000 | 1.681 sec/iter\n",
      "Epoch: 191 | Batch: 008 / 011 | Total loss: 1.432 | Reg loss: 0.039 | Tree loss: 1.432 | Accuracy: 0.483500 | 1.681 sec/iter\n",
      "Epoch: 191 | Batch: 009 / 011 | Total loss: 1.428 | Reg loss: 0.039 | Tree loss: 1.428 | Accuracy: 0.491500 | 1.681 sec/iter\n",
      "Epoch: 191 | Batch: 010 / 011 | Total loss: 1.418 | Reg loss: 0.039 | Tree loss: 1.418 | Accuracy: 0.529010 | 1.681 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 192 | Batch: 000 / 011 | Total loss: 1.615 | Reg loss: 0.039 | Tree loss: 1.615 | Accuracy: 0.414000 | 1.684 sec/iter\n",
      "Epoch: 192 | Batch: 001 / 011 | Total loss: 1.595 | Reg loss: 0.039 | Tree loss: 1.595 | Accuracy: 0.410500 | 1.684 sec/iter\n",
      "Epoch: 192 | Batch: 002 / 011 | Total loss: 1.562 | Reg loss: 0.039 | Tree loss: 1.562 | Accuracy: 0.437500 | 1.684 sec/iter\n",
      "Epoch: 192 | Batch: 003 / 011 | Total loss: 1.511 | Reg loss: 0.039 | Tree loss: 1.511 | Accuracy: 0.473500 | 1.684 sec/iter\n",
      "Epoch: 192 | Batch: 004 / 011 | Total loss: 1.478 | Reg loss: 0.039 | Tree loss: 1.478 | Accuracy: 0.493000 | 1.684 sec/iter\n",
      "Epoch: 192 | Batch: 005 / 011 | Total loss: 1.459 | Reg loss: 0.039 | Tree loss: 1.459 | Accuracy: 0.499000 | 1.683 sec/iter\n",
      "Epoch: 192 | Batch: 006 / 011 | Total loss: 1.411 | Reg loss: 0.039 | Tree loss: 1.411 | Accuracy: 0.526500 | 1.683 sec/iter\n",
      "Epoch: 192 | Batch: 007 / 011 | Total loss: 1.421 | Reg loss: 0.039 | Tree loss: 1.421 | Accuracy: 0.487500 | 1.683 sec/iter\n",
      "Epoch: 192 | Batch: 008 / 011 | Total loss: 1.440 | Reg loss: 0.039 | Tree loss: 1.440 | Accuracy: 0.480000 | 1.683 sec/iter\n",
      "Epoch: 192 | Batch: 009 / 011 | Total loss: 1.420 | Reg loss: 0.039 | Tree loss: 1.420 | Accuracy: 0.502500 | 1.683 sec/iter\n",
      "Epoch: 192 | Batch: 010 / 011 | Total loss: 1.428 | Reg loss: 0.039 | Tree loss: 1.428 | Accuracy: 0.481229 | 1.683 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 193 | Batch: 000 / 011 | Total loss: 1.619 | Reg loss: 0.039 | Tree loss: 1.619 | Accuracy: 0.411500 | 1.683 sec/iter\n",
      "Epoch: 193 | Batch: 001 / 011 | Total loss: 1.585 | Reg loss: 0.039 | Tree loss: 1.585 | Accuracy: 0.411500 | 1.683 sec/iter\n",
      "Epoch: 193 | Batch: 002 / 011 | Total loss: 1.531 | Reg loss: 0.039 | Tree loss: 1.531 | Accuracy: 0.434000 | 1.683 sec/iter\n",
      "Epoch: 193 | Batch: 003 / 011 | Total loss: 1.534 | Reg loss: 0.039 | Tree loss: 1.534 | Accuracy: 0.449500 | 1.683 sec/iter\n",
      "Epoch: 193 | Batch: 004 / 011 | Total loss: 1.492 | Reg loss: 0.039 | Tree loss: 1.492 | Accuracy: 0.465000 | 1.683 sec/iter\n",
      "Epoch: 193 | Batch: 005 / 011 | Total loss: 1.452 | Reg loss: 0.039 | Tree loss: 1.452 | Accuracy: 0.487500 | 1.683 sec/iter\n",
      "Epoch: 193 | Batch: 006 / 011 | Total loss: 1.429 | Reg loss: 0.039 | Tree loss: 1.429 | Accuracy: 0.486500 | 1.682 sec/iter\n",
      "Epoch: 193 | Batch: 007 / 011 | Total loss: 1.428 | Reg loss: 0.039 | Tree loss: 1.428 | Accuracy: 0.488000 | 1.682 sec/iter\n",
      "Epoch: 193 | Batch: 008 / 011 | Total loss: 1.409 | Reg loss: 0.039 | Tree loss: 1.409 | Accuracy: 0.490000 | 1.682 sec/iter\n",
      "Epoch: 193 | Batch: 009 / 011 | Total loss: 1.432 | Reg loss: 0.039 | Tree loss: 1.432 | Accuracy: 0.481000 | 1.682 sec/iter\n",
      "Epoch: 193 | Batch: 010 / 011 | Total loss: 1.443 | Reg loss: 0.039 | Tree loss: 1.443 | Accuracy: 0.436860 | 1.682 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 194 | Batch: 000 / 011 | Total loss: 1.614 | Reg loss: 0.039 | Tree loss: 1.614 | Accuracy: 0.388500 | 1.685 sec/iter\n",
      "Epoch: 194 | Batch: 001 / 011 | Total loss: 1.595 | Reg loss: 0.039 | Tree loss: 1.595 | Accuracy: 0.412000 | 1.685 sec/iter\n",
      "Epoch: 194 | Batch: 002 / 011 | Total loss: 1.567 | Reg loss: 0.039 | Tree loss: 1.567 | Accuracy: 0.428500 | 1.685 sec/iter\n",
      "Epoch: 194 | Batch: 003 / 011 | Total loss: 1.519 | Reg loss: 0.039 | Tree loss: 1.519 | Accuracy: 0.451500 | 1.685 sec/iter\n",
      "Epoch: 194 | Batch: 004 / 011 | Total loss: 1.471 | Reg loss: 0.039 | Tree loss: 1.471 | Accuracy: 0.480500 | 1.685 sec/iter\n",
      "Epoch: 194 | Batch: 005 / 011 | Total loss: 1.457 | Reg loss: 0.039 | Tree loss: 1.457 | Accuracy: 0.493500 | 1.684 sec/iter\n",
      "Epoch: 194 | Batch: 006 / 011 | Total loss: 1.442 | Reg loss: 0.039 | Tree loss: 1.442 | Accuracy: 0.481000 | 1.684 sec/iter\n",
      "Epoch: 194 | Batch: 007 / 011 | Total loss: 1.402 | Reg loss: 0.039 | Tree loss: 1.402 | Accuracy: 0.517500 | 1.684 sec/iter\n",
      "Epoch: 194 | Batch: 008 / 011 | Total loss: 1.434 | Reg loss: 0.039 | Tree loss: 1.434 | Accuracy: 0.491000 | 1.683 sec/iter\n",
      "Epoch: 194 | Batch: 009 / 011 | Total loss: 1.402 | Reg loss: 0.039 | Tree loss: 1.402 | Accuracy: 0.491500 | 1.683 sec/iter\n",
      "Epoch: 194 | Batch: 010 / 011 | Total loss: 1.439 | Reg loss: 0.039 | Tree loss: 1.439 | Accuracy: 0.498294 | 1.683 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 195 | Batch: 000 / 011 | Total loss: 1.628 | Reg loss: 0.039 | Tree loss: 1.628 | Accuracy: 0.408500 | 1.686 sec/iter\n",
      "Epoch: 195 | Batch: 001 / 011 | Total loss: 1.582 | Reg loss: 0.039 | Tree loss: 1.582 | Accuracy: 0.394500 | 1.686 sec/iter\n",
      "Epoch: 195 | Batch: 002 / 011 | Total loss: 1.555 | Reg loss: 0.039 | Tree loss: 1.555 | Accuracy: 0.431000 | 1.686 sec/iter\n",
      "Epoch: 195 | Batch: 003 / 011 | Total loss: 1.503 | Reg loss: 0.039 | Tree loss: 1.503 | Accuracy: 0.453500 | 1.686 sec/iter\n",
      "Epoch: 195 | Batch: 004 / 011 | Total loss: 1.461 | Reg loss: 0.039 | Tree loss: 1.461 | Accuracy: 0.502000 | 1.686 sec/iter\n",
      "Epoch: 195 | Batch: 005 / 011 | Total loss: 1.447 | Reg loss: 0.039 | Tree loss: 1.447 | Accuracy: 0.468000 | 1.686 sec/iter\n",
      "Epoch: 195 | Batch: 006 / 011 | Total loss: 1.446 | Reg loss: 0.039 | Tree loss: 1.446 | Accuracy: 0.481000 | 1.686 sec/iter\n",
      "Epoch: 195 | Batch: 007 / 011 | Total loss: 1.432 | Reg loss: 0.039 | Tree loss: 1.432 | Accuracy: 0.486500 | 1.685 sec/iter\n",
      "Epoch: 195 | Batch: 008 / 011 | Total loss: 1.419 | Reg loss: 0.039 | Tree loss: 1.419 | Accuracy: 0.490500 | 1.685 sec/iter\n",
      "Epoch: 195 | Batch: 009 / 011 | Total loss: 1.414 | Reg loss: 0.039 | Tree loss: 1.414 | Accuracy: 0.499500 | 1.685 sec/iter\n",
      "Epoch: 195 | Batch: 010 / 011 | Total loss: 1.428 | Reg loss: 0.039 | Tree loss: 1.428 | Accuracy: 0.481229 | 1.685 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 196 | Batch: 000 / 011 | Total loss: 1.636 | Reg loss: 0.039 | Tree loss: 1.636 | Accuracy: 0.390000 | 1.685 sec/iter\n",
      "Epoch: 196 | Batch: 001 / 011 | Total loss: 1.582 | Reg loss: 0.039 | Tree loss: 1.582 | Accuracy: 0.414000 | 1.685 sec/iter\n",
      "Epoch: 196 | Batch: 002 / 011 | Total loss: 1.586 | Reg loss: 0.039 | Tree loss: 1.586 | Accuracy: 0.417500 | 1.685 sec/iter\n",
      "Epoch: 196 | Batch: 003 / 011 | Total loss: 1.524 | Reg loss: 0.039 | Tree loss: 1.524 | Accuracy: 0.432500 | 1.685 sec/iter\n",
      "Epoch: 196 | Batch: 004 / 011 | Total loss: 1.453 | Reg loss: 0.039 | Tree loss: 1.453 | Accuracy: 0.484500 | 1.685 sec/iter\n",
      "Epoch: 196 | Batch: 005 / 011 | Total loss: 1.447 | Reg loss: 0.039 | Tree loss: 1.447 | Accuracy: 0.484000 | 1.685 sec/iter\n",
      "Epoch: 196 | Batch: 006 / 011 | Total loss: 1.431 | Reg loss: 0.039 | Tree loss: 1.431 | Accuracy: 0.491000 | 1.685 sec/iter\n",
      "Epoch: 196 | Batch: 007 / 011 | Total loss: 1.421 | Reg loss: 0.039 | Tree loss: 1.421 | Accuracy: 0.502000 | 1.684 sec/iter\n",
      "Epoch: 196 | Batch: 008 / 011 | Total loss: 1.423 | Reg loss: 0.039 | Tree loss: 1.423 | Accuracy: 0.498000 | 1.684 sec/iter\n",
      "Epoch: 196 | Batch: 009 / 011 | Total loss: 1.414 | Reg loss: 0.039 | Tree loss: 1.414 | Accuracy: 0.490500 | 1.684 sec/iter\n",
      "Epoch: 196 | Batch: 010 / 011 | Total loss: 1.450 | Reg loss: 0.039 | Tree loss: 1.450 | Accuracy: 0.419795 | 1.684 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 197 | Batch: 000 / 011 | Total loss: 1.653 | Reg loss: 0.039 | Tree loss: 1.653 | Accuracy: 0.389000 | 1.687 sec/iter\n",
      "Epoch: 197 | Batch: 001 / 011 | Total loss: 1.592 | Reg loss: 0.039 | Tree loss: 1.592 | Accuracy: 0.412500 | 1.687 sec/iter\n",
      "Epoch: 197 | Batch: 002 / 011 | Total loss: 1.545 | Reg loss: 0.039 | Tree loss: 1.545 | Accuracy: 0.445000 | 1.687 sec/iter\n",
      "Epoch: 197 | Batch: 003 / 011 | Total loss: 1.517 | Reg loss: 0.039 | Tree loss: 1.517 | Accuracy: 0.479500 | 1.687 sec/iter\n",
      "Epoch: 197 | Batch: 004 / 011 | Total loss: 1.477 | Reg loss: 0.039 | Tree loss: 1.477 | Accuracy: 0.465500 | 1.687 sec/iter\n",
      "Epoch: 197 | Batch: 005 / 011 | Total loss: 1.427 | Reg loss: 0.039 | Tree loss: 1.427 | Accuracy: 0.496000 | 1.687 sec/iter\n",
      "Epoch: 197 | Batch: 006 / 011 | Total loss: 1.454 | Reg loss: 0.039 | Tree loss: 1.454 | Accuracy: 0.503500 | 1.687 sec/iter\n",
      "Epoch: 197 | Batch: 007 / 011 | Total loss: 1.433 | Reg loss: 0.039 | Tree loss: 1.433 | Accuracy: 0.490500 | 1.687 sec/iter\n",
      "Epoch: 197 | Batch: 008 / 011 | Total loss: 1.420 | Reg loss: 0.039 | Tree loss: 1.420 | Accuracy: 0.493000 | 1.686 sec/iter\n",
      "Epoch: 197 | Batch: 009 / 011 | Total loss: 1.403 | Reg loss: 0.039 | Tree loss: 1.403 | Accuracy: 0.502500 | 1.686 sec/iter\n",
      "Epoch: 197 | Batch: 010 / 011 | Total loss: 1.380 | Reg loss: 0.039 | Tree loss: 1.380 | Accuracy: 0.505119 | 1.686 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 198 | Batch: 000 / 011 | Total loss: 1.624 | Reg loss: 0.039 | Tree loss: 1.624 | Accuracy: 0.411500 | 1.686 sec/iter\n",
      "Epoch: 198 | Batch: 001 / 011 | Total loss: 1.573 | Reg loss: 0.039 | Tree loss: 1.573 | Accuracy: 0.427000 | 1.686 sec/iter\n",
      "Epoch: 198 | Batch: 002 / 011 | Total loss: 1.541 | Reg loss: 0.039 | Tree loss: 1.541 | Accuracy: 0.446500 | 1.686 sec/iter\n",
      "Epoch: 198 | Batch: 003 / 011 | Total loss: 1.530 | Reg loss: 0.039 | Tree loss: 1.530 | Accuracy: 0.444500 | 1.686 sec/iter\n",
      "Epoch: 198 | Batch: 004 / 011 | Total loss: 1.451 | Reg loss: 0.039 | Tree loss: 1.451 | Accuracy: 0.494500 | 1.686 sec/iter\n",
      "Epoch: 198 | Batch: 005 / 011 | Total loss: 1.448 | Reg loss: 0.039 | Tree loss: 1.448 | Accuracy: 0.487000 | 1.686 sec/iter\n",
      "Epoch: 198 | Batch: 006 / 011 | Total loss: 1.462 | Reg loss: 0.039 | Tree loss: 1.462 | Accuracy: 0.480500 | 1.686 sec/iter\n",
      "Epoch: 198 | Batch: 007 / 011 | Total loss: 1.401 | Reg loss: 0.039 | Tree loss: 1.401 | Accuracy: 0.498500 | 1.686 sec/iter\n",
      "Epoch: 198 | Batch: 008 / 011 | Total loss: 1.424 | Reg loss: 0.039 | Tree loss: 1.424 | Accuracy: 0.501000 | 1.686 sec/iter\n",
      "Epoch: 198 | Batch: 009 / 011 | Total loss: 1.432 | Reg loss: 0.039 | Tree loss: 1.432 | Accuracy: 0.483000 | 1.685 sec/iter\n",
      "Epoch: 198 | Batch: 010 / 011 | Total loss: 1.410 | Reg loss: 0.039 | Tree loss: 1.410 | Accuracy: 0.508532 | 1.685 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 199 | Batch: 000 / 011 | Total loss: 1.617 | Reg loss: 0.039 | Tree loss: 1.617 | Accuracy: 0.394000 | 1.689 sec/iter\n",
      "Epoch: 199 | Batch: 001 / 011 | Total loss: 1.589 | Reg loss: 0.039 | Tree loss: 1.589 | Accuracy: 0.416500 | 1.688 sec/iter\n",
      "Epoch: 199 | Batch: 002 / 011 | Total loss: 1.539 | Reg loss: 0.039 | Tree loss: 1.539 | Accuracy: 0.440500 | 1.688 sec/iter\n",
      "Epoch: 199 | Batch: 003 / 011 | Total loss: 1.522 | Reg loss: 0.039 | Tree loss: 1.522 | Accuracy: 0.455000 | 1.688 sec/iter\n",
      "Epoch: 199 | Batch: 004 / 011 | Total loss: 1.477 | Reg loss: 0.039 | Tree loss: 1.477 | Accuracy: 0.481500 | 1.688 sec/iter\n",
      "Epoch: 199 | Batch: 005 / 011 | Total loss: 1.474 | Reg loss: 0.039 | Tree loss: 1.474 | Accuracy: 0.476500 | 1.688 sec/iter\n",
      "Epoch: 199 | Batch: 006 / 011 | Total loss: 1.417 | Reg loss: 0.039 | Tree loss: 1.417 | Accuracy: 0.502500 | 1.688 sec/iter\n",
      "Epoch: 199 | Batch: 007 / 011 | Total loss: 1.418 | Reg loss: 0.039 | Tree loss: 1.418 | Accuracy: 0.502500 | 1.688 sec/iter\n",
      "Epoch: 199 | Batch: 008 / 011 | Total loss: 1.413 | Reg loss: 0.039 | Tree loss: 1.413 | Accuracy: 0.519000 | 1.688 sec/iter\n",
      "Epoch: 199 | Batch: 009 / 011 | Total loss: 1.419 | Reg loss: 0.039 | Tree loss: 1.419 | Accuracy: 0.489500 | 1.688 sec/iter\n",
      "Epoch: 199 | Batch: 010 / 011 | Total loss: 1.521 | Reg loss: 0.040 | Tree loss: 1.521 | Accuracy: 0.409556 | 1.687 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 200 | Batch: 000 / 011 | Total loss: 1.616 | Reg loss: 0.039 | Tree loss: 1.616 | Accuracy: 0.406000 | 1.688 sec/iter\n",
      "Epoch: 200 | Batch: 001 / 011 | Total loss: 1.606 | Reg loss: 0.039 | Tree loss: 1.606 | Accuracy: 0.411500 | 1.687 sec/iter\n",
      "Epoch: 200 | Batch: 002 / 011 | Total loss: 1.530 | Reg loss: 0.039 | Tree loss: 1.530 | Accuracy: 0.463500 | 1.687 sec/iter\n",
      "Epoch: 200 | Batch: 003 / 011 | Total loss: 1.522 | Reg loss: 0.039 | Tree loss: 1.522 | Accuracy: 0.465500 | 1.687 sec/iter\n",
      "Epoch: 200 | Batch: 004 / 011 | Total loss: 1.470 | Reg loss: 0.039 | Tree loss: 1.470 | Accuracy: 0.472000 | 1.687 sec/iter\n",
      "Epoch: 200 | Batch: 005 / 011 | Total loss: 1.443 | Reg loss: 0.039 | Tree loss: 1.443 | Accuracy: 0.500500 | 1.687 sec/iter\n",
      "Epoch: 200 | Batch: 006 / 011 | Total loss: 1.433 | Reg loss: 0.039 | Tree loss: 1.433 | Accuracy: 0.487000 | 1.687 sec/iter\n",
      "Epoch: 200 | Batch: 007 / 011 | Total loss: 1.442 | Reg loss: 0.039 | Tree loss: 1.442 | Accuracy: 0.482000 | 1.687 sec/iter\n",
      "Epoch: 200 | Batch: 008 / 011 | Total loss: 1.438 | Reg loss: 0.039 | Tree loss: 1.438 | Accuracy: 0.497500 | 1.687 sec/iter\n",
      "Epoch: 200 | Batch: 009 / 011 | Total loss: 1.410 | Reg loss: 0.039 | Tree loss: 1.410 | Accuracy: 0.499500 | 1.687 sec/iter\n",
      "Epoch: 200 | Batch: 010 / 011 | Total loss: 1.376 | Reg loss: 0.040 | Tree loss: 1.376 | Accuracy: 0.474403 | 1.686 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 201 | Batch: 000 / 011 | Total loss: 1.639 | Reg loss: 0.039 | Tree loss: 1.639 | Accuracy: 0.392500 | 1.69 sec/iter\n",
      "Epoch: 201 | Batch: 001 / 011 | Total loss: 1.569 | Reg loss: 0.039 | Tree loss: 1.569 | Accuracy: 0.430000 | 1.69 sec/iter\n",
      "Epoch: 201 | Batch: 002 / 011 | Total loss: 1.556 | Reg loss: 0.039 | Tree loss: 1.556 | Accuracy: 0.429000 | 1.69 sec/iter\n",
      "Epoch: 201 | Batch: 003 / 011 | Total loss: 1.490 | Reg loss: 0.039 | Tree loss: 1.490 | Accuracy: 0.471000 | 1.689 sec/iter\n",
      "Epoch: 201 | Batch: 004 / 011 | Total loss: 1.480 | Reg loss: 0.039 | Tree loss: 1.480 | Accuracy: 0.481000 | 1.689 sec/iter\n",
      "Epoch: 201 | Batch: 005 / 011 | Total loss: 1.453 | Reg loss: 0.039 | Tree loss: 1.453 | Accuracy: 0.489000 | 1.689 sec/iter\n",
      "Epoch: 201 | Batch: 006 / 011 | Total loss: 1.448 | Reg loss: 0.039 | Tree loss: 1.448 | Accuracy: 0.495000 | 1.688 sec/iter\n",
      "Epoch: 201 | Batch: 007 / 011 | Total loss: 1.416 | Reg loss: 0.039 | Tree loss: 1.416 | Accuracy: 0.512000 | 1.688 sec/iter\n",
      "Epoch: 201 | Batch: 008 / 011 | Total loss: 1.412 | Reg loss: 0.039 | Tree loss: 1.412 | Accuracy: 0.478500 | 1.688 sec/iter\n",
      "Epoch: 201 | Batch: 009 / 011 | Total loss: 1.411 | Reg loss: 0.040 | Tree loss: 1.411 | Accuracy: 0.487000 | 1.688 sec/iter\n",
      "Epoch: 201 | Batch: 010 / 011 | Total loss: 1.429 | Reg loss: 0.040 | Tree loss: 1.429 | Accuracy: 0.470990 | 1.687 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 202 | Batch: 000 / 011 | Total loss: 1.598 | Reg loss: 0.039 | Tree loss: 1.598 | Accuracy: 0.416000 | 1.69 sec/iter\n",
      "Epoch: 202 | Batch: 001 / 011 | Total loss: 1.591 | Reg loss: 0.039 | Tree loss: 1.591 | Accuracy: 0.404000 | 1.689 sec/iter\n",
      "Epoch: 202 | Batch: 002 / 011 | Total loss: 1.554 | Reg loss: 0.039 | Tree loss: 1.554 | Accuracy: 0.440500 | 1.689 sec/iter\n",
      "Epoch: 202 | Batch: 003 / 011 | Total loss: 1.501 | Reg loss: 0.039 | Tree loss: 1.501 | Accuracy: 0.466000 | 1.689 sec/iter\n",
      "Epoch: 202 | Batch: 004 / 011 | Total loss: 1.466 | Reg loss: 0.039 | Tree loss: 1.466 | Accuracy: 0.476000 | 1.689 sec/iter\n",
      "Epoch: 202 | Batch: 005 / 011 | Total loss: 1.464 | Reg loss: 0.039 | Tree loss: 1.464 | Accuracy: 0.484500 | 1.689 sec/iter\n",
      "Epoch: 202 | Batch: 006 / 011 | Total loss: 1.438 | Reg loss: 0.039 | Tree loss: 1.438 | Accuracy: 0.496500 | 1.689 sec/iter\n",
      "Epoch: 202 | Batch: 007 / 011 | Total loss: 1.413 | Reg loss: 0.039 | Tree loss: 1.413 | Accuracy: 0.497000 | 1.689 sec/iter\n",
      "Epoch: 202 | Batch: 008 / 011 | Total loss: 1.431 | Reg loss: 0.039 | Tree loss: 1.431 | Accuracy: 0.476500 | 1.689 sec/iter\n",
      "Epoch: 202 | Batch: 009 / 011 | Total loss: 1.444 | Reg loss: 0.040 | Tree loss: 1.444 | Accuracy: 0.493000 | 1.689 sec/iter\n",
      "Epoch: 202 | Batch: 010 / 011 | Total loss: 1.400 | Reg loss: 0.040 | Tree loss: 1.400 | Accuracy: 0.505119 | 1.688 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 203 | Batch: 000 / 011 | Total loss: 1.627 | Reg loss: 0.039 | Tree loss: 1.627 | Accuracy: 0.397000 | 1.689 sec/iter\n",
      "Epoch: 203 | Batch: 001 / 011 | Total loss: 1.592 | Reg loss: 0.039 | Tree loss: 1.592 | Accuracy: 0.398500 | 1.689 sec/iter\n",
      "Epoch: 203 | Batch: 002 / 011 | Total loss: 1.550 | Reg loss: 0.039 | Tree loss: 1.550 | Accuracy: 0.423500 | 1.689 sec/iter\n",
      "Epoch: 203 | Batch: 003 / 011 | Total loss: 1.520 | Reg loss: 0.039 | Tree loss: 1.520 | Accuracy: 0.453000 | 1.689 sec/iter\n",
      "Epoch: 203 | Batch: 004 / 011 | Total loss: 1.464 | Reg loss: 0.039 | Tree loss: 1.464 | Accuracy: 0.489000 | 1.689 sec/iter\n",
      "Epoch: 203 | Batch: 005 / 011 | Total loss: 1.460 | Reg loss: 0.039 | Tree loss: 1.460 | Accuracy: 0.496000 | 1.689 sec/iter\n",
      "Epoch: 203 | Batch: 006 / 011 | Total loss: 1.418 | Reg loss: 0.039 | Tree loss: 1.418 | Accuracy: 0.514500 | 1.689 sec/iter\n",
      "Epoch: 203 | Batch: 007 / 011 | Total loss: 1.413 | Reg loss: 0.039 | Tree loss: 1.413 | Accuracy: 0.518500 | 1.689 sec/iter\n",
      "Epoch: 203 | Batch: 008 / 011 | Total loss: 1.410 | Reg loss: 0.040 | Tree loss: 1.410 | Accuracy: 0.496000 | 1.689 sec/iter\n",
      "Epoch: 203 | Batch: 009 / 011 | Total loss: 1.442 | Reg loss: 0.040 | Tree loss: 1.442 | Accuracy: 0.490000 | 1.689 sec/iter\n",
      "Epoch: 203 | Batch: 010 / 011 | Total loss: 1.453 | Reg loss: 0.040 | Tree loss: 1.453 | Accuracy: 0.447099 | 1.688 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 204 | Batch: 000 / 011 | Total loss: 1.611 | Reg loss: 0.039 | Tree loss: 1.611 | Accuracy: 0.408500 | 1.689 sec/iter\n",
      "Epoch: 204 | Batch: 001 / 011 | Total loss: 1.589 | Reg loss: 0.039 | Tree loss: 1.589 | Accuracy: 0.415000 | 1.689 sec/iter\n",
      "Epoch: 204 | Batch: 002 / 011 | Total loss: 1.533 | Reg loss: 0.039 | Tree loss: 1.533 | Accuracy: 0.445000 | 1.689 sec/iter\n",
      "Epoch: 204 | Batch: 003 / 011 | Total loss: 1.514 | Reg loss: 0.039 | Tree loss: 1.514 | Accuracy: 0.454000 | 1.689 sec/iter\n",
      "Epoch: 204 | Batch: 004 / 011 | Total loss: 1.486 | Reg loss: 0.039 | Tree loss: 1.486 | Accuracy: 0.460000 | 1.689 sec/iter\n",
      "Epoch: 204 | Batch: 005 / 011 | Total loss: 1.420 | Reg loss: 0.039 | Tree loss: 1.420 | Accuracy: 0.520500 | 1.689 sec/iter\n",
      "Epoch: 204 | Batch: 006 / 011 | Total loss: 1.438 | Reg loss: 0.039 | Tree loss: 1.438 | Accuracy: 0.488500 | 1.689 sec/iter\n",
      "Epoch: 204 | Batch: 007 / 011 | Total loss: 1.456 | Reg loss: 0.039 | Tree loss: 1.456 | Accuracy: 0.473000 | 1.689 sec/iter\n",
      "Epoch: 204 | Batch: 008 / 011 | Total loss: 1.413 | Reg loss: 0.040 | Tree loss: 1.413 | Accuracy: 0.501500 | 1.689 sec/iter\n",
      "Epoch: 204 | Batch: 009 / 011 | Total loss: 1.421 | Reg loss: 0.040 | Tree loss: 1.421 | Accuracy: 0.503000 | 1.689 sec/iter\n",
      "Epoch: 204 | Batch: 010 / 011 | Total loss: 1.485 | Reg loss: 0.040 | Tree loss: 1.485 | Accuracy: 0.419795 | 1.688 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 205 | Batch: 000 / 011 | Total loss: 1.640 | Reg loss: 0.039 | Tree loss: 1.640 | Accuracy: 0.381500 | 1.69 sec/iter\n",
      "Epoch: 205 | Batch: 001 / 011 | Total loss: 1.597 | Reg loss: 0.039 | Tree loss: 1.597 | Accuracy: 0.402000 | 1.69 sec/iter\n",
      "Epoch: 205 | Batch: 002 / 011 | Total loss: 1.544 | Reg loss: 0.039 | Tree loss: 1.544 | Accuracy: 0.436000 | 1.69 sec/iter\n",
      "Epoch: 205 | Batch: 003 / 011 | Total loss: 1.501 | Reg loss: 0.039 | Tree loss: 1.501 | Accuracy: 0.458500 | 1.69 sec/iter\n",
      "Epoch: 205 | Batch: 004 / 011 | Total loss: 1.479 | Reg loss: 0.039 | Tree loss: 1.479 | Accuracy: 0.477000 | 1.69 sec/iter\n",
      "Epoch: 205 | Batch: 005 / 011 | Total loss: 1.426 | Reg loss: 0.039 | Tree loss: 1.426 | Accuracy: 0.521000 | 1.69 sec/iter\n",
      "Epoch: 205 | Batch: 006 / 011 | Total loss: 1.453 | Reg loss: 0.039 | Tree loss: 1.453 | Accuracy: 0.493500 | 1.69 sec/iter\n",
      "Epoch: 205 | Batch: 007 / 011 | Total loss: 1.411 | Reg loss: 0.040 | Tree loss: 1.411 | Accuracy: 0.514000 | 1.69 sec/iter\n",
      "Epoch: 205 | Batch: 008 / 011 | Total loss: 1.406 | Reg loss: 0.040 | Tree loss: 1.406 | Accuracy: 0.504500 | 1.69 sec/iter\n",
      "Epoch: 205 | Batch: 009 / 011 | Total loss: 1.425 | Reg loss: 0.040 | Tree loss: 1.425 | Accuracy: 0.481500 | 1.69 sec/iter\n",
      "Epoch: 205 | Batch: 010 / 011 | Total loss: 1.480 | Reg loss: 0.040 | Tree loss: 1.480 | Accuracy: 0.477816 | 1.689 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 206 | Batch: 000 / 011 | Total loss: 1.629 | Reg loss: 0.039 | Tree loss: 1.629 | Accuracy: 0.392000 | 1.689 sec/iter\n",
      "Epoch: 206 | Batch: 001 / 011 | Total loss: 1.581 | Reg loss: 0.039 | Tree loss: 1.581 | Accuracy: 0.416000 | 1.689 sec/iter\n",
      "Epoch: 206 | Batch: 002 / 011 | Total loss: 1.536 | Reg loss: 0.039 | Tree loss: 1.536 | Accuracy: 0.441500 | 1.689 sec/iter\n",
      "Epoch: 206 | Batch: 003 / 011 | Total loss: 1.487 | Reg loss: 0.039 | Tree loss: 1.487 | Accuracy: 0.461000 | 1.689 sec/iter\n",
      "Epoch: 206 | Batch: 004 / 011 | Total loss: 1.466 | Reg loss: 0.039 | Tree loss: 1.466 | Accuracy: 0.496000 | 1.689 sec/iter\n",
      "Epoch: 206 | Batch: 005 / 011 | Total loss: 1.447 | Reg loss: 0.039 | Tree loss: 1.447 | Accuracy: 0.500500 | 1.689 sec/iter\n",
      "Epoch: 206 | Batch: 006 / 011 | Total loss: 1.458 | Reg loss: 0.039 | Tree loss: 1.458 | Accuracy: 0.490500 | 1.689 sec/iter\n",
      "Epoch: 206 | Batch: 007 / 011 | Total loss: 1.404 | Reg loss: 0.040 | Tree loss: 1.404 | Accuracy: 0.493000 | 1.689 sec/iter\n",
      "Epoch: 206 | Batch: 008 / 011 | Total loss: 1.419 | Reg loss: 0.040 | Tree loss: 1.419 | Accuracy: 0.499000 | 1.689 sec/iter\n",
      "Epoch: 206 | Batch: 009 / 011 | Total loss: 1.441 | Reg loss: 0.040 | Tree loss: 1.441 | Accuracy: 0.503000 | 1.689 sec/iter\n",
      "Epoch: 206 | Batch: 010 / 011 | Total loss: 1.427 | Reg loss: 0.040 | Tree loss: 1.427 | Accuracy: 0.457338 | 1.688 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 207 | Batch: 000 / 011 | Total loss: 1.625 | Reg loss: 0.039 | Tree loss: 1.625 | Accuracy: 0.399000 | 1.692 sec/iter\n",
      "Epoch: 207 | Batch: 001 / 011 | Total loss: 1.588 | Reg loss: 0.039 | Tree loss: 1.588 | Accuracy: 0.430000 | 1.692 sec/iter\n",
      "Epoch: 207 | Batch: 002 / 011 | Total loss: 1.544 | Reg loss: 0.039 | Tree loss: 1.544 | Accuracy: 0.444000 | 1.691 sec/iter\n",
      "Epoch: 207 | Batch: 003 / 011 | Total loss: 1.508 | Reg loss: 0.039 | Tree loss: 1.508 | Accuracy: 0.459000 | 1.691 sec/iter\n",
      "Epoch: 207 | Batch: 004 / 011 | Total loss: 1.470 | Reg loss: 0.039 | Tree loss: 1.470 | Accuracy: 0.487000 | 1.691 sec/iter\n",
      "Epoch: 207 | Batch: 005 / 011 | Total loss: 1.468 | Reg loss: 0.039 | Tree loss: 1.468 | Accuracy: 0.497000 | 1.691 sec/iter\n",
      "Epoch: 207 | Batch: 006 / 011 | Total loss: 1.435 | Reg loss: 0.040 | Tree loss: 1.435 | Accuracy: 0.519500 | 1.691 sec/iter\n",
      "Epoch: 207 | Batch: 007 / 011 | Total loss: 1.435 | Reg loss: 0.040 | Tree loss: 1.435 | Accuracy: 0.499000 | 1.691 sec/iter\n",
      "Epoch: 207 | Batch: 008 / 011 | Total loss: 1.405 | Reg loss: 0.040 | Tree loss: 1.405 | Accuracy: 0.507000 | 1.691 sec/iter\n",
      "Epoch: 207 | Batch: 009 / 011 | Total loss: 1.399 | Reg loss: 0.040 | Tree loss: 1.399 | Accuracy: 0.512000 | 1.691 sec/iter\n",
      "Epoch: 207 | Batch: 010 / 011 | Total loss: 1.457 | Reg loss: 0.040 | Tree loss: 1.457 | Accuracy: 0.511945 | 1.691 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 208 | Batch: 000 / 011 | Total loss: 1.614 | Reg loss: 0.039 | Tree loss: 1.614 | Accuracy: 0.402500 | 1.691 sec/iter\n",
      "Epoch: 208 | Batch: 001 / 011 | Total loss: 1.567 | Reg loss: 0.039 | Tree loss: 1.567 | Accuracy: 0.435000 | 1.691 sec/iter\n",
      "Epoch: 208 | Batch: 002 / 011 | Total loss: 1.542 | Reg loss: 0.039 | Tree loss: 1.542 | Accuracy: 0.431000 | 1.691 sec/iter\n",
      "Epoch: 208 | Batch: 003 / 011 | Total loss: 1.533 | Reg loss: 0.039 | Tree loss: 1.533 | Accuracy: 0.437000 | 1.69 sec/iter\n",
      "Epoch: 208 | Batch: 004 / 011 | Total loss: 1.480 | Reg loss: 0.039 | Tree loss: 1.480 | Accuracy: 0.453500 | 1.69 sec/iter\n",
      "Epoch: 208 | Batch: 005 / 011 | Total loss: 1.425 | Reg loss: 0.039 | Tree loss: 1.425 | Accuracy: 0.505000 | 1.69 sec/iter\n",
      "Epoch: 208 | Batch: 006 / 011 | Total loss: 1.431 | Reg loss: 0.040 | Tree loss: 1.431 | Accuracy: 0.510500 | 1.69 sec/iter\n",
      "Epoch: 208 | Batch: 007 / 011 | Total loss: 1.427 | Reg loss: 0.040 | Tree loss: 1.427 | Accuracy: 0.488000 | 1.69 sec/iter\n",
      "Epoch: 208 | Batch: 008 / 011 | Total loss: 1.422 | Reg loss: 0.040 | Tree loss: 1.422 | Accuracy: 0.499000 | 1.69 sec/iter\n",
      "Epoch: 208 | Batch: 009 / 011 | Total loss: 1.411 | Reg loss: 0.040 | Tree loss: 1.411 | Accuracy: 0.496500 | 1.69 sec/iter\n",
      "Epoch: 208 | Batch: 010 / 011 | Total loss: 1.417 | Reg loss: 0.040 | Tree loss: 1.417 | Accuracy: 0.494881 | 1.69 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 209 | Batch: 000 / 011 | Total loss: 1.605 | Reg loss: 0.039 | Tree loss: 1.605 | Accuracy: 0.395000 | 1.69 sec/iter\n",
      "Epoch: 209 | Batch: 001 / 011 | Total loss: 1.573 | Reg loss: 0.039 | Tree loss: 1.573 | Accuracy: 0.412000 | 1.69 sec/iter\n",
      "Epoch: 209 | Batch: 002 / 011 | Total loss: 1.572 | Reg loss: 0.039 | Tree loss: 1.572 | Accuracy: 0.425000 | 1.689 sec/iter\n",
      "Epoch: 209 | Batch: 003 / 011 | Total loss: 1.537 | Reg loss: 0.039 | Tree loss: 1.537 | Accuracy: 0.448000 | 1.689 sec/iter\n",
      "Epoch: 209 | Batch: 004 / 011 | Total loss: 1.475 | Reg loss: 0.039 | Tree loss: 1.475 | Accuracy: 0.466000 | 1.689 sec/iter\n",
      "Epoch: 209 | Batch: 005 / 011 | Total loss: 1.437 | Reg loss: 0.040 | Tree loss: 1.437 | Accuracy: 0.506500 | 1.689 sec/iter\n",
      "Epoch: 209 | Batch: 006 / 011 | Total loss: 1.431 | Reg loss: 0.040 | Tree loss: 1.431 | Accuracy: 0.500500 | 1.688 sec/iter\n",
      "Epoch: 209 | Batch: 007 / 011 | Total loss: 1.426 | Reg loss: 0.040 | Tree loss: 1.426 | Accuracy: 0.492500 | 1.688 sec/iter\n",
      "Epoch: 209 | Batch: 008 / 011 | Total loss: 1.396 | Reg loss: 0.040 | Tree loss: 1.396 | Accuracy: 0.492500 | 1.688 sec/iter\n",
      "Epoch: 209 | Batch: 009 / 011 | Total loss: 1.410 | Reg loss: 0.040 | Tree loss: 1.410 | Accuracy: 0.494000 | 1.688 sec/iter\n",
      "Epoch: 209 | Batch: 010 / 011 | Total loss: 1.374 | Reg loss: 0.040 | Tree loss: 1.374 | Accuracy: 0.474403 | 1.687 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 210 | Batch: 000 / 011 | Total loss: 1.631 | Reg loss: 0.039 | Tree loss: 1.631 | Accuracy: 0.383000 | 1.69 sec/iter\n",
      "Epoch: 210 | Batch: 001 / 011 | Total loss: 1.582 | Reg loss: 0.039 | Tree loss: 1.582 | Accuracy: 0.400000 | 1.69 sec/iter\n",
      "Epoch: 210 | Batch: 002 / 011 | Total loss: 1.534 | Reg loss: 0.039 | Tree loss: 1.534 | Accuracy: 0.427000 | 1.69 sec/iter\n",
      "Epoch: 210 | Batch: 003 / 011 | Total loss: 1.475 | Reg loss: 0.039 | Tree loss: 1.475 | Accuracy: 0.471000 | 1.69 sec/iter\n",
      "Epoch: 210 | Batch: 004 / 011 | Total loss: 1.477 | Reg loss: 0.039 | Tree loss: 1.477 | Accuracy: 0.475000 | 1.69 sec/iter\n",
      "Epoch: 210 | Batch: 005 / 011 | Total loss: 1.445 | Reg loss: 0.040 | Tree loss: 1.445 | Accuracy: 0.484000 | 1.69 sec/iter\n",
      "Epoch: 210 | Batch: 006 / 011 | Total loss: 1.413 | Reg loss: 0.040 | Tree loss: 1.413 | Accuracy: 0.515000 | 1.689 sec/iter\n",
      "Epoch: 210 | Batch: 007 / 011 | Total loss: 1.431 | Reg loss: 0.040 | Tree loss: 1.431 | Accuracy: 0.473000 | 1.689 sec/iter\n",
      "Epoch: 210 | Batch: 008 / 011 | Total loss: 1.419 | Reg loss: 0.040 | Tree loss: 1.419 | Accuracy: 0.490500 | 1.689 sec/iter\n",
      "Epoch: 210 | Batch: 009 / 011 | Total loss: 1.441 | Reg loss: 0.040 | Tree loss: 1.441 | Accuracy: 0.498000 | 1.689 sec/iter\n",
      "Epoch: 210 | Batch: 010 / 011 | Total loss: 1.419 | Reg loss: 0.040 | Tree loss: 1.419 | Accuracy: 0.477816 | 1.689 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 211 | Batch: 000 / 011 | Total loss: 1.604 | Reg loss: 0.039 | Tree loss: 1.604 | Accuracy: 0.382000 | 1.689 sec/iter\n",
      "Epoch: 211 | Batch: 001 / 011 | Total loss: 1.579 | Reg loss: 0.039 | Tree loss: 1.579 | Accuracy: 0.417500 | 1.689 sec/iter\n",
      "Epoch: 211 | Batch: 002 / 011 | Total loss: 1.553 | Reg loss: 0.039 | Tree loss: 1.553 | Accuracy: 0.432500 | 1.689 sec/iter\n",
      "Epoch: 211 | Batch: 003 / 011 | Total loss: 1.497 | Reg loss: 0.039 | Tree loss: 1.497 | Accuracy: 0.455500 | 1.689 sec/iter\n",
      "Epoch: 211 | Batch: 004 / 011 | Total loss: 1.476 | Reg loss: 0.040 | Tree loss: 1.476 | Accuracy: 0.459500 | 1.689 sec/iter\n",
      "Epoch: 211 | Batch: 005 / 011 | Total loss: 1.458 | Reg loss: 0.040 | Tree loss: 1.458 | Accuracy: 0.469000 | 1.689 sec/iter\n",
      "Epoch: 211 | Batch: 006 / 011 | Total loss: 1.430 | Reg loss: 0.040 | Tree loss: 1.430 | Accuracy: 0.513500 | 1.689 sec/iter\n",
      "Epoch: 211 | Batch: 007 / 011 | Total loss: 1.424 | Reg loss: 0.040 | Tree loss: 1.424 | Accuracy: 0.486500 | 1.689 sec/iter\n",
      "Epoch: 211 | Batch: 008 / 011 | Total loss: 1.447 | Reg loss: 0.040 | Tree loss: 1.447 | Accuracy: 0.480000 | 1.689 sec/iter\n",
      "Epoch: 211 | Batch: 009 / 011 | Total loss: 1.418 | Reg loss: 0.040 | Tree loss: 1.418 | Accuracy: 0.497500 | 1.689 sec/iter\n",
      "Epoch: 211 | Batch: 010 / 011 | Total loss: 1.357 | Reg loss: 0.040 | Tree loss: 1.357 | Accuracy: 0.511945 | 1.688 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 212 | Batch: 000 / 011 | Total loss: 1.619 | Reg loss: 0.039 | Tree loss: 1.619 | Accuracy: 0.396500 | 1.691 sec/iter\n",
      "Epoch: 212 | Batch: 001 / 011 | Total loss: 1.587 | Reg loss: 0.039 | Tree loss: 1.587 | Accuracy: 0.418500 | 1.691 sec/iter\n",
      "Epoch: 212 | Batch: 002 / 011 | Total loss: 1.548 | Reg loss: 0.039 | Tree loss: 1.548 | Accuracy: 0.449000 | 1.691 sec/iter\n",
      "Epoch: 212 | Batch: 003 / 011 | Total loss: 1.493 | Reg loss: 0.040 | Tree loss: 1.493 | Accuracy: 0.458500 | 1.691 sec/iter\n",
      "Epoch: 212 | Batch: 004 / 011 | Total loss: 1.461 | Reg loss: 0.040 | Tree loss: 1.461 | Accuracy: 0.496000 | 1.691 sec/iter\n",
      "Epoch: 212 | Batch: 005 / 011 | Total loss: 1.434 | Reg loss: 0.040 | Tree loss: 1.434 | Accuracy: 0.495000 | 1.691 sec/iter\n",
      "Epoch: 212 | Batch: 006 / 011 | Total loss: 1.432 | Reg loss: 0.040 | Tree loss: 1.432 | Accuracy: 0.499000 | 1.69 sec/iter\n",
      "Epoch: 212 | Batch: 007 / 011 | Total loss: 1.409 | Reg loss: 0.040 | Tree loss: 1.409 | Accuracy: 0.506500 | 1.69 sec/iter\n",
      "Epoch: 212 | Batch: 008 / 011 | Total loss: 1.432 | Reg loss: 0.040 | Tree loss: 1.432 | Accuracy: 0.484000 | 1.69 sec/iter\n",
      "Epoch: 212 | Batch: 009 / 011 | Total loss: 1.435 | Reg loss: 0.040 | Tree loss: 1.435 | Accuracy: 0.480500 | 1.69 sec/iter\n",
      "Epoch: 212 | Batch: 010 / 011 | Total loss: 1.447 | Reg loss: 0.040 | Tree loss: 1.447 | Accuracy: 0.453925 | 1.69 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 213 | Batch: 000 / 011 | Total loss: 1.636 | Reg loss: 0.039 | Tree loss: 1.636 | Accuracy: 0.390500 | 1.69 sec/iter\n",
      "Epoch: 213 | Batch: 001 / 011 | Total loss: 1.588 | Reg loss: 0.039 | Tree loss: 1.588 | Accuracy: 0.401000 | 1.69 sec/iter\n",
      "Epoch: 213 | Batch: 002 / 011 | Total loss: 1.556 | Reg loss: 0.039 | Tree loss: 1.556 | Accuracy: 0.439500 | 1.69 sec/iter\n",
      "Epoch: 213 | Batch: 003 / 011 | Total loss: 1.491 | Reg loss: 0.040 | Tree loss: 1.491 | Accuracy: 0.466000 | 1.69 sec/iter\n",
      "Epoch: 213 | Batch: 004 / 011 | Total loss: 1.468 | Reg loss: 0.040 | Tree loss: 1.468 | Accuracy: 0.482000 | 1.69 sec/iter\n",
      "Epoch: 213 | Batch: 005 / 011 | Total loss: 1.464 | Reg loss: 0.040 | Tree loss: 1.464 | Accuracy: 0.488500 | 1.69 sec/iter\n",
      "Epoch: 213 | Batch: 006 / 011 | Total loss: 1.417 | Reg loss: 0.040 | Tree loss: 1.417 | Accuracy: 0.510500 | 1.689 sec/iter\n",
      "Epoch: 213 | Batch: 007 / 011 | Total loss: 1.420 | Reg loss: 0.040 | Tree loss: 1.420 | Accuracy: 0.509500 | 1.689 sec/iter\n",
      "Epoch: 213 | Batch: 008 / 011 | Total loss: 1.436 | Reg loss: 0.040 | Tree loss: 1.436 | Accuracy: 0.482000 | 1.689 sec/iter\n",
      "Epoch: 213 | Batch: 009 / 011 | Total loss: 1.394 | Reg loss: 0.040 | Tree loss: 1.394 | Accuracy: 0.491500 | 1.689 sec/iter\n",
      "Epoch: 213 | Batch: 010 / 011 | Total loss: 1.424 | Reg loss: 0.040 | Tree loss: 1.424 | Accuracy: 0.453925 | 1.689 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 214 | Batch: 000 / 011 | Total loss: 1.595 | Reg loss: 0.039 | Tree loss: 1.595 | Accuracy: 0.407000 | 1.692 sec/iter\n",
      "Epoch: 214 | Batch: 001 / 011 | Total loss: 1.583 | Reg loss: 0.039 | Tree loss: 1.583 | Accuracy: 0.408000 | 1.692 sec/iter\n",
      "Epoch: 214 | Batch: 002 / 011 | Total loss: 1.554 | Reg loss: 0.040 | Tree loss: 1.554 | Accuracy: 0.438500 | 1.692 sec/iter\n",
      "Epoch: 214 | Batch: 003 / 011 | Total loss: 1.524 | Reg loss: 0.040 | Tree loss: 1.524 | Accuracy: 0.457500 | 1.692 sec/iter\n",
      "Epoch: 214 | Batch: 004 / 011 | Total loss: 1.471 | Reg loss: 0.040 | Tree loss: 1.471 | Accuracy: 0.481000 | 1.692 sec/iter\n",
      "Epoch: 214 | Batch: 005 / 011 | Total loss: 1.433 | Reg loss: 0.040 | Tree loss: 1.433 | Accuracy: 0.508500 | 1.692 sec/iter\n",
      "Epoch: 214 | Batch: 006 / 011 | Total loss: 1.448 | Reg loss: 0.040 | Tree loss: 1.448 | Accuracy: 0.491500 | 1.691 sec/iter\n",
      "Epoch: 214 | Batch: 007 / 011 | Total loss: 1.420 | Reg loss: 0.040 | Tree loss: 1.420 | Accuracy: 0.509000 | 1.691 sec/iter\n",
      "Epoch: 214 | Batch: 008 / 011 | Total loss: 1.411 | Reg loss: 0.040 | Tree loss: 1.411 | Accuracy: 0.498500 | 1.691 sec/iter\n",
      "Epoch: 214 | Batch: 009 / 011 | Total loss: 1.424 | Reg loss: 0.040 | Tree loss: 1.424 | Accuracy: 0.486000 | 1.691 sec/iter\n",
      "Epoch: 214 | Batch: 010 / 011 | Total loss: 1.353 | Reg loss: 0.040 | Tree loss: 1.353 | Accuracy: 0.515358 | 1.691 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 215 | Batch: 000 / 011 | Total loss: 1.624 | Reg loss: 0.040 | Tree loss: 1.624 | Accuracy: 0.396000 | 1.691 sec/iter\n",
      "Epoch: 215 | Batch: 001 / 011 | Total loss: 1.589 | Reg loss: 0.040 | Tree loss: 1.589 | Accuracy: 0.411500 | 1.691 sec/iter\n",
      "Epoch: 215 | Batch: 002 / 011 | Total loss: 1.517 | Reg loss: 0.040 | Tree loss: 1.517 | Accuracy: 0.468500 | 1.691 sec/iter\n",
      "Epoch: 215 | Batch: 003 / 011 | Total loss: 1.529 | Reg loss: 0.040 | Tree loss: 1.529 | Accuracy: 0.431000 | 1.691 sec/iter\n",
      "Epoch: 215 | Batch: 004 / 011 | Total loss: 1.476 | Reg loss: 0.040 | Tree loss: 1.476 | Accuracy: 0.472500 | 1.691 sec/iter\n",
      "Epoch: 215 | Batch: 005 / 011 | Total loss: 1.440 | Reg loss: 0.040 | Tree loss: 1.440 | Accuracy: 0.498000 | 1.691 sec/iter\n",
      "Epoch: 215 | Batch: 006 / 011 | Total loss: 1.418 | Reg loss: 0.040 | Tree loss: 1.418 | Accuracy: 0.521500 | 1.691 sec/iter\n",
      "Epoch: 215 | Batch: 007 / 011 | Total loss: 1.429 | Reg loss: 0.040 | Tree loss: 1.429 | Accuracy: 0.505500 | 1.69 sec/iter\n",
      "Epoch: 215 | Batch: 008 / 011 | Total loss: 1.445 | Reg loss: 0.040 | Tree loss: 1.445 | Accuracy: 0.493500 | 1.69 sec/iter\n",
      "Epoch: 215 | Batch: 009 / 011 | Total loss: 1.403 | Reg loss: 0.040 | Tree loss: 1.403 | Accuracy: 0.510500 | 1.69 sec/iter\n",
      "Epoch: 215 | Batch: 010 / 011 | Total loss: 1.358 | Reg loss: 0.040 | Tree loss: 1.358 | Accuracy: 0.556314 | 1.69 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 216 | Batch: 000 / 011 | Total loss: 1.625 | Reg loss: 0.040 | Tree loss: 1.625 | Accuracy: 0.387500 | 1.693 sec/iter\n",
      "Epoch: 216 | Batch: 001 / 011 | Total loss: 1.579 | Reg loss: 0.040 | Tree loss: 1.579 | Accuracy: 0.419500 | 1.693 sec/iter\n",
      "Epoch: 216 | Batch: 002 / 011 | Total loss: 1.564 | Reg loss: 0.040 | Tree loss: 1.564 | Accuracy: 0.421000 | 1.693 sec/iter\n",
      "Epoch: 216 | Batch: 003 / 011 | Total loss: 1.513 | Reg loss: 0.040 | Tree loss: 1.513 | Accuracy: 0.465000 | 1.692 sec/iter\n",
      "Epoch: 216 | Batch: 004 / 011 | Total loss: 1.487 | Reg loss: 0.040 | Tree loss: 1.487 | Accuracy: 0.478000 | 1.692 sec/iter\n",
      "Epoch: 216 | Batch: 005 / 011 | Total loss: 1.440 | Reg loss: 0.040 | Tree loss: 1.440 | Accuracy: 0.486500 | 1.692 sec/iter\n",
      "Epoch: 216 | Batch: 006 / 011 | Total loss: 1.425 | Reg loss: 0.040 | Tree loss: 1.425 | Accuracy: 0.506500 | 1.692 sec/iter\n",
      "Epoch: 216 | Batch: 007 / 011 | Total loss: 1.424 | Reg loss: 0.040 | Tree loss: 1.424 | Accuracy: 0.506000 | 1.691 sec/iter\n",
      "Epoch: 216 | Batch: 008 / 011 | Total loss: 1.416 | Reg loss: 0.040 | Tree loss: 1.416 | Accuracy: 0.512500 | 1.691 sec/iter\n",
      "Epoch: 216 | Batch: 009 / 011 | Total loss: 1.389 | Reg loss: 0.040 | Tree loss: 1.389 | Accuracy: 0.514000 | 1.691 sec/iter\n",
      "Epoch: 216 | Batch: 010 / 011 | Total loss: 1.429 | Reg loss: 0.040 | Tree loss: 1.429 | Accuracy: 0.457338 | 1.69 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 217 | Batch: 000 / 011 | Total loss: 1.610 | Reg loss: 0.040 | Tree loss: 1.610 | Accuracy: 0.394500 | 1.694 sec/iter\n",
      "Epoch: 217 | Batch: 001 / 011 | Total loss: 1.615 | Reg loss: 0.040 | Tree loss: 1.615 | Accuracy: 0.397000 | 1.694 sec/iter\n",
      "Epoch: 217 | Batch: 002 / 011 | Total loss: 1.544 | Reg loss: 0.040 | Tree loss: 1.544 | Accuracy: 0.437000 | 1.693 sec/iter\n",
      "Epoch: 217 | Batch: 003 / 011 | Total loss: 1.512 | Reg loss: 0.040 | Tree loss: 1.512 | Accuracy: 0.450500 | 1.693 sec/iter\n",
      "Epoch: 217 | Batch: 004 / 011 | Total loss: 1.459 | Reg loss: 0.040 | Tree loss: 1.459 | Accuracy: 0.483500 | 1.693 sec/iter\n",
      "Epoch: 217 | Batch: 005 / 011 | Total loss: 1.448 | Reg loss: 0.040 | Tree loss: 1.448 | Accuracy: 0.497500 | 1.693 sec/iter\n",
      "Epoch: 217 | Batch: 006 / 011 | Total loss: 1.455 | Reg loss: 0.040 | Tree loss: 1.455 | Accuracy: 0.483000 | 1.693 sec/iter\n",
      "Epoch: 217 | Batch: 007 / 011 | Total loss: 1.400 | Reg loss: 0.040 | Tree loss: 1.400 | Accuracy: 0.519000 | 1.693 sec/iter\n",
      "Epoch: 217 | Batch: 008 / 011 | Total loss: 1.400 | Reg loss: 0.040 | Tree loss: 1.400 | Accuracy: 0.506000 | 1.693 sec/iter\n",
      "Epoch: 217 | Batch: 009 / 011 | Total loss: 1.412 | Reg loss: 0.040 | Tree loss: 1.412 | Accuracy: 0.503500 | 1.693 sec/iter\n",
      "Epoch: 217 | Batch: 010 / 011 | Total loss: 1.443 | Reg loss: 0.040 | Tree loss: 1.443 | Accuracy: 0.484642 | 1.692 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 218 | Batch: 000 / 011 | Total loss: 1.619 | Reg loss: 0.040 | Tree loss: 1.619 | Accuracy: 0.405500 | 1.693 sec/iter\n",
      "Epoch: 218 | Batch: 001 / 011 | Total loss: 1.579 | Reg loss: 0.040 | Tree loss: 1.579 | Accuracy: 0.413000 | 1.693 sec/iter\n",
      "Epoch: 218 | Batch: 002 / 011 | Total loss: 1.542 | Reg loss: 0.040 | Tree loss: 1.542 | Accuracy: 0.438500 | 1.692 sec/iter\n",
      "Epoch: 218 | Batch: 003 / 011 | Total loss: 1.491 | Reg loss: 0.040 | Tree loss: 1.491 | Accuracy: 0.466000 | 1.692 sec/iter\n",
      "Epoch: 218 | Batch: 004 / 011 | Total loss: 1.491 | Reg loss: 0.040 | Tree loss: 1.491 | Accuracy: 0.472500 | 1.692 sec/iter\n",
      "Epoch: 218 | Batch: 005 / 011 | Total loss: 1.433 | Reg loss: 0.040 | Tree loss: 1.433 | Accuracy: 0.507500 | 1.692 sec/iter\n",
      "Epoch: 218 | Batch: 006 / 011 | Total loss: 1.442 | Reg loss: 0.040 | Tree loss: 1.442 | Accuracy: 0.502500 | 1.692 sec/iter\n",
      "Epoch: 218 | Batch: 007 / 011 | Total loss: 1.407 | Reg loss: 0.040 | Tree loss: 1.407 | Accuracy: 0.522500 | 1.692 sec/iter\n",
      "Epoch: 218 | Batch: 008 / 011 | Total loss: 1.426 | Reg loss: 0.040 | Tree loss: 1.426 | Accuracy: 0.493500 | 1.692 sec/iter\n",
      "Epoch: 218 | Batch: 009 / 011 | Total loss: 1.419 | Reg loss: 0.040 | Tree loss: 1.419 | Accuracy: 0.497000 | 1.692 sec/iter\n",
      "Epoch: 218 | Batch: 010 / 011 | Total loss: 1.479 | Reg loss: 0.040 | Tree loss: 1.479 | Accuracy: 0.450512 | 1.691 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 219 | Batch: 000 / 011 | Total loss: 1.619 | Reg loss: 0.040 | Tree loss: 1.619 | Accuracy: 0.400000 | 1.695 sec/iter\n",
      "Epoch: 219 | Batch: 001 / 011 | Total loss: 1.569 | Reg loss: 0.040 | Tree loss: 1.569 | Accuracy: 0.418000 | 1.694 sec/iter\n",
      "Epoch: 219 | Batch: 002 / 011 | Total loss: 1.537 | Reg loss: 0.040 | Tree loss: 1.537 | Accuracy: 0.451500 | 1.694 sec/iter\n",
      "Epoch: 219 | Batch: 003 / 011 | Total loss: 1.523 | Reg loss: 0.040 | Tree loss: 1.523 | Accuracy: 0.454000 | 1.694 sec/iter\n",
      "Epoch: 219 | Batch: 004 / 011 | Total loss: 1.484 | Reg loss: 0.040 | Tree loss: 1.484 | Accuracy: 0.471000 | 1.694 sec/iter\n",
      "Epoch: 219 | Batch: 005 / 011 | Total loss: 1.433 | Reg loss: 0.040 | Tree loss: 1.433 | Accuracy: 0.493000 | 1.694 sec/iter\n",
      "Epoch: 219 | Batch: 006 / 011 | Total loss: 1.438 | Reg loss: 0.040 | Tree loss: 1.438 | Accuracy: 0.511000 | 1.694 sec/iter\n",
      "Epoch: 219 | Batch: 007 / 011 | Total loss: 1.403 | Reg loss: 0.040 | Tree loss: 1.403 | Accuracy: 0.495500 | 1.694 sec/iter\n",
      "Epoch: 219 | Batch: 008 / 011 | Total loss: 1.403 | Reg loss: 0.040 | Tree loss: 1.403 | Accuracy: 0.508500 | 1.694 sec/iter\n",
      "Epoch: 219 | Batch: 009 / 011 | Total loss: 1.455 | Reg loss: 0.040 | Tree loss: 1.455 | Accuracy: 0.472000 | 1.694 sec/iter\n",
      "Epoch: 219 | Batch: 010 / 011 | Total loss: 1.364 | Reg loss: 0.040 | Tree loss: 1.364 | Accuracy: 0.515358 | 1.693 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 220 | Batch: 000 / 011 | Total loss: 1.593 | Reg loss: 0.040 | Tree loss: 1.593 | Accuracy: 0.406000 | 1.694 sec/iter\n",
      "Epoch: 220 | Batch: 001 / 011 | Total loss: 1.616 | Reg loss: 0.040 | Tree loss: 1.616 | Accuracy: 0.391500 | 1.694 sec/iter\n",
      "Epoch: 220 | Batch: 002 / 011 | Total loss: 1.555 | Reg loss: 0.040 | Tree loss: 1.555 | Accuracy: 0.436500 | 1.693 sec/iter\n",
      "Epoch: 220 | Batch: 003 / 011 | Total loss: 1.521 | Reg loss: 0.040 | Tree loss: 1.521 | Accuracy: 0.440500 | 1.693 sec/iter\n",
      "Epoch: 220 | Batch: 004 / 011 | Total loss: 1.469 | Reg loss: 0.040 | Tree loss: 1.469 | Accuracy: 0.484500 | 1.693 sec/iter\n",
      "Epoch: 220 | Batch: 005 / 011 | Total loss: 1.445 | Reg loss: 0.040 | Tree loss: 1.445 | Accuracy: 0.479000 | 1.693 sec/iter\n",
      "Epoch: 220 | Batch: 006 / 011 | Total loss: 1.425 | Reg loss: 0.040 | Tree loss: 1.425 | Accuracy: 0.508000 | 1.693 sec/iter\n",
      "Epoch: 220 | Batch: 007 / 011 | Total loss: 1.408 | Reg loss: 0.040 | Tree loss: 1.408 | Accuracy: 0.506000 | 1.693 sec/iter\n",
      "Epoch: 220 | Batch: 008 / 011 | Total loss: 1.408 | Reg loss: 0.040 | Tree loss: 1.408 | Accuracy: 0.507000 | 1.693 sec/iter\n",
      "Epoch: 220 | Batch: 009 / 011 | Total loss: 1.414 | Reg loss: 0.040 | Tree loss: 1.414 | Accuracy: 0.496000 | 1.693 sec/iter\n",
      "Epoch: 220 | Batch: 010 / 011 | Total loss: 1.357 | Reg loss: 0.040 | Tree loss: 1.357 | Accuracy: 0.522184 | 1.692 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 221 | Batch: 000 / 011 | Total loss: 1.599 | Reg loss: 0.040 | Tree loss: 1.599 | Accuracy: 0.398500 | 1.696 sec/iter\n",
      "Epoch: 221 | Batch: 001 / 011 | Total loss: 1.575 | Reg loss: 0.040 | Tree loss: 1.575 | Accuracy: 0.437000 | 1.695 sec/iter\n",
      "Epoch: 221 | Batch: 002 / 011 | Total loss: 1.516 | Reg loss: 0.040 | Tree loss: 1.516 | Accuracy: 0.470500 | 1.695 sec/iter\n",
      "Epoch: 221 | Batch: 003 / 011 | Total loss: 1.494 | Reg loss: 0.040 | Tree loss: 1.494 | Accuracy: 0.477500 | 1.695 sec/iter\n",
      "Epoch: 221 | Batch: 004 / 011 | Total loss: 1.478 | Reg loss: 0.040 | Tree loss: 1.478 | Accuracy: 0.476000 | 1.695 sec/iter\n",
      "Epoch: 221 | Batch: 005 / 011 | Total loss: 1.469 | Reg loss: 0.040 | Tree loss: 1.469 | Accuracy: 0.471500 | 1.695 sec/iter\n",
      "Epoch: 221 | Batch: 006 / 011 | Total loss: 1.429 | Reg loss: 0.040 | Tree loss: 1.429 | Accuracy: 0.485500 | 1.695 sec/iter\n",
      "Epoch: 221 | Batch: 007 / 011 | Total loss: 1.418 | Reg loss: 0.040 | Tree loss: 1.418 | Accuracy: 0.498000 | 1.695 sec/iter\n",
      "Epoch: 221 | Batch: 008 / 011 | Total loss: 1.424 | Reg loss: 0.040 | Tree loss: 1.424 | Accuracy: 0.496500 | 1.695 sec/iter\n",
      "Epoch: 221 | Batch: 009 / 011 | Total loss: 1.427 | Reg loss: 0.040 | Tree loss: 1.427 | Accuracy: 0.476500 | 1.695 sec/iter\n",
      "Epoch: 221 | Batch: 010 / 011 | Total loss: 1.434 | Reg loss: 0.040 | Tree loss: 1.434 | Accuracy: 0.484642 | 1.694 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 222 | Batch: 000 / 011 | Total loss: 1.604 | Reg loss: 0.040 | Tree loss: 1.604 | Accuracy: 0.414000 | 1.695 sec/iter\n",
      "Epoch: 222 | Batch: 001 / 011 | Total loss: 1.566 | Reg loss: 0.040 | Tree loss: 1.566 | Accuracy: 0.410000 | 1.695 sec/iter\n",
      "Epoch: 222 | Batch: 002 / 011 | Total loss: 1.551 | Reg loss: 0.040 | Tree loss: 1.551 | Accuracy: 0.422500 | 1.694 sec/iter\n",
      "Epoch: 222 | Batch: 003 / 011 | Total loss: 1.500 | Reg loss: 0.040 | Tree loss: 1.500 | Accuracy: 0.469000 | 1.694 sec/iter\n",
      "Epoch: 222 | Batch: 004 / 011 | Total loss: 1.467 | Reg loss: 0.040 | Tree loss: 1.467 | Accuracy: 0.458500 | 1.694 sec/iter\n",
      "Epoch: 222 | Batch: 005 / 011 | Total loss: 1.475 | Reg loss: 0.040 | Tree loss: 1.475 | Accuracy: 0.464000 | 1.694 sec/iter\n",
      "Epoch: 222 | Batch: 006 / 011 | Total loss: 1.434 | Reg loss: 0.040 | Tree loss: 1.434 | Accuracy: 0.500000 | 1.694 sec/iter\n",
      "Epoch: 222 | Batch: 007 / 011 | Total loss: 1.418 | Reg loss: 0.040 | Tree loss: 1.418 | Accuracy: 0.495000 | 1.694 sec/iter\n",
      "Epoch: 222 | Batch: 008 / 011 | Total loss: 1.417 | Reg loss: 0.040 | Tree loss: 1.417 | Accuracy: 0.511500 | 1.694 sec/iter\n",
      "Epoch: 222 | Batch: 009 / 011 | Total loss: 1.433 | Reg loss: 0.040 | Tree loss: 1.433 | Accuracy: 0.486000 | 1.694 sec/iter\n",
      "Epoch: 222 | Batch: 010 / 011 | Total loss: 1.373 | Reg loss: 0.040 | Tree loss: 1.373 | Accuracy: 0.508532 | 1.693 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 223 | Batch: 000 / 011 | Total loss: 1.613 | Reg loss: 0.040 | Tree loss: 1.613 | Accuracy: 0.386000 | 1.697 sec/iter\n",
      "Epoch: 223 | Batch: 001 / 011 | Total loss: 1.561 | Reg loss: 0.040 | Tree loss: 1.561 | Accuracy: 0.409500 | 1.696 sec/iter\n",
      "Epoch: 223 | Batch: 002 / 011 | Total loss: 1.549 | Reg loss: 0.040 | Tree loss: 1.549 | Accuracy: 0.431000 | 1.696 sec/iter\n",
      "Epoch: 223 | Batch: 003 / 011 | Total loss: 1.526 | Reg loss: 0.040 | Tree loss: 1.526 | Accuracy: 0.450500 | 1.696 sec/iter\n",
      "Epoch: 223 | Batch: 004 / 011 | Total loss: 1.458 | Reg loss: 0.040 | Tree loss: 1.458 | Accuracy: 0.495000 | 1.696 sec/iter\n",
      "Epoch: 223 | Batch: 005 / 011 | Total loss: 1.460 | Reg loss: 0.040 | Tree loss: 1.460 | Accuracy: 0.485500 | 1.695 sec/iter\n",
      "Epoch: 223 | Batch: 006 / 011 | Total loss: 1.409 | Reg loss: 0.040 | Tree loss: 1.409 | Accuracy: 0.514000 | 1.695 sec/iter\n",
      "Epoch: 223 | Batch: 007 / 011 | Total loss: 1.412 | Reg loss: 0.040 | Tree loss: 1.412 | Accuracy: 0.519000 | 1.695 sec/iter\n",
      "Epoch: 223 | Batch: 008 / 011 | Total loss: 1.438 | Reg loss: 0.040 | Tree loss: 1.438 | Accuracy: 0.484000 | 1.695 sec/iter\n",
      "Epoch: 223 | Batch: 009 / 011 | Total loss: 1.421 | Reg loss: 0.040 | Tree loss: 1.421 | Accuracy: 0.489000 | 1.694 sec/iter\n",
      "Epoch: 223 | Batch: 010 / 011 | Total loss: 1.408 | Reg loss: 0.040 | Tree loss: 1.408 | Accuracy: 0.494881 | 1.694 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 224 | Batch: 000 / 011 | Total loss: 1.610 | Reg loss: 0.040 | Tree loss: 1.610 | Accuracy: 0.393500 | 1.697 sec/iter\n",
      "Epoch: 224 | Batch: 001 / 011 | Total loss: 1.596 | Reg loss: 0.040 | Tree loss: 1.596 | Accuracy: 0.383500 | 1.697 sec/iter\n",
      "Epoch: 224 | Batch: 002 / 011 | Total loss: 1.554 | Reg loss: 0.040 | Tree loss: 1.554 | Accuracy: 0.421500 | 1.697 sec/iter\n",
      "Epoch: 224 | Batch: 003 / 011 | Total loss: 1.491 | Reg loss: 0.040 | Tree loss: 1.491 | Accuracy: 0.459000 | 1.697 sec/iter\n",
      "Epoch: 224 | Batch: 004 / 011 | Total loss: 1.475 | Reg loss: 0.040 | Tree loss: 1.475 | Accuracy: 0.450500 | 1.697 sec/iter\n",
      "Epoch: 224 | Batch: 005 / 011 | Total loss: 1.437 | Reg loss: 0.040 | Tree loss: 1.437 | Accuracy: 0.495000 | 1.697 sec/iter\n",
      "Epoch: 224 | Batch: 006 / 011 | Total loss: 1.436 | Reg loss: 0.040 | Tree loss: 1.436 | Accuracy: 0.509000 | 1.697 sec/iter\n",
      "Epoch: 224 | Batch: 007 / 011 | Total loss: 1.421 | Reg loss: 0.040 | Tree loss: 1.421 | Accuracy: 0.499000 | 1.696 sec/iter\n",
      "Epoch: 224 | Batch: 008 / 011 | Total loss: 1.395 | Reg loss: 0.040 | Tree loss: 1.395 | Accuracy: 0.513000 | 1.696 sec/iter\n",
      "Epoch: 224 | Batch: 009 / 011 | Total loss: 1.411 | Reg loss: 0.040 | Tree loss: 1.411 | Accuracy: 0.508500 | 1.696 sec/iter\n",
      "Epoch: 224 | Batch: 010 / 011 | Total loss: 1.420 | Reg loss: 0.040 | Tree loss: 1.420 | Accuracy: 0.508532 | 1.696 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 225 | Batch: 000 / 011 | Total loss: 1.611 | Reg loss: 0.040 | Tree loss: 1.611 | Accuracy: 0.397000 | 1.696 sec/iter\n",
      "Epoch: 225 | Batch: 001 / 011 | Total loss: 1.579 | Reg loss: 0.040 | Tree loss: 1.579 | Accuracy: 0.409500 | 1.696 sec/iter\n",
      "Epoch: 225 | Batch: 002 / 011 | Total loss: 1.532 | Reg loss: 0.040 | Tree loss: 1.532 | Accuracy: 0.445500 | 1.696 sec/iter\n",
      "Epoch: 225 | Batch: 003 / 011 | Total loss: 1.495 | Reg loss: 0.040 | Tree loss: 1.495 | Accuracy: 0.463000 | 1.696 sec/iter\n",
      "Epoch: 225 | Batch: 004 / 011 | Total loss: 1.452 | Reg loss: 0.040 | Tree loss: 1.452 | Accuracy: 0.491500 | 1.696 sec/iter\n",
      "Epoch: 225 | Batch: 005 / 011 | Total loss: 1.446 | Reg loss: 0.040 | Tree loss: 1.446 | Accuracy: 0.495500 | 1.696 sec/iter\n",
      "Epoch: 225 | Batch: 006 / 011 | Total loss: 1.450 | Reg loss: 0.040 | Tree loss: 1.450 | Accuracy: 0.473500 | 1.696 sec/iter\n",
      "Epoch: 225 | Batch: 007 / 011 | Total loss: 1.423 | Reg loss: 0.040 | Tree loss: 1.423 | Accuracy: 0.489000 | 1.696 sec/iter\n",
      "Epoch: 225 | Batch: 008 / 011 | Total loss: 1.428 | Reg loss: 0.040 | Tree loss: 1.428 | Accuracy: 0.508000 | 1.695 sec/iter\n",
      "Epoch: 225 | Batch: 009 / 011 | Total loss: 1.405 | Reg loss: 0.040 | Tree loss: 1.405 | Accuracy: 0.491000 | 1.695 sec/iter\n",
      "Epoch: 225 | Batch: 010 / 011 | Total loss: 1.449 | Reg loss: 0.040 | Tree loss: 1.449 | Accuracy: 0.450512 | 1.695 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 226 | Batch: 000 / 011 | Total loss: 1.627 | Reg loss: 0.040 | Tree loss: 1.627 | Accuracy: 0.381000 | 1.698 sec/iter\n",
      "Epoch: 226 | Batch: 001 / 011 | Total loss: 1.596 | Reg loss: 0.040 | Tree loss: 1.596 | Accuracy: 0.395000 | 1.698 sec/iter\n",
      "Epoch: 226 | Batch: 002 / 011 | Total loss: 1.553 | Reg loss: 0.040 | Tree loss: 1.553 | Accuracy: 0.425000 | 1.698 sec/iter\n",
      "Epoch: 226 | Batch: 003 / 011 | Total loss: 1.512 | Reg loss: 0.040 | Tree loss: 1.512 | Accuracy: 0.439500 | 1.698 sec/iter\n",
      "Epoch: 226 | Batch: 004 / 011 | Total loss: 1.479 | Reg loss: 0.040 | Tree loss: 1.479 | Accuracy: 0.471000 | 1.698 sec/iter\n",
      "Epoch: 226 | Batch: 005 / 011 | Total loss: 1.444 | Reg loss: 0.040 | Tree loss: 1.444 | Accuracy: 0.489000 | 1.698 sec/iter\n",
      "Epoch: 226 | Batch: 006 / 011 | Total loss: 1.397 | Reg loss: 0.040 | Tree loss: 1.397 | Accuracy: 0.528500 | 1.697 sec/iter\n",
      "Epoch: 226 | Batch: 007 / 011 | Total loss: 1.401 | Reg loss: 0.040 | Tree loss: 1.401 | Accuracy: 0.493000 | 1.697 sec/iter\n",
      "Epoch: 226 | Batch: 008 / 011 | Total loss: 1.423 | Reg loss: 0.040 | Tree loss: 1.423 | Accuracy: 0.499000 | 1.697 sec/iter\n",
      "Epoch: 226 | Batch: 009 / 011 | Total loss: 1.416 | Reg loss: 0.040 | Tree loss: 1.416 | Accuracy: 0.490500 | 1.697 sec/iter\n",
      "Epoch: 226 | Batch: 010 / 011 | Total loss: 1.406 | Reg loss: 0.040 | Tree loss: 1.406 | Accuracy: 0.508532 | 1.697 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 227 | Batch: 000 / 011 | Total loss: 1.614 | Reg loss: 0.040 | Tree loss: 1.614 | Accuracy: 0.383000 | 1.697 sec/iter\n",
      "Epoch: 227 | Batch: 001 / 011 | Total loss: 1.590 | Reg loss: 0.040 | Tree loss: 1.590 | Accuracy: 0.401500 | 1.697 sec/iter\n",
      "Epoch: 227 | Batch: 002 / 011 | Total loss: 1.529 | Reg loss: 0.040 | Tree loss: 1.529 | Accuracy: 0.427000 | 1.697 sec/iter\n",
      "Epoch: 227 | Batch: 003 / 011 | Total loss: 1.509 | Reg loss: 0.040 | Tree loss: 1.509 | Accuracy: 0.446500 | 1.697 sec/iter\n",
      "Epoch: 227 | Batch: 004 / 011 | Total loss: 1.440 | Reg loss: 0.040 | Tree loss: 1.440 | Accuracy: 0.486000 | 1.697 sec/iter\n",
      "Epoch: 227 | Batch: 005 / 011 | Total loss: 1.444 | Reg loss: 0.040 | Tree loss: 1.444 | Accuracy: 0.488500 | 1.697 sec/iter\n",
      "Epoch: 227 | Batch: 006 / 011 | Total loss: 1.440 | Reg loss: 0.040 | Tree loss: 1.440 | Accuracy: 0.506000 | 1.697 sec/iter\n",
      "Epoch: 227 | Batch: 007 / 011 | Total loss: 1.423 | Reg loss: 0.040 | Tree loss: 1.423 | Accuracy: 0.504000 | 1.696 sec/iter\n",
      "Epoch: 227 | Batch: 008 / 011 | Total loss: 1.418 | Reg loss: 0.040 | Tree loss: 1.418 | Accuracy: 0.488500 | 1.696 sec/iter\n",
      "Epoch: 227 | Batch: 009 / 011 | Total loss: 1.417 | Reg loss: 0.040 | Tree loss: 1.417 | Accuracy: 0.493500 | 1.696 sec/iter\n",
      "Epoch: 227 | Batch: 010 / 011 | Total loss: 1.407 | Reg loss: 0.040 | Tree loss: 1.407 | Accuracy: 0.419795 | 1.696 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 228 | Batch: 000 / 011 | Total loss: 1.616 | Reg loss: 0.040 | Tree loss: 1.616 | Accuracy: 0.390000 | 1.699 sec/iter\n",
      "Epoch: 228 | Batch: 001 / 011 | Total loss: 1.587 | Reg loss: 0.040 | Tree loss: 1.587 | Accuracy: 0.400000 | 1.699 sec/iter\n",
      "Epoch: 228 | Batch: 002 / 011 | Total loss: 1.538 | Reg loss: 0.040 | Tree loss: 1.538 | Accuracy: 0.437000 | 1.699 sec/iter\n",
      "Epoch: 228 | Batch: 003 / 011 | Total loss: 1.501 | Reg loss: 0.040 | Tree loss: 1.501 | Accuracy: 0.442000 | 1.699 sec/iter\n",
      "Epoch: 228 | Batch: 004 / 011 | Total loss: 1.486 | Reg loss: 0.040 | Tree loss: 1.486 | Accuracy: 0.454000 | 1.698 sec/iter\n",
      "Epoch: 228 | Batch: 005 / 011 | Total loss: 1.418 | Reg loss: 0.040 | Tree loss: 1.418 | Accuracy: 0.502500 | 1.698 sec/iter\n",
      "Epoch: 228 | Batch: 006 / 011 | Total loss: 1.422 | Reg loss: 0.040 | Tree loss: 1.422 | Accuracy: 0.513000 | 1.698 sec/iter\n",
      "Epoch: 228 | Batch: 007 / 011 | Total loss: 1.424 | Reg loss: 0.040 | Tree loss: 1.424 | Accuracy: 0.505500 | 1.698 sec/iter\n",
      "Epoch: 228 | Batch: 008 / 011 | Total loss: 1.407 | Reg loss: 0.040 | Tree loss: 1.407 | Accuracy: 0.495500 | 1.698 sec/iter\n",
      "Epoch: 228 | Batch: 009 / 011 | Total loss: 1.426 | Reg loss: 0.040 | Tree loss: 1.426 | Accuracy: 0.484000 | 1.698 sec/iter\n",
      "Epoch: 228 | Batch: 010 / 011 | Total loss: 1.380 | Reg loss: 0.040 | Tree loss: 1.380 | Accuracy: 0.546075 | 1.697 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 229 | Batch: 000 / 011 | Total loss: 1.618 | Reg loss: 0.040 | Tree loss: 1.618 | Accuracy: 0.391000 | 1.697 sec/iter\n",
      "Epoch: 229 | Batch: 001 / 011 | Total loss: 1.584 | Reg loss: 0.040 | Tree loss: 1.584 | Accuracy: 0.413500 | 1.697 sec/iter\n",
      "Epoch: 229 | Batch: 002 / 011 | Total loss: 1.529 | Reg loss: 0.040 | Tree loss: 1.529 | Accuracy: 0.454000 | 1.697 sec/iter\n",
      "Epoch: 229 | Batch: 003 / 011 | Total loss: 1.499 | Reg loss: 0.040 | Tree loss: 1.499 | Accuracy: 0.463500 | 1.697 sec/iter\n",
      "Epoch: 229 | Batch: 004 / 011 | Total loss: 1.455 | Reg loss: 0.040 | Tree loss: 1.455 | Accuracy: 0.487000 | 1.697 sec/iter\n",
      "Epoch: 229 | Batch: 005 / 011 | Total loss: 1.449 | Reg loss: 0.040 | Tree loss: 1.449 | Accuracy: 0.507500 | 1.697 sec/iter\n",
      "Epoch: 229 | Batch: 006 / 011 | Total loss: 1.424 | Reg loss: 0.040 | Tree loss: 1.424 | Accuracy: 0.518000 | 1.697 sec/iter\n",
      "Epoch: 229 | Batch: 007 / 011 | Total loss: 1.409 | Reg loss: 0.040 | Tree loss: 1.409 | Accuracy: 0.498500 | 1.697 sec/iter\n",
      "Epoch: 229 | Batch: 008 / 011 | Total loss: 1.438 | Reg loss: 0.040 | Tree loss: 1.438 | Accuracy: 0.476000 | 1.696 sec/iter\n",
      "Epoch: 229 | Batch: 009 / 011 | Total loss: 1.410 | Reg loss: 0.040 | Tree loss: 1.410 | Accuracy: 0.509500 | 1.696 sec/iter\n",
      "Epoch: 229 | Batch: 010 / 011 | Total loss: 1.441 | Reg loss: 0.040 | Tree loss: 1.441 | Accuracy: 0.484642 | 1.696 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 230 | Batch: 000 / 011 | Total loss: 1.609 | Reg loss: 0.040 | Tree loss: 1.609 | Accuracy: 0.412000 | 1.698 sec/iter\n",
      "Epoch: 230 | Batch: 001 / 011 | Total loss: 1.601 | Reg loss: 0.040 | Tree loss: 1.601 | Accuracy: 0.400000 | 1.698 sec/iter\n",
      "Epoch: 230 | Batch: 002 / 011 | Total loss: 1.551 | Reg loss: 0.040 | Tree loss: 1.551 | Accuracy: 0.426500 | 1.698 sec/iter\n",
      "Epoch: 230 | Batch: 003 / 011 | Total loss: 1.522 | Reg loss: 0.040 | Tree loss: 1.522 | Accuracy: 0.434500 | 1.698 sec/iter\n",
      "Epoch: 230 | Batch: 004 / 011 | Total loss: 1.461 | Reg loss: 0.040 | Tree loss: 1.461 | Accuracy: 0.472500 | 1.697 sec/iter\n",
      "Epoch: 230 | Batch: 005 / 011 | Total loss: 1.437 | Reg loss: 0.040 | Tree loss: 1.437 | Accuracy: 0.489000 | 1.697 sec/iter\n",
      "Epoch: 230 | Batch: 006 / 011 | Total loss: 1.446 | Reg loss: 0.040 | Tree loss: 1.446 | Accuracy: 0.495500 | 1.697 sec/iter\n",
      "Epoch: 230 | Batch: 007 / 011 | Total loss: 1.416 | Reg loss: 0.040 | Tree loss: 1.416 | Accuracy: 0.512000 | 1.696 sec/iter\n",
      "Epoch: 230 | Batch: 008 / 011 | Total loss: 1.413 | Reg loss: 0.040 | Tree loss: 1.413 | Accuracy: 0.511000 | 1.696 sec/iter\n",
      "Epoch: 230 | Batch: 009 / 011 | Total loss: 1.416 | Reg loss: 0.040 | Tree loss: 1.416 | Accuracy: 0.507000 | 1.696 sec/iter\n",
      "Epoch: 230 | Batch: 010 / 011 | Total loss: 1.354 | Reg loss: 0.040 | Tree loss: 1.354 | Accuracy: 0.508532 | 1.696 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 231 | Batch: 000 / 011 | Total loss: 1.598 | Reg loss: 0.040 | Tree loss: 1.598 | Accuracy: 0.406000 | 1.698 sec/iter\n",
      "Epoch: 231 | Batch: 001 / 011 | Total loss: 1.576 | Reg loss: 0.040 | Tree loss: 1.576 | Accuracy: 0.425500 | 1.697 sec/iter\n",
      "Epoch: 231 | Batch: 002 / 011 | Total loss: 1.553 | Reg loss: 0.040 | Tree loss: 1.553 | Accuracy: 0.422500 | 1.697 sec/iter\n",
      "Epoch: 231 | Batch: 003 / 011 | Total loss: 1.502 | Reg loss: 0.040 | Tree loss: 1.502 | Accuracy: 0.451500 | 1.697 sec/iter\n",
      "Epoch: 231 | Batch: 004 / 011 | Total loss: 1.468 | Reg loss: 0.040 | Tree loss: 1.468 | Accuracy: 0.488000 | 1.697 sec/iter\n",
      "Epoch: 231 | Batch: 005 / 011 | Total loss: 1.443 | Reg loss: 0.040 | Tree loss: 1.443 | Accuracy: 0.503000 | 1.697 sec/iter\n",
      "Epoch: 231 | Batch: 006 / 011 | Total loss: 1.432 | Reg loss: 0.040 | Tree loss: 1.432 | Accuracy: 0.495500 | 1.697 sec/iter\n",
      "Epoch: 231 | Batch: 007 / 011 | Total loss: 1.437 | Reg loss: 0.040 | Tree loss: 1.437 | Accuracy: 0.478000 | 1.697 sec/iter\n",
      "Epoch: 231 | Batch: 008 / 011 | Total loss: 1.417 | Reg loss: 0.040 | Tree loss: 1.417 | Accuracy: 0.513000 | 1.697 sec/iter\n",
      "Epoch: 231 | Batch: 009 / 011 | Total loss: 1.415 | Reg loss: 0.040 | Tree loss: 1.415 | Accuracy: 0.502500 | 1.696 sec/iter\n",
      "Epoch: 231 | Batch: 010 / 011 | Total loss: 1.357 | Reg loss: 0.040 | Tree loss: 1.357 | Accuracy: 0.522184 | 1.696 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 232 | Batch: 000 / 011 | Total loss: 1.610 | Reg loss: 0.040 | Tree loss: 1.610 | Accuracy: 0.407500 | 1.696 sec/iter\n",
      "Epoch: 232 | Batch: 001 / 011 | Total loss: 1.569 | Reg loss: 0.040 | Tree loss: 1.569 | Accuracy: 0.426500 | 1.696 sec/iter\n",
      "Epoch: 232 | Batch: 002 / 011 | Total loss: 1.533 | Reg loss: 0.040 | Tree loss: 1.533 | Accuracy: 0.438000 | 1.696 sec/iter\n",
      "Epoch: 232 | Batch: 003 / 011 | Total loss: 1.502 | Reg loss: 0.040 | Tree loss: 1.502 | Accuracy: 0.464500 | 1.696 sec/iter\n",
      "Epoch: 232 | Batch: 004 / 011 | Total loss: 1.479 | Reg loss: 0.040 | Tree loss: 1.479 | Accuracy: 0.462000 | 1.696 sec/iter\n",
      "Epoch: 232 | Batch: 005 / 011 | Total loss: 1.451 | Reg loss: 0.040 | Tree loss: 1.451 | Accuracy: 0.486000 | 1.695 sec/iter\n",
      "Epoch: 232 | Batch: 006 / 011 | Total loss: 1.420 | Reg loss: 0.040 | Tree loss: 1.420 | Accuracy: 0.494000 | 1.695 sec/iter\n",
      "Epoch: 232 | Batch: 007 / 011 | Total loss: 1.402 | Reg loss: 0.040 | Tree loss: 1.402 | Accuracy: 0.503500 | 1.695 sec/iter\n",
      "Epoch: 232 | Batch: 008 / 011 | Total loss: 1.424 | Reg loss: 0.040 | Tree loss: 1.424 | Accuracy: 0.512500 | 1.695 sec/iter\n",
      "Epoch: 232 | Batch: 009 / 011 | Total loss: 1.423 | Reg loss: 0.040 | Tree loss: 1.423 | Accuracy: 0.511500 | 1.695 sec/iter\n",
      "Epoch: 232 | Batch: 010 / 011 | Total loss: 1.455 | Reg loss: 0.040 | Tree loss: 1.455 | Accuracy: 0.467577 | 1.694 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 233 | Batch: 000 / 011 | Total loss: 1.620 | Reg loss: 0.040 | Tree loss: 1.620 | Accuracy: 0.412500 | 1.697 sec/iter\n",
      "Epoch: 233 | Batch: 001 / 011 | Total loss: 1.581 | Reg loss: 0.040 | Tree loss: 1.581 | Accuracy: 0.420500 | 1.697 sec/iter\n",
      "Epoch: 233 | Batch: 002 / 011 | Total loss: 1.530 | Reg loss: 0.040 | Tree loss: 1.530 | Accuracy: 0.452000 | 1.697 sec/iter\n",
      "Epoch: 233 | Batch: 003 / 011 | Total loss: 1.503 | Reg loss: 0.040 | Tree loss: 1.503 | Accuracy: 0.456500 | 1.696 sec/iter\n",
      "Epoch: 233 | Batch: 004 / 011 | Total loss: 1.466 | Reg loss: 0.040 | Tree loss: 1.466 | Accuracy: 0.469000 | 1.696 sec/iter\n",
      "Epoch: 233 | Batch: 005 / 011 | Total loss: 1.440 | Reg loss: 0.040 | Tree loss: 1.440 | Accuracy: 0.496500 | 1.696 sec/iter\n",
      "Epoch: 233 | Batch: 006 / 011 | Total loss: 1.443 | Reg loss: 0.040 | Tree loss: 1.443 | Accuracy: 0.491000 | 1.696 sec/iter\n",
      "Epoch: 233 | Batch: 007 / 011 | Total loss: 1.424 | Reg loss: 0.040 | Tree loss: 1.424 | Accuracy: 0.494500 | 1.696 sec/iter\n",
      "Epoch: 233 | Batch: 008 / 011 | Total loss: 1.405 | Reg loss: 0.040 | Tree loss: 1.405 | Accuracy: 0.508500 | 1.696 sec/iter\n",
      "Epoch: 233 | Batch: 009 / 011 | Total loss: 1.427 | Reg loss: 0.040 | Tree loss: 1.427 | Accuracy: 0.473500 | 1.696 sec/iter\n",
      "Epoch: 233 | Batch: 010 / 011 | Total loss: 1.397 | Reg loss: 0.040 | Tree loss: 1.397 | Accuracy: 0.498294 | 1.695 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 234 | Batch: 000 / 011 | Total loss: 1.627 | Reg loss: 0.040 | Tree loss: 1.627 | Accuracy: 0.398500 | 1.695 sec/iter\n",
      "Epoch: 234 | Batch: 001 / 011 | Total loss: 1.575 | Reg loss: 0.040 | Tree loss: 1.575 | Accuracy: 0.425500 | 1.695 sec/iter\n",
      "Epoch: 234 | Batch: 002 / 011 | Total loss: 1.526 | Reg loss: 0.040 | Tree loss: 1.526 | Accuracy: 0.456000 | 1.695 sec/iter\n",
      "Epoch: 234 | Batch: 003 / 011 | Total loss: 1.495 | Reg loss: 0.040 | Tree loss: 1.495 | Accuracy: 0.455500 | 1.695 sec/iter\n",
      "Epoch: 234 | Batch: 004 / 011 | Total loss: 1.467 | Reg loss: 0.040 | Tree loss: 1.467 | Accuracy: 0.495000 | 1.695 sec/iter\n",
      "Epoch: 234 | Batch: 005 / 011 | Total loss: 1.455 | Reg loss: 0.040 | Tree loss: 1.455 | Accuracy: 0.492000 | 1.695 sec/iter\n",
      "Epoch: 234 | Batch: 006 / 011 | Total loss: 1.434 | Reg loss: 0.040 | Tree loss: 1.434 | Accuracy: 0.494000 | 1.695 sec/iter\n",
      "Epoch: 234 | Batch: 007 / 011 | Total loss: 1.433 | Reg loss: 0.040 | Tree loss: 1.433 | Accuracy: 0.487500 | 1.695 sec/iter\n",
      "Epoch: 234 | Batch: 008 / 011 | Total loss: 1.402 | Reg loss: 0.040 | Tree loss: 1.402 | Accuracy: 0.496500 | 1.694 sec/iter\n",
      "Epoch: 234 | Batch: 009 / 011 | Total loss: 1.408 | Reg loss: 0.040 | Tree loss: 1.408 | Accuracy: 0.499500 | 1.694 sec/iter\n",
      "Epoch: 234 | Batch: 010 / 011 | Total loss: 1.423 | Reg loss: 0.040 | Tree loss: 1.423 | Accuracy: 0.488055 | 1.694 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 235 | Batch: 000 / 011 | Total loss: 1.637 | Reg loss: 0.040 | Tree loss: 1.637 | Accuracy: 0.386500 | 1.696 sec/iter\n",
      "Epoch: 235 | Batch: 001 / 011 | Total loss: 1.581 | Reg loss: 0.040 | Tree loss: 1.581 | Accuracy: 0.416500 | 1.696 sec/iter\n",
      "Epoch: 235 | Batch: 002 / 011 | Total loss: 1.544 | Reg loss: 0.040 | Tree loss: 1.544 | Accuracy: 0.440000 | 1.696 sec/iter\n",
      "Epoch: 235 | Batch: 003 / 011 | Total loss: 1.512 | Reg loss: 0.040 | Tree loss: 1.512 | Accuracy: 0.460500 | 1.696 sec/iter\n",
      "Epoch: 235 | Batch: 004 / 011 | Total loss: 1.455 | Reg loss: 0.040 | Tree loss: 1.455 | Accuracy: 0.485500 | 1.696 sec/iter\n",
      "Epoch: 235 | Batch: 005 / 011 | Total loss: 1.436 | Reg loss: 0.040 | Tree loss: 1.436 | Accuracy: 0.499000 | 1.696 sec/iter\n",
      "Epoch: 235 | Batch: 006 / 011 | Total loss: 1.443 | Reg loss: 0.040 | Tree loss: 1.443 | Accuracy: 0.489000 | 1.696 sec/iter\n",
      "Epoch: 235 | Batch: 007 / 011 | Total loss: 1.412 | Reg loss: 0.040 | Tree loss: 1.412 | Accuracy: 0.490500 | 1.695 sec/iter\n",
      "Epoch: 235 | Batch: 008 / 011 | Total loss: 1.389 | Reg loss: 0.040 | Tree loss: 1.389 | Accuracy: 0.500500 | 1.695 sec/iter\n",
      "Epoch: 235 | Batch: 009 / 011 | Total loss: 1.397 | Reg loss: 0.040 | Tree loss: 1.397 | Accuracy: 0.503000 | 1.695 sec/iter\n",
      "Epoch: 235 | Batch: 010 / 011 | Total loss: 1.423 | Reg loss: 0.040 | Tree loss: 1.423 | Accuracy: 0.505119 | 1.695 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 236 | Batch: 000 / 011 | Total loss: 1.645 | Reg loss: 0.040 | Tree loss: 1.645 | Accuracy: 0.386000 | 1.695 sec/iter\n",
      "Epoch: 236 | Batch: 001 / 011 | Total loss: 1.602 | Reg loss: 0.040 | Tree loss: 1.602 | Accuracy: 0.389000 | 1.695 sec/iter\n",
      "Epoch: 236 | Batch: 002 / 011 | Total loss: 1.537 | Reg loss: 0.040 | Tree loss: 1.537 | Accuracy: 0.443500 | 1.695 sec/iter\n",
      "Epoch: 236 | Batch: 003 / 011 | Total loss: 1.484 | Reg loss: 0.040 | Tree loss: 1.484 | Accuracy: 0.484000 | 1.694 sec/iter\n",
      "Epoch: 236 | Batch: 004 / 011 | Total loss: 1.471 | Reg loss: 0.040 | Tree loss: 1.471 | Accuracy: 0.477500 | 1.694 sec/iter\n",
      "Epoch: 236 | Batch: 005 / 011 | Total loss: 1.439 | Reg loss: 0.040 | Tree loss: 1.439 | Accuracy: 0.499000 | 1.694 sec/iter\n",
      "Epoch: 236 | Batch: 006 / 011 | Total loss: 1.432 | Reg loss: 0.040 | Tree loss: 1.432 | Accuracy: 0.488000 | 1.694 sec/iter\n",
      "Epoch: 236 | Batch: 007 / 011 | Total loss: 1.420 | Reg loss: 0.040 | Tree loss: 1.420 | Accuracy: 0.490500 | 1.694 sec/iter\n",
      "Epoch: 236 | Batch: 008 / 011 | Total loss: 1.399 | Reg loss: 0.040 | Tree loss: 1.399 | Accuracy: 0.511500 | 1.694 sec/iter\n",
      "Epoch: 236 | Batch: 009 / 011 | Total loss: 1.408 | Reg loss: 0.040 | Tree loss: 1.408 | Accuracy: 0.510500 | 1.694 sec/iter\n",
      "Epoch: 236 | Batch: 010 / 011 | Total loss: 1.366 | Reg loss: 0.040 | Tree loss: 1.366 | Accuracy: 0.505119 | 1.693 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 237 | Batch: 000 / 011 | Total loss: 1.614 | Reg loss: 0.040 | Tree loss: 1.614 | Accuracy: 0.399000 | 1.696 sec/iter\n",
      "Epoch: 237 | Batch: 001 / 011 | Total loss: 1.588 | Reg loss: 0.040 | Tree loss: 1.588 | Accuracy: 0.410500 | 1.696 sec/iter\n",
      "Epoch: 237 | Batch: 002 / 011 | Total loss: 1.528 | Reg loss: 0.040 | Tree loss: 1.528 | Accuracy: 0.442000 | 1.695 sec/iter\n",
      "Epoch: 237 | Batch: 003 / 011 | Total loss: 1.490 | Reg loss: 0.040 | Tree loss: 1.490 | Accuracy: 0.465500 | 1.695 sec/iter\n",
      "Epoch: 237 | Batch: 004 / 011 | Total loss: 1.473 | Reg loss: 0.040 | Tree loss: 1.473 | Accuracy: 0.460500 | 1.695 sec/iter\n",
      "Epoch: 237 | Batch: 005 / 011 | Total loss: 1.439 | Reg loss: 0.040 | Tree loss: 1.439 | Accuracy: 0.489000 | 1.694 sec/iter\n",
      "Epoch: 237 | Batch: 006 / 011 | Total loss: 1.433 | Reg loss: 0.040 | Tree loss: 1.433 | Accuracy: 0.509000 | 1.694 sec/iter\n",
      "Epoch: 237 | Batch: 007 / 011 | Total loss: 1.425 | Reg loss: 0.040 | Tree loss: 1.425 | Accuracy: 0.501500 | 1.694 sec/iter\n",
      "Epoch: 237 | Batch: 008 / 011 | Total loss: 1.411 | Reg loss: 0.040 | Tree loss: 1.411 | Accuracy: 0.509000 | 1.694 sec/iter\n",
      "Epoch: 237 | Batch: 009 / 011 | Total loss: 1.428 | Reg loss: 0.040 | Tree loss: 1.428 | Accuracy: 0.488500 | 1.694 sec/iter\n",
      "Epoch: 237 | Batch: 010 / 011 | Total loss: 1.368 | Reg loss: 0.040 | Tree loss: 1.368 | Accuracy: 0.522184 | 1.693 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 238 | Batch: 000 / 011 | Total loss: 1.604 | Reg loss: 0.040 | Tree loss: 1.604 | Accuracy: 0.402000 | 1.695 sec/iter\n",
      "Epoch: 238 | Batch: 001 / 011 | Total loss: 1.583 | Reg loss: 0.040 | Tree loss: 1.583 | Accuracy: 0.419500 | 1.695 sec/iter\n",
      "Epoch: 238 | Batch: 002 / 011 | Total loss: 1.548 | Reg loss: 0.040 | Tree loss: 1.548 | Accuracy: 0.436000 | 1.695 sec/iter\n",
      "Epoch: 238 | Batch: 003 / 011 | Total loss: 1.496 | Reg loss: 0.040 | Tree loss: 1.496 | Accuracy: 0.458500 | 1.695 sec/iter\n",
      "Epoch: 238 | Batch: 004 / 011 | Total loss: 1.466 | Reg loss: 0.040 | Tree loss: 1.466 | Accuracy: 0.479000 | 1.695 sec/iter\n",
      "Epoch: 238 | Batch: 005 / 011 | Total loss: 1.443 | Reg loss: 0.040 | Tree loss: 1.443 | Accuracy: 0.489000 | 1.694 sec/iter\n",
      "Epoch: 238 | Batch: 006 / 011 | Total loss: 1.420 | Reg loss: 0.040 | Tree loss: 1.420 | Accuracy: 0.499000 | 1.694 sec/iter\n",
      "Epoch: 238 | Batch: 007 / 011 | Total loss: 1.438 | Reg loss: 0.040 | Tree loss: 1.438 | Accuracy: 0.503500 | 1.694 sec/iter\n",
      "Epoch: 238 | Batch: 008 / 011 | Total loss: 1.410 | Reg loss: 0.040 | Tree loss: 1.410 | Accuracy: 0.481000 | 1.694 sec/iter\n",
      "Epoch: 238 | Batch: 009 / 011 | Total loss: 1.400 | Reg loss: 0.040 | Tree loss: 1.400 | Accuracy: 0.509500 | 1.694 sec/iter\n",
      "Epoch: 238 | Batch: 010 / 011 | Total loss: 1.387 | Reg loss: 0.040 | Tree loss: 1.387 | Accuracy: 0.522184 | 1.694 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 239 | Batch: 000 / 011 | Total loss: 1.596 | Reg loss: 0.040 | Tree loss: 1.596 | Accuracy: 0.408000 | 1.694 sec/iter\n",
      "Epoch: 239 | Batch: 001 / 011 | Total loss: 1.607 | Reg loss: 0.040 | Tree loss: 1.607 | Accuracy: 0.407500 | 1.693 sec/iter\n",
      "Epoch: 239 | Batch: 002 / 011 | Total loss: 1.517 | Reg loss: 0.040 | Tree loss: 1.517 | Accuracy: 0.447500 | 1.693 sec/iter\n",
      "Epoch: 239 | Batch: 003 / 011 | Total loss: 1.501 | Reg loss: 0.040 | Tree loss: 1.501 | Accuracy: 0.478500 | 1.693 sec/iter\n",
      "Epoch: 239 | Batch: 004 / 011 | Total loss: 1.489 | Reg loss: 0.040 | Tree loss: 1.489 | Accuracy: 0.477000 | 1.693 sec/iter\n",
      "Epoch: 239 | Batch: 005 / 011 | Total loss: 1.454 | Reg loss: 0.040 | Tree loss: 1.454 | Accuracy: 0.497000 | 1.693 sec/iter\n",
      "Epoch: 239 | Batch: 006 / 011 | Total loss: 1.419 | Reg loss: 0.040 | Tree loss: 1.419 | Accuracy: 0.530000 | 1.693 sec/iter\n",
      "Epoch: 239 | Batch: 007 / 011 | Total loss: 1.416 | Reg loss: 0.040 | Tree loss: 1.416 | Accuracy: 0.511500 | 1.693 sec/iter\n",
      "Epoch: 239 | Batch: 008 / 011 | Total loss: 1.411 | Reg loss: 0.040 | Tree loss: 1.411 | Accuracy: 0.504000 | 1.693 sec/iter\n",
      "Epoch: 239 | Batch: 009 / 011 | Total loss: 1.410 | Reg loss: 0.040 | Tree loss: 1.410 | Accuracy: 0.491000 | 1.692 sec/iter\n",
      "Epoch: 239 | Batch: 010 / 011 | Total loss: 1.440 | Reg loss: 0.040 | Tree loss: 1.440 | Accuracy: 0.460751 | 1.692 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 240 | Batch: 000 / 011 | Total loss: 1.634 | Reg loss: 0.040 | Tree loss: 1.634 | Accuracy: 0.388000 | 1.694 sec/iter\n",
      "Epoch: 240 | Batch: 001 / 011 | Total loss: 1.602 | Reg loss: 0.040 | Tree loss: 1.602 | Accuracy: 0.416500 | 1.694 sec/iter\n",
      "Epoch: 240 | Batch: 002 / 011 | Total loss: 1.535 | Reg loss: 0.040 | Tree loss: 1.535 | Accuracy: 0.446000 | 1.694 sec/iter\n",
      "Epoch: 240 | Batch: 003 / 011 | Total loss: 1.483 | Reg loss: 0.040 | Tree loss: 1.483 | Accuracy: 0.475000 | 1.694 sec/iter\n",
      "Epoch: 240 | Batch: 004 / 011 | Total loss: 1.465 | Reg loss: 0.040 | Tree loss: 1.465 | Accuracy: 0.465500 | 1.694 sec/iter\n",
      "Epoch: 240 | Batch: 005 / 011 | Total loss: 1.440 | Reg loss: 0.040 | Tree loss: 1.440 | Accuracy: 0.510500 | 1.694 sec/iter\n",
      "Epoch: 240 | Batch: 006 / 011 | Total loss: 1.427 | Reg loss: 0.040 | Tree loss: 1.427 | Accuracy: 0.510500 | 1.694 sec/iter\n",
      "Epoch: 240 | Batch: 007 / 011 | Total loss: 1.414 | Reg loss: 0.040 | Tree loss: 1.414 | Accuracy: 0.520000 | 1.694 sec/iter\n",
      "Epoch: 240 | Batch: 008 / 011 | Total loss: 1.411 | Reg loss: 0.040 | Tree loss: 1.411 | Accuracy: 0.494500 | 1.693 sec/iter\n",
      "Epoch: 240 | Batch: 009 / 011 | Total loss: 1.413 | Reg loss: 0.040 | Tree loss: 1.413 | Accuracy: 0.510000 | 1.693 sec/iter\n",
      "Epoch: 240 | Batch: 010 / 011 | Total loss: 1.387 | Reg loss: 0.040 | Tree loss: 1.387 | Accuracy: 0.488055 | 1.693 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 241 | Batch: 000 / 011 | Total loss: 1.608 | Reg loss: 0.040 | Tree loss: 1.608 | Accuracy: 0.408000 | 1.693 sec/iter\n",
      "Epoch: 241 | Batch: 001 / 011 | Total loss: 1.584 | Reg loss: 0.040 | Tree loss: 1.584 | Accuracy: 0.424000 | 1.693 sec/iter\n",
      "Epoch: 241 | Batch: 002 / 011 | Total loss: 1.525 | Reg loss: 0.040 | Tree loss: 1.525 | Accuracy: 0.454000 | 1.693 sec/iter\n",
      "Epoch: 241 | Batch: 003 / 011 | Total loss: 1.500 | Reg loss: 0.040 | Tree loss: 1.500 | Accuracy: 0.467500 | 1.693 sec/iter\n",
      "Epoch: 241 | Batch: 004 / 011 | Total loss: 1.462 | Reg loss: 0.040 | Tree loss: 1.462 | Accuracy: 0.482000 | 1.692 sec/iter\n",
      "Epoch: 241 | Batch: 005 / 011 | Total loss: 1.442 | Reg loss: 0.040 | Tree loss: 1.442 | Accuracy: 0.493500 | 1.692 sec/iter\n",
      "Epoch: 241 | Batch: 006 / 011 | Total loss: 1.432 | Reg loss: 0.040 | Tree loss: 1.432 | Accuracy: 0.498000 | 1.692 sec/iter\n",
      "Epoch: 241 | Batch: 007 / 011 | Total loss: 1.428 | Reg loss: 0.040 | Tree loss: 1.428 | Accuracy: 0.498000 | 1.692 sec/iter\n",
      "Epoch: 241 | Batch: 008 / 011 | Total loss: 1.410 | Reg loss: 0.040 | Tree loss: 1.410 | Accuracy: 0.492500 | 1.692 sec/iter\n",
      "Epoch: 241 | Batch: 009 / 011 | Total loss: 1.418 | Reg loss: 0.040 | Tree loss: 1.418 | Accuracy: 0.495000 | 1.692 sec/iter\n",
      "Epoch: 241 | Batch: 010 / 011 | Total loss: 1.407 | Reg loss: 0.040 | Tree loss: 1.407 | Accuracy: 0.464164 | 1.691 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 242 | Batch: 000 / 011 | Total loss: 1.606 | Reg loss: 0.040 | Tree loss: 1.606 | Accuracy: 0.397000 | 1.694 sec/iter\n",
      "Epoch: 242 | Batch: 001 / 011 | Total loss: 1.584 | Reg loss: 0.040 | Tree loss: 1.584 | Accuracy: 0.395500 | 1.694 sec/iter\n",
      "Epoch: 242 | Batch: 002 / 011 | Total loss: 1.532 | Reg loss: 0.040 | Tree loss: 1.532 | Accuracy: 0.437500 | 1.694 sec/iter\n",
      "Epoch: 242 | Batch: 003 / 011 | Total loss: 1.503 | Reg loss: 0.040 | Tree loss: 1.503 | Accuracy: 0.459000 | 1.693 sec/iter\n",
      "Epoch: 242 | Batch: 004 / 011 | Total loss: 1.465 | Reg loss: 0.040 | Tree loss: 1.465 | Accuracy: 0.470000 | 1.693 sec/iter\n",
      "Epoch: 242 | Batch: 005 / 011 | Total loss: 1.444 | Reg loss: 0.040 | Tree loss: 1.444 | Accuracy: 0.494000 | 1.693 sec/iter\n",
      "Epoch: 242 | Batch: 006 / 011 | Total loss: 1.421 | Reg loss: 0.040 | Tree loss: 1.421 | Accuracy: 0.513500 | 1.693 sec/iter\n",
      "Epoch: 242 | Batch: 007 / 011 | Total loss: 1.417 | Reg loss: 0.040 | Tree loss: 1.417 | Accuracy: 0.492500 | 1.693 sec/iter\n",
      "Epoch: 242 | Batch: 008 / 011 | Total loss: 1.421 | Reg loss: 0.040 | Tree loss: 1.421 | Accuracy: 0.481000 | 1.693 sec/iter\n",
      "Epoch: 242 | Batch: 009 / 011 | Total loss: 1.415 | Reg loss: 0.040 | Tree loss: 1.415 | Accuracy: 0.496000 | 1.693 sec/iter\n",
      "Epoch: 242 | Batch: 010 / 011 | Total loss: 1.421 | Reg loss: 0.040 | Tree loss: 1.421 | Accuracy: 0.481229 | 1.692 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 243 | Batch: 000 / 011 | Total loss: 1.638 | Reg loss: 0.040 | Tree loss: 1.638 | Accuracy: 0.374500 | 1.692 sec/iter\n",
      "Epoch: 243 | Batch: 001 / 011 | Total loss: 1.562 | Reg loss: 0.040 | Tree loss: 1.562 | Accuracy: 0.427000 | 1.692 sec/iter\n",
      "Epoch: 243 | Batch: 002 / 011 | Total loss: 1.523 | Reg loss: 0.040 | Tree loss: 1.523 | Accuracy: 0.450000 | 1.692 sec/iter\n",
      "Epoch: 243 | Batch: 003 / 011 | Total loss: 1.502 | Reg loss: 0.040 | Tree loss: 1.502 | Accuracy: 0.472500 | 1.692 sec/iter\n",
      "Epoch: 243 | Batch: 004 / 011 | Total loss: 1.467 | Reg loss: 0.040 | Tree loss: 1.467 | Accuracy: 0.487500 | 1.692 sec/iter\n",
      "Epoch: 243 | Batch: 005 / 011 | Total loss: 1.441 | Reg loss: 0.040 | Tree loss: 1.441 | Accuracy: 0.490500 | 1.692 sec/iter\n",
      "Epoch: 243 | Batch: 006 / 011 | Total loss: 1.410 | Reg loss: 0.040 | Tree loss: 1.410 | Accuracy: 0.512000 | 1.692 sec/iter\n",
      "Epoch: 243 | Batch: 007 / 011 | Total loss: 1.415 | Reg loss: 0.040 | Tree loss: 1.415 | Accuracy: 0.502000 | 1.691 sec/iter\n",
      "Epoch: 243 | Batch: 008 / 011 | Total loss: 1.411 | Reg loss: 0.040 | Tree loss: 1.411 | Accuracy: 0.510500 | 1.691 sec/iter\n",
      "Epoch: 243 | Batch: 009 / 011 | Total loss: 1.426 | Reg loss: 0.040 | Tree loss: 1.426 | Accuracy: 0.496000 | 1.691 sec/iter\n",
      "Epoch: 243 | Batch: 010 / 011 | Total loss: 1.470 | Reg loss: 0.040 | Tree loss: 1.470 | Accuracy: 0.450512 | 1.691 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 244 | Batch: 000 / 011 | Total loss: 1.614 | Reg loss: 0.040 | Tree loss: 1.614 | Accuracy: 0.407000 | 1.693 sec/iter\n",
      "Epoch: 244 | Batch: 001 / 011 | Total loss: 1.574 | Reg loss: 0.040 | Tree loss: 1.574 | Accuracy: 0.416000 | 1.693 sec/iter\n",
      "Epoch: 244 | Batch: 002 / 011 | Total loss: 1.544 | Reg loss: 0.040 | Tree loss: 1.544 | Accuracy: 0.437500 | 1.693 sec/iter\n",
      "Epoch: 244 | Batch: 003 / 011 | Total loss: 1.510 | Reg loss: 0.040 | Tree loss: 1.510 | Accuracy: 0.466500 | 1.693 sec/iter\n",
      "Epoch: 244 | Batch: 004 / 011 | Total loss: 1.464 | Reg loss: 0.040 | Tree loss: 1.464 | Accuracy: 0.470500 | 1.692 sec/iter\n",
      "Epoch: 244 | Batch: 005 / 011 | Total loss: 1.414 | Reg loss: 0.040 | Tree loss: 1.414 | Accuracy: 0.497500 | 1.692 sec/iter\n",
      "Epoch: 244 | Batch: 006 / 011 | Total loss: 1.437 | Reg loss: 0.040 | Tree loss: 1.437 | Accuracy: 0.500500 | 1.692 sec/iter\n",
      "Epoch: 244 | Batch: 007 / 011 | Total loss: 1.424 | Reg loss: 0.040 | Tree loss: 1.424 | Accuracy: 0.507000 | 1.691 sec/iter\n",
      "Epoch: 244 | Batch: 008 / 011 | Total loss: 1.421 | Reg loss: 0.040 | Tree loss: 1.421 | Accuracy: 0.510000 | 1.691 sec/iter\n",
      "Epoch: 244 | Batch: 009 / 011 | Total loss: 1.419 | Reg loss: 0.040 | Tree loss: 1.419 | Accuracy: 0.492500 | 1.691 sec/iter\n",
      "Epoch: 244 | Batch: 010 / 011 | Total loss: 1.402 | Reg loss: 0.040 | Tree loss: 1.402 | Accuracy: 0.488055 | 1.691 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 245 | Batch: 000 / 011 | Total loss: 1.632 | Reg loss: 0.040 | Tree loss: 1.632 | Accuracy: 0.373000 | 1.693 sec/iter\n",
      "Epoch: 245 | Batch: 001 / 011 | Total loss: 1.583 | Reg loss: 0.040 | Tree loss: 1.583 | Accuracy: 0.428000 | 1.692 sec/iter\n",
      "Epoch: 245 | Batch: 002 / 011 | Total loss: 1.546 | Reg loss: 0.040 | Tree loss: 1.546 | Accuracy: 0.441000 | 1.692 sec/iter\n",
      "Epoch: 245 | Batch: 003 / 011 | Total loss: 1.481 | Reg loss: 0.040 | Tree loss: 1.481 | Accuracy: 0.487500 | 1.692 sec/iter\n",
      "Epoch: 245 | Batch: 004 / 011 | Total loss: 1.462 | Reg loss: 0.040 | Tree loss: 1.462 | Accuracy: 0.472500 | 1.692 sec/iter\n",
      "Epoch: 245 | Batch: 005 / 011 | Total loss: 1.449 | Reg loss: 0.040 | Tree loss: 1.449 | Accuracy: 0.487000 | 1.692 sec/iter\n",
      "Epoch: 245 | Batch: 006 / 011 | Total loss: 1.420 | Reg loss: 0.040 | Tree loss: 1.420 | Accuracy: 0.530000 | 1.692 sec/iter\n",
      "Epoch: 245 | Batch: 007 / 011 | Total loss: 1.433 | Reg loss: 0.040 | Tree loss: 1.433 | Accuracy: 0.506000 | 1.692 sec/iter\n",
      "Epoch: 245 | Batch: 008 / 011 | Total loss: 1.403 | Reg loss: 0.040 | Tree loss: 1.403 | Accuracy: 0.517500 | 1.692 sec/iter\n",
      "Epoch: 245 | Batch: 009 / 011 | Total loss: 1.415 | Reg loss: 0.040 | Tree loss: 1.415 | Accuracy: 0.491000 | 1.691 sec/iter\n",
      "Epoch: 245 | Batch: 010 / 011 | Total loss: 1.430 | Reg loss: 0.040 | Tree loss: 1.430 | Accuracy: 0.498294 | 1.691 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 246 | Batch: 000 / 011 | Total loss: 1.618 | Reg loss: 0.040 | Tree loss: 1.618 | Accuracy: 0.394500 | 1.691 sec/iter\n",
      "Epoch: 246 | Batch: 001 / 011 | Total loss: 1.560 | Reg loss: 0.040 | Tree loss: 1.560 | Accuracy: 0.419500 | 1.691 sec/iter\n",
      "Epoch: 246 | Batch: 002 / 011 | Total loss: 1.553 | Reg loss: 0.040 | Tree loss: 1.553 | Accuracy: 0.438000 | 1.691 sec/iter\n",
      "Epoch: 246 | Batch: 003 / 011 | Total loss: 1.499 | Reg loss: 0.040 | Tree loss: 1.499 | Accuracy: 0.456500 | 1.691 sec/iter\n",
      "Epoch: 246 | Batch: 004 / 011 | Total loss: 1.461 | Reg loss: 0.040 | Tree loss: 1.461 | Accuracy: 0.493000 | 1.691 sec/iter\n",
      "Epoch: 246 | Batch: 005 / 011 | Total loss: 1.440 | Reg loss: 0.040 | Tree loss: 1.440 | Accuracy: 0.492500 | 1.691 sec/iter\n",
      "Epoch: 246 | Batch: 006 / 011 | Total loss: 1.431 | Reg loss: 0.040 | Tree loss: 1.431 | Accuracy: 0.505000 | 1.69 sec/iter\n",
      "Epoch: 246 | Batch: 007 / 011 | Total loss: 1.402 | Reg loss: 0.040 | Tree loss: 1.402 | Accuracy: 0.523500 | 1.69 sec/iter\n",
      "Epoch: 246 | Batch: 008 / 011 | Total loss: 1.434 | Reg loss: 0.040 | Tree loss: 1.434 | Accuracy: 0.503000 | 1.69 sec/iter\n",
      "Epoch: 246 | Batch: 009 / 011 | Total loss: 1.427 | Reg loss: 0.040 | Tree loss: 1.427 | Accuracy: 0.506000 | 1.69 sec/iter\n",
      "Epoch: 246 | Batch: 010 / 011 | Total loss: 1.344 | Reg loss: 0.040 | Tree loss: 1.344 | Accuracy: 0.477816 | 1.69 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 247 | Batch: 000 / 011 | Total loss: 1.641 | Reg loss: 0.040 | Tree loss: 1.641 | Accuracy: 0.388500 | 1.692 sec/iter\n",
      "Epoch: 247 | Batch: 001 / 011 | Total loss: 1.590 | Reg loss: 0.040 | Tree loss: 1.590 | Accuracy: 0.415000 | 1.692 sec/iter\n",
      "Epoch: 247 | Batch: 002 / 011 | Total loss: 1.527 | Reg loss: 0.040 | Tree loss: 1.527 | Accuracy: 0.451500 | 1.692 sec/iter\n",
      "Epoch: 247 | Batch: 003 / 011 | Total loss: 1.485 | Reg loss: 0.040 | Tree loss: 1.485 | Accuracy: 0.463500 | 1.692 sec/iter\n",
      "Epoch: 247 | Batch: 004 / 011 | Total loss: 1.450 | Reg loss: 0.040 | Tree loss: 1.450 | Accuracy: 0.478000 | 1.691 sec/iter\n",
      "Epoch: 247 | Batch: 005 / 011 | Total loss: 1.415 | Reg loss: 0.040 | Tree loss: 1.415 | Accuracy: 0.519500 | 1.691 sec/iter\n",
      "Epoch: 247 | Batch: 006 / 011 | Total loss: 1.426 | Reg loss: 0.040 | Tree loss: 1.426 | Accuracy: 0.497000 | 1.691 sec/iter\n",
      "Epoch: 247 | Batch: 007 / 011 | Total loss: 1.415 | Reg loss: 0.040 | Tree loss: 1.415 | Accuracy: 0.502500 | 1.691 sec/iter\n",
      "Epoch: 247 | Batch: 008 / 011 | Total loss: 1.430 | Reg loss: 0.040 | Tree loss: 1.430 | Accuracy: 0.494500 | 1.691 sec/iter\n",
      "Epoch: 247 | Batch: 009 / 011 | Total loss: 1.414 | Reg loss: 0.040 | Tree loss: 1.414 | Accuracy: 0.503500 | 1.691 sec/iter\n",
      "Epoch: 247 | Batch: 010 / 011 | Total loss: 1.441 | Reg loss: 0.040 | Tree loss: 1.441 | Accuracy: 0.494881 | 1.691 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 248 | Batch: 000 / 011 | Total loss: 1.627 | Reg loss: 0.040 | Tree loss: 1.627 | Accuracy: 0.394500 | 1.691 sec/iter\n",
      "Epoch: 248 | Batch: 001 / 011 | Total loss: 1.574 | Reg loss: 0.040 | Tree loss: 1.574 | Accuracy: 0.430500 | 1.69 sec/iter\n",
      "Epoch: 248 | Batch: 002 / 011 | Total loss: 1.574 | Reg loss: 0.040 | Tree loss: 1.574 | Accuracy: 0.414000 | 1.69 sec/iter\n",
      "Epoch: 248 | Batch: 003 / 011 | Total loss: 1.495 | Reg loss: 0.040 | Tree loss: 1.495 | Accuracy: 0.460500 | 1.69 sec/iter\n",
      "Epoch: 248 | Batch: 004 / 011 | Total loss: 1.459 | Reg loss: 0.040 | Tree loss: 1.459 | Accuracy: 0.485500 | 1.69 sec/iter\n",
      "Epoch: 248 | Batch: 005 / 011 | Total loss: 1.456 | Reg loss: 0.040 | Tree loss: 1.456 | Accuracy: 0.474500 | 1.69 sec/iter\n",
      "Epoch: 248 | Batch: 006 / 011 | Total loss: 1.436 | Reg loss: 0.040 | Tree loss: 1.436 | Accuracy: 0.514000 | 1.69 sec/iter\n",
      "Epoch: 248 | Batch: 007 / 011 | Total loss: 1.405 | Reg loss: 0.040 | Tree loss: 1.405 | Accuracy: 0.529500 | 1.69 sec/iter\n",
      "Epoch: 248 | Batch: 008 / 011 | Total loss: 1.391 | Reg loss: 0.040 | Tree loss: 1.391 | Accuracy: 0.525500 | 1.69 sec/iter\n",
      "Epoch: 248 | Batch: 009 / 011 | Total loss: 1.426 | Reg loss: 0.040 | Tree loss: 1.426 | Accuracy: 0.499000 | 1.689 sec/iter\n",
      "Epoch: 248 | Batch: 010 / 011 | Total loss: 1.369 | Reg loss: 0.040 | Tree loss: 1.369 | Accuracy: 0.494881 | 1.689 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 249 | Batch: 000 / 011 | Total loss: 1.599 | Reg loss: 0.040 | Tree loss: 1.599 | Accuracy: 0.406000 | 1.691 sec/iter\n",
      "Epoch: 249 | Batch: 001 / 011 | Total loss: 1.596 | Reg loss: 0.040 | Tree loss: 1.596 | Accuracy: 0.394500 | 1.691 sec/iter\n",
      "Epoch: 249 | Batch: 002 / 011 | Total loss: 1.534 | Reg loss: 0.040 | Tree loss: 1.534 | Accuracy: 0.448500 | 1.691 sec/iter\n",
      "Epoch: 249 | Batch: 003 / 011 | Total loss: 1.503 | Reg loss: 0.040 | Tree loss: 1.503 | Accuracy: 0.435000 | 1.691 sec/iter\n",
      "Epoch: 249 | Batch: 004 / 011 | Total loss: 1.473 | Reg loss: 0.040 | Tree loss: 1.473 | Accuracy: 0.475000 | 1.691 sec/iter\n",
      "Epoch: 249 | Batch: 005 / 011 | Total loss: 1.433 | Reg loss: 0.040 | Tree loss: 1.433 | Accuracy: 0.510000 | 1.691 sec/iter\n",
      "Epoch: 249 | Batch: 006 / 011 | Total loss: 1.431 | Reg loss: 0.040 | Tree loss: 1.431 | Accuracy: 0.512000 | 1.691 sec/iter\n",
      "Epoch: 249 | Batch: 007 / 011 | Total loss: 1.444 | Reg loss: 0.040 | Tree loss: 1.444 | Accuracy: 0.499500 | 1.691 sec/iter\n",
      "Epoch: 249 | Batch: 008 / 011 | Total loss: 1.396 | Reg loss: 0.040 | Tree loss: 1.396 | Accuracy: 0.497000 | 1.69 sec/iter\n",
      "Epoch: 249 | Batch: 009 / 011 | Total loss: 1.415 | Reg loss: 0.040 | Tree loss: 1.415 | Accuracy: 0.500500 | 1.69 sec/iter\n",
      "Epoch: 249 | Batch: 010 / 011 | Total loss: 1.375 | Reg loss: 0.040 | Tree loss: 1.375 | Accuracy: 0.511945 | 1.69 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 250 | Batch: 000 / 011 | Total loss: 1.601 | Reg loss: 0.040 | Tree loss: 1.601 | Accuracy: 0.393000 | 1.69 sec/iter\n",
      "Epoch: 250 | Batch: 001 / 011 | Total loss: 1.577 | Reg loss: 0.040 | Tree loss: 1.577 | Accuracy: 0.408500 | 1.69 sec/iter\n",
      "Epoch: 250 | Batch: 002 / 011 | Total loss: 1.529 | Reg loss: 0.040 | Tree loss: 1.529 | Accuracy: 0.449000 | 1.69 sec/iter\n",
      "Epoch: 250 | Batch: 003 / 011 | Total loss: 1.511 | Reg loss: 0.040 | Tree loss: 1.511 | Accuracy: 0.462500 | 1.69 sec/iter\n",
      "Epoch: 250 | Batch: 004 / 011 | Total loss: 1.457 | Reg loss: 0.040 | Tree loss: 1.457 | Accuracy: 0.478500 | 1.689 sec/iter\n",
      "Epoch: 250 | Batch: 005 / 011 | Total loss: 1.451 | Reg loss: 0.040 | Tree loss: 1.451 | Accuracy: 0.496000 | 1.689 sec/iter\n",
      "Epoch: 250 | Batch: 006 / 011 | Total loss: 1.418 | Reg loss: 0.040 | Tree loss: 1.418 | Accuracy: 0.506000 | 1.689 sec/iter\n",
      "Epoch: 250 | Batch: 007 / 011 | Total loss: 1.423 | Reg loss: 0.040 | Tree loss: 1.423 | Accuracy: 0.520500 | 1.689 sec/iter\n",
      "Epoch: 250 | Batch: 008 / 011 | Total loss: 1.420 | Reg loss: 0.040 | Tree loss: 1.420 | Accuracy: 0.512500 | 1.689 sec/iter\n",
      "Epoch: 250 | Batch: 009 / 011 | Total loss: 1.414 | Reg loss: 0.040 | Tree loss: 1.414 | Accuracy: 0.491000 | 1.689 sec/iter\n",
      "Epoch: 250 | Batch: 010 / 011 | Total loss: 1.349 | Reg loss: 0.040 | Tree loss: 1.349 | Accuracy: 0.532423 | 1.688 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 251 | Batch: 000 / 011 | Total loss: 1.630 | Reg loss: 0.040 | Tree loss: 1.630 | Accuracy: 0.387000 | 1.691 sec/iter\n",
      "Epoch: 251 | Batch: 001 / 011 | Total loss: 1.566 | Reg loss: 0.040 | Tree loss: 1.566 | Accuracy: 0.419500 | 1.691 sec/iter\n",
      "Epoch: 251 | Batch: 002 / 011 | Total loss: 1.533 | Reg loss: 0.040 | Tree loss: 1.533 | Accuracy: 0.441500 | 1.69 sec/iter\n",
      "Epoch: 251 | Batch: 003 / 011 | Total loss: 1.518 | Reg loss: 0.040 | Tree loss: 1.518 | Accuracy: 0.430500 | 1.69 sec/iter\n",
      "Epoch: 251 | Batch: 004 / 011 | Total loss: 1.443 | Reg loss: 0.040 | Tree loss: 1.443 | Accuracy: 0.494500 | 1.69 sec/iter\n",
      "Epoch: 251 | Batch: 005 / 011 | Total loss: 1.437 | Reg loss: 0.040 | Tree loss: 1.437 | Accuracy: 0.509000 | 1.69 sec/iter\n",
      "Epoch: 251 | Batch: 006 / 011 | Total loss: 1.435 | Reg loss: 0.040 | Tree loss: 1.435 | Accuracy: 0.508000 | 1.689 sec/iter\n",
      "Epoch: 251 | Batch: 007 / 011 | Total loss: 1.415 | Reg loss: 0.040 | Tree loss: 1.415 | Accuracy: 0.521000 | 1.689 sec/iter\n",
      "Epoch: 251 | Batch: 008 / 011 | Total loss: 1.418 | Reg loss: 0.040 | Tree loss: 1.418 | Accuracy: 0.488000 | 1.689 sec/iter\n",
      "Epoch: 251 | Batch: 009 / 011 | Total loss: 1.404 | Reg loss: 0.040 | Tree loss: 1.404 | Accuracy: 0.488000 | 1.689 sec/iter\n",
      "Epoch: 251 | Batch: 010 / 011 | Total loss: 1.436 | Reg loss: 0.040 | Tree loss: 1.436 | Accuracy: 0.474403 | 1.688 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 252 | Batch: 000 / 011 | Total loss: 1.590 | Reg loss: 0.040 | Tree loss: 1.590 | Accuracy: 0.415500 | 1.69 sec/iter\n",
      "Epoch: 252 | Batch: 001 / 011 | Total loss: 1.583 | Reg loss: 0.040 | Tree loss: 1.583 | Accuracy: 0.409000 | 1.69 sec/iter\n",
      "Epoch: 252 | Batch: 002 / 011 | Total loss: 1.541 | Reg loss: 0.040 | Tree loss: 1.541 | Accuracy: 0.438000 | 1.69 sec/iter\n",
      "Epoch: 252 | Batch: 003 / 011 | Total loss: 1.507 | Reg loss: 0.040 | Tree loss: 1.507 | Accuracy: 0.456000 | 1.69 sec/iter\n",
      "Epoch: 252 | Batch: 004 / 011 | Total loss: 1.461 | Reg loss: 0.040 | Tree loss: 1.461 | Accuracy: 0.492500 | 1.69 sec/iter\n",
      "Epoch: 252 | Batch: 005 / 011 | Total loss: 1.454 | Reg loss: 0.040 | Tree loss: 1.454 | Accuracy: 0.498500 | 1.69 sec/iter\n",
      "Epoch: 252 | Batch: 006 / 011 | Total loss: 1.425 | Reg loss: 0.040 | Tree loss: 1.425 | Accuracy: 0.524500 | 1.689 sec/iter\n",
      "Epoch: 252 | Batch: 007 / 011 | Total loss: 1.413 | Reg loss: 0.040 | Tree loss: 1.413 | Accuracy: 0.507000 | 1.689 sec/iter\n",
      "Epoch: 252 | Batch: 008 / 011 | Total loss: 1.415 | Reg loss: 0.040 | Tree loss: 1.415 | Accuracy: 0.513500 | 1.689 sec/iter\n",
      "Epoch: 252 | Batch: 009 / 011 | Total loss: 1.419 | Reg loss: 0.040 | Tree loss: 1.419 | Accuracy: 0.490500 | 1.689 sec/iter\n",
      "Epoch: 252 | Batch: 010 / 011 | Total loss: 1.395 | Reg loss: 0.040 | Tree loss: 1.395 | Accuracy: 0.511945 | 1.689 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 253 | Batch: 000 / 011 | Total loss: 1.617 | Reg loss: 0.040 | Tree loss: 1.617 | Accuracy: 0.401500 | 1.689 sec/iter\n",
      "Epoch: 253 | Batch: 001 / 011 | Total loss: 1.591 | Reg loss: 0.040 | Tree loss: 1.591 | Accuracy: 0.407000 | 1.689 sec/iter\n",
      "Epoch: 253 | Batch: 002 / 011 | Total loss: 1.519 | Reg loss: 0.040 | Tree loss: 1.519 | Accuracy: 0.437000 | 1.688 sec/iter\n",
      "Epoch: 253 | Batch: 003 / 011 | Total loss: 1.491 | Reg loss: 0.040 | Tree loss: 1.491 | Accuracy: 0.486000 | 1.688 sec/iter\n",
      "Epoch: 253 | Batch: 004 / 011 | Total loss: 1.476 | Reg loss: 0.040 | Tree loss: 1.476 | Accuracy: 0.483000 | 1.688 sec/iter\n",
      "Epoch: 253 | Batch: 005 / 011 | Total loss: 1.430 | Reg loss: 0.040 | Tree loss: 1.430 | Accuracy: 0.503500 | 1.688 sec/iter\n",
      "Epoch: 253 | Batch: 006 / 011 | Total loss: 1.416 | Reg loss: 0.040 | Tree loss: 1.416 | Accuracy: 0.516500 | 1.688 sec/iter\n",
      "Epoch: 253 | Batch: 007 / 011 | Total loss: 1.439 | Reg loss: 0.040 | Tree loss: 1.439 | Accuracy: 0.485000 | 1.688 sec/iter\n",
      "Epoch: 253 | Batch: 008 / 011 | Total loss: 1.416 | Reg loss: 0.040 | Tree loss: 1.416 | Accuracy: 0.511500 | 1.688 sec/iter\n",
      "Epoch: 253 | Batch: 009 / 011 | Total loss: 1.396 | Reg loss: 0.040 | Tree loss: 1.396 | Accuracy: 0.513000 | 1.688 sec/iter\n",
      "Epoch: 253 | Batch: 010 / 011 | Total loss: 1.425 | Reg loss: 0.040 | Tree loss: 1.425 | Accuracy: 0.467577 | 1.687 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 254 | Batch: 000 / 011 | Total loss: 1.598 | Reg loss: 0.040 | Tree loss: 1.598 | Accuracy: 0.409000 | 1.69 sec/iter\n",
      "Epoch: 254 | Batch: 001 / 011 | Total loss: 1.581 | Reg loss: 0.040 | Tree loss: 1.581 | Accuracy: 0.421000 | 1.689 sec/iter\n",
      "Epoch: 254 | Batch: 002 / 011 | Total loss: 1.549 | Reg loss: 0.040 | Tree loss: 1.549 | Accuracy: 0.425000 | 1.689 sec/iter\n",
      "Epoch: 254 | Batch: 003 / 011 | Total loss: 1.494 | Reg loss: 0.040 | Tree loss: 1.494 | Accuracy: 0.474000 | 1.689 sec/iter\n",
      "Epoch: 254 | Batch: 004 / 011 | Total loss: 1.453 | Reg loss: 0.040 | Tree loss: 1.453 | Accuracy: 0.502000 | 1.689 sec/iter\n",
      "Epoch: 254 | Batch: 005 / 011 | Total loss: 1.441 | Reg loss: 0.040 | Tree loss: 1.441 | Accuracy: 0.491000 | 1.689 sec/iter\n",
      "Epoch: 254 | Batch: 006 / 011 | Total loss: 1.437 | Reg loss: 0.040 | Tree loss: 1.437 | Accuracy: 0.505500 | 1.689 sec/iter\n",
      "Epoch: 254 | Batch: 007 / 011 | Total loss: 1.419 | Reg loss: 0.040 | Tree loss: 1.419 | Accuracy: 0.514500 | 1.689 sec/iter\n",
      "Epoch: 254 | Batch: 008 / 011 | Total loss: 1.417 | Reg loss: 0.040 | Tree loss: 1.417 | Accuracy: 0.514000 | 1.689 sec/iter\n",
      "Epoch: 254 | Batch: 009 / 011 | Total loss: 1.422 | Reg loss: 0.040 | Tree loss: 1.422 | Accuracy: 0.490000 | 1.689 sec/iter\n",
      "Epoch: 254 | Batch: 010 / 011 | Total loss: 1.397 | Reg loss: 0.040 | Tree loss: 1.397 | Accuracy: 0.484642 | 1.688 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 255 | Batch: 000 / 011 | Total loss: 1.619 | Reg loss: 0.040 | Tree loss: 1.619 | Accuracy: 0.404500 | 1.688 sec/iter\n",
      "Epoch: 255 | Batch: 001 / 011 | Total loss: 1.568 | Reg loss: 0.040 | Tree loss: 1.568 | Accuracy: 0.419000 | 1.688 sec/iter\n",
      "Epoch: 255 | Batch: 002 / 011 | Total loss: 1.529 | Reg loss: 0.040 | Tree loss: 1.529 | Accuracy: 0.443000 | 1.688 sec/iter\n",
      "Epoch: 255 | Batch: 003 / 011 | Total loss: 1.490 | Reg loss: 0.040 | Tree loss: 1.490 | Accuracy: 0.459500 | 1.688 sec/iter\n",
      "Epoch: 255 | Batch: 004 / 011 | Total loss: 1.471 | Reg loss: 0.040 | Tree loss: 1.471 | Accuracy: 0.469000 | 1.688 sec/iter\n",
      "Epoch: 255 | Batch: 005 / 011 | Total loss: 1.434 | Reg loss: 0.040 | Tree loss: 1.434 | Accuracy: 0.487500 | 1.688 sec/iter\n",
      "Epoch: 255 | Batch: 006 / 011 | Total loss: 1.413 | Reg loss: 0.040 | Tree loss: 1.413 | Accuracy: 0.535000 | 1.688 sec/iter\n",
      "Epoch: 255 | Batch: 007 / 011 | Total loss: 1.428 | Reg loss: 0.040 | Tree loss: 1.428 | Accuracy: 0.500000 | 1.687 sec/iter\n",
      "Epoch: 255 | Batch: 008 / 011 | Total loss: 1.408 | Reg loss: 0.040 | Tree loss: 1.408 | Accuracy: 0.500000 | 1.687 sec/iter\n",
      "Epoch: 255 | Batch: 009 / 011 | Total loss: 1.423 | Reg loss: 0.040 | Tree loss: 1.423 | Accuracy: 0.480000 | 1.687 sec/iter\n",
      "Epoch: 255 | Batch: 010 / 011 | Total loss: 1.451 | Reg loss: 0.040 | Tree loss: 1.451 | Accuracy: 0.488055 | 1.687 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 256 | Batch: 000 / 011 | Total loss: 1.617 | Reg loss: 0.040 | Tree loss: 1.617 | Accuracy: 0.390000 | 1.689 sec/iter\n",
      "Epoch: 256 | Batch: 001 / 011 | Total loss: 1.576 | Reg loss: 0.040 | Tree loss: 1.576 | Accuracy: 0.406500 | 1.689 sec/iter\n",
      "Epoch: 256 | Batch: 002 / 011 | Total loss: 1.546 | Reg loss: 0.040 | Tree loss: 1.546 | Accuracy: 0.423500 | 1.689 sec/iter\n",
      "Epoch: 256 | Batch: 003 / 011 | Total loss: 1.484 | Reg loss: 0.040 | Tree loss: 1.484 | Accuracy: 0.473500 | 1.689 sec/iter\n",
      "Epoch: 256 | Batch: 004 / 011 | Total loss: 1.446 | Reg loss: 0.040 | Tree loss: 1.446 | Accuracy: 0.481000 | 1.689 sec/iter\n",
      "Epoch: 256 | Batch: 005 / 011 | Total loss: 1.452 | Reg loss: 0.040 | Tree loss: 1.452 | Accuracy: 0.485000 | 1.688 sec/iter\n",
      "Epoch: 256 | Batch: 006 / 011 | Total loss: 1.411 | Reg loss: 0.040 | Tree loss: 1.411 | Accuracy: 0.496500 | 1.688 sec/iter\n",
      "Epoch: 256 | Batch: 007 / 011 | Total loss: 1.429 | Reg loss: 0.040 | Tree loss: 1.429 | Accuracy: 0.494500 | 1.688 sec/iter\n",
      "Epoch: 256 | Batch: 008 / 011 | Total loss: 1.395 | Reg loss: 0.040 | Tree loss: 1.395 | Accuracy: 0.506000 | 1.688 sec/iter\n",
      "Epoch: 256 | Batch: 009 / 011 | Total loss: 1.423 | Reg loss: 0.040 | Tree loss: 1.423 | Accuracy: 0.515000 | 1.688 sec/iter\n",
      "Epoch: 256 | Batch: 010 / 011 | Total loss: 1.469 | Reg loss: 0.040 | Tree loss: 1.469 | Accuracy: 0.430034 | 1.688 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 257 | Batch: 000 / 011 | Total loss: 1.614 | Reg loss: 0.040 | Tree loss: 1.614 | Accuracy: 0.387500 | 1.688 sec/iter\n",
      "Epoch: 257 | Batch: 001 / 011 | Total loss: 1.551 | Reg loss: 0.040 | Tree loss: 1.551 | Accuracy: 0.418500 | 1.688 sec/iter\n",
      "Epoch: 257 | Batch: 002 / 011 | Total loss: 1.552 | Reg loss: 0.040 | Tree loss: 1.552 | Accuracy: 0.417000 | 1.687 sec/iter\n",
      "Epoch: 257 | Batch: 003 / 011 | Total loss: 1.468 | Reg loss: 0.040 | Tree loss: 1.468 | Accuracy: 0.481000 | 1.687 sec/iter\n",
      "Epoch: 257 | Batch: 004 / 011 | Total loss: 1.483 | Reg loss: 0.040 | Tree loss: 1.483 | Accuracy: 0.454000 | 1.687 sec/iter\n",
      "Epoch: 257 | Batch: 005 / 011 | Total loss: 1.425 | Reg loss: 0.040 | Tree loss: 1.425 | Accuracy: 0.492500 | 1.687 sec/iter\n",
      "Epoch: 257 | Batch: 006 / 011 | Total loss: 1.412 | Reg loss: 0.040 | Tree loss: 1.412 | Accuracy: 0.519500 | 1.687 sec/iter\n",
      "Epoch: 257 | Batch: 007 / 011 | Total loss: 1.416 | Reg loss: 0.040 | Tree loss: 1.416 | Accuracy: 0.508000 | 1.687 sec/iter\n",
      "Epoch: 257 | Batch: 008 / 011 | Total loss: 1.410 | Reg loss: 0.040 | Tree loss: 1.410 | Accuracy: 0.510500 | 1.687 sec/iter\n",
      "Epoch: 257 | Batch: 009 / 011 | Total loss: 1.438 | Reg loss: 0.040 | Tree loss: 1.438 | Accuracy: 0.504500 | 1.687 sec/iter\n",
      "Epoch: 257 | Batch: 010 / 011 | Total loss: 1.526 | Reg loss: 0.040 | Tree loss: 1.526 | Accuracy: 0.436860 | 1.686 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 258 | Batch: 000 / 011 | Total loss: 1.607 | Reg loss: 0.040 | Tree loss: 1.607 | Accuracy: 0.399000 | 1.688 sec/iter\n",
      "Epoch: 258 | Batch: 001 / 011 | Total loss: 1.593 | Reg loss: 0.040 | Tree loss: 1.593 | Accuracy: 0.404500 | 1.688 sec/iter\n",
      "Epoch: 258 | Batch: 002 / 011 | Total loss: 1.571 | Reg loss: 0.040 | Tree loss: 1.571 | Accuracy: 0.429500 | 1.688 sec/iter\n",
      "Epoch: 258 | Batch: 003 / 011 | Total loss: 1.486 | Reg loss: 0.040 | Tree loss: 1.486 | Accuracy: 0.461000 | 1.688 sec/iter\n",
      "Epoch: 258 | Batch: 004 / 011 | Total loss: 1.457 | Reg loss: 0.040 | Tree loss: 1.457 | Accuracy: 0.489000 | 1.688 sec/iter\n",
      "Epoch: 258 | Batch: 005 / 011 | Total loss: 1.441 | Reg loss: 0.040 | Tree loss: 1.441 | Accuracy: 0.489000 | 1.687 sec/iter\n",
      "Epoch: 258 | Batch: 006 / 011 | Total loss: 1.411 | Reg loss: 0.040 | Tree loss: 1.411 | Accuracy: 0.526500 | 1.687 sec/iter\n",
      "Epoch: 258 | Batch: 007 / 011 | Total loss: 1.428 | Reg loss: 0.040 | Tree loss: 1.428 | Accuracy: 0.497000 | 1.687 sec/iter\n",
      "Epoch: 258 | Batch: 008 / 011 | Total loss: 1.408 | Reg loss: 0.040 | Tree loss: 1.408 | Accuracy: 0.522000 | 1.687 sec/iter\n",
      "Epoch: 258 | Batch: 009 / 011 | Total loss: 1.385 | Reg loss: 0.040 | Tree loss: 1.385 | Accuracy: 0.505000 | 1.687 sec/iter\n",
      "Epoch: 258 | Batch: 010 / 011 | Total loss: 1.397 | Reg loss: 0.040 | Tree loss: 1.397 | Accuracy: 0.464164 | 1.686 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 259 | Batch: 000 / 011 | Total loss: 1.625 | Reg loss: 0.040 | Tree loss: 1.625 | Accuracy: 0.403000 | 1.688 sec/iter\n",
      "Epoch: 259 | Batch: 001 / 011 | Total loss: 1.608 | Reg loss: 0.040 | Tree loss: 1.608 | Accuracy: 0.403000 | 1.688 sec/iter\n",
      "Epoch: 259 | Batch: 002 / 011 | Total loss: 1.531 | Reg loss: 0.040 | Tree loss: 1.531 | Accuracy: 0.435000 | 1.688 sec/iter\n",
      "Epoch: 259 | Batch: 003 / 011 | Total loss: 1.488 | Reg loss: 0.040 | Tree loss: 1.488 | Accuracy: 0.460000 | 1.687 sec/iter\n",
      "Epoch: 259 | Batch: 004 / 011 | Total loss: 1.497 | Reg loss: 0.040 | Tree loss: 1.497 | Accuracy: 0.472500 | 1.687 sec/iter\n",
      "Epoch: 259 | Batch: 005 / 011 | Total loss: 1.428 | Reg loss: 0.040 | Tree loss: 1.428 | Accuracy: 0.515000 | 1.687 sec/iter\n",
      "Epoch: 259 | Batch: 006 / 011 | Total loss: 1.425 | Reg loss: 0.040 | Tree loss: 1.425 | Accuracy: 0.514000 | 1.687 sec/iter\n",
      "Epoch: 259 | Batch: 007 / 011 | Total loss: 1.400 | Reg loss: 0.040 | Tree loss: 1.400 | Accuracy: 0.528500 | 1.687 sec/iter\n",
      "Epoch: 259 | Batch: 008 / 011 | Total loss: 1.401 | Reg loss: 0.040 | Tree loss: 1.401 | Accuracy: 0.504000 | 1.687 sec/iter\n",
      "Epoch: 259 | Batch: 009 / 011 | Total loss: 1.392 | Reg loss: 0.040 | Tree loss: 1.392 | Accuracy: 0.516000 | 1.687 sec/iter\n",
      "Epoch: 259 | Batch: 010 / 011 | Total loss: 1.402 | Reg loss: 0.040 | Tree loss: 1.402 | Accuracy: 0.515358 | 1.687 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 260 | Batch: 000 / 011 | Total loss: 1.639 | Reg loss: 0.040 | Tree loss: 1.639 | Accuracy: 0.387000 | 1.686 sec/iter\n",
      "Epoch: 260 | Batch: 001 / 011 | Total loss: 1.571 | Reg loss: 0.040 | Tree loss: 1.571 | Accuracy: 0.422000 | 1.686 sec/iter\n",
      "Epoch: 260 | Batch: 002 / 011 | Total loss: 1.527 | Reg loss: 0.040 | Tree loss: 1.527 | Accuracy: 0.448500 | 1.686 sec/iter\n",
      "Epoch: 260 | Batch: 003 / 011 | Total loss: 1.501 | Reg loss: 0.040 | Tree loss: 1.501 | Accuracy: 0.456000 | 1.686 sec/iter\n",
      "Epoch: 260 | Batch: 004 / 011 | Total loss: 1.469 | Reg loss: 0.040 | Tree loss: 1.469 | Accuracy: 0.487500 | 1.686 sec/iter\n",
      "Epoch: 260 | Batch: 005 / 011 | Total loss: 1.431 | Reg loss: 0.040 | Tree loss: 1.431 | Accuracy: 0.493500 | 1.686 sec/iter\n",
      "Epoch: 260 | Batch: 006 / 011 | Total loss: 1.414 | Reg loss: 0.040 | Tree loss: 1.414 | Accuracy: 0.510000 | 1.686 sec/iter\n",
      "Epoch: 260 | Batch: 007 / 011 | Total loss: 1.416 | Reg loss: 0.040 | Tree loss: 1.416 | Accuracy: 0.503000 | 1.686 sec/iter\n",
      "Epoch: 260 | Batch: 008 / 011 | Total loss: 1.398 | Reg loss: 0.040 | Tree loss: 1.398 | Accuracy: 0.526500 | 1.686 sec/iter\n",
      "Epoch: 260 | Batch: 009 / 011 | Total loss: 1.410 | Reg loss: 0.040 | Tree loss: 1.410 | Accuracy: 0.510000 | 1.685 sec/iter\n",
      "Epoch: 260 | Batch: 010 / 011 | Total loss: 1.339 | Reg loss: 0.040 | Tree loss: 1.339 | Accuracy: 0.529010 | 1.685 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 261 | Batch: 000 / 011 | Total loss: 1.615 | Reg loss: 0.040 | Tree loss: 1.615 | Accuracy: 0.404500 | 1.687 sec/iter\n",
      "Epoch: 261 | Batch: 001 / 011 | Total loss: 1.582 | Reg loss: 0.040 | Tree loss: 1.582 | Accuracy: 0.414500 | 1.687 sec/iter\n",
      "Epoch: 261 | Batch: 002 / 011 | Total loss: 1.533 | Reg loss: 0.040 | Tree loss: 1.533 | Accuracy: 0.440500 | 1.687 sec/iter\n",
      "Epoch: 261 | Batch: 003 / 011 | Total loss: 1.496 | Reg loss: 0.040 | Tree loss: 1.496 | Accuracy: 0.472000 | 1.687 sec/iter\n",
      "Epoch: 261 | Batch: 004 / 011 | Total loss: 1.446 | Reg loss: 0.040 | Tree loss: 1.446 | Accuracy: 0.493500 | 1.687 sec/iter\n",
      "Epoch: 261 | Batch: 005 / 011 | Total loss: 1.446 | Reg loss: 0.040 | Tree loss: 1.446 | Accuracy: 0.491000 | 1.687 sec/iter\n",
      "Epoch: 261 | Batch: 006 / 011 | Total loss: 1.416 | Reg loss: 0.040 | Tree loss: 1.416 | Accuracy: 0.531500 | 1.687 sec/iter\n",
      "Epoch: 261 | Batch: 007 / 011 | Total loss: 1.417 | Reg loss: 0.040 | Tree loss: 1.417 | Accuracy: 0.508500 | 1.687 sec/iter\n",
      "Epoch: 261 | Batch: 008 / 011 | Total loss: 1.408 | Reg loss: 0.040 | Tree loss: 1.408 | Accuracy: 0.498500 | 1.686 sec/iter\n",
      "Epoch: 261 | Batch: 009 / 011 | Total loss: 1.431 | Reg loss: 0.040 | Tree loss: 1.431 | Accuracy: 0.495500 | 1.686 sec/iter\n",
      "Epoch: 261 | Batch: 010 / 011 | Total loss: 1.411 | Reg loss: 0.040 | Tree loss: 1.411 | Accuracy: 0.481229 | 1.686 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 262 | Batch: 000 / 011 | Total loss: 1.599 | Reg loss: 0.040 | Tree loss: 1.599 | Accuracy: 0.416000 | 1.686 sec/iter\n",
      "Epoch: 262 | Batch: 001 / 011 | Total loss: 1.588 | Reg loss: 0.040 | Tree loss: 1.588 | Accuracy: 0.410000 | 1.686 sec/iter\n",
      "Epoch: 262 | Batch: 002 / 011 | Total loss: 1.536 | Reg loss: 0.040 | Tree loss: 1.536 | Accuracy: 0.414000 | 1.686 sec/iter\n",
      "Epoch: 262 | Batch: 003 / 011 | Total loss: 1.508 | Reg loss: 0.040 | Tree loss: 1.508 | Accuracy: 0.458500 | 1.686 sec/iter\n",
      "Epoch: 262 | Batch: 004 / 011 | Total loss: 1.471 | Reg loss: 0.040 | Tree loss: 1.471 | Accuracy: 0.477000 | 1.686 sec/iter\n",
      "Epoch: 262 | Batch: 005 / 011 | Total loss: 1.421 | Reg loss: 0.040 | Tree loss: 1.421 | Accuracy: 0.510500 | 1.685 sec/iter\n",
      "Epoch: 262 | Batch: 006 / 011 | Total loss: 1.409 | Reg loss: 0.040 | Tree loss: 1.409 | Accuracy: 0.526000 | 1.685 sec/iter\n",
      "Epoch: 262 | Batch: 007 / 011 | Total loss: 1.434 | Reg loss: 0.040 | Tree loss: 1.434 | Accuracy: 0.491000 | 1.685 sec/iter\n",
      "Epoch: 262 | Batch: 008 / 011 | Total loss: 1.426 | Reg loss: 0.040 | Tree loss: 1.426 | Accuracy: 0.503000 | 1.685 sec/iter\n",
      "Epoch: 262 | Batch: 009 / 011 | Total loss: 1.414 | Reg loss: 0.040 | Tree loss: 1.414 | Accuracy: 0.495000 | 1.685 sec/iter\n",
      "Epoch: 262 | Batch: 010 / 011 | Total loss: 1.350 | Reg loss: 0.040 | Tree loss: 1.350 | Accuracy: 0.525597 | 1.685 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 263 | Batch: 000 / 011 | Total loss: 1.604 | Reg loss: 0.040 | Tree loss: 1.604 | Accuracy: 0.400500 | 1.687 sec/iter\n",
      "Epoch: 263 | Batch: 001 / 011 | Total loss: 1.578 | Reg loss: 0.040 | Tree loss: 1.578 | Accuracy: 0.408000 | 1.687 sec/iter\n",
      "Epoch: 263 | Batch: 002 / 011 | Total loss: 1.533 | Reg loss: 0.040 | Tree loss: 1.533 | Accuracy: 0.439500 | 1.687 sec/iter\n",
      "Epoch: 263 | Batch: 003 / 011 | Total loss: 1.530 | Reg loss: 0.040 | Tree loss: 1.530 | Accuracy: 0.422500 | 1.686 sec/iter\n",
      "Epoch: 263 | Batch: 004 / 011 | Total loss: 1.457 | Reg loss: 0.040 | Tree loss: 1.457 | Accuracy: 0.471500 | 1.686 sec/iter\n",
      "Epoch: 263 | Batch: 005 / 011 | Total loss: 1.429 | Reg loss: 0.040 | Tree loss: 1.429 | Accuracy: 0.504000 | 1.686 sec/iter\n",
      "Epoch: 263 | Batch: 006 / 011 | Total loss: 1.428 | Reg loss: 0.040 | Tree loss: 1.428 | Accuracy: 0.502500 | 1.686 sec/iter\n",
      "Epoch: 263 | Batch: 007 / 011 | Total loss: 1.399 | Reg loss: 0.040 | Tree loss: 1.399 | Accuracy: 0.539000 | 1.686 sec/iter\n",
      "Epoch: 263 | Batch: 008 / 011 | Total loss: 1.419 | Reg loss: 0.040 | Tree loss: 1.419 | Accuracy: 0.493500 | 1.686 sec/iter\n",
      "Epoch: 263 | Batch: 009 / 011 | Total loss: 1.415 | Reg loss: 0.040 | Tree loss: 1.415 | Accuracy: 0.485000 | 1.686 sec/iter\n",
      "Epoch: 263 | Batch: 010 / 011 | Total loss: 1.380 | Reg loss: 0.040 | Tree loss: 1.380 | Accuracy: 0.529010 | 1.686 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 264 | Batch: 000 / 011 | Total loss: 1.583 | Reg loss: 0.040 | Tree loss: 1.583 | Accuracy: 0.398500 | 1.686 sec/iter\n",
      "Epoch: 264 | Batch: 001 / 011 | Total loss: 1.578 | Reg loss: 0.040 | Tree loss: 1.578 | Accuracy: 0.411500 | 1.685 sec/iter\n",
      "Epoch: 264 | Batch: 002 / 011 | Total loss: 1.534 | Reg loss: 0.040 | Tree loss: 1.534 | Accuracy: 0.422000 | 1.685 sec/iter\n",
      "Epoch: 264 | Batch: 003 / 011 | Total loss: 1.537 | Reg loss: 0.040 | Tree loss: 1.537 | Accuracy: 0.417000 | 1.685 sec/iter\n",
      "Epoch: 264 | Batch: 004 / 011 | Total loss: 1.473 | Reg loss: 0.040 | Tree loss: 1.473 | Accuracy: 0.479000 | 1.685 sec/iter\n",
      "Epoch: 264 | Batch: 005 / 011 | Total loss: 1.447 | Reg loss: 0.040 | Tree loss: 1.447 | Accuracy: 0.507000 | 1.685 sec/iter\n",
      "Epoch: 264 | Batch: 006 / 011 | Total loss: 1.422 | Reg loss: 0.040 | Tree loss: 1.422 | Accuracy: 0.522500 | 1.685 sec/iter\n",
      "Epoch: 264 | Batch: 007 / 011 | Total loss: 1.388 | Reg loss: 0.040 | Tree loss: 1.388 | Accuracy: 0.517000 | 1.685 sec/iter\n",
      "Epoch: 264 | Batch: 008 / 011 | Total loss: 1.419 | Reg loss: 0.040 | Tree loss: 1.419 | Accuracy: 0.496000 | 1.685 sec/iter\n",
      "Epoch: 264 | Batch: 009 / 011 | Total loss: 1.404 | Reg loss: 0.040 | Tree loss: 1.404 | Accuracy: 0.494500 | 1.684 sec/iter\n",
      "Epoch: 264 | Batch: 010 / 011 | Total loss: 1.428 | Reg loss: 0.040 | Tree loss: 1.428 | Accuracy: 0.477816 | 1.684 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 8: 0.9723756906077345\n",
      "Epoch: 265 | Batch: 000 / 011 | Total loss: 1.608 | Reg loss: 0.040 | Tree loss: 1.608 | Accuracy: 0.388500 | 1.686 sec/iter\n",
      "Epoch: 265 | Batch: 001 / 011 | Total loss: 1.575 | Reg loss: 0.040 | Tree loss: 1.575 | Accuracy: 0.390000 | 1.686 sec/iter\n",
      "Epoch: 265 | Batch: 002 / 011 | Total loss: 1.531 | Reg loss: 0.040 | Tree loss: 1.531 | Accuracy: 0.413500 | 1.686 sec/iter\n",
      "Epoch: 265 | Batch: 003 / 011 | Total loss: 1.494 | Reg loss: 0.040 | Tree loss: 1.494 | Accuracy: 0.453500 | 1.686 sec/iter\n",
      "Epoch: 265 | Batch: 004 / 011 | Total loss: 1.478 | Reg loss: 0.040 | Tree loss: 1.478 | Accuracy: 0.468000 | 1.686 sec/iter\n",
      "Epoch: 265 | Batch: 005 / 011 | Total loss: 1.439 | Reg loss: 0.040 | Tree loss: 1.439 | Accuracy: 0.483000 | 1.685 sec/iter\n",
      "Epoch: 265 | Batch: 006 / 011 | Total loss: 1.418 | Reg loss: 0.040 | Tree loss: 1.418 | Accuracy: 0.508000 | 1.685 sec/iter\n",
      "Epoch: 265 | Batch: 007 / 011 | Total loss: 1.418 | Reg loss: 0.040 | Tree loss: 1.418 | Accuracy: 0.493500 | 1.685 sec/iter\n",
      "Epoch: 265 | Batch: 008 / 011 | Total loss: 1.440 | Reg loss: 0.040 | Tree loss: 1.440 | Accuracy: 0.497000 | 1.685 sec/iter\n",
      "Epoch: 265 | Batch: 009 / 011 | Total loss: 1.387 | Reg loss: 0.040 | Tree loss: 1.387 | Accuracy: 0.511000 | 1.685 sec/iter\n",
      "Epoch: 265 | Batch: 010 / 011 | Total loss: 1.350 | Reg loss: 0.040 | Tree loss: 1.350 | Accuracy: 0.535836 | 1.684 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 266 | Batch: 000 / 011 | Total loss: 1.627 | Reg loss: 0.040 | Tree loss: 1.627 | Accuracy: 0.382500 | 1.686 sec/iter\n",
      "Epoch: 266 | Batch: 001 / 011 | Total loss: 1.576 | Reg loss: 0.040 | Tree loss: 1.576 | Accuracy: 0.400000 | 1.686 sec/iter\n",
      "Epoch: 266 | Batch: 002 / 011 | Total loss: 1.555 | Reg loss: 0.040 | Tree loss: 1.555 | Accuracy: 0.426500 | 1.686 sec/iter\n",
      "Epoch: 266 | Batch: 003 / 011 | Total loss: 1.496 | Reg loss: 0.040 | Tree loss: 1.496 | Accuracy: 0.457000 | 1.685 sec/iter\n",
      "Epoch: 266 | Batch: 004 / 011 | Total loss: 1.478 | Reg loss: 0.040 | Tree loss: 1.478 | Accuracy: 0.462500 | 1.685 sec/iter\n",
      "Epoch: 266 | Batch: 005 / 011 | Total loss: 1.424 | Reg loss: 0.040 | Tree loss: 1.424 | Accuracy: 0.509000 | 1.685 sec/iter\n",
      "Epoch: 266 | Batch: 006 / 011 | Total loss: 1.424 | Reg loss: 0.040 | Tree loss: 1.424 | Accuracy: 0.510500 | 1.685 sec/iter\n",
      "Epoch: 266 | Batch: 007 / 011 | Total loss: 1.423 | Reg loss: 0.040 | Tree loss: 1.423 | Accuracy: 0.525500 | 1.685 sec/iter\n",
      "Epoch: 266 | Batch: 008 / 011 | Total loss: 1.393 | Reg loss: 0.040 | Tree loss: 1.393 | Accuracy: 0.483000 | 1.685 sec/iter\n",
      "Epoch: 266 | Batch: 009 / 011 | Total loss: 1.407 | Reg loss: 0.040 | Tree loss: 1.407 | Accuracy: 0.496000 | 1.685 sec/iter\n",
      "Epoch: 266 | Batch: 010 / 011 | Total loss: 1.350 | Reg loss: 0.040 | Tree loss: 1.350 | Accuracy: 0.522184 | 1.684 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 267 | Batch: 000 / 011 | Total loss: 1.598 | Reg loss: 0.040 | Tree loss: 1.598 | Accuracy: 0.402500 | 1.684 sec/iter\n",
      "Epoch: 267 | Batch: 001 / 011 | Total loss: 1.602 | Reg loss: 0.040 | Tree loss: 1.602 | Accuracy: 0.392500 | 1.684 sec/iter\n",
      "Epoch: 267 | Batch: 002 / 011 | Total loss: 1.558 | Reg loss: 0.040 | Tree loss: 1.558 | Accuracy: 0.421500 | 1.684 sec/iter\n",
      "Epoch: 267 | Batch: 003 / 011 | Total loss: 1.511 | Reg loss: 0.040 | Tree loss: 1.511 | Accuracy: 0.432500 | 1.684 sec/iter\n",
      "Epoch: 267 | Batch: 004 / 011 | Total loss: 1.457 | Reg loss: 0.040 | Tree loss: 1.457 | Accuracy: 0.483000 | 1.684 sec/iter\n",
      "Epoch: 267 | Batch: 005 / 011 | Total loss: 1.438 | Reg loss: 0.040 | Tree loss: 1.438 | Accuracy: 0.464500 | 1.684 sec/iter\n",
      "Epoch: 267 | Batch: 006 / 011 | Total loss: 1.412 | Reg loss: 0.040 | Tree loss: 1.412 | Accuracy: 0.502000 | 1.684 sec/iter\n",
      "Epoch: 267 | Batch: 007 / 011 | Total loss: 1.411 | Reg loss: 0.040 | Tree loss: 1.411 | Accuracy: 0.515000 | 1.684 sec/iter\n",
      "Epoch: 267 | Batch: 008 / 011 | Total loss: 1.409 | Reg loss: 0.040 | Tree loss: 1.409 | Accuracy: 0.504000 | 1.684 sec/iter\n",
      "Epoch: 267 | Batch: 009 / 011 | Total loss: 1.403 | Reg loss: 0.040 | Tree loss: 1.403 | Accuracy: 0.505000 | 1.683 sec/iter\n",
      "Epoch: 267 | Batch: 010 / 011 | Total loss: 1.438 | Reg loss: 0.040 | Tree loss: 1.438 | Accuracy: 0.470990 | 1.683 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 268 | Batch: 000 / 011 | Total loss: 1.603 | Reg loss: 0.040 | Tree loss: 1.603 | Accuracy: 0.402500 | 1.685 sec/iter\n",
      "Epoch: 268 | Batch: 001 / 011 | Total loss: 1.584 | Reg loss: 0.040 | Tree loss: 1.584 | Accuracy: 0.424000 | 1.685 sec/iter\n",
      "Epoch: 268 | Batch: 002 / 011 | Total loss: 1.544 | Reg loss: 0.040 | Tree loss: 1.544 | Accuracy: 0.436500 | 1.685 sec/iter\n",
      "Epoch: 268 | Batch: 003 / 011 | Total loss: 1.488 | Reg loss: 0.040 | Tree loss: 1.488 | Accuracy: 0.465000 | 1.685 sec/iter\n",
      "Epoch: 268 | Batch: 004 / 011 | Total loss: 1.460 | Reg loss: 0.040 | Tree loss: 1.460 | Accuracy: 0.472000 | 1.685 sec/iter\n",
      "Epoch: 268 | Batch: 005 / 011 | Total loss: 1.440 | Reg loss: 0.040 | Tree loss: 1.440 | Accuracy: 0.504500 | 1.685 sec/iter\n",
      "Epoch: 268 | Batch: 006 / 011 | Total loss: 1.429 | Reg loss: 0.040 | Tree loss: 1.429 | Accuracy: 0.503500 | 1.685 sec/iter\n",
      "Epoch: 268 | Batch: 007 / 011 | Total loss: 1.403 | Reg loss: 0.040 | Tree loss: 1.403 | Accuracy: 0.514000 | 1.685 sec/iter\n",
      "Epoch: 268 | Batch: 008 / 011 | Total loss: 1.409 | Reg loss: 0.040 | Tree loss: 1.409 | Accuracy: 0.501500 | 1.684 sec/iter\n",
      "Epoch: 268 | Batch: 009 / 011 | Total loss: 1.438 | Reg loss: 0.040 | Tree loss: 1.438 | Accuracy: 0.485000 | 1.684 sec/iter\n",
      "Epoch: 268 | Batch: 010 / 011 | Total loss: 1.402 | Reg loss: 0.040 | Tree loss: 1.402 | Accuracy: 0.488055 | 1.684 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 269 | Batch: 000 / 011 | Total loss: 1.627 | Reg loss: 0.040 | Tree loss: 1.627 | Accuracy: 0.403000 | 1.684 sec/iter\n",
      "Epoch: 269 | Batch: 001 / 011 | Total loss: 1.595 | Reg loss: 0.040 | Tree loss: 1.595 | Accuracy: 0.397000 | 1.684 sec/iter\n",
      "Epoch: 269 | Batch: 002 / 011 | Total loss: 1.545 | Reg loss: 0.040 | Tree loss: 1.545 | Accuracy: 0.430000 | 1.684 sec/iter\n",
      "Epoch: 269 | Batch: 003 / 011 | Total loss: 1.508 | Reg loss: 0.040 | Tree loss: 1.508 | Accuracy: 0.445000 | 1.684 sec/iter\n",
      "Epoch: 269 | Batch: 004 / 011 | Total loss: 1.447 | Reg loss: 0.040 | Tree loss: 1.447 | Accuracy: 0.482500 | 1.684 sec/iter\n",
      "Epoch: 269 | Batch: 005 / 011 | Total loss: 1.427 | Reg loss: 0.040 | Tree loss: 1.427 | Accuracy: 0.500000 | 1.683 sec/iter\n",
      "Epoch: 269 | Batch: 006 / 011 | Total loss: 1.425 | Reg loss: 0.040 | Tree loss: 1.425 | Accuracy: 0.495000 | 1.683 sec/iter\n",
      "Epoch: 269 | Batch: 007 / 011 | Total loss: 1.415 | Reg loss: 0.040 | Tree loss: 1.415 | Accuracy: 0.522500 | 1.683 sec/iter\n",
      "Epoch: 269 | Batch: 008 / 011 | Total loss: 1.400 | Reg loss: 0.040 | Tree loss: 1.400 | Accuracy: 0.509500 | 1.683 sec/iter\n",
      "Epoch: 269 | Batch: 009 / 011 | Total loss: 1.407 | Reg loss: 0.040 | Tree loss: 1.407 | Accuracy: 0.507000 | 1.683 sec/iter\n",
      "Epoch: 269 | Batch: 010 / 011 | Total loss: 1.443 | Reg loss: 0.040 | Tree loss: 1.443 | Accuracy: 0.474403 | 1.683 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 8: 0.9723756906077345\n",
      "Epoch: 270 | Batch: 000 / 011 | Total loss: 1.598 | Reg loss: 0.040 | Tree loss: 1.598 | Accuracy: 0.409500 | 1.685 sec/iter\n",
      "Epoch: 270 | Batch: 001 / 011 | Total loss: 1.600 | Reg loss: 0.040 | Tree loss: 1.600 | Accuracy: 0.390500 | 1.685 sec/iter\n",
      "Epoch: 270 | Batch: 002 / 011 | Total loss: 1.571 | Reg loss: 0.040 | Tree loss: 1.571 | Accuracy: 0.433000 | 1.685 sec/iter\n",
      "Epoch: 270 | Batch: 003 / 011 | Total loss: 1.507 | Reg loss: 0.040 | Tree loss: 1.507 | Accuracy: 0.458500 | 1.684 sec/iter\n",
      "Epoch: 270 | Batch: 004 / 011 | Total loss: 1.453 | Reg loss: 0.040 | Tree loss: 1.453 | Accuracy: 0.482500 | 1.684 sec/iter\n",
      "Epoch: 270 | Batch: 005 / 011 | Total loss: 1.417 | Reg loss: 0.040 | Tree loss: 1.417 | Accuracy: 0.499000 | 1.684 sec/iter\n",
      "Epoch: 270 | Batch: 006 / 011 | Total loss: 1.427 | Reg loss: 0.040 | Tree loss: 1.427 | Accuracy: 0.507500 | 1.684 sec/iter\n",
      "Epoch: 270 | Batch: 007 / 011 | Total loss: 1.415 | Reg loss: 0.040 | Tree loss: 1.415 | Accuracy: 0.522500 | 1.684 sec/iter\n",
      "Epoch: 270 | Batch: 008 / 011 | Total loss: 1.420 | Reg loss: 0.040 | Tree loss: 1.420 | Accuracy: 0.501000 | 1.684 sec/iter\n",
      "Epoch: 270 | Batch: 009 / 011 | Total loss: 1.396 | Reg loss: 0.040 | Tree loss: 1.396 | Accuracy: 0.502500 | 1.684 sec/iter\n",
      "Epoch: 270 | Batch: 010 / 011 | Total loss: 1.392 | Reg loss: 0.040 | Tree loss: 1.392 | Accuracy: 0.508532 | 1.684 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 271 | Batch: 000 / 011 | Total loss: 1.594 | Reg loss: 0.040 | Tree loss: 1.594 | Accuracy: 0.401500 | 1.683 sec/iter\n",
      "Epoch: 271 | Batch: 001 / 011 | Total loss: 1.560 | Reg loss: 0.040 | Tree loss: 1.560 | Accuracy: 0.413500 | 1.683 sec/iter\n",
      "Epoch: 271 | Batch: 002 / 011 | Total loss: 1.545 | Reg loss: 0.040 | Tree loss: 1.545 | Accuracy: 0.431500 | 1.683 sec/iter\n",
      "Epoch: 271 | Batch: 003 / 011 | Total loss: 1.515 | Reg loss: 0.040 | Tree loss: 1.515 | Accuracy: 0.440000 | 1.683 sec/iter\n",
      "Epoch: 271 | Batch: 004 / 011 | Total loss: 1.462 | Reg loss: 0.040 | Tree loss: 1.462 | Accuracy: 0.478000 | 1.683 sec/iter\n",
      "Epoch: 271 | Batch: 005 / 011 | Total loss: 1.434 | Reg loss: 0.040 | Tree loss: 1.434 | Accuracy: 0.470000 | 1.683 sec/iter\n",
      "Epoch: 271 | Batch: 006 / 011 | Total loss: 1.415 | Reg loss: 0.040 | Tree loss: 1.415 | Accuracy: 0.501500 | 1.683 sec/iter\n",
      "Epoch: 271 | Batch: 007 / 011 | Total loss: 1.438 | Reg loss: 0.040 | Tree loss: 1.438 | Accuracy: 0.509500 | 1.683 sec/iter\n",
      "Epoch: 271 | Batch: 008 / 011 | Total loss: 1.399 | Reg loss: 0.040 | Tree loss: 1.399 | Accuracy: 0.519000 | 1.683 sec/iter\n",
      "Epoch: 271 | Batch: 009 / 011 | Total loss: 1.414 | Reg loss: 0.040 | Tree loss: 1.414 | Accuracy: 0.496500 | 1.682 sec/iter\n",
      "Epoch: 271 | Batch: 010 / 011 | Total loss: 1.405 | Reg loss: 0.040 | Tree loss: 1.405 | Accuracy: 0.470990 | 1.682 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 272 | Batch: 000 / 011 | Total loss: 1.611 | Reg loss: 0.040 | Tree loss: 1.611 | Accuracy: 0.402000 | 1.683 sec/iter\n",
      "Epoch: 272 | Batch: 001 / 011 | Total loss: 1.579 | Reg loss: 0.040 | Tree loss: 1.579 | Accuracy: 0.411000 | 1.683 sec/iter\n",
      "Epoch: 272 | Batch: 002 / 011 | Total loss: 1.532 | Reg loss: 0.040 | Tree loss: 1.532 | Accuracy: 0.434000 | 1.683 sec/iter\n",
      "Epoch: 272 | Batch: 003 / 011 | Total loss: 1.501 | Reg loss: 0.040 | Tree loss: 1.501 | Accuracy: 0.459500 | 1.682 sec/iter\n",
      "Epoch: 272 | Batch: 004 / 011 | Total loss: 1.477 | Reg loss: 0.040 | Tree loss: 1.477 | Accuracy: 0.484000 | 1.682 sec/iter\n",
      "Epoch: 272 | Batch: 005 / 011 | Total loss: 1.429 | Reg loss: 0.040 | Tree loss: 1.429 | Accuracy: 0.490500 | 1.682 sec/iter\n",
      "Epoch: 272 | Batch: 006 / 011 | Total loss: 1.414 | Reg loss: 0.040 | Tree loss: 1.414 | Accuracy: 0.520000 | 1.682 sec/iter\n",
      "Epoch: 272 | Batch: 007 / 011 | Total loss: 1.413 | Reg loss: 0.040 | Tree loss: 1.413 | Accuracy: 0.511000 | 1.682 sec/iter\n",
      "Epoch: 272 | Batch: 008 / 011 | Total loss: 1.412 | Reg loss: 0.040 | Tree loss: 1.412 | Accuracy: 0.490000 | 1.682 sec/iter\n",
      "Epoch: 272 | Batch: 009 / 011 | Total loss: 1.407 | Reg loss: 0.040 | Tree loss: 1.407 | Accuracy: 0.500000 | 1.682 sec/iter\n",
      "Epoch: 272 | Batch: 010 / 011 | Total loss: 1.399 | Reg loss: 0.040 | Tree loss: 1.399 | Accuracy: 0.511945 | 1.681 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 273 | Batch: 000 / 011 | Total loss: 1.620 | Reg loss: 0.040 | Tree loss: 1.620 | Accuracy: 0.392500 | 1.681 sec/iter\n",
      "Epoch: 273 | Batch: 001 / 011 | Total loss: 1.603 | Reg loss: 0.040 | Tree loss: 1.603 | Accuracy: 0.397000 | 1.681 sec/iter\n",
      "Epoch: 273 | Batch: 002 / 011 | Total loss: 1.536 | Reg loss: 0.040 | Tree loss: 1.536 | Accuracy: 0.434000 | 1.681 sec/iter\n",
      "Epoch: 273 | Batch: 003 / 011 | Total loss: 1.471 | Reg loss: 0.040 | Tree loss: 1.471 | Accuracy: 0.475000 | 1.681 sec/iter\n",
      "Epoch: 273 | Batch: 004 / 011 | Total loss: 1.476 | Reg loss: 0.040 | Tree loss: 1.476 | Accuracy: 0.481000 | 1.681 sec/iter\n",
      "Epoch: 273 | Batch: 005 / 011 | Total loss: 1.434 | Reg loss: 0.040 | Tree loss: 1.434 | Accuracy: 0.503500 | 1.681 sec/iter\n",
      "Epoch: 273 | Batch: 006 / 011 | Total loss: 1.418 | Reg loss: 0.040 | Tree loss: 1.418 | Accuracy: 0.499500 | 1.68 sec/iter\n",
      "Epoch: 273 | Batch: 007 / 011 | Total loss: 1.387 | Reg loss: 0.040 | Tree loss: 1.387 | Accuracy: 0.516500 | 1.68 sec/iter\n",
      "Epoch: 273 | Batch: 008 / 011 | Total loss: 1.414 | Reg loss: 0.040 | Tree loss: 1.414 | Accuracy: 0.498000 | 1.68 sec/iter\n",
      "Epoch: 273 | Batch: 009 / 011 | Total loss: 1.423 | Reg loss: 0.040 | Tree loss: 1.423 | Accuracy: 0.495500 | 1.68 sec/iter\n",
      "Epoch: 273 | Batch: 010 / 011 | Total loss: 1.363 | Reg loss: 0.040 | Tree loss: 1.363 | Accuracy: 0.484642 | 1.68 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 274 | Batch: 000 / 011 | Total loss: 1.611 | Reg loss: 0.040 | Tree loss: 1.611 | Accuracy: 0.397500 | 1.68 sec/iter\n",
      "Epoch: 274 | Batch: 001 / 011 | Total loss: 1.594 | Reg loss: 0.040 | Tree loss: 1.594 | Accuracy: 0.415500 | 1.68 sec/iter\n",
      "Epoch: 274 | Batch: 002 / 011 | Total loss: 1.540 | Reg loss: 0.040 | Tree loss: 1.540 | Accuracy: 0.446500 | 1.68 sec/iter\n",
      "Epoch: 274 | Batch: 003 / 011 | Total loss: 1.492 | Reg loss: 0.040 | Tree loss: 1.492 | Accuracy: 0.474000 | 1.68 sec/iter\n",
      "Epoch: 274 | Batch: 004 / 011 | Total loss: 1.436 | Reg loss: 0.040 | Tree loss: 1.436 | Accuracy: 0.486000 | 1.679 sec/iter\n",
      "Epoch: 274 | Batch: 005 / 011 | Total loss: 1.445 | Reg loss: 0.040 | Tree loss: 1.445 | Accuracy: 0.512500 | 1.679 sec/iter\n",
      "Epoch: 274 | Batch: 006 / 011 | Total loss: 1.420 | Reg loss: 0.040 | Tree loss: 1.420 | Accuracy: 0.493500 | 1.679 sec/iter\n",
      "Epoch: 274 | Batch: 007 / 011 | Total loss: 1.411 | Reg loss: 0.040 | Tree loss: 1.411 | Accuracy: 0.516500 | 1.679 sec/iter\n",
      "Epoch: 274 | Batch: 008 / 011 | Total loss: 1.391 | Reg loss: 0.040 | Tree loss: 1.391 | Accuracy: 0.510500 | 1.679 sec/iter\n",
      "Epoch: 274 | Batch: 009 / 011 | Total loss: 1.437 | Reg loss: 0.040 | Tree loss: 1.437 | Accuracy: 0.465500 | 1.679 sec/iter\n",
      "Epoch: 274 | Batch: 010 / 011 | Total loss: 1.424 | Reg loss: 0.040 | Tree loss: 1.424 | Accuracy: 0.488055 | 1.679 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 275 | Batch: 000 / 011 | Total loss: 1.594 | Reg loss: 0.040 | Tree loss: 1.594 | Accuracy: 0.415500 | 1.681 sec/iter\n",
      "Epoch: 275 | Batch: 001 / 011 | Total loss: 1.609 | Reg loss: 0.040 | Tree loss: 1.609 | Accuracy: 0.392000 | 1.681 sec/iter\n",
      "Epoch: 275 | Batch: 002 / 011 | Total loss: 1.536 | Reg loss: 0.040 | Tree loss: 1.536 | Accuracy: 0.436500 | 1.68 sec/iter\n",
      "Epoch: 275 | Batch: 003 / 011 | Total loss: 1.497 | Reg loss: 0.040 | Tree loss: 1.497 | Accuracy: 0.464500 | 1.68 sec/iter\n",
      "Epoch: 275 | Batch: 004 / 011 | Total loss: 1.463 | Reg loss: 0.040 | Tree loss: 1.463 | Accuracy: 0.484500 | 1.68 sec/iter\n",
      "Epoch: 275 | Batch: 005 / 011 | Total loss: 1.448 | Reg loss: 0.040 | Tree loss: 1.448 | Accuracy: 0.506500 | 1.68 sec/iter\n",
      "Epoch: 275 | Batch: 006 / 011 | Total loss: 1.428 | Reg loss: 0.040 | Tree loss: 1.428 | Accuracy: 0.504000 | 1.68 sec/iter\n",
      "Epoch: 275 | Batch: 007 / 011 | Total loss: 1.402 | Reg loss: 0.040 | Tree loss: 1.402 | Accuracy: 0.502500 | 1.68 sec/iter\n",
      "Epoch: 275 | Batch: 008 / 011 | Total loss: 1.404 | Reg loss: 0.040 | Tree loss: 1.404 | Accuracy: 0.492500 | 1.68 sec/iter\n",
      "Epoch: 275 | Batch: 009 / 011 | Total loss: 1.401 | Reg loss: 0.040 | Tree loss: 1.401 | Accuracy: 0.496500 | 1.68 sec/iter\n",
      "Epoch: 275 | Batch: 010 / 011 | Total loss: 1.460 | Reg loss: 0.040 | Tree loss: 1.460 | Accuracy: 0.460751 | 1.679 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 276 | Batch: 000 / 011 | Total loss: 1.601 | Reg loss: 0.040 | Tree loss: 1.601 | Accuracy: 0.410500 | 1.679 sec/iter\n",
      "Epoch: 276 | Batch: 001 / 011 | Total loss: 1.600 | Reg loss: 0.040 | Tree loss: 1.600 | Accuracy: 0.408000 | 1.679 sec/iter\n",
      "Epoch: 276 | Batch: 002 / 011 | Total loss: 1.552 | Reg loss: 0.040 | Tree loss: 1.552 | Accuracy: 0.427500 | 1.679 sec/iter\n",
      "Epoch: 276 | Batch: 003 / 011 | Total loss: 1.490 | Reg loss: 0.040 | Tree loss: 1.490 | Accuracy: 0.459500 | 1.679 sec/iter\n",
      "Epoch: 276 | Batch: 004 / 011 | Total loss: 1.479 | Reg loss: 0.040 | Tree loss: 1.479 | Accuracy: 0.475500 | 1.679 sec/iter\n",
      "Epoch: 276 | Batch: 005 / 011 | Total loss: 1.435 | Reg loss: 0.040 | Tree loss: 1.435 | Accuracy: 0.498000 | 1.679 sec/iter\n",
      "Epoch: 276 | Batch: 006 / 011 | Total loss: 1.410 | Reg loss: 0.040 | Tree loss: 1.410 | Accuracy: 0.498000 | 1.679 sec/iter\n",
      "Epoch: 276 | Batch: 007 / 011 | Total loss: 1.423 | Reg loss: 0.040 | Tree loss: 1.423 | Accuracy: 0.523000 | 1.679 sec/iter\n",
      "Epoch: 276 | Batch: 008 / 011 | Total loss: 1.406 | Reg loss: 0.040 | Tree loss: 1.406 | Accuracy: 0.512000 | 1.679 sec/iter\n",
      "Epoch: 276 | Batch: 009 / 011 | Total loss: 1.395 | Reg loss: 0.040 | Tree loss: 1.395 | Accuracy: 0.494500 | 1.678 sec/iter\n",
      "Epoch: 276 | Batch: 010 / 011 | Total loss: 1.403 | Reg loss: 0.040 | Tree loss: 1.403 | Accuracy: 0.481229 | 1.678 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 277 | Batch: 000 / 011 | Total loss: 1.623 | Reg loss: 0.040 | Tree loss: 1.623 | Accuracy: 0.390000 | 1.68 sec/iter\n",
      "Epoch: 277 | Batch: 001 / 011 | Total loss: 1.584 | Reg loss: 0.040 | Tree loss: 1.584 | Accuracy: 0.417500 | 1.68 sec/iter\n",
      "Epoch: 277 | Batch: 002 / 011 | Total loss: 1.534 | Reg loss: 0.040 | Tree loss: 1.534 | Accuracy: 0.444000 | 1.68 sec/iter\n",
      "Epoch: 277 | Batch: 003 / 011 | Total loss: 1.484 | Reg loss: 0.040 | Tree loss: 1.484 | Accuracy: 0.452500 | 1.68 sec/iter\n",
      "Epoch: 277 | Batch: 004 / 011 | Total loss: 1.457 | Reg loss: 0.040 | Tree loss: 1.457 | Accuracy: 0.486500 | 1.68 sec/iter\n",
      "Epoch: 277 | Batch: 005 / 011 | Total loss: 1.436 | Reg loss: 0.040 | Tree loss: 1.436 | Accuracy: 0.486500 | 1.68 sec/iter\n",
      "Epoch: 277 | Batch: 006 / 011 | Total loss: 1.428 | Reg loss: 0.040 | Tree loss: 1.428 | Accuracy: 0.501000 | 1.68 sec/iter\n",
      "Epoch: 277 | Batch: 007 / 011 | Total loss: 1.414 | Reg loss: 0.040 | Tree loss: 1.414 | Accuracy: 0.509000 | 1.68 sec/iter\n",
      "Epoch: 277 | Batch: 008 / 011 | Total loss: 1.420 | Reg loss: 0.040 | Tree loss: 1.420 | Accuracy: 0.485000 | 1.679 sec/iter\n",
      "Epoch: 277 | Batch: 009 / 011 | Total loss: 1.407 | Reg loss: 0.040 | Tree loss: 1.407 | Accuracy: 0.495500 | 1.679 sec/iter\n",
      "Epoch: 277 | Batch: 010 / 011 | Total loss: 1.457 | Reg loss: 0.040 | Tree loss: 1.457 | Accuracy: 0.460751 | 1.679 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 278 | Batch: 000 / 011 | Total loss: 1.620 | Reg loss: 0.040 | Tree loss: 1.620 | Accuracy: 0.388000 | 1.679 sec/iter\n",
      "Epoch: 278 | Batch: 001 / 011 | Total loss: 1.566 | Reg loss: 0.040 | Tree loss: 1.566 | Accuracy: 0.412000 | 1.679 sec/iter\n",
      "Epoch: 278 | Batch: 002 / 011 | Total loss: 1.523 | Reg loss: 0.040 | Tree loss: 1.523 | Accuracy: 0.438500 | 1.679 sec/iter\n",
      "Epoch: 278 | Batch: 003 / 011 | Total loss: 1.487 | Reg loss: 0.040 | Tree loss: 1.487 | Accuracy: 0.485500 | 1.679 sec/iter\n",
      "Epoch: 278 | Batch: 004 / 011 | Total loss: 1.488 | Reg loss: 0.040 | Tree loss: 1.488 | Accuracy: 0.455500 | 1.679 sec/iter\n",
      "Epoch: 278 | Batch: 005 / 011 | Total loss: 1.455 | Reg loss: 0.040 | Tree loss: 1.455 | Accuracy: 0.478500 | 1.679 sec/iter\n",
      "Epoch: 278 | Batch: 006 / 011 | Total loss: 1.407 | Reg loss: 0.040 | Tree loss: 1.407 | Accuracy: 0.510500 | 1.678 sec/iter\n",
      "Epoch: 278 | Batch: 007 / 011 | Total loss: 1.401 | Reg loss: 0.040 | Tree loss: 1.401 | Accuracy: 0.543000 | 1.678 sec/iter\n",
      "Epoch: 278 | Batch: 008 / 011 | Total loss: 1.426 | Reg loss: 0.040 | Tree loss: 1.426 | Accuracy: 0.501000 | 1.678 sec/iter\n",
      "Epoch: 278 | Batch: 009 / 011 | Total loss: 1.413 | Reg loss: 0.040 | Tree loss: 1.413 | Accuracy: 0.497500 | 1.678 sec/iter\n",
      "Epoch: 278 | Batch: 010 / 011 | Total loss: 1.391 | Reg loss: 0.040 | Tree loss: 1.391 | Accuracy: 0.508532 | 1.678 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 279 | Batch: 000 / 011 | Total loss: 1.603 | Reg loss: 0.040 | Tree loss: 1.603 | Accuracy: 0.395000 | 1.68 sec/iter\n",
      "Epoch: 279 | Batch: 001 / 011 | Total loss: 1.583 | Reg loss: 0.040 | Tree loss: 1.583 | Accuracy: 0.403500 | 1.68 sec/iter\n",
      "Epoch: 279 | Batch: 002 / 011 | Total loss: 1.534 | Reg loss: 0.040 | Tree loss: 1.534 | Accuracy: 0.440000 | 1.68 sec/iter\n",
      "Epoch: 279 | Batch: 003 / 011 | Total loss: 1.484 | Reg loss: 0.040 | Tree loss: 1.484 | Accuracy: 0.472500 | 1.68 sec/iter\n",
      "Epoch: 279 | Batch: 004 / 011 | Total loss: 1.446 | Reg loss: 0.040 | Tree loss: 1.446 | Accuracy: 0.500500 | 1.679 sec/iter\n",
      "Epoch: 279 | Batch: 005 / 011 | Total loss: 1.440 | Reg loss: 0.040 | Tree loss: 1.440 | Accuracy: 0.493500 | 1.679 sec/iter\n",
      "Epoch: 279 | Batch: 006 / 011 | Total loss: 1.431 | Reg loss: 0.040 | Tree loss: 1.431 | Accuracy: 0.494500 | 1.679 sec/iter\n",
      "Epoch: 279 | Batch: 007 / 011 | Total loss: 1.405 | Reg loss: 0.040 | Tree loss: 1.405 | Accuracy: 0.526000 | 1.679 sec/iter\n",
      "Epoch: 279 | Batch: 008 / 011 | Total loss: 1.411 | Reg loss: 0.040 | Tree loss: 1.411 | Accuracy: 0.486500 | 1.679 sec/iter\n",
      "Epoch: 279 | Batch: 009 / 011 | Total loss: 1.417 | Reg loss: 0.040 | Tree loss: 1.417 | Accuracy: 0.493500 | 1.679 sec/iter\n",
      "Epoch: 279 | Batch: 010 / 011 | Total loss: 1.406 | Reg loss: 0.040 | Tree loss: 1.406 | Accuracy: 0.460751 | 1.679 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 280 | Batch: 000 / 011 | Total loss: 1.624 | Reg loss: 0.040 | Tree loss: 1.624 | Accuracy: 0.389500 | 1.679 sec/iter\n",
      "Epoch: 280 | Batch: 001 / 011 | Total loss: 1.587 | Reg loss: 0.040 | Tree loss: 1.587 | Accuracy: 0.417000 | 1.679 sec/iter\n",
      "Epoch: 280 | Batch: 002 / 011 | Total loss: 1.535 | Reg loss: 0.040 | Tree loss: 1.535 | Accuracy: 0.427500 | 1.678 sec/iter\n",
      "Epoch: 280 | Batch: 003 / 011 | Total loss: 1.509 | Reg loss: 0.040 | Tree loss: 1.509 | Accuracy: 0.466500 | 1.678 sec/iter\n",
      "Epoch: 280 | Batch: 004 / 011 | Total loss: 1.466 | Reg loss: 0.040 | Tree loss: 1.466 | Accuracy: 0.491500 | 1.678 sec/iter\n",
      "Epoch: 280 | Batch: 005 / 011 | Total loss: 1.418 | Reg loss: 0.040 | Tree loss: 1.418 | Accuracy: 0.522500 | 1.678 sec/iter\n",
      "Epoch: 280 | Batch: 006 / 011 | Total loss: 1.405 | Reg loss: 0.040 | Tree loss: 1.405 | Accuracy: 0.517500 | 1.677 sec/iter\n",
      "Epoch: 280 | Batch: 007 / 011 | Total loss: 1.427 | Reg loss: 0.040 | Tree loss: 1.427 | Accuracy: 0.527000 | 1.677 sec/iter\n",
      "Epoch: 280 | Batch: 008 / 011 | Total loss: 1.416 | Reg loss: 0.040 | Tree loss: 1.416 | Accuracy: 0.497500 | 1.677 sec/iter\n",
      "Epoch: 280 | Batch: 009 / 011 | Total loss: 1.397 | Reg loss: 0.040 | Tree loss: 1.397 | Accuracy: 0.494500 | 1.677 sec/iter\n",
      "Epoch: 280 | Batch: 010 / 011 | Total loss: 1.434 | Reg loss: 0.040 | Tree loss: 1.434 | Accuracy: 0.498294 | 1.677 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 281 | Batch: 000 / 011 | Total loss: 1.594 | Reg loss: 0.040 | Tree loss: 1.594 | Accuracy: 0.394500 | 1.678 sec/iter\n",
      "Epoch: 281 | Batch: 001 / 011 | Total loss: 1.568 | Reg loss: 0.040 | Tree loss: 1.568 | Accuracy: 0.418000 | 1.678 sec/iter\n",
      "Epoch: 281 | Batch: 002 / 011 | Total loss: 1.547 | Reg loss: 0.040 | Tree loss: 1.547 | Accuracy: 0.441000 | 1.678 sec/iter\n",
      "Epoch: 281 | Batch: 003 / 011 | Total loss: 1.499 | Reg loss: 0.040 | Tree loss: 1.499 | Accuracy: 0.468000 | 1.678 sec/iter\n",
      "Epoch: 281 | Batch: 004 / 011 | Total loss: 1.488 | Reg loss: 0.040 | Tree loss: 1.488 | Accuracy: 0.479500 | 1.678 sec/iter\n",
      "Epoch: 281 | Batch: 005 / 011 | Total loss: 1.430 | Reg loss: 0.040 | Tree loss: 1.430 | Accuracy: 0.492000 | 1.677 sec/iter\n",
      "Epoch: 281 | Batch: 006 / 011 | Total loss: 1.414 | Reg loss: 0.040 | Tree loss: 1.414 | Accuracy: 0.500000 | 1.677 sec/iter\n",
      "Epoch: 281 | Batch: 007 / 011 | Total loss: 1.403 | Reg loss: 0.040 | Tree loss: 1.403 | Accuracy: 0.534500 | 1.677 sec/iter\n",
      "Epoch: 281 | Batch: 008 / 011 | Total loss: 1.414 | Reg loss: 0.040 | Tree loss: 1.414 | Accuracy: 0.503500 | 1.677 sec/iter\n",
      "Epoch: 281 | Batch: 009 / 011 | Total loss: 1.426 | Reg loss: 0.040 | Tree loss: 1.426 | Accuracy: 0.483000 | 1.677 sec/iter\n",
      "Epoch: 281 | Batch: 010 / 011 | Total loss: 1.432 | Reg loss: 0.040 | Tree loss: 1.432 | Accuracy: 0.464164 | 1.677 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 282 | Batch: 000 / 011 | Total loss: 1.616 | Reg loss: 0.040 | Tree loss: 1.616 | Accuracy: 0.384500 | 1.677 sec/iter\n",
      "Epoch: 282 | Batch: 001 / 011 | Total loss: 1.560 | Reg loss: 0.040 | Tree loss: 1.560 | Accuracy: 0.412000 | 1.677 sec/iter\n",
      "Epoch: 282 | Batch: 002 / 011 | Total loss: 1.520 | Reg loss: 0.040 | Tree loss: 1.520 | Accuracy: 0.444500 | 1.677 sec/iter\n",
      "Epoch: 282 | Batch: 003 / 011 | Total loss: 1.501 | Reg loss: 0.040 | Tree loss: 1.501 | Accuracy: 0.448000 | 1.676 sec/iter\n",
      "Epoch: 282 | Batch: 004 / 011 | Total loss: 1.468 | Reg loss: 0.040 | Tree loss: 1.468 | Accuracy: 0.459000 | 1.676 sec/iter\n",
      "Epoch: 282 | Batch: 005 / 011 | Total loss: 1.451 | Reg loss: 0.040 | Tree loss: 1.451 | Accuracy: 0.481500 | 1.676 sec/iter\n",
      "Epoch: 282 | Batch: 006 / 011 | Total loss: 1.426 | Reg loss: 0.040 | Tree loss: 1.426 | Accuracy: 0.503000 | 1.676 sec/iter\n",
      "Epoch: 282 | Batch: 007 / 011 | Total loss: 1.417 | Reg loss: 0.040 | Tree loss: 1.417 | Accuracy: 0.492500 | 1.676 sec/iter\n",
      "Epoch: 282 | Batch: 008 / 011 | Total loss: 1.407 | Reg loss: 0.040 | Tree loss: 1.407 | Accuracy: 0.496500 | 1.676 sec/iter\n",
      "Epoch: 282 | Batch: 009 / 011 | Total loss: 1.403 | Reg loss: 0.040 | Tree loss: 1.403 | Accuracy: 0.513500 | 1.676 sec/iter\n",
      "Epoch: 282 | Batch: 010 / 011 | Total loss: 1.400 | Reg loss: 0.040 | Tree loss: 1.400 | Accuracy: 0.447099 | 1.675 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 283 | Batch: 000 / 011 | Total loss: 1.585 | Reg loss: 0.040 | Tree loss: 1.585 | Accuracy: 0.411000 | 1.677 sec/iter\n",
      "Epoch: 283 | Batch: 001 / 011 | Total loss: 1.581 | Reg loss: 0.040 | Tree loss: 1.581 | Accuracy: 0.411000 | 1.677 sec/iter\n",
      "Epoch: 283 | Batch: 002 / 011 | Total loss: 1.540 | Reg loss: 0.040 | Tree loss: 1.540 | Accuracy: 0.434000 | 1.677 sec/iter\n",
      "Epoch: 283 | Batch: 003 / 011 | Total loss: 1.498 | Reg loss: 0.040 | Tree loss: 1.498 | Accuracy: 0.449500 | 1.677 sec/iter\n",
      "Epoch: 283 | Batch: 004 / 011 | Total loss: 1.454 | Reg loss: 0.040 | Tree loss: 1.454 | Accuracy: 0.489000 | 1.677 sec/iter\n",
      "Epoch: 283 | Batch: 005 / 011 | Total loss: 1.438 | Reg loss: 0.040 | Tree loss: 1.438 | Accuracy: 0.510500 | 1.677 sec/iter\n",
      "Epoch: 283 | Batch: 006 / 011 | Total loss: 1.412 | Reg loss: 0.040 | Tree loss: 1.412 | Accuracy: 0.498500 | 1.677 sec/iter\n",
      "Epoch: 283 | Batch: 007 / 011 | Total loss: 1.410 | Reg loss: 0.040 | Tree loss: 1.410 | Accuracy: 0.517500 | 1.677 sec/iter\n",
      "Epoch: 283 | Batch: 008 / 011 | Total loss: 1.428 | Reg loss: 0.040 | Tree loss: 1.428 | Accuracy: 0.496000 | 1.677 sec/iter\n",
      "Epoch: 283 | Batch: 009 / 011 | Total loss: 1.416 | Reg loss: 0.040 | Tree loss: 1.416 | Accuracy: 0.490000 | 1.677 sec/iter\n",
      "Epoch: 283 | Batch: 010 / 011 | Total loss: 1.413 | Reg loss: 0.040 | Tree loss: 1.413 | Accuracy: 0.481229 | 1.676 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 284 | Batch: 000 / 011 | Total loss: 1.622 | Reg loss: 0.040 | Tree loss: 1.622 | Accuracy: 0.389500 | 1.676 sec/iter\n",
      "Epoch: 284 | Batch: 001 / 011 | Total loss: 1.568 | Reg loss: 0.040 | Tree loss: 1.568 | Accuracy: 0.416000 | 1.676 sec/iter\n",
      "Epoch: 284 | Batch: 002 / 011 | Total loss: 1.521 | Reg loss: 0.040 | Tree loss: 1.521 | Accuracy: 0.443000 | 1.676 sec/iter\n",
      "Epoch: 284 | Batch: 003 / 011 | Total loss: 1.494 | Reg loss: 0.040 | Tree loss: 1.494 | Accuracy: 0.481500 | 1.676 sec/iter\n",
      "Epoch: 284 | Batch: 004 / 011 | Total loss: 1.457 | Reg loss: 0.040 | Tree loss: 1.457 | Accuracy: 0.482000 | 1.676 sec/iter\n",
      "Epoch: 284 | Batch: 005 / 011 | Total loss: 1.445 | Reg loss: 0.040 | Tree loss: 1.445 | Accuracy: 0.496500 | 1.676 sec/iter\n",
      "Epoch: 284 | Batch: 006 / 011 | Total loss: 1.436 | Reg loss: 0.040 | Tree loss: 1.436 | Accuracy: 0.505500 | 1.676 sec/iter\n",
      "Epoch: 284 | Batch: 007 / 011 | Total loss: 1.417 | Reg loss: 0.040 | Tree loss: 1.417 | Accuracy: 0.513000 | 1.676 sec/iter\n",
      "Epoch: 284 | Batch: 008 / 011 | Total loss: 1.415 | Reg loss: 0.040 | Tree loss: 1.415 | Accuracy: 0.504500 | 1.676 sec/iter\n",
      "Epoch: 284 | Batch: 009 / 011 | Total loss: 1.420 | Reg loss: 0.040 | Tree loss: 1.420 | Accuracy: 0.479500 | 1.675 sec/iter\n",
      "Epoch: 284 | Batch: 010 / 011 | Total loss: 1.371 | Reg loss: 0.040 | Tree loss: 1.371 | Accuracy: 0.488055 | 1.675 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 285 | Batch: 000 / 011 | Total loss: 1.627 | Reg loss: 0.040 | Tree loss: 1.627 | Accuracy: 0.396500 | 1.677 sec/iter\n",
      "Epoch: 285 | Batch: 001 / 011 | Total loss: 1.587 | Reg loss: 0.040 | Tree loss: 1.587 | Accuracy: 0.396000 | 1.677 sec/iter\n",
      "Epoch: 285 | Batch: 002 / 011 | Total loss: 1.531 | Reg loss: 0.040 | Tree loss: 1.531 | Accuracy: 0.431500 | 1.677 sec/iter\n",
      "Epoch: 285 | Batch: 003 / 011 | Total loss: 1.497 | Reg loss: 0.040 | Tree loss: 1.497 | Accuracy: 0.470000 | 1.677 sec/iter\n",
      "Epoch: 285 | Batch: 004 / 011 | Total loss: 1.438 | Reg loss: 0.040 | Tree loss: 1.438 | Accuracy: 0.487500 | 1.677 sec/iter\n",
      "Epoch: 285 | Batch: 005 / 011 | Total loss: 1.427 | Reg loss: 0.040 | Tree loss: 1.427 | Accuracy: 0.500000 | 1.677 sec/iter\n",
      "Epoch: 285 | Batch: 006 / 011 | Total loss: 1.446 | Reg loss: 0.040 | Tree loss: 1.446 | Accuracy: 0.501000 | 1.677 sec/iter\n",
      "Epoch: 285 | Batch: 007 / 011 | Total loss: 1.416 | Reg loss: 0.040 | Tree loss: 1.416 | Accuracy: 0.508000 | 1.676 sec/iter\n",
      "Epoch: 285 | Batch: 008 / 011 | Total loss: 1.407 | Reg loss: 0.040 | Tree loss: 1.407 | Accuracy: 0.514000 | 1.676 sec/iter\n",
      "Epoch: 285 | Batch: 009 / 011 | Total loss: 1.404 | Reg loss: 0.040 | Tree loss: 1.404 | Accuracy: 0.507000 | 1.676 sec/iter\n",
      "Epoch: 285 | Batch: 010 / 011 | Total loss: 1.403 | Reg loss: 0.040 | Tree loss: 1.403 | Accuracy: 0.498294 | 1.676 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 286 | Batch: 000 / 011 | Total loss: 1.619 | Reg loss: 0.040 | Tree loss: 1.619 | Accuracy: 0.406000 | 1.676 sec/iter\n",
      "Epoch: 286 | Batch: 001 / 011 | Total loss: 1.563 | Reg loss: 0.040 | Tree loss: 1.563 | Accuracy: 0.415000 | 1.676 sec/iter\n",
      "Epoch: 286 | Batch: 002 / 011 | Total loss: 1.526 | Reg loss: 0.040 | Tree loss: 1.526 | Accuracy: 0.447000 | 1.676 sec/iter\n",
      "Epoch: 286 | Batch: 003 / 011 | Total loss: 1.519 | Reg loss: 0.040 | Tree loss: 1.519 | Accuracy: 0.449500 | 1.676 sec/iter\n",
      "Epoch: 286 | Batch: 004 / 011 | Total loss: 1.464 | Reg loss: 0.040 | Tree loss: 1.464 | Accuracy: 0.474000 | 1.676 sec/iter\n",
      "Epoch: 286 | Batch: 005 / 011 | Total loss: 1.414 | Reg loss: 0.040 | Tree loss: 1.414 | Accuracy: 0.512000 | 1.676 sec/iter\n",
      "Epoch: 286 | Batch: 006 / 011 | Total loss: 1.447 | Reg loss: 0.040 | Tree loss: 1.447 | Accuracy: 0.502500 | 1.675 sec/iter\n",
      "Epoch: 286 | Batch: 007 / 011 | Total loss: 1.401 | Reg loss: 0.040 | Tree loss: 1.401 | Accuracy: 0.523000 | 1.675 sec/iter\n",
      "Epoch: 286 | Batch: 008 / 011 | Total loss: 1.400 | Reg loss: 0.040 | Tree loss: 1.400 | Accuracy: 0.516000 | 1.675 sec/iter\n",
      "Epoch: 286 | Batch: 009 / 011 | Total loss: 1.407 | Reg loss: 0.040 | Tree loss: 1.407 | Accuracy: 0.513500 | 1.675 sec/iter\n",
      "Epoch: 286 | Batch: 010 / 011 | Total loss: 1.438 | Reg loss: 0.040 | Tree loss: 1.438 | Accuracy: 0.488055 | 1.675 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 287 | Batch: 000 / 011 | Total loss: 1.619 | Reg loss: 0.040 | Tree loss: 1.619 | Accuracy: 0.397000 | 1.677 sec/iter\n",
      "Epoch: 287 | Batch: 001 / 011 | Total loss: 1.606 | Reg loss: 0.040 | Tree loss: 1.606 | Accuracy: 0.399000 | 1.677 sec/iter\n",
      "Epoch: 287 | Batch: 002 / 011 | Total loss: 1.543 | Reg loss: 0.040 | Tree loss: 1.543 | Accuracy: 0.445000 | 1.677 sec/iter\n",
      "Epoch: 287 | Batch: 003 / 011 | Total loss: 1.466 | Reg loss: 0.040 | Tree loss: 1.466 | Accuracy: 0.468000 | 1.676 sec/iter\n",
      "Epoch: 287 | Batch: 004 / 011 | Total loss: 1.468 | Reg loss: 0.040 | Tree loss: 1.468 | Accuracy: 0.491000 | 1.676 sec/iter\n",
      "Epoch: 287 | Batch: 005 / 011 | Total loss: 1.448 | Reg loss: 0.040 | Tree loss: 1.448 | Accuracy: 0.507500 | 1.676 sec/iter\n",
      "Epoch: 287 | Batch: 006 / 011 | Total loss: 1.411 | Reg loss: 0.040 | Tree loss: 1.411 | Accuracy: 0.538500 | 1.676 sec/iter\n",
      "Epoch: 287 | Batch: 007 / 011 | Total loss: 1.410 | Reg loss: 0.040 | Tree loss: 1.410 | Accuracy: 0.522500 | 1.675 sec/iter\n",
      "Epoch: 287 | Batch: 008 / 011 | Total loss: 1.382 | Reg loss: 0.040 | Tree loss: 1.382 | Accuracy: 0.523000 | 1.675 sec/iter\n",
      "Epoch: 287 | Batch: 009 / 011 | Total loss: 1.409 | Reg loss: 0.040 | Tree loss: 1.409 | Accuracy: 0.486500 | 1.675 sec/iter\n",
      "Epoch: 287 | Batch: 010 / 011 | Total loss: 1.447 | Reg loss: 0.040 | Tree loss: 1.447 | Accuracy: 0.450512 | 1.675 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 288 | Batch: 000 / 011 | Total loss: 1.608 | Reg loss: 0.040 | Tree loss: 1.608 | Accuracy: 0.391000 | 1.676 sec/iter\n",
      "Epoch: 288 | Batch: 001 / 011 | Total loss: 1.594 | Reg loss: 0.040 | Tree loss: 1.594 | Accuracy: 0.409000 | 1.676 sec/iter\n",
      "Epoch: 288 | Batch: 002 / 011 | Total loss: 1.537 | Reg loss: 0.040 | Tree loss: 1.537 | Accuracy: 0.436000 | 1.676 sec/iter\n",
      "Epoch: 288 | Batch: 003 / 011 | Total loss: 1.499 | Reg loss: 0.040 | Tree loss: 1.499 | Accuracy: 0.448000 | 1.676 sec/iter\n",
      "Epoch: 288 | Batch: 004 / 011 | Total loss: 1.466 | Reg loss: 0.040 | Tree loss: 1.466 | Accuracy: 0.474000 | 1.676 sec/iter\n",
      "Epoch: 288 | Batch: 005 / 011 | Total loss: 1.430 | Reg loss: 0.040 | Tree loss: 1.430 | Accuracy: 0.504500 | 1.676 sec/iter\n",
      "Epoch: 288 | Batch: 006 / 011 | Total loss: 1.440 | Reg loss: 0.040 | Tree loss: 1.440 | Accuracy: 0.500500 | 1.676 sec/iter\n",
      "Epoch: 288 | Batch: 007 / 011 | Total loss: 1.394 | Reg loss: 0.040 | Tree loss: 1.394 | Accuracy: 0.525000 | 1.676 sec/iter\n",
      "Epoch: 288 | Batch: 008 / 011 | Total loss: 1.413 | Reg loss: 0.040 | Tree loss: 1.413 | Accuracy: 0.509000 | 1.676 sec/iter\n",
      "Epoch: 288 | Batch: 009 / 011 | Total loss: 1.410 | Reg loss: 0.040 | Tree loss: 1.410 | Accuracy: 0.488000 | 1.676 sec/iter\n",
      "Epoch: 288 | Batch: 010 / 011 | Total loss: 1.417 | Reg loss: 0.040 | Tree loss: 1.417 | Accuracy: 0.481229 | 1.675 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 289 | Batch: 000 / 011 | Total loss: 1.633 | Reg loss: 0.040 | Tree loss: 1.633 | Accuracy: 0.381500 | 1.675 sec/iter\n",
      "Epoch: 289 | Batch: 001 / 011 | Total loss: 1.570 | Reg loss: 0.040 | Tree loss: 1.570 | Accuracy: 0.417000 | 1.675 sec/iter\n",
      "Epoch: 289 | Batch: 002 / 011 | Total loss: 1.549 | Reg loss: 0.040 | Tree loss: 1.549 | Accuracy: 0.425500 | 1.675 sec/iter\n",
      "Epoch: 289 | Batch: 003 / 011 | Total loss: 1.496 | Reg loss: 0.040 | Tree loss: 1.496 | Accuracy: 0.457500 | 1.675 sec/iter\n",
      "Epoch: 289 | Batch: 004 / 011 | Total loss: 1.470 | Reg loss: 0.040 | Tree loss: 1.470 | Accuracy: 0.470500 | 1.675 sec/iter\n",
      "Epoch: 289 | Batch: 005 / 011 | Total loss: 1.418 | Reg loss: 0.040 | Tree loss: 1.418 | Accuracy: 0.512000 | 1.675 sec/iter\n",
      "Epoch: 289 | Batch: 006 / 011 | Total loss: 1.416 | Reg loss: 0.040 | Tree loss: 1.416 | Accuracy: 0.507000 | 1.675 sec/iter\n",
      "Epoch: 289 | Batch: 007 / 011 | Total loss: 1.408 | Reg loss: 0.040 | Tree loss: 1.408 | Accuracy: 0.513000 | 1.675 sec/iter\n",
      "Epoch: 289 | Batch: 008 / 011 | Total loss: 1.399 | Reg loss: 0.040 | Tree loss: 1.399 | Accuracy: 0.518500 | 1.675 sec/iter\n",
      "Epoch: 289 | Batch: 009 / 011 | Total loss: 1.411 | Reg loss: 0.040 | Tree loss: 1.411 | Accuracy: 0.494000 | 1.674 sec/iter\n",
      "Epoch: 289 | Batch: 010 / 011 | Total loss: 1.389 | Reg loss: 0.040 | Tree loss: 1.389 | Accuracy: 0.529010 | 1.674 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 290 | Batch: 000 / 011 | Total loss: 1.622 | Reg loss: 0.040 | Tree loss: 1.622 | Accuracy: 0.394000 | 1.676 sec/iter\n",
      "Epoch: 290 | Batch: 001 / 011 | Total loss: 1.587 | Reg loss: 0.040 | Tree loss: 1.587 | Accuracy: 0.418500 | 1.676 sec/iter\n",
      "Epoch: 290 | Batch: 002 / 011 | Total loss: 1.550 | Reg loss: 0.040 | Tree loss: 1.550 | Accuracy: 0.435500 | 1.676 sec/iter\n",
      "Epoch: 290 | Batch: 003 / 011 | Total loss: 1.508 | Reg loss: 0.040 | Tree loss: 1.508 | Accuracy: 0.460500 | 1.676 sec/iter\n",
      "Epoch: 290 | Batch: 004 / 011 | Total loss: 1.478 | Reg loss: 0.040 | Tree loss: 1.478 | Accuracy: 0.472500 | 1.676 sec/iter\n",
      "Epoch: 290 | Batch: 005 / 011 | Total loss: 1.419 | Reg loss: 0.040 | Tree loss: 1.419 | Accuracy: 0.497000 | 1.675 sec/iter\n",
      "Epoch: 290 | Batch: 006 / 011 | Total loss: 1.427 | Reg loss: 0.040 | Tree loss: 1.427 | Accuracy: 0.504000 | 1.675 sec/iter\n",
      "Epoch: 290 | Batch: 007 / 011 | Total loss: 1.393 | Reg loss: 0.040 | Tree loss: 1.393 | Accuracy: 0.520000 | 1.675 sec/iter\n",
      "Epoch: 290 | Batch: 008 / 011 | Total loss: 1.410 | Reg loss: 0.040 | Tree loss: 1.410 | Accuracy: 0.490500 | 1.675 sec/iter\n",
      "Epoch: 290 | Batch: 009 / 011 | Total loss: 1.370 | Reg loss: 0.040 | Tree loss: 1.370 | Accuracy: 0.516500 | 1.675 sec/iter\n",
      "Epoch: 290 | Batch: 010 / 011 | Total loss: 1.467 | Reg loss: 0.040 | Tree loss: 1.467 | Accuracy: 0.467577 | 1.675 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 291 | Batch: 000 / 011 | Total loss: 1.616 | Reg loss: 0.040 | Tree loss: 1.616 | Accuracy: 0.388000 | 1.675 sec/iter\n",
      "Epoch: 291 | Batch: 001 / 011 | Total loss: 1.596 | Reg loss: 0.040 | Tree loss: 1.596 | Accuracy: 0.401500 | 1.675 sec/iter\n",
      "Epoch: 291 | Batch: 002 / 011 | Total loss: 1.536 | Reg loss: 0.040 | Tree loss: 1.536 | Accuracy: 0.454500 | 1.675 sec/iter\n",
      "Epoch: 291 | Batch: 003 / 011 | Total loss: 1.470 | Reg loss: 0.040 | Tree loss: 1.470 | Accuracy: 0.479000 | 1.675 sec/iter\n",
      "Epoch: 291 | Batch: 004 / 011 | Total loss: 1.459 | Reg loss: 0.040 | Tree loss: 1.459 | Accuracy: 0.489000 | 1.675 sec/iter\n",
      "Epoch: 291 | Batch: 005 / 011 | Total loss: 1.436 | Reg loss: 0.040 | Tree loss: 1.436 | Accuracy: 0.497000 | 1.674 sec/iter\n",
      "Epoch: 291 | Batch: 006 / 011 | Total loss: 1.415 | Reg loss: 0.040 | Tree loss: 1.415 | Accuracy: 0.524000 | 1.674 sec/iter\n",
      "Epoch: 291 | Batch: 007 / 011 | Total loss: 1.416 | Reg loss: 0.040 | Tree loss: 1.416 | Accuracy: 0.509000 | 1.674 sec/iter\n",
      "Epoch: 291 | Batch: 008 / 011 | Total loss: 1.422 | Reg loss: 0.040 | Tree loss: 1.422 | Accuracy: 0.486500 | 1.674 sec/iter\n",
      "Epoch: 291 | Batch: 009 / 011 | Total loss: 1.401 | Reg loss: 0.040 | Tree loss: 1.401 | Accuracy: 0.496000 | 1.674 sec/iter\n",
      "Epoch: 291 | Batch: 010 / 011 | Total loss: 1.432 | Reg loss: 0.040 | Tree loss: 1.432 | Accuracy: 0.477816 | 1.674 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 292 | Batch: 000 / 011 | Total loss: 1.623 | Reg loss: 0.040 | Tree loss: 1.623 | Accuracy: 0.376500 | 1.676 sec/iter\n",
      "Epoch: 292 | Batch: 001 / 011 | Total loss: 1.563 | Reg loss: 0.040 | Tree loss: 1.563 | Accuracy: 0.435000 | 1.676 sec/iter\n",
      "Epoch: 292 | Batch: 002 / 011 | Total loss: 1.529 | Reg loss: 0.040 | Tree loss: 1.529 | Accuracy: 0.450000 | 1.676 sec/iter\n",
      "Epoch: 292 | Batch: 003 / 011 | Total loss: 1.524 | Reg loss: 0.040 | Tree loss: 1.524 | Accuracy: 0.450000 | 1.675 sec/iter\n",
      "Epoch: 292 | Batch: 004 / 011 | Total loss: 1.463 | Reg loss: 0.040 | Tree loss: 1.463 | Accuracy: 0.479500 | 1.675 sec/iter\n",
      "Epoch: 292 | Batch: 005 / 011 | Total loss: 1.426 | Reg loss: 0.040 | Tree loss: 1.426 | Accuracy: 0.505500 | 1.675 sec/iter\n",
      "Epoch: 292 | Batch: 006 / 011 | Total loss: 1.444 | Reg loss: 0.040 | Tree loss: 1.444 | Accuracy: 0.494500 | 1.675 sec/iter\n",
      "Epoch: 292 | Batch: 007 / 011 | Total loss: 1.389 | Reg loss: 0.040 | Tree loss: 1.389 | Accuracy: 0.529500 | 1.675 sec/iter\n",
      "Epoch: 292 | Batch: 008 / 011 | Total loss: 1.412 | Reg loss: 0.040 | Tree loss: 1.412 | Accuracy: 0.491000 | 1.675 sec/iter\n",
      "Epoch: 292 | Batch: 009 / 011 | Total loss: 1.384 | Reg loss: 0.040 | Tree loss: 1.384 | Accuracy: 0.503500 | 1.675 sec/iter\n",
      "Epoch: 292 | Batch: 010 / 011 | Total loss: 1.393 | Reg loss: 0.040 | Tree loss: 1.393 | Accuracy: 0.494881 | 1.675 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 293 | Batch: 000 / 011 | Total loss: 1.613 | Reg loss: 0.040 | Tree loss: 1.613 | Accuracy: 0.410000 | 1.675 sec/iter\n",
      "Epoch: 293 | Batch: 001 / 011 | Total loss: 1.596 | Reg loss: 0.040 | Tree loss: 1.596 | Accuracy: 0.397500 | 1.675 sec/iter\n",
      "Epoch: 293 | Batch: 002 / 011 | Total loss: 1.527 | Reg loss: 0.040 | Tree loss: 1.527 | Accuracy: 0.437500 | 1.674 sec/iter\n",
      "Epoch: 293 | Batch: 003 / 011 | Total loss: 1.530 | Reg loss: 0.040 | Tree loss: 1.530 | Accuracy: 0.429500 | 1.674 sec/iter\n",
      "Epoch: 293 | Batch: 004 / 011 | Total loss: 1.452 | Reg loss: 0.040 | Tree loss: 1.452 | Accuracy: 0.487500 | 1.674 sec/iter\n",
      "Epoch: 293 | Batch: 005 / 011 | Total loss: 1.429 | Reg loss: 0.040 | Tree loss: 1.429 | Accuracy: 0.489500 | 1.674 sec/iter\n",
      "Epoch: 293 | Batch: 006 / 011 | Total loss: 1.427 | Reg loss: 0.040 | Tree loss: 1.427 | Accuracy: 0.526500 | 1.674 sec/iter\n",
      "Epoch: 293 | Batch: 007 / 011 | Total loss: 1.419 | Reg loss: 0.040 | Tree loss: 1.419 | Accuracy: 0.516500 | 1.674 sec/iter\n",
      "Epoch: 293 | Batch: 008 / 011 | Total loss: 1.399 | Reg loss: 0.040 | Tree loss: 1.399 | Accuracy: 0.516500 | 1.674 sec/iter\n",
      "Epoch: 293 | Batch: 009 / 011 | Total loss: 1.396 | Reg loss: 0.040 | Tree loss: 1.396 | Accuracy: 0.500500 | 1.674 sec/iter\n",
      "Epoch: 293 | Batch: 010 / 011 | Total loss: 1.410 | Reg loss: 0.040 | Tree loss: 1.410 | Accuracy: 0.464164 | 1.674 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 294 | Batch: 000 / 011 | Total loss: 1.612 | Reg loss: 0.040 | Tree loss: 1.612 | Accuracy: 0.392000 | 1.676 sec/iter\n",
      "Epoch: 294 | Batch: 001 / 011 | Total loss: 1.571 | Reg loss: 0.040 | Tree loss: 1.571 | Accuracy: 0.416500 | 1.675 sec/iter\n",
      "Epoch: 294 | Batch: 002 / 011 | Total loss: 1.554 | Reg loss: 0.040 | Tree loss: 1.554 | Accuracy: 0.421500 | 1.675 sec/iter\n",
      "Epoch: 294 | Batch: 003 / 011 | Total loss: 1.497 | Reg loss: 0.040 | Tree loss: 1.497 | Accuracy: 0.478000 | 1.675 sec/iter\n",
      "Epoch: 294 | Batch: 004 / 011 | Total loss: 1.469 | Reg loss: 0.040 | Tree loss: 1.469 | Accuracy: 0.485500 | 1.675 sec/iter\n",
      "Epoch: 294 | Batch: 005 / 011 | Total loss: 1.443 | Reg loss: 0.040 | Tree loss: 1.443 | Accuracy: 0.494500 | 1.675 sec/iter\n",
      "Epoch: 294 | Batch: 006 / 011 | Total loss: 1.428 | Reg loss: 0.040 | Tree loss: 1.428 | Accuracy: 0.512500 | 1.674 sec/iter\n",
      "Epoch: 294 | Batch: 007 / 011 | Total loss: 1.382 | Reg loss: 0.040 | Tree loss: 1.382 | Accuracy: 0.528000 | 1.674 sec/iter\n",
      "Epoch: 294 | Batch: 008 / 011 | Total loss: 1.396 | Reg loss: 0.040 | Tree loss: 1.396 | Accuracy: 0.512000 | 1.674 sec/iter\n",
      "Epoch: 294 | Batch: 009 / 011 | Total loss: 1.432 | Reg loss: 0.040 | Tree loss: 1.432 | Accuracy: 0.495500 | 1.674 sec/iter\n",
      "Epoch: 294 | Batch: 010 / 011 | Total loss: 1.317 | Reg loss: 0.040 | Tree loss: 1.317 | Accuracy: 0.552901 | 1.674 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 295 | Batch: 000 / 011 | Total loss: 1.600 | Reg loss: 0.040 | Tree loss: 1.600 | Accuracy: 0.397500 | 1.675 sec/iter\n",
      "Epoch: 295 | Batch: 001 / 011 | Total loss: 1.595 | Reg loss: 0.040 | Tree loss: 1.595 | Accuracy: 0.394000 | 1.675 sec/iter\n",
      "Epoch: 295 | Batch: 002 / 011 | Total loss: 1.533 | Reg loss: 0.040 | Tree loss: 1.533 | Accuracy: 0.432500 | 1.675 sec/iter\n",
      "Epoch: 295 | Batch: 003 / 011 | Total loss: 1.480 | Reg loss: 0.040 | Tree loss: 1.480 | Accuracy: 0.477500 | 1.675 sec/iter\n",
      "Epoch: 295 | Batch: 004 / 011 | Total loss: 1.466 | Reg loss: 0.040 | Tree loss: 1.466 | Accuracy: 0.486000 | 1.675 sec/iter\n",
      "Epoch: 295 | Batch: 005 / 011 | Total loss: 1.426 | Reg loss: 0.040 | Tree loss: 1.426 | Accuracy: 0.506000 | 1.675 sec/iter\n",
      "Epoch: 295 | Batch: 006 / 011 | Total loss: 1.414 | Reg loss: 0.040 | Tree loss: 1.414 | Accuracy: 0.520500 | 1.675 sec/iter\n",
      "Epoch: 295 | Batch: 007 / 011 | Total loss: 1.420 | Reg loss: 0.040 | Tree loss: 1.420 | Accuracy: 0.511500 | 1.674 sec/iter\n",
      "Epoch: 295 | Batch: 008 / 011 | Total loss: 1.410 | Reg loss: 0.040 | Tree loss: 1.410 | Accuracy: 0.492500 | 1.674 sec/iter\n",
      "Epoch: 295 | Batch: 009 / 011 | Total loss: 1.407 | Reg loss: 0.040 | Tree loss: 1.407 | Accuracy: 0.527000 | 1.674 sec/iter\n",
      "Epoch: 295 | Batch: 010 / 011 | Total loss: 1.403 | Reg loss: 0.040 | Tree loss: 1.403 | Accuracy: 0.443686 | 1.674 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 296 | Batch: 000 / 011 | Total loss: 1.627 | Reg loss: 0.040 | Tree loss: 1.627 | Accuracy: 0.381000 | 1.674 sec/iter\n",
      "Epoch: 296 | Batch: 001 / 011 | Total loss: 1.578 | Reg loss: 0.040 | Tree loss: 1.578 | Accuracy: 0.411000 | 1.674 sec/iter\n",
      "Epoch: 296 | Batch: 002 / 011 | Total loss: 1.533 | Reg loss: 0.040 | Tree loss: 1.533 | Accuracy: 0.456000 | 1.674 sec/iter\n",
      "Epoch: 296 | Batch: 003 / 011 | Total loss: 1.484 | Reg loss: 0.040 | Tree loss: 1.484 | Accuracy: 0.474500 | 1.674 sec/iter\n",
      "Epoch: 296 | Batch: 004 / 011 | Total loss: 1.460 | Reg loss: 0.040 | Tree loss: 1.460 | Accuracy: 0.468500 | 1.674 sec/iter\n",
      "Epoch: 296 | Batch: 005 / 011 | Total loss: 1.452 | Reg loss: 0.040 | Tree loss: 1.452 | Accuracy: 0.495000 | 1.674 sec/iter\n",
      "Epoch: 296 | Batch: 006 / 011 | Total loss: 1.426 | Reg loss: 0.040 | Tree loss: 1.426 | Accuracy: 0.520000 | 1.674 sec/iter\n",
      "Epoch: 296 | Batch: 007 / 011 | Total loss: 1.390 | Reg loss: 0.040 | Tree loss: 1.390 | Accuracy: 0.528500 | 1.673 sec/iter\n",
      "Epoch: 296 | Batch: 008 / 011 | Total loss: 1.418 | Reg loss: 0.040 | Tree loss: 1.418 | Accuracy: 0.514000 | 1.673 sec/iter\n",
      "Epoch: 296 | Batch: 009 / 011 | Total loss: 1.416 | Reg loss: 0.040 | Tree loss: 1.416 | Accuracy: 0.502000 | 1.673 sec/iter\n",
      "Epoch: 296 | Batch: 010 / 011 | Total loss: 1.430 | Reg loss: 0.040 | Tree loss: 1.430 | Accuracy: 0.470990 | 1.673 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 297 | Batch: 000 / 011 | Total loss: 1.600 | Reg loss: 0.040 | Tree loss: 1.600 | Accuracy: 0.399000 | 1.675 sec/iter\n",
      "Epoch: 297 | Batch: 001 / 011 | Total loss: 1.560 | Reg loss: 0.040 | Tree loss: 1.560 | Accuracy: 0.422500 | 1.675 sec/iter\n",
      "Epoch: 297 | Batch: 002 / 011 | Total loss: 1.530 | Reg loss: 0.040 | Tree loss: 1.530 | Accuracy: 0.426000 | 1.675 sec/iter\n",
      "Epoch: 297 | Batch: 003 / 011 | Total loss: 1.480 | Reg loss: 0.040 | Tree loss: 1.480 | Accuracy: 0.469500 | 1.675 sec/iter\n",
      "Epoch: 297 | Batch: 004 / 011 | Total loss: 1.507 | Reg loss: 0.040 | Tree loss: 1.507 | Accuracy: 0.457000 | 1.675 sec/iter\n",
      "Epoch: 297 | Batch: 005 / 011 | Total loss: 1.435 | Reg loss: 0.040 | Tree loss: 1.435 | Accuracy: 0.507000 | 1.675 sec/iter\n",
      "Epoch: 297 | Batch: 006 / 011 | Total loss: 1.429 | Reg loss: 0.040 | Tree loss: 1.429 | Accuracy: 0.500000 | 1.675 sec/iter\n",
      "Epoch: 297 | Batch: 007 / 011 | Total loss: 1.405 | Reg loss: 0.040 | Tree loss: 1.405 | Accuracy: 0.509000 | 1.675 sec/iter\n",
      "Epoch: 297 | Batch: 008 / 011 | Total loss: 1.392 | Reg loss: 0.040 | Tree loss: 1.392 | Accuracy: 0.524000 | 1.675 sec/iter\n",
      "Epoch: 297 | Batch: 009 / 011 | Total loss: 1.412 | Reg loss: 0.040 | Tree loss: 1.412 | Accuracy: 0.494500 | 1.675 sec/iter\n",
      "Epoch: 297 | Batch: 010 / 011 | Total loss: 1.430 | Reg loss: 0.040 | Tree loss: 1.430 | Accuracy: 0.508532 | 1.675 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 298 | Batch: 000 / 011 | Total loss: 1.599 | Reg loss: 0.040 | Tree loss: 1.599 | Accuracy: 0.403000 | 1.676 sec/iter\n",
      "Epoch: 298 | Batch: 001 / 011 | Total loss: 1.591 | Reg loss: 0.040 | Tree loss: 1.591 | Accuracy: 0.388000 | 1.676 sec/iter\n",
      "Epoch: 298 | Batch: 002 / 011 | Total loss: 1.513 | Reg loss: 0.040 | Tree loss: 1.513 | Accuracy: 0.453500 | 1.676 sec/iter\n",
      "Epoch: 298 | Batch: 003 / 011 | Total loss: 1.495 | Reg loss: 0.040 | Tree loss: 1.495 | Accuracy: 0.476000 | 1.676 sec/iter\n",
      "Epoch: 298 | Batch: 004 / 011 | Total loss: 1.465 | Reg loss: 0.040 | Tree loss: 1.465 | Accuracy: 0.475500 | 1.676 sec/iter\n",
      "Epoch: 298 | Batch: 005 / 011 | Total loss: 1.429 | Reg loss: 0.040 | Tree loss: 1.429 | Accuracy: 0.512500 | 1.677 sec/iter\n",
      "Epoch: 298 | Batch: 006 / 011 | Total loss: 1.427 | Reg loss: 0.040 | Tree loss: 1.427 | Accuracy: 0.524000 | 1.677 sec/iter\n",
      "Epoch: 298 | Batch: 007 / 011 | Total loss: 1.415 | Reg loss: 0.040 | Tree loss: 1.415 | Accuracy: 0.515500 | 1.677 sec/iter\n",
      "Epoch: 298 | Batch: 008 / 011 | Total loss: 1.415 | Reg loss: 0.040 | Tree loss: 1.415 | Accuracy: 0.504000 | 1.677 sec/iter\n",
      "Epoch: 298 | Batch: 009 / 011 | Total loss: 1.402 | Reg loss: 0.040 | Tree loss: 1.402 | Accuracy: 0.496000 | 1.677 sec/iter\n",
      "Epoch: 298 | Batch: 010 / 011 | Total loss: 1.414 | Reg loss: 0.040 | Tree loss: 1.414 | Accuracy: 0.491468 | 1.676 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 299 | Batch: 000 / 011 | Total loss: 1.615 | Reg loss: 0.040 | Tree loss: 1.615 | Accuracy: 0.392000 | 1.678 sec/iter\n",
      "Epoch: 299 | Batch: 001 / 011 | Total loss: 1.569 | Reg loss: 0.040 | Tree loss: 1.569 | Accuracy: 0.414500 | 1.678 sec/iter\n",
      "Epoch: 299 | Batch: 002 / 011 | Total loss: 1.539 | Reg loss: 0.040 | Tree loss: 1.539 | Accuracy: 0.436000 | 1.678 sec/iter\n",
      "Epoch: 299 | Batch: 003 / 011 | Total loss: 1.489 | Reg loss: 0.040 | Tree loss: 1.489 | Accuracy: 0.449500 | 1.678 sec/iter\n",
      "Epoch: 299 | Batch: 004 / 011 | Total loss: 1.495 | Reg loss: 0.040 | Tree loss: 1.495 | Accuracy: 0.455000 | 1.678 sec/iter\n",
      "Epoch: 299 | Batch: 005 / 011 | Total loss: 1.429 | Reg loss: 0.040 | Tree loss: 1.429 | Accuracy: 0.500000 | 1.678 sec/iter\n",
      "Epoch: 299 | Batch: 006 / 011 | Total loss: 1.412 | Reg loss: 0.040 | Tree loss: 1.412 | Accuracy: 0.500500 | 1.678 sec/iter\n",
      "Epoch: 299 | Batch: 007 / 011 | Total loss: 1.400 | Reg loss: 0.040 | Tree loss: 1.400 | Accuracy: 0.520500 | 1.678 sec/iter\n",
      "Epoch: 299 | Batch: 008 / 011 | Total loss: 1.411 | Reg loss: 0.040 | Tree loss: 1.411 | Accuracy: 0.512500 | 1.678 sec/iter\n",
      "Epoch: 299 | Batch: 009 / 011 | Total loss: 1.405 | Reg loss: 0.040 | Tree loss: 1.405 | Accuracy: 0.495000 | 1.678 sec/iter\n",
      "Epoch: 299 | Batch: 010 / 011 | Total loss: 1.383 | Reg loss: 0.040 | Tree loss: 1.383 | Accuracy: 0.491468 | 1.678 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 300 | Batch: 000 / 011 | Total loss: 1.600 | Reg loss: 0.040 | Tree loss: 1.600 | Accuracy: 0.404000 | 1.68 sec/iter\n",
      "Epoch: 300 | Batch: 001 / 011 | Total loss: 1.590 | Reg loss: 0.040 | Tree loss: 1.590 | Accuracy: 0.401500 | 1.679 sec/iter\n",
      "Epoch: 300 | Batch: 002 / 011 | Total loss: 1.523 | Reg loss: 0.040 | Tree loss: 1.523 | Accuracy: 0.459000 | 1.679 sec/iter\n",
      "Epoch: 300 | Batch: 003 / 011 | Total loss: 1.489 | Reg loss: 0.040 | Tree loss: 1.489 | Accuracy: 0.452000 | 1.679 sec/iter\n",
      "Epoch: 300 | Batch: 004 / 011 | Total loss: 1.473 | Reg loss: 0.040 | Tree loss: 1.473 | Accuracy: 0.490000 | 1.679 sec/iter\n",
      "Epoch: 300 | Batch: 005 / 011 | Total loss: 1.424 | Reg loss: 0.040 | Tree loss: 1.424 | Accuracy: 0.499500 | 1.679 sec/iter\n",
      "Epoch: 300 | Batch: 006 / 011 | Total loss: 1.421 | Reg loss: 0.040 | Tree loss: 1.421 | Accuracy: 0.503500 | 1.679 sec/iter\n",
      "Epoch: 300 | Batch: 007 / 011 | Total loss: 1.414 | Reg loss: 0.040 | Tree loss: 1.414 | Accuracy: 0.510000 | 1.679 sec/iter\n",
      "Epoch: 300 | Batch: 008 / 011 | Total loss: 1.424 | Reg loss: 0.040 | Tree loss: 1.424 | Accuracy: 0.484500 | 1.679 sec/iter\n",
      "Epoch: 300 | Batch: 009 / 011 | Total loss: 1.406 | Reg loss: 0.040 | Tree loss: 1.406 | Accuracy: 0.489000 | 1.679 sec/iter\n",
      "Epoch: 300 | Batch: 010 / 011 | Total loss: 1.407 | Reg loss: 0.040 | Tree loss: 1.407 | Accuracy: 0.508532 | 1.679 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 301 | Batch: 000 / 011 | Total loss: 1.585 | Reg loss: 0.040 | Tree loss: 1.585 | Accuracy: 0.391500 | 1.68 sec/iter\n",
      "Epoch: 301 | Batch: 001 / 011 | Total loss: 1.582 | Reg loss: 0.040 | Tree loss: 1.582 | Accuracy: 0.399500 | 1.68 sec/iter\n",
      "Epoch: 301 | Batch: 002 / 011 | Total loss: 1.554 | Reg loss: 0.040 | Tree loss: 1.554 | Accuracy: 0.425000 | 1.68 sec/iter\n",
      "Epoch: 301 | Batch: 003 / 011 | Total loss: 1.505 | Reg loss: 0.040 | Tree loss: 1.505 | Accuracy: 0.455000 | 1.68 sec/iter\n",
      "Epoch: 301 | Batch: 004 / 011 | Total loss: 1.461 | Reg loss: 0.040 | Tree loss: 1.461 | Accuracy: 0.492000 | 1.68 sec/iter\n",
      "Epoch: 301 | Batch: 005 / 011 | Total loss: 1.446 | Reg loss: 0.040 | Tree loss: 1.446 | Accuracy: 0.498000 | 1.68 sec/iter\n",
      "Epoch: 301 | Batch: 006 / 011 | Total loss: 1.422 | Reg loss: 0.040 | Tree loss: 1.422 | Accuracy: 0.504000 | 1.68 sec/iter\n",
      "Epoch: 301 | Batch: 007 / 011 | Total loss: 1.415 | Reg loss: 0.040 | Tree loss: 1.415 | Accuracy: 0.500500 | 1.68 sec/iter\n",
      "Epoch: 301 | Batch: 008 / 011 | Total loss: 1.401 | Reg loss: 0.040 | Tree loss: 1.401 | Accuracy: 0.493500 | 1.68 sec/iter\n",
      "Epoch: 301 | Batch: 009 / 011 | Total loss: 1.399 | Reg loss: 0.040 | Tree loss: 1.399 | Accuracy: 0.484500 | 1.68 sec/iter\n",
      "Epoch: 301 | Batch: 010 / 011 | Total loss: 1.434 | Reg loss: 0.040 | Tree loss: 1.434 | Accuracy: 0.474403 | 1.68 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 302 | Batch: 000 / 011 | Total loss: 1.595 | Reg loss: 0.040 | Tree loss: 1.595 | Accuracy: 0.407500 | 1.681 sec/iter\n",
      "Epoch: 302 | Batch: 001 / 011 | Total loss: 1.582 | Reg loss: 0.040 | Tree loss: 1.582 | Accuracy: 0.417000 | 1.681 sec/iter\n",
      "Epoch: 302 | Batch: 002 / 011 | Total loss: 1.540 | Reg loss: 0.040 | Tree loss: 1.540 | Accuracy: 0.444000 | 1.681 sec/iter\n",
      "Epoch: 302 | Batch: 003 / 011 | Total loss: 1.507 | Reg loss: 0.040 | Tree loss: 1.507 | Accuracy: 0.464500 | 1.681 sec/iter\n",
      "Epoch: 302 | Batch: 004 / 011 | Total loss: 1.437 | Reg loss: 0.040 | Tree loss: 1.437 | Accuracy: 0.497000 | 1.681 sec/iter\n",
      "Epoch: 302 | Batch: 005 / 011 | Total loss: 1.440 | Reg loss: 0.040 | Tree loss: 1.440 | Accuracy: 0.493000 | 1.681 sec/iter\n",
      "Epoch: 302 | Batch: 006 / 011 | Total loss: 1.419 | Reg loss: 0.040 | Tree loss: 1.419 | Accuracy: 0.521000 | 1.681 sec/iter\n",
      "Epoch: 302 | Batch: 007 / 011 | Total loss: 1.428 | Reg loss: 0.040 | Tree loss: 1.428 | Accuracy: 0.503000 | 1.681 sec/iter\n",
      "Epoch: 302 | Batch: 008 / 011 | Total loss: 1.402 | Reg loss: 0.040 | Tree loss: 1.402 | Accuracy: 0.493500 | 1.681 sec/iter\n",
      "Epoch: 302 | Batch: 009 / 011 | Total loss: 1.404 | Reg loss: 0.040 | Tree loss: 1.404 | Accuracy: 0.500500 | 1.681 sec/iter\n",
      "Epoch: 302 | Batch: 010 / 011 | Total loss: 1.385 | Reg loss: 0.040 | Tree loss: 1.385 | Accuracy: 0.505119 | 1.681 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 303 | Batch: 000 / 011 | Total loss: 1.626 | Reg loss: 0.040 | Tree loss: 1.626 | Accuracy: 0.386000 | 1.683 sec/iter\n",
      "Epoch: 303 | Batch: 001 / 011 | Total loss: 1.568 | Reg loss: 0.040 | Tree loss: 1.568 | Accuracy: 0.426500 | 1.683 sec/iter\n",
      "Epoch: 303 | Batch: 002 / 011 | Total loss: 1.519 | Reg loss: 0.040 | Tree loss: 1.519 | Accuracy: 0.447500 | 1.683 sec/iter\n",
      "Epoch: 303 | Batch: 003 / 011 | Total loss: 1.491 | Reg loss: 0.040 | Tree loss: 1.491 | Accuracy: 0.461000 | 1.683 sec/iter\n",
      "Epoch: 303 | Batch: 004 / 011 | Total loss: 1.463 | Reg loss: 0.040 | Tree loss: 1.463 | Accuracy: 0.480000 | 1.683 sec/iter\n",
      "Epoch: 303 | Batch: 005 / 011 | Total loss: 1.442 | Reg loss: 0.040 | Tree loss: 1.442 | Accuracy: 0.487500 | 1.683 sec/iter\n",
      "Epoch: 303 | Batch: 006 / 011 | Total loss: 1.406 | Reg loss: 0.040 | Tree loss: 1.406 | Accuracy: 0.518000 | 1.683 sec/iter\n",
      "Epoch: 303 | Batch: 007 / 011 | Total loss: 1.422 | Reg loss: 0.040 | Tree loss: 1.422 | Accuracy: 0.515500 | 1.683 sec/iter\n",
      "Epoch: 303 | Batch: 008 / 011 | Total loss: 1.418 | Reg loss: 0.040 | Tree loss: 1.418 | Accuracy: 0.509500 | 1.683 sec/iter\n",
      "Epoch: 303 | Batch: 009 / 011 | Total loss: 1.403 | Reg loss: 0.040 | Tree loss: 1.403 | Accuracy: 0.491500 | 1.683 sec/iter\n",
      "Epoch: 303 | Batch: 010 / 011 | Total loss: 1.353 | Reg loss: 0.040 | Tree loss: 1.353 | Accuracy: 0.525597 | 1.683 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 304 | Batch: 000 / 011 | Total loss: 1.597 | Reg loss: 0.040 | Tree loss: 1.597 | Accuracy: 0.400500 | 1.684 sec/iter\n",
      "Epoch: 304 | Batch: 001 / 011 | Total loss: 1.573 | Reg loss: 0.040 | Tree loss: 1.573 | Accuracy: 0.438000 | 1.684 sec/iter\n",
      "Epoch: 304 | Batch: 002 / 011 | Total loss: 1.565 | Reg loss: 0.040 | Tree loss: 1.565 | Accuracy: 0.430500 | 1.684 sec/iter\n",
      "Epoch: 304 | Batch: 003 / 011 | Total loss: 1.477 | Reg loss: 0.040 | Tree loss: 1.477 | Accuracy: 0.463000 | 1.684 sec/iter\n",
      "Epoch: 304 | Batch: 004 / 011 | Total loss: 1.494 | Reg loss: 0.040 | Tree loss: 1.494 | Accuracy: 0.465500 | 1.684 sec/iter\n",
      "Epoch: 304 | Batch: 005 / 011 | Total loss: 1.449 | Reg loss: 0.040 | Tree loss: 1.449 | Accuracy: 0.499500 | 1.684 sec/iter\n",
      "Epoch: 304 | Batch: 006 / 011 | Total loss: 1.396 | Reg loss: 0.040 | Tree loss: 1.396 | Accuracy: 0.529000 | 1.684 sec/iter\n",
      "Epoch: 304 | Batch: 007 / 011 | Total loss: 1.411 | Reg loss: 0.040 | Tree loss: 1.411 | Accuracy: 0.504500 | 1.684 sec/iter\n",
      "Epoch: 304 | Batch: 008 / 011 | Total loss: 1.390 | Reg loss: 0.040 | Tree loss: 1.390 | Accuracy: 0.506500 | 1.684 sec/iter\n",
      "Epoch: 304 | Batch: 009 / 011 | Total loss: 1.403 | Reg loss: 0.040 | Tree loss: 1.403 | Accuracy: 0.496500 | 1.684 sec/iter\n",
      "Epoch: 304 | Batch: 010 / 011 | Total loss: 1.416 | Reg loss: 0.040 | Tree loss: 1.416 | Accuracy: 0.440273 | 1.684 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 305 | Batch: 000 / 011 | Total loss: 1.598 | Reg loss: 0.040 | Tree loss: 1.598 | Accuracy: 0.384000 | 1.686 sec/iter\n",
      "Epoch: 305 | Batch: 001 / 011 | Total loss: 1.588 | Reg loss: 0.040 | Tree loss: 1.588 | Accuracy: 0.402500 | 1.686 sec/iter\n",
      "Epoch: 305 | Batch: 002 / 011 | Total loss: 1.542 | Reg loss: 0.040 | Tree loss: 1.542 | Accuracy: 0.437500 | 1.686 sec/iter\n",
      "Epoch: 305 | Batch: 003 / 011 | Total loss: 1.477 | Reg loss: 0.040 | Tree loss: 1.477 | Accuracy: 0.472500 | 1.686 sec/iter\n",
      "Epoch: 305 | Batch: 004 / 011 | Total loss: 1.463 | Reg loss: 0.040 | Tree loss: 1.463 | Accuracy: 0.474000 | 1.686 sec/iter\n",
      "Epoch: 305 | Batch: 005 / 011 | Total loss: 1.430 | Reg loss: 0.040 | Tree loss: 1.430 | Accuracy: 0.499500 | 1.686 sec/iter\n",
      "Epoch: 305 | Batch: 006 / 011 | Total loss: 1.409 | Reg loss: 0.040 | Tree loss: 1.409 | Accuracy: 0.507500 | 1.686 sec/iter\n",
      "Epoch: 305 | Batch: 007 / 011 | Total loss: 1.427 | Reg loss: 0.040 | Tree loss: 1.427 | Accuracy: 0.498500 | 1.686 sec/iter\n",
      "Epoch: 305 | Batch: 008 / 011 | Total loss: 1.400 | Reg loss: 0.040 | Tree loss: 1.400 | Accuracy: 0.507500 | 1.686 sec/iter\n",
      "Epoch: 305 | Batch: 009 / 011 | Total loss: 1.427 | Reg loss: 0.040 | Tree loss: 1.427 | Accuracy: 0.493000 | 1.686 sec/iter\n",
      "Epoch: 305 | Batch: 010 / 011 | Total loss: 1.424 | Reg loss: 0.040 | Tree loss: 1.424 | Accuracy: 0.450512 | 1.686 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 306 | Batch: 000 / 011 | Total loss: 1.612 | Reg loss: 0.040 | Tree loss: 1.612 | Accuracy: 0.392000 | 1.686 sec/iter\n",
      "Epoch: 306 | Batch: 001 / 011 | Total loss: 1.603 | Reg loss: 0.040 | Tree loss: 1.603 | Accuracy: 0.399000 | 1.686 sec/iter\n",
      "Epoch: 306 | Batch: 002 / 011 | Total loss: 1.525 | Reg loss: 0.040 | Tree loss: 1.525 | Accuracy: 0.452000 | 1.686 sec/iter\n",
      "Epoch: 306 | Batch: 003 / 011 | Total loss: 1.496 | Reg loss: 0.040 | Tree loss: 1.496 | Accuracy: 0.466500 | 1.686 sec/iter\n",
      "Epoch: 306 | Batch: 004 / 011 | Total loss: 1.475 | Reg loss: 0.040 | Tree loss: 1.475 | Accuracy: 0.487500 | 1.686 sec/iter\n",
      "Epoch: 306 | Batch: 005 / 011 | Total loss: 1.442 | Reg loss: 0.040 | Tree loss: 1.442 | Accuracy: 0.495500 | 1.686 sec/iter\n",
      "Epoch: 306 | Batch: 006 / 011 | Total loss: 1.408 | Reg loss: 0.040 | Tree loss: 1.408 | Accuracy: 0.530500 | 1.686 sec/iter\n",
      "Epoch: 306 | Batch: 007 / 011 | Total loss: 1.401 | Reg loss: 0.040 | Tree loss: 1.401 | Accuracy: 0.529500 | 1.686 sec/iter\n",
      "Epoch: 306 | Batch: 008 / 011 | Total loss: 1.418 | Reg loss: 0.040 | Tree loss: 1.418 | Accuracy: 0.500500 | 1.686 sec/iter\n",
      "Epoch: 306 | Batch: 009 / 011 | Total loss: 1.388 | Reg loss: 0.040 | Tree loss: 1.388 | Accuracy: 0.509500 | 1.686 sec/iter\n",
      "Epoch: 306 | Batch: 010 / 011 | Total loss: 1.380 | Reg loss: 0.040 | Tree loss: 1.380 | Accuracy: 0.467577 | 1.686 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 307 | Batch: 000 / 011 | Total loss: 1.600 | Reg loss: 0.040 | Tree loss: 1.600 | Accuracy: 0.405500 | 1.687 sec/iter\n",
      "Epoch: 307 | Batch: 001 / 011 | Total loss: 1.582 | Reg loss: 0.040 | Tree loss: 1.582 | Accuracy: 0.404500 | 1.687 sec/iter\n",
      "Epoch: 307 | Batch: 002 / 011 | Total loss: 1.523 | Reg loss: 0.040 | Tree loss: 1.523 | Accuracy: 0.424000 | 1.687 sec/iter\n",
      "Epoch: 307 | Batch: 003 / 011 | Total loss: 1.505 | Reg loss: 0.040 | Tree loss: 1.505 | Accuracy: 0.458500 | 1.688 sec/iter\n",
      "Epoch: 307 | Batch: 004 / 011 | Total loss: 1.480 | Reg loss: 0.040 | Tree loss: 1.480 | Accuracy: 0.470000 | 1.688 sec/iter\n",
      "Epoch: 307 | Batch: 005 / 011 | Total loss: 1.439 | Reg loss: 0.040 | Tree loss: 1.439 | Accuracy: 0.518000 | 1.688 sec/iter\n",
      "Epoch: 307 | Batch: 006 / 011 | Total loss: 1.415 | Reg loss: 0.040 | Tree loss: 1.415 | Accuracy: 0.530000 | 1.688 sec/iter\n",
      "Epoch: 307 | Batch: 007 / 011 | Total loss: 1.409 | Reg loss: 0.040 | Tree loss: 1.409 | Accuracy: 0.495500 | 1.688 sec/iter\n",
      "Epoch: 307 | Batch: 008 / 011 | Total loss: 1.393 | Reg loss: 0.040 | Tree loss: 1.393 | Accuracy: 0.525500 | 1.688 sec/iter\n",
      "Epoch: 307 | Batch: 009 / 011 | Total loss: 1.417 | Reg loss: 0.040 | Tree loss: 1.417 | Accuracy: 0.515500 | 1.688 sec/iter\n",
      "Epoch: 307 | Batch: 010 / 011 | Total loss: 1.394 | Reg loss: 0.040 | Tree loss: 1.394 | Accuracy: 0.522184 | 1.688 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 308 | Batch: 000 / 011 | Total loss: 1.607 | Reg loss: 0.040 | Tree loss: 1.607 | Accuracy: 0.400500 | 1.689 sec/iter\n",
      "Epoch: 308 | Batch: 001 / 011 | Total loss: 1.560 | Reg loss: 0.040 | Tree loss: 1.560 | Accuracy: 0.413000 | 1.689 sec/iter\n",
      "Epoch: 308 | Batch: 002 / 011 | Total loss: 1.526 | Reg loss: 0.040 | Tree loss: 1.526 | Accuracy: 0.466500 | 1.689 sec/iter\n",
      "Epoch: 308 | Batch: 003 / 011 | Total loss: 1.497 | Reg loss: 0.040 | Tree loss: 1.497 | Accuracy: 0.468000 | 1.689 sec/iter\n",
      "Epoch: 308 | Batch: 004 / 011 | Total loss: 1.456 | Reg loss: 0.040 | Tree loss: 1.456 | Accuracy: 0.478000 | 1.689 sec/iter\n",
      "Epoch: 308 | Batch: 005 / 011 | Total loss: 1.445 | Reg loss: 0.040 | Tree loss: 1.445 | Accuracy: 0.480500 | 1.689 sec/iter\n",
      "Epoch: 308 | Batch: 006 / 011 | Total loss: 1.423 | Reg loss: 0.040 | Tree loss: 1.423 | Accuracy: 0.510000 | 1.689 sec/iter\n",
      "Epoch: 308 | Batch: 007 / 011 | Total loss: 1.419 | Reg loss: 0.040 | Tree loss: 1.419 | Accuracy: 0.530500 | 1.689 sec/iter\n",
      "Epoch: 308 | Batch: 008 / 011 | Total loss: 1.392 | Reg loss: 0.040 | Tree loss: 1.392 | Accuracy: 0.519500 | 1.689 sec/iter\n",
      "Epoch: 308 | Batch: 009 / 011 | Total loss: 1.425 | Reg loss: 0.040 | Tree loss: 1.425 | Accuracy: 0.487500 | 1.689 sec/iter\n",
      "Epoch: 308 | Batch: 010 / 011 | Total loss: 1.381 | Reg loss: 0.040 | Tree loss: 1.381 | Accuracy: 0.511945 | 1.689 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 309 | Batch: 000 / 011 | Total loss: 1.620 | Reg loss: 0.040 | Tree loss: 1.620 | Accuracy: 0.386500 | 1.69 sec/iter\n",
      "Epoch: 309 | Batch: 001 / 011 | Total loss: 1.554 | Reg loss: 0.040 | Tree loss: 1.554 | Accuracy: 0.419500 | 1.69 sec/iter\n",
      "Epoch: 309 | Batch: 002 / 011 | Total loss: 1.538 | Reg loss: 0.040 | Tree loss: 1.538 | Accuracy: 0.421500 | 1.69 sec/iter\n",
      "Epoch: 309 | Batch: 003 / 011 | Total loss: 1.492 | Reg loss: 0.040 | Tree loss: 1.492 | Accuracy: 0.456500 | 1.69 sec/iter\n",
      "Epoch: 309 | Batch: 004 / 011 | Total loss: 1.481 | Reg loss: 0.040 | Tree loss: 1.481 | Accuracy: 0.475500 | 1.69 sec/iter\n",
      "Epoch: 309 | Batch: 005 / 011 | Total loss: 1.434 | Reg loss: 0.040 | Tree loss: 1.434 | Accuracy: 0.506000 | 1.69 sec/iter\n",
      "Epoch: 309 | Batch: 006 / 011 | Total loss: 1.431 | Reg loss: 0.040 | Tree loss: 1.431 | Accuracy: 0.508000 | 1.691 sec/iter\n",
      "Epoch: 309 | Batch: 007 / 011 | Total loss: 1.414 | Reg loss: 0.040 | Tree loss: 1.414 | Accuracy: 0.492500 | 1.691 sec/iter\n",
      "Epoch: 309 | Batch: 008 / 011 | Total loss: 1.400 | Reg loss: 0.040 | Tree loss: 1.400 | Accuracy: 0.504000 | 1.691 sec/iter\n",
      "Epoch: 309 | Batch: 009 / 011 | Total loss: 1.402 | Reg loss: 0.040 | Tree loss: 1.402 | Accuracy: 0.504500 | 1.691 sec/iter\n",
      "Epoch: 309 | Batch: 010 / 011 | Total loss: 1.419 | Reg loss: 0.040 | Tree loss: 1.419 | Accuracy: 0.460751 | 1.691 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 310 | Batch: 000 / 011 | Total loss: 1.603 | Reg loss: 0.040 | Tree loss: 1.603 | Accuracy: 0.389500 | 1.692 sec/iter\n",
      "Epoch: 310 | Batch: 001 / 011 | Total loss: 1.592 | Reg loss: 0.040 | Tree loss: 1.592 | Accuracy: 0.403000 | 1.692 sec/iter\n",
      "Epoch: 310 | Batch: 002 / 011 | Total loss: 1.522 | Reg loss: 0.040 | Tree loss: 1.522 | Accuracy: 0.458500 | 1.692 sec/iter\n",
      "Epoch: 310 | Batch: 003 / 011 | Total loss: 1.472 | Reg loss: 0.040 | Tree loss: 1.472 | Accuracy: 0.490000 | 1.692 sec/iter\n",
      "Epoch: 310 | Batch: 004 / 011 | Total loss: 1.454 | Reg loss: 0.040 | Tree loss: 1.454 | Accuracy: 0.503000 | 1.692 sec/iter\n",
      "Epoch: 310 | Batch: 005 / 011 | Total loss: 1.426 | Reg loss: 0.040 | Tree loss: 1.426 | Accuracy: 0.506500 | 1.692 sec/iter\n",
      "Epoch: 310 | Batch: 006 / 011 | Total loss: 1.438 | Reg loss: 0.040 | Tree loss: 1.438 | Accuracy: 0.503000 | 1.692 sec/iter\n",
      "Epoch: 310 | Batch: 007 / 011 | Total loss: 1.414 | Reg loss: 0.040 | Tree loss: 1.414 | Accuracy: 0.511500 | 1.692 sec/iter\n",
      "Epoch: 310 | Batch: 008 / 011 | Total loss: 1.429 | Reg loss: 0.040 | Tree loss: 1.429 | Accuracy: 0.509000 | 1.692 sec/iter\n",
      "Epoch: 310 | Batch: 009 / 011 | Total loss: 1.433 | Reg loss: 0.040 | Tree loss: 1.433 | Accuracy: 0.492500 | 1.693 sec/iter\n",
      "Epoch: 310 | Batch: 010 / 011 | Total loss: 1.363 | Reg loss: 0.040 | Tree loss: 1.363 | Accuracy: 0.539249 | 1.692 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 311 | Batch: 000 / 011 | Total loss: 1.620 | Reg loss: 0.040 | Tree loss: 1.620 | Accuracy: 0.389000 | 1.694 sec/iter\n",
      "Epoch: 311 | Batch: 001 / 011 | Total loss: 1.579 | Reg loss: 0.040 | Tree loss: 1.579 | Accuracy: 0.416000 | 1.694 sec/iter\n",
      "Epoch: 311 | Batch: 002 / 011 | Total loss: 1.527 | Reg loss: 0.040 | Tree loss: 1.527 | Accuracy: 0.437000 | 1.694 sec/iter\n",
      "Epoch: 311 | Batch: 003 / 011 | Total loss: 1.490 | Reg loss: 0.040 | Tree loss: 1.490 | Accuracy: 0.450500 | 1.694 sec/iter\n",
      "Epoch: 311 | Batch: 004 / 011 | Total loss: 1.490 | Reg loss: 0.040 | Tree loss: 1.490 | Accuracy: 0.448500 | 1.694 sec/iter\n",
      "Epoch: 311 | Batch: 005 / 011 | Total loss: 1.424 | Reg loss: 0.040 | Tree loss: 1.424 | Accuracy: 0.491000 | 1.693 sec/iter\n",
      "Epoch: 311 | Batch: 006 / 011 | Total loss: 1.397 | Reg loss: 0.040 | Tree loss: 1.397 | Accuracy: 0.504500 | 1.693 sec/iter\n",
      "Epoch: 311 | Batch: 007 / 011 | Total loss: 1.400 | Reg loss: 0.040 | Tree loss: 1.400 | Accuracy: 0.509000 | 1.693 sec/iter\n",
      "Epoch: 311 | Batch: 008 / 011 | Total loss: 1.420 | Reg loss: 0.040 | Tree loss: 1.420 | Accuracy: 0.483500 | 1.693 sec/iter\n",
      "Epoch: 311 | Batch: 009 / 011 | Total loss: 1.422 | Reg loss: 0.040 | Tree loss: 1.422 | Accuracy: 0.502000 | 1.693 sec/iter\n",
      "Epoch: 311 | Batch: 010 / 011 | Total loss: 1.384 | Reg loss: 0.040 | Tree loss: 1.384 | Accuracy: 0.488055 | 1.693 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 312 | Batch: 000 / 011 | Total loss: 1.599 | Reg loss: 0.040 | Tree loss: 1.599 | Accuracy: 0.401500 | 1.694 sec/iter\n",
      "Epoch: 312 | Batch: 001 / 011 | Total loss: 1.577 | Reg loss: 0.040 | Tree loss: 1.577 | Accuracy: 0.393000 | 1.694 sec/iter\n",
      "Epoch: 312 | Batch: 002 / 011 | Total loss: 1.548 | Reg loss: 0.040 | Tree loss: 1.548 | Accuracy: 0.440500 | 1.694 sec/iter\n",
      "Epoch: 312 | Batch: 003 / 011 | Total loss: 1.483 | Reg loss: 0.040 | Tree loss: 1.483 | Accuracy: 0.485500 | 1.694 sec/iter\n",
      "Epoch: 312 | Batch: 004 / 011 | Total loss: 1.473 | Reg loss: 0.040 | Tree loss: 1.473 | Accuracy: 0.469500 | 1.694 sec/iter\n",
      "Epoch: 312 | Batch: 005 / 011 | Total loss: 1.417 | Reg loss: 0.040 | Tree loss: 1.417 | Accuracy: 0.491000 | 1.694 sec/iter\n",
      "Epoch: 312 | Batch: 006 / 011 | Total loss: 1.427 | Reg loss: 0.040 | Tree loss: 1.427 | Accuracy: 0.511000 | 1.694 sec/iter\n",
      "Epoch: 312 | Batch: 007 / 011 | Total loss: 1.411 | Reg loss: 0.040 | Tree loss: 1.411 | Accuracy: 0.514000 | 1.694 sec/iter\n",
      "Epoch: 312 | Batch: 008 / 011 | Total loss: 1.401 | Reg loss: 0.040 | Tree loss: 1.401 | Accuracy: 0.513500 | 1.694 sec/iter\n",
      "Epoch: 312 | Batch: 009 / 011 | Total loss: 1.422 | Reg loss: 0.040 | Tree loss: 1.422 | Accuracy: 0.490000 | 1.694 sec/iter\n",
      "Epoch: 312 | Batch: 010 / 011 | Total loss: 1.402 | Reg loss: 0.040 | Tree loss: 1.402 | Accuracy: 0.470990 | 1.694 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 313 | Batch: 000 / 011 | Total loss: 1.598 | Reg loss: 0.040 | Tree loss: 1.598 | Accuracy: 0.393500 | 1.695 sec/iter\n",
      "Epoch: 313 | Batch: 001 / 011 | Total loss: 1.581 | Reg loss: 0.040 | Tree loss: 1.581 | Accuracy: 0.389500 | 1.695 sec/iter\n",
      "Epoch: 313 | Batch: 002 / 011 | Total loss: 1.549 | Reg loss: 0.040 | Tree loss: 1.549 | Accuracy: 0.432500 | 1.696 sec/iter\n",
      "Epoch: 313 | Batch: 003 / 011 | Total loss: 1.508 | Reg loss: 0.040 | Tree loss: 1.508 | Accuracy: 0.459500 | 1.696 sec/iter\n",
      "Epoch: 313 | Batch: 004 / 011 | Total loss: 1.464 | Reg loss: 0.040 | Tree loss: 1.464 | Accuracy: 0.449000 | 1.696 sec/iter\n",
      "Epoch: 313 | Batch: 005 / 011 | Total loss: 1.414 | Reg loss: 0.040 | Tree loss: 1.414 | Accuracy: 0.507000 | 1.696 sec/iter\n",
      "Epoch: 313 | Batch: 006 / 011 | Total loss: 1.426 | Reg loss: 0.040 | Tree loss: 1.426 | Accuracy: 0.521000 | 1.696 sec/iter\n",
      "Epoch: 313 | Batch: 007 / 011 | Total loss: 1.410 | Reg loss: 0.040 | Tree loss: 1.410 | Accuracy: 0.510500 | 1.696 sec/iter\n",
      "Epoch: 313 | Batch: 008 / 011 | Total loss: 1.401 | Reg loss: 0.040 | Tree loss: 1.401 | Accuracy: 0.510000 | 1.696 sec/iter\n",
      "Epoch: 313 | Batch: 009 / 011 | Total loss: 1.392 | Reg loss: 0.040 | Tree loss: 1.392 | Accuracy: 0.503000 | 1.696 sec/iter\n",
      "Epoch: 313 | Batch: 010 / 011 | Total loss: 1.415 | Reg loss: 0.040 | Tree loss: 1.415 | Accuracy: 0.474403 | 1.696 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 314 | Batch: 000 / 011 | Total loss: 1.614 | Reg loss: 0.040 | Tree loss: 1.614 | Accuracy: 0.383000 | 1.697 sec/iter\n",
      "Epoch: 314 | Batch: 001 / 011 | Total loss: 1.581 | Reg loss: 0.040 | Tree loss: 1.581 | Accuracy: 0.411500 | 1.697 sec/iter\n",
      "Epoch: 314 | Batch: 002 / 011 | Total loss: 1.525 | Reg loss: 0.040 | Tree loss: 1.525 | Accuracy: 0.436500 | 1.697 sec/iter\n",
      "Epoch: 314 | Batch: 003 / 011 | Total loss: 1.485 | Reg loss: 0.040 | Tree loss: 1.485 | Accuracy: 0.481500 | 1.697 sec/iter\n",
      "Epoch: 314 | Batch: 004 / 011 | Total loss: 1.468 | Reg loss: 0.040 | Tree loss: 1.468 | Accuracy: 0.467500 | 1.697 sec/iter\n",
      "Epoch: 314 | Batch: 005 / 011 | Total loss: 1.431 | Reg loss: 0.040 | Tree loss: 1.431 | Accuracy: 0.495500 | 1.697 sec/iter\n",
      "Epoch: 314 | Batch: 006 / 011 | Total loss: 1.428 | Reg loss: 0.040 | Tree loss: 1.428 | Accuracy: 0.509500 | 1.697 sec/iter\n",
      "Epoch: 314 | Batch: 007 / 011 | Total loss: 1.412 | Reg loss: 0.040 | Tree loss: 1.412 | Accuracy: 0.519500 | 1.697 sec/iter\n",
      "Epoch: 314 | Batch: 008 / 011 | Total loss: 1.402 | Reg loss: 0.040 | Tree loss: 1.402 | Accuracy: 0.515500 | 1.697 sec/iter\n",
      "Epoch: 314 | Batch: 009 / 011 | Total loss: 1.399 | Reg loss: 0.040 | Tree loss: 1.399 | Accuracy: 0.512000 | 1.697 sec/iter\n",
      "Epoch: 314 | Batch: 010 / 011 | Total loss: 1.357 | Reg loss: 0.040 | Tree loss: 1.357 | Accuracy: 0.494881 | 1.697 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 315 | Batch: 000 / 011 | Total loss: 1.596 | Reg loss: 0.040 | Tree loss: 1.596 | Accuracy: 0.408500 | 1.699 sec/iter\n",
      "Epoch: 315 | Batch: 001 / 011 | Total loss: 1.596 | Reg loss: 0.040 | Tree loss: 1.596 | Accuracy: 0.391500 | 1.699 sec/iter\n",
      "Epoch: 315 | Batch: 002 / 011 | Total loss: 1.548 | Reg loss: 0.040 | Tree loss: 1.548 | Accuracy: 0.423000 | 1.699 sec/iter\n",
      "Epoch: 315 | Batch: 003 / 011 | Total loss: 1.511 | Reg loss: 0.040 | Tree loss: 1.511 | Accuracy: 0.444500 | 1.699 sec/iter\n",
      "Epoch: 315 | Batch: 004 / 011 | Total loss: 1.475 | Reg loss: 0.040 | Tree loss: 1.475 | Accuracy: 0.474000 | 1.699 sec/iter\n",
      "Epoch: 315 | Batch: 005 / 011 | Total loss: 1.423 | Reg loss: 0.040 | Tree loss: 1.423 | Accuracy: 0.500500 | 1.699 sec/iter\n",
      "Epoch: 315 | Batch: 006 / 011 | Total loss: 1.427 | Reg loss: 0.040 | Tree loss: 1.427 | Accuracy: 0.507500 | 1.699 sec/iter\n",
      "Epoch: 315 | Batch: 007 / 011 | Total loss: 1.384 | Reg loss: 0.040 | Tree loss: 1.384 | Accuracy: 0.518500 | 1.699 sec/iter\n",
      "Epoch: 315 | Batch: 008 / 011 | Total loss: 1.404 | Reg loss: 0.040 | Tree loss: 1.404 | Accuracy: 0.524500 | 1.699 sec/iter\n",
      "Epoch: 315 | Batch: 009 / 011 | Total loss: 1.389 | Reg loss: 0.040 | Tree loss: 1.389 | Accuracy: 0.499500 | 1.699 sec/iter\n",
      "Epoch: 315 | Batch: 010 / 011 | Total loss: 1.408 | Reg loss: 0.040 | Tree loss: 1.408 | Accuracy: 0.474403 | 1.699 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 316 | Batch: 000 / 011 | Total loss: 1.605 | Reg loss: 0.040 | Tree loss: 1.605 | Accuracy: 0.396000 | 1.7 sec/iter\n",
      "Epoch: 316 | Batch: 001 / 011 | Total loss: 1.584 | Reg loss: 0.040 | Tree loss: 1.584 | Accuracy: 0.406500 | 1.7 sec/iter\n",
      "Epoch: 316 | Batch: 002 / 011 | Total loss: 1.522 | Reg loss: 0.040 | Tree loss: 1.522 | Accuracy: 0.444000 | 1.7 sec/iter\n",
      "Epoch: 316 | Batch: 003 / 011 | Total loss: 1.496 | Reg loss: 0.040 | Tree loss: 1.496 | Accuracy: 0.453500 | 1.7 sec/iter\n",
      "Epoch: 316 | Batch: 004 / 011 | Total loss: 1.455 | Reg loss: 0.040 | Tree loss: 1.455 | Accuracy: 0.496000 | 1.7 sec/iter\n",
      "Epoch: 316 | Batch: 005 / 011 | Total loss: 1.445 | Reg loss: 0.040 | Tree loss: 1.445 | Accuracy: 0.507000 | 1.7 sec/iter\n",
      "Epoch: 316 | Batch: 006 / 011 | Total loss: 1.440 | Reg loss: 0.040 | Tree loss: 1.440 | Accuracy: 0.513000 | 1.7 sec/iter\n",
      "Epoch: 316 | Batch: 007 / 011 | Total loss: 1.394 | Reg loss: 0.040 | Tree loss: 1.394 | Accuracy: 0.523000 | 1.7 sec/iter\n",
      "Epoch: 316 | Batch: 008 / 011 | Total loss: 1.416 | Reg loss: 0.040 | Tree loss: 1.416 | Accuracy: 0.506000 | 1.7 sec/iter\n",
      "Epoch: 316 | Batch: 009 / 011 | Total loss: 1.396 | Reg loss: 0.040 | Tree loss: 1.396 | Accuracy: 0.513000 | 1.7 sec/iter\n",
      "Epoch: 316 | Batch: 010 / 011 | Total loss: 1.392 | Reg loss: 0.040 | Tree loss: 1.392 | Accuracy: 0.494881 | 1.7 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 317 | Batch: 000 / 011 | Total loss: 1.599 | Reg loss: 0.040 | Tree loss: 1.599 | Accuracy: 0.407000 | 1.701 sec/iter\n",
      "Epoch: 317 | Batch: 001 / 011 | Total loss: 1.560 | Reg loss: 0.040 | Tree loss: 1.560 | Accuracy: 0.421000 | 1.701 sec/iter\n",
      "Epoch: 317 | Batch: 002 / 011 | Total loss: 1.510 | Reg loss: 0.040 | Tree loss: 1.510 | Accuracy: 0.446500 | 1.701 sec/iter\n",
      "Epoch: 317 | Batch: 003 / 011 | Total loss: 1.518 | Reg loss: 0.040 | Tree loss: 1.518 | Accuracy: 0.449500 | 1.701 sec/iter\n",
      "Epoch: 317 | Batch: 004 / 011 | Total loss: 1.470 | Reg loss: 0.040 | Tree loss: 1.470 | Accuracy: 0.465000 | 1.701 sec/iter\n",
      "Epoch: 317 | Batch: 005 / 011 | Total loss: 1.447 | Reg loss: 0.040 | Tree loss: 1.447 | Accuracy: 0.491000 | 1.701 sec/iter\n",
      "Epoch: 317 | Batch: 006 / 011 | Total loss: 1.424 | Reg loss: 0.040 | Tree loss: 1.424 | Accuracy: 0.513000 | 1.701 sec/iter\n",
      "Epoch: 317 | Batch: 007 / 011 | Total loss: 1.411 | Reg loss: 0.040 | Tree loss: 1.411 | Accuracy: 0.521500 | 1.701 sec/iter\n",
      "Epoch: 317 | Batch: 008 / 011 | Total loss: 1.385 | Reg loss: 0.040 | Tree loss: 1.385 | Accuracy: 0.513000 | 1.701 sec/iter\n",
      "Epoch: 317 | Batch: 009 / 011 | Total loss: 1.421 | Reg loss: 0.040 | Tree loss: 1.421 | Accuracy: 0.499000 | 1.701 sec/iter\n",
      "Epoch: 317 | Batch: 010 / 011 | Total loss: 1.484 | Reg loss: 0.040 | Tree loss: 1.484 | Accuracy: 0.474403 | 1.701 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 318 | Batch: 000 / 011 | Total loss: 1.622 | Reg loss: 0.040 | Tree loss: 1.622 | Accuracy: 0.402000 | 1.702 sec/iter\n",
      "Epoch: 318 | Batch: 001 / 011 | Total loss: 1.551 | Reg loss: 0.040 | Tree loss: 1.551 | Accuracy: 0.429000 | 1.702 sec/iter\n",
      "Epoch: 318 | Batch: 002 / 011 | Total loss: 1.552 | Reg loss: 0.040 | Tree loss: 1.552 | Accuracy: 0.433500 | 1.702 sec/iter\n",
      "Epoch: 318 | Batch: 003 / 011 | Total loss: 1.482 | Reg loss: 0.040 | Tree loss: 1.482 | Accuracy: 0.469500 | 1.702 sec/iter\n",
      "Epoch: 318 | Batch: 004 / 011 | Total loss: 1.473 | Reg loss: 0.040 | Tree loss: 1.473 | Accuracy: 0.461000 | 1.702 sec/iter\n",
      "Epoch: 318 | Batch: 005 / 011 | Total loss: 1.429 | Reg loss: 0.040 | Tree loss: 1.429 | Accuracy: 0.510500 | 1.702 sec/iter\n",
      "Epoch: 318 | Batch: 006 / 011 | Total loss: 1.427 | Reg loss: 0.040 | Tree loss: 1.427 | Accuracy: 0.482500 | 1.702 sec/iter\n",
      "Epoch: 318 | Batch: 007 / 011 | Total loss: 1.420 | Reg loss: 0.040 | Tree loss: 1.420 | Accuracy: 0.508000 | 1.702 sec/iter\n",
      "Epoch: 318 | Batch: 008 / 011 | Total loss: 1.392 | Reg loss: 0.040 | Tree loss: 1.392 | Accuracy: 0.510500 | 1.702 sec/iter\n",
      "Epoch: 318 | Batch: 009 / 011 | Total loss: 1.405 | Reg loss: 0.040 | Tree loss: 1.405 | Accuracy: 0.492500 | 1.702 sec/iter\n",
      "Epoch: 318 | Batch: 010 / 011 | Total loss: 1.468 | Reg loss: 0.040 | Tree loss: 1.468 | Accuracy: 0.430034 | 1.702 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 319 | Batch: 000 / 011 | Total loss: 1.609 | Reg loss: 0.040 | Tree loss: 1.609 | Accuracy: 0.399000 | 1.703 sec/iter\n",
      "Epoch: 319 | Batch: 001 / 011 | Total loss: 1.557 | Reg loss: 0.040 | Tree loss: 1.557 | Accuracy: 0.418500 | 1.703 sec/iter\n",
      "Epoch: 319 | Batch: 002 / 011 | Total loss: 1.538 | Reg loss: 0.040 | Tree loss: 1.538 | Accuracy: 0.438000 | 1.703 sec/iter\n",
      "Epoch: 319 | Batch: 003 / 011 | Total loss: 1.515 | Reg loss: 0.040 | Tree loss: 1.515 | Accuracy: 0.448000 | 1.703 sec/iter\n",
      "Epoch: 319 | Batch: 004 / 011 | Total loss: 1.465 | Reg loss: 0.040 | Tree loss: 1.465 | Accuracy: 0.460500 | 1.703 sec/iter\n",
      "Epoch: 319 | Batch: 005 / 011 | Total loss: 1.439 | Reg loss: 0.040 | Tree loss: 1.439 | Accuracy: 0.511000 | 1.703 sec/iter\n",
      "Epoch: 319 | Batch: 006 / 011 | Total loss: 1.435 | Reg loss: 0.040 | Tree loss: 1.435 | Accuracy: 0.520500 | 1.703 sec/iter\n",
      "Epoch: 319 | Batch: 007 / 011 | Total loss: 1.404 | Reg loss: 0.040 | Tree loss: 1.404 | Accuracy: 0.517000 | 1.703 sec/iter\n",
      "Epoch: 319 | Batch: 008 / 011 | Total loss: 1.395 | Reg loss: 0.040 | Tree loss: 1.395 | Accuracy: 0.510500 | 1.703 sec/iter\n",
      "Epoch: 319 | Batch: 009 / 011 | Total loss: 1.383 | Reg loss: 0.040 | Tree loss: 1.383 | Accuracy: 0.517000 | 1.703 sec/iter\n",
      "Epoch: 319 | Batch: 010 / 011 | Total loss: 1.381 | Reg loss: 0.040 | Tree loss: 1.381 | Accuracy: 0.563140 | 1.703 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 320 | Batch: 000 / 011 | Total loss: 1.606 | Reg loss: 0.040 | Tree loss: 1.606 | Accuracy: 0.404500 | 1.704 sec/iter\n",
      "Epoch: 320 | Batch: 001 / 011 | Total loss: 1.569 | Reg loss: 0.040 | Tree loss: 1.569 | Accuracy: 0.405500 | 1.704 sec/iter\n",
      "Epoch: 320 | Batch: 002 / 011 | Total loss: 1.555 | Reg loss: 0.040 | Tree loss: 1.555 | Accuracy: 0.436500 | 1.704 sec/iter\n",
      "Epoch: 320 | Batch: 003 / 011 | Total loss: 1.487 | Reg loss: 0.040 | Tree loss: 1.487 | Accuracy: 0.470500 | 1.704 sec/iter\n",
      "Epoch: 320 | Batch: 004 / 011 | Total loss: 1.469 | Reg loss: 0.040 | Tree loss: 1.469 | Accuracy: 0.469500 | 1.704 sec/iter\n",
      "Epoch: 320 | Batch: 005 / 011 | Total loss: 1.436 | Reg loss: 0.040 | Tree loss: 1.436 | Accuracy: 0.505000 | 1.704 sec/iter\n",
      "Epoch: 320 | Batch: 006 / 011 | Total loss: 1.403 | Reg loss: 0.040 | Tree loss: 1.403 | Accuracy: 0.520000 | 1.704 sec/iter\n",
      "Epoch: 320 | Batch: 007 / 011 | Total loss: 1.405 | Reg loss: 0.040 | Tree loss: 1.405 | Accuracy: 0.501000 | 1.704 sec/iter\n",
      "Epoch: 320 | Batch: 008 / 011 | Total loss: 1.404 | Reg loss: 0.040 | Tree loss: 1.404 | Accuracy: 0.515000 | 1.704 sec/iter\n",
      "Epoch: 320 | Batch: 009 / 011 | Total loss: 1.419 | Reg loss: 0.040 | Tree loss: 1.419 | Accuracy: 0.477500 | 1.704 sec/iter\n",
      "Epoch: 320 | Batch: 010 / 011 | Total loss: 1.370 | Reg loss: 0.040 | Tree loss: 1.370 | Accuracy: 0.491468 | 1.704 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 321 | Batch: 000 / 011 | Total loss: 1.590 | Reg loss: 0.040 | Tree loss: 1.590 | Accuracy: 0.426500 | 1.706 sec/iter\n",
      "Epoch: 321 | Batch: 001 / 011 | Total loss: 1.590 | Reg loss: 0.040 | Tree loss: 1.590 | Accuracy: 0.407000 | 1.706 sec/iter\n",
      "Epoch: 321 | Batch: 002 / 011 | Total loss: 1.549 | Reg loss: 0.040 | Tree loss: 1.549 | Accuracy: 0.415000 | 1.706 sec/iter\n",
      "Epoch: 321 | Batch: 003 / 011 | Total loss: 1.494 | Reg loss: 0.040 | Tree loss: 1.494 | Accuracy: 0.457500 | 1.706 sec/iter\n",
      "Epoch: 321 | Batch: 004 / 011 | Total loss: 1.468 | Reg loss: 0.040 | Tree loss: 1.468 | Accuracy: 0.461000 | 1.706 sec/iter\n",
      "Epoch: 321 | Batch: 005 / 011 | Total loss: 1.446 | Reg loss: 0.040 | Tree loss: 1.446 | Accuracy: 0.500000 | 1.706 sec/iter\n",
      "Epoch: 321 | Batch: 006 / 011 | Total loss: 1.439 | Reg loss: 0.040 | Tree loss: 1.439 | Accuracy: 0.511500 | 1.706 sec/iter\n",
      "Epoch: 321 | Batch: 007 / 011 | Total loss: 1.385 | Reg loss: 0.040 | Tree loss: 1.385 | Accuracy: 0.534000 | 1.706 sec/iter\n",
      "Epoch: 321 | Batch: 008 / 011 | Total loss: 1.383 | Reg loss: 0.040 | Tree loss: 1.383 | Accuracy: 0.543500 | 1.706 sec/iter\n",
      "Epoch: 321 | Batch: 009 / 011 | Total loss: 1.414 | Reg loss: 0.040 | Tree loss: 1.414 | Accuracy: 0.505500 | 1.706 sec/iter\n",
      "Epoch: 321 | Batch: 010 / 011 | Total loss: 1.449 | Reg loss: 0.040 | Tree loss: 1.449 | Accuracy: 0.430034 | 1.706 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 322 | Batch: 000 / 011 | Total loss: 1.604 | Reg loss: 0.040 | Tree loss: 1.604 | Accuracy: 0.402000 | 1.707 sec/iter\n",
      "Epoch: 322 | Batch: 001 / 011 | Total loss: 1.558 | Reg loss: 0.040 | Tree loss: 1.558 | Accuracy: 0.417500 | 1.707 sec/iter\n",
      "Epoch: 322 | Batch: 002 / 011 | Total loss: 1.527 | Reg loss: 0.040 | Tree loss: 1.527 | Accuracy: 0.444500 | 1.707 sec/iter\n",
      "Epoch: 322 | Batch: 003 / 011 | Total loss: 1.494 | Reg loss: 0.040 | Tree loss: 1.494 | Accuracy: 0.451500 | 1.707 sec/iter\n",
      "Epoch: 322 | Batch: 004 / 011 | Total loss: 1.455 | Reg loss: 0.040 | Tree loss: 1.455 | Accuracy: 0.475000 | 1.707 sec/iter\n",
      "Epoch: 322 | Batch: 005 / 011 | Total loss: 1.447 | Reg loss: 0.040 | Tree loss: 1.447 | Accuracy: 0.483000 | 1.707 sec/iter\n",
      "Epoch: 322 | Batch: 006 / 011 | Total loss: 1.415 | Reg loss: 0.040 | Tree loss: 1.415 | Accuracy: 0.511500 | 1.707 sec/iter\n",
      "Epoch: 322 | Batch: 007 / 011 | Total loss: 1.442 | Reg loss: 0.040 | Tree loss: 1.442 | Accuracy: 0.511500 | 1.707 sec/iter\n",
      "Epoch: 322 | Batch: 008 / 011 | Total loss: 1.399 | Reg loss: 0.040 | Tree loss: 1.399 | Accuracy: 0.507000 | 1.707 sec/iter\n",
      "Epoch: 322 | Batch: 009 / 011 | Total loss: 1.401 | Reg loss: 0.040 | Tree loss: 1.401 | Accuracy: 0.491500 | 1.707 sec/iter\n",
      "Epoch: 322 | Batch: 010 / 011 | Total loss: 1.409 | Reg loss: 0.040 | Tree loss: 1.409 | Accuracy: 0.501706 | 1.707 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 323 | Batch: 000 / 011 | Total loss: 1.609 | Reg loss: 0.040 | Tree loss: 1.609 | Accuracy: 0.400000 | 1.707 sec/iter\n",
      "Epoch: 323 | Batch: 001 / 011 | Total loss: 1.574 | Reg loss: 0.040 | Tree loss: 1.574 | Accuracy: 0.395000 | 1.707 sec/iter\n",
      "Epoch: 323 | Batch: 002 / 011 | Total loss: 1.533 | Reg loss: 0.040 | Tree loss: 1.533 | Accuracy: 0.435500 | 1.707 sec/iter\n",
      "Epoch: 323 | Batch: 003 / 011 | Total loss: 1.491 | Reg loss: 0.040 | Tree loss: 1.491 | Accuracy: 0.473500 | 1.707 sec/iter\n",
      "Epoch: 323 | Batch: 004 / 011 | Total loss: 1.454 | Reg loss: 0.040 | Tree loss: 1.454 | Accuracy: 0.484000 | 1.708 sec/iter\n",
      "Epoch: 323 | Batch: 005 / 011 | Total loss: 1.431 | Reg loss: 0.040 | Tree loss: 1.431 | Accuracy: 0.498000 | 1.708 sec/iter\n",
      "Epoch: 323 | Batch: 006 / 011 | Total loss: 1.432 | Reg loss: 0.040 | Tree loss: 1.432 | Accuracy: 0.525500 | 1.708 sec/iter\n",
      "Epoch: 323 | Batch: 007 / 011 | Total loss: 1.417 | Reg loss: 0.040 | Tree loss: 1.417 | Accuracy: 0.506000 | 1.708 sec/iter\n",
      "Epoch: 323 | Batch: 008 / 011 | Total loss: 1.392 | Reg loss: 0.040 | Tree loss: 1.392 | Accuracy: 0.519000 | 1.708 sec/iter\n",
      "Epoch: 323 | Batch: 009 / 011 | Total loss: 1.405 | Reg loss: 0.040 | Tree loss: 1.405 | Accuracy: 0.492000 | 1.708 sec/iter\n",
      "Epoch: 323 | Batch: 010 / 011 | Total loss: 1.356 | Reg loss: 0.040 | Tree loss: 1.356 | Accuracy: 0.535836 | 1.708 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 324 | Batch: 000 / 011 | Total loss: 1.634 | Reg loss: 0.040 | Tree loss: 1.634 | Accuracy: 0.379000 | 1.709 sec/iter\n",
      "Epoch: 324 | Batch: 001 / 011 | Total loss: 1.569 | Reg loss: 0.040 | Tree loss: 1.569 | Accuracy: 0.425500 | 1.709 sec/iter\n",
      "Epoch: 324 | Batch: 002 / 011 | Total loss: 1.530 | Reg loss: 0.040 | Tree loss: 1.530 | Accuracy: 0.441500 | 1.709 sec/iter\n",
      "Epoch: 324 | Batch: 003 / 011 | Total loss: 1.492 | Reg loss: 0.040 | Tree loss: 1.492 | Accuracy: 0.458000 | 1.709 sec/iter\n",
      "Epoch: 324 | Batch: 004 / 011 | Total loss: 1.454 | Reg loss: 0.040 | Tree loss: 1.454 | Accuracy: 0.475000 | 1.709 sec/iter\n",
      "Epoch: 324 | Batch: 005 / 011 | Total loss: 1.401 | Reg loss: 0.040 | Tree loss: 1.401 | Accuracy: 0.522500 | 1.709 sec/iter\n",
      "Epoch: 324 | Batch: 006 / 011 | Total loss: 1.398 | Reg loss: 0.040 | Tree loss: 1.398 | Accuracy: 0.524500 | 1.709 sec/iter\n",
      "Epoch: 324 | Batch: 007 / 011 | Total loss: 1.429 | Reg loss: 0.040 | Tree loss: 1.429 | Accuracy: 0.496000 | 1.709 sec/iter\n",
      "Epoch: 324 | Batch: 008 / 011 | Total loss: 1.422 | Reg loss: 0.040 | Tree loss: 1.422 | Accuracy: 0.509500 | 1.709 sec/iter\n",
      "Epoch: 324 | Batch: 009 / 011 | Total loss: 1.403 | Reg loss: 0.040 | Tree loss: 1.403 | Accuracy: 0.488000 | 1.709 sec/iter\n",
      "Epoch: 324 | Batch: 010 / 011 | Total loss: 1.371 | Reg loss: 0.040 | Tree loss: 1.371 | Accuracy: 0.491468 | 1.709 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 325 | Batch: 000 / 011 | Total loss: 1.571 | Reg loss: 0.040 | Tree loss: 1.571 | Accuracy: 0.419000 | 1.71 sec/iter\n",
      "Epoch: 325 | Batch: 001 / 011 | Total loss: 1.594 | Reg loss: 0.040 | Tree loss: 1.594 | Accuracy: 0.402500 | 1.71 sec/iter\n",
      "Epoch: 325 | Batch: 002 / 011 | Total loss: 1.540 | Reg loss: 0.040 | Tree loss: 1.540 | Accuracy: 0.438000 | 1.71 sec/iter\n",
      "Epoch: 325 | Batch: 003 / 011 | Total loss: 1.503 | Reg loss: 0.040 | Tree loss: 1.503 | Accuracy: 0.454000 | 1.71 sec/iter\n",
      "Epoch: 325 | Batch: 004 / 011 | Total loss: 1.441 | Reg loss: 0.040 | Tree loss: 1.441 | Accuracy: 0.493000 | 1.71 sec/iter\n",
      "Epoch: 325 | Batch: 005 / 011 | Total loss: 1.454 | Reg loss: 0.040 | Tree loss: 1.454 | Accuracy: 0.493500 | 1.71 sec/iter\n",
      "Epoch: 325 | Batch: 006 / 011 | Total loss: 1.412 | Reg loss: 0.040 | Tree loss: 1.412 | Accuracy: 0.540000 | 1.711 sec/iter\n",
      "Epoch: 325 | Batch: 007 / 011 | Total loss: 1.397 | Reg loss: 0.040 | Tree loss: 1.397 | Accuracy: 0.530500 | 1.711 sec/iter\n",
      "Epoch: 325 | Batch: 008 / 011 | Total loss: 1.403 | Reg loss: 0.040 | Tree loss: 1.403 | Accuracy: 0.505000 | 1.711 sec/iter\n",
      "Epoch: 325 | Batch: 009 / 011 | Total loss: 1.429 | Reg loss: 0.040 | Tree loss: 1.429 | Accuracy: 0.505000 | 1.711 sec/iter\n",
      "Epoch: 325 | Batch: 010 / 011 | Total loss: 1.439 | Reg loss: 0.040 | Tree loss: 1.439 | Accuracy: 0.470990 | 1.71 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 326 | Batch: 000 / 011 | Total loss: 1.604 | Reg loss: 0.040 | Tree loss: 1.604 | Accuracy: 0.394500 | 1.711 sec/iter\n",
      "Epoch: 326 | Batch: 001 / 011 | Total loss: 1.574 | Reg loss: 0.040 | Tree loss: 1.574 | Accuracy: 0.416000 | 1.711 sec/iter\n",
      "Epoch: 326 | Batch: 002 / 011 | Total loss: 1.555 | Reg loss: 0.040 | Tree loss: 1.555 | Accuracy: 0.425500 | 1.711 sec/iter\n",
      "Epoch: 326 | Batch: 003 / 011 | Total loss: 1.493 | Reg loss: 0.040 | Tree loss: 1.493 | Accuracy: 0.457500 | 1.711 sec/iter\n",
      "Epoch: 326 | Batch: 004 / 011 | Total loss: 1.470 | Reg loss: 0.040 | Tree loss: 1.470 | Accuracy: 0.458500 | 1.712 sec/iter\n",
      "Epoch: 326 | Batch: 005 / 011 | Total loss: 1.432 | Reg loss: 0.040 | Tree loss: 1.432 | Accuracy: 0.511000 | 1.712 sec/iter\n",
      "Epoch: 326 | Batch: 006 / 011 | Total loss: 1.412 | Reg loss: 0.040 | Tree loss: 1.412 | Accuracy: 0.513500 | 1.712 sec/iter\n",
      "Epoch: 326 | Batch: 007 / 011 | Total loss: 1.404 | Reg loss: 0.040 | Tree loss: 1.404 | Accuracy: 0.518000 | 1.712 sec/iter\n",
      "Epoch: 326 | Batch: 008 / 011 | Total loss: 1.401 | Reg loss: 0.040 | Tree loss: 1.401 | Accuracy: 0.541000 | 1.712 sec/iter\n",
      "Epoch: 326 | Batch: 009 / 011 | Total loss: 1.409 | Reg loss: 0.040 | Tree loss: 1.409 | Accuracy: 0.487000 | 1.712 sec/iter\n",
      "Epoch: 326 | Batch: 010 / 011 | Total loss: 1.410 | Reg loss: 0.040 | Tree loss: 1.410 | Accuracy: 0.477816 | 1.712 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 327 | Batch: 000 / 011 | Total loss: 1.615 | Reg loss: 0.040 | Tree loss: 1.615 | Accuracy: 0.407000 | 1.713 sec/iter\n",
      "Epoch: 327 | Batch: 001 / 011 | Total loss: 1.549 | Reg loss: 0.040 | Tree loss: 1.549 | Accuracy: 0.438000 | 1.713 sec/iter\n",
      "Epoch: 327 | Batch: 002 / 011 | Total loss: 1.536 | Reg loss: 0.040 | Tree loss: 1.536 | Accuracy: 0.432500 | 1.713 sec/iter\n",
      "Epoch: 327 | Batch: 003 / 011 | Total loss: 1.477 | Reg loss: 0.040 | Tree loss: 1.477 | Accuracy: 0.481000 | 1.713 sec/iter\n",
      "Epoch: 327 | Batch: 004 / 011 | Total loss: 1.470 | Reg loss: 0.040 | Tree loss: 1.470 | Accuracy: 0.471500 | 1.713 sec/iter\n",
      "Epoch: 327 | Batch: 005 / 011 | Total loss: 1.445 | Reg loss: 0.040 | Tree loss: 1.445 | Accuracy: 0.503500 | 1.713 sec/iter\n",
      "Epoch: 327 | Batch: 006 / 011 | Total loss: 1.411 | Reg loss: 0.040 | Tree loss: 1.411 | Accuracy: 0.521000 | 1.713 sec/iter\n",
      "Epoch: 327 | Batch: 007 / 011 | Total loss: 1.411 | Reg loss: 0.040 | Tree loss: 1.411 | Accuracy: 0.504000 | 1.713 sec/iter\n",
      "Epoch: 327 | Batch: 008 / 011 | Total loss: 1.428 | Reg loss: 0.040 | Tree loss: 1.428 | Accuracy: 0.488000 | 1.713 sec/iter\n",
      "Epoch: 327 | Batch: 009 / 011 | Total loss: 1.416 | Reg loss: 0.040 | Tree loss: 1.416 | Accuracy: 0.498000 | 1.713 sec/iter\n",
      "Epoch: 327 | Batch: 010 / 011 | Total loss: 1.398 | Reg loss: 0.040 | Tree loss: 1.398 | Accuracy: 0.539249 | 1.713 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 328 | Batch: 000 / 011 | Total loss: 1.601 | Reg loss: 0.040 | Tree loss: 1.601 | Accuracy: 0.408500 | 1.713 sec/iter\n",
      "Epoch: 328 | Batch: 001 / 011 | Total loss: 1.576 | Reg loss: 0.040 | Tree loss: 1.576 | Accuracy: 0.409000 | 1.713 sec/iter\n",
      "Epoch: 328 | Batch: 002 / 011 | Total loss: 1.546 | Reg loss: 0.040 | Tree loss: 1.546 | Accuracy: 0.414000 | 1.713 sec/iter\n",
      "Epoch: 328 | Batch: 003 / 011 | Total loss: 1.479 | Reg loss: 0.040 | Tree loss: 1.479 | Accuracy: 0.457500 | 1.713 sec/iter\n",
      "Epoch: 328 | Batch: 004 / 011 | Total loss: 1.464 | Reg loss: 0.040 | Tree loss: 1.464 | Accuracy: 0.471000 | 1.713 sec/iter\n",
      "Epoch: 328 | Batch: 005 / 011 | Total loss: 1.447 | Reg loss: 0.040 | Tree loss: 1.447 | Accuracy: 0.477500 | 1.713 sec/iter\n",
      "Epoch: 328 | Batch: 006 / 011 | Total loss: 1.398 | Reg loss: 0.040 | Tree loss: 1.398 | Accuracy: 0.517000 | 1.713 sec/iter\n",
      "Epoch: 328 | Batch: 007 / 011 | Total loss: 1.417 | Reg loss: 0.040 | Tree loss: 1.417 | Accuracy: 0.515500 | 1.713 sec/iter\n",
      "Epoch: 328 | Batch: 008 / 011 | Total loss: 1.396 | Reg loss: 0.040 | Tree loss: 1.396 | Accuracy: 0.532000 | 1.713 sec/iter\n",
      "Epoch: 328 | Batch: 009 / 011 | Total loss: 1.415 | Reg loss: 0.040 | Tree loss: 1.415 | Accuracy: 0.497500 | 1.713 sec/iter\n",
      "Epoch: 328 | Batch: 010 / 011 | Total loss: 1.411 | Reg loss: 0.040 | Tree loss: 1.411 | Accuracy: 0.491468 | 1.713 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 329 | Batch: 000 / 011 | Total loss: 1.614 | Reg loss: 0.040 | Tree loss: 1.614 | Accuracy: 0.382500 | 1.715 sec/iter\n",
      "Epoch: 329 | Batch: 001 / 011 | Total loss: 1.550 | Reg loss: 0.040 | Tree loss: 1.550 | Accuracy: 0.444500 | 1.714 sec/iter\n",
      "Epoch: 329 | Batch: 002 / 011 | Total loss: 1.536 | Reg loss: 0.040 | Tree loss: 1.536 | Accuracy: 0.445500 | 1.714 sec/iter\n",
      "Epoch: 329 | Batch: 003 / 011 | Total loss: 1.470 | Reg loss: 0.040 | Tree loss: 1.470 | Accuracy: 0.479500 | 1.714 sec/iter\n",
      "Epoch: 329 | Batch: 004 / 011 | Total loss: 1.466 | Reg loss: 0.040 | Tree loss: 1.466 | Accuracy: 0.497000 | 1.715 sec/iter\n",
      "Epoch: 329 | Batch: 005 / 011 | Total loss: 1.442 | Reg loss: 0.040 | Tree loss: 1.442 | Accuracy: 0.505500 | 1.715 sec/iter\n",
      "Epoch: 329 | Batch: 006 / 011 | Total loss: 1.427 | Reg loss: 0.040 | Tree loss: 1.427 | Accuracy: 0.526500 | 1.715 sec/iter\n",
      "Epoch: 329 | Batch: 007 / 011 | Total loss: 1.412 | Reg loss: 0.040 | Tree loss: 1.412 | Accuracy: 0.515500 | 1.715 sec/iter\n",
      "Epoch: 329 | Batch: 008 / 011 | Total loss: 1.406 | Reg loss: 0.040 | Tree loss: 1.406 | Accuracy: 0.518000 | 1.715 sec/iter\n",
      "Epoch: 329 | Batch: 009 / 011 | Total loss: 1.417 | Reg loss: 0.040 | Tree loss: 1.417 | Accuracy: 0.505000 | 1.715 sec/iter\n",
      "Epoch: 329 | Batch: 010 / 011 | Total loss: 1.422 | Reg loss: 0.040 | Tree loss: 1.422 | Accuracy: 0.522184 | 1.715 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 330 | Batch: 000 / 011 | Total loss: 1.602 | Reg loss: 0.040 | Tree loss: 1.602 | Accuracy: 0.414500 | 1.716 sec/iter\n",
      "Epoch: 330 | Batch: 001 / 011 | Total loss: 1.564 | Reg loss: 0.040 | Tree loss: 1.564 | Accuracy: 0.417000 | 1.716 sec/iter\n",
      "Epoch: 330 | Batch: 002 / 011 | Total loss: 1.538 | Reg loss: 0.040 | Tree loss: 1.538 | Accuracy: 0.416500 | 1.716 sec/iter\n",
      "Epoch: 330 | Batch: 003 / 011 | Total loss: 1.511 | Reg loss: 0.040 | Tree loss: 1.511 | Accuracy: 0.442000 | 1.716 sec/iter\n",
      "Epoch: 330 | Batch: 004 / 011 | Total loss: 1.472 | Reg loss: 0.040 | Tree loss: 1.472 | Accuracy: 0.457500 | 1.716 sec/iter\n",
      "Epoch: 330 | Batch: 005 / 011 | Total loss: 1.451 | Reg loss: 0.040 | Tree loss: 1.451 | Accuracy: 0.467000 | 1.716 sec/iter\n",
      "Epoch: 330 | Batch: 006 / 011 | Total loss: 1.406 | Reg loss: 0.040 | Tree loss: 1.406 | Accuracy: 0.528500 | 1.716 sec/iter\n",
      "Epoch: 330 | Batch: 007 / 011 | Total loss: 1.405 | Reg loss: 0.040 | Tree loss: 1.405 | Accuracy: 0.510000 | 1.716 sec/iter\n",
      "Epoch: 330 | Batch: 008 / 011 | Total loss: 1.406 | Reg loss: 0.040 | Tree loss: 1.406 | Accuracy: 0.527500 | 1.716 sec/iter\n",
      "Epoch: 330 | Batch: 009 / 011 | Total loss: 1.407 | Reg loss: 0.040 | Tree loss: 1.407 | Accuracy: 0.508000 | 1.716 sec/iter\n",
      "Epoch: 330 | Batch: 010 / 011 | Total loss: 1.444 | Reg loss: 0.040 | Tree loss: 1.444 | Accuracy: 0.450512 | 1.716 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 331 | Batch: 000 / 011 | Total loss: 1.594 | Reg loss: 0.040 | Tree loss: 1.594 | Accuracy: 0.404000 | 1.717 sec/iter\n",
      "Epoch: 331 | Batch: 001 / 011 | Total loss: 1.559 | Reg loss: 0.040 | Tree loss: 1.559 | Accuracy: 0.438000 | 1.717 sec/iter\n",
      "Epoch: 331 | Batch: 002 / 011 | Total loss: 1.548 | Reg loss: 0.040 | Tree loss: 1.548 | Accuracy: 0.457000 | 1.717 sec/iter\n",
      "Epoch: 331 | Batch: 003 / 011 | Total loss: 1.490 | Reg loss: 0.040 | Tree loss: 1.490 | Accuracy: 0.474500 | 1.717 sec/iter\n",
      "Epoch: 331 | Batch: 004 / 011 | Total loss: 1.454 | Reg loss: 0.040 | Tree loss: 1.454 | Accuracy: 0.489500 | 1.717 sec/iter\n",
      "Epoch: 331 | Batch: 005 / 011 | Total loss: 1.443 | Reg loss: 0.040 | Tree loss: 1.443 | Accuracy: 0.502000 | 1.717 sec/iter\n",
      "Epoch: 331 | Batch: 006 / 011 | Total loss: 1.400 | Reg loss: 0.040 | Tree loss: 1.400 | Accuracy: 0.529000 | 1.717 sec/iter\n",
      "Epoch: 331 | Batch: 007 / 011 | Total loss: 1.405 | Reg loss: 0.040 | Tree loss: 1.405 | Accuracy: 0.525500 | 1.717 sec/iter\n",
      "Epoch: 331 | Batch: 008 / 011 | Total loss: 1.416 | Reg loss: 0.040 | Tree loss: 1.416 | Accuracy: 0.497500 | 1.717 sec/iter\n",
      "Epoch: 331 | Batch: 009 / 011 | Total loss: 1.430 | Reg loss: 0.040 | Tree loss: 1.430 | Accuracy: 0.485500 | 1.717 sec/iter\n",
      "Epoch: 331 | Batch: 010 / 011 | Total loss: 1.403 | Reg loss: 0.040 | Tree loss: 1.403 | Accuracy: 0.477816 | 1.717 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 332 | Batch: 000 / 011 | Total loss: 1.629 | Reg loss: 0.040 | Tree loss: 1.629 | Accuracy: 0.393000 | 1.718 sec/iter\n",
      "Epoch: 332 | Batch: 001 / 011 | Total loss: 1.596 | Reg loss: 0.040 | Tree loss: 1.596 | Accuracy: 0.398000 | 1.718 sec/iter\n",
      "Epoch: 332 | Batch: 002 / 011 | Total loss: 1.527 | Reg loss: 0.040 | Tree loss: 1.527 | Accuracy: 0.438000 | 1.718 sec/iter\n",
      "Epoch: 332 | Batch: 003 / 011 | Total loss: 1.491 | Reg loss: 0.040 | Tree loss: 1.491 | Accuracy: 0.461500 | 1.718 sec/iter\n",
      "Epoch: 332 | Batch: 004 / 011 | Total loss: 1.461 | Reg loss: 0.040 | Tree loss: 1.461 | Accuracy: 0.469500 | 1.718 sec/iter\n",
      "Epoch: 332 | Batch: 005 / 011 | Total loss: 1.448 | Reg loss: 0.040 | Tree loss: 1.448 | Accuracy: 0.490500 | 1.718 sec/iter\n",
      "Epoch: 332 | Batch: 006 / 011 | Total loss: 1.422 | Reg loss: 0.040 | Tree loss: 1.422 | Accuracy: 0.520500 | 1.718 sec/iter\n",
      "Epoch: 332 | Batch: 007 / 011 | Total loss: 1.406 | Reg loss: 0.040 | Tree loss: 1.406 | Accuracy: 0.518000 | 1.718 sec/iter\n",
      "Epoch: 332 | Batch: 008 / 011 | Total loss: 1.375 | Reg loss: 0.040 | Tree loss: 1.375 | Accuracy: 0.530500 | 1.718 sec/iter\n",
      "Epoch: 332 | Batch: 009 / 011 | Total loss: 1.409 | Reg loss: 0.040 | Tree loss: 1.409 | Accuracy: 0.498000 | 1.718 sec/iter\n",
      "Epoch: 332 | Batch: 010 / 011 | Total loss: 1.396 | Reg loss: 0.040 | Tree loss: 1.396 | Accuracy: 0.481229 | 1.718 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 333 | Batch: 000 / 011 | Total loss: 1.585 | Reg loss: 0.040 | Tree loss: 1.585 | Accuracy: 0.423000 | 1.72 sec/iter\n",
      "Epoch: 333 | Batch: 001 / 011 | Total loss: 1.565 | Reg loss: 0.040 | Tree loss: 1.565 | Accuracy: 0.429500 | 1.719 sec/iter\n",
      "Epoch: 333 | Batch: 002 / 011 | Total loss: 1.525 | Reg loss: 0.040 | Tree loss: 1.525 | Accuracy: 0.453500 | 1.719 sec/iter\n",
      "Epoch: 333 | Batch: 003 / 011 | Total loss: 1.530 | Reg loss: 0.040 | Tree loss: 1.530 | Accuracy: 0.451500 | 1.719 sec/iter\n",
      "Epoch: 333 | Batch: 004 / 011 | Total loss: 1.457 | Reg loss: 0.040 | Tree loss: 1.457 | Accuracy: 0.491500 | 1.719 sec/iter\n",
      "Epoch: 333 | Batch: 005 / 011 | Total loss: 1.432 | Reg loss: 0.040 | Tree loss: 1.432 | Accuracy: 0.505000 | 1.719 sec/iter\n",
      "Epoch: 333 | Batch: 006 / 011 | Total loss: 1.427 | Reg loss: 0.040 | Tree loss: 1.427 | Accuracy: 0.514000 | 1.719 sec/iter\n",
      "Epoch: 333 | Batch: 007 / 011 | Total loss: 1.429 | Reg loss: 0.040 | Tree loss: 1.429 | Accuracy: 0.500000 | 1.719 sec/iter\n",
      "Epoch: 333 | Batch: 008 / 011 | Total loss: 1.401 | Reg loss: 0.040 | Tree loss: 1.401 | Accuracy: 0.511000 | 1.719 sec/iter\n",
      "Epoch: 333 | Batch: 009 / 011 | Total loss: 1.399 | Reg loss: 0.040 | Tree loss: 1.399 | Accuracy: 0.493000 | 1.72 sec/iter\n",
      "Epoch: 333 | Batch: 010 / 011 | Total loss: 1.376 | Reg loss: 0.040 | Tree loss: 1.376 | Accuracy: 0.515358 | 1.72 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 334 | Batch: 000 / 011 | Total loss: 1.604 | Reg loss: 0.040 | Tree loss: 1.604 | Accuracy: 0.410000 | 1.72 sec/iter\n",
      "Epoch: 334 | Batch: 001 / 011 | Total loss: 1.550 | Reg loss: 0.040 | Tree loss: 1.550 | Accuracy: 0.438500 | 1.72 sec/iter\n",
      "Epoch: 334 | Batch: 002 / 011 | Total loss: 1.538 | Reg loss: 0.040 | Tree loss: 1.538 | Accuracy: 0.427000 | 1.72 sec/iter\n",
      "Epoch: 334 | Batch: 003 / 011 | Total loss: 1.499 | Reg loss: 0.040 | Tree loss: 1.499 | Accuracy: 0.467500 | 1.72 sec/iter\n",
      "Epoch: 334 | Batch: 004 / 011 | Total loss: 1.480 | Reg loss: 0.040 | Tree loss: 1.480 | Accuracy: 0.448500 | 1.72 sec/iter\n",
      "Epoch: 334 | Batch: 005 / 011 | Total loss: 1.427 | Reg loss: 0.040 | Tree loss: 1.427 | Accuracy: 0.498000 | 1.72 sec/iter\n",
      "Epoch: 334 | Batch: 006 / 011 | Total loss: 1.398 | Reg loss: 0.040 | Tree loss: 1.398 | Accuracy: 0.510500 | 1.72 sec/iter\n",
      "Epoch: 334 | Batch: 007 / 011 | Total loss: 1.412 | Reg loss: 0.040 | Tree loss: 1.412 | Accuracy: 0.520500 | 1.72 sec/iter\n",
      "Epoch: 334 | Batch: 008 / 011 | Total loss: 1.433 | Reg loss: 0.040 | Tree loss: 1.433 | Accuracy: 0.483500 | 1.72 sec/iter\n",
      "Epoch: 334 | Batch: 009 / 011 | Total loss: 1.405 | Reg loss: 0.040 | Tree loss: 1.405 | Accuracy: 0.496500 | 1.72 sec/iter\n",
      "Epoch: 334 | Batch: 010 / 011 | Total loss: 1.436 | Reg loss: 0.040 | Tree loss: 1.436 | Accuracy: 0.464164 | 1.72 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 335 | Batch: 000 / 011 | Total loss: 1.605 | Reg loss: 0.040 | Tree loss: 1.605 | Accuracy: 0.413000 | 1.721 sec/iter\n",
      "Epoch: 335 | Batch: 001 / 011 | Total loss: 1.588 | Reg loss: 0.040 | Tree loss: 1.588 | Accuracy: 0.408000 | 1.721 sec/iter\n",
      "Epoch: 335 | Batch: 002 / 011 | Total loss: 1.534 | Reg loss: 0.040 | Tree loss: 1.534 | Accuracy: 0.440500 | 1.721 sec/iter\n",
      "Epoch: 335 | Batch: 003 / 011 | Total loss: 1.480 | Reg loss: 0.040 | Tree loss: 1.480 | Accuracy: 0.474000 | 1.721 sec/iter\n",
      "Epoch: 335 | Batch: 004 / 011 | Total loss: 1.460 | Reg loss: 0.040 | Tree loss: 1.460 | Accuracy: 0.469000 | 1.721 sec/iter\n",
      "Epoch: 335 | Batch: 005 / 011 | Total loss: 1.441 | Reg loss: 0.040 | Tree loss: 1.441 | Accuracy: 0.513000 | 1.721 sec/iter\n",
      "Epoch: 335 | Batch: 006 / 011 | Total loss: 1.416 | Reg loss: 0.040 | Tree loss: 1.416 | Accuracy: 0.519500 | 1.721 sec/iter\n",
      "Epoch: 335 | Batch: 007 / 011 | Total loss: 1.398 | Reg loss: 0.040 | Tree loss: 1.398 | Accuracy: 0.529500 | 1.721 sec/iter\n",
      "Epoch: 335 | Batch: 008 / 011 | Total loss: 1.420 | Reg loss: 0.040 | Tree loss: 1.420 | Accuracy: 0.495000 | 1.721 sec/iter\n",
      "Epoch: 335 | Batch: 009 / 011 | Total loss: 1.408 | Reg loss: 0.040 | Tree loss: 1.408 | Accuracy: 0.484500 | 1.721 sec/iter\n",
      "Epoch: 335 | Batch: 010 / 011 | Total loss: 1.430 | Reg loss: 0.040 | Tree loss: 1.430 | Accuracy: 0.501706 | 1.721 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 336 | Batch: 000 / 011 | Total loss: 1.595 | Reg loss: 0.040 | Tree loss: 1.595 | Accuracy: 0.424000 | 1.722 sec/iter\n",
      "Epoch: 336 | Batch: 001 / 011 | Total loss: 1.564 | Reg loss: 0.040 | Tree loss: 1.564 | Accuracy: 0.422500 | 1.722 sec/iter\n",
      "Epoch: 336 | Batch: 002 / 011 | Total loss: 1.528 | Reg loss: 0.040 | Tree loss: 1.528 | Accuracy: 0.457000 | 1.722 sec/iter\n",
      "Epoch: 336 | Batch: 003 / 011 | Total loss: 1.495 | Reg loss: 0.040 | Tree loss: 1.495 | Accuracy: 0.464500 | 1.722 sec/iter\n",
      "Epoch: 336 | Batch: 004 / 011 | Total loss: 1.478 | Reg loss: 0.040 | Tree loss: 1.478 | Accuracy: 0.471500 | 1.722 sec/iter\n",
      "Epoch: 336 | Batch: 005 / 011 | Total loss: 1.432 | Reg loss: 0.040 | Tree loss: 1.432 | Accuracy: 0.503000 | 1.722 sec/iter\n",
      "Epoch: 336 | Batch: 006 / 011 | Total loss: 1.418 | Reg loss: 0.040 | Tree loss: 1.418 | Accuracy: 0.514500 | 1.722 sec/iter\n",
      "Epoch: 336 | Batch: 007 / 011 | Total loss: 1.415 | Reg loss: 0.040 | Tree loss: 1.415 | Accuracy: 0.536500 | 1.722 sec/iter\n",
      "Epoch: 336 | Batch: 008 / 011 | Total loss: 1.413 | Reg loss: 0.040 | Tree loss: 1.413 | Accuracy: 0.487500 | 1.722 sec/iter\n",
      "Epoch: 336 | Batch: 009 / 011 | Total loss: 1.407 | Reg loss: 0.040 | Tree loss: 1.407 | Accuracy: 0.495500 | 1.722 sec/iter\n",
      "Epoch: 336 | Batch: 010 / 011 | Total loss: 1.403 | Reg loss: 0.040 | Tree loss: 1.403 | Accuracy: 0.460751 | 1.722 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 337 | Batch: 000 / 011 | Total loss: 1.632 | Reg loss: 0.040 | Tree loss: 1.632 | Accuracy: 0.384000 | 1.723 sec/iter\n",
      "Epoch: 337 | Batch: 001 / 011 | Total loss: 1.565 | Reg loss: 0.040 | Tree loss: 1.565 | Accuracy: 0.440000 | 1.723 sec/iter\n",
      "Epoch: 337 | Batch: 002 / 011 | Total loss: 1.559 | Reg loss: 0.040 | Tree loss: 1.559 | Accuracy: 0.409000 | 1.723 sec/iter\n",
      "Epoch: 337 | Batch: 003 / 011 | Total loss: 1.516 | Reg loss: 0.040 | Tree loss: 1.516 | Accuracy: 0.454500 | 1.724 sec/iter\n",
      "Epoch: 337 | Batch: 004 / 011 | Total loss: 1.447 | Reg loss: 0.040 | Tree loss: 1.447 | Accuracy: 0.496000 | 1.724 sec/iter\n",
      "Epoch: 337 | Batch: 005 / 011 | Total loss: 1.432 | Reg loss: 0.040 | Tree loss: 1.432 | Accuracy: 0.507000 | 1.724 sec/iter\n",
      "Epoch: 337 | Batch: 006 / 011 | Total loss: 1.388 | Reg loss: 0.040 | Tree loss: 1.388 | Accuracy: 0.531000 | 1.724 sec/iter\n",
      "Epoch: 337 | Batch: 007 / 011 | Total loss: 1.418 | Reg loss: 0.040 | Tree loss: 1.418 | Accuracy: 0.505500 | 1.724 sec/iter\n",
      "Epoch: 337 | Batch: 008 / 011 | Total loss: 1.412 | Reg loss: 0.040 | Tree loss: 1.412 | Accuracy: 0.481000 | 1.724 sec/iter\n",
      "Epoch: 337 | Batch: 009 / 011 | Total loss: 1.387 | Reg loss: 0.040 | Tree loss: 1.387 | Accuracy: 0.501500 | 1.724 sec/iter\n",
      "Epoch: 337 | Batch: 010 / 011 | Total loss: 1.436 | Reg loss: 0.040 | Tree loss: 1.436 | Accuracy: 0.460751 | 1.724 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 338 | Batch: 000 / 011 | Total loss: 1.611 | Reg loss: 0.040 | Tree loss: 1.611 | Accuracy: 0.399500 | 1.725 sec/iter\n",
      "Epoch: 338 | Batch: 001 / 011 | Total loss: 1.589 | Reg loss: 0.040 | Tree loss: 1.589 | Accuracy: 0.414500 | 1.725 sec/iter\n",
      "Epoch: 338 | Batch: 002 / 011 | Total loss: 1.524 | Reg loss: 0.040 | Tree loss: 1.524 | Accuracy: 0.439000 | 1.725 sec/iter\n",
      "Epoch: 338 | Batch: 003 / 011 | Total loss: 1.504 | Reg loss: 0.040 | Tree loss: 1.504 | Accuracy: 0.471500 | 1.725 sec/iter\n",
      "Epoch: 338 | Batch: 004 / 011 | Total loss: 1.461 | Reg loss: 0.040 | Tree loss: 1.461 | Accuracy: 0.478500 | 1.725 sec/iter\n",
      "Epoch: 338 | Batch: 005 / 011 | Total loss: 1.446 | Reg loss: 0.040 | Tree loss: 1.446 | Accuracy: 0.486000 | 1.725 sec/iter\n",
      "Epoch: 338 | Batch: 006 / 011 | Total loss: 1.415 | Reg loss: 0.040 | Tree loss: 1.415 | Accuracy: 0.500000 | 1.725 sec/iter\n",
      "Epoch: 338 | Batch: 007 / 011 | Total loss: 1.430 | Reg loss: 0.040 | Tree loss: 1.430 | Accuracy: 0.504000 | 1.725 sec/iter\n",
      "Epoch: 338 | Batch: 008 / 011 | Total loss: 1.393 | Reg loss: 0.040 | Tree loss: 1.393 | Accuracy: 0.500500 | 1.725 sec/iter\n",
      "Epoch: 338 | Batch: 009 / 011 | Total loss: 1.378 | Reg loss: 0.040 | Tree loss: 1.378 | Accuracy: 0.494000 | 1.725 sec/iter\n",
      "Epoch: 338 | Batch: 010 / 011 | Total loss: 1.385 | Reg loss: 0.040 | Tree loss: 1.385 | Accuracy: 0.525597 | 1.725 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 339 | Batch: 000 / 011 | Total loss: 1.618 | Reg loss: 0.040 | Tree loss: 1.618 | Accuracy: 0.394000 | 1.725 sec/iter\n",
      "Epoch: 339 | Batch: 001 / 011 | Total loss: 1.574 | Reg loss: 0.040 | Tree loss: 1.574 | Accuracy: 0.422000 | 1.725 sec/iter\n",
      "Epoch: 339 | Batch: 002 / 011 | Total loss: 1.548 | Reg loss: 0.040 | Tree loss: 1.548 | Accuracy: 0.419500 | 1.725 sec/iter\n",
      "Epoch: 339 | Batch: 003 / 011 | Total loss: 1.490 | Reg loss: 0.040 | Tree loss: 1.490 | Accuracy: 0.471000 | 1.725 sec/iter\n",
      "Epoch: 339 | Batch: 004 / 011 | Total loss: 1.475 | Reg loss: 0.040 | Tree loss: 1.475 | Accuracy: 0.473000 | 1.725 sec/iter\n",
      "Epoch: 339 | Batch: 005 / 011 | Total loss: 1.440 | Reg loss: 0.040 | Tree loss: 1.440 | Accuracy: 0.503500 | 1.725 sec/iter\n",
      "Epoch: 339 | Batch: 006 / 011 | Total loss: 1.422 | Reg loss: 0.040 | Tree loss: 1.422 | Accuracy: 0.523500 | 1.725 sec/iter\n",
      "Epoch: 339 | Batch: 007 / 011 | Total loss: 1.409 | Reg loss: 0.040 | Tree loss: 1.409 | Accuracy: 0.513000 | 1.725 sec/iter\n",
      "Epoch: 339 | Batch: 008 / 011 | Total loss: 1.393 | Reg loss: 0.040 | Tree loss: 1.393 | Accuracy: 0.511500 | 1.725 sec/iter\n",
      "Epoch: 339 | Batch: 009 / 011 | Total loss: 1.367 | Reg loss: 0.040 | Tree loss: 1.367 | Accuracy: 0.512500 | 1.725 sec/iter\n",
      "Epoch: 339 | Batch: 010 / 011 | Total loss: 1.417 | Reg loss: 0.040 | Tree loss: 1.417 | Accuracy: 0.453925 | 1.725 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 340 | Batch: 000 / 011 | Total loss: 1.612 | Reg loss: 0.040 | Tree loss: 1.612 | Accuracy: 0.410000 | 1.726 sec/iter\n",
      "Epoch: 340 | Batch: 001 / 011 | Total loss: 1.594 | Reg loss: 0.040 | Tree loss: 1.594 | Accuracy: 0.405000 | 1.726 sec/iter\n",
      "Epoch: 340 | Batch: 002 / 011 | Total loss: 1.530 | Reg loss: 0.040 | Tree loss: 1.530 | Accuracy: 0.449500 | 1.726 sec/iter\n",
      "Epoch: 340 | Batch: 003 / 011 | Total loss: 1.516 | Reg loss: 0.040 | Tree loss: 1.516 | Accuracy: 0.445000 | 1.726 sec/iter\n",
      "Epoch: 340 | Batch: 004 / 011 | Total loss: 1.473 | Reg loss: 0.040 | Tree loss: 1.473 | Accuracy: 0.466500 | 1.726 sec/iter\n",
      "Epoch: 340 | Batch: 005 / 011 | Total loss: 1.429 | Reg loss: 0.040 | Tree loss: 1.429 | Accuracy: 0.508500 | 1.726 sec/iter\n",
      "Epoch: 340 | Batch: 006 / 011 | Total loss: 1.400 | Reg loss: 0.040 | Tree loss: 1.400 | Accuracy: 0.527500 | 1.726 sec/iter\n",
      "Epoch: 340 | Batch: 007 / 011 | Total loss: 1.416 | Reg loss: 0.040 | Tree loss: 1.416 | Accuracy: 0.518000 | 1.726 sec/iter\n",
      "Epoch: 340 | Batch: 008 / 011 | Total loss: 1.393 | Reg loss: 0.040 | Tree loss: 1.393 | Accuracy: 0.514000 | 1.726 sec/iter\n",
      "Epoch: 340 | Batch: 009 / 011 | Total loss: 1.395 | Reg loss: 0.040 | Tree loss: 1.395 | Accuracy: 0.513000 | 1.726 sec/iter\n",
      "Epoch: 340 | Batch: 010 / 011 | Total loss: 1.420 | Reg loss: 0.040 | Tree loss: 1.420 | Accuracy: 0.501706 | 1.726 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 341 | Batch: 000 / 011 | Total loss: 1.605 | Reg loss: 0.040 | Tree loss: 1.605 | Accuracy: 0.415500 | 1.727 sec/iter\n",
      "Epoch: 341 | Batch: 001 / 011 | Total loss: 1.596 | Reg loss: 0.040 | Tree loss: 1.596 | Accuracy: 0.398500 | 1.727 sec/iter\n",
      "Epoch: 341 | Batch: 002 / 011 | Total loss: 1.523 | Reg loss: 0.040 | Tree loss: 1.523 | Accuracy: 0.443000 | 1.727 sec/iter\n",
      "Epoch: 341 | Batch: 003 / 011 | Total loss: 1.488 | Reg loss: 0.040 | Tree loss: 1.488 | Accuracy: 0.454000 | 1.727 sec/iter\n",
      "Epoch: 341 | Batch: 004 / 011 | Total loss: 1.480 | Reg loss: 0.040 | Tree loss: 1.480 | Accuracy: 0.476500 | 1.727 sec/iter\n",
      "Epoch: 341 | Batch: 005 / 011 | Total loss: 1.428 | Reg loss: 0.040 | Tree loss: 1.428 | Accuracy: 0.491000 | 1.727 sec/iter\n",
      "Epoch: 341 | Batch: 006 / 011 | Total loss: 1.420 | Reg loss: 0.040 | Tree loss: 1.420 | Accuracy: 0.512500 | 1.728 sec/iter\n",
      "Epoch: 341 | Batch: 007 / 011 | Total loss: 1.399 | Reg loss: 0.040 | Tree loss: 1.399 | Accuracy: 0.518000 | 1.728 sec/iter\n",
      "Epoch: 341 | Batch: 008 / 011 | Total loss: 1.389 | Reg loss: 0.040 | Tree loss: 1.389 | Accuracy: 0.514500 | 1.728 sec/iter\n",
      "Epoch: 341 | Batch: 009 / 011 | Total loss: 1.416 | Reg loss: 0.040 | Tree loss: 1.416 | Accuracy: 0.491500 | 1.727 sec/iter\n",
      "Epoch: 341 | Batch: 010 / 011 | Total loss: 1.391 | Reg loss: 0.040 | Tree loss: 1.391 | Accuracy: 0.501706 | 1.727 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 342 | Batch: 000 / 011 | Total loss: 1.606 | Reg loss: 0.040 | Tree loss: 1.606 | Accuracy: 0.406500 | 1.729 sec/iter\n",
      "Epoch: 342 | Batch: 001 / 011 | Total loss: 1.566 | Reg loss: 0.040 | Tree loss: 1.566 | Accuracy: 0.419000 | 1.729 sec/iter\n",
      "Epoch: 342 | Batch: 002 / 011 | Total loss: 1.540 | Reg loss: 0.040 | Tree loss: 1.540 | Accuracy: 0.418000 | 1.729 sec/iter\n",
      "Epoch: 342 | Batch: 003 / 011 | Total loss: 1.468 | Reg loss: 0.040 | Tree loss: 1.468 | Accuracy: 0.486000 | 1.729 sec/iter\n",
      "Epoch: 342 | Batch: 004 / 011 | Total loss: 1.477 | Reg loss: 0.040 | Tree loss: 1.477 | Accuracy: 0.473000 | 1.729 sec/iter\n",
      "Epoch: 342 | Batch: 005 / 011 | Total loss: 1.440 | Reg loss: 0.040 | Tree loss: 1.440 | Accuracy: 0.488000 | 1.729 sec/iter\n",
      "Epoch: 342 | Batch: 006 / 011 | Total loss: 1.403 | Reg loss: 0.040 | Tree loss: 1.403 | Accuracy: 0.514000 | 1.729 sec/iter\n",
      "Epoch: 342 | Batch: 007 / 011 | Total loss: 1.405 | Reg loss: 0.040 | Tree loss: 1.405 | Accuracy: 0.509500 | 1.729 sec/iter\n",
      "Epoch: 342 | Batch: 008 / 011 | Total loss: 1.424 | Reg loss: 0.040 | Tree loss: 1.424 | Accuracy: 0.506500 | 1.729 sec/iter\n",
      "Epoch: 342 | Batch: 009 / 011 | Total loss: 1.405 | Reg loss: 0.040 | Tree loss: 1.405 | Accuracy: 0.502000 | 1.729 sec/iter\n",
      "Epoch: 342 | Batch: 010 / 011 | Total loss: 1.437 | Reg loss: 0.040 | Tree loss: 1.437 | Accuracy: 0.481229 | 1.729 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 343 | Batch: 000 / 011 | Total loss: 1.596 | Reg loss: 0.040 | Tree loss: 1.596 | Accuracy: 0.402500 | 1.73 sec/iter\n",
      "Epoch: 343 | Batch: 001 / 011 | Total loss: 1.571 | Reg loss: 0.040 | Tree loss: 1.571 | Accuracy: 0.423500 | 1.73 sec/iter\n",
      "Epoch: 343 | Batch: 002 / 011 | Total loss: 1.532 | Reg loss: 0.040 | Tree loss: 1.532 | Accuracy: 0.428000 | 1.73 sec/iter\n",
      "Epoch: 343 | Batch: 003 / 011 | Total loss: 1.485 | Reg loss: 0.040 | Tree loss: 1.485 | Accuracy: 0.465000 | 1.73 sec/iter\n",
      "Epoch: 343 | Batch: 004 / 011 | Total loss: 1.460 | Reg loss: 0.040 | Tree loss: 1.460 | Accuracy: 0.479500 | 1.73 sec/iter\n",
      "Epoch: 343 | Batch: 005 / 011 | Total loss: 1.433 | Reg loss: 0.040 | Tree loss: 1.433 | Accuracy: 0.499500 | 1.73 sec/iter\n",
      "Epoch: 343 | Batch: 006 / 011 | Total loss: 1.417 | Reg loss: 0.040 | Tree loss: 1.417 | Accuracy: 0.515000 | 1.73 sec/iter\n",
      "Epoch: 343 | Batch: 007 / 011 | Total loss: 1.435 | Reg loss: 0.040 | Tree loss: 1.435 | Accuracy: 0.496000 | 1.73 sec/iter\n",
      "Epoch: 343 | Batch: 008 / 011 | Total loss: 1.423 | Reg loss: 0.040 | Tree loss: 1.423 | Accuracy: 0.498500 | 1.73 sec/iter\n",
      "Epoch: 343 | Batch: 009 / 011 | Total loss: 1.392 | Reg loss: 0.040 | Tree loss: 1.392 | Accuracy: 0.513500 | 1.73 sec/iter\n",
      "Epoch: 343 | Batch: 010 / 011 | Total loss: 1.369 | Reg loss: 0.040 | Tree loss: 1.369 | Accuracy: 0.484642 | 1.73 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 344 | Batch: 000 / 011 | Total loss: 1.616 | Reg loss: 0.040 | Tree loss: 1.616 | Accuracy: 0.389000 | 1.731 sec/iter\n",
      "Epoch: 344 | Batch: 001 / 011 | Total loss: 1.596 | Reg loss: 0.040 | Tree loss: 1.596 | Accuracy: 0.408000 | 1.731 sec/iter\n",
      "Epoch: 344 | Batch: 002 / 011 | Total loss: 1.524 | Reg loss: 0.040 | Tree loss: 1.524 | Accuracy: 0.442000 | 1.731 sec/iter\n",
      "Epoch: 344 | Batch: 003 / 011 | Total loss: 1.497 | Reg loss: 0.040 | Tree loss: 1.497 | Accuracy: 0.471500 | 1.731 sec/iter\n",
      "Epoch: 344 | Batch: 004 / 011 | Total loss: 1.458 | Reg loss: 0.040 | Tree loss: 1.458 | Accuracy: 0.474000 | 1.731 sec/iter\n",
      "Epoch: 344 | Batch: 005 / 011 | Total loss: 1.444 | Reg loss: 0.040 | Tree loss: 1.444 | Accuracy: 0.487000 | 1.731 sec/iter\n",
      "Epoch: 344 | Batch: 006 / 011 | Total loss: 1.397 | Reg loss: 0.040 | Tree loss: 1.397 | Accuracy: 0.526500 | 1.731 sec/iter\n",
      "Epoch: 344 | Batch: 007 / 011 | Total loss: 1.416 | Reg loss: 0.040 | Tree loss: 1.416 | Accuracy: 0.513000 | 1.731 sec/iter\n",
      "Epoch: 344 | Batch: 008 / 011 | Total loss: 1.378 | Reg loss: 0.040 | Tree loss: 1.378 | Accuracy: 0.529000 | 1.731 sec/iter\n",
      "Epoch: 344 | Batch: 009 / 011 | Total loss: 1.407 | Reg loss: 0.040 | Tree loss: 1.407 | Accuracy: 0.502500 | 1.731 sec/iter\n",
      "Epoch: 344 | Batch: 010 / 011 | Total loss: 1.439 | Reg loss: 0.040 | Tree loss: 1.439 | Accuracy: 0.436860 | 1.731 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 345 | Batch: 000 / 011 | Total loss: 1.598 | Reg loss: 0.040 | Tree loss: 1.598 | Accuracy: 0.400000 | 1.732 sec/iter\n",
      "Epoch: 345 | Batch: 001 / 011 | Total loss: 1.577 | Reg loss: 0.040 | Tree loss: 1.577 | Accuracy: 0.413000 | 1.732 sec/iter\n",
      "Epoch: 345 | Batch: 002 / 011 | Total loss: 1.541 | Reg loss: 0.040 | Tree loss: 1.541 | Accuracy: 0.428000 | 1.732 sec/iter\n",
      "Epoch: 345 | Batch: 003 / 011 | Total loss: 1.477 | Reg loss: 0.040 | Tree loss: 1.477 | Accuracy: 0.461500 | 1.732 sec/iter\n",
      "Epoch: 345 | Batch: 004 / 011 | Total loss: 1.458 | Reg loss: 0.040 | Tree loss: 1.458 | Accuracy: 0.485000 | 1.732 sec/iter\n",
      "Epoch: 345 | Batch: 005 / 011 | Total loss: 1.436 | Reg loss: 0.040 | Tree loss: 1.436 | Accuracy: 0.504000 | 1.732 sec/iter\n",
      "Epoch: 345 | Batch: 006 / 011 | Total loss: 1.414 | Reg loss: 0.040 | Tree loss: 1.414 | Accuracy: 0.506500 | 1.732 sec/iter\n",
      "Epoch: 345 | Batch: 007 / 011 | Total loss: 1.432 | Reg loss: 0.040 | Tree loss: 1.432 | Accuracy: 0.512500 | 1.732 sec/iter\n",
      "Epoch: 345 | Batch: 008 / 011 | Total loss: 1.420 | Reg loss: 0.040 | Tree loss: 1.420 | Accuracy: 0.497500 | 1.732 sec/iter\n",
      "Epoch: 345 | Batch: 009 / 011 | Total loss: 1.388 | Reg loss: 0.040 | Tree loss: 1.388 | Accuracy: 0.509000 | 1.732 sec/iter\n",
      "Epoch: 345 | Batch: 010 / 011 | Total loss: 1.444 | Reg loss: 0.040 | Tree loss: 1.444 | Accuracy: 0.484642 | 1.732 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 346 | Batch: 000 / 011 | Total loss: 1.595 | Reg loss: 0.040 | Tree loss: 1.595 | Accuracy: 0.410500 | 1.733 sec/iter\n",
      "Epoch: 346 | Batch: 001 / 011 | Total loss: 1.573 | Reg loss: 0.040 | Tree loss: 1.573 | Accuracy: 0.422000 | 1.733 sec/iter\n",
      "Epoch: 346 | Batch: 002 / 011 | Total loss: 1.555 | Reg loss: 0.040 | Tree loss: 1.555 | Accuracy: 0.435000 | 1.733 sec/iter\n",
      "Epoch: 346 | Batch: 003 / 011 | Total loss: 1.497 | Reg loss: 0.040 | Tree loss: 1.497 | Accuracy: 0.470500 | 1.733 sec/iter\n",
      "Epoch: 346 | Batch: 004 / 011 | Total loss: 1.455 | Reg loss: 0.040 | Tree loss: 1.455 | Accuracy: 0.464000 | 1.733 sec/iter\n",
      "Epoch: 346 | Batch: 005 / 011 | Total loss: 1.440 | Reg loss: 0.040 | Tree loss: 1.440 | Accuracy: 0.500500 | 1.733 sec/iter\n",
      "Epoch: 346 | Batch: 006 / 011 | Total loss: 1.412 | Reg loss: 0.040 | Tree loss: 1.412 | Accuracy: 0.517500 | 1.733 sec/iter\n",
      "Epoch: 346 | Batch: 007 / 011 | Total loss: 1.432 | Reg loss: 0.040 | Tree loss: 1.432 | Accuracy: 0.502500 | 1.733 sec/iter\n",
      "Epoch: 346 | Batch: 008 / 011 | Total loss: 1.398 | Reg loss: 0.040 | Tree loss: 1.398 | Accuracy: 0.497000 | 1.733 sec/iter\n",
      "Epoch: 346 | Batch: 009 / 011 | Total loss: 1.398 | Reg loss: 0.040 | Tree loss: 1.398 | Accuracy: 0.486000 | 1.733 sec/iter\n",
      "Epoch: 346 | Batch: 010 / 011 | Total loss: 1.366 | Reg loss: 0.040 | Tree loss: 1.366 | Accuracy: 0.549488 | 1.733 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 347 | Batch: 000 / 011 | Total loss: 1.622 | Reg loss: 0.040 | Tree loss: 1.622 | Accuracy: 0.402000 | 1.734 sec/iter\n",
      "Epoch: 347 | Batch: 001 / 011 | Total loss: 1.555 | Reg loss: 0.040 | Tree loss: 1.555 | Accuracy: 0.418000 | 1.734 sec/iter\n",
      "Epoch: 347 | Batch: 002 / 011 | Total loss: 1.548 | Reg loss: 0.040 | Tree loss: 1.548 | Accuracy: 0.416500 | 1.734 sec/iter\n",
      "Epoch: 347 | Batch: 003 / 011 | Total loss: 1.492 | Reg loss: 0.040 | Tree loss: 1.492 | Accuracy: 0.460000 | 1.734 sec/iter\n",
      "Epoch: 347 | Batch: 004 / 011 | Total loss: 1.461 | Reg loss: 0.040 | Tree loss: 1.461 | Accuracy: 0.503000 | 1.734 sec/iter\n",
      "Epoch: 347 | Batch: 005 / 011 | Total loss: 1.416 | Reg loss: 0.040 | Tree loss: 1.416 | Accuracy: 0.526000 | 1.734 sec/iter\n",
      "Epoch: 347 | Batch: 006 / 011 | Total loss: 1.427 | Reg loss: 0.040 | Tree loss: 1.427 | Accuracy: 0.530000 | 1.734 sec/iter\n",
      "Epoch: 347 | Batch: 007 / 011 | Total loss: 1.419 | Reg loss: 0.040 | Tree loss: 1.419 | Accuracy: 0.528000 | 1.734 sec/iter\n",
      "Epoch: 347 | Batch: 008 / 011 | Total loss: 1.401 | Reg loss: 0.040 | Tree loss: 1.401 | Accuracy: 0.513000 | 1.734 sec/iter\n",
      "Epoch: 347 | Batch: 009 / 011 | Total loss: 1.390 | Reg loss: 0.040 | Tree loss: 1.390 | Accuracy: 0.495000 | 1.734 sec/iter\n",
      "Epoch: 347 | Batch: 010 / 011 | Total loss: 1.378 | Reg loss: 0.040 | Tree loss: 1.378 | Accuracy: 0.559727 | 1.734 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 348 | Batch: 000 / 011 | Total loss: 1.598 | Reg loss: 0.040 | Tree loss: 1.598 | Accuracy: 0.411500 | 1.735 sec/iter\n",
      "Epoch: 348 | Batch: 001 / 011 | Total loss: 1.566 | Reg loss: 0.040 | Tree loss: 1.566 | Accuracy: 0.431500 | 1.735 sec/iter\n",
      "Epoch: 348 | Batch: 002 / 011 | Total loss: 1.528 | Reg loss: 0.040 | Tree loss: 1.528 | Accuracy: 0.436000 | 1.735 sec/iter\n",
      "Epoch: 348 | Batch: 003 / 011 | Total loss: 1.509 | Reg loss: 0.040 | Tree loss: 1.509 | Accuracy: 0.423500 | 1.735 sec/iter\n",
      "Epoch: 348 | Batch: 004 / 011 | Total loss: 1.461 | Reg loss: 0.040 | Tree loss: 1.461 | Accuracy: 0.473500 | 1.735 sec/iter\n",
      "Epoch: 348 | Batch: 005 / 011 | Total loss: 1.451 | Reg loss: 0.040 | Tree loss: 1.451 | Accuracy: 0.479000 | 1.735 sec/iter\n",
      "Epoch: 348 | Batch: 006 / 011 | Total loss: 1.427 | Reg loss: 0.040 | Tree loss: 1.427 | Accuracy: 0.512000 | 1.735 sec/iter\n",
      "Epoch: 348 | Batch: 007 / 011 | Total loss: 1.389 | Reg loss: 0.040 | Tree loss: 1.389 | Accuracy: 0.528000 | 1.735 sec/iter\n",
      "Epoch: 348 | Batch: 008 / 011 | Total loss: 1.411 | Reg loss: 0.040 | Tree loss: 1.411 | Accuracy: 0.493000 | 1.735 sec/iter\n",
      "Epoch: 348 | Batch: 009 / 011 | Total loss: 1.394 | Reg loss: 0.040 | Tree loss: 1.394 | Accuracy: 0.491000 | 1.735 sec/iter\n",
      "Epoch: 348 | Batch: 010 / 011 | Total loss: 1.337 | Reg loss: 0.040 | Tree loss: 1.337 | Accuracy: 0.529010 | 1.735 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 349 | Batch: 000 / 011 | Total loss: 1.606 | Reg loss: 0.040 | Tree loss: 1.606 | Accuracy: 0.405000 | 1.736 sec/iter\n",
      "Epoch: 349 | Batch: 001 / 011 | Total loss: 1.571 | Reg loss: 0.040 | Tree loss: 1.571 | Accuracy: 0.428500 | 1.736 sec/iter\n",
      "Epoch: 349 | Batch: 002 / 011 | Total loss: 1.527 | Reg loss: 0.040 | Tree loss: 1.527 | Accuracy: 0.437000 | 1.736 sec/iter\n",
      "Epoch: 349 | Batch: 003 / 011 | Total loss: 1.472 | Reg loss: 0.040 | Tree loss: 1.472 | Accuracy: 0.480000 | 1.736 sec/iter\n",
      "Epoch: 349 | Batch: 004 / 011 | Total loss: 1.440 | Reg loss: 0.040 | Tree loss: 1.440 | Accuracy: 0.497000 | 1.736 sec/iter\n",
      "Epoch: 349 | Batch: 005 / 011 | Total loss: 1.445 | Reg loss: 0.040 | Tree loss: 1.445 | Accuracy: 0.499500 | 1.736 sec/iter\n",
      "Epoch: 349 | Batch: 006 / 011 | Total loss: 1.432 | Reg loss: 0.040 | Tree loss: 1.432 | Accuracy: 0.524000 | 1.736 sec/iter\n",
      "Epoch: 349 | Batch: 007 / 011 | Total loss: 1.411 | Reg loss: 0.040 | Tree loss: 1.411 | Accuracy: 0.529000 | 1.736 sec/iter\n",
      "Epoch: 349 | Batch: 008 / 011 | Total loss: 1.426 | Reg loss: 0.040 | Tree loss: 1.426 | Accuracy: 0.501500 | 1.736 sec/iter\n",
      "Epoch: 349 | Batch: 009 / 011 | Total loss: 1.406 | Reg loss: 0.040 | Tree loss: 1.406 | Accuracy: 0.488000 | 1.736 sec/iter\n",
      "Epoch: 349 | Batch: 010 / 011 | Total loss: 1.436 | Reg loss: 0.040 | Tree loss: 1.436 | Accuracy: 0.477816 | 1.736 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 350 | Batch: 000 / 011 | Total loss: 1.615 | Reg loss: 0.040 | Tree loss: 1.615 | Accuracy: 0.395000 | 1.736 sec/iter\n",
      "Epoch: 350 | Batch: 001 / 011 | Total loss: 1.561 | Reg loss: 0.040 | Tree loss: 1.561 | Accuracy: 0.428500 | 1.736 sec/iter\n",
      "Epoch: 350 | Batch: 002 / 011 | Total loss: 1.524 | Reg loss: 0.040 | Tree loss: 1.524 | Accuracy: 0.434000 | 1.736 sec/iter\n",
      "Epoch: 350 | Batch: 003 / 011 | Total loss: 1.507 | Reg loss: 0.040 | Tree loss: 1.507 | Accuracy: 0.443500 | 1.736 sec/iter\n",
      "Epoch: 350 | Batch: 004 / 011 | Total loss: 1.476 | Reg loss: 0.040 | Tree loss: 1.476 | Accuracy: 0.467500 | 1.736 sec/iter\n",
      "Epoch: 350 | Batch: 005 / 011 | Total loss: 1.433 | Reg loss: 0.040 | Tree loss: 1.433 | Accuracy: 0.499000 | 1.736 sec/iter\n",
      "Epoch: 350 | Batch: 006 / 011 | Total loss: 1.429 | Reg loss: 0.040 | Tree loss: 1.429 | Accuracy: 0.504000 | 1.736 sec/iter\n",
      "Epoch: 350 | Batch: 007 / 011 | Total loss: 1.397 | Reg loss: 0.040 | Tree loss: 1.397 | Accuracy: 0.527000 | 1.736 sec/iter\n",
      "Epoch: 350 | Batch: 008 / 011 | Total loss: 1.414 | Reg loss: 0.040 | Tree loss: 1.414 | Accuracy: 0.501000 | 1.736 sec/iter\n",
      "Epoch: 350 | Batch: 009 / 011 | Total loss: 1.389 | Reg loss: 0.040 | Tree loss: 1.389 | Accuracy: 0.491000 | 1.736 sec/iter\n",
      "Epoch: 350 | Batch: 010 / 011 | Total loss: 1.412 | Reg loss: 0.040 | Tree loss: 1.412 | Accuracy: 0.505119 | 1.736 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 351 | Batch: 000 / 011 | Total loss: 1.591 | Reg loss: 0.040 | Tree loss: 1.591 | Accuracy: 0.408000 | 1.738 sec/iter\n",
      "Epoch: 351 | Batch: 001 / 011 | Total loss: 1.562 | Reg loss: 0.040 | Tree loss: 1.562 | Accuracy: 0.424000 | 1.738 sec/iter\n",
      "Epoch: 351 | Batch: 002 / 011 | Total loss: 1.531 | Reg loss: 0.040 | Tree loss: 1.531 | Accuracy: 0.433500 | 1.738 sec/iter\n",
      "Epoch: 351 | Batch: 003 / 011 | Total loss: 1.512 | Reg loss: 0.040 | Tree loss: 1.512 | Accuracy: 0.443000 | 1.738 sec/iter\n",
      "Epoch: 351 | Batch: 004 / 011 | Total loss: 1.476 | Reg loss: 0.040 | Tree loss: 1.476 | Accuracy: 0.468000 | 1.738 sec/iter\n",
      "Epoch: 351 | Batch: 005 / 011 | Total loss: 1.442 | Reg loss: 0.040 | Tree loss: 1.442 | Accuracy: 0.500000 | 1.738 sec/iter\n",
      "Epoch: 351 | Batch: 006 / 011 | Total loss: 1.413 | Reg loss: 0.040 | Tree loss: 1.413 | Accuracy: 0.519500 | 1.738 sec/iter\n",
      "Epoch: 351 | Batch: 007 / 011 | Total loss: 1.435 | Reg loss: 0.040 | Tree loss: 1.435 | Accuracy: 0.505500 | 1.738 sec/iter\n",
      "Epoch: 351 | Batch: 008 / 011 | Total loss: 1.377 | Reg loss: 0.040 | Tree loss: 1.377 | Accuracy: 0.513500 | 1.738 sec/iter\n",
      "Epoch: 351 | Batch: 009 / 011 | Total loss: 1.417 | Reg loss: 0.040 | Tree loss: 1.417 | Accuracy: 0.491500 | 1.738 sec/iter\n",
      "Epoch: 351 | Batch: 010 / 011 | Total loss: 1.358 | Reg loss: 0.040 | Tree loss: 1.358 | Accuracy: 0.488055 | 1.738 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 352 | Batch: 000 / 011 | Total loss: 1.608 | Reg loss: 0.040 | Tree loss: 1.608 | Accuracy: 0.395500 | 1.739 sec/iter\n",
      "Epoch: 352 | Batch: 001 / 011 | Total loss: 1.564 | Reg loss: 0.040 | Tree loss: 1.564 | Accuracy: 0.411500 | 1.739 sec/iter\n",
      "Epoch: 352 | Batch: 002 / 011 | Total loss: 1.532 | Reg loss: 0.040 | Tree loss: 1.532 | Accuracy: 0.423000 | 1.739 sec/iter\n",
      "Epoch: 352 | Batch: 003 / 011 | Total loss: 1.471 | Reg loss: 0.040 | Tree loss: 1.471 | Accuracy: 0.479000 | 1.739 sec/iter\n",
      "Epoch: 352 | Batch: 004 / 011 | Total loss: 1.448 | Reg loss: 0.040 | Tree loss: 1.448 | Accuracy: 0.468500 | 1.739 sec/iter\n",
      "Epoch: 352 | Batch: 005 / 011 | Total loss: 1.434 | Reg loss: 0.040 | Tree loss: 1.434 | Accuracy: 0.503000 | 1.739 sec/iter\n",
      "Epoch: 352 | Batch: 006 / 011 | Total loss: 1.418 | Reg loss: 0.040 | Tree loss: 1.418 | Accuracy: 0.503500 | 1.739 sec/iter\n",
      "Epoch: 352 | Batch: 007 / 011 | Total loss: 1.413 | Reg loss: 0.040 | Tree loss: 1.413 | Accuracy: 0.514500 | 1.739 sec/iter\n",
      "Epoch: 352 | Batch: 008 / 011 | Total loss: 1.427 | Reg loss: 0.040 | Tree loss: 1.427 | Accuracy: 0.489000 | 1.739 sec/iter\n",
      "Epoch: 352 | Batch: 009 / 011 | Total loss: 1.413 | Reg loss: 0.040 | Tree loss: 1.413 | Accuracy: 0.508500 | 1.739 sec/iter\n",
      "Epoch: 352 | Batch: 010 / 011 | Total loss: 1.382 | Reg loss: 0.040 | Tree loss: 1.382 | Accuracy: 0.498294 | 1.739 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 353 | Batch: 000 / 011 | Total loss: 1.625 | Reg loss: 0.040 | Tree loss: 1.625 | Accuracy: 0.385000 | 1.74 sec/iter\n",
      "Epoch: 353 | Batch: 001 / 011 | Total loss: 1.563 | Reg loss: 0.040 | Tree loss: 1.563 | Accuracy: 0.430500 | 1.74 sec/iter\n",
      "Epoch: 353 | Batch: 002 / 011 | Total loss: 1.527 | Reg loss: 0.040 | Tree loss: 1.527 | Accuracy: 0.435000 | 1.74 sec/iter\n",
      "Epoch: 353 | Batch: 003 / 011 | Total loss: 1.497 | Reg loss: 0.040 | Tree loss: 1.497 | Accuracy: 0.450000 | 1.74 sec/iter\n",
      "Epoch: 353 | Batch: 004 / 011 | Total loss: 1.440 | Reg loss: 0.040 | Tree loss: 1.440 | Accuracy: 0.481000 | 1.74 sec/iter\n",
      "Epoch: 353 | Batch: 005 / 011 | Total loss: 1.402 | Reg loss: 0.040 | Tree loss: 1.402 | Accuracy: 0.511500 | 1.74 sec/iter\n",
      "Epoch: 353 | Batch: 006 / 011 | Total loss: 1.430 | Reg loss: 0.040 | Tree loss: 1.430 | Accuracy: 0.501500 | 1.74 sec/iter\n",
      "Epoch: 353 | Batch: 007 / 011 | Total loss: 1.426 | Reg loss: 0.040 | Tree loss: 1.426 | Accuracy: 0.512500 | 1.74 sec/iter\n",
      "Epoch: 353 | Batch: 008 / 011 | Total loss: 1.412 | Reg loss: 0.040 | Tree loss: 1.412 | Accuracy: 0.501500 | 1.74 sec/iter\n",
      "Epoch: 353 | Batch: 009 / 011 | Total loss: 1.406 | Reg loss: 0.040 | Tree loss: 1.406 | Accuracy: 0.505000 | 1.74 sec/iter\n",
      "Epoch: 353 | Batch: 010 / 011 | Total loss: 1.395 | Reg loss: 0.040 | Tree loss: 1.395 | Accuracy: 0.467577 | 1.74 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 354 | Batch: 000 / 011 | Total loss: 1.606 | Reg loss: 0.040 | Tree loss: 1.606 | Accuracy: 0.394500 | 1.741 sec/iter\n",
      "Epoch: 354 | Batch: 001 / 011 | Total loss: 1.586 | Reg loss: 0.040 | Tree loss: 1.586 | Accuracy: 0.410000 | 1.741 sec/iter\n",
      "Epoch: 354 | Batch: 002 / 011 | Total loss: 1.525 | Reg loss: 0.040 | Tree loss: 1.525 | Accuracy: 0.429500 | 1.741 sec/iter\n",
      "Epoch: 354 | Batch: 003 / 011 | Total loss: 1.493 | Reg loss: 0.040 | Tree loss: 1.493 | Accuracy: 0.459000 | 1.741 sec/iter\n",
      "Epoch: 354 | Batch: 004 / 011 | Total loss: 1.448 | Reg loss: 0.040 | Tree loss: 1.448 | Accuracy: 0.474500 | 1.741 sec/iter\n",
      "Epoch: 354 | Batch: 005 / 011 | Total loss: 1.428 | Reg loss: 0.040 | Tree loss: 1.428 | Accuracy: 0.488000 | 1.741 sec/iter\n",
      "Epoch: 354 | Batch: 006 / 011 | Total loss: 1.419 | Reg loss: 0.040 | Tree loss: 1.419 | Accuracy: 0.509500 | 1.741 sec/iter\n",
      "Epoch: 354 | Batch: 007 / 011 | Total loss: 1.405 | Reg loss: 0.040 | Tree loss: 1.405 | Accuracy: 0.525500 | 1.741 sec/iter\n",
      "Epoch: 354 | Batch: 008 / 011 | Total loss: 1.412 | Reg loss: 0.040 | Tree loss: 1.412 | Accuracy: 0.502500 | 1.741 sec/iter\n",
      "Epoch: 354 | Batch: 009 / 011 | Total loss: 1.414 | Reg loss: 0.040 | Tree loss: 1.414 | Accuracy: 0.496500 | 1.741 sec/iter\n",
      "Epoch: 354 | Batch: 010 / 011 | Total loss: 1.394 | Reg loss: 0.040 | Tree loss: 1.394 | Accuracy: 0.501706 | 1.741 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 355 | Batch: 000 / 011 | Total loss: 1.616 | Reg loss: 0.040 | Tree loss: 1.616 | Accuracy: 0.396000 | 1.742 sec/iter\n",
      "Epoch: 355 | Batch: 001 / 011 | Total loss: 1.591 | Reg loss: 0.040 | Tree loss: 1.591 | Accuracy: 0.409000 | 1.742 sec/iter\n",
      "Epoch: 355 | Batch: 002 / 011 | Total loss: 1.526 | Reg loss: 0.040 | Tree loss: 1.526 | Accuracy: 0.434500 | 1.742 sec/iter\n",
      "Epoch: 355 | Batch: 003 / 011 | Total loss: 1.480 | Reg loss: 0.040 | Tree loss: 1.480 | Accuracy: 0.457000 | 1.742 sec/iter\n",
      "Epoch: 355 | Batch: 004 / 011 | Total loss: 1.436 | Reg loss: 0.040 | Tree loss: 1.436 | Accuracy: 0.493500 | 1.742 sec/iter\n",
      "Epoch: 355 | Batch: 005 / 011 | Total loss: 1.418 | Reg loss: 0.040 | Tree loss: 1.418 | Accuracy: 0.510000 | 1.742 sec/iter\n",
      "Epoch: 355 | Batch: 006 / 011 | Total loss: 1.439 | Reg loss: 0.040 | Tree loss: 1.439 | Accuracy: 0.488000 | 1.742 sec/iter\n",
      "Epoch: 355 | Batch: 007 / 011 | Total loss: 1.432 | Reg loss: 0.040 | Tree loss: 1.432 | Accuracy: 0.499000 | 1.742 sec/iter\n",
      "Epoch: 355 | Batch: 008 / 011 | Total loss: 1.400 | Reg loss: 0.040 | Tree loss: 1.400 | Accuracy: 0.510000 | 1.742 sec/iter\n",
      "Epoch: 355 | Batch: 009 / 011 | Total loss: 1.396 | Reg loss: 0.040 | Tree loss: 1.396 | Accuracy: 0.494000 | 1.742 sec/iter\n",
      "Epoch: 355 | Batch: 010 / 011 | Total loss: 1.390 | Reg loss: 0.040 | Tree loss: 1.390 | Accuracy: 0.525597 | 1.742 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 356 | Batch: 000 / 011 | Total loss: 1.601 | Reg loss: 0.040 | Tree loss: 1.601 | Accuracy: 0.390000 | 1.742 sec/iter\n",
      "Epoch: 356 | Batch: 001 / 011 | Total loss: 1.559 | Reg loss: 0.040 | Tree loss: 1.559 | Accuracy: 0.424000 | 1.742 sec/iter\n",
      "Epoch: 356 | Batch: 002 / 011 | Total loss: 1.538 | Reg loss: 0.040 | Tree loss: 1.538 | Accuracy: 0.433000 | 1.743 sec/iter\n",
      "Epoch: 356 | Batch: 003 / 011 | Total loss: 1.498 | Reg loss: 0.040 | Tree loss: 1.498 | Accuracy: 0.455500 | 1.743 sec/iter\n",
      "Epoch: 356 | Batch: 004 / 011 | Total loss: 1.451 | Reg loss: 0.040 | Tree loss: 1.451 | Accuracy: 0.473500 | 1.743 sec/iter\n",
      "Epoch: 356 | Batch: 005 / 011 | Total loss: 1.432 | Reg loss: 0.040 | Tree loss: 1.432 | Accuracy: 0.484000 | 1.743 sec/iter\n",
      "Epoch: 356 | Batch: 006 / 011 | Total loss: 1.419 | Reg loss: 0.040 | Tree loss: 1.419 | Accuracy: 0.498000 | 1.743 sec/iter\n",
      "Epoch: 356 | Batch: 007 / 011 | Total loss: 1.414 | Reg loss: 0.040 | Tree loss: 1.414 | Accuracy: 0.499500 | 1.743 sec/iter\n",
      "Epoch: 356 | Batch: 008 / 011 | Total loss: 1.408 | Reg loss: 0.040 | Tree loss: 1.408 | Accuracy: 0.511000 | 1.743 sec/iter\n",
      "Epoch: 356 | Batch: 009 / 011 | Total loss: 1.421 | Reg loss: 0.040 | Tree loss: 1.421 | Accuracy: 0.491000 | 1.743 sec/iter\n",
      "Epoch: 356 | Batch: 010 / 011 | Total loss: 1.410 | Reg loss: 0.040 | Tree loss: 1.410 | Accuracy: 0.484642 | 1.743 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 357 | Batch: 000 / 011 | Total loss: 1.590 | Reg loss: 0.040 | Tree loss: 1.590 | Accuracy: 0.413500 | 1.743 sec/iter\n",
      "Epoch: 357 | Batch: 001 / 011 | Total loss: 1.575 | Reg loss: 0.040 | Tree loss: 1.575 | Accuracy: 0.399500 | 1.743 sec/iter\n",
      "Epoch: 357 | Batch: 002 / 011 | Total loss: 1.540 | Reg loss: 0.040 | Tree loss: 1.540 | Accuracy: 0.431500 | 1.743 sec/iter\n",
      "Epoch: 357 | Batch: 003 / 011 | Total loss: 1.504 | Reg loss: 0.040 | Tree loss: 1.504 | Accuracy: 0.465000 | 1.744 sec/iter\n",
      "Epoch: 357 | Batch: 004 / 011 | Total loss: 1.449 | Reg loss: 0.040 | Tree loss: 1.449 | Accuracy: 0.479000 | 1.744 sec/iter\n",
      "Epoch: 357 | Batch: 005 / 011 | Total loss: 1.442 | Reg loss: 0.040 | Tree loss: 1.442 | Accuracy: 0.480000 | 1.744 sec/iter\n",
      "Epoch: 357 | Batch: 006 / 011 | Total loss: 1.425 | Reg loss: 0.040 | Tree loss: 1.425 | Accuracy: 0.493500 | 1.744 sec/iter\n",
      "Epoch: 357 | Batch: 007 / 011 | Total loss: 1.418 | Reg loss: 0.040 | Tree loss: 1.418 | Accuracy: 0.502500 | 1.744 sec/iter\n",
      "Epoch: 357 | Batch: 008 / 011 | Total loss: 1.403 | Reg loss: 0.040 | Tree loss: 1.403 | Accuracy: 0.503000 | 1.744 sec/iter\n",
      "Epoch: 357 | Batch: 009 / 011 | Total loss: 1.406 | Reg loss: 0.040 | Tree loss: 1.406 | Accuracy: 0.507000 | 1.744 sec/iter\n",
      "Epoch: 357 | Batch: 010 / 011 | Total loss: 1.398 | Reg loss: 0.040 | Tree loss: 1.398 | Accuracy: 0.491468 | 1.743 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 358 | Batch: 000 / 011 | Total loss: 1.624 | Reg loss: 0.040 | Tree loss: 1.624 | Accuracy: 0.377500 | 1.745 sec/iter\n",
      "Epoch: 358 | Batch: 001 / 011 | Total loss: 1.550 | Reg loss: 0.040 | Tree loss: 1.550 | Accuracy: 0.436000 | 1.745 sec/iter\n",
      "Epoch: 358 | Batch: 002 / 011 | Total loss: 1.567 | Reg loss: 0.040 | Tree loss: 1.567 | Accuracy: 0.413000 | 1.745 sec/iter\n",
      "Epoch: 358 | Batch: 003 / 011 | Total loss: 1.486 | Reg loss: 0.040 | Tree loss: 1.486 | Accuracy: 0.457500 | 1.745 sec/iter\n",
      "Epoch: 358 | Batch: 004 / 011 | Total loss: 1.438 | Reg loss: 0.040 | Tree loss: 1.438 | Accuracy: 0.492000 | 1.745 sec/iter\n",
      "Epoch: 358 | Batch: 005 / 011 | Total loss: 1.436 | Reg loss: 0.040 | Tree loss: 1.436 | Accuracy: 0.490000 | 1.745 sec/iter\n",
      "Epoch: 358 | Batch: 006 / 011 | Total loss: 1.424 | Reg loss: 0.040 | Tree loss: 1.424 | Accuracy: 0.504500 | 1.745 sec/iter\n",
      "Epoch: 358 | Batch: 007 / 011 | Total loss: 1.422 | Reg loss: 0.040 | Tree loss: 1.422 | Accuracy: 0.502500 | 1.745 sec/iter\n",
      "Epoch: 358 | Batch: 008 / 011 | Total loss: 1.421 | Reg loss: 0.040 | Tree loss: 1.421 | Accuracy: 0.512000 | 1.745 sec/iter\n",
      "Epoch: 358 | Batch: 009 / 011 | Total loss: 1.368 | Reg loss: 0.040 | Tree loss: 1.368 | Accuracy: 0.511500 | 1.745 sec/iter\n",
      "Epoch: 358 | Batch: 010 / 011 | Total loss: 1.475 | Reg loss: 0.040 | Tree loss: 1.475 | Accuracy: 0.501706 | 1.745 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 359 | Batch: 000 / 011 | Total loss: 1.599 | Reg loss: 0.040 | Tree loss: 1.599 | Accuracy: 0.407000 | 1.746 sec/iter\n",
      "Epoch: 359 | Batch: 001 / 011 | Total loss: 1.581 | Reg loss: 0.040 | Tree loss: 1.581 | Accuracy: 0.419000 | 1.746 sec/iter\n",
      "Epoch: 359 | Batch: 002 / 011 | Total loss: 1.526 | Reg loss: 0.040 | Tree loss: 1.526 | Accuracy: 0.431000 | 1.746 sec/iter\n",
      "Epoch: 359 | Batch: 003 / 011 | Total loss: 1.489 | Reg loss: 0.040 | Tree loss: 1.489 | Accuracy: 0.458500 | 1.746 sec/iter\n",
      "Epoch: 359 | Batch: 004 / 011 | Total loss: 1.489 | Reg loss: 0.040 | Tree loss: 1.489 | Accuracy: 0.461000 | 1.746 sec/iter\n",
      "Epoch: 359 | Batch: 005 / 011 | Total loss: 1.418 | Reg loss: 0.040 | Tree loss: 1.418 | Accuracy: 0.509500 | 1.746 sec/iter\n",
      "Epoch: 359 | Batch: 006 / 011 | Total loss: 1.422 | Reg loss: 0.040 | Tree loss: 1.422 | Accuracy: 0.523000 | 1.746 sec/iter\n",
      "Epoch: 359 | Batch: 007 / 011 | Total loss: 1.433 | Reg loss: 0.040 | Tree loss: 1.433 | Accuracy: 0.524500 | 1.746 sec/iter\n",
      "Epoch: 359 | Batch: 008 / 011 | Total loss: 1.395 | Reg loss: 0.040 | Tree loss: 1.395 | Accuracy: 0.503500 | 1.746 sec/iter\n",
      "Epoch: 359 | Batch: 009 / 011 | Total loss: 1.423 | Reg loss: 0.040 | Tree loss: 1.423 | Accuracy: 0.484500 | 1.746 sec/iter\n",
      "Epoch: 359 | Batch: 010 / 011 | Total loss: 1.329 | Reg loss: 0.040 | Tree loss: 1.329 | Accuracy: 0.511945 | 1.746 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 360 | Batch: 000 / 011 | Total loss: 1.602 | Reg loss: 0.040 | Tree loss: 1.602 | Accuracy: 0.409000 | 1.747 sec/iter\n",
      "Epoch: 360 | Batch: 001 / 011 | Total loss: 1.564 | Reg loss: 0.040 | Tree loss: 1.564 | Accuracy: 0.421000 | 1.747 sec/iter\n",
      "Epoch: 360 | Batch: 002 / 011 | Total loss: 1.537 | Reg loss: 0.040 | Tree loss: 1.537 | Accuracy: 0.436000 | 1.747 sec/iter\n",
      "Epoch: 360 | Batch: 003 / 011 | Total loss: 1.487 | Reg loss: 0.040 | Tree loss: 1.487 | Accuracy: 0.471000 | 1.747 sec/iter\n",
      "Epoch: 360 | Batch: 004 / 011 | Total loss: 1.482 | Reg loss: 0.040 | Tree loss: 1.482 | Accuracy: 0.459000 | 1.747 sec/iter\n",
      "Epoch: 360 | Batch: 005 / 011 | Total loss: 1.427 | Reg loss: 0.040 | Tree loss: 1.427 | Accuracy: 0.503000 | 1.747 sec/iter\n",
      "Epoch: 360 | Batch: 006 / 011 | Total loss: 1.423 | Reg loss: 0.040 | Tree loss: 1.423 | Accuracy: 0.512000 | 1.747 sec/iter\n",
      "Epoch: 360 | Batch: 007 / 011 | Total loss: 1.412 | Reg loss: 0.040 | Tree loss: 1.412 | Accuracy: 0.523500 | 1.747 sec/iter\n",
      "Epoch: 360 | Batch: 008 / 011 | Total loss: 1.386 | Reg loss: 0.040 | Tree loss: 1.386 | Accuracy: 0.521500 | 1.747 sec/iter\n",
      "Epoch: 360 | Batch: 009 / 011 | Total loss: 1.407 | Reg loss: 0.040 | Tree loss: 1.407 | Accuracy: 0.508000 | 1.747 sec/iter\n",
      "Epoch: 360 | Batch: 010 / 011 | Total loss: 1.375 | Reg loss: 0.040 | Tree loss: 1.375 | Accuracy: 0.511945 | 1.747 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 361 | Batch: 000 / 011 | Total loss: 1.613 | Reg loss: 0.040 | Tree loss: 1.613 | Accuracy: 0.405000 | 1.747 sec/iter\n",
      "Epoch: 361 | Batch: 001 / 011 | Total loss: 1.594 | Reg loss: 0.040 | Tree loss: 1.594 | Accuracy: 0.405500 | 1.747 sec/iter\n",
      "Epoch: 361 | Batch: 002 / 011 | Total loss: 1.511 | Reg loss: 0.040 | Tree loss: 1.511 | Accuracy: 0.438500 | 1.747 sec/iter\n",
      "Epoch: 361 | Batch: 003 / 011 | Total loss: 1.494 | Reg loss: 0.040 | Tree loss: 1.494 | Accuracy: 0.469500 | 1.747 sec/iter\n",
      "Epoch: 361 | Batch: 004 / 011 | Total loss: 1.468 | Reg loss: 0.040 | Tree loss: 1.468 | Accuracy: 0.474000 | 1.747 sec/iter\n",
      "Epoch: 361 | Batch: 005 / 011 | Total loss: 1.437 | Reg loss: 0.040 | Tree loss: 1.437 | Accuracy: 0.494500 | 1.747 sec/iter\n",
      "Epoch: 361 | Batch: 006 / 011 | Total loss: 1.400 | Reg loss: 0.040 | Tree loss: 1.400 | Accuracy: 0.520000 | 1.747 sec/iter\n",
      "Epoch: 361 | Batch: 007 / 011 | Total loss: 1.405 | Reg loss: 0.040 | Tree loss: 1.405 | Accuracy: 0.519500 | 1.747 sec/iter\n",
      "Epoch: 361 | Batch: 008 / 011 | Total loss: 1.398 | Reg loss: 0.040 | Tree loss: 1.398 | Accuracy: 0.527000 | 1.747 sec/iter\n",
      "Epoch: 361 | Batch: 009 / 011 | Total loss: 1.406 | Reg loss: 0.040 | Tree loss: 1.406 | Accuracy: 0.496500 | 1.747 sec/iter\n",
      "Epoch: 361 | Batch: 010 / 011 | Total loss: 1.421 | Reg loss: 0.040 | Tree loss: 1.421 | Accuracy: 0.491468 | 1.747 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 362 | Batch: 000 / 011 | Total loss: 1.615 | Reg loss: 0.040 | Tree loss: 1.615 | Accuracy: 0.396500 | 1.748 sec/iter\n",
      "Epoch: 362 | Batch: 001 / 011 | Total loss: 1.543 | Reg loss: 0.040 | Tree loss: 1.543 | Accuracy: 0.432000 | 1.748 sec/iter\n",
      "Epoch: 362 | Batch: 002 / 011 | Total loss: 1.530 | Reg loss: 0.040 | Tree loss: 1.530 | Accuracy: 0.438500 | 1.748 sec/iter\n",
      "Epoch: 362 | Batch: 003 / 011 | Total loss: 1.498 | Reg loss: 0.040 | Tree loss: 1.498 | Accuracy: 0.448000 | 1.748 sec/iter\n",
      "Epoch: 362 | Batch: 004 / 011 | Total loss: 1.467 | Reg loss: 0.040 | Tree loss: 1.467 | Accuracy: 0.495500 | 1.748 sec/iter\n",
      "Epoch: 362 | Batch: 005 / 011 | Total loss: 1.434 | Reg loss: 0.040 | Tree loss: 1.434 | Accuracy: 0.509500 | 1.748 sec/iter\n",
      "Epoch: 362 | Batch: 006 / 011 | Total loss: 1.429 | Reg loss: 0.040 | Tree loss: 1.429 | Accuracy: 0.519000 | 1.748 sec/iter\n",
      "Epoch: 362 | Batch: 007 / 011 | Total loss: 1.420 | Reg loss: 0.040 | Tree loss: 1.420 | Accuracy: 0.513000 | 1.749 sec/iter\n",
      "Epoch: 362 | Batch: 008 / 011 | Total loss: 1.391 | Reg loss: 0.040 | Tree loss: 1.391 | Accuracy: 0.502000 | 1.748 sec/iter\n",
      "Epoch: 362 | Batch: 009 / 011 | Total loss: 1.393 | Reg loss: 0.040 | Tree loss: 1.393 | Accuracy: 0.503500 | 1.749 sec/iter\n",
      "Epoch: 362 | Batch: 010 / 011 | Total loss: 1.459 | Reg loss: 0.040 | Tree loss: 1.459 | Accuracy: 0.494881 | 1.748 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 363 | Batch: 000 / 011 | Total loss: 1.592 | Reg loss: 0.040 | Tree loss: 1.592 | Accuracy: 0.401000 | 1.749 sec/iter\n",
      "Epoch: 363 | Batch: 001 / 011 | Total loss: 1.550 | Reg loss: 0.040 | Tree loss: 1.550 | Accuracy: 0.413500 | 1.749 sec/iter\n",
      "Epoch: 363 | Batch: 002 / 011 | Total loss: 1.537 | Reg loss: 0.040 | Tree loss: 1.537 | Accuracy: 0.440000 | 1.749 sec/iter\n",
      "Epoch: 363 | Batch: 003 / 011 | Total loss: 1.503 | Reg loss: 0.040 | Tree loss: 1.503 | Accuracy: 0.458500 | 1.749 sec/iter\n",
      "Epoch: 363 | Batch: 004 / 011 | Total loss: 1.443 | Reg loss: 0.040 | Tree loss: 1.443 | Accuracy: 0.481000 | 1.749 sec/iter\n",
      "Epoch: 363 | Batch: 005 / 011 | Total loss: 1.441 | Reg loss: 0.040 | Tree loss: 1.441 | Accuracy: 0.493000 | 1.749 sec/iter\n",
      "Epoch: 363 | Batch: 006 / 011 | Total loss: 1.427 | Reg loss: 0.040 | Tree loss: 1.427 | Accuracy: 0.507000 | 1.749 sec/iter\n",
      "Epoch: 363 | Batch: 007 / 011 | Total loss: 1.420 | Reg loss: 0.040 | Tree loss: 1.420 | Accuracy: 0.513000 | 1.749 sec/iter\n",
      "Epoch: 363 | Batch: 008 / 011 | Total loss: 1.403 | Reg loss: 0.040 | Tree loss: 1.403 | Accuracy: 0.510000 | 1.749 sec/iter\n",
      "Epoch: 363 | Batch: 009 / 011 | Total loss: 1.395 | Reg loss: 0.040 | Tree loss: 1.395 | Accuracy: 0.509500 | 1.75 sec/iter\n",
      "Epoch: 363 | Batch: 010 / 011 | Total loss: 1.405 | Reg loss: 0.040 | Tree loss: 1.405 | Accuracy: 0.491468 | 1.75 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 364 | Batch: 000 / 011 | Total loss: 1.617 | Reg loss: 0.040 | Tree loss: 1.617 | Accuracy: 0.391000 | 1.75 sec/iter\n",
      "Epoch: 364 | Batch: 001 / 011 | Total loss: 1.576 | Reg loss: 0.040 | Tree loss: 1.576 | Accuracy: 0.420000 | 1.75 sec/iter\n",
      "Epoch: 364 | Batch: 002 / 011 | Total loss: 1.538 | Reg loss: 0.040 | Tree loss: 1.538 | Accuracy: 0.433000 | 1.751 sec/iter\n",
      "Epoch: 364 | Batch: 003 / 011 | Total loss: 1.489 | Reg loss: 0.040 | Tree loss: 1.489 | Accuracy: 0.439000 | 1.751 sec/iter\n",
      "Epoch: 364 | Batch: 004 / 011 | Total loss: 1.474 | Reg loss: 0.040 | Tree loss: 1.474 | Accuracy: 0.471500 | 1.751 sec/iter\n",
      "Epoch: 364 | Batch: 005 / 011 | Total loss: 1.411 | Reg loss: 0.040 | Tree loss: 1.411 | Accuracy: 0.530500 | 1.75 sec/iter\n",
      "Epoch: 364 | Batch: 006 / 011 | Total loss: 1.413 | Reg loss: 0.040 | Tree loss: 1.413 | Accuracy: 0.528500 | 1.75 sec/iter\n",
      "Epoch: 364 | Batch: 007 / 011 | Total loss: 1.402 | Reg loss: 0.040 | Tree loss: 1.402 | Accuracy: 0.521000 | 1.75 sec/iter\n",
      "Epoch: 364 | Batch: 008 / 011 | Total loss: 1.401 | Reg loss: 0.040 | Tree loss: 1.401 | Accuracy: 0.517500 | 1.75 sec/iter\n",
      "Epoch: 364 | Batch: 009 / 011 | Total loss: 1.395 | Reg loss: 0.040 | Tree loss: 1.395 | Accuracy: 0.509500 | 1.75 sec/iter\n",
      "Epoch: 364 | Batch: 010 / 011 | Total loss: 1.396 | Reg loss: 0.040 | Tree loss: 1.396 | Accuracy: 0.522184 | 1.75 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 365 | Batch: 000 / 011 | Total loss: 1.581 | Reg loss: 0.040 | Tree loss: 1.581 | Accuracy: 0.433000 | 1.752 sec/iter\n",
      "Epoch: 365 | Batch: 001 / 011 | Total loss: 1.589 | Reg loss: 0.040 | Tree loss: 1.589 | Accuracy: 0.404500 | 1.752 sec/iter\n",
      "Epoch: 365 | Batch: 002 / 011 | Total loss: 1.541 | Reg loss: 0.040 | Tree loss: 1.541 | Accuracy: 0.418000 | 1.752 sec/iter\n",
      "Epoch: 365 | Batch: 003 / 011 | Total loss: 1.495 | Reg loss: 0.040 | Tree loss: 1.495 | Accuracy: 0.446000 | 1.752 sec/iter\n",
      "Epoch: 365 | Batch: 004 / 011 | Total loss: 1.470 | Reg loss: 0.040 | Tree loss: 1.470 | Accuracy: 0.469500 | 1.752 sec/iter\n",
      "Epoch: 365 | Batch: 005 / 011 | Total loss: 1.409 | Reg loss: 0.040 | Tree loss: 1.409 | Accuracy: 0.505500 | 1.752 sec/iter\n",
      "Epoch: 365 | Batch: 006 / 011 | Total loss: 1.422 | Reg loss: 0.040 | Tree loss: 1.422 | Accuracy: 0.519500 | 1.752 sec/iter\n",
      "Epoch: 365 | Batch: 007 / 011 | Total loss: 1.412 | Reg loss: 0.040 | Tree loss: 1.412 | Accuracy: 0.521500 | 1.752 sec/iter\n",
      "Epoch: 365 | Batch: 008 / 011 | Total loss: 1.423 | Reg loss: 0.040 | Tree loss: 1.423 | Accuracy: 0.478000 | 1.752 sec/iter\n",
      "Epoch: 365 | Batch: 009 / 011 | Total loss: 1.402 | Reg loss: 0.040 | Tree loss: 1.402 | Accuracy: 0.491500 | 1.752 sec/iter\n",
      "Epoch: 365 | Batch: 010 / 011 | Total loss: 1.345 | Reg loss: 0.040 | Tree loss: 1.345 | Accuracy: 0.522184 | 1.752 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 366 | Batch: 000 / 011 | Total loss: 1.626 | Reg loss: 0.040 | Tree loss: 1.626 | Accuracy: 0.397500 | 1.753 sec/iter\n",
      "Epoch: 366 | Batch: 001 / 011 | Total loss: 1.553 | Reg loss: 0.040 | Tree loss: 1.553 | Accuracy: 0.424000 | 1.753 sec/iter\n",
      "Epoch: 366 | Batch: 002 / 011 | Total loss: 1.535 | Reg loss: 0.040 | Tree loss: 1.535 | Accuracy: 0.433000 | 1.753 sec/iter\n",
      "Epoch: 366 | Batch: 003 / 011 | Total loss: 1.480 | Reg loss: 0.040 | Tree loss: 1.480 | Accuracy: 0.458000 | 1.753 sec/iter\n",
      "Epoch: 366 | Batch: 004 / 011 | Total loss: 1.465 | Reg loss: 0.040 | Tree loss: 1.465 | Accuracy: 0.467000 | 1.753 sec/iter\n",
      "Epoch: 366 | Batch: 005 / 011 | Total loss: 1.424 | Reg loss: 0.040 | Tree loss: 1.424 | Accuracy: 0.493000 | 1.752 sec/iter\n",
      "Epoch: 366 | Batch: 006 / 011 | Total loss: 1.413 | Reg loss: 0.040 | Tree loss: 1.413 | Accuracy: 0.514500 | 1.752 sec/iter\n",
      "Epoch: 366 | Batch: 007 / 011 | Total loss: 1.412 | Reg loss: 0.040 | Tree loss: 1.412 | Accuracy: 0.515000 | 1.752 sec/iter\n",
      "Epoch: 366 | Batch: 008 / 011 | Total loss: 1.421 | Reg loss: 0.040 | Tree loss: 1.421 | Accuracy: 0.505000 | 1.752 sec/iter\n",
      "Epoch: 366 | Batch: 009 / 011 | Total loss: 1.401 | Reg loss: 0.040 | Tree loss: 1.401 | Accuracy: 0.515500 | 1.752 sec/iter\n",
      "Epoch: 366 | Batch: 010 / 011 | Total loss: 1.430 | Reg loss: 0.040 | Tree loss: 1.430 | Accuracy: 0.515358 | 1.752 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 367 | Batch: 000 / 011 | Total loss: 1.618 | Reg loss: 0.040 | Tree loss: 1.618 | Accuracy: 0.392500 | 1.753 sec/iter\n",
      "Epoch: 367 | Batch: 001 / 011 | Total loss: 1.560 | Reg loss: 0.040 | Tree loss: 1.560 | Accuracy: 0.403500 | 1.753 sec/iter\n",
      "Epoch: 367 | Batch: 002 / 011 | Total loss: 1.523 | Reg loss: 0.040 | Tree loss: 1.523 | Accuracy: 0.454500 | 1.753 sec/iter\n",
      "Epoch: 367 | Batch: 003 / 011 | Total loss: 1.505 | Reg loss: 0.040 | Tree loss: 1.505 | Accuracy: 0.446000 | 1.753 sec/iter\n",
      "Epoch: 367 | Batch: 004 / 011 | Total loss: 1.444 | Reg loss: 0.040 | Tree loss: 1.444 | Accuracy: 0.497500 | 1.753 sec/iter\n",
      "Epoch: 367 | Batch: 005 / 011 | Total loss: 1.466 | Reg loss: 0.040 | Tree loss: 1.466 | Accuracy: 0.491000 | 1.753 sec/iter\n",
      "Epoch: 367 | Batch: 006 / 011 | Total loss: 1.426 | Reg loss: 0.040 | Tree loss: 1.426 | Accuracy: 0.486000 | 1.753 sec/iter\n",
      "Epoch: 367 | Batch: 007 / 011 | Total loss: 1.408 | Reg loss: 0.040 | Tree loss: 1.408 | Accuracy: 0.515500 | 1.753 sec/iter\n",
      "Epoch: 367 | Batch: 008 / 011 | Total loss: 1.393 | Reg loss: 0.040 | Tree loss: 1.393 | Accuracy: 0.519500 | 1.753 sec/iter\n",
      "Epoch: 367 | Batch: 009 / 011 | Total loss: 1.393 | Reg loss: 0.040 | Tree loss: 1.393 | Accuracy: 0.510500 | 1.753 sec/iter\n",
      "Epoch: 367 | Batch: 010 / 011 | Total loss: 1.372 | Reg loss: 0.040 | Tree loss: 1.372 | Accuracy: 0.488055 | 1.753 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 368 | Batch: 000 / 011 | Total loss: 1.628 | Reg loss: 0.040 | Tree loss: 1.628 | Accuracy: 0.395000 | 1.754 sec/iter\n",
      "Epoch: 368 | Batch: 001 / 011 | Total loss: 1.564 | Reg loss: 0.040 | Tree loss: 1.564 | Accuracy: 0.411500 | 1.754 sec/iter\n",
      "Epoch: 368 | Batch: 002 / 011 | Total loss: 1.541 | Reg loss: 0.040 | Tree loss: 1.541 | Accuracy: 0.438000 | 1.754 sec/iter\n",
      "Epoch: 368 | Batch: 003 / 011 | Total loss: 1.487 | Reg loss: 0.040 | Tree loss: 1.487 | Accuracy: 0.456500 | 1.754 sec/iter\n",
      "Epoch: 368 | Batch: 004 / 011 | Total loss: 1.463 | Reg loss: 0.040 | Tree loss: 1.463 | Accuracy: 0.481000 | 1.754 sec/iter\n",
      "Epoch: 368 | Batch: 005 / 011 | Total loss: 1.420 | Reg loss: 0.040 | Tree loss: 1.420 | Accuracy: 0.532500 | 1.754 sec/iter\n",
      "Epoch: 368 | Batch: 006 / 011 | Total loss: 1.422 | Reg loss: 0.040 | Tree loss: 1.422 | Accuracy: 0.515500 | 1.754 sec/iter\n",
      "Epoch: 368 | Batch: 007 / 011 | Total loss: 1.405 | Reg loss: 0.040 | Tree loss: 1.405 | Accuracy: 0.540000 | 1.754 sec/iter\n",
      "Epoch: 368 | Batch: 008 / 011 | Total loss: 1.416 | Reg loss: 0.040 | Tree loss: 1.416 | Accuracy: 0.512000 | 1.754 sec/iter\n",
      "Epoch: 368 | Batch: 009 / 011 | Total loss: 1.394 | Reg loss: 0.040 | Tree loss: 1.394 | Accuracy: 0.498000 | 1.754 sec/iter\n",
      "Epoch: 368 | Batch: 010 / 011 | Total loss: 1.406 | Reg loss: 0.040 | Tree loss: 1.406 | Accuracy: 0.549488 | 1.754 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 369 | Batch: 000 / 011 | Total loss: 1.623 | Reg loss: 0.040 | Tree loss: 1.623 | Accuracy: 0.393500 | 1.755 sec/iter\n",
      "Epoch: 369 | Batch: 001 / 011 | Total loss: 1.582 | Reg loss: 0.040 | Tree loss: 1.582 | Accuracy: 0.405000 | 1.755 sec/iter\n",
      "Epoch: 369 | Batch: 002 / 011 | Total loss: 1.538 | Reg loss: 0.040 | Tree loss: 1.538 | Accuracy: 0.436000 | 1.755 sec/iter\n",
      "Epoch: 369 | Batch: 003 / 011 | Total loss: 1.467 | Reg loss: 0.040 | Tree loss: 1.467 | Accuracy: 0.465500 | 1.755 sec/iter\n",
      "Epoch: 369 | Batch: 004 / 011 | Total loss: 1.469 | Reg loss: 0.040 | Tree loss: 1.469 | Accuracy: 0.455500 | 1.755 sec/iter\n",
      "Epoch: 369 | Batch: 005 / 011 | Total loss: 1.445 | Reg loss: 0.040 | Tree loss: 1.445 | Accuracy: 0.489500 | 1.755 sec/iter\n",
      "Epoch: 369 | Batch: 006 / 011 | Total loss: 1.398 | Reg loss: 0.040 | Tree loss: 1.398 | Accuracy: 0.529000 | 1.755 sec/iter\n",
      "Epoch: 369 | Batch: 007 / 011 | Total loss: 1.404 | Reg loss: 0.040 | Tree loss: 1.404 | Accuracy: 0.523000 | 1.755 sec/iter\n",
      "Epoch: 369 | Batch: 008 / 011 | Total loss: 1.424 | Reg loss: 0.040 | Tree loss: 1.424 | Accuracy: 0.495500 | 1.754 sec/iter\n",
      "Epoch: 369 | Batch: 009 / 011 | Total loss: 1.391 | Reg loss: 0.040 | Tree loss: 1.391 | Accuracy: 0.508000 | 1.754 sec/iter\n",
      "Epoch: 369 | Batch: 010 / 011 | Total loss: 1.407 | Reg loss: 0.040 | Tree loss: 1.407 | Accuracy: 0.470990 | 1.754 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 370 | Batch: 000 / 011 | Total loss: 1.603 | Reg loss: 0.040 | Tree loss: 1.603 | Accuracy: 0.388500 | 1.754 sec/iter\n",
      "Epoch: 370 | Batch: 001 / 011 | Total loss: 1.564 | Reg loss: 0.040 | Tree loss: 1.564 | Accuracy: 0.421500 | 1.754 sec/iter\n",
      "Epoch: 370 | Batch: 002 / 011 | Total loss: 1.548 | Reg loss: 0.040 | Tree loss: 1.548 | Accuracy: 0.427000 | 1.754 sec/iter\n",
      "Epoch: 370 | Batch: 003 / 011 | Total loss: 1.487 | Reg loss: 0.040 | Tree loss: 1.487 | Accuracy: 0.469000 | 1.754 sec/iter\n",
      "Epoch: 370 | Batch: 004 / 011 | Total loss: 1.465 | Reg loss: 0.040 | Tree loss: 1.465 | Accuracy: 0.477500 | 1.754 sec/iter\n",
      "Epoch: 370 | Batch: 005 / 011 | Total loss: 1.417 | Reg loss: 0.040 | Tree loss: 1.417 | Accuracy: 0.532000 | 1.754 sec/iter\n",
      "Epoch: 370 | Batch: 006 / 011 | Total loss: 1.410 | Reg loss: 0.040 | Tree loss: 1.410 | Accuracy: 0.524500 | 1.754 sec/iter\n",
      "Epoch: 370 | Batch: 007 / 011 | Total loss: 1.417 | Reg loss: 0.040 | Tree loss: 1.417 | Accuracy: 0.521500 | 1.753 sec/iter\n",
      "Epoch: 370 | Batch: 008 / 011 | Total loss: 1.429 | Reg loss: 0.040 | Tree loss: 1.429 | Accuracy: 0.492500 | 1.753 sec/iter\n",
      "Epoch: 370 | Batch: 009 / 011 | Total loss: 1.403 | Reg loss: 0.040 | Tree loss: 1.403 | Accuracy: 0.492000 | 1.753 sec/iter\n",
      "Epoch: 370 | Batch: 010 / 011 | Total loss: 1.391 | Reg loss: 0.040 | Tree loss: 1.391 | Accuracy: 0.505119 | 1.753 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 371 | Batch: 000 / 011 | Total loss: 1.595 | Reg loss: 0.040 | Tree loss: 1.595 | Accuracy: 0.425000 | 1.754 sec/iter\n",
      "Epoch: 371 | Batch: 001 / 011 | Total loss: 1.586 | Reg loss: 0.040 | Tree loss: 1.586 | Accuracy: 0.402000 | 1.754 sec/iter\n",
      "Epoch: 371 | Batch: 002 / 011 | Total loss: 1.547 | Reg loss: 0.040 | Tree loss: 1.547 | Accuracy: 0.424500 | 1.754 sec/iter\n",
      "Epoch: 371 | Batch: 003 / 011 | Total loss: 1.497 | Reg loss: 0.040 | Tree loss: 1.497 | Accuracy: 0.462500 | 1.754 sec/iter\n",
      "Epoch: 371 | Batch: 004 / 011 | Total loss: 1.458 | Reg loss: 0.040 | Tree loss: 1.458 | Accuracy: 0.477500 | 1.754 sec/iter\n",
      "Epoch: 371 | Batch: 005 / 011 | Total loss: 1.411 | Reg loss: 0.040 | Tree loss: 1.411 | Accuracy: 0.484500 | 1.754 sec/iter\n",
      "Epoch: 371 | Batch: 006 / 011 | Total loss: 1.425 | Reg loss: 0.040 | Tree loss: 1.425 | Accuracy: 0.503500 | 1.754 sec/iter\n",
      "Epoch: 371 | Batch: 007 / 011 | Total loss: 1.388 | Reg loss: 0.040 | Tree loss: 1.388 | Accuracy: 0.523500 | 1.754 sec/iter\n",
      "Epoch: 371 | Batch: 008 / 011 | Total loss: 1.425 | Reg loss: 0.040 | Tree loss: 1.425 | Accuracy: 0.498500 | 1.754 sec/iter\n",
      "Epoch: 371 | Batch: 009 / 011 | Total loss: 1.413 | Reg loss: 0.040 | Tree loss: 1.413 | Accuracy: 0.508000 | 1.754 sec/iter\n",
      "Epoch: 371 | Batch: 010 / 011 | Total loss: 1.409 | Reg loss: 0.040 | Tree loss: 1.409 | Accuracy: 0.515358 | 1.753 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 372 | Batch: 000 / 011 | Total loss: 1.610 | Reg loss: 0.040 | Tree loss: 1.610 | Accuracy: 0.403000 | 1.753 sec/iter\n",
      "Epoch: 372 | Batch: 001 / 011 | Total loss: 1.598 | Reg loss: 0.040 | Tree loss: 1.598 | Accuracy: 0.385000 | 1.753 sec/iter\n",
      "Epoch: 372 | Batch: 002 / 011 | Total loss: 1.514 | Reg loss: 0.040 | Tree loss: 1.514 | Accuracy: 0.435000 | 1.753 sec/iter\n",
      "Epoch: 372 | Batch: 003 / 011 | Total loss: 1.476 | Reg loss: 0.040 | Tree loss: 1.476 | Accuracy: 0.462500 | 1.753 sec/iter\n",
      "Epoch: 372 | Batch: 004 / 011 | Total loss: 1.461 | Reg loss: 0.040 | Tree loss: 1.461 | Accuracy: 0.480500 | 1.753 sec/iter\n",
      "Epoch: 372 | Batch: 005 / 011 | Total loss: 1.444 | Reg loss: 0.040 | Tree loss: 1.444 | Accuracy: 0.498000 | 1.753 sec/iter\n",
      "Epoch: 372 | Batch: 006 / 011 | Total loss: 1.411 | Reg loss: 0.040 | Tree loss: 1.411 | Accuracy: 0.516500 | 1.753 sec/iter\n",
      "Epoch: 372 | Batch: 007 / 011 | Total loss: 1.391 | Reg loss: 0.040 | Tree loss: 1.391 | Accuracy: 0.539000 | 1.753 sec/iter\n",
      "Epoch: 372 | Batch: 008 / 011 | Total loss: 1.410 | Reg loss: 0.040 | Tree loss: 1.410 | Accuracy: 0.503000 | 1.753 sec/iter\n",
      "Epoch: 372 | Batch: 009 / 011 | Total loss: 1.413 | Reg loss: 0.040 | Tree loss: 1.413 | Accuracy: 0.498500 | 1.752 sec/iter\n",
      "Epoch: 372 | Batch: 010 / 011 | Total loss: 1.365 | Reg loss: 0.040 | Tree loss: 1.365 | Accuracy: 0.529010 | 1.752 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 373 | Batch: 000 / 011 | Total loss: 1.608 | Reg loss: 0.040 | Tree loss: 1.608 | Accuracy: 0.387000 | 1.752 sec/iter\n",
      "Epoch: 373 | Batch: 001 / 011 | Total loss: 1.579 | Reg loss: 0.040 | Tree loss: 1.579 | Accuracy: 0.394000 | 1.752 sec/iter\n",
      "Epoch: 373 | Batch: 002 / 011 | Total loss: 1.567 | Reg loss: 0.040 | Tree loss: 1.567 | Accuracy: 0.419500 | 1.752 sec/iter\n",
      "Epoch: 373 | Batch: 003 / 011 | Total loss: 1.475 | Reg loss: 0.040 | Tree loss: 1.475 | Accuracy: 0.478000 | 1.752 sec/iter\n",
      "Epoch: 373 | Batch: 004 / 011 | Total loss: 1.472 | Reg loss: 0.040 | Tree loss: 1.472 | Accuracy: 0.471000 | 1.752 sec/iter\n",
      "Epoch: 373 | Batch: 005 / 011 | Total loss: 1.427 | Reg loss: 0.040 | Tree loss: 1.427 | Accuracy: 0.492500 | 1.751 sec/iter\n",
      "Epoch: 373 | Batch: 006 / 011 | Total loss: 1.422 | Reg loss: 0.040 | Tree loss: 1.422 | Accuracy: 0.513000 | 1.751 sec/iter\n",
      "Epoch: 373 | Batch: 007 / 011 | Total loss: 1.396 | Reg loss: 0.040 | Tree loss: 1.396 | Accuracy: 0.535500 | 1.751 sec/iter\n",
      "Epoch: 373 | Batch: 008 / 011 | Total loss: 1.408 | Reg loss: 0.040 | Tree loss: 1.408 | Accuracy: 0.493500 | 1.751 sec/iter\n",
      "Epoch: 373 | Batch: 009 / 011 | Total loss: 1.387 | Reg loss: 0.040 | Tree loss: 1.387 | Accuracy: 0.507500 | 1.751 sec/iter\n",
      "Epoch: 373 | Batch: 010 / 011 | Total loss: 1.400 | Reg loss: 0.040 | Tree loss: 1.400 | Accuracy: 0.484642 | 1.751 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 374 | Batch: 000 / 011 | Total loss: 1.612 | Reg loss: 0.040 | Tree loss: 1.612 | Accuracy: 0.396000 | 1.751 sec/iter\n",
      "Epoch: 374 | Batch: 001 / 011 | Total loss: 1.561 | Reg loss: 0.040 | Tree loss: 1.561 | Accuracy: 0.406000 | 1.751 sec/iter\n",
      "Epoch: 374 | Batch: 002 / 011 | Total loss: 1.546 | Reg loss: 0.040 | Tree loss: 1.546 | Accuracy: 0.427000 | 1.751 sec/iter\n",
      "Epoch: 374 | Batch: 003 / 011 | Total loss: 1.488 | Reg loss: 0.040 | Tree loss: 1.488 | Accuracy: 0.449000 | 1.751 sec/iter\n",
      "Epoch: 374 | Batch: 004 / 011 | Total loss: 1.452 | Reg loss: 0.040 | Tree loss: 1.452 | Accuracy: 0.483000 | 1.751 sec/iter\n",
      "Epoch: 374 | Batch: 005 / 011 | Total loss: 1.440 | Reg loss: 0.040 | Tree loss: 1.440 | Accuracy: 0.481500 | 1.751 sec/iter\n",
      "Epoch: 374 | Batch: 006 / 011 | Total loss: 1.406 | Reg loss: 0.040 | Tree loss: 1.406 | Accuracy: 0.530000 | 1.751 sec/iter\n",
      "Epoch: 374 | Batch: 007 / 011 | Total loss: 1.418 | Reg loss: 0.040 | Tree loss: 1.418 | Accuracy: 0.509000 | 1.751 sec/iter\n",
      "Epoch: 374 | Batch: 008 / 011 | Total loss: 1.394 | Reg loss: 0.040 | Tree loss: 1.394 | Accuracy: 0.513000 | 1.751 sec/iter\n",
      "Epoch: 374 | Batch: 009 / 011 | Total loss: 1.404 | Reg loss: 0.040 | Tree loss: 1.404 | Accuracy: 0.488500 | 1.751 sec/iter\n",
      "Epoch: 374 | Batch: 010 / 011 | Total loss: 1.405 | Reg loss: 0.040 | Tree loss: 1.405 | Accuracy: 0.498294 | 1.751 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 375 | Batch: 000 / 011 | Total loss: 1.615 | Reg loss: 0.040 | Tree loss: 1.615 | Accuracy: 0.399500 | 1.752 sec/iter\n",
      "Epoch: 375 | Batch: 001 / 011 | Total loss: 1.592 | Reg loss: 0.040 | Tree loss: 1.592 | Accuracy: 0.404000 | 1.752 sec/iter\n",
      "Epoch: 375 | Batch: 002 / 011 | Total loss: 1.516 | Reg loss: 0.040 | Tree loss: 1.516 | Accuracy: 0.441500 | 1.752 sec/iter\n",
      "Epoch: 375 | Batch: 003 / 011 | Total loss: 1.483 | Reg loss: 0.040 | Tree loss: 1.483 | Accuracy: 0.464000 | 1.752 sec/iter\n",
      "Epoch: 375 | Batch: 004 / 011 | Total loss: 1.464 | Reg loss: 0.040 | Tree loss: 1.464 | Accuracy: 0.468500 | 1.752 sec/iter\n",
      "Epoch: 375 | Batch: 005 / 011 | Total loss: 1.438 | Reg loss: 0.040 | Tree loss: 1.438 | Accuracy: 0.471500 | 1.752 sec/iter\n",
      "Epoch: 375 | Batch: 006 / 011 | Total loss: 1.407 | Reg loss: 0.040 | Tree loss: 1.407 | Accuracy: 0.509000 | 1.752 sec/iter\n",
      "Epoch: 375 | Batch: 007 / 011 | Total loss: 1.428 | Reg loss: 0.040 | Tree loss: 1.428 | Accuracy: 0.521500 | 1.752 sec/iter\n",
      "Epoch: 375 | Batch: 008 / 011 | Total loss: 1.403 | Reg loss: 0.040 | Tree loss: 1.403 | Accuracy: 0.516500 | 1.752 sec/iter\n",
      "Epoch: 375 | Batch: 009 / 011 | Total loss: 1.399 | Reg loss: 0.040 | Tree loss: 1.399 | Accuracy: 0.500500 | 1.752 sec/iter\n",
      "Epoch: 375 | Batch: 010 / 011 | Total loss: 1.378 | Reg loss: 0.040 | Tree loss: 1.378 | Accuracy: 0.467577 | 1.752 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 376 | Batch: 000 / 011 | Total loss: 1.613 | Reg loss: 0.040 | Tree loss: 1.613 | Accuracy: 0.384500 | 1.753 sec/iter\n",
      "Epoch: 376 | Batch: 001 / 011 | Total loss: 1.569 | Reg loss: 0.040 | Tree loss: 1.569 | Accuracy: 0.420500 | 1.753 sec/iter\n",
      "Epoch: 376 | Batch: 002 / 011 | Total loss: 1.523 | Reg loss: 0.040 | Tree loss: 1.523 | Accuracy: 0.442000 | 1.753 sec/iter\n",
      "Epoch: 376 | Batch: 003 / 011 | Total loss: 1.499 | Reg loss: 0.040 | Tree loss: 1.499 | Accuracy: 0.453000 | 1.753 sec/iter\n",
      "Epoch: 376 | Batch: 004 / 011 | Total loss: 1.452 | Reg loss: 0.040 | Tree loss: 1.452 | Accuracy: 0.480000 | 1.753 sec/iter\n",
      "Epoch: 376 | Batch: 005 / 011 | Total loss: 1.437 | Reg loss: 0.040 | Tree loss: 1.437 | Accuracy: 0.499500 | 1.753 sec/iter\n",
      "Epoch: 376 | Batch: 006 / 011 | Total loss: 1.411 | Reg loss: 0.040 | Tree loss: 1.411 | Accuracy: 0.517500 | 1.753 sec/iter\n",
      "Epoch: 376 | Batch: 007 / 011 | Total loss: 1.393 | Reg loss: 0.040 | Tree loss: 1.393 | Accuracy: 0.535500 | 1.753 sec/iter\n",
      "Epoch: 376 | Batch: 008 / 011 | Total loss: 1.436 | Reg loss: 0.040 | Tree loss: 1.436 | Accuracy: 0.474500 | 1.753 sec/iter\n",
      "Epoch: 376 | Batch: 009 / 011 | Total loss: 1.413 | Reg loss: 0.040 | Tree loss: 1.413 | Accuracy: 0.484000 | 1.753 sec/iter\n",
      "Epoch: 376 | Batch: 010 / 011 | Total loss: 1.441 | Reg loss: 0.040 | Tree loss: 1.441 | Accuracy: 0.450512 | 1.753 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 377 | Batch: 000 / 011 | Total loss: 1.613 | Reg loss: 0.040 | Tree loss: 1.613 | Accuracy: 0.411500 | 1.754 sec/iter\n",
      "Epoch: 377 | Batch: 001 / 011 | Total loss: 1.548 | Reg loss: 0.040 | Tree loss: 1.548 | Accuracy: 0.417000 | 1.754 sec/iter\n",
      "Epoch: 377 | Batch: 002 / 011 | Total loss: 1.515 | Reg loss: 0.040 | Tree loss: 1.515 | Accuracy: 0.437000 | 1.754 sec/iter\n",
      "Epoch: 377 | Batch: 003 / 011 | Total loss: 1.509 | Reg loss: 0.040 | Tree loss: 1.509 | Accuracy: 0.452000 | 1.754 sec/iter\n",
      "Epoch: 377 | Batch: 004 / 011 | Total loss: 1.461 | Reg loss: 0.040 | Tree loss: 1.461 | Accuracy: 0.478500 | 1.754 sec/iter\n",
      "Epoch: 377 | Batch: 005 / 011 | Total loss: 1.436 | Reg loss: 0.040 | Tree loss: 1.436 | Accuracy: 0.491000 | 1.754 sec/iter\n",
      "Epoch: 377 | Batch: 006 / 011 | Total loss: 1.429 | Reg loss: 0.040 | Tree loss: 1.429 | Accuracy: 0.499000 | 1.754 sec/iter\n",
      "Epoch: 377 | Batch: 007 / 011 | Total loss: 1.396 | Reg loss: 0.040 | Tree loss: 1.396 | Accuracy: 0.522000 | 1.754 sec/iter\n",
      "Epoch: 377 | Batch: 008 / 011 | Total loss: 1.421 | Reg loss: 0.040 | Tree loss: 1.421 | Accuracy: 0.509500 | 1.754 sec/iter\n",
      "Epoch: 377 | Batch: 009 / 011 | Total loss: 1.415 | Reg loss: 0.040 | Tree loss: 1.415 | Accuracy: 0.497000 | 1.754 sec/iter\n",
      "Epoch: 377 | Batch: 010 / 011 | Total loss: 1.403 | Reg loss: 0.040 | Tree loss: 1.403 | Accuracy: 0.460751 | 1.754 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 378 | Batch: 000 / 011 | Total loss: 1.614 | Reg loss: 0.040 | Tree loss: 1.614 | Accuracy: 0.402500 | 1.755 sec/iter\n",
      "Epoch: 378 | Batch: 001 / 011 | Total loss: 1.556 | Reg loss: 0.040 | Tree loss: 1.556 | Accuracy: 0.418500 | 1.755 sec/iter\n",
      "Epoch: 378 | Batch: 002 / 011 | Total loss: 1.544 | Reg loss: 0.040 | Tree loss: 1.544 | Accuracy: 0.434000 | 1.755 sec/iter\n",
      "Epoch: 378 | Batch: 003 / 011 | Total loss: 1.502 | Reg loss: 0.040 | Tree loss: 1.502 | Accuracy: 0.457000 | 1.755 sec/iter\n",
      "Epoch: 378 | Batch: 004 / 011 | Total loss: 1.460 | Reg loss: 0.040 | Tree loss: 1.460 | Accuracy: 0.472000 | 1.755 sec/iter\n",
      "Epoch: 378 | Batch: 005 / 011 | Total loss: 1.420 | Reg loss: 0.040 | Tree loss: 1.420 | Accuracy: 0.527500 | 1.755 sec/iter\n",
      "Epoch: 378 | Batch: 006 / 011 | Total loss: 1.430 | Reg loss: 0.040 | Tree loss: 1.430 | Accuracy: 0.508500 | 1.755 sec/iter\n",
      "Epoch: 378 | Batch: 007 / 011 | Total loss: 1.391 | Reg loss: 0.040 | Tree loss: 1.391 | Accuracy: 0.510500 | 1.755 sec/iter\n",
      "Epoch: 378 | Batch: 008 / 011 | Total loss: 1.408 | Reg loss: 0.040 | Tree loss: 1.408 | Accuracy: 0.496000 | 1.755 sec/iter\n",
      "Epoch: 378 | Batch: 009 / 011 | Total loss: 1.407 | Reg loss: 0.040 | Tree loss: 1.407 | Accuracy: 0.501500 | 1.755 sec/iter\n",
      "Epoch: 378 | Batch: 010 / 011 | Total loss: 1.400 | Reg loss: 0.040 | Tree loss: 1.400 | Accuracy: 0.511945 | 1.755 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 379 | Batch: 000 / 011 | Total loss: 1.599 | Reg loss: 0.040 | Tree loss: 1.599 | Accuracy: 0.407500 | 1.755 sec/iter\n",
      "Epoch: 379 | Batch: 001 / 011 | Total loss: 1.556 | Reg loss: 0.040 | Tree loss: 1.556 | Accuracy: 0.419000 | 1.755 sec/iter\n",
      "Epoch: 379 | Batch: 002 / 011 | Total loss: 1.554 | Reg loss: 0.040 | Tree loss: 1.554 | Accuracy: 0.429000 | 1.755 sec/iter\n",
      "Epoch: 379 | Batch: 003 / 011 | Total loss: 1.496 | Reg loss: 0.040 | Tree loss: 1.496 | Accuracy: 0.465500 | 1.755 sec/iter\n",
      "Epoch: 379 | Batch: 004 / 011 | Total loss: 1.458 | Reg loss: 0.040 | Tree loss: 1.458 | Accuracy: 0.467500 | 1.755 sec/iter\n",
      "Epoch: 379 | Batch: 005 / 011 | Total loss: 1.441 | Reg loss: 0.040 | Tree loss: 1.441 | Accuracy: 0.494000 | 1.755 sec/iter\n",
      "Epoch: 379 | Batch: 006 / 011 | Total loss: 1.412 | Reg loss: 0.040 | Tree loss: 1.412 | Accuracy: 0.531000 | 1.755 sec/iter\n",
      "Epoch: 379 | Batch: 007 / 011 | Total loss: 1.405 | Reg loss: 0.040 | Tree loss: 1.405 | Accuracy: 0.521500 | 1.755 sec/iter\n",
      "Epoch: 379 | Batch: 008 / 011 | Total loss: 1.404 | Reg loss: 0.040 | Tree loss: 1.404 | Accuracy: 0.505500 | 1.755 sec/iter\n",
      "Epoch: 379 | Batch: 009 / 011 | Total loss: 1.413 | Reg loss: 0.040 | Tree loss: 1.413 | Accuracy: 0.496500 | 1.755 sec/iter\n",
      "Epoch: 379 | Batch: 010 / 011 | Total loss: 1.399 | Reg loss: 0.040 | Tree loss: 1.399 | Accuracy: 0.498294 | 1.755 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 380 | Batch: 000 / 011 | Total loss: 1.588 | Reg loss: 0.040 | Tree loss: 1.588 | Accuracy: 0.424500 | 1.756 sec/iter\n",
      "Epoch: 380 | Batch: 001 / 011 | Total loss: 1.567 | Reg loss: 0.040 | Tree loss: 1.567 | Accuracy: 0.419000 | 1.756 sec/iter\n",
      "Epoch: 380 | Batch: 002 / 011 | Total loss: 1.547 | Reg loss: 0.040 | Tree loss: 1.547 | Accuracy: 0.429000 | 1.756 sec/iter\n",
      "Epoch: 380 | Batch: 003 / 011 | Total loss: 1.495 | Reg loss: 0.040 | Tree loss: 1.495 | Accuracy: 0.447000 | 1.756 sec/iter\n",
      "Epoch: 380 | Batch: 004 / 011 | Total loss: 1.451 | Reg loss: 0.040 | Tree loss: 1.451 | Accuracy: 0.490000 | 1.756 sec/iter\n",
      "Epoch: 380 | Batch: 005 / 011 | Total loss: 1.429 | Reg loss: 0.040 | Tree loss: 1.429 | Accuracy: 0.500000 | 1.756 sec/iter\n",
      "Epoch: 380 | Batch: 006 / 011 | Total loss: 1.423 | Reg loss: 0.040 | Tree loss: 1.423 | Accuracy: 0.518000 | 1.756 sec/iter\n",
      "Epoch: 380 | Batch: 007 / 011 | Total loss: 1.419 | Reg loss: 0.040 | Tree loss: 1.419 | Accuracy: 0.525000 | 1.756 sec/iter\n",
      "Epoch: 380 | Batch: 008 / 011 | Total loss: 1.407 | Reg loss: 0.040 | Tree loss: 1.407 | Accuracy: 0.503500 | 1.756 sec/iter\n",
      "Epoch: 380 | Batch: 009 / 011 | Total loss: 1.401 | Reg loss: 0.040 | Tree loss: 1.401 | Accuracy: 0.510500 | 1.756 sec/iter\n",
      "Epoch: 380 | Batch: 010 / 011 | Total loss: 1.392 | Reg loss: 0.040 | Tree loss: 1.392 | Accuracy: 0.491468 | 1.756 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 381 | Batch: 000 / 011 | Total loss: 1.598 | Reg loss: 0.040 | Tree loss: 1.598 | Accuracy: 0.406500 | 1.757 sec/iter\n",
      "Epoch: 381 | Batch: 001 / 011 | Total loss: 1.574 | Reg loss: 0.040 | Tree loss: 1.574 | Accuracy: 0.413000 | 1.757 sec/iter\n",
      "Epoch: 381 | Batch: 002 / 011 | Total loss: 1.547 | Reg loss: 0.040 | Tree loss: 1.547 | Accuracy: 0.431500 | 1.757 sec/iter\n",
      "Epoch: 381 | Batch: 003 / 011 | Total loss: 1.487 | Reg loss: 0.040 | Tree loss: 1.487 | Accuracy: 0.452000 | 1.757 sec/iter\n",
      "Epoch: 381 | Batch: 004 / 011 | Total loss: 1.451 | Reg loss: 0.040 | Tree loss: 1.451 | Accuracy: 0.472000 | 1.757 sec/iter\n",
      "Epoch: 381 | Batch: 005 / 011 | Total loss: 1.426 | Reg loss: 0.040 | Tree loss: 1.426 | Accuracy: 0.501500 | 1.757 sec/iter\n",
      "Epoch: 381 | Batch: 006 / 011 | Total loss: 1.421 | Reg loss: 0.040 | Tree loss: 1.421 | Accuracy: 0.502500 | 1.757 sec/iter\n",
      "Epoch: 381 | Batch: 007 / 011 | Total loss: 1.397 | Reg loss: 0.040 | Tree loss: 1.397 | Accuracy: 0.517000 | 1.757 sec/iter\n",
      "Epoch: 381 | Batch: 008 / 011 | Total loss: 1.418 | Reg loss: 0.040 | Tree loss: 1.418 | Accuracy: 0.497500 | 1.757 sec/iter\n",
      "Epoch: 381 | Batch: 009 / 011 | Total loss: 1.402 | Reg loss: 0.040 | Tree loss: 1.402 | Accuracy: 0.496000 | 1.757 sec/iter\n",
      "Epoch: 381 | Batch: 010 / 011 | Total loss: 1.459 | Reg loss: 0.040 | Tree loss: 1.459 | Accuracy: 0.457338 | 1.757 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 382 | Batch: 000 / 011 | Total loss: 1.575 | Reg loss: 0.040 | Tree loss: 1.575 | Accuracy: 0.405000 | 1.758 sec/iter\n",
      "Epoch: 382 | Batch: 001 / 011 | Total loss: 1.583 | Reg loss: 0.040 | Tree loss: 1.583 | Accuracy: 0.404000 | 1.758 sec/iter\n",
      "Epoch: 382 | Batch: 002 / 011 | Total loss: 1.549 | Reg loss: 0.040 | Tree loss: 1.549 | Accuracy: 0.433500 | 1.758 sec/iter\n",
      "Epoch: 382 | Batch: 003 / 011 | Total loss: 1.491 | Reg loss: 0.040 | Tree loss: 1.491 | Accuracy: 0.460000 | 1.758 sec/iter\n",
      "Epoch: 382 | Batch: 004 / 011 | Total loss: 1.464 | Reg loss: 0.040 | Tree loss: 1.464 | Accuracy: 0.471500 | 1.758 sec/iter\n",
      "Epoch: 382 | Batch: 005 / 011 | Total loss: 1.423 | Reg loss: 0.040 | Tree loss: 1.423 | Accuracy: 0.507500 | 1.758 sec/iter\n",
      "Epoch: 382 | Batch: 006 / 011 | Total loss: 1.430 | Reg loss: 0.040 | Tree loss: 1.430 | Accuracy: 0.507000 | 1.758 sec/iter\n",
      "Epoch: 382 | Batch: 007 / 011 | Total loss: 1.392 | Reg loss: 0.040 | Tree loss: 1.392 | Accuracy: 0.535500 | 1.758 sec/iter\n",
      "Epoch: 382 | Batch: 008 / 011 | Total loss: 1.408 | Reg loss: 0.040 | Tree loss: 1.408 | Accuracy: 0.486000 | 1.758 sec/iter\n",
      "Epoch: 382 | Batch: 009 / 011 | Total loss: 1.411 | Reg loss: 0.040 | Tree loss: 1.411 | Accuracy: 0.468000 | 1.758 sec/iter\n",
      "Epoch: 382 | Batch: 010 / 011 | Total loss: 1.411 | Reg loss: 0.040 | Tree loss: 1.411 | Accuracy: 0.474403 | 1.758 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 383 | Batch: 000 / 011 | Total loss: 1.612 | Reg loss: 0.040 | Tree loss: 1.612 | Accuracy: 0.405000 | 1.759 sec/iter\n",
      "Epoch: 383 | Batch: 001 / 011 | Total loss: 1.564 | Reg loss: 0.040 | Tree loss: 1.564 | Accuracy: 0.410500 | 1.759 sec/iter\n",
      "Epoch: 383 | Batch: 002 / 011 | Total loss: 1.536 | Reg loss: 0.040 | Tree loss: 1.536 | Accuracy: 0.437500 | 1.759 sec/iter\n",
      "Epoch: 383 | Batch: 003 / 011 | Total loss: 1.490 | Reg loss: 0.040 | Tree loss: 1.490 | Accuracy: 0.457000 | 1.759 sec/iter\n",
      "Epoch: 383 | Batch: 004 / 011 | Total loss: 1.430 | Reg loss: 0.040 | Tree loss: 1.430 | Accuracy: 0.491500 | 1.759 sec/iter\n",
      "Epoch: 383 | Batch: 005 / 011 | Total loss: 1.434 | Reg loss: 0.040 | Tree loss: 1.434 | Accuracy: 0.486500 | 1.759 sec/iter\n",
      "Epoch: 383 | Batch: 006 / 011 | Total loss: 1.427 | Reg loss: 0.040 | Tree loss: 1.427 | Accuracy: 0.501000 | 1.759 sec/iter\n",
      "Epoch: 383 | Batch: 007 / 011 | Total loss: 1.421 | Reg loss: 0.040 | Tree loss: 1.421 | Accuracy: 0.519000 | 1.759 sec/iter\n",
      "Epoch: 383 | Batch: 008 / 011 | Total loss: 1.418 | Reg loss: 0.040 | Tree loss: 1.418 | Accuracy: 0.504500 | 1.759 sec/iter\n",
      "Epoch: 383 | Batch: 009 / 011 | Total loss: 1.405 | Reg loss: 0.040 | Tree loss: 1.405 | Accuracy: 0.495000 | 1.759 sec/iter\n",
      "Epoch: 383 | Batch: 010 / 011 | Total loss: 1.434 | Reg loss: 0.040 | Tree loss: 1.434 | Accuracy: 0.460751 | 1.759 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 384 | Batch: 000 / 011 | Total loss: 1.598 | Reg loss: 0.040 | Tree loss: 1.598 | Accuracy: 0.397000 | 1.76 sec/iter\n",
      "Epoch: 384 | Batch: 001 / 011 | Total loss: 1.575 | Reg loss: 0.040 | Tree loss: 1.575 | Accuracy: 0.396000 | 1.76 sec/iter\n",
      "Epoch: 384 | Batch: 002 / 011 | Total loss: 1.538 | Reg loss: 0.040 | Tree loss: 1.538 | Accuracy: 0.435500 | 1.76 sec/iter\n",
      "Epoch: 384 | Batch: 003 / 011 | Total loss: 1.508 | Reg loss: 0.040 | Tree loss: 1.508 | Accuracy: 0.463500 | 1.76 sec/iter\n",
      "Epoch: 384 | Batch: 004 / 011 | Total loss: 1.450 | Reg loss: 0.040 | Tree loss: 1.450 | Accuracy: 0.478500 | 1.76 sec/iter\n",
      "Epoch: 384 | Batch: 005 / 011 | Total loss: 1.449 | Reg loss: 0.040 | Tree loss: 1.449 | Accuracy: 0.493500 | 1.76 sec/iter\n",
      "Epoch: 384 | Batch: 006 / 011 | Total loss: 1.407 | Reg loss: 0.040 | Tree loss: 1.407 | Accuracy: 0.516500 | 1.76 sec/iter\n",
      "Epoch: 384 | Batch: 007 / 011 | Total loss: 1.394 | Reg loss: 0.040 | Tree loss: 1.394 | Accuracy: 0.517000 | 1.76 sec/iter\n",
      "Epoch: 384 | Batch: 008 / 011 | Total loss: 1.418 | Reg loss: 0.040 | Tree loss: 1.418 | Accuracy: 0.504000 | 1.76 sec/iter\n",
      "Epoch: 384 | Batch: 009 / 011 | Total loss: 1.401 | Reg loss: 0.040 | Tree loss: 1.401 | Accuracy: 0.512000 | 1.76 sec/iter\n",
      "Epoch: 384 | Batch: 010 / 011 | Total loss: 1.394 | Reg loss: 0.040 | Tree loss: 1.394 | Accuracy: 0.484642 | 1.76 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 385 | Batch: 000 / 011 | Total loss: 1.616 | Reg loss: 0.040 | Tree loss: 1.616 | Accuracy: 0.390500 | 1.76 sec/iter\n",
      "Epoch: 385 | Batch: 001 / 011 | Total loss: 1.555 | Reg loss: 0.040 | Tree loss: 1.555 | Accuracy: 0.412500 | 1.76 sec/iter\n",
      "Epoch: 385 | Batch: 002 / 011 | Total loss: 1.531 | Reg loss: 0.040 | Tree loss: 1.531 | Accuracy: 0.434500 | 1.76 sec/iter\n",
      "Epoch: 385 | Batch: 003 / 011 | Total loss: 1.492 | Reg loss: 0.040 | Tree loss: 1.492 | Accuracy: 0.438000 | 1.76 sec/iter\n",
      "Epoch: 385 | Batch: 004 / 011 | Total loss: 1.449 | Reg loss: 0.040 | Tree loss: 1.449 | Accuracy: 0.470000 | 1.76 sec/iter\n",
      "Epoch: 385 | Batch: 005 / 011 | Total loss: 1.446 | Reg loss: 0.040 | Tree loss: 1.446 | Accuracy: 0.476000 | 1.76 sec/iter\n",
      "Epoch: 385 | Batch: 006 / 011 | Total loss: 1.429 | Reg loss: 0.040 | Tree loss: 1.429 | Accuracy: 0.496500 | 1.76 sec/iter\n",
      "Epoch: 385 | Batch: 007 / 011 | Total loss: 1.420 | Reg loss: 0.040 | Tree loss: 1.420 | Accuracy: 0.519500 | 1.76 sec/iter\n",
      "Epoch: 385 | Batch: 008 / 011 | Total loss: 1.393 | Reg loss: 0.040 | Tree loss: 1.393 | Accuracy: 0.517000 | 1.76 sec/iter\n",
      "Epoch: 385 | Batch: 009 / 011 | Total loss: 1.427 | Reg loss: 0.040 | Tree loss: 1.427 | Accuracy: 0.486000 | 1.76 sec/iter\n",
      "Epoch: 385 | Batch: 010 / 011 | Total loss: 1.377 | Reg loss: 0.040 | Tree loss: 1.377 | Accuracy: 0.535836 | 1.76 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 386 | Batch: 000 / 011 | Total loss: 1.616 | Reg loss: 0.040 | Tree loss: 1.616 | Accuracy: 0.380500 | 1.761 sec/iter\n",
      "Epoch: 386 | Batch: 001 / 011 | Total loss: 1.575 | Reg loss: 0.040 | Tree loss: 1.575 | Accuracy: 0.404500 | 1.761 sec/iter\n",
      "Epoch: 386 | Batch: 002 / 011 | Total loss: 1.534 | Reg loss: 0.040 | Tree loss: 1.534 | Accuracy: 0.415500 | 1.761 sec/iter\n",
      "Epoch: 386 | Batch: 003 / 011 | Total loss: 1.478 | Reg loss: 0.040 | Tree loss: 1.478 | Accuracy: 0.465500 | 1.761 sec/iter\n",
      "Epoch: 386 | Batch: 004 / 011 | Total loss: 1.478 | Reg loss: 0.040 | Tree loss: 1.478 | Accuracy: 0.473500 | 1.761 sec/iter\n",
      "Epoch: 386 | Batch: 005 / 011 | Total loss: 1.438 | Reg loss: 0.040 | Tree loss: 1.438 | Accuracy: 0.510500 | 1.761 sec/iter\n",
      "Epoch: 386 | Batch: 006 / 011 | Total loss: 1.395 | Reg loss: 0.040 | Tree loss: 1.395 | Accuracy: 0.521500 | 1.761 sec/iter\n",
      "Epoch: 386 | Batch: 007 / 011 | Total loss: 1.403 | Reg loss: 0.040 | Tree loss: 1.403 | Accuracy: 0.529000 | 1.761 sec/iter\n",
      "Epoch: 386 | Batch: 008 / 011 | Total loss: 1.405 | Reg loss: 0.040 | Tree loss: 1.405 | Accuracy: 0.509000 | 1.761 sec/iter\n",
      "Epoch: 386 | Batch: 009 / 011 | Total loss: 1.395 | Reg loss: 0.040 | Tree loss: 1.395 | Accuracy: 0.515500 | 1.761 sec/iter\n",
      "Epoch: 386 | Batch: 010 / 011 | Total loss: 1.385 | Reg loss: 0.040 | Tree loss: 1.385 | Accuracy: 0.508532 | 1.761 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 387 | Batch: 000 / 011 | Total loss: 1.606 | Reg loss: 0.040 | Tree loss: 1.606 | Accuracy: 0.394500 | 1.761 sec/iter\n",
      "Epoch: 387 | Batch: 001 / 011 | Total loss: 1.582 | Reg loss: 0.040 | Tree loss: 1.582 | Accuracy: 0.406500 | 1.761 sec/iter\n",
      "Epoch: 387 | Batch: 002 / 011 | Total loss: 1.531 | Reg loss: 0.040 | Tree loss: 1.531 | Accuracy: 0.435500 | 1.761 sec/iter\n",
      "Epoch: 387 | Batch: 003 / 011 | Total loss: 1.459 | Reg loss: 0.040 | Tree loss: 1.459 | Accuracy: 0.481000 | 1.761 sec/iter\n",
      "Epoch: 387 | Batch: 004 / 011 | Total loss: 1.473 | Reg loss: 0.040 | Tree loss: 1.473 | Accuracy: 0.469000 | 1.761 sec/iter\n",
      "Epoch: 387 | Batch: 005 / 011 | Total loss: 1.426 | Reg loss: 0.040 | Tree loss: 1.426 | Accuracy: 0.505000 | 1.761 sec/iter\n",
      "Epoch: 387 | Batch: 006 / 011 | Total loss: 1.420 | Reg loss: 0.040 | Tree loss: 1.420 | Accuracy: 0.509500 | 1.76 sec/iter\n",
      "Epoch: 387 | Batch: 007 / 011 | Total loss: 1.412 | Reg loss: 0.040 | Tree loss: 1.412 | Accuracy: 0.516000 | 1.76 sec/iter\n",
      "Epoch: 387 | Batch: 008 / 011 | Total loss: 1.402 | Reg loss: 0.040 | Tree loss: 1.402 | Accuracy: 0.495000 | 1.76 sec/iter\n",
      "Epoch: 387 | Batch: 009 / 011 | Total loss: 1.424 | Reg loss: 0.040 | Tree loss: 1.424 | Accuracy: 0.490000 | 1.76 sec/iter\n",
      "Epoch: 387 | Batch: 010 / 011 | Total loss: 1.352 | Reg loss: 0.040 | Tree loss: 1.352 | Accuracy: 0.566553 | 1.76 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 388 | Batch: 000 / 011 | Total loss: 1.621 | Reg loss: 0.040 | Tree loss: 1.621 | Accuracy: 0.377500 | 1.761 sec/iter\n",
      "Epoch: 388 | Batch: 001 / 011 | Total loss: 1.562 | Reg loss: 0.040 | Tree loss: 1.562 | Accuracy: 0.402500 | 1.761 sec/iter\n",
      "Epoch: 388 | Batch: 002 / 011 | Total loss: 1.517 | Reg loss: 0.040 | Tree loss: 1.517 | Accuracy: 0.446000 | 1.761 sec/iter\n",
      "Epoch: 388 | Batch: 003 / 011 | Total loss: 1.492 | Reg loss: 0.040 | Tree loss: 1.492 | Accuracy: 0.461000 | 1.761 sec/iter\n",
      "Epoch: 388 | Batch: 004 / 011 | Total loss: 1.466 | Reg loss: 0.040 | Tree loss: 1.466 | Accuracy: 0.452500 | 1.761 sec/iter\n",
      "Epoch: 388 | Batch: 005 / 011 | Total loss: 1.448 | Reg loss: 0.040 | Tree loss: 1.448 | Accuracy: 0.491500 | 1.761 sec/iter\n",
      "Epoch: 388 | Batch: 006 / 011 | Total loss: 1.416 | Reg loss: 0.040 | Tree loss: 1.416 | Accuracy: 0.503000 | 1.761 sec/iter\n",
      "Epoch: 388 | Batch: 007 / 011 | Total loss: 1.402 | Reg loss: 0.040 | Tree loss: 1.402 | Accuracy: 0.523500 | 1.761 sec/iter\n",
      "Epoch: 388 | Batch: 008 / 011 | Total loss: 1.395 | Reg loss: 0.040 | Tree loss: 1.395 | Accuracy: 0.506500 | 1.761 sec/iter\n",
      "Epoch: 388 | Batch: 009 / 011 | Total loss: 1.395 | Reg loss: 0.040 | Tree loss: 1.395 | Accuracy: 0.492500 | 1.761 sec/iter\n",
      "Epoch: 388 | Batch: 010 / 011 | Total loss: 1.460 | Reg loss: 0.040 | Tree loss: 1.460 | Accuracy: 0.484642 | 1.761 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 389 | Batch: 000 / 011 | Total loss: 1.600 | Reg loss: 0.040 | Tree loss: 1.600 | Accuracy: 0.405000 | 1.762 sec/iter\n",
      "Epoch: 389 | Batch: 001 / 011 | Total loss: 1.570 | Reg loss: 0.040 | Tree loss: 1.570 | Accuracy: 0.409000 | 1.762 sec/iter\n",
      "Epoch: 389 | Batch: 002 / 011 | Total loss: 1.552 | Reg loss: 0.040 | Tree loss: 1.552 | Accuracy: 0.419500 | 1.762 sec/iter\n",
      "Epoch: 389 | Batch: 003 / 011 | Total loss: 1.500 | Reg loss: 0.040 | Tree loss: 1.500 | Accuracy: 0.427500 | 1.762 sec/iter\n",
      "Epoch: 389 | Batch: 004 / 011 | Total loss: 1.471 | Reg loss: 0.040 | Tree loss: 1.471 | Accuracy: 0.465000 | 1.762 sec/iter\n",
      "Epoch: 389 | Batch: 005 / 011 | Total loss: 1.416 | Reg loss: 0.040 | Tree loss: 1.416 | Accuracy: 0.510000 | 1.762 sec/iter\n",
      "Epoch: 389 | Batch: 006 / 011 | Total loss: 1.396 | Reg loss: 0.040 | Tree loss: 1.396 | Accuracy: 0.526500 | 1.762 sec/iter\n",
      "Epoch: 389 | Batch: 007 / 011 | Total loss: 1.417 | Reg loss: 0.040 | Tree loss: 1.417 | Accuracy: 0.517500 | 1.762 sec/iter\n",
      "Epoch: 389 | Batch: 008 / 011 | Total loss: 1.426 | Reg loss: 0.040 | Tree loss: 1.426 | Accuracy: 0.503000 | 1.762 sec/iter\n",
      "Epoch: 389 | Batch: 009 / 011 | Total loss: 1.405 | Reg loss: 0.040 | Tree loss: 1.405 | Accuracy: 0.497000 | 1.762 sec/iter\n",
      "Epoch: 389 | Batch: 010 / 011 | Total loss: 1.387 | Reg loss: 0.040 | Tree loss: 1.387 | Accuracy: 0.474403 | 1.762 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 390 | Batch: 000 / 011 | Total loss: 1.643 | Reg loss: 0.040 | Tree loss: 1.643 | Accuracy: 0.382000 | 1.763 sec/iter\n",
      "Epoch: 390 | Batch: 001 / 011 | Total loss: 1.567 | Reg loss: 0.040 | Tree loss: 1.567 | Accuracy: 0.398000 | 1.763 sec/iter\n",
      "Epoch: 390 | Batch: 002 / 011 | Total loss: 1.553 | Reg loss: 0.040 | Tree loss: 1.553 | Accuracy: 0.427000 | 1.763 sec/iter\n",
      "Epoch: 390 | Batch: 003 / 011 | Total loss: 1.503 | Reg loss: 0.040 | Tree loss: 1.503 | Accuracy: 0.436500 | 1.763 sec/iter\n",
      "Epoch: 390 | Batch: 004 / 011 | Total loss: 1.461 | Reg loss: 0.040 | Tree loss: 1.461 | Accuracy: 0.463500 | 1.763 sec/iter\n",
      "Epoch: 390 | Batch: 005 / 011 | Total loss: 1.428 | Reg loss: 0.040 | Tree loss: 1.428 | Accuracy: 0.505500 | 1.763 sec/iter\n",
      "Epoch: 390 | Batch: 006 / 011 | Total loss: 1.403 | Reg loss: 0.040 | Tree loss: 1.403 | Accuracy: 0.510000 | 1.763 sec/iter\n",
      "Epoch: 390 | Batch: 007 / 011 | Total loss: 1.404 | Reg loss: 0.040 | Tree loss: 1.404 | Accuracy: 0.530500 | 1.763 sec/iter\n",
      "Epoch: 390 | Batch: 008 / 011 | Total loss: 1.395 | Reg loss: 0.040 | Tree loss: 1.395 | Accuracy: 0.513000 | 1.763 sec/iter\n",
      "Epoch: 390 | Batch: 009 / 011 | Total loss: 1.409 | Reg loss: 0.040 | Tree loss: 1.409 | Accuracy: 0.502000 | 1.763 sec/iter\n",
      "Epoch: 390 | Batch: 010 / 011 | Total loss: 1.414 | Reg loss: 0.040 | Tree loss: 1.414 | Accuracy: 0.522184 | 1.763 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 391 | Batch: 000 / 011 | Total loss: 1.598 | Reg loss: 0.040 | Tree loss: 1.598 | Accuracy: 0.400000 | 1.763 sec/iter\n",
      "Epoch: 391 | Batch: 001 / 011 | Total loss: 1.575 | Reg loss: 0.040 | Tree loss: 1.575 | Accuracy: 0.399500 | 1.763 sec/iter\n",
      "Epoch: 391 | Batch: 002 / 011 | Total loss: 1.500 | Reg loss: 0.040 | Tree loss: 1.500 | Accuracy: 0.447500 | 1.763 sec/iter\n",
      "Epoch: 391 | Batch: 003 / 011 | Total loss: 1.486 | Reg loss: 0.040 | Tree loss: 1.486 | Accuracy: 0.459500 | 1.763 sec/iter\n",
      "Epoch: 391 | Batch: 004 / 011 | Total loss: 1.451 | Reg loss: 0.040 | Tree loss: 1.451 | Accuracy: 0.484500 | 1.763 sec/iter\n",
      "Epoch: 391 | Batch: 005 / 011 | Total loss: 1.415 | Reg loss: 0.040 | Tree loss: 1.415 | Accuracy: 0.494000 | 1.763 sec/iter\n",
      "Epoch: 391 | Batch: 006 / 011 | Total loss: 1.417 | Reg loss: 0.040 | Tree loss: 1.417 | Accuracy: 0.514000 | 1.763 sec/iter\n",
      "Epoch: 391 | Batch: 007 / 011 | Total loss: 1.424 | Reg loss: 0.040 | Tree loss: 1.424 | Accuracy: 0.507000 | 1.763 sec/iter\n",
      "Epoch: 391 | Batch: 008 / 011 | Total loss: 1.429 | Reg loss: 0.040 | Tree loss: 1.429 | Accuracy: 0.492500 | 1.763 sec/iter\n",
      "Epoch: 391 | Batch: 009 / 011 | Total loss: 1.426 | Reg loss: 0.040 | Tree loss: 1.426 | Accuracy: 0.476500 | 1.763 sec/iter\n",
      "Epoch: 391 | Batch: 010 / 011 | Total loss: 1.370 | Reg loss: 0.040 | Tree loss: 1.370 | Accuracy: 0.532423 | 1.763 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 392 | Batch: 000 / 011 | Total loss: 1.620 | Reg loss: 0.040 | Tree loss: 1.620 | Accuracy: 0.386000 | 1.764 sec/iter\n",
      "Epoch: 392 | Batch: 001 / 011 | Total loss: 1.577 | Reg loss: 0.040 | Tree loss: 1.577 | Accuracy: 0.393500 | 1.764 sec/iter\n",
      "Epoch: 392 | Batch: 002 / 011 | Total loss: 1.514 | Reg loss: 0.040 | Tree loss: 1.514 | Accuracy: 0.444500 | 1.764 sec/iter\n",
      "Epoch: 392 | Batch: 003 / 011 | Total loss: 1.484 | Reg loss: 0.040 | Tree loss: 1.484 | Accuracy: 0.450500 | 1.764 sec/iter\n",
      "Epoch: 392 | Batch: 004 / 011 | Total loss: 1.440 | Reg loss: 0.040 | Tree loss: 1.440 | Accuracy: 0.476500 | 1.764 sec/iter\n",
      "Epoch: 392 | Batch: 005 / 011 | Total loss: 1.453 | Reg loss: 0.040 | Tree loss: 1.453 | Accuracy: 0.482000 | 1.764 sec/iter\n",
      "Epoch: 392 | Batch: 006 / 011 | Total loss: 1.415 | Reg loss: 0.040 | Tree loss: 1.415 | Accuracy: 0.500500 | 1.764 sec/iter\n",
      "Epoch: 392 | Batch: 007 / 011 | Total loss: 1.412 | Reg loss: 0.040 | Tree loss: 1.412 | Accuracy: 0.515500 | 1.764 sec/iter\n",
      "Epoch: 392 | Batch: 008 / 011 | Total loss: 1.409 | Reg loss: 0.040 | Tree loss: 1.409 | Accuracy: 0.539000 | 1.764 sec/iter\n",
      "Epoch: 392 | Batch: 009 / 011 | Total loss: 1.412 | Reg loss: 0.040 | Tree loss: 1.412 | Accuracy: 0.494000 | 1.764 sec/iter\n",
      "Epoch: 392 | Batch: 010 / 011 | Total loss: 1.333 | Reg loss: 0.040 | Tree loss: 1.333 | Accuracy: 0.532423 | 1.764 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 393 | Batch: 000 / 011 | Total loss: 1.598 | Reg loss: 0.040 | Tree loss: 1.598 | Accuracy: 0.414000 | 1.765 sec/iter\n",
      "Epoch: 393 | Batch: 001 / 011 | Total loss: 1.559 | Reg loss: 0.040 | Tree loss: 1.559 | Accuracy: 0.397000 | 1.765 sec/iter\n",
      "Epoch: 393 | Batch: 002 / 011 | Total loss: 1.527 | Reg loss: 0.040 | Tree loss: 1.527 | Accuracy: 0.430500 | 1.765 sec/iter\n",
      "Epoch: 393 | Batch: 003 / 011 | Total loss: 1.465 | Reg loss: 0.040 | Tree loss: 1.465 | Accuracy: 0.484000 | 1.765 sec/iter\n",
      "Epoch: 393 | Batch: 004 / 011 | Total loss: 1.459 | Reg loss: 0.040 | Tree loss: 1.459 | Accuracy: 0.480000 | 1.765 sec/iter\n",
      "Epoch: 393 | Batch: 005 / 011 | Total loss: 1.442 | Reg loss: 0.040 | Tree loss: 1.442 | Accuracy: 0.496500 | 1.765 sec/iter\n",
      "Epoch: 393 | Batch: 006 / 011 | Total loss: 1.402 | Reg loss: 0.040 | Tree loss: 1.402 | Accuracy: 0.533500 | 1.765 sec/iter\n",
      "Epoch: 393 | Batch: 007 / 011 | Total loss: 1.406 | Reg loss: 0.040 | Tree loss: 1.406 | Accuracy: 0.519500 | 1.765 sec/iter\n",
      "Epoch: 393 | Batch: 008 / 011 | Total loss: 1.428 | Reg loss: 0.040 | Tree loss: 1.428 | Accuracy: 0.514000 | 1.765 sec/iter\n",
      "Epoch: 393 | Batch: 009 / 011 | Total loss: 1.441 | Reg loss: 0.040 | Tree loss: 1.441 | Accuracy: 0.469000 | 1.765 sec/iter\n",
      "Epoch: 393 | Batch: 010 / 011 | Total loss: 1.385 | Reg loss: 0.040 | Tree loss: 1.385 | Accuracy: 0.518771 | 1.765 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 394 | Batch: 000 / 011 | Total loss: 1.609 | Reg loss: 0.040 | Tree loss: 1.609 | Accuracy: 0.396000 | 1.766 sec/iter\n",
      "Epoch: 394 | Batch: 001 / 011 | Total loss: 1.569 | Reg loss: 0.040 | Tree loss: 1.569 | Accuracy: 0.417500 | 1.766 sec/iter\n",
      "Epoch: 394 | Batch: 002 / 011 | Total loss: 1.541 | Reg loss: 0.040 | Tree loss: 1.541 | Accuracy: 0.437500 | 1.766 sec/iter\n",
      "Epoch: 394 | Batch: 003 / 011 | Total loss: 1.500 | Reg loss: 0.040 | Tree loss: 1.500 | Accuracy: 0.442500 | 1.766 sec/iter\n",
      "Epoch: 394 | Batch: 004 / 011 | Total loss: 1.443 | Reg loss: 0.040 | Tree loss: 1.443 | Accuracy: 0.487500 | 1.766 sec/iter\n",
      "Epoch: 394 | Batch: 005 / 011 | Total loss: 1.434 | Reg loss: 0.040 | Tree loss: 1.434 | Accuracy: 0.503500 | 1.766 sec/iter\n",
      "Epoch: 394 | Batch: 006 / 011 | Total loss: 1.420 | Reg loss: 0.040 | Tree loss: 1.420 | Accuracy: 0.510500 | 1.766 sec/iter\n",
      "Epoch: 394 | Batch: 007 / 011 | Total loss: 1.402 | Reg loss: 0.040 | Tree loss: 1.402 | Accuracy: 0.522000 | 1.766 sec/iter\n",
      "Epoch: 394 | Batch: 008 / 011 | Total loss: 1.393 | Reg loss: 0.040 | Tree loss: 1.393 | Accuracy: 0.513500 | 1.766 sec/iter\n",
      "Epoch: 394 | Batch: 009 / 011 | Total loss: 1.411 | Reg loss: 0.040 | Tree loss: 1.411 | Accuracy: 0.504500 | 1.766 sec/iter\n",
      "Epoch: 394 | Batch: 010 / 011 | Total loss: 1.357 | Reg loss: 0.040 | Tree loss: 1.357 | Accuracy: 0.511945 | 1.766 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 8: 0.9723756906077345\n",
      "Epoch: 395 | Batch: 000 / 011 | Total loss: 1.598 | Reg loss: 0.040 | Tree loss: 1.598 | Accuracy: 0.414500 | 1.767 sec/iter\n",
      "Epoch: 395 | Batch: 001 / 011 | Total loss: 1.585 | Reg loss: 0.040 | Tree loss: 1.585 | Accuracy: 0.414000 | 1.767 sec/iter\n",
      "Epoch: 395 | Batch: 002 / 011 | Total loss: 1.531 | Reg loss: 0.040 | Tree loss: 1.531 | Accuracy: 0.445500 | 1.767 sec/iter\n",
      "Epoch: 395 | Batch: 003 / 011 | Total loss: 1.479 | Reg loss: 0.040 | Tree loss: 1.479 | Accuracy: 0.463000 | 1.767 sec/iter\n",
      "Epoch: 395 | Batch: 004 / 011 | Total loss: 1.462 | Reg loss: 0.040 | Tree loss: 1.462 | Accuracy: 0.485000 | 1.767 sec/iter\n",
      "Epoch: 395 | Batch: 005 / 011 | Total loss: 1.429 | Reg loss: 0.040 | Tree loss: 1.429 | Accuracy: 0.529500 | 1.767 sec/iter\n",
      "Epoch: 395 | Batch: 006 / 011 | Total loss: 1.408 | Reg loss: 0.040 | Tree loss: 1.408 | Accuracy: 0.513500 | 1.767 sec/iter\n",
      "Epoch: 395 | Batch: 007 / 011 | Total loss: 1.408 | Reg loss: 0.040 | Tree loss: 1.408 | Accuracy: 0.505500 | 1.767 sec/iter\n",
      "Epoch: 395 | Batch: 008 / 011 | Total loss: 1.401 | Reg loss: 0.040 | Tree loss: 1.401 | Accuracy: 0.519000 | 1.767 sec/iter\n",
      "Epoch: 395 | Batch: 009 / 011 | Total loss: 1.401 | Reg loss: 0.040 | Tree loss: 1.401 | Accuracy: 0.508500 | 1.767 sec/iter\n",
      "Epoch: 395 | Batch: 010 / 011 | Total loss: 1.415 | Reg loss: 0.040 | Tree loss: 1.415 | Accuracy: 0.484642 | 1.767 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 396 | Batch: 000 / 011 | Total loss: 1.613 | Reg loss: 0.040 | Tree loss: 1.613 | Accuracy: 0.394000 | 1.767 sec/iter\n",
      "Epoch: 396 | Batch: 001 / 011 | Total loss: 1.581 | Reg loss: 0.040 | Tree loss: 1.581 | Accuracy: 0.401500 | 1.767 sec/iter\n",
      "Epoch: 396 | Batch: 002 / 011 | Total loss: 1.514 | Reg loss: 0.040 | Tree loss: 1.514 | Accuracy: 0.447500 | 1.767 sec/iter\n",
      "Epoch: 396 | Batch: 003 / 011 | Total loss: 1.502 | Reg loss: 0.040 | Tree loss: 1.502 | Accuracy: 0.450000 | 1.767 sec/iter\n",
      "Epoch: 396 | Batch: 004 / 011 | Total loss: 1.433 | Reg loss: 0.040 | Tree loss: 1.433 | Accuracy: 0.486500 | 1.767 sec/iter\n",
      "Epoch: 396 | Batch: 005 / 011 | Total loss: 1.427 | Reg loss: 0.040 | Tree loss: 1.427 | Accuracy: 0.498500 | 1.767 sec/iter\n",
      "Epoch: 396 | Batch: 006 / 011 | Total loss: 1.431 | Reg loss: 0.040 | Tree loss: 1.431 | Accuracy: 0.517000 | 1.767 sec/iter\n",
      "Epoch: 396 | Batch: 007 / 011 | Total loss: 1.404 | Reg loss: 0.040 | Tree loss: 1.404 | Accuracy: 0.515500 | 1.767 sec/iter\n",
      "Epoch: 396 | Batch: 008 / 011 | Total loss: 1.402 | Reg loss: 0.040 | Tree loss: 1.402 | Accuracy: 0.514500 | 1.767 sec/iter\n",
      "Epoch: 396 | Batch: 009 / 011 | Total loss: 1.429 | Reg loss: 0.040 | Tree loss: 1.429 | Accuracy: 0.483000 | 1.767 sec/iter\n",
      "Epoch: 396 | Batch: 010 / 011 | Total loss: 1.400 | Reg loss: 0.040 | Tree loss: 1.400 | Accuracy: 0.501706 | 1.767 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 397 | Batch: 000 / 011 | Total loss: 1.620 | Reg loss: 0.040 | Tree loss: 1.620 | Accuracy: 0.402000 | 1.768 sec/iter\n",
      "Epoch: 397 | Batch: 001 / 011 | Total loss: 1.541 | Reg loss: 0.040 | Tree loss: 1.541 | Accuracy: 0.419500 | 1.768 sec/iter\n",
      "Epoch: 397 | Batch: 002 / 011 | Total loss: 1.536 | Reg loss: 0.040 | Tree loss: 1.536 | Accuracy: 0.443000 | 1.768 sec/iter\n",
      "Epoch: 397 | Batch: 003 / 011 | Total loss: 1.499 | Reg loss: 0.040 | Tree loss: 1.499 | Accuracy: 0.464000 | 1.768 sec/iter\n",
      "Epoch: 397 | Batch: 004 / 011 | Total loss: 1.448 | Reg loss: 0.040 | Tree loss: 1.448 | Accuracy: 0.499000 | 1.768 sec/iter\n",
      "Epoch: 397 | Batch: 005 / 011 | Total loss: 1.427 | Reg loss: 0.040 | Tree loss: 1.427 | Accuracy: 0.514500 | 1.768 sec/iter\n",
      "Epoch: 397 | Batch: 006 / 011 | Total loss: 1.406 | Reg loss: 0.040 | Tree loss: 1.406 | Accuracy: 0.519000 | 1.768 sec/iter\n",
      "Epoch: 397 | Batch: 007 / 011 | Total loss: 1.405 | Reg loss: 0.040 | Tree loss: 1.405 | Accuracy: 0.525000 | 1.768 sec/iter\n",
      "Epoch: 397 | Batch: 008 / 011 | Total loss: 1.413 | Reg loss: 0.040 | Tree loss: 1.413 | Accuracy: 0.509500 | 1.768 sec/iter\n",
      "Epoch: 397 | Batch: 009 / 011 | Total loss: 1.409 | Reg loss: 0.040 | Tree loss: 1.409 | Accuracy: 0.496500 | 1.768 sec/iter\n",
      "Epoch: 397 | Batch: 010 / 011 | Total loss: 1.467 | Reg loss: 0.040 | Tree loss: 1.467 | Accuracy: 0.491468 | 1.768 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 398 | Batch: 000 / 011 | Total loss: 1.592 | Reg loss: 0.040 | Tree loss: 1.592 | Accuracy: 0.406000 | 1.769 sec/iter\n",
      "Epoch: 398 | Batch: 001 / 011 | Total loss: 1.585 | Reg loss: 0.040 | Tree loss: 1.585 | Accuracy: 0.414500 | 1.769 sec/iter\n",
      "Epoch: 398 | Batch: 002 / 011 | Total loss: 1.511 | Reg loss: 0.040 | Tree loss: 1.511 | Accuracy: 0.458500 | 1.769 sec/iter\n",
      "Epoch: 398 | Batch: 003 / 011 | Total loss: 1.496 | Reg loss: 0.040 | Tree loss: 1.496 | Accuracy: 0.443000 | 1.769 sec/iter\n",
      "Epoch: 398 | Batch: 004 / 011 | Total loss: 1.478 | Reg loss: 0.040 | Tree loss: 1.478 | Accuracy: 0.475500 | 1.769 sec/iter\n",
      "Epoch: 398 | Batch: 005 / 011 | Total loss: 1.437 | Reg loss: 0.040 | Tree loss: 1.437 | Accuracy: 0.470500 | 1.769 sec/iter\n",
      "Epoch: 398 | Batch: 006 / 011 | Total loss: 1.405 | Reg loss: 0.040 | Tree loss: 1.405 | Accuracy: 0.527000 | 1.769 sec/iter\n",
      "Epoch: 398 | Batch: 007 / 011 | Total loss: 1.411 | Reg loss: 0.040 | Tree loss: 1.411 | Accuracy: 0.513000 | 1.769 sec/iter\n",
      "Epoch: 398 | Batch: 008 / 011 | Total loss: 1.420 | Reg loss: 0.040 | Tree loss: 1.420 | Accuracy: 0.488500 | 1.768 sec/iter\n",
      "Epoch: 398 | Batch: 009 / 011 | Total loss: 1.388 | Reg loss: 0.040 | Tree loss: 1.388 | Accuracy: 0.510000 | 1.768 sec/iter\n",
      "Epoch: 398 | Batch: 010 / 011 | Total loss: 1.402 | Reg loss: 0.040 | Tree loss: 1.402 | Accuracy: 0.460751 | 1.768 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 399 | Batch: 000 / 011 | Total loss: 1.618 | Reg loss: 0.040 | Tree loss: 1.618 | Accuracy: 0.391000 | 1.769 sec/iter\n",
      "Epoch: 399 | Batch: 001 / 011 | Total loss: 1.569 | Reg loss: 0.040 | Tree loss: 1.569 | Accuracy: 0.414500 | 1.769 sec/iter\n",
      "Epoch: 399 | Batch: 002 / 011 | Total loss: 1.524 | Reg loss: 0.040 | Tree loss: 1.524 | Accuracy: 0.437500 | 1.769 sec/iter\n",
      "Epoch: 399 | Batch: 003 / 011 | Total loss: 1.486 | Reg loss: 0.040 | Tree loss: 1.486 | Accuracy: 0.447000 | 1.769 sec/iter\n",
      "Epoch: 399 | Batch: 004 / 011 | Total loss: 1.471 | Reg loss: 0.040 | Tree loss: 1.471 | Accuracy: 0.453000 | 1.769 sec/iter\n",
      "Epoch: 399 | Batch: 005 / 011 | Total loss: 1.424 | Reg loss: 0.040 | Tree loss: 1.424 | Accuracy: 0.485000 | 1.769 sec/iter\n",
      "Epoch: 399 | Batch: 006 / 011 | Total loss: 1.419 | Reg loss: 0.040 | Tree loss: 1.419 | Accuracy: 0.514000 | 1.769 sec/iter\n",
      "Epoch: 399 | Batch: 007 / 011 | Total loss: 1.410 | Reg loss: 0.040 | Tree loss: 1.410 | Accuracy: 0.515000 | 1.769 sec/iter\n",
      "Epoch: 399 | Batch: 008 / 011 | Total loss: 1.404 | Reg loss: 0.040 | Tree loss: 1.404 | Accuracy: 0.492500 | 1.769 sec/iter\n",
      "Epoch: 399 | Batch: 009 / 011 | Total loss: 1.392 | Reg loss: 0.040 | Tree loss: 1.392 | Accuracy: 0.504500 | 1.769 sec/iter\n",
      "Epoch: 399 | Batch: 010 / 011 | Total loss: 1.423 | Reg loss: 0.040 | Tree loss: 1.423 | Accuracy: 0.470990 | 1.768 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 400 | Batch: 000 / 011 | Total loss: 1.621 | Reg loss: 0.040 | Tree loss: 1.621 | Accuracy: 0.386500 | 1.768 sec/iter\n",
      "Epoch: 400 | Batch: 001 / 011 | Total loss: 1.559 | Reg loss: 0.040 | Tree loss: 1.559 | Accuracy: 0.418500 | 1.768 sec/iter\n",
      "Epoch: 400 | Batch: 002 / 011 | Total loss: 1.545 | Reg loss: 0.040 | Tree loss: 1.545 | Accuracy: 0.436500 | 1.768 sec/iter\n",
      "Epoch: 400 | Batch: 003 / 011 | Total loss: 1.479 | Reg loss: 0.040 | Tree loss: 1.479 | Accuracy: 0.475500 | 1.768 sec/iter\n",
      "Epoch: 400 | Batch: 004 / 011 | Total loss: 1.462 | Reg loss: 0.040 | Tree loss: 1.462 | Accuracy: 0.487500 | 1.768 sec/iter\n",
      "Epoch: 400 | Batch: 005 / 011 | Total loss: 1.439 | Reg loss: 0.040 | Tree loss: 1.439 | Accuracy: 0.489500 | 1.768 sec/iter\n",
      "Epoch: 400 | Batch: 006 / 011 | Total loss: 1.398 | Reg loss: 0.040 | Tree loss: 1.398 | Accuracy: 0.528500 | 1.768 sec/iter\n",
      "Epoch: 400 | Batch: 007 / 011 | Total loss: 1.406 | Reg loss: 0.040 | Tree loss: 1.406 | Accuracy: 0.513000 | 1.768 sec/iter\n",
      "Epoch: 400 | Batch: 008 / 011 | Total loss: 1.421 | Reg loss: 0.040 | Tree loss: 1.421 | Accuracy: 0.485000 | 1.768 sec/iter\n",
      "Epoch: 400 | Batch: 009 / 011 | Total loss: 1.390 | Reg loss: 0.040 | Tree loss: 1.390 | Accuracy: 0.514000 | 1.768 sec/iter\n",
      "Epoch: 400 | Batch: 010 / 011 | Total loss: 1.412 | Reg loss: 0.040 | Tree loss: 1.412 | Accuracy: 0.498294 | 1.767 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 401 | Batch: 000 / 011 | Total loss: 1.635 | Reg loss: 0.040 | Tree loss: 1.635 | Accuracy: 0.372500 | 1.769 sec/iter\n",
      "Epoch: 401 | Batch: 001 / 011 | Total loss: 1.593 | Reg loss: 0.040 | Tree loss: 1.593 | Accuracy: 0.383000 | 1.768 sec/iter\n",
      "Epoch: 401 | Batch: 002 / 011 | Total loss: 1.512 | Reg loss: 0.040 | Tree loss: 1.512 | Accuracy: 0.449000 | 1.768 sec/iter\n",
      "Epoch: 401 | Batch: 003 / 011 | Total loss: 1.503 | Reg loss: 0.040 | Tree loss: 1.503 | Accuracy: 0.466500 | 1.768 sec/iter\n",
      "Epoch: 401 | Batch: 004 / 011 | Total loss: 1.452 | Reg loss: 0.040 | Tree loss: 1.452 | Accuracy: 0.467000 | 1.768 sec/iter\n",
      "Epoch: 401 | Batch: 005 / 011 | Total loss: 1.422 | Reg loss: 0.040 | Tree loss: 1.422 | Accuracy: 0.497000 | 1.768 sec/iter\n",
      "Epoch: 401 | Batch: 006 / 011 | Total loss: 1.425 | Reg loss: 0.040 | Tree loss: 1.425 | Accuracy: 0.493500 | 1.768 sec/iter\n",
      "Epoch: 401 | Batch: 007 / 011 | Total loss: 1.379 | Reg loss: 0.040 | Tree loss: 1.379 | Accuracy: 0.547000 | 1.768 sec/iter\n",
      "Epoch: 401 | Batch: 008 / 011 | Total loss: 1.418 | Reg loss: 0.040 | Tree loss: 1.418 | Accuracy: 0.495000 | 1.768 sec/iter\n",
      "Epoch: 401 | Batch: 009 / 011 | Total loss: 1.398 | Reg loss: 0.040 | Tree loss: 1.398 | Accuracy: 0.517000 | 1.768 sec/iter\n",
      "Epoch: 401 | Batch: 010 / 011 | Total loss: 1.386 | Reg loss: 0.040 | Tree loss: 1.386 | Accuracy: 0.511945 | 1.768 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 402 | Batch: 000 / 011 | Total loss: 1.594 | Reg loss: 0.040 | Tree loss: 1.594 | Accuracy: 0.410500 | 1.768 sec/iter\n",
      "Epoch: 402 | Batch: 001 / 011 | Total loss: 1.594 | Reg loss: 0.040 | Tree loss: 1.594 | Accuracy: 0.398000 | 1.767 sec/iter\n",
      "Epoch: 402 | Batch: 002 / 011 | Total loss: 1.537 | Reg loss: 0.040 | Tree loss: 1.537 | Accuracy: 0.413000 | 1.767 sec/iter\n",
      "Epoch: 402 | Batch: 003 / 011 | Total loss: 1.487 | Reg loss: 0.040 | Tree loss: 1.487 | Accuracy: 0.463000 | 1.767 sec/iter\n",
      "Epoch: 402 | Batch: 004 / 011 | Total loss: 1.464 | Reg loss: 0.040 | Tree loss: 1.464 | Accuracy: 0.482500 | 1.767 sec/iter\n",
      "Epoch: 402 | Batch: 005 / 011 | Total loss: 1.431 | Reg loss: 0.040 | Tree loss: 1.431 | Accuracy: 0.481000 | 1.767 sec/iter\n",
      "Epoch: 402 | Batch: 006 / 011 | Total loss: 1.420 | Reg loss: 0.040 | Tree loss: 1.420 | Accuracy: 0.497500 | 1.767 sec/iter\n",
      "Epoch: 402 | Batch: 007 / 011 | Total loss: 1.404 | Reg loss: 0.040 | Tree loss: 1.404 | Accuracy: 0.505000 | 1.767 sec/iter\n",
      "Epoch: 402 | Batch: 008 / 011 | Total loss: 1.405 | Reg loss: 0.040 | Tree loss: 1.405 | Accuracy: 0.506000 | 1.767 sec/iter\n",
      "Epoch: 402 | Batch: 009 / 011 | Total loss: 1.413 | Reg loss: 0.040 | Tree loss: 1.413 | Accuracy: 0.509500 | 1.766 sec/iter\n",
      "Epoch: 402 | Batch: 010 / 011 | Total loss: 1.452 | Reg loss: 0.040 | Tree loss: 1.452 | Accuracy: 0.436860 | 1.766 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 403 | Batch: 000 / 011 | Total loss: 1.594 | Reg loss: 0.040 | Tree loss: 1.594 | Accuracy: 0.398500 | 1.767 sec/iter\n",
      "Epoch: 403 | Batch: 001 / 011 | Total loss: 1.575 | Reg loss: 0.040 | Tree loss: 1.575 | Accuracy: 0.398500 | 1.767 sec/iter\n",
      "Epoch: 403 | Batch: 002 / 011 | Total loss: 1.562 | Reg loss: 0.040 | Tree loss: 1.562 | Accuracy: 0.422500 | 1.767 sec/iter\n",
      "Epoch: 403 | Batch: 003 / 011 | Total loss: 1.482 | Reg loss: 0.040 | Tree loss: 1.482 | Accuracy: 0.470500 | 1.767 sec/iter\n",
      "Epoch: 403 | Batch: 004 / 011 | Total loss: 1.470 | Reg loss: 0.040 | Tree loss: 1.470 | Accuracy: 0.473000 | 1.767 sec/iter\n",
      "Epoch: 403 | Batch: 005 / 011 | Total loss: 1.425 | Reg loss: 0.040 | Tree loss: 1.425 | Accuracy: 0.506500 | 1.766 sec/iter\n",
      "Epoch: 403 | Batch: 006 / 011 | Total loss: 1.427 | Reg loss: 0.040 | Tree loss: 1.427 | Accuracy: 0.502500 | 1.766 sec/iter\n",
      "Epoch: 403 | Batch: 007 / 011 | Total loss: 1.412 | Reg loss: 0.040 | Tree loss: 1.412 | Accuracy: 0.528000 | 1.766 sec/iter\n",
      "Epoch: 403 | Batch: 008 / 011 | Total loss: 1.396 | Reg loss: 0.040 | Tree loss: 1.396 | Accuracy: 0.511000 | 1.766 sec/iter\n",
      "Epoch: 403 | Batch: 009 / 011 | Total loss: 1.401 | Reg loss: 0.040 | Tree loss: 1.401 | Accuracy: 0.511500 | 1.766 sec/iter\n",
      "Epoch: 403 | Batch: 010 / 011 | Total loss: 1.451 | Reg loss: 0.040 | Tree loss: 1.451 | Accuracy: 0.450512 | 1.766 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 404 | Batch: 000 / 011 | Total loss: 1.588 | Reg loss: 0.040 | Tree loss: 1.588 | Accuracy: 0.405000 | 1.766 sec/iter\n",
      "Epoch: 404 | Batch: 001 / 011 | Total loss: 1.574 | Reg loss: 0.040 | Tree loss: 1.574 | Accuracy: 0.414500 | 1.766 sec/iter\n",
      "Epoch: 404 | Batch: 002 / 011 | Total loss: 1.546 | Reg loss: 0.040 | Tree loss: 1.546 | Accuracy: 0.426500 | 1.766 sec/iter\n",
      "Epoch: 404 | Batch: 003 / 011 | Total loss: 1.498 | Reg loss: 0.040 | Tree loss: 1.498 | Accuracy: 0.458000 | 1.766 sec/iter\n",
      "Epoch: 404 | Batch: 004 / 011 | Total loss: 1.452 | Reg loss: 0.040 | Tree loss: 1.452 | Accuracy: 0.467000 | 1.765 sec/iter\n",
      "Epoch: 404 | Batch: 005 / 011 | Total loss: 1.425 | Reg loss: 0.040 | Tree loss: 1.425 | Accuracy: 0.491500 | 1.765 sec/iter\n",
      "Epoch: 404 | Batch: 006 / 011 | Total loss: 1.403 | Reg loss: 0.040 | Tree loss: 1.403 | Accuracy: 0.506000 | 1.765 sec/iter\n",
      "Epoch: 404 | Batch: 007 / 011 | Total loss: 1.418 | Reg loss: 0.040 | Tree loss: 1.418 | Accuracy: 0.520000 | 1.765 sec/iter\n",
      "Epoch: 404 | Batch: 008 / 011 | Total loss: 1.394 | Reg loss: 0.040 | Tree loss: 1.394 | Accuracy: 0.509500 | 1.765 sec/iter\n",
      "Epoch: 404 | Batch: 009 / 011 | Total loss: 1.434 | Reg loss: 0.040 | Tree loss: 1.434 | Accuracy: 0.494000 | 1.765 sec/iter\n",
      "Epoch: 404 | Batch: 010 / 011 | Total loss: 1.380 | Reg loss: 0.040 | Tree loss: 1.380 | Accuracy: 0.464164 | 1.765 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 405 | Batch: 000 / 011 | Total loss: 1.592 | Reg loss: 0.040 | Tree loss: 1.592 | Accuracy: 0.410000 | 1.766 sec/iter\n",
      "Epoch: 405 | Batch: 001 / 011 | Total loss: 1.592 | Reg loss: 0.040 | Tree loss: 1.592 | Accuracy: 0.389000 | 1.766 sec/iter\n",
      "Epoch: 405 | Batch: 002 / 011 | Total loss: 1.527 | Reg loss: 0.040 | Tree loss: 1.527 | Accuracy: 0.442000 | 1.766 sec/iter\n",
      "Epoch: 405 | Batch: 003 / 011 | Total loss: 1.488 | Reg loss: 0.040 | Tree loss: 1.488 | Accuracy: 0.462000 | 1.766 sec/iter\n",
      "Epoch: 405 | Batch: 004 / 011 | Total loss: 1.453 | Reg loss: 0.040 | Tree loss: 1.453 | Accuracy: 0.474000 | 1.766 sec/iter\n",
      "Epoch: 405 | Batch: 005 / 011 | Total loss: 1.444 | Reg loss: 0.040 | Tree loss: 1.444 | Accuracy: 0.492000 | 1.766 sec/iter\n",
      "Epoch: 405 | Batch: 006 / 011 | Total loss: 1.442 | Reg loss: 0.040 | Tree loss: 1.442 | Accuracy: 0.507500 | 1.766 sec/iter\n",
      "Epoch: 405 | Batch: 007 / 011 | Total loss: 1.393 | Reg loss: 0.040 | Tree loss: 1.393 | Accuracy: 0.530500 | 1.766 sec/iter\n",
      "Epoch: 405 | Batch: 008 / 011 | Total loss: 1.400 | Reg loss: 0.040 | Tree loss: 1.400 | Accuracy: 0.512000 | 1.766 sec/iter\n",
      "Epoch: 405 | Batch: 009 / 011 | Total loss: 1.414 | Reg loss: 0.040 | Tree loss: 1.414 | Accuracy: 0.494500 | 1.766 sec/iter\n",
      "Epoch: 405 | Batch: 010 / 011 | Total loss: 1.370 | Reg loss: 0.040 | Tree loss: 1.370 | Accuracy: 0.529010 | 1.765 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 406 | Batch: 000 / 011 | Total loss: 1.582 | Reg loss: 0.040 | Tree loss: 1.582 | Accuracy: 0.407000 | 1.765 sec/iter\n",
      "Epoch: 406 | Batch: 001 / 011 | Total loss: 1.580 | Reg loss: 0.040 | Tree loss: 1.580 | Accuracy: 0.418000 | 1.765 sec/iter\n",
      "Epoch: 406 | Batch: 002 / 011 | Total loss: 1.531 | Reg loss: 0.040 | Tree loss: 1.531 | Accuracy: 0.440000 | 1.765 sec/iter\n",
      "Epoch: 406 | Batch: 003 / 011 | Total loss: 1.494 | Reg loss: 0.040 | Tree loss: 1.494 | Accuracy: 0.464000 | 1.765 sec/iter\n",
      "Epoch: 406 | Batch: 004 / 011 | Total loss: 1.473 | Reg loss: 0.040 | Tree loss: 1.473 | Accuracy: 0.469000 | 1.765 sec/iter\n",
      "Epoch: 406 | Batch: 005 / 011 | Total loss: 1.420 | Reg loss: 0.040 | Tree loss: 1.420 | Accuracy: 0.503000 | 1.765 sec/iter\n",
      "Epoch: 406 | Batch: 006 / 011 | Total loss: 1.393 | Reg loss: 0.040 | Tree loss: 1.393 | Accuracy: 0.527500 | 1.765 sec/iter\n",
      "Epoch: 406 | Batch: 007 / 011 | Total loss: 1.439 | Reg loss: 0.040 | Tree loss: 1.439 | Accuracy: 0.516000 | 1.765 sec/iter\n",
      "Epoch: 406 | Batch: 008 / 011 | Total loss: 1.407 | Reg loss: 0.040 | Tree loss: 1.407 | Accuracy: 0.508000 | 1.765 sec/iter\n",
      "Epoch: 406 | Batch: 009 / 011 | Total loss: 1.394 | Reg loss: 0.040 | Tree loss: 1.394 | Accuracy: 0.503500 | 1.765 sec/iter\n",
      "Epoch: 406 | Batch: 010 / 011 | Total loss: 1.364 | Reg loss: 0.040 | Tree loss: 1.364 | Accuracy: 0.505119 | 1.765 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 407 | Batch: 000 / 011 | Total loss: 1.612 | Reg loss: 0.040 | Tree loss: 1.612 | Accuracy: 0.406500 | 1.766 sec/iter\n",
      "Epoch: 407 | Batch: 001 / 011 | Total loss: 1.553 | Reg loss: 0.040 | Tree loss: 1.553 | Accuracy: 0.416000 | 1.766 sec/iter\n",
      "Epoch: 407 | Batch: 002 / 011 | Total loss: 1.527 | Reg loss: 0.040 | Tree loss: 1.527 | Accuracy: 0.436000 | 1.766 sec/iter\n",
      "Epoch: 407 | Batch: 003 / 011 | Total loss: 1.513 | Reg loss: 0.040 | Tree loss: 1.513 | Accuracy: 0.440500 | 1.766 sec/iter\n",
      "Epoch: 407 | Batch: 004 / 011 | Total loss: 1.460 | Reg loss: 0.040 | Tree loss: 1.460 | Accuracy: 0.453500 | 1.766 sec/iter\n",
      "Epoch: 407 | Batch: 005 / 011 | Total loss: 1.425 | Reg loss: 0.040 | Tree loss: 1.425 | Accuracy: 0.504000 | 1.766 sec/iter\n",
      "Epoch: 407 | Batch: 006 / 011 | Total loss: 1.433 | Reg loss: 0.040 | Tree loss: 1.433 | Accuracy: 0.504000 | 1.766 sec/iter\n",
      "Epoch: 407 | Batch: 007 / 011 | Total loss: 1.397 | Reg loss: 0.040 | Tree loss: 1.397 | Accuracy: 0.532000 | 1.766 sec/iter\n",
      "Epoch: 407 | Batch: 008 / 011 | Total loss: 1.390 | Reg loss: 0.040 | Tree loss: 1.390 | Accuracy: 0.521000 | 1.766 sec/iter\n",
      "Epoch: 407 | Batch: 009 / 011 | Total loss: 1.406 | Reg loss: 0.040 | Tree loss: 1.406 | Accuracy: 0.509500 | 1.766 sec/iter\n",
      "Epoch: 407 | Batch: 010 / 011 | Total loss: 1.394 | Reg loss: 0.040 | Tree loss: 1.394 | Accuracy: 0.518771 | 1.765 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 408 | Batch: 000 / 011 | Total loss: 1.594 | Reg loss: 0.040 | Tree loss: 1.594 | Accuracy: 0.397500 | 1.765 sec/iter\n",
      "Epoch: 408 | Batch: 001 / 011 | Total loss: 1.574 | Reg loss: 0.040 | Tree loss: 1.574 | Accuracy: 0.398500 | 1.765 sec/iter\n",
      "Epoch: 408 | Batch: 002 / 011 | Total loss: 1.524 | Reg loss: 0.040 | Tree loss: 1.524 | Accuracy: 0.439000 | 1.765 sec/iter\n",
      "Epoch: 408 | Batch: 003 / 011 | Total loss: 1.495 | Reg loss: 0.040 | Tree loss: 1.495 | Accuracy: 0.478000 | 1.765 sec/iter\n",
      "Epoch: 408 | Batch: 004 / 011 | Total loss: 1.456 | Reg loss: 0.040 | Tree loss: 1.456 | Accuracy: 0.499000 | 1.765 sec/iter\n",
      "Epoch: 408 | Batch: 005 / 011 | Total loss: 1.442 | Reg loss: 0.040 | Tree loss: 1.442 | Accuracy: 0.505500 | 1.765 sec/iter\n",
      "Epoch: 408 | Batch: 006 / 011 | Total loss: 1.424 | Reg loss: 0.040 | Tree loss: 1.424 | Accuracy: 0.507500 | 1.765 sec/iter\n",
      "Epoch: 408 | Batch: 007 / 011 | Total loss: 1.414 | Reg loss: 0.040 | Tree loss: 1.414 | Accuracy: 0.527000 | 1.765 sec/iter\n",
      "Epoch: 408 | Batch: 008 / 011 | Total loss: 1.408 | Reg loss: 0.040 | Tree loss: 1.408 | Accuracy: 0.513000 | 1.765 sec/iter\n",
      "Epoch: 408 | Batch: 009 / 011 | Total loss: 1.390 | Reg loss: 0.040 | Tree loss: 1.390 | Accuracy: 0.515000 | 1.765 sec/iter\n",
      "Epoch: 408 | Batch: 010 / 011 | Total loss: 1.434 | Reg loss: 0.040 | Tree loss: 1.434 | Accuracy: 0.467577 | 1.764 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 409 | Batch: 000 / 011 | Total loss: 1.595 | Reg loss: 0.040 | Tree loss: 1.595 | Accuracy: 0.403500 | 1.766 sec/iter\n",
      "Epoch: 409 | Batch: 001 / 011 | Total loss: 1.569 | Reg loss: 0.040 | Tree loss: 1.569 | Accuracy: 0.411500 | 1.766 sec/iter\n",
      "Epoch: 409 | Batch: 002 / 011 | Total loss: 1.534 | Reg loss: 0.040 | Tree loss: 1.534 | Accuracy: 0.432500 | 1.766 sec/iter\n",
      "Epoch: 409 | Batch: 003 / 011 | Total loss: 1.506 | Reg loss: 0.040 | Tree loss: 1.506 | Accuracy: 0.437500 | 1.766 sec/iter\n",
      "Epoch: 409 | Batch: 004 / 011 | Total loss: 1.471 | Reg loss: 0.040 | Tree loss: 1.471 | Accuracy: 0.462000 | 1.766 sec/iter\n",
      "Epoch: 409 | Batch: 005 / 011 | Total loss: 1.436 | Reg loss: 0.040 | Tree loss: 1.436 | Accuracy: 0.478500 | 1.765 sec/iter\n",
      "Epoch: 409 | Batch: 006 / 011 | Total loss: 1.396 | Reg loss: 0.040 | Tree loss: 1.396 | Accuracy: 0.532500 | 1.765 sec/iter\n",
      "Epoch: 409 | Batch: 007 / 011 | Total loss: 1.414 | Reg loss: 0.040 | Tree loss: 1.414 | Accuracy: 0.518000 | 1.765 sec/iter\n",
      "Epoch: 409 | Batch: 008 / 011 | Total loss: 1.398 | Reg loss: 0.040 | Tree loss: 1.398 | Accuracy: 0.499000 | 1.765 sec/iter\n",
      "Epoch: 409 | Batch: 009 / 011 | Total loss: 1.427 | Reg loss: 0.040 | Tree loss: 1.427 | Accuracy: 0.482000 | 1.765 sec/iter\n",
      "Epoch: 409 | Batch: 010 / 011 | Total loss: 1.358 | Reg loss: 0.040 | Tree loss: 1.358 | Accuracy: 0.491468 | 1.764 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 410 | Batch: 000 / 011 | Total loss: 1.605 | Reg loss: 0.040 | Tree loss: 1.605 | Accuracy: 0.405500 | 1.766 sec/iter\n",
      "Epoch: 410 | Batch: 001 / 011 | Total loss: 1.591 | Reg loss: 0.040 | Tree loss: 1.591 | Accuracy: 0.392000 | 1.766 sec/iter\n",
      "Epoch: 410 | Batch: 002 / 011 | Total loss: 1.535 | Reg loss: 0.040 | Tree loss: 1.535 | Accuracy: 0.437000 | 1.766 sec/iter\n",
      "Epoch: 410 | Batch: 003 / 011 | Total loss: 1.502 | Reg loss: 0.040 | Tree loss: 1.502 | Accuracy: 0.435000 | 1.766 sec/iter\n",
      "Epoch: 410 | Batch: 004 / 011 | Total loss: 1.432 | Reg loss: 0.040 | Tree loss: 1.432 | Accuracy: 0.494500 | 1.766 sec/iter\n",
      "Epoch: 410 | Batch: 005 / 011 | Total loss: 1.438 | Reg loss: 0.040 | Tree loss: 1.438 | Accuracy: 0.495000 | 1.766 sec/iter\n",
      "Epoch: 410 | Batch: 006 / 011 | Total loss: 1.432 | Reg loss: 0.040 | Tree loss: 1.432 | Accuracy: 0.499500 | 1.765 sec/iter\n",
      "Epoch: 410 | Batch: 007 / 011 | Total loss: 1.394 | Reg loss: 0.040 | Tree loss: 1.394 | Accuracy: 0.546500 | 1.765 sec/iter\n",
      "Epoch: 410 | Batch: 008 / 011 | Total loss: 1.403 | Reg loss: 0.040 | Tree loss: 1.403 | Accuracy: 0.521000 | 1.765 sec/iter\n",
      "Epoch: 410 | Batch: 009 / 011 | Total loss: 1.411 | Reg loss: 0.040 | Tree loss: 1.411 | Accuracy: 0.501000 | 1.765 sec/iter\n",
      "Epoch: 410 | Batch: 010 / 011 | Total loss: 1.354 | Reg loss: 0.040 | Tree loss: 1.354 | Accuracy: 0.515358 | 1.765 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 411 | Batch: 000 / 011 | Total loss: 1.625 | Reg loss: 0.040 | Tree loss: 1.625 | Accuracy: 0.393000 | 1.765 sec/iter\n",
      "Epoch: 411 | Batch: 001 / 011 | Total loss: 1.578 | Reg loss: 0.040 | Tree loss: 1.578 | Accuracy: 0.406000 | 1.765 sec/iter\n",
      "Epoch: 411 | Batch: 002 / 011 | Total loss: 1.560 | Reg loss: 0.040 | Tree loss: 1.560 | Accuracy: 0.408000 | 1.765 sec/iter\n",
      "Epoch: 411 | Batch: 003 / 011 | Total loss: 1.480 | Reg loss: 0.040 | Tree loss: 1.480 | Accuracy: 0.457500 | 1.765 sec/iter\n",
      "Epoch: 411 | Batch: 004 / 011 | Total loss: 1.455 | Reg loss: 0.040 | Tree loss: 1.455 | Accuracy: 0.467500 | 1.765 sec/iter\n",
      "Epoch: 411 | Batch: 005 / 011 | Total loss: 1.424 | Reg loss: 0.040 | Tree loss: 1.424 | Accuracy: 0.496500 | 1.765 sec/iter\n",
      "Epoch: 411 | Batch: 006 / 011 | Total loss: 1.407 | Reg loss: 0.040 | Tree loss: 1.407 | Accuracy: 0.523500 | 1.765 sec/iter\n",
      "Epoch: 411 | Batch: 007 / 011 | Total loss: 1.403 | Reg loss: 0.040 | Tree loss: 1.403 | Accuracy: 0.524000 | 1.765 sec/iter\n",
      "Epoch: 411 | Batch: 008 / 011 | Total loss: 1.402 | Reg loss: 0.040 | Tree loss: 1.402 | Accuracy: 0.520000 | 1.765 sec/iter\n",
      "Epoch: 411 | Batch: 009 / 011 | Total loss: 1.415 | Reg loss: 0.040 | Tree loss: 1.415 | Accuracy: 0.489000 | 1.764 sec/iter\n",
      "Epoch: 411 | Batch: 010 / 011 | Total loss: 1.375 | Reg loss: 0.040 | Tree loss: 1.375 | Accuracy: 0.488055 | 1.764 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 412 | Batch: 000 / 011 | Total loss: 1.598 | Reg loss: 0.040 | Tree loss: 1.598 | Accuracy: 0.406500 | 1.766 sec/iter\n",
      "Epoch: 412 | Batch: 001 / 011 | Total loss: 1.556 | Reg loss: 0.040 | Tree loss: 1.556 | Accuracy: 0.422500 | 1.766 sec/iter\n",
      "Epoch: 412 | Batch: 002 / 011 | Total loss: 1.553 | Reg loss: 0.040 | Tree loss: 1.553 | Accuracy: 0.425000 | 1.766 sec/iter\n",
      "Epoch: 412 | Batch: 003 / 011 | Total loss: 1.507 | Reg loss: 0.040 | Tree loss: 1.507 | Accuracy: 0.456500 | 1.766 sec/iter\n",
      "Epoch: 412 | Batch: 004 / 011 | Total loss: 1.433 | Reg loss: 0.040 | Tree loss: 1.433 | Accuracy: 0.504500 | 1.765 sec/iter\n",
      "Epoch: 412 | Batch: 005 / 011 | Total loss: 1.425 | Reg loss: 0.040 | Tree loss: 1.425 | Accuracy: 0.506500 | 1.765 sec/iter\n",
      "Epoch: 412 | Batch: 006 / 011 | Total loss: 1.431 | Reg loss: 0.040 | Tree loss: 1.431 | Accuracy: 0.534500 | 1.765 sec/iter\n",
      "Epoch: 412 | Batch: 007 / 011 | Total loss: 1.424 | Reg loss: 0.040 | Tree loss: 1.424 | Accuracy: 0.512000 | 1.765 sec/iter\n",
      "Epoch: 412 | Batch: 008 / 011 | Total loss: 1.406 | Reg loss: 0.040 | Tree loss: 1.406 | Accuracy: 0.529500 | 1.765 sec/iter\n",
      "Epoch: 412 | Batch: 009 / 011 | Total loss: 1.394 | Reg loss: 0.040 | Tree loss: 1.394 | Accuracy: 0.518000 | 1.765 sec/iter\n",
      "Epoch: 412 | Batch: 010 / 011 | Total loss: 1.442 | Reg loss: 0.040 | Tree loss: 1.442 | Accuracy: 0.481229 | 1.765 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 413 | Batch: 000 / 011 | Total loss: 1.606 | Reg loss: 0.040 | Tree loss: 1.606 | Accuracy: 0.410000 | 1.765 sec/iter\n",
      "Epoch: 413 | Batch: 001 / 011 | Total loss: 1.563 | Reg loss: 0.040 | Tree loss: 1.563 | Accuracy: 0.415000 | 1.765 sec/iter\n",
      "Epoch: 413 | Batch: 002 / 011 | Total loss: 1.541 | Reg loss: 0.040 | Tree loss: 1.541 | Accuracy: 0.432500 | 1.765 sec/iter\n",
      "Epoch: 413 | Batch: 003 / 011 | Total loss: 1.492 | Reg loss: 0.040 | Tree loss: 1.492 | Accuracy: 0.453000 | 1.765 sec/iter\n",
      "Epoch: 413 | Batch: 004 / 011 | Total loss: 1.455 | Reg loss: 0.040 | Tree loss: 1.455 | Accuracy: 0.467500 | 1.765 sec/iter\n",
      "Epoch: 413 | Batch: 005 / 011 | Total loss: 1.425 | Reg loss: 0.040 | Tree loss: 1.425 | Accuracy: 0.490500 | 1.765 sec/iter\n",
      "Epoch: 413 | Batch: 006 / 011 | Total loss: 1.419 | Reg loss: 0.040 | Tree loss: 1.419 | Accuracy: 0.516000 | 1.765 sec/iter\n",
      "Epoch: 413 | Batch: 007 / 011 | Total loss: 1.418 | Reg loss: 0.040 | Tree loss: 1.418 | Accuracy: 0.519500 | 1.764 sec/iter\n",
      "Epoch: 413 | Batch: 008 / 011 | Total loss: 1.425 | Reg loss: 0.040 | Tree loss: 1.425 | Accuracy: 0.498000 | 1.764 sec/iter\n",
      "Epoch: 413 | Batch: 009 / 011 | Total loss: 1.404 | Reg loss: 0.040 | Tree loss: 1.404 | Accuracy: 0.488500 | 1.764 sec/iter\n",
      "Epoch: 413 | Batch: 010 / 011 | Total loss: 1.376 | Reg loss: 0.040 | Tree loss: 1.376 | Accuracy: 0.477816 | 1.764 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 414 | Batch: 000 / 011 | Total loss: 1.578 | Reg loss: 0.040 | Tree loss: 1.578 | Accuracy: 0.418000 | 1.766 sec/iter\n",
      "Epoch: 414 | Batch: 001 / 011 | Total loss: 1.581 | Reg loss: 0.040 | Tree loss: 1.581 | Accuracy: 0.414500 | 1.766 sec/iter\n",
      "Epoch: 414 | Batch: 002 / 011 | Total loss: 1.547 | Reg loss: 0.040 | Tree loss: 1.547 | Accuracy: 0.415500 | 1.765 sec/iter\n",
      "Epoch: 414 | Batch: 003 / 011 | Total loss: 1.490 | Reg loss: 0.040 | Tree loss: 1.490 | Accuracy: 0.471500 | 1.765 sec/iter\n",
      "Epoch: 414 | Batch: 004 / 011 | Total loss: 1.451 | Reg loss: 0.040 | Tree loss: 1.451 | Accuracy: 0.491500 | 1.765 sec/iter\n",
      "Epoch: 414 | Batch: 005 / 011 | Total loss: 1.425 | Reg loss: 0.040 | Tree loss: 1.425 | Accuracy: 0.512000 | 1.765 sec/iter\n",
      "Epoch: 414 | Batch: 006 / 011 | Total loss: 1.424 | Reg loss: 0.040 | Tree loss: 1.424 | Accuracy: 0.515000 | 1.765 sec/iter\n",
      "Epoch: 414 | Batch: 007 / 011 | Total loss: 1.394 | Reg loss: 0.040 | Tree loss: 1.394 | Accuracy: 0.546500 | 1.765 sec/iter\n",
      "Epoch: 414 | Batch: 008 / 011 | Total loss: 1.415 | Reg loss: 0.040 | Tree loss: 1.415 | Accuracy: 0.519500 | 1.765 sec/iter\n",
      "Epoch: 414 | Batch: 009 / 011 | Total loss: 1.424 | Reg loss: 0.040 | Tree loss: 1.424 | Accuracy: 0.499000 | 1.765 sec/iter\n",
      "Epoch: 414 | Batch: 010 / 011 | Total loss: 1.447 | Reg loss: 0.040 | Tree loss: 1.447 | Accuracy: 0.491468 | 1.765 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 415 | Batch: 000 / 011 | Total loss: 1.619 | Reg loss: 0.040 | Tree loss: 1.619 | Accuracy: 0.404000 | 1.765 sec/iter\n",
      "Epoch: 415 | Batch: 001 / 011 | Total loss: 1.564 | Reg loss: 0.040 | Tree loss: 1.564 | Accuracy: 0.410000 | 1.765 sec/iter\n",
      "Epoch: 415 | Batch: 002 / 011 | Total loss: 1.521 | Reg loss: 0.040 | Tree loss: 1.521 | Accuracy: 0.438500 | 1.765 sec/iter\n",
      "Epoch: 415 | Batch: 003 / 011 | Total loss: 1.500 | Reg loss: 0.040 | Tree loss: 1.500 | Accuracy: 0.454000 | 1.765 sec/iter\n",
      "Epoch: 415 | Batch: 004 / 011 | Total loss: 1.483 | Reg loss: 0.040 | Tree loss: 1.483 | Accuracy: 0.472500 | 1.765 sec/iter\n",
      "Epoch: 415 | Batch: 005 / 011 | Total loss: 1.415 | Reg loss: 0.040 | Tree loss: 1.415 | Accuracy: 0.501000 | 1.764 sec/iter\n",
      "Epoch: 415 | Batch: 006 / 011 | Total loss: 1.385 | Reg loss: 0.040 | Tree loss: 1.385 | Accuracy: 0.531000 | 1.764 sec/iter\n",
      "Epoch: 415 | Batch: 007 / 011 | Total loss: 1.398 | Reg loss: 0.040 | Tree loss: 1.398 | Accuracy: 0.512500 | 1.764 sec/iter\n",
      "Epoch: 415 | Batch: 008 / 011 | Total loss: 1.412 | Reg loss: 0.040 | Tree loss: 1.412 | Accuracy: 0.493000 | 1.764 sec/iter\n",
      "Epoch: 415 | Batch: 009 / 011 | Total loss: 1.412 | Reg loss: 0.040 | Tree loss: 1.412 | Accuracy: 0.496000 | 1.764 sec/iter\n",
      "Epoch: 415 | Batch: 010 / 011 | Total loss: 1.449 | Reg loss: 0.040 | Tree loss: 1.449 | Accuracy: 0.467577 | 1.764 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 416 | Batch: 000 / 011 | Total loss: 1.607 | Reg loss: 0.040 | Tree loss: 1.607 | Accuracy: 0.398000 | 1.766 sec/iter\n",
      "Epoch: 416 | Batch: 001 / 011 | Total loss: 1.581 | Reg loss: 0.040 | Tree loss: 1.581 | Accuracy: 0.418000 | 1.765 sec/iter\n",
      "Epoch: 416 | Batch: 002 / 011 | Total loss: 1.537 | Reg loss: 0.040 | Tree loss: 1.537 | Accuracy: 0.415000 | 1.765 sec/iter\n",
      "Epoch: 416 | Batch: 003 / 011 | Total loss: 1.473 | Reg loss: 0.040 | Tree loss: 1.473 | Accuracy: 0.463000 | 1.765 sec/iter\n",
      "Epoch: 416 | Batch: 004 / 011 | Total loss: 1.462 | Reg loss: 0.040 | Tree loss: 1.462 | Accuracy: 0.474000 | 1.765 sec/iter\n",
      "Epoch: 416 | Batch: 005 / 011 | Total loss: 1.430 | Reg loss: 0.040 | Tree loss: 1.430 | Accuracy: 0.485000 | 1.765 sec/iter\n",
      "Epoch: 416 | Batch: 006 / 011 | Total loss: 1.424 | Reg loss: 0.040 | Tree loss: 1.424 | Accuracy: 0.508500 | 1.765 sec/iter\n",
      "Epoch: 416 | Batch: 007 / 011 | Total loss: 1.406 | Reg loss: 0.040 | Tree loss: 1.406 | Accuracy: 0.520500 | 1.764 sec/iter\n",
      "Epoch: 416 | Batch: 008 / 011 | Total loss: 1.391 | Reg loss: 0.040 | Tree loss: 1.391 | Accuracy: 0.521000 | 1.764 sec/iter\n",
      "Epoch: 416 | Batch: 009 / 011 | Total loss: 1.418 | Reg loss: 0.040 | Tree loss: 1.418 | Accuracy: 0.465000 | 1.764 sec/iter\n",
      "Epoch: 416 | Batch: 010 / 011 | Total loss: 1.421 | Reg loss: 0.040 | Tree loss: 1.421 | Accuracy: 0.488055 | 1.764 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 417 | Batch: 000 / 011 | Total loss: 1.612 | Reg loss: 0.040 | Tree loss: 1.612 | Accuracy: 0.395000 | 1.765 sec/iter\n",
      "Epoch: 417 | Batch: 001 / 011 | Total loss: 1.581 | Reg loss: 0.040 | Tree loss: 1.581 | Accuracy: 0.427000 | 1.765 sec/iter\n",
      "Epoch: 417 | Batch: 002 / 011 | Total loss: 1.543 | Reg loss: 0.040 | Tree loss: 1.543 | Accuracy: 0.435000 | 1.765 sec/iter\n",
      "Epoch: 417 | Batch: 003 / 011 | Total loss: 1.487 | Reg loss: 0.040 | Tree loss: 1.487 | Accuracy: 0.449500 | 1.765 sec/iter\n",
      "Epoch: 417 | Batch: 004 / 011 | Total loss: 1.453 | Reg loss: 0.040 | Tree loss: 1.453 | Accuracy: 0.475000 | 1.765 sec/iter\n",
      "Epoch: 417 | Batch: 005 / 011 | Total loss: 1.438 | Reg loss: 0.040 | Tree loss: 1.438 | Accuracy: 0.501000 | 1.765 sec/iter\n",
      "Epoch: 417 | Batch: 006 / 011 | Total loss: 1.417 | Reg loss: 0.040 | Tree loss: 1.417 | Accuracy: 0.515000 | 1.765 sec/iter\n",
      "Epoch: 417 | Batch: 007 / 011 | Total loss: 1.381 | Reg loss: 0.040 | Tree loss: 1.381 | Accuracy: 0.534500 | 1.765 sec/iter\n",
      "Epoch: 417 | Batch: 008 / 011 | Total loss: 1.403 | Reg loss: 0.040 | Tree loss: 1.403 | Accuracy: 0.513000 | 1.765 sec/iter\n",
      "Epoch: 417 | Batch: 009 / 011 | Total loss: 1.432 | Reg loss: 0.040 | Tree loss: 1.432 | Accuracy: 0.467500 | 1.765 sec/iter\n",
      "Epoch: 417 | Batch: 010 / 011 | Total loss: 1.403 | Reg loss: 0.040 | Tree loss: 1.403 | Accuracy: 0.494881 | 1.765 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 418 | Batch: 000 / 011 | Total loss: 1.597 | Reg loss: 0.040 | Tree loss: 1.597 | Accuracy: 0.405000 | 1.765 sec/iter\n",
      "Epoch: 418 | Batch: 001 / 011 | Total loss: 1.559 | Reg loss: 0.040 | Tree loss: 1.559 | Accuracy: 0.426000 | 1.765 sec/iter\n",
      "Epoch: 418 | Batch: 002 / 011 | Total loss: 1.543 | Reg loss: 0.040 | Tree loss: 1.543 | Accuracy: 0.414500 | 1.764 sec/iter\n",
      "Epoch: 418 | Batch: 003 / 011 | Total loss: 1.492 | Reg loss: 0.040 | Tree loss: 1.492 | Accuracy: 0.447500 | 1.764 sec/iter\n",
      "Epoch: 418 | Batch: 004 / 011 | Total loss: 1.450 | Reg loss: 0.040 | Tree loss: 1.450 | Accuracy: 0.482000 | 1.764 sec/iter\n",
      "Epoch: 418 | Batch: 005 / 011 | Total loss: 1.425 | Reg loss: 0.040 | Tree loss: 1.425 | Accuracy: 0.484000 | 1.764 sec/iter\n",
      "Epoch: 418 | Batch: 006 / 011 | Total loss: 1.409 | Reg loss: 0.040 | Tree loss: 1.409 | Accuracy: 0.522500 | 1.764 sec/iter\n",
      "Epoch: 418 | Batch: 007 / 011 | Total loss: 1.396 | Reg loss: 0.040 | Tree loss: 1.396 | Accuracy: 0.527500 | 1.764 sec/iter\n",
      "Epoch: 418 | Batch: 008 / 011 | Total loss: 1.433 | Reg loss: 0.040 | Tree loss: 1.433 | Accuracy: 0.487500 | 1.764 sec/iter\n",
      "Epoch: 418 | Batch: 009 / 011 | Total loss: 1.410 | Reg loss: 0.040 | Tree loss: 1.410 | Accuracy: 0.516500 | 1.764 sec/iter\n",
      "Epoch: 418 | Batch: 010 / 011 | Total loss: 1.483 | Reg loss: 0.040 | Tree loss: 1.483 | Accuracy: 0.467577 | 1.764 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 419 | Batch: 000 / 011 | Total loss: 1.606 | Reg loss: 0.040 | Tree loss: 1.606 | Accuracy: 0.368000 | 1.765 sec/iter\n",
      "Epoch: 419 | Batch: 001 / 011 | Total loss: 1.566 | Reg loss: 0.040 | Tree loss: 1.566 | Accuracy: 0.416000 | 1.765 sec/iter\n",
      "Epoch: 419 | Batch: 002 / 011 | Total loss: 1.539 | Reg loss: 0.040 | Tree loss: 1.539 | Accuracy: 0.405500 | 1.765 sec/iter\n",
      "Epoch: 419 | Batch: 003 / 011 | Total loss: 1.489 | Reg loss: 0.040 | Tree loss: 1.489 | Accuracy: 0.446000 | 1.765 sec/iter\n",
      "Epoch: 419 | Batch: 004 / 011 | Total loss: 1.463 | Reg loss: 0.040 | Tree loss: 1.463 | Accuracy: 0.479000 | 1.765 sec/iter\n",
      "Epoch: 419 | Batch: 005 / 011 | Total loss: 1.415 | Reg loss: 0.040 | Tree loss: 1.415 | Accuracy: 0.518000 | 1.765 sec/iter\n",
      "Epoch: 419 | Batch: 006 / 011 | Total loss: 1.437 | Reg loss: 0.040 | Tree loss: 1.437 | Accuracy: 0.502000 | 1.765 sec/iter\n",
      "Epoch: 419 | Batch: 007 / 011 | Total loss: 1.418 | Reg loss: 0.040 | Tree loss: 1.418 | Accuracy: 0.505500 | 1.765 sec/iter\n",
      "Epoch: 419 | Batch: 008 / 011 | Total loss: 1.385 | Reg loss: 0.040 | Tree loss: 1.385 | Accuracy: 0.502500 | 1.765 sec/iter\n",
      "Epoch: 419 | Batch: 009 / 011 | Total loss: 1.426 | Reg loss: 0.040 | Tree loss: 1.426 | Accuracy: 0.484000 | 1.765 sec/iter\n",
      "Epoch: 419 | Batch: 010 / 011 | Total loss: 1.422 | Reg loss: 0.040 | Tree loss: 1.422 | Accuracy: 0.443686 | 1.764 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 420 | Batch: 000 / 011 | Total loss: 1.611 | Reg loss: 0.040 | Tree loss: 1.611 | Accuracy: 0.406000 | 1.764 sec/iter\n",
      "Epoch: 420 | Batch: 001 / 011 | Total loss: 1.562 | Reg loss: 0.040 | Tree loss: 1.562 | Accuracy: 0.391000 | 1.764 sec/iter\n",
      "Epoch: 420 | Batch: 002 / 011 | Total loss: 1.527 | Reg loss: 0.040 | Tree loss: 1.527 | Accuracy: 0.434500 | 1.764 sec/iter\n",
      "Epoch: 420 | Batch: 003 / 011 | Total loss: 1.502 | Reg loss: 0.040 | Tree loss: 1.502 | Accuracy: 0.436000 | 1.764 sec/iter\n",
      "Epoch: 420 | Batch: 004 / 011 | Total loss: 1.470 | Reg loss: 0.040 | Tree loss: 1.470 | Accuracy: 0.468500 | 1.764 sec/iter\n",
      "Epoch: 420 | Batch: 005 / 011 | Total loss: 1.432 | Reg loss: 0.040 | Tree loss: 1.432 | Accuracy: 0.500500 | 1.764 sec/iter\n",
      "Epoch: 420 | Batch: 006 / 011 | Total loss: 1.408 | Reg loss: 0.040 | Tree loss: 1.408 | Accuracy: 0.530500 | 1.764 sec/iter\n",
      "Epoch: 420 | Batch: 007 / 011 | Total loss: 1.396 | Reg loss: 0.040 | Tree loss: 1.396 | Accuracy: 0.532000 | 1.764 sec/iter\n",
      "Epoch: 420 | Batch: 008 / 011 | Total loss: 1.412 | Reg loss: 0.040 | Tree loss: 1.412 | Accuracy: 0.516500 | 1.764 sec/iter\n",
      "Epoch: 420 | Batch: 009 / 011 | Total loss: 1.413 | Reg loss: 0.040 | Tree loss: 1.413 | Accuracy: 0.516000 | 1.764 sec/iter\n",
      "Epoch: 420 | Batch: 010 / 011 | Total loss: 1.423 | Reg loss: 0.040 | Tree loss: 1.423 | Accuracy: 0.467577 | 1.764 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 421 | Batch: 000 / 011 | Total loss: 1.581 | Reg loss: 0.040 | Tree loss: 1.581 | Accuracy: 0.392000 | 1.765 sec/iter\n",
      "Epoch: 421 | Batch: 001 / 011 | Total loss: 1.578 | Reg loss: 0.040 | Tree loss: 1.578 | Accuracy: 0.414500 | 1.765 sec/iter\n",
      "Epoch: 421 | Batch: 002 / 011 | Total loss: 1.544 | Reg loss: 0.040 | Tree loss: 1.544 | Accuracy: 0.411500 | 1.765 sec/iter\n",
      "Epoch: 421 | Batch: 003 / 011 | Total loss: 1.497 | Reg loss: 0.040 | Tree loss: 1.497 | Accuracy: 0.445500 | 1.765 sec/iter\n",
      "Epoch: 421 | Batch: 004 / 011 | Total loss: 1.466 | Reg loss: 0.040 | Tree loss: 1.466 | Accuracy: 0.457500 | 1.765 sec/iter\n",
      "Epoch: 421 | Batch: 005 / 011 | Total loss: 1.434 | Reg loss: 0.040 | Tree loss: 1.434 | Accuracy: 0.504000 | 1.765 sec/iter\n",
      "Epoch: 421 | Batch: 006 / 011 | Total loss: 1.437 | Reg loss: 0.040 | Tree loss: 1.437 | Accuracy: 0.504000 | 1.765 sec/iter\n",
      "Epoch: 421 | Batch: 007 / 011 | Total loss: 1.414 | Reg loss: 0.040 | Tree loss: 1.414 | Accuracy: 0.524500 | 1.765 sec/iter\n",
      "Epoch: 421 | Batch: 008 / 011 | Total loss: 1.395 | Reg loss: 0.040 | Tree loss: 1.395 | Accuracy: 0.513000 | 1.765 sec/iter\n",
      "Epoch: 421 | Batch: 009 / 011 | Total loss: 1.384 | Reg loss: 0.040 | Tree loss: 1.384 | Accuracy: 0.519000 | 1.764 sec/iter\n",
      "Epoch: 421 | Batch: 010 / 011 | Total loss: 1.350 | Reg loss: 0.040 | Tree loss: 1.350 | Accuracy: 0.498294 | 1.764 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 422 | Batch: 000 / 011 | Total loss: 1.594 | Reg loss: 0.040 | Tree loss: 1.594 | Accuracy: 0.389500 | 1.764 sec/iter\n",
      "Epoch: 422 | Batch: 001 / 011 | Total loss: 1.586 | Reg loss: 0.040 | Tree loss: 1.586 | Accuracy: 0.413000 | 1.764 sec/iter\n",
      "Epoch: 422 | Batch: 002 / 011 | Total loss: 1.554 | Reg loss: 0.040 | Tree loss: 1.554 | Accuracy: 0.409500 | 1.764 sec/iter\n",
      "Epoch: 422 | Batch: 003 / 011 | Total loss: 1.504 | Reg loss: 0.040 | Tree loss: 1.504 | Accuracy: 0.440000 | 1.764 sec/iter\n",
      "Epoch: 422 | Batch: 004 / 011 | Total loss: 1.432 | Reg loss: 0.040 | Tree loss: 1.432 | Accuracy: 0.493500 | 1.764 sec/iter\n",
      "Epoch: 422 | Batch: 005 / 011 | Total loss: 1.430 | Reg loss: 0.040 | Tree loss: 1.430 | Accuracy: 0.494500 | 1.764 sec/iter\n",
      "Epoch: 422 | Batch: 006 / 011 | Total loss: 1.415 | Reg loss: 0.040 | Tree loss: 1.415 | Accuracy: 0.506500 | 1.764 sec/iter\n",
      "Epoch: 422 | Batch: 007 / 011 | Total loss: 1.404 | Reg loss: 0.040 | Tree loss: 1.404 | Accuracy: 0.515500 | 1.764 sec/iter\n",
      "Epoch: 422 | Batch: 008 / 011 | Total loss: 1.405 | Reg loss: 0.040 | Tree loss: 1.405 | Accuracy: 0.496500 | 1.764 sec/iter\n",
      "Epoch: 422 | Batch: 009 / 011 | Total loss: 1.396 | Reg loss: 0.040 | Tree loss: 1.396 | Accuracy: 0.492500 | 1.764 sec/iter\n",
      "Epoch: 422 | Batch: 010 / 011 | Total loss: 1.447 | Reg loss: 0.040 | Tree loss: 1.447 | Accuracy: 0.494881 | 1.763 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 423 | Batch: 000 / 011 | Total loss: 1.618 | Reg loss: 0.040 | Tree loss: 1.618 | Accuracy: 0.403000 | 1.765 sec/iter\n",
      "Epoch: 423 | Batch: 001 / 011 | Total loss: 1.582 | Reg loss: 0.040 | Tree loss: 1.582 | Accuracy: 0.408500 | 1.765 sec/iter\n",
      "Epoch: 423 | Batch: 002 / 011 | Total loss: 1.502 | Reg loss: 0.040 | Tree loss: 1.502 | Accuracy: 0.435500 | 1.765 sec/iter\n",
      "Epoch: 423 | Batch: 003 / 011 | Total loss: 1.485 | Reg loss: 0.040 | Tree loss: 1.485 | Accuracy: 0.461000 | 1.765 sec/iter\n",
      "Epoch: 423 | Batch: 004 / 011 | Total loss: 1.448 | Reg loss: 0.040 | Tree loss: 1.448 | Accuracy: 0.480000 | 1.764 sec/iter\n",
      "Epoch: 423 | Batch: 005 / 011 | Total loss: 1.426 | Reg loss: 0.040 | Tree loss: 1.426 | Accuracy: 0.490500 | 1.764 sec/iter\n",
      "Epoch: 423 | Batch: 006 / 011 | Total loss: 1.427 | Reg loss: 0.040 | Tree loss: 1.427 | Accuracy: 0.514500 | 1.764 sec/iter\n",
      "Epoch: 423 | Batch: 007 / 011 | Total loss: 1.408 | Reg loss: 0.040 | Tree loss: 1.408 | Accuracy: 0.528500 | 1.764 sec/iter\n",
      "Epoch: 423 | Batch: 008 / 011 | Total loss: 1.408 | Reg loss: 0.040 | Tree loss: 1.408 | Accuracy: 0.514000 | 1.764 sec/iter\n",
      "Epoch: 423 | Batch: 009 / 011 | Total loss: 1.403 | Reg loss: 0.040 | Tree loss: 1.403 | Accuracy: 0.512500 | 1.764 sec/iter\n",
      "Epoch: 423 | Batch: 010 / 011 | Total loss: 1.433 | Reg loss: 0.040 | Tree loss: 1.433 | Accuracy: 0.508532 | 1.763 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 424 | Batch: 000 / 011 | Total loss: 1.620 | Reg loss: 0.040 | Tree loss: 1.620 | Accuracy: 0.399000 | 1.765 sec/iter\n",
      "Epoch: 424 | Batch: 001 / 011 | Total loss: 1.593 | Reg loss: 0.040 | Tree loss: 1.593 | Accuracy: 0.378500 | 1.765 sec/iter\n",
      "Epoch: 424 | Batch: 002 / 011 | Total loss: 1.528 | Reg loss: 0.040 | Tree loss: 1.528 | Accuracy: 0.428500 | 1.765 sec/iter\n",
      "Epoch: 424 | Batch: 003 / 011 | Total loss: 1.499 | Reg loss: 0.040 | Tree loss: 1.499 | Accuracy: 0.450000 | 1.765 sec/iter\n",
      "Epoch: 424 | Batch: 004 / 011 | Total loss: 1.459 | Reg loss: 0.040 | Tree loss: 1.459 | Accuracy: 0.477500 | 1.765 sec/iter\n",
      "Epoch: 424 | Batch: 005 / 011 | Total loss: 1.426 | Reg loss: 0.040 | Tree loss: 1.426 | Accuracy: 0.497500 | 1.765 sec/iter\n",
      "Epoch: 424 | Batch: 006 / 011 | Total loss: 1.384 | Reg loss: 0.040 | Tree loss: 1.384 | Accuracy: 0.527000 | 1.765 sec/iter\n",
      "Epoch: 424 | Batch: 007 / 011 | Total loss: 1.395 | Reg loss: 0.040 | Tree loss: 1.395 | Accuracy: 0.539000 | 1.764 sec/iter\n",
      "Epoch: 424 | Batch: 008 / 011 | Total loss: 1.409 | Reg loss: 0.040 | Tree loss: 1.409 | Accuracy: 0.528500 | 1.764 sec/iter\n",
      "Epoch: 424 | Batch: 009 / 011 | Total loss: 1.402 | Reg loss: 0.040 | Tree loss: 1.402 | Accuracy: 0.494500 | 1.764 sec/iter\n",
      "Epoch: 424 | Batch: 010 / 011 | Total loss: 1.462 | Reg loss: 0.040 | Tree loss: 1.462 | Accuracy: 0.436860 | 1.764 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 425 | Batch: 000 / 011 | Total loss: 1.623 | Reg loss: 0.040 | Tree loss: 1.623 | Accuracy: 0.396000 | 1.764 sec/iter\n",
      "Epoch: 425 | Batch: 001 / 011 | Total loss: 1.561 | Reg loss: 0.040 | Tree loss: 1.561 | Accuracy: 0.410500 | 1.764 sec/iter\n",
      "Epoch: 425 | Batch: 002 / 011 | Total loss: 1.529 | Reg loss: 0.040 | Tree loss: 1.529 | Accuracy: 0.433000 | 1.764 sec/iter\n",
      "Epoch: 425 | Batch: 003 / 011 | Total loss: 1.494 | Reg loss: 0.040 | Tree loss: 1.494 | Accuracy: 0.452000 | 1.764 sec/iter\n",
      "Epoch: 425 | Batch: 004 / 011 | Total loss: 1.461 | Reg loss: 0.040 | Tree loss: 1.461 | Accuracy: 0.489500 | 1.764 sec/iter\n",
      "Epoch: 425 | Batch: 005 / 011 | Total loss: 1.437 | Reg loss: 0.040 | Tree loss: 1.437 | Accuracy: 0.513500 | 1.764 sec/iter\n",
      "Epoch: 425 | Batch: 006 / 011 | Total loss: 1.399 | Reg loss: 0.040 | Tree loss: 1.399 | Accuracy: 0.522000 | 1.764 sec/iter\n",
      "Epoch: 425 | Batch: 007 / 011 | Total loss: 1.413 | Reg loss: 0.040 | Tree loss: 1.413 | Accuracy: 0.516500 | 1.764 sec/iter\n",
      "Epoch: 425 | Batch: 008 / 011 | Total loss: 1.398 | Reg loss: 0.040 | Tree loss: 1.398 | Accuracy: 0.531500 | 1.764 sec/iter\n",
      "Epoch: 425 | Batch: 009 / 011 | Total loss: 1.414 | Reg loss: 0.040 | Tree loss: 1.414 | Accuracy: 0.487000 | 1.763 sec/iter\n",
      "Epoch: 425 | Batch: 010 / 011 | Total loss: 1.466 | Reg loss: 0.040 | Tree loss: 1.466 | Accuracy: 0.443686 | 1.763 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 426 | Batch: 000 / 011 | Total loss: 1.621 | Reg loss: 0.040 | Tree loss: 1.621 | Accuracy: 0.391000 | 1.765 sec/iter\n",
      "Epoch: 426 | Batch: 001 / 011 | Total loss: 1.550 | Reg loss: 0.040 | Tree loss: 1.550 | Accuracy: 0.420000 | 1.765 sec/iter\n",
      "Epoch: 426 | Batch: 002 / 011 | Total loss: 1.534 | Reg loss: 0.040 | Tree loss: 1.534 | Accuracy: 0.436500 | 1.765 sec/iter\n",
      "Epoch: 426 | Batch: 003 / 011 | Total loss: 1.488 | Reg loss: 0.040 | Tree loss: 1.488 | Accuracy: 0.452500 | 1.765 sec/iter\n",
      "Epoch: 426 | Batch: 004 / 011 | Total loss: 1.472 | Reg loss: 0.040 | Tree loss: 1.472 | Accuracy: 0.469000 | 1.764 sec/iter\n",
      "Epoch: 426 | Batch: 005 / 011 | Total loss: 1.442 | Reg loss: 0.040 | Tree loss: 1.442 | Accuracy: 0.484000 | 1.764 sec/iter\n",
      "Epoch: 426 | Batch: 006 / 011 | Total loss: 1.416 | Reg loss: 0.040 | Tree loss: 1.416 | Accuracy: 0.519000 | 1.764 sec/iter\n",
      "Epoch: 426 | Batch: 007 / 011 | Total loss: 1.384 | Reg loss: 0.040 | Tree loss: 1.384 | Accuracy: 0.526000 | 1.764 sec/iter\n",
      "Epoch: 426 | Batch: 008 / 011 | Total loss: 1.400 | Reg loss: 0.040 | Tree loss: 1.400 | Accuracy: 0.507500 | 1.764 sec/iter\n",
      "Epoch: 426 | Batch: 009 / 011 | Total loss: 1.413 | Reg loss: 0.040 | Tree loss: 1.413 | Accuracy: 0.519500 | 1.764 sec/iter\n",
      "Epoch: 426 | Batch: 010 / 011 | Total loss: 1.351 | Reg loss: 0.040 | Tree loss: 1.351 | Accuracy: 0.498294 | 1.764 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 427 | Batch: 000 / 011 | Total loss: 1.581 | Reg loss: 0.040 | Tree loss: 1.581 | Accuracy: 0.406000 | 1.764 sec/iter\n",
      "Epoch: 427 | Batch: 001 / 011 | Total loss: 1.558 | Reg loss: 0.040 | Tree loss: 1.558 | Accuracy: 0.411000 | 1.764 sec/iter\n",
      "Epoch: 427 | Batch: 002 / 011 | Total loss: 1.527 | Reg loss: 0.040 | Tree loss: 1.527 | Accuracy: 0.443500 | 1.764 sec/iter\n",
      "Epoch: 427 | Batch: 003 / 011 | Total loss: 1.505 | Reg loss: 0.040 | Tree loss: 1.505 | Accuracy: 0.448000 | 1.764 sec/iter\n",
      "Epoch: 427 | Batch: 004 / 011 | Total loss: 1.485 | Reg loss: 0.040 | Tree loss: 1.485 | Accuracy: 0.473500 | 1.764 sec/iter\n",
      "Epoch: 427 | Batch: 005 / 011 | Total loss: 1.430 | Reg loss: 0.040 | Tree loss: 1.430 | Accuracy: 0.509500 | 1.764 sec/iter\n",
      "Epoch: 427 | Batch: 006 / 011 | Total loss: 1.419 | Reg loss: 0.040 | Tree loss: 1.419 | Accuracy: 0.545500 | 1.764 sec/iter\n",
      "Epoch: 427 | Batch: 007 / 011 | Total loss: 1.403 | Reg loss: 0.040 | Tree loss: 1.403 | Accuracy: 0.535000 | 1.764 sec/iter\n",
      "Epoch: 427 | Batch: 008 / 011 | Total loss: 1.400 | Reg loss: 0.040 | Tree loss: 1.400 | Accuracy: 0.515500 | 1.763 sec/iter\n",
      "Epoch: 427 | Batch: 009 / 011 | Total loss: 1.419 | Reg loss: 0.040 | Tree loss: 1.419 | Accuracy: 0.514500 | 1.763 sec/iter\n",
      "Epoch: 427 | Batch: 010 / 011 | Total loss: 1.402 | Reg loss: 0.040 | Tree loss: 1.402 | Accuracy: 0.518771 | 1.763 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 428 | Batch: 000 / 011 | Total loss: 1.605 | Reg loss: 0.040 | Tree loss: 1.605 | Accuracy: 0.407000 | 1.765 sec/iter\n",
      "Epoch: 428 | Batch: 001 / 011 | Total loss: 1.577 | Reg loss: 0.040 | Tree loss: 1.577 | Accuracy: 0.418500 | 1.765 sec/iter\n",
      "Epoch: 428 | Batch: 002 / 011 | Total loss: 1.534 | Reg loss: 0.040 | Tree loss: 1.534 | Accuracy: 0.436500 | 1.764 sec/iter\n",
      "Epoch: 428 | Batch: 003 / 011 | Total loss: 1.509 | Reg loss: 0.040 | Tree loss: 1.509 | Accuracy: 0.441000 | 1.764 sec/iter\n",
      "Epoch: 428 | Batch: 004 / 011 | Total loss: 1.464 | Reg loss: 0.040 | Tree loss: 1.464 | Accuracy: 0.460500 | 1.764 sec/iter\n",
      "Epoch: 428 | Batch: 005 / 011 | Total loss: 1.427 | Reg loss: 0.040 | Tree loss: 1.427 | Accuracy: 0.506000 | 1.764 sec/iter\n",
      "Epoch: 428 | Batch: 006 / 011 | Total loss: 1.418 | Reg loss: 0.040 | Tree loss: 1.418 | Accuracy: 0.535000 | 1.764 sec/iter\n",
      "Epoch: 428 | Batch: 007 / 011 | Total loss: 1.409 | Reg loss: 0.040 | Tree loss: 1.409 | Accuracy: 0.533000 | 1.764 sec/iter\n",
      "Epoch: 428 | Batch: 008 / 011 | Total loss: 1.383 | Reg loss: 0.040 | Tree loss: 1.383 | Accuracy: 0.513500 | 1.764 sec/iter\n",
      "Epoch: 428 | Batch: 009 / 011 | Total loss: 1.389 | Reg loss: 0.040 | Tree loss: 1.389 | Accuracy: 0.520000 | 1.764 sec/iter\n",
      "Epoch: 428 | Batch: 010 / 011 | Total loss: 1.358 | Reg loss: 0.040 | Tree loss: 1.358 | Accuracy: 0.508532 | 1.764 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 429 | Batch: 000 / 011 | Total loss: 1.606 | Reg loss: 0.040 | Tree loss: 1.606 | Accuracy: 0.389500 | 1.764 sec/iter\n",
      "Epoch: 429 | Batch: 001 / 011 | Total loss: 1.562 | Reg loss: 0.040 | Tree loss: 1.562 | Accuracy: 0.410500 | 1.764 sec/iter\n",
      "Epoch: 429 | Batch: 002 / 011 | Total loss: 1.550 | Reg loss: 0.040 | Tree loss: 1.550 | Accuracy: 0.433000 | 1.764 sec/iter\n",
      "Epoch: 429 | Batch: 003 / 011 | Total loss: 1.483 | Reg loss: 0.040 | Tree loss: 1.483 | Accuracy: 0.465000 | 1.764 sec/iter\n",
      "Epoch: 429 | Batch: 004 / 011 | Total loss: 1.443 | Reg loss: 0.040 | Tree loss: 1.443 | Accuracy: 0.482000 | 1.764 sec/iter\n",
      "Epoch: 429 | Batch: 005 / 011 | Total loss: 1.418 | Reg loss: 0.040 | Tree loss: 1.418 | Accuracy: 0.503500 | 1.764 sec/iter\n",
      "Epoch: 429 | Batch: 006 / 011 | Total loss: 1.443 | Reg loss: 0.040 | Tree loss: 1.443 | Accuracy: 0.511000 | 1.763 sec/iter\n",
      "Epoch: 429 | Batch: 007 / 011 | Total loss: 1.395 | Reg loss: 0.040 | Tree loss: 1.395 | Accuracy: 0.515000 | 1.763 sec/iter\n",
      "Epoch: 429 | Batch: 008 / 011 | Total loss: 1.400 | Reg loss: 0.040 | Tree loss: 1.400 | Accuracy: 0.498000 | 1.763 sec/iter\n",
      "Epoch: 429 | Batch: 009 / 011 | Total loss: 1.413 | Reg loss: 0.040 | Tree loss: 1.413 | Accuracy: 0.500000 | 1.763 sec/iter\n",
      "Epoch: 429 | Batch: 010 / 011 | Total loss: 1.430 | Reg loss: 0.040 | Tree loss: 1.430 | Accuracy: 0.477816 | 1.763 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 430 | Batch: 000 / 011 | Total loss: 1.624 | Reg loss: 0.040 | Tree loss: 1.624 | Accuracy: 0.397000 | 1.765 sec/iter\n",
      "Epoch: 430 | Batch: 001 / 011 | Total loss: 1.584 | Reg loss: 0.040 | Tree loss: 1.584 | Accuracy: 0.405500 | 1.764 sec/iter\n",
      "Epoch: 430 | Batch: 002 / 011 | Total loss: 1.537 | Reg loss: 0.040 | Tree loss: 1.537 | Accuracy: 0.427000 | 1.764 sec/iter\n",
      "Epoch: 430 | Batch: 003 / 011 | Total loss: 1.458 | Reg loss: 0.040 | Tree loss: 1.458 | Accuracy: 0.470500 | 1.764 sec/iter\n",
      "Epoch: 430 | Batch: 004 / 011 | Total loss: 1.472 | Reg loss: 0.040 | Tree loss: 1.472 | Accuracy: 0.471500 | 1.764 sec/iter\n",
      "Epoch: 430 | Batch: 005 / 011 | Total loss: 1.421 | Reg loss: 0.040 | Tree loss: 1.421 | Accuracy: 0.501000 | 1.764 sec/iter\n",
      "Epoch: 430 | Batch: 006 / 011 | Total loss: 1.389 | Reg loss: 0.040 | Tree loss: 1.389 | Accuracy: 0.538000 | 1.764 sec/iter\n",
      "Epoch: 430 | Batch: 007 / 011 | Total loss: 1.414 | Reg loss: 0.040 | Tree loss: 1.414 | Accuracy: 0.525500 | 1.764 sec/iter\n",
      "Epoch: 430 | Batch: 008 / 011 | Total loss: 1.405 | Reg loss: 0.040 | Tree loss: 1.405 | Accuracy: 0.518500 | 1.763 sec/iter\n",
      "Epoch: 430 | Batch: 009 / 011 | Total loss: 1.431 | Reg loss: 0.040 | Tree loss: 1.431 | Accuracy: 0.481000 | 1.763 sec/iter\n",
      "Epoch: 430 | Batch: 010 / 011 | Total loss: 1.371 | Reg loss: 0.040 | Tree loss: 1.371 | Accuracy: 0.518771 | 1.763 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 431 | Batch: 000 / 011 | Total loss: 1.626 | Reg loss: 0.040 | Tree loss: 1.626 | Accuracy: 0.387500 | 1.764 sec/iter\n",
      "Epoch: 431 | Batch: 001 / 011 | Total loss: 1.552 | Reg loss: 0.040 | Tree loss: 1.552 | Accuracy: 0.406000 | 1.764 sec/iter\n",
      "Epoch: 431 | Batch: 002 / 011 | Total loss: 1.516 | Reg loss: 0.040 | Tree loss: 1.516 | Accuracy: 0.455500 | 1.764 sec/iter\n",
      "Epoch: 431 | Batch: 003 / 011 | Total loss: 1.497 | Reg loss: 0.040 | Tree loss: 1.497 | Accuracy: 0.462500 | 1.764 sec/iter\n",
      "Epoch: 431 | Batch: 004 / 011 | Total loss: 1.453 | Reg loss: 0.040 | Tree loss: 1.453 | Accuracy: 0.487500 | 1.764 sec/iter\n",
      "Epoch: 431 | Batch: 005 / 011 | Total loss: 1.429 | Reg loss: 0.040 | Tree loss: 1.429 | Accuracy: 0.519500 | 1.764 sec/iter\n",
      "Epoch: 431 | Batch: 006 / 011 | Total loss: 1.410 | Reg loss: 0.040 | Tree loss: 1.410 | Accuracy: 0.528500 | 1.764 sec/iter\n",
      "Epoch: 431 | Batch: 007 / 011 | Total loss: 1.414 | Reg loss: 0.040 | Tree loss: 1.414 | Accuracy: 0.531500 | 1.764 sec/iter\n",
      "Epoch: 431 | Batch: 008 / 011 | Total loss: 1.420 | Reg loss: 0.040 | Tree loss: 1.420 | Accuracy: 0.519000 | 1.764 sec/iter\n",
      "Epoch: 431 | Batch: 009 / 011 | Total loss: 1.418 | Reg loss: 0.040 | Tree loss: 1.418 | Accuracy: 0.505000 | 1.764 sec/iter\n",
      "Epoch: 431 | Batch: 010 / 011 | Total loss: 1.385 | Reg loss: 0.040 | Tree loss: 1.385 | Accuracy: 0.508532 | 1.764 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 432 | Batch: 000 / 011 | Total loss: 1.586 | Reg loss: 0.040 | Tree loss: 1.586 | Accuracy: 0.407500 | 1.764 sec/iter\n",
      "Epoch: 432 | Batch: 001 / 011 | Total loss: 1.549 | Reg loss: 0.040 | Tree loss: 1.549 | Accuracy: 0.427500 | 1.764 sec/iter\n",
      "Epoch: 432 | Batch: 002 / 011 | Total loss: 1.528 | Reg loss: 0.040 | Tree loss: 1.528 | Accuracy: 0.443000 | 1.764 sec/iter\n",
      "Epoch: 432 | Batch: 003 / 011 | Total loss: 1.472 | Reg loss: 0.040 | Tree loss: 1.472 | Accuracy: 0.451000 | 1.763 sec/iter\n",
      "Epoch: 432 | Batch: 004 / 011 | Total loss: 1.480 | Reg loss: 0.040 | Tree loss: 1.480 | Accuracy: 0.467500 | 1.763 sec/iter\n",
      "Epoch: 432 | Batch: 005 / 011 | Total loss: 1.436 | Reg loss: 0.040 | Tree loss: 1.436 | Accuracy: 0.508000 | 1.763 sec/iter\n",
      "Epoch: 432 | Batch: 006 / 011 | Total loss: 1.418 | Reg loss: 0.040 | Tree loss: 1.418 | Accuracy: 0.529500 | 1.763 sec/iter\n",
      "Epoch: 432 | Batch: 007 / 011 | Total loss: 1.441 | Reg loss: 0.040 | Tree loss: 1.441 | Accuracy: 0.523000 | 1.763 sec/iter\n",
      "Epoch: 432 | Batch: 008 / 011 | Total loss: 1.402 | Reg loss: 0.040 | Tree loss: 1.402 | Accuracy: 0.509000 | 1.763 sec/iter\n",
      "Epoch: 432 | Batch: 009 / 011 | Total loss: 1.401 | Reg loss: 0.040 | Tree loss: 1.401 | Accuracy: 0.503000 | 1.763 sec/iter\n",
      "Epoch: 432 | Batch: 010 / 011 | Total loss: 1.488 | Reg loss: 0.040 | Tree loss: 1.488 | Accuracy: 0.470990 | 1.763 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 433 | Batch: 000 / 011 | Total loss: 1.605 | Reg loss: 0.040 | Tree loss: 1.605 | Accuracy: 0.398500 | 1.764 sec/iter\n",
      "Epoch: 433 | Batch: 001 / 011 | Total loss: 1.603 | Reg loss: 0.040 | Tree loss: 1.603 | Accuracy: 0.399000 | 1.764 sec/iter\n",
      "Epoch: 433 | Batch: 002 / 011 | Total loss: 1.538 | Reg loss: 0.040 | Tree loss: 1.538 | Accuracy: 0.432000 | 1.764 sec/iter\n",
      "Epoch: 433 | Batch: 003 / 011 | Total loss: 1.483 | Reg loss: 0.040 | Tree loss: 1.483 | Accuracy: 0.473000 | 1.764 sec/iter\n",
      "Epoch: 433 | Batch: 004 / 011 | Total loss: 1.467 | Reg loss: 0.040 | Tree loss: 1.467 | Accuracy: 0.496000 | 1.764 sec/iter\n",
      "Epoch: 433 | Batch: 005 / 011 | Total loss: 1.460 | Reg loss: 0.040 | Tree loss: 1.460 | Accuracy: 0.492500 | 1.764 sec/iter\n",
      "Epoch: 433 | Batch: 006 / 011 | Total loss: 1.393 | Reg loss: 0.040 | Tree loss: 1.393 | Accuracy: 0.528000 | 1.764 sec/iter\n",
      "Epoch: 433 | Batch: 007 / 011 | Total loss: 1.407 | Reg loss: 0.040 | Tree loss: 1.407 | Accuracy: 0.523500 | 1.764 sec/iter\n",
      "Epoch: 433 | Batch: 008 / 011 | Total loss: 1.387 | Reg loss: 0.040 | Tree loss: 1.387 | Accuracy: 0.510500 | 1.764 sec/iter\n",
      "Epoch: 433 | Batch: 009 / 011 | Total loss: 1.401 | Reg loss: 0.040 | Tree loss: 1.401 | Accuracy: 0.516500 | 1.764 sec/iter\n",
      "Epoch: 433 | Batch: 010 / 011 | Total loss: 1.397 | Reg loss: 0.040 | Tree loss: 1.397 | Accuracy: 0.515358 | 1.764 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 434 | Batch: 000 / 011 | Total loss: 1.602 | Reg loss: 0.040 | Tree loss: 1.602 | Accuracy: 0.406500 | 1.764 sec/iter\n",
      "Epoch: 434 | Batch: 001 / 011 | Total loss: 1.564 | Reg loss: 0.040 | Tree loss: 1.564 | Accuracy: 0.414000 | 1.763 sec/iter\n",
      "Epoch: 434 | Batch: 002 / 011 | Total loss: 1.504 | Reg loss: 0.040 | Tree loss: 1.504 | Accuracy: 0.455000 | 1.763 sec/iter\n",
      "Epoch: 434 | Batch: 003 / 011 | Total loss: 1.487 | Reg loss: 0.040 | Tree loss: 1.487 | Accuracy: 0.494000 | 1.763 sec/iter\n",
      "Epoch: 434 | Batch: 004 / 011 | Total loss: 1.477 | Reg loss: 0.040 | Tree loss: 1.477 | Accuracy: 0.494000 | 1.763 sec/iter\n",
      "Epoch: 434 | Batch: 005 / 011 | Total loss: 1.431 | Reg loss: 0.040 | Tree loss: 1.431 | Accuracy: 0.514000 | 1.763 sec/iter\n",
      "Epoch: 434 | Batch: 006 / 011 | Total loss: 1.428 | Reg loss: 0.040 | Tree loss: 1.428 | Accuracy: 0.529500 | 1.763 sec/iter\n",
      "Epoch: 434 | Batch: 007 / 011 | Total loss: 1.421 | Reg loss: 0.040 | Tree loss: 1.421 | Accuracy: 0.502500 | 1.763 sec/iter\n",
      "Epoch: 434 | Batch: 008 / 011 | Total loss: 1.394 | Reg loss: 0.040 | Tree loss: 1.394 | Accuracy: 0.511000 | 1.763 sec/iter\n",
      "Epoch: 434 | Batch: 009 / 011 | Total loss: 1.415 | Reg loss: 0.040 | Tree loss: 1.415 | Accuracy: 0.502500 | 1.763 sec/iter\n",
      "Epoch: 434 | Batch: 010 / 011 | Total loss: 1.353 | Reg loss: 0.040 | Tree loss: 1.353 | Accuracy: 0.546075 | 1.763 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 435 | Batch: 000 / 011 | Total loss: 1.605 | Reg loss: 0.040 | Tree loss: 1.605 | Accuracy: 0.402000 | 1.764 sec/iter\n",
      "Epoch: 435 | Batch: 001 / 011 | Total loss: 1.581 | Reg loss: 0.040 | Tree loss: 1.581 | Accuracy: 0.410000 | 1.764 sec/iter\n",
      "Epoch: 435 | Batch: 002 / 011 | Total loss: 1.538 | Reg loss: 0.040 | Tree loss: 1.538 | Accuracy: 0.428500 | 1.764 sec/iter\n",
      "Epoch: 435 | Batch: 003 / 011 | Total loss: 1.480 | Reg loss: 0.040 | Tree loss: 1.480 | Accuracy: 0.465500 | 1.764 sec/iter\n",
      "Epoch: 435 | Batch: 004 / 011 | Total loss: 1.470 | Reg loss: 0.040 | Tree loss: 1.470 | Accuracy: 0.471000 | 1.764 sec/iter\n",
      "Epoch: 435 | Batch: 005 / 011 | Total loss: 1.420 | Reg loss: 0.040 | Tree loss: 1.420 | Accuracy: 0.517000 | 1.764 sec/iter\n",
      "Epoch: 435 | Batch: 006 / 011 | Total loss: 1.441 | Reg loss: 0.040 | Tree loss: 1.441 | Accuracy: 0.502000 | 1.764 sec/iter\n",
      "Epoch: 435 | Batch: 007 / 011 | Total loss: 1.396 | Reg loss: 0.040 | Tree loss: 1.396 | Accuracy: 0.533000 | 1.764 sec/iter\n",
      "Epoch: 435 | Batch: 008 / 011 | Total loss: 1.408 | Reg loss: 0.040 | Tree loss: 1.408 | Accuracy: 0.511500 | 1.764 sec/iter\n",
      "Epoch: 435 | Batch: 009 / 011 | Total loss: 1.403 | Reg loss: 0.040 | Tree loss: 1.403 | Accuracy: 0.513000 | 1.764 sec/iter\n",
      "Epoch: 435 | Batch: 010 / 011 | Total loss: 1.296 | Reg loss: 0.040 | Tree loss: 1.296 | Accuracy: 0.515358 | 1.763 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 436 | Batch: 000 / 011 | Total loss: 1.614 | Reg loss: 0.040 | Tree loss: 1.614 | Accuracy: 0.397500 | 1.763 sec/iter\n",
      "Epoch: 436 | Batch: 001 / 011 | Total loss: 1.578 | Reg loss: 0.040 | Tree loss: 1.578 | Accuracy: 0.397000 | 1.763 sec/iter\n",
      "Epoch: 436 | Batch: 002 / 011 | Total loss: 1.543 | Reg loss: 0.040 | Tree loss: 1.543 | Accuracy: 0.434000 | 1.763 sec/iter\n",
      "Epoch: 436 | Batch: 003 / 011 | Total loss: 1.479 | Reg loss: 0.040 | Tree loss: 1.479 | Accuracy: 0.471500 | 1.763 sec/iter\n",
      "Epoch: 436 | Batch: 004 / 011 | Total loss: 1.453 | Reg loss: 0.040 | Tree loss: 1.453 | Accuracy: 0.477500 | 1.763 sec/iter\n",
      "Epoch: 436 | Batch: 005 / 011 | Total loss: 1.430 | Reg loss: 0.040 | Tree loss: 1.430 | Accuracy: 0.498500 | 1.763 sec/iter\n",
      "Epoch: 436 | Batch: 006 / 011 | Total loss: 1.437 | Reg loss: 0.040 | Tree loss: 1.437 | Accuracy: 0.516000 | 1.763 sec/iter\n",
      "Epoch: 436 | Batch: 007 / 011 | Total loss: 1.408 | Reg loss: 0.040 | Tree loss: 1.408 | Accuracy: 0.524000 | 1.763 sec/iter\n",
      "Epoch: 436 | Batch: 008 / 011 | Total loss: 1.392 | Reg loss: 0.040 | Tree loss: 1.392 | Accuracy: 0.510000 | 1.763 sec/iter\n",
      "Epoch: 436 | Batch: 009 / 011 | Total loss: 1.390 | Reg loss: 0.040 | Tree loss: 1.390 | Accuracy: 0.520500 | 1.763 sec/iter\n",
      "Epoch: 436 | Batch: 010 / 011 | Total loss: 1.410 | Reg loss: 0.040 | Tree loss: 1.410 | Accuracy: 0.535836 | 1.763 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 437 | Batch: 000 / 011 | Total loss: 1.603 | Reg loss: 0.040 | Tree loss: 1.603 | Accuracy: 0.397500 | 1.764 sec/iter\n",
      "Epoch: 437 | Batch: 001 / 011 | Total loss: 1.575 | Reg loss: 0.040 | Tree loss: 1.575 | Accuracy: 0.406000 | 1.764 sec/iter\n",
      "Epoch: 437 | Batch: 002 / 011 | Total loss: 1.548 | Reg loss: 0.040 | Tree loss: 1.548 | Accuracy: 0.422000 | 1.764 sec/iter\n",
      "Epoch: 437 | Batch: 003 / 011 | Total loss: 1.489 | Reg loss: 0.040 | Tree loss: 1.489 | Accuracy: 0.451000 | 1.764 sec/iter\n",
      "Epoch: 437 | Batch: 004 / 011 | Total loss: 1.458 | Reg loss: 0.040 | Tree loss: 1.458 | Accuracy: 0.468000 | 1.764 sec/iter\n",
      "Epoch: 437 | Batch: 005 / 011 | Total loss: 1.444 | Reg loss: 0.040 | Tree loss: 1.444 | Accuracy: 0.486500 | 1.763 sec/iter\n",
      "Epoch: 437 | Batch: 006 / 011 | Total loss: 1.415 | Reg loss: 0.040 | Tree loss: 1.415 | Accuracy: 0.525000 | 1.763 sec/iter\n",
      "Epoch: 437 | Batch: 007 / 011 | Total loss: 1.391 | Reg loss: 0.040 | Tree loss: 1.391 | Accuracy: 0.532500 | 1.763 sec/iter\n",
      "Epoch: 437 | Batch: 008 / 011 | Total loss: 1.408 | Reg loss: 0.040 | Tree loss: 1.408 | Accuracy: 0.517000 | 1.763 sec/iter\n",
      "Epoch: 437 | Batch: 009 / 011 | Total loss: 1.398 | Reg loss: 0.040 | Tree loss: 1.398 | Accuracy: 0.516500 | 1.763 sec/iter\n",
      "Epoch: 437 | Batch: 010 / 011 | Total loss: 1.326 | Reg loss: 0.040 | Tree loss: 1.326 | Accuracy: 0.539249 | 1.763 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 438 | Batch: 000 / 011 | Total loss: 1.602 | Reg loss: 0.040 | Tree loss: 1.602 | Accuracy: 0.405500 | 1.764 sec/iter\n",
      "Epoch: 438 | Batch: 001 / 011 | Total loss: 1.591 | Reg loss: 0.040 | Tree loss: 1.591 | Accuracy: 0.399000 | 1.764 sec/iter\n",
      "Epoch: 438 | Batch: 002 / 011 | Total loss: 1.526 | Reg loss: 0.040 | Tree loss: 1.526 | Accuracy: 0.441500 | 1.764 sec/iter\n",
      "Epoch: 438 | Batch: 003 / 011 | Total loss: 1.483 | Reg loss: 0.040 | Tree loss: 1.483 | Accuracy: 0.472000 | 1.764 sec/iter\n",
      "Epoch: 438 | Batch: 004 / 011 | Total loss: 1.481 | Reg loss: 0.040 | Tree loss: 1.481 | Accuracy: 0.471500 | 1.764 sec/iter\n",
      "Epoch: 438 | Batch: 005 / 011 | Total loss: 1.427 | Reg loss: 0.040 | Tree loss: 1.427 | Accuracy: 0.496500 | 1.764 sec/iter\n",
      "Epoch: 438 | Batch: 006 / 011 | Total loss: 1.407 | Reg loss: 0.040 | Tree loss: 1.407 | Accuracy: 0.525500 | 1.764 sec/iter\n",
      "Epoch: 438 | Batch: 007 / 011 | Total loss: 1.416 | Reg loss: 0.040 | Tree loss: 1.416 | Accuracy: 0.516000 | 1.764 sec/iter\n",
      "Epoch: 438 | Batch: 008 / 011 | Total loss: 1.383 | Reg loss: 0.040 | Tree loss: 1.383 | Accuracy: 0.511500 | 1.764 sec/iter\n",
      "Epoch: 438 | Batch: 009 / 011 | Total loss: 1.396 | Reg loss: 0.040 | Tree loss: 1.396 | Accuracy: 0.496000 | 1.763 sec/iter\n",
      "Epoch: 438 | Batch: 010 / 011 | Total loss: 1.437 | Reg loss: 0.040 | Tree loss: 1.437 | Accuracy: 0.498294 | 1.763 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 439 | Batch: 000 / 011 | Total loss: 1.617 | Reg loss: 0.040 | Tree loss: 1.617 | Accuracy: 0.388000 | 1.763 sec/iter\n",
      "Epoch: 439 | Batch: 001 / 011 | Total loss: 1.589 | Reg loss: 0.040 | Tree loss: 1.589 | Accuracy: 0.402000 | 1.763 sec/iter\n",
      "Epoch: 439 | Batch: 002 / 011 | Total loss: 1.530 | Reg loss: 0.040 | Tree loss: 1.530 | Accuracy: 0.446500 | 1.763 sec/iter\n",
      "Epoch: 439 | Batch: 003 / 011 | Total loss: 1.496 | Reg loss: 0.040 | Tree loss: 1.496 | Accuracy: 0.459000 | 1.763 sec/iter\n",
      "Epoch: 439 | Batch: 004 / 011 | Total loss: 1.435 | Reg loss: 0.040 | Tree loss: 1.435 | Accuracy: 0.475500 | 1.763 sec/iter\n",
      "Epoch: 439 | Batch: 005 / 011 | Total loss: 1.427 | Reg loss: 0.040 | Tree loss: 1.427 | Accuracy: 0.508000 | 1.763 sec/iter\n",
      "Epoch: 439 | Batch: 006 / 011 | Total loss: 1.425 | Reg loss: 0.040 | Tree loss: 1.425 | Accuracy: 0.513000 | 1.763 sec/iter\n",
      "Epoch: 439 | Batch: 007 / 011 | Total loss: 1.417 | Reg loss: 0.040 | Tree loss: 1.417 | Accuracy: 0.512000 | 1.763 sec/iter\n",
      "Epoch: 439 | Batch: 008 / 011 | Total loss: 1.381 | Reg loss: 0.040 | Tree loss: 1.381 | Accuracy: 0.506500 | 1.763 sec/iter\n",
      "Epoch: 439 | Batch: 009 / 011 | Total loss: 1.400 | Reg loss: 0.040 | Tree loss: 1.400 | Accuracy: 0.514000 | 1.763 sec/iter\n",
      "Epoch: 439 | Batch: 010 / 011 | Total loss: 1.505 | Reg loss: 0.040 | Tree loss: 1.505 | Accuracy: 0.453925 | 1.762 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 440 | Batch: 000 / 011 | Total loss: 1.626 | Reg loss: 0.040 | Tree loss: 1.626 | Accuracy: 0.380000 | 1.764 sec/iter\n",
      "Epoch: 440 | Batch: 001 / 011 | Total loss: 1.561 | Reg loss: 0.040 | Tree loss: 1.561 | Accuracy: 0.428500 | 1.764 sec/iter\n",
      "Epoch: 440 | Batch: 002 / 011 | Total loss: 1.514 | Reg loss: 0.040 | Tree loss: 1.514 | Accuracy: 0.456000 | 1.764 sec/iter\n",
      "Epoch: 440 | Batch: 003 / 011 | Total loss: 1.494 | Reg loss: 0.040 | Tree loss: 1.494 | Accuracy: 0.461500 | 1.764 sec/iter\n",
      "Epoch: 440 | Batch: 004 / 011 | Total loss: 1.446 | Reg loss: 0.040 | Tree loss: 1.446 | Accuracy: 0.500000 | 1.764 sec/iter\n",
      "Epoch: 440 | Batch: 005 / 011 | Total loss: 1.419 | Reg loss: 0.040 | Tree loss: 1.419 | Accuracy: 0.495500 | 1.764 sec/iter\n",
      "Epoch: 440 | Batch: 006 / 011 | Total loss: 1.428 | Reg loss: 0.040 | Tree loss: 1.428 | Accuracy: 0.511000 | 1.764 sec/iter\n",
      "Epoch: 440 | Batch: 007 / 011 | Total loss: 1.417 | Reg loss: 0.040 | Tree loss: 1.417 | Accuracy: 0.501500 | 1.763 sec/iter\n",
      "Epoch: 440 | Batch: 008 / 011 | Total loss: 1.418 | Reg loss: 0.040 | Tree loss: 1.418 | Accuracy: 0.478500 | 1.763 sec/iter\n",
      "Epoch: 440 | Batch: 009 / 011 | Total loss: 1.404 | Reg loss: 0.040 | Tree loss: 1.404 | Accuracy: 0.502000 | 1.763 sec/iter\n",
      "Epoch: 440 | Batch: 010 / 011 | Total loss: 1.411 | Reg loss: 0.040 | Tree loss: 1.411 | Accuracy: 0.491468 | 1.763 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 441 | Batch: 000 / 011 | Total loss: 1.596 | Reg loss: 0.040 | Tree loss: 1.596 | Accuracy: 0.389000 | 1.763 sec/iter\n",
      "Epoch: 441 | Batch: 001 / 011 | Total loss: 1.567 | Reg loss: 0.040 | Tree loss: 1.567 | Accuracy: 0.418000 | 1.763 sec/iter\n",
      "Epoch: 441 | Batch: 002 / 011 | Total loss: 1.536 | Reg loss: 0.040 | Tree loss: 1.536 | Accuracy: 0.433000 | 1.763 sec/iter\n",
      "Epoch: 441 | Batch: 003 / 011 | Total loss: 1.509 | Reg loss: 0.040 | Tree loss: 1.509 | Accuracy: 0.432500 | 1.763 sec/iter\n",
      "Epoch: 441 | Batch: 004 / 011 | Total loss: 1.461 | Reg loss: 0.040 | Tree loss: 1.461 | Accuracy: 0.481000 | 1.763 sec/iter\n",
      "Epoch: 441 | Batch: 005 / 011 | Total loss: 1.450 | Reg loss: 0.040 | Tree loss: 1.450 | Accuracy: 0.482000 | 1.763 sec/iter\n",
      "Epoch: 441 | Batch: 006 / 011 | Total loss: 1.421 | Reg loss: 0.040 | Tree loss: 1.421 | Accuracy: 0.501000 | 1.763 sec/iter\n",
      "Epoch: 441 | Batch: 007 / 011 | Total loss: 1.397 | Reg loss: 0.040 | Tree loss: 1.397 | Accuracy: 0.531500 | 1.763 sec/iter\n",
      "Epoch: 441 | Batch: 008 / 011 | Total loss: 1.390 | Reg loss: 0.040 | Tree loss: 1.390 | Accuracy: 0.531500 | 1.763 sec/iter\n",
      "Epoch: 441 | Batch: 009 / 011 | Total loss: 1.411 | Reg loss: 0.040 | Tree loss: 1.411 | Accuracy: 0.512000 | 1.763 sec/iter\n",
      "Epoch: 441 | Batch: 010 / 011 | Total loss: 1.407 | Reg loss: 0.040 | Tree loss: 1.407 | Accuracy: 0.477816 | 1.762 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 442 | Batch: 000 / 011 | Total loss: 1.600 | Reg loss: 0.040 | Tree loss: 1.600 | Accuracy: 0.389500 | 1.764 sec/iter\n",
      "Epoch: 442 | Batch: 001 / 011 | Total loss: 1.569 | Reg loss: 0.040 | Tree loss: 1.569 | Accuracy: 0.412000 | 1.764 sec/iter\n",
      "Epoch: 442 | Batch: 002 / 011 | Total loss: 1.508 | Reg loss: 0.040 | Tree loss: 1.508 | Accuracy: 0.445500 | 1.764 sec/iter\n",
      "Epoch: 442 | Batch: 003 / 011 | Total loss: 1.495 | Reg loss: 0.040 | Tree loss: 1.495 | Accuracy: 0.452000 | 1.764 sec/iter\n",
      "Epoch: 442 | Batch: 004 / 011 | Total loss: 1.469 | Reg loss: 0.040 | Tree loss: 1.469 | Accuracy: 0.474500 | 1.764 sec/iter\n",
      "Epoch: 442 | Batch: 005 / 011 | Total loss: 1.436 | Reg loss: 0.040 | Tree loss: 1.436 | Accuracy: 0.491000 | 1.763 sec/iter\n",
      "Epoch: 442 | Batch: 006 / 011 | Total loss: 1.421 | Reg loss: 0.040 | Tree loss: 1.421 | Accuracy: 0.498500 | 1.763 sec/iter\n",
      "Epoch: 442 | Batch: 007 / 011 | Total loss: 1.402 | Reg loss: 0.040 | Tree loss: 1.402 | Accuracy: 0.516500 | 1.763 sec/iter\n",
      "Epoch: 442 | Batch: 008 / 011 | Total loss: 1.421 | Reg loss: 0.040 | Tree loss: 1.421 | Accuracy: 0.493000 | 1.763 sec/iter\n",
      "Epoch: 442 | Batch: 009 / 011 | Total loss: 1.410 | Reg loss: 0.040 | Tree loss: 1.410 | Accuracy: 0.494500 | 1.763 sec/iter\n",
      "Epoch: 442 | Batch: 010 / 011 | Total loss: 1.397 | Reg loss: 0.040 | Tree loss: 1.397 | Accuracy: 0.498294 | 1.763 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 443 | Batch: 000 / 011 | Total loss: 1.589 | Reg loss: 0.040 | Tree loss: 1.589 | Accuracy: 0.422000 | 1.763 sec/iter\n",
      "Epoch: 443 | Batch: 001 / 011 | Total loss: 1.577 | Reg loss: 0.040 | Tree loss: 1.577 | Accuracy: 0.416000 | 1.763 sec/iter\n",
      "Epoch: 443 | Batch: 002 / 011 | Total loss: 1.536 | Reg loss: 0.040 | Tree loss: 1.536 | Accuracy: 0.428000 | 1.763 sec/iter\n",
      "Epoch: 443 | Batch: 003 / 011 | Total loss: 1.492 | Reg loss: 0.040 | Tree loss: 1.492 | Accuracy: 0.462000 | 1.763 sec/iter\n",
      "Epoch: 443 | Batch: 004 / 011 | Total loss: 1.469 | Reg loss: 0.040 | Tree loss: 1.469 | Accuracy: 0.485000 | 1.763 sec/iter\n",
      "Epoch: 443 | Batch: 005 / 011 | Total loss: 1.438 | Reg loss: 0.040 | Tree loss: 1.438 | Accuracy: 0.497500 | 1.763 sec/iter\n",
      "Epoch: 443 | Batch: 006 / 011 | Total loss: 1.417 | Reg loss: 0.040 | Tree loss: 1.417 | Accuracy: 0.521500 | 1.763 sec/iter\n",
      "Epoch: 443 | Batch: 007 / 011 | Total loss: 1.395 | Reg loss: 0.040 | Tree loss: 1.395 | Accuracy: 0.529500 | 1.763 sec/iter\n",
      "Epoch: 443 | Batch: 008 / 011 | Total loss: 1.388 | Reg loss: 0.040 | Tree loss: 1.388 | Accuracy: 0.513000 | 1.763 sec/iter\n",
      "Epoch: 443 | Batch: 009 / 011 | Total loss: 1.429 | Reg loss: 0.040 | Tree loss: 1.429 | Accuracy: 0.494000 | 1.762 sec/iter\n",
      "Epoch: 443 | Batch: 010 / 011 | Total loss: 1.455 | Reg loss: 0.040 | Tree loss: 1.455 | Accuracy: 0.430034 | 1.762 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 444 | Batch: 000 / 011 | Total loss: 1.608 | Reg loss: 0.040 | Tree loss: 1.608 | Accuracy: 0.399000 | 1.764 sec/iter\n",
      "Epoch: 444 | Batch: 001 / 011 | Total loss: 1.587 | Reg loss: 0.040 | Tree loss: 1.587 | Accuracy: 0.404000 | 1.764 sec/iter\n",
      "Epoch: 444 | Batch: 002 / 011 | Total loss: 1.520 | Reg loss: 0.040 | Tree loss: 1.520 | Accuracy: 0.437500 | 1.764 sec/iter\n",
      "Epoch: 444 | Batch: 003 / 011 | Total loss: 1.500 | Reg loss: 0.040 | Tree loss: 1.500 | Accuracy: 0.464500 | 1.763 sec/iter\n",
      "Epoch: 444 | Batch: 004 / 011 | Total loss: 1.448 | Reg loss: 0.040 | Tree loss: 1.448 | Accuracy: 0.485500 | 1.763 sec/iter\n",
      "Epoch: 444 | Batch: 005 / 011 | Total loss: 1.435 | Reg loss: 0.040 | Tree loss: 1.435 | Accuracy: 0.497000 | 1.763 sec/iter\n",
      "Epoch: 444 | Batch: 006 / 011 | Total loss: 1.424 | Reg loss: 0.040 | Tree loss: 1.424 | Accuracy: 0.538500 | 1.763 sec/iter\n",
      "Epoch: 444 | Batch: 007 / 011 | Total loss: 1.415 | Reg loss: 0.040 | Tree loss: 1.415 | Accuracy: 0.518500 | 1.763 sec/iter\n",
      "Epoch: 444 | Batch: 008 / 011 | Total loss: 1.395 | Reg loss: 0.040 | Tree loss: 1.395 | Accuracy: 0.519500 | 1.763 sec/iter\n",
      "Epoch: 444 | Batch: 009 / 011 | Total loss: 1.384 | Reg loss: 0.040 | Tree loss: 1.384 | Accuracy: 0.530500 | 1.762 sec/iter\n",
      "Epoch: 444 | Batch: 010 / 011 | Total loss: 1.380 | Reg loss: 0.040 | Tree loss: 1.380 | Accuracy: 0.522184 | 1.762 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 445 | Batch: 000 / 011 | Total loss: 1.603 | Reg loss: 0.040 | Tree loss: 1.603 | Accuracy: 0.412500 | 1.764 sec/iter\n",
      "Epoch: 445 | Batch: 001 / 011 | Total loss: 1.579 | Reg loss: 0.040 | Tree loss: 1.579 | Accuracy: 0.401000 | 1.764 sec/iter\n",
      "Epoch: 445 | Batch: 002 / 011 | Total loss: 1.515 | Reg loss: 0.040 | Tree loss: 1.515 | Accuracy: 0.446500 | 1.763 sec/iter\n",
      "Epoch: 445 | Batch: 003 / 011 | Total loss: 1.481 | Reg loss: 0.040 | Tree loss: 1.481 | Accuracy: 0.492000 | 1.763 sec/iter\n",
      "Epoch: 445 | Batch: 004 / 011 | Total loss: 1.462 | Reg loss: 0.040 | Tree loss: 1.462 | Accuracy: 0.493000 | 1.763 sec/iter\n",
      "Epoch: 445 | Batch: 005 / 011 | Total loss: 1.443 | Reg loss: 0.040 | Tree loss: 1.443 | Accuracy: 0.497500 | 1.763 sec/iter\n",
      "Epoch: 445 | Batch: 006 / 011 | Total loss: 1.408 | Reg loss: 0.040 | Tree loss: 1.408 | Accuracy: 0.507500 | 1.763 sec/iter\n",
      "Epoch: 445 | Batch: 007 / 011 | Total loss: 1.407 | Reg loss: 0.040 | Tree loss: 1.407 | Accuracy: 0.533500 | 1.763 sec/iter\n",
      "Epoch: 445 | Batch: 008 / 011 | Total loss: 1.401 | Reg loss: 0.040 | Tree loss: 1.401 | Accuracy: 0.521000 | 1.763 sec/iter\n",
      "Epoch: 445 | Batch: 009 / 011 | Total loss: 1.424 | Reg loss: 0.040 | Tree loss: 1.424 | Accuracy: 0.502500 | 1.763 sec/iter\n",
      "Epoch: 445 | Batch: 010 / 011 | Total loss: 1.413 | Reg loss: 0.040 | Tree loss: 1.413 | Accuracy: 0.484642 | 1.763 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 446 | Batch: 000 / 011 | Total loss: 1.605 | Reg loss: 0.040 | Tree loss: 1.605 | Accuracy: 0.401000 | 1.763 sec/iter\n",
      "Epoch: 446 | Batch: 001 / 011 | Total loss: 1.544 | Reg loss: 0.040 | Tree loss: 1.544 | Accuracy: 0.431500 | 1.763 sec/iter\n",
      "Epoch: 446 | Batch: 002 / 011 | Total loss: 1.535 | Reg loss: 0.040 | Tree loss: 1.535 | Accuracy: 0.449500 | 1.763 sec/iter\n",
      "Epoch: 446 | Batch: 003 / 011 | Total loss: 1.517 | Reg loss: 0.040 | Tree loss: 1.517 | Accuracy: 0.439500 | 1.763 sec/iter\n",
      "Epoch: 446 | Batch: 004 / 011 | Total loss: 1.454 | Reg loss: 0.040 | Tree loss: 1.454 | Accuracy: 0.486000 | 1.763 sec/iter\n",
      "Epoch: 446 | Batch: 005 / 011 | Total loss: 1.428 | Reg loss: 0.040 | Tree loss: 1.428 | Accuracy: 0.510000 | 1.762 sec/iter\n",
      "Epoch: 446 | Batch: 006 / 011 | Total loss: 1.420 | Reg loss: 0.040 | Tree loss: 1.420 | Accuracy: 0.519500 | 1.762 sec/iter\n",
      "Epoch: 446 | Batch: 007 / 011 | Total loss: 1.409 | Reg loss: 0.040 | Tree loss: 1.409 | Accuracy: 0.514000 | 1.762 sec/iter\n",
      "Epoch: 446 | Batch: 008 / 011 | Total loss: 1.394 | Reg loss: 0.040 | Tree loss: 1.394 | Accuracy: 0.531000 | 1.762 sec/iter\n",
      "Epoch: 446 | Batch: 009 / 011 | Total loss: 1.404 | Reg loss: 0.040 | Tree loss: 1.404 | Accuracy: 0.499500 | 1.762 sec/iter\n",
      "Epoch: 446 | Batch: 010 / 011 | Total loss: 1.438 | Reg loss: 0.040 | Tree loss: 1.438 | Accuracy: 0.529010 | 1.762 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 447 | Batch: 000 / 011 | Total loss: 1.610 | Reg loss: 0.040 | Tree loss: 1.610 | Accuracy: 0.402000 | 1.763 sec/iter\n",
      "Epoch: 447 | Batch: 001 / 011 | Total loss: 1.570 | Reg loss: 0.040 | Tree loss: 1.570 | Accuracy: 0.431000 | 1.763 sec/iter\n",
      "Epoch: 447 | Batch: 002 / 011 | Total loss: 1.515 | Reg loss: 0.040 | Tree loss: 1.515 | Accuracy: 0.446500 | 1.763 sec/iter\n",
      "Epoch: 447 | Batch: 003 / 011 | Total loss: 1.493 | Reg loss: 0.040 | Tree loss: 1.493 | Accuracy: 0.439000 | 1.763 sec/iter\n",
      "Epoch: 447 | Batch: 004 / 011 | Total loss: 1.467 | Reg loss: 0.040 | Tree loss: 1.467 | Accuracy: 0.475000 | 1.763 sec/iter\n",
      "Epoch: 447 | Batch: 005 / 011 | Total loss: 1.421 | Reg loss: 0.040 | Tree loss: 1.421 | Accuracy: 0.507500 | 1.763 sec/iter\n",
      "Epoch: 447 | Batch: 006 / 011 | Total loss: 1.422 | Reg loss: 0.040 | Tree loss: 1.422 | Accuracy: 0.503500 | 1.763 sec/iter\n",
      "Epoch: 447 | Batch: 007 / 011 | Total loss: 1.388 | Reg loss: 0.040 | Tree loss: 1.388 | Accuracy: 0.529000 | 1.763 sec/iter\n",
      "Epoch: 447 | Batch: 008 / 011 | Total loss: 1.431 | Reg loss: 0.040 | Tree loss: 1.431 | Accuracy: 0.491500 | 1.762 sec/iter\n",
      "Epoch: 447 | Batch: 009 / 011 | Total loss: 1.397 | Reg loss: 0.040 | Tree loss: 1.397 | Accuracy: 0.489000 | 1.762 sec/iter\n",
      "Epoch: 447 | Batch: 010 / 011 | Total loss: 1.446 | Reg loss: 0.040 | Tree loss: 1.446 | Accuracy: 0.457338 | 1.762 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 448 | Batch: 000 / 011 | Total loss: 1.615 | Reg loss: 0.040 | Tree loss: 1.615 | Accuracy: 0.393500 | 1.762 sec/iter\n",
      "Epoch: 448 | Batch: 001 / 011 | Total loss: 1.583 | Reg loss: 0.040 | Tree loss: 1.583 | Accuracy: 0.399500 | 1.762 sec/iter\n",
      "Epoch: 448 | Batch: 002 / 011 | Total loss: 1.518 | Reg loss: 0.040 | Tree loss: 1.518 | Accuracy: 0.458000 | 1.762 sec/iter\n",
      "Epoch: 448 | Batch: 003 / 011 | Total loss: 1.491 | Reg loss: 0.040 | Tree loss: 1.491 | Accuracy: 0.457000 | 1.762 sec/iter\n",
      "Epoch: 448 | Batch: 004 / 011 | Total loss: 1.454 | Reg loss: 0.040 | Tree loss: 1.454 | Accuracy: 0.490500 | 1.762 sec/iter\n",
      "Epoch: 448 | Batch: 005 / 011 | Total loss: 1.439 | Reg loss: 0.040 | Tree loss: 1.439 | Accuracy: 0.486000 | 1.762 sec/iter\n",
      "Epoch: 448 | Batch: 006 / 011 | Total loss: 1.421 | Reg loss: 0.040 | Tree loss: 1.421 | Accuracy: 0.507500 | 1.762 sec/iter\n",
      "Epoch: 448 | Batch: 007 / 011 | Total loss: 1.430 | Reg loss: 0.040 | Tree loss: 1.430 | Accuracy: 0.509000 | 1.762 sec/iter\n",
      "Epoch: 448 | Batch: 008 / 011 | Total loss: 1.383 | Reg loss: 0.040 | Tree loss: 1.383 | Accuracy: 0.512000 | 1.762 sec/iter\n",
      "Epoch: 448 | Batch: 009 / 011 | Total loss: 1.408 | Reg loss: 0.040 | Tree loss: 1.408 | Accuracy: 0.504500 | 1.761 sec/iter\n",
      "Epoch: 448 | Batch: 010 / 011 | Total loss: 1.372 | Reg loss: 0.040 | Tree loss: 1.372 | Accuracy: 0.518771 | 1.761 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 449 | Batch: 000 / 011 | Total loss: 1.599 | Reg loss: 0.040 | Tree loss: 1.599 | Accuracy: 0.403000 | 1.762 sec/iter\n",
      "Epoch: 449 | Batch: 001 / 011 | Total loss: 1.580 | Reg loss: 0.040 | Tree loss: 1.580 | Accuracy: 0.411500 | 1.762 sec/iter\n",
      "Epoch: 449 | Batch: 002 / 011 | Total loss: 1.517 | Reg loss: 0.040 | Tree loss: 1.517 | Accuracy: 0.434500 | 1.762 sec/iter\n",
      "Epoch: 449 | Batch: 003 / 011 | Total loss: 1.492 | Reg loss: 0.040 | Tree loss: 1.492 | Accuracy: 0.442000 | 1.762 sec/iter\n",
      "Epoch: 449 | Batch: 004 / 011 | Total loss: 1.450 | Reg loss: 0.040 | Tree loss: 1.450 | Accuracy: 0.498000 | 1.762 sec/iter\n",
      "Epoch: 449 | Batch: 005 / 011 | Total loss: 1.436 | Reg loss: 0.040 | Tree loss: 1.436 | Accuracy: 0.508500 | 1.762 sec/iter\n",
      "Epoch: 449 | Batch: 006 / 011 | Total loss: 1.421 | Reg loss: 0.040 | Tree loss: 1.421 | Accuracy: 0.497500 | 1.762 sec/iter\n",
      "Epoch: 449 | Batch: 007 / 011 | Total loss: 1.412 | Reg loss: 0.040 | Tree loss: 1.412 | Accuracy: 0.520500 | 1.762 sec/iter\n",
      "Epoch: 449 | Batch: 008 / 011 | Total loss: 1.411 | Reg loss: 0.040 | Tree loss: 1.411 | Accuracy: 0.501000 | 1.762 sec/iter\n",
      "Epoch: 449 | Batch: 009 / 011 | Total loss: 1.408 | Reg loss: 0.040 | Tree loss: 1.408 | Accuracy: 0.492000 | 1.762 sec/iter\n",
      "Epoch: 449 | Batch: 010 / 011 | Total loss: 1.364 | Reg loss: 0.040 | Tree loss: 1.364 | Accuracy: 0.501706 | 1.762 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 450 | Batch: 000 / 011 | Total loss: 1.593 | Reg loss: 0.040 | Tree loss: 1.593 | Accuracy: 0.406000 | 1.762 sec/iter\n",
      "Epoch: 450 | Batch: 001 / 011 | Total loss: 1.560 | Reg loss: 0.040 | Tree loss: 1.560 | Accuracy: 0.419000 | 1.761 sec/iter\n",
      "Epoch: 450 | Batch: 002 / 011 | Total loss: 1.517 | Reg loss: 0.040 | Tree loss: 1.517 | Accuracy: 0.451500 | 1.761 sec/iter\n",
      "Epoch: 450 | Batch: 003 / 011 | Total loss: 1.511 | Reg loss: 0.040 | Tree loss: 1.511 | Accuracy: 0.447000 | 1.761 sec/iter\n",
      "Epoch: 450 | Batch: 004 / 011 | Total loss: 1.451 | Reg loss: 0.040 | Tree loss: 1.451 | Accuracy: 0.481000 | 1.761 sec/iter\n",
      "Epoch: 450 | Batch: 005 / 011 | Total loss: 1.456 | Reg loss: 0.040 | Tree loss: 1.456 | Accuracy: 0.472000 | 1.761 sec/iter\n",
      "Epoch: 450 | Batch: 006 / 011 | Total loss: 1.405 | Reg loss: 0.040 | Tree loss: 1.405 | Accuracy: 0.517500 | 1.761 sec/iter\n",
      "Epoch: 450 | Batch: 007 / 011 | Total loss: 1.423 | Reg loss: 0.040 | Tree loss: 1.423 | Accuracy: 0.507000 | 1.761 sec/iter\n",
      "Epoch: 450 | Batch: 008 / 011 | Total loss: 1.397 | Reg loss: 0.040 | Tree loss: 1.397 | Accuracy: 0.509500 | 1.761 sec/iter\n",
      "Epoch: 450 | Batch: 009 / 011 | Total loss: 1.408 | Reg loss: 0.040 | Tree loss: 1.408 | Accuracy: 0.514000 | 1.761 sec/iter\n",
      "Epoch: 450 | Batch: 010 / 011 | Total loss: 1.410 | Reg loss: 0.040 | Tree loss: 1.410 | Accuracy: 0.491468 | 1.761 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 451 | Batch: 000 / 011 | Total loss: 1.618 | Reg loss: 0.040 | Tree loss: 1.618 | Accuracy: 0.391000 | 1.762 sec/iter\n",
      "Epoch: 451 | Batch: 001 / 011 | Total loss: 1.558 | Reg loss: 0.040 | Tree loss: 1.558 | Accuracy: 0.418500 | 1.762 sec/iter\n",
      "Epoch: 451 | Batch: 002 / 011 | Total loss: 1.536 | Reg loss: 0.040 | Tree loss: 1.536 | Accuracy: 0.422500 | 1.762 sec/iter\n",
      "Epoch: 451 | Batch: 003 / 011 | Total loss: 1.513 | Reg loss: 0.040 | Tree loss: 1.513 | Accuracy: 0.437500 | 1.761 sec/iter\n",
      "Epoch: 451 | Batch: 004 / 011 | Total loss: 1.469 | Reg loss: 0.040 | Tree loss: 1.469 | Accuracy: 0.456000 | 1.761 sec/iter\n",
      "Epoch: 451 | Batch: 005 / 011 | Total loss: 1.429 | Reg loss: 0.040 | Tree loss: 1.429 | Accuracy: 0.485000 | 1.761 sec/iter\n",
      "Epoch: 451 | Batch: 006 / 011 | Total loss: 1.409 | Reg loss: 0.040 | Tree loss: 1.409 | Accuracy: 0.511500 | 1.761 sec/iter\n",
      "Epoch: 451 | Batch: 007 / 011 | Total loss: 1.404 | Reg loss: 0.040 | Tree loss: 1.404 | Accuracy: 0.538500 | 1.761 sec/iter\n",
      "Epoch: 451 | Batch: 008 / 011 | Total loss: 1.404 | Reg loss: 0.040 | Tree loss: 1.404 | Accuracy: 0.514500 | 1.761 sec/iter\n",
      "Epoch: 451 | Batch: 009 / 011 | Total loss: 1.394 | Reg loss: 0.040 | Tree loss: 1.394 | Accuracy: 0.509000 | 1.761 sec/iter\n",
      "Epoch: 451 | Batch: 010 / 011 | Total loss: 1.426 | Reg loss: 0.040 | Tree loss: 1.426 | Accuracy: 0.470990 | 1.76 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 452 | Batch: 000 / 011 | Total loss: 1.609 | Reg loss: 0.040 | Tree loss: 1.609 | Accuracy: 0.390500 | 1.761 sec/iter\n",
      "Epoch: 452 | Batch: 001 / 011 | Total loss: 1.579 | Reg loss: 0.040 | Tree loss: 1.579 | Accuracy: 0.411500 | 1.761 sec/iter\n",
      "Epoch: 452 | Batch: 002 / 011 | Total loss: 1.528 | Reg loss: 0.040 | Tree loss: 1.528 | Accuracy: 0.429500 | 1.761 sec/iter\n",
      "Epoch: 452 | Batch: 003 / 011 | Total loss: 1.496 | Reg loss: 0.040 | Tree loss: 1.496 | Accuracy: 0.460000 | 1.761 sec/iter\n",
      "Epoch: 452 | Batch: 004 / 011 | Total loss: 1.458 | Reg loss: 0.040 | Tree loss: 1.458 | Accuracy: 0.465500 | 1.761 sec/iter\n",
      "Epoch: 452 | Batch: 005 / 011 | Total loss: 1.426 | Reg loss: 0.040 | Tree loss: 1.426 | Accuracy: 0.497000 | 1.761 sec/iter\n",
      "Epoch: 452 | Batch: 006 / 011 | Total loss: 1.425 | Reg loss: 0.040 | Tree loss: 1.425 | Accuracy: 0.510000 | 1.761 sec/iter\n",
      "Epoch: 452 | Batch: 007 / 011 | Total loss: 1.408 | Reg loss: 0.040 | Tree loss: 1.408 | Accuracy: 0.530500 | 1.761 sec/iter\n",
      "Epoch: 452 | Batch: 008 / 011 | Total loss: 1.394 | Reg loss: 0.040 | Tree loss: 1.394 | Accuracy: 0.524500 | 1.761 sec/iter\n",
      "Epoch: 452 | Batch: 009 / 011 | Total loss: 1.407 | Reg loss: 0.040 | Tree loss: 1.407 | Accuracy: 0.494000 | 1.761 sec/iter\n",
      "Epoch: 452 | Batch: 010 / 011 | Total loss: 1.367 | Reg loss: 0.040 | Tree loss: 1.367 | Accuracy: 0.481229 | 1.76 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 453 | Batch: 000 / 011 | Total loss: 1.603 | Reg loss: 0.040 | Tree loss: 1.603 | Accuracy: 0.390000 | 1.76 sec/iter\n",
      "Epoch: 453 | Batch: 001 / 011 | Total loss: 1.568 | Reg loss: 0.040 | Tree loss: 1.568 | Accuracy: 0.425500 | 1.76 sec/iter\n",
      "Epoch: 453 | Batch: 002 / 011 | Total loss: 1.550 | Reg loss: 0.040 | Tree loss: 1.550 | Accuracy: 0.422000 | 1.76 sec/iter\n",
      "Epoch: 453 | Batch: 003 / 011 | Total loss: 1.492 | Reg loss: 0.040 | Tree loss: 1.492 | Accuracy: 0.444500 | 1.76 sec/iter\n",
      "Epoch: 453 | Batch: 004 / 011 | Total loss: 1.452 | Reg loss: 0.040 | Tree loss: 1.452 | Accuracy: 0.482000 | 1.76 sec/iter\n",
      "Epoch: 453 | Batch: 005 / 011 | Total loss: 1.422 | Reg loss: 0.040 | Tree loss: 1.422 | Accuracy: 0.511500 | 1.76 sec/iter\n",
      "Epoch: 453 | Batch: 006 / 011 | Total loss: 1.432 | Reg loss: 0.040 | Tree loss: 1.432 | Accuracy: 0.500000 | 1.76 sec/iter\n",
      "Epoch: 453 | Batch: 007 / 011 | Total loss: 1.405 | Reg loss: 0.040 | Tree loss: 1.405 | Accuracy: 0.525000 | 1.76 sec/iter\n",
      "Epoch: 453 | Batch: 008 / 011 | Total loss: 1.413 | Reg loss: 0.040 | Tree loss: 1.413 | Accuracy: 0.504500 | 1.76 sec/iter\n",
      "Epoch: 453 | Batch: 009 / 011 | Total loss: 1.381 | Reg loss: 0.040 | Tree loss: 1.381 | Accuracy: 0.521500 | 1.76 sec/iter\n",
      "Epoch: 453 | Batch: 010 / 011 | Total loss: 1.371 | Reg loss: 0.040 | Tree loss: 1.371 | Accuracy: 0.529010 | 1.759 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 454 | Batch: 000 / 011 | Total loss: 1.595 | Reg loss: 0.040 | Tree loss: 1.595 | Accuracy: 0.419500 | 1.761 sec/iter\n",
      "Epoch: 454 | Batch: 001 / 011 | Total loss: 1.575 | Reg loss: 0.040 | Tree loss: 1.575 | Accuracy: 0.419000 | 1.761 sec/iter\n",
      "Epoch: 454 | Batch: 002 / 011 | Total loss: 1.544 | Reg loss: 0.040 | Tree loss: 1.544 | Accuracy: 0.425500 | 1.76 sec/iter\n",
      "Epoch: 454 | Batch: 003 / 011 | Total loss: 1.477 | Reg loss: 0.040 | Tree loss: 1.477 | Accuracy: 0.466500 | 1.76 sec/iter\n",
      "Epoch: 454 | Batch: 004 / 011 | Total loss: 1.464 | Reg loss: 0.040 | Tree loss: 1.464 | Accuracy: 0.469000 | 1.76 sec/iter\n",
      "Epoch: 454 | Batch: 005 / 011 | Total loss: 1.434 | Reg loss: 0.040 | Tree loss: 1.434 | Accuracy: 0.511000 | 1.76 sec/iter\n",
      "Epoch: 454 | Batch: 006 / 011 | Total loss: 1.414 | Reg loss: 0.040 | Tree loss: 1.414 | Accuracy: 0.525500 | 1.76 sec/iter\n",
      "Epoch: 454 | Batch: 007 / 011 | Total loss: 1.395 | Reg loss: 0.040 | Tree loss: 1.395 | Accuracy: 0.528000 | 1.76 sec/iter\n",
      "Epoch: 454 | Batch: 008 / 011 | Total loss: 1.420 | Reg loss: 0.040 | Tree loss: 1.420 | Accuracy: 0.505500 | 1.76 sec/iter\n",
      "Epoch: 454 | Batch: 009 / 011 | Total loss: 1.399 | Reg loss: 0.040 | Tree loss: 1.399 | Accuracy: 0.492000 | 1.76 sec/iter\n",
      "Epoch: 454 | Batch: 010 / 011 | Total loss: 1.370 | Reg loss: 0.040 | Tree loss: 1.370 | Accuracy: 0.508532 | 1.76 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 455 | Batch: 000 / 011 | Total loss: 1.606 | Reg loss: 0.040 | Tree loss: 1.606 | Accuracy: 0.393500 | 1.76 sec/iter\n",
      "Epoch: 455 | Batch: 001 / 011 | Total loss: 1.565 | Reg loss: 0.040 | Tree loss: 1.565 | Accuracy: 0.423500 | 1.76 sec/iter\n",
      "Epoch: 455 | Batch: 002 / 011 | Total loss: 1.529 | Reg loss: 0.040 | Tree loss: 1.529 | Accuracy: 0.444000 | 1.76 sec/iter\n",
      "Epoch: 455 | Batch: 003 / 011 | Total loss: 1.482 | Reg loss: 0.040 | Tree loss: 1.482 | Accuracy: 0.463500 | 1.759 sec/iter\n",
      "Epoch: 455 | Batch: 004 / 011 | Total loss: 1.488 | Reg loss: 0.040 | Tree loss: 1.488 | Accuracy: 0.457500 | 1.759 sec/iter\n",
      "Epoch: 455 | Batch: 005 / 011 | Total loss: 1.430 | Reg loss: 0.040 | Tree loss: 1.430 | Accuracy: 0.488500 | 1.759 sec/iter\n",
      "Epoch: 455 | Batch: 006 / 011 | Total loss: 1.431 | Reg loss: 0.040 | Tree loss: 1.431 | Accuracy: 0.531500 | 1.759 sec/iter\n",
      "Epoch: 455 | Batch: 007 / 011 | Total loss: 1.403 | Reg loss: 0.040 | Tree loss: 1.403 | Accuracy: 0.527500 | 1.759 sec/iter\n",
      "Epoch: 455 | Batch: 008 / 011 | Total loss: 1.381 | Reg loss: 0.040 | Tree loss: 1.381 | Accuracy: 0.517000 | 1.759 sec/iter\n",
      "Epoch: 455 | Batch: 009 / 011 | Total loss: 1.401 | Reg loss: 0.040 | Tree loss: 1.401 | Accuracy: 0.507000 | 1.759 sec/iter\n",
      "Epoch: 455 | Batch: 010 / 011 | Total loss: 1.425 | Reg loss: 0.040 | Tree loss: 1.425 | Accuracy: 0.460751 | 1.759 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 456 | Batch: 000 / 011 | Total loss: 1.602 | Reg loss: 0.040 | Tree loss: 1.602 | Accuracy: 0.397500 | 1.76 sec/iter\n",
      "Epoch: 456 | Batch: 001 / 011 | Total loss: 1.578 | Reg loss: 0.040 | Tree loss: 1.578 | Accuracy: 0.415500 | 1.76 sec/iter\n",
      "Epoch: 456 | Batch: 002 / 011 | Total loss: 1.514 | Reg loss: 0.040 | Tree loss: 1.514 | Accuracy: 0.455000 | 1.76 sec/iter\n",
      "Epoch: 456 | Batch: 003 / 011 | Total loss: 1.489 | Reg loss: 0.040 | Tree loss: 1.489 | Accuracy: 0.465000 | 1.76 sec/iter\n",
      "Epoch: 456 | Batch: 004 / 011 | Total loss: 1.452 | Reg loss: 0.040 | Tree loss: 1.452 | Accuracy: 0.479000 | 1.76 sec/iter\n",
      "Epoch: 456 | Batch: 005 / 011 | Total loss: 1.420 | Reg loss: 0.040 | Tree loss: 1.420 | Accuracy: 0.511500 | 1.76 sec/iter\n",
      "Epoch: 456 | Batch: 006 / 011 | Total loss: 1.415 | Reg loss: 0.040 | Tree loss: 1.415 | Accuracy: 0.527500 | 1.76 sec/iter\n",
      "Epoch: 456 | Batch: 007 / 011 | Total loss: 1.436 | Reg loss: 0.040 | Tree loss: 1.436 | Accuracy: 0.502500 | 1.759 sec/iter\n",
      "Epoch: 456 | Batch: 008 / 011 | Total loss: 1.393 | Reg loss: 0.040 | Tree loss: 1.393 | Accuracy: 0.520500 | 1.759 sec/iter\n",
      "Epoch: 456 | Batch: 009 / 011 | Total loss: 1.426 | Reg loss: 0.040 | Tree loss: 1.426 | Accuracy: 0.475500 | 1.759 sec/iter\n",
      "Epoch: 456 | Batch: 010 / 011 | Total loss: 1.403 | Reg loss: 0.040 | Tree loss: 1.403 | Accuracy: 0.508532 | 1.759 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 457 | Batch: 000 / 011 | Total loss: 1.619 | Reg loss: 0.040 | Tree loss: 1.619 | Accuracy: 0.406000 | 1.759 sec/iter\n",
      "Epoch: 457 | Batch: 001 / 011 | Total loss: 1.583 | Reg loss: 0.040 | Tree loss: 1.583 | Accuracy: 0.414500 | 1.759 sec/iter\n",
      "Epoch: 457 | Batch: 002 / 011 | Total loss: 1.519 | Reg loss: 0.040 | Tree loss: 1.519 | Accuracy: 0.441000 | 1.759 sec/iter\n",
      "Epoch: 457 | Batch: 003 / 011 | Total loss: 1.464 | Reg loss: 0.040 | Tree loss: 1.464 | Accuracy: 0.468500 | 1.759 sec/iter\n",
      "Epoch: 457 | Batch: 004 / 011 | Total loss: 1.462 | Reg loss: 0.040 | Tree loss: 1.462 | Accuracy: 0.475500 | 1.759 sec/iter\n",
      "Epoch: 457 | Batch: 005 / 011 | Total loss: 1.430 | Reg loss: 0.040 | Tree loss: 1.430 | Accuracy: 0.500000 | 1.759 sec/iter\n",
      "Epoch: 457 | Batch: 006 / 011 | Total loss: 1.417 | Reg loss: 0.040 | Tree loss: 1.417 | Accuracy: 0.513000 | 1.759 sec/iter\n",
      "Epoch: 457 | Batch: 007 / 011 | Total loss: 1.405 | Reg loss: 0.040 | Tree loss: 1.405 | Accuracy: 0.519500 | 1.759 sec/iter\n",
      "Epoch: 457 | Batch: 008 / 011 | Total loss: 1.418 | Reg loss: 0.040 | Tree loss: 1.418 | Accuracy: 0.509000 | 1.758 sec/iter\n",
      "Epoch: 457 | Batch: 009 / 011 | Total loss: 1.405 | Reg loss: 0.040 | Tree loss: 1.405 | Accuracy: 0.505500 | 1.758 sec/iter\n",
      "Epoch: 457 | Batch: 010 / 011 | Total loss: 1.425 | Reg loss: 0.040 | Tree loss: 1.425 | Accuracy: 0.467577 | 1.758 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 458 | Batch: 000 / 011 | Total loss: 1.614 | Reg loss: 0.040 | Tree loss: 1.614 | Accuracy: 0.394500 | 1.759 sec/iter\n",
      "Epoch: 458 | Batch: 001 / 011 | Total loss: 1.560 | Reg loss: 0.040 | Tree loss: 1.560 | Accuracy: 0.421000 | 1.759 sec/iter\n",
      "Epoch: 458 | Batch: 002 / 011 | Total loss: 1.527 | Reg loss: 0.040 | Tree loss: 1.527 | Accuracy: 0.438000 | 1.759 sec/iter\n",
      "Epoch: 458 | Batch: 003 / 011 | Total loss: 1.496 | Reg loss: 0.040 | Tree loss: 1.496 | Accuracy: 0.455500 | 1.759 sec/iter\n",
      "Epoch: 458 | Batch: 004 / 011 | Total loss: 1.445 | Reg loss: 0.040 | Tree loss: 1.445 | Accuracy: 0.502500 | 1.759 sec/iter\n",
      "Epoch: 458 | Batch: 005 / 011 | Total loss: 1.441 | Reg loss: 0.040 | Tree loss: 1.441 | Accuracy: 0.499000 | 1.759 sec/iter\n",
      "Epoch: 458 | Batch: 006 / 011 | Total loss: 1.416 | Reg loss: 0.040 | Tree loss: 1.416 | Accuracy: 0.513000 | 1.759 sec/iter\n",
      "Epoch: 458 | Batch: 007 / 011 | Total loss: 1.417 | Reg loss: 0.040 | Tree loss: 1.417 | Accuracy: 0.510000 | 1.758 sec/iter\n",
      "Epoch: 458 | Batch: 008 / 011 | Total loss: 1.417 | Reg loss: 0.040 | Tree loss: 1.417 | Accuracy: 0.494500 | 1.758 sec/iter\n",
      "Epoch: 458 | Batch: 009 / 011 | Total loss: 1.399 | Reg loss: 0.040 | Tree loss: 1.399 | Accuracy: 0.504000 | 1.758 sec/iter\n",
      "Epoch: 458 | Batch: 010 / 011 | Total loss: 1.440 | Reg loss: 0.040 | Tree loss: 1.440 | Accuracy: 0.477816 | 1.758 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 459 | Batch: 000 / 011 | Total loss: 1.588 | Reg loss: 0.040 | Tree loss: 1.588 | Accuracy: 0.403500 | 1.759 sec/iter\n",
      "Epoch: 459 | Batch: 001 / 011 | Total loss: 1.571 | Reg loss: 0.040 | Tree loss: 1.571 | Accuracy: 0.415000 | 1.759 sec/iter\n",
      "Epoch: 459 | Batch: 002 / 011 | Total loss: 1.544 | Reg loss: 0.040 | Tree loss: 1.544 | Accuracy: 0.420000 | 1.759 sec/iter\n",
      "Epoch: 459 | Batch: 003 / 011 | Total loss: 1.480 | Reg loss: 0.040 | Tree loss: 1.480 | Accuracy: 0.452000 | 1.759 sec/iter\n",
      "Epoch: 459 | Batch: 004 / 011 | Total loss: 1.458 | Reg loss: 0.040 | Tree loss: 1.458 | Accuracy: 0.486500 | 1.759 sec/iter\n",
      "Epoch: 459 | Batch: 005 / 011 | Total loss: 1.426 | Reg loss: 0.040 | Tree loss: 1.426 | Accuracy: 0.502000 | 1.759 sec/iter\n",
      "Epoch: 459 | Batch: 006 / 011 | Total loss: 1.398 | Reg loss: 0.040 | Tree loss: 1.398 | Accuracy: 0.505000 | 1.758 sec/iter\n",
      "Epoch: 459 | Batch: 007 / 011 | Total loss: 1.418 | Reg loss: 0.040 | Tree loss: 1.418 | Accuracy: 0.522500 | 1.758 sec/iter\n",
      "Epoch: 459 | Batch: 008 / 011 | Total loss: 1.431 | Reg loss: 0.040 | Tree loss: 1.431 | Accuracy: 0.508500 | 1.758 sec/iter\n",
      "Epoch: 459 | Batch: 009 / 011 | Total loss: 1.405 | Reg loss: 0.040 | Tree loss: 1.405 | Accuracy: 0.515000 | 1.758 sec/iter\n",
      "Epoch: 459 | Batch: 010 / 011 | Total loss: 1.415 | Reg loss: 0.040 | Tree loss: 1.415 | Accuracy: 0.498294 | 1.758 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 460 | Batch: 000 / 011 | Total loss: 1.609 | Reg loss: 0.040 | Tree loss: 1.609 | Accuracy: 0.417000 | 1.758 sec/iter\n",
      "Epoch: 460 | Batch: 001 / 011 | Total loss: 1.571 | Reg loss: 0.040 | Tree loss: 1.571 | Accuracy: 0.420500 | 1.758 sec/iter\n",
      "Epoch: 460 | Batch: 002 / 011 | Total loss: 1.535 | Reg loss: 0.040 | Tree loss: 1.535 | Accuracy: 0.432500 | 1.758 sec/iter\n",
      "Epoch: 460 | Batch: 003 / 011 | Total loss: 1.493 | Reg loss: 0.040 | Tree loss: 1.493 | Accuracy: 0.452500 | 1.758 sec/iter\n",
      "Epoch: 460 | Batch: 004 / 011 | Total loss: 1.463 | Reg loss: 0.040 | Tree loss: 1.463 | Accuracy: 0.481500 | 1.758 sec/iter\n",
      "Epoch: 460 | Batch: 005 / 011 | Total loss: 1.419 | Reg loss: 0.040 | Tree loss: 1.419 | Accuracy: 0.530000 | 1.758 sec/iter\n",
      "Epoch: 460 | Batch: 006 / 011 | Total loss: 1.385 | Reg loss: 0.040 | Tree loss: 1.385 | Accuracy: 0.519500 | 1.758 sec/iter\n",
      "Epoch: 460 | Batch: 007 / 011 | Total loss: 1.414 | Reg loss: 0.040 | Tree loss: 1.414 | Accuracy: 0.507000 | 1.757 sec/iter\n",
      "Epoch: 460 | Batch: 008 / 011 | Total loss: 1.405 | Reg loss: 0.040 | Tree loss: 1.405 | Accuracy: 0.513000 | 1.757 sec/iter\n",
      "Epoch: 460 | Batch: 009 / 011 | Total loss: 1.421 | Reg loss: 0.040 | Tree loss: 1.421 | Accuracy: 0.483500 | 1.757 sec/iter\n",
      "Epoch: 460 | Batch: 010 / 011 | Total loss: 1.429 | Reg loss: 0.040 | Tree loss: 1.429 | Accuracy: 0.508532 | 1.757 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 461 | Batch: 000 / 011 | Total loss: 1.588 | Reg loss: 0.040 | Tree loss: 1.588 | Accuracy: 0.405000 | 1.758 sec/iter\n",
      "Epoch: 461 | Batch: 001 / 011 | Total loss: 1.581 | Reg loss: 0.040 | Tree loss: 1.581 | Accuracy: 0.410000 | 1.758 sec/iter\n",
      "Epoch: 461 | Batch: 002 / 011 | Total loss: 1.513 | Reg loss: 0.040 | Tree loss: 1.513 | Accuracy: 0.425500 | 1.758 sec/iter\n",
      "Epoch: 461 | Batch: 003 / 011 | Total loss: 1.499 | Reg loss: 0.040 | Tree loss: 1.499 | Accuracy: 0.447000 | 1.758 sec/iter\n",
      "Epoch: 461 | Batch: 004 / 011 | Total loss: 1.466 | Reg loss: 0.040 | Tree loss: 1.466 | Accuracy: 0.471500 | 1.758 sec/iter\n",
      "Epoch: 461 | Batch: 005 / 011 | Total loss: 1.446 | Reg loss: 0.040 | Tree loss: 1.446 | Accuracy: 0.492000 | 1.758 sec/iter\n",
      "Epoch: 461 | Batch: 006 / 011 | Total loss: 1.434 | Reg loss: 0.040 | Tree loss: 1.434 | Accuracy: 0.502000 | 1.758 sec/iter\n",
      "Epoch: 461 | Batch: 007 / 011 | Total loss: 1.399 | Reg loss: 0.040 | Tree loss: 1.399 | Accuracy: 0.518500 | 1.758 sec/iter\n",
      "Epoch: 461 | Batch: 008 / 011 | Total loss: 1.397 | Reg loss: 0.040 | Tree loss: 1.397 | Accuracy: 0.514500 | 1.758 sec/iter\n",
      "Epoch: 461 | Batch: 009 / 011 | Total loss: 1.421 | Reg loss: 0.040 | Tree loss: 1.421 | Accuracy: 0.503500 | 1.758 sec/iter\n",
      "Epoch: 461 | Batch: 010 / 011 | Total loss: 1.360 | Reg loss: 0.040 | Tree loss: 1.360 | Accuracy: 0.515358 | 1.757 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 462 | Batch: 000 / 011 | Total loss: 1.594 | Reg loss: 0.040 | Tree loss: 1.594 | Accuracy: 0.404500 | 1.757 sec/iter\n",
      "Epoch: 462 | Batch: 001 / 011 | Total loss: 1.576 | Reg loss: 0.040 | Tree loss: 1.576 | Accuracy: 0.392500 | 1.757 sec/iter\n",
      "Epoch: 462 | Batch: 002 / 011 | Total loss: 1.515 | Reg loss: 0.040 | Tree loss: 1.515 | Accuracy: 0.426000 | 1.757 sec/iter\n",
      "Epoch: 462 | Batch: 003 / 011 | Total loss: 1.493 | Reg loss: 0.040 | Tree loss: 1.493 | Accuracy: 0.457500 | 1.757 sec/iter\n",
      "Epoch: 462 | Batch: 004 / 011 | Total loss: 1.472 | Reg loss: 0.040 | Tree loss: 1.472 | Accuracy: 0.476000 | 1.757 sec/iter\n",
      "Epoch: 462 | Batch: 005 / 011 | Total loss: 1.426 | Reg loss: 0.040 | Tree loss: 1.426 | Accuracy: 0.517000 | 1.757 sec/iter\n",
      "Epoch: 462 | Batch: 006 / 011 | Total loss: 1.404 | Reg loss: 0.040 | Tree loss: 1.404 | Accuracy: 0.521500 | 1.757 sec/iter\n",
      "Epoch: 462 | Batch: 007 / 011 | Total loss: 1.401 | Reg loss: 0.040 | Tree loss: 1.401 | Accuracy: 0.528000 | 1.757 sec/iter\n",
      "Epoch: 462 | Batch: 008 / 011 | Total loss: 1.415 | Reg loss: 0.040 | Tree loss: 1.415 | Accuracy: 0.518500 | 1.757 sec/iter\n",
      "Epoch: 462 | Batch: 009 / 011 | Total loss: 1.399 | Reg loss: 0.040 | Tree loss: 1.399 | Accuracy: 0.510500 | 1.757 sec/iter\n",
      "Epoch: 462 | Batch: 010 / 011 | Total loss: 1.508 | Reg loss: 0.040 | Tree loss: 1.508 | Accuracy: 0.457338 | 1.756 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 463 | Batch: 000 / 011 | Total loss: 1.594 | Reg loss: 0.040 | Tree loss: 1.594 | Accuracy: 0.400500 | 1.758 sec/iter\n",
      "Epoch: 463 | Batch: 001 / 011 | Total loss: 1.559 | Reg loss: 0.040 | Tree loss: 1.559 | Accuracy: 0.419500 | 1.758 sec/iter\n",
      "Epoch: 463 | Batch: 002 / 011 | Total loss: 1.506 | Reg loss: 0.040 | Tree loss: 1.506 | Accuracy: 0.432500 | 1.757 sec/iter\n",
      "Epoch: 463 | Batch: 003 / 011 | Total loss: 1.504 | Reg loss: 0.040 | Tree loss: 1.504 | Accuracy: 0.462500 | 1.757 sec/iter\n",
      "Epoch: 463 | Batch: 004 / 011 | Total loss: 1.478 | Reg loss: 0.040 | Tree loss: 1.478 | Accuracy: 0.474500 | 1.757 sec/iter\n",
      "Epoch: 463 | Batch: 005 / 011 | Total loss: 1.439 | Reg loss: 0.040 | Tree loss: 1.439 | Accuracy: 0.503000 | 1.757 sec/iter\n",
      "Epoch: 463 | Batch: 006 / 011 | Total loss: 1.379 | Reg loss: 0.040 | Tree loss: 1.379 | Accuracy: 0.545000 | 1.757 sec/iter\n",
      "Epoch: 463 | Batch: 007 / 011 | Total loss: 1.428 | Reg loss: 0.040 | Tree loss: 1.428 | Accuracy: 0.522000 | 1.757 sec/iter\n",
      "Epoch: 463 | Batch: 008 / 011 | Total loss: 1.424 | Reg loss: 0.040 | Tree loss: 1.424 | Accuracy: 0.518500 | 1.757 sec/iter\n",
      "Epoch: 463 | Batch: 009 / 011 | Total loss: 1.413 | Reg loss: 0.040 | Tree loss: 1.413 | Accuracy: 0.492000 | 1.757 sec/iter\n",
      "Epoch: 463 | Batch: 010 / 011 | Total loss: 1.432 | Reg loss: 0.040 | Tree loss: 1.432 | Accuracy: 0.498294 | 1.757 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 464 | Batch: 000 / 011 | Total loss: 1.605 | Reg loss: 0.040 | Tree loss: 1.605 | Accuracy: 0.396000 | 1.757 sec/iter\n",
      "Epoch: 464 | Batch: 001 / 011 | Total loss: 1.570 | Reg loss: 0.040 | Tree loss: 1.570 | Accuracy: 0.418500 | 1.757 sec/iter\n",
      "Epoch: 464 | Batch: 002 / 011 | Total loss: 1.524 | Reg loss: 0.040 | Tree loss: 1.524 | Accuracy: 0.428000 | 1.757 sec/iter\n",
      "Epoch: 464 | Batch: 003 / 011 | Total loss: 1.508 | Reg loss: 0.040 | Tree loss: 1.508 | Accuracy: 0.443000 | 1.757 sec/iter\n",
      "Epoch: 464 | Batch: 004 / 011 | Total loss: 1.445 | Reg loss: 0.040 | Tree loss: 1.445 | Accuracy: 0.487000 | 1.756 sec/iter\n",
      "Epoch: 464 | Batch: 005 / 011 | Total loss: 1.435 | Reg loss: 0.040 | Tree loss: 1.435 | Accuracy: 0.490500 | 1.756 sec/iter\n",
      "Epoch: 464 | Batch: 006 / 011 | Total loss: 1.433 | Reg loss: 0.040 | Tree loss: 1.433 | Accuracy: 0.520000 | 1.756 sec/iter\n",
      "Epoch: 464 | Batch: 007 / 011 | Total loss: 1.389 | Reg loss: 0.040 | Tree loss: 1.389 | Accuracy: 0.550000 | 1.756 sec/iter\n",
      "Epoch: 464 | Batch: 008 / 011 | Total loss: 1.403 | Reg loss: 0.040 | Tree loss: 1.403 | Accuracy: 0.496500 | 1.756 sec/iter\n",
      "Epoch: 464 | Batch: 009 / 011 | Total loss: 1.413 | Reg loss: 0.040 | Tree loss: 1.413 | Accuracy: 0.494500 | 1.756 sec/iter\n",
      "Epoch: 464 | Batch: 010 / 011 | Total loss: 1.328 | Reg loss: 0.040 | Tree loss: 1.328 | Accuracy: 0.529010 | 1.756 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 8: 0.9723756906077345\n",
      "Epoch: 465 | Batch: 000 / 011 | Total loss: 1.621 | Reg loss: 0.040 | Tree loss: 1.621 | Accuracy: 0.378000 | 1.757 sec/iter\n",
      "Epoch: 465 | Batch: 001 / 011 | Total loss: 1.568 | Reg loss: 0.040 | Tree loss: 1.568 | Accuracy: 0.406000 | 1.757 sec/iter\n",
      "Epoch: 465 | Batch: 002 / 011 | Total loss: 1.518 | Reg loss: 0.040 | Tree loss: 1.518 | Accuracy: 0.430500 | 1.757 sec/iter\n",
      "Epoch: 465 | Batch: 003 / 011 | Total loss: 1.489 | Reg loss: 0.040 | Tree loss: 1.489 | Accuracy: 0.457500 | 1.757 sec/iter\n",
      "Epoch: 465 | Batch: 004 / 011 | Total loss: 1.455 | Reg loss: 0.040 | Tree loss: 1.455 | Accuracy: 0.486000 | 1.757 sec/iter\n",
      "Epoch: 465 | Batch: 005 / 011 | Total loss: 1.428 | Reg loss: 0.040 | Tree loss: 1.428 | Accuracy: 0.523000 | 1.756 sec/iter\n",
      "Epoch: 465 | Batch: 006 / 011 | Total loss: 1.410 | Reg loss: 0.040 | Tree loss: 1.410 | Accuracy: 0.515000 | 1.756 sec/iter\n",
      "Epoch: 465 | Batch: 007 / 011 | Total loss: 1.420 | Reg loss: 0.040 | Tree loss: 1.420 | Accuracy: 0.520500 | 1.756 sec/iter\n",
      "Epoch: 465 | Batch: 008 / 011 | Total loss: 1.400 | Reg loss: 0.040 | Tree loss: 1.400 | Accuracy: 0.497000 | 1.756 sec/iter\n",
      "Epoch: 465 | Batch: 009 / 011 | Total loss: 1.419 | Reg loss: 0.040 | Tree loss: 1.419 | Accuracy: 0.496500 | 1.756 sec/iter\n",
      "Epoch: 465 | Batch: 010 / 011 | Total loss: 1.405 | Reg loss: 0.040 | Tree loss: 1.405 | Accuracy: 0.460751 | 1.756 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 466 | Batch: 000 / 011 | Total loss: 1.606 | Reg loss: 0.040 | Tree loss: 1.606 | Accuracy: 0.402500 | 1.757 sec/iter\n",
      "Epoch: 466 | Batch: 001 / 011 | Total loss: 1.584 | Reg loss: 0.040 | Tree loss: 1.584 | Accuracy: 0.404500 | 1.756 sec/iter\n",
      "Epoch: 466 | Batch: 002 / 011 | Total loss: 1.528 | Reg loss: 0.040 | Tree loss: 1.528 | Accuracy: 0.433500 | 1.756 sec/iter\n",
      "Epoch: 466 | Batch: 003 / 011 | Total loss: 1.515 | Reg loss: 0.040 | Tree loss: 1.515 | Accuracy: 0.442500 | 1.756 sec/iter\n",
      "Epoch: 466 | Batch: 004 / 011 | Total loss: 1.475 | Reg loss: 0.040 | Tree loss: 1.475 | Accuracy: 0.466000 | 1.756 sec/iter\n",
      "Epoch: 466 | Batch: 005 / 011 | Total loss: 1.420 | Reg loss: 0.040 | Tree loss: 1.420 | Accuracy: 0.493000 | 1.756 sec/iter\n",
      "Epoch: 466 | Batch: 006 / 011 | Total loss: 1.408 | Reg loss: 0.040 | Tree loss: 1.408 | Accuracy: 0.509500 | 1.756 sec/iter\n",
      "Epoch: 466 | Batch: 007 / 011 | Total loss: 1.416 | Reg loss: 0.040 | Tree loss: 1.416 | Accuracy: 0.532000 | 1.756 sec/iter\n",
      "Epoch: 466 | Batch: 008 / 011 | Total loss: 1.377 | Reg loss: 0.040 | Tree loss: 1.377 | Accuracy: 0.530000 | 1.756 sec/iter\n",
      "Epoch: 466 | Batch: 009 / 011 | Total loss: 1.400 | Reg loss: 0.040 | Tree loss: 1.400 | Accuracy: 0.507000 | 1.756 sec/iter\n",
      "Epoch: 466 | Batch: 010 / 011 | Total loss: 1.417 | Reg loss: 0.040 | Tree loss: 1.417 | Accuracy: 0.522184 | 1.756 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 467 | Batch: 000 / 011 | Total loss: 1.604 | Reg loss: 0.040 | Tree loss: 1.604 | Accuracy: 0.402000 | 1.756 sec/iter\n",
      "Epoch: 467 | Batch: 001 / 011 | Total loss: 1.574 | Reg loss: 0.040 | Tree loss: 1.574 | Accuracy: 0.405000 | 1.756 sec/iter\n",
      "Epoch: 467 | Batch: 002 / 011 | Total loss: 1.543 | Reg loss: 0.040 | Tree loss: 1.543 | Accuracy: 0.440000 | 1.756 sec/iter\n",
      "Epoch: 467 | Batch: 003 / 011 | Total loss: 1.497 | Reg loss: 0.040 | Tree loss: 1.497 | Accuracy: 0.451000 | 1.755 sec/iter\n",
      "Epoch: 467 | Batch: 004 / 011 | Total loss: 1.451 | Reg loss: 0.040 | Tree loss: 1.451 | Accuracy: 0.476500 | 1.755 sec/iter\n",
      "Epoch: 467 | Batch: 005 / 011 | Total loss: 1.449 | Reg loss: 0.040 | Tree loss: 1.449 | Accuracy: 0.488000 | 1.755 sec/iter\n",
      "Epoch: 467 | Batch: 006 / 011 | Total loss: 1.414 | Reg loss: 0.040 | Tree loss: 1.414 | Accuracy: 0.510000 | 1.755 sec/iter\n",
      "Epoch: 467 | Batch: 007 / 011 | Total loss: 1.402 | Reg loss: 0.040 | Tree loss: 1.402 | Accuracy: 0.516000 | 1.755 sec/iter\n",
      "Epoch: 467 | Batch: 008 / 011 | Total loss: 1.407 | Reg loss: 0.040 | Tree loss: 1.407 | Accuracy: 0.492000 | 1.755 sec/iter\n",
      "Epoch: 467 | Batch: 009 / 011 | Total loss: 1.384 | Reg loss: 0.040 | Tree loss: 1.384 | Accuracy: 0.502000 | 1.755 sec/iter\n",
      "Epoch: 467 | Batch: 010 / 011 | Total loss: 1.419 | Reg loss: 0.040 | Tree loss: 1.419 | Accuracy: 0.467577 | 1.755 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 468 | Batch: 000 / 011 | Total loss: 1.600 | Reg loss: 0.040 | Tree loss: 1.600 | Accuracy: 0.404000 | 1.756 sec/iter\n",
      "Epoch: 468 | Batch: 001 / 011 | Total loss: 1.575 | Reg loss: 0.040 | Tree loss: 1.575 | Accuracy: 0.415500 | 1.756 sec/iter\n",
      "Epoch: 468 | Batch: 002 / 011 | Total loss: 1.549 | Reg loss: 0.040 | Tree loss: 1.549 | Accuracy: 0.426500 | 1.756 sec/iter\n",
      "Epoch: 468 | Batch: 003 / 011 | Total loss: 1.494 | Reg loss: 0.040 | Tree loss: 1.494 | Accuracy: 0.463500 | 1.756 sec/iter\n",
      "Epoch: 468 | Batch: 004 / 011 | Total loss: 1.458 | Reg loss: 0.040 | Tree loss: 1.458 | Accuracy: 0.471500 | 1.756 sec/iter\n",
      "Epoch: 468 | Batch: 005 / 011 | Total loss: 1.413 | Reg loss: 0.040 | Tree loss: 1.413 | Accuracy: 0.498500 | 1.756 sec/iter\n",
      "Epoch: 468 | Batch: 006 / 011 | Total loss: 1.426 | Reg loss: 0.040 | Tree loss: 1.426 | Accuracy: 0.506500 | 1.756 sec/iter\n",
      "Epoch: 468 | Batch: 007 / 011 | Total loss: 1.405 | Reg loss: 0.040 | Tree loss: 1.405 | Accuracy: 0.523500 | 1.755 sec/iter\n",
      "Epoch: 468 | Batch: 008 / 011 | Total loss: 1.406 | Reg loss: 0.040 | Tree loss: 1.406 | Accuracy: 0.517500 | 1.755 sec/iter\n",
      "Epoch: 468 | Batch: 009 / 011 | Total loss: 1.411 | Reg loss: 0.040 | Tree loss: 1.411 | Accuracy: 0.506000 | 1.755 sec/iter\n",
      "Epoch: 468 | Batch: 010 / 011 | Total loss: 1.446 | Reg loss: 0.040 | Tree loss: 1.446 | Accuracy: 0.508532 | 1.755 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 469 | Batch: 000 / 011 | Total loss: 1.601 | Reg loss: 0.040 | Tree loss: 1.601 | Accuracy: 0.406500 | 1.755 sec/iter\n",
      "Epoch: 469 | Batch: 001 / 011 | Total loss: 1.577 | Reg loss: 0.040 | Tree loss: 1.577 | Accuracy: 0.404500 | 1.755 sec/iter\n",
      "Epoch: 469 | Batch: 002 / 011 | Total loss: 1.542 | Reg loss: 0.040 | Tree loss: 1.542 | Accuracy: 0.416500 | 1.755 sec/iter\n",
      "Epoch: 469 | Batch: 003 / 011 | Total loss: 1.490 | Reg loss: 0.040 | Tree loss: 1.490 | Accuracy: 0.461000 | 1.755 sec/iter\n",
      "Epoch: 469 | Batch: 004 / 011 | Total loss: 1.456 | Reg loss: 0.040 | Tree loss: 1.456 | Accuracy: 0.463000 | 1.755 sec/iter\n",
      "Epoch: 469 | Batch: 005 / 011 | Total loss: 1.423 | Reg loss: 0.040 | Tree loss: 1.423 | Accuracy: 0.504500 | 1.755 sec/iter\n",
      "Epoch: 469 | Batch: 006 / 011 | Total loss: 1.415 | Reg loss: 0.040 | Tree loss: 1.415 | Accuracy: 0.495000 | 1.755 sec/iter\n",
      "Epoch: 469 | Batch: 007 / 011 | Total loss: 1.401 | Reg loss: 0.040 | Tree loss: 1.401 | Accuracy: 0.533000 | 1.755 sec/iter\n",
      "Epoch: 469 | Batch: 008 / 011 | Total loss: 1.388 | Reg loss: 0.040 | Tree loss: 1.388 | Accuracy: 0.501000 | 1.755 sec/iter\n",
      "Epoch: 469 | Batch: 009 / 011 | Total loss: 1.422 | Reg loss: 0.040 | Tree loss: 1.422 | Accuracy: 0.492500 | 1.754 sec/iter\n",
      "Epoch: 469 | Batch: 010 / 011 | Total loss: 1.381 | Reg loss: 0.040 | Tree loss: 1.381 | Accuracy: 0.481229 | 1.754 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 470 | Batch: 000 / 011 | Total loss: 1.586 | Reg loss: 0.040 | Tree loss: 1.586 | Accuracy: 0.406000 | 1.755 sec/iter\n",
      "Epoch: 470 | Batch: 001 / 011 | Total loss: 1.574 | Reg loss: 0.040 | Tree loss: 1.574 | Accuracy: 0.391500 | 1.755 sec/iter\n",
      "Epoch: 470 | Batch: 002 / 011 | Total loss: 1.530 | Reg loss: 0.040 | Tree loss: 1.530 | Accuracy: 0.429000 | 1.755 sec/iter\n",
      "Epoch: 470 | Batch: 003 / 011 | Total loss: 1.495 | Reg loss: 0.040 | Tree loss: 1.495 | Accuracy: 0.436500 | 1.755 sec/iter\n",
      "Epoch: 470 | Batch: 004 / 011 | Total loss: 1.458 | Reg loss: 0.040 | Tree loss: 1.458 | Accuracy: 0.467500 | 1.755 sec/iter\n",
      "Epoch: 470 | Batch: 005 / 011 | Total loss: 1.434 | Reg loss: 0.040 | Tree loss: 1.434 | Accuracy: 0.509500 | 1.755 sec/iter\n",
      "Epoch: 470 | Batch: 006 / 011 | Total loss: 1.416 | Reg loss: 0.040 | Tree loss: 1.416 | Accuracy: 0.530500 | 1.755 sec/iter\n",
      "Epoch: 470 | Batch: 007 / 011 | Total loss: 1.406 | Reg loss: 0.040 | Tree loss: 1.406 | Accuracy: 0.533000 | 1.755 sec/iter\n",
      "Epoch: 470 | Batch: 008 / 011 | Total loss: 1.414 | Reg loss: 0.040 | Tree loss: 1.414 | Accuracy: 0.516000 | 1.755 sec/iter\n",
      "Epoch: 470 | Batch: 009 / 011 | Total loss: 1.398 | Reg loss: 0.040 | Tree loss: 1.398 | Accuracy: 0.506000 | 1.755 sec/iter\n",
      "Epoch: 470 | Batch: 010 / 011 | Total loss: 1.417 | Reg loss: 0.040 | Tree loss: 1.417 | Accuracy: 0.481229 | 1.755 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 471 | Batch: 000 / 011 | Total loss: 1.598 | Reg loss: 0.040 | Tree loss: 1.598 | Accuracy: 0.399500 | 1.755 sec/iter\n",
      "Epoch: 471 | Batch: 001 / 011 | Total loss: 1.556 | Reg loss: 0.040 | Tree loss: 1.556 | Accuracy: 0.420000 | 1.755 sec/iter\n",
      "Epoch: 471 | Batch: 002 / 011 | Total loss: 1.553 | Reg loss: 0.040 | Tree loss: 1.553 | Accuracy: 0.423500 | 1.754 sec/iter\n",
      "Epoch: 471 | Batch: 003 / 011 | Total loss: 1.495 | Reg loss: 0.040 | Tree loss: 1.495 | Accuracy: 0.455500 | 1.754 sec/iter\n",
      "Epoch: 471 | Batch: 004 / 011 | Total loss: 1.439 | Reg loss: 0.040 | Tree loss: 1.439 | Accuracy: 0.485500 | 1.754 sec/iter\n",
      "Epoch: 471 | Batch: 005 / 011 | Total loss: 1.419 | Reg loss: 0.040 | Tree loss: 1.419 | Accuracy: 0.504500 | 1.754 sec/iter\n",
      "Epoch: 471 | Batch: 006 / 011 | Total loss: 1.401 | Reg loss: 0.040 | Tree loss: 1.401 | Accuracy: 0.507500 | 1.754 sec/iter\n",
      "Epoch: 471 | Batch: 007 / 011 | Total loss: 1.412 | Reg loss: 0.040 | Tree loss: 1.412 | Accuracy: 0.519000 | 1.754 sec/iter\n",
      "Epoch: 471 | Batch: 008 / 011 | Total loss: 1.426 | Reg loss: 0.040 | Tree loss: 1.426 | Accuracy: 0.505500 | 1.754 sec/iter\n",
      "Epoch: 471 | Batch: 009 / 011 | Total loss: 1.397 | Reg loss: 0.040 | Tree loss: 1.397 | Accuracy: 0.503000 | 1.754 sec/iter\n",
      "Epoch: 471 | Batch: 010 / 011 | Total loss: 1.430 | Reg loss: 0.040 | Tree loss: 1.430 | Accuracy: 0.474403 | 1.754 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 472 | Batch: 000 / 011 | Total loss: 1.632 | Reg loss: 0.040 | Tree loss: 1.632 | Accuracy: 0.377500 | 1.755 sec/iter\n",
      "Epoch: 472 | Batch: 001 / 011 | Total loss: 1.564 | Reg loss: 0.040 | Tree loss: 1.564 | Accuracy: 0.416500 | 1.755 sec/iter\n",
      "Epoch: 472 | Batch: 002 / 011 | Total loss: 1.532 | Reg loss: 0.040 | Tree loss: 1.532 | Accuracy: 0.449500 | 1.755 sec/iter\n",
      "Epoch: 472 | Batch: 003 / 011 | Total loss: 1.492 | Reg loss: 0.040 | Tree loss: 1.492 | Accuracy: 0.468500 | 1.755 sec/iter\n",
      "Epoch: 472 | Batch: 004 / 011 | Total loss: 1.460 | Reg loss: 0.040 | Tree loss: 1.460 | Accuracy: 0.484000 | 1.754 sec/iter\n",
      "Epoch: 472 | Batch: 005 / 011 | Total loss: 1.425 | Reg loss: 0.040 | Tree loss: 1.425 | Accuracy: 0.508500 | 1.754 sec/iter\n",
      "Epoch: 472 | Batch: 006 / 011 | Total loss: 1.402 | Reg loss: 0.040 | Tree loss: 1.402 | Accuracy: 0.539000 | 1.754 sec/iter\n",
      "Epoch: 472 | Batch: 007 / 011 | Total loss: 1.396 | Reg loss: 0.040 | Tree loss: 1.396 | Accuracy: 0.525500 | 1.754 sec/iter\n",
      "Epoch: 472 | Batch: 008 / 011 | Total loss: 1.413 | Reg loss: 0.040 | Tree loss: 1.413 | Accuracy: 0.497000 | 1.754 sec/iter\n",
      "Epoch: 472 | Batch: 009 / 011 | Total loss: 1.419 | Reg loss: 0.040 | Tree loss: 1.419 | Accuracy: 0.475500 | 1.754 sec/iter\n",
      "Epoch: 472 | Batch: 010 / 011 | Total loss: 1.367 | Reg loss: 0.040 | Tree loss: 1.367 | Accuracy: 0.508532 | 1.754 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 473 | Batch: 000 / 011 | Total loss: 1.606 | Reg loss: 0.040 | Tree loss: 1.606 | Accuracy: 0.399500 | 1.754 sec/iter\n",
      "Epoch: 473 | Batch: 001 / 011 | Total loss: 1.589 | Reg loss: 0.040 | Tree loss: 1.589 | Accuracy: 0.396000 | 1.754 sec/iter\n",
      "Epoch: 473 | Batch: 002 / 011 | Total loss: 1.533 | Reg loss: 0.040 | Tree loss: 1.533 | Accuracy: 0.419500 | 1.755 sec/iter\n",
      "Epoch: 473 | Batch: 003 / 011 | Total loss: 1.474 | Reg loss: 0.040 | Tree loss: 1.474 | Accuracy: 0.460000 | 1.755 sec/iter\n",
      "Epoch: 473 | Batch: 004 / 011 | Total loss: 1.446 | Reg loss: 0.040 | Tree loss: 1.446 | Accuracy: 0.478500 | 1.755 sec/iter\n",
      "Epoch: 473 | Batch: 005 / 011 | Total loss: 1.433 | Reg loss: 0.040 | Tree loss: 1.433 | Accuracy: 0.482000 | 1.755 sec/iter\n",
      "Epoch: 473 | Batch: 006 / 011 | Total loss: 1.411 | Reg loss: 0.040 | Tree loss: 1.411 | Accuracy: 0.521500 | 1.755 sec/iter\n",
      "Epoch: 473 | Batch: 007 / 011 | Total loss: 1.396 | Reg loss: 0.040 | Tree loss: 1.396 | Accuracy: 0.534000 | 1.755 sec/iter\n",
      "Epoch: 473 | Batch: 008 / 011 | Total loss: 1.419 | Reg loss: 0.040 | Tree loss: 1.419 | Accuracy: 0.514500 | 1.755 sec/iter\n",
      "Epoch: 473 | Batch: 009 / 011 | Total loss: 1.412 | Reg loss: 0.040 | Tree loss: 1.412 | Accuracy: 0.492500 | 1.755 sec/iter\n",
      "Epoch: 473 | Batch: 010 / 011 | Total loss: 1.390 | Reg loss: 0.040 | Tree loss: 1.390 | Accuracy: 0.522184 | 1.755 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 474 | Batch: 000 / 011 | Total loss: 1.616 | Reg loss: 0.040 | Tree loss: 1.616 | Accuracy: 0.377000 | 1.755 sec/iter\n",
      "Epoch: 474 | Batch: 001 / 011 | Total loss: 1.567 | Reg loss: 0.040 | Tree loss: 1.567 | Accuracy: 0.409000 | 1.755 sec/iter\n",
      "Epoch: 474 | Batch: 002 / 011 | Total loss: 1.560 | Reg loss: 0.040 | Tree loss: 1.560 | Accuracy: 0.418500 | 1.755 sec/iter\n",
      "Epoch: 474 | Batch: 003 / 011 | Total loss: 1.483 | Reg loss: 0.040 | Tree loss: 1.483 | Accuracy: 0.468000 | 1.755 sec/iter\n",
      "Epoch: 474 | Batch: 004 / 011 | Total loss: 1.452 | Reg loss: 0.040 | Tree loss: 1.452 | Accuracy: 0.472000 | 1.755 sec/iter\n",
      "Epoch: 474 | Batch: 005 / 011 | Total loss: 1.439 | Reg loss: 0.040 | Tree loss: 1.439 | Accuracy: 0.486500 | 1.755 sec/iter\n",
      "Epoch: 474 | Batch: 006 / 011 | Total loss: 1.420 | Reg loss: 0.040 | Tree loss: 1.420 | Accuracy: 0.520500 | 1.755 sec/iter\n",
      "Epoch: 474 | Batch: 007 / 011 | Total loss: 1.398 | Reg loss: 0.040 | Tree loss: 1.398 | Accuracy: 0.517500 | 1.755 sec/iter\n",
      "Epoch: 474 | Batch: 008 / 011 | Total loss: 1.385 | Reg loss: 0.040 | Tree loss: 1.385 | Accuracy: 0.529500 | 1.755 sec/iter\n",
      "Epoch: 474 | Batch: 009 / 011 | Total loss: 1.411 | Reg loss: 0.040 | Tree loss: 1.411 | Accuracy: 0.490500 | 1.755 sec/iter\n",
      "Epoch: 474 | Batch: 010 / 011 | Total loss: 1.433 | Reg loss: 0.040 | Tree loss: 1.433 | Accuracy: 0.498294 | 1.755 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 475 | Batch: 000 / 011 | Total loss: 1.602 | Reg loss: 0.040 | Tree loss: 1.602 | Accuracy: 0.410000 | 1.756 sec/iter\n",
      "Epoch: 475 | Batch: 001 / 011 | Total loss: 1.554 | Reg loss: 0.040 | Tree loss: 1.554 | Accuracy: 0.422500 | 1.756 sec/iter\n",
      "Epoch: 475 | Batch: 002 / 011 | Total loss: 1.521 | Reg loss: 0.040 | Tree loss: 1.521 | Accuracy: 0.433000 | 1.756 sec/iter\n",
      "Epoch: 475 | Batch: 003 / 011 | Total loss: 1.487 | Reg loss: 0.040 | Tree loss: 1.487 | Accuracy: 0.461000 | 1.756 sec/iter\n",
      "Epoch: 475 | Batch: 004 / 011 | Total loss: 1.456 | Reg loss: 0.040 | Tree loss: 1.456 | Accuracy: 0.491500 | 1.756 sec/iter\n",
      "Epoch: 475 | Batch: 005 / 011 | Total loss: 1.439 | Reg loss: 0.040 | Tree loss: 1.439 | Accuracy: 0.492000 | 1.756 sec/iter\n",
      "Epoch: 475 | Batch: 006 / 011 | Total loss: 1.434 | Reg loss: 0.040 | Tree loss: 1.434 | Accuracy: 0.502500 | 1.756 sec/iter\n",
      "Epoch: 475 | Batch: 007 / 011 | Total loss: 1.403 | Reg loss: 0.040 | Tree loss: 1.403 | Accuracy: 0.521500 | 1.756 sec/iter\n",
      "Epoch: 475 | Batch: 008 / 011 | Total loss: 1.395 | Reg loss: 0.040 | Tree loss: 1.395 | Accuracy: 0.510000 | 1.756 sec/iter\n",
      "Epoch: 475 | Batch: 009 / 011 | Total loss: 1.425 | Reg loss: 0.040 | Tree loss: 1.425 | Accuracy: 0.518000 | 1.756 sec/iter\n",
      "Epoch: 475 | Batch: 010 / 011 | Total loss: 1.395 | Reg loss: 0.040 | Tree loss: 1.395 | Accuracy: 0.481229 | 1.756 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 476 | Batch: 000 / 011 | Total loss: 1.600 | Reg loss: 0.040 | Tree loss: 1.600 | Accuracy: 0.398000 | 1.757 sec/iter\n",
      "Epoch: 476 | Batch: 001 / 011 | Total loss: 1.555 | Reg loss: 0.040 | Tree loss: 1.555 | Accuracy: 0.434000 | 1.757 sec/iter\n",
      "Epoch: 476 | Batch: 002 / 011 | Total loss: 1.538 | Reg loss: 0.040 | Tree loss: 1.538 | Accuracy: 0.433500 | 1.757 sec/iter\n",
      "Epoch: 476 | Batch: 003 / 011 | Total loss: 1.508 | Reg loss: 0.040 | Tree loss: 1.508 | Accuracy: 0.441500 | 1.757 sec/iter\n",
      "Epoch: 476 | Batch: 004 / 011 | Total loss: 1.473 | Reg loss: 0.040 | Tree loss: 1.473 | Accuracy: 0.460000 | 1.757 sec/iter\n",
      "Epoch: 476 | Batch: 005 / 011 | Total loss: 1.412 | Reg loss: 0.040 | Tree loss: 1.412 | Accuracy: 0.531500 | 1.757 sec/iter\n",
      "Epoch: 476 | Batch: 006 / 011 | Total loss: 1.415 | Reg loss: 0.040 | Tree loss: 1.415 | Accuracy: 0.506000 | 1.757 sec/iter\n",
      "Epoch: 476 | Batch: 007 / 011 | Total loss: 1.393 | Reg loss: 0.040 | Tree loss: 1.393 | Accuracy: 0.528000 | 1.757 sec/iter\n",
      "Epoch: 476 | Batch: 008 / 011 | Total loss: 1.417 | Reg loss: 0.040 | Tree loss: 1.417 | Accuracy: 0.498500 | 1.757 sec/iter\n",
      "Epoch: 476 | Batch: 009 / 011 | Total loss: 1.402 | Reg loss: 0.040 | Tree loss: 1.402 | Accuracy: 0.490000 | 1.757 sec/iter\n",
      "Epoch: 476 | Batch: 010 / 011 | Total loss: 1.411 | Reg loss: 0.040 | Tree loss: 1.411 | Accuracy: 0.477816 | 1.757 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 477 | Batch: 000 / 011 | Total loss: 1.599 | Reg loss: 0.040 | Tree loss: 1.599 | Accuracy: 0.398000 | 1.758 sec/iter\n",
      "Epoch: 477 | Batch: 001 / 011 | Total loss: 1.582 | Reg loss: 0.040 | Tree loss: 1.582 | Accuracy: 0.411500 | 1.758 sec/iter\n",
      "Epoch: 477 | Batch: 002 / 011 | Total loss: 1.518 | Reg loss: 0.040 | Tree loss: 1.518 | Accuracy: 0.449500 | 1.758 sec/iter\n",
      "Epoch: 477 | Batch: 003 / 011 | Total loss: 1.465 | Reg loss: 0.040 | Tree loss: 1.465 | Accuracy: 0.467500 | 1.758 sec/iter\n",
      "Epoch: 477 | Batch: 004 / 011 | Total loss: 1.481 | Reg loss: 0.040 | Tree loss: 1.481 | Accuracy: 0.470500 | 1.758 sec/iter\n",
      "Epoch: 477 | Batch: 005 / 011 | Total loss: 1.417 | Reg loss: 0.040 | Tree loss: 1.417 | Accuracy: 0.501500 | 1.758 sec/iter\n",
      "Epoch: 477 | Batch: 006 / 011 | Total loss: 1.403 | Reg loss: 0.040 | Tree loss: 1.403 | Accuracy: 0.537000 | 1.758 sec/iter\n",
      "Epoch: 477 | Batch: 007 / 011 | Total loss: 1.411 | Reg loss: 0.040 | Tree loss: 1.411 | Accuracy: 0.506500 | 1.758 sec/iter\n",
      "Epoch: 477 | Batch: 008 / 011 | Total loss: 1.419 | Reg loss: 0.040 | Tree loss: 1.419 | Accuracy: 0.498000 | 1.758 sec/iter\n",
      "Epoch: 477 | Batch: 009 / 011 | Total loss: 1.439 | Reg loss: 0.040 | Tree loss: 1.439 | Accuracy: 0.486500 | 1.758 sec/iter\n",
      "Epoch: 477 | Batch: 010 / 011 | Total loss: 1.375 | Reg loss: 0.040 | Tree loss: 1.375 | Accuracy: 0.518771 | 1.757 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 478 | Batch: 000 / 011 | Total loss: 1.606 | Reg loss: 0.040 | Tree loss: 1.606 | Accuracy: 0.404000 | 1.757 sec/iter\n",
      "Epoch: 478 | Batch: 001 / 011 | Total loss: 1.576 | Reg loss: 0.040 | Tree loss: 1.576 | Accuracy: 0.414500 | 1.757 sec/iter\n",
      "Epoch: 478 | Batch: 002 / 011 | Total loss: 1.497 | Reg loss: 0.040 | Tree loss: 1.497 | Accuracy: 0.460000 | 1.757 sec/iter\n",
      "Epoch: 478 | Batch: 003 / 011 | Total loss: 1.487 | Reg loss: 0.040 | Tree loss: 1.487 | Accuracy: 0.460000 | 1.757 sec/iter\n",
      "Epoch: 478 | Batch: 004 / 011 | Total loss: 1.464 | Reg loss: 0.040 | Tree loss: 1.464 | Accuracy: 0.459000 | 1.757 sec/iter\n",
      "Epoch: 478 | Batch: 005 / 011 | Total loss: 1.443 | Reg loss: 0.040 | Tree loss: 1.443 | Accuracy: 0.489500 | 1.757 sec/iter\n",
      "Epoch: 478 | Batch: 006 / 011 | Total loss: 1.433 | Reg loss: 0.040 | Tree loss: 1.433 | Accuracy: 0.501500 | 1.757 sec/iter\n",
      "Epoch: 478 | Batch: 007 / 011 | Total loss: 1.416 | Reg loss: 0.040 | Tree loss: 1.416 | Accuracy: 0.499500 | 1.757 sec/iter\n",
      "Epoch: 478 | Batch: 008 / 011 | Total loss: 1.393 | Reg loss: 0.040 | Tree loss: 1.393 | Accuracy: 0.521000 | 1.757 sec/iter\n",
      "Epoch: 478 | Batch: 009 / 011 | Total loss: 1.413 | Reg loss: 0.040 | Tree loss: 1.413 | Accuracy: 0.514000 | 1.757 sec/iter\n",
      "Epoch: 478 | Batch: 010 / 011 | Total loss: 1.373 | Reg loss: 0.040 | Tree loss: 1.373 | Accuracy: 0.529010 | 1.757 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 479 | Batch: 000 / 011 | Total loss: 1.607 | Reg loss: 0.040 | Tree loss: 1.607 | Accuracy: 0.399500 | 1.758 sec/iter\n",
      "Epoch: 479 | Batch: 001 / 011 | Total loss: 1.572 | Reg loss: 0.040 | Tree loss: 1.572 | Accuracy: 0.412500 | 1.758 sec/iter\n",
      "Epoch: 479 | Batch: 002 / 011 | Total loss: 1.516 | Reg loss: 0.040 | Tree loss: 1.516 | Accuracy: 0.460500 | 1.758 sec/iter\n",
      "Epoch: 479 | Batch: 003 / 011 | Total loss: 1.476 | Reg loss: 0.040 | Tree loss: 1.476 | Accuracy: 0.474500 | 1.758 sec/iter\n",
      "Epoch: 479 | Batch: 004 / 011 | Total loss: 1.464 | Reg loss: 0.040 | Tree loss: 1.464 | Accuracy: 0.472500 | 1.758 sec/iter\n",
      "Epoch: 479 | Batch: 005 / 011 | Total loss: 1.446 | Reg loss: 0.040 | Tree loss: 1.446 | Accuracy: 0.493000 | 1.758 sec/iter\n",
      "Epoch: 479 | Batch: 006 / 011 | Total loss: 1.418 | Reg loss: 0.040 | Tree loss: 1.418 | Accuracy: 0.512000 | 1.759 sec/iter\n",
      "Epoch: 479 | Batch: 007 / 011 | Total loss: 1.400 | Reg loss: 0.040 | Tree loss: 1.400 | Accuracy: 0.524500 | 1.758 sec/iter\n",
      "Epoch: 479 | Batch: 008 / 011 | Total loss: 1.385 | Reg loss: 0.040 | Tree loss: 1.385 | Accuracy: 0.514000 | 1.758 sec/iter\n",
      "Epoch: 479 | Batch: 009 / 011 | Total loss: 1.417 | Reg loss: 0.040 | Tree loss: 1.417 | Accuracy: 0.512000 | 1.758 sec/iter\n",
      "Epoch: 479 | Batch: 010 / 011 | Total loss: 1.444 | Reg loss: 0.040 | Tree loss: 1.444 | Accuracy: 0.494881 | 1.758 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 480 | Batch: 000 / 011 | Total loss: 1.602 | Reg loss: 0.040 | Tree loss: 1.602 | Accuracy: 0.410000 | 1.759 sec/iter\n",
      "Epoch: 480 | Batch: 001 / 011 | Total loss: 1.574 | Reg loss: 0.040 | Tree loss: 1.574 | Accuracy: 0.416500 | 1.759 sec/iter\n",
      "Epoch: 480 | Batch: 002 / 011 | Total loss: 1.523 | Reg loss: 0.040 | Tree loss: 1.523 | Accuracy: 0.440000 | 1.759 sec/iter\n",
      "Epoch: 480 | Batch: 003 / 011 | Total loss: 1.467 | Reg loss: 0.040 | Tree loss: 1.467 | Accuracy: 0.465000 | 1.759 sec/iter\n",
      "Epoch: 480 | Batch: 004 / 011 | Total loss: 1.469 | Reg loss: 0.040 | Tree loss: 1.469 | Accuracy: 0.470000 | 1.759 sec/iter\n",
      "Epoch: 480 | Batch: 005 / 011 | Total loss: 1.416 | Reg loss: 0.040 | Tree loss: 1.416 | Accuracy: 0.492000 | 1.759 sec/iter\n",
      "Epoch: 480 | Batch: 006 / 011 | Total loss: 1.458 | Reg loss: 0.040 | Tree loss: 1.458 | Accuracy: 0.506000 | 1.759 sec/iter\n",
      "Epoch: 480 | Batch: 007 / 011 | Total loss: 1.394 | Reg loss: 0.040 | Tree loss: 1.394 | Accuracy: 0.506000 | 1.759 sec/iter\n",
      "Epoch: 480 | Batch: 008 / 011 | Total loss: 1.423 | Reg loss: 0.040 | Tree loss: 1.423 | Accuracy: 0.504000 | 1.759 sec/iter\n",
      "Epoch: 480 | Batch: 009 / 011 | Total loss: 1.406 | Reg loss: 0.040 | Tree loss: 1.406 | Accuracy: 0.503500 | 1.759 sec/iter\n",
      "Epoch: 480 | Batch: 010 / 011 | Total loss: 1.383 | Reg loss: 0.040 | Tree loss: 1.383 | Accuracy: 0.505119 | 1.759 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 481 | Batch: 000 / 011 | Total loss: 1.623 | Reg loss: 0.040 | Tree loss: 1.623 | Accuracy: 0.394000 | 1.76 sec/iter\n",
      "Epoch: 481 | Batch: 001 / 011 | Total loss: 1.570 | Reg loss: 0.040 | Tree loss: 1.570 | Accuracy: 0.413500 | 1.76 sec/iter\n",
      "Epoch: 481 | Batch: 002 / 011 | Total loss: 1.546 | Reg loss: 0.040 | Tree loss: 1.546 | Accuracy: 0.433000 | 1.76 sec/iter\n",
      "Epoch: 481 | Batch: 003 / 011 | Total loss: 1.493 | Reg loss: 0.040 | Tree loss: 1.493 | Accuracy: 0.466000 | 1.76 sec/iter\n",
      "Epoch: 481 | Batch: 004 / 011 | Total loss: 1.453 | Reg loss: 0.040 | Tree loss: 1.453 | Accuracy: 0.478500 | 1.76 sec/iter\n",
      "Epoch: 481 | Batch: 005 / 011 | Total loss: 1.437 | Reg loss: 0.040 | Tree loss: 1.437 | Accuracy: 0.501500 | 1.76 sec/iter\n",
      "Epoch: 481 | Batch: 006 / 011 | Total loss: 1.395 | Reg loss: 0.040 | Tree loss: 1.395 | Accuracy: 0.535500 | 1.76 sec/iter\n",
      "Epoch: 481 | Batch: 007 / 011 | Total loss: 1.398 | Reg loss: 0.040 | Tree loss: 1.398 | Accuracy: 0.517500 | 1.76 sec/iter\n",
      "Epoch: 481 | Batch: 008 / 011 | Total loss: 1.426 | Reg loss: 0.040 | Tree loss: 1.426 | Accuracy: 0.499500 | 1.76 sec/iter\n",
      "Epoch: 481 | Batch: 009 / 011 | Total loss: 1.392 | Reg loss: 0.040 | Tree loss: 1.392 | Accuracy: 0.508000 | 1.76 sec/iter\n",
      "Epoch: 481 | Batch: 010 / 011 | Total loss: 1.372 | Reg loss: 0.040 | Tree loss: 1.372 | Accuracy: 0.474403 | 1.76 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 482 | Batch: 000 / 011 | Total loss: 1.596 | Reg loss: 0.040 | Tree loss: 1.596 | Accuracy: 0.419500 | 1.761 sec/iter\n",
      "Epoch: 482 | Batch: 001 / 011 | Total loss: 1.559 | Reg loss: 0.040 | Tree loss: 1.559 | Accuracy: 0.423500 | 1.761 sec/iter\n",
      "Epoch: 482 | Batch: 002 / 011 | Total loss: 1.536 | Reg loss: 0.040 | Tree loss: 1.536 | Accuracy: 0.431000 | 1.761 sec/iter\n",
      "Epoch: 482 | Batch: 003 / 011 | Total loss: 1.505 | Reg loss: 0.040 | Tree loss: 1.505 | Accuracy: 0.443000 | 1.761 sec/iter\n",
      "Epoch: 482 | Batch: 004 / 011 | Total loss: 1.483 | Reg loss: 0.040 | Tree loss: 1.483 | Accuracy: 0.467500 | 1.761 sec/iter\n",
      "Epoch: 482 | Batch: 005 / 011 | Total loss: 1.432 | Reg loss: 0.040 | Tree loss: 1.432 | Accuracy: 0.494500 | 1.761 sec/iter\n",
      "Epoch: 482 | Batch: 006 / 011 | Total loss: 1.412 | Reg loss: 0.040 | Tree loss: 1.412 | Accuracy: 0.529000 | 1.761 sec/iter\n",
      "Epoch: 482 | Batch: 007 / 011 | Total loss: 1.402 | Reg loss: 0.040 | Tree loss: 1.402 | Accuracy: 0.529000 | 1.761 sec/iter\n",
      "Epoch: 482 | Batch: 008 / 011 | Total loss: 1.408 | Reg loss: 0.040 | Tree loss: 1.408 | Accuracy: 0.510000 | 1.761 sec/iter\n",
      "Epoch: 482 | Batch: 009 / 011 | Total loss: 1.395 | Reg loss: 0.040 | Tree loss: 1.395 | Accuracy: 0.532000 | 1.761 sec/iter\n",
      "Epoch: 482 | Batch: 010 / 011 | Total loss: 1.414 | Reg loss: 0.040 | Tree loss: 1.414 | Accuracy: 0.470990 | 1.761 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 483 | Batch: 000 / 011 | Total loss: 1.608 | Reg loss: 0.040 | Tree loss: 1.608 | Accuracy: 0.392500 | 1.761 sec/iter\n",
      "Epoch: 483 | Batch: 001 / 011 | Total loss: 1.549 | Reg loss: 0.040 | Tree loss: 1.549 | Accuracy: 0.429000 | 1.761 sec/iter\n",
      "Epoch: 483 | Batch: 002 / 011 | Total loss: 1.534 | Reg loss: 0.040 | Tree loss: 1.534 | Accuracy: 0.429500 | 1.761 sec/iter\n",
      "Epoch: 483 | Batch: 003 / 011 | Total loss: 1.520 | Reg loss: 0.040 | Tree loss: 1.520 | Accuracy: 0.432500 | 1.761 sec/iter\n",
      "Epoch: 483 | Batch: 004 / 011 | Total loss: 1.476 | Reg loss: 0.040 | Tree loss: 1.476 | Accuracy: 0.478500 | 1.761 sec/iter\n",
      "Epoch: 483 | Batch: 005 / 011 | Total loss: 1.413 | Reg loss: 0.040 | Tree loss: 1.413 | Accuracy: 0.507500 | 1.761 sec/iter\n",
      "Epoch: 483 | Batch: 006 / 011 | Total loss: 1.423 | Reg loss: 0.040 | Tree loss: 1.423 | Accuracy: 0.515000 | 1.761 sec/iter\n",
      "Epoch: 483 | Batch: 007 / 011 | Total loss: 1.391 | Reg loss: 0.040 | Tree loss: 1.391 | Accuracy: 0.536000 | 1.761 sec/iter\n",
      "Epoch: 483 | Batch: 008 / 011 | Total loss: 1.405 | Reg loss: 0.040 | Tree loss: 1.405 | Accuracy: 0.488500 | 1.762 sec/iter\n",
      "Epoch: 483 | Batch: 009 / 011 | Total loss: 1.396 | Reg loss: 0.040 | Tree loss: 1.396 | Accuracy: 0.503500 | 1.761 sec/iter\n",
      "Epoch: 483 | Batch: 010 / 011 | Total loss: 1.396 | Reg loss: 0.040 | Tree loss: 1.396 | Accuracy: 0.477816 | 1.761 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 484 | Batch: 000 / 011 | Total loss: 1.607 | Reg loss: 0.040 | Tree loss: 1.607 | Accuracy: 0.402000 | 1.762 sec/iter\n",
      "Epoch: 484 | Batch: 001 / 011 | Total loss: 1.593 | Reg loss: 0.040 | Tree loss: 1.593 | Accuracy: 0.409000 | 1.762 sec/iter\n",
      "Epoch: 484 | Batch: 002 / 011 | Total loss: 1.518 | Reg loss: 0.040 | Tree loss: 1.518 | Accuracy: 0.447500 | 1.761 sec/iter\n",
      "Epoch: 484 | Batch: 003 / 011 | Total loss: 1.493 | Reg loss: 0.040 | Tree loss: 1.493 | Accuracy: 0.469000 | 1.762 sec/iter\n",
      "Epoch: 484 | Batch: 004 / 011 | Total loss: 1.487 | Reg loss: 0.040 | Tree loss: 1.487 | Accuracy: 0.478500 | 1.762 sec/iter\n",
      "Epoch: 484 | Batch: 005 / 011 | Total loss: 1.422 | Reg loss: 0.040 | Tree loss: 1.422 | Accuracy: 0.507000 | 1.762 sec/iter\n",
      "Epoch: 484 | Batch: 006 / 011 | Total loss: 1.406 | Reg loss: 0.040 | Tree loss: 1.406 | Accuracy: 0.541000 | 1.762 sec/iter\n",
      "Epoch: 484 | Batch: 007 / 011 | Total loss: 1.377 | Reg loss: 0.040 | Tree loss: 1.377 | Accuracy: 0.525000 | 1.762 sec/iter\n",
      "Epoch: 484 | Batch: 008 / 011 | Total loss: 1.400 | Reg loss: 0.040 | Tree loss: 1.400 | Accuracy: 0.507500 | 1.762 sec/iter\n",
      "Epoch: 484 | Batch: 009 / 011 | Total loss: 1.410 | Reg loss: 0.040 | Tree loss: 1.410 | Accuracy: 0.505500 | 1.762 sec/iter\n",
      "Epoch: 484 | Batch: 010 / 011 | Total loss: 1.463 | Reg loss: 0.040 | Tree loss: 1.463 | Accuracy: 0.464164 | 1.762 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 485 | Batch: 000 / 011 | Total loss: 1.608 | Reg loss: 0.040 | Tree loss: 1.608 | Accuracy: 0.386500 | 1.762 sec/iter\n",
      "Epoch: 485 | Batch: 001 / 011 | Total loss: 1.588 | Reg loss: 0.040 | Tree loss: 1.588 | Accuracy: 0.415500 | 1.762 sec/iter\n",
      "Epoch: 485 | Batch: 002 / 011 | Total loss: 1.517 | Reg loss: 0.040 | Tree loss: 1.517 | Accuracy: 0.452500 | 1.762 sec/iter\n",
      "Epoch: 485 | Batch: 003 / 011 | Total loss: 1.489 | Reg loss: 0.040 | Tree loss: 1.489 | Accuracy: 0.467000 | 1.762 sec/iter\n",
      "Epoch: 485 | Batch: 004 / 011 | Total loss: 1.454 | Reg loss: 0.040 | Tree loss: 1.454 | Accuracy: 0.473000 | 1.762 sec/iter\n",
      "Epoch: 485 | Batch: 005 / 011 | Total loss: 1.434 | Reg loss: 0.040 | Tree loss: 1.434 | Accuracy: 0.498000 | 1.763 sec/iter\n",
      "Epoch: 485 | Batch: 006 / 011 | Total loss: 1.398 | Reg loss: 0.040 | Tree loss: 1.398 | Accuracy: 0.542500 | 1.763 sec/iter\n",
      "Epoch: 485 | Batch: 007 / 011 | Total loss: 1.390 | Reg loss: 0.040 | Tree loss: 1.390 | Accuracy: 0.528500 | 1.762 sec/iter\n",
      "Epoch: 485 | Batch: 008 / 011 | Total loss: 1.416 | Reg loss: 0.040 | Tree loss: 1.416 | Accuracy: 0.516500 | 1.763 sec/iter\n",
      "Epoch: 485 | Batch: 009 / 011 | Total loss: 1.436 | Reg loss: 0.040 | Tree loss: 1.436 | Accuracy: 0.490500 | 1.763 sec/iter\n",
      "Epoch: 485 | Batch: 010 / 011 | Total loss: 1.403 | Reg loss: 0.040 | Tree loss: 1.403 | Accuracy: 0.508532 | 1.763 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 486 | Batch: 000 / 011 | Total loss: 1.615 | Reg loss: 0.040 | Tree loss: 1.615 | Accuracy: 0.381500 | 1.763 sec/iter\n",
      "Epoch: 486 | Batch: 001 / 011 | Total loss: 1.564 | Reg loss: 0.040 | Tree loss: 1.564 | Accuracy: 0.433000 | 1.763 sec/iter\n",
      "Epoch: 486 | Batch: 002 / 011 | Total loss: 1.532 | Reg loss: 0.040 | Tree loss: 1.532 | Accuracy: 0.444000 | 1.763 sec/iter\n",
      "Epoch: 486 | Batch: 003 / 011 | Total loss: 1.488 | Reg loss: 0.040 | Tree loss: 1.488 | Accuracy: 0.473500 | 1.763 sec/iter\n",
      "Epoch: 486 | Batch: 004 / 011 | Total loss: 1.451 | Reg loss: 0.040 | Tree loss: 1.451 | Accuracy: 0.490500 | 1.763 sec/iter\n",
      "Epoch: 486 | Batch: 005 / 011 | Total loss: 1.440 | Reg loss: 0.040 | Tree loss: 1.440 | Accuracy: 0.505000 | 1.763 sec/iter\n",
      "Epoch: 486 | Batch: 006 / 011 | Total loss: 1.414 | Reg loss: 0.040 | Tree loss: 1.414 | Accuracy: 0.511000 | 1.763 sec/iter\n",
      "Epoch: 486 | Batch: 007 / 011 | Total loss: 1.396 | Reg loss: 0.040 | Tree loss: 1.396 | Accuracy: 0.531000 | 1.763 sec/iter\n",
      "Epoch: 486 | Batch: 008 / 011 | Total loss: 1.414 | Reg loss: 0.040 | Tree loss: 1.414 | Accuracy: 0.501000 | 1.763 sec/iter\n",
      "Epoch: 486 | Batch: 009 / 011 | Total loss: 1.403 | Reg loss: 0.040 | Tree loss: 1.403 | Accuracy: 0.507000 | 1.763 sec/iter\n",
      "Epoch: 486 | Batch: 010 / 011 | Total loss: 1.414 | Reg loss: 0.040 | Tree loss: 1.414 | Accuracy: 0.505119 | 1.763 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 487 | Batch: 000 / 011 | Total loss: 1.609 | Reg loss: 0.040 | Tree loss: 1.609 | Accuracy: 0.397000 | 1.764 sec/iter\n",
      "Epoch: 487 | Batch: 001 / 011 | Total loss: 1.554 | Reg loss: 0.040 | Tree loss: 1.554 | Accuracy: 0.418000 | 1.764 sec/iter\n",
      "Epoch: 487 | Batch: 002 / 011 | Total loss: 1.518 | Reg loss: 0.040 | Tree loss: 1.518 | Accuracy: 0.450500 | 1.764 sec/iter\n",
      "Epoch: 487 | Batch: 003 / 011 | Total loss: 1.493 | Reg loss: 0.040 | Tree loss: 1.493 | Accuracy: 0.439000 | 1.764 sec/iter\n",
      "Epoch: 487 | Batch: 004 / 011 | Total loss: 1.478 | Reg loss: 0.040 | Tree loss: 1.478 | Accuracy: 0.453500 | 1.764 sec/iter\n",
      "Epoch: 487 | Batch: 005 / 011 | Total loss: 1.406 | Reg loss: 0.040 | Tree loss: 1.406 | Accuracy: 0.522500 | 1.764 sec/iter\n",
      "Epoch: 487 | Batch: 006 / 011 | Total loss: 1.426 | Reg loss: 0.040 | Tree loss: 1.426 | Accuracy: 0.527500 | 1.764 sec/iter\n",
      "Epoch: 487 | Batch: 007 / 011 | Total loss: 1.405 | Reg loss: 0.040 | Tree loss: 1.405 | Accuracy: 0.516500 | 1.764 sec/iter\n",
      "Epoch: 487 | Batch: 008 / 011 | Total loss: 1.415 | Reg loss: 0.040 | Tree loss: 1.415 | Accuracy: 0.490000 | 1.764 sec/iter\n",
      "Epoch: 487 | Batch: 009 / 011 | Total loss: 1.412 | Reg loss: 0.040 | Tree loss: 1.412 | Accuracy: 0.476000 | 1.764 sec/iter\n",
      "Epoch: 487 | Batch: 010 / 011 | Total loss: 1.443 | Reg loss: 0.040 | Tree loss: 1.443 | Accuracy: 0.488055 | 1.764 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 488 | Batch: 000 / 011 | Total loss: 1.594 | Reg loss: 0.040 | Tree loss: 1.594 | Accuracy: 0.392000 | 1.765 sec/iter\n",
      "Epoch: 488 | Batch: 001 / 011 | Total loss: 1.584 | Reg loss: 0.040 | Tree loss: 1.584 | Accuracy: 0.382500 | 1.765 sec/iter\n",
      "Epoch: 488 | Batch: 002 / 011 | Total loss: 1.522 | Reg loss: 0.040 | Tree loss: 1.522 | Accuracy: 0.428500 | 1.765 sec/iter\n",
      "Epoch: 488 | Batch: 003 / 011 | Total loss: 1.472 | Reg loss: 0.040 | Tree loss: 1.472 | Accuracy: 0.464000 | 1.765 sec/iter\n",
      "Epoch: 488 | Batch: 004 / 011 | Total loss: 1.476 | Reg loss: 0.040 | Tree loss: 1.476 | Accuracy: 0.475500 | 1.765 sec/iter\n",
      "Epoch: 488 | Batch: 005 / 011 | Total loss: 1.446 | Reg loss: 0.040 | Tree loss: 1.446 | Accuracy: 0.478500 | 1.765 sec/iter\n",
      "Epoch: 488 | Batch: 006 / 011 | Total loss: 1.412 | Reg loss: 0.040 | Tree loss: 1.412 | Accuracy: 0.516000 | 1.765 sec/iter\n",
      "Epoch: 488 | Batch: 007 / 011 | Total loss: 1.424 | Reg loss: 0.040 | Tree loss: 1.424 | Accuracy: 0.502000 | 1.765 sec/iter\n",
      "Epoch: 488 | Batch: 008 / 011 | Total loss: 1.406 | Reg loss: 0.040 | Tree loss: 1.406 | Accuracy: 0.520500 | 1.765 sec/iter\n",
      "Epoch: 488 | Batch: 009 / 011 | Total loss: 1.384 | Reg loss: 0.040 | Tree loss: 1.384 | Accuracy: 0.525500 | 1.765 sec/iter\n",
      "Epoch: 488 | Batch: 010 / 011 | Total loss: 1.444 | Reg loss: 0.040 | Tree loss: 1.444 | Accuracy: 0.477816 | 1.765 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 489 | Batch: 000 / 011 | Total loss: 1.634 | Reg loss: 0.040 | Tree loss: 1.634 | Accuracy: 0.377000 | 1.765 sec/iter\n",
      "Epoch: 489 | Batch: 001 / 011 | Total loss: 1.566 | Reg loss: 0.040 | Tree loss: 1.566 | Accuracy: 0.403000 | 1.765 sec/iter\n",
      "Epoch: 489 | Batch: 002 / 011 | Total loss: 1.516 | Reg loss: 0.040 | Tree loss: 1.516 | Accuracy: 0.448000 | 1.764 sec/iter\n",
      "Epoch: 489 | Batch: 003 / 011 | Total loss: 1.495 | Reg loss: 0.040 | Tree loss: 1.495 | Accuracy: 0.468500 | 1.764 sec/iter\n",
      "Epoch: 489 | Batch: 004 / 011 | Total loss: 1.434 | Reg loss: 0.040 | Tree loss: 1.434 | Accuracy: 0.500000 | 1.764 sec/iter\n",
      "Epoch: 489 | Batch: 005 / 011 | Total loss: 1.431 | Reg loss: 0.040 | Tree loss: 1.431 | Accuracy: 0.491500 | 1.764 sec/iter\n",
      "Epoch: 489 | Batch: 006 / 011 | Total loss: 1.413 | Reg loss: 0.040 | Tree loss: 1.413 | Accuracy: 0.500000 | 1.764 sec/iter\n",
      "Epoch: 489 | Batch: 007 / 011 | Total loss: 1.410 | Reg loss: 0.040 | Tree loss: 1.410 | Accuracy: 0.503000 | 1.764 sec/iter\n",
      "Epoch: 489 | Batch: 008 / 011 | Total loss: 1.389 | Reg loss: 0.040 | Tree loss: 1.389 | Accuracy: 0.503500 | 1.764 sec/iter\n",
      "Epoch: 489 | Batch: 009 / 011 | Total loss: 1.428 | Reg loss: 0.040 | Tree loss: 1.428 | Accuracy: 0.490500 | 1.764 sec/iter\n",
      "Epoch: 489 | Batch: 010 / 011 | Total loss: 1.420 | Reg loss: 0.040 | Tree loss: 1.420 | Accuracy: 0.491468 | 1.764 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 490 | Batch: 000 / 011 | Total loss: 1.628 | Reg loss: 0.040 | Tree loss: 1.628 | Accuracy: 0.398500 | 1.764 sec/iter\n",
      "Epoch: 490 | Batch: 001 / 011 | Total loss: 1.577 | Reg loss: 0.040 | Tree loss: 1.577 | Accuracy: 0.413000 | 1.764 sec/iter\n",
      "Epoch: 490 | Batch: 002 / 011 | Total loss: 1.491 | Reg loss: 0.040 | Tree loss: 1.491 | Accuracy: 0.446000 | 1.764 sec/iter\n",
      "Epoch: 490 | Batch: 003 / 011 | Total loss: 1.501 | Reg loss: 0.040 | Tree loss: 1.501 | Accuracy: 0.449000 | 1.764 sec/iter\n",
      "Epoch: 490 | Batch: 004 / 011 | Total loss: 1.476 | Reg loss: 0.040 | Tree loss: 1.476 | Accuracy: 0.472500 | 1.764 sec/iter\n",
      "Epoch: 490 | Batch: 005 / 011 | Total loss: 1.427 | Reg loss: 0.040 | Tree loss: 1.427 | Accuracy: 0.494500 | 1.764 sec/iter\n",
      "Epoch: 490 | Batch: 006 / 011 | Total loss: 1.420 | Reg loss: 0.040 | Tree loss: 1.420 | Accuracy: 0.510500 | 1.764 sec/iter\n",
      "Epoch: 490 | Batch: 007 / 011 | Total loss: 1.410 | Reg loss: 0.040 | Tree loss: 1.410 | Accuracy: 0.526500 | 1.764 sec/iter\n",
      "Epoch: 490 | Batch: 008 / 011 | Total loss: 1.409 | Reg loss: 0.040 | Tree loss: 1.409 | Accuracy: 0.506500 | 1.764 sec/iter\n",
      "Epoch: 490 | Batch: 009 / 011 | Total loss: 1.404 | Reg loss: 0.040 | Tree loss: 1.404 | Accuracy: 0.507000 | 1.764 sec/iter\n",
      "Epoch: 490 | Batch: 010 / 011 | Total loss: 1.434 | Reg loss: 0.040 | Tree loss: 1.434 | Accuracy: 0.508532 | 1.764 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 491 | Batch: 000 / 011 | Total loss: 1.612 | Reg loss: 0.040 | Tree loss: 1.612 | Accuracy: 0.389500 | 1.765 sec/iter\n",
      "Epoch: 491 | Batch: 001 / 011 | Total loss: 1.558 | Reg loss: 0.040 | Tree loss: 1.558 | Accuracy: 0.414000 | 1.765 sec/iter\n",
      "Epoch: 491 | Batch: 002 / 011 | Total loss: 1.548 | Reg loss: 0.040 | Tree loss: 1.548 | Accuracy: 0.438500 | 1.765 sec/iter\n",
      "Epoch: 491 | Batch: 003 / 011 | Total loss: 1.486 | Reg loss: 0.040 | Tree loss: 1.486 | Accuracy: 0.462500 | 1.765 sec/iter\n",
      "Epoch: 491 | Batch: 004 / 011 | Total loss: 1.439 | Reg loss: 0.040 | Tree loss: 1.439 | Accuracy: 0.485500 | 1.765 sec/iter\n",
      "Epoch: 491 | Batch: 005 / 011 | Total loss: 1.435 | Reg loss: 0.040 | Tree loss: 1.435 | Accuracy: 0.502000 | 1.765 sec/iter\n",
      "Epoch: 491 | Batch: 006 / 011 | Total loss: 1.426 | Reg loss: 0.040 | Tree loss: 1.426 | Accuracy: 0.497000 | 1.765 sec/iter\n",
      "Epoch: 491 | Batch: 007 / 011 | Total loss: 1.420 | Reg loss: 0.040 | Tree loss: 1.420 | Accuracy: 0.520500 | 1.765 sec/iter\n",
      "Epoch: 491 | Batch: 008 / 011 | Total loss: 1.409 | Reg loss: 0.040 | Tree loss: 1.409 | Accuracy: 0.517000 | 1.765 sec/iter\n",
      "Epoch: 491 | Batch: 009 / 011 | Total loss: 1.397 | Reg loss: 0.040 | Tree loss: 1.397 | Accuracy: 0.505500 | 1.765 sec/iter\n",
      "Epoch: 491 | Batch: 010 / 011 | Total loss: 1.387 | Reg loss: 0.040 | Tree loss: 1.387 | Accuracy: 0.443686 | 1.765 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 492 | Batch: 000 / 011 | Total loss: 1.616 | Reg loss: 0.040 | Tree loss: 1.616 | Accuracy: 0.387500 | 1.766 sec/iter\n",
      "Epoch: 492 | Batch: 001 / 011 | Total loss: 1.565 | Reg loss: 0.040 | Tree loss: 1.565 | Accuracy: 0.426000 | 1.766 sec/iter\n",
      "Epoch: 492 | Batch: 002 / 011 | Total loss: 1.553 | Reg loss: 0.040 | Tree loss: 1.553 | Accuracy: 0.412500 | 1.766 sec/iter\n",
      "Epoch: 492 | Batch: 003 / 011 | Total loss: 1.486 | Reg loss: 0.040 | Tree loss: 1.486 | Accuracy: 0.476000 | 1.766 sec/iter\n",
      "Epoch: 492 | Batch: 004 / 011 | Total loss: 1.442 | Reg loss: 0.040 | Tree loss: 1.442 | Accuracy: 0.502500 | 1.766 sec/iter\n",
      "Epoch: 492 | Batch: 005 / 011 | Total loss: 1.456 | Reg loss: 0.040 | Tree loss: 1.456 | Accuracy: 0.489500 | 1.766 sec/iter\n",
      "Epoch: 492 | Batch: 006 / 011 | Total loss: 1.398 | Reg loss: 0.040 | Tree loss: 1.398 | Accuracy: 0.527500 | 1.766 sec/iter\n",
      "Epoch: 492 | Batch: 007 / 011 | Total loss: 1.397 | Reg loss: 0.040 | Tree loss: 1.397 | Accuracy: 0.515000 | 1.766 sec/iter\n",
      "Epoch: 492 | Batch: 008 / 011 | Total loss: 1.391 | Reg loss: 0.040 | Tree loss: 1.391 | Accuracy: 0.536500 | 1.766 sec/iter\n",
      "Epoch: 492 | Batch: 009 / 011 | Total loss: 1.423 | Reg loss: 0.040 | Tree loss: 1.423 | Accuracy: 0.493000 | 1.766 sec/iter\n",
      "Epoch: 492 | Batch: 010 / 011 | Total loss: 1.433 | Reg loss: 0.040 | Tree loss: 1.433 | Accuracy: 0.488055 | 1.766 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 493 | Batch: 000 / 011 | Total loss: 1.625 | Reg loss: 0.040 | Tree loss: 1.625 | Accuracy: 0.392500 | 1.767 sec/iter\n",
      "Epoch: 493 | Batch: 001 / 011 | Total loss: 1.562 | Reg loss: 0.040 | Tree loss: 1.562 | Accuracy: 0.429500 | 1.767 sec/iter\n",
      "Epoch: 493 | Batch: 002 / 011 | Total loss: 1.534 | Reg loss: 0.040 | Tree loss: 1.534 | Accuracy: 0.428500 | 1.767 sec/iter\n",
      "Epoch: 493 | Batch: 003 / 011 | Total loss: 1.490 | Reg loss: 0.040 | Tree loss: 1.490 | Accuracy: 0.466000 | 1.767 sec/iter\n",
      "Epoch: 493 | Batch: 004 / 011 | Total loss: 1.458 | Reg loss: 0.040 | Tree loss: 1.458 | Accuracy: 0.462000 | 1.767 sec/iter\n",
      "Epoch: 493 | Batch: 005 / 011 | Total loss: 1.419 | Reg loss: 0.040 | Tree loss: 1.419 | Accuracy: 0.497000 | 1.767 sec/iter\n",
      "Epoch: 493 | Batch: 006 / 011 | Total loss: 1.426 | Reg loss: 0.040 | Tree loss: 1.426 | Accuracy: 0.513500 | 1.767 sec/iter\n",
      "Epoch: 493 | Batch: 007 / 011 | Total loss: 1.417 | Reg loss: 0.040 | Tree loss: 1.417 | Accuracy: 0.507000 | 1.767 sec/iter\n",
      "Epoch: 493 | Batch: 008 / 011 | Total loss: 1.405 | Reg loss: 0.040 | Tree loss: 1.405 | Accuracy: 0.523000 | 1.767 sec/iter\n",
      "Epoch: 493 | Batch: 009 / 011 | Total loss: 1.400 | Reg loss: 0.040 | Tree loss: 1.400 | Accuracy: 0.487000 | 1.767 sec/iter\n",
      "Epoch: 493 | Batch: 010 / 011 | Total loss: 1.371 | Reg loss: 0.040 | Tree loss: 1.371 | Accuracy: 0.508532 | 1.767 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 494 | Batch: 000 / 011 | Total loss: 1.601 | Reg loss: 0.040 | Tree loss: 1.601 | Accuracy: 0.385500 | 1.767 sec/iter\n",
      "Epoch: 494 | Batch: 001 / 011 | Total loss: 1.582 | Reg loss: 0.040 | Tree loss: 1.582 | Accuracy: 0.417500 | 1.767 sec/iter\n",
      "Epoch: 494 | Batch: 002 / 011 | Total loss: 1.552 | Reg loss: 0.040 | Tree loss: 1.552 | Accuracy: 0.429500 | 1.767 sec/iter\n",
      "Epoch: 494 | Batch: 003 / 011 | Total loss: 1.495 | Reg loss: 0.040 | Tree loss: 1.495 | Accuracy: 0.455000 | 1.767 sec/iter\n",
      "Epoch: 494 | Batch: 004 / 011 | Total loss: 1.452 | Reg loss: 0.040 | Tree loss: 1.452 | Accuracy: 0.480500 | 1.767 sec/iter\n",
      "Epoch: 494 | Batch: 005 / 011 | Total loss: 1.437 | Reg loss: 0.040 | Tree loss: 1.437 | Accuracy: 0.503000 | 1.767 sec/iter\n",
      "Epoch: 494 | Batch: 006 / 011 | Total loss: 1.382 | Reg loss: 0.040 | Tree loss: 1.382 | Accuracy: 0.531000 | 1.768 sec/iter\n",
      "Epoch: 494 | Batch: 007 / 011 | Total loss: 1.404 | Reg loss: 0.040 | Tree loss: 1.404 | Accuracy: 0.527000 | 1.767 sec/iter\n",
      "Epoch: 494 | Batch: 008 / 011 | Total loss: 1.403 | Reg loss: 0.040 | Tree loss: 1.403 | Accuracy: 0.513000 | 1.767 sec/iter\n",
      "Epoch: 494 | Batch: 009 / 011 | Total loss: 1.412 | Reg loss: 0.040 | Tree loss: 1.412 | Accuracy: 0.497000 | 1.767 sec/iter\n",
      "Epoch: 494 | Batch: 010 / 011 | Total loss: 1.390 | Reg loss: 0.040 | Tree loss: 1.390 | Accuracy: 0.535836 | 1.767 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 495 | Batch: 000 / 011 | Total loss: 1.616 | Reg loss: 0.040 | Tree loss: 1.616 | Accuracy: 0.397500 | 1.767 sec/iter\n",
      "Epoch: 495 | Batch: 001 / 011 | Total loss: 1.543 | Reg loss: 0.040 | Tree loss: 1.543 | Accuracy: 0.435000 | 1.767 sec/iter\n",
      "Epoch: 495 | Batch: 002 / 011 | Total loss: 1.548 | Reg loss: 0.040 | Tree loss: 1.548 | Accuracy: 0.410500 | 1.767 sec/iter\n",
      "Epoch: 495 | Batch: 003 / 011 | Total loss: 1.483 | Reg loss: 0.040 | Tree loss: 1.483 | Accuracy: 0.451500 | 1.767 sec/iter\n",
      "Epoch: 495 | Batch: 004 / 011 | Total loss: 1.454 | Reg loss: 0.040 | Tree loss: 1.454 | Accuracy: 0.479000 | 1.767 sec/iter\n",
      "Epoch: 495 | Batch: 005 / 011 | Total loss: 1.435 | Reg loss: 0.040 | Tree loss: 1.435 | Accuracy: 0.497000 | 1.767 sec/iter\n",
      "Epoch: 495 | Batch: 006 / 011 | Total loss: 1.408 | Reg loss: 0.040 | Tree loss: 1.408 | Accuracy: 0.517500 | 1.768 sec/iter\n",
      "Epoch: 495 | Batch: 007 / 011 | Total loss: 1.392 | Reg loss: 0.040 | Tree loss: 1.392 | Accuracy: 0.534000 | 1.768 sec/iter\n",
      "Epoch: 495 | Batch: 008 / 011 | Total loss: 1.414 | Reg loss: 0.040 | Tree loss: 1.414 | Accuracy: 0.511000 | 1.768 sec/iter\n",
      "Epoch: 495 | Batch: 009 / 011 | Total loss: 1.423 | Reg loss: 0.040 | Tree loss: 1.423 | Accuracy: 0.498000 | 1.768 sec/iter\n",
      "Epoch: 495 | Batch: 010 / 011 | Total loss: 1.409 | Reg loss: 0.040 | Tree loss: 1.409 | Accuracy: 0.488055 | 1.768 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 496 | Batch: 000 / 011 | Total loss: 1.609 | Reg loss: 0.040 | Tree loss: 1.609 | Accuracy: 0.385000 | 1.768 sec/iter\n",
      "Epoch: 496 | Batch: 001 / 011 | Total loss: 1.588 | Reg loss: 0.040 | Tree loss: 1.588 | Accuracy: 0.416000 | 1.768 sec/iter\n",
      "Epoch: 496 | Batch: 002 / 011 | Total loss: 1.529 | Reg loss: 0.040 | Tree loss: 1.529 | Accuracy: 0.451500 | 1.768 sec/iter\n",
      "Epoch: 496 | Batch: 003 / 011 | Total loss: 1.516 | Reg loss: 0.040 | Tree loss: 1.516 | Accuracy: 0.443000 | 1.768 sec/iter\n",
      "Epoch: 496 | Batch: 004 / 011 | Total loss: 1.440 | Reg loss: 0.040 | Tree loss: 1.440 | Accuracy: 0.483000 | 1.768 sec/iter\n",
      "Epoch: 496 | Batch: 005 / 011 | Total loss: 1.423 | Reg loss: 0.040 | Tree loss: 1.423 | Accuracy: 0.497000 | 1.769 sec/iter\n",
      "Epoch: 496 | Batch: 006 / 011 | Total loss: 1.414 | Reg loss: 0.040 | Tree loss: 1.414 | Accuracy: 0.520500 | 1.769 sec/iter\n",
      "Epoch: 496 | Batch: 007 / 011 | Total loss: 1.400 | Reg loss: 0.040 | Tree loss: 1.400 | Accuracy: 0.516500 | 1.768 sec/iter\n",
      "Epoch: 496 | Batch: 008 / 011 | Total loss: 1.401 | Reg loss: 0.040 | Tree loss: 1.401 | Accuracy: 0.507000 | 1.768 sec/iter\n",
      "Epoch: 496 | Batch: 009 / 011 | Total loss: 1.409 | Reg loss: 0.040 | Tree loss: 1.409 | Accuracy: 0.517000 | 1.768 sec/iter\n",
      "Epoch: 496 | Batch: 010 / 011 | Total loss: 1.399 | Reg loss: 0.040 | Tree loss: 1.399 | Accuracy: 0.532423 | 1.768 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 497 | Batch: 000 / 011 | Total loss: 1.620 | Reg loss: 0.040 | Tree loss: 1.620 | Accuracy: 0.380500 | 1.769 sec/iter\n",
      "Epoch: 497 | Batch: 001 / 011 | Total loss: 1.564 | Reg loss: 0.040 | Tree loss: 1.564 | Accuracy: 0.420000 | 1.769 sec/iter\n",
      "Epoch: 497 | Batch: 002 / 011 | Total loss: 1.526 | Reg loss: 0.040 | Tree loss: 1.526 | Accuracy: 0.435500 | 1.769 sec/iter\n",
      "Epoch: 497 | Batch: 003 / 011 | Total loss: 1.506 | Reg loss: 0.040 | Tree loss: 1.506 | Accuracy: 0.469500 | 1.769 sec/iter\n",
      "Epoch: 497 | Batch: 004 / 011 | Total loss: 1.461 | Reg loss: 0.040 | Tree loss: 1.461 | Accuracy: 0.488000 | 1.769 sec/iter\n",
      "Epoch: 497 | Batch: 005 / 011 | Total loss: 1.437 | Reg loss: 0.040 | Tree loss: 1.437 | Accuracy: 0.476000 | 1.769 sec/iter\n",
      "Epoch: 497 | Batch: 006 / 011 | Total loss: 1.403 | Reg loss: 0.040 | Tree loss: 1.403 | Accuracy: 0.514000 | 1.769 sec/iter\n",
      "Epoch: 497 | Batch: 007 / 011 | Total loss: 1.393 | Reg loss: 0.040 | Tree loss: 1.393 | Accuracy: 0.537000 | 1.769 sec/iter\n",
      "Epoch: 497 | Batch: 008 / 011 | Total loss: 1.399 | Reg loss: 0.040 | Tree loss: 1.399 | Accuracy: 0.518500 | 1.769 sec/iter\n",
      "Epoch: 497 | Batch: 009 / 011 | Total loss: 1.422 | Reg loss: 0.040 | Tree loss: 1.422 | Accuracy: 0.483000 | 1.769 sec/iter\n",
      "Epoch: 497 | Batch: 010 / 011 | Total loss: 1.386 | Reg loss: 0.040 | Tree loss: 1.386 | Accuracy: 0.522184 | 1.769 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 498 | Batch: 000 / 011 | Total loss: 1.587 | Reg loss: 0.040 | Tree loss: 1.587 | Accuracy: 0.413500 | 1.77 sec/iter\n",
      "Epoch: 498 | Batch: 001 / 011 | Total loss: 1.560 | Reg loss: 0.040 | Tree loss: 1.560 | Accuracy: 0.413500 | 1.77 sec/iter\n",
      "Epoch: 498 | Batch: 002 / 011 | Total loss: 1.542 | Reg loss: 0.040 | Tree loss: 1.542 | Accuracy: 0.428000 | 1.77 sec/iter\n",
      "Epoch: 498 | Batch: 003 / 011 | Total loss: 1.483 | Reg loss: 0.040 | Tree loss: 1.483 | Accuracy: 0.452500 | 1.77 sec/iter\n",
      "Epoch: 498 | Batch: 004 / 011 | Total loss: 1.464 | Reg loss: 0.040 | Tree loss: 1.464 | Accuracy: 0.459500 | 1.77 sec/iter\n",
      "Epoch: 498 | Batch: 005 / 011 | Total loss: 1.435 | Reg loss: 0.040 | Tree loss: 1.435 | Accuracy: 0.498500 | 1.77 sec/iter\n",
      "Epoch: 498 | Batch: 006 / 011 | Total loss: 1.413 | Reg loss: 0.040 | Tree loss: 1.413 | Accuracy: 0.506000 | 1.77 sec/iter\n",
      "Epoch: 498 | Batch: 007 / 011 | Total loss: 1.393 | Reg loss: 0.040 | Tree loss: 1.393 | Accuracy: 0.513500 | 1.769 sec/iter\n",
      "Epoch: 498 | Batch: 008 / 011 | Total loss: 1.435 | Reg loss: 0.040 | Tree loss: 1.435 | Accuracy: 0.505500 | 1.769 sec/iter\n",
      "Epoch: 498 | Batch: 009 / 011 | Total loss: 1.405 | Reg loss: 0.040 | Tree loss: 1.405 | Accuracy: 0.498000 | 1.769 sec/iter\n",
      "Epoch: 498 | Batch: 010 / 011 | Total loss: 1.387 | Reg loss: 0.040 | Tree loss: 1.387 | Accuracy: 0.501706 | 1.769 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 499 | Batch: 000 / 011 | Total loss: 1.604 | Reg loss: 0.040 | Tree loss: 1.604 | Accuracy: 0.389000 | 1.769 sec/iter\n",
      "Epoch: 499 | Batch: 001 / 011 | Total loss: 1.561 | Reg loss: 0.040 | Tree loss: 1.561 | Accuracy: 0.411000 | 1.769 sec/iter\n",
      "Epoch: 499 | Batch: 002 / 011 | Total loss: 1.541 | Reg loss: 0.040 | Tree loss: 1.541 | Accuracy: 0.440500 | 1.769 sec/iter\n",
      "Epoch: 499 | Batch: 003 / 011 | Total loss: 1.478 | Reg loss: 0.040 | Tree loss: 1.478 | Accuracy: 0.480000 | 1.769 sec/iter\n",
      "Epoch: 499 | Batch: 004 / 011 | Total loss: 1.427 | Reg loss: 0.040 | Tree loss: 1.427 | Accuracy: 0.505500 | 1.769 sec/iter\n",
      "Epoch: 499 | Batch: 005 / 011 | Total loss: 1.438 | Reg loss: 0.040 | Tree loss: 1.438 | Accuracy: 0.493000 | 1.769 sec/iter\n",
      "Epoch: 499 | Batch: 006 / 011 | Total loss: 1.422 | Reg loss: 0.040 | Tree loss: 1.422 | Accuracy: 0.515500 | 1.769 sec/iter\n",
      "Epoch: 499 | Batch: 007 / 011 | Total loss: 1.405 | Reg loss: 0.040 | Tree loss: 1.405 | Accuracy: 0.531500 | 1.769 sec/iter\n",
      "Epoch: 499 | Batch: 008 / 011 | Total loss: 1.425 | Reg loss: 0.040 | Tree loss: 1.425 | Accuracy: 0.500500 | 1.769 sec/iter\n",
      "Epoch: 499 | Batch: 009 / 011 | Total loss: 1.416 | Reg loss: 0.040 | Tree loss: 1.416 | Accuracy: 0.482000 | 1.768 sec/iter\n",
      "Epoch: 499 | Batch: 010 / 011 | Total loss: 1.405 | Reg loss: 0.040 | Tree loss: 1.405 | Accuracy: 0.522184 | 1.768 sec/iter\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "iteration = 0\n",
    "for epoch in range(epochs):\n",
    "    # Training\n",
    "    avg_sp = show_sparseness(tree)\n",
    "    sparsity.append(avg_sp)\n",
    "    iteration = do_epoch(tree, tree_loader, device, log_interval, losses, accs, epoch, iteration)\n",
    "    \n",
    "    if epoch % 1 == 0:\n",
    "        prune_tree(tree, factor=5)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel('Iteration')\n",
    "plt.plot(accs, label='Accuracy vs iteration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel('Iteration')\n",
    "plt.plot(losses, label='Loss vs iteration')\n",
    "plt.yscale(\"log\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "weights = tree.inner_nodes.weight.cpu().detach().numpy().flatten()\n",
    "plt.hist(weights, bins=500)\n",
    "weights_std = np.std(weights)\n",
    "weights_mean = np.mean(weights)\n",
    "plt.axvline(weights_mean + weights_std, color='r')\n",
    "plt.axvline(weights_mean - weights_std, color='r')\n",
    "plt.title(f\"Mean: {weights_mean}   |   STD: {weights_std}\")\n",
    "plt.yscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average height: 8.48062015503876\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1200x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(15, 10), dpi=80)\n",
    "avg_height, root = tree.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accumulate samples in the leaves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of patterns: 129\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of patterns: {len(root.get_leaves())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = 'MLE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eitan.k/.local/lib/python3.6/site-packages/torch/nn/functional.py:1709: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    }
   ],
   "source": [
    "root.clear_leaves_samples()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (data, target) in enumerate(tree_loader):\n",
    "        root.accumulate_samples(data, method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tighten boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129\n",
      "============== Pattern 1 ==============\n",
      "============== Pattern 2 ==============\n",
      "============== Pattern 3 ==============\n",
      "============== Pattern 4 ==============\n",
      "============== Pattern 5 ==============\n",
      "============== Pattern 6 ==============\n",
      "2\n",
      "============== Pattern 7 ==============\n",
      "============== Pattern 8 ==============\n",
      "============== Pattern 9 ==============\n",
      "============== Pattern 10 ==============\n",
      "============== Pattern 11 ==============\n",
      "============== Pattern 12 ==============\n",
      "============== Pattern 13 ==============\n",
      "============== Pattern 14 ==============\n",
      "============== Pattern 15 ==============\n",
      "78\n",
      "============== Pattern 16 ==============\n",
      "============== Pattern 17 ==============\n",
      "============== Pattern 18 ==============\n",
      "============== Pattern 19 ==============\n",
      "============== Pattern 20 ==============\n",
      "4\n",
      "============== Pattern 21 ==============\n",
      "============== Pattern 22 ==============\n",
      "============== Pattern 23 ==============\n",
      "============== Pattern 24 ==============\n",
      "============== Pattern 25 ==============\n",
      "============== Pattern 26 ==============\n",
      "============== Pattern 27 ==============\n",
      "============== Pattern 28 ==============\n",
      "============== Pattern 29 ==============\n",
      "============== Pattern 30 ==============\n",
      "16304\n",
      "============== Pattern 31 ==============\n",
      "============== Pattern 32 ==============\n",
      "============== Pattern 33 ==============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eitan.k/EntangledExplainableClustering/soft_decision_tree/sdt_model.py:14: RuntimeWarning: divide by zero encountered in log\n",
      "  return np.log(1 / (1 - x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Pattern 34 ==============\n",
      "============== Pattern 35 ==============\n",
      "============== Pattern 36 ==============\n",
      "============== Pattern 37 ==============\n",
      "============== Pattern 38 ==============\n",
      "============== Pattern 39 ==============\n",
      "============== Pattern 40 ==============\n",
      "============== Pattern 41 ==============\n",
      "============== Pattern 42 ==============\n",
      "============== Pattern 43 ==============\n",
      "============== Pattern 44 ==============\n",
      "============== Pattern 45 ==============\n",
      "============== Pattern 46 ==============\n",
      "============== Pattern 47 ==============\n",
      "============== Pattern 48 ==============\n",
      "============== Pattern 49 ==============\n",
      "============== Pattern 50 ==============\n",
      "============== Pattern 51 ==============\n",
      "============== Pattern 52 ==============\n",
      "============== Pattern 53 ==============\n",
      "============== Pattern 54 ==============\n",
      "============== Pattern 55 ==============\n",
      "============== Pattern 56 ==============\n",
      "============== Pattern 57 ==============\n",
      "============== Pattern 58 ==============\n",
      "============== Pattern 59 ==============\n",
      "============== Pattern 60 ==============\n",
      "============== Pattern 61 ==============\n",
      "============== Pattern 62 ==============\n",
      "============== Pattern 63 ==============\n",
      "============== Pattern 64 ==============\n",
      "============== Pattern 65 ==============\n",
      "============== Pattern 66 ==============\n",
      "============== Pattern 67 ==============\n",
      "============== Pattern 68 ==============\n",
      "============== Pattern 69 ==============\n",
      "============== Pattern 70 ==============\n",
      "============== Pattern 71 ==============\n",
      "============== Pattern 72 ==============\n",
      "============== Pattern 73 ==============\n",
      "============== Pattern 74 ==============\n",
      "============== Pattern 75 ==============\n",
      "============== Pattern 76 ==============\n",
      "============== Pattern 77 ==============\n",
      "============== Pattern 78 ==============\n",
      "============== Pattern 79 ==============\n",
      "============== Pattern 80 ==============\n",
      "============== Pattern 81 ==============\n",
      "============== Pattern 82 ==============\n",
      "============== Pattern 83 ==============\n",
      "============== Pattern 84 ==============\n",
      "============== Pattern 85 ==============\n",
      "============== Pattern 86 ==============\n",
      "============== Pattern 87 ==============\n",
      "============== Pattern 88 ==============\n",
      "============== Pattern 89 ==============\n",
      "============== Pattern 90 ==============\n",
      "============== Pattern 91 ==============\n",
      "============== Pattern 92 ==============\n",
      "============== Pattern 93 ==============\n",
      "============== Pattern 94 ==============\n",
      "============== Pattern 95 ==============\n",
      "============== Pattern 96 ==============\n",
      "============== Pattern 97 ==============\n",
      "============== Pattern 98 ==============\n",
      "============== Pattern 99 ==============\n",
      "============== Pattern 100 ==============\n",
      "============== Pattern 101 ==============\n",
      "============== Pattern 102 ==============\n",
      "============== Pattern 103 ==============\n",
      "============== Pattern 104 ==============\n",
      "============== Pattern 105 ==============\n",
      "============== Pattern 106 ==============\n",
      "============== Pattern 107 ==============\n",
      "============== Pattern 108 ==============\n",
      "============== Pattern 109 ==============\n",
      "============== Pattern 110 ==============\n",
      "============== Pattern 111 ==============\n",
      "============== Pattern 112 ==============\n",
      "============== Pattern 113 ==============\n",
      "============== Pattern 114 ==============\n",
      "============== Pattern 115 ==============\n",
      "============== Pattern 116 ==============\n",
      "============== Pattern 117 ==============\n",
      "============== Pattern 118 ==============\n",
      "============== Pattern 119 ==============\n",
      "============== Pattern 120 ==============\n",
      "============== Pattern 121 ==============\n",
      "============== Pattern 122 ==============\n",
      "============== Pattern 123 ==============\n",
      "============== Pattern 124 ==============\n",
      "3776\n",
      "============== Pattern 125 ==============\n",
      "============== Pattern 126 ==============\n",
      "============== Pattern 127 ==============\n",
      "============== Pattern 128 ==============\n",
      "============== Pattern 129 ==============\n",
      "Average comprehensibility: 79.84496124031008\n",
      "std comprehensibility: 16.184234654597876\n",
      "var comprehensibility: 261.9294513550869\n",
      "minimum comprehensibility: 20\n",
      "maximum comprehensibility: 98\n"
     ]
    }
   ],
   "source": [
    "signal_names = dataset.dataset.all_signals\n",
    "normalizers = torch.tensor([])\n",
    "attr_names = []\n",
    "for signal_name in signal_names:\n",
    "    attr_names += [f\"T{i}.{signal_name}\" for i in range(sampled.shape[-1])]\n",
    "    sensor_norm = torch.tensor([torch.tensor(dataset.dataset.sensor_maxs[signal_name]) for _ in range(sampled.shape[-1])])\n",
    "    normalizers = torch.cat([normalizers, sensor_norm])\n",
    "    \n",
    "\n",
    "# print(attr_names)\n",
    "leaves = root.get_leaves()\n",
    "sum_comprehensibility = 0\n",
    "comprehensibilities = []\n",
    "for pattern_counter, leaf in enumerate(leaves):\n",
    "    leaf.reset_path()\n",
    "    leaf.tighten_with_accumulated_samples()\n",
    "    conds = leaf.get_path_conditions(attr_names)\n",
    "    for cond in conds:\n",
    "        cond.weights = cond.weights / normalizers\n",
    "    print(f\"============== Pattern {pattern_counter + 1} ==============\")\n",
    "    comprehensibilities.append(sum([cond.comprehensibility for cond in conds]))\n",
    "    \n",
    "print(f\"Average comprehensibility: {np.mean(comprehensibilities)}\")\n",
    "print(f\"std comprehensibility: {np.std(comprehensibilities)}\")\n",
    "print(f\"var comprehensibility: {np.var(comprehensibilities)}\")\n",
    "print(f\"minimum comprehensibility: {np.min(comprehensibilities)}\")\n",
    "print(f\"maximum comprehensibility: {np.max(comprehensibilities)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
