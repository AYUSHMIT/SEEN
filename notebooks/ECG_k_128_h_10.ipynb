{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib widget\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import torch.nn as nn\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from stream_generators.mit_bih import MITBIH\n",
    "from utils.MatplotlibUtils import reduce_dims_and_plot\n",
    "from network.auto_encoder import AutoEncoder\n",
    "from losses.knn_loss import KNNLoss, ClassificationKNNLoss\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from soft_decision_tree.sdt_model import SDT\n",
    "from sklearn.metrics import davies_bouldin_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 64\n",
    "tree_depth = 10\n",
    "batch_size = 512\n",
    "device = 'cpu'\n",
    "train_data_path = r'/mnt/qnap/ekosman/mitbih_train.csv'\n",
    "test_data_path = r'/mnt/qnap/ekosman/mitbih_test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_iter = torch.utils.data.DataLoader(MITBIH(train_data_path),\n",
    "                                             batch_size=batch_size,\n",
    "                                             shuffle=True,\n",
    "                                             num_workers=1,\n",
    "                                             pin_memory=True,\n",
    "                                             drop_last=True)\n",
    "\n",
    "test_data_iter = torch.utils.data.DataLoader(MITBIH(test_data_path),\n",
    "                                             batch_size=batch_size,\n",
    "                                             shuffle=True,\n",
    "                                             num_workers=1,\n",
    "                                             pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(32, 32, kernel_size=5, stride=1, padding=2)\n",
    "        self.conv2 = nn.Conv1d(32, 32, kernel_size=5, stride=1, padding=2)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool = nn.MaxPool1d(kernel_size=5, stride=2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = x\n",
    "        y = self.conv1(y)\n",
    "        y = self.relu1(y)\n",
    "        y = self.conv2(y)\n",
    "        y = y + x\n",
    "        y = self.relu2(y)\n",
    "        y = self.pool(y)\n",
    "        return y\n",
    "\n",
    "\n",
    "class ECGModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ECGModel, self).__init__()\n",
    "        self.conv = nn.Conv1d(1, 32, kernel_size=5, stride=1, padding=1)\n",
    "        self.block1 = ConvBlock()\n",
    "        self.block2 = ConvBlock()\n",
    "        self.block3 = ConvBlock()\n",
    "        self.block4 = ConvBlock()\n",
    "        self.block5 = ConvBlock()\n",
    "        self.fc1 = nn.Linear(64, 32)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(32, 5)\n",
    "\n",
    "    def forward(self, x, return_interm=False):\n",
    "        x = self.conv(x)\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.block4(x)\n",
    "        x = self.block5(x)\n",
    "        interm = x.flatten(1)\n",
    "        x = self.fc1(interm)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        if return_interm:\n",
    "            return x, interm\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_crt = ClassificationKNNLoss(k=k).to(device)\n",
    "\n",
    "def train(model, loader, optimizer, device):\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    for iteration, (batch, target) in enumerate(loader):\n",
    "        batch = batch.to(device)\n",
    "        target = target.to(device)\n",
    "        outputs, interm = model(batch, return_interm=True)\n",
    "        mse_loss = F.cross_entropy(outputs, target)\n",
    "        mse_loss = mse_loss.sum(dim=-1).mean()\n",
    "        try:\n",
    "            knn_loss = knn_crt(interm, target)\n",
    "            if torch.isinf(knn_loss):\n",
    "                knn_loss = torch.tensor(0).to(device)\n",
    "        except ValueError:\n",
    "            knn_loss = torch.tensor(0).to(device)\n",
    "        loss = mse_loss + knn_loss\n",
    "        total_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if iteration % log_every == 0:\n",
    "            print(f\"Epoch {epoch} / {epochs} | iteration {iteration} / {len(loader)} | Total Loss: {loss.item()} | KNN Loss: {knn_loss.item()} | CLS Loss: {mse_loss.item()}\")\n",
    "\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model, loader, device):\n",
    "    model.eval()\n",
    "    \n",
    "    correct = 0\n",
    "    for iteration, (batch, target) in enumerate(loader):\n",
    "        batch = batch.to(device)\n",
    "        target = target.to(device)\n",
    "        y_pred = model(batch).argmax(dim=-1)\n",
    "        correct += y_pred.eq(target.view(-1).data).sum()\n",
    "    \n",
    "    return correct / len(loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Params: 53957\n"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "lr = 1e-3\n",
    "log_every = 10\n",
    "\n",
    "model = ECGModel().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "num_params = sum([p.numel() for p in model.parameters()])\n",
    "print(f'#Params: {num_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eitan.k/.local/lib/python3.6/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: CUDA driver initialization failed, you might not have a CUDA gpu. (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 200 | iteration 0 / 171 | Total Loss: 7.394340515136719 | KNN Loss: 5.863130569458008 | CLS Loss: 1.53121018409729\n",
      "Epoch 1 / 200 | iteration 10 / 171 | Total Loss: 5.612790107727051 | KNN Loss: 4.877473831176758 | CLS Loss: 0.7353165149688721\n",
      "Epoch 1 / 200 | iteration 20 / 171 | Total Loss: 5.20296573638916 | KNN Loss: 4.562880992889404 | CLS Loss: 0.640084981918335\n",
      "Epoch 1 / 200 | iteration 30 / 171 | Total Loss: 5.0755534172058105 | KNN Loss: 4.533079147338867 | CLS Loss: 0.542474091053009\n",
      "Epoch 1 / 200 | iteration 40 / 171 | Total Loss: 4.917437553405762 | KNN Loss: 4.460722923278809 | CLS Loss: 0.45671454071998596\n",
      "Epoch 1 / 200 | iteration 50 / 171 | Total Loss: 4.9738054275512695 | KNN Loss: 4.410120010375977 | CLS Loss: 0.5636852979660034\n",
      "Epoch 1 / 200 | iteration 60 / 171 | Total Loss: 4.993725299835205 | KNN Loss: 4.4247589111328125 | CLS Loss: 0.5689665079116821\n",
      "Epoch 1 / 200 | iteration 70 / 171 | Total Loss: 4.924576759338379 | KNN Loss: 4.419571399688721 | CLS Loss: 0.5050052404403687\n",
      "Epoch 1 / 200 | iteration 80 / 171 | Total Loss: 4.859672546386719 | KNN Loss: 4.37296724319458 | CLS Loss: 0.48670512437820435\n",
      "Epoch 1 / 200 | iteration 90 / 171 | Total Loss: 4.895450115203857 | KNN Loss: 4.427886486053467 | CLS Loss: 0.46756380796432495\n",
      "Epoch 1 / 200 | iteration 100 / 171 | Total Loss: 4.777615547180176 | KNN Loss: 4.403937816619873 | CLS Loss: 0.3736775517463684\n",
      "Epoch 1 / 200 | iteration 110 / 171 | Total Loss: 4.780716896057129 | KNN Loss: 4.380603790283203 | CLS Loss: 0.4001130163669586\n",
      "Epoch 1 / 200 | iteration 120 / 171 | Total Loss: 4.7790656089782715 | KNN Loss: 4.368017196655273 | CLS Loss: 0.4110482633113861\n",
      "Epoch 1 / 200 | iteration 130 / 171 | Total Loss: 4.672855377197266 | KNN Loss: 4.40724515914917 | CLS Loss: 0.2656101882457733\n",
      "Epoch 1 / 200 | iteration 140 / 171 | Total Loss: 4.796299457550049 | KNN Loss: 4.419988632202148 | CLS Loss: 0.37631088495254517\n",
      "Epoch 1 / 200 | iteration 150 / 171 | Total Loss: 4.708394527435303 | KNN Loss: 4.393124103546143 | CLS Loss: 0.3152703642845154\n",
      "Epoch 1 / 200 | iteration 160 / 171 | Total Loss: 4.690085411071777 | KNN Loss: 4.338721752166748 | CLS Loss: 0.3513634204864502\n",
      "Epoch 1 / 200 | iteration 170 / 171 | Total Loss: 4.680228233337402 | KNN Loss: 4.343932151794434 | CLS Loss: 0.33629629015922546\n",
      "Epoch: 001, Loss: 4.9651, Train: 0.9260, Valid: 0.9260, Best: 0.9260\n",
      "Epoch 2 / 200 | iteration 0 / 171 | Total Loss: 4.71596097946167 | KNN Loss: 4.336399555206299 | CLS Loss: 0.3795613944530487\n",
      "Epoch 2 / 200 | iteration 10 / 171 | Total Loss: 4.659811019897461 | KNN Loss: 4.347435474395752 | CLS Loss: 0.3123754858970642\n",
      "Epoch 2 / 200 | iteration 20 / 171 | Total Loss: 4.598097324371338 | KNN Loss: 4.359053611755371 | CLS Loss: 0.23904381692409515\n",
      "Epoch 2 / 200 | iteration 30 / 171 | Total Loss: 4.593738079071045 | KNN Loss: 4.371452331542969 | CLS Loss: 0.22228556871414185\n",
      "Epoch 2 / 200 | iteration 40 / 171 | Total Loss: 4.643912315368652 | KNN Loss: 4.3139824867248535 | CLS Loss: 0.3299297094345093\n",
      "Epoch 2 / 200 | iteration 50 / 171 | Total Loss: 4.644227027893066 | KNN Loss: 4.374048709869385 | CLS Loss: 0.2701784372329712\n",
      "Epoch 2 / 200 | iteration 60 / 171 | Total Loss: 4.603218078613281 | KNN Loss: 4.340843677520752 | CLS Loss: 0.262374609708786\n",
      "Epoch 2 / 200 | iteration 70 / 171 | Total Loss: 4.625216484069824 | KNN Loss: 4.331838607788086 | CLS Loss: 0.29337766766548157\n",
      "Epoch 2 / 200 | iteration 80 / 171 | Total Loss: 4.541072845458984 | KNN Loss: 4.326622009277344 | CLS Loss: 0.21445059776306152\n",
      "Epoch 2 / 200 | iteration 90 / 171 | Total Loss: 4.584342002868652 | KNN Loss: 4.308960437774658 | CLS Loss: 0.27538132667541504\n",
      "Epoch 2 / 200 | iteration 100 / 171 | Total Loss: 4.528733253479004 | KNN Loss: 4.317209243774414 | CLS Loss: 0.2115238606929779\n",
      "Epoch 2 / 200 | iteration 110 / 171 | Total Loss: 4.610853672027588 | KNN Loss: 4.31043004989624 | CLS Loss: 0.3004235625267029\n",
      "Epoch 2 / 200 | iteration 120 / 171 | Total Loss: 4.502851963043213 | KNN Loss: 4.320936679840088 | CLS Loss: 0.18191523849964142\n",
      "Epoch 2 / 200 | iteration 130 / 171 | Total Loss: 4.484511375427246 | KNN Loss: 4.309606552124023 | CLS Loss: 0.17490459978580475\n",
      "Epoch 2 / 200 | iteration 140 / 171 | Total Loss: 4.506913185119629 | KNN Loss: 4.2765045166015625 | CLS Loss: 0.2304086536169052\n",
      "Epoch 2 / 200 | iteration 150 / 171 | Total Loss: 4.540704727172852 | KNN Loss: 4.311816692352295 | CLS Loss: 0.22888819873332977\n",
      "Epoch 2 / 200 | iteration 160 / 171 | Total Loss: 4.515021800994873 | KNN Loss: 4.325352191925049 | CLS Loss: 0.1896696835756302\n",
      "Epoch 2 / 200 | iteration 170 / 171 | Total Loss: 4.570695400238037 | KNN Loss: 4.338114261627197 | CLS Loss: 0.2325809895992279\n",
      "Epoch: 002, Loss: 4.5768, Train: 0.9442, Valid: 0.9430, Best: 0.9430\n",
      "Epoch 3 / 200 | iteration 0 / 171 | Total Loss: 4.491880893707275 | KNN Loss: 4.344360828399658 | CLS Loss: 0.1475202590227127\n",
      "Epoch 3 / 200 | iteration 10 / 171 | Total Loss: 4.509356498718262 | KNN Loss: 4.307199478149414 | CLS Loss: 0.20215719938278198\n",
      "Epoch 3 / 200 | iteration 20 / 171 | Total Loss: 4.4677019119262695 | KNN Loss: 4.319863319396973 | CLS Loss: 0.14783871173858643\n",
      "Epoch 3 / 200 | iteration 30 / 171 | Total Loss: 4.550123691558838 | KNN Loss: 4.328098773956299 | CLS Loss: 0.2220250368118286\n",
      "Epoch 3 / 200 | iteration 40 / 171 | Total Loss: 4.463845729827881 | KNN Loss: 4.323960781097412 | CLS Loss: 0.1398850679397583\n",
      "Epoch 3 / 200 | iteration 50 / 171 | Total Loss: 4.4862470626831055 | KNN Loss: 4.324982643127441 | CLS Loss: 0.16126449406147003\n",
      "Epoch 3 / 200 | iteration 60 / 171 | Total Loss: 4.4809370040893555 | KNN Loss: 4.267355442047119 | CLS Loss: 0.2135816216468811\n",
      "Epoch 3 / 200 | iteration 70 / 171 | Total Loss: 4.453398704528809 | KNN Loss: 4.288827896118164 | CLS Loss: 0.16457059979438782\n",
      "Epoch 3 / 200 | iteration 80 / 171 | Total Loss: 4.530871391296387 | KNN Loss: 4.318359375 | CLS Loss: 0.21251177787780762\n",
      "Epoch 3 / 200 | iteration 90 / 171 | Total Loss: 4.434281826019287 | KNN Loss: 4.312477111816406 | CLS Loss: 0.12180467694997787\n",
      "Epoch 3 / 200 | iteration 100 / 171 | Total Loss: 4.569517135620117 | KNN Loss: 4.367918491363525 | CLS Loss: 0.20159876346588135\n",
      "Epoch 3 / 200 | iteration 110 / 171 | Total Loss: 4.46439790725708 | KNN Loss: 4.286557197570801 | CLS Loss: 0.1778404861688614\n",
      "Epoch 3 / 200 | iteration 120 / 171 | Total Loss: 4.459035873413086 | KNN Loss: 4.281913757324219 | CLS Loss: 0.17712228000164032\n",
      "Epoch 3 / 200 | iteration 130 / 171 | Total Loss: 4.44544792175293 | KNN Loss: 4.300535678863525 | CLS Loss: 0.144912451505661\n",
      "Epoch 3 / 200 | iteration 140 / 171 | Total Loss: 4.428615570068359 | KNN Loss: 4.29124641418457 | CLS Loss: 0.13736917078495026\n",
      "Epoch 3 / 200 | iteration 150 / 171 | Total Loss: 4.49955415725708 | KNN Loss: 4.3029913902282715 | CLS Loss: 0.19656258821487427\n",
      "Epoch 3 / 200 | iteration 160 / 171 | Total Loss: 4.508092403411865 | KNN Loss: 4.3445515632629395 | CLS Loss: 0.16354086995124817\n",
      "Epoch 3 / 200 | iteration 170 / 171 | Total Loss: 4.426764965057373 | KNN Loss: 4.277972221374512 | CLS Loss: 0.14879265427589417\n",
      "Epoch: 003, Loss: 4.4807, Train: 0.9641, Valid: 0.9622, Best: 0.9622\n",
      "Epoch 4 / 200 | iteration 0 / 171 | Total Loss: 4.443874835968018 | KNN Loss: 4.280352592468262 | CLS Loss: 0.16352222859859467\n",
      "Epoch 4 / 200 | iteration 10 / 171 | Total Loss: 4.445949554443359 | KNN Loss: 4.304504871368408 | CLS Loss: 0.14144490659236908\n",
      "Epoch 4 / 200 | iteration 20 / 171 | Total Loss: 4.391397953033447 | KNN Loss: 4.266303539276123 | CLS Loss: 0.1250944882631302\n",
      "Epoch 4 / 200 | iteration 30 / 171 | Total Loss: 4.424411773681641 | KNN Loss: 4.335590839385986 | CLS Loss: 0.0888209268450737\n",
      "Epoch 4 / 200 | iteration 40 / 171 | Total Loss: 4.40311861038208 | KNN Loss: 4.269894599914551 | CLS Loss: 0.1332239955663681\n",
      "Epoch 4 / 200 | iteration 50 / 171 | Total Loss: 4.425288200378418 | KNN Loss: 4.294032573699951 | CLS Loss: 0.1312555968761444\n",
      "Epoch 4 / 200 | iteration 60 / 171 | Total Loss: 4.400121212005615 | KNN Loss: 4.2849602699279785 | CLS Loss: 0.11516071856021881\n",
      "Epoch 4 / 200 | iteration 70 / 171 | Total Loss: 4.442378997802734 | KNN Loss: 4.323642253875732 | CLS Loss: 0.11873680353164673\n",
      "Epoch 4 / 200 | iteration 80 / 171 | Total Loss: 4.433467864990234 | KNN Loss: 4.307845592498779 | CLS Loss: 0.12562249600887299\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 / 200 | iteration 90 / 171 | Total Loss: 4.373753070831299 | KNN Loss: 4.259344100952148 | CLS Loss: 0.11440882086753845\n",
      "Epoch 4 / 200 | iteration 100 / 171 | Total Loss: 4.385516166687012 | KNN Loss: 4.288597106933594 | CLS Loss: 0.09691883623600006\n",
      "Epoch 4 / 200 | iteration 110 / 171 | Total Loss: 4.467936038970947 | KNN Loss: 4.299871921539307 | CLS Loss: 0.1680641770362854\n",
      "Epoch 4 / 200 | iteration 120 / 171 | Total Loss: 4.395086288452148 | KNN Loss: 4.275415420532227 | CLS Loss: 0.11967094987630844\n",
      "Epoch 4 / 200 | iteration 130 / 171 | Total Loss: 4.4419846534729 | KNN Loss: 4.279983043670654 | CLS Loss: 0.1620015949010849\n",
      "Epoch 4 / 200 | iteration 140 / 171 | Total Loss: 4.393794059753418 | KNN Loss: 4.27605676651001 | CLS Loss: 0.11773736774921417\n",
      "Epoch 4 / 200 | iteration 150 / 171 | Total Loss: 4.46120548248291 | KNN Loss: 4.291041851043701 | CLS Loss: 0.17016372084617615\n",
      "Epoch 4 / 200 | iteration 160 / 171 | Total Loss: 4.411794185638428 | KNN Loss: 4.347096920013428 | CLS Loss: 0.0646972730755806\n",
      "Epoch 4 / 200 | iteration 170 / 171 | Total Loss: 4.4481096267700195 | KNN Loss: 4.306741714477539 | CLS Loss: 0.14136794209480286\n",
      "Epoch: 004, Loss: 4.4256, Train: 0.9702, Valid: 0.9672, Best: 0.9672\n",
      "Epoch 5 / 200 | iteration 0 / 171 | Total Loss: 4.377488613128662 | KNN Loss: 4.291090488433838 | CLS Loss: 0.08639802038669586\n",
      "Epoch 5 / 200 | iteration 10 / 171 | Total Loss: 4.4321513175964355 | KNN Loss: 4.321197509765625 | CLS Loss: 0.11095364391803741\n",
      "Epoch 5 / 200 | iteration 20 / 171 | Total Loss: 4.40584659576416 | KNN Loss: 4.317019462585449 | CLS Loss: 0.08882725983858109\n",
      "Epoch 5 / 200 | iteration 30 / 171 | Total Loss: 4.418765068054199 | KNN Loss: 4.29074239730835 | CLS Loss: 0.12802286446094513\n",
      "Epoch 5 / 200 | iteration 40 / 171 | Total Loss: 4.422760486602783 | KNN Loss: 4.322567939758301 | CLS Loss: 0.10019274055957794\n",
      "Epoch 5 / 200 | iteration 50 / 171 | Total Loss: 4.422915458679199 | KNN Loss: 4.3057684898376465 | CLS Loss: 0.11714692413806915\n",
      "Epoch 5 / 200 | iteration 60 / 171 | Total Loss: 4.487347602844238 | KNN Loss: 4.342167377471924 | CLS Loss: 0.14518004655838013\n",
      "Epoch 5 / 200 | iteration 70 / 171 | Total Loss: 4.414919376373291 | KNN Loss: 4.275418281555176 | CLS Loss: 0.13950131833553314\n",
      "Epoch 5 / 200 | iteration 80 / 171 | Total Loss: 4.360028266906738 | KNN Loss: 4.269812107086182 | CLS Loss: 0.09021616727113724\n",
      "Epoch 5 / 200 | iteration 90 / 171 | Total Loss: 4.3692803382873535 | KNN Loss: 4.276316165924072 | CLS Loss: 0.09296420216560364\n",
      "Epoch 5 / 200 | iteration 100 / 171 | Total Loss: 4.3579325675964355 | KNN Loss: 4.279934883117676 | CLS Loss: 0.07799755036830902\n",
      "Epoch 5 / 200 | iteration 110 / 171 | Total Loss: 4.386319160461426 | KNN Loss: 4.276605129241943 | CLS Loss: 0.10971392691135406\n",
      "Epoch 5 / 200 | iteration 120 / 171 | Total Loss: 4.365034580230713 | KNN Loss: 4.282962322235107 | CLS Loss: 0.08207221329212189\n",
      "Epoch 5 / 200 | iteration 130 / 171 | Total Loss: 4.457281112670898 | KNN Loss: 4.32774019241333 | CLS Loss: 0.12954087555408478\n",
      "Epoch 5 / 200 | iteration 140 / 171 | Total Loss: 4.353448390960693 | KNN Loss: 4.278708457946777 | CLS Loss: 0.07474007457494736\n",
      "Epoch 5 / 200 | iteration 150 / 171 | Total Loss: 4.315164089202881 | KNN Loss: 4.23895263671875 | CLS Loss: 0.07621141523122787\n",
      "Epoch 5 / 200 | iteration 160 / 171 | Total Loss: 4.459245681762695 | KNN Loss: 4.3163580894470215 | CLS Loss: 0.14288735389709473\n",
      "Epoch 5 / 200 | iteration 170 / 171 | Total Loss: 4.402795314788818 | KNN Loss: 4.3008952140808105 | CLS Loss: 0.1018998771905899\n",
      "Epoch: 005, Loss: 4.3928, Train: 0.9691, Valid: 0.9655, Best: 0.9672\n",
      "Epoch 6 / 200 | iteration 0 / 171 | Total Loss: 4.3691816329956055 | KNN Loss: 4.263437271118164 | CLS Loss: 0.10574425011873245\n",
      "Epoch 6 / 200 | iteration 10 / 171 | Total Loss: 4.348224639892578 | KNN Loss: 4.273686408996582 | CLS Loss: 0.0745382159948349\n",
      "Epoch 6 / 200 | iteration 20 / 171 | Total Loss: 4.363881587982178 | KNN Loss: 4.280636787414551 | CLS Loss: 0.08324459195137024\n",
      "Epoch 6 / 200 | iteration 30 / 171 | Total Loss: 4.355806827545166 | KNN Loss: 4.268455982208252 | CLS Loss: 0.08735093474388123\n",
      "Epoch 6 / 200 | iteration 40 / 171 | Total Loss: 4.331631183624268 | KNN Loss: 4.243982315063477 | CLS Loss: 0.08764878660440445\n",
      "Epoch 6 / 200 | iteration 50 / 171 | Total Loss: 4.34805965423584 | KNN Loss: 4.24617862701416 | CLS Loss: 0.10188115388154984\n",
      "Epoch 6 / 200 | iteration 60 / 171 | Total Loss: 4.365006446838379 | KNN Loss: 4.264211177825928 | CLS Loss: 0.10079528391361237\n",
      "Epoch 6 / 200 | iteration 70 / 171 | Total Loss: 4.370314121246338 | KNN Loss: 4.256260395050049 | CLS Loss: 0.11405354738235474\n",
      "Epoch 6 / 200 | iteration 80 / 171 | Total Loss: 4.352452278137207 | KNN Loss: 4.2454376220703125 | CLS Loss: 0.10701480507850647\n",
      "Epoch 6 / 200 | iteration 90 / 171 | Total Loss: 4.3543901443481445 | KNN Loss: 4.2774834632873535 | CLS Loss: 0.07690653949975967\n",
      "Epoch 6 / 200 | iteration 100 / 171 | Total Loss: 4.400126934051514 | KNN Loss: 4.279871463775635 | CLS Loss: 0.12025553733110428\n",
      "Epoch 6 / 200 | iteration 110 / 171 | Total Loss: 4.378850936889648 | KNN Loss: 4.259433269500732 | CLS Loss: 0.11941755563020706\n",
      "Epoch 6 / 200 | iteration 120 / 171 | Total Loss: 4.4207940101623535 | KNN Loss: 4.303954124450684 | CLS Loss: 0.11683980375528336\n",
      "Epoch 6 / 200 | iteration 130 / 171 | Total Loss: 4.364468097686768 | KNN Loss: 4.266334533691406 | CLS Loss: 0.0981336161494255\n",
      "Epoch 6 / 200 | iteration 140 / 171 | Total Loss: 4.345520496368408 | KNN Loss: 4.247052192687988 | CLS Loss: 0.09846823662519455\n",
      "Epoch 6 / 200 | iteration 150 / 171 | Total Loss: 4.349569320678711 | KNN Loss: 4.26032829284668 | CLS Loss: 0.08924105018377304\n",
      "Epoch 6 / 200 | iteration 160 / 171 | Total Loss: 4.36723518371582 | KNN Loss: 4.290198802947998 | CLS Loss: 0.07703643292188644\n",
      "Epoch 6 / 200 | iteration 170 / 171 | Total Loss: 4.404850006103516 | KNN Loss: 4.294493198394775 | CLS Loss: 0.11035677045583725\n",
      "Epoch: 006, Loss: 4.3672, Train: 0.9777, Valid: 0.9747, Best: 0.9747\n",
      "Epoch 7 / 200 | iteration 0 / 171 | Total Loss: 4.39334774017334 | KNN Loss: 4.276278018951416 | CLS Loss: 0.11706969887018204\n",
      "Epoch 7 / 200 | iteration 10 / 171 | Total Loss: 4.332215785980225 | KNN Loss: 4.2527079582214355 | CLS Loss: 0.07950787991285324\n",
      "Epoch 7 / 200 | iteration 20 / 171 | Total Loss: 4.3141937255859375 | KNN Loss: 4.256509304046631 | CLS Loss: 0.05768433213233948\n",
      "Epoch 7 / 200 | iteration 30 / 171 | Total Loss: 4.364973068237305 | KNN Loss: 4.298440456390381 | CLS Loss: 0.06653272360563278\n",
      "Epoch 7 / 200 | iteration 40 / 171 | Total Loss: 4.341868877410889 | KNN Loss: 4.30249547958374 | CLS Loss: 0.03937354311347008\n",
      "Epoch 7 / 200 | iteration 50 / 171 | Total Loss: 4.347552299499512 | KNN Loss: 4.233338356018066 | CLS Loss: 0.1142139732837677\n",
      "Epoch 7 / 200 | iteration 60 / 171 | Total Loss: 4.392513275146484 | KNN Loss: 4.276900291442871 | CLS Loss: 0.11561302095651627\n",
      "Epoch 7 / 200 | iteration 70 / 171 | Total Loss: 4.3695068359375 | KNN Loss: 4.2823944091796875 | CLS Loss: 0.08711224049329758\n",
      "Epoch 7 / 200 | iteration 80 / 171 | Total Loss: 4.333276271820068 | KNN Loss: 4.242326259613037 | CLS Loss: 0.09095019102096558\n",
      "Epoch 7 / 200 | iteration 90 / 171 | Total Loss: 4.316303730010986 | KNN Loss: 4.248053550720215 | CLS Loss: 0.06825007498264313\n",
      "Epoch 7 / 200 | iteration 100 / 171 | Total Loss: 4.352168560028076 | KNN Loss: 4.254634380340576 | CLS Loss: 0.09753399342298508\n",
      "Epoch 7 / 200 | iteration 110 / 171 | Total Loss: 4.336479663848877 | KNN Loss: 4.265700817108154 | CLS Loss: 0.07077892869710922\n",
      "Epoch 7 / 200 | iteration 120 / 171 | Total Loss: 4.411590576171875 | KNN Loss: 4.280231952667236 | CLS Loss: 0.13135866820812225\n",
      "Epoch 7 / 200 | iteration 130 / 171 | Total Loss: 4.337140083312988 | KNN Loss: 4.236564636230469 | CLS Loss: 0.10057563334703445\n"
     ]
    }
   ],
   "source": [
    "best_valid_acc = 0\n",
    "losses = []\n",
    "train_accs = []\n",
    "val_accs = []\n",
    "for epoch in range(1, epochs + 1):\n",
    "    loss = train(model, train_data_iter, optimizer, device)\n",
    "#     print(f\"Loss: {loss} =============================\")\n",
    "    losses.append(loss)\n",
    "    train_acc = test(model, train_data_iter, device)\n",
    "    train_accs.append(train_acc)\n",
    "    valid_acc = test(model, test_data_iter, device)\n",
    "    val_accs.append(valid_acc)\n",
    "    if valid_acc > best_valid_acc:\n",
    "        best_valid_acc = valid_acc\n",
    "    if epoch % 1 == 0:\n",
    "        print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, '\n",
    "              f'Train: {train_acc:.4f}, Valid: {valid_acc:.4f}, '\n",
    "              f'Best: {best_valid_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(model, test_data_iter, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(losses, label='train loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(train_accs, label='train accuracy')\n",
    "plt.plot(val_accs, label='validation accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_samples = torch.tensor([])\n",
    "projections = torch.tensor([])\n",
    "labels = torch.tensor([])\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, y in tqdm(test_data_iter):\n",
    "        test_samples = torch.cat([test_samples, x])\n",
    "        labels = torch.cat([labels, y])\n",
    "        x = x.to(device)\n",
    "        _, interm = model(x, True)\n",
    "        projections = torch.cat([projections, interm.detach().cpu().flatten(1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = pairwise_distances(projections)\n",
    "# distances = np.triu(distances)\n",
    "distances_f = distances.flatten()\n",
    "\n",
    "plt.matshow(distances)\n",
    "plt.colorbar()\n",
    "plt.figure()\n",
    "plt.hist(distances_f[distances_f > 0], bins=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = DBSCAN(eps=2, min_samples=10).fit_predict(projections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of inliers: {sum(clusters != -1) / len(clusters)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexity = 100\n",
    "p = reduce_dims_and_plot(projections[clusters != -1],\n",
    "                         y=clusters[clusters != -1],\n",
    "                         title=f'perplexity: {perplexity}',\n",
    "                         file_name=None,\n",
    "                         perplexity=perplexity,\n",
    "                         library='Multicore-TSNE',\n",
    "                         perform_PCA=False,\n",
    "                         projected=None,\n",
    "                         figure_type='2d',\n",
    "                         show_figure=True,\n",
    "                         close_figure=False,\n",
    "                         text=None)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Soft-Decision-Tree given the self-labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_dataset = list(zip(test_samples.flatten(1)[clusters!=-1], clusters[clusters != -1]))\n",
    "batch_size = 512\n",
    "tree_loader = torch.utils.data.DataLoader(tree_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define how we prune the weights of a node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_node(node_weights, factor=1):\n",
    "    w = node_weights.cpu().detach().numpy()\n",
    "    mean_ = np.mean(w)\n",
    "    std_ = np.std(w)\n",
    "    node_weights[((mean_ - std_ * factor) < node_weights) & (node_weights < (mean_ + std_ * factor))] = 0\n",
    "    return node_weights\n",
    "\n",
    "def prune_node_keep(node_weights, keep=4):\n",
    "    w = node_weights.cpu().detach().numpy()\n",
    "    throw_idx = np.argsort(abs(w))[:-keep]\n",
    "    node_weights[throw_idx] = 0\n",
    "    return node_weights\n",
    "\n",
    "def prune_tree(tree_, factor):\n",
    "    new_weights = tree_.inner_nodes.weight.clone()\n",
    "    for i in range(new_weights.shape[0]):\n",
    "        res = prune_node_keep(new_weights[i, :], factor)\n",
    "        new_weights[i, :] = res\n",
    "\n",
    "    with torch.no_grad():\n",
    "        tree_.inner_nodes.weight.copy_(new_weights)\n",
    "        \n",
    "def sparseness(x):\n",
    "    s = []\n",
    "    for i in range(x.shape[0]):\n",
    "        x_ = x[i, :]\n",
    "        sp = (len(x_) - torch.norm(x_, 0).item()) / len(x_)\n",
    "        s.append(sp)\n",
    "    return np.mean(s)\n",
    "\n",
    "def compute_regularization_by_level(tree):\n",
    "    total_reg = 0\n",
    "    for i in range(tree.inner_nodes.weight.shape[0]):\n",
    "        cur_level = np.floor(np.log2(i+1))\n",
    "        node_reg = torch.norm(tree.inner_nodes.weight[i].view(-1), 2)\n",
    "        total_reg += 2**(-cur_level) * node_reg\n",
    "    return total_reg\n",
    "\n",
    "def show_sparseness(tree):\n",
    "    avg_sp = sparseness(tree.inner_nodes.weight)\n",
    "    print(f\"Average sparseness: {avg_sp}\")\n",
    "    layer = 0\n",
    "    sps = []\n",
    "    for i in range(tree.inner_nodes.weight.shape[0]):\n",
    "        cur_layer = int(np.floor(np.log2(i+1)))\n",
    "        if cur_layer != layer:\n",
    "            print(f\"layer {layer}: {np.mean(sps)}\")\n",
    "            sps = []\n",
    "            layer = cur_layer\n",
    "\n",
    "        x_ = tree.inner_nodes.weight[i, :]\n",
    "        sp = (len(x_) - torch.norm(x_, 0).item()) / len(x_)\n",
    "        sps.append(sp)\n",
    "        \n",
    "    return avg_sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_epoch(model, loader, device, log_interval, losses, accs, epoch, iteration):\n",
    "    model = model.train()\n",
    "    for batch_idx, (data, target) in enumerate(loader):\n",
    "        iteration += 1\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        output, penalty = tree.forward(data)\n",
    "\n",
    "        # Loss\n",
    "        loss_tree = criterion(output, target.view(-1))\n",
    "\n",
    "        # Penalty\n",
    "        loss_tree += penalty\n",
    "\n",
    "        # Sparse regularization\n",
    "#         fc_params = torch.cat([x.view(-1) for x in tree.inner_nodes.parameters()])\n",
    "#         regularization = sparsity_lamda * torch.norm(fc_params, 2)\n",
    "        regularization = sparsity_lamda * compute_regularization_by_level(tree)\n",
    "        loss = loss_tree\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        pred = output.data.max(1)[1]\n",
    "        correct = pred.eq(target.view(-1).data).sum()\n",
    "        accs.append(correct.item() / data.size()[0])\n",
    "\n",
    "        # Print training status\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(f\"Epoch: {epoch:02d} | Batch: {batch_idx:03d} / {len(loader):03d} | Total loss: {loss.item():.3f} | Reg loss: {regularization.item():.3f} | Tree loss: {loss_tree.item():.3f} | Accuracy: {correct.item() / data.size()[0]:03f} | {round((time.time() - start_time) / iteration, 3)} sec/iter\")\n",
    "            \n",
    "    return iteration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 5e-3\n",
    "weight_decay = 5e-4\n",
    "sparsity_lamda = 2e-3\n",
    "epochs = 400\n",
    "log_interval = 100\n",
    "use_cuda = device != 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = SDT(input_dim=test_samples.shape[2], output_dim=len(set(clusters)) - 1, depth=tree_depth, lamda=1e-3, use_cuda=use_cuda)\n",
    "optimizer = torch.optim.Adam(tree.parameters(),\n",
    "                                 lr=lr,\n",
    "                                 weight_decay=weight_decay)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "tree = tree.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "accs = []\n",
    "sparsity = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "iteration = 0\n",
    "for epoch in range(epochs):\n",
    "    # Training\n",
    "    avg_sp = show_sparseness(tree)\n",
    "    sparsity.append(avg_sp)\n",
    "    iteration = do_epoch(tree, tree_loader, device, log_interval, losses, accs, epoch, iteration)\n",
    "    \n",
    "    if epoch % 1 == 0:\n",
    "        prune_tree(tree, factor=3)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel('Iteration')\n",
    "plt.plot(accs, label='Accuracy vs iteration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel('Iteration')\n",
    "plt.plot(losses, label='Loss vs iteration')\n",
    "# plt.yscale(\"log\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "weights = tree.inner_nodes.weight.cpu().detach().numpy().flatten()\n",
    "plt.hist(weights, bins=500)\n",
    "weights_std = np.std(weights)\n",
    "weights_mean = np.mean(weights)\n",
    "plt.axvline(weights_mean + weights_std, color='r')\n",
    "plt.axvline(weights_mean - weights_std, color='r')\n",
    "plt.title(f\"Mean: {weights_mean}   |   STD: {weights_std}\")\n",
    "plt.yscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10), dpi=80)\n",
    "avg_height, root = tree.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accumulate samples in the leaves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of patterns: {len(root.get_leaves())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = 'greedy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root.clear_leaves_samples()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (data, target) in enumerate(tree_loader):\n",
    "        root.accumulate_samples(data, method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tighten boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr_names = [f\"T_{i}\" for i in range(test_samples.shape[2])]\n",
    "leaves = root.get_leaves()\n",
    "sum_comprehensibility = 0\n",
    "comprehensibilities = []\n",
    "for pattern_counter, leaf in enumerate(leaves):\n",
    "    leaf.reset_path()\n",
    "    leaf.tighten_with_accumulated_samples()\n",
    "    conds = leaf.get_path_conditions(attr_names)\n",
    "    print(f\"============== Pattern {pattern_counter + 1} ==============\")\n",
    "    comprehensibilities.append(sum([cond.comprehensibility for cond in conds]))\n",
    "    \n",
    "print(f\"Average comprehensibility: {np.mean(comprehensibilities)}\")\n",
    "print(f\"std comprehensibility: {np.std(comprehensibilities)}\")\n",
    "print(f\"var comprehensibility: {np.var(comprehensibilities)}\")\n",
    "print(f\"minimum comprehensibility: {np.min(comprehensibilities)}\")\n",
    "print(f\"maximum comprehensibility: {np.max(comprehensibilities)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
