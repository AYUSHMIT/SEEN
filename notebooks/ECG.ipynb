{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib widget\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import torch.nn as nn\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from stream_generators.mit_bih import MITBIH\n",
    "from utils.MatplotlibUtils import reduce_dims_and_plot\n",
    "from network.auto_encoder import AutoEncoder\n",
    "from losses.knn_loss import KNNLoss, ClassificationKNNLoss\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from soft_decision_tree.sdt_model import SDT\n",
    "from sklearn.metrics import davies_bouldin_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 8\n",
    "tree_depth = 6\n",
    "batch_size = 512\n",
    "device = 'cpu'\n",
    "train_data_path = r'F:\\Downloads\\archive\\mitbih_train.csv'\n",
    "test_data_path = r'F:\\Downloads\\archive\\mitbih_test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_iter = torch.utils.data.DataLoader(MITBIH(train_data_path),\n",
    "                                             batch_size=batch_size,\n",
    "                                             shuffle=True,\n",
    "                                             num_workers=1,\n",
    "                                             pin_memory=True,\n",
    "                                             drop_last=True)\n",
    "\n",
    "test_data_iter = torch.utils.data.DataLoader(MITBIH(test_data_path),\n",
    "                                             batch_size=batch_size,\n",
    "                                             shuffle=True,\n",
    "                                             num_workers=1,\n",
    "                                             pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(32, 32, kernel_size=5, stride=1, padding=2)\n",
    "        self.conv2 = nn.Conv1d(32, 32, kernel_size=5, stride=1, padding=2)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool = nn.MaxPool1d(kernel_size=5, stride=2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = x\n",
    "        y = self.conv1(y)\n",
    "        y = self.relu1(y)\n",
    "        y = self.conv2(y)\n",
    "        y = y + x\n",
    "        y = self.relu2(y)\n",
    "        y = self.pool(y)\n",
    "        return y\n",
    "\n",
    "\n",
    "class ECGModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ECGModel, self).__init__()\n",
    "        self.conv = nn.Conv1d(1, 32, kernel_size=5, stride=1, padding=1)\n",
    "        self.block1 = ConvBlock()\n",
    "        self.block2 = ConvBlock()\n",
    "        self.block3 = ConvBlock()\n",
    "        self.block4 = ConvBlock()\n",
    "        self.block5 = ConvBlock()\n",
    "        self.fc1 = nn.Linear(64, 32)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(32, 5)\n",
    "\n",
    "    def forward(self, x, return_interm=False):\n",
    "        x = self.conv(x)\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.block4(x)\n",
    "        x = self.block5(x)\n",
    "        interm = x.flatten(1)\n",
    "        x = self.fc1(interm)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        if return_interm:\n",
    "            return x, interm\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_crt = ClassificationKNNLoss(k=k).to(device)\n",
    "\n",
    "def train(model, loader, optimizer, device):\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    for iteration, (batch, target) in enumerate(loader):\n",
    "        batch = batch.to(device)\n",
    "        target = target.to(device)\n",
    "        outputs, interm = model(batch, return_interm=True)\n",
    "        mse_loss = F.cross_entropy(outputs, target)\n",
    "        mse_loss = mse_loss.sum(dim=-1).mean()\n",
    "        try:\n",
    "            knn_loss = knn_crt(interm, target)\n",
    "            if torch.isinf(knn_loss):\n",
    "                knn_loss = torch.tensor(0)\n",
    "        except ValueError:\n",
    "            knn_loss = torch.tensor(0)\n",
    "        loss = mse_loss + knn_loss\n",
    "        total_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if iteration % log_every == 0:\n",
    "            print(f\"Epoch {epoch} / {epochs} | iteration {iteration} / {len(loader)} | Total Loss: {loss.item()} | KNN Loss: {knn_loss.item()} | CLS Loss: {mse_loss.item()}\")\n",
    "\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model, loader, device):\n",
    "    model.eval()\n",
    "    \n",
    "    correct = 0\n",
    "    for iteration, (batch, target) in enumerate(loader):\n",
    "        batch = batch.to(device)\n",
    "        target = target.to(device)\n",
    "        y_pred = model(batch).argmax(dim=-1)\n",
    "        correct += y_pred.eq(target.view(-1).data).sum()\n",
    "    \n",
    "    return correct / len(loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Params: 53957\n"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "lr = 1e-3\n",
    "log_every = 10\n",
    "\n",
    "model = ECGModel()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "num_params = sum([p.numel() for p in model.parameters()])\n",
    "print(f'#Params: {num_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 200 | iteration 0 / 171 | Total Loss: 7.22037935256958 | KNN Loss: 5.639108180999756 | CLS Loss: 1.5812710523605347\n",
      "Epoch 1 / 200 | iteration 10 / 171 | Total Loss: 4.512310028076172 | KNN Loss: 3.8615217208862305 | CLS Loss: 0.6507881879806519\n",
      "Epoch 1 / 200 | iteration 20 / 171 | Total Loss: 3.386953353881836 | KNN Loss: 2.649182081222534 | CLS Loss: 0.737771213054657\n",
      "Epoch 1 / 200 | iteration 30 / 171 | Total Loss: 3.1240243911743164 | KNN Loss: 2.560368776321411 | CLS Loss: 0.5636554956436157\n",
      "Epoch 1 / 200 | iteration 40 / 171 | Total Loss: 3.0672225952148438 | KNN Loss: 2.531372547149658 | CLS Loss: 0.5358500480651855\n",
      "Epoch 1 / 200 | iteration 50 / 171 | Total Loss: 3.016354560852051 | KNN Loss: 2.437026023864746 | CLS Loss: 0.5793285965919495\n",
      "Epoch 1 / 200 | iteration 60 / 171 | Total Loss: 2.989715099334717 | KNN Loss: 2.495742082595825 | CLS Loss: 0.493973046541214\n",
      "Epoch 1 / 200 | iteration 70 / 171 | Total Loss: 2.927281379699707 | KNN Loss: 2.4779787063598633 | CLS Loss: 0.4493025839328766\n",
      "Epoch 1 / 200 | iteration 80 / 171 | Total Loss: 2.9567368030548096 | KNN Loss: 2.497347116470337 | CLS Loss: 0.45938971638679504\n",
      "Epoch 1 / 200 | iteration 90 / 171 | Total Loss: 2.9091269969940186 | KNN Loss: 2.479003429412842 | CLS Loss: 0.43012359738349915\n",
      "Epoch 1 / 200 | iteration 100 / 171 | Total Loss: 2.920321226119995 | KNN Loss: 2.4787795543670654 | CLS Loss: 0.44154173135757446\n",
      "Epoch 1 / 200 | iteration 110 / 171 | Total Loss: 2.8289945125579834 | KNN Loss: 2.459540367126465 | CLS Loss: 0.36945420503616333\n",
      "Epoch 1 / 200 | iteration 120 / 171 | Total Loss: 2.875887155532837 | KNN Loss: 2.472320556640625 | CLS Loss: 0.4035666584968567\n",
      "Epoch 1 / 200 | iteration 130 / 171 | Total Loss: 2.879185676574707 | KNN Loss: 2.4704442024230957 | CLS Loss: 0.4087415635585785\n",
      "Epoch 1 / 200 | iteration 140 / 171 | Total Loss: 2.8989944458007812 | KNN Loss: 2.5027475357055664 | CLS Loss: 0.3962470293045044\n",
      "Epoch 1 / 200 | iteration 150 / 171 | Total Loss: 2.7965919971466064 | KNN Loss: 2.5140321254730225 | CLS Loss: 0.28255993127822876\n",
      "Epoch 1 / 200 | iteration 160 / 171 | Total Loss: 2.826075792312622 | KNN Loss: 2.511681079864502 | CLS Loss: 0.31439465284347534\n",
      "Epoch 1 / 200 | iteration 170 / 171 | Total Loss: 2.8866488933563232 | KNN Loss: 2.503333330154419 | CLS Loss: 0.38331565260887146\n",
      "Epoch: 001, Loss: 3.2052, Train: 0.9101, Valid: 0.9079, Best: 0.9079\n",
      "Epoch 2 / 200 | iteration 0 / 171 | Total Loss: 2.7953779697418213 | KNN Loss: 2.464682102203369 | CLS Loss: 0.330695778131485\n",
      "Epoch 2 / 200 | iteration 10 / 171 | Total Loss: 2.8310134410858154 | KNN Loss: 2.5115814208984375 | CLS Loss: 0.3194320499897003\n",
      "Epoch 2 / 200 | iteration 20 / 171 | Total Loss: 2.795603036880493 | KNN Loss: 2.492485284805298 | CLS Loss: 0.3031177818775177\n",
      "Epoch 2 / 200 | iteration 30 / 171 | Total Loss: 2.8436121940612793 | KNN Loss: 2.507675886154175 | CLS Loss: 0.33593621850013733\n",
      "Epoch 2 / 200 | iteration 40 / 171 | Total Loss: 2.7627744674682617 | KNN Loss: 2.4628145694732666 | CLS Loss: 0.29995986819267273\n",
      "Epoch 2 / 200 | iteration 50 / 171 | Total Loss: 2.801422357559204 | KNN Loss: 2.479611396789551 | CLS Loss: 0.32181087136268616\n",
      "Epoch 2 / 200 | iteration 60 / 171 | Total Loss: 2.7239274978637695 | KNN Loss: 2.4992294311523438 | CLS Loss: 0.22469812631607056\n",
      "Epoch 2 / 200 | iteration 70 / 171 | Total Loss: 2.8084778785705566 | KNN Loss: 2.5122368335723877 | CLS Loss: 0.2962409555912018\n",
      "Epoch 2 / 200 | iteration 80 / 171 | Total Loss: 2.733083724975586 | KNN Loss: 2.514087200164795 | CLS Loss: 0.21899649500846863\n",
      "Epoch 2 / 200 | iteration 90 / 171 | Total Loss: 2.7755019664764404 | KNN Loss: 2.493361234664917 | CLS Loss: 0.28214067220687866\n",
      "Epoch 2 / 200 | iteration 100 / 171 | Total Loss: 2.75569224357605 | KNN Loss: 2.50077748298645 | CLS Loss: 0.2549148499965668\n",
      "Epoch 2 / 200 | iteration 110 / 171 | Total Loss: 2.7695577144622803 | KNN Loss: 2.5229811668395996 | CLS Loss: 0.24657654762268066\n",
      "Epoch 2 / 200 | iteration 120 / 171 | Total Loss: 2.7683653831481934 | KNN Loss: 2.499629020690918 | CLS Loss: 0.26873642206192017\n",
      "Epoch 2 / 200 | iteration 130 / 171 | Total Loss: 2.7369658946990967 | KNN Loss: 2.486412286758423 | CLS Loss: 0.2505536675453186\n",
      "Epoch 2 / 200 | iteration 140 / 171 | Total Loss: 2.701738119125366 | KNN Loss: 2.510439395904541 | CLS Loss: 0.1912987232208252\n",
      "Epoch 2 / 200 | iteration 150 / 171 | Total Loss: 2.6923229694366455 | KNN Loss: 2.4706315994262695 | CLS Loss: 0.22169145941734314\n",
      "Epoch 2 / 200 | iteration 160 / 171 | Total Loss: 2.695054292678833 | KNN Loss: 2.474550485610962 | CLS Loss: 0.22050383687019348\n",
      "Epoch 2 / 200 | iteration 170 / 171 | Total Loss: 2.706785202026367 | KNN Loss: 2.4896111488342285 | CLS Loss: 0.21717403829097748\n",
      "Epoch: 002, Loss: 2.7669, Train: 0.9406, Valid: 0.9404, Best: 0.9404\n",
      "Epoch 3 / 200 | iteration 0 / 171 | Total Loss: 2.7211573123931885 | KNN Loss: 2.5166778564453125 | CLS Loss: 0.20447944104671478\n",
      "Epoch 3 / 200 | iteration 10 / 171 | Total Loss: 2.691173791885376 | KNN Loss: 2.507122039794922 | CLS Loss: 0.1840517669916153\n",
      "Epoch 3 / 200 | iteration 20 / 171 | Total Loss: 2.65714168548584 | KNN Loss: 2.5053837299346924 | CLS Loss: 0.1517578512430191\n",
      "Epoch 3 / 200 | iteration 30 / 171 | Total Loss: 2.6792101860046387 | KNN Loss: 2.461427927017212 | CLS Loss: 0.21778236329555511\n",
      "Epoch 3 / 200 | iteration 40 / 171 | Total Loss: 2.67529034614563 | KNN Loss: 2.4982221126556396 | CLS Loss: 0.17706815898418427\n",
      "Epoch 3 / 200 | iteration 50 / 171 | Total Loss: 2.6273281574249268 | KNN Loss: 2.504653215408325 | CLS Loss: 0.12267495691776276\n",
      "Epoch 3 / 200 | iteration 60 / 171 | Total Loss: 2.673062801361084 | KNN Loss: 2.501588821411133 | CLS Loss: 0.17147406935691833\n",
      "Epoch 3 / 200 | iteration 70 / 171 | Total Loss: 2.675048589706421 | KNN Loss: 2.5375049114227295 | CLS Loss: 0.13754358887672424\n",
      "Epoch 3 / 200 | iteration 80 / 171 | Total Loss: 2.6521897315979004 | KNN Loss: 2.527151346206665 | CLS Loss: 0.12503832578659058\n",
      "Epoch 3 / 200 | iteration 90 / 171 | Total Loss: 2.6883187294006348 | KNN Loss: 2.4845361709594727 | CLS Loss: 0.2037825882434845\n",
      "Epoch 3 / 200 | iteration 100 / 171 | Total Loss: 2.636523962020874 | KNN Loss: 2.509490489959717 | CLS Loss: 0.12703350186347961\n",
      "Epoch 3 / 200 | iteration 110 / 171 | Total Loss: 2.6434240341186523 | KNN Loss: 2.4898345470428467 | CLS Loss: 0.15358960628509521\n",
      "Epoch 3 / 200 | iteration 120 / 171 | Total Loss: 2.6998281478881836 | KNN Loss: 2.4955742359161377 | CLS Loss: 0.2042538970708847\n",
      "Epoch 3 / 200 | iteration 130 / 171 | Total Loss: 2.662729501724243 | KNN Loss: 2.4991157054901123 | CLS Loss: 0.16361385583877563\n",
      "Epoch 3 / 200 | iteration 140 / 171 | Total Loss: 2.6559243202209473 | KNN Loss: 2.4909770488739014 | CLS Loss: 0.1649472564458847\n",
      "Epoch 3 / 200 | iteration 150 / 171 | Total Loss: 2.6626298427581787 | KNN Loss: 2.4857120513916016 | CLS Loss: 0.17691779136657715\n",
      "Epoch 3 / 200 | iteration 160 / 171 | Total Loss: 2.6189754009246826 | KNN Loss: 2.465498447418213 | CLS Loss: 0.15347695350646973\n",
      "Epoch 3 / 200 | iteration 170 / 171 | Total Loss: 2.6320176124572754 | KNN Loss: 2.441337823867798 | CLS Loss: 0.19067984819412231\n",
      "Epoch: 003, Loss: 2.6726, Train: 0.9608, Valid: 0.9595, Best: 0.9595\n",
      "Epoch 4 / 200 | iteration 0 / 171 | Total Loss: 2.671665668487549 | KNN Loss: 2.4789559841156006 | CLS Loss: 0.19270974397659302\n",
      "Epoch 4 / 200 | iteration 10 / 171 | Total Loss: 2.6508352756500244 | KNN Loss: 2.479680061340332 | CLS Loss: 0.1711551994085312\n",
      "Epoch 4 / 200 | iteration 20 / 171 | Total Loss: 2.6499009132385254 | KNN Loss: 2.4550881385803223 | CLS Loss: 0.19481265544891357\n",
      "Epoch 4 / 200 | iteration 30 / 171 | Total Loss: 2.6482045650482178 | KNN Loss: 2.489682197570801 | CLS Loss: 0.15852242708206177\n",
      "Epoch 4 / 200 | iteration 40 / 171 | Total Loss: 2.611964225769043 | KNN Loss: 2.442495107650757 | CLS Loss: 0.16946910321712494\n",
      "Epoch 4 / 200 | iteration 50 / 171 | Total Loss: 2.635146141052246 | KNN Loss: 2.4977309703826904 | CLS Loss: 0.13741512596607208\n",
      "Epoch 4 / 200 | iteration 60 / 171 | Total Loss: 2.6316604614257812 | KNN Loss: 2.507627487182617 | CLS Loss: 0.1240328997373581\n",
      "Epoch 4 / 200 | iteration 70 / 171 | Total Loss: 2.606128692626953 | KNN Loss: 2.4373226165771484 | CLS Loss: 0.16880615055561066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 / 200 | iteration 80 / 171 | Total Loss: 2.629610061645508 | KNN Loss: 2.456813335418701 | CLS Loss: 0.17279668152332306\n",
      "Epoch 4 / 200 | iteration 90 / 171 | Total Loss: 2.5800623893737793 | KNN Loss: 2.4619479179382324 | CLS Loss: 0.11811457574367523\n",
      "Epoch 4 / 200 | iteration 100 / 171 | Total Loss: 2.6214852333068848 | KNN Loss: 2.4368479251861572 | CLS Loss: 0.1846373826265335\n",
      "Epoch 4 / 200 | iteration 110 / 171 | Total Loss: 2.652789831161499 | KNN Loss: 2.5058364868164062 | CLS Loss: 0.1469532549381256\n",
      "Epoch 4 / 200 | iteration 120 / 171 | Total Loss: 2.631235361099243 | KNN Loss: 2.4475133419036865 | CLS Loss: 0.183722123503685\n",
      "Epoch 4 / 200 | iteration 130 / 171 | Total Loss: 2.621601104736328 | KNN Loss: 2.4818685054779053 | CLS Loss: 0.13973253965377808\n",
      "Epoch 4 / 200 | iteration 140 / 171 | Total Loss: 2.620903253555298 | KNN Loss: 2.517056465148926 | CLS Loss: 0.10384675860404968\n",
      "Epoch 4 / 200 | iteration 150 / 171 | Total Loss: 2.6423487663269043 | KNN Loss: 2.483466386795044 | CLS Loss: 0.158882275223732\n",
      "Epoch 4 / 200 | iteration 160 / 171 | Total Loss: 2.6103079319000244 | KNN Loss: 2.46496319770813 | CLS Loss: 0.14534474909305573\n",
      "Epoch 4 / 200 | iteration 170 / 171 | Total Loss: 2.6086158752441406 | KNN Loss: 2.4564316272735596 | CLS Loss: 0.1521843671798706\n",
      "Epoch: 004, Loss: 2.6226, Train: 0.9675, Valid: 0.9659, Best: 0.9659\n",
      "Epoch 5 / 200 | iteration 0 / 171 | Total Loss: 2.568864345550537 | KNN Loss: 2.4747862815856934 | CLS Loss: 0.09407810866832733\n",
      "Epoch 5 / 200 | iteration 10 / 171 | Total Loss: 2.614171028137207 | KNN Loss: 2.433502197265625 | CLS Loss: 0.1806689202785492\n",
      "Epoch 5 / 200 | iteration 20 / 171 | Total Loss: 2.614419937133789 | KNN Loss: 2.4818432331085205 | CLS Loss: 0.13257676362991333\n",
      "Epoch 5 / 200 | iteration 30 / 171 | Total Loss: 2.5896477699279785 | KNN Loss: 2.4453399181365967 | CLS Loss: 0.144307941198349\n",
      "Epoch 5 / 200 | iteration 40 / 171 | Total Loss: 2.6357264518737793 | KNN Loss: 2.5235595703125 | CLS Loss: 0.11216691136360168\n",
      "Epoch 5 / 200 | iteration 50 / 171 | Total Loss: 2.5698535442352295 | KNN Loss: 2.4752445220947266 | CLS Loss: 0.09460891783237457\n",
      "Epoch 5 / 200 | iteration 60 / 171 | Total Loss: 2.5841386318206787 | KNN Loss: 2.478217601776123 | CLS Loss: 0.1059209555387497\n",
      "Epoch 5 / 200 | iteration 70 / 171 | Total Loss: 2.602154493331909 | KNN Loss: 2.450728416442871 | CLS Loss: 0.1514260321855545\n",
      "Epoch 5 / 200 | iteration 80 / 171 | Total Loss: 2.5890777111053467 | KNN Loss: 2.4560892581939697 | CLS Loss: 0.13298846781253815\n",
      "Epoch 5 / 200 | iteration 90 / 171 | Total Loss: 2.6139087677001953 | KNN Loss: 2.497290849685669 | CLS Loss: 0.11661797016859055\n",
      "Epoch 5 / 200 | iteration 100 / 171 | Total Loss: 2.621500015258789 | KNN Loss: 2.4384984970092773 | CLS Loss: 0.18300144374370575\n",
      "Epoch 5 / 200 | iteration 110 / 171 | Total Loss: 2.6160953044891357 | KNN Loss: 2.4576151371002197 | CLS Loss: 0.15848009288311005\n",
      "Epoch 5 / 200 | iteration 120 / 171 | Total Loss: 2.5685205459594727 | KNN Loss: 2.4568569660186768 | CLS Loss: 0.11166352033615112\n",
      "Epoch 5 / 200 | iteration 130 / 171 | Total Loss: 2.5511515140533447 | KNN Loss: 2.435931444168091 | CLS Loss: 0.11521996557712555\n",
      "Epoch 5 / 200 | iteration 140 / 171 | Total Loss: 2.5697033405303955 | KNN Loss: 2.427133321762085 | CLS Loss: 0.14257004857063293\n",
      "Epoch 5 / 200 | iteration 150 / 171 | Total Loss: 2.553434371948242 | KNN Loss: 2.4460387229919434 | CLS Loss: 0.10739573836326599\n",
      "Epoch 5 / 200 | iteration 160 / 171 | Total Loss: 2.564152240753174 | KNN Loss: 2.4877724647521973 | CLS Loss: 0.07637985050678253\n",
      "Epoch 5 / 200 | iteration 170 / 171 | Total Loss: 2.583681106567383 | KNN Loss: 2.4594178199768066 | CLS Loss: 0.12426330149173737\n",
      "Epoch: 005, Loss: 2.5851, Train: 0.9680, Valid: 0.9658, Best: 0.9659\n",
      "Epoch 6 / 200 | iteration 0 / 171 | Total Loss: 2.5919501781463623 | KNN Loss: 2.4928150177001953 | CLS Loss: 0.09913520514965057\n",
      "Epoch 6 / 200 | iteration 10 / 171 | Total Loss: 2.5968363285064697 | KNN Loss: 2.4416286945343018 | CLS Loss: 0.15520769357681274\n",
      "Epoch 6 / 200 | iteration 20 / 171 | Total Loss: 2.5620391368865967 | KNN Loss: 2.438302516937256 | CLS Loss: 0.12373662739992142\n",
      "Epoch 6 / 200 | iteration 30 / 171 | Total Loss: 2.529824733734131 | KNN Loss: 2.422750234603882 | CLS Loss: 0.10707461088895798\n",
      "Epoch 6 / 200 | iteration 40 / 171 | Total Loss: 2.54602313041687 | KNN Loss: 2.397899866104126 | CLS Loss: 0.1481233537197113\n",
      "Epoch 6 / 200 | iteration 50 / 171 | Total Loss: 2.548680305480957 | KNN Loss: 2.437371015548706 | CLS Loss: 0.1113092452287674\n",
      "Epoch 6 / 200 | iteration 60 / 171 | Total Loss: 2.5461063385009766 | KNN Loss: 2.4224417209625244 | CLS Loss: 0.12366464734077454\n",
      "Epoch 6 / 200 | iteration 70 / 171 | Total Loss: 2.499809503555298 | KNN Loss: 2.430621385574341 | CLS Loss: 0.06918812543153763\n",
      "Epoch 6 / 200 | iteration 80 / 171 | Total Loss: 2.5339455604553223 | KNN Loss: 2.461826801300049 | CLS Loss: 0.07211875915527344\n",
      "Epoch 6 / 200 | iteration 90 / 171 | Total Loss: 2.552915573120117 | KNN Loss: 2.4651970863342285 | CLS Loss: 0.08771850913763046\n",
      "Epoch 6 / 200 | iteration 100 / 171 | Total Loss: 2.5523436069488525 | KNN Loss: 2.4785284996032715 | CLS Loss: 0.07381504774093628\n",
      "Epoch 6 / 200 | iteration 110 / 171 | Total Loss: 2.5336685180664062 | KNN Loss: 2.4228429794311523 | CLS Loss: 0.11082547903060913\n",
      "Epoch 6 / 200 | iteration 120 / 171 | Total Loss: 2.551476001739502 | KNN Loss: 2.4578394889831543 | CLS Loss: 0.09363657981157303\n",
      "Epoch 6 / 200 | iteration 130 / 171 | Total Loss: 2.5573863983154297 | KNN Loss: 2.4386067390441895 | CLS Loss: 0.11877956986427307\n",
      "Epoch 6 / 200 | iteration 140 / 171 | Total Loss: 2.575106143951416 | KNN Loss: 2.474869728088379 | CLS Loss: 0.10023637861013412\n",
      "Epoch 6 / 200 | iteration 150 / 171 | Total Loss: 2.5581912994384766 | KNN Loss: 2.4558486938476562 | CLS Loss: 0.10234253853559494\n",
      "Epoch 6 / 200 | iteration 160 / 171 | Total Loss: 2.5479793548583984 | KNN Loss: 2.4439070224761963 | CLS Loss: 0.10407224297523499\n",
      "Epoch 6 / 200 | iteration 170 / 171 | Total Loss: 2.561052083969116 | KNN Loss: 2.4523048400878906 | CLS Loss: 0.10874728858470917\n",
      "Epoch: 006, Loss: 2.5596, Train: 0.9749, Valid: 0.9725, Best: 0.9725\n",
      "Epoch 7 / 200 | iteration 0 / 171 | Total Loss: 2.532932758331299 | KNN Loss: 2.4780256748199463 | CLS Loss: 0.05490715056657791\n",
      "Epoch 7 / 200 | iteration 10 / 171 | Total Loss: 2.5228304862976074 | KNN Loss: 2.434561252593994 | CLS Loss: 0.08826931565999985\n",
      "Epoch 7 / 200 | iteration 20 / 171 | Total Loss: 2.5960099697113037 | KNN Loss: 2.4757659435272217 | CLS Loss: 0.12024400383234024\n",
      "Epoch 7 / 200 | iteration 30 / 171 | Total Loss: 2.506837844848633 | KNN Loss: 2.4170310497283936 | CLS Loss: 0.08980680257081985\n",
      "Epoch 7 / 200 | iteration 40 / 171 | Total Loss: 2.533097743988037 | KNN Loss: 2.425604820251465 | CLS Loss: 0.10749295353889465\n",
      "Epoch 7 / 200 | iteration 50 / 171 | Total Loss: 2.5360960960388184 | KNN Loss: 2.4490597248077393 | CLS Loss: 0.08703642338514328\n",
      "Epoch 7 / 200 | iteration 60 / 171 | Total Loss: 2.5297112464904785 | KNN Loss: 2.437307357788086 | CLS Loss: 0.09240376949310303\n",
      "Epoch 7 / 200 | iteration 70 / 171 | Total Loss: 2.535423755645752 | KNN Loss: 2.4379961490631104 | CLS Loss: 0.09742768853902817\n",
      "Epoch 7 / 200 | iteration 80 / 171 | Total Loss: 2.519324541091919 | KNN Loss: 2.3966121673583984 | CLS Loss: 0.12271247804164886\n",
      "Epoch 7 / 200 | iteration 90 / 171 | Total Loss: 2.5692827701568604 | KNN Loss: 2.4482228755950928 | CLS Loss: 0.12105992436408997\n",
      "Epoch 7 / 200 | iteration 100 / 171 | Total Loss: 2.5774269104003906 | KNN Loss: 2.478557586669922 | CLS Loss: 0.09886927157640457\n",
      "Epoch 7 / 200 | iteration 110 / 171 | Total Loss: 2.560730457305908 | KNN Loss: 2.4598591327667236 | CLS Loss: 0.10087137669324875\n",
      "Epoch 7 / 200 | iteration 120 / 171 | Total Loss: 2.5240840911865234 | KNN Loss: 2.4296088218688965 | CLS Loss: 0.09447534382343292\n",
      "Epoch 7 / 200 | iteration 130 / 171 | Total Loss: 2.498429775238037 | KNN Loss: 2.425858497619629 | CLS Loss: 0.07257130742073059\n",
      "Epoch 7 / 200 | iteration 140 / 171 | Total Loss: 2.510537624359131 | KNN Loss: 2.402961492538452 | CLS Loss: 0.10757621377706528\n",
      "Epoch 7 / 200 | iteration 150 / 171 | Total Loss: 2.5254323482513428 | KNN Loss: 2.3993473052978516 | CLS Loss: 0.126085102558136\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 / 200 | iteration 160 / 171 | Total Loss: 2.5354063510894775 | KNN Loss: 2.4283711910247803 | CLS Loss: 0.10703523457050323\n",
      "Epoch 7 / 200 | iteration 170 / 171 | Total Loss: 2.580207347869873 | KNN Loss: 2.476850986480713 | CLS Loss: 0.10335638374090195\n",
      "Epoch: 007, Loss: 2.5394, Train: 0.9768, Valid: 0.9744, Best: 0.9744\n",
      "Epoch 8 / 200 | iteration 0 / 171 | Total Loss: 2.5281362533569336 | KNN Loss: 2.450173854827881 | CLS Loss: 0.0779624953866005\n",
      "Epoch 8 / 200 | iteration 10 / 171 | Total Loss: 2.50212025642395 | KNN Loss: 2.4447734355926514 | CLS Loss: 0.05734681338071823\n",
      "Epoch 8 / 200 | iteration 20 / 171 | Total Loss: 2.564859628677368 | KNN Loss: 2.468865394592285 | CLS Loss: 0.09599415212869644\n",
      "Epoch 8 / 200 | iteration 30 / 171 | Total Loss: 2.560581684112549 | KNN Loss: 2.4350554943084717 | CLS Loss: 0.12552624940872192\n",
      "Epoch 8 / 200 | iteration 40 / 171 | Total Loss: 2.508180618286133 | KNN Loss: 2.434363842010498 | CLS Loss: 0.07381674647331238\n",
      "Epoch 8 / 200 | iteration 50 / 171 | Total Loss: 2.4813125133514404 | KNN Loss: 2.4088656902313232 | CLS Loss: 0.07244693487882614\n",
      "Epoch 8 / 200 | iteration 60 / 171 | Total Loss: 2.489614486694336 | KNN Loss: 2.368201971054077 | CLS Loss: 0.12141246348619461\n",
      "Epoch 8 / 200 | iteration 70 / 171 | Total Loss: 2.5435638427734375 | KNN Loss: 2.4256937503814697 | CLS Loss: 0.11787000298500061\n",
      "Epoch 8 / 200 | iteration 80 / 171 | Total Loss: 2.5765035152435303 | KNN Loss: 2.435276985168457 | CLS Loss: 0.14122658967971802\n",
      "Epoch 8 / 200 | iteration 90 / 171 | Total Loss: 2.566815137863159 | KNN Loss: 2.480999231338501 | CLS Loss: 0.08581594377756119\n",
      "Epoch 8 / 200 | iteration 100 / 171 | Total Loss: 2.4726855754852295 | KNN Loss: 2.397160291671753 | CLS Loss: 0.07552537322044373\n",
      "Epoch 8 / 200 | iteration 110 / 171 | Total Loss: 2.5794224739074707 | KNN Loss: 2.482306480407715 | CLS Loss: 0.09711587429046631\n",
      "Epoch 8 / 200 | iteration 120 / 171 | Total Loss: 2.4999399185180664 | KNN Loss: 2.409517526626587 | CLS Loss: 0.09042242169380188\n",
      "Epoch 8 / 200 | iteration 130 / 171 | Total Loss: 2.4929816722869873 | KNN Loss: 2.419877529144287 | CLS Loss: 0.07310409843921661\n",
      "Epoch 8 / 200 | iteration 140 / 171 | Total Loss: 2.506988525390625 | KNN Loss: 2.4176244735717773 | CLS Loss: 0.08936399966478348\n",
      "Epoch 8 / 200 | iteration 150 / 171 | Total Loss: 2.5304760932922363 | KNN Loss: 2.4477157592773438 | CLS Loss: 0.08276043087244034\n",
      "Epoch 8 / 200 | iteration 160 / 171 | Total Loss: 2.513549566268921 | KNN Loss: 2.369527816772461 | CLS Loss: 0.14402171969413757\n",
      "Epoch 8 / 200 | iteration 170 / 171 | Total Loss: 2.5212972164154053 | KNN Loss: 2.434584856033325 | CLS Loss: 0.08671233057975769\n",
      "Epoch: 008, Loss: 2.5222, Train: 0.9789, Valid: 0.9759, Best: 0.9759\n",
      "Epoch 9 / 200 | iteration 0 / 171 | Total Loss: 2.5048413276672363 | KNN Loss: 2.3979294300079346 | CLS Loss: 0.1069117933511734\n",
      "Epoch 9 / 200 | iteration 10 / 171 | Total Loss: 2.539245843887329 | KNN Loss: 2.4336938858032227 | CLS Loss: 0.10555187612771988\n",
      "Epoch 9 / 200 | iteration 20 / 171 | Total Loss: 2.4874796867370605 | KNN Loss: 2.395386219024658 | CLS Loss: 0.09209346026182175\n",
      "Epoch 9 / 200 | iteration 30 / 171 | Total Loss: 2.5350937843322754 | KNN Loss: 2.413297653198242 | CLS Loss: 0.12179618328809738\n",
      "Epoch 9 / 200 | iteration 40 / 171 | Total Loss: 2.5044684410095215 | KNN Loss: 2.402557134628296 | CLS Loss: 0.10191129893064499\n",
      "Epoch 9 / 200 | iteration 50 / 171 | Total Loss: 2.5044703483581543 | KNN Loss: 2.4087042808532715 | CLS Loss: 0.09576601535081863\n",
      "Epoch 9 / 200 | iteration 60 / 171 | Total Loss: 2.5109446048736572 | KNN Loss: 2.4278926849365234 | CLS Loss: 0.08305181562900543\n",
      "Epoch 9 / 200 | iteration 70 / 171 | Total Loss: 2.4544599056243896 | KNN Loss: 2.401313543319702 | CLS Loss: 0.05314628407359123\n",
      "Epoch 9 / 200 | iteration 80 / 171 | Total Loss: 2.525056838989258 | KNN Loss: 2.406187057495117 | CLS Loss: 0.11886971443891525\n",
      "Epoch 9 / 200 | iteration 90 / 171 | Total Loss: 2.4874353408813477 | KNN Loss: 2.4079654216766357 | CLS Loss: 0.07946997135877609\n",
      "Epoch 9 / 200 | iteration 100 / 171 | Total Loss: 2.512744665145874 | KNN Loss: 2.4045162200927734 | CLS Loss: 0.10822837799787521\n",
      "Epoch 9 / 200 | iteration 110 / 171 | Total Loss: 2.4880118370056152 | KNN Loss: 2.4141745567321777 | CLS Loss: 0.07383719831705093\n",
      "Epoch 9 / 200 | iteration 120 / 171 | Total Loss: 2.470538377761841 | KNN Loss: 2.398582696914673 | CLS Loss: 0.07195567339658737\n",
      "Epoch 9 / 200 | iteration 130 / 171 | Total Loss: 2.5393779277801514 | KNN Loss: 2.4252607822418213 | CLS Loss: 0.11411719769239426\n",
      "Epoch 9 / 200 | iteration 140 / 171 | Total Loss: 2.5120887756347656 | KNN Loss: 2.42647123336792 | CLS Loss: 0.08561764657497406\n",
      "Epoch 9 / 200 | iteration 150 / 171 | Total Loss: 2.531351089477539 | KNN Loss: 2.403721809387207 | CLS Loss: 0.12762917578220367\n",
      "Epoch 9 / 200 | iteration 160 / 171 | Total Loss: 2.4879391193389893 | KNN Loss: 2.420921802520752 | CLS Loss: 0.06701739132404327\n",
      "Epoch 9 / 200 | iteration 170 / 171 | Total Loss: 2.511192560195923 | KNN Loss: 2.4103333950042725 | CLS Loss: 0.10085926949977875\n",
      "Epoch: 009, Loss: 2.5043, Train: 0.9796, Valid: 0.9767, Best: 0.9767\n",
      "Epoch 10 / 200 | iteration 0 / 171 | Total Loss: 2.5331602096557617 | KNN Loss: 2.4436147212982178 | CLS Loss: 0.08954540640115738\n",
      "Epoch 10 / 200 | iteration 10 / 171 | Total Loss: 2.4822113513946533 | KNN Loss: 2.4145326614379883 | CLS Loss: 0.06767871230840683\n",
      "Epoch 10 / 200 | iteration 20 / 171 | Total Loss: 2.512674570083618 | KNN Loss: 2.434823989868164 | CLS Loss: 0.07785052806138992\n",
      "Epoch 10 / 200 | iteration 30 / 171 | Total Loss: 2.52897047996521 | KNN Loss: 2.4387643337249756 | CLS Loss: 0.09020620584487915\n",
      "Epoch 10 / 200 | iteration 40 / 171 | Total Loss: 2.5023891925811768 | KNN Loss: 2.3731062412261963 | CLS Loss: 0.12928296625614166\n",
      "Epoch 10 / 200 | iteration 50 / 171 | Total Loss: 2.492326021194458 | KNN Loss: 2.3910129070281982 | CLS Loss: 0.10131318867206573\n",
      "Epoch 10 / 200 | iteration 60 / 171 | Total Loss: 2.5037975311279297 | KNN Loss: 2.344163417816162 | CLS Loss: 0.15963420271873474\n",
      "Epoch 10 / 200 | iteration 70 / 171 | Total Loss: 2.4784228801727295 | KNN Loss: 2.3915984630584717 | CLS Loss: 0.08682449907064438\n",
      "Epoch 10 / 200 | iteration 80 / 171 | Total Loss: 2.4906792640686035 | KNN Loss: 2.407576322555542 | CLS Loss: 0.0831029936671257\n",
      "Epoch 10 / 200 | iteration 90 / 171 | Total Loss: 2.510617971420288 | KNN Loss: 2.4306223392486572 | CLS Loss: 0.07999564707279205\n",
      "Epoch 10 / 200 | iteration 100 / 171 | Total Loss: 2.52715802192688 | KNN Loss: 2.4142637252807617 | CLS Loss: 0.11289438605308533\n",
      "Epoch 10 / 200 | iteration 110 / 171 | Total Loss: 2.5147500038146973 | KNN Loss: 2.3961880207061768 | CLS Loss: 0.11856191605329514\n",
      "Epoch 10 / 200 | iteration 120 / 171 | Total Loss: 2.5189754962921143 | KNN Loss: 2.468923807144165 | CLS Loss: 0.05005168914794922\n",
      "Epoch 10 / 200 | iteration 130 / 171 | Total Loss: 2.513556718826294 | KNN Loss: 2.4333980083465576 | CLS Loss: 0.0801587924361229\n",
      "Epoch 10 / 200 | iteration 140 / 171 | Total Loss: 2.489428758621216 | KNN Loss: 2.41312313079834 | CLS Loss: 0.07630571722984314\n",
      "Epoch 10 / 200 | iteration 150 / 171 | Total Loss: 2.4925498962402344 | KNN Loss: 2.4003500938415527 | CLS Loss: 0.09219968318939209\n",
      "Epoch 10 / 200 | iteration 160 / 171 | Total Loss: 2.4864766597747803 | KNN Loss: 2.3951635360717773 | CLS Loss: 0.09131316095590591\n",
      "Epoch 10 / 200 | iteration 170 / 171 | Total Loss: 2.4590871334075928 | KNN Loss: 2.379326820373535 | CLS Loss: 0.07976042479276657\n",
      "Epoch: 010, Loss: 2.4996, Train: 0.9795, Valid: 0.9768, Best: 0.9768\n",
      "Epoch 11 / 200 | iteration 0 / 171 | Total Loss: 2.47971773147583 | KNN Loss: 2.420320510864258 | CLS Loss: 0.05939726158976555\n",
      "Epoch 11 / 200 | iteration 10 / 171 | Total Loss: 2.458080291748047 | KNN Loss: 2.3548331260681152 | CLS Loss: 0.10324709862470627\n",
      "Epoch 11 / 200 | iteration 20 / 171 | Total Loss: 2.486572742462158 | KNN Loss: 2.3878307342529297 | CLS Loss: 0.09874199330806732\n",
      "Epoch 11 / 200 | iteration 30 / 171 | Total Loss: 2.499058485031128 | KNN Loss: 2.405134439468384 | CLS Loss: 0.09392410516738892\n",
      "Epoch 11 / 200 | iteration 40 / 171 | Total Loss: 2.528928279876709 | KNN Loss: 2.43758487701416 | CLS Loss: 0.09134328365325928\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 / 200 | iteration 50 / 171 | Total Loss: 2.516437292098999 | KNN Loss: 2.4461705684661865 | CLS Loss: 0.07026666402816772\n",
      "Epoch 11 / 200 | iteration 60 / 171 | Total Loss: 2.5064377784729004 | KNN Loss: 2.4180960655212402 | CLS Loss: 0.08834180235862732\n",
      "Epoch 11 / 200 | iteration 70 / 171 | Total Loss: 2.5585741996765137 | KNN Loss: 2.4586586952209473 | CLS Loss: 0.0999155044555664\n",
      "Epoch 11 / 200 | iteration 80 / 171 | Total Loss: 2.5331742763519287 | KNN Loss: 2.415907859802246 | CLS Loss: 0.11726631969213486\n",
      "Epoch 11 / 200 | iteration 90 / 171 | Total Loss: 2.494556188583374 | KNN Loss: 2.416490077972412 | CLS Loss: 0.07806612551212311\n",
      "Epoch 11 / 200 | iteration 100 / 171 | Total Loss: 2.5526835918426514 | KNN Loss: 2.4261972904205322 | CLS Loss: 0.12648634612560272\n",
      "Epoch 11 / 200 | iteration 110 / 171 | Total Loss: 2.509809732437134 | KNN Loss: 2.426271677017212 | CLS Loss: 0.08353807032108307\n",
      "Epoch 11 / 200 | iteration 120 / 171 | Total Loss: 2.495009660720825 | KNN Loss: 2.417149543762207 | CLS Loss: 0.07786018401384354\n",
      "Epoch 11 / 200 | iteration 130 / 171 | Total Loss: 2.4527499675750732 | KNN Loss: 2.3612475395202637 | CLS Loss: 0.09150251001119614\n",
      "Epoch 11 / 200 | iteration 140 / 171 | Total Loss: 2.483511447906494 | KNN Loss: 2.426624298095703 | CLS Loss: 0.056887105107307434\n",
      "Epoch 11 / 200 | iteration 150 / 171 | Total Loss: 2.5546326637268066 | KNN Loss: 2.4513285160064697 | CLS Loss: 0.10330415517091751\n",
      "Epoch 11 / 200 | iteration 160 / 171 | Total Loss: 2.5092129707336426 | KNN Loss: 2.402930498123169 | CLS Loss: 0.10628259181976318\n",
      "Epoch 11 / 200 | iteration 170 / 171 | Total Loss: 2.558161735534668 | KNN Loss: 2.406890392303467 | CLS Loss: 0.151271253824234\n",
      "Epoch: 011, Loss: 2.4952, Train: 0.9808, Valid: 0.9783, Best: 0.9783\n",
      "Epoch 12 / 200 | iteration 0 / 171 | Total Loss: 2.462474822998047 | KNN Loss: 2.3884739875793457 | CLS Loss: 0.07400083541870117\n",
      "Epoch 12 / 200 | iteration 10 / 171 | Total Loss: 2.47794508934021 | KNN Loss: 2.4128904342651367 | CLS Loss: 0.06505471467971802\n",
      "Epoch 12 / 200 | iteration 20 / 171 | Total Loss: 2.481715679168701 | KNN Loss: 2.4345932006835938 | CLS Loss: 0.04712243005633354\n",
      "Epoch 12 / 200 | iteration 30 / 171 | Total Loss: 2.487147092819214 | KNN Loss: 2.3576269149780273 | CLS Loss: 0.1295202523469925\n",
      "Epoch 12 / 200 | iteration 40 / 171 | Total Loss: 2.478238105773926 | KNN Loss: 2.414172649383545 | CLS Loss: 0.0640653520822525\n",
      "Epoch 12 / 200 | iteration 50 / 171 | Total Loss: 2.532086133956909 | KNN Loss: 2.4258739948272705 | CLS Loss: 0.10621218383312225\n",
      "Epoch 12 / 200 | iteration 60 / 171 | Total Loss: 2.4723098278045654 | KNN Loss: 2.3986754417419434 | CLS Loss: 0.07363443076610565\n",
      "Epoch 12 / 200 | iteration 70 / 171 | Total Loss: 2.4662888050079346 | KNN Loss: 2.4042797088623047 | CLS Loss: 0.06200909614562988\n",
      "Epoch 12 / 200 | iteration 80 / 171 | Total Loss: 2.4424991607666016 | KNN Loss: 2.3939380645751953 | CLS Loss: 0.048561062663793564\n",
      "Epoch 12 / 200 | iteration 90 / 171 | Total Loss: 2.4976627826690674 | KNN Loss: 2.4153525829315186 | CLS Loss: 0.08231021463871002\n",
      "Epoch 12 / 200 | iteration 100 / 171 | Total Loss: 2.4952542781829834 | KNN Loss: 2.4461357593536377 | CLS Loss: 0.049118444323539734\n",
      "Epoch 12 / 200 | iteration 110 / 171 | Total Loss: 2.5016531944274902 | KNN Loss: 2.4236533641815186 | CLS Loss: 0.07799986749887466\n",
      "Epoch 12 / 200 | iteration 120 / 171 | Total Loss: 2.4836950302124023 | KNN Loss: 2.4369728565216064 | CLS Loss: 0.046722233295440674\n",
      "Epoch 12 / 200 | iteration 130 / 171 | Total Loss: 2.4862873554229736 | KNN Loss: 2.4053139686584473 | CLS Loss: 0.08097349107265472\n",
      "Epoch 12 / 200 | iteration 140 / 171 | Total Loss: 2.5037622451782227 | KNN Loss: 2.443836212158203 | CLS Loss: 0.05992592126131058\n",
      "Epoch 12 / 200 | iteration 150 / 171 | Total Loss: 2.4415786266326904 | KNN Loss: 2.360530376434326 | CLS Loss: 0.08104818314313889\n",
      "Epoch 12 / 200 | iteration 160 / 171 | Total Loss: 2.4865548610687256 | KNN Loss: 2.403733015060425 | CLS Loss: 0.0828218087553978\n",
      "Epoch 12 / 200 | iteration 170 / 171 | Total Loss: 2.479574680328369 | KNN Loss: 2.4063384532928467 | CLS Loss: 0.07323624193668365\n",
      "Epoch: 012, Loss: 2.4876, Train: 0.9806, Valid: 0.9779, Best: 0.9783\n",
      "Epoch 13 / 200 | iteration 0 / 171 | Total Loss: 2.4287235736846924 | KNN Loss: 2.3654189109802246 | CLS Loss: 0.06330475211143494\n",
      "Epoch 13 / 200 | iteration 10 / 171 | Total Loss: 2.4680564403533936 | KNN Loss: 2.3890292644500732 | CLS Loss: 0.07902727276086807\n",
      "Epoch 13 / 200 | iteration 20 / 171 | Total Loss: 2.4668304920196533 | KNN Loss: 2.399685859680176 | CLS Loss: 0.06714474409818649\n",
      "Epoch 13 / 200 | iteration 30 / 171 | Total Loss: 2.4709699153900146 | KNN Loss: 2.403531789779663 | CLS Loss: 0.06743815541267395\n",
      "Epoch 13 / 200 | iteration 40 / 171 | Total Loss: 2.4727609157562256 | KNN Loss: 2.3980460166931152 | CLS Loss: 0.07471484690904617\n",
      "Epoch 13 / 200 | iteration 50 / 171 | Total Loss: 2.4980287551879883 | KNN Loss: 2.426060438156128 | CLS Loss: 0.0719681978225708\n",
      "Epoch 13 / 200 | iteration 60 / 171 | Total Loss: 2.4974591732025146 | KNN Loss: 2.4012081623077393 | CLS Loss: 0.09625105559825897\n",
      "Epoch 13 / 200 | iteration 70 / 171 | Total Loss: 2.4700636863708496 | KNN Loss: 2.3910844326019287 | CLS Loss: 0.07897915691137314\n",
      "Epoch 13 / 200 | iteration 80 / 171 | Total Loss: 2.479259967803955 | KNN Loss: 2.399357318878174 | CLS Loss: 0.07990260422229767\n",
      "Epoch 13 / 200 | iteration 90 / 171 | Total Loss: 2.4738895893096924 | KNN Loss: 2.4118738174438477 | CLS Loss: 0.06201581284403801\n",
      "Epoch 13 / 200 | iteration 100 / 171 | Total Loss: 2.4653096199035645 | KNN Loss: 2.4292409420013428 | CLS Loss: 0.0360686294734478\n",
      "Epoch 13 / 200 | iteration 110 / 171 | Total Loss: 2.511849880218506 | KNN Loss: 2.440573215484619 | CLS Loss: 0.07127659767866135\n",
      "Epoch 13 / 200 | iteration 120 / 171 | Total Loss: 2.4586808681488037 | KNN Loss: 2.4128873348236084 | CLS Loss: 0.045793432742357254\n",
      "Epoch 13 / 200 | iteration 130 / 171 | Total Loss: 2.454935312271118 | KNN Loss: 2.3945391178131104 | CLS Loss: 0.06039614602923393\n",
      "Epoch 13 / 200 | iteration 140 / 171 | Total Loss: 2.469102382659912 | KNN Loss: 2.399489164352417 | CLS Loss: 0.06961319595575333\n",
      "Epoch 13 / 200 | iteration 150 / 171 | Total Loss: 2.5204954147338867 | KNN Loss: 2.397019624710083 | CLS Loss: 0.12347590923309326\n",
      "Epoch 13 / 200 | iteration 160 / 171 | Total Loss: 2.499281167984009 | KNN Loss: 2.4282121658325195 | CLS Loss: 0.07106909900903702\n",
      "Epoch 13 / 200 | iteration 170 / 171 | Total Loss: 2.4971463680267334 | KNN Loss: 2.4393951892852783 | CLS Loss: 0.057751286774873734\n",
      "Epoch: 013, Loss: 2.4797, Train: 0.9815, Valid: 0.9787, Best: 0.9787\n",
      "Epoch 14 / 200 | iteration 0 / 171 | Total Loss: 2.472374200820923 | KNN Loss: 2.421257734298706 | CLS Loss: 0.05111643671989441\n",
      "Epoch 14 / 200 | iteration 10 / 171 | Total Loss: 2.496643543243408 | KNN Loss: 2.4442591667175293 | CLS Loss: 0.052384357899427414\n",
      "Epoch 14 / 200 | iteration 20 / 171 | Total Loss: 2.4584550857543945 | KNN Loss: 2.352560520172119 | CLS Loss: 0.10589467734098434\n",
      "Epoch 14 / 200 | iteration 30 / 171 | Total Loss: 2.442643404006958 | KNN Loss: 2.383291006088257 | CLS Loss: 0.059352412819862366\n",
      "Epoch 14 / 200 | iteration 40 / 171 | Total Loss: 2.510019540786743 | KNN Loss: 2.417160749435425 | CLS Loss: 0.0928586795926094\n",
      "Epoch 14 / 200 | iteration 50 / 171 | Total Loss: 2.4810068607330322 | KNN Loss: 2.384549379348755 | CLS Loss: 0.09645741432905197\n",
      "Epoch 14 / 200 | iteration 60 / 171 | Total Loss: 2.460644483566284 | KNN Loss: 2.39607310295105 | CLS Loss: 0.0645713359117508\n",
      "Epoch 14 / 200 | iteration 70 / 171 | Total Loss: 2.5324251651763916 | KNN Loss: 2.4583163261413574 | CLS Loss: 0.07410888373851776\n",
      "Epoch 14 / 200 | iteration 80 / 171 | Total Loss: 2.4822094440460205 | KNN Loss: 2.364496946334839 | CLS Loss: 0.11771252751350403\n",
      "Epoch 14 / 200 | iteration 90 / 171 | Total Loss: 2.4684250354766846 | KNN Loss: 2.3847384452819824 | CLS Loss: 0.08368657529354095\n",
      "Epoch 14 / 200 | iteration 100 / 171 | Total Loss: 2.4342029094696045 | KNN Loss: 2.3700661659240723 | CLS Loss: 0.06413672864437103\n",
      "Epoch 14 / 200 | iteration 110 / 171 | Total Loss: 2.5002806186676025 | KNN Loss: 2.4167988300323486 | CLS Loss: 0.08348184823989868\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 / 200 | iteration 120 / 171 | Total Loss: 2.502730131149292 | KNN Loss: 2.3985047340393066 | CLS Loss: 0.10422539710998535\n",
      "Epoch 14 / 200 | iteration 130 / 171 | Total Loss: 2.501847505569458 | KNN Loss: 2.415691375732422 | CLS Loss: 0.08615612238645554\n",
      "Epoch 14 / 200 | iteration 140 / 171 | Total Loss: 2.4611289501190186 | KNN Loss: 2.4113149642944336 | CLS Loss: 0.04981403425335884\n",
      "Epoch 14 / 200 | iteration 150 / 171 | Total Loss: 2.4937100410461426 | KNN Loss: 2.424264669418335 | CLS Loss: 0.06944538652896881\n",
      "Epoch 14 / 200 | iteration 160 / 171 | Total Loss: 2.4458136558532715 | KNN Loss: 2.397916078567505 | CLS Loss: 0.04789765551686287\n",
      "Epoch 14 / 200 | iteration 170 / 171 | Total Loss: 2.480311870574951 | KNN Loss: 2.4273149967193604 | CLS Loss: 0.05299697443842888\n",
      "Epoch: 014, Loss: 2.4819, Train: 0.9803, Valid: 0.9776, Best: 0.9787\n",
      "Epoch 15 / 200 | iteration 0 / 171 | Total Loss: 2.5016870498657227 | KNN Loss: 2.434678316116333 | CLS Loss: 0.06700881570577621\n",
      "Epoch 15 / 200 | iteration 10 / 171 | Total Loss: 2.4539196491241455 | KNN Loss: 2.399921417236328 | CLS Loss: 0.05399821326136589\n",
      "Epoch 15 / 200 | iteration 20 / 171 | Total Loss: 2.520871639251709 | KNN Loss: 2.438377857208252 | CLS Loss: 0.08249370008707047\n",
      "Epoch 15 / 200 | iteration 30 / 171 | Total Loss: 2.4591472148895264 | KNN Loss: 2.400059938430786 | CLS Loss: 0.05908723175525665\n",
      "Epoch 15 / 200 | iteration 40 / 171 | Total Loss: 2.4399261474609375 | KNN Loss: 2.386726140975952 | CLS Loss: 0.05320001021027565\n",
      "Epoch 15 / 200 | iteration 50 / 171 | Total Loss: 2.468245029449463 | KNN Loss: 2.4256296157836914 | CLS Loss: 0.042615462094545364\n",
      "Epoch 15 / 200 | iteration 60 / 171 | Total Loss: 2.4975805282592773 | KNN Loss: 2.4153079986572266 | CLS Loss: 0.08227258920669556\n",
      "Epoch 15 / 200 | iteration 70 / 171 | Total Loss: 2.4917757511138916 | KNN Loss: 2.3844668865203857 | CLS Loss: 0.10730881989002228\n",
      "Epoch 15 / 200 | iteration 80 / 171 | Total Loss: 2.439579963684082 | KNN Loss: 2.3807804584503174 | CLS Loss: 0.058799561113119125\n",
      "Epoch 15 / 200 | iteration 90 / 171 | Total Loss: 2.5177793502807617 | KNN Loss: 2.4334332942962646 | CLS Loss: 0.08434594422578812\n",
      "Epoch 15 / 200 | iteration 100 / 171 | Total Loss: 2.491097927093506 | KNN Loss: 2.3805699348449707 | CLS Loss: 0.11052791774272919\n",
      "Epoch 15 / 200 | iteration 110 / 171 | Total Loss: 2.473940134048462 | KNN Loss: 2.419382333755493 | CLS Loss: 0.054557688534259796\n",
      "Epoch 15 / 200 | iteration 120 / 171 | Total Loss: 2.4496119022369385 | KNN Loss: 2.3698582649230957 | CLS Loss: 0.07975362986326218\n",
      "Epoch 15 / 200 | iteration 130 / 171 | Total Loss: 2.45192289352417 | KNN Loss: 2.403890371322632 | CLS Loss: 0.048032574355602264\n",
      "Epoch 15 / 200 | iteration 140 / 171 | Total Loss: 2.423783540725708 | KNN Loss: 2.353480100631714 | CLS Loss: 0.07030332833528519\n",
      "Epoch 15 / 200 | iteration 150 / 171 | Total Loss: 2.4722952842712402 | KNN Loss: 2.3817384243011475 | CLS Loss: 0.09055674076080322\n",
      "Epoch 15 / 200 | iteration 160 / 171 | Total Loss: 2.4335978031158447 | KNN Loss: 2.4067459106445312 | CLS Loss: 0.026851801201701164\n",
      "Epoch 15 / 200 | iteration 170 / 171 | Total Loss: 2.509549617767334 | KNN Loss: 2.4270668029785156 | CLS Loss: 0.0824827328324318\n",
      "Epoch: 015, Loss: 2.4765, Train: 0.9834, Valid: 0.9803, Best: 0.9803\n",
      "Epoch 16 / 200 | iteration 0 / 171 | Total Loss: 2.466728925704956 | KNN Loss: 2.4251155853271484 | CLS Loss: 0.041613396257162094\n",
      "Epoch 16 / 200 | iteration 10 / 171 | Total Loss: 2.44003963470459 | KNN Loss: 2.3907063007354736 | CLS Loss: 0.04933343827724457\n",
      "Epoch 16 / 200 | iteration 20 / 171 | Total Loss: 2.4557905197143555 | KNN Loss: 2.415916681289673 | CLS Loss: 0.0398738868534565\n",
      "Epoch 16 / 200 | iteration 30 / 171 | Total Loss: 2.5180068016052246 | KNN Loss: 2.466735601425171 | CLS Loss: 0.05127120018005371\n",
      "Epoch 16 / 200 | iteration 40 / 171 | Total Loss: 2.5020830631256104 | KNN Loss: 2.3814432621002197 | CLS Loss: 0.12063989043235779\n",
      "Epoch 16 / 200 | iteration 50 / 171 | Total Loss: 2.517284870147705 | KNN Loss: 2.427576780319214 | CLS Loss: 0.08970800042152405\n",
      "Epoch 16 / 200 | iteration 60 / 171 | Total Loss: 2.4648101329803467 | KNN Loss: 2.4070663452148438 | CLS Loss: 0.05774381384253502\n",
      "Epoch 16 / 200 | iteration 70 / 171 | Total Loss: 2.4877960681915283 | KNN Loss: 2.4019505977630615 | CLS Loss: 0.08584558218717575\n",
      "Epoch 16 / 200 | iteration 80 / 171 | Total Loss: 2.46048641204834 | KNN Loss: 2.4095890522003174 | CLS Loss: 0.050897400826215744\n",
      "Epoch 16 / 200 | iteration 90 / 171 | Total Loss: 2.456409454345703 | KNN Loss: 2.3950631618499756 | CLS Loss: 0.06134619563817978\n",
      "Epoch 16 / 200 | iteration 100 / 171 | Total Loss: 2.487051010131836 | KNN Loss: 2.4025914669036865 | CLS Loss: 0.0844595655798912\n",
      "Epoch 16 / 200 | iteration 110 / 171 | Total Loss: 2.4888672828674316 | KNN Loss: 2.4475085735321045 | CLS Loss: 0.041358672082424164\n",
      "Epoch 16 / 200 | iteration 120 / 171 | Total Loss: 2.460829496383667 | KNN Loss: 2.398930549621582 | CLS Loss: 0.06189887598156929\n",
      "Epoch 16 / 200 | iteration 130 / 171 | Total Loss: 2.4382436275482178 | KNN Loss: 2.376406669616699 | CLS Loss: 0.06183703988790512\n",
      "Epoch 16 / 200 | iteration 140 / 171 | Total Loss: 2.5154635906219482 | KNN Loss: 2.431797981262207 | CLS Loss: 0.08366554975509644\n",
      "Epoch 16 / 200 | iteration 150 / 171 | Total Loss: 2.438127279281616 | KNN Loss: 2.417825937271118 | CLS Loss: 0.020301252603530884\n",
      "Epoch 16 / 200 | iteration 160 / 171 | Total Loss: 2.485252618789673 | KNN Loss: 2.41178822517395 | CLS Loss: 0.07346449792385101\n",
      "Epoch 16 / 200 | iteration 170 / 171 | Total Loss: 2.452444076538086 | KNN Loss: 2.405261993408203 | CLS Loss: 0.0471821129322052\n",
      "Epoch: 016, Loss: 2.4747, Train: 0.9840, Valid: 0.9808, Best: 0.9808\n",
      "Epoch 17 / 200 | iteration 0 / 171 | Total Loss: 2.4568581581115723 | KNN Loss: 2.386319637298584 | CLS Loss: 0.0705384910106659\n",
      "Epoch 17 / 200 | iteration 10 / 171 | Total Loss: 2.457599639892578 | KNN Loss: 2.3996403217315674 | CLS Loss: 0.05795927345752716\n",
      "Epoch 17 / 200 | iteration 20 / 171 | Total Loss: 2.4677488803863525 | KNN Loss: 2.3729586601257324 | CLS Loss: 0.09479022771120071\n",
      "Epoch 17 / 200 | iteration 30 / 171 | Total Loss: 2.4634976387023926 | KNN Loss: 2.439652919769287 | CLS Loss: 0.023844720795750618\n",
      "Epoch 17 / 200 | iteration 40 / 171 | Total Loss: 2.4887049198150635 | KNN Loss: 2.384119749069214 | CLS Loss: 0.10458507388830185\n",
      "Epoch 17 / 200 | iteration 50 / 171 | Total Loss: 2.4397988319396973 | KNN Loss: 2.354585886001587 | CLS Loss: 0.08521294593811035\n",
      "Epoch 17 / 200 | iteration 60 / 171 | Total Loss: 2.4844040870666504 | KNN Loss: 2.4362285137176514 | CLS Loss: 0.04817567020654678\n",
      "Epoch 17 / 200 | iteration 70 / 171 | Total Loss: 2.468857765197754 | KNN Loss: 2.4053499698638916 | CLS Loss: 0.06350773572921753\n",
      "Epoch 17 / 200 | iteration 80 / 171 | Total Loss: 2.4504427909851074 | KNN Loss: 2.3900399208068848 | CLS Loss: 0.06040282920002937\n",
      "Epoch 17 / 200 | iteration 90 / 171 | Total Loss: 2.451986789703369 | KNN Loss: 2.3769288063049316 | CLS Loss: 0.07505790144205093\n",
      "Epoch 17 / 200 | iteration 100 / 171 | Total Loss: 2.505974054336548 | KNN Loss: 2.4421842098236084 | CLS Loss: 0.06378986686468124\n",
      "Epoch 17 / 200 | iteration 110 / 171 | Total Loss: 2.430159330368042 | KNN Loss: 2.3903095722198486 | CLS Loss: 0.03984970971941948\n",
      "Epoch 17 / 200 | iteration 120 / 171 | Total Loss: 2.4800939559936523 | KNN Loss: 2.4325060844421387 | CLS Loss: 0.04758777469396591\n",
      "Epoch 17 / 200 | iteration 130 / 171 | Total Loss: 2.4621829986572266 | KNN Loss: 2.4132745265960693 | CLS Loss: 0.04890847206115723\n",
      "Epoch 17 / 200 | iteration 140 / 171 | Total Loss: 2.4783318042755127 | KNN Loss: 2.427269697189331 | CLS Loss: 0.051061999052762985\n",
      "Epoch 17 / 200 | iteration 150 / 171 | Total Loss: 2.4455668926239014 | KNN Loss: 2.363585948944092 | CLS Loss: 0.08198104053735733\n",
      "Epoch 17 / 200 | iteration 160 / 171 | Total Loss: 2.5081377029418945 | KNN Loss: 2.4339077472686768 | CLS Loss: 0.07423000037670135\n",
      "Epoch 17 / 200 | iteration 170 / 171 | Total Loss: 2.4523520469665527 | KNN Loss: 2.411900520324707 | CLS Loss: 0.04045162349939346\n",
      "Epoch: 017, Loss: 2.4704, Train: 0.9821, Valid: 0.9788, Best: 0.9808\n",
      "Epoch 18 / 200 | iteration 0 / 171 | Total Loss: 2.473982810974121 | KNN Loss: 2.428128957748413 | CLS Loss: 0.04585373401641846\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 / 200 | iteration 10 / 171 | Total Loss: 2.5146872997283936 | KNN Loss: 2.407222032546997 | CLS Loss: 0.10746519267559052\n",
      "Epoch 18 / 200 | iteration 20 / 171 | Total Loss: 2.4523472785949707 | KNN Loss: 2.3862624168395996 | CLS Loss: 0.06608489900827408\n",
      "Epoch 18 / 200 | iteration 30 / 171 | Total Loss: 2.4582390785217285 | KNN Loss: 2.399864912033081 | CLS Loss: 0.05837418511509895\n",
      "Epoch 18 / 200 | iteration 40 / 171 | Total Loss: 2.457495927810669 | KNN Loss: 2.4064888954162598 | CLS Loss: 0.051007144153118134\n",
      "Epoch 18 / 200 | iteration 50 / 171 | Total Loss: 2.5120275020599365 | KNN Loss: 2.434089422225952 | CLS Loss: 0.07793819159269333\n",
      "Epoch 18 / 200 | iteration 60 / 171 | Total Loss: 2.496138095855713 | KNN Loss: 2.4454262256622314 | CLS Loss: 0.050711825489997864\n",
      "Epoch 18 / 200 | iteration 70 / 171 | Total Loss: 2.4730207920074463 | KNN Loss: 2.4284307956695557 | CLS Loss: 0.04458993300795555\n",
      "Epoch 18 / 200 | iteration 80 / 171 | Total Loss: 2.4442408084869385 | KNN Loss: 2.401585817337036 | CLS Loss: 0.04265499487519264\n",
      "Epoch 18 / 200 | iteration 90 / 171 | Total Loss: 2.4496569633483887 | KNN Loss: 2.393953323364258 | CLS Loss: 0.05570359155535698\n",
      "Epoch 18 / 200 | iteration 100 / 171 | Total Loss: 2.4721107482910156 | KNN Loss: 2.417999267578125 | CLS Loss: 0.05411148443818092\n",
      "Epoch 18 / 200 | iteration 110 / 171 | Total Loss: 2.514805316925049 | KNN Loss: 2.41322922706604 | CLS Loss: 0.10157620161771774\n",
      "Epoch 18 / 200 | iteration 120 / 171 | Total Loss: 2.5001964569091797 | KNN Loss: 2.4174628257751465 | CLS Loss: 0.08273354172706604\n",
      "Epoch 18 / 200 | iteration 130 / 171 | Total Loss: 2.452310085296631 | KNN Loss: 2.403330087661743 | CLS Loss: 0.04898003861308098\n",
      "Epoch 18 / 200 | iteration 140 / 171 | Total Loss: 2.5108253955841064 | KNN Loss: 2.4103052616119385 | CLS Loss: 0.10052020102739334\n",
      "Epoch 18 / 200 | iteration 150 / 171 | Total Loss: 2.4677610397338867 | KNN Loss: 2.4197306632995605 | CLS Loss: 0.04803035780787468\n",
      "Epoch 18 / 200 | iteration 160 / 171 | Total Loss: 2.4358935356140137 | KNN Loss: 2.374459743499756 | CLS Loss: 0.06143387779593468\n",
      "Epoch 18 / 200 | iteration 170 / 171 | Total Loss: 2.4567861557006836 | KNN Loss: 2.3987603187561035 | CLS Loss: 0.05802588537335396\n",
      "Epoch: 018, Loss: 2.4676, Train: 0.9848, Valid: 0.9813, Best: 0.9813\n",
      "Epoch 19 / 200 | iteration 0 / 171 | Total Loss: 2.465257406234741 | KNN Loss: 2.39904522895813 | CLS Loss: 0.06621213257312775\n",
      "Epoch 19 / 200 | iteration 10 / 171 | Total Loss: 2.4369261264801025 | KNN Loss: 2.3656458854675293 | CLS Loss: 0.07128030806779861\n",
      "Epoch 19 / 200 | iteration 20 / 171 | Total Loss: 2.450176477432251 | KNN Loss: 2.3932385444641113 | CLS Loss: 0.05693793669342995\n",
      "Epoch 19 / 200 | iteration 30 / 171 | Total Loss: 2.454429864883423 | KNN Loss: 2.41416597366333 | CLS Loss: 0.040263984352350235\n",
      "Epoch 19 / 200 | iteration 40 / 171 | Total Loss: 2.484039783477783 | KNN Loss: 2.411278009414673 | CLS Loss: 0.07276176661252975\n",
      "Epoch 19 / 200 | iteration 50 / 171 | Total Loss: 2.4504687786102295 | KNN Loss: 2.395021915435791 | CLS Loss: 0.05544689670205116\n",
      "Epoch 19 / 200 | iteration 60 / 171 | Total Loss: 2.4292616844177246 | KNN Loss: 2.382791519165039 | CLS Loss: 0.04647006466984749\n",
      "Epoch 19 / 200 | iteration 70 / 171 | Total Loss: 2.489889621734619 | KNN Loss: 2.4504494667053223 | CLS Loss: 0.03944021463394165\n",
      "Epoch 19 / 200 | iteration 80 / 171 | Total Loss: 2.439566135406494 | KNN Loss: 2.3844683170318604 | CLS Loss: 0.05509791150689125\n",
      "Epoch 19 / 200 | iteration 90 / 171 | Total Loss: 2.466994524002075 | KNN Loss: 2.433516263961792 | CLS Loss: 0.03347824141383171\n",
      "Epoch 19 / 200 | iteration 100 / 171 | Total Loss: 2.473134756088257 | KNN Loss: 2.3944151401519775 | CLS Loss: 0.07871970534324646\n",
      "Epoch 19 / 200 | iteration 110 / 171 | Total Loss: 2.4535229206085205 | KNN Loss: 2.3772366046905518 | CLS Loss: 0.0762862041592598\n",
      "Epoch 19 / 200 | iteration 120 / 171 | Total Loss: 2.424739122390747 | KNN Loss: 2.3750340938568115 | CLS Loss: 0.049705080687999725\n",
      "Epoch 19 / 200 | iteration 130 / 171 | Total Loss: 2.459561586380005 | KNN Loss: 2.400038719177246 | CLS Loss: 0.05952276289463043\n",
      "Epoch 19 / 200 | iteration 140 / 171 | Total Loss: 2.470264434814453 | KNN Loss: 2.4334278106689453 | CLS Loss: 0.036836594343185425\n",
      "Epoch 19 / 200 | iteration 150 / 171 | Total Loss: 2.457329273223877 | KNN Loss: 2.4064419269561768 | CLS Loss: 0.05088736489415169\n",
      "Epoch 19 / 200 | iteration 160 / 171 | Total Loss: 2.4788870811462402 | KNN Loss: 2.4479501247406006 | CLS Loss: 0.030936989933252335\n",
      "Epoch 19 / 200 | iteration 170 / 171 | Total Loss: 2.4802615642547607 | KNN Loss: 2.3749661445617676 | CLS Loss: 0.10529550909996033\n",
      "Epoch: 019, Loss: 2.4601, Train: 0.9852, Valid: 0.9812, Best: 0.9813\n",
      "Epoch 20 / 200 | iteration 0 / 171 | Total Loss: 2.4359264373779297 | KNN Loss: 2.407902240753174 | CLS Loss: 0.028024081140756607\n",
      "Epoch 20 / 200 | iteration 10 / 171 | Total Loss: 2.4603514671325684 | KNN Loss: 2.3987934589385986 | CLS Loss: 0.061557892709970474\n",
      "Epoch 20 / 200 | iteration 20 / 171 | Total Loss: 2.4487416744232178 | KNN Loss: 2.409011125564575 | CLS Loss: 0.03973066434264183\n",
      "Epoch 20 / 200 | iteration 30 / 171 | Total Loss: 2.477080821990967 | KNN Loss: 2.3991641998291016 | CLS Loss: 0.07791650295257568\n",
      "Epoch 20 / 200 | iteration 40 / 171 | Total Loss: 2.4261748790740967 | KNN Loss: 2.36293888092041 | CLS Loss: 0.06323609501123428\n",
      "Epoch 20 / 200 | iteration 50 / 171 | Total Loss: 2.479109764099121 | KNN Loss: 2.3828258514404297 | CLS Loss: 0.09628396481275558\n",
      "Epoch 20 / 200 | iteration 60 / 171 | Total Loss: 2.4448344707489014 | KNN Loss: 2.374833345413208 | CLS Loss: 0.07000121474266052\n",
      "Epoch 20 / 200 | iteration 70 / 171 | Total Loss: 2.447622299194336 | KNN Loss: 2.405435085296631 | CLS Loss: 0.04218709468841553\n",
      "Epoch 20 / 200 | iteration 80 / 171 | Total Loss: 2.44374942779541 | KNN Loss: 2.384432315826416 | CLS Loss: 0.05931701511144638\n",
      "Epoch 20 / 200 | iteration 90 / 171 | Total Loss: 2.435141086578369 | KNN Loss: 2.3613901138305664 | CLS Loss: 0.07375101745128632\n",
      "Epoch 20 / 200 | iteration 100 / 171 | Total Loss: 2.4643521308898926 | KNN Loss: 2.381312370300293 | CLS Loss: 0.0830397829413414\n",
      "Epoch 20 / 200 | iteration 110 / 171 | Total Loss: 2.5086843967437744 | KNN Loss: 2.4433696269989014 | CLS Loss: 0.06531473249197006\n",
      "Epoch 20 / 200 | iteration 120 / 171 | Total Loss: 2.422703742980957 | KNN Loss: 2.3608531951904297 | CLS Loss: 0.061850666999816895\n",
      "Epoch 20 / 200 | iteration 130 / 171 | Total Loss: 2.4464361667633057 | KNN Loss: 2.403618097305298 | CLS Loss: 0.04281795769929886\n",
      "Epoch 20 / 200 | iteration 140 / 171 | Total Loss: 2.4108967781066895 | KNN Loss: 2.373159170150757 | CLS Loss: 0.03773749619722366\n",
      "Epoch 20 / 200 | iteration 150 / 171 | Total Loss: 2.442065477371216 | KNN Loss: 2.4001879692077637 | CLS Loss: 0.04187760502099991\n",
      "Epoch 20 / 200 | iteration 160 / 171 | Total Loss: 2.451279640197754 | KNN Loss: 2.4019618034362793 | CLS Loss: 0.049317922443151474\n",
      "Epoch 20 / 200 | iteration 170 / 171 | Total Loss: 2.440608501434326 | KNN Loss: 2.3805339336395264 | CLS Loss: 0.06007461994886398\n",
      "Epoch: 020, Loss: 2.4603, Train: 0.9847, Valid: 0.9815, Best: 0.9815\n",
      "Epoch 21 / 200 | iteration 0 / 171 | Total Loss: 2.445059061050415 | KNN Loss: 2.4011685848236084 | CLS Loss: 0.04389045014977455\n",
      "Epoch 21 / 200 | iteration 10 / 171 | Total Loss: 2.445244312286377 | KNN Loss: 2.391099691390991 | CLS Loss: 0.05414464697241783\n",
      "Epoch 21 / 200 | iteration 20 / 171 | Total Loss: 2.443340301513672 | KNN Loss: 2.408257246017456 | CLS Loss: 0.03508298099040985\n",
      "Epoch 21 / 200 | iteration 30 / 171 | Total Loss: 2.436159372329712 | KNN Loss: 2.3756418228149414 | CLS Loss: 0.060517568141222\n",
      "Epoch 21 / 200 | iteration 40 / 171 | Total Loss: 2.4335286617279053 | KNN Loss: 2.325605869293213 | CLS Loss: 0.10792288929224014\n",
      "Epoch 21 / 200 | iteration 50 / 171 | Total Loss: 2.4409072399139404 | KNN Loss: 2.3986127376556396 | CLS Loss: 0.042294565588235855\n",
      "Epoch 21 / 200 | iteration 60 / 171 | Total Loss: 2.502614736557007 | KNN Loss: 2.4088523387908936 | CLS Loss: 0.09376244246959686\n",
      "Epoch 21 / 200 | iteration 70 / 171 | Total Loss: 2.444751501083374 | KNN Loss: 2.3845272064208984 | CLS Loss: 0.06022435799241066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 / 200 | iteration 80 / 171 | Total Loss: 2.434865713119507 | KNN Loss: 2.3630244731903076 | CLS Loss: 0.07184115052223206\n",
      "Epoch 21 / 200 | iteration 90 / 171 | Total Loss: 2.446728467941284 | KNN Loss: 2.3893675804138184 | CLS Loss: 0.05736100301146507\n",
      "Epoch 21 / 200 | iteration 100 / 171 | Total Loss: 2.5034406185150146 | KNN Loss: 2.438753366470337 | CLS Loss: 0.06468720734119415\n",
      "Epoch 21 / 200 | iteration 110 / 171 | Total Loss: 2.453659772872925 | KNN Loss: 2.418555974960327 | CLS Loss: 0.035103872418403625\n",
      "Epoch 21 / 200 | iteration 120 / 171 | Total Loss: 2.442380905151367 | KNN Loss: 2.373149871826172 | CLS Loss: 0.06923096626996994\n",
      "Epoch 21 / 200 | iteration 130 / 171 | Total Loss: 2.4614107608795166 | KNN Loss: 2.4326438903808594 | CLS Loss: 0.028766868636012077\n",
      "Epoch 21 / 200 | iteration 140 / 171 | Total Loss: 2.460458278656006 | KNN Loss: 2.3848159313201904 | CLS Loss: 0.07564227283000946\n",
      "Epoch 21 / 200 | iteration 150 / 171 | Total Loss: 2.4344379901885986 | KNN Loss: 2.3804593086242676 | CLS Loss: 0.05397874489426613\n",
      "Epoch 21 / 200 | iteration 160 / 171 | Total Loss: 2.441690444946289 | KNN Loss: 2.3723959922790527 | CLS Loss: 0.06929439306259155\n",
      "Epoch 21 / 200 | iteration 170 / 171 | Total Loss: 2.495779275894165 | KNN Loss: 2.4579391479492188 | CLS Loss: 0.03784024342894554\n",
      "Epoch: 021, Loss: 2.4590, Train: 0.9869, Valid: 0.9823, Best: 0.9823\n",
      "Epoch 22 / 200 | iteration 0 / 171 | Total Loss: 2.420536518096924 | KNN Loss: 2.3813164234161377 | CLS Loss: 0.03922010585665703\n",
      "Epoch 22 / 200 | iteration 10 / 171 | Total Loss: 2.4587109088897705 | KNN Loss: 2.3862509727478027 | CLS Loss: 0.07246001064777374\n",
      "Epoch 22 / 200 | iteration 20 / 171 | Total Loss: 2.471621036529541 | KNN Loss: 2.3944945335388184 | CLS Loss: 0.07712642103433609\n",
      "Epoch 22 / 200 | iteration 30 / 171 | Total Loss: 2.4371073246002197 | KNN Loss: 2.411001682281494 | CLS Loss: 0.026105554774403572\n",
      "Epoch 22 / 200 | iteration 40 / 171 | Total Loss: 2.4614455699920654 | KNN Loss: 2.414249897003174 | CLS Loss: 0.047195594757795334\n",
      "Epoch 22 / 200 | iteration 50 / 171 | Total Loss: 2.431473970413208 | KNN Loss: 2.39160418510437 | CLS Loss: 0.039869725704193115\n",
      "Epoch 22 / 200 | iteration 60 / 171 | Total Loss: 2.482787609100342 | KNN Loss: 2.4417407512664795 | CLS Loss: 0.04104680195450783\n",
      "Epoch 22 / 200 | iteration 70 / 171 | Total Loss: 2.449307441711426 | KNN Loss: 2.3827931880950928 | CLS Loss: 0.06651437282562256\n",
      "Epoch 22 / 200 | iteration 80 / 171 | Total Loss: 2.478187084197998 | KNN Loss: 2.425909996032715 | CLS Loss: 0.052277106791734695\n",
      "Epoch 22 / 200 | iteration 90 / 171 | Total Loss: 2.454068899154663 | KNN Loss: 2.383214235305786 | CLS Loss: 0.07085472345352173\n",
      "Epoch 22 / 200 | iteration 100 / 171 | Total Loss: 2.4489903450012207 | KNN Loss: 2.394622325897217 | CLS Loss: 0.054368115961551666\n",
      "Epoch 22 / 200 | iteration 110 / 171 | Total Loss: 2.4719343185424805 | KNN Loss: 2.42923641204834 | CLS Loss: 0.04269779473543167\n",
      "Epoch 22 / 200 | iteration 120 / 171 | Total Loss: 2.442389965057373 | KNN Loss: 2.390789031982422 | CLS Loss: 0.05160101130604744\n",
      "Epoch 22 / 200 | iteration 130 / 171 | Total Loss: 2.445321798324585 | KNN Loss: 2.3945565223693848 | CLS Loss: 0.050765261054039\n",
      "Epoch 22 / 200 | iteration 140 / 171 | Total Loss: 2.4519593715667725 | KNN Loss: 2.398286819458008 | CLS Loss: 0.05367255583405495\n",
      "Epoch 22 / 200 | iteration 150 / 171 | Total Loss: 2.437096118927002 | KNN Loss: 2.3804755210876465 | CLS Loss: 0.05662068352103233\n",
      "Epoch 22 / 200 | iteration 160 / 171 | Total Loss: 2.442495346069336 | KNN Loss: 2.3740768432617188 | CLS Loss: 0.0684184804558754\n",
      "Epoch 22 / 200 | iteration 170 / 171 | Total Loss: 2.446951389312744 | KNN Loss: 2.4081530570983887 | CLS Loss: 0.03879836946725845\n",
      "Epoch: 022, Loss: 2.4555, Train: 0.9867, Valid: 0.9827, Best: 0.9827\n",
      "Epoch 23 / 200 | iteration 0 / 171 | Total Loss: 2.4952406883239746 | KNN Loss: 2.4141287803649902 | CLS Loss: 0.08111180365085602\n",
      "Epoch 23 / 200 | iteration 10 / 171 | Total Loss: 2.4545505046844482 | KNN Loss: 2.3885228633880615 | CLS Loss: 0.06602774560451508\n",
      "Epoch 23 / 200 | iteration 20 / 171 | Total Loss: 2.4686248302459717 | KNN Loss: 2.394244432449341 | CLS Loss: 0.07438037544488907\n",
      "Epoch 23 / 200 | iteration 30 / 171 | Total Loss: 2.4304323196411133 | KNN Loss: 2.3974337577819824 | CLS Loss: 0.032998643815517426\n",
      "Epoch 23 / 200 | iteration 40 / 171 | Total Loss: 2.4542737007141113 | KNN Loss: 2.3778467178344727 | CLS Loss: 0.07642688602209091\n",
      "Epoch 23 / 200 | iteration 50 / 171 | Total Loss: 2.4491991996765137 | KNN Loss: 2.3705034255981445 | CLS Loss: 0.07869575172662735\n",
      "Epoch 23 / 200 | iteration 60 / 171 | Total Loss: 2.4157321453094482 | KNN Loss: 2.3440492153167725 | CLS Loss: 0.07168283313512802\n",
      "Epoch 23 / 200 | iteration 70 / 171 | Total Loss: 2.432802438735962 | KNN Loss: 2.397913932800293 | CLS Loss: 0.03488846868276596\n",
      "Epoch 23 / 200 | iteration 80 / 171 | Total Loss: 2.451094627380371 | KNN Loss: 2.3910160064697266 | CLS Loss: 0.06007860600948334\n"
     ]
    }
   ],
   "source": [
    "best_valid_acc = 0\n",
    "losses = []\n",
    "train_accs = []\n",
    "val_accs = []\n",
    "for epoch in range(1, epochs + 1):\n",
    "    loss = train(model, train_data_iter, optimizer, device)\n",
    "#     print(f\"Loss: {loss} =============================\")\n",
    "    losses.append(loss)\n",
    "    train_acc = test(model, train_data_iter, device)\n",
    "    train_accs.append(train_acc)\n",
    "    valid_acc = test(model, test_data_iter, device)\n",
    "    val_accs.append(valid_acc)\n",
    "    if valid_acc > best_valid_acc:\n",
    "        best_valid_acc = valid_acc\n",
    "    if epoch % 1 == 0:\n",
    "        print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, '\n",
    "              f'Train: {train_acc:.4f}, Valid: {valid_acc:.4f}, '\n",
    "              f'Best: {best_valid_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(model, test_data_iter, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(losses, label='train loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(train_accs, label='train accuracy')\n",
    "plt.plot(val_accs, label='validation accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_samples = torch.tensor([])\n",
    "projections = torch.tensor([])\n",
    "labels = torch.tensor([])\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, y in tqdm(test_data_iter):\n",
    "        test_samples = torch.cat([test_samples, x])\n",
    "        labels = torch.cat([labels, y])\n",
    "        x = x.to(device)\n",
    "        _, interm = model(x, True)\n",
    "        projections = torch.cat([projections, interm.detach().cpu().flatten(1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = pairwise_distances(projections)\n",
    "# distances = np.triu(distances)\n",
    "distances_f = distances.flatten()\n",
    "\n",
    "plt.matshow(distances)\n",
    "plt.colorbar()\n",
    "plt.figure()\n",
    "plt.hist(distances_f[distances_f > 0], bins=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "for i in np.arange(0.5, 4, 0.1):\n",
    "    clusters = DBSCAN(eps=i, min_samples=10).fit_predict(projections)\n",
    "    print(f\"Number of inliers: {sum(clusters != -1) / len(clusters)}\")\n",
    "    res.append(sum(clusters != -1) / len(clusters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eitan\\Anaconda3\\envs\\research\\lib\\site-packages\\ipykernel_launcher.py:1: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48225a36337d4830a9d37f9f0bed7b39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(np.arange(0.5, 4, 0.1), res)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = DBSCAN(eps=2, min_samples=10).fit_predict(projections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of inliers: 0.8540952903019505\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of inliers: {sum(clusters != -1) / len(clusters)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eitan\\PycharmProjects\\EntangledExplainableClustering\\utils\\MatplotlibUtils.py:167: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig = plt.figure()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe96a99e97a841278206f5d9b9d99381",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "perplexity = 100\n",
    "p = reduce_dims_and_plot(projections[clusters != -1],\n",
    "                         y=clusters[clusters != -1],\n",
    "                         title=f'perplexity: {perplexity}',\n",
    "                         file_name=None,\n",
    "                         perplexity=perplexity,\n",
    "                         library='Multicore-TSNE',\n",
    "                         perform_PCA=False,\n",
    "                         projected=None,\n",
    "                         figure_type='2d',\n",
    "                         show_figure=True,\n",
    "                         close_figure=False,\n",
    "                         text=None)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Soft-Decision-Tree given the self-labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_dataset = list(zip(test_samples.flatten(1)[clusters!=-1], clusters[clusters != -1]))\n",
    "batch_size = 512\n",
    "tree_loader = torch.utils.data.DataLoader(tree_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define how we prune the weights of a node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_node(node_weights, factor=1):\n",
    "    w = node_weights.cpu().detach().numpy()\n",
    "    mean_ = np.mean(w)\n",
    "    std_ = np.std(w)\n",
    "    node_weights[((mean_ - std_ * factor) < node_weights) & (node_weights < (mean_ + std_ * factor))] = 0\n",
    "    return node_weights\n",
    "\n",
    "def prune_node_keep(node_weights, keep=4):\n",
    "    w = node_weights.cpu().detach().numpy()\n",
    "    throw_idx = np.argsort(abs(w))[:-keep]\n",
    "    node_weights[throw_idx] = 0\n",
    "    return node_weights\n",
    "\n",
    "def prune_tree(tree_, factor):\n",
    "    new_weights = tree_.inner_nodes.weight.clone()\n",
    "    for i in range(new_weights.shape[0]):\n",
    "        res = prune_node_keep(new_weights[i, :], factor)\n",
    "        new_weights[i, :] = res\n",
    "\n",
    "    with torch.no_grad():\n",
    "        tree_.inner_nodes.weight.copy_(new_weights)\n",
    "        \n",
    "def sparseness(x):\n",
    "    s = []\n",
    "    for i in range(x.shape[0]):\n",
    "        x_ = x[i, :]\n",
    "        sp = (len(x_) - torch.norm(x_, 0).item()) / len(x_)\n",
    "        s.append(sp)\n",
    "    return np.mean(s)\n",
    "\n",
    "def compute_regularization_by_level(tree):\n",
    "    total_reg = 0\n",
    "    for i in range(tree.inner_nodes.weight.shape[0]):\n",
    "        cur_level = np.floor(np.log2(i+1))\n",
    "        node_reg = torch.norm(tree.inner_nodes.weight[i].view(-1), 2)\n",
    "        total_reg += 2**(-cur_level) * node_reg\n",
    "    return total_reg\n",
    "\n",
    "def show_sparseness(tree):\n",
    "    avg_sp = sparseness(tree.inner_nodes.weight)\n",
    "    print(f\"Average sparseness: {avg_sp}\")\n",
    "    layer = 0\n",
    "    sps = []\n",
    "    for i in range(tree.inner_nodes.weight.shape[0]):\n",
    "        cur_layer = int(np.floor(np.log2(i+1)))\n",
    "        if cur_layer != layer:\n",
    "            print(f\"layer {layer}: {np.mean(sps)}\")\n",
    "            sps = []\n",
    "            layer = cur_layer\n",
    "\n",
    "        x_ = tree.inner_nodes.weight[i, :]\n",
    "        sp = (len(x_) - torch.norm(x_, 0).item()) / len(x_)\n",
    "        sps.append(sp)\n",
    "        \n",
    "    return avg_sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_epoch(model, loader, device, log_interval, losses, accs, epoch, iteration):\n",
    "    model = model.train()\n",
    "    for batch_idx, (data, target) in enumerate(loader):\n",
    "        iteration += 1\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        output, penalty = tree.forward(data)\n",
    "\n",
    "        # Loss\n",
    "        loss_tree = criterion(output, target.view(-1))\n",
    "\n",
    "        # Penalty\n",
    "        loss_tree += penalty\n",
    "\n",
    "        # Sparse regularization\n",
    "#         fc_params = torch.cat([x.view(-1) for x in tree.inner_nodes.parameters()])\n",
    "#         regularization = sparsity_lamda * torch.norm(fc_params, 2)\n",
    "        regularization = sparsity_lamda * compute_regularization_by_level(tree)\n",
    "        loss = loss_tree\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        pred = output.data.max(1)[1]\n",
    "        correct = pred.eq(target.view(-1).data).sum()\n",
    "        accs.append(correct.item() / data.size()[0])\n",
    "\n",
    "        # Print training status\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(f\"Epoch: {epoch:02d} | Batch: {batch_idx:03d} / {len(loader):03d} | Total loss: {loss.item():.3f} | Reg loss: {regularization.item():.3f} | Tree loss: {loss_tree.item():.3f} | Accuracy: {correct.item() / data.size()[0]:03f} | {round((time.time() - start_time) / iteration, 3)} sec/iter\")\n",
    "            \n",
    "    return iteration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 5e-3\n",
    "weight_decay = 5e-4\n",
    "sparsity_lamda = 2e-3\n",
    "epochs = 400\n",
    "log_interval = 100\n",
    "use_cuda = device != 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = SDT(input_dim=test_samples.shape[2], output_dim=len(set(clusters)) - 1, depth=tree_depth, lamda=1e-3, use_cuda=use_cuda)\n",
    "optimizer = torch.optim.Adam(tree.parameters(),\n",
    "                                 lr=lr,\n",
    "                                 weight_decay=weight_decay)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "tree = tree.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "accs = []\n",
    "sparsity = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.0\n",
      "layer 0: 0.0\n",
      "layer 1: 0.0\n",
      "layer 2: 0.0\n",
      "layer 3: 0.0\n",
      "layer 4: 0.0\n",
      "Epoch: 00 | Batch: 000 / 037 | Total loss: 3.030 | Reg loss: 0.007 | Tree loss: 3.030 | Accuracy: 0.017578 | 0.125 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 01 | Batch: 000 / 037 | Total loss: 2.942 | Reg loss: 0.004 | Tree loss: 2.942 | Accuracy: 0.210938 | 0.113 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 02 | Batch: 000 / 037 | Total loss: 2.902 | Reg loss: 0.007 | Tree loss: 2.902 | Accuracy: 0.140625 | 0.113 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 03 | Batch: 000 / 037 | Total loss: 2.818 | Reg loss: 0.009 | Tree loss: 2.818 | Accuracy: 0.201172 | 0.116 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 04 | Batch: 000 / 037 | Total loss: 2.760 | Reg loss: 0.011 | Tree loss: 2.760 | Accuracy: 0.187500 | 0.126 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 05 | Batch: 000 / 037 | Total loss: 2.718 | Reg loss: 0.013 | Tree loss: 2.718 | Accuracy: 0.195312 | 0.131 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 06 | Batch: 000 / 037 | Total loss: 2.705 | Reg loss: 0.015 | Tree loss: 2.705 | Accuracy: 0.189453 | 0.132 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 07 | Batch: 000 / 037 | Total loss: 2.684 | Reg loss: 0.017 | Tree loss: 2.684 | Accuracy: 0.166016 | 0.132 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 08 | Batch: 000 / 037 | Total loss: 2.658 | Reg loss: 0.018 | Tree loss: 2.658 | Accuracy: 0.185547 | 0.131 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 09 | Batch: 000 / 037 | Total loss: 2.568 | Reg loss: 0.019 | Tree loss: 2.568 | Accuracy: 0.224609 | 0.129 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 10 | Batch: 000 / 037 | Total loss: 2.553 | Reg loss: 0.021 | Tree loss: 2.553 | Accuracy: 0.199219 | 0.128 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 11 | Batch: 000 / 037 | Total loss: 2.590 | Reg loss: 0.022 | Tree loss: 2.590 | Accuracy: 0.177734 | 0.127 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 12 | Batch: 000 / 037 | Total loss: 2.536 | Reg loss: 0.023 | Tree loss: 2.536 | Accuracy: 0.220703 | 0.128 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 13 | Batch: 000 / 037 | Total loss: 2.508 | Reg loss: 0.024 | Tree loss: 2.508 | Accuracy: 0.201172 | 0.127 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 14 | Batch: 000 / 037 | Total loss: 2.427 | Reg loss: 0.025 | Tree loss: 2.427 | Accuracy: 0.224609 | 0.126 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 15 | Batch: 000 / 037 | Total loss: 2.406 | Reg loss: 0.026 | Tree loss: 2.406 | Accuracy: 0.238281 | 0.125 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 16 | Batch: 000 / 037 | Total loss: 2.453 | Reg loss: 0.027 | Tree loss: 2.453 | Accuracy: 0.236328 | 0.124 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 17 | Batch: 000 / 037 | Total loss: 2.393 | Reg loss: 0.028 | Tree loss: 2.393 | Accuracy: 0.220703 | 0.123 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 18 | Batch: 000 / 037 | Total loss: 2.349 | Reg loss: 0.029 | Tree loss: 2.349 | Accuracy: 0.238281 | 0.123 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 19 | Batch: 000 / 037 | Total loss: 2.360 | Reg loss: 0.030 | Tree loss: 2.360 | Accuracy: 0.232422 | 0.122 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 20 | Batch: 000 / 037 | Total loss: 2.332 | Reg loss: 0.030 | Tree loss: 2.332 | Accuracy: 0.244141 | 0.122 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 21 | Batch: 000 / 037 | Total loss: 2.323 | Reg loss: 0.031 | Tree loss: 2.323 | Accuracy: 0.234375 | 0.121 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 22 | Batch: 000 / 037 | Total loss: 2.256 | Reg loss: 0.032 | Tree loss: 2.256 | Accuracy: 0.246094 | 0.121 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 23 | Batch: 000 / 037 | Total loss: 2.306 | Reg loss: 0.032 | Tree loss: 2.306 | Accuracy: 0.214844 | 0.12 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 24 | Batch: 000 / 037 | Total loss: 2.218 | Reg loss: 0.033 | Tree loss: 2.218 | Accuracy: 0.253906 | 0.12 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 25 | Batch: 000 / 037 | Total loss: 2.232 | Reg loss: 0.034 | Tree loss: 2.232 | Accuracy: 0.236328 | 0.121 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 26 | Batch: 000 / 037 | Total loss: 2.200 | Reg loss: 0.034 | Tree loss: 2.200 | Accuracy: 0.265625 | 0.121 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 27 | Batch: 000 / 037 | Total loss: 2.191 | Reg loss: 0.035 | Tree loss: 2.191 | Accuracy: 0.267578 | 0.121 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 28 | Batch: 000 / 037 | Total loss: 2.187 | Reg loss: 0.035 | Tree loss: 2.187 | Accuracy: 0.255859 | 0.121 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 29 | Batch: 000 / 037 | Total loss: 2.166 | Reg loss: 0.036 | Tree loss: 2.166 | Accuracy: 0.251953 | 0.12 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 30 | Batch: 000 / 037 | Total loss: 2.127 | Reg loss: 0.036 | Tree loss: 2.127 | Accuracy: 0.271484 | 0.12 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 31 | Batch: 000 / 037 | Total loss: 2.233 | Reg loss: 0.036 | Tree loss: 2.233 | Accuracy: 0.226562 | 0.12 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 32 | Batch: 000 / 037 | Total loss: 2.080 | Reg loss: 0.037 | Tree loss: 2.080 | Accuracy: 0.316406 | 0.119 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 33 | Batch: 000 / 037 | Total loss: 2.117 | Reg loss: 0.037 | Tree loss: 2.117 | Accuracy: 0.261719 | 0.119 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 34 | Batch: 000 / 037 | Total loss: 2.080 | Reg loss: 0.038 | Tree loss: 2.080 | Accuracy: 0.298828 | 0.119 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 35 | Batch: 000 / 037 | Total loss: 2.099 | Reg loss: 0.038 | Tree loss: 2.099 | Accuracy: 0.275391 | 0.119 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 36 | Batch: 000 / 037 | Total loss: 2.057 | Reg loss: 0.038 | Tree loss: 2.057 | Accuracy: 0.300781 | 0.118 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 37 | Batch: 000 / 037 | Total loss: 2.029 | Reg loss: 0.039 | Tree loss: 2.029 | Accuracy: 0.310547 | 0.118 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 38 | Batch: 000 / 037 | Total loss: 2.062 | Reg loss: 0.039 | Tree loss: 2.062 | Accuracy: 0.314453 | 0.118 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 39 | Batch: 000 / 037 | Total loss: 2.086 | Reg loss: 0.039 | Tree loss: 2.086 | Accuracy: 0.300781 | 0.118 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 40 | Batch: 000 / 037 | Total loss: 2.039 | Reg loss: 0.040 | Tree loss: 2.039 | Accuracy: 0.310547 | 0.117 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 41 | Batch: 000 / 037 | Total loss: 2.034 | Reg loss: 0.040 | Tree loss: 2.034 | Accuracy: 0.306641 | 0.117 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 42 | Batch: 000 / 037 | Total loss: 1.982 | Reg loss: 0.040 | Tree loss: 1.982 | Accuracy: 0.326172 | 0.117 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 43 | Batch: 000 / 037 | Total loss: 2.004 | Reg loss: 0.041 | Tree loss: 2.004 | Accuracy: 0.351562 | 0.117 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 44 | Batch: 000 / 037 | Total loss: 2.017 | Reg loss: 0.041 | Tree loss: 2.017 | Accuracy: 0.320312 | 0.117 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 45 | Batch: 000 / 037 | Total loss: 1.949 | Reg loss: 0.041 | Tree loss: 1.949 | Accuracy: 0.380859 | 0.117 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 46 | Batch: 000 / 037 | Total loss: 1.975 | Reg loss: 0.042 | Tree loss: 1.975 | Accuracy: 0.371094 | 0.117 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 47 | Batch: 000 / 037 | Total loss: 1.907 | Reg loss: 0.042 | Tree loss: 1.907 | Accuracy: 0.378906 | 0.117 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 48 | Batch: 000 / 037 | Total loss: 1.868 | Reg loss: 0.042 | Tree loss: 1.868 | Accuracy: 0.380859 | 0.117 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 49 | Batch: 000 / 037 | Total loss: 1.902 | Reg loss: 0.042 | Tree loss: 1.902 | Accuracy: 0.384766 | 0.117 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 50 | Batch: 000 / 037 | Total loss: 1.865 | Reg loss: 0.043 | Tree loss: 1.865 | Accuracy: 0.400391 | 0.117 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 51 | Batch: 000 / 037 | Total loss: 1.832 | Reg loss: 0.043 | Tree loss: 1.832 | Accuracy: 0.404297 | 0.116 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 52 | Batch: 000 / 037 | Total loss: 1.842 | Reg loss: 0.043 | Tree loss: 1.842 | Accuracy: 0.410156 | 0.116 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 53 | Batch: 000 / 037 | Total loss: 1.909 | Reg loss: 0.043 | Tree loss: 1.909 | Accuracy: 0.410156 | 0.116 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 54 | Batch: 000 / 037 | Total loss: 1.860 | Reg loss: 0.044 | Tree loss: 1.860 | Accuracy: 0.445312 | 0.116 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 55 | Batch: 000 / 037 | Total loss: 1.832 | Reg loss: 0.044 | Tree loss: 1.832 | Accuracy: 0.429688 | 0.116 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 56 | Batch: 000 / 037 | Total loss: 1.827 | Reg loss: 0.044 | Tree loss: 1.827 | Accuracy: 0.421875 | 0.116 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 57 | Batch: 000 / 037 | Total loss: 1.815 | Reg loss: 0.044 | Tree loss: 1.815 | Accuracy: 0.451172 | 0.116 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 58 | Batch: 000 / 037 | Total loss: 1.825 | Reg loss: 0.044 | Tree loss: 1.825 | Accuracy: 0.457031 | 0.116 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 59 | Batch: 000 / 037 | Total loss: 1.748 | Reg loss: 0.044 | Tree loss: 1.748 | Accuracy: 0.496094 | 0.116 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 60 | Batch: 000 / 037 | Total loss: 1.791 | Reg loss: 0.044 | Tree loss: 1.791 | Accuracy: 0.462891 | 0.116 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 61 | Batch: 000 / 037 | Total loss: 1.808 | Reg loss: 0.044 | Tree loss: 1.808 | Accuracy: 0.484375 | 0.116 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 62 | Batch: 000 / 037 | Total loss: 1.800 | Reg loss: 0.044 | Tree loss: 1.800 | Accuracy: 0.482422 | 0.116 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 63 | Batch: 000 / 037 | Total loss: 1.859 | Reg loss: 0.044 | Tree loss: 1.859 | Accuracy: 0.441406 | 0.116 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 64 | Batch: 000 / 037 | Total loss: 1.893 | Reg loss: 0.044 | Tree loss: 1.893 | Accuracy: 0.433594 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 65 | Batch: 000 / 037 | Total loss: 1.842 | Reg loss: 0.044 | Tree loss: 1.842 | Accuracy: 0.460938 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 66 | Batch: 000 / 037 | Total loss: 1.829 | Reg loss: 0.044 | Tree loss: 1.829 | Accuracy: 0.439453 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 67 | Batch: 000 / 037 | Total loss: 1.837 | Reg loss: 0.044 | Tree loss: 1.837 | Accuracy: 0.455078 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 68 | Batch: 000 / 037 | Total loss: 1.827 | Reg loss: 0.044 | Tree loss: 1.827 | Accuracy: 0.431641 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 69 | Batch: 000 / 037 | Total loss: 1.924 | Reg loss: 0.044 | Tree loss: 1.924 | Accuracy: 0.441406 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 70 | Batch: 000 / 037 | Total loss: 1.902 | Reg loss: 0.044 | Tree loss: 1.902 | Accuracy: 0.421875 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 71 | Batch: 000 / 037 | Total loss: 1.811 | Reg loss: 0.044 | Tree loss: 1.811 | Accuracy: 0.437500 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 72 | Batch: 000 / 037 | Total loss: 1.840 | Reg loss: 0.044 | Tree loss: 1.840 | Accuracy: 0.457031 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 73 | Batch: 000 / 037 | Total loss: 1.828 | Reg loss: 0.044 | Tree loss: 1.828 | Accuracy: 0.449219 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 74 | Batch: 000 / 037 | Total loss: 1.835 | Reg loss: 0.044 | Tree loss: 1.835 | Accuracy: 0.435547 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 75 | Batch: 000 / 037 | Total loss: 1.893 | Reg loss: 0.044 | Tree loss: 1.893 | Accuracy: 0.421875 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 76 | Batch: 000 / 037 | Total loss: 1.843 | Reg loss: 0.043 | Tree loss: 1.843 | Accuracy: 0.458984 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 77 | Batch: 000 / 037 | Total loss: 1.760 | Reg loss: 0.043 | Tree loss: 1.760 | Accuracy: 0.476562 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 78 | Batch: 000 / 037 | Total loss: 1.811 | Reg loss: 0.043 | Tree loss: 1.811 | Accuracy: 0.435547 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 79 | Batch: 000 / 037 | Total loss: 1.771 | Reg loss: 0.043 | Tree loss: 1.771 | Accuracy: 0.472656 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 80 | Batch: 000 / 037 | Total loss: 1.848 | Reg loss: 0.043 | Tree loss: 1.848 | Accuracy: 0.470703 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 81 | Batch: 000 / 037 | Total loss: 1.771 | Reg loss: 0.043 | Tree loss: 1.771 | Accuracy: 0.443359 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 82 | Batch: 000 / 037 | Total loss: 1.829 | Reg loss: 0.043 | Tree loss: 1.829 | Accuracy: 0.472656 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 83 | Batch: 000 / 037 | Total loss: 1.822 | Reg loss: 0.043 | Tree loss: 1.822 | Accuracy: 0.458984 | 0.114 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 84 | Batch: 000 / 037 | Total loss: 1.864 | Reg loss: 0.043 | Tree loss: 1.864 | Accuracy: 0.437500 | 0.114 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 85 | Batch: 000 / 037 | Total loss: 1.757 | Reg loss: 0.043 | Tree loss: 1.757 | Accuracy: 0.488281 | 0.114 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 86 | Batch: 000 / 037 | Total loss: 1.855 | Reg loss: 0.043 | Tree loss: 1.855 | Accuracy: 0.449219 | 0.114 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 87 | Batch: 000 / 037 | Total loss: 1.803 | Reg loss: 0.043 | Tree loss: 1.803 | Accuracy: 0.457031 | 0.114 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 88 | Batch: 000 / 037 | Total loss: 1.816 | Reg loss: 0.043 | Tree loss: 1.816 | Accuracy: 0.425781 | 0.114 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 89 | Batch: 000 / 037 | Total loss: 1.839 | Reg loss: 0.043 | Tree loss: 1.839 | Accuracy: 0.439453 | 0.114 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 90 | Batch: 000 / 037 | Total loss: 1.761 | Reg loss: 0.043 | Tree loss: 1.761 | Accuracy: 0.447266 | 0.114 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 91 | Batch: 000 / 037 | Total loss: 1.789 | Reg loss: 0.043 | Tree loss: 1.789 | Accuracy: 0.470703 | 0.114 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 92 | Batch: 000 / 037 | Total loss: 1.781 | Reg loss: 0.042 | Tree loss: 1.781 | Accuracy: 0.451172 | 0.114 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 93 | Batch: 000 / 037 | Total loss: 1.800 | Reg loss: 0.042 | Tree loss: 1.800 | Accuracy: 0.468750 | 0.114 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 94 | Batch: 000 / 037 | Total loss: 1.802 | Reg loss: 0.042 | Tree loss: 1.802 | Accuracy: 0.460938 | 0.114 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 95 | Batch: 000 / 037 | Total loss: 1.757 | Reg loss: 0.042 | Tree loss: 1.757 | Accuracy: 0.451172 | 0.114 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 96 | Batch: 000 / 037 | Total loss: 1.770 | Reg loss: 0.042 | Tree loss: 1.770 | Accuracy: 0.478516 | 0.114 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 97 | Batch: 000 / 037 | Total loss: 1.762 | Reg loss: 0.042 | Tree loss: 1.762 | Accuracy: 0.451172 | 0.114 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 98 | Batch: 000 / 037 | Total loss: 1.787 | Reg loss: 0.042 | Tree loss: 1.787 | Accuracy: 0.458984 | 0.114 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 99 | Batch: 000 / 037 | Total loss: 1.780 | Reg loss: 0.042 | Tree loss: 1.780 | Accuracy: 0.433594 | 0.114 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 100 | Batch: 000 / 037 | Total loss: 1.806 | Reg loss: 0.042 | Tree loss: 1.806 | Accuracy: 0.433594 | 0.114 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 101 | Batch: 000 / 037 | Total loss: 1.842 | Reg loss: 0.042 | Tree loss: 1.842 | Accuracy: 0.447266 | 0.114 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 102 | Batch: 000 / 037 | Total loss: 1.833 | Reg loss: 0.042 | Tree loss: 1.833 | Accuracy: 0.433594 | 0.114 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 103 | Batch: 000 / 037 | Total loss: 1.817 | Reg loss: 0.042 | Tree loss: 1.817 | Accuracy: 0.449219 | 0.114 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 104 | Batch: 000 / 037 | Total loss: 1.761 | Reg loss: 0.042 | Tree loss: 1.761 | Accuracy: 0.462891 | 0.114 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 105 | Batch: 000 / 037 | Total loss: 1.838 | Reg loss: 0.042 | Tree loss: 1.838 | Accuracy: 0.435547 | 0.114 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 106 | Batch: 000 / 037 | Total loss: 1.787 | Reg loss: 0.042 | Tree loss: 1.787 | Accuracy: 0.441406 | 0.114 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 107 | Batch: 000 / 037 | Total loss: 1.782 | Reg loss: 0.042 | Tree loss: 1.782 | Accuracy: 0.480469 | 0.114 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 108 | Batch: 000 / 037 | Total loss: 1.807 | Reg loss: 0.042 | Tree loss: 1.807 | Accuracy: 0.439453 | 0.114 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 109 | Batch: 000 / 037 | Total loss: 1.829 | Reg loss: 0.042 | Tree loss: 1.829 | Accuracy: 0.423828 | 0.114 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 110 | Batch: 000 / 037 | Total loss: 1.805 | Reg loss: 0.042 | Tree loss: 1.805 | Accuracy: 0.443359 | 0.114 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 111 | Batch: 000 / 037 | Total loss: 1.789 | Reg loss: 0.042 | Tree loss: 1.789 | Accuracy: 0.449219 | 0.114 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 112 | Batch: 000 / 037 | Total loss: 1.746 | Reg loss: 0.042 | Tree loss: 1.746 | Accuracy: 0.476562 | 0.114 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 113 | Batch: 000 / 037 | Total loss: 1.733 | Reg loss: 0.042 | Tree loss: 1.733 | Accuracy: 0.451172 | 0.114 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 114 | Batch: 000 / 037 | Total loss: 1.753 | Reg loss: 0.042 | Tree loss: 1.753 | Accuracy: 0.486328 | 0.114 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 115 | Batch: 000 / 037 | Total loss: 1.767 | Reg loss: 0.042 | Tree loss: 1.767 | Accuracy: 0.445312 | 0.114 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 116 | Batch: 000 / 037 | Total loss: 1.756 | Reg loss: 0.042 | Tree loss: 1.756 | Accuracy: 0.417969 | 0.114 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 117 | Batch: 000 / 037 | Total loss: 1.787 | Reg loss: 0.041 | Tree loss: 1.787 | Accuracy: 0.447266 | 0.114 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 118 | Batch: 000 / 037 | Total loss: 1.822 | Reg loss: 0.041 | Tree loss: 1.822 | Accuracy: 0.425781 | 0.114 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 119 | Batch: 000 / 037 | Total loss: 1.779 | Reg loss: 0.041 | Tree loss: 1.779 | Accuracy: 0.458984 | 0.114 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 120 | Batch: 000 / 037 | Total loss: 1.737 | Reg loss: 0.041 | Tree loss: 1.737 | Accuracy: 0.472656 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 121 | Batch: 000 / 037 | Total loss: 1.804 | Reg loss: 0.041 | Tree loss: 1.804 | Accuracy: 0.457031 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 122 | Batch: 000 / 037 | Total loss: 1.800 | Reg loss: 0.041 | Tree loss: 1.800 | Accuracy: 0.437500 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 123 | Batch: 000 / 037 | Total loss: 1.729 | Reg loss: 0.041 | Tree loss: 1.729 | Accuracy: 0.439453 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 124 | Batch: 000 / 037 | Total loss: 1.762 | Reg loss: 0.041 | Tree loss: 1.762 | Accuracy: 0.414062 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 125 | Batch: 000 / 037 | Total loss: 1.753 | Reg loss: 0.041 | Tree loss: 1.753 | Accuracy: 0.445312 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 126 | Batch: 000 / 037 | Total loss: 1.762 | Reg loss: 0.041 | Tree loss: 1.762 | Accuracy: 0.431641 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 127 | Batch: 000 / 037 | Total loss: 1.788 | Reg loss: 0.041 | Tree loss: 1.788 | Accuracy: 0.464844 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 128 | Batch: 000 / 037 | Total loss: 1.787 | Reg loss: 0.041 | Tree loss: 1.787 | Accuracy: 0.417969 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 129 | Batch: 000 / 037 | Total loss: 1.782 | Reg loss: 0.041 | Tree loss: 1.782 | Accuracy: 0.425781 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 130 | Batch: 000 / 037 | Total loss: 1.867 | Reg loss: 0.041 | Tree loss: 1.867 | Accuracy: 0.412109 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 131 | Batch: 000 / 037 | Total loss: 1.710 | Reg loss: 0.041 | Tree loss: 1.710 | Accuracy: 0.482422 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 132 | Batch: 000 / 037 | Total loss: 1.761 | Reg loss: 0.041 | Tree loss: 1.761 | Accuracy: 0.480469 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 133 | Batch: 000 / 037 | Total loss: 1.792 | Reg loss: 0.041 | Tree loss: 1.792 | Accuracy: 0.439453 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 134 | Batch: 000 / 037 | Total loss: 1.687 | Reg loss: 0.041 | Tree loss: 1.687 | Accuracy: 0.492188 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 135 | Batch: 000 / 037 | Total loss: 1.796 | Reg loss: 0.041 | Tree loss: 1.796 | Accuracy: 0.416016 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 136 | Batch: 000 / 037 | Total loss: 1.734 | Reg loss: 0.041 | Tree loss: 1.734 | Accuracy: 0.455078 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 137 | Batch: 000 / 037 | Total loss: 1.765 | Reg loss: 0.041 | Tree loss: 1.765 | Accuracy: 0.462891 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 138 | Batch: 000 / 037 | Total loss: 1.756 | Reg loss: 0.041 | Tree loss: 1.756 | Accuracy: 0.458984 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 139 | Batch: 000 / 037 | Total loss: 1.791 | Reg loss: 0.041 | Tree loss: 1.791 | Accuracy: 0.445312 | 0.115 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 140 | Batch: 000 / 037 | Total loss: 1.821 | Reg loss: 0.041 | Tree loss: 1.821 | Accuracy: 0.431641 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 141 | Batch: 000 / 037 | Total loss: 1.743 | Reg loss: 0.041 | Tree loss: 1.743 | Accuracy: 0.484375 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 142 | Batch: 000 / 037 | Total loss: 1.732 | Reg loss: 0.041 | Tree loss: 1.732 | Accuracy: 0.474609 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 143 | Batch: 000 / 037 | Total loss: 1.762 | Reg loss: 0.041 | Tree loss: 1.762 | Accuracy: 0.458984 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 144 | Batch: 000 / 037 | Total loss: 1.844 | Reg loss: 0.041 | Tree loss: 1.844 | Accuracy: 0.412109 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 145 | Batch: 000 / 037 | Total loss: 1.768 | Reg loss: 0.041 | Tree loss: 1.768 | Accuracy: 0.464844 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 146 | Batch: 000 / 037 | Total loss: 1.785 | Reg loss: 0.040 | Tree loss: 1.785 | Accuracy: 0.462891 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 147 | Batch: 000 / 037 | Total loss: 1.816 | Reg loss: 0.040 | Tree loss: 1.816 | Accuracy: 0.437500 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 148 | Batch: 000 / 037 | Total loss: 1.773 | Reg loss: 0.040 | Tree loss: 1.773 | Accuracy: 0.468750 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 149 | Batch: 000 / 037 | Total loss: 1.806 | Reg loss: 0.040 | Tree loss: 1.806 | Accuracy: 0.470703 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 150 | Batch: 000 / 037 | Total loss: 1.798 | Reg loss: 0.040 | Tree loss: 1.798 | Accuracy: 0.449219 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 151 | Batch: 000 / 037 | Total loss: 1.807 | Reg loss: 0.040 | Tree loss: 1.807 | Accuracy: 0.443359 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 152 | Batch: 000 / 037 | Total loss: 1.821 | Reg loss: 0.040 | Tree loss: 1.821 | Accuracy: 0.453125 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 153 | Batch: 000 / 037 | Total loss: 1.833 | Reg loss: 0.040 | Tree loss: 1.833 | Accuracy: 0.429688 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 154 | Batch: 000 / 037 | Total loss: 1.816 | Reg loss: 0.040 | Tree loss: 1.816 | Accuracy: 0.427734 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 155 | Batch: 000 / 037 | Total loss: 1.695 | Reg loss: 0.040 | Tree loss: 1.695 | Accuracy: 0.464844 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 156 | Batch: 000 / 037 | Total loss: 1.773 | Reg loss: 0.040 | Tree loss: 1.773 | Accuracy: 0.435547 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 157 | Batch: 000 / 037 | Total loss: 1.802 | Reg loss: 0.040 | Tree loss: 1.802 | Accuracy: 0.464844 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 158 | Batch: 000 / 037 | Total loss: 1.776 | Reg loss: 0.040 | Tree loss: 1.776 | Accuracy: 0.476562 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 159 | Batch: 000 / 037 | Total loss: 1.685 | Reg loss: 0.040 | Tree loss: 1.685 | Accuracy: 0.500000 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 160 | Batch: 000 / 037 | Total loss: 1.809 | Reg loss: 0.040 | Tree loss: 1.809 | Accuracy: 0.447266 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 161 | Batch: 000 / 037 | Total loss: 1.793 | Reg loss: 0.040 | Tree loss: 1.793 | Accuracy: 0.449219 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 162 | Batch: 000 / 037 | Total loss: 1.782 | Reg loss: 0.040 | Tree loss: 1.782 | Accuracy: 0.427734 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 163 | Batch: 000 / 037 | Total loss: 1.778 | Reg loss: 0.040 | Tree loss: 1.778 | Accuracy: 0.451172 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 164 | Batch: 000 / 037 | Total loss: 1.784 | Reg loss: 0.040 | Tree loss: 1.784 | Accuracy: 0.457031 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 165 | Batch: 000 / 037 | Total loss: 1.757 | Reg loss: 0.040 | Tree loss: 1.757 | Accuracy: 0.478516 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 166 | Batch: 000 / 037 | Total loss: 1.720 | Reg loss: 0.040 | Tree loss: 1.720 | Accuracy: 0.447266 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 167 | Batch: 000 / 037 | Total loss: 1.678 | Reg loss: 0.040 | Tree loss: 1.678 | Accuracy: 0.517578 | 0.115 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 168 | Batch: 000 / 037 | Total loss: 1.766 | Reg loss: 0.040 | Tree loss: 1.766 | Accuracy: 0.500000 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 169 | Batch: 000 / 037 | Total loss: 1.762 | Reg loss: 0.040 | Tree loss: 1.762 | Accuracy: 0.466797 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 170 | Batch: 000 / 037 | Total loss: 1.771 | Reg loss: 0.040 | Tree loss: 1.771 | Accuracy: 0.453125 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 171 | Batch: 000 / 037 | Total loss: 1.792 | Reg loss: 0.040 | Tree loss: 1.792 | Accuracy: 0.447266 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 172 | Batch: 000 / 037 | Total loss: 1.739 | Reg loss: 0.040 | Tree loss: 1.739 | Accuracy: 0.480469 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 173 | Batch: 000 / 037 | Total loss: 1.745 | Reg loss: 0.040 | Tree loss: 1.745 | Accuracy: 0.451172 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 174 | Batch: 000 / 037 | Total loss: 1.723 | Reg loss: 0.040 | Tree loss: 1.723 | Accuracy: 0.484375 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 175 | Batch: 000 / 037 | Total loss: 1.801 | Reg loss: 0.040 | Tree loss: 1.801 | Accuracy: 0.445312 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 176 | Batch: 000 / 037 | Total loss: 1.745 | Reg loss: 0.039 | Tree loss: 1.745 | Accuracy: 0.468750 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 177 | Batch: 000 / 037 | Total loss: 1.786 | Reg loss: 0.039 | Tree loss: 1.786 | Accuracy: 0.458984 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 178 | Batch: 000 / 037 | Total loss: 1.768 | Reg loss: 0.039 | Tree loss: 1.768 | Accuracy: 0.468750 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 179 | Batch: 000 / 037 | Total loss: 1.776 | Reg loss: 0.039 | Tree loss: 1.776 | Accuracy: 0.443359 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 180 | Batch: 000 / 037 | Total loss: 1.718 | Reg loss: 0.039 | Tree loss: 1.718 | Accuracy: 0.462891 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 181 | Batch: 000 / 037 | Total loss: 1.753 | Reg loss: 0.039 | Tree loss: 1.753 | Accuracy: 0.494141 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 182 | Batch: 000 / 037 | Total loss: 1.706 | Reg loss: 0.039 | Tree loss: 1.706 | Accuracy: 0.478516 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 183 | Batch: 000 / 037 | Total loss: 1.820 | Reg loss: 0.039 | Tree loss: 1.820 | Accuracy: 0.435547 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 184 | Batch: 000 / 037 | Total loss: 1.720 | Reg loss: 0.039 | Tree loss: 1.720 | Accuracy: 0.464844 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 185 | Batch: 000 / 037 | Total loss: 1.636 | Reg loss: 0.039 | Tree loss: 1.636 | Accuracy: 0.531250 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 186 | Batch: 000 / 037 | Total loss: 1.760 | Reg loss: 0.039 | Tree loss: 1.760 | Accuracy: 0.441406 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 187 | Batch: 000 / 037 | Total loss: 1.785 | Reg loss: 0.039 | Tree loss: 1.785 | Accuracy: 0.447266 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 188 | Batch: 000 / 037 | Total loss: 1.682 | Reg loss: 0.039 | Tree loss: 1.682 | Accuracy: 0.494141 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 189 | Batch: 000 / 037 | Total loss: 1.815 | Reg loss: 0.039 | Tree loss: 1.815 | Accuracy: 0.445312 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 190 | Batch: 000 / 037 | Total loss: 1.779 | Reg loss: 0.039 | Tree loss: 1.779 | Accuracy: 0.478516 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 191 | Batch: 000 / 037 | Total loss: 1.664 | Reg loss: 0.039 | Tree loss: 1.664 | Accuracy: 0.496094 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 192 | Batch: 000 / 037 | Total loss: 1.799 | Reg loss: 0.039 | Tree loss: 1.799 | Accuracy: 0.458984 | 0.116 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 193 | Batch: 000 / 037 | Total loss: 1.792 | Reg loss: 0.039 | Tree loss: 1.792 | Accuracy: 0.455078 | 0.116 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 194 | Batch: 000 / 037 | Total loss: 1.770 | Reg loss: 0.039 | Tree loss: 1.770 | Accuracy: 0.478516 | 0.116 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 195 | Batch: 000 / 037 | Total loss: 1.821 | Reg loss: 0.039 | Tree loss: 1.821 | Accuracy: 0.423828 | 0.116 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 196 | Batch: 000 / 037 | Total loss: 1.751 | Reg loss: 0.039 | Tree loss: 1.751 | Accuracy: 0.496094 | 0.116 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 197 | Batch: 000 / 037 | Total loss: 1.748 | Reg loss: 0.039 | Tree loss: 1.748 | Accuracy: 0.449219 | 0.116 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 198 | Batch: 000 / 037 | Total loss: 1.766 | Reg loss: 0.038 | Tree loss: 1.766 | Accuracy: 0.455078 | 0.116 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 199 | Batch: 000 / 037 | Total loss: 1.812 | Reg loss: 0.038 | Tree loss: 1.812 | Accuracy: 0.423828 | 0.116 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 200 | Batch: 000 / 037 | Total loss: 1.797 | Reg loss: 0.038 | Tree loss: 1.797 | Accuracy: 0.421875 | 0.116 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 201 | Batch: 000 / 037 | Total loss: 1.730 | Reg loss: 0.038 | Tree loss: 1.730 | Accuracy: 0.460938 | 0.116 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 202 | Batch: 000 / 037 | Total loss: 1.786 | Reg loss: 0.038 | Tree loss: 1.786 | Accuracy: 0.457031 | 0.116 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 203 | Batch: 000 / 037 | Total loss: 1.735 | Reg loss: 0.038 | Tree loss: 1.735 | Accuracy: 0.482422 | 0.116 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 204 | Batch: 000 / 037 | Total loss: 1.784 | Reg loss: 0.038 | Tree loss: 1.784 | Accuracy: 0.425781 | 0.116 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 205 | Batch: 000 / 037 | Total loss: 1.803 | Reg loss: 0.038 | Tree loss: 1.803 | Accuracy: 0.433594 | 0.116 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 206 | Batch: 000 / 037 | Total loss: 1.752 | Reg loss: 0.038 | Tree loss: 1.752 | Accuracy: 0.451172 | 0.116 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 207 | Batch: 000 / 037 | Total loss: 1.773 | Reg loss: 0.038 | Tree loss: 1.773 | Accuracy: 0.468750 | 0.116 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 208 | Batch: 000 / 037 | Total loss: 1.763 | Reg loss: 0.038 | Tree loss: 1.763 | Accuracy: 0.474609 | 0.116 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 209 | Batch: 000 / 037 | Total loss: 1.773 | Reg loss: 0.038 | Tree loss: 1.773 | Accuracy: 0.447266 | 0.116 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 210 | Batch: 000 / 037 | Total loss: 1.777 | Reg loss: 0.038 | Tree loss: 1.777 | Accuracy: 0.457031 | 0.116 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 211 | Batch: 000 / 037 | Total loss: 1.779 | Reg loss: 0.038 | Tree loss: 1.779 | Accuracy: 0.431641 | 0.116 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 212 | Batch: 000 / 037 | Total loss: 1.798 | Reg loss: 0.038 | Tree loss: 1.798 | Accuracy: 0.455078 | 0.116 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 213 | Batch: 000 / 037 | Total loss: 1.773 | Reg loss: 0.038 | Tree loss: 1.773 | Accuracy: 0.457031 | 0.116 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 214 | Batch: 000 / 037 | Total loss: 1.794 | Reg loss: 0.038 | Tree loss: 1.794 | Accuracy: 0.451172 | 0.116 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 215 | Batch: 000 / 037 | Total loss: 1.766 | Reg loss: 0.038 | Tree loss: 1.766 | Accuracy: 0.464844 | 0.116 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 216 | Batch: 000 / 037 | Total loss: 1.718 | Reg loss: 0.038 | Tree loss: 1.718 | Accuracy: 0.480469 | 0.116 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 217 | Batch: 000 / 037 | Total loss: 1.786 | Reg loss: 0.038 | Tree loss: 1.786 | Accuracy: 0.451172 | 0.116 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 218 | Batch: 000 / 037 | Total loss: 1.736 | Reg loss: 0.038 | Tree loss: 1.736 | Accuracy: 0.455078 | 0.116 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 219 | Batch: 000 / 037 | Total loss: 1.838 | Reg loss: 0.038 | Tree loss: 1.838 | Accuracy: 0.423828 | 0.116 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 220 | Batch: 000 / 037 | Total loss: 1.738 | Reg loss: 0.038 | Tree loss: 1.738 | Accuracy: 0.455078 | 0.116 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 221 | Batch: 000 / 037 | Total loss: 1.740 | Reg loss: 0.038 | Tree loss: 1.740 | Accuracy: 0.468750 | 0.116 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 222 | Batch: 000 / 037 | Total loss: 1.685 | Reg loss: 0.038 | Tree loss: 1.685 | Accuracy: 0.480469 | 0.116 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 223 | Batch: 000 / 037 | Total loss: 1.755 | Reg loss: 0.038 | Tree loss: 1.755 | Accuracy: 0.453125 | 0.116 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 224 | Batch: 000 / 037 | Total loss: 1.688 | Reg loss: 0.038 | Tree loss: 1.688 | Accuracy: 0.515625 | 0.116 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 225 | Batch: 000 / 037 | Total loss: 1.742 | Reg loss: 0.038 | Tree loss: 1.742 | Accuracy: 0.466797 | 0.116 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 226 | Batch: 000 / 037 | Total loss: 1.777 | Reg loss: 0.037 | Tree loss: 1.777 | Accuracy: 0.447266 | 0.116 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 227 | Batch: 000 / 037 | Total loss: 1.797 | Reg loss: 0.037 | Tree loss: 1.797 | Accuracy: 0.449219 | 0.116 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 228 | Batch: 000 / 037 | Total loss: 1.743 | Reg loss: 0.037 | Tree loss: 1.743 | Accuracy: 0.460938 | 0.116 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 229 | Batch: 000 / 037 | Total loss: 1.747 | Reg loss: 0.037 | Tree loss: 1.747 | Accuracy: 0.453125 | 0.116 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 230 | Batch: 000 / 037 | Total loss: 1.729 | Reg loss: 0.037 | Tree loss: 1.729 | Accuracy: 0.484375 | 0.116 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 231 | Batch: 000 / 037 | Total loss: 1.736 | Reg loss: 0.037 | Tree loss: 1.736 | Accuracy: 0.457031 | 0.116 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 232 | Batch: 000 / 037 | Total loss: 1.726 | Reg loss: 0.037 | Tree loss: 1.726 | Accuracy: 0.498047 | 0.116 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 233 | Batch: 000 / 037 | Total loss: 1.798 | Reg loss: 0.037 | Tree loss: 1.798 | Accuracy: 0.437500 | 0.116 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 234 | Batch: 000 / 037 | Total loss: 1.744 | Reg loss: 0.037 | Tree loss: 1.744 | Accuracy: 0.478516 | 0.116 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 235 | Batch: 000 / 037 | Total loss: 1.755 | Reg loss: 0.037 | Tree loss: 1.755 | Accuracy: 0.494141 | 0.116 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 236 | Batch: 000 / 037 | Total loss: 1.750 | Reg loss: 0.037 | Tree loss: 1.750 | Accuracy: 0.476562 | 0.116 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 237 | Batch: 000 / 037 | Total loss: 1.782 | Reg loss: 0.037 | Tree loss: 1.782 | Accuracy: 0.429688 | 0.116 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 238 | Batch: 000 / 037 | Total loss: 1.735 | Reg loss: 0.037 | Tree loss: 1.735 | Accuracy: 0.427734 | 0.116 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 239 | Batch: 000 / 037 | Total loss: 1.818 | Reg loss: 0.037 | Tree loss: 1.818 | Accuracy: 0.460938 | 0.116 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 240 | Batch: 000 / 037 | Total loss: 1.787 | Reg loss: 0.037 | Tree loss: 1.787 | Accuracy: 0.455078 | 0.116 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 241 | Batch: 000 / 037 | Total loss: 1.747 | Reg loss: 0.037 | Tree loss: 1.747 | Accuracy: 0.482422 | 0.116 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 242 | Batch: 000 / 037 | Total loss: 1.811 | Reg loss: 0.037 | Tree loss: 1.811 | Accuracy: 0.445312 | 0.116 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 243 | Batch: 000 / 037 | Total loss: 1.765 | Reg loss: 0.037 | Tree loss: 1.765 | Accuracy: 0.496094 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 244 | Batch: 000 / 037 | Total loss: 1.818 | Reg loss: 0.037 | Tree loss: 1.818 | Accuracy: 0.443359 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 245 | Batch: 000 / 037 | Total loss: 1.776 | Reg loss: 0.037 | Tree loss: 1.776 | Accuracy: 0.474609 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 246 | Batch: 000 / 037 | Total loss: 1.724 | Reg loss: 0.037 | Tree loss: 1.724 | Accuracy: 0.488281 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 247 | Batch: 000 / 037 | Total loss: 1.749 | Reg loss: 0.036 | Tree loss: 1.749 | Accuracy: 0.458984 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 248 | Batch: 000 / 037 | Total loss: 1.707 | Reg loss: 0.036 | Tree loss: 1.707 | Accuracy: 0.482422 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 249 | Batch: 000 / 037 | Total loss: 1.818 | Reg loss: 0.036 | Tree loss: 1.818 | Accuracy: 0.453125 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 250 | Batch: 000 / 037 | Total loss: 1.775 | Reg loss: 0.036 | Tree loss: 1.775 | Accuracy: 0.449219 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 251 | Batch: 000 / 037 | Total loss: 1.696 | Reg loss: 0.036 | Tree loss: 1.696 | Accuracy: 0.480469 | 0.115 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 252 | Batch: 000 / 037 | Total loss: 1.759 | Reg loss: 0.036 | Tree loss: 1.759 | Accuracy: 0.482422 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 253 | Batch: 000 / 037 | Total loss: 1.834 | Reg loss: 0.036 | Tree loss: 1.834 | Accuracy: 0.445312 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 254 | Batch: 000 / 037 | Total loss: 1.752 | Reg loss: 0.036 | Tree loss: 1.752 | Accuracy: 0.462891 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 255 | Batch: 000 / 037 | Total loss: 1.771 | Reg loss: 0.036 | Tree loss: 1.771 | Accuracy: 0.484375 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 256 | Batch: 000 / 037 | Total loss: 1.719 | Reg loss: 0.036 | Tree loss: 1.719 | Accuracy: 0.490234 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 257 | Batch: 000 / 037 | Total loss: 1.794 | Reg loss: 0.036 | Tree loss: 1.794 | Accuracy: 0.441406 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 258 | Batch: 000 / 037 | Total loss: 1.778 | Reg loss: 0.036 | Tree loss: 1.778 | Accuracy: 0.484375 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 259 | Batch: 000 / 037 | Total loss: 1.784 | Reg loss: 0.036 | Tree loss: 1.784 | Accuracy: 0.468750 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 260 | Batch: 000 / 037 | Total loss: 1.753 | Reg loss: 0.036 | Tree loss: 1.753 | Accuracy: 0.484375 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 261 | Batch: 000 / 037 | Total loss: 1.760 | Reg loss: 0.036 | Tree loss: 1.760 | Accuracy: 0.472656 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 262 | Batch: 000 / 037 | Total loss: 1.753 | Reg loss: 0.036 | Tree loss: 1.753 | Accuracy: 0.417969 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 263 | Batch: 000 / 037 | Total loss: 1.791 | Reg loss: 0.036 | Tree loss: 1.791 | Accuracy: 0.466797 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 264 | Batch: 000 / 037 | Total loss: 1.764 | Reg loss: 0.036 | Tree loss: 1.764 | Accuracy: 0.437500 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 265 | Batch: 000 / 037 | Total loss: 1.839 | Reg loss: 0.036 | Tree loss: 1.839 | Accuracy: 0.455078 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 266 | Batch: 000 / 037 | Total loss: 1.772 | Reg loss: 0.036 | Tree loss: 1.772 | Accuracy: 0.462891 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 267 | Batch: 000 / 037 | Total loss: 1.765 | Reg loss: 0.036 | Tree loss: 1.765 | Accuracy: 0.468750 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 268 | Batch: 000 / 037 | Total loss: 1.760 | Reg loss: 0.036 | Tree loss: 1.760 | Accuracy: 0.480469 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 269 | Batch: 000 / 037 | Total loss: 1.794 | Reg loss: 0.036 | Tree loss: 1.794 | Accuracy: 0.470703 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 270 | Batch: 000 / 037 | Total loss: 1.713 | Reg loss: 0.036 | Tree loss: 1.713 | Accuracy: 0.470703 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 271 | Batch: 000 / 037 | Total loss: 1.823 | Reg loss: 0.036 | Tree loss: 1.823 | Accuracy: 0.482422 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 272 | Batch: 000 / 037 | Total loss: 1.724 | Reg loss: 0.036 | Tree loss: 1.724 | Accuracy: 0.464844 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 273 | Batch: 000 / 037 | Total loss: 1.695 | Reg loss: 0.036 | Tree loss: 1.695 | Accuracy: 0.474609 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 274 | Batch: 000 / 037 | Total loss: 1.717 | Reg loss: 0.036 | Tree loss: 1.717 | Accuracy: 0.501953 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 275 | Batch: 000 / 037 | Total loss: 1.779 | Reg loss: 0.036 | Tree loss: 1.779 | Accuracy: 0.458984 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 276 | Batch: 000 / 037 | Total loss: 1.766 | Reg loss: 0.036 | Tree loss: 1.766 | Accuracy: 0.470703 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 277 | Batch: 000 / 037 | Total loss: 1.748 | Reg loss: 0.036 | Tree loss: 1.748 | Accuracy: 0.458984 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 278 | Batch: 000 / 037 | Total loss: 1.769 | Reg loss: 0.036 | Tree loss: 1.769 | Accuracy: 0.441406 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 279 | Batch: 000 / 037 | Total loss: 1.758 | Reg loss: 0.036 | Tree loss: 1.758 | Accuracy: 0.457031 | 0.115 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 280 | Batch: 000 / 037 | Total loss: 1.754 | Reg loss: 0.036 | Tree loss: 1.754 | Accuracy: 0.470703 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 281 | Batch: 000 / 037 | Total loss: 1.772 | Reg loss: 0.036 | Tree loss: 1.772 | Accuracy: 0.480469 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 282 | Batch: 000 / 037 | Total loss: 1.729 | Reg loss: 0.036 | Tree loss: 1.729 | Accuracy: 0.470703 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 283 | Batch: 000 / 037 | Total loss: 1.822 | Reg loss: 0.036 | Tree loss: 1.822 | Accuracy: 0.455078 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 284 | Batch: 000 / 037 | Total loss: 1.770 | Reg loss: 0.036 | Tree loss: 1.770 | Accuracy: 0.472656 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 285 | Batch: 000 / 037 | Total loss: 1.788 | Reg loss: 0.036 | Tree loss: 1.788 | Accuracy: 0.439453 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 286 | Batch: 000 / 037 | Total loss: 1.838 | Reg loss: 0.036 | Tree loss: 1.838 | Accuracy: 0.429688 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 287 | Batch: 000 / 037 | Total loss: 1.699 | Reg loss: 0.036 | Tree loss: 1.699 | Accuracy: 0.474609 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 288 | Batch: 000 / 037 | Total loss: 1.737 | Reg loss: 0.036 | Tree loss: 1.737 | Accuracy: 0.453125 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 289 | Batch: 000 / 037 | Total loss: 1.809 | Reg loss: 0.036 | Tree loss: 1.809 | Accuracy: 0.437500 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 290 | Batch: 000 / 037 | Total loss: 1.773 | Reg loss: 0.036 | Tree loss: 1.773 | Accuracy: 0.468750 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 291 | Batch: 000 / 037 | Total loss: 1.803 | Reg loss: 0.036 | Tree loss: 1.803 | Accuracy: 0.457031 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 292 | Batch: 000 / 037 | Total loss: 1.820 | Reg loss: 0.036 | Tree loss: 1.820 | Accuracy: 0.439453 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 293 | Batch: 000 / 037 | Total loss: 1.723 | Reg loss: 0.036 | Tree loss: 1.723 | Accuracy: 0.486328 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 294 | Batch: 000 / 037 | Total loss: 1.767 | Reg loss: 0.036 | Tree loss: 1.767 | Accuracy: 0.507812 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 295 | Batch: 000 / 037 | Total loss: 1.760 | Reg loss: 0.036 | Tree loss: 1.760 | Accuracy: 0.455078 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 296 | Batch: 000 / 037 | Total loss: 1.792 | Reg loss: 0.036 | Tree loss: 1.792 | Accuracy: 0.460938 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 297 | Batch: 000 / 037 | Total loss: 1.738 | Reg loss: 0.036 | Tree loss: 1.738 | Accuracy: 0.480469 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 298 | Batch: 000 / 037 | Total loss: 1.780 | Reg loss: 0.036 | Tree loss: 1.780 | Accuracy: 0.431641 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 299 | Batch: 000 / 037 | Total loss: 1.805 | Reg loss: 0.036 | Tree loss: 1.805 | Accuracy: 0.429688 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 300 | Batch: 000 / 037 | Total loss: 1.775 | Reg loss: 0.036 | Tree loss: 1.775 | Accuracy: 0.482422 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 301 | Batch: 000 / 037 | Total loss: 1.724 | Reg loss: 0.036 | Tree loss: 1.724 | Accuracy: 0.482422 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 302 | Batch: 000 / 037 | Total loss: 1.727 | Reg loss: 0.036 | Tree loss: 1.727 | Accuracy: 0.466797 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 303 | Batch: 000 / 037 | Total loss: 1.745 | Reg loss: 0.036 | Tree loss: 1.745 | Accuracy: 0.462891 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 304 | Batch: 000 / 037 | Total loss: 1.774 | Reg loss: 0.036 | Tree loss: 1.774 | Accuracy: 0.480469 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 305 | Batch: 000 / 037 | Total loss: 1.804 | Reg loss: 0.036 | Tree loss: 1.804 | Accuracy: 0.445312 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 306 | Batch: 000 / 037 | Total loss: 1.778 | Reg loss: 0.036 | Tree loss: 1.778 | Accuracy: 0.427734 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 307 | Batch: 000 / 037 | Total loss: 1.753 | Reg loss: 0.036 | Tree loss: 1.753 | Accuracy: 0.480469 | 0.115 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 308 | Batch: 000 / 037 | Total loss: 1.761 | Reg loss: 0.036 | Tree loss: 1.761 | Accuracy: 0.460938 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 309 | Batch: 000 / 037 | Total loss: 1.805 | Reg loss: 0.036 | Tree loss: 1.805 | Accuracy: 0.457031 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 310 | Batch: 000 / 037 | Total loss: 1.837 | Reg loss: 0.036 | Tree loss: 1.837 | Accuracy: 0.412109 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 311 | Batch: 000 / 037 | Total loss: 1.742 | Reg loss: 0.036 | Tree loss: 1.742 | Accuracy: 0.462891 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 312 | Batch: 000 / 037 | Total loss: 1.797 | Reg loss: 0.036 | Tree loss: 1.797 | Accuracy: 0.458984 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 313 | Batch: 000 / 037 | Total loss: 1.758 | Reg loss: 0.036 | Tree loss: 1.758 | Accuracy: 0.455078 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 314 | Batch: 000 / 037 | Total loss: 1.738 | Reg loss: 0.036 | Tree loss: 1.738 | Accuracy: 0.441406 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 315 | Batch: 000 / 037 | Total loss: 1.798 | Reg loss: 0.036 | Tree loss: 1.798 | Accuracy: 0.449219 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 316 | Batch: 000 / 037 | Total loss: 1.719 | Reg loss: 0.036 | Tree loss: 1.719 | Accuracy: 0.478516 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 317 | Batch: 000 / 037 | Total loss: 1.756 | Reg loss: 0.036 | Tree loss: 1.756 | Accuracy: 0.494141 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 318 | Batch: 000 / 037 | Total loss: 1.747 | Reg loss: 0.036 | Tree loss: 1.747 | Accuracy: 0.486328 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 319 | Batch: 000 / 037 | Total loss: 1.775 | Reg loss: 0.036 | Tree loss: 1.775 | Accuracy: 0.470703 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 320 | Batch: 000 / 037 | Total loss: 1.807 | Reg loss: 0.036 | Tree loss: 1.807 | Accuracy: 0.437500 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 321 | Batch: 000 / 037 | Total loss: 1.730 | Reg loss: 0.036 | Tree loss: 1.730 | Accuracy: 0.466797 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 322 | Batch: 000 / 037 | Total loss: 1.780 | Reg loss: 0.036 | Tree loss: 1.780 | Accuracy: 0.466797 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 323 | Batch: 000 / 037 | Total loss: 1.732 | Reg loss: 0.036 | Tree loss: 1.732 | Accuracy: 0.468750 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 324 | Batch: 000 / 037 | Total loss: 1.768 | Reg loss: 0.036 | Tree loss: 1.768 | Accuracy: 0.468750 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 325 | Batch: 000 / 037 | Total loss: 1.748 | Reg loss: 0.036 | Tree loss: 1.748 | Accuracy: 0.429688 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 326 | Batch: 000 / 037 | Total loss: 1.763 | Reg loss: 0.036 | Tree loss: 1.763 | Accuracy: 0.462891 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 327 | Batch: 000 / 037 | Total loss: 1.723 | Reg loss: 0.036 | Tree loss: 1.723 | Accuracy: 0.464844 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 328 | Batch: 000 / 037 | Total loss: 1.754 | Reg loss: 0.036 | Tree loss: 1.754 | Accuracy: 0.466797 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 329 | Batch: 000 / 037 | Total loss: 1.793 | Reg loss: 0.036 | Tree loss: 1.793 | Accuracy: 0.445312 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 330 | Batch: 000 / 037 | Total loss: 1.777 | Reg loss: 0.036 | Tree loss: 1.777 | Accuracy: 0.470703 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 331 | Batch: 000 / 037 | Total loss: 1.791 | Reg loss: 0.036 | Tree loss: 1.791 | Accuracy: 0.501953 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 332 | Batch: 000 / 037 | Total loss: 1.816 | Reg loss: 0.036 | Tree loss: 1.816 | Accuracy: 0.429688 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 333 | Batch: 000 / 037 | Total loss: 1.775 | Reg loss: 0.036 | Tree loss: 1.775 | Accuracy: 0.453125 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 334 | Batch: 000 / 037 | Total loss: 1.847 | Reg loss: 0.036 | Tree loss: 1.847 | Accuracy: 0.439453 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 335 | Batch: 000 / 037 | Total loss: 1.812 | Reg loss: 0.036 | Tree loss: 1.812 | Accuracy: 0.449219 | 0.115 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 336 | Batch: 000 / 037 | Total loss: 1.783 | Reg loss: 0.036 | Tree loss: 1.783 | Accuracy: 0.466797 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 337 | Batch: 000 / 037 | Total loss: 1.768 | Reg loss: 0.036 | Tree loss: 1.768 | Accuracy: 0.457031 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 338 | Batch: 000 / 037 | Total loss: 1.751 | Reg loss: 0.036 | Tree loss: 1.751 | Accuracy: 0.470703 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 339 | Batch: 000 / 037 | Total loss: 1.788 | Reg loss: 0.036 | Tree loss: 1.788 | Accuracy: 0.458984 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 340 | Batch: 000 / 037 | Total loss: 1.782 | Reg loss: 0.036 | Tree loss: 1.782 | Accuracy: 0.500000 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 341 | Batch: 000 / 037 | Total loss: 1.709 | Reg loss: 0.036 | Tree loss: 1.709 | Accuracy: 0.482422 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 342 | Batch: 000 / 037 | Total loss: 1.785 | Reg loss: 0.036 | Tree loss: 1.785 | Accuracy: 0.451172 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 343 | Batch: 000 / 037 | Total loss: 1.801 | Reg loss: 0.036 | Tree loss: 1.801 | Accuracy: 0.441406 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 344 | Batch: 000 / 037 | Total loss: 1.768 | Reg loss: 0.036 | Tree loss: 1.768 | Accuracy: 0.494141 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 345 | Batch: 000 / 037 | Total loss: 1.719 | Reg loss: 0.036 | Tree loss: 1.719 | Accuracy: 0.476562 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 346 | Batch: 000 / 037 | Total loss: 1.726 | Reg loss: 0.036 | Tree loss: 1.726 | Accuracy: 0.511719 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 347 | Batch: 000 / 037 | Total loss: 1.772 | Reg loss: 0.036 | Tree loss: 1.772 | Accuracy: 0.460938 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 348 | Batch: 000 / 037 | Total loss: 1.781 | Reg loss: 0.036 | Tree loss: 1.781 | Accuracy: 0.419922 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 349 | Batch: 000 / 037 | Total loss: 1.780 | Reg loss: 0.036 | Tree loss: 1.780 | Accuracy: 0.464844 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 350 | Batch: 000 / 037 | Total loss: 1.770 | Reg loss: 0.036 | Tree loss: 1.770 | Accuracy: 0.466797 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 351 | Batch: 000 / 037 | Total loss: 1.708 | Reg loss: 0.036 | Tree loss: 1.708 | Accuracy: 0.507812 | 0.115 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 352 | Batch: 000 / 037 | Total loss: 1.748 | Reg loss: 0.036 | Tree loss: 1.748 | Accuracy: 0.449219 | 0.116 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 353 | Batch: 000 / 037 | Total loss: 1.748 | Reg loss: 0.036 | Tree loss: 1.748 | Accuracy: 0.480469 | 0.116 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 354 | Batch: 000 / 037 | Total loss: 1.811 | Reg loss: 0.036 | Tree loss: 1.811 | Accuracy: 0.453125 | 0.116 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 355 | Batch: 000 / 037 | Total loss: 1.761 | Reg loss: 0.036 | Tree loss: 1.761 | Accuracy: 0.445312 | 0.116 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 356 | Batch: 000 / 037 | Total loss: 1.757 | Reg loss: 0.036 | Tree loss: 1.757 | Accuracy: 0.457031 | 0.116 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 357 | Batch: 000 / 037 | Total loss: 1.788 | Reg loss: 0.036 | Tree loss: 1.788 | Accuracy: 0.453125 | 0.116 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 358 | Batch: 000 / 037 | Total loss: 1.729 | Reg loss: 0.036 | Tree loss: 1.729 | Accuracy: 0.488281 | 0.116 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 359 | Batch: 000 / 037 | Total loss: 1.749 | Reg loss: 0.036 | Tree loss: 1.749 | Accuracy: 0.466797 | 0.116 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 360 | Batch: 000 / 037 | Total loss: 1.763 | Reg loss: 0.036 | Tree loss: 1.763 | Accuracy: 0.453125 | 0.116 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 361 | Batch: 000 / 037 | Total loss: 1.728 | Reg loss: 0.036 | Tree loss: 1.728 | Accuracy: 0.468750 | 0.116 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 362 | Batch: 000 / 037 | Total loss: 1.746 | Reg loss: 0.036 | Tree loss: 1.746 | Accuracy: 0.472656 | 0.116 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 363 | Batch: 000 / 037 | Total loss: 1.727 | Reg loss: 0.036 | Tree loss: 1.727 | Accuracy: 0.449219 | 0.116 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 364 | Batch: 000 / 037 | Total loss: 1.762 | Reg loss: 0.036 | Tree loss: 1.762 | Accuracy: 0.466797 | 0.116 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 365 | Batch: 000 / 037 | Total loss: 1.751 | Reg loss: 0.036 | Tree loss: 1.751 | Accuracy: 0.462891 | 0.116 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 366 | Batch: 000 / 037 | Total loss: 1.766 | Reg loss: 0.036 | Tree loss: 1.766 | Accuracy: 0.443359 | 0.116 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 367 | Batch: 000 / 037 | Total loss: 1.731 | Reg loss: 0.036 | Tree loss: 1.731 | Accuracy: 0.472656 | 0.116 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 368 | Batch: 000 / 037 | Total loss: 1.806 | Reg loss: 0.036 | Tree loss: 1.806 | Accuracy: 0.406250 | 0.116 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 369 | Batch: 000 / 037 | Total loss: 1.766 | Reg loss: 0.036 | Tree loss: 1.766 | Accuracy: 0.484375 | 0.116 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 370 | Batch: 000 / 037 | Total loss: 1.717 | Reg loss: 0.036 | Tree loss: 1.717 | Accuracy: 0.464844 | 0.116 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 371 | Batch: 000 / 037 | Total loss: 1.797 | Reg loss: 0.036 | Tree loss: 1.797 | Accuracy: 0.443359 | 0.116 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 372 | Batch: 000 / 037 | Total loss: 1.729 | Reg loss: 0.036 | Tree loss: 1.729 | Accuracy: 0.449219 | 0.116 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 373 | Batch: 000 / 037 | Total loss: 1.793 | Reg loss: 0.036 | Tree loss: 1.793 | Accuracy: 0.435547 | 0.116 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 374 | Batch: 000 / 037 | Total loss: 1.691 | Reg loss: 0.036 | Tree loss: 1.691 | Accuracy: 0.494141 | 0.116 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 375 | Batch: 000 / 037 | Total loss: 1.759 | Reg loss: 0.036 | Tree loss: 1.759 | Accuracy: 0.464844 | 0.116 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 376 | Batch: 000 / 037 | Total loss: 1.707 | Reg loss: 0.036 | Tree loss: 1.707 | Accuracy: 0.494141 | 0.116 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 377 | Batch: 000 / 037 | Total loss: 1.726 | Reg loss: 0.036 | Tree loss: 1.726 | Accuracy: 0.482422 | 0.116 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 378 | Batch: 000 / 037 | Total loss: 1.761 | Reg loss: 0.036 | Tree loss: 1.761 | Accuracy: 0.470703 | 0.116 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 379 | Batch: 000 / 037 | Total loss: 1.778 | Reg loss: 0.036 | Tree loss: 1.778 | Accuracy: 0.462891 | 0.116 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 380 | Batch: 000 / 037 | Total loss: 1.711 | Reg loss: 0.036 | Tree loss: 1.711 | Accuracy: 0.476562 | 0.116 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 381 | Batch: 000 / 037 | Total loss: 1.786 | Reg loss: 0.036 | Tree loss: 1.786 | Accuracy: 0.468750 | 0.116 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 382 | Batch: 000 / 037 | Total loss: 1.812 | Reg loss: 0.036 | Tree loss: 1.812 | Accuracy: 0.439453 | 0.116 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 383 | Batch: 000 / 037 | Total loss: 1.790 | Reg loss: 0.036 | Tree loss: 1.790 | Accuracy: 0.449219 | 0.116 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 384 | Batch: 000 / 037 | Total loss: 1.855 | Reg loss: 0.036 | Tree loss: 1.855 | Accuracy: 0.421875 | 0.116 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 385 | Batch: 000 / 037 | Total loss: 1.754 | Reg loss: 0.036 | Tree loss: 1.754 | Accuracy: 0.449219 | 0.116 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 386 | Batch: 000 / 037 | Total loss: 1.815 | Reg loss: 0.036 | Tree loss: 1.815 | Accuracy: 0.458984 | 0.116 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 387 | Batch: 000 / 037 | Total loss: 1.812 | Reg loss: 0.036 | Tree loss: 1.812 | Accuracy: 0.419922 | 0.116 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 388 | Batch: 000 / 037 | Total loss: 1.854 | Reg loss: 0.036 | Tree loss: 1.854 | Accuracy: 0.433594 | 0.116 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 389 | Batch: 000 / 037 | Total loss: 1.705 | Reg loss: 0.036 | Tree loss: 1.705 | Accuracy: 0.496094 | 0.116 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 390 | Batch: 000 / 037 | Total loss: 1.794 | Reg loss: 0.036 | Tree loss: 1.794 | Accuracy: 0.427734 | 0.116 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 391 | Batch: 000 / 037 | Total loss: 1.791 | Reg loss: 0.036 | Tree loss: 1.791 | Accuracy: 0.449219 | 0.116 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 392 | Batch: 000 / 037 | Total loss: 1.769 | Reg loss: 0.036 | Tree loss: 1.769 | Accuracy: 0.447266 | 0.116 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 393 | Batch: 000 / 037 | Total loss: 1.743 | Reg loss: 0.036 | Tree loss: 1.743 | Accuracy: 0.464844 | 0.116 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 394 | Batch: 000 / 037 | Total loss: 1.727 | Reg loss: 0.036 | Tree loss: 1.727 | Accuracy: 0.507812 | 0.116 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 395 | Batch: 000 / 037 | Total loss: 1.732 | Reg loss: 0.036 | Tree loss: 1.732 | Accuracy: 0.488281 | 0.116 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 396 | Batch: 000 / 037 | Total loss: 1.750 | Reg loss: 0.036 | Tree loss: 1.750 | Accuracy: 0.484375 | 0.116 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 397 | Batch: 000 / 037 | Total loss: 1.771 | Reg loss: 0.036 | Tree loss: 1.771 | Accuracy: 0.484375 | 0.116 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 398 | Batch: 000 / 037 | Total loss: 1.793 | Reg loss: 0.036 | Tree loss: 1.793 | Accuracy: 0.447266 | 0.116 sec/iter\n",
      "Average sparseness: 0.984042553191489\n",
      "layer 0: 0.9840425531914894\n",
      "layer 1: 0.9840425531914894\n",
      "layer 2: 0.9840425531914894\n",
      "layer 3: 0.9840425531914894\n",
      "layer 4: 0.9840425531914894\n",
      "Epoch: 399 | Batch: 000 / 037 | Total loss: 1.793 | Reg loss: 0.036 | Tree loss: 1.793 | Accuracy: 0.453125 | 0.116 sec/iter\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "iteration = 0\n",
    "for epoch in range(epochs):\n",
    "    # Training\n",
    "    avg_sp = show_sparseness(tree)\n",
    "    sparsity.append(avg_sp)\n",
    "    iteration = do_epoch(tree, tree_loader, device, log_interval, losses, accs, epoch, iteration)\n",
    "    \n",
    "    if epoch % 1 == 0:\n",
    "        prune_tree(tree, factor=3)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eitan\\Anaconda3\\envs\\research\\lib\\site-packages\\ipykernel_launcher.py:1: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2be9f927c4a4e718cb98176e4db1569",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel('Iteration')\n",
    "plt.plot(accs, label='Accuracy vs iteration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eitan\\Anaconda3\\envs\\research\\lib\\site-packages\\ipykernel_launcher.py:1: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b57609ccb5b4651a27a30c5b88ee4cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eitan\\Anaconda3\\envs\\research\\lib\\site-packages\\ipykernel_launcher.py:8: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89fc9a3d73264a4691fbc13744f4aba1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel('Iteration')\n",
    "plt.plot(losses, label='Loss vs iteration')\n",
    "# plt.yscale(\"log\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "weights = tree.inner_nodes.weight.cpu().detach().numpy().flatten()\n",
    "plt.hist(weights, bins=500)\n",
    "weights_std = np.std(weights)\n",
    "weights_mean = np.mean(weights)\n",
    "plt.axvline(weights_mean + weights_std, color='r')\n",
    "plt.axvline(weights_mean - weights_std, color='r')\n",
    "plt.title(f\"Mean: {weights_mean}   |   STD: {weights_std}\")\n",
    "plt.yscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eitan\\Anaconda3\\envs\\research\\lib\\site-packages\\ipykernel_launcher.py:1: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "865ce2276aa54f8bb66e11da7407e2ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average height: 5.117647058823529\n"
     ]
    }
   ],
   "source": [
    "plt.figure(figsize=(15, 10), dpi=80)\n",
    "avg_height, root = tree.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accumulate samples in the leaves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of patterns: {len(root.get_leaves())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = 'greedy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eitan\\Anaconda3\\envs\\research\\lib\\site-packages\\torch\\nn\\functional.py:1709: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    }
   ],
   "source": [
    "root.clear_leaves_samples()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (data, target) in enumerate(tree_loader):\n",
    "        root.accumulate_samples(data, method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tighten boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-96-82137e73da49>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mattr_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# print(attr_names)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mleaves\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mroot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_leaves\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0msum_comprehensibility\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "attr_names = dataset.items\n",
    "\n",
    "# print(attr_names)\n",
    "leaves = root.get_leaves()\n",
    "sum_comprehensibility = 0\n",
    "comprehensibilities = []\n",
    "for pattern_counter, leaf in enumerate(leaves):\n",
    "    leaf.reset_path()\n",
    "    leaf.tighten_with_accumulated_samples()\n",
    "    conds = leaf.get_path_conditions(attr_names)\n",
    "    print(f\"============== Pattern {pattern_counter + 1} ==============\")\n",
    "    comprehensibilities.append(sum([cond.comprehensibility for cond in conds]))\n",
    "    \n",
    "print(f\"Average comprehensibility: {np.mean(comprehensibilities)}\")\n",
    "print(f\"std comprehensibility: {np.std(comprehensibilities)}\")\n",
    "print(f\"var comprehensibility: {np.var(comprehensibilities)}\")\n",
    "print(f\"minimum comprehensibility: {np.min(comprehensibilities)}\")\n",
    "print(f\"maximum comprehensibility: {np.max(comprehensibilities)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
