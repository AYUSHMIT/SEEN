{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib widget\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import torch.nn as nn\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from stream_generators.market_basket_dataset import MarketBasketDataset, BinaryEncodingTransform, RemoveItemsTransform\n",
    "from utils.MatplotlibUtils import reduce_dims_and_plot\n",
    "from network.auto_encoder import AutoEncoder\n",
    "from losses.knn_loss import KNNLoss\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from soft_decision_tree.sdt_model import SDT\n",
    "from sklearn.metrics import davies_bouldin_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 8\n",
    "tree_depth = 10\n",
    "device = 'cuda'\n",
    "dataset_path = r\"/mnt/qnap/ekosman/Groceries_dataset.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the market basket dataset and use one-hot encoding for items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MarketBasketDataset(dataset_path=dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoEncoder(dataset.n_items, 50, 4).train().to(device)\n",
    "epochs = 500\n",
    "lr = 5e-3\n",
    "batch_size = 512\n",
    "log_every = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.transform = torchvision.transforms.Compose([\n",
    "    RemoveItemsTransform(p=0.5),\n",
    "    BinaryEncodingTransform(mapping=dataset.items_to_idx),\n",
    "]\n",
    ")\n",
    "dataset.target_transform = torchvision.transforms.Compose([\n",
    "    BinaryEncodingTransform(mapping=dataset.items_to_idx),\n",
    "]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 / 500 | iteration 0 / 30 | Total Loss: 8.206886291503906 | KNN Loss: 6.2244954109191895 | BCE Loss: 1.9823912382125854\n",
      "Epoch 0 / 500 | iteration 5 / 30 | Total Loss: 8.132919311523438 | KNN Loss: 6.224689483642578 | BCE Loss: 1.9082294702529907\n",
      "Epoch 0 / 500 | iteration 10 / 30 | Total Loss: 8.164770126342773 | KNN Loss: 6.224438190460205 | BCE Loss: 1.9403314590454102\n",
      "Epoch 0 / 500 | iteration 15 / 30 | Total Loss: 8.090507507324219 | KNN Loss: 6.223604202270508 | BCE Loss: 1.8669028282165527\n",
      "Epoch 0 / 500 | iteration 20 / 30 | Total Loss: 8.19241714477539 | KNN Loss: 6.222877025604248 | BCE Loss: 1.9695402383804321\n",
      "Epoch 0 / 500 | iteration 25 / 30 | Total Loss: 8.14727783203125 | KNN Loss: 6.222434997558594 | BCE Loss: 1.9248433113098145\n",
      "Epoch 1 / 500 | iteration 0 / 30 | Total Loss: 8.140485763549805 | KNN Loss: 6.221712589263916 | BCE Loss: 1.9187731742858887\n",
      "Epoch 1 / 500 | iteration 5 / 30 | Total Loss: 8.113654136657715 | KNN Loss: 6.220485687255859 | BCE Loss: 1.8931680917739868\n",
      "Epoch 1 / 500 | iteration 10 / 30 | Total Loss: 8.11220932006836 | KNN Loss: 6.220448017120361 | BCE Loss: 1.8917615413665771\n",
      "Epoch 1 / 500 | iteration 15 / 30 | Total Loss: 8.111407279968262 | KNN Loss: 6.2192816734313965 | BCE Loss: 1.8921258449554443\n",
      "Epoch 1 / 500 | iteration 20 / 30 | Total Loss: 8.122200012207031 | KNN Loss: 6.218594074249268 | BCE Loss: 1.9036064147949219\n",
      "Epoch 1 / 500 | iteration 25 / 30 | Total Loss: 8.143512725830078 | KNN Loss: 6.217563629150391 | BCE Loss: 1.925949215888977\n",
      "Epoch 2 / 500 | iteration 0 / 30 | Total Loss: 8.035306930541992 | KNN Loss: 6.21688985824585 | BCE Loss: 1.8184170722961426\n",
      "Epoch 2 / 500 | iteration 5 / 30 | Total Loss: 8.057554244995117 | KNN Loss: 6.216377258300781 | BCE Loss: 1.841177225112915\n",
      "Epoch 2 / 500 | iteration 10 / 30 | Total Loss: 8.058425903320312 | KNN Loss: 6.214138984680176 | BCE Loss: 1.8442864418029785\n",
      "Epoch 2 / 500 | iteration 15 / 30 | Total Loss: 8.091414451599121 | KNN Loss: 6.213581562042236 | BCE Loss: 1.8778328895568848\n",
      "Epoch 2 / 500 | iteration 20 / 30 | Total Loss: 8.027599334716797 | KNN Loss: 6.213305473327637 | BCE Loss: 1.8142943382263184\n",
      "Epoch 2 / 500 | iteration 25 / 30 | Total Loss: 8.00279712677002 | KNN Loss: 6.212226390838623 | BCE Loss: 1.7905704975128174\n",
      "Epoch 3 / 500 | iteration 0 / 30 | Total Loss: 8.020759582519531 | KNN Loss: 6.210707187652588 | BCE Loss: 1.8100519180297852\n",
      "Epoch 3 / 500 | iteration 5 / 30 | Total Loss: 7.980267524719238 | KNN Loss: 6.2104411125183105 | BCE Loss: 1.7698261737823486\n",
      "Epoch 3 / 500 | iteration 10 / 30 | Total Loss: 8.021960258483887 | KNN Loss: 6.209131717681885 | BCE Loss: 1.812828779220581\n",
      "Epoch 3 / 500 | iteration 15 / 30 | Total Loss: 8.02358341217041 | KNN Loss: 6.2063984870910645 | BCE Loss: 1.8171846866607666\n",
      "Epoch 3 / 500 | iteration 20 / 30 | Total Loss: 7.97226619720459 | KNN Loss: 6.204983711242676 | BCE Loss: 1.767282247543335\n",
      "Epoch 3 / 500 | iteration 25 / 30 | Total Loss: 7.961034297943115 | KNN Loss: 6.20225715637207 | BCE Loss: 1.758777141571045\n",
      "Epoch 4 / 500 | iteration 0 / 30 | Total Loss: 7.922757625579834 | KNN Loss: 6.199924468994141 | BCE Loss: 1.7228331565856934\n",
      "Epoch 4 / 500 | iteration 5 / 30 | Total Loss: 7.9216766357421875 | KNN Loss: 6.1982598304748535 | BCE Loss: 1.723417043685913\n",
      "Epoch 4 / 500 | iteration 10 / 30 | Total Loss: 7.931648254394531 | KNN Loss: 6.195713996887207 | BCE Loss: 1.7359342575073242\n",
      "Epoch 4 / 500 | iteration 15 / 30 | Total Loss: 7.911131858825684 | KNN Loss: 6.193287372589111 | BCE Loss: 1.7178447246551514\n",
      "Epoch 4 / 500 | iteration 20 / 30 | Total Loss: 7.873849868774414 | KNN Loss: 6.189155578613281 | BCE Loss: 1.6846940517425537\n",
      "Epoch 4 / 500 | iteration 25 / 30 | Total Loss: 7.831228256225586 | KNN Loss: 6.181679725646973 | BCE Loss: 1.6495482921600342\n",
      "Epoch 5 / 500 | iteration 0 / 30 | Total Loss: 7.795814514160156 | KNN Loss: 6.174234867095947 | BCE Loss: 1.621579885482788\n",
      "Epoch 5 / 500 | iteration 5 / 30 | Total Loss: 7.831811904907227 | KNN Loss: 6.174697399139404 | BCE Loss: 1.6571142673492432\n",
      "Epoch 5 / 500 | iteration 10 / 30 | Total Loss: 7.822223663330078 | KNN Loss: 6.168753147125244 | BCE Loss: 1.6534702777862549\n",
      "Epoch 5 / 500 | iteration 15 / 30 | Total Loss: 7.781157493591309 | KNN Loss: 6.164982795715332 | BCE Loss: 1.6161746978759766\n",
      "Epoch 5 / 500 | iteration 20 / 30 | Total Loss: 7.778480529785156 | KNN Loss: 6.1537861824035645 | BCE Loss: 1.6246941089630127\n",
      "Epoch 5 / 500 | iteration 25 / 30 | Total Loss: 7.8024983406066895 | KNN Loss: 6.148416996002197 | BCE Loss: 1.6540814638137817\n",
      "Epoch 6 / 500 | iteration 0 / 30 | Total Loss: 7.743654251098633 | KNN Loss: 6.132312774658203 | BCE Loss: 1.6113417148590088\n",
      "Epoch 6 / 500 | iteration 5 / 30 | Total Loss: 7.699706077575684 | KNN Loss: 6.114456653594971 | BCE Loss: 1.585249662399292\n",
      "Epoch 6 / 500 | iteration 10 / 30 | Total Loss: 7.633624076843262 | KNN Loss: 6.099757194519043 | BCE Loss: 1.5338671207427979\n",
      "Epoch 6 / 500 | iteration 15 / 30 | Total Loss: 7.624058723449707 | KNN Loss: 6.096997261047363 | BCE Loss: 1.5270614624023438\n",
      "Epoch 6 / 500 | iteration 20 / 30 | Total Loss: 7.575802803039551 | KNN Loss: 6.070068836212158 | BCE Loss: 1.5057339668273926\n",
      "Epoch 6 / 500 | iteration 25 / 30 | Total Loss: 7.5526251792907715 | KNN Loss: 6.048883438110352 | BCE Loss: 1.50374174118042\n",
      "Epoch 7 / 500 | iteration 0 / 30 | Total Loss: 7.4820356369018555 | KNN Loss: 6.020638942718506 | BCE Loss: 1.4613964557647705\n",
      "Epoch 7 / 500 | iteration 5 / 30 | Total Loss: 7.418879508972168 | KNN Loss: 5.98611307144165 | BCE Loss: 1.4327664375305176\n",
      "Epoch 7 / 500 | iteration 10 / 30 | Total Loss: 7.388350486755371 | KNN Loss: 5.965550899505615 | BCE Loss: 1.4227993488311768\n",
      "Epoch 7 / 500 | iteration 15 / 30 | Total Loss: 7.293239593505859 | KNN Loss: 5.9162750244140625 | BCE Loss: 1.376964807510376\n",
      "Epoch 7 / 500 | iteration 20 / 30 | Total Loss: 7.296710968017578 | KNN Loss: 5.903741359710693 | BCE Loss: 1.3929696083068848\n",
      "Epoch 7 / 500 | iteration 25 / 30 | Total Loss: 7.244434356689453 | KNN Loss: 5.855132579803467 | BCE Loss: 1.3893017768859863\n",
      "Epoch 8 / 500 | iteration 0 / 30 | Total Loss: 7.1162428855896 | KNN Loss: 5.791977882385254 | BCE Loss: 1.3242650032043457\n",
      "Epoch 8 / 500 | iteration 5 / 30 | Total Loss: 7.0427327156066895 | KNN Loss: 5.752486228942871 | BCE Loss: 1.2902464866638184\n",
      "Epoch 8 / 500 | iteration 10 / 30 | Total Loss: 6.967356204986572 | KNN Loss: 5.66989803314209 | BCE Loss: 1.2974581718444824\n",
      "Epoch 8 / 500 | iteration 15 / 30 | Total Loss: 6.8459577560424805 | KNN Loss: 5.599984645843506 | BCE Loss: 1.2459728717803955\n",
      "Epoch 8 / 500 | iteration 20 / 30 | Total Loss: 6.765192985534668 | KNN Loss: 5.503559112548828 | BCE Loss: 1.261634111404419\n",
      "Epoch 8 / 500 | iteration 25 / 30 | Total Loss: 6.54534387588501 | KNN Loss: 5.356659889221191 | BCE Loss: 1.1886838674545288\n",
      "Epoch 9 / 500 | iteration 0 / 30 | Total Loss: 6.440369606018066 | KNN Loss: 5.267782211303711 | BCE Loss: 1.1725873947143555\n",
      "Epoch 9 / 500 | iteration 5 / 30 | Total Loss: 6.369118690490723 | KNN Loss: 5.176418304443359 | BCE Loss: 1.1927003860473633\n",
      "Epoch 9 / 500 | iteration 10 / 30 | Total Loss: 6.1479291915893555 | KNN Loss: 5.004745960235596 | BCE Loss: 1.1431833505630493\n",
      "Epoch 9 / 500 | iteration 15 / 30 | Total Loss: 5.996345520019531 | KNN Loss: 4.845911979675293 | BCE Loss: 1.1504335403442383\n",
      "Epoch 9 / 500 | iteration 20 / 30 | Total Loss: 5.879918098449707 | KNN Loss: 4.731431484222412 | BCE Loss: 1.148486614227295\n",
      "Epoch 9 / 500 | iteration 25 / 30 | Total Loss: 5.663540840148926 | KNN Loss: 4.547356605529785 | BCE Loss: 1.1161842346191406\n",
      "Epoch 10 / 500 | iteration 0 / 30 | Total Loss: 5.557154655456543 | KNN Loss: 4.434840679168701 | BCE Loss: 1.1223139762878418\n",
      "Epoch 10 / 500 | iteration 5 / 30 | Total Loss: 5.426546573638916 | KNN Loss: 4.324443817138672 | BCE Loss: 1.1021027565002441\n",
      "Epoch 10 / 500 | iteration 10 / 30 | Total Loss: 5.2883429527282715 | KNN Loss: 4.164265155792236 | BCE Loss: 1.1240777969360352\n",
      "Epoch 10 / 500 | iteration 15 / 30 | Total Loss: 5.315520286560059 | KNN Loss: 4.151098728179932 | BCE Loss: 1.1644213199615479\n",
      "Epoch 10 / 500 | iteration 20 / 30 | Total Loss: 5.1284637451171875 | KNN Loss: 3.9997572898864746 | BCE Loss: 1.128706693649292\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 / 500 | iteration 25 / 30 | Total Loss: 4.986264228820801 | KNN Loss: 3.8754231929779053 | BCE Loss: 1.1108407974243164\n",
      "Epoch 11 / 500 | iteration 0 / 30 | Total Loss: 4.874971389770508 | KNN Loss: 3.7803409099578857 | BCE Loss: 1.094630241394043\n",
      "Epoch 11 / 500 | iteration 5 / 30 | Total Loss: 4.799947738647461 | KNN Loss: 3.687481641769409 | BCE Loss: 1.1124663352966309\n",
      "Epoch 11 / 500 | iteration 10 / 30 | Total Loss: 4.735187530517578 | KNN Loss: 3.6294658184051514 | BCE Loss: 1.1057215929031372\n",
      "Epoch 11 / 500 | iteration 15 / 30 | Total Loss: 4.600740432739258 | KNN Loss: 3.5104541778564453 | BCE Loss: 1.090286135673523\n",
      "Epoch 11 / 500 | iteration 20 / 30 | Total Loss: 4.573747158050537 | KNN Loss: 3.4555721282958984 | BCE Loss: 1.1181750297546387\n",
      "Epoch 11 / 500 | iteration 25 / 30 | Total Loss: 4.487905979156494 | KNN Loss: 3.3704724311828613 | BCE Loss: 1.1174335479736328\n",
      "Epoch 12 / 500 | iteration 0 / 30 | Total Loss: 4.407155990600586 | KNN Loss: 3.301198720932007 | BCE Loss: 1.10595703125\n",
      "Epoch 12 / 500 | iteration 5 / 30 | Total Loss: 4.367830276489258 | KNN Loss: 3.284430980682373 | BCE Loss: 1.0833992958068848\n",
      "Epoch 12 / 500 | iteration 10 / 30 | Total Loss: 4.273639678955078 | KNN Loss: 3.1728715896606445 | BCE Loss: 1.1007680892944336\n",
      "Epoch 12 / 500 | iteration 15 / 30 | Total Loss: 4.25234842300415 | KNN Loss: 3.1666457653045654 | BCE Loss: 1.0857027769088745\n",
      "Epoch 12 / 500 | iteration 20 / 30 | Total Loss: 4.201260089874268 | KNN Loss: 3.105759620666504 | BCE Loss: 1.0955004692077637\n",
      "Epoch 12 / 500 | iteration 25 / 30 | Total Loss: 4.158287048339844 | KNN Loss: 3.0812368392944336 | BCE Loss: 1.0770502090454102\n",
      "Epoch 13 / 500 | iteration 0 / 30 | Total Loss: 4.216130256652832 | KNN Loss: 3.1184990406036377 | BCE Loss: 1.0976309776306152\n",
      "Epoch 13 / 500 | iteration 5 / 30 | Total Loss: 4.160879611968994 | KNN Loss: 3.078015089035034 | BCE Loss: 1.08286452293396\n",
      "Epoch 13 / 500 | iteration 10 / 30 | Total Loss: 4.068639755249023 | KNN Loss: 2.9650676250457764 | BCE Loss: 1.103571891784668\n",
      "Epoch 13 / 500 | iteration 15 / 30 | Total Loss: 4.110932350158691 | KNN Loss: 3.0333547592163086 | BCE Loss: 1.077577829360962\n",
      "Epoch 13 / 500 | iteration 20 / 30 | Total Loss: 3.9881439208984375 | KNN Loss: 2.9386045932769775 | BCE Loss: 1.04953932762146\n",
      "Epoch 13 / 500 | iteration 25 / 30 | Total Loss: 4.026093482971191 | KNN Loss: 2.9366815090179443 | BCE Loss: 1.089411735534668\n",
      "Epoch 14 / 500 | iteration 0 / 30 | Total Loss: 3.9262051582336426 | KNN Loss: 2.848623514175415 | BCE Loss: 1.0775816440582275\n",
      "Epoch 14 / 500 | iteration 5 / 30 | Total Loss: 3.9060277938842773 | KNN Loss: 2.829514741897583 | BCE Loss: 1.0765131711959839\n",
      "Epoch 14 / 500 | iteration 10 / 30 | Total Loss: 3.9333012104034424 | KNN Loss: 2.849318265914917 | BCE Loss: 1.0839829444885254\n",
      "Epoch 14 / 500 | iteration 15 / 30 | Total Loss: 3.9231135845184326 | KNN Loss: 2.859713554382324 | BCE Loss: 1.0634000301361084\n",
      "Epoch 14 / 500 | iteration 20 / 30 | Total Loss: 3.9257142543792725 | KNN Loss: 2.8554961681365967 | BCE Loss: 1.0702180862426758\n",
      "Epoch 14 / 500 | iteration 25 / 30 | Total Loss: 3.9245810508728027 | KNN Loss: 2.839820384979248 | BCE Loss: 1.0847606658935547\n",
      "Epoch 15 / 500 | iteration 0 / 30 | Total Loss: 3.891620635986328 | KNN Loss: 2.8135969638824463 | BCE Loss: 1.0780235528945923\n",
      "Epoch 15 / 500 | iteration 5 / 30 | Total Loss: 3.8737239837646484 | KNN Loss: 2.7982873916625977 | BCE Loss: 1.0754365921020508\n",
      "Epoch 15 / 500 | iteration 10 / 30 | Total Loss: 3.8196864128112793 | KNN Loss: 2.7632434368133545 | BCE Loss: 1.0564429759979248\n",
      "Epoch 15 / 500 | iteration 15 / 30 | Total Loss: 3.8110527992248535 | KNN Loss: 2.768409013748169 | BCE Loss: 1.0426437854766846\n",
      "Epoch 15 / 500 | iteration 20 / 30 | Total Loss: 3.8029086589813232 | KNN Loss: 2.7461636066436768 | BCE Loss: 1.0567450523376465\n",
      "Epoch 15 / 500 | iteration 25 / 30 | Total Loss: 3.858421564102173 | KNN Loss: 2.794481039047241 | BCE Loss: 1.0639405250549316\n",
      "Epoch 16 / 500 | iteration 0 / 30 | Total Loss: 3.864047050476074 | KNN Loss: 2.7837073802948 | BCE Loss: 1.0803395509719849\n",
      "Epoch 16 / 500 | iteration 5 / 30 | Total Loss: 3.8844635486602783 | KNN Loss: 2.792149782180786 | BCE Loss: 1.0923137664794922\n",
      "Epoch 16 / 500 | iteration 10 / 30 | Total Loss: 3.829953908920288 | KNN Loss: 2.7699732780456543 | BCE Loss: 1.0599806308746338\n",
      "Epoch 16 / 500 | iteration 15 / 30 | Total Loss: 3.9137754440307617 | KNN Loss: 2.839203357696533 | BCE Loss: 1.0745720863342285\n",
      "Epoch 16 / 500 | iteration 20 / 30 | Total Loss: 3.8432345390319824 | KNN Loss: 2.771514892578125 | BCE Loss: 1.0717196464538574\n",
      "Epoch 16 / 500 | iteration 25 / 30 | Total Loss: 3.837643623352051 | KNN Loss: 2.7647080421447754 | BCE Loss: 1.0729355812072754\n",
      "Epoch 17 / 500 | iteration 0 / 30 | Total Loss: 3.744199275970459 | KNN Loss: 2.688588857650757 | BCE Loss: 1.0556105375289917\n",
      "Epoch 17 / 500 | iteration 5 / 30 | Total Loss: 3.7585339546203613 | KNN Loss: 2.711148500442505 | BCE Loss: 1.047385334968567\n",
      "Epoch 17 / 500 | iteration 10 / 30 | Total Loss: 3.779536724090576 | KNN Loss: 2.7217319011688232 | BCE Loss: 1.057804822921753\n",
      "Epoch 17 / 500 | iteration 15 / 30 | Total Loss: 3.838463306427002 | KNN Loss: 2.7805898189544678 | BCE Loss: 1.0578733682632446\n",
      "Epoch 17 / 500 | iteration 20 / 30 | Total Loss: 3.7932446002960205 | KNN Loss: 2.7267065048217773 | BCE Loss: 1.0665380954742432\n",
      "Epoch 17 / 500 | iteration 25 / 30 | Total Loss: 3.801546573638916 | KNN Loss: 2.7617101669311523 | BCE Loss: 1.0398364067077637\n",
      "Epoch 18 / 500 | iteration 0 / 30 | Total Loss: 3.846008539199829 | KNN Loss: 2.7707648277282715 | BCE Loss: 1.0752437114715576\n",
      "Epoch 18 / 500 | iteration 5 / 30 | Total Loss: 3.8177597522735596 | KNN Loss: 2.760072708129883 | BCE Loss: 1.0576870441436768\n",
      "Epoch 18 / 500 | iteration 10 / 30 | Total Loss: 3.8093879222869873 | KNN Loss: 2.728961944580078 | BCE Loss: 1.0804259777069092\n",
      "Epoch 18 / 500 | iteration 15 / 30 | Total Loss: 3.7447726726531982 | KNN Loss: 2.6922943592071533 | BCE Loss: 1.052478313446045\n",
      "Epoch 18 / 500 | iteration 20 / 30 | Total Loss: 3.753903388977051 | KNN Loss: 2.688657760620117 | BCE Loss: 1.0652457475662231\n",
      "Epoch 18 / 500 | iteration 25 / 30 | Total Loss: 3.811403512954712 | KNN Loss: 2.732943534851074 | BCE Loss: 1.0784599781036377\n",
      "Epoch 19 / 500 | iteration 0 / 30 | Total Loss: 3.763002395629883 | KNN Loss: 2.7097299098968506 | BCE Loss: 1.0532724857330322\n",
      "Epoch 19 / 500 | iteration 5 / 30 | Total Loss: 3.7184395790100098 | KNN Loss: 2.6858344078063965 | BCE Loss: 1.0326050519943237\n",
      "Epoch 19 / 500 | iteration 10 / 30 | Total Loss: 3.792476177215576 | KNN Loss: 2.7259316444396973 | BCE Loss: 1.066544532775879\n",
      "Epoch 19 / 500 | iteration 15 / 30 | Total Loss: 3.732699394226074 | KNN Loss: 2.6687662601470947 | BCE Loss: 1.063933253288269\n",
      "Epoch 19 / 500 | iteration 20 / 30 | Total Loss: 3.7248501777648926 | KNN Loss: 2.6533398628234863 | BCE Loss: 1.0715101957321167\n",
      "Epoch 19 / 500 | iteration 25 / 30 | Total Loss: 3.879610538482666 | KNN Loss: 2.806039571762085 | BCE Loss: 1.073570966720581\n",
      "Epoch 20 / 500 | iteration 0 / 30 | Total Loss: 3.750751256942749 | KNN Loss: 2.683924436569214 | BCE Loss: 1.0668268203735352\n",
      "Epoch 20 / 500 | iteration 5 / 30 | Total Loss: 3.7367329597473145 | KNN Loss: 2.6732394695281982 | BCE Loss: 1.0634934902191162\n",
      "Epoch 20 / 500 | iteration 10 / 30 | Total Loss: 3.700895309448242 | KNN Loss: 2.670114755630493 | BCE Loss: 1.0307804346084595\n",
      "Epoch 20 / 500 | iteration 15 / 30 | Total Loss: 3.7017264366149902 | KNN Loss: 2.6516337394714355 | BCE Loss: 1.0500928163528442\n",
      "Epoch 20 / 500 | iteration 20 / 30 | Total Loss: 3.744861125946045 | KNN Loss: 2.701596736907959 | BCE Loss: 1.043264389038086\n",
      "Epoch 20 / 500 | iteration 25 / 30 | Total Loss: 3.718688488006592 | KNN Loss: 2.6753382682800293 | BCE Loss: 1.0433502197265625\n",
      "Epoch 21 / 500 | iteration 0 / 30 | Total Loss: 3.744192361831665 | KNN Loss: 2.6646811962127686 | BCE Loss: 1.0795111656188965\n",
      "Epoch 21 / 500 | iteration 5 / 30 | Total Loss: 3.7105791568756104 | KNN Loss: 2.647458076477051 | BCE Loss: 1.0631210803985596\n",
      "Epoch 21 / 500 | iteration 10 / 30 | Total Loss: 3.742948532104492 | KNN Loss: 2.669589042663574 | BCE Loss: 1.0733596086502075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 / 500 | iteration 15 / 30 | Total Loss: 3.745793104171753 | KNN Loss: 2.7118005752563477 | BCE Loss: 1.0339925289154053\n",
      "Epoch 21 / 500 | iteration 20 / 30 | Total Loss: 3.7594337463378906 | KNN Loss: 2.709543466567993 | BCE Loss: 1.049890398979187\n",
      "Epoch 21 / 500 | iteration 25 / 30 | Total Loss: 3.7088677883148193 | KNN Loss: 2.6439218521118164 | BCE Loss: 1.064945936203003\n",
      "Epoch 22 / 500 | iteration 0 / 30 | Total Loss: 3.787175178527832 | KNN Loss: 2.700822591781616 | BCE Loss: 1.0863525867462158\n",
      "Epoch 22 / 500 | iteration 5 / 30 | Total Loss: 3.7421951293945312 | KNN Loss: 2.6866912841796875 | BCE Loss: 1.0555038452148438\n",
      "Epoch 22 / 500 | iteration 10 / 30 | Total Loss: 3.7248144149780273 | KNN Loss: 2.6636526584625244 | BCE Loss: 1.0611616373062134\n",
      "Epoch 22 / 500 | iteration 15 / 30 | Total Loss: 3.732130289077759 | KNN Loss: 2.6874048709869385 | BCE Loss: 1.0447254180908203\n",
      "Epoch 22 / 500 | iteration 20 / 30 | Total Loss: 3.753727674484253 | KNN Loss: 2.7074761390686035 | BCE Loss: 1.0462515354156494\n",
      "Epoch 22 / 500 | iteration 25 / 30 | Total Loss: 3.697096824645996 | KNN Loss: 2.6325483322143555 | BCE Loss: 1.0645484924316406\n",
      "Epoch 23 / 500 | iteration 0 / 30 | Total Loss: 3.7132959365844727 | KNN Loss: 2.6636946201324463 | BCE Loss: 1.0496013164520264\n",
      "Epoch 23 / 500 | iteration 5 / 30 | Total Loss: 3.7345566749572754 | KNN Loss: 2.668146848678589 | BCE Loss: 1.0664098262786865\n",
      "Epoch 23 / 500 | iteration 10 / 30 | Total Loss: 3.7439541816711426 | KNN Loss: 2.700204372406006 | BCE Loss: 1.0437496900558472\n",
      "Epoch 23 / 500 | iteration 15 / 30 | Total Loss: 3.6739087104797363 | KNN Loss: 2.629042148590088 | BCE Loss: 1.0448665618896484\n",
      "Epoch 23 / 500 | iteration 20 / 30 | Total Loss: 3.720702886581421 | KNN Loss: 2.670325994491577 | BCE Loss: 1.0503768920898438\n",
      "Epoch 23 / 500 | iteration 25 / 30 | Total Loss: 3.6348929405212402 | KNN Loss: 2.57338285446167 | BCE Loss: 1.0615099668502808\n",
      "Epoch 24 / 500 | iteration 0 / 30 | Total Loss: 3.6590025424957275 | KNN Loss: 2.5914695262908936 | BCE Loss: 1.067533016204834\n",
      "Epoch 24 / 500 | iteration 5 / 30 | Total Loss: 3.699636936187744 | KNN Loss: 2.6387832164764404 | BCE Loss: 1.0608538389205933\n",
      "Epoch 24 / 500 | iteration 10 / 30 | Total Loss: 3.6457648277282715 | KNN Loss: 2.5940279960632324 | BCE Loss: 1.051736831665039\n",
      "Epoch 24 / 500 | iteration 15 / 30 | Total Loss: 3.644843339920044 | KNN Loss: 2.6020843982696533 | BCE Loss: 1.0427589416503906\n",
      "Epoch 24 / 500 | iteration 20 / 30 | Total Loss: 3.676407814025879 | KNN Loss: 2.626321792602539 | BCE Loss: 1.0500860214233398\n",
      "Epoch 24 / 500 | iteration 25 / 30 | Total Loss: 3.634380578994751 | KNN Loss: 2.6046509742736816 | BCE Loss: 1.0297296047210693\n",
      "Epoch 25 / 500 | iteration 0 / 30 | Total Loss: 3.6393182277679443 | KNN Loss: 2.5804526805877686 | BCE Loss: 1.0588655471801758\n",
      "Epoch 25 / 500 | iteration 5 / 30 | Total Loss: 3.639723777770996 | KNN Loss: 2.5984456539154053 | BCE Loss: 1.0412781238555908\n",
      "Epoch 25 / 500 | iteration 10 / 30 | Total Loss: 3.6632888317108154 | KNN Loss: 2.610835313796997 | BCE Loss: 1.0524535179138184\n",
      "Epoch 25 / 500 | iteration 15 / 30 | Total Loss: 3.602147102355957 | KNN Loss: 2.5896997451782227 | BCE Loss: 1.0124473571777344\n",
      "Epoch 25 / 500 | iteration 20 / 30 | Total Loss: 3.6815412044525146 | KNN Loss: 2.653482675552368 | BCE Loss: 1.0280585289001465\n",
      "Epoch 25 / 500 | iteration 25 / 30 | Total Loss: 3.6208157539367676 | KNN Loss: 2.5726683139801025 | BCE Loss: 1.048147439956665\n",
      "Epoch 26 / 500 | iteration 0 / 30 | Total Loss: 3.6628963947296143 | KNN Loss: 2.6389763355255127 | BCE Loss: 1.0239200592041016\n",
      "Epoch 26 / 500 | iteration 5 / 30 | Total Loss: 3.668001651763916 | KNN Loss: 2.628147840499878 | BCE Loss: 1.039853811264038\n",
      "Epoch 26 / 500 | iteration 10 / 30 | Total Loss: 3.666616678237915 | KNN Loss: 2.6051113605499268 | BCE Loss: 1.0615053176879883\n",
      "Epoch 26 / 500 | iteration 15 / 30 | Total Loss: 3.7081565856933594 | KNN Loss: 2.65775990486145 | BCE Loss: 1.0503965616226196\n",
      "Epoch 26 / 500 | iteration 20 / 30 | Total Loss: 3.694924831390381 | KNN Loss: 2.648996353149414 | BCE Loss: 1.0459284782409668\n",
      "Epoch 26 / 500 | iteration 25 / 30 | Total Loss: 3.644134521484375 | KNN Loss: 2.5955121517181396 | BCE Loss: 1.0486223697662354\n",
      "Epoch 27 / 500 | iteration 0 / 30 | Total Loss: 3.8034679889678955 | KNN Loss: 2.718449592590332 | BCE Loss: 1.0850183963775635\n",
      "Epoch 27 / 500 | iteration 5 / 30 | Total Loss: 3.6388254165649414 | KNN Loss: 2.590921401977539 | BCE Loss: 1.047904133796692\n",
      "Epoch 27 / 500 | iteration 10 / 30 | Total Loss: 3.674227237701416 | KNN Loss: 2.6110148429870605 | BCE Loss: 1.0632123947143555\n",
      "Epoch 27 / 500 | iteration 15 / 30 | Total Loss: 3.68505597114563 | KNN Loss: 2.5952775478363037 | BCE Loss: 1.0897784233093262\n",
      "Epoch 27 / 500 | iteration 20 / 30 | Total Loss: 3.67954683303833 | KNN Loss: 2.639800786972046 | BCE Loss: 1.0397461652755737\n",
      "Epoch 27 / 500 | iteration 25 / 30 | Total Loss: 3.675098419189453 | KNN Loss: 2.617220878601074 | BCE Loss: 1.0578776597976685\n",
      "Epoch 28 / 500 | iteration 0 / 30 | Total Loss: 3.695906400680542 | KNN Loss: 2.647085428237915 | BCE Loss: 1.048820972442627\n",
      "Epoch 28 / 500 | iteration 5 / 30 | Total Loss: 3.639070987701416 | KNN Loss: 2.616642713546753 | BCE Loss: 1.022428274154663\n",
      "Epoch 28 / 500 | iteration 10 / 30 | Total Loss: 3.706430196762085 | KNN Loss: 2.6400864124298096 | BCE Loss: 1.0663437843322754\n",
      "Epoch 28 / 500 | iteration 15 / 30 | Total Loss: 3.6919445991516113 | KNN Loss: 2.6403706073760986 | BCE Loss: 1.0515739917755127\n",
      "Epoch 28 / 500 | iteration 20 / 30 | Total Loss: 3.661268711090088 | KNN Loss: 2.6251566410064697 | BCE Loss: 1.0361119508743286\n",
      "Epoch 28 / 500 | iteration 25 / 30 | Total Loss: 3.639805793762207 | KNN Loss: 2.581042766571045 | BCE Loss: 1.058763027191162\n",
      "Epoch 29 / 500 | iteration 0 / 30 | Total Loss: 3.672553300857544 | KNN Loss: 2.6141457557678223 | BCE Loss: 1.0584075450897217\n",
      "Epoch 29 / 500 | iteration 5 / 30 | Total Loss: 3.6344053745269775 | KNN Loss: 2.5742123126983643 | BCE Loss: 1.0601930618286133\n",
      "Epoch 29 / 500 | iteration 10 / 30 | Total Loss: 3.7197425365448 | KNN Loss: 2.6563448905944824 | BCE Loss: 1.0633976459503174\n",
      "Epoch 29 / 500 | iteration 15 / 30 | Total Loss: 3.652677059173584 | KNN Loss: 2.5989508628845215 | BCE Loss: 1.0537261962890625\n",
      "Epoch 29 / 500 | iteration 20 / 30 | Total Loss: 3.6277122497558594 | KNN Loss: 2.586822271347046 | BCE Loss: 1.040890097618103\n",
      "Epoch 29 / 500 | iteration 25 / 30 | Total Loss: 3.680588483810425 | KNN Loss: 2.6302597522735596 | BCE Loss: 1.0503287315368652\n",
      "Epoch 30 / 500 | iteration 0 / 30 | Total Loss: 3.611574411392212 | KNN Loss: 2.5623292922973633 | BCE Loss: 1.0492451190948486\n",
      "Epoch 30 / 500 | iteration 5 / 30 | Total Loss: 3.6501126289367676 | KNN Loss: 2.61513090133667 | BCE Loss: 1.0349818468093872\n",
      "Epoch 30 / 500 | iteration 10 / 30 | Total Loss: 3.6325621604919434 | KNN Loss: 2.5819272994995117 | BCE Loss: 1.0506348609924316\n",
      "Epoch 30 / 500 | iteration 15 / 30 | Total Loss: 3.597726345062256 | KNN Loss: 2.5616893768310547 | BCE Loss: 1.0360368490219116\n",
      "Epoch 30 / 500 | iteration 20 / 30 | Total Loss: 3.6719489097595215 | KNN Loss: 2.6422460079193115 | BCE Loss: 1.0297027826309204\n",
      "Epoch 30 / 500 | iteration 25 / 30 | Total Loss: 3.610593318939209 | KNN Loss: 2.5778582096099854 | BCE Loss: 1.0327351093292236\n",
      "Epoch 31 / 500 | iteration 0 / 30 | Total Loss: 3.6344752311706543 | KNN Loss: 2.5751583576202393 | BCE Loss: 1.059316873550415\n",
      "Epoch 31 / 500 | iteration 5 / 30 | Total Loss: 3.6172842979431152 | KNN Loss: 2.596010208129883 | BCE Loss: 1.021274209022522\n",
      "Epoch 31 / 500 | iteration 10 / 30 | Total Loss: 3.598987102508545 | KNN Loss: 2.5644471645355225 | BCE Loss: 1.0345399379730225\n",
      "Epoch 31 / 500 | iteration 15 / 30 | Total Loss: 3.6517527103424072 | KNN Loss: 2.6028647422790527 | BCE Loss: 1.0488879680633545\n",
      "Epoch 31 / 500 | iteration 20 / 30 | Total Loss: 3.6139068603515625 | KNN Loss: 2.5966758728027344 | BCE Loss: 1.0172308683395386\n",
      "Epoch 31 / 500 | iteration 25 / 30 | Total Loss: 3.6051690578460693 | KNN Loss: 2.5768232345581055 | BCE Loss: 1.0283458232879639\n",
      "Epoch 32 / 500 | iteration 0 / 30 | Total Loss: 3.6555795669555664 | KNN Loss: 2.6324362754821777 | BCE Loss: 1.0231432914733887\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32 / 500 | iteration 5 / 30 | Total Loss: 3.6425485610961914 | KNN Loss: 2.6196279525756836 | BCE Loss: 1.0229206085205078\n",
      "Epoch 32 / 500 | iteration 10 / 30 | Total Loss: 3.6593141555786133 | KNN Loss: 2.5833194255828857 | BCE Loss: 1.0759947299957275\n",
      "Epoch 32 / 500 | iteration 15 / 30 | Total Loss: 3.5900542736053467 | KNN Loss: 2.5517096519470215 | BCE Loss: 1.0383446216583252\n",
      "Epoch 32 / 500 | iteration 20 / 30 | Total Loss: 3.6452982425689697 | KNN Loss: 2.5981411933898926 | BCE Loss: 1.0471570491790771\n",
      "Epoch 32 / 500 | iteration 25 / 30 | Total Loss: 3.558393716812134 | KNN Loss: 2.5428829193115234 | BCE Loss: 1.0155107975006104\n",
      "Epoch 33 / 500 | iteration 0 / 30 | Total Loss: 3.5885677337646484 | KNN Loss: 2.533275842666626 | BCE Loss: 1.0552918910980225\n",
      "Epoch 33 / 500 | iteration 5 / 30 | Total Loss: 3.6900157928466797 | KNN Loss: 2.650986671447754 | BCE Loss: 1.0390291213989258\n",
      "Epoch 33 / 500 | iteration 10 / 30 | Total Loss: 3.6802167892456055 | KNN Loss: 2.642678737640381 | BCE Loss: 1.0375380516052246\n",
      "Epoch 33 / 500 | iteration 15 / 30 | Total Loss: 3.6424546241760254 | KNN Loss: 2.5813896656036377 | BCE Loss: 1.0610649585723877\n",
      "Epoch 33 / 500 | iteration 20 / 30 | Total Loss: 3.5877389907836914 | KNN Loss: 2.567999839782715 | BCE Loss: 1.0197392702102661\n",
      "Epoch 33 / 500 | iteration 25 / 30 | Total Loss: 3.6132588386535645 | KNN Loss: 2.573744058609009 | BCE Loss: 1.0395148992538452\n",
      "Epoch 34 / 500 | iteration 0 / 30 | Total Loss: 3.6409409046173096 | KNN Loss: 2.599458694458008 | BCE Loss: 1.0414822101593018\n",
      "Epoch 34 / 500 | iteration 5 / 30 | Total Loss: 3.6714560985565186 | KNN Loss: 2.6376988887786865 | BCE Loss: 1.033757209777832\n",
      "Epoch 34 / 500 | iteration 10 / 30 | Total Loss: 3.6226613521575928 | KNN Loss: 2.595637798309326 | BCE Loss: 1.0270235538482666\n",
      "Epoch 34 / 500 | iteration 15 / 30 | Total Loss: 3.583407163619995 | KNN Loss: 2.5583903789520264 | BCE Loss: 1.0250167846679688\n",
      "Epoch 34 / 500 | iteration 20 / 30 | Total Loss: 3.586437225341797 | KNN Loss: 2.558440923690796 | BCE Loss: 1.0279964208602905\n",
      "Epoch 34 / 500 | iteration 25 / 30 | Total Loss: 3.652991533279419 | KNN Loss: 2.6119675636291504 | BCE Loss: 1.0410239696502686\n",
      "Epoch 35 / 500 | iteration 0 / 30 | Total Loss: 3.582181692123413 | KNN Loss: 2.553352117538452 | BCE Loss: 1.028829574584961\n",
      "Epoch 35 / 500 | iteration 5 / 30 | Total Loss: 3.653995990753174 | KNN Loss: 2.6096363067626953 | BCE Loss: 1.0443596839904785\n",
      "Epoch 35 / 500 | iteration 10 / 30 | Total Loss: 3.5980610847473145 | KNN Loss: 2.5625548362731934 | BCE Loss: 1.035506248474121\n",
      "Epoch 35 / 500 | iteration 15 / 30 | Total Loss: 3.6357693672180176 | KNN Loss: 2.6002004146575928 | BCE Loss: 1.0355688333511353\n",
      "Epoch 35 / 500 | iteration 20 / 30 | Total Loss: 3.707439661026001 | KNN Loss: 2.6311540603637695 | BCE Loss: 1.0762856006622314\n",
      "Epoch 35 / 500 | iteration 25 / 30 | Total Loss: 3.5610249042510986 | KNN Loss: 2.5208044052124023 | BCE Loss: 1.0402204990386963\n",
      "Epoch 36 / 500 | iteration 0 / 30 | Total Loss: 3.689807415008545 | KNN Loss: 2.6503143310546875 | BCE Loss: 1.039493203163147\n",
      "Epoch 36 / 500 | iteration 5 / 30 | Total Loss: 3.6299569606781006 | KNN Loss: 2.5538554191589355 | BCE Loss: 1.076101541519165\n",
      "Epoch 36 / 500 | iteration 10 / 30 | Total Loss: 3.6104538440704346 | KNN Loss: 2.566225051879883 | BCE Loss: 1.0442287921905518\n",
      "Epoch 36 / 500 | iteration 15 / 30 | Total Loss: 3.6311333179473877 | KNN Loss: 2.5782265663146973 | BCE Loss: 1.0529067516326904\n",
      "Epoch 36 / 500 | iteration 20 / 30 | Total Loss: 3.5705108642578125 | KNN Loss: 2.561566114425659 | BCE Loss: 1.0089447498321533\n",
      "Epoch 36 / 500 | iteration 25 / 30 | Total Loss: 3.603126049041748 | KNN Loss: 2.586758613586426 | BCE Loss: 1.0163674354553223\n",
      "Epoch 37 / 500 | iteration 0 / 30 | Total Loss: 3.5923562049865723 | KNN Loss: 2.55117130279541 | BCE Loss: 1.0411847829818726\n",
      "Epoch 37 / 500 | iteration 5 / 30 | Total Loss: 3.5605900287628174 | KNN Loss: 2.5006766319274902 | BCE Loss: 1.0599133968353271\n",
      "Epoch 37 / 500 | iteration 10 / 30 | Total Loss: 3.5829529762268066 | KNN Loss: 2.571072578430176 | BCE Loss: 1.0118803977966309\n",
      "Epoch 37 / 500 | iteration 15 / 30 | Total Loss: 3.570505142211914 | KNN Loss: 2.5481956005096436 | BCE Loss: 1.02230966091156\n",
      "Epoch 37 / 500 | iteration 20 / 30 | Total Loss: 3.713759660720825 | KNN Loss: 2.668236255645752 | BCE Loss: 1.0455234050750732\n",
      "Epoch 37 / 500 | iteration 25 / 30 | Total Loss: 3.6211791038513184 | KNN Loss: 2.580249071121216 | BCE Loss: 1.0409300327301025\n",
      "Epoch 38 / 500 | iteration 0 / 30 | Total Loss: 3.583376884460449 | KNN Loss: 2.5538578033447266 | BCE Loss: 1.029518961906433\n",
      "Epoch 38 / 500 | iteration 5 / 30 | Total Loss: 3.6577401161193848 | KNN Loss: 2.6055850982666016 | BCE Loss: 1.0521550178527832\n",
      "Epoch 38 / 500 | iteration 10 / 30 | Total Loss: 3.5375356674194336 | KNN Loss: 2.5200998783111572 | BCE Loss: 1.0174357891082764\n",
      "Epoch 38 / 500 | iteration 15 / 30 | Total Loss: 3.5630176067352295 | KNN Loss: 2.5178377628326416 | BCE Loss: 1.045179843902588\n",
      "Epoch 38 / 500 | iteration 20 / 30 | Total Loss: 3.621500253677368 | KNN Loss: 2.574315071105957 | BCE Loss: 1.0471851825714111\n",
      "Epoch 38 / 500 | iteration 25 / 30 | Total Loss: 3.546931266784668 | KNN Loss: 2.530365467071533 | BCE Loss: 1.0165657997131348\n",
      "Epoch 39 / 500 | iteration 0 / 30 | Total Loss: 3.6203346252441406 | KNN Loss: 2.5809669494628906 | BCE Loss: 1.0393677949905396\n",
      "Epoch 39 / 500 | iteration 5 / 30 | Total Loss: 3.603165626525879 | KNN Loss: 2.530522108078003 | BCE Loss: 1.072643518447876\n",
      "Epoch 39 / 500 | iteration 10 / 30 | Total Loss: 3.585336923599243 | KNN Loss: 2.5645358562469482 | BCE Loss: 1.020801067352295\n",
      "Epoch 39 / 500 | iteration 15 / 30 | Total Loss: 3.5616073608398438 | KNN Loss: 2.5349578857421875 | BCE Loss: 1.0266495943069458\n",
      "Epoch 39 / 500 | iteration 20 / 30 | Total Loss: 3.609046220779419 | KNN Loss: 2.555823802947998 | BCE Loss: 1.053222417831421\n",
      "Epoch 39 / 500 | iteration 25 / 30 | Total Loss: 3.5930023193359375 | KNN Loss: 2.5486137866973877 | BCE Loss: 1.0443885326385498\n",
      "Epoch 40 / 500 | iteration 0 / 30 | Total Loss: 3.653810501098633 | KNN Loss: 2.636155605316162 | BCE Loss: 1.0176550149917603\n",
      "Epoch 40 / 500 | iteration 5 / 30 | Total Loss: 3.670753002166748 | KNN Loss: 2.6157796382904053 | BCE Loss: 1.0549733638763428\n",
      "Epoch 40 / 500 | iteration 10 / 30 | Total Loss: 3.5894670486450195 | KNN Loss: 2.536550998687744 | BCE Loss: 1.0529160499572754\n",
      "Epoch 40 / 500 | iteration 15 / 30 | Total Loss: 3.602855920791626 | KNN Loss: 2.5613906383514404 | BCE Loss: 1.0414652824401855\n",
      "Epoch 40 / 500 | iteration 20 / 30 | Total Loss: 3.596367597579956 | KNN Loss: 2.5625555515289307 | BCE Loss: 1.0338120460510254\n",
      "Epoch 40 / 500 | iteration 25 / 30 | Total Loss: 3.5579824447631836 | KNN Loss: 2.543466329574585 | BCE Loss: 1.0145162343978882\n",
      "Epoch 41 / 500 | iteration 0 / 30 | Total Loss: 3.5307841300964355 | KNN Loss: 2.500605821609497 | BCE Loss: 1.0301783084869385\n",
      "Epoch 41 / 500 | iteration 5 / 30 | Total Loss: 3.619880199432373 | KNN Loss: 2.5905189514160156 | BCE Loss: 1.0293611288070679\n",
      "Epoch 41 / 500 | iteration 10 / 30 | Total Loss: 3.5798747539520264 | KNN Loss: 2.5188117027282715 | BCE Loss: 1.0610630512237549\n",
      "Epoch 41 / 500 | iteration 15 / 30 | Total Loss: 3.599548578262329 | KNN Loss: 2.5780696868896484 | BCE Loss: 1.0214788913726807\n",
      "Epoch 41 / 500 | iteration 20 / 30 | Total Loss: 3.6028363704681396 | KNN Loss: 2.5421547889709473 | BCE Loss: 1.0606815814971924\n",
      "Epoch 41 / 500 | iteration 25 / 30 | Total Loss: 3.6030914783477783 | KNN Loss: 2.5716326236724854 | BCE Loss: 1.031458854675293\n",
      "Epoch 42 / 500 | iteration 0 / 30 | Total Loss: 3.630380153656006 | KNN Loss: 2.593287706375122 | BCE Loss: 1.0370925664901733\n",
      "Epoch 42 / 500 | iteration 5 / 30 | Total Loss: 3.6931071281433105 | KNN Loss: 2.6413822174072266 | BCE Loss: 1.0517247915267944\n",
      "Epoch 42 / 500 | iteration 10 / 30 | Total Loss: 3.677194595336914 | KNN Loss: 2.635308027267456 | BCE Loss: 1.0418864488601685\n",
      "Epoch 42 / 500 | iteration 15 / 30 | Total Loss: 3.601630926132202 | KNN Loss: 2.5436131954193115 | BCE Loss: 1.0580177307128906\n",
      "Epoch 42 / 500 | iteration 20 / 30 | Total Loss: 3.5752346515655518 | KNN Loss: 2.5505502223968506 | BCE Loss: 1.0246844291687012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42 / 500 | iteration 25 / 30 | Total Loss: 3.618813991546631 | KNN Loss: 2.572598457336426 | BCE Loss: 1.046215534210205\n",
      "Epoch 43 / 500 | iteration 0 / 30 | Total Loss: 3.6307477951049805 | KNN Loss: 2.569920539855957 | BCE Loss: 1.0608272552490234\n",
      "Epoch 43 / 500 | iteration 5 / 30 | Total Loss: 3.6177334785461426 | KNN Loss: 2.5625157356262207 | BCE Loss: 1.0552177429199219\n",
      "Epoch 43 / 500 | iteration 10 / 30 | Total Loss: 3.590005874633789 | KNN Loss: 2.553828716278076 | BCE Loss: 1.036177158355713\n",
      "Epoch 43 / 500 | iteration 15 / 30 | Total Loss: 3.61464262008667 | KNN Loss: 2.6170718669891357 | BCE Loss: 0.9975708723068237\n",
      "Epoch 43 / 500 | iteration 20 / 30 | Total Loss: 3.642763614654541 | KNN Loss: 2.5822277069091797 | BCE Loss: 1.0605359077453613\n",
      "Epoch 43 / 500 | iteration 25 / 30 | Total Loss: 3.558879852294922 | KNN Loss: 2.5587778091430664 | BCE Loss: 1.0001020431518555\n",
      "Epoch 44 / 500 | iteration 0 / 30 | Total Loss: 3.5804293155670166 | KNN Loss: 2.5366017818450928 | BCE Loss: 1.0438275337219238\n",
      "Epoch 44 / 500 | iteration 5 / 30 | Total Loss: 3.5872995853424072 | KNN Loss: 2.5783915519714355 | BCE Loss: 1.0089080333709717\n",
      "Epoch 44 / 500 | iteration 10 / 30 | Total Loss: 3.600383996963501 | KNN Loss: 2.5369040966033936 | BCE Loss: 1.0634799003601074\n",
      "Epoch 44 / 500 | iteration 15 / 30 | Total Loss: 3.6078152656555176 | KNN Loss: 2.5820369720458984 | BCE Loss: 1.0257782936096191\n",
      "Epoch 44 / 500 | iteration 20 / 30 | Total Loss: 3.5746192932128906 | KNN Loss: 2.5405776500701904 | BCE Loss: 1.0340416431427002\n",
      "Epoch 44 / 500 | iteration 25 / 30 | Total Loss: 3.5643389225006104 | KNN Loss: 2.5228800773620605 | BCE Loss: 1.0414588451385498\n",
      "Epoch 45 / 500 | iteration 0 / 30 | Total Loss: 3.554266929626465 | KNN Loss: 2.5332326889038086 | BCE Loss: 1.0210341215133667\n",
      "Epoch 45 / 500 | iteration 5 / 30 | Total Loss: 3.5382394790649414 | KNN Loss: 2.521390438079834 | BCE Loss: 1.0168490409851074\n",
      "Epoch 45 / 500 | iteration 10 / 30 | Total Loss: 3.546123743057251 | KNN Loss: 2.519570827484131 | BCE Loss: 1.0265529155731201\n",
      "Epoch 45 / 500 | iteration 15 / 30 | Total Loss: 3.568725109100342 | KNN Loss: 2.5062708854675293 | BCE Loss: 1.0624542236328125\n",
      "Epoch 45 / 500 | iteration 20 / 30 | Total Loss: 3.5580456256866455 | KNN Loss: 2.5335328578948975 | BCE Loss: 1.024512767791748\n",
      "Epoch 45 / 500 | iteration 25 / 30 | Total Loss: 3.5622591972351074 | KNN Loss: 2.503293514251709 | BCE Loss: 1.0589656829833984\n",
      "Epoch 46 / 500 | iteration 0 / 30 | Total Loss: 3.5726897716522217 | KNN Loss: 2.566380739212036 | BCE Loss: 1.0063090324401855\n",
      "Epoch 46 / 500 | iteration 5 / 30 | Total Loss: 3.618446111679077 | KNN Loss: 2.578517198562622 | BCE Loss: 1.039928913116455\n",
      "Epoch 46 / 500 | iteration 10 / 30 | Total Loss: 3.5957210063934326 | KNN Loss: 2.5796005725860596 | BCE Loss: 1.016120433807373\n",
      "Epoch 46 / 500 | iteration 15 / 30 | Total Loss: 3.5388238430023193 | KNN Loss: 2.4991562366485596 | BCE Loss: 1.0396676063537598\n",
      "Epoch 46 / 500 | iteration 20 / 30 | Total Loss: 3.6013593673706055 | KNN Loss: 2.5426583290100098 | BCE Loss: 1.0587011575698853\n",
      "Epoch 46 / 500 | iteration 25 / 30 | Total Loss: 3.68686580657959 | KNN Loss: 2.613278388977051 | BCE Loss: 1.073587417602539\n",
      "Epoch 47 / 500 | iteration 0 / 30 | Total Loss: 3.5796239376068115 | KNN Loss: 2.562757730484009 | BCE Loss: 1.0168662071228027\n",
      "Epoch 47 / 500 | iteration 5 / 30 | Total Loss: 3.612642526626587 | KNN Loss: 2.5962069034576416 | BCE Loss: 1.0164356231689453\n",
      "Epoch 47 / 500 | iteration 10 / 30 | Total Loss: 3.6376655101776123 | KNN Loss: 2.631829261779785 | BCE Loss: 1.0058362483978271\n",
      "Epoch 47 / 500 | iteration 15 / 30 | Total Loss: 3.6327743530273438 | KNN Loss: 2.5741899013519287 | BCE Loss: 1.058584451675415\n",
      "Epoch 47 / 500 | iteration 20 / 30 | Total Loss: 3.5965311527252197 | KNN Loss: 2.5538370609283447 | BCE Loss: 1.042694091796875\n",
      "Epoch 47 / 500 | iteration 25 / 30 | Total Loss: 3.6439638137817383 | KNN Loss: 2.582897186279297 | BCE Loss: 1.0610665082931519\n",
      "Epoch 48 / 500 | iteration 0 / 30 | Total Loss: 3.5525715351104736 | KNN Loss: 2.5275561809539795 | BCE Loss: 1.0250153541564941\n",
      "Epoch 48 / 500 | iteration 5 / 30 | Total Loss: 3.5163047313690186 | KNN Loss: 2.523693561553955 | BCE Loss: 0.9926112294197083\n",
      "Epoch 48 / 500 | iteration 10 / 30 | Total Loss: 3.6275277137756348 | KNN Loss: 2.590104579925537 | BCE Loss: 1.037423014640808\n",
      "Epoch 48 / 500 | iteration 15 / 30 | Total Loss: 3.583646297454834 | KNN Loss: 2.556044101715088 | BCE Loss: 1.027602195739746\n",
      "Epoch 48 / 500 | iteration 20 / 30 | Total Loss: 3.5707268714904785 | KNN Loss: 2.5334200859069824 | BCE Loss: 1.037306785583496\n",
      "Epoch 48 / 500 | iteration 25 / 30 | Total Loss: 3.574070930480957 | KNN Loss: 2.5663352012634277 | BCE Loss: 1.0077358484268188\n",
      "Epoch 49 / 500 | iteration 0 / 30 | Total Loss: 3.690448760986328 | KNN Loss: 2.6432459354400635 | BCE Loss: 1.0472028255462646\n",
      "Epoch 49 / 500 | iteration 5 / 30 | Total Loss: 3.6470181941986084 | KNN Loss: 2.612492084503174 | BCE Loss: 1.0345261096954346\n",
      "Epoch 49 / 500 | iteration 10 / 30 | Total Loss: 3.565009117126465 | KNN Loss: 2.531285047531128 | BCE Loss: 1.033724069595337\n",
      "Epoch 49 / 500 | iteration 15 / 30 | Total Loss: 3.5476677417755127 | KNN Loss: 2.5174059867858887 | BCE Loss: 1.030261754989624\n",
      "Epoch 49 / 500 | iteration 20 / 30 | Total Loss: 3.5497887134552 | KNN Loss: 2.5237739086151123 | BCE Loss: 1.026014804840088\n",
      "Epoch 49 / 500 | iteration 25 / 30 | Total Loss: 3.5718369483947754 | KNN Loss: 2.524444103240967 | BCE Loss: 1.0473928451538086\n",
      "Epoch 50 / 500 | iteration 0 / 30 | Total Loss: 3.5957164764404297 | KNN Loss: 2.5437233448028564 | BCE Loss: 1.0519931316375732\n",
      "Epoch 50 / 500 | iteration 5 / 30 | Total Loss: 3.5687248706817627 | KNN Loss: 2.519542932510376 | BCE Loss: 1.0491819381713867\n",
      "Epoch 50 / 500 | iteration 10 / 30 | Total Loss: 3.5539004802703857 | KNN Loss: 2.528926134109497 | BCE Loss: 1.0249743461608887\n",
      "Epoch 50 / 500 | iteration 15 / 30 | Total Loss: 3.573669910430908 | KNN Loss: 2.519362688064575 | BCE Loss: 1.054307222366333\n",
      "Epoch 50 / 500 | iteration 20 / 30 | Total Loss: 3.52127742767334 | KNN Loss: 2.5215935707092285 | BCE Loss: 0.9996838569641113\n",
      "Epoch 50 / 500 | iteration 25 / 30 | Total Loss: 3.5874409675598145 | KNN Loss: 2.5777266025543213 | BCE Loss: 1.0097143650054932\n",
      "Epoch 51 / 500 | iteration 0 / 30 | Total Loss: 3.5709595680236816 | KNN Loss: 2.5688068866729736 | BCE Loss: 1.0021525621414185\n",
      "Epoch 51 / 500 | iteration 5 / 30 | Total Loss: 3.568955898284912 | KNN Loss: 2.5362799167633057 | BCE Loss: 1.0326759815216064\n",
      "Epoch 51 / 500 | iteration 10 / 30 | Total Loss: 3.55849552154541 | KNN Loss: 2.533184766769409 | BCE Loss: 1.0253108739852905\n",
      "Epoch 51 / 500 | iteration 15 / 30 | Total Loss: 3.5556955337524414 | KNN Loss: 2.543757915496826 | BCE Loss: 1.0119377374649048\n",
      "Epoch 51 / 500 | iteration 20 / 30 | Total Loss: 3.6579058170318604 | KNN Loss: 2.628721237182617 | BCE Loss: 1.0291845798492432\n",
      "Epoch 51 / 500 | iteration 25 / 30 | Total Loss: 3.647960901260376 | KNN Loss: 2.5927553176879883 | BCE Loss: 1.0552055835723877\n",
      "Epoch 52 / 500 | iteration 0 / 30 | Total Loss: 3.571349620819092 | KNN Loss: 2.5529685020446777 | BCE Loss: 1.018381118774414\n",
      "Epoch 52 / 500 | iteration 5 / 30 | Total Loss: 3.6038970947265625 | KNN Loss: 2.5696940422058105 | BCE Loss: 1.034203052520752\n",
      "Epoch 52 / 500 | iteration 10 / 30 | Total Loss: 3.5576138496398926 | KNN Loss: 2.5198256969451904 | BCE Loss: 1.0377881526947021\n",
      "Epoch 52 / 500 | iteration 15 / 30 | Total Loss: 3.578827381134033 | KNN Loss: 2.5372989177703857 | BCE Loss: 1.0415284633636475\n",
      "Epoch 52 / 500 | iteration 20 / 30 | Total Loss: 3.567652702331543 | KNN Loss: 2.5462613105773926 | BCE Loss: 1.0213913917541504\n",
      "Epoch 52 / 500 | iteration 25 / 30 | Total Loss: 3.566838502883911 | KNN Loss: 2.5251357555389404 | BCE Loss: 1.0417027473449707\n",
      "Epoch 53 / 500 | iteration 0 / 30 | Total Loss: 3.5391182899475098 | KNN Loss: 2.5308353900909424 | BCE Loss: 1.0082828998565674\n",
      "Epoch 53 / 500 | iteration 5 / 30 | Total Loss: 3.586392402648926 | KNN Loss: 2.5927436351776123 | BCE Loss: 0.9936487674713135\n",
      "Epoch 53 / 500 | iteration 10 / 30 | Total Loss: 3.586028814315796 | KNN Loss: 2.5529732704162598 | BCE Loss: 1.0330555438995361\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53 / 500 | iteration 15 / 30 | Total Loss: 3.5945534706115723 | KNN Loss: 2.554644823074341 | BCE Loss: 1.0399086475372314\n",
      "Epoch 53 / 500 | iteration 20 / 30 | Total Loss: 3.570094585418701 | KNN Loss: 2.533419132232666 | BCE Loss: 1.0366753339767456\n",
      "Epoch 53 / 500 | iteration 25 / 30 | Total Loss: 3.58140230178833 | KNN Loss: 2.5324249267578125 | BCE Loss: 1.0489773750305176\n",
      "Epoch 54 / 500 | iteration 0 / 30 | Total Loss: 3.5327978134155273 | KNN Loss: 2.5236971378326416 | BCE Loss: 1.0091006755828857\n",
      "Epoch 54 / 500 | iteration 5 / 30 | Total Loss: 3.5243842601776123 | KNN Loss: 2.516045331954956 | BCE Loss: 1.0083389282226562\n",
      "Epoch 54 / 500 | iteration 10 / 30 | Total Loss: 3.6296427249908447 | KNN Loss: 2.563041925430298 | BCE Loss: 1.0666007995605469\n",
      "Epoch 54 / 500 | iteration 15 / 30 | Total Loss: 3.588491916656494 | KNN Loss: 2.5645387172698975 | BCE Loss: 1.0239530801773071\n",
      "Epoch 54 / 500 | iteration 20 / 30 | Total Loss: 3.616178512573242 | KNN Loss: 2.58389949798584 | BCE Loss: 1.0322790145874023\n",
      "Epoch 54 / 500 | iteration 25 / 30 | Total Loss: 3.5444345474243164 | KNN Loss: 2.5211169719696045 | BCE Loss: 1.023317575454712\n",
      "Epoch 55 / 500 | iteration 0 / 30 | Total Loss: 3.5893757343292236 | KNN Loss: 2.514075994491577 | BCE Loss: 1.0752997398376465\n",
      "Epoch 55 / 500 | iteration 5 / 30 | Total Loss: 3.512373924255371 | KNN Loss: 2.4990012645721436 | BCE Loss: 1.0133726596832275\n",
      "Epoch 55 / 500 | iteration 10 / 30 | Total Loss: 3.563399314880371 | KNN Loss: 2.5414113998413086 | BCE Loss: 1.0219879150390625\n",
      "Epoch 55 / 500 | iteration 15 / 30 | Total Loss: 3.5412135124206543 | KNN Loss: 2.5234134197235107 | BCE Loss: 1.0178000926971436\n",
      "Epoch 55 / 500 | iteration 20 / 30 | Total Loss: 3.5732803344726562 | KNN Loss: 2.5411739349365234 | BCE Loss: 1.0321063995361328\n",
      "Epoch 55 / 500 | iteration 25 / 30 | Total Loss: 3.530704975128174 | KNN Loss: 2.4726178646087646 | BCE Loss: 1.0580871105194092\n",
      "Epoch 56 / 500 | iteration 0 / 30 | Total Loss: 3.607940673828125 | KNN Loss: 2.583552360534668 | BCE Loss: 1.024388313293457\n",
      "Epoch 56 / 500 | iteration 5 / 30 | Total Loss: 3.5958499908447266 | KNN Loss: 2.5636026859283447 | BCE Loss: 1.0322471857070923\n",
      "Epoch 56 / 500 | iteration 10 / 30 | Total Loss: 3.595377206802368 | KNN Loss: 2.560702323913574 | BCE Loss: 1.034674882888794\n",
      "Epoch 56 / 500 | iteration 15 / 30 | Total Loss: 3.5514044761657715 | KNN Loss: 2.5232889652252197 | BCE Loss: 1.0281155109405518\n",
      "Epoch 56 / 500 | iteration 20 / 30 | Total Loss: 3.591789960861206 | KNN Loss: 2.5482017993927 | BCE Loss: 1.0435881614685059\n",
      "Epoch 56 / 500 | iteration 25 / 30 | Total Loss: 3.546933650970459 | KNN Loss: 2.513054370880127 | BCE Loss: 1.0338793992996216\n",
      "Epoch 57 / 500 | iteration 0 / 30 | Total Loss: 3.5462489128112793 | KNN Loss: 2.533602714538574 | BCE Loss: 1.012646198272705\n",
      "Epoch 57 / 500 | iteration 5 / 30 | Total Loss: 3.5855352878570557 | KNN Loss: 2.5622596740722656 | BCE Loss: 1.02327561378479\n",
      "Epoch 57 / 500 | iteration 10 / 30 | Total Loss: 3.56439208984375 | KNN Loss: 2.520195960998535 | BCE Loss: 1.0441960096359253\n",
      "Epoch 57 / 500 | iteration 15 / 30 | Total Loss: 3.593761920928955 | KNN Loss: 2.5675716400146484 | BCE Loss: 1.0261904001235962\n",
      "Epoch 57 / 500 | iteration 20 / 30 | Total Loss: 3.5068392753601074 | KNN Loss: 2.488264560699463 | BCE Loss: 1.0185747146606445\n",
      "Epoch 57 / 500 | iteration 25 / 30 | Total Loss: 3.5254459381103516 | KNN Loss: 2.49787974357605 | BCE Loss: 1.0275661945343018\n",
      "Epoch 58 / 500 | iteration 0 / 30 | Total Loss: 3.5870521068573 | KNN Loss: 2.5430328845977783 | BCE Loss: 1.0440192222595215\n",
      "Epoch 58 / 500 | iteration 5 / 30 | Total Loss: 3.5704431533813477 | KNN Loss: 2.5253350734710693 | BCE Loss: 1.0451081991195679\n",
      "Epoch 58 / 500 | iteration 10 / 30 | Total Loss: 3.522657871246338 | KNN Loss: 2.5259664058685303 | BCE Loss: 0.9966913461685181\n",
      "Epoch 58 / 500 | iteration 15 / 30 | Total Loss: 3.5437655448913574 | KNN Loss: 2.516796350479126 | BCE Loss: 1.026969313621521\n",
      "Epoch 58 / 500 | iteration 20 / 30 | Total Loss: 3.556403875350952 | KNN Loss: 2.520322799682617 | BCE Loss: 1.036081075668335\n",
      "Epoch 58 / 500 | iteration 25 / 30 | Total Loss: 3.5560953617095947 | KNN Loss: 2.537224292755127 | BCE Loss: 1.0188710689544678\n",
      "Epoch 59 / 500 | iteration 0 / 30 | Total Loss: 3.5466132164001465 | KNN Loss: 2.500345230102539 | BCE Loss: 1.0462678670883179\n",
      "Epoch 59 / 500 | iteration 5 / 30 | Total Loss: 3.5897984504699707 | KNN Loss: 2.5721778869628906 | BCE Loss: 1.0176204442977905\n",
      "Epoch 59 / 500 | iteration 10 / 30 | Total Loss: 3.592137575149536 | KNN Loss: 2.5499725341796875 | BCE Loss: 1.0421650409698486\n",
      "Epoch 59 / 500 | iteration 15 / 30 | Total Loss: 3.538165330886841 | KNN Loss: 2.5163280963897705 | BCE Loss: 1.0218372344970703\n",
      "Epoch 59 / 500 | iteration 20 / 30 | Total Loss: 3.564737319946289 | KNN Loss: 2.5352814197540283 | BCE Loss: 1.0294560194015503\n",
      "Epoch 59 / 500 | iteration 25 / 30 | Total Loss: 3.5414164066314697 | KNN Loss: 2.5040371417999268 | BCE Loss: 1.037379264831543\n",
      "Epoch 60 / 500 | iteration 0 / 30 | Total Loss: 3.5508809089660645 | KNN Loss: 2.542410135269165 | BCE Loss: 1.008470892906189\n",
      "Epoch 60 / 500 | iteration 5 / 30 | Total Loss: 3.6257476806640625 | KNN Loss: 2.592022657394409 | BCE Loss: 1.0337250232696533\n",
      "Epoch 60 / 500 | iteration 10 / 30 | Total Loss: 3.570162296295166 | KNN Loss: 2.568587303161621 | BCE Loss: 1.0015751123428345\n",
      "Epoch 60 / 500 | iteration 15 / 30 | Total Loss: 3.539510726928711 | KNN Loss: 2.53456711769104 | BCE Loss: 1.0049434900283813\n",
      "Epoch 60 / 500 | iteration 20 / 30 | Total Loss: 3.5410757064819336 | KNN Loss: 2.5178589820861816 | BCE Loss: 1.023216724395752\n",
      "Epoch 60 / 500 | iteration 25 / 30 | Total Loss: 3.5476694107055664 | KNN Loss: 2.5448827743530273 | BCE Loss: 1.002786636352539\n",
      "Epoch 61 / 500 | iteration 0 / 30 | Total Loss: 3.547478437423706 | KNN Loss: 2.5180087089538574 | BCE Loss: 1.0294697284698486\n",
      "Epoch 61 / 500 | iteration 5 / 30 | Total Loss: 3.5474607944488525 | KNN Loss: 2.538792848587036 | BCE Loss: 1.0086679458618164\n",
      "Epoch 61 / 500 | iteration 10 / 30 | Total Loss: 3.544281244277954 | KNN Loss: 2.528909206390381 | BCE Loss: 1.0153720378875732\n",
      "Epoch 61 / 500 | iteration 15 / 30 | Total Loss: 3.5529727935791016 | KNN Loss: 2.5101847648620605 | BCE Loss: 1.0427879095077515\n",
      "Epoch 61 / 500 | iteration 20 / 30 | Total Loss: 3.537630081176758 | KNN Loss: 2.5230143070220947 | BCE Loss: 1.0146156549453735\n",
      "Epoch 61 / 500 | iteration 25 / 30 | Total Loss: 3.5294575691223145 | KNN Loss: 2.5347540378570557 | BCE Loss: 0.9947034120559692\n",
      "Epoch 62 / 500 | iteration 0 / 30 | Total Loss: 3.5320894718170166 | KNN Loss: 2.5135631561279297 | BCE Loss: 1.018526315689087\n",
      "Epoch 62 / 500 | iteration 5 / 30 | Total Loss: 3.5769455432891846 | KNN Loss: 2.564744710922241 | BCE Loss: 1.0122008323669434\n",
      "Epoch 62 / 500 | iteration 10 / 30 | Total Loss: 3.638843059539795 | KNN Loss: 2.575319290161133 | BCE Loss: 1.0635236501693726\n",
      "Epoch 62 / 500 | iteration 15 / 30 | Total Loss: 3.613396167755127 | KNN Loss: 2.5760717391967773 | BCE Loss: 1.0373245477676392\n",
      "Epoch 62 / 500 | iteration 20 / 30 | Total Loss: 3.520519733428955 | KNN Loss: 2.502885341644287 | BCE Loss: 1.017634391784668\n",
      "Epoch 62 / 500 | iteration 25 / 30 | Total Loss: 3.5627925395965576 | KNN Loss: 2.5399491786956787 | BCE Loss: 1.022843360900879\n",
      "Epoch 63 / 500 | iteration 0 / 30 | Total Loss: 3.603271961212158 | KNN Loss: 2.5696518421173096 | BCE Loss: 1.033619999885559\n",
      "Epoch 63 / 500 | iteration 5 / 30 | Total Loss: 3.578828811645508 | KNN Loss: 2.5404131412506104 | BCE Loss: 1.038415789604187\n",
      "Epoch 63 / 500 | iteration 10 / 30 | Total Loss: 3.6121253967285156 | KNN Loss: 2.543520212173462 | BCE Loss: 1.0686051845550537\n",
      "Epoch 63 / 500 | iteration 15 / 30 | Total Loss: 3.5621554851531982 | KNN Loss: 2.5319268703460693 | BCE Loss: 1.030228614807129\n",
      "Epoch 63 / 500 | iteration 20 / 30 | Total Loss: 3.5264859199523926 | KNN Loss: 2.498471975326538 | BCE Loss: 1.0280139446258545\n",
      "Epoch 63 / 500 | iteration 25 / 30 | Total Loss: 3.561711311340332 | KNN Loss: 2.510568618774414 | BCE Loss: 1.051142692565918\n",
      "Epoch 64 / 500 | iteration 0 / 30 | Total Loss: 3.512409210205078 | KNN Loss: 2.4947805404663086 | BCE Loss: 1.017628788948059\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64 / 500 | iteration 5 / 30 | Total Loss: 3.6058554649353027 | KNN Loss: 2.599349021911621 | BCE Loss: 1.0065064430236816\n",
      "Epoch 64 / 500 | iteration 10 / 30 | Total Loss: 3.5871920585632324 | KNN Loss: 2.5305991172790527 | BCE Loss: 1.0565928220748901\n",
      "Epoch 64 / 500 | iteration 15 / 30 | Total Loss: 3.581334114074707 | KNN Loss: 2.5197882652282715 | BCE Loss: 1.0615458488464355\n",
      "Epoch 64 / 500 | iteration 20 / 30 | Total Loss: 3.5523552894592285 | KNN Loss: 2.511107921600342 | BCE Loss: 1.0412473678588867\n",
      "Epoch 64 / 500 | iteration 25 / 30 | Total Loss: 3.5439321994781494 | KNN Loss: 2.5204761028289795 | BCE Loss: 1.02345609664917\n",
      "Epoch 65 / 500 | iteration 0 / 30 | Total Loss: 3.5868639945983887 | KNN Loss: 2.555251121520996 | BCE Loss: 1.0316128730773926\n",
      "Epoch 65 / 500 | iteration 5 / 30 | Total Loss: 3.5048317909240723 | KNN Loss: 2.4932897090911865 | BCE Loss: 1.0115419626235962\n",
      "Epoch 65 / 500 | iteration 10 / 30 | Total Loss: 3.5775623321533203 | KNN Loss: 2.5503509044647217 | BCE Loss: 1.0272114276885986\n",
      "Epoch 65 / 500 | iteration 15 / 30 | Total Loss: 3.5582635402679443 | KNN Loss: 2.536576509475708 | BCE Loss: 1.0216870307922363\n",
      "Epoch 65 / 500 | iteration 20 / 30 | Total Loss: 3.5508875846862793 | KNN Loss: 2.516209125518799 | BCE Loss: 1.0346784591674805\n",
      "Epoch 65 / 500 | iteration 25 / 30 | Total Loss: 3.579012155532837 | KNN Loss: 2.5485799312591553 | BCE Loss: 1.0304322242736816\n",
      "Epoch 66 / 500 | iteration 0 / 30 | Total Loss: 3.5040080547332764 | KNN Loss: 2.48132061958313 | BCE Loss: 1.0226874351501465\n",
      "Epoch 66 / 500 | iteration 5 / 30 | Total Loss: 3.585435390472412 | KNN Loss: 2.537142276763916 | BCE Loss: 1.0482932329177856\n",
      "Epoch 66 / 500 | iteration 10 / 30 | Total Loss: 3.5411558151245117 | KNN Loss: 2.522386312484741 | BCE Loss: 1.01876962184906\n",
      "Epoch 66 / 500 | iteration 15 / 30 | Total Loss: 3.5237903594970703 | KNN Loss: 2.507584571838379 | BCE Loss: 1.0162057876586914\n",
      "Epoch 66 / 500 | iteration 20 / 30 | Total Loss: 3.545684814453125 | KNN Loss: 2.5363059043884277 | BCE Loss: 1.0093787908554077\n",
      "Epoch 66 / 500 | iteration 25 / 30 | Total Loss: 3.5406057834625244 | KNN Loss: 2.514564037322998 | BCE Loss: 1.0260417461395264\n",
      "Epoch 67 / 500 | iteration 0 / 30 | Total Loss: 3.5358829498291016 | KNN Loss: 2.536015272140503 | BCE Loss: 0.9998677372932434\n",
      "Epoch 67 / 500 | iteration 5 / 30 | Total Loss: 3.5558950901031494 | KNN Loss: 2.5200304985046387 | BCE Loss: 1.0358645915985107\n",
      "Epoch 67 / 500 | iteration 10 / 30 | Total Loss: 3.547670364379883 | KNN Loss: 2.507766008377075 | BCE Loss: 1.039904236793518\n",
      "Epoch 67 / 500 | iteration 15 / 30 | Total Loss: 3.604797840118408 | KNN Loss: 2.5816640853881836 | BCE Loss: 1.023133635520935\n",
      "Epoch 67 / 500 | iteration 20 / 30 | Total Loss: 3.520784854888916 | KNN Loss: 2.506458044052124 | BCE Loss: 1.0143269300460815\n",
      "Epoch 67 / 500 | iteration 25 / 30 | Total Loss: 3.5353317260742188 | KNN Loss: 2.5073742866516113 | BCE Loss: 1.0279574394226074\n",
      "Epoch 68 / 500 | iteration 0 / 30 | Total Loss: 3.5308687686920166 | KNN Loss: 2.529151201248169 | BCE Loss: 1.0017175674438477\n",
      "Epoch 68 / 500 | iteration 5 / 30 | Total Loss: 3.668443202972412 | KNN Loss: 2.5754475593566895 | BCE Loss: 1.0929956436157227\n",
      "Epoch 68 / 500 | iteration 10 / 30 | Total Loss: 3.561873197555542 | KNN Loss: 2.4987337589263916 | BCE Loss: 1.0631394386291504\n",
      "Epoch 68 / 500 | iteration 15 / 30 | Total Loss: 3.587766170501709 | KNN Loss: 2.561774492263794 | BCE Loss: 1.025991678237915\n",
      "Epoch 68 / 500 | iteration 20 / 30 | Total Loss: 3.5209715366363525 | KNN Loss: 2.5170786380767822 | BCE Loss: 1.0038928985595703\n",
      "Epoch 68 / 500 | iteration 25 / 30 | Total Loss: 3.470245122909546 | KNN Loss: 2.4733455181121826 | BCE Loss: 0.9968996047973633\n",
      "Epoch 69 / 500 | iteration 0 / 30 | Total Loss: 3.557960271835327 | KNN Loss: 2.5584497451782227 | BCE Loss: 0.9995105862617493\n",
      "Epoch 69 / 500 | iteration 5 / 30 | Total Loss: 3.5403993129730225 | KNN Loss: 2.5285167694091797 | BCE Loss: 1.0118825435638428\n",
      "Epoch 69 / 500 | iteration 10 / 30 | Total Loss: 3.543607234954834 | KNN Loss: 2.5049495697021484 | BCE Loss: 1.0386576652526855\n",
      "Epoch 69 / 500 | iteration 15 / 30 | Total Loss: 3.571775436401367 | KNN Loss: 2.499009132385254 | BCE Loss: 1.0727661848068237\n",
      "Epoch 69 / 500 | iteration 20 / 30 | Total Loss: 3.5199217796325684 | KNN Loss: 2.481320858001709 | BCE Loss: 1.0386009216308594\n",
      "Epoch 69 / 500 | iteration 25 / 30 | Total Loss: 3.56669545173645 | KNN Loss: 2.5115115642547607 | BCE Loss: 1.0551838874816895\n",
      "Epoch 70 / 500 | iteration 0 / 30 | Total Loss: 3.556489944458008 | KNN Loss: 2.513106107711792 | BCE Loss: 1.0433839559555054\n",
      "Epoch 70 / 500 | iteration 5 / 30 | Total Loss: 3.4961493015289307 | KNN Loss: 2.5016891956329346 | BCE Loss: 0.9944601058959961\n",
      "Epoch 70 / 500 | iteration 10 / 30 | Total Loss: 3.593634843826294 | KNN Loss: 2.533933401107788 | BCE Loss: 1.0597014427185059\n",
      "Epoch 70 / 500 | iteration 15 / 30 | Total Loss: 3.5038483142852783 | KNN Loss: 2.5001955032348633 | BCE Loss: 1.003652811050415\n",
      "Epoch 70 / 500 | iteration 20 / 30 | Total Loss: 3.518815755844116 | KNN Loss: 2.5044877529144287 | BCE Loss: 1.0143280029296875\n",
      "Epoch 70 / 500 | iteration 25 / 30 | Total Loss: 3.542732000350952 | KNN Loss: 2.52807879447937 | BCE Loss: 1.014653205871582\n",
      "Epoch 71 / 500 | iteration 0 / 30 | Total Loss: 3.6024279594421387 | KNN Loss: 2.5604734420776367 | BCE Loss: 1.041954517364502\n",
      "Epoch 71 / 500 | iteration 5 / 30 | Total Loss: 3.5495386123657227 | KNN Loss: 2.521174430847168 | BCE Loss: 1.0283641815185547\n",
      "Epoch 71 / 500 | iteration 10 / 30 | Total Loss: 3.518479347229004 | KNN Loss: 2.5008819103240967 | BCE Loss: 1.0175973176956177\n",
      "Epoch 71 / 500 | iteration 15 / 30 | Total Loss: 3.529680013656616 | KNN Loss: 2.5316786766052246 | BCE Loss: 0.9980012774467468\n",
      "Epoch 71 / 500 | iteration 20 / 30 | Total Loss: 3.5159921646118164 | KNN Loss: 2.511474609375 | BCE Loss: 1.0045175552368164\n",
      "Epoch 71 / 500 | iteration 25 / 30 | Total Loss: 3.526343584060669 | KNN Loss: 2.502664089202881 | BCE Loss: 1.023679494857788\n",
      "Epoch 72 / 500 | iteration 0 / 30 | Total Loss: 3.5315399169921875 | KNN Loss: 2.5138282775878906 | BCE Loss: 1.0177116394042969\n",
      "Epoch 72 / 500 | iteration 5 / 30 | Total Loss: 3.54347825050354 | KNN Loss: 2.5110573768615723 | BCE Loss: 1.0324208736419678\n",
      "Epoch 72 / 500 | iteration 10 / 30 | Total Loss: 3.5522587299346924 | KNN Loss: 2.548332452774048 | BCE Loss: 1.0039262771606445\n",
      "Epoch 72 / 500 | iteration 15 / 30 | Total Loss: 3.5987982749938965 | KNN Loss: 2.5409154891967773 | BCE Loss: 1.0578826665878296\n",
      "Epoch 72 / 500 | iteration 20 / 30 | Total Loss: 3.5733232498168945 | KNN Loss: 2.519556760787964 | BCE Loss: 1.0537664890289307\n",
      "Epoch 72 / 500 | iteration 25 / 30 | Total Loss: 3.605924367904663 | KNN Loss: 2.552232027053833 | BCE Loss: 1.05369234085083\n",
      "Epoch 73 / 500 | iteration 0 / 30 | Total Loss: 3.5635387897491455 | KNN Loss: 2.54953670501709 | BCE Loss: 1.0140020847320557\n",
      "Epoch 73 / 500 | iteration 5 / 30 | Total Loss: 3.536220073699951 | KNN Loss: 2.5074682235717773 | BCE Loss: 1.0287519693374634\n",
      "Epoch 73 / 500 | iteration 10 / 30 | Total Loss: 3.580333948135376 | KNN Loss: 2.529737710952759 | BCE Loss: 1.0505962371826172\n",
      "Epoch 73 / 500 | iteration 15 / 30 | Total Loss: 3.5689895153045654 | KNN Loss: 2.555910348892212 | BCE Loss: 1.0130791664123535\n",
      "Epoch 73 / 500 | iteration 20 / 30 | Total Loss: 3.5322208404541016 | KNN Loss: 2.519585371017456 | BCE Loss: 1.0126354694366455\n",
      "Epoch 73 / 500 | iteration 25 / 30 | Total Loss: 3.5938262939453125 | KNN Loss: 2.563486337661743 | BCE Loss: 1.0303399562835693\n",
      "Epoch 74 / 500 | iteration 0 / 30 | Total Loss: 3.5795207023620605 | KNN Loss: 2.557121515274048 | BCE Loss: 1.0223991870880127\n",
      "Epoch 74 / 500 | iteration 5 / 30 | Total Loss: 3.5519609451293945 | KNN Loss: 2.5253994464874268 | BCE Loss: 1.0265613794326782\n",
      "Epoch 74 / 500 | iteration 10 / 30 | Total Loss: 3.5764408111572266 | KNN Loss: 2.538329601287842 | BCE Loss: 1.0381113290786743\n",
      "Epoch 74 / 500 | iteration 15 / 30 | Total Loss: 3.5774927139282227 | KNN Loss: 2.546740770339966 | BCE Loss: 1.0307520627975464\n",
      "Epoch 74 / 500 | iteration 20 / 30 | Total Loss: 3.587559223175049 | KNN Loss: 2.5493361949920654 | BCE Loss: 1.0382230281829834\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74 / 500 | iteration 25 / 30 | Total Loss: 3.5620498657226562 | KNN Loss: 2.523221254348755 | BCE Loss: 1.0388284921646118\n",
      "Epoch 75 / 500 | iteration 0 / 30 | Total Loss: 3.5502638816833496 | KNN Loss: 2.5138258934020996 | BCE Loss: 1.0364378690719604\n",
      "Epoch 75 / 500 | iteration 5 / 30 | Total Loss: 3.5414342880249023 | KNN Loss: 2.5267117023468018 | BCE Loss: 1.0147227048873901\n",
      "Epoch 75 / 500 | iteration 10 / 30 | Total Loss: 3.581902265548706 | KNN Loss: 2.5560367107391357 | BCE Loss: 1.0258655548095703\n",
      "Epoch 75 / 500 | iteration 15 / 30 | Total Loss: 3.600609540939331 | KNN Loss: 2.578886032104492 | BCE Loss: 1.0217235088348389\n",
      "Epoch 75 / 500 | iteration 20 / 30 | Total Loss: 3.589806079864502 | KNN Loss: 2.5425126552581787 | BCE Loss: 1.0472933053970337\n",
      "Epoch 75 / 500 | iteration 25 / 30 | Total Loss: 3.543903112411499 | KNN Loss: 2.526517391204834 | BCE Loss: 1.017385721206665\n",
      "Epoch 76 / 500 | iteration 0 / 30 | Total Loss: 3.5625858306884766 | KNN Loss: 2.5251457691192627 | BCE Loss: 1.0374400615692139\n",
      "Epoch 76 / 500 | iteration 5 / 30 | Total Loss: 3.5188326835632324 | KNN Loss: 2.497910737991333 | BCE Loss: 1.0209218263626099\n",
      "Epoch 76 / 500 | iteration 10 / 30 | Total Loss: 3.5473666191101074 | KNN Loss: 2.5174996852874756 | BCE Loss: 1.0298670530319214\n",
      "Epoch 76 / 500 | iteration 15 / 30 | Total Loss: 3.538914680480957 | KNN Loss: 2.5158824920654297 | BCE Loss: 1.0230320692062378\n",
      "Epoch 76 / 500 | iteration 20 / 30 | Total Loss: 3.5474417209625244 | KNN Loss: 2.5120584964752197 | BCE Loss: 1.0353832244873047\n",
      "Epoch 76 / 500 | iteration 25 / 30 | Total Loss: 3.545620918273926 | KNN Loss: 2.5177342891693115 | BCE Loss: 1.0278867483139038\n",
      "Epoch 77 / 500 | iteration 0 / 30 | Total Loss: 3.58650279045105 | KNN Loss: 2.5505635738372803 | BCE Loss: 1.0359392166137695\n",
      "Epoch 77 / 500 | iteration 5 / 30 | Total Loss: 3.5591630935668945 | KNN Loss: 2.4947338104248047 | BCE Loss: 1.0644292831420898\n",
      "Epoch 77 / 500 | iteration 10 / 30 | Total Loss: 3.5475711822509766 | KNN Loss: 2.519044876098633 | BCE Loss: 1.0285263061523438\n",
      "Epoch 77 / 500 | iteration 15 / 30 | Total Loss: 3.5626542568206787 | KNN Loss: 2.536496639251709 | BCE Loss: 1.0261576175689697\n",
      "Epoch 77 / 500 | iteration 20 / 30 | Total Loss: 3.550140857696533 | KNN Loss: 2.5139529705047607 | BCE Loss: 1.0361878871917725\n",
      "Epoch 77 / 500 | iteration 25 / 30 | Total Loss: 3.535121440887451 | KNN Loss: 2.4942126274108887 | BCE Loss: 1.040908694267273\n",
      "Epoch 78 / 500 | iteration 0 / 30 | Total Loss: 3.5396227836608887 | KNN Loss: 2.5124878883361816 | BCE Loss: 1.027134895324707\n",
      "Epoch 78 / 500 | iteration 5 / 30 | Total Loss: 3.556077241897583 | KNN Loss: 2.5106334686279297 | BCE Loss: 1.0454437732696533\n",
      "Epoch 78 / 500 | iteration 10 / 30 | Total Loss: 3.5266878604888916 | KNN Loss: 2.5011253356933594 | BCE Loss: 1.0255625247955322\n",
      "Epoch 78 / 500 | iteration 15 / 30 | Total Loss: 3.5164103507995605 | KNN Loss: 2.512092113494873 | BCE Loss: 1.0043182373046875\n",
      "Epoch 78 / 500 | iteration 20 / 30 | Total Loss: 3.551755666732788 | KNN Loss: 2.4972825050354004 | BCE Loss: 1.0544731616973877\n",
      "Epoch 78 / 500 | iteration 25 / 30 | Total Loss: 3.542430877685547 | KNN Loss: 2.5144567489624023 | BCE Loss: 1.0279741287231445\n",
      "Epoch 79 / 500 | iteration 0 / 30 | Total Loss: 3.552556037902832 | KNN Loss: 2.524366855621338 | BCE Loss: 1.0281893014907837\n",
      "Epoch 79 / 500 | iteration 5 / 30 | Total Loss: 3.5735538005828857 | KNN Loss: 2.5470316410064697 | BCE Loss: 1.026522159576416\n",
      "Epoch 79 / 500 | iteration 10 / 30 | Total Loss: 3.5457048416137695 | KNN Loss: 2.512470006942749 | BCE Loss: 1.0332348346710205\n",
      "Epoch 79 / 500 | iteration 15 / 30 | Total Loss: 3.5440480709075928 | KNN Loss: 2.5093586444854736 | BCE Loss: 1.0346894264221191\n",
      "Epoch 79 / 500 | iteration 20 / 30 | Total Loss: 3.5714991092681885 | KNN Loss: 2.5649662017822266 | BCE Loss: 1.006532907485962\n",
      "Epoch 79 / 500 | iteration 25 / 30 | Total Loss: 3.589980125427246 | KNN Loss: 2.532383918762207 | BCE Loss: 1.057596206665039\n",
      "Epoch 80 / 500 | iteration 0 / 30 | Total Loss: 3.603538751602173 | KNN Loss: 2.5571022033691406 | BCE Loss: 1.0464365482330322\n",
      "Epoch 80 / 500 | iteration 5 / 30 | Total Loss: 3.54677677154541 | KNN Loss: 2.535715103149414 | BCE Loss: 1.0110617876052856\n",
      "Epoch 80 / 500 | iteration 10 / 30 | Total Loss: 3.5944955348968506 | KNN Loss: 2.5631566047668457 | BCE Loss: 1.0313389301300049\n",
      "Epoch 80 / 500 | iteration 15 / 30 | Total Loss: 3.5369887351989746 | KNN Loss: 2.5386803150177 | BCE Loss: 0.9983083009719849\n",
      "Epoch 80 / 500 | iteration 20 / 30 | Total Loss: 3.516352891921997 | KNN Loss: 2.5204074382781982 | BCE Loss: 0.9959454536437988\n",
      "Epoch 80 / 500 | iteration 25 / 30 | Total Loss: 3.5812644958496094 | KNN Loss: 2.567957878112793 | BCE Loss: 1.0133064985275269\n",
      "Epoch 81 / 500 | iteration 0 / 30 | Total Loss: 3.551628589630127 | KNN Loss: 2.540850877761841 | BCE Loss: 1.0107777118682861\n",
      "Epoch 81 / 500 | iteration 5 / 30 | Total Loss: 3.5313961505889893 | KNN Loss: 2.525972604751587 | BCE Loss: 1.0054235458374023\n",
      "Epoch 81 / 500 | iteration 10 / 30 | Total Loss: 3.58111834526062 | KNN Loss: 2.5352163314819336 | BCE Loss: 1.0459020137786865\n",
      "Epoch 81 / 500 | iteration 15 / 30 | Total Loss: 3.5924623012542725 | KNN Loss: 2.5567550659179688 | BCE Loss: 1.0357072353363037\n",
      "Epoch 81 / 500 | iteration 20 / 30 | Total Loss: 3.5442490577697754 | KNN Loss: 2.5234858989715576 | BCE Loss: 1.0207630395889282\n",
      "Epoch 81 / 500 | iteration 25 / 30 | Total Loss: 3.587317943572998 | KNN Loss: 2.5187249183654785 | BCE Loss: 1.0685930252075195\n",
      "Epoch 82 / 500 | iteration 0 / 30 | Total Loss: 3.5016884803771973 | KNN Loss: 2.5025992393493652 | BCE Loss: 0.9990891218185425\n",
      "Epoch 82 / 500 | iteration 5 / 30 | Total Loss: 3.565748453140259 | KNN Loss: 2.5254292488098145 | BCE Loss: 1.0403192043304443\n",
      "Epoch 82 / 500 | iteration 10 / 30 | Total Loss: 3.593365430831909 | KNN Loss: 2.565206289291382 | BCE Loss: 1.0281591415405273\n",
      "Epoch 82 / 500 | iteration 15 / 30 | Total Loss: 3.6068077087402344 | KNN Loss: 2.5512444972991943 | BCE Loss: 1.05556321144104\n",
      "Epoch 82 / 500 | iteration 20 / 30 | Total Loss: 3.587282657623291 | KNN Loss: 2.546391010284424 | BCE Loss: 1.0408916473388672\n",
      "Epoch 82 / 500 | iteration 25 / 30 | Total Loss: 3.5405375957489014 | KNN Loss: 2.5221312046051025 | BCE Loss: 1.0184063911437988\n",
      "Epoch 83 / 500 | iteration 0 / 30 | Total Loss: 3.535637378692627 | KNN Loss: 2.508556365966797 | BCE Loss: 1.02708101272583\n",
      "Epoch 83 / 500 | iteration 5 / 30 | Total Loss: 3.51072359085083 | KNN Loss: 2.50288462638855 | BCE Loss: 1.0078389644622803\n",
      "Epoch 83 / 500 | iteration 10 / 30 | Total Loss: 3.5237576961517334 | KNN Loss: 2.4899017810821533 | BCE Loss: 1.03385591506958\n",
      "Epoch 83 / 500 | iteration 15 / 30 | Total Loss: 3.585628032684326 | KNN Loss: 2.552926540374756 | BCE Loss: 1.0327013731002808\n",
      "Epoch 83 / 500 | iteration 20 / 30 | Total Loss: 3.5100674629211426 | KNN Loss: 2.500261068344116 | BCE Loss: 1.009806513786316\n",
      "Epoch 83 / 500 | iteration 25 / 30 | Total Loss: 3.5099332332611084 | KNN Loss: 2.5346009731292725 | BCE Loss: 0.9753322601318359\n",
      "Epoch 84 / 500 | iteration 0 / 30 | Total Loss: 3.561328411102295 | KNN Loss: 2.536853551864624 | BCE Loss: 1.0244749784469604\n",
      "Epoch 84 / 500 | iteration 5 / 30 | Total Loss: 3.521789312362671 | KNN Loss: 2.5106539726257324 | BCE Loss: 1.0111353397369385\n",
      "Epoch 84 / 500 | iteration 10 / 30 | Total Loss: 3.577530860900879 | KNN Loss: 2.515036106109619 | BCE Loss: 1.0624946355819702\n",
      "Epoch 84 / 500 | iteration 15 / 30 | Total Loss: 3.5862910747528076 | KNN Loss: 2.5501480102539062 | BCE Loss: 1.0361430644989014\n",
      "Epoch 84 / 500 | iteration 20 / 30 | Total Loss: 3.5238630771636963 | KNN Loss: 2.511353015899658 | BCE Loss: 1.012510061264038\n",
      "Epoch 84 / 500 | iteration 25 / 30 | Total Loss: 3.5423433780670166 | KNN Loss: 2.4845919609069824 | BCE Loss: 1.0577514171600342\n",
      "Epoch 85 / 500 | iteration 0 / 30 | Total Loss: 3.5288610458374023 | KNN Loss: 2.4941658973693848 | BCE Loss: 1.0346952676773071\n",
      "Epoch 85 / 500 | iteration 5 / 30 | Total Loss: 3.5330779552459717 | KNN Loss: 2.5141794681549072 | BCE Loss: 1.0188984870910645\n",
      "Epoch 85 / 500 | iteration 10 / 30 | Total Loss: 3.573279619216919 | KNN Loss: 2.5426313877105713 | BCE Loss: 1.0306482315063477\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85 / 500 | iteration 15 / 30 | Total Loss: 3.5573840141296387 | KNN Loss: 2.5480592250823975 | BCE Loss: 1.0093249082565308\n",
      "Epoch 85 / 500 | iteration 20 / 30 | Total Loss: 3.517387866973877 | KNN Loss: 2.503220319747925 | BCE Loss: 1.0141675472259521\n",
      "Epoch 85 / 500 | iteration 25 / 30 | Total Loss: 3.579596996307373 | KNN Loss: 2.5244650840759277 | BCE Loss: 1.0551319122314453\n",
      "Epoch 86 / 500 | iteration 0 / 30 | Total Loss: 3.58402419090271 | KNN Loss: 2.5128889083862305 | BCE Loss: 1.0711352825164795\n",
      "Epoch 86 / 500 | iteration 5 / 30 | Total Loss: 3.5946221351623535 | KNN Loss: 2.5446226596832275 | BCE Loss: 1.0499993562698364\n",
      "Epoch 86 / 500 | iteration 10 / 30 | Total Loss: 3.5960631370544434 | KNN Loss: 2.5850255489349365 | BCE Loss: 1.0110375881195068\n",
      "Epoch 86 / 500 | iteration 15 / 30 | Total Loss: 3.520005464553833 | KNN Loss: 2.5244715213775635 | BCE Loss: 0.9955338835716248\n",
      "Epoch 86 / 500 | iteration 20 / 30 | Total Loss: 3.572930097579956 | KNN Loss: 2.525005340576172 | BCE Loss: 1.0479247570037842\n",
      "Epoch 86 / 500 | iteration 25 / 30 | Total Loss: 3.562138557434082 | KNN Loss: 2.5227417945861816 | BCE Loss: 1.0393967628479004\n",
      "Epoch 87 / 500 | iteration 0 / 30 | Total Loss: 3.557497978210449 | KNN Loss: 2.5070478916168213 | BCE Loss: 1.0504502058029175\n",
      "Epoch 87 / 500 | iteration 5 / 30 | Total Loss: 3.6060824394226074 | KNN Loss: 2.552964925765991 | BCE Loss: 1.0531173944473267\n",
      "Epoch 87 / 500 | iteration 10 / 30 | Total Loss: 3.5067319869995117 | KNN Loss: 2.4749789237976074 | BCE Loss: 1.0317529439926147\n",
      "Epoch 87 / 500 | iteration 15 / 30 | Total Loss: 3.485158920288086 | KNN Loss: 2.4830498695373535 | BCE Loss: 1.002109169960022\n",
      "Epoch 87 / 500 | iteration 20 / 30 | Total Loss: 3.521470308303833 | KNN Loss: 2.49277925491333 | BCE Loss: 1.028691053390503\n",
      "Epoch 87 / 500 | iteration 25 / 30 | Total Loss: 3.5367398262023926 | KNN Loss: 2.512741804122925 | BCE Loss: 1.0239980220794678\n",
      "Epoch 88 / 500 | iteration 0 / 30 | Total Loss: 3.527707099914551 | KNN Loss: 2.5224857330322266 | BCE Loss: 1.0052214860916138\n",
      "Epoch 88 / 500 | iteration 5 / 30 | Total Loss: 3.589138984680176 | KNN Loss: 2.5257105827331543 | BCE Loss: 1.063428282737732\n",
      "Epoch 88 / 500 | iteration 10 / 30 | Total Loss: 3.5591516494750977 | KNN Loss: 2.5461623668670654 | BCE Loss: 1.0129891633987427\n",
      "Epoch 88 / 500 | iteration 15 / 30 | Total Loss: 3.515090227127075 | KNN Loss: 2.5014514923095703 | BCE Loss: 1.0136387348175049\n",
      "Epoch 88 / 500 | iteration 20 / 30 | Total Loss: 3.554321765899658 | KNN Loss: 2.546776294708252 | BCE Loss: 1.0075454711914062\n",
      "Epoch 88 / 500 | iteration 25 / 30 | Total Loss: 3.5246942043304443 | KNN Loss: 2.5063865184783936 | BCE Loss: 1.0183076858520508\n",
      "Epoch 89 / 500 | iteration 0 / 30 | Total Loss: 3.5730152130126953 | KNN Loss: 2.5246827602386475 | BCE Loss: 1.0483324527740479\n",
      "Epoch 89 / 500 | iteration 5 / 30 | Total Loss: 3.5655391216278076 | KNN Loss: 2.5200369358062744 | BCE Loss: 1.0455021858215332\n",
      "Epoch 89 / 500 | iteration 10 / 30 | Total Loss: 3.5077104568481445 | KNN Loss: 2.5003855228424072 | BCE Loss: 1.0073249340057373\n",
      "Epoch 89 / 500 | iteration 15 / 30 | Total Loss: 3.5390617847442627 | KNN Loss: 2.5126845836639404 | BCE Loss: 1.0263772010803223\n",
      "Epoch 89 / 500 | iteration 20 / 30 | Total Loss: 3.5051026344299316 | KNN Loss: 2.4968762397766113 | BCE Loss: 1.0082265138626099\n",
      "Epoch 89 / 500 | iteration 25 / 30 | Total Loss: 3.540717363357544 | KNN Loss: 2.519484281539917 | BCE Loss: 1.021233081817627\n",
      "Epoch 90 / 500 | iteration 0 / 30 | Total Loss: 3.5519497394561768 | KNN Loss: 2.517716407775879 | BCE Loss: 1.0342333316802979\n",
      "Epoch 90 / 500 | iteration 5 / 30 | Total Loss: 3.517437219619751 | KNN Loss: 2.4937710762023926 | BCE Loss: 1.0236661434173584\n",
      "Epoch 90 / 500 | iteration 10 / 30 | Total Loss: 3.5375173091888428 | KNN Loss: 2.483365058898926 | BCE Loss: 1.054152250289917\n",
      "Epoch 90 / 500 | iteration 15 / 30 | Total Loss: 3.5181117057800293 | KNN Loss: 2.5224859714508057 | BCE Loss: 0.9956256151199341\n",
      "Epoch 90 / 500 | iteration 20 / 30 | Total Loss: 3.552036762237549 | KNN Loss: 2.530195713043213 | BCE Loss: 1.0218411684036255\n",
      "Epoch 90 / 500 | iteration 25 / 30 | Total Loss: 3.522022247314453 | KNN Loss: 2.4913294315338135 | BCE Loss: 1.03069269657135\n",
      "Epoch 91 / 500 | iteration 0 / 30 | Total Loss: 3.5389344692230225 | KNN Loss: 2.4938888549804688 | BCE Loss: 1.0450456142425537\n",
      "Epoch 91 / 500 | iteration 5 / 30 | Total Loss: 3.549515724182129 | KNN Loss: 2.526578187942505 | BCE Loss: 1.0229374170303345\n",
      "Epoch 91 / 500 | iteration 10 / 30 | Total Loss: 3.522153377532959 | KNN Loss: 2.504683017730713 | BCE Loss: 1.017470359802246\n",
      "Epoch 91 / 500 | iteration 15 / 30 | Total Loss: 3.5754616260528564 | KNN Loss: 2.5341062545776367 | BCE Loss: 1.0413553714752197\n",
      "Epoch 91 / 500 | iteration 20 / 30 | Total Loss: 3.544424057006836 | KNN Loss: 2.5124714374542236 | BCE Loss: 1.0319526195526123\n",
      "Epoch 91 / 500 | iteration 25 / 30 | Total Loss: 3.509443998336792 | KNN Loss: 2.487611770629883 | BCE Loss: 1.0218322277069092\n",
      "Epoch 92 / 500 | iteration 0 / 30 | Total Loss: 3.5529468059539795 | KNN Loss: 2.547173261642456 | BCE Loss: 1.0057735443115234\n",
      "Epoch 92 / 500 | iteration 5 / 30 | Total Loss: 3.617187023162842 | KNN Loss: 2.5584840774536133 | BCE Loss: 1.0587029457092285\n",
      "Epoch 92 / 500 | iteration 10 / 30 | Total Loss: 3.5335497856140137 | KNN Loss: 2.52436900138855 | BCE Loss: 1.0091809034347534\n",
      "Epoch 92 / 500 | iteration 15 / 30 | Total Loss: 3.5344271659851074 | KNN Loss: 2.5078818798065186 | BCE Loss: 1.0265452861785889\n",
      "Epoch 92 / 500 | iteration 20 / 30 | Total Loss: 3.543522596359253 | KNN Loss: 2.5246529579162598 | BCE Loss: 1.0188696384429932\n",
      "Epoch 92 / 500 | iteration 25 / 30 | Total Loss: 3.542689323425293 | KNN Loss: 2.5351083278656006 | BCE Loss: 1.0075809955596924\n",
      "Epoch 93 / 500 | iteration 0 / 30 | Total Loss: 3.559360980987549 | KNN Loss: 2.526679039001465 | BCE Loss: 1.032681941986084\n",
      "Epoch 93 / 500 | iteration 5 / 30 | Total Loss: 3.5686984062194824 | KNN Loss: 2.5296716690063477 | BCE Loss: 1.0390268564224243\n",
      "Epoch 93 / 500 | iteration 10 / 30 | Total Loss: 3.5263805389404297 | KNN Loss: 2.51581072807312 | BCE Loss: 1.0105698108673096\n",
      "Epoch 93 / 500 | iteration 15 / 30 | Total Loss: 3.5451056957244873 | KNN Loss: 2.5002450942993164 | BCE Loss: 1.044860601425171\n",
      "Epoch 93 / 500 | iteration 20 / 30 | Total Loss: 3.5140068531036377 | KNN Loss: 2.5048022270202637 | BCE Loss: 1.009204626083374\n",
      "Epoch 93 / 500 | iteration 25 / 30 | Total Loss: 3.520092010498047 | KNN Loss: 2.503453254699707 | BCE Loss: 1.0166387557983398\n",
      "Epoch 94 / 500 | iteration 0 / 30 | Total Loss: 3.4930615425109863 | KNN Loss: 2.467482566833496 | BCE Loss: 1.0255789756774902\n",
      "Epoch 94 / 500 | iteration 5 / 30 | Total Loss: 3.567748785018921 | KNN Loss: 2.5296878814697266 | BCE Loss: 1.0380609035491943\n",
      "Epoch 94 / 500 | iteration 10 / 30 | Total Loss: 3.548349142074585 | KNN Loss: 2.5105888843536377 | BCE Loss: 1.0377602577209473\n",
      "Epoch 94 / 500 | iteration 15 / 30 | Total Loss: 3.56247878074646 | KNN Loss: 2.542590379714966 | BCE Loss: 1.0198884010314941\n",
      "Epoch 94 / 500 | iteration 20 / 30 | Total Loss: 3.5363478660583496 | KNN Loss: 2.5140442848205566 | BCE Loss: 1.0223034620285034\n",
      "Epoch 94 / 500 | iteration 25 / 30 | Total Loss: 3.568812847137451 | KNN Loss: 2.5350193977355957 | BCE Loss: 1.033793568611145\n",
      "Epoch 95 / 500 | iteration 0 / 30 | Total Loss: 3.5319290161132812 | KNN Loss: 2.5156280994415283 | BCE Loss: 1.0163007974624634\n",
      "Epoch 95 / 500 | iteration 5 / 30 | Total Loss: 3.5672807693481445 | KNN Loss: 2.5334184169769287 | BCE Loss: 1.0338623523712158\n",
      "Epoch 95 / 500 | iteration 10 / 30 | Total Loss: 3.5271005630493164 | KNN Loss: 2.501771926879883 | BCE Loss: 1.025328516960144\n",
      "Epoch 95 / 500 | iteration 15 / 30 | Total Loss: 3.5655384063720703 | KNN Loss: 2.538672924041748 | BCE Loss: 1.0268653631210327\n",
      "Epoch 95 / 500 | iteration 20 / 30 | Total Loss: 3.5732433795928955 | KNN Loss: 2.512183666229248 | BCE Loss: 1.0610597133636475\n",
      "Epoch 95 / 500 | iteration 25 / 30 | Total Loss: 3.541393280029297 | KNN Loss: 2.504020929336548 | BCE Loss: 1.037372350692749\n",
      "Epoch 96 / 500 | iteration 0 / 30 | Total Loss: 3.5451536178588867 | KNN Loss: 2.5452933311462402 | BCE Loss: 0.9998602867126465\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96 / 500 | iteration 5 / 30 | Total Loss: 3.5368642807006836 | KNN Loss: 2.5356953144073486 | BCE Loss: 1.0011690855026245\n",
      "Epoch 96 / 500 | iteration 10 / 30 | Total Loss: 3.5779151916503906 | KNN Loss: 2.537531852722168 | BCE Loss: 1.0403833389282227\n",
      "Epoch 96 / 500 | iteration 15 / 30 | Total Loss: 3.532299041748047 | KNN Loss: 2.51983380317688 | BCE Loss: 1.012465238571167\n",
      "Epoch 96 / 500 | iteration 20 / 30 | Total Loss: 3.562077760696411 | KNN Loss: 2.5262606143951416 | BCE Loss: 1.0358171463012695\n",
      "Epoch 96 / 500 | iteration 25 / 30 | Total Loss: 3.5010180473327637 | KNN Loss: 2.4703311920166016 | BCE Loss: 1.0306869745254517\n",
      "Epoch 97 / 500 | iteration 0 / 30 | Total Loss: 3.5403804779052734 | KNN Loss: 2.4941577911376953 | BCE Loss: 1.0462225675582886\n",
      "Epoch 97 / 500 | iteration 5 / 30 | Total Loss: 3.541656255722046 | KNN Loss: 2.531221866607666 | BCE Loss: 1.0104343891143799\n",
      "Epoch 97 / 500 | iteration 10 / 30 | Total Loss: 3.5166468620300293 | KNN Loss: 2.4980311393737793 | BCE Loss: 1.01861572265625\n",
      "Epoch 97 / 500 | iteration 15 / 30 | Total Loss: 3.503491163253784 | KNN Loss: 2.477544069290161 | BCE Loss: 1.025947093963623\n",
      "Epoch 97 / 500 | iteration 20 / 30 | Total Loss: 3.5373263359069824 | KNN Loss: 2.496678352355957 | BCE Loss: 1.040648102760315\n",
      "Epoch 97 / 500 | iteration 25 / 30 | Total Loss: 3.562152862548828 | KNN Loss: 2.5249643325805664 | BCE Loss: 1.0371884107589722\n",
      "Epoch 98 / 500 | iteration 0 / 30 | Total Loss: 3.566082239151001 | KNN Loss: 2.5518171787261963 | BCE Loss: 1.0142650604248047\n",
      "Epoch 98 / 500 | iteration 5 / 30 | Total Loss: 3.5786426067352295 | KNN Loss: 2.539309024810791 | BCE Loss: 1.0393335819244385\n",
      "Epoch 98 / 500 | iteration 10 / 30 | Total Loss: 3.578524351119995 | KNN Loss: 2.536874532699585 | BCE Loss: 1.0416498184204102\n",
      "Epoch 98 / 500 | iteration 15 / 30 | Total Loss: 3.5691328048706055 | KNN Loss: 2.5284500122070312 | BCE Loss: 1.0406827926635742\n",
      "Epoch 98 / 500 | iteration 20 / 30 | Total Loss: 3.5045526027679443 | KNN Loss: 2.486599922180176 | BCE Loss: 1.0179526805877686\n",
      "Epoch 98 / 500 | iteration 25 / 30 | Total Loss: 3.534513473510742 | KNN Loss: 2.525305986404419 | BCE Loss: 1.0092073678970337\n",
      "Epoch 99 / 500 | iteration 0 / 30 | Total Loss: 3.5598201751708984 | KNN Loss: 2.5246617794036865 | BCE Loss: 1.0351582765579224\n",
      "Epoch 99 / 500 | iteration 5 / 30 | Total Loss: 3.5360658168792725 | KNN Loss: 2.4831130504608154 | BCE Loss: 1.052952766418457\n",
      "Epoch 99 / 500 | iteration 10 / 30 | Total Loss: 3.578436851501465 | KNN Loss: 2.558715581893921 | BCE Loss: 1.0197211503982544\n",
      "Epoch 99 / 500 | iteration 15 / 30 | Total Loss: 3.4942209720611572 | KNN Loss: 2.4935622215270996 | BCE Loss: 1.0006587505340576\n",
      "Epoch 99 / 500 | iteration 20 / 30 | Total Loss: 3.5102787017822266 | KNN Loss: 2.4783036708831787 | BCE Loss: 1.0319750308990479\n",
      "Epoch 99 / 500 | iteration 25 / 30 | Total Loss: 3.5053505897521973 | KNN Loss: 2.518585205078125 | BCE Loss: 0.9867655038833618\n",
      "Epoch 100 / 500 | iteration 0 / 30 | Total Loss: 3.534419536590576 | KNN Loss: 2.505274534225464 | BCE Loss: 1.0291451215744019\n",
      "Epoch 100 / 500 | iteration 5 / 30 | Total Loss: 3.521054983139038 | KNN Loss: 2.496213912963867 | BCE Loss: 1.024841070175171\n",
      "Epoch 100 / 500 | iteration 10 / 30 | Total Loss: 3.5863053798675537 | KNN Loss: 2.5430660247802734 | BCE Loss: 1.0432393550872803\n",
      "Epoch 100 / 500 | iteration 15 / 30 | Total Loss: 3.540860652923584 | KNN Loss: 2.498478651046753 | BCE Loss: 1.0423818826675415\n",
      "Epoch 100 / 500 | iteration 20 / 30 | Total Loss: 3.5320873260498047 | KNN Loss: 2.501293659210205 | BCE Loss: 1.0307936668395996\n",
      "Epoch 100 / 500 | iteration 25 / 30 | Total Loss: 3.548536777496338 | KNN Loss: 2.5286455154418945 | BCE Loss: 1.0198912620544434\n",
      "Epoch 101 / 500 | iteration 0 / 30 | Total Loss: 3.540863037109375 | KNN Loss: 2.5287716388702393 | BCE Loss: 1.0120913982391357\n",
      "Epoch 101 / 500 | iteration 5 / 30 | Total Loss: 3.5251572132110596 | KNN Loss: 2.5012922286987305 | BCE Loss: 1.023864984512329\n",
      "Epoch 101 / 500 | iteration 10 / 30 | Total Loss: 3.5987191200256348 | KNN Loss: 2.5506153106689453 | BCE Loss: 1.0481038093566895\n",
      "Epoch 101 / 500 | iteration 15 / 30 | Total Loss: 3.5781028270721436 | KNN Loss: 2.537666082382202 | BCE Loss: 1.0404367446899414\n",
      "Epoch 101 / 500 | iteration 20 / 30 | Total Loss: 3.5200247764587402 | KNN Loss: 2.500408411026001 | BCE Loss: 1.0196162462234497\n",
      "Epoch 101 / 500 | iteration 25 / 30 | Total Loss: 3.5831785202026367 | KNN Loss: 2.568737745285034 | BCE Loss: 1.0144407749176025\n",
      "Epoch 102 / 500 | iteration 0 / 30 | Total Loss: 3.582425832748413 | KNN Loss: 2.5447661876678467 | BCE Loss: 1.0376596450805664\n",
      "Epoch 102 / 500 | iteration 5 / 30 | Total Loss: 3.5994393825531006 | KNN Loss: 2.581817150115967 | BCE Loss: 1.0176222324371338\n",
      "Epoch 102 / 500 | iteration 10 / 30 | Total Loss: 3.549800395965576 | KNN Loss: 2.5360729694366455 | BCE Loss: 1.0137275457382202\n",
      "Epoch 102 / 500 | iteration 15 / 30 | Total Loss: 3.5484039783477783 | KNN Loss: 2.523597478866577 | BCE Loss: 1.0248064994812012\n",
      "Epoch 102 / 500 | iteration 20 / 30 | Total Loss: 3.4816229343414307 | KNN Loss: 2.462958574295044 | BCE Loss: 1.0186643600463867\n",
      "Epoch 102 / 500 | iteration 25 / 30 | Total Loss: 3.4804439544677734 | KNN Loss: 2.4833014011383057 | BCE Loss: 0.9971424341201782\n",
      "Epoch 103 / 500 | iteration 0 / 30 | Total Loss: 3.4936323165893555 | KNN Loss: 2.4854660034179688 | BCE Loss: 1.0081663131713867\n",
      "Epoch 103 / 500 | iteration 5 / 30 | Total Loss: 3.5163886547088623 | KNN Loss: 2.5100502967834473 | BCE Loss: 1.006338357925415\n",
      "Epoch 103 / 500 | iteration 10 / 30 | Total Loss: 3.484285831451416 | KNN Loss: 2.5016565322875977 | BCE Loss: 0.9826294183731079\n",
      "Epoch 103 / 500 | iteration 15 / 30 | Total Loss: 3.4979543685913086 | KNN Loss: 2.4919633865356445 | BCE Loss: 1.005990982055664\n",
      "Epoch 103 / 500 | iteration 20 / 30 | Total Loss: 3.550043821334839 | KNN Loss: 2.512742519378662 | BCE Loss: 1.0373013019561768\n",
      "Epoch 103 / 500 | iteration 25 / 30 | Total Loss: 3.5179872512817383 | KNN Loss: 2.502626657485962 | BCE Loss: 1.0153605937957764\n",
      "Epoch 104 / 500 | iteration 0 / 30 | Total Loss: 3.496346950531006 | KNN Loss: 2.4713475704193115 | BCE Loss: 1.0249994993209839\n",
      "Epoch 104 / 500 | iteration 5 / 30 | Total Loss: 3.5365567207336426 | KNN Loss: 2.514388084411621 | BCE Loss: 1.022168517112732\n",
      "Epoch 104 / 500 | iteration 10 / 30 | Total Loss: 3.5312929153442383 | KNN Loss: 2.4982595443725586 | BCE Loss: 1.0330332517623901\n",
      "Epoch 104 / 500 | iteration 15 / 30 | Total Loss: 3.5549843311309814 | KNN Loss: 2.535295248031616 | BCE Loss: 1.0196890830993652\n",
      "Epoch 104 / 500 | iteration 20 / 30 | Total Loss: 3.5383684635162354 | KNN Loss: 2.4853665828704834 | BCE Loss: 1.053001880645752\n",
      "Epoch 104 / 500 | iteration 25 / 30 | Total Loss: 3.5275697708129883 | KNN Loss: 2.508613348007202 | BCE Loss: 1.0189564228057861\n",
      "Epoch 105 / 500 | iteration 0 / 30 | Total Loss: 3.513540029525757 | KNN Loss: 2.4853999614715576 | BCE Loss: 1.0281400680541992\n",
      "Epoch 105 / 500 | iteration 5 / 30 | Total Loss: 3.5814576148986816 | KNN Loss: 2.5448720455169678 | BCE Loss: 1.0365855693817139\n",
      "Epoch 105 / 500 | iteration 10 / 30 | Total Loss: 3.5061628818511963 | KNN Loss: 2.4530537128448486 | BCE Loss: 1.0531091690063477\n",
      "Epoch 105 / 500 | iteration 15 / 30 | Total Loss: 3.549124240875244 | KNN Loss: 2.541328191757202 | BCE Loss: 1.0077961683273315\n",
      "Epoch 105 / 500 | iteration 20 / 30 | Total Loss: 3.5727272033691406 | KNN Loss: 2.5280637741088867 | BCE Loss: 1.0446633100509644\n",
      "Epoch 105 / 500 | iteration 25 / 30 | Total Loss: 3.519198417663574 | KNN Loss: 2.4863648414611816 | BCE Loss: 1.0328336954116821\n",
      "Epoch 106 / 500 | iteration 0 / 30 | Total Loss: 3.523632287979126 | KNN Loss: 2.4940247535705566 | BCE Loss: 1.0296075344085693\n",
      "Epoch 106 / 500 | iteration 5 / 30 | Total Loss: 3.549717903137207 | KNN Loss: 2.536956787109375 | BCE Loss: 1.0127609968185425\n",
      "Epoch 106 / 500 | iteration 10 / 30 | Total Loss: 3.4928195476531982 | KNN Loss: 2.4973599910736084 | BCE Loss: 0.9954596161842346\n",
      "Epoch 106 / 500 | iteration 15 / 30 | Total Loss: 3.5219693183898926 | KNN Loss: 2.4940125942230225 | BCE Loss: 1.0279567241668701\n",
      "Epoch 106 / 500 | iteration 20 / 30 | Total Loss: 3.5268349647521973 | KNN Loss: 2.516371011734009 | BCE Loss: 1.010464072227478\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 106 / 500 | iteration 25 / 30 | Total Loss: 3.5235190391540527 | KNN Loss: 2.501573085784912 | BCE Loss: 1.021945834159851\n",
      "Epoch 107 / 500 | iteration 0 / 30 | Total Loss: 3.477726936340332 | KNN Loss: 2.458770275115967 | BCE Loss: 1.0189567804336548\n",
      "Epoch 107 / 500 | iteration 5 / 30 | Total Loss: 3.5238239765167236 | KNN Loss: 2.492575168609619 | BCE Loss: 1.0312488079071045\n",
      "Epoch 107 / 500 | iteration 10 / 30 | Total Loss: 3.53653621673584 | KNN Loss: 2.516874074935913 | BCE Loss: 1.0196621417999268\n",
      "Epoch 107 / 500 | iteration 15 / 30 | Total Loss: 3.4995992183685303 | KNN Loss: 2.486217498779297 | BCE Loss: 1.0133817195892334\n",
      "Epoch 107 / 500 | iteration 20 / 30 | Total Loss: 3.4829905033111572 | KNN Loss: 2.4659807682037354 | BCE Loss: 1.0170097351074219\n",
      "Epoch 107 / 500 | iteration 25 / 30 | Total Loss: 3.4860730171203613 | KNN Loss: 2.4955649375915527 | BCE Loss: 0.9905081987380981\n",
      "Epoch 108 / 500 | iteration 0 / 30 | Total Loss: 3.5263938903808594 | KNN Loss: 2.514806032180786 | BCE Loss: 1.0115878582000732\n",
      "Epoch 108 / 500 | iteration 5 / 30 | Total Loss: 3.5012919902801514 | KNN Loss: 2.4825167655944824 | BCE Loss: 1.018775224685669\n",
      "Epoch 108 / 500 | iteration 10 / 30 | Total Loss: 3.5433461666107178 | KNN Loss: 2.518296480178833 | BCE Loss: 1.0250496864318848\n",
      "Epoch 108 / 500 | iteration 15 / 30 | Total Loss: 3.5848770141601562 | KNN Loss: 2.562784194946289 | BCE Loss: 1.0220927000045776\n",
      "Epoch 108 / 500 | iteration 20 / 30 | Total Loss: 3.5332539081573486 | KNN Loss: 2.5002400875091553 | BCE Loss: 1.0330138206481934\n",
      "Epoch 108 / 500 | iteration 25 / 30 | Total Loss: 3.471096992492676 | KNN Loss: 2.4875149726867676 | BCE Loss: 0.9835819005966187\n",
      "Epoch 109 / 500 | iteration 0 / 30 | Total Loss: 3.53843355178833 | KNN Loss: 2.5217108726501465 | BCE Loss: 1.016722559928894\n",
      "Epoch 109 / 500 | iteration 5 / 30 | Total Loss: 3.5664689540863037 | KNN Loss: 2.541445255279541 | BCE Loss: 1.0250236988067627\n",
      "Epoch 109 / 500 | iteration 10 / 30 | Total Loss: 3.537672281265259 | KNN Loss: 2.519542694091797 | BCE Loss: 1.018129587173462\n",
      "Epoch 109 / 500 | iteration 15 / 30 | Total Loss: 3.5371012687683105 | KNN Loss: 2.5137481689453125 | BCE Loss: 1.023353099822998\n",
      "Epoch 109 / 500 | iteration 20 / 30 | Total Loss: 3.512903928756714 | KNN Loss: 2.519132375717163 | BCE Loss: 0.9937716126441956\n",
      "Epoch 109 / 500 | iteration 25 / 30 | Total Loss: 3.4664652347564697 | KNN Loss: 2.4636926651000977 | BCE Loss: 1.002772569656372\n",
      "Epoch 110 / 500 | iteration 0 / 30 | Total Loss: 3.485407829284668 | KNN Loss: 2.484764814376831 | BCE Loss: 1.000643014907837\n",
      "Epoch 110 / 500 | iteration 5 / 30 | Total Loss: 3.546652317047119 | KNN Loss: 2.5080082416534424 | BCE Loss: 1.0386440753936768\n",
      "Epoch 110 / 500 | iteration 10 / 30 | Total Loss: 3.529804229736328 | KNN Loss: 2.4899189472198486 | BCE Loss: 1.039885401725769\n",
      "Epoch 110 / 500 | iteration 15 / 30 | Total Loss: 3.529160737991333 | KNN Loss: 2.4978110790252686 | BCE Loss: 1.0313496589660645\n",
      "Epoch 110 / 500 | iteration 20 / 30 | Total Loss: 3.5193865299224854 | KNN Loss: 2.5244271755218506 | BCE Loss: 0.9949593544006348\n",
      "Epoch 110 / 500 | iteration 25 / 30 | Total Loss: 3.5454587936401367 | KNN Loss: 2.517881393432617 | BCE Loss: 1.0275774002075195\n",
      "Epoch 111 / 500 | iteration 0 / 30 | Total Loss: 3.541123628616333 | KNN Loss: 2.49517822265625 | BCE Loss: 1.045945405960083\n",
      "Epoch 111 / 500 | iteration 5 / 30 | Total Loss: 3.548769950866699 | KNN Loss: 2.502856731414795 | BCE Loss: 1.0459132194519043\n",
      "Epoch 111 / 500 | iteration 10 / 30 | Total Loss: 3.553713083267212 | KNN Loss: 2.531898260116577 | BCE Loss: 1.0218148231506348\n",
      "Epoch 111 / 500 | iteration 15 / 30 | Total Loss: 3.540818691253662 | KNN Loss: 2.512443780899048 | BCE Loss: 1.0283750295639038\n",
      "Epoch 111 / 500 | iteration 20 / 30 | Total Loss: 3.544407844543457 | KNN Loss: 2.5079455375671387 | BCE Loss: 1.0364621877670288\n",
      "Epoch 111 / 500 | iteration 25 / 30 | Total Loss: 3.5149078369140625 | KNN Loss: 2.5051515102386475 | BCE Loss: 1.0097562074661255\n",
      "Epoch 112 / 500 | iteration 0 / 30 | Total Loss: 3.5216050148010254 | KNN Loss: 2.4935107231140137 | BCE Loss: 1.0280942916870117\n",
      "Epoch 112 / 500 | iteration 5 / 30 | Total Loss: 3.475024700164795 | KNN Loss: 2.4579389095306396 | BCE Loss: 1.0170856714248657\n",
      "Epoch 112 / 500 | iteration 10 / 30 | Total Loss: 3.494401454925537 | KNN Loss: 2.4676260948181152 | BCE Loss: 1.0267753601074219\n",
      "Epoch 112 / 500 | iteration 15 / 30 | Total Loss: 3.514817476272583 | KNN Loss: 2.48684024810791 | BCE Loss: 1.0279772281646729\n",
      "Epoch 112 / 500 | iteration 20 / 30 | Total Loss: 3.5271286964416504 | KNN Loss: 2.5058867931365967 | BCE Loss: 1.0212417840957642\n",
      "Epoch 112 / 500 | iteration 25 / 30 | Total Loss: 3.4937307834625244 | KNN Loss: 2.4888932704925537 | BCE Loss: 1.0048375129699707\n",
      "Epoch 113 / 500 | iteration 0 / 30 | Total Loss: 3.5371131896972656 | KNN Loss: 2.5147287845611572 | BCE Loss: 1.022384524345398\n",
      "Epoch 113 / 500 | iteration 5 / 30 | Total Loss: 3.490936040878296 | KNN Loss: 2.4885497093200684 | BCE Loss: 1.0023863315582275\n",
      "Epoch 113 / 500 | iteration 10 / 30 | Total Loss: 3.5141983032226562 | KNN Loss: 2.5075106620788574 | BCE Loss: 1.0066875219345093\n",
      "Epoch 113 / 500 | iteration 15 / 30 | Total Loss: 3.530486822128296 | KNN Loss: 2.5057952404022217 | BCE Loss: 1.0246915817260742\n",
      "Epoch 113 / 500 | iteration 20 / 30 | Total Loss: 3.5444211959838867 | KNN Loss: 2.5120530128479004 | BCE Loss: 1.0323681831359863\n",
      "Epoch 113 / 500 | iteration 25 / 30 | Total Loss: 3.5127298831939697 | KNN Loss: 2.4765305519104004 | BCE Loss: 1.0361993312835693\n",
      "Epoch 114 / 500 | iteration 0 / 30 | Total Loss: 3.5018301010131836 | KNN Loss: 2.49231219291687 | BCE Loss: 1.009518027305603\n",
      "Epoch 114 / 500 | iteration 5 / 30 | Total Loss: 3.5250892639160156 | KNN Loss: 2.5299999713897705 | BCE Loss: 0.9950892925262451\n",
      "Epoch 114 / 500 | iteration 10 / 30 | Total Loss: 3.456888198852539 | KNN Loss: 2.4832093715667725 | BCE Loss: 0.973678708076477\n",
      "Epoch 114 / 500 | iteration 15 / 30 | Total Loss: 3.5422871112823486 | KNN Loss: 2.535249710083008 | BCE Loss: 1.0070374011993408\n",
      "Epoch 114 / 500 | iteration 20 / 30 | Total Loss: 3.4976296424865723 | KNN Loss: 2.4832987785339355 | BCE Loss: 1.0143308639526367\n",
      "Epoch 114 / 500 | iteration 25 / 30 | Total Loss: 3.5198659896850586 | KNN Loss: 2.492082118988037 | BCE Loss: 1.0277838706970215\n",
      "Epoch 115 / 500 | iteration 0 / 30 | Total Loss: 3.567067861557007 | KNN Loss: 2.5090904235839844 | BCE Loss: 1.0579774379730225\n",
      "Epoch 115 / 500 | iteration 5 / 30 | Total Loss: 3.5390267372131348 | KNN Loss: 2.520275592803955 | BCE Loss: 1.0187511444091797\n",
      "Epoch 115 / 500 | iteration 10 / 30 | Total Loss: 3.4776246547698975 | KNN Loss: 2.4565179347991943 | BCE Loss: 1.0211067199707031\n",
      "Epoch 115 / 500 | iteration 15 / 30 | Total Loss: 3.518160343170166 | KNN Loss: 2.5030581951141357 | BCE Loss: 1.0151021480560303\n",
      "Epoch 115 / 500 | iteration 20 / 30 | Total Loss: 3.52042293548584 | KNN Loss: 2.4989113807678223 | BCE Loss: 1.021511435508728\n",
      "Epoch 115 / 500 | iteration 25 / 30 | Total Loss: 3.506039619445801 | KNN Loss: 2.517071008682251 | BCE Loss: 0.9889687299728394\n",
      "Epoch 116 / 500 | iteration 0 / 30 | Total Loss: 3.5039329528808594 | KNN Loss: 2.482719659805298 | BCE Loss: 1.0212132930755615\n",
      "Epoch 116 / 500 | iteration 5 / 30 | Total Loss: 3.583580493927002 | KNN Loss: 2.5281577110290527 | BCE Loss: 1.0554227828979492\n",
      "Epoch 116 / 500 | iteration 10 / 30 | Total Loss: 3.5204315185546875 | KNN Loss: 2.4940450191497803 | BCE Loss: 1.0263864994049072\n",
      "Epoch 116 / 500 | iteration 15 / 30 | Total Loss: 3.502645254135132 | KNN Loss: 2.481076955795288 | BCE Loss: 1.0215682983398438\n",
      "Epoch 116 / 500 | iteration 20 / 30 | Total Loss: 3.5001494884490967 | KNN Loss: 2.486359119415283 | BCE Loss: 1.0137903690338135\n",
      "Epoch 116 / 500 | iteration 25 / 30 | Total Loss: 3.5161070823669434 | KNN Loss: 2.4937944412231445 | BCE Loss: 1.0223125219345093\n",
      "Epoch   117: reducing learning rate of group 0 to 3.5000e-03.\n",
      "Epoch 117 / 500 | iteration 0 / 30 | Total Loss: 3.5006284713745117 | KNN Loss: 2.5129568576812744 | BCE Loss: 0.9876714944839478\n",
      "Epoch 117 / 500 | iteration 5 / 30 | Total Loss: 3.5500295162200928 | KNN Loss: 2.505920886993408 | BCE Loss: 1.0441086292266846\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 117 / 500 | iteration 10 / 30 | Total Loss: 3.516819477081299 | KNN Loss: 2.4686055183410645 | BCE Loss: 1.0482139587402344\n",
      "Epoch 117 / 500 | iteration 15 / 30 | Total Loss: 3.5354788303375244 | KNN Loss: 2.5247154235839844 | BCE Loss: 1.01076340675354\n",
      "Epoch 117 / 500 | iteration 20 / 30 | Total Loss: 3.5077829360961914 | KNN Loss: 2.481762647628784 | BCE Loss: 1.0260204076766968\n",
      "Epoch 117 / 500 | iteration 25 / 30 | Total Loss: 3.5098652839660645 | KNN Loss: 2.4714391231536865 | BCE Loss: 1.038426160812378\n",
      "Epoch 118 / 500 | iteration 0 / 30 | Total Loss: 3.472809314727783 | KNN Loss: 2.4850997924804688 | BCE Loss: 0.9877094030380249\n",
      "Epoch 118 / 500 | iteration 5 / 30 | Total Loss: 3.562181234359741 | KNN Loss: 2.5617263317108154 | BCE Loss: 1.0004549026489258\n",
      "Epoch 118 / 500 | iteration 10 / 30 | Total Loss: 3.5523910522460938 | KNN Loss: 2.532783269882202 | BCE Loss: 1.0196079015731812\n",
      "Epoch 118 / 500 | iteration 15 / 30 | Total Loss: 3.491797685623169 | KNN Loss: 2.494600534439087 | BCE Loss: 0.9971970915794373\n",
      "Epoch 118 / 500 | iteration 20 / 30 | Total Loss: 3.5524325370788574 | KNN Loss: 2.4919583797454834 | BCE Loss: 1.060474157333374\n",
      "Epoch 118 / 500 | iteration 25 / 30 | Total Loss: 3.4952573776245117 | KNN Loss: 2.4936795234680176 | BCE Loss: 1.0015777349472046\n",
      "Epoch 119 / 500 | iteration 0 / 30 | Total Loss: 3.537583351135254 | KNN Loss: 2.4705400466918945 | BCE Loss: 1.0670433044433594\n",
      "Epoch 119 / 500 | iteration 5 / 30 | Total Loss: 3.4770684242248535 | KNN Loss: 2.4684014320373535 | BCE Loss: 1.0086669921875\n",
      "Epoch 119 / 500 | iteration 10 / 30 | Total Loss: 3.5318408012390137 | KNN Loss: 2.49367618560791 | BCE Loss: 1.038164496421814\n",
      "Epoch 119 / 500 | iteration 15 / 30 | Total Loss: 3.5530481338500977 | KNN Loss: 2.5326178073883057 | BCE Loss: 1.020430326461792\n",
      "Epoch 119 / 500 | iteration 20 / 30 | Total Loss: 3.5407681465148926 | KNN Loss: 2.522444009780884 | BCE Loss: 1.0183240175247192\n",
      "Epoch 119 / 500 | iteration 25 / 30 | Total Loss: 3.4988999366760254 | KNN Loss: 2.5000340938568115 | BCE Loss: 0.9988659024238586\n",
      "Epoch 120 / 500 | iteration 0 / 30 | Total Loss: 3.4850308895111084 | KNN Loss: 2.480116128921509 | BCE Loss: 1.0049147605895996\n",
      "Epoch 120 / 500 | iteration 5 / 30 | Total Loss: 3.5242183208465576 | KNN Loss: 2.4752237796783447 | BCE Loss: 1.048994541168213\n",
      "Epoch 120 / 500 | iteration 10 / 30 | Total Loss: 3.5418167114257812 | KNN Loss: 2.493657112121582 | BCE Loss: 1.0481595993041992\n",
      "Epoch 120 / 500 | iteration 15 / 30 | Total Loss: 3.5150601863861084 | KNN Loss: 2.4991188049316406 | BCE Loss: 1.0159413814544678\n",
      "Epoch 120 / 500 | iteration 20 / 30 | Total Loss: 3.5223636627197266 | KNN Loss: 2.5211589336395264 | BCE Loss: 1.0012047290802002\n",
      "Epoch 120 / 500 | iteration 25 / 30 | Total Loss: 3.4915239810943604 | KNN Loss: 2.4991888999938965 | BCE Loss: 0.9923351407051086\n",
      "Epoch 121 / 500 | iteration 0 / 30 | Total Loss: 3.533463716506958 | KNN Loss: 2.4916985034942627 | BCE Loss: 1.0417652130126953\n",
      "Epoch 121 / 500 | iteration 5 / 30 | Total Loss: 3.5533320903778076 | KNN Loss: 2.5203592777252197 | BCE Loss: 1.032972812652588\n",
      "Epoch 121 / 500 | iteration 10 / 30 | Total Loss: 3.5397746562957764 | KNN Loss: 2.5223803520202637 | BCE Loss: 1.0173943042755127\n",
      "Epoch 121 / 500 | iteration 15 / 30 | Total Loss: 3.5219640731811523 | KNN Loss: 2.502068042755127 | BCE Loss: 1.0198960304260254\n",
      "Epoch 121 / 500 | iteration 20 / 30 | Total Loss: 3.5127758979797363 | KNN Loss: 2.516500234603882 | BCE Loss: 0.996275782585144\n",
      "Epoch 121 / 500 | iteration 25 / 30 | Total Loss: 3.5181169509887695 | KNN Loss: 2.522892475128174 | BCE Loss: 0.9952244758605957\n",
      "Epoch 122 / 500 | iteration 0 / 30 | Total Loss: 3.555309295654297 | KNN Loss: 2.505781412124634 | BCE Loss: 1.049527883529663\n",
      "Epoch 122 / 500 | iteration 5 / 30 | Total Loss: 3.505897283554077 | KNN Loss: 2.500046491622925 | BCE Loss: 1.0058507919311523\n",
      "Epoch 122 / 500 | iteration 10 / 30 | Total Loss: 3.5800909996032715 | KNN Loss: 2.561335563659668 | BCE Loss: 1.018755316734314\n",
      "Epoch 122 / 500 | iteration 15 / 30 | Total Loss: 3.5647940635681152 | KNN Loss: 2.549971103668213 | BCE Loss: 1.014823079109192\n",
      "Epoch 122 / 500 | iteration 20 / 30 | Total Loss: 3.5274152755737305 | KNN Loss: 2.507908582687378 | BCE Loss: 1.019506812095642\n",
      "Epoch 122 / 500 | iteration 25 / 30 | Total Loss: 3.4576754570007324 | KNN Loss: 2.475803852081299 | BCE Loss: 0.9818716645240784\n",
      "Epoch 123 / 500 | iteration 0 / 30 | Total Loss: 3.5329084396362305 | KNN Loss: 2.5006372928619385 | BCE Loss: 1.0322710275650024\n",
      "Epoch 123 / 500 | iteration 5 / 30 | Total Loss: 3.4906067848205566 | KNN Loss: 2.4811768531799316 | BCE Loss: 1.009429931640625\n",
      "Epoch 123 / 500 | iteration 10 / 30 | Total Loss: 3.515533447265625 | KNN Loss: 2.4759581089019775 | BCE Loss: 1.039575219154358\n",
      "Epoch 123 / 500 | iteration 15 / 30 | Total Loss: 3.5206494331359863 | KNN Loss: 2.507328748703003 | BCE Loss: 1.013320803642273\n",
      "Epoch 123 / 500 | iteration 20 / 30 | Total Loss: 3.5014946460723877 | KNN Loss: 2.5057859420776367 | BCE Loss: 0.9957086443901062\n",
      "Epoch 123 / 500 | iteration 25 / 30 | Total Loss: 3.4641261100769043 | KNN Loss: 2.473487615585327 | BCE Loss: 0.9906383752822876\n",
      "Epoch 124 / 500 | iteration 0 / 30 | Total Loss: 3.5137929916381836 | KNN Loss: 2.481938362121582 | BCE Loss: 1.0318547487258911\n",
      "Epoch 124 / 500 | iteration 5 / 30 | Total Loss: 3.529998540878296 | KNN Loss: 2.518660068511963 | BCE Loss: 1.011338472366333\n",
      "Epoch 124 / 500 | iteration 10 / 30 | Total Loss: 3.509601593017578 | KNN Loss: 2.478604793548584 | BCE Loss: 1.0309967994689941\n",
      "Epoch 124 / 500 | iteration 15 / 30 | Total Loss: 3.532747745513916 | KNN Loss: 2.5330569744110107 | BCE Loss: 0.9996906518936157\n",
      "Epoch 124 / 500 | iteration 20 / 30 | Total Loss: 3.526700973510742 | KNN Loss: 2.48930287361145 | BCE Loss: 1.0373982191085815\n",
      "Epoch 124 / 500 | iteration 25 / 30 | Total Loss: 3.510134220123291 | KNN Loss: 2.4919238090515137 | BCE Loss: 1.0182104110717773\n",
      "Epoch 125 / 500 | iteration 0 / 30 | Total Loss: 3.5205163955688477 | KNN Loss: 2.4763879776000977 | BCE Loss: 1.0441282987594604\n",
      "Epoch 125 / 500 | iteration 5 / 30 | Total Loss: 3.496385097503662 | KNN Loss: 2.486363649368286 | BCE Loss: 1.010021448135376\n",
      "Epoch 125 / 500 | iteration 10 / 30 | Total Loss: 3.4651756286621094 | KNN Loss: 2.45928692817688 | BCE Loss: 1.005888819694519\n",
      "Epoch 125 / 500 | iteration 15 / 30 | Total Loss: 3.512544631958008 | KNN Loss: 2.476231336593628 | BCE Loss: 1.0363132953643799\n",
      "Epoch 125 / 500 | iteration 20 / 30 | Total Loss: 3.487452268600464 | KNN Loss: 2.5031087398529053 | BCE Loss: 0.9843435883522034\n",
      "Epoch 125 / 500 | iteration 25 / 30 | Total Loss: 3.4894206523895264 | KNN Loss: 2.4796059131622314 | BCE Loss: 1.009814739227295\n",
      "Epoch 126 / 500 | iteration 0 / 30 | Total Loss: 3.5138447284698486 | KNN Loss: 2.5110666751861572 | BCE Loss: 1.0027780532836914\n",
      "Epoch 126 / 500 | iteration 5 / 30 | Total Loss: 3.562178134918213 | KNN Loss: 2.5487728118896484 | BCE Loss: 1.0134053230285645\n",
      "Epoch 126 / 500 | iteration 10 / 30 | Total Loss: 3.544316053390503 | KNN Loss: 2.533690929412842 | BCE Loss: 1.0106251239776611\n",
      "Epoch 126 / 500 | iteration 15 / 30 | Total Loss: 3.4841747283935547 | KNN Loss: 2.480912208557129 | BCE Loss: 1.0032626390457153\n",
      "Epoch 126 / 500 | iteration 20 / 30 | Total Loss: 3.542567253112793 | KNN Loss: 2.521777629852295 | BCE Loss: 1.020789623260498\n",
      "Epoch 126 / 500 | iteration 25 / 30 | Total Loss: 3.5453169345855713 | KNN Loss: 2.5196752548217773 | BCE Loss: 1.025641679763794\n",
      "Epoch 127 / 500 | iteration 0 / 30 | Total Loss: 3.5436267852783203 | KNN Loss: 2.5237557888031006 | BCE Loss: 1.0198711156845093\n",
      "Epoch 127 / 500 | iteration 5 / 30 | Total Loss: 3.5486674308776855 | KNN Loss: 2.5215039253234863 | BCE Loss: 1.0271636247634888\n",
      "Epoch 127 / 500 | iteration 10 / 30 | Total Loss: 3.5440454483032227 | KNN Loss: 2.5226869583129883 | BCE Loss: 1.0213584899902344\n",
      "Epoch 127 / 500 | iteration 15 / 30 | Total Loss: 3.5355424880981445 | KNN Loss: 2.4931814670562744 | BCE Loss: 1.0423610210418701\n",
      "Epoch 127 / 500 | iteration 20 / 30 | Total Loss: 3.502187728881836 | KNN Loss: 2.4904253482818604 | BCE Loss: 1.0117623805999756\n",
      "Epoch 127 / 500 | iteration 25 / 30 | Total Loss: 3.5559563636779785 | KNN Loss: 2.5120930671691895 | BCE Loss: 1.043863296508789\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 128 / 500 | iteration 0 / 30 | Total Loss: 3.4790713787078857 | KNN Loss: 2.482372999191284 | BCE Loss: 0.9966983199119568\n",
      "Epoch 128 / 500 | iteration 5 / 30 | Total Loss: 3.540515899658203 | KNN Loss: 2.486239194869995 | BCE Loss: 1.054276704788208\n",
      "Epoch 128 / 500 | iteration 10 / 30 | Total Loss: 3.512518882751465 | KNN Loss: 2.4698238372802734 | BCE Loss: 1.0426949262619019\n",
      "Epoch 128 / 500 | iteration 15 / 30 | Total Loss: 3.5281336307525635 | KNN Loss: 2.4905502796173096 | BCE Loss: 1.037583351135254\n",
      "Epoch 128 / 500 | iteration 20 / 30 | Total Loss: 3.5035970211029053 | KNN Loss: 2.519258975982666 | BCE Loss: 0.984338104724884\n",
      "Epoch 128 / 500 | iteration 25 / 30 | Total Loss: 3.501039505004883 | KNN Loss: 2.4848451614379883 | BCE Loss: 1.0161943435668945\n",
      "Epoch 129 / 500 | iteration 0 / 30 | Total Loss: 3.522975444793701 | KNN Loss: 2.5222346782684326 | BCE Loss: 1.000740885734558\n",
      "Epoch 129 / 500 | iteration 5 / 30 | Total Loss: 3.4650051593780518 | KNN Loss: 2.4814882278442383 | BCE Loss: 0.9835169315338135\n",
      "Epoch 129 / 500 | iteration 10 / 30 | Total Loss: 3.563145637512207 | KNN Loss: 2.522374153137207 | BCE Loss: 1.040771484375\n",
      "Epoch 129 / 500 | iteration 15 / 30 | Total Loss: 3.500215530395508 | KNN Loss: 2.4633541107177734 | BCE Loss: 1.0368614196777344\n",
      "Epoch 129 / 500 | iteration 20 / 30 | Total Loss: 3.515718460083008 | KNN Loss: 2.5005404949188232 | BCE Loss: 1.0151779651641846\n",
      "Epoch 129 / 500 | iteration 25 / 30 | Total Loss: 3.5318827629089355 | KNN Loss: 2.497495174407959 | BCE Loss: 1.0343877077102661\n",
      "Epoch 130 / 500 | iteration 0 / 30 | Total Loss: 3.567348003387451 | KNN Loss: 2.531771421432495 | BCE Loss: 1.035576581954956\n",
      "Epoch 130 / 500 | iteration 5 / 30 | Total Loss: 3.527189254760742 | KNN Loss: 2.5029938220977783 | BCE Loss: 1.0241954326629639\n",
      "Epoch 130 / 500 | iteration 10 / 30 | Total Loss: 3.6151585578918457 | KNN Loss: 2.571247100830078 | BCE Loss: 1.043911337852478\n",
      "Epoch 130 / 500 | iteration 15 / 30 | Total Loss: 3.4824631214141846 | KNN Loss: 2.478694200515747 | BCE Loss: 1.0037689208984375\n",
      "Epoch 130 / 500 | iteration 20 / 30 | Total Loss: 3.5043954849243164 | KNN Loss: 2.505741834640503 | BCE Loss: 0.9986535906791687\n",
      "Epoch 130 / 500 | iteration 25 / 30 | Total Loss: 3.514166831970215 | KNN Loss: 2.4898297786712646 | BCE Loss: 1.0243370532989502\n",
      "Epoch 131 / 500 | iteration 0 / 30 | Total Loss: 3.476649284362793 | KNN Loss: 2.4697906970977783 | BCE Loss: 1.0068585872650146\n",
      "Epoch 131 / 500 | iteration 5 / 30 | Total Loss: 3.551224946975708 | KNN Loss: 2.4945719242095947 | BCE Loss: 1.0566530227661133\n",
      "Epoch 131 / 500 | iteration 10 / 30 | Total Loss: 3.523860454559326 | KNN Loss: 2.5056326389312744 | BCE Loss: 1.0182278156280518\n",
      "Epoch 131 / 500 | iteration 15 / 30 | Total Loss: 3.5092759132385254 | KNN Loss: 2.487285614013672 | BCE Loss: 1.0219902992248535\n",
      "Epoch 131 / 500 | iteration 20 / 30 | Total Loss: 3.501032829284668 | KNN Loss: 2.4931674003601074 | BCE Loss: 1.0078654289245605\n",
      "Epoch 131 / 500 | iteration 25 / 30 | Total Loss: 3.5186333656311035 | KNN Loss: 2.5154552459716797 | BCE Loss: 1.0031782388687134\n",
      "Epoch 132 / 500 | iteration 0 / 30 | Total Loss: 3.525423526763916 | KNN Loss: 2.5083870887756348 | BCE Loss: 1.0170365571975708\n",
      "Epoch 132 / 500 | iteration 5 / 30 | Total Loss: 3.533745050430298 | KNN Loss: 2.529736280441284 | BCE Loss: 1.0040087699890137\n",
      "Epoch 132 / 500 | iteration 10 / 30 | Total Loss: 3.5168862342834473 | KNN Loss: 2.4972970485687256 | BCE Loss: 1.0195891857147217\n",
      "Epoch 132 / 500 | iteration 15 / 30 | Total Loss: 3.5282480716705322 | KNN Loss: 2.499509334564209 | BCE Loss: 1.0287387371063232\n",
      "Epoch 132 / 500 | iteration 20 / 30 | Total Loss: 3.510275363922119 | KNN Loss: 2.4787096977233887 | BCE Loss: 1.0315656661987305\n",
      "Epoch 132 / 500 | iteration 25 / 30 | Total Loss: 3.541942596435547 | KNN Loss: 2.5045583248138428 | BCE Loss: 1.0373841524124146\n",
      "Epoch 133 / 500 | iteration 0 / 30 | Total Loss: 3.467383861541748 | KNN Loss: 2.455803871154785 | BCE Loss: 1.011579990386963\n",
      "Epoch 133 / 500 | iteration 5 / 30 | Total Loss: 3.5158464908599854 | KNN Loss: 2.495875120162964 | BCE Loss: 1.0199713706970215\n",
      "Epoch 133 / 500 | iteration 10 / 30 | Total Loss: 3.540299892425537 | KNN Loss: 2.524949789047241 | BCE Loss: 1.015350103378296\n",
      "Epoch 133 / 500 | iteration 15 / 30 | Total Loss: 3.5356593132019043 | KNN Loss: 2.514759063720703 | BCE Loss: 1.0209003686904907\n",
      "Epoch 133 / 500 | iteration 20 / 30 | Total Loss: 3.5086591243743896 | KNN Loss: 2.500950813293457 | BCE Loss: 1.0077083110809326\n",
      "Epoch 133 / 500 | iteration 25 / 30 | Total Loss: 3.5402870178222656 | KNN Loss: 2.4859838485717773 | BCE Loss: 1.0543031692504883\n",
      "Epoch 134 / 500 | iteration 0 / 30 | Total Loss: 3.5033316612243652 | KNN Loss: 2.474036693572998 | BCE Loss: 1.0292949676513672\n",
      "Epoch 134 / 500 | iteration 5 / 30 | Total Loss: 3.526254892349243 | KNN Loss: 2.5048630237579346 | BCE Loss: 1.0213918685913086\n",
      "Epoch 134 / 500 | iteration 10 / 30 | Total Loss: 3.5421016216278076 | KNN Loss: 2.528869152069092 | BCE Loss: 1.0132324695587158\n",
      "Epoch 134 / 500 | iteration 15 / 30 | Total Loss: 3.4905524253845215 | KNN Loss: 2.4832777976989746 | BCE Loss: 1.0072747468948364\n",
      "Epoch 134 / 500 | iteration 20 / 30 | Total Loss: 3.5212483406066895 | KNN Loss: 2.4984090328216553 | BCE Loss: 1.0228393077850342\n",
      "Epoch 134 / 500 | iteration 25 / 30 | Total Loss: 3.538546085357666 | KNN Loss: 2.50763201713562 | BCE Loss: 1.0309139490127563\n",
      "Epoch 135 / 500 | iteration 0 / 30 | Total Loss: 3.554144859313965 | KNN Loss: 2.518303394317627 | BCE Loss: 1.035841464996338\n",
      "Epoch 135 / 500 | iteration 5 / 30 | Total Loss: 3.488416910171509 | KNN Loss: 2.471904993057251 | BCE Loss: 1.0165119171142578\n",
      "Epoch 135 / 500 | iteration 10 / 30 | Total Loss: 3.550776243209839 | KNN Loss: 2.4887313842773438 | BCE Loss: 1.0620448589324951\n",
      "Epoch 135 / 500 | iteration 15 / 30 | Total Loss: 3.520446300506592 | KNN Loss: 2.4722020626068115 | BCE Loss: 1.0482441186904907\n",
      "Epoch 135 / 500 | iteration 20 / 30 | Total Loss: 3.5551846027374268 | KNN Loss: 2.4898250102996826 | BCE Loss: 1.0653595924377441\n",
      "Epoch 135 / 500 | iteration 25 / 30 | Total Loss: 3.48118257522583 | KNN Loss: 2.4780380725860596 | BCE Loss: 1.0031445026397705\n",
      "Epoch 136 / 500 | iteration 0 / 30 | Total Loss: 3.5024335384368896 | KNN Loss: 2.499800443649292 | BCE Loss: 1.0026330947875977\n",
      "Epoch 136 / 500 | iteration 5 / 30 | Total Loss: 3.514211893081665 | KNN Loss: 2.502372980117798 | BCE Loss: 1.0118389129638672\n",
      "Epoch 136 / 500 | iteration 10 / 30 | Total Loss: 3.5315935611724854 | KNN Loss: 2.501164674758911 | BCE Loss: 1.0304288864135742\n",
      "Epoch 136 / 500 | iteration 15 / 30 | Total Loss: 3.5176873207092285 | KNN Loss: 2.480393648147583 | BCE Loss: 1.0372936725616455\n",
      "Epoch 136 / 500 | iteration 20 / 30 | Total Loss: 3.531709671020508 | KNN Loss: 2.480090856552124 | BCE Loss: 1.0516188144683838\n",
      "Epoch 136 / 500 | iteration 25 / 30 | Total Loss: 3.5406088829040527 | KNN Loss: 2.4897172451019287 | BCE Loss: 1.0508915185928345\n",
      "Epoch 137 / 500 | iteration 0 / 30 | Total Loss: 3.498021125793457 | KNN Loss: 2.4766926765441895 | BCE Loss: 1.0213284492492676\n",
      "Epoch 137 / 500 | iteration 5 / 30 | Total Loss: 3.5401134490966797 | KNN Loss: 2.4981775283813477 | BCE Loss: 1.041935920715332\n",
      "Epoch 137 / 500 | iteration 10 / 30 | Total Loss: 3.4855096340179443 | KNN Loss: 2.4790115356445312 | BCE Loss: 1.006498098373413\n",
      "Epoch 137 / 500 | iteration 15 / 30 | Total Loss: 3.5269289016723633 | KNN Loss: 2.490723133087158 | BCE Loss: 1.0362058877944946\n",
      "Epoch 137 / 500 | iteration 20 / 30 | Total Loss: 3.5053861141204834 | KNN Loss: 2.474944591522217 | BCE Loss: 1.0304415225982666\n",
      "Epoch 137 / 500 | iteration 25 / 30 | Total Loss: 3.485926628112793 | KNN Loss: 2.4890236854553223 | BCE Loss: 0.9969030618667603\n",
      "Epoch 138 / 500 | iteration 0 / 30 | Total Loss: 3.533717155456543 | KNN Loss: 2.499575614929199 | BCE Loss: 1.0341416597366333\n",
      "Epoch 138 / 500 | iteration 5 / 30 | Total Loss: 3.470024824142456 | KNN Loss: 2.4588124752044678 | BCE Loss: 1.0112123489379883\n",
      "Epoch 138 / 500 | iteration 10 / 30 | Total Loss: 3.5398733615875244 | KNN Loss: 2.5151970386505127 | BCE Loss: 1.0246763229370117\n",
      "Epoch 138 / 500 | iteration 15 / 30 | Total Loss: 3.550778865814209 | KNN Loss: 2.52266788482666 | BCE Loss: 1.0281108617782593\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 138 / 500 | iteration 20 / 30 | Total Loss: 3.5242679119110107 | KNN Loss: 2.5149524211883545 | BCE Loss: 1.0093154907226562\n",
      "Epoch 138 / 500 | iteration 25 / 30 | Total Loss: 3.5178122520446777 | KNN Loss: 2.5162384510040283 | BCE Loss: 1.0015736818313599\n",
      "Epoch 139 / 500 | iteration 0 / 30 | Total Loss: 3.47829270362854 | KNN Loss: 2.46437668800354 | BCE Loss: 1.013916015625\n",
      "Epoch 139 / 500 | iteration 5 / 30 | Total Loss: 3.548544406890869 | KNN Loss: 2.5250816345214844 | BCE Loss: 1.0234627723693848\n",
      "Epoch 139 / 500 | iteration 10 / 30 | Total Loss: 3.496713638305664 | KNN Loss: 2.5032238960266113 | BCE Loss: 0.9934898614883423\n",
      "Epoch 139 / 500 | iteration 15 / 30 | Total Loss: 3.49935245513916 | KNN Loss: 2.4931440353393555 | BCE Loss: 1.0062083005905151\n",
      "Epoch 139 / 500 | iteration 20 / 30 | Total Loss: 3.5211572647094727 | KNN Loss: 2.493079662322998 | BCE Loss: 1.028077483177185\n",
      "Epoch 139 / 500 | iteration 25 / 30 | Total Loss: 3.5016798973083496 | KNN Loss: 2.4818763732910156 | BCE Loss: 1.0198034048080444\n",
      "Epoch 140 / 500 | iteration 0 / 30 | Total Loss: 3.50528883934021 | KNN Loss: 2.486477851867676 | BCE Loss: 1.0188109874725342\n",
      "Epoch 140 / 500 | iteration 5 / 30 | Total Loss: 3.480421304702759 | KNN Loss: 2.4613184928894043 | BCE Loss: 1.0191028118133545\n",
      "Epoch 140 / 500 | iteration 10 / 30 | Total Loss: 3.531205177307129 | KNN Loss: 2.502519369125366 | BCE Loss: 1.0286856889724731\n",
      "Epoch 140 / 500 | iteration 15 / 30 | Total Loss: 3.5143609046936035 | KNN Loss: 2.4889447689056396 | BCE Loss: 1.0254160165786743\n",
      "Epoch 140 / 500 | iteration 20 / 30 | Total Loss: 3.5204217433929443 | KNN Loss: 2.501415967941284 | BCE Loss: 1.0190057754516602\n",
      "Epoch 140 / 500 | iteration 25 / 30 | Total Loss: 3.464580535888672 | KNN Loss: 2.4636664390563965 | BCE Loss: 1.0009140968322754\n",
      "Epoch 141 / 500 | iteration 0 / 30 | Total Loss: 3.545210361480713 | KNN Loss: 2.5184261798858643 | BCE Loss: 1.026784062385559\n",
      "Epoch 141 / 500 | iteration 5 / 30 | Total Loss: 3.537026882171631 | KNN Loss: 2.510955333709717 | BCE Loss: 1.026071548461914\n",
      "Epoch 141 / 500 | iteration 10 / 30 | Total Loss: 3.487445831298828 | KNN Loss: 2.479429244995117 | BCE Loss: 1.008016586303711\n",
      "Epoch 141 / 500 | iteration 15 / 30 | Total Loss: 3.502702236175537 | KNN Loss: 2.473621368408203 | BCE Loss: 1.029080867767334\n",
      "Epoch 141 / 500 | iteration 20 / 30 | Total Loss: 3.4653167724609375 | KNN Loss: 2.4780304431915283 | BCE Loss: 0.9872862696647644\n",
      "Epoch 141 / 500 | iteration 25 / 30 | Total Loss: 3.4839515686035156 | KNN Loss: 2.497234344482422 | BCE Loss: 0.9867171049118042\n",
      "Epoch 142 / 500 | iteration 0 / 30 | Total Loss: 3.5246334075927734 | KNN Loss: 2.496217966079712 | BCE Loss: 1.0284154415130615\n",
      "Epoch 142 / 500 | iteration 5 / 30 | Total Loss: 3.5031700134277344 | KNN Loss: 2.4855549335479736 | BCE Loss: 1.0176150798797607\n",
      "Epoch 142 / 500 | iteration 10 / 30 | Total Loss: 3.520063877105713 | KNN Loss: 2.504567861557007 | BCE Loss: 1.015496015548706\n",
      "Epoch 142 / 500 | iteration 15 / 30 | Total Loss: 3.5078840255737305 | KNN Loss: 2.46763277053833 | BCE Loss: 1.04025137424469\n",
      "Epoch 142 / 500 | iteration 20 / 30 | Total Loss: 3.5206093788146973 | KNN Loss: 2.508497953414917 | BCE Loss: 1.0121115446090698\n",
      "Epoch 142 / 500 | iteration 25 / 30 | Total Loss: 3.5229620933532715 | KNN Loss: 2.5045437812805176 | BCE Loss: 1.018418312072754\n",
      "Epoch 143 / 500 | iteration 0 / 30 | Total Loss: 3.486022472381592 | KNN Loss: 2.498910665512085 | BCE Loss: 0.9871116876602173\n",
      "Epoch 143 / 500 | iteration 5 / 30 | Total Loss: 3.5001041889190674 | KNN Loss: 2.49358868598938 | BCE Loss: 1.0065155029296875\n",
      "Epoch 143 / 500 | iteration 10 / 30 | Total Loss: 3.5175933837890625 | KNN Loss: 2.4924302101135254 | BCE Loss: 1.0251630544662476\n",
      "Epoch 143 / 500 | iteration 15 / 30 | Total Loss: 3.486668825149536 | KNN Loss: 2.4883525371551514 | BCE Loss: 0.9983162879943848\n",
      "Epoch 143 / 500 | iteration 20 / 30 | Total Loss: 3.523322582244873 | KNN Loss: 2.5049290657043457 | BCE Loss: 1.0183935165405273\n",
      "Epoch 143 / 500 | iteration 25 / 30 | Total Loss: 3.54591703414917 | KNN Loss: 2.5231502056121826 | BCE Loss: 1.0227668285369873\n",
      "Epoch   144: reducing learning rate of group 0 to 2.4500e-03.\n",
      "Epoch 144 / 500 | iteration 0 / 30 | Total Loss: 3.4846301078796387 | KNN Loss: 2.4608614444732666 | BCE Loss: 1.0237685441970825\n",
      "Epoch 144 / 500 | iteration 5 / 30 | Total Loss: 3.517383575439453 | KNN Loss: 2.4962143898010254 | BCE Loss: 1.0211691856384277\n",
      "Epoch 144 / 500 | iteration 10 / 30 | Total Loss: 3.5870466232299805 | KNN Loss: 2.5520434379577637 | BCE Loss: 1.0350033044815063\n",
      "Epoch 144 / 500 | iteration 15 / 30 | Total Loss: 3.5337114334106445 | KNN Loss: 2.5177576541900635 | BCE Loss: 1.015953779220581\n",
      "Epoch 144 / 500 | iteration 20 / 30 | Total Loss: 3.487349510192871 | KNN Loss: 2.4599084854125977 | BCE Loss: 1.0274410247802734\n",
      "Epoch 144 / 500 | iteration 25 / 30 | Total Loss: 3.5520877838134766 | KNN Loss: 2.4931161403656006 | BCE Loss: 1.058971643447876\n",
      "Epoch 145 / 500 | iteration 0 / 30 | Total Loss: 3.5086753368377686 | KNN Loss: 2.4967522621154785 | BCE Loss: 1.01192307472229\n",
      "Epoch 145 / 500 | iteration 5 / 30 | Total Loss: 3.4619903564453125 | KNN Loss: 2.46097731590271 | BCE Loss: 1.0010130405426025\n",
      "Epoch 145 / 500 | iteration 10 / 30 | Total Loss: 3.5298359394073486 | KNN Loss: 2.485591411590576 | BCE Loss: 1.0442445278167725\n",
      "Epoch 145 / 500 | iteration 15 / 30 | Total Loss: 3.4951062202453613 | KNN Loss: 2.4862070083618164 | BCE Loss: 1.008899211883545\n",
      "Epoch 145 / 500 | iteration 20 / 30 | Total Loss: 3.531146287918091 | KNN Loss: 2.478616714477539 | BCE Loss: 1.0525295734405518\n",
      "Epoch 145 / 500 | iteration 25 / 30 | Total Loss: 3.5236339569091797 | KNN Loss: 2.494464159011841 | BCE Loss: 1.0291697978973389\n",
      "Epoch 146 / 500 | iteration 0 / 30 | Total Loss: 3.493997573852539 | KNN Loss: 2.494403123855591 | BCE Loss: 0.9995943903923035\n",
      "Epoch 146 / 500 | iteration 5 / 30 | Total Loss: 3.5182831287384033 | KNN Loss: 2.4837186336517334 | BCE Loss: 1.03456449508667\n",
      "Epoch 146 / 500 | iteration 10 / 30 | Total Loss: 3.5019278526306152 | KNN Loss: 2.477720022201538 | BCE Loss: 1.0242078304290771\n",
      "Epoch 146 / 500 | iteration 15 / 30 | Total Loss: 3.526581048965454 | KNN Loss: 2.533874034881592 | BCE Loss: 0.9927069544792175\n",
      "Epoch 146 / 500 | iteration 20 / 30 | Total Loss: 3.506847381591797 | KNN Loss: 2.4765207767486572 | BCE Loss: 1.03032648563385\n",
      "Epoch 146 / 500 | iteration 25 / 30 | Total Loss: 3.519026756286621 | KNN Loss: 2.503239870071411 | BCE Loss: 1.01578688621521\n",
      "Epoch 147 / 500 | iteration 0 / 30 | Total Loss: 3.4991331100463867 | KNN Loss: 2.496954917907715 | BCE Loss: 1.0021780729293823\n",
      "Epoch 147 / 500 | iteration 5 / 30 | Total Loss: 3.493990421295166 | KNN Loss: 2.4751901626586914 | BCE Loss: 1.0188002586364746\n",
      "Epoch 147 / 500 | iteration 10 / 30 | Total Loss: 3.5068202018737793 | KNN Loss: 2.485960006713867 | BCE Loss: 1.0208603143692017\n",
      "Epoch 147 / 500 | iteration 15 / 30 | Total Loss: 3.522160530090332 | KNN Loss: 2.498824119567871 | BCE Loss: 1.023336410522461\n",
      "Epoch 147 / 500 | iteration 20 / 30 | Total Loss: 3.5188698768615723 | KNN Loss: 2.4981796741485596 | BCE Loss: 1.0206902027130127\n",
      "Epoch 147 / 500 | iteration 25 / 30 | Total Loss: 3.520472764968872 | KNN Loss: 2.514477491378784 | BCE Loss: 1.005995273590088\n",
      "Epoch 148 / 500 | iteration 0 / 30 | Total Loss: 3.4676480293273926 | KNN Loss: 2.4769411087036133 | BCE Loss: 0.9907068610191345\n",
      "Epoch 148 / 500 | iteration 5 / 30 | Total Loss: 3.5040059089660645 | KNN Loss: 2.4847240447998047 | BCE Loss: 1.0192819833755493\n",
      "Epoch 148 / 500 | iteration 10 / 30 | Total Loss: 3.480363368988037 | KNN Loss: 2.4703404903411865 | BCE Loss: 1.010022759437561\n",
      "Epoch 148 / 500 | iteration 15 / 30 | Total Loss: 3.5246031284332275 | KNN Loss: 2.4831225872039795 | BCE Loss: 1.041480541229248\n",
      "Epoch 148 / 500 | iteration 20 / 30 | Total Loss: 3.5180299282073975 | KNN Loss: 2.4976000785827637 | BCE Loss: 1.0204298496246338\n",
      "Epoch 148 / 500 | iteration 25 / 30 | Total Loss: 3.5277676582336426 | KNN Loss: 2.4810094833374023 | BCE Loss: 1.0467581748962402\n",
      "Epoch 149 / 500 | iteration 0 / 30 | Total Loss: 3.5577943325042725 | KNN Loss: 2.507448196411133 | BCE Loss: 1.0503461360931396\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 149 / 500 | iteration 5 / 30 | Total Loss: 3.5108957290649414 | KNN Loss: 2.5202534198760986 | BCE Loss: 0.9906424283981323\n",
      "Epoch 149 / 500 | iteration 10 / 30 | Total Loss: 3.495527744293213 | KNN Loss: 2.477023124694824 | BCE Loss: 1.0185045003890991\n",
      "Epoch 149 / 500 | iteration 15 / 30 | Total Loss: 3.5112931728363037 | KNN Loss: 2.50504732131958 | BCE Loss: 1.0062458515167236\n",
      "Epoch 149 / 500 | iteration 20 / 30 | Total Loss: 3.5018668174743652 | KNN Loss: 2.460127592086792 | BCE Loss: 1.0417392253875732\n",
      "Epoch 149 / 500 | iteration 25 / 30 | Total Loss: 3.5237412452697754 | KNN Loss: 2.500802755355835 | BCE Loss: 1.0229384899139404\n",
      "Epoch 150 / 500 | iteration 0 / 30 | Total Loss: 3.5362730026245117 | KNN Loss: 2.5196688175201416 | BCE Loss: 1.0166040658950806\n",
      "Epoch 150 / 500 | iteration 5 / 30 | Total Loss: 3.5245282649993896 | KNN Loss: 2.4881811141967773 | BCE Loss: 1.0363471508026123\n",
      "Epoch 150 / 500 | iteration 10 / 30 | Total Loss: 3.5430285930633545 | KNN Loss: 2.5110576152801514 | BCE Loss: 1.0319709777832031\n",
      "Epoch 150 / 500 | iteration 15 / 30 | Total Loss: 3.441131114959717 | KNN Loss: 2.4442484378814697 | BCE Loss: 0.9968827962875366\n",
      "Epoch 150 / 500 | iteration 20 / 30 | Total Loss: 3.539997100830078 | KNN Loss: 2.5100295543670654 | BCE Loss: 1.0299676656723022\n",
      "Epoch 150 / 500 | iteration 25 / 30 | Total Loss: 3.5440425872802734 | KNN Loss: 2.5272934436798096 | BCE Loss: 1.0167491436004639\n",
      "Epoch 151 / 500 | iteration 0 / 30 | Total Loss: 3.451552152633667 | KNN Loss: 2.447239637374878 | BCE Loss: 1.004312515258789\n",
      "Epoch 151 / 500 | iteration 5 / 30 | Total Loss: 3.5123891830444336 | KNN Loss: 2.4739153385162354 | BCE Loss: 1.0384738445281982\n",
      "Epoch 151 / 500 | iteration 10 / 30 | Total Loss: 3.5072312355041504 | KNN Loss: 2.4798948764801025 | BCE Loss: 1.0273364782333374\n",
      "Epoch 151 / 500 | iteration 15 / 30 | Total Loss: 3.548405647277832 | KNN Loss: 2.514387845993042 | BCE Loss: 1.03401780128479\n",
      "Epoch 151 / 500 | iteration 20 / 30 | Total Loss: 3.4808106422424316 | KNN Loss: 2.4768166542053223 | BCE Loss: 1.0039939880371094\n",
      "Epoch 151 / 500 | iteration 25 / 30 | Total Loss: 3.4994473457336426 | KNN Loss: 2.4647364616394043 | BCE Loss: 1.0347107648849487\n",
      "Epoch 152 / 500 | iteration 0 / 30 | Total Loss: 3.5041615962982178 | KNN Loss: 2.484753370285034 | BCE Loss: 1.0194082260131836\n",
      "Epoch 152 / 500 | iteration 5 / 30 | Total Loss: 3.5593435764312744 | KNN Loss: 2.506547451019287 | BCE Loss: 1.0527961254119873\n",
      "Epoch 152 / 500 | iteration 10 / 30 | Total Loss: 3.4704980850219727 | KNN Loss: 2.4725148677825928 | BCE Loss: 0.9979832768440247\n",
      "Epoch 152 / 500 | iteration 15 / 30 | Total Loss: 3.500178575515747 | KNN Loss: 2.477487087249756 | BCE Loss: 1.0226914882659912\n",
      "Epoch 152 / 500 | iteration 20 / 30 | Total Loss: 3.4962923526763916 | KNN Loss: 2.511202096939087 | BCE Loss: 0.9850901961326599\n",
      "Epoch 152 / 500 | iteration 25 / 30 | Total Loss: 3.509645938873291 | KNN Loss: 2.4736926555633545 | BCE Loss: 1.0359532833099365\n",
      "Epoch 153 / 500 | iteration 0 / 30 | Total Loss: 3.4791009426116943 | KNN Loss: 2.4760348796844482 | BCE Loss: 1.003066062927246\n",
      "Epoch 153 / 500 | iteration 5 / 30 | Total Loss: 3.4829792976379395 | KNN Loss: 2.485673189163208 | BCE Loss: 0.9973061084747314\n",
      "Epoch 153 / 500 | iteration 10 / 30 | Total Loss: 3.5169990062713623 | KNN Loss: 2.498033046722412 | BCE Loss: 1.0189659595489502\n",
      "Epoch 153 / 500 | iteration 15 / 30 | Total Loss: 3.4899423122406006 | KNN Loss: 2.4785594940185547 | BCE Loss: 1.011382818222046\n",
      "Epoch 153 / 500 | iteration 20 / 30 | Total Loss: 3.5196547508239746 | KNN Loss: 2.4776864051818848 | BCE Loss: 1.0419683456420898\n",
      "Epoch 153 / 500 | iteration 25 / 30 | Total Loss: 3.4520390033721924 | KNN Loss: 2.47554874420166 | BCE Loss: 0.9764902591705322\n",
      "Epoch 154 / 500 | iteration 0 / 30 | Total Loss: 3.5087432861328125 | KNN Loss: 2.4644649028778076 | BCE Loss: 1.0442783832550049\n",
      "Epoch 154 / 500 | iteration 5 / 30 | Total Loss: 3.5550875663757324 | KNN Loss: 2.5197038650512695 | BCE Loss: 1.035383701324463\n",
      "Epoch 154 / 500 | iteration 10 / 30 | Total Loss: 3.5333926677703857 | KNN Loss: 2.482879161834717 | BCE Loss: 1.050513505935669\n",
      "Epoch 154 / 500 | iteration 15 / 30 | Total Loss: 3.505335807800293 | KNN Loss: 2.5044403076171875 | BCE Loss: 1.000895619392395\n",
      "Epoch 154 / 500 | iteration 20 / 30 | Total Loss: 3.509753465652466 | KNN Loss: 2.483966588973999 | BCE Loss: 1.0257868766784668\n",
      "Epoch 154 / 500 | iteration 25 / 30 | Total Loss: 3.5240392684936523 | KNN Loss: 2.4719841480255127 | BCE Loss: 1.0520551204681396\n",
      "Epoch   155: reducing learning rate of group 0 to 1.7150e-03.\n",
      "Epoch 155 / 500 | iteration 0 / 30 | Total Loss: 3.519634246826172 | KNN Loss: 2.479130268096924 | BCE Loss: 1.040503978729248\n",
      "Epoch 155 / 500 | iteration 5 / 30 | Total Loss: 3.5319182872772217 | KNN Loss: 2.5040276050567627 | BCE Loss: 1.027890682220459\n",
      "Epoch 155 / 500 | iteration 10 / 30 | Total Loss: 3.4739599227905273 | KNN Loss: 2.475067377090454 | BCE Loss: 0.9988926649093628\n",
      "Epoch 155 / 500 | iteration 15 / 30 | Total Loss: 3.496835231781006 | KNN Loss: 2.4901442527770996 | BCE Loss: 1.0066910982131958\n",
      "Epoch 155 / 500 | iteration 20 / 30 | Total Loss: 3.481419801712036 | KNN Loss: 2.474013566970825 | BCE Loss: 1.007406234741211\n",
      "Epoch 155 / 500 | iteration 25 / 30 | Total Loss: 3.504622220993042 | KNN Loss: 2.4987356662750244 | BCE Loss: 1.0058865547180176\n",
      "Epoch 156 / 500 | iteration 0 / 30 | Total Loss: 3.486480474472046 | KNN Loss: 2.483708143234253 | BCE Loss: 1.002772331237793\n",
      "Epoch 156 / 500 | iteration 5 / 30 | Total Loss: 3.514961004257202 | KNN Loss: 2.508979558944702 | BCE Loss: 1.0059814453125\n",
      "Epoch 156 / 500 | iteration 10 / 30 | Total Loss: 3.540395736694336 | KNN Loss: 2.5012435913085938 | BCE Loss: 1.0391521453857422\n",
      "Epoch 156 / 500 | iteration 15 / 30 | Total Loss: 3.4645042419433594 | KNN Loss: 2.4645116329193115 | BCE Loss: 0.9999927282333374\n",
      "Epoch 156 / 500 | iteration 20 / 30 | Total Loss: 3.488359212875366 | KNN Loss: 2.496262788772583 | BCE Loss: 0.9920964241027832\n",
      "Epoch 156 / 500 | iteration 25 / 30 | Total Loss: 3.4803237915039062 | KNN Loss: 2.4529128074645996 | BCE Loss: 1.0274109840393066\n",
      "Epoch 157 / 500 | iteration 0 / 30 | Total Loss: 3.4878573417663574 | KNN Loss: 2.4795620441436768 | BCE Loss: 1.0082954168319702\n",
      "Epoch 157 / 500 | iteration 5 / 30 | Total Loss: 3.502413272857666 | KNN Loss: 2.4619927406311035 | BCE Loss: 1.040420651435852\n",
      "Epoch 157 / 500 | iteration 10 / 30 | Total Loss: 3.4945805072784424 | KNN Loss: 2.4562621116638184 | BCE Loss: 1.038318395614624\n",
      "Epoch 157 / 500 | iteration 15 / 30 | Total Loss: 3.4761950969696045 | KNN Loss: 2.4685254096984863 | BCE Loss: 1.0076696872711182\n",
      "Epoch 157 / 500 | iteration 20 / 30 | Total Loss: 3.509819507598877 | KNN Loss: 2.4802491664886475 | BCE Loss: 1.02957022190094\n",
      "Epoch 157 / 500 | iteration 25 / 30 | Total Loss: 3.479482650756836 | KNN Loss: 2.4665822982788086 | BCE Loss: 1.012900471687317\n",
      "Epoch 158 / 500 | iteration 0 / 30 | Total Loss: 3.522092342376709 | KNN Loss: 2.471010208129883 | BCE Loss: 1.0510820150375366\n",
      "Epoch 158 / 500 | iteration 5 / 30 | Total Loss: 3.4993910789489746 | KNN Loss: 2.4883058071136475 | BCE Loss: 1.0110853910446167\n",
      "Epoch 158 / 500 | iteration 10 / 30 | Total Loss: 3.5075340270996094 | KNN Loss: 2.5036964416503906 | BCE Loss: 1.0038375854492188\n",
      "Epoch 158 / 500 | iteration 15 / 30 | Total Loss: 3.5019071102142334 | KNN Loss: 2.4863011837005615 | BCE Loss: 1.0156059265136719\n",
      "Epoch 158 / 500 | iteration 20 / 30 | Total Loss: 3.513305425643921 | KNN Loss: 2.477473258972168 | BCE Loss: 1.035832166671753\n",
      "Epoch 158 / 500 | iteration 25 / 30 | Total Loss: 3.5074119567871094 | KNN Loss: 2.50998854637146 | BCE Loss: 0.9974234104156494\n",
      "Epoch 159 / 500 | iteration 0 / 30 | Total Loss: 3.5129528045654297 | KNN Loss: 2.47489333152771 | BCE Loss: 1.0380594730377197\n",
      "Epoch 159 / 500 | iteration 5 / 30 | Total Loss: 3.507636070251465 | KNN Loss: 2.504220485687256 | BCE Loss: 1.0034154653549194\n",
      "Epoch 159 / 500 | iteration 10 / 30 | Total Loss: 3.5060300827026367 | KNN Loss: 2.4842851161956787 | BCE Loss: 1.021744966506958\n",
      "Epoch 159 / 500 | iteration 15 / 30 | Total Loss: 3.469338893890381 | KNN Loss: 2.4567503929138184 | BCE Loss: 1.012588381767273\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 159 / 500 | iteration 20 / 30 | Total Loss: 3.4902005195617676 | KNN Loss: 2.4705018997192383 | BCE Loss: 1.0196987390518188\n",
      "Epoch 159 / 500 | iteration 25 / 30 | Total Loss: 3.485769271850586 | KNN Loss: 2.4574851989746094 | BCE Loss: 1.0282840728759766\n",
      "Epoch 160 / 500 | iteration 0 / 30 | Total Loss: 3.524022102355957 | KNN Loss: 2.493981122970581 | BCE Loss: 1.030040979385376\n",
      "Epoch 160 / 500 | iteration 5 / 30 | Total Loss: 3.4356391429901123 | KNN Loss: 2.457348108291626 | BCE Loss: 0.9782910346984863\n",
      "Epoch 160 / 500 | iteration 10 / 30 | Total Loss: 3.5191798210144043 | KNN Loss: 2.4984936714172363 | BCE Loss: 1.020686149597168\n",
      "Epoch 160 / 500 | iteration 15 / 30 | Total Loss: 3.49527645111084 | KNN Loss: 2.482365369796753 | BCE Loss: 1.0129109621047974\n",
      "Epoch 160 / 500 | iteration 20 / 30 | Total Loss: 3.486483573913574 | KNN Loss: 2.488933563232422 | BCE Loss: 0.9975500702857971\n",
      "Epoch 160 / 500 | iteration 25 / 30 | Total Loss: 3.5069117546081543 | KNN Loss: 2.4892706871032715 | BCE Loss: 1.0176410675048828\n",
      "Epoch 161 / 500 | iteration 0 / 30 | Total Loss: 3.5085818767547607 | KNN Loss: 2.4826548099517822 | BCE Loss: 1.0259270668029785\n",
      "Epoch 161 / 500 | iteration 5 / 30 | Total Loss: 3.5150506496429443 | KNN Loss: 2.492138385772705 | BCE Loss: 1.0229122638702393\n",
      "Epoch 161 / 500 | iteration 10 / 30 | Total Loss: 3.485348701477051 | KNN Loss: 2.4711339473724365 | BCE Loss: 1.0142146348953247\n",
      "Epoch 161 / 500 | iteration 15 / 30 | Total Loss: 3.482916831970215 | KNN Loss: 2.4630961418151855 | BCE Loss: 1.0198206901550293\n",
      "Epoch 161 / 500 | iteration 20 / 30 | Total Loss: 3.514986753463745 | KNN Loss: 2.4973061084747314 | BCE Loss: 1.0176806449890137\n",
      "Epoch 161 / 500 | iteration 25 / 30 | Total Loss: 3.5259640216827393 | KNN Loss: 2.4822728633880615 | BCE Loss: 1.0436911582946777\n",
      "Epoch 162 / 500 | iteration 0 / 30 | Total Loss: 3.5096704959869385 | KNN Loss: 2.4836604595184326 | BCE Loss: 1.0260100364685059\n",
      "Epoch 162 / 500 | iteration 5 / 30 | Total Loss: 3.5261499881744385 | KNN Loss: 2.4916930198669434 | BCE Loss: 1.0344569683074951\n",
      "Epoch 162 / 500 | iteration 10 / 30 | Total Loss: 3.4420547485351562 | KNN Loss: 2.467219829559326 | BCE Loss: 0.9748348593711853\n",
      "Epoch 162 / 500 | iteration 15 / 30 | Total Loss: 3.488788366317749 | KNN Loss: 2.4900543689727783 | BCE Loss: 0.9987339377403259\n",
      "Epoch 162 / 500 | iteration 20 / 30 | Total Loss: 3.487265110015869 | KNN Loss: 2.454500436782837 | BCE Loss: 1.0327646732330322\n",
      "Epoch 162 / 500 | iteration 25 / 30 | Total Loss: 3.4823830127716064 | KNN Loss: 2.459587812423706 | BCE Loss: 1.0227952003479004\n",
      "Epoch 163 / 500 | iteration 0 / 30 | Total Loss: 3.5497748851776123 | KNN Loss: 2.489684581756592 | BCE Loss: 1.0600903034210205\n",
      "Epoch 163 / 500 | iteration 5 / 30 | Total Loss: 3.568737030029297 | KNN Loss: 2.5258758068084717 | BCE Loss: 1.0428611040115356\n",
      "Epoch 163 / 500 | iteration 10 / 30 | Total Loss: 3.4705944061279297 | KNN Loss: 2.459841251373291 | BCE Loss: 1.0107531547546387\n",
      "Epoch 163 / 500 | iteration 15 / 30 | Total Loss: 3.54411244392395 | KNN Loss: 2.5013561248779297 | BCE Loss: 1.0427563190460205\n",
      "Epoch 163 / 500 | iteration 20 / 30 | Total Loss: 3.496349334716797 | KNN Loss: 2.4621896743774414 | BCE Loss: 1.0341596603393555\n",
      "Epoch 163 / 500 | iteration 25 / 30 | Total Loss: 3.4757766723632812 | KNN Loss: 2.4509599208831787 | BCE Loss: 1.024816632270813\n",
      "Epoch 164 / 500 | iteration 0 / 30 | Total Loss: 3.4755847454071045 | KNN Loss: 2.4810397624969482 | BCE Loss: 0.9945449829101562\n",
      "Epoch 164 / 500 | iteration 5 / 30 | Total Loss: 3.458404302597046 | KNN Loss: 2.4433414936065674 | BCE Loss: 1.0150628089904785\n",
      "Epoch 164 / 500 | iteration 10 / 30 | Total Loss: 3.5050718784332275 | KNN Loss: 2.465841770172119 | BCE Loss: 1.0392301082611084\n",
      "Epoch 164 / 500 | iteration 15 / 30 | Total Loss: 3.476731538772583 | KNN Loss: 2.478940963745117 | BCE Loss: 0.9977905750274658\n",
      "Epoch 164 / 500 | iteration 20 / 30 | Total Loss: 3.511272668838501 | KNN Loss: 2.4727413654327393 | BCE Loss: 1.0385313034057617\n",
      "Epoch 164 / 500 | iteration 25 / 30 | Total Loss: 3.539876937866211 | KNN Loss: 2.468977689743042 | BCE Loss: 1.0708991289138794\n",
      "Epoch 165 / 500 | iteration 0 / 30 | Total Loss: 3.486064910888672 | KNN Loss: 2.4668264389038086 | BCE Loss: 1.0192384719848633\n",
      "Epoch 165 / 500 | iteration 5 / 30 | Total Loss: 3.510850429534912 | KNN Loss: 2.4841983318328857 | BCE Loss: 1.026652216911316\n",
      "Epoch 165 / 500 | iteration 10 / 30 | Total Loss: 3.504059076309204 | KNN Loss: 2.48335599899292 | BCE Loss: 1.0207030773162842\n",
      "Epoch 165 / 500 | iteration 15 / 30 | Total Loss: 3.483470916748047 | KNN Loss: 2.4861576557159424 | BCE Loss: 0.9973131418228149\n",
      "Epoch 165 / 500 | iteration 20 / 30 | Total Loss: 3.5463757514953613 | KNN Loss: 2.5061514377593994 | BCE Loss: 1.040224313735962\n",
      "Epoch 165 / 500 | iteration 25 / 30 | Total Loss: 3.4873602390289307 | KNN Loss: 2.4685397148132324 | BCE Loss: 1.0188205242156982\n",
      "Epoch 166 / 500 | iteration 0 / 30 | Total Loss: 3.494760036468506 | KNN Loss: 2.4699509143829346 | BCE Loss: 1.0248090028762817\n",
      "Epoch 166 / 500 | iteration 5 / 30 | Total Loss: 3.512418031692505 | KNN Loss: 2.4886012077331543 | BCE Loss: 1.0238168239593506\n",
      "Epoch 166 / 500 | iteration 10 / 30 | Total Loss: 3.4571762084960938 | KNN Loss: 2.4648208618164062 | BCE Loss: 0.992355227470398\n",
      "Epoch 166 / 500 | iteration 15 / 30 | Total Loss: 3.502077102661133 | KNN Loss: 2.4728074073791504 | BCE Loss: 1.0292696952819824\n",
      "Epoch 166 / 500 | iteration 20 / 30 | Total Loss: 3.475004196166992 | KNN Loss: 2.449422836303711 | BCE Loss: 1.0255812406539917\n",
      "Epoch 166 / 500 | iteration 25 / 30 | Total Loss: 3.5199198722839355 | KNN Loss: 2.5085976123809814 | BCE Loss: 1.0113223791122437\n",
      "Epoch 167 / 500 | iteration 0 / 30 | Total Loss: 3.475423812866211 | KNN Loss: 2.472491502761841 | BCE Loss: 1.0029321908950806\n",
      "Epoch 167 / 500 | iteration 5 / 30 | Total Loss: 3.521907091140747 | KNN Loss: 2.49318528175354 | BCE Loss: 1.028721809387207\n",
      "Epoch 167 / 500 | iteration 10 / 30 | Total Loss: 3.477160930633545 | KNN Loss: 2.4856514930725098 | BCE Loss: 0.9915093779563904\n",
      "Epoch 167 / 500 | iteration 15 / 30 | Total Loss: 3.5569334030151367 | KNN Loss: 2.4981324672698975 | BCE Loss: 1.0588010549545288\n",
      "Epoch 167 / 500 | iteration 20 / 30 | Total Loss: 3.4714314937591553 | KNN Loss: 2.4549450874328613 | BCE Loss: 1.016486406326294\n",
      "Epoch 167 / 500 | iteration 25 / 30 | Total Loss: 3.475940704345703 | KNN Loss: 2.4664077758789062 | BCE Loss: 1.0095329284667969\n",
      "Epoch 168 / 500 | iteration 0 / 30 | Total Loss: 3.4922738075256348 | KNN Loss: 2.469217300415039 | BCE Loss: 1.0230565071105957\n",
      "Epoch 168 / 500 | iteration 5 / 30 | Total Loss: 3.5032339096069336 | KNN Loss: 2.4790046215057373 | BCE Loss: 1.0242292881011963\n",
      "Epoch 168 / 500 | iteration 10 / 30 | Total Loss: 3.4556965827941895 | KNN Loss: 2.4549641609191895 | BCE Loss: 1.0007325410842896\n",
      "Epoch 168 / 500 | iteration 15 / 30 | Total Loss: 3.541008234024048 | KNN Loss: 2.4984278678894043 | BCE Loss: 1.0425803661346436\n",
      "Epoch 168 / 500 | iteration 20 / 30 | Total Loss: 3.500685453414917 | KNN Loss: 2.458101987838745 | BCE Loss: 1.0425834655761719\n",
      "Epoch 168 / 500 | iteration 25 / 30 | Total Loss: 3.478520631790161 | KNN Loss: 2.461984395980835 | BCE Loss: 1.0165362358093262\n",
      "Epoch 169 / 500 | iteration 0 / 30 | Total Loss: 3.4866394996643066 | KNN Loss: 2.462531328201294 | BCE Loss: 1.0241081714630127\n",
      "Epoch 169 / 500 | iteration 5 / 30 | Total Loss: 3.49881649017334 | KNN Loss: 2.483269214630127 | BCE Loss: 1.0155471563339233\n",
      "Epoch 169 / 500 | iteration 10 / 30 | Total Loss: 3.457927703857422 | KNN Loss: 2.460190534591675 | BCE Loss: 0.9977370500564575\n",
      "Epoch 169 / 500 | iteration 15 / 30 | Total Loss: 3.5269079208374023 | KNN Loss: 2.4941442012786865 | BCE Loss: 1.0327636003494263\n",
      "Epoch 169 / 500 | iteration 20 / 30 | Total Loss: 3.520977020263672 | KNN Loss: 2.498579978942871 | BCE Loss: 1.0223971605300903\n",
      "Epoch 169 / 500 | iteration 25 / 30 | Total Loss: 3.494898796081543 | KNN Loss: 2.47113037109375 | BCE Loss: 1.023768424987793\n",
      "Epoch 170 / 500 | iteration 0 / 30 | Total Loss: 3.5071985721588135 | KNN Loss: 2.472867488861084 | BCE Loss: 1.0343310832977295\n",
      "Epoch 170 / 500 | iteration 5 / 30 | Total Loss: 3.4915618896484375 | KNN Loss: 2.4753940105438232 | BCE Loss: 1.0161679983139038\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 170 / 500 | iteration 10 / 30 | Total Loss: 3.489757537841797 | KNN Loss: 2.4722371101379395 | BCE Loss: 1.017520546913147\n",
      "Epoch 170 / 500 | iteration 15 / 30 | Total Loss: 3.495889663696289 | KNN Loss: 2.4910104274749756 | BCE Loss: 1.0048792362213135\n",
      "Epoch 170 / 500 | iteration 20 / 30 | Total Loss: 3.5067591667175293 | KNN Loss: 2.4838387966156006 | BCE Loss: 1.0229203701019287\n",
      "Epoch 170 / 500 | iteration 25 / 30 | Total Loss: 3.490398406982422 | KNN Loss: 2.4731409549713135 | BCE Loss: 1.0172574520111084\n",
      "Epoch 171 / 500 | iteration 0 / 30 | Total Loss: 3.472928047180176 | KNN Loss: 2.476259231567383 | BCE Loss: 0.9966689348220825\n",
      "Epoch 171 / 500 | iteration 5 / 30 | Total Loss: 3.509270191192627 | KNN Loss: 2.478125810623169 | BCE Loss: 1.031144380569458\n",
      "Epoch 171 / 500 | iteration 10 / 30 | Total Loss: 3.511808156967163 | KNN Loss: 2.486237049102783 | BCE Loss: 1.0255711078643799\n",
      "Epoch 171 / 500 | iteration 15 / 30 | Total Loss: 3.481624126434326 | KNN Loss: 2.4920318126678467 | BCE Loss: 0.9895923733711243\n",
      "Epoch 171 / 500 | iteration 20 / 30 | Total Loss: 3.5236191749572754 | KNN Loss: 2.505241632461548 | BCE Loss: 1.0183775424957275\n",
      "Epoch 171 / 500 | iteration 25 / 30 | Total Loss: 3.480119466781616 | KNN Loss: 2.4705843925476074 | BCE Loss: 1.0095350742340088\n",
      "Epoch 172 / 500 | iteration 0 / 30 | Total Loss: 3.5007195472717285 | KNN Loss: 2.4719812870025635 | BCE Loss: 1.028738260269165\n",
      "Epoch 172 / 500 | iteration 5 / 30 | Total Loss: 3.4564530849456787 | KNN Loss: 2.458568811416626 | BCE Loss: 0.9978842735290527\n",
      "Epoch 172 / 500 | iteration 10 / 30 | Total Loss: 3.496110439300537 | KNN Loss: 2.4826104640960693 | BCE Loss: 1.0134999752044678\n",
      "Epoch 172 / 500 | iteration 15 / 30 | Total Loss: 3.4825453758239746 | KNN Loss: 2.4765677452087402 | BCE Loss: 1.0059775114059448\n",
      "Epoch 172 / 500 | iteration 20 / 30 | Total Loss: 3.482722043991089 | KNN Loss: 2.473982572555542 | BCE Loss: 1.0087394714355469\n",
      "Epoch 172 / 500 | iteration 25 / 30 | Total Loss: 3.4838409423828125 | KNN Loss: 2.459416389465332 | BCE Loss: 1.0244245529174805\n",
      "Epoch 173 / 500 | iteration 0 / 30 | Total Loss: 3.472564697265625 | KNN Loss: 2.460073709487915 | BCE Loss: 1.01249098777771\n",
      "Epoch 173 / 500 | iteration 5 / 30 | Total Loss: 3.4848849773406982 | KNN Loss: 2.4669456481933594 | BCE Loss: 1.0179393291473389\n",
      "Epoch 173 / 500 | iteration 10 / 30 | Total Loss: 3.4608349800109863 | KNN Loss: 2.455350399017334 | BCE Loss: 1.0054845809936523\n",
      "Epoch 173 / 500 | iteration 15 / 30 | Total Loss: 3.4865915775299072 | KNN Loss: 2.483480215072632 | BCE Loss: 1.0031113624572754\n",
      "Epoch 173 / 500 | iteration 20 / 30 | Total Loss: 3.5215251445770264 | KNN Loss: 2.4823734760284424 | BCE Loss: 1.039151668548584\n",
      "Epoch 173 / 500 | iteration 25 / 30 | Total Loss: 3.492978572845459 | KNN Loss: 2.4635403156280518 | BCE Loss: 1.0294383764266968\n",
      "Epoch 174 / 500 | iteration 0 / 30 | Total Loss: 3.4779419898986816 | KNN Loss: 2.471724033355713 | BCE Loss: 1.0062179565429688\n",
      "Epoch 174 / 500 | iteration 5 / 30 | Total Loss: 3.5053861141204834 | KNN Loss: 2.476308584213257 | BCE Loss: 1.0290775299072266\n",
      "Epoch 174 / 500 | iteration 10 / 30 | Total Loss: 3.473365545272827 | KNN Loss: 2.476001024246216 | BCE Loss: 0.9973645210266113\n",
      "Epoch 174 / 500 | iteration 15 / 30 | Total Loss: 3.5289647579193115 | KNN Loss: 2.500756025314331 | BCE Loss: 1.0282087326049805\n",
      "Epoch 174 / 500 | iteration 20 / 30 | Total Loss: 3.4927353858947754 | KNN Loss: 2.4759182929992676 | BCE Loss: 1.0168170928955078\n",
      "Epoch 174 / 500 | iteration 25 / 30 | Total Loss: 3.5340585708618164 | KNN Loss: 2.49153470993042 | BCE Loss: 1.042523741722107\n",
      "Epoch 175 / 500 | iteration 0 / 30 | Total Loss: 3.50215744972229 | KNN Loss: 2.4587161540985107 | BCE Loss: 1.0434412956237793\n",
      "Epoch 175 / 500 | iteration 5 / 30 | Total Loss: 3.52266263961792 | KNN Loss: 2.482365846633911 | BCE Loss: 1.0402967929840088\n",
      "Epoch 175 / 500 | iteration 10 / 30 | Total Loss: 3.547654628753662 | KNN Loss: 2.5120599269866943 | BCE Loss: 1.0355947017669678\n",
      "Epoch 175 / 500 | iteration 15 / 30 | Total Loss: 3.490603446960449 | KNN Loss: 2.4655442237854004 | BCE Loss: 1.0250593423843384\n",
      "Epoch 175 / 500 | iteration 20 / 30 | Total Loss: 3.501835346221924 | KNN Loss: 2.4771156311035156 | BCE Loss: 1.0247197151184082\n",
      "Epoch 175 / 500 | iteration 25 / 30 | Total Loss: 3.46889591217041 | KNN Loss: 2.476715087890625 | BCE Loss: 0.9921807050704956\n",
      "Epoch 176 / 500 | iteration 0 / 30 | Total Loss: 3.5027878284454346 | KNN Loss: 2.4844810962677 | BCE Loss: 1.0183067321777344\n",
      "Epoch 176 / 500 | iteration 5 / 30 | Total Loss: 3.4827098846435547 | KNN Loss: 2.4712021350860596 | BCE Loss: 1.0115078687667847\n",
      "Epoch 176 / 500 | iteration 10 / 30 | Total Loss: 3.525531768798828 | KNN Loss: 2.4997968673706055 | BCE Loss: 1.025734782218933\n",
      "Epoch 176 / 500 | iteration 15 / 30 | Total Loss: 3.5201783180236816 | KNN Loss: 2.5234506130218506 | BCE Loss: 0.9967278242111206\n",
      "Epoch 176 / 500 | iteration 20 / 30 | Total Loss: 3.532700538635254 | KNN Loss: 2.5151913166046143 | BCE Loss: 1.0175092220306396\n",
      "Epoch 176 / 500 | iteration 25 / 30 | Total Loss: 3.5046825408935547 | KNN Loss: 2.4859161376953125 | BCE Loss: 1.0187664031982422\n",
      "Epoch 177 / 500 | iteration 0 / 30 | Total Loss: 3.491407871246338 | KNN Loss: 2.476547956466675 | BCE Loss: 1.014859914779663\n",
      "Epoch 177 / 500 | iteration 5 / 30 | Total Loss: 3.487393856048584 | KNN Loss: 2.462359666824341 | BCE Loss: 1.0250341892242432\n",
      "Epoch 177 / 500 | iteration 10 / 30 | Total Loss: 3.502378463745117 | KNN Loss: 2.462681531906128 | BCE Loss: 1.0396968126296997\n",
      "Epoch 177 / 500 | iteration 15 / 30 | Total Loss: 3.5140738487243652 | KNN Loss: 2.4848508834838867 | BCE Loss: 1.029222846031189\n",
      "Epoch 177 / 500 | iteration 20 / 30 | Total Loss: 3.4725334644317627 | KNN Loss: 2.4529170989990234 | BCE Loss: 1.0196163654327393\n",
      "Epoch 177 / 500 | iteration 25 / 30 | Total Loss: 3.4979782104492188 | KNN Loss: 2.4726998805999756 | BCE Loss: 1.0252782106399536\n",
      "Epoch 178 / 500 | iteration 0 / 30 | Total Loss: 3.472149610519409 | KNN Loss: 2.46981143951416 | BCE Loss: 1.002338171005249\n",
      "Epoch 178 / 500 | iteration 5 / 30 | Total Loss: 3.5067813396453857 | KNN Loss: 2.509644031524658 | BCE Loss: 0.9971373677253723\n",
      "Epoch 178 / 500 | iteration 10 / 30 | Total Loss: 3.4776768684387207 | KNN Loss: 2.476252317428589 | BCE Loss: 1.0014245510101318\n",
      "Epoch 178 / 500 | iteration 15 / 30 | Total Loss: 3.4836621284484863 | KNN Loss: 2.4776272773742676 | BCE Loss: 1.0060347318649292\n",
      "Epoch 178 / 500 | iteration 20 / 30 | Total Loss: 3.49899959564209 | KNN Loss: 2.4761240482330322 | BCE Loss: 1.0228756666183472\n",
      "Epoch 178 / 500 | iteration 25 / 30 | Total Loss: 3.4911394119262695 | KNN Loss: 2.4631826877593994 | BCE Loss: 1.0279568433761597\n",
      "Epoch 179 / 500 | iteration 0 / 30 | Total Loss: 3.5082876682281494 | KNN Loss: 2.4661636352539062 | BCE Loss: 1.0421240329742432\n",
      "Epoch 179 / 500 | iteration 5 / 30 | Total Loss: 3.4926536083221436 | KNN Loss: 2.4756486415863037 | BCE Loss: 1.0170049667358398\n",
      "Epoch 179 / 500 | iteration 10 / 30 | Total Loss: 3.5141568183898926 | KNN Loss: 2.5102736949920654 | BCE Loss: 1.0038832426071167\n",
      "Epoch 179 / 500 | iteration 15 / 30 | Total Loss: 3.45265793800354 | KNN Loss: 2.447718381881714 | BCE Loss: 1.0049395561218262\n",
      "Epoch 179 / 500 | iteration 20 / 30 | Total Loss: 3.4940381050109863 | KNN Loss: 2.5189857482910156 | BCE Loss: 0.9750523567199707\n",
      "Epoch 179 / 500 | iteration 25 / 30 | Total Loss: 3.4986250400543213 | KNN Loss: 2.464395046234131 | BCE Loss: 1.0342299938201904\n",
      "Epoch 180 / 500 | iteration 0 / 30 | Total Loss: 3.506929397583008 | KNN Loss: 2.4848575592041016 | BCE Loss: 1.0220718383789062\n",
      "Epoch 180 / 500 | iteration 5 / 30 | Total Loss: 3.4632790088653564 | KNN Loss: 2.4681954383850098 | BCE Loss: 0.9950835108757019\n",
      "Epoch 180 / 500 | iteration 10 / 30 | Total Loss: 3.4712882041931152 | KNN Loss: 2.4783709049224854 | BCE Loss: 0.9929173588752747\n",
      "Epoch 180 / 500 | iteration 15 / 30 | Total Loss: 3.5300819873809814 | KNN Loss: 2.4821975231170654 | BCE Loss: 1.047884464263916\n",
      "Epoch 180 / 500 | iteration 20 / 30 | Total Loss: 3.5134003162384033 | KNN Loss: 2.456523895263672 | BCE Loss: 1.0568764209747314\n",
      "Epoch 180 / 500 | iteration 25 / 30 | Total Loss: 3.4988479614257812 | KNN Loss: 2.4622905254364014 | BCE Loss: 1.0365574359893799\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 181 / 500 | iteration 0 / 30 | Total Loss: 3.509568929672241 | KNN Loss: 2.5079567432403564 | BCE Loss: 1.0016121864318848\n",
      "Epoch 181 / 500 | iteration 5 / 30 | Total Loss: 3.528179168701172 | KNN Loss: 2.5269272327423096 | BCE Loss: 1.0012519359588623\n",
      "Epoch 181 / 500 | iteration 10 / 30 | Total Loss: 3.516800880432129 | KNN Loss: 2.4994313716888428 | BCE Loss: 1.0173693895339966\n",
      "Epoch 181 / 500 | iteration 15 / 30 | Total Loss: 3.5282649993896484 | KNN Loss: 2.521934986114502 | BCE Loss: 1.0063300132751465\n",
      "Epoch 181 / 500 | iteration 20 / 30 | Total Loss: 3.506962299346924 | KNN Loss: 2.4782400131225586 | BCE Loss: 1.0287224054336548\n",
      "Epoch 181 / 500 | iteration 25 / 30 | Total Loss: 3.523735761642456 | KNN Loss: 2.478365898132324 | BCE Loss: 1.0453698635101318\n",
      "Epoch 182 / 500 | iteration 0 / 30 | Total Loss: 3.521090269088745 | KNN Loss: 2.487921953201294 | BCE Loss: 1.0331683158874512\n",
      "Epoch 182 / 500 | iteration 5 / 30 | Total Loss: 3.5310826301574707 | KNN Loss: 2.5038719177246094 | BCE Loss: 1.0272107124328613\n",
      "Epoch 182 / 500 | iteration 10 / 30 | Total Loss: 3.489849805831909 | KNN Loss: 2.4965593814849854 | BCE Loss: 0.993290364742279\n",
      "Epoch 182 / 500 | iteration 15 / 30 | Total Loss: 3.5077786445617676 | KNN Loss: 2.4840526580810547 | BCE Loss: 1.0237258672714233\n",
      "Epoch 182 / 500 | iteration 20 / 30 | Total Loss: 3.4964284896850586 | KNN Loss: 2.4686007499694824 | BCE Loss: 1.0278277397155762\n",
      "Epoch 182 / 500 | iteration 25 / 30 | Total Loss: 3.4996206760406494 | KNN Loss: 2.4783284664154053 | BCE Loss: 1.0212922096252441\n",
      "Epoch 183 / 500 | iteration 0 / 30 | Total Loss: 3.5303328037261963 | KNN Loss: 2.4850146770477295 | BCE Loss: 1.0453181266784668\n",
      "Epoch 183 / 500 | iteration 5 / 30 | Total Loss: 3.4941353797912598 | KNN Loss: 2.4814388751983643 | BCE Loss: 1.012696623802185\n",
      "Epoch 183 / 500 | iteration 10 / 30 | Total Loss: 3.471107244491577 | KNN Loss: 2.4500577449798584 | BCE Loss: 1.0210494995117188\n",
      "Epoch 183 / 500 | iteration 15 / 30 | Total Loss: 3.486802577972412 | KNN Loss: 2.4693210124969482 | BCE Loss: 1.0174815654754639\n",
      "Epoch 183 / 500 | iteration 20 / 30 | Total Loss: 3.4923248291015625 | KNN Loss: 2.481586456298828 | BCE Loss: 1.0107383728027344\n",
      "Epoch 183 / 500 | iteration 25 / 30 | Total Loss: 3.5305962562561035 | KNN Loss: 2.4970176219940186 | BCE Loss: 1.0335787534713745\n",
      "Epoch 184 / 500 | iteration 0 / 30 | Total Loss: 3.489330768585205 | KNN Loss: 2.4572620391845703 | BCE Loss: 1.0320686101913452\n",
      "Epoch 184 / 500 | iteration 5 / 30 | Total Loss: 3.49634051322937 | KNN Loss: 2.4571104049682617 | BCE Loss: 1.0392301082611084\n",
      "Epoch 184 / 500 | iteration 10 / 30 | Total Loss: 3.490475654602051 | KNN Loss: 2.4796135425567627 | BCE Loss: 1.0108622312545776\n",
      "Epoch 184 / 500 | iteration 15 / 30 | Total Loss: 3.5068321228027344 | KNN Loss: 2.4846367835998535 | BCE Loss: 1.0221953392028809\n",
      "Epoch 184 / 500 | iteration 20 / 30 | Total Loss: 3.482915163040161 | KNN Loss: 2.463575601577759 | BCE Loss: 1.0193395614624023\n",
      "Epoch 184 / 500 | iteration 25 / 30 | Total Loss: 3.5490529537200928 | KNN Loss: 2.5377097129821777 | BCE Loss: 1.011343240737915\n",
      "Epoch   185: reducing learning rate of group 0 to 1.2005e-03.\n",
      "Epoch 185 / 500 | iteration 0 / 30 | Total Loss: 3.475865602493286 | KNN Loss: 2.47554087638855 | BCE Loss: 1.0003247261047363\n",
      "Epoch 185 / 500 | iteration 5 / 30 | Total Loss: 3.439669609069824 | KNN Loss: 2.476888656616211 | BCE Loss: 0.9627808332443237\n",
      "Epoch 185 / 500 | iteration 10 / 30 | Total Loss: 3.487679958343506 | KNN Loss: 2.4726271629333496 | BCE Loss: 1.0150527954101562\n",
      "Epoch 185 / 500 | iteration 15 / 30 | Total Loss: 3.4839582443237305 | KNN Loss: 2.4691951274871826 | BCE Loss: 1.0147631168365479\n",
      "Epoch 185 / 500 | iteration 20 / 30 | Total Loss: 3.5088491439819336 | KNN Loss: 2.5081441402435303 | BCE Loss: 1.0007051229476929\n",
      "Epoch 185 / 500 | iteration 25 / 30 | Total Loss: 3.497654914855957 | KNN Loss: 2.498173236846924 | BCE Loss: 0.9994817972183228\n",
      "Epoch 186 / 500 | iteration 0 / 30 | Total Loss: 3.4698662757873535 | KNN Loss: 2.464503049850464 | BCE Loss: 1.0053632259368896\n",
      "Epoch 186 / 500 | iteration 5 / 30 | Total Loss: 3.4860856533050537 | KNN Loss: 2.444643020629883 | BCE Loss: 1.041442632675171\n",
      "Epoch 186 / 500 | iteration 10 / 30 | Total Loss: 3.5002310276031494 | KNN Loss: 2.4814891815185547 | BCE Loss: 1.0187418460845947\n",
      "Epoch 186 / 500 | iteration 15 / 30 | Total Loss: 3.5165412425994873 | KNN Loss: 2.5053493976593018 | BCE Loss: 1.0111918449401855\n",
      "Epoch 186 / 500 | iteration 20 / 30 | Total Loss: 3.482769012451172 | KNN Loss: 2.459099769592285 | BCE Loss: 1.0236691236495972\n",
      "Epoch 186 / 500 | iteration 25 / 30 | Total Loss: 3.4992542266845703 | KNN Loss: 2.4662094116210938 | BCE Loss: 1.0330449342727661\n",
      "Epoch 187 / 500 | iteration 0 / 30 | Total Loss: 3.517357587814331 | KNN Loss: 2.4724483489990234 | BCE Loss: 1.0449092388153076\n",
      "Epoch 187 / 500 | iteration 5 / 30 | Total Loss: 3.4916799068450928 | KNN Loss: 2.4843006134033203 | BCE Loss: 1.0073792934417725\n",
      "Epoch 187 / 500 | iteration 10 / 30 | Total Loss: 3.5028581619262695 | KNN Loss: 2.4923202991485596 | BCE Loss: 1.01053786277771\n",
      "Epoch 187 / 500 | iteration 15 / 30 | Total Loss: 3.4978842735290527 | KNN Loss: 2.457197666168213 | BCE Loss: 1.0406866073608398\n",
      "Epoch 187 / 500 | iteration 20 / 30 | Total Loss: 3.4642157554626465 | KNN Loss: 2.4612033367156982 | BCE Loss: 1.0030122995376587\n",
      "Epoch 187 / 500 | iteration 25 / 30 | Total Loss: 3.486299753189087 | KNN Loss: 2.4978818893432617 | BCE Loss: 0.9884178638458252\n",
      "Epoch 188 / 500 | iteration 0 / 30 | Total Loss: 3.4710912704467773 | KNN Loss: 2.4836935997009277 | BCE Loss: 0.9873977899551392\n",
      "Epoch 188 / 500 | iteration 5 / 30 | Total Loss: 3.483163356781006 | KNN Loss: 2.4821643829345703 | BCE Loss: 1.000998854637146\n",
      "Epoch 188 / 500 | iteration 10 / 30 | Total Loss: 3.507444143295288 | KNN Loss: 2.499333620071411 | BCE Loss: 1.008110523223877\n",
      "Epoch 188 / 500 | iteration 15 / 30 | Total Loss: 3.4735355377197266 | KNN Loss: 2.4587855339050293 | BCE Loss: 1.0147501230239868\n",
      "Epoch 188 / 500 | iteration 20 / 30 | Total Loss: 3.4909555912017822 | KNN Loss: 2.4902596473693848 | BCE Loss: 1.0006959438323975\n",
      "Epoch 188 / 500 | iteration 25 / 30 | Total Loss: 3.5034937858581543 | KNN Loss: 2.4938714504241943 | BCE Loss: 1.00962233543396\n",
      "Epoch 189 / 500 | iteration 0 / 30 | Total Loss: 3.4968676567077637 | KNN Loss: 2.482567548751831 | BCE Loss: 1.0143001079559326\n",
      "Epoch 189 / 500 | iteration 5 / 30 | Total Loss: 3.5175693035125732 | KNN Loss: 2.4614017009735107 | BCE Loss: 1.0561676025390625\n",
      "Epoch 189 / 500 | iteration 10 / 30 | Total Loss: 3.517806053161621 | KNN Loss: 2.489346504211426 | BCE Loss: 1.0284595489501953\n",
      "Epoch 189 / 500 | iteration 15 / 30 | Total Loss: 3.442085027694702 | KNN Loss: 2.4486706256866455 | BCE Loss: 0.9934143424034119\n",
      "Epoch 189 / 500 | iteration 20 / 30 | Total Loss: 3.515383243560791 | KNN Loss: 2.4599199295043945 | BCE Loss: 1.055463194847107\n",
      "Epoch 189 / 500 | iteration 25 / 30 | Total Loss: 3.4851505756378174 | KNN Loss: 2.451746940612793 | BCE Loss: 1.0334036350250244\n",
      "Epoch 190 / 500 | iteration 0 / 30 | Total Loss: 3.510612726211548 | KNN Loss: 2.502349376678467 | BCE Loss: 1.008263349533081\n",
      "Epoch 190 / 500 | iteration 5 / 30 | Total Loss: 3.482672691345215 | KNN Loss: 2.484408378601074 | BCE Loss: 0.9982644319534302\n",
      "Epoch 190 / 500 | iteration 10 / 30 | Total Loss: 3.519700288772583 | KNN Loss: 2.4993669986724854 | BCE Loss: 1.0203332901000977\n",
      "Epoch 190 / 500 | iteration 15 / 30 | Total Loss: 3.5446901321411133 | KNN Loss: 2.50593638420105 | BCE Loss: 1.0387537479400635\n",
      "Epoch 190 / 500 | iteration 20 / 30 | Total Loss: 3.4837496280670166 | KNN Loss: 2.4651994705200195 | BCE Loss: 1.018550157546997\n",
      "Epoch 190 / 500 | iteration 25 / 30 | Total Loss: 3.481813430786133 | KNN Loss: 2.4706106185913086 | BCE Loss: 1.0112026929855347\n",
      "Epoch 191 / 500 | iteration 0 / 30 | Total Loss: 3.505706310272217 | KNN Loss: 2.4515955448150635 | BCE Loss: 1.0541108846664429\n",
      "Epoch 191 / 500 | iteration 5 / 30 | Total Loss: 3.469949960708618 | KNN Loss: 2.469250440597534 | BCE Loss: 1.000699520111084\n",
      "Epoch 191 / 500 | iteration 10 / 30 | Total Loss: 3.4451520442962646 | KNN Loss: 2.4481217861175537 | BCE Loss: 0.9970302581787109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 191 / 500 | iteration 15 / 30 | Total Loss: 3.493678569793701 | KNN Loss: 2.46427321434021 | BCE Loss: 1.0294053554534912\n",
      "Epoch 191 / 500 | iteration 20 / 30 | Total Loss: 3.5171847343444824 | KNN Loss: 2.478317975997925 | BCE Loss: 1.0388667583465576\n",
      "Epoch 191 / 500 | iteration 25 / 30 | Total Loss: 3.4994025230407715 | KNN Loss: 2.4978771209716797 | BCE Loss: 1.0015255212783813\n",
      "Epoch 192 / 500 | iteration 0 / 30 | Total Loss: 3.4948947429656982 | KNN Loss: 2.467897653579712 | BCE Loss: 1.0269970893859863\n",
      "Epoch 192 / 500 | iteration 5 / 30 | Total Loss: 3.498119354248047 | KNN Loss: 2.4874825477600098 | BCE Loss: 1.0106369256973267\n",
      "Epoch 192 / 500 | iteration 10 / 30 | Total Loss: 3.5351531505584717 | KNN Loss: 2.5009243488311768 | BCE Loss: 1.034228801727295\n",
      "Epoch 192 / 500 | iteration 15 / 30 | Total Loss: 3.454158067703247 | KNN Loss: 2.4661145210266113 | BCE Loss: 0.9880436062812805\n",
      "Epoch 192 / 500 | iteration 20 / 30 | Total Loss: 3.4966297149658203 | KNN Loss: 2.4917263984680176 | BCE Loss: 1.0049034357070923\n",
      "Epoch 192 / 500 | iteration 25 / 30 | Total Loss: 3.484203815460205 | KNN Loss: 2.467162847518921 | BCE Loss: 1.0170409679412842\n",
      "Epoch 193 / 500 | iteration 0 / 30 | Total Loss: 3.4787890911102295 | KNN Loss: 2.4677846431732178 | BCE Loss: 1.0110044479370117\n",
      "Epoch 193 / 500 | iteration 5 / 30 | Total Loss: 3.4607067108154297 | KNN Loss: 2.4459333419799805 | BCE Loss: 1.0147732496261597\n",
      "Epoch 193 / 500 | iteration 10 / 30 | Total Loss: 3.5146706104278564 | KNN Loss: 2.4947609901428223 | BCE Loss: 1.0199096202850342\n",
      "Epoch 193 / 500 | iteration 15 / 30 | Total Loss: 3.54140305519104 | KNN Loss: 2.494235038757324 | BCE Loss: 1.0471680164337158\n",
      "Epoch 193 / 500 | iteration 20 / 30 | Total Loss: 3.4814043045043945 | KNN Loss: 2.4692459106445312 | BCE Loss: 1.0121585130691528\n",
      "Epoch 193 / 500 | iteration 25 / 30 | Total Loss: 3.493893623352051 | KNN Loss: 2.467460870742798 | BCE Loss: 1.026432752609253\n",
      "Epoch 194 / 500 | iteration 0 / 30 | Total Loss: 3.5109145641326904 | KNN Loss: 2.493885040283203 | BCE Loss: 1.0170295238494873\n",
      "Epoch 194 / 500 | iteration 5 / 30 | Total Loss: 3.51061749458313 | KNN Loss: 2.4847192764282227 | BCE Loss: 1.0258982181549072\n",
      "Epoch 194 / 500 | iteration 10 / 30 | Total Loss: 3.5122952461242676 | KNN Loss: 2.5049548149108887 | BCE Loss: 1.0073403120040894\n",
      "Epoch 194 / 500 | iteration 15 / 30 | Total Loss: 3.504272699356079 | KNN Loss: 2.4953529834747314 | BCE Loss: 1.0089197158813477\n",
      "Epoch 194 / 500 | iteration 20 / 30 | Total Loss: 3.4640753269195557 | KNN Loss: 2.455016613006592 | BCE Loss: 1.0090587139129639\n",
      "Epoch 194 / 500 | iteration 25 / 30 | Total Loss: 3.479509115219116 | KNN Loss: 2.4608216285705566 | BCE Loss: 1.0186874866485596\n",
      "Epoch 195 / 500 | iteration 0 / 30 | Total Loss: 3.483224868774414 | KNN Loss: 2.478889226913452 | BCE Loss: 1.0043355226516724\n",
      "Epoch 195 / 500 | iteration 5 / 30 | Total Loss: 3.5138070583343506 | KNN Loss: 2.491628646850586 | BCE Loss: 1.0221784114837646\n",
      "Epoch 195 / 500 | iteration 10 / 30 | Total Loss: 3.473484992980957 | KNN Loss: 2.4671013355255127 | BCE Loss: 1.0063835382461548\n",
      "Epoch 195 / 500 | iteration 15 / 30 | Total Loss: 3.503872871398926 | KNN Loss: 2.475362539291382 | BCE Loss: 1.028510332107544\n",
      "Epoch 195 / 500 | iteration 20 / 30 | Total Loss: 3.49690580368042 | KNN Loss: 2.4813177585601807 | BCE Loss: 1.0155880451202393\n",
      "Epoch 195 / 500 | iteration 25 / 30 | Total Loss: 3.4796223640441895 | KNN Loss: 2.4682486057281494 | BCE Loss: 1.0113736391067505\n",
      "Epoch 196 / 500 | iteration 0 / 30 | Total Loss: 3.485482931137085 | KNN Loss: 2.4821949005126953 | BCE Loss: 1.0032880306243896\n",
      "Epoch 196 / 500 | iteration 5 / 30 | Total Loss: 3.5280098915100098 | KNN Loss: 2.485900402069092 | BCE Loss: 1.042109489440918\n",
      "Epoch 196 / 500 | iteration 10 / 30 | Total Loss: 3.469356060028076 | KNN Loss: 2.4574203491210938 | BCE Loss: 1.0119355916976929\n",
      "Epoch 196 / 500 | iteration 15 / 30 | Total Loss: 3.5094828605651855 | KNN Loss: 2.4902148246765137 | BCE Loss: 1.0192680358886719\n",
      "Epoch 196 / 500 | iteration 20 / 30 | Total Loss: 3.4616096019744873 | KNN Loss: 2.4848597049713135 | BCE Loss: 0.9767499566078186\n",
      "Epoch 196 / 500 | iteration 25 / 30 | Total Loss: 3.4998998641967773 | KNN Loss: 2.470717430114746 | BCE Loss: 1.0291824340820312\n",
      "Epoch 197 / 500 | iteration 0 / 30 | Total Loss: 3.4951696395874023 | KNN Loss: 2.4806106090545654 | BCE Loss: 1.0145591497421265\n",
      "Epoch 197 / 500 | iteration 5 / 30 | Total Loss: 3.498755693435669 | KNN Loss: 2.4856667518615723 | BCE Loss: 1.0130889415740967\n",
      "Epoch 197 / 500 | iteration 10 / 30 | Total Loss: 3.512082815170288 | KNN Loss: 2.4885075092315674 | BCE Loss: 1.0235753059387207\n",
      "Epoch 197 / 500 | iteration 15 / 30 | Total Loss: 3.5063352584838867 | KNN Loss: 2.49896240234375 | BCE Loss: 1.0073728561401367\n",
      "Epoch 197 / 500 | iteration 20 / 30 | Total Loss: 3.498198986053467 | KNN Loss: 2.5072097778320312 | BCE Loss: 0.9909892678260803\n",
      "Epoch 197 / 500 | iteration 25 / 30 | Total Loss: 3.4978997707366943 | KNN Loss: 2.4663946628570557 | BCE Loss: 1.0315051078796387\n",
      "Epoch 198 / 500 | iteration 0 / 30 | Total Loss: 3.5037784576416016 | KNN Loss: 2.519970655441284 | BCE Loss: 0.9838078022003174\n",
      "Epoch 198 / 500 | iteration 5 / 30 | Total Loss: 3.5186238288879395 | KNN Loss: 2.4882495403289795 | BCE Loss: 1.03037428855896\n",
      "Epoch 198 / 500 | iteration 10 / 30 | Total Loss: 3.51847767829895 | KNN Loss: 2.490910768508911 | BCE Loss: 1.027566909790039\n",
      "Epoch 198 / 500 | iteration 15 / 30 | Total Loss: 3.4494431018829346 | KNN Loss: 2.460839033126831 | BCE Loss: 0.9886040687561035\n",
      "Epoch 198 / 500 | iteration 20 / 30 | Total Loss: 3.476105213165283 | KNN Loss: 2.4672462940216064 | BCE Loss: 1.0088587999343872\n",
      "Epoch 198 / 500 | iteration 25 / 30 | Total Loss: 3.473606824874878 | KNN Loss: 2.4654078483581543 | BCE Loss: 1.0081989765167236\n",
      "Epoch 199 / 500 | iteration 0 / 30 | Total Loss: 3.5041542053222656 | KNN Loss: 2.495023488998413 | BCE Loss: 1.0091307163238525\n",
      "Epoch 199 / 500 | iteration 5 / 30 | Total Loss: 3.502108335494995 | KNN Loss: 2.4675848484039307 | BCE Loss: 1.0345234870910645\n",
      "Epoch 199 / 500 | iteration 10 / 30 | Total Loss: 3.505183458328247 | KNN Loss: 2.474931240081787 | BCE Loss: 1.03025221824646\n",
      "Epoch 199 / 500 | iteration 15 / 30 | Total Loss: 3.497554063796997 | KNN Loss: 2.4830543994903564 | BCE Loss: 1.0144996643066406\n",
      "Epoch 199 / 500 | iteration 20 / 30 | Total Loss: 3.5492563247680664 | KNN Loss: 2.52652907371521 | BCE Loss: 1.0227272510528564\n",
      "Epoch 199 / 500 | iteration 25 / 30 | Total Loss: 3.49979567527771 | KNN Loss: 2.491739273071289 | BCE Loss: 1.008056402206421\n",
      "Epoch 200 / 500 | iteration 0 / 30 | Total Loss: 3.4903812408447266 | KNN Loss: 2.4848108291625977 | BCE Loss: 1.005570411682129\n",
      "Epoch 200 / 500 | iteration 5 / 30 | Total Loss: 3.5412139892578125 | KNN Loss: 2.496830463409424 | BCE Loss: 1.0443835258483887\n",
      "Epoch 200 / 500 | iteration 10 / 30 | Total Loss: 3.476839542388916 | KNN Loss: 2.4652936458587646 | BCE Loss: 1.0115457773208618\n",
      "Epoch 200 / 500 | iteration 15 / 30 | Total Loss: 3.444221258163452 | KNN Loss: 2.457636594772339 | BCE Loss: 0.9865846633911133\n",
      "Epoch 200 / 500 | iteration 20 / 30 | Total Loss: 3.5524868965148926 | KNN Loss: 2.492591381072998 | BCE Loss: 1.0598955154418945\n",
      "Epoch 200 / 500 | iteration 25 / 30 | Total Loss: 3.4666874408721924 | KNN Loss: 2.464963436126709 | BCE Loss: 1.0017240047454834\n",
      "Epoch   201: reducing learning rate of group 0 to 8.4035e-04.\n",
      "Epoch 201 / 500 | iteration 0 / 30 | Total Loss: 3.4813947677612305 | KNN Loss: 2.4640612602233887 | BCE Loss: 1.0173335075378418\n",
      "Epoch 201 / 500 | iteration 5 / 30 | Total Loss: 3.5280699729919434 | KNN Loss: 2.491544485092163 | BCE Loss: 1.0365254878997803\n",
      "Epoch 201 / 500 | iteration 10 / 30 | Total Loss: 3.459812879562378 | KNN Loss: 2.460075616836548 | BCE Loss: 0.9997373223304749\n",
      "Epoch 201 / 500 | iteration 15 / 30 | Total Loss: 3.4549427032470703 | KNN Loss: 2.4668383598327637 | BCE Loss: 0.9881042838096619\n",
      "Epoch 201 / 500 | iteration 20 / 30 | Total Loss: 3.4806556701660156 | KNN Loss: 2.469759941101074 | BCE Loss: 1.010895848274231\n",
      "Epoch 201 / 500 | iteration 25 / 30 | Total Loss: 3.5170693397521973 | KNN Loss: 2.4754161834716797 | BCE Loss: 1.041653037071228\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 202 / 500 | iteration 0 / 30 | Total Loss: 3.4852137565612793 | KNN Loss: 2.469900369644165 | BCE Loss: 1.0153132677078247\n",
      "Epoch 202 / 500 | iteration 5 / 30 | Total Loss: 3.506436347961426 | KNN Loss: 2.481440305709839 | BCE Loss: 1.0249959230422974\n",
      "Epoch 202 / 500 | iteration 10 / 30 | Total Loss: 3.485416889190674 | KNN Loss: 2.4587881565093994 | BCE Loss: 1.0266287326812744\n",
      "Epoch 202 / 500 | iteration 15 / 30 | Total Loss: 3.506577253341675 | KNN Loss: 2.4914398193359375 | BCE Loss: 1.0151374340057373\n",
      "Epoch 202 / 500 | iteration 20 / 30 | Total Loss: 3.50209379196167 | KNN Loss: 2.495753526687622 | BCE Loss: 1.0063402652740479\n",
      "Epoch 202 / 500 | iteration 25 / 30 | Total Loss: 3.4758317470550537 | KNN Loss: 2.4733033180236816 | BCE Loss: 1.002528429031372\n",
      "Epoch 203 / 500 | iteration 0 / 30 | Total Loss: 3.4789905548095703 | KNN Loss: 2.444753408432007 | BCE Loss: 1.0342371463775635\n",
      "Epoch 203 / 500 | iteration 5 / 30 | Total Loss: 3.4938454627990723 | KNN Loss: 2.466472625732422 | BCE Loss: 1.0273727178573608\n",
      "Epoch 203 / 500 | iteration 10 / 30 | Total Loss: 3.477956771850586 | KNN Loss: 2.476287603378296 | BCE Loss: 1.00166916847229\n",
      "Epoch 203 / 500 | iteration 15 / 30 | Total Loss: 3.481200695037842 | KNN Loss: 2.4655284881591797 | BCE Loss: 1.0156723260879517\n",
      "Epoch 203 / 500 | iteration 20 / 30 | Total Loss: 3.442143440246582 | KNN Loss: 2.4585514068603516 | BCE Loss: 0.9835920929908752\n",
      "Epoch 203 / 500 | iteration 25 / 30 | Total Loss: 3.505138874053955 | KNN Loss: 2.488135814666748 | BCE Loss: 1.017003059387207\n",
      "Epoch 204 / 500 | iteration 0 / 30 | Total Loss: 3.457343816757202 | KNN Loss: 2.4491536617279053 | BCE Loss: 1.0081901550292969\n",
      "Epoch 204 / 500 | iteration 5 / 30 | Total Loss: 3.4610767364501953 | KNN Loss: 2.472646951675415 | BCE Loss: 0.9884299039840698\n",
      "Epoch 204 / 500 | iteration 10 / 30 | Total Loss: 3.454737424850464 | KNN Loss: 2.447946786880493 | BCE Loss: 1.0067906379699707\n",
      "Epoch 204 / 500 | iteration 15 / 30 | Total Loss: 3.5135316848754883 | KNN Loss: 2.4711532592773438 | BCE Loss: 1.0423784255981445\n",
      "Epoch 204 / 500 | iteration 20 / 30 | Total Loss: 3.5101096630096436 | KNN Loss: 2.471839666366577 | BCE Loss: 1.0382699966430664\n",
      "Epoch 204 / 500 | iteration 25 / 30 | Total Loss: 3.5170350074768066 | KNN Loss: 2.4807281494140625 | BCE Loss: 1.0363068580627441\n",
      "Epoch 205 / 500 | iteration 0 / 30 | Total Loss: 3.5398898124694824 | KNN Loss: 2.489309787750244 | BCE Loss: 1.0505800247192383\n",
      "Epoch 205 / 500 | iteration 5 / 30 | Total Loss: 3.4887959957122803 | KNN Loss: 2.48761248588562 | BCE Loss: 1.0011835098266602\n",
      "Epoch 205 / 500 | iteration 10 / 30 | Total Loss: 3.518726110458374 | KNN Loss: 2.4758999347686768 | BCE Loss: 1.0428261756896973\n",
      "Epoch 205 / 500 | iteration 15 / 30 | Total Loss: 3.4894938468933105 | KNN Loss: 2.471827507019043 | BCE Loss: 1.0176663398742676\n",
      "Epoch 205 / 500 | iteration 20 / 30 | Total Loss: 3.477393865585327 | KNN Loss: 2.4473161697387695 | BCE Loss: 1.0300776958465576\n",
      "Epoch 205 / 500 | iteration 25 / 30 | Total Loss: 3.461958408355713 | KNN Loss: 2.4695582389831543 | BCE Loss: 0.9924001693725586\n",
      "Epoch 206 / 500 | iteration 0 / 30 | Total Loss: 3.496471881866455 | KNN Loss: 2.4731268882751465 | BCE Loss: 1.0233449935913086\n",
      "Epoch 206 / 500 | iteration 5 / 30 | Total Loss: 3.5096020698547363 | KNN Loss: 2.4705026149749756 | BCE Loss: 1.0390994548797607\n",
      "Epoch 206 / 500 | iteration 10 / 30 | Total Loss: 3.5068764686584473 | KNN Loss: 2.473191738128662 | BCE Loss: 1.0336846113204956\n",
      "Epoch 206 / 500 | iteration 15 / 30 | Total Loss: 3.4854540824890137 | KNN Loss: 2.4633703231811523 | BCE Loss: 1.0220837593078613\n",
      "Epoch 206 / 500 | iteration 20 / 30 | Total Loss: 3.456644296646118 | KNN Loss: 2.4558398723602295 | BCE Loss: 1.0008044242858887\n",
      "Epoch 206 / 500 | iteration 25 / 30 | Total Loss: 3.4863085746765137 | KNN Loss: 2.4610981941223145 | BCE Loss: 1.0252104997634888\n",
      "Epoch 207 / 500 | iteration 0 / 30 | Total Loss: 3.4927709102630615 | KNN Loss: 2.47749400138855 | BCE Loss: 1.0152769088745117\n",
      "Epoch 207 / 500 | iteration 5 / 30 | Total Loss: 3.476879835128784 | KNN Loss: 2.45784854888916 | BCE Loss: 1.019031286239624\n",
      "Epoch 207 / 500 | iteration 10 / 30 | Total Loss: 3.501067638397217 | KNN Loss: 2.482773780822754 | BCE Loss: 1.0182937383651733\n",
      "Epoch 207 / 500 | iteration 15 / 30 | Total Loss: 3.4758825302124023 | KNN Loss: 2.4478323459625244 | BCE Loss: 1.0280503034591675\n",
      "Epoch 207 / 500 | iteration 20 / 30 | Total Loss: 3.464167594909668 | KNN Loss: 2.4652137756347656 | BCE Loss: 0.9989539384841919\n",
      "Epoch 207 / 500 | iteration 25 / 30 | Total Loss: 3.491589069366455 | KNN Loss: 2.4562408924102783 | BCE Loss: 1.0353480577468872\n",
      "Epoch 208 / 500 | iteration 0 / 30 | Total Loss: 3.4756393432617188 | KNN Loss: 2.472898006439209 | BCE Loss: 1.0027412176132202\n",
      "Epoch 208 / 500 | iteration 5 / 30 | Total Loss: 3.4648444652557373 | KNN Loss: 2.461578369140625 | BCE Loss: 1.0032660961151123\n",
      "Epoch 208 / 500 | iteration 10 / 30 | Total Loss: 3.5065953731536865 | KNN Loss: 2.481506824493408 | BCE Loss: 1.0250885486602783\n",
      "Epoch 208 / 500 | iteration 15 / 30 | Total Loss: 3.505013942718506 | KNN Loss: 2.4678027629852295 | BCE Loss: 1.0372111797332764\n",
      "Epoch 208 / 500 | iteration 20 / 30 | Total Loss: 3.5147018432617188 | KNN Loss: 2.464219093322754 | BCE Loss: 1.0504827499389648\n",
      "Epoch 208 / 500 | iteration 25 / 30 | Total Loss: 3.481215476989746 | KNN Loss: 2.471658229827881 | BCE Loss: 1.0095571279525757\n",
      "Epoch 209 / 500 | iteration 0 / 30 | Total Loss: 3.479344129562378 | KNN Loss: 2.465242624282837 | BCE Loss: 1.014101505279541\n",
      "Epoch 209 / 500 | iteration 5 / 30 | Total Loss: 3.484463691711426 | KNN Loss: 2.455293655395508 | BCE Loss: 1.0291699171066284\n",
      "Epoch 209 / 500 | iteration 10 / 30 | Total Loss: 3.501926898956299 | KNN Loss: 2.487460136413574 | BCE Loss: 1.0144667625427246\n",
      "Epoch 209 / 500 | iteration 15 / 30 | Total Loss: 3.4651877880096436 | KNN Loss: 2.456191301345825 | BCE Loss: 1.0089964866638184\n",
      "Epoch 209 / 500 | iteration 20 / 30 | Total Loss: 3.4909276962280273 | KNN Loss: 2.4620935916900635 | BCE Loss: 1.0288341045379639\n",
      "Epoch 209 / 500 | iteration 25 / 30 | Total Loss: 3.4672584533691406 | KNN Loss: 2.457047939300537 | BCE Loss: 1.0102105140686035\n",
      "Epoch 210 / 500 | iteration 0 / 30 | Total Loss: 3.5148544311523438 | KNN Loss: 2.474346876144409 | BCE Loss: 1.040507435798645\n",
      "Epoch 210 / 500 | iteration 5 / 30 | Total Loss: 3.496626853942871 | KNN Loss: 2.4559824466705322 | BCE Loss: 1.0406444072723389\n",
      "Epoch 210 / 500 | iteration 10 / 30 | Total Loss: 3.464855670928955 | KNN Loss: 2.453380584716797 | BCE Loss: 1.0114750862121582\n",
      "Epoch 210 / 500 | iteration 15 / 30 | Total Loss: 3.464334726333618 | KNN Loss: 2.4635212421417236 | BCE Loss: 1.0008134841918945\n",
      "Epoch 210 / 500 | iteration 20 / 30 | Total Loss: 3.4765727519989014 | KNN Loss: 2.468979597091675 | BCE Loss: 1.0075931549072266\n",
      "Epoch 210 / 500 | iteration 25 / 30 | Total Loss: 3.472921848297119 | KNN Loss: 2.4752302169799805 | BCE Loss: 0.9976917505264282\n",
      "Epoch 211 / 500 | iteration 0 / 30 | Total Loss: 3.51466703414917 | KNN Loss: 2.488832712173462 | BCE Loss: 1.025834321975708\n",
      "Epoch 211 / 500 | iteration 5 / 30 | Total Loss: 3.4797840118408203 | KNN Loss: 2.4757821559906006 | BCE Loss: 1.0040019750595093\n",
      "Epoch 211 / 500 | iteration 10 / 30 | Total Loss: 3.4522533416748047 | KNN Loss: 2.4403934478759766 | BCE Loss: 1.0118600130081177\n",
      "Epoch 211 / 500 | iteration 15 / 30 | Total Loss: 3.5045547485351562 | KNN Loss: 2.4679641723632812 | BCE Loss: 1.0365904569625854\n",
      "Epoch 211 / 500 | iteration 20 / 30 | Total Loss: 3.477290630340576 | KNN Loss: 2.4531726837158203 | BCE Loss: 1.0241179466247559\n",
      "Epoch 211 / 500 | iteration 25 / 30 | Total Loss: 3.465482711791992 | KNN Loss: 2.4648170471191406 | BCE Loss: 1.000665545463562\n",
      "Epoch 212 / 500 | iteration 0 / 30 | Total Loss: 3.44920015335083 | KNN Loss: 2.4729955196380615 | BCE Loss: 0.9762046337127686\n",
      "Epoch 212 / 500 | iteration 5 / 30 | Total Loss: 3.5299453735351562 | KNN Loss: 2.483999252319336 | BCE Loss: 1.0459461212158203\n",
      "Epoch 212 / 500 | iteration 10 / 30 | Total Loss: 3.5003292560577393 | KNN Loss: 2.4904072284698486 | BCE Loss: 1.0099220275878906\n",
      "Epoch 212 / 500 | iteration 15 / 30 | Total Loss: 3.448462724685669 | KNN Loss: 2.4446492195129395 | BCE Loss: 1.0038135051727295\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 212 / 500 | iteration 20 / 30 | Total Loss: 3.4997994899749756 | KNN Loss: 2.480531692504883 | BCE Loss: 1.0192677974700928\n",
      "Epoch 212 / 500 | iteration 25 / 30 | Total Loss: 3.475440502166748 | KNN Loss: 2.4762654304504395 | BCE Loss: 0.9991750121116638\n",
      "Epoch 213 / 500 | iteration 0 / 30 | Total Loss: 3.491387367248535 | KNN Loss: 2.4822981357574463 | BCE Loss: 1.0090892314910889\n",
      "Epoch 213 / 500 | iteration 5 / 30 | Total Loss: 3.4993159770965576 | KNN Loss: 2.447472333908081 | BCE Loss: 1.0518436431884766\n",
      "Epoch 213 / 500 | iteration 10 / 30 | Total Loss: 3.532686948776245 | KNN Loss: 2.50586199760437 | BCE Loss: 1.026824951171875\n",
      "Epoch 213 / 500 | iteration 15 / 30 | Total Loss: 3.4772043228149414 | KNN Loss: 2.4476232528686523 | BCE Loss: 1.0295811891555786\n",
      "Epoch 213 / 500 | iteration 20 / 30 | Total Loss: 3.489011287689209 | KNN Loss: 2.4446699619293213 | BCE Loss: 1.0443414449691772\n",
      "Epoch 213 / 500 | iteration 25 / 30 | Total Loss: 3.4883484840393066 | KNN Loss: 2.474738597869873 | BCE Loss: 1.0136100053787231\n",
      "Epoch 214 / 500 | iteration 0 / 30 | Total Loss: 3.4772086143493652 | KNN Loss: 2.4703783988952637 | BCE Loss: 1.006830096244812\n",
      "Epoch 214 / 500 | iteration 5 / 30 | Total Loss: 3.4935431480407715 | KNN Loss: 2.487614393234253 | BCE Loss: 1.0059287548065186\n",
      "Epoch 214 / 500 | iteration 10 / 30 | Total Loss: 3.4966773986816406 | KNN Loss: 2.4795162677764893 | BCE Loss: 1.0171611309051514\n",
      "Epoch 214 / 500 | iteration 15 / 30 | Total Loss: 3.4829611778259277 | KNN Loss: 2.47477388381958 | BCE Loss: 1.0081872940063477\n",
      "Epoch 214 / 500 | iteration 20 / 30 | Total Loss: 3.4894485473632812 | KNN Loss: 2.490006685256958 | BCE Loss: 0.9994418025016785\n",
      "Epoch 214 / 500 | iteration 25 / 30 | Total Loss: 3.519376039505005 | KNN Loss: 2.5020012855529785 | BCE Loss: 1.0173747539520264\n",
      "Epoch 215 / 500 | iteration 0 / 30 | Total Loss: 3.480499744415283 | KNN Loss: 2.4789674282073975 | BCE Loss: 1.0015323162078857\n",
      "Epoch 215 / 500 | iteration 5 / 30 | Total Loss: 3.4904162883758545 | KNN Loss: 2.4734857082366943 | BCE Loss: 1.0169305801391602\n",
      "Epoch 215 / 500 | iteration 10 / 30 | Total Loss: 3.527040958404541 | KNN Loss: 2.47866153717041 | BCE Loss: 1.0483795404434204\n",
      "Epoch 215 / 500 | iteration 15 / 30 | Total Loss: 3.527459144592285 | KNN Loss: 2.5046799182891846 | BCE Loss: 1.0227793455123901\n",
      "Epoch 215 / 500 | iteration 20 / 30 | Total Loss: 3.5266199111938477 | KNN Loss: 2.506671190261841 | BCE Loss: 1.0199487209320068\n",
      "Epoch 215 / 500 | iteration 25 / 30 | Total Loss: 3.5261104106903076 | KNN Loss: 2.5036373138427734 | BCE Loss: 1.0224730968475342\n",
      "Epoch 216 / 500 | iteration 0 / 30 | Total Loss: 3.4767141342163086 | KNN Loss: 2.4650607109069824 | BCE Loss: 1.0116533041000366\n",
      "Epoch 216 / 500 | iteration 5 / 30 | Total Loss: 3.4735660552978516 | KNN Loss: 2.4612441062927246 | BCE Loss: 1.012321949005127\n",
      "Epoch 216 / 500 | iteration 10 / 30 | Total Loss: 3.5344204902648926 | KNN Loss: 2.500948667526245 | BCE Loss: 1.0334718227386475\n",
      "Epoch 216 / 500 | iteration 15 / 30 | Total Loss: 3.4538583755493164 | KNN Loss: 2.471248149871826 | BCE Loss: 0.9826102256774902\n",
      "Epoch 216 / 500 | iteration 20 / 30 | Total Loss: 3.4938690662384033 | KNN Loss: 2.4950287342071533 | BCE Loss: 0.9988402724266052\n",
      "Epoch 216 / 500 | iteration 25 / 30 | Total Loss: 3.4962916374206543 | KNN Loss: 2.4840216636657715 | BCE Loss: 1.0122699737548828\n",
      "Epoch 217 / 500 | iteration 0 / 30 | Total Loss: 3.4966535568237305 | KNN Loss: 2.49988055229187 | BCE Loss: 0.9967731237411499\n",
      "Epoch 217 / 500 | iteration 5 / 30 | Total Loss: 3.4981517791748047 | KNN Loss: 2.5041959285736084 | BCE Loss: 0.9939559102058411\n",
      "Epoch 217 / 500 | iteration 10 / 30 | Total Loss: 3.482023239135742 | KNN Loss: 2.482985258102417 | BCE Loss: 0.99903804063797\n",
      "Epoch 217 / 500 | iteration 15 / 30 | Total Loss: 3.492370843887329 | KNN Loss: 2.4900522232055664 | BCE Loss: 1.0023186206817627\n",
      "Epoch 217 / 500 | iteration 20 / 30 | Total Loss: 3.4910738468170166 | KNN Loss: 2.481612205505371 | BCE Loss: 1.0094616413116455\n",
      "Epoch 217 / 500 | iteration 25 / 30 | Total Loss: 3.5183229446411133 | KNN Loss: 2.491422414779663 | BCE Loss: 1.0269004106521606\n",
      "Epoch 218 / 500 | iteration 0 / 30 | Total Loss: 3.458597183227539 | KNN Loss: 2.4512293338775635 | BCE Loss: 1.0073678493499756\n",
      "Epoch 218 / 500 | iteration 5 / 30 | Total Loss: 3.5206193923950195 | KNN Loss: 2.477863311767578 | BCE Loss: 1.0427559614181519\n",
      "Epoch 218 / 500 | iteration 10 / 30 | Total Loss: 3.5178022384643555 | KNN Loss: 2.4683780670166016 | BCE Loss: 1.0494240522384644\n",
      "Epoch 218 / 500 | iteration 15 / 30 | Total Loss: 3.4736557006835938 | KNN Loss: 2.4557392597198486 | BCE Loss: 1.0179164409637451\n",
      "Epoch 218 / 500 | iteration 20 / 30 | Total Loss: 3.55493426322937 | KNN Loss: 2.4983935356140137 | BCE Loss: 1.0565407276153564\n",
      "Epoch 218 / 500 | iteration 25 / 30 | Total Loss: 3.494741916656494 | KNN Loss: 2.4734036922454834 | BCE Loss: 1.0213381052017212\n",
      "Epoch 219 / 500 | iteration 0 / 30 | Total Loss: 3.4997518062591553 | KNN Loss: 2.468474864959717 | BCE Loss: 1.0312769412994385\n",
      "Epoch 219 / 500 | iteration 5 / 30 | Total Loss: 3.4975156784057617 | KNN Loss: 2.4669382572174072 | BCE Loss: 1.0305774211883545\n",
      "Epoch 219 / 500 | iteration 10 / 30 | Total Loss: 3.4746599197387695 | KNN Loss: 2.4821972846984863 | BCE Loss: 0.9924627542495728\n",
      "Epoch 219 / 500 | iteration 15 / 30 | Total Loss: 3.489140033721924 | KNN Loss: 2.496527671813965 | BCE Loss: 0.9926122426986694\n",
      "Epoch 219 / 500 | iteration 20 / 30 | Total Loss: 3.501772165298462 | KNN Loss: 2.4620354175567627 | BCE Loss: 1.0397367477416992\n",
      "Epoch 219 / 500 | iteration 25 / 30 | Total Loss: 3.489445686340332 | KNN Loss: 2.446946382522583 | BCE Loss: 1.0424994230270386\n",
      "Epoch 220 / 500 | iteration 0 / 30 | Total Loss: 3.5046873092651367 | KNN Loss: 2.4800448417663574 | BCE Loss: 1.0246424674987793\n",
      "Epoch 220 / 500 | iteration 5 / 30 | Total Loss: 3.4842638969421387 | KNN Loss: 2.4495677947998047 | BCE Loss: 1.034696102142334\n",
      "Epoch 220 / 500 | iteration 10 / 30 | Total Loss: 3.5293564796447754 | KNN Loss: 2.4944212436676025 | BCE Loss: 1.0349353551864624\n",
      "Epoch 220 / 500 | iteration 15 / 30 | Total Loss: 3.472810745239258 | KNN Loss: 2.471048355102539 | BCE Loss: 1.0017622709274292\n",
      "Epoch 220 / 500 | iteration 20 / 30 | Total Loss: 3.460845947265625 | KNN Loss: 2.4654927253723145 | BCE Loss: 0.9953533411026001\n",
      "Epoch 220 / 500 | iteration 25 / 30 | Total Loss: 3.4623069763183594 | KNN Loss: 2.470412015914917 | BCE Loss: 0.9918949604034424\n",
      "Epoch 221 / 500 | iteration 0 / 30 | Total Loss: 3.449106216430664 | KNN Loss: 2.4541213512420654 | BCE Loss: 0.9949848055839539\n",
      "Epoch 221 / 500 | iteration 5 / 30 | Total Loss: 3.4872703552246094 | KNN Loss: 2.4836080074310303 | BCE Loss: 1.003662347793579\n",
      "Epoch 221 / 500 | iteration 10 / 30 | Total Loss: 3.4624524116516113 | KNN Loss: 2.454674005508423 | BCE Loss: 1.007778286933899\n",
      "Epoch 221 / 500 | iteration 15 / 30 | Total Loss: 3.4645979404449463 | KNN Loss: 2.4424612522125244 | BCE Loss: 1.0221366882324219\n",
      "Epoch 221 / 500 | iteration 20 / 30 | Total Loss: 3.4816808700561523 | KNN Loss: 2.469285249710083 | BCE Loss: 1.0123956203460693\n",
      "Epoch 221 / 500 | iteration 25 / 30 | Total Loss: 3.457986831665039 | KNN Loss: 2.467803955078125 | BCE Loss: 0.9901828765869141\n",
      "Epoch 222 / 500 | iteration 0 / 30 | Total Loss: 3.5205488204956055 | KNN Loss: 2.5004069805145264 | BCE Loss: 1.020141839981079\n",
      "Epoch 222 / 500 | iteration 5 / 30 | Total Loss: 3.5172104835510254 | KNN Loss: 2.500580072402954 | BCE Loss: 1.0166304111480713\n",
      "Epoch 222 / 500 | iteration 10 / 30 | Total Loss: 3.470705032348633 | KNN Loss: 2.4554758071899414 | BCE Loss: 1.0152292251586914\n",
      "Epoch 222 / 500 | iteration 15 / 30 | Total Loss: 3.4906821250915527 | KNN Loss: 2.443817377090454 | BCE Loss: 1.0468647480010986\n",
      "Epoch 222 / 500 | iteration 20 / 30 | Total Loss: 3.5290660858154297 | KNN Loss: 2.5058372020721436 | BCE Loss: 1.0232288837432861\n",
      "Epoch 222 / 500 | iteration 25 / 30 | Total Loss: 3.5166187286376953 | KNN Loss: 2.499661684036255 | BCE Loss: 1.01695716381073\n",
      "Epoch 223 / 500 | iteration 0 / 30 | Total Loss: 3.4912421703338623 | KNN Loss: 2.480454683303833 | BCE Loss: 1.0107874870300293\n",
      "Epoch 223 / 500 | iteration 5 / 30 | Total Loss: 3.5215089321136475 | KNN Loss: 2.451597213745117 | BCE Loss: 1.0699117183685303\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 223 / 500 | iteration 10 / 30 | Total Loss: 3.48211932182312 | KNN Loss: 2.4569292068481445 | BCE Loss: 1.0251901149749756\n",
      "Epoch 223 / 500 | iteration 15 / 30 | Total Loss: 3.4521803855895996 | KNN Loss: 2.4801275730133057 | BCE Loss: 0.972052812576294\n",
      "Epoch 223 / 500 | iteration 20 / 30 | Total Loss: 3.4767961502075195 | KNN Loss: 2.451019525527954 | BCE Loss: 1.0257765054702759\n",
      "Epoch 223 / 500 | iteration 25 / 30 | Total Loss: 3.50527286529541 | KNN Loss: 2.473127603530884 | BCE Loss: 1.0321452617645264\n",
      "Epoch 224 / 500 | iteration 0 / 30 | Total Loss: 3.4720869064331055 | KNN Loss: 2.4628007411956787 | BCE Loss: 1.0092862844467163\n",
      "Epoch 224 / 500 | iteration 5 / 30 | Total Loss: 3.488077163696289 | KNN Loss: 2.473170042037964 | BCE Loss: 1.0149070024490356\n",
      "Epoch 224 / 500 | iteration 10 / 30 | Total Loss: 3.4708476066589355 | KNN Loss: 2.4547536373138428 | BCE Loss: 1.0160940885543823\n",
      "Epoch 224 / 500 | iteration 15 / 30 | Total Loss: 3.517003059387207 | KNN Loss: 2.487504005432129 | BCE Loss: 1.0294989347457886\n",
      "Epoch 224 / 500 | iteration 20 / 30 | Total Loss: 3.5038576126098633 | KNN Loss: 2.466417074203491 | BCE Loss: 1.0374406576156616\n",
      "Epoch 224 / 500 | iteration 25 / 30 | Total Loss: 3.4783074855804443 | KNN Loss: 2.483271837234497 | BCE Loss: 0.9950356483459473\n",
      "Epoch 225 / 500 | iteration 0 / 30 | Total Loss: 3.4637441635131836 | KNN Loss: 2.4495365619659424 | BCE Loss: 1.0142074823379517\n",
      "Epoch 225 / 500 | iteration 5 / 30 | Total Loss: 3.5227980613708496 | KNN Loss: 2.489727735519409 | BCE Loss: 1.0330702066421509\n",
      "Epoch 225 / 500 | iteration 10 / 30 | Total Loss: 3.4867091178894043 | KNN Loss: 2.4816150665283203 | BCE Loss: 1.005094051361084\n",
      "Epoch 225 / 500 | iteration 15 / 30 | Total Loss: 3.519122362136841 | KNN Loss: 2.496894598007202 | BCE Loss: 1.0222277641296387\n",
      "Epoch 225 / 500 | iteration 20 / 30 | Total Loss: 3.5370635986328125 | KNN Loss: 2.4897563457489014 | BCE Loss: 1.0473072528839111\n",
      "Epoch 225 / 500 | iteration 25 / 30 | Total Loss: 3.5283021926879883 | KNN Loss: 2.499591827392578 | BCE Loss: 1.0287102460861206\n",
      "Epoch 226 / 500 | iteration 0 / 30 | Total Loss: 3.5321803092956543 | KNN Loss: 2.496835708618164 | BCE Loss: 1.0353446006774902\n",
      "Epoch 226 / 500 | iteration 5 / 30 | Total Loss: 3.437544107437134 | KNN Loss: 2.448559284210205 | BCE Loss: 0.9889848828315735\n",
      "Epoch 226 / 500 | iteration 10 / 30 | Total Loss: 3.4997477531433105 | KNN Loss: 2.4768893718719482 | BCE Loss: 1.0228585004806519\n",
      "Epoch 226 / 500 | iteration 15 / 30 | Total Loss: 3.4812283515930176 | KNN Loss: 2.4751644134521484 | BCE Loss: 1.0060640573501587\n",
      "Epoch 226 / 500 | iteration 20 / 30 | Total Loss: 3.5265982151031494 | KNN Loss: 2.4771785736083984 | BCE Loss: 1.049419641494751\n",
      "Epoch 226 / 500 | iteration 25 / 30 | Total Loss: 3.447646141052246 | KNN Loss: 2.4420366287231445 | BCE Loss: 1.005609393119812\n",
      "Epoch 227 / 500 | iteration 0 / 30 | Total Loss: 3.502748966217041 | KNN Loss: 2.506941556930542 | BCE Loss: 0.9958072900772095\n",
      "Epoch 227 / 500 | iteration 5 / 30 | Total Loss: 3.4803805351257324 | KNN Loss: 2.442413568496704 | BCE Loss: 1.0379669666290283\n",
      "Epoch 227 / 500 | iteration 10 / 30 | Total Loss: 3.4707157611846924 | KNN Loss: 2.4650444984436035 | BCE Loss: 1.0056712627410889\n",
      "Epoch 227 / 500 | iteration 15 / 30 | Total Loss: 3.4687418937683105 | KNN Loss: 2.441757917404175 | BCE Loss: 1.0269839763641357\n",
      "Epoch 227 / 500 | iteration 20 / 30 | Total Loss: 3.508010149002075 | KNN Loss: 2.4844918251037598 | BCE Loss: 1.0235183238983154\n",
      "Epoch 227 / 500 | iteration 25 / 30 | Total Loss: 3.4365577697753906 | KNN Loss: 2.4265213012695312 | BCE Loss: 1.0100363492965698\n",
      "Epoch 228 / 500 | iteration 0 / 30 | Total Loss: 3.455495834350586 | KNN Loss: 2.450697183609009 | BCE Loss: 1.0047986507415771\n",
      "Epoch 228 / 500 | iteration 5 / 30 | Total Loss: 3.4948465824127197 | KNN Loss: 2.475978374481201 | BCE Loss: 1.0188682079315186\n",
      "Epoch 228 / 500 | iteration 10 / 30 | Total Loss: 3.4736452102661133 | KNN Loss: 2.466383934020996 | BCE Loss: 1.0072612762451172\n",
      "Epoch 228 / 500 | iteration 15 / 30 | Total Loss: 3.4571690559387207 | KNN Loss: 2.48030161857605 | BCE Loss: 0.9768675565719604\n",
      "Epoch 228 / 500 | iteration 20 / 30 | Total Loss: 3.481001377105713 | KNN Loss: 2.478764057159424 | BCE Loss: 1.0022374391555786\n",
      "Epoch 228 / 500 | iteration 25 / 30 | Total Loss: 3.469822883605957 | KNN Loss: 2.458494186401367 | BCE Loss: 1.0113286972045898\n",
      "Epoch 229 / 500 | iteration 0 / 30 | Total Loss: 3.4692978858947754 | KNN Loss: 2.4621763229370117 | BCE Loss: 1.0071215629577637\n",
      "Epoch 229 / 500 | iteration 5 / 30 | Total Loss: 3.4846463203430176 | KNN Loss: 2.4828782081604004 | BCE Loss: 1.0017679929733276\n",
      "Epoch 229 / 500 | iteration 10 / 30 | Total Loss: 3.4981584548950195 | KNN Loss: 2.452908515930176 | BCE Loss: 1.0452500581741333\n",
      "Epoch 229 / 500 | iteration 15 / 30 | Total Loss: 3.468909740447998 | KNN Loss: 2.4452919960021973 | BCE Loss: 1.0236177444458008\n",
      "Epoch 229 / 500 | iteration 20 / 30 | Total Loss: 3.4392991065979004 | KNN Loss: 2.4423537254333496 | BCE Loss: 0.9969454407691956\n",
      "Epoch 229 / 500 | iteration 25 / 30 | Total Loss: 3.4699857234954834 | KNN Loss: 2.4581503868103027 | BCE Loss: 1.0118353366851807\n",
      "Epoch 230 / 500 | iteration 0 / 30 | Total Loss: 3.4775567054748535 | KNN Loss: 2.4488322734832764 | BCE Loss: 1.0287245512008667\n",
      "Epoch 230 / 500 | iteration 5 / 30 | Total Loss: 3.517446994781494 | KNN Loss: 2.486645221710205 | BCE Loss: 1.0308018922805786\n",
      "Epoch 230 / 500 | iteration 10 / 30 | Total Loss: 3.451936960220337 | KNN Loss: 2.4497900009155273 | BCE Loss: 1.0021469593048096\n",
      "Epoch 230 / 500 | iteration 15 / 30 | Total Loss: 3.48405122756958 | KNN Loss: 2.4505395889282227 | BCE Loss: 1.0335116386413574\n",
      "Epoch 230 / 500 | iteration 20 / 30 | Total Loss: 3.4899940490722656 | KNN Loss: 2.4795494079589844 | BCE Loss: 1.0104446411132812\n",
      "Epoch 230 / 500 | iteration 25 / 30 | Total Loss: 3.5036749839782715 | KNN Loss: 2.5188584327697754 | BCE Loss: 0.9848165512084961\n",
      "Epoch   231: reducing learning rate of group 0 to 5.8824e-04.\n",
      "Epoch 231 / 500 | iteration 0 / 30 | Total Loss: 3.488420248031616 | KNN Loss: 2.4852218627929688 | BCE Loss: 1.0031983852386475\n",
      "Epoch 231 / 500 | iteration 5 / 30 | Total Loss: 3.515920639038086 | KNN Loss: 2.4880728721618652 | BCE Loss: 1.0278478860855103\n",
      "Epoch 231 / 500 | iteration 10 / 30 | Total Loss: 3.4650511741638184 | KNN Loss: 2.4545440673828125 | BCE Loss: 1.0105072259902954\n",
      "Epoch 231 / 500 | iteration 15 / 30 | Total Loss: 3.5081934928894043 | KNN Loss: 2.4644784927368164 | BCE Loss: 1.0437148809432983\n",
      "Epoch 231 / 500 | iteration 20 / 30 | Total Loss: 3.41780424118042 | KNN Loss: 2.430419683456421 | BCE Loss: 0.987384557723999\n",
      "Epoch 231 / 500 | iteration 25 / 30 | Total Loss: 3.4672834873199463 | KNN Loss: 2.4459140300750732 | BCE Loss: 1.021369457244873\n",
      "Epoch 232 / 500 | iteration 0 / 30 | Total Loss: 3.492194414138794 | KNN Loss: 2.480379581451416 | BCE Loss: 1.011814832687378\n",
      "Epoch 232 / 500 | iteration 5 / 30 | Total Loss: 3.4638662338256836 | KNN Loss: 2.453852891921997 | BCE Loss: 1.0100133419036865\n",
      "Epoch 232 / 500 | iteration 10 / 30 | Total Loss: 3.489903688430786 | KNN Loss: 2.4773266315460205 | BCE Loss: 1.0125770568847656\n",
      "Epoch 232 / 500 | iteration 15 / 30 | Total Loss: 3.4917266368865967 | KNN Loss: 2.4787180423736572 | BCE Loss: 1.0130085945129395\n",
      "Epoch 232 / 500 | iteration 20 / 30 | Total Loss: 3.494027614593506 | KNN Loss: 2.482020378112793 | BCE Loss: 1.0120073556900024\n",
      "Epoch 232 / 500 | iteration 25 / 30 | Total Loss: 3.472660779953003 | KNN Loss: 2.4684786796569824 | BCE Loss: 1.0041821002960205\n",
      "Epoch 233 / 500 | iteration 0 / 30 | Total Loss: 3.4656195640563965 | KNN Loss: 2.4613709449768066 | BCE Loss: 1.0042486190795898\n",
      "Epoch 233 / 500 | iteration 5 / 30 | Total Loss: 3.455258846282959 | KNN Loss: 2.4348762035369873 | BCE Loss: 1.0203826427459717\n",
      "Epoch 233 / 500 | iteration 10 / 30 | Total Loss: 3.508338451385498 | KNN Loss: 2.4955592155456543 | BCE Loss: 1.0127792358398438\n",
      "Epoch 233 / 500 | iteration 15 / 30 | Total Loss: 3.4703104496002197 | KNN Loss: 2.4605302810668945 | BCE Loss: 1.0097801685333252\n",
      "Epoch 233 / 500 | iteration 20 / 30 | Total Loss: 3.5072381496429443 | KNN Loss: 2.4731101989746094 | BCE Loss: 1.034127950668335\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 233 / 500 | iteration 25 / 30 | Total Loss: 3.4762868881225586 | KNN Loss: 2.4581706523895264 | BCE Loss: 1.0181162357330322\n",
      "Epoch 234 / 500 | iteration 0 / 30 | Total Loss: 3.47845721244812 | KNN Loss: 2.479384422302246 | BCE Loss: 0.9990727305412292\n",
      "Epoch 234 / 500 | iteration 5 / 30 | Total Loss: 3.484494686126709 | KNN Loss: 2.4609875679016113 | BCE Loss: 1.0235071182250977\n",
      "Epoch 234 / 500 | iteration 10 / 30 | Total Loss: 3.4246671199798584 | KNN Loss: 2.4542312622070312 | BCE Loss: 0.9704358577728271\n",
      "Epoch 234 / 500 | iteration 15 / 30 | Total Loss: 3.5096564292907715 | KNN Loss: 2.5033481121063232 | BCE Loss: 1.0063083171844482\n",
      "Epoch 234 / 500 | iteration 20 / 30 | Total Loss: 3.464242935180664 | KNN Loss: 2.4319450855255127 | BCE Loss: 1.0322977304458618\n",
      "Epoch 234 / 500 | iteration 25 / 30 | Total Loss: 3.4825663566589355 | KNN Loss: 2.4584901332855225 | BCE Loss: 1.0240763425827026\n",
      "Epoch 235 / 500 | iteration 0 / 30 | Total Loss: 3.4697084426879883 | KNN Loss: 2.4671332836151123 | BCE Loss: 1.0025752782821655\n",
      "Epoch 235 / 500 | iteration 5 / 30 | Total Loss: 3.4751288890838623 | KNN Loss: 2.4501535892486572 | BCE Loss: 1.024975299835205\n",
      "Epoch 235 / 500 | iteration 10 / 30 | Total Loss: 3.4741883277893066 | KNN Loss: 2.45883846282959 | BCE Loss: 1.0153498649597168\n",
      "Epoch 235 / 500 | iteration 15 / 30 | Total Loss: 3.4810688495635986 | KNN Loss: 2.4614429473876953 | BCE Loss: 1.0196259021759033\n",
      "Epoch 235 / 500 | iteration 20 / 30 | Total Loss: 3.454562187194824 | KNN Loss: 2.456334114074707 | BCE Loss: 0.9982280135154724\n",
      "Epoch 235 / 500 | iteration 25 / 30 | Total Loss: 3.471529245376587 | KNN Loss: 2.4610979557037354 | BCE Loss: 1.0104312896728516\n",
      "Epoch 236 / 500 | iteration 0 / 30 | Total Loss: 3.4578731060028076 | KNN Loss: 2.448385715484619 | BCE Loss: 1.0094873905181885\n",
      "Epoch 236 / 500 | iteration 5 / 30 | Total Loss: 3.470318555831909 | KNN Loss: 2.466402530670166 | BCE Loss: 1.0039160251617432\n",
      "Epoch 236 / 500 | iteration 10 / 30 | Total Loss: 3.516935110092163 | KNN Loss: 2.4799141883850098 | BCE Loss: 1.0370209217071533\n",
      "Epoch 236 / 500 | iteration 15 / 30 | Total Loss: 3.48368501663208 | KNN Loss: 2.4731345176696777 | BCE Loss: 1.0105504989624023\n",
      "Epoch 236 / 500 | iteration 20 / 30 | Total Loss: 3.4822499752044678 | KNN Loss: 2.4596784114837646 | BCE Loss: 1.0225715637207031\n",
      "Epoch 236 / 500 | iteration 25 / 30 | Total Loss: 3.4830737113952637 | KNN Loss: 2.442300319671631 | BCE Loss: 1.0407732725143433\n",
      "Epoch 237 / 500 | iteration 0 / 30 | Total Loss: 3.5215303897857666 | KNN Loss: 2.4889614582061768 | BCE Loss: 1.0325689315795898\n",
      "Epoch 237 / 500 | iteration 5 / 30 | Total Loss: 3.4580321311950684 | KNN Loss: 2.442267417907715 | BCE Loss: 1.0157647132873535\n",
      "Epoch 237 / 500 | iteration 10 / 30 | Total Loss: 3.470897912979126 | KNN Loss: 2.473966598510742 | BCE Loss: 0.996931254863739\n",
      "Epoch 237 / 500 | iteration 15 / 30 | Total Loss: 3.435668706893921 | KNN Loss: 2.4252426624298096 | BCE Loss: 1.0104260444641113\n",
      "Epoch 237 / 500 | iteration 20 / 30 | Total Loss: 3.471740245819092 | KNN Loss: 2.477041721343994 | BCE Loss: 0.9946986436843872\n",
      "Epoch 237 / 500 | iteration 25 / 30 | Total Loss: 3.4837448596954346 | KNN Loss: 2.451777458190918 | BCE Loss: 1.0319674015045166\n",
      "Epoch 238 / 500 | iteration 0 / 30 | Total Loss: 3.467637062072754 | KNN Loss: 2.439802646636963 | BCE Loss: 1.0278342962265015\n",
      "Epoch 238 / 500 | iteration 5 / 30 | Total Loss: 3.470028877258301 | KNN Loss: 2.4783029556274414 | BCE Loss: 0.9917259216308594\n",
      "Epoch 238 / 500 | iteration 10 / 30 | Total Loss: 3.4963018894195557 | KNN Loss: 2.4542288780212402 | BCE Loss: 1.0420730113983154\n",
      "Epoch 238 / 500 | iteration 15 / 30 | Total Loss: 3.4947242736816406 | KNN Loss: 2.4615437984466553 | BCE Loss: 1.0331804752349854\n",
      "Epoch 238 / 500 | iteration 20 / 30 | Total Loss: 3.453329563140869 | KNN Loss: 2.4613406658172607 | BCE Loss: 0.9919888973236084\n",
      "Epoch 238 / 500 | iteration 25 / 30 | Total Loss: 3.4810779094696045 | KNN Loss: 2.4757156372070312 | BCE Loss: 1.0053622722625732\n",
      "Epoch 239 / 500 | iteration 0 / 30 | Total Loss: 3.5064752101898193 | KNN Loss: 2.4667234420776367 | BCE Loss: 1.0397517681121826\n",
      "Epoch 239 / 500 | iteration 5 / 30 | Total Loss: 3.472712993621826 | KNN Loss: 2.468392848968506 | BCE Loss: 1.0043202638626099\n",
      "Epoch 239 / 500 | iteration 10 / 30 | Total Loss: 3.504809617996216 | KNN Loss: 2.4641146659851074 | BCE Loss: 1.0406949520111084\n",
      "Epoch 239 / 500 | iteration 15 / 30 | Total Loss: 3.4521002769470215 | KNN Loss: 2.464967966079712 | BCE Loss: 0.98713219165802\n",
      "Epoch 239 / 500 | iteration 20 / 30 | Total Loss: 3.4640235900878906 | KNN Loss: 2.4515981674194336 | BCE Loss: 1.012425422668457\n",
      "Epoch 239 / 500 | iteration 25 / 30 | Total Loss: 3.4927990436553955 | KNN Loss: 2.4641575813293457 | BCE Loss: 1.0286414623260498\n",
      "Epoch 240 / 500 | iteration 0 / 30 | Total Loss: 3.5019803047180176 | KNN Loss: 2.4621596336364746 | BCE Loss: 1.039820671081543\n",
      "Epoch 240 / 500 | iteration 5 / 30 | Total Loss: 3.454549551010132 | KNN Loss: 2.4409613609313965 | BCE Loss: 1.0135881900787354\n",
      "Epoch 240 / 500 | iteration 10 / 30 | Total Loss: 3.4514007568359375 | KNN Loss: 2.4423987865448 | BCE Loss: 1.0090020895004272\n",
      "Epoch 240 / 500 | iteration 15 / 30 | Total Loss: 3.4669675827026367 | KNN Loss: 2.467907428741455 | BCE Loss: 0.9990602731704712\n",
      "Epoch 240 / 500 | iteration 20 / 30 | Total Loss: 3.485568046569824 | KNN Loss: 2.471226453781128 | BCE Loss: 1.0143415927886963\n",
      "Epoch 240 / 500 | iteration 25 / 30 | Total Loss: 3.4751410484313965 | KNN Loss: 2.454322576522827 | BCE Loss: 1.0208184719085693\n",
      "Epoch 241 / 500 | iteration 0 / 30 | Total Loss: 3.457322597503662 | KNN Loss: 2.460503578186035 | BCE Loss: 0.9968190789222717\n",
      "Epoch 241 / 500 | iteration 5 / 30 | Total Loss: 3.44891095161438 | KNN Loss: 2.445204019546509 | BCE Loss: 1.003706932067871\n",
      "Epoch 241 / 500 | iteration 10 / 30 | Total Loss: 3.4615845680236816 | KNN Loss: 2.4552218914031982 | BCE Loss: 1.0063626766204834\n",
      "Epoch 241 / 500 | iteration 15 / 30 | Total Loss: 3.4986283779144287 | KNN Loss: 2.467287302017212 | BCE Loss: 1.0313410758972168\n",
      "Epoch 241 / 500 | iteration 20 / 30 | Total Loss: 3.531489372253418 | KNN Loss: 2.4747872352600098 | BCE Loss: 1.0567021369934082\n",
      "Epoch 241 / 500 | iteration 25 / 30 | Total Loss: 3.504983425140381 | KNN Loss: 2.4710912704467773 | BCE Loss: 1.0338921546936035\n",
      "Epoch 242 / 500 | iteration 0 / 30 | Total Loss: 3.543531656265259 | KNN Loss: 2.4966912269592285 | BCE Loss: 1.0468404293060303\n",
      "Epoch 242 / 500 | iteration 5 / 30 | Total Loss: 3.4903621673583984 | KNN Loss: 2.4639875888824463 | BCE Loss: 1.0263744592666626\n",
      "Epoch 242 / 500 | iteration 10 / 30 | Total Loss: 3.4512014389038086 | KNN Loss: 2.474510431289673 | BCE Loss: 0.9766910672187805\n",
      "Epoch 242 / 500 | iteration 15 / 30 | Total Loss: 3.460444927215576 | KNN Loss: 2.4598851203918457 | BCE Loss: 1.000559687614441\n",
      "Epoch 242 / 500 | iteration 20 / 30 | Total Loss: 3.4787471294403076 | KNN Loss: 2.4627685546875 | BCE Loss: 1.0159785747528076\n",
      "Epoch 242 / 500 | iteration 25 / 30 | Total Loss: 3.491947650909424 | KNN Loss: 2.4749834537506104 | BCE Loss: 1.016964077949524\n",
      "Epoch 243 / 500 | iteration 0 / 30 | Total Loss: 3.4838662147521973 | KNN Loss: 2.4472947120666504 | BCE Loss: 1.0365715026855469\n",
      "Epoch 243 / 500 | iteration 5 / 30 | Total Loss: 3.477698802947998 | KNN Loss: 2.4675986766815186 | BCE Loss: 1.0101001262664795\n",
      "Epoch 243 / 500 | iteration 10 / 30 | Total Loss: 3.449538230895996 | KNN Loss: 2.4558796882629395 | BCE Loss: 0.9936585426330566\n",
      "Epoch 243 / 500 | iteration 15 / 30 | Total Loss: 3.5204379558563232 | KNN Loss: 2.4929211139678955 | BCE Loss: 1.0275168418884277\n",
      "Epoch 243 / 500 | iteration 20 / 30 | Total Loss: 3.505610942840576 | KNN Loss: 2.470865488052368 | BCE Loss: 1.034745454788208\n",
      "Epoch 243 / 500 | iteration 25 / 30 | Total Loss: 3.446357011795044 | KNN Loss: 2.458242654800415 | BCE Loss: 0.9881143569946289\n",
      "Epoch 244 / 500 | iteration 0 / 30 | Total Loss: 3.4847683906555176 | KNN Loss: 2.4627134799957275 | BCE Loss: 1.02205491065979\n",
      "Epoch 244 / 500 | iteration 5 / 30 | Total Loss: 3.4808590412139893 | KNN Loss: 2.4628143310546875 | BCE Loss: 1.0180447101593018\n",
      "Epoch 244 / 500 | iteration 10 / 30 | Total Loss: 3.480118989944458 | KNN Loss: 2.44669508934021 | BCE Loss: 1.033423900604248\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 244 / 500 | iteration 15 / 30 | Total Loss: 3.574016571044922 | KNN Loss: 2.5398333072662354 | BCE Loss: 1.034183144569397\n",
      "Epoch 244 / 500 | iteration 20 / 30 | Total Loss: 3.437544345855713 | KNN Loss: 2.460221290588379 | BCE Loss: 0.9773229956626892\n",
      "Epoch 244 / 500 | iteration 25 / 30 | Total Loss: 3.470226287841797 | KNN Loss: 2.4775280952453613 | BCE Loss: 0.9926981925964355\n",
      "Epoch 245 / 500 | iteration 0 / 30 | Total Loss: 3.4323129653930664 | KNN Loss: 2.4348177909851074 | BCE Loss: 0.9974952936172485\n",
      "Epoch 245 / 500 | iteration 5 / 30 | Total Loss: 3.449394464492798 | KNN Loss: 2.451300621032715 | BCE Loss: 0.9980939030647278\n",
      "Epoch 245 / 500 | iteration 10 / 30 | Total Loss: 3.4762632846832275 | KNN Loss: 2.457416534423828 | BCE Loss: 1.0188467502593994\n",
      "Epoch 245 / 500 | iteration 15 / 30 | Total Loss: 3.488162040710449 | KNN Loss: 2.459139823913574 | BCE Loss: 1.029022216796875\n",
      "Epoch 245 / 500 | iteration 20 / 30 | Total Loss: 3.5028584003448486 | KNN Loss: 2.4790284633636475 | BCE Loss: 1.0238299369812012\n",
      "Epoch 245 / 500 | iteration 25 / 30 | Total Loss: 3.473214626312256 | KNN Loss: 2.451803684234619 | BCE Loss: 1.0214108228683472\n",
      "Epoch 246 / 500 | iteration 0 / 30 | Total Loss: 3.4696602821350098 | KNN Loss: 2.463024139404297 | BCE Loss: 1.006636142730713\n",
      "Epoch 246 / 500 | iteration 5 / 30 | Total Loss: 3.4934613704681396 | KNN Loss: 2.465294599533081 | BCE Loss: 1.0281667709350586\n",
      "Epoch 246 / 500 | iteration 10 / 30 | Total Loss: 3.4713544845581055 | KNN Loss: 2.4604718685150146 | BCE Loss: 1.0108824968338013\n",
      "Epoch 246 / 500 | iteration 15 / 30 | Total Loss: 3.4744367599487305 | KNN Loss: 2.451204776763916 | BCE Loss: 1.023231863975525\n",
      "Epoch 246 / 500 | iteration 20 / 30 | Total Loss: 3.481975555419922 | KNN Loss: 2.4585442543029785 | BCE Loss: 1.0234311819076538\n",
      "Epoch 246 / 500 | iteration 25 / 30 | Total Loss: 3.5141842365264893 | KNN Loss: 2.4914908409118652 | BCE Loss: 1.022693395614624\n",
      "Epoch 247 / 500 | iteration 0 / 30 | Total Loss: 3.492709159851074 | KNN Loss: 2.4815380573272705 | BCE Loss: 1.0111712217330933\n",
      "Epoch 247 / 500 | iteration 5 / 30 | Total Loss: 3.5141077041625977 | KNN Loss: 2.491687059402466 | BCE Loss: 1.0224206447601318\n",
      "Epoch 247 / 500 | iteration 10 / 30 | Total Loss: 3.491555690765381 | KNN Loss: 2.4494941234588623 | BCE Loss: 1.042061686515808\n",
      "Epoch 247 / 500 | iteration 15 / 30 | Total Loss: 3.443225622177124 | KNN Loss: 2.4581398963928223 | BCE Loss: 0.9850857257843018\n",
      "Epoch 247 / 500 | iteration 20 / 30 | Total Loss: 3.4883041381835938 | KNN Loss: 2.470458745956421 | BCE Loss: 1.0178453922271729\n",
      "Epoch 247 / 500 | iteration 25 / 30 | Total Loss: 3.508676290512085 | KNN Loss: 2.476135492324829 | BCE Loss: 1.0325407981872559\n",
      "Epoch 248 / 500 | iteration 0 / 30 | Total Loss: 3.4672811031341553 | KNN Loss: 2.464878559112549 | BCE Loss: 1.0024025440216064\n",
      "Epoch 248 / 500 | iteration 5 / 30 | Total Loss: 3.4783425331115723 | KNN Loss: 2.4859979152679443 | BCE Loss: 0.9923446178436279\n",
      "Epoch 248 / 500 | iteration 10 / 30 | Total Loss: 3.471442699432373 | KNN Loss: 2.4878623485565186 | BCE Loss: 0.9835802912712097\n",
      "Epoch 248 / 500 | iteration 15 / 30 | Total Loss: 3.4856624603271484 | KNN Loss: 2.458219528198242 | BCE Loss: 1.0274430513381958\n",
      "Epoch 248 / 500 | iteration 20 / 30 | Total Loss: 3.4629669189453125 | KNN Loss: 2.4602012634277344 | BCE Loss: 1.0027657747268677\n",
      "Epoch 248 / 500 | iteration 25 / 30 | Total Loss: 3.512329339981079 | KNN Loss: 2.4701786041259766 | BCE Loss: 1.0421507358551025\n",
      "Epoch   249: reducing learning rate of group 0 to 4.1177e-04.\n",
      "Epoch 249 / 500 | iteration 0 / 30 | Total Loss: 3.516981601715088 | KNN Loss: 2.4606950283050537 | BCE Loss: 1.0562866926193237\n",
      "Epoch 249 / 500 | iteration 5 / 30 | Total Loss: 3.4223833084106445 | KNN Loss: 2.4415793418884277 | BCE Loss: 0.9808038473129272\n",
      "Epoch 249 / 500 | iteration 10 / 30 | Total Loss: 3.4920191764831543 | KNN Loss: 2.458315849304199 | BCE Loss: 1.033703327178955\n",
      "Epoch 249 / 500 | iteration 15 / 30 | Total Loss: 3.478689193725586 | KNN Loss: 2.447528839111328 | BCE Loss: 1.0311603546142578\n",
      "Epoch 249 / 500 | iteration 20 / 30 | Total Loss: 3.46054744720459 | KNN Loss: 2.4388368129730225 | BCE Loss: 1.0217106342315674\n",
      "Epoch 249 / 500 | iteration 25 / 30 | Total Loss: 3.492884635925293 | KNN Loss: 2.4818174839019775 | BCE Loss: 1.0110671520233154\n",
      "Epoch 250 / 500 | iteration 0 / 30 | Total Loss: 3.4485654830932617 | KNN Loss: 2.459454298019409 | BCE Loss: 0.9891111850738525\n",
      "Epoch 250 / 500 | iteration 5 / 30 | Total Loss: 3.5023231506347656 | KNN Loss: 2.4494080543518066 | BCE Loss: 1.052915096282959\n",
      "Epoch 250 / 500 | iteration 10 / 30 | Total Loss: 3.467081308364868 | KNN Loss: 2.463435173034668 | BCE Loss: 1.0036461353302002\n",
      "Epoch 250 / 500 | iteration 15 / 30 | Total Loss: 3.424260139465332 | KNN Loss: 2.435378074645996 | BCE Loss: 0.9888820648193359\n",
      "Epoch 250 / 500 | iteration 20 / 30 | Total Loss: 3.4818472862243652 | KNN Loss: 2.4533352851867676 | BCE Loss: 1.0285120010375977\n",
      "Epoch 250 / 500 | iteration 25 / 30 | Total Loss: 3.483227491378784 | KNN Loss: 2.479574680328369 | BCE Loss: 1.003652811050415\n",
      "Epoch 251 / 500 | iteration 0 / 30 | Total Loss: 3.5091476440429688 | KNN Loss: 2.483466863632202 | BCE Loss: 1.0256807804107666\n",
      "Epoch 251 / 500 | iteration 5 / 30 | Total Loss: 3.487678050994873 | KNN Loss: 2.4666013717651367 | BCE Loss: 1.0210765600204468\n",
      "Epoch 251 / 500 | iteration 10 / 30 | Total Loss: 3.491825580596924 | KNN Loss: 2.4447364807128906 | BCE Loss: 1.0470890998840332\n",
      "Epoch 251 / 500 | iteration 15 / 30 | Total Loss: 3.493014335632324 | KNN Loss: 2.4771268367767334 | BCE Loss: 1.0158874988555908\n",
      "Epoch 251 / 500 | iteration 20 / 30 | Total Loss: 3.4570727348327637 | KNN Loss: 2.440011501312256 | BCE Loss: 1.0170612335205078\n",
      "Epoch 251 / 500 | iteration 25 / 30 | Total Loss: 3.4925599098205566 | KNN Loss: 2.452160596847534 | BCE Loss: 1.0403993129730225\n",
      "Epoch 252 / 500 | iteration 0 / 30 | Total Loss: 3.4858012199401855 | KNN Loss: 2.444735527038574 | BCE Loss: 1.0410656929016113\n",
      "Epoch 252 / 500 | iteration 5 / 30 | Total Loss: 3.5061182975769043 | KNN Loss: 2.4742321968078613 | BCE Loss: 1.031886100769043\n",
      "Epoch 252 / 500 | iteration 10 / 30 | Total Loss: 3.459613084793091 | KNN Loss: 2.433499574661255 | BCE Loss: 1.026113510131836\n",
      "Epoch 252 / 500 | iteration 15 / 30 | Total Loss: 3.481553316116333 | KNN Loss: 2.48478102684021 | BCE Loss: 0.9967723488807678\n",
      "Epoch 252 / 500 | iteration 20 / 30 | Total Loss: 3.4738285541534424 | KNN Loss: 2.475459575653076 | BCE Loss: 0.9983689785003662\n",
      "Epoch 252 / 500 | iteration 25 / 30 | Total Loss: 3.453613042831421 | KNN Loss: 2.4441475868225098 | BCE Loss: 1.0094654560089111\n",
      "Epoch 253 / 500 | iteration 0 / 30 | Total Loss: 3.4558725357055664 | KNN Loss: 2.4525272846221924 | BCE Loss: 1.0033453702926636\n",
      "Epoch 253 / 500 | iteration 5 / 30 | Total Loss: 3.4680912494659424 | KNN Loss: 2.44920015335083 | BCE Loss: 1.0188910961151123\n",
      "Epoch 253 / 500 | iteration 10 / 30 | Total Loss: 3.5039830207824707 | KNN Loss: 2.48270583152771 | BCE Loss: 1.0212773084640503\n",
      "Epoch 253 / 500 | iteration 15 / 30 | Total Loss: 3.4765923023223877 | KNN Loss: 2.4497573375701904 | BCE Loss: 1.0268349647521973\n",
      "Epoch 253 / 500 | iteration 20 / 30 | Total Loss: 3.448333263397217 | KNN Loss: 2.433277130126953 | BCE Loss: 1.0150562524795532\n",
      "Epoch 253 / 500 | iteration 25 / 30 | Total Loss: 3.459200859069824 | KNN Loss: 2.4655866622924805 | BCE Loss: 0.9936141967773438\n",
      "Epoch 254 / 500 | iteration 0 / 30 | Total Loss: 3.4857289791107178 | KNN Loss: 2.470165729522705 | BCE Loss: 1.0155632495880127\n",
      "Epoch 254 / 500 | iteration 5 / 30 | Total Loss: 3.4469776153564453 | KNN Loss: 2.4372289180755615 | BCE Loss: 1.0097486972808838\n",
      "Epoch 254 / 500 | iteration 10 / 30 | Total Loss: 3.4679226875305176 | KNN Loss: 2.462677001953125 | BCE Loss: 1.0052456855773926\n",
      "Epoch 254 / 500 | iteration 15 / 30 | Total Loss: 3.5142650604248047 | KNN Loss: 2.489112615585327 | BCE Loss: 1.025152325630188\n",
      "Epoch 254 / 500 | iteration 20 / 30 | Total Loss: 3.464421272277832 | KNN Loss: 2.442767381668091 | BCE Loss: 1.0216538906097412\n",
      "Epoch 254 / 500 | iteration 25 / 30 | Total Loss: 3.521120071411133 | KNN Loss: 2.459672451019287 | BCE Loss: 1.0614476203918457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 255 / 500 | iteration 0 / 30 | Total Loss: 3.429410696029663 | KNN Loss: 2.442307233810425 | BCE Loss: 0.9871034026145935\n",
      "Epoch 255 / 500 | iteration 5 / 30 | Total Loss: 3.551013946533203 | KNN Loss: 2.4945268630981445 | BCE Loss: 1.0564870834350586\n",
      "Epoch 255 / 500 | iteration 10 / 30 | Total Loss: 3.4544155597686768 | KNN Loss: 2.4412341117858887 | BCE Loss: 1.013181447982788\n",
      "Epoch 255 / 500 | iteration 15 / 30 | Total Loss: 3.54347825050354 | KNN Loss: 2.5289134979248047 | BCE Loss: 1.0145647525787354\n",
      "Epoch 255 / 500 | iteration 20 / 30 | Total Loss: 3.4640920162200928 | KNN Loss: 2.4676883220672607 | BCE Loss: 0.996403694152832\n",
      "Epoch 255 / 500 | iteration 25 / 30 | Total Loss: 3.48828125 | KNN Loss: 2.468348979949951 | BCE Loss: 1.0199322700500488\n",
      "Epoch 256 / 500 | iteration 0 / 30 | Total Loss: 3.474470615386963 | KNN Loss: 2.4420266151428223 | BCE Loss: 1.032443881034851\n",
      "Epoch 256 / 500 | iteration 5 / 30 | Total Loss: 3.4802210330963135 | KNN Loss: 2.4563381671905518 | BCE Loss: 1.0238828659057617\n",
      "Epoch 256 / 500 | iteration 10 / 30 | Total Loss: 3.4436256885528564 | KNN Loss: 2.445539712905884 | BCE Loss: 0.9980859160423279\n",
      "Epoch 256 / 500 | iteration 15 / 30 | Total Loss: 3.4868171215057373 | KNN Loss: 2.4659483432769775 | BCE Loss: 1.0208687782287598\n",
      "Epoch 256 / 500 | iteration 20 / 30 | Total Loss: 3.495049476623535 | KNN Loss: 2.471060276031494 | BCE Loss: 1.0239893198013306\n",
      "Epoch 256 / 500 | iteration 25 / 30 | Total Loss: 3.4962117671966553 | KNN Loss: 2.472069025039673 | BCE Loss: 1.0241427421569824\n",
      "Epoch 257 / 500 | iteration 0 / 30 | Total Loss: 3.5244240760803223 | KNN Loss: 2.4904427528381348 | BCE Loss: 1.033981442451477\n",
      "Epoch 257 / 500 | iteration 5 / 30 | Total Loss: 3.496079444885254 | KNN Loss: 2.5003111362457275 | BCE Loss: 0.9957683086395264\n",
      "Epoch 257 / 500 | iteration 10 / 30 | Total Loss: 3.4555482864379883 | KNN Loss: 2.4372191429138184 | BCE Loss: 1.0183292627334595\n",
      "Epoch 257 / 500 | iteration 15 / 30 | Total Loss: 3.4613089561462402 | KNN Loss: 2.443263292312622 | BCE Loss: 1.0180456638336182\n",
      "Epoch 257 / 500 | iteration 20 / 30 | Total Loss: 3.4889097213745117 | KNN Loss: 2.4623169898986816 | BCE Loss: 1.02659273147583\n",
      "Epoch 257 / 500 | iteration 25 / 30 | Total Loss: 3.4701223373413086 | KNN Loss: 2.4374208450317383 | BCE Loss: 1.0327016115188599\n",
      "Epoch 258 / 500 | iteration 0 / 30 | Total Loss: 3.5132811069488525 | KNN Loss: 2.4739110469818115 | BCE Loss: 1.039370059967041\n",
      "Epoch 258 / 500 | iteration 5 / 30 | Total Loss: 3.4880619049072266 | KNN Loss: 2.4806125164031982 | BCE Loss: 1.0074492692947388\n",
      "Epoch 258 / 500 | iteration 10 / 30 | Total Loss: 3.4346656799316406 | KNN Loss: 2.4343504905700684 | BCE Loss: 1.0003151893615723\n",
      "Epoch 258 / 500 | iteration 15 / 30 | Total Loss: 3.446035385131836 | KNN Loss: 2.4463603496551514 | BCE Loss: 0.9996750950813293\n",
      "Epoch 258 / 500 | iteration 20 / 30 | Total Loss: 3.4510483741760254 | KNN Loss: 2.439640998840332 | BCE Loss: 1.0114073753356934\n",
      "Epoch 258 / 500 | iteration 25 / 30 | Total Loss: 3.4655203819274902 | KNN Loss: 2.4637138843536377 | BCE Loss: 1.001806616783142\n",
      "Epoch 259 / 500 | iteration 0 / 30 | Total Loss: 3.522190809249878 | KNN Loss: 2.4828460216522217 | BCE Loss: 1.0393447875976562\n",
      "Epoch 259 / 500 | iteration 5 / 30 | Total Loss: 3.4699549674987793 | KNN Loss: 2.480825901031494 | BCE Loss: 0.9891290068626404\n",
      "Epoch 259 / 500 | iteration 10 / 30 | Total Loss: 3.4760828018188477 | KNN Loss: 2.4572463035583496 | BCE Loss: 1.018836498260498\n",
      "Epoch 259 / 500 | iteration 15 / 30 | Total Loss: 3.4883852005004883 | KNN Loss: 2.4933624267578125 | BCE Loss: 0.9950228333473206\n",
      "Epoch 259 / 500 | iteration 20 / 30 | Total Loss: 3.4992873668670654 | KNN Loss: 2.4563112258911133 | BCE Loss: 1.0429761409759521\n",
      "Epoch 259 / 500 | iteration 25 / 30 | Total Loss: 3.4873409271240234 | KNN Loss: 2.443727731704712 | BCE Loss: 1.0436131954193115\n",
      "Epoch 260 / 500 | iteration 0 / 30 | Total Loss: 3.4839892387390137 | KNN Loss: 2.46394419670105 | BCE Loss: 1.0200450420379639\n",
      "Epoch 260 / 500 | iteration 5 / 30 | Total Loss: 3.4590139389038086 | KNN Loss: 2.4470486640930176 | BCE Loss: 1.0119651556015015\n",
      "Epoch 260 / 500 | iteration 10 / 30 | Total Loss: 3.462717294692993 | KNN Loss: 2.4426393508911133 | BCE Loss: 1.0200779438018799\n",
      "Epoch 260 / 500 | iteration 15 / 30 | Total Loss: 3.4586973190307617 | KNN Loss: 2.4430651664733887 | BCE Loss: 1.015632152557373\n",
      "Epoch 260 / 500 | iteration 20 / 30 | Total Loss: 3.488497495651245 | KNN Loss: 2.4591915607452393 | BCE Loss: 1.0293059349060059\n",
      "Epoch 260 / 500 | iteration 25 / 30 | Total Loss: 3.484020233154297 | KNN Loss: 2.4459147453308105 | BCE Loss: 1.0381054878234863\n",
      "Epoch   261: reducing learning rate of group 0 to 2.8824e-04.\n",
      "Epoch 261 / 500 | iteration 0 / 30 | Total Loss: 3.47487211227417 | KNN Loss: 2.454803943634033 | BCE Loss: 1.0200681686401367\n",
      "Epoch 261 / 500 | iteration 5 / 30 | Total Loss: 3.445232391357422 | KNN Loss: 2.4434525966644287 | BCE Loss: 1.0017796754837036\n",
      "Epoch 261 / 500 | iteration 10 / 30 | Total Loss: 3.4425768852233887 | KNN Loss: 2.450683355331421 | BCE Loss: 0.9918935298919678\n",
      "Epoch 261 / 500 | iteration 15 / 30 | Total Loss: 3.4974374771118164 | KNN Loss: 2.4857239723205566 | BCE Loss: 1.0117136240005493\n",
      "Epoch 261 / 500 | iteration 20 / 30 | Total Loss: 3.487701416015625 | KNN Loss: 2.4679062366485596 | BCE Loss: 1.019795298576355\n",
      "Epoch 261 / 500 | iteration 25 / 30 | Total Loss: 3.50579833984375 | KNN Loss: 2.4762730598449707 | BCE Loss: 1.0295251607894897\n",
      "Epoch 262 / 500 | iteration 0 / 30 | Total Loss: 3.4686331748962402 | KNN Loss: 2.4502618312835693 | BCE Loss: 1.0183712244033813\n",
      "Epoch 262 / 500 | iteration 5 / 30 | Total Loss: 3.4704439640045166 | KNN Loss: 2.4636974334716797 | BCE Loss: 1.006746530532837\n",
      "Epoch 262 / 500 | iteration 10 / 30 | Total Loss: 3.472400188446045 | KNN Loss: 2.4412219524383545 | BCE Loss: 1.03117835521698\n",
      "Epoch 262 / 500 | iteration 15 / 30 | Total Loss: 3.473323345184326 | KNN Loss: 2.4711437225341797 | BCE Loss: 1.002179741859436\n",
      "Epoch 262 / 500 | iteration 20 / 30 | Total Loss: 3.483433246612549 | KNN Loss: 2.4641568660736084 | BCE Loss: 1.0192763805389404\n",
      "Epoch 262 / 500 | iteration 25 / 30 | Total Loss: 3.444852828979492 | KNN Loss: 2.4556214809417725 | BCE Loss: 0.9892312288284302\n",
      "Epoch 263 / 500 | iteration 0 / 30 | Total Loss: 3.4864563941955566 | KNN Loss: 2.478043794631958 | BCE Loss: 1.008412480354309\n",
      "Epoch 263 / 500 | iteration 5 / 30 | Total Loss: 3.489811897277832 | KNN Loss: 2.4751086235046387 | BCE Loss: 1.014703392982483\n",
      "Epoch 263 / 500 | iteration 10 / 30 | Total Loss: 3.494547128677368 | KNN Loss: 2.48158597946167 | BCE Loss: 1.0129611492156982\n",
      "Epoch 263 / 500 | iteration 15 / 30 | Total Loss: 3.4826488494873047 | KNN Loss: 2.4507546424865723 | BCE Loss: 1.0318942070007324\n",
      "Epoch 263 / 500 | iteration 20 / 30 | Total Loss: 3.5232081413269043 | KNN Loss: 2.4840879440307617 | BCE Loss: 1.0391203165054321\n",
      "Epoch 263 / 500 | iteration 25 / 30 | Total Loss: 3.488154649734497 | KNN Loss: 2.452296257019043 | BCE Loss: 1.035858392715454\n",
      "Epoch 264 / 500 | iteration 0 / 30 | Total Loss: 3.4981155395507812 | KNN Loss: 2.4733870029449463 | BCE Loss: 1.024728536605835\n",
      "Epoch 264 / 500 | iteration 5 / 30 | Total Loss: 3.5103564262390137 | KNN Loss: 2.4937000274658203 | BCE Loss: 1.0166563987731934\n",
      "Epoch 264 / 500 | iteration 10 / 30 | Total Loss: 3.4382073879241943 | KNN Loss: 2.4338088035583496 | BCE Loss: 1.0043985843658447\n",
      "Epoch 264 / 500 | iteration 15 / 30 | Total Loss: 3.463669538497925 | KNN Loss: 2.4238526821136475 | BCE Loss: 1.0398168563842773\n",
      "Epoch 264 / 500 | iteration 20 / 30 | Total Loss: 3.4835400581359863 | KNN Loss: 2.4708261489868164 | BCE Loss: 1.01271390914917\n",
      "Epoch 264 / 500 | iteration 25 / 30 | Total Loss: 3.464181423187256 | KNN Loss: 2.4514918327331543 | BCE Loss: 1.012689471244812\n",
      "Epoch 265 / 500 | iteration 0 / 30 | Total Loss: 3.4835357666015625 | KNN Loss: 2.4457573890686035 | BCE Loss: 1.037778377532959\n",
      "Epoch 265 / 500 | iteration 5 / 30 | Total Loss: 3.464489698410034 | KNN Loss: 2.478034019470215 | BCE Loss: 0.9864557385444641\n",
      "Epoch 265 / 500 | iteration 10 / 30 | Total Loss: 3.460136651992798 | KNN Loss: 2.4437689781188965 | BCE Loss: 1.0163676738739014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 265 / 500 | iteration 15 / 30 | Total Loss: 3.4921107292175293 | KNN Loss: 2.4892146587371826 | BCE Loss: 1.0028961896896362\n",
      "Epoch 265 / 500 | iteration 20 / 30 | Total Loss: 3.4874446392059326 | KNN Loss: 2.452831983566284 | BCE Loss: 1.0346126556396484\n",
      "Epoch 265 / 500 | iteration 25 / 30 | Total Loss: 3.4669370651245117 | KNN Loss: 2.456583023071289 | BCE Loss: 1.0103540420532227\n",
      "Epoch 266 / 500 | iteration 0 / 30 | Total Loss: 3.451132297515869 | KNN Loss: 2.461760997772217 | BCE Loss: 0.9893711805343628\n",
      "Epoch 266 / 500 | iteration 5 / 30 | Total Loss: 3.4446444511413574 | KNN Loss: 2.4657046794891357 | BCE Loss: 0.9789396524429321\n",
      "Epoch 266 / 500 | iteration 10 / 30 | Total Loss: 3.482452869415283 | KNN Loss: 2.4561593532562256 | BCE Loss: 1.0262935161590576\n",
      "Epoch 266 / 500 | iteration 15 / 30 | Total Loss: 3.453648090362549 | KNN Loss: 2.429795503616333 | BCE Loss: 1.0238525867462158\n",
      "Epoch 266 / 500 | iteration 20 / 30 | Total Loss: 3.472653865814209 | KNN Loss: 2.4637365341186523 | BCE Loss: 1.0089173316955566\n",
      "Epoch 266 / 500 | iteration 25 / 30 | Total Loss: 3.4535531997680664 | KNN Loss: 2.438755750656128 | BCE Loss: 1.014797568321228\n",
      "Epoch 267 / 500 | iteration 0 / 30 | Total Loss: 3.4687650203704834 | KNN Loss: 2.4506545066833496 | BCE Loss: 1.0181105136871338\n",
      "Epoch 267 / 500 | iteration 5 / 30 | Total Loss: 3.4580893516540527 | KNN Loss: 2.4538655281066895 | BCE Loss: 1.0042239427566528\n",
      "Epoch 267 / 500 | iteration 10 / 30 | Total Loss: 3.4470019340515137 | KNN Loss: 2.430060386657715 | BCE Loss: 1.0169415473937988\n",
      "Epoch 267 / 500 | iteration 15 / 30 | Total Loss: 3.4787869453430176 | KNN Loss: 2.46651554107666 | BCE Loss: 1.0122714042663574\n",
      "Epoch 267 / 500 | iteration 20 / 30 | Total Loss: 3.4972314834594727 | KNN Loss: 2.466080904006958 | BCE Loss: 1.0311506986618042\n",
      "Epoch 267 / 500 | iteration 25 / 30 | Total Loss: 3.5235655307769775 | KNN Loss: 2.472364902496338 | BCE Loss: 1.0512006282806396\n",
      "Epoch 268 / 500 | iteration 0 / 30 | Total Loss: 3.4586031436920166 | KNN Loss: 2.454179286956787 | BCE Loss: 1.0044238567352295\n",
      "Epoch 268 / 500 | iteration 5 / 30 | Total Loss: 3.4671642780303955 | KNN Loss: 2.4339022636413574 | BCE Loss: 1.033262014389038\n",
      "Epoch 268 / 500 | iteration 10 / 30 | Total Loss: 3.4601104259490967 | KNN Loss: 2.4525341987609863 | BCE Loss: 1.0075762271881104\n",
      "Epoch 268 / 500 | iteration 15 / 30 | Total Loss: 3.4829249382019043 | KNN Loss: 2.4545466899871826 | BCE Loss: 1.0283781290054321\n",
      "Epoch 268 / 500 | iteration 20 / 30 | Total Loss: 3.460226535797119 | KNN Loss: 2.4642107486724854 | BCE Loss: 0.9960156679153442\n",
      "Epoch 268 / 500 | iteration 25 / 30 | Total Loss: 3.488359212875366 | KNN Loss: 2.46840238571167 | BCE Loss: 1.0199568271636963\n",
      "Epoch 269 / 500 | iteration 0 / 30 | Total Loss: 3.4928884506225586 | KNN Loss: 2.4596142768859863 | BCE Loss: 1.0332741737365723\n",
      "Epoch 269 / 500 | iteration 5 / 30 | Total Loss: 3.464852809906006 | KNN Loss: 2.434526205062866 | BCE Loss: 1.0303266048431396\n",
      "Epoch 269 / 500 | iteration 10 / 30 | Total Loss: 3.458878517150879 | KNN Loss: 2.447355031967163 | BCE Loss: 1.0115236043930054\n",
      "Epoch 269 / 500 | iteration 15 / 30 | Total Loss: 3.4630465507507324 | KNN Loss: 2.449946403503418 | BCE Loss: 1.013100028038025\n",
      "Epoch 269 / 500 | iteration 20 / 30 | Total Loss: 3.5089616775512695 | KNN Loss: 2.4977500438690186 | BCE Loss: 1.0112115144729614\n",
      "Epoch 269 / 500 | iteration 25 / 30 | Total Loss: 3.456254005432129 | KNN Loss: 2.4264774322509766 | BCE Loss: 1.0297765731811523\n",
      "Epoch 270 / 500 | iteration 0 / 30 | Total Loss: 3.4794373512268066 | KNN Loss: 2.4770119190216064 | BCE Loss: 1.0024255514144897\n",
      "Epoch 270 / 500 | iteration 5 / 30 | Total Loss: 3.461697816848755 | KNN Loss: 2.447936534881592 | BCE Loss: 1.013761281967163\n",
      "Epoch 270 / 500 | iteration 10 / 30 | Total Loss: 3.4556360244750977 | KNN Loss: 2.437105894088745 | BCE Loss: 1.0185301303863525\n",
      "Epoch 270 / 500 | iteration 15 / 30 | Total Loss: 3.4822545051574707 | KNN Loss: 2.458547830581665 | BCE Loss: 1.0237066745758057\n",
      "Epoch 270 / 500 | iteration 20 / 30 | Total Loss: 3.477005958557129 | KNN Loss: 2.4447362422943115 | BCE Loss: 1.0322697162628174\n",
      "Epoch 270 / 500 | iteration 25 / 30 | Total Loss: 3.471839189529419 | KNN Loss: 2.451775312423706 | BCE Loss: 1.020063877105713\n",
      "Epoch 271 / 500 | iteration 0 / 30 | Total Loss: 3.534186363220215 | KNN Loss: 2.492105484008789 | BCE Loss: 1.0420809984207153\n",
      "Epoch 271 / 500 | iteration 5 / 30 | Total Loss: 3.4579732418060303 | KNN Loss: 2.451582193374634 | BCE Loss: 1.0063910484313965\n",
      "Epoch 271 / 500 | iteration 10 / 30 | Total Loss: 3.486222982406616 | KNN Loss: 2.4516420364379883 | BCE Loss: 1.034580945968628\n",
      "Epoch 271 / 500 | iteration 15 / 30 | Total Loss: 3.472839117050171 | KNN Loss: 2.479877471923828 | BCE Loss: 0.9929616451263428\n",
      "Epoch 271 / 500 | iteration 20 / 30 | Total Loss: 3.470545768737793 | KNN Loss: 2.4578397274017334 | BCE Loss: 1.0127060413360596\n",
      "Epoch 271 / 500 | iteration 25 / 30 | Total Loss: 3.4687962532043457 | KNN Loss: 2.4545342922210693 | BCE Loss: 1.0142619609832764\n",
      "Epoch 272 / 500 | iteration 0 / 30 | Total Loss: 3.4675259590148926 | KNN Loss: 2.4545516967773438 | BCE Loss: 1.0129743814468384\n",
      "Epoch 272 / 500 | iteration 5 / 30 | Total Loss: 3.460132598876953 | KNN Loss: 2.449207305908203 | BCE Loss: 1.01092529296875\n",
      "Epoch 272 / 500 | iteration 10 / 30 | Total Loss: 3.490665912628174 | KNN Loss: 2.4590141773223877 | BCE Loss: 1.0316517353057861\n",
      "Epoch 272 / 500 | iteration 15 / 30 | Total Loss: 3.4715428352355957 | KNN Loss: 2.487666368484497 | BCE Loss: 0.9838763475418091\n",
      "Epoch 272 / 500 | iteration 20 / 30 | Total Loss: 3.468916416168213 | KNN Loss: 2.4560141563415527 | BCE Loss: 1.0129022598266602\n",
      "Epoch 272 / 500 | iteration 25 / 30 | Total Loss: 3.4723918437957764 | KNN Loss: 2.443516969680786 | BCE Loss: 1.0288748741149902\n",
      "Epoch 273 / 500 | iteration 0 / 30 | Total Loss: 3.47019624710083 | KNN Loss: 2.4754457473754883 | BCE Loss: 0.9947506189346313\n",
      "Epoch 273 / 500 | iteration 5 / 30 | Total Loss: 3.517941951751709 | KNN Loss: 2.486539125442505 | BCE Loss: 1.031402826309204\n",
      "Epoch 273 / 500 | iteration 10 / 30 | Total Loss: 3.465834617614746 | KNN Loss: 2.453664541244507 | BCE Loss: 1.0121700763702393\n",
      "Epoch 273 / 500 | iteration 15 / 30 | Total Loss: 3.473125696182251 | KNN Loss: 2.4599404335021973 | BCE Loss: 1.0131852626800537\n",
      "Epoch 273 / 500 | iteration 20 / 30 | Total Loss: 3.520022392272949 | KNN Loss: 2.490626096725464 | BCE Loss: 1.029396414756775\n",
      "Epoch 273 / 500 | iteration 25 / 30 | Total Loss: 3.483025074005127 | KNN Loss: 2.465275526046753 | BCE Loss: 1.017749547958374\n",
      "Epoch 274 / 500 | iteration 0 / 30 | Total Loss: 3.4934685230255127 | KNN Loss: 2.468174457550049 | BCE Loss: 1.0252940654754639\n",
      "Epoch 274 / 500 | iteration 5 / 30 | Total Loss: 3.463547706604004 | KNN Loss: 2.4317076206207275 | BCE Loss: 1.0318399667739868\n",
      "Epoch 274 / 500 | iteration 10 / 30 | Total Loss: 3.5158534049987793 | KNN Loss: 2.4869794845581055 | BCE Loss: 1.0288739204406738\n",
      "Epoch 274 / 500 | iteration 15 / 30 | Total Loss: 3.4901185035705566 | KNN Loss: 2.4819893836975098 | BCE Loss: 1.0081291198730469\n",
      "Epoch 274 / 500 | iteration 20 / 30 | Total Loss: 3.454705238342285 | KNN Loss: 2.4425809383392334 | BCE Loss: 1.0121241807937622\n",
      "Epoch 274 / 500 | iteration 25 / 30 | Total Loss: 3.48399019241333 | KNN Loss: 2.468456983566284 | BCE Loss: 1.015533208847046\n",
      "Epoch 275 / 500 | iteration 0 / 30 | Total Loss: 3.43911075592041 | KNN Loss: 2.4346184730529785 | BCE Loss: 1.0044922828674316\n",
      "Epoch 275 / 500 | iteration 5 / 30 | Total Loss: 3.5067553520202637 | KNN Loss: 2.46712064743042 | BCE Loss: 1.0396348237991333\n",
      "Epoch 275 / 500 | iteration 10 / 30 | Total Loss: 3.485894203186035 | KNN Loss: 2.46624755859375 | BCE Loss: 1.0196466445922852\n",
      "Epoch 275 / 500 | iteration 15 / 30 | Total Loss: 3.5110225677490234 | KNN Loss: 2.4380533695220947 | BCE Loss: 1.0729691982269287\n",
      "Epoch 275 / 500 | iteration 20 / 30 | Total Loss: 3.473141670227051 | KNN Loss: 2.4655520915985107 | BCE Loss: 1.0075896978378296\n",
      "Epoch 275 / 500 | iteration 25 / 30 | Total Loss: 3.464038848876953 | KNN Loss: 2.431002616882324 | BCE Loss: 1.0330363512039185\n",
      "Epoch 276 / 500 | iteration 0 / 30 | Total Loss: 3.468545913696289 | KNN Loss: 2.445345878601074 | BCE Loss: 1.0231999158859253\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 276 / 500 | iteration 5 / 30 | Total Loss: 3.510298728942871 | KNN Loss: 2.470303773880005 | BCE Loss: 1.0399950742721558\n",
      "Epoch 276 / 500 | iteration 10 / 30 | Total Loss: 3.5128023624420166 | KNN Loss: 2.480029582977295 | BCE Loss: 1.0327727794647217\n",
      "Epoch 276 / 500 | iteration 15 / 30 | Total Loss: 3.441740036010742 | KNN Loss: 2.4424004554748535 | BCE Loss: 0.9993396401405334\n",
      "Epoch 276 / 500 | iteration 20 / 30 | Total Loss: 3.4356942176818848 | KNN Loss: 2.4335148334503174 | BCE Loss: 1.0021793842315674\n",
      "Epoch 276 / 500 | iteration 25 / 30 | Total Loss: 3.4870920181274414 | KNN Loss: 2.454796552658081 | BCE Loss: 1.03229558467865\n",
      "Epoch 277 / 500 | iteration 0 / 30 | Total Loss: 3.447707176208496 | KNN Loss: 2.4312632083892822 | BCE Loss: 1.0164440870285034\n",
      "Epoch 277 / 500 | iteration 5 / 30 | Total Loss: 3.5148987770080566 | KNN Loss: 2.5022695064544678 | BCE Loss: 1.0126291513442993\n",
      "Epoch 277 / 500 | iteration 10 / 30 | Total Loss: 3.424781560897827 | KNN Loss: 2.4106760025024414 | BCE Loss: 1.0141055583953857\n",
      "Epoch 277 / 500 | iteration 15 / 30 | Total Loss: 3.4779958724975586 | KNN Loss: 2.463000535964966 | BCE Loss: 1.0149953365325928\n",
      "Epoch 277 / 500 | iteration 20 / 30 | Total Loss: 3.4687891006469727 | KNN Loss: 2.4502720832824707 | BCE Loss: 1.018517017364502\n",
      "Epoch 277 / 500 | iteration 25 / 30 | Total Loss: 3.491039514541626 | KNN Loss: 2.4693987369537354 | BCE Loss: 1.0216407775878906\n",
      "Epoch 278 / 500 | iteration 0 / 30 | Total Loss: 3.488046884536743 | KNN Loss: 2.449378252029419 | BCE Loss: 1.0386686325073242\n",
      "Epoch 278 / 500 | iteration 5 / 30 | Total Loss: 3.4517579078674316 | KNN Loss: 2.4467978477478027 | BCE Loss: 1.004960060119629\n",
      "Epoch 278 / 500 | iteration 10 / 30 | Total Loss: 3.4486770629882812 | KNN Loss: 2.437925338745117 | BCE Loss: 1.0107516050338745\n",
      "Epoch 278 / 500 | iteration 15 / 30 | Total Loss: 3.4693827629089355 | KNN Loss: 2.44014835357666 | BCE Loss: 1.0292344093322754\n",
      "Epoch 278 / 500 | iteration 20 / 30 | Total Loss: 3.4138007164001465 | KNN Loss: 2.4460434913635254 | BCE Loss: 0.9677572250366211\n",
      "Epoch 278 / 500 | iteration 25 / 30 | Total Loss: 3.4678633213043213 | KNN Loss: 2.461359739303589 | BCE Loss: 1.0065035820007324\n",
      "Epoch 279 / 500 | iteration 0 / 30 | Total Loss: 3.4518678188323975 | KNN Loss: 2.4595675468444824 | BCE Loss: 0.9923002123832703\n",
      "Epoch 279 / 500 | iteration 5 / 30 | Total Loss: 3.502201795578003 | KNN Loss: 2.46091890335083 | BCE Loss: 1.0412828922271729\n",
      "Epoch 279 / 500 | iteration 10 / 30 | Total Loss: 3.4727742671966553 | KNN Loss: 2.455223560333252 | BCE Loss: 1.0175507068634033\n",
      "Epoch 279 / 500 | iteration 15 / 30 | Total Loss: 3.483107805252075 | KNN Loss: 2.4605696201324463 | BCE Loss: 1.022538185119629\n",
      "Epoch 279 / 500 | iteration 20 / 30 | Total Loss: 3.4849894046783447 | KNN Loss: 2.477806329727173 | BCE Loss: 1.0071830749511719\n",
      "Epoch 279 / 500 | iteration 25 / 30 | Total Loss: 3.523214817047119 | KNN Loss: 2.497749090194702 | BCE Loss: 1.025465726852417\n",
      "Epoch 280 / 500 | iteration 0 / 30 | Total Loss: 3.4707014560699463 | KNN Loss: 2.458686351776123 | BCE Loss: 1.0120151042938232\n",
      "Epoch 280 / 500 | iteration 5 / 30 | Total Loss: 3.498656749725342 | KNN Loss: 2.4671590328216553 | BCE Loss: 1.0314977169036865\n",
      "Epoch 280 / 500 | iteration 10 / 30 | Total Loss: 3.520778179168701 | KNN Loss: 2.4986934661865234 | BCE Loss: 1.0220848321914673\n",
      "Epoch 280 / 500 | iteration 15 / 30 | Total Loss: 3.453383445739746 | KNN Loss: 2.4609243869781494 | BCE Loss: 0.9924589991569519\n",
      "Epoch 280 / 500 | iteration 20 / 30 | Total Loss: 3.530120849609375 | KNN Loss: 2.487872838973999 | BCE Loss: 1.0422478914260864\n",
      "Epoch 280 / 500 | iteration 25 / 30 | Total Loss: 3.421483039855957 | KNN Loss: 2.4296658039093018 | BCE Loss: 0.9918171167373657\n",
      "Epoch 281 / 500 | iteration 0 / 30 | Total Loss: 3.460254669189453 | KNN Loss: 2.4444949626922607 | BCE Loss: 1.0157597064971924\n",
      "Epoch 281 / 500 | iteration 5 / 30 | Total Loss: 3.539341926574707 | KNN Loss: 2.520366668701172 | BCE Loss: 1.0189752578735352\n",
      "Epoch 281 / 500 | iteration 10 / 30 | Total Loss: 3.4678893089294434 | KNN Loss: 2.4571306705474854 | BCE Loss: 1.0107585191726685\n",
      "Epoch 281 / 500 | iteration 15 / 30 | Total Loss: 3.505600690841675 | KNN Loss: 2.4672675132751465 | BCE Loss: 1.0383331775665283\n",
      "Epoch 281 / 500 | iteration 20 / 30 | Total Loss: 3.487592935562134 | KNN Loss: 2.455288887023926 | BCE Loss: 1.032304048538208\n",
      "Epoch 281 / 500 | iteration 25 / 30 | Total Loss: 3.4766182899475098 | KNN Loss: 2.458683729171753 | BCE Loss: 1.0179346799850464\n",
      "Epoch   282: reducing learning rate of group 0 to 2.0177e-04.\n",
      "Epoch 282 / 500 | iteration 0 / 30 | Total Loss: 3.461620569229126 | KNN Loss: 2.4346611499786377 | BCE Loss: 1.0269594192504883\n",
      "Epoch 282 / 500 | iteration 5 / 30 | Total Loss: 3.46935772895813 | KNN Loss: 2.4340906143188477 | BCE Loss: 1.0352671146392822\n",
      "Epoch 282 / 500 | iteration 10 / 30 | Total Loss: 3.4825267791748047 | KNN Loss: 2.450655221939087 | BCE Loss: 1.0318715572357178\n",
      "Epoch 282 / 500 | iteration 15 / 30 | Total Loss: 3.4696667194366455 | KNN Loss: 2.4511911869049072 | BCE Loss: 1.0184755325317383\n",
      "Epoch 282 / 500 | iteration 20 / 30 | Total Loss: 3.409111976623535 | KNN Loss: 2.4418132305145264 | BCE Loss: 0.9672988653182983\n",
      "Epoch 282 / 500 | iteration 25 / 30 | Total Loss: 3.430156707763672 | KNN Loss: 2.4487416744232178 | BCE Loss: 0.9814149141311646\n",
      "Epoch 283 / 500 | iteration 0 / 30 | Total Loss: 3.5037968158721924 | KNN Loss: 2.4747390747070312 | BCE Loss: 1.0290577411651611\n",
      "Epoch 283 / 500 | iteration 5 / 30 | Total Loss: 3.482342004776001 | KNN Loss: 2.4585628509521484 | BCE Loss: 1.0237791538238525\n",
      "Epoch 283 / 500 | iteration 10 / 30 | Total Loss: 3.466534376144409 | KNN Loss: 2.4534881114959717 | BCE Loss: 1.0130462646484375\n",
      "Epoch 283 / 500 | iteration 15 / 30 | Total Loss: 3.424025535583496 | KNN Loss: 2.423269033432007 | BCE Loss: 1.0007565021514893\n",
      "Epoch 283 / 500 | iteration 20 / 30 | Total Loss: 3.4816091060638428 | KNN Loss: 2.4563279151916504 | BCE Loss: 1.0252811908721924\n",
      "Epoch 283 / 500 | iteration 25 / 30 | Total Loss: 3.420337677001953 | KNN Loss: 2.449291706085205 | BCE Loss: 0.9710460305213928\n",
      "Epoch 284 / 500 | iteration 0 / 30 | Total Loss: 3.477647066116333 | KNN Loss: 2.4660086631774902 | BCE Loss: 1.0116384029388428\n",
      "Epoch 284 / 500 | iteration 5 / 30 | Total Loss: 3.472043037414551 | KNN Loss: 2.4341483116149902 | BCE Loss: 1.0378947257995605\n",
      "Epoch 284 / 500 | iteration 10 / 30 | Total Loss: 3.439488649368286 | KNN Loss: 2.420952081680298 | BCE Loss: 1.0185365676879883\n",
      "Epoch 284 / 500 | iteration 15 / 30 | Total Loss: 3.4241509437561035 | KNN Loss: 2.4124791622161865 | BCE Loss: 1.0116719007492065\n",
      "Epoch 284 / 500 | iteration 20 / 30 | Total Loss: 3.4956352710723877 | KNN Loss: 2.4757235050201416 | BCE Loss: 1.019911766052246\n",
      "Epoch 284 / 500 | iteration 25 / 30 | Total Loss: 3.5010170936584473 | KNN Loss: 2.474982976913452 | BCE Loss: 1.0260339975357056\n",
      "Epoch 285 / 500 | iteration 0 / 30 | Total Loss: 3.4488656520843506 | KNN Loss: 2.4301953315734863 | BCE Loss: 1.0186703205108643\n",
      "Epoch 285 / 500 | iteration 5 / 30 | Total Loss: 3.521721124649048 | KNN Loss: 2.4793412685394287 | BCE Loss: 1.0423798561096191\n",
      "Epoch 285 / 500 | iteration 10 / 30 | Total Loss: 3.442294120788574 | KNN Loss: 2.4419190883636475 | BCE Loss: 1.0003750324249268\n",
      "Epoch 285 / 500 | iteration 15 / 30 | Total Loss: 3.475630044937134 | KNN Loss: 2.4597909450531006 | BCE Loss: 1.0158390998840332\n",
      "Epoch 285 / 500 | iteration 20 / 30 | Total Loss: 3.5024611949920654 | KNN Loss: 2.4431874752044678 | BCE Loss: 1.0592737197875977\n",
      "Epoch 285 / 500 | iteration 25 / 30 | Total Loss: 3.4988465309143066 | KNN Loss: 2.453207492828369 | BCE Loss: 1.0456390380859375\n",
      "Epoch 286 / 500 | iteration 0 / 30 | Total Loss: 3.4371066093444824 | KNN Loss: 2.418290615081787 | BCE Loss: 1.0188159942626953\n",
      "Epoch 286 / 500 | iteration 5 / 30 | Total Loss: 3.479766845703125 | KNN Loss: 2.462643623352051 | BCE Loss: 1.0171232223510742\n",
      "Epoch 286 / 500 | iteration 10 / 30 | Total Loss: 3.4875388145446777 | KNN Loss: 2.4523653984069824 | BCE Loss: 1.0351734161376953\n",
      "Epoch 286 / 500 | iteration 15 / 30 | Total Loss: 3.4501445293426514 | KNN Loss: 2.438507318496704 | BCE Loss: 1.0116372108459473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 286 / 500 | iteration 20 / 30 | Total Loss: 3.4823737144470215 | KNN Loss: 2.461528778076172 | BCE Loss: 1.0208449363708496\n",
      "Epoch 286 / 500 | iteration 25 / 30 | Total Loss: 3.485708236694336 | KNN Loss: 2.4700653553009033 | BCE Loss: 1.0156430006027222\n",
      "Epoch 287 / 500 | iteration 0 / 30 | Total Loss: 3.476346492767334 | KNN Loss: 2.468585729598999 | BCE Loss: 1.007760763168335\n",
      "Epoch 287 / 500 | iteration 5 / 30 | Total Loss: 3.450305938720703 | KNN Loss: 2.446007251739502 | BCE Loss: 1.0042986869812012\n",
      "Epoch 287 / 500 | iteration 10 / 30 | Total Loss: 3.4579336643218994 | KNN Loss: 2.4602715969085693 | BCE Loss: 0.9976620674133301\n",
      "Epoch 287 / 500 | iteration 15 / 30 | Total Loss: 3.516634225845337 | KNN Loss: 2.496877670288086 | BCE Loss: 1.019756555557251\n",
      "Epoch 287 / 500 | iteration 20 / 30 | Total Loss: 3.4482507705688477 | KNN Loss: 2.442563533782959 | BCE Loss: 1.0056873559951782\n",
      "Epoch 287 / 500 | iteration 25 / 30 | Total Loss: 3.4620256423950195 | KNN Loss: 2.4473774433135986 | BCE Loss: 1.014648199081421\n",
      "Epoch 288 / 500 | iteration 0 / 30 | Total Loss: 3.4677553176879883 | KNN Loss: 2.4569571018218994 | BCE Loss: 1.0107982158660889\n",
      "Epoch 288 / 500 | iteration 5 / 30 | Total Loss: 3.4540014266967773 | KNN Loss: 2.449155330657959 | BCE Loss: 1.0048459768295288\n",
      "Epoch 288 / 500 | iteration 10 / 30 | Total Loss: 3.4808826446533203 | KNN Loss: 2.4528403282165527 | BCE Loss: 1.0280423164367676\n",
      "Epoch 288 / 500 | iteration 15 / 30 | Total Loss: 3.4835352897644043 | KNN Loss: 2.4509565830230713 | BCE Loss: 1.032578706741333\n",
      "Epoch 288 / 500 | iteration 20 / 30 | Total Loss: 3.4974632263183594 | KNN Loss: 2.465158700942993 | BCE Loss: 1.0323045253753662\n",
      "Epoch 288 / 500 | iteration 25 / 30 | Total Loss: 3.4444797039031982 | KNN Loss: 2.440938949584961 | BCE Loss: 1.0035407543182373\n",
      "Epoch 289 / 500 | iteration 0 / 30 | Total Loss: 3.4779775142669678 | KNN Loss: 2.470599889755249 | BCE Loss: 1.0073776245117188\n",
      "Epoch 289 / 500 | iteration 5 / 30 | Total Loss: 3.457247257232666 | KNN Loss: 2.450681209564209 | BCE Loss: 1.0065659284591675\n",
      "Epoch 289 / 500 | iteration 10 / 30 | Total Loss: 3.449925422668457 | KNN Loss: 2.4473695755004883 | BCE Loss: 1.0025558471679688\n",
      "Epoch 289 / 500 | iteration 15 / 30 | Total Loss: 3.485205888748169 | KNN Loss: 2.4630749225616455 | BCE Loss: 1.0221309661865234\n",
      "Epoch 289 / 500 | iteration 20 / 30 | Total Loss: 3.5285308361053467 | KNN Loss: 2.4734232425689697 | BCE Loss: 1.055107593536377\n",
      "Epoch 289 / 500 | iteration 25 / 30 | Total Loss: 3.465420722961426 | KNN Loss: 2.471365451812744 | BCE Loss: 0.9940552711486816\n",
      "Epoch 290 / 500 | iteration 0 / 30 | Total Loss: 3.4595813751220703 | KNN Loss: 2.439419746398926 | BCE Loss: 1.020161509513855\n",
      "Epoch 290 / 500 | iteration 5 / 30 | Total Loss: 3.465456485748291 | KNN Loss: 2.4718165397644043 | BCE Loss: 0.9936400651931763\n",
      "Epoch 290 / 500 | iteration 10 / 30 | Total Loss: 3.4917774200439453 | KNN Loss: 2.4696402549743652 | BCE Loss: 1.02213716506958\n",
      "Epoch 290 / 500 | iteration 15 / 30 | Total Loss: 3.465791702270508 | KNN Loss: 2.456148624420166 | BCE Loss: 1.0096430778503418\n",
      "Epoch 290 / 500 | iteration 20 / 30 | Total Loss: 3.4451494216918945 | KNN Loss: 2.4425508975982666 | BCE Loss: 1.002598524093628\n",
      "Epoch 290 / 500 | iteration 25 / 30 | Total Loss: 3.413356304168701 | KNN Loss: 2.434171676635742 | BCE Loss: 0.979184627532959\n",
      "Epoch 291 / 500 | iteration 0 / 30 | Total Loss: 3.441251516342163 | KNN Loss: 2.427031993865967 | BCE Loss: 1.0142195224761963\n",
      "Epoch 291 / 500 | iteration 5 / 30 | Total Loss: 3.485154390335083 | KNN Loss: 2.445906639099121 | BCE Loss: 1.039247751235962\n",
      "Epoch 291 / 500 | iteration 10 / 30 | Total Loss: 3.508166790008545 | KNN Loss: 2.464696168899536 | BCE Loss: 1.0434706211090088\n",
      "Epoch 291 / 500 | iteration 15 / 30 | Total Loss: 3.5022854804992676 | KNN Loss: 2.4756968021392822 | BCE Loss: 1.0265886783599854\n",
      "Epoch 291 / 500 | iteration 20 / 30 | Total Loss: 3.485767364501953 | KNN Loss: 2.462369203567505 | BCE Loss: 1.0233982801437378\n",
      "Epoch 291 / 500 | iteration 25 / 30 | Total Loss: 3.4979708194732666 | KNN Loss: 2.5114340782165527 | BCE Loss: 0.9865367412567139\n",
      "Epoch 292 / 500 | iteration 0 / 30 | Total Loss: 3.489719867706299 | KNN Loss: 2.4894773960113525 | BCE Loss: 1.0002425909042358\n",
      "Epoch 292 / 500 | iteration 5 / 30 | Total Loss: 3.4584484100341797 | KNN Loss: 2.4291012287139893 | BCE Loss: 1.02934730052948\n",
      "Epoch 292 / 500 | iteration 10 / 30 | Total Loss: 3.469346046447754 | KNN Loss: 2.440051555633545 | BCE Loss: 1.029294490814209\n",
      "Epoch 292 / 500 | iteration 15 / 30 | Total Loss: 3.4270412921905518 | KNN Loss: 2.4279513359069824 | BCE Loss: 0.9990899562835693\n",
      "Epoch 292 / 500 | iteration 20 / 30 | Total Loss: 3.4804224967956543 | KNN Loss: 2.445512533187866 | BCE Loss: 1.034909963607788\n",
      "Epoch 292 / 500 | iteration 25 / 30 | Total Loss: 3.493889093399048 | KNN Loss: 2.4789116382598877 | BCE Loss: 1.0149774551391602\n",
      "Epoch 293 / 500 | iteration 0 / 30 | Total Loss: 3.476194381713867 | KNN Loss: 2.4480834007263184 | BCE Loss: 1.0281109809875488\n",
      "Epoch 293 / 500 | iteration 5 / 30 | Total Loss: 3.4052228927612305 | KNN Loss: 2.419315814971924 | BCE Loss: 0.9859070777893066\n",
      "Epoch 293 / 500 | iteration 10 / 30 | Total Loss: 3.454930305480957 | KNN Loss: 2.4564032554626465 | BCE Loss: 0.9985270500183105\n",
      "Epoch 293 / 500 | iteration 15 / 30 | Total Loss: 3.5045042037963867 | KNN Loss: 2.4757962226867676 | BCE Loss: 1.0287078619003296\n",
      "Epoch 293 / 500 | iteration 20 / 30 | Total Loss: 3.4777634143829346 | KNN Loss: 2.466170072555542 | BCE Loss: 1.0115933418273926\n",
      "Epoch 293 / 500 | iteration 25 / 30 | Total Loss: 3.432391405105591 | KNN Loss: 2.4334874153137207 | BCE Loss: 0.9989039897918701\n",
      "Epoch 294 / 500 | iteration 0 / 30 | Total Loss: 3.45857572555542 | KNN Loss: 2.427727460861206 | BCE Loss: 1.0308481454849243\n",
      "Epoch 294 / 500 | iteration 5 / 30 | Total Loss: 3.4778218269348145 | KNN Loss: 2.4519479274749756 | BCE Loss: 1.0258740186691284\n",
      "Epoch 294 / 500 | iteration 10 / 30 | Total Loss: 3.470700740814209 | KNN Loss: 2.426398992538452 | BCE Loss: 1.0443017482757568\n",
      "Epoch 294 / 500 | iteration 15 / 30 | Total Loss: 3.5204544067382812 | KNN Loss: 2.5193963050842285 | BCE Loss: 1.0010581016540527\n",
      "Epoch 294 / 500 | iteration 20 / 30 | Total Loss: 3.495725154876709 | KNN Loss: 2.4688196182250977 | BCE Loss: 1.0269055366516113\n",
      "Epoch 294 / 500 | iteration 25 / 30 | Total Loss: 3.473484516143799 | KNN Loss: 2.4652698040008545 | BCE Loss: 1.0082148313522339\n",
      "Epoch   295: reducing learning rate of group 0 to 1.4124e-04.\n",
      "Epoch 295 / 500 | iteration 0 / 30 | Total Loss: 3.4596643447875977 | KNN Loss: 2.453298568725586 | BCE Loss: 1.0063656568527222\n",
      "Epoch 295 / 500 | iteration 5 / 30 | Total Loss: 3.4818947315216064 | KNN Loss: 2.475677967071533 | BCE Loss: 1.0062167644500732\n",
      "Epoch 295 / 500 | iteration 10 / 30 | Total Loss: 3.4036777019500732 | KNN Loss: 2.4262518882751465 | BCE Loss: 0.9774258732795715\n",
      "Epoch 295 / 500 | iteration 15 / 30 | Total Loss: 3.503471851348877 | KNN Loss: 2.4507811069488525 | BCE Loss: 1.0526906251907349\n",
      "Epoch 295 / 500 | iteration 20 / 30 | Total Loss: 3.4826154708862305 | KNN Loss: 2.480621576309204 | BCE Loss: 1.001994013786316\n",
      "Epoch 295 / 500 | iteration 25 / 30 | Total Loss: 3.4777727127075195 | KNN Loss: 2.4471333026885986 | BCE Loss: 1.030639410018921\n",
      "Epoch 296 / 500 | iteration 0 / 30 | Total Loss: 3.4589920043945312 | KNN Loss: 2.4789490699768066 | BCE Loss: 0.9800428152084351\n",
      "Epoch 296 / 500 | iteration 5 / 30 | Total Loss: 3.4876153469085693 | KNN Loss: 2.4864087104797363 | BCE Loss: 1.001206636428833\n",
      "Epoch 296 / 500 | iteration 10 / 30 | Total Loss: 3.468283176422119 | KNN Loss: 2.465994119644165 | BCE Loss: 1.0022891759872437\n",
      "Epoch 296 / 500 | iteration 15 / 30 | Total Loss: 3.4754245281219482 | KNN Loss: 2.439751148223877 | BCE Loss: 1.0356733798980713\n",
      "Epoch 296 / 500 | iteration 20 / 30 | Total Loss: 3.4193058013916016 | KNN Loss: 2.4342987537384033 | BCE Loss: 0.9850071668624878\n",
      "Epoch 296 / 500 | iteration 25 / 30 | Total Loss: 3.439863681793213 | KNN Loss: 2.4384939670562744 | BCE Loss: 1.001369595527649\n",
      "Epoch 297 / 500 | iteration 0 / 30 | Total Loss: 3.508207082748413 | KNN Loss: 2.4711833000183105 | BCE Loss: 1.0370237827301025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 297 / 500 | iteration 5 / 30 | Total Loss: 3.493544340133667 | KNN Loss: 2.486621856689453 | BCE Loss: 1.0069224834442139\n",
      "Epoch 297 / 500 | iteration 10 / 30 | Total Loss: 3.4447829723358154 | KNN Loss: 2.447726249694824 | BCE Loss: 0.9970566630363464\n",
      "Epoch 297 / 500 | iteration 15 / 30 | Total Loss: 3.4541220664978027 | KNN Loss: 2.4394986629486084 | BCE Loss: 1.0146234035491943\n",
      "Epoch 297 / 500 | iteration 20 / 30 | Total Loss: 3.4610347747802734 | KNN Loss: 2.4465529918670654 | BCE Loss: 1.014481782913208\n",
      "Epoch 297 / 500 | iteration 25 / 30 | Total Loss: 3.4590916633605957 | KNN Loss: 2.443528890609741 | BCE Loss: 1.0155627727508545\n",
      "Epoch 298 / 500 | iteration 0 / 30 | Total Loss: 3.4590492248535156 | KNN Loss: 2.4450724124908447 | BCE Loss: 1.013976812362671\n",
      "Epoch 298 / 500 | iteration 5 / 30 | Total Loss: 3.4908151626586914 | KNN Loss: 2.456394910812378 | BCE Loss: 1.0344202518463135\n",
      "Epoch 298 / 500 | iteration 10 / 30 | Total Loss: 3.4581780433654785 | KNN Loss: 2.437140941619873 | BCE Loss: 1.021037220954895\n",
      "Epoch 298 / 500 | iteration 15 / 30 | Total Loss: 3.503019332885742 | KNN Loss: 2.4568488597869873 | BCE Loss: 1.0461703538894653\n",
      "Epoch 298 / 500 | iteration 20 / 30 | Total Loss: 3.454497814178467 | KNN Loss: 2.4812848567962646 | BCE Loss: 0.9732128381729126\n",
      "Epoch 298 / 500 | iteration 25 / 30 | Total Loss: 3.4968557357788086 | KNN Loss: 2.4825356006622314 | BCE Loss: 1.0143202543258667\n",
      "Epoch 299 / 500 | iteration 0 / 30 | Total Loss: 3.4363389015197754 | KNN Loss: 2.441927433013916 | BCE Loss: 0.9944113492965698\n",
      "Epoch 299 / 500 | iteration 5 / 30 | Total Loss: 3.4798388481140137 | KNN Loss: 2.480332136154175 | BCE Loss: 0.9995068311691284\n",
      "Epoch 299 / 500 | iteration 10 / 30 | Total Loss: 3.4768948554992676 | KNN Loss: 2.4487226009368896 | BCE Loss: 1.0281723737716675\n",
      "Epoch 299 / 500 | iteration 15 / 30 | Total Loss: 3.4645473957061768 | KNN Loss: 2.4520490169525146 | BCE Loss: 1.012498378753662\n",
      "Epoch 299 / 500 | iteration 20 / 30 | Total Loss: 3.429988384246826 | KNN Loss: 2.4209187030792236 | BCE Loss: 1.009069561958313\n",
      "Epoch 299 / 500 | iteration 25 / 30 | Total Loss: 3.4713144302368164 | KNN Loss: 2.4648966789245605 | BCE Loss: 1.0064178705215454\n",
      "Epoch 300 / 500 | iteration 0 / 30 | Total Loss: 3.4675822257995605 | KNN Loss: 2.457941770553589 | BCE Loss: 1.0096404552459717\n",
      "Epoch 300 / 500 | iteration 5 / 30 | Total Loss: 3.4672398567199707 | KNN Loss: 2.482935905456543 | BCE Loss: 0.9843040704727173\n",
      "Epoch 300 / 500 | iteration 10 / 30 | Total Loss: 3.4597811698913574 | KNN Loss: 2.4554097652435303 | BCE Loss: 1.0043714046478271\n",
      "Epoch 300 / 500 | iteration 15 / 30 | Total Loss: 3.451911449432373 | KNN Loss: 2.4516303539276123 | BCE Loss: 1.0002809762954712\n",
      "Epoch 300 / 500 | iteration 20 / 30 | Total Loss: 3.500234365463257 | KNN Loss: 2.474097490310669 | BCE Loss: 1.026136875152588\n",
      "Epoch 300 / 500 | iteration 25 / 30 | Total Loss: 3.4435160160064697 | KNN Loss: 2.438462972640991 | BCE Loss: 1.0050530433654785\n",
      "Epoch 301 / 500 | iteration 0 / 30 | Total Loss: 3.519510507583618 | KNN Loss: 2.5152063369750977 | BCE Loss: 1.0043041706085205\n",
      "Epoch 301 / 500 | iteration 5 / 30 | Total Loss: 3.457721710205078 | KNN Loss: 2.4387643337249756 | BCE Loss: 1.018957495689392\n",
      "Epoch 301 / 500 | iteration 10 / 30 | Total Loss: 3.433825969696045 | KNN Loss: 2.429168224334717 | BCE Loss: 1.0046578645706177\n",
      "Epoch 301 / 500 | iteration 15 / 30 | Total Loss: 3.444965362548828 | KNN Loss: 2.431515693664551 | BCE Loss: 1.013449788093567\n",
      "Epoch 301 / 500 | iteration 20 / 30 | Total Loss: 3.4736597537994385 | KNN Loss: 2.4449872970581055 | BCE Loss: 1.028672456741333\n",
      "Epoch 301 / 500 | iteration 25 / 30 | Total Loss: 3.4708359241485596 | KNN Loss: 2.485708236694336 | BCE Loss: 0.9851276278495789\n",
      "Epoch 302 / 500 | iteration 0 / 30 | Total Loss: 3.487051010131836 | KNN Loss: 2.4461607933044434 | BCE Loss: 1.040890097618103\n",
      "Epoch 302 / 500 | iteration 5 / 30 | Total Loss: 3.4475739002227783 | KNN Loss: 2.4480011463165283 | BCE Loss: 0.9995728135108948\n",
      "Epoch 302 / 500 | iteration 10 / 30 | Total Loss: 3.465055465698242 | KNN Loss: 2.4531548023223877 | BCE Loss: 1.011900782585144\n",
      "Epoch 302 / 500 | iteration 15 / 30 | Total Loss: 3.4987003803253174 | KNN Loss: 2.470933198928833 | BCE Loss: 1.0277671813964844\n",
      "Epoch 302 / 500 | iteration 20 / 30 | Total Loss: 3.473379611968994 | KNN Loss: 2.4896113872528076 | BCE Loss: 0.9837683439254761\n",
      "Epoch 302 / 500 | iteration 25 / 30 | Total Loss: 3.5006675720214844 | KNN Loss: 2.4785046577453613 | BCE Loss: 1.0221630334854126\n",
      "Epoch 303 / 500 | iteration 0 / 30 | Total Loss: 3.4337751865386963 | KNN Loss: 2.4253368377685547 | BCE Loss: 1.0084383487701416\n",
      "Epoch 303 / 500 | iteration 5 / 30 | Total Loss: 3.425664186477661 | KNN Loss: 2.4258222579956055 | BCE Loss: 0.9998418688774109\n",
      "Epoch 303 / 500 | iteration 10 / 30 | Total Loss: 3.4532155990600586 | KNN Loss: 2.4448916912078857 | BCE Loss: 1.0083240270614624\n",
      "Epoch 303 / 500 | iteration 15 / 30 | Total Loss: 3.4751949310302734 | KNN Loss: 2.4388914108276367 | BCE Loss: 1.0363034009933472\n",
      "Epoch 303 / 500 | iteration 20 / 30 | Total Loss: 3.4662787914276123 | KNN Loss: 2.4374523162841797 | BCE Loss: 1.0288264751434326\n",
      "Epoch 303 / 500 | iteration 25 / 30 | Total Loss: 3.459596633911133 | KNN Loss: 2.4551501274108887 | BCE Loss: 1.0044465065002441\n",
      "Epoch 304 / 500 | iteration 0 / 30 | Total Loss: 3.4503421783447266 | KNN Loss: 2.431544780731201 | BCE Loss: 1.018797516822815\n",
      "Epoch 304 / 500 | iteration 5 / 30 | Total Loss: 3.4839229583740234 | KNN Loss: 2.4427945613861084 | BCE Loss: 1.0411285161972046\n",
      "Epoch 304 / 500 | iteration 10 / 30 | Total Loss: 3.461016893386841 | KNN Loss: 2.465919017791748 | BCE Loss: 0.9950979351997375\n",
      "Epoch 304 / 500 | iteration 15 / 30 | Total Loss: 3.419570207595825 | KNN Loss: 2.4261860847473145 | BCE Loss: 0.9933841228485107\n",
      "Epoch 304 / 500 | iteration 20 / 30 | Total Loss: 3.5081615447998047 | KNN Loss: 2.464137315750122 | BCE Loss: 1.0440242290496826\n",
      "Epoch 304 / 500 | iteration 25 / 30 | Total Loss: 3.3867077827453613 | KNN Loss: 2.403937339782715 | BCE Loss: 0.9827704429626465\n",
      "Epoch 305 / 500 | iteration 0 / 30 | Total Loss: 3.4496970176696777 | KNN Loss: 2.4404165744781494 | BCE Loss: 1.0092804431915283\n",
      "Epoch 305 / 500 | iteration 5 / 30 | Total Loss: 3.442115306854248 | KNN Loss: 2.4280524253845215 | BCE Loss: 1.0140630006790161\n",
      "Epoch 305 / 500 | iteration 10 / 30 | Total Loss: 3.4685633182525635 | KNN Loss: 2.4340991973876953 | BCE Loss: 1.0344641208648682\n",
      "Epoch 305 / 500 | iteration 15 / 30 | Total Loss: 3.456766366958618 | KNN Loss: 2.44806170463562 | BCE Loss: 1.008704662322998\n",
      "Epoch 305 / 500 | iteration 20 / 30 | Total Loss: 3.461695671081543 | KNN Loss: 2.45450496673584 | BCE Loss: 1.0071907043457031\n",
      "Epoch 305 / 500 | iteration 25 / 30 | Total Loss: 3.4477555751800537 | KNN Loss: 2.4422340393066406 | BCE Loss: 1.005521535873413\n",
      "Epoch 306 / 500 | iteration 0 / 30 | Total Loss: 3.471389055252075 | KNN Loss: 2.4578397274017334 | BCE Loss: 1.0135493278503418\n",
      "Epoch 306 / 500 | iteration 5 / 30 | Total Loss: 3.4931139945983887 | KNN Loss: 2.4715576171875 | BCE Loss: 1.0215562582015991\n",
      "Epoch 306 / 500 | iteration 10 / 30 | Total Loss: 3.459580183029175 | KNN Loss: 2.4600441455841064 | BCE Loss: 0.9995360374450684\n",
      "Epoch 306 / 500 | iteration 15 / 30 | Total Loss: 3.4576168060302734 | KNN Loss: 2.4367849826812744 | BCE Loss: 1.020831823348999\n",
      "Epoch 306 / 500 | iteration 20 / 30 | Total Loss: 3.4219472408294678 | KNN Loss: 2.422569751739502 | BCE Loss: 0.9993774890899658\n",
      "Epoch 306 / 500 | iteration 25 / 30 | Total Loss: 3.4900870323181152 | KNN Loss: 2.4617393016815186 | BCE Loss: 1.0283476114273071\n",
      "Epoch 307 / 500 | iteration 0 / 30 | Total Loss: 3.4866347312927246 | KNN Loss: 2.462639570236206 | BCE Loss: 1.0239951610565186\n",
      "Epoch 307 / 500 | iteration 5 / 30 | Total Loss: 3.475843906402588 | KNN Loss: 2.4279112815856934 | BCE Loss: 1.0479326248168945\n",
      "Epoch 307 / 500 | iteration 10 / 30 | Total Loss: 3.4772305488586426 | KNN Loss: 2.437856435775757 | BCE Loss: 1.0393739938735962\n",
      "Epoch 307 / 500 | iteration 15 / 30 | Total Loss: 3.4700088500976562 | KNN Loss: 2.4642982482910156 | BCE Loss: 1.005710482597351\n",
      "Epoch 307 / 500 | iteration 20 / 30 | Total Loss: 3.4803504943847656 | KNN Loss: 2.4698400497436523 | BCE Loss: 1.0105105638504028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 307 / 500 | iteration 25 / 30 | Total Loss: 3.483628273010254 | KNN Loss: 2.480242967605591 | BCE Loss: 1.003385305404663\n",
      "Epoch 308 / 500 | iteration 0 / 30 | Total Loss: 3.459257125854492 | KNN Loss: 2.433971643447876 | BCE Loss: 1.0252854824066162\n",
      "Epoch 308 / 500 | iteration 5 / 30 | Total Loss: 3.478952646255493 | KNN Loss: 2.4826176166534424 | BCE Loss: 0.9963350892066956\n",
      "Epoch 308 / 500 | iteration 10 / 30 | Total Loss: 3.4686474800109863 | KNN Loss: 2.454085350036621 | BCE Loss: 1.0145621299743652\n",
      "Epoch 308 / 500 | iteration 15 / 30 | Total Loss: 3.435175895690918 | KNN Loss: 2.4399054050445557 | BCE Loss: 0.9952703714370728\n",
      "Epoch 308 / 500 | iteration 20 / 30 | Total Loss: 3.4631505012512207 | KNN Loss: 2.4547712802886963 | BCE Loss: 1.0083792209625244\n",
      "Epoch 308 / 500 | iteration 25 / 30 | Total Loss: 3.4567677974700928 | KNN Loss: 2.4304306507110596 | BCE Loss: 1.0263371467590332\n",
      "Epoch 309 / 500 | iteration 0 / 30 | Total Loss: 3.4513254165649414 | KNN Loss: 2.433621406555176 | BCE Loss: 1.017703890800476\n",
      "Epoch 309 / 500 | iteration 5 / 30 | Total Loss: 3.489863395690918 | KNN Loss: 2.4757132530212402 | BCE Loss: 1.0141500234603882\n",
      "Epoch 309 / 500 | iteration 10 / 30 | Total Loss: 3.485520362854004 | KNN Loss: 2.464501142501831 | BCE Loss: 1.0210192203521729\n",
      "Epoch 309 / 500 | iteration 15 / 30 | Total Loss: 3.4633281230926514 | KNN Loss: 2.4321696758270264 | BCE Loss: 1.031158447265625\n",
      "Epoch 309 / 500 | iteration 20 / 30 | Total Loss: 3.4351747035980225 | KNN Loss: 2.4598515033721924 | BCE Loss: 0.9753232002258301\n",
      "Epoch 309 / 500 | iteration 25 / 30 | Total Loss: 3.467150926589966 | KNN Loss: 2.441983938217163 | BCE Loss: 1.0251669883728027\n",
      "Epoch   310: reducing learning rate of group 0 to 9.8866e-05.\n",
      "Epoch 310 / 500 | iteration 0 / 30 | Total Loss: 3.4551448822021484 | KNN Loss: 2.4407830238342285 | BCE Loss: 1.0143619775772095\n",
      "Epoch 310 / 500 | iteration 5 / 30 | Total Loss: 3.4448184967041016 | KNN Loss: 2.4454874992370605 | BCE Loss: 0.9993308782577515\n",
      "Epoch 310 / 500 | iteration 10 / 30 | Total Loss: 3.4338321685791016 | KNN Loss: 2.4481184482574463 | BCE Loss: 0.9857138395309448\n",
      "Epoch 310 / 500 | iteration 15 / 30 | Total Loss: 3.475008964538574 | KNN Loss: 2.461196184158325 | BCE Loss: 1.013812780380249\n",
      "Epoch 310 / 500 | iteration 20 / 30 | Total Loss: 3.497086763381958 | KNN Loss: 2.4657115936279297 | BCE Loss: 1.0313751697540283\n",
      "Epoch 310 / 500 | iteration 25 / 30 | Total Loss: 3.4407339096069336 | KNN Loss: 2.4054152965545654 | BCE Loss: 1.0353187322616577\n",
      "Epoch 311 / 500 | iteration 0 / 30 | Total Loss: 3.4253482818603516 | KNN Loss: 2.437987804412842 | BCE Loss: 0.9873604774475098\n",
      "Epoch 311 / 500 | iteration 5 / 30 | Total Loss: 3.4516854286193848 | KNN Loss: 2.449802875518799 | BCE Loss: 1.0018826723098755\n",
      "Epoch 311 / 500 | iteration 10 / 30 | Total Loss: 3.456031084060669 | KNN Loss: 2.4356961250305176 | BCE Loss: 1.0203349590301514\n",
      "Epoch 311 / 500 | iteration 15 / 30 | Total Loss: 3.4503347873687744 | KNN Loss: 2.4450578689575195 | BCE Loss: 1.0052769184112549\n",
      "Epoch 311 / 500 | iteration 20 / 30 | Total Loss: 3.471001625061035 | KNN Loss: 2.4384512901306152 | BCE Loss: 1.03255033493042\n",
      "Epoch 311 / 500 | iteration 25 / 30 | Total Loss: 3.4525070190429688 | KNN Loss: 2.4405388832092285 | BCE Loss: 1.0119682550430298\n",
      "Epoch 312 / 500 | iteration 0 / 30 | Total Loss: 3.5166077613830566 | KNN Loss: 2.4719910621643066 | BCE Loss: 1.04461669921875\n",
      "Epoch 312 / 500 | iteration 5 / 30 | Total Loss: 3.4648914337158203 | KNN Loss: 2.4376235008239746 | BCE Loss: 1.0272679328918457\n",
      "Epoch 312 / 500 | iteration 10 / 30 | Total Loss: 3.4482014179229736 | KNN Loss: 2.444575786590576 | BCE Loss: 1.0036256313323975\n",
      "Epoch 312 / 500 | iteration 15 / 30 | Total Loss: 3.4981210231781006 | KNN Loss: 2.476694107055664 | BCE Loss: 1.0214269161224365\n",
      "Epoch 312 / 500 | iteration 20 / 30 | Total Loss: 3.4845356941223145 | KNN Loss: 2.4623398780822754 | BCE Loss: 1.022195816040039\n",
      "Epoch 312 / 500 | iteration 25 / 30 | Total Loss: 3.459415912628174 | KNN Loss: 2.432619094848633 | BCE Loss: 1.0267966985702515\n",
      "Epoch 313 / 500 | iteration 0 / 30 | Total Loss: 3.450312852859497 | KNN Loss: 2.4384114742279053 | BCE Loss: 1.0119013786315918\n",
      "Epoch 313 / 500 | iteration 5 / 30 | Total Loss: 3.481581926345825 | KNN Loss: 2.4822778701782227 | BCE Loss: 0.9993041157722473\n",
      "Epoch 313 / 500 | iteration 10 / 30 | Total Loss: 3.4943907260894775 | KNN Loss: 2.4605863094329834 | BCE Loss: 1.0338044166564941\n",
      "Epoch 313 / 500 | iteration 15 / 30 | Total Loss: 3.4786810874938965 | KNN Loss: 2.445835590362549 | BCE Loss: 1.0328454971313477\n",
      "Epoch 313 / 500 | iteration 20 / 30 | Total Loss: 3.4706785678863525 | KNN Loss: 2.461794376373291 | BCE Loss: 1.0088841915130615\n",
      "Epoch 313 / 500 | iteration 25 / 30 | Total Loss: 3.4581284523010254 | KNN Loss: 2.466355800628662 | BCE Loss: 0.9917725324630737\n",
      "Epoch 314 / 500 | iteration 0 / 30 | Total Loss: 3.480797290802002 | KNN Loss: 2.4531984329223633 | BCE Loss: 1.0275988578796387\n",
      "Epoch 314 / 500 | iteration 5 / 30 | Total Loss: 3.4632508754730225 | KNN Loss: 2.444310188293457 | BCE Loss: 1.0189406871795654\n",
      "Epoch 314 / 500 | iteration 10 / 30 | Total Loss: 3.427844524383545 | KNN Loss: 2.45743727684021 | BCE Loss: 0.970407247543335\n",
      "Epoch 314 / 500 | iteration 15 / 30 | Total Loss: 3.4373416900634766 | KNN Loss: 2.4439213275909424 | BCE Loss: 0.9934203624725342\n",
      "Epoch 314 / 500 | iteration 20 / 30 | Total Loss: 3.47141695022583 | KNN Loss: 2.4521842002868652 | BCE Loss: 1.0192327499389648\n",
      "Epoch 314 / 500 | iteration 25 / 30 | Total Loss: 3.463749885559082 | KNN Loss: 2.442840576171875 | BCE Loss: 1.020909309387207\n",
      "Epoch 315 / 500 | iteration 0 / 30 | Total Loss: 3.4729790687561035 | KNN Loss: 2.4626412391662598 | BCE Loss: 1.0103378295898438\n",
      "Epoch 315 / 500 | iteration 5 / 30 | Total Loss: 3.520132064819336 | KNN Loss: 2.4843225479125977 | BCE Loss: 1.0358093976974487\n",
      "Epoch 315 / 500 | iteration 10 / 30 | Total Loss: 3.4627959728240967 | KNN Loss: 2.437762975692749 | BCE Loss: 1.0250329971313477\n",
      "Epoch 315 / 500 | iteration 15 / 30 | Total Loss: 3.467495918273926 | KNN Loss: 2.471892833709717 | BCE Loss: 0.9956030249595642\n",
      "Epoch 315 / 500 | iteration 20 / 30 | Total Loss: 3.4747138023376465 | KNN Loss: 2.455591917037964 | BCE Loss: 1.019121766090393\n",
      "Epoch 315 / 500 | iteration 25 / 30 | Total Loss: 3.466787815093994 | KNN Loss: 2.4590396881103516 | BCE Loss: 1.007748007774353\n",
      "Epoch 316 / 500 | iteration 0 / 30 | Total Loss: 3.4749996662139893 | KNN Loss: 2.4390413761138916 | BCE Loss: 1.0359582901000977\n",
      "Epoch 316 / 500 | iteration 5 / 30 | Total Loss: 3.5224337577819824 | KNN Loss: 2.4719765186309814 | BCE Loss: 1.050457239151001\n",
      "Epoch 316 / 500 | iteration 10 / 30 | Total Loss: 3.472055196762085 | KNN Loss: 2.4478678703308105 | BCE Loss: 1.0241873264312744\n",
      "Epoch 316 / 500 | iteration 15 / 30 | Total Loss: 3.450493574142456 | KNN Loss: 2.4374139308929443 | BCE Loss: 1.0130796432495117\n",
      "Epoch 316 / 500 | iteration 20 / 30 | Total Loss: 3.4119043350219727 | KNN Loss: 2.433544635772705 | BCE Loss: 0.9783596396446228\n",
      "Epoch 316 / 500 | iteration 25 / 30 | Total Loss: 3.459312915802002 | KNN Loss: 2.4456920623779297 | BCE Loss: 1.0136209726333618\n",
      "Epoch 317 / 500 | iteration 0 / 30 | Total Loss: 3.500718593597412 | KNN Loss: 2.487440586090088 | BCE Loss: 1.0132780075073242\n",
      "Epoch 317 / 500 | iteration 5 / 30 | Total Loss: 3.48789119720459 | KNN Loss: 2.4521048069000244 | BCE Loss: 1.0357863903045654\n",
      "Epoch 317 / 500 | iteration 10 / 30 | Total Loss: 3.4546709060668945 | KNN Loss: 2.4684977531433105 | BCE Loss: 0.9861730933189392\n",
      "Epoch 317 / 500 | iteration 15 / 30 | Total Loss: 3.444080114364624 | KNN Loss: 2.4449868202209473 | BCE Loss: 0.9990932941436768\n",
      "Epoch 317 / 500 | iteration 20 / 30 | Total Loss: 3.451197385787964 | KNN Loss: 2.4382412433624268 | BCE Loss: 1.012956142425537\n",
      "Epoch 317 / 500 | iteration 25 / 30 | Total Loss: 3.4163079261779785 | KNN Loss: 2.4202492237091064 | BCE Loss: 0.9960587024688721\n",
      "Epoch 318 / 500 | iteration 0 / 30 | Total Loss: 3.4660539627075195 | KNN Loss: 2.462394952774048 | BCE Loss: 1.0036591291427612\n",
      "Epoch 318 / 500 | iteration 5 / 30 | Total Loss: 3.496016502380371 | KNN Loss: 2.4726755619049072 | BCE Loss: 1.0233410596847534\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 318 / 500 | iteration 10 / 30 | Total Loss: 3.483295440673828 | KNN Loss: 2.460132598876953 | BCE Loss: 1.023162841796875\n",
      "Epoch 318 / 500 | iteration 15 / 30 | Total Loss: 3.439887285232544 | KNN Loss: 2.427950620651245 | BCE Loss: 1.0119366645812988\n",
      "Epoch 318 / 500 | iteration 20 / 30 | Total Loss: 3.464860200881958 | KNN Loss: 2.4646353721618652 | BCE Loss: 1.0002248287200928\n",
      "Epoch 318 / 500 | iteration 25 / 30 | Total Loss: 3.458597183227539 | KNN Loss: 2.4608895778656006 | BCE Loss: 0.9977074861526489\n",
      "Epoch 319 / 500 | iteration 0 / 30 | Total Loss: 3.491584300994873 | KNN Loss: 2.5006511211395264 | BCE Loss: 0.9909331202507019\n",
      "Epoch 319 / 500 | iteration 5 / 30 | Total Loss: 3.4923014640808105 | KNN Loss: 2.490429162979126 | BCE Loss: 1.001872181892395\n",
      "Epoch 319 / 500 | iteration 10 / 30 | Total Loss: 3.5051076412200928 | KNN Loss: 2.467672824859619 | BCE Loss: 1.0374348163604736\n",
      "Epoch 319 / 500 | iteration 15 / 30 | Total Loss: 3.4858639240264893 | KNN Loss: 2.4609615802764893 | BCE Loss: 1.02490234375\n",
      "Epoch 319 / 500 | iteration 20 / 30 | Total Loss: 3.419900417327881 | KNN Loss: 2.4448070526123047 | BCE Loss: 0.975093424320221\n",
      "Epoch 319 / 500 | iteration 25 / 30 | Total Loss: 3.421743154525757 | KNN Loss: 2.4231698513031006 | BCE Loss: 0.9985733032226562\n",
      "Epoch 320 / 500 | iteration 0 / 30 | Total Loss: 3.4678640365600586 | KNN Loss: 2.4558911323547363 | BCE Loss: 1.0119730234146118\n",
      "Epoch 320 / 500 | iteration 5 / 30 | Total Loss: 3.410581350326538 | KNN Loss: 2.409550666809082 | BCE Loss: 1.001030683517456\n",
      "Epoch 320 / 500 | iteration 10 / 30 | Total Loss: 3.456700086593628 | KNN Loss: 2.4370150566101074 | BCE Loss: 1.0196850299835205\n",
      "Epoch 320 / 500 | iteration 15 / 30 | Total Loss: 3.446665048599243 | KNN Loss: 2.425861358642578 | BCE Loss: 1.020803689956665\n",
      "Epoch 320 / 500 | iteration 20 / 30 | Total Loss: 3.4631099700927734 | KNN Loss: 2.471083402633667 | BCE Loss: 0.9920266270637512\n",
      "Epoch 320 / 500 | iteration 25 / 30 | Total Loss: 3.4749133586883545 | KNN Loss: 2.4413349628448486 | BCE Loss: 1.0335783958435059\n",
      "Epoch   321: reducing learning rate of group 0 to 6.9206e-05.\n",
      "Epoch 321 / 500 | iteration 0 / 30 | Total Loss: 3.461583375930786 | KNN Loss: 2.452901840209961 | BCE Loss: 1.0086815357208252\n",
      "Epoch 321 / 500 | iteration 5 / 30 | Total Loss: 3.484572649002075 | KNN Loss: 2.4684739112854004 | BCE Loss: 1.0160987377166748\n",
      "Epoch 321 / 500 | iteration 10 / 30 | Total Loss: 3.4652011394500732 | KNN Loss: 2.454134702682495 | BCE Loss: 1.0110664367675781\n",
      "Epoch 321 / 500 | iteration 15 / 30 | Total Loss: 3.431699752807617 | KNN Loss: 2.43289852142334 | BCE Loss: 0.9988012909889221\n",
      "Epoch 321 / 500 | iteration 20 / 30 | Total Loss: 3.4945595264434814 | KNN Loss: 2.4770073890686035 | BCE Loss: 1.017552137374878\n",
      "Epoch 321 / 500 | iteration 25 / 30 | Total Loss: 3.5213088989257812 | KNN Loss: 2.503401279449463 | BCE Loss: 1.0179076194763184\n",
      "Epoch 322 / 500 | iteration 0 / 30 | Total Loss: 3.5073156356811523 | KNN Loss: 2.474759817123413 | BCE Loss: 1.0325558185577393\n",
      "Epoch 322 / 500 | iteration 5 / 30 | Total Loss: 3.506556987762451 | KNN Loss: 2.4713451862335205 | BCE Loss: 1.0352116823196411\n",
      "Epoch 322 / 500 | iteration 10 / 30 | Total Loss: 3.4844961166381836 | KNN Loss: 2.473029375076294 | BCE Loss: 1.0114666223526\n",
      "Epoch 322 / 500 | iteration 15 / 30 | Total Loss: 3.468723773956299 | KNN Loss: 2.445786952972412 | BCE Loss: 1.0229368209838867\n",
      "Epoch 322 / 500 | iteration 20 / 30 | Total Loss: 3.5261170864105225 | KNN Loss: 2.4809670448303223 | BCE Loss: 1.0451500415802002\n",
      "Epoch 322 / 500 | iteration 25 / 30 | Total Loss: 3.4230027198791504 | KNN Loss: 2.4445748329162598 | BCE Loss: 0.9784278869628906\n",
      "Epoch 323 / 500 | iteration 0 / 30 | Total Loss: 3.455749034881592 | KNN Loss: 2.4441158771514893 | BCE Loss: 1.0116331577301025\n",
      "Epoch 323 / 500 | iteration 5 / 30 | Total Loss: 3.4569575786590576 | KNN Loss: 2.459188938140869 | BCE Loss: 0.9977685809135437\n",
      "Epoch 323 / 500 | iteration 10 / 30 | Total Loss: 3.4504096508026123 | KNN Loss: 2.4492337703704834 | BCE Loss: 1.001175880432129\n",
      "Epoch 323 / 500 | iteration 15 / 30 | Total Loss: 3.4842867851257324 | KNN Loss: 2.4498820304870605 | BCE Loss: 1.0344047546386719\n",
      "Epoch 323 / 500 | iteration 20 / 30 | Total Loss: 3.4512131214141846 | KNN Loss: 2.4479103088378906 | BCE Loss: 1.003302812576294\n",
      "Epoch 323 / 500 | iteration 25 / 30 | Total Loss: 3.476273536682129 | KNN Loss: 2.457547664642334 | BCE Loss: 1.018725872039795\n",
      "Epoch 324 / 500 | iteration 0 / 30 | Total Loss: 3.4642913341522217 | KNN Loss: 2.4506161212921143 | BCE Loss: 1.0136752128601074\n",
      "Epoch 324 / 500 | iteration 5 / 30 | Total Loss: 3.4742939472198486 | KNN Loss: 2.458944082260132 | BCE Loss: 1.0153498649597168\n",
      "Epoch 324 / 500 | iteration 10 / 30 | Total Loss: 3.4752185344696045 | KNN Loss: 2.4512791633605957 | BCE Loss: 1.0239393711090088\n",
      "Epoch 324 / 500 | iteration 15 / 30 | Total Loss: 3.474658966064453 | KNN Loss: 2.4581410884857178 | BCE Loss: 1.0165177583694458\n",
      "Epoch 324 / 500 | iteration 20 / 30 | Total Loss: 3.432760715484619 | KNN Loss: 2.421718120574951 | BCE Loss: 1.0110424757003784\n",
      "Epoch 324 / 500 | iteration 25 / 30 | Total Loss: 3.445009708404541 | KNN Loss: 2.447331428527832 | BCE Loss: 0.9976781606674194\n",
      "Epoch 325 / 500 | iteration 0 / 30 | Total Loss: 3.5204944610595703 | KNN Loss: 2.4912636280059814 | BCE Loss: 1.0292307138442993\n",
      "Epoch 325 / 500 | iteration 5 / 30 | Total Loss: 3.5134801864624023 | KNN Loss: 2.460346221923828 | BCE Loss: 1.0531339645385742\n",
      "Epoch 325 / 500 | iteration 10 / 30 | Total Loss: 3.4778637886047363 | KNN Loss: 2.4796924591064453 | BCE Loss: 0.998171329498291\n",
      "Epoch 325 / 500 | iteration 15 / 30 | Total Loss: 3.464832067489624 | KNN Loss: 2.4415788650512695 | BCE Loss: 1.0232532024383545\n",
      "Epoch 325 / 500 | iteration 20 / 30 | Total Loss: 3.4395248889923096 | KNN Loss: 2.4313840866088867 | BCE Loss: 1.0081408023834229\n",
      "Epoch 325 / 500 | iteration 25 / 30 | Total Loss: 3.5026795864105225 | KNN Loss: 2.4596121311187744 | BCE Loss: 1.043067455291748\n",
      "Epoch 326 / 500 | iteration 0 / 30 | Total Loss: 3.479088544845581 | KNN Loss: 2.451319932937622 | BCE Loss: 1.027768611907959\n",
      "Epoch 326 / 500 | iteration 5 / 30 | Total Loss: 3.4850542545318604 | KNN Loss: 2.4654412269592285 | BCE Loss: 1.0196130275726318\n",
      "Epoch 326 / 500 | iteration 10 / 30 | Total Loss: 3.4622321128845215 | KNN Loss: 2.4736526012420654 | BCE Loss: 0.9885796308517456\n",
      "Epoch 326 / 500 | iteration 15 / 30 | Total Loss: 3.5034751892089844 | KNN Loss: 2.468212604522705 | BCE Loss: 1.0352624654769897\n",
      "Epoch 326 / 500 | iteration 20 / 30 | Total Loss: 3.4362144470214844 | KNN Loss: 2.4147043228149414 | BCE Loss: 1.0215102434158325\n",
      "Epoch 326 / 500 | iteration 25 / 30 | Total Loss: 3.491917848587036 | KNN Loss: 2.471938371658325 | BCE Loss: 1.019979476928711\n",
      "Epoch 327 / 500 | iteration 0 / 30 | Total Loss: 3.463273048400879 | KNN Loss: 2.434197425842285 | BCE Loss: 1.0290757417678833\n",
      "Epoch 327 / 500 | iteration 5 / 30 | Total Loss: 3.468656539916992 | KNN Loss: 2.4582622051239014 | BCE Loss: 1.0103942155838013\n",
      "Epoch 327 / 500 | iteration 10 / 30 | Total Loss: 3.496608257293701 | KNN Loss: 2.4508607387542725 | BCE Loss: 1.0457473993301392\n",
      "Epoch 327 / 500 | iteration 15 / 30 | Total Loss: 3.433081865310669 | KNN Loss: 2.431846857070923 | BCE Loss: 1.001235008239746\n",
      "Epoch 327 / 500 | iteration 20 / 30 | Total Loss: 3.5021018981933594 | KNN Loss: 2.4814438819885254 | BCE Loss: 1.020658016204834\n",
      "Epoch 327 / 500 | iteration 25 / 30 | Total Loss: 3.43957781791687 | KNN Loss: 2.439314603805542 | BCE Loss: 1.0002632141113281\n",
      "Epoch 328 / 500 | iteration 0 / 30 | Total Loss: 3.461399555206299 | KNN Loss: 2.4399502277374268 | BCE Loss: 1.0214492082595825\n",
      "Epoch 328 / 500 | iteration 5 / 30 | Total Loss: 3.4746007919311523 | KNN Loss: 2.464843273162842 | BCE Loss: 1.0097575187683105\n",
      "Epoch 328 / 500 | iteration 10 / 30 | Total Loss: 3.460033893585205 | KNN Loss: 2.471860885620117 | BCE Loss: 0.9881728887557983\n",
      "Epoch 328 / 500 | iteration 15 / 30 | Total Loss: 3.5145604610443115 | KNN Loss: 2.500504732131958 | BCE Loss: 1.0140557289123535\n",
      "Epoch 328 / 500 | iteration 20 / 30 | Total Loss: 3.515308380126953 | KNN Loss: 2.472381114959717 | BCE Loss: 1.0429271459579468\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 328 / 500 | iteration 25 / 30 | Total Loss: 3.466801643371582 | KNN Loss: 2.441115617752075 | BCE Loss: 1.0256860256195068\n",
      "Epoch 329 / 500 | iteration 0 / 30 | Total Loss: 3.4257402420043945 | KNN Loss: 2.4347081184387207 | BCE Loss: 0.9910321235656738\n",
      "Epoch 329 / 500 | iteration 5 / 30 | Total Loss: 3.4547743797302246 | KNN Loss: 2.451307773590088 | BCE Loss: 1.0034664869308472\n",
      "Epoch 329 / 500 | iteration 10 / 30 | Total Loss: 3.469719886779785 | KNN Loss: 2.455233573913574 | BCE Loss: 1.014486312866211\n",
      "Epoch 329 / 500 | iteration 15 / 30 | Total Loss: 3.4591481685638428 | KNN Loss: 2.4565937519073486 | BCE Loss: 1.0025544166564941\n",
      "Epoch 329 / 500 | iteration 20 / 30 | Total Loss: 3.4851789474487305 | KNN Loss: 2.4718945026397705 | BCE Loss: 1.01328444480896\n",
      "Epoch 329 / 500 | iteration 25 / 30 | Total Loss: 3.4757766723632812 | KNN Loss: 2.4731595516204834 | BCE Loss: 1.0026172399520874\n",
      "Epoch 330 / 500 | iteration 0 / 30 | Total Loss: 3.5093994140625 | KNN Loss: 2.475611448287964 | BCE Loss: 1.0337879657745361\n",
      "Epoch 330 / 500 | iteration 5 / 30 | Total Loss: 3.460783004760742 | KNN Loss: 2.4750630855560303 | BCE Loss: 0.9857200384140015\n",
      "Epoch 330 / 500 | iteration 10 / 30 | Total Loss: 3.457937717437744 | KNN Loss: 2.4519219398498535 | BCE Loss: 1.0060158967971802\n",
      "Epoch 330 / 500 | iteration 15 / 30 | Total Loss: 3.469425678253174 | KNN Loss: 2.4603493213653564 | BCE Loss: 1.0090763568878174\n",
      "Epoch 330 / 500 | iteration 20 / 30 | Total Loss: 3.5356290340423584 | KNN Loss: 2.5282819271087646 | BCE Loss: 1.0073471069335938\n",
      "Epoch 330 / 500 | iteration 25 / 30 | Total Loss: 3.51191782951355 | KNN Loss: 2.4765305519104004 | BCE Loss: 1.0353872776031494\n",
      "Epoch 331 / 500 | iteration 0 / 30 | Total Loss: 3.497588634490967 | KNN Loss: 2.479820489883423 | BCE Loss: 1.017768144607544\n",
      "Epoch 331 / 500 | iteration 5 / 30 | Total Loss: 3.4785335063934326 | KNN Loss: 2.4439597129821777 | BCE Loss: 1.0345737934112549\n",
      "Epoch 331 / 500 | iteration 10 / 30 | Total Loss: 3.4898202419281006 | KNN Loss: 2.464311361312866 | BCE Loss: 1.0255088806152344\n",
      "Epoch 331 / 500 | iteration 15 / 30 | Total Loss: 3.462531089782715 | KNN Loss: 2.447167158126831 | BCE Loss: 1.0153640508651733\n",
      "Epoch 331 / 500 | iteration 20 / 30 | Total Loss: 3.413743734359741 | KNN Loss: 2.4098687171936035 | BCE Loss: 1.0038750171661377\n",
      "Epoch 331 / 500 | iteration 25 / 30 | Total Loss: 3.433027505874634 | KNN Loss: 2.408838987350464 | BCE Loss: 1.02418851852417\n",
      "Epoch   332: reducing learning rate of group 0 to 4.8445e-05.\n",
      "Epoch 332 / 500 | iteration 0 / 30 | Total Loss: 3.5015761852264404 | KNN Loss: 2.4722378253936768 | BCE Loss: 1.0293383598327637\n",
      "Epoch 332 / 500 | iteration 5 / 30 | Total Loss: 3.4850120544433594 | KNN Loss: 2.471428394317627 | BCE Loss: 1.0135836601257324\n",
      "Epoch 332 / 500 | iteration 10 / 30 | Total Loss: 3.476343870162964 | KNN Loss: 2.4521749019622803 | BCE Loss: 1.0241689682006836\n",
      "Epoch 332 / 500 | iteration 15 / 30 | Total Loss: 3.4512171745300293 | KNN Loss: 2.442711114883423 | BCE Loss: 1.0085060596466064\n",
      "Epoch 332 / 500 | iteration 20 / 30 | Total Loss: 3.4467995166778564 | KNN Loss: 2.4552385807037354 | BCE Loss: 0.9915608763694763\n",
      "Epoch 332 / 500 | iteration 25 / 30 | Total Loss: 3.464489459991455 | KNN Loss: 2.443711519241333 | BCE Loss: 1.020777940750122\n",
      "Epoch 333 / 500 | iteration 0 / 30 | Total Loss: 3.454697370529175 | KNN Loss: 2.4508488178253174 | BCE Loss: 1.0038485527038574\n",
      "Epoch 333 / 500 | iteration 5 / 30 | Total Loss: 3.482776403427124 | KNN Loss: 2.4647624492645264 | BCE Loss: 1.0180139541625977\n",
      "Epoch 333 / 500 | iteration 10 / 30 | Total Loss: 3.4601852893829346 | KNN Loss: 2.4316561222076416 | BCE Loss: 1.028529167175293\n",
      "Epoch 333 / 500 | iteration 15 / 30 | Total Loss: 3.45438814163208 | KNN Loss: 2.448529005050659 | BCE Loss: 1.0058590173721313\n",
      "Epoch 333 / 500 | iteration 20 / 30 | Total Loss: 3.4452662467956543 | KNN Loss: 2.449907064437866 | BCE Loss: 0.9953591823577881\n",
      "Epoch 333 / 500 | iteration 25 / 30 | Total Loss: 3.450870990753174 | KNN Loss: 2.462186098098755 | BCE Loss: 0.9886850118637085\n",
      "Epoch 334 / 500 | iteration 0 / 30 | Total Loss: 3.5086350440979004 | KNN Loss: 2.4616055488586426 | BCE Loss: 1.0470293760299683\n",
      "Epoch 334 / 500 | iteration 5 / 30 | Total Loss: 3.476242780685425 | KNN Loss: 2.455087423324585 | BCE Loss: 1.0211553573608398\n",
      "Epoch 334 / 500 | iteration 10 / 30 | Total Loss: 3.495216131210327 | KNN Loss: 2.4898617267608643 | BCE Loss: 1.005354404449463\n",
      "Epoch 334 / 500 | iteration 15 / 30 | Total Loss: 3.477874994277954 | KNN Loss: 2.452106237411499 | BCE Loss: 1.025768756866455\n",
      "Epoch 334 / 500 | iteration 20 / 30 | Total Loss: 3.475343704223633 | KNN Loss: 2.4724111557006836 | BCE Loss: 1.0029324293136597\n",
      "Epoch 334 / 500 | iteration 25 / 30 | Total Loss: 3.446223020553589 | KNN Loss: 2.418311595916748 | BCE Loss: 1.0279114246368408\n",
      "Epoch 335 / 500 | iteration 0 / 30 | Total Loss: 3.471710681915283 | KNN Loss: 2.45003604888916 | BCE Loss: 1.021674633026123\n",
      "Epoch 335 / 500 | iteration 5 / 30 | Total Loss: 3.470142126083374 | KNN Loss: 2.444451332092285 | BCE Loss: 1.0256907939910889\n",
      "Epoch 335 / 500 | iteration 10 / 30 | Total Loss: 3.4825990200042725 | KNN Loss: 2.4611923694610596 | BCE Loss: 1.021406650543213\n",
      "Epoch 335 / 500 | iteration 15 / 30 | Total Loss: 3.4545819759368896 | KNN Loss: 2.4402689933776855 | BCE Loss: 1.014312982559204\n",
      "Epoch 335 / 500 | iteration 20 / 30 | Total Loss: 3.4921603202819824 | KNN Loss: 2.4772372245788574 | BCE Loss: 1.014923095703125\n",
      "Epoch 335 / 500 | iteration 25 / 30 | Total Loss: 3.492530345916748 | KNN Loss: 2.453963279724121 | BCE Loss: 1.038567066192627\n",
      "Epoch 336 / 500 | iteration 0 / 30 | Total Loss: 3.42966365814209 | KNN Loss: 2.429461717605591 | BCE Loss: 1.000201940536499\n",
      "Epoch 336 / 500 | iteration 5 / 30 | Total Loss: 3.468642473220825 | KNN Loss: 2.4316015243530273 | BCE Loss: 1.0370409488677979\n",
      "Epoch 336 / 500 | iteration 10 / 30 | Total Loss: 3.4576809406280518 | KNN Loss: 2.4300661087036133 | BCE Loss: 1.0276148319244385\n",
      "Epoch 336 / 500 | iteration 15 / 30 | Total Loss: 3.466160535812378 | KNN Loss: 2.4320223331451416 | BCE Loss: 1.0341382026672363\n",
      "Epoch 336 / 500 | iteration 20 / 30 | Total Loss: 3.463026285171509 | KNN Loss: 2.456040143966675 | BCE Loss: 1.006986141204834\n",
      "Epoch 336 / 500 | iteration 25 / 30 | Total Loss: 3.4248859882354736 | KNN Loss: 2.4256529808044434 | BCE Loss: 0.9992329478263855\n",
      "Epoch 337 / 500 | iteration 0 / 30 | Total Loss: 3.4637105464935303 | KNN Loss: 2.4526007175445557 | BCE Loss: 1.0111098289489746\n",
      "Epoch 337 / 500 | iteration 5 / 30 | Total Loss: 3.4532077312469482 | KNN Loss: 2.4552886486053467 | BCE Loss: 0.9979190826416016\n",
      "Epoch 337 / 500 | iteration 10 / 30 | Total Loss: 3.4824793338775635 | KNN Loss: 2.439635753631592 | BCE Loss: 1.0428435802459717\n",
      "Epoch 337 / 500 | iteration 15 / 30 | Total Loss: 3.4819109439849854 | KNN Loss: 2.4681200981140137 | BCE Loss: 1.0137908458709717\n",
      "Epoch 337 / 500 | iteration 20 / 30 | Total Loss: 3.4308018684387207 | KNN Loss: 2.4296271800994873 | BCE Loss: 1.0011746883392334\n",
      "Epoch 337 / 500 | iteration 25 / 30 | Total Loss: 3.4629597663879395 | KNN Loss: 2.4523696899414062 | BCE Loss: 1.0105900764465332\n",
      "Epoch 338 / 500 | iteration 0 / 30 | Total Loss: 3.4363067150115967 | KNN Loss: 2.4604599475860596 | BCE Loss: 0.9758467078208923\n",
      "Epoch 338 / 500 | iteration 5 / 30 | Total Loss: 3.482069969177246 | KNN Loss: 2.4630587100982666 | BCE Loss: 1.0190112590789795\n",
      "Epoch 338 / 500 | iteration 10 / 30 | Total Loss: 3.439568042755127 | KNN Loss: 2.435453176498413 | BCE Loss: 1.0041148662567139\n",
      "Epoch 338 / 500 | iteration 15 / 30 | Total Loss: 3.4608535766601562 | KNN Loss: 2.448537588119507 | BCE Loss: 1.0123158693313599\n",
      "Epoch 338 / 500 | iteration 20 / 30 | Total Loss: 3.464493989944458 | KNN Loss: 2.466643810272217 | BCE Loss: 0.9978501200675964\n",
      "Epoch 338 / 500 | iteration 25 / 30 | Total Loss: 3.5239052772521973 | KNN Loss: 2.495734691619873 | BCE Loss: 1.0281705856323242\n",
      "Epoch 339 / 500 | iteration 0 / 30 | Total Loss: 3.506061315536499 | KNN Loss: 2.463385820388794 | BCE Loss: 1.042675495147705\n",
      "Epoch 339 / 500 | iteration 5 / 30 | Total Loss: 3.4408981800079346 | KNN Loss: 2.4583020210266113 | BCE Loss: 0.9825961589813232\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 339 / 500 | iteration 10 / 30 | Total Loss: 3.4653587341308594 | KNN Loss: 2.474175453186035 | BCE Loss: 0.9911832213401794\n",
      "Epoch 339 / 500 | iteration 15 / 30 | Total Loss: 3.4786505699157715 | KNN Loss: 2.444380760192871 | BCE Loss: 1.03426992893219\n",
      "Epoch 339 / 500 | iteration 20 / 30 | Total Loss: 3.4617252349853516 | KNN Loss: 2.4548959732055664 | BCE Loss: 1.0068292617797852\n",
      "Epoch 339 / 500 | iteration 25 / 30 | Total Loss: 3.4984078407287598 | KNN Loss: 2.4705822467803955 | BCE Loss: 1.0278254747390747\n",
      "Epoch 340 / 500 | iteration 0 / 30 | Total Loss: 3.4546103477478027 | KNN Loss: 2.452166795730591 | BCE Loss: 1.002443552017212\n",
      "Epoch 340 / 500 | iteration 5 / 30 | Total Loss: 3.4487791061401367 | KNN Loss: 2.4425504207611084 | BCE Loss: 1.0062286853790283\n",
      "Epoch 340 / 500 | iteration 10 / 30 | Total Loss: 3.507533073425293 | KNN Loss: 2.469407558441162 | BCE Loss: 1.0381255149841309\n",
      "Epoch 340 / 500 | iteration 15 / 30 | Total Loss: 3.4662647247314453 | KNN Loss: 2.435795783996582 | BCE Loss: 1.0304690599441528\n",
      "Epoch 340 / 500 | iteration 20 / 30 | Total Loss: 3.4787890911102295 | KNN Loss: 2.4587814807891846 | BCE Loss: 1.020007610321045\n",
      "Epoch 340 / 500 | iteration 25 / 30 | Total Loss: 3.463099956512451 | KNN Loss: 2.4178290367126465 | BCE Loss: 1.0452710390090942\n",
      "Epoch 341 / 500 | iteration 0 / 30 | Total Loss: 3.4267420768737793 | KNN Loss: 2.411226272583008 | BCE Loss: 1.0155158042907715\n",
      "Epoch 341 / 500 | iteration 5 / 30 | Total Loss: 3.4709370136260986 | KNN Loss: 2.4487226009368896 | BCE Loss: 1.022214412689209\n",
      "Epoch 341 / 500 | iteration 10 / 30 | Total Loss: 3.4648666381835938 | KNN Loss: 2.4400525093078613 | BCE Loss: 1.0248140096664429\n",
      "Epoch 341 / 500 | iteration 15 / 30 | Total Loss: 3.4508581161499023 | KNN Loss: 2.474604606628418 | BCE Loss: 0.9762536287307739\n",
      "Epoch 341 / 500 | iteration 20 / 30 | Total Loss: 3.4564106464385986 | KNN Loss: 2.449958324432373 | BCE Loss: 1.0064523220062256\n",
      "Epoch 341 / 500 | iteration 25 / 30 | Total Loss: 3.4661431312561035 | KNN Loss: 2.450495481491089 | BCE Loss: 1.0156476497650146\n",
      "Epoch 342 / 500 | iteration 0 / 30 | Total Loss: 3.4737110137939453 | KNN Loss: 2.466825008392334 | BCE Loss: 1.0068860054016113\n",
      "Epoch 342 / 500 | iteration 5 / 30 | Total Loss: 3.433790683746338 | KNN Loss: 2.459754705429077 | BCE Loss: 0.9740360379219055\n",
      "Epoch 342 / 500 | iteration 10 / 30 | Total Loss: 3.464244842529297 | KNN Loss: 2.4528188705444336 | BCE Loss: 1.0114260911941528\n",
      "Epoch 342 / 500 | iteration 15 / 30 | Total Loss: 3.52305269241333 | KNN Loss: 2.486382246017456 | BCE Loss: 1.036670446395874\n",
      "Epoch 342 / 500 | iteration 20 / 30 | Total Loss: 3.4729058742523193 | KNN Loss: 2.441333293914795 | BCE Loss: 1.0315725803375244\n",
      "Epoch 342 / 500 | iteration 25 / 30 | Total Loss: 3.475321054458618 | KNN Loss: 2.441620349884033 | BCE Loss: 1.033700704574585\n",
      "Epoch 343 / 500 | iteration 0 / 30 | Total Loss: 3.493029832839966 | KNN Loss: 2.4664509296417236 | BCE Loss: 1.0265789031982422\n",
      "Epoch 343 / 500 | iteration 5 / 30 | Total Loss: 3.4408931732177734 | KNN Loss: 2.4238462448120117 | BCE Loss: 1.0170469284057617\n",
      "Epoch 343 / 500 | iteration 10 / 30 | Total Loss: 3.4697318077087402 | KNN Loss: 2.442115068435669 | BCE Loss: 1.0276167392730713\n",
      "Epoch 343 / 500 | iteration 15 / 30 | Total Loss: 3.4578163623809814 | KNN Loss: 2.450515031814575 | BCE Loss: 1.0073013305664062\n",
      "Epoch 343 / 500 | iteration 20 / 30 | Total Loss: 3.450650215148926 | KNN Loss: 2.4240057468414307 | BCE Loss: 1.0266445875167847\n",
      "Epoch 343 / 500 | iteration 25 / 30 | Total Loss: 3.509049892425537 | KNN Loss: 2.4522316455841064 | BCE Loss: 1.0568182468414307\n",
      "Epoch 344 / 500 | iteration 0 / 30 | Total Loss: 3.4598517417907715 | KNN Loss: 2.452116012573242 | BCE Loss: 1.0077356100082397\n",
      "Epoch 344 / 500 | iteration 5 / 30 | Total Loss: 3.458757162094116 | KNN Loss: 2.4431099891662598 | BCE Loss: 1.0156471729278564\n",
      "Epoch 344 / 500 | iteration 10 / 30 | Total Loss: 3.43294095993042 | KNN Loss: 2.430619716644287 | BCE Loss: 1.0023212432861328\n",
      "Epoch 344 / 500 | iteration 15 / 30 | Total Loss: 3.4592103958129883 | KNN Loss: 2.4400384426116943 | BCE Loss: 1.0191718339920044\n",
      "Epoch 344 / 500 | iteration 20 / 30 | Total Loss: 3.51377534866333 | KNN Loss: 2.4674644470214844 | BCE Loss: 1.0463109016418457\n",
      "Epoch 344 / 500 | iteration 25 / 30 | Total Loss: 3.4825539588928223 | KNN Loss: 2.4419379234313965 | BCE Loss: 1.0406160354614258\n",
      "Epoch 345 / 500 | iteration 0 / 30 | Total Loss: 3.4633686542510986 | KNN Loss: 2.422591209411621 | BCE Loss: 1.0407774448394775\n",
      "Epoch 345 / 500 | iteration 5 / 30 | Total Loss: 3.446732997894287 | KNN Loss: 2.4289133548736572 | BCE Loss: 1.0178195238113403\n",
      "Epoch 345 / 500 | iteration 10 / 30 | Total Loss: 3.465786933898926 | KNN Loss: 2.45042085647583 | BCE Loss: 1.0153660774230957\n",
      "Epoch 345 / 500 | iteration 15 / 30 | Total Loss: 3.489361047744751 | KNN Loss: 2.476470947265625 | BCE Loss: 1.012890100479126\n",
      "Epoch 345 / 500 | iteration 20 / 30 | Total Loss: 3.4404852390289307 | KNN Loss: 2.41927170753479 | BCE Loss: 1.0212135314941406\n",
      "Epoch 345 / 500 | iteration 25 / 30 | Total Loss: 3.491349220275879 | KNN Loss: 2.458653450012207 | BCE Loss: 1.0326957702636719\n",
      "Epoch 346 / 500 | iteration 0 / 30 | Total Loss: 3.428874969482422 | KNN Loss: 2.4410414695739746 | BCE Loss: 0.9878336191177368\n",
      "Epoch 346 / 500 | iteration 5 / 30 | Total Loss: 3.416200876235962 | KNN Loss: 2.431859254837036 | BCE Loss: 0.9843416213989258\n",
      "Epoch 346 / 500 | iteration 10 / 30 | Total Loss: 3.50691556930542 | KNN Loss: 2.4851455688476562 | BCE Loss: 1.0217700004577637\n",
      "Epoch 346 / 500 | iteration 15 / 30 | Total Loss: 3.5117909908294678 | KNN Loss: 2.4579079151153564 | BCE Loss: 1.0538830757141113\n",
      "Epoch 346 / 500 | iteration 20 / 30 | Total Loss: 3.4739439487457275 | KNN Loss: 2.43249249458313 | BCE Loss: 1.0414514541625977\n",
      "Epoch 346 / 500 | iteration 25 / 30 | Total Loss: 3.5081214904785156 | KNN Loss: 2.484405279159546 | BCE Loss: 1.0237162113189697\n",
      "Epoch 347 / 500 | iteration 0 / 30 | Total Loss: 3.485417604446411 | KNN Loss: 2.4466850757598877 | BCE Loss: 1.0387325286865234\n",
      "Epoch 347 / 500 | iteration 5 / 30 | Total Loss: 3.4668846130371094 | KNN Loss: 2.4632937908172607 | BCE Loss: 1.003590703010559\n",
      "Epoch 347 / 500 | iteration 10 / 30 | Total Loss: 3.4843013286590576 | KNN Loss: 2.4603612422943115 | BCE Loss: 1.023940086364746\n",
      "Epoch 347 / 500 | iteration 15 / 30 | Total Loss: 3.468752861022949 | KNN Loss: 2.437385320663452 | BCE Loss: 1.0313676595687866\n",
      "Epoch 347 / 500 | iteration 20 / 30 | Total Loss: 3.4729461669921875 | KNN Loss: 2.4258761405944824 | BCE Loss: 1.0470699071884155\n",
      "Epoch 347 / 500 | iteration 25 / 30 | Total Loss: 3.4268882274627686 | KNN Loss: 2.4496994018554688 | BCE Loss: 0.9771888256072998\n",
      "Epoch 348 / 500 | iteration 0 / 30 | Total Loss: 3.428180694580078 | KNN Loss: 2.44731068611145 | BCE Loss: 0.9808701276779175\n",
      "Epoch 348 / 500 | iteration 5 / 30 | Total Loss: 3.465867280960083 | KNN Loss: 2.434948444366455 | BCE Loss: 1.030918836593628\n",
      "Epoch 348 / 500 | iteration 10 / 30 | Total Loss: 3.446192741394043 | KNN Loss: 2.440666437149048 | BCE Loss: 1.0055264234542847\n",
      "Epoch 348 / 500 | iteration 15 / 30 | Total Loss: 3.473997116088867 | KNN Loss: 2.49119234085083 | BCE Loss: 0.9828047752380371\n",
      "Epoch 348 / 500 | iteration 20 / 30 | Total Loss: 3.4495038986206055 | KNN Loss: 2.4496750831604004 | BCE Loss: 0.9998288154602051\n",
      "Epoch 348 / 500 | iteration 25 / 30 | Total Loss: 3.482314109802246 | KNN Loss: 2.4450995922088623 | BCE Loss: 1.0372145175933838\n",
      "Epoch   349: reducing learning rate of group 0 to 3.3911e-05.\n",
      "Epoch 349 / 500 | iteration 0 / 30 | Total Loss: 3.463261604309082 | KNN Loss: 2.4572644233703613 | BCE Loss: 1.0059973001480103\n",
      "Epoch 349 / 500 | iteration 5 / 30 | Total Loss: 3.479323387145996 | KNN Loss: 2.468050718307495 | BCE Loss: 1.0112725496292114\n",
      "Epoch 349 / 500 | iteration 10 / 30 | Total Loss: 3.4683732986450195 | KNN Loss: 2.4474828243255615 | BCE Loss: 1.0208903551101685\n",
      "Epoch 349 / 500 | iteration 15 / 30 | Total Loss: 3.458249807357788 | KNN Loss: 2.4345264434814453 | BCE Loss: 1.0237233638763428\n",
      "Epoch 349 / 500 | iteration 20 / 30 | Total Loss: 3.50510573387146 | KNN Loss: 2.477072238922119 | BCE Loss: 1.0280334949493408\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 349 / 500 | iteration 25 / 30 | Total Loss: 3.4649462699890137 | KNN Loss: 2.447113275527954 | BCE Loss: 1.0178329944610596\n",
      "Epoch 350 / 500 | iteration 0 / 30 | Total Loss: 3.4282803535461426 | KNN Loss: 2.448997735977173 | BCE Loss: 0.9792827367782593\n",
      "Epoch 350 / 500 | iteration 5 / 30 | Total Loss: 3.42014217376709 | KNN Loss: 2.41744327545166 | BCE Loss: 1.0026988983154297\n",
      "Epoch 350 / 500 | iteration 10 / 30 | Total Loss: 3.465639352798462 | KNN Loss: 2.444157123565674 | BCE Loss: 1.021482229232788\n",
      "Epoch 350 / 500 | iteration 15 / 30 | Total Loss: 3.4842031002044678 | KNN Loss: 2.469306707382202 | BCE Loss: 1.0148963928222656\n",
      "Epoch 350 / 500 | iteration 20 / 30 | Total Loss: 3.453723192214966 | KNN Loss: 2.4256370067596436 | BCE Loss: 1.0280861854553223\n",
      "Epoch 350 / 500 | iteration 25 / 30 | Total Loss: 3.4656639099121094 | KNN Loss: 2.465247631072998 | BCE Loss: 1.0004163980484009\n",
      "Epoch 351 / 500 | iteration 0 / 30 | Total Loss: 3.4614477157592773 | KNN Loss: 2.422975778579712 | BCE Loss: 1.0384718179702759\n",
      "Epoch 351 / 500 | iteration 5 / 30 | Total Loss: 3.463893413543701 | KNN Loss: 2.460951805114746 | BCE Loss: 1.002941608428955\n",
      "Epoch 351 / 500 | iteration 10 / 30 | Total Loss: 3.5239078998565674 | KNN Loss: 2.483205556869507 | BCE Loss: 1.0407023429870605\n",
      "Epoch 351 / 500 | iteration 15 / 30 | Total Loss: 3.4381015300750732 | KNN Loss: 2.432222366333008 | BCE Loss: 1.0058791637420654\n",
      "Epoch 351 / 500 | iteration 20 / 30 | Total Loss: 3.520155668258667 | KNN Loss: 2.4837565422058105 | BCE Loss: 1.0363991260528564\n",
      "Epoch 351 / 500 | iteration 25 / 30 | Total Loss: 3.478423595428467 | KNN Loss: 2.46003794670105 | BCE Loss: 1.018385648727417\n",
      "Epoch 352 / 500 | iteration 0 / 30 | Total Loss: 3.442800283432007 | KNN Loss: 2.4464032649993896 | BCE Loss: 0.9963970184326172\n",
      "Epoch 352 / 500 | iteration 5 / 30 | Total Loss: 3.466681480407715 | KNN Loss: 2.4435601234436035 | BCE Loss: 1.0231213569641113\n",
      "Epoch 352 / 500 | iteration 10 / 30 | Total Loss: 3.4850716590881348 | KNN Loss: 2.4709596633911133 | BCE Loss: 1.014112114906311\n",
      "Epoch 352 / 500 | iteration 15 / 30 | Total Loss: 3.4514076709747314 | KNN Loss: 2.4596056938171387 | BCE Loss: 0.9918019771575928\n",
      "Epoch 352 / 500 | iteration 20 / 30 | Total Loss: 3.4531192779541016 | KNN Loss: 2.4489810466766357 | BCE Loss: 1.0041381120681763\n",
      "Epoch 352 / 500 | iteration 25 / 30 | Total Loss: 3.4594948291778564 | KNN Loss: 2.4256837368011475 | BCE Loss: 1.033811092376709\n",
      "Epoch 353 / 500 | iteration 0 / 30 | Total Loss: 3.4364559650421143 | KNN Loss: 2.442324161529541 | BCE Loss: 0.9941317439079285\n",
      "Epoch 353 / 500 | iteration 5 / 30 | Total Loss: 3.4209113121032715 | KNN Loss: 2.425793409347534 | BCE Loss: 0.9951178431510925\n",
      "Epoch 353 / 500 | iteration 10 / 30 | Total Loss: 3.497117042541504 | KNN Loss: 2.45182466506958 | BCE Loss: 1.0452924966812134\n",
      "Epoch 353 / 500 | iteration 15 / 30 | Total Loss: 3.509478807449341 | KNN Loss: 2.464853525161743 | BCE Loss: 1.0446252822875977\n",
      "Epoch 353 / 500 | iteration 20 / 30 | Total Loss: 3.482816696166992 | KNN Loss: 2.4684293270111084 | BCE Loss: 1.0143874883651733\n",
      "Epoch 353 / 500 | iteration 25 / 30 | Total Loss: 3.479731559753418 | KNN Loss: 2.465404748916626 | BCE Loss: 1.0143266916275024\n",
      "Epoch 354 / 500 | iteration 0 / 30 | Total Loss: 3.463078022003174 | KNN Loss: 2.4672391414642334 | BCE Loss: 0.9958388805389404\n",
      "Epoch 354 / 500 | iteration 5 / 30 | Total Loss: 3.460352897644043 | KNN Loss: 2.439232110977173 | BCE Loss: 1.0211207866668701\n",
      "Epoch 354 / 500 | iteration 10 / 30 | Total Loss: 3.4839136600494385 | KNN Loss: 2.4680583477020264 | BCE Loss: 1.015855312347412\n",
      "Epoch 354 / 500 | iteration 15 / 30 | Total Loss: 3.491957187652588 | KNN Loss: 2.439849853515625 | BCE Loss: 1.0521074533462524\n",
      "Epoch 354 / 500 | iteration 20 / 30 | Total Loss: 3.4827051162719727 | KNN Loss: 2.47127103805542 | BCE Loss: 1.0114340782165527\n",
      "Epoch 354 / 500 | iteration 25 / 30 | Total Loss: 3.4813976287841797 | KNN Loss: 2.475644826889038 | BCE Loss: 1.0057528018951416\n",
      "Epoch 355 / 500 | iteration 0 / 30 | Total Loss: 3.473330497741699 | KNN Loss: 2.4560065269470215 | BCE Loss: 1.0173239707946777\n",
      "Epoch 355 / 500 | iteration 5 / 30 | Total Loss: 3.458563804626465 | KNN Loss: 2.4302456378936768 | BCE Loss: 1.0283180475234985\n",
      "Epoch 355 / 500 | iteration 10 / 30 | Total Loss: 3.465052366256714 | KNN Loss: 2.447638750076294 | BCE Loss: 1.01741361618042\n",
      "Epoch 355 / 500 | iteration 15 / 30 | Total Loss: 3.4717941284179688 | KNN Loss: 2.464893102645874 | BCE Loss: 1.0069009065628052\n",
      "Epoch 355 / 500 | iteration 20 / 30 | Total Loss: 3.4587764739990234 | KNN Loss: 2.436075448989868 | BCE Loss: 1.0227009057998657\n",
      "Epoch 355 / 500 | iteration 25 / 30 | Total Loss: 3.472058057785034 | KNN Loss: 2.463019609451294 | BCE Loss: 1.0090384483337402\n",
      "Epoch 356 / 500 | iteration 0 / 30 | Total Loss: 3.470252752304077 | KNN Loss: 2.445509433746338 | BCE Loss: 1.0247433185577393\n",
      "Epoch 356 / 500 | iteration 5 / 30 | Total Loss: 3.451730966567993 | KNN Loss: 2.418255567550659 | BCE Loss: 1.033475399017334\n",
      "Epoch 356 / 500 | iteration 10 / 30 | Total Loss: 3.479501962661743 | KNN Loss: 2.4569826126098633 | BCE Loss: 1.0225193500518799\n",
      "Epoch 356 / 500 | iteration 15 / 30 | Total Loss: 3.4556384086608887 | KNN Loss: 2.4608685970306396 | BCE Loss: 0.9947696924209595\n",
      "Epoch 356 / 500 | iteration 20 / 30 | Total Loss: 3.466622829437256 | KNN Loss: 2.450726270675659 | BCE Loss: 1.0158966779708862\n",
      "Epoch 356 / 500 | iteration 25 / 30 | Total Loss: 3.4436118602752686 | KNN Loss: 2.461435556411743 | BCE Loss: 0.9821763038635254\n",
      "Epoch 357 / 500 | iteration 0 / 30 | Total Loss: 3.436508893966675 | KNN Loss: 2.4309911727905273 | BCE Loss: 1.0055177211761475\n",
      "Epoch 357 / 500 | iteration 5 / 30 | Total Loss: 3.4784274101257324 | KNN Loss: 2.4579081535339355 | BCE Loss: 1.0205192565917969\n",
      "Epoch 357 / 500 | iteration 10 / 30 | Total Loss: 3.5135629177093506 | KNN Loss: 2.4545743465423584 | BCE Loss: 1.0589885711669922\n",
      "Epoch 357 / 500 | iteration 15 / 30 | Total Loss: 3.4832639694213867 | KNN Loss: 2.474391222000122 | BCE Loss: 1.008872628211975\n",
      "Epoch 357 / 500 | iteration 20 / 30 | Total Loss: 3.4801955223083496 | KNN Loss: 2.4530091285705566 | BCE Loss: 1.027186393737793\n",
      "Epoch 357 / 500 | iteration 25 / 30 | Total Loss: 3.4798593521118164 | KNN Loss: 2.4575085639953613 | BCE Loss: 1.0223509073257446\n",
      "Epoch 358 / 500 | iteration 0 / 30 | Total Loss: 3.485069990158081 | KNN Loss: 2.464615821838379 | BCE Loss: 1.0204541683197021\n",
      "Epoch 358 / 500 | iteration 5 / 30 | Total Loss: 3.481602191925049 | KNN Loss: 2.436549186706543 | BCE Loss: 1.0450531244277954\n",
      "Epoch 358 / 500 | iteration 10 / 30 | Total Loss: 3.446187973022461 | KNN Loss: 2.4468142986297607 | BCE Loss: 0.9993736743927002\n",
      "Epoch 358 / 500 | iteration 15 / 30 | Total Loss: 3.498392105102539 | KNN Loss: 2.4647066593170166 | BCE Loss: 1.0336854457855225\n",
      "Epoch 358 / 500 | iteration 20 / 30 | Total Loss: 3.4645535945892334 | KNN Loss: 2.451040506362915 | BCE Loss: 1.0135130882263184\n",
      "Epoch 358 / 500 | iteration 25 / 30 | Total Loss: 3.4544754028320312 | KNN Loss: 2.417711019515991 | BCE Loss: 1.0367642641067505\n",
      "Epoch 359 / 500 | iteration 0 / 30 | Total Loss: 3.4786972999572754 | KNN Loss: 2.479323625564575 | BCE Loss: 0.9993735551834106\n",
      "Epoch 359 / 500 | iteration 5 / 30 | Total Loss: 3.4376282691955566 | KNN Loss: 2.422450065612793 | BCE Loss: 1.0151782035827637\n",
      "Epoch 359 / 500 | iteration 10 / 30 | Total Loss: 3.434257745742798 | KNN Loss: 2.4396705627441406 | BCE Loss: 0.9945871829986572\n",
      "Epoch 359 / 500 | iteration 15 / 30 | Total Loss: 3.4075732231140137 | KNN Loss: 2.4292593002319336 | BCE Loss: 0.9783139228820801\n",
      "Epoch 359 / 500 | iteration 20 / 30 | Total Loss: 3.4810242652893066 | KNN Loss: 2.4474549293518066 | BCE Loss: 1.0335693359375\n",
      "Epoch 359 / 500 | iteration 25 / 30 | Total Loss: 3.4946987628936768 | KNN Loss: 2.4682343006134033 | BCE Loss: 1.0264644622802734\n",
      "Epoch   360: reducing learning rate of group 0 to 2.3738e-05.\n",
      "Epoch 360 / 500 | iteration 0 / 30 | Total Loss: 3.508551597595215 | KNN Loss: 2.4732730388641357 | BCE Loss: 1.035278558731079\n",
      "Epoch 360 / 500 | iteration 5 / 30 | Total Loss: 3.4857261180877686 | KNN Loss: 2.472893238067627 | BCE Loss: 1.0128328800201416\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 360 / 500 | iteration 10 / 30 | Total Loss: 3.442615032196045 | KNN Loss: 2.430036783218384 | BCE Loss: 1.0125782489776611\n",
      "Epoch 360 / 500 | iteration 15 / 30 | Total Loss: 3.413458824157715 | KNN Loss: 2.4186089038848877 | BCE Loss: 0.9948499202728271\n",
      "Epoch 360 / 500 | iteration 20 / 30 | Total Loss: 3.5144314765930176 | KNN Loss: 2.482703924179077 | BCE Loss: 1.03172767162323\n",
      "Epoch 360 / 500 | iteration 25 / 30 | Total Loss: 3.4646236896514893 | KNN Loss: 2.4516429901123047 | BCE Loss: 1.0129806995391846\n",
      "Epoch 361 / 500 | iteration 0 / 30 | Total Loss: 3.5039286613464355 | KNN Loss: 2.4803898334503174 | BCE Loss: 1.0235388278961182\n",
      "Epoch 361 / 500 | iteration 5 / 30 | Total Loss: 3.4024136066436768 | KNN Loss: 2.4189484119415283 | BCE Loss: 0.9834652543067932\n",
      "Epoch 361 / 500 | iteration 10 / 30 | Total Loss: 3.475543975830078 | KNN Loss: 2.477698802947998 | BCE Loss: 0.9978450536727905\n",
      "Epoch 361 / 500 | iteration 15 / 30 | Total Loss: 3.463165283203125 | KNN Loss: 2.4322941303253174 | BCE Loss: 1.0308711528778076\n",
      "Epoch 361 / 500 | iteration 20 / 30 | Total Loss: 3.464468002319336 | KNN Loss: 2.4589462280273438 | BCE Loss: 1.0055217742919922\n",
      "Epoch 361 / 500 | iteration 25 / 30 | Total Loss: 3.4606761932373047 | KNN Loss: 2.4588215351104736 | BCE Loss: 1.001854658126831\n",
      "Epoch 362 / 500 | iteration 0 / 30 | Total Loss: 3.4729971885681152 | KNN Loss: 2.468134880065918 | BCE Loss: 1.0048621892929077\n",
      "Epoch 362 / 500 | iteration 5 / 30 | Total Loss: 3.4559545516967773 | KNN Loss: 2.457540273666382 | BCE Loss: 0.9984143972396851\n",
      "Epoch 362 / 500 | iteration 10 / 30 | Total Loss: 3.4714553356170654 | KNN Loss: 2.4290921688079834 | BCE Loss: 1.042363166809082\n",
      "Epoch 362 / 500 | iteration 15 / 30 | Total Loss: 3.4673640727996826 | KNN Loss: 2.4457485675811768 | BCE Loss: 1.0216155052185059\n",
      "Epoch 362 / 500 | iteration 20 / 30 | Total Loss: 3.481853485107422 | KNN Loss: 2.4640395641326904 | BCE Loss: 1.0178139209747314\n",
      "Epoch 362 / 500 | iteration 25 / 30 | Total Loss: 3.487509250640869 | KNN Loss: 2.474461317062378 | BCE Loss: 1.0130479335784912\n",
      "Epoch 363 / 500 | iteration 0 / 30 | Total Loss: 3.4863336086273193 | KNN Loss: 2.4509239196777344 | BCE Loss: 1.035409688949585\n",
      "Epoch 363 / 500 | iteration 5 / 30 | Total Loss: 3.4758288860321045 | KNN Loss: 2.420306444168091 | BCE Loss: 1.0555224418640137\n",
      "Epoch 363 / 500 | iteration 10 / 30 | Total Loss: 3.443671703338623 | KNN Loss: 2.4230401515960693 | BCE Loss: 1.0206316709518433\n",
      "Epoch 363 / 500 | iteration 15 / 30 | Total Loss: 3.494072198867798 | KNN Loss: 2.485442638397217 | BCE Loss: 1.008629560470581\n",
      "Epoch 363 / 500 | iteration 20 / 30 | Total Loss: 3.4541478157043457 | KNN Loss: 2.4462480545043945 | BCE Loss: 1.0078996419906616\n",
      "Epoch 363 / 500 | iteration 25 / 30 | Total Loss: 3.4438796043395996 | KNN Loss: 2.4302902221679688 | BCE Loss: 1.0135892629623413\n",
      "Epoch 364 / 500 | iteration 0 / 30 | Total Loss: 3.440810203552246 | KNN Loss: 2.437554121017456 | BCE Loss: 1.0032562017440796\n",
      "Epoch 364 / 500 | iteration 5 / 30 | Total Loss: 3.461723566055298 | KNN Loss: 2.4609596729278564 | BCE Loss: 1.0007638931274414\n",
      "Epoch 364 / 500 | iteration 10 / 30 | Total Loss: 3.461735963821411 | KNN Loss: 2.45579195022583 | BCE Loss: 1.005944013595581\n",
      "Epoch 364 / 500 | iteration 15 / 30 | Total Loss: 3.473592519760132 | KNN Loss: 2.4347310066223145 | BCE Loss: 1.0388615131378174\n",
      "Epoch 364 / 500 | iteration 20 / 30 | Total Loss: 3.4381275177001953 | KNN Loss: 2.416120767593384 | BCE Loss: 1.0220067501068115\n",
      "Epoch 364 / 500 | iteration 25 / 30 | Total Loss: 3.4536290168762207 | KNN Loss: 2.443110466003418 | BCE Loss: 1.0105185508728027\n",
      "Epoch 365 / 500 | iteration 0 / 30 | Total Loss: 3.5070414543151855 | KNN Loss: 2.4997432231903076 | BCE Loss: 1.0072981119155884\n",
      "Epoch 365 / 500 | iteration 5 / 30 | Total Loss: 3.4673080444335938 | KNN Loss: 2.4405508041381836 | BCE Loss: 1.0267572402954102\n",
      "Epoch 365 / 500 | iteration 10 / 30 | Total Loss: 3.451277494430542 | KNN Loss: 2.4220480918884277 | BCE Loss: 1.0292294025421143\n",
      "Epoch 365 / 500 | iteration 15 / 30 | Total Loss: 3.4591715335845947 | KNN Loss: 2.449535846710205 | BCE Loss: 1.0096356868743896\n",
      "Epoch 365 / 500 | iteration 20 / 30 | Total Loss: 3.4584579467773438 | KNN Loss: 2.452834129333496 | BCE Loss: 1.0056238174438477\n",
      "Epoch 365 / 500 | iteration 25 / 30 | Total Loss: 3.468916177749634 | KNN Loss: 2.4869611263275146 | BCE Loss: 0.9819550514221191\n",
      "Epoch 366 / 500 | iteration 0 / 30 | Total Loss: 3.4772984981536865 | KNN Loss: 2.457179307937622 | BCE Loss: 1.0201191902160645\n",
      "Epoch 366 / 500 | iteration 5 / 30 | Total Loss: 3.422631025314331 | KNN Loss: 2.4168508052825928 | BCE Loss: 1.0057802200317383\n",
      "Epoch 366 / 500 | iteration 10 / 30 | Total Loss: 3.455538749694824 | KNN Loss: 2.4352707862854004 | BCE Loss: 1.0202678442001343\n",
      "Epoch 366 / 500 | iteration 15 / 30 | Total Loss: 3.4589650630950928 | KNN Loss: 2.4531214237213135 | BCE Loss: 1.0058436393737793\n",
      "Epoch 366 / 500 | iteration 20 / 30 | Total Loss: 3.49282169342041 | KNN Loss: 2.4787533283233643 | BCE Loss: 1.014068365097046\n",
      "Epoch 366 / 500 | iteration 25 / 30 | Total Loss: 3.4301438331604004 | KNN Loss: 2.4030723571777344 | BCE Loss: 1.0270713567733765\n",
      "Epoch 367 / 500 | iteration 0 / 30 | Total Loss: 3.479679584503174 | KNN Loss: 2.459352731704712 | BCE Loss: 1.0203269720077515\n",
      "Epoch 367 / 500 | iteration 5 / 30 | Total Loss: 3.4480068683624268 | KNN Loss: 2.464128255844116 | BCE Loss: 0.9838785529136658\n",
      "Epoch 367 / 500 | iteration 10 / 30 | Total Loss: 3.503103733062744 | KNN Loss: 2.5017354488372803 | BCE Loss: 1.0013682842254639\n",
      "Epoch 367 / 500 | iteration 15 / 30 | Total Loss: 3.4573042392730713 | KNN Loss: 2.4575676918029785 | BCE Loss: 0.9997366070747375\n",
      "Epoch 367 / 500 | iteration 20 / 30 | Total Loss: 3.520268440246582 | KNN Loss: 2.4829838275909424 | BCE Loss: 1.0372846126556396\n",
      "Epoch 367 / 500 | iteration 25 / 30 | Total Loss: 3.4403295516967773 | KNN Loss: 2.435771942138672 | BCE Loss: 1.0045576095581055\n",
      "Epoch 368 / 500 | iteration 0 / 30 | Total Loss: 3.4426326751708984 | KNN Loss: 2.4567923545837402 | BCE Loss: 0.9858402013778687\n",
      "Epoch 368 / 500 | iteration 5 / 30 | Total Loss: 3.481613874435425 | KNN Loss: 2.4426662921905518 | BCE Loss: 1.038947582244873\n",
      "Epoch 368 / 500 | iteration 10 / 30 | Total Loss: 3.451028347015381 | KNN Loss: 2.450451374053955 | BCE Loss: 1.0005770921707153\n",
      "Epoch 368 / 500 | iteration 15 / 30 | Total Loss: 3.49416184425354 | KNN Loss: 2.467629909515381 | BCE Loss: 1.0265319347381592\n",
      "Epoch 368 / 500 | iteration 20 / 30 | Total Loss: 3.4719552993774414 | KNN Loss: 2.4416441917419434 | BCE Loss: 1.030311107635498\n",
      "Epoch 368 / 500 | iteration 25 / 30 | Total Loss: 3.457181692123413 | KNN Loss: 2.4492852687835693 | BCE Loss: 1.0078964233398438\n",
      "Epoch 369 / 500 | iteration 0 / 30 | Total Loss: 3.4103145599365234 | KNN Loss: 2.430041790008545 | BCE Loss: 0.9802727103233337\n",
      "Epoch 369 / 500 | iteration 5 / 30 | Total Loss: 3.485651969909668 | KNN Loss: 2.45635986328125 | BCE Loss: 1.0292919874191284\n",
      "Epoch 369 / 500 | iteration 10 / 30 | Total Loss: 3.437798023223877 | KNN Loss: 2.439157009124756 | BCE Loss: 0.9986409544944763\n",
      "Epoch 369 / 500 | iteration 15 / 30 | Total Loss: 3.5046277046203613 | KNN Loss: 2.456721782684326 | BCE Loss: 1.0479059219360352\n",
      "Epoch 369 / 500 | iteration 20 / 30 | Total Loss: 3.521195888519287 | KNN Loss: 2.4866058826446533 | BCE Loss: 1.0345901250839233\n",
      "Epoch 369 / 500 | iteration 25 / 30 | Total Loss: 3.4646072387695312 | KNN Loss: 2.4720470905303955 | BCE Loss: 0.9925600290298462\n",
      "Epoch 370 / 500 | iteration 0 / 30 | Total Loss: 3.4888100624084473 | KNN Loss: 2.440924644470215 | BCE Loss: 1.0478854179382324\n",
      "Epoch 370 / 500 | iteration 5 / 30 | Total Loss: 3.466060161590576 | KNN Loss: 2.4403936862945557 | BCE Loss: 1.0256664752960205\n",
      "Epoch 370 / 500 | iteration 10 / 30 | Total Loss: 3.4455370903015137 | KNN Loss: 2.4377012252807617 | BCE Loss: 1.007835865020752\n",
      "Epoch 370 / 500 | iteration 15 / 30 | Total Loss: 3.428309440612793 | KNN Loss: 2.4137556552886963 | BCE Loss: 1.0145537853240967\n",
      "Epoch 370 / 500 | iteration 20 / 30 | Total Loss: 3.454389810562134 | KNN Loss: 2.4735846519470215 | BCE Loss: 0.9808051586151123\n",
      "Epoch 370 / 500 | iteration 25 / 30 | Total Loss: 3.488002300262451 | KNN Loss: 2.465099811553955 | BCE Loss: 1.0229026079177856\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   371: reducing learning rate of group 0 to 1.6616e-05.\n",
      "Epoch 371 / 500 | iteration 0 / 30 | Total Loss: 3.4461357593536377 | KNN Loss: 2.433032989501953 | BCE Loss: 1.0131027698516846\n",
      "Epoch 371 / 500 | iteration 5 / 30 | Total Loss: 3.5088186264038086 | KNN Loss: 2.47672700881958 | BCE Loss: 1.0320916175842285\n",
      "Epoch 371 / 500 | iteration 10 / 30 | Total Loss: 3.442269802093506 | KNN Loss: 2.445831775665283 | BCE Loss: 0.9964380264282227\n",
      "Epoch 371 / 500 | iteration 15 / 30 | Total Loss: 3.407585620880127 | KNN Loss: 2.4274895191192627 | BCE Loss: 0.9800959825515747\n",
      "Epoch 371 / 500 | iteration 20 / 30 | Total Loss: 3.5030508041381836 | KNN Loss: 2.4553632736206055 | BCE Loss: 1.0476876497268677\n",
      "Epoch 371 / 500 | iteration 25 / 30 | Total Loss: 3.4787588119506836 | KNN Loss: 2.4788661003112793 | BCE Loss: 0.9998928308486938\n",
      "Epoch 372 / 500 | iteration 0 / 30 | Total Loss: 3.4816625118255615 | KNN Loss: 2.4454808235168457 | BCE Loss: 1.0361816883087158\n",
      "Epoch 372 / 500 | iteration 5 / 30 | Total Loss: 3.4661202430725098 | KNN Loss: 2.4541878700256348 | BCE Loss: 1.0119322538375854\n",
      "Epoch 372 / 500 | iteration 10 / 30 | Total Loss: 3.4449801445007324 | KNN Loss: 2.432675838470459 | BCE Loss: 1.0123043060302734\n",
      "Epoch 372 / 500 | iteration 15 / 30 | Total Loss: 3.456435441970825 | KNN Loss: 2.439826726913452 | BCE Loss: 1.016608715057373\n",
      "Epoch 372 / 500 | iteration 20 / 30 | Total Loss: 3.481748104095459 | KNN Loss: 2.4757802486419678 | BCE Loss: 1.0059677362442017\n",
      "Epoch 372 / 500 | iteration 25 / 30 | Total Loss: 3.489351272583008 | KNN Loss: 2.476480007171631 | BCE Loss: 1.012871265411377\n",
      "Epoch 373 / 500 | iteration 0 / 30 | Total Loss: 3.438309669494629 | KNN Loss: 2.447960376739502 | BCE Loss: 0.9903491735458374\n",
      "Epoch 373 / 500 | iteration 5 / 30 | Total Loss: 3.492079496383667 | KNN Loss: 2.475405216217041 | BCE Loss: 1.016674280166626\n",
      "Epoch 373 / 500 | iteration 10 / 30 | Total Loss: 3.4759671688079834 | KNN Loss: 2.435481071472168 | BCE Loss: 1.0404860973358154\n",
      "Epoch 373 / 500 | iteration 15 / 30 | Total Loss: 3.4875426292419434 | KNN Loss: 2.4690139293670654 | BCE Loss: 1.018528699874878\n",
      "Epoch 373 / 500 | iteration 20 / 30 | Total Loss: 3.4315805435180664 | KNN Loss: 2.443524122238159 | BCE Loss: 0.9880564212799072\n",
      "Epoch 373 / 500 | iteration 25 / 30 | Total Loss: 3.485053777694702 | KNN Loss: 2.449237108230591 | BCE Loss: 1.0358166694641113\n",
      "Epoch 374 / 500 | iteration 0 / 30 | Total Loss: 3.4680802822113037 | KNN Loss: 2.4422965049743652 | BCE Loss: 1.0257837772369385\n",
      "Epoch 374 / 500 | iteration 5 / 30 | Total Loss: 3.476130962371826 | KNN Loss: 2.449587345123291 | BCE Loss: 1.0265436172485352\n",
      "Epoch 374 / 500 | iteration 10 / 30 | Total Loss: 3.4975574016571045 | KNN Loss: 2.4881486892700195 | BCE Loss: 1.009408712387085\n",
      "Epoch 374 / 500 | iteration 15 / 30 | Total Loss: 3.4786226749420166 | KNN Loss: 2.4184844493865967 | BCE Loss: 1.06013822555542\n",
      "Epoch 374 / 500 | iteration 20 / 30 | Total Loss: 3.440530776977539 | KNN Loss: 2.4254162311553955 | BCE Loss: 1.015114665031433\n",
      "Epoch 374 / 500 | iteration 25 / 30 | Total Loss: 3.4664463996887207 | KNN Loss: 2.4565212726593018 | BCE Loss: 1.009925127029419\n",
      "Epoch 375 / 500 | iteration 0 / 30 | Total Loss: 3.470158576965332 | KNN Loss: 2.4476025104522705 | BCE Loss: 1.0225560665130615\n",
      "Epoch 375 / 500 | iteration 5 / 30 | Total Loss: 3.465477466583252 | KNN Loss: 2.4446282386779785 | BCE Loss: 1.0208491086959839\n",
      "Epoch 375 / 500 | iteration 10 / 30 | Total Loss: 3.468378782272339 | KNN Loss: 2.465529441833496 | BCE Loss: 1.0028493404388428\n",
      "Epoch 375 / 500 | iteration 15 / 30 | Total Loss: 3.4727346897125244 | KNN Loss: 2.457359790802002 | BCE Loss: 1.0153748989105225\n",
      "Epoch 375 / 500 | iteration 20 / 30 | Total Loss: 3.5034408569335938 | KNN Loss: 2.4686601161956787 | BCE Loss: 1.0347808599472046\n",
      "Epoch 375 / 500 | iteration 25 / 30 | Total Loss: 3.4549665451049805 | KNN Loss: 2.438443422317505 | BCE Loss: 1.016523003578186\n",
      "Epoch 376 / 500 | iteration 0 / 30 | Total Loss: 3.4592702388763428 | KNN Loss: 2.426854372024536 | BCE Loss: 1.0324158668518066\n",
      "Epoch 376 / 500 | iteration 5 / 30 | Total Loss: 3.4616940021514893 | KNN Loss: 2.44840931892395 | BCE Loss: 1.013284683227539\n",
      "Epoch 376 / 500 | iteration 10 / 30 | Total Loss: 3.4455184936523438 | KNN Loss: 2.436382293701172 | BCE Loss: 1.0091363191604614\n",
      "Epoch 376 / 500 | iteration 15 / 30 | Total Loss: 3.4608352184295654 | KNN Loss: 2.4628634452819824 | BCE Loss: 0.9979718327522278\n",
      "Epoch 376 / 500 | iteration 20 / 30 | Total Loss: 3.411221981048584 | KNN Loss: 2.4445242881774902 | BCE Loss: 0.9666976928710938\n",
      "Epoch 376 / 500 | iteration 25 / 30 | Total Loss: 3.5067834854125977 | KNN Loss: 2.4692277908325195 | BCE Loss: 1.0375556945800781\n",
      "Epoch 377 / 500 | iteration 0 / 30 | Total Loss: 3.476471185684204 | KNN Loss: 2.4626927375793457 | BCE Loss: 1.0137784481048584\n",
      "Epoch 377 / 500 | iteration 5 / 30 | Total Loss: 3.478670835494995 | KNN Loss: 2.474656105041504 | BCE Loss: 1.0040147304534912\n",
      "Epoch 377 / 500 | iteration 10 / 30 | Total Loss: 3.4642281532287598 | KNN Loss: 2.444201707839966 | BCE Loss: 1.0200263261795044\n",
      "Epoch 377 / 500 | iteration 15 / 30 | Total Loss: 3.454847812652588 | KNN Loss: 2.453413486480713 | BCE Loss: 1.001434326171875\n",
      "Epoch 377 / 500 | iteration 20 / 30 | Total Loss: 3.4995315074920654 | KNN Loss: 2.4776108264923096 | BCE Loss: 1.0219206809997559\n",
      "Epoch 377 / 500 | iteration 25 / 30 | Total Loss: 3.4327917098999023 | KNN Loss: 2.4366297721862793 | BCE Loss: 0.9961618185043335\n",
      "Epoch 378 / 500 | iteration 0 / 30 | Total Loss: 3.468994140625 | KNN Loss: 2.4288454055786133 | BCE Loss: 1.0401487350463867\n",
      "Epoch 378 / 500 | iteration 5 / 30 | Total Loss: 3.424389123916626 | KNN Loss: 2.4295942783355713 | BCE Loss: 0.9947949051856995\n",
      "Epoch 378 / 500 | iteration 10 / 30 | Total Loss: 3.4611430168151855 | KNN Loss: 2.432931423187256 | BCE Loss: 1.0282115936279297\n",
      "Epoch 378 / 500 | iteration 15 / 30 | Total Loss: 3.440349578857422 | KNN Loss: 2.434572219848633 | BCE Loss: 1.0057772397994995\n",
      "Epoch 378 / 500 | iteration 20 / 30 | Total Loss: 3.4823169708251953 | KNN Loss: 2.4676647186279297 | BCE Loss: 1.014652132987976\n",
      "Epoch 378 / 500 | iteration 25 / 30 | Total Loss: 3.46028470993042 | KNN Loss: 2.424844741821289 | BCE Loss: 1.0354399681091309\n",
      "Epoch 379 / 500 | iteration 0 / 30 | Total Loss: 3.496087074279785 | KNN Loss: 2.4626963138580322 | BCE Loss: 1.033390760421753\n",
      "Epoch 379 / 500 | iteration 5 / 30 | Total Loss: 3.4498963356018066 | KNN Loss: 2.4384407997131348 | BCE Loss: 1.0114555358886719\n",
      "Epoch 379 / 500 | iteration 10 / 30 | Total Loss: 3.463951826095581 | KNN Loss: 2.4352829456329346 | BCE Loss: 1.0286688804626465\n",
      "Epoch 379 / 500 | iteration 15 / 30 | Total Loss: 3.458613157272339 | KNN Loss: 2.42307710647583 | BCE Loss: 1.0355360507965088\n",
      "Epoch 379 / 500 | iteration 20 / 30 | Total Loss: 3.4760260581970215 | KNN Loss: 2.4447903633117676 | BCE Loss: 1.0312355756759644\n",
      "Epoch 379 / 500 | iteration 25 / 30 | Total Loss: 3.4494075775146484 | KNN Loss: 2.4457473754882812 | BCE Loss: 1.0036603212356567\n",
      "Epoch 380 / 500 | iteration 0 / 30 | Total Loss: 3.4588253498077393 | KNN Loss: 2.4420084953308105 | BCE Loss: 1.0168168544769287\n",
      "Epoch 380 / 500 | iteration 5 / 30 | Total Loss: 3.4515600204467773 | KNN Loss: 2.456444263458252 | BCE Loss: 0.9951156377792358\n",
      "Epoch 380 / 500 | iteration 10 / 30 | Total Loss: 3.4843268394470215 | KNN Loss: 2.4426589012145996 | BCE Loss: 1.0416678190231323\n",
      "Epoch 380 / 500 | iteration 15 / 30 | Total Loss: 3.452110767364502 | KNN Loss: 2.461808204650879 | BCE Loss: 0.9903026223182678\n",
      "Epoch 380 / 500 | iteration 20 / 30 | Total Loss: 3.477703332901001 | KNN Loss: 2.4506242275238037 | BCE Loss: 1.0270791053771973\n",
      "Epoch 380 / 500 | iteration 25 / 30 | Total Loss: 3.518662214279175 | KNN Loss: 2.503037691116333 | BCE Loss: 1.0156245231628418\n",
      "Epoch 381 / 500 | iteration 0 / 30 | Total Loss: 3.4385876655578613 | KNN Loss: 2.4243974685668945 | BCE Loss: 1.0141901969909668\n",
      "Epoch 381 / 500 | iteration 5 / 30 | Total Loss: 3.4283335208892822 | KNN Loss: 2.441009283065796 | BCE Loss: 0.9873242974281311\n",
      "Epoch 381 / 500 | iteration 10 / 30 | Total Loss: 3.454378128051758 | KNN Loss: 2.461819648742676 | BCE Loss: 0.9925583600997925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 381 / 500 | iteration 15 / 30 | Total Loss: 3.4975507259368896 | KNN Loss: 2.4710843563079834 | BCE Loss: 1.0264663696289062\n",
      "Epoch 381 / 500 | iteration 20 / 30 | Total Loss: 3.489368438720703 | KNN Loss: 2.464390993118286 | BCE Loss: 1.0249775648117065\n",
      "Epoch 381 / 500 | iteration 25 / 30 | Total Loss: 3.441452741622925 | KNN Loss: 2.459196090698242 | BCE Loss: 0.9822566509246826\n",
      "Epoch   382: reducing learning rate of group 0 to 1.1632e-05.\n",
      "Epoch 382 / 500 | iteration 0 / 30 | Total Loss: 3.4230570793151855 | KNN Loss: 2.423502206802368 | BCE Loss: 0.9995548725128174\n",
      "Epoch 382 / 500 | iteration 5 / 30 | Total Loss: 3.463864803314209 | KNN Loss: 2.440539598464966 | BCE Loss: 1.0233252048492432\n",
      "Epoch 382 / 500 | iteration 10 / 30 | Total Loss: 3.420931816101074 | KNN Loss: 2.441098213195801 | BCE Loss: 0.9798336029052734\n",
      "Epoch 382 / 500 | iteration 15 / 30 | Total Loss: 3.4768993854522705 | KNN Loss: 2.4749045372009277 | BCE Loss: 1.0019948482513428\n",
      "Epoch 382 / 500 | iteration 20 / 30 | Total Loss: 3.494415521621704 | KNN Loss: 2.474514961242676 | BCE Loss: 1.0199005603790283\n",
      "Epoch 382 / 500 | iteration 25 / 30 | Total Loss: 3.4383203983306885 | KNN Loss: 2.4373114109039307 | BCE Loss: 1.0010089874267578\n",
      "Epoch 383 / 500 | iteration 0 / 30 | Total Loss: 3.448854446411133 | KNN Loss: 2.4310154914855957 | BCE Loss: 1.017838954925537\n",
      "Epoch 383 / 500 | iteration 5 / 30 | Total Loss: 3.4778647422790527 | KNN Loss: 2.448828935623169 | BCE Loss: 1.0290358066558838\n",
      "Epoch 383 / 500 | iteration 10 / 30 | Total Loss: 3.498833656311035 | KNN Loss: 2.4418067932128906 | BCE Loss: 1.0570268630981445\n",
      "Epoch 383 / 500 | iteration 15 / 30 | Total Loss: 3.436629295349121 | KNN Loss: 2.437497615814209 | BCE Loss: 0.9991317987442017\n",
      "Epoch 383 / 500 | iteration 20 / 30 | Total Loss: 3.509099245071411 | KNN Loss: 2.4561781883239746 | BCE Loss: 1.0529210567474365\n",
      "Epoch 383 / 500 | iteration 25 / 30 | Total Loss: 3.48964524269104 | KNN Loss: 2.4812469482421875 | BCE Loss: 1.0083982944488525\n",
      "Epoch 384 / 500 | iteration 0 / 30 | Total Loss: 3.4542670249938965 | KNN Loss: 2.4244420528411865 | BCE Loss: 1.0298248529434204\n",
      "Epoch 384 / 500 | iteration 5 / 30 | Total Loss: 3.4796199798583984 | KNN Loss: 2.431704521179199 | BCE Loss: 1.0479154586791992\n",
      "Epoch 384 / 500 | iteration 10 / 30 | Total Loss: 3.441539764404297 | KNN Loss: 2.436694383621216 | BCE Loss: 1.004845380783081\n",
      "Epoch 384 / 500 | iteration 15 / 30 | Total Loss: 3.4746198654174805 | KNN Loss: 2.478199005126953 | BCE Loss: 0.9964208602905273\n",
      "Epoch 384 / 500 | iteration 20 / 30 | Total Loss: 3.4201231002807617 | KNN Loss: 2.415147066116333 | BCE Loss: 1.0049759149551392\n",
      "Epoch 384 / 500 | iteration 25 / 30 | Total Loss: 3.4654970169067383 | KNN Loss: 2.471842050552368 | BCE Loss: 0.9936550259590149\n",
      "Epoch 385 / 500 | iteration 0 / 30 | Total Loss: 3.4334986209869385 | KNN Loss: 2.445335865020752 | BCE Loss: 0.9881626963615417\n",
      "Epoch 385 / 500 | iteration 5 / 30 | Total Loss: 3.4416160583496094 | KNN Loss: 2.4339654445648193 | BCE Loss: 1.0076504945755005\n",
      "Epoch 385 / 500 | iteration 10 / 30 | Total Loss: 3.4703025817871094 | KNN Loss: 2.452474355697632 | BCE Loss: 1.017828106880188\n",
      "Epoch 385 / 500 | iteration 15 / 30 | Total Loss: 3.4435558319091797 | KNN Loss: 2.4520771503448486 | BCE Loss: 0.991478681564331\n",
      "Epoch 385 / 500 | iteration 20 / 30 | Total Loss: 3.4334263801574707 | KNN Loss: 2.442291259765625 | BCE Loss: 0.9911350607872009\n",
      "Epoch 385 / 500 | iteration 25 / 30 | Total Loss: 3.4984400272369385 | KNN Loss: 2.4673209190368652 | BCE Loss: 1.0311191082000732\n",
      "Epoch 386 / 500 | iteration 0 / 30 | Total Loss: 3.4164552688598633 | KNN Loss: 2.444786310195923 | BCE Loss: 0.9716689586639404\n",
      "Epoch 386 / 500 | iteration 5 / 30 | Total Loss: 3.438305377960205 | KNN Loss: 2.4274537563323975 | BCE Loss: 1.010851502418518\n",
      "Epoch 386 / 500 | iteration 10 / 30 | Total Loss: 3.5258307456970215 | KNN Loss: 2.506777763366699 | BCE Loss: 1.0190531015396118\n",
      "Epoch 386 / 500 | iteration 15 / 30 | Total Loss: 3.42163348197937 | KNN Loss: 2.4218385219573975 | BCE Loss: 0.9997949004173279\n",
      "Epoch 386 / 500 | iteration 20 / 30 | Total Loss: 3.445993423461914 | KNN Loss: 2.4352152347564697 | BCE Loss: 1.0107780694961548\n",
      "Epoch 386 / 500 | iteration 25 / 30 | Total Loss: 3.4932408332824707 | KNN Loss: 2.4699838161468506 | BCE Loss: 1.0232571363449097\n",
      "Epoch 387 / 500 | iteration 0 / 30 | Total Loss: 3.4341654777526855 | KNN Loss: 2.4288110733032227 | BCE Loss: 1.005354404449463\n",
      "Epoch 387 / 500 | iteration 5 / 30 | Total Loss: 3.4879870414733887 | KNN Loss: 2.482475757598877 | BCE Loss: 1.0055114030838013\n",
      "Epoch 387 / 500 | iteration 10 / 30 | Total Loss: 3.4440605640411377 | KNN Loss: 2.4602811336517334 | BCE Loss: 0.9837793707847595\n",
      "Epoch 387 / 500 | iteration 15 / 30 | Total Loss: 3.4753618240356445 | KNN Loss: 2.449531316757202 | BCE Loss: 1.0258303880691528\n",
      "Epoch 387 / 500 | iteration 20 / 30 | Total Loss: 3.4701266288757324 | KNN Loss: 2.4762561321258545 | BCE Loss: 0.9938704967498779\n",
      "Epoch 387 / 500 | iteration 25 / 30 | Total Loss: 3.4827470779418945 | KNN Loss: 2.4587161540985107 | BCE Loss: 1.0240310430526733\n",
      "Epoch 388 / 500 | iteration 0 / 30 | Total Loss: 3.4775900840759277 | KNN Loss: 2.4516708850860596 | BCE Loss: 1.0259190797805786\n",
      "Epoch 388 / 500 | iteration 5 / 30 | Total Loss: 3.46182918548584 | KNN Loss: 2.4872477054595947 | BCE Loss: 0.9745815992355347\n",
      "Epoch 388 / 500 | iteration 10 / 30 | Total Loss: 3.4960412979125977 | KNN Loss: 2.4718527793884277 | BCE Loss: 1.02418851852417\n",
      "Epoch 388 / 500 | iteration 15 / 30 | Total Loss: 3.4605793952941895 | KNN Loss: 2.438253164291382 | BCE Loss: 1.0223263502120972\n",
      "Epoch 388 / 500 | iteration 20 / 30 | Total Loss: 3.413055419921875 | KNN Loss: 2.417928695678711 | BCE Loss: 0.9951267242431641\n",
      "Epoch 388 / 500 | iteration 25 / 30 | Total Loss: 3.426609992980957 | KNN Loss: 2.4198977947235107 | BCE Loss: 1.0067121982574463\n",
      "Epoch 389 / 500 | iteration 0 / 30 | Total Loss: 3.422178030014038 | KNN Loss: 2.417043924331665 | BCE Loss: 1.005134105682373\n",
      "Epoch 389 / 500 | iteration 5 / 30 | Total Loss: 3.4675867557525635 | KNN Loss: 2.4567885398864746 | BCE Loss: 1.0107982158660889\n",
      "Epoch 389 / 500 | iteration 10 / 30 | Total Loss: 3.5170788764953613 | KNN Loss: 2.4793806076049805 | BCE Loss: 1.0376983880996704\n",
      "Epoch 389 / 500 | iteration 15 / 30 | Total Loss: 3.4316487312316895 | KNN Loss: 2.4577486515045166 | BCE Loss: 0.9739000797271729\n",
      "Epoch 389 / 500 | iteration 20 / 30 | Total Loss: 3.4698081016540527 | KNN Loss: 2.4588944911956787 | BCE Loss: 1.0109134912490845\n",
      "Epoch 389 / 500 | iteration 25 / 30 | Total Loss: 3.463785171508789 | KNN Loss: 2.442126750946045 | BCE Loss: 1.0216584205627441\n",
      "Epoch 390 / 500 | iteration 0 / 30 | Total Loss: 3.4578053951263428 | KNN Loss: 2.4406635761260986 | BCE Loss: 1.0171418190002441\n",
      "Epoch 390 / 500 | iteration 5 / 30 | Total Loss: 3.4870331287384033 | KNN Loss: 2.439117908477783 | BCE Loss: 1.0479152202606201\n",
      "Epoch 390 / 500 | iteration 10 / 30 | Total Loss: 3.435433864593506 | KNN Loss: 2.4431893825531006 | BCE Loss: 0.9922444820404053\n",
      "Epoch 390 / 500 | iteration 15 / 30 | Total Loss: 3.4399123191833496 | KNN Loss: 2.413909912109375 | BCE Loss: 1.0260024070739746\n",
      "Epoch 390 / 500 | iteration 20 / 30 | Total Loss: 3.4719161987304688 | KNN Loss: 2.453273296356201 | BCE Loss: 1.018642783164978\n",
      "Epoch 390 / 500 | iteration 25 / 30 | Total Loss: 3.4608776569366455 | KNN Loss: 2.4473907947540283 | BCE Loss: 1.0134868621826172\n",
      "Epoch 391 / 500 | iteration 0 / 30 | Total Loss: 3.4838833808898926 | KNN Loss: 2.4573163986206055 | BCE Loss: 1.026566982269287\n",
      "Epoch 391 / 500 | iteration 5 / 30 | Total Loss: 3.493472099304199 | KNN Loss: 2.476168155670166 | BCE Loss: 1.0173040628433228\n",
      "Epoch 391 / 500 | iteration 10 / 30 | Total Loss: 3.4635813236236572 | KNN Loss: 2.4574573040008545 | BCE Loss: 1.0061240196228027\n",
      "Epoch 391 / 500 | iteration 15 / 30 | Total Loss: 3.475132465362549 | KNN Loss: 2.464207410812378 | BCE Loss: 1.0109249353408813\n",
      "Epoch 391 / 500 | iteration 20 / 30 | Total Loss: 3.4396231174468994 | KNN Loss: 2.444526433944702 | BCE Loss: 0.995096743106842\n",
      "Epoch 391 / 500 | iteration 25 / 30 | Total Loss: 3.4690816402435303 | KNN Loss: 2.434690237045288 | BCE Loss: 1.0343914031982422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 392 / 500 | iteration 0 / 30 | Total Loss: 3.4671812057495117 | KNN Loss: 2.4407200813293457 | BCE Loss: 1.0264610052108765\n",
      "Epoch 392 / 500 | iteration 5 / 30 | Total Loss: 3.426889419555664 | KNN Loss: 2.4338812828063965 | BCE Loss: 0.9930081963539124\n",
      "Epoch 392 / 500 | iteration 10 / 30 | Total Loss: 3.4560275077819824 | KNN Loss: 2.4413228034973145 | BCE Loss: 1.014704704284668\n",
      "Epoch 392 / 500 | iteration 15 / 30 | Total Loss: 3.4801790714263916 | KNN Loss: 2.470973014831543 | BCE Loss: 1.0092060565948486\n",
      "Epoch 392 / 500 | iteration 20 / 30 | Total Loss: 3.497077703475952 | KNN Loss: 2.4936747550964355 | BCE Loss: 1.0034029483795166\n",
      "Epoch 392 / 500 | iteration 25 / 30 | Total Loss: 3.487529754638672 | KNN Loss: 2.466723918914795 | BCE Loss: 1.0208057165145874\n",
      "Epoch   393: reducing learning rate of group 0 to 8.1421e-06.\n",
      "Epoch 393 / 500 | iteration 0 / 30 | Total Loss: 3.4667410850524902 | KNN Loss: 2.4551947116851807 | BCE Loss: 1.0115463733673096\n",
      "Epoch 393 / 500 | iteration 5 / 30 | Total Loss: 3.4781970977783203 | KNN Loss: 2.4376115798950195 | BCE Loss: 1.0405855178833008\n",
      "Epoch 393 / 500 | iteration 10 / 30 | Total Loss: 3.5074753761291504 | KNN Loss: 2.4926390647888184 | BCE Loss: 1.014836311340332\n",
      "Epoch 393 / 500 | iteration 15 / 30 | Total Loss: 3.4682438373565674 | KNN Loss: 2.4307613372802734 | BCE Loss: 1.037482500076294\n",
      "Epoch 393 / 500 | iteration 20 / 30 | Total Loss: 3.4405698776245117 | KNN Loss: 2.4397361278533936 | BCE Loss: 1.0008337497711182\n",
      "Epoch 393 / 500 | iteration 25 / 30 | Total Loss: 3.455989360809326 | KNN Loss: 2.4449219703674316 | BCE Loss: 1.011067509651184\n",
      "Epoch 394 / 500 | iteration 0 / 30 | Total Loss: 3.499974012374878 | KNN Loss: 2.469456672668457 | BCE Loss: 1.030517339706421\n",
      "Epoch 394 / 500 | iteration 5 / 30 | Total Loss: 3.495854139328003 | KNN Loss: 2.475390911102295 | BCE Loss: 1.020463228225708\n",
      "Epoch 394 / 500 | iteration 10 / 30 | Total Loss: 3.5114905834198 | KNN Loss: 2.4683785438537598 | BCE Loss: 1.04311203956604\n",
      "Epoch 394 / 500 | iteration 15 / 30 | Total Loss: 3.4584200382232666 | KNN Loss: 2.4560341835021973 | BCE Loss: 1.0023858547210693\n",
      "Epoch 394 / 500 | iteration 20 / 30 | Total Loss: 3.513536214828491 | KNN Loss: 2.455568552017212 | BCE Loss: 1.0579676628112793\n",
      "Epoch 394 / 500 | iteration 25 / 30 | Total Loss: 3.4637980461120605 | KNN Loss: 2.47694993019104 | BCE Loss: 0.9868480563163757\n",
      "Epoch 395 / 500 | iteration 0 / 30 | Total Loss: 3.4923105239868164 | KNN Loss: 2.4699490070343018 | BCE Loss: 1.0223616361618042\n",
      "Epoch 395 / 500 | iteration 5 / 30 | Total Loss: 3.431776523590088 | KNN Loss: 2.419142961502075 | BCE Loss: 1.0126336812973022\n",
      "Epoch 395 / 500 | iteration 10 / 30 | Total Loss: 3.4499497413635254 | KNN Loss: 2.462855339050293 | BCE Loss: 0.9870942831039429\n",
      "Epoch 395 / 500 | iteration 15 / 30 | Total Loss: 3.462719440460205 | KNN Loss: 2.459972858428955 | BCE Loss: 1.0027467012405396\n",
      "Epoch 395 / 500 | iteration 20 / 30 | Total Loss: 3.495748996734619 | KNN Loss: 2.477613925933838 | BCE Loss: 1.0181350708007812\n",
      "Epoch 395 / 500 | iteration 25 / 30 | Total Loss: 3.4501380920410156 | KNN Loss: 2.425527572631836 | BCE Loss: 1.0246104001998901\n",
      "Epoch 396 / 500 | iteration 0 / 30 | Total Loss: 3.4896607398986816 | KNN Loss: 2.4654674530029297 | BCE Loss: 1.024193286895752\n",
      "Epoch 396 / 500 | iteration 5 / 30 | Total Loss: 3.49875807762146 | KNN Loss: 2.437985897064209 | BCE Loss: 1.060772180557251\n",
      "Epoch 396 / 500 | iteration 10 / 30 | Total Loss: 3.4763426780700684 | KNN Loss: 2.477821111679077 | BCE Loss: 0.9985216856002808\n",
      "Epoch 396 / 500 | iteration 15 / 30 | Total Loss: 3.4620513916015625 | KNN Loss: 2.45375919342041 | BCE Loss: 1.008292317390442\n",
      "Epoch 396 / 500 | iteration 20 / 30 | Total Loss: 3.4284863471984863 | KNN Loss: 2.4181067943573 | BCE Loss: 1.010379672050476\n",
      "Epoch 396 / 500 | iteration 25 / 30 | Total Loss: 3.491170883178711 | KNN Loss: 2.4729440212249756 | BCE Loss: 1.0182268619537354\n",
      "Epoch 397 / 500 | iteration 0 / 30 | Total Loss: 3.5054116249084473 | KNN Loss: 2.457761526107788 | BCE Loss: 1.0476500988006592\n",
      "Epoch 397 / 500 | iteration 5 / 30 | Total Loss: 3.4872140884399414 | KNN Loss: 2.479140043258667 | BCE Loss: 1.008074164390564\n",
      "Epoch 397 / 500 | iteration 10 / 30 | Total Loss: 3.4502334594726562 | KNN Loss: 2.4539403915405273 | BCE Loss: 0.9962930679321289\n",
      "Epoch 397 / 500 | iteration 15 / 30 | Total Loss: 3.4810304641723633 | KNN Loss: 2.4677460193634033 | BCE Loss: 1.0132843255996704\n",
      "Epoch 397 / 500 | iteration 20 / 30 | Total Loss: 3.463283061981201 | KNN Loss: 2.4801952838897705 | BCE Loss: 0.9830878973007202\n",
      "Epoch 397 / 500 | iteration 25 / 30 | Total Loss: 3.4579546451568604 | KNN Loss: 2.448427438735962 | BCE Loss: 1.0095272064208984\n",
      "Epoch 398 / 500 | iteration 0 / 30 | Total Loss: 3.4787745475769043 | KNN Loss: 2.437709331512451 | BCE Loss: 1.0410652160644531\n",
      "Epoch 398 / 500 | iteration 5 / 30 | Total Loss: 3.4540810585021973 | KNN Loss: 2.459355354309082 | BCE Loss: 0.9947255849838257\n",
      "Epoch 398 / 500 | iteration 10 / 30 | Total Loss: 3.4604501724243164 | KNN Loss: 2.450223922729492 | BCE Loss: 1.0102263689041138\n",
      "Epoch 398 / 500 | iteration 15 / 30 | Total Loss: 3.448306083679199 | KNN Loss: 2.4479095935821533 | BCE Loss: 1.000396490097046\n",
      "Epoch 398 / 500 | iteration 20 / 30 | Total Loss: 3.462939739227295 | KNN Loss: 2.4542148113250732 | BCE Loss: 1.0087249279022217\n",
      "Epoch 398 / 500 | iteration 25 / 30 | Total Loss: 3.4664363861083984 | KNN Loss: 2.468125343322754 | BCE Loss: 0.9983110427856445\n",
      "Epoch 399 / 500 | iteration 0 / 30 | Total Loss: 3.468428134918213 | KNN Loss: 2.4505107402801514 | BCE Loss: 1.0179173946380615\n",
      "Epoch 399 / 500 | iteration 5 / 30 | Total Loss: 3.461930274963379 | KNN Loss: 2.4454715251922607 | BCE Loss: 1.0164586305618286\n",
      "Epoch 399 / 500 | iteration 10 / 30 | Total Loss: 3.4716806411743164 | KNN Loss: 2.4348413944244385 | BCE Loss: 1.036839246749878\n",
      "Epoch 399 / 500 | iteration 15 / 30 | Total Loss: 3.4697113037109375 | KNN Loss: 2.4454610347747803 | BCE Loss: 1.0242502689361572\n",
      "Epoch 399 / 500 | iteration 20 / 30 | Total Loss: 3.4621195793151855 | KNN Loss: 2.4111266136169434 | BCE Loss: 1.0509928464889526\n",
      "Epoch 399 / 500 | iteration 25 / 30 | Total Loss: 3.4305214881896973 | KNN Loss: 2.4251227378845215 | BCE Loss: 1.0053988695144653\n",
      "Epoch 400 / 500 | iteration 0 / 30 | Total Loss: 3.439603567123413 | KNN Loss: 2.4596469402313232 | BCE Loss: 0.9799566268920898\n",
      "Epoch 400 / 500 | iteration 5 / 30 | Total Loss: 3.4812350273132324 | KNN Loss: 2.4447362422943115 | BCE Loss: 1.0364986658096313\n",
      "Epoch 400 / 500 | iteration 10 / 30 | Total Loss: 3.4823837280273438 | KNN Loss: 2.478823184967041 | BCE Loss: 1.0035605430603027\n",
      "Epoch 400 / 500 | iteration 15 / 30 | Total Loss: 3.4906206130981445 | KNN Loss: 2.465639591217041 | BCE Loss: 1.0249810218811035\n",
      "Epoch 400 / 500 | iteration 20 / 30 | Total Loss: 3.455094814300537 | KNN Loss: 2.442326784133911 | BCE Loss: 1.012768030166626\n",
      "Epoch 400 / 500 | iteration 25 / 30 | Total Loss: 3.4519758224487305 | KNN Loss: 2.430647134780884 | BCE Loss: 1.0213288068771362\n",
      "Epoch 401 / 500 | iteration 0 / 30 | Total Loss: 3.46655535697937 | KNN Loss: 2.4417171478271484 | BCE Loss: 1.0248382091522217\n",
      "Epoch 401 / 500 | iteration 5 / 30 | Total Loss: 3.499239206314087 | KNN Loss: 2.4678351879119873 | BCE Loss: 1.0314040184020996\n",
      "Epoch 401 / 500 | iteration 10 / 30 | Total Loss: 3.4676482677459717 | KNN Loss: 2.452038049697876 | BCE Loss: 1.0156102180480957\n",
      "Epoch 401 / 500 | iteration 15 / 30 | Total Loss: 3.4360220432281494 | KNN Loss: 2.423707962036133 | BCE Loss: 1.0123140811920166\n",
      "Epoch 401 / 500 | iteration 20 / 30 | Total Loss: 3.4744186401367188 | KNN Loss: 2.4572207927703857 | BCE Loss: 1.017197847366333\n",
      "Epoch 401 / 500 | iteration 25 / 30 | Total Loss: 3.481943130493164 | KNN Loss: 2.48376202583313 | BCE Loss: 0.998181164264679\n",
      "Epoch 402 / 500 | iteration 0 / 30 | Total Loss: 3.4432239532470703 | KNN Loss: 2.4495761394500732 | BCE Loss: 0.9936478137969971\n",
      "Epoch 402 / 500 | iteration 5 / 30 | Total Loss: 3.472562551498413 | KNN Loss: 2.4578638076782227 | BCE Loss: 1.0146987438201904\n",
      "Epoch 402 / 500 | iteration 10 / 30 | Total Loss: 3.4982964992523193 | KNN Loss: 2.4812259674072266 | BCE Loss: 1.0170705318450928\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 402 / 500 | iteration 15 / 30 | Total Loss: 3.4513816833496094 | KNN Loss: 2.4249818325042725 | BCE Loss: 1.026399850845337\n",
      "Epoch 402 / 500 | iteration 20 / 30 | Total Loss: 3.47743821144104 | KNN Loss: 2.424076557159424 | BCE Loss: 1.0533616542816162\n",
      "Epoch 402 / 500 | iteration 25 / 30 | Total Loss: 3.446455955505371 | KNN Loss: 2.477160692214966 | BCE Loss: 0.96929532289505\n",
      "Epoch 403 / 500 | iteration 0 / 30 | Total Loss: 3.480337619781494 | KNN Loss: 2.448223114013672 | BCE Loss: 1.0321145057678223\n",
      "Epoch 403 / 500 | iteration 5 / 30 | Total Loss: 3.4990200996398926 | KNN Loss: 2.4762418270111084 | BCE Loss: 1.0227781534194946\n",
      "Epoch 403 / 500 | iteration 10 / 30 | Total Loss: 3.4707510471343994 | KNN Loss: 2.4629528522491455 | BCE Loss: 1.007798194885254\n",
      "Epoch 403 / 500 | iteration 15 / 30 | Total Loss: 3.4756102561950684 | KNN Loss: 2.47312593460083 | BCE Loss: 1.0024843215942383\n",
      "Epoch 403 / 500 | iteration 20 / 30 | Total Loss: 3.5126137733459473 | KNN Loss: 2.4883270263671875 | BCE Loss: 1.0242867469787598\n",
      "Epoch 403 / 500 | iteration 25 / 30 | Total Loss: 3.4223313331604004 | KNN Loss: 2.4328227043151855 | BCE Loss: 0.9895086884498596\n",
      "Epoch 404 / 500 | iteration 0 / 30 | Total Loss: 3.4453442096710205 | KNN Loss: 2.420173406600952 | BCE Loss: 1.0251708030700684\n",
      "Epoch 404 / 500 | iteration 5 / 30 | Total Loss: 3.463390350341797 | KNN Loss: 2.449824571609497 | BCE Loss: 1.0135657787322998\n",
      "Epoch 404 / 500 | iteration 10 / 30 | Total Loss: 3.4348089694976807 | KNN Loss: 2.434098720550537 | BCE Loss: 1.0007102489471436\n",
      "Epoch 404 / 500 | iteration 15 / 30 | Total Loss: 3.457245111465454 | KNN Loss: 2.448176145553589 | BCE Loss: 1.0090689659118652\n",
      "Epoch 404 / 500 | iteration 20 / 30 | Total Loss: 3.4572713375091553 | KNN Loss: 2.4275410175323486 | BCE Loss: 1.0297303199768066\n",
      "Epoch 404 / 500 | iteration 25 / 30 | Total Loss: 3.486762523651123 | KNN Loss: 2.454482078552246 | BCE Loss: 1.032280445098877\n",
      "Epoch 405 / 500 | iteration 0 / 30 | Total Loss: 3.5139505863189697 | KNN Loss: 2.4916250705718994 | BCE Loss: 1.0223255157470703\n",
      "Epoch 405 / 500 | iteration 5 / 30 | Total Loss: 3.4980852603912354 | KNN Loss: 2.467329263687134 | BCE Loss: 1.0307559967041016\n",
      "Epoch 405 / 500 | iteration 10 / 30 | Total Loss: 3.4623358249664307 | KNN Loss: 2.4554128646850586 | BCE Loss: 1.006922960281372\n",
      "Epoch 405 / 500 | iteration 15 / 30 | Total Loss: 3.4298534393310547 | KNN Loss: 2.443545341491699 | BCE Loss: 0.986308217048645\n",
      "Epoch 405 / 500 | iteration 20 / 30 | Total Loss: 3.4425933361053467 | KNN Loss: 2.43428111076355 | BCE Loss: 1.0083122253417969\n",
      "Epoch 405 / 500 | iteration 25 / 30 | Total Loss: 3.4817097187042236 | KNN Loss: 2.4555575847625732 | BCE Loss: 1.0261521339416504\n",
      "Epoch 406 / 500 | iteration 0 / 30 | Total Loss: 3.4860236644744873 | KNN Loss: 2.454763889312744 | BCE Loss: 1.0312597751617432\n",
      "Epoch 406 / 500 | iteration 5 / 30 | Total Loss: 3.512758255004883 | KNN Loss: 2.4696877002716064 | BCE Loss: 1.0430704355239868\n",
      "Epoch 406 / 500 | iteration 10 / 30 | Total Loss: 3.45310115814209 | KNN Loss: 2.436110496520996 | BCE Loss: 1.0169906616210938\n",
      "Epoch 406 / 500 | iteration 15 / 30 | Total Loss: 3.451585292816162 | KNN Loss: 2.4426329135894775 | BCE Loss: 1.0089523792266846\n",
      "Epoch 406 / 500 | iteration 20 / 30 | Total Loss: 3.465479850769043 | KNN Loss: 2.4635250568389893 | BCE Loss: 1.0019547939300537\n",
      "Epoch 406 / 500 | iteration 25 / 30 | Total Loss: 3.469411611557007 | KNN Loss: 2.461723804473877 | BCE Loss: 1.0076878070831299\n",
      "Epoch 407 / 500 | iteration 0 / 30 | Total Loss: 3.453831195831299 | KNN Loss: 2.424795150756836 | BCE Loss: 1.029036045074463\n",
      "Epoch 407 / 500 | iteration 5 / 30 | Total Loss: 3.4548425674438477 | KNN Loss: 2.4407806396484375 | BCE Loss: 1.0140619277954102\n",
      "Epoch 407 / 500 | iteration 10 / 30 | Total Loss: 3.47955322265625 | KNN Loss: 2.4590768814086914 | BCE Loss: 1.0204763412475586\n",
      "Epoch 407 / 500 | iteration 15 / 30 | Total Loss: 3.4446005821228027 | KNN Loss: 2.4454233646392822 | BCE Loss: 0.9991772770881653\n",
      "Epoch 407 / 500 | iteration 20 / 30 | Total Loss: 3.460273265838623 | KNN Loss: 2.42565655708313 | BCE Loss: 1.0346167087554932\n",
      "Epoch 407 / 500 | iteration 25 / 30 | Total Loss: 3.474689245223999 | KNN Loss: 2.4689712524414062 | BCE Loss: 1.0057179927825928\n",
      "Epoch 408 / 500 | iteration 0 / 30 | Total Loss: 3.4708926677703857 | KNN Loss: 2.450383424758911 | BCE Loss: 1.0205092430114746\n",
      "Epoch 408 / 500 | iteration 5 / 30 | Total Loss: 3.4517621994018555 | KNN Loss: 2.442478895187378 | BCE Loss: 1.009283423423767\n",
      "Epoch 408 / 500 | iteration 10 / 30 | Total Loss: 3.512742519378662 | KNN Loss: 2.455000877380371 | BCE Loss: 1.0577415227890015\n",
      "Epoch 408 / 500 | iteration 15 / 30 | Total Loss: 3.4532690048217773 | KNN Loss: 2.4628121852874756 | BCE Loss: 0.9904568791389465\n",
      "Epoch 408 / 500 | iteration 20 / 30 | Total Loss: 3.4786453247070312 | KNN Loss: 2.4595866203308105 | BCE Loss: 1.0190587043762207\n",
      "Epoch 408 / 500 | iteration 25 / 30 | Total Loss: 3.467113733291626 | KNN Loss: 2.4543280601501465 | BCE Loss: 1.0127856731414795\n",
      "Epoch 409 / 500 | iteration 0 / 30 | Total Loss: 3.4534873962402344 | KNN Loss: 2.4183578491210938 | BCE Loss: 1.0351296663284302\n",
      "Epoch 409 / 500 | iteration 5 / 30 | Total Loss: 3.464588165283203 | KNN Loss: 2.4433281421661377 | BCE Loss: 1.0212600231170654\n",
      "Epoch 409 / 500 | iteration 10 / 30 | Total Loss: 3.4595413208007812 | KNN Loss: 2.432149648666382 | BCE Loss: 1.0273916721343994\n",
      "Epoch 409 / 500 | iteration 15 / 30 | Total Loss: 3.477262020111084 | KNN Loss: 2.4610230922698975 | BCE Loss: 1.0162389278411865\n",
      "Epoch 409 / 500 | iteration 20 / 30 | Total Loss: 3.4259510040283203 | KNN Loss: 2.4386708736419678 | BCE Loss: 0.9872801899909973\n",
      "Epoch 409 / 500 | iteration 25 / 30 | Total Loss: 3.4774930477142334 | KNN Loss: 2.437129497528076 | BCE Loss: 1.0403635501861572\n",
      "Epoch 410 / 500 | iteration 0 / 30 | Total Loss: 3.493447780609131 | KNN Loss: 2.4695255756378174 | BCE Loss: 1.023922324180603\n",
      "Epoch 410 / 500 | iteration 5 / 30 | Total Loss: 3.4517312049865723 | KNN Loss: 2.437490463256836 | BCE Loss: 1.0142406225204468\n",
      "Epoch 410 / 500 | iteration 10 / 30 | Total Loss: 3.480658769607544 | KNN Loss: 2.464864492416382 | BCE Loss: 1.015794277191162\n",
      "Epoch 410 / 500 | iteration 15 / 30 | Total Loss: 3.452108860015869 | KNN Loss: 2.424806594848633 | BCE Loss: 1.0273021459579468\n",
      "Epoch 410 / 500 | iteration 20 / 30 | Total Loss: 3.4948716163635254 | KNN Loss: 2.4668498039245605 | BCE Loss: 1.0280218124389648\n",
      "Epoch 410 / 500 | iteration 25 / 30 | Total Loss: 3.5472354888916016 | KNN Loss: 2.4830105304718018 | BCE Loss: 1.0642249584197998\n",
      "Epoch 411 / 500 | iteration 0 / 30 | Total Loss: 3.454275608062744 | KNN Loss: 2.434597969055176 | BCE Loss: 1.019677758216858\n",
      "Epoch 411 / 500 | iteration 5 / 30 | Total Loss: 3.4250664710998535 | KNN Loss: 2.420846939086914 | BCE Loss: 1.0042195320129395\n",
      "Epoch 411 / 500 | iteration 10 / 30 | Total Loss: 3.489687442779541 | KNN Loss: 2.477146625518799 | BCE Loss: 1.0125408172607422\n",
      "Epoch 411 / 500 | iteration 15 / 30 | Total Loss: 3.4742605686187744 | KNN Loss: 2.449148178100586 | BCE Loss: 1.0251123905181885\n",
      "Epoch 411 / 500 | iteration 20 / 30 | Total Loss: 3.4787769317626953 | KNN Loss: 2.442790985107422 | BCE Loss: 1.0359859466552734\n",
      "Epoch 411 / 500 | iteration 25 / 30 | Total Loss: 3.4936351776123047 | KNN Loss: 2.480988025665283 | BCE Loss: 1.0126471519470215\n",
      "Epoch 412 / 500 | iteration 0 / 30 | Total Loss: 3.471467971801758 | KNN Loss: 2.439251184463501 | BCE Loss: 1.0322167873382568\n",
      "Epoch 412 / 500 | iteration 5 / 30 | Total Loss: 3.43664813041687 | KNN Loss: 2.4274051189422607 | BCE Loss: 1.0092430114746094\n",
      "Epoch 412 / 500 | iteration 10 / 30 | Total Loss: 3.4814653396606445 | KNN Loss: 2.453268051147461 | BCE Loss: 1.028197169303894\n",
      "Epoch 412 / 500 | iteration 15 / 30 | Total Loss: 3.436305522918701 | KNN Loss: 2.440561294555664 | BCE Loss: 0.9957441091537476\n",
      "Epoch 412 / 500 | iteration 20 / 30 | Total Loss: 3.471764087677002 | KNN Loss: 2.430804967880249 | BCE Loss: 1.0409590005874634\n",
      "Epoch 412 / 500 | iteration 25 / 30 | Total Loss: 3.4985904693603516 | KNN Loss: 2.475675344467163 | BCE Loss: 1.0229151248931885\n",
      "Epoch 413 / 500 | iteration 0 / 30 | Total Loss: 3.498711347579956 | KNN Loss: 2.4634950160980225 | BCE Loss: 1.0352163314819336\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 413 / 500 | iteration 5 / 30 | Total Loss: 3.4677071571350098 | KNN Loss: 2.45778751373291 | BCE Loss: 1.0099196434020996\n",
      "Epoch 413 / 500 | iteration 10 / 30 | Total Loss: 3.472837448120117 | KNN Loss: 2.4448535442352295 | BCE Loss: 1.0279839038848877\n",
      "Epoch 413 / 500 | iteration 15 / 30 | Total Loss: 3.4805819988250732 | KNN Loss: 2.4786388874053955 | BCE Loss: 1.0019431114196777\n",
      "Epoch 413 / 500 | iteration 20 / 30 | Total Loss: 3.4549567699432373 | KNN Loss: 2.4527933597564697 | BCE Loss: 1.0021634101867676\n",
      "Epoch 413 / 500 | iteration 25 / 30 | Total Loss: 3.462805986404419 | KNN Loss: 2.4537529945373535 | BCE Loss: 1.0090529918670654\n",
      "Epoch 414 / 500 | iteration 0 / 30 | Total Loss: 3.4220261573791504 | KNN Loss: 2.43038010597229 | BCE Loss: 0.9916461110115051\n",
      "Epoch 414 / 500 | iteration 5 / 30 | Total Loss: 3.4872870445251465 | KNN Loss: 2.4481289386749268 | BCE Loss: 1.0391581058502197\n",
      "Epoch 414 / 500 | iteration 10 / 30 | Total Loss: 3.5133628845214844 | KNN Loss: 2.4883201122283936 | BCE Loss: 1.0250427722930908\n",
      "Epoch 414 / 500 | iteration 15 / 30 | Total Loss: 3.452531337738037 | KNN Loss: 2.4177114963531494 | BCE Loss: 1.0348197221755981\n",
      "Epoch 414 / 500 | iteration 20 / 30 | Total Loss: 3.42562198638916 | KNN Loss: 2.428304672241211 | BCE Loss: 0.9973173141479492\n",
      "Epoch 414 / 500 | iteration 25 / 30 | Total Loss: 3.4896750450134277 | KNN Loss: 2.462291955947876 | BCE Loss: 1.0273832082748413\n",
      "Epoch   415: reducing learning rate of group 0 to 5.6994e-06.\n",
      "Epoch 415 / 500 | iteration 0 / 30 | Total Loss: 3.4794986248016357 | KNN Loss: 2.436351776123047 | BCE Loss: 1.0431468486785889\n",
      "Epoch 415 / 500 | iteration 5 / 30 | Total Loss: 3.4865217208862305 | KNN Loss: 2.478390693664551 | BCE Loss: 1.0081310272216797\n",
      "Epoch 415 / 500 | iteration 10 / 30 | Total Loss: 3.4496679306030273 | KNN Loss: 2.431844711303711 | BCE Loss: 1.0178232192993164\n",
      "Epoch 415 / 500 | iteration 15 / 30 | Total Loss: 3.431674003601074 | KNN Loss: 2.4617092609405518 | BCE Loss: 0.9699647426605225\n",
      "Epoch 415 / 500 | iteration 20 / 30 | Total Loss: 3.5069832801818848 | KNN Loss: 2.473391056060791 | BCE Loss: 1.0335923433303833\n",
      "Epoch 415 / 500 | iteration 25 / 30 | Total Loss: 3.4431285858154297 | KNN Loss: 2.4315543174743652 | BCE Loss: 1.011574149131775\n",
      "Epoch 416 / 500 | iteration 0 / 30 | Total Loss: 3.4240903854370117 | KNN Loss: 2.446256399154663 | BCE Loss: 0.9778341054916382\n",
      "Epoch 416 / 500 | iteration 5 / 30 | Total Loss: 3.4749081134796143 | KNN Loss: 2.42842435836792 | BCE Loss: 1.0464837551116943\n",
      "Epoch 416 / 500 | iteration 10 / 30 | Total Loss: 3.454437255859375 | KNN Loss: 2.460169553756714 | BCE Loss: 0.9942676424980164\n",
      "Epoch 416 / 500 | iteration 15 / 30 | Total Loss: 3.463275194168091 | KNN Loss: 2.464970827102661 | BCE Loss: 0.9983043074607849\n",
      "Epoch 416 / 500 | iteration 20 / 30 | Total Loss: 3.5128231048583984 | KNN Loss: 2.494189500808716 | BCE Loss: 1.0186336040496826\n",
      "Epoch 416 / 500 | iteration 25 / 30 | Total Loss: 3.484131336212158 | KNN Loss: 2.4429473876953125 | BCE Loss: 1.0411839485168457\n",
      "Epoch 417 / 500 | iteration 0 / 30 | Total Loss: 3.454312324523926 | KNN Loss: 2.4372456073760986 | BCE Loss: 1.0170667171478271\n",
      "Epoch 417 / 500 | iteration 5 / 30 | Total Loss: 3.47419810295105 | KNN Loss: 2.4461519718170166 | BCE Loss: 1.0280461311340332\n",
      "Epoch 417 / 500 | iteration 10 / 30 | Total Loss: 3.468276262283325 | KNN Loss: 2.4504990577697754 | BCE Loss: 1.0177772045135498\n",
      "Epoch 417 / 500 | iteration 15 / 30 | Total Loss: 3.4590611457824707 | KNN Loss: 2.4352469444274902 | BCE Loss: 1.0238142013549805\n",
      "Epoch 417 / 500 | iteration 20 / 30 | Total Loss: 3.4657845497131348 | KNN Loss: 2.4524238109588623 | BCE Loss: 1.0133607387542725\n",
      "Epoch 417 / 500 | iteration 25 / 30 | Total Loss: 3.453345537185669 | KNN Loss: 2.434128761291504 | BCE Loss: 1.019216775894165\n",
      "Epoch 418 / 500 | iteration 0 / 30 | Total Loss: 3.4833459854125977 | KNN Loss: 2.4467179775238037 | BCE Loss: 1.036628007888794\n",
      "Epoch 418 / 500 | iteration 5 / 30 | Total Loss: 3.4386656284332275 | KNN Loss: 2.426116704940796 | BCE Loss: 1.0125489234924316\n",
      "Epoch 418 / 500 | iteration 10 / 30 | Total Loss: 3.4694132804870605 | KNN Loss: 2.443896770477295 | BCE Loss: 1.0255165100097656\n",
      "Epoch 418 / 500 | iteration 15 / 30 | Total Loss: 3.467292308807373 | KNN Loss: 2.4526398181915283 | BCE Loss: 1.0146523714065552\n",
      "Epoch 418 / 500 | iteration 20 / 30 | Total Loss: 3.432511806488037 | KNN Loss: 2.4524362087249756 | BCE Loss: 0.9800757169723511\n",
      "Epoch 418 / 500 | iteration 25 / 30 | Total Loss: 3.4695241451263428 | KNN Loss: 2.452179193496704 | BCE Loss: 1.0173449516296387\n",
      "Epoch 419 / 500 | iteration 0 / 30 | Total Loss: 3.4980854988098145 | KNN Loss: 2.4753496646881104 | BCE Loss: 1.022735834121704\n",
      "Epoch 419 / 500 | iteration 5 / 30 | Total Loss: 3.497220277786255 | KNN Loss: 2.44769287109375 | BCE Loss: 1.0495274066925049\n",
      "Epoch 419 / 500 | iteration 10 / 30 | Total Loss: 3.461693525314331 | KNN Loss: 2.4470815658569336 | BCE Loss: 1.0146119594573975\n",
      "Epoch 419 / 500 | iteration 15 / 30 | Total Loss: 3.4659931659698486 | KNN Loss: 2.4453556537628174 | BCE Loss: 1.0206375122070312\n",
      "Epoch 419 / 500 | iteration 20 / 30 | Total Loss: 3.4877078533172607 | KNN Loss: 2.4509024620056152 | BCE Loss: 1.0368053913116455\n",
      "Epoch 419 / 500 | iteration 25 / 30 | Total Loss: 3.5068461894989014 | KNN Loss: 2.4403886795043945 | BCE Loss: 1.0664575099945068\n",
      "Epoch 420 / 500 | iteration 0 / 30 | Total Loss: 3.463219404220581 | KNN Loss: 2.449761390686035 | BCE Loss: 1.013458013534546\n",
      "Epoch 420 / 500 | iteration 5 / 30 | Total Loss: 3.4893152713775635 | KNN Loss: 2.450843334197998 | BCE Loss: 1.0384719371795654\n",
      "Epoch 420 / 500 | iteration 10 / 30 | Total Loss: 3.5436344146728516 | KNN Loss: 2.502640962600708 | BCE Loss: 1.040993571281433\n",
      "Epoch 420 / 500 | iteration 15 / 30 | Total Loss: 3.4548187255859375 | KNN Loss: 2.464601755142212 | BCE Loss: 0.9902170896530151\n",
      "Epoch 420 / 500 | iteration 20 / 30 | Total Loss: 3.4577105045318604 | KNN Loss: 2.4513416290283203 | BCE Loss: 1.00636887550354\n",
      "Epoch 420 / 500 | iteration 25 / 30 | Total Loss: 3.473301649093628 | KNN Loss: 2.439667224884033 | BCE Loss: 1.0336344242095947\n",
      "Epoch 421 / 500 | iteration 0 / 30 | Total Loss: 3.4387950897216797 | KNN Loss: 2.426877498626709 | BCE Loss: 1.0119175910949707\n",
      "Epoch 421 / 500 | iteration 5 / 30 | Total Loss: 3.4513754844665527 | KNN Loss: 2.438638925552368 | BCE Loss: 1.012736439704895\n",
      "Epoch 421 / 500 | iteration 10 / 30 | Total Loss: 3.5146429538726807 | KNN Loss: 2.495264768600464 | BCE Loss: 1.0193781852722168\n",
      "Epoch 421 / 500 | iteration 15 / 30 | Total Loss: 3.449439764022827 | KNN Loss: 2.432640552520752 | BCE Loss: 1.0167992115020752\n",
      "Epoch 421 / 500 | iteration 20 / 30 | Total Loss: 3.4826183319091797 | KNN Loss: 2.4790725708007812 | BCE Loss: 1.0035457611083984\n",
      "Epoch 421 / 500 | iteration 25 / 30 | Total Loss: 3.442446231842041 | KNN Loss: 2.422361135482788 | BCE Loss: 1.020085096359253\n",
      "Epoch 422 / 500 | iteration 0 / 30 | Total Loss: 3.4500226974487305 | KNN Loss: 2.448610305786133 | BCE Loss: 1.001412272453308\n",
      "Epoch 422 / 500 | iteration 5 / 30 | Total Loss: 3.4612512588500977 | KNN Loss: 2.4621074199676514 | BCE Loss: 0.9991437792778015\n",
      "Epoch 422 / 500 | iteration 10 / 30 | Total Loss: 3.461310863494873 | KNN Loss: 2.461517095565796 | BCE Loss: 0.9997936487197876\n",
      "Epoch 422 / 500 | iteration 15 / 30 | Total Loss: 3.48356294631958 | KNN Loss: 2.433957815170288 | BCE Loss: 1.049605131149292\n",
      "Epoch 422 / 500 | iteration 20 / 30 | Total Loss: 3.434056520462036 | KNN Loss: 2.445676565170288 | BCE Loss: 0.9883798956871033\n",
      "Epoch 422 / 500 | iteration 25 / 30 | Total Loss: 3.449718475341797 | KNN Loss: 2.444230556488037 | BCE Loss: 1.0054879188537598\n",
      "Epoch 423 / 500 | iteration 0 / 30 | Total Loss: 3.459231376647949 | KNN Loss: 2.446274995803833 | BCE Loss: 1.0129563808441162\n",
      "Epoch 423 / 500 | iteration 5 / 30 | Total Loss: 3.4941751956939697 | KNN Loss: 2.463207244873047 | BCE Loss: 1.0309679508209229\n",
      "Epoch 423 / 500 | iteration 10 / 30 | Total Loss: 3.501762866973877 | KNN Loss: 2.4698328971862793 | BCE Loss: 1.0319299697875977\n",
      "Epoch 423 / 500 | iteration 15 / 30 | Total Loss: 3.485569953918457 | KNN Loss: 2.4786486625671387 | BCE Loss: 1.006921410560608\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 423 / 500 | iteration 20 / 30 | Total Loss: 3.412867784500122 | KNN Loss: 2.417356252670288 | BCE Loss: 0.995511531829834\n",
      "Epoch 423 / 500 | iteration 25 / 30 | Total Loss: 3.4994609355926514 | KNN Loss: 2.478468894958496 | BCE Loss: 1.0209920406341553\n",
      "Epoch 424 / 500 | iteration 0 / 30 | Total Loss: 3.459414482116699 | KNN Loss: 2.4588637351989746 | BCE Loss: 1.000550627708435\n",
      "Epoch 424 / 500 | iteration 5 / 30 | Total Loss: 3.415191173553467 | KNN Loss: 2.41640043258667 | BCE Loss: 0.9987908601760864\n",
      "Epoch 424 / 500 | iteration 10 / 30 | Total Loss: 3.480778694152832 | KNN Loss: 2.469973564147949 | BCE Loss: 1.0108051300048828\n",
      "Epoch 424 / 500 | iteration 15 / 30 | Total Loss: 3.4566850662231445 | KNN Loss: 2.4379448890686035 | BCE Loss: 1.018740177154541\n",
      "Epoch 424 / 500 | iteration 20 / 30 | Total Loss: 3.447805404663086 | KNN Loss: 2.4166512489318848 | BCE Loss: 1.0311541557312012\n",
      "Epoch 424 / 500 | iteration 25 / 30 | Total Loss: 3.4985873699188232 | KNN Loss: 2.4598560333251953 | BCE Loss: 1.038731336593628\n",
      "Epoch 425 / 500 | iteration 0 / 30 | Total Loss: 3.455751657485962 | KNN Loss: 2.4405510425567627 | BCE Loss: 1.0152006149291992\n",
      "Epoch 425 / 500 | iteration 5 / 30 | Total Loss: 3.4999680519104004 | KNN Loss: 2.4740796089172363 | BCE Loss: 1.025888442993164\n",
      "Epoch 425 / 500 | iteration 10 / 30 | Total Loss: 3.473837375640869 | KNN Loss: 2.459406852722168 | BCE Loss: 1.0144305229187012\n",
      "Epoch 425 / 500 | iteration 15 / 30 | Total Loss: 3.446453094482422 | KNN Loss: 2.45424747467041 | BCE Loss: 0.9922056198120117\n",
      "Epoch 425 / 500 | iteration 20 / 30 | Total Loss: 3.4724040031433105 | KNN Loss: 2.449124574661255 | BCE Loss: 1.0232795476913452\n",
      "Epoch 425 / 500 | iteration 25 / 30 | Total Loss: 3.423553705215454 | KNN Loss: 2.4125664234161377 | BCE Loss: 1.0109872817993164\n",
      "Epoch   426: reducing learning rate of group 0 to 3.9896e-06.\n",
      "Epoch 426 / 500 | iteration 0 / 30 | Total Loss: 3.4684534072875977 | KNN Loss: 2.450075149536133 | BCE Loss: 1.0183782577514648\n",
      "Epoch 426 / 500 | iteration 5 / 30 | Total Loss: 3.4606754779815674 | KNN Loss: 2.4370949268341064 | BCE Loss: 1.023580551147461\n",
      "Epoch 426 / 500 | iteration 10 / 30 | Total Loss: 3.457474946975708 | KNN Loss: 2.4333174228668213 | BCE Loss: 1.0241575241088867\n",
      "Epoch 426 / 500 | iteration 15 / 30 | Total Loss: 3.4214701652526855 | KNN Loss: 2.4387764930725098 | BCE Loss: 0.9826937317848206\n",
      "Epoch 426 / 500 | iteration 20 / 30 | Total Loss: 3.4766573905944824 | KNN Loss: 2.4512863159179688 | BCE Loss: 1.0253710746765137\n",
      "Epoch 426 / 500 | iteration 25 / 30 | Total Loss: 3.4954757690429688 | KNN Loss: 2.4776012897491455 | BCE Loss: 1.0178744792938232\n",
      "Epoch 427 / 500 | iteration 0 / 30 | Total Loss: 3.48995304107666 | KNN Loss: 2.4787752628326416 | BCE Loss: 1.011177897453308\n",
      "Epoch 427 / 500 | iteration 5 / 30 | Total Loss: 3.528667449951172 | KNN Loss: 2.4626266956329346 | BCE Loss: 1.0660408735275269\n",
      "Epoch 427 / 500 | iteration 10 / 30 | Total Loss: 3.454801559448242 | KNN Loss: 2.4572415351867676 | BCE Loss: 0.9975601434707642\n",
      "Epoch 427 / 500 | iteration 15 / 30 | Total Loss: 3.447272777557373 | KNN Loss: 2.435863733291626 | BCE Loss: 1.0114091634750366\n",
      "Epoch 427 / 500 | iteration 20 / 30 | Total Loss: 3.448601484298706 | KNN Loss: 2.443129062652588 | BCE Loss: 1.0054724216461182\n",
      "Epoch 427 / 500 | iteration 25 / 30 | Total Loss: 3.4995622634887695 | KNN Loss: 2.4860405921936035 | BCE Loss: 1.013521671295166\n",
      "Epoch 428 / 500 | iteration 0 / 30 | Total Loss: 3.4704039096832275 | KNN Loss: 2.455383062362671 | BCE Loss: 1.0150208473205566\n",
      "Epoch 428 / 500 | iteration 5 / 30 | Total Loss: 3.466538906097412 | KNN Loss: 2.475832223892212 | BCE Loss: 0.9907065629959106\n",
      "Epoch 428 / 500 | iteration 10 / 30 | Total Loss: 3.4655468463897705 | KNN Loss: 2.4547595977783203 | BCE Loss: 1.0107872486114502\n",
      "Epoch 428 / 500 | iteration 15 / 30 | Total Loss: 3.4153599739074707 | KNN Loss: 2.4248342514038086 | BCE Loss: 0.9905256032943726\n",
      "Epoch 428 / 500 | iteration 20 / 30 | Total Loss: 3.461094379425049 | KNN Loss: 2.440182685852051 | BCE Loss: 1.0209115743637085\n",
      "Epoch 428 / 500 | iteration 25 / 30 | Total Loss: 3.4426257610321045 | KNN Loss: 2.441819190979004 | BCE Loss: 1.0008065700531006\n",
      "Epoch 429 / 500 | iteration 0 / 30 | Total Loss: 3.4761803150177 | KNN Loss: 2.449275493621826 | BCE Loss: 1.026904821395874\n",
      "Epoch 429 / 500 | iteration 5 / 30 | Total Loss: 3.52142333984375 | KNN Loss: 2.492278575897217 | BCE Loss: 1.0291446447372437\n",
      "Epoch 429 / 500 | iteration 10 / 30 | Total Loss: 3.425304889678955 | KNN Loss: 2.4326515197753906 | BCE Loss: 0.9926533699035645\n",
      "Epoch 429 / 500 | iteration 15 / 30 | Total Loss: 3.4910831451416016 | KNN Loss: 2.471353054046631 | BCE Loss: 1.0197300910949707\n",
      "Epoch 429 / 500 | iteration 20 / 30 | Total Loss: 3.4610695838928223 | KNN Loss: 2.430908441543579 | BCE Loss: 1.0301612615585327\n",
      "Epoch 429 / 500 | iteration 25 / 30 | Total Loss: 3.4534173011779785 | KNN Loss: 2.4394469261169434 | BCE Loss: 1.0139703750610352\n",
      "Epoch 430 / 500 | iteration 0 / 30 | Total Loss: 3.516505002975464 | KNN Loss: 2.4722414016723633 | BCE Loss: 1.0442636013031006\n",
      "Epoch 430 / 500 | iteration 5 / 30 | Total Loss: 3.4840502738952637 | KNN Loss: 2.469352960586548 | BCE Loss: 1.0146974325180054\n",
      "Epoch 430 / 500 | iteration 10 / 30 | Total Loss: 3.4937760829925537 | KNN Loss: 2.4822452068328857 | BCE Loss: 1.011530876159668\n",
      "Epoch 430 / 500 | iteration 15 / 30 | Total Loss: 3.485640525817871 | KNN Loss: 2.4914844036102295 | BCE Loss: 0.9941560626029968\n",
      "Epoch 430 / 500 | iteration 20 / 30 | Total Loss: 3.5005054473876953 | KNN Loss: 2.4694650173187256 | BCE Loss: 1.0310404300689697\n",
      "Epoch 430 / 500 | iteration 25 / 30 | Total Loss: 3.41619873046875 | KNN Loss: 2.4330244064331055 | BCE Loss: 0.9831743836402893\n",
      "Epoch 431 / 500 | iteration 0 / 30 | Total Loss: 3.4846854209899902 | KNN Loss: 2.471503496170044 | BCE Loss: 1.0131819248199463\n",
      "Epoch 431 / 500 | iteration 5 / 30 | Total Loss: 3.452068328857422 | KNN Loss: 2.422606945037842 | BCE Loss: 1.02946138381958\n",
      "Epoch 431 / 500 | iteration 10 / 30 | Total Loss: 3.5127925872802734 | KNN Loss: 2.490246295928955 | BCE Loss: 1.0225461721420288\n",
      "Epoch 431 / 500 | iteration 15 / 30 | Total Loss: 3.479012966156006 | KNN Loss: 2.4210081100463867 | BCE Loss: 1.0580048561096191\n",
      "Epoch 431 / 500 | iteration 20 / 30 | Total Loss: 3.478487491607666 | KNN Loss: 2.451777219772339 | BCE Loss: 1.0267101526260376\n",
      "Epoch 431 / 500 | iteration 25 / 30 | Total Loss: 3.4661355018615723 | KNN Loss: 2.4557299613952637 | BCE Loss: 1.0104055404663086\n",
      "Epoch 432 / 500 | iteration 0 / 30 | Total Loss: 3.446559190750122 | KNN Loss: 2.4647738933563232 | BCE Loss: 0.981785237789154\n",
      "Epoch 432 / 500 | iteration 5 / 30 | Total Loss: 3.434084892272949 | KNN Loss: 2.426741123199463 | BCE Loss: 1.0073437690734863\n",
      "Epoch 432 / 500 | iteration 10 / 30 | Total Loss: 3.4452438354492188 | KNN Loss: 2.4505257606506348 | BCE Loss: 0.9947180151939392\n",
      "Epoch 432 / 500 | iteration 15 / 30 | Total Loss: 3.5125980377197266 | KNN Loss: 2.4569363594055176 | BCE Loss: 1.0556617975234985\n",
      "Epoch 432 / 500 | iteration 20 / 30 | Total Loss: 3.4534475803375244 | KNN Loss: 2.425051212310791 | BCE Loss: 1.0283963680267334\n",
      "Epoch 432 / 500 | iteration 25 / 30 | Total Loss: 3.479962110519409 | KNN Loss: 2.474815607070923 | BCE Loss: 1.0051465034484863\n",
      "Epoch 433 / 500 | iteration 0 / 30 | Total Loss: 3.533942699432373 | KNN Loss: 2.4729466438293457 | BCE Loss: 1.0609959363937378\n",
      "Epoch 433 / 500 | iteration 5 / 30 | Total Loss: 3.458078145980835 | KNN Loss: 2.4369969367980957 | BCE Loss: 1.0210812091827393\n",
      "Epoch 433 / 500 | iteration 10 / 30 | Total Loss: 3.49686598777771 | KNN Loss: 2.465970754623413 | BCE Loss: 1.0308952331542969\n",
      "Epoch 433 / 500 | iteration 15 / 30 | Total Loss: 3.4151835441589355 | KNN Loss: 2.42927885055542 | BCE Loss: 0.9859046936035156\n",
      "Epoch 433 / 500 | iteration 20 / 30 | Total Loss: 3.4306106567382812 | KNN Loss: 2.4248132705688477 | BCE Loss: 1.005797266960144\n",
      "Epoch 433 / 500 | iteration 25 / 30 | Total Loss: 3.4726603031158447 | KNN Loss: 2.448240280151367 | BCE Loss: 1.0244200229644775\n",
      "Epoch 434 / 500 | iteration 0 / 30 | Total Loss: 3.474982261657715 | KNN Loss: 2.4382576942443848 | BCE Loss: 1.03672456741333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 434 / 500 | iteration 5 / 30 | Total Loss: 3.496371269226074 | KNN Loss: 2.46533203125 | BCE Loss: 1.0310393571853638\n",
      "Epoch 434 / 500 | iteration 10 / 30 | Total Loss: 3.450620174407959 | KNN Loss: 2.459688186645508 | BCE Loss: 0.9909321069717407\n",
      "Epoch 434 / 500 | iteration 15 / 30 | Total Loss: 3.4168701171875 | KNN Loss: 2.4359307289123535 | BCE Loss: 0.980939507484436\n",
      "Epoch 434 / 500 | iteration 20 / 30 | Total Loss: 3.5168821811676025 | KNN Loss: 2.4583756923675537 | BCE Loss: 1.0585064888000488\n",
      "Epoch 434 / 500 | iteration 25 / 30 | Total Loss: 3.4602153301239014 | KNN Loss: 2.4537246227264404 | BCE Loss: 1.006490707397461\n",
      "Epoch 435 / 500 | iteration 0 / 30 | Total Loss: 3.467649459838867 | KNN Loss: 2.4405202865600586 | BCE Loss: 1.0271291732788086\n",
      "Epoch 435 / 500 | iteration 5 / 30 | Total Loss: 3.4647293090820312 | KNN Loss: 2.442828893661499 | BCE Loss: 1.0219004154205322\n",
      "Epoch 435 / 500 | iteration 10 / 30 | Total Loss: 3.462029218673706 | KNN Loss: 2.4423508644104004 | BCE Loss: 1.0196783542633057\n",
      "Epoch 435 / 500 | iteration 15 / 30 | Total Loss: 3.458199977874756 | KNN Loss: 2.4498324394226074 | BCE Loss: 1.0083675384521484\n",
      "Epoch 435 / 500 | iteration 20 / 30 | Total Loss: 3.4733786582946777 | KNN Loss: 2.4458625316619873 | BCE Loss: 1.0275161266326904\n",
      "Epoch 435 / 500 | iteration 25 / 30 | Total Loss: 3.419915199279785 | KNN Loss: 2.4134318828582764 | BCE Loss: 1.0064831972122192\n",
      "Epoch 436 / 500 | iteration 0 / 30 | Total Loss: 3.478208065032959 | KNN Loss: 2.4384453296661377 | BCE Loss: 1.0397626161575317\n",
      "Epoch 436 / 500 | iteration 5 / 30 | Total Loss: 3.4581236839294434 | KNN Loss: 2.4519262313842773 | BCE Loss: 1.0061975717544556\n",
      "Epoch 436 / 500 | iteration 10 / 30 | Total Loss: 3.4573047161102295 | KNN Loss: 2.4458322525024414 | BCE Loss: 1.011472463607788\n",
      "Epoch 436 / 500 | iteration 15 / 30 | Total Loss: 3.488140106201172 | KNN Loss: 2.4579648971557617 | BCE Loss: 1.0301752090454102\n",
      "Epoch 436 / 500 | iteration 20 / 30 | Total Loss: 3.4394114017486572 | KNN Loss: 2.4374594688415527 | BCE Loss: 1.0019519329071045\n",
      "Epoch 436 / 500 | iteration 25 / 30 | Total Loss: 3.5028021335601807 | KNN Loss: 2.4645590782165527 | BCE Loss: 1.038243055343628\n",
      "Epoch   437: reducing learning rate of group 0 to 2.7927e-06.\n",
      "Epoch 437 / 500 | iteration 0 / 30 | Total Loss: 3.4547390937805176 | KNN Loss: 2.4624929428100586 | BCE Loss: 0.992246150970459\n",
      "Epoch 437 / 500 | iteration 5 / 30 | Total Loss: 3.43868350982666 | KNN Loss: 2.434643030166626 | BCE Loss: 1.0040403604507446\n",
      "Epoch 437 / 500 | iteration 10 / 30 | Total Loss: 3.4308383464813232 | KNN Loss: 2.4354183673858643 | BCE Loss: 0.9954199194908142\n",
      "Epoch 437 / 500 | iteration 15 / 30 | Total Loss: 3.4785470962524414 | KNN Loss: 2.452767848968506 | BCE Loss: 1.025779366493225\n",
      "Epoch 437 / 500 | iteration 20 / 30 | Total Loss: 3.499967575073242 | KNN Loss: 2.4783544540405273 | BCE Loss: 1.0216130018234253\n",
      "Epoch 437 / 500 | iteration 25 / 30 | Total Loss: 3.477263927459717 | KNN Loss: 2.469527244567871 | BCE Loss: 1.0077365636825562\n",
      "Epoch 438 / 500 | iteration 0 / 30 | Total Loss: 3.495300769805908 | KNN Loss: 2.4691948890686035 | BCE Loss: 1.0261059999465942\n",
      "Epoch 438 / 500 | iteration 5 / 30 | Total Loss: 3.453125 | KNN Loss: 2.4314398765563965 | BCE Loss: 1.021685242652893\n",
      "Epoch 438 / 500 | iteration 10 / 30 | Total Loss: 3.4649510383605957 | KNN Loss: 2.455543279647827 | BCE Loss: 1.009407639503479\n",
      "Epoch 438 / 500 | iteration 15 / 30 | Total Loss: 3.432076930999756 | KNN Loss: 2.422626256942749 | BCE Loss: 1.0094507932662964\n",
      "Epoch 438 / 500 | iteration 20 / 30 | Total Loss: 3.5208957195281982 | KNN Loss: 2.475698709487915 | BCE Loss: 1.0451970100402832\n",
      "Epoch 438 / 500 | iteration 25 / 30 | Total Loss: 3.5131306648254395 | KNN Loss: 2.4822545051574707 | BCE Loss: 1.0308761596679688\n",
      "Epoch 439 / 500 | iteration 0 / 30 | Total Loss: 3.4566850662231445 | KNN Loss: 2.4325313568115234 | BCE Loss: 1.0241538286209106\n",
      "Epoch 439 / 500 | iteration 5 / 30 | Total Loss: 3.4454147815704346 | KNN Loss: 2.436112642288208 | BCE Loss: 1.0093021392822266\n",
      "Epoch 439 / 500 | iteration 10 / 30 | Total Loss: 3.444896697998047 | KNN Loss: 2.447453022003174 | BCE Loss: 0.9974435567855835\n",
      "Epoch 439 / 500 | iteration 15 / 30 | Total Loss: 3.471853256225586 | KNN Loss: 2.445957660675049 | BCE Loss: 1.025895595550537\n",
      "Epoch 439 / 500 | iteration 20 / 30 | Total Loss: 3.4794065952301025 | KNN Loss: 2.4656119346618652 | BCE Loss: 1.0137946605682373\n",
      "Epoch 439 / 500 | iteration 25 / 30 | Total Loss: 3.510284900665283 | KNN Loss: 2.470163345336914 | BCE Loss: 1.0401216745376587\n",
      "Epoch 440 / 500 | iteration 0 / 30 | Total Loss: 3.4601662158966064 | KNN Loss: 2.4391722679138184 | BCE Loss: 1.020993947982788\n",
      "Epoch 440 / 500 | iteration 5 / 30 | Total Loss: 3.506643295288086 | KNN Loss: 2.488520383834839 | BCE Loss: 1.018122911453247\n",
      "Epoch 440 / 500 | iteration 10 / 30 | Total Loss: 3.529603958129883 | KNN Loss: 2.4773943424224854 | BCE Loss: 1.052209734916687\n",
      "Epoch 440 / 500 | iteration 15 / 30 | Total Loss: 3.482473850250244 | KNN Loss: 2.4504029750823975 | BCE Loss: 1.0320707559585571\n",
      "Epoch 440 / 500 | iteration 20 / 30 | Total Loss: 3.4626386165618896 | KNN Loss: 2.448888063430786 | BCE Loss: 1.0137505531311035\n",
      "Epoch 440 / 500 | iteration 25 / 30 | Total Loss: 3.4478564262390137 | KNN Loss: 2.4665586948394775 | BCE Loss: 0.9812977910041809\n",
      "Epoch 441 / 500 | iteration 0 / 30 | Total Loss: 3.450314998626709 | KNN Loss: 2.4489681720733643 | BCE Loss: 1.0013467073440552\n",
      "Epoch 441 / 500 | iteration 5 / 30 | Total Loss: 3.4640274047851562 | KNN Loss: 2.470736503601074 | BCE Loss: 0.9932910203933716\n",
      "Epoch 441 / 500 | iteration 10 / 30 | Total Loss: 3.478515148162842 | KNN Loss: 2.4537692070007324 | BCE Loss: 1.0247459411621094\n",
      "Epoch 441 / 500 | iteration 15 / 30 | Total Loss: 3.5146467685699463 | KNN Loss: 2.4720354080200195 | BCE Loss: 1.0426113605499268\n",
      "Epoch 441 / 500 | iteration 20 / 30 | Total Loss: 3.4949402809143066 | KNN Loss: 2.4561336040496826 | BCE Loss: 1.038806676864624\n",
      "Epoch 441 / 500 | iteration 25 / 30 | Total Loss: 3.435792922973633 | KNN Loss: 2.4400088787078857 | BCE Loss: 0.9957841038703918\n",
      "Epoch 442 / 500 | iteration 0 / 30 | Total Loss: 3.4915804862976074 | KNN Loss: 2.460566997528076 | BCE Loss: 1.0310136079788208\n",
      "Epoch 442 / 500 | iteration 5 / 30 | Total Loss: 3.474565029144287 | KNN Loss: 2.469163417816162 | BCE Loss: 1.005401611328125\n",
      "Epoch 442 / 500 | iteration 10 / 30 | Total Loss: 3.513370990753174 | KNN Loss: 2.464852809906006 | BCE Loss: 1.0485180616378784\n",
      "Epoch 442 / 500 | iteration 15 / 30 | Total Loss: 3.5318403244018555 | KNN Loss: 2.4688427448272705 | BCE Loss: 1.0629974603652954\n",
      "Epoch 442 / 500 | iteration 20 / 30 | Total Loss: 3.465717077255249 | KNN Loss: 2.4684290885925293 | BCE Loss: 0.9972879886627197\n",
      "Epoch 442 / 500 | iteration 25 / 30 | Total Loss: 3.48150372505188 | KNN Loss: 2.4634995460510254 | BCE Loss: 1.0180041790008545\n",
      "Epoch 443 / 500 | iteration 0 / 30 | Total Loss: 3.4580671787261963 | KNN Loss: 2.463416337966919 | BCE Loss: 0.9946508407592773\n",
      "Epoch 443 / 500 | iteration 5 / 30 | Total Loss: 3.451965808868408 | KNN Loss: 2.4300484657287598 | BCE Loss: 1.0219173431396484\n",
      "Epoch 443 / 500 | iteration 10 / 30 | Total Loss: 3.4385814666748047 | KNN Loss: 2.4278934001922607 | BCE Loss: 1.010688066482544\n",
      "Epoch 443 / 500 | iteration 15 / 30 | Total Loss: 3.498979091644287 | KNN Loss: 2.464329481124878 | BCE Loss: 1.0346496105194092\n",
      "Epoch 443 / 500 | iteration 20 / 30 | Total Loss: 3.4601407051086426 | KNN Loss: 2.4284934997558594 | BCE Loss: 1.0316472053527832\n",
      "Epoch 443 / 500 | iteration 25 / 30 | Total Loss: 3.489281415939331 | KNN Loss: 2.473128080368042 | BCE Loss: 1.016153335571289\n",
      "Epoch 444 / 500 | iteration 0 / 30 | Total Loss: 3.480276107788086 | KNN Loss: 2.4535772800445557 | BCE Loss: 1.0266989469528198\n",
      "Epoch 444 / 500 | iteration 5 / 30 | Total Loss: 3.4429373741149902 | KNN Loss: 2.4220659732818604 | BCE Loss: 1.0208715200424194\n",
      "Epoch 444 / 500 | iteration 10 / 30 | Total Loss: 3.493551015853882 | KNN Loss: 2.4747233390808105 | BCE Loss: 1.0188276767730713\n",
      "Epoch 444 / 500 | iteration 15 / 30 | Total Loss: 3.4845292568206787 | KNN Loss: 2.436277389526367 | BCE Loss: 1.0482518672943115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 444 / 500 | iteration 20 / 30 | Total Loss: 3.4964520931243896 | KNN Loss: 2.4649245738983154 | BCE Loss: 1.0315275192260742\n",
      "Epoch 444 / 500 | iteration 25 / 30 | Total Loss: 3.439265727996826 | KNN Loss: 2.4136388301849365 | BCE Loss: 1.0256270170211792\n",
      "Epoch 445 / 500 | iteration 0 / 30 | Total Loss: 3.471036195755005 | KNN Loss: 2.4681406021118164 | BCE Loss: 1.0028955936431885\n",
      "Epoch 445 / 500 | iteration 5 / 30 | Total Loss: 3.4477429389953613 | KNN Loss: 2.438974618911743 | BCE Loss: 1.0087684392929077\n",
      "Epoch 445 / 500 | iteration 10 / 30 | Total Loss: 3.4605307579040527 | KNN Loss: 2.4481873512268066 | BCE Loss: 1.0123432874679565\n",
      "Epoch 445 / 500 | iteration 15 / 30 | Total Loss: 3.410706043243408 | KNN Loss: 2.438807487487793 | BCE Loss: 0.9718984961509705\n",
      "Epoch 445 / 500 | iteration 20 / 30 | Total Loss: 3.47870135307312 | KNN Loss: 2.4583938121795654 | BCE Loss: 1.0203075408935547\n",
      "Epoch 445 / 500 | iteration 25 / 30 | Total Loss: 3.456961154937744 | KNN Loss: 2.4455015659332275 | BCE Loss: 1.011459469795227\n",
      "Epoch 446 / 500 | iteration 0 / 30 | Total Loss: 3.4361066818237305 | KNN Loss: 2.4366583824157715 | BCE Loss: 0.999448299407959\n",
      "Epoch 446 / 500 | iteration 5 / 30 | Total Loss: 3.4894678592681885 | KNN Loss: 2.4599099159240723 | BCE Loss: 1.0295579433441162\n",
      "Epoch 446 / 500 | iteration 10 / 30 | Total Loss: 3.4927964210510254 | KNN Loss: 2.4757606983184814 | BCE Loss: 1.017035722732544\n",
      "Epoch 446 / 500 | iteration 15 / 30 | Total Loss: 3.4851315021514893 | KNN Loss: 2.4640016555786133 | BCE Loss: 1.021129846572876\n",
      "Epoch 446 / 500 | iteration 20 / 30 | Total Loss: 3.487630844116211 | KNN Loss: 2.4913229942321777 | BCE Loss: 0.9963077306747437\n",
      "Epoch 446 / 500 | iteration 25 / 30 | Total Loss: 3.468329429626465 | KNN Loss: 2.4561543464660645 | BCE Loss: 1.0121750831604004\n",
      "Epoch 447 / 500 | iteration 0 / 30 | Total Loss: 3.4885830879211426 | KNN Loss: 2.4796571731567383 | BCE Loss: 1.0089260339736938\n",
      "Epoch 447 / 500 | iteration 5 / 30 | Total Loss: 3.4948980808258057 | KNN Loss: 2.4864611625671387 | BCE Loss: 1.008436918258667\n",
      "Epoch 447 / 500 | iteration 10 / 30 | Total Loss: 3.434870958328247 | KNN Loss: 2.436487913131714 | BCE Loss: 0.998383104801178\n",
      "Epoch 447 / 500 | iteration 15 / 30 | Total Loss: 3.4841110706329346 | KNN Loss: 2.4557974338531494 | BCE Loss: 1.0283136367797852\n",
      "Epoch 447 / 500 | iteration 20 / 30 | Total Loss: 3.4549789428710938 | KNN Loss: 2.4467968940734863 | BCE Loss: 1.0081820487976074\n",
      "Epoch 447 / 500 | iteration 25 / 30 | Total Loss: 3.4812960624694824 | KNN Loss: 2.4574191570281982 | BCE Loss: 1.0238770246505737\n",
      "Epoch   448: reducing learning rate of group 0 to 1.9549e-06.\n",
      "Epoch 448 / 500 | iteration 0 / 30 | Total Loss: 3.4541802406311035 | KNN Loss: 2.4487123489379883 | BCE Loss: 1.0054678916931152\n",
      "Epoch 448 / 500 | iteration 5 / 30 | Total Loss: 3.440589189529419 | KNN Loss: 2.4538278579711914 | BCE Loss: 0.9867613315582275\n",
      "Epoch 448 / 500 | iteration 10 / 30 | Total Loss: 3.442392110824585 | KNN Loss: 2.431501626968384 | BCE Loss: 1.0108904838562012\n",
      "Epoch 448 / 500 | iteration 15 / 30 | Total Loss: 3.493910312652588 | KNN Loss: 2.4756407737731934 | BCE Loss: 1.0182695388793945\n",
      "Epoch 448 / 500 | iteration 20 / 30 | Total Loss: 3.493971109390259 | KNN Loss: 2.477346181869507 | BCE Loss: 1.016624927520752\n",
      "Epoch 448 / 500 | iteration 25 / 30 | Total Loss: 3.4676897525787354 | KNN Loss: 2.440950393676758 | BCE Loss: 1.0267393589019775\n",
      "Epoch 449 / 500 | iteration 0 / 30 | Total Loss: 3.4335949420928955 | KNN Loss: 2.44020676612854 | BCE Loss: 0.9933881759643555\n",
      "Epoch 449 / 500 | iteration 5 / 30 | Total Loss: 3.465423107147217 | KNN Loss: 2.4580078125 | BCE Loss: 1.0074152946472168\n",
      "Epoch 449 / 500 | iteration 10 / 30 | Total Loss: 3.417774200439453 | KNN Loss: 2.4284253120422363 | BCE Loss: 0.9893487691879272\n",
      "Epoch 449 / 500 | iteration 15 / 30 | Total Loss: 3.4861791133880615 | KNN Loss: 2.478081703186035 | BCE Loss: 1.0080974102020264\n",
      "Epoch 449 / 500 | iteration 20 / 30 | Total Loss: 3.465770959854126 | KNN Loss: 2.4473886489868164 | BCE Loss: 1.0183823108673096\n",
      "Epoch 449 / 500 | iteration 25 / 30 | Total Loss: 3.4708964824676514 | KNN Loss: 2.4462718963623047 | BCE Loss: 1.0246245861053467\n",
      "Epoch 450 / 500 | iteration 0 / 30 | Total Loss: 3.483551025390625 | KNN Loss: 2.4426040649414062 | BCE Loss: 1.0409468412399292\n",
      "Epoch 450 / 500 | iteration 5 / 30 | Total Loss: 3.4682321548461914 | KNN Loss: 2.463026285171509 | BCE Loss: 1.0052058696746826\n",
      "Epoch 450 / 500 | iteration 10 / 30 | Total Loss: 3.4351272583007812 | KNN Loss: 2.4504780769348145 | BCE Loss: 0.9846490621566772\n",
      "Epoch 450 / 500 | iteration 15 / 30 | Total Loss: 3.451861619949341 | KNN Loss: 2.4504237174987793 | BCE Loss: 1.0014379024505615\n",
      "Epoch 450 / 500 | iteration 20 / 30 | Total Loss: 3.433152914047241 | KNN Loss: 2.4160053730010986 | BCE Loss: 1.0171475410461426\n",
      "Epoch 450 / 500 | iteration 25 / 30 | Total Loss: 3.4860074520111084 | KNN Loss: 2.4520263671875 | BCE Loss: 1.0339810848236084\n",
      "Epoch 451 / 500 | iteration 0 / 30 | Total Loss: 3.4260034561157227 | KNN Loss: 2.4337520599365234 | BCE Loss: 0.9922513365745544\n",
      "Epoch 451 / 500 | iteration 5 / 30 | Total Loss: 3.4528284072875977 | KNN Loss: 2.441380023956299 | BCE Loss: 1.0114485025405884\n",
      "Epoch 451 / 500 | iteration 10 / 30 | Total Loss: 3.477064609527588 | KNN Loss: 2.469364643096924 | BCE Loss: 1.007699966430664\n",
      "Epoch 451 / 500 | iteration 15 / 30 | Total Loss: 3.4356374740600586 | KNN Loss: 2.422349691390991 | BCE Loss: 1.0132877826690674\n",
      "Epoch 451 / 500 | iteration 20 / 30 | Total Loss: 3.4769022464752197 | KNN Loss: 2.4624948501586914 | BCE Loss: 1.0144073963165283\n",
      "Epoch 451 / 500 | iteration 25 / 30 | Total Loss: 3.4409003257751465 | KNN Loss: 2.416240692138672 | BCE Loss: 1.024659514427185\n",
      "Epoch 452 / 500 | iteration 0 / 30 | Total Loss: 3.501412868499756 | KNN Loss: 2.4641575813293457 | BCE Loss: 1.0372551679611206\n",
      "Epoch 452 / 500 | iteration 5 / 30 | Total Loss: 3.478829860687256 | KNN Loss: 2.4696455001831055 | BCE Loss: 1.0091843605041504\n",
      "Epoch 452 / 500 | iteration 10 / 30 | Total Loss: 3.4845070838928223 | KNN Loss: 2.471295118331909 | BCE Loss: 1.0132120847702026\n",
      "Epoch 452 / 500 | iteration 15 / 30 | Total Loss: 3.4689769744873047 | KNN Loss: 2.469618797302246 | BCE Loss: 0.9993581771850586\n",
      "Epoch 452 / 500 | iteration 20 / 30 | Total Loss: 3.5170791149139404 | KNN Loss: 2.479050397872925 | BCE Loss: 1.0380287170410156\n",
      "Epoch 452 / 500 | iteration 25 / 30 | Total Loss: 3.4217689037323 | KNN Loss: 2.420809507369995 | BCE Loss: 1.0009593963623047\n",
      "Epoch 453 / 500 | iteration 0 / 30 | Total Loss: 3.4715044498443604 | KNN Loss: 2.4478487968444824 | BCE Loss: 1.023655652999878\n",
      "Epoch 453 / 500 | iteration 5 / 30 | Total Loss: 3.4793877601623535 | KNN Loss: 2.461738348007202 | BCE Loss: 1.0176494121551514\n",
      "Epoch 453 / 500 | iteration 10 / 30 | Total Loss: 3.4670310020446777 | KNN Loss: 2.455488681793213 | BCE Loss: 1.0115424394607544\n",
      "Epoch 453 / 500 | iteration 15 / 30 | Total Loss: 3.4932613372802734 | KNN Loss: 2.444558620452881 | BCE Loss: 1.0487028360366821\n",
      "Epoch 453 / 500 | iteration 20 / 30 | Total Loss: 3.4806814193725586 | KNN Loss: 2.4564640522003174 | BCE Loss: 1.0242173671722412\n",
      "Epoch 453 / 500 | iteration 25 / 30 | Total Loss: 3.4811954498291016 | KNN Loss: 2.4483494758605957 | BCE Loss: 1.0328459739685059\n",
      "Epoch 454 / 500 | iteration 0 / 30 | Total Loss: 3.4993114471435547 | KNN Loss: 2.477750062942505 | BCE Loss: 1.0215612649917603\n",
      "Epoch 454 / 500 | iteration 5 / 30 | Total Loss: 3.487293243408203 | KNN Loss: 2.475335121154785 | BCE Loss: 1.011958122253418\n",
      "Epoch 454 / 500 | iteration 10 / 30 | Total Loss: 3.4773600101470947 | KNN Loss: 2.4459071159362793 | BCE Loss: 1.0314528942108154\n",
      "Epoch 454 / 500 | iteration 15 / 30 | Total Loss: 3.5020461082458496 | KNN Loss: 2.4582810401916504 | BCE Loss: 1.0437651872634888\n",
      "Epoch 454 / 500 | iteration 20 / 30 | Total Loss: 3.457984447479248 | KNN Loss: 2.44979190826416 | BCE Loss: 1.008192539215088\n",
      "Epoch 454 / 500 | iteration 25 / 30 | Total Loss: 3.4596219062805176 | KNN Loss: 2.45054030418396 | BCE Loss: 1.0090816020965576\n",
      "Epoch 455 / 500 | iteration 0 / 30 | Total Loss: 3.4882664680480957 | KNN Loss: 2.466348648071289 | BCE Loss: 1.0219178199768066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 455 / 500 | iteration 5 / 30 | Total Loss: 3.506587505340576 | KNN Loss: 2.482028007507324 | BCE Loss: 1.0245593786239624\n",
      "Epoch 455 / 500 | iteration 10 / 30 | Total Loss: 3.4664971828460693 | KNN Loss: 2.4481492042541504 | BCE Loss: 1.018347978591919\n",
      "Epoch 455 / 500 | iteration 15 / 30 | Total Loss: 3.4151315689086914 | KNN Loss: 2.4421021938323975 | BCE Loss: 0.9730292558670044\n",
      "Epoch 455 / 500 | iteration 20 / 30 | Total Loss: 3.4269938468933105 | KNN Loss: 2.422834634780884 | BCE Loss: 1.0041590929031372\n",
      "Epoch 455 / 500 | iteration 25 / 30 | Total Loss: 3.4638149738311768 | KNN Loss: 2.476300001144409 | BCE Loss: 0.9875149130821228\n",
      "Epoch 456 / 500 | iteration 0 / 30 | Total Loss: 3.5066330432891846 | KNN Loss: 2.4695498943328857 | BCE Loss: 1.0370831489562988\n",
      "Epoch 456 / 500 | iteration 5 / 30 | Total Loss: 3.490180730819702 | KNN Loss: 2.455274820327759 | BCE Loss: 1.0349059104919434\n",
      "Epoch 456 / 500 | iteration 10 / 30 | Total Loss: 3.4882771968841553 | KNN Loss: 2.4746177196502686 | BCE Loss: 1.0136594772338867\n",
      "Epoch 456 / 500 | iteration 15 / 30 | Total Loss: 3.4784059524536133 | KNN Loss: 2.4363386631011963 | BCE Loss: 1.0420671701431274\n",
      "Epoch 456 / 500 | iteration 20 / 30 | Total Loss: 3.4745545387268066 | KNN Loss: 2.4706220626831055 | BCE Loss: 1.0039324760437012\n",
      "Epoch 456 / 500 | iteration 25 / 30 | Total Loss: 3.4767954349517822 | KNN Loss: 2.477381706237793 | BCE Loss: 0.999413788318634\n",
      "Epoch 457 / 500 | iteration 0 / 30 | Total Loss: 3.5036044120788574 | KNN Loss: 2.473814010620117 | BCE Loss: 1.0297904014587402\n",
      "Epoch 457 / 500 | iteration 5 / 30 | Total Loss: 3.4907798767089844 | KNN Loss: 2.4950363636016846 | BCE Loss: 0.9957433938980103\n",
      "Epoch 457 / 500 | iteration 10 / 30 | Total Loss: 3.4479503631591797 | KNN Loss: 2.4463083744049072 | BCE Loss: 1.0016419887542725\n",
      "Epoch 457 / 500 | iteration 15 / 30 | Total Loss: 3.4908759593963623 | KNN Loss: 2.4567012786865234 | BCE Loss: 1.0341746807098389\n",
      "Epoch 457 / 500 | iteration 20 / 30 | Total Loss: 3.436382293701172 | KNN Loss: 2.447354793548584 | BCE Loss: 0.9890275001525879\n",
      "Epoch 457 / 500 | iteration 25 / 30 | Total Loss: 3.4471912384033203 | KNN Loss: 2.4432339668273926 | BCE Loss: 1.0039573907852173\n",
      "Epoch 458 / 500 | iteration 0 / 30 | Total Loss: 3.452387809753418 | KNN Loss: 2.445042133331299 | BCE Loss: 1.0073456764221191\n",
      "Epoch 458 / 500 | iteration 5 / 30 | Total Loss: 3.4731221199035645 | KNN Loss: 2.4629287719726562 | BCE Loss: 1.0101933479309082\n",
      "Epoch 458 / 500 | iteration 10 / 30 | Total Loss: 3.446080207824707 | KNN Loss: 2.431270122528076 | BCE Loss: 1.0148100852966309\n",
      "Epoch 458 / 500 | iteration 15 / 30 | Total Loss: 3.4777915477752686 | KNN Loss: 2.4627790451049805 | BCE Loss: 1.015012502670288\n",
      "Epoch 458 / 500 | iteration 20 / 30 | Total Loss: 3.4068117141723633 | KNN Loss: 2.4333157539367676 | BCE Loss: 0.9734960794448853\n",
      "Epoch 458 / 500 | iteration 25 / 30 | Total Loss: 3.4756107330322266 | KNN Loss: 2.46142315864563 | BCE Loss: 1.0141874551773071\n",
      "Epoch   459: reducing learning rate of group 0 to 1.3684e-06.\n",
      "Epoch 459 / 500 | iteration 0 / 30 | Total Loss: 3.4714345932006836 | KNN Loss: 2.4507808685302734 | BCE Loss: 1.0206537246704102\n",
      "Epoch 459 / 500 | iteration 5 / 30 | Total Loss: 3.4574127197265625 | KNN Loss: 2.4674127101898193 | BCE Loss: 0.9899998903274536\n",
      "Epoch 459 / 500 | iteration 10 / 30 | Total Loss: 3.488644599914551 | KNN Loss: 2.4699788093566895 | BCE Loss: 1.0186656713485718\n",
      "Epoch 459 / 500 | iteration 15 / 30 | Total Loss: 3.4821228981018066 | KNN Loss: 2.4584009647369385 | BCE Loss: 1.0237219333648682\n",
      "Epoch 459 / 500 | iteration 20 / 30 | Total Loss: 3.4690396785736084 | KNN Loss: 2.44690203666687 | BCE Loss: 1.0221376419067383\n",
      "Epoch 459 / 500 | iteration 25 / 30 | Total Loss: 3.4641332626342773 | KNN Loss: 2.4606707096099854 | BCE Loss: 1.0034626722335815\n",
      "Epoch 460 / 500 | iteration 0 / 30 | Total Loss: 3.4471535682678223 | KNN Loss: 2.4661567211151123 | BCE Loss: 0.9809969663619995\n",
      "Epoch 460 / 500 | iteration 5 / 30 | Total Loss: 3.491448163986206 | KNN Loss: 2.477339029312134 | BCE Loss: 1.0141091346740723\n",
      "Epoch 460 / 500 | iteration 10 / 30 | Total Loss: 3.4528589248657227 | KNN Loss: 2.430715560913086 | BCE Loss: 1.0221434831619263\n",
      "Epoch 460 / 500 | iteration 15 / 30 | Total Loss: 3.5007474422454834 | KNN Loss: 2.470510721206665 | BCE Loss: 1.0302367210388184\n",
      "Epoch 460 / 500 | iteration 20 / 30 | Total Loss: 3.483070135116577 | KNN Loss: 2.4857747554779053 | BCE Loss: 0.9972953796386719\n",
      "Epoch 460 / 500 | iteration 25 / 30 | Total Loss: 3.4891738891601562 | KNN Loss: 2.458467721939087 | BCE Loss: 1.0307061672210693\n",
      "Epoch 461 / 500 | iteration 0 / 30 | Total Loss: 3.514512538909912 | KNN Loss: 2.5070223808288574 | BCE Loss: 1.0074901580810547\n",
      "Epoch 461 / 500 | iteration 5 / 30 | Total Loss: 3.4452004432678223 | KNN Loss: 2.471552610397339 | BCE Loss: 0.9736478328704834\n",
      "Epoch 461 / 500 | iteration 10 / 30 | Total Loss: 3.495579481124878 | KNN Loss: 2.443539619445801 | BCE Loss: 1.0520398616790771\n",
      "Epoch 461 / 500 | iteration 15 / 30 | Total Loss: 3.488128423690796 | KNN Loss: 2.4460902214050293 | BCE Loss: 1.0420382022857666\n",
      "Epoch 461 / 500 | iteration 20 / 30 | Total Loss: 3.4620728492736816 | KNN Loss: 2.4564521312713623 | BCE Loss: 1.0056207180023193\n",
      "Epoch 461 / 500 | iteration 25 / 30 | Total Loss: 3.4734697341918945 | KNN Loss: 2.4547581672668457 | BCE Loss: 1.0187116861343384\n",
      "Epoch 462 / 500 | iteration 0 / 30 | Total Loss: 3.4646236896514893 | KNN Loss: 2.46055006980896 | BCE Loss: 1.0040736198425293\n",
      "Epoch 462 / 500 | iteration 5 / 30 | Total Loss: 3.4909021854400635 | KNN Loss: 2.45289945602417 | BCE Loss: 1.0380027294158936\n",
      "Epoch 462 / 500 | iteration 10 / 30 | Total Loss: 3.4400525093078613 | KNN Loss: 2.4453794956207275 | BCE Loss: 0.9946731328964233\n",
      "Epoch 462 / 500 | iteration 15 / 30 | Total Loss: 3.447580337524414 | KNN Loss: 2.41288161277771 | BCE Loss: 1.034698724746704\n",
      "Epoch 462 / 500 | iteration 20 / 30 | Total Loss: 3.4935569763183594 | KNN Loss: 2.4712562561035156 | BCE Loss: 1.0223006010055542\n",
      "Epoch 462 / 500 | iteration 25 / 30 | Total Loss: 3.4763221740722656 | KNN Loss: 2.4432334899902344 | BCE Loss: 1.0330888032913208\n",
      "Epoch 463 / 500 | iteration 0 / 30 | Total Loss: 3.4501798152923584 | KNN Loss: 2.441601514816284 | BCE Loss: 1.0085783004760742\n",
      "Epoch 463 / 500 | iteration 5 / 30 | Total Loss: 3.5052568912506104 | KNN Loss: 2.4682703018188477 | BCE Loss: 1.0369865894317627\n",
      "Epoch 463 / 500 | iteration 10 / 30 | Total Loss: 3.4828903675079346 | KNN Loss: 2.4668033123016357 | BCE Loss: 1.0160870552062988\n",
      "Epoch 463 / 500 | iteration 15 / 30 | Total Loss: 3.4623656272888184 | KNN Loss: 2.465352773666382 | BCE Loss: 0.9970128536224365\n",
      "Epoch 463 / 500 | iteration 20 / 30 | Total Loss: 3.4974937438964844 | KNN Loss: 2.4693572521209717 | BCE Loss: 1.0281364917755127\n",
      "Epoch 463 / 500 | iteration 25 / 30 | Total Loss: 3.4757485389709473 | KNN Loss: 2.4635045528411865 | BCE Loss: 1.0122439861297607\n",
      "Epoch 464 / 500 | iteration 0 / 30 | Total Loss: 3.4750514030456543 | KNN Loss: 2.4611880779266357 | BCE Loss: 1.0138633251190186\n",
      "Epoch 464 / 500 | iteration 5 / 30 | Total Loss: 3.4724297523498535 | KNN Loss: 2.450490713119507 | BCE Loss: 1.0219390392303467\n",
      "Epoch 464 / 500 | iteration 10 / 30 | Total Loss: 3.4617416858673096 | KNN Loss: 2.4577410221099854 | BCE Loss: 1.0040006637573242\n",
      "Epoch 464 / 500 | iteration 15 / 30 | Total Loss: 3.4583094120025635 | KNN Loss: 2.426037073135376 | BCE Loss: 1.0322723388671875\n",
      "Epoch 464 / 500 | iteration 20 / 30 | Total Loss: 3.5152130126953125 | KNN Loss: 2.5024547576904297 | BCE Loss: 1.0127583742141724\n",
      "Epoch 464 / 500 | iteration 25 / 30 | Total Loss: 3.5039865970611572 | KNN Loss: 2.476624011993408 | BCE Loss: 1.027362585067749\n",
      "Epoch 465 / 500 | iteration 0 / 30 | Total Loss: 3.4622600078582764 | KNN Loss: 2.449087619781494 | BCE Loss: 1.0131723880767822\n",
      "Epoch 465 / 500 | iteration 5 / 30 | Total Loss: 3.42269229888916 | KNN Loss: 2.426811456680298 | BCE Loss: 0.9958808422088623\n",
      "Epoch 465 / 500 | iteration 10 / 30 | Total Loss: 3.4767074584960938 | KNN Loss: 2.465141773223877 | BCE Loss: 1.0115656852722168\n",
      "Epoch 465 / 500 | iteration 15 / 30 | Total Loss: 3.483680009841919 | KNN Loss: 2.4583561420440674 | BCE Loss: 1.0253238677978516\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 465 / 500 | iteration 20 / 30 | Total Loss: 3.490302085876465 | KNN Loss: 2.4632110595703125 | BCE Loss: 1.0270910263061523\n",
      "Epoch 465 / 500 | iteration 25 / 30 | Total Loss: 3.4911444187164307 | KNN Loss: 2.46443247795105 | BCE Loss: 1.0267119407653809\n",
      "Epoch 466 / 500 | iteration 0 / 30 | Total Loss: 3.4638800621032715 | KNN Loss: 2.4437785148620605 | BCE Loss: 1.020101547241211\n",
      "Epoch 466 / 500 | iteration 5 / 30 | Total Loss: 3.459263563156128 | KNN Loss: 2.4353206157684326 | BCE Loss: 1.0239429473876953\n",
      "Epoch 466 / 500 | iteration 10 / 30 | Total Loss: 3.4581027030944824 | KNN Loss: 2.446424961090088 | BCE Loss: 1.0116777420043945\n",
      "Epoch 466 / 500 | iteration 15 / 30 | Total Loss: 3.4539642333984375 | KNN Loss: 2.453716993331909 | BCE Loss: 1.0002471208572388\n",
      "Epoch 466 / 500 | iteration 20 / 30 | Total Loss: 3.4721603393554688 | KNN Loss: 2.465996265411377 | BCE Loss: 1.0061639547348022\n",
      "Epoch 466 / 500 | iteration 25 / 30 | Total Loss: 3.4632041454315186 | KNN Loss: 2.45499849319458 | BCE Loss: 1.0082056522369385\n",
      "Epoch 467 / 500 | iteration 0 / 30 | Total Loss: 3.4394521713256836 | KNN Loss: 2.4260847568511963 | BCE Loss: 1.0133672952651978\n",
      "Epoch 467 / 500 | iteration 5 / 30 | Total Loss: 3.4322376251220703 | KNN Loss: 2.417203426361084 | BCE Loss: 1.0150341987609863\n",
      "Epoch 467 / 500 | iteration 10 / 30 | Total Loss: 3.4666178226470947 | KNN Loss: 2.434084177017212 | BCE Loss: 1.0325336456298828\n",
      "Epoch 467 / 500 | iteration 15 / 30 | Total Loss: 3.5073108673095703 | KNN Loss: 2.4990174770355225 | BCE Loss: 1.0082932710647583\n",
      "Epoch 467 / 500 | iteration 20 / 30 | Total Loss: 3.4338088035583496 | KNN Loss: 2.441236972808838 | BCE Loss: 0.9925719499588013\n",
      "Epoch 467 / 500 | iteration 25 / 30 | Total Loss: 3.46458101272583 | KNN Loss: 2.4262616634368896 | BCE Loss: 1.03831946849823\n",
      "Epoch 468 / 500 | iteration 0 / 30 | Total Loss: 3.453155040740967 | KNN Loss: 2.430091381072998 | BCE Loss: 1.0230636596679688\n",
      "Epoch 468 / 500 | iteration 5 / 30 | Total Loss: 3.4430794715881348 | KNN Loss: 2.444666862487793 | BCE Loss: 0.9984127283096313\n",
      "Epoch 468 / 500 | iteration 10 / 30 | Total Loss: 3.4472014904022217 | KNN Loss: 2.442366361618042 | BCE Loss: 1.0048351287841797\n",
      "Epoch 468 / 500 | iteration 15 / 30 | Total Loss: 3.454413414001465 | KNN Loss: 2.440086841583252 | BCE Loss: 1.0143264532089233\n",
      "Epoch 468 / 500 | iteration 20 / 30 | Total Loss: 3.4853785037994385 | KNN Loss: 2.435127019882202 | BCE Loss: 1.0502514839172363\n",
      "Epoch 468 / 500 | iteration 25 / 30 | Total Loss: 3.455458879470825 | KNN Loss: 2.4312891960144043 | BCE Loss: 1.024169683456421\n",
      "Epoch 469 / 500 | iteration 0 / 30 | Total Loss: 3.468383312225342 | KNN Loss: 2.449887275695801 | BCE Loss: 1.018496036529541\n",
      "Epoch 469 / 500 | iteration 5 / 30 | Total Loss: 3.4874253273010254 | KNN Loss: 2.4618616104125977 | BCE Loss: 1.0255637168884277\n",
      "Epoch 469 / 500 | iteration 10 / 30 | Total Loss: 3.469473361968994 | KNN Loss: 2.4561476707458496 | BCE Loss: 1.013325810432434\n",
      "Epoch 469 / 500 | iteration 15 / 30 | Total Loss: 3.4464375972747803 | KNN Loss: 2.4456682205200195 | BCE Loss: 1.0007693767547607\n",
      "Epoch 469 / 500 | iteration 20 / 30 | Total Loss: 3.4397263526916504 | KNN Loss: 2.436460256576538 | BCE Loss: 1.0032660961151123\n",
      "Epoch 469 / 500 | iteration 25 / 30 | Total Loss: 3.4350366592407227 | KNN Loss: 2.4513895511627197 | BCE Loss: 0.9836471676826477\n",
      "Epoch   470: reducing learning rate of group 0 to 9.5791e-07.\n",
      "Epoch 470 / 500 | iteration 0 / 30 | Total Loss: 3.4412453174591064 | KNN Loss: 2.441053867340088 | BCE Loss: 1.0001914501190186\n",
      "Epoch 470 / 500 | iteration 5 / 30 | Total Loss: 3.464668035507202 | KNN Loss: 2.463008403778076 | BCE Loss: 1.001659631729126\n",
      "Epoch 470 / 500 | iteration 10 / 30 | Total Loss: 3.4100167751312256 | KNN Loss: 2.453061580657959 | BCE Loss: 0.9569552540779114\n",
      "Epoch 470 / 500 | iteration 15 / 30 | Total Loss: 3.526871919631958 | KNN Loss: 2.49171781539917 | BCE Loss: 1.035154104232788\n",
      "Epoch 470 / 500 | iteration 20 / 30 | Total Loss: 3.502512216567993 | KNN Loss: 2.4702630043029785 | BCE Loss: 1.0322492122650146\n",
      "Epoch 470 / 500 | iteration 25 / 30 | Total Loss: 3.446094274520874 | KNN Loss: 2.4299960136413574 | BCE Loss: 1.0160982608795166\n",
      "Epoch 471 / 500 | iteration 0 / 30 | Total Loss: 3.416938304901123 | KNN Loss: 2.417325019836426 | BCE Loss: 0.9996131658554077\n",
      "Epoch 471 / 500 | iteration 5 / 30 | Total Loss: 3.4702494144439697 | KNN Loss: 2.4414494037628174 | BCE Loss: 1.0288000106811523\n",
      "Epoch 471 / 500 | iteration 10 / 30 | Total Loss: 3.481245279312134 | KNN Loss: 2.431041717529297 | BCE Loss: 1.050203561782837\n",
      "Epoch 471 / 500 | iteration 15 / 30 | Total Loss: 3.4864816665649414 | KNN Loss: 2.4796369075775146 | BCE Loss: 1.0068447589874268\n",
      "Epoch 471 / 500 | iteration 20 / 30 | Total Loss: 3.4839565753936768 | KNN Loss: 2.450202703475952 | BCE Loss: 1.0337538719177246\n",
      "Epoch 471 / 500 | iteration 25 / 30 | Total Loss: 3.4876046180725098 | KNN Loss: 2.507565498352051 | BCE Loss: 0.980039119720459\n",
      "Epoch 472 / 500 | iteration 0 / 30 | Total Loss: 3.497452735900879 | KNN Loss: 2.4702649116516113 | BCE Loss: 1.0271879434585571\n",
      "Epoch 472 / 500 | iteration 5 / 30 | Total Loss: 3.4408442974090576 | KNN Loss: 2.4263460636138916 | BCE Loss: 1.014498233795166\n",
      "Epoch 472 / 500 | iteration 10 / 30 | Total Loss: 3.499838352203369 | KNN Loss: 2.4517781734466553 | BCE Loss: 1.0480600595474243\n",
      "Epoch 472 / 500 | iteration 15 / 30 | Total Loss: 3.4356682300567627 | KNN Loss: 2.4264707565307617 | BCE Loss: 1.009197473526001\n",
      "Epoch 472 / 500 | iteration 20 / 30 | Total Loss: 3.4831464290618896 | KNN Loss: 2.460493564605713 | BCE Loss: 1.0226528644561768\n",
      "Epoch 472 / 500 | iteration 25 / 30 | Total Loss: 3.470513105392456 | KNN Loss: 2.424955368041992 | BCE Loss: 1.0455577373504639\n",
      "Epoch 473 / 500 | iteration 0 / 30 | Total Loss: 3.474593162536621 | KNN Loss: 2.4588794708251953 | BCE Loss: 1.0157136917114258\n",
      "Epoch 473 / 500 | iteration 5 / 30 | Total Loss: 3.469783306121826 | KNN Loss: 2.445418119430542 | BCE Loss: 1.0243651866912842\n",
      "Epoch 473 / 500 | iteration 10 / 30 | Total Loss: 3.4339685440063477 | KNN Loss: 2.4539778232574463 | BCE Loss: 0.9799907207489014\n",
      "Epoch 473 / 500 | iteration 15 / 30 | Total Loss: 3.483921527862549 | KNN Loss: 2.467648506164551 | BCE Loss: 1.016273021697998\n",
      "Epoch 473 / 500 | iteration 20 / 30 | Total Loss: 3.4822373390197754 | KNN Loss: 2.4562008380889893 | BCE Loss: 1.0260365009307861\n",
      "Epoch 473 / 500 | iteration 25 / 30 | Total Loss: 3.436373233795166 | KNN Loss: 2.444465160369873 | BCE Loss: 0.991908073425293\n",
      "Epoch 474 / 500 | iteration 0 / 30 | Total Loss: 3.44854998588562 | KNN Loss: 2.440181255340576 | BCE Loss: 1.008368730545044\n",
      "Epoch 474 / 500 | iteration 5 / 30 | Total Loss: 3.458617687225342 | KNN Loss: 2.436422109603882 | BCE Loss: 1.02219557762146\n",
      "Epoch 474 / 500 | iteration 10 / 30 | Total Loss: 3.505141019821167 | KNN Loss: 2.484365463256836 | BCE Loss: 1.020775556564331\n",
      "Epoch 474 / 500 | iteration 15 / 30 | Total Loss: 3.5184731483459473 | KNN Loss: 2.4882161617279053 | BCE Loss: 1.0302571058273315\n",
      "Epoch 474 / 500 | iteration 20 / 30 | Total Loss: 3.4512457847595215 | KNN Loss: 2.4414806365966797 | BCE Loss: 1.0097651481628418\n",
      "Epoch 474 / 500 | iteration 25 / 30 | Total Loss: 3.4499638080596924 | KNN Loss: 2.4356839656829834 | BCE Loss: 1.014279842376709\n",
      "Epoch 475 / 500 | iteration 0 / 30 | Total Loss: 3.4646997451782227 | KNN Loss: 2.4490103721618652 | BCE Loss: 1.0156892538070679\n",
      "Epoch 475 / 500 | iteration 5 / 30 | Total Loss: 3.4456372261047363 | KNN Loss: 2.4188170433044434 | BCE Loss: 1.0268203020095825\n",
      "Epoch 475 / 500 | iteration 10 / 30 | Total Loss: 3.5330114364624023 | KNN Loss: 2.4848716259002686 | BCE Loss: 1.0481398105621338\n",
      "Epoch 475 / 500 | iteration 15 / 30 | Total Loss: 3.4504828453063965 | KNN Loss: 2.4186155796051025 | BCE Loss: 1.031867265701294\n",
      "Epoch 475 / 500 | iteration 20 / 30 | Total Loss: 3.505950450897217 | KNN Loss: 2.4776551723480225 | BCE Loss: 1.0282952785491943\n",
      "Epoch 475 / 500 | iteration 25 / 30 | Total Loss: 3.4260010719299316 | KNN Loss: 2.4222309589385986 | BCE Loss: 1.0037699937820435\n",
      "Epoch 476 / 500 | iteration 0 / 30 | Total Loss: 3.465318202972412 | KNN Loss: 2.443952798843384 | BCE Loss: 1.0213655233383179\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 476 / 500 | iteration 5 / 30 | Total Loss: 3.435896158218384 | KNN Loss: 2.4375128746032715 | BCE Loss: 0.9983832240104675\n",
      "Epoch 476 / 500 | iteration 10 / 30 | Total Loss: 3.4385604858398438 | KNN Loss: 2.4401745796203613 | BCE Loss: 0.9983858466148376\n",
      "Epoch 476 / 500 | iteration 15 / 30 | Total Loss: 3.512115240097046 | KNN Loss: 2.4807064533233643 | BCE Loss: 1.0314087867736816\n",
      "Epoch 476 / 500 | iteration 20 / 30 | Total Loss: 3.46596097946167 | KNN Loss: 2.4419243335723877 | BCE Loss: 1.0240366458892822\n",
      "Epoch 476 / 500 | iteration 25 / 30 | Total Loss: 3.487907886505127 | KNN Loss: 2.4812941551208496 | BCE Loss: 1.0066137313842773\n",
      "Epoch 477 / 500 | iteration 0 / 30 | Total Loss: 3.494438648223877 | KNN Loss: 2.4712114334106445 | BCE Loss: 1.0232272148132324\n",
      "Epoch 477 / 500 | iteration 5 / 30 | Total Loss: 3.5344932079315186 | KNN Loss: 2.517587423324585 | BCE Loss: 1.0169057846069336\n",
      "Epoch 477 / 500 | iteration 10 / 30 | Total Loss: 3.449920654296875 | KNN Loss: 2.443779468536377 | BCE Loss: 1.0061410665512085\n",
      "Epoch 477 / 500 | iteration 15 / 30 | Total Loss: 3.4377553462982178 | KNN Loss: 2.4204769134521484 | BCE Loss: 1.0172784328460693\n",
      "Epoch 477 / 500 | iteration 20 / 30 | Total Loss: 3.4694578647613525 | KNN Loss: 2.463479995727539 | BCE Loss: 1.0059778690338135\n",
      "Epoch 477 / 500 | iteration 25 / 30 | Total Loss: 3.454000234603882 | KNN Loss: 2.4496872425079346 | BCE Loss: 1.0043129920959473\n",
      "Epoch 478 / 500 | iteration 0 / 30 | Total Loss: 3.475677490234375 | KNN Loss: 2.4580278396606445 | BCE Loss: 1.01764976978302\n",
      "Epoch 478 / 500 | iteration 5 / 30 | Total Loss: 3.4763174057006836 | KNN Loss: 2.4664196968078613 | BCE Loss: 1.0098977088928223\n",
      "Epoch 478 / 500 | iteration 10 / 30 | Total Loss: 3.4944705963134766 | KNN Loss: 2.4476687908172607 | BCE Loss: 1.0468018054962158\n",
      "Epoch 478 / 500 | iteration 15 / 30 | Total Loss: 3.4833757877349854 | KNN Loss: 2.4731154441833496 | BCE Loss: 1.0102603435516357\n",
      "Epoch 478 / 500 | iteration 20 / 30 | Total Loss: 3.460082530975342 | KNN Loss: 2.4444894790649414 | BCE Loss: 1.0155929327011108\n",
      "Epoch 478 / 500 | iteration 25 / 30 | Total Loss: 3.422764301300049 | KNN Loss: 2.4287819862365723 | BCE Loss: 0.9939823150634766\n",
      "Epoch 479 / 500 | iteration 0 / 30 | Total Loss: 3.4712705612182617 | KNN Loss: 2.441199541091919 | BCE Loss: 1.0300709009170532\n",
      "Epoch 479 / 500 | iteration 5 / 30 | Total Loss: 3.470442295074463 | KNN Loss: 2.4556002616882324 | BCE Loss: 1.01484215259552\n",
      "Epoch 479 / 500 | iteration 10 / 30 | Total Loss: 3.479310989379883 | KNN Loss: 2.451343059539795 | BCE Loss: 1.0279680490493774\n",
      "Epoch 479 / 500 | iteration 15 / 30 | Total Loss: 3.4646005630493164 | KNN Loss: 2.4474682807922363 | BCE Loss: 1.01713228225708\n",
      "Epoch 479 / 500 | iteration 20 / 30 | Total Loss: 3.4542930126190186 | KNN Loss: 2.449038505554199 | BCE Loss: 1.0052545070648193\n",
      "Epoch 479 / 500 | iteration 25 / 30 | Total Loss: 3.4792919158935547 | KNN Loss: 2.457761287689209 | BCE Loss: 1.0215306282043457\n",
      "Epoch 480 / 500 | iteration 0 / 30 | Total Loss: 3.497011184692383 | KNN Loss: 2.466607093811035 | BCE Loss: 1.030403971672058\n",
      "Epoch 480 / 500 | iteration 5 / 30 | Total Loss: 3.4895994663238525 | KNN Loss: 2.456629514694214 | BCE Loss: 1.0329699516296387\n",
      "Epoch 480 / 500 | iteration 10 / 30 | Total Loss: 3.452833890914917 | KNN Loss: 2.4359874725341797 | BCE Loss: 1.0168464183807373\n",
      "Epoch 480 / 500 | iteration 15 / 30 | Total Loss: 3.4497628211975098 | KNN Loss: 2.447727918624878 | BCE Loss: 1.0020349025726318\n",
      "Epoch 480 / 500 | iteration 20 / 30 | Total Loss: 3.468203067779541 | KNN Loss: 2.4495997428894043 | BCE Loss: 1.0186034440994263\n",
      "Epoch 480 / 500 | iteration 25 / 30 | Total Loss: 3.440692186355591 | KNN Loss: 2.4317188262939453 | BCE Loss: 1.0089733600616455\n",
      "Epoch   481: reducing learning rate of group 0 to 6.7053e-07.\n",
      "Epoch 481 / 500 | iteration 0 / 30 | Total Loss: 3.455329656600952 | KNN Loss: 2.452270984649658 | BCE Loss: 1.003058671951294\n",
      "Epoch 481 / 500 | iteration 5 / 30 | Total Loss: 3.4789834022521973 | KNN Loss: 2.454721689224243 | BCE Loss: 1.0242618322372437\n",
      "Epoch 481 / 500 | iteration 10 / 30 | Total Loss: 3.5002670288085938 | KNN Loss: 2.472294807434082 | BCE Loss: 1.0279722213745117\n",
      "Epoch 481 / 500 | iteration 15 / 30 | Total Loss: 3.457334518432617 | KNN Loss: 2.445173978805542 | BCE Loss: 1.0121604204177856\n",
      "Epoch 481 / 500 | iteration 20 / 30 | Total Loss: 3.4642136096954346 | KNN Loss: 2.449100971221924 | BCE Loss: 1.0151126384735107\n",
      "Epoch 481 / 500 | iteration 25 / 30 | Total Loss: 3.47666072845459 | KNN Loss: 2.4825680255889893 | BCE Loss: 0.9940927028656006\n",
      "Epoch 482 / 500 | iteration 0 / 30 | Total Loss: 3.457273483276367 | KNN Loss: 2.4589502811431885 | BCE Loss: 0.9983230829238892\n",
      "Epoch 482 / 500 | iteration 5 / 30 | Total Loss: 3.457098960876465 | KNN Loss: 2.4686481952667236 | BCE Loss: 0.9884507656097412\n",
      "Epoch 482 / 500 | iteration 10 / 30 | Total Loss: 3.4573991298675537 | KNN Loss: 2.4557557106018066 | BCE Loss: 1.001643419265747\n",
      "Epoch 482 / 500 | iteration 15 / 30 | Total Loss: 3.436656951904297 | KNN Loss: 2.431183338165283 | BCE Loss: 1.0054736137390137\n",
      "Epoch 482 / 500 | iteration 20 / 30 | Total Loss: 3.470262289047241 | KNN Loss: 2.467984914779663 | BCE Loss: 1.0022773742675781\n",
      "Epoch 482 / 500 | iteration 25 / 30 | Total Loss: 3.4985594749450684 | KNN Loss: 2.452796220779419 | BCE Loss: 1.0457632541656494\n",
      "Epoch 483 / 500 | iteration 0 / 30 | Total Loss: 3.4509615898132324 | KNN Loss: 2.4564337730407715 | BCE Loss: 0.9945279359817505\n",
      "Epoch 483 / 500 | iteration 5 / 30 | Total Loss: 3.456592082977295 | KNN Loss: 2.451687812805176 | BCE Loss: 1.0049042701721191\n",
      "Epoch 483 / 500 | iteration 10 / 30 | Total Loss: 3.4695284366607666 | KNN Loss: 2.4657857418060303 | BCE Loss: 1.0037426948547363\n",
      "Epoch 483 / 500 | iteration 15 / 30 | Total Loss: 3.4659316539764404 | KNN Loss: 2.4310150146484375 | BCE Loss: 1.034916639328003\n",
      "Epoch 483 / 500 | iteration 20 / 30 | Total Loss: 3.434715986251831 | KNN Loss: 2.432913064956665 | BCE Loss: 1.001802921295166\n",
      "Epoch 483 / 500 | iteration 25 / 30 | Total Loss: 3.439990520477295 | KNN Loss: 2.4433956146240234 | BCE Loss: 0.996595025062561\n",
      "Epoch 484 / 500 | iteration 0 / 30 | Total Loss: 3.4754245281219482 | KNN Loss: 2.465226888656616 | BCE Loss: 1.010197639465332\n",
      "Epoch 484 / 500 | iteration 5 / 30 | Total Loss: 3.4704318046569824 | KNN Loss: 2.464207649230957 | BCE Loss: 1.006224274635315\n",
      "Epoch 484 / 500 | iteration 10 / 30 | Total Loss: 3.475893974304199 | KNN Loss: 2.482351779937744 | BCE Loss: 0.9935420751571655\n",
      "Epoch 484 / 500 | iteration 15 / 30 | Total Loss: 3.4471120834350586 | KNN Loss: 2.42326021194458 | BCE Loss: 1.0238518714904785\n",
      "Epoch 484 / 500 | iteration 20 / 30 | Total Loss: 3.4328808784484863 | KNN Loss: 2.4324185848236084 | BCE Loss: 1.0004621744155884\n",
      "Epoch 484 / 500 | iteration 25 / 30 | Total Loss: 3.4232070446014404 | KNN Loss: 2.4320571422576904 | BCE Loss: 0.99114990234375\n",
      "Epoch 485 / 500 | iteration 0 / 30 | Total Loss: 3.4507644176483154 | KNN Loss: 2.423424482345581 | BCE Loss: 1.0273399353027344\n",
      "Epoch 485 / 500 | iteration 5 / 30 | Total Loss: 3.5172557830810547 | KNN Loss: 2.4875376224517822 | BCE Loss: 1.0297181606292725\n",
      "Epoch 485 / 500 | iteration 10 / 30 | Total Loss: 3.4440157413482666 | KNN Loss: 2.4296205043792725 | BCE Loss: 1.0143952369689941\n",
      "Epoch 485 / 500 | iteration 15 / 30 | Total Loss: 3.46461820602417 | KNN Loss: 2.4569358825683594 | BCE Loss: 1.0076824426651\n",
      "Epoch 485 / 500 | iteration 20 / 30 | Total Loss: 3.458641767501831 | KNN Loss: 2.439462900161743 | BCE Loss: 1.019178867340088\n",
      "Epoch 485 / 500 | iteration 25 / 30 | Total Loss: 3.4819536209106445 | KNN Loss: 2.4817733764648438 | BCE Loss: 1.0001803636550903\n",
      "Epoch 486 / 500 | iteration 0 / 30 | Total Loss: 3.4837937355041504 | KNN Loss: 2.475985288619995 | BCE Loss: 1.0078084468841553\n",
      "Epoch 486 / 500 | iteration 5 / 30 | Total Loss: 3.4585182666778564 | KNN Loss: 2.445472002029419 | BCE Loss: 1.0130462646484375\n",
      "Epoch 486 / 500 | iteration 10 / 30 | Total Loss: 3.4883384704589844 | KNN Loss: 2.4655954837799072 | BCE Loss: 1.0227429866790771\n",
      "Epoch 486 / 500 | iteration 15 / 30 | Total Loss: 3.461460590362549 | KNN Loss: 2.4488046169281006 | BCE Loss: 1.0126559734344482\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 486 / 500 | iteration 20 / 30 | Total Loss: 3.497659206390381 | KNN Loss: 2.462867498397827 | BCE Loss: 1.0347917079925537\n",
      "Epoch 486 / 500 | iteration 25 / 30 | Total Loss: 3.446634531021118 | KNN Loss: 2.426318883895874 | BCE Loss: 1.0203156471252441\n",
      "Epoch 487 / 500 | iteration 0 / 30 | Total Loss: 3.4695658683776855 | KNN Loss: 2.4240596294403076 | BCE Loss: 1.0455061197280884\n",
      "Epoch 487 / 500 | iteration 5 / 30 | Total Loss: 3.4758872985839844 | KNN Loss: 2.467365026473999 | BCE Loss: 1.0085222721099854\n",
      "Epoch 487 / 500 | iteration 10 / 30 | Total Loss: 3.442422866821289 | KNN Loss: 2.435115098953247 | BCE Loss: 1.007307767868042\n",
      "Epoch 487 / 500 | iteration 15 / 30 | Total Loss: 3.4759304523468018 | KNN Loss: 2.4420318603515625 | BCE Loss: 1.0338985919952393\n",
      "Epoch 487 / 500 | iteration 20 / 30 | Total Loss: 3.4397456645965576 | KNN Loss: 2.422353982925415 | BCE Loss: 1.0173916816711426\n",
      "Epoch 487 / 500 | iteration 25 / 30 | Total Loss: 3.460203170776367 | KNN Loss: 2.4624454975128174 | BCE Loss: 0.9977577924728394\n",
      "Epoch 488 / 500 | iteration 0 / 30 | Total Loss: 3.4771440029144287 | KNN Loss: 2.4551022052764893 | BCE Loss: 1.0220417976379395\n",
      "Epoch 488 / 500 | iteration 5 / 30 | Total Loss: 3.444711446762085 | KNN Loss: 2.4283862113952637 | BCE Loss: 1.0163252353668213\n",
      "Epoch 488 / 500 | iteration 10 / 30 | Total Loss: 3.4586894512176514 | KNN Loss: 2.4751486778259277 | BCE Loss: 0.9835408329963684\n",
      "Epoch 488 / 500 | iteration 15 / 30 | Total Loss: 3.503965377807617 | KNN Loss: 2.469858407974243 | BCE Loss: 1.034106969833374\n",
      "Epoch 488 / 500 | iteration 20 / 30 | Total Loss: 3.449336051940918 | KNN Loss: 2.442258834838867 | BCE Loss: 1.0070772171020508\n",
      "Epoch 488 / 500 | iteration 25 / 30 | Total Loss: 3.471139430999756 | KNN Loss: 2.4525046348571777 | BCE Loss: 1.0186346769332886\n",
      "Epoch 489 / 500 | iteration 0 / 30 | Total Loss: 3.4600698947906494 | KNN Loss: 2.4237425327301025 | BCE Loss: 1.0363273620605469\n",
      "Epoch 489 / 500 | iteration 5 / 30 | Total Loss: 3.4643850326538086 | KNN Loss: 2.428699016571045 | BCE Loss: 1.0356860160827637\n",
      "Epoch 489 / 500 | iteration 10 / 30 | Total Loss: 3.4572486877441406 | KNN Loss: 2.451610803604126 | BCE Loss: 1.005637764930725\n",
      "Epoch 489 / 500 | iteration 15 / 30 | Total Loss: 3.4409289360046387 | KNN Loss: 2.423447608947754 | BCE Loss: 1.0174813270568848\n",
      "Epoch 489 / 500 | iteration 20 / 30 | Total Loss: 3.4815824031829834 | KNN Loss: 2.4645938873291016 | BCE Loss: 1.0169885158538818\n",
      "Epoch 489 / 500 | iteration 25 / 30 | Total Loss: 3.4593660831451416 | KNN Loss: 2.4496984481811523 | BCE Loss: 1.0096676349639893\n",
      "Epoch 490 / 500 | iteration 0 / 30 | Total Loss: 3.4882168769836426 | KNN Loss: 2.4656617641448975 | BCE Loss: 1.0225551128387451\n",
      "Epoch 490 / 500 | iteration 5 / 30 | Total Loss: 3.512606143951416 | KNN Loss: 2.481172561645508 | BCE Loss: 1.0314337015151978\n",
      "Epoch 490 / 500 | iteration 10 / 30 | Total Loss: 3.4594550132751465 | KNN Loss: 2.4467904567718506 | BCE Loss: 1.012664556503296\n",
      "Epoch 490 / 500 | iteration 15 / 30 | Total Loss: 3.457317352294922 | KNN Loss: 2.458317995071411 | BCE Loss: 0.9989994168281555\n",
      "Epoch 490 / 500 | iteration 20 / 30 | Total Loss: 3.5061421394348145 | KNN Loss: 2.4562299251556396 | BCE Loss: 1.0499122142791748\n",
      "Epoch 490 / 500 | iteration 25 / 30 | Total Loss: 3.4529073238372803 | KNN Loss: 2.4381539821624756 | BCE Loss: 1.0147533416748047\n",
      "Epoch 491 / 500 | iteration 0 / 30 | Total Loss: 3.505256175994873 | KNN Loss: 2.5009329319000244 | BCE Loss: 1.0043232440948486\n",
      "Epoch 491 / 500 | iteration 5 / 30 | Total Loss: 3.510180950164795 | KNN Loss: 2.475065231323242 | BCE Loss: 1.0351157188415527\n",
      "Epoch 491 / 500 | iteration 10 / 30 | Total Loss: 3.4168474674224854 | KNN Loss: 2.4248361587524414 | BCE Loss: 0.9920113682746887\n",
      "Epoch 491 / 500 | iteration 15 / 30 | Total Loss: 3.473193645477295 | KNN Loss: 2.4484264850616455 | BCE Loss: 1.0247671604156494\n",
      "Epoch 491 / 500 | iteration 20 / 30 | Total Loss: 3.4663219451904297 | KNN Loss: 2.4614670276641846 | BCE Loss: 1.0048549175262451\n",
      "Epoch 491 / 500 | iteration 25 / 30 | Total Loss: 3.4780988693237305 | KNN Loss: 2.450396776199341 | BCE Loss: 1.0277020931243896\n",
      "Epoch   492: reducing learning rate of group 0 to 4.6937e-07.\n",
      "Epoch 492 / 500 | iteration 0 / 30 | Total Loss: 3.517280101776123 | KNN Loss: 2.4950716495513916 | BCE Loss: 1.022208333015442\n",
      "Epoch 492 / 500 | iteration 5 / 30 | Total Loss: 3.446401596069336 | KNN Loss: 2.4396214485168457 | BCE Loss: 1.0067800283432007\n",
      "Epoch 492 / 500 | iteration 10 / 30 | Total Loss: 3.4440174102783203 | KNN Loss: 2.4292571544647217 | BCE Loss: 1.014760136604309\n",
      "Epoch 492 / 500 | iteration 15 / 30 | Total Loss: 3.464385986328125 | KNN Loss: 2.4497756958007812 | BCE Loss: 1.0146102905273438\n",
      "Epoch 492 / 500 | iteration 20 / 30 | Total Loss: 3.439854860305786 | KNN Loss: 2.429161310195923 | BCE Loss: 1.0106935501098633\n",
      "Epoch 492 / 500 | iteration 25 / 30 | Total Loss: 3.4522597789764404 | KNN Loss: 2.4458210468292236 | BCE Loss: 1.0064387321472168\n",
      "Epoch 493 / 500 | iteration 0 / 30 | Total Loss: 3.495011806488037 | KNN Loss: 2.4553091526031494 | BCE Loss: 1.0397027730941772\n",
      "Epoch 493 / 500 | iteration 5 / 30 | Total Loss: 3.4199509620666504 | KNN Loss: 2.4135425090789795 | BCE Loss: 1.0064083337783813\n",
      "Epoch 493 / 500 | iteration 10 / 30 | Total Loss: 3.5022692680358887 | KNN Loss: 2.4701714515686035 | BCE Loss: 1.0320978164672852\n",
      "Epoch 493 / 500 | iteration 15 / 30 | Total Loss: 3.454618453979492 | KNN Loss: 2.4359986782073975 | BCE Loss: 1.0186198949813843\n",
      "Epoch 493 / 500 | iteration 20 / 30 | Total Loss: 3.415764331817627 | KNN Loss: 2.4163172245025635 | BCE Loss: 0.9994471073150635\n",
      "Epoch 493 / 500 | iteration 25 / 30 | Total Loss: 3.4396872520446777 | KNN Loss: 2.4147207736968994 | BCE Loss: 1.0249664783477783\n",
      "Epoch 494 / 500 | iteration 0 / 30 | Total Loss: 3.4968323707580566 | KNN Loss: 2.451087474822998 | BCE Loss: 1.0457448959350586\n",
      "Epoch 494 / 500 | iteration 5 / 30 | Total Loss: 3.4733405113220215 | KNN Loss: 2.4230244159698486 | BCE Loss: 1.0503160953521729\n",
      "Epoch 494 / 500 | iteration 10 / 30 | Total Loss: 3.496962308883667 | KNN Loss: 2.4754059314727783 | BCE Loss: 1.0215563774108887\n",
      "Epoch 494 / 500 | iteration 15 / 30 | Total Loss: 3.437689781188965 | KNN Loss: 2.4308950901031494 | BCE Loss: 1.0067945718765259\n",
      "Epoch 494 / 500 | iteration 20 / 30 | Total Loss: 3.4646902084350586 | KNN Loss: 2.4719457626342773 | BCE Loss: 0.992744505405426\n",
      "Epoch 494 / 500 | iteration 25 / 30 | Total Loss: 3.507081985473633 | KNN Loss: 2.4750664234161377 | BCE Loss: 1.0320155620574951\n",
      "Epoch 495 / 500 | iteration 0 / 30 | Total Loss: 3.4732205867767334 | KNN Loss: 2.4633395671844482 | BCE Loss: 1.0098810195922852\n",
      "Epoch 495 / 500 | iteration 5 / 30 | Total Loss: 3.47564697265625 | KNN Loss: 2.457001209259033 | BCE Loss: 1.0186457633972168\n",
      "Epoch 495 / 500 | iteration 10 / 30 | Total Loss: 3.4123101234436035 | KNN Loss: 2.4277286529541016 | BCE Loss: 0.984581470489502\n",
      "Epoch 495 / 500 | iteration 15 / 30 | Total Loss: 3.4843225479125977 | KNN Loss: 2.4537529945373535 | BCE Loss: 1.0305694341659546\n",
      "Epoch 495 / 500 | iteration 20 / 30 | Total Loss: 3.5151360034942627 | KNN Loss: 2.5004653930664062 | BCE Loss: 1.0146706104278564\n",
      "Epoch 495 / 500 | iteration 25 / 30 | Total Loss: 3.4455270767211914 | KNN Loss: 2.4496893882751465 | BCE Loss: 0.9958376288414001\n",
      "Epoch 496 / 500 | iteration 0 / 30 | Total Loss: 3.4553442001342773 | KNN Loss: 2.4330475330352783 | BCE Loss: 1.0222965478897095\n",
      "Epoch 496 / 500 | iteration 5 / 30 | Total Loss: 3.493818759918213 | KNN Loss: 2.4812171459198 | BCE Loss: 1.012601613998413\n",
      "Epoch 496 / 500 | iteration 10 / 30 | Total Loss: 3.4296975135803223 | KNN Loss: 2.399451494216919 | BCE Loss: 1.0302460193634033\n",
      "Epoch 496 / 500 | iteration 15 / 30 | Total Loss: 3.4805619716644287 | KNN Loss: 2.454461097717285 | BCE Loss: 1.0261008739471436\n",
      "Epoch 496 / 500 | iteration 20 / 30 | Total Loss: 3.429508924484253 | KNN Loss: 2.437981128692627 | BCE Loss: 0.991527795791626\n",
      "Epoch 496 / 500 | iteration 25 / 30 | Total Loss: 3.469573497772217 | KNN Loss: 2.4696879386901855 | BCE Loss: 0.9998854994773865\n",
      "Epoch 497 / 500 | iteration 0 / 30 | Total Loss: 3.4686760902404785 | KNN Loss: 2.438140869140625 | BCE Loss: 1.0305352210998535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 497 / 500 | iteration 5 / 30 | Total Loss: 3.4742002487182617 | KNN Loss: 2.4685566425323486 | BCE Loss: 1.0056437253952026\n",
      "Epoch 497 / 500 | iteration 10 / 30 | Total Loss: 3.459146738052368 | KNN Loss: 2.4547905921936035 | BCE Loss: 1.0043561458587646\n",
      "Epoch 497 / 500 | iteration 15 / 30 | Total Loss: 3.43691349029541 | KNN Loss: 2.4403629302978516 | BCE Loss: 0.996550440788269\n",
      "Epoch 497 / 500 | iteration 20 / 30 | Total Loss: 3.407719373703003 | KNN Loss: 2.4244635105133057 | BCE Loss: 0.9832558631896973\n",
      "Epoch 497 / 500 | iteration 25 / 30 | Total Loss: 3.4951319694519043 | KNN Loss: 2.4849376678466797 | BCE Loss: 1.0101943016052246\n",
      "Epoch 498 / 500 | iteration 0 / 30 | Total Loss: 3.4707562923431396 | KNN Loss: 2.453183650970459 | BCE Loss: 1.0175726413726807\n",
      "Epoch 498 / 500 | iteration 5 / 30 | Total Loss: 3.4635910987854004 | KNN Loss: 2.4739856719970703 | BCE Loss: 0.9896055459976196\n",
      "Epoch 498 / 500 | iteration 10 / 30 | Total Loss: 3.4412012100219727 | KNN Loss: 2.442117929458618 | BCE Loss: 0.9990831613540649\n",
      "Epoch 498 / 500 | iteration 15 / 30 | Total Loss: 3.4706602096557617 | KNN Loss: 2.432156562805176 | BCE Loss: 1.0385037660598755\n",
      "Epoch 498 / 500 | iteration 20 / 30 | Total Loss: 3.5114810466766357 | KNN Loss: 2.4614601135253906 | BCE Loss: 1.0500209331512451\n",
      "Epoch 498 / 500 | iteration 25 / 30 | Total Loss: 3.4643800258636475 | KNN Loss: 2.458859920501709 | BCE Loss: 1.0055201053619385\n",
      "Epoch 499 / 500 | iteration 0 / 30 | Total Loss: 3.461097240447998 | KNN Loss: 2.4470934867858887 | BCE Loss: 1.0140037536621094\n",
      "Epoch 499 / 500 | iteration 5 / 30 | Total Loss: 3.423396110534668 | KNN Loss: 2.427722454071045 | BCE Loss: 0.9956737756729126\n",
      "Epoch 499 / 500 | iteration 10 / 30 | Total Loss: 3.4796109199523926 | KNN Loss: 2.4492294788360596 | BCE Loss: 1.030381441116333\n",
      "Epoch 499 / 500 | iteration 15 / 30 | Total Loss: 3.4992871284484863 | KNN Loss: 2.4893248081207275 | BCE Loss: 1.0099623203277588\n",
      "Epoch 499 / 500 | iteration 20 / 30 | Total Loss: 3.4405810832977295 | KNN Loss: 2.4346230030059814 | BCE Loss: 1.005958080291748\n",
      "Epoch 499 / 500 | iteration 25 / 30 | Total Loss: 3.4503426551818848 | KNN Loss: 2.4416284561157227 | BCE Loss: 1.0087143182754517\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "data_iter = torch.utils.data.DataLoader(dataset,\n",
    "                                     batch_size=batch_size,\n",
    "                                     shuffle=True,\n",
    "                                     num_workers=1,\n",
    "                                     pin_memory=True)\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', verbose=True, factor=0.7, threshold=1e-4)\n",
    "knn_crt = KNNLoss(k=k).to(device)\n",
    "losses = []\n",
    "alpha = 10/170\n",
    "gamma = 2\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for iteration, (batch, target) in enumerate(data_iter):\n",
    "        batch = batch.to(device)\n",
    "        target = target.to(device)\n",
    "        outputs, iterm = model(batch, return_intermidiate=True)\n",
    "        mse_loss = F.binary_cross_entropy_with_logits(outputs, target, reduction='none')\n",
    "        mask = torch.ones_like(mse_loss)\n",
    "        mask[target == 0] = alpha ** gamma\n",
    "        mask[target == 1] = (1 - alpha) ** gamma\n",
    "        mse_loss = (mse_loss * mask).sum(dim=-1).mean()\n",
    "        try:\n",
    "            knn_loss = knn_crt(iterm)\n",
    "            if torch.isinf(knn_loss):\n",
    "                knn_loss = 0\n",
    "        except ValueError:\n",
    "            knn_loss = torch.tensor(0)\n",
    "        loss = mse_loss + knn_loss\n",
    "        total_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if iteration % log_every == 0:\n",
    "            print(f\"Epoch {epoch} / {epochs} | iteration {iteration} / {len(data_iter)} | Total Loss: {loss.item()} | KNN Loss: {knn_loss.item()} | BCE Loss: {mse_loss.item()}\")\n",
    "    \n",
    "    scheduler.step(total_loss / (iteration + 1))\n",
    "    losses.append(total_loss / (iteration + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.8485,  4.5208,  3.0494,  2.1339,  3.6342,  0.9707,  1.8464,  2.4946,\n",
      "          2.8739,  1.1835,  2.6940,  2.8041,  0.4477,  2.2759,  0.6752,  1.7183,\n",
      "          1.1334,  3.7789,  2.5588,  0.8831,  1.1705,  1.5609,  2.6076,  2.4545,\n",
      "          3.0419,  0.7232,  2.4458,  0.8959,  1.3016,  0.3329, -0.1169,  0.7758,\n",
      "          0.1671, -0.0449,  0.5050,  0.4908,  0.8311,  1.7182,  1.0354,  0.3517,\n",
      "          0.7787, -0.2847,  0.0153,  1.3845,  2.5655,  0.6838, -0.5005, -0.4219,\n",
      "          1.6837,  2.9835,  2.2762, -0.1934,  0.7293,  0.9027, -0.5613,  0.6813,\n",
      "          1.0101,  0.4168,  1.8063,  1.5576, -0.4391,  0.3404,  0.1638,  1.2698,\n",
      "          1.8243,  0.3636, -1.4577, -0.0348,  2.2464,  2.4268,  1.2849,  0.8021,\n",
      "          0.1023,  2.4085,  1.5305,  1.6561,  0.4774,  0.2325,  0.6024,  1.7814,\n",
      "          0.1514,  0.6269,  0.3638, -0.0341,  0.1555, -0.8348, -2.1540, -0.0870,\n",
      "          0.7430, -2.2588,  0.6612, -0.3409, -0.6863, -0.9409,  0.9034,  1.6642,\n",
      "         -0.6596, -0.6228,  0.8221,  1.2827,  1.1026, -1.1023,  0.3716,  1.3034,\n",
      "         -1.0276, -1.2320, -0.2675, -0.1269, -1.0792, -1.6048, -0.6790, -3.0345,\n",
      "         -0.6235,  1.5343,  1.8057, -0.2971, -0.7629, -0.3237,  1.1447, -2.7430,\n",
      "         -0.5243, -0.5697,  0.6657, -0.2258, -0.1717, -0.9665, -1.4692,  0.9517,\n",
      "          0.1154, -0.5075, -0.2377, -0.3614, -1.0423, -0.8922, -0.0625,  0.5760,\n",
      "         -0.2011,  0.1196, -2.8598, -0.6452, -1.7339,  0.5708, -1.5193, -0.7421,\n",
      "         -0.6648, -0.4615, -1.7980, -0.9468, -2.6524, -0.9328, -1.1852,  0.0320,\n",
      "         -1.8054, -0.7607, -1.6404, -0.6975, -4.6038, -0.7055, -0.1188, -0.9928,\n",
      "         -2.2113, -1.6084, -1.3061, -1.5829, -3.0283, -2.4423, -4.2628]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor(-4.6038, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "tensor(4.5208, device='cuda:0', grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "outputs, iterm = model(dataset[67][0].unsqueeze(0).to(device), return_intermidiate=True)\n",
    "print(outputs)\n",
    "print(outputs.min())\n",
    "print(outputs.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ed6d588eaa8436fa14ca3e44fac5fd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.transform = torchvision.transforms.Compose([\n",
    "    BinaryEncodingTransform(mapping=dataset.items_to_idx),\n",
    "]\n",
    ")\n",
    "dataset.target_transform = torchvision.transforms.Compose([\n",
    "    BinaryEncodingTransform(mapping=dataset.items_to_idx),\n",
    "]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_ = [d[0].cpu() for d in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:00<00:00, 77.00it/s]\n"
     ]
    }
   ],
   "source": [
    "model = model.eval().cpu()\n",
    "projections = model.calculate_intermidiate(dataset_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d554c267c8f6452297a45880461afc91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3df049ecf86b40eaa96ba1bcc8247fcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "distances = pairwise_distances(projections)\n",
    "# distances = np.triu(distances)\n",
    "distances_f = distances.flatten()\n",
    "\n",
    "plt.matshow(distances)\n",
    "plt.colorbar()\n",
    "plt.figure()\n",
    "plt.hist(distances_f[distances_f > 0], bins=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit DBSCAN and calculate indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = DBSCAN(eps=0.2, min_samples=80).fit_predict(projections)\n",
    "# scores = []\n",
    "# best_score = float('inf')\n",
    "# clusters = None\n",
    "# range_ = list(range(5, 20))\n",
    "# for k in tqdm(range_):\n",
    "#     y = GaussianMixture(n_components=k).fit_predict(projections)\n",
    "#     cur_score = davies_bouldin_score(projections, y)\n",
    "#     scores.append(cur_score)\n",
    "    \n",
    "#     if cur_score < best_score:\n",
    "#         best_score = cur_score\n",
    "#         clusters = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4220d2ed17704a43b951f429f22d39e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "perplexity = 100\n",
    "\n",
    "p = reduce_dims_and_plot(projections[clusters != -1],\n",
    "                         y=clusters[clusters != -1],\n",
    "                         title=f'perplexity: {perplexity}',\n",
    "                         file_name=None,\n",
    "                         perplexity=perplexity,\n",
    "                         library='Multicore-TSNE',\n",
    "                         perform_PCA=False,\n",
    "                         projected=None,\n",
    "                         figure_type='2d',\n",
    "                         show_figure=True,\n",
    "                         close_figure=False,\n",
    "                         text=None)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn import tree\n",
    "# from sklearn.tree import _tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_dataset = torch.stack(dataset_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = DecisionTreeClassifier(max_depth=200, min_samples_leaf=5)\n",
    "# clf = clf.fit(tensor_dataset[clusters!=-1], clusters[clusters != -1])\n",
    "# print(clf.score(tensor_dataset[clusters!=-1], clusters[clusters != -1]))\n",
    "# print(clf.get_depth())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores = []\n",
    "# for min_samples in range(1,50, 1):\n",
    "#     clf = DecisionTreeClassifier(max_depth=200, min_samples_leaf=min_samples)\n",
    "#     clf = clf.fit(tensor_dataset[clusters!=-1], clusters[clusters != -1])\n",
    "#     scores.append(clf.score(tensor_dataset[clusters!=-1], clusters[clusters != -1]))\n",
    "    \n",
    "# plt.figure()\n",
    "# plt.plot(list(range(1,50, 1)), scores)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rules(tree, feature_names, class_names):\n",
    "    tree_ = tree.tree_\n",
    "    feature_name = [\n",
    "        feature_names[i] if i != _tree.TREE_UNDEFINED else \"undefined!\"\n",
    "        for i in tree_.feature\n",
    "    ]\n",
    "\n",
    "    paths = []\n",
    "    path = []\n",
    "    \n",
    "    def recurse(node, path, paths):\n",
    "        if tree_.feature[node] != _tree.TREE_UNDEFINED:\n",
    "            name = feature_name[node]\n",
    "            threshold = tree_.threshold[node]\n",
    "            p1, p2 = list(path), list(path)\n",
    "#             p1 += [f\"({name} <= {np.round(threshold, 3)})\"]\n",
    "            p1 += [(name, '<=', np.round(threshold, 3))]\n",
    "            recurse(tree_.children_left[node], p1, paths)\n",
    "            p2 += [(name, '>', np.round(threshold, 3))]\n",
    "            recurse(tree_.children_right[node], p2, paths)\n",
    "        else:\n",
    "            path += [(tree_.value[node], tree_.n_node_samples[node])]\n",
    "            paths += [path]\n",
    "            \n",
    "    recurse(0, path, paths)\n",
    "\n",
    "    # sort by samples count\n",
    "    samples_count = [p[-1][1] for p in paths]\n",
    "    ii = list(np.argsort(samples_count))\n",
    "    paths = [paths[i] for i in reversed(ii)]\n",
    "    \n",
    "    rules = []\n",
    "    for path in paths:\n",
    "        rule = []\n",
    "        \n",
    "        for p in path[:-1]:\n",
    "            rule += [p]\n",
    "        target = \" then \"\n",
    "        if class_names is None:\n",
    "            target += \"response: \"+str(np.round(path[-1][0][0][0],3))\n",
    "        else:\n",
    "            classes = path[-1][0][0]\n",
    "            l = np.argmax(classes)\n",
    "            target += f\"class: {class_names[l]} (proba: {np.round(100.0*classes[l]/np.sum(classes),2)}%)\"\n",
    "           \n",
    "        proba = np.round(100.0*classes[l]/np.sum(classes),2)\n",
    "        target += f\" | based on {path[-1][1]:,} samples\"\n",
    "        rule_wrapper = {'target': target, 'rule': rule, 'proba': proba}\n",
    "        rules += [rule_wrapper]\n",
    "        \n",
    "    return rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rules = get_rules(clf, dataset.items, clusters[clusters != -1])\n",
    "\n",
    "# for rule in rules:\n",
    "#     n_pos = 0\n",
    "#     for c,p,v in rule['rule']:\n",
    "#         if p == '>':\n",
    "#             n_pos += 1\n",
    "#     rule['pos'] = n_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure()\n",
    "# probs = [r['proba'] for r in rules]\n",
    "# plt.hist(probs, bins = 100)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rules = sorted(rules, key=lambda x:x['pos'])\n",
    "# rules = [r for r in rules if r['proba'] > 50]\n",
    "# print(len(rules))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(17):\n",
    "#     r_i = rules[i]\n",
    "#     print(f\"------------- rule {i} length {len(r_i)} -------------\")\n",
    "#     print(r_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Soft-Decision-Tree given the self-labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_dataset = list(zip(tensor_dataset[clusters!=-1], clusters[clusters != -1]))\n",
    "batch_size = 512\n",
    "tree_loader = torch.utils.data.DataLoader(tree_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define how we prune the weights of a node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_node(node_weights, factor=1):\n",
    "    w = node_weights.cpu().detach().numpy()\n",
    "    mean_ = np.mean(w)\n",
    "    std_ = np.std(w)\n",
    "    node_weights[((mean_ - std_ * factor) < node_weights) & (node_weights < (mean_ + std_ * factor))] = 0\n",
    "    return node_weights\n",
    "\n",
    "def prune_node_keep(node_weights, keep=4):\n",
    "    w = node_weights.cpu().detach().numpy()\n",
    "    throw_idx = np.argsort(abs(w))[:-keep]\n",
    "    node_weights[throw_idx] = 0\n",
    "    return node_weights\n",
    "\n",
    "def prune_tree(tree_, factor):\n",
    "    new_weights = tree_.inner_nodes.weight.clone()\n",
    "    for i in range(new_weights.shape[0]):\n",
    "        res = prune_node_keep(new_weights[i, :], factor)\n",
    "        new_weights[i, :] = res\n",
    "\n",
    "    with torch.no_grad():\n",
    "        tree_.inner_nodes.weight.copy_(new_weights)\n",
    "        \n",
    "def sparseness(x):\n",
    "    s = []\n",
    "    for i in range(x.shape[0]):\n",
    "        x_ = x[i, :]\n",
    "        sp = (len(x_) - torch.norm(x_, 0).item()) / len(x_)\n",
    "        s.append(sp)\n",
    "    return np.mean(s)\n",
    "\n",
    "def compute_regularization_by_level(tree):\n",
    "    total_reg = 0\n",
    "    for i in range(tree.inner_nodes.weight.shape[0]):\n",
    "        cur_level = np.floor(np.log2(i+1))\n",
    "        node_reg = torch.norm(tree.inner_nodes.weight[i].view(-1), 2)\n",
    "        total_reg += 2**(-cur_level) * node_reg\n",
    "    return total_reg\n",
    "\n",
    "def show_sparseness(tree):\n",
    "    avg_sp = sparseness(tree.inner_nodes.weight)\n",
    "    print(f\"Average sparseness: {avg_sp}\")\n",
    "    layer = 0\n",
    "    sps = []\n",
    "    for i in range(tree.inner_nodes.weight.shape[0]):\n",
    "        cur_layer = int(np.floor(np.log2(i+1)))\n",
    "        if cur_layer != layer:\n",
    "            print(f\"layer {layer}: {np.mean(sps)}\")\n",
    "            sps = []\n",
    "            layer = cur_layer\n",
    "\n",
    "        x_ = tree.inner_nodes.weight[i, :]\n",
    "        sp = (len(x_) - torch.norm(x_, 0).item()) / len(x_)\n",
    "        sps.append(sp)\n",
    "        \n",
    "    return avg_sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_epoch(model, loader, device, log_interval, losses, accs, epoch, iteration):\n",
    "    model = model.train()\n",
    "    for batch_idx, (data, target) in enumerate(loader):\n",
    "        iteration += 1\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        output, penalty = tree.forward(data)\n",
    "\n",
    "        # Loss\n",
    "        loss_tree = criterion(output, target.view(-1))\n",
    "\n",
    "        # Penalty\n",
    "        loss_tree += penalty\n",
    "\n",
    "        # Sparse regularization\n",
    "#         fc_params = torch.cat([x.view(-1) for x in tree.inner_nodes.parameters()])\n",
    "#         regularization = sparsity_lamda * torch.norm(fc_params, 2)\n",
    "        regularization = sparsity_lamda * compute_regularization_by_level(tree)\n",
    "        loss = loss_tree\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        pred = output.data.max(1)[1]\n",
    "        correct = pred.eq(target.view(-1).data).sum()\n",
    "        accs.append(correct.item() / data.size()[0])\n",
    "\n",
    "        # Print training status\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(f\"Epoch: {epoch:02d} | Batch: {batch_idx:03d} / {len(loader):03d} | Total loss: {loss.item():.3f} | Reg loss: {regularization.item():.3f} | Tree loss: {loss_tree.item():.3f} | Accuracy: {correct.item() / data.size()[0]:03f} | {round((time.time() - start_time) / iteration, 3)} sec/iter\")\n",
    "            \n",
    "    return iteration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 5e-3\n",
    "weight_decay = 5e-4\n",
    "sparsity_lamda = 2e-3\n",
    "epochs = 100\n",
    "output_dim = len(set(clusters))\n",
    "log_interval = 1\n",
    "use_cuda = device != 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = SDT(input_dim=tensor_dataset.shape[1], output_dim=len(clusters - 1), depth=tree_depth, lamda=1e-3, use_cuda=use_cuda)\n",
    "optimizer = torch.optim.Adam(tree.parameters(),\n",
    "                                 lr=lr,\n",
    "                                 weight_decay=weight_decay)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "tree = tree.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "accs = []\n",
    "sparsity = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.0\n",
      "layer 0: 0.0\n",
      "layer 1: 0.0\n",
      "layer 2: 0.0\n",
      "layer 3: 0.0\n",
      "layer 4: 0.0\n",
      "layer 5: 0.0\n",
      "layer 6: 0.0\n",
      "layer 7: 0.0\n",
      "layer 8: 0.0\n",
      "Epoch: 00 | Batch: 000 / 002 | Total loss: 9.645 | Reg loss: 0.012 | Tree loss: 9.645 | Accuracy: 0.000000 | 1.038 sec/iter\n",
      "Epoch: 00 | Batch: 001 / 002 | Total loss: 9.639 | Reg loss: 0.011 | Tree loss: 9.639 | Accuracy: 0.000000 | 0.952 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 01 | Batch: 000 / 002 | Total loss: 9.635 | Reg loss: 0.003 | Tree loss: 9.635 | Accuracy: 0.000000 | 1.088 sec/iter\n",
      "Epoch: 01 | Batch: 001 / 002 | Total loss: 9.628 | Reg loss: 0.003 | Tree loss: 9.628 | Accuracy: 0.000000 | 1.028 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 02 | Batch: 000 / 002 | Total loss: 9.630 | Reg loss: 0.003 | Tree loss: 9.630 | Accuracy: 0.000000 | 1.09 sec/iter\n",
      "Epoch: 02 | Batch: 001 / 002 | Total loss: 9.625 | Reg loss: 0.003 | Tree loss: 9.625 | Accuracy: 0.000000 | 1.05 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 03 | Batch: 000 / 002 | Total loss: 9.628 | Reg loss: 0.003 | Tree loss: 9.628 | Accuracy: 0.000000 | 1.09 sec/iter\n",
      "Epoch: 03 | Batch: 001 / 002 | Total loss: 9.621 | Reg loss: 0.003 | Tree loss: 9.621 | Accuracy: 0.000000 | 1.06 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 04 | Batch: 000 / 002 | Total loss: 9.625 | Reg loss: 0.003 | Tree loss: 9.625 | Accuracy: 0.000000 | 1.089 sec/iter\n",
      "Epoch: 04 | Batch: 001 / 002 | Total loss: 9.616 | Reg loss: 0.003 | Tree loss: 9.616 | Accuracy: 0.000000 | 1.063 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 05 | Batch: 000 / 002 | Total loss: 9.621 | Reg loss: 0.003 | Tree loss: 9.621 | Accuracy: 0.000000 | 1.087 sec/iter\n",
      "Epoch: 05 | Batch: 001 / 002 | Total loss: 9.613 | Reg loss: 0.003 | Tree loss: 9.613 | Accuracy: 0.000000 | 1.067 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 06 | Batch: 000 / 002 | Total loss: 9.616 | Reg loss: 0.003 | Tree loss: 9.616 | Accuracy: 0.000000 | 1.088 sec/iter\n",
      "Epoch: 06 | Batch: 001 / 002 | Total loss: 9.611 | Reg loss: 0.003 | Tree loss: 9.611 | Accuracy: 0.000000 | 1.07 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 07 | Batch: 000 / 002 | Total loss: 9.614 | Reg loss: 0.003 | Tree loss: 9.614 | Accuracy: 0.000000 | 1.088 sec/iter\n",
      "Epoch: 07 | Batch: 001 / 002 | Total loss: 9.607 | Reg loss: 0.003 | Tree loss: 9.607 | Accuracy: 0.000000 | 1.073 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 08 | Batch: 000 / 002 | Total loss: 9.608 | Reg loss: 0.003 | Tree loss: 9.608 | Accuracy: 0.000000 | 1.091 sec/iter\n",
      "Epoch: 08 | Batch: 001 / 002 | Total loss: 9.603 | Reg loss: 0.003 | Tree loss: 9.603 | Accuracy: 0.000000 | 1.078 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 09 | Batch: 000 / 002 | Total loss: 9.604 | Reg loss: 0.003 | Tree loss: 9.604 | Accuracy: 0.000000 | 1.091 sec/iter\n",
      "Epoch: 09 | Batch: 001 / 002 | Total loss: 9.597 | Reg loss: 0.003 | Tree loss: 9.597 | Accuracy: 0.000000 | 1.081 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 10 | Batch: 000 / 002 | Total loss: 9.600 | Reg loss: 0.003 | Tree loss: 9.600 | Accuracy: 0.000000 | 1.096 sec/iter\n",
      "Epoch: 10 | Batch: 001 / 002 | Total loss: 9.592 | Reg loss: 0.004 | Tree loss: 9.592 | Accuracy: 0.000000 | 1.085 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 11 | Batch: 000 / 002 | Total loss: 9.596 | Reg loss: 0.003 | Tree loss: 9.596 | Accuracy: 0.000000 | 1.091 sec/iter\n",
      "Epoch: 11 | Batch: 001 / 002 | Total loss: 9.587 | Reg loss: 0.004 | Tree loss: 9.587 | Accuracy: 0.000000 | 1.08 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 12 | Batch: 000 / 002 | Total loss: 9.591 | Reg loss: 0.004 | Tree loss: 9.591 | Accuracy: 0.000000 | 1.091 sec/iter\n",
      "Epoch: 12 | Batch: 001 / 002 | Total loss: 9.583 | Reg loss: 0.004 | Tree loss: 9.583 | Accuracy: 0.000000 | 1.082 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 13 | Batch: 000 / 002 | Total loss: 9.586 | Reg loss: 0.004 | Tree loss: 9.586 | Accuracy: 0.000000 | 1.091 sec/iter\n",
      "Epoch: 13 | Batch: 001 / 002 | Total loss: 9.579 | Reg loss: 0.004 | Tree loss: 9.579 | Accuracy: 0.002347 | 1.082 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 14 | Batch: 000 / 002 | Total loss: 9.582 | Reg loss: 0.004 | Tree loss: 9.582 | Accuracy: 0.000000 | 1.092 sec/iter\n",
      "Epoch: 14 | Batch: 001 / 002 | Total loss: 9.573 | Reg loss: 0.004 | Tree loss: 9.573 | Accuracy: 0.002347 | 1.084 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15 | Batch: 000 / 002 | Total loss: 9.578 | Reg loss: 0.004 | Tree loss: 9.578 | Accuracy: 0.000000 | 1.092 sec/iter\n",
      "Epoch: 15 | Batch: 001 / 002 | Total loss: 9.568 | Reg loss: 0.004 | Tree loss: 9.568 | Accuracy: 0.009390 | 1.084 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 16 | Batch: 000 / 002 | Total loss: 9.572 | Reg loss: 0.004 | Tree loss: 9.572 | Accuracy: 0.001953 | 1.093 sec/iter\n",
      "Epoch: 16 | Batch: 001 / 002 | Total loss: 9.567 | Reg loss: 0.004 | Tree loss: 9.567 | Accuracy: 0.007042 | 1.086 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 17 | Batch: 000 / 002 | Total loss: 9.568 | Reg loss: 0.004 | Tree loss: 9.568 | Accuracy: 0.001953 | 1.092 sec/iter\n",
      "Epoch: 17 | Batch: 001 / 002 | Total loss: 9.562 | Reg loss: 0.005 | Tree loss: 9.562 | Accuracy: 0.021127 | 1.085 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 18 | Batch: 000 / 002 | Total loss: 9.564 | Reg loss: 0.005 | Tree loss: 9.564 | Accuracy: 0.011719 | 1.093 sec/iter\n",
      "Epoch: 18 | Batch: 001 / 002 | Total loss: 9.558 | Reg loss: 0.005 | Tree loss: 9.558 | Accuracy: 0.037559 | 1.086 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 19 | Batch: 000 / 002 | Total loss: 9.560 | Reg loss: 0.005 | Tree loss: 9.560 | Accuracy: 0.023438 | 1.092 sec/iter\n",
      "Epoch: 19 | Batch: 001 / 002 | Total loss: 9.554 | Reg loss: 0.005 | Tree loss: 9.554 | Accuracy: 0.093897 | 1.085 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 20 | Batch: 000 / 002 | Total loss: 9.555 | Reg loss: 0.005 | Tree loss: 9.555 | Accuracy: 0.054688 | 1.092 sec/iter\n",
      "Epoch: 20 | Batch: 001 / 002 | Total loss: 9.551 | Reg loss: 0.005 | Tree loss: 9.551 | Accuracy: 0.150235 | 1.086 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 21 | Batch: 000 / 002 | Total loss: 9.552 | Reg loss: 0.005 | Tree loss: 9.552 | Accuracy: 0.126953 | 1.093 sec/iter\n",
      "Epoch: 21 | Batch: 001 / 002 | Total loss: 9.546 | Reg loss: 0.005 | Tree loss: 9.546 | Accuracy: 0.208920 | 1.087 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 22 | Batch: 000 / 002 | Total loss: 9.548 | Reg loss: 0.005 | Tree loss: 9.548 | Accuracy: 0.185547 | 1.093 sec/iter\n",
      "Epoch: 22 | Batch: 001 / 002 | Total loss: 9.542 | Reg loss: 0.005 | Tree loss: 9.542 | Accuracy: 0.276995 | 1.088 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 23 | Batch: 000 / 002 | Total loss: 9.545 | Reg loss: 0.005 | Tree loss: 9.545 | Accuracy: 0.269531 | 1.093 sec/iter\n",
      "Epoch: 23 | Batch: 001 / 002 | Total loss: 9.536 | Reg loss: 0.005 | Tree loss: 9.536 | Accuracy: 0.295775 | 1.088 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 24 | Batch: 000 / 002 | Total loss: 9.540 | Reg loss: 0.005 | Tree loss: 9.540 | Accuracy: 0.285156 | 1.094 sec/iter\n",
      "Epoch: 24 | Batch: 001 / 002 | Total loss: 9.534 | Reg loss: 0.005 | Tree loss: 9.534 | Accuracy: 0.276995 | 1.089 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 25 | Batch: 000 / 002 | Total loss: 9.535 | Reg loss: 0.005 | Tree loss: 9.535 | Accuracy: 0.306641 | 1.094 sec/iter\n",
      "Epoch: 25 | Batch: 001 / 002 | Total loss: 9.531 | Reg loss: 0.005 | Tree loss: 9.531 | Accuracy: 0.251174 | 1.089 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 26 | Batch: 000 / 002 | Total loss: 9.531 | Reg loss: 0.005 | Tree loss: 9.531 | Accuracy: 0.296875 | 1.094 sec/iter\n",
      "Epoch: 26 | Batch: 001 / 002 | Total loss: 9.528 | Reg loss: 0.005 | Tree loss: 9.528 | Accuracy: 0.262911 | 1.089 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 27 | Batch: 000 / 002 | Total loss: 9.529 | Reg loss: 0.005 | Tree loss: 9.529 | Accuracy: 0.277344 | 1.094 sec/iter\n",
      "Epoch: 27 | Batch: 001 / 002 | Total loss: 9.521 | Reg loss: 0.006 | Tree loss: 9.521 | Accuracy: 0.286385 | 1.09 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 28 | Batch: 000 / 002 | Total loss: 9.526 | Reg loss: 0.006 | Tree loss: 9.526 | Accuracy: 0.273438 | 1.094 sec/iter\n",
      "Epoch: 28 | Batch: 001 / 002 | Total loss: 9.517 | Reg loss: 0.006 | Tree loss: 9.517 | Accuracy: 0.291080 | 1.09 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 29 | Batch: 000 / 002 | Total loss: 9.521 | Reg loss: 0.006 | Tree loss: 9.521 | Accuracy: 0.277344 | 1.094 sec/iter\n",
      "Epoch: 29 | Batch: 001 / 002 | Total loss: 9.514 | Reg loss: 0.006 | Tree loss: 9.514 | Accuracy: 0.286385 | 1.09 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 30 | Batch: 000 / 002 | Total loss: 9.516 | Reg loss: 0.006 | Tree loss: 9.516 | Accuracy: 0.289062 | 1.093 sec/iter\n",
      "Epoch: 30 | Batch: 001 / 002 | Total loss: 9.512 | Reg loss: 0.006 | Tree loss: 9.512 | Accuracy: 0.272300 | 1.089 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 31 | Batch: 000 / 002 | Total loss: 9.514 | Reg loss: 0.006 | Tree loss: 9.514 | Accuracy: 0.269531 | 1.093 sec/iter\n",
      "Epoch: 31 | Batch: 001 / 002 | Total loss: 9.505 | Reg loss: 0.006 | Tree loss: 9.505 | Accuracy: 0.295775 | 1.09 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 32 | Batch: 000 / 002 | Total loss: 9.510 | Reg loss: 0.006 | Tree loss: 9.510 | Accuracy: 0.259766 | 1.093 sec/iter\n",
      "Epoch: 32 | Batch: 001 / 002 | Total loss: 9.503 | Reg loss: 0.006 | Tree loss: 9.503 | Accuracy: 0.307512 | 1.09 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 33 | Batch: 000 / 002 | Total loss: 9.505 | Reg loss: 0.006 | Tree loss: 9.505 | Accuracy: 0.283203 | 1.093 sec/iter\n",
      "Epoch: 33 | Batch: 001 / 002 | Total loss: 9.499 | Reg loss: 0.006 | Tree loss: 9.499 | Accuracy: 0.279343 | 1.089 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 34 | Batch: 000 / 002 | Total loss: 9.502 | Reg loss: 0.006 | Tree loss: 9.502 | Accuracy: 0.279297 | 1.091 sec/iter\n",
      "Epoch: 34 | Batch: 001 / 002 | Total loss: 9.495 | Reg loss: 0.006 | Tree loss: 9.495 | Accuracy: 0.284038 | 1.088 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 35 | Batch: 000 / 002 | Total loss: 9.498 | Reg loss: 0.006 | Tree loss: 9.498 | Accuracy: 0.273438 | 1.091 sec/iter\n",
      "Epoch: 35 | Batch: 001 / 002 | Total loss: 9.491 | Reg loss: 0.006 | Tree loss: 9.491 | Accuracy: 0.291080 | 1.088 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 36 | Batch: 000 / 002 | Total loss: 9.493 | Reg loss: 0.006 | Tree loss: 9.493 | Accuracy: 0.294922 | 1.091 sec/iter\n",
      "Epoch: 36 | Batch: 001 / 002 | Total loss: 9.489 | Reg loss: 0.006 | Tree loss: 9.489 | Accuracy: 0.265258 | 1.088 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 37 | Batch: 000 / 002 | Total loss: 9.491 | Reg loss: 0.006 | Tree loss: 9.491 | Accuracy: 0.255859 | 1.091 sec/iter\n",
      "Epoch: 37 | Batch: 001 / 002 | Total loss: 9.482 | Reg loss: 0.006 | Tree loss: 9.482 | Accuracy: 0.312207 | 1.088 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 38 | Batch: 000 / 002 | Total loss: 9.489 | Reg loss: 0.006 | Tree loss: 9.489 | Accuracy: 0.265625 | 1.091 sec/iter\n",
      "Epoch: 38 | Batch: 001 / 002 | Total loss: 9.477 | Reg loss: 0.006 | Tree loss: 9.477 | Accuracy: 0.300469 | 1.088 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 39 | Batch: 000 / 002 | Total loss: 9.484 | Reg loss: 0.006 | Tree loss: 9.484 | Accuracy: 0.271484 | 1.092 sec/iter\n",
      "Epoch: 39 | Batch: 001 / 002 | Total loss: 9.474 | Reg loss: 0.007 | Tree loss: 9.474 | Accuracy: 0.293427 | 1.089 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 40 | Batch: 000 / 002 | Total loss: 9.479 | Reg loss: 0.007 | Tree loss: 9.479 | Accuracy: 0.287109 | 1.092 sec/iter\n",
      "Epoch: 40 | Batch: 001 / 002 | Total loss: 9.472 | Reg loss: 0.007 | Tree loss: 9.472 | Accuracy: 0.274648 | 1.089 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 41 | Batch: 000 / 002 | Total loss: 9.477 | Reg loss: 0.007 | Tree loss: 9.477 | Accuracy: 0.265625 | 1.092 sec/iter\n",
      "Epoch: 41 | Batch: 001 / 002 | Total loss: 9.465 | Reg loss: 0.007 | Tree loss: 9.465 | Accuracy: 0.300469 | 1.089 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 42 | Batch: 000 / 002 | Total loss: 9.472 | Reg loss: 0.007 | Tree loss: 9.472 | Accuracy: 0.273438 | 1.092 sec/iter\n",
      "Epoch: 42 | Batch: 001 / 002 | Total loss: 9.463 | Reg loss: 0.007 | Tree loss: 9.463 | Accuracy: 0.291080 | 1.089 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 43 | Batch: 000 / 002 | Total loss: 9.468 | Reg loss: 0.007 | Tree loss: 9.468 | Accuracy: 0.283203 | 1.092 sec/iter\n",
      "Epoch: 43 | Batch: 001 / 002 | Total loss: 9.460 | Reg loss: 0.007 | Tree loss: 9.460 | Accuracy: 0.279343 | 1.089 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 44 | Batch: 000 / 002 | Total loss: 9.462 | Reg loss: 0.007 | Tree loss: 9.462 | Accuracy: 0.296875 | 1.092 sec/iter\n",
      "Epoch: 44 | Batch: 001 / 002 | Total loss: 9.458 | Reg loss: 0.007 | Tree loss: 9.458 | Accuracy: 0.262911 | 1.089 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 45 | Batch: 000 / 002 | Total loss: 9.458 | Reg loss: 0.007 | Tree loss: 9.458 | Accuracy: 0.302734 | 1.092 sec/iter\n",
      "Epoch: 45 | Batch: 001 / 002 | Total loss: 9.454 | Reg loss: 0.007 | Tree loss: 9.454 | Accuracy: 0.255869 | 1.09 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 46 | Batch: 000 / 002 | Total loss: 9.457 | Reg loss: 0.007 | Tree loss: 9.457 | Accuracy: 0.267578 | 1.092 sec/iter\n",
      "Epoch: 46 | Batch: 001 / 002 | Total loss: 9.446 | Reg loss: 0.007 | Tree loss: 9.446 | Accuracy: 0.298122 | 1.089 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 47 | Batch: 000 / 002 | Total loss: 9.448 | Reg loss: 0.007 | Tree loss: 9.448 | Accuracy: 0.298828 | 1.092 sec/iter\n",
      "Epoch: 47 | Batch: 001 / 002 | Total loss: 9.448 | Reg loss: 0.007 | Tree loss: 9.448 | Accuracy: 0.260563 | 1.09 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 48 | Batch: 000 / 002 | Total loss: 9.451 | Reg loss: 0.007 | Tree loss: 9.451 | Accuracy: 0.257812 | 1.092 sec/iter\n",
      "Epoch: 48 | Batch: 001 / 002 | Total loss: 9.435 | Reg loss: 0.007 | Tree loss: 9.435 | Accuracy: 0.309859 | 1.089 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 49 | Batch: 000 / 002 | Total loss: 9.445 | Reg loss: 0.007 | Tree loss: 9.445 | Accuracy: 0.263672 | 1.092 sec/iter\n",
      "Epoch: 49 | Batch: 001 / 002 | Total loss: 9.434 | Reg loss: 0.007 | Tree loss: 9.434 | Accuracy: 0.302817 | 1.09 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 50 | Batch: 000 / 002 | Total loss: 9.436 | Reg loss: 0.007 | Tree loss: 9.436 | Accuracy: 0.296875 | 1.092 sec/iter\n",
      "Epoch: 50 | Batch: 001 / 002 | Total loss: 9.436 | Reg loss: 0.008 | Tree loss: 9.436 | Accuracy: 0.262911 | 1.09 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 51 | Batch: 000 / 002 | Total loss: 9.436 | Reg loss: 0.007 | Tree loss: 9.436 | Accuracy: 0.271484 | 1.092 sec/iter\n",
      "Epoch: 51 | Batch: 001 / 002 | Total loss: 9.426 | Reg loss: 0.008 | Tree loss: 9.426 | Accuracy: 0.293427 | 1.09 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 52 | Batch: 000 / 002 | Total loss: 9.435 | Reg loss: 0.008 | Tree loss: 9.435 | Accuracy: 0.253906 | 1.092 sec/iter\n",
      "Epoch: 52 | Batch: 001 / 002 | Total loss: 9.418 | Reg loss: 0.008 | Tree loss: 9.418 | Accuracy: 0.314554 | 1.09 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 53 | Batch: 000 / 002 | Total loss: 9.428 | Reg loss: 0.008 | Tree loss: 9.428 | Accuracy: 0.277344 | 1.092 sec/iter\n",
      "Epoch: 53 | Batch: 001 / 002 | Total loss: 9.418 | Reg loss: 0.008 | Tree loss: 9.418 | Accuracy: 0.286385 | 1.09 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 54 | Batch: 000 / 002 | Total loss: 9.421 | Reg loss: 0.008 | Tree loss: 9.421 | Accuracy: 0.294922 | 1.092 sec/iter\n",
      "Epoch: 54 | Batch: 001 / 002 | Total loss: 9.416 | Reg loss: 0.008 | Tree loss: 9.416 | Accuracy: 0.265258 | 1.09 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 55 | Batch: 000 / 002 | Total loss: 9.419 | Reg loss: 0.008 | Tree loss: 9.419 | Accuracy: 0.263672 | 1.092 sec/iter\n",
      "Epoch: 55 | Batch: 001 / 002 | Total loss: 9.409 | Reg loss: 0.008 | Tree loss: 9.409 | Accuracy: 0.302817 | 1.09 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 56 | Batch: 000 / 002 | Total loss: 9.418 | Reg loss: 0.008 | Tree loss: 9.418 | Accuracy: 0.248047 | 1.092 sec/iter\n",
      "Epoch: 56 | Batch: 001 / 002 | Total loss: 9.401 | Reg loss: 0.008 | Tree loss: 9.401 | Accuracy: 0.321596 | 1.09 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 57 | Batch: 000 / 002 | Total loss: 9.409 | Reg loss: 0.008 | Tree loss: 9.409 | Accuracy: 0.289062 | 1.091 sec/iter\n",
      "Epoch: 57 | Batch: 001 / 002 | Total loss: 9.402 | Reg loss: 0.008 | Tree loss: 9.402 | Accuracy: 0.272300 | 1.089 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 58 | Batch: 000 / 002 | Total loss: 9.404 | Reg loss: 0.008 | Tree loss: 9.404 | Accuracy: 0.291016 | 1.091 sec/iter\n",
      "Epoch: 58 | Batch: 001 / 002 | Total loss: 9.398 | Reg loss: 0.008 | Tree loss: 9.398 | Accuracy: 0.269953 | 1.089 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 59 | Batch: 000 / 002 | Total loss: 9.404 | Reg loss: 0.008 | Tree loss: 9.404 | Accuracy: 0.267578 | 1.091 sec/iter\n",
      "Epoch: 59 | Batch: 001 / 002 | Total loss: 9.387 | Reg loss: 0.008 | Tree loss: 9.387 | Accuracy: 0.298122 | 1.089 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 60 | Batch: 000 / 002 | Total loss: 9.390 | Reg loss: 0.008 | Tree loss: 9.390 | Accuracy: 0.314453 | 1.092 sec/iter\n",
      "Epoch: 60 | Batch: 001 / 002 | Total loss: 9.395 | Reg loss: 0.008 | Tree loss: 9.395 | Accuracy: 0.241784 | 1.09 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 61 | Batch: 000 / 002 | Total loss: 9.388 | Reg loss: 0.008 | Tree loss: 9.388 | Accuracy: 0.292969 | 1.092 sec/iter\n",
      "Epoch: 61 | Batch: 001 / 002 | Total loss: 9.387 | Reg loss: 0.009 | Tree loss: 9.387 | Accuracy: 0.267606 | 1.09 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 62 | Batch: 000 / 002 | Total loss: 9.389 | Reg loss: 0.009 | Tree loss: 9.389 | Accuracy: 0.263672 | 1.092 sec/iter\n",
      "Epoch: 62 | Batch: 001 / 002 | Total loss: 9.374 | Reg loss: 0.009 | Tree loss: 9.374 | Accuracy: 0.302817 | 1.09 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 63 | Batch: 000 / 002 | Total loss: 9.377 | Reg loss: 0.009 | Tree loss: 9.377 | Accuracy: 0.296875 | 1.092 sec/iter\n",
      "Epoch: 63 | Batch: 001 / 002 | Total loss: 9.379 | Reg loss: 0.009 | Tree loss: 9.379 | Accuracy: 0.262911 | 1.09 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 64 | Batch: 000 / 002 | Total loss: 9.377 | Reg loss: 0.009 | Tree loss: 9.377 | Accuracy: 0.273438 | 1.092 sec/iter\n",
      "Epoch: 64 | Batch: 001 / 002 | Total loss: 9.367 | Reg loss: 0.009 | Tree loss: 9.367 | Accuracy: 0.291080 | 1.09 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 65 | Batch: 000 / 002 | Total loss: 9.367 | Reg loss: 0.009 | Tree loss: 9.367 | Accuracy: 0.308594 | 1.092 sec/iter\n",
      "Epoch: 65 | Batch: 001 / 002 | Total loss: 9.369 | Reg loss: 0.009 | Tree loss: 9.369 | Accuracy: 0.248826 | 1.09 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 66 | Batch: 000 / 002 | Total loss: 9.365 | Reg loss: 0.009 | Tree loss: 9.365 | Accuracy: 0.279297 | 1.092 sec/iter\n",
      "Epoch: 66 | Batch: 001 / 002 | Total loss: 9.359 | Reg loss: 0.009 | Tree loss: 9.359 | Accuracy: 0.284038 | 1.09 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 67 | Batch: 000 / 002 | Total loss: 9.357 | Reg loss: 0.009 | Tree loss: 9.357 | Accuracy: 0.302734 | 1.091 sec/iter\n",
      "Epoch: 67 | Batch: 001 / 002 | Total loss: 9.359 | Reg loss: 0.009 | Tree loss: 9.359 | Accuracy: 0.255869 | 1.089 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 68 | Batch: 000 / 002 | Total loss: 9.354 | Reg loss: 0.009 | Tree loss: 9.354 | Accuracy: 0.279297 | 1.091 sec/iter\n",
      "Epoch: 68 | Batch: 001 / 002 | Total loss: 9.350 | Reg loss: 0.009 | Tree loss: 9.350 | Accuracy: 0.284038 | 1.09 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 69 | Batch: 000 / 002 | Total loss: 9.347 | Reg loss: 0.009 | Tree loss: 9.347 | Accuracy: 0.296875 | 1.091 sec/iter\n",
      "Epoch: 69 | Batch: 001 / 002 | Total loss: 9.347 | Reg loss: 0.009 | Tree loss: 9.347 | Accuracy: 0.262911 | 1.09 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 70 | Batch: 000 / 002 | Total loss: 9.344 | Reg loss: 0.009 | Tree loss: 9.344 | Accuracy: 0.289062 | 1.091 sec/iter\n",
      "Epoch: 70 | Batch: 001 / 002 | Total loss: 9.338 | Reg loss: 0.009 | Tree loss: 9.338 | Accuracy: 0.272300 | 1.09 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 71 | Batch: 000 / 002 | Total loss: 9.344 | Reg loss: 0.009 | Tree loss: 9.344 | Accuracy: 0.273438 | 1.091 sec/iter\n",
      "Epoch: 71 | Batch: 001 / 002 | Total loss: 9.325 | Reg loss: 0.010 | Tree loss: 9.325 | Accuracy: 0.291080 | 1.09 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 72 | Batch: 000 / 002 | Total loss: 9.334 | Reg loss: 0.010 | Tree loss: 9.334 | Accuracy: 0.292969 | 1.091 sec/iter\n",
      "Epoch: 72 | Batch: 001 / 002 | Total loss: 9.325 | Reg loss: 0.010 | Tree loss: 9.325 | Accuracy: 0.267606 | 1.09 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 73 | Batch: 000 / 002 | Total loss: 9.330 | Reg loss: 0.010 | Tree loss: 9.330 | Accuracy: 0.281250 | 1.091 sec/iter\n",
      "Epoch: 73 | Batch: 001 / 002 | Total loss: 9.317 | Reg loss: 0.010 | Tree loss: 9.317 | Accuracy: 0.281690 | 1.09 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 74 | Batch: 000 / 002 | Total loss: 9.323 | Reg loss: 0.010 | Tree loss: 9.323 | Accuracy: 0.279297 | 1.091 sec/iter\n",
      "Epoch: 74 | Batch: 001 / 002 | Total loss: 9.313 | Reg loss: 0.010 | Tree loss: 9.313 | Accuracy: 0.284038 | 1.09 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 75 | Batch: 000 / 002 | Total loss: 9.315 | Reg loss: 0.010 | Tree loss: 9.315 | Accuracy: 0.291016 | 1.091 sec/iter\n",
      "Epoch: 75 | Batch: 001 / 002 | Total loss: 9.309 | Reg loss: 0.010 | Tree loss: 9.309 | Accuracy: 0.269953 | 1.09 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 76 | Batch: 000 / 002 | Total loss: 9.313 | Reg loss: 0.010 | Tree loss: 9.313 | Accuracy: 0.279297 | 1.091 sec/iter\n",
      "Epoch: 76 | Batch: 001 / 002 | Total loss: 9.298 | Reg loss: 0.010 | Tree loss: 9.298 | Accuracy: 0.284038 | 1.09 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 77 | Batch: 000 / 002 | Total loss: 9.308 | Reg loss: 0.010 | Tree loss: 9.308 | Accuracy: 0.273438 | 1.091 sec/iter\n",
      "Epoch: 77 | Batch: 001 / 002 | Total loss: 9.289 | Reg loss: 0.010 | Tree loss: 9.289 | Accuracy: 0.291080 | 1.09 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 78 | Batch: 000 / 002 | Total loss: 9.301 | Reg loss: 0.010 | Tree loss: 9.301 | Accuracy: 0.263672 | 1.092 sec/iter\n",
      "Epoch: 78 | Batch: 001 / 002 | Total loss: 9.284 | Reg loss: 0.011 | Tree loss: 9.284 | Accuracy: 0.302817 | 1.09 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 79 | Batch: 000 / 002 | Total loss: 9.295 | Reg loss: 0.011 | Tree loss: 9.295 | Accuracy: 0.261719 | 1.092 sec/iter\n",
      "Epoch: 79 | Batch: 001 / 002 | Total loss: 9.276 | Reg loss: 0.011 | Tree loss: 9.276 | Accuracy: 0.305164 | 1.09 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 80 | Batch: 000 / 002 | Total loss: 9.285 | Reg loss: 0.011 | Tree loss: 9.285 | Accuracy: 0.292969 | 1.091 sec/iter\n",
      "Epoch: 80 | Batch: 001 / 002 | Total loss: 9.273 | Reg loss: 0.011 | Tree loss: 9.273 | Accuracy: 0.267606 | 1.09 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 81 | Batch: 000 / 002 | Total loss: 9.271 | Reg loss: 0.011 | Tree loss: 9.271 | Accuracy: 0.308594 | 1.092 sec/iter\n",
      "Epoch: 81 | Batch: 001 / 002 | Total loss: 9.275 | Reg loss: 0.011 | Tree loss: 9.275 | Accuracy: 0.248826 | 1.09 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 82 | Batch: 000 / 002 | Total loss: 9.269 | Reg loss: 0.011 | Tree loss: 9.269 | Accuracy: 0.292969 | 1.092 sec/iter\n",
      "Epoch: 82 | Batch: 001 / 002 | Total loss: 9.261 | Reg loss: 0.011 | Tree loss: 9.261 | Accuracy: 0.267606 | 1.09 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 83 | Batch: 000 / 002 | Total loss: 9.264 | Reg loss: 0.011 | Tree loss: 9.264 | Accuracy: 0.287109 | 1.092 sec/iter\n",
      "Epoch: 83 | Batch: 001 / 002 | Total loss: 9.251 | Reg loss: 0.011 | Tree loss: 9.251 | Accuracy: 0.274648 | 1.09 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 84 | Batch: 000 / 002 | Total loss: 9.248 | Reg loss: 0.011 | Tree loss: 9.248 | Accuracy: 0.292969 | 1.092 sec/iter\n",
      "Epoch: 84 | Batch: 001 / 002 | Total loss: 9.254 | Reg loss: 0.011 | Tree loss: 9.254 | Accuracy: 0.267606 | 1.091 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 85 | Batch: 000 / 002 | Total loss: 9.247 | Reg loss: 0.011 | Tree loss: 9.247 | Accuracy: 0.279297 | 1.092 sec/iter\n",
      "Epoch: 85 | Batch: 001 / 002 | Total loss: 9.237 | Reg loss: 0.012 | Tree loss: 9.237 | Accuracy: 0.284038 | 1.09 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 86 | Batch: 000 / 002 | Total loss: 9.238 | Reg loss: 0.012 | Tree loss: 9.238 | Accuracy: 0.294922 | 1.092 sec/iter\n",
      "Epoch: 86 | Batch: 001 / 002 | Total loss: 9.230 | Reg loss: 0.012 | Tree loss: 9.230 | Accuracy: 0.265258 | 1.09 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 87 | Batch: 000 / 002 | Total loss: 9.230 | Reg loss: 0.012 | Tree loss: 9.230 | Accuracy: 0.291016 | 1.092 sec/iter\n",
      "Epoch: 87 | Batch: 001 / 002 | Total loss: 9.222 | Reg loss: 0.012 | Tree loss: 9.222 | Accuracy: 0.269953 | 1.09 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 88 | Batch: 000 / 002 | Total loss: 9.225 | Reg loss: 0.012 | Tree loss: 9.225 | Accuracy: 0.287109 | 1.092 sec/iter\n",
      "Epoch: 88 | Batch: 001 / 002 | Total loss: 9.209 | Reg loss: 0.012 | Tree loss: 9.209 | Accuracy: 0.274648 | 1.09 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 89 | Batch: 000 / 002 | Total loss: 9.214 | Reg loss: 0.012 | Tree loss: 9.214 | Accuracy: 0.273438 | 1.092 sec/iter\n",
      "Epoch: 89 | Batch: 001 / 002 | Total loss: 9.204 | Reg loss: 0.012 | Tree loss: 9.204 | Accuracy: 0.291080 | 1.091 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 90 | Batch: 000 / 002 | Total loss: 9.209 | Reg loss: 0.012 | Tree loss: 9.209 | Accuracy: 0.267578 | 1.092 sec/iter\n",
      "Epoch: 90 | Batch: 001 / 002 | Total loss: 9.189 | Reg loss: 0.012 | Tree loss: 9.189 | Accuracy: 0.298122 | 1.091 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 91 | Batch: 000 / 002 | Total loss: 9.194 | Reg loss: 0.012 | Tree loss: 9.194 | Accuracy: 0.292969 | 1.092 sec/iter\n",
      "Epoch: 91 | Batch: 001 / 002 | Total loss: 9.188 | Reg loss: 0.013 | Tree loss: 9.188 | Accuracy: 0.267606 | 1.091 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 92 | Batch: 000 / 002 | Total loss: 9.190 | Reg loss: 0.013 | Tree loss: 9.190 | Accuracy: 0.263672 | 1.092 sec/iter\n",
      "Epoch: 92 | Batch: 001 / 002 | Total loss: 9.172 | Reg loss: 0.013 | Tree loss: 9.172 | Accuracy: 0.302817 | 1.091 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 93 | Batch: 000 / 002 | Total loss: 9.177 | Reg loss: 0.013 | Tree loss: 9.177 | Accuracy: 0.292969 | 1.092 sec/iter\n",
      "Epoch: 93 | Batch: 001 / 002 | Total loss: 9.166 | Reg loss: 0.013 | Tree loss: 9.166 | Accuracy: 0.267606 | 1.091 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 94 | Batch: 000 / 002 | Total loss: 9.170 | Reg loss: 0.013 | Tree loss: 9.170 | Accuracy: 0.279297 | 1.093 sec/iter\n",
      "Epoch: 94 | Batch: 001 / 002 | Total loss: 9.154 | Reg loss: 0.013 | Tree loss: 9.154 | Accuracy: 0.284038 | 1.091 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 95 | Batch: 000 / 002 | Total loss: 9.168 | Reg loss: 0.013 | Tree loss: 9.168 | Accuracy: 0.261719 | 1.093 sec/iter\n",
      "Epoch: 95 | Batch: 001 / 002 | Total loss: 9.133 | Reg loss: 0.013 | Tree loss: 9.133 | Accuracy: 0.305164 | 1.091 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 96 | Batch: 000 / 002 | Total loss: 9.145 | Reg loss: 0.013 | Tree loss: 9.145 | Accuracy: 0.289062 | 1.093 sec/iter\n",
      "Epoch: 96 | Batch: 001 / 002 | Total loss: 9.138 | Reg loss: 0.013 | Tree loss: 9.138 | Accuracy: 0.272300 | 1.091 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 97 | Batch: 000 / 002 | Total loss: 9.137 | Reg loss: 0.013 | Tree loss: 9.137 | Accuracy: 0.294922 | 1.092 sec/iter\n",
      "Epoch: 97 | Batch: 001 / 002 | Total loss: 9.124 | Reg loss: 0.014 | Tree loss: 9.124 | Accuracy: 0.265258 | 1.091 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 98 | Batch: 000 / 002 | Total loss: 9.134 | Reg loss: 0.014 | Tree loss: 9.134 | Accuracy: 0.273438 | 1.092 sec/iter\n",
      "Epoch: 98 | Batch: 001 / 002 | Total loss: 9.103 | Reg loss: 0.014 | Tree loss: 9.103 | Accuracy: 0.291080 | 1.091 sec/iter\n",
      "Average sparseness: 0.9821428571428573\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "layer 5: 0.9821428571428571\n",
      "layer 6: 0.982142857142857\n",
      "layer 7: 0.9821428571428573\n",
      "layer 8: 0.9821428571428573\n",
      "Epoch: 99 | Batch: 000 / 002 | Total loss: 9.117 | Reg loss: 0.014 | Tree loss: 9.117 | Accuracy: 0.283203 | 1.092 sec/iter\n",
      "Epoch: 99 | Batch: 001 / 002 | Total loss: 9.099 | Reg loss: 0.014 | Tree loss: 9.099 | Accuracy: 0.279343 | 1.091 sec/iter\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "iteration = 0\n",
    "for epoch in range(epochs):\n",
    "    # Training\n",
    "    avg_sp = show_sparseness(tree)\n",
    "    sparsity.append(avg_sp)\n",
    "    iteration = do_epoch(tree, tree_loader, device, log_interval, losses, accs, epoch, iteration)\n",
    "    \n",
    "    if epoch % 1 == 0:\n",
    "        prune_tree(tree, factor=3)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3098e57d967e4246936a2d482766ce3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel('Iteration')\n",
    "plt.plot(accs, label='Accuracy vs iteration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6f77ad50a9d43eca240009edc15b33f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e6f43f36de24c6fb8ee026cfcb4c44a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel('Iteration')\n",
    "plt.plot(losses, label='Loss vs iteration')\n",
    "plt.yscale(\"log\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "weights = tree.inner_nodes.weight.cpu().detach().numpy().flatten()\n",
    "plt.hist(weights, bins=500)\n",
    "weights_std = np.std(weights)\n",
    "weights_mean = np.mean(weights)\n",
    "plt.axvline(weights_mean + weights_std, color='r')\n",
    "plt.axvline(weights_mean - weights_std, color='r')\n",
    "plt.title(f\"Mean: {weights_mean}   |   STD: {weights_std}\")\n",
    "plt.yscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cfdcd7e42324daf863b3fe7df3b673d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average height: 10.0\n"
     ]
    }
   ],
   "source": [
    "plt.figure(figsize=(15, 10), dpi=80)\n",
    "avg_height, root = tree.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accumulate samples in the leaves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of patterns: 1024\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of patterns: {len(root.get_leaves())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = 'greedy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eitan.k/.local/lib/python3.6/site-packages/torch/nn/functional.py:1709: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    }
   ],
   "source": [
    "root.clear_leaves_samples()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (data, target) in enumerate(tree_loader):\n",
    "        root.accumulate_samples(data, method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tighten boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Pattern 1 ==============\n",
      "============== Pattern 2 ==============\n",
      "============== Pattern 3 ==============\n",
      "============== Pattern 4 ==============\n",
      "============== Pattern 5 ==============\n",
      "============== Pattern 6 ==============\n",
      "============== Pattern 7 ==============\n",
      "============== Pattern 8 ==============\n",
      "============== Pattern 9 ==============\n",
      "============== Pattern 10 ==============\n",
      "============== Pattern 11 ==============\n",
      "============== Pattern 12 ==============\n",
      "============== Pattern 13 ==============\n",
      "============== Pattern 14 ==============\n",
      "============== Pattern 15 ==============\n",
      "============== Pattern 16 ==============\n",
      "============== Pattern 17 ==============\n",
      "============== Pattern 18 ==============\n",
      "============== Pattern 19 ==============\n",
      "============== Pattern 20 ==============\n",
      "============== Pattern 21 ==============\n",
      "============== Pattern 22 ==============\n",
      "============== Pattern 23 ==============\n",
      "============== Pattern 24 ==============\n",
      "============== Pattern 25 ==============\n",
      "============== Pattern 26 ==============\n",
      "============== Pattern 27 ==============\n",
      "============== Pattern 28 ==============\n",
      "============== Pattern 29 ==============\n",
      "============== Pattern 30 ==============\n",
      "============== Pattern 31 ==============\n",
      "============== Pattern 32 ==============\n",
      "============== Pattern 33 ==============\n",
      "============== Pattern 34 ==============\n",
      "============== Pattern 35 ==============\n",
      "============== Pattern 36 ==============\n",
      "============== Pattern 37 ==============\n",
      "============== Pattern 38 ==============\n",
      "============== Pattern 39 ==============\n",
      "============== Pattern 40 ==============\n",
      "============== Pattern 41 ==============\n",
      "============== Pattern 42 ==============\n",
      "============== Pattern 43 ==============\n",
      "============== Pattern 44 ==============\n",
      "============== Pattern 45 ==============\n",
      "============== Pattern 46 ==============\n",
      "============== Pattern 47 ==============\n",
      "============== Pattern 48 ==============\n",
      "============== Pattern 49 ==============\n",
      "============== Pattern 50 ==============\n",
      "============== Pattern 51 ==============\n",
      "============== Pattern 52 ==============\n",
      "============== Pattern 53 ==============\n",
      "============== Pattern 54 ==============\n",
      "============== Pattern 55 ==============\n",
      "============== Pattern 56 ==============\n",
      "============== Pattern 57 ==============\n",
      "============== Pattern 58 ==============\n",
      "============== Pattern 59 ==============\n",
      "============== Pattern 60 ==============\n",
      "============== Pattern 61 ==============\n",
      "============== Pattern 62 ==============\n",
      "============== Pattern 63 ==============\n",
      "============== Pattern 64 ==============\n",
      "============== Pattern 65 ==============\n",
      "501\n",
      "============== Pattern 66 ==============\n",
      "============== Pattern 67 ==============\n",
      "============== Pattern 68 ==============\n",
      "============== Pattern 69 ==============\n",
      "============== Pattern 70 ==============\n",
      "============== Pattern 71 ==============\n",
      "============== Pattern 72 ==============\n",
      "188\n",
      "============== Pattern 73 ==============\n",
      "============== Pattern 74 ==============\n",
      "============== Pattern 75 ==============\n",
      "============== Pattern 76 ==============\n",
      "============== Pattern 77 ==============\n",
      "============== Pattern 78 ==============\n",
      "============== Pattern 79 ==============\n",
      "============== Pattern 80 ==============\n",
      "============== Pattern 81 ==============\n",
      "42\n",
      "============== Pattern 82 ==============\n",
      "============== Pattern 83 ==============\n",
      "============== Pattern 84 ==============\n",
      "============== Pattern 85 ==============\n",
      "============== Pattern 86 ==============\n",
      "============== Pattern 87 ==============\n",
      "============== Pattern 88 ==============\n",
      "============== Pattern 89 ==============\n",
      "============== Pattern 90 ==============\n",
      "============== Pattern 91 ==============\n",
      "============== Pattern 92 ==============\n",
      "============== Pattern 93 ==============\n",
      "============== Pattern 94 ==============\n",
      "============== Pattern 95 ==============\n",
      "============== Pattern 96 ==============\n",
      "============== Pattern 97 ==============\n",
      "============== Pattern 98 ==============\n",
      "============== Pattern 99 ==============\n",
      "============== Pattern 100 ==============\n",
      "============== Pattern 101 ==============\n",
      "============== Pattern 102 ==============\n",
      "============== Pattern 103 ==============\n",
      "============== Pattern 104 =============="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eitan.k/EntangledExplainableClustering/soft_decision_tree/sdt_model.py:14: RuntimeWarning: divide by zero encountered in log\n",
      "  return np.log(1 / (1 - x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============== Pattern 105 ==============\n",
      "============== Pattern 106 ==============\n",
      "============== Pattern 107 ==============\n",
      "============== Pattern 108 ==============\n",
      "============== Pattern 109 ==============\n",
      "============== Pattern 110 ==============\n",
      "============== Pattern 111 ==============\n",
      "============== Pattern 112 ==============\n",
      "============== Pattern 113 ==============\n",
      "============== Pattern 114 ==============\n",
      "============== Pattern 115 ==============\n",
      "============== Pattern 116 ==============\n",
      "============== Pattern 117 ==============\n",
      "============== Pattern 118 ==============\n",
      "============== Pattern 119 ==============\n",
      "============== Pattern 120 ==============\n",
      "============== Pattern 121 ==============\n",
      "============== Pattern 122 ==============\n",
      "============== Pattern 123 ==============\n",
      "============== Pattern 124 ==============\n",
      "============== Pattern 125 ==============\n",
      "============== Pattern 126 ==============\n",
      "============== Pattern 127 ==============\n",
      "============== Pattern 128 ==============\n",
      "============== Pattern 129 ==============\n",
      "============== Pattern 130 ==============\n",
      "============== Pattern 131 ==============\n",
      "7\n",
      "============== Pattern 132 ==============\n",
      "============== Pattern 133 ==============\n",
      "============== Pattern 134 ==============\n",
      "============== Pattern 135 ==============\n",
      "============== Pattern 136 ==============\n",
      "============== Pattern 137 ==============\n",
      "============== Pattern 138 ==============\n",
      "============== Pattern 139 ==============\n",
      "============== Pattern 140 ==============\n",
      "1\n",
      "============== Pattern 141 ==============\n",
      "============== Pattern 142 ==============\n",
      "============== Pattern 143 ==============\n",
      "============== Pattern 144 ==============\n",
      "============== Pattern 145 ==============\n",
      "============== Pattern 146 ==============\n",
      "============== Pattern 147 ==============\n",
      "============== Pattern 148 ==============\n",
      "============== Pattern 149 ==============\n",
      "============== Pattern 150 ==============\n",
      "============== Pattern 151 ==============\n",
      "============== Pattern 152 ==============\n",
      "============== Pattern 153 ==============\n",
      "============== Pattern 154 ==============\n",
      "============== Pattern 155 ==============\n",
      "1\n",
      "============== Pattern 156 ==============\n",
      "============== Pattern 157 ==============\n",
      "============== Pattern 158 ==============\n",
      "============== Pattern 159 ==============\n",
      "============== Pattern 160 ==============\n",
      "============== Pattern 161 ==============\n",
      "============== Pattern 162 ==============\n",
      "============== Pattern 163 ==============\n",
      "============== Pattern 164 ==============\n",
      "============== Pattern 165 ==============\n",
      "============== Pattern 166 ==============\n",
      "============== Pattern 167 ==============\n",
      "============== Pattern 168 ==============\n",
      "============== Pattern 169 ==============\n",
      "============== Pattern 170 ==============\n",
      "============== Pattern 171 ==============\n",
      "============== Pattern 172 ==============\n",
      "============== Pattern 173 ==============\n",
      "============== Pattern 174 ==============\n",
      "============== Pattern 175 ==============\n",
      "============== Pattern 176 ==============\n",
      "============== Pattern 177 ==============\n",
      "============== Pattern 178 ==============\n",
      "============== Pattern 179 ==============\n",
      "============== Pattern 180 ==============\n",
      "============== Pattern 181 ==============\n",
      "============== Pattern 182 ==============\n",
      "============== Pattern 183 ==============\n",
      "============== Pattern 184 ==============\n",
      "============== Pattern 185 ==============\n",
      "============== Pattern 186 ==============\n",
      "============== Pattern 187 ==============\n",
      "============== Pattern 188 ==============\n",
      "============== Pattern 189 ==============\n",
      "============== Pattern 190 ==============\n",
      "============== Pattern 191 ==============\n",
      "============== Pattern 192 ==============\n",
      "============== Pattern 193 ==============\n",
      "============== Pattern 194 ==============\n",
      "============== Pattern 195 ==============\n",
      "============== Pattern 196 ==============\n",
      "============== Pattern 197 ==============\n",
      "============== Pattern 198 ==============\n",
      "============== Pattern 199 ==============\n",
      "============== Pattern 200 ==============\n",
      "============== Pattern 201 ==============\n",
      "============== Pattern 202 ==============\n",
      "============== Pattern 203 ==============\n",
      "============== Pattern 204 ==============\n",
      "============== Pattern 205 ==============\n",
      "============== Pattern 206 ==============\n",
      "============== Pattern 207 ==============\n",
      "============== Pattern 208 ==============\n",
      "============== Pattern 209 ==============\n",
      "============== Pattern 210 ==============\n",
      "============== Pattern 211 ==============\n",
      "============== Pattern 212 ==============\n",
      "============== Pattern 213 ==============\n",
      "============== Pattern 214 ==============\n",
      "============== Pattern 215 ==============\n",
      "============== Pattern 216 ==============\n",
      "============== Pattern 217 ==============\n",
      "============== Pattern 218 ==============\n",
      "============== Pattern 219 ==============\n",
      "============== Pattern 220 ==============\n",
      "============== Pattern 221 ==============\n",
      "============== Pattern 222 ==============\n",
      "============== Pattern 223 ==============\n",
      "============== Pattern 224 ==============\n",
      "============== Pattern 225 ==============\n",
      "============== Pattern 226 ==============\n",
      "============== Pattern 227 ==============\n",
      "============== Pattern 228 ==============\n",
      "============== Pattern 229 ==============\n",
      "============== Pattern 230 ==============\n",
      "============== Pattern 231 ==============\n",
      "============== Pattern 232 ==============\n",
      "============== Pattern 233 ==============\n",
      "============== Pattern 234 ==============\n",
      "============== Pattern 235 ==============\n",
      "============== Pattern 236 ==============\n",
      "============== Pattern 237 ==============\n",
      "============== Pattern 238 ==============\n",
      "============== Pattern 239 ==============\n",
      "============== Pattern 240 ==============\n",
      "============== Pattern 241 ==============\n",
      "============== Pattern 242 ==============\n",
      "============== Pattern 243 ==============\n",
      "============== Pattern 244 ==============\n",
      "============== Pattern 245 ==============\n",
      "============== Pattern 246 ==============\n",
      "============== Pattern 247 ==============\n",
      "============== Pattern 248 ==============\n",
      "============== Pattern 249 ==============\n",
      "============== Pattern 250 ==============\n",
      "============== Pattern 251 ==============\n",
      "============== Pattern 252 ==============\n",
      "============== Pattern 253 ==============\n",
      "============== Pattern 254 ==============\n",
      "============== Pattern 255 ==============\n",
      "============== Pattern 256 ==============\n",
      "============== Pattern 257 ==============\n",
      "136\n",
      "============== Pattern 258 ==============\n",
      "============== Pattern 259 ==============\n",
      "============== Pattern 260 ==============\n",
      "============== Pattern 261 ==============\n",
      "============== Pattern 262 ==============\n",
      "============== Pattern 263 ==============\n",
      "============== Pattern 264 ==============\n",
      "============== Pattern 265 ==============\n",
      "============== Pattern 266 ==============\n",
      "============== Pattern 267 ==============\n",
      "============== Pattern 268 ==============\n",
      "============== Pattern 269 ==============\n",
      "============== Pattern 270 ==============\n",
      "============== Pattern 271 ==============\n",
      "============== Pattern 272 ==============\n",
      "============== Pattern 273 ==============\n",
      "============== Pattern 274 ==============\n",
      "============== Pattern 275 ==============\n",
      "============== Pattern 276 ==============\n",
      "============== Pattern 277 ==============\n",
      "============== Pattern 278 ==============\n",
      "============== Pattern 279 ==============\n",
      "============== Pattern 280 ==============\n",
      "============== Pattern 281 ==============\n",
      "============== Pattern 282 ==============\n",
      "============== Pattern 283 ==============\n",
      "============== Pattern 284 ==============\n",
      "============== Pattern 285 ==============\n",
      "============== Pattern 286 ==============\n",
      "============== Pattern 287 ==============\n",
      "============== Pattern 288 ==============\n",
      "============== Pattern 289 ==============\n",
      "============== Pattern 290 ==============\n",
      "============== Pattern 291 ==============\n",
      "============== Pattern 292 ==============\n",
      "============== Pattern 293 ==============\n",
      "============== Pattern 294 ==============\n",
      "============== Pattern 295 ==============\n",
      "============== Pattern 296 ==============\n",
      "============== Pattern 297 ==============\n",
      "============== Pattern 298 ==============\n",
      "============== Pattern 299 ==============\n",
      "============== Pattern 300 ==============\n",
      "============== Pattern 301 ==============\n",
      "============== Pattern 302 ==============\n",
      "============== Pattern 303 ==============\n",
      "============== Pattern 304 ==============\n",
      "============== Pattern 305 ==============\n",
      "============== Pattern 306 ==============\n",
      "============== Pattern 307 ==============\n",
      "============== Pattern 308 ==============\n",
      "============== Pattern 309 ==============\n",
      "============== Pattern 310 ==============\n",
      "============== Pattern 311 ==============\n",
      "============== Pattern 312 ==============\n",
      "============== Pattern 313 ==============\n",
      "============== Pattern 314 ==============\n",
      "============== Pattern 315 ==============\n",
      "============== Pattern 316 ==============\n",
      "============== Pattern 317 ==============\n",
      "============== Pattern 318 ==============\n",
      "============== Pattern 319 ==============\n",
      "============== Pattern 320 ==============\n",
      "============== Pattern 321 ==============\n",
      "============== Pattern 322 ==============\n",
      "============== Pattern 323 ==============\n",
      "============== Pattern 324 ==============\n",
      "============== Pattern 325 ==============\n",
      "============== Pattern 326 ==============\n",
      "============== Pattern 327 ==============\n",
      "============== Pattern 328 ==============\n",
      "============== Pattern 329 ==============\n",
      "============== Pattern 330 ==============\n",
      "============== Pattern 331 ==============\n",
      "============== Pattern 332 ==============\n",
      "============== Pattern 333 ==============\n",
      "============== Pattern 334 ==============\n",
      "============== Pattern 335 ==============\n",
      "============== Pattern 336 ==============\n",
      "============== Pattern 337 ==============\n",
      "============== Pattern 338 ==============\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Pattern 339 ==============\n",
      "============== Pattern 340 ==============\n",
      "============== Pattern 341 ==============\n",
      "============== Pattern 342 ==============\n",
      "============== Pattern 343 ==============\n",
      "============== Pattern 344 ==============\n",
      "============== Pattern 345 ==============\n",
      "============== Pattern 346 ==============\n",
      "============== Pattern 347 ==============\n",
      "============== Pattern 348 ==============\n",
      "============== Pattern 349 ==============\n",
      "============== Pattern 350 ==============\n",
      "============== Pattern 351 ==============\n",
      "============== Pattern 352 ==============\n",
      "============== Pattern 353 ==============\n",
      "============== Pattern 354 ==============\n",
      "============== Pattern 355 ==============\n",
      "============== Pattern 356 ==============\n",
      "============== Pattern 357 ==============\n",
      "============== Pattern 358 ==============\n",
      "============== Pattern 359 ==============\n",
      "============== Pattern 360 ==============\n",
      "============== Pattern 361 ==============\n",
      "============== Pattern 362 ==============\n",
      "============== Pattern 363 ==============\n",
      "============== Pattern 364 ==============\n",
      "============== Pattern 365 ==============\n",
      "============== Pattern 366 ==============\n",
      "============== Pattern 367 ==============\n",
      "============== Pattern 368 ==============\n",
      "============== Pattern 369 ==============\n",
      "============== Pattern 370 ==============\n",
      "============== Pattern 371 ==============\n",
      "============== Pattern 372 ==============\n",
      "============== Pattern 373 ==============\n",
      "============== Pattern 374 ==============\n",
      "============== Pattern 375 ==============\n",
      "============== Pattern 376 ==============\n",
      "============== Pattern 377 ==============\n",
      "============== Pattern 378 ==============\n",
      "============== Pattern 379 ==============\n",
      "============== Pattern 380 ==============\n",
      "============== Pattern 381 ==============\n",
      "============== Pattern 382 ==============\n",
      "============== Pattern 383 ==============\n",
      "============== Pattern 384 ==============\n",
      "============== Pattern 385 ==============\n",
      "============== Pattern 386 ==============\n",
      "============== Pattern 387 ==============\n",
      "============== Pattern 388 ==============\n",
      "============== Pattern 389 ==============\n",
      "============== Pattern 390 ==============\n",
      "============== Pattern 391 ==============\n",
      "============== Pattern 392 ==============\n",
      "3\n",
      "============== Pattern 393 ==============\n",
      "============== Pattern 394 ==============\n",
      "============== Pattern 395 ==============\n",
      "============== Pattern 396 ==============\n",
      "============== Pattern 397 ==============\n",
      "============== Pattern 398 ==============\n",
      "============== Pattern 399 ==============\n",
      "============== Pattern 400 ==============\n",
      "============== Pattern 401 ==============\n",
      "3\n",
      "============== Pattern 402 ==============\n",
      "============== Pattern 403 ==============\n",
      "============== Pattern 404 ==============\n",
      "============== Pattern 405 ==============\n",
      "============== Pattern 406 ==============\n",
      "============== Pattern 407 ==============\n",
      "============== Pattern 408 ==============\n",
      "============== Pattern 409 ==============\n",
      "============== Pattern 410 ==============\n",
      "============== Pattern 411 ==============\n",
      "============== Pattern 412 ==============\n",
      "============== Pattern 413 ==============\n",
      "============== Pattern 414 ==============\n",
      "============== Pattern 415 ==============\n",
      "============== Pattern 416 ==============\n",
      "============== Pattern 417 ==============\n",
      "============== Pattern 418 ==============\n",
      "============== Pattern 419 ==============\n",
      "============== Pattern 420 ==============\n",
      "2\n",
      "============== Pattern 421 ==============\n",
      "============== Pattern 422 ==============\n",
      "============== Pattern 423 ==============\n",
      "============== Pattern 424 ==============\n",
      "============== Pattern 425 ==============\n",
      "============== Pattern 426 ==============\n",
      "============== Pattern 427 ==============\n",
      "============== Pattern 428 ==============\n",
      "============== Pattern 429 ==============\n",
      "============== Pattern 430 ==============\n",
      "============== Pattern 431 ==============\n",
      "============== Pattern 432 ==============\n",
      "============== Pattern 433 ==============\n",
      "============== Pattern 434 ==============\n",
      "============== Pattern 435 ==============\n",
      "============== Pattern 436 ==============\n",
      "============== Pattern 437 ==============\n",
      "============== Pattern 438 ==============\n",
      "============== Pattern 439 ==============\n",
      "============== Pattern 440 ==============\n",
      "============== Pattern 441 ==============\n",
      "============== Pattern 442 ==============\n",
      "============== Pattern 443 ==============\n",
      "============== Pattern 444 ==============\n",
      "============== Pattern 445 ==============\n",
      "============== Pattern 446 ==============\n",
      "============== Pattern 447 ==============\n",
      "============== Pattern 448 ==============\n",
      "============== Pattern 449 ==============\n",
      "============== Pattern 450 ==============\n",
      "============== Pattern 451 ==============\n",
      "============== Pattern 452 ==============\n",
      "============== Pattern 453 ==============\n",
      "============== Pattern 454 ==============\n",
      "1\n",
      "============== Pattern 455 ==============\n",
      "============== Pattern 456 ==============\n",
      "============== Pattern 457 ==============\n",
      "============== Pattern 458 ==============\n",
      "============== Pattern 459 ==============\n",
      "============== Pattern 460 ==============\n",
      "============== Pattern 461 ==============\n",
      "============== Pattern 462 ==============\n",
      "============== Pattern 463 ==============\n",
      "============== Pattern 464 ==============\n",
      "============== Pattern 465 ==============\n",
      "============== Pattern 466 ==============\n",
      "============== Pattern 467 ==============\n",
      "============== Pattern 468 ==============\n",
      "============== Pattern 469 ==============\n",
      "============== Pattern 470 ==============\n",
      "============== Pattern 471 ==============\n",
      "============== Pattern 472 ==============\n",
      "============== Pattern 473 ==============\n",
      "============== Pattern 474 ==============\n",
      "============== Pattern 475 ==============\n",
      "============== Pattern 476 ==============\n",
      "============== Pattern 477 ==============\n",
      "============== Pattern 478 ==============\n",
      "============== Pattern 479 ==============\n",
      "============== Pattern 480 ==============\n",
      "============== Pattern 481 ==============\n",
      "============== Pattern 482 ==============\n",
      "============== Pattern 483 ==============\n",
      "============== Pattern 484 ==============\n",
      "============== Pattern 485 ==============\n",
      "============== Pattern 486 ==============\n",
      "============== Pattern 487 ==============\n",
      "============== Pattern 488 ==============\n",
      "============== Pattern 489 ==============\n",
      "============== Pattern 490 ==============\n",
      "============== Pattern 491 ==============\n",
      "============== Pattern 492 ==============\n",
      "============== Pattern 493 ==============\n",
      "============== Pattern 494 ==============\n",
      "============== Pattern 495 ==============\n",
      "============== Pattern 496 ==============\n",
      "============== Pattern 497 ==============\n",
      "============== Pattern 498 ==============\n",
      "============== Pattern 499 ==============\n",
      "============== Pattern 500 ==============\n",
      "============== Pattern 501 ==============\n",
      "============== Pattern 502 ==============\n",
      "============== Pattern 503 ==============\n",
      "============== Pattern 504 ==============\n",
      "============== Pattern 505 ==============\n",
      "============== Pattern 506 ==============\n",
      "============== Pattern 507 ==============\n",
      "============== Pattern 508 ==============\n",
      "============== Pattern 509 ==============\n",
      "============== Pattern 510 ==============\n",
      "============== Pattern 511 ==============\n",
      "============== Pattern 512 ==============\n",
      "============== Pattern 513 ==============\n",
      "============== Pattern 514 ==============\n",
      "============== Pattern 515 ==============\n",
      "============== Pattern 516 ==============\n",
      "============== Pattern 517 ==============\n",
      "============== Pattern 518 ==============\n",
      "============== Pattern 519 ==============\n",
      "============== Pattern 520 ==============\n",
      "============== Pattern 521 ==============\n",
      "============== Pattern 522 ==============\n",
      "============== Pattern 523 ==============\n",
      "============== Pattern 524 ==============\n",
      "============== Pattern 525 ==============\n",
      "============== Pattern 526 ==============\n",
      "============== Pattern 527 ==============\n",
      "============== Pattern 528 ==============\n",
      "============== Pattern 529 ==============\n",
      "============== Pattern 530 ==============\n",
      "============== Pattern 531 ==============\n",
      "============== Pattern 532 ==============\n",
      "============== Pattern 533 ==============\n",
      "============== Pattern 534 ==============\n",
      "============== Pattern 535 ==============\n",
      "============== Pattern 536 ==============\n",
      "============== Pattern 537 ==============\n",
      "============== Pattern 538 ==============\n",
      "============== Pattern 539 ==============\n",
      "============== Pattern 540 ==============\n",
      "============== Pattern 541 ==============\n",
      "============== Pattern 542 ==============\n",
      "============== Pattern 543 ==============\n",
      "============== Pattern 544 ==============\n",
      "============== Pattern 545 ==============\n",
      "============== Pattern 546 ==============\n",
      "============== Pattern 547 ==============\n",
      "============== Pattern 548 ==============\n",
      "============== Pattern 549 ==============\n",
      "============== Pattern 550 ==============\n",
      "============== Pattern 551 ==============\n",
      "============== Pattern 552 ==============\n",
      "============== Pattern 553 ==============\n",
      "============== Pattern 554 ==============\n",
      "============== Pattern 555 ==============\n",
      "============== Pattern 556 ==============\n",
      "============== Pattern 557 ==============\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Pattern 558 ==============\n",
      "============== Pattern 559 ==============\n",
      "============== Pattern 560 ==============\n",
      "============== Pattern 561 ==============\n",
      "============== Pattern 562 ==============\n",
      "============== Pattern 563 ==============\n",
      "============== Pattern 564 ==============\n",
      "============== Pattern 565 ==============\n",
      "============== Pattern 566 ==============\n",
      "============== Pattern 567 ==============\n",
      "============== Pattern 568 ==============\n",
      "============== Pattern 569 ==============\n",
      "============== Pattern 570 ==============\n",
      "============== Pattern 571 ==============\n",
      "============== Pattern 572 ==============\n",
      "============== Pattern 573 ==============\n",
      "============== Pattern 574 ==============\n",
      "============== Pattern 575 ==============\n",
      "============== Pattern 576 ==============\n",
      "============== Pattern 577 ==============\n",
      "============== Pattern 578 ==============\n",
      "============== Pattern 579 ==============\n",
      "============== Pattern 580 ==============\n",
      "============== Pattern 581 ==============\n",
      "============== Pattern 582 ==============\n",
      "============== Pattern 583 ==============\n",
      "============== Pattern 584 ==============\n",
      "============== Pattern 585 ==============\n",
      "============== Pattern 586 ==============\n",
      "============== Pattern 587 ==============\n",
      "============== Pattern 588 ==============\n",
      "============== Pattern 589 ==============\n",
      "============== Pattern 590 ==============\n",
      "============== Pattern 591 ==============\n",
      "============== Pattern 592 ==============\n",
      "============== Pattern 593 ==============\n",
      "============== Pattern 594 ==============\n",
      "============== Pattern 595 ==============\n",
      "============== Pattern 596 ==============\n",
      "============== Pattern 597 ==============\n",
      "============== Pattern 598 ==============\n",
      "============== Pattern 599 ==============\n",
      "============== Pattern 600 ==============\n",
      "============== Pattern 601 ==============\n",
      "============== Pattern 602 ==============\n",
      "============== Pattern 603 ==============\n",
      "============== Pattern 604 ==============\n",
      "============== Pattern 605 ==============\n",
      "============== Pattern 606 ==============\n",
      "============== Pattern 607 ==============\n",
      "============== Pattern 608 ==============\n",
      "============== Pattern 609 ==============\n",
      "============== Pattern 610 ==============\n",
      "============== Pattern 611 ==============\n",
      "============== Pattern 612 ==============\n",
      "============== Pattern 613 ==============\n",
      "============== Pattern 614 ==============\n",
      "============== Pattern 615 ==============\n",
      "============== Pattern 616 ==============\n",
      "============== Pattern 617 ==============\n",
      "============== Pattern 618 ==============\n",
      "============== Pattern 619 ==============\n",
      "============== Pattern 620 ==============\n",
      "============== Pattern 621 ==============\n",
      "============== Pattern 622 ==============\n",
      "============== Pattern 623 ==============\n",
      "============== Pattern 624 ==============\n",
      "============== Pattern 625 ==============\n",
      "============== Pattern 626 ==============\n",
      "============== Pattern 627 ==============\n",
      "============== Pattern 628 ==============\n",
      "============== Pattern 629 ==============\n",
      "============== Pattern 630 ==============\n",
      "============== Pattern 631 ==============\n",
      "============== Pattern 632 ==============\n",
      "============== Pattern 633 ==============\n",
      "============== Pattern 634 ==============\n",
      "============== Pattern 635 ==============\n",
      "============== Pattern 636 ==============\n",
      "============== Pattern 637 ==============\n",
      "============== Pattern 638 ==============\n",
      "============== Pattern 639 ==============\n",
      "============== Pattern 640 ==============\n",
      "============== Pattern 641 ==============\n",
      "============== Pattern 642 ==============\n",
      "============== Pattern 643 ==============\n",
      "============== Pattern 644 ==============\n",
      "============== Pattern 645 ==============\n",
      "============== Pattern 646 ==============\n",
      "============== Pattern 647 ==============\n",
      "============== Pattern 648 ==============\n",
      "============== Pattern 649 ==============\n",
      "============== Pattern 650 ==============\n",
      "============== Pattern 651 ==============\n",
      "============== Pattern 652 ==============\n",
      "============== Pattern 653 ==============\n",
      "============== Pattern 654 ==============\n",
      "============== Pattern 655 ==============\n",
      "============== Pattern 656 ==============\n",
      "============== Pattern 657 ==============\n",
      "============== Pattern 658 ==============\n",
      "============== Pattern 659 ==============\n",
      "============== Pattern 660 ==============\n",
      "============== Pattern 661 ==============\n",
      "============== Pattern 662 ==============\n",
      "============== Pattern 663 ==============\n",
      "============== Pattern 664 ==============\n",
      "============== Pattern 665 ==============\n",
      "============== Pattern 666 ==============\n",
      "============== Pattern 667 ==============\n",
      "============== Pattern 668 ==============\n",
      "============== Pattern 669 ==============\n",
      "============== Pattern 670 ==============\n",
      "============== Pattern 671 ==============\n",
      "============== Pattern 672 ==============\n",
      "============== Pattern 673 ==============\n",
      "============== Pattern 674 ==============\n",
      "============== Pattern 675 ==============\n",
      "============== Pattern 676 ==============\n",
      "============== Pattern 677 ==============\n",
      "============== Pattern 678 ==============\n",
      "============== Pattern 679 ==============\n",
      "============== Pattern 680 ==============\n",
      "18\n",
      "============== Pattern 681 ==============\n",
      "1\n",
      "============== Pattern 682 ==============\n",
      "============== Pattern 683 ==============\n",
      "============== Pattern 684 ==============\n",
      "15\n",
      "============== Pattern 685 ==============\n",
      "9\n",
      "============== Pattern 686 ==============\n",
      "5\n",
      "============== Pattern 687 ==============\n",
      "============== Pattern 688 ==============\n",
      "============== Pattern 689 ==============\n",
      "============== Pattern 690 ==============\n",
      "============== Pattern 691 ==============\n",
      "============== Pattern 692 ==============\n",
      "============== Pattern 693 ==============\n",
      "============== Pattern 694 ==============\n",
      "============== Pattern 695 ==============\n",
      "============== Pattern 696 ==============\n",
      "============== Pattern 697 ==============\n",
      "============== Pattern 698 ==============\n",
      "============== Pattern 699 ==============\n",
      "============== Pattern 700 ==============\n",
      "============== Pattern 701 ==============\n",
      "============== Pattern 702 ==============\n",
      "============== Pattern 703 ==============\n",
      "============== Pattern 704 ==============\n",
      "============== Pattern 705 ==============\n",
      "============== Pattern 706 ==============\n",
      "============== Pattern 707 ==============\n",
      "============== Pattern 708 ==============\n",
      "============== Pattern 709 ==============\n",
      "============== Pattern 710 ==============\n",
      "============== Pattern 711 ==============\n",
      "============== Pattern 712 ==============\n",
      "============== Pattern 713 ==============\n",
      "============== Pattern 714 ==============\n",
      "============== Pattern 715 ==============\n",
      "============== Pattern 716 ==============\n",
      "============== Pattern 717 ==============\n",
      "============== Pattern 718 ==============\n",
      "============== Pattern 719 ==============\n",
      "============== Pattern 720 ==============\n",
      "============== Pattern 721 ==============\n",
      "============== Pattern 722 ==============\n",
      "============== Pattern 723 ==============\n",
      "============== Pattern 724 ==============\n",
      "============== Pattern 725 ==============\n",
      "============== Pattern 726 ==============\n",
      "============== Pattern 727 ==============\n",
      "============== Pattern 728 ==============\n",
      "============== Pattern 729 ==============\n",
      "============== Pattern 730 ==============\n",
      "============== Pattern 731 ==============\n",
      "============== Pattern 732 ==============\n",
      "============== Pattern 733 ==============\n",
      "============== Pattern 734 ==============\n",
      "============== Pattern 735 ==============\n",
      "============== Pattern 736 ==============\n",
      "============== Pattern 737 ==============\n",
      "============== Pattern 738 ==============\n",
      "============== Pattern 739 ==============\n",
      "============== Pattern 740 ==============\n",
      "============== Pattern 741 ==============\n",
      "============== Pattern 742 ==============\n",
      "============== Pattern 743 ==============\n",
      "============== Pattern 744 ==============\n",
      "============== Pattern 745 ==============\n",
      "============== Pattern 746 ==============\n",
      "============== Pattern 747 ==============\n",
      "============== Pattern 748 ==============\n",
      "============== Pattern 749 ==============\n",
      "============== Pattern 750 ==============\n",
      "============== Pattern 751 ==============\n",
      "============== Pattern 752 ==============\n",
      "5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Pattern 753 ==============\n",
      "============== Pattern 754 ==============\n",
      "============== Pattern 755 ==============\n",
      "============== Pattern 756 ==============\n",
      "============== Pattern 757 ==============\n",
      "============== Pattern 758 ==============\n",
      "============== Pattern 759 ==============\n",
      "============== Pattern 760 ==============\n",
      "============== Pattern 761 ==============\n",
      "============== Pattern 762 ==============\n",
      "============== Pattern 763 ==============\n",
      "============== Pattern 764 ==============\n",
      "============== Pattern 765 ==============\n",
      "============== Pattern 766 ==============\n",
      "============== Pattern 767 ==============\n",
      "============== Pattern 768 ==============\n",
      "============== Pattern 769 ==============\n",
      "============== Pattern 770 ==============\n",
      "============== Pattern 771 ==============\n",
      "============== Pattern 772 ==============\n",
      "============== Pattern 773 ==============\n",
      "============== Pattern 774 ==============\n",
      "============== Pattern 775 ==============\n",
      "============== Pattern 776 ==============\n",
      "============== Pattern 777 ==============\n",
      "============== Pattern 778 ==============\n",
      "============== Pattern 779 ==============\n",
      "============== Pattern 780 ==============\n",
      "============== Pattern 781 ==============\n",
      "============== Pattern 782 ==============\n",
      "============== Pattern 783 ==============\n",
      "============== Pattern 784 ==============\n",
      "============== Pattern 785 ==============\n",
      "============== Pattern 786 ==============\n",
      "============== Pattern 787 ==============\n",
      "============== Pattern 788 ==============\n",
      "============== Pattern 789 ==============\n",
      "============== Pattern 790 ==============\n",
      "============== Pattern 791 ==============\n",
      "============== Pattern 792 ==============\n",
      "============== Pattern 793 ==============\n",
      "============== Pattern 794 ==============\n",
      "============== Pattern 795 ==============\n",
      "============== Pattern 796 ==============\n",
      "============== Pattern 797 ==============\n",
      "============== Pattern 798 ==============\n",
      "============== Pattern 799 ==============\n",
      "============== Pattern 800 ==============\n",
      "============== Pattern 801 ==============\n",
      "============== Pattern 802 ==============\n",
      "============== Pattern 803 ==============\n",
      "============== Pattern 804 ==============\n",
      "============== Pattern 805 ==============\n",
      "============== Pattern 806 ==============\n",
      "============== Pattern 807 ==============\n",
      "============== Pattern 808 ==============\n",
      "============== Pattern 809 ==============\n",
      "============== Pattern 810 ==============\n",
      "============== Pattern 811 ==============\n",
      "============== Pattern 812 ==============\n",
      "============== Pattern 813 ==============\n",
      "============== Pattern 814 ==============\n",
      "============== Pattern 815 ==============\n",
      "============== Pattern 816 ==============\n",
      "============== Pattern 817 ==============\n",
      "============== Pattern 818 ==============\n",
      "============== Pattern 819 ==============\n",
      "============== Pattern 820 ==============\n",
      "============== Pattern 821 ==============\n",
      "============== Pattern 822 ==============\n",
      "============== Pattern 823 ==============\n",
      "============== Pattern 824 ==============\n",
      "============== Pattern 825 ==============\n",
      "============== Pattern 826 ==============\n",
      "============== Pattern 827 ==============\n",
      "============== Pattern 828 ==============\n",
      "============== Pattern 829 ==============\n",
      "============== Pattern 830 ==============\n",
      "============== Pattern 831 ==============\n",
      "============== Pattern 832 ==============\n",
      "============== Pattern 833 ==============\n",
      "============== Pattern 834 ==============\n",
      "============== Pattern 835 ==============\n",
      "============== Pattern 836 ==============\n",
      "============== Pattern 837 ==============\n",
      "============== Pattern 838 ==============\n",
      "============== Pattern 839 ==============\n",
      "============== Pattern 840 ==============\n",
      "============== Pattern 841 ==============\n",
      "============== Pattern 842 ==============\n",
      "============== Pattern 843 ==============\n",
      "============== Pattern 844 ==============\n",
      "============== Pattern 845 ==============\n",
      "============== Pattern 846 ==============\n",
      "============== Pattern 847 ==============\n",
      "============== Pattern 848 ==============\n",
      "============== Pattern 849 ==============\n",
      "============== Pattern 850 ==============\n",
      "============== Pattern 851 ==============\n",
      "============== Pattern 852 ==============\n",
      "============== Pattern 853 ==============\n",
      "============== Pattern 854 ==============\n",
      "============== Pattern 855 ==============\n",
      "============== Pattern 856 ==============\n",
      "============== Pattern 857 ==============\n",
      "============== Pattern 858 ==============\n",
      "============== Pattern 859 ==============\n",
      "============== Pattern 860 ==============\n",
      "============== Pattern 861 ==============\n",
      "============== Pattern 862 ==============\n",
      "============== Pattern 863 ==============\n",
      "============== Pattern 864 ==============\n",
      "============== Pattern 865 ==============\n",
      "============== Pattern 866 ==============\n",
      "============== Pattern 867 ==============\n",
      "============== Pattern 868 ==============\n",
      "============== Pattern 869 ==============\n",
      "============== Pattern 870 ==============\n",
      "============== Pattern 871 ==============\n",
      "============== Pattern 872 ==============\n",
      "============== Pattern 873 ==============\n",
      "============== Pattern 874 ==============\n",
      "============== Pattern 875 ==============\n",
      "============== Pattern 876 ==============\n",
      "============== Pattern 877 ==============\n",
      "============== Pattern 878 ==============\n",
      "============== Pattern 879 ==============\n",
      "============== Pattern 880 ==============\n",
      "============== Pattern 881 ==============\n",
      "============== Pattern 882 ==============\n",
      "============== Pattern 883 ==============\n",
      "============== Pattern 884 ==============\n",
      "============== Pattern 885 ==============\n",
      "============== Pattern 886 ==============\n",
      "============== Pattern 887 ==============\n",
      "============== Pattern 888 ==============\n",
      "============== Pattern 889 ==============\n",
      "============== Pattern 890 ==============\n",
      "============== Pattern 891 ==============\n",
      "============== Pattern 892 ==============\n",
      "============== Pattern 893 ==============\n",
      "============== Pattern 894 ==============\n",
      "============== Pattern 895 ==============\n",
      "============== Pattern 896 ==============\n",
      "============== Pattern 897 ==============\n",
      "============== Pattern 898 ==============\n",
      "============== Pattern 899 ==============\n",
      "============== Pattern 900 ==============\n",
      "============== Pattern 901 ==============\n",
      "============== Pattern 902 ==============\n",
      "============== Pattern 903 ==============\n",
      "============== Pattern 904 ==============\n",
      "============== Pattern 905 ==============\n",
      "============== Pattern 906 ==============\n",
      "============== Pattern 907 ==============\n",
      "============== Pattern 908 ==============\n",
      "============== Pattern 909 ==============\n",
      "============== Pattern 910 ==============\n",
      "============== Pattern 911 ==============\n",
      "============== Pattern 912 ==============\n",
      "============== Pattern 913 ==============\n",
      "============== Pattern 914 ==============\n",
      "============== Pattern 915 ==============\n",
      "============== Pattern 916 ==============\n",
      "============== Pattern 917 ==============\n",
      "============== Pattern 918 ==============\n",
      "============== Pattern 919 ==============\n",
      "============== Pattern 920 ==============\n",
      "============== Pattern 921 ==============\n",
      "============== Pattern 922 ==============\n",
      "============== Pattern 923 ==============\n",
      "============== Pattern 924 ==============\n",
      "============== Pattern 925 ==============\n",
      "============== Pattern 926 ==============\n",
      "============== Pattern 927 ==============\n",
      "============== Pattern 928 ==============\n",
      "============== Pattern 929 ==============\n",
      "============== Pattern 930 ==============\n",
      "============== Pattern 931 ==============\n",
      "============== Pattern 932 ==============\n",
      "============== Pattern 933 ==============\n",
      "============== Pattern 934 ==============\n",
      "============== Pattern 935 ==============\n",
      "============== Pattern 936 ==============\n",
      "============== Pattern 937 ==============\n",
      "============== Pattern 938 ==============\n",
      "============== Pattern 939 ==============\n",
      "============== Pattern 940 ==============\n",
      "============== Pattern 941 ==============\n",
      "============== Pattern 942 ==============\n",
      "============== Pattern 943 ==============\n",
      "============== Pattern 944 ==============\n",
      "============== Pattern 945 ==============\n",
      "============== Pattern 946 ==============\n",
      "============== Pattern 947 ==============\n",
      "============== Pattern 948 ==============\n",
      "============== Pattern 949 ==============\n",
      "============== Pattern 950 ==============\n",
      "============== Pattern 951 ==============\n",
      "============== Pattern 952 ==============\n",
      "============== Pattern 953 ==============\n",
      "============== Pattern 954 ==============\n",
      "============== Pattern 955 ==============\n",
      "============== Pattern 956 ==============\n",
      "============== Pattern 957 ==============\n",
      "============== Pattern 958 ==============\n",
      "============== Pattern 959 ==============\n",
      "============== Pattern 960 ==============\n",
      "============== Pattern 961 ==============\n",
      "============== Pattern 962 ==============\n",
      "============== Pattern 963 ==============\n",
      "============== Pattern 964 ==============\n",
      "============== Pattern 965 ==============\n",
      "============== Pattern 966 ==============\n",
      "============== Pattern 967 ==============\n",
      "============== Pattern 968 ==============\n",
      "============== Pattern 969 ==============\n",
      "============== Pattern 970 ==============\n",
      "============== Pattern 971 ==============\n",
      "============== Pattern 972 ==============\n",
      "============== Pattern 973 ==============\n",
      "============== Pattern 974 ==============\n",
      "============== Pattern 975 ==============\n",
      "============== Pattern 976 ==============\n",
      "============== Pattern 977 ==============\n",
      "============== Pattern 978 ==============\n",
      "============== Pattern 979 ==============\n",
      "============== Pattern 980 ==============\n",
      "============== Pattern 981 ==============\n",
      "============== Pattern 982 ==============\n",
      "============== Pattern 983 ==============\n",
      "============== Pattern 984 ==============\n",
      "============== Pattern 985 ==============\n",
      "============== Pattern 986 ==============\n",
      "============== Pattern 987 ==============\n",
      "============== Pattern 988 ==============\n",
      "============== Pattern 989 ==============\n",
      "============== Pattern 990 ==============\n",
      "============== Pattern 991 ==============\n",
      "============== Pattern 992 ==============\n",
      "============== Pattern 993 ==============\n",
      "============== Pattern 994 ==============\n",
      "============== Pattern 995 ==============\n",
      "============== Pattern 996 ==============\n",
      "============== Pattern 997 ==============\n",
      "============== Pattern 998 ==============\n",
      "============== Pattern 999 ==============\n",
      "============== Pattern 1000 ==============\n",
      "============== Pattern 1001 ==============\n",
      "============== Pattern 1002 ==============\n",
      "============== Pattern 1003 ==============\n",
      "============== Pattern 1004 ==============\n",
      "============== Pattern 1005 ==============\n",
      "============== Pattern 1006 ==============\n",
      "============== Pattern 1007 ==============\n",
      "============== Pattern 1008 ==============\n",
      "============== Pattern 1009 ==============\n",
      "============== Pattern 1010 ==============\n",
      "============== Pattern 1011 ==============\n",
      "============== Pattern 1012 ==============\n",
      "============== Pattern 1013 ==============\n",
      "============== Pattern 1014 ==============\n",
      "============== Pattern 1015 ==============\n",
      "============== Pattern 1016 ==============\n",
      "============== Pattern 1017 ==============\n",
      "============== Pattern 1018 ==============\n",
      "============== Pattern 1019 ==============\n",
      "============== Pattern 1020 ==============\n",
      "============== Pattern 1021 ==============\n",
      "============== Pattern 1022 ==============\n",
      "============== Pattern 1023 ==============\n",
      "============== Pattern 1024 ==============\n",
      "Average comprehensibility: 50.94921875\n",
      "std comprehensibility: 2.7834189883394194\n",
      "var comprehensibility: 7.7474212646484375\n",
      "minimum comprehensibility: 44\n",
      "maximum comprehensibility: 60\n"
     ]
    }
   ],
   "source": [
    "attr_names = dataset.items\n",
    "\n",
    "# print(attr_names)\n",
    "leaves = root.get_leaves()\n",
    "sum_comprehensibility = 0\n",
    "comprehensibilities = []\n",
    "for pattern_counter, leaf in enumerate(leaves):\n",
    "    leaf.reset_path()\n",
    "    leaf.tighten_with_accumulated_samples()\n",
    "    conds = leaf.get_path_conditions(attr_names)\n",
    "    print(f\"============== Pattern {pattern_counter + 1} ==============\")\n",
    "    comprehensibilities.append(sum([cond.comprehensibility for cond in conds]))\n",
    "    \n",
    "print(f\"Average comprehensibility: {np.mean(comprehensibilities)}\")\n",
    "print(f\"std comprehensibility: {np.std(comprehensibilities)}\")\n",
    "print(f\"var comprehensibility: {np.var(comprehensibilities)}\")\n",
    "print(f\"minimum comprehensibility: {np.min(comprehensibilities)}\")\n",
    "print(f\"maximum comprehensibility: {np.max(comprehensibilities)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
