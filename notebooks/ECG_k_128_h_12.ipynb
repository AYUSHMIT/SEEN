{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib widget\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import torch.nn as nn\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from stream_generators.mit_bih import MITBIH\n",
    "from utils.MatplotlibUtils import reduce_dims_and_plot\n",
    "from network.auto_encoder import AutoEncoder\n",
    "from losses.knn_loss import KNNLoss, ClassificationKNNLoss\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from soft_decision_tree.sdt_model import SDT\n",
    "from sklearn.metrics import davies_bouldin_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 128\n",
    "tree_depth = 12\n",
    "batch_size = 512\n",
    "device = 'cpu'\n",
    "train_data_path = r'/mnt/qnap/ekosman/mitbih_train.csv'\n",
    "test_data_path = r'/mnt/qnap/ekosman/mitbih_test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_iter = torch.utils.data.DataLoader(MITBIH(train_data_path),\n",
    "                                             batch_size=batch_size,\n",
    "                                             shuffle=True,\n",
    "                                             num_workers=1,\n",
    "                                             pin_memory=True,\n",
    "                                             drop_last=True)\n",
    "\n",
    "test_data_iter = torch.utils.data.DataLoader(MITBIH(test_data_path),\n",
    "                                             batch_size=batch_size,\n",
    "                                             shuffle=True,\n",
    "                                             num_workers=1,\n",
    "                                             pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(32, 32, kernel_size=5, stride=1, padding=2)\n",
    "        self.conv2 = nn.Conv1d(32, 32, kernel_size=5, stride=1, padding=2)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool = nn.MaxPool1d(kernel_size=5, stride=2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = x\n",
    "        y = self.conv1(y)\n",
    "        y = self.relu1(y)\n",
    "        y = self.conv2(y)\n",
    "        y = y + x\n",
    "        y = self.relu2(y)\n",
    "        y = self.pool(y)\n",
    "        return y\n",
    "\n",
    "\n",
    "class ECGModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ECGModel, self).__init__()\n",
    "        self.conv = nn.Conv1d(1, 32, kernel_size=5, stride=1, padding=1)\n",
    "        self.block1 = ConvBlock()\n",
    "        self.block2 = ConvBlock()\n",
    "        self.block3 = ConvBlock()\n",
    "        self.block4 = ConvBlock()\n",
    "        self.block5 = ConvBlock()\n",
    "        self.fc1 = nn.Linear(64, 32)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(32, 5)\n",
    "\n",
    "    def forward(self, x, return_interm=False):\n",
    "        x = self.conv(x)\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.block4(x)\n",
    "        x = self.block5(x)\n",
    "        interm = x.flatten(1)\n",
    "        x = self.fc1(interm)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        if return_interm:\n",
    "            return x, interm\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_crt = ClassificationKNNLoss(k=k).to(device)\n",
    "\n",
    "def train(model, loader, optimizer, device):\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    for iteration, (batch, target) in enumerate(loader):\n",
    "        batch = batch.to(device)\n",
    "        target = target.to(device)\n",
    "        outputs, interm = model(batch, return_interm=True)\n",
    "        mse_loss = F.cross_entropy(outputs, target)\n",
    "        mse_loss = mse_loss.sum(dim=-1).mean()\n",
    "        try:\n",
    "            knn_loss = knn_crt(interm, target)\n",
    "            if torch.isinf(knn_loss):\n",
    "                knn_loss = torch.tensor(0).to(device)\n",
    "        except ValueError:\n",
    "            knn_loss = torch.tensor(0).to(device)\n",
    "        loss = mse_loss + knn_loss\n",
    "        total_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if iteration % log_every == 0:\n",
    "            print(f\"Epoch {epoch} / {epochs} | iteration {iteration} / {len(loader)} | Total Loss: {loss.item()} | KNN Loss: {knn_loss.item()} | CLS Loss: {mse_loss.item()}\")\n",
    "\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model, loader, device):\n",
    "    model.eval()\n",
    "    \n",
    "    correct = 0\n",
    "    for iteration, (batch, target) in enumerate(loader):\n",
    "        batch = batch.to(device)\n",
    "        target = target.to(device)\n",
    "        y_pred = model(batch).argmax(dim=-1)\n",
    "        correct += y_pred.eq(target.view(-1).data).sum()\n",
    "    \n",
    "    return correct / len(loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Params: 53957\n"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "lr = 1e-3\n",
    "log_every = 10\n",
    "\n",
    "model = ECGModel().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "num_params = sum([p.numel() for p in model.parameters()])\n",
    "print(f'#Params: {num_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eitan.k/.local/lib/python3.6/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: CUDA driver initialization failed, you might not have a CUDA gpu. (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 200 | iteration 0 / 171 | Total Loss: 7.600358486175537 | KNN Loss: 5.915103435516357 | CLS Loss: 1.6852549314498901\n",
      "Epoch 1 / 200 | iteration 10 / 171 | Total Loss: 6.007096767425537 | KNN Loss: 5.319669723510742 | CLS Loss: 0.6874269843101501\n",
      "Epoch 1 / 200 | iteration 20 / 171 | Total Loss: 5.801949977874756 | KNN Loss: 5.157888889312744 | CLS Loss: 0.6440610289573669\n",
      "Epoch 1 / 200 | iteration 30 / 171 | Total Loss: 5.727632999420166 | KNN Loss: 5.123403072357178 | CLS Loss: 0.6042298078536987\n",
      "Epoch 1 / 200 | iteration 40 / 171 | Total Loss: 5.63745641708374 | KNN Loss: 5.082511901855469 | CLS Loss: 0.5549446940422058\n",
      "Epoch 1 / 200 | iteration 50 / 171 | Total Loss: 5.573057651519775 | KNN Loss: 5.031651496887207 | CLS Loss: 0.5414061546325684\n",
      "Epoch 1 / 200 | iteration 60 / 171 | Total Loss: 5.594612121582031 | KNN Loss: 5.078127861022949 | CLS Loss: 0.5164840817451477\n",
      "Epoch 1 / 200 | iteration 70 / 171 | Total Loss: 5.527932643890381 | KNN Loss: 5.050487518310547 | CLS Loss: 0.47744518518447876\n",
      "Epoch 1 / 200 | iteration 80 / 171 | Total Loss: 5.486606597900391 | KNN Loss: 5.032014846801758 | CLS Loss: 0.45459163188934326\n",
      "Epoch 1 / 200 | iteration 90 / 171 | Total Loss: 5.461806774139404 | KNN Loss: 5.019109725952148 | CLS Loss: 0.44269710779190063\n",
      "Epoch 1 / 200 | iteration 100 / 171 | Total Loss: 5.388957977294922 | KNN Loss: 4.998069763183594 | CLS Loss: 0.3908880650997162\n",
      "Epoch 1 / 200 | iteration 110 / 171 | Total Loss: 5.392045497894287 | KNN Loss: 5.024843692779541 | CLS Loss: 0.3672020137310028\n",
      "Epoch 1 / 200 | iteration 120 / 171 | Total Loss: 5.325538635253906 | KNN Loss: 5.018682956695557 | CLS Loss: 0.3068554401397705\n",
      "Epoch 1 / 200 | iteration 130 / 171 | Total Loss: 5.370345592498779 | KNN Loss: 4.987960338592529 | CLS Loss: 0.38238537311553955\n",
      "Epoch 1 / 200 | iteration 140 / 171 | Total Loss: 5.289689064025879 | KNN Loss: 4.979521751403809 | CLS Loss: 0.3101675510406494\n",
      "Epoch 1 / 200 | iteration 150 / 171 | Total Loss: 5.23192024230957 | KNN Loss: 4.965460777282715 | CLS Loss: 0.26645946502685547\n",
      "Epoch 1 / 200 | iteration 160 / 171 | Total Loss: 5.3194074630737305 | KNN Loss: 5.0002617835998535 | CLS Loss: 0.3191456198692322\n",
      "Epoch 1 / 200 | iteration 170 / 171 | Total Loss: 5.260915279388428 | KNN Loss: 4.96419620513916 | CLS Loss: 0.2967192232608795\n",
      "Epoch: 001, Loss: 5.5603, Train: 0.8921, Valid: 0.8928, Best: 0.8928\n",
      "Epoch 2 / 200 | iteration 0 / 171 | Total Loss: 5.382564544677734 | KNN Loss: 5.0080790519714355 | CLS Loss: 0.3744855225086212\n",
      "Epoch 2 / 200 | iteration 10 / 171 | Total Loss: 5.264586925506592 | KNN Loss: 4.983417510986328 | CLS Loss: 0.28116926550865173\n",
      "Epoch 2 / 200 | iteration 20 / 171 | Total Loss: 5.274704456329346 | KNN Loss: 4.9849090576171875 | CLS Loss: 0.289795458316803\n",
      "Epoch 2 / 200 | iteration 30 / 171 | Total Loss: 5.204345226287842 | KNN Loss: 4.956789016723633 | CLS Loss: 0.24755613505840302\n",
      "Epoch 2 / 200 | iteration 40 / 171 | Total Loss: 5.203339576721191 | KNN Loss: 4.968314170837402 | CLS Loss: 0.23502549529075623\n",
      "Epoch 2 / 200 | iteration 50 / 171 | Total Loss: 5.2526350021362305 | KNN Loss: 4.959702968597412 | CLS Loss: 0.2929319143295288\n",
      "Epoch 2 / 200 | iteration 60 / 171 | Total Loss: 5.211284637451172 | KNN Loss: 4.930798530578613 | CLS Loss: 0.2804860770702362\n",
      "Epoch 2 / 200 | iteration 70 / 171 | Total Loss: 5.200891971588135 | KNN Loss: 4.948056697845459 | CLS Loss: 0.252835214138031\n",
      "Epoch 2 / 200 | iteration 80 / 171 | Total Loss: 5.111537933349609 | KNN Loss: 4.920437812805176 | CLS Loss: 0.19109998643398285\n",
      "Epoch 2 / 200 | iteration 90 / 171 | Total Loss: 5.133670330047607 | KNN Loss: 4.909416675567627 | CLS Loss: 0.22425369918346405\n",
      "Epoch 2 / 200 | iteration 100 / 171 | Total Loss: 5.1060895919799805 | KNN Loss: 4.905925750732422 | CLS Loss: 0.200163796544075\n",
      "Epoch 2 / 200 | iteration 110 / 171 | Total Loss: 5.161034107208252 | KNN Loss: 4.932912349700928 | CLS Loss: 0.2281215339899063\n",
      "Epoch 2 / 200 | iteration 120 / 171 | Total Loss: 5.208843231201172 | KNN Loss: 4.923816204071045 | CLS Loss: 0.28502726554870605\n",
      "Epoch 2 / 200 | iteration 130 / 171 | Total Loss: 5.138069152832031 | KNN Loss: 4.872004985809326 | CLS Loss: 0.266063928604126\n",
      "Epoch 2 / 200 | iteration 140 / 171 | Total Loss: 5.070436000823975 | KNN Loss: 4.900020122528076 | CLS Loss: 0.1704157292842865\n",
      "Epoch 2 / 200 | iteration 150 / 171 | Total Loss: 5.177902698516846 | KNN Loss: 4.919617176055908 | CLS Loss: 0.25828561186790466\n",
      "Epoch 2 / 200 | iteration 160 / 171 | Total Loss: 5.1004509925842285 | KNN Loss: 4.913761615753174 | CLS Loss: 0.18668930232524872\n",
      "Epoch 2 / 200 | iteration 170 / 171 | Total Loss: 5.1657843589782715 | KNN Loss: 4.939182281494141 | CLS Loss: 0.22660218179225922\n",
      "Epoch: 002, Loss: 5.1742, Train: 0.9565, Valid: 0.9557, Best: 0.9557\n",
      "Epoch 3 / 200 | iteration 0 / 171 | Total Loss: 5.055805206298828 | KNN Loss: 4.91023063659668 | CLS Loss: 0.14557456970214844\n",
      "Epoch 3 / 200 | iteration 10 / 171 | Total Loss: 5.063643932342529 | KNN Loss: 4.906431198120117 | CLS Loss: 0.15721267461776733\n",
      "Epoch 3 / 200 | iteration 20 / 171 | Total Loss: 5.0526652336120605 | KNN Loss: 4.875794887542725 | CLS Loss: 0.1768701672554016\n",
      "Epoch 3 / 200 | iteration 30 / 171 | Total Loss: 5.104241847991943 | KNN Loss: 4.94126033782959 | CLS Loss: 0.162981316447258\n",
      "Epoch 3 / 200 | iteration 40 / 171 | Total Loss: 5.06691837310791 | KNN Loss: 4.939614295959473 | CLS Loss: 0.12730422616004944\n",
      "Epoch 3 / 200 | iteration 50 / 171 | Total Loss: 5.0178303718566895 | KNN Loss: 4.858636379241943 | CLS Loss: 0.1591939628124237\n",
      "Epoch 3 / 200 | iteration 60 / 171 | Total Loss: 5.018104553222656 | KNN Loss: 4.883741855621338 | CLS Loss: 0.13436250388622284\n",
      "Epoch 3 / 200 | iteration 70 / 171 | Total Loss: 5.004451274871826 | KNN Loss: 4.844132423400879 | CLS Loss: 0.16031873226165771\n",
      "Epoch 3 / 200 | iteration 80 / 171 | Total Loss: 5.10439920425415 | KNN Loss: 4.934660911560059 | CLS Loss: 0.16973815858364105\n",
      "Epoch 3 / 200 | iteration 90 / 171 | Total Loss: 5.081684589385986 | KNN Loss: 4.903265476226807 | CLS Loss: 0.17841902375221252\n",
      "Epoch 3 / 200 | iteration 100 / 171 | Total Loss: 5.058647155761719 | KNN Loss: 4.901793479919434 | CLS Loss: 0.15685348212718964\n",
      "Epoch 3 / 200 | iteration 110 / 171 | Total Loss: 5.014585494995117 | KNN Loss: 4.915821075439453 | CLS Loss: 0.09876418858766556\n",
      "Epoch 3 / 200 | iteration 120 / 171 | Total Loss: 5.012760639190674 | KNN Loss: 4.890522480010986 | CLS Loss: 0.12223827093839645\n",
      "Epoch 3 / 200 | iteration 130 / 171 | Total Loss: 5.013501167297363 | KNN Loss: 4.870610237121582 | CLS Loss: 0.14289070665836334\n",
      "Epoch 3 / 200 | iteration 140 / 171 | Total Loss: 5.027350902557373 | KNN Loss: 4.888626575469971 | CLS Loss: 0.13872428238391876\n",
      "Epoch 3 / 200 | iteration 150 / 171 | Total Loss: 5.0629448890686035 | KNN Loss: 4.907775402069092 | CLS Loss: 0.1551695168018341\n",
      "Epoch 3 / 200 | iteration 160 / 171 | Total Loss: 5.0511064529418945 | KNN Loss: 4.928562164306641 | CLS Loss: 0.12254409492015839\n",
      "Epoch 3 / 200 | iteration 170 / 171 | Total Loss: 5.046058177947998 | KNN Loss: 4.929337024688721 | CLS Loss: 0.11672110855579376\n",
      "Epoch: 003, Loss: 5.0487, Train: 0.9692, Valid: 0.9671, Best: 0.9671\n",
      "Epoch 4 / 200 | iteration 0 / 171 | Total Loss: 4.974501132965088 | KNN Loss: 4.8734564781188965 | CLS Loss: 0.10104487836360931\n",
      "Epoch 4 / 200 | iteration 10 / 171 | Total Loss: 5.003594875335693 | KNN Loss: 4.877479076385498 | CLS Loss: 0.12611575424671173\n",
      "Epoch 4 / 200 | iteration 20 / 171 | Total Loss: 4.9867939949035645 | KNN Loss: 4.873881816864014 | CLS Loss: 0.11291216313838959\n",
      "Epoch 4 / 200 | iteration 30 / 171 | Total Loss: 4.981293201446533 | KNN Loss: 4.875214576721191 | CLS Loss: 0.10607879608869553\n",
      "Epoch 4 / 200 | iteration 40 / 171 | Total Loss: 5.018680572509766 | KNN Loss: 4.871400356292725 | CLS Loss: 0.1472802311182022\n",
      "Epoch 4 / 200 | iteration 50 / 171 | Total Loss: 5.139040946960449 | KNN Loss: 4.917233943939209 | CLS Loss: 0.2218068391084671\n",
      "Epoch 4 / 200 | iteration 60 / 171 | Total Loss: 4.974401950836182 | KNN Loss: 4.8503642082214355 | CLS Loss: 0.12403776496648788\n",
      "Epoch 4 / 200 | iteration 70 / 171 | Total Loss: 5.011995315551758 | KNN Loss: 4.909111976623535 | CLS Loss: 0.1028834730386734\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 / 200 | iteration 80 / 171 | Total Loss: 4.983331203460693 | KNN Loss: 4.883627414703369 | CLS Loss: 0.09970363974571228\n",
      "Epoch 4 / 200 | iteration 90 / 171 | Total Loss: 5.015748023986816 | KNN Loss: 4.884199142456055 | CLS Loss: 0.13154886662960052\n",
      "Epoch 4 / 200 | iteration 100 / 171 | Total Loss: 5.044870376586914 | KNN Loss: 4.889216423034668 | CLS Loss: 0.15565384924411774\n",
      "Epoch 4 / 200 | iteration 110 / 171 | Total Loss: 5.039339542388916 | KNN Loss: 4.890556335449219 | CLS Loss: 0.14878319203853607\n",
      "Epoch 4 / 200 | iteration 120 / 171 | Total Loss: 5.022307872772217 | KNN Loss: 4.898890495300293 | CLS Loss: 0.12341748178005219\n",
      "Epoch 4 / 200 | iteration 130 / 171 | Total Loss: 5.003188610076904 | KNN Loss: 4.889073371887207 | CLS Loss: 0.11411505192518234\n",
      "Epoch 4 / 200 | iteration 140 / 171 | Total Loss: 4.9481658935546875 | KNN Loss: 4.844954013824463 | CLS Loss: 0.10321195423603058\n",
      "Epoch 4 / 200 | iteration 150 / 171 | Total Loss: 4.926411151885986 | KNN Loss: 4.837088108062744 | CLS Loss: 0.08932308852672577\n",
      "Epoch 4 / 200 | iteration 160 / 171 | Total Loss: 4.914461612701416 | KNN Loss: 4.8387131690979 | CLS Loss: 0.07574833929538727\n",
      "Epoch 4 / 200 | iteration 170 / 171 | Total Loss: 4.9553985595703125 | KNN Loss: 4.837272644042969 | CLS Loss: 0.11812610924243927\n",
      "Epoch: 004, Loss: 4.9923, Train: 0.9711, Valid: 0.9692, Best: 0.9692\n",
      "Epoch 5 / 200 | iteration 0 / 171 | Total Loss: 4.970154762268066 | KNN Loss: 4.857291221618652 | CLS Loss: 0.11286360770463943\n",
      "Epoch 5 / 200 | iteration 10 / 171 | Total Loss: 4.940997123718262 | KNN Loss: 4.8392791748046875 | CLS Loss: 0.10171808302402496\n",
      "Epoch 5 / 200 | iteration 20 / 171 | Total Loss: 4.998610973358154 | KNN Loss: 4.867245674133301 | CLS Loss: 0.13136525452136993\n",
      "Epoch 5 / 200 | iteration 30 / 171 | Total Loss: 4.969512462615967 | KNN Loss: 4.853833198547363 | CLS Loss: 0.11567940562963486\n",
      "Epoch 5 / 200 | iteration 40 / 171 | Total Loss: 5.005216598510742 | KNN Loss: 4.889901161193848 | CLS Loss: 0.11531525105237961\n",
      "Epoch 5 / 200 | iteration 50 / 171 | Total Loss: 4.966745853424072 | KNN Loss: 4.875283718109131 | CLS Loss: 0.09146198630332947\n",
      "Epoch 5 / 200 | iteration 60 / 171 | Total Loss: 4.954083442687988 | KNN Loss: 4.823897838592529 | CLS Loss: 0.13018539547920227\n",
      "Epoch 5 / 200 | iteration 70 / 171 | Total Loss: 4.9667067527771 | KNN Loss: 4.844234943389893 | CLS Loss: 0.1224718764424324\n",
      "Epoch 5 / 200 | iteration 80 / 171 | Total Loss: 4.986710548400879 | KNN Loss: 4.871940612792969 | CLS Loss: 0.11476976424455643\n",
      "Epoch 5 / 200 | iteration 90 / 171 | Total Loss: 4.9623494148254395 | KNN Loss: 4.862349987030029 | CLS Loss: 0.09999939799308777\n",
      "Epoch 5 / 200 | iteration 100 / 171 | Total Loss: 4.968419551849365 | KNN Loss: 4.858837127685547 | CLS Loss: 0.10958245396614075\n",
      "Epoch 5 / 200 | iteration 110 / 171 | Total Loss: 4.965130805969238 | KNN Loss: 4.842029571533203 | CLS Loss: 0.12310144305229187\n",
      "Epoch 5 / 200 | iteration 120 / 171 | Total Loss: 4.958991527557373 | KNN Loss: 4.887448787689209 | CLS Loss: 0.07154268771409988\n",
      "Epoch 5 / 200 | iteration 130 / 171 | Total Loss: 4.922854423522949 | KNN Loss: 4.812110900878906 | CLS Loss: 0.11074365675449371\n",
      "Epoch 5 / 200 | iteration 140 / 171 | Total Loss: 4.957306385040283 | KNN Loss: 4.848191738128662 | CLS Loss: 0.10911472886800766\n",
      "Epoch 5 / 200 | iteration 150 / 171 | Total Loss: 4.946374893188477 | KNN Loss: 4.861387252807617 | CLS Loss: 0.08498778939247131\n",
      "Epoch 5 / 200 | iteration 160 / 171 | Total Loss: 4.8942461013793945 | KNN Loss: 4.825356483459473 | CLS Loss: 0.06888975948095322\n",
      "Epoch 5 / 200 | iteration 170 / 171 | Total Loss: 4.92258358001709 | KNN Loss: 4.846462726593018 | CLS Loss: 0.07612092047929764\n",
      "Epoch: 005, Loss: 4.9562, Train: 0.9752, Valid: 0.9734, Best: 0.9734\n",
      "Epoch 6 / 200 | iteration 0 / 171 | Total Loss: 4.899156093597412 | KNN Loss: 4.829954624176025 | CLS Loss: 0.06920168548822403\n",
      "Epoch 6 / 200 | iteration 10 / 171 | Total Loss: 4.957895278930664 | KNN Loss: 4.881444454193115 | CLS Loss: 0.07645063102245331\n",
      "Epoch 6 / 200 | iteration 20 / 171 | Total Loss: 4.955748558044434 | KNN Loss: 4.856415271759033 | CLS Loss: 0.09933304786682129\n",
      "Epoch 6 / 200 | iteration 30 / 171 | Total Loss: 4.971407890319824 | KNN Loss: 4.85101842880249 | CLS Loss: 0.12038969248533249\n",
      "Epoch 6 / 200 | iteration 40 / 171 | Total Loss: 4.934101104736328 | KNN Loss: 4.836745262145996 | CLS Loss: 0.09735599905252457\n",
      "Epoch 6 / 200 | iteration 50 / 171 | Total Loss: 4.940647125244141 | KNN Loss: 4.837314128875732 | CLS Loss: 0.10333307087421417\n",
      "Epoch 6 / 200 | iteration 60 / 171 | Total Loss: 4.9215874671936035 | KNN Loss: 4.834764003753662 | CLS Loss: 0.08682328462600708\n",
      "Epoch 6 / 200 | iteration 70 / 171 | Total Loss: 4.955750942230225 | KNN Loss: 4.867982387542725 | CLS Loss: 0.0877685472369194\n",
      "Epoch 6 / 200 | iteration 80 / 171 | Total Loss: 4.941352367401123 | KNN Loss: 4.847639083862305 | CLS Loss: 0.0937134176492691\n",
      "Epoch 6 / 200 | iteration 90 / 171 | Total Loss: 4.921261787414551 | KNN Loss: 4.834317684173584 | CLS Loss: 0.08694417029619217\n",
      "Epoch 6 / 200 | iteration 100 / 171 | Total Loss: 4.918806076049805 | KNN Loss: 4.823380470275879 | CLS Loss: 0.09542559832334518\n",
      "Epoch 6 / 200 | iteration 110 / 171 | Total Loss: 4.944101810455322 | KNN Loss: 4.84644889831543 | CLS Loss: 0.09765299409627914\n",
      "Epoch 6 / 200 | iteration 120 / 171 | Total Loss: 4.949118137359619 | KNN Loss: 4.852904796600342 | CLS Loss: 0.09621329605579376\n",
      "Epoch 6 / 200 | iteration 130 / 171 | Total Loss: 4.964853286743164 | KNN Loss: 4.847469329833984 | CLS Loss: 0.11738375574350357\n",
      "Epoch 6 / 200 | iteration 140 / 171 | Total Loss: 4.975950241088867 | KNN Loss: 4.831357479095459 | CLS Loss: 0.14459291100502014\n",
      "Epoch 6 / 200 | iteration 150 / 171 | Total Loss: 4.921387672424316 | KNN Loss: 4.819742679595947 | CLS Loss: 0.10164522379636765\n",
      "Epoch 6 / 200 | iteration 160 / 171 | Total Loss: 4.916486740112305 | KNN Loss: 4.816494464874268 | CLS Loss: 0.09999239444732666\n",
      "Epoch 6 / 200 | iteration 170 / 171 | Total Loss: 4.895477771759033 | KNN Loss: 4.802792549133301 | CLS Loss: 0.09268510341644287\n",
      "Epoch: 006, Loss: 4.9272, Train: 0.9771, Valid: 0.9753, Best: 0.9753\n",
      "Epoch 7 / 200 | iteration 0 / 171 | Total Loss: 4.900181770324707 | KNN Loss: 4.798369884490967 | CLS Loss: 0.10181188583374023\n",
      "Epoch 7 / 200 | iteration 10 / 171 | Total Loss: 4.90471887588501 | KNN Loss: 4.81744909286499 | CLS Loss: 0.08726996183395386\n",
      "Epoch 7 / 200 | iteration 20 / 171 | Total Loss: 4.93300199508667 | KNN Loss: 4.852703094482422 | CLS Loss: 0.08029886335134506\n",
      "Epoch 7 / 200 | iteration 30 / 171 | Total Loss: 4.891607284545898 | KNN Loss: 4.813366413116455 | CLS Loss: 0.07824104279279709\n",
      "Epoch 7 / 200 | iteration 40 / 171 | Total Loss: 4.843906402587891 | KNN Loss: 4.767857074737549 | CLS Loss: 0.07604911178350449\n",
      "Epoch 7 / 200 | iteration 50 / 171 | Total Loss: 4.92141580581665 | KNN Loss: 4.835410118103027 | CLS Loss: 0.08600569516420364\n",
      "Epoch 7 / 200 | iteration 60 / 171 | Total Loss: 4.886087417602539 | KNN Loss: 4.80461311340332 | CLS Loss: 0.08147452026605606\n",
      "Epoch 7 / 200 | iteration 70 / 171 | Total Loss: 4.911693096160889 | KNN Loss: 4.829170227050781 | CLS Loss: 0.082522913813591\n",
      "Epoch 7 / 200 | iteration 80 / 171 | Total Loss: 4.92109489440918 | KNN Loss: 4.857789516448975 | CLS Loss: 0.06330519914627075\n",
      "Epoch 7 / 200 | iteration 90 / 171 | Total Loss: 4.934235572814941 | KNN Loss: 4.846381664276123 | CLS Loss: 0.08785378187894821\n",
      "Epoch 7 / 200 | iteration 100 / 171 | Total Loss: 4.867187976837158 | KNN Loss: 4.784618377685547 | CLS Loss: 0.08256970345973969\n",
      "Epoch 7 / 200 | iteration 110 / 171 | Total Loss: 4.931652069091797 | KNN Loss: 4.838103771209717 | CLS Loss: 0.09354809671640396\n",
      "Epoch 7 / 200 | iteration 120 / 171 | Total Loss: 4.921172142028809 | KNN Loss: 4.845093250274658 | CLS Loss: 0.07607893645763397\n"
     ]
    }
   ],
   "source": [
    "best_valid_acc = 0\n",
    "losses = []\n",
    "train_accs = []\n",
    "val_accs = []\n",
    "for epoch in range(1, epochs + 1):\n",
    "    loss = train(model, train_data_iter, optimizer, device)\n",
    "#     print(f\"Loss: {loss} =============================\")\n",
    "    losses.append(loss)\n",
    "    train_acc = test(model, train_data_iter, device)\n",
    "    train_accs.append(train_acc)\n",
    "    valid_acc = test(model, test_data_iter, device)\n",
    "    val_accs.append(valid_acc)\n",
    "    if valid_acc > best_valid_acc:\n",
    "        best_valid_acc = valid_acc\n",
    "    if epoch % 1 == 0:\n",
    "        print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, '\n",
    "              f'Train: {train_acc:.4f}, Valid: {valid_acc:.4f}, '\n",
    "              f'Best: {best_valid_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(model, test_data_iter, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(losses, label='train loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(train_accs, label='train accuracy')\n",
    "plt.plot(val_accs, label='validation accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_samples = torch.tensor([])\n",
    "projections = torch.tensor([])\n",
    "labels = torch.tensor([])\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, y in tqdm(test_data_iter):\n",
    "        test_samples = torch.cat([test_samples, x])\n",
    "        labels = torch.cat([labels, y])\n",
    "        x = x.to(device)\n",
    "        _, interm = model(x, True)\n",
    "        projections = torch.cat([projections, interm.detach().cpu().flatten(1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = pairwise_distances(projections)\n",
    "# distances = np.triu(distances)\n",
    "distances_f = distances.flatten()\n",
    "\n",
    "plt.matshow(distances)\n",
    "plt.colorbar()\n",
    "plt.figure()\n",
    "plt.hist(distances_f[distances_f > 0], bins=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = DBSCAN(eps=2, min_samples=10).fit_predict(projections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of inliers: {sum(clusters != -1) / len(clusters)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexity = 100\n",
    "p = reduce_dims_and_plot(projections[clusters != -1],\n",
    "                         y=clusters[clusters != -1],\n",
    "                         title=f'perplexity: {perplexity}',\n",
    "                         file_name=None,\n",
    "                         perplexity=perplexity,\n",
    "                         library='Multicore-TSNE',\n",
    "                         perform_PCA=False,\n",
    "                         projected=None,\n",
    "                         figure_type='2d',\n",
    "                         show_figure=True,\n",
    "                         close_figure=False,\n",
    "                         text=None)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Soft-Decision-Tree given the self-labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_dataset = list(zip(test_samples.flatten(1)[clusters!=-1], clusters[clusters != -1]))\n",
    "batch_size = 512\n",
    "tree_loader = torch.utils.data.DataLoader(tree_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define how we prune the weights of a node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_node(node_weights, factor=1):\n",
    "    w = node_weights.cpu().detach().numpy()\n",
    "    mean_ = np.mean(w)\n",
    "    std_ = np.std(w)\n",
    "    node_weights[((mean_ - std_ * factor) < node_weights) & (node_weights < (mean_ + std_ * factor))] = 0\n",
    "    return node_weights\n",
    "\n",
    "def prune_node_keep(node_weights, keep=4):\n",
    "    w = node_weights.cpu().detach().numpy()\n",
    "    throw_idx = np.argsort(abs(w))[:-keep]\n",
    "    node_weights[throw_idx] = 0\n",
    "    return node_weights\n",
    "\n",
    "def prune_tree(tree_, factor):\n",
    "    new_weights = tree_.inner_nodes.weight.clone()\n",
    "    for i in range(new_weights.shape[0]):\n",
    "        res = prune_node_keep(new_weights[i, :], factor)\n",
    "        new_weights[i, :] = res\n",
    "\n",
    "    with torch.no_grad():\n",
    "        tree_.inner_nodes.weight.copy_(new_weights)\n",
    "        \n",
    "def sparseness(x):\n",
    "    s = []\n",
    "    for i in range(x.shape[0]):\n",
    "        x_ = x[i, :]\n",
    "        sp = (len(x_) - torch.norm(x_, 0).item()) / len(x_)\n",
    "        s.append(sp)\n",
    "    return np.mean(s)\n",
    "\n",
    "def compute_regularization_by_level(tree):\n",
    "    total_reg = 0\n",
    "    for i in range(tree.inner_nodes.weight.shape[0]):\n",
    "        cur_level = np.floor(np.log2(i+1))\n",
    "        node_reg = torch.norm(tree.inner_nodes.weight[i].view(-1), 2)\n",
    "        total_reg += 2**(-cur_level) * node_reg\n",
    "    return total_reg\n",
    "\n",
    "def show_sparseness(tree):\n",
    "    avg_sp = sparseness(tree.inner_nodes.weight)\n",
    "    print(f\"Average sparseness: {avg_sp}\")\n",
    "    layer = 0\n",
    "    sps = []\n",
    "    for i in range(tree.inner_nodes.weight.shape[0]):\n",
    "        cur_layer = int(np.floor(np.log2(i+1)))\n",
    "        if cur_layer != layer:\n",
    "            print(f\"layer {layer}: {np.mean(sps)}\")\n",
    "            sps = []\n",
    "            layer = cur_layer\n",
    "\n",
    "        x_ = tree.inner_nodes.weight[i, :]\n",
    "        sp = (len(x_) - torch.norm(x_, 0).item()) / len(x_)\n",
    "        sps.append(sp)\n",
    "        \n",
    "    return avg_sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_epoch(model, loader, device, log_interval, losses, accs, epoch, iteration):\n",
    "    model = model.train()\n",
    "    for batch_idx, (data, target) in enumerate(loader):\n",
    "        iteration += 1\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        output, penalty = tree.forward(data)\n",
    "\n",
    "        # Loss\n",
    "        loss_tree = criterion(output, target.view(-1))\n",
    "\n",
    "        # Penalty\n",
    "        loss_tree += penalty\n",
    "\n",
    "        # Sparse regularization\n",
    "#         fc_params = torch.cat([x.view(-1) for x in tree.inner_nodes.parameters()])\n",
    "#         regularization = sparsity_lamda * torch.norm(fc_params, 2)\n",
    "        regularization = sparsity_lamda * compute_regularization_by_level(tree)\n",
    "        loss = loss_tree\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        pred = output.data.max(1)[1]\n",
    "        correct = pred.eq(target.view(-1).data).sum()\n",
    "        accs.append(correct.item() / data.size()[0])\n",
    "\n",
    "        # Print training status\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(f\"Epoch: {epoch:02d} | Batch: {batch_idx:03d} / {len(loader):03d} | Total loss: {loss.item():.3f} | Reg loss: {regularization.item():.3f} | Tree loss: {loss_tree.item():.3f} | Accuracy: {correct.item() / data.size()[0]:03f} | {round((time.time() - start_time) / iteration, 3)} sec/iter\")\n",
    "            \n",
    "    return iteration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 5e-3\n",
    "weight_decay = 5e-4\n",
    "sparsity_lamda = 2e-3\n",
    "epochs = 400\n",
    "log_interval = 100\n",
    "use_cuda = device != 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = SDT(input_dim=test_samples.shape[2], output_dim=len(set(clusters)) - 1, depth=tree_depth, lamda=1e-3, use_cuda=use_cuda)\n",
    "optimizer = torch.optim.Adam(tree.parameters(),\n",
    "                                 lr=lr,\n",
    "                                 weight_decay=weight_decay)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "tree = tree.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "accs = []\n",
    "sparsity = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "iteration = 0\n",
    "for epoch in range(epochs):\n",
    "    # Training\n",
    "    avg_sp = show_sparseness(tree)\n",
    "    sparsity.append(avg_sp)\n",
    "    iteration = do_epoch(tree, tree_loader, device, log_interval, losses, accs, epoch, iteration)\n",
    "    \n",
    "    if epoch % 1 == 0:\n",
    "        prune_tree(tree, factor=3)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel('Iteration')\n",
    "plt.plot(accs, label='Accuracy vs iteration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel('Iteration')\n",
    "plt.plot(losses, label='Loss vs iteration')\n",
    "# plt.yscale(\"log\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "weights = tree.inner_nodes.weight.cpu().detach().numpy().flatten()\n",
    "plt.hist(weights, bins=500)\n",
    "weights_std = np.std(weights)\n",
    "weights_mean = np.mean(weights)\n",
    "plt.axvline(weights_mean + weights_std, color='r')\n",
    "plt.axvline(weights_mean - weights_std, color='r')\n",
    "plt.title(f\"Mean: {weights_mean}   |   STD: {weights_std}\")\n",
    "plt.yscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10), dpi=80)\n",
    "avg_height, root = tree.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accumulate samples in the leaves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of patterns: {len(root.get_leaves())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = 'greedy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root.clear_leaves_samples()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (data, target) in enumerate(tree_loader):\n",
    "        root.accumulate_samples(data, method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tighten boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr_names = [f\"T_{i}\" for i in range(test_samples.shape[2])]\n",
    "leaves = root.get_leaves()\n",
    "sum_comprehensibility = 0\n",
    "comprehensibilities = []\n",
    "for pattern_counter, leaf in enumerate(leaves):\n",
    "    leaf.reset_path()\n",
    "    leaf.tighten_with_accumulated_samples()\n",
    "    conds = leaf.get_path_conditions(attr_names)\n",
    "    print(f\"============== Pattern {pattern_counter + 1} ==============\")\n",
    "    comprehensibilities.append(sum([cond.comprehensibility for cond in conds]))\n",
    "    \n",
    "print(f\"Average comprehensibility: {np.mean(comprehensibilities)}\")\n",
    "print(f\"std comprehensibility: {np.std(comprehensibilities)}\")\n",
    "print(f\"var comprehensibility: {np.var(comprehensibilities)}\")\n",
    "print(f\"minimum comprehensibility: {np.min(comprehensibilities)}\")\n",
    "print(f\"maximum comprehensibility: {np.max(comprehensibilities)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
