{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib widget\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import torch.nn as nn\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from stream_generators.market_basket_dataset import MarketBasketDataset, BinaryEncodingTransform, RemoveItemsTransform\n",
    "from utils.MatplotlibUtils import reduce_dims_and_plot\n",
    "from network.auto_encoder import AutoEncoder\n",
    "from losses.knn_loss import KNNLoss\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from soft_decision_tree.sdt_model import SDT\n",
    "from sklearn.metrics import davies_bouldin_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 16\n",
    "tree_depth = 6\n",
    "device = 'cuda'\n",
    "dataset_path = r\"/mnt/qnap/ekosman/Groceries_dataset.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the market basket dataset and use one-hot encoding for items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MarketBasketDataset(dataset_path=dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoEncoder(dataset.n_items, 50, 4).train().to(device)\n",
    "epochs = 500\n",
    "lr = 5e-3\n",
    "batch_size = 512\n",
    "log_every = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.transform = torchvision.transforms.Compose([\n",
    "    RemoveItemsTransform(p=0.5),\n",
    "    BinaryEncodingTransform(mapping=dataset.items_to_idx),\n",
    "]\n",
    ")\n",
    "dataset.target_transform = torchvision.transforms.Compose([\n",
    "    BinaryEncodingTransform(mapping=dataset.items_to_idx),\n",
    "]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 / 500 | iteration 0 / 30 | Total Loss: 8.210041999816895 | KNN Loss: 6.227721214294434 | BCE Loss: 1.98232102394104\n",
      "Epoch 0 / 500 | iteration 5 / 30 | Total Loss: 8.172706604003906 | KNN Loss: 6.227598190307617 | BCE Loss: 1.9451082944869995\n",
      "Epoch 0 / 500 | iteration 10 / 30 | Total Loss: 8.17885684967041 | KNN Loss: 6.227121353149414 | BCE Loss: 1.9517356157302856\n",
      "Epoch 0 / 500 | iteration 15 / 30 | Total Loss: 8.124919891357422 | KNN Loss: 6.22711181640625 | BCE Loss: 1.8978075981140137\n",
      "Epoch 0 / 500 | iteration 20 / 30 | Total Loss: 8.122896194458008 | KNN Loss: 6.22642707824707 | BCE Loss: 1.896469235420227\n",
      "Epoch 0 / 500 | iteration 25 / 30 | Total Loss: 8.110965728759766 | KNN Loss: 6.225857257843018 | BCE Loss: 1.8851089477539062\n",
      "Epoch 1 / 500 | iteration 0 / 30 | Total Loss: 8.14926528930664 | KNN Loss: 6.225307464599609 | BCE Loss: 1.9239575862884521\n",
      "Epoch 1 / 500 | iteration 5 / 30 | Total Loss: 8.112594604492188 | KNN Loss: 6.2245941162109375 | BCE Loss: 1.8880000114440918\n",
      "Epoch 1 / 500 | iteration 10 / 30 | Total Loss: 8.113907814025879 | KNN Loss: 6.224557876586914 | BCE Loss: 1.8893496990203857\n",
      "Epoch 1 / 500 | iteration 15 / 30 | Total Loss: 8.089406967163086 | KNN Loss: 6.224455833435059 | BCE Loss: 1.8649511337280273\n",
      "Epoch 1 / 500 | iteration 20 / 30 | Total Loss: 8.140917778015137 | KNN Loss: 6.223893642425537 | BCE Loss: 1.917023777961731\n",
      "Epoch 1 / 500 | iteration 25 / 30 | Total Loss: 8.077149391174316 | KNN Loss: 6.223790645599365 | BCE Loss: 1.8533589839935303\n",
      "Epoch 2 / 500 | iteration 0 / 30 | Total Loss: 8.074583053588867 | KNN Loss: 6.2226786613464355 | BCE Loss: 1.8519048690795898\n",
      "Epoch 2 / 500 | iteration 5 / 30 | Total Loss: 8.045351028442383 | KNN Loss: 6.222557544708252 | BCE Loss: 1.8227930068969727\n",
      "Epoch 2 / 500 | iteration 10 / 30 | Total Loss: 8.12161922454834 | KNN Loss: 6.222242832183838 | BCE Loss: 1.8993765115737915\n",
      "Epoch 2 / 500 | iteration 15 / 30 | Total Loss: 8.088617324829102 | KNN Loss: 6.221652507781982 | BCE Loss: 1.8669646978378296\n",
      "Epoch 2 / 500 | iteration 20 / 30 | Total Loss: 8.055861473083496 | KNN Loss: 6.221045970916748 | BCE Loss: 1.8348158597946167\n",
      "Epoch 2 / 500 | iteration 25 / 30 | Total Loss: 8.056529998779297 | KNN Loss: 6.2203569412231445 | BCE Loss: 1.8361729383468628\n",
      "Epoch 3 / 500 | iteration 0 / 30 | Total Loss: 8.039130210876465 | KNN Loss: 6.220117568969727 | BCE Loss: 1.8190124034881592\n",
      "Epoch 3 / 500 | iteration 5 / 30 | Total Loss: 8.055362701416016 | KNN Loss: 6.219604969024658 | BCE Loss: 1.8357577323913574\n",
      "Epoch 3 / 500 | iteration 10 / 30 | Total Loss: 7.986468315124512 | KNN Loss: 6.217700481414795 | BCE Loss: 1.7687679529190063\n",
      "Epoch 3 / 500 | iteration 15 / 30 | Total Loss: 8.052776336669922 | KNN Loss: 6.2187724113464355 | BCE Loss: 1.8340044021606445\n",
      "Epoch 3 / 500 | iteration 20 / 30 | Total Loss: 8.032245635986328 | KNN Loss: 6.218465805053711 | BCE Loss: 1.8137800693511963\n",
      "Epoch 3 / 500 | iteration 25 / 30 | Total Loss: 7.967491149902344 | KNN Loss: 6.216617107391357 | BCE Loss: 1.7508742809295654\n",
      "Epoch 4 / 500 | iteration 0 / 30 | Total Loss: 7.997302055358887 | KNN Loss: 6.2157464027404785 | BCE Loss: 1.7815556526184082\n",
      "Epoch 4 / 500 | iteration 5 / 30 | Total Loss: 7.9608540534973145 | KNN Loss: 6.214975357055664 | BCE Loss: 1.7458786964416504\n",
      "Epoch 4 / 500 | iteration 10 / 30 | Total Loss: 7.941085338592529 | KNN Loss: 6.213384628295898 | BCE Loss: 1.7277007102966309\n",
      "Epoch 4 / 500 | iteration 15 / 30 | Total Loss: 8.013326644897461 | KNN Loss: 6.211349964141846 | BCE Loss: 1.8019771575927734\n",
      "Epoch 4 / 500 | iteration 20 / 30 | Total Loss: 7.97330904006958 | KNN Loss: 6.21123743057251 | BCE Loss: 1.7620716094970703\n",
      "Epoch 4 / 500 | iteration 25 / 30 | Total Loss: 7.9387688636779785 | KNN Loss: 6.209994792938232 | BCE Loss: 1.728774070739746\n",
      "Epoch 5 / 500 | iteration 0 / 30 | Total Loss: 7.932154655456543 | KNN Loss: 6.206907272338867 | BCE Loss: 1.7252475023269653\n",
      "Epoch 5 / 500 | iteration 5 / 30 | Total Loss: 7.8541412353515625 | KNN Loss: 6.206615924835205 | BCE Loss: 1.6475253105163574\n",
      "Epoch 5 / 500 | iteration 10 / 30 | Total Loss: 7.888500213623047 | KNN Loss: 6.2035675048828125 | BCE Loss: 1.6849324703216553\n",
      "Epoch 5 / 500 | iteration 15 / 30 | Total Loss: 7.895021438598633 | KNN Loss: 6.2032341957092285 | BCE Loss: 1.6917874813079834\n",
      "Epoch 5 / 500 | iteration 20 / 30 | Total Loss: 7.891773223876953 | KNN Loss: 6.202686309814453 | BCE Loss: 1.689087152481079\n",
      "Epoch 5 / 500 | iteration 25 / 30 | Total Loss: 7.862407684326172 | KNN Loss: 6.197975158691406 | BCE Loss: 1.6644327640533447\n",
      "Epoch 6 / 500 | iteration 0 / 30 | Total Loss: 7.799746513366699 | KNN Loss: 6.197331428527832 | BCE Loss: 1.602414846420288\n",
      "Epoch 6 / 500 | iteration 5 / 30 | Total Loss: 7.815403938293457 | KNN Loss: 6.1962480545043945 | BCE Loss: 1.6191556453704834\n",
      "Epoch 6 / 500 | iteration 10 / 30 | Total Loss: 7.828273773193359 | KNN Loss: 6.189911842346191 | BCE Loss: 1.6383618116378784\n",
      "Epoch 6 / 500 | iteration 15 / 30 | Total Loss: 7.792712211608887 | KNN Loss: 6.186118125915527 | BCE Loss: 1.6065938472747803\n",
      "Epoch 6 / 500 | iteration 20 / 30 | Total Loss: 7.792706489562988 | KNN Loss: 6.182440757751465 | BCE Loss: 1.6102659702301025\n",
      "Epoch 6 / 500 | iteration 25 / 30 | Total Loss: 7.73843240737915 | KNN Loss: 6.179773330688477 | BCE Loss: 1.5586590766906738\n",
      "Epoch 7 / 500 | iteration 0 / 30 | Total Loss: 7.708521842956543 | KNN Loss: 6.174516201019287 | BCE Loss: 1.5340056419372559\n",
      "Epoch 7 / 500 | iteration 5 / 30 | Total Loss: 7.669511318206787 | KNN Loss: 6.163791179656982 | BCE Loss: 1.5057201385498047\n",
      "Epoch 7 / 500 | iteration 10 / 30 | Total Loss: 7.689517974853516 | KNN Loss: 6.165896892547607 | BCE Loss: 1.5236213207244873\n",
      "Epoch 7 / 500 | iteration 15 / 30 | Total Loss: 7.663230895996094 | KNN Loss: 6.150887966156006 | BCE Loss: 1.512342929840088\n",
      "Epoch 7 / 500 | iteration 20 / 30 | Total Loss: 7.597343444824219 | KNN Loss: 6.150469779968262 | BCE Loss: 1.4468739032745361\n",
      "Epoch 7 / 500 | iteration 25 / 30 | Total Loss: 7.619543075561523 | KNN Loss: 6.144448757171631 | BCE Loss: 1.4750943183898926\n",
      "Epoch 8 / 500 | iteration 0 / 30 | Total Loss: 7.56773567199707 | KNN Loss: 6.124588489532471 | BCE Loss: 1.4431474208831787\n",
      "Epoch 8 / 500 | iteration 5 / 30 | Total Loss: 7.543415069580078 | KNN Loss: 6.118897914886475 | BCE Loss: 1.4245173931121826\n",
      "Epoch 8 / 500 | iteration 10 / 30 | Total Loss: 7.479384422302246 | KNN Loss: 6.11198616027832 | BCE Loss: 1.3673980236053467\n",
      "Epoch 8 / 500 | iteration 15 / 30 | Total Loss: 7.470930099487305 | KNN Loss: 6.104652404785156 | BCE Loss: 1.3662779331207275\n",
      "Epoch 8 / 500 | iteration 20 / 30 | Total Loss: 7.4236555099487305 | KNN Loss: 6.07764196395874 | BCE Loss: 1.3460133075714111\n",
      "Epoch 8 / 500 | iteration 25 / 30 | Total Loss: 7.3948974609375 | KNN Loss: 6.054572582244873 | BCE Loss: 1.3403246402740479\n",
      "Epoch 9 / 500 | iteration 0 / 30 | Total Loss: 7.330551624298096 | KNN Loss: 6.020902156829834 | BCE Loss: 1.3096495866775513\n",
      "Epoch 9 / 500 | iteration 5 / 30 | Total Loss: 7.282639026641846 | KNN Loss: 6.014715194702148 | BCE Loss: 1.2679238319396973\n",
      "Epoch 9 / 500 | iteration 10 / 30 | Total Loss: 7.2608442306518555 | KNN Loss: 5.994413375854492 | BCE Loss: 1.2664307355880737\n",
      "Epoch 9 / 500 | iteration 15 / 30 | Total Loss: 7.171113967895508 | KNN Loss: 5.951793193817139 | BCE Loss: 1.21932053565979\n",
      "Epoch 9 / 500 | iteration 20 / 30 | Total Loss: 7.156651496887207 | KNN Loss: 5.937870502471924 | BCE Loss: 1.2187809944152832\n",
      "Epoch 9 / 500 | iteration 25 / 30 | Total Loss: 7.103865623474121 | KNN Loss: 5.896152973175049 | BCE Loss: 1.2077125310897827\n",
      "Epoch 10 / 500 | iteration 0 / 30 | Total Loss: 7.032194137573242 | KNN Loss: 5.848710060119629 | BCE Loss: 1.1834838390350342\n",
      "Epoch 10 / 500 | iteration 5 / 30 | Total Loss: 6.973489761352539 | KNN Loss: 5.782871723175049 | BCE Loss: 1.1906182765960693\n",
      "Epoch 10 / 500 | iteration 10 / 30 | Total Loss: 6.966930866241455 | KNN Loss: 5.782370567321777 | BCE Loss: 1.1845602989196777\n",
      "Epoch 10 / 500 | iteration 15 / 30 | Total Loss: 6.849748611450195 | KNN Loss: 5.677330493927002 | BCE Loss: 1.1724178791046143\n",
      "Epoch 10 / 500 | iteration 20 / 30 | Total Loss: 6.675086975097656 | KNN Loss: 5.566064834594727 | BCE Loss: 1.1090221405029297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 / 500 | iteration 25 / 30 | Total Loss: 6.615598201751709 | KNN Loss: 5.525787353515625 | BCE Loss: 1.089810848236084\n",
      "Epoch 11 / 500 | iteration 0 / 30 | Total Loss: 6.5095672607421875 | KNN Loss: 5.394506454467773 | BCE Loss: 1.115060567855835\n",
      "Epoch 11 / 500 | iteration 5 / 30 | Total Loss: 6.456127166748047 | KNN Loss: 5.326240062713623 | BCE Loss: 1.1298872232437134\n",
      "Epoch 11 / 500 | iteration 10 / 30 | Total Loss: 6.258234977722168 | KNN Loss: 5.168399810791016 | BCE Loss: 1.0898349285125732\n",
      "Epoch 11 / 500 | iteration 15 / 30 | Total Loss: 6.153439521789551 | KNN Loss: 5.073712348937988 | BCE Loss: 1.0797271728515625\n",
      "Epoch 11 / 500 | iteration 20 / 30 | Total Loss: 6.1178364753723145 | KNN Loss: 4.985212326049805 | BCE Loss: 1.1326241493225098\n",
      "Epoch 11 / 500 | iteration 25 / 30 | Total Loss: 5.943134307861328 | KNN Loss: 4.838218688964844 | BCE Loss: 1.1049153804779053\n",
      "Epoch 12 / 500 | iteration 0 / 30 | Total Loss: 5.827757835388184 | KNN Loss: 4.71121072769165 | BCE Loss: 1.1165473461151123\n",
      "Epoch 12 / 500 | iteration 5 / 30 | Total Loss: 5.7227888107299805 | KNN Loss: 4.6150803565979 | BCE Loss: 1.1077086925506592\n",
      "Epoch 12 / 500 | iteration 10 / 30 | Total Loss: 5.596391677856445 | KNN Loss: 4.502439975738525 | BCE Loss: 1.093951940536499\n",
      "Epoch 12 / 500 | iteration 15 / 30 | Total Loss: 5.518259525299072 | KNN Loss: 4.41288423538208 | BCE Loss: 1.1053754091262817\n",
      "Epoch 12 / 500 | iteration 20 / 30 | Total Loss: 5.383727073669434 | KNN Loss: 4.3093647956848145 | BCE Loss: 1.07436203956604\n",
      "Epoch 12 / 500 | iteration 25 / 30 | Total Loss: 5.312526702880859 | KNN Loss: 4.216777324676514 | BCE Loss: 1.0957496166229248\n",
      "Epoch 13 / 500 | iteration 0 / 30 | Total Loss: 5.2435688972473145 | KNN Loss: 4.135204792022705 | BCE Loss: 1.1083641052246094\n",
      "Epoch 13 / 500 | iteration 5 / 30 | Total Loss: 5.195558547973633 | KNN Loss: 4.068668365478516 | BCE Loss: 1.1268904209136963\n",
      "Epoch 13 / 500 | iteration 10 / 30 | Total Loss: 5.089654922485352 | KNN Loss: 3.9925811290740967 | BCE Loss: 1.097074031829834\n",
      "Epoch 13 / 500 | iteration 15 / 30 | Total Loss: 5.025076866149902 | KNN Loss: 3.9046359062194824 | BCE Loss: 1.120441198348999\n",
      "Epoch 13 / 500 | iteration 20 / 30 | Total Loss: 4.957322120666504 | KNN Loss: 3.868986129760742 | BCE Loss: 1.0883359909057617\n",
      "Epoch 13 / 500 | iteration 25 / 30 | Total Loss: 4.922740936279297 | KNN Loss: 3.8069658279418945 | BCE Loss: 1.1157748699188232\n",
      "Epoch 14 / 500 | iteration 0 / 30 | Total Loss: 4.844633102416992 | KNN Loss: 3.7592082023620605 | BCE Loss: 1.0854250192642212\n",
      "Epoch 14 / 500 | iteration 5 / 30 | Total Loss: 4.772429466247559 | KNN Loss: 3.6985530853271484 | BCE Loss: 1.0738766193389893\n",
      "Epoch 14 / 500 | iteration 10 / 30 | Total Loss: 4.756817817687988 | KNN Loss: 3.6757009029388428 | BCE Loss: 1.0811171531677246\n",
      "Epoch 14 / 500 | iteration 15 / 30 | Total Loss: 4.684844970703125 | KNN Loss: 3.597728729248047 | BCE Loss: 1.0871164798736572\n",
      "Epoch 14 / 500 | iteration 20 / 30 | Total Loss: 4.644962310791016 | KNN Loss: 3.5845260620117188 | BCE Loss: 1.060436487197876\n",
      "Epoch 14 / 500 | iteration 25 / 30 | Total Loss: 4.623227119445801 | KNN Loss: 3.553671360015869 | BCE Loss: 1.0695559978485107\n",
      "Epoch 15 / 500 | iteration 0 / 30 | Total Loss: 4.594283580780029 | KNN Loss: 3.51987886428833 | BCE Loss: 1.0744045972824097\n",
      "Epoch 15 / 500 | iteration 5 / 30 | Total Loss: 4.588141918182373 | KNN Loss: 3.514188528060913 | BCE Loss: 1.0739535093307495\n",
      "Epoch 15 / 500 | iteration 10 / 30 | Total Loss: 4.547061920166016 | KNN Loss: 3.457305431365967 | BCE Loss: 1.0897563695907593\n",
      "Epoch 15 / 500 | iteration 15 / 30 | Total Loss: 4.504199028015137 | KNN Loss: 3.442919969558716 | BCE Loss: 1.061279296875\n",
      "Epoch 15 / 500 | iteration 20 / 30 | Total Loss: 4.478452205657959 | KNN Loss: 3.4004085063934326 | BCE Loss: 1.0780436992645264\n",
      "Epoch 15 / 500 | iteration 25 / 30 | Total Loss: 4.498950481414795 | KNN Loss: 3.421922445297241 | BCE Loss: 1.0770281553268433\n",
      "Epoch 16 / 500 | iteration 0 / 30 | Total Loss: 4.501823902130127 | KNN Loss: 3.399031639099121 | BCE Loss: 1.1027922630310059\n",
      "Epoch 16 / 500 | iteration 5 / 30 | Total Loss: 4.492077350616455 | KNN Loss: 3.4088706970214844 | BCE Loss: 1.0832067728042603\n",
      "Epoch 16 / 500 | iteration 10 / 30 | Total Loss: 4.45022439956665 | KNN Loss: 3.3736913204193115 | BCE Loss: 1.0765329599380493\n",
      "Epoch 16 / 500 | iteration 15 / 30 | Total Loss: 4.43302059173584 | KNN Loss: 3.342803478240967 | BCE Loss: 1.090217113494873\n",
      "Epoch 16 / 500 | iteration 20 / 30 | Total Loss: 4.420387268066406 | KNN Loss: 3.3273673057556152 | BCE Loss: 1.0930198431015015\n",
      "Epoch 16 / 500 | iteration 25 / 30 | Total Loss: 4.4354143142700195 | KNN Loss: 3.353095531463623 | BCE Loss: 1.0823190212249756\n",
      "Epoch 17 / 500 | iteration 0 / 30 | Total Loss: 4.3797101974487305 | KNN Loss: 3.3234703540802 | BCE Loss: 1.0562398433685303\n",
      "Epoch 17 / 500 | iteration 5 / 30 | Total Loss: 4.418813705444336 | KNN Loss: 3.3467326164245605 | BCE Loss: 1.072081208229065\n",
      "Epoch 17 / 500 | iteration 10 / 30 | Total Loss: 4.427496910095215 | KNN Loss: 3.3452415466308594 | BCE Loss: 1.082255482673645\n",
      "Epoch 17 / 500 | iteration 15 / 30 | Total Loss: 4.423548698425293 | KNN Loss: 3.343416929244995 | BCE Loss: 1.0801315307617188\n",
      "Epoch 17 / 500 | iteration 20 / 30 | Total Loss: 4.371916770935059 | KNN Loss: 3.2918105125427246 | BCE Loss: 1.0801060199737549\n",
      "Epoch 17 / 500 | iteration 25 / 30 | Total Loss: 4.347738742828369 | KNN Loss: 3.2903590202331543 | BCE Loss: 1.0573797225952148\n",
      "Epoch 18 / 500 | iteration 0 / 30 | Total Loss: 4.314268112182617 | KNN Loss: 3.254197120666504 | BCE Loss: 1.0600707530975342\n",
      "Epoch 18 / 500 | iteration 5 / 30 | Total Loss: 4.344537734985352 | KNN Loss: 3.282156467437744 | BCE Loss: 1.0623810291290283\n",
      "Epoch 18 / 500 | iteration 10 / 30 | Total Loss: 4.304048538208008 | KNN Loss: 3.249751091003418 | BCE Loss: 1.0542972087860107\n",
      "Epoch 18 / 500 | iteration 15 / 30 | Total Loss: 4.330506324768066 | KNN Loss: 3.267866373062134 | BCE Loss: 1.062639832496643\n",
      "Epoch 18 / 500 | iteration 20 / 30 | Total Loss: 4.3688130378723145 | KNN Loss: 3.3047263622283936 | BCE Loss: 1.0640865564346313\n",
      "Epoch 18 / 500 | iteration 25 / 30 | Total Loss: 4.325054168701172 | KNN Loss: 3.246385335922241 | BCE Loss: 1.0786685943603516\n",
      "Epoch 19 / 500 | iteration 0 / 30 | Total Loss: 4.312798023223877 | KNN Loss: 3.231975793838501 | BCE Loss: 1.080822229385376\n",
      "Epoch 19 / 500 | iteration 5 / 30 | Total Loss: 4.304300308227539 | KNN Loss: 3.2432894706726074 | BCE Loss: 1.0610105991363525\n",
      "Epoch 19 / 500 | iteration 10 / 30 | Total Loss: 4.3290605545043945 | KNN Loss: 3.2660317420959473 | BCE Loss: 1.0630288124084473\n",
      "Epoch 19 / 500 | iteration 15 / 30 | Total Loss: 4.2572102546691895 | KNN Loss: 3.1993367671966553 | BCE Loss: 1.0578734874725342\n",
      "Epoch 19 / 500 | iteration 20 / 30 | Total Loss: 4.273653984069824 | KNN Loss: 3.2107725143432617 | BCE Loss: 1.0628814697265625\n",
      "Epoch 19 / 500 | iteration 25 / 30 | Total Loss: 4.257851600646973 | KNN Loss: 3.1949143409729004 | BCE Loss: 1.0629374980926514\n",
      "Epoch 20 / 500 | iteration 0 / 30 | Total Loss: 4.298424243927002 | KNN Loss: 3.2292871475219727 | BCE Loss: 1.0691370964050293\n",
      "Epoch 20 / 500 | iteration 5 / 30 | Total Loss: 4.233437538146973 | KNN Loss: 3.2016944885253906 | BCE Loss: 1.0317432880401611\n",
      "Epoch 20 / 500 | iteration 10 / 30 | Total Loss: 4.272260665893555 | KNN Loss: 3.1945064067840576 | BCE Loss: 1.0777544975280762\n",
      "Epoch 20 / 500 | iteration 15 / 30 | Total Loss: 4.281281471252441 | KNN Loss: 3.227391004562378 | BCE Loss: 1.0538907051086426\n",
      "Epoch 20 / 500 | iteration 20 / 30 | Total Loss: 4.259964942932129 | KNN Loss: 3.205076217651367 | BCE Loss: 1.0548889636993408\n",
      "Epoch 20 / 500 | iteration 25 / 30 | Total Loss: 4.261946201324463 | KNN Loss: 3.2179298400878906 | BCE Loss: 1.0440163612365723\n",
      "Epoch 21 / 500 | iteration 0 / 30 | Total Loss: 4.24594783782959 | KNN Loss: 3.181166172027588 | BCE Loss: 1.0647814273834229\n",
      "Epoch 21 / 500 | iteration 5 / 30 | Total Loss: 4.261815071105957 | KNN Loss: 3.235698699951172 | BCE Loss: 1.026116132736206\n",
      "Epoch 21 / 500 | iteration 10 / 30 | Total Loss: 4.255365371704102 | KNN Loss: 3.184304714202881 | BCE Loss: 1.0710606575012207\n",
      "Epoch 21 / 500 | iteration 15 / 30 | Total Loss: 4.2586870193481445 | KNN Loss: 3.2295126914978027 | BCE Loss: 1.0291743278503418\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 / 500 | iteration 20 / 30 | Total Loss: 4.275885581970215 | KNN Loss: 3.237459421157837 | BCE Loss: 1.0384260416030884\n",
      "Epoch 21 / 500 | iteration 25 / 30 | Total Loss: 4.278428554534912 | KNN Loss: 3.2170639038085938 | BCE Loss: 1.0613646507263184\n",
      "Epoch 22 / 500 | iteration 0 / 30 | Total Loss: 4.236767768859863 | KNN Loss: 3.1986160278320312 | BCE Loss: 1.0381519794464111\n",
      "Epoch 22 / 500 | iteration 5 / 30 | Total Loss: 4.295625686645508 | KNN Loss: 3.219217300415039 | BCE Loss: 1.0764086246490479\n",
      "Epoch 22 / 500 | iteration 10 / 30 | Total Loss: 4.2434821128845215 | KNN Loss: 3.205167531967163 | BCE Loss: 1.038314700126648\n",
      "Epoch 22 / 500 | iteration 15 / 30 | Total Loss: 4.253411293029785 | KNN Loss: 3.2115750312805176 | BCE Loss: 1.0418360233306885\n",
      "Epoch 22 / 500 | iteration 20 / 30 | Total Loss: 4.2888031005859375 | KNN Loss: 3.2246108055114746 | BCE Loss: 1.064192533493042\n",
      "Epoch 22 / 500 | iteration 25 / 30 | Total Loss: 4.273770332336426 | KNN Loss: 3.2208034992218018 | BCE Loss: 1.052966833114624\n",
      "Epoch 23 / 500 | iteration 0 / 30 | Total Loss: 4.291235446929932 | KNN Loss: 3.2474796772003174 | BCE Loss: 1.0437556505203247\n",
      "Epoch 23 / 500 | iteration 5 / 30 | Total Loss: 4.231999397277832 | KNN Loss: 3.1921467781066895 | BCE Loss: 1.0398527383804321\n",
      "Epoch 23 / 500 | iteration 10 / 30 | Total Loss: 4.2857255935668945 | KNN Loss: 3.228851079940796 | BCE Loss: 1.0568747520446777\n",
      "Epoch 23 / 500 | iteration 15 / 30 | Total Loss: 4.292937755584717 | KNN Loss: 3.235872507095337 | BCE Loss: 1.0570652484893799\n",
      "Epoch 23 / 500 | iteration 20 / 30 | Total Loss: 4.259731292724609 | KNN Loss: 3.1991655826568604 | BCE Loss: 1.0605659484863281\n",
      "Epoch 23 / 500 | iteration 25 / 30 | Total Loss: 4.195792198181152 | KNN Loss: 3.17425274848938 | BCE Loss: 1.0215394496917725\n",
      "Epoch 24 / 500 | iteration 0 / 30 | Total Loss: 4.236244201660156 | KNN Loss: 3.192988395690918 | BCE Loss: 1.0432560443878174\n",
      "Epoch 24 / 500 | iteration 5 / 30 | Total Loss: 4.284298896789551 | KNN Loss: 3.215970993041992 | BCE Loss: 1.0683281421661377\n",
      "Epoch 24 / 500 | iteration 10 / 30 | Total Loss: 4.218023300170898 | KNN Loss: 3.1770880222320557 | BCE Loss: 1.0409355163574219\n",
      "Epoch 24 / 500 | iteration 15 / 30 | Total Loss: 4.217215538024902 | KNN Loss: 3.1798343658447266 | BCE Loss: 1.0373809337615967\n",
      "Epoch 24 / 500 | iteration 20 / 30 | Total Loss: 4.203799247741699 | KNN Loss: 3.1809003353118896 | BCE Loss: 1.0228990316390991\n",
      "Epoch 24 / 500 | iteration 25 / 30 | Total Loss: 4.229394912719727 | KNN Loss: 3.1845781803131104 | BCE Loss: 1.0448168516159058\n",
      "Epoch 25 / 500 | iteration 0 / 30 | Total Loss: 4.22985315322876 | KNN Loss: 3.1704940795898438 | BCE Loss: 1.059359073638916\n",
      "Epoch 25 / 500 | iteration 5 / 30 | Total Loss: 4.268131256103516 | KNN Loss: 3.1980140209198 | BCE Loss: 1.070117473602295\n",
      "Epoch 25 / 500 | iteration 10 / 30 | Total Loss: 4.219852447509766 | KNN Loss: 3.197829246520996 | BCE Loss: 1.0220234394073486\n",
      "Epoch 25 / 500 | iteration 15 / 30 | Total Loss: 4.206644058227539 | KNN Loss: 3.1687333583831787 | BCE Loss: 1.03791081905365\n",
      "Epoch 25 / 500 | iteration 20 / 30 | Total Loss: 4.260965347290039 | KNN Loss: 3.233213424682617 | BCE Loss: 1.0277516841888428\n",
      "Epoch 25 / 500 | iteration 25 / 30 | Total Loss: 4.2873854637146 | KNN Loss: 3.2325780391693115 | BCE Loss: 1.0548073053359985\n",
      "Epoch 26 / 500 | iteration 0 / 30 | Total Loss: 4.249783992767334 | KNN Loss: 3.207242012023926 | BCE Loss: 1.0425420999526978\n",
      "Epoch 26 / 500 | iteration 5 / 30 | Total Loss: 4.225109577178955 | KNN Loss: 3.196167230606079 | BCE Loss: 1.0289422273635864\n",
      "Epoch 26 / 500 | iteration 10 / 30 | Total Loss: 4.259208679199219 | KNN Loss: 3.192462921142578 | BCE Loss: 1.0667457580566406\n",
      "Epoch 26 / 500 | iteration 15 / 30 | Total Loss: 4.132180690765381 | KNN Loss: 3.1516430377960205 | BCE Loss: 0.9805377721786499\n",
      "Epoch 26 / 500 | iteration 20 / 30 | Total Loss: 4.241905212402344 | KNN Loss: 3.186040163040161 | BCE Loss: 1.0558652877807617\n",
      "Epoch 26 / 500 | iteration 25 / 30 | Total Loss: 4.181576728820801 | KNN Loss: 3.1550402641296387 | BCE Loss: 1.0265365839004517\n",
      "Epoch 27 / 500 | iteration 0 / 30 | Total Loss: 4.171083450317383 | KNN Loss: 3.123051166534424 | BCE Loss: 1.048032283782959\n",
      "Epoch 27 / 500 | iteration 5 / 30 | Total Loss: 4.180330276489258 | KNN Loss: 3.1474273204803467 | BCE Loss: 1.032902717590332\n",
      "Epoch 27 / 500 | iteration 10 / 30 | Total Loss: 4.226275444030762 | KNN Loss: 3.198373794555664 | BCE Loss: 1.0279014110565186\n",
      "Epoch 27 / 500 | iteration 15 / 30 | Total Loss: 4.224957466125488 | KNN Loss: 3.1938045024871826 | BCE Loss: 1.0311532020568848\n",
      "Epoch 27 / 500 | iteration 20 / 30 | Total Loss: 4.236167907714844 | KNN Loss: 3.1713597774505615 | BCE Loss: 1.0648078918457031\n",
      "Epoch 27 / 500 | iteration 25 / 30 | Total Loss: 4.246152400970459 | KNN Loss: 3.1610264778137207 | BCE Loss: 1.0851259231567383\n",
      "Epoch 28 / 500 | iteration 0 / 30 | Total Loss: 4.192626953125 | KNN Loss: 3.163529396057129 | BCE Loss: 1.029097318649292\n",
      "Epoch 28 / 500 | iteration 5 / 30 | Total Loss: 4.257289886474609 | KNN Loss: 3.1924538612365723 | BCE Loss: 1.0648362636566162\n",
      "Epoch 28 / 500 | iteration 10 / 30 | Total Loss: 4.1896586418151855 | KNN Loss: 3.1639232635498047 | BCE Loss: 1.0257353782653809\n",
      "Epoch 28 / 500 | iteration 15 / 30 | Total Loss: 4.221877098083496 | KNN Loss: 3.170100688934326 | BCE Loss: 1.0517761707305908\n",
      "Epoch 28 / 500 | iteration 20 / 30 | Total Loss: 4.186861515045166 | KNN Loss: 3.156848192214966 | BCE Loss: 1.0300132036209106\n",
      "Epoch 28 / 500 | iteration 25 / 30 | Total Loss: 4.229109764099121 | KNN Loss: 3.1752517223358154 | BCE Loss: 1.0538578033447266\n",
      "Epoch 29 / 500 | iteration 0 / 30 | Total Loss: 4.176415920257568 | KNN Loss: 3.131970167160034 | BCE Loss: 1.0444457530975342\n",
      "Epoch 29 / 500 | iteration 5 / 30 | Total Loss: 4.216913223266602 | KNN Loss: 3.163313627243042 | BCE Loss: 1.0535995960235596\n",
      "Epoch 29 / 500 | iteration 10 / 30 | Total Loss: 4.194046974182129 | KNN Loss: 3.154284954071045 | BCE Loss: 1.0397617816925049\n",
      "Epoch 29 / 500 | iteration 15 / 30 | Total Loss: 4.189700603485107 | KNN Loss: 3.1616897583007812 | BCE Loss: 1.0280108451843262\n",
      "Epoch 29 / 500 | iteration 20 / 30 | Total Loss: 4.195430278778076 | KNN Loss: 3.1399824619293213 | BCE Loss: 1.0554476976394653\n",
      "Epoch 29 / 500 | iteration 25 / 30 | Total Loss: 4.158000946044922 | KNN Loss: 3.1346800327301025 | BCE Loss: 1.0233211517333984\n",
      "Epoch 30 / 500 | iteration 0 / 30 | Total Loss: 4.2421369552612305 | KNN Loss: 3.190460681915283 | BCE Loss: 1.0516765117645264\n",
      "Epoch 30 / 500 | iteration 5 / 30 | Total Loss: 4.219038009643555 | KNN Loss: 3.18408203125 | BCE Loss: 1.0349562168121338\n",
      "Epoch 30 / 500 | iteration 10 / 30 | Total Loss: 4.234055042266846 | KNN Loss: 3.1914262771606445 | BCE Loss: 1.0426287651062012\n",
      "Epoch 30 / 500 | iteration 15 / 30 | Total Loss: 4.21010160446167 | KNN Loss: 3.1592297554016113 | BCE Loss: 1.0508718490600586\n",
      "Epoch 30 / 500 | iteration 20 / 30 | Total Loss: 4.178943634033203 | KNN Loss: 3.1586334705352783 | BCE Loss: 1.0203101634979248\n",
      "Epoch 30 / 500 | iteration 25 / 30 | Total Loss: 4.187336444854736 | KNN Loss: 3.1410794258117676 | BCE Loss: 1.0462568998336792\n",
      "Epoch 31 / 500 | iteration 0 / 30 | Total Loss: 4.191127777099609 | KNN Loss: 3.177651882171631 | BCE Loss: 1.0134761333465576\n",
      "Epoch 31 / 500 | iteration 5 / 30 | Total Loss: 4.2798590660095215 | KNN Loss: 3.221996307373047 | BCE Loss: 1.0578627586364746\n",
      "Epoch 31 / 500 | iteration 10 / 30 | Total Loss: 4.173649787902832 | KNN Loss: 3.14748215675354 | BCE Loss: 1.0261675119400024\n",
      "Epoch 31 / 500 | iteration 15 / 30 | Total Loss: 4.201825141906738 | KNN Loss: 3.1618335247039795 | BCE Loss: 1.0399916172027588\n",
      "Epoch 31 / 500 | iteration 20 / 30 | Total Loss: 4.183205604553223 | KNN Loss: 3.1432301998138428 | BCE Loss: 1.0399755239486694\n",
      "Epoch 31 / 500 | iteration 25 / 30 | Total Loss: 4.164946556091309 | KNN Loss: 3.122729778289795 | BCE Loss: 1.0422168970108032\n",
      "Epoch 32 / 500 | iteration 0 / 30 | Total Loss: 4.139760494232178 | KNN Loss: 3.1211354732513428 | BCE Loss: 1.018625020980835\n",
      "Epoch 32 / 500 | iteration 5 / 30 | Total Loss: 4.1596221923828125 | KNN Loss: 3.143338441848755 | BCE Loss: 1.0162837505340576\n",
      "Epoch 32 / 500 | iteration 10 / 30 | Total Loss: 4.19075870513916 | KNN Loss: 3.132089138031006 | BCE Loss: 1.0586695671081543\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32 / 500 | iteration 15 / 30 | Total Loss: 4.146391868591309 | KNN Loss: 3.1157655715942383 | BCE Loss: 1.0306265354156494\n",
      "Epoch 32 / 500 | iteration 20 / 30 | Total Loss: 4.150697231292725 | KNN Loss: 3.132648468017578 | BCE Loss: 1.0180487632751465\n",
      "Epoch 32 / 500 | iteration 25 / 30 | Total Loss: 4.123737335205078 | KNN Loss: 3.1169633865356445 | BCE Loss: 1.0067739486694336\n",
      "Epoch 33 / 500 | iteration 0 / 30 | Total Loss: 4.14653205871582 | KNN Loss: 3.1173195838928223 | BCE Loss: 1.0292123556137085\n",
      "Epoch 33 / 500 | iteration 5 / 30 | Total Loss: 4.153446197509766 | KNN Loss: 3.1160006523132324 | BCE Loss: 1.0374457836151123\n",
      "Epoch 33 / 500 | iteration 10 / 30 | Total Loss: 4.164685249328613 | KNN Loss: 3.132547616958618 | BCE Loss: 1.0321377515792847\n",
      "Epoch 33 / 500 | iteration 15 / 30 | Total Loss: 4.202447414398193 | KNN Loss: 3.1936850547790527 | BCE Loss: 1.0087623596191406\n",
      "Epoch 33 / 500 | iteration 20 / 30 | Total Loss: 4.221055507659912 | KNN Loss: 3.13733172416687 | BCE Loss: 1.083723783493042\n",
      "Epoch 33 / 500 | iteration 25 / 30 | Total Loss: 4.198957920074463 | KNN Loss: 3.170443534851074 | BCE Loss: 1.0285145044326782\n",
      "Epoch 34 / 500 | iteration 0 / 30 | Total Loss: 4.170346260070801 | KNN Loss: 3.1378731727600098 | BCE Loss: 1.0324729681015015\n",
      "Epoch 34 / 500 | iteration 5 / 30 | Total Loss: 4.190130233764648 | KNN Loss: 3.137542247772217 | BCE Loss: 1.0525882244110107\n",
      "Epoch 34 / 500 | iteration 10 / 30 | Total Loss: 4.184818267822266 | KNN Loss: 3.144446611404419 | BCE Loss: 1.0403717756271362\n",
      "Epoch 34 / 500 | iteration 15 / 30 | Total Loss: 4.1616315841674805 | KNN Loss: 3.134873151779175 | BCE Loss: 1.0267584323883057\n",
      "Epoch 34 / 500 | iteration 20 / 30 | Total Loss: 4.142943382263184 | KNN Loss: 3.121537208557129 | BCE Loss: 1.0214059352874756\n",
      "Epoch 34 / 500 | iteration 25 / 30 | Total Loss: 4.151675224304199 | KNN Loss: 3.1297595500946045 | BCE Loss: 1.0219155550003052\n",
      "Epoch 35 / 500 | iteration 0 / 30 | Total Loss: 4.116240978240967 | KNN Loss: 3.095090627670288 | BCE Loss: 1.0211503505706787\n",
      "Epoch 35 / 500 | iteration 5 / 30 | Total Loss: 4.163813591003418 | KNN Loss: 3.1303045749664307 | BCE Loss: 1.0335092544555664\n",
      "Epoch 35 / 500 | iteration 10 / 30 | Total Loss: 4.211138725280762 | KNN Loss: 3.175503730773926 | BCE Loss: 1.035634994506836\n",
      "Epoch 35 / 500 | iteration 15 / 30 | Total Loss: 4.178763389587402 | KNN Loss: 3.150484800338745 | BCE Loss: 1.0282788276672363\n",
      "Epoch 35 / 500 | iteration 20 / 30 | Total Loss: 4.198225498199463 | KNN Loss: 3.1684749126434326 | BCE Loss: 1.0297504663467407\n",
      "Epoch 35 / 500 | iteration 25 / 30 | Total Loss: 4.237459182739258 | KNN Loss: 3.172590732574463 | BCE Loss: 1.0648682117462158\n",
      "Epoch 36 / 500 | iteration 0 / 30 | Total Loss: 4.172605514526367 | KNN Loss: 3.1150763034820557 | BCE Loss: 1.0575292110443115\n",
      "Epoch 36 / 500 | iteration 5 / 30 | Total Loss: 4.185324668884277 | KNN Loss: 3.1655361652374268 | BCE Loss: 1.0197882652282715\n",
      "Epoch 36 / 500 | iteration 10 / 30 | Total Loss: 4.117201805114746 | KNN Loss: 3.1222851276397705 | BCE Loss: 0.9949164390563965\n",
      "Epoch 36 / 500 | iteration 15 / 30 | Total Loss: 4.137076377868652 | KNN Loss: 3.115123748779297 | BCE Loss: 1.0219526290893555\n",
      "Epoch 36 / 500 | iteration 20 / 30 | Total Loss: 4.190935134887695 | KNN Loss: 3.145634412765503 | BCE Loss: 1.0453009605407715\n",
      "Epoch 36 / 500 | iteration 25 / 30 | Total Loss: 4.183239936828613 | KNN Loss: 3.141317844390869 | BCE Loss: 1.041921854019165\n",
      "Epoch 37 / 500 | iteration 0 / 30 | Total Loss: 4.132111549377441 | KNN Loss: 3.0929415225982666 | BCE Loss: 1.039170265197754\n",
      "Epoch 37 / 500 | iteration 5 / 30 | Total Loss: 4.185922145843506 | KNN Loss: 3.1488234996795654 | BCE Loss: 1.0370985269546509\n",
      "Epoch 37 / 500 | iteration 10 / 30 | Total Loss: 4.168955326080322 | KNN Loss: 3.1248457431793213 | BCE Loss: 1.044109582901001\n",
      "Epoch 37 / 500 | iteration 15 / 30 | Total Loss: 4.1419830322265625 | KNN Loss: 3.1202683448791504 | BCE Loss: 1.021714687347412\n",
      "Epoch 37 / 500 | iteration 20 / 30 | Total Loss: 4.138070106506348 | KNN Loss: 3.108419179916382 | BCE Loss: 1.0296509265899658\n",
      "Epoch 37 / 500 | iteration 25 / 30 | Total Loss: 4.147233009338379 | KNN Loss: 3.1245944499969482 | BCE Loss: 1.0226387977600098\n",
      "Epoch 38 / 500 | iteration 0 / 30 | Total Loss: 4.168203353881836 | KNN Loss: 3.1444575786590576 | BCE Loss: 1.0237455368041992\n",
      "Epoch 38 / 500 | iteration 5 / 30 | Total Loss: 4.174529552459717 | KNN Loss: 3.13081431388855 | BCE Loss: 1.043715238571167\n",
      "Epoch 38 / 500 | iteration 10 / 30 | Total Loss: 4.180225372314453 | KNN Loss: 3.142784357070923 | BCE Loss: 1.0374410152435303\n",
      "Epoch 38 / 500 | iteration 15 / 30 | Total Loss: 4.169467926025391 | KNN Loss: 3.1448097229003906 | BCE Loss: 1.024657964706421\n",
      "Epoch 38 / 500 | iteration 20 / 30 | Total Loss: 4.140631675720215 | KNN Loss: 3.1216013431549072 | BCE Loss: 1.0190304517745972\n",
      "Epoch 38 / 500 | iteration 25 / 30 | Total Loss: 4.158977508544922 | KNN Loss: 3.1201610565185547 | BCE Loss: 1.0388166904449463\n",
      "Epoch 39 / 500 | iteration 0 / 30 | Total Loss: 4.153977870941162 | KNN Loss: 3.1117632389068604 | BCE Loss: 1.0422145128250122\n",
      "Epoch 39 / 500 | iteration 5 / 30 | Total Loss: 4.143054962158203 | KNN Loss: 3.124265432357788 | BCE Loss: 1.018789529800415\n",
      "Epoch 39 / 500 | iteration 10 / 30 | Total Loss: 4.238110065460205 | KNN Loss: 3.1890785694122314 | BCE Loss: 1.049031376838684\n",
      "Epoch 39 / 500 | iteration 15 / 30 | Total Loss: 4.187029838562012 | KNN Loss: 3.1324591636657715 | BCE Loss: 1.0545709133148193\n",
      "Epoch 39 / 500 | iteration 20 / 30 | Total Loss: 4.168413162231445 | KNN Loss: 3.162425994873047 | BCE Loss: 1.0059874057769775\n",
      "Epoch 39 / 500 | iteration 25 / 30 | Total Loss: 4.165133476257324 | KNN Loss: 3.129786491394043 | BCE Loss: 1.0353467464447021\n",
      "Epoch 40 / 500 | iteration 0 / 30 | Total Loss: 4.121864318847656 | KNN Loss: 3.1055665016174316 | BCE Loss: 1.0162980556488037\n",
      "Epoch 40 / 500 | iteration 5 / 30 | Total Loss: 4.203918933868408 | KNN Loss: 3.154960870742798 | BCE Loss: 1.0489581823349\n",
      "Epoch 40 / 500 | iteration 10 / 30 | Total Loss: 4.130913734436035 | KNN Loss: 3.097484588623047 | BCE Loss: 1.0334291458129883\n",
      "Epoch 40 / 500 | iteration 15 / 30 | Total Loss: 4.104551792144775 | KNN Loss: 3.1118528842926025 | BCE Loss: 0.9926987886428833\n",
      "Epoch 40 / 500 | iteration 20 / 30 | Total Loss: 4.137528419494629 | KNN Loss: 3.125793695449829 | BCE Loss: 1.0117347240447998\n",
      "Epoch 40 / 500 | iteration 25 / 30 | Total Loss: 4.183771133422852 | KNN Loss: 3.1493260860443115 | BCE Loss: 1.034444808959961\n",
      "Epoch 41 / 500 | iteration 0 / 30 | Total Loss: 4.140546798706055 | KNN Loss: 3.126126527786255 | BCE Loss: 1.014420509338379\n",
      "Epoch 41 / 500 | iteration 5 / 30 | Total Loss: 4.161087512969971 | KNN Loss: 3.1285624504089355 | BCE Loss: 1.0325249433517456\n",
      "Epoch 41 / 500 | iteration 10 / 30 | Total Loss: 4.147448539733887 | KNN Loss: 3.1198861598968506 | BCE Loss: 1.0275623798370361\n",
      "Epoch 41 / 500 | iteration 15 / 30 | Total Loss: 4.131574630737305 | KNN Loss: 3.1082966327667236 | BCE Loss: 1.023277759552002\n",
      "Epoch 41 / 500 | iteration 20 / 30 | Total Loss: 4.215851783752441 | KNN Loss: 3.164245843887329 | BCE Loss: 1.0516061782836914\n",
      "Epoch 41 / 500 | iteration 25 / 30 | Total Loss: 4.139488697052002 | KNN Loss: 3.1252729892730713 | BCE Loss: 1.0142157077789307\n",
      "Epoch 42 / 500 | iteration 0 / 30 | Total Loss: 4.1462507247924805 | KNN Loss: 3.1105539798736572 | BCE Loss: 1.0356968641281128\n",
      "Epoch 42 / 500 | iteration 5 / 30 | Total Loss: 4.184034824371338 | KNN Loss: 3.1195998191833496 | BCE Loss: 1.0644350051879883\n",
      "Epoch 42 / 500 | iteration 10 / 30 | Total Loss: 4.191308975219727 | KNN Loss: 3.1500792503356934 | BCE Loss: 1.0412299633026123\n",
      "Epoch 42 / 500 | iteration 15 / 30 | Total Loss: 4.141376495361328 | KNN Loss: 3.1140997409820557 | BCE Loss: 1.0272765159606934\n",
      "Epoch 42 / 500 | iteration 20 / 30 | Total Loss: 4.168967247009277 | KNN Loss: 3.1398024559020996 | BCE Loss: 1.0291647911071777\n",
      "Epoch 42 / 500 | iteration 25 / 30 | Total Loss: 4.118834495544434 | KNN Loss: 3.1169562339782715 | BCE Loss: 1.0018783807754517\n",
      "Epoch 43 / 500 | iteration 0 / 30 | Total Loss: 4.180228233337402 | KNN Loss: 3.1499526500701904 | BCE Loss: 1.0302754640579224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43 / 500 | iteration 5 / 30 | Total Loss: 4.191071510314941 | KNN Loss: 3.13553786277771 | BCE Loss: 1.055533528327942\n",
      "Epoch 43 / 500 | iteration 10 / 30 | Total Loss: 4.151248931884766 | KNN Loss: 3.1207659244537354 | BCE Loss: 1.0304827690124512\n",
      "Epoch 43 / 500 | iteration 15 / 30 | Total Loss: 4.1442718505859375 | KNN Loss: 3.1346371173858643 | BCE Loss: 1.0096344947814941\n",
      "Epoch 43 / 500 | iteration 20 / 30 | Total Loss: 4.177558898925781 | KNN Loss: 3.1473333835601807 | BCE Loss: 1.0302252769470215\n",
      "Epoch 43 / 500 | iteration 25 / 30 | Total Loss: 4.125032901763916 | KNN Loss: 3.108682632446289 | BCE Loss: 1.016350269317627\n",
      "Epoch 44 / 500 | iteration 0 / 30 | Total Loss: 4.140707015991211 | KNN Loss: 3.1019270420074463 | BCE Loss: 1.0387797355651855\n",
      "Epoch 44 / 500 | iteration 5 / 30 | Total Loss: 4.171140193939209 | KNN Loss: 3.1368513107299805 | BCE Loss: 1.0342888832092285\n",
      "Epoch 44 / 500 | iteration 10 / 30 | Total Loss: 4.199906349182129 | KNN Loss: 3.154616594314575 | BCE Loss: 1.0452896356582642\n",
      "Epoch 44 / 500 | iteration 15 / 30 | Total Loss: 4.154479026794434 | KNN Loss: 3.121098518371582 | BCE Loss: 1.0333807468414307\n",
      "Epoch 44 / 500 | iteration 20 / 30 | Total Loss: 4.173285484313965 | KNN Loss: 3.1221327781677246 | BCE Loss: 1.0511524677276611\n",
      "Epoch 44 / 500 | iteration 25 / 30 | Total Loss: 4.162047386169434 | KNN Loss: 3.1249027252197266 | BCE Loss: 1.0371448993682861\n",
      "Epoch 45 / 500 | iteration 0 / 30 | Total Loss: 4.160495758056641 | KNN Loss: 3.1210193634033203 | BCE Loss: 1.0394762754440308\n",
      "Epoch 45 / 500 | iteration 5 / 30 | Total Loss: 4.159090518951416 | KNN Loss: 3.113996982574463 | BCE Loss: 1.0450935363769531\n",
      "Epoch 45 / 500 | iteration 10 / 30 | Total Loss: 4.164712429046631 | KNN Loss: 3.1423823833465576 | BCE Loss: 1.0223300457000732\n",
      "Epoch 45 / 500 | iteration 15 / 30 | Total Loss: 4.216745376586914 | KNN Loss: 3.1481330394744873 | BCE Loss: 1.0686120986938477\n",
      "Epoch 45 / 500 | iteration 20 / 30 | Total Loss: 4.112608909606934 | KNN Loss: 3.089378595352173 | BCE Loss: 1.0232303142547607\n",
      "Epoch 45 / 500 | iteration 25 / 30 | Total Loss: 4.10490608215332 | KNN Loss: 3.0818421840667725 | BCE Loss: 1.0230638980865479\n",
      "Epoch 46 / 500 | iteration 0 / 30 | Total Loss: 4.144569396972656 | KNN Loss: 3.100039482116699 | BCE Loss: 1.0445301532745361\n",
      "Epoch 46 / 500 | iteration 5 / 30 | Total Loss: 4.142940521240234 | KNN Loss: 3.122699022293091 | BCE Loss: 1.0202412605285645\n",
      "Epoch 46 / 500 | iteration 10 / 30 | Total Loss: 4.163878917694092 | KNN Loss: 3.1396570205688477 | BCE Loss: 1.0242218971252441\n",
      "Epoch 46 / 500 | iteration 15 / 30 | Total Loss: 4.1177215576171875 | KNN Loss: 3.0913875102996826 | BCE Loss: 1.0263338088989258\n",
      "Epoch 46 / 500 | iteration 20 / 30 | Total Loss: 4.159194469451904 | KNN Loss: 3.1133675575256348 | BCE Loss: 1.04582679271698\n",
      "Epoch 46 / 500 | iteration 25 / 30 | Total Loss: 4.1525092124938965 | KNN Loss: 3.1336328983306885 | BCE Loss: 1.018876314163208\n",
      "Epoch 47 / 500 | iteration 0 / 30 | Total Loss: 4.15218448638916 | KNN Loss: 3.1231253147125244 | BCE Loss: 1.0290592908859253\n",
      "Epoch 47 / 500 | iteration 5 / 30 | Total Loss: 4.188579559326172 | KNN Loss: 3.1437313556671143 | BCE Loss: 1.0448482036590576\n",
      "Epoch 47 / 500 | iteration 10 / 30 | Total Loss: 4.1620097160339355 | KNN Loss: 3.1287989616394043 | BCE Loss: 1.0332106351852417\n",
      "Epoch 47 / 500 | iteration 15 / 30 | Total Loss: 4.108702659606934 | KNN Loss: 3.1093826293945312 | BCE Loss: 0.9993197917938232\n",
      "Epoch 47 / 500 | iteration 20 / 30 | Total Loss: 4.1420464515686035 | KNN Loss: 3.114311695098877 | BCE Loss: 1.0277348756790161\n",
      "Epoch 47 / 500 | iteration 25 / 30 | Total Loss: 4.148334503173828 | KNN Loss: 3.1248598098754883 | BCE Loss: 1.0234746932983398\n",
      "Epoch 48 / 500 | iteration 0 / 30 | Total Loss: 4.124287128448486 | KNN Loss: 3.1220650672912598 | BCE Loss: 1.0022220611572266\n",
      "Epoch 48 / 500 | iteration 5 / 30 | Total Loss: 4.143351078033447 | KNN Loss: 3.1293892860412598 | BCE Loss: 1.013961672782898\n",
      "Epoch 48 / 500 | iteration 10 / 30 | Total Loss: 4.205009460449219 | KNN Loss: 3.1531312465667725 | BCE Loss: 1.0518784523010254\n",
      "Epoch 48 / 500 | iteration 15 / 30 | Total Loss: 4.1727447509765625 | KNN Loss: 3.127187967300415 | BCE Loss: 1.0455570220947266\n",
      "Epoch 48 / 500 | iteration 20 / 30 | Total Loss: 4.151012420654297 | KNN Loss: 3.122297525405884 | BCE Loss: 1.0287151336669922\n",
      "Epoch 48 / 500 | iteration 25 / 30 | Total Loss: 4.149158000946045 | KNN Loss: 3.1301870346069336 | BCE Loss: 1.0189710855484009\n",
      "Epoch 49 / 500 | iteration 0 / 30 | Total Loss: 4.171356201171875 | KNN Loss: 3.1134960651397705 | BCE Loss: 1.057860016822815\n",
      "Epoch 49 / 500 | iteration 5 / 30 | Total Loss: 4.1144208908081055 | KNN Loss: 3.1072609424591064 | BCE Loss: 1.0071600675582886\n",
      "Epoch 49 / 500 | iteration 10 / 30 | Total Loss: 4.158158779144287 | KNN Loss: 3.1221303939819336 | BCE Loss: 1.0360283851623535\n",
      "Epoch 49 / 500 | iteration 15 / 30 | Total Loss: 4.172756671905518 | KNN Loss: 3.130004644393921 | BCE Loss: 1.0427520275115967\n",
      "Epoch 49 / 500 | iteration 20 / 30 | Total Loss: 4.155356407165527 | KNN Loss: 3.1127099990844727 | BCE Loss: 1.0426464080810547\n",
      "Epoch 49 / 500 | iteration 25 / 30 | Total Loss: 4.135303974151611 | KNN Loss: 3.093031167984009 | BCE Loss: 1.0422728061676025\n",
      "Epoch 50 / 500 | iteration 0 / 30 | Total Loss: 4.115363121032715 | KNN Loss: 3.098273992538452 | BCE Loss: 1.0170892477035522\n",
      "Epoch 50 / 500 | iteration 5 / 30 | Total Loss: 4.186504364013672 | KNN Loss: 3.1229875087738037 | BCE Loss: 1.0635168552398682\n",
      "Epoch 50 / 500 | iteration 10 / 30 | Total Loss: 4.138082027435303 | KNN Loss: 3.131664991378784 | BCE Loss: 1.006416916847229\n",
      "Epoch 50 / 500 | iteration 15 / 30 | Total Loss: 4.139842987060547 | KNN Loss: 3.1087303161621094 | BCE Loss: 1.0311126708984375\n",
      "Epoch 50 / 500 | iteration 20 / 30 | Total Loss: 4.133101463317871 | KNN Loss: 3.11740779876709 | BCE Loss: 1.0156939029693604\n",
      "Epoch 50 / 500 | iteration 25 / 30 | Total Loss: 4.14833402633667 | KNN Loss: 3.1309397220611572 | BCE Loss: 1.0173943042755127\n",
      "Epoch 51 / 500 | iteration 0 / 30 | Total Loss: 4.160219192504883 | KNN Loss: 3.1058156490325928 | BCE Loss: 1.0544037818908691\n",
      "Epoch 51 / 500 | iteration 5 / 30 | Total Loss: 4.177812576293945 | KNN Loss: 3.1586036682128906 | BCE Loss: 1.0192089080810547\n",
      "Epoch 51 / 500 | iteration 10 / 30 | Total Loss: 4.093343257904053 | KNN Loss: 3.076225996017456 | BCE Loss: 1.0171172618865967\n",
      "Epoch 51 / 500 | iteration 15 / 30 | Total Loss: 4.200991153717041 | KNN Loss: 3.136286735534668 | BCE Loss: 1.064704418182373\n",
      "Epoch 51 / 500 | iteration 20 / 30 | Total Loss: 4.126685619354248 | KNN Loss: 3.0806829929351807 | BCE Loss: 1.046002745628357\n",
      "Epoch 51 / 500 | iteration 25 / 30 | Total Loss: 4.132721900939941 | KNN Loss: 3.114107131958008 | BCE Loss: 1.0186148881912231\n",
      "Epoch 52 / 500 | iteration 0 / 30 | Total Loss: 4.159316062927246 | KNN Loss: 3.105999231338501 | BCE Loss: 1.0533169507980347\n",
      "Epoch 52 / 500 | iteration 5 / 30 | Total Loss: 4.199935436248779 | KNN Loss: 3.1586415767669678 | BCE Loss: 1.041293978691101\n",
      "Epoch 52 / 500 | iteration 10 / 30 | Total Loss: 4.129847049713135 | KNN Loss: 3.1243975162506104 | BCE Loss: 1.0054495334625244\n",
      "Epoch 52 / 500 | iteration 15 / 30 | Total Loss: 4.129900932312012 | KNN Loss: 3.1244657039642334 | BCE Loss: 1.0054349899291992\n",
      "Epoch 52 / 500 | iteration 20 / 30 | Total Loss: 4.138247013092041 | KNN Loss: 3.1253089904785156 | BCE Loss: 1.012938141822815\n",
      "Epoch 52 / 500 | iteration 25 / 30 | Total Loss: 4.099600315093994 | KNN Loss: 3.097710132598877 | BCE Loss: 1.0018903017044067\n",
      "Epoch 53 / 500 | iteration 0 / 30 | Total Loss: 4.145288467407227 | KNN Loss: 3.1136279106140137 | BCE Loss: 1.0316603183746338\n",
      "Epoch 53 / 500 | iteration 5 / 30 | Total Loss: 4.205507755279541 | KNN Loss: 3.138474464416504 | BCE Loss: 1.0670331716537476\n",
      "Epoch 53 / 500 | iteration 10 / 30 | Total Loss: 4.1711626052856445 | KNN Loss: 3.1363143920898438 | BCE Loss: 1.0348482131958008\n",
      "Epoch 53 / 500 | iteration 15 / 30 | Total Loss: 4.17268180847168 | KNN Loss: 3.127375841140747 | BCE Loss: 1.0453057289123535\n",
      "Epoch 53 / 500 | iteration 20 / 30 | Total Loss: 4.171755790710449 | KNN Loss: 3.120785713195801 | BCE Loss: 1.0509700775146484\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53 / 500 | iteration 25 / 30 | Total Loss: 4.139155864715576 | KNN Loss: 3.0930471420288086 | BCE Loss: 1.046108603477478\n",
      "Epoch 54 / 500 | iteration 0 / 30 | Total Loss: 4.133483409881592 | KNN Loss: 3.113740921020508 | BCE Loss: 1.0197426080703735\n",
      "Epoch 54 / 500 | iteration 5 / 30 | Total Loss: 4.1554155349731445 | KNN Loss: 3.1146538257598877 | BCE Loss: 1.0407614707946777\n",
      "Epoch 54 / 500 | iteration 10 / 30 | Total Loss: 4.141876220703125 | KNN Loss: 3.1183860301971436 | BCE Loss: 1.0234904289245605\n",
      "Epoch 54 / 500 | iteration 15 / 30 | Total Loss: 4.105504989624023 | KNN Loss: 3.0996224880218506 | BCE Loss: 1.0058822631835938\n",
      "Epoch 54 / 500 | iteration 20 / 30 | Total Loss: 4.167599201202393 | KNN Loss: 3.135105609893799 | BCE Loss: 1.0324937105178833\n",
      "Epoch 54 / 500 | iteration 25 / 30 | Total Loss: 4.158660888671875 | KNN Loss: 3.1281347274780273 | BCE Loss: 1.0305259227752686\n",
      "Epoch 55 / 500 | iteration 0 / 30 | Total Loss: 4.115081787109375 | KNN Loss: 3.0937089920043945 | BCE Loss: 1.0213725566864014\n",
      "Epoch 55 / 500 | iteration 5 / 30 | Total Loss: 4.1322784423828125 | KNN Loss: 3.1339502334594727 | BCE Loss: 0.9983283877372742\n",
      "Epoch 55 / 500 | iteration 10 / 30 | Total Loss: 4.137989044189453 | KNN Loss: 3.119842052459717 | BCE Loss: 1.0181467533111572\n",
      "Epoch 55 / 500 | iteration 15 / 30 | Total Loss: 4.178170204162598 | KNN Loss: 3.143341302871704 | BCE Loss: 1.0348286628723145\n",
      "Epoch 55 / 500 | iteration 20 / 30 | Total Loss: 4.153738021850586 | KNN Loss: 3.1218011379241943 | BCE Loss: 1.0319366455078125\n",
      "Epoch 55 / 500 | iteration 25 / 30 | Total Loss: 4.170350074768066 | KNN Loss: 3.1279890537261963 | BCE Loss: 1.042360782623291\n",
      "Epoch 56 / 500 | iteration 0 / 30 | Total Loss: 4.136870861053467 | KNN Loss: 3.1313540935516357 | BCE Loss: 1.0055166482925415\n",
      "Epoch 56 / 500 | iteration 5 / 30 | Total Loss: 4.136659622192383 | KNN Loss: 3.1292858123779297 | BCE Loss: 1.0073740482330322\n",
      "Epoch 56 / 500 | iteration 10 / 30 | Total Loss: 4.14481258392334 | KNN Loss: 3.1113922595977783 | BCE Loss: 1.033420443534851\n",
      "Epoch 56 / 500 | iteration 15 / 30 | Total Loss: 4.154959678649902 | KNN Loss: 3.132490396499634 | BCE Loss: 1.0224692821502686\n",
      "Epoch 56 / 500 | iteration 20 / 30 | Total Loss: 4.158797264099121 | KNN Loss: 3.1169891357421875 | BCE Loss: 1.0418081283569336\n",
      "Epoch 56 / 500 | iteration 25 / 30 | Total Loss: 4.169534683227539 | KNN Loss: 3.1208221912384033 | BCE Loss: 1.0487127304077148\n",
      "Epoch 57 / 500 | iteration 0 / 30 | Total Loss: 4.140718460083008 | KNN Loss: 3.130622148513794 | BCE Loss: 1.010096549987793\n",
      "Epoch 57 / 500 | iteration 5 / 30 | Total Loss: 4.162497043609619 | KNN Loss: 3.1467483043670654 | BCE Loss: 1.0157487392425537\n",
      "Epoch 57 / 500 | iteration 10 / 30 | Total Loss: 4.156817436218262 | KNN Loss: 3.1303787231445312 | BCE Loss: 1.0264384746551514\n",
      "Epoch 57 / 500 | iteration 15 / 30 | Total Loss: 4.180823802947998 | KNN Loss: 3.1209707260131836 | BCE Loss: 1.059852957725525\n",
      "Epoch 57 / 500 | iteration 20 / 30 | Total Loss: 4.180624008178711 | KNN Loss: 3.16729998588562 | BCE Loss: 1.0133237838745117\n",
      "Epoch 57 / 500 | iteration 25 / 30 | Total Loss: 4.178182125091553 | KNN Loss: 3.1240921020507812 | BCE Loss: 1.054089903831482\n",
      "Epoch 58 / 500 | iteration 0 / 30 | Total Loss: 4.149239540100098 | KNN Loss: 3.1286113262176514 | BCE Loss: 1.0206279754638672\n",
      "Epoch 58 / 500 | iteration 5 / 30 | Total Loss: 4.112948417663574 | KNN Loss: 3.110856294631958 | BCE Loss: 1.0020920038223267\n",
      "Epoch 58 / 500 | iteration 10 / 30 | Total Loss: 4.175429344177246 | KNN Loss: 3.1387155055999756 | BCE Loss: 1.0367136001586914\n",
      "Epoch 58 / 500 | iteration 15 / 30 | Total Loss: 4.111543655395508 | KNN Loss: 3.0952703952789307 | BCE Loss: 1.016273021697998\n",
      "Epoch 58 / 500 | iteration 20 / 30 | Total Loss: 4.105926990509033 | KNN Loss: 3.08255672454834 | BCE Loss: 1.023370385169983\n",
      "Epoch 58 / 500 | iteration 25 / 30 | Total Loss: 4.1483845710754395 | KNN Loss: 3.0997209548950195 | BCE Loss: 1.0486637353897095\n",
      "Epoch 59 / 500 | iteration 0 / 30 | Total Loss: 4.120509624481201 | KNN Loss: 3.1126692295074463 | BCE Loss: 1.0078403949737549\n",
      "Epoch 59 / 500 | iteration 5 / 30 | Total Loss: 4.148280143737793 | KNN Loss: 3.127807855606079 | BCE Loss: 1.0204720497131348\n",
      "Epoch 59 / 500 | iteration 10 / 30 | Total Loss: 4.141506671905518 | KNN Loss: 3.115138530731201 | BCE Loss: 1.0263681411743164\n",
      "Epoch 59 / 500 | iteration 15 / 30 | Total Loss: 4.142961502075195 | KNN Loss: 3.0979042053222656 | BCE Loss: 1.0450570583343506\n",
      "Epoch 59 / 500 | iteration 20 / 30 | Total Loss: 4.146169662475586 | KNN Loss: 3.138579845428467 | BCE Loss: 1.0075896978378296\n",
      "Epoch 59 / 500 | iteration 25 / 30 | Total Loss: 4.139805793762207 | KNN Loss: 3.114352226257324 | BCE Loss: 1.0254535675048828\n",
      "Epoch 60 / 500 | iteration 0 / 30 | Total Loss: 4.113699913024902 | KNN Loss: 3.089402914047241 | BCE Loss: 1.024296760559082\n",
      "Epoch 60 / 500 | iteration 5 / 30 | Total Loss: 4.160334587097168 | KNN Loss: 3.1482670307159424 | BCE Loss: 1.0120676755905151\n",
      "Epoch 60 / 500 | iteration 10 / 30 | Total Loss: 4.1378912925720215 | KNN Loss: 3.1237404346466064 | BCE Loss: 1.0141507387161255\n",
      "Epoch 60 / 500 | iteration 15 / 30 | Total Loss: 4.2020063400268555 | KNN Loss: 3.1705636978149414 | BCE Loss: 1.0314425230026245\n",
      "Epoch 60 / 500 | iteration 20 / 30 | Total Loss: 4.1033711433410645 | KNN Loss: 3.09718656539917 | BCE Loss: 1.0061845779418945\n",
      "Epoch 60 / 500 | iteration 25 / 30 | Total Loss: 4.118109226226807 | KNN Loss: 3.09126877784729 | BCE Loss: 1.0268405675888062\n",
      "Epoch 61 / 500 | iteration 0 / 30 | Total Loss: 4.071280479431152 | KNN Loss: 3.0859858989715576 | BCE Loss: 0.9852943420410156\n",
      "Epoch 61 / 500 | iteration 5 / 30 | Total Loss: 4.15095329284668 | KNN Loss: 3.1045689582824707 | BCE Loss: 1.046384334564209\n",
      "Epoch 61 / 500 | iteration 10 / 30 | Total Loss: 4.145724296569824 | KNN Loss: 3.092323064804077 | BCE Loss: 1.053401231765747\n",
      "Epoch 61 / 500 | iteration 15 / 30 | Total Loss: 4.181002140045166 | KNN Loss: 3.132603883743286 | BCE Loss: 1.0483981370925903\n",
      "Epoch 61 / 500 | iteration 20 / 30 | Total Loss: 4.1220197677612305 | KNN Loss: 3.0981216430664062 | BCE Loss: 1.0238983631134033\n",
      "Epoch 61 / 500 | iteration 25 / 30 | Total Loss: 4.1321892738342285 | KNN Loss: 3.1073200702667236 | BCE Loss: 1.0248692035675049\n",
      "Epoch 62 / 500 | iteration 0 / 30 | Total Loss: 4.147637844085693 | KNN Loss: 3.100316286087036 | BCE Loss: 1.0473214387893677\n",
      "Epoch 62 / 500 | iteration 5 / 30 | Total Loss: 4.140989303588867 | KNN Loss: 3.136974334716797 | BCE Loss: 1.0040152072906494\n",
      "Epoch 62 / 500 | iteration 10 / 30 | Total Loss: 4.131312370300293 | KNN Loss: 3.1110334396362305 | BCE Loss: 1.0202786922454834\n",
      "Epoch 62 / 500 | iteration 15 / 30 | Total Loss: 4.093422889709473 | KNN Loss: 3.082648277282715 | BCE Loss: 1.0107746124267578\n",
      "Epoch 62 / 500 | iteration 20 / 30 | Total Loss: 4.135030746459961 | KNN Loss: 3.1163856983184814 | BCE Loss: 1.0186450481414795\n",
      "Epoch 62 / 500 | iteration 25 / 30 | Total Loss: 4.0991106033325195 | KNN Loss: 3.052583694458008 | BCE Loss: 1.0465269088745117\n",
      "Epoch 63 / 500 | iteration 0 / 30 | Total Loss: 4.10703182220459 | KNN Loss: 3.094925880432129 | BCE Loss: 1.0121060609817505\n",
      "Epoch 63 / 500 | iteration 5 / 30 | Total Loss: 4.169034957885742 | KNN Loss: 3.118496894836426 | BCE Loss: 1.0505380630493164\n",
      "Epoch 63 / 500 | iteration 10 / 30 | Total Loss: 4.165177345275879 | KNN Loss: 3.125277519226074 | BCE Loss: 1.0398995876312256\n",
      "Epoch 63 / 500 | iteration 15 / 30 | Total Loss: 4.076233386993408 | KNN Loss: 3.088160514831543 | BCE Loss: 0.9880728721618652\n",
      "Epoch 63 / 500 | iteration 20 / 30 | Total Loss: 4.137245178222656 | KNN Loss: 3.0982937812805176 | BCE Loss: 1.0389516353607178\n",
      "Epoch 63 / 500 | iteration 25 / 30 | Total Loss: 4.115385055541992 | KNN Loss: 3.0931990146636963 | BCE Loss: 1.0221858024597168\n",
      "Epoch 64 / 500 | iteration 0 / 30 | Total Loss: 4.1490349769592285 | KNN Loss: 3.143366813659668 | BCE Loss: 1.005668044090271\n",
      "Epoch 64 / 500 | iteration 5 / 30 | Total Loss: 4.129988670349121 | KNN Loss: 3.1292433738708496 | BCE Loss: 1.0007452964782715\n",
      "Epoch 64 / 500 | iteration 10 / 30 | Total Loss: 4.188250541687012 | KNN Loss: 3.128631353378296 | BCE Loss: 1.0596191883087158\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64 / 500 | iteration 15 / 30 | Total Loss: 4.135832786560059 | KNN Loss: 3.1009888648986816 | BCE Loss: 1.0348438024520874\n",
      "Epoch 64 / 500 | iteration 20 / 30 | Total Loss: 4.106701850891113 | KNN Loss: 3.088606357574463 | BCE Loss: 1.0180954933166504\n",
      "Epoch 64 / 500 | iteration 25 / 30 | Total Loss: 4.1288251876831055 | KNN Loss: 3.0770132541656494 | BCE Loss: 1.051811933517456\n",
      "Epoch 65 / 500 | iteration 0 / 30 | Total Loss: 4.142237186431885 | KNN Loss: 3.1151328086853027 | BCE Loss: 1.027104377746582\n",
      "Epoch 65 / 500 | iteration 5 / 30 | Total Loss: 4.1665472984313965 | KNN Loss: 3.1144158840179443 | BCE Loss: 1.0521315336227417\n",
      "Epoch 65 / 500 | iteration 10 / 30 | Total Loss: 4.144742965698242 | KNN Loss: 3.1329174041748047 | BCE Loss: 1.0118255615234375\n",
      "Epoch 65 / 500 | iteration 15 / 30 | Total Loss: 4.131760597229004 | KNN Loss: 3.119110584259033 | BCE Loss: 1.0126498937606812\n",
      "Epoch 65 / 500 | iteration 20 / 30 | Total Loss: 4.157909393310547 | KNN Loss: 3.124241352081299 | BCE Loss: 1.033667802810669\n",
      "Epoch 65 / 500 | iteration 25 / 30 | Total Loss: 4.165169715881348 | KNN Loss: 3.14058256149292 | BCE Loss: 1.0245869159698486\n",
      "Epoch 66 / 500 | iteration 0 / 30 | Total Loss: 4.12797737121582 | KNN Loss: 3.1059043407440186 | BCE Loss: 1.0220732688903809\n",
      "Epoch 66 / 500 | iteration 5 / 30 | Total Loss: 4.1849470138549805 | KNN Loss: 3.1298840045928955 | BCE Loss: 1.055063009262085\n",
      "Epoch 66 / 500 | iteration 10 / 30 | Total Loss: 4.191804885864258 | KNN Loss: 3.150059223175049 | BCE Loss: 1.0417454242706299\n",
      "Epoch 66 / 500 | iteration 15 / 30 | Total Loss: 4.116138935089111 | KNN Loss: 3.1135294437408447 | BCE Loss: 1.0026096105575562\n",
      "Epoch 66 / 500 | iteration 20 / 30 | Total Loss: 4.161484718322754 | KNN Loss: 3.112184524536133 | BCE Loss: 1.0493004322052002\n",
      "Epoch 66 / 500 | iteration 25 / 30 | Total Loss: 4.165151596069336 | KNN Loss: 3.121020555496216 | BCE Loss: 1.0441309213638306\n",
      "Epoch 67 / 500 | iteration 0 / 30 | Total Loss: 4.145693778991699 | KNN Loss: 3.1142594814300537 | BCE Loss: 1.0314342975616455\n",
      "Epoch 67 / 500 | iteration 5 / 30 | Total Loss: 4.130227088928223 | KNN Loss: 3.1309621334075928 | BCE Loss: 0.9992650747299194\n",
      "Epoch 67 / 500 | iteration 10 / 30 | Total Loss: 4.138517379760742 | KNN Loss: 3.128992795944214 | BCE Loss: 1.0095245838165283\n",
      "Epoch 67 / 500 | iteration 15 / 30 | Total Loss: 4.140310287475586 | KNN Loss: 3.105893611907959 | BCE Loss: 1.034416913986206\n",
      "Epoch 67 / 500 | iteration 20 / 30 | Total Loss: 4.106436729431152 | KNN Loss: 3.0865933895111084 | BCE Loss: 1.0198431015014648\n",
      "Epoch 67 / 500 | iteration 25 / 30 | Total Loss: 4.133160591125488 | KNN Loss: 3.100968360900879 | BCE Loss: 1.0321924686431885\n",
      "Epoch 68 / 500 | iteration 0 / 30 | Total Loss: 4.135992050170898 | KNN Loss: 3.1031265258789062 | BCE Loss: 1.032865285873413\n",
      "Epoch 68 / 500 | iteration 5 / 30 | Total Loss: 4.148355484008789 | KNN Loss: 3.107480049133301 | BCE Loss: 1.0408753156661987\n",
      "Epoch 68 / 500 | iteration 10 / 30 | Total Loss: 4.156266212463379 | KNN Loss: 3.1423428058624268 | BCE Loss: 1.0139232873916626\n",
      "Epoch 68 / 500 | iteration 15 / 30 | Total Loss: 4.14495849609375 | KNN Loss: 3.1076536178588867 | BCE Loss: 1.0373048782348633\n",
      "Epoch 68 / 500 | iteration 20 / 30 | Total Loss: 4.121479511260986 | KNN Loss: 3.0989487171173096 | BCE Loss: 1.0225307941436768\n",
      "Epoch 68 / 500 | iteration 25 / 30 | Total Loss: 4.117168426513672 | KNN Loss: 3.102631092071533 | BCE Loss: 1.0145373344421387\n",
      "Epoch 69 / 500 | iteration 0 / 30 | Total Loss: 4.1570634841918945 | KNN Loss: 3.1201794147491455 | BCE Loss: 1.0368839502334595\n",
      "Epoch 69 / 500 | iteration 5 / 30 | Total Loss: 4.188335418701172 | KNN Loss: 3.1374802589416504 | BCE Loss: 1.0508551597595215\n",
      "Epoch 69 / 500 | iteration 10 / 30 | Total Loss: 4.152124404907227 | KNN Loss: 3.1275830268859863 | BCE Loss: 1.0245412588119507\n",
      "Epoch 69 / 500 | iteration 15 / 30 | Total Loss: 4.133840560913086 | KNN Loss: 3.0932838916778564 | BCE Loss: 1.0405564308166504\n",
      "Epoch 69 / 500 | iteration 20 / 30 | Total Loss: 4.092665672302246 | KNN Loss: 3.08414626121521 | BCE Loss: 1.0085194110870361\n",
      "Epoch 69 / 500 | iteration 25 / 30 | Total Loss: 4.128602981567383 | KNN Loss: 3.0759189128875732 | BCE Loss: 1.0526840686798096\n",
      "Epoch 70 / 500 | iteration 0 / 30 | Total Loss: 4.13203239440918 | KNN Loss: 3.1128990650177 | BCE Loss: 1.0191333293914795\n",
      "Epoch 70 / 500 | iteration 5 / 30 | Total Loss: 4.1601033210754395 | KNN Loss: 3.132056713104248 | BCE Loss: 1.0280466079711914\n",
      "Epoch 70 / 500 | iteration 10 / 30 | Total Loss: 4.163002967834473 | KNN Loss: 3.1413118839263916 | BCE Loss: 1.0216912031173706\n",
      "Epoch 70 / 500 | iteration 15 / 30 | Total Loss: 4.126625061035156 | KNN Loss: 3.098179817199707 | BCE Loss: 1.0284452438354492\n",
      "Epoch 70 / 500 | iteration 20 / 30 | Total Loss: 4.135255813598633 | KNN Loss: 3.1179025173187256 | BCE Loss: 1.0173530578613281\n",
      "Epoch 70 / 500 | iteration 25 / 30 | Total Loss: 4.089612007141113 | KNN Loss: 3.1016318798065186 | BCE Loss: 0.9879801869392395\n",
      "Epoch 71 / 500 | iteration 0 / 30 | Total Loss: 4.155376434326172 | KNN Loss: 3.1114349365234375 | BCE Loss: 1.0439412593841553\n",
      "Epoch 71 / 500 | iteration 5 / 30 | Total Loss: 4.145138740539551 | KNN Loss: 3.122809886932373 | BCE Loss: 1.0223286151885986\n",
      "Epoch 71 / 500 | iteration 10 / 30 | Total Loss: 4.133502960205078 | KNN Loss: 3.1181793212890625 | BCE Loss: 1.0153236389160156\n",
      "Epoch 71 / 500 | iteration 15 / 30 | Total Loss: 4.129559516906738 | KNN Loss: 3.0785071849823 | BCE Loss: 1.0510523319244385\n",
      "Epoch 71 / 500 | iteration 20 / 30 | Total Loss: 4.122168064117432 | KNN Loss: 3.0979831218719482 | BCE Loss: 1.0241848230361938\n",
      "Epoch 71 / 500 | iteration 25 / 30 | Total Loss: 4.169039249420166 | KNN Loss: 3.1304540634155273 | BCE Loss: 1.0385851860046387\n",
      "Epoch 72 / 500 | iteration 0 / 30 | Total Loss: 4.144281387329102 | KNN Loss: 3.09303617477417 | BCE Loss: 1.0512449741363525\n",
      "Epoch 72 / 500 | iteration 5 / 30 | Total Loss: 4.127645969390869 | KNN Loss: 3.1053600311279297 | BCE Loss: 1.02228581905365\n",
      "Epoch 72 / 500 | iteration 10 / 30 | Total Loss: 4.110821723937988 | KNN Loss: 3.0901927947998047 | BCE Loss: 1.0206286907196045\n",
      "Epoch 72 / 500 | iteration 15 / 30 | Total Loss: 4.136974811553955 | KNN Loss: 3.105806589126587 | BCE Loss: 1.0311682224273682\n",
      "Epoch 72 / 500 | iteration 20 / 30 | Total Loss: 4.121550559997559 | KNN Loss: 3.1042797565460205 | BCE Loss: 1.0172709226608276\n",
      "Epoch 72 / 500 | iteration 25 / 30 | Total Loss: 4.1405534744262695 | KNN Loss: 3.103256940841675 | BCE Loss: 1.0372965335845947\n",
      "Epoch 73 / 500 | iteration 0 / 30 | Total Loss: 4.141757965087891 | KNN Loss: 3.133301019668579 | BCE Loss: 1.0084569454193115\n",
      "Epoch 73 / 500 | iteration 5 / 30 | Total Loss: 4.146064758300781 | KNN Loss: 3.098271608352661 | BCE Loss: 1.0477932691574097\n",
      "Epoch 73 / 500 | iteration 10 / 30 | Total Loss: 4.172722816467285 | KNN Loss: 3.141035318374634 | BCE Loss: 1.0316874980926514\n",
      "Epoch 73 / 500 | iteration 15 / 30 | Total Loss: 4.131160736083984 | KNN Loss: 3.12980580329895 | BCE Loss: 1.001354694366455\n",
      "Epoch 73 / 500 | iteration 20 / 30 | Total Loss: 4.089827060699463 | KNN Loss: 3.081911325454712 | BCE Loss: 1.0079156160354614\n",
      "Epoch 73 / 500 | iteration 25 / 30 | Total Loss: 4.113592147827148 | KNN Loss: 3.0934336185455322 | BCE Loss: 1.0201587677001953\n",
      "Epoch    74: reducing learning rate of group 0 to 3.5000e-03.\n",
      "Epoch 74 / 500 | iteration 0 / 30 | Total Loss: 4.158656120300293 | KNN Loss: 3.09892201423645 | BCE Loss: 1.0597341060638428\n",
      "Epoch 74 / 500 | iteration 5 / 30 | Total Loss: 4.114789962768555 | KNN Loss: 3.1095855236053467 | BCE Loss: 1.005204677581787\n",
      "Epoch 74 / 500 | iteration 10 / 30 | Total Loss: 4.146251678466797 | KNN Loss: 3.117657423019409 | BCE Loss: 1.0285940170288086\n",
      "Epoch 74 / 500 | iteration 15 / 30 | Total Loss: 4.147406578063965 | KNN Loss: 3.09651517868042 | BCE Loss: 1.050891637802124\n",
      "Epoch 74 / 500 | iteration 20 / 30 | Total Loss: 4.076035499572754 | KNN Loss: 3.05692458152771 | BCE Loss: 1.0191107988357544\n",
      "Epoch 74 / 500 | iteration 25 / 30 | Total Loss: 4.0916643142700195 | KNN Loss: 3.054800271987915 | BCE Loss: 1.0368642807006836\n",
      "Epoch 75 / 500 | iteration 0 / 30 | Total Loss: 4.100564956665039 | KNN Loss: 3.105780839920044 | BCE Loss: 0.9947842955589294\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75 / 500 | iteration 5 / 30 | Total Loss: 4.125607490539551 | KNN Loss: 3.1087586879730225 | BCE Loss: 1.0168486833572388\n",
      "Epoch 75 / 500 | iteration 10 / 30 | Total Loss: 4.123086452484131 | KNN Loss: 3.1121490001678467 | BCE Loss: 1.0109375715255737\n",
      "Epoch 75 / 500 | iteration 15 / 30 | Total Loss: 4.100192070007324 | KNN Loss: 3.0890591144561768 | BCE Loss: 1.0111329555511475\n",
      "Epoch 75 / 500 | iteration 20 / 30 | Total Loss: 4.167860984802246 | KNN Loss: 3.118926525115967 | BCE Loss: 1.0489342212677002\n",
      "Epoch 75 / 500 | iteration 25 / 30 | Total Loss: 4.104001045227051 | KNN Loss: 3.07480525970459 | BCE Loss: 1.0291955471038818\n",
      "Epoch 76 / 500 | iteration 0 / 30 | Total Loss: 4.150948524475098 | KNN Loss: 3.088179588317871 | BCE Loss: 1.0627690553665161\n",
      "Epoch 76 / 500 | iteration 5 / 30 | Total Loss: 4.140726089477539 | KNN Loss: 3.09153151512146 | BCE Loss: 1.049194574356079\n",
      "Epoch 76 / 500 | iteration 10 / 30 | Total Loss: 4.098135471343994 | KNN Loss: 3.070617437362671 | BCE Loss: 1.0275180339813232\n",
      "Epoch 76 / 500 | iteration 15 / 30 | Total Loss: 4.075664520263672 | KNN Loss: 3.0745530128479004 | BCE Loss: 1.0011112689971924\n",
      "Epoch 76 / 500 | iteration 20 / 30 | Total Loss: 4.068725109100342 | KNN Loss: 3.056609869003296 | BCE Loss: 1.0121153593063354\n",
      "Epoch 76 / 500 | iteration 25 / 30 | Total Loss: 4.12998104095459 | KNN Loss: 3.0866358280181885 | BCE Loss: 1.043345332145691\n",
      "Epoch 77 / 500 | iteration 0 / 30 | Total Loss: 4.139015197753906 | KNN Loss: 3.1169934272766113 | BCE Loss: 1.022022008895874\n",
      "Epoch 77 / 500 | iteration 5 / 30 | Total Loss: 4.130404472351074 | KNN Loss: 3.1095776557922363 | BCE Loss: 1.020826816558838\n",
      "Epoch 77 / 500 | iteration 10 / 30 | Total Loss: 4.144631385803223 | KNN Loss: 3.10292649269104 | BCE Loss: 1.0417048931121826\n",
      "Epoch 77 / 500 | iteration 15 / 30 | Total Loss: 4.111834526062012 | KNN Loss: 3.0907962322235107 | BCE Loss: 1.021038293838501\n",
      "Epoch 77 / 500 | iteration 20 / 30 | Total Loss: 4.100220680236816 | KNN Loss: 3.0661303997039795 | BCE Loss: 1.0340900421142578\n",
      "Epoch 77 / 500 | iteration 25 / 30 | Total Loss: 4.126595497131348 | KNN Loss: 3.083749294281006 | BCE Loss: 1.0428459644317627\n",
      "Epoch 78 / 500 | iteration 0 / 30 | Total Loss: 4.076746940612793 | KNN Loss: 3.0787298679351807 | BCE Loss: 0.9980168342590332\n",
      "Epoch 78 / 500 | iteration 5 / 30 | Total Loss: 4.082755088806152 | KNN Loss: 3.0908620357513428 | BCE Loss: 0.9918931722640991\n",
      "Epoch 78 / 500 | iteration 10 / 30 | Total Loss: 4.124423027038574 | KNN Loss: 3.078125 | BCE Loss: 1.0462982654571533\n",
      "Epoch 78 / 500 | iteration 15 / 30 | Total Loss: 4.1158528327941895 | KNN Loss: 3.08955979347229 | BCE Loss: 1.0262929201126099\n",
      "Epoch 78 / 500 | iteration 20 / 30 | Total Loss: 4.094260215759277 | KNN Loss: 3.0797829627990723 | BCE Loss: 1.0144771337509155\n",
      "Epoch 78 / 500 | iteration 25 / 30 | Total Loss: 4.079431056976318 | KNN Loss: 3.0721888542175293 | BCE Loss: 1.0072423219680786\n",
      "Epoch 79 / 500 | iteration 0 / 30 | Total Loss: 4.10491943359375 | KNN Loss: 3.066657304763794 | BCE Loss: 1.0382623672485352\n",
      "Epoch 79 / 500 | iteration 5 / 30 | Total Loss: 4.155296802520752 | KNN Loss: 3.089280366897583 | BCE Loss: 1.066016435623169\n",
      "Epoch 79 / 500 | iteration 10 / 30 | Total Loss: 4.114051342010498 | KNN Loss: 3.0920960903167725 | BCE Loss: 1.0219552516937256\n",
      "Epoch 79 / 500 | iteration 15 / 30 | Total Loss: 4.122869491577148 | KNN Loss: 3.104475975036621 | BCE Loss: 1.0183937549591064\n",
      "Epoch 79 / 500 | iteration 20 / 30 | Total Loss: 4.064382553100586 | KNN Loss: 3.0515706539154053 | BCE Loss: 1.0128120183944702\n",
      "Epoch 79 / 500 | iteration 25 / 30 | Total Loss: 4.074580192565918 | KNN Loss: 3.0741002559661865 | BCE Loss: 1.000480055809021\n",
      "Epoch 80 / 500 | iteration 0 / 30 | Total Loss: 4.113198757171631 | KNN Loss: 3.113267660140991 | BCE Loss: 0.9999310374259949\n",
      "Epoch 80 / 500 | iteration 5 / 30 | Total Loss: 4.117665767669678 | KNN Loss: 3.11253023147583 | BCE Loss: 1.0051355361938477\n",
      "Epoch 80 / 500 | iteration 10 / 30 | Total Loss: 4.147771835327148 | KNN Loss: 3.1004695892333984 | BCE Loss: 1.04730224609375\n",
      "Epoch 80 / 500 | iteration 15 / 30 | Total Loss: 4.174160957336426 | KNN Loss: 3.138240098953247 | BCE Loss: 1.0359207391738892\n",
      "Epoch 80 / 500 | iteration 20 / 30 | Total Loss: 4.104446887969971 | KNN Loss: 3.0699100494384766 | BCE Loss: 1.0345368385314941\n",
      "Epoch 80 / 500 | iteration 25 / 30 | Total Loss: 4.103466987609863 | KNN Loss: 3.0783839225769043 | BCE Loss: 1.0250828266143799\n",
      "Epoch 81 / 500 | iteration 0 / 30 | Total Loss: 4.158831596374512 | KNN Loss: 3.103400707244873 | BCE Loss: 1.0554308891296387\n",
      "Epoch 81 / 500 | iteration 5 / 30 | Total Loss: 4.121774196624756 | KNN Loss: 3.11466121673584 | BCE Loss: 1.007112979888916\n",
      "Epoch 81 / 500 | iteration 10 / 30 | Total Loss: 4.1242780685424805 | KNN Loss: 3.104041337966919 | BCE Loss: 1.0202369689941406\n",
      "Epoch 81 / 500 | iteration 15 / 30 | Total Loss: 4.10821533203125 | KNN Loss: 3.1032705307006836 | BCE Loss: 1.0049450397491455\n",
      "Epoch 81 / 500 | iteration 20 / 30 | Total Loss: 4.125576972961426 | KNN Loss: 3.085219144821167 | BCE Loss: 1.0403575897216797\n",
      "Epoch 81 / 500 | iteration 25 / 30 | Total Loss: 4.136963844299316 | KNN Loss: 3.1121702194213867 | BCE Loss: 1.0247933864593506\n",
      "Epoch 82 / 500 | iteration 0 / 30 | Total Loss: 4.102380275726318 | KNN Loss: 3.0882935523986816 | BCE Loss: 1.0140867233276367\n",
      "Epoch 82 / 500 | iteration 5 / 30 | Total Loss: 4.134746551513672 | KNN Loss: 3.086869716644287 | BCE Loss: 1.0478770732879639\n",
      "Epoch 82 / 500 | iteration 10 / 30 | Total Loss: 4.134011268615723 | KNN Loss: 3.1209309101104736 | BCE Loss: 1.0130804777145386\n",
      "Epoch 82 / 500 | iteration 15 / 30 | Total Loss: 4.083514213562012 | KNN Loss: 3.079432964324951 | BCE Loss: 1.0040812492370605\n",
      "Epoch 82 / 500 | iteration 20 / 30 | Total Loss: 4.122677803039551 | KNN Loss: 3.1045210361480713 | BCE Loss: 1.0181570053100586\n",
      "Epoch 82 / 500 | iteration 25 / 30 | Total Loss: 4.1538405418396 | KNN Loss: 3.121101140975952 | BCE Loss: 1.0327394008636475\n",
      "Epoch 83 / 500 | iteration 0 / 30 | Total Loss: 4.109994411468506 | KNN Loss: 3.107638359069824 | BCE Loss: 1.0023560523986816\n",
      "Epoch 83 / 500 | iteration 5 / 30 | Total Loss: 4.166259288787842 | KNN Loss: 3.1182188987731934 | BCE Loss: 1.0480403900146484\n",
      "Epoch 83 / 500 | iteration 10 / 30 | Total Loss: 4.140474796295166 | KNN Loss: 3.117918014526367 | BCE Loss: 1.0225569009780884\n",
      "Epoch 83 / 500 | iteration 15 / 30 | Total Loss: 4.108994483947754 | KNN Loss: 3.0971925258636475 | BCE Loss: 1.0118019580841064\n",
      "Epoch 83 / 500 | iteration 20 / 30 | Total Loss: 4.1237030029296875 | KNN Loss: 3.0992398262023926 | BCE Loss: 1.0244629383087158\n",
      "Epoch 83 / 500 | iteration 25 / 30 | Total Loss: 4.159344673156738 | KNN Loss: 3.087682008743286 | BCE Loss: 1.071662425994873\n",
      "Epoch 84 / 500 | iteration 0 / 30 | Total Loss: 4.128230094909668 | KNN Loss: 3.112896203994751 | BCE Loss: 1.015333652496338\n",
      "Epoch 84 / 500 | iteration 5 / 30 | Total Loss: 4.1267499923706055 | KNN Loss: 3.1209027767181396 | BCE Loss: 1.0058469772338867\n",
      "Epoch 84 / 500 | iteration 10 / 30 | Total Loss: 4.161482334136963 | KNN Loss: 3.1180872917175293 | BCE Loss: 1.0433950424194336\n",
      "Epoch 84 / 500 | iteration 15 / 30 | Total Loss: 4.1563920974731445 | KNN Loss: 3.1258082389831543 | BCE Loss: 1.0305839776992798\n",
      "Epoch 84 / 500 | iteration 20 / 30 | Total Loss: 4.133166790008545 | KNN Loss: 3.0952162742614746 | BCE Loss: 1.0379505157470703\n",
      "Epoch 84 / 500 | iteration 25 / 30 | Total Loss: 4.09783935546875 | KNN Loss: 3.0846924781799316 | BCE Loss: 1.0131471157073975\n",
      "Epoch 85 / 500 | iteration 0 / 30 | Total Loss: 4.099473476409912 | KNN Loss: 3.0947232246398926 | BCE Loss: 1.0047502517700195\n",
      "Epoch 85 / 500 | iteration 5 / 30 | Total Loss: 4.11668062210083 | KNN Loss: 3.097841739654541 | BCE Loss: 1.018838882446289\n",
      "Epoch 85 / 500 | iteration 10 / 30 | Total Loss: 4.108458518981934 | KNN Loss: 3.090996742248535 | BCE Loss: 1.0174616575241089\n",
      "Epoch 85 / 500 | iteration 15 / 30 | Total Loss: 4.096604347229004 | KNN Loss: 3.075636863708496 | BCE Loss: 1.0209674835205078\n",
      "Epoch 85 / 500 | iteration 20 / 30 | Total Loss: 4.114081859588623 | KNN Loss: 3.112814426422119 | BCE Loss: 1.001267433166504\n",
      "Epoch 85 / 500 | iteration 25 / 30 | Total Loss: 4.113489627838135 | KNN Loss: 3.1019949913024902 | BCE Loss: 1.0114946365356445\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86 / 500 | iteration 0 / 30 | Total Loss: 4.119589328765869 | KNN Loss: 3.0916733741760254 | BCE Loss: 1.0279159545898438\n",
      "Epoch 86 / 500 | iteration 5 / 30 | Total Loss: 4.151787757873535 | KNN Loss: 3.115217685699463 | BCE Loss: 1.0365698337554932\n",
      "Epoch 86 / 500 | iteration 10 / 30 | Total Loss: 4.180403709411621 | KNN Loss: 3.1145479679107666 | BCE Loss: 1.0658559799194336\n",
      "Epoch 86 / 500 | iteration 15 / 30 | Total Loss: 4.167194366455078 | KNN Loss: 3.1339354515075684 | BCE Loss: 1.0332587957382202\n",
      "Epoch 86 / 500 | iteration 20 / 30 | Total Loss: 4.0770111083984375 | KNN Loss: 3.0747954845428467 | BCE Loss: 1.0022157430648804\n",
      "Epoch 86 / 500 | iteration 25 / 30 | Total Loss: 4.0813140869140625 | KNN Loss: 3.0861048698425293 | BCE Loss: 0.9952092170715332\n",
      "Epoch 87 / 500 | iteration 0 / 30 | Total Loss: 4.140326499938965 | KNN Loss: 3.091454029083252 | BCE Loss: 1.048872709274292\n",
      "Epoch 87 / 500 | iteration 5 / 30 | Total Loss: 4.075321674346924 | KNN Loss: 3.0539586544036865 | BCE Loss: 1.0213631391525269\n",
      "Epoch 87 / 500 | iteration 10 / 30 | Total Loss: 4.093046188354492 | KNN Loss: 3.091163396835327 | BCE Loss: 1.0018830299377441\n",
      "Epoch 87 / 500 | iteration 15 / 30 | Total Loss: 4.100989818572998 | KNN Loss: 3.067455768585205 | BCE Loss: 1.033534049987793\n",
      "Epoch 87 / 500 | iteration 20 / 30 | Total Loss: 4.103997707366943 | KNN Loss: 3.0405211448669434 | BCE Loss: 1.0634766817092896\n",
      "Epoch 87 / 500 | iteration 25 / 30 | Total Loss: 4.11669921875 | KNN Loss: 3.0923633575439453 | BCE Loss: 1.0243359804153442\n",
      "Epoch 88 / 500 | iteration 0 / 30 | Total Loss: 4.084821701049805 | KNN Loss: 3.0868382453918457 | BCE Loss: 0.9979835152626038\n",
      "Epoch 88 / 500 | iteration 5 / 30 | Total Loss: 4.139655590057373 | KNN Loss: 3.1117849349975586 | BCE Loss: 1.0278706550598145\n",
      "Epoch 88 / 500 | iteration 10 / 30 | Total Loss: 4.0984296798706055 | KNN Loss: 3.0892374515533447 | BCE Loss: 1.0091923475265503\n",
      "Epoch 88 / 500 | iteration 15 / 30 | Total Loss: 4.120246887207031 | KNN Loss: 3.0946712493896484 | BCE Loss: 1.025575876235962\n",
      "Epoch 88 / 500 | iteration 20 / 30 | Total Loss: 4.07472562789917 | KNN Loss: 3.0507616996765137 | BCE Loss: 1.0239639282226562\n",
      "Epoch 88 / 500 | iteration 25 / 30 | Total Loss: 4.1191487312316895 | KNN Loss: 3.074063777923584 | BCE Loss: 1.0450849533081055\n",
      "Epoch 89 / 500 | iteration 0 / 30 | Total Loss: 4.152737617492676 | KNN Loss: 3.0880868434906006 | BCE Loss: 1.0646507740020752\n",
      "Epoch 89 / 500 | iteration 5 / 30 | Total Loss: 4.147324085235596 | KNN Loss: 3.0998024940490723 | BCE Loss: 1.0475215911865234\n",
      "Epoch 89 / 500 | iteration 10 / 30 | Total Loss: 4.140732288360596 | KNN Loss: 3.1113362312316895 | BCE Loss: 1.0293959379196167\n",
      "Epoch 89 / 500 | iteration 15 / 30 | Total Loss: 4.121297359466553 | KNN Loss: 3.1235387325286865 | BCE Loss: 0.997758686542511\n",
      "Epoch 89 / 500 | iteration 20 / 30 | Total Loss: 4.093652248382568 | KNN Loss: 3.088846445083618 | BCE Loss: 1.0048058032989502\n",
      "Epoch 89 / 500 | iteration 25 / 30 | Total Loss: 4.148325443267822 | KNN Loss: 3.1249823570251465 | BCE Loss: 1.0233430862426758\n",
      "Epoch 90 / 500 | iteration 0 / 30 | Total Loss: 4.060123443603516 | KNN Loss: 3.0374646186828613 | BCE Loss: 1.0226588249206543\n",
      "Epoch 90 / 500 | iteration 5 / 30 | Total Loss: 4.106259346008301 | KNN Loss: 3.0749850273132324 | BCE Loss: 1.0312745571136475\n",
      "Epoch 90 / 500 | iteration 10 / 30 | Total Loss: 4.130536079406738 | KNN Loss: 3.091698169708252 | BCE Loss: 1.0388379096984863\n",
      "Epoch 90 / 500 | iteration 15 / 30 | Total Loss: 4.13067626953125 | KNN Loss: 3.0992743968963623 | BCE Loss: 1.0314021110534668\n",
      "Epoch 90 / 500 | iteration 20 / 30 | Total Loss: 4.082207679748535 | KNN Loss: 3.063523292541504 | BCE Loss: 1.0186846256256104\n",
      "Epoch 90 / 500 | iteration 25 / 30 | Total Loss: 4.0740203857421875 | KNN Loss: 3.08321213722229 | BCE Loss: 0.9908083081245422\n",
      "Epoch 91 / 500 | iteration 0 / 30 | Total Loss: 4.087536811828613 | KNN Loss: 3.076803684234619 | BCE Loss: 1.0107333660125732\n",
      "Epoch 91 / 500 | iteration 5 / 30 | Total Loss: 4.1273298263549805 | KNN Loss: 3.110281229019165 | BCE Loss: 1.0170485973358154\n",
      "Epoch 91 / 500 | iteration 10 / 30 | Total Loss: 4.153451919555664 | KNN Loss: 3.100407123565674 | BCE Loss: 1.0530447959899902\n",
      "Epoch 91 / 500 | iteration 15 / 30 | Total Loss: 4.096824645996094 | KNN Loss: 3.087552547454834 | BCE Loss: 1.0092723369598389\n",
      "Epoch 91 / 500 | iteration 20 / 30 | Total Loss: 4.074443817138672 | KNN Loss: 3.0770184993743896 | BCE Loss: 0.9974251985549927\n",
      "Epoch 91 / 500 | iteration 25 / 30 | Total Loss: 4.090134620666504 | KNN Loss: 3.0614852905273438 | BCE Loss: 1.028649091720581\n",
      "Epoch 92 / 500 | iteration 0 / 30 | Total Loss: 4.091291427612305 | KNN Loss: 3.07169246673584 | BCE Loss: 1.0195989608764648\n",
      "Epoch 92 / 500 | iteration 5 / 30 | Total Loss: 4.119995594024658 | KNN Loss: 3.1054842472076416 | BCE Loss: 1.0145114660263062\n",
      "Epoch 92 / 500 | iteration 10 / 30 | Total Loss: 4.111064910888672 | KNN Loss: 3.084071159362793 | BCE Loss: 1.026993751525879\n",
      "Epoch 92 / 500 | iteration 15 / 30 | Total Loss: 4.083182334899902 | KNN Loss: 3.0688464641571045 | BCE Loss: 1.0143357515335083\n",
      "Epoch 92 / 500 | iteration 20 / 30 | Total Loss: 4.108279228210449 | KNN Loss: 3.075096845626831 | BCE Loss: 1.0331823825836182\n",
      "Epoch 92 / 500 | iteration 25 / 30 | Total Loss: 4.1361589431762695 | KNN Loss: 3.078155994415283 | BCE Loss: 1.0580029487609863\n",
      "Epoch 93 / 500 | iteration 0 / 30 | Total Loss: 4.136048316955566 | KNN Loss: 3.0955519676208496 | BCE Loss: 1.0404963493347168\n",
      "Epoch 93 / 500 | iteration 5 / 30 | Total Loss: 4.11822509765625 | KNN Loss: 3.080626964569092 | BCE Loss: 1.0375982522964478\n",
      "Epoch 93 / 500 | iteration 10 / 30 | Total Loss: 4.047915458679199 | KNN Loss: 3.0563127994537354 | BCE Loss: 0.9916025400161743\n",
      "Epoch 93 / 500 | iteration 15 / 30 | Total Loss: 4.118579387664795 | KNN Loss: 3.091092824935913 | BCE Loss: 1.0274865627288818\n",
      "Epoch 93 / 500 | iteration 20 / 30 | Total Loss: 4.103700160980225 | KNN Loss: 3.0657894611358643 | BCE Loss: 1.03791081905365\n",
      "Epoch 93 / 500 | iteration 25 / 30 | Total Loss: 4.096368312835693 | KNN Loss: 3.069023847579956 | BCE Loss: 1.0273444652557373\n",
      "Epoch 94 / 500 | iteration 0 / 30 | Total Loss: 4.095719337463379 | KNN Loss: 3.0971665382385254 | BCE Loss: 0.9985529184341431\n",
      "Epoch 94 / 500 | iteration 5 / 30 | Total Loss: 4.1481781005859375 | KNN Loss: 3.097696542739868 | BCE Loss: 1.0504817962646484\n",
      "Epoch 94 / 500 | iteration 10 / 30 | Total Loss: 4.116548538208008 | KNN Loss: 3.096101999282837 | BCE Loss: 1.02044677734375\n",
      "Epoch 94 / 500 | iteration 15 / 30 | Total Loss: 4.073330879211426 | KNN Loss: 3.053234338760376 | BCE Loss: 1.0200966596603394\n",
      "Epoch 94 / 500 | iteration 20 / 30 | Total Loss: 4.111258029937744 | KNN Loss: 3.1043224334716797 | BCE Loss: 1.0069355964660645\n",
      "Epoch 94 / 500 | iteration 25 / 30 | Total Loss: 4.137224197387695 | KNN Loss: 3.1019539833068848 | BCE Loss: 1.0352704524993896\n",
      "Epoch 95 / 500 | iteration 0 / 30 | Total Loss: 4.089152812957764 | KNN Loss: 3.0781760215759277 | BCE Loss: 1.0109766721725464\n",
      "Epoch 95 / 500 | iteration 5 / 30 | Total Loss: 4.098191738128662 | KNN Loss: 3.0946807861328125 | BCE Loss: 1.00351083278656\n",
      "Epoch 95 / 500 | iteration 10 / 30 | Total Loss: 4.149362564086914 | KNN Loss: 3.1301615238189697 | BCE Loss: 1.0192008018493652\n",
      "Epoch 95 / 500 | iteration 15 / 30 | Total Loss: 4.098111152648926 | KNN Loss: 3.098109245300293 | BCE Loss: 1.0000019073486328\n",
      "Epoch 95 / 500 | iteration 20 / 30 | Total Loss: 4.117602825164795 | KNN Loss: 3.0929160118103027 | BCE Loss: 1.0246868133544922\n",
      "Epoch 95 / 500 | iteration 25 / 30 | Total Loss: 4.107433319091797 | KNN Loss: 3.102412462234497 | BCE Loss: 1.005021095275879\n",
      "Epoch 96 / 500 | iteration 0 / 30 | Total Loss: 4.104151248931885 | KNN Loss: 3.0830812454223633 | BCE Loss: 1.0210700035095215\n",
      "Epoch 96 / 500 | iteration 5 / 30 | Total Loss: 4.124393463134766 | KNN Loss: 3.1001336574554443 | BCE Loss: 1.0242599248886108\n",
      "Epoch 96 / 500 | iteration 10 / 30 | Total Loss: 4.099001884460449 | KNN Loss: 3.0723931789398193 | BCE Loss: 1.026608943939209\n",
      "Epoch 96 / 500 | iteration 15 / 30 | Total Loss: 4.095518112182617 | KNN Loss: 3.062847137451172 | BCE Loss: 1.0326710939407349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96 / 500 | iteration 20 / 30 | Total Loss: 4.152474403381348 | KNN Loss: 3.1283633708953857 | BCE Loss: 1.0241107940673828\n",
      "Epoch 96 / 500 | iteration 25 / 30 | Total Loss: 4.121400833129883 | KNN Loss: 3.1151132583618164 | BCE Loss: 1.0062874555587769\n",
      "Epoch 97 / 500 | iteration 0 / 30 | Total Loss: 4.149895191192627 | KNN Loss: 3.139977216720581 | BCE Loss: 1.009917974472046\n",
      "Epoch 97 / 500 | iteration 5 / 30 | Total Loss: 4.170612335205078 | KNN Loss: 3.137399673461914 | BCE Loss: 1.033212661743164\n",
      "Epoch 97 / 500 | iteration 10 / 30 | Total Loss: 4.135873317718506 | KNN Loss: 3.1026041507720947 | BCE Loss: 1.0332691669464111\n",
      "Epoch 97 / 500 | iteration 15 / 30 | Total Loss: 4.139756202697754 | KNN Loss: 3.085341215133667 | BCE Loss: 1.054415225982666\n",
      "Epoch 97 / 500 | iteration 20 / 30 | Total Loss: 4.106787204742432 | KNN Loss: 3.083601236343384 | BCE Loss: 1.0231859683990479\n",
      "Epoch 97 / 500 | iteration 25 / 30 | Total Loss: 4.129411220550537 | KNN Loss: 3.100297451019287 | BCE Loss: 1.02911376953125\n",
      "Epoch 98 / 500 | iteration 0 / 30 | Total Loss: 4.132912635803223 | KNN Loss: 3.100661277770996 | BCE Loss: 1.0322514772415161\n",
      "Epoch 98 / 500 | iteration 5 / 30 | Total Loss: 4.14930534362793 | KNN Loss: 3.1221859455108643 | BCE Loss: 1.0271191596984863\n",
      "Epoch 98 / 500 | iteration 10 / 30 | Total Loss: 4.153506278991699 | KNN Loss: 3.1199049949645996 | BCE Loss: 1.0336015224456787\n",
      "Epoch 98 / 500 | iteration 15 / 30 | Total Loss: 4.166013240814209 | KNN Loss: 3.1430506706237793 | BCE Loss: 1.0229626893997192\n",
      "Epoch 98 / 500 | iteration 20 / 30 | Total Loss: 4.150254249572754 | KNN Loss: 3.114802837371826 | BCE Loss: 1.0354514122009277\n",
      "Epoch 98 / 500 | iteration 25 / 30 | Total Loss: 4.053098678588867 | KNN Loss: 3.0352580547332764 | BCE Loss: 1.01784086227417\n",
      "Epoch 99 / 500 | iteration 0 / 30 | Total Loss: 4.103087902069092 | KNN Loss: 3.085906982421875 | BCE Loss: 1.0171809196472168\n",
      "Epoch 99 / 500 | iteration 5 / 30 | Total Loss: 4.158013820648193 | KNN Loss: 3.1207540035247803 | BCE Loss: 1.0372599363327026\n",
      "Epoch 99 / 500 | iteration 10 / 30 | Total Loss: 4.079315662384033 | KNN Loss: 3.0703964233398438 | BCE Loss: 1.0089191198349\n",
      "Epoch 99 / 500 | iteration 15 / 30 | Total Loss: 4.124375343322754 | KNN Loss: 3.098341226577759 | BCE Loss: 1.026033878326416\n",
      "Epoch 99 / 500 | iteration 20 / 30 | Total Loss: 4.094799995422363 | KNN Loss: 3.0708391666412354 | BCE Loss: 1.023961067199707\n",
      "Epoch 99 / 500 | iteration 25 / 30 | Total Loss: 4.117504119873047 | KNN Loss: 3.0690221786499023 | BCE Loss: 1.048481822013855\n",
      "Epoch 100 / 500 | iteration 0 / 30 | Total Loss: 4.108353614807129 | KNN Loss: 3.0929718017578125 | BCE Loss: 1.0153820514678955\n",
      "Epoch 100 / 500 | iteration 5 / 30 | Total Loss: 4.12289571762085 | KNN Loss: 3.120182752609253 | BCE Loss: 1.0027130842208862\n",
      "Epoch 100 / 500 | iteration 10 / 30 | Total Loss: 4.134371757507324 | KNN Loss: 3.0917301177978516 | BCE Loss: 1.0426418781280518\n",
      "Epoch 100 / 500 | iteration 15 / 30 | Total Loss: 4.129937171936035 | KNN Loss: 3.1058764457702637 | BCE Loss: 1.0240607261657715\n",
      "Epoch 100 / 500 | iteration 20 / 30 | Total Loss: 4.073971748352051 | KNN Loss: 3.064239501953125 | BCE Loss: 1.0097324848175049\n",
      "Epoch 100 / 500 | iteration 25 / 30 | Total Loss: 4.091889381408691 | KNN Loss: 3.0514256954193115 | BCE Loss: 1.0404636859893799\n",
      "Epoch 101 / 500 | iteration 0 / 30 | Total Loss: 4.120934963226318 | KNN Loss: 3.1062934398651123 | BCE Loss: 1.014641523361206\n",
      "Epoch 101 / 500 | iteration 5 / 30 | Total Loss: 4.095696449279785 | KNN Loss: 3.0928795337677 | BCE Loss: 1.0028170347213745\n",
      "Epoch 101 / 500 | iteration 10 / 30 | Total Loss: 4.1355180740356445 | KNN Loss: 3.113222122192383 | BCE Loss: 1.0222959518432617\n",
      "Epoch 101 / 500 | iteration 15 / 30 | Total Loss: 4.117025375366211 | KNN Loss: 3.1027138233184814 | BCE Loss: 1.0143117904663086\n",
      "Epoch 101 / 500 | iteration 20 / 30 | Total Loss: 4.1190948486328125 | KNN Loss: 3.1241323947906494 | BCE Loss: 0.9949623346328735\n",
      "Epoch 101 / 500 | iteration 25 / 30 | Total Loss: 4.105001449584961 | KNN Loss: 3.0859079360961914 | BCE Loss: 1.0190935134887695\n",
      "Epoch   102: reducing learning rate of group 0 to 2.4500e-03.\n",
      "Epoch 102 / 500 | iteration 0 / 30 | Total Loss: 4.095435619354248 | KNN Loss: 3.0827479362487793 | BCE Loss: 1.0126875638961792\n",
      "Epoch 102 / 500 | iteration 5 / 30 | Total Loss: 4.1394548416137695 | KNN Loss: 3.1046388149261475 | BCE Loss: 1.0348162651062012\n",
      "Epoch 102 / 500 | iteration 10 / 30 | Total Loss: 4.141558647155762 | KNN Loss: 3.10776948928833 | BCE Loss: 1.0337893962860107\n",
      "Epoch 102 / 500 | iteration 15 / 30 | Total Loss: 4.14078950881958 | KNN Loss: 3.094392776489258 | BCE Loss: 1.0463966131210327\n",
      "Epoch 102 / 500 | iteration 20 / 30 | Total Loss: 4.120301246643066 | KNN Loss: 3.1067090034484863 | BCE Loss: 1.01359224319458\n",
      "Epoch 102 / 500 | iteration 25 / 30 | Total Loss: 4.104090213775635 | KNN Loss: 3.070223331451416 | BCE Loss: 1.0338668823242188\n",
      "Epoch 103 / 500 | iteration 0 / 30 | Total Loss: 4.073470115661621 | KNN Loss: 3.0800929069519043 | BCE Loss: 0.9933769702911377\n",
      "Epoch 103 / 500 | iteration 5 / 30 | Total Loss: 4.1615095138549805 | KNN Loss: 3.1009726524353027 | BCE Loss: 1.0605366230010986\n",
      "Epoch 103 / 500 | iteration 10 / 30 | Total Loss: 4.142816066741943 | KNN Loss: 3.09613299369812 | BCE Loss: 1.0466831922531128\n",
      "Epoch 103 / 500 | iteration 15 / 30 | Total Loss: 4.13861083984375 | KNN Loss: 3.0963597297668457 | BCE Loss: 1.0422511100769043\n",
      "Epoch 103 / 500 | iteration 20 / 30 | Total Loss: 4.1205925941467285 | KNN Loss: 3.089240550994873 | BCE Loss: 1.031352162361145\n",
      "Epoch 103 / 500 | iteration 25 / 30 | Total Loss: 4.07544469833374 | KNN Loss: 3.0506253242492676 | BCE Loss: 1.0248193740844727\n",
      "Epoch 104 / 500 | iteration 0 / 30 | Total Loss: 4.112451553344727 | KNN Loss: 3.073835611343384 | BCE Loss: 1.0386161804199219\n",
      "Epoch 104 / 500 | iteration 5 / 30 | Total Loss: 4.124824523925781 | KNN Loss: 3.103902578353882 | BCE Loss: 1.0209219455718994\n",
      "Epoch 104 / 500 | iteration 10 / 30 | Total Loss: 4.083748817443848 | KNN Loss: 3.083158016204834 | BCE Loss: 1.0005905628204346\n",
      "Epoch 104 / 500 | iteration 15 / 30 | Total Loss: 4.138650894165039 | KNN Loss: 3.1129519939422607 | BCE Loss: 1.0256991386413574\n",
      "Epoch 104 / 500 | iteration 20 / 30 | Total Loss: 4.133800983428955 | KNN Loss: 3.1216588020324707 | BCE Loss: 1.012142300605774\n",
      "Epoch 104 / 500 | iteration 25 / 30 | Total Loss: 4.142733573913574 | KNN Loss: 3.1014232635498047 | BCE Loss: 1.0413105487823486\n",
      "Epoch 105 / 500 | iteration 0 / 30 | Total Loss: 4.117766380310059 | KNN Loss: 3.0924763679504395 | BCE Loss: 1.0252902507781982\n",
      "Epoch 105 / 500 | iteration 5 / 30 | Total Loss: 4.146895408630371 | KNN Loss: 3.1285455226898193 | BCE Loss: 1.0183497667312622\n",
      "Epoch 105 / 500 | iteration 10 / 30 | Total Loss: 4.143545150756836 | KNN Loss: 3.1288201808929443 | BCE Loss: 1.0147247314453125\n",
      "Epoch 105 / 500 | iteration 15 / 30 | Total Loss: 4.16030216217041 | KNN Loss: 3.131983757019043 | BCE Loss: 1.0283184051513672\n",
      "Epoch 105 / 500 | iteration 20 / 30 | Total Loss: 4.115782737731934 | KNN Loss: 3.0698864459991455 | BCE Loss: 1.045896291732788\n",
      "Epoch 105 / 500 | iteration 25 / 30 | Total Loss: 4.081499099731445 | KNN Loss: 3.071160316467285 | BCE Loss: 1.0103387832641602\n",
      "Epoch 106 / 500 | iteration 0 / 30 | Total Loss: 4.10002326965332 | KNN Loss: 3.0751211643218994 | BCE Loss: 1.024902105331421\n",
      "Epoch 106 / 500 | iteration 5 / 30 | Total Loss: 4.094939708709717 | KNN Loss: 3.0950989723205566 | BCE Loss: 0.9998405575752258\n",
      "Epoch 106 / 500 | iteration 10 / 30 | Total Loss: 4.132927894592285 | KNN Loss: 3.109510898590088 | BCE Loss: 1.0234172344207764\n",
      "Epoch 106 / 500 | iteration 15 / 30 | Total Loss: 4.1392822265625 | KNN Loss: 3.079446315765381 | BCE Loss: 1.0598359107971191\n",
      "Epoch 106 / 500 | iteration 20 / 30 | Total Loss: 4.101421356201172 | KNN Loss: 3.108917713165283 | BCE Loss: 0.9925037026405334\n",
      "Epoch 106 / 500 | iteration 25 / 30 | Total Loss: 4.094980239868164 | KNN Loss: 3.0680103302001953 | BCE Loss: 1.0269700288772583\n",
      "Epoch 107 / 500 | iteration 0 / 30 | Total Loss: 4.166618347167969 | KNN Loss: 3.151787519454956 | BCE Loss: 1.0148310661315918\n",
      "Epoch 107 / 500 | iteration 5 / 30 | Total Loss: 4.112334251403809 | KNN Loss: 3.1071953773498535 | BCE Loss: 1.0051391124725342\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 107 / 500 | iteration 10 / 30 | Total Loss: 4.162463188171387 | KNN Loss: 3.1162052154541016 | BCE Loss: 1.0462580919265747\n",
      "Epoch 107 / 500 | iteration 15 / 30 | Total Loss: 4.099435329437256 | KNN Loss: 3.073514938354492 | BCE Loss: 1.0259203910827637\n",
      "Epoch 107 / 500 | iteration 20 / 30 | Total Loss: 4.131713390350342 | KNN Loss: 3.1121251583099365 | BCE Loss: 1.0195882320404053\n",
      "Epoch 107 / 500 | iteration 25 / 30 | Total Loss: 4.130538463592529 | KNN Loss: 3.0965569019317627 | BCE Loss: 1.0339815616607666\n",
      "Epoch 108 / 500 | iteration 0 / 30 | Total Loss: 4.085044860839844 | KNN Loss: 3.093395471572876 | BCE Loss: 0.9916493892669678\n",
      "Epoch 108 / 500 | iteration 5 / 30 | Total Loss: 4.1126298904418945 | KNN Loss: 3.1008825302124023 | BCE Loss: 1.0117472410202026\n",
      "Epoch 108 / 500 | iteration 10 / 30 | Total Loss: 4.131220817565918 | KNN Loss: 3.103867769241333 | BCE Loss: 1.027353048324585\n",
      "Epoch 108 / 500 | iteration 15 / 30 | Total Loss: 4.147238731384277 | KNN Loss: 3.1005184650421143 | BCE Loss: 1.0467205047607422\n",
      "Epoch 108 / 500 | iteration 20 / 30 | Total Loss: 4.086483478546143 | KNN Loss: 3.0860960483551025 | BCE Loss: 1.0003873109817505\n",
      "Epoch 108 / 500 | iteration 25 / 30 | Total Loss: 4.107933044433594 | KNN Loss: 3.0970191955566406 | BCE Loss: 1.0109140872955322\n",
      "Epoch 109 / 500 | iteration 0 / 30 | Total Loss: 4.089868545532227 | KNN Loss: 3.082642078399658 | BCE Loss: 1.0072267055511475\n",
      "Epoch 109 / 500 | iteration 5 / 30 | Total Loss: 4.121811866760254 | KNN Loss: 3.0824456214904785 | BCE Loss: 1.0393664836883545\n",
      "Epoch 109 / 500 | iteration 10 / 30 | Total Loss: 4.147857189178467 | KNN Loss: 3.1303722858428955 | BCE Loss: 1.0174849033355713\n",
      "Epoch 109 / 500 | iteration 15 / 30 | Total Loss: 4.090235710144043 | KNN Loss: 3.0824007987976074 | BCE Loss: 1.0078346729278564\n",
      "Epoch 109 / 500 | iteration 20 / 30 | Total Loss: 4.069537162780762 | KNN Loss: 3.0513205528259277 | BCE Loss: 1.018216609954834\n",
      "Epoch 109 / 500 | iteration 25 / 30 | Total Loss: 4.1350836753845215 | KNN Loss: 3.102640151977539 | BCE Loss: 1.032443642616272\n",
      "Epoch 110 / 500 | iteration 0 / 30 | Total Loss: 4.132626056671143 | KNN Loss: 3.104325532913208 | BCE Loss: 1.0283005237579346\n",
      "Epoch 110 / 500 | iteration 5 / 30 | Total Loss: 4.08189582824707 | KNN Loss: 3.0695621967315674 | BCE Loss: 1.012333869934082\n",
      "Epoch 110 / 500 | iteration 10 / 30 | Total Loss: 4.097658157348633 | KNN Loss: 3.096857786178589 | BCE Loss: 1.0008002519607544\n",
      "Epoch 110 / 500 | iteration 15 / 30 | Total Loss: 4.063264846801758 | KNN Loss: 3.0597422122955322 | BCE Loss: 1.0035228729248047\n",
      "Epoch 110 / 500 | iteration 20 / 30 | Total Loss: 4.118488311767578 | KNN Loss: 3.075777769088745 | BCE Loss: 1.042710542678833\n",
      "Epoch 110 / 500 | iteration 25 / 30 | Total Loss: 4.092310905456543 | KNN Loss: 3.0692479610443115 | BCE Loss: 1.0230629444122314\n",
      "Epoch 111 / 500 | iteration 0 / 30 | Total Loss: 4.11589241027832 | KNN Loss: 3.0786020755767822 | BCE Loss: 1.037290096282959\n",
      "Epoch 111 / 500 | iteration 5 / 30 | Total Loss: 4.135215759277344 | KNN Loss: 3.0939016342163086 | BCE Loss: 1.041313886642456\n",
      "Epoch 111 / 500 | iteration 10 / 30 | Total Loss: 4.119526386260986 | KNN Loss: 3.101058006286621 | BCE Loss: 1.0184682607650757\n",
      "Epoch 111 / 500 | iteration 15 / 30 | Total Loss: 4.106600284576416 | KNN Loss: 3.0911667346954346 | BCE Loss: 1.015433430671692\n",
      "Epoch 111 / 500 | iteration 20 / 30 | Total Loss: 4.100019931793213 | KNN Loss: 3.0513720512390137 | BCE Loss: 1.0486478805541992\n",
      "Epoch 111 / 500 | iteration 25 / 30 | Total Loss: 4.119146823883057 | KNN Loss: 3.1057846546173096 | BCE Loss: 1.013362169265747\n",
      "Epoch 112 / 500 | iteration 0 / 30 | Total Loss: 4.143370628356934 | KNN Loss: 3.1112897396087646 | BCE Loss: 1.0320806503295898\n",
      "Epoch 112 / 500 | iteration 5 / 30 | Total Loss: 4.12000846862793 | KNN Loss: 3.074387550354004 | BCE Loss: 1.0456209182739258\n",
      "Epoch 112 / 500 | iteration 10 / 30 | Total Loss: 4.1313018798828125 | KNN Loss: 3.092649221420288 | BCE Loss: 1.0386526584625244\n",
      "Epoch 112 / 500 | iteration 15 / 30 | Total Loss: 4.074529647827148 | KNN Loss: 3.07065486907959 | BCE Loss: 1.0038750171661377\n",
      "Epoch 112 / 500 | iteration 20 / 30 | Total Loss: 4.111298561096191 | KNN Loss: 3.068942070007324 | BCE Loss: 1.0423564910888672\n",
      "Epoch 112 / 500 | iteration 25 / 30 | Total Loss: 4.086611747741699 | KNN Loss: 3.061368703842163 | BCE Loss: 1.0252432823181152\n",
      "Epoch 113 / 500 | iteration 0 / 30 | Total Loss: 4.1358113288879395 | KNN Loss: 3.0682220458984375 | BCE Loss: 1.0675894021987915\n",
      "Epoch 113 / 500 | iteration 5 / 30 | Total Loss: 4.106083869934082 | KNN Loss: 3.0871214866638184 | BCE Loss: 1.0189626216888428\n",
      "Epoch 113 / 500 | iteration 10 / 30 | Total Loss: 4.074458122253418 | KNN Loss: 3.0618557929992676 | BCE Loss: 1.0126025676727295\n",
      "Epoch 113 / 500 | iteration 15 / 30 | Total Loss: 4.131065845489502 | KNN Loss: 3.113283157348633 | BCE Loss: 1.0177826881408691\n",
      "Epoch 113 / 500 | iteration 20 / 30 | Total Loss: 4.0850443840026855 | KNN Loss: 3.0691492557525635 | BCE Loss: 1.0158952474594116\n",
      "Epoch 113 / 500 | iteration 25 / 30 | Total Loss: 4.122635364532471 | KNN Loss: 3.0822761058807373 | BCE Loss: 1.0403592586517334\n",
      "Epoch 114 / 500 | iteration 0 / 30 | Total Loss: 4.077078819274902 | KNN Loss: 3.06172513961792 | BCE Loss: 1.0153535604476929\n",
      "Epoch 114 / 500 | iteration 5 / 30 | Total Loss: 4.13704252243042 | KNN Loss: 3.0941970348358154 | BCE Loss: 1.0428454875946045\n",
      "Epoch 114 / 500 | iteration 10 / 30 | Total Loss: 4.106185436248779 | KNN Loss: 3.0776283740997314 | BCE Loss: 1.0285570621490479\n",
      "Epoch 114 / 500 | iteration 15 / 30 | Total Loss: 4.108104228973389 | KNN Loss: 3.070451498031616 | BCE Loss: 1.0376527309417725\n",
      "Epoch 114 / 500 | iteration 20 / 30 | Total Loss: 4.084909439086914 | KNN Loss: 3.057584047317505 | BCE Loss: 1.0273255109786987\n",
      "Epoch 114 / 500 | iteration 25 / 30 | Total Loss: 4.096283912658691 | KNN Loss: 3.081864595413208 | BCE Loss: 1.0144190788269043\n",
      "Epoch 115 / 500 | iteration 0 / 30 | Total Loss: 4.146225452423096 | KNN Loss: 3.093160390853882 | BCE Loss: 1.0530651807785034\n",
      "Epoch 115 / 500 | iteration 5 / 30 | Total Loss: 4.094418525695801 | KNN Loss: 3.0855467319488525 | BCE Loss: 1.0088717937469482\n",
      "Epoch 115 / 500 | iteration 10 / 30 | Total Loss: 4.058620452880859 | KNN Loss: 3.06203556060791 | BCE Loss: 0.9965847730636597\n",
      "Epoch 115 / 500 | iteration 15 / 30 | Total Loss: 4.12781286239624 | KNN Loss: 3.1084041595458984 | BCE Loss: 1.0194087028503418\n",
      "Epoch 115 / 500 | iteration 20 / 30 | Total Loss: 4.124421119689941 | KNN Loss: 3.100691318511963 | BCE Loss: 1.0237300395965576\n",
      "Epoch 115 / 500 | iteration 25 / 30 | Total Loss: 4.103869915008545 | KNN Loss: 3.078864812850952 | BCE Loss: 1.0250051021575928\n",
      "Epoch 116 / 500 | iteration 0 / 30 | Total Loss: 4.117803573608398 | KNN Loss: 3.0974156856536865 | BCE Loss: 1.0203876495361328\n",
      "Epoch 116 / 500 | iteration 5 / 30 | Total Loss: 4.090519428253174 | KNN Loss: 3.0762200355529785 | BCE Loss: 1.0142993927001953\n",
      "Epoch 116 / 500 | iteration 10 / 30 | Total Loss: 4.100081443786621 | KNN Loss: 3.07161808013916 | BCE Loss: 1.028463363647461\n",
      "Epoch 116 / 500 | iteration 15 / 30 | Total Loss: 4.1158552169799805 | KNN Loss: 3.0888073444366455 | BCE Loss: 1.0270477533340454\n",
      "Epoch 116 / 500 | iteration 20 / 30 | Total Loss: 4.15300178527832 | KNN Loss: 3.099686622619629 | BCE Loss: 1.053315281867981\n",
      "Epoch 116 / 500 | iteration 25 / 30 | Total Loss: 4.057898998260498 | KNN Loss: 3.0555951595306396 | BCE Loss: 1.002303957939148\n",
      "Epoch 117 / 500 | iteration 0 / 30 | Total Loss: 4.169688701629639 | KNN Loss: 3.121661901473999 | BCE Loss: 1.0480268001556396\n",
      "Epoch 117 / 500 | iteration 5 / 30 | Total Loss: 4.11182975769043 | KNN Loss: 3.081538438796997 | BCE Loss: 1.0302913188934326\n",
      "Epoch 117 / 500 | iteration 10 / 30 | Total Loss: 4.108719825744629 | KNN Loss: 3.0850114822387695 | BCE Loss: 1.0237082242965698\n",
      "Epoch 117 / 500 | iteration 15 / 30 | Total Loss: 4.164441108703613 | KNN Loss: 3.1304001808166504 | BCE Loss: 1.034041166305542\n",
      "Epoch 117 / 500 | iteration 20 / 30 | Total Loss: 4.047834873199463 | KNN Loss: 3.0342345237731934 | BCE Loss: 1.0136003494262695\n",
      "Epoch 117 / 500 | iteration 25 / 30 | Total Loss: 4.054568290710449 | KNN Loss: 3.022984027862549 | BCE Loss: 1.0315842628479004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118 / 500 | iteration 0 / 30 | Total Loss: 4.1111860275268555 | KNN Loss: 3.0812578201293945 | BCE Loss: 1.0299279689788818\n",
      "Epoch 118 / 500 | iteration 5 / 30 | Total Loss: 4.108087062835693 | KNN Loss: 3.1178455352783203 | BCE Loss: 0.990241527557373\n",
      "Epoch 118 / 500 | iteration 10 / 30 | Total Loss: 4.099915504455566 | KNN Loss: 3.0915944576263428 | BCE Loss: 1.0083210468292236\n",
      "Epoch 118 / 500 | iteration 15 / 30 | Total Loss: 4.100764751434326 | KNN Loss: 3.069977045059204 | BCE Loss: 1.0307875871658325\n",
      "Epoch 118 / 500 | iteration 20 / 30 | Total Loss: 4.126580238342285 | KNN Loss: 3.1091701984405518 | BCE Loss: 1.0174102783203125\n",
      "Epoch 118 / 500 | iteration 25 / 30 | Total Loss: 4.078463554382324 | KNN Loss: 3.0735013484954834 | BCE Loss: 1.0049623250961304\n",
      "Epoch 119 / 500 | iteration 0 / 30 | Total Loss: 4.062147617340088 | KNN Loss: 3.0353941917419434 | BCE Loss: 1.0267534255981445\n",
      "Epoch 119 / 500 | iteration 5 / 30 | Total Loss: 4.1114702224731445 | KNN Loss: 3.0701568126678467 | BCE Loss: 1.041313648223877\n",
      "Epoch 119 / 500 | iteration 10 / 30 | Total Loss: 4.166989326477051 | KNN Loss: 3.1197588443756104 | BCE Loss: 1.0472302436828613\n",
      "Epoch 119 / 500 | iteration 15 / 30 | Total Loss: 4.115011215209961 | KNN Loss: 3.1306312084198 | BCE Loss: 0.9843798279762268\n",
      "Epoch 119 / 500 | iteration 20 / 30 | Total Loss: 4.09075403213501 | KNN Loss: 3.074052333831787 | BCE Loss: 1.0167016983032227\n",
      "Epoch 119 / 500 | iteration 25 / 30 | Total Loss: 4.093003273010254 | KNN Loss: 3.082048177719116 | BCE Loss: 1.0109553337097168\n",
      "Epoch 120 / 500 | iteration 0 / 30 | Total Loss: 4.067470550537109 | KNN Loss: 3.0693564414978027 | BCE Loss: 0.9981138706207275\n",
      "Epoch 120 / 500 | iteration 5 / 30 | Total Loss: 4.099419593811035 | KNN Loss: 3.0818960666656494 | BCE Loss: 1.0175237655639648\n",
      "Epoch 120 / 500 | iteration 10 / 30 | Total Loss: 4.080915451049805 | KNN Loss: 3.060885190963745 | BCE Loss: 1.02003014087677\n",
      "Epoch 120 / 500 | iteration 15 / 30 | Total Loss: 4.074840545654297 | KNN Loss: 3.0702641010284424 | BCE Loss: 1.0045766830444336\n",
      "Epoch 120 / 500 | iteration 20 / 30 | Total Loss: 4.046167373657227 | KNN Loss: 3.0581977367401123 | BCE Loss: 0.9879695177078247\n",
      "Epoch 120 / 500 | iteration 25 / 30 | Total Loss: 4.107722282409668 | KNN Loss: 3.058339834213257 | BCE Loss: 1.0493826866149902\n",
      "Epoch 121 / 500 | iteration 0 / 30 | Total Loss: 4.065350532531738 | KNN Loss: 3.05249285697937 | BCE Loss: 1.0128579139709473\n",
      "Epoch 121 / 500 | iteration 5 / 30 | Total Loss: 4.1124796867370605 | KNN Loss: 3.0954651832580566 | BCE Loss: 1.0170143842697144\n",
      "Epoch 121 / 500 | iteration 10 / 30 | Total Loss: 4.075151443481445 | KNN Loss: 3.0671303272247314 | BCE Loss: 1.0080211162567139\n",
      "Epoch 121 / 500 | iteration 15 / 30 | Total Loss: 4.099509239196777 | KNN Loss: 3.098832368850708 | BCE Loss: 1.0006766319274902\n",
      "Epoch 121 / 500 | iteration 20 / 30 | Total Loss: 4.080003261566162 | KNN Loss: 3.0976271629333496 | BCE Loss: 0.9823762774467468\n",
      "Epoch 121 / 500 | iteration 25 / 30 | Total Loss: 4.146781921386719 | KNN Loss: 3.0855536460876465 | BCE Loss: 1.0612280368804932\n",
      "Epoch 122 / 500 | iteration 0 / 30 | Total Loss: 4.132969856262207 | KNN Loss: 3.0849781036376953 | BCE Loss: 1.0479915142059326\n",
      "Epoch 122 / 500 | iteration 5 / 30 | Total Loss: 4.141659736633301 | KNN Loss: 3.0998284816741943 | BCE Loss: 1.0418310165405273\n",
      "Epoch 122 / 500 | iteration 10 / 30 | Total Loss: 4.1126251220703125 | KNN Loss: 3.0733449459075928 | BCE Loss: 1.0392799377441406\n",
      "Epoch 122 / 500 | iteration 15 / 30 | Total Loss: 4.138795852661133 | KNN Loss: 3.115079402923584 | BCE Loss: 1.0237162113189697\n",
      "Epoch 122 / 500 | iteration 20 / 30 | Total Loss: 4.131242752075195 | KNN Loss: 3.120157480239868 | BCE Loss: 1.011085033416748\n",
      "Epoch 122 / 500 | iteration 25 / 30 | Total Loss: 4.094600200653076 | KNN Loss: 3.065498113632202 | BCE Loss: 1.0291019678115845\n",
      "Epoch 123 / 500 | iteration 0 / 30 | Total Loss: 4.110208511352539 | KNN Loss: 3.077253580093384 | BCE Loss: 1.0329549312591553\n",
      "Epoch 123 / 500 | iteration 5 / 30 | Total Loss: 4.101274490356445 | KNN Loss: 3.0828135013580322 | BCE Loss: 1.018460750579834\n",
      "Epoch 123 / 500 | iteration 10 / 30 | Total Loss: 4.087042808532715 | KNN Loss: 3.0831716060638428 | BCE Loss: 1.003871202468872\n",
      "Epoch 123 / 500 | iteration 15 / 30 | Total Loss: 4.110851764678955 | KNN Loss: 3.091318368911743 | BCE Loss: 1.0195332765579224\n",
      "Epoch 123 / 500 | iteration 20 / 30 | Total Loss: 4.099848747253418 | KNN Loss: 3.0822346210479736 | BCE Loss: 1.0176141262054443\n",
      "Epoch 123 / 500 | iteration 25 / 30 | Total Loss: 4.0968828201293945 | KNN Loss: 3.080573797225952 | BCE Loss: 1.0163090229034424\n",
      "Epoch 124 / 500 | iteration 0 / 30 | Total Loss: 4.101563930511475 | KNN Loss: 3.0758843421936035 | BCE Loss: 1.0256794691085815\n",
      "Epoch 124 / 500 | iteration 5 / 30 | Total Loss: 4.121716499328613 | KNN Loss: 3.1086618900299072 | BCE Loss: 1.013054609298706\n",
      "Epoch 124 / 500 | iteration 10 / 30 | Total Loss: 4.095633506774902 | KNN Loss: 3.0939695835113525 | BCE Loss: 1.0016639232635498\n",
      "Epoch 124 / 500 | iteration 15 / 30 | Total Loss: 4.133604049682617 | KNN Loss: 3.085062026977539 | BCE Loss: 1.0485422611236572\n",
      "Epoch 124 / 500 | iteration 20 / 30 | Total Loss: 4.097990036010742 | KNN Loss: 3.081594467163086 | BCE Loss: 1.0163955688476562\n",
      "Epoch 124 / 500 | iteration 25 / 30 | Total Loss: 4.080086708068848 | KNN Loss: 3.0506486892700195 | BCE Loss: 1.0294380187988281\n",
      "Epoch 125 / 500 | iteration 0 / 30 | Total Loss: 4.070982933044434 | KNN Loss: 3.067307949066162 | BCE Loss: 1.0036749839782715\n",
      "Epoch 125 / 500 | iteration 5 / 30 | Total Loss: 4.138829231262207 | KNN Loss: 3.130624532699585 | BCE Loss: 1.0082049369812012\n",
      "Epoch 125 / 500 | iteration 10 / 30 | Total Loss: 4.110864639282227 | KNN Loss: 3.0810763835906982 | BCE Loss: 1.0297881364822388\n",
      "Epoch 125 / 500 | iteration 15 / 30 | Total Loss: 4.089053153991699 | KNN Loss: 3.091613531112671 | BCE Loss: 0.9974395632743835\n",
      "Epoch 125 / 500 | iteration 20 / 30 | Total Loss: 4.125548362731934 | KNN Loss: 3.0868825912475586 | BCE Loss: 1.038666009902954\n",
      "Epoch 125 / 500 | iteration 25 / 30 | Total Loss: 4.104741096496582 | KNN Loss: 3.1063666343688965 | BCE Loss: 0.9983744025230408\n",
      "Epoch 126 / 500 | iteration 0 / 30 | Total Loss: 4.0868330001831055 | KNN Loss: 3.077788829803467 | BCE Loss: 1.0090441703796387\n",
      "Epoch 126 / 500 | iteration 5 / 30 | Total Loss: 4.106481552124023 | KNN Loss: 3.1154677867889404 | BCE Loss: 0.991013765335083\n",
      "Epoch 126 / 500 | iteration 10 / 30 | Total Loss: 4.1110734939575195 | KNN Loss: 3.094226360321045 | BCE Loss: 1.0168472528457642\n",
      "Epoch 126 / 500 | iteration 15 / 30 | Total Loss: 4.113872528076172 | KNN Loss: 3.110672950744629 | BCE Loss: 1.003199577331543\n",
      "Epoch 126 / 500 | iteration 20 / 30 | Total Loss: 4.069613933563232 | KNN Loss: 3.0620005130767822 | BCE Loss: 1.0076134204864502\n",
      "Epoch 126 / 500 | iteration 25 / 30 | Total Loss: 4.114789009094238 | KNN Loss: 3.0754170417785645 | BCE Loss: 1.039372205734253\n",
      "Epoch 127 / 500 | iteration 0 / 30 | Total Loss: 4.112339973449707 | KNN Loss: 3.0858137607574463 | BCE Loss: 1.0265259742736816\n",
      "Epoch 127 / 500 | iteration 5 / 30 | Total Loss: 4.132177829742432 | KNN Loss: 3.0966386795043945 | BCE Loss: 1.035539150238037\n",
      "Epoch 127 / 500 | iteration 10 / 30 | Total Loss: 4.144891262054443 | KNN Loss: 3.1116580963134766 | BCE Loss: 1.0332330465316772\n",
      "Epoch 127 / 500 | iteration 15 / 30 | Total Loss: 4.1075263023376465 | KNN Loss: 3.090165615081787 | BCE Loss: 1.0173605680465698\n",
      "Epoch 127 / 500 | iteration 20 / 30 | Total Loss: 4.100968360900879 | KNN Loss: 3.0708813667297363 | BCE Loss: 1.0300872325897217\n",
      "Epoch 127 / 500 | iteration 25 / 30 | Total Loss: 4.107282638549805 | KNN Loss: 3.105510950088501 | BCE Loss: 1.0017714500427246\n",
      "Epoch 128 / 500 | iteration 0 / 30 | Total Loss: 4.041416168212891 | KNN Loss: 3.0384693145751953 | BCE Loss: 1.0029468536376953\n",
      "Epoch 128 / 500 | iteration 5 / 30 | Total Loss: 4.113249778747559 | KNN Loss: 3.090256452560425 | BCE Loss: 1.0229933261871338\n",
      "Epoch 128 / 500 | iteration 10 / 30 | Total Loss: 4.1065263748168945 | KNN Loss: 3.1085524559020996 | BCE Loss: 0.9979736804962158\n",
      "Epoch 128 / 500 | iteration 15 / 30 | Total Loss: 4.160778522491455 | KNN Loss: 3.1065614223480225 | BCE Loss: 1.0542172193527222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 128 / 500 | iteration 20 / 30 | Total Loss: 4.091609001159668 | KNN Loss: 3.044062852859497 | BCE Loss: 1.04754638671875\n",
      "Epoch 128 / 500 | iteration 25 / 30 | Total Loss: 4.114238262176514 | KNN Loss: 3.0691139698028564 | BCE Loss: 1.0451242923736572\n",
      "Epoch 129 / 500 | iteration 0 / 30 | Total Loss: 4.1301069259643555 | KNN Loss: 3.0957961082458496 | BCE Loss: 1.0343109369277954\n",
      "Epoch 129 / 500 | iteration 5 / 30 | Total Loss: 4.105341911315918 | KNN Loss: 3.099771738052368 | BCE Loss: 1.0055701732635498\n",
      "Epoch 129 / 500 | iteration 10 / 30 | Total Loss: 4.133753776550293 | KNN Loss: 3.1015212535858154 | BCE Loss: 1.0322322845458984\n",
      "Epoch 129 / 500 | iteration 15 / 30 | Total Loss: 4.095797538757324 | KNN Loss: 3.047170400619507 | BCE Loss: 1.0486270189285278\n",
      "Epoch 129 / 500 | iteration 20 / 30 | Total Loss: 4.125022888183594 | KNN Loss: 3.1062185764312744 | BCE Loss: 1.0188043117523193\n",
      "Epoch 129 / 500 | iteration 25 / 30 | Total Loss: 4.086182117462158 | KNN Loss: 3.0766000747680664 | BCE Loss: 1.0095820426940918\n",
      "Epoch 130 / 500 | iteration 0 / 30 | Total Loss: 4.134255409240723 | KNN Loss: 3.1025569438934326 | BCE Loss: 1.0316987037658691\n",
      "Epoch 130 / 500 | iteration 5 / 30 | Total Loss: 4.157317161560059 | KNN Loss: 3.131551504135132 | BCE Loss: 1.0257654190063477\n",
      "Epoch 130 / 500 | iteration 10 / 30 | Total Loss: 4.12559175491333 | KNN Loss: 3.115405559539795 | BCE Loss: 1.0101863145828247\n",
      "Epoch 130 / 500 | iteration 15 / 30 | Total Loss: 4.121478080749512 | KNN Loss: 3.0819337368011475 | BCE Loss: 1.0395445823669434\n",
      "Epoch 130 / 500 | iteration 20 / 30 | Total Loss: 4.07894229888916 | KNN Loss: 3.0577316284179688 | BCE Loss: 1.0212104320526123\n",
      "Epoch 130 / 500 | iteration 25 / 30 | Total Loss: 4.084479808807373 | KNN Loss: 3.063974618911743 | BCE Loss: 1.0205051898956299\n",
      "Epoch 131 / 500 | iteration 0 / 30 | Total Loss: 4.110093116760254 | KNN Loss: 3.112825870513916 | BCE Loss: 0.9972673654556274\n",
      "Epoch 131 / 500 | iteration 5 / 30 | Total Loss: 4.108760833740234 | KNN Loss: 3.104090929031372 | BCE Loss: 1.0046696662902832\n",
      "Epoch 131 / 500 | iteration 10 / 30 | Total Loss: 4.1391472816467285 | KNN Loss: 3.1128828525543213 | BCE Loss: 1.0262644290924072\n",
      "Epoch 131 / 500 | iteration 15 / 30 | Total Loss: 4.059182167053223 | KNN Loss: 3.0503718852996826 | BCE Loss: 1.008810043334961\n",
      "Epoch 131 / 500 | iteration 20 / 30 | Total Loss: 4.10332727432251 | KNN Loss: 3.0940914154052734 | BCE Loss: 1.0092358589172363\n",
      "Epoch 131 / 500 | iteration 25 / 30 | Total Loss: 4.089715957641602 | KNN Loss: 3.072004795074463 | BCE Loss: 1.0177111625671387\n",
      "Epoch 132 / 500 | iteration 0 / 30 | Total Loss: 4.056183338165283 | KNN Loss: 3.0638740062713623 | BCE Loss: 0.9923093318939209\n",
      "Epoch 132 / 500 | iteration 5 / 30 | Total Loss: 4.158371448516846 | KNN Loss: 3.1117398738861084 | BCE Loss: 1.0466314554214478\n",
      "Epoch 132 / 500 | iteration 10 / 30 | Total Loss: 4.078784942626953 | KNN Loss: 3.071777582168579 | BCE Loss: 1.007007360458374\n",
      "Epoch 132 / 500 | iteration 15 / 30 | Total Loss: 4.097722053527832 | KNN Loss: 3.0877466201782227 | BCE Loss: 1.0099756717681885\n",
      "Epoch 132 / 500 | iteration 20 / 30 | Total Loss: 4.080355644226074 | KNN Loss: 3.056833028793335 | BCE Loss: 1.0235227346420288\n",
      "Epoch 132 / 500 | iteration 25 / 30 | Total Loss: 4.112542152404785 | KNN Loss: 3.0947794914245605 | BCE Loss: 1.0177627801895142\n",
      "Epoch   133: reducing learning rate of group 0 to 1.7150e-03.\n",
      "Epoch 133 / 500 | iteration 0 / 30 | Total Loss: 4.131738662719727 | KNN Loss: 3.092369794845581 | BCE Loss: 1.039368987083435\n",
      "Epoch 133 / 500 | iteration 5 / 30 | Total Loss: 4.089837074279785 | KNN Loss: 3.0793275833129883 | BCE Loss: 1.0105092525482178\n",
      "Epoch 133 / 500 | iteration 10 / 30 | Total Loss: 4.106931686401367 | KNN Loss: 3.0764968395233154 | BCE Loss: 1.0304349660873413\n",
      "Epoch 133 / 500 | iteration 15 / 30 | Total Loss: 4.0827836990356445 | KNN Loss: 3.0735769271850586 | BCE Loss: 1.009207010269165\n",
      "Epoch 133 / 500 | iteration 20 / 30 | Total Loss: 4.073177337646484 | KNN Loss: 3.082066535949707 | BCE Loss: 0.9911110401153564\n",
      "Epoch 133 / 500 | iteration 25 / 30 | Total Loss: 4.1480865478515625 | KNN Loss: 3.105940580368042 | BCE Loss: 1.0421459674835205\n",
      "Epoch 134 / 500 | iteration 0 / 30 | Total Loss: 4.1046142578125 | KNN Loss: 3.0687825679779053 | BCE Loss: 1.0358314514160156\n",
      "Epoch 134 / 500 | iteration 5 / 30 | Total Loss: 4.094605445861816 | KNN Loss: 3.0440480709075928 | BCE Loss: 1.0505571365356445\n",
      "Epoch 134 / 500 | iteration 10 / 30 | Total Loss: 4.0956878662109375 | KNN Loss: 3.075352668762207 | BCE Loss: 1.02033531665802\n",
      "Epoch 134 / 500 | iteration 15 / 30 | Total Loss: 4.110711574554443 | KNN Loss: 3.0743355751037598 | BCE Loss: 1.036375880241394\n",
      "Epoch 134 / 500 | iteration 20 / 30 | Total Loss: 4.058084964752197 | KNN Loss: 3.059213161468506 | BCE Loss: 0.9988718032836914\n",
      "Epoch 134 / 500 | iteration 25 / 30 | Total Loss: 4.080758094787598 | KNN Loss: 3.0636069774627686 | BCE Loss: 1.0171509981155396\n",
      "Epoch 135 / 500 | iteration 0 / 30 | Total Loss: 4.092751502990723 | KNN Loss: 3.0413036346435547 | BCE Loss: 1.0514476299285889\n",
      "Epoch 135 / 500 | iteration 5 / 30 | Total Loss: 4.071793556213379 | KNN Loss: 3.0589382648468018 | BCE Loss: 1.0128555297851562\n",
      "Epoch 135 / 500 | iteration 10 / 30 | Total Loss: 4.128273010253906 | KNN Loss: 3.100017786026001 | BCE Loss: 1.0282552242279053\n",
      "Epoch 135 / 500 | iteration 15 / 30 | Total Loss: 4.098575115203857 | KNN Loss: 3.0646114349365234 | BCE Loss: 1.033963680267334\n",
      "Epoch 135 / 500 | iteration 20 / 30 | Total Loss: 4.114189147949219 | KNN Loss: 3.0721306800842285 | BCE Loss: 1.0420584678649902\n",
      "Epoch 135 / 500 | iteration 25 / 30 | Total Loss: 4.117602348327637 | KNN Loss: 3.090155839920044 | BCE Loss: 1.0274467468261719\n",
      "Epoch 136 / 500 | iteration 0 / 30 | Total Loss: 4.082039833068848 | KNN Loss: 3.077188730239868 | BCE Loss: 1.0048511028289795\n",
      "Epoch 136 / 500 | iteration 5 / 30 | Total Loss: 4.1109395027160645 | KNN Loss: 3.0760293006896973 | BCE Loss: 1.0349100828170776\n",
      "Epoch 136 / 500 | iteration 10 / 30 | Total Loss: 4.108914852142334 | KNN Loss: 3.0927093029022217 | BCE Loss: 1.0162056684494019\n",
      "Epoch 136 / 500 | iteration 15 / 30 | Total Loss: 4.098987579345703 | KNN Loss: 3.064012050628662 | BCE Loss: 1.0349757671356201\n",
      "Epoch 136 / 500 | iteration 20 / 30 | Total Loss: 4.060250282287598 | KNN Loss: 3.030078411102295 | BCE Loss: 1.0301716327667236\n",
      "Epoch 136 / 500 | iteration 25 / 30 | Total Loss: 4.070207118988037 | KNN Loss: 3.04170298576355 | BCE Loss: 1.0285041332244873\n",
      "Epoch 137 / 500 | iteration 0 / 30 | Total Loss: 4.103167533874512 | KNN Loss: 3.1085996627807617 | BCE Loss: 0.9945677518844604\n",
      "Epoch 137 / 500 | iteration 5 / 30 | Total Loss: 4.115450859069824 | KNN Loss: 3.085446357727051 | BCE Loss: 1.0300043821334839\n",
      "Epoch 137 / 500 | iteration 10 / 30 | Total Loss: 4.058415412902832 | KNN Loss: 3.0754756927490234 | BCE Loss: 0.9829399585723877\n",
      "Epoch 137 / 500 | iteration 15 / 30 | Total Loss: 4.116554260253906 | KNN Loss: 3.0984880924224854 | BCE Loss: 1.0180659294128418\n",
      "Epoch 137 / 500 | iteration 20 / 30 | Total Loss: 4.083019733428955 | KNN Loss: 3.066627264022827 | BCE Loss: 1.016392469406128\n",
      "Epoch 137 / 500 | iteration 25 / 30 | Total Loss: 4.090495586395264 | KNN Loss: 3.0513768196105957 | BCE Loss: 1.039118766784668\n",
      "Epoch 138 / 500 | iteration 0 / 30 | Total Loss: 4.139214515686035 | KNN Loss: 3.0661587715148926 | BCE Loss: 1.0730555057525635\n",
      "Epoch 138 / 500 | iteration 5 / 30 | Total Loss: 4.105820655822754 | KNN Loss: 3.099588394165039 | BCE Loss: 1.0062322616577148\n",
      "Epoch 138 / 500 | iteration 10 / 30 | Total Loss: 4.07371187210083 | KNN Loss: 3.075101375579834 | BCE Loss: 0.9986104965209961\n",
      "Epoch 138 / 500 | iteration 15 / 30 | Total Loss: 4.063162803649902 | KNN Loss: 3.0524306297302246 | BCE Loss: 1.0107319355010986\n",
      "Epoch 138 / 500 | iteration 20 / 30 | Total Loss: 4.064260959625244 | KNN Loss: 3.0470187664031982 | BCE Loss: 1.017242193222046\n",
      "Epoch 138 / 500 | iteration 25 / 30 | Total Loss: 4.110904693603516 | KNN Loss: 3.078691244125366 | BCE Loss: 1.0322134494781494\n",
      "Epoch 139 / 500 | iteration 0 / 30 | Total Loss: 4.0898542404174805 | KNN Loss: 3.0734236240386963 | BCE Loss: 1.0164308547973633\n",
      "Epoch 139 / 500 | iteration 5 / 30 | Total Loss: 4.118223190307617 | KNN Loss: 3.1033356189727783 | BCE Loss: 1.0148873329162598\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 139 / 500 | iteration 10 / 30 | Total Loss: 4.079288959503174 | KNN Loss: 3.070758581161499 | BCE Loss: 1.0085302591323853\n",
      "Epoch 139 / 500 | iteration 15 / 30 | Total Loss: 4.08736515045166 | KNN Loss: 3.087836980819702 | BCE Loss: 0.9995284080505371\n",
      "Epoch 139 / 500 | iteration 20 / 30 | Total Loss: 4.0745134353637695 | KNN Loss: 3.0727200508117676 | BCE Loss: 1.001793384552002\n",
      "Epoch 139 / 500 | iteration 25 / 30 | Total Loss: 4.080516815185547 | KNN Loss: 3.086458683013916 | BCE Loss: 0.9940581917762756\n",
      "Epoch 140 / 500 | iteration 0 / 30 | Total Loss: 4.069692611694336 | KNN Loss: 3.0683250427246094 | BCE Loss: 1.0013676881790161\n",
      "Epoch 140 / 500 | iteration 5 / 30 | Total Loss: 4.059331893920898 | KNN Loss: 3.061249256134033 | BCE Loss: 0.9980824589729309\n",
      "Epoch 140 / 500 | iteration 10 / 30 | Total Loss: 4.127124786376953 | KNN Loss: 3.114865303039551 | BCE Loss: 1.0122597217559814\n",
      "Epoch 140 / 500 | iteration 15 / 30 | Total Loss: 4.1382598876953125 | KNN Loss: 3.0867886543273926 | BCE Loss: 1.051471471786499\n",
      "Epoch 140 / 500 | iteration 20 / 30 | Total Loss: 4.121188640594482 | KNN Loss: 3.0924928188323975 | BCE Loss: 1.028695821762085\n",
      "Epoch 140 / 500 | iteration 25 / 30 | Total Loss: 4.11318302154541 | KNN Loss: 3.061898708343506 | BCE Loss: 1.0512844324111938\n",
      "Epoch 141 / 500 | iteration 0 / 30 | Total Loss: 4.093208312988281 | KNN Loss: 3.066253185272217 | BCE Loss: 1.0269548892974854\n",
      "Epoch 141 / 500 | iteration 5 / 30 | Total Loss: 4.110064506530762 | KNN Loss: 3.091538190841675 | BCE Loss: 1.018526554107666\n",
      "Epoch 141 / 500 | iteration 10 / 30 | Total Loss: 4.092658996582031 | KNN Loss: 3.080618381500244 | BCE Loss: 1.0120408535003662\n",
      "Epoch 141 / 500 | iteration 15 / 30 | Total Loss: 4.074701309204102 | KNN Loss: 3.073063373565674 | BCE Loss: 1.0016381740570068\n",
      "Epoch 141 / 500 | iteration 20 / 30 | Total Loss: 4.114056587219238 | KNN Loss: 3.04571533203125 | BCE Loss: 1.0683414936065674\n",
      "Epoch 141 / 500 | iteration 25 / 30 | Total Loss: 4.077279090881348 | KNN Loss: 3.050225257873535 | BCE Loss: 1.027053713798523\n",
      "Epoch 142 / 500 | iteration 0 / 30 | Total Loss: 4.105052947998047 | KNN Loss: 3.0648956298828125 | BCE Loss: 1.0401570796966553\n",
      "Epoch 142 / 500 | iteration 5 / 30 | Total Loss: 4.076299667358398 | KNN Loss: 3.0635952949523926 | BCE Loss: 1.0127042531967163\n",
      "Epoch 142 / 500 | iteration 10 / 30 | Total Loss: 4.087240695953369 | KNN Loss: 3.054722309112549 | BCE Loss: 1.0325183868408203\n",
      "Epoch 142 / 500 | iteration 15 / 30 | Total Loss: 4.0926618576049805 | KNN Loss: 3.0958902835845947 | BCE Loss: 0.9967713356018066\n",
      "Epoch 142 / 500 | iteration 20 / 30 | Total Loss: 4.110418796539307 | KNN Loss: 3.0870275497436523 | BCE Loss: 1.0233912467956543\n",
      "Epoch 142 / 500 | iteration 25 / 30 | Total Loss: 4.06315803527832 | KNN Loss: 3.0514063835144043 | BCE Loss: 1.0117518901824951\n",
      "Epoch 143 / 500 | iteration 0 / 30 | Total Loss: 4.076818943023682 | KNN Loss: 3.0606777667999268 | BCE Loss: 1.0161411762237549\n",
      "Epoch 143 / 500 | iteration 5 / 30 | Total Loss: 4.093677043914795 | KNN Loss: 3.066270589828491 | BCE Loss: 1.0274063348770142\n",
      "Epoch 143 / 500 | iteration 10 / 30 | Total Loss: 4.0863566398620605 | KNN Loss: 3.086505889892578 | BCE Loss: 0.9998508095741272\n",
      "Epoch 143 / 500 | iteration 15 / 30 | Total Loss: 4.101290225982666 | KNN Loss: 3.0609006881713867 | BCE Loss: 1.0403896570205688\n",
      "Epoch 143 / 500 | iteration 20 / 30 | Total Loss: 4.066245079040527 | KNN Loss: 3.039593458175659 | BCE Loss: 1.026651382446289\n",
      "Epoch 143 / 500 | iteration 25 / 30 | Total Loss: 4.143739223480225 | KNN Loss: 3.0853488445281982 | BCE Loss: 1.0583902597427368\n",
      "Epoch 144 / 500 | iteration 0 / 30 | Total Loss: 4.103996276855469 | KNN Loss: 3.08246111869812 | BCE Loss: 1.0215353965759277\n",
      "Epoch 144 / 500 | iteration 5 / 30 | Total Loss: 4.091840744018555 | KNN Loss: 3.068514347076416 | BCE Loss: 1.0233265161514282\n",
      "Epoch 144 / 500 | iteration 10 / 30 | Total Loss: 4.093209743499756 | KNN Loss: 3.0785045623779297 | BCE Loss: 1.0147050619125366\n",
      "Epoch 144 / 500 | iteration 15 / 30 | Total Loss: 4.090789318084717 | KNN Loss: 3.073496103286743 | BCE Loss: 1.017293095588684\n",
      "Epoch 144 / 500 | iteration 20 / 30 | Total Loss: 4.082034587860107 | KNN Loss: 3.057626485824585 | BCE Loss: 1.024408221244812\n",
      "Epoch 144 / 500 | iteration 25 / 30 | Total Loss: 4.0678324699401855 | KNN Loss: 3.0317118167877197 | BCE Loss: 1.0361207723617554\n",
      "Epoch 145 / 500 | iteration 0 / 30 | Total Loss: 4.033306121826172 | KNN Loss: 3.0462474822998047 | BCE Loss: 0.9870587587356567\n",
      "Epoch 145 / 500 | iteration 5 / 30 | Total Loss: 4.097889423370361 | KNN Loss: 3.0786712169647217 | BCE Loss: 1.0192182064056396\n",
      "Epoch 145 / 500 | iteration 10 / 30 | Total Loss: 4.115365028381348 | KNN Loss: 3.0965192317962646 | BCE Loss: 1.0188459157943726\n",
      "Epoch 145 / 500 | iteration 15 / 30 | Total Loss: 4.071372032165527 | KNN Loss: 3.0714054107666016 | BCE Loss: 0.9999666213989258\n",
      "Epoch 145 / 500 | iteration 20 / 30 | Total Loss: 4.082653522491455 | KNN Loss: 3.0451314449310303 | BCE Loss: 1.0375220775604248\n",
      "Epoch 145 / 500 | iteration 25 / 30 | Total Loss: 4.05933952331543 | KNN Loss: 3.055302143096924 | BCE Loss: 1.004037618637085\n",
      "Epoch 146 / 500 | iteration 0 / 30 | Total Loss: 4.053967475891113 | KNN Loss: 3.047346353530884 | BCE Loss: 1.0066208839416504\n",
      "Epoch 146 / 500 | iteration 5 / 30 | Total Loss: 4.087265491485596 | KNN Loss: 3.071892499923706 | BCE Loss: 1.0153729915618896\n",
      "Epoch 146 / 500 | iteration 10 / 30 | Total Loss: 4.07706356048584 | KNN Loss: 3.061213731765747 | BCE Loss: 1.0158497095108032\n",
      "Epoch 146 / 500 | iteration 15 / 30 | Total Loss: 4.115640163421631 | KNN Loss: 3.082904577255249 | BCE Loss: 1.0327355861663818\n",
      "Epoch 146 / 500 | iteration 20 / 30 | Total Loss: 4.101354598999023 | KNN Loss: 3.0772674083709717 | BCE Loss: 1.0240874290466309\n",
      "Epoch 146 / 500 | iteration 25 / 30 | Total Loss: 4.10372257232666 | KNN Loss: 3.0791079998016357 | BCE Loss: 1.0246148109436035\n",
      "Epoch 147 / 500 | iteration 0 / 30 | Total Loss: 4.106174945831299 | KNN Loss: 3.083441972732544 | BCE Loss: 1.0227328538894653\n",
      "Epoch 147 / 500 | iteration 5 / 30 | Total Loss: 4.103110313415527 | KNN Loss: 3.0548999309539795 | BCE Loss: 1.0482103824615479\n",
      "Epoch 147 / 500 | iteration 10 / 30 | Total Loss: 4.054609298706055 | KNN Loss: 3.047952651977539 | BCE Loss: 1.0066564083099365\n",
      "Epoch 147 / 500 | iteration 15 / 30 | Total Loss: 4.109996795654297 | KNN Loss: 3.074793577194214 | BCE Loss: 1.035203218460083\n",
      "Epoch 147 / 500 | iteration 20 / 30 | Total Loss: 4.097945213317871 | KNN Loss: 3.070697069168091 | BCE Loss: 1.0272481441497803\n",
      "Epoch 147 / 500 | iteration 25 / 30 | Total Loss: 4.119967460632324 | KNN Loss: 3.0784807205200195 | BCE Loss: 1.0414865016937256\n",
      "Epoch 148 / 500 | iteration 0 / 30 | Total Loss: 4.070378303527832 | KNN Loss: 3.0663864612579346 | BCE Loss: 1.0039920806884766\n",
      "Epoch 148 / 500 | iteration 5 / 30 | Total Loss: 4.07152795791626 | KNN Loss: 3.0745372772216797 | BCE Loss: 0.9969906806945801\n",
      "Epoch 148 / 500 | iteration 10 / 30 | Total Loss: 4.106679439544678 | KNN Loss: 3.0939807891845703 | BCE Loss: 1.012698769569397\n",
      "Epoch 148 / 500 | iteration 15 / 30 | Total Loss: 4.066542148590088 | KNN Loss: 3.0704448223114014 | BCE Loss: 0.9960971474647522\n",
      "Epoch 148 / 500 | iteration 20 / 30 | Total Loss: 4.126859664916992 | KNN Loss: 3.0881707668304443 | BCE Loss: 1.0386887788772583\n",
      "Epoch 148 / 500 | iteration 25 / 30 | Total Loss: 4.108290672302246 | KNN Loss: 3.07110595703125 | BCE Loss: 1.037184476852417\n",
      "Epoch 149 / 500 | iteration 0 / 30 | Total Loss: 4.090493679046631 | KNN Loss: 3.0757548809051514 | BCE Loss: 1.0147387981414795\n",
      "Epoch 149 / 500 | iteration 5 / 30 | Total Loss: 4.060759544372559 | KNN Loss: 3.067570447921753 | BCE Loss: 0.9931890368461609\n",
      "Epoch 149 / 500 | iteration 10 / 30 | Total Loss: 4.131973743438721 | KNN Loss: 3.101212501525879 | BCE Loss: 1.0307613611221313\n",
      "Epoch 149 / 500 | iteration 15 / 30 | Total Loss: 4.098004341125488 | KNN Loss: 3.0593552589416504 | BCE Loss: 1.0386492013931274\n",
      "Epoch 149 / 500 | iteration 20 / 30 | Total Loss: 4.086129188537598 | KNN Loss: 3.0657505989074707 | BCE Loss: 1.0203784704208374\n",
      "Epoch 149 / 500 | iteration 25 / 30 | Total Loss: 4.1359100341796875 | KNN Loss: 3.1088993549346924 | BCE Loss: 1.0270109176635742\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 150 / 500 | iteration 0 / 30 | Total Loss: 4.097102165222168 | KNN Loss: 3.075808525085449 | BCE Loss: 1.0212934017181396\n",
      "Epoch 150 / 500 | iteration 5 / 30 | Total Loss: 4.078984260559082 | KNN Loss: 3.0572636127471924 | BCE Loss: 1.0217204093933105\n",
      "Epoch 150 / 500 | iteration 10 / 30 | Total Loss: 4.123972415924072 | KNN Loss: 3.091752767562866 | BCE Loss: 1.032219648361206\n",
      "Epoch 150 / 500 | iteration 15 / 30 | Total Loss: 4.081401824951172 | KNN Loss: 3.0490920543670654 | BCE Loss: 1.0323097705841064\n",
      "Epoch 150 / 500 | iteration 20 / 30 | Total Loss: 4.056092739105225 | KNN Loss: 3.0398778915405273 | BCE Loss: 1.0162148475646973\n",
      "Epoch 150 / 500 | iteration 25 / 30 | Total Loss: 4.085788726806641 | KNN Loss: 3.053041696548462 | BCE Loss: 1.0327467918395996\n",
      "Epoch 151 / 500 | iteration 0 / 30 | Total Loss: 4.060632228851318 | KNN Loss: 3.053041934967041 | BCE Loss: 1.0075902938842773\n",
      "Epoch 151 / 500 | iteration 5 / 30 | Total Loss: 4.103055953979492 | KNN Loss: 3.0907936096191406 | BCE Loss: 1.0122623443603516\n",
      "Epoch 151 / 500 | iteration 10 / 30 | Total Loss: 4.109619140625 | KNN Loss: 3.07373309135437 | BCE Loss: 1.0358858108520508\n",
      "Epoch 151 / 500 | iteration 15 / 30 | Total Loss: 4.114240646362305 | KNN Loss: 3.086270809173584 | BCE Loss: 1.0279699563980103\n",
      "Epoch 151 / 500 | iteration 20 / 30 | Total Loss: 4.080299377441406 | KNN Loss: 3.065316915512085 | BCE Loss: 1.0149824619293213\n",
      "Epoch 151 / 500 | iteration 25 / 30 | Total Loss: 4.099443435668945 | KNN Loss: 3.0614025592803955 | BCE Loss: 1.0380406379699707\n",
      "Epoch 152 / 500 | iteration 0 / 30 | Total Loss: 4.077769756317139 | KNN Loss: 3.0645909309387207 | BCE Loss: 1.0131789445877075\n",
      "Epoch 152 / 500 | iteration 5 / 30 | Total Loss: 4.118424415588379 | KNN Loss: 3.092625141143799 | BCE Loss: 1.025799036026001\n",
      "Epoch 152 / 500 | iteration 10 / 30 | Total Loss: 4.103272438049316 | KNN Loss: 3.112636089324951 | BCE Loss: 0.9906361103057861\n",
      "Epoch 152 / 500 | iteration 15 / 30 | Total Loss: 4.089727401733398 | KNN Loss: 3.073357582092285 | BCE Loss: 1.0163695812225342\n",
      "Epoch 152 / 500 | iteration 20 / 30 | Total Loss: 4.075786113739014 | KNN Loss: 3.0539774894714355 | BCE Loss: 1.0218085050582886\n",
      "Epoch 152 / 500 | iteration 25 / 30 | Total Loss: 4.124310493469238 | KNN Loss: 3.08976149559021 | BCE Loss: 1.0345492362976074\n",
      "Epoch 153 / 500 | iteration 0 / 30 | Total Loss: 4.11550235748291 | KNN Loss: 3.089860200881958 | BCE Loss: 1.0256420373916626\n",
      "Epoch 153 / 500 | iteration 5 / 30 | Total Loss: 4.143599510192871 | KNN Loss: 3.105912208557129 | BCE Loss: 1.0376875400543213\n",
      "Epoch 153 / 500 | iteration 10 / 30 | Total Loss: 4.130010604858398 | KNN Loss: 3.090543031692505 | BCE Loss: 1.0394678115844727\n",
      "Epoch 153 / 500 | iteration 15 / 30 | Total Loss: 4.096330165863037 | KNN Loss: 3.0737054347991943 | BCE Loss: 1.0226247310638428\n",
      "Epoch 153 / 500 | iteration 20 / 30 | Total Loss: 4.113326072692871 | KNN Loss: 3.086237668991089 | BCE Loss: 1.0270884037017822\n",
      "Epoch 153 / 500 | iteration 25 / 30 | Total Loss: 4.0913286209106445 | KNN Loss: 3.054185152053833 | BCE Loss: 1.0371434688568115\n",
      "Epoch 154 / 500 | iteration 0 / 30 | Total Loss: 4.078151226043701 | KNN Loss: 3.0726234912872314 | BCE Loss: 1.0055278539657593\n",
      "Epoch 154 / 500 | iteration 5 / 30 | Total Loss: 4.106852054595947 | KNN Loss: 3.0733962059020996 | BCE Loss: 1.033455729484558\n",
      "Epoch 154 / 500 | iteration 10 / 30 | Total Loss: 4.0865912437438965 | KNN Loss: 3.0745577812194824 | BCE Loss: 1.0120333433151245\n",
      "Epoch 154 / 500 | iteration 15 / 30 | Total Loss: 4.087110996246338 | KNN Loss: 3.068305253982544 | BCE Loss: 1.018805742263794\n",
      "Epoch 154 / 500 | iteration 20 / 30 | Total Loss: 4.0403337478637695 | KNN Loss: 3.0586347579956055 | BCE Loss: 0.9816990494728088\n",
      "Epoch 154 / 500 | iteration 25 / 30 | Total Loss: 4.103799819946289 | KNN Loss: 3.0714170932769775 | BCE Loss: 1.032382607460022\n",
      "Epoch   155: reducing learning rate of group 0 to 1.2005e-03.\n",
      "Epoch 155 / 500 | iteration 0 / 30 | Total Loss: 4.07289457321167 | KNN Loss: 3.063027858734131 | BCE Loss: 1.009866714477539\n",
      "Epoch 155 / 500 | iteration 5 / 30 | Total Loss: 4.102242469787598 | KNN Loss: 3.0887696743011475 | BCE Loss: 1.0134730339050293\n",
      "Epoch 155 / 500 | iteration 10 / 30 | Total Loss: 4.131247520446777 | KNN Loss: 3.0603859424591064 | BCE Loss: 1.07086181640625\n",
      "Epoch 155 / 500 | iteration 15 / 30 | Total Loss: 4.134543418884277 | KNN Loss: 3.1082334518432617 | BCE Loss: 1.0263097286224365\n",
      "Epoch 155 / 500 | iteration 20 / 30 | Total Loss: 4.078044891357422 | KNN Loss: 3.0600221157073975 | BCE Loss: 1.0180226564407349\n",
      "Epoch 155 / 500 | iteration 25 / 30 | Total Loss: 4.062438011169434 | KNN Loss: 3.0481598377227783 | BCE Loss: 1.0142784118652344\n",
      "Epoch 156 / 500 | iteration 0 / 30 | Total Loss: 4.093672275543213 | KNN Loss: 3.066774606704712 | BCE Loss: 1.0268975496292114\n",
      "Epoch 156 / 500 | iteration 5 / 30 | Total Loss: 4.125375747680664 | KNN Loss: 3.1110785007476807 | BCE Loss: 1.0142974853515625\n",
      "Epoch 156 / 500 | iteration 10 / 30 | Total Loss: 4.142684459686279 | KNN Loss: 3.107577085494995 | BCE Loss: 1.0351072549819946\n",
      "Epoch 156 / 500 | iteration 15 / 30 | Total Loss: 4.1267313957214355 | KNN Loss: 3.0805132389068604 | BCE Loss: 1.0462181568145752\n",
      "Epoch 156 / 500 | iteration 20 / 30 | Total Loss: 4.068853378295898 | KNN Loss: 3.0295183658599854 | BCE Loss: 1.039334774017334\n",
      "Epoch 156 / 500 | iteration 25 / 30 | Total Loss: 4.092939376831055 | KNN Loss: 3.0791127681732178 | BCE Loss: 1.0138264894485474\n",
      "Epoch 157 / 500 | iteration 0 / 30 | Total Loss: 4.113262176513672 | KNN Loss: 3.081511974334717 | BCE Loss: 1.0317503213882446\n",
      "Epoch 157 / 500 | iteration 5 / 30 | Total Loss: 4.113526344299316 | KNN Loss: 3.084641695022583 | BCE Loss: 1.0288848876953125\n",
      "Epoch 157 / 500 | iteration 10 / 30 | Total Loss: 4.1163330078125 | KNN Loss: 3.107656717300415 | BCE Loss: 1.0086764097213745\n",
      "Epoch 157 / 500 | iteration 15 / 30 | Total Loss: 4.093069076538086 | KNN Loss: 3.075526475906372 | BCE Loss: 1.0175426006317139\n",
      "Epoch 157 / 500 | iteration 20 / 30 | Total Loss: 4.107870101928711 | KNN Loss: 3.0722129344940186 | BCE Loss: 1.0356571674346924\n",
      "Epoch 157 / 500 | iteration 25 / 30 | Total Loss: 4.120734214782715 | KNN Loss: 3.101156711578369 | BCE Loss: 1.0195777416229248\n",
      "Epoch 158 / 500 | iteration 0 / 30 | Total Loss: 4.109804153442383 | KNN Loss: 3.0580551624298096 | BCE Loss: 1.0517492294311523\n",
      "Epoch 158 / 500 | iteration 5 / 30 | Total Loss: 4.093019962310791 | KNN Loss: 3.059454917907715 | BCE Loss: 1.0335650444030762\n",
      "Epoch 158 / 500 | iteration 10 / 30 | Total Loss: 4.109956741333008 | KNN Loss: 3.09212327003479 | BCE Loss: 1.0178337097167969\n",
      "Epoch 158 / 500 | iteration 15 / 30 | Total Loss: 4.0589141845703125 | KNN Loss: 3.071117639541626 | BCE Loss: 0.9877963066101074\n",
      "Epoch 158 / 500 | iteration 20 / 30 | Total Loss: 4.129563808441162 | KNN Loss: 3.078383207321167 | BCE Loss: 1.0511806011199951\n",
      "Epoch 158 / 500 | iteration 25 / 30 | Total Loss: 4.105830669403076 | KNN Loss: 3.091498851776123 | BCE Loss: 1.0143318176269531\n",
      "Epoch 159 / 500 | iteration 0 / 30 | Total Loss: 4.070706367492676 | KNN Loss: 3.074230432510376 | BCE Loss: 0.9964756965637207\n",
      "Epoch 159 / 500 | iteration 5 / 30 | Total Loss: 4.107831001281738 | KNN Loss: 3.0798604488372803 | BCE Loss: 1.027970790863037\n",
      "Epoch 159 / 500 | iteration 10 / 30 | Total Loss: 4.071335792541504 | KNN Loss: 3.0641846656799316 | BCE Loss: 1.0071512460708618\n",
      "Epoch 159 / 500 | iteration 15 / 30 | Total Loss: 4.134869575500488 | KNN Loss: 3.0883569717407227 | BCE Loss: 1.0465128421783447\n",
      "Epoch 159 / 500 | iteration 20 / 30 | Total Loss: 4.079587459564209 | KNN Loss: 3.0831732749938965 | BCE Loss: 0.9964141845703125\n",
      "Epoch 159 / 500 | iteration 25 / 30 | Total Loss: 4.106297492980957 | KNN Loss: 3.1044957637786865 | BCE Loss: 1.001801609992981\n",
      "Epoch 160 / 500 | iteration 0 / 30 | Total Loss: 4.087343215942383 | KNN Loss: 3.0653698444366455 | BCE Loss: 1.0219732522964478\n",
      "Epoch 160 / 500 | iteration 5 / 30 | Total Loss: 4.043771266937256 | KNN Loss: 3.0461275577545166 | BCE Loss: 0.9976436495780945\n",
      "Epoch 160 / 500 | iteration 10 / 30 | Total Loss: 4.091852188110352 | KNN Loss: 3.088313579559326 | BCE Loss: 1.0035383701324463\n",
      "Epoch 160 / 500 | iteration 15 / 30 | Total Loss: 4.131533622741699 | KNN Loss: 3.1017911434173584 | BCE Loss: 1.0297424793243408\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 160 / 500 | iteration 20 / 30 | Total Loss: 4.079030513763428 | KNN Loss: 3.0839531421661377 | BCE Loss: 0.9950774908065796\n",
      "Epoch 160 / 500 | iteration 25 / 30 | Total Loss: 4.097964286804199 | KNN Loss: 3.0737640857696533 | BCE Loss: 1.0241999626159668\n",
      "Epoch 161 / 500 | iteration 0 / 30 | Total Loss: 4.105476379394531 | KNN Loss: 3.0887420177459717 | BCE Loss: 1.0167344808578491\n",
      "Epoch 161 / 500 | iteration 5 / 30 | Total Loss: 4.075845718383789 | KNN Loss: 3.056375741958618 | BCE Loss: 1.0194697380065918\n",
      "Epoch 161 / 500 | iteration 10 / 30 | Total Loss: 4.1096510887146 | KNN Loss: 3.073732852935791 | BCE Loss: 1.0359183549880981\n",
      "Epoch 161 / 500 | iteration 15 / 30 | Total Loss: 4.0920257568359375 | KNN Loss: 3.0931198596954346 | BCE Loss: 0.9989060163497925\n",
      "Epoch 161 / 500 | iteration 20 / 30 | Total Loss: 4.068831920623779 | KNN Loss: 3.0522940158843994 | BCE Loss: 1.0165379047393799\n",
      "Epoch 161 / 500 | iteration 25 / 30 | Total Loss: 4.122918605804443 | KNN Loss: 3.083812952041626 | BCE Loss: 1.0391056537628174\n",
      "Epoch 162 / 500 | iteration 0 / 30 | Total Loss: 4.106978416442871 | KNN Loss: 3.087118625640869 | BCE Loss: 1.019859790802002\n",
      "Epoch 162 / 500 | iteration 5 / 30 | Total Loss: 4.0948686599731445 | KNN Loss: 3.0932836532592773 | BCE Loss: 1.0015850067138672\n",
      "Epoch 162 / 500 | iteration 10 / 30 | Total Loss: 4.089236259460449 | KNN Loss: 3.085637092590332 | BCE Loss: 1.0035994052886963\n",
      "Epoch 162 / 500 | iteration 15 / 30 | Total Loss: 4.129874229431152 | KNN Loss: 3.086371660232544 | BCE Loss: 1.0435025691986084\n",
      "Epoch 162 / 500 | iteration 20 / 30 | Total Loss: 4.1140923500061035 | KNN Loss: 3.0735864639282227 | BCE Loss: 1.0405058860778809\n",
      "Epoch 162 / 500 | iteration 25 / 30 | Total Loss: 4.076058864593506 | KNN Loss: 3.0762405395507812 | BCE Loss: 0.9998184442520142\n",
      "Epoch 163 / 500 | iteration 0 / 30 | Total Loss: 4.035340785980225 | KNN Loss: 3.0544235706329346 | BCE Loss: 0.9809170365333557\n",
      "Epoch 163 / 500 | iteration 5 / 30 | Total Loss: 4.110393524169922 | KNN Loss: 3.071729898452759 | BCE Loss: 1.0386638641357422\n",
      "Epoch 163 / 500 | iteration 10 / 30 | Total Loss: 4.1082868576049805 | KNN Loss: 3.083712100982666 | BCE Loss: 1.0245749950408936\n",
      "Epoch 163 / 500 | iteration 15 / 30 | Total Loss: 4.162527084350586 | KNN Loss: 3.113811492919922 | BCE Loss: 1.048715353012085\n",
      "Epoch 163 / 500 | iteration 20 / 30 | Total Loss: 4.077986717224121 | KNN Loss: 3.075284004211426 | BCE Loss: 1.0027029514312744\n",
      "Epoch 163 / 500 | iteration 25 / 30 | Total Loss: 4.110729694366455 | KNN Loss: 3.0926766395568848 | BCE Loss: 1.0180530548095703\n",
      "Epoch 164 / 500 | iteration 0 / 30 | Total Loss: 4.066611289978027 | KNN Loss: 3.0573129653930664 | BCE Loss: 1.0092982053756714\n",
      "Epoch 164 / 500 | iteration 5 / 30 | Total Loss: 4.1252031326293945 | KNN Loss: 3.081084966659546 | BCE Loss: 1.0441181659698486\n",
      "Epoch 164 / 500 | iteration 10 / 30 | Total Loss: 4.101067543029785 | KNN Loss: 3.0775434970855713 | BCE Loss: 1.023524284362793\n",
      "Epoch 164 / 500 | iteration 15 / 30 | Total Loss: 4.117509841918945 | KNN Loss: 3.0861642360687256 | BCE Loss: 1.0313454866409302\n",
      "Epoch 164 / 500 | iteration 20 / 30 | Total Loss: 4.101656436920166 | KNN Loss: 3.0780436992645264 | BCE Loss: 1.02361261844635\n",
      "Epoch 164 / 500 | iteration 25 / 30 | Total Loss: 4.1290130615234375 | KNN Loss: 3.0926513671875 | BCE Loss: 1.0363616943359375\n",
      "Epoch 165 / 500 | iteration 0 / 30 | Total Loss: 4.099148750305176 | KNN Loss: 3.0783586502075195 | BCE Loss: 1.0207899808883667\n",
      "Epoch 165 / 500 | iteration 5 / 30 | Total Loss: 4.058530330657959 | KNN Loss: 3.047679901123047 | BCE Loss: 1.010850429534912\n",
      "Epoch 165 / 500 | iteration 10 / 30 | Total Loss: 4.0747270584106445 | KNN Loss: 3.0881426334381104 | BCE Loss: 0.9865841865539551\n",
      "Epoch 165 / 500 | iteration 15 / 30 | Total Loss: 4.103975296020508 | KNN Loss: 3.0914158821105957 | BCE Loss: 1.0125596523284912\n",
      "Epoch 165 / 500 | iteration 20 / 30 | Total Loss: 4.091294765472412 | KNN Loss: 3.092026948928833 | BCE Loss: 0.9992677569389343\n",
      "Epoch 165 / 500 | iteration 25 / 30 | Total Loss: 4.146968841552734 | KNN Loss: 3.0936453342437744 | BCE Loss: 1.05332350730896\n",
      "Epoch   166: reducing learning rate of group 0 to 8.4035e-04.\n",
      "Epoch 166 / 500 | iteration 0 / 30 | Total Loss: 4.071256637573242 | KNN Loss: 3.0437114238739014 | BCE Loss: 1.0275449752807617\n",
      "Epoch 166 / 500 | iteration 5 / 30 | Total Loss: 4.0530314445495605 | KNN Loss: 3.040015935897827 | BCE Loss: 1.0130153894424438\n",
      "Epoch 166 / 500 | iteration 10 / 30 | Total Loss: 4.108084678649902 | KNN Loss: 3.0838816165924072 | BCE Loss: 1.0242031812667847\n",
      "Epoch 166 / 500 | iteration 15 / 30 | Total Loss: 4.067775726318359 | KNN Loss: 3.0748443603515625 | BCE Loss: 0.992931604385376\n",
      "Epoch 166 / 500 | iteration 20 / 30 | Total Loss: 4.136717319488525 | KNN Loss: 3.1199944019317627 | BCE Loss: 1.0167229175567627\n",
      "Epoch 166 / 500 | iteration 25 / 30 | Total Loss: 4.1082305908203125 | KNN Loss: 3.0926575660705566 | BCE Loss: 1.0155730247497559\n",
      "Epoch 167 / 500 | iteration 0 / 30 | Total Loss: 4.1054158210754395 | KNN Loss: 3.0891172885894775 | BCE Loss: 1.0162986516952515\n",
      "Epoch 167 / 500 | iteration 5 / 30 | Total Loss: 4.092685699462891 | KNN Loss: 3.0571846961975098 | BCE Loss: 1.0355007648468018\n",
      "Epoch 167 / 500 | iteration 10 / 30 | Total Loss: 4.078017711639404 | KNN Loss: 3.0533363819122314 | BCE Loss: 1.0246813297271729\n",
      "Epoch 167 / 500 | iteration 15 / 30 | Total Loss: 4.08961820602417 | KNN Loss: 3.0730412006378174 | BCE Loss: 1.0165770053863525\n",
      "Epoch 167 / 500 | iteration 20 / 30 | Total Loss: 4.093379020690918 | KNN Loss: 3.050614356994629 | BCE Loss: 1.042764663696289\n",
      "Epoch 167 / 500 | iteration 25 / 30 | Total Loss: 4.098300933837891 | KNN Loss: 3.067488431930542 | BCE Loss: 1.0308127403259277\n",
      "Epoch 168 / 500 | iteration 0 / 30 | Total Loss: 4.116218090057373 | KNN Loss: 3.0955469608306885 | BCE Loss: 1.020671010017395\n",
      "Epoch 168 / 500 | iteration 5 / 30 | Total Loss: 4.076642036437988 | KNN Loss: 3.0710959434509277 | BCE Loss: 1.0055458545684814\n",
      "Epoch 168 / 500 | iteration 10 / 30 | Total Loss: 4.098398208618164 | KNN Loss: 3.0924763679504395 | BCE Loss: 1.0059218406677246\n",
      "Epoch 168 / 500 | iteration 15 / 30 | Total Loss: 4.07930850982666 | KNN Loss: 3.0485901832580566 | BCE Loss: 1.0307183265686035\n",
      "Epoch 168 / 500 | iteration 20 / 30 | Total Loss: 4.10198450088501 | KNN Loss: 3.0778682231903076 | BCE Loss: 1.0241161584854126\n",
      "Epoch 168 / 500 | iteration 25 / 30 | Total Loss: 4.106878280639648 | KNN Loss: 3.071427822113037 | BCE Loss: 1.0354506969451904\n",
      "Epoch 169 / 500 | iteration 0 / 30 | Total Loss: 4.018621921539307 | KNN Loss: 3.0303056240081787 | BCE Loss: 0.9883161187171936\n",
      "Epoch 169 / 500 | iteration 5 / 30 | Total Loss: 4.081756591796875 | KNN Loss: 3.0506999492645264 | BCE Loss: 1.0310564041137695\n",
      "Epoch 169 / 500 | iteration 10 / 30 | Total Loss: 4.099628925323486 | KNN Loss: 3.0901083946228027 | BCE Loss: 1.0095205307006836\n",
      "Epoch 169 / 500 | iteration 15 / 30 | Total Loss: 4.142120361328125 | KNN Loss: 3.08274507522583 | BCE Loss: 1.0593750476837158\n",
      "Epoch 169 / 500 | iteration 20 / 30 | Total Loss: 4.154202461242676 | KNN Loss: 3.100562572479248 | BCE Loss: 1.0536397695541382\n",
      "Epoch 169 / 500 | iteration 25 / 30 | Total Loss: 4.0596771240234375 | KNN Loss: 3.0533053874969482 | BCE Loss: 1.0063718557357788\n",
      "Epoch 170 / 500 | iteration 0 / 30 | Total Loss: 4.108160018920898 | KNN Loss: 3.086885690689087 | BCE Loss: 1.0212745666503906\n",
      "Epoch 170 / 500 | iteration 5 / 30 | Total Loss: 4.075023651123047 | KNN Loss: 3.0721802711486816 | BCE Loss: 1.0028434991836548\n",
      "Epoch 170 / 500 | iteration 10 / 30 | Total Loss: 4.08056640625 | KNN Loss: 3.0495567321777344 | BCE Loss: 1.0310096740722656\n",
      "Epoch 170 / 500 | iteration 15 / 30 | Total Loss: 4.090479850769043 | KNN Loss: 3.053497552871704 | BCE Loss: 1.0369820594787598\n",
      "Epoch 170 / 500 | iteration 20 / 30 | Total Loss: 4.095934867858887 | KNN Loss: 3.0638997554779053 | BCE Loss: 1.0320351123809814\n",
      "Epoch 170 / 500 | iteration 25 / 30 | Total Loss: 4.089266777038574 | KNN Loss: 3.0509326457977295 | BCE Loss: 1.0383338928222656\n",
      "Epoch 171 / 500 | iteration 0 / 30 | Total Loss: 4.109886169433594 | KNN Loss: 3.1002626419067383 | BCE Loss: 1.0096232891082764\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 171 / 500 | iteration 5 / 30 | Total Loss: 4.132905960083008 | KNN Loss: 3.0833632946014404 | BCE Loss: 1.0495426654815674\n",
      "Epoch 171 / 500 | iteration 10 / 30 | Total Loss: 4.038260459899902 | KNN Loss: 3.0444841384887695 | BCE Loss: 0.993776261806488\n",
      "Epoch 171 / 500 | iteration 15 / 30 | Total Loss: 4.109405517578125 | KNN Loss: 3.061530828475952 | BCE Loss: 1.0478746891021729\n",
      "Epoch 171 / 500 | iteration 20 / 30 | Total Loss: 4.061433792114258 | KNN Loss: 3.0315868854522705 | BCE Loss: 1.0298469066619873\n",
      "Epoch 171 / 500 | iteration 25 / 30 | Total Loss: 4.1186628341674805 | KNN Loss: 3.0880637168884277 | BCE Loss: 1.0305991172790527\n",
      "Epoch 172 / 500 | iteration 0 / 30 | Total Loss: 4.073940277099609 | KNN Loss: 3.0552175045013428 | BCE Loss: 1.0187228918075562\n",
      "Epoch 172 / 500 | iteration 5 / 30 | Total Loss: 4.057873249053955 | KNN Loss: 3.0278451442718506 | BCE Loss: 1.030028223991394\n",
      "Epoch 172 / 500 | iteration 10 / 30 | Total Loss: 4.080341339111328 | KNN Loss: 3.071286678314209 | BCE Loss: 1.0090547800064087\n",
      "Epoch 172 / 500 | iteration 15 / 30 | Total Loss: 4.098474502563477 | KNN Loss: 3.063894748687744 | BCE Loss: 1.0345799922943115\n",
      "Epoch 172 / 500 | iteration 20 / 30 | Total Loss: 4.035909175872803 | KNN Loss: 3.0402703285217285 | BCE Loss: 0.9956389665603638\n",
      "Epoch 172 / 500 | iteration 25 / 30 | Total Loss: 4.082663536071777 | KNN Loss: 3.057634115219116 | BCE Loss: 1.0250294208526611\n",
      "Epoch 173 / 500 | iteration 0 / 30 | Total Loss: 4.053798198699951 | KNN Loss: 3.0640132427215576 | BCE Loss: 0.9897850155830383\n",
      "Epoch 173 / 500 | iteration 5 / 30 | Total Loss: 4.1197733879089355 | KNN Loss: 3.094334602355957 | BCE Loss: 1.025438666343689\n",
      "Epoch 173 / 500 | iteration 10 / 30 | Total Loss: 4.070186614990234 | KNN Loss: 3.0675628185272217 | BCE Loss: 1.0026237964630127\n",
      "Epoch 173 / 500 | iteration 15 / 30 | Total Loss: 4.092819690704346 | KNN Loss: 3.07712984085083 | BCE Loss: 1.0156899690628052\n",
      "Epoch 173 / 500 | iteration 20 / 30 | Total Loss: 4.11147403717041 | KNN Loss: 3.057310104370117 | BCE Loss: 1.0541640520095825\n",
      "Epoch 173 / 500 | iteration 25 / 30 | Total Loss: 4.132081031799316 | KNN Loss: 3.1000893115997314 | BCE Loss: 1.0319914817810059\n",
      "Epoch 174 / 500 | iteration 0 / 30 | Total Loss: 4.058069705963135 | KNN Loss: 3.041677713394165 | BCE Loss: 1.0163918733596802\n",
      "Epoch 174 / 500 | iteration 5 / 30 | Total Loss: 4.0819807052612305 | KNN Loss: 3.0428898334503174 | BCE Loss: 1.0390909910202026\n",
      "Epoch 174 / 500 | iteration 10 / 30 | Total Loss: 4.068194389343262 | KNN Loss: 3.083130359649658 | BCE Loss: 0.9850640296936035\n",
      "Epoch 174 / 500 | iteration 15 / 30 | Total Loss: 4.136058330535889 | KNN Loss: 3.080111026763916 | BCE Loss: 1.0559473037719727\n",
      "Epoch 174 / 500 | iteration 20 / 30 | Total Loss: 4.063263893127441 | KNN Loss: 3.0300774574279785 | BCE Loss: 1.0331865549087524\n",
      "Epoch 174 / 500 | iteration 25 / 30 | Total Loss: 4.096460342407227 | KNN Loss: 3.0724587440490723 | BCE Loss: 1.0240015983581543\n",
      "Epoch 175 / 500 | iteration 0 / 30 | Total Loss: 4.0688676834106445 | KNN Loss: 3.0606963634490967 | BCE Loss: 1.0081713199615479\n",
      "Epoch 175 / 500 | iteration 5 / 30 | Total Loss: 4.07230281829834 | KNN Loss: 3.0385358333587646 | BCE Loss: 1.0337669849395752\n",
      "Epoch 175 / 500 | iteration 10 / 30 | Total Loss: 4.05853271484375 | KNN Loss: 3.0829946994781494 | BCE Loss: 0.9755377769470215\n",
      "Epoch 175 / 500 | iteration 15 / 30 | Total Loss: 4.0852460861206055 | KNN Loss: 3.0548956394195557 | BCE Loss: 1.0303505659103394\n",
      "Epoch 175 / 500 | iteration 20 / 30 | Total Loss: 4.090169906616211 | KNN Loss: 3.08207631111145 | BCE Loss: 1.0080937147140503\n",
      "Epoch 175 / 500 | iteration 25 / 30 | Total Loss: 4.078561782836914 | KNN Loss: 3.099039316177368 | BCE Loss: 0.9795223474502563\n",
      "Epoch 176 / 500 | iteration 0 / 30 | Total Loss: 4.041263580322266 | KNN Loss: 3.037264108657837 | BCE Loss: 1.0039993524551392\n",
      "Epoch 176 / 500 | iteration 5 / 30 | Total Loss: 4.093159198760986 | KNN Loss: 3.0744423866271973 | BCE Loss: 1.018716812133789\n",
      "Epoch 176 / 500 | iteration 10 / 30 | Total Loss: 4.09678840637207 | KNN Loss: 3.0887181758880615 | BCE Loss: 1.0080701112747192\n",
      "Epoch 176 / 500 | iteration 15 / 30 | Total Loss: 4.0483293533325195 | KNN Loss: 3.066957473754883 | BCE Loss: 0.9813721179962158\n",
      "Epoch 176 / 500 | iteration 20 / 30 | Total Loss: 4.107958793640137 | KNN Loss: 3.080942392349243 | BCE Loss: 1.027016520500183\n",
      "Epoch 176 / 500 | iteration 25 / 30 | Total Loss: 4.1039204597473145 | KNN Loss: 3.0799286365509033 | BCE Loss: 1.0239918231964111\n",
      "Epoch   177: reducing learning rate of group 0 to 5.8824e-04.\n",
      "Epoch 177 / 500 | iteration 0 / 30 | Total Loss: 4.07026481628418 | KNN Loss: 3.053790807723999 | BCE Loss: 1.0164742469787598\n",
      "Epoch 177 / 500 | iteration 5 / 30 | Total Loss: 4.096820831298828 | KNN Loss: 3.0854506492614746 | BCE Loss: 1.0113699436187744\n",
      "Epoch 177 / 500 | iteration 10 / 30 | Total Loss: 4.078400135040283 | KNN Loss: 3.073772430419922 | BCE Loss: 1.0046278238296509\n",
      "Epoch 177 / 500 | iteration 15 / 30 | Total Loss: 4.074885368347168 | KNN Loss: 3.049734115600586 | BCE Loss: 1.025151252746582\n",
      "Epoch 177 / 500 | iteration 20 / 30 | Total Loss: 4.125489234924316 | KNN Loss: 3.0971343517303467 | BCE Loss: 1.0283548831939697\n",
      "Epoch 177 / 500 | iteration 25 / 30 | Total Loss: 4.087983131408691 | KNN Loss: 3.0514938831329346 | BCE Loss: 1.0364890098571777\n",
      "Epoch 178 / 500 | iteration 0 / 30 | Total Loss: 4.102089881896973 | KNN Loss: 3.0947864055633545 | BCE Loss: 1.007303237915039\n",
      "Epoch 178 / 500 | iteration 5 / 30 | Total Loss: 4.059296607971191 | KNN Loss: 3.0499093532562256 | BCE Loss: 1.009387493133545\n",
      "Epoch 178 / 500 | iteration 10 / 30 | Total Loss: 4.076106071472168 | KNN Loss: 3.0662829875946045 | BCE Loss: 1.0098233222961426\n",
      "Epoch 178 / 500 | iteration 15 / 30 | Total Loss: 4.1295576095581055 | KNN Loss: 3.0932981967926025 | BCE Loss: 1.036259412765503\n",
      "Epoch 178 / 500 | iteration 20 / 30 | Total Loss: 4.054141998291016 | KNN Loss: 3.077425241470337 | BCE Loss: 0.9767166376113892\n",
      "Epoch 178 / 500 | iteration 25 / 30 | Total Loss: 4.091207504272461 | KNN Loss: 3.074733018875122 | BCE Loss: 1.0164744853973389\n",
      "Epoch 179 / 500 | iteration 0 / 30 | Total Loss: 4.093271255493164 | KNN Loss: 3.0601816177368164 | BCE Loss: 1.0330897569656372\n",
      "Epoch 179 / 500 | iteration 5 / 30 | Total Loss: 4.070496559143066 | KNN Loss: 3.0540664196014404 | BCE Loss: 1.0164299011230469\n",
      "Epoch 179 / 500 | iteration 10 / 30 | Total Loss: 4.085892200469971 | KNN Loss: 3.063494920730591 | BCE Loss: 1.0223973989486694\n",
      "Epoch 179 / 500 | iteration 15 / 30 | Total Loss: 4.042721271514893 | KNN Loss: 3.036078453063965 | BCE Loss: 1.0066426992416382\n",
      "Epoch 179 / 500 | iteration 20 / 30 | Total Loss: 4.08010196685791 | KNN Loss: 3.0917000770568848 | BCE Loss: 0.9884018301963806\n",
      "Epoch 179 / 500 | iteration 25 / 30 | Total Loss: 4.056098937988281 | KNN Loss: 3.063410758972168 | BCE Loss: 0.9926882386207581\n",
      "Epoch 180 / 500 | iteration 0 / 30 | Total Loss: 4.0702714920043945 | KNN Loss: 3.0510807037353516 | BCE Loss: 1.019190788269043\n",
      "Epoch 180 / 500 | iteration 5 / 30 | Total Loss: 4.106490612030029 | KNN Loss: 3.0604233741760254 | BCE Loss: 1.046067237854004\n",
      "Epoch 180 / 500 | iteration 10 / 30 | Total Loss: 4.094332218170166 | KNN Loss: 3.061230182647705 | BCE Loss: 1.033102035522461\n",
      "Epoch 180 / 500 | iteration 15 / 30 | Total Loss: 4.075024604797363 | KNN Loss: 3.0517313480377197 | BCE Loss: 1.023293375968933\n",
      "Epoch 180 / 500 | iteration 20 / 30 | Total Loss: 4.067301273345947 | KNN Loss: 3.0628135204315186 | BCE Loss: 1.0044878721237183\n",
      "Epoch 180 / 500 | iteration 25 / 30 | Total Loss: 4.116464614868164 | KNN Loss: 3.080897569656372 | BCE Loss: 1.035567045211792\n",
      "Epoch 181 / 500 | iteration 0 / 30 | Total Loss: 4.104694843292236 | KNN Loss: 3.057112693786621 | BCE Loss: 1.0475820302963257\n",
      "Epoch 181 / 500 | iteration 5 / 30 | Total Loss: 4.143916130065918 | KNN Loss: 3.108272075653076 | BCE Loss: 1.0356441736221313\n",
      "Epoch 181 / 500 | iteration 10 / 30 | Total Loss: 4.1033711433410645 | KNN Loss: 3.0624566078186035 | BCE Loss: 1.040914535522461\n",
      "Epoch 181 / 500 | iteration 15 / 30 | Total Loss: 4.071660995483398 | KNN Loss: 3.066098213195801 | BCE Loss: 1.0055630207061768\n",
      "Epoch 181 / 500 | iteration 20 / 30 | Total Loss: 4.102546691894531 | KNN Loss: 3.0720016956329346 | BCE Loss: 1.0305451154708862\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 181 / 500 | iteration 25 / 30 | Total Loss: 4.098759651184082 | KNN Loss: 3.0786404609680176 | BCE Loss: 1.020119309425354\n",
      "Epoch 182 / 500 | iteration 0 / 30 | Total Loss: 4.0855326652526855 | KNN Loss: 3.062173843383789 | BCE Loss: 1.023358702659607\n",
      "Epoch 182 / 500 | iteration 5 / 30 | Total Loss: 4.079740524291992 | KNN Loss: 3.054070472717285 | BCE Loss: 1.0256702899932861\n",
      "Epoch 182 / 500 | iteration 10 / 30 | Total Loss: 4.116410255432129 | KNN Loss: 3.075468063354492 | BCE Loss: 1.0409424304962158\n",
      "Epoch 182 / 500 | iteration 15 / 30 | Total Loss: 4.053072929382324 | KNN Loss: 3.015141010284424 | BCE Loss: 1.03793203830719\n",
      "Epoch 182 / 500 | iteration 20 / 30 | Total Loss: 4.076537609100342 | KNN Loss: 3.0754430294036865 | BCE Loss: 1.0010946989059448\n",
      "Epoch 182 / 500 | iteration 25 / 30 | Total Loss: 4.041234970092773 | KNN Loss: 3.054825782775879 | BCE Loss: 0.9864093065261841\n",
      "Epoch 183 / 500 | iteration 0 / 30 | Total Loss: 4.013128280639648 | KNN Loss: 3.042893409729004 | BCE Loss: 0.9702349901199341\n",
      "Epoch 183 / 500 | iteration 5 / 30 | Total Loss: 4.098954677581787 | KNN Loss: 3.0929408073425293 | BCE Loss: 1.0060138702392578\n",
      "Epoch 183 / 500 | iteration 10 / 30 | Total Loss: 4.101917743682861 | KNN Loss: 3.0593481063842773 | BCE Loss: 1.042569637298584\n",
      "Epoch 183 / 500 | iteration 15 / 30 | Total Loss: 4.1194562911987305 | KNN Loss: 3.1095235347747803 | BCE Loss: 1.009932518005371\n",
      "Epoch 183 / 500 | iteration 20 / 30 | Total Loss: 4.081718444824219 | KNN Loss: 3.0703868865966797 | BCE Loss: 1.01133131980896\n",
      "Epoch 183 / 500 | iteration 25 / 30 | Total Loss: 4.095889091491699 | KNN Loss: 3.0756194591522217 | BCE Loss: 1.0202693939208984\n",
      "Epoch 184 / 500 | iteration 0 / 30 | Total Loss: 4.090300559997559 | KNN Loss: 3.0535364151000977 | BCE Loss: 1.0367640256881714\n",
      "Epoch 184 / 500 | iteration 5 / 30 | Total Loss: 4.1014485359191895 | KNN Loss: 3.077548027038574 | BCE Loss: 1.0239005088806152\n",
      "Epoch 184 / 500 | iteration 10 / 30 | Total Loss: 4.086873531341553 | KNN Loss: 3.064121723175049 | BCE Loss: 1.0227516889572144\n",
      "Epoch 184 / 500 | iteration 15 / 30 | Total Loss: 4.054521083831787 | KNN Loss: 3.0849788188934326 | BCE Loss: 0.969542384147644\n",
      "Epoch 184 / 500 | iteration 20 / 30 | Total Loss: 4.068971633911133 | KNN Loss: 3.06113338470459 | BCE Loss: 1.0078381299972534\n",
      "Epoch 184 / 500 | iteration 25 / 30 | Total Loss: 4.117752552032471 | KNN Loss: 3.0879557132720947 | BCE Loss: 1.029796838760376\n",
      "Epoch 185 / 500 | iteration 0 / 30 | Total Loss: 4.097158432006836 | KNN Loss: 3.0581490993499756 | BCE Loss: 1.0390090942382812\n",
      "Epoch 185 / 500 | iteration 5 / 30 | Total Loss: 4.099279880523682 | KNN Loss: 3.076725959777832 | BCE Loss: 1.0225539207458496\n",
      "Epoch 185 / 500 | iteration 10 / 30 | Total Loss: 4.108297348022461 | KNN Loss: 3.079298496246338 | BCE Loss: 1.0289987325668335\n",
      "Epoch 185 / 500 | iteration 15 / 30 | Total Loss: 4.1196489334106445 | KNN Loss: 3.08597731590271 | BCE Loss: 1.0336713790893555\n",
      "Epoch 185 / 500 | iteration 20 / 30 | Total Loss: 4.110060214996338 | KNN Loss: 3.0750694274902344 | BCE Loss: 1.0349907875061035\n",
      "Epoch 185 / 500 | iteration 25 / 30 | Total Loss: 4.108336448669434 | KNN Loss: 3.076188564300537 | BCE Loss: 1.0321478843688965\n",
      "Epoch 186 / 500 | iteration 0 / 30 | Total Loss: 4.077320575714111 | KNN Loss: 3.0486321449279785 | BCE Loss: 1.0286885499954224\n",
      "Epoch 186 / 500 | iteration 5 / 30 | Total Loss: 4.135012626647949 | KNN Loss: 3.0757784843444824 | BCE Loss: 1.0592341423034668\n",
      "Epoch 186 / 500 | iteration 10 / 30 | Total Loss: 4.148933410644531 | KNN Loss: 3.1134591102600098 | BCE Loss: 1.0354743003845215\n",
      "Epoch 186 / 500 | iteration 15 / 30 | Total Loss: 4.093648910522461 | KNN Loss: 3.0511972904205322 | BCE Loss: 1.0424516201019287\n",
      "Epoch 186 / 500 | iteration 20 / 30 | Total Loss: 4.128393650054932 | KNN Loss: 3.1074471473693848 | BCE Loss: 1.0209465026855469\n",
      "Epoch 186 / 500 | iteration 25 / 30 | Total Loss: 4.1406941413879395 | KNN Loss: 3.111377239227295 | BCE Loss: 1.0293169021606445\n",
      "Epoch 187 / 500 | iteration 0 / 30 | Total Loss: 4.117179870605469 | KNN Loss: 3.106288433074951 | BCE Loss: 1.0108914375305176\n",
      "Epoch 187 / 500 | iteration 5 / 30 | Total Loss: 4.135976791381836 | KNN Loss: 3.1091582775115967 | BCE Loss: 1.0268187522888184\n",
      "Epoch 187 / 500 | iteration 10 / 30 | Total Loss: 4.107398986816406 | KNN Loss: 3.072713613510132 | BCE Loss: 1.0346856117248535\n",
      "Epoch 187 / 500 | iteration 15 / 30 | Total Loss: 4.103565216064453 | KNN Loss: 3.085648536682129 | BCE Loss: 1.0179165601730347\n",
      "Epoch 187 / 500 | iteration 20 / 30 | Total Loss: 4.107749938964844 | KNN Loss: 3.067089557647705 | BCE Loss: 1.0406602621078491\n",
      "Epoch 187 / 500 | iteration 25 / 30 | Total Loss: 4.082974433898926 | KNN Loss: 3.030383348464966 | BCE Loss: 1.0525908470153809\n",
      "Epoch   188: reducing learning rate of group 0 to 4.1177e-04.\n",
      "Epoch 188 / 500 | iteration 0 / 30 | Total Loss: 4.103280067443848 | KNN Loss: 3.0791866779327393 | BCE Loss: 1.0240936279296875\n",
      "Epoch 188 / 500 | iteration 5 / 30 | Total Loss: 4.103732585906982 | KNN Loss: 3.0677807331085205 | BCE Loss: 1.035951852798462\n",
      "Epoch 188 / 500 | iteration 10 / 30 | Total Loss: 4.107111930847168 | KNN Loss: 3.0712337493896484 | BCE Loss: 1.0358779430389404\n",
      "Epoch 188 / 500 | iteration 15 / 30 | Total Loss: 4.072542667388916 | KNN Loss: 3.0671682357788086 | BCE Loss: 1.0053744316101074\n",
      "Epoch 188 / 500 | iteration 20 / 30 | Total Loss: 4.072940349578857 | KNN Loss: 3.066246271133423 | BCE Loss: 1.006693959236145\n",
      "Epoch 188 / 500 | iteration 25 / 30 | Total Loss: 4.093274116516113 | KNN Loss: 3.0932796001434326 | BCE Loss: 0.9999943375587463\n",
      "Epoch 189 / 500 | iteration 0 / 30 | Total Loss: 4.071777820587158 | KNN Loss: 3.0680954456329346 | BCE Loss: 1.0036823749542236\n",
      "Epoch 189 / 500 | iteration 5 / 30 | Total Loss: 4.128667831420898 | KNN Loss: 3.097820520401001 | BCE Loss: 1.0308470726013184\n",
      "Epoch 189 / 500 | iteration 10 / 30 | Total Loss: 4.103855609893799 | KNN Loss: 3.0695714950561523 | BCE Loss: 1.034284234046936\n",
      "Epoch 189 / 500 | iteration 15 / 30 | Total Loss: 4.04432487487793 | KNN Loss: 3.041078567504883 | BCE Loss: 1.0032460689544678\n",
      "Epoch 189 / 500 | iteration 20 / 30 | Total Loss: 4.1074066162109375 | KNN Loss: 3.0722134113311768 | BCE Loss: 1.0351932048797607\n",
      "Epoch 189 / 500 | iteration 25 / 30 | Total Loss: 4.084313869476318 | KNN Loss: 3.05580735206604 | BCE Loss: 1.0285063982009888\n",
      "Epoch 190 / 500 | iteration 0 / 30 | Total Loss: 4.1429548263549805 | KNN Loss: 3.1084909439086914 | BCE Loss: 1.0344641208648682\n",
      "Epoch 190 / 500 | iteration 5 / 30 | Total Loss: 4.073108196258545 | KNN Loss: 3.060572385787964 | BCE Loss: 1.0125359296798706\n",
      "Epoch 190 / 500 | iteration 10 / 30 | Total Loss: 4.069265365600586 | KNN Loss: 3.052248477935791 | BCE Loss: 1.0170166492462158\n",
      "Epoch 190 / 500 | iteration 15 / 30 | Total Loss: 4.102659225463867 | KNN Loss: 3.094115972518921 | BCE Loss: 1.0085432529449463\n",
      "Epoch 190 / 500 | iteration 20 / 30 | Total Loss: 4.15554141998291 | KNN Loss: 3.121833324432373 | BCE Loss: 1.0337079763412476\n",
      "Epoch 190 / 500 | iteration 25 / 30 | Total Loss: 4.153924465179443 | KNN Loss: 3.1121985912323 | BCE Loss: 1.0417258739471436\n",
      "Epoch 191 / 500 | iteration 0 / 30 | Total Loss: 4.06561279296875 | KNN Loss: 3.081190586090088 | BCE Loss: 0.9844220876693726\n",
      "Epoch 191 / 500 | iteration 5 / 30 | Total Loss: 4.125006198883057 | KNN Loss: 3.0682449340820312 | BCE Loss: 1.0567611455917358\n",
      "Epoch 191 / 500 | iteration 10 / 30 | Total Loss: 4.086789608001709 | KNN Loss: 3.0623619556427 | BCE Loss: 1.0244276523590088\n",
      "Epoch 191 / 500 | iteration 15 / 30 | Total Loss: 4.051798343658447 | KNN Loss: 3.0288562774658203 | BCE Loss: 1.0229421854019165\n",
      "Epoch 191 / 500 | iteration 20 / 30 | Total Loss: 4.103588581085205 | KNN Loss: 3.0958170890808105 | BCE Loss: 1.007771372795105\n",
      "Epoch 191 / 500 | iteration 25 / 30 | Total Loss: 4.15631628036499 | KNN Loss: 3.1265764236450195 | BCE Loss: 1.0297397375106812\n",
      "Epoch 192 / 500 | iteration 0 / 30 | Total Loss: 4.072841644287109 | KNN Loss: 3.0610158443450928 | BCE Loss: 1.0118259191513062\n",
      "Epoch 192 / 500 | iteration 5 / 30 | Total Loss: 4.099452972412109 | KNN Loss: 3.0595881938934326 | BCE Loss: 1.0398647785186768\n",
      "Epoch 192 / 500 | iteration 10 / 30 | Total Loss: 4.087080001831055 | KNN Loss: 3.0609190464019775 | BCE Loss: 1.0261610746383667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 192 / 500 | iteration 15 / 30 | Total Loss: 4.070567607879639 | KNN Loss: 3.064936876296997 | BCE Loss: 1.0056307315826416\n",
      "Epoch 192 / 500 | iteration 20 / 30 | Total Loss: 4.099040985107422 | KNN Loss: 3.0886318683624268 | BCE Loss: 1.010408878326416\n",
      "Epoch 192 / 500 | iteration 25 / 30 | Total Loss: 4.061402797698975 | KNN Loss: 3.048452377319336 | BCE Loss: 1.0129504203796387\n",
      "Epoch 193 / 500 | iteration 0 / 30 | Total Loss: 4.109785079956055 | KNN Loss: 3.0636441707611084 | BCE Loss: 1.0461411476135254\n",
      "Epoch 193 / 500 | iteration 5 / 30 | Total Loss: 4.073755264282227 | KNN Loss: 3.053832769393921 | BCE Loss: 1.0199224948883057\n",
      "Epoch 193 / 500 | iteration 10 / 30 | Total Loss: 4.095701694488525 | KNN Loss: 3.082777261734009 | BCE Loss: 1.0129244327545166\n",
      "Epoch 193 / 500 | iteration 15 / 30 | Total Loss: 4.049081802368164 | KNN Loss: 3.0294525623321533 | BCE Loss: 1.0196294784545898\n",
      "Epoch 193 / 500 | iteration 20 / 30 | Total Loss: 4.121363162994385 | KNN Loss: 3.0922415256500244 | BCE Loss: 1.0291216373443604\n",
      "Epoch 193 / 500 | iteration 25 / 30 | Total Loss: 4.061522483825684 | KNN Loss: 3.053790807723999 | BCE Loss: 1.007731556892395\n",
      "Epoch 194 / 500 | iteration 0 / 30 | Total Loss: 4.098691940307617 | KNN Loss: 3.072777509689331 | BCE Loss: 1.025914192199707\n",
      "Epoch 194 / 500 | iteration 5 / 30 | Total Loss: 4.114737510681152 | KNN Loss: 3.069378137588501 | BCE Loss: 1.0453596115112305\n",
      "Epoch 194 / 500 | iteration 10 / 30 | Total Loss: 4.081739902496338 | KNN Loss: 3.0675530433654785 | BCE Loss: 1.0141867399215698\n",
      "Epoch 194 / 500 | iteration 15 / 30 | Total Loss: 4.124415874481201 | KNN Loss: 3.095506191253662 | BCE Loss: 1.0289098024368286\n",
      "Epoch 194 / 500 | iteration 20 / 30 | Total Loss: 4.081008434295654 | KNN Loss: 3.0674655437469482 | BCE Loss: 1.0135430097579956\n",
      "Epoch 194 / 500 | iteration 25 / 30 | Total Loss: 4.086271286010742 | KNN Loss: 3.0657033920288086 | BCE Loss: 1.0205681324005127\n",
      "Epoch 195 / 500 | iteration 0 / 30 | Total Loss: 4.072702407836914 | KNN Loss: 3.050881862640381 | BCE Loss: 1.0218204259872437\n",
      "Epoch 195 / 500 | iteration 5 / 30 | Total Loss: 4.076166152954102 | KNN Loss: 3.055110216140747 | BCE Loss: 1.0210556983947754\n",
      "Epoch 195 / 500 | iteration 10 / 30 | Total Loss: 4.089295864105225 | KNN Loss: 3.0520193576812744 | BCE Loss: 1.0372765064239502\n",
      "Epoch 195 / 500 | iteration 15 / 30 | Total Loss: 4.102437496185303 | KNN Loss: 3.071132183074951 | BCE Loss: 1.0313053131103516\n",
      "Epoch 195 / 500 | iteration 20 / 30 | Total Loss: 4.0968475341796875 | KNN Loss: 3.0838229656219482 | BCE Loss: 1.0130245685577393\n",
      "Epoch 195 / 500 | iteration 25 / 30 | Total Loss: 4.059983253479004 | KNN Loss: 3.0526669025421143 | BCE Loss: 1.0073163509368896\n",
      "Epoch 196 / 500 | iteration 0 / 30 | Total Loss: 4.10020637512207 | KNN Loss: 3.0819292068481445 | BCE Loss: 1.0182771682739258\n",
      "Epoch 196 / 500 | iteration 5 / 30 | Total Loss: 4.083200454711914 | KNN Loss: 3.0911316871643066 | BCE Loss: 0.9920690059661865\n",
      "Epoch 196 / 500 | iteration 10 / 30 | Total Loss: 4.10597562789917 | KNN Loss: 3.0577480792999268 | BCE Loss: 1.0482275485992432\n",
      "Epoch 196 / 500 | iteration 15 / 30 | Total Loss: 4.0720367431640625 | KNN Loss: 3.0567140579223633 | BCE Loss: 1.0153228044509888\n",
      "Epoch 196 / 500 | iteration 20 / 30 | Total Loss: 4.0306525230407715 | KNN Loss: 3.043813467025757 | BCE Loss: 0.9868388772010803\n",
      "Epoch 196 / 500 | iteration 25 / 30 | Total Loss: 4.056764602661133 | KNN Loss: 3.0688865184783936 | BCE Loss: 0.9878782033920288\n",
      "Epoch 197 / 500 | iteration 0 / 30 | Total Loss: 4.085085868835449 | KNN Loss: 3.0459225177764893 | BCE Loss: 1.0391632318496704\n",
      "Epoch 197 / 500 | iteration 5 / 30 | Total Loss: 4.046555519104004 | KNN Loss: 3.0351994037628174 | BCE Loss: 1.0113558769226074\n",
      "Epoch 197 / 500 | iteration 10 / 30 | Total Loss: 4.049140930175781 | KNN Loss: 3.044330358505249 | BCE Loss: 1.0048108100891113\n",
      "Epoch 197 / 500 | iteration 15 / 30 | Total Loss: 4.11228609085083 | KNN Loss: 3.0783259868621826 | BCE Loss: 1.033960223197937\n",
      "Epoch 197 / 500 | iteration 20 / 30 | Total Loss: 4.098978042602539 | KNN Loss: 3.070249080657959 | BCE Loss: 1.02872896194458\n",
      "Epoch 197 / 500 | iteration 25 / 30 | Total Loss: 4.011984348297119 | KNN Loss: 3.0266706943511963 | BCE Loss: 0.9853137731552124\n",
      "Epoch 198 / 500 | iteration 0 / 30 | Total Loss: 4.1613054275512695 | KNN Loss: 3.1362857818603516 | BCE Loss: 1.025019645690918\n",
      "Epoch 198 / 500 | iteration 5 / 30 | Total Loss: 4.087228775024414 | KNN Loss: 3.0584001541137695 | BCE Loss: 1.028828740119934\n",
      "Epoch 198 / 500 | iteration 10 / 30 | Total Loss: 4.093571186065674 | KNN Loss: 3.0608294010162354 | BCE Loss: 1.032741904258728\n",
      "Epoch 198 / 500 | iteration 15 / 30 | Total Loss: 4.105973720550537 | KNN Loss: 3.1026079654693604 | BCE Loss: 1.0033658742904663\n",
      "Epoch 198 / 500 | iteration 20 / 30 | Total Loss: 4.0853657722473145 | KNN Loss: 3.0617871284484863 | BCE Loss: 1.0235785245895386\n",
      "Epoch 198 / 500 | iteration 25 / 30 | Total Loss: 4.07635498046875 | KNN Loss: 3.0549168586730957 | BCE Loss: 1.0214383602142334\n",
      "Epoch 199 / 500 | iteration 0 / 30 | Total Loss: 4.106283187866211 | KNN Loss: 3.0853147506713867 | BCE Loss: 1.0209686756134033\n",
      "Epoch 199 / 500 | iteration 5 / 30 | Total Loss: 4.079401969909668 | KNN Loss: 3.0621461868286133 | BCE Loss: 1.0172555446624756\n",
      "Epoch 199 / 500 | iteration 10 / 30 | Total Loss: 4.103110313415527 | KNN Loss: 3.06742000579834 | BCE Loss: 1.0356903076171875\n",
      "Epoch 199 / 500 | iteration 15 / 30 | Total Loss: 4.055632591247559 | KNN Loss: 3.0400514602661133 | BCE Loss: 1.0155812501907349\n",
      "Epoch 199 / 500 | iteration 20 / 30 | Total Loss: 4.101001739501953 | KNN Loss: 3.071549415588379 | BCE Loss: 1.0294524431228638\n",
      "Epoch 199 / 500 | iteration 25 / 30 | Total Loss: 4.035943984985352 | KNN Loss: 3.03839373588562 | BCE Loss: 0.9975504875183105\n",
      "Epoch   200: reducing learning rate of group 0 to 2.8824e-04.\n",
      "Epoch 200 / 500 | iteration 0 / 30 | Total Loss: 4.034760475158691 | KNN Loss: 3.0222232341766357 | BCE Loss: 1.0125374794006348\n",
      "Epoch 200 / 500 | iteration 5 / 30 | Total Loss: 4.077498435974121 | KNN Loss: 3.0652382373809814 | BCE Loss: 1.0122599601745605\n",
      "Epoch 200 / 500 | iteration 10 / 30 | Total Loss: 4.093029975891113 | KNN Loss: 3.0845649242401123 | BCE Loss: 1.008465051651001\n",
      "Epoch 200 / 500 | iteration 15 / 30 | Total Loss: 4.082396507263184 | KNN Loss: 3.065704584121704 | BCE Loss: 1.0166921615600586\n",
      "Epoch 200 / 500 | iteration 20 / 30 | Total Loss: 4.069681167602539 | KNN Loss: 3.0645127296447754 | BCE Loss: 1.0051685571670532\n",
      "Epoch 200 / 500 | iteration 25 / 30 | Total Loss: 4.130965709686279 | KNN Loss: 3.1050639152526855 | BCE Loss: 1.0259017944335938\n",
      "Epoch 201 / 500 | iteration 0 / 30 | Total Loss: 4.095153331756592 | KNN Loss: 3.0809426307678223 | BCE Loss: 1.014210820198059\n",
      "Epoch 201 / 500 | iteration 5 / 30 | Total Loss: 4.083546161651611 | KNN Loss: 3.086989164352417 | BCE Loss: 0.9965571761131287\n",
      "Epoch 201 / 500 | iteration 10 / 30 | Total Loss: 4.084065914154053 | KNN Loss: 3.065154790878296 | BCE Loss: 1.0189111232757568\n",
      "Epoch 201 / 500 | iteration 15 / 30 | Total Loss: 4.0769243240356445 | KNN Loss: 3.055447816848755 | BCE Loss: 1.0214765071868896\n",
      "Epoch 201 / 500 | iteration 20 / 30 | Total Loss: 4.102052211761475 | KNN Loss: 3.073920488357544 | BCE Loss: 1.0281317234039307\n",
      "Epoch 201 / 500 | iteration 25 / 30 | Total Loss: 4.111913681030273 | KNN Loss: 3.0745279788970947 | BCE Loss: 1.0373857021331787\n",
      "Epoch 202 / 500 | iteration 0 / 30 | Total Loss: 4.066762924194336 | KNN Loss: 3.054201364517212 | BCE Loss: 1.012561321258545\n",
      "Epoch 202 / 500 | iteration 5 / 30 | Total Loss: 4.0927019119262695 | KNN Loss: 3.0508618354797363 | BCE Loss: 1.0418403148651123\n",
      "Epoch 202 / 500 | iteration 10 / 30 | Total Loss: 4.040840148925781 | KNN Loss: 3.04075288772583 | BCE Loss: 1.000087022781372\n",
      "Epoch 202 / 500 | iteration 15 / 30 | Total Loss: 4.046490669250488 | KNN Loss: 3.019028425216675 | BCE Loss: 1.0274624824523926\n",
      "Epoch 202 / 500 | iteration 20 / 30 | Total Loss: 4.150702953338623 | KNN Loss: 3.1248085498809814 | BCE Loss: 1.0258944034576416\n",
      "Epoch 202 / 500 | iteration 25 / 30 | Total Loss: 4.086461067199707 | KNN Loss: 3.0779383182525635 | BCE Loss: 1.0085229873657227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 203 / 500 | iteration 0 / 30 | Total Loss: 4.118307113647461 | KNN Loss: 3.099837064743042 | BCE Loss: 1.0184698104858398\n",
      "Epoch 203 / 500 | iteration 5 / 30 | Total Loss: 4.082684516906738 | KNN Loss: 3.0623574256896973 | BCE Loss: 1.020326852798462\n",
      "Epoch 203 / 500 | iteration 10 / 30 | Total Loss: 4.06756591796875 | KNN Loss: 3.0335588455200195 | BCE Loss: 1.0340068340301514\n",
      "Epoch 203 / 500 | iteration 15 / 30 | Total Loss: 4.056091785430908 | KNN Loss: 3.0439164638519287 | BCE Loss: 1.01217520236969\n",
      "Epoch 203 / 500 | iteration 20 / 30 | Total Loss: 4.111517906188965 | KNN Loss: 3.0536048412323 | BCE Loss: 1.057912826538086\n",
      "Epoch 203 / 500 | iteration 25 / 30 | Total Loss: 4.061524391174316 | KNN Loss: 3.045161724090576 | BCE Loss: 1.0163625478744507\n",
      "Epoch 204 / 500 | iteration 0 / 30 | Total Loss: 4.067591667175293 | KNN Loss: 3.0468993186950684 | BCE Loss: 1.020692229270935\n",
      "Epoch 204 / 500 | iteration 5 / 30 | Total Loss: 4.079185485839844 | KNN Loss: 3.0642359256744385 | BCE Loss: 1.0149493217468262\n",
      "Epoch 204 / 500 | iteration 10 / 30 | Total Loss: 4.073474407196045 | KNN Loss: 3.0498952865600586 | BCE Loss: 1.0235792398452759\n",
      "Epoch 204 / 500 | iteration 15 / 30 | Total Loss: 4.052750587463379 | KNN Loss: 3.0440995693206787 | BCE Loss: 1.0086512565612793\n",
      "Epoch 204 / 500 | iteration 20 / 30 | Total Loss: 4.072410583496094 | KNN Loss: 3.074176788330078 | BCE Loss: 0.9982336163520813\n",
      "Epoch 204 / 500 | iteration 25 / 30 | Total Loss: 4.064035892486572 | KNN Loss: 3.052018165588379 | BCE Loss: 1.012017846107483\n",
      "Epoch 205 / 500 | iteration 0 / 30 | Total Loss: 4.1390838623046875 | KNN Loss: 3.1165666580200195 | BCE Loss: 1.0225169658660889\n",
      "Epoch 205 / 500 | iteration 5 / 30 | Total Loss: 4.138205528259277 | KNN Loss: 3.11086106300354 | BCE Loss: 1.0273444652557373\n",
      "Epoch 205 / 500 | iteration 10 / 30 | Total Loss: 4.06178092956543 | KNN Loss: 3.040788412094116 | BCE Loss: 1.0209925174713135\n",
      "Epoch 205 / 500 | iteration 15 / 30 | Total Loss: 4.128854274749756 | KNN Loss: 3.110511541366577 | BCE Loss: 1.0183427333831787\n",
      "Epoch 205 / 500 | iteration 20 / 30 | Total Loss: 4.050210952758789 | KNN Loss: 3.0698814392089844 | BCE Loss: 0.9803293943405151\n",
      "Epoch 205 / 500 | iteration 25 / 30 | Total Loss: 4.092398166656494 | KNN Loss: 3.0321264266967773 | BCE Loss: 1.0602716207504272\n",
      "Epoch 206 / 500 | iteration 0 / 30 | Total Loss: 4.088367938995361 | KNN Loss: 3.0828957557678223 | BCE Loss: 1.005472183227539\n",
      "Epoch 206 / 500 | iteration 5 / 30 | Total Loss: 4.06497859954834 | KNN Loss: 3.051915407180786 | BCE Loss: 1.0130630731582642\n",
      "Epoch 206 / 500 | iteration 10 / 30 | Total Loss: 4.120820999145508 | KNN Loss: 3.0517241954803467 | BCE Loss: 1.069096565246582\n",
      "Epoch 206 / 500 | iteration 15 / 30 | Total Loss: 4.095926284790039 | KNN Loss: 3.094809055328369 | BCE Loss: 1.001117467880249\n",
      "Epoch 206 / 500 | iteration 20 / 30 | Total Loss: 4.151564598083496 | KNN Loss: 3.1326491832733154 | BCE Loss: 1.0189156532287598\n",
      "Epoch 206 / 500 | iteration 25 / 30 | Total Loss: 4.05728006362915 | KNN Loss: 3.0406877994537354 | BCE Loss: 1.016592264175415\n",
      "Epoch 207 / 500 | iteration 0 / 30 | Total Loss: 4.090371608734131 | KNN Loss: 3.0766444206237793 | BCE Loss: 1.013727068901062\n",
      "Epoch 207 / 500 | iteration 5 / 30 | Total Loss: 4.102138519287109 | KNN Loss: 3.0943403244018555 | BCE Loss: 1.007798194885254\n",
      "Epoch 207 / 500 | iteration 10 / 30 | Total Loss: 4.094864845275879 | KNN Loss: 3.0599632263183594 | BCE Loss: 1.0349018573760986\n",
      "Epoch 207 / 500 | iteration 15 / 30 | Total Loss: 4.062191963195801 | KNN Loss: 3.0423576831817627 | BCE Loss: 1.0198341608047485\n",
      "Epoch 207 / 500 | iteration 20 / 30 | Total Loss: 4.085222244262695 | KNN Loss: 3.0575926303863525 | BCE Loss: 1.0276293754577637\n",
      "Epoch 207 / 500 | iteration 25 / 30 | Total Loss: 4.069235801696777 | KNN Loss: 3.0586726665496826 | BCE Loss: 1.0105633735656738\n",
      "Epoch 208 / 500 | iteration 0 / 30 | Total Loss: 4.0828399658203125 | KNN Loss: 3.072922468185425 | BCE Loss: 1.0099174976348877\n",
      "Epoch 208 / 500 | iteration 5 / 30 | Total Loss: 4.123891830444336 | KNN Loss: 3.0980591773986816 | BCE Loss: 1.0258326530456543\n",
      "Epoch 208 / 500 | iteration 10 / 30 | Total Loss: 4.079755783081055 | KNN Loss: 3.0578079223632812 | BCE Loss: 1.0219480991363525\n",
      "Epoch 208 / 500 | iteration 15 / 30 | Total Loss: 4.0916314125061035 | KNN Loss: 3.070199728012085 | BCE Loss: 1.021431565284729\n",
      "Epoch 208 / 500 | iteration 20 / 30 | Total Loss: 4.099881649017334 | KNN Loss: 3.101085662841797 | BCE Loss: 0.9987961649894714\n",
      "Epoch 208 / 500 | iteration 25 / 30 | Total Loss: 4.106584548950195 | KNN Loss: 3.0759990215301514 | BCE Loss: 1.0305852890014648\n",
      "Epoch 209 / 500 | iteration 0 / 30 | Total Loss: 4.114561557769775 | KNN Loss: 3.0947704315185547 | BCE Loss: 1.0197910070419312\n",
      "Epoch 209 / 500 | iteration 5 / 30 | Total Loss: 4.113820552825928 | KNN Loss: 3.0882303714752197 | BCE Loss: 1.025590181350708\n",
      "Epoch 209 / 500 | iteration 10 / 30 | Total Loss: 4.129974842071533 | KNN Loss: 3.060528516769409 | BCE Loss: 1.0694462060928345\n",
      "Epoch 209 / 500 | iteration 15 / 30 | Total Loss: 4.139707088470459 | KNN Loss: 3.1021006107330322 | BCE Loss: 1.0376065969467163\n",
      "Epoch 209 / 500 | iteration 20 / 30 | Total Loss: 4.092863082885742 | KNN Loss: 3.067948341369629 | BCE Loss: 1.0249148607254028\n",
      "Epoch 209 / 500 | iteration 25 / 30 | Total Loss: 4.075167655944824 | KNN Loss: 3.044821262359619 | BCE Loss: 1.0303466320037842\n",
      "Epoch 210 / 500 | iteration 0 / 30 | Total Loss: 4.049032688140869 | KNN Loss: 3.028745174407959 | BCE Loss: 1.0202875137329102\n",
      "Epoch 210 / 500 | iteration 5 / 30 | Total Loss: 4.108144283294678 | KNN Loss: 3.0877676010131836 | BCE Loss: 1.0203766822814941\n",
      "Epoch 210 / 500 | iteration 10 / 30 | Total Loss: 4.118997573852539 | KNN Loss: 3.077853202819824 | BCE Loss: 1.0411443710327148\n",
      "Epoch 210 / 500 | iteration 15 / 30 | Total Loss: 4.083012580871582 | KNN Loss: 3.0647847652435303 | BCE Loss: 1.0182275772094727\n",
      "Epoch 210 / 500 | iteration 20 / 30 | Total Loss: 4.13130521774292 | KNN Loss: 3.133601188659668 | BCE Loss: 0.9977041482925415\n",
      "Epoch 210 / 500 | iteration 25 / 30 | Total Loss: 4.079946517944336 | KNN Loss: 3.086524248123169 | BCE Loss: 0.9934220314025879\n",
      "Epoch   211: reducing learning rate of group 0 to 2.0177e-04.\n",
      "Epoch 211 / 500 | iteration 0 / 30 | Total Loss: 4.091104507446289 | KNN Loss: 3.054539918899536 | BCE Loss: 1.036564588546753\n",
      "Epoch 211 / 500 | iteration 5 / 30 | Total Loss: 4.072870254516602 | KNN Loss: 3.0610713958740234 | BCE Loss: 1.011798620223999\n",
      "Epoch 211 / 500 | iteration 10 / 30 | Total Loss: 4.057436943054199 | KNN Loss: 3.049949884414673 | BCE Loss: 1.0074870586395264\n",
      "Epoch 211 / 500 | iteration 15 / 30 | Total Loss: 4.093075752258301 | KNN Loss: 3.071244478225708 | BCE Loss: 1.0218311548233032\n",
      "Epoch 211 / 500 | iteration 20 / 30 | Total Loss: 4.128479957580566 | KNN Loss: 3.0821874141693115 | BCE Loss: 1.046292781829834\n",
      "Epoch 211 / 500 | iteration 25 / 30 | Total Loss: 4.129131317138672 | KNN Loss: 3.0841140747070312 | BCE Loss: 1.0450174808502197\n",
      "Epoch 212 / 500 | iteration 0 / 30 | Total Loss: 4.062955856323242 | KNN Loss: 3.0548179149627686 | BCE Loss: 1.0081379413604736\n",
      "Epoch 212 / 500 | iteration 5 / 30 | Total Loss: 4.09188175201416 | KNN Loss: 3.056901216506958 | BCE Loss: 1.0349807739257812\n",
      "Epoch 212 / 500 | iteration 10 / 30 | Total Loss: 4.070242881774902 | KNN Loss: 3.043191432952881 | BCE Loss: 1.0270516872406006\n",
      "Epoch 212 / 500 | iteration 15 / 30 | Total Loss: 4.021831512451172 | KNN Loss: 3.028183937072754 | BCE Loss: 0.9936477541923523\n",
      "Epoch 212 / 500 | iteration 20 / 30 | Total Loss: 4.123885631561279 | KNN Loss: 3.1139352321624756 | BCE Loss: 1.0099505186080933\n",
      "Epoch 212 / 500 | iteration 25 / 30 | Total Loss: 4.0362396240234375 | KNN Loss: 3.026923179626465 | BCE Loss: 1.0093166828155518\n",
      "Epoch 213 / 500 | iteration 0 / 30 | Total Loss: 4.114115238189697 | KNN Loss: 3.083008289337158 | BCE Loss: 1.0311070680618286\n",
      "Epoch 213 / 500 | iteration 5 / 30 | Total Loss: 4.138983726501465 | KNN Loss: 3.0917062759399414 | BCE Loss: 1.0472774505615234\n",
      "Epoch 213 / 500 | iteration 10 / 30 | Total Loss: 4.088810443878174 | KNN Loss: 3.0512678623199463 | BCE Loss: 1.0375425815582275\n",
      "Epoch 213 / 500 | iteration 15 / 30 | Total Loss: 4.082148551940918 | KNN Loss: 3.0420494079589844 | BCE Loss: 1.0400993824005127\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 213 / 500 | iteration 20 / 30 | Total Loss: 4.03912353515625 | KNN Loss: 3.0429651737213135 | BCE Loss: 0.9961584806442261\n",
      "Epoch 213 / 500 | iteration 25 / 30 | Total Loss: 4.074508190155029 | KNN Loss: 3.0805087089538574 | BCE Loss: 0.9939993619918823\n",
      "Epoch 214 / 500 | iteration 0 / 30 | Total Loss: 4.083720684051514 | KNN Loss: 3.05077862739563 | BCE Loss: 1.0329420566558838\n",
      "Epoch 214 / 500 | iteration 5 / 30 | Total Loss: 4.0949273109436035 | KNN Loss: 3.0743186473846436 | BCE Loss: 1.0206087827682495\n",
      "Epoch 214 / 500 | iteration 10 / 30 | Total Loss: 4.110234260559082 | KNN Loss: 3.0588455200195312 | BCE Loss: 1.0513886213302612\n",
      "Epoch 214 / 500 | iteration 15 / 30 | Total Loss: 4.031532287597656 | KNN Loss: 3.032217502593994 | BCE Loss: 0.9993147253990173\n",
      "Epoch 214 / 500 | iteration 20 / 30 | Total Loss: 4.060774803161621 | KNN Loss: 3.0407092571258545 | BCE Loss: 1.0200657844543457\n",
      "Epoch 214 / 500 | iteration 25 / 30 | Total Loss: 4.075634956359863 | KNN Loss: 3.057074785232544 | BCE Loss: 1.0185604095458984\n",
      "Epoch 215 / 500 | iteration 0 / 30 | Total Loss: 4.081105709075928 | KNN Loss: 3.0996603965759277 | BCE Loss: 0.9814454317092896\n",
      "Epoch 215 / 500 | iteration 5 / 30 | Total Loss: 4.136687278747559 | KNN Loss: 3.084385633468628 | BCE Loss: 1.0523014068603516\n",
      "Epoch 215 / 500 | iteration 10 / 30 | Total Loss: 4.104160785675049 | KNN Loss: 3.0795390605926514 | BCE Loss: 1.0246217250823975\n",
      "Epoch 215 / 500 | iteration 15 / 30 | Total Loss: 4.065434455871582 | KNN Loss: 3.0475409030914307 | BCE Loss: 1.0178934335708618\n",
      "Epoch 215 / 500 | iteration 20 / 30 | Total Loss: 4.088799476623535 | KNN Loss: 3.0757129192352295 | BCE Loss: 1.0130866765975952\n",
      "Epoch 215 / 500 | iteration 25 / 30 | Total Loss: 4.091464996337891 | KNN Loss: 3.081239938735962 | BCE Loss: 1.0102251768112183\n",
      "Epoch 216 / 500 | iteration 0 / 30 | Total Loss: 4.095613479614258 | KNN Loss: 3.067289113998413 | BCE Loss: 1.0283243656158447\n",
      "Epoch 216 / 500 | iteration 5 / 30 | Total Loss: 4.0970139503479 | KNN Loss: 3.0772223472595215 | BCE Loss: 1.0197917222976685\n",
      "Epoch 216 / 500 | iteration 10 / 30 | Total Loss: 4.054080009460449 | KNN Loss: 3.0422725677490234 | BCE Loss: 1.0118072032928467\n",
      "Epoch 216 / 500 | iteration 15 / 30 | Total Loss: 4.12001895904541 | KNN Loss: 3.0926146507263184 | BCE Loss: 1.027404546737671\n",
      "Epoch 216 / 500 | iteration 20 / 30 | Total Loss: 4.03720760345459 | KNN Loss: 3.0604872703552246 | BCE Loss: 0.9767202138900757\n",
      "Epoch 216 / 500 | iteration 25 / 30 | Total Loss: 4.115283012390137 | KNN Loss: 3.0971131324768066 | BCE Loss: 1.0181701183319092\n",
      "Epoch 217 / 500 | iteration 0 / 30 | Total Loss: 4.066553592681885 | KNN Loss: 3.0650246143341064 | BCE Loss: 1.0015288591384888\n",
      "Epoch 217 / 500 | iteration 5 / 30 | Total Loss: 4.121146202087402 | KNN Loss: 3.106348991394043 | BCE Loss: 1.0147974491119385\n",
      "Epoch 217 / 500 | iteration 10 / 30 | Total Loss: 4.066943645477295 | KNN Loss: 3.0806539058685303 | BCE Loss: 0.9862898588180542\n",
      "Epoch 217 / 500 | iteration 15 / 30 | Total Loss: 4.097416877746582 | KNN Loss: 3.0813422203063965 | BCE Loss: 1.0160748958587646\n",
      "Epoch 217 / 500 | iteration 20 / 30 | Total Loss: 4.154790878295898 | KNN Loss: 3.1199066638946533 | BCE Loss: 1.0348844528198242\n",
      "Epoch 217 / 500 | iteration 25 / 30 | Total Loss: 4.079167366027832 | KNN Loss: 3.0585625171661377 | BCE Loss: 1.0206049680709839\n",
      "Epoch 218 / 500 | iteration 0 / 30 | Total Loss: 4.0551300048828125 | KNN Loss: 3.066128730773926 | BCE Loss: 0.9890010356903076\n",
      "Epoch 218 / 500 | iteration 5 / 30 | Total Loss: 4.07540225982666 | KNN Loss: 3.054758310317993 | BCE Loss: 1.020644187927246\n",
      "Epoch 218 / 500 | iteration 10 / 30 | Total Loss: 4.164364337921143 | KNN Loss: 3.1136937141418457 | BCE Loss: 1.0506706237792969\n",
      "Epoch 218 / 500 | iteration 15 / 30 | Total Loss: 4.076752185821533 | KNN Loss: 3.060631036758423 | BCE Loss: 1.0161211490631104\n",
      "Epoch 218 / 500 | iteration 20 / 30 | Total Loss: 4.05718994140625 | KNN Loss: 3.052990436553955 | BCE Loss: 1.004199743270874\n",
      "Epoch 218 / 500 | iteration 25 / 30 | Total Loss: 4.107541561126709 | KNN Loss: 3.0809929370880127 | BCE Loss: 1.0265486240386963\n",
      "Epoch 219 / 500 | iteration 0 / 30 | Total Loss: 4.075655937194824 | KNN Loss: 3.0503907203674316 | BCE Loss: 1.0252649784088135\n",
      "Epoch 219 / 500 | iteration 5 / 30 | Total Loss: 4.114710807800293 | KNN Loss: 3.0785610675811768 | BCE Loss: 1.036149501800537\n",
      "Epoch 219 / 500 | iteration 10 / 30 | Total Loss: 4.072904586791992 | KNN Loss: 3.0695607662200928 | BCE Loss: 1.0033437013626099\n",
      "Epoch 219 / 500 | iteration 15 / 30 | Total Loss: 4.078972816467285 | KNN Loss: 3.0543477535247803 | BCE Loss: 1.0246250629425049\n",
      "Epoch 219 / 500 | iteration 20 / 30 | Total Loss: 4.017409801483154 | KNN Loss: 3.021294116973877 | BCE Loss: 0.9961155652999878\n",
      "Epoch 219 / 500 | iteration 25 / 30 | Total Loss: 4.076890468597412 | KNN Loss: 3.0565853118896484 | BCE Loss: 1.0203051567077637\n",
      "Epoch 220 / 500 | iteration 0 / 30 | Total Loss: 4.097535610198975 | KNN Loss: 3.0581212043762207 | BCE Loss: 1.039414405822754\n",
      "Epoch 220 / 500 | iteration 5 / 30 | Total Loss: 4.102167129516602 | KNN Loss: 3.062579870223999 | BCE Loss: 1.0395872592926025\n",
      "Epoch 220 / 500 | iteration 10 / 30 | Total Loss: 4.021301746368408 | KNN Loss: 3.0279903411865234 | BCE Loss: 0.9933115839958191\n",
      "Epoch 220 / 500 | iteration 15 / 30 | Total Loss: 4.120652198791504 | KNN Loss: 3.0481488704681396 | BCE Loss: 1.0725034475326538\n",
      "Epoch 220 / 500 | iteration 20 / 30 | Total Loss: 4.14501953125 | KNN Loss: 3.0936644077301025 | BCE Loss: 1.0513551235198975\n",
      "Epoch 220 / 500 | iteration 25 / 30 | Total Loss: 4.088109970092773 | KNN Loss: 3.0654361248016357 | BCE Loss: 1.0226737260818481\n",
      "Epoch 221 / 500 | iteration 0 / 30 | Total Loss: 4.061172008514404 | KNN Loss: 3.042546510696411 | BCE Loss: 1.0186253786087036\n",
      "Epoch 221 / 500 | iteration 5 / 30 | Total Loss: 4.119012832641602 | KNN Loss: 3.064115285873413 | BCE Loss: 1.054897665977478\n",
      "Epoch 221 / 500 | iteration 10 / 30 | Total Loss: 4.082697868347168 | KNN Loss: 3.060295581817627 | BCE Loss: 1.022402048110962\n",
      "Epoch 221 / 500 | iteration 15 / 30 | Total Loss: 4.051260948181152 | KNN Loss: 3.039802074432373 | BCE Loss: 1.0114588737487793\n",
      "Epoch 221 / 500 | iteration 20 / 30 | Total Loss: 4.08491325378418 | KNN Loss: 3.071167469024658 | BCE Loss: 1.0137457847595215\n",
      "Epoch 221 / 500 | iteration 25 / 30 | Total Loss: 4.088831901550293 | KNN Loss: 3.066179037094116 | BCE Loss: 1.0226526260375977\n",
      "Epoch 222 / 500 | iteration 0 / 30 | Total Loss: 4.095179557800293 | KNN Loss: 3.060502767562866 | BCE Loss: 1.0346767902374268\n",
      "Epoch 222 / 500 | iteration 5 / 30 | Total Loss: 4.088232517242432 | KNN Loss: 3.04223895072937 | BCE Loss: 1.045993447303772\n",
      "Epoch 222 / 500 | iteration 10 / 30 | Total Loss: 4.110908031463623 | KNN Loss: 3.069491147994995 | BCE Loss: 1.0414170026779175\n",
      "Epoch 222 / 500 | iteration 15 / 30 | Total Loss: 4.089563369750977 | KNN Loss: 3.044192314147949 | BCE Loss: 1.0453712940216064\n",
      "Epoch 222 / 500 | iteration 20 / 30 | Total Loss: 4.133700370788574 | KNN Loss: 3.099323272705078 | BCE Loss: 1.0343773365020752\n",
      "Epoch 222 / 500 | iteration 25 / 30 | Total Loss: 4.064793109893799 | KNN Loss: 3.0728986263275146 | BCE Loss: 0.9918943643569946\n",
      "Epoch 223 / 500 | iteration 0 / 30 | Total Loss: 4.113968849182129 | KNN Loss: 3.0757761001586914 | BCE Loss: 1.0381929874420166\n",
      "Epoch 223 / 500 | iteration 5 / 30 | Total Loss: 4.119686126708984 | KNN Loss: 3.0846405029296875 | BCE Loss: 1.0350453853607178\n",
      "Epoch 223 / 500 | iteration 10 / 30 | Total Loss: 4.088010311126709 | KNN Loss: 3.080381155014038 | BCE Loss: 1.0076290369033813\n",
      "Epoch 223 / 500 | iteration 15 / 30 | Total Loss: 4.083586692810059 | KNN Loss: 3.0457286834716797 | BCE Loss: 1.037858009338379\n",
      "Epoch 223 / 500 | iteration 20 / 30 | Total Loss: 4.064315319061279 | KNN Loss: 3.0379831790924072 | BCE Loss: 1.0263322591781616\n",
      "Epoch 223 / 500 | iteration 25 / 30 | Total Loss: 4.068856239318848 | KNN Loss: 3.049363851547241 | BCE Loss: 1.0194923877716064\n",
      "Epoch 224 / 500 | iteration 0 / 30 | Total Loss: 4.078168869018555 | KNN Loss: 3.06681489944458 | BCE Loss: 1.0113540887832642\n",
      "Epoch 224 / 500 | iteration 5 / 30 | Total Loss: 4.077884674072266 | KNN Loss: 3.062065601348877 | BCE Loss: 1.0158191919326782\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 224 / 500 | iteration 10 / 30 | Total Loss: 4.067376136779785 | KNN Loss: 3.0481350421905518 | BCE Loss: 1.0192409753799438\n",
      "Epoch 224 / 500 | iteration 15 / 30 | Total Loss: 4.119959831237793 | KNN Loss: 3.0695080757141113 | BCE Loss: 1.0504515171051025\n",
      "Epoch 224 / 500 | iteration 20 / 30 | Total Loss: 4.108808517456055 | KNN Loss: 3.0777597427368164 | BCE Loss: 1.0310487747192383\n",
      "Epoch 224 / 500 | iteration 25 / 30 | Total Loss: 4.042533874511719 | KNN Loss: 3.0351309776306152 | BCE Loss: 1.0074028968811035\n",
      "Epoch 225 / 500 | iteration 0 / 30 | Total Loss: 4.070557117462158 | KNN Loss: 3.0749635696411133 | BCE Loss: 0.9955934286117554\n",
      "Epoch 225 / 500 | iteration 5 / 30 | Total Loss: 4.1121039390563965 | KNN Loss: 3.1139438152313232 | BCE Loss: 0.9981602430343628\n",
      "Epoch 225 / 500 | iteration 10 / 30 | Total Loss: 4.0951385498046875 | KNN Loss: 3.09186053276062 | BCE Loss: 1.003278136253357\n",
      "Epoch 225 / 500 | iteration 15 / 30 | Total Loss: 4.094564437866211 | KNN Loss: 3.0406112670898438 | BCE Loss: 1.0539531707763672\n",
      "Epoch 225 / 500 | iteration 20 / 30 | Total Loss: 4.068653106689453 | KNN Loss: 3.065603733062744 | BCE Loss: 1.003049373626709\n",
      "Epoch 225 / 500 | iteration 25 / 30 | Total Loss: 4.132298469543457 | KNN Loss: 3.0775959491729736 | BCE Loss: 1.0547027587890625\n",
      "Epoch   226: reducing learning rate of group 0 to 1.4124e-04.\n",
      "Epoch 226 / 500 | iteration 0 / 30 | Total Loss: 4.123142719268799 | KNN Loss: 3.0914885997772217 | BCE Loss: 1.0316541194915771\n",
      "Epoch 226 / 500 | iteration 5 / 30 | Total Loss: 4.077662944793701 | KNN Loss: 3.0755624771118164 | BCE Loss: 1.0021004676818848\n",
      "Epoch 226 / 500 | iteration 10 / 30 | Total Loss: 4.031258583068848 | KNN Loss: 3.0417675971984863 | BCE Loss: 0.9894909858703613\n",
      "Epoch 226 / 500 | iteration 15 / 30 | Total Loss: 4.096269607543945 | KNN Loss: 3.074793815612793 | BCE Loss: 1.021475911140442\n",
      "Epoch 226 / 500 | iteration 20 / 30 | Total Loss: 4.122425079345703 | KNN Loss: 3.0931828022003174 | BCE Loss: 1.0292425155639648\n",
      "Epoch 226 / 500 | iteration 25 / 30 | Total Loss: 4.035254001617432 | KNN Loss: 3.0451650619506836 | BCE Loss: 0.9900890588760376\n",
      "Epoch 227 / 500 | iteration 0 / 30 | Total Loss: 4.039965629577637 | KNN Loss: 3.0490615367889404 | BCE Loss: 0.9909039735794067\n",
      "Epoch 227 / 500 | iteration 5 / 30 | Total Loss: 4.145716667175293 | KNN Loss: 3.105071783065796 | BCE Loss: 1.0406450033187866\n",
      "Epoch 227 / 500 | iteration 10 / 30 | Total Loss: 4.093277931213379 | KNN Loss: 3.089940309524536 | BCE Loss: 1.0033378601074219\n",
      "Epoch 227 / 500 | iteration 15 / 30 | Total Loss: 4.068319320678711 | KNN Loss: 3.0565974712371826 | BCE Loss: 1.0117218494415283\n",
      "Epoch 227 / 500 | iteration 20 / 30 | Total Loss: 4.0581955909729 | KNN Loss: 3.0285568237304688 | BCE Loss: 1.0296387672424316\n",
      "Epoch 227 / 500 | iteration 25 / 30 | Total Loss: 4.125298976898193 | KNN Loss: 3.0889732837677 | BCE Loss: 1.0363256931304932\n",
      "Epoch 228 / 500 | iteration 0 / 30 | Total Loss: 4.084207534790039 | KNN Loss: 3.051208972930908 | BCE Loss: 1.0329985618591309\n",
      "Epoch 228 / 500 | iteration 5 / 30 | Total Loss: 4.074631690979004 | KNN Loss: 3.0546765327453613 | BCE Loss: 1.0199549198150635\n",
      "Epoch 228 / 500 | iteration 10 / 30 | Total Loss: 4.10044527053833 | KNN Loss: 3.0708115100860596 | BCE Loss: 1.029633641242981\n",
      "Epoch 228 / 500 | iteration 15 / 30 | Total Loss: 4.110567092895508 | KNN Loss: 3.1061012744903564 | BCE Loss: 1.0044660568237305\n",
      "Epoch 228 / 500 | iteration 20 / 30 | Total Loss: 4.112277984619141 | KNN Loss: 3.089078903198242 | BCE Loss: 1.0231989622116089\n",
      "Epoch 228 / 500 | iteration 25 / 30 | Total Loss: 4.075910568237305 | KNN Loss: 3.0524470806121826 | BCE Loss: 1.0234637260437012\n",
      "Epoch 229 / 500 | iteration 0 / 30 | Total Loss: 4.0978593826293945 | KNN Loss: 3.0729055404663086 | BCE Loss: 1.024953842163086\n",
      "Epoch 229 / 500 | iteration 5 / 30 | Total Loss: 4.093054294586182 | KNN Loss: 3.0913898944854736 | BCE Loss: 1.001664400100708\n",
      "Epoch 229 / 500 | iteration 10 / 30 | Total Loss: 4.102222919464111 | KNN Loss: 3.085658550262451 | BCE Loss: 1.0165643692016602\n",
      "Epoch 229 / 500 | iteration 15 / 30 | Total Loss: 4.115581035614014 | KNN Loss: 3.11380934715271 | BCE Loss: 1.0017718076705933\n",
      "Epoch 229 / 500 | iteration 20 / 30 | Total Loss: 4.096269607543945 | KNN Loss: 3.0600602626800537 | BCE Loss: 1.0362095832824707\n",
      "Epoch 229 / 500 | iteration 25 / 30 | Total Loss: 4.057615280151367 | KNN Loss: 3.0371217727661133 | BCE Loss: 1.020493507385254\n",
      "Epoch 230 / 500 | iteration 0 / 30 | Total Loss: 4.104022026062012 | KNN Loss: 3.072788953781128 | BCE Loss: 1.031233310699463\n",
      "Epoch 230 / 500 | iteration 5 / 30 | Total Loss: 4.037858009338379 | KNN Loss: 3.0264573097229004 | BCE Loss: 1.0114009380340576\n",
      "Epoch 230 / 500 | iteration 10 / 30 | Total Loss: 4.087761402130127 | KNN Loss: 3.0817530155181885 | BCE Loss: 1.006008267402649\n",
      "Epoch 230 / 500 | iteration 15 / 30 | Total Loss: 4.0954999923706055 | KNN Loss: 3.085153818130493 | BCE Loss: 1.0103461742401123\n",
      "Epoch 230 / 500 | iteration 20 / 30 | Total Loss: 4.056653022766113 | KNN Loss: 3.0571742057800293 | BCE Loss: 0.9994785785675049\n",
      "Epoch 230 / 500 | iteration 25 / 30 | Total Loss: 4.092653751373291 | KNN Loss: 3.0614118576049805 | BCE Loss: 1.0312420129776\n",
      "Epoch 231 / 500 | iteration 0 / 30 | Total Loss: 4.074484825134277 | KNN Loss: 3.0614795684814453 | BCE Loss: 1.013005018234253\n",
      "Epoch 231 / 500 | iteration 5 / 30 | Total Loss: 4.098211288452148 | KNN Loss: 3.079686403274536 | BCE Loss: 1.0185251235961914\n",
      "Epoch 231 / 500 | iteration 10 / 30 | Total Loss: 4.14804744720459 | KNN Loss: 3.1114695072174072 | BCE Loss: 1.036577820777893\n",
      "Epoch 231 / 500 | iteration 15 / 30 | Total Loss: 4.051716327667236 | KNN Loss: 3.0318145751953125 | BCE Loss: 1.0199016332626343\n",
      "Epoch 231 / 500 | iteration 20 / 30 | Total Loss: 4.056374549865723 | KNN Loss: 3.0447847843170166 | BCE Loss: 1.0115900039672852\n",
      "Epoch 231 / 500 | iteration 25 / 30 | Total Loss: 4.073949813842773 | KNN Loss: 3.0603365898132324 | BCE Loss: 1.0136134624481201\n",
      "Epoch 232 / 500 | iteration 0 / 30 | Total Loss: 4.08073616027832 | KNN Loss: 3.078676700592041 | BCE Loss: 1.0020592212677002\n",
      "Epoch 232 / 500 | iteration 5 / 30 | Total Loss: 4.085422039031982 | KNN Loss: 3.060032844543457 | BCE Loss: 1.0253891944885254\n",
      "Epoch 232 / 500 | iteration 10 / 30 | Total Loss: 4.029537677764893 | KNN Loss: 3.0346999168395996 | BCE Loss: 0.9948378801345825\n",
      "Epoch 232 / 500 | iteration 15 / 30 | Total Loss: 4.105588912963867 | KNN Loss: 3.075383424758911 | BCE Loss: 1.030205488204956\n",
      "Epoch 232 / 500 | iteration 20 / 30 | Total Loss: 4.052006721496582 | KNN Loss: 3.045494556427002 | BCE Loss: 1.0065122842788696\n",
      "Epoch 232 / 500 | iteration 25 / 30 | Total Loss: 4.073412895202637 | KNN Loss: 3.0765442848205566 | BCE Loss: 0.9968685507774353\n",
      "Epoch 233 / 500 | iteration 0 / 30 | Total Loss: 4.111021518707275 | KNN Loss: 3.0866169929504395 | BCE Loss: 1.0244046449661255\n",
      "Epoch 233 / 500 | iteration 5 / 30 | Total Loss: 4.081742286682129 | KNN Loss: 3.0716588497161865 | BCE Loss: 1.0100834369659424\n",
      "Epoch 233 / 500 | iteration 10 / 30 | Total Loss: 4.098329544067383 | KNN Loss: 3.0525310039520264 | BCE Loss: 1.045798659324646\n",
      "Epoch 233 / 500 | iteration 15 / 30 | Total Loss: 4.088866233825684 | KNN Loss: 3.0373220443725586 | BCE Loss: 1.051544427871704\n",
      "Epoch 233 / 500 | iteration 20 / 30 | Total Loss: 4.084451675415039 | KNN Loss: 3.062952995300293 | BCE Loss: 1.021498680114746\n",
      "Epoch 233 / 500 | iteration 25 / 30 | Total Loss: 4.107250213623047 | KNN Loss: 3.0895090103149414 | BCE Loss: 1.0177409648895264\n",
      "Epoch 234 / 500 | iteration 0 / 30 | Total Loss: 4.1516242027282715 | KNN Loss: 3.1215617656707764 | BCE Loss: 1.0300625562667847\n",
      "Epoch 234 / 500 | iteration 5 / 30 | Total Loss: 4.087097644805908 | KNN Loss: 3.0541982650756836 | BCE Loss: 1.0328993797302246\n",
      "Epoch 234 / 500 | iteration 10 / 30 | Total Loss: 4.061530113220215 | KNN Loss: 3.0604360103607178 | BCE Loss: 1.0010943412780762\n",
      "Epoch 234 / 500 | iteration 15 / 30 | Total Loss: 4.0876007080078125 | KNN Loss: 3.0717782974243164 | BCE Loss: 1.0158222913742065\n",
      "Epoch 234 / 500 | iteration 20 / 30 | Total Loss: 4.093754291534424 | KNN Loss: 3.07649564743042 | BCE Loss: 1.0172585248947144\n",
      "Epoch 234 / 500 | iteration 25 / 30 | Total Loss: 4.0989532470703125 | KNN Loss: 3.0692451000213623 | BCE Loss: 1.0297083854675293\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 235 / 500 | iteration 0 / 30 | Total Loss: 4.06601619720459 | KNN Loss: 3.0709445476531982 | BCE Loss: 0.9950718879699707\n",
      "Epoch 235 / 500 | iteration 5 / 30 | Total Loss: 4.1497578620910645 | KNN Loss: 3.096395969390869 | BCE Loss: 1.0533617734909058\n",
      "Epoch 235 / 500 | iteration 10 / 30 | Total Loss: 4.094093322753906 | KNN Loss: 3.077775001525879 | BCE Loss: 1.0163183212280273\n",
      "Epoch 235 / 500 | iteration 15 / 30 | Total Loss: 4.051914215087891 | KNN Loss: 3.075132131576538 | BCE Loss: 0.9767818450927734\n",
      "Epoch 235 / 500 | iteration 20 / 30 | Total Loss: 4.080904006958008 | KNN Loss: 3.0622057914733887 | BCE Loss: 1.01869797706604\n",
      "Epoch 235 / 500 | iteration 25 / 30 | Total Loss: 4.054556846618652 | KNN Loss: 3.0456247329711914 | BCE Loss: 1.00893235206604\n",
      "Epoch 236 / 500 | iteration 0 / 30 | Total Loss: 4.122065544128418 | KNN Loss: 3.085259437561035 | BCE Loss: 1.0368061065673828\n",
      "Epoch 236 / 500 | iteration 5 / 30 | Total Loss: 4.1009674072265625 | KNN Loss: 3.092278003692627 | BCE Loss: 1.0086894035339355\n",
      "Epoch 236 / 500 | iteration 10 / 30 | Total Loss: 4.1141357421875 | KNN Loss: 3.084686756134033 | BCE Loss: 1.029449224472046\n",
      "Epoch 236 / 500 | iteration 15 / 30 | Total Loss: 4.107489109039307 | KNN Loss: 3.0816707611083984 | BCE Loss: 1.0258183479309082\n",
      "Epoch 236 / 500 | iteration 20 / 30 | Total Loss: 4.05775260925293 | KNN Loss: 3.0480706691741943 | BCE Loss: 1.0096819400787354\n",
      "Epoch 236 / 500 | iteration 25 / 30 | Total Loss: 4.088966369628906 | KNN Loss: 3.0722076892852783 | BCE Loss: 1.0167585611343384\n",
      "Epoch   237: reducing learning rate of group 0 to 9.8866e-05.\n",
      "Epoch 237 / 500 | iteration 0 / 30 | Total Loss: 4.083921432495117 | KNN Loss: 3.040694236755371 | BCE Loss: 1.0432270765304565\n",
      "Epoch 237 / 500 | iteration 5 / 30 | Total Loss: 4.041384696960449 | KNN Loss: 3.026456117630005 | BCE Loss: 1.0149283409118652\n",
      "Epoch 237 / 500 | iteration 10 / 30 | Total Loss: 4.06822395324707 | KNN Loss: 3.0498206615448 | BCE Loss: 1.0184035301208496\n",
      "Epoch 237 / 500 | iteration 15 / 30 | Total Loss: 4.120007514953613 | KNN Loss: 3.082794427871704 | BCE Loss: 1.0372133255004883\n",
      "Epoch 237 / 500 | iteration 20 / 30 | Total Loss: 4.071528434753418 | KNN Loss: 3.0666558742523193 | BCE Loss: 1.0048725605010986\n",
      "Epoch 237 / 500 | iteration 25 / 30 | Total Loss: 4.077392101287842 | KNN Loss: 3.0655813217163086 | BCE Loss: 1.0118107795715332\n",
      "Epoch 238 / 500 | iteration 0 / 30 | Total Loss: 4.081754684448242 | KNN Loss: 3.0558907985687256 | BCE Loss: 1.0258640050888062\n",
      "Epoch 238 / 500 | iteration 5 / 30 | Total Loss: 4.0907487869262695 | KNN Loss: 3.0771470069885254 | BCE Loss: 1.0136020183563232\n",
      "Epoch 238 / 500 | iteration 10 / 30 | Total Loss: 4.109982013702393 | KNN Loss: 3.0783119201660156 | BCE Loss: 1.031670093536377\n",
      "Epoch 238 / 500 | iteration 15 / 30 | Total Loss: 4.081833839416504 | KNN Loss: 3.0668270587921143 | BCE Loss: 1.0150070190429688\n",
      "Epoch 238 / 500 | iteration 20 / 30 | Total Loss: 4.116641998291016 | KNN Loss: 3.0990853309631348 | BCE Loss: 1.0175564289093018\n",
      "Epoch 238 / 500 | iteration 25 / 30 | Total Loss: 4.081076145172119 | KNN Loss: 3.0397591590881348 | BCE Loss: 1.0413169860839844\n",
      "Epoch 239 / 500 | iteration 0 / 30 | Total Loss: 4.1188154220581055 | KNN Loss: 3.049473524093628 | BCE Loss: 1.0693416595458984\n",
      "Epoch 239 / 500 | iteration 5 / 30 | Total Loss: 4.05106782913208 | KNN Loss: 3.0420258045196533 | BCE Loss: 1.0090420246124268\n",
      "Epoch 239 / 500 | iteration 10 / 30 | Total Loss: 4.0708112716674805 | KNN Loss: 3.0704925060272217 | BCE Loss: 1.0003187656402588\n",
      "Epoch 239 / 500 | iteration 15 / 30 | Total Loss: 4.079158782958984 | KNN Loss: 3.0533769130706787 | BCE Loss: 1.0257818698883057\n",
      "Epoch 239 / 500 | iteration 20 / 30 | Total Loss: 4.087559700012207 | KNN Loss: 3.041513204574585 | BCE Loss: 1.046046495437622\n",
      "Epoch 239 / 500 | iteration 25 / 30 | Total Loss: 4.101238250732422 | KNN Loss: 3.0730888843536377 | BCE Loss: 1.0281493663787842\n",
      "Epoch 240 / 500 | iteration 0 / 30 | Total Loss: 4.109850883483887 | KNN Loss: 3.0907554626464844 | BCE Loss: 1.0190951824188232\n",
      "Epoch 240 / 500 | iteration 5 / 30 | Total Loss: 4.070413589477539 | KNN Loss: 3.060370683670044 | BCE Loss: 1.010042667388916\n",
      "Epoch 240 / 500 | iteration 10 / 30 | Total Loss: 4.061223030090332 | KNN Loss: 3.0298924446105957 | BCE Loss: 1.0313305854797363\n",
      "Epoch 240 / 500 | iteration 15 / 30 | Total Loss: 4.078247547149658 | KNN Loss: 3.1009562015533447 | BCE Loss: 0.9772911667823792\n",
      "Epoch 240 / 500 | iteration 20 / 30 | Total Loss: 4.141375541687012 | KNN Loss: 3.103536605834961 | BCE Loss: 1.0378389358520508\n",
      "Epoch 240 / 500 | iteration 25 / 30 | Total Loss: 4.140942573547363 | KNN Loss: 3.10776424407959 | BCE Loss: 1.0331785678863525\n",
      "Epoch 241 / 500 | iteration 0 / 30 | Total Loss: 4.064127445220947 | KNN Loss: 3.043919324874878 | BCE Loss: 1.0202081203460693\n",
      "Epoch 241 / 500 | iteration 5 / 30 | Total Loss: 4.063648223876953 | KNN Loss: 3.0601203441619873 | BCE Loss: 1.0035277605056763\n",
      "Epoch 241 / 500 | iteration 10 / 30 | Total Loss: 4.109000205993652 | KNN Loss: 3.0632989406585693 | BCE Loss: 1.045701503753662\n",
      "Epoch 241 / 500 | iteration 15 / 30 | Total Loss: 4.108136177062988 | KNN Loss: 3.0602688789367676 | BCE Loss: 1.0478672981262207\n",
      "Epoch 241 / 500 | iteration 20 / 30 | Total Loss: 4.119962692260742 | KNN Loss: 3.0839486122131348 | BCE Loss: 1.0360140800476074\n",
      "Epoch 241 / 500 | iteration 25 / 30 | Total Loss: 4.033467769622803 | KNN Loss: 3.0375936031341553 | BCE Loss: 0.995874285697937\n",
      "Epoch 242 / 500 | iteration 0 / 30 | Total Loss: 4.098998069763184 | KNN Loss: 3.054927110671997 | BCE Loss: 1.044070839881897\n",
      "Epoch 242 / 500 | iteration 5 / 30 | Total Loss: 4.1036763191223145 | KNN Loss: 3.0862574577331543 | BCE Loss: 1.0174188613891602\n",
      "Epoch 242 / 500 | iteration 10 / 30 | Total Loss: 4.119776725769043 | KNN Loss: 3.0896265506744385 | BCE Loss: 1.0301501750946045\n",
      "Epoch 242 / 500 | iteration 15 / 30 | Total Loss: 4.086667060852051 | KNN Loss: 3.0835649967193604 | BCE Loss: 1.0031018257141113\n",
      "Epoch 242 / 500 | iteration 20 / 30 | Total Loss: 4.078229904174805 | KNN Loss: 3.0571725368499756 | BCE Loss: 1.02105712890625\n",
      "Epoch 242 / 500 | iteration 25 / 30 | Total Loss: 4.094697952270508 | KNN Loss: 3.1001088619232178 | BCE Loss: 0.9945889115333557\n",
      "Epoch 243 / 500 | iteration 0 / 30 | Total Loss: 4.095213413238525 | KNN Loss: 3.072627544403076 | BCE Loss: 1.0225857496261597\n",
      "Epoch 243 / 500 | iteration 5 / 30 | Total Loss: 4.143520832061768 | KNN Loss: 3.1064136028289795 | BCE Loss: 1.037107229232788\n",
      "Epoch 243 / 500 | iteration 10 / 30 | Total Loss: 4.082487106323242 | KNN Loss: 3.056550979614258 | BCE Loss: 1.0259363651275635\n",
      "Epoch 243 / 500 | iteration 15 / 30 | Total Loss: 4.121604919433594 | KNN Loss: 3.1002109050750732 | BCE Loss: 1.0213937759399414\n",
      "Epoch 243 / 500 | iteration 20 / 30 | Total Loss: 4.098676681518555 | KNN Loss: 3.0554580688476562 | BCE Loss: 1.0432186126708984\n",
      "Epoch 243 / 500 | iteration 25 / 30 | Total Loss: 4.095073699951172 | KNN Loss: 3.0795302391052246 | BCE Loss: 1.0155435800552368\n",
      "Epoch 244 / 500 | iteration 0 / 30 | Total Loss: 4.097301006317139 | KNN Loss: 3.08254075050354 | BCE Loss: 1.0147602558135986\n",
      "Epoch 244 / 500 | iteration 5 / 30 | Total Loss: 4.070959091186523 | KNN Loss: 3.0479743480682373 | BCE Loss: 1.022984504699707\n",
      "Epoch 244 / 500 | iteration 10 / 30 | Total Loss: 4.117026329040527 | KNN Loss: 3.0671801567077637 | BCE Loss: 1.0498462915420532\n",
      "Epoch 244 / 500 | iteration 15 / 30 | Total Loss: 4.099135875701904 | KNN Loss: 3.1015920639038086 | BCE Loss: 0.9975437521934509\n",
      "Epoch 244 / 500 | iteration 20 / 30 | Total Loss: 4.07952356338501 | KNN Loss: 3.063965320587158 | BCE Loss: 1.0155582427978516\n",
      "Epoch 244 / 500 | iteration 25 / 30 | Total Loss: 4.110200881958008 | KNN Loss: 3.0884339809417725 | BCE Loss: 1.0217666625976562\n",
      "Epoch 245 / 500 | iteration 0 / 30 | Total Loss: 4.115281581878662 | KNN Loss: 3.0878286361694336 | BCE Loss: 1.027452826499939\n",
      "Epoch 245 / 500 | iteration 5 / 30 | Total Loss: 4.0810699462890625 | KNN Loss: 3.082733392715454 | BCE Loss: 0.9983367323875427\n",
      "Epoch 245 / 500 | iteration 10 / 30 | Total Loss: 4.068173408508301 | KNN Loss: 3.087669610977173 | BCE Loss: 0.9805039167404175\n",
      "Epoch 245 / 500 | iteration 15 / 30 | Total Loss: 4.090574264526367 | KNN Loss: 3.0679702758789062 | BCE Loss: 1.022603988647461\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 245 / 500 | iteration 20 / 30 | Total Loss: 4.092745304107666 | KNN Loss: 3.0629520416259766 | BCE Loss: 1.029793381690979\n",
      "Epoch 245 / 500 | iteration 25 / 30 | Total Loss: 4.09627103805542 | KNN Loss: 3.0671637058258057 | BCE Loss: 1.0291073322296143\n",
      "Epoch 246 / 500 | iteration 0 / 30 | Total Loss: 4.116108417510986 | KNN Loss: 3.0819621086120605 | BCE Loss: 1.0341463088989258\n",
      "Epoch 246 / 500 | iteration 5 / 30 | Total Loss: 4.1358723640441895 | KNN Loss: 3.088146924972534 | BCE Loss: 1.0477254390716553\n",
      "Epoch 246 / 500 | iteration 10 / 30 | Total Loss: 4.155173301696777 | KNN Loss: 3.119962692260742 | BCE Loss: 1.0352104902267456\n",
      "Epoch 246 / 500 | iteration 15 / 30 | Total Loss: 4.098528861999512 | KNN Loss: 3.052542209625244 | BCE Loss: 1.0459866523742676\n",
      "Epoch 246 / 500 | iteration 20 / 30 | Total Loss: 4.1012115478515625 | KNN Loss: 3.073317289352417 | BCE Loss: 1.0278942584991455\n",
      "Epoch 246 / 500 | iteration 25 / 30 | Total Loss: 4.093395233154297 | KNN Loss: 3.0665283203125 | BCE Loss: 1.0268670320510864\n",
      "Epoch 247 / 500 | iteration 0 / 30 | Total Loss: 4.096114635467529 | KNN Loss: 3.0642263889312744 | BCE Loss: 1.0318882465362549\n",
      "Epoch 247 / 500 | iteration 5 / 30 | Total Loss: 4.095788955688477 | KNN Loss: 3.080893039703369 | BCE Loss: 1.0148961544036865\n",
      "Epoch 247 / 500 | iteration 10 / 30 | Total Loss: 4.100290775299072 | KNN Loss: 3.079573154449463 | BCE Loss: 1.0207176208496094\n",
      "Epoch 247 / 500 | iteration 15 / 30 | Total Loss: 4.075969219207764 | KNN Loss: 3.0782177448272705 | BCE Loss: 0.9977515339851379\n",
      "Epoch 247 / 500 | iteration 20 / 30 | Total Loss: 4.046308994293213 | KNN Loss: 3.0245327949523926 | BCE Loss: 1.0217760801315308\n",
      "Epoch 247 / 500 | iteration 25 / 30 | Total Loss: 4.142025470733643 | KNN Loss: 3.0936708450317383 | BCE Loss: 1.0483546257019043\n",
      "Epoch   248: reducing learning rate of group 0 to 6.9206e-05.\n",
      "Epoch 248 / 500 | iteration 0 / 30 | Total Loss: 4.055501461029053 | KNN Loss: 3.0603456497192383 | BCE Loss: 0.9951558113098145\n",
      "Epoch 248 / 500 | iteration 5 / 30 | Total Loss: 4.126931190490723 | KNN Loss: 3.0785510540008545 | BCE Loss: 1.048379898071289\n",
      "Epoch 248 / 500 | iteration 10 / 30 | Total Loss: 4.053036689758301 | KNN Loss: 3.038325786590576 | BCE Loss: 1.0147106647491455\n",
      "Epoch 248 / 500 | iteration 15 / 30 | Total Loss: 4.076909065246582 | KNN Loss: 3.0604898929595947 | BCE Loss: 1.0164189338684082\n",
      "Epoch 248 / 500 | iteration 20 / 30 | Total Loss: 4.111793041229248 | KNN Loss: 3.093480110168457 | BCE Loss: 1.0183128118515015\n",
      "Epoch 248 / 500 | iteration 25 / 30 | Total Loss: 4.123244285583496 | KNN Loss: 3.088827610015869 | BCE Loss: 1.0344164371490479\n",
      "Epoch 249 / 500 | iteration 0 / 30 | Total Loss: 4.121461868286133 | KNN Loss: 3.084095001220703 | BCE Loss: 1.0373669862747192\n",
      "Epoch 249 / 500 | iteration 5 / 30 | Total Loss: 4.046197414398193 | KNN Loss: 3.0347063541412354 | BCE Loss: 1.011491060256958\n",
      "Epoch 249 / 500 | iteration 10 / 30 | Total Loss: 4.096707820892334 | KNN Loss: 3.0851333141326904 | BCE Loss: 1.0115745067596436\n",
      "Epoch 249 / 500 | iteration 15 / 30 | Total Loss: 4.087115287780762 | KNN Loss: 3.077929735183716 | BCE Loss: 1.0091854333877563\n",
      "Epoch 249 / 500 | iteration 20 / 30 | Total Loss: 4.06980037689209 | KNN Loss: 3.0536129474639893 | BCE Loss: 1.0161876678466797\n",
      "Epoch 249 / 500 | iteration 25 / 30 | Total Loss: 4.109486103057861 | KNN Loss: 3.101551055908203 | BCE Loss: 1.0079350471496582\n",
      "Epoch 250 / 500 | iteration 0 / 30 | Total Loss: 4.053722381591797 | KNN Loss: 3.0678915977478027 | BCE Loss: 0.9858306050300598\n",
      "Epoch 250 / 500 | iteration 5 / 30 | Total Loss: 4.0966386795043945 | KNN Loss: 3.0688388347625732 | BCE Loss: 1.0277997255325317\n",
      "Epoch 250 / 500 | iteration 10 / 30 | Total Loss: 4.073045253753662 | KNN Loss: 3.0801665782928467 | BCE Loss: 0.9928785562515259\n",
      "Epoch 250 / 500 | iteration 15 / 30 | Total Loss: 4.066269874572754 | KNN Loss: 3.045492172241211 | BCE Loss: 1.020777940750122\n",
      "Epoch 250 / 500 | iteration 20 / 30 | Total Loss: 4.074702262878418 | KNN Loss: 3.0557074546813965 | BCE Loss: 1.0189945697784424\n",
      "Epoch 250 / 500 | iteration 25 / 30 | Total Loss: 4.067824840545654 | KNN Loss: 3.057325601577759 | BCE Loss: 1.0104992389678955\n",
      "Epoch 251 / 500 | iteration 0 / 30 | Total Loss: 4.063549518585205 | KNN Loss: 3.0595526695251465 | BCE Loss: 1.0039968490600586\n",
      "Epoch 251 / 500 | iteration 5 / 30 | Total Loss: 4.076874732971191 | KNN Loss: 3.0497372150421143 | BCE Loss: 1.0271377563476562\n",
      "Epoch 251 / 500 | iteration 10 / 30 | Total Loss: 4.094304084777832 | KNN Loss: 3.0543510913848877 | BCE Loss: 1.0399532318115234\n",
      "Epoch 251 / 500 | iteration 15 / 30 | Total Loss: 4.056209087371826 | KNN Loss: 3.04447865486145 | BCE Loss: 1.011730432510376\n",
      "Epoch 251 / 500 | iteration 20 / 30 | Total Loss: 4.11258602142334 | KNN Loss: 3.0755534172058105 | BCE Loss: 1.0370324850082397\n",
      "Epoch 251 / 500 | iteration 25 / 30 | Total Loss: 4.094759941101074 | KNN Loss: 3.057098627090454 | BCE Loss: 1.0376615524291992\n",
      "Epoch 252 / 500 | iteration 0 / 30 | Total Loss: 4.071556091308594 | KNN Loss: 3.0563437938690186 | BCE Loss: 1.0152124166488647\n",
      "Epoch 252 / 500 | iteration 5 / 30 | Total Loss: 4.10772705078125 | KNN Loss: 3.0882925987243652 | BCE Loss: 1.0194346904754639\n",
      "Epoch 252 / 500 | iteration 10 / 30 | Total Loss: 4.0930657386779785 | KNN Loss: 3.045877456665039 | BCE Loss: 1.0471882820129395\n",
      "Epoch 252 / 500 | iteration 15 / 30 | Total Loss: 4.098050117492676 | KNN Loss: 3.0931243896484375 | BCE Loss: 1.0049254894256592\n",
      "Epoch 252 / 500 | iteration 20 / 30 | Total Loss: 4.1252121925354 | KNN Loss: 3.084575891494751 | BCE Loss: 1.0406363010406494\n",
      "Epoch 252 / 500 | iteration 25 / 30 | Total Loss: 4.12077522277832 | KNN Loss: 3.0932116508483887 | BCE Loss: 1.0275636911392212\n",
      "Epoch 253 / 500 | iteration 0 / 30 | Total Loss: 4.139830112457275 | KNN Loss: 3.0986685752868652 | BCE Loss: 1.0411615371704102\n",
      "Epoch 253 / 500 | iteration 5 / 30 | Total Loss: 4.070967674255371 | KNN Loss: 3.055628538131714 | BCE Loss: 1.0153390169143677\n",
      "Epoch 253 / 500 | iteration 10 / 30 | Total Loss: 4.056658744812012 | KNN Loss: 3.051954507827759 | BCE Loss: 1.0047041177749634\n",
      "Epoch 253 / 500 | iteration 15 / 30 | Total Loss: 4.062960147857666 | KNN Loss: 3.0402650833129883 | BCE Loss: 1.0226950645446777\n",
      "Epoch 253 / 500 | iteration 20 / 30 | Total Loss: 4.092476844787598 | KNN Loss: 3.0556933879852295 | BCE Loss: 1.036783218383789\n",
      "Epoch 253 / 500 | iteration 25 / 30 | Total Loss: 4.085916519165039 | KNN Loss: 3.054262399673462 | BCE Loss: 1.0316541194915771\n",
      "Epoch 254 / 500 | iteration 0 / 30 | Total Loss: 4.0259270668029785 | KNN Loss: 3.0354409217834473 | BCE Loss: 0.9904860854148865\n",
      "Epoch 254 / 500 | iteration 5 / 30 | Total Loss: 4.047102928161621 | KNN Loss: 3.048788070678711 | BCE Loss: 0.9983150959014893\n",
      "Epoch 254 / 500 | iteration 10 / 30 | Total Loss: 4.116059303283691 | KNN Loss: 3.0867958068847656 | BCE Loss: 1.0292637348175049\n",
      "Epoch 254 / 500 | iteration 15 / 30 | Total Loss: 4.050445079803467 | KNN Loss: 3.024268388748169 | BCE Loss: 1.0261766910552979\n",
      "Epoch 254 / 500 | iteration 20 / 30 | Total Loss: 4.0477094650268555 | KNN Loss: 3.0297200679779053 | BCE Loss: 1.0179892778396606\n",
      "Epoch 254 / 500 | iteration 25 / 30 | Total Loss: 4.0804524421691895 | KNN Loss: 3.071211099624634 | BCE Loss: 1.0092412233352661\n",
      "Epoch 255 / 500 | iteration 0 / 30 | Total Loss: 4.151267051696777 | KNN Loss: 3.0944440364837646 | BCE Loss: 1.0568230152130127\n",
      "Epoch 255 / 500 | iteration 5 / 30 | Total Loss: 4.075953483581543 | KNN Loss: 3.038450002670288 | BCE Loss: 1.0375033617019653\n",
      "Epoch 255 / 500 | iteration 10 / 30 | Total Loss: 4.078510761260986 | KNN Loss: 3.066706418991089 | BCE Loss: 1.011804223060608\n",
      "Epoch 255 / 500 | iteration 15 / 30 | Total Loss: 4.107132911682129 | KNN Loss: 3.069546937942505 | BCE Loss: 1.0375862121582031\n",
      "Epoch 255 / 500 | iteration 20 / 30 | Total Loss: 4.062440395355225 | KNN Loss: 3.072298526763916 | BCE Loss: 0.9901418089866638\n",
      "Epoch 255 / 500 | iteration 25 / 30 | Total Loss: 4.063567638397217 | KNN Loss: 3.0396339893341064 | BCE Loss: 1.0239336490631104\n",
      "Epoch 256 / 500 | iteration 0 / 30 | Total Loss: 4.066694259643555 | KNN Loss: 3.047429323196411 | BCE Loss: 1.019264817237854\n",
      "Epoch 256 / 500 | iteration 5 / 30 | Total Loss: 4.14853572845459 | KNN Loss: 3.1109957695007324 | BCE Loss: 1.0375401973724365\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 256 / 500 | iteration 10 / 30 | Total Loss: 4.121406078338623 | KNN Loss: 3.079143524169922 | BCE Loss: 1.0422626733779907\n",
      "Epoch 256 / 500 | iteration 15 / 30 | Total Loss: 4.051721096038818 | KNN Loss: 3.049287796020508 | BCE Loss: 1.0024333000183105\n",
      "Epoch 256 / 500 | iteration 20 / 30 | Total Loss: 4.115566253662109 | KNN Loss: 3.0803139209747314 | BCE Loss: 1.0352520942687988\n",
      "Epoch 256 / 500 | iteration 25 / 30 | Total Loss: 4.069867134094238 | KNN Loss: 3.05568265914917 | BCE Loss: 1.0141843557357788\n",
      "Epoch 257 / 500 | iteration 0 / 30 | Total Loss: 4.062214374542236 | KNN Loss: 3.0572218894958496 | BCE Loss: 1.0049926042556763\n",
      "Epoch 257 / 500 | iteration 5 / 30 | Total Loss: 4.060173511505127 | KNN Loss: 3.048293352127075 | BCE Loss: 1.0118802785873413\n",
      "Epoch 257 / 500 | iteration 10 / 30 | Total Loss: 4.121858596801758 | KNN Loss: 3.095674514770508 | BCE Loss: 1.0261842012405396\n",
      "Epoch 257 / 500 | iteration 15 / 30 | Total Loss: 4.082003116607666 | KNN Loss: 3.0496068000793457 | BCE Loss: 1.0323961973190308\n",
      "Epoch 257 / 500 | iteration 20 / 30 | Total Loss: 4.075748920440674 | KNN Loss: 3.0945632457733154 | BCE Loss: 0.9811855554580688\n",
      "Epoch 257 / 500 | iteration 25 / 30 | Total Loss: 4.039173126220703 | KNN Loss: 3.042348623275757 | BCE Loss: 0.9968247413635254\n",
      "Epoch 258 / 500 | iteration 0 / 30 | Total Loss: 4.132943153381348 | KNN Loss: 3.0947744846343994 | BCE Loss: 1.0381684303283691\n",
      "Epoch 258 / 500 | iteration 5 / 30 | Total Loss: 4.080010890960693 | KNN Loss: 3.0663139820098877 | BCE Loss: 1.0136970281600952\n",
      "Epoch 258 / 500 | iteration 10 / 30 | Total Loss: 4.04155969619751 | KNN Loss: 3.037522077560425 | BCE Loss: 1.004037618637085\n",
      "Epoch 258 / 500 | iteration 15 / 30 | Total Loss: 4.075560569763184 | KNN Loss: 3.0539422035217285 | BCE Loss: 1.021618366241455\n",
      "Epoch 258 / 500 | iteration 20 / 30 | Total Loss: 4.064874649047852 | KNN Loss: 3.0503268241882324 | BCE Loss: 1.01454758644104\n",
      "Epoch 258 / 500 | iteration 25 / 30 | Total Loss: 4.088730812072754 | KNN Loss: 3.0622897148132324 | BCE Loss: 1.026440978050232\n",
      "Epoch   259: reducing learning rate of group 0 to 4.8445e-05.\n",
      "Epoch 259 / 500 | iteration 0 / 30 | Total Loss: 4.064116477966309 | KNN Loss: 3.0646681785583496 | BCE Loss: 0.9994481801986694\n",
      "Epoch 259 / 500 | iteration 5 / 30 | Total Loss: 4.042247295379639 | KNN Loss: 3.031195878982544 | BCE Loss: 1.0110514163970947\n",
      "Epoch 259 / 500 | iteration 10 / 30 | Total Loss: 4.034677505493164 | KNN Loss: 3.025604486465454 | BCE Loss: 1.0090731382369995\n",
      "Epoch 259 / 500 | iteration 15 / 30 | Total Loss: 4.0984649658203125 | KNN Loss: 3.055521011352539 | BCE Loss: 1.0429437160491943\n",
      "Epoch 259 / 500 | iteration 20 / 30 | Total Loss: 4.072355270385742 | KNN Loss: 3.0494604110717773 | BCE Loss: 1.0228947401046753\n",
      "Epoch 259 / 500 | iteration 25 / 30 | Total Loss: 4.136810779571533 | KNN Loss: 3.1010310649871826 | BCE Loss: 1.035779595375061\n",
      "Epoch 260 / 500 | iteration 0 / 30 | Total Loss: 4.133612632751465 | KNN Loss: 3.090888500213623 | BCE Loss: 1.0427240133285522\n",
      "Epoch 260 / 500 | iteration 5 / 30 | Total Loss: 4.080859184265137 | KNN Loss: 3.082401752471924 | BCE Loss: 0.9984574317932129\n",
      "Epoch 260 / 500 | iteration 10 / 30 | Total Loss: 4.049571990966797 | KNN Loss: 3.0362002849578857 | BCE Loss: 1.0133719444274902\n",
      "Epoch 260 / 500 | iteration 15 / 30 | Total Loss: 4.071539402008057 | KNN Loss: 3.069269895553589 | BCE Loss: 1.0022696256637573\n",
      "Epoch 260 / 500 | iteration 20 / 30 | Total Loss: 4.09664249420166 | KNN Loss: 3.071132183074951 | BCE Loss: 1.025510311126709\n",
      "Epoch 260 / 500 | iteration 25 / 30 | Total Loss: 4.068623065948486 | KNN Loss: 3.0458366870880127 | BCE Loss: 1.0227863788604736\n",
      "Epoch 261 / 500 | iteration 0 / 30 | Total Loss: 4.09512996673584 | KNN Loss: 3.07474946975708 | BCE Loss: 1.0203807353973389\n",
      "Epoch 261 / 500 | iteration 5 / 30 | Total Loss: 4.092100143432617 | KNN Loss: 3.091054677963257 | BCE Loss: 1.0010453462600708\n",
      "Epoch 261 / 500 | iteration 10 / 30 | Total Loss: 4.120777606964111 | KNN Loss: 3.1161539554595947 | BCE Loss: 1.0046237707138062\n",
      "Epoch 261 / 500 | iteration 15 / 30 | Total Loss: 4.07789421081543 | KNN Loss: 3.057487964630127 | BCE Loss: 1.0204062461853027\n",
      "Epoch 261 / 500 | iteration 20 / 30 | Total Loss: 4.029512882232666 | KNN Loss: 3.0278313159942627 | BCE Loss: 1.0016816854476929\n",
      "Epoch 261 / 500 | iteration 25 / 30 | Total Loss: 4.1396708488464355 | KNN Loss: 3.101003885269165 | BCE Loss: 1.0386669635772705\n",
      "Epoch 262 / 500 | iteration 0 / 30 | Total Loss: 4.085834503173828 | KNN Loss: 3.0939884185791016 | BCE Loss: 0.9918460249900818\n",
      "Epoch 262 / 500 | iteration 5 / 30 | Total Loss: 4.096227645874023 | KNN Loss: 3.055455207824707 | BCE Loss: 1.0407723188400269\n",
      "Epoch 262 / 500 | iteration 10 / 30 | Total Loss: 4.0638837814331055 | KNN Loss: 3.0496461391448975 | BCE Loss: 1.014237642288208\n",
      "Epoch 262 / 500 | iteration 15 / 30 | Total Loss: 4.1415019035339355 | KNN Loss: 3.102642297744751 | BCE Loss: 1.0388596057891846\n",
      "Epoch 262 / 500 | iteration 20 / 30 | Total Loss: 4.082233428955078 | KNN Loss: 3.0715813636779785 | BCE Loss: 1.01065194606781\n",
      "Epoch 262 / 500 | iteration 25 / 30 | Total Loss: 4.1069746017456055 | KNN Loss: 3.1006762981414795 | BCE Loss: 1.006298542022705\n",
      "Epoch 263 / 500 | iteration 0 / 30 | Total Loss: 4.134839057922363 | KNN Loss: 3.078322410583496 | BCE Loss: 1.0565165281295776\n",
      "Epoch 263 / 500 | iteration 5 / 30 | Total Loss: 4.074434757232666 | KNN Loss: 3.0620408058166504 | BCE Loss: 1.0123940706253052\n",
      "Epoch 263 / 500 | iteration 10 / 30 | Total Loss: 4.123110771179199 | KNN Loss: 3.101297616958618 | BCE Loss: 1.021813154220581\n",
      "Epoch 263 / 500 | iteration 15 / 30 | Total Loss: 4.081521511077881 | KNN Loss: 3.0432770252227783 | BCE Loss: 1.038244605064392\n",
      "Epoch 263 / 500 | iteration 20 / 30 | Total Loss: 4.059099197387695 | KNN Loss: 3.0677707195281982 | BCE Loss: 0.9913284182548523\n",
      "Epoch 263 / 500 | iteration 25 / 30 | Total Loss: 4.165351867675781 | KNN Loss: 3.109590768814087 | BCE Loss: 1.0557612180709839\n",
      "Epoch 264 / 500 | iteration 0 / 30 | Total Loss: 4.039881229400635 | KNN Loss: 3.0403871536254883 | BCE Loss: 0.9994940161705017\n",
      "Epoch 264 / 500 | iteration 5 / 30 | Total Loss: 4.043798923492432 | KNN Loss: 3.052320957183838 | BCE Loss: 0.9914779663085938\n",
      "Epoch 264 / 500 | iteration 10 / 30 | Total Loss: 4.063980579376221 | KNN Loss: 3.033952474594116 | BCE Loss: 1.0300281047821045\n",
      "Epoch 264 / 500 | iteration 15 / 30 | Total Loss: 4.113379955291748 | KNN Loss: 3.090715169906616 | BCE Loss: 1.0226649045944214\n",
      "Epoch 264 / 500 | iteration 20 / 30 | Total Loss: 4.105769157409668 | KNN Loss: 3.086785316467285 | BCE Loss: 1.018984079360962\n",
      "Epoch 264 / 500 | iteration 25 / 30 | Total Loss: 4.050379753112793 | KNN Loss: 3.0479867458343506 | BCE Loss: 1.002393126487732\n",
      "Epoch 265 / 500 | iteration 0 / 30 | Total Loss: 4.110325336456299 | KNN Loss: 3.0877342224121094 | BCE Loss: 1.0225911140441895\n",
      "Epoch 265 / 500 | iteration 5 / 30 | Total Loss: 4.117424488067627 | KNN Loss: 3.1023294925689697 | BCE Loss: 1.0150949954986572\n",
      "Epoch 265 / 500 | iteration 10 / 30 | Total Loss: 4.054574012756348 | KNN Loss: 3.02290415763855 | BCE Loss: 1.031670093536377\n",
      "Epoch 265 / 500 | iteration 15 / 30 | Total Loss: 4.100649833679199 | KNN Loss: 3.077582597732544 | BCE Loss: 1.0230673551559448\n",
      "Epoch 265 / 500 | iteration 20 / 30 | Total Loss: 4.075063705444336 | KNN Loss: 3.0728461742401123 | BCE Loss: 1.0022177696228027\n",
      "Epoch 265 / 500 | iteration 25 / 30 | Total Loss: 4.109777927398682 | KNN Loss: 3.108346700668335 | BCE Loss: 1.0014313459396362\n",
      "Epoch 266 / 500 | iteration 0 / 30 | Total Loss: 4.102872848510742 | KNN Loss: 3.0766499042510986 | BCE Loss: 1.0262227058410645\n",
      "Epoch 266 / 500 | iteration 5 / 30 | Total Loss: 4.097291946411133 | KNN Loss: 3.063159227371216 | BCE Loss: 1.034132957458496\n",
      "Epoch 266 / 500 | iteration 10 / 30 | Total Loss: 4.110008239746094 | KNN Loss: 3.078178644180298 | BCE Loss: 1.031829595565796\n",
      "Epoch 266 / 500 | iteration 15 / 30 | Total Loss: 4.047584056854248 | KNN Loss: 3.0385830402374268 | BCE Loss: 1.0090008974075317\n",
      "Epoch 266 / 500 | iteration 20 / 30 | Total Loss: 4.07717227935791 | KNN Loss: 3.070652484893799 | BCE Loss: 1.0065199136734009\n",
      "Epoch 266 / 500 | iteration 25 / 30 | Total Loss: 4.089473247528076 | KNN Loss: 3.056652545928955 | BCE Loss: 1.0328205823898315\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 267 / 500 | iteration 0 / 30 | Total Loss: 4.087404251098633 | KNN Loss: 3.083503484725952 | BCE Loss: 1.0039010047912598\n",
      "Epoch 267 / 500 | iteration 5 / 30 | Total Loss: 4.100742340087891 | KNN Loss: 3.069709062576294 | BCE Loss: 1.0310335159301758\n",
      "Epoch 267 / 500 | iteration 10 / 30 | Total Loss: 4.065103530883789 | KNN Loss: 3.0453507900238037 | BCE Loss: 1.0197527408599854\n",
      "Epoch 267 / 500 | iteration 15 / 30 | Total Loss: 4.119292259216309 | KNN Loss: 3.0666182041168213 | BCE Loss: 1.0526740550994873\n",
      "Epoch 267 / 500 | iteration 20 / 30 | Total Loss: 4.068384170532227 | KNN Loss: 3.054471492767334 | BCE Loss: 1.0139127969741821\n",
      "Epoch 267 / 500 | iteration 25 / 30 | Total Loss: 4.084905624389648 | KNN Loss: 3.0917134284973145 | BCE Loss: 0.9931920766830444\n",
      "Epoch 268 / 500 | iteration 0 / 30 | Total Loss: 4.088055610656738 | KNN Loss: 3.0530197620391846 | BCE Loss: 1.0350360870361328\n",
      "Epoch 268 / 500 | iteration 5 / 30 | Total Loss: 4.071255207061768 | KNN Loss: 3.0426154136657715 | BCE Loss: 1.0286396741867065\n",
      "Epoch 268 / 500 | iteration 10 / 30 | Total Loss: 4.0782470703125 | KNN Loss: 3.0671980381011963 | BCE Loss: 1.0110487937927246\n",
      "Epoch 268 / 500 | iteration 15 / 30 | Total Loss: 4.103592395782471 | KNN Loss: 3.0691404342651367 | BCE Loss: 1.0344520807266235\n",
      "Epoch 268 / 500 | iteration 20 / 30 | Total Loss: 4.0544562339782715 | KNN Loss: 3.0527536869049072 | BCE Loss: 1.0017024278640747\n",
      "Epoch 268 / 500 | iteration 25 / 30 | Total Loss: 4.067363739013672 | KNN Loss: 3.067789077758789 | BCE Loss: 0.9995747804641724\n",
      "Epoch 269 / 500 | iteration 0 / 30 | Total Loss: 4.084847450256348 | KNN Loss: 3.0645453929901123 | BCE Loss: 1.0203022956848145\n",
      "Epoch 269 / 500 | iteration 5 / 30 | Total Loss: 4.101498126983643 | KNN Loss: 3.1001663208007812 | BCE Loss: 1.0013318061828613\n",
      "Epoch 269 / 500 | iteration 10 / 30 | Total Loss: 4.0899810791015625 | KNN Loss: 3.066946506500244 | BCE Loss: 1.0230343341827393\n",
      "Epoch 269 / 500 | iteration 15 / 30 | Total Loss: 4.10837459564209 | KNN Loss: 3.0908613204956055 | BCE Loss: 1.0175132751464844\n",
      "Epoch 269 / 500 | iteration 20 / 30 | Total Loss: 4.103618621826172 | KNN Loss: 3.070009469985962 | BCE Loss: 1.0336090326309204\n",
      "Epoch 269 / 500 | iteration 25 / 30 | Total Loss: 4.0605573654174805 | KNN Loss: 3.0500142574310303 | BCE Loss: 1.0105433464050293\n",
      "Epoch   270: reducing learning rate of group 0 to 3.3911e-05.\n",
      "Epoch 270 / 500 | iteration 0 / 30 | Total Loss: 4.084705829620361 | KNN Loss: 3.0662424564361572 | BCE Loss: 1.018463373184204\n",
      "Epoch 270 / 500 | iteration 5 / 30 | Total Loss: 4.068603992462158 | KNN Loss: 3.0553066730499268 | BCE Loss: 1.0132973194122314\n",
      "Epoch 270 / 500 | iteration 10 / 30 | Total Loss: 4.075386047363281 | KNN Loss: 3.0516841411590576 | BCE Loss: 1.0237020254135132\n",
      "Epoch 270 / 500 | iteration 15 / 30 | Total Loss: 4.09491491317749 | KNN Loss: 3.0662765502929688 | BCE Loss: 1.0286383628845215\n",
      "Epoch 270 / 500 | iteration 20 / 30 | Total Loss: 4.115801811218262 | KNN Loss: 3.0948922634124756 | BCE Loss: 1.020909309387207\n",
      "Epoch 270 / 500 | iteration 25 / 30 | Total Loss: 4.134954452514648 | KNN Loss: 3.0917985439300537 | BCE Loss: 1.0431559085845947\n",
      "Epoch 271 / 500 | iteration 0 / 30 | Total Loss: 4.054067611694336 | KNN Loss: 3.036180257797241 | BCE Loss: 1.0178872346878052\n",
      "Epoch 271 / 500 | iteration 5 / 30 | Total Loss: 4.1119914054870605 | KNN Loss: 3.051460027694702 | BCE Loss: 1.0605313777923584\n",
      "Epoch 271 / 500 | iteration 10 / 30 | Total Loss: 4.077018737792969 | KNN Loss: 3.0511412620544434 | BCE Loss: 1.0258774757385254\n",
      "Epoch 271 / 500 | iteration 15 / 30 | Total Loss: 4.057345390319824 | KNN Loss: 3.0765326023101807 | BCE Loss: 0.9808126091957092\n",
      "Epoch 271 / 500 | iteration 20 / 30 | Total Loss: 4.064095973968506 | KNN Loss: 3.0476083755493164 | BCE Loss: 1.016487717628479\n",
      "Epoch 271 / 500 | iteration 25 / 30 | Total Loss: 4.062042236328125 | KNN Loss: 3.0414655208587646 | BCE Loss: 1.0205764770507812\n",
      "Epoch 272 / 500 | iteration 0 / 30 | Total Loss: 4.083835601806641 | KNN Loss: 3.0806102752685547 | BCE Loss: 1.003225326538086\n",
      "Epoch 272 / 500 | iteration 5 / 30 | Total Loss: 4.047169208526611 | KNN Loss: 3.025844097137451 | BCE Loss: 1.0213249921798706\n",
      "Epoch 272 / 500 | iteration 10 / 30 | Total Loss: 4.061581611633301 | KNN Loss: 3.039214849472046 | BCE Loss: 1.022367000579834\n",
      "Epoch 272 / 500 | iteration 15 / 30 | Total Loss: 4.054536819458008 | KNN Loss: 3.0353775024414062 | BCE Loss: 1.0191595554351807\n",
      "Epoch 272 / 500 | iteration 20 / 30 | Total Loss: 4.056791305541992 | KNN Loss: 3.044797420501709 | BCE Loss: 1.0119941234588623\n",
      "Epoch 272 / 500 | iteration 25 / 30 | Total Loss: 4.068371772766113 | KNN Loss: 3.035144329071045 | BCE Loss: 1.0332274436950684\n",
      "Epoch 273 / 500 | iteration 0 / 30 | Total Loss: 4.068676471710205 | KNN Loss: 3.061014413833618 | BCE Loss: 1.007662057876587\n",
      "Epoch 273 / 500 | iteration 5 / 30 | Total Loss: 4.145708084106445 | KNN Loss: 3.0956645011901855 | BCE Loss: 1.0500438213348389\n",
      "Epoch 273 / 500 | iteration 10 / 30 | Total Loss: 4.1177167892456055 | KNN Loss: 3.083082914352417 | BCE Loss: 1.0346338748931885\n",
      "Epoch 273 / 500 | iteration 15 / 30 | Total Loss: 4.084542274475098 | KNN Loss: 3.0661394596099854 | BCE Loss: 1.0184028148651123\n",
      "Epoch 273 / 500 | iteration 20 / 30 | Total Loss: 4.09175968170166 | KNN Loss: 3.0778658390045166 | BCE Loss: 1.013893723487854\n",
      "Epoch 273 / 500 | iteration 25 / 30 | Total Loss: 4.058681964874268 | KNN Loss: 3.069183826446533 | BCE Loss: 0.9894982576370239\n",
      "Epoch 274 / 500 | iteration 0 / 30 | Total Loss: 4.054896354675293 | KNN Loss: 3.0433349609375 | BCE Loss: 1.011561393737793\n",
      "Epoch 274 / 500 | iteration 5 / 30 | Total Loss: 4.068768501281738 | KNN Loss: 3.0424246788024902 | BCE Loss: 1.0263440608978271\n",
      "Epoch 274 / 500 | iteration 10 / 30 | Total Loss: 4.082583427429199 | KNN Loss: 3.0782508850097656 | BCE Loss: 1.0043327808380127\n",
      "Epoch 274 / 500 | iteration 15 / 30 | Total Loss: 4.063279151916504 | KNN Loss: 3.056291103363037 | BCE Loss: 1.006988286972046\n",
      "Epoch 274 / 500 | iteration 20 / 30 | Total Loss: 4.089954376220703 | KNN Loss: 3.0643436908721924 | BCE Loss: 1.0256108045578003\n",
      "Epoch 274 / 500 | iteration 25 / 30 | Total Loss: 4.15264892578125 | KNN Loss: 3.1293210983276367 | BCE Loss: 1.0233275890350342\n",
      "Epoch 275 / 500 | iteration 0 / 30 | Total Loss: 4.073222637176514 | KNN Loss: 3.0465872287750244 | BCE Loss: 1.0266352891921997\n",
      "Epoch 275 / 500 | iteration 5 / 30 | Total Loss: 4.09689474105835 | KNN Loss: 3.0743815898895264 | BCE Loss: 1.0225132703781128\n",
      "Epoch 275 / 500 | iteration 10 / 30 | Total Loss: 4.053919315338135 | KNN Loss: 3.0572211742401123 | BCE Loss: 0.996698260307312\n",
      "Epoch 275 / 500 | iteration 15 / 30 | Total Loss: 4.074793338775635 | KNN Loss: 3.074453115463257 | BCE Loss: 1.000340223312378\n",
      "Epoch 275 / 500 | iteration 20 / 30 | Total Loss: 4.100556373596191 | KNN Loss: 3.090409994125366 | BCE Loss: 1.010146141052246\n",
      "Epoch 275 / 500 | iteration 25 / 30 | Total Loss: 4.053840637207031 | KNN Loss: 3.0488314628601074 | BCE Loss: 1.005009412765503\n",
      "Epoch 276 / 500 | iteration 0 / 30 | Total Loss: 4.0662736892700195 | KNN Loss: 3.0545878410339355 | BCE Loss: 1.0116856098175049\n",
      "Epoch 276 / 500 | iteration 5 / 30 | Total Loss: 4.119877815246582 | KNN Loss: 3.0984601974487305 | BCE Loss: 1.0214173793792725\n",
      "Epoch 276 / 500 | iteration 10 / 30 | Total Loss: 4.138260364532471 | KNN Loss: 3.1116020679473877 | BCE Loss: 1.026658296585083\n",
      "Epoch 276 / 500 | iteration 15 / 30 | Total Loss: 4.118244171142578 | KNN Loss: 3.071909189224243 | BCE Loss: 1.0463348627090454\n",
      "Epoch 276 / 500 | iteration 20 / 30 | Total Loss: 4.086801528930664 | KNN Loss: 3.0941574573516846 | BCE Loss: 0.9926440715789795\n",
      "Epoch 276 / 500 | iteration 25 / 30 | Total Loss: 4.085901260375977 | KNN Loss: 3.0580215454101562 | BCE Loss: 1.0278794765472412\n",
      "Epoch 277 / 500 | iteration 0 / 30 | Total Loss: 4.037677764892578 | KNN Loss: 3.070190668106079 | BCE Loss: 0.9674869179725647\n",
      "Epoch 277 / 500 | iteration 5 / 30 | Total Loss: 4.075511932373047 | KNN Loss: 3.085731029510498 | BCE Loss: 0.9897807836532593\n",
      "Epoch 277 / 500 | iteration 10 / 30 | Total Loss: 4.015689849853516 | KNN Loss: 3.0247132778167725 | BCE Loss: 0.9909764528274536\n",
      "Epoch 277 / 500 | iteration 15 / 30 | Total Loss: 4.104700088500977 | KNN Loss: 3.068873167037964 | BCE Loss: 1.0358266830444336\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 277 / 500 | iteration 20 / 30 | Total Loss: 4.075835704803467 | KNN Loss: 3.0584115982055664 | BCE Loss: 1.0174241065979004\n",
      "Epoch 277 / 500 | iteration 25 / 30 | Total Loss: 4.054621696472168 | KNN Loss: 3.030271291732788 | BCE Loss: 1.024350643157959\n",
      "Epoch 278 / 500 | iteration 0 / 30 | Total Loss: 4.063046455383301 | KNN Loss: 3.0533480644226074 | BCE Loss: 1.009698510169983\n",
      "Epoch 278 / 500 | iteration 5 / 30 | Total Loss: 4.140239238739014 | KNN Loss: 3.120058059692383 | BCE Loss: 1.0201812982559204\n",
      "Epoch 278 / 500 | iteration 10 / 30 | Total Loss: 4.077526092529297 | KNN Loss: 3.06807541847229 | BCE Loss: 1.009450912475586\n",
      "Epoch 278 / 500 | iteration 15 / 30 | Total Loss: 4.066543102264404 | KNN Loss: 3.0354506969451904 | BCE Loss: 1.0310924053192139\n",
      "Epoch 278 / 500 | iteration 20 / 30 | Total Loss: 4.089733123779297 | KNN Loss: 3.051955461502075 | BCE Loss: 1.0377776622772217\n",
      "Epoch 278 / 500 | iteration 25 / 30 | Total Loss: 4.086796760559082 | KNN Loss: 3.0740296840667725 | BCE Loss: 1.0127670764923096\n",
      "Epoch 279 / 500 | iteration 0 / 30 | Total Loss: 4.064083576202393 | KNN Loss: 3.029770851135254 | BCE Loss: 1.0343128442764282\n",
      "Epoch 279 / 500 | iteration 5 / 30 | Total Loss: 4.092543125152588 | KNN Loss: 3.0830843448638916 | BCE Loss: 1.0094586610794067\n",
      "Epoch 279 / 500 | iteration 10 / 30 | Total Loss: 4.095157623291016 | KNN Loss: 3.090367317199707 | BCE Loss: 1.0047900676727295\n",
      "Epoch 279 / 500 | iteration 15 / 30 | Total Loss: 4.09765625 | KNN Loss: 3.053177833557129 | BCE Loss: 1.044478416442871\n",
      "Epoch 279 / 500 | iteration 20 / 30 | Total Loss: 4.050261497497559 | KNN Loss: 3.068995237350464 | BCE Loss: 0.981266438961029\n",
      "Epoch 279 / 500 | iteration 25 / 30 | Total Loss: 4.122147083282471 | KNN Loss: 3.1002187728881836 | BCE Loss: 1.021928310394287\n",
      "Epoch 280 / 500 | iteration 0 / 30 | Total Loss: 4.118457317352295 | KNN Loss: 3.0745842456817627 | BCE Loss: 1.0438729524612427\n",
      "Epoch 280 / 500 | iteration 5 / 30 | Total Loss: 4.072882652282715 | KNN Loss: 3.06931209564209 | BCE Loss: 1.003570318222046\n",
      "Epoch 280 / 500 | iteration 10 / 30 | Total Loss: 4.046969413757324 | KNN Loss: 3.0592846870422363 | BCE Loss: 0.9876846671104431\n",
      "Epoch 280 / 500 | iteration 15 / 30 | Total Loss: 4.08355712890625 | KNN Loss: 3.046017646789551 | BCE Loss: 1.0375394821166992\n",
      "Epoch 280 / 500 | iteration 20 / 30 | Total Loss: 4.068936347961426 | KNN Loss: 3.067181348800659 | BCE Loss: 1.0017547607421875\n",
      "Epoch 280 / 500 | iteration 25 / 30 | Total Loss: 4.083847999572754 | KNN Loss: 3.051835060119629 | BCE Loss: 1.0320128202438354\n",
      "Epoch   281: reducing learning rate of group 0 to 2.3738e-05.\n",
      "Epoch 281 / 500 | iteration 0 / 30 | Total Loss: 4.108254432678223 | KNN Loss: 3.0775110721588135 | BCE Loss: 1.03074312210083\n",
      "Epoch 281 / 500 | iteration 5 / 30 | Total Loss: 4.066946029663086 | KNN Loss: 3.0624005794525146 | BCE Loss: 1.0045452117919922\n",
      "Epoch 281 / 500 | iteration 10 / 30 | Total Loss: 4.086638927459717 | KNN Loss: 3.0602025985717773 | BCE Loss: 1.0264363288879395\n",
      "Epoch 281 / 500 | iteration 15 / 30 | Total Loss: 4.052892684936523 | KNN Loss: 3.0436806678771973 | BCE Loss: 1.009211778640747\n",
      "Epoch 281 / 500 | iteration 20 / 30 | Total Loss: 4.105203151702881 | KNN Loss: 3.094240188598633 | BCE Loss: 1.0109630823135376\n",
      "Epoch 281 / 500 | iteration 25 / 30 | Total Loss: 4.114521503448486 | KNN Loss: 3.089982748031616 | BCE Loss: 1.0245387554168701\n",
      "Epoch 282 / 500 | iteration 0 / 30 | Total Loss: 4.110995292663574 | KNN Loss: 3.0805859565734863 | BCE Loss: 1.0304094552993774\n",
      "Epoch 282 / 500 | iteration 5 / 30 | Total Loss: 4.062465190887451 | KNN Loss: 3.049259901046753 | BCE Loss: 1.0132052898406982\n",
      "Epoch 282 / 500 | iteration 10 / 30 | Total Loss: 4.080378532409668 | KNN Loss: 3.0582172870635986 | BCE Loss: 1.0221612453460693\n",
      "Epoch 282 / 500 | iteration 15 / 30 | Total Loss: 4.077797889709473 | KNN Loss: 3.092994213104248 | BCE Loss: 0.9848034381866455\n",
      "Epoch 282 / 500 | iteration 20 / 30 | Total Loss: 4.098039150238037 | KNN Loss: 3.0824902057647705 | BCE Loss: 1.0155490636825562\n",
      "Epoch 282 / 500 | iteration 25 / 30 | Total Loss: 4.091091156005859 | KNN Loss: 3.0604634284973145 | BCE Loss: 1.030627727508545\n",
      "Epoch 283 / 500 | iteration 0 / 30 | Total Loss: 4.09779691696167 | KNN Loss: 3.086000919342041 | BCE Loss: 1.011795997619629\n",
      "Epoch 283 / 500 | iteration 5 / 30 | Total Loss: 4.072187900543213 | KNN Loss: 3.0640945434570312 | BCE Loss: 1.0080933570861816\n",
      "Epoch 283 / 500 | iteration 10 / 30 | Total Loss: 4.095905780792236 | KNN Loss: 3.05373477935791 | BCE Loss: 1.0421710014343262\n",
      "Epoch 283 / 500 | iteration 15 / 30 | Total Loss: 4.064167022705078 | KNN Loss: 3.0607945919036865 | BCE Loss: 1.0033726692199707\n",
      "Epoch 283 / 500 | iteration 20 / 30 | Total Loss: 4.073783874511719 | KNN Loss: 3.0823588371276855 | BCE Loss: 0.9914250373840332\n",
      "Epoch 283 / 500 | iteration 25 / 30 | Total Loss: 4.075308799743652 | KNN Loss: 3.046929121017456 | BCE Loss: 1.0283796787261963\n",
      "Epoch 284 / 500 | iteration 0 / 30 | Total Loss: 4.1182146072387695 | KNN Loss: 3.0732314586639404 | BCE Loss: 1.04498291015625\n",
      "Epoch 284 / 500 | iteration 5 / 30 | Total Loss: 4.106550216674805 | KNN Loss: 3.086691379547119 | BCE Loss: 1.019858717918396\n",
      "Epoch 284 / 500 | iteration 10 / 30 | Total Loss: 4.05702018737793 | KNN Loss: 3.0415141582489014 | BCE Loss: 1.0155061483383179\n",
      "Epoch 284 / 500 | iteration 15 / 30 | Total Loss: 4.1477556228637695 | KNN Loss: 3.1061153411865234 | BCE Loss: 1.041640281677246\n",
      "Epoch 284 / 500 | iteration 20 / 30 | Total Loss: 4.101894378662109 | KNN Loss: 3.088869094848633 | BCE Loss: 1.0130252838134766\n",
      "Epoch 284 / 500 | iteration 25 / 30 | Total Loss: 4.066413879394531 | KNN Loss: 3.057745933532715 | BCE Loss: 1.0086679458618164\n",
      "Epoch 285 / 500 | iteration 0 / 30 | Total Loss: 4.14763069152832 | KNN Loss: 3.0750153064727783 | BCE Loss: 1.072615146636963\n",
      "Epoch 285 / 500 | iteration 5 / 30 | Total Loss: 4.07551908493042 | KNN Loss: 3.0501153469085693 | BCE Loss: 1.0254038572311401\n",
      "Epoch 285 / 500 | iteration 10 / 30 | Total Loss: 4.089733123779297 | KNN Loss: 3.0784289836883545 | BCE Loss: 1.0113043785095215\n",
      "Epoch 285 / 500 | iteration 15 / 30 | Total Loss: 4.079161643981934 | KNN Loss: 3.074089765548706 | BCE Loss: 1.0050716400146484\n",
      "Epoch 285 / 500 | iteration 20 / 30 | Total Loss: 4.069591522216797 | KNN Loss: 3.0548133850097656 | BCE Loss: 1.0147783756256104\n",
      "Epoch 285 / 500 | iteration 25 / 30 | Total Loss: 4.131076812744141 | KNN Loss: 3.0853517055511475 | BCE Loss: 1.0457253456115723\n",
      "Epoch 286 / 500 | iteration 0 / 30 | Total Loss: 4.107458114624023 | KNN Loss: 3.072932243347168 | BCE Loss: 1.0345258712768555\n",
      "Epoch 286 / 500 | iteration 5 / 30 | Total Loss: 4.090777397155762 | KNN Loss: 3.060150384902954 | BCE Loss: 1.0306270122528076\n",
      "Epoch 286 / 500 | iteration 10 / 30 | Total Loss: 4.059191703796387 | KNN Loss: 3.038151979446411 | BCE Loss: 1.0210399627685547\n",
      "Epoch 286 / 500 | iteration 15 / 30 | Total Loss: 4.0918097496032715 | KNN Loss: 3.067877769470215 | BCE Loss: 1.0239320993423462\n",
      "Epoch 286 / 500 | iteration 20 / 30 | Total Loss: 4.074542999267578 | KNN Loss: 3.048322916030884 | BCE Loss: 1.0262200832366943\n",
      "Epoch 286 / 500 | iteration 25 / 30 | Total Loss: 4.063666343688965 | KNN Loss: 3.057420253753662 | BCE Loss: 1.0062460899353027\n",
      "Epoch 287 / 500 | iteration 0 / 30 | Total Loss: 4.124547958374023 | KNN Loss: 3.0896410942077637 | BCE Loss: 1.0349071025848389\n",
      "Epoch 287 / 500 | iteration 5 / 30 | Total Loss: 4.115207672119141 | KNN Loss: 3.0772714614868164 | BCE Loss: 1.0379363298416138\n",
      "Epoch 287 / 500 | iteration 10 / 30 | Total Loss: 4.10466194152832 | KNN Loss: 3.075284957885742 | BCE Loss: 1.029376745223999\n",
      "Epoch 287 / 500 | iteration 15 / 30 | Total Loss: 4.076852798461914 | KNN Loss: 3.0424704551696777 | BCE Loss: 1.0343821048736572\n",
      "Epoch 287 / 500 | iteration 20 / 30 | Total Loss: 4.066843032836914 | KNN Loss: 3.053061008453369 | BCE Loss: 1.0137817859649658\n",
      "Epoch 287 / 500 | iteration 25 / 30 | Total Loss: 4.114625453948975 | KNN Loss: 3.0763933658599854 | BCE Loss: 1.0382319688796997\n",
      "Epoch 288 / 500 | iteration 0 / 30 | Total Loss: 4.103560447692871 | KNN Loss: 3.069032669067383 | BCE Loss: 1.0345275402069092\n",
      "Epoch 288 / 500 | iteration 5 / 30 | Total Loss: 4.072788715362549 | KNN Loss: 3.056720733642578 | BCE Loss: 1.0160678625106812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 288 / 500 | iteration 10 / 30 | Total Loss: 4.066539764404297 | KNN Loss: 3.0562262535095215 | BCE Loss: 1.0103133916854858\n",
      "Epoch 288 / 500 | iteration 15 / 30 | Total Loss: 4.100150108337402 | KNN Loss: 3.0956203937530518 | BCE Loss: 1.0045299530029297\n",
      "Epoch 288 / 500 | iteration 20 / 30 | Total Loss: 4.098986625671387 | KNN Loss: 3.046168565750122 | BCE Loss: 1.052817940711975\n",
      "Epoch 288 / 500 | iteration 25 / 30 | Total Loss: 4.035426139831543 | KNN Loss: 3.0289559364318848 | BCE Loss: 1.0064704418182373\n",
      "Epoch 289 / 500 | iteration 0 / 30 | Total Loss: 4.111501693725586 | KNN Loss: 3.0806198120117188 | BCE Loss: 1.030881643295288\n",
      "Epoch 289 / 500 | iteration 5 / 30 | Total Loss: 4.087698936462402 | KNN Loss: 3.068136215209961 | BCE Loss: 1.0195629596710205\n",
      "Epoch 289 / 500 | iteration 10 / 30 | Total Loss: 4.036451816558838 | KNN Loss: 3.0418875217437744 | BCE Loss: 0.9945644736289978\n",
      "Epoch 289 / 500 | iteration 15 / 30 | Total Loss: 4.057404041290283 | KNN Loss: 3.048755407333374 | BCE Loss: 1.0086486339569092\n",
      "Epoch 289 / 500 | iteration 20 / 30 | Total Loss: 4.125600337982178 | KNN Loss: 3.0963358879089355 | BCE Loss: 1.0292644500732422\n",
      "Epoch 289 / 500 | iteration 25 / 30 | Total Loss: 4.070667266845703 | KNN Loss: 3.0749924182891846 | BCE Loss: 0.995674729347229\n",
      "Epoch 290 / 500 | iteration 0 / 30 | Total Loss: 4.096481800079346 | KNN Loss: 3.068347930908203 | BCE Loss: 1.0281338691711426\n",
      "Epoch 290 / 500 | iteration 5 / 30 | Total Loss: 4.09414005279541 | KNN Loss: 3.089893341064453 | BCE Loss: 1.004246711730957\n",
      "Epoch 290 / 500 | iteration 10 / 30 | Total Loss: 4.047492980957031 | KNN Loss: 3.090308904647827 | BCE Loss: 0.9571843147277832\n",
      "Epoch 290 / 500 | iteration 15 / 30 | Total Loss: 4.081230163574219 | KNN Loss: 3.0417394638061523 | BCE Loss: 1.0394909381866455\n",
      "Epoch 290 / 500 | iteration 20 / 30 | Total Loss: 4.102807998657227 | KNN Loss: 3.0622997283935547 | BCE Loss: 1.040508508682251\n",
      "Epoch 290 / 500 | iteration 25 / 30 | Total Loss: 4.056371688842773 | KNN Loss: 3.03981876373291 | BCE Loss: 1.0165531635284424\n",
      "Epoch 291 / 500 | iteration 0 / 30 | Total Loss: 4.08505916595459 | KNN Loss: 3.0677216053009033 | BCE Loss: 1.017337441444397\n",
      "Epoch 291 / 500 | iteration 5 / 30 | Total Loss: 4.060564994812012 | KNN Loss: 3.0657854080200195 | BCE Loss: 0.9947795867919922\n",
      "Epoch 291 / 500 | iteration 10 / 30 | Total Loss: 4.061210632324219 | KNN Loss: 3.0575032234191895 | BCE Loss: 1.0037071704864502\n",
      "Epoch 291 / 500 | iteration 15 / 30 | Total Loss: 4.062799453735352 | KNN Loss: 3.041476011276245 | BCE Loss: 1.021323561668396\n",
      "Epoch 291 / 500 | iteration 20 / 30 | Total Loss: 4.083829879760742 | KNN Loss: 3.069938898086548 | BCE Loss: 1.0138912200927734\n",
      "Epoch 291 / 500 | iteration 25 / 30 | Total Loss: 4.080508708953857 | KNN Loss: 3.0640387535095215 | BCE Loss: 1.016469955444336\n",
      "Epoch   292: reducing learning rate of group 0 to 1.6616e-05.\n",
      "Epoch 292 / 500 | iteration 0 / 30 | Total Loss: 4.150835037231445 | KNN Loss: 3.1074979305267334 | BCE Loss: 1.043337345123291\n",
      "Epoch 292 / 500 | iteration 5 / 30 | Total Loss: 4.073219299316406 | KNN Loss: 3.043369770050049 | BCE Loss: 1.0298495292663574\n",
      "Epoch 292 / 500 | iteration 10 / 30 | Total Loss: 4.084683418273926 | KNN Loss: 3.0567562580108643 | BCE Loss: 1.0279271602630615\n",
      "Epoch 292 / 500 | iteration 15 / 30 | Total Loss: 4.058960914611816 | KNN Loss: 3.069960355758667 | BCE Loss: 0.9890007972717285\n",
      "Epoch 292 / 500 | iteration 20 / 30 | Total Loss: 4.0835723876953125 | KNN Loss: 3.061077117919922 | BCE Loss: 1.0224955081939697\n",
      "Epoch 292 / 500 | iteration 25 / 30 | Total Loss: 4.0741777420043945 | KNN Loss: 3.0676140785217285 | BCE Loss: 1.006563425064087\n",
      "Epoch 293 / 500 | iteration 0 / 30 | Total Loss: 4.11199426651001 | KNN Loss: 3.070646047592163 | BCE Loss: 1.0413483381271362\n",
      "Epoch 293 / 500 | iteration 5 / 30 | Total Loss: 4.0894365310668945 | KNN Loss: 3.0798511505126953 | BCE Loss: 1.0095853805541992\n",
      "Epoch 293 / 500 | iteration 10 / 30 | Total Loss: 4.105776786804199 | KNN Loss: 3.095374345779419 | BCE Loss: 1.0104025602340698\n",
      "Epoch 293 / 500 | iteration 15 / 30 | Total Loss: 4.058204174041748 | KNN Loss: 3.0504729747772217 | BCE Loss: 1.0077311992645264\n",
      "Epoch 293 / 500 | iteration 20 / 30 | Total Loss: 4.107961654663086 | KNN Loss: 3.06491756439209 | BCE Loss: 1.043043851852417\n",
      "Epoch 293 / 500 | iteration 25 / 30 | Total Loss: 4.053947925567627 | KNN Loss: 3.047414541244507 | BCE Loss: 1.0065333843231201\n",
      "Epoch 294 / 500 | iteration 0 / 30 | Total Loss: 4.081017017364502 | KNN Loss: 3.082807779312134 | BCE Loss: 0.9982090592384338\n",
      "Epoch 294 / 500 | iteration 5 / 30 | Total Loss: 4.151588439941406 | KNN Loss: 3.094498872756958 | BCE Loss: 1.0570898056030273\n",
      "Epoch 294 / 500 | iteration 10 / 30 | Total Loss: 4.075307846069336 | KNN Loss: 3.053892135620117 | BCE Loss: 1.0214157104492188\n",
      "Epoch 294 / 500 | iteration 15 / 30 | Total Loss: 4.098697662353516 | KNN Loss: 3.0718555450439453 | BCE Loss: 1.0268423557281494\n",
      "Epoch 294 / 500 | iteration 20 / 30 | Total Loss: 4.093013286590576 | KNN Loss: 3.0660972595214844 | BCE Loss: 1.0269161462783813\n",
      "Epoch 294 / 500 | iteration 25 / 30 | Total Loss: 4.057987213134766 | KNN Loss: 3.0492935180664062 | BCE Loss: 1.0086939334869385\n",
      "Epoch 295 / 500 | iteration 0 / 30 | Total Loss: 4.085681915283203 | KNN Loss: 3.05782413482666 | BCE Loss: 1.027858018875122\n",
      "Epoch 295 / 500 | iteration 5 / 30 | Total Loss: 4.050970554351807 | KNN Loss: 3.0308196544647217 | BCE Loss: 1.020150899887085\n",
      "Epoch 295 / 500 | iteration 10 / 30 | Total Loss: 4.090999603271484 | KNN Loss: 3.056591749191284 | BCE Loss: 1.0344078540802002\n",
      "Epoch 295 / 500 | iteration 15 / 30 | Total Loss: 4.089694023132324 | KNN Loss: 3.044217586517334 | BCE Loss: 1.0454761981964111\n",
      "Epoch 295 / 500 | iteration 20 / 30 | Total Loss: 4.077871322631836 | KNN Loss: 3.0651135444641113 | BCE Loss: 1.0127580165863037\n",
      "Epoch 295 / 500 | iteration 25 / 30 | Total Loss: 4.093414783477783 | KNN Loss: 3.052978038787842 | BCE Loss: 1.0404367446899414\n",
      "Epoch 296 / 500 | iteration 0 / 30 | Total Loss: 4.129061698913574 | KNN Loss: 3.0908241271972656 | BCE Loss: 1.0382375717163086\n",
      "Epoch 296 / 500 | iteration 5 / 30 | Total Loss: 4.093531131744385 | KNN Loss: 3.096229314804077 | BCE Loss: 0.9973018169403076\n",
      "Epoch 296 / 500 | iteration 10 / 30 | Total Loss: 4.170938491821289 | KNN Loss: 3.0916407108306885 | BCE Loss: 1.0792977809906006\n",
      "Epoch 296 / 500 | iteration 15 / 30 | Total Loss: 4.067243576049805 | KNN Loss: 3.0367345809936523 | BCE Loss: 1.0305089950561523\n",
      "Epoch 296 / 500 | iteration 20 / 30 | Total Loss: 4.105681419372559 | KNN Loss: 3.106755256652832 | BCE Loss: 0.9989261627197266\n",
      "Epoch 296 / 500 | iteration 25 / 30 | Total Loss: 4.088437557220459 | KNN Loss: 3.070002555847168 | BCE Loss: 1.018435001373291\n",
      "Epoch 297 / 500 | iteration 0 / 30 | Total Loss: 4.109051704406738 | KNN Loss: 3.1149942874908447 | BCE Loss: 0.9940574169158936\n",
      "Epoch 297 / 500 | iteration 5 / 30 | Total Loss: 4.072372913360596 | KNN Loss: 3.0721495151519775 | BCE Loss: 1.0002233982086182\n",
      "Epoch 297 / 500 | iteration 10 / 30 | Total Loss: 4.038145065307617 | KNN Loss: 3.026078462600708 | BCE Loss: 1.0120667219161987\n",
      "Epoch 297 / 500 | iteration 15 / 30 | Total Loss: 4.112948417663574 | KNN Loss: 3.0795352458953857 | BCE Loss: 1.0334131717681885\n",
      "Epoch 297 / 500 | iteration 20 / 30 | Total Loss: 4.051966667175293 | KNN Loss: 3.054192304611206 | BCE Loss: 0.9977743625640869\n",
      "Epoch 297 / 500 | iteration 25 / 30 | Total Loss: 4.125328063964844 | KNN Loss: 3.1301400661468506 | BCE Loss: 0.9951878786087036\n",
      "Epoch 298 / 500 | iteration 0 / 30 | Total Loss: 4.093933582305908 | KNN Loss: 3.0521013736724854 | BCE Loss: 1.0418323278427124\n",
      "Epoch 298 / 500 | iteration 5 / 30 | Total Loss: 4.069276332855225 | KNN Loss: 3.0431652069091797 | BCE Loss: 1.026111125946045\n",
      "Epoch 298 / 500 | iteration 10 / 30 | Total Loss: 4.1193060874938965 | KNN Loss: 3.0828866958618164 | BCE Loss: 1.03641939163208\n",
      "Epoch 298 / 500 | iteration 15 / 30 | Total Loss: 4.104290008544922 | KNN Loss: 3.0727474689483643 | BCE Loss: 1.0315425395965576\n",
      "Epoch 298 / 500 | iteration 20 / 30 | Total Loss: 4.08394718170166 | KNN Loss: 3.0517518520355225 | BCE Loss: 1.0321955680847168\n",
      "Epoch 298 / 500 | iteration 25 / 30 | Total Loss: 4.040488243103027 | KNN Loss: 3.048062562942505 | BCE Loss: 0.9924256801605225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 299 / 500 | iteration 0 / 30 | Total Loss: 4.1044464111328125 | KNN Loss: 3.078803777694702 | BCE Loss: 1.0256426334381104\n",
      "Epoch 299 / 500 | iteration 5 / 30 | Total Loss: 4.040267467498779 | KNN Loss: 3.037220001220703 | BCE Loss: 1.0030474662780762\n",
      "Epoch 299 / 500 | iteration 10 / 30 | Total Loss: 4.0728230476379395 | KNN Loss: 3.063408613204956 | BCE Loss: 1.009414553642273\n",
      "Epoch 299 / 500 | iteration 15 / 30 | Total Loss: 4.089882850646973 | KNN Loss: 3.0767266750335693 | BCE Loss: 1.0131564140319824\n",
      "Epoch 299 / 500 | iteration 20 / 30 | Total Loss: 4.052104473114014 | KNN Loss: 3.034122943878174 | BCE Loss: 1.0179816484451294\n",
      "Epoch 299 / 500 | iteration 25 / 30 | Total Loss: 4.053789138793945 | KNN Loss: 3.049194097518921 | BCE Loss: 1.0045948028564453\n",
      "Epoch 300 / 500 | iteration 0 / 30 | Total Loss: 4.108120918273926 | KNN Loss: 3.0961012840270996 | BCE Loss: 1.012019395828247\n",
      "Epoch 300 / 500 | iteration 5 / 30 | Total Loss: 4.086413383483887 | KNN Loss: 3.0748889446258545 | BCE Loss: 1.0115242004394531\n",
      "Epoch 300 / 500 | iteration 10 / 30 | Total Loss: 4.081171989440918 | KNN Loss: 3.0649843215942383 | BCE Loss: 1.0161875486373901\n",
      "Epoch 300 / 500 | iteration 15 / 30 | Total Loss: 4.1026105880737305 | KNN Loss: 3.095891237258911 | BCE Loss: 1.0067195892333984\n",
      "Epoch 300 / 500 | iteration 20 / 30 | Total Loss: 4.08450984954834 | KNN Loss: 3.0602869987487793 | BCE Loss: 1.0242226123809814\n",
      "Epoch 300 / 500 | iteration 25 / 30 | Total Loss: 4.0450568199157715 | KNN Loss: 3.0307388305664062 | BCE Loss: 1.0143181085586548\n",
      "Epoch 301 / 500 | iteration 0 / 30 | Total Loss: 4.089034080505371 | KNN Loss: 3.0824217796325684 | BCE Loss: 1.0066124200820923\n",
      "Epoch 301 / 500 | iteration 5 / 30 | Total Loss: 4.104179382324219 | KNN Loss: 3.0981481075286865 | BCE Loss: 1.0060315132141113\n",
      "Epoch 301 / 500 | iteration 10 / 30 | Total Loss: 4.094497203826904 | KNN Loss: 3.063466787338257 | BCE Loss: 1.031030535697937\n",
      "Epoch 301 / 500 | iteration 15 / 30 | Total Loss: 4.092945098876953 | KNN Loss: 3.054692029953003 | BCE Loss: 1.038252830505371\n",
      "Epoch 301 / 500 | iteration 20 / 30 | Total Loss: 4.152242183685303 | KNN Loss: 3.154357433319092 | BCE Loss: 0.9978847503662109\n",
      "Epoch 301 / 500 | iteration 25 / 30 | Total Loss: 4.137854099273682 | KNN Loss: 3.081526279449463 | BCE Loss: 1.0563279390335083\n",
      "Epoch 302 / 500 | iteration 0 / 30 | Total Loss: 4.0592851638793945 | KNN Loss: 3.05435848236084 | BCE Loss: 1.0049265623092651\n",
      "Epoch 302 / 500 | iteration 5 / 30 | Total Loss: 4.081720352172852 | KNN Loss: 3.048027753829956 | BCE Loss: 1.0336928367614746\n",
      "Epoch 302 / 500 | iteration 10 / 30 | Total Loss: 4.114705562591553 | KNN Loss: 3.07700777053833 | BCE Loss: 1.0376977920532227\n",
      "Epoch 302 / 500 | iteration 15 / 30 | Total Loss: 4.111650466918945 | KNN Loss: 3.0720903873443604 | BCE Loss: 1.039560317993164\n",
      "Epoch 302 / 500 | iteration 20 / 30 | Total Loss: 4.038093090057373 | KNN Loss: 3.0461978912353516 | BCE Loss: 0.991895318031311\n",
      "Epoch 302 / 500 | iteration 25 / 30 | Total Loss: 4.113440036773682 | KNN Loss: 3.068978786468506 | BCE Loss: 1.0444613695144653\n",
      "Epoch   303: reducing learning rate of group 0 to 1.1632e-05.\n",
      "Epoch 303 / 500 | iteration 0 / 30 | Total Loss: 4.123800277709961 | KNN Loss: 3.0760908126831055 | BCE Loss: 1.0477097034454346\n",
      "Epoch 303 / 500 | iteration 5 / 30 | Total Loss: 4.079980373382568 | KNN Loss: 3.042706251144409 | BCE Loss: 1.0372741222381592\n",
      "Epoch 303 / 500 | iteration 10 / 30 | Total Loss: 4.083824157714844 | KNN Loss: 3.0650763511657715 | BCE Loss: 1.0187475681304932\n",
      "Epoch 303 / 500 | iteration 15 / 30 | Total Loss: 4.110713005065918 | KNN Loss: 3.0744991302490234 | BCE Loss: 1.0362138748168945\n",
      "Epoch 303 / 500 | iteration 20 / 30 | Total Loss: 4.111116886138916 | KNN Loss: 3.0858988761901855 | BCE Loss: 1.0252180099487305\n",
      "Epoch 303 / 500 | iteration 25 / 30 | Total Loss: 4.078741073608398 | KNN Loss: 3.0567665100097656 | BCE Loss: 1.021974802017212\n",
      "Epoch 304 / 500 | iteration 0 / 30 | Total Loss: 4.105949401855469 | KNN Loss: 3.0680835247039795 | BCE Loss: 1.0378656387329102\n",
      "Epoch 304 / 500 | iteration 5 / 30 | Total Loss: 4.085615634918213 | KNN Loss: 3.0584311485290527 | BCE Loss: 1.0271844863891602\n",
      "Epoch 304 / 500 | iteration 10 / 30 | Total Loss: 4.0987982749938965 | KNN Loss: 3.0849170684814453 | BCE Loss: 1.0138812065124512\n",
      "Epoch 304 / 500 | iteration 15 / 30 | Total Loss: 4.081789493560791 | KNN Loss: 3.0534443855285645 | BCE Loss: 1.028344988822937\n",
      "Epoch 304 / 500 | iteration 20 / 30 | Total Loss: 4.0853352546691895 | KNN Loss: 3.0668134689331055 | BCE Loss: 1.018521785736084\n",
      "Epoch 304 / 500 | iteration 25 / 30 | Total Loss: 4.165271282196045 | KNN Loss: 3.115280866622925 | BCE Loss: 1.0499902963638306\n",
      "Epoch 305 / 500 | iteration 0 / 30 | Total Loss: 4.162208557128906 | KNN Loss: 3.1222310066223145 | BCE Loss: 1.0399775505065918\n",
      "Epoch 305 / 500 | iteration 5 / 30 | Total Loss: 4.097344875335693 | KNN Loss: 3.0872802734375 | BCE Loss: 1.010064721107483\n",
      "Epoch 305 / 500 | iteration 10 / 30 | Total Loss: 4.116400718688965 | KNN Loss: 3.1072020530700684 | BCE Loss: 1.0091984272003174\n",
      "Epoch 305 / 500 | iteration 15 / 30 | Total Loss: 4.088023662567139 | KNN Loss: 3.057262659072876 | BCE Loss: 1.0307610034942627\n",
      "Epoch 305 / 500 | iteration 20 / 30 | Total Loss: 4.0630035400390625 | KNN Loss: 3.043797492980957 | BCE Loss: 1.0192062854766846\n",
      "Epoch 305 / 500 | iteration 25 / 30 | Total Loss: 4.043214797973633 | KNN Loss: 3.032820224761963 | BCE Loss: 1.0103944540023804\n",
      "Epoch 306 / 500 | iteration 0 / 30 | Total Loss: 4.087564468383789 | KNN Loss: 3.07791805267334 | BCE Loss: 1.0096461772918701\n",
      "Epoch 306 / 500 | iteration 5 / 30 | Total Loss: 4.0720295906066895 | KNN Loss: 3.0477256774902344 | BCE Loss: 1.024303913116455\n",
      "Epoch 306 / 500 | iteration 10 / 30 | Total Loss: 4.069519996643066 | KNN Loss: 3.0683770179748535 | BCE Loss: 1.0011427402496338\n",
      "Epoch 306 / 500 | iteration 15 / 30 | Total Loss: 4.099384307861328 | KNN Loss: 3.1044437885284424 | BCE Loss: 0.9949403405189514\n",
      "Epoch 306 / 500 | iteration 20 / 30 | Total Loss: 4.115909576416016 | KNN Loss: 3.064856767654419 | BCE Loss: 1.0510526895523071\n",
      "Epoch 306 / 500 | iteration 25 / 30 | Total Loss: 4.073902130126953 | KNN Loss: 3.0483546257019043 | BCE Loss: 1.025547742843628\n",
      "Epoch 307 / 500 | iteration 0 / 30 | Total Loss: 4.040905475616455 | KNN Loss: 3.0782761573791504 | BCE Loss: 0.962629497051239\n",
      "Epoch 307 / 500 | iteration 5 / 30 | Total Loss: 4.101200580596924 | KNN Loss: 3.08864426612854 | BCE Loss: 1.0125563144683838\n",
      "Epoch 307 / 500 | iteration 10 / 30 | Total Loss: 4.07423210144043 | KNN Loss: 3.0480058193206787 | BCE Loss: 1.02622652053833\n",
      "Epoch 307 / 500 | iteration 15 / 30 | Total Loss: 4.064633369445801 | KNN Loss: 3.045299530029297 | BCE Loss: 1.0193339586257935\n",
      "Epoch 307 / 500 | iteration 20 / 30 | Total Loss: 4.123859882354736 | KNN Loss: 3.077901840209961 | BCE Loss: 1.045958161354065\n",
      "Epoch 307 / 500 | iteration 25 / 30 | Total Loss: 4.078765869140625 | KNN Loss: 3.060553550720215 | BCE Loss: 1.018212080001831\n",
      "Epoch 308 / 500 | iteration 0 / 30 | Total Loss: 4.15647554397583 | KNN Loss: 3.1098790168762207 | BCE Loss: 1.046596646308899\n",
      "Epoch 308 / 500 | iteration 5 / 30 | Total Loss: 4.091557025909424 | KNN Loss: 3.064568519592285 | BCE Loss: 1.0269885063171387\n",
      "Epoch 308 / 500 | iteration 10 / 30 | Total Loss: 4.054838180541992 | KNN Loss: 3.04414701461792 | BCE Loss: 1.0106911659240723\n",
      "Epoch 308 / 500 | iteration 15 / 30 | Total Loss: 4.082921028137207 | KNN Loss: 3.0688602924346924 | BCE Loss: 1.0140604972839355\n",
      "Epoch 308 / 500 | iteration 20 / 30 | Total Loss: 4.097718238830566 | KNN Loss: 3.080379009246826 | BCE Loss: 1.0173393487930298\n",
      "Epoch 308 / 500 | iteration 25 / 30 | Total Loss: 4.074333190917969 | KNN Loss: 3.075035572052002 | BCE Loss: 0.9992978572845459\n",
      "Epoch 309 / 500 | iteration 0 / 30 | Total Loss: 4.119666576385498 | KNN Loss: 3.0935933589935303 | BCE Loss: 1.0260732173919678\n",
      "Epoch 309 / 500 | iteration 5 / 30 | Total Loss: 4.101259231567383 | KNN Loss: 3.048543930053711 | BCE Loss: 1.0527153015136719\n",
      "Epoch 309 / 500 | iteration 10 / 30 | Total Loss: 4.094945907592773 | KNN Loss: 3.0916213989257812 | BCE Loss: 1.0033247470855713\n",
      "Epoch 309 / 500 | iteration 15 / 30 | Total Loss: 4.1070170402526855 | KNN Loss: 3.095374345779419 | BCE Loss: 1.0116428136825562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 309 / 500 | iteration 20 / 30 | Total Loss: 4.0407395362854 | KNN Loss: 3.025773048400879 | BCE Loss: 1.0149664878845215\n",
      "Epoch 309 / 500 | iteration 25 / 30 | Total Loss: 4.068409442901611 | KNN Loss: 3.055664539337158 | BCE Loss: 1.0127449035644531\n",
      "Epoch 310 / 500 | iteration 0 / 30 | Total Loss: 4.109676361083984 | KNN Loss: 3.090897560119629 | BCE Loss: 1.0187790393829346\n",
      "Epoch 310 / 500 | iteration 5 / 30 | Total Loss: 4.047722339630127 | KNN Loss: 3.057224750518799 | BCE Loss: 0.9904977083206177\n",
      "Epoch 310 / 500 | iteration 10 / 30 | Total Loss: 4.035503387451172 | KNN Loss: 3.031756639480591 | BCE Loss: 1.0037468671798706\n",
      "Epoch 310 / 500 | iteration 15 / 30 | Total Loss: 4.065025806427002 | KNN Loss: 3.052802801132202 | BCE Loss: 1.0122230052947998\n",
      "Epoch 310 / 500 | iteration 20 / 30 | Total Loss: 4.140834808349609 | KNN Loss: 3.069694757461548 | BCE Loss: 1.0711400508880615\n",
      "Epoch 310 / 500 | iteration 25 / 30 | Total Loss: 4.116403102874756 | KNN Loss: 3.0655839443206787 | BCE Loss: 1.0508191585540771\n",
      "Epoch 311 / 500 | iteration 0 / 30 | Total Loss: 4.056179046630859 | KNN Loss: 3.0457324981689453 | BCE Loss: 1.0104464292526245\n",
      "Epoch 311 / 500 | iteration 5 / 30 | Total Loss: 4.062577247619629 | KNN Loss: 3.038763999938965 | BCE Loss: 1.0238133668899536\n",
      "Epoch 311 / 500 | iteration 10 / 30 | Total Loss: 4.067810535430908 | KNN Loss: 3.0976974964141846 | BCE Loss: 0.9701129198074341\n",
      "Epoch 311 / 500 | iteration 15 / 30 | Total Loss: 4.127969264984131 | KNN Loss: 3.0703272819519043 | BCE Loss: 1.0576421022415161\n",
      "Epoch 311 / 500 | iteration 20 / 30 | Total Loss: 4.102938652038574 | KNN Loss: 3.079149007797241 | BCE Loss: 1.023789882659912\n",
      "Epoch 311 / 500 | iteration 25 / 30 | Total Loss: 4.084344863891602 | KNN Loss: 3.075619697570801 | BCE Loss: 1.0087254047393799\n",
      "Epoch 312 / 500 | iteration 0 / 30 | Total Loss: 4.023784637451172 | KNN Loss: 3.0116937160491943 | BCE Loss: 1.012090802192688\n",
      "Epoch 312 / 500 | iteration 5 / 30 | Total Loss: 4.054733753204346 | KNN Loss: 3.0450711250305176 | BCE Loss: 1.0096626281738281\n",
      "Epoch 312 / 500 | iteration 10 / 30 | Total Loss: 4.089290142059326 | KNN Loss: 3.0510311126708984 | BCE Loss: 1.0382589101791382\n",
      "Epoch 312 / 500 | iteration 15 / 30 | Total Loss: 4.086835861206055 | KNN Loss: 3.0786056518554688 | BCE Loss: 1.0082303285598755\n",
      "Epoch 312 / 500 | iteration 20 / 30 | Total Loss: 4.084227561950684 | KNN Loss: 3.0425667762756348 | BCE Loss: 1.0416606664657593\n",
      "Epoch 312 / 500 | iteration 25 / 30 | Total Loss: 4.0970540046691895 | KNN Loss: 3.0821805000305176 | BCE Loss: 1.0148735046386719\n",
      "Epoch 313 / 500 | iteration 0 / 30 | Total Loss: 4.1059770584106445 | KNN Loss: 3.0676379203796387 | BCE Loss: 1.0383391380310059\n",
      "Epoch 313 / 500 | iteration 5 / 30 | Total Loss: 4.073037624359131 | KNN Loss: 3.0634610652923584 | BCE Loss: 1.0095765590667725\n",
      "Epoch 313 / 500 | iteration 10 / 30 | Total Loss: 4.087741851806641 | KNN Loss: 3.0613858699798584 | BCE Loss: 1.0263561010360718\n",
      "Epoch 313 / 500 | iteration 15 / 30 | Total Loss: 4.09380578994751 | KNN Loss: 3.066323757171631 | BCE Loss: 1.027482032775879\n",
      "Epoch 313 / 500 | iteration 20 / 30 | Total Loss: 4.036375999450684 | KNN Loss: 3.037426471710205 | BCE Loss: 0.9989496469497681\n",
      "Epoch 313 / 500 | iteration 25 / 30 | Total Loss: 4.091723442077637 | KNN Loss: 3.0729548931121826 | BCE Loss: 1.0187687873840332\n",
      "Epoch   314: reducing learning rate of group 0 to 8.1421e-06.\n",
      "Epoch 314 / 500 | iteration 0 / 30 | Total Loss: 4.085577964782715 | KNN Loss: 3.055123805999756 | BCE Loss: 1.0304540395736694\n",
      "Epoch 314 / 500 | iteration 5 / 30 | Total Loss: 4.113705635070801 | KNN Loss: 3.102686643600464 | BCE Loss: 1.0110187530517578\n",
      "Epoch 314 / 500 | iteration 10 / 30 | Total Loss: 4.096982002258301 | KNN Loss: 3.0690157413482666 | BCE Loss: 1.027966022491455\n",
      "Epoch 314 / 500 | iteration 15 / 30 | Total Loss: 4.073559284210205 | KNN Loss: 3.0689964294433594 | BCE Loss: 1.0045627355575562\n",
      "Epoch 314 / 500 | iteration 20 / 30 | Total Loss: 4.0640645027160645 | KNN Loss: 3.040642261505127 | BCE Loss: 1.0234222412109375\n",
      "Epoch 314 / 500 | iteration 25 / 30 | Total Loss: 4.127898216247559 | KNN Loss: 3.109389066696167 | BCE Loss: 1.0185091495513916\n",
      "Epoch 315 / 500 | iteration 0 / 30 | Total Loss: 4.081798553466797 | KNN Loss: 3.0629916191101074 | BCE Loss: 1.0188066959381104\n",
      "Epoch 315 / 500 | iteration 5 / 30 | Total Loss: 4.121927261352539 | KNN Loss: 3.0775957107543945 | BCE Loss: 1.044331669807434\n",
      "Epoch 315 / 500 | iteration 10 / 30 | Total Loss: 4.127032279968262 | KNN Loss: 3.086867332458496 | BCE Loss: 1.0401647090911865\n",
      "Epoch 315 / 500 | iteration 15 / 30 | Total Loss: 4.04678201675415 | KNN Loss: 3.0452847480773926 | BCE Loss: 1.0014972686767578\n",
      "Epoch 315 / 500 | iteration 20 / 30 | Total Loss: 4.073880672454834 | KNN Loss: 3.057908296585083 | BCE Loss: 1.0159724950790405\n",
      "Epoch 315 / 500 | iteration 25 / 30 | Total Loss: 4.105849742889404 | KNN Loss: 3.0872559547424316 | BCE Loss: 1.0185937881469727\n",
      "Epoch 316 / 500 | iteration 0 / 30 | Total Loss: 4.0962018966674805 | KNN Loss: 3.0500733852386475 | BCE Loss: 1.046128511428833\n",
      "Epoch 316 / 500 | iteration 5 / 30 | Total Loss: 4.108019828796387 | KNN Loss: 3.096609115600586 | BCE Loss: 1.0114104747772217\n",
      "Epoch 316 / 500 | iteration 10 / 30 | Total Loss: 4.058010101318359 | KNN Loss: 3.0747580528259277 | BCE Loss: 0.9832521677017212\n",
      "Epoch 316 / 500 | iteration 15 / 30 | Total Loss: 4.080591201782227 | KNN Loss: 3.055605888366699 | BCE Loss: 1.0249855518341064\n",
      "Epoch 316 / 500 | iteration 20 / 30 | Total Loss: 4.074377059936523 | KNN Loss: 3.062727928161621 | BCE Loss: 1.0116488933563232\n",
      "Epoch 316 / 500 | iteration 25 / 30 | Total Loss: 4.099958896636963 | KNN Loss: 3.0834856033325195 | BCE Loss: 1.016473412513733\n",
      "Epoch 317 / 500 | iteration 0 / 30 | Total Loss: 4.10405969619751 | KNN Loss: 3.075896739959717 | BCE Loss: 1.028162956237793\n",
      "Epoch 317 / 500 | iteration 5 / 30 | Total Loss: 4.0526604652404785 | KNN Loss: 3.0421791076660156 | BCE Loss: 1.0104812383651733\n",
      "Epoch 317 / 500 | iteration 10 / 30 | Total Loss: 4.1246747970581055 | KNN Loss: 3.0665323734283447 | BCE Loss: 1.0581421852111816\n",
      "Epoch 317 / 500 | iteration 15 / 30 | Total Loss: 4.060739517211914 | KNN Loss: 3.04258394241333 | BCE Loss: 1.0181554555892944\n",
      "Epoch 317 / 500 | iteration 20 / 30 | Total Loss: 4.031498432159424 | KNN Loss: 3.0258820056915283 | BCE Loss: 1.005616545677185\n",
      "Epoch 317 / 500 | iteration 25 / 30 | Total Loss: 4.040228366851807 | KNN Loss: 3.032376766204834 | BCE Loss: 1.007851481437683\n",
      "Epoch 318 / 500 | iteration 0 / 30 | Total Loss: 4.09234619140625 | KNN Loss: 3.087007761001587 | BCE Loss: 1.0053383111953735\n",
      "Epoch 318 / 500 | iteration 5 / 30 | Total Loss: 4.076808452606201 | KNN Loss: 3.052368640899658 | BCE Loss: 1.0244399309158325\n",
      "Epoch 318 / 500 | iteration 10 / 30 | Total Loss: 4.03415060043335 | KNN Loss: 3.046069860458374 | BCE Loss: 0.9880808591842651\n",
      "Epoch 318 / 500 | iteration 15 / 30 | Total Loss: 4.085239887237549 | KNN Loss: 3.0578529834747314 | BCE Loss: 1.0273869037628174\n",
      "Epoch 318 / 500 | iteration 20 / 30 | Total Loss: 4.118678569793701 | KNN Loss: 3.0786056518554688 | BCE Loss: 1.0400727987289429\n",
      "Epoch 318 / 500 | iteration 25 / 30 | Total Loss: 4.081145286560059 | KNN Loss: 3.073272943496704 | BCE Loss: 1.0078721046447754\n",
      "Epoch 319 / 500 | iteration 0 / 30 | Total Loss: 4.141592979431152 | KNN Loss: 3.122546672821045 | BCE Loss: 1.0190463066101074\n",
      "Epoch 319 / 500 | iteration 5 / 30 | Total Loss: 4.0546746253967285 | KNN Loss: 3.0421559810638428 | BCE Loss: 1.0125187635421753\n",
      "Epoch 319 / 500 | iteration 10 / 30 | Total Loss: 4.082056045532227 | KNN Loss: 3.078486680984497 | BCE Loss: 1.0035693645477295\n",
      "Epoch 319 / 500 | iteration 15 / 30 | Total Loss: 4.089113712310791 | KNN Loss: 3.057054281234741 | BCE Loss: 1.0320593118667603\n",
      "Epoch 319 / 500 | iteration 20 / 30 | Total Loss: 4.122906684875488 | KNN Loss: 3.0734469890594482 | BCE Loss: 1.0494599342346191\n",
      "Epoch 319 / 500 | iteration 25 / 30 | Total Loss: 4.10706090927124 | KNN Loss: 3.0868279933929443 | BCE Loss: 1.020232915878296\n",
      "Epoch 320 / 500 | iteration 0 / 30 | Total Loss: 4.059543609619141 | KNN Loss: 3.031883478164673 | BCE Loss: 1.0276598930358887\n",
      "Epoch 320 / 500 | iteration 5 / 30 | Total Loss: 4.083226203918457 | KNN Loss: 3.049757719039917 | BCE Loss: 1.033468246459961\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 320 / 500 | iteration 10 / 30 | Total Loss: 4.0772504806518555 | KNN Loss: 3.0411808490753174 | BCE Loss: 1.0360698699951172\n",
      "Epoch 320 / 500 | iteration 15 / 30 | Total Loss: 4.073978424072266 | KNN Loss: 3.0452327728271484 | BCE Loss: 1.0287456512451172\n",
      "Epoch 320 / 500 | iteration 20 / 30 | Total Loss: 4.067128658294678 | KNN Loss: 3.046569347381592 | BCE Loss: 1.0205594301223755\n",
      "Epoch 320 / 500 | iteration 25 / 30 | Total Loss: 4.069195747375488 | KNN Loss: 3.0501182079315186 | BCE Loss: 1.0190777778625488\n",
      "Epoch 321 / 500 | iteration 0 / 30 | Total Loss: 4.038179874420166 | KNN Loss: 3.0461928844451904 | BCE Loss: 0.9919869899749756\n",
      "Epoch 321 / 500 | iteration 5 / 30 | Total Loss: 4.111441612243652 | KNN Loss: 3.0863091945648193 | BCE Loss: 1.025132656097412\n",
      "Epoch 321 / 500 | iteration 10 / 30 | Total Loss: 4.10088586807251 | KNN Loss: 3.1105129718780518 | BCE Loss: 0.9903728365898132\n",
      "Epoch 321 / 500 | iteration 15 / 30 | Total Loss: 4.069765090942383 | KNN Loss: 3.062263250350952 | BCE Loss: 1.0075016021728516\n",
      "Epoch 321 / 500 | iteration 20 / 30 | Total Loss: 4.125579833984375 | KNN Loss: 3.0895979404449463 | BCE Loss: 1.0359818935394287\n",
      "Epoch 321 / 500 | iteration 25 / 30 | Total Loss: 4.054032325744629 | KNN Loss: 3.0496673583984375 | BCE Loss: 1.0043647289276123\n",
      "Epoch 322 / 500 | iteration 0 / 30 | Total Loss: 4.078577041625977 | KNN Loss: 3.0625576972961426 | BCE Loss: 1.016019344329834\n",
      "Epoch 322 / 500 | iteration 5 / 30 | Total Loss: 4.063345909118652 | KNN Loss: 3.0408742427825928 | BCE Loss: 1.0224716663360596\n",
      "Epoch 322 / 500 | iteration 10 / 30 | Total Loss: 4.065906047821045 | KNN Loss: 3.042816162109375 | BCE Loss: 1.0230897665023804\n",
      "Epoch 322 / 500 | iteration 15 / 30 | Total Loss: 4.087921142578125 | KNN Loss: 3.0538055896759033 | BCE Loss: 1.0341154336929321\n",
      "Epoch 322 / 500 | iteration 20 / 30 | Total Loss: 4.072024822235107 | KNN Loss: 3.057813882827759 | BCE Loss: 1.0142109394073486\n",
      "Epoch 322 / 500 | iteration 25 / 30 | Total Loss: 4.087306976318359 | KNN Loss: 3.0672409534454346 | BCE Loss: 1.0200660228729248\n",
      "Epoch 323 / 500 | iteration 0 / 30 | Total Loss: 4.044036865234375 | KNN Loss: 3.0247912406921387 | BCE Loss: 1.0192453861236572\n",
      "Epoch 323 / 500 | iteration 5 / 30 | Total Loss: 4.110873222351074 | KNN Loss: 3.0692179203033447 | BCE Loss: 1.0416550636291504\n",
      "Epoch 323 / 500 | iteration 10 / 30 | Total Loss: 4.055576324462891 | KNN Loss: 3.0445055961608887 | BCE Loss: 1.011070966720581\n",
      "Epoch 323 / 500 | iteration 15 / 30 | Total Loss: 4.118661403656006 | KNN Loss: 3.1062333583831787 | BCE Loss: 1.0124281644821167\n",
      "Epoch 323 / 500 | iteration 20 / 30 | Total Loss: 4.047534465789795 | KNN Loss: 3.0364913940429688 | BCE Loss: 1.0110430717468262\n",
      "Epoch 323 / 500 | iteration 25 / 30 | Total Loss: 4.07420539855957 | KNN Loss: 3.0459296703338623 | BCE Loss: 1.028275966644287\n",
      "Epoch 324 / 500 | iteration 0 / 30 | Total Loss: 4.066043853759766 | KNN Loss: 3.0550131797790527 | BCE Loss: 1.011030912399292\n",
      "Epoch 324 / 500 | iteration 5 / 30 | Total Loss: 4.078996658325195 | KNN Loss: 3.069519281387329 | BCE Loss: 1.0094772577285767\n",
      "Epoch 324 / 500 | iteration 10 / 30 | Total Loss: 4.071943759918213 | KNN Loss: 3.046879529953003 | BCE Loss: 1.02506422996521\n",
      "Epoch 324 / 500 | iteration 15 / 30 | Total Loss: 4.110446929931641 | KNN Loss: 3.091491460800171 | BCE Loss: 1.0189552307128906\n",
      "Epoch 324 / 500 | iteration 20 / 30 | Total Loss: 4.0735650062561035 | KNN Loss: 3.0708513259887695 | BCE Loss: 1.0027135610580444\n",
      "Epoch 324 / 500 | iteration 25 / 30 | Total Loss: 4.1143646240234375 | KNN Loss: 3.0620486736297607 | BCE Loss: 1.0523161888122559\n",
      "Epoch   325: reducing learning rate of group 0 to 5.6994e-06.\n",
      "Epoch 325 / 500 | iteration 0 / 30 | Total Loss: 4.105386734008789 | KNN Loss: 3.066751003265381 | BCE Loss: 1.0386359691619873\n",
      "Epoch 325 / 500 | iteration 5 / 30 | Total Loss: 4.091496467590332 | KNN Loss: 3.0848705768585205 | BCE Loss: 1.0066256523132324\n",
      "Epoch 325 / 500 | iteration 10 / 30 | Total Loss: 4.074249744415283 | KNN Loss: 3.0517358779907227 | BCE Loss: 1.0225138664245605\n",
      "Epoch 325 / 500 | iteration 15 / 30 | Total Loss: 4.073358535766602 | KNN Loss: 3.0732381343841553 | BCE Loss: 1.0001201629638672\n",
      "Epoch 325 / 500 | iteration 20 / 30 | Total Loss: 4.141051292419434 | KNN Loss: 3.0944087505340576 | BCE Loss: 1.0466423034667969\n",
      "Epoch 325 / 500 | iteration 25 / 30 | Total Loss: 4.062358856201172 | KNN Loss: 3.041705369949341 | BCE Loss: 1.020653486251831\n",
      "Epoch 326 / 500 | iteration 0 / 30 | Total Loss: 4.104284763336182 | KNN Loss: 3.073897123336792 | BCE Loss: 1.0303876399993896\n",
      "Epoch 326 / 500 | iteration 5 / 30 | Total Loss: 4.075876712799072 | KNN Loss: 3.0643651485443115 | BCE Loss: 1.0115116834640503\n",
      "Epoch 326 / 500 | iteration 10 / 30 | Total Loss: 4.094509124755859 | KNN Loss: 3.062394618988037 | BCE Loss: 1.0321147441864014\n",
      "Epoch 326 / 500 | iteration 15 / 30 | Total Loss: 4.020730495452881 | KNN Loss: 3.040238857269287 | BCE Loss: 0.9804916381835938\n",
      "Epoch 326 / 500 | iteration 20 / 30 | Total Loss: 4.081640243530273 | KNN Loss: 3.061516761779785 | BCE Loss: 1.0201234817504883\n",
      "Epoch 326 / 500 | iteration 25 / 30 | Total Loss: 4.108349800109863 | KNN Loss: 3.095756769180298 | BCE Loss: 1.0125932693481445\n",
      "Epoch 327 / 500 | iteration 0 / 30 | Total Loss: 4.085152626037598 | KNN Loss: 3.05637264251709 | BCE Loss: 1.028780221939087\n",
      "Epoch 327 / 500 | iteration 5 / 30 | Total Loss: 4.071722030639648 | KNN Loss: 3.061220169067383 | BCE Loss: 1.010501742362976\n",
      "Epoch 327 / 500 | iteration 10 / 30 | Total Loss: 4.127716541290283 | KNN Loss: 3.0924160480499268 | BCE Loss: 1.035300374031067\n",
      "Epoch 327 / 500 | iteration 15 / 30 | Total Loss: 4.105834007263184 | KNN Loss: 3.055138349533081 | BCE Loss: 1.0506956577301025\n",
      "Epoch 327 / 500 | iteration 20 / 30 | Total Loss: 4.055647373199463 | KNN Loss: 3.0618174076080322 | BCE Loss: 0.9938300251960754\n",
      "Epoch 327 / 500 | iteration 25 / 30 | Total Loss: 4.078917980194092 | KNN Loss: 3.082181215286255 | BCE Loss: 0.9967366456985474\n",
      "Epoch 328 / 500 | iteration 0 / 30 | Total Loss: 4.129593849182129 | KNN Loss: 3.1087570190429688 | BCE Loss: 1.0208370685577393\n",
      "Epoch 328 / 500 | iteration 5 / 30 | Total Loss: 4.06894588470459 | KNN Loss: 3.0557808876037598 | BCE Loss: 1.013164758682251\n",
      "Epoch 328 / 500 | iteration 10 / 30 | Total Loss: 4.071560382843018 | KNN Loss: 3.0668108463287354 | BCE Loss: 1.0047495365142822\n",
      "Epoch 328 / 500 | iteration 15 / 30 | Total Loss: 4.111175537109375 | KNN Loss: 3.0986831188201904 | BCE Loss: 1.0124926567077637\n",
      "Epoch 328 / 500 | iteration 20 / 30 | Total Loss: 4.090424060821533 | KNN Loss: 3.099641799926758 | BCE Loss: 0.9907823801040649\n",
      "Epoch 328 / 500 | iteration 25 / 30 | Total Loss: 4.047576904296875 | KNN Loss: 3.042410373687744 | BCE Loss: 1.0051662921905518\n",
      "Epoch 329 / 500 | iteration 0 / 30 | Total Loss: 4.081883430480957 | KNN Loss: 3.0700197219848633 | BCE Loss: 1.0118637084960938\n",
      "Epoch 329 / 500 | iteration 5 / 30 | Total Loss: 4.067737579345703 | KNN Loss: 3.079301357269287 | BCE Loss: 0.9884363412857056\n",
      "Epoch 329 / 500 | iteration 10 / 30 | Total Loss: 4.1161417961120605 | KNN Loss: 3.0645151138305664 | BCE Loss: 1.0516266822814941\n",
      "Epoch 329 / 500 | iteration 15 / 30 | Total Loss: 4.074162006378174 | KNN Loss: 3.080660343170166 | BCE Loss: 0.9935016632080078\n",
      "Epoch 329 / 500 | iteration 20 / 30 | Total Loss: 4.12342643737793 | KNN Loss: 3.0566651821136475 | BCE Loss: 1.0667613744735718\n",
      "Epoch 329 / 500 | iteration 25 / 30 | Total Loss: 4.070295810699463 | KNN Loss: 3.0674798488616943 | BCE Loss: 1.0028159618377686\n",
      "Epoch 330 / 500 | iteration 0 / 30 | Total Loss: 4.124873161315918 | KNN Loss: 3.0897693634033203 | BCE Loss: 1.0351035594940186\n",
      "Epoch 330 / 500 | iteration 5 / 30 | Total Loss: 4.05541467666626 | KNN Loss: 3.051856517791748 | BCE Loss: 1.0035581588745117\n",
      "Epoch 330 / 500 | iteration 10 / 30 | Total Loss: 4.0517730712890625 | KNN Loss: 3.0457401275634766 | BCE Loss: 1.0060327053070068\n",
      "Epoch 330 / 500 | iteration 15 / 30 | Total Loss: 4.0968756675720215 | KNN Loss: 3.0794246196746826 | BCE Loss: 1.0174509286880493\n",
      "Epoch 330 / 500 | iteration 20 / 30 | Total Loss: 4.156064033508301 | KNN Loss: 3.0875377655029297 | BCE Loss: 1.0685265064239502\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 330 / 500 | iteration 25 / 30 | Total Loss: 4.119354248046875 | KNN Loss: 3.1064727306365967 | BCE Loss: 1.0128815174102783\n",
      "Epoch 331 / 500 | iteration 0 / 30 | Total Loss: 4.112932205200195 | KNN Loss: 3.095289468765259 | BCE Loss: 1.0176424980163574\n",
      "Epoch 331 / 500 | iteration 5 / 30 | Total Loss: 4.0741190910339355 | KNN Loss: 3.0485615730285645 | BCE Loss: 1.025557518005371\n",
      "Epoch 331 / 500 | iteration 10 / 30 | Total Loss: 4.160473346710205 | KNN Loss: 3.0666799545288086 | BCE Loss: 1.0937933921813965\n",
      "Epoch 331 / 500 | iteration 15 / 30 | Total Loss: 4.115240573883057 | KNN Loss: 3.058366537094116 | BCE Loss: 1.05687415599823\n",
      "Epoch 331 / 500 | iteration 20 / 30 | Total Loss: 4.138321876525879 | KNN Loss: 3.10361385345459 | BCE Loss: 1.0347082614898682\n",
      "Epoch 331 / 500 | iteration 25 / 30 | Total Loss: 4.082952976226807 | KNN Loss: 3.0658583641052246 | BCE Loss: 1.017094612121582\n",
      "Epoch 332 / 500 | iteration 0 / 30 | Total Loss: 4.059178352355957 | KNN Loss: 3.0628654956817627 | BCE Loss: 0.9963128566741943\n",
      "Epoch 332 / 500 | iteration 5 / 30 | Total Loss: 4.07947301864624 | KNN Loss: 3.076524257659912 | BCE Loss: 1.0029487609863281\n",
      "Epoch 332 / 500 | iteration 10 / 30 | Total Loss: 4.05082368850708 | KNN Loss: 3.056630849838257 | BCE Loss: 0.9941926598548889\n",
      "Epoch 332 / 500 | iteration 15 / 30 | Total Loss: 4.040728569030762 | KNN Loss: 3.014150857925415 | BCE Loss: 1.0265777111053467\n",
      "Epoch 332 / 500 | iteration 20 / 30 | Total Loss: 4.0634355545043945 | KNN Loss: 3.024851083755493 | BCE Loss: 1.0385844707489014\n",
      "Epoch 332 / 500 | iteration 25 / 30 | Total Loss: 4.07457971572876 | KNN Loss: 3.045426368713379 | BCE Loss: 1.0291533470153809\n",
      "Epoch 333 / 500 | iteration 0 / 30 | Total Loss: 4.069940567016602 | KNN Loss: 3.06970477104187 | BCE Loss: 1.0002360343933105\n",
      "Epoch 333 / 500 | iteration 5 / 30 | Total Loss: 4.077626705169678 | KNN Loss: 3.0607337951660156 | BCE Loss: 1.016892910003662\n",
      "Epoch 333 / 500 | iteration 10 / 30 | Total Loss: 4.121281147003174 | KNN Loss: 3.063258647918701 | BCE Loss: 1.0580224990844727\n",
      "Epoch 333 / 500 | iteration 15 / 30 | Total Loss: 4.0471343994140625 | KNN Loss: 3.036440849304199 | BCE Loss: 1.0106935501098633\n",
      "Epoch 333 / 500 | iteration 20 / 30 | Total Loss: 4.0510969161987305 | KNN Loss: 3.039011240005493 | BCE Loss: 1.0120855569839478\n",
      "Epoch 333 / 500 | iteration 25 / 30 | Total Loss: 4.106875896453857 | KNN Loss: 3.0863986015319824 | BCE Loss: 1.020477294921875\n",
      "Epoch 334 / 500 | iteration 0 / 30 | Total Loss: 4.066854953765869 | KNN Loss: 3.0378174781799316 | BCE Loss: 1.029037594795227\n",
      "Epoch 334 / 500 | iteration 5 / 30 | Total Loss: 4.061094284057617 | KNN Loss: 3.0447347164154053 | BCE Loss: 1.0163596868515015\n",
      "Epoch 334 / 500 | iteration 10 / 30 | Total Loss: 4.082906246185303 | KNN Loss: 3.067639112472534 | BCE Loss: 1.0152671337127686\n",
      "Epoch 334 / 500 | iteration 15 / 30 | Total Loss: 4.118756294250488 | KNN Loss: 3.1096091270446777 | BCE Loss: 1.0091472864151\n",
      "Epoch 334 / 500 | iteration 20 / 30 | Total Loss: 4.068466663360596 | KNN Loss: 3.0776569843292236 | BCE Loss: 0.9908095002174377\n",
      "Epoch 334 / 500 | iteration 25 / 30 | Total Loss: 4.090371131896973 | KNN Loss: 3.063814640045166 | BCE Loss: 1.0265562534332275\n",
      "Epoch 335 / 500 | iteration 0 / 30 | Total Loss: 4.048905372619629 | KNN Loss: 3.0455379486083984 | BCE Loss: 1.0033671855926514\n",
      "Epoch 335 / 500 | iteration 5 / 30 | Total Loss: 4.081752300262451 | KNN Loss: 3.0837297439575195 | BCE Loss: 0.9980223774909973\n",
      "Epoch 335 / 500 | iteration 10 / 30 | Total Loss: 4.097572326660156 | KNN Loss: 3.0480566024780273 | BCE Loss: 1.049515962600708\n",
      "Epoch 335 / 500 | iteration 15 / 30 | Total Loss: 4.085890769958496 | KNN Loss: 3.0344974994659424 | BCE Loss: 1.0513930320739746\n",
      "Epoch 335 / 500 | iteration 20 / 30 | Total Loss: 4.125866413116455 | KNN Loss: 3.0810389518737793 | BCE Loss: 1.0448273420333862\n",
      "Epoch 335 / 500 | iteration 25 / 30 | Total Loss: 4.076052188873291 | KNN Loss: 3.0464630126953125 | BCE Loss: 1.0295891761779785\n",
      "Epoch   336: reducing learning rate of group 0 to 3.9896e-06.\n",
      "Epoch 336 / 500 | iteration 0 / 30 | Total Loss: 4.088675498962402 | KNN Loss: 3.0662612915039062 | BCE Loss: 1.0224143266677856\n",
      "Epoch 336 / 500 | iteration 5 / 30 | Total Loss: 4.074146270751953 | KNN Loss: 3.0384037494659424 | BCE Loss: 1.0357425212860107\n",
      "Epoch 336 / 500 | iteration 10 / 30 | Total Loss: 4.07780122756958 | KNN Loss: 3.0711822509765625 | BCE Loss: 1.006618857383728\n",
      "Epoch 336 / 500 | iteration 15 / 30 | Total Loss: 4.063209056854248 | KNN Loss: 3.0699193477630615 | BCE Loss: 0.9932898283004761\n",
      "Epoch 336 / 500 | iteration 20 / 30 | Total Loss: 4.091672420501709 | KNN Loss: 3.0857858657836914 | BCE Loss: 1.005886435508728\n",
      "Epoch 336 / 500 | iteration 25 / 30 | Total Loss: 4.096162796020508 | KNN Loss: 3.0642168521881104 | BCE Loss: 1.0319461822509766\n",
      "Epoch 337 / 500 | iteration 0 / 30 | Total Loss: 4.065607070922852 | KNN Loss: 3.0582592487335205 | BCE Loss: 1.0073479413986206\n",
      "Epoch 337 / 500 | iteration 5 / 30 | Total Loss: 4.101484775543213 | KNN Loss: 3.0893185138702393 | BCE Loss: 1.0121663808822632\n",
      "Epoch 337 / 500 | iteration 10 / 30 | Total Loss: 4.072328567504883 | KNN Loss: 3.050424337387085 | BCE Loss: 1.021904468536377\n",
      "Epoch 337 / 500 | iteration 15 / 30 | Total Loss: 4.086406707763672 | KNN Loss: 3.07438063621521 | BCE Loss: 1.012026309967041\n",
      "Epoch 337 / 500 | iteration 20 / 30 | Total Loss: 4.087194919586182 | KNN Loss: 3.0539400577545166 | BCE Loss: 1.0332547426223755\n",
      "Epoch 337 / 500 | iteration 25 / 30 | Total Loss: 4.103435516357422 | KNN Loss: 3.0823609828948975 | BCE Loss: 1.0210747718811035\n",
      "Epoch 338 / 500 | iteration 0 / 30 | Total Loss: 4.094954490661621 | KNN Loss: 3.084413766860962 | BCE Loss: 1.0105407238006592\n",
      "Epoch 338 / 500 | iteration 5 / 30 | Total Loss: 4.148958683013916 | KNN Loss: 3.108504295349121 | BCE Loss: 1.040454387664795\n",
      "Epoch 338 / 500 | iteration 10 / 30 | Total Loss: 4.091858863830566 | KNN Loss: 3.06733775138855 | BCE Loss: 1.0245208740234375\n",
      "Epoch 338 / 500 | iteration 15 / 30 | Total Loss: 4.070784568786621 | KNN Loss: 3.05489444732666 | BCE Loss: 1.01589035987854\n",
      "Epoch 338 / 500 | iteration 20 / 30 | Total Loss: 4.053934574127197 | KNN Loss: 3.0370922088623047 | BCE Loss: 1.0168423652648926\n",
      "Epoch 338 / 500 | iteration 25 / 30 | Total Loss: 4.0976362228393555 | KNN Loss: 3.0918025970458984 | BCE Loss: 1.0058338642120361\n",
      "Epoch 339 / 500 | iteration 0 / 30 | Total Loss: 4.109677314758301 | KNN Loss: 3.075335741043091 | BCE Loss: 1.03434157371521\n",
      "Epoch 339 / 500 | iteration 5 / 30 | Total Loss: 4.090566635131836 | KNN Loss: 3.0603365898132324 | BCE Loss: 1.0302302837371826\n",
      "Epoch 339 / 500 | iteration 10 / 30 | Total Loss: 4.062224388122559 | KNN Loss: 3.0481631755828857 | BCE Loss: 1.0140612125396729\n",
      "Epoch 339 / 500 | iteration 15 / 30 | Total Loss: 4.095110893249512 | KNN Loss: 3.0839316844940186 | BCE Loss: 1.0111794471740723\n",
      "Epoch 339 / 500 | iteration 20 / 30 | Total Loss: 4.080826759338379 | KNN Loss: 3.0713722705841064 | BCE Loss: 1.0094544887542725\n",
      "Epoch 339 / 500 | iteration 25 / 30 | Total Loss: 4.061484336853027 | KNN Loss: 3.064359426498413 | BCE Loss: 0.9971248507499695\n",
      "Epoch 340 / 500 | iteration 0 / 30 | Total Loss: 4.136728763580322 | KNN Loss: 3.1026463508605957 | BCE Loss: 1.0340824127197266\n",
      "Epoch 340 / 500 | iteration 5 / 30 | Total Loss: 4.080908298492432 | KNN Loss: 3.070406675338745 | BCE Loss: 1.0105016231536865\n",
      "Epoch 340 / 500 | iteration 10 / 30 | Total Loss: 4.101188659667969 | KNN Loss: 3.085649251937866 | BCE Loss: 1.0155391693115234\n",
      "Epoch 340 / 500 | iteration 15 / 30 | Total Loss: 4.082376480102539 | KNN Loss: 3.0652403831481934 | BCE Loss: 1.0171359777450562\n",
      "Epoch 340 / 500 | iteration 20 / 30 | Total Loss: 4.072339057922363 | KNN Loss: 3.0713887214660645 | BCE Loss: 1.0009502172470093\n",
      "Epoch 340 / 500 | iteration 25 / 30 | Total Loss: 4.10892391204834 | KNN Loss: 3.060037612915039 | BCE Loss: 1.0488865375518799\n",
      "Epoch 341 / 500 | iteration 0 / 30 | Total Loss: 4.167330265045166 | KNN Loss: 3.128392219543457 | BCE Loss: 1.0389381647109985\n",
      "Epoch 341 / 500 | iteration 5 / 30 | Total Loss: 4.100883483886719 | KNN Loss: 3.0858609676361084 | BCE Loss: 1.0150222778320312\n",
      "Epoch 341 / 500 | iteration 10 / 30 | Total Loss: 4.082813262939453 | KNN Loss: 3.0297725200653076 | BCE Loss: 1.0530407428741455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 341 / 500 | iteration 15 / 30 | Total Loss: 4.106731414794922 | KNN Loss: 3.081313371658325 | BCE Loss: 1.0254181623458862\n",
      "Epoch 341 / 500 | iteration 20 / 30 | Total Loss: 4.084405422210693 | KNN Loss: 3.052861213684082 | BCE Loss: 1.0315440893173218\n",
      "Epoch 341 / 500 | iteration 25 / 30 | Total Loss: 4.122188091278076 | KNN Loss: 3.0916287899017334 | BCE Loss: 1.0305593013763428\n",
      "Epoch 342 / 500 | iteration 0 / 30 | Total Loss: 4.083104610443115 | KNN Loss: 3.0706849098205566 | BCE Loss: 1.012419581413269\n",
      "Epoch 342 / 500 | iteration 5 / 30 | Total Loss: 4.105259895324707 | KNN Loss: 3.079033613204956 | BCE Loss: 1.02622652053833\n",
      "Epoch 342 / 500 | iteration 10 / 30 | Total Loss: 4.050450325012207 | KNN Loss: 3.0529236793518066 | BCE Loss: 0.9975264072418213\n",
      "Epoch 342 / 500 | iteration 15 / 30 | Total Loss: 4.088236331939697 | KNN Loss: 3.0754010677337646 | BCE Loss: 1.0128352642059326\n",
      "Epoch 342 / 500 | iteration 20 / 30 | Total Loss: 4.0567851066589355 | KNN Loss: 3.045332193374634 | BCE Loss: 1.0114529132843018\n",
      "Epoch 342 / 500 | iteration 25 / 30 | Total Loss: 4.046402454376221 | KNN Loss: 3.057724952697754 | BCE Loss: 0.9886776804924011\n",
      "Epoch 343 / 500 | iteration 0 / 30 | Total Loss: 4.132763385772705 | KNN Loss: 3.077882766723633 | BCE Loss: 1.0548804998397827\n",
      "Epoch 343 / 500 | iteration 5 / 30 | Total Loss: 4.074397087097168 | KNN Loss: 3.066181182861328 | BCE Loss: 1.0082156658172607\n",
      "Epoch 343 / 500 | iteration 10 / 30 | Total Loss: 4.0420732498168945 | KNN Loss: 3.0304925441741943 | BCE Loss: 1.0115807056427002\n",
      "Epoch 343 / 500 | iteration 15 / 30 | Total Loss: 4.090831279754639 | KNN Loss: 3.070155143737793 | BCE Loss: 1.0206761360168457\n",
      "Epoch 343 / 500 | iteration 20 / 30 | Total Loss: 4.056896686553955 | KNN Loss: 3.017411231994629 | BCE Loss: 1.0394854545593262\n",
      "Epoch 343 / 500 | iteration 25 / 30 | Total Loss: 4.10777473449707 | KNN Loss: 3.064939498901367 | BCE Loss: 1.0428353548049927\n",
      "Epoch 344 / 500 | iteration 0 / 30 | Total Loss: 4.069677352905273 | KNN Loss: 3.0481271743774414 | BCE Loss: 1.0215504169464111\n",
      "Epoch 344 / 500 | iteration 5 / 30 | Total Loss: 4.029755115509033 | KNN Loss: 3.02382755279541 | BCE Loss: 1.005927562713623\n",
      "Epoch 344 / 500 | iteration 10 / 30 | Total Loss: 4.150217056274414 | KNN Loss: 3.1198229789733887 | BCE Loss: 1.0303938388824463\n",
      "Epoch 344 / 500 | iteration 15 / 30 | Total Loss: 4.0781989097595215 | KNN Loss: 3.062525987625122 | BCE Loss: 1.0156729221343994\n",
      "Epoch 344 / 500 | iteration 20 / 30 | Total Loss: 4.076707363128662 | KNN Loss: 3.067762613296509 | BCE Loss: 1.0089447498321533\n",
      "Epoch 344 / 500 | iteration 25 / 30 | Total Loss: 4.050476551055908 | KNN Loss: 3.071617841720581 | BCE Loss: 0.9788587093353271\n",
      "Epoch 345 / 500 | iteration 0 / 30 | Total Loss: 4.071110725402832 | KNN Loss: 3.039839267730713 | BCE Loss: 1.03127121925354\n",
      "Epoch 345 / 500 | iteration 5 / 30 | Total Loss: 4.08882999420166 | KNN Loss: 3.1005587577819824 | BCE Loss: 0.9882713556289673\n",
      "Epoch 345 / 500 | iteration 10 / 30 | Total Loss: 4.119930267333984 | KNN Loss: 3.0803494453430176 | BCE Loss: 1.039581060409546\n",
      "Epoch 345 / 500 | iteration 15 / 30 | Total Loss: 4.134847640991211 | KNN Loss: 3.0984582901000977 | BCE Loss: 1.0363893508911133\n",
      "Epoch 345 / 500 | iteration 20 / 30 | Total Loss: 4.051931858062744 | KNN Loss: 3.0553324222564697 | BCE Loss: 0.9965994954109192\n",
      "Epoch 345 / 500 | iteration 25 / 30 | Total Loss: 4.096843719482422 | KNN Loss: 3.083246946334839 | BCE Loss: 1.0135968923568726\n",
      "Epoch 346 / 500 | iteration 0 / 30 | Total Loss: 4.0450520515441895 | KNN Loss: 3.0113909244537354 | BCE Loss: 1.0336610078811646\n",
      "Epoch 346 / 500 | iteration 5 / 30 | Total Loss: 4.071412563323975 | KNN Loss: 3.0365753173828125 | BCE Loss: 1.0348373651504517\n",
      "Epoch 346 / 500 | iteration 10 / 30 | Total Loss: 4.13306999206543 | KNN Loss: 3.107712745666504 | BCE Loss: 1.0253571271896362\n",
      "Epoch 346 / 500 | iteration 15 / 30 | Total Loss: 4.131932258605957 | KNN Loss: 3.0894787311553955 | BCE Loss: 1.0424537658691406\n",
      "Epoch 346 / 500 | iteration 20 / 30 | Total Loss: 4.084402084350586 | KNN Loss: 3.0974764823913574 | BCE Loss: 0.9869253635406494\n",
      "Epoch 346 / 500 | iteration 25 / 30 | Total Loss: 4.054754734039307 | KNN Loss: 3.0245163440704346 | BCE Loss: 1.030238389968872\n",
      "Epoch   347: reducing learning rate of group 0 to 2.7927e-06.\n",
      "Epoch 347 / 500 | iteration 0 / 30 | Total Loss: 4.086887359619141 | KNN Loss: 3.0525879859924316 | BCE Loss: 1.0342991352081299\n",
      "Epoch 347 / 500 | iteration 5 / 30 | Total Loss: 4.074319839477539 | KNN Loss: 3.0455381870269775 | BCE Loss: 1.028781533241272\n",
      "Epoch 347 / 500 | iteration 10 / 30 | Total Loss: 4.101497650146484 | KNN Loss: 3.0700929164886475 | BCE Loss: 1.031404733657837\n",
      "Epoch 347 / 500 | iteration 15 / 30 | Total Loss: 4.0724196434021 | KNN Loss: 3.053835153579712 | BCE Loss: 1.0185843706130981\n",
      "Epoch 347 / 500 | iteration 20 / 30 | Total Loss: 4.128846168518066 | KNN Loss: 3.077855348587036 | BCE Loss: 1.0509909391403198\n",
      "Epoch 347 / 500 | iteration 25 / 30 | Total Loss: 4.110693454742432 | KNN Loss: 3.0794570446014404 | BCE Loss: 1.0312365293502808\n",
      "Epoch 348 / 500 | iteration 0 / 30 | Total Loss: 4.080778121948242 | KNN Loss: 3.049453020095825 | BCE Loss: 1.031325101852417\n",
      "Epoch 348 / 500 | iteration 5 / 30 | Total Loss: 4.092845916748047 | KNN Loss: 3.0584075450897217 | BCE Loss: 1.0344383716583252\n",
      "Epoch 348 / 500 | iteration 10 / 30 | Total Loss: 4.132976055145264 | KNN Loss: 3.1132681369781494 | BCE Loss: 1.0197079181671143\n",
      "Epoch 348 / 500 | iteration 15 / 30 | Total Loss: 4.074665546417236 | KNN Loss: 3.0713376998901367 | BCE Loss: 1.0033279657363892\n",
      "Epoch 348 / 500 | iteration 20 / 30 | Total Loss: 4.139960765838623 | KNN Loss: 3.1075220108032227 | BCE Loss: 1.0324387550354004\n",
      "Epoch 348 / 500 | iteration 25 / 30 | Total Loss: 4.081085205078125 | KNN Loss: 3.0838475227355957 | BCE Loss: 0.9972375631332397\n",
      "Epoch 349 / 500 | iteration 0 / 30 | Total Loss: 4.106429100036621 | KNN Loss: 3.0842809677124023 | BCE Loss: 1.0221481323242188\n",
      "Epoch 349 / 500 | iteration 5 / 30 | Total Loss: 4.074402809143066 | KNN Loss: 3.0515246391296387 | BCE Loss: 1.0228781700134277\n",
      "Epoch 349 / 500 | iteration 10 / 30 | Total Loss: 4.047579765319824 | KNN Loss: 3.051105499267578 | BCE Loss: 0.9964742660522461\n",
      "Epoch 349 / 500 | iteration 15 / 30 | Total Loss: 4.042828559875488 | KNN Loss: 3.0670723915100098 | BCE Loss: 0.9757559895515442\n",
      "Epoch 349 / 500 | iteration 20 / 30 | Total Loss: 4.071822166442871 | KNN Loss: 3.047253370285034 | BCE Loss: 1.0245685577392578\n",
      "Epoch 349 / 500 | iteration 25 / 30 | Total Loss: 4.104276657104492 | KNN Loss: 3.073472261428833 | BCE Loss: 1.03080415725708\n",
      "Epoch 350 / 500 | iteration 0 / 30 | Total Loss: 4.069026470184326 | KNN Loss: 3.0605030059814453 | BCE Loss: 1.0085233449935913\n",
      "Epoch 350 / 500 | iteration 5 / 30 | Total Loss: 4.091374397277832 | KNN Loss: 3.0686447620391846 | BCE Loss: 1.0227298736572266\n",
      "Epoch 350 / 500 | iteration 10 / 30 | Total Loss: 4.086370944976807 | KNN Loss: 3.085956335067749 | BCE Loss: 1.000414490699768\n",
      "Epoch 350 / 500 | iteration 15 / 30 | Total Loss: 4.055303573608398 | KNN Loss: 3.032876968383789 | BCE Loss: 1.0224263668060303\n",
      "Epoch 350 / 500 | iteration 20 / 30 | Total Loss: 4.11735725402832 | KNN Loss: 3.08685040473938 | BCE Loss: 1.0305067300796509\n",
      "Epoch 350 / 500 | iteration 25 / 30 | Total Loss: 4.057427883148193 | KNN Loss: 3.0430164337158203 | BCE Loss: 1.014411449432373\n",
      "Epoch 351 / 500 | iteration 0 / 30 | Total Loss: 4.105759620666504 | KNN Loss: 3.0804340839385986 | BCE Loss: 1.0253257751464844\n",
      "Epoch 351 / 500 | iteration 5 / 30 | Total Loss: 4.064767360687256 | KNN Loss: 3.0597262382507324 | BCE Loss: 1.005041241645813\n",
      "Epoch 351 / 500 | iteration 10 / 30 | Total Loss: 4.123723983764648 | KNN Loss: 3.1029062271118164 | BCE Loss: 1.020817756652832\n",
      "Epoch 351 / 500 | iteration 15 / 30 | Total Loss: 4.15946102142334 | KNN Loss: 3.113661527633667 | BCE Loss: 1.0457994937896729\n",
      "Epoch 351 / 500 | iteration 20 / 30 | Total Loss: 4.070377349853516 | KNN Loss: 3.052082061767578 | BCE Loss: 1.0182950496673584\n",
      "Epoch 351 / 500 | iteration 25 / 30 | Total Loss: 4.101968288421631 | KNN Loss: 3.0843868255615234 | BCE Loss: 1.017581582069397\n",
      "Epoch 352 / 500 | iteration 0 / 30 | Total Loss: 4.095241546630859 | KNN Loss: 3.0759475231170654 | BCE Loss: 1.0192937850952148\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 352 / 500 | iteration 5 / 30 | Total Loss: 4.077304840087891 | KNN Loss: 3.067039966583252 | BCE Loss: 1.0102648735046387\n",
      "Epoch 352 / 500 | iteration 10 / 30 | Total Loss: 4.093416690826416 | KNN Loss: 3.0683653354644775 | BCE Loss: 1.0250513553619385\n",
      "Epoch 352 / 500 | iteration 15 / 30 | Total Loss: 4.045453071594238 | KNN Loss: 3.0505919456481934 | BCE Loss: 0.9948611259460449\n",
      "Epoch 352 / 500 | iteration 20 / 30 | Total Loss: 4.085879802703857 | KNN Loss: 3.0567078590393066 | BCE Loss: 1.0291719436645508\n",
      "Epoch 352 / 500 | iteration 25 / 30 | Total Loss: 4.075547695159912 | KNN Loss: 3.0738303661346436 | BCE Loss: 1.0017173290252686\n",
      "Epoch 353 / 500 | iteration 0 / 30 | Total Loss: 4.078812122344971 | KNN Loss: 3.053678035736084 | BCE Loss: 1.0251339673995972\n",
      "Epoch 353 / 500 | iteration 5 / 30 | Total Loss: 4.125414848327637 | KNN Loss: 3.0956194400787354 | BCE Loss: 1.0297956466674805\n",
      "Epoch 353 / 500 | iteration 10 / 30 | Total Loss: 4.072569847106934 | KNN Loss: 3.051170587539673 | BCE Loss: 1.0213993787765503\n",
      "Epoch 353 / 500 | iteration 15 / 30 | Total Loss: 4.052520751953125 | KNN Loss: 3.027092695236206 | BCE Loss: 1.0254281759262085\n",
      "Epoch 353 / 500 | iteration 20 / 30 | Total Loss: 4.074958324432373 | KNN Loss: 3.0513808727264404 | BCE Loss: 1.0235775709152222\n",
      "Epoch 353 / 500 | iteration 25 / 30 | Total Loss: 4.088509559631348 | KNN Loss: 3.043684244155884 | BCE Loss: 1.0448250770568848\n",
      "Epoch 354 / 500 | iteration 0 / 30 | Total Loss: 4.102652072906494 | KNN Loss: 3.1015899181365967 | BCE Loss: 1.0010621547698975\n",
      "Epoch 354 / 500 | iteration 5 / 30 | Total Loss: 4.148582458496094 | KNN Loss: 3.079164981842041 | BCE Loss: 1.0694175958633423\n",
      "Epoch 354 / 500 | iteration 10 / 30 | Total Loss: 4.104738712310791 | KNN Loss: 3.077725410461426 | BCE Loss: 1.0270134210586548\n",
      "Epoch 354 / 500 | iteration 15 / 30 | Total Loss: 4.086115837097168 | KNN Loss: 3.097418785095215 | BCE Loss: 0.9886971712112427\n",
      "Epoch 354 / 500 | iteration 20 / 30 | Total Loss: 4.149871349334717 | KNN Loss: 3.085428476333618 | BCE Loss: 1.0644429922103882\n",
      "Epoch 354 / 500 | iteration 25 / 30 | Total Loss: 4.0870537757873535 | KNN Loss: 3.0819764137268066 | BCE Loss: 1.0050773620605469\n",
      "Epoch 355 / 500 | iteration 0 / 30 | Total Loss: 4.067328929901123 | KNN Loss: 3.0679779052734375 | BCE Loss: 0.9993510246276855\n",
      "Epoch 355 / 500 | iteration 5 / 30 | Total Loss: 4.073760032653809 | KNN Loss: 3.0585882663726807 | BCE Loss: 1.015171766281128\n",
      "Epoch 355 / 500 | iteration 10 / 30 | Total Loss: 4.106761455535889 | KNN Loss: 3.080831289291382 | BCE Loss: 1.0259300470352173\n",
      "Epoch 355 / 500 | iteration 15 / 30 | Total Loss: 4.067060470581055 | KNN Loss: 3.0345191955566406 | BCE Loss: 1.0325415134429932\n",
      "Epoch 355 / 500 | iteration 20 / 30 | Total Loss: 4.112911224365234 | KNN Loss: 3.0919792652130127 | BCE Loss: 1.0209321975708008\n",
      "Epoch 355 / 500 | iteration 25 / 30 | Total Loss: 4.079118728637695 | KNN Loss: 3.054908275604248 | BCE Loss: 1.0242102146148682\n",
      "Epoch 356 / 500 | iteration 0 / 30 | Total Loss: 4.134148597717285 | KNN Loss: 3.0901172161102295 | BCE Loss: 1.0440313816070557\n",
      "Epoch 356 / 500 | iteration 5 / 30 | Total Loss: 4.079667091369629 | KNN Loss: 3.037489891052246 | BCE Loss: 1.042177438735962\n",
      "Epoch 356 / 500 | iteration 10 / 30 | Total Loss: 4.087268352508545 | KNN Loss: 3.0624806880950928 | BCE Loss: 1.0247876644134521\n",
      "Epoch 356 / 500 | iteration 15 / 30 | Total Loss: 4.077310562133789 | KNN Loss: 3.0670690536499023 | BCE Loss: 1.0102413892745972\n",
      "Epoch 356 / 500 | iteration 20 / 30 | Total Loss: 4.112541198730469 | KNN Loss: 3.0907797813415527 | BCE Loss: 1.0217616558074951\n",
      "Epoch 356 / 500 | iteration 25 / 30 | Total Loss: 4.171842098236084 | KNN Loss: 3.1271088123321533 | BCE Loss: 1.0447332859039307\n",
      "Epoch 357 / 500 | iteration 0 / 30 | Total Loss: 4.080541133880615 | KNN Loss: 3.0555336475372314 | BCE Loss: 1.0250076055526733\n",
      "Epoch 357 / 500 | iteration 5 / 30 | Total Loss: 4.099395751953125 | KNN Loss: 3.0955588817596436 | BCE Loss: 1.0038371086120605\n",
      "Epoch 357 / 500 | iteration 10 / 30 | Total Loss: 4.116816997528076 | KNN Loss: 3.090496063232422 | BCE Loss: 1.0263209342956543\n",
      "Epoch 357 / 500 | iteration 15 / 30 | Total Loss: 4.087140083312988 | KNN Loss: 3.0519540309906006 | BCE Loss: 1.0351858139038086\n",
      "Epoch 357 / 500 | iteration 20 / 30 | Total Loss: 4.088837623596191 | KNN Loss: 3.0744247436523438 | BCE Loss: 1.0144128799438477\n",
      "Epoch 357 / 500 | iteration 25 / 30 | Total Loss: 4.066584587097168 | KNN Loss: 3.0498833656311035 | BCE Loss: 1.0167014598846436\n",
      "Epoch   358: reducing learning rate of group 0 to 1.9549e-06.\n",
      "Epoch 358 / 500 | iteration 0 / 30 | Total Loss: 4.04936408996582 | KNN Loss: 3.041008710861206 | BCE Loss: 1.0083551406860352\n",
      "Epoch 358 / 500 | iteration 5 / 30 | Total Loss: 4.132657527923584 | KNN Loss: 3.078122854232788 | BCE Loss: 1.0545345544815063\n",
      "Epoch 358 / 500 | iteration 10 / 30 | Total Loss: 4.125378608703613 | KNN Loss: 3.076768398284912 | BCE Loss: 1.0486102104187012\n",
      "Epoch 358 / 500 | iteration 15 / 30 | Total Loss: 4.040865898132324 | KNN Loss: 3.0390570163726807 | BCE Loss: 1.0018086433410645\n",
      "Epoch 358 / 500 | iteration 20 / 30 | Total Loss: 4.099430561065674 | KNN Loss: 3.0960090160369873 | BCE Loss: 1.0034215450286865\n",
      "Epoch 358 / 500 | iteration 25 / 30 | Total Loss: 4.066348552703857 | KNN Loss: 3.055262804031372 | BCE Loss: 1.0110857486724854\n",
      "Epoch 359 / 500 | iteration 0 / 30 | Total Loss: 4.049869060516357 | KNN Loss: 3.0436692237854004 | BCE Loss: 1.006199836730957\n",
      "Epoch 359 / 500 | iteration 5 / 30 | Total Loss: 4.085644245147705 | KNN Loss: 3.079543113708496 | BCE Loss: 1.006101131439209\n",
      "Epoch 359 / 500 | iteration 10 / 30 | Total Loss: 4.11622428894043 | KNN Loss: 3.080260753631592 | BCE Loss: 1.035963773727417\n",
      "Epoch 359 / 500 | iteration 15 / 30 | Total Loss: 4.0975341796875 | KNN Loss: 3.051780939102173 | BCE Loss: 1.045753002166748\n",
      "Epoch 359 / 500 | iteration 20 / 30 | Total Loss: 4.067571640014648 | KNN Loss: 3.060744524002075 | BCE Loss: 1.0068268775939941\n",
      "Epoch 359 / 500 | iteration 25 / 30 | Total Loss: 4.08740758895874 | KNN Loss: 3.0768580436706543 | BCE Loss: 1.010549545288086\n",
      "Epoch 360 / 500 | iteration 0 / 30 | Total Loss: 4.086096286773682 | KNN Loss: 3.0617027282714844 | BCE Loss: 1.0243935585021973\n",
      "Epoch 360 / 500 | iteration 5 / 30 | Total Loss: 4.10765266418457 | KNN Loss: 3.0740084648132324 | BCE Loss: 1.033644199371338\n",
      "Epoch 360 / 500 | iteration 10 / 30 | Total Loss: 4.059447288513184 | KNN Loss: 3.0619614124298096 | BCE Loss: 0.9974859356880188\n",
      "Epoch 360 / 500 | iteration 15 / 30 | Total Loss: 4.077935695648193 | KNN Loss: 3.083625078201294 | BCE Loss: 0.9943107962608337\n",
      "Epoch 360 / 500 | iteration 20 / 30 | Total Loss: 4.0615620613098145 | KNN Loss: 3.0448598861694336 | BCE Loss: 1.0167022943496704\n",
      "Epoch 360 / 500 | iteration 25 / 30 | Total Loss: 4.095608711242676 | KNN Loss: 3.063079833984375 | BCE Loss: 1.0325286388397217\n",
      "Epoch 361 / 500 | iteration 0 / 30 | Total Loss: 4.054595947265625 | KNN Loss: 3.0377964973449707 | BCE Loss: 1.0167992115020752\n",
      "Epoch 361 / 500 | iteration 5 / 30 | Total Loss: 4.078371047973633 | KNN Loss: 3.062636375427246 | BCE Loss: 1.0157346725463867\n",
      "Epoch 361 / 500 | iteration 10 / 30 | Total Loss: 4.058777809143066 | KNN Loss: 3.038882255554199 | BCE Loss: 1.0198955535888672\n",
      "Epoch 361 / 500 | iteration 15 / 30 | Total Loss: 4.11118221282959 | KNN Loss: 3.0571062564849854 | BCE Loss: 1.0540757179260254\n",
      "Epoch 361 / 500 | iteration 20 / 30 | Total Loss: 4.090726852416992 | KNN Loss: 3.098944902420044 | BCE Loss: 0.9917817115783691\n",
      "Epoch 361 / 500 | iteration 25 / 30 | Total Loss: 4.088957786560059 | KNN Loss: 3.073216438293457 | BCE Loss: 1.0157413482666016\n",
      "Epoch 362 / 500 | iteration 0 / 30 | Total Loss: 4.071621417999268 | KNN Loss: 3.0520083904266357 | BCE Loss: 1.0196131467819214\n",
      "Epoch 362 / 500 | iteration 5 / 30 | Total Loss: 4.0585784912109375 | KNN Loss: 3.047447681427002 | BCE Loss: 1.0111310482025146\n",
      "Epoch 362 / 500 | iteration 10 / 30 | Total Loss: 4.104935646057129 | KNN Loss: 3.0843350887298584 | BCE Loss: 1.0206005573272705\n",
      "Epoch 362 / 500 | iteration 15 / 30 | Total Loss: 4.048220634460449 | KNN Loss: 3.0547497272491455 | BCE Loss: 0.9934707880020142\n",
      "Epoch 362 / 500 | iteration 20 / 30 | Total Loss: 4.10608434677124 | KNN Loss: 3.0990190505981445 | BCE Loss: 1.0070651769638062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 362 / 500 | iteration 25 / 30 | Total Loss: 4.123946189880371 | KNN Loss: 3.072725534439087 | BCE Loss: 1.0512207746505737\n",
      "Epoch 363 / 500 | iteration 0 / 30 | Total Loss: 4.049709320068359 | KNN Loss: 3.045161485671997 | BCE Loss: 1.0045475959777832\n",
      "Epoch 363 / 500 | iteration 5 / 30 | Total Loss: 4.064548492431641 | KNN Loss: 3.035670280456543 | BCE Loss: 1.028878092765808\n",
      "Epoch 363 / 500 | iteration 10 / 30 | Total Loss: 4.082615852355957 | KNN Loss: 3.053337335586548 | BCE Loss: 1.0292786359786987\n",
      "Epoch 363 / 500 | iteration 15 / 30 | Total Loss: 4.0843186378479 | KNN Loss: 3.0676252841949463 | BCE Loss: 1.0166934728622437\n",
      "Epoch 363 / 500 | iteration 20 / 30 | Total Loss: 4.100780963897705 | KNN Loss: 3.0641186237335205 | BCE Loss: 1.0366623401641846\n",
      "Epoch 363 / 500 | iteration 25 / 30 | Total Loss: 4.079537391662598 | KNN Loss: 3.0617446899414062 | BCE Loss: 1.0177929401397705\n",
      "Epoch 364 / 500 | iteration 0 / 30 | Total Loss: 4.135427951812744 | KNN Loss: 3.105100393295288 | BCE Loss: 1.030327558517456\n",
      "Epoch 364 / 500 | iteration 5 / 30 | Total Loss: 4.087286472320557 | KNN Loss: 3.0465946197509766 | BCE Loss: 1.04069185256958\n",
      "Epoch 364 / 500 | iteration 10 / 30 | Total Loss: 4.092606544494629 | KNN Loss: 3.0603690147399902 | BCE Loss: 1.0322374105453491\n",
      "Epoch 364 / 500 | iteration 15 / 30 | Total Loss: 4.043030738830566 | KNN Loss: 3.0479865074157715 | BCE Loss: 0.9950440526008606\n",
      "Epoch 364 / 500 | iteration 20 / 30 | Total Loss: 4.080718040466309 | KNN Loss: 3.042820692062378 | BCE Loss: 1.0378974676132202\n",
      "Epoch 364 / 500 | iteration 25 / 30 | Total Loss: 4.055665016174316 | KNN Loss: 3.070789337158203 | BCE Loss: 0.9848756790161133\n",
      "Epoch 365 / 500 | iteration 0 / 30 | Total Loss: 4.129607200622559 | KNN Loss: 3.0902466773986816 | BCE Loss: 1.0393602848052979\n",
      "Epoch 365 / 500 | iteration 5 / 30 | Total Loss: 4.111234664916992 | KNN Loss: 3.0668182373046875 | BCE Loss: 1.0444164276123047\n",
      "Epoch 365 / 500 | iteration 10 / 30 | Total Loss: 4.025571346282959 | KNN Loss: 3.0457844734191895 | BCE Loss: 0.9797869920730591\n",
      "Epoch 365 / 500 | iteration 15 / 30 | Total Loss: 4.060177326202393 | KNN Loss: 3.059727191925049 | BCE Loss: 1.0004502534866333\n",
      "Epoch 365 / 500 | iteration 20 / 30 | Total Loss: 4.150691986083984 | KNN Loss: 3.1036598682403564 | BCE Loss: 1.047032356262207\n",
      "Epoch 365 / 500 | iteration 25 / 30 | Total Loss: 4.115405559539795 | KNN Loss: 3.0880942344665527 | BCE Loss: 1.0273114442825317\n",
      "Epoch 366 / 500 | iteration 0 / 30 | Total Loss: 4.080545425415039 | KNN Loss: 3.0579612255096436 | BCE Loss: 1.022584080696106\n",
      "Epoch 366 / 500 | iteration 5 / 30 | Total Loss: 4.0794997215271 | KNN Loss: 3.070166826248169 | BCE Loss: 1.0093328952789307\n",
      "Epoch 366 / 500 | iteration 10 / 30 | Total Loss: 4.121619701385498 | KNN Loss: 3.0727314949035645 | BCE Loss: 1.0488882064819336\n",
      "Epoch 366 / 500 | iteration 15 / 30 | Total Loss: 4.113092422485352 | KNN Loss: 3.0627524852752686 | BCE Loss: 1.050340175628662\n",
      "Epoch 366 / 500 | iteration 20 / 30 | Total Loss: 4.129286766052246 | KNN Loss: 3.1210317611694336 | BCE Loss: 1.0082552433013916\n",
      "Epoch 366 / 500 | iteration 25 / 30 | Total Loss: 4.105731964111328 | KNN Loss: 3.0721168518066406 | BCE Loss: 1.0336153507232666\n",
      "Epoch 367 / 500 | iteration 0 / 30 | Total Loss: 4.104211807250977 | KNN Loss: 3.0816893577575684 | BCE Loss: 1.0225224494934082\n",
      "Epoch 367 / 500 | iteration 5 / 30 | Total Loss: 4.060431003570557 | KNN Loss: 3.047546863555908 | BCE Loss: 1.0128841400146484\n",
      "Epoch 367 / 500 | iteration 10 / 30 | Total Loss: 4.101548671722412 | KNN Loss: 3.070761203765869 | BCE Loss: 1.030787467956543\n",
      "Epoch 367 / 500 | iteration 15 / 30 | Total Loss: 4.132092475891113 | KNN Loss: 3.101710081100464 | BCE Loss: 1.0303823947906494\n",
      "Epoch 367 / 500 | iteration 20 / 30 | Total Loss: 4.089092254638672 | KNN Loss: 3.074699640274048 | BCE Loss: 1.0143928527832031\n",
      "Epoch 367 / 500 | iteration 25 / 30 | Total Loss: 4.10898494720459 | KNN Loss: 3.064070224761963 | BCE Loss: 1.044914960861206\n",
      "Epoch 368 / 500 | iteration 0 / 30 | Total Loss: 4.081194877624512 | KNN Loss: 3.0460731983184814 | BCE Loss: 1.0351219177246094\n",
      "Epoch 368 / 500 | iteration 5 / 30 | Total Loss: 4.112803936004639 | KNN Loss: 3.0860681533813477 | BCE Loss: 1.026735782623291\n",
      "Epoch 368 / 500 | iteration 10 / 30 | Total Loss: 4.037257671356201 | KNN Loss: 3.010471820831299 | BCE Loss: 1.0267858505249023\n",
      "Epoch 368 / 500 | iteration 15 / 30 | Total Loss: 4.078207969665527 | KNN Loss: 3.05094575881958 | BCE Loss: 1.0272620916366577\n",
      "Epoch 368 / 500 | iteration 20 / 30 | Total Loss: 4.069886207580566 | KNN Loss: 3.0390632152557373 | BCE Loss: 1.0308228731155396\n",
      "Epoch 368 / 500 | iteration 25 / 30 | Total Loss: 4.076725959777832 | KNN Loss: 3.0372674465179443 | BCE Loss: 1.0394587516784668\n",
      "Epoch 369 / 500 | iteration 0 / 30 | Total Loss: 4.091546058654785 | KNN Loss: 3.075139045715332 | BCE Loss: 1.016406774520874\n",
      "Epoch 369 / 500 | iteration 5 / 30 | Total Loss: 4.156551361083984 | KNN Loss: 3.0923969745635986 | BCE Loss: 1.0641542673110962\n",
      "Epoch 369 / 500 | iteration 10 / 30 | Total Loss: 4.0907416343688965 | KNN Loss: 3.0534234046936035 | BCE Loss: 1.0373181104660034\n",
      "Epoch 369 / 500 | iteration 15 / 30 | Total Loss: 4.051093578338623 | KNN Loss: 3.041127920150757 | BCE Loss: 1.0099656581878662\n",
      "Epoch 369 / 500 | iteration 20 / 30 | Total Loss: 4.065949440002441 | KNN Loss: 3.0594637393951416 | BCE Loss: 1.006485939025879\n",
      "Epoch 369 / 500 | iteration 25 / 30 | Total Loss: 4.0996503829956055 | KNN Loss: 3.0846381187438965 | BCE Loss: 1.015012264251709\n",
      "Epoch 370 / 500 | iteration 0 / 30 | Total Loss: 4.0761799812316895 | KNN Loss: 3.07292103767395 | BCE Loss: 1.0032589435577393\n",
      "Epoch 370 / 500 | iteration 5 / 30 | Total Loss: 4.037383079528809 | KNN Loss: 3.0525126457214355 | BCE Loss: 0.9848703145980835\n",
      "Epoch 370 / 500 | iteration 10 / 30 | Total Loss: 4.0981340408325195 | KNN Loss: 3.069488525390625 | BCE Loss: 1.0286457538604736\n",
      "Epoch 370 / 500 | iteration 15 / 30 | Total Loss: 4.071480751037598 | KNN Loss: 3.0598344802856445 | BCE Loss: 1.0116463899612427\n",
      "Epoch 370 / 500 | iteration 20 / 30 | Total Loss: 4.127381324768066 | KNN Loss: 3.0829081535339355 | BCE Loss: 1.04447340965271\n",
      "Epoch 370 / 500 | iteration 25 / 30 | Total Loss: 4.054887294769287 | KNN Loss: 3.0628206729888916 | BCE Loss: 0.9920667409896851\n",
      "Epoch 371 / 500 | iteration 0 / 30 | Total Loss: 4.097519397735596 | KNN Loss: 3.055884838104248 | BCE Loss: 1.0416345596313477\n",
      "Epoch 371 / 500 | iteration 5 / 30 | Total Loss: 4.1112518310546875 | KNN Loss: 3.065687656402588 | BCE Loss: 1.0455642938613892\n",
      "Epoch 371 / 500 | iteration 10 / 30 | Total Loss: 4.066066265106201 | KNN Loss: 3.067793369293213 | BCE Loss: 0.9982727766036987\n",
      "Epoch 371 / 500 | iteration 15 / 30 | Total Loss: 4.092639446258545 | KNN Loss: 3.06649112701416 | BCE Loss: 1.0261484384536743\n",
      "Epoch 371 / 500 | iteration 20 / 30 | Total Loss: 4.112673759460449 | KNN Loss: 3.079991102218628 | BCE Loss: 1.0326824188232422\n",
      "Epoch 371 / 500 | iteration 25 / 30 | Total Loss: 4.071873664855957 | KNN Loss: 3.05411696434021 | BCE Loss: 1.0177569389343262\n",
      "Epoch 372 / 500 | iteration 0 / 30 | Total Loss: 3.994831085205078 | KNN Loss: 3.0164382457733154 | BCE Loss: 0.9783928394317627\n",
      "Epoch 372 / 500 | iteration 5 / 30 | Total Loss: 4.038450717926025 | KNN Loss: 3.056018114089966 | BCE Loss: 0.9824326634407043\n",
      "Epoch 372 / 500 | iteration 10 / 30 | Total Loss: 4.086482524871826 | KNN Loss: 3.040682792663574 | BCE Loss: 1.045799732208252\n",
      "Epoch 372 / 500 | iteration 15 / 30 | Total Loss: 4.092658996582031 | KNN Loss: 3.057537078857422 | BCE Loss: 1.0351221561431885\n",
      "Epoch 372 / 500 | iteration 20 / 30 | Total Loss: 4.106550693511963 | KNN Loss: 3.0771663188934326 | BCE Loss: 1.0293843746185303\n",
      "Epoch 372 / 500 | iteration 25 / 30 | Total Loss: 4.110716819763184 | KNN Loss: 3.0997414588928223 | BCE Loss: 1.0109752416610718\n",
      "Epoch 373 / 500 | iteration 0 / 30 | Total Loss: 4.144102573394775 | KNN Loss: 3.1340856552124023 | BCE Loss: 1.010016918182373\n",
      "Epoch 373 / 500 | iteration 5 / 30 | Total Loss: 4.067448139190674 | KNN Loss: 3.0725646018981934 | BCE Loss: 0.99488365650177\n",
      "Epoch 373 / 500 | iteration 10 / 30 | Total Loss: 4.13726806640625 | KNN Loss: 3.077632427215576 | BCE Loss: 1.0596355199813843\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 373 / 500 | iteration 15 / 30 | Total Loss: 4.077079772949219 | KNN Loss: 3.048875331878662 | BCE Loss: 1.0282044410705566\n",
      "Epoch 373 / 500 | iteration 20 / 30 | Total Loss: 4.113705635070801 | KNN Loss: 3.0762808322906494 | BCE Loss: 1.0374246835708618\n",
      "Epoch 373 / 500 | iteration 25 / 30 | Total Loss: 4.07818078994751 | KNN Loss: 3.077791213989258 | BCE Loss: 1.000389575958252\n",
      "Epoch 374 / 500 | iteration 0 / 30 | Total Loss: 4.085204601287842 | KNN Loss: 3.080888271331787 | BCE Loss: 1.0043163299560547\n",
      "Epoch 374 / 500 | iteration 5 / 30 | Total Loss: 4.062018394470215 | KNN Loss: 3.0552868843078613 | BCE Loss: 1.0067312717437744\n",
      "Epoch 374 / 500 | iteration 10 / 30 | Total Loss: 4.046649932861328 | KNN Loss: 3.0492103099823 | BCE Loss: 0.9974395036697388\n",
      "Epoch 374 / 500 | iteration 15 / 30 | Total Loss: 4.079708576202393 | KNN Loss: 3.0769402980804443 | BCE Loss: 1.0027683973312378\n",
      "Epoch 374 / 500 | iteration 20 / 30 | Total Loss: 4.092165470123291 | KNN Loss: 3.054375410079956 | BCE Loss: 1.037790060043335\n",
      "Epoch 374 / 500 | iteration 25 / 30 | Total Loss: 4.054724216461182 | KNN Loss: 3.047481060028076 | BCE Loss: 1.007243275642395\n",
      "Epoch   375: reducing learning rate of group 0 to 1.3684e-06.\n",
      "Epoch 375 / 500 | iteration 0 / 30 | Total Loss: 4.094254493713379 | KNN Loss: 3.0613393783569336 | BCE Loss: 1.0329148769378662\n",
      "Epoch 375 / 500 | iteration 5 / 30 | Total Loss: 4.070817947387695 | KNN Loss: 3.080744743347168 | BCE Loss: 0.9900734424591064\n",
      "Epoch 375 / 500 | iteration 10 / 30 | Total Loss: 4.065283298492432 | KNN Loss: 3.0382282733917236 | BCE Loss: 1.0270549058914185\n",
      "Epoch 375 / 500 | iteration 15 / 30 | Total Loss: 4.084133625030518 | KNN Loss: 3.0555837154388428 | BCE Loss: 1.0285499095916748\n",
      "Epoch 375 / 500 | iteration 20 / 30 | Total Loss: 4.039902687072754 | KNN Loss: 3.0484538078308105 | BCE Loss: 0.9914488792419434\n",
      "Epoch 375 / 500 | iteration 25 / 30 | Total Loss: 4.0790276527404785 | KNN Loss: 3.0466957092285156 | BCE Loss: 1.032331943511963\n",
      "Epoch 376 / 500 | iteration 0 / 30 | Total Loss: 4.083105087280273 | KNN Loss: 3.0903449058532715 | BCE Loss: 0.9927600622177124\n",
      "Epoch 376 / 500 | iteration 5 / 30 | Total Loss: 4.089765548706055 | KNN Loss: 3.0993309020996094 | BCE Loss: 0.9904348850250244\n",
      "Epoch 376 / 500 | iteration 10 / 30 | Total Loss: 4.083662986755371 | KNN Loss: 3.056652545928955 | BCE Loss: 1.027010440826416\n",
      "Epoch 376 / 500 | iteration 15 / 30 | Total Loss: 4.073147296905518 | KNN Loss: 3.066014289855957 | BCE Loss: 1.007132887840271\n",
      "Epoch 376 / 500 | iteration 20 / 30 | Total Loss: 4.098121643066406 | KNN Loss: 3.0689992904663086 | BCE Loss: 1.0291225910186768\n",
      "Epoch 376 / 500 | iteration 25 / 30 | Total Loss: 4.139374256134033 | KNN Loss: 3.0925915241241455 | BCE Loss: 1.0467828512191772\n",
      "Epoch 377 / 500 | iteration 0 / 30 | Total Loss: 4.11121940612793 | KNN Loss: 3.087869644165039 | BCE Loss: 1.0233495235443115\n",
      "Epoch 377 / 500 | iteration 5 / 30 | Total Loss: 4.043981552124023 | KNN Loss: 3.031083583831787 | BCE Loss: 1.0128977298736572\n",
      "Epoch 377 / 500 | iteration 10 / 30 | Total Loss: 4.070937156677246 | KNN Loss: 3.0488991737365723 | BCE Loss: 1.022038221359253\n",
      "Epoch 377 / 500 | iteration 15 / 30 | Total Loss: 4.054638862609863 | KNN Loss: 3.032745838165283 | BCE Loss: 1.0218929052352905\n",
      "Epoch 377 / 500 | iteration 20 / 30 | Total Loss: 4.060340881347656 | KNN Loss: 3.0721468925476074 | BCE Loss: 0.9881938695907593\n",
      "Epoch 377 / 500 | iteration 25 / 30 | Total Loss: 4.121365547180176 | KNN Loss: 3.05232834815979 | BCE Loss: 1.0690370798110962\n",
      "Epoch 378 / 500 | iteration 0 / 30 | Total Loss: 4.083710670471191 | KNN Loss: 3.079191207885742 | BCE Loss: 1.0045194625854492\n",
      "Epoch 378 / 500 | iteration 5 / 30 | Total Loss: 4.059131622314453 | KNN Loss: 3.030017852783203 | BCE Loss: 1.029114007949829\n",
      "Epoch 378 / 500 | iteration 10 / 30 | Total Loss: 4.066226959228516 | KNN Loss: 3.047952651977539 | BCE Loss: 1.0182744264602661\n",
      "Epoch 378 / 500 | iteration 15 / 30 | Total Loss: 4.085300922393799 | KNN Loss: 3.057955503463745 | BCE Loss: 1.0273454189300537\n",
      "Epoch 378 / 500 | iteration 20 / 30 | Total Loss: 4.096127986907959 | KNN Loss: 3.0667531490325928 | BCE Loss: 1.0293747186660767\n",
      "Epoch 378 / 500 | iteration 25 / 30 | Total Loss: 4.086678504943848 | KNN Loss: 3.072500467300415 | BCE Loss: 1.0141780376434326\n",
      "Epoch 379 / 500 | iteration 0 / 30 | Total Loss: 4.044552803039551 | KNN Loss: 3.0383260250091553 | BCE Loss: 1.006226658821106\n",
      "Epoch 379 / 500 | iteration 5 / 30 | Total Loss: 4.104366302490234 | KNN Loss: 3.0573625564575195 | BCE Loss: 1.047003984451294\n",
      "Epoch 379 / 500 | iteration 10 / 30 | Total Loss: 4.066267013549805 | KNN Loss: 3.0927398204803467 | BCE Loss: 0.9735270738601685\n",
      "Epoch 379 / 500 | iteration 15 / 30 | Total Loss: 4.0511474609375 | KNN Loss: 3.0242421627044678 | BCE Loss: 1.0269052982330322\n",
      "Epoch 379 / 500 | iteration 20 / 30 | Total Loss: 4.1040730476379395 | KNN Loss: 3.06695556640625 | BCE Loss: 1.0371174812316895\n",
      "Epoch 379 / 500 | iteration 25 / 30 | Total Loss: 4.067413330078125 | KNN Loss: 3.062737464904785 | BCE Loss: 1.0046756267547607\n",
      "Epoch 380 / 500 | iteration 0 / 30 | Total Loss: 4.083483695983887 | KNN Loss: 3.0695130825042725 | BCE Loss: 1.0139706134796143\n",
      "Epoch 380 / 500 | iteration 5 / 30 | Total Loss: 4.055464744567871 | KNN Loss: 3.0645034313201904 | BCE Loss: 0.9909614324569702\n",
      "Epoch 380 / 500 | iteration 10 / 30 | Total Loss: 4.070112705230713 | KNN Loss: 3.0548534393310547 | BCE Loss: 1.0152591466903687\n",
      "Epoch 380 / 500 | iteration 15 / 30 | Total Loss: 4.079237937927246 | KNN Loss: 3.045358657836914 | BCE Loss: 1.0338795185089111\n",
      "Epoch 380 / 500 | iteration 20 / 30 | Total Loss: 4.072129726409912 | KNN Loss: 3.075214147567749 | BCE Loss: 0.9969154000282288\n",
      "Epoch 380 / 500 | iteration 25 / 30 | Total Loss: 4.020263671875 | KNN Loss: 3.013685464859009 | BCE Loss: 1.0065784454345703\n",
      "Epoch 381 / 500 | iteration 0 / 30 | Total Loss: 4.107274532318115 | KNN Loss: 3.083369731903076 | BCE Loss: 1.0239049196243286\n",
      "Epoch 381 / 500 | iteration 5 / 30 | Total Loss: 4.08237361907959 | KNN Loss: 3.0643250942230225 | BCE Loss: 1.018048644065857\n",
      "Epoch 381 / 500 | iteration 10 / 30 | Total Loss: 4.073905944824219 | KNN Loss: 3.072542905807495 | BCE Loss: 1.0013632774353027\n",
      "Epoch 381 / 500 | iteration 15 / 30 | Total Loss: 4.083558082580566 | KNN Loss: 3.0750198364257812 | BCE Loss: 1.0085383653640747\n",
      "Epoch 381 / 500 | iteration 20 / 30 | Total Loss: 4.14272928237915 | KNN Loss: 3.1391537189483643 | BCE Loss: 1.0035755634307861\n",
      "Epoch 381 / 500 | iteration 25 / 30 | Total Loss: 4.080290794372559 | KNN Loss: 3.0490481853485107 | BCE Loss: 1.031242847442627\n",
      "Epoch 382 / 500 | iteration 0 / 30 | Total Loss: 4.0618577003479 | KNN Loss: 3.053490400314331 | BCE Loss: 1.0083673000335693\n",
      "Epoch 382 / 500 | iteration 5 / 30 | Total Loss: 4.112743377685547 | KNN Loss: 3.1072397232055664 | BCE Loss: 1.0055036544799805\n",
      "Epoch 382 / 500 | iteration 10 / 30 | Total Loss: 4.098679542541504 | KNN Loss: 3.073334217071533 | BCE Loss: 1.0253455638885498\n",
      "Epoch 382 / 500 | iteration 15 / 30 | Total Loss: 4.076932907104492 | KNN Loss: 3.0765366554260254 | BCE Loss: 1.0003962516784668\n",
      "Epoch 382 / 500 | iteration 20 / 30 | Total Loss: 4.094493389129639 | KNN Loss: 3.0380959510803223 | BCE Loss: 1.0563974380493164\n",
      "Epoch 382 / 500 | iteration 25 / 30 | Total Loss: 4.065956115722656 | KNN Loss: 3.0276436805725098 | BCE Loss: 1.0383126735687256\n",
      "Epoch 383 / 500 | iteration 0 / 30 | Total Loss: 4.068724632263184 | KNN Loss: 3.045884847640991 | BCE Loss: 1.0228396654129028\n",
      "Epoch 383 / 500 | iteration 5 / 30 | Total Loss: 4.066775321960449 | KNN Loss: 3.0363969802856445 | BCE Loss: 1.0303785800933838\n",
      "Epoch 383 / 500 | iteration 10 / 30 | Total Loss: 4.079692363739014 | KNN Loss: 3.074110984802246 | BCE Loss: 1.005581259727478\n",
      "Epoch 383 / 500 | iteration 15 / 30 | Total Loss: 4.091640949249268 | KNN Loss: 3.0760161876678467 | BCE Loss: 1.0156246423721313\n",
      "Epoch 383 / 500 | iteration 20 / 30 | Total Loss: 4.113325119018555 | KNN Loss: 3.0805583000183105 | BCE Loss: 1.0327668190002441\n",
      "Epoch 383 / 500 | iteration 25 / 30 | Total Loss: 4.069609642028809 | KNN Loss: 3.055314540863037 | BCE Loss: 1.0142951011657715\n",
      "Epoch 384 / 500 | iteration 0 / 30 | Total Loss: 4.12150764465332 | KNN Loss: 3.1017942428588867 | BCE Loss: 1.0197131633758545\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 384 / 500 | iteration 5 / 30 | Total Loss: 4.063702583312988 | KNN Loss: 3.0532169342041016 | BCE Loss: 1.0104856491088867\n",
      "Epoch 384 / 500 | iteration 10 / 30 | Total Loss: 4.079419136047363 | KNN Loss: 3.0547778606414795 | BCE Loss: 1.0246412754058838\n",
      "Epoch 384 / 500 | iteration 15 / 30 | Total Loss: 4.058086395263672 | KNN Loss: 3.0505003929138184 | BCE Loss: 1.0075857639312744\n",
      "Epoch 384 / 500 | iteration 20 / 30 | Total Loss: 4.090585708618164 | KNN Loss: 3.0575969219207764 | BCE Loss: 1.0329887866973877\n",
      "Epoch 384 / 500 | iteration 25 / 30 | Total Loss: 4.1158857345581055 | KNN Loss: 3.0565316677093506 | BCE Loss: 1.0593538284301758\n",
      "Epoch 385 / 500 | iteration 0 / 30 | Total Loss: 4.118457794189453 | KNN Loss: 3.062504768371582 | BCE Loss: 1.0559532642364502\n",
      "Epoch 385 / 500 | iteration 5 / 30 | Total Loss: 4.095338821411133 | KNN Loss: 3.0789670944213867 | BCE Loss: 1.016371488571167\n",
      "Epoch 385 / 500 | iteration 10 / 30 | Total Loss: 4.069477081298828 | KNN Loss: 3.0654807090759277 | BCE Loss: 1.0039961338043213\n",
      "Epoch 385 / 500 | iteration 15 / 30 | Total Loss: 4.0618085861206055 | KNN Loss: 3.0616836547851562 | BCE Loss: 1.0001249313354492\n",
      "Epoch 385 / 500 | iteration 20 / 30 | Total Loss: 4.073360919952393 | KNN Loss: 3.045313596725464 | BCE Loss: 1.0280473232269287\n",
      "Epoch 385 / 500 | iteration 25 / 30 | Total Loss: 4.095392227172852 | KNN Loss: 3.0739169120788574 | BCE Loss: 1.021475076675415\n",
      "Epoch   386: reducing learning rate of group 0 to 9.5791e-07.\n",
      "Epoch 386 / 500 | iteration 0 / 30 | Total Loss: 4.064141273498535 | KNN Loss: 3.068037748336792 | BCE Loss: 0.9961032867431641\n",
      "Epoch 386 / 500 | iteration 5 / 30 | Total Loss: 4.048534393310547 | KNN Loss: 3.0180046558380127 | BCE Loss: 1.030529499053955\n",
      "Epoch 386 / 500 | iteration 10 / 30 | Total Loss: 4.089695930480957 | KNN Loss: 3.069411277770996 | BCE Loss: 1.0202845335006714\n",
      "Epoch 386 / 500 | iteration 15 / 30 | Total Loss: 4.0638628005981445 | KNN Loss: 3.030318260192871 | BCE Loss: 1.0335447788238525\n",
      "Epoch 386 / 500 | iteration 20 / 30 | Total Loss: 4.087739944458008 | KNN Loss: 3.072511911392212 | BCE Loss: 1.0152281522750854\n",
      "Epoch 386 / 500 | iteration 25 / 30 | Total Loss: 4.096543312072754 | KNN Loss: 3.064434289932251 | BCE Loss: 1.032109022140503\n",
      "Epoch 387 / 500 | iteration 0 / 30 | Total Loss: 4.093951225280762 | KNN Loss: 3.0531117916107178 | BCE Loss: 1.040839672088623\n",
      "Epoch 387 / 500 | iteration 5 / 30 | Total Loss: 4.0944414138793945 | KNN Loss: 3.1202261447906494 | BCE Loss: 0.9742151498794556\n",
      "Epoch 387 / 500 | iteration 10 / 30 | Total Loss: 4.101135730743408 | KNN Loss: 3.06374192237854 | BCE Loss: 1.0373938083648682\n",
      "Epoch 387 / 500 | iteration 15 / 30 | Total Loss: 4.134181499481201 | KNN Loss: 3.1090166568756104 | BCE Loss: 1.0251649618148804\n",
      "Epoch 387 / 500 | iteration 20 / 30 | Total Loss: 4.0939130783081055 | KNN Loss: 3.0790514945983887 | BCE Loss: 1.0148613452911377\n",
      "Epoch 387 / 500 | iteration 25 / 30 | Total Loss: 4.097602367401123 | KNN Loss: 3.0448110103607178 | BCE Loss: 1.0527913570404053\n",
      "Epoch 388 / 500 | iteration 0 / 30 | Total Loss: 4.122269153594971 | KNN Loss: 3.0964183807373047 | BCE Loss: 1.025850772857666\n",
      "Epoch 388 / 500 | iteration 5 / 30 | Total Loss: 4.086767673492432 | KNN Loss: 3.0447676181793213 | BCE Loss: 1.0420000553131104\n",
      "Epoch 388 / 500 | iteration 10 / 30 | Total Loss: 4.086246490478516 | KNN Loss: 3.0569350719451904 | BCE Loss: 1.029311180114746\n",
      "Epoch 388 / 500 | iteration 15 / 30 | Total Loss: 4.105352401733398 | KNN Loss: 3.0620639324188232 | BCE Loss: 1.0432884693145752\n",
      "Epoch 388 / 500 | iteration 20 / 30 | Total Loss: 4.139687538146973 | KNN Loss: 3.1273515224456787 | BCE Loss: 1.012336015701294\n",
      "Epoch 388 / 500 | iteration 25 / 30 | Total Loss: 4.0887861251831055 | KNN Loss: 3.048189163208008 | BCE Loss: 1.0405972003936768\n",
      "Epoch 389 / 500 | iteration 0 / 30 | Total Loss: 4.0816731452941895 | KNN Loss: 3.0623855590820312 | BCE Loss: 1.0192877054214478\n",
      "Epoch 389 / 500 | iteration 5 / 30 | Total Loss: 4.106424331665039 | KNN Loss: 3.086820125579834 | BCE Loss: 1.0196040868759155\n",
      "Epoch 389 / 500 | iteration 10 / 30 | Total Loss: 4.0641961097717285 | KNN Loss: 3.0516743659973145 | BCE Loss: 1.0125218629837036\n",
      "Epoch 389 / 500 | iteration 15 / 30 | Total Loss: 4.1445088386535645 | KNN Loss: 3.0841822624206543 | BCE Loss: 1.0603265762329102\n",
      "Epoch 389 / 500 | iteration 20 / 30 | Total Loss: 4.066967010498047 | KNN Loss: 3.075011968612671 | BCE Loss: 0.9919552803039551\n",
      "Epoch 389 / 500 | iteration 25 / 30 | Total Loss: 4.085660934448242 | KNN Loss: 3.0585994720458984 | BCE Loss: 1.0270617008209229\n",
      "Epoch 390 / 500 | iteration 0 / 30 | Total Loss: 4.042858123779297 | KNN Loss: 3.0343499183654785 | BCE Loss: 1.0085079669952393\n",
      "Epoch 390 / 500 | iteration 5 / 30 | Total Loss: 4.081307411193848 | KNN Loss: 3.0328240394592285 | BCE Loss: 1.0484836101531982\n",
      "Epoch 390 / 500 | iteration 10 / 30 | Total Loss: 4.077633857727051 | KNN Loss: 3.061089515686035 | BCE Loss: 1.016544222831726\n",
      "Epoch 390 / 500 | iteration 15 / 30 | Total Loss: 4.097887992858887 | KNN Loss: 3.0617456436157227 | BCE Loss: 1.036142110824585\n",
      "Epoch 390 / 500 | iteration 20 / 30 | Total Loss: 4.098907947540283 | KNN Loss: 3.061044216156006 | BCE Loss: 1.0378637313842773\n",
      "Epoch 390 / 500 | iteration 25 / 30 | Total Loss: 4.0870842933654785 | KNN Loss: 3.0900206565856934 | BCE Loss: 0.9970637559890747\n",
      "Epoch 391 / 500 | iteration 0 / 30 | Total Loss: 4.14688777923584 | KNN Loss: 3.123887538909912 | BCE Loss: 1.0230004787445068\n",
      "Epoch 391 / 500 | iteration 5 / 30 | Total Loss: 4.074211120605469 | KNN Loss: 3.0421531200408936 | BCE Loss: 1.0320582389831543\n",
      "Epoch 391 / 500 | iteration 10 / 30 | Total Loss: 4.093451499938965 | KNN Loss: 3.069298028945923 | BCE Loss: 1.0241535902023315\n",
      "Epoch 391 / 500 | iteration 15 / 30 | Total Loss: 4.057591438293457 | KNN Loss: 3.0433719158172607 | BCE Loss: 1.0142197608947754\n",
      "Epoch 391 / 500 | iteration 20 / 30 | Total Loss: 4.078429222106934 | KNN Loss: 3.0556814670562744 | BCE Loss: 1.0227479934692383\n",
      "Epoch 391 / 500 | iteration 25 / 30 | Total Loss: 4.100970268249512 | KNN Loss: 3.092244863510132 | BCE Loss: 1.008725643157959\n",
      "Epoch 392 / 500 | iteration 0 / 30 | Total Loss: 4.133550643920898 | KNN Loss: 3.0939836502075195 | BCE Loss: 1.039566993713379\n",
      "Epoch 392 / 500 | iteration 5 / 30 | Total Loss: 4.118696212768555 | KNN Loss: 3.0771918296813965 | BCE Loss: 1.0415043830871582\n",
      "Epoch 392 / 500 | iteration 10 / 30 | Total Loss: 4.108135223388672 | KNN Loss: 3.0964863300323486 | BCE Loss: 1.0116488933563232\n",
      "Epoch 392 / 500 | iteration 15 / 30 | Total Loss: 4.053953170776367 | KNN Loss: 3.0478017330169678 | BCE Loss: 1.0061514377593994\n",
      "Epoch 392 / 500 | iteration 20 / 30 | Total Loss: 4.059974670410156 | KNN Loss: 3.0370819568634033 | BCE Loss: 1.022892713546753\n",
      "Epoch 392 / 500 | iteration 25 / 30 | Total Loss: 4.05195951461792 | KNN Loss: 3.0419483184814453 | BCE Loss: 1.0100113153457642\n",
      "Epoch 393 / 500 | iteration 0 / 30 | Total Loss: 4.087679862976074 | KNN Loss: 3.0704762935638428 | BCE Loss: 1.0172038078308105\n",
      "Epoch 393 / 500 | iteration 5 / 30 | Total Loss: 4.049136161804199 | KNN Loss: 3.039731502532959 | BCE Loss: 1.0094044208526611\n",
      "Epoch 393 / 500 | iteration 10 / 30 | Total Loss: 4.09672212600708 | KNN Loss: 3.0503156185150146 | BCE Loss: 1.046406626701355\n",
      "Epoch 393 / 500 | iteration 15 / 30 | Total Loss: 4.125084400177002 | KNN Loss: 3.0718531608581543 | BCE Loss: 1.0532312393188477\n",
      "Epoch 393 / 500 | iteration 20 / 30 | Total Loss: 4.063308238983154 | KNN Loss: 3.0490822792053223 | BCE Loss: 1.0142258405685425\n",
      "Epoch 393 / 500 | iteration 25 / 30 | Total Loss: 4.058551788330078 | KNN Loss: 3.0325231552124023 | BCE Loss: 1.0260286331176758\n",
      "Epoch 394 / 500 | iteration 0 / 30 | Total Loss: 4.092472553253174 | KNN Loss: 3.061946153640747 | BCE Loss: 1.0305262804031372\n",
      "Epoch 394 / 500 | iteration 5 / 30 | Total Loss: 4.145763397216797 | KNN Loss: 3.0866076946258545 | BCE Loss: 1.0591554641723633\n",
      "Epoch 394 / 500 | iteration 10 / 30 | Total Loss: 4.064569473266602 | KNN Loss: 3.037388324737549 | BCE Loss: 1.0271813869476318\n",
      "Epoch 394 / 500 | iteration 15 / 30 | Total Loss: 4.084013938903809 | KNN Loss: 3.0655970573425293 | BCE Loss: 1.0184167623519897\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 394 / 500 | iteration 20 / 30 | Total Loss: 4.120943546295166 | KNN Loss: 3.109389305114746 | BCE Loss: 1.0115541219711304\n",
      "Epoch 394 / 500 | iteration 25 / 30 | Total Loss: 4.090363502502441 | KNN Loss: 3.077444076538086 | BCE Loss: 1.0129191875457764\n",
      "Epoch 395 / 500 | iteration 0 / 30 | Total Loss: 4.07285213470459 | KNN Loss: 3.046447515487671 | BCE Loss: 1.0264043807983398\n",
      "Epoch 395 / 500 | iteration 5 / 30 | Total Loss: 4.096619129180908 | KNN Loss: 3.0946435928344727 | BCE Loss: 1.001975655555725\n",
      "Epoch 395 / 500 | iteration 10 / 30 | Total Loss: 4.078974723815918 | KNN Loss: 3.0472629070281982 | BCE Loss: 1.0317116975784302\n",
      "Epoch 395 / 500 | iteration 15 / 30 | Total Loss: 4.114849090576172 | KNN Loss: 3.079066276550293 | BCE Loss: 1.035782814025879\n",
      "Epoch 395 / 500 | iteration 20 / 30 | Total Loss: 4.036906719207764 | KNN Loss: 3.030313014984131 | BCE Loss: 1.0065938234329224\n",
      "Epoch 395 / 500 | iteration 25 / 30 | Total Loss: 4.116434574127197 | KNN Loss: 3.059527635574341 | BCE Loss: 1.0569069385528564\n",
      "Epoch 396 / 500 | iteration 0 / 30 | Total Loss: 4.096580505371094 | KNN Loss: 3.0850791931152344 | BCE Loss: 1.0115010738372803\n",
      "Epoch 396 / 500 | iteration 5 / 30 | Total Loss: 4.115618705749512 | KNN Loss: 3.075502634048462 | BCE Loss: 1.040116310119629\n",
      "Epoch 396 / 500 | iteration 10 / 30 | Total Loss: 4.080148220062256 | KNN Loss: 3.062077522277832 | BCE Loss: 1.0180706977844238\n",
      "Epoch 396 / 500 | iteration 15 / 30 | Total Loss: 4.092300891876221 | KNN Loss: 3.0849971771240234 | BCE Loss: 1.0073035955429077\n",
      "Epoch 396 / 500 | iteration 20 / 30 | Total Loss: 4.052247047424316 | KNN Loss: 3.0494234561920166 | BCE Loss: 1.0028235912322998\n",
      "Epoch 396 / 500 | iteration 25 / 30 | Total Loss: 4.051584243774414 | KNN Loss: 3.0436339378356934 | BCE Loss: 1.0079503059387207\n",
      "Epoch   397: reducing learning rate of group 0 to 6.7053e-07.\n",
      "Epoch 397 / 500 | iteration 0 / 30 | Total Loss: 4.072688102722168 | KNN Loss: 3.0471389293670654 | BCE Loss: 1.0255489349365234\n",
      "Epoch 397 / 500 | iteration 5 / 30 | Total Loss: 4.04022216796875 | KNN Loss: 3.0363192558288574 | BCE Loss: 1.0039031505584717\n",
      "Epoch 397 / 500 | iteration 10 / 30 | Total Loss: 4.091314792633057 | KNN Loss: 3.0771865844726562 | BCE Loss: 1.01412832736969\n",
      "Epoch 397 / 500 | iteration 15 / 30 | Total Loss: 4.022163391113281 | KNN Loss: 3.033128499984741 | BCE Loss: 0.9890347719192505\n",
      "Epoch 397 / 500 | iteration 20 / 30 | Total Loss: 4.097143650054932 | KNN Loss: 3.0560030937194824 | BCE Loss: 1.0411404371261597\n",
      "Epoch 397 / 500 | iteration 25 / 30 | Total Loss: 4.084174156188965 | KNN Loss: 3.082310199737549 | BCE Loss: 1.0018641948699951\n",
      "Epoch 398 / 500 | iteration 0 / 30 | Total Loss: 4.094443321228027 | KNN Loss: 3.0621681213378906 | BCE Loss: 1.0322751998901367\n",
      "Epoch 398 / 500 | iteration 5 / 30 | Total Loss: 4.1236653327941895 | KNN Loss: 3.0799672603607178 | BCE Loss: 1.0436981916427612\n",
      "Epoch 398 / 500 | iteration 10 / 30 | Total Loss: 4.124077796936035 | KNN Loss: 3.0824005603790283 | BCE Loss: 1.0416769981384277\n",
      "Epoch 398 / 500 | iteration 15 / 30 | Total Loss: 4.092947959899902 | KNN Loss: 3.0913407802581787 | BCE Loss: 1.0016069412231445\n",
      "Epoch 398 / 500 | iteration 20 / 30 | Total Loss: 4.131476402282715 | KNN Loss: 3.099513053894043 | BCE Loss: 1.0319631099700928\n",
      "Epoch 398 / 500 | iteration 25 / 30 | Total Loss: 4.10422420501709 | KNN Loss: 3.0906574726104736 | BCE Loss: 1.0135669708251953\n",
      "Epoch 399 / 500 | iteration 0 / 30 | Total Loss: 4.100331783294678 | KNN Loss: 3.094026565551758 | BCE Loss: 1.00630521774292\n",
      "Epoch 399 / 500 | iteration 5 / 30 | Total Loss: 4.07668924331665 | KNN Loss: 3.033565044403076 | BCE Loss: 1.0431240797042847\n",
      "Epoch 399 / 500 | iteration 10 / 30 | Total Loss: 4.096962928771973 | KNN Loss: 3.103273868560791 | BCE Loss: 0.9936890602111816\n",
      "Epoch 399 / 500 | iteration 15 / 30 | Total Loss: 4.1175713539123535 | KNN Loss: 3.074634313583374 | BCE Loss: 1.04293692111969\n",
      "Epoch 399 / 500 | iteration 20 / 30 | Total Loss: 4.1091508865356445 | KNN Loss: 3.091707706451416 | BCE Loss: 1.0174431800842285\n",
      "Epoch 399 / 500 | iteration 25 / 30 | Total Loss: 4.077665328979492 | KNN Loss: 3.0574564933776855 | BCE Loss: 1.0202088356018066\n",
      "Epoch 400 / 500 | iteration 0 / 30 | Total Loss: 4.099058151245117 | KNN Loss: 3.076066493988037 | BCE Loss: 1.02299165725708\n",
      "Epoch 400 / 500 | iteration 5 / 30 | Total Loss: 4.146392345428467 | KNN Loss: 3.115612745285034 | BCE Loss: 1.0307796001434326\n",
      "Epoch 400 / 500 | iteration 10 / 30 | Total Loss: 4.0571184158325195 | KNN Loss: 3.054718017578125 | BCE Loss: 1.0024006366729736\n",
      "Epoch 400 / 500 | iteration 15 / 30 | Total Loss: 4.079287052154541 | KNN Loss: 3.066972255706787 | BCE Loss: 1.012314796447754\n",
      "Epoch 400 / 500 | iteration 20 / 30 | Total Loss: 4.103381156921387 | KNN Loss: 3.0918657779693604 | BCE Loss: 1.0115156173706055\n",
      "Epoch 400 / 500 | iteration 25 / 30 | Total Loss: 4.044193267822266 | KNN Loss: 3.0379080772399902 | BCE Loss: 1.0062854290008545\n",
      "Epoch 401 / 500 | iteration 0 / 30 | Total Loss: 4.101200580596924 | KNN Loss: 3.057758092880249 | BCE Loss: 1.0434423685073853\n",
      "Epoch 401 / 500 | iteration 5 / 30 | Total Loss: 4.09171199798584 | KNN Loss: 3.0454225540161133 | BCE Loss: 1.0462896823883057\n",
      "Epoch 401 / 500 | iteration 10 / 30 | Total Loss: 4.08034086227417 | KNN Loss: 3.06145977973938 | BCE Loss: 1.01888108253479\n",
      "Epoch 401 / 500 | iteration 15 / 30 | Total Loss: 4.060512542724609 | KNN Loss: 3.0736470222473145 | BCE Loss: 0.9868654012680054\n",
      "Epoch 401 / 500 | iteration 20 / 30 | Total Loss: 4.101231575012207 | KNN Loss: 3.0891342163085938 | BCE Loss: 1.0120975971221924\n",
      "Epoch 401 / 500 | iteration 25 / 30 | Total Loss: 4.049375057220459 | KNN Loss: 3.0668187141418457 | BCE Loss: 0.982556164264679\n",
      "Epoch 402 / 500 | iteration 0 / 30 | Total Loss: 4.077139854431152 | KNN Loss: 3.0572376251220703 | BCE Loss: 1.019902229309082\n",
      "Epoch 402 / 500 | iteration 5 / 30 | Total Loss: 4.119955062866211 | KNN Loss: 3.0896782875061035 | BCE Loss: 1.0302765369415283\n",
      "Epoch 402 / 500 | iteration 10 / 30 | Total Loss: 4.073901653289795 | KNN Loss: 3.05692982673645 | BCE Loss: 1.0169717073440552\n",
      "Epoch 402 / 500 | iteration 15 / 30 | Total Loss: 4.0800628662109375 | KNN Loss: 3.067045211791992 | BCE Loss: 1.0130175352096558\n",
      "Epoch 402 / 500 | iteration 20 / 30 | Total Loss: 4.070731163024902 | KNN Loss: 3.065742254257202 | BCE Loss: 1.0049889087677002\n",
      "Epoch 402 / 500 | iteration 25 / 30 | Total Loss: 4.084958553314209 | KNN Loss: 3.097210168838501 | BCE Loss: 0.9877485632896423\n",
      "Epoch 403 / 500 | iteration 0 / 30 | Total Loss: 4.139801025390625 | KNN Loss: 3.0825607776641846 | BCE Loss: 1.05724036693573\n",
      "Epoch 403 / 500 | iteration 5 / 30 | Total Loss: 4.105987071990967 | KNN Loss: 3.071167230606079 | BCE Loss: 1.0348198413848877\n",
      "Epoch 403 / 500 | iteration 10 / 30 | Total Loss: 4.0951008796691895 | KNN Loss: 3.0969953536987305 | BCE Loss: 0.9981054067611694\n",
      "Epoch 403 / 500 | iteration 15 / 30 | Total Loss: 4.110921859741211 | KNN Loss: 3.0809731483459473 | BCE Loss: 1.0299484729766846\n",
      "Epoch 403 / 500 | iteration 20 / 30 | Total Loss: 4.118015766143799 | KNN Loss: 3.0900187492370605 | BCE Loss: 1.0279971361160278\n",
      "Epoch 403 / 500 | iteration 25 / 30 | Total Loss: 4.053187847137451 | KNN Loss: 3.0321497917175293 | BCE Loss: 1.0210379362106323\n",
      "Epoch 404 / 500 | iteration 0 / 30 | Total Loss: 4.097476005554199 | KNN Loss: 3.0724878311157227 | BCE Loss: 1.0249881744384766\n",
      "Epoch 404 / 500 | iteration 5 / 30 | Total Loss: 4.082304000854492 | KNN Loss: 3.0317583084106445 | BCE Loss: 1.0505459308624268\n",
      "Epoch 404 / 500 | iteration 10 / 30 | Total Loss: 4.057033538818359 | KNN Loss: 3.049208402633667 | BCE Loss: 1.0078251361846924\n",
      "Epoch 404 / 500 | iteration 15 / 30 | Total Loss: 4.057185649871826 | KNN Loss: 3.067025899887085 | BCE Loss: 0.9901596307754517\n",
      "Epoch 404 / 500 | iteration 20 / 30 | Total Loss: 4.048192501068115 | KNN Loss: 3.0446362495422363 | BCE Loss: 1.0035563707351685\n",
      "Epoch 404 / 500 | iteration 25 / 30 | Total Loss: 4.044960021972656 | KNN Loss: 3.0425522327423096 | BCE Loss: 1.0024075508117676\n",
      "Epoch 405 / 500 | iteration 0 / 30 | Total Loss: 4.085043907165527 | KNN Loss: 3.0702919960021973 | BCE Loss: 1.0147521495819092\n",
      "Epoch 405 / 500 | iteration 5 / 30 | Total Loss: 4.04498291015625 | KNN Loss: 3.0400753021240234 | BCE Loss: 1.0049073696136475\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 405 / 500 | iteration 10 / 30 | Total Loss: 4.068042755126953 | KNN Loss: 3.046755790710449 | BCE Loss: 1.021286964416504\n",
      "Epoch 405 / 500 | iteration 15 / 30 | Total Loss: 4.120184421539307 | KNN Loss: 3.0868475437164307 | BCE Loss: 1.0333367586135864\n",
      "Epoch 405 / 500 | iteration 20 / 30 | Total Loss: 4.073808670043945 | KNN Loss: 3.06144118309021 | BCE Loss: 1.0123677253723145\n",
      "Epoch 405 / 500 | iteration 25 / 30 | Total Loss: 4.043251991271973 | KNN Loss: 3.048901319503784 | BCE Loss: 0.994350790977478\n",
      "Epoch 406 / 500 | iteration 0 / 30 | Total Loss: 4.099719524383545 | KNN Loss: 3.0810751914978027 | BCE Loss: 1.0186443328857422\n",
      "Epoch 406 / 500 | iteration 5 / 30 | Total Loss: 4.093502998352051 | KNN Loss: 3.0630900859832764 | BCE Loss: 1.0304131507873535\n",
      "Epoch 406 / 500 | iteration 10 / 30 | Total Loss: 4.115434169769287 | KNN Loss: 3.067732095718384 | BCE Loss: 1.0477020740509033\n",
      "Epoch 406 / 500 | iteration 15 / 30 | Total Loss: 4.104669094085693 | KNN Loss: 3.1048500537872314 | BCE Loss: 0.9998191595077515\n",
      "Epoch 406 / 500 | iteration 20 / 30 | Total Loss: 4.07468318939209 | KNN Loss: 3.0604264736175537 | BCE Loss: 1.0142567157745361\n",
      "Epoch 406 / 500 | iteration 25 / 30 | Total Loss: 4.055601596832275 | KNN Loss: 3.044764280319214 | BCE Loss: 1.010837197303772\n",
      "Epoch 407 / 500 | iteration 0 / 30 | Total Loss: 4.066181182861328 | KNN Loss: 3.0450215339660645 | BCE Loss: 1.0211594104766846\n",
      "Epoch 407 / 500 | iteration 5 / 30 | Total Loss: 4.106158256530762 | KNN Loss: 3.0810422897338867 | BCE Loss: 1.025115728378296\n",
      "Epoch 407 / 500 | iteration 10 / 30 | Total Loss: 4.07735013961792 | KNN Loss: 3.081815719604492 | BCE Loss: 0.9955344200134277\n",
      "Epoch 407 / 500 | iteration 15 / 30 | Total Loss: 4.154330253601074 | KNN Loss: 3.0888242721557617 | BCE Loss: 1.0655059814453125\n",
      "Epoch 407 / 500 | iteration 20 / 30 | Total Loss: 4.090207576751709 | KNN Loss: 3.0955970287323 | BCE Loss: 0.9946104288101196\n",
      "Epoch 407 / 500 | iteration 25 / 30 | Total Loss: 4.074795722961426 | KNN Loss: 3.066746711730957 | BCE Loss: 1.0080492496490479\n",
      "Epoch   408: reducing learning rate of group 0 to 4.6937e-07.\n",
      "Epoch 408 / 500 | iteration 0 / 30 | Total Loss: 4.102084159851074 | KNN Loss: 3.057281017303467 | BCE Loss: 1.0448029041290283\n",
      "Epoch 408 / 500 | iteration 5 / 30 | Total Loss: 4.094465255737305 | KNN Loss: 3.0506644248962402 | BCE Loss: 1.0438005924224854\n",
      "Epoch 408 / 500 | iteration 10 / 30 | Total Loss: 4.074440956115723 | KNN Loss: 3.0576443672180176 | BCE Loss: 1.016796350479126\n",
      "Epoch 408 / 500 | iteration 15 / 30 | Total Loss: 4.090318202972412 | KNN Loss: 3.0703749656677246 | BCE Loss: 1.019943356513977\n",
      "Epoch 408 / 500 | iteration 20 / 30 | Total Loss: 4.080416679382324 | KNN Loss: 3.0515952110290527 | BCE Loss: 1.0288217067718506\n",
      "Epoch 408 / 500 | iteration 25 / 30 | Total Loss: 4.1405205726623535 | KNN Loss: 3.119516611099243 | BCE Loss: 1.0210039615631104\n",
      "Epoch 409 / 500 | iteration 0 / 30 | Total Loss: 4.041269302368164 | KNN Loss: 3.055426597595215 | BCE Loss: 0.9858428835868835\n",
      "Epoch 409 / 500 | iteration 5 / 30 | Total Loss: 4.129111289978027 | KNN Loss: 3.082683563232422 | BCE Loss: 1.0464279651641846\n",
      "Epoch 409 / 500 | iteration 10 / 30 | Total Loss: 4.1069416999816895 | KNN Loss: 3.0741498470306396 | BCE Loss: 1.0327918529510498\n",
      "Epoch 409 / 500 | iteration 15 / 30 | Total Loss: 4.116259574890137 | KNN Loss: 3.0721893310546875 | BCE Loss: 1.0440703630447388\n",
      "Epoch 409 / 500 | iteration 20 / 30 | Total Loss: 4.114270210266113 | KNN Loss: 3.065943956375122 | BCE Loss: 1.048326015472412\n",
      "Epoch 409 / 500 | iteration 25 / 30 | Total Loss: 4.098562240600586 | KNN Loss: 3.0716404914855957 | BCE Loss: 1.0269215106964111\n",
      "Epoch 410 / 500 | iteration 0 / 30 | Total Loss: 4.060946464538574 | KNN Loss: 3.0350985527038574 | BCE Loss: 1.0258479118347168\n",
      "Epoch 410 / 500 | iteration 5 / 30 | Total Loss: 4.113325595855713 | KNN Loss: 3.056683301925659 | BCE Loss: 1.0566422939300537\n",
      "Epoch 410 / 500 | iteration 10 / 30 | Total Loss: 4.075099945068359 | KNN Loss: 3.0483200550079346 | BCE Loss: 1.0267796516418457\n",
      "Epoch 410 / 500 | iteration 15 / 30 | Total Loss: 4.107656002044678 | KNN Loss: 3.060340404510498 | BCE Loss: 1.0473155975341797\n",
      "Epoch 410 / 500 | iteration 20 / 30 | Total Loss: 4.082583427429199 | KNN Loss: 3.0764198303222656 | BCE Loss: 1.0061637163162231\n",
      "Epoch 410 / 500 | iteration 25 / 30 | Total Loss: 4.067911624908447 | KNN Loss: 3.051017999649048 | BCE Loss: 1.0168935060501099\n",
      "Epoch 411 / 500 | iteration 0 / 30 | Total Loss: 4.089032173156738 | KNN Loss: 3.0491456985473633 | BCE Loss: 1.039886236190796\n",
      "Epoch 411 / 500 | iteration 5 / 30 | Total Loss: 4.107063293457031 | KNN Loss: 3.0686769485473633 | BCE Loss: 1.0383861064910889\n",
      "Epoch 411 / 500 | iteration 10 / 30 | Total Loss: 4.040938377380371 | KNN Loss: 3.034181594848633 | BCE Loss: 1.0067565441131592\n",
      "Epoch 411 / 500 | iteration 15 / 30 | Total Loss: 4.133618354797363 | KNN Loss: 3.080923080444336 | BCE Loss: 1.0526955127716064\n",
      "Epoch 411 / 500 | iteration 20 / 30 | Total Loss: 4.098570346832275 | KNN Loss: 3.0575196743011475 | BCE Loss: 1.041050672531128\n",
      "Epoch 411 / 500 | iteration 25 / 30 | Total Loss: 4.038671493530273 | KNN Loss: 3.0379981994628906 | BCE Loss: 1.000673532485962\n",
      "Epoch 412 / 500 | iteration 0 / 30 | Total Loss: 4.052624225616455 | KNN Loss: 3.0355453491210938 | BCE Loss: 1.0170788764953613\n",
      "Epoch 412 / 500 | iteration 5 / 30 | Total Loss: 4.158298015594482 | KNN Loss: 3.1011552810668945 | BCE Loss: 1.057142734527588\n",
      "Epoch 412 / 500 | iteration 10 / 30 | Total Loss: 4.117740631103516 | KNN Loss: 3.0702736377716064 | BCE Loss: 1.0474668741226196\n",
      "Epoch 412 / 500 | iteration 15 / 30 | Total Loss: 4.123779296875 | KNN Loss: 3.097687244415283 | BCE Loss: 1.0260920524597168\n",
      "Epoch 412 / 500 | iteration 20 / 30 | Total Loss: 4.076624393463135 | KNN Loss: 3.0541582107543945 | BCE Loss: 1.0224661827087402\n",
      "Epoch 412 / 500 | iteration 25 / 30 | Total Loss: 4.0698628425598145 | KNN Loss: 3.063915491104126 | BCE Loss: 1.005947470664978\n",
      "Epoch 413 / 500 | iteration 0 / 30 | Total Loss: 4.094202518463135 | KNN Loss: 3.08012318611145 | BCE Loss: 1.0140793323516846\n",
      "Epoch 413 / 500 | iteration 5 / 30 | Total Loss: 4.070422172546387 | KNN Loss: 3.0394043922424316 | BCE Loss: 1.031017541885376\n",
      "Epoch 413 / 500 | iteration 10 / 30 | Total Loss: 4.064085960388184 | KNN Loss: 3.0599565505981445 | BCE Loss: 1.0041295289993286\n",
      "Epoch 413 / 500 | iteration 15 / 30 | Total Loss: 4.084068775177002 | KNN Loss: 3.05148983001709 | BCE Loss: 1.0325788259506226\n",
      "Epoch 413 / 500 | iteration 20 / 30 | Total Loss: 4.068741798400879 | KNN Loss: 3.0584704875946045 | BCE Loss: 1.0102715492248535\n",
      "Epoch 413 / 500 | iteration 25 / 30 | Total Loss: 4.06657600402832 | KNN Loss: 3.057816505432129 | BCE Loss: 1.0087594985961914\n",
      "Epoch 414 / 500 | iteration 0 / 30 | Total Loss: 4.086788654327393 | KNN Loss: 3.0520598888397217 | BCE Loss: 1.034728765487671\n",
      "Epoch 414 / 500 | iteration 5 / 30 | Total Loss: 4.098909854888916 | KNN Loss: 3.0751326084136963 | BCE Loss: 1.0237772464752197\n",
      "Epoch 414 / 500 | iteration 10 / 30 | Total Loss: 4.1259026527404785 | KNN Loss: 3.0752522945404053 | BCE Loss: 1.0506503582000732\n",
      "Epoch 414 / 500 | iteration 15 / 30 | Total Loss: 4.013450622558594 | KNN Loss: 3.0158801078796387 | BCE Loss: 0.9975703358650208\n",
      "Epoch 414 / 500 | iteration 20 / 30 | Total Loss: 4.071454048156738 | KNN Loss: 3.065418004989624 | BCE Loss: 1.0060362815856934\n",
      "Epoch 414 / 500 | iteration 25 / 30 | Total Loss: 4.0167059898376465 | KNN Loss: 3.0296390056610107 | BCE Loss: 0.9870668053627014\n",
      "Epoch 415 / 500 | iteration 0 / 30 | Total Loss: 4.099303245544434 | KNN Loss: 3.089725971221924 | BCE Loss: 1.0095772743225098\n",
      "Epoch 415 / 500 | iteration 5 / 30 | Total Loss: 4.021729469299316 | KNN Loss: 3.006772756576538 | BCE Loss: 1.0149564743041992\n",
      "Epoch 415 / 500 | iteration 10 / 30 | Total Loss: 4.118027210235596 | KNN Loss: 3.095520496368408 | BCE Loss: 1.0225067138671875\n",
      "Epoch 415 / 500 | iteration 15 / 30 | Total Loss: 4.04425573348999 | KNN Loss: 3.0496392250061035 | BCE Loss: 0.9946165680885315\n",
      "Epoch 415 / 500 | iteration 20 / 30 | Total Loss: 4.057138442993164 | KNN Loss: 3.022336483001709 | BCE Loss: 1.034801959991455\n",
      "Epoch 415 / 500 | iteration 25 / 30 | Total Loss: 4.097463607788086 | KNN Loss: 3.0703046321868896 | BCE Loss: 1.0271587371826172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 416 / 500 | iteration 0 / 30 | Total Loss: 4.081423759460449 | KNN Loss: 3.0442419052124023 | BCE Loss: 1.0371817350387573\n",
      "Epoch 416 / 500 | iteration 5 / 30 | Total Loss: 4.0857133865356445 | KNN Loss: 3.070901870727539 | BCE Loss: 1.0148117542266846\n",
      "Epoch 416 / 500 | iteration 10 / 30 | Total Loss: 4.059178829193115 | KNN Loss: 3.054871082305908 | BCE Loss: 1.004307746887207\n",
      "Epoch 416 / 500 | iteration 15 / 30 | Total Loss: 4.07711124420166 | KNN Loss: 3.0650458335876465 | BCE Loss: 1.0120652914047241\n",
      "Epoch 416 / 500 | iteration 20 / 30 | Total Loss: 4.08748722076416 | KNN Loss: 3.0678727626800537 | BCE Loss: 1.0196146965026855\n",
      "Epoch 416 / 500 | iteration 25 / 30 | Total Loss: 4.0726823806762695 | KNN Loss: 3.0769901275634766 | BCE Loss: 0.995692253112793\n",
      "Epoch 417 / 500 | iteration 0 / 30 | Total Loss: 4.075466156005859 | KNN Loss: 3.068356990814209 | BCE Loss: 1.00710928440094\n",
      "Epoch 417 / 500 | iteration 5 / 30 | Total Loss: 4.062924385070801 | KNN Loss: 3.0262625217437744 | BCE Loss: 1.0366621017456055\n",
      "Epoch 417 / 500 | iteration 10 / 30 | Total Loss: 4.098409652709961 | KNN Loss: 3.0741262435913086 | BCE Loss: 1.0242831707000732\n",
      "Epoch 417 / 500 | iteration 15 / 30 | Total Loss: 4.097570896148682 | KNN Loss: 3.063302516937256 | BCE Loss: 1.0342682600021362\n",
      "Epoch 417 / 500 | iteration 20 / 30 | Total Loss: 4.1247453689575195 | KNN Loss: 3.087235689163208 | BCE Loss: 1.0375099182128906\n",
      "Epoch 417 / 500 | iteration 25 / 30 | Total Loss: 4.084193706512451 | KNN Loss: 3.066337823867798 | BCE Loss: 1.0178557634353638\n",
      "Epoch 418 / 500 | iteration 0 / 30 | Total Loss: 4.070882797241211 | KNN Loss: 3.0595242977142334 | BCE Loss: 1.0113582611083984\n",
      "Epoch 418 / 500 | iteration 5 / 30 | Total Loss: 4.115044593811035 | KNN Loss: 3.106384515762329 | BCE Loss: 1.0086601972579956\n",
      "Epoch 418 / 500 | iteration 10 / 30 | Total Loss: 4.080195903778076 | KNN Loss: 3.081892967224121 | BCE Loss: 0.9983028769493103\n",
      "Epoch 418 / 500 | iteration 15 / 30 | Total Loss: 4.026801109313965 | KNN Loss: 3.032041072845459 | BCE Loss: 0.9947599172592163\n",
      "Epoch 418 / 500 | iteration 20 / 30 | Total Loss: 4.064202308654785 | KNN Loss: 3.039461851119995 | BCE Loss: 1.02474045753479\n",
      "Epoch 418 / 500 | iteration 25 / 30 | Total Loss: 4.106029510498047 | KNN Loss: 3.1086528301239014 | BCE Loss: 0.9973767399787903\n",
      "Epoch   419: reducing learning rate of group 0 to 3.2856e-07.\n",
      "Epoch 419 / 500 | iteration 0 / 30 | Total Loss: 4.111433982849121 | KNN Loss: 3.104278087615967 | BCE Loss: 1.0071558952331543\n",
      "Epoch 419 / 500 | iteration 5 / 30 | Total Loss: 4.089122772216797 | KNN Loss: 3.081575870513916 | BCE Loss: 1.0075466632843018\n",
      "Epoch 419 / 500 | iteration 10 / 30 | Total Loss: 4.106304168701172 | KNN Loss: 3.104336977005005 | BCE Loss: 1.001967191696167\n",
      "Epoch 419 / 500 | iteration 15 / 30 | Total Loss: 4.1124420166015625 | KNN Loss: 3.1046342849731445 | BCE Loss: 1.0078078508377075\n",
      "Epoch 419 / 500 | iteration 20 / 30 | Total Loss: 4.065788269042969 | KNN Loss: 3.078348159790039 | BCE Loss: 0.9874398708343506\n",
      "Epoch 419 / 500 | iteration 25 / 30 | Total Loss: 4.085805892944336 | KNN Loss: 3.0675013065338135 | BCE Loss: 1.0183048248291016\n",
      "Epoch 420 / 500 | iteration 0 / 30 | Total Loss: 4.1163434982299805 | KNN Loss: 3.1044764518737793 | BCE Loss: 1.0118670463562012\n",
      "Epoch 420 / 500 | iteration 5 / 30 | Total Loss: 4.100515842437744 | KNN Loss: 3.0786514282226562 | BCE Loss: 1.021864414215088\n",
      "Epoch 420 / 500 | iteration 10 / 30 | Total Loss: 4.010632514953613 | KNN Loss: 3.0342698097229004 | BCE Loss: 0.9763628840446472\n",
      "Epoch 420 / 500 | iteration 15 / 30 | Total Loss: 4.030068397521973 | KNN Loss: 3.0392143726348877 | BCE Loss: 0.9908540844917297\n",
      "Epoch 420 / 500 | iteration 20 / 30 | Total Loss: 4.079756736755371 | KNN Loss: 3.0671913623809814 | BCE Loss: 1.0125656127929688\n",
      "Epoch 420 / 500 | iteration 25 / 30 | Total Loss: 4.069774150848389 | KNN Loss: 3.0480806827545166 | BCE Loss: 1.021693468093872\n",
      "Epoch 421 / 500 | iteration 0 / 30 | Total Loss: 4.100612640380859 | KNN Loss: 3.088057041168213 | BCE Loss: 1.0125558376312256\n",
      "Epoch 421 / 500 | iteration 5 / 30 | Total Loss: 4.096790313720703 | KNN Loss: 3.0669825077056885 | BCE Loss: 1.0298075675964355\n",
      "Epoch 421 / 500 | iteration 10 / 30 | Total Loss: 4.119938850402832 | KNN Loss: 3.081890106201172 | BCE Loss: 1.0380487442016602\n",
      "Epoch 421 / 500 | iteration 15 / 30 | Total Loss: 4.112788200378418 | KNN Loss: 3.0773098468780518 | BCE Loss: 1.0354783535003662\n",
      "Epoch 421 / 500 | iteration 20 / 30 | Total Loss: 4.046552658081055 | KNN Loss: 3.054769277572632 | BCE Loss: 0.9917832016944885\n",
      "Epoch 421 / 500 | iteration 25 / 30 | Total Loss: 4.109516620635986 | KNN Loss: 3.0717010498046875 | BCE Loss: 1.0378155708312988\n",
      "Epoch 422 / 500 | iteration 0 / 30 | Total Loss: 4.061580657958984 | KNN Loss: 3.049560785293579 | BCE Loss: 1.0120196342468262\n",
      "Epoch 422 / 500 | iteration 5 / 30 | Total Loss: 4.03641414642334 | KNN Loss: 3.0306637287139893 | BCE Loss: 1.0057506561279297\n",
      "Epoch 422 / 500 | iteration 10 / 30 | Total Loss: 4.103224754333496 | KNN Loss: 3.081367015838623 | BCE Loss: 1.0218578577041626\n",
      "Epoch 422 / 500 | iteration 15 / 30 | Total Loss: 4.082497596740723 | KNN Loss: 3.0769474506378174 | BCE Loss: 1.0055503845214844\n",
      "Epoch 422 / 500 | iteration 20 / 30 | Total Loss: 4.0758867263793945 | KNN Loss: 3.055408239364624 | BCE Loss: 1.0204782485961914\n",
      "Epoch 422 / 500 | iteration 25 / 30 | Total Loss: 4.089781761169434 | KNN Loss: 3.0786139965057373 | BCE Loss: 1.0111676454544067\n",
      "Epoch 423 / 500 | iteration 0 / 30 | Total Loss: 4.043605804443359 | KNN Loss: 3.046753406524658 | BCE Loss: 0.9968525171279907\n",
      "Epoch 423 / 500 | iteration 5 / 30 | Total Loss: 4.081204414367676 | KNN Loss: 3.0816516876220703 | BCE Loss: 0.999552845954895\n",
      "Epoch 423 / 500 | iteration 10 / 30 | Total Loss: 4.126575946807861 | KNN Loss: 3.085211753845215 | BCE Loss: 1.0413641929626465\n",
      "Epoch 423 / 500 | iteration 15 / 30 | Total Loss: 4.107015609741211 | KNN Loss: 3.060075044631958 | BCE Loss: 1.046940565109253\n",
      "Epoch 423 / 500 | iteration 20 / 30 | Total Loss: 4.082388877868652 | KNN Loss: 3.063683271408081 | BCE Loss: 1.0187056064605713\n",
      "Epoch 423 / 500 | iteration 25 / 30 | Total Loss: 4.100334167480469 | KNN Loss: 3.0635814666748047 | BCE Loss: 1.036752462387085\n",
      "Epoch 424 / 500 | iteration 0 / 30 | Total Loss: 4.072495937347412 | KNN Loss: 3.076838254928589 | BCE Loss: 0.9956576824188232\n",
      "Epoch 424 / 500 | iteration 5 / 30 | Total Loss: 4.098260879516602 | KNN Loss: 3.0683627128601074 | BCE Loss: 1.029897928237915\n",
      "Epoch 424 / 500 | iteration 10 / 30 | Total Loss: 4.115188121795654 | KNN Loss: 3.056926965713501 | BCE Loss: 1.0582610368728638\n",
      "Epoch 424 / 500 | iteration 15 / 30 | Total Loss: 4.087120532989502 | KNN Loss: 3.0676097869873047 | BCE Loss: 1.0195107460021973\n",
      "Epoch 424 / 500 | iteration 20 / 30 | Total Loss: 4.083069801330566 | KNN Loss: 3.0843136310577393 | BCE Loss: 0.9987561106681824\n",
      "Epoch 424 / 500 | iteration 25 / 30 | Total Loss: 4.137407302856445 | KNN Loss: 3.0903968811035156 | BCE Loss: 1.0470101833343506\n",
      "Epoch 425 / 500 | iteration 0 / 30 | Total Loss: 4.123840808868408 | KNN Loss: 3.093407154083252 | BCE Loss: 1.0304336547851562\n",
      "Epoch 425 / 500 | iteration 5 / 30 | Total Loss: 4.057582378387451 | KNN Loss: 3.0603134632110596 | BCE Loss: 0.9972690343856812\n",
      "Epoch 425 / 500 | iteration 10 / 30 | Total Loss: 4.1327104568481445 | KNN Loss: 3.0851171016693115 | BCE Loss: 1.047593116760254\n",
      "Epoch 425 / 500 | iteration 15 / 30 | Total Loss: 4.0822014808654785 | KNN Loss: 3.073129177093506 | BCE Loss: 1.0090723037719727\n",
      "Epoch 425 / 500 | iteration 20 / 30 | Total Loss: 4.106375217437744 | KNN Loss: 3.0784687995910645 | BCE Loss: 1.0279064178466797\n",
      "Epoch 425 / 500 | iteration 25 / 30 | Total Loss: 4.063353538513184 | KNN Loss: 3.0506813526153564 | BCE Loss: 1.0126724243164062\n",
      "Epoch 426 / 500 | iteration 0 / 30 | Total Loss: 4.080223560333252 | KNN Loss: 3.0793144702911377 | BCE Loss: 1.0009090900421143\n",
      "Epoch 426 / 500 | iteration 5 / 30 | Total Loss: 4.100368976593018 | KNN Loss: 3.0521469116210938 | BCE Loss: 1.0482219457626343\n",
      "Epoch 426 / 500 | iteration 10 / 30 | Total Loss: 4.072108268737793 | KNN Loss: 3.072831153869629 | BCE Loss: 0.9992772340774536\n",
      "Epoch 426 / 500 | iteration 15 / 30 | Total Loss: 4.100411415100098 | KNN Loss: 3.0767836570739746 | BCE Loss: 1.023627519607544\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 426 / 500 | iteration 20 / 30 | Total Loss: 4.068086624145508 | KNN Loss: 3.0495188236236572 | BCE Loss: 1.018567681312561\n",
      "Epoch 426 / 500 | iteration 25 / 30 | Total Loss: 4.062498092651367 | KNN Loss: 3.049300193786621 | BCE Loss: 1.0131981372833252\n",
      "Epoch 427 / 500 | iteration 0 / 30 | Total Loss: 4.11154842376709 | KNN Loss: 3.0783779621124268 | BCE Loss: 1.0331707000732422\n",
      "Epoch 427 / 500 | iteration 5 / 30 | Total Loss: 4.056885719299316 | KNN Loss: 3.038106918334961 | BCE Loss: 1.0187785625457764\n",
      "Epoch 427 / 500 | iteration 10 / 30 | Total Loss: 4.104218006134033 | KNN Loss: 3.092028856277466 | BCE Loss: 1.012189269065857\n",
      "Epoch 427 / 500 | iteration 15 / 30 | Total Loss: 4.0536723136901855 | KNN Loss: 3.0459482669830322 | BCE Loss: 1.0077241659164429\n",
      "Epoch 427 / 500 | iteration 20 / 30 | Total Loss: 4.096477031707764 | KNN Loss: 3.077831268310547 | BCE Loss: 1.0186456441879272\n",
      "Epoch 427 / 500 | iteration 25 / 30 | Total Loss: 4.044920921325684 | KNN Loss: 3.0555672645568848 | BCE Loss: 0.9893535375595093\n",
      "Epoch 428 / 500 | iteration 0 / 30 | Total Loss: 4.095083713531494 | KNN Loss: 3.0675101280212402 | BCE Loss: 1.027573585510254\n",
      "Epoch 428 / 500 | iteration 5 / 30 | Total Loss: 4.071096420288086 | KNN Loss: 3.0403943061828613 | BCE Loss: 1.0307021141052246\n",
      "Epoch 428 / 500 | iteration 10 / 30 | Total Loss: 4.104583740234375 | KNN Loss: 3.0687146186828613 | BCE Loss: 1.0358688831329346\n",
      "Epoch 428 / 500 | iteration 15 / 30 | Total Loss: 4.063358306884766 | KNN Loss: 3.0753984451293945 | BCE Loss: 0.9879599213600159\n",
      "Epoch 428 / 500 | iteration 20 / 30 | Total Loss: 4.086844444274902 | KNN Loss: 3.074633836746216 | BCE Loss: 1.0122108459472656\n",
      "Epoch 428 / 500 | iteration 25 / 30 | Total Loss: 4.067781448364258 | KNN Loss: 3.0435967445373535 | BCE Loss: 1.0241849422454834\n",
      "Epoch 429 / 500 | iteration 0 / 30 | Total Loss: 4.138342380523682 | KNN Loss: 3.10550856590271 | BCE Loss: 1.0328336954116821\n",
      "Epoch 429 / 500 | iteration 5 / 30 | Total Loss: 4.059717655181885 | KNN Loss: 3.056226968765259 | BCE Loss: 1.0034905672073364\n",
      "Epoch 429 / 500 | iteration 10 / 30 | Total Loss: 4.029942035675049 | KNN Loss: 3.0383105278015137 | BCE Loss: 0.9916316270828247\n",
      "Epoch 429 / 500 | iteration 15 / 30 | Total Loss: 4.083062171936035 | KNN Loss: 3.0464353561401367 | BCE Loss: 1.036626935005188\n",
      "Epoch 429 / 500 | iteration 20 / 30 | Total Loss: 4.0784525871276855 | KNN Loss: 3.051889181137085 | BCE Loss: 1.026563286781311\n",
      "Epoch 429 / 500 | iteration 25 / 30 | Total Loss: 4.117929458618164 | KNN Loss: 3.0867996215820312 | BCE Loss: 1.031130075454712\n",
      "Epoch   430: reducing learning rate of group 0 to 2.2999e-07.\n",
      "Epoch 430 / 500 | iteration 0 / 30 | Total Loss: 4.08160924911499 | KNN Loss: 3.0485668182373047 | BCE Loss: 1.0330424308776855\n",
      "Epoch 430 / 500 | iteration 5 / 30 | Total Loss: 4.092432022094727 | KNN Loss: 3.0746967792510986 | BCE Loss: 1.017735481262207\n",
      "Epoch 430 / 500 | iteration 10 / 30 | Total Loss: 4.076253890991211 | KNN Loss: 3.0563180446624756 | BCE Loss: 1.0199360847473145\n",
      "Epoch 430 / 500 | iteration 15 / 30 | Total Loss: 4.086257457733154 | KNN Loss: 3.0629539489746094 | BCE Loss: 1.0233033895492554\n",
      "Epoch 430 / 500 | iteration 20 / 30 | Total Loss: 4.093242645263672 | KNN Loss: 3.0771429538726807 | BCE Loss: 1.0160996913909912\n",
      "Epoch 430 / 500 | iteration 25 / 30 | Total Loss: 4.072658538818359 | KNN Loss: 3.088268280029297 | BCE Loss: 0.9843901991844177\n",
      "Epoch 431 / 500 | iteration 0 / 30 | Total Loss: 4.059868812561035 | KNN Loss: 3.019714593887329 | BCE Loss: 1.0401544570922852\n",
      "Epoch 431 / 500 | iteration 5 / 30 | Total Loss: 4.081831932067871 | KNN Loss: 3.0746009349823 | BCE Loss: 1.0072309970855713\n",
      "Epoch 431 / 500 | iteration 10 / 30 | Total Loss: 4.10788631439209 | KNN Loss: 3.0655975341796875 | BCE Loss: 1.0422890186309814\n",
      "Epoch 431 / 500 | iteration 15 / 30 | Total Loss: 4.140398979187012 | KNN Loss: 3.097158193588257 | BCE Loss: 1.0432409048080444\n",
      "Epoch 431 / 500 | iteration 20 / 30 | Total Loss: 4.1137189865112305 | KNN Loss: 3.091653347015381 | BCE Loss: 1.0220657587051392\n",
      "Epoch 431 / 500 | iteration 25 / 30 | Total Loss: 4.0806565284729 | KNN Loss: 3.0796890258789062 | BCE Loss: 1.0009675025939941\n",
      "Epoch 432 / 500 | iteration 0 / 30 | Total Loss: 4.045287132263184 | KNN Loss: 3.029527425765991 | BCE Loss: 1.0157597064971924\n",
      "Epoch 432 / 500 | iteration 5 / 30 | Total Loss: 4.114441394805908 | KNN Loss: 3.071918249130249 | BCE Loss: 1.0425231456756592\n",
      "Epoch 432 / 500 | iteration 10 / 30 | Total Loss: 4.052109718322754 | KNN Loss: 3.0350635051727295 | BCE Loss: 1.0170459747314453\n",
      "Epoch 432 / 500 | iteration 15 / 30 | Total Loss: 4.063892364501953 | KNN Loss: 3.0262467861175537 | BCE Loss: 1.0376454591751099\n",
      "Epoch 432 / 500 | iteration 20 / 30 | Total Loss: 4.102869987487793 | KNN Loss: 3.0715370178222656 | BCE Loss: 1.0313327312469482\n",
      "Epoch 432 / 500 | iteration 25 / 30 | Total Loss: 4.067037105560303 | KNN Loss: 3.0712029933929443 | BCE Loss: 0.995834231376648\n",
      "Epoch 433 / 500 | iteration 0 / 30 | Total Loss: 4.056947231292725 | KNN Loss: 3.043057441711426 | BCE Loss: 1.0138897895812988\n",
      "Epoch 433 / 500 | iteration 5 / 30 | Total Loss: 4.117611885070801 | KNN Loss: 3.0806643962860107 | BCE Loss: 1.0369476079940796\n",
      "Epoch 433 / 500 | iteration 10 / 30 | Total Loss: 4.046339988708496 | KNN Loss: 3.0428292751312256 | BCE Loss: 1.0035107135772705\n",
      "Epoch 433 / 500 | iteration 15 / 30 | Total Loss: 4.057330131530762 | KNN Loss: 3.041058301925659 | BCE Loss: 1.0162715911865234\n",
      "Epoch 433 / 500 | iteration 20 / 30 | Total Loss: 4.071919918060303 | KNN Loss: 3.0457067489624023 | BCE Loss: 1.0262131690979004\n",
      "Epoch 433 / 500 | iteration 25 / 30 | Total Loss: 4.069957733154297 | KNN Loss: 3.033629894256592 | BCE Loss: 1.036327838897705\n",
      "Epoch 434 / 500 | iteration 0 / 30 | Total Loss: 4.1555585861206055 | KNN Loss: 3.112377882003784 | BCE Loss: 1.0431809425354004\n",
      "Epoch 434 / 500 | iteration 5 / 30 | Total Loss: 3.995394706726074 | KNN Loss: 3.0174968242645264 | BCE Loss: 0.9778978824615479\n",
      "Epoch 434 / 500 | iteration 10 / 30 | Total Loss: 4.075209140777588 | KNN Loss: 3.05961537361145 | BCE Loss: 1.0155937671661377\n",
      "Epoch 434 / 500 | iteration 15 / 30 | Total Loss: 4.104607582092285 | KNN Loss: 3.064549446105957 | BCE Loss: 1.040057897567749\n",
      "Epoch 434 / 500 | iteration 20 / 30 | Total Loss: 4.082665920257568 | KNN Loss: 3.0692672729492188 | BCE Loss: 1.01339852809906\n",
      "Epoch 434 / 500 | iteration 25 / 30 | Total Loss: 4.081759452819824 | KNN Loss: 3.0474846363067627 | BCE Loss: 1.0342745780944824\n",
      "Epoch 435 / 500 | iteration 0 / 30 | Total Loss: 4.094612121582031 | KNN Loss: 3.0701544284820557 | BCE Loss: 1.0244574546813965\n",
      "Epoch 435 / 500 | iteration 5 / 30 | Total Loss: 4.093526840209961 | KNN Loss: 3.078148603439331 | BCE Loss: 1.0153783559799194\n",
      "Epoch 435 / 500 | iteration 10 / 30 | Total Loss: 4.051738739013672 | KNN Loss: 3.056001901626587 | BCE Loss: 0.9957369565963745\n",
      "Epoch 435 / 500 | iteration 15 / 30 | Total Loss: 4.113152980804443 | KNN Loss: 3.0661330223083496 | BCE Loss: 1.0470199584960938\n",
      "Epoch 435 / 500 | iteration 20 / 30 | Total Loss: 4.106095314025879 | KNN Loss: 3.088045597076416 | BCE Loss: 1.018049716949463\n",
      "Epoch 435 / 500 | iteration 25 / 30 | Total Loss: 4.08108377456665 | KNN Loss: 3.0827178955078125 | BCE Loss: 0.9983659386634827\n",
      "Epoch 436 / 500 | iteration 0 / 30 | Total Loss: 4.06955099105835 | KNN Loss: 3.0771217346191406 | BCE Loss: 0.992429256439209\n",
      "Epoch 436 / 500 | iteration 5 / 30 | Total Loss: 4.07867431640625 | KNN Loss: 3.0616538524627686 | BCE Loss: 1.0170202255249023\n",
      "Epoch 436 / 500 | iteration 10 / 30 | Total Loss: 4.102027893066406 | KNN Loss: 3.056795120239258 | BCE Loss: 1.0452326536178589\n",
      "Epoch 436 / 500 | iteration 15 / 30 | Total Loss: 4.119915008544922 | KNN Loss: 3.078490734100342 | BCE Loss: 1.0414245128631592\n",
      "Epoch 436 / 500 | iteration 20 / 30 | Total Loss: 4.069854259490967 | KNN Loss: 3.066908359527588 | BCE Loss: 1.002945899963379\n",
      "Epoch 436 / 500 | iteration 25 / 30 | Total Loss: 4.045827388763428 | KNN Loss: 3.0488579273223877 | BCE Loss: 0.99696946144104\n",
      "Epoch 437 / 500 | iteration 0 / 30 | Total Loss: 4.1054863929748535 | KNN Loss: 3.107905864715576 | BCE Loss: 0.9975804090499878\n",
      "Epoch 437 / 500 | iteration 5 / 30 | Total Loss: 4.0790228843688965 | KNN Loss: 3.0594584941864014 | BCE Loss: 1.0195643901824951\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 437 / 500 | iteration 10 / 30 | Total Loss: 4.0638628005981445 | KNN Loss: 3.057809591293335 | BCE Loss: 1.0060529708862305\n",
      "Epoch 437 / 500 | iteration 15 / 30 | Total Loss: 4.075961589813232 | KNN Loss: 3.0576694011688232 | BCE Loss: 1.0182920694351196\n",
      "Epoch 437 / 500 | iteration 20 / 30 | Total Loss: 4.070868015289307 | KNN Loss: 3.0687456130981445 | BCE Loss: 1.002122402191162\n",
      "Epoch 437 / 500 | iteration 25 / 30 | Total Loss: 4.07330322265625 | KNN Loss: 3.0627620220184326 | BCE Loss: 1.0105412006378174\n",
      "Epoch 438 / 500 | iteration 0 / 30 | Total Loss: 4.084773540496826 | KNN Loss: 3.057629346847534 | BCE Loss: 1.0271443128585815\n",
      "Epoch 438 / 500 | iteration 5 / 30 | Total Loss: 4.05793571472168 | KNN Loss: 3.0460476875305176 | BCE Loss: 1.011888027191162\n",
      "Epoch 438 / 500 | iteration 10 / 30 | Total Loss: 4.082894802093506 | KNN Loss: 3.0746188163757324 | BCE Loss: 1.0082759857177734\n",
      "Epoch 438 / 500 | iteration 15 / 30 | Total Loss: 4.098641395568848 | KNN Loss: 3.077237129211426 | BCE Loss: 1.0214040279388428\n",
      "Epoch 438 / 500 | iteration 20 / 30 | Total Loss: 4.0962395668029785 | KNN Loss: 3.1070809364318848 | BCE Loss: 0.9891584515571594\n",
      "Epoch 438 / 500 | iteration 25 / 30 | Total Loss: 4.071503162384033 | KNN Loss: 3.07249116897583 | BCE Loss: 0.9990118741989136\n",
      "Epoch 439 / 500 | iteration 0 / 30 | Total Loss: 4.066761016845703 | KNN Loss: 3.0371499061584473 | BCE Loss: 1.0296109914779663\n",
      "Epoch 439 / 500 | iteration 5 / 30 | Total Loss: 4.101195335388184 | KNN Loss: 3.0965073108673096 | BCE Loss: 1.0046879053115845\n",
      "Epoch 439 / 500 | iteration 10 / 30 | Total Loss: 4.058386325836182 | KNN Loss: 3.048274278640747 | BCE Loss: 1.0101121664047241\n",
      "Epoch 439 / 500 | iteration 15 / 30 | Total Loss: 4.15908145904541 | KNN Loss: 3.1193783283233643 | BCE Loss: 1.039703369140625\n",
      "Epoch 439 / 500 | iteration 20 / 30 | Total Loss: 4.022879123687744 | KNN Loss: 3.0213871002197266 | BCE Loss: 1.0014921426773071\n",
      "Epoch 439 / 500 | iteration 25 / 30 | Total Loss: 4.080578327178955 | KNN Loss: 3.050916910171509 | BCE Loss: 1.0296614170074463\n",
      "Epoch 440 / 500 | iteration 0 / 30 | Total Loss: 4.060095310211182 | KNN Loss: 3.0551884174346924 | BCE Loss: 1.0049070119857788\n",
      "Epoch 440 / 500 | iteration 5 / 30 | Total Loss: 4.065382480621338 | KNN Loss: 3.0430965423583984 | BCE Loss: 1.0222859382629395\n",
      "Epoch 440 / 500 | iteration 10 / 30 | Total Loss: 4.13301420211792 | KNN Loss: 3.0810177326202393 | BCE Loss: 1.0519963502883911\n",
      "Epoch 440 / 500 | iteration 15 / 30 | Total Loss: 4.105474948883057 | KNN Loss: 3.0789897441864014 | BCE Loss: 1.0264852046966553\n",
      "Epoch 440 / 500 | iteration 20 / 30 | Total Loss: 4.106697082519531 | KNN Loss: 3.0753188133239746 | BCE Loss: 1.0313785076141357\n",
      "Epoch 440 / 500 | iteration 25 / 30 | Total Loss: 4.046549320220947 | KNN Loss: 3.0415596961975098 | BCE Loss: 1.0049896240234375\n",
      "Epoch   441: reducing learning rate of group 0 to 1.6100e-07.\n",
      "Epoch 441 / 500 | iteration 0 / 30 | Total Loss: 4.135141372680664 | KNN Loss: 3.0877492427825928 | BCE Loss: 1.0473920106887817\n",
      "Epoch 441 / 500 | iteration 5 / 30 | Total Loss: 4.040619850158691 | KNN Loss: 3.041010856628418 | BCE Loss: 0.9996089935302734\n",
      "Epoch 441 / 500 | iteration 10 / 30 | Total Loss: 4.090203762054443 | KNN Loss: 3.053208827972412 | BCE Loss: 1.0369949340820312\n",
      "Epoch 441 / 500 | iteration 15 / 30 | Total Loss: 4.119606971740723 | KNN Loss: 3.1065080165863037 | BCE Loss: 1.0130990743637085\n",
      "Epoch 441 / 500 | iteration 20 / 30 | Total Loss: 4.075001239776611 | KNN Loss: 3.044106960296631 | BCE Loss: 1.03089439868927\n",
      "Epoch 441 / 500 | iteration 25 / 30 | Total Loss: 4.0784406661987305 | KNN Loss: 3.0412142276763916 | BCE Loss: 1.0372263193130493\n",
      "Epoch 442 / 500 | iteration 0 / 30 | Total Loss: 4.117773532867432 | KNN Loss: 3.100700616836548 | BCE Loss: 1.0170727968215942\n",
      "Epoch 442 / 500 | iteration 5 / 30 | Total Loss: 4.066431045532227 | KNN Loss: 3.0603413581848145 | BCE Loss: 1.0060899257659912\n",
      "Epoch 442 / 500 | iteration 10 / 30 | Total Loss: 4.08880615234375 | KNN Loss: 3.072800636291504 | BCE Loss: 1.016005516052246\n",
      "Epoch 442 / 500 | iteration 15 / 30 | Total Loss: 4.121253967285156 | KNN Loss: 3.0615735054016113 | BCE Loss: 1.059680700302124\n",
      "Epoch 442 / 500 | iteration 20 / 30 | Total Loss: 4.058058261871338 | KNN Loss: 3.043674945831299 | BCE Loss: 1.014383316040039\n",
      "Epoch 442 / 500 | iteration 25 / 30 | Total Loss: 4.086694240570068 | KNN Loss: 3.0726125240325928 | BCE Loss: 1.014081597328186\n",
      "Epoch 443 / 500 | iteration 0 / 30 | Total Loss: 4.090726375579834 | KNN Loss: 3.0829222202301025 | BCE Loss: 1.0078041553497314\n",
      "Epoch 443 / 500 | iteration 5 / 30 | Total Loss: 4.074440002441406 | KNN Loss: 3.0576071739196777 | BCE Loss: 1.016832709312439\n",
      "Epoch 443 / 500 | iteration 10 / 30 | Total Loss: 4.144408702850342 | KNN Loss: 3.111279249191284 | BCE Loss: 1.0331294536590576\n",
      "Epoch 443 / 500 | iteration 15 / 30 | Total Loss: 4.090673446655273 | KNN Loss: 3.039872646331787 | BCE Loss: 1.0508005619049072\n",
      "Epoch 443 / 500 | iteration 20 / 30 | Total Loss: 4.074555397033691 | KNN Loss: 3.065054178237915 | BCE Loss: 1.0095014572143555\n",
      "Epoch 443 / 500 | iteration 25 / 30 | Total Loss: 4.058692455291748 | KNN Loss: 3.05489444732666 | BCE Loss: 1.0037981271743774\n",
      "Epoch 444 / 500 | iteration 0 / 30 | Total Loss: 4.101431846618652 | KNN Loss: 3.0867111682891846 | BCE Loss: 1.0147206783294678\n",
      "Epoch 444 / 500 | iteration 5 / 30 | Total Loss: 4.106529235839844 | KNN Loss: 3.1016719341278076 | BCE Loss: 1.0048573017120361\n",
      "Epoch 444 / 500 | iteration 10 / 30 | Total Loss: 4.0970940589904785 | KNN Loss: 3.0892245769500732 | BCE Loss: 1.0078694820404053\n",
      "Epoch 444 / 500 | iteration 15 / 30 | Total Loss: 4.094974517822266 | KNN Loss: 3.067190647125244 | BCE Loss: 1.0277841091156006\n",
      "Epoch 444 / 500 | iteration 20 / 30 | Total Loss: 4.055727481842041 | KNN Loss: 3.047281265258789 | BCE Loss: 1.008446216583252\n",
      "Epoch 444 / 500 | iteration 25 / 30 | Total Loss: 4.118361473083496 | KNN Loss: 3.08310866355896 | BCE Loss: 1.035252571105957\n",
      "Epoch 445 / 500 | iteration 0 / 30 | Total Loss: 4.077122688293457 | KNN Loss: 3.059361219406128 | BCE Loss: 1.01776123046875\n",
      "Epoch 445 / 500 | iteration 5 / 30 | Total Loss: 4.073394298553467 | KNN Loss: 3.06135630607605 | BCE Loss: 1.0120378732681274\n",
      "Epoch 445 / 500 | iteration 10 / 30 | Total Loss: 4.124789237976074 | KNN Loss: 3.072512149810791 | BCE Loss: 1.0522773265838623\n",
      "Epoch 445 / 500 | iteration 15 / 30 | Total Loss: 4.089575290679932 | KNN Loss: 3.08573317527771 | BCE Loss: 1.0038422346115112\n",
      "Epoch 445 / 500 | iteration 20 / 30 | Total Loss: 4.0820441246032715 | KNN Loss: 3.04349422454834 | BCE Loss: 1.038549780845642\n",
      "Epoch 445 / 500 | iteration 25 / 30 | Total Loss: 4.116952896118164 | KNN Loss: 3.082059144973755 | BCE Loss: 1.0348936319351196\n",
      "Epoch 446 / 500 | iteration 0 / 30 | Total Loss: 4.092304706573486 | KNN Loss: 3.0692460536956787 | BCE Loss: 1.023058533668518\n",
      "Epoch 446 / 500 | iteration 5 / 30 | Total Loss: 4.048705101013184 | KNN Loss: 3.0420098304748535 | BCE Loss: 1.00669527053833\n",
      "Epoch 446 / 500 | iteration 10 / 30 | Total Loss: 4.091025352478027 | KNN Loss: 3.0745186805725098 | BCE Loss: 1.0165066719055176\n",
      "Epoch 446 / 500 | iteration 15 / 30 | Total Loss: 4.097201824188232 | KNN Loss: 3.0734899044036865 | BCE Loss: 1.0237118005752563\n",
      "Epoch 446 / 500 | iteration 20 / 30 | Total Loss: 4.091744899749756 | KNN Loss: 3.0590877532958984 | BCE Loss: 1.032657265663147\n",
      "Epoch 446 / 500 | iteration 25 / 30 | Total Loss: 4.094376564025879 | KNN Loss: 3.0687754154205322 | BCE Loss: 1.0256009101867676\n",
      "Epoch 447 / 500 | iteration 0 / 30 | Total Loss: 4.059187889099121 | KNN Loss: 3.0399811267852783 | BCE Loss: 1.0192067623138428\n",
      "Epoch 447 / 500 | iteration 5 / 30 | Total Loss: 4.070293426513672 | KNN Loss: 3.07411527633667 | BCE Loss: 0.9961780905723572\n",
      "Epoch 447 / 500 | iteration 10 / 30 | Total Loss: 4.123976707458496 | KNN Loss: 3.096625804901123 | BCE Loss: 1.0273510217666626\n",
      "Epoch 447 / 500 | iteration 15 / 30 | Total Loss: 4.074434757232666 | KNN Loss: 3.0677552223205566 | BCE Loss: 1.0066794157028198\n",
      "Epoch 447 / 500 | iteration 20 / 30 | Total Loss: 4.033239364624023 | KNN Loss: 3.0479233264923096 | BCE Loss: 0.985316276550293\n",
      "Epoch 447 / 500 | iteration 25 / 30 | Total Loss: 4.085616111755371 | KNN Loss: 3.0571939945220947 | BCE Loss: 1.028422236442566\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 448 / 500 | iteration 0 / 30 | Total Loss: 4.125553131103516 | KNN Loss: 3.0818722248077393 | BCE Loss: 1.0436809062957764\n",
      "Epoch 448 / 500 | iteration 5 / 30 | Total Loss: 4.056644439697266 | KNN Loss: 3.0316333770751953 | BCE Loss: 1.0250113010406494\n",
      "Epoch 448 / 500 | iteration 10 / 30 | Total Loss: 4.055647373199463 | KNN Loss: 3.04300594329834 | BCE Loss: 1.012641429901123\n",
      "Epoch 448 / 500 | iteration 15 / 30 | Total Loss: 4.073719024658203 | KNN Loss: 3.0572991371154785 | BCE Loss: 1.0164196491241455\n",
      "Epoch 448 / 500 | iteration 20 / 30 | Total Loss: 4.058377265930176 | KNN Loss: 3.069915533065796 | BCE Loss: 0.9884615540504456\n",
      "Epoch 448 / 500 | iteration 25 / 30 | Total Loss: 4.1179118156433105 | KNN Loss: 3.0713958740234375 | BCE Loss: 1.0465160608291626\n",
      "Epoch 449 / 500 | iteration 0 / 30 | Total Loss: 4.121257305145264 | KNN Loss: 3.0971975326538086 | BCE Loss: 1.024059772491455\n",
      "Epoch 449 / 500 | iteration 5 / 30 | Total Loss: 4.09030818939209 | KNN Loss: 3.055854558944702 | BCE Loss: 1.0344538688659668\n",
      "Epoch 449 / 500 | iteration 10 / 30 | Total Loss: 4.0756144523620605 | KNN Loss: 3.0624706745147705 | BCE Loss: 1.01314377784729\n",
      "Epoch 449 / 500 | iteration 15 / 30 | Total Loss: 4.085545063018799 | KNN Loss: 3.066091775894165 | BCE Loss: 1.0194531679153442\n",
      "Epoch 449 / 500 | iteration 20 / 30 | Total Loss: 4.0886945724487305 | KNN Loss: 3.06487774848938 | BCE Loss: 1.0238170623779297\n",
      "Epoch 449 / 500 | iteration 25 / 30 | Total Loss: 4.026939392089844 | KNN Loss: 3.0325000286102295 | BCE Loss: 0.9944391846656799\n",
      "Epoch 450 / 500 | iteration 0 / 30 | Total Loss: 4.058349609375 | KNN Loss: 3.043910026550293 | BCE Loss: 1.014439344406128\n",
      "Epoch 450 / 500 | iteration 5 / 30 | Total Loss: 4.044529438018799 | KNN Loss: 3.0467591285705566 | BCE Loss: 0.9977704882621765\n",
      "Epoch 450 / 500 | iteration 10 / 30 | Total Loss: 4.0734758377075195 | KNN Loss: 3.0661697387695312 | BCE Loss: 1.0073060989379883\n",
      "Epoch 450 / 500 | iteration 15 / 30 | Total Loss: 4.0721940994262695 | KNN Loss: 3.056678295135498 | BCE Loss: 1.0155158042907715\n",
      "Epoch 450 / 500 | iteration 20 / 30 | Total Loss: 4.053215026855469 | KNN Loss: 3.036858320236206 | BCE Loss: 1.0163564682006836\n",
      "Epoch 450 / 500 | iteration 25 / 30 | Total Loss: 4.08635950088501 | KNN Loss: 3.051812171936035 | BCE Loss: 1.034547209739685\n",
      "Epoch 451 / 500 | iteration 0 / 30 | Total Loss: 4.075711727142334 | KNN Loss: 3.0358123779296875 | BCE Loss: 1.0398993492126465\n",
      "Epoch 451 / 500 | iteration 5 / 30 | Total Loss: 4.058343410491943 | KNN Loss: 3.0311508178710938 | BCE Loss: 1.0271925926208496\n",
      "Epoch 451 / 500 | iteration 10 / 30 | Total Loss: 4.0614423751831055 | KNN Loss: 3.0478408336639404 | BCE Loss: 1.0136014223098755\n",
      "Epoch 451 / 500 | iteration 15 / 30 | Total Loss: 4.067647933959961 | KNN Loss: 3.044081926345825 | BCE Loss: 1.0235660076141357\n",
      "Epoch 451 / 500 | iteration 20 / 30 | Total Loss: 4.12373161315918 | KNN Loss: 3.099637508392334 | BCE Loss: 1.0240938663482666\n",
      "Epoch 451 / 500 | iteration 25 / 30 | Total Loss: 4.097192764282227 | KNN Loss: 3.0829203128814697 | BCE Loss: 1.014272689819336\n",
      "Epoch   452: reducing learning rate of group 0 to 1.1270e-07.\n",
      "Epoch 452 / 500 | iteration 0 / 30 | Total Loss: 4.113351345062256 | KNN Loss: 3.083425521850586 | BCE Loss: 1.02992582321167\n",
      "Epoch 452 / 500 | iteration 5 / 30 | Total Loss: 4.123966217041016 | KNN Loss: 3.113729476928711 | BCE Loss: 1.0102365016937256\n",
      "Epoch 452 / 500 | iteration 10 / 30 | Total Loss: 4.0700812339782715 | KNN Loss: 3.058926582336426 | BCE Loss: 1.0111546516418457\n",
      "Epoch 452 / 500 | iteration 15 / 30 | Total Loss: 4.083508491516113 | KNN Loss: 3.0538384914398193 | BCE Loss: 1.0296697616577148\n",
      "Epoch 452 / 500 | iteration 20 / 30 | Total Loss: 4.078202247619629 | KNN Loss: 3.0522305965423584 | BCE Loss: 1.0259718894958496\n",
      "Epoch 452 / 500 | iteration 25 / 30 | Total Loss: 4.133937835693359 | KNN Loss: 3.092146873474121 | BCE Loss: 1.0417909622192383\n",
      "Epoch 453 / 500 | iteration 0 / 30 | Total Loss: 4.087700843811035 | KNN Loss: 3.0822622776031494 | BCE Loss: 1.0054386854171753\n",
      "Epoch 453 / 500 | iteration 5 / 30 | Total Loss: 4.095818519592285 | KNN Loss: 3.099133253097534 | BCE Loss: 0.9966850280761719\n",
      "Epoch 453 / 500 | iteration 10 / 30 | Total Loss: 4.106442451477051 | KNN Loss: 3.0862789154052734 | BCE Loss: 1.0201637744903564\n",
      "Epoch 453 / 500 | iteration 15 / 30 | Total Loss: 4.108104228973389 | KNN Loss: 3.0987203121185303 | BCE Loss: 1.0093839168548584\n",
      "Epoch 453 / 500 | iteration 20 / 30 | Total Loss: 4.080671310424805 | KNN Loss: 3.056619167327881 | BCE Loss: 1.0240519046783447\n",
      "Epoch 453 / 500 | iteration 25 / 30 | Total Loss: 4.060939788818359 | KNN Loss: 3.05745530128479 | BCE Loss: 1.0034842491149902\n",
      "Epoch 454 / 500 | iteration 0 / 30 | Total Loss: 4.138178825378418 | KNN Loss: 3.084913492202759 | BCE Loss: 1.05326509475708\n",
      "Epoch 454 / 500 | iteration 5 / 30 | Total Loss: 4.02963399887085 | KNN Loss: 3.0451242923736572 | BCE Loss: 0.9845095872879028\n",
      "Epoch 454 / 500 | iteration 10 / 30 | Total Loss: 4.040769100189209 | KNN Loss: 3.03244686126709 | BCE Loss: 1.0083222389221191\n",
      "Epoch 454 / 500 | iteration 15 / 30 | Total Loss: 4.068530082702637 | KNN Loss: 3.0469284057617188 | BCE Loss: 1.0216014385223389\n",
      "Epoch 454 / 500 | iteration 20 / 30 | Total Loss: 4.087923049926758 | KNN Loss: 3.0604965686798096 | BCE Loss: 1.0274262428283691\n",
      "Epoch 454 / 500 | iteration 25 / 30 | Total Loss: 4.083878517150879 | KNN Loss: 3.086223602294922 | BCE Loss: 0.997654914855957\n",
      "Epoch 455 / 500 | iteration 0 / 30 | Total Loss: 4.0581159591674805 | KNN Loss: 3.0454819202423096 | BCE Loss: 1.0126339197158813\n",
      "Epoch 455 / 500 | iteration 5 / 30 | Total Loss: 4.072885513305664 | KNN Loss: 3.045239210128784 | BCE Loss: 1.0276464223861694\n",
      "Epoch 455 / 500 | iteration 10 / 30 | Total Loss: 4.077982425689697 | KNN Loss: 3.0540997982025146 | BCE Loss: 1.0238826274871826\n",
      "Epoch 455 / 500 | iteration 15 / 30 | Total Loss: 4.059292316436768 | KNN Loss: 3.0758566856384277 | BCE Loss: 0.9834354519844055\n",
      "Epoch 455 / 500 | iteration 20 / 30 | Total Loss: 4.122507095336914 | KNN Loss: 3.092219829559326 | BCE Loss: 1.030287265777588\n",
      "Epoch 455 / 500 | iteration 25 / 30 | Total Loss: 4.053760051727295 | KNN Loss: 3.044543504714966 | BCE Loss: 1.0092166662216187\n",
      "Epoch 456 / 500 | iteration 0 / 30 | Total Loss: 4.086763381958008 | KNN Loss: 3.0637001991271973 | BCE Loss: 1.0230629444122314\n",
      "Epoch 456 / 500 | iteration 5 / 30 | Total Loss: 4.021492004394531 | KNN Loss: 3.0414888858795166 | BCE Loss: 0.9800029993057251\n",
      "Epoch 456 / 500 | iteration 10 / 30 | Total Loss: 4.05554723739624 | KNN Loss: 3.02622389793396 | BCE Loss: 1.0293233394622803\n",
      "Epoch 456 / 500 | iteration 15 / 30 | Total Loss: 4.093600273132324 | KNN Loss: 3.0630481243133545 | BCE Loss: 1.0305523872375488\n",
      "Epoch 456 / 500 | iteration 20 / 30 | Total Loss: 4.03074312210083 | KNN Loss: 3.0349202156066895 | BCE Loss: 0.9958229064941406\n",
      "Epoch 456 / 500 | iteration 25 / 30 | Total Loss: 4.090546607971191 | KNN Loss: 3.0436229705810547 | BCE Loss: 1.0469237565994263\n",
      "Epoch 457 / 500 | iteration 0 / 30 | Total Loss: 4.089631080627441 | KNN Loss: 3.056272029876709 | BCE Loss: 1.0333588123321533\n",
      "Epoch 457 / 500 | iteration 5 / 30 | Total Loss: 4.057525157928467 | KNN Loss: 3.0654170513153076 | BCE Loss: 0.9921082258224487\n",
      "Epoch 457 / 500 | iteration 10 / 30 | Total Loss: 4.055933952331543 | KNN Loss: 3.0571343898773193 | BCE Loss: 0.9987994432449341\n",
      "Epoch 457 / 500 | iteration 15 / 30 | Total Loss: 4.057363510131836 | KNN Loss: 3.0537052154541016 | BCE Loss: 1.0036580562591553\n",
      "Epoch 457 / 500 | iteration 20 / 30 | Total Loss: 4.079906940460205 | KNN Loss: 3.0635101795196533 | BCE Loss: 1.0163967609405518\n",
      "Epoch 457 / 500 | iteration 25 / 30 | Total Loss: 4.047883033752441 | KNN Loss: 3.0327837467193604 | BCE Loss: 1.015099287033081\n",
      "Epoch 458 / 500 | iteration 0 / 30 | Total Loss: 4.051384925842285 | KNN Loss: 3.040393829345703 | BCE Loss: 1.0109913349151611\n",
      "Epoch 458 / 500 | iteration 5 / 30 | Total Loss: 4.0964674949646 | KNN Loss: 3.0526933670043945 | BCE Loss: 1.043774127960205\n",
      "Epoch 458 / 500 | iteration 10 / 30 | Total Loss: 4.084017753601074 | KNN Loss: 3.0654308795928955 | BCE Loss: 1.0185871124267578\n",
      "Epoch 458 / 500 | iteration 15 / 30 | Total Loss: 4.084392547607422 | KNN Loss: 3.045731544494629 | BCE Loss: 1.038661003112793\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 458 / 500 | iteration 20 / 30 | Total Loss: 4.085227966308594 | KNN Loss: 3.048189640045166 | BCE Loss: 1.0370380878448486\n",
      "Epoch 458 / 500 | iteration 25 / 30 | Total Loss: 4.100359916687012 | KNN Loss: 3.0805132389068604 | BCE Loss: 1.019846796989441\n",
      "Epoch 459 / 500 | iteration 0 / 30 | Total Loss: 4.06060791015625 | KNN Loss: 3.0455482006073 | BCE Loss: 1.0150599479675293\n",
      "Epoch 459 / 500 | iteration 5 / 30 | Total Loss: 4.077927589416504 | KNN Loss: 3.0534939765930176 | BCE Loss: 1.0244338512420654\n",
      "Epoch 459 / 500 | iteration 10 / 30 | Total Loss: 4.124232769012451 | KNN Loss: 3.058230400085449 | BCE Loss: 1.066002368927002\n",
      "Epoch 459 / 500 | iteration 15 / 30 | Total Loss: 4.094308376312256 | KNN Loss: 3.0752480030059814 | BCE Loss: 1.0190603733062744\n",
      "Epoch 459 / 500 | iteration 20 / 30 | Total Loss: 4.099851608276367 | KNN Loss: 3.10188627243042 | BCE Loss: 0.9979653358459473\n",
      "Epoch 459 / 500 | iteration 25 / 30 | Total Loss: 4.0859375 | KNN Loss: 3.0637705326080322 | BCE Loss: 1.0221670866012573\n",
      "Epoch 460 / 500 | iteration 0 / 30 | Total Loss: 4.117051601409912 | KNN Loss: 3.10395884513855 | BCE Loss: 1.0130928754806519\n",
      "Epoch 460 / 500 | iteration 5 / 30 | Total Loss: 4.066559791564941 | KNN Loss: 3.0565223693847656 | BCE Loss: 1.0100373029708862\n",
      "Epoch 460 / 500 | iteration 10 / 30 | Total Loss: 4.062382221221924 | KNN Loss: 3.0498883724212646 | BCE Loss: 1.0124938488006592\n",
      "Epoch 460 / 500 | iteration 15 / 30 | Total Loss: 4.146315097808838 | KNN Loss: 3.098881721496582 | BCE Loss: 1.0474333763122559\n",
      "Epoch 460 / 500 | iteration 20 / 30 | Total Loss: 4.076967239379883 | KNN Loss: 3.062014102935791 | BCE Loss: 1.0149528980255127\n",
      "Epoch 460 / 500 | iteration 25 / 30 | Total Loss: 4.091833114624023 | KNN Loss: 3.094668388366699 | BCE Loss: 0.9971648454666138\n",
      "Epoch 461 / 500 | iteration 0 / 30 | Total Loss: 4.1038594245910645 | KNN Loss: 3.0777029991149902 | BCE Loss: 1.0261564254760742\n",
      "Epoch 461 / 500 | iteration 5 / 30 | Total Loss: 4.089000701904297 | KNN Loss: 3.069570302963257 | BCE Loss: 1.0194306373596191\n",
      "Epoch 461 / 500 | iteration 10 / 30 | Total Loss: 4.089025497436523 | KNN Loss: 3.0560288429260254 | BCE Loss: 1.0329968929290771\n",
      "Epoch 461 / 500 | iteration 15 / 30 | Total Loss: 4.097344875335693 | KNN Loss: 3.069537401199341 | BCE Loss: 1.0278074741363525\n",
      "Epoch 461 / 500 | iteration 20 / 30 | Total Loss: 4.0991315841674805 | KNN Loss: 3.059668779373169 | BCE Loss: 1.0394630432128906\n",
      "Epoch 461 / 500 | iteration 25 / 30 | Total Loss: 4.008000373840332 | KNN Loss: 3.012770652770996 | BCE Loss: 0.995229959487915\n",
      "Epoch 462 / 500 | iteration 0 / 30 | Total Loss: 4.05026912689209 | KNN Loss: 3.040032148361206 | BCE Loss: 1.010237216949463\n",
      "Epoch 462 / 500 | iteration 5 / 30 | Total Loss: 4.093687057495117 | KNN Loss: 3.0657360553741455 | BCE Loss: 1.0279512405395508\n",
      "Epoch 462 / 500 | iteration 10 / 30 | Total Loss: 4.128415584564209 | KNN Loss: 3.1089608669281006 | BCE Loss: 1.019454836845398\n",
      "Epoch 462 / 500 | iteration 15 / 30 | Total Loss: 4.086113452911377 | KNN Loss: 3.048980236053467 | BCE Loss: 1.0371332168579102\n",
      "Epoch 462 / 500 | iteration 20 / 30 | Total Loss: 4.103277206420898 | KNN Loss: 3.074183225631714 | BCE Loss: 1.029093861579895\n",
      "Epoch 462 / 500 | iteration 25 / 30 | Total Loss: 4.101076602935791 | KNN Loss: 3.0475270748138428 | BCE Loss: 1.0535495281219482\n",
      "Epoch   463: reducing learning rate of group 0 to 7.8888e-08.\n",
      "Epoch 463 / 500 | iteration 0 / 30 | Total Loss: 4.096235752105713 | KNN Loss: 3.060067892074585 | BCE Loss: 1.0361679792404175\n",
      "Epoch 463 / 500 | iteration 5 / 30 | Total Loss: 4.03475284576416 | KNN Loss: 3.0415353775024414 | BCE Loss: 0.9932172298431396\n",
      "Epoch 463 / 500 | iteration 10 / 30 | Total Loss: 4.119567394256592 | KNN Loss: 3.078094720840454 | BCE Loss: 1.0414726734161377\n",
      "Epoch 463 / 500 | iteration 15 / 30 | Total Loss: 4.081521987915039 | KNN Loss: 3.0511748790740967 | BCE Loss: 1.0303469896316528\n",
      "Epoch 463 / 500 | iteration 20 / 30 | Total Loss: 4.122415542602539 | KNN Loss: 3.0799474716186523 | BCE Loss: 1.0424680709838867\n",
      "Epoch 463 / 500 | iteration 25 / 30 | Total Loss: 4.094025611877441 | KNN Loss: 3.084723949432373 | BCE Loss: 1.0093015432357788\n",
      "Epoch 464 / 500 | iteration 0 / 30 | Total Loss: 4.104599952697754 | KNN Loss: 3.069241762161255 | BCE Loss: 1.035358190536499\n",
      "Epoch 464 / 500 | iteration 5 / 30 | Total Loss: 4.065269470214844 | KNN Loss: 3.0424234867095947 | BCE Loss: 1.022845983505249\n",
      "Epoch 464 / 500 | iteration 10 / 30 | Total Loss: 4.089233875274658 | KNN Loss: 3.065227508544922 | BCE Loss: 1.0240062475204468\n",
      "Epoch 464 / 500 | iteration 15 / 30 | Total Loss: 4.088263034820557 | KNN Loss: 3.0735831260681152 | BCE Loss: 1.0146797895431519\n",
      "Epoch 464 / 500 | iteration 20 / 30 | Total Loss: 4.073220252990723 | KNN Loss: 3.060512065887451 | BCE Loss: 1.012708067893982\n",
      "Epoch 464 / 500 | iteration 25 / 30 | Total Loss: 4.060636043548584 | KNN Loss: 3.0739076137542725 | BCE Loss: 0.9867284297943115\n",
      "Epoch 465 / 500 | iteration 0 / 30 | Total Loss: 4.083920001983643 | KNN Loss: 3.074070930480957 | BCE Loss: 1.009848952293396\n",
      "Epoch 465 / 500 | iteration 5 / 30 | Total Loss: 4.0422258377075195 | KNN Loss: 3.0395846366882324 | BCE Loss: 1.0026414394378662\n",
      "Epoch 465 / 500 | iteration 10 / 30 | Total Loss: 4.11685848236084 | KNN Loss: 3.083080291748047 | BCE Loss: 1.033778190612793\n",
      "Epoch 465 / 500 | iteration 15 / 30 | Total Loss: 4.0970458984375 | KNN Loss: 3.087175130844116 | BCE Loss: 1.009871006011963\n",
      "Epoch 465 / 500 | iteration 20 / 30 | Total Loss: 4.110039710998535 | KNN Loss: 3.0844106674194336 | BCE Loss: 1.0256290435791016\n",
      "Epoch 465 / 500 | iteration 25 / 30 | Total Loss: 4.0869975090026855 | KNN Loss: 3.069195032119751 | BCE Loss: 1.017802357673645\n",
      "Epoch 466 / 500 | iteration 0 / 30 | Total Loss: 4.097498893737793 | KNN Loss: 3.0430283546447754 | BCE Loss: 1.0544706583023071\n",
      "Epoch 466 / 500 | iteration 5 / 30 | Total Loss: 4.0812201499938965 | KNN Loss: 3.058206796646118 | BCE Loss: 1.0230134725570679\n",
      "Epoch 466 / 500 | iteration 10 / 30 | Total Loss: 4.075189590454102 | KNN Loss: 3.0613656044006348 | BCE Loss: 1.0138237476348877\n",
      "Epoch 466 / 500 | iteration 15 / 30 | Total Loss: 4.123037338256836 | KNN Loss: 3.0623064041137695 | BCE Loss: 1.0607311725616455\n",
      "Epoch 466 / 500 | iteration 20 / 30 | Total Loss: 4.098787307739258 | KNN Loss: 3.0558295249938965 | BCE Loss: 1.0429577827453613\n",
      "Epoch 466 / 500 | iteration 25 / 30 | Total Loss: 4.096794605255127 | KNN Loss: 3.0657095909118652 | BCE Loss: 1.0310850143432617\n",
      "Epoch 467 / 500 | iteration 0 / 30 | Total Loss: 4.107354164123535 | KNN Loss: 3.0780351161956787 | BCE Loss: 1.0293190479278564\n",
      "Epoch 467 / 500 | iteration 5 / 30 | Total Loss: 4.044631481170654 | KNN Loss: 3.0242044925689697 | BCE Loss: 1.0204271078109741\n",
      "Epoch 467 / 500 | iteration 10 / 30 | Total Loss: 4.054749488830566 | KNN Loss: 3.040081262588501 | BCE Loss: 1.014668345451355\n",
      "Epoch 467 / 500 | iteration 15 / 30 | Total Loss: 4.107744216918945 | KNN Loss: 3.07792329788208 | BCE Loss: 1.0298211574554443\n",
      "Epoch 467 / 500 | iteration 20 / 30 | Total Loss: 4.0724687576293945 | KNN Loss: 3.040895700454712 | BCE Loss: 1.0315732955932617\n",
      "Epoch 467 / 500 | iteration 25 / 30 | Total Loss: 4.070379734039307 | KNN Loss: 3.0535924434661865 | BCE Loss: 1.0167872905731201\n",
      "Epoch 468 / 500 | iteration 0 / 30 | Total Loss: 4.049944877624512 | KNN Loss: 3.0460667610168457 | BCE Loss: 1.0038783550262451\n",
      "Epoch 468 / 500 | iteration 5 / 30 | Total Loss: 4.104074001312256 | KNN Loss: 3.0911762714385986 | BCE Loss: 1.0128977298736572\n",
      "Epoch 468 / 500 | iteration 10 / 30 | Total Loss: 4.072689056396484 | KNN Loss: 3.062098979949951 | BCE Loss: 1.0105900764465332\n",
      "Epoch 468 / 500 | iteration 15 / 30 | Total Loss: 4.063512802124023 | KNN Loss: 3.023144006729126 | BCE Loss: 1.0403685569763184\n",
      "Epoch 468 / 500 | iteration 20 / 30 | Total Loss: 4.073840141296387 | KNN Loss: 3.047520160675049 | BCE Loss: 1.026319980621338\n",
      "Epoch 468 / 500 | iteration 25 / 30 | Total Loss: 4.049381256103516 | KNN Loss: 3.0386834144592285 | BCE Loss: 1.010697603225708\n",
      "Epoch 469 / 500 | iteration 0 / 30 | Total Loss: 4.083333969116211 | KNN Loss: 3.0755062103271484 | BCE Loss: 1.0078279972076416\n",
      "Epoch 469 / 500 | iteration 5 / 30 | Total Loss: 4.1421966552734375 | KNN Loss: 3.100918769836426 | BCE Loss: 1.0412777662277222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 469 / 500 | iteration 10 / 30 | Total Loss: 4.0941572189331055 | KNN Loss: 3.043483257293701 | BCE Loss: 1.0506737232208252\n",
      "Epoch 469 / 500 | iteration 15 / 30 | Total Loss: 4.117823123931885 | KNN Loss: 3.0962131023406982 | BCE Loss: 1.021609902381897\n",
      "Epoch 469 / 500 | iteration 20 / 30 | Total Loss: 4.04673957824707 | KNN Loss: 3.030496120452881 | BCE Loss: 1.016243577003479\n",
      "Epoch 469 / 500 | iteration 25 / 30 | Total Loss: 4.1146559715271 | KNN Loss: 3.081186532974243 | BCE Loss: 1.033469557762146\n",
      "Epoch 470 / 500 | iteration 0 / 30 | Total Loss: 4.117110252380371 | KNN Loss: 3.0767993927001953 | BCE Loss: 1.0403110980987549\n",
      "Epoch 470 / 500 | iteration 5 / 30 | Total Loss: 4.091004371643066 | KNN Loss: 3.0672128200531006 | BCE Loss: 1.023791790008545\n",
      "Epoch 470 / 500 | iteration 10 / 30 | Total Loss: 4.059557914733887 | KNN Loss: 3.0564918518066406 | BCE Loss: 1.0030661821365356\n",
      "Epoch 470 / 500 | iteration 15 / 30 | Total Loss: 4.03196907043457 | KNN Loss: 3.050567388534546 | BCE Loss: 0.9814019203186035\n",
      "Epoch 470 / 500 | iteration 20 / 30 | Total Loss: 4.07109260559082 | KNN Loss: 3.067077398300171 | BCE Loss: 1.0040154457092285\n",
      "Epoch 470 / 500 | iteration 25 / 30 | Total Loss: 4.025066375732422 | KNN Loss: 3.0206451416015625 | BCE Loss: 1.0044214725494385\n",
      "Epoch 471 / 500 | iteration 0 / 30 | Total Loss: 4.070128917694092 | KNN Loss: 3.059810161590576 | BCE Loss: 1.0103187561035156\n",
      "Epoch 471 / 500 | iteration 5 / 30 | Total Loss: 4.119828224182129 | KNN Loss: 3.1037864685058594 | BCE Loss: 1.0160415172576904\n",
      "Epoch 471 / 500 | iteration 10 / 30 | Total Loss: 4.090888500213623 | KNN Loss: 3.0687830448150635 | BCE Loss: 1.02210533618927\n",
      "Epoch 471 / 500 | iteration 15 / 30 | Total Loss: 4.11358642578125 | KNN Loss: 3.0759615898132324 | BCE Loss: 1.0376245975494385\n",
      "Epoch 471 / 500 | iteration 20 / 30 | Total Loss: 4.071691989898682 | KNN Loss: 3.0719728469848633 | BCE Loss: 0.9997190833091736\n",
      "Epoch 471 / 500 | iteration 25 / 30 | Total Loss: 4.133667945861816 | KNN Loss: 3.111114025115967 | BCE Loss: 1.0225539207458496\n",
      "Epoch 472 / 500 | iteration 0 / 30 | Total Loss: 4.076884746551514 | KNN Loss: 3.066248655319214 | BCE Loss: 1.0106359720230103\n",
      "Epoch 472 / 500 | iteration 5 / 30 | Total Loss: 4.074150085449219 | KNN Loss: 3.0533626079559326 | BCE Loss: 1.0207874774932861\n",
      "Epoch 472 / 500 | iteration 10 / 30 | Total Loss: 4.122420310974121 | KNN Loss: 3.100782632827759 | BCE Loss: 1.0216379165649414\n",
      "Epoch 472 / 500 | iteration 15 / 30 | Total Loss: 4.086501598358154 | KNN Loss: 3.043421983718872 | BCE Loss: 1.0430797338485718\n",
      "Epoch 472 / 500 | iteration 20 / 30 | Total Loss: 4.060885429382324 | KNN Loss: 3.0600831508636475 | BCE Loss: 1.0008025169372559\n",
      "Epoch 472 / 500 | iteration 25 / 30 | Total Loss: 4.104061126708984 | KNN Loss: 3.0635721683502197 | BCE Loss: 1.0404891967773438\n",
      "Epoch 473 / 500 | iteration 0 / 30 | Total Loss: 4.067139625549316 | KNN Loss: 3.0419745445251465 | BCE Loss: 1.02516508102417\n",
      "Epoch 473 / 500 | iteration 5 / 30 | Total Loss: 4.093801021575928 | KNN Loss: 3.0852227210998535 | BCE Loss: 1.0085783004760742\n",
      "Epoch 473 / 500 | iteration 10 / 30 | Total Loss: 4.022727012634277 | KNN Loss: 3.028613567352295 | BCE Loss: 0.9941133856773376\n",
      "Epoch 473 / 500 | iteration 15 / 30 | Total Loss: 4.1191816329956055 | KNN Loss: 3.123443841934204 | BCE Loss: 0.9957377314567566\n",
      "Epoch 473 / 500 | iteration 20 / 30 | Total Loss: 4.063018798828125 | KNN Loss: 3.0604071617126465 | BCE Loss: 1.0026116371154785\n",
      "Epoch 473 / 500 | iteration 25 / 30 | Total Loss: 4.075017929077148 | KNN Loss: 3.071017026901245 | BCE Loss: 1.0040006637573242\n",
      "Epoch   474: reducing learning rate of group 0 to 5.5221e-08.\n",
      "Epoch 474 / 500 | iteration 0 / 30 | Total Loss: 4.0517497062683105 | KNN Loss: 3.0661444664001465 | BCE Loss: 0.9856053590774536\n",
      "Epoch 474 / 500 | iteration 5 / 30 | Total Loss: 4.090085983276367 | KNN Loss: 3.057130813598633 | BCE Loss: 1.032955288887024\n",
      "Epoch 474 / 500 | iteration 10 / 30 | Total Loss: 4.116245746612549 | KNN Loss: 3.0901787281036377 | BCE Loss: 1.0260670185089111\n",
      "Epoch 474 / 500 | iteration 15 / 30 | Total Loss: 4.0759968757629395 | KNN Loss: 3.0403144359588623 | BCE Loss: 1.0356824398040771\n",
      "Epoch 474 / 500 | iteration 20 / 30 | Total Loss: 4.079981803894043 | KNN Loss: 3.0628461837768555 | BCE Loss: 1.0171358585357666\n",
      "Epoch 474 / 500 | iteration 25 / 30 | Total Loss: 4.076207637786865 | KNN Loss: 3.0683515071868896 | BCE Loss: 1.0078561305999756\n",
      "Epoch 475 / 500 | iteration 0 / 30 | Total Loss: 4.1260151863098145 | KNN Loss: 3.0937139987945557 | BCE Loss: 1.0323011875152588\n",
      "Epoch 475 / 500 | iteration 5 / 30 | Total Loss: 4.0488433837890625 | KNN Loss: 3.0337600708007812 | BCE Loss: 1.0150831937789917\n",
      "Epoch 475 / 500 | iteration 10 / 30 | Total Loss: 4.066709518432617 | KNN Loss: 3.067404270172119 | BCE Loss: 0.999305248260498\n",
      "Epoch 475 / 500 | iteration 15 / 30 | Total Loss: 4.036053657531738 | KNN Loss: 3.0201189517974854 | BCE Loss: 1.0159344673156738\n",
      "Epoch 475 / 500 | iteration 20 / 30 | Total Loss: 4.038376808166504 | KNN Loss: 3.040586471557617 | BCE Loss: 0.9977905750274658\n",
      "Epoch 475 / 500 | iteration 25 / 30 | Total Loss: 4.132438659667969 | KNN Loss: 3.0900542736053467 | BCE Loss: 1.042384147644043\n",
      "Epoch 476 / 500 | iteration 0 / 30 | Total Loss: 4.0976433753967285 | KNN Loss: 3.0643856525421143 | BCE Loss: 1.0332577228546143\n",
      "Epoch 476 / 500 | iteration 5 / 30 | Total Loss: 4.110362529754639 | KNN Loss: 3.0732688903808594 | BCE Loss: 1.0370936393737793\n",
      "Epoch 476 / 500 | iteration 10 / 30 | Total Loss: 4.073885917663574 | KNN Loss: 3.0646462440490723 | BCE Loss: 1.0092394351959229\n",
      "Epoch 476 / 500 | iteration 15 / 30 | Total Loss: 4.058320045471191 | KNN Loss: 3.039870500564575 | BCE Loss: 1.018449306488037\n",
      "Epoch 476 / 500 | iteration 20 / 30 | Total Loss: 4.093105316162109 | KNN Loss: 3.067990303039551 | BCE Loss: 1.0251152515411377\n",
      "Epoch 476 / 500 | iteration 25 / 30 | Total Loss: 4.104335784912109 | KNN Loss: 3.0848164558410645 | BCE Loss: 1.019519329071045\n",
      "Epoch 477 / 500 | iteration 0 / 30 | Total Loss: 4.0919365882873535 | KNN Loss: 3.051297187805176 | BCE Loss: 1.0406394004821777\n",
      "Epoch 477 / 500 | iteration 5 / 30 | Total Loss: 4.026761054992676 | KNN Loss: 3.014277696609497 | BCE Loss: 1.0124831199645996\n",
      "Epoch 477 / 500 | iteration 10 / 30 | Total Loss: 4.092159271240234 | KNN Loss: 3.0789196491241455 | BCE Loss: 1.0132396221160889\n",
      "Epoch 477 / 500 | iteration 15 / 30 | Total Loss: 4.093571186065674 | KNN Loss: 3.0621602535247803 | BCE Loss: 1.0314109325408936\n",
      "Epoch 477 / 500 | iteration 20 / 30 | Total Loss: 4.072300434112549 | KNN Loss: 3.071425437927246 | BCE Loss: 1.0008749961853027\n",
      "Epoch 477 / 500 | iteration 25 / 30 | Total Loss: 4.105294704437256 | KNN Loss: 3.069504976272583 | BCE Loss: 1.0357896089553833\n",
      "Epoch 478 / 500 | iteration 0 / 30 | Total Loss: 4.057374954223633 | KNN Loss: 3.058748960494995 | BCE Loss: 0.9986259341239929\n",
      "Epoch 478 / 500 | iteration 5 / 30 | Total Loss: 4.071878910064697 | KNN Loss: 3.0609612464904785 | BCE Loss: 1.0109177827835083\n",
      "Epoch 478 / 500 | iteration 10 / 30 | Total Loss: 4.048496723175049 | KNN Loss: 3.061505079269409 | BCE Loss: 0.9869914650917053\n",
      "Epoch 478 / 500 | iteration 15 / 30 | Total Loss: 4.0560407638549805 | KNN Loss: 3.0548601150512695 | BCE Loss: 1.00118088722229\n",
      "Epoch 478 / 500 | iteration 20 / 30 | Total Loss: 4.090604305267334 | KNN Loss: 3.0839247703552246 | BCE Loss: 1.006679654121399\n",
      "Epoch 478 / 500 | iteration 25 / 30 | Total Loss: 4.100595951080322 | KNN Loss: 3.0829503536224365 | BCE Loss: 1.0176454782485962\n",
      "Epoch 479 / 500 | iteration 0 / 30 | Total Loss: 4.106921672821045 | KNN Loss: 3.090590476989746 | BCE Loss: 1.0163311958312988\n",
      "Epoch 479 / 500 | iteration 5 / 30 | Total Loss: 4.120471000671387 | KNN Loss: 3.081094980239868 | BCE Loss: 1.0393760204315186\n",
      "Epoch 479 / 500 | iteration 10 / 30 | Total Loss: 4.083101272583008 | KNN Loss: 3.090362071990967 | BCE Loss: 0.9927389621734619\n",
      "Epoch 479 / 500 | iteration 15 / 30 | Total Loss: 4.126797676086426 | KNN Loss: 3.108731746673584 | BCE Loss: 1.0180656909942627\n",
      "Epoch 479 / 500 | iteration 20 / 30 | Total Loss: 4.065485954284668 | KNN Loss: 3.0508334636688232 | BCE Loss: 1.0146526098251343\n",
      "Epoch 479 / 500 | iteration 25 / 30 | Total Loss: 4.078006267547607 | KNN Loss: 3.0785014629364014 | BCE Loss: 0.9995047450065613\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 480 / 500 | iteration 0 / 30 | Total Loss: 4.090553283691406 | KNN Loss: 3.075035810470581 | BCE Loss: 1.0155173540115356\n",
      "Epoch 480 / 500 | iteration 5 / 30 | Total Loss: 4.072437286376953 | KNN Loss: 3.0551910400390625 | BCE Loss: 1.0172460079193115\n",
      "Epoch 480 / 500 | iteration 10 / 30 | Total Loss: 4.121167182922363 | KNN Loss: 3.074995994567871 | BCE Loss: 1.0461711883544922\n",
      "Epoch 480 / 500 | iteration 15 / 30 | Total Loss: 4.090211868286133 | KNN Loss: 3.0612542629241943 | BCE Loss: 1.0289576053619385\n",
      "Epoch 480 / 500 | iteration 20 / 30 | Total Loss: 4.082388877868652 | KNN Loss: 3.0901005268096924 | BCE Loss: 0.9922885894775391\n",
      "Epoch 480 / 500 | iteration 25 / 30 | Total Loss: 4.121117115020752 | KNN Loss: 3.07875657081604 | BCE Loss: 1.0423604249954224\n",
      "Epoch 481 / 500 | iteration 0 / 30 | Total Loss: 4.074170112609863 | KNN Loss: 3.0565381050109863 | BCE Loss: 1.017632246017456\n",
      "Epoch 481 / 500 | iteration 5 / 30 | Total Loss: 4.0654191970825195 | KNN Loss: 3.064542531967163 | BCE Loss: 1.0008764266967773\n",
      "Epoch 481 / 500 | iteration 10 / 30 | Total Loss: 4.071741104125977 | KNN Loss: 3.0627222061157227 | BCE Loss: 1.009019136428833\n",
      "Epoch 481 / 500 | iteration 15 / 30 | Total Loss: 4.125110149383545 | KNN Loss: 3.1069157123565674 | BCE Loss: 1.0181944370269775\n",
      "Epoch 481 / 500 | iteration 20 / 30 | Total Loss: 4.079813003540039 | KNN Loss: 3.0741865634918213 | BCE Loss: 1.0056264400482178\n",
      "Epoch 481 / 500 | iteration 25 / 30 | Total Loss: 4.101977825164795 | KNN Loss: 3.087028980255127 | BCE Loss: 1.0149489641189575\n",
      "Epoch 482 / 500 | iteration 0 / 30 | Total Loss: 4.074493408203125 | KNN Loss: 3.0659332275390625 | BCE Loss: 1.0085601806640625\n",
      "Epoch 482 / 500 | iteration 5 / 30 | Total Loss: 4.081841468811035 | KNN Loss: 3.077000141143799 | BCE Loss: 1.0048413276672363\n",
      "Epoch 482 / 500 | iteration 10 / 30 | Total Loss: 4.096488952636719 | KNN Loss: 3.0680177211761475 | BCE Loss: 1.0284709930419922\n",
      "Epoch 482 / 500 | iteration 15 / 30 | Total Loss: 4.089751720428467 | KNN Loss: 3.0819621086120605 | BCE Loss: 1.0077896118164062\n",
      "Epoch 482 / 500 | iteration 20 / 30 | Total Loss: 4.111763000488281 | KNN Loss: 3.0626001358032227 | BCE Loss: 1.0491628646850586\n",
      "Epoch 482 / 500 | iteration 25 / 30 | Total Loss: 4.073250770568848 | KNN Loss: 3.073472499847412 | BCE Loss: 0.9997782111167908\n",
      "Epoch 483 / 500 | iteration 0 / 30 | Total Loss: 4.07211971282959 | KNN Loss: 3.0516555309295654 | BCE Loss: 1.020464301109314\n",
      "Epoch 483 / 500 | iteration 5 / 30 | Total Loss: 4.0593037605285645 | KNN Loss: 3.043095350265503 | BCE Loss: 1.0162084102630615\n",
      "Epoch 483 / 500 | iteration 10 / 30 | Total Loss: 4.04802131652832 | KNN Loss: 3.04280948638916 | BCE Loss: 1.0052118301391602\n",
      "Epoch 483 / 500 | iteration 15 / 30 | Total Loss: 4.104180335998535 | KNN Loss: 3.0895800590515137 | BCE Loss: 1.0146000385284424\n",
      "Epoch 483 / 500 | iteration 20 / 30 | Total Loss: 4.068023681640625 | KNN Loss: 3.056979179382324 | BCE Loss: 1.0110442638397217\n",
      "Epoch 483 / 500 | iteration 25 / 30 | Total Loss: 4.047828197479248 | KNN Loss: 3.020214319229126 | BCE Loss: 1.027613878250122\n",
      "Epoch 484 / 500 | iteration 0 / 30 | Total Loss: 4.061182975769043 | KNN Loss: 3.058673143386841 | BCE Loss: 1.002509593963623\n",
      "Epoch 484 / 500 | iteration 5 / 30 | Total Loss: 4.093174934387207 | KNN Loss: 3.0665111541748047 | BCE Loss: 1.0266637802124023\n",
      "Epoch 484 / 500 | iteration 10 / 30 | Total Loss: 4.051312446594238 | KNN Loss: 3.0447494983673096 | BCE Loss: 1.0065630674362183\n",
      "Epoch 484 / 500 | iteration 15 / 30 | Total Loss: 4.085649490356445 | KNN Loss: 3.0373599529266357 | BCE Loss: 1.04828941822052\n",
      "Epoch 484 / 500 | iteration 20 / 30 | Total Loss: 4.1148152351379395 | KNN Loss: 3.048766613006592 | BCE Loss: 1.0660486221313477\n",
      "Epoch 484 / 500 | iteration 25 / 30 | Total Loss: 4.070337295532227 | KNN Loss: 3.063807725906372 | BCE Loss: 1.0065293312072754\n",
      "Epoch   485: reducing learning rate of group 0 to 3.8655e-08.\n",
      "Epoch 485 / 500 | iteration 0 / 30 | Total Loss: 4.131187438964844 | KNN Loss: 3.1166980266571045 | BCE Loss: 1.0144894123077393\n",
      "Epoch 485 / 500 | iteration 5 / 30 | Total Loss: 4.117232799530029 | KNN Loss: 3.1173288822174072 | BCE Loss: 0.9999037981033325\n",
      "Epoch 485 / 500 | iteration 10 / 30 | Total Loss: 4.110610008239746 | KNN Loss: 3.090944290161133 | BCE Loss: 1.0196654796600342\n",
      "Epoch 485 / 500 | iteration 15 / 30 | Total Loss: 4.099314212799072 | KNN Loss: 3.0510852336883545 | BCE Loss: 1.0482289791107178\n",
      "Epoch 485 / 500 | iteration 20 / 30 | Total Loss: 4.05360221862793 | KNN Loss: 3.018836498260498 | BCE Loss: 1.0347659587860107\n",
      "Epoch 485 / 500 | iteration 25 / 30 | Total Loss: 4.10190486907959 | KNN Loss: 3.0825302600860596 | BCE Loss: 1.0193746089935303\n",
      "Epoch 486 / 500 | iteration 0 / 30 | Total Loss: 4.102346420288086 | KNN Loss: 3.079127550125122 | BCE Loss: 1.0232186317443848\n",
      "Epoch 486 / 500 | iteration 5 / 30 | Total Loss: 4.0616607666015625 | KNN Loss: 3.0606565475463867 | BCE Loss: 1.0010039806365967\n",
      "Epoch 486 / 500 | iteration 10 / 30 | Total Loss: 4.082042217254639 | KNN Loss: 3.051198720932007 | BCE Loss: 1.0308434963226318\n",
      "Epoch 486 / 500 | iteration 15 / 30 | Total Loss: 4.144145965576172 | KNN Loss: 3.082582712173462 | BCE Loss: 1.061563491821289\n",
      "Epoch 486 / 500 | iteration 20 / 30 | Total Loss: 4.060234069824219 | KNN Loss: 3.057948350906372 | BCE Loss: 1.0022859573364258\n",
      "Epoch 486 / 500 | iteration 25 / 30 | Total Loss: 4.1674628257751465 | KNN Loss: 3.13968563079834 | BCE Loss: 1.0277771949768066\n",
      "Epoch 487 / 500 | iteration 0 / 30 | Total Loss: 4.0666913986206055 | KNN Loss: 3.06093692779541 | BCE Loss: 1.0057547092437744\n",
      "Epoch 487 / 500 | iteration 5 / 30 | Total Loss: 4.009838104248047 | KNN Loss: 3.019191265106201 | BCE Loss: 0.9906466007232666\n",
      "Epoch 487 / 500 | iteration 10 / 30 | Total Loss: 4.072498798370361 | KNN Loss: 3.0650389194488525 | BCE Loss: 1.0074598789215088\n",
      "Epoch 487 / 500 | iteration 15 / 30 | Total Loss: 4.139899253845215 | KNN Loss: 3.088529109954834 | BCE Loss: 1.05137038230896\n",
      "Epoch 487 / 500 | iteration 20 / 30 | Total Loss: 4.108119010925293 | KNN Loss: 3.097399950027466 | BCE Loss: 1.0107191801071167\n",
      "Epoch 487 / 500 | iteration 25 / 30 | Total Loss: 4.118626594543457 | KNN Loss: 3.071693181991577 | BCE Loss: 1.0469335317611694\n",
      "Epoch 488 / 500 | iteration 0 / 30 | Total Loss: 4.064034461975098 | KNN Loss: 3.057386636734009 | BCE Loss: 1.0066478252410889\n",
      "Epoch 488 / 500 | iteration 5 / 30 | Total Loss: 4.051610469818115 | KNN Loss: 3.054441213607788 | BCE Loss: 0.9971690773963928\n",
      "Epoch 488 / 500 | iteration 10 / 30 | Total Loss: 4.137552738189697 | KNN Loss: 3.0654184818267822 | BCE Loss: 1.0721341371536255\n",
      "Epoch 488 / 500 | iteration 15 / 30 | Total Loss: 4.03682804107666 | KNN Loss: 3.036062717437744 | BCE Loss: 1.000765085220337\n",
      "Epoch 488 / 500 | iteration 20 / 30 | Total Loss: 4.069108009338379 | KNN Loss: 3.041841983795166 | BCE Loss: 1.0272659063339233\n",
      "Epoch 488 / 500 | iteration 25 / 30 | Total Loss: 4.100186347961426 | KNN Loss: 3.0773448944091797 | BCE Loss: 1.022841215133667\n",
      "Epoch 489 / 500 | iteration 0 / 30 | Total Loss: 4.110724925994873 | KNN Loss: 3.06440806388855 | BCE Loss: 1.0463168621063232\n",
      "Epoch 489 / 500 | iteration 5 / 30 | Total Loss: 4.125402450561523 | KNN Loss: 3.1165785789489746 | BCE Loss: 1.008824110031128\n",
      "Epoch 489 / 500 | iteration 10 / 30 | Total Loss: 4.084530830383301 | KNN Loss: 3.0398190021514893 | BCE Loss: 1.0447118282318115\n",
      "Epoch 489 / 500 | iteration 15 / 30 | Total Loss: 4.059696197509766 | KNN Loss: 3.0479958057403564 | BCE Loss: 1.0117006301879883\n",
      "Epoch 489 / 500 | iteration 20 / 30 | Total Loss: 4.032383441925049 | KNN Loss: 3.0373876094818115 | BCE Loss: 0.9949958324432373\n",
      "Epoch 489 / 500 | iteration 25 / 30 | Total Loss: 4.065548896789551 | KNN Loss: 3.045653820037842 | BCE Loss: 1.0198948383331299\n",
      "Epoch 490 / 500 | iteration 0 / 30 | Total Loss: 4.119404315948486 | KNN Loss: 3.090559244155884 | BCE Loss: 1.0288450717926025\n",
      "Epoch 490 / 500 | iteration 5 / 30 | Total Loss: 4.105242729187012 | KNN Loss: 3.101728916168213 | BCE Loss: 1.0035138130187988\n",
      "Epoch 490 / 500 | iteration 10 / 30 | Total Loss: 4.077497482299805 | KNN Loss: 3.0511512756347656 | BCE Loss: 1.0263463258743286\n",
      "Epoch 490 / 500 | iteration 15 / 30 | Total Loss: 4.070064067840576 | KNN Loss: 3.036224126815796 | BCE Loss: 1.0338398218154907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 490 / 500 | iteration 20 / 30 | Total Loss: 4.043079376220703 | KNN Loss: 3.027883291244507 | BCE Loss: 1.0151962041854858\n",
      "Epoch 490 / 500 | iteration 25 / 30 | Total Loss: 4.08083438873291 | KNN Loss: 3.0618417263031006 | BCE Loss: 1.01899254322052\n",
      "Epoch 491 / 500 | iteration 0 / 30 | Total Loss: 4.067022323608398 | KNN Loss: 3.072608470916748 | BCE Loss: 0.9944136738777161\n",
      "Epoch 491 / 500 | iteration 5 / 30 | Total Loss: 4.042782783508301 | KNN Loss: 3.047851324081421 | BCE Loss: 0.9949314594268799\n",
      "Epoch 491 / 500 | iteration 10 / 30 | Total Loss: 4.111721038818359 | KNN Loss: 3.0793263912200928 | BCE Loss: 1.0323948860168457\n",
      "Epoch 491 / 500 | iteration 15 / 30 | Total Loss: 4.1432647705078125 | KNN Loss: 3.1072874069213867 | BCE Loss: 1.0359772443771362\n",
      "Epoch 491 / 500 | iteration 20 / 30 | Total Loss: 4.0979437828063965 | KNN Loss: 3.1002767086029053 | BCE Loss: 0.9976669549942017\n",
      "Epoch 491 / 500 | iteration 25 / 30 | Total Loss: 4.115687370300293 | KNN Loss: 3.079522132873535 | BCE Loss: 1.036165475845337\n",
      "Epoch 492 / 500 | iteration 0 / 30 | Total Loss: 4.090846061706543 | KNN Loss: 3.067359447479248 | BCE Loss: 1.023486614227295\n",
      "Epoch 492 / 500 | iteration 5 / 30 | Total Loss: 4.066311836242676 | KNN Loss: 3.05558180809021 | BCE Loss: 1.0107300281524658\n",
      "Epoch 492 / 500 | iteration 10 / 30 | Total Loss: 4.081552028656006 | KNN Loss: 3.061453342437744 | BCE Loss: 1.0200986862182617\n",
      "Epoch 492 / 500 | iteration 15 / 30 | Total Loss: 4.093151092529297 | KNN Loss: 3.092064380645752 | BCE Loss: 1.0010864734649658\n",
      "Epoch 492 / 500 | iteration 20 / 30 | Total Loss: 4.0949625968933105 | KNN Loss: 3.0593252182006836 | BCE Loss: 1.035637378692627\n",
      "Epoch 492 / 500 | iteration 25 / 30 | Total Loss: 4.1063432693481445 | KNN Loss: 3.0704762935638428 | BCE Loss: 1.0358669757843018\n",
      "Epoch 493 / 500 | iteration 0 / 30 | Total Loss: 4.12092924118042 | KNN Loss: 3.0755960941314697 | BCE Loss: 1.0453331470489502\n",
      "Epoch 493 / 500 | iteration 5 / 30 | Total Loss: 4.105583190917969 | KNN Loss: 3.0554888248443604 | BCE Loss: 1.0500941276550293\n",
      "Epoch 493 / 500 | iteration 10 / 30 | Total Loss: 4.119350910186768 | KNN Loss: 3.0923447608947754 | BCE Loss: 1.0270061492919922\n",
      "Epoch 493 / 500 | iteration 15 / 30 | Total Loss: 4.033992767333984 | KNN Loss: 3.0315680503845215 | BCE Loss: 1.002424716949463\n",
      "Epoch 493 / 500 | iteration 20 / 30 | Total Loss: 4.103635311126709 | KNN Loss: 3.103578567504883 | BCE Loss: 1.0000568628311157\n",
      "Epoch 493 / 500 | iteration 25 / 30 | Total Loss: 4.1297993659973145 | KNN Loss: 3.1156232357025146 | BCE Loss: 1.0141760110855103\n",
      "Epoch 494 / 500 | iteration 0 / 30 | Total Loss: 4.056695461273193 | KNN Loss: 3.0329339504241943 | BCE Loss: 1.023761510848999\n",
      "Epoch 494 / 500 | iteration 5 / 30 | Total Loss: 4.102170467376709 | KNN Loss: 3.0885636806488037 | BCE Loss: 1.0136067867279053\n",
      "Epoch 494 / 500 | iteration 10 / 30 | Total Loss: 4.092964172363281 | KNN Loss: 3.0575006008148193 | BCE Loss: 1.035463571548462\n",
      "Epoch 494 / 500 | iteration 15 / 30 | Total Loss: 4.103187561035156 | KNN Loss: 3.098285436630249 | BCE Loss: 1.0049022436141968\n",
      "Epoch 494 / 500 | iteration 20 / 30 | Total Loss: 4.0999674797058105 | KNN Loss: 3.0619962215423584 | BCE Loss: 1.0379712581634521\n",
      "Epoch 494 / 500 | iteration 25 / 30 | Total Loss: 4.093186378479004 | KNN Loss: 3.0673680305480957 | BCE Loss: 1.0258184671401978\n",
      "Epoch 495 / 500 | iteration 0 / 30 | Total Loss: 4.063962459564209 | KNN Loss: 3.032740592956543 | BCE Loss: 1.0312217473983765\n",
      "Epoch 495 / 500 | iteration 5 / 30 | Total Loss: 4.05793571472168 | KNN Loss: 3.0611557960510254 | BCE Loss: 0.9967796802520752\n",
      "Epoch 495 / 500 | iteration 10 / 30 | Total Loss: 4.061776638031006 | KNN Loss: 3.0499308109283447 | BCE Loss: 1.0118459463119507\n",
      "Epoch 495 / 500 | iteration 15 / 30 | Total Loss: 4.1013360023498535 | KNN Loss: 3.078669309616089 | BCE Loss: 1.0226666927337646\n",
      "Epoch 495 / 500 | iteration 20 / 30 | Total Loss: 4.086084842681885 | KNN Loss: 3.063239097595215 | BCE Loss: 1.02284574508667\n",
      "Epoch 495 / 500 | iteration 25 / 30 | Total Loss: 4.09359884262085 | KNN Loss: 3.068653106689453 | BCE Loss: 1.0249457359313965\n",
      "Epoch   496: reducing learning rate of group 0 to 2.7058e-08.\n",
      "Epoch 496 / 500 | iteration 0 / 30 | Total Loss: 4.078784942626953 | KNN Loss: 3.0561699867248535 | BCE Loss: 1.0226150751113892\n",
      "Epoch 496 / 500 | iteration 5 / 30 | Total Loss: 4.0815019607543945 | KNN Loss: 3.0565431118011475 | BCE Loss: 1.0249589681625366\n",
      "Epoch 496 / 500 | iteration 10 / 30 | Total Loss: 4.045499801635742 | KNN Loss: 3.033874750137329 | BCE Loss: 1.011625051498413\n",
      "Epoch 496 / 500 | iteration 15 / 30 | Total Loss: 4.115738868713379 | KNN Loss: 3.087233066558838 | BCE Loss: 1.028505802154541\n",
      "Epoch 496 / 500 | iteration 20 / 30 | Total Loss: 4.103107929229736 | KNN Loss: 3.0467212200164795 | BCE Loss: 1.0563867092132568\n",
      "Epoch 496 / 500 | iteration 25 / 30 | Total Loss: 4.134649276733398 | KNN Loss: 3.1106324195861816 | BCE Loss: 1.0240166187286377\n",
      "Epoch 497 / 500 | iteration 0 / 30 | Total Loss: 4.065394878387451 | KNN Loss: 3.0557315349578857 | BCE Loss: 1.009663462638855\n",
      "Epoch 497 / 500 | iteration 5 / 30 | Total Loss: 4.114086151123047 | KNN Loss: 3.065716028213501 | BCE Loss: 1.048370122909546\n",
      "Epoch 497 / 500 | iteration 10 / 30 | Total Loss: 4.068124294281006 | KNN Loss: 3.0641286373138428 | BCE Loss: 1.0039955377578735\n",
      "Epoch 497 / 500 | iteration 15 / 30 | Total Loss: 4.115955352783203 | KNN Loss: 3.0737054347991943 | BCE Loss: 1.0422499179840088\n",
      "Epoch 497 / 500 | iteration 20 / 30 | Total Loss: 4.042499542236328 | KNN Loss: 3.031954288482666 | BCE Loss: 1.0105451345443726\n",
      "Epoch 497 / 500 | iteration 25 / 30 | Total Loss: 4.084329605102539 | KNN Loss: 3.0413317680358887 | BCE Loss: 1.0429975986480713\n",
      "Epoch 498 / 500 | iteration 0 / 30 | Total Loss: 4.076325416564941 | KNN Loss: 3.05566143989563 | BCE Loss: 1.0206642150878906\n",
      "Epoch 498 / 500 | iteration 5 / 30 | Total Loss: 4.020871162414551 | KNN Loss: 3.03347110748291 | BCE Loss: 0.9874002933502197\n",
      "Epoch 498 / 500 | iteration 10 / 30 | Total Loss: 4.049812316894531 | KNN Loss: 3.0636141300201416 | BCE Loss: 0.9861983060836792\n",
      "Epoch 498 / 500 | iteration 15 / 30 | Total Loss: 4.103935241699219 | KNN Loss: 3.080125093460083 | BCE Loss: 1.0238102674484253\n",
      "Epoch 498 / 500 | iteration 20 / 30 | Total Loss: 4.069911956787109 | KNN Loss: 3.0855069160461426 | BCE Loss: 0.9844049215316772\n",
      "Epoch 498 / 500 | iteration 25 / 30 | Total Loss: 4.1161956787109375 | KNN Loss: 3.0941689014434814 | BCE Loss: 1.022026538848877\n",
      "Epoch 499 / 500 | iteration 0 / 30 | Total Loss: 4.046210765838623 | KNN Loss: 3.0280468463897705 | BCE Loss: 1.018163800239563\n",
      "Epoch 499 / 500 | iteration 5 / 30 | Total Loss: 4.076347351074219 | KNN Loss: 3.0453341007232666 | BCE Loss: 1.0310133695602417\n",
      "Epoch 499 / 500 | iteration 10 / 30 | Total Loss: 4.11663818359375 | KNN Loss: 3.078869104385376 | BCE Loss: 1.037768840789795\n",
      "Epoch 499 / 500 | iteration 15 / 30 | Total Loss: 4.10075569152832 | KNN Loss: 3.059612274169922 | BCE Loss: 1.0411431789398193\n",
      "Epoch 499 / 500 | iteration 20 / 30 | Total Loss: 4.032196998596191 | KNN Loss: 3.0425171852111816 | BCE Loss: 0.9896799325942993\n",
      "Epoch 499 / 500 | iteration 25 / 30 | Total Loss: 4.098682403564453 | KNN Loss: 3.061506986618042 | BCE Loss: 1.0371754169464111\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "data_iter = torch.utils.data.DataLoader(dataset,\n",
    "                                     batch_size=batch_size,\n",
    "                                     shuffle=True,\n",
    "                                     num_workers=1,\n",
    "                                     pin_memory=True)\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', verbose=True, factor=0.7, threshold=1e-4)\n",
    "knn_crt = KNNLoss(k=k).to(device)\n",
    "losses = []\n",
    "alpha = 10/170\n",
    "gamma = 2\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for iteration, (batch, target) in enumerate(data_iter):\n",
    "        batch = batch.to(device)\n",
    "        target = target.to(device)\n",
    "        outputs, iterm = model(batch, return_intermidiate=True)\n",
    "        mse_loss = F.binary_cross_entropy_with_logits(outputs, target, reduction='none')\n",
    "        mask = torch.ones_like(mse_loss)\n",
    "        mask[target == 0] = alpha ** gamma\n",
    "        mask[target == 1] = (1 - alpha) ** gamma\n",
    "        mse_loss = (mse_loss * mask).sum(dim=-1).mean()\n",
    "        try:\n",
    "            knn_loss = knn_crt(iterm)\n",
    "            if torch.isinf(knn_loss):\n",
    "                knn_loss = 0\n",
    "        except ValueError:\n",
    "            knn_loss = torch.tensor(0)\n",
    "        loss = mse_loss + knn_loss\n",
    "        total_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if iteration % log_every == 0:\n",
    "            print(f\"Epoch {epoch} / {epochs} | iteration {iteration} / {len(data_iter)} | Total Loss: {loss.item()} | KNN Loss: {knn_loss.item()} | BCE Loss: {mse_loss.item()}\")\n",
    "    \n",
    "    scheduler.step(total_loss / (iteration + 1))\n",
    "    losses.append(total_loss / (iteration + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3.1004e+00,  2.8424e+00,  2.8250e+00,  3.0073e+00,  2.9105e+00,\n",
      "          6.3167e-01,  2.4144e+00,  2.4124e+00,  2.2677e+00,  1.7566e+00,\n",
      "          2.4531e+00,  2.0375e+00,  9.6963e-01,  1.9713e+00,  1.1873e+00,\n",
      "          1.7947e+00,  2.8798e+00,  2.7177e+00,  2.5930e+00,  2.2776e+00,\n",
      "          1.6449e+00,  3.1229e+00,  2.5383e+00,  2.7050e+00,  2.7791e+00,\n",
      "          1.9863e+00,  2.1137e+00,  1.4696e+00,  1.4666e+00,  4.0106e-01,\n",
      "         -2.4083e-01,  1.1589e+00,  3.1289e-01,  1.0797e+00,  1.6853e+00,\n",
      "          1.5168e+00,  7.8847e-01,  3.2470e+00,  6.9854e-01,  1.3243e+00,\n",
      "          1.1100e+00, -5.2067e-01, -3.9265e-02,  2.4637e+00,  2.3308e+00,\n",
      "          6.6560e-01, -1.2188e-02,  2.0878e-01,  1.6554e+00,  2.5232e+00,\n",
      "          1.8229e+00,  1.0992e-01,  1.2048e+00,  6.4537e-01, -4.4819e-01,\n",
      "          1.3678e+00,  1.6303e+00,  1.3435e+00,  1.4171e+00,  2.0011e+00,\n",
      "          8.6431e-01,  8.2060e-01,  2.8669e-01,  1.8782e+00,  1.3908e+00,\n",
      "          1.7580e+00, -1.9129e+00,  3.7968e-01,  2.4707e+00,  2.3693e+00,\n",
      "          2.3959e+00,  5.6828e-01,  1.4394e+00,  2.0479e+00,  1.7535e+00,\n",
      "          1.4174e+00,  3.7699e-01,  8.7511e-01,  4.6510e-01,  1.5376e+00,\n",
      "          7.4587e-02,  5.0278e-01,  1.5418e+00, -3.9161e-01,  2.8993e-01,\n",
      "         -8.0925e-01, -2.3021e+00, -3.2713e-01,  5.7144e-01, -1.9130e+00,\n",
      "          5.9727e-01, -6.4115e-04, -5.0955e-01, -8.0017e-01,  7.1770e-01,\n",
      "          8.5411e-01, -8.9583e-01, -6.8894e-01,  4.7231e-01,  1.1996e+00,\n",
      "          7.2904e-01, -1.1094e+00,  9.4796e-01,  1.1647e+00, -1.2618e+00,\n",
      "         -1.1374e+00, -6.7917e-02,  2.3522e-01, -9.3050e-01, -1.5346e+00,\n",
      "         -5.3139e-01, -2.6564e+00, -3.0471e-01,  1.9174e+00,  1.4555e+00,\n",
      "         -2.0939e-01, -4.7530e-01,  3.7414e-02,  1.7290e+00, -2.4335e+00,\n",
      "          3.1698e-01, -4.0344e-02,  4.9353e-01, -5.5749e-01,  1.4593e-01,\n",
      "         -9.7236e-01, -9.6424e-01,  1.1049e+00,  4.6115e-01, -4.9607e-01,\n",
      "          4.6124e-01, -6.3271e-01, -1.3404e+00, -1.5027e-01, -5.8751e-01,\n",
      "          9.9470e-01, -3.4169e-01,  2.5509e-01, -1.9952e+00, -8.6563e-01,\n",
      "         -1.1692e+00,  7.2343e-01, -1.9106e+00, -9.2795e-01, -1.1360e+00,\n",
      "         -5.6014e-01, -1.6354e+00, -9.3660e-01, -2.2531e+00, -9.0725e-01,\n",
      "         -1.1615e+00, -4.4734e-01, -1.5741e+00,  6.4196e-01, -1.5082e+00,\n",
      "         -4.3014e-01, -3.4347e+00,  1.8591e-01,  1.2040e-02, -5.7290e-01,\n",
      "         -2.1949e+00, -1.6116e+00, -1.2054e+00, -1.3109e+00, -2.4285e+00,\n",
      "         -2.3348e+00, -3.5934e+00]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor(-3.5934, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "tensor(3.2470, device='cuda:0', grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "outputs, iterm = model(dataset[67][0].unsqueeze(0).to(device), return_intermidiate=True)\n",
    "print(outputs)\n",
    "print(outputs.min())\n",
    "print(outputs.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d89fdf185fa40958fa8148723ce3873",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.transform = torchvision.transforms.Compose([\n",
    "    BinaryEncodingTransform(mapping=dataset.items_to_idx),\n",
    "]\n",
    ")\n",
    "dataset.target_transform = torchvision.transforms.Compose([\n",
    "    BinaryEncodingTransform(mapping=dataset.items_to_idx),\n",
    "]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_ = [d[0].to('cpu') for d in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:00<00:00, 102.32it/s]\n"
     ]
    }
   ],
   "source": [
    "model = model.eval().to('cpu')\n",
    "projections = model.calculate_intermidiate(dataset_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b211000551864cc2a196b18087cc6218",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3156b1e08944c2cb47a759ed92d27a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "distances = pairwise_distances(projections)\n",
    "# distances = np.triu(distances)\n",
    "distances_f = distances.flatten()\n",
    "\n",
    "plt.matshow(distances)\n",
    "plt.colorbar()\n",
    "plt.figure()\n",
    "plt.hist(distances_f[distances_f > 0], bins=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit DBSCAN and calculate indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = DBSCAN(eps=0.2, min_samples=80).fit_predict(projections)\n",
    "# scores = []\n",
    "# best_score = float('inf')\n",
    "# clusters = None\n",
    "# range_ = list(range(5, 20))\n",
    "# for k in tqdm(range_):\n",
    "#     y = GaussianMixture(n_components=k).fit_predict(projections)\n",
    "#     cur_score = davies_bouldin_score(projections, y)\n",
    "#     scores.append(cur_score)\n",
    "    \n",
    "#     if cur_score < best_score:\n",
    "#         best_score = cur_score\n",
    "#         clusters = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b756669203d74556a26cc16f4905621e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "perplexity = 100\n",
    "\n",
    "p = reduce_dims_and_plot(projections[clusters != -1],\n",
    "                         y=clusters[clusters != -1],\n",
    "                         title=f'perplexity: {perplexity}',\n",
    "                         file_name=None,\n",
    "                         perplexity=perplexity,\n",
    "                         library='Multicore-TSNE',\n",
    "                         perform_PCA=False,\n",
    "                         projected=None,\n",
    "                         figure_type='2d',\n",
    "                         show_figure=True,\n",
    "                         close_figure=False,\n",
    "                         text=None)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn import tree\n",
    "# from sklearn.tree import _tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_dataset = torch.stack(dataset_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = DecisionTreeClassifier(max_depth=200, min_samples_leaf=5)\n",
    "# clf = clf.fit(tensor_dataset[clusters!=-1], clusters[clusters != -1])\n",
    "# print(clf.score(tensor_dataset[clusters!=-1], clusters[clusters != -1]))\n",
    "# print(clf.get_depth())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores = []\n",
    "# for min_samples in range(1,50, 1):\n",
    "#     clf = DecisionTreeClassifier(max_depth=200, min_samples_leaf=min_samples)\n",
    "#     clf = clf.fit(tensor_dataset[clusters!=-1], clusters[clusters != -1])\n",
    "#     scores.append(clf.score(tensor_dataset[clusters!=-1], clusters[clusters != -1]))\n",
    "    \n",
    "# plt.figure()\n",
    "# plt.plot(list(range(1,50, 1)), scores)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rules(tree, feature_names, class_names):\n",
    "    tree_ = tree.tree_\n",
    "    feature_name = [\n",
    "        feature_names[i] if i != _tree.TREE_UNDEFINED else \"undefined!\"\n",
    "        for i in tree_.feature\n",
    "    ]\n",
    "\n",
    "    paths = []\n",
    "    path = []\n",
    "    \n",
    "    def recurse(node, path, paths):\n",
    "        if tree_.feature[node] != _tree.TREE_UNDEFINED:\n",
    "            name = feature_name[node]\n",
    "            threshold = tree_.threshold[node]\n",
    "            p1, p2 = list(path), list(path)\n",
    "#             p1 += [f\"({name} <= {np.round(threshold, 3)})\"]\n",
    "            p1 += [(name, '<=', np.round(threshold, 3))]\n",
    "            recurse(tree_.children_left[node], p1, paths)\n",
    "            p2 += [(name, '>', np.round(threshold, 3))]\n",
    "            recurse(tree_.children_right[node], p2, paths)\n",
    "        else:\n",
    "            path += [(tree_.value[node], tree_.n_node_samples[node])]\n",
    "            paths += [path]\n",
    "            \n",
    "    recurse(0, path, paths)\n",
    "\n",
    "    # sort by samples count\n",
    "    samples_count = [p[-1][1] for p in paths]\n",
    "    ii = list(np.argsort(samples_count))\n",
    "    paths = [paths[i] for i in reversed(ii)]\n",
    "    \n",
    "    rules = []\n",
    "    for path in paths:\n",
    "        rule = []\n",
    "        \n",
    "        for p in path[:-1]:\n",
    "            rule += [p]\n",
    "        target = \" then \"\n",
    "        if class_names is None:\n",
    "            target += \"response: \"+str(np.round(path[-1][0][0][0],3))\n",
    "        else:\n",
    "            classes = path[-1][0][0]\n",
    "            l = np.argmax(classes)\n",
    "            target += f\"class: {class_names[l]} (proba: {np.round(100.0*classes[l]/np.sum(classes),2)}%)\"\n",
    "           \n",
    "        proba = np.round(100.0*classes[l]/np.sum(classes),2)\n",
    "        target += f\" | based on {path[-1][1]:,} samples\"\n",
    "        rule_wrapper = {'target': target, 'rule': rule, 'proba': proba}\n",
    "        rules += [rule_wrapper]\n",
    "        \n",
    "    return rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rules = get_rules(clf, dataset.items, clusters[clusters != -1])\n",
    "\n",
    "# for rule in rules:\n",
    "#     n_pos = 0\n",
    "#     for c,p,v in rule['rule']:\n",
    "#         if p == '>':\n",
    "#             n_pos += 1\n",
    "#     rule['pos'] = n_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure()\n",
    "# probs = [r['proba'] for r in rules]\n",
    "# plt.hist(probs, bins = 100)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rules = sorted(rules, key=lambda x:x['pos'])\n",
    "# rules = [r for r in rules if r['proba'] > 50]\n",
    "# print(len(rules))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(17):\n",
    "#     r_i = rules[i]\n",
    "#     print(f\"------------- rule {i} length {len(r_i)} -------------\")\n",
    "#     print(r_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Soft-Decision-Tree given the self-labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_dataset = list(zip(tensor_dataset[clusters!=-1], clusters[clusters != -1]))\n",
    "batch_size = 512\n",
    "tree_loader = torch.utils.data.DataLoader(tree_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define how we prune the weights of a node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_node(node_weights, factor=1):\n",
    "    w = node_weights.cpu().detach().numpy()\n",
    "    mean_ = np.mean(w)\n",
    "    std_ = np.std(w)\n",
    "    node_weights[((mean_ - std_ * factor) < node_weights) & (node_weights < (mean_ + std_ * factor))] = 0\n",
    "    return node_weights\n",
    "\n",
    "def prune_node_keep(node_weights, keep=4):\n",
    "    w = node_weights.cpu().detach().numpy()\n",
    "    throw_idx = np.argsort(abs(w))[:-keep]\n",
    "    node_weights[throw_idx] = 0\n",
    "    return node_weights\n",
    "\n",
    "def prune_tree(tree_, factor):\n",
    "    new_weights = tree_.inner_nodes.weight.clone()\n",
    "    for i in range(new_weights.shape[0]):\n",
    "        res = prune_node_keep(new_weights[i, :], factor)\n",
    "        new_weights[i, :] = res\n",
    "\n",
    "    with torch.no_grad():\n",
    "        tree_.inner_nodes.weight.copy_(new_weights)\n",
    "        \n",
    "def sparseness(x):\n",
    "    s = []\n",
    "    for i in range(x.shape[0]):\n",
    "        x_ = x[i, :]\n",
    "        sp = (len(x_) - torch.norm(x_, 0).item()) / len(x_)\n",
    "        s.append(sp)\n",
    "    return np.mean(s)\n",
    "\n",
    "def compute_regularization_by_level(tree):\n",
    "    total_reg = 0\n",
    "    for i in range(tree.inner_nodes.weight.shape[0]):\n",
    "        cur_level = np.floor(np.log2(i+1))\n",
    "        node_reg = torch.norm(tree.inner_nodes.weight[i].view(-1), 2)\n",
    "        total_reg += 2**(-cur_level) * node_reg\n",
    "    return total_reg\n",
    "\n",
    "def show_sparseness(tree):\n",
    "    avg_sp = sparseness(tree.inner_nodes.weight)\n",
    "    print(f\"Average sparseness: {avg_sp}\")\n",
    "    layer = 0\n",
    "    sps = []\n",
    "    for i in range(tree.inner_nodes.weight.shape[0]):\n",
    "        cur_layer = int(np.floor(np.log2(i+1)))\n",
    "        if cur_layer != layer:\n",
    "            print(f\"layer {layer}: {np.mean(sps)}\")\n",
    "            sps = []\n",
    "            layer = cur_layer\n",
    "\n",
    "        x_ = tree.inner_nodes.weight[i, :]\n",
    "        sp = (len(x_) - torch.norm(x_, 0).item()) / len(x_)\n",
    "        sps.append(sp)\n",
    "        \n",
    "    return avg_sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_epoch(model, loader, device, log_interval, losses, accs, epoch, iteration):\n",
    "    model = model.train()\n",
    "    for batch_idx, (data, target) in enumerate(loader):\n",
    "        iteration += 1\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        output, penalty = tree.forward(data)\n",
    "\n",
    "        # Loss\n",
    "        loss_tree = criterion(output, target.view(-1))\n",
    "\n",
    "        # Penalty\n",
    "        loss_tree += penalty\n",
    "\n",
    "        # Sparse regularization\n",
    "#         fc_params = torch.cat([x.view(-1) for x in tree.inner_nodes.parameters()])\n",
    "#         regularization = sparsity_lamda * torch.norm(fc_params, 2)\n",
    "        regularization = sparsity_lamda * compute_regularization_by_level(tree)\n",
    "        loss = loss_tree\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        pred = output.data.max(1)[1]\n",
    "        correct = pred.eq(target.view(-1).data).sum()\n",
    "        accs.append(correct.item() / data.size()[0])\n",
    "\n",
    "        # Print training status\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(f\"Epoch: {epoch:02d} | Batch: {batch_idx:03d} / {len(loader):03d} | Total loss: {loss.item():.3f} | Reg loss: {regularization.item():.3f} | Tree loss: {loss_tree.item():.3f} | Accuracy: {correct.item() / data.size()[0]:03f} | {round((time.time() - start_time) / iteration, 3)} sec/iter\")\n",
    "            \n",
    "    return iteration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 5e-3\n",
    "weight_decay = 5e-4\n",
    "sparsity_lamda = 2e-3\n",
    "epochs = 100\n",
    "output_dim = len(set(clusters))\n",
    "log_interval = 1\n",
    "use_cuda = device != 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = SDT(input_dim=tensor_dataset.shape[1], output_dim=len(clusters - 1), depth=tree_depth, lamda=1e-3, use_cuda=use_cuda)\n",
    "optimizer = torch.optim.Adam(tree.parameters(),\n",
    "                                 lr=lr,\n",
    "                                 weight_decay=weight_decay)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "tree = tree.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "accs = []\n",
    "sparsity = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.0\n",
      "layer 0: 0.0\n",
      "layer 1: 0.0\n",
      "layer 2: 0.0\n",
      "layer 3: 0.0\n",
      "layer 4: 0.0\n",
      "Epoch: 00 | Batch: 000 / 016 | Total loss: 9.646 | Reg loss: 0.007 | Tree loss: 9.646 | Accuracy: 0.000000 | 0.108 sec/iter\n",
      "Epoch: 00 | Batch: 001 / 016 | Total loss: 9.650 | Reg loss: 0.007 | Tree loss: 9.650 | Accuracy: 0.000000 | 0.09 sec/iter\n",
      "Epoch: 00 | Batch: 002 / 016 | Total loss: 9.630 | Reg loss: 0.007 | Tree loss: 9.630 | Accuracy: 0.000000 | 0.081 sec/iter\n",
      "Epoch: 00 | Batch: 003 / 016 | Total loss: 9.629 | Reg loss: 0.006 | Tree loss: 9.629 | Accuracy: 0.000000 | 0.078 sec/iter\n",
      "Epoch: 00 | Batch: 004 / 016 | Total loss: 9.625 | Reg loss: 0.006 | Tree loss: 9.625 | Accuracy: 0.000000 | 0.074 sec/iter\n",
      "Epoch: 00 | Batch: 005 / 016 | Total loss: 9.605 | Reg loss: 0.006 | Tree loss: 9.605 | Accuracy: 0.000000 | 0.071 sec/iter\n",
      "Epoch: 00 | Batch: 006 / 016 | Total loss: 9.603 | Reg loss: 0.006 | Tree loss: 9.603 | Accuracy: 0.000000 | 0.069 sec/iter\n",
      "Epoch: 00 | Batch: 007 / 016 | Total loss: 9.616 | Reg loss: 0.006 | Tree loss: 9.616 | Accuracy: 0.000000 | 0.068 sec/iter\n",
      "Epoch: 00 | Batch: 008 / 016 | Total loss: 9.597 | Reg loss: 0.006 | Tree loss: 9.597 | Accuracy: 0.000000 | 0.065 sec/iter\n",
      "Epoch: 00 | Batch: 009 / 016 | Total loss: 9.596 | Reg loss: 0.006 | Tree loss: 9.596 | Accuracy: 0.000000 | 0.064 sec/iter\n",
      "Epoch: 00 | Batch: 010 / 016 | Total loss: 9.587 | Reg loss: 0.006 | Tree loss: 9.587 | Accuracy: 0.000000 | 0.063 sec/iter\n",
      "Epoch: 00 | Batch: 011 / 016 | Total loss: 9.586 | Reg loss: 0.006 | Tree loss: 9.586 | Accuracy: 0.000000 | 0.062 sec/iter\n",
      "Epoch: 00 | Batch: 012 / 016 | Total loss: 9.579 | Reg loss: 0.006 | Tree loss: 9.579 | Accuracy: 0.000000 | 0.062 sec/iter\n",
      "Epoch: 00 | Batch: 013 / 016 | Total loss: 9.581 | Reg loss: 0.006 | Tree loss: 9.581 | Accuracy: 0.000000 | 0.061 sec/iter\n",
      "Epoch: 00 | Batch: 014 / 016 | Total loss: 9.578 | Reg loss: 0.006 | Tree loss: 9.578 | Accuracy: 0.000000 | 0.06 sec/iter\n",
      "Epoch: 00 | Batch: 015 / 016 | Total loss: 9.568 | Reg loss: 0.006 | Tree loss: 9.568 | Accuracy: 0.000000 | 0.06 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 01 | Batch: 000 / 016 | Total loss: 9.575 | Reg loss: 0.002 | Tree loss: 9.575 | Accuracy: 0.000000 | 0.06 sec/iter\n",
      "Epoch: 01 | Batch: 001 / 016 | Total loss: 9.584 | Reg loss: 0.002 | Tree loss: 9.584 | Accuracy: 0.000000 | 0.06 sec/iter\n",
      "Epoch: 01 | Batch: 002 / 016 | Total loss: 9.569 | Reg loss: 0.003 | Tree loss: 9.569 | Accuracy: 0.000000 | 0.06 sec/iter\n",
      "Epoch: 01 | Batch: 003 / 016 | Total loss: 9.551 | Reg loss: 0.003 | Tree loss: 9.551 | Accuracy: 0.000000 | 0.06 sec/iter\n",
      "Epoch: 01 | Batch: 004 / 016 | Total loss: 9.561 | Reg loss: 0.003 | Tree loss: 9.561 | Accuracy: 0.000000 | 0.06 sec/iter\n",
      "Epoch: 01 | Batch: 005 / 016 | Total loss: 9.543 | Reg loss: 0.003 | Tree loss: 9.543 | Accuracy: 0.000000 | 0.059 sec/iter\n",
      "Epoch: 01 | Batch: 006 / 016 | Total loss: 9.546 | Reg loss: 0.003 | Tree loss: 9.546 | Accuracy: 0.001953 | 0.059 sec/iter\n",
      "Epoch: 01 | Batch: 007 / 016 | Total loss: 9.538 | Reg loss: 0.004 | Tree loss: 9.538 | Accuracy: 0.003906 | 0.059 sec/iter\n",
      "Epoch: 01 | Batch: 008 / 016 | Total loss: 9.536 | Reg loss: 0.004 | Tree loss: 9.536 | Accuracy: 0.003906 | 0.059 sec/iter\n",
      "Epoch: 01 | Batch: 009 / 016 | Total loss: 9.523 | Reg loss: 0.004 | Tree loss: 9.523 | Accuracy: 0.001953 | 0.058 sec/iter\n",
      "Epoch: 01 | Batch: 010 / 016 | Total loss: 9.534 | Reg loss: 0.004 | Tree loss: 9.534 | Accuracy: 0.011719 | 0.058 sec/iter\n",
      "Epoch: 01 | Batch: 011 / 016 | Total loss: 9.515 | Reg loss: 0.004 | Tree loss: 9.515 | Accuracy: 0.044922 | 0.058 sec/iter\n",
      "Epoch: 01 | Batch: 012 / 016 | Total loss: 9.514 | Reg loss: 0.005 | Tree loss: 9.514 | Accuracy: 0.052734 | 0.058 sec/iter\n",
      "Epoch: 01 | Batch: 013 / 016 | Total loss: 9.507 | Reg loss: 0.005 | Tree loss: 9.507 | Accuracy: 0.042969 | 0.057 sec/iter\n",
      "Epoch: 01 | Batch: 014 / 016 | Total loss: 9.507 | Reg loss: 0.005 | Tree loss: 9.507 | Accuracy: 0.087891 | 0.057 sec/iter\n",
      "Epoch: 01 | Batch: 015 / 016 | Total loss: 9.492 | Reg loss: 0.005 | Tree loss: 9.492 | Accuracy: 0.094017 | 0.057 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 02 | Batch: 000 / 016 | Total loss: 9.511 | Reg loss: 0.003 | Tree loss: 9.511 | Accuracy: 0.013672 | 0.057 sec/iter\n",
      "Epoch: 02 | Batch: 001 / 016 | Total loss: 9.521 | Reg loss: 0.003 | Tree loss: 9.521 | Accuracy: 0.011719 | 0.057 sec/iter\n",
      "Epoch: 02 | Batch: 002 / 016 | Total loss: 9.502 | Reg loss: 0.003 | Tree loss: 9.502 | Accuracy: 0.046875 | 0.057 sec/iter\n",
      "Epoch: 02 | Batch: 003 / 016 | Total loss: 9.497 | Reg loss: 0.004 | Tree loss: 9.497 | Accuracy: 0.089844 | 0.058 sec/iter\n",
      "Epoch: 02 | Batch: 004 / 016 | Total loss: 9.485 | Reg loss: 0.004 | Tree loss: 9.485 | Accuracy: 0.126953 | 0.058 sec/iter\n",
      "Epoch: 02 | Batch: 005 / 016 | Total loss: 9.479 | Reg loss: 0.004 | Tree loss: 9.479 | Accuracy: 0.123047 | 0.058 sec/iter\n",
      "Epoch: 02 | Batch: 006 / 016 | Total loss: 9.482 | Reg loss: 0.004 | Tree loss: 9.482 | Accuracy: 0.099609 | 0.058 sec/iter\n",
      "Epoch: 02 | Batch: 007 / 016 | Total loss: 9.468 | Reg loss: 0.004 | Tree loss: 9.468 | Accuracy: 0.117188 | 0.058 sec/iter\n",
      "Epoch: 02 | Batch: 008 / 016 | Total loss: 9.468 | Reg loss: 0.004 | Tree loss: 9.468 | Accuracy: 0.109375 | 0.058 sec/iter\n",
      "Epoch: 02 | Batch: 009 / 016 | Total loss: 9.462 | Reg loss: 0.005 | Tree loss: 9.462 | Accuracy: 0.117188 | 0.058 sec/iter\n",
      "Epoch: 02 | Batch: 010 / 016 | Total loss: 9.476 | Reg loss: 0.005 | Tree loss: 9.476 | Accuracy: 0.097656 | 0.058 sec/iter\n",
      "Epoch: 02 | Batch: 011 / 016 | Total loss: 9.458 | Reg loss: 0.005 | Tree loss: 9.458 | Accuracy: 0.097656 | 0.058 sec/iter\n",
      "Epoch: 02 | Batch: 012 / 016 | Total loss: 9.457 | Reg loss: 0.005 | Tree loss: 9.457 | Accuracy: 0.085938 | 0.058 sec/iter\n",
      "Epoch: 02 | Batch: 013 / 016 | Total loss: 9.440 | Reg loss: 0.005 | Tree loss: 9.440 | Accuracy: 0.111328 | 0.058 sec/iter\n",
      "Epoch: 02 | Batch: 014 / 016 | Total loss: 9.426 | Reg loss: 0.006 | Tree loss: 9.426 | Accuracy: 0.125000 | 0.057 sec/iter\n",
      "Epoch: 02 | Batch: 015 / 016 | Total loss: 9.428 | Reg loss: 0.006 | Tree loss: 9.428 | Accuracy: 0.104701 | 0.057 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 03 | Batch: 000 / 016 | Total loss: 9.457 | Reg loss: 0.004 | Tree loss: 9.457 | Accuracy: 0.121094 | 0.057 sec/iter\n",
      "Epoch: 03 | Batch: 001 / 016 | Total loss: 9.437 | Reg loss: 0.004 | Tree loss: 9.437 | Accuracy: 0.121094 | 0.058 sec/iter\n",
      "Epoch: 03 | Batch: 002 / 016 | Total loss: 9.434 | Reg loss: 0.004 | Tree loss: 9.434 | Accuracy: 0.103516 | 0.058 sec/iter\n",
      "Epoch: 03 | Batch: 003 / 016 | Total loss: 9.439 | Reg loss: 0.004 | Tree loss: 9.439 | Accuracy: 0.101562 | 0.058 sec/iter\n",
      "Epoch: 03 | Batch: 004 / 016 | Total loss: 9.433 | Reg loss: 0.005 | Tree loss: 9.433 | Accuracy: 0.105469 | 0.058 sec/iter\n",
      "Epoch: 03 | Batch: 005 / 016 | Total loss: 9.420 | Reg loss: 0.005 | Tree loss: 9.420 | Accuracy: 0.105469 | 0.058 sec/iter\n",
      "Epoch: 03 | Batch: 006 / 016 | Total loss: 9.416 | Reg loss: 0.005 | Tree loss: 9.416 | Accuracy: 0.115234 | 0.058 sec/iter\n",
      "Epoch: 03 | Batch: 007 / 016 | Total loss: 9.424 | Reg loss: 0.005 | Tree loss: 9.424 | Accuracy: 0.097656 | 0.058 sec/iter\n",
      "Epoch: 03 | Batch: 008 / 016 | Total loss: 9.404 | Reg loss: 0.005 | Tree loss: 9.404 | Accuracy: 0.128906 | 0.058 sec/iter\n",
      "Epoch: 03 | Batch: 009 / 016 | Total loss: 9.401 | Reg loss: 0.005 | Tree loss: 9.401 | Accuracy: 0.103516 | 0.058 sec/iter\n",
      "Epoch: 03 | Batch: 010 / 016 | Total loss: 9.393 | Reg loss: 0.006 | Tree loss: 9.393 | Accuracy: 0.103516 | 0.058 sec/iter\n",
      "Epoch: 03 | Batch: 011 / 016 | Total loss: 9.380 | Reg loss: 0.006 | Tree loss: 9.380 | Accuracy: 0.119141 | 0.058 sec/iter\n",
      "Epoch: 03 | Batch: 012 / 016 | Total loss: 9.384 | Reg loss: 0.006 | Tree loss: 9.384 | Accuracy: 0.107422 | 0.058 sec/iter\n",
      "Epoch: 03 | Batch: 013 / 016 | Total loss: 9.389 | Reg loss: 0.006 | Tree loss: 9.389 | Accuracy: 0.095703 | 0.058 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 03 | Batch: 014 / 016 | Total loss: 9.360 | Reg loss: 0.007 | Tree loss: 9.360 | Accuracy: 0.115234 | 0.058 sec/iter\n",
      "Epoch: 03 | Batch: 015 / 016 | Total loss: 9.366 | Reg loss: 0.007 | Tree loss: 9.366 | Accuracy: 0.106838 | 0.058 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 04 | Batch: 000 / 016 | Total loss: 9.391 | Reg loss: 0.005 | Tree loss: 9.391 | Accuracy: 0.091797 | 0.058 sec/iter\n",
      "Epoch: 04 | Batch: 001 / 016 | Total loss: 9.373 | Reg loss: 0.005 | Tree loss: 9.373 | Accuracy: 0.125000 | 0.058 sec/iter\n",
      "Epoch: 04 | Batch: 002 / 016 | Total loss: 9.383 | Reg loss: 0.005 | Tree loss: 9.383 | Accuracy: 0.091797 | 0.058 sec/iter\n",
      "Epoch: 04 | Batch: 003 / 016 | Total loss: 9.364 | Reg loss: 0.005 | Tree loss: 9.364 | Accuracy: 0.132812 | 0.058 sec/iter\n",
      "Epoch: 04 | Batch: 004 / 016 | Total loss: 9.367 | Reg loss: 0.006 | Tree loss: 9.367 | Accuracy: 0.125000 | 0.058 sec/iter\n",
      "Epoch: 04 | Batch: 005 / 016 | Total loss: 9.370 | Reg loss: 0.006 | Tree loss: 9.370 | Accuracy: 0.093750 | 0.058 sec/iter\n",
      "Epoch: 04 | Batch: 006 / 016 | Total loss: 9.357 | Reg loss: 0.006 | Tree loss: 9.357 | Accuracy: 0.099609 | 0.058 sec/iter\n",
      "Epoch: 04 | Batch: 007 / 016 | Total loss: 9.347 | Reg loss: 0.006 | Tree loss: 9.347 | Accuracy: 0.115234 | 0.058 sec/iter\n",
      "Epoch: 04 | Batch: 008 / 016 | Total loss: 9.345 | Reg loss: 0.006 | Tree loss: 9.345 | Accuracy: 0.105469 | 0.058 sec/iter\n",
      "Epoch: 04 | Batch: 009 / 016 | Total loss: 9.332 | Reg loss: 0.006 | Tree loss: 9.332 | Accuracy: 0.115234 | 0.058 sec/iter\n",
      "Epoch: 04 | Batch: 010 / 016 | Total loss: 9.330 | Reg loss: 0.007 | Tree loss: 9.330 | Accuracy: 0.111328 | 0.058 sec/iter\n",
      "Epoch: 04 | Batch: 011 / 016 | Total loss: 9.325 | Reg loss: 0.007 | Tree loss: 9.325 | Accuracy: 0.099609 | 0.058 sec/iter\n",
      "Epoch: 04 | Batch: 012 / 016 | Total loss: 9.318 | Reg loss: 0.007 | Tree loss: 9.318 | Accuracy: 0.113281 | 0.058 sec/iter\n",
      "Epoch: 04 | Batch: 013 / 016 | Total loss: 9.310 | Reg loss: 0.007 | Tree loss: 9.310 | Accuracy: 0.117188 | 0.058 sec/iter\n",
      "Epoch: 04 | Batch: 014 / 016 | Total loss: 9.296 | Reg loss: 0.007 | Tree loss: 9.296 | Accuracy: 0.121094 | 0.058 sec/iter\n",
      "Epoch: 04 | Batch: 015 / 016 | Total loss: 9.300 | Reg loss: 0.008 | Tree loss: 9.300 | Accuracy: 0.091880 | 0.058 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 05 | Batch: 000 / 016 | Total loss: 9.336 | Reg loss: 0.006 | Tree loss: 9.336 | Accuracy: 0.095703 | 0.058 sec/iter\n",
      "Epoch: 05 | Batch: 001 / 016 | Total loss: 9.312 | Reg loss: 0.006 | Tree loss: 9.312 | Accuracy: 0.113281 | 0.058 sec/iter\n",
      "Epoch: 05 | Batch: 002 / 016 | Total loss: 9.311 | Reg loss: 0.006 | Tree loss: 9.311 | Accuracy: 0.119141 | 0.058 sec/iter\n",
      "Epoch: 05 | Batch: 003 / 016 | Total loss: 9.318 | Reg loss: 0.006 | Tree loss: 9.318 | Accuracy: 0.111328 | 0.058 sec/iter\n",
      "Epoch: 05 | Batch: 004 / 016 | Total loss: 9.306 | Reg loss: 0.007 | Tree loss: 9.306 | Accuracy: 0.097656 | 0.058 sec/iter\n",
      "Epoch: 05 | Batch: 005 / 016 | Total loss: 9.289 | Reg loss: 0.007 | Tree loss: 9.289 | Accuracy: 0.117188 | 0.058 sec/iter\n",
      "Epoch: 05 | Batch: 006 / 016 | Total loss: 9.288 | Reg loss: 0.007 | Tree loss: 9.288 | Accuracy: 0.111328 | 0.058 sec/iter\n",
      "Epoch: 05 | Batch: 007 / 016 | Total loss: 9.280 | Reg loss: 0.007 | Tree loss: 9.280 | Accuracy: 0.109375 | 0.058 sec/iter\n",
      "Epoch: 05 | Batch: 008 / 016 | Total loss: 9.288 | Reg loss: 0.007 | Tree loss: 9.288 | Accuracy: 0.097656 | 0.058 sec/iter\n",
      "Epoch: 05 | Batch: 009 / 016 | Total loss: 9.256 | Reg loss: 0.007 | Tree loss: 9.256 | Accuracy: 0.134766 | 0.058 sec/iter\n",
      "Epoch: 05 | Batch: 010 / 016 | Total loss: 9.254 | Reg loss: 0.008 | Tree loss: 9.254 | Accuracy: 0.125000 | 0.058 sec/iter\n",
      "Epoch: 05 | Batch: 011 / 016 | Total loss: 9.250 | Reg loss: 0.008 | Tree loss: 9.250 | Accuracy: 0.095703 | 0.058 sec/iter\n",
      "Epoch: 05 | Batch: 012 / 016 | Total loss: 9.252 | Reg loss: 0.008 | Tree loss: 9.252 | Accuracy: 0.111328 | 0.058 sec/iter\n",
      "Epoch: 05 | Batch: 013 / 016 | Total loss: 9.250 | Reg loss: 0.008 | Tree loss: 9.250 | Accuracy: 0.091797 | 0.058 sec/iter\n",
      "Epoch: 05 | Batch: 014 / 016 | Total loss: 9.236 | Reg loss: 0.009 | Tree loss: 9.236 | Accuracy: 0.113281 | 0.058 sec/iter\n",
      "Epoch: 05 | Batch: 015 / 016 | Total loss: 9.224 | Reg loss: 0.009 | Tree loss: 9.224 | Accuracy: 0.106838 | 0.058 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 06 | Batch: 000 / 016 | Total loss: 9.249 | Reg loss: 0.007 | Tree loss: 9.249 | Accuracy: 0.128906 | 0.058 sec/iter\n",
      "Epoch: 06 | Batch: 001 / 016 | Total loss: 9.253 | Reg loss: 0.007 | Tree loss: 9.253 | Accuracy: 0.121094 | 0.058 sec/iter\n",
      "Epoch: 06 | Batch: 002 / 016 | Total loss: 9.243 | Reg loss: 0.007 | Tree loss: 9.243 | Accuracy: 0.113281 | 0.059 sec/iter\n",
      "Epoch: 06 | Batch: 003 / 016 | Total loss: 9.237 | Reg loss: 0.007 | Tree loss: 9.237 | Accuracy: 0.111328 | 0.059 sec/iter\n",
      "Epoch: 06 | Batch: 004 / 016 | Total loss: 9.236 | Reg loss: 0.008 | Tree loss: 9.236 | Accuracy: 0.101562 | 0.059 sec/iter\n",
      "Epoch: 06 | Batch: 005 / 016 | Total loss: 9.234 | Reg loss: 0.008 | Tree loss: 9.234 | Accuracy: 0.099609 | 0.059 sec/iter\n",
      "Epoch: 06 | Batch: 006 / 016 | Total loss: 9.236 | Reg loss: 0.008 | Tree loss: 9.236 | Accuracy: 0.107422 | 0.059 sec/iter\n",
      "Epoch: 06 | Batch: 007 / 016 | Total loss: 9.214 | Reg loss: 0.008 | Tree loss: 9.214 | Accuracy: 0.109375 | 0.059 sec/iter\n",
      "Epoch: 06 | Batch: 008 / 016 | Total loss: 9.197 | Reg loss: 0.008 | Tree loss: 9.197 | Accuracy: 0.117188 | 0.059 sec/iter\n",
      "Epoch: 06 | Batch: 009 / 016 | Total loss: 9.220 | Reg loss: 0.009 | Tree loss: 9.220 | Accuracy: 0.080078 | 0.059 sec/iter\n",
      "Epoch: 06 | Batch: 010 / 016 | Total loss: 9.183 | Reg loss: 0.009 | Tree loss: 9.183 | Accuracy: 0.109375 | 0.059 sec/iter\n",
      "Epoch: 06 | Batch: 011 / 016 | Total loss: 9.186 | Reg loss: 0.009 | Tree loss: 9.186 | Accuracy: 0.121094 | 0.059 sec/iter\n",
      "Epoch: 06 | Batch: 012 / 016 | Total loss: 9.164 | Reg loss: 0.009 | Tree loss: 9.164 | Accuracy: 0.115234 | 0.058 sec/iter\n",
      "Epoch: 06 | Batch: 013 / 016 | Total loss: 9.157 | Reg loss: 0.010 | Tree loss: 9.157 | Accuracy: 0.115234 | 0.058 sec/iter\n",
      "Epoch: 06 | Batch: 014 / 016 | Total loss: 9.166 | Reg loss: 0.010 | Tree loss: 9.166 | Accuracy: 0.101562 | 0.058 sec/iter\n",
      "Epoch: 06 | Batch: 015 / 016 | Total loss: 9.154 | Reg loss: 0.010 | Tree loss: 9.154 | Accuracy: 0.098291 | 0.058 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 07 | Batch: 000 / 016 | Total loss: 9.204 | Reg loss: 0.008 | Tree loss: 9.204 | Accuracy: 0.095703 | 0.059 sec/iter\n",
      "Epoch: 07 | Batch: 001 / 016 | Total loss: 9.193 | Reg loss: 0.008 | Tree loss: 9.193 | Accuracy: 0.103516 | 0.059 sec/iter\n",
      "Epoch: 07 | Batch: 002 / 016 | Total loss: 9.181 | Reg loss: 0.008 | Tree loss: 9.181 | Accuracy: 0.109375 | 0.059 sec/iter\n",
      "Epoch: 07 | Batch: 003 / 016 | Total loss: 9.169 | Reg loss: 0.009 | Tree loss: 9.169 | Accuracy: 0.107422 | 0.059 sec/iter\n",
      "Epoch: 07 | Batch: 004 / 016 | Total loss: 9.155 | Reg loss: 0.009 | Tree loss: 9.155 | Accuracy: 0.111328 | 0.059 sec/iter\n",
      "Epoch: 07 | Batch: 005 / 016 | Total loss: 9.148 | Reg loss: 0.009 | Tree loss: 9.148 | Accuracy: 0.126953 | 0.059 sec/iter\n",
      "Epoch: 07 | Batch: 006 / 016 | Total loss: 9.155 | Reg loss: 0.009 | Tree loss: 9.155 | Accuracy: 0.103516 | 0.059 sec/iter\n",
      "Epoch: 07 | Batch: 007 / 016 | Total loss: 9.137 | Reg loss: 0.009 | Tree loss: 9.137 | Accuracy: 0.125000 | 0.059 sec/iter\n",
      "Epoch: 07 | Batch: 008 / 016 | Total loss: 9.130 | Reg loss: 0.009 | Tree loss: 9.130 | Accuracy: 0.111328 | 0.059 sec/iter\n",
      "Epoch: 07 | Batch: 009 / 016 | Total loss: 9.124 | Reg loss: 0.010 | Tree loss: 9.124 | Accuracy: 0.105469 | 0.059 sec/iter\n",
      "Epoch: 07 | Batch: 010 / 016 | Total loss: 9.109 | Reg loss: 0.010 | Tree loss: 9.109 | Accuracy: 0.111328 | 0.059 sec/iter\n",
      "Epoch: 07 | Batch: 011 / 016 | Total loss: 9.102 | Reg loss: 0.010 | Tree loss: 9.102 | Accuracy: 0.109375 | 0.059 sec/iter\n",
      "Epoch: 07 | Batch: 012 / 016 | Total loss: 9.100 | Reg loss: 0.010 | Tree loss: 9.100 | Accuracy: 0.099609 | 0.059 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 07 | Batch: 013 / 016 | Total loss: 9.084 | Reg loss: 0.011 | Tree loss: 9.084 | Accuracy: 0.087891 | 0.058 sec/iter\n",
      "Epoch: 07 | Batch: 014 / 016 | Total loss: 9.072 | Reg loss: 0.011 | Tree loss: 9.072 | Accuracy: 0.121094 | 0.058 sec/iter\n",
      "Epoch: 07 | Batch: 015 / 016 | Total loss: 9.063 | Reg loss: 0.011 | Tree loss: 9.063 | Accuracy: 0.123932 | 0.058 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 08 | Batch: 000 / 016 | Total loss: 9.107 | Reg loss: 0.009 | Tree loss: 9.107 | Accuracy: 0.121094 | 0.058 sec/iter\n",
      "Epoch: 08 | Batch: 001 / 016 | Total loss: 9.108 | Reg loss: 0.009 | Tree loss: 9.108 | Accuracy: 0.119141 | 0.059 sec/iter\n",
      "Epoch: 08 | Batch: 002 / 016 | Total loss: 9.108 | Reg loss: 0.010 | Tree loss: 9.108 | Accuracy: 0.111328 | 0.059 sec/iter\n",
      "Epoch: 08 | Batch: 003 / 016 | Total loss: 9.089 | Reg loss: 0.010 | Tree loss: 9.089 | Accuracy: 0.111328 | 0.058 sec/iter\n",
      "Epoch: 08 | Batch: 004 / 016 | Total loss: 9.085 | Reg loss: 0.010 | Tree loss: 9.085 | Accuracy: 0.113281 | 0.058 sec/iter\n",
      "Epoch: 08 | Batch: 005 / 016 | Total loss: 9.080 | Reg loss: 0.010 | Tree loss: 9.080 | Accuracy: 0.117188 | 0.058 sec/iter\n",
      "Epoch: 08 | Batch: 006 / 016 | Total loss: 9.073 | Reg loss: 0.010 | Tree loss: 9.073 | Accuracy: 0.099609 | 0.058 sec/iter\n",
      "Epoch: 08 | Batch: 007 / 016 | Total loss: 9.057 | Reg loss: 0.010 | Tree loss: 9.057 | Accuracy: 0.115234 | 0.058 sec/iter\n",
      "Epoch: 08 | Batch: 008 / 016 | Total loss: 9.056 | Reg loss: 0.011 | Tree loss: 9.056 | Accuracy: 0.101562 | 0.058 sec/iter\n",
      "Epoch: 08 | Batch: 009 / 016 | Total loss: 9.025 | Reg loss: 0.011 | Tree loss: 9.025 | Accuracy: 0.130859 | 0.058 sec/iter\n",
      "Epoch: 08 | Batch: 010 / 016 | Total loss: 9.021 | Reg loss: 0.011 | Tree loss: 9.021 | Accuracy: 0.115234 | 0.058 sec/iter\n",
      "Epoch: 08 | Batch: 011 / 016 | Total loss: 9.034 | Reg loss: 0.011 | Tree loss: 9.034 | Accuracy: 0.083984 | 0.058 sec/iter\n",
      "Epoch: 08 | Batch: 012 / 016 | Total loss: 8.998 | Reg loss: 0.012 | Tree loss: 8.998 | Accuracy: 0.109375 | 0.058 sec/iter\n",
      "Epoch: 08 | Batch: 013 / 016 | Total loss: 9.017 | Reg loss: 0.012 | Tree loss: 9.017 | Accuracy: 0.091797 | 0.058 sec/iter\n",
      "Epoch: 08 | Batch: 014 / 016 | Total loss: 8.997 | Reg loss: 0.012 | Tree loss: 8.997 | Accuracy: 0.105469 | 0.058 sec/iter\n",
      "Epoch: 08 | Batch: 015 / 016 | Total loss: 8.988 | Reg loss: 0.013 | Tree loss: 8.988 | Accuracy: 0.104701 | 0.058 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 09 | Batch: 000 / 016 | Total loss: 9.051 | Reg loss: 0.011 | Tree loss: 9.051 | Accuracy: 0.111328 | 0.058 sec/iter\n",
      "Epoch: 09 | Batch: 001 / 016 | Total loss: 9.039 | Reg loss: 0.011 | Tree loss: 9.039 | Accuracy: 0.099609 | 0.058 sec/iter\n",
      "Epoch: 09 | Batch: 002 / 016 | Total loss: 9.030 | Reg loss: 0.011 | Tree loss: 9.030 | Accuracy: 0.095703 | 0.058 sec/iter\n",
      "Epoch: 09 | Batch: 003 / 016 | Total loss: 9.011 | Reg loss: 0.011 | Tree loss: 9.011 | Accuracy: 0.115234 | 0.058 sec/iter\n",
      "Epoch: 09 | Batch: 004 / 016 | Total loss: 9.007 | Reg loss: 0.011 | Tree loss: 9.007 | Accuracy: 0.093750 | 0.058 sec/iter\n",
      "Epoch: 09 | Batch: 005 / 016 | Total loss: 8.995 | Reg loss: 0.011 | Tree loss: 8.995 | Accuracy: 0.123047 | 0.058 sec/iter\n",
      "Epoch: 09 | Batch: 006 / 016 | Total loss: 8.980 | Reg loss: 0.011 | Tree loss: 8.980 | Accuracy: 0.109375 | 0.058 sec/iter\n",
      "Epoch: 09 | Batch: 007 / 016 | Total loss: 8.969 | Reg loss: 0.012 | Tree loss: 8.969 | Accuracy: 0.117188 | 0.058 sec/iter\n",
      "Epoch: 09 | Batch: 008 / 016 | Total loss: 8.957 | Reg loss: 0.012 | Tree loss: 8.957 | Accuracy: 0.109375 | 0.058 sec/iter\n",
      "Epoch: 09 | Batch: 009 / 016 | Total loss: 8.958 | Reg loss: 0.012 | Tree loss: 8.958 | Accuracy: 0.128906 | 0.058 sec/iter\n",
      "Epoch: 09 | Batch: 010 / 016 | Total loss: 8.941 | Reg loss: 0.012 | Tree loss: 8.941 | Accuracy: 0.091797 | 0.058 sec/iter\n",
      "Epoch: 09 | Batch: 011 / 016 | Total loss: 8.932 | Reg loss: 0.013 | Tree loss: 8.932 | Accuracy: 0.097656 | 0.058 sec/iter\n",
      "Epoch: 09 | Batch: 012 / 016 | Total loss: 8.919 | Reg loss: 0.013 | Tree loss: 8.919 | Accuracy: 0.117188 | 0.058 sec/iter\n",
      "Epoch: 09 | Batch: 013 / 016 | Total loss: 8.905 | Reg loss: 0.013 | Tree loss: 8.905 | Accuracy: 0.111328 | 0.058 sec/iter\n",
      "Epoch: 09 | Batch: 014 / 016 | Total loss: 8.887 | Reg loss: 0.013 | Tree loss: 8.887 | Accuracy: 0.117188 | 0.058 sec/iter\n",
      "Epoch: 09 | Batch: 015 / 016 | Total loss: 8.884 | Reg loss: 0.014 | Tree loss: 8.884 | Accuracy: 0.113248 | 0.058 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 10 | Batch: 000 / 016 | Total loss: 8.948 | Reg loss: 0.012 | Tree loss: 8.948 | Accuracy: 0.103516 | 0.058 sec/iter\n",
      "Epoch: 10 | Batch: 001 / 016 | Total loss: 8.950 | Reg loss: 0.012 | Tree loss: 8.950 | Accuracy: 0.113281 | 0.058 sec/iter\n",
      "Epoch: 10 | Batch: 002 / 016 | Total loss: 8.942 | Reg loss: 0.012 | Tree loss: 8.942 | Accuracy: 0.115234 | 0.058 sec/iter\n",
      "Epoch: 10 | Batch: 003 / 016 | Total loss: 8.911 | Reg loss: 0.012 | Tree loss: 8.911 | Accuracy: 0.128906 | 0.058 sec/iter\n",
      "Epoch: 10 | Batch: 004 / 016 | Total loss: 8.942 | Reg loss: 0.012 | Tree loss: 8.942 | Accuracy: 0.072266 | 0.058 sec/iter\n",
      "Epoch: 10 | Batch: 005 / 016 | Total loss: 8.896 | Reg loss: 0.012 | Tree loss: 8.896 | Accuracy: 0.125000 | 0.059 sec/iter\n",
      "Epoch: 10 | Batch: 006 / 016 | Total loss: 8.894 | Reg loss: 0.012 | Tree loss: 8.894 | Accuracy: 0.107422 | 0.059 sec/iter\n",
      "Epoch: 10 | Batch: 007 / 016 | Total loss: 8.869 | Reg loss: 0.013 | Tree loss: 8.869 | Accuracy: 0.117188 | 0.059 sec/iter\n",
      "Epoch: 10 | Batch: 008 / 016 | Total loss: 8.870 | Reg loss: 0.013 | Tree loss: 8.870 | Accuracy: 0.091797 | 0.059 sec/iter\n",
      "Epoch: 10 | Batch: 009 / 016 | Total loss: 8.866 | Reg loss: 0.013 | Tree loss: 8.866 | Accuracy: 0.109375 | 0.059 sec/iter\n",
      "Epoch: 10 | Batch: 010 / 016 | Total loss: 8.850 | Reg loss: 0.013 | Tree loss: 8.850 | Accuracy: 0.093750 | 0.059 sec/iter\n",
      "Epoch: 10 | Batch: 011 / 016 | Total loss: 8.838 | Reg loss: 0.014 | Tree loss: 8.838 | Accuracy: 0.109375 | 0.059 sec/iter\n",
      "Epoch: 10 | Batch: 012 / 016 | Total loss: 8.832 | Reg loss: 0.014 | Tree loss: 8.832 | Accuracy: 0.109375 | 0.059 sec/iter\n",
      "Epoch: 10 | Batch: 013 / 016 | Total loss: 8.803 | Reg loss: 0.014 | Tree loss: 8.803 | Accuracy: 0.119141 | 0.059 sec/iter\n",
      "Epoch: 10 | Batch: 014 / 016 | Total loss: 8.795 | Reg loss: 0.014 | Tree loss: 8.795 | Accuracy: 0.128906 | 0.059 sec/iter\n",
      "Epoch: 10 | Batch: 015 / 016 | Total loss: 8.798 | Reg loss: 0.015 | Tree loss: 8.798 | Accuracy: 0.104701 | 0.059 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 11 | Batch: 000 / 016 | Total loss: 8.873 | Reg loss: 0.013 | Tree loss: 8.873 | Accuracy: 0.113281 | 0.059 sec/iter\n",
      "Epoch: 11 | Batch: 001 / 016 | Total loss: 8.843 | Reg loss: 0.013 | Tree loss: 8.843 | Accuracy: 0.123047 | 0.059 sec/iter\n",
      "Epoch: 11 | Batch: 002 / 016 | Total loss: 8.858 | Reg loss: 0.013 | Tree loss: 8.858 | Accuracy: 0.097656 | 0.059 sec/iter\n",
      "Epoch: 11 | Batch: 003 / 016 | Total loss: 8.855 | Reg loss: 0.013 | Tree loss: 8.855 | Accuracy: 0.103516 | 0.059 sec/iter\n",
      "Epoch: 11 | Batch: 004 / 016 | Total loss: 8.810 | Reg loss: 0.013 | Tree loss: 8.810 | Accuracy: 0.126953 | 0.059 sec/iter\n",
      "Epoch: 11 | Batch: 005 / 016 | Total loss: 8.801 | Reg loss: 0.013 | Tree loss: 8.801 | Accuracy: 0.130859 | 0.059 sec/iter\n",
      "Epoch: 11 | Batch: 006 / 016 | Total loss: 8.815 | Reg loss: 0.013 | Tree loss: 8.815 | Accuracy: 0.091797 | 0.059 sec/iter\n",
      "Epoch: 11 | Batch: 007 / 016 | Total loss: 8.777 | Reg loss: 0.014 | Tree loss: 8.777 | Accuracy: 0.091797 | 0.059 sec/iter\n",
      "Epoch: 11 | Batch: 008 / 016 | Total loss: 8.763 | Reg loss: 0.014 | Tree loss: 8.763 | Accuracy: 0.109375 | 0.058 sec/iter\n",
      "Epoch: 11 | Batch: 009 / 016 | Total loss: 8.788 | Reg loss: 0.014 | Tree loss: 8.788 | Accuracy: 0.064453 | 0.058 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11 | Batch: 010 / 016 | Total loss: 8.765 | Reg loss: 0.014 | Tree loss: 8.765 | Accuracy: 0.091797 | 0.058 sec/iter\n",
      "Epoch: 11 | Batch: 011 / 016 | Total loss: 8.719 | Reg loss: 0.015 | Tree loss: 8.719 | Accuracy: 0.132812 | 0.058 sec/iter\n",
      "Epoch: 11 | Batch: 012 / 016 | Total loss: 8.702 | Reg loss: 0.015 | Tree loss: 8.702 | Accuracy: 0.113281 | 0.058 sec/iter\n",
      "Epoch: 11 | Batch: 013 / 016 | Total loss: 8.717 | Reg loss: 0.015 | Tree loss: 8.717 | Accuracy: 0.113281 | 0.058 sec/iter\n",
      "Epoch: 11 | Batch: 014 / 016 | Total loss: 8.704 | Reg loss: 0.015 | Tree loss: 8.704 | Accuracy: 0.117188 | 0.058 sec/iter\n",
      "Epoch: 11 | Batch: 015 / 016 | Total loss: 8.673 | Reg loss: 0.016 | Tree loss: 8.673 | Accuracy: 0.121795 | 0.058 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 12 | Batch: 000 / 016 | Total loss: 8.786 | Reg loss: 0.014 | Tree loss: 8.786 | Accuracy: 0.115234 | 0.058 sec/iter\n",
      "Epoch: 12 | Batch: 001 / 016 | Total loss: 8.770 | Reg loss: 0.014 | Tree loss: 8.770 | Accuracy: 0.103516 | 0.058 sec/iter\n",
      "Epoch: 12 | Batch: 002 / 016 | Total loss: 8.735 | Reg loss: 0.014 | Tree loss: 8.735 | Accuracy: 0.103516 | 0.058 sec/iter\n",
      "Epoch: 12 | Batch: 003 / 016 | Total loss: 8.736 | Reg loss: 0.014 | Tree loss: 8.736 | Accuracy: 0.103516 | 0.058 sec/iter\n",
      "Epoch: 12 | Batch: 004 / 016 | Total loss: 8.720 | Reg loss: 0.014 | Tree loss: 8.720 | Accuracy: 0.123047 | 0.058 sec/iter\n",
      "Epoch: 12 | Batch: 005 / 016 | Total loss: 8.734 | Reg loss: 0.014 | Tree loss: 8.734 | Accuracy: 0.085938 | 0.058 sec/iter\n",
      "Epoch: 12 | Batch: 006 / 016 | Total loss: 8.700 | Reg loss: 0.014 | Tree loss: 8.700 | Accuracy: 0.117188 | 0.058 sec/iter\n",
      "Epoch: 12 | Batch: 007 / 016 | Total loss: 8.675 | Reg loss: 0.015 | Tree loss: 8.675 | Accuracy: 0.109375 | 0.058 sec/iter\n",
      "Epoch: 12 | Batch: 008 / 016 | Total loss: 8.693 | Reg loss: 0.015 | Tree loss: 8.693 | Accuracy: 0.113281 | 0.058 sec/iter\n",
      "Epoch: 12 | Batch: 009 / 016 | Total loss: 8.657 | Reg loss: 0.015 | Tree loss: 8.657 | Accuracy: 0.107422 | 0.058 sec/iter\n",
      "Epoch: 12 | Batch: 010 / 016 | Total loss: 8.632 | Reg loss: 0.015 | Tree loss: 8.632 | Accuracy: 0.119141 | 0.058 sec/iter\n",
      "Epoch: 12 | Batch: 011 / 016 | Total loss: 8.629 | Reg loss: 0.015 | Tree loss: 8.629 | Accuracy: 0.099609 | 0.058 sec/iter\n",
      "Epoch: 12 | Batch: 012 / 016 | Total loss: 8.615 | Reg loss: 0.016 | Tree loss: 8.615 | Accuracy: 0.105469 | 0.058 sec/iter\n",
      "Epoch: 12 | Batch: 013 / 016 | Total loss: 8.600 | Reg loss: 0.016 | Tree loss: 8.600 | Accuracy: 0.121094 | 0.058 sec/iter\n",
      "Epoch: 12 | Batch: 014 / 016 | Total loss: 8.581 | Reg loss: 0.016 | Tree loss: 8.581 | Accuracy: 0.115234 | 0.058 sec/iter\n",
      "Epoch: 12 | Batch: 015 / 016 | Total loss: 8.601 | Reg loss: 0.017 | Tree loss: 8.601 | Accuracy: 0.094017 | 0.058 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 13 | Batch: 000 / 016 | Total loss: 8.681 | Reg loss: 0.015 | Tree loss: 8.681 | Accuracy: 0.123047 | 0.058 sec/iter\n",
      "Epoch: 13 | Batch: 001 / 016 | Total loss: 8.659 | Reg loss: 0.015 | Tree loss: 8.659 | Accuracy: 0.119141 | 0.058 sec/iter\n",
      "Epoch: 13 | Batch: 002 / 016 | Total loss: 8.645 | Reg loss: 0.015 | Tree loss: 8.645 | Accuracy: 0.099609 | 0.058 sec/iter\n",
      "Epoch: 13 | Batch: 003 / 016 | Total loss: 8.647 | Reg loss: 0.015 | Tree loss: 8.647 | Accuracy: 0.130859 | 0.058 sec/iter\n",
      "Epoch: 13 | Batch: 004 / 016 | Total loss: 8.630 | Reg loss: 0.015 | Tree loss: 8.630 | Accuracy: 0.105469 | 0.058 sec/iter\n",
      "Epoch: 13 | Batch: 005 / 016 | Total loss: 8.618 | Reg loss: 0.015 | Tree loss: 8.618 | Accuracy: 0.103516 | 0.058 sec/iter\n",
      "Epoch: 13 | Batch: 006 / 016 | Total loss: 8.621 | Reg loss: 0.015 | Tree loss: 8.621 | Accuracy: 0.101562 | 0.058 sec/iter\n",
      "Epoch: 13 | Batch: 007 / 016 | Total loss: 8.578 | Reg loss: 0.016 | Tree loss: 8.578 | Accuracy: 0.111328 | 0.058 sec/iter\n",
      "Epoch: 13 | Batch: 008 / 016 | Total loss: 8.577 | Reg loss: 0.016 | Tree loss: 8.577 | Accuracy: 0.105469 | 0.058 sec/iter\n",
      "Epoch: 13 | Batch: 009 / 016 | Total loss: 8.549 | Reg loss: 0.016 | Tree loss: 8.549 | Accuracy: 0.101562 | 0.058 sec/iter\n",
      "Epoch: 13 | Batch: 010 / 016 | Total loss: 8.537 | Reg loss: 0.016 | Tree loss: 8.537 | Accuracy: 0.101562 | 0.058 sec/iter\n",
      "Epoch: 13 | Batch: 011 / 016 | Total loss: 8.520 | Reg loss: 0.016 | Tree loss: 8.520 | Accuracy: 0.109375 | 0.058 sec/iter\n",
      "Epoch: 13 | Batch: 012 / 016 | Total loss: 8.501 | Reg loss: 0.017 | Tree loss: 8.501 | Accuracy: 0.109375 | 0.058 sec/iter\n",
      "Epoch: 13 | Batch: 013 / 016 | Total loss: 8.494 | Reg loss: 0.017 | Tree loss: 8.494 | Accuracy: 0.115234 | 0.058 sec/iter\n",
      "Epoch: 13 | Batch: 014 / 016 | Total loss: 8.491 | Reg loss: 0.017 | Tree loss: 8.491 | Accuracy: 0.095703 | 0.058 sec/iter\n",
      "Epoch: 13 | Batch: 015 / 016 | Total loss: 8.454 | Reg loss: 0.017 | Tree loss: 8.454 | Accuracy: 0.106838 | 0.058 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 14 | Batch: 000 / 016 | Total loss: 8.583 | Reg loss: 0.016 | Tree loss: 8.583 | Accuracy: 0.097656 | 0.058 sec/iter\n",
      "Epoch: 14 | Batch: 001 / 016 | Total loss: 8.570 | Reg loss: 0.016 | Tree loss: 8.570 | Accuracy: 0.089844 | 0.058 sec/iter\n",
      "Epoch: 14 | Batch: 002 / 016 | Total loss: 8.559 | Reg loss: 0.016 | Tree loss: 8.559 | Accuracy: 0.119141 | 0.058 sec/iter\n",
      "Epoch: 14 | Batch: 003 / 016 | Total loss: 8.570 | Reg loss: 0.016 | Tree loss: 8.570 | Accuracy: 0.085938 | 0.058 sec/iter\n",
      "Epoch: 14 | Batch: 004 / 016 | Total loss: 8.522 | Reg loss: 0.016 | Tree loss: 8.522 | Accuracy: 0.113281 | 0.058 sec/iter\n",
      "Epoch: 14 | Batch: 005 / 016 | Total loss: 8.509 | Reg loss: 0.016 | Tree loss: 8.509 | Accuracy: 0.109375 | 0.058 sec/iter\n",
      "Epoch: 14 | Batch: 006 / 016 | Total loss: 8.483 | Reg loss: 0.016 | Tree loss: 8.483 | Accuracy: 0.103516 | 0.058 sec/iter\n",
      "Epoch: 14 | Batch: 007 / 016 | Total loss: 8.501 | Reg loss: 0.016 | Tree loss: 8.501 | Accuracy: 0.123047 | 0.058 sec/iter\n",
      "Epoch: 14 | Batch: 008 / 016 | Total loss: 8.452 | Reg loss: 0.017 | Tree loss: 8.452 | Accuracy: 0.111328 | 0.058 sec/iter\n",
      "Epoch: 14 | Batch: 009 / 016 | Total loss: 8.435 | Reg loss: 0.017 | Tree loss: 8.435 | Accuracy: 0.115234 | 0.058 sec/iter\n",
      "Epoch: 14 | Batch: 010 / 016 | Total loss: 8.452 | Reg loss: 0.017 | Tree loss: 8.452 | Accuracy: 0.109375 | 0.058 sec/iter\n",
      "Epoch: 14 | Batch: 011 / 016 | Total loss: 8.410 | Reg loss: 0.017 | Tree loss: 8.410 | Accuracy: 0.087891 | 0.058 sec/iter\n",
      "Epoch: 14 | Batch: 012 / 016 | Total loss: 8.383 | Reg loss: 0.017 | Tree loss: 8.383 | Accuracy: 0.113281 | 0.058 sec/iter\n",
      "Epoch: 14 | Batch: 013 / 016 | Total loss: 8.374 | Reg loss: 0.018 | Tree loss: 8.374 | Accuracy: 0.113281 | 0.058 sec/iter\n",
      "Epoch: 14 | Batch: 014 / 016 | Total loss: 8.360 | Reg loss: 0.018 | Tree loss: 8.360 | Accuracy: 0.115234 | 0.058 sec/iter\n",
      "Epoch: 14 | Batch: 015 / 016 | Total loss: 8.330 | Reg loss: 0.018 | Tree loss: 8.330 | Accuracy: 0.111111 | 0.058 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 15 | Batch: 000 / 016 | Total loss: 8.463 | Reg loss: 0.016 | Tree loss: 8.463 | Accuracy: 0.142578 | 0.058 sec/iter\n",
      "Epoch: 15 | Batch: 001 / 016 | Total loss: 8.464 | Reg loss: 0.016 | Tree loss: 8.464 | Accuracy: 0.101562 | 0.058 sec/iter\n",
      "Epoch: 15 | Batch: 002 / 016 | Total loss: 8.454 | Reg loss: 0.017 | Tree loss: 8.454 | Accuracy: 0.099609 | 0.058 sec/iter\n",
      "Epoch: 15 | Batch: 003 / 016 | Total loss: 8.473 | Reg loss: 0.017 | Tree loss: 8.473 | Accuracy: 0.089844 | 0.058 sec/iter\n",
      "Epoch: 15 | Batch: 004 / 016 | Total loss: 8.404 | Reg loss: 0.017 | Tree loss: 8.404 | Accuracy: 0.126953 | 0.058 sec/iter\n",
      "Epoch: 15 | Batch: 005 / 016 | Total loss: 8.391 | Reg loss: 0.017 | Tree loss: 8.391 | Accuracy: 0.115234 | 0.058 sec/iter\n",
      "Epoch: 15 | Batch: 006 / 016 | Total loss: 8.383 | Reg loss: 0.017 | Tree loss: 8.383 | Accuracy: 0.107422 | 0.058 sec/iter\n",
      "Epoch: 15 | Batch: 007 / 016 | Total loss: 8.392 | Reg loss: 0.017 | Tree loss: 8.392 | Accuracy: 0.095703 | 0.058 sec/iter\n",
      "Epoch: 15 | Batch: 008 / 016 | Total loss: 8.350 | Reg loss: 0.017 | Tree loss: 8.350 | Accuracy: 0.085938 | 0.058 sec/iter\n",
      "Epoch: 15 | Batch: 009 / 016 | Total loss: 8.326 | Reg loss: 0.017 | Tree loss: 8.326 | Accuracy: 0.097656 | 0.058 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15 | Batch: 010 / 016 | Total loss: 8.314 | Reg loss: 0.018 | Tree loss: 8.314 | Accuracy: 0.083984 | 0.058 sec/iter\n",
      "Epoch: 15 | Batch: 011 / 016 | Total loss: 8.309 | Reg loss: 0.018 | Tree loss: 8.309 | Accuracy: 0.125000 | 0.058 sec/iter\n",
      "Epoch: 15 | Batch: 012 / 016 | Total loss: 8.258 | Reg loss: 0.018 | Tree loss: 8.258 | Accuracy: 0.109375 | 0.058 sec/iter\n",
      "Epoch: 15 | Batch: 013 / 016 | Total loss: 8.287 | Reg loss: 0.018 | Tree loss: 8.287 | Accuracy: 0.097656 | 0.058 sec/iter\n",
      "Epoch: 15 | Batch: 014 / 016 | Total loss: 8.257 | Reg loss: 0.018 | Tree loss: 8.257 | Accuracy: 0.095703 | 0.058 sec/iter\n",
      "Epoch: 15 | Batch: 015 / 016 | Total loss: 8.222 | Reg loss: 0.019 | Tree loss: 8.222 | Accuracy: 0.117521 | 0.058 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 16 | Batch: 000 / 016 | Total loss: 8.400 | Reg loss: 0.017 | Tree loss: 8.400 | Accuracy: 0.103516 | 0.058 sec/iter\n",
      "Epoch: 16 | Batch: 001 / 016 | Total loss: 8.373 | Reg loss: 0.017 | Tree loss: 8.373 | Accuracy: 0.099609 | 0.058 sec/iter\n",
      "Epoch: 16 | Batch: 002 / 016 | Total loss: 8.321 | Reg loss: 0.017 | Tree loss: 8.321 | Accuracy: 0.113281 | 0.058 sec/iter\n",
      "Epoch: 16 | Batch: 003 / 016 | Total loss: 8.321 | Reg loss: 0.017 | Tree loss: 8.321 | Accuracy: 0.105469 | 0.058 sec/iter\n",
      "Epoch: 16 | Batch: 004 / 016 | Total loss: 8.307 | Reg loss: 0.018 | Tree loss: 8.307 | Accuracy: 0.105469 | 0.058 sec/iter\n",
      "Epoch: 16 | Batch: 005 / 016 | Total loss: 8.274 | Reg loss: 0.018 | Tree loss: 8.274 | Accuracy: 0.125000 | 0.058 sec/iter\n",
      "Epoch: 16 | Batch: 006 / 016 | Total loss: 8.287 | Reg loss: 0.018 | Tree loss: 8.287 | Accuracy: 0.097656 | 0.058 sec/iter\n",
      "Epoch: 16 | Batch: 007 / 016 | Total loss: 8.284 | Reg loss: 0.018 | Tree loss: 8.284 | Accuracy: 0.078125 | 0.058 sec/iter\n",
      "Epoch: 16 | Batch: 008 / 016 | Total loss: 8.241 | Reg loss: 0.018 | Tree loss: 8.241 | Accuracy: 0.105469 | 0.058 sec/iter\n",
      "Epoch: 16 | Batch: 009 / 016 | Total loss: 8.222 | Reg loss: 0.018 | Tree loss: 8.222 | Accuracy: 0.105469 | 0.058 sec/iter\n",
      "Epoch: 16 | Batch: 010 / 016 | Total loss: 8.196 | Reg loss: 0.018 | Tree loss: 8.196 | Accuracy: 0.117188 | 0.058 sec/iter\n",
      "Epoch: 16 | Batch: 011 / 016 | Total loss: 8.192 | Reg loss: 0.019 | Tree loss: 8.192 | Accuracy: 0.109375 | 0.058 sec/iter\n",
      "Epoch: 16 | Batch: 012 / 016 | Total loss: 8.150 | Reg loss: 0.019 | Tree loss: 8.150 | Accuracy: 0.103516 | 0.058 sec/iter\n",
      "Epoch: 16 | Batch: 013 / 016 | Total loss: 8.130 | Reg loss: 0.019 | Tree loss: 8.130 | Accuracy: 0.121094 | 0.058 sec/iter\n",
      "Epoch: 16 | Batch: 014 / 016 | Total loss: 8.145 | Reg loss: 0.019 | Tree loss: 8.145 | Accuracy: 0.111328 | 0.058 sec/iter\n",
      "Epoch: 16 | Batch: 015 / 016 | Total loss: 8.121 | Reg loss: 0.019 | Tree loss: 8.121 | Accuracy: 0.089744 | 0.058 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 17 | Batch: 000 / 016 | Total loss: 8.257 | Reg loss: 0.018 | Tree loss: 8.257 | Accuracy: 0.105469 | 0.058 sec/iter\n",
      "Epoch: 17 | Batch: 001 / 016 | Total loss: 8.250 | Reg loss: 0.018 | Tree loss: 8.250 | Accuracy: 0.091797 | 0.058 sec/iter\n",
      "Epoch: 17 | Batch: 002 / 016 | Total loss: 8.246 | Reg loss: 0.018 | Tree loss: 8.246 | Accuracy: 0.115234 | 0.058 sec/iter\n",
      "Epoch: 17 | Batch: 003 / 016 | Total loss: 8.216 | Reg loss: 0.018 | Tree loss: 8.216 | Accuracy: 0.080078 | 0.058 sec/iter\n",
      "Epoch: 17 | Batch: 004 / 016 | Total loss: 8.211 | Reg loss: 0.018 | Tree loss: 8.211 | Accuracy: 0.107422 | 0.058 sec/iter\n",
      "Epoch: 17 | Batch: 005 / 016 | Total loss: 8.185 | Reg loss: 0.018 | Tree loss: 8.185 | Accuracy: 0.105469 | 0.058 sec/iter\n",
      "Epoch: 17 | Batch: 006 / 016 | Total loss: 8.176 | Reg loss: 0.018 | Tree loss: 8.176 | Accuracy: 0.105469 | 0.058 sec/iter\n",
      "Epoch: 17 | Batch: 007 / 016 | Total loss: 8.154 | Reg loss: 0.019 | Tree loss: 8.154 | Accuracy: 0.091797 | 0.058 sec/iter\n",
      "Epoch: 17 | Batch: 008 / 016 | Total loss: 8.090 | Reg loss: 0.019 | Tree loss: 8.090 | Accuracy: 0.107422 | 0.058 sec/iter\n",
      "Epoch: 17 | Batch: 009 / 016 | Total loss: 8.125 | Reg loss: 0.019 | Tree loss: 8.125 | Accuracy: 0.095703 | 0.058 sec/iter\n",
      "Epoch: 17 | Batch: 010 / 016 | Total loss: 8.077 | Reg loss: 0.019 | Tree loss: 8.077 | Accuracy: 0.113281 | 0.058 sec/iter\n",
      "Epoch: 17 | Batch: 011 / 016 | Total loss: 8.066 | Reg loss: 0.019 | Tree loss: 8.066 | Accuracy: 0.115234 | 0.058 sec/iter\n",
      "Epoch: 17 | Batch: 012 / 016 | Total loss: 8.043 | Reg loss: 0.019 | Tree loss: 8.043 | Accuracy: 0.101562 | 0.058 sec/iter\n",
      "Epoch: 17 | Batch: 013 / 016 | Total loss: 8.024 | Reg loss: 0.020 | Tree loss: 8.024 | Accuracy: 0.087891 | 0.058 sec/iter\n",
      "Epoch: 17 | Batch: 014 / 016 | Total loss: 8.006 | Reg loss: 0.020 | Tree loss: 8.006 | Accuracy: 0.115234 | 0.058 sec/iter\n",
      "Epoch: 17 | Batch: 015 / 016 | Total loss: 8.017 | Reg loss: 0.020 | Tree loss: 8.017 | Accuracy: 0.117521 | 0.058 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 18 | Batch: 000 / 016 | Total loss: 8.171 | Reg loss: 0.019 | Tree loss: 8.171 | Accuracy: 0.083984 | 0.058 sec/iter\n",
      "Epoch: 18 | Batch: 001 / 016 | Total loss: 8.152 | Reg loss: 0.019 | Tree loss: 8.152 | Accuracy: 0.083984 | 0.058 sec/iter\n",
      "Epoch: 18 | Batch: 002 / 016 | Total loss: 8.110 | Reg loss: 0.019 | Tree loss: 8.110 | Accuracy: 0.109375 | 0.058 sec/iter\n",
      "Epoch: 18 | Batch: 003 / 016 | Total loss: 8.102 | Reg loss: 0.019 | Tree loss: 8.102 | Accuracy: 0.123047 | 0.058 sec/iter\n",
      "Epoch: 18 | Batch: 004 / 016 | Total loss: 8.082 | Reg loss: 0.019 | Tree loss: 8.082 | Accuracy: 0.099609 | 0.058 sec/iter\n",
      "Epoch: 18 | Batch: 005 / 016 | Total loss: 8.067 | Reg loss: 0.019 | Tree loss: 8.067 | Accuracy: 0.113281 | 0.058 sec/iter\n",
      "Epoch: 18 | Batch: 006 / 016 | Total loss: 8.036 | Reg loss: 0.019 | Tree loss: 8.036 | Accuracy: 0.101562 | 0.058 sec/iter\n",
      "Epoch: 18 | Batch: 007 / 016 | Total loss: 8.032 | Reg loss: 0.019 | Tree loss: 8.032 | Accuracy: 0.109375 | 0.058 sec/iter\n",
      "Epoch: 18 | Batch: 008 / 016 | Total loss: 8.036 | Reg loss: 0.019 | Tree loss: 8.036 | Accuracy: 0.105469 | 0.058 sec/iter\n",
      "Epoch: 18 | Batch: 009 / 016 | Total loss: 7.990 | Reg loss: 0.020 | Tree loss: 7.990 | Accuracy: 0.095703 | 0.058 sec/iter\n",
      "Epoch: 18 | Batch: 010 / 016 | Total loss: 7.945 | Reg loss: 0.020 | Tree loss: 7.945 | Accuracy: 0.107422 | 0.058 sec/iter\n",
      "Epoch: 18 | Batch: 011 / 016 | Total loss: 7.956 | Reg loss: 0.020 | Tree loss: 7.956 | Accuracy: 0.123047 | 0.058 sec/iter\n",
      "Epoch: 18 | Batch: 012 / 016 | Total loss: 7.945 | Reg loss: 0.020 | Tree loss: 7.945 | Accuracy: 0.099609 | 0.058 sec/iter\n",
      "Epoch: 18 | Batch: 013 / 016 | Total loss: 7.909 | Reg loss: 0.020 | Tree loss: 7.909 | Accuracy: 0.117188 | 0.058 sec/iter\n",
      "Epoch: 18 | Batch: 014 / 016 | Total loss: 7.891 | Reg loss: 0.020 | Tree loss: 7.891 | Accuracy: 0.109375 | 0.058 sec/iter\n",
      "Epoch: 18 | Batch: 015 / 016 | Total loss: 7.860 | Reg loss: 0.021 | Tree loss: 7.860 | Accuracy: 0.081197 | 0.058 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 19 | Batch: 000 / 016 | Total loss: 8.043 | Reg loss: 0.019 | Tree loss: 8.043 | Accuracy: 0.091797 | 0.058 sec/iter\n",
      "Epoch: 19 | Batch: 001 / 016 | Total loss: 8.030 | Reg loss: 0.019 | Tree loss: 8.030 | Accuracy: 0.095703 | 0.058 sec/iter\n",
      "Epoch: 19 | Batch: 002 / 016 | Total loss: 7.975 | Reg loss: 0.019 | Tree loss: 7.975 | Accuracy: 0.113281 | 0.058 sec/iter\n",
      "Epoch: 19 | Batch: 003 / 016 | Total loss: 7.988 | Reg loss: 0.020 | Tree loss: 7.988 | Accuracy: 0.103516 | 0.058 sec/iter\n",
      "Epoch: 19 | Batch: 004 / 016 | Total loss: 8.006 | Reg loss: 0.020 | Tree loss: 8.006 | Accuracy: 0.082031 | 0.058 sec/iter\n",
      "Epoch: 19 | Batch: 005 / 016 | Total loss: 7.947 | Reg loss: 0.020 | Tree loss: 7.947 | Accuracy: 0.089844 | 0.058 sec/iter\n",
      "Epoch: 19 | Batch: 006 / 016 | Total loss: 7.930 | Reg loss: 0.020 | Tree loss: 7.930 | Accuracy: 0.101562 | 0.058 sec/iter\n",
      "Epoch: 19 | Batch: 007 / 016 | Total loss: 7.914 | Reg loss: 0.020 | Tree loss: 7.914 | Accuracy: 0.097656 | 0.058 sec/iter\n",
      "Epoch: 19 | Batch: 008 / 016 | Total loss: 7.868 | Reg loss: 0.020 | Tree loss: 7.868 | Accuracy: 0.117188 | 0.058 sec/iter\n",
      "Epoch: 19 | Batch: 009 / 016 | Total loss: 7.881 | Reg loss: 0.020 | Tree loss: 7.881 | Accuracy: 0.097656 | 0.058 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19 | Batch: 010 / 016 | Total loss: 7.854 | Reg loss: 0.020 | Tree loss: 7.854 | Accuracy: 0.107422 | 0.058 sec/iter\n",
      "Epoch: 19 | Batch: 011 / 016 | Total loss: 7.856 | Reg loss: 0.021 | Tree loss: 7.856 | Accuracy: 0.093750 | 0.058 sec/iter\n",
      "Epoch: 19 | Batch: 012 / 016 | Total loss: 7.787 | Reg loss: 0.021 | Tree loss: 7.787 | Accuracy: 0.105469 | 0.058 sec/iter\n",
      "Epoch: 19 | Batch: 013 / 016 | Total loss: 7.755 | Reg loss: 0.021 | Tree loss: 7.755 | Accuracy: 0.109375 | 0.058 sec/iter\n",
      "Epoch: 19 | Batch: 014 / 016 | Total loss: 7.799 | Reg loss: 0.021 | Tree loss: 7.799 | Accuracy: 0.091797 | 0.058 sec/iter\n",
      "Epoch: 19 | Batch: 015 / 016 | Total loss: 7.779 | Reg loss: 0.021 | Tree loss: 7.779 | Accuracy: 0.094017 | 0.058 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 20 | Batch: 000 / 016 | Total loss: 7.943 | Reg loss: 0.020 | Tree loss: 7.943 | Accuracy: 0.103516 | 0.058 sec/iter\n",
      "Epoch: 20 | Batch: 001 / 016 | Total loss: 7.867 | Reg loss: 0.020 | Tree loss: 7.867 | Accuracy: 0.105469 | 0.058 sec/iter\n",
      "Epoch: 20 | Batch: 002 / 016 | Total loss: 7.876 | Reg loss: 0.020 | Tree loss: 7.876 | Accuracy: 0.105469 | 0.058 sec/iter\n",
      "Epoch: 20 | Batch: 003 / 016 | Total loss: 7.866 | Reg loss: 0.020 | Tree loss: 7.866 | Accuracy: 0.119141 | 0.058 sec/iter\n",
      "Epoch: 20 | Batch: 004 / 016 | Total loss: 7.863 | Reg loss: 0.020 | Tree loss: 7.863 | Accuracy: 0.117188 | 0.058 sec/iter\n",
      "Epoch: 20 | Batch: 005 / 016 | Total loss: 7.819 | Reg loss: 0.020 | Tree loss: 7.819 | Accuracy: 0.091797 | 0.058 sec/iter\n",
      "Epoch: 20 | Batch: 006 / 016 | Total loss: 7.822 | Reg loss: 0.020 | Tree loss: 7.822 | Accuracy: 0.109375 | 0.058 sec/iter\n",
      "Epoch: 20 | Batch: 007 / 016 | Total loss: 7.794 | Reg loss: 0.021 | Tree loss: 7.794 | Accuracy: 0.097656 | 0.058 sec/iter\n",
      "Epoch: 20 | Batch: 008 / 016 | Total loss: 7.790 | Reg loss: 0.021 | Tree loss: 7.790 | Accuracy: 0.076172 | 0.058 sec/iter\n",
      "Epoch: 20 | Batch: 009 / 016 | Total loss: 7.763 | Reg loss: 0.021 | Tree loss: 7.763 | Accuracy: 0.087891 | 0.058 sec/iter\n",
      "Epoch: 20 | Batch: 010 / 016 | Total loss: 7.713 | Reg loss: 0.021 | Tree loss: 7.713 | Accuracy: 0.121094 | 0.058 sec/iter\n",
      "Epoch: 20 | Batch: 011 / 016 | Total loss: 7.709 | Reg loss: 0.021 | Tree loss: 7.709 | Accuracy: 0.101562 | 0.058 sec/iter\n",
      "Epoch: 20 | Batch: 012 / 016 | Total loss: 7.709 | Reg loss: 0.021 | Tree loss: 7.709 | Accuracy: 0.089844 | 0.058 sec/iter\n",
      "Epoch: 20 | Batch: 013 / 016 | Total loss: 7.665 | Reg loss: 0.021 | Tree loss: 7.665 | Accuracy: 0.087891 | 0.058 sec/iter\n",
      "Epoch: 20 | Batch: 014 / 016 | Total loss: 7.672 | Reg loss: 0.022 | Tree loss: 7.672 | Accuracy: 0.070312 | 0.058 sec/iter\n",
      "Epoch: 20 | Batch: 015 / 016 | Total loss: 7.658 | Reg loss: 0.022 | Tree loss: 7.658 | Accuracy: 0.094017 | 0.058 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 21 | Batch: 000 / 016 | Total loss: 7.859 | Reg loss: 0.021 | Tree loss: 7.859 | Accuracy: 0.107422 | 0.058 sec/iter\n",
      "Epoch: 21 | Batch: 001 / 016 | Total loss: 7.807 | Reg loss: 0.021 | Tree loss: 7.807 | Accuracy: 0.089844 | 0.058 sec/iter\n",
      "Epoch: 21 | Batch: 002 / 016 | Total loss: 7.766 | Reg loss: 0.021 | Tree loss: 7.766 | Accuracy: 0.113281 | 0.058 sec/iter\n",
      "Epoch: 21 | Batch: 003 / 016 | Total loss: 7.772 | Reg loss: 0.021 | Tree loss: 7.772 | Accuracy: 0.089844 | 0.058 sec/iter\n",
      "Epoch: 21 | Batch: 004 / 016 | Total loss: 7.716 | Reg loss: 0.021 | Tree loss: 7.716 | Accuracy: 0.125000 | 0.058 sec/iter\n",
      "Epoch: 21 | Batch: 005 / 016 | Total loss: 7.664 | Reg loss: 0.021 | Tree loss: 7.664 | Accuracy: 0.126953 | 0.058 sec/iter\n",
      "Epoch: 21 | Batch: 006 / 016 | Total loss: 7.718 | Reg loss: 0.021 | Tree loss: 7.718 | Accuracy: 0.074219 | 0.058 sec/iter\n",
      "Epoch: 21 | Batch: 007 / 016 | Total loss: 7.712 | Reg loss: 0.021 | Tree loss: 7.712 | Accuracy: 0.074219 | 0.058 sec/iter\n",
      "Epoch: 21 | Batch: 008 / 016 | Total loss: 7.638 | Reg loss: 0.021 | Tree loss: 7.638 | Accuracy: 0.089844 | 0.058 sec/iter\n",
      "Epoch: 21 | Batch: 009 / 016 | Total loss: 7.630 | Reg loss: 0.021 | Tree loss: 7.630 | Accuracy: 0.095703 | 0.058 sec/iter\n",
      "Epoch: 21 | Batch: 010 / 016 | Total loss: 7.628 | Reg loss: 0.022 | Tree loss: 7.628 | Accuracy: 0.093750 | 0.058 sec/iter\n",
      "Epoch: 21 | Batch: 011 / 016 | Total loss: 7.581 | Reg loss: 0.022 | Tree loss: 7.581 | Accuracy: 0.105469 | 0.058 sec/iter\n",
      "Epoch: 21 | Batch: 012 / 016 | Total loss: 7.571 | Reg loss: 0.022 | Tree loss: 7.571 | Accuracy: 0.093750 | 0.058 sec/iter\n",
      "Epoch: 21 | Batch: 013 / 016 | Total loss: 7.582 | Reg loss: 0.022 | Tree loss: 7.582 | Accuracy: 0.099609 | 0.058 sec/iter\n",
      "Epoch: 21 | Batch: 014 / 016 | Total loss: 7.514 | Reg loss: 0.022 | Tree loss: 7.514 | Accuracy: 0.107422 | 0.058 sec/iter\n",
      "Epoch: 21 | Batch: 015 / 016 | Total loss: 7.498 | Reg loss: 0.022 | Tree loss: 7.498 | Accuracy: 0.104701 | 0.058 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 22 | Batch: 000 / 016 | Total loss: 7.678 | Reg loss: 0.021 | Tree loss: 7.678 | Accuracy: 0.111328 | 0.058 sec/iter\n",
      "Epoch: 22 | Batch: 001 / 016 | Total loss: 7.615 | Reg loss: 0.021 | Tree loss: 7.615 | Accuracy: 0.128906 | 0.058 sec/iter\n",
      "Epoch: 22 | Batch: 002 / 016 | Total loss: 7.652 | Reg loss: 0.021 | Tree loss: 7.652 | Accuracy: 0.109375 | 0.058 sec/iter\n",
      "Epoch: 22 | Batch: 003 / 016 | Total loss: 7.621 | Reg loss: 0.021 | Tree loss: 7.621 | Accuracy: 0.087891 | 0.058 sec/iter\n",
      "Epoch: 22 | Batch: 004 / 016 | Total loss: 7.574 | Reg loss: 0.022 | Tree loss: 7.574 | Accuracy: 0.105469 | 0.058 sec/iter\n",
      "Epoch: 22 | Batch: 005 / 016 | Total loss: 7.598 | Reg loss: 0.022 | Tree loss: 7.598 | Accuracy: 0.109375 | 0.058 sec/iter\n",
      "Epoch: 22 | Batch: 006 / 016 | Total loss: 7.618 | Reg loss: 0.022 | Tree loss: 7.618 | Accuracy: 0.089844 | 0.058 sec/iter\n",
      "Epoch: 22 | Batch: 007 / 016 | Total loss: 7.557 | Reg loss: 0.022 | Tree loss: 7.557 | Accuracy: 0.105469 | 0.058 sec/iter\n",
      "Epoch: 22 | Batch: 008 / 016 | Total loss: 7.531 | Reg loss: 0.022 | Tree loss: 7.531 | Accuracy: 0.083984 | 0.058 sec/iter\n",
      "Epoch: 22 | Batch: 009 / 016 | Total loss: 7.528 | Reg loss: 0.022 | Tree loss: 7.528 | Accuracy: 0.082031 | 0.058 sec/iter\n",
      "Epoch: 22 | Batch: 010 / 016 | Total loss: 7.526 | Reg loss: 0.022 | Tree loss: 7.526 | Accuracy: 0.091797 | 0.058 sec/iter\n",
      "Epoch: 22 | Batch: 011 / 016 | Total loss: 7.466 | Reg loss: 0.022 | Tree loss: 7.466 | Accuracy: 0.089844 | 0.058 sec/iter\n",
      "Epoch: 22 | Batch: 012 / 016 | Total loss: 7.475 | Reg loss: 0.022 | Tree loss: 7.475 | Accuracy: 0.095703 | 0.058 sec/iter\n",
      "Epoch: 22 | Batch: 013 / 016 | Total loss: 7.473 | Reg loss: 0.023 | Tree loss: 7.473 | Accuracy: 0.093750 | 0.058 sec/iter\n",
      "Epoch: 22 | Batch: 014 / 016 | Total loss: 7.462 | Reg loss: 0.023 | Tree loss: 7.462 | Accuracy: 0.076172 | 0.058 sec/iter\n",
      "Epoch: 22 | Batch: 015 / 016 | Total loss: 7.436 | Reg loss: 0.023 | Tree loss: 7.436 | Accuracy: 0.074786 | 0.058 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 23 | Batch: 000 / 016 | Total loss: 7.573 | Reg loss: 0.022 | Tree loss: 7.573 | Accuracy: 0.107422 | 0.058 sec/iter\n",
      "Epoch: 23 | Batch: 001 / 016 | Total loss: 7.552 | Reg loss: 0.022 | Tree loss: 7.552 | Accuracy: 0.115234 | 0.058 sec/iter\n",
      "Epoch: 23 | Batch: 002 / 016 | Total loss: 7.547 | Reg loss: 0.022 | Tree loss: 7.547 | Accuracy: 0.105469 | 0.058 sec/iter\n",
      "Epoch: 23 | Batch: 003 / 016 | Total loss: 7.518 | Reg loss: 0.022 | Tree loss: 7.518 | Accuracy: 0.105469 | 0.058 sec/iter\n",
      "Epoch: 23 | Batch: 004 / 016 | Total loss: 7.496 | Reg loss: 0.022 | Tree loss: 7.496 | Accuracy: 0.091797 | 0.058 sec/iter\n",
      "Epoch: 23 | Batch: 005 / 016 | Total loss: 7.492 | Reg loss: 0.022 | Tree loss: 7.492 | Accuracy: 0.089844 | 0.058 sec/iter\n",
      "Epoch: 23 | Batch: 006 / 016 | Total loss: 7.453 | Reg loss: 0.022 | Tree loss: 7.453 | Accuracy: 0.097656 | 0.058 sec/iter\n",
      "Epoch: 23 | Batch: 007 / 016 | Total loss: 7.440 | Reg loss: 0.022 | Tree loss: 7.440 | Accuracy: 0.103516 | 0.058 sec/iter\n",
      "Epoch: 23 | Batch: 008 / 016 | Total loss: 7.423 | Reg loss: 0.022 | Tree loss: 7.423 | Accuracy: 0.093750 | 0.058 sec/iter\n",
      "Epoch: 23 | Batch: 009 / 016 | Total loss: 7.428 | Reg loss: 0.023 | Tree loss: 7.428 | Accuracy: 0.083984 | 0.058 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23 | Batch: 010 / 016 | Total loss: 7.406 | Reg loss: 0.023 | Tree loss: 7.406 | Accuracy: 0.091797 | 0.058 sec/iter\n",
      "Epoch: 23 | Batch: 011 / 016 | Total loss: 7.387 | Reg loss: 0.023 | Tree loss: 7.387 | Accuracy: 0.095703 | 0.058 sec/iter\n",
      "Epoch: 23 | Batch: 012 / 016 | Total loss: 7.334 | Reg loss: 0.023 | Tree loss: 7.334 | Accuracy: 0.072266 | 0.058 sec/iter\n",
      "Epoch: 23 | Batch: 013 / 016 | Total loss: 7.345 | Reg loss: 0.023 | Tree loss: 7.345 | Accuracy: 0.091797 | 0.058 sec/iter\n",
      "Epoch: 23 | Batch: 014 / 016 | Total loss: 7.295 | Reg loss: 0.023 | Tree loss: 7.295 | Accuracy: 0.078125 | 0.058 sec/iter\n",
      "Epoch: 23 | Batch: 015 / 016 | Total loss: 7.286 | Reg loss: 0.023 | Tree loss: 7.286 | Accuracy: 0.083333 | 0.058 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 24 | Batch: 000 / 016 | Total loss: 7.455 | Reg loss: 0.022 | Tree loss: 7.455 | Accuracy: 0.101562 | 0.058 sec/iter\n",
      "Epoch: 24 | Batch: 001 / 016 | Total loss: 7.443 | Reg loss: 0.022 | Tree loss: 7.443 | Accuracy: 0.099609 | 0.058 sec/iter\n",
      "Epoch: 24 | Batch: 002 / 016 | Total loss: 7.435 | Reg loss: 0.022 | Tree loss: 7.435 | Accuracy: 0.101562 | 0.058 sec/iter\n",
      "Epoch: 24 | Batch: 003 / 016 | Total loss: 7.399 | Reg loss: 0.023 | Tree loss: 7.399 | Accuracy: 0.107422 | 0.058 sec/iter\n",
      "Epoch: 24 | Batch: 004 / 016 | Total loss: 7.398 | Reg loss: 0.023 | Tree loss: 7.398 | Accuracy: 0.074219 | 0.058 sec/iter\n",
      "Epoch: 24 | Batch: 005 / 016 | Total loss: 7.385 | Reg loss: 0.023 | Tree loss: 7.385 | Accuracy: 0.105469 | 0.058 sec/iter\n",
      "Epoch: 24 | Batch: 006 / 016 | Total loss: 7.365 | Reg loss: 0.023 | Tree loss: 7.365 | Accuracy: 0.095703 | 0.058 sec/iter\n",
      "Epoch: 24 | Batch: 007 / 016 | Total loss: 7.311 | Reg loss: 0.023 | Tree loss: 7.311 | Accuracy: 0.107422 | 0.058 sec/iter\n",
      "Epoch: 24 | Batch: 008 / 016 | Total loss: 7.328 | Reg loss: 0.023 | Tree loss: 7.328 | Accuracy: 0.103516 | 0.058 sec/iter\n",
      "Epoch: 24 | Batch: 009 / 016 | Total loss: 7.282 | Reg loss: 0.023 | Tree loss: 7.282 | Accuracy: 0.103516 | 0.058 sec/iter\n",
      "Epoch: 24 | Batch: 010 / 016 | Total loss: 7.260 | Reg loss: 0.023 | Tree loss: 7.260 | Accuracy: 0.087891 | 0.058 sec/iter\n",
      "Epoch: 24 | Batch: 011 / 016 | Total loss: 7.264 | Reg loss: 0.023 | Tree loss: 7.264 | Accuracy: 0.068359 | 0.058 sec/iter\n",
      "Epoch: 24 | Batch: 012 / 016 | Total loss: 7.222 | Reg loss: 0.023 | Tree loss: 7.222 | Accuracy: 0.068359 | 0.058 sec/iter\n",
      "Epoch: 24 | Batch: 013 / 016 | Total loss: 7.258 | Reg loss: 0.023 | Tree loss: 7.258 | Accuracy: 0.082031 | 0.058 sec/iter\n",
      "Epoch: 24 | Batch: 014 / 016 | Total loss: 7.192 | Reg loss: 0.024 | Tree loss: 7.192 | Accuracy: 0.099609 | 0.058 sec/iter\n",
      "Epoch: 24 | Batch: 015 / 016 | Total loss: 7.170 | Reg loss: 0.024 | Tree loss: 7.170 | Accuracy: 0.081197 | 0.058 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 25 | Batch: 000 / 016 | Total loss: 7.327 | Reg loss: 0.023 | Tree loss: 7.327 | Accuracy: 0.123047 | 0.058 sec/iter\n",
      "Epoch: 25 | Batch: 001 / 016 | Total loss: 7.367 | Reg loss: 0.023 | Tree loss: 7.367 | Accuracy: 0.087891 | 0.058 sec/iter\n",
      "Epoch: 25 | Batch: 002 / 016 | Total loss: 7.279 | Reg loss: 0.023 | Tree loss: 7.279 | Accuracy: 0.113281 | 0.058 sec/iter\n",
      "Epoch: 25 | Batch: 003 / 016 | Total loss: 7.280 | Reg loss: 0.023 | Tree loss: 7.280 | Accuracy: 0.082031 | 0.058 sec/iter\n",
      "Epoch: 25 | Batch: 004 / 016 | Total loss: 7.290 | Reg loss: 0.023 | Tree loss: 7.290 | Accuracy: 0.083984 | 0.058 sec/iter\n",
      "Epoch: 25 | Batch: 005 / 016 | Total loss: 7.236 | Reg loss: 0.023 | Tree loss: 7.236 | Accuracy: 0.105469 | 0.058 sec/iter\n",
      "Epoch: 25 | Batch: 006 / 016 | Total loss: 7.247 | Reg loss: 0.023 | Tree loss: 7.247 | Accuracy: 0.078125 | 0.058 sec/iter\n",
      "Epoch: 25 | Batch: 007 / 016 | Total loss: 7.204 | Reg loss: 0.023 | Tree loss: 7.204 | Accuracy: 0.082031 | 0.058 sec/iter\n",
      "Epoch: 25 | Batch: 008 / 016 | Total loss: 7.211 | Reg loss: 0.023 | Tree loss: 7.211 | Accuracy: 0.093750 | 0.058 sec/iter\n",
      "Epoch: 25 | Batch: 009 / 016 | Total loss: 7.206 | Reg loss: 0.023 | Tree loss: 7.206 | Accuracy: 0.076172 | 0.058 sec/iter\n",
      "Epoch: 25 | Batch: 010 / 016 | Total loss: 7.164 | Reg loss: 0.024 | Tree loss: 7.164 | Accuracy: 0.087891 | 0.058 sec/iter\n",
      "Epoch: 25 | Batch: 011 / 016 | Total loss: 7.139 | Reg loss: 0.024 | Tree loss: 7.139 | Accuracy: 0.103516 | 0.058 sec/iter\n",
      "Epoch: 25 | Batch: 012 / 016 | Total loss: 7.111 | Reg loss: 0.024 | Tree loss: 7.111 | Accuracy: 0.082031 | 0.058 sec/iter\n",
      "Epoch: 25 | Batch: 013 / 016 | Total loss: 7.121 | Reg loss: 0.024 | Tree loss: 7.121 | Accuracy: 0.087891 | 0.058 sec/iter\n",
      "Epoch: 25 | Batch: 014 / 016 | Total loss: 7.074 | Reg loss: 0.024 | Tree loss: 7.074 | Accuracy: 0.101562 | 0.058 sec/iter\n",
      "Epoch: 25 | Batch: 015 / 016 | Total loss: 7.146 | Reg loss: 0.024 | Tree loss: 7.146 | Accuracy: 0.079060 | 0.058 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 26 | Batch: 000 / 016 | Total loss: 7.235 | Reg loss: 0.023 | Tree loss: 7.235 | Accuracy: 0.087891 | 0.058 sec/iter\n",
      "Epoch: 26 | Batch: 001 / 016 | Total loss: 7.208 | Reg loss: 0.023 | Tree loss: 7.208 | Accuracy: 0.097656 | 0.058 sec/iter\n",
      "Epoch: 26 | Batch: 002 / 016 | Total loss: 7.203 | Reg loss: 0.023 | Tree loss: 7.203 | Accuracy: 0.107422 | 0.058 sec/iter\n",
      "Epoch: 26 | Batch: 003 / 016 | Total loss: 7.228 | Reg loss: 0.024 | Tree loss: 7.228 | Accuracy: 0.085938 | 0.058 sec/iter\n",
      "Epoch: 26 | Batch: 004 / 016 | Total loss: 7.166 | Reg loss: 0.024 | Tree loss: 7.166 | Accuracy: 0.095703 | 0.058 sec/iter\n",
      "Epoch: 26 | Batch: 005 / 016 | Total loss: 7.162 | Reg loss: 0.024 | Tree loss: 7.162 | Accuracy: 0.068359 | 0.058 sec/iter\n",
      "Epoch: 26 | Batch: 006 / 016 | Total loss: 7.157 | Reg loss: 0.024 | Tree loss: 7.157 | Accuracy: 0.072266 | 0.058 sec/iter\n",
      "Epoch: 26 | Batch: 007 / 016 | Total loss: 7.089 | Reg loss: 0.024 | Tree loss: 7.089 | Accuracy: 0.113281 | 0.058 sec/iter\n",
      "Epoch: 26 | Batch: 008 / 016 | Total loss: 7.091 | Reg loss: 0.024 | Tree loss: 7.091 | Accuracy: 0.082031 | 0.058 sec/iter\n",
      "Epoch: 26 | Batch: 009 / 016 | Total loss: 7.071 | Reg loss: 0.024 | Tree loss: 7.071 | Accuracy: 0.093750 | 0.058 sec/iter\n",
      "Epoch: 26 | Batch: 010 / 016 | Total loss: 7.035 | Reg loss: 0.024 | Tree loss: 7.035 | Accuracy: 0.101562 | 0.058 sec/iter\n",
      "Epoch: 26 | Batch: 011 / 016 | Total loss: 7.055 | Reg loss: 0.024 | Tree loss: 7.055 | Accuracy: 0.099609 | 0.058 sec/iter\n",
      "Epoch: 26 | Batch: 012 / 016 | Total loss: 7.045 | Reg loss: 0.024 | Tree loss: 7.045 | Accuracy: 0.082031 | 0.058 sec/iter\n",
      "Epoch: 26 | Batch: 013 / 016 | Total loss: 6.967 | Reg loss: 0.024 | Tree loss: 6.967 | Accuracy: 0.089844 | 0.058 sec/iter\n",
      "Epoch: 26 | Batch: 014 / 016 | Total loss: 7.010 | Reg loss: 0.024 | Tree loss: 7.010 | Accuracy: 0.076172 | 0.058 sec/iter\n",
      "Epoch: 26 | Batch: 015 / 016 | Total loss: 6.937 | Reg loss: 0.025 | Tree loss: 6.937 | Accuracy: 0.085470 | 0.058 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 27 | Batch: 000 / 016 | Total loss: 7.135 | Reg loss: 0.024 | Tree loss: 7.135 | Accuracy: 0.103516 | 0.058 sec/iter\n",
      "Epoch: 27 | Batch: 001 / 016 | Total loss: 7.112 | Reg loss: 0.024 | Tree loss: 7.112 | Accuracy: 0.097656 | 0.058 sec/iter\n",
      "Epoch: 27 | Batch: 002 / 016 | Total loss: 7.099 | Reg loss: 0.024 | Tree loss: 7.099 | Accuracy: 0.091797 | 0.058 sec/iter\n",
      "Epoch: 27 | Batch: 003 / 016 | Total loss: 7.035 | Reg loss: 0.024 | Tree loss: 7.035 | Accuracy: 0.128906 | 0.058 sec/iter\n",
      "Epoch: 27 | Batch: 004 / 016 | Total loss: 7.045 | Reg loss: 0.024 | Tree loss: 7.045 | Accuracy: 0.087891 | 0.058 sec/iter\n",
      "Epoch: 27 | Batch: 005 / 016 | Total loss: 7.040 | Reg loss: 0.024 | Tree loss: 7.040 | Accuracy: 0.082031 | 0.058 sec/iter\n",
      "Epoch: 27 | Batch: 006 / 016 | Total loss: 7.053 | Reg loss: 0.024 | Tree loss: 7.053 | Accuracy: 0.095703 | 0.058 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 27 | Batch: 007 / 016 | Total loss: 6.987 | Reg loss: 0.024 | Tree loss: 6.987 | Accuracy: 0.083984 | 0.058 sec/iter\n",
      "Epoch: 27 | Batch: 008 / 016 | Total loss: 6.974 | Reg loss: 0.024 | Tree loss: 6.974 | Accuracy: 0.095703 | 0.058 sec/iter\n",
      "Epoch: 27 | Batch: 009 / 016 | Total loss: 7.004 | Reg loss: 0.024 | Tree loss: 7.004 | Accuracy: 0.087891 | 0.058 sec/iter\n",
      "Epoch: 27 | Batch: 010 / 016 | Total loss: 6.950 | Reg loss: 0.024 | Tree loss: 6.950 | Accuracy: 0.085938 | 0.058 sec/iter\n",
      "Epoch: 27 | Batch: 011 / 016 | Total loss: 6.928 | Reg loss: 0.025 | Tree loss: 6.928 | Accuracy: 0.087891 | 0.058 sec/iter\n",
      "Epoch: 27 | Batch: 012 / 016 | Total loss: 6.951 | Reg loss: 0.025 | Tree loss: 6.951 | Accuracy: 0.080078 | 0.058 sec/iter\n",
      "Epoch: 27 | Batch: 013 / 016 | Total loss: 6.890 | Reg loss: 0.025 | Tree loss: 6.890 | Accuracy: 0.083984 | 0.058 sec/iter\n",
      "Epoch: 27 | Batch: 014 / 016 | Total loss: 6.849 | Reg loss: 0.025 | Tree loss: 6.849 | Accuracy: 0.080078 | 0.058 sec/iter\n",
      "Epoch: 27 | Batch: 015 / 016 | Total loss: 6.909 | Reg loss: 0.025 | Tree loss: 6.909 | Accuracy: 0.081197 | 0.058 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 28 | Batch: 000 / 016 | Total loss: 7.019 | Reg loss: 0.024 | Tree loss: 7.019 | Accuracy: 0.074219 | 0.058 sec/iter\n",
      "Epoch: 28 | Batch: 001 / 016 | Total loss: 7.045 | Reg loss: 0.024 | Tree loss: 7.045 | Accuracy: 0.074219 | 0.058 sec/iter\n",
      "Epoch: 28 | Batch: 002 / 016 | Total loss: 7.007 | Reg loss: 0.024 | Tree loss: 7.007 | Accuracy: 0.082031 | 0.058 sec/iter\n",
      "Epoch: 28 | Batch: 003 / 016 | Total loss: 6.979 | Reg loss: 0.024 | Tree loss: 6.979 | Accuracy: 0.099609 | 0.058 sec/iter\n",
      "Epoch: 28 | Batch: 004 / 016 | Total loss: 6.970 | Reg loss: 0.024 | Tree loss: 6.970 | Accuracy: 0.107422 | 0.058 sec/iter\n",
      "Epoch: 28 | Batch: 005 / 016 | Total loss: 6.921 | Reg loss: 0.024 | Tree loss: 6.921 | Accuracy: 0.103516 | 0.058 sec/iter\n",
      "Epoch: 28 | Batch: 006 / 016 | Total loss: 6.932 | Reg loss: 0.025 | Tree loss: 6.932 | Accuracy: 0.095703 | 0.058 sec/iter\n",
      "Epoch: 28 | Batch: 007 / 016 | Total loss: 6.935 | Reg loss: 0.025 | Tree loss: 6.935 | Accuracy: 0.074219 | 0.058 sec/iter\n",
      "Epoch: 28 | Batch: 008 / 016 | Total loss: 6.836 | Reg loss: 0.025 | Tree loss: 6.836 | Accuracy: 0.101562 | 0.058 sec/iter\n",
      "Epoch: 28 | Batch: 009 / 016 | Total loss: 6.849 | Reg loss: 0.025 | Tree loss: 6.849 | Accuracy: 0.091797 | 0.058 sec/iter\n",
      "Epoch: 28 | Batch: 010 / 016 | Total loss: 6.869 | Reg loss: 0.025 | Tree loss: 6.869 | Accuracy: 0.083984 | 0.058 sec/iter\n",
      "Epoch: 28 | Batch: 011 / 016 | Total loss: 6.826 | Reg loss: 0.025 | Tree loss: 6.826 | Accuracy: 0.087891 | 0.058 sec/iter\n",
      "Epoch: 28 | Batch: 012 / 016 | Total loss: 6.809 | Reg loss: 0.025 | Tree loss: 6.809 | Accuracy: 0.070312 | 0.058 sec/iter\n",
      "Epoch: 28 | Batch: 013 / 016 | Total loss: 6.764 | Reg loss: 0.025 | Tree loss: 6.764 | Accuracy: 0.068359 | 0.058 sec/iter\n",
      "Epoch: 28 | Batch: 014 / 016 | Total loss: 6.742 | Reg loss: 0.025 | Tree loss: 6.742 | Accuracy: 0.115234 | 0.058 sec/iter\n",
      "Epoch: 28 | Batch: 015 / 016 | Total loss: 6.783 | Reg loss: 0.025 | Tree loss: 6.783 | Accuracy: 0.081197 | 0.058 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 29 | Batch: 000 / 016 | Total loss: 6.916 | Reg loss: 0.025 | Tree loss: 6.916 | Accuracy: 0.089844 | 0.058 sec/iter\n",
      "Epoch: 29 | Batch: 001 / 016 | Total loss: 6.921 | Reg loss: 0.025 | Tree loss: 6.921 | Accuracy: 0.074219 | 0.058 sec/iter\n",
      "Epoch: 29 | Batch: 002 / 016 | Total loss: 6.927 | Reg loss: 0.025 | Tree loss: 6.927 | Accuracy: 0.083984 | 0.058 sec/iter\n",
      "Epoch: 29 | Batch: 003 / 016 | Total loss: 6.856 | Reg loss: 0.025 | Tree loss: 6.856 | Accuracy: 0.105469 | 0.058 sec/iter\n",
      "Epoch: 29 | Batch: 004 / 016 | Total loss: 6.857 | Reg loss: 0.025 | Tree loss: 6.857 | Accuracy: 0.087891 | 0.058 sec/iter\n",
      "Epoch: 29 | Batch: 005 / 016 | Total loss: 6.835 | Reg loss: 0.025 | Tree loss: 6.835 | Accuracy: 0.109375 | 0.058 sec/iter\n",
      "Epoch: 29 | Batch: 006 / 016 | Total loss: 6.804 | Reg loss: 0.025 | Tree loss: 6.804 | Accuracy: 0.095703 | 0.058 sec/iter\n",
      "Epoch: 29 | Batch: 007 / 016 | Total loss: 6.773 | Reg loss: 0.025 | Tree loss: 6.773 | Accuracy: 0.107422 | 0.058 sec/iter\n",
      "Epoch: 29 | Batch: 008 / 016 | Total loss: 6.771 | Reg loss: 0.025 | Tree loss: 6.771 | Accuracy: 0.101562 | 0.058 sec/iter\n",
      "Epoch: 29 | Batch: 009 / 016 | Total loss: 6.795 | Reg loss: 0.025 | Tree loss: 6.795 | Accuracy: 0.083984 | 0.058 sec/iter\n",
      "Epoch: 29 | Batch: 010 / 016 | Total loss: 6.738 | Reg loss: 0.025 | Tree loss: 6.738 | Accuracy: 0.085938 | 0.058 sec/iter\n",
      "Epoch: 29 | Batch: 011 / 016 | Total loss: 6.743 | Reg loss: 0.025 | Tree loss: 6.743 | Accuracy: 0.085938 | 0.058 sec/iter\n",
      "Epoch: 29 | Batch: 012 / 016 | Total loss: 6.697 | Reg loss: 0.025 | Tree loss: 6.697 | Accuracy: 0.093750 | 0.058 sec/iter\n",
      "Epoch: 29 | Batch: 013 / 016 | Total loss: 6.679 | Reg loss: 0.025 | Tree loss: 6.679 | Accuracy: 0.072266 | 0.058 sec/iter\n",
      "Epoch: 29 | Batch: 014 / 016 | Total loss: 6.685 | Reg loss: 0.026 | Tree loss: 6.685 | Accuracy: 0.080078 | 0.058 sec/iter\n",
      "Epoch: 29 | Batch: 015 / 016 | Total loss: 6.664 | Reg loss: 0.026 | Tree loss: 6.664 | Accuracy: 0.074786 | 0.058 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 30 | Batch: 000 / 016 | Total loss: 6.816 | Reg loss: 0.025 | Tree loss: 6.816 | Accuracy: 0.074219 | 0.058 sec/iter\n",
      "Epoch: 30 | Batch: 001 / 016 | Total loss: 6.810 | Reg loss: 0.025 | Tree loss: 6.810 | Accuracy: 0.091797 | 0.058 sec/iter\n",
      "Epoch: 30 | Batch: 002 / 016 | Total loss: 6.788 | Reg loss: 0.025 | Tree loss: 6.788 | Accuracy: 0.095703 | 0.058 sec/iter\n",
      "Epoch: 30 | Batch: 003 / 016 | Total loss: 6.788 | Reg loss: 0.025 | Tree loss: 6.788 | Accuracy: 0.080078 | 0.058 sec/iter\n",
      "Epoch: 30 | Batch: 004 / 016 | Total loss: 6.705 | Reg loss: 0.025 | Tree loss: 6.705 | Accuracy: 0.093750 | 0.058 sec/iter\n",
      "Epoch: 30 | Batch: 005 / 016 | Total loss: 6.721 | Reg loss: 0.025 | Tree loss: 6.721 | Accuracy: 0.111328 | 0.058 sec/iter\n",
      "Epoch: 30 | Batch: 006 / 016 | Total loss: 6.733 | Reg loss: 0.025 | Tree loss: 6.733 | Accuracy: 0.099609 | 0.058 sec/iter\n",
      "Epoch: 30 | Batch: 007 / 016 | Total loss: 6.715 | Reg loss: 0.025 | Tree loss: 6.715 | Accuracy: 0.083984 | 0.058 sec/iter\n",
      "Epoch: 30 | Batch: 008 / 016 | Total loss: 6.657 | Reg loss: 0.025 | Tree loss: 6.657 | Accuracy: 0.080078 | 0.058 sec/iter\n",
      "Epoch: 30 | Batch: 009 / 016 | Total loss: 6.667 | Reg loss: 0.025 | Tree loss: 6.667 | Accuracy: 0.089844 | 0.058 sec/iter\n",
      "Epoch: 30 | Batch: 010 / 016 | Total loss: 6.660 | Reg loss: 0.026 | Tree loss: 6.660 | Accuracy: 0.076172 | 0.058 sec/iter\n",
      "Epoch: 30 | Batch: 011 / 016 | Total loss: 6.632 | Reg loss: 0.026 | Tree loss: 6.632 | Accuracy: 0.074219 | 0.058 sec/iter\n",
      "Epoch: 30 | Batch: 012 / 016 | Total loss: 6.611 | Reg loss: 0.026 | Tree loss: 6.611 | Accuracy: 0.095703 | 0.058 sec/iter\n",
      "Epoch: 30 | Batch: 013 / 016 | Total loss: 6.581 | Reg loss: 0.026 | Tree loss: 6.581 | Accuracy: 0.080078 | 0.058 sec/iter\n",
      "Epoch: 30 | Batch: 014 / 016 | Total loss: 6.598 | Reg loss: 0.026 | Tree loss: 6.598 | Accuracy: 0.080078 | 0.058 sec/iter\n",
      "Epoch: 30 | Batch: 015 / 016 | Total loss: 6.602 | Reg loss: 0.026 | Tree loss: 6.602 | Accuracy: 0.076923 | 0.058 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 31 | Batch: 000 / 016 | Total loss: 6.747 | Reg loss: 0.025 | Tree loss: 6.747 | Accuracy: 0.107422 | 0.058 sec/iter\n",
      "Epoch: 31 | Batch: 001 / 016 | Total loss: 6.696 | Reg loss: 0.025 | Tree loss: 6.696 | Accuracy: 0.119141 | 0.058 sec/iter\n",
      "Epoch: 31 | Batch: 002 / 016 | Total loss: 6.698 | Reg loss: 0.025 | Tree loss: 6.698 | Accuracy: 0.091797 | 0.058 sec/iter\n",
      "Epoch: 31 | Batch: 003 / 016 | Total loss: 6.661 | Reg loss: 0.025 | Tree loss: 6.661 | Accuracy: 0.087891 | 0.058 sec/iter\n",
      "Epoch: 31 | Batch: 004 / 016 | Total loss: 6.644 | Reg loss: 0.026 | Tree loss: 6.644 | Accuracy: 0.087891 | 0.058 sec/iter\n",
      "Epoch: 31 | Batch: 005 / 016 | Total loss: 6.659 | Reg loss: 0.026 | Tree loss: 6.659 | Accuracy: 0.097656 | 0.058 sec/iter\n",
      "Epoch: 31 | Batch: 006 / 016 | Total loss: 6.619 | Reg loss: 0.026 | Tree loss: 6.619 | Accuracy: 0.103516 | 0.058 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 31 | Batch: 007 / 016 | Total loss: 6.618 | Reg loss: 0.026 | Tree loss: 6.618 | Accuracy: 0.072266 | 0.058 sec/iter\n",
      "Epoch: 31 | Batch: 008 / 016 | Total loss: 6.581 | Reg loss: 0.026 | Tree loss: 6.581 | Accuracy: 0.087891 | 0.058 sec/iter\n",
      "Epoch: 31 | Batch: 009 / 016 | Total loss: 6.549 | Reg loss: 0.026 | Tree loss: 6.549 | Accuracy: 0.089844 | 0.058 sec/iter\n",
      "Epoch: 31 | Batch: 010 / 016 | Total loss: 6.560 | Reg loss: 0.026 | Tree loss: 6.560 | Accuracy: 0.091797 | 0.058 sec/iter\n",
      "Epoch: 31 | Batch: 011 / 016 | Total loss: 6.529 | Reg loss: 0.026 | Tree loss: 6.529 | Accuracy: 0.072266 | 0.058 sec/iter\n",
      "Epoch: 31 | Batch: 012 / 016 | Total loss: 6.520 | Reg loss: 0.026 | Tree loss: 6.520 | Accuracy: 0.068359 | 0.058 sec/iter\n",
      "Epoch: 31 | Batch: 013 / 016 | Total loss: 6.469 | Reg loss: 0.026 | Tree loss: 6.469 | Accuracy: 0.085938 | 0.058 sec/iter\n",
      "Epoch: 31 | Batch: 014 / 016 | Total loss: 6.531 | Reg loss: 0.026 | Tree loss: 6.531 | Accuracy: 0.068359 | 0.058 sec/iter\n",
      "Epoch: 31 | Batch: 015 / 016 | Total loss: 6.453 | Reg loss: 0.026 | Tree loss: 6.453 | Accuracy: 0.089744 | 0.058 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 32 | Batch: 000 / 016 | Total loss: 6.623 | Reg loss: 0.026 | Tree loss: 6.623 | Accuracy: 0.089844 | 0.058 sec/iter\n",
      "Epoch: 32 | Batch: 001 / 016 | Total loss: 6.619 | Reg loss: 0.026 | Tree loss: 6.619 | Accuracy: 0.103516 | 0.058 sec/iter\n",
      "Epoch: 32 | Batch: 002 / 016 | Total loss: 6.632 | Reg loss: 0.026 | Tree loss: 6.632 | Accuracy: 0.080078 | 0.058 sec/iter\n",
      "Epoch: 32 | Batch: 003 / 016 | Total loss: 6.559 | Reg loss: 0.026 | Tree loss: 6.559 | Accuracy: 0.093750 | 0.058 sec/iter\n",
      "Epoch: 32 | Batch: 004 / 016 | Total loss: 6.605 | Reg loss: 0.026 | Tree loss: 6.605 | Accuracy: 0.076172 | 0.058 sec/iter\n",
      "Epoch: 32 | Batch: 005 / 016 | Total loss: 6.507 | Reg loss: 0.026 | Tree loss: 6.507 | Accuracy: 0.091797 | 0.058 sec/iter\n",
      "Epoch: 32 | Batch: 006 / 016 | Total loss: 6.539 | Reg loss: 0.026 | Tree loss: 6.539 | Accuracy: 0.087891 | 0.058 sec/iter\n",
      "Epoch: 32 | Batch: 007 / 016 | Total loss: 6.486 | Reg loss: 0.026 | Tree loss: 6.486 | Accuracy: 0.105469 | 0.058 sec/iter\n",
      "Epoch: 32 | Batch: 008 / 016 | Total loss: 6.480 | Reg loss: 0.026 | Tree loss: 6.480 | Accuracy: 0.089844 | 0.058 sec/iter\n",
      "Epoch: 32 | Batch: 009 / 016 | Total loss: 6.443 | Reg loss: 0.026 | Tree loss: 6.443 | Accuracy: 0.099609 | 0.058 sec/iter\n",
      "Epoch: 32 | Batch: 010 / 016 | Total loss: 6.428 | Reg loss: 0.026 | Tree loss: 6.428 | Accuracy: 0.101562 | 0.058 sec/iter\n",
      "Epoch: 32 | Batch: 011 / 016 | Total loss: 6.458 | Reg loss: 0.026 | Tree loss: 6.458 | Accuracy: 0.076172 | 0.058 sec/iter\n",
      "Epoch: 32 | Batch: 012 / 016 | Total loss: 6.465 | Reg loss: 0.026 | Tree loss: 6.465 | Accuracy: 0.095703 | 0.058 sec/iter\n",
      "Epoch: 32 | Batch: 013 / 016 | Total loss: 6.392 | Reg loss: 0.026 | Tree loss: 6.392 | Accuracy: 0.089844 | 0.058 sec/iter\n",
      "Epoch: 32 | Batch: 014 / 016 | Total loss: 6.424 | Reg loss: 0.026 | Tree loss: 6.424 | Accuracy: 0.089844 | 0.058 sec/iter\n",
      "Epoch: 32 | Batch: 015 / 016 | Total loss: 6.365 | Reg loss: 0.027 | Tree loss: 6.365 | Accuracy: 0.085470 | 0.058 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 33 | Batch: 000 / 016 | Total loss: 6.562 | Reg loss: 0.026 | Tree loss: 6.562 | Accuracy: 0.076172 | 0.058 sec/iter\n",
      "Epoch: 33 | Batch: 001 / 016 | Total loss: 6.499 | Reg loss: 0.026 | Tree loss: 6.499 | Accuracy: 0.097656 | 0.058 sec/iter\n",
      "Epoch: 33 | Batch: 002 / 016 | Total loss: 6.482 | Reg loss: 0.026 | Tree loss: 6.482 | Accuracy: 0.107422 | 0.058 sec/iter\n",
      "Epoch: 33 | Batch: 003 / 016 | Total loss: 6.500 | Reg loss: 0.026 | Tree loss: 6.500 | Accuracy: 0.089844 | 0.058 sec/iter\n",
      "Epoch: 33 | Batch: 004 / 016 | Total loss: 6.492 | Reg loss: 0.026 | Tree loss: 6.492 | Accuracy: 0.083984 | 0.058 sec/iter\n",
      "Epoch: 33 | Batch: 005 / 016 | Total loss: 6.464 | Reg loss: 0.026 | Tree loss: 6.464 | Accuracy: 0.080078 | 0.058 sec/iter\n",
      "Epoch: 33 | Batch: 006 / 016 | Total loss: 6.428 | Reg loss: 0.026 | Tree loss: 6.428 | Accuracy: 0.085938 | 0.058 sec/iter\n",
      "Epoch: 33 | Batch: 007 / 016 | Total loss: 6.395 | Reg loss: 0.026 | Tree loss: 6.395 | Accuracy: 0.085938 | 0.058 sec/iter\n",
      "Epoch: 33 | Batch: 008 / 016 | Total loss: 6.400 | Reg loss: 0.026 | Tree loss: 6.400 | Accuracy: 0.087891 | 0.058 sec/iter\n",
      "Epoch: 33 | Batch: 009 / 016 | Total loss: 6.394 | Reg loss: 0.026 | Tree loss: 6.394 | Accuracy: 0.095703 | 0.058 sec/iter\n",
      "Epoch: 33 | Batch: 010 / 016 | Total loss: 6.354 | Reg loss: 0.027 | Tree loss: 6.354 | Accuracy: 0.083984 | 0.058 sec/iter\n",
      "Epoch: 33 | Batch: 011 / 016 | Total loss: 6.340 | Reg loss: 0.027 | Tree loss: 6.340 | Accuracy: 0.082031 | 0.058 sec/iter\n",
      "Epoch: 33 | Batch: 012 / 016 | Total loss: 6.378 | Reg loss: 0.027 | Tree loss: 6.378 | Accuracy: 0.083984 | 0.058 sec/iter\n",
      "Epoch: 33 | Batch: 013 / 016 | Total loss: 6.326 | Reg loss: 0.027 | Tree loss: 6.326 | Accuracy: 0.089844 | 0.058 sec/iter\n",
      "Epoch: 33 | Batch: 014 / 016 | Total loss: 6.287 | Reg loss: 0.027 | Tree loss: 6.287 | Accuracy: 0.089844 | 0.058 sec/iter\n",
      "Epoch: 33 | Batch: 015 / 016 | Total loss: 6.258 | Reg loss: 0.027 | Tree loss: 6.258 | Accuracy: 0.094017 | 0.058 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 34 | Batch: 000 / 016 | Total loss: 6.445 | Reg loss: 0.026 | Tree loss: 6.445 | Accuracy: 0.095703 | 0.058 sec/iter\n",
      "Epoch: 34 | Batch: 001 / 016 | Total loss: 6.403 | Reg loss: 0.026 | Tree loss: 6.403 | Accuracy: 0.103516 | 0.058 sec/iter\n",
      "Epoch: 34 | Batch: 002 / 016 | Total loss: 6.428 | Reg loss: 0.026 | Tree loss: 6.428 | Accuracy: 0.083984 | 0.058 sec/iter\n",
      "Epoch: 34 | Batch: 003 / 016 | Total loss: 6.348 | Reg loss: 0.026 | Tree loss: 6.348 | Accuracy: 0.113281 | 0.058 sec/iter\n",
      "Epoch: 34 | Batch: 004 / 016 | Total loss: 6.330 | Reg loss: 0.026 | Tree loss: 6.330 | Accuracy: 0.105469 | 0.058 sec/iter\n",
      "Epoch: 34 | Batch: 005 / 016 | Total loss: 6.339 | Reg loss: 0.027 | Tree loss: 6.339 | Accuracy: 0.078125 | 0.058 sec/iter\n",
      "Epoch: 34 | Batch: 006 / 016 | Total loss: 6.364 | Reg loss: 0.027 | Tree loss: 6.364 | Accuracy: 0.083984 | 0.058 sec/iter\n",
      "Epoch: 34 | Batch: 007 / 016 | Total loss: 6.319 | Reg loss: 0.027 | Tree loss: 6.319 | Accuracy: 0.101562 | 0.058 sec/iter\n",
      "Epoch: 34 | Batch: 008 / 016 | Total loss: 6.385 | Reg loss: 0.027 | Tree loss: 6.385 | Accuracy: 0.070312 | 0.058 sec/iter\n",
      "Epoch: 34 | Batch: 009 / 016 | Total loss: 6.284 | Reg loss: 0.027 | Tree loss: 6.284 | Accuracy: 0.080078 | 0.058 sec/iter\n",
      "Epoch: 34 | Batch: 010 / 016 | Total loss: 6.272 | Reg loss: 0.027 | Tree loss: 6.272 | Accuracy: 0.082031 | 0.058 sec/iter\n",
      "Epoch: 34 | Batch: 011 / 016 | Total loss: 6.282 | Reg loss: 0.027 | Tree loss: 6.282 | Accuracy: 0.097656 | 0.058 sec/iter\n",
      "Epoch: 34 | Batch: 012 / 016 | Total loss: 6.212 | Reg loss: 0.027 | Tree loss: 6.212 | Accuracy: 0.080078 | 0.058 sec/iter\n",
      "Epoch: 34 | Batch: 013 / 016 | Total loss: 6.259 | Reg loss: 0.027 | Tree loss: 6.259 | Accuracy: 0.085938 | 0.058 sec/iter\n",
      "Epoch: 34 | Batch: 014 / 016 | Total loss: 6.276 | Reg loss: 0.027 | Tree loss: 6.276 | Accuracy: 0.070312 | 0.058 sec/iter\n",
      "Epoch: 34 | Batch: 015 / 016 | Total loss: 6.185 | Reg loss: 0.027 | Tree loss: 6.185 | Accuracy: 0.079060 | 0.058 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 35 | Batch: 000 / 016 | Total loss: 6.342 | Reg loss: 0.027 | Tree loss: 6.342 | Accuracy: 0.101562 | 0.058 sec/iter\n",
      "Epoch: 35 | Batch: 001 / 016 | Total loss: 6.321 | Reg loss: 0.027 | Tree loss: 6.321 | Accuracy: 0.093750 | 0.058 sec/iter\n",
      "Epoch: 35 | Batch: 002 / 016 | Total loss: 6.343 | Reg loss: 0.027 | Tree loss: 6.343 | Accuracy: 0.089844 | 0.058 sec/iter\n",
      "Epoch: 35 | Batch: 003 / 016 | Total loss: 6.321 | Reg loss: 0.027 | Tree loss: 6.321 | Accuracy: 0.074219 | 0.058 sec/iter\n",
      "Epoch: 35 | Batch: 004 / 016 | Total loss: 6.312 | Reg loss: 0.027 | Tree loss: 6.312 | Accuracy: 0.109375 | 0.058 sec/iter\n",
      "Epoch: 35 | Batch: 005 / 016 | Total loss: 6.273 | Reg loss: 0.027 | Tree loss: 6.273 | Accuracy: 0.078125 | 0.058 sec/iter\n",
      "Epoch: 35 | Batch: 006 / 016 | Total loss: 6.214 | Reg loss: 0.027 | Tree loss: 6.214 | Accuracy: 0.121094 | 0.058 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 35 | Batch: 007 / 016 | Total loss: 6.243 | Reg loss: 0.027 | Tree loss: 6.243 | Accuracy: 0.080078 | 0.058 sec/iter\n",
      "Epoch: 35 | Batch: 008 / 016 | Total loss: 6.220 | Reg loss: 0.027 | Tree loss: 6.220 | Accuracy: 0.087891 | 0.058 sec/iter\n",
      "Epoch: 35 | Batch: 009 / 016 | Total loss: 6.183 | Reg loss: 0.027 | Tree loss: 6.183 | Accuracy: 0.105469 | 0.058 sec/iter\n",
      "Epoch: 35 | Batch: 010 / 016 | Total loss: 6.176 | Reg loss: 0.027 | Tree loss: 6.176 | Accuracy: 0.095703 | 0.058 sec/iter\n",
      "Epoch: 35 | Batch: 011 / 016 | Total loss: 6.216 | Reg loss: 0.027 | Tree loss: 6.216 | Accuracy: 0.058594 | 0.058 sec/iter\n",
      "Epoch: 35 | Batch: 012 / 016 | Total loss: 6.185 | Reg loss: 0.027 | Tree loss: 6.185 | Accuracy: 0.080078 | 0.058 sec/iter\n",
      "Epoch: 35 | Batch: 013 / 016 | Total loss: 6.157 | Reg loss: 0.027 | Tree loss: 6.157 | Accuracy: 0.083984 | 0.058 sec/iter\n",
      "Epoch: 35 | Batch: 014 / 016 | Total loss: 6.136 | Reg loss: 0.027 | Tree loss: 6.136 | Accuracy: 0.076172 | 0.058 sec/iter\n",
      "Epoch: 35 | Batch: 015 / 016 | Total loss: 6.099 | Reg loss: 0.027 | Tree loss: 6.099 | Accuracy: 0.096154 | 0.058 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 36 | Batch: 000 / 016 | Total loss: 6.295 | Reg loss: 0.027 | Tree loss: 6.295 | Accuracy: 0.087891 | 0.058 sec/iter\n",
      "Epoch: 36 | Batch: 001 / 016 | Total loss: 6.208 | Reg loss: 0.027 | Tree loss: 6.208 | Accuracy: 0.099609 | 0.058 sec/iter\n",
      "Epoch: 36 | Batch: 002 / 016 | Total loss: 6.268 | Reg loss: 0.027 | Tree loss: 6.268 | Accuracy: 0.085938 | 0.058 sec/iter\n",
      "Epoch: 36 | Batch: 003 / 016 | Total loss: 6.207 | Reg loss: 0.027 | Tree loss: 6.207 | Accuracy: 0.111328 | 0.058 sec/iter\n",
      "Epoch: 36 | Batch: 004 / 016 | Total loss: 6.172 | Reg loss: 0.027 | Tree loss: 6.172 | Accuracy: 0.089844 | 0.058 sec/iter\n",
      "Epoch: 36 | Batch: 005 / 016 | Total loss: 6.222 | Reg loss: 0.027 | Tree loss: 6.222 | Accuracy: 0.083984 | 0.058 sec/iter\n",
      "Epoch: 36 | Batch: 006 / 016 | Total loss: 6.186 | Reg loss: 0.027 | Tree loss: 6.186 | Accuracy: 0.093750 | 0.058 sec/iter\n",
      "Epoch: 36 | Batch: 007 / 016 | Total loss: 6.164 | Reg loss: 0.027 | Tree loss: 6.164 | Accuracy: 0.095703 | 0.058 sec/iter\n",
      "Epoch: 36 | Batch: 008 / 016 | Total loss: 6.145 | Reg loss: 0.027 | Tree loss: 6.145 | Accuracy: 0.076172 | 0.058 sec/iter\n",
      "Epoch: 36 | Batch: 009 / 016 | Total loss: 6.120 | Reg loss: 0.027 | Tree loss: 6.120 | Accuracy: 0.091797 | 0.058 sec/iter\n",
      "Epoch: 36 | Batch: 010 / 016 | Total loss: 6.119 | Reg loss: 0.027 | Tree loss: 6.119 | Accuracy: 0.083984 | 0.058 sec/iter\n",
      "Epoch: 36 | Batch: 011 / 016 | Total loss: 6.083 | Reg loss: 0.027 | Tree loss: 6.083 | Accuracy: 0.099609 | 0.058 sec/iter\n",
      "Epoch: 36 | Batch: 012 / 016 | Total loss: 6.082 | Reg loss: 0.027 | Tree loss: 6.082 | Accuracy: 0.089844 | 0.058 sec/iter\n",
      "Epoch: 36 | Batch: 013 / 016 | Total loss: 6.033 | Reg loss: 0.028 | Tree loss: 6.033 | Accuracy: 0.103516 | 0.058 sec/iter\n",
      "Epoch: 36 | Batch: 014 / 016 | Total loss: 6.031 | Reg loss: 0.028 | Tree loss: 6.031 | Accuracy: 0.085938 | 0.058 sec/iter\n",
      "Epoch: 36 | Batch: 015 / 016 | Total loss: 6.063 | Reg loss: 0.028 | Tree loss: 6.063 | Accuracy: 0.068376 | 0.058 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 37 | Batch: 000 / 016 | Total loss: 6.213 | Reg loss: 0.027 | Tree loss: 6.213 | Accuracy: 0.101562 | 0.058 sec/iter\n",
      "Epoch: 37 | Batch: 001 / 016 | Total loss: 6.129 | Reg loss: 0.027 | Tree loss: 6.129 | Accuracy: 0.093750 | 0.058 sec/iter\n",
      "Epoch: 37 | Batch: 002 / 016 | Total loss: 6.140 | Reg loss: 0.027 | Tree loss: 6.140 | Accuracy: 0.089844 | 0.058 sec/iter\n",
      "Epoch: 37 | Batch: 003 / 016 | Total loss: 6.102 | Reg loss: 0.027 | Tree loss: 6.102 | Accuracy: 0.097656 | 0.058 sec/iter\n",
      "Epoch: 37 | Batch: 004 / 016 | Total loss: 6.148 | Reg loss: 0.027 | Tree loss: 6.148 | Accuracy: 0.074219 | 0.058 sec/iter\n",
      "Epoch: 37 | Batch: 005 / 016 | Total loss: 6.134 | Reg loss: 0.027 | Tree loss: 6.134 | Accuracy: 0.076172 | 0.058 sec/iter\n",
      "Epoch: 37 | Batch: 006 / 016 | Total loss: 6.105 | Reg loss: 0.027 | Tree loss: 6.105 | Accuracy: 0.099609 | 0.058 sec/iter\n",
      "Epoch: 37 | Batch: 007 / 016 | Total loss: 6.050 | Reg loss: 0.027 | Tree loss: 6.050 | Accuracy: 0.105469 | 0.058 sec/iter\n",
      "Epoch: 37 | Batch: 008 / 016 | Total loss: 6.092 | Reg loss: 0.028 | Tree loss: 6.092 | Accuracy: 0.089844 | 0.058 sec/iter\n",
      "Epoch: 37 | Batch: 009 / 016 | Total loss: 6.100 | Reg loss: 0.028 | Tree loss: 6.100 | Accuracy: 0.082031 | 0.058 sec/iter\n",
      "Epoch: 37 | Batch: 010 / 016 | Total loss: 6.008 | Reg loss: 0.028 | Tree loss: 6.008 | Accuracy: 0.093750 | 0.058 sec/iter\n",
      "Epoch: 37 | Batch: 011 / 016 | Total loss: 6.024 | Reg loss: 0.028 | Tree loss: 6.024 | Accuracy: 0.080078 | 0.058 sec/iter\n",
      "Epoch: 37 | Batch: 012 / 016 | Total loss: 5.991 | Reg loss: 0.028 | Tree loss: 5.991 | Accuracy: 0.089844 | 0.058 sec/iter\n",
      "Epoch: 37 | Batch: 013 / 016 | Total loss: 5.958 | Reg loss: 0.028 | Tree loss: 5.958 | Accuracy: 0.097656 | 0.058 sec/iter\n",
      "Epoch: 37 | Batch: 014 / 016 | Total loss: 5.934 | Reg loss: 0.028 | Tree loss: 5.934 | Accuracy: 0.082031 | 0.058 sec/iter\n",
      "Epoch: 37 | Batch: 015 / 016 | Total loss: 5.950 | Reg loss: 0.028 | Tree loss: 5.950 | Accuracy: 0.100427 | 0.058 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 38 | Batch: 000 / 016 | Total loss: 6.142 | Reg loss: 0.028 | Tree loss: 6.142 | Accuracy: 0.089844 | 0.058 sec/iter\n",
      "Epoch: 38 | Batch: 001 / 016 | Total loss: 6.131 | Reg loss: 0.028 | Tree loss: 6.131 | Accuracy: 0.082031 | 0.058 sec/iter\n",
      "Epoch: 38 | Batch: 002 / 016 | Total loss: 6.065 | Reg loss: 0.028 | Tree loss: 6.065 | Accuracy: 0.105469 | 0.058 sec/iter\n",
      "Epoch: 38 | Batch: 003 / 016 | Total loss: 6.094 | Reg loss: 0.028 | Tree loss: 6.094 | Accuracy: 0.078125 | 0.058 sec/iter\n",
      "Epoch: 38 | Batch: 004 / 016 | Total loss: 6.028 | Reg loss: 0.028 | Tree loss: 6.028 | Accuracy: 0.097656 | 0.058 sec/iter\n",
      "Epoch: 38 | Batch: 005 / 016 | Total loss: 6.017 | Reg loss: 0.028 | Tree loss: 6.017 | Accuracy: 0.097656 | 0.058 sec/iter\n",
      "Epoch: 38 | Batch: 006 / 016 | Total loss: 6.017 | Reg loss: 0.028 | Tree loss: 6.017 | Accuracy: 0.078125 | 0.058 sec/iter\n",
      "Epoch: 38 | Batch: 007 / 016 | Total loss: 6.012 | Reg loss: 0.028 | Tree loss: 6.012 | Accuracy: 0.068359 | 0.058 sec/iter\n",
      "Epoch: 38 | Batch: 008 / 016 | Total loss: 5.956 | Reg loss: 0.028 | Tree loss: 5.956 | Accuracy: 0.082031 | 0.058 sec/iter\n",
      "Epoch: 38 | Batch: 009 / 016 | Total loss: 5.954 | Reg loss: 0.028 | Tree loss: 5.954 | Accuracy: 0.085938 | 0.058 sec/iter\n",
      "Epoch: 38 | Batch: 010 / 016 | Total loss: 5.893 | Reg loss: 0.028 | Tree loss: 5.893 | Accuracy: 0.097656 | 0.058 sec/iter\n",
      "Epoch: 38 | Batch: 011 / 016 | Total loss: 5.909 | Reg loss: 0.028 | Tree loss: 5.909 | Accuracy: 0.109375 | 0.058 sec/iter\n",
      "Epoch: 38 | Batch: 012 / 016 | Total loss: 5.895 | Reg loss: 0.028 | Tree loss: 5.895 | Accuracy: 0.103516 | 0.058 sec/iter\n",
      "Epoch: 38 | Batch: 013 / 016 | Total loss: 5.954 | Reg loss: 0.028 | Tree loss: 5.954 | Accuracy: 0.082031 | 0.058 sec/iter\n",
      "Epoch: 38 | Batch: 014 / 016 | Total loss: 5.900 | Reg loss: 0.028 | Tree loss: 5.900 | Accuracy: 0.103516 | 0.058 sec/iter\n",
      "Epoch: 38 | Batch: 015 / 016 | Total loss: 5.830 | Reg loss: 0.028 | Tree loss: 5.830 | Accuracy: 0.100427 | 0.058 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 39 | Batch: 000 / 016 | Total loss: 6.056 | Reg loss: 0.028 | Tree loss: 6.056 | Accuracy: 0.101562 | 0.058 sec/iter\n",
      "Epoch: 39 | Batch: 001 / 016 | Total loss: 6.015 | Reg loss: 0.028 | Tree loss: 6.015 | Accuracy: 0.078125 | 0.058 sec/iter\n",
      "Epoch: 39 | Batch: 002 / 016 | Total loss: 5.971 | Reg loss: 0.028 | Tree loss: 5.971 | Accuracy: 0.109375 | 0.058 sec/iter\n",
      "Epoch: 39 | Batch: 003 / 016 | Total loss: 5.936 | Reg loss: 0.028 | Tree loss: 5.936 | Accuracy: 0.070312 | 0.058 sec/iter\n",
      "Epoch: 39 | Batch: 004 / 016 | Total loss: 6.014 | Reg loss: 0.028 | Tree loss: 6.014 | Accuracy: 0.095703 | 0.058 sec/iter\n",
      "Epoch: 39 | Batch: 005 / 016 | Total loss: 5.965 | Reg loss: 0.028 | Tree loss: 5.965 | Accuracy: 0.082031 | 0.058 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 39 | Batch: 006 / 016 | Total loss: 5.928 | Reg loss: 0.028 | Tree loss: 5.928 | Accuracy: 0.091797 | 0.058 sec/iter\n",
      "Epoch: 39 | Batch: 007 / 016 | Total loss: 5.908 | Reg loss: 0.028 | Tree loss: 5.908 | Accuracy: 0.105469 | 0.058 sec/iter\n",
      "Epoch: 39 | Batch: 008 / 016 | Total loss: 5.917 | Reg loss: 0.028 | Tree loss: 5.917 | Accuracy: 0.095703 | 0.058 sec/iter\n",
      "Epoch: 39 | Batch: 009 / 016 | Total loss: 5.871 | Reg loss: 0.028 | Tree loss: 5.871 | Accuracy: 0.107422 | 0.058 sec/iter\n",
      "Epoch: 39 | Batch: 010 / 016 | Total loss: 5.863 | Reg loss: 0.028 | Tree loss: 5.863 | Accuracy: 0.072266 | 0.058 sec/iter\n",
      "Epoch: 39 | Batch: 011 / 016 | Total loss: 5.888 | Reg loss: 0.028 | Tree loss: 5.888 | Accuracy: 0.085938 | 0.058 sec/iter\n",
      "Epoch: 39 | Batch: 012 / 016 | Total loss: 5.814 | Reg loss: 0.028 | Tree loss: 5.814 | Accuracy: 0.095703 | 0.058 sec/iter\n",
      "Epoch: 39 | Batch: 013 / 016 | Total loss: 5.834 | Reg loss: 0.028 | Tree loss: 5.834 | Accuracy: 0.078125 | 0.058 sec/iter\n",
      "Epoch: 39 | Batch: 014 / 016 | Total loss: 5.825 | Reg loss: 0.028 | Tree loss: 5.825 | Accuracy: 0.097656 | 0.058 sec/iter\n",
      "Epoch: 39 | Batch: 015 / 016 | Total loss: 5.763 | Reg loss: 0.028 | Tree loss: 5.763 | Accuracy: 0.094017 | 0.058 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 40 | Batch: 000 / 016 | Total loss: 5.942 | Reg loss: 0.028 | Tree loss: 5.942 | Accuracy: 0.082031 | 0.058 sec/iter\n",
      "Epoch: 40 | Batch: 001 / 016 | Total loss: 5.912 | Reg loss: 0.028 | Tree loss: 5.912 | Accuracy: 0.111328 | 0.058 sec/iter\n",
      "Epoch: 40 | Batch: 002 / 016 | Total loss: 5.903 | Reg loss: 0.028 | Tree loss: 5.903 | Accuracy: 0.101562 | 0.058 sec/iter\n",
      "Epoch: 40 | Batch: 003 / 016 | Total loss: 5.906 | Reg loss: 0.028 | Tree loss: 5.906 | Accuracy: 0.101562 | 0.058 sec/iter\n",
      "Epoch: 40 | Batch: 004 / 016 | Total loss: 5.874 | Reg loss: 0.028 | Tree loss: 5.874 | Accuracy: 0.085938 | 0.058 sec/iter\n",
      "Epoch: 40 | Batch: 005 / 016 | Total loss: 5.876 | Reg loss: 0.028 | Tree loss: 5.876 | Accuracy: 0.082031 | 0.058 sec/iter\n",
      "Epoch: 40 | Batch: 006 / 016 | Total loss: 5.860 | Reg loss: 0.028 | Tree loss: 5.860 | Accuracy: 0.095703 | 0.058 sec/iter\n",
      "Epoch: 40 | Batch: 007 / 016 | Total loss: 5.805 | Reg loss: 0.028 | Tree loss: 5.805 | Accuracy: 0.089844 | 0.058 sec/iter\n",
      "Epoch: 40 | Batch: 008 / 016 | Total loss: 5.795 | Reg loss: 0.028 | Tree loss: 5.795 | Accuracy: 0.101562 | 0.058 sec/iter\n",
      "Epoch: 40 | Batch: 009 / 016 | Total loss: 5.882 | Reg loss: 0.028 | Tree loss: 5.882 | Accuracy: 0.076172 | 0.058 sec/iter\n",
      "Epoch: 40 | Batch: 010 / 016 | Total loss: 5.835 | Reg loss: 0.028 | Tree loss: 5.835 | Accuracy: 0.076172 | 0.058 sec/iter\n",
      "Epoch: 40 | Batch: 011 / 016 | Total loss: 5.778 | Reg loss: 0.028 | Tree loss: 5.778 | Accuracy: 0.103516 | 0.058 sec/iter\n",
      "Epoch: 40 | Batch: 012 / 016 | Total loss: 5.751 | Reg loss: 0.028 | Tree loss: 5.751 | Accuracy: 0.083984 | 0.058 sec/iter\n",
      "Epoch: 40 | Batch: 013 / 016 | Total loss: 5.803 | Reg loss: 0.029 | Tree loss: 5.803 | Accuracy: 0.087891 | 0.058 sec/iter\n",
      "Epoch: 40 | Batch: 014 / 016 | Total loss: 5.732 | Reg loss: 0.029 | Tree loss: 5.732 | Accuracy: 0.093750 | 0.058 sec/iter\n",
      "Epoch: 40 | Batch: 015 / 016 | Total loss: 5.715 | Reg loss: 0.029 | Tree loss: 5.715 | Accuracy: 0.096154 | 0.058 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 41 | Batch: 000 / 016 | Total loss: 5.856 | Reg loss: 0.028 | Tree loss: 5.856 | Accuracy: 0.105469 | 0.058 sec/iter\n",
      "Epoch: 41 | Batch: 001 / 016 | Total loss: 5.828 | Reg loss: 0.028 | Tree loss: 5.828 | Accuracy: 0.107422 | 0.058 sec/iter\n",
      "Epoch: 41 | Batch: 002 / 016 | Total loss: 5.848 | Reg loss: 0.028 | Tree loss: 5.848 | Accuracy: 0.082031 | 0.058 sec/iter\n",
      "Epoch: 41 | Batch: 003 / 016 | Total loss: 5.805 | Reg loss: 0.028 | Tree loss: 5.805 | Accuracy: 0.099609 | 0.058 sec/iter\n",
      "Epoch: 41 | Batch: 004 / 016 | Total loss: 5.812 | Reg loss: 0.028 | Tree loss: 5.812 | Accuracy: 0.076172 | 0.058 sec/iter\n",
      "Epoch: 41 | Batch: 005 / 016 | Total loss: 5.749 | Reg loss: 0.028 | Tree loss: 5.749 | Accuracy: 0.074219 | 0.058 sec/iter\n",
      "Epoch: 41 | Batch: 006 / 016 | Total loss: 5.805 | Reg loss: 0.028 | Tree loss: 5.805 | Accuracy: 0.093750 | 0.058 sec/iter\n",
      "Epoch: 41 | Batch: 007 / 016 | Total loss: 5.766 | Reg loss: 0.028 | Tree loss: 5.766 | Accuracy: 0.111328 | 0.058 sec/iter\n",
      "Epoch: 41 | Batch: 008 / 016 | Total loss: 5.811 | Reg loss: 0.028 | Tree loss: 5.811 | Accuracy: 0.080078 | 0.058 sec/iter\n",
      "Epoch: 41 | Batch: 009 / 016 | Total loss: 5.738 | Reg loss: 0.029 | Tree loss: 5.738 | Accuracy: 0.087891 | 0.058 sec/iter\n",
      "Epoch: 41 | Batch: 010 / 016 | Total loss: 5.720 | Reg loss: 0.029 | Tree loss: 5.720 | Accuracy: 0.091797 | 0.058 sec/iter\n",
      "Epoch: 41 | Batch: 011 / 016 | Total loss: 5.675 | Reg loss: 0.029 | Tree loss: 5.675 | Accuracy: 0.119141 | 0.058 sec/iter\n",
      "Epoch: 41 | Batch: 012 / 016 | Total loss: 5.732 | Reg loss: 0.029 | Tree loss: 5.732 | Accuracy: 0.087891 | 0.058 sec/iter\n",
      "Epoch: 41 | Batch: 013 / 016 | Total loss: 5.687 | Reg loss: 0.029 | Tree loss: 5.687 | Accuracy: 0.089844 | 0.058 sec/iter\n",
      "Epoch: 41 | Batch: 014 / 016 | Total loss: 5.667 | Reg loss: 0.029 | Tree loss: 5.667 | Accuracy: 0.105469 | 0.058 sec/iter\n",
      "Epoch: 41 | Batch: 015 / 016 | Total loss: 5.706 | Reg loss: 0.029 | Tree loss: 5.706 | Accuracy: 0.079060 | 0.058 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 42 | Batch: 000 / 016 | Total loss: 5.812 | Reg loss: 0.028 | Tree loss: 5.812 | Accuracy: 0.111328 | 0.058 sec/iter\n",
      "Epoch: 42 | Batch: 001 / 016 | Total loss: 5.808 | Reg loss: 0.028 | Tree loss: 5.808 | Accuracy: 0.080078 | 0.058 sec/iter\n",
      "Epoch: 42 | Batch: 002 / 016 | Total loss: 5.759 | Reg loss: 0.028 | Tree loss: 5.759 | Accuracy: 0.125000 | 0.058 sec/iter\n",
      "Epoch: 42 | Batch: 003 / 016 | Total loss: 5.774 | Reg loss: 0.029 | Tree loss: 5.774 | Accuracy: 0.068359 | 0.058 sec/iter\n",
      "Epoch: 42 | Batch: 004 / 016 | Total loss: 5.745 | Reg loss: 0.029 | Tree loss: 5.745 | Accuracy: 0.095703 | 0.058 sec/iter\n",
      "Epoch: 42 | Batch: 005 / 016 | Total loss: 5.741 | Reg loss: 0.029 | Tree loss: 5.741 | Accuracy: 0.099609 | 0.058 sec/iter\n",
      "Epoch: 42 | Batch: 006 / 016 | Total loss: 5.721 | Reg loss: 0.029 | Tree loss: 5.721 | Accuracy: 0.085938 | 0.058 sec/iter\n",
      "Epoch: 42 | Batch: 007 / 016 | Total loss: 5.743 | Reg loss: 0.029 | Tree loss: 5.743 | Accuracy: 0.078125 | 0.058 sec/iter\n",
      "Epoch: 42 | Batch: 008 / 016 | Total loss: 5.627 | Reg loss: 0.029 | Tree loss: 5.627 | Accuracy: 0.111328 | 0.058 sec/iter\n",
      "Epoch: 42 | Batch: 009 / 016 | Total loss: 5.652 | Reg loss: 0.029 | Tree loss: 5.652 | Accuracy: 0.076172 | 0.058 sec/iter\n",
      "Epoch: 42 | Batch: 010 / 016 | Total loss: 5.624 | Reg loss: 0.029 | Tree loss: 5.624 | Accuracy: 0.101562 | 0.058 sec/iter\n",
      "Epoch: 42 | Batch: 011 / 016 | Total loss: 5.654 | Reg loss: 0.029 | Tree loss: 5.654 | Accuracy: 0.095703 | 0.058 sec/iter\n",
      "Epoch: 42 | Batch: 012 / 016 | Total loss: 5.651 | Reg loss: 0.029 | Tree loss: 5.651 | Accuracy: 0.076172 | 0.058 sec/iter\n",
      "Epoch: 42 | Batch: 013 / 016 | Total loss: 5.631 | Reg loss: 0.029 | Tree loss: 5.631 | Accuracy: 0.105469 | 0.058 sec/iter\n",
      "Epoch: 42 | Batch: 014 / 016 | Total loss: 5.540 | Reg loss: 0.029 | Tree loss: 5.540 | Accuracy: 0.095703 | 0.058 sec/iter\n",
      "Epoch: 42 | Batch: 015 / 016 | Total loss: 5.593 | Reg loss: 0.029 | Tree loss: 5.593 | Accuracy: 0.087607 | 0.058 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 43 | Batch: 000 / 016 | Total loss: 5.732 | Reg loss: 0.029 | Tree loss: 5.732 | Accuracy: 0.095703 | 0.058 sec/iter\n",
      "Epoch: 43 | Batch: 001 / 016 | Total loss: 5.787 | Reg loss: 0.029 | Tree loss: 5.787 | Accuracy: 0.070312 | 0.058 sec/iter\n",
      "Epoch: 43 | Batch: 002 / 016 | Total loss: 5.743 | Reg loss: 0.029 | Tree loss: 5.743 | Accuracy: 0.093750 | 0.058 sec/iter\n",
      "Epoch: 43 | Batch: 003 / 016 | Total loss: 5.651 | Reg loss: 0.029 | Tree loss: 5.651 | Accuracy: 0.095703 | 0.058 sec/iter\n",
      "Epoch: 43 | Batch: 004 / 016 | Total loss: 5.674 | Reg loss: 0.029 | Tree loss: 5.674 | Accuracy: 0.080078 | 0.058 sec/iter\n",
      "Epoch: 43 | Batch: 005 / 016 | Total loss: 5.639 | Reg loss: 0.029 | Tree loss: 5.639 | Accuracy: 0.107422 | 0.058 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 43 | Batch: 006 / 016 | Total loss: 5.629 | Reg loss: 0.029 | Tree loss: 5.629 | Accuracy: 0.111328 | 0.058 sec/iter\n",
      "Epoch: 43 | Batch: 007 / 016 | Total loss: 5.615 | Reg loss: 0.029 | Tree loss: 5.615 | Accuracy: 0.093750 | 0.058 sec/iter\n",
      "Epoch: 43 | Batch: 008 / 016 | Total loss: 5.567 | Reg loss: 0.029 | Tree loss: 5.567 | Accuracy: 0.103516 | 0.058 sec/iter\n",
      "Epoch: 43 | Batch: 009 / 016 | Total loss: 5.596 | Reg loss: 0.029 | Tree loss: 5.596 | Accuracy: 0.091797 | 0.058 sec/iter\n",
      "Epoch: 43 | Batch: 010 / 016 | Total loss: 5.616 | Reg loss: 0.029 | Tree loss: 5.616 | Accuracy: 0.080078 | 0.058 sec/iter\n",
      "Epoch: 43 | Batch: 011 / 016 | Total loss: 5.580 | Reg loss: 0.029 | Tree loss: 5.580 | Accuracy: 0.083984 | 0.058 sec/iter\n",
      "Epoch: 43 | Batch: 012 / 016 | Total loss: 5.579 | Reg loss: 0.029 | Tree loss: 5.579 | Accuracy: 0.103516 | 0.058 sec/iter\n",
      "Epoch: 43 | Batch: 013 / 016 | Total loss: 5.544 | Reg loss: 0.029 | Tree loss: 5.544 | Accuracy: 0.091797 | 0.058 sec/iter\n",
      "Epoch: 43 | Batch: 014 / 016 | Total loss: 5.509 | Reg loss: 0.029 | Tree loss: 5.509 | Accuracy: 0.109375 | 0.058 sec/iter\n",
      "Epoch: 43 | Batch: 015 / 016 | Total loss: 5.517 | Reg loss: 0.029 | Tree loss: 5.517 | Accuracy: 0.076923 | 0.058 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 44 | Batch: 000 / 016 | Total loss: 5.679 | Reg loss: 0.029 | Tree loss: 5.679 | Accuracy: 0.091797 | 0.058 sec/iter\n",
      "Epoch: 44 | Batch: 001 / 016 | Total loss: 5.704 | Reg loss: 0.029 | Tree loss: 5.704 | Accuracy: 0.082031 | 0.058 sec/iter\n",
      "Epoch: 44 | Batch: 002 / 016 | Total loss: 5.613 | Reg loss: 0.029 | Tree loss: 5.613 | Accuracy: 0.093750 | 0.058 sec/iter\n",
      "Epoch: 44 | Batch: 003 / 016 | Total loss: 5.645 | Reg loss: 0.029 | Tree loss: 5.645 | Accuracy: 0.074219 | 0.058 sec/iter\n",
      "Epoch: 44 | Batch: 004 / 016 | Total loss: 5.590 | Reg loss: 0.029 | Tree loss: 5.590 | Accuracy: 0.093750 | 0.058 sec/iter\n",
      "Epoch: 44 | Batch: 005 / 016 | Total loss: 5.636 | Reg loss: 0.029 | Tree loss: 5.636 | Accuracy: 0.085938 | 0.058 sec/iter\n",
      "Epoch: 44 | Batch: 006 / 016 | Total loss: 5.546 | Reg loss: 0.029 | Tree loss: 5.546 | Accuracy: 0.113281 | 0.058 sec/iter\n",
      "Epoch: 44 | Batch: 007 / 016 | Total loss: 5.522 | Reg loss: 0.029 | Tree loss: 5.522 | Accuracy: 0.101562 | 0.058 sec/iter\n",
      "Epoch: 44 | Batch: 008 / 016 | Total loss: 5.559 | Reg loss: 0.029 | Tree loss: 5.559 | Accuracy: 0.089844 | 0.058 sec/iter\n",
      "Epoch: 44 | Batch: 009 / 016 | Total loss: 5.477 | Reg loss: 0.029 | Tree loss: 5.477 | Accuracy: 0.117188 | 0.058 sec/iter\n",
      "Epoch: 44 | Batch: 010 / 016 | Total loss: 5.520 | Reg loss: 0.029 | Tree loss: 5.520 | Accuracy: 0.095703 | 0.058 sec/iter\n",
      "Epoch: 44 | Batch: 011 / 016 | Total loss: 5.528 | Reg loss: 0.029 | Tree loss: 5.528 | Accuracy: 0.093750 | 0.058 sec/iter\n",
      "Epoch: 44 | Batch: 012 / 016 | Total loss: 5.463 | Reg loss: 0.029 | Tree loss: 5.463 | Accuracy: 0.105469 | 0.058 sec/iter\n",
      "Epoch: 44 | Batch: 013 / 016 | Total loss: 5.471 | Reg loss: 0.029 | Tree loss: 5.471 | Accuracy: 0.091797 | 0.058 sec/iter\n",
      "Epoch: 44 | Batch: 014 / 016 | Total loss: 5.499 | Reg loss: 0.029 | Tree loss: 5.499 | Accuracy: 0.080078 | 0.058 sec/iter\n",
      "Epoch: 44 | Batch: 015 / 016 | Total loss: 5.474 | Reg loss: 0.029 | Tree loss: 5.474 | Accuracy: 0.083333 | 0.058 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 45 | Batch: 000 / 016 | Total loss: 5.584 | Reg loss: 0.029 | Tree loss: 5.584 | Accuracy: 0.085938 | 0.058 sec/iter\n",
      "Epoch: 45 | Batch: 001 / 016 | Total loss: 5.569 | Reg loss: 0.029 | Tree loss: 5.569 | Accuracy: 0.117188 | 0.058 sec/iter\n",
      "Epoch: 45 | Batch: 002 / 016 | Total loss: 5.583 | Reg loss: 0.029 | Tree loss: 5.583 | Accuracy: 0.101562 | 0.058 sec/iter\n",
      "Epoch: 45 | Batch: 003 / 016 | Total loss: 5.510 | Reg loss: 0.029 | Tree loss: 5.510 | Accuracy: 0.115234 | 0.058 sec/iter\n",
      "Epoch: 45 | Batch: 004 / 016 | Total loss: 5.557 | Reg loss: 0.029 | Tree loss: 5.557 | Accuracy: 0.066406 | 0.058 sec/iter\n",
      "Epoch: 45 | Batch: 005 / 016 | Total loss: 5.526 | Reg loss: 0.029 | Tree loss: 5.526 | Accuracy: 0.080078 | 0.058 sec/iter\n",
      "Epoch: 45 | Batch: 006 / 016 | Total loss: 5.569 | Reg loss: 0.029 | Tree loss: 5.569 | Accuracy: 0.097656 | 0.058 sec/iter\n",
      "Epoch: 45 | Batch: 007 / 016 | Total loss: 5.518 | Reg loss: 0.029 | Tree loss: 5.518 | Accuracy: 0.089844 | 0.058 sec/iter\n",
      "Epoch: 45 | Batch: 008 / 016 | Total loss: 5.487 | Reg loss: 0.029 | Tree loss: 5.487 | Accuracy: 0.087891 | 0.058 sec/iter\n",
      "Epoch: 45 | Batch: 009 / 016 | Total loss: 5.534 | Reg loss: 0.029 | Tree loss: 5.534 | Accuracy: 0.085938 | 0.058 sec/iter\n",
      "Epoch: 45 | Batch: 010 / 016 | Total loss: 5.438 | Reg loss: 0.029 | Tree loss: 5.438 | Accuracy: 0.105469 | 0.058 sec/iter\n",
      "Epoch: 45 | Batch: 011 / 016 | Total loss: 5.429 | Reg loss: 0.029 | Tree loss: 5.429 | Accuracy: 0.097656 | 0.058 sec/iter\n",
      "Epoch: 45 | Batch: 012 / 016 | Total loss: 5.436 | Reg loss: 0.029 | Tree loss: 5.436 | Accuracy: 0.070312 | 0.058 sec/iter\n",
      "Epoch: 45 | Batch: 013 / 016 | Total loss: 5.417 | Reg loss: 0.030 | Tree loss: 5.417 | Accuracy: 0.085938 | 0.058 sec/iter\n",
      "Epoch: 45 | Batch: 014 / 016 | Total loss: 5.376 | Reg loss: 0.030 | Tree loss: 5.376 | Accuracy: 0.113281 | 0.058 sec/iter\n",
      "Epoch: 45 | Batch: 015 / 016 | Total loss: 5.369 | Reg loss: 0.030 | Tree loss: 5.369 | Accuracy: 0.100427 | 0.058 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 46 | Batch: 000 / 016 | Total loss: 5.500 | Reg loss: 0.029 | Tree loss: 5.500 | Accuracy: 0.126953 | 0.058 sec/iter\n",
      "Epoch: 46 | Batch: 001 / 016 | Total loss: 5.518 | Reg loss: 0.029 | Tree loss: 5.518 | Accuracy: 0.115234 | 0.058 sec/iter\n",
      "Epoch: 46 | Batch: 002 / 016 | Total loss: 5.510 | Reg loss: 0.029 | Tree loss: 5.510 | Accuracy: 0.099609 | 0.058 sec/iter\n",
      "Epoch: 46 | Batch: 003 / 016 | Total loss: 5.483 | Reg loss: 0.029 | Tree loss: 5.483 | Accuracy: 0.091797 | 0.058 sec/iter\n",
      "Epoch: 46 | Batch: 004 / 016 | Total loss: 5.505 | Reg loss: 0.029 | Tree loss: 5.505 | Accuracy: 0.072266 | 0.058 sec/iter\n",
      "Epoch: 46 | Batch: 005 / 016 | Total loss: 5.508 | Reg loss: 0.029 | Tree loss: 5.508 | Accuracy: 0.097656 | 0.058 sec/iter\n",
      "Epoch: 46 | Batch: 006 / 016 | Total loss: 5.486 | Reg loss: 0.029 | Tree loss: 5.486 | Accuracy: 0.082031 | 0.058 sec/iter\n",
      "Epoch: 46 | Batch: 007 / 016 | Total loss: 5.483 | Reg loss: 0.029 | Tree loss: 5.483 | Accuracy: 0.076172 | 0.058 sec/iter\n",
      "Epoch: 46 | Batch: 008 / 016 | Total loss: 5.381 | Reg loss: 0.029 | Tree loss: 5.381 | Accuracy: 0.109375 | 0.058 sec/iter\n",
      "Epoch: 46 | Batch: 009 / 016 | Total loss: 5.413 | Reg loss: 0.030 | Tree loss: 5.413 | Accuracy: 0.093750 | 0.058 sec/iter\n",
      "Epoch: 46 | Batch: 010 / 016 | Total loss: 5.387 | Reg loss: 0.030 | Tree loss: 5.387 | Accuracy: 0.097656 | 0.058 sec/iter\n",
      "Epoch: 46 | Batch: 011 / 016 | Total loss: 5.354 | Reg loss: 0.030 | Tree loss: 5.354 | Accuracy: 0.115234 | 0.058 sec/iter\n",
      "Epoch: 46 | Batch: 012 / 016 | Total loss: 5.371 | Reg loss: 0.030 | Tree loss: 5.371 | Accuracy: 0.082031 | 0.058 sec/iter\n",
      "Epoch: 46 | Batch: 013 / 016 | Total loss: 5.353 | Reg loss: 0.030 | Tree loss: 5.353 | Accuracy: 0.074219 | 0.058 sec/iter\n",
      "Epoch: 46 | Batch: 014 / 016 | Total loss: 5.375 | Reg loss: 0.030 | Tree loss: 5.375 | Accuracy: 0.089844 | 0.058 sec/iter\n",
      "Epoch: 46 | Batch: 015 / 016 | Total loss: 5.282 | Reg loss: 0.030 | Tree loss: 5.282 | Accuracy: 0.102564 | 0.058 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 47 | Batch: 000 / 016 | Total loss: 5.469 | Reg loss: 0.029 | Tree loss: 5.469 | Accuracy: 0.107422 | 0.058 sec/iter\n",
      "Epoch: 47 | Batch: 001 / 016 | Total loss: 5.462 | Reg loss: 0.029 | Tree loss: 5.462 | Accuracy: 0.087891 | 0.058 sec/iter\n",
      "Epoch: 47 | Batch: 002 / 016 | Total loss: 5.462 | Reg loss: 0.029 | Tree loss: 5.462 | Accuracy: 0.097656 | 0.058 sec/iter\n",
      "Epoch: 47 | Batch: 003 / 016 | Total loss: 5.440 | Reg loss: 0.029 | Tree loss: 5.440 | Accuracy: 0.117188 | 0.058 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 47 | Batch: 004 / 016 | Total loss: 5.435 | Reg loss: 0.030 | Tree loss: 5.435 | Accuracy: 0.083984 | 0.058 sec/iter\n",
      "Epoch: 47 | Batch: 005 / 016 | Total loss: 5.421 | Reg loss: 0.030 | Tree loss: 5.421 | Accuracy: 0.085938 | 0.058 sec/iter\n",
      "Epoch: 47 | Batch: 006 / 016 | Total loss: 5.400 | Reg loss: 0.030 | Tree loss: 5.400 | Accuracy: 0.093750 | 0.058 sec/iter\n",
      "Epoch: 47 | Batch: 007 / 016 | Total loss: 5.436 | Reg loss: 0.030 | Tree loss: 5.436 | Accuracy: 0.095703 | 0.058 sec/iter\n",
      "Epoch: 47 | Batch: 008 / 016 | Total loss: 5.331 | Reg loss: 0.030 | Tree loss: 5.331 | Accuracy: 0.095703 | 0.058 sec/iter\n",
      "Epoch: 47 | Batch: 009 / 016 | Total loss: 5.310 | Reg loss: 0.030 | Tree loss: 5.310 | Accuracy: 0.095703 | 0.058 sec/iter\n",
      "Epoch: 47 | Batch: 010 / 016 | Total loss: 5.271 | Reg loss: 0.030 | Tree loss: 5.271 | Accuracy: 0.101562 | 0.058 sec/iter\n",
      "Epoch: 47 | Batch: 011 / 016 | Total loss: 5.342 | Reg loss: 0.030 | Tree loss: 5.342 | Accuracy: 0.103516 | 0.058 sec/iter\n",
      "Epoch: 47 | Batch: 012 / 016 | Total loss: 5.281 | Reg loss: 0.030 | Tree loss: 5.281 | Accuracy: 0.105469 | 0.058 sec/iter\n",
      "Epoch: 47 | Batch: 013 / 016 | Total loss: 5.299 | Reg loss: 0.030 | Tree loss: 5.299 | Accuracy: 0.099609 | 0.058 sec/iter\n",
      "Epoch: 47 | Batch: 014 / 016 | Total loss: 5.313 | Reg loss: 0.030 | Tree loss: 5.313 | Accuracy: 0.083984 | 0.058 sec/iter\n",
      "Epoch: 47 | Batch: 015 / 016 | Total loss: 5.288 | Reg loss: 0.030 | Tree loss: 5.288 | Accuracy: 0.094017 | 0.058 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 48 | Batch: 000 / 016 | Total loss: 5.446 | Reg loss: 0.030 | Tree loss: 5.446 | Accuracy: 0.095703 | 0.058 sec/iter\n",
      "Epoch: 48 | Batch: 001 / 016 | Total loss: 5.427 | Reg loss: 0.030 | Tree loss: 5.427 | Accuracy: 0.111328 | 0.058 sec/iter\n",
      "Epoch: 48 | Batch: 002 / 016 | Total loss: 5.372 | Reg loss: 0.030 | Tree loss: 5.372 | Accuracy: 0.123047 | 0.058 sec/iter\n",
      "Epoch: 48 | Batch: 003 / 016 | Total loss: 5.405 | Reg loss: 0.030 | Tree loss: 5.405 | Accuracy: 0.113281 | 0.058 sec/iter\n",
      "Epoch: 48 | Batch: 004 / 016 | Total loss: 5.375 | Reg loss: 0.030 | Tree loss: 5.375 | Accuracy: 0.125000 | 0.058 sec/iter\n",
      "Epoch: 48 | Batch: 005 / 016 | Total loss: 5.334 | Reg loss: 0.030 | Tree loss: 5.334 | Accuracy: 0.119141 | 0.058 sec/iter\n",
      "Epoch: 48 | Batch: 006 / 016 | Total loss: 5.305 | Reg loss: 0.030 | Tree loss: 5.305 | Accuracy: 0.074219 | 0.058 sec/iter\n",
      "Epoch: 48 | Batch: 007 / 016 | Total loss: 5.329 | Reg loss: 0.030 | Tree loss: 5.329 | Accuracy: 0.101562 | 0.058 sec/iter\n",
      "Epoch: 48 | Batch: 008 / 016 | Total loss: 5.316 | Reg loss: 0.030 | Tree loss: 5.316 | Accuracy: 0.091797 | 0.058 sec/iter\n",
      "Epoch: 48 | Batch: 009 / 016 | Total loss: 5.291 | Reg loss: 0.030 | Tree loss: 5.291 | Accuracy: 0.085938 | 0.058 sec/iter\n",
      "Epoch: 48 | Batch: 010 / 016 | Total loss: 5.253 | Reg loss: 0.030 | Tree loss: 5.253 | Accuracy: 0.089844 | 0.058 sec/iter\n",
      "Epoch: 48 | Batch: 011 / 016 | Total loss: 5.306 | Reg loss: 0.030 | Tree loss: 5.306 | Accuracy: 0.080078 | 0.058 sec/iter\n",
      "Epoch: 48 | Batch: 012 / 016 | Total loss: 5.233 | Reg loss: 0.030 | Tree loss: 5.233 | Accuracy: 0.085938 | 0.058 sec/iter\n",
      "Epoch: 48 | Batch: 013 / 016 | Total loss: 5.260 | Reg loss: 0.030 | Tree loss: 5.260 | Accuracy: 0.083984 | 0.058 sec/iter\n",
      "Epoch: 48 | Batch: 014 / 016 | Total loss: 5.211 | Reg loss: 0.030 | Tree loss: 5.211 | Accuracy: 0.091797 | 0.058 sec/iter\n",
      "Epoch: 48 | Batch: 015 / 016 | Total loss: 5.165 | Reg loss: 0.030 | Tree loss: 5.165 | Accuracy: 0.102564 | 0.058 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 49 | Batch: 000 / 016 | Total loss: 5.362 | Reg loss: 0.030 | Tree loss: 5.362 | Accuracy: 0.119141 | 0.058 sec/iter\n",
      "Epoch: 49 | Batch: 001 / 016 | Total loss: 5.351 | Reg loss: 0.030 | Tree loss: 5.351 | Accuracy: 0.095703 | 0.058 sec/iter\n",
      "Epoch: 49 | Batch: 002 / 016 | Total loss: 5.350 | Reg loss: 0.030 | Tree loss: 5.350 | Accuracy: 0.101562 | 0.058 sec/iter\n",
      "Epoch: 49 | Batch: 003 / 016 | Total loss: 5.319 | Reg loss: 0.030 | Tree loss: 5.319 | Accuracy: 0.111328 | 0.058 sec/iter\n",
      "Epoch: 49 | Batch: 004 / 016 | Total loss: 5.328 | Reg loss: 0.030 | Tree loss: 5.328 | Accuracy: 0.095703 | 0.058 sec/iter\n",
      "Epoch: 49 | Batch: 005 / 016 | Total loss: 5.245 | Reg loss: 0.030 | Tree loss: 5.245 | Accuracy: 0.113281 | 0.058 sec/iter\n",
      "Epoch: 49 | Batch: 006 / 016 | Total loss: 5.234 | Reg loss: 0.030 | Tree loss: 5.234 | Accuracy: 0.109375 | 0.058 sec/iter\n",
      "Epoch: 49 | Batch: 007 / 016 | Total loss: 5.222 | Reg loss: 0.030 | Tree loss: 5.222 | Accuracy: 0.117188 | 0.058 sec/iter\n",
      "Epoch: 49 | Batch: 008 / 016 | Total loss: 5.243 | Reg loss: 0.030 | Tree loss: 5.243 | Accuracy: 0.066406 | 0.058 sec/iter\n",
      "Epoch: 49 | Batch: 009 / 016 | Total loss: 5.228 | Reg loss: 0.030 | Tree loss: 5.228 | Accuracy: 0.107422 | 0.058 sec/iter\n",
      "Epoch: 49 | Batch: 010 / 016 | Total loss: 5.248 | Reg loss: 0.030 | Tree loss: 5.248 | Accuracy: 0.083984 | 0.058 sec/iter\n",
      "Epoch: 49 | Batch: 011 / 016 | Total loss: 5.229 | Reg loss: 0.030 | Tree loss: 5.229 | Accuracy: 0.091797 | 0.058 sec/iter\n",
      "Epoch: 49 | Batch: 012 / 016 | Total loss: 5.208 | Reg loss: 0.030 | Tree loss: 5.208 | Accuracy: 0.087891 | 0.058 sec/iter\n",
      "Epoch: 49 | Batch: 013 / 016 | Total loss: 5.204 | Reg loss: 0.030 | Tree loss: 5.204 | Accuracy: 0.095703 | 0.058 sec/iter\n",
      "Epoch: 49 | Batch: 014 / 016 | Total loss: 5.209 | Reg loss: 0.030 | Tree loss: 5.209 | Accuracy: 0.091797 | 0.058 sec/iter\n",
      "Epoch: 49 | Batch: 015 / 016 | Total loss: 5.174 | Reg loss: 0.030 | Tree loss: 5.174 | Accuracy: 0.089744 | 0.058 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 50 | Batch: 000 / 016 | Total loss: 5.271 | Reg loss: 0.030 | Tree loss: 5.271 | Accuracy: 0.121094 | 0.058 sec/iter\n",
      "Epoch: 50 | Batch: 001 / 016 | Total loss: 5.276 | Reg loss: 0.030 | Tree loss: 5.276 | Accuracy: 0.117188 | 0.058 sec/iter\n",
      "Epoch: 50 | Batch: 002 / 016 | Total loss: 5.268 | Reg loss: 0.030 | Tree loss: 5.268 | Accuracy: 0.099609 | 0.058 sec/iter\n",
      "Epoch: 50 | Batch: 003 / 016 | Total loss: 5.316 | Reg loss: 0.030 | Tree loss: 5.316 | Accuracy: 0.109375 | 0.058 sec/iter\n",
      "Epoch: 50 | Batch: 004 / 016 | Total loss: 5.272 | Reg loss: 0.030 | Tree loss: 5.272 | Accuracy: 0.089844 | 0.058 sec/iter\n",
      "Epoch: 50 | Batch: 005 / 016 | Total loss: 5.242 | Reg loss: 0.030 | Tree loss: 5.242 | Accuracy: 0.125000 | 0.058 sec/iter\n",
      "Epoch: 50 | Batch: 006 / 016 | Total loss: 5.220 | Reg loss: 0.030 | Tree loss: 5.220 | Accuracy: 0.097656 | 0.058 sec/iter\n",
      "Epoch: 50 | Batch: 007 / 016 | Total loss: 5.165 | Reg loss: 0.030 | Tree loss: 5.165 | Accuracy: 0.121094 | 0.058 sec/iter\n",
      "Epoch: 50 | Batch: 008 / 016 | Total loss: 5.185 | Reg loss: 0.030 | Tree loss: 5.185 | Accuracy: 0.093750 | 0.058 sec/iter\n",
      "Epoch: 50 | Batch: 009 / 016 | Total loss: 5.173 | Reg loss: 0.030 | Tree loss: 5.173 | Accuracy: 0.107422 | 0.057 sec/iter\n",
      "Epoch: 50 | Batch: 010 / 016 | Total loss: 5.206 | Reg loss: 0.030 | Tree loss: 5.206 | Accuracy: 0.083984 | 0.057 sec/iter\n",
      "Epoch: 50 | Batch: 011 / 016 | Total loss: 5.216 | Reg loss: 0.030 | Tree loss: 5.216 | Accuracy: 0.099609 | 0.057 sec/iter\n",
      "Epoch: 50 | Batch: 012 / 016 | Total loss: 5.166 | Reg loss: 0.030 | Tree loss: 5.166 | Accuracy: 0.076172 | 0.057 sec/iter\n",
      "Epoch: 50 | Batch: 013 / 016 | Total loss: 5.160 | Reg loss: 0.030 | Tree loss: 5.160 | Accuracy: 0.099609 | 0.057 sec/iter\n",
      "Epoch: 50 | Batch: 014 / 016 | Total loss: 5.069 | Reg loss: 0.030 | Tree loss: 5.069 | Accuracy: 0.083984 | 0.057 sec/iter\n",
      "Epoch: 50 | Batch: 015 / 016 | Total loss: 5.094 | Reg loss: 0.030 | Tree loss: 5.094 | Accuracy: 0.091880 | 0.057 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 51 | Batch: 000 / 016 | Total loss: 5.319 | Reg loss: 0.030 | Tree loss: 5.319 | Accuracy: 0.080078 | 0.057 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 51 | Batch: 001 / 016 | Total loss: 5.239 | Reg loss: 0.030 | Tree loss: 5.239 | Accuracy: 0.115234 | 0.057 sec/iter\n",
      "Epoch: 51 | Batch: 002 / 016 | Total loss: 5.206 | Reg loss: 0.030 | Tree loss: 5.206 | Accuracy: 0.115234 | 0.057 sec/iter\n",
      "Epoch: 51 | Batch: 003 / 016 | Total loss: 5.195 | Reg loss: 0.030 | Tree loss: 5.195 | Accuracy: 0.119141 | 0.057 sec/iter\n",
      "Epoch: 51 | Batch: 004 / 016 | Total loss: 5.174 | Reg loss: 0.030 | Tree loss: 5.174 | Accuracy: 0.128906 | 0.057 sec/iter\n",
      "Epoch: 51 | Batch: 005 / 016 | Total loss: 5.189 | Reg loss: 0.030 | Tree loss: 5.189 | Accuracy: 0.085938 | 0.057 sec/iter\n",
      "Epoch: 51 | Batch: 006 / 016 | Total loss: 5.228 | Reg loss: 0.030 | Tree loss: 5.228 | Accuracy: 0.095703 | 0.058 sec/iter\n",
      "Epoch: 51 | Batch: 007 / 016 | Total loss: 5.134 | Reg loss: 0.030 | Tree loss: 5.134 | Accuracy: 0.103516 | 0.058 sec/iter\n",
      "Epoch: 51 | Batch: 008 / 016 | Total loss: 5.130 | Reg loss: 0.030 | Tree loss: 5.130 | Accuracy: 0.128906 | 0.058 sec/iter\n",
      "Epoch: 51 | Batch: 009 / 016 | Total loss: 5.182 | Reg loss: 0.030 | Tree loss: 5.182 | Accuracy: 0.080078 | 0.058 sec/iter\n",
      "Epoch: 51 | Batch: 010 / 016 | Total loss: 5.116 | Reg loss: 0.030 | Tree loss: 5.116 | Accuracy: 0.095703 | 0.058 sec/iter\n",
      "Epoch: 51 | Batch: 011 / 016 | Total loss: 5.115 | Reg loss: 0.030 | Tree loss: 5.115 | Accuracy: 0.097656 | 0.058 sec/iter\n",
      "Epoch: 51 | Batch: 012 / 016 | Total loss: 5.095 | Reg loss: 0.030 | Tree loss: 5.095 | Accuracy: 0.099609 | 0.058 sec/iter\n",
      "Epoch: 51 | Batch: 013 / 016 | Total loss: 5.055 | Reg loss: 0.030 | Tree loss: 5.055 | Accuracy: 0.082031 | 0.058 sec/iter\n",
      "Epoch: 51 | Batch: 014 / 016 | Total loss: 5.068 | Reg loss: 0.030 | Tree loss: 5.068 | Accuracy: 0.091797 | 0.058 sec/iter\n",
      "Epoch: 51 | Batch: 015 / 016 | Total loss: 5.024 | Reg loss: 0.031 | Tree loss: 5.024 | Accuracy: 0.115385 | 0.058 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 52 | Batch: 000 / 016 | Total loss: 5.177 | Reg loss: 0.030 | Tree loss: 5.177 | Accuracy: 0.121094 | 0.058 sec/iter\n",
      "Epoch: 52 | Batch: 001 / 016 | Total loss: 5.166 | Reg loss: 0.030 | Tree loss: 5.166 | Accuracy: 0.113281 | 0.058 sec/iter\n",
      "Epoch: 52 | Batch: 002 / 016 | Total loss: 5.198 | Reg loss: 0.030 | Tree loss: 5.198 | Accuracy: 0.099609 | 0.058 sec/iter\n",
      "Epoch: 52 | Batch: 003 / 016 | Total loss: 5.168 | Reg loss: 0.030 | Tree loss: 5.168 | Accuracy: 0.097656 | 0.058 sec/iter\n",
      "Epoch: 52 | Batch: 004 / 016 | Total loss: 5.147 | Reg loss: 0.030 | Tree loss: 5.147 | Accuracy: 0.103516 | 0.058 sec/iter\n",
      "Epoch: 52 | Batch: 005 / 016 | Total loss: 5.147 | Reg loss: 0.030 | Tree loss: 5.147 | Accuracy: 0.111328 | 0.058 sec/iter\n",
      "Epoch: 52 | Batch: 006 / 016 | Total loss: 5.103 | Reg loss: 0.030 | Tree loss: 5.103 | Accuracy: 0.117188 | 0.058 sec/iter\n",
      "Epoch: 52 | Batch: 007 / 016 | Total loss: 5.125 | Reg loss: 0.030 | Tree loss: 5.125 | Accuracy: 0.103516 | 0.058 sec/iter\n",
      "Epoch: 52 | Batch: 008 / 016 | Total loss: 5.081 | Reg loss: 0.030 | Tree loss: 5.081 | Accuracy: 0.111328 | 0.058 sec/iter\n",
      "Epoch: 52 | Batch: 009 / 016 | Total loss: 5.065 | Reg loss: 0.030 | Tree loss: 5.065 | Accuracy: 0.113281 | 0.058 sec/iter\n",
      "Epoch: 52 | Batch: 010 / 016 | Total loss: 5.076 | Reg loss: 0.030 | Tree loss: 5.076 | Accuracy: 0.103516 | 0.058 sec/iter\n",
      "Epoch: 52 | Batch: 011 / 016 | Total loss: 5.046 | Reg loss: 0.030 | Tree loss: 5.046 | Accuracy: 0.115234 | 0.058 sec/iter\n",
      "Epoch: 52 | Batch: 012 / 016 | Total loss: 5.017 | Reg loss: 0.031 | Tree loss: 5.017 | Accuracy: 0.099609 | 0.058 sec/iter\n",
      "Epoch: 52 | Batch: 013 / 016 | Total loss: 5.088 | Reg loss: 0.031 | Tree loss: 5.088 | Accuracy: 0.085938 | 0.058 sec/iter\n",
      "Epoch: 52 | Batch: 014 / 016 | Total loss: 5.052 | Reg loss: 0.031 | Tree loss: 5.052 | Accuracy: 0.078125 | 0.058 sec/iter\n",
      "Epoch: 52 | Batch: 015 / 016 | Total loss: 5.022 | Reg loss: 0.031 | Tree loss: 5.022 | Accuracy: 0.081197 | 0.058 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 53 | Batch: 000 / 016 | Total loss: 5.119 | Reg loss: 0.030 | Tree loss: 5.119 | Accuracy: 0.123047 | 0.058 sec/iter\n",
      "Epoch: 53 | Batch: 001 / 016 | Total loss: 5.192 | Reg loss: 0.030 | Tree loss: 5.192 | Accuracy: 0.089844 | 0.058 sec/iter\n",
      "Epoch: 53 | Batch: 002 / 016 | Total loss: 5.127 | Reg loss: 0.030 | Tree loss: 5.127 | Accuracy: 0.097656 | 0.058 sec/iter\n",
      "Epoch: 53 | Batch: 003 / 016 | Total loss: 5.140 | Reg loss: 0.030 | Tree loss: 5.140 | Accuracy: 0.101562 | 0.058 sec/iter\n",
      "Epoch: 53 | Batch: 004 / 016 | Total loss: 5.095 | Reg loss: 0.030 | Tree loss: 5.095 | Accuracy: 0.083984 | 0.058 sec/iter\n",
      "Epoch: 53 | Batch: 005 / 016 | Total loss: 5.102 | Reg loss: 0.030 | Tree loss: 5.102 | Accuracy: 0.097656 | 0.058 sec/iter\n",
      "Epoch: 53 | Batch: 006 / 016 | Total loss: 5.104 | Reg loss: 0.030 | Tree loss: 5.104 | Accuracy: 0.105469 | 0.058 sec/iter\n",
      "Epoch: 53 | Batch: 007 / 016 | Total loss: 5.052 | Reg loss: 0.030 | Tree loss: 5.052 | Accuracy: 0.119141 | 0.058 sec/iter\n",
      "Epoch: 53 | Batch: 008 / 016 | Total loss: 5.070 | Reg loss: 0.030 | Tree loss: 5.070 | Accuracy: 0.109375 | 0.058 sec/iter\n",
      "Epoch: 53 | Batch: 009 / 016 | Total loss: 4.996 | Reg loss: 0.031 | Tree loss: 4.996 | Accuracy: 0.113281 | 0.058 sec/iter\n",
      "Epoch: 53 | Batch: 010 / 016 | Total loss: 5.045 | Reg loss: 0.031 | Tree loss: 5.045 | Accuracy: 0.095703 | 0.058 sec/iter\n",
      "Epoch: 53 | Batch: 011 / 016 | Total loss: 5.032 | Reg loss: 0.031 | Tree loss: 5.032 | Accuracy: 0.105469 | 0.058 sec/iter\n",
      "Epoch: 53 | Batch: 012 / 016 | Total loss: 4.946 | Reg loss: 0.031 | Tree loss: 4.946 | Accuracy: 0.113281 | 0.058 sec/iter\n",
      "Epoch: 53 | Batch: 013 / 016 | Total loss: 4.987 | Reg loss: 0.031 | Tree loss: 4.987 | Accuracy: 0.107422 | 0.058 sec/iter\n",
      "Epoch: 53 | Batch: 014 / 016 | Total loss: 4.996 | Reg loss: 0.031 | Tree loss: 4.996 | Accuracy: 0.093750 | 0.058 sec/iter\n",
      "Epoch: 53 | Batch: 015 / 016 | Total loss: 4.915 | Reg loss: 0.031 | Tree loss: 4.915 | Accuracy: 0.123932 | 0.058 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 54 | Batch: 000 / 016 | Total loss: 5.106 | Reg loss: 0.030 | Tree loss: 5.106 | Accuracy: 0.105469 | 0.058 sec/iter\n",
      "Epoch: 54 | Batch: 001 / 016 | Total loss: 5.025 | Reg loss: 0.030 | Tree loss: 5.025 | Accuracy: 0.125000 | 0.058 sec/iter\n",
      "Epoch: 54 | Batch: 002 / 016 | Total loss: 5.113 | Reg loss: 0.030 | Tree loss: 5.113 | Accuracy: 0.113281 | 0.058 sec/iter\n",
      "Epoch: 54 | Batch: 003 / 016 | Total loss: 5.099 | Reg loss: 0.030 | Tree loss: 5.099 | Accuracy: 0.107422 | 0.058 sec/iter\n",
      "Epoch: 54 | Batch: 004 / 016 | Total loss: 5.087 | Reg loss: 0.030 | Tree loss: 5.087 | Accuracy: 0.117188 | 0.058 sec/iter\n",
      "Epoch: 54 | Batch: 005 / 016 | Total loss: 5.006 | Reg loss: 0.030 | Tree loss: 5.006 | Accuracy: 0.109375 | 0.058 sec/iter\n",
      "Epoch: 54 | Batch: 006 / 016 | Total loss: 5.098 | Reg loss: 0.031 | Tree loss: 5.098 | Accuracy: 0.095703 | 0.058 sec/iter\n",
      "Epoch: 54 | Batch: 007 / 016 | Total loss: 5.030 | Reg loss: 0.031 | Tree loss: 5.030 | Accuracy: 0.095703 | 0.058 sec/iter\n",
      "Epoch: 54 | Batch: 008 / 016 | Total loss: 5.051 | Reg loss: 0.031 | Tree loss: 5.051 | Accuracy: 0.113281 | 0.058 sec/iter\n",
      "Epoch: 54 | Batch: 009 / 016 | Total loss: 5.069 | Reg loss: 0.031 | Tree loss: 5.069 | Accuracy: 0.099609 | 0.058 sec/iter\n",
      "Epoch: 54 | Batch: 010 / 016 | Total loss: 4.966 | Reg loss: 0.031 | Tree loss: 4.966 | Accuracy: 0.089844 | 0.058 sec/iter\n",
      "Epoch: 54 | Batch: 011 / 016 | Total loss: 4.974 | Reg loss: 0.031 | Tree loss: 4.974 | Accuracy: 0.113281 | 0.058 sec/iter\n",
      "Epoch: 54 | Batch: 012 / 016 | Total loss: 4.995 | Reg loss: 0.031 | Tree loss: 4.995 | Accuracy: 0.085938 | 0.058 sec/iter\n",
      "Epoch: 54 | Batch: 013 / 016 | Total loss: 4.830 | Reg loss: 0.031 | Tree loss: 4.830 | Accuracy: 0.121094 | 0.058 sec/iter\n",
      "Epoch: 54 | Batch: 014 / 016 | Total loss: 4.856 | Reg loss: 0.031 | Tree loss: 4.856 | Accuracy: 0.109375 | 0.058 sec/iter\n",
      "Epoch: 54 | Batch: 015 / 016 | Total loss: 4.883 | Reg loss: 0.031 | Tree loss: 4.883 | Accuracy: 0.096154 | 0.058 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 55 | Batch: 000 / 016 | Total loss: 5.115 | Reg loss: 0.031 | Tree loss: 5.115 | Accuracy: 0.105469 | 0.058 sec/iter\n",
      "Epoch: 55 | Batch: 001 / 016 | Total loss: 5.050 | Reg loss: 0.031 | Tree loss: 5.050 | Accuracy: 0.123047 | 0.058 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 55 | Batch: 002 / 016 | Total loss: 5.071 | Reg loss: 0.031 | Tree loss: 5.071 | Accuracy: 0.101562 | 0.058 sec/iter\n",
      "Epoch: 55 | Batch: 003 / 016 | Total loss: 5.047 | Reg loss: 0.031 | Tree loss: 5.047 | Accuracy: 0.105469 | 0.058 sec/iter\n",
      "Epoch: 55 | Batch: 004 / 016 | Total loss: 5.003 | Reg loss: 0.031 | Tree loss: 5.003 | Accuracy: 0.123047 | 0.058 sec/iter\n",
      "Epoch: 55 | Batch: 005 / 016 | Total loss: 4.946 | Reg loss: 0.031 | Tree loss: 4.946 | Accuracy: 0.111328 | 0.058 sec/iter\n",
      "Epoch: 55 | Batch: 006 / 016 | Total loss: 5.013 | Reg loss: 0.031 | Tree loss: 5.013 | Accuracy: 0.103516 | 0.058 sec/iter\n",
      "Epoch: 55 | Batch: 007 / 016 | Total loss: 4.903 | Reg loss: 0.031 | Tree loss: 4.903 | Accuracy: 0.130859 | 0.058 sec/iter\n",
      "Epoch: 55 | Batch: 008 / 016 | Total loss: 4.968 | Reg loss: 0.031 | Tree loss: 4.968 | Accuracy: 0.083984 | 0.058 sec/iter\n",
      "Epoch: 55 | Batch: 009 / 016 | Total loss: 4.997 | Reg loss: 0.031 | Tree loss: 4.997 | Accuracy: 0.080078 | 0.058 sec/iter\n",
      "Epoch: 55 | Batch: 010 / 016 | Total loss: 4.929 | Reg loss: 0.031 | Tree loss: 4.929 | Accuracy: 0.103516 | 0.058 sec/iter\n",
      "Epoch: 55 | Batch: 011 / 016 | Total loss: 4.948 | Reg loss: 0.031 | Tree loss: 4.948 | Accuracy: 0.107422 | 0.058 sec/iter\n",
      "Epoch: 55 | Batch: 012 / 016 | Total loss: 4.962 | Reg loss: 0.031 | Tree loss: 4.962 | Accuracy: 0.085938 | 0.058 sec/iter\n",
      "Epoch: 55 | Batch: 013 / 016 | Total loss: 4.832 | Reg loss: 0.031 | Tree loss: 4.832 | Accuracy: 0.132812 | 0.057 sec/iter\n",
      "Epoch: 55 | Batch: 014 / 016 | Total loss: 4.895 | Reg loss: 0.031 | Tree loss: 4.895 | Accuracy: 0.095703 | 0.057 sec/iter\n",
      "Epoch: 55 | Batch: 015 / 016 | Total loss: 4.793 | Reg loss: 0.031 | Tree loss: 4.793 | Accuracy: 0.102564 | 0.057 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 56 | Batch: 000 / 016 | Total loss: 5.057 | Reg loss: 0.031 | Tree loss: 5.057 | Accuracy: 0.125000 | 0.057 sec/iter\n",
      "Epoch: 56 | Batch: 001 / 016 | Total loss: 5.021 | Reg loss: 0.031 | Tree loss: 5.021 | Accuracy: 0.105469 | 0.057 sec/iter\n",
      "Epoch: 56 | Batch: 002 / 016 | Total loss: 4.984 | Reg loss: 0.031 | Tree loss: 4.984 | Accuracy: 0.103516 | 0.057 sec/iter\n",
      "Epoch: 56 | Batch: 003 / 016 | Total loss: 4.962 | Reg loss: 0.031 | Tree loss: 4.962 | Accuracy: 0.132812 | 0.057 sec/iter\n",
      "Epoch: 56 | Batch: 004 / 016 | Total loss: 4.968 | Reg loss: 0.031 | Tree loss: 4.968 | Accuracy: 0.101562 | 0.057 sec/iter\n",
      "Epoch: 56 | Batch: 005 / 016 | Total loss: 4.945 | Reg loss: 0.031 | Tree loss: 4.945 | Accuracy: 0.115234 | 0.057 sec/iter\n",
      "Epoch: 56 | Batch: 006 / 016 | Total loss: 4.910 | Reg loss: 0.031 | Tree loss: 4.910 | Accuracy: 0.103516 | 0.057 sec/iter\n",
      "Epoch: 56 | Batch: 007 / 016 | Total loss: 4.938 | Reg loss: 0.031 | Tree loss: 4.938 | Accuracy: 0.128906 | 0.057 sec/iter\n",
      "Epoch: 56 | Batch: 008 / 016 | Total loss: 4.927 | Reg loss: 0.031 | Tree loss: 4.927 | Accuracy: 0.093750 | 0.057 sec/iter\n",
      "Epoch: 56 | Batch: 009 / 016 | Total loss: 4.992 | Reg loss: 0.031 | Tree loss: 4.992 | Accuracy: 0.078125 | 0.057 sec/iter\n",
      "Epoch: 56 | Batch: 010 / 016 | Total loss: 4.887 | Reg loss: 0.031 | Tree loss: 4.887 | Accuracy: 0.123047 | 0.057 sec/iter\n",
      "Epoch: 56 | Batch: 011 / 016 | Total loss: 4.862 | Reg loss: 0.031 | Tree loss: 4.862 | Accuracy: 0.105469 | 0.057 sec/iter\n",
      "Epoch: 56 | Batch: 012 / 016 | Total loss: 4.886 | Reg loss: 0.031 | Tree loss: 4.886 | Accuracy: 0.097656 | 0.057 sec/iter\n",
      "Epoch: 56 | Batch: 013 / 016 | Total loss: 4.809 | Reg loss: 0.031 | Tree loss: 4.809 | Accuracy: 0.115234 | 0.057 sec/iter\n",
      "Epoch: 56 | Batch: 014 / 016 | Total loss: 4.849 | Reg loss: 0.031 | Tree loss: 4.849 | Accuracy: 0.078125 | 0.057 sec/iter\n",
      "Epoch: 56 | Batch: 015 / 016 | Total loss: 4.805 | Reg loss: 0.031 | Tree loss: 4.805 | Accuracy: 0.083333 | 0.057 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 57 | Batch: 000 / 016 | Total loss: 5.020 | Reg loss: 0.031 | Tree loss: 5.020 | Accuracy: 0.099609 | 0.057 sec/iter\n",
      "Epoch: 57 | Batch: 001 / 016 | Total loss: 4.968 | Reg loss: 0.031 | Tree loss: 4.968 | Accuracy: 0.082031 | 0.057 sec/iter\n",
      "Epoch: 57 | Batch: 002 / 016 | Total loss: 4.982 | Reg loss: 0.031 | Tree loss: 4.982 | Accuracy: 0.109375 | 0.057 sec/iter\n",
      "Epoch: 57 | Batch: 003 / 016 | Total loss: 4.974 | Reg loss: 0.031 | Tree loss: 4.974 | Accuracy: 0.099609 | 0.057 sec/iter\n",
      "Epoch: 57 | Batch: 004 / 016 | Total loss: 4.911 | Reg loss: 0.031 | Tree loss: 4.911 | Accuracy: 0.126953 | 0.057 sec/iter\n",
      "Epoch: 57 | Batch: 005 / 016 | Total loss: 4.871 | Reg loss: 0.031 | Tree loss: 4.871 | Accuracy: 0.123047 | 0.057 sec/iter\n",
      "Epoch: 57 | Batch: 006 / 016 | Total loss: 4.962 | Reg loss: 0.031 | Tree loss: 4.962 | Accuracy: 0.107422 | 0.057 sec/iter\n",
      "Epoch: 57 | Batch: 007 / 016 | Total loss: 4.951 | Reg loss: 0.031 | Tree loss: 4.951 | Accuracy: 0.093750 | 0.057 sec/iter\n",
      "Epoch: 57 | Batch: 008 / 016 | Total loss: 4.906 | Reg loss: 0.031 | Tree loss: 4.906 | Accuracy: 0.093750 | 0.057 sec/iter\n",
      "Epoch: 57 | Batch: 009 / 016 | Total loss: 4.759 | Reg loss: 0.031 | Tree loss: 4.759 | Accuracy: 0.123047 | 0.057 sec/iter\n",
      "Epoch: 57 | Batch: 010 / 016 | Total loss: 4.855 | Reg loss: 0.031 | Tree loss: 4.855 | Accuracy: 0.101562 | 0.057 sec/iter\n",
      "Epoch: 57 | Batch: 011 / 016 | Total loss: 4.841 | Reg loss: 0.031 | Tree loss: 4.841 | Accuracy: 0.109375 | 0.057 sec/iter\n",
      "Epoch: 57 | Batch: 012 / 016 | Total loss: 4.827 | Reg loss: 0.031 | Tree loss: 4.827 | Accuracy: 0.117188 | 0.057 sec/iter\n",
      "Epoch: 57 | Batch: 013 / 016 | Total loss: 4.810 | Reg loss: 0.031 | Tree loss: 4.810 | Accuracy: 0.105469 | 0.057 sec/iter\n",
      "Epoch: 57 | Batch: 014 / 016 | Total loss: 4.788 | Reg loss: 0.031 | Tree loss: 4.788 | Accuracy: 0.093750 | 0.057 sec/iter\n",
      "Epoch: 57 | Batch: 015 / 016 | Total loss: 4.738 | Reg loss: 0.031 | Tree loss: 4.738 | Accuracy: 0.108974 | 0.057 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 58 | Batch: 000 / 016 | Total loss: 4.952 | Reg loss: 0.031 | Tree loss: 4.952 | Accuracy: 0.115234 | 0.057 sec/iter\n",
      "Epoch: 58 | Batch: 001 / 016 | Total loss: 4.917 | Reg loss: 0.031 | Tree loss: 4.917 | Accuracy: 0.099609 | 0.058 sec/iter\n",
      "Epoch: 58 | Batch: 002 / 016 | Total loss: 5.010 | Reg loss: 0.031 | Tree loss: 5.010 | Accuracy: 0.097656 | 0.058 sec/iter\n",
      "Epoch: 58 | Batch: 003 / 016 | Total loss: 4.939 | Reg loss: 0.031 | Tree loss: 4.939 | Accuracy: 0.093750 | 0.058 sec/iter\n",
      "Epoch: 58 | Batch: 004 / 016 | Total loss: 4.925 | Reg loss: 0.031 | Tree loss: 4.925 | Accuracy: 0.103516 | 0.058 sec/iter\n",
      "Epoch: 58 | Batch: 005 / 016 | Total loss: 4.870 | Reg loss: 0.031 | Tree loss: 4.870 | Accuracy: 0.093750 | 0.058 sec/iter\n",
      "Epoch: 58 | Batch: 006 / 016 | Total loss: 4.819 | Reg loss: 0.031 | Tree loss: 4.819 | Accuracy: 0.101562 | 0.058 sec/iter\n",
      "Epoch: 58 | Batch: 007 / 016 | Total loss: 4.881 | Reg loss: 0.031 | Tree loss: 4.881 | Accuracy: 0.109375 | 0.058 sec/iter\n",
      "Epoch: 58 | Batch: 008 / 016 | Total loss: 4.786 | Reg loss: 0.031 | Tree loss: 4.786 | Accuracy: 0.097656 | 0.057 sec/iter\n",
      "Epoch: 58 | Batch: 009 / 016 | Total loss: 4.798 | Reg loss: 0.031 | Tree loss: 4.798 | Accuracy: 0.099609 | 0.057 sec/iter\n",
      "Epoch: 58 | Batch: 010 / 016 | Total loss: 4.863 | Reg loss: 0.031 | Tree loss: 4.863 | Accuracy: 0.113281 | 0.057 sec/iter\n",
      "Epoch: 58 | Batch: 011 / 016 | Total loss: 4.857 | Reg loss: 0.031 | Tree loss: 4.857 | Accuracy: 0.111328 | 0.057 sec/iter\n",
      "Epoch: 58 | Batch: 012 / 016 | Total loss: 4.746 | Reg loss: 0.031 | Tree loss: 4.746 | Accuracy: 0.134766 | 0.057 sec/iter\n",
      "Epoch: 58 | Batch: 013 / 016 | Total loss: 4.760 | Reg loss: 0.031 | Tree loss: 4.760 | Accuracy: 0.119141 | 0.057 sec/iter\n",
      "Epoch: 58 | Batch: 014 / 016 | Total loss: 4.716 | Reg loss: 0.031 | Tree loss: 4.716 | Accuracy: 0.115234 | 0.057 sec/iter\n",
      "Epoch: 58 | Batch: 015 / 016 | Total loss: 4.710 | Reg loss: 0.031 | Tree loss: 4.710 | Accuracy: 0.098291 | 0.057 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 59 | Batch: 000 / 016 | Total loss: 4.921 | Reg loss: 0.031 | Tree loss: 4.921 | Accuracy: 0.113281 | 0.057 sec/iter\n",
      "Epoch: 59 | Batch: 001 / 016 | Total loss: 4.848 | Reg loss: 0.031 | Tree loss: 4.848 | Accuracy: 0.130859 | 0.057 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 59 | Batch: 002 / 016 | Total loss: 4.935 | Reg loss: 0.031 | Tree loss: 4.935 | Accuracy: 0.107422 | 0.057 sec/iter\n",
      "Epoch: 59 | Batch: 003 / 016 | Total loss: 4.820 | Reg loss: 0.031 | Tree loss: 4.820 | Accuracy: 0.105469 | 0.057 sec/iter\n",
      "Epoch: 59 | Batch: 004 / 016 | Total loss: 4.859 | Reg loss: 0.031 | Tree loss: 4.859 | Accuracy: 0.119141 | 0.057 sec/iter\n",
      "Epoch: 59 | Batch: 005 / 016 | Total loss: 4.809 | Reg loss: 0.031 | Tree loss: 4.809 | Accuracy: 0.111328 | 0.057 sec/iter\n",
      "Epoch: 59 | Batch: 006 / 016 | Total loss: 4.741 | Reg loss: 0.031 | Tree loss: 4.741 | Accuracy: 0.121094 | 0.057 sec/iter\n",
      "Epoch: 59 | Batch: 007 / 016 | Total loss: 4.801 | Reg loss: 0.031 | Tree loss: 4.801 | Accuracy: 0.103516 | 0.057 sec/iter\n",
      "Epoch: 59 | Batch: 008 / 016 | Total loss: 4.782 | Reg loss: 0.031 | Tree loss: 4.782 | Accuracy: 0.095703 | 0.057 sec/iter\n",
      "Epoch: 59 | Batch: 009 / 016 | Total loss: 4.793 | Reg loss: 0.031 | Tree loss: 4.793 | Accuracy: 0.109375 | 0.057 sec/iter\n",
      "Epoch: 59 | Batch: 010 / 016 | Total loss: 4.775 | Reg loss: 0.031 | Tree loss: 4.775 | Accuracy: 0.115234 | 0.057 sec/iter\n",
      "Epoch: 59 | Batch: 011 / 016 | Total loss: 4.785 | Reg loss: 0.031 | Tree loss: 4.785 | Accuracy: 0.089844 | 0.057 sec/iter\n",
      "Epoch: 59 | Batch: 012 / 016 | Total loss: 4.847 | Reg loss: 0.031 | Tree loss: 4.847 | Accuracy: 0.080078 | 0.057 sec/iter\n",
      "Epoch: 59 | Batch: 013 / 016 | Total loss: 4.727 | Reg loss: 0.031 | Tree loss: 4.727 | Accuracy: 0.089844 | 0.057 sec/iter\n",
      "Epoch: 59 | Batch: 014 / 016 | Total loss: 4.759 | Reg loss: 0.031 | Tree loss: 4.759 | Accuracy: 0.095703 | 0.057 sec/iter\n",
      "Epoch: 59 | Batch: 015 / 016 | Total loss: 4.756 | Reg loss: 0.031 | Tree loss: 4.756 | Accuracy: 0.113248 | 0.057 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 60 | Batch: 000 / 016 | Total loss: 4.870 | Reg loss: 0.031 | Tree loss: 4.870 | Accuracy: 0.134766 | 0.057 sec/iter\n",
      "Epoch: 60 | Batch: 001 / 016 | Total loss: 4.916 | Reg loss: 0.031 | Tree loss: 4.916 | Accuracy: 0.091797 | 0.057 sec/iter\n",
      "Epoch: 60 | Batch: 002 / 016 | Total loss: 4.815 | Reg loss: 0.031 | Tree loss: 4.815 | Accuracy: 0.111328 | 0.057 sec/iter\n",
      "Epoch: 60 | Batch: 003 / 016 | Total loss: 4.843 | Reg loss: 0.031 | Tree loss: 4.843 | Accuracy: 0.105469 | 0.057 sec/iter\n",
      "Epoch: 60 | Batch: 004 / 016 | Total loss: 4.806 | Reg loss: 0.031 | Tree loss: 4.806 | Accuracy: 0.128906 | 0.057 sec/iter\n",
      "Epoch: 60 | Batch: 005 / 016 | Total loss: 4.814 | Reg loss: 0.031 | Tree loss: 4.814 | Accuracy: 0.113281 | 0.057 sec/iter\n",
      "Epoch: 60 | Batch: 006 / 016 | Total loss: 4.774 | Reg loss: 0.031 | Tree loss: 4.774 | Accuracy: 0.091797 | 0.057 sec/iter\n",
      "Epoch: 60 | Batch: 007 / 016 | Total loss: 4.743 | Reg loss: 0.031 | Tree loss: 4.743 | Accuracy: 0.113281 | 0.057 sec/iter\n",
      "Epoch: 60 | Batch: 008 / 016 | Total loss: 4.834 | Reg loss: 0.031 | Tree loss: 4.834 | Accuracy: 0.080078 | 0.057 sec/iter\n",
      "Epoch: 60 | Batch: 009 / 016 | Total loss: 4.713 | Reg loss: 0.031 | Tree loss: 4.713 | Accuracy: 0.109375 | 0.057 sec/iter\n",
      "Epoch: 60 | Batch: 010 / 016 | Total loss: 4.748 | Reg loss: 0.031 | Tree loss: 4.748 | Accuracy: 0.099609 | 0.057 sec/iter\n",
      "Epoch: 60 | Batch: 011 / 016 | Total loss: 4.761 | Reg loss: 0.031 | Tree loss: 4.761 | Accuracy: 0.089844 | 0.057 sec/iter\n",
      "Epoch: 60 | Batch: 012 / 016 | Total loss: 4.663 | Reg loss: 0.031 | Tree loss: 4.663 | Accuracy: 0.111328 | 0.057 sec/iter\n",
      "Epoch: 60 | Batch: 013 / 016 | Total loss: 4.713 | Reg loss: 0.031 | Tree loss: 4.713 | Accuracy: 0.103516 | 0.057 sec/iter\n",
      "Epoch: 60 | Batch: 014 / 016 | Total loss: 4.693 | Reg loss: 0.031 | Tree loss: 4.693 | Accuracy: 0.091797 | 0.057 sec/iter\n",
      "Epoch: 60 | Batch: 015 / 016 | Total loss: 4.675 | Reg loss: 0.031 | Tree loss: 4.675 | Accuracy: 0.123932 | 0.057 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 61 | Batch: 000 / 016 | Total loss: 4.856 | Reg loss: 0.031 | Tree loss: 4.856 | Accuracy: 0.099609 | 0.057 sec/iter\n",
      "Epoch: 61 | Batch: 001 / 016 | Total loss: 4.876 | Reg loss: 0.031 | Tree loss: 4.876 | Accuracy: 0.093750 | 0.057 sec/iter\n",
      "Epoch: 61 | Batch: 002 / 016 | Total loss: 4.813 | Reg loss: 0.031 | Tree loss: 4.813 | Accuracy: 0.115234 | 0.057 sec/iter\n",
      "Epoch: 61 | Batch: 003 / 016 | Total loss: 4.819 | Reg loss: 0.031 | Tree loss: 4.819 | Accuracy: 0.101562 | 0.057 sec/iter\n",
      "Epoch: 61 | Batch: 004 / 016 | Total loss: 4.765 | Reg loss: 0.031 | Tree loss: 4.765 | Accuracy: 0.125000 | 0.057 sec/iter\n",
      "Epoch: 61 | Batch: 005 / 016 | Total loss: 4.777 | Reg loss: 0.031 | Tree loss: 4.777 | Accuracy: 0.115234 | 0.057 sec/iter\n",
      "Epoch: 61 | Batch: 006 / 016 | Total loss: 4.754 | Reg loss: 0.031 | Tree loss: 4.754 | Accuracy: 0.107422 | 0.057 sec/iter\n",
      "Epoch: 61 | Batch: 007 / 016 | Total loss: 4.746 | Reg loss: 0.031 | Tree loss: 4.746 | Accuracy: 0.111328 | 0.057 sec/iter\n",
      "Epoch: 61 | Batch: 008 / 016 | Total loss: 4.711 | Reg loss: 0.031 | Tree loss: 4.711 | Accuracy: 0.123047 | 0.057 sec/iter\n",
      "Epoch: 61 | Batch: 009 / 016 | Total loss: 4.707 | Reg loss: 0.031 | Tree loss: 4.707 | Accuracy: 0.085938 | 0.057 sec/iter\n",
      "Epoch: 61 | Batch: 010 / 016 | Total loss: 4.728 | Reg loss: 0.031 | Tree loss: 4.728 | Accuracy: 0.103516 | 0.057 sec/iter\n",
      "Epoch: 61 | Batch: 011 / 016 | Total loss: 4.694 | Reg loss: 0.031 | Tree loss: 4.694 | Accuracy: 0.115234 | 0.057 sec/iter\n",
      "Epoch: 61 | Batch: 012 / 016 | Total loss: 4.714 | Reg loss: 0.031 | Tree loss: 4.714 | Accuracy: 0.101562 | 0.057 sec/iter\n",
      "Epoch: 61 | Batch: 013 / 016 | Total loss: 4.646 | Reg loss: 0.031 | Tree loss: 4.646 | Accuracy: 0.103516 | 0.057 sec/iter\n",
      "Epoch: 61 | Batch: 014 / 016 | Total loss: 4.596 | Reg loss: 0.031 | Tree loss: 4.596 | Accuracy: 0.091797 | 0.057 sec/iter\n",
      "Epoch: 61 | Batch: 015 / 016 | Total loss: 4.643 | Reg loss: 0.031 | Tree loss: 4.643 | Accuracy: 0.100427 | 0.057 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 62 | Batch: 000 / 016 | Total loss: 4.881 | Reg loss: 0.031 | Tree loss: 4.881 | Accuracy: 0.097656 | 0.057 sec/iter\n",
      "Epoch: 62 | Batch: 001 / 016 | Total loss: 4.846 | Reg loss: 0.031 | Tree loss: 4.846 | Accuracy: 0.089844 | 0.057 sec/iter\n",
      "Epoch: 62 | Batch: 002 / 016 | Total loss: 4.774 | Reg loss: 0.031 | Tree loss: 4.774 | Accuracy: 0.111328 | 0.057 sec/iter\n",
      "Epoch: 62 | Batch: 003 / 016 | Total loss: 4.717 | Reg loss: 0.031 | Tree loss: 4.717 | Accuracy: 0.111328 | 0.057 sec/iter\n",
      "Epoch: 62 | Batch: 004 / 016 | Total loss: 4.796 | Reg loss: 0.031 | Tree loss: 4.796 | Accuracy: 0.093750 | 0.057 sec/iter\n",
      "Epoch: 62 | Batch: 005 / 016 | Total loss: 4.710 | Reg loss: 0.031 | Tree loss: 4.710 | Accuracy: 0.117188 | 0.057 sec/iter\n",
      "Epoch: 62 | Batch: 006 / 016 | Total loss: 4.821 | Reg loss: 0.031 | Tree loss: 4.821 | Accuracy: 0.080078 | 0.057 sec/iter\n",
      "Epoch: 62 | Batch: 007 / 016 | Total loss: 4.679 | Reg loss: 0.031 | Tree loss: 4.679 | Accuracy: 0.111328 | 0.057 sec/iter\n",
      "Epoch: 62 | Batch: 008 / 016 | Total loss: 4.673 | Reg loss: 0.031 | Tree loss: 4.673 | Accuracy: 0.117188 | 0.057 sec/iter\n",
      "Epoch: 62 | Batch: 009 / 016 | Total loss: 4.687 | Reg loss: 0.031 | Tree loss: 4.687 | Accuracy: 0.111328 | 0.057 sec/iter\n",
      "Epoch: 62 | Batch: 010 / 016 | Total loss: 4.712 | Reg loss: 0.031 | Tree loss: 4.712 | Accuracy: 0.068359 | 0.057 sec/iter\n",
      "Epoch: 62 | Batch: 011 / 016 | Total loss: 4.540 | Reg loss: 0.031 | Tree loss: 4.540 | Accuracy: 0.150391 | 0.057 sec/iter\n",
      "Epoch: 62 | Batch: 012 / 016 | Total loss: 4.630 | Reg loss: 0.031 | Tree loss: 4.630 | Accuracy: 0.101562 | 0.057 sec/iter\n",
      "Epoch: 62 | Batch: 013 / 016 | Total loss: 4.634 | Reg loss: 0.031 | Tree loss: 4.634 | Accuracy: 0.103516 | 0.057 sec/iter\n",
      "Epoch: 62 | Batch: 014 / 016 | Total loss: 4.626 | Reg loss: 0.031 | Tree loss: 4.626 | Accuracy: 0.132812 | 0.057 sec/iter\n",
      "Epoch: 62 | Batch: 015 / 016 | Total loss: 4.599 | Reg loss: 0.032 | Tree loss: 4.599 | Accuracy: 0.104701 | 0.057 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 63 | Batch: 000 / 016 | Total loss: 4.784 | Reg loss: 0.031 | Tree loss: 4.784 | Accuracy: 0.087891 | 0.057 sec/iter\n",
      "Epoch: 63 | Batch: 001 / 016 | Total loss: 4.808 | Reg loss: 0.031 | Tree loss: 4.808 | Accuracy: 0.113281 | 0.057 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 63 | Batch: 002 / 016 | Total loss: 4.735 | Reg loss: 0.031 | Tree loss: 4.735 | Accuracy: 0.103516 | 0.057 sec/iter\n",
      "Epoch: 63 | Batch: 003 / 016 | Total loss: 4.726 | Reg loss: 0.031 | Tree loss: 4.726 | Accuracy: 0.123047 | 0.057 sec/iter\n",
      "Epoch: 63 | Batch: 004 / 016 | Total loss: 4.737 | Reg loss: 0.031 | Tree loss: 4.737 | Accuracy: 0.119141 | 0.057 sec/iter\n",
      "Epoch: 63 | Batch: 005 / 016 | Total loss: 4.674 | Reg loss: 0.031 | Tree loss: 4.674 | Accuracy: 0.113281 | 0.057 sec/iter\n",
      "Epoch: 63 | Batch: 006 / 016 | Total loss: 4.660 | Reg loss: 0.031 | Tree loss: 4.660 | Accuracy: 0.093750 | 0.057 sec/iter\n",
      "Epoch: 63 | Batch: 007 / 016 | Total loss: 4.716 | Reg loss: 0.031 | Tree loss: 4.716 | Accuracy: 0.083984 | 0.057 sec/iter\n",
      "Epoch: 63 | Batch: 008 / 016 | Total loss: 4.619 | Reg loss: 0.031 | Tree loss: 4.619 | Accuracy: 0.097656 | 0.057 sec/iter\n",
      "Epoch: 63 | Batch: 009 / 016 | Total loss: 4.630 | Reg loss: 0.031 | Tree loss: 4.630 | Accuracy: 0.123047 | 0.057 sec/iter\n",
      "Epoch: 63 | Batch: 010 / 016 | Total loss: 4.654 | Reg loss: 0.031 | Tree loss: 4.654 | Accuracy: 0.111328 | 0.057 sec/iter\n",
      "Epoch: 63 | Batch: 011 / 016 | Total loss: 4.690 | Reg loss: 0.031 | Tree loss: 4.690 | Accuracy: 0.105469 | 0.057 sec/iter\n",
      "Epoch: 63 | Batch: 012 / 016 | Total loss: 4.645 | Reg loss: 0.031 | Tree loss: 4.645 | Accuracy: 0.099609 | 0.057 sec/iter\n",
      "Epoch: 63 | Batch: 013 / 016 | Total loss: 4.586 | Reg loss: 0.032 | Tree loss: 4.586 | Accuracy: 0.113281 | 0.057 sec/iter\n",
      "Epoch: 63 | Batch: 014 / 016 | Total loss: 4.600 | Reg loss: 0.032 | Tree loss: 4.600 | Accuracy: 0.107422 | 0.057 sec/iter\n",
      "Epoch: 63 | Batch: 015 / 016 | Total loss: 4.561 | Reg loss: 0.032 | Tree loss: 4.561 | Accuracy: 0.106838 | 0.057 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 64 | Batch: 000 / 016 | Total loss: 4.708 | Reg loss: 0.031 | Tree loss: 4.708 | Accuracy: 0.125000 | 0.057 sec/iter\n",
      "Epoch: 64 | Batch: 001 / 016 | Total loss: 4.766 | Reg loss: 0.031 | Tree loss: 4.766 | Accuracy: 0.107422 | 0.057 sec/iter\n",
      "Epoch: 64 | Batch: 002 / 016 | Total loss: 4.789 | Reg loss: 0.031 | Tree loss: 4.789 | Accuracy: 0.101562 | 0.057 sec/iter\n",
      "Epoch: 64 | Batch: 003 / 016 | Total loss: 4.709 | Reg loss: 0.031 | Tree loss: 4.709 | Accuracy: 0.105469 | 0.057 sec/iter\n",
      "Epoch: 64 | Batch: 004 / 016 | Total loss: 4.749 | Reg loss: 0.031 | Tree loss: 4.749 | Accuracy: 0.078125 | 0.057 sec/iter\n",
      "Epoch: 64 | Batch: 005 / 016 | Total loss: 4.629 | Reg loss: 0.031 | Tree loss: 4.629 | Accuracy: 0.109375 | 0.057 sec/iter\n",
      "Epoch: 64 | Batch: 006 / 016 | Total loss: 4.655 | Reg loss: 0.031 | Tree loss: 4.655 | Accuracy: 0.117188 | 0.057 sec/iter\n",
      "Epoch: 64 | Batch: 007 / 016 | Total loss: 4.700 | Reg loss: 0.031 | Tree loss: 4.700 | Accuracy: 0.097656 | 0.057 sec/iter\n",
      "Epoch: 64 | Batch: 008 / 016 | Total loss: 4.621 | Reg loss: 0.031 | Tree loss: 4.621 | Accuracy: 0.113281 | 0.057 sec/iter\n",
      "Epoch: 64 | Batch: 009 / 016 | Total loss: 4.583 | Reg loss: 0.031 | Tree loss: 4.583 | Accuracy: 0.103516 | 0.057 sec/iter\n",
      "Epoch: 64 | Batch: 010 / 016 | Total loss: 4.576 | Reg loss: 0.031 | Tree loss: 4.576 | Accuracy: 0.085938 | 0.057 sec/iter\n",
      "Epoch: 64 | Batch: 011 / 016 | Total loss: 4.585 | Reg loss: 0.031 | Tree loss: 4.585 | Accuracy: 0.107422 | 0.057 sec/iter\n",
      "Epoch: 64 | Batch: 012 / 016 | Total loss: 4.576 | Reg loss: 0.032 | Tree loss: 4.576 | Accuracy: 0.125000 | 0.057 sec/iter\n",
      "Epoch: 64 | Batch: 013 / 016 | Total loss: 4.549 | Reg loss: 0.032 | Tree loss: 4.549 | Accuracy: 0.117188 | 0.057 sec/iter\n",
      "Epoch: 64 | Batch: 014 / 016 | Total loss: 4.556 | Reg loss: 0.032 | Tree loss: 4.556 | Accuracy: 0.089844 | 0.057 sec/iter\n",
      "Epoch: 64 | Batch: 015 / 016 | Total loss: 4.597 | Reg loss: 0.032 | Tree loss: 4.597 | Accuracy: 0.115385 | 0.057 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 65 | Batch: 000 / 016 | Total loss: 4.752 | Reg loss: 0.031 | Tree loss: 4.752 | Accuracy: 0.089844 | 0.057 sec/iter\n",
      "Epoch: 65 | Batch: 001 / 016 | Total loss: 4.736 | Reg loss: 0.031 | Tree loss: 4.736 | Accuracy: 0.091797 | 0.057 sec/iter\n",
      "Epoch: 65 | Batch: 002 / 016 | Total loss: 4.646 | Reg loss: 0.031 | Tree loss: 4.646 | Accuracy: 0.107422 | 0.057 sec/iter\n",
      "Epoch: 65 | Batch: 003 / 016 | Total loss: 4.625 | Reg loss: 0.031 | Tree loss: 4.625 | Accuracy: 0.138672 | 0.057 sec/iter\n",
      "Epoch: 65 | Batch: 004 / 016 | Total loss: 4.640 | Reg loss: 0.031 | Tree loss: 4.640 | Accuracy: 0.111328 | 0.057 sec/iter\n",
      "Epoch: 65 | Batch: 005 / 016 | Total loss: 4.695 | Reg loss: 0.031 | Tree loss: 4.695 | Accuracy: 0.103516 | 0.057 sec/iter\n",
      "Epoch: 65 | Batch: 006 / 016 | Total loss: 4.682 | Reg loss: 0.031 | Tree loss: 4.682 | Accuracy: 0.095703 | 0.057 sec/iter\n",
      "Epoch: 65 | Batch: 007 / 016 | Total loss: 4.578 | Reg loss: 0.031 | Tree loss: 4.578 | Accuracy: 0.119141 | 0.057 sec/iter\n",
      "Epoch: 65 | Batch: 008 / 016 | Total loss: 4.626 | Reg loss: 0.031 | Tree loss: 4.626 | Accuracy: 0.099609 | 0.057 sec/iter\n",
      "Epoch: 65 | Batch: 009 / 016 | Total loss: 4.619 | Reg loss: 0.031 | Tree loss: 4.619 | Accuracy: 0.101562 | 0.057 sec/iter\n",
      "Epoch: 65 | Batch: 010 / 016 | Total loss: 4.586 | Reg loss: 0.032 | Tree loss: 4.586 | Accuracy: 0.113281 | 0.057 sec/iter\n",
      "Epoch: 65 | Batch: 011 / 016 | Total loss: 4.540 | Reg loss: 0.032 | Tree loss: 4.540 | Accuracy: 0.126953 | 0.057 sec/iter\n",
      "Epoch: 65 | Batch: 012 / 016 | Total loss: 4.500 | Reg loss: 0.032 | Tree loss: 4.500 | Accuracy: 0.121094 | 0.057 sec/iter\n",
      "Epoch: 65 | Batch: 013 / 016 | Total loss: 4.587 | Reg loss: 0.032 | Tree loss: 4.587 | Accuracy: 0.076172 | 0.057 sec/iter\n",
      "Epoch: 65 | Batch: 014 / 016 | Total loss: 4.550 | Reg loss: 0.032 | Tree loss: 4.550 | Accuracy: 0.101562 | 0.057 sec/iter\n",
      "Epoch: 65 | Batch: 015 / 016 | Total loss: 4.513 | Reg loss: 0.032 | Tree loss: 4.513 | Accuracy: 0.108974 | 0.057 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 66 | Batch: 000 / 016 | Total loss: 4.680 | Reg loss: 0.031 | Tree loss: 4.680 | Accuracy: 0.119141 | 0.057 sec/iter\n",
      "Epoch: 66 | Batch: 001 / 016 | Total loss: 4.725 | Reg loss: 0.031 | Tree loss: 4.725 | Accuracy: 0.099609 | 0.057 sec/iter\n",
      "Epoch: 66 | Batch: 002 / 016 | Total loss: 4.652 | Reg loss: 0.031 | Tree loss: 4.652 | Accuracy: 0.103516 | 0.057 sec/iter\n",
      "Epoch: 66 | Batch: 003 / 016 | Total loss: 4.726 | Reg loss: 0.031 | Tree loss: 4.726 | Accuracy: 0.109375 | 0.057 sec/iter\n",
      "Epoch: 66 | Batch: 004 / 016 | Total loss: 4.639 | Reg loss: 0.031 | Tree loss: 4.639 | Accuracy: 0.123047 | 0.057 sec/iter\n",
      "Epoch: 66 | Batch: 005 / 016 | Total loss: 4.558 | Reg loss: 0.031 | Tree loss: 4.558 | Accuracy: 0.121094 | 0.057 sec/iter\n",
      "Epoch: 66 | Batch: 006 / 016 | Total loss: 4.623 | Reg loss: 0.031 | Tree loss: 4.623 | Accuracy: 0.089844 | 0.057 sec/iter\n",
      "Epoch: 66 | Batch: 007 / 016 | Total loss: 4.617 | Reg loss: 0.031 | Tree loss: 4.617 | Accuracy: 0.093750 | 0.057 sec/iter\n",
      "Epoch: 66 | Batch: 008 / 016 | Total loss: 4.558 | Reg loss: 0.032 | Tree loss: 4.558 | Accuracy: 0.105469 | 0.057 sec/iter\n",
      "Epoch: 66 | Batch: 009 / 016 | Total loss: 4.648 | Reg loss: 0.032 | Tree loss: 4.648 | Accuracy: 0.089844 | 0.057 sec/iter\n",
      "Epoch: 66 | Batch: 010 / 016 | Total loss: 4.501 | Reg loss: 0.032 | Tree loss: 4.501 | Accuracy: 0.119141 | 0.057 sec/iter\n",
      "Epoch: 66 | Batch: 011 / 016 | Total loss: 4.491 | Reg loss: 0.032 | Tree loss: 4.491 | Accuracy: 0.105469 | 0.057 sec/iter\n",
      "Epoch: 66 | Batch: 012 / 016 | Total loss: 4.532 | Reg loss: 0.032 | Tree loss: 4.532 | Accuracy: 0.101562 | 0.057 sec/iter\n",
      "Epoch: 66 | Batch: 013 / 016 | Total loss: 4.485 | Reg loss: 0.032 | Tree loss: 4.485 | Accuracy: 0.125000 | 0.057 sec/iter\n",
      "Epoch: 66 | Batch: 014 / 016 | Total loss: 4.505 | Reg loss: 0.032 | Tree loss: 4.505 | Accuracy: 0.109375 | 0.057 sec/iter\n",
      "Epoch: 66 | Batch: 015 / 016 | Total loss: 4.506 | Reg loss: 0.032 | Tree loss: 4.506 | Accuracy: 0.096154 | 0.057 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 67 | Batch: 000 / 016 | Total loss: 4.687 | Reg loss: 0.031 | Tree loss: 4.687 | Accuracy: 0.083984 | 0.057 sec/iter\n",
      "Epoch: 67 | Batch: 001 / 016 | Total loss: 4.653 | Reg loss: 0.031 | Tree loss: 4.653 | Accuracy: 0.095703 | 0.057 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 67 | Batch: 002 / 016 | Total loss: 4.633 | Reg loss: 0.031 | Tree loss: 4.633 | Accuracy: 0.099609 | 0.057 sec/iter\n",
      "Epoch: 67 | Batch: 003 / 016 | Total loss: 4.657 | Reg loss: 0.031 | Tree loss: 4.657 | Accuracy: 0.109375 | 0.057 sec/iter\n",
      "Epoch: 67 | Batch: 004 / 016 | Total loss: 4.523 | Reg loss: 0.031 | Tree loss: 4.523 | Accuracy: 0.123047 | 0.057 sec/iter\n",
      "Epoch: 67 | Batch: 005 / 016 | Total loss: 4.596 | Reg loss: 0.031 | Tree loss: 4.596 | Accuracy: 0.101562 | 0.057 sec/iter\n",
      "Epoch: 67 | Batch: 006 / 016 | Total loss: 4.602 | Reg loss: 0.032 | Tree loss: 4.602 | Accuracy: 0.130859 | 0.057 sec/iter\n",
      "Epoch: 67 | Batch: 007 / 016 | Total loss: 4.633 | Reg loss: 0.032 | Tree loss: 4.633 | Accuracy: 0.115234 | 0.057 sec/iter\n",
      "Epoch: 67 | Batch: 008 / 016 | Total loss: 4.585 | Reg loss: 0.032 | Tree loss: 4.585 | Accuracy: 0.107422 | 0.057 sec/iter\n",
      "Epoch: 67 | Batch: 009 / 016 | Total loss: 4.563 | Reg loss: 0.032 | Tree loss: 4.563 | Accuracy: 0.082031 | 0.057 sec/iter\n",
      "Epoch: 67 | Batch: 010 / 016 | Total loss: 4.529 | Reg loss: 0.032 | Tree loss: 4.529 | Accuracy: 0.083984 | 0.057 sec/iter\n",
      "Epoch: 67 | Batch: 011 / 016 | Total loss: 4.492 | Reg loss: 0.032 | Tree loss: 4.492 | Accuracy: 0.099609 | 0.057 sec/iter\n",
      "Epoch: 67 | Batch: 012 / 016 | Total loss: 4.515 | Reg loss: 0.032 | Tree loss: 4.515 | Accuracy: 0.109375 | 0.057 sec/iter\n",
      "Epoch: 67 | Batch: 013 / 016 | Total loss: 4.470 | Reg loss: 0.032 | Tree loss: 4.470 | Accuracy: 0.121094 | 0.057 sec/iter\n",
      "Epoch: 67 | Batch: 014 / 016 | Total loss: 4.442 | Reg loss: 0.032 | Tree loss: 4.442 | Accuracy: 0.113281 | 0.057 sec/iter\n",
      "Epoch: 67 | Batch: 015 / 016 | Total loss: 4.457 | Reg loss: 0.032 | Tree loss: 4.457 | Accuracy: 0.130342 | 0.057 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 68 | Batch: 000 / 016 | Total loss: 4.671 | Reg loss: 0.031 | Tree loss: 4.671 | Accuracy: 0.111328 | 0.057 sec/iter\n",
      "Epoch: 68 | Batch: 001 / 016 | Total loss: 4.628 | Reg loss: 0.031 | Tree loss: 4.628 | Accuracy: 0.109375 | 0.057 sec/iter\n",
      "Epoch: 68 | Batch: 002 / 016 | Total loss: 4.737 | Reg loss: 0.031 | Tree loss: 4.737 | Accuracy: 0.089844 | 0.057 sec/iter\n",
      "Epoch: 68 | Batch: 003 / 016 | Total loss: 4.667 | Reg loss: 0.032 | Tree loss: 4.667 | Accuracy: 0.111328 | 0.057 sec/iter\n",
      "Epoch: 68 | Batch: 004 / 016 | Total loss: 4.541 | Reg loss: 0.032 | Tree loss: 4.541 | Accuracy: 0.103516 | 0.057 sec/iter\n",
      "Epoch: 68 | Batch: 005 / 016 | Total loss: 4.606 | Reg loss: 0.032 | Tree loss: 4.606 | Accuracy: 0.091797 | 0.057 sec/iter\n",
      "Epoch: 68 | Batch: 006 / 016 | Total loss: 4.527 | Reg loss: 0.032 | Tree loss: 4.527 | Accuracy: 0.134766 | 0.057 sec/iter\n",
      "Epoch: 68 | Batch: 007 / 016 | Total loss: 4.540 | Reg loss: 0.032 | Tree loss: 4.540 | Accuracy: 0.099609 | 0.057 sec/iter\n",
      "Epoch: 68 | Batch: 008 / 016 | Total loss: 4.471 | Reg loss: 0.032 | Tree loss: 4.471 | Accuracy: 0.109375 | 0.057 sec/iter\n",
      "Epoch: 68 | Batch: 009 / 016 | Total loss: 4.522 | Reg loss: 0.032 | Tree loss: 4.522 | Accuracy: 0.121094 | 0.057 sec/iter\n",
      "Epoch: 68 | Batch: 010 / 016 | Total loss: 4.510 | Reg loss: 0.032 | Tree loss: 4.510 | Accuracy: 0.095703 | 0.057 sec/iter\n",
      "Epoch: 68 | Batch: 011 / 016 | Total loss: 4.473 | Reg loss: 0.032 | Tree loss: 4.473 | Accuracy: 0.123047 | 0.057 sec/iter\n",
      "Epoch: 68 | Batch: 012 / 016 | Total loss: 4.492 | Reg loss: 0.032 | Tree loss: 4.492 | Accuracy: 0.095703 | 0.057 sec/iter\n",
      "Epoch: 68 | Batch: 013 / 016 | Total loss: 4.430 | Reg loss: 0.032 | Tree loss: 4.430 | Accuracy: 0.091797 | 0.057 sec/iter\n",
      "Epoch: 68 | Batch: 014 / 016 | Total loss: 4.382 | Reg loss: 0.032 | Tree loss: 4.382 | Accuracy: 0.113281 | 0.057 sec/iter\n",
      "Epoch: 68 | Batch: 015 / 016 | Total loss: 4.446 | Reg loss: 0.032 | Tree loss: 4.446 | Accuracy: 0.108974 | 0.057 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 69 | Batch: 000 / 016 | Total loss: 4.642 | Reg loss: 0.032 | Tree loss: 4.642 | Accuracy: 0.117188 | 0.057 sec/iter\n",
      "Epoch: 69 | Batch: 001 / 016 | Total loss: 4.636 | Reg loss: 0.032 | Tree loss: 4.636 | Accuracy: 0.105469 | 0.057 sec/iter\n",
      "Epoch: 69 | Batch: 002 / 016 | Total loss: 4.643 | Reg loss: 0.032 | Tree loss: 4.643 | Accuracy: 0.089844 | 0.057 sec/iter\n",
      "Epoch: 69 | Batch: 003 / 016 | Total loss: 4.560 | Reg loss: 0.032 | Tree loss: 4.560 | Accuracy: 0.105469 | 0.057 sec/iter\n",
      "Epoch: 69 | Batch: 004 / 016 | Total loss: 4.523 | Reg loss: 0.032 | Tree loss: 4.523 | Accuracy: 0.130859 | 0.057 sec/iter\n",
      "Epoch: 69 | Batch: 005 / 016 | Total loss: 4.517 | Reg loss: 0.032 | Tree loss: 4.517 | Accuracy: 0.113281 | 0.057 sec/iter\n",
      "Epoch: 69 | Batch: 006 / 016 | Total loss: 4.535 | Reg loss: 0.032 | Tree loss: 4.535 | Accuracy: 0.095703 | 0.057 sec/iter\n",
      "Epoch: 69 | Batch: 007 / 016 | Total loss: 4.518 | Reg loss: 0.032 | Tree loss: 4.518 | Accuracy: 0.113281 | 0.057 sec/iter\n",
      "Epoch: 69 | Batch: 008 / 016 | Total loss: 4.589 | Reg loss: 0.032 | Tree loss: 4.589 | Accuracy: 0.085938 | 0.057 sec/iter\n",
      "Epoch: 69 | Batch: 009 / 016 | Total loss: 4.421 | Reg loss: 0.032 | Tree loss: 4.421 | Accuracy: 0.109375 | 0.057 sec/iter\n",
      "Epoch: 69 | Batch: 010 / 016 | Total loss: 4.547 | Reg loss: 0.032 | Tree loss: 4.547 | Accuracy: 0.095703 | 0.057 sec/iter\n",
      "Epoch: 69 | Batch: 011 / 016 | Total loss: 4.401 | Reg loss: 0.032 | Tree loss: 4.401 | Accuracy: 0.101562 | 0.057 sec/iter\n",
      "Epoch: 69 | Batch: 012 / 016 | Total loss: 4.451 | Reg loss: 0.032 | Tree loss: 4.451 | Accuracy: 0.103516 | 0.057 sec/iter\n",
      "Epoch: 69 | Batch: 013 / 016 | Total loss: 4.416 | Reg loss: 0.032 | Tree loss: 4.416 | Accuracy: 0.123047 | 0.057 sec/iter\n",
      "Epoch: 69 | Batch: 014 / 016 | Total loss: 4.418 | Reg loss: 0.032 | Tree loss: 4.418 | Accuracy: 0.113281 | 0.057 sec/iter\n",
      "Epoch: 69 | Batch: 015 / 016 | Total loss: 4.447 | Reg loss: 0.032 | Tree loss: 4.447 | Accuracy: 0.111111 | 0.057 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 70 | Batch: 000 / 016 | Total loss: 4.623 | Reg loss: 0.032 | Tree loss: 4.623 | Accuracy: 0.099609 | 0.057 sec/iter\n",
      "Epoch: 70 | Batch: 001 / 016 | Total loss: 4.617 | Reg loss: 0.032 | Tree loss: 4.617 | Accuracy: 0.113281 | 0.057 sec/iter\n",
      "Epoch: 70 | Batch: 002 / 016 | Total loss: 4.545 | Reg loss: 0.032 | Tree loss: 4.545 | Accuracy: 0.095703 | 0.057 sec/iter\n",
      "Epoch: 70 | Batch: 003 / 016 | Total loss: 4.541 | Reg loss: 0.032 | Tree loss: 4.541 | Accuracy: 0.121094 | 0.057 sec/iter\n",
      "Epoch: 70 | Batch: 004 / 016 | Total loss: 4.494 | Reg loss: 0.032 | Tree loss: 4.494 | Accuracy: 0.117188 | 0.057 sec/iter\n",
      "Epoch: 70 | Batch: 005 / 016 | Total loss: 4.576 | Reg loss: 0.032 | Tree loss: 4.576 | Accuracy: 0.091797 | 0.057 sec/iter\n",
      "Epoch: 70 | Batch: 006 / 016 | Total loss: 4.544 | Reg loss: 0.032 | Tree loss: 4.544 | Accuracy: 0.105469 | 0.057 sec/iter\n",
      "Epoch: 70 | Batch: 007 / 016 | Total loss: 4.552 | Reg loss: 0.032 | Tree loss: 4.552 | Accuracy: 0.107422 | 0.057 sec/iter\n",
      "Epoch: 70 | Batch: 008 / 016 | Total loss: 4.430 | Reg loss: 0.032 | Tree loss: 4.430 | Accuracy: 0.107422 | 0.057 sec/iter\n",
      "Epoch: 70 | Batch: 009 / 016 | Total loss: 4.470 | Reg loss: 0.032 | Tree loss: 4.470 | Accuracy: 0.105469 | 0.057 sec/iter\n",
      "Epoch: 70 | Batch: 010 / 016 | Total loss: 4.421 | Reg loss: 0.032 | Tree loss: 4.421 | Accuracy: 0.111328 | 0.057 sec/iter\n",
      "Epoch: 70 | Batch: 011 / 016 | Total loss: 4.446 | Reg loss: 0.032 | Tree loss: 4.446 | Accuracy: 0.083984 | 0.057 sec/iter\n",
      "Epoch: 70 | Batch: 012 / 016 | Total loss: 4.490 | Reg loss: 0.032 | Tree loss: 4.490 | Accuracy: 0.123047 | 0.057 sec/iter\n",
      "Epoch: 70 | Batch: 013 / 016 | Total loss: 4.392 | Reg loss: 0.032 | Tree loss: 4.392 | Accuracy: 0.128906 | 0.057 sec/iter\n",
      "Epoch: 70 | Batch: 014 / 016 | Total loss: 4.362 | Reg loss: 0.032 | Tree loss: 4.362 | Accuracy: 0.091797 | 0.057 sec/iter\n",
      "Epoch: 70 | Batch: 015 / 016 | Total loss: 4.404 | Reg loss: 0.032 | Tree loss: 4.404 | Accuracy: 0.102564 | 0.057 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 71 | Batch: 000 / 016 | Total loss: 4.656 | Reg loss: 0.032 | Tree loss: 4.656 | Accuracy: 0.078125 | 0.057 sec/iter\n",
      "Epoch: 71 | Batch: 001 / 016 | Total loss: 4.525 | Reg loss: 0.032 | Tree loss: 4.525 | Accuracy: 0.113281 | 0.057 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 71 | Batch: 002 / 016 | Total loss: 4.614 | Reg loss: 0.032 | Tree loss: 4.614 | Accuracy: 0.087891 | 0.057 sec/iter\n",
      "Epoch: 71 | Batch: 003 / 016 | Total loss: 4.588 | Reg loss: 0.032 | Tree loss: 4.588 | Accuracy: 0.095703 | 0.057 sec/iter\n",
      "Epoch: 71 | Batch: 004 / 016 | Total loss: 4.514 | Reg loss: 0.032 | Tree loss: 4.514 | Accuracy: 0.125000 | 0.057 sec/iter\n",
      "Epoch: 71 | Batch: 005 / 016 | Total loss: 4.508 | Reg loss: 0.032 | Tree loss: 4.508 | Accuracy: 0.099609 | 0.057 sec/iter\n",
      "Epoch: 71 | Batch: 006 / 016 | Total loss: 4.476 | Reg loss: 0.032 | Tree loss: 4.476 | Accuracy: 0.107422 | 0.057 sec/iter\n",
      "Epoch: 71 | Batch: 007 / 016 | Total loss: 4.478 | Reg loss: 0.032 | Tree loss: 4.478 | Accuracy: 0.103516 | 0.057 sec/iter\n",
      "Epoch: 71 | Batch: 008 / 016 | Total loss: 4.409 | Reg loss: 0.032 | Tree loss: 4.409 | Accuracy: 0.115234 | 0.057 sec/iter\n",
      "Epoch: 71 | Batch: 009 / 016 | Total loss: 4.382 | Reg loss: 0.032 | Tree loss: 4.382 | Accuracy: 0.115234 | 0.057 sec/iter\n",
      "Epoch: 71 | Batch: 010 / 016 | Total loss: 4.389 | Reg loss: 0.032 | Tree loss: 4.389 | Accuracy: 0.125000 | 0.057 sec/iter\n",
      "Epoch: 71 | Batch: 011 / 016 | Total loss: 4.506 | Reg loss: 0.032 | Tree loss: 4.506 | Accuracy: 0.089844 | 0.057 sec/iter\n",
      "Epoch: 71 | Batch: 012 / 016 | Total loss: 4.376 | Reg loss: 0.032 | Tree loss: 4.376 | Accuracy: 0.138672 | 0.057 sec/iter\n",
      "Epoch: 71 | Batch: 013 / 016 | Total loss: 4.394 | Reg loss: 0.032 | Tree loss: 4.394 | Accuracy: 0.097656 | 0.057 sec/iter\n",
      "Epoch: 71 | Batch: 014 / 016 | Total loss: 4.363 | Reg loss: 0.032 | Tree loss: 4.363 | Accuracy: 0.115234 | 0.057 sec/iter\n",
      "Epoch: 71 | Batch: 015 / 016 | Total loss: 4.390 | Reg loss: 0.032 | Tree loss: 4.390 | Accuracy: 0.100427 | 0.057 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 72 | Batch: 000 / 016 | Total loss: 4.579 | Reg loss: 0.032 | Tree loss: 4.579 | Accuracy: 0.099609 | 0.057 sec/iter\n",
      "Epoch: 72 | Batch: 001 / 016 | Total loss: 4.568 | Reg loss: 0.032 | Tree loss: 4.568 | Accuracy: 0.089844 | 0.057 sec/iter\n",
      "Epoch: 72 | Batch: 002 / 016 | Total loss: 4.498 | Reg loss: 0.032 | Tree loss: 4.498 | Accuracy: 0.113281 | 0.057 sec/iter\n",
      "Epoch: 72 | Batch: 003 / 016 | Total loss: 4.509 | Reg loss: 0.032 | Tree loss: 4.509 | Accuracy: 0.117188 | 0.057 sec/iter\n",
      "Epoch: 72 | Batch: 004 / 016 | Total loss: 4.444 | Reg loss: 0.032 | Tree loss: 4.444 | Accuracy: 0.130859 | 0.057 sec/iter\n",
      "Epoch: 72 | Batch: 005 / 016 | Total loss: 4.489 | Reg loss: 0.032 | Tree loss: 4.489 | Accuracy: 0.101562 | 0.057 sec/iter\n",
      "Epoch: 72 | Batch: 006 / 016 | Total loss: 4.461 | Reg loss: 0.032 | Tree loss: 4.461 | Accuracy: 0.091797 | 0.057 sec/iter\n",
      "Epoch: 72 | Batch: 007 / 016 | Total loss: 4.460 | Reg loss: 0.032 | Tree loss: 4.460 | Accuracy: 0.097656 | 0.057 sec/iter\n",
      "Epoch: 72 | Batch: 008 / 016 | Total loss: 4.394 | Reg loss: 0.032 | Tree loss: 4.394 | Accuracy: 0.123047 | 0.057 sec/iter\n",
      "Epoch: 72 | Batch: 009 / 016 | Total loss: 4.428 | Reg loss: 0.032 | Tree loss: 4.428 | Accuracy: 0.107422 | 0.057 sec/iter\n",
      "Epoch: 72 | Batch: 010 / 016 | Total loss: 4.383 | Reg loss: 0.032 | Tree loss: 4.383 | Accuracy: 0.121094 | 0.057 sec/iter\n",
      "Epoch: 72 | Batch: 011 / 016 | Total loss: 4.457 | Reg loss: 0.032 | Tree loss: 4.457 | Accuracy: 0.105469 | 0.057 sec/iter\n",
      "Epoch: 72 | Batch: 012 / 016 | Total loss: 4.438 | Reg loss: 0.032 | Tree loss: 4.438 | Accuracy: 0.109375 | 0.057 sec/iter\n",
      "Epoch: 72 | Batch: 013 / 016 | Total loss: 4.346 | Reg loss: 0.032 | Tree loss: 4.346 | Accuracy: 0.107422 | 0.057 sec/iter\n",
      "Epoch: 72 | Batch: 014 / 016 | Total loss: 4.368 | Reg loss: 0.032 | Tree loss: 4.368 | Accuracy: 0.095703 | 0.057 sec/iter\n",
      "Epoch: 72 | Batch: 015 / 016 | Total loss: 4.413 | Reg loss: 0.032 | Tree loss: 4.413 | Accuracy: 0.098291 | 0.057 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 73 | Batch: 000 / 016 | Total loss: 4.543 | Reg loss: 0.032 | Tree loss: 4.543 | Accuracy: 0.093750 | 0.057 sec/iter\n",
      "Epoch: 73 | Batch: 001 / 016 | Total loss: 4.524 | Reg loss: 0.032 | Tree loss: 4.524 | Accuracy: 0.097656 | 0.057 sec/iter\n",
      "Epoch: 73 | Batch: 002 / 016 | Total loss: 4.528 | Reg loss: 0.032 | Tree loss: 4.528 | Accuracy: 0.107422 | 0.057 sec/iter\n",
      "Epoch: 73 | Batch: 003 / 016 | Total loss: 4.509 | Reg loss: 0.032 | Tree loss: 4.509 | Accuracy: 0.113281 | 0.057 sec/iter\n",
      "Epoch: 73 | Batch: 004 / 016 | Total loss: 4.459 | Reg loss: 0.032 | Tree loss: 4.459 | Accuracy: 0.109375 | 0.057 sec/iter\n",
      "Epoch: 73 | Batch: 005 / 016 | Total loss: 4.437 | Reg loss: 0.032 | Tree loss: 4.437 | Accuracy: 0.087891 | 0.057 sec/iter\n",
      "Epoch: 73 | Batch: 006 / 016 | Total loss: 4.439 | Reg loss: 0.032 | Tree loss: 4.439 | Accuracy: 0.113281 | 0.057 sec/iter\n",
      "Epoch: 73 | Batch: 007 / 016 | Total loss: 4.417 | Reg loss: 0.032 | Tree loss: 4.417 | Accuracy: 0.113281 | 0.057 sec/iter\n",
      "Epoch: 73 | Batch: 008 / 016 | Total loss: 4.497 | Reg loss: 0.032 | Tree loss: 4.497 | Accuracy: 0.083984 | 0.057 sec/iter\n",
      "Epoch: 73 | Batch: 009 / 016 | Total loss: 4.461 | Reg loss: 0.032 | Tree loss: 4.461 | Accuracy: 0.095703 | 0.057 sec/iter\n",
      "Epoch: 73 | Batch: 010 / 016 | Total loss: 4.390 | Reg loss: 0.032 | Tree loss: 4.390 | Accuracy: 0.093750 | 0.057 sec/iter\n",
      "Epoch: 73 | Batch: 011 / 016 | Total loss: 4.354 | Reg loss: 0.032 | Tree loss: 4.354 | Accuracy: 0.109375 | 0.057 sec/iter\n",
      "Epoch: 73 | Batch: 012 / 016 | Total loss: 4.338 | Reg loss: 0.032 | Tree loss: 4.338 | Accuracy: 0.117188 | 0.057 sec/iter\n",
      "Epoch: 73 | Batch: 013 / 016 | Total loss: 4.312 | Reg loss: 0.032 | Tree loss: 4.312 | Accuracy: 0.152344 | 0.057 sec/iter\n",
      "Epoch: 73 | Batch: 014 / 016 | Total loss: 4.328 | Reg loss: 0.032 | Tree loss: 4.328 | Accuracy: 0.099609 | 0.057 sec/iter\n",
      "Epoch: 73 | Batch: 015 / 016 | Total loss: 4.384 | Reg loss: 0.032 | Tree loss: 4.384 | Accuracy: 0.123932 | 0.057 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 74 | Batch: 000 / 016 | Total loss: 4.500 | Reg loss: 0.032 | Tree loss: 4.500 | Accuracy: 0.095703 | 0.057 sec/iter\n",
      "Epoch: 74 | Batch: 001 / 016 | Total loss: 4.521 | Reg loss: 0.032 | Tree loss: 4.521 | Accuracy: 0.109375 | 0.057 sec/iter\n",
      "Epoch: 74 | Batch: 002 / 016 | Total loss: 4.498 | Reg loss: 0.032 | Tree loss: 4.498 | Accuracy: 0.103516 | 0.057 sec/iter\n",
      "Epoch: 74 | Batch: 003 / 016 | Total loss: 4.400 | Reg loss: 0.032 | Tree loss: 4.400 | Accuracy: 0.119141 | 0.057 sec/iter\n",
      "Epoch: 74 | Batch: 004 / 016 | Total loss: 4.531 | Reg loss: 0.032 | Tree loss: 4.531 | Accuracy: 0.080078 | 0.057 sec/iter\n",
      "Epoch: 74 | Batch: 005 / 016 | Total loss: 4.453 | Reg loss: 0.032 | Tree loss: 4.453 | Accuracy: 0.099609 | 0.057 sec/iter\n",
      "Epoch: 74 | Batch: 006 / 016 | Total loss: 4.460 | Reg loss: 0.032 | Tree loss: 4.460 | Accuracy: 0.091797 | 0.057 sec/iter\n",
      "Epoch: 74 | Batch: 007 / 016 | Total loss: 4.460 | Reg loss: 0.032 | Tree loss: 4.460 | Accuracy: 0.080078 | 0.057 sec/iter\n",
      "Epoch: 74 | Batch: 008 / 016 | Total loss: 4.368 | Reg loss: 0.032 | Tree loss: 4.368 | Accuracy: 0.119141 | 0.057 sec/iter\n",
      "Epoch: 74 | Batch: 009 / 016 | Total loss: 4.335 | Reg loss: 0.032 | Tree loss: 4.335 | Accuracy: 0.140625 | 0.057 sec/iter\n",
      "Epoch: 74 | Batch: 010 / 016 | Total loss: 4.358 | Reg loss: 0.032 | Tree loss: 4.358 | Accuracy: 0.121094 | 0.057 sec/iter\n",
      "Epoch: 74 | Batch: 011 / 016 | Total loss: 4.387 | Reg loss: 0.032 | Tree loss: 4.387 | Accuracy: 0.123047 | 0.057 sec/iter\n",
      "Epoch: 74 | Batch: 012 / 016 | Total loss: 4.398 | Reg loss: 0.032 | Tree loss: 4.398 | Accuracy: 0.101562 | 0.057 sec/iter\n",
      "Epoch: 74 | Batch: 013 / 016 | Total loss: 4.324 | Reg loss: 0.032 | Tree loss: 4.324 | Accuracy: 0.113281 | 0.057 sec/iter\n",
      "Epoch: 74 | Batch: 014 / 016 | Total loss: 4.331 | Reg loss: 0.032 | Tree loss: 4.331 | Accuracy: 0.107422 | 0.057 sec/iter\n",
      "Epoch: 74 | Batch: 015 / 016 | Total loss: 4.290 | Reg loss: 0.032 | Tree loss: 4.290 | Accuracy: 0.104701 | 0.057 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 75 | Batch: 000 / 016 | Total loss: 4.478 | Reg loss: 0.032 | Tree loss: 4.478 | Accuracy: 0.126953 | 0.057 sec/iter\n",
      "Epoch: 75 | Batch: 001 / 016 | Total loss: 4.485 | Reg loss: 0.032 | Tree loss: 4.485 | Accuracy: 0.101562 | 0.057 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 75 | Batch: 002 / 016 | Total loss: 4.477 | Reg loss: 0.032 | Tree loss: 4.477 | Accuracy: 0.097656 | 0.057 sec/iter\n",
      "Epoch: 75 | Batch: 003 / 016 | Total loss: 4.480 | Reg loss: 0.032 | Tree loss: 4.480 | Accuracy: 0.111328 | 0.057 sec/iter\n",
      "Epoch: 75 | Batch: 004 / 016 | Total loss: 4.484 | Reg loss: 0.032 | Tree loss: 4.484 | Accuracy: 0.093750 | 0.057 sec/iter\n",
      "Epoch: 75 | Batch: 005 / 016 | Total loss: 4.400 | Reg loss: 0.032 | Tree loss: 4.400 | Accuracy: 0.132812 | 0.057 sec/iter\n",
      "Epoch: 75 | Batch: 006 / 016 | Total loss: 4.516 | Reg loss: 0.032 | Tree loss: 4.516 | Accuracy: 0.072266 | 0.057 sec/iter\n",
      "Epoch: 75 | Batch: 007 / 016 | Total loss: 4.378 | Reg loss: 0.032 | Tree loss: 4.378 | Accuracy: 0.097656 | 0.057 sec/iter\n",
      "Epoch: 75 | Batch: 008 / 016 | Total loss: 4.461 | Reg loss: 0.032 | Tree loss: 4.461 | Accuracy: 0.103516 | 0.057 sec/iter\n",
      "Epoch: 75 | Batch: 009 / 016 | Total loss: 4.326 | Reg loss: 0.032 | Tree loss: 4.326 | Accuracy: 0.119141 | 0.057 sec/iter\n",
      "Epoch: 75 | Batch: 010 / 016 | Total loss: 4.301 | Reg loss: 0.032 | Tree loss: 4.301 | Accuracy: 0.113281 | 0.057 sec/iter\n",
      "Epoch: 75 | Batch: 011 / 016 | Total loss: 4.436 | Reg loss: 0.032 | Tree loss: 4.436 | Accuracy: 0.085938 | 0.057 sec/iter\n",
      "Epoch: 75 | Batch: 012 / 016 | Total loss: 4.312 | Reg loss: 0.032 | Tree loss: 4.312 | Accuracy: 0.109375 | 0.057 sec/iter\n",
      "Epoch: 75 | Batch: 013 / 016 | Total loss: 4.311 | Reg loss: 0.032 | Tree loss: 4.311 | Accuracy: 0.101562 | 0.057 sec/iter\n",
      "Epoch: 75 | Batch: 014 / 016 | Total loss: 4.249 | Reg loss: 0.032 | Tree loss: 4.249 | Accuracy: 0.115234 | 0.057 sec/iter\n",
      "Epoch: 75 | Batch: 015 / 016 | Total loss: 4.224 | Reg loss: 0.032 | Tree loss: 4.224 | Accuracy: 0.132479 | 0.057 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 76 | Batch: 000 / 016 | Total loss: 4.459 | Reg loss: 0.032 | Tree loss: 4.459 | Accuracy: 0.113281 | 0.057 sec/iter\n",
      "Epoch: 76 | Batch: 001 / 016 | Total loss: 4.495 | Reg loss: 0.032 | Tree loss: 4.495 | Accuracy: 0.107422 | 0.057 sec/iter\n",
      "Epoch: 76 | Batch: 002 / 016 | Total loss: 4.456 | Reg loss: 0.032 | Tree loss: 4.456 | Accuracy: 0.130859 | 0.057 sec/iter\n",
      "Epoch: 76 | Batch: 003 / 016 | Total loss: 4.455 | Reg loss: 0.032 | Tree loss: 4.455 | Accuracy: 0.107422 | 0.057 sec/iter\n",
      "Epoch: 76 | Batch: 004 / 016 | Total loss: 4.414 | Reg loss: 0.032 | Tree loss: 4.414 | Accuracy: 0.093750 | 0.057 sec/iter\n",
      "Epoch: 76 | Batch: 005 / 016 | Total loss: 4.361 | Reg loss: 0.032 | Tree loss: 4.361 | Accuracy: 0.125000 | 0.057 sec/iter\n",
      "Epoch: 76 | Batch: 006 / 016 | Total loss: 4.336 | Reg loss: 0.032 | Tree loss: 4.336 | Accuracy: 0.126953 | 0.057 sec/iter\n",
      "Epoch: 76 | Batch: 007 / 016 | Total loss: 4.353 | Reg loss: 0.032 | Tree loss: 4.353 | Accuracy: 0.105469 | 0.057 sec/iter\n",
      "Epoch: 76 | Batch: 008 / 016 | Total loss: 4.408 | Reg loss: 0.032 | Tree loss: 4.408 | Accuracy: 0.089844 | 0.057 sec/iter\n",
      "Epoch: 76 | Batch: 009 / 016 | Total loss: 4.422 | Reg loss: 0.032 | Tree loss: 4.422 | Accuracy: 0.093750 | 0.057 sec/iter\n",
      "Epoch: 76 | Batch: 010 / 016 | Total loss: 4.355 | Reg loss: 0.032 | Tree loss: 4.355 | Accuracy: 0.105469 | 0.057 sec/iter\n",
      "Epoch: 76 | Batch: 011 / 016 | Total loss: 4.360 | Reg loss: 0.032 | Tree loss: 4.360 | Accuracy: 0.091797 | 0.057 sec/iter\n",
      "Epoch: 76 | Batch: 012 / 016 | Total loss: 4.344 | Reg loss: 0.032 | Tree loss: 4.344 | Accuracy: 0.109375 | 0.057 sec/iter\n",
      "Epoch: 76 | Batch: 013 / 016 | Total loss: 4.315 | Reg loss: 0.032 | Tree loss: 4.315 | Accuracy: 0.105469 | 0.057 sec/iter\n",
      "Epoch: 76 | Batch: 014 / 016 | Total loss: 4.270 | Reg loss: 0.032 | Tree loss: 4.270 | Accuracy: 0.109375 | 0.057 sec/iter\n",
      "Epoch: 76 | Batch: 015 / 016 | Total loss: 4.245 | Reg loss: 0.032 | Tree loss: 4.245 | Accuracy: 0.094017 | 0.057 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 77 | Batch: 000 / 016 | Total loss: 4.477 | Reg loss: 0.032 | Tree loss: 4.477 | Accuracy: 0.105469 | 0.057 sec/iter\n",
      "Epoch: 77 | Batch: 001 / 016 | Total loss: 4.463 | Reg loss: 0.032 | Tree loss: 4.463 | Accuracy: 0.097656 | 0.057 sec/iter\n",
      "Epoch: 77 | Batch: 002 / 016 | Total loss: 4.413 | Reg loss: 0.032 | Tree loss: 4.413 | Accuracy: 0.111328 | 0.057 sec/iter\n",
      "Epoch: 77 | Batch: 003 / 016 | Total loss: 4.400 | Reg loss: 0.032 | Tree loss: 4.400 | Accuracy: 0.111328 | 0.057 sec/iter\n",
      "Epoch: 77 | Batch: 004 / 016 | Total loss: 4.383 | Reg loss: 0.032 | Tree loss: 4.383 | Accuracy: 0.123047 | 0.058 sec/iter\n",
      "Epoch: 77 | Batch: 005 / 016 | Total loss: 4.404 | Reg loss: 0.032 | Tree loss: 4.404 | Accuracy: 0.095703 | 0.058 sec/iter\n",
      "Epoch: 77 | Batch: 006 / 016 | Total loss: 4.423 | Reg loss: 0.032 | Tree loss: 4.423 | Accuracy: 0.095703 | 0.058 sec/iter\n",
      "Epoch: 77 | Batch: 007 / 016 | Total loss: 4.397 | Reg loss: 0.032 | Tree loss: 4.397 | Accuracy: 0.109375 | 0.058 sec/iter\n",
      "Epoch: 77 | Batch: 008 / 016 | Total loss: 4.393 | Reg loss: 0.032 | Tree loss: 4.393 | Accuracy: 0.085938 | 0.058 sec/iter\n",
      "Epoch: 77 | Batch: 009 / 016 | Total loss: 4.325 | Reg loss: 0.032 | Tree loss: 4.325 | Accuracy: 0.089844 | 0.058 sec/iter\n",
      "Epoch: 77 | Batch: 010 / 016 | Total loss: 4.344 | Reg loss: 0.032 | Tree loss: 4.344 | Accuracy: 0.099609 | 0.058 sec/iter\n",
      "Epoch: 77 | Batch: 011 / 016 | Total loss: 4.314 | Reg loss: 0.032 | Tree loss: 4.314 | Accuracy: 0.113281 | 0.058 sec/iter\n",
      "Epoch: 77 | Batch: 012 / 016 | Total loss: 4.263 | Reg loss: 0.032 | Tree loss: 4.263 | Accuracy: 0.121094 | 0.058 sec/iter\n",
      "Epoch: 77 | Batch: 013 / 016 | Total loss: 4.319 | Reg loss: 0.032 | Tree loss: 4.319 | Accuracy: 0.103516 | 0.058 sec/iter\n",
      "Epoch: 77 | Batch: 014 / 016 | Total loss: 4.270 | Reg loss: 0.032 | Tree loss: 4.270 | Accuracy: 0.130859 | 0.058 sec/iter\n",
      "Epoch: 77 | Batch: 015 / 016 | Total loss: 4.197 | Reg loss: 0.032 | Tree loss: 4.197 | Accuracy: 0.117521 | 0.058 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 78 | Batch: 000 / 016 | Total loss: 4.453 | Reg loss: 0.032 | Tree loss: 4.453 | Accuracy: 0.103516 | 0.058 sec/iter\n",
      "Epoch: 78 | Batch: 001 / 016 | Total loss: 4.517 | Reg loss: 0.032 | Tree loss: 4.517 | Accuracy: 0.099609 | 0.058 sec/iter\n",
      "Epoch: 78 | Batch: 002 / 016 | Total loss: 4.352 | Reg loss: 0.032 | Tree loss: 4.352 | Accuracy: 0.121094 | 0.058 sec/iter\n",
      "Epoch: 78 | Batch: 003 / 016 | Total loss: 4.411 | Reg loss: 0.032 | Tree loss: 4.411 | Accuracy: 0.121094 | 0.058 sec/iter\n",
      "Epoch: 78 | Batch: 004 / 016 | Total loss: 4.401 | Reg loss: 0.032 | Tree loss: 4.401 | Accuracy: 0.111328 | 0.058 sec/iter\n",
      "Epoch: 78 | Batch: 005 / 016 | Total loss: 4.402 | Reg loss: 0.032 | Tree loss: 4.402 | Accuracy: 0.105469 | 0.058 sec/iter\n",
      "Epoch: 78 | Batch: 006 / 016 | Total loss: 4.337 | Reg loss: 0.032 | Tree loss: 4.337 | Accuracy: 0.119141 | 0.058 sec/iter\n",
      "Epoch: 78 | Batch: 007 / 016 | Total loss: 4.387 | Reg loss: 0.032 | Tree loss: 4.387 | Accuracy: 0.109375 | 0.058 sec/iter\n",
      "Epoch: 78 | Batch: 008 / 016 | Total loss: 4.342 | Reg loss: 0.032 | Tree loss: 4.342 | Accuracy: 0.091797 | 0.058 sec/iter\n",
      "Epoch: 78 | Batch: 009 / 016 | Total loss: 4.334 | Reg loss: 0.032 | Tree loss: 4.334 | Accuracy: 0.093750 | 0.058 sec/iter\n",
      "Epoch: 78 | Batch: 010 / 016 | Total loss: 4.280 | Reg loss: 0.032 | Tree loss: 4.280 | Accuracy: 0.103516 | 0.058 sec/iter\n",
      "Epoch: 78 | Batch: 011 / 016 | Total loss: 4.277 | Reg loss: 0.032 | Tree loss: 4.277 | Accuracy: 0.099609 | 0.058 sec/iter\n",
      "Epoch: 78 | Batch: 012 / 016 | Total loss: 4.347 | Reg loss: 0.032 | Tree loss: 4.347 | Accuracy: 0.082031 | 0.058 sec/iter\n",
      "Epoch: 78 | Batch: 013 / 016 | Total loss: 4.254 | Reg loss: 0.032 | Tree loss: 4.254 | Accuracy: 0.113281 | 0.058 sec/iter\n",
      "Epoch: 78 | Batch: 014 / 016 | Total loss: 4.237 | Reg loss: 0.032 | Tree loss: 4.237 | Accuracy: 0.119141 | 0.058 sec/iter\n",
      "Epoch: 78 | Batch: 015 / 016 | Total loss: 4.209 | Reg loss: 0.032 | Tree loss: 4.209 | Accuracy: 0.115385 | 0.058 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 79 | Batch: 000 / 016 | Total loss: 4.518 | Reg loss: 0.032 | Tree loss: 4.518 | Accuracy: 0.082031 | 0.058 sec/iter\n",
      "Epoch: 79 | Batch: 001 / 016 | Total loss: 4.470 | Reg loss: 0.032 | Tree loss: 4.470 | Accuracy: 0.078125 | 0.058 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 79 | Batch: 002 / 016 | Total loss: 4.363 | Reg loss: 0.032 | Tree loss: 4.363 | Accuracy: 0.121094 | 0.058 sec/iter\n",
      "Epoch: 79 | Batch: 003 / 016 | Total loss: 4.440 | Reg loss: 0.032 | Tree loss: 4.440 | Accuracy: 0.113281 | 0.058 sec/iter\n",
      "Epoch: 79 | Batch: 004 / 016 | Total loss: 4.341 | Reg loss: 0.032 | Tree loss: 4.341 | Accuracy: 0.103516 | 0.058 sec/iter\n",
      "Epoch: 79 | Batch: 005 / 016 | Total loss: 4.411 | Reg loss: 0.032 | Tree loss: 4.411 | Accuracy: 0.087891 | 0.058 sec/iter\n",
      "Epoch: 79 | Batch: 006 / 016 | Total loss: 4.393 | Reg loss: 0.032 | Tree loss: 4.393 | Accuracy: 0.091797 | 0.058 sec/iter\n",
      "Epoch: 79 | Batch: 007 / 016 | Total loss: 4.297 | Reg loss: 0.032 | Tree loss: 4.297 | Accuracy: 0.111328 | 0.058 sec/iter\n",
      "Epoch: 79 | Batch: 008 / 016 | Total loss: 4.362 | Reg loss: 0.032 | Tree loss: 4.362 | Accuracy: 0.111328 | 0.058 sec/iter\n",
      "Epoch: 79 | Batch: 009 / 016 | Total loss: 4.199 | Reg loss: 0.032 | Tree loss: 4.199 | Accuracy: 0.148438 | 0.058 sec/iter\n",
      "Epoch: 79 | Batch: 010 / 016 | Total loss: 4.308 | Reg loss: 0.032 | Tree loss: 4.308 | Accuracy: 0.115234 | 0.058 sec/iter\n",
      "Epoch: 79 | Batch: 011 / 016 | Total loss: 4.282 | Reg loss: 0.032 | Tree loss: 4.282 | Accuracy: 0.119141 | 0.058 sec/iter\n",
      "Epoch: 79 | Batch: 012 / 016 | Total loss: 4.285 | Reg loss: 0.032 | Tree loss: 4.285 | Accuracy: 0.083984 | 0.058 sec/iter\n",
      "Epoch: 79 | Batch: 013 / 016 | Total loss: 4.228 | Reg loss: 0.032 | Tree loss: 4.228 | Accuracy: 0.107422 | 0.058 sec/iter\n",
      "Epoch: 79 | Batch: 014 / 016 | Total loss: 4.209 | Reg loss: 0.032 | Tree loss: 4.209 | Accuracy: 0.128906 | 0.058 sec/iter\n",
      "Epoch: 79 | Batch: 015 / 016 | Total loss: 4.198 | Reg loss: 0.032 | Tree loss: 4.198 | Accuracy: 0.106838 | 0.058 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 80 | Batch: 000 / 016 | Total loss: 4.368 | Reg loss: 0.032 | Tree loss: 4.368 | Accuracy: 0.103516 | 0.058 sec/iter\n",
      "Epoch: 80 | Batch: 001 / 016 | Total loss: 4.415 | Reg loss: 0.032 | Tree loss: 4.415 | Accuracy: 0.101562 | 0.058 sec/iter\n",
      "Epoch: 80 | Batch: 002 / 016 | Total loss: 4.417 | Reg loss: 0.032 | Tree loss: 4.417 | Accuracy: 0.099609 | 0.058 sec/iter\n",
      "Epoch: 80 | Batch: 003 / 016 | Total loss: 4.453 | Reg loss: 0.032 | Tree loss: 4.453 | Accuracy: 0.089844 | 0.058 sec/iter\n",
      "Epoch: 80 | Batch: 004 / 016 | Total loss: 4.317 | Reg loss: 0.032 | Tree loss: 4.317 | Accuracy: 0.113281 | 0.058 sec/iter\n",
      "Epoch: 80 | Batch: 005 / 016 | Total loss: 4.328 | Reg loss: 0.032 | Tree loss: 4.328 | Accuracy: 0.115234 | 0.058 sec/iter\n",
      "Epoch: 80 | Batch: 006 / 016 | Total loss: 4.364 | Reg loss: 0.032 | Tree loss: 4.364 | Accuracy: 0.105469 | 0.058 sec/iter\n",
      "Epoch: 80 | Batch: 007 / 016 | Total loss: 4.392 | Reg loss: 0.032 | Tree loss: 4.392 | Accuracy: 0.087891 | 0.058 sec/iter\n",
      "Epoch: 80 | Batch: 008 / 016 | Total loss: 4.261 | Reg loss: 0.032 | Tree loss: 4.261 | Accuracy: 0.123047 | 0.058 sec/iter\n",
      "Epoch: 80 | Batch: 009 / 016 | Total loss: 4.337 | Reg loss: 0.032 | Tree loss: 4.337 | Accuracy: 0.126953 | 0.058 sec/iter\n",
      "Epoch: 80 | Batch: 010 / 016 | Total loss: 4.262 | Reg loss: 0.032 | Tree loss: 4.262 | Accuracy: 0.097656 | 0.058 sec/iter\n",
      "Epoch: 80 | Batch: 011 / 016 | Total loss: 4.254 | Reg loss: 0.032 | Tree loss: 4.254 | Accuracy: 0.113281 | 0.058 sec/iter\n",
      "Epoch: 80 | Batch: 012 / 016 | Total loss: 4.295 | Reg loss: 0.032 | Tree loss: 4.295 | Accuracy: 0.119141 | 0.058 sec/iter\n",
      "Epoch: 80 | Batch: 013 / 016 | Total loss: 4.306 | Reg loss: 0.032 | Tree loss: 4.306 | Accuracy: 0.083984 | 0.058 sec/iter\n",
      "Epoch: 80 | Batch: 014 / 016 | Total loss: 4.192 | Reg loss: 0.032 | Tree loss: 4.192 | Accuracy: 0.105469 | 0.058 sec/iter\n",
      "Epoch: 80 | Batch: 015 / 016 | Total loss: 4.111 | Reg loss: 0.032 | Tree loss: 4.111 | Accuracy: 0.128205 | 0.058 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 81 | Batch: 000 / 016 | Total loss: 4.478 | Reg loss: 0.032 | Tree loss: 4.478 | Accuracy: 0.097656 | 0.058 sec/iter\n",
      "Epoch: 81 | Batch: 001 / 016 | Total loss: 4.430 | Reg loss: 0.032 | Tree loss: 4.430 | Accuracy: 0.089844 | 0.058 sec/iter\n",
      "Epoch: 81 | Batch: 002 / 016 | Total loss: 4.344 | Reg loss: 0.032 | Tree loss: 4.344 | Accuracy: 0.121094 | 0.058 sec/iter\n",
      "Epoch: 81 | Batch: 003 / 016 | Total loss: 4.377 | Reg loss: 0.032 | Tree loss: 4.377 | Accuracy: 0.115234 | 0.058 sec/iter\n",
      "Epoch: 81 | Batch: 004 / 016 | Total loss: 4.369 | Reg loss: 0.032 | Tree loss: 4.369 | Accuracy: 0.115234 | 0.058 sec/iter\n",
      "Epoch: 81 | Batch: 005 / 016 | Total loss: 4.400 | Reg loss: 0.032 | Tree loss: 4.400 | Accuracy: 0.082031 | 0.058 sec/iter\n",
      "Epoch: 81 | Batch: 006 / 016 | Total loss: 4.335 | Reg loss: 0.032 | Tree loss: 4.335 | Accuracy: 0.109375 | 0.058 sec/iter\n",
      "Epoch: 81 | Batch: 007 / 016 | Total loss: 4.280 | Reg loss: 0.032 | Tree loss: 4.280 | Accuracy: 0.113281 | 0.058 sec/iter\n",
      "Epoch: 81 | Batch: 008 / 016 | Total loss: 4.369 | Reg loss: 0.032 | Tree loss: 4.369 | Accuracy: 0.093750 | 0.058 sec/iter\n",
      "Epoch: 81 | Batch: 009 / 016 | Total loss: 4.238 | Reg loss: 0.032 | Tree loss: 4.238 | Accuracy: 0.105469 | 0.058 sec/iter\n",
      "Epoch: 81 | Batch: 010 / 016 | Total loss: 4.265 | Reg loss: 0.032 | Tree loss: 4.265 | Accuracy: 0.101562 | 0.058 sec/iter\n",
      "Epoch: 81 | Batch: 011 / 016 | Total loss: 4.236 | Reg loss: 0.032 | Tree loss: 4.236 | Accuracy: 0.103516 | 0.058 sec/iter\n",
      "Epoch: 81 | Batch: 012 / 016 | Total loss: 4.207 | Reg loss: 0.032 | Tree loss: 4.207 | Accuracy: 0.109375 | 0.058 sec/iter\n",
      "Epoch: 81 | Batch: 013 / 016 | Total loss: 4.186 | Reg loss: 0.032 | Tree loss: 4.186 | Accuracy: 0.115234 | 0.058 sec/iter\n",
      "Epoch: 81 | Batch: 014 / 016 | Total loss: 4.139 | Reg loss: 0.032 | Tree loss: 4.139 | Accuracy: 0.111328 | 0.058 sec/iter\n",
      "Epoch: 81 | Batch: 015 / 016 | Total loss: 4.207 | Reg loss: 0.032 | Tree loss: 4.207 | Accuracy: 0.123932 | 0.058 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 82 | Batch: 000 / 016 | Total loss: 4.411 | Reg loss: 0.032 | Tree loss: 4.411 | Accuracy: 0.142578 | 0.058 sec/iter\n",
      "Epoch: 82 | Batch: 001 / 016 | Total loss: 4.413 | Reg loss: 0.032 | Tree loss: 4.413 | Accuracy: 0.083984 | 0.058 sec/iter\n",
      "Epoch: 82 | Batch: 002 / 016 | Total loss: 4.333 | Reg loss: 0.032 | Tree loss: 4.333 | Accuracy: 0.089844 | 0.058 sec/iter\n",
      "Epoch: 82 | Batch: 003 / 016 | Total loss: 4.362 | Reg loss: 0.032 | Tree loss: 4.362 | Accuracy: 0.097656 | 0.058 sec/iter\n",
      "Epoch: 82 | Batch: 004 / 016 | Total loss: 4.317 | Reg loss: 0.032 | Tree loss: 4.317 | Accuracy: 0.119141 | 0.058 sec/iter\n",
      "Epoch: 82 | Batch: 005 / 016 | Total loss: 4.272 | Reg loss: 0.032 | Tree loss: 4.272 | Accuracy: 0.121094 | 0.058 sec/iter\n",
      "Epoch: 82 | Batch: 006 / 016 | Total loss: 4.280 | Reg loss: 0.032 | Tree loss: 4.280 | Accuracy: 0.115234 | 0.058 sec/iter\n",
      "Epoch: 82 | Batch: 007 / 016 | Total loss: 4.277 | Reg loss: 0.032 | Tree loss: 4.277 | Accuracy: 0.111328 | 0.058 sec/iter\n",
      "Epoch: 82 | Batch: 008 / 016 | Total loss: 4.343 | Reg loss: 0.032 | Tree loss: 4.343 | Accuracy: 0.074219 | 0.058 sec/iter\n",
      "Epoch: 82 | Batch: 009 / 016 | Total loss: 4.361 | Reg loss: 0.032 | Tree loss: 4.361 | Accuracy: 0.101562 | 0.058 sec/iter\n",
      "Epoch: 82 | Batch: 010 / 016 | Total loss: 4.289 | Reg loss: 0.032 | Tree loss: 4.289 | Accuracy: 0.123047 | 0.058 sec/iter\n",
      "Epoch: 82 | Batch: 011 / 016 | Total loss: 4.250 | Reg loss: 0.032 | Tree loss: 4.250 | Accuracy: 0.101562 | 0.058 sec/iter\n",
      "Epoch: 82 | Batch: 012 / 016 | Total loss: 4.227 | Reg loss: 0.032 | Tree loss: 4.227 | Accuracy: 0.115234 | 0.058 sec/iter\n",
      "Epoch: 82 | Batch: 013 / 016 | Total loss: 4.220 | Reg loss: 0.032 | Tree loss: 4.220 | Accuracy: 0.095703 | 0.058 sec/iter\n",
      "Epoch: 82 | Batch: 014 / 016 | Total loss: 4.194 | Reg loss: 0.032 | Tree loss: 4.194 | Accuracy: 0.099609 | 0.058 sec/iter\n",
      "Epoch: 82 | Batch: 015 / 016 | Total loss: 4.098 | Reg loss: 0.032 | Tree loss: 4.098 | Accuracy: 0.121795 | 0.058 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 83 | Batch: 000 / 016 | Total loss: 4.380 | Reg loss: 0.032 | Tree loss: 4.380 | Accuracy: 0.119141 | 0.058 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 83 | Batch: 001 / 016 | Total loss: 4.320 | Reg loss: 0.032 | Tree loss: 4.320 | Accuracy: 0.107422 | 0.058 sec/iter\n",
      "Epoch: 83 | Batch: 002 / 016 | Total loss: 4.347 | Reg loss: 0.032 | Tree loss: 4.347 | Accuracy: 0.109375 | 0.058 sec/iter\n",
      "Epoch: 83 | Batch: 003 / 016 | Total loss: 4.353 | Reg loss: 0.032 | Tree loss: 4.353 | Accuracy: 0.107422 | 0.058 sec/iter\n",
      "Epoch: 83 | Batch: 004 / 016 | Total loss: 4.281 | Reg loss: 0.032 | Tree loss: 4.281 | Accuracy: 0.125000 | 0.058 sec/iter\n",
      "Epoch: 83 | Batch: 005 / 016 | Total loss: 4.320 | Reg loss: 0.032 | Tree loss: 4.320 | Accuracy: 0.113281 | 0.058 sec/iter\n",
      "Epoch: 83 | Batch: 006 / 016 | Total loss: 4.321 | Reg loss: 0.032 | Tree loss: 4.321 | Accuracy: 0.097656 | 0.058 sec/iter\n",
      "Epoch: 83 | Batch: 007 / 016 | Total loss: 4.226 | Reg loss: 0.032 | Tree loss: 4.226 | Accuracy: 0.125000 | 0.058 sec/iter\n",
      "Epoch: 83 | Batch: 008 / 016 | Total loss: 4.352 | Reg loss: 0.032 | Tree loss: 4.352 | Accuracy: 0.097656 | 0.058 sec/iter\n",
      "Epoch: 83 | Batch: 009 / 016 | Total loss: 4.219 | Reg loss: 0.032 | Tree loss: 4.219 | Accuracy: 0.103516 | 0.058 sec/iter\n",
      "Epoch: 83 | Batch: 010 / 016 | Total loss: 4.366 | Reg loss: 0.032 | Tree loss: 4.366 | Accuracy: 0.068359 | 0.058 sec/iter\n",
      "Epoch: 83 | Batch: 011 / 016 | Total loss: 4.275 | Reg loss: 0.032 | Tree loss: 4.275 | Accuracy: 0.107422 | 0.058 sec/iter\n",
      "Epoch: 83 | Batch: 012 / 016 | Total loss: 4.228 | Reg loss: 0.032 | Tree loss: 4.228 | Accuracy: 0.097656 | 0.058 sec/iter\n",
      "Epoch: 83 | Batch: 013 / 016 | Total loss: 4.110 | Reg loss: 0.032 | Tree loss: 4.110 | Accuracy: 0.128906 | 0.058 sec/iter\n",
      "Epoch: 83 | Batch: 014 / 016 | Total loss: 4.175 | Reg loss: 0.032 | Tree loss: 4.175 | Accuracy: 0.091797 | 0.058 sec/iter\n",
      "Epoch: 83 | Batch: 015 / 016 | Total loss: 4.183 | Reg loss: 0.032 | Tree loss: 4.183 | Accuracy: 0.106838 | 0.058 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 84 | Batch: 000 / 016 | Total loss: 4.405 | Reg loss: 0.032 | Tree loss: 4.405 | Accuracy: 0.117188 | 0.058 sec/iter\n",
      "Epoch: 84 | Batch: 001 / 016 | Total loss: 4.424 | Reg loss: 0.032 | Tree loss: 4.424 | Accuracy: 0.111328 | 0.058 sec/iter\n",
      "Epoch: 84 | Batch: 002 / 016 | Total loss: 4.363 | Reg loss: 0.032 | Tree loss: 4.363 | Accuracy: 0.074219 | 0.058 sec/iter\n",
      "Epoch: 84 | Batch: 003 / 016 | Total loss: 4.278 | Reg loss: 0.032 | Tree loss: 4.278 | Accuracy: 0.109375 | 0.058 sec/iter\n",
      "Epoch: 84 | Batch: 004 / 016 | Total loss: 4.316 | Reg loss: 0.032 | Tree loss: 4.316 | Accuracy: 0.113281 | 0.058 sec/iter\n",
      "Epoch: 84 | Batch: 005 / 016 | Total loss: 4.255 | Reg loss: 0.032 | Tree loss: 4.255 | Accuracy: 0.126953 | 0.058 sec/iter\n",
      "Epoch: 84 | Batch: 006 / 016 | Total loss: 4.281 | Reg loss: 0.032 | Tree loss: 4.281 | Accuracy: 0.089844 | 0.058 sec/iter\n",
      "Epoch: 84 | Batch: 007 / 016 | Total loss: 4.265 | Reg loss: 0.032 | Tree loss: 4.265 | Accuracy: 0.109375 | 0.058 sec/iter\n",
      "Epoch: 84 | Batch: 008 / 016 | Total loss: 4.243 | Reg loss: 0.032 | Tree loss: 4.243 | Accuracy: 0.113281 | 0.058 sec/iter\n",
      "Epoch: 84 | Batch: 009 / 016 | Total loss: 4.377 | Reg loss: 0.032 | Tree loss: 4.377 | Accuracy: 0.076172 | 0.058 sec/iter\n",
      "Epoch: 84 | Batch: 010 / 016 | Total loss: 4.280 | Reg loss: 0.032 | Tree loss: 4.280 | Accuracy: 0.111328 | 0.058 sec/iter\n",
      "Epoch: 84 | Batch: 011 / 016 | Total loss: 4.194 | Reg loss: 0.032 | Tree loss: 4.194 | Accuracy: 0.121094 | 0.058 sec/iter\n",
      "Epoch: 84 | Batch: 012 / 016 | Total loss: 4.191 | Reg loss: 0.032 | Tree loss: 4.191 | Accuracy: 0.103516 | 0.058 sec/iter\n",
      "Epoch: 84 | Batch: 013 / 016 | Total loss: 4.179 | Reg loss: 0.032 | Tree loss: 4.179 | Accuracy: 0.111328 | 0.058 sec/iter\n",
      "Epoch: 84 | Batch: 014 / 016 | Total loss: 4.107 | Reg loss: 0.032 | Tree loss: 4.107 | Accuracy: 0.115234 | 0.058 sec/iter\n",
      "Epoch: 84 | Batch: 015 / 016 | Total loss: 4.103 | Reg loss: 0.032 | Tree loss: 4.103 | Accuracy: 0.102564 | 0.058 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 85 | Batch: 000 / 016 | Total loss: 4.311 | Reg loss: 0.032 | Tree loss: 4.311 | Accuracy: 0.125000 | 0.058 sec/iter\n",
      "Epoch: 85 | Batch: 001 / 016 | Total loss: 4.348 | Reg loss: 0.032 | Tree loss: 4.348 | Accuracy: 0.111328 | 0.058 sec/iter\n",
      "Epoch: 85 | Batch: 002 / 016 | Total loss: 4.381 | Reg loss: 0.032 | Tree loss: 4.381 | Accuracy: 0.101562 | 0.058 sec/iter\n",
      "Epoch: 85 | Batch: 003 / 016 | Total loss: 4.317 | Reg loss: 0.032 | Tree loss: 4.317 | Accuracy: 0.119141 | 0.058 sec/iter\n",
      "Epoch: 85 | Batch: 004 / 016 | Total loss: 4.342 | Reg loss: 0.032 | Tree loss: 4.342 | Accuracy: 0.113281 | 0.058 sec/iter\n",
      "Epoch: 85 | Batch: 005 / 016 | Total loss: 4.308 | Reg loss: 0.032 | Tree loss: 4.308 | Accuracy: 0.099609 | 0.058 sec/iter\n",
      "Epoch: 85 | Batch: 006 / 016 | Total loss: 4.248 | Reg loss: 0.032 | Tree loss: 4.248 | Accuracy: 0.107422 | 0.058 sec/iter\n",
      "Epoch: 85 | Batch: 007 / 016 | Total loss: 4.233 | Reg loss: 0.032 | Tree loss: 4.233 | Accuracy: 0.123047 | 0.058 sec/iter\n",
      "Epoch: 85 | Batch: 008 / 016 | Total loss: 4.229 | Reg loss: 0.032 | Tree loss: 4.229 | Accuracy: 0.111328 | 0.058 sec/iter\n",
      "Epoch: 85 | Batch: 009 / 016 | Total loss: 4.263 | Reg loss: 0.032 | Tree loss: 4.263 | Accuracy: 0.089844 | 0.058 sec/iter\n",
      "Epoch: 85 | Batch: 010 / 016 | Total loss: 4.202 | Reg loss: 0.032 | Tree loss: 4.202 | Accuracy: 0.103516 | 0.058 sec/iter\n",
      "Epoch: 85 | Batch: 011 / 016 | Total loss: 4.220 | Reg loss: 0.032 | Tree loss: 4.220 | Accuracy: 0.097656 | 0.058 sec/iter\n",
      "Epoch: 85 | Batch: 012 / 016 | Total loss: 4.213 | Reg loss: 0.032 | Tree loss: 4.213 | Accuracy: 0.103516 | 0.058 sec/iter\n",
      "Epoch: 85 | Batch: 013 / 016 | Total loss: 4.228 | Reg loss: 0.032 | Tree loss: 4.228 | Accuracy: 0.083984 | 0.058 sec/iter\n",
      "Epoch: 85 | Batch: 014 / 016 | Total loss: 4.146 | Reg loss: 0.032 | Tree loss: 4.146 | Accuracy: 0.099609 | 0.058 sec/iter\n",
      "Epoch: 85 | Batch: 015 / 016 | Total loss: 4.084 | Reg loss: 0.032 | Tree loss: 4.084 | Accuracy: 0.117521 | 0.058 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 86 | Batch: 000 / 016 | Total loss: 4.324 | Reg loss: 0.032 | Tree loss: 4.324 | Accuracy: 0.115234 | 0.058 sec/iter\n",
      "Epoch: 86 | Batch: 001 / 016 | Total loss: 4.397 | Reg loss: 0.032 | Tree loss: 4.397 | Accuracy: 0.097656 | 0.058 sec/iter\n",
      "Epoch: 86 | Batch: 002 / 016 | Total loss: 4.272 | Reg loss: 0.032 | Tree loss: 4.272 | Accuracy: 0.126953 | 0.058 sec/iter\n",
      "Epoch: 86 | Batch: 003 / 016 | Total loss: 4.319 | Reg loss: 0.032 | Tree loss: 4.319 | Accuracy: 0.101562 | 0.058 sec/iter\n",
      "Epoch: 86 | Batch: 004 / 016 | Total loss: 4.249 | Reg loss: 0.032 | Tree loss: 4.249 | Accuracy: 0.121094 | 0.058 sec/iter\n",
      "Epoch: 86 | Batch: 005 / 016 | Total loss: 4.284 | Reg loss: 0.032 | Tree loss: 4.284 | Accuracy: 0.093750 | 0.058 sec/iter\n",
      "Epoch: 86 | Batch: 006 / 016 | Total loss: 4.304 | Reg loss: 0.032 | Tree loss: 4.304 | Accuracy: 0.097656 | 0.058 sec/iter\n",
      "Epoch: 86 | Batch: 007 / 016 | Total loss: 4.228 | Reg loss: 0.032 | Tree loss: 4.228 | Accuracy: 0.109375 | 0.058 sec/iter\n",
      "Epoch: 86 | Batch: 008 / 016 | Total loss: 4.195 | Reg loss: 0.032 | Tree loss: 4.195 | Accuracy: 0.126953 | 0.058 sec/iter\n",
      "Epoch: 86 | Batch: 009 / 016 | Total loss: 4.222 | Reg loss: 0.032 | Tree loss: 4.222 | Accuracy: 0.105469 | 0.058 sec/iter\n",
      "Epoch: 86 | Batch: 010 / 016 | Total loss: 4.183 | Reg loss: 0.032 | Tree loss: 4.183 | Accuracy: 0.101562 | 0.058 sec/iter\n",
      "Epoch: 86 | Batch: 011 / 016 | Total loss: 4.251 | Reg loss: 0.032 | Tree loss: 4.251 | Accuracy: 0.091797 | 0.058 sec/iter\n",
      "Epoch: 86 | Batch: 012 / 016 | Total loss: 4.185 | Reg loss: 0.032 | Tree loss: 4.185 | Accuracy: 0.113281 | 0.058 sec/iter\n",
      "Epoch: 86 | Batch: 013 / 016 | Total loss: 4.236 | Reg loss: 0.032 | Tree loss: 4.236 | Accuracy: 0.076172 | 0.058 sec/iter\n",
      "Epoch: 86 | Batch: 014 / 016 | Total loss: 4.145 | Reg loss: 0.032 | Tree loss: 4.145 | Accuracy: 0.109375 | 0.058 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 86 | Batch: 015 / 016 | Total loss: 4.110 | Reg loss: 0.032 | Tree loss: 4.110 | Accuracy: 0.119658 | 0.058 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 87 | Batch: 000 / 016 | Total loss: 4.260 | Reg loss: 0.032 | Tree loss: 4.260 | Accuracy: 0.113281 | 0.058 sec/iter\n",
      "Epoch: 87 | Batch: 001 / 016 | Total loss: 4.387 | Reg loss: 0.032 | Tree loss: 4.387 | Accuracy: 0.083984 | 0.058 sec/iter\n",
      "Epoch: 87 | Batch: 002 / 016 | Total loss: 4.282 | Reg loss: 0.032 | Tree loss: 4.282 | Accuracy: 0.132812 | 0.058 sec/iter\n",
      "Epoch: 87 | Batch: 003 / 016 | Total loss: 4.293 | Reg loss: 0.032 | Tree loss: 4.293 | Accuracy: 0.126953 | 0.058 sec/iter\n",
      "Epoch: 87 | Batch: 004 / 016 | Total loss: 4.365 | Reg loss: 0.032 | Tree loss: 4.365 | Accuracy: 0.105469 | 0.058 sec/iter\n",
      "Epoch: 87 | Batch: 005 / 016 | Total loss: 4.278 | Reg loss: 0.032 | Tree loss: 4.278 | Accuracy: 0.119141 | 0.058 sec/iter\n",
      "Epoch: 87 | Batch: 006 / 016 | Total loss: 4.285 | Reg loss: 0.032 | Tree loss: 4.285 | Accuracy: 0.083984 | 0.058 sec/iter\n",
      "Epoch: 87 | Batch: 007 / 016 | Total loss: 4.267 | Reg loss: 0.032 | Tree loss: 4.267 | Accuracy: 0.113281 | 0.058 sec/iter\n",
      "Epoch: 87 | Batch: 008 / 016 | Total loss: 4.205 | Reg loss: 0.032 | Tree loss: 4.205 | Accuracy: 0.097656 | 0.058 sec/iter\n",
      "Epoch: 87 | Batch: 009 / 016 | Total loss: 4.235 | Reg loss: 0.032 | Tree loss: 4.235 | Accuracy: 0.091797 | 0.058 sec/iter\n",
      "Epoch: 87 | Batch: 010 / 016 | Total loss: 4.151 | Reg loss: 0.032 | Tree loss: 4.151 | Accuracy: 0.089844 | 0.058 sec/iter\n",
      "Epoch: 87 | Batch: 011 / 016 | Total loss: 4.180 | Reg loss: 0.032 | Tree loss: 4.180 | Accuracy: 0.097656 | 0.058 sec/iter\n",
      "Epoch: 87 | Batch: 012 / 016 | Total loss: 4.178 | Reg loss: 0.032 | Tree loss: 4.178 | Accuracy: 0.097656 | 0.058 sec/iter\n",
      "Epoch: 87 | Batch: 013 / 016 | Total loss: 4.135 | Reg loss: 0.032 | Tree loss: 4.135 | Accuracy: 0.111328 | 0.058 sec/iter\n",
      "Epoch: 87 | Batch: 014 / 016 | Total loss: 4.132 | Reg loss: 0.032 | Tree loss: 4.132 | Accuracy: 0.123047 | 0.058 sec/iter\n",
      "Epoch: 87 | Batch: 015 / 016 | Total loss: 4.107 | Reg loss: 0.032 | Tree loss: 4.107 | Accuracy: 0.119658 | 0.058 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 88 | Batch: 000 / 016 | Total loss: 4.337 | Reg loss: 0.032 | Tree loss: 4.337 | Accuracy: 0.093750 | 0.058 sec/iter\n",
      "Epoch: 88 | Batch: 001 / 016 | Total loss: 4.322 | Reg loss: 0.032 | Tree loss: 4.322 | Accuracy: 0.099609 | 0.058 sec/iter\n",
      "Epoch: 88 | Batch: 002 / 016 | Total loss: 4.328 | Reg loss: 0.032 | Tree loss: 4.328 | Accuracy: 0.113281 | 0.058 sec/iter\n",
      "Epoch: 88 | Batch: 003 / 016 | Total loss: 4.313 | Reg loss: 0.032 | Tree loss: 4.313 | Accuracy: 0.091797 | 0.058 sec/iter\n",
      "Epoch: 88 | Batch: 004 / 016 | Total loss: 4.280 | Reg loss: 0.032 | Tree loss: 4.280 | Accuracy: 0.111328 | 0.058 sec/iter\n",
      "Epoch: 88 | Batch: 005 / 016 | Total loss: 4.301 | Reg loss: 0.032 | Tree loss: 4.301 | Accuracy: 0.078125 | 0.058 sec/iter\n",
      "Epoch: 88 | Batch: 006 / 016 | Total loss: 4.262 | Reg loss: 0.032 | Tree loss: 4.262 | Accuracy: 0.117188 | 0.058 sec/iter\n",
      "Epoch: 88 | Batch: 007 / 016 | Total loss: 4.190 | Reg loss: 0.032 | Tree loss: 4.190 | Accuracy: 0.111328 | 0.058 sec/iter\n",
      "Epoch: 88 | Batch: 008 / 016 | Total loss: 4.211 | Reg loss: 0.032 | Tree loss: 4.211 | Accuracy: 0.101562 | 0.058 sec/iter\n",
      "Epoch: 88 | Batch: 009 / 016 | Total loss: 4.167 | Reg loss: 0.032 | Tree loss: 4.167 | Accuracy: 0.125000 | 0.058 sec/iter\n",
      "Epoch: 88 | Batch: 010 / 016 | Total loss: 4.174 | Reg loss: 0.032 | Tree loss: 4.174 | Accuracy: 0.099609 | 0.058 sec/iter\n",
      "Epoch: 88 | Batch: 011 / 016 | Total loss: 4.192 | Reg loss: 0.032 | Tree loss: 4.192 | Accuracy: 0.115234 | 0.058 sec/iter\n",
      "Epoch: 88 | Batch: 012 / 016 | Total loss: 4.182 | Reg loss: 0.032 | Tree loss: 4.182 | Accuracy: 0.101562 | 0.058 sec/iter\n",
      "Epoch: 88 | Batch: 013 / 016 | Total loss: 4.113 | Reg loss: 0.032 | Tree loss: 4.113 | Accuracy: 0.119141 | 0.058 sec/iter\n",
      "Epoch: 88 | Batch: 014 / 016 | Total loss: 4.122 | Reg loss: 0.032 | Tree loss: 4.122 | Accuracy: 0.091797 | 0.058 sec/iter\n",
      "Epoch: 88 | Batch: 015 / 016 | Total loss: 4.090 | Reg loss: 0.032 | Tree loss: 4.090 | Accuracy: 0.138889 | 0.058 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 89 | Batch: 000 / 016 | Total loss: 4.307 | Reg loss: 0.032 | Tree loss: 4.307 | Accuracy: 0.107422 | 0.058 sec/iter\n",
      "Epoch: 89 | Batch: 001 / 016 | Total loss: 4.304 | Reg loss: 0.032 | Tree loss: 4.304 | Accuracy: 0.126953 | 0.058 sec/iter\n",
      "Epoch: 89 | Batch: 002 / 016 | Total loss: 4.300 | Reg loss: 0.032 | Tree loss: 4.300 | Accuracy: 0.089844 | 0.058 sec/iter\n",
      "Epoch: 89 | Batch: 003 / 016 | Total loss: 4.277 | Reg loss: 0.032 | Tree loss: 4.277 | Accuracy: 0.109375 | 0.058 sec/iter\n",
      "Epoch: 89 | Batch: 004 / 016 | Total loss: 4.370 | Reg loss: 0.032 | Tree loss: 4.370 | Accuracy: 0.085938 | 0.058 sec/iter\n",
      "Epoch: 89 | Batch: 005 / 016 | Total loss: 4.264 | Reg loss: 0.032 | Tree loss: 4.264 | Accuracy: 0.074219 | 0.058 sec/iter\n",
      "Epoch: 89 | Batch: 006 / 016 | Total loss: 4.223 | Reg loss: 0.032 | Tree loss: 4.223 | Accuracy: 0.113281 | 0.058 sec/iter\n",
      "Epoch: 89 | Batch: 007 / 016 | Total loss: 4.241 | Reg loss: 0.032 | Tree loss: 4.241 | Accuracy: 0.113281 | 0.058 sec/iter\n",
      "Epoch: 89 | Batch: 008 / 016 | Total loss: 4.157 | Reg loss: 0.032 | Tree loss: 4.157 | Accuracy: 0.126953 | 0.058 sec/iter\n",
      "Epoch: 89 | Batch: 009 / 016 | Total loss: 4.076 | Reg loss: 0.032 | Tree loss: 4.076 | Accuracy: 0.150391 | 0.058 sec/iter\n",
      "Epoch: 89 | Batch: 010 / 016 | Total loss: 4.185 | Reg loss: 0.032 | Tree loss: 4.185 | Accuracy: 0.113281 | 0.058 sec/iter\n",
      "Epoch: 89 | Batch: 011 / 016 | Total loss: 4.234 | Reg loss: 0.032 | Tree loss: 4.234 | Accuracy: 0.085938 | 0.058 sec/iter\n",
      "Epoch: 89 | Batch: 012 / 016 | Total loss: 4.126 | Reg loss: 0.032 | Tree loss: 4.126 | Accuracy: 0.119141 | 0.058 sec/iter\n",
      "Epoch: 89 | Batch: 013 / 016 | Total loss: 4.089 | Reg loss: 0.032 | Tree loss: 4.089 | Accuracy: 0.101562 | 0.058 sec/iter\n",
      "Epoch: 89 | Batch: 014 / 016 | Total loss: 4.142 | Reg loss: 0.032 | Tree loss: 4.142 | Accuracy: 0.085938 | 0.058 sec/iter\n",
      "Epoch: 89 | Batch: 015 / 016 | Total loss: 4.139 | Reg loss: 0.032 | Tree loss: 4.139 | Accuracy: 0.102564 | 0.058 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 90 | Batch: 000 / 016 | Total loss: 4.338 | Reg loss: 0.032 | Tree loss: 4.338 | Accuracy: 0.099609 | 0.058 sec/iter\n",
      "Epoch: 90 | Batch: 001 / 016 | Total loss: 4.305 | Reg loss: 0.032 | Tree loss: 4.305 | Accuracy: 0.107422 | 0.058 sec/iter\n",
      "Epoch: 90 | Batch: 002 / 016 | Total loss: 4.263 | Reg loss: 0.032 | Tree loss: 4.263 | Accuracy: 0.113281 | 0.058 sec/iter\n",
      "Epoch: 90 | Batch: 003 / 016 | Total loss: 4.238 | Reg loss: 0.032 | Tree loss: 4.238 | Accuracy: 0.117188 | 0.058 sec/iter\n",
      "Epoch: 90 | Batch: 004 / 016 | Total loss: 4.217 | Reg loss: 0.032 | Tree loss: 4.217 | Accuracy: 0.138672 | 0.058 sec/iter\n",
      "Epoch: 90 | Batch: 005 / 016 | Total loss: 4.197 | Reg loss: 0.032 | Tree loss: 4.197 | Accuracy: 0.101562 | 0.058 sec/iter\n",
      "Epoch: 90 | Batch: 006 / 016 | Total loss: 4.241 | Reg loss: 0.032 | Tree loss: 4.241 | Accuracy: 0.091797 | 0.058 sec/iter\n",
      "Epoch: 90 | Batch: 007 / 016 | Total loss: 4.233 | Reg loss: 0.032 | Tree loss: 4.233 | Accuracy: 0.105469 | 0.058 sec/iter\n",
      "Epoch: 90 | Batch: 008 / 016 | Total loss: 4.150 | Reg loss: 0.032 | Tree loss: 4.150 | Accuracy: 0.113281 | 0.058 sec/iter\n",
      "Epoch: 90 | Batch: 009 / 016 | Total loss: 4.155 | Reg loss: 0.032 | Tree loss: 4.155 | Accuracy: 0.138672 | 0.058 sec/iter\n",
      "Epoch: 90 | Batch: 010 / 016 | Total loss: 4.196 | Reg loss: 0.032 | Tree loss: 4.196 | Accuracy: 0.097656 | 0.058 sec/iter\n",
      "Epoch: 90 | Batch: 011 / 016 | Total loss: 4.102 | Reg loss: 0.032 | Tree loss: 4.102 | Accuracy: 0.111328 | 0.058 sec/iter\n",
      "Epoch: 90 | Batch: 012 / 016 | Total loss: 4.189 | Reg loss: 0.032 | Tree loss: 4.189 | Accuracy: 0.089844 | 0.058 sec/iter\n",
      "Epoch: 90 | Batch: 013 / 016 | Total loss: 4.149 | Reg loss: 0.032 | Tree loss: 4.149 | Accuracy: 0.093750 | 0.058 sec/iter\n",
      "Epoch: 90 | Batch: 014 / 016 | Total loss: 4.157 | Reg loss: 0.032 | Tree loss: 4.157 | Accuracy: 0.083984 | 0.058 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 90 | Batch: 015 / 016 | Total loss: 4.153 | Reg loss: 0.032 | Tree loss: 4.153 | Accuracy: 0.102564 | 0.058 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 91 | Batch: 000 / 016 | Total loss: 4.259 | Reg loss: 0.032 | Tree loss: 4.259 | Accuracy: 0.119141 | 0.058 sec/iter\n",
      "Epoch: 91 | Batch: 001 / 016 | Total loss: 4.310 | Reg loss: 0.032 | Tree loss: 4.310 | Accuracy: 0.097656 | 0.058 sec/iter\n",
      "Epoch: 91 | Batch: 002 / 016 | Total loss: 4.260 | Reg loss: 0.032 | Tree loss: 4.260 | Accuracy: 0.093750 | 0.058 sec/iter\n",
      "Epoch: 91 | Batch: 003 / 016 | Total loss: 4.216 | Reg loss: 0.032 | Tree loss: 4.216 | Accuracy: 0.125000 | 0.058 sec/iter\n",
      "Epoch: 91 | Batch: 004 / 016 | Total loss: 4.278 | Reg loss: 0.032 | Tree loss: 4.278 | Accuracy: 0.085938 | 0.058 sec/iter\n",
      "Epoch: 91 | Batch: 005 / 016 | Total loss: 4.231 | Reg loss: 0.032 | Tree loss: 4.231 | Accuracy: 0.126953 | 0.058 sec/iter\n",
      "Epoch: 91 | Batch: 006 / 016 | Total loss: 4.269 | Reg loss: 0.032 | Tree loss: 4.269 | Accuracy: 0.101562 | 0.058 sec/iter\n",
      "Epoch: 91 | Batch: 007 / 016 | Total loss: 4.180 | Reg loss: 0.032 | Tree loss: 4.180 | Accuracy: 0.109375 | 0.058 sec/iter\n",
      "Epoch: 91 | Batch: 008 / 016 | Total loss: 4.220 | Reg loss: 0.032 | Tree loss: 4.220 | Accuracy: 0.093750 | 0.058 sec/iter\n",
      "Epoch: 91 | Batch: 009 / 016 | Total loss: 4.206 | Reg loss: 0.032 | Tree loss: 4.206 | Accuracy: 0.095703 | 0.058 sec/iter\n",
      "Epoch: 91 | Batch: 010 / 016 | Total loss: 4.109 | Reg loss: 0.032 | Tree loss: 4.109 | Accuracy: 0.115234 | 0.058 sec/iter\n",
      "Epoch: 91 | Batch: 011 / 016 | Total loss: 4.090 | Reg loss: 0.032 | Tree loss: 4.090 | Accuracy: 0.130859 | 0.058 sec/iter\n",
      "Epoch: 91 | Batch: 012 / 016 | Total loss: 4.110 | Reg loss: 0.032 | Tree loss: 4.110 | Accuracy: 0.119141 | 0.058 sec/iter\n",
      "Epoch: 91 | Batch: 013 / 016 | Total loss: 4.095 | Reg loss: 0.032 | Tree loss: 4.095 | Accuracy: 0.097656 | 0.058 sec/iter\n",
      "Epoch: 91 | Batch: 014 / 016 | Total loss: 4.146 | Reg loss: 0.032 | Tree loss: 4.146 | Accuracy: 0.093750 | 0.058 sec/iter\n",
      "Epoch: 91 | Batch: 015 / 016 | Total loss: 4.160 | Reg loss: 0.032 | Tree loss: 4.160 | Accuracy: 0.100427 | 0.058 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 92 | Batch: 000 / 016 | Total loss: 4.278 | Reg loss: 0.032 | Tree loss: 4.278 | Accuracy: 0.085938 | 0.058 sec/iter\n",
      "Epoch: 92 | Batch: 001 / 016 | Total loss: 4.319 | Reg loss: 0.032 | Tree loss: 4.319 | Accuracy: 0.109375 | 0.058 sec/iter\n",
      "Epoch: 92 | Batch: 002 / 016 | Total loss: 4.284 | Reg loss: 0.032 | Tree loss: 4.284 | Accuracy: 0.080078 | 0.058 sec/iter\n",
      "Epoch: 92 | Batch: 003 / 016 | Total loss: 4.319 | Reg loss: 0.032 | Tree loss: 4.319 | Accuracy: 0.095703 | 0.058 sec/iter\n",
      "Epoch: 92 | Batch: 004 / 016 | Total loss: 4.266 | Reg loss: 0.032 | Tree loss: 4.266 | Accuracy: 0.083984 | 0.058 sec/iter\n",
      "Epoch: 92 | Batch: 005 / 016 | Total loss: 4.276 | Reg loss: 0.032 | Tree loss: 4.276 | Accuracy: 0.091797 | 0.058 sec/iter\n",
      "Epoch: 92 | Batch: 006 / 016 | Total loss: 4.153 | Reg loss: 0.032 | Tree loss: 4.153 | Accuracy: 0.150391 | 0.058 sec/iter\n",
      "Epoch: 92 | Batch: 007 / 016 | Total loss: 4.211 | Reg loss: 0.032 | Tree loss: 4.211 | Accuracy: 0.097656 | 0.058 sec/iter\n",
      "Epoch: 92 | Batch: 008 / 016 | Total loss: 4.197 | Reg loss: 0.032 | Tree loss: 4.197 | Accuracy: 0.101562 | 0.058 sec/iter\n",
      "Epoch: 92 | Batch: 009 / 016 | Total loss: 4.144 | Reg loss: 0.032 | Tree loss: 4.144 | Accuracy: 0.095703 | 0.058 sec/iter\n",
      "Epoch: 92 | Batch: 010 / 016 | Total loss: 4.141 | Reg loss: 0.032 | Tree loss: 4.141 | Accuracy: 0.117188 | 0.058 sec/iter\n",
      "Epoch: 92 | Batch: 011 / 016 | Total loss: 4.084 | Reg loss: 0.032 | Tree loss: 4.084 | Accuracy: 0.136719 | 0.058 sec/iter\n",
      "Epoch: 92 | Batch: 012 / 016 | Total loss: 4.070 | Reg loss: 0.032 | Tree loss: 4.070 | Accuracy: 0.117188 | 0.058 sec/iter\n",
      "Epoch: 92 | Batch: 013 / 016 | Total loss: 4.040 | Reg loss: 0.032 | Tree loss: 4.040 | Accuracy: 0.121094 | 0.058 sec/iter\n",
      "Epoch: 92 | Batch: 014 / 016 | Total loss: 4.066 | Reg loss: 0.032 | Tree loss: 4.066 | Accuracy: 0.126953 | 0.058 sec/iter\n",
      "Epoch: 92 | Batch: 015 / 016 | Total loss: 4.154 | Reg loss: 0.032 | Tree loss: 4.154 | Accuracy: 0.096154 | 0.058 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 93 | Batch: 000 / 016 | Total loss: 4.348 | Reg loss: 0.032 | Tree loss: 4.348 | Accuracy: 0.087891 | 0.058 sec/iter\n",
      "Epoch: 93 | Batch: 001 / 016 | Total loss: 4.208 | Reg loss: 0.032 | Tree loss: 4.208 | Accuracy: 0.111328 | 0.058 sec/iter\n",
      "Epoch: 93 | Batch: 002 / 016 | Total loss: 4.260 | Reg loss: 0.032 | Tree loss: 4.260 | Accuracy: 0.117188 | 0.058 sec/iter\n",
      "Epoch: 93 | Batch: 003 / 016 | Total loss: 4.211 | Reg loss: 0.032 | Tree loss: 4.211 | Accuracy: 0.119141 | 0.058 sec/iter\n",
      "Epoch: 93 | Batch: 004 / 016 | Total loss: 4.334 | Reg loss: 0.032 | Tree loss: 4.334 | Accuracy: 0.085938 | 0.058 sec/iter\n",
      "Epoch: 93 | Batch: 005 / 016 | Total loss: 4.229 | Reg loss: 0.032 | Tree loss: 4.229 | Accuracy: 0.093750 | 0.058 sec/iter\n",
      "Epoch: 93 | Batch: 006 / 016 | Total loss: 4.163 | Reg loss: 0.032 | Tree loss: 4.163 | Accuracy: 0.115234 | 0.058 sec/iter\n",
      "Epoch: 93 | Batch: 007 / 016 | Total loss: 4.170 | Reg loss: 0.032 | Tree loss: 4.170 | Accuracy: 0.125000 | 0.058 sec/iter\n",
      "Epoch: 93 | Batch: 008 / 016 | Total loss: 4.181 | Reg loss: 0.032 | Tree loss: 4.181 | Accuracy: 0.101562 | 0.058 sec/iter\n",
      "Epoch: 93 | Batch: 009 / 016 | Total loss: 4.164 | Reg loss: 0.032 | Tree loss: 4.164 | Accuracy: 0.087891 | 0.058 sec/iter\n",
      "Epoch: 93 | Batch: 010 / 016 | Total loss: 4.137 | Reg loss: 0.032 | Tree loss: 4.137 | Accuracy: 0.126953 | 0.058 sec/iter\n",
      "Epoch: 93 | Batch: 011 / 016 | Total loss: 4.113 | Reg loss: 0.032 | Tree loss: 4.113 | Accuracy: 0.093750 | 0.058 sec/iter\n",
      "Epoch: 93 | Batch: 012 / 016 | Total loss: 4.054 | Reg loss: 0.032 | Tree loss: 4.054 | Accuracy: 0.123047 | 0.058 sec/iter\n",
      "Epoch: 93 | Batch: 013 / 016 | Total loss: 4.100 | Reg loss: 0.032 | Tree loss: 4.100 | Accuracy: 0.105469 | 0.058 sec/iter\n",
      "Epoch: 93 | Batch: 014 / 016 | Total loss: 4.050 | Reg loss: 0.032 | Tree loss: 4.050 | Accuracy: 0.101562 | 0.058 sec/iter\n",
      "Epoch: 93 | Batch: 015 / 016 | Total loss: 4.021 | Reg loss: 0.032 | Tree loss: 4.021 | Accuracy: 0.111111 | 0.058 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 94 | Batch: 000 / 016 | Total loss: 4.306 | Reg loss: 0.032 | Tree loss: 4.306 | Accuracy: 0.103516 | 0.058 sec/iter\n",
      "Epoch: 94 | Batch: 001 / 016 | Total loss: 4.224 | Reg loss: 0.032 | Tree loss: 4.224 | Accuracy: 0.125000 | 0.058 sec/iter\n",
      "Epoch: 94 | Batch: 002 / 016 | Total loss: 4.225 | Reg loss: 0.032 | Tree loss: 4.225 | Accuracy: 0.097656 | 0.058 sec/iter\n",
      "Epoch: 94 | Batch: 003 / 016 | Total loss: 4.146 | Reg loss: 0.032 | Tree loss: 4.146 | Accuracy: 0.111328 | 0.058 sec/iter\n",
      "Epoch: 94 | Batch: 004 / 016 | Total loss: 4.187 | Reg loss: 0.032 | Tree loss: 4.187 | Accuracy: 0.089844 | 0.058 sec/iter\n",
      "Epoch: 94 | Batch: 005 / 016 | Total loss: 4.281 | Reg loss: 0.032 | Tree loss: 4.281 | Accuracy: 0.076172 | 0.058 sec/iter\n",
      "Epoch: 94 | Batch: 006 / 016 | Total loss: 4.170 | Reg loss: 0.032 | Tree loss: 4.170 | Accuracy: 0.117188 | 0.058 sec/iter\n",
      "Epoch: 94 | Batch: 007 / 016 | Total loss: 4.170 | Reg loss: 0.032 | Tree loss: 4.170 | Accuracy: 0.099609 | 0.058 sec/iter\n",
      "Epoch: 94 | Batch: 008 / 016 | Total loss: 4.191 | Reg loss: 0.032 | Tree loss: 4.191 | Accuracy: 0.103516 | 0.058 sec/iter\n",
      "Epoch: 94 | Batch: 009 / 016 | Total loss: 4.053 | Reg loss: 0.032 | Tree loss: 4.053 | Accuracy: 0.128906 | 0.058 sec/iter\n",
      "Epoch: 94 | Batch: 010 / 016 | Total loss: 4.080 | Reg loss: 0.032 | Tree loss: 4.080 | Accuracy: 0.105469 | 0.058 sec/iter\n",
      "Epoch: 94 | Batch: 011 / 016 | Total loss: 4.137 | Reg loss: 0.032 | Tree loss: 4.137 | Accuracy: 0.115234 | 0.058 sec/iter\n",
      "Epoch: 94 | Batch: 012 / 016 | Total loss: 4.052 | Reg loss: 0.032 | Tree loss: 4.052 | Accuracy: 0.107422 | 0.058 sec/iter\n",
      "Epoch: 94 | Batch: 013 / 016 | Total loss: 4.090 | Reg loss: 0.032 | Tree loss: 4.090 | Accuracy: 0.087891 | 0.058 sec/iter\n",
      "Epoch: 94 | Batch: 014 / 016 | Total loss: 4.048 | Reg loss: 0.032 | Tree loss: 4.048 | Accuracy: 0.140625 | 0.058 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 94 | Batch: 015 / 016 | Total loss: 4.160 | Reg loss: 0.032 | Tree loss: 4.160 | Accuracy: 0.094017 | 0.058 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 95 | Batch: 000 / 016 | Total loss: 4.283 | Reg loss: 0.032 | Tree loss: 4.283 | Accuracy: 0.105469 | 0.058 sec/iter\n",
      "Epoch: 95 | Batch: 001 / 016 | Total loss: 4.199 | Reg loss: 0.032 | Tree loss: 4.199 | Accuracy: 0.103516 | 0.058 sec/iter\n",
      "Epoch: 95 | Batch: 002 / 016 | Total loss: 4.191 | Reg loss: 0.032 | Tree loss: 4.191 | Accuracy: 0.115234 | 0.058 sec/iter\n",
      "Epoch: 95 | Batch: 003 / 016 | Total loss: 4.202 | Reg loss: 0.032 | Tree loss: 4.202 | Accuracy: 0.107422 | 0.058 sec/iter\n",
      "Epoch: 95 | Batch: 004 / 016 | Total loss: 4.152 | Reg loss: 0.032 | Tree loss: 4.152 | Accuracy: 0.109375 | 0.058 sec/iter\n",
      "Epoch: 95 | Batch: 005 / 016 | Total loss: 4.247 | Reg loss: 0.032 | Tree loss: 4.247 | Accuracy: 0.087891 | 0.058 sec/iter\n",
      "Epoch: 95 | Batch: 006 / 016 | Total loss: 4.176 | Reg loss: 0.032 | Tree loss: 4.176 | Accuracy: 0.109375 | 0.058 sec/iter\n",
      "Epoch: 95 | Batch: 007 / 016 | Total loss: 4.165 | Reg loss: 0.032 | Tree loss: 4.165 | Accuracy: 0.103516 | 0.058 sec/iter\n",
      "Epoch: 95 | Batch: 008 / 016 | Total loss: 4.091 | Reg loss: 0.032 | Tree loss: 4.091 | Accuracy: 0.119141 | 0.058 sec/iter\n",
      "Epoch: 95 | Batch: 009 / 016 | Total loss: 4.107 | Reg loss: 0.032 | Tree loss: 4.107 | Accuracy: 0.113281 | 0.058 sec/iter\n",
      "Epoch: 95 | Batch: 010 / 016 | Total loss: 4.139 | Reg loss: 0.032 | Tree loss: 4.139 | Accuracy: 0.117188 | 0.058 sec/iter\n",
      "Epoch: 95 | Batch: 011 / 016 | Total loss: 4.191 | Reg loss: 0.032 | Tree loss: 4.191 | Accuracy: 0.093750 | 0.058 sec/iter\n",
      "Epoch: 95 | Batch: 012 / 016 | Total loss: 4.042 | Reg loss: 0.032 | Tree loss: 4.042 | Accuracy: 0.125000 | 0.058 sec/iter\n",
      "Epoch: 95 | Batch: 013 / 016 | Total loss: 3.987 | Reg loss: 0.032 | Tree loss: 3.987 | Accuracy: 0.109375 | 0.058 sec/iter\n",
      "Epoch: 95 | Batch: 014 / 016 | Total loss: 4.068 | Reg loss: 0.032 | Tree loss: 4.068 | Accuracy: 0.085938 | 0.058 sec/iter\n",
      "Epoch: 95 | Batch: 015 / 016 | Total loss: 4.037 | Reg loss: 0.032 | Tree loss: 4.037 | Accuracy: 0.104701 | 0.058 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 96 | Batch: 000 / 016 | Total loss: 4.268 | Reg loss: 0.032 | Tree loss: 4.268 | Accuracy: 0.095703 | 0.058 sec/iter\n",
      "Epoch: 96 | Batch: 001 / 016 | Total loss: 4.164 | Reg loss: 0.032 | Tree loss: 4.164 | Accuracy: 0.109375 | 0.058 sec/iter\n",
      "Epoch: 96 | Batch: 002 / 016 | Total loss: 4.194 | Reg loss: 0.032 | Tree loss: 4.194 | Accuracy: 0.121094 | 0.058 sec/iter\n",
      "Epoch: 96 | Batch: 003 / 016 | Total loss: 4.206 | Reg loss: 0.032 | Tree loss: 4.206 | Accuracy: 0.113281 | 0.058 sec/iter\n",
      "Epoch: 96 | Batch: 004 / 016 | Total loss: 4.255 | Reg loss: 0.032 | Tree loss: 4.255 | Accuracy: 0.089844 | 0.058 sec/iter\n",
      "Epoch: 96 | Batch: 005 / 016 | Total loss: 4.195 | Reg loss: 0.032 | Tree loss: 4.195 | Accuracy: 0.089844 | 0.058 sec/iter\n",
      "Epoch: 96 | Batch: 006 / 016 | Total loss: 4.125 | Reg loss: 0.032 | Tree loss: 4.125 | Accuracy: 0.103516 | 0.058 sec/iter\n",
      "Epoch: 96 | Batch: 007 / 016 | Total loss: 4.095 | Reg loss: 0.032 | Tree loss: 4.095 | Accuracy: 0.109375 | 0.058 sec/iter\n",
      "Epoch: 96 | Batch: 008 / 016 | Total loss: 4.099 | Reg loss: 0.032 | Tree loss: 4.099 | Accuracy: 0.119141 | 0.058 sec/iter\n",
      "Epoch: 96 | Batch: 009 / 016 | Total loss: 4.082 | Reg loss: 0.032 | Tree loss: 4.082 | Accuracy: 0.117188 | 0.058 sec/iter\n",
      "Epoch: 96 | Batch: 010 / 016 | Total loss: 4.066 | Reg loss: 0.032 | Tree loss: 4.066 | Accuracy: 0.115234 | 0.058 sec/iter\n",
      "Epoch: 96 | Batch: 011 / 016 | Total loss: 4.132 | Reg loss: 0.032 | Tree loss: 4.132 | Accuracy: 0.113281 | 0.058 sec/iter\n",
      "Epoch: 96 | Batch: 012 / 016 | Total loss: 4.018 | Reg loss: 0.032 | Tree loss: 4.018 | Accuracy: 0.103516 | 0.058 sec/iter\n",
      "Epoch: 96 | Batch: 013 / 016 | Total loss: 4.009 | Reg loss: 0.032 | Tree loss: 4.009 | Accuracy: 0.113281 | 0.058 sec/iter\n",
      "Epoch: 96 | Batch: 014 / 016 | Total loss: 4.094 | Reg loss: 0.032 | Tree loss: 4.094 | Accuracy: 0.095703 | 0.058 sec/iter\n",
      "Epoch: 96 | Batch: 015 / 016 | Total loss: 4.048 | Reg loss: 0.032 | Tree loss: 4.048 | Accuracy: 0.098291 | 0.058 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 97 | Batch: 000 / 016 | Total loss: 4.169 | Reg loss: 0.032 | Tree loss: 4.169 | Accuracy: 0.105469 | 0.058 sec/iter\n",
      "Epoch: 97 | Batch: 001 / 016 | Total loss: 4.146 | Reg loss: 0.032 | Tree loss: 4.146 | Accuracy: 0.115234 | 0.058 sec/iter\n",
      "Epoch: 97 | Batch: 002 / 016 | Total loss: 4.247 | Reg loss: 0.032 | Tree loss: 4.247 | Accuracy: 0.103516 | 0.058 sec/iter\n",
      "Epoch: 97 | Batch: 003 / 016 | Total loss: 4.112 | Reg loss: 0.032 | Tree loss: 4.112 | Accuracy: 0.128906 | 0.058 sec/iter\n",
      "Epoch: 97 | Batch: 004 / 016 | Total loss: 4.293 | Reg loss: 0.032 | Tree loss: 4.293 | Accuracy: 0.087891 | 0.058 sec/iter\n",
      "Epoch: 97 | Batch: 005 / 016 | Total loss: 4.102 | Reg loss: 0.032 | Tree loss: 4.102 | Accuracy: 0.119141 | 0.058 sec/iter\n",
      "Epoch: 97 | Batch: 006 / 016 | Total loss: 4.116 | Reg loss: 0.032 | Tree loss: 4.116 | Accuracy: 0.109375 | 0.058 sec/iter\n",
      "Epoch: 97 | Batch: 007 / 016 | Total loss: 4.064 | Reg loss: 0.032 | Tree loss: 4.064 | Accuracy: 0.119141 | 0.058 sec/iter\n",
      "Epoch: 97 | Batch: 008 / 016 | Total loss: 4.134 | Reg loss: 0.032 | Tree loss: 4.134 | Accuracy: 0.095703 | 0.058 sec/iter\n",
      "Epoch: 97 | Batch: 009 / 016 | Total loss: 4.128 | Reg loss: 0.032 | Tree loss: 4.128 | Accuracy: 0.093750 | 0.058 sec/iter\n",
      "Epoch: 97 | Batch: 010 / 016 | Total loss: 4.071 | Reg loss: 0.032 | Tree loss: 4.071 | Accuracy: 0.091797 | 0.058 sec/iter\n",
      "Epoch: 97 | Batch: 011 / 016 | Total loss: 4.024 | Reg loss: 0.032 | Tree loss: 4.024 | Accuracy: 0.121094 | 0.058 sec/iter\n",
      "Epoch: 97 | Batch: 012 / 016 | Total loss: 4.083 | Reg loss: 0.032 | Tree loss: 4.083 | Accuracy: 0.105469 | 0.058 sec/iter\n",
      "Epoch: 97 | Batch: 013 / 016 | Total loss: 3.988 | Reg loss: 0.032 | Tree loss: 3.988 | Accuracy: 0.113281 | 0.058 sec/iter\n",
      "Epoch: 97 | Batch: 014 / 016 | Total loss: 4.088 | Reg loss: 0.032 | Tree loss: 4.088 | Accuracy: 0.111328 | 0.058 sec/iter\n",
      "Epoch: 97 | Batch: 015 / 016 | Total loss: 4.053 | Reg loss: 0.032 | Tree loss: 4.053 | Accuracy: 0.089744 | 0.058 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 98 | Batch: 000 / 016 | Total loss: 4.132 | Reg loss: 0.032 | Tree loss: 4.132 | Accuracy: 0.128906 | 0.058 sec/iter\n",
      "Epoch: 98 | Batch: 001 / 016 | Total loss: 4.242 | Reg loss: 0.032 | Tree loss: 4.242 | Accuracy: 0.095703 | 0.058 sec/iter\n",
      "Epoch: 98 | Batch: 002 / 016 | Total loss: 4.222 | Reg loss: 0.032 | Tree loss: 4.222 | Accuracy: 0.062500 | 0.058 sec/iter\n",
      "Epoch: 98 | Batch: 003 / 016 | Total loss: 4.177 | Reg loss: 0.032 | Tree loss: 4.177 | Accuracy: 0.111328 | 0.058 sec/iter\n",
      "Epoch: 98 | Batch: 004 / 016 | Total loss: 4.134 | Reg loss: 0.032 | Tree loss: 4.134 | Accuracy: 0.132812 | 0.058 sec/iter\n",
      "Epoch: 98 | Batch: 005 / 016 | Total loss: 4.186 | Reg loss: 0.032 | Tree loss: 4.186 | Accuracy: 0.091797 | 0.058 sec/iter\n",
      "Epoch: 98 | Batch: 006 / 016 | Total loss: 4.135 | Reg loss: 0.032 | Tree loss: 4.135 | Accuracy: 0.093750 | 0.058 sec/iter\n",
      "Epoch: 98 | Batch: 007 / 016 | Total loss: 4.077 | Reg loss: 0.032 | Tree loss: 4.077 | Accuracy: 0.130859 | 0.058 sec/iter\n",
      "Epoch: 98 | Batch: 008 / 016 | Total loss: 4.066 | Reg loss: 0.032 | Tree loss: 4.066 | Accuracy: 0.121094 | 0.058 sec/iter\n",
      "Epoch: 98 | Batch: 009 / 016 | Total loss: 4.091 | Reg loss: 0.032 | Tree loss: 4.091 | Accuracy: 0.091797 | 0.058 sec/iter\n",
      "Epoch: 98 | Batch: 010 / 016 | Total loss: 4.037 | Reg loss: 0.032 | Tree loss: 4.037 | Accuracy: 0.103516 | 0.058 sec/iter\n",
      "Epoch: 98 | Batch: 011 / 016 | Total loss: 4.037 | Reg loss: 0.032 | Tree loss: 4.037 | Accuracy: 0.126953 | 0.058 sec/iter\n",
      "Epoch: 98 | Batch: 012 / 016 | Total loss: 4.000 | Reg loss: 0.032 | Tree loss: 4.000 | Accuracy: 0.097656 | 0.058 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 98 | Batch: 013 / 016 | Total loss: 3.986 | Reg loss: 0.032 | Tree loss: 3.986 | Accuracy: 0.123047 | 0.058 sec/iter\n",
      "Epoch: 98 | Batch: 014 / 016 | Total loss: 4.051 | Reg loss: 0.032 | Tree loss: 4.051 | Accuracy: 0.105469 | 0.058 sec/iter\n",
      "Epoch: 98 | Batch: 015 / 016 | Total loss: 4.014 | Reg loss: 0.032 | Tree loss: 4.014 | Accuracy: 0.094017 | 0.058 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 99 | Batch: 000 / 016 | Total loss: 4.145 | Reg loss: 0.032 | Tree loss: 4.145 | Accuracy: 0.119141 | 0.058 sec/iter\n",
      "Epoch: 99 | Batch: 001 / 016 | Total loss: 4.189 | Reg loss: 0.032 | Tree loss: 4.189 | Accuracy: 0.089844 | 0.058 sec/iter\n",
      "Epoch: 99 | Batch: 002 / 016 | Total loss: 4.146 | Reg loss: 0.032 | Tree loss: 4.146 | Accuracy: 0.113281 | 0.058 sec/iter\n",
      "Epoch: 99 | Batch: 003 / 016 | Total loss: 4.172 | Reg loss: 0.032 | Tree loss: 4.172 | Accuracy: 0.095703 | 0.058 sec/iter\n",
      "Epoch: 99 | Batch: 004 / 016 | Total loss: 4.162 | Reg loss: 0.032 | Tree loss: 4.162 | Accuracy: 0.111328 | 0.058 sec/iter\n",
      "Epoch: 99 | Batch: 005 / 016 | Total loss: 4.046 | Reg loss: 0.032 | Tree loss: 4.046 | Accuracy: 0.105469 | 0.058 sec/iter\n",
      "Epoch: 99 | Batch: 006 / 016 | Total loss: 4.118 | Reg loss: 0.032 | Tree loss: 4.118 | Accuracy: 0.125000 | 0.058 sec/iter\n",
      "Epoch: 99 | Batch: 007 / 016 | Total loss: 4.121 | Reg loss: 0.032 | Tree loss: 4.121 | Accuracy: 0.091797 | 0.058 sec/iter\n",
      "Epoch: 99 | Batch: 008 / 016 | Total loss: 4.044 | Reg loss: 0.032 | Tree loss: 4.044 | Accuracy: 0.097656 | 0.058 sec/iter\n",
      "Epoch: 99 | Batch: 009 / 016 | Total loss: 4.098 | Reg loss: 0.032 | Tree loss: 4.098 | Accuracy: 0.105469 | 0.058 sec/iter\n",
      "Epoch: 99 | Batch: 010 / 016 | Total loss: 4.051 | Reg loss: 0.032 | Tree loss: 4.051 | Accuracy: 0.117188 | 0.058 sec/iter\n",
      "Epoch: 99 | Batch: 011 / 016 | Total loss: 4.039 | Reg loss: 0.032 | Tree loss: 4.039 | Accuracy: 0.115234 | 0.058 sec/iter\n",
      "Epoch: 99 | Batch: 012 / 016 | Total loss: 4.064 | Reg loss: 0.032 | Tree loss: 4.064 | Accuracy: 0.087891 | 0.058 sec/iter\n",
      "Epoch: 99 | Batch: 013 / 016 | Total loss: 3.996 | Reg loss: 0.032 | Tree loss: 3.996 | Accuracy: 0.097656 | 0.058 sec/iter\n",
      "Epoch: 99 | Batch: 014 / 016 | Total loss: 3.991 | Reg loss: 0.032 | Tree loss: 3.991 | Accuracy: 0.115234 | 0.058 sec/iter\n",
      "Epoch: 99 | Batch: 015 / 016 | Total loss: 3.973 | Reg loss: 0.032 | Tree loss: 3.973 | Accuracy: 0.126068 | 0.058 sec/iter\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "iteration = 0\n",
    "for epoch in range(epochs):\n",
    "    # Training\n",
    "    avg_sp = show_sparseness(tree)\n",
    "    sparsity.append(avg_sp)\n",
    "    iteration = do_epoch(tree, tree_loader, device, log_interval, losses, accs, epoch, iteration)\n",
    "    \n",
    "    if epoch % 1 == 0:\n",
    "        prune_tree(tree, factor=3)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4149686b658042a8976213f6e52727c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel('Iteration')\n",
    "plt.plot(accs, label='Accuracy vs iteration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1332d0f13eb421388024ed06ffbd762",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce5c8bfa42294feaa059e767cf1c5758",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel('Iteration')\n",
    "plt.plot(losses, label='Loss vs iteration')\n",
    "plt.yscale(\"log\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "weights = tree.inner_nodes.weight.cpu().detach().numpy().flatten()\n",
    "plt.hist(weights, bins=500)\n",
    "weights_std = np.std(weights)\n",
    "weights_mean = np.mean(weights)\n",
    "plt.axvline(weights_mean + weights_std, color='r')\n",
    "plt.axvline(weights_mean - weights_std, color='r')\n",
    "plt.title(f\"Mean: {weights_mean}   |   STD: {weights_std}\")\n",
    "plt.yscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3916af8658df4fa4891af43f4d72658b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average height: 5.892857142857143\n"
     ]
    }
   ],
   "source": [
    "plt.figure(figsize=(15, 10), dpi=80)\n",
    "avg_height, root = tree.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accumulate samples in the leaves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of patterns: 56\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of patterns: {len(root.get_leaves())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = 'greedy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eitan.k/.local/lib/python3.6/site-packages/torch/nn/functional.py:1709: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    }
   ],
   "source": [
    "root.clear_leaves_samples()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (data, target) in enumerate(tree_loader):\n",
    "        root.accumulate_samples(data, method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tighten boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Pattern 1 ==============\n",
      "============== Pattern 2 ==============\n",
      "============== Pattern 3 ==============\n",
      "============== Pattern 4 ==============\n",
      "============== Pattern 5 ==============\n",
      "============== Pattern 6 ==============\n",
      "============== Pattern 7 ==============\n",
      "============== Pattern 8 ==============\n",
      "============== Pattern 9 ==============\n",
      "============== Pattern 10 ==============\n",
      "============== Pattern 11 ==============\n",
      "============== Pattern 12 ==============\n",
      "============== Pattern 13 ==============\n",
      "============== Pattern 14 ==============\n",
      "============== Pattern 15 ==============\n",
      "============== Pattern 16 ==============\n",
      "============== Pattern 17 ==============\n",
      "============== Pattern 18 ==============\n",
      "============== Pattern 19 ==============\n",
      "============== Pattern 20 ==============\n",
      "============== Pattern 21 ==============\n",
      "============== Pattern 22 ==============\n",
      "============== Pattern 23 ==============\n",
      "============== Pattern 24 ==============\n",
      "============== Pattern 25 ==============\n",
      "============== Pattern 26 ==============\n",
      "============== Pattern 27 ==============\n",
      "============== Pattern 28 ==============\n",
      "============== Pattern 29 ==============\n",
      "============== Pattern 30 ==============\n",
      "============== Pattern 31 ==============\n",
      "============== Pattern 32 ==============\n",
      "============== Pattern 33 ==============\n",
      "============== Pattern 34 ==============\n",
      "============== Pattern 35 ==============\n",
      "============== Pattern 36 ==============\n",
      "6672\n",
      "============== Pattern 37 ==============\n",
      "============== Pattern 38 ==============\n",
      "============== Pattern 39 ==============\n",
      "============== Pattern 40 ==============\n",
      "============== Pattern 41 ==============\n",
      "============== Pattern 42 ==============\n",
      "1476\n",
      "============== Pattern 43 ==============\n",
      "============== Pattern 44 ==============\n",
      "============== Pattern 45 ==============\n",
      "============== Pattern 46 ==============\n",
      "============== Pattern 47 ==============\n",
      "============== Pattern 48 ==============\n",
      "============== Pattern 49 ==============\n",
      "============== Pattern 50 ==============\n",
      "============== Pattern 51 ==============\n",
      "============== Pattern 52 ==============\n",
      "============== Pattern 53 ==============\n",
      "============== Pattern 54 ==============\n",
      "============== Pattern 55 ==============\n",
      "============== Pattern 56 ==============\n",
      "Average comprehensibility: 27.857142857142858\n",
      "std comprehensibility: 2.2632827882506943\n",
      "var comprehensibility: 5.122448979591836\n",
      "minimum comprehensibility: 20\n",
      "maximum comprehensibility: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eitan.k/EntangledExplainableClustering/soft_decision_tree/sdt_model.py:14: RuntimeWarning: divide by zero encountered in log\n",
      "  return np.log(1 / (1 - x))\n"
     ]
    }
   ],
   "source": [
    "attr_names = dataset.items\n",
    "\n",
    "# print(attr_names)\n",
    "leaves = root.get_leaves()\n",
    "sum_comprehensibility = 0\n",
    "comprehensibilities = []\n",
    "for pattern_counter, leaf in enumerate(leaves):\n",
    "    leaf.reset_path()\n",
    "    leaf.tighten_with_accumulated_samples()\n",
    "    conds = leaf.get_path_conditions(attr_names)\n",
    "    print(f\"============== Pattern {pattern_counter + 1} ==============\")\n",
    "    comprehensibilities.append(sum([cond.comprehensibility for cond in conds]))\n",
    "    \n",
    "print(f\"Average comprehensibility: {np.mean(comprehensibilities)}\")\n",
    "print(f\"std comprehensibility: {np.std(comprehensibilities)}\")\n",
    "print(f\"var comprehensibility: {np.var(comprehensibilities)}\")\n",
    "print(f\"minimum comprehensibility: {np.min(comprehensibilities)}\")\n",
    "print(f\"maximum comprehensibility: {np.max(comprehensibilities)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
