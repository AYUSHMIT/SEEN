{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib widget\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import torch.nn as nn\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from stream_generators.market_basket_dataset import MarketBasketDataset, BinaryEncodingTransform, RemoveItemsTransform\n",
    "from utils.MatplotlibUtils import reduce_dims_and_plot\n",
    "from network.auto_encoder import AutoEncoder\n",
    "from losses.knn_loss import KNNLoss\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from soft_decision_tree.sdt_model import SDT\n",
    "from sklearn.metrics import davies_bouldin_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 4\n",
    "tree_depth = 6\n",
    "device = 'cuda'\n",
    "dataset_path = r\"/mnt/qnap/ekosman/Groceries_dataset.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the market basket dataset and use one-hot encoding for items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MarketBasketDataset(dataset_path=dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoEncoder(dataset.n_items, 50, 4).train().to(device)\n",
    "epochs = 500\n",
    "lr = 5e-3\n",
    "batch_size = 512\n",
    "log_every = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.transform = torchvision.transforms.Compose([\n",
    "    RemoveItemsTransform(p=0.5),\n",
    "    BinaryEncodingTransform(mapping=dataset.items_to_idx),\n",
    "]\n",
    ")\n",
    "dataset.target_transform = torchvision.transforms.Compose([\n",
    "    BinaryEncodingTransform(mapping=dataset.items_to_idx),\n",
    "]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 / 500 | iteration 0 / 30 | Total Loss: 1.9548475742340088 | KNN Loss: 6.224452495574951 | BCE Loss: 1.9548475742340088\n",
      "Epoch 0 / 500 | iteration 5 / 30 | Total Loss: 1.9269031286239624 | KNN Loss: 6.223893642425537 | BCE Loss: 1.9269031286239624\n",
      "Epoch 0 / 500 | iteration 10 / 30 | Total Loss: 1.930891513824463 | KNN Loss: 6.223770618438721 | BCE Loss: 1.930891513824463\n",
      "Epoch 0 / 500 | iteration 15 / 30 | Total Loss: 1.908257246017456 | KNN Loss: 6.223718166351318 | BCE Loss: 1.908257246017456\n",
      "Epoch 0 / 500 | iteration 20 / 30 | Total Loss: 1.9198890924453735 | KNN Loss: 6.224004745483398 | BCE Loss: 1.9198890924453735\n",
      "Epoch 0 / 500 | iteration 25 / 30 | Total Loss: 1.941080093383789 | KNN Loss: 6.224161624908447 | BCE Loss: 1.941080093383789\n",
      "Epoch 1 / 500 | iteration 0 / 30 | Total Loss: 1.9609971046447754 | KNN Loss: 6.224413871765137 | BCE Loss: 1.9609971046447754\n",
      "Epoch 1 / 500 | iteration 5 / 30 | Total Loss: 1.9316774606704712 | KNN Loss: 6.2242889404296875 | BCE Loss: 1.9316774606704712\n",
      "Epoch 1 / 500 | iteration 10 / 30 | Total Loss: 1.8698887825012207 | KNN Loss: 6.224161148071289 | BCE Loss: 1.8698887825012207\n",
      "Epoch 1 / 500 | iteration 15 / 30 | Total Loss: 1.9247727394104004 | KNN Loss: 6.224132061004639 | BCE Loss: 1.9247727394104004\n",
      "Epoch 1 / 500 | iteration 20 / 30 | Total Loss: 1.8669730424880981 | KNN Loss: 6.224402904510498 | BCE Loss: 1.8669730424880981\n",
      "Epoch 1 / 500 | iteration 25 / 30 | Total Loss: 1.86465322971344 | KNN Loss: 6.2241926193237305 | BCE Loss: 1.86465322971344\n",
      "Epoch 2 / 500 | iteration 0 / 30 | Total Loss: 1.870262622833252 | KNN Loss: 6.224307537078857 | BCE Loss: 1.870262622833252\n",
      "Epoch 2 / 500 | iteration 5 / 30 | Total Loss: 1.8825042247772217 | KNN Loss: 6.224482536315918 | BCE Loss: 1.8825042247772217\n",
      "Epoch 2 / 500 | iteration 10 / 30 | Total Loss: 1.8052293062210083 | KNN Loss: 6.224188804626465 | BCE Loss: 1.8052293062210083\n",
      "Epoch 2 / 500 | iteration 15 / 30 | Total Loss: 1.8287339210510254 | KNN Loss: 6.224494457244873 | BCE Loss: 1.8287339210510254\n",
      "Epoch 2 / 500 | iteration 20 / 30 | Total Loss: 1.8903846740722656 | KNN Loss: 6.2237443923950195 | BCE Loss: 1.8903846740722656\n",
      "Epoch 2 / 500 | iteration 25 / 30 | Total Loss: 1.8765873908996582 | KNN Loss: 6.224012851715088 | BCE Loss: 1.8765873908996582\n",
      "Epoch 3 / 500 | iteration 0 / 30 | Total Loss: 1.8379786014556885 | KNN Loss: 6.224447727203369 | BCE Loss: 1.8379786014556885\n",
      "Epoch 3 / 500 | iteration 5 / 30 | Total Loss: 1.8043410778045654 | KNN Loss: 6.224050045013428 | BCE Loss: 1.8043410778045654\n",
      "Epoch 3 / 500 | iteration 10 / 30 | Total Loss: 1.791799783706665 | KNN Loss: 6.224218845367432 | BCE Loss: 1.791799783706665\n",
      "Epoch 3 / 500 | iteration 15 / 30 | Total Loss: 1.8297510147094727 | KNN Loss: 6.223922252655029 | BCE Loss: 1.8297510147094727\n",
      "Epoch 3 / 500 | iteration 20 / 30 | Total Loss: 1.7759478092193604 | KNN Loss: 6.223931789398193 | BCE Loss: 1.7759478092193604\n",
      "Epoch 3 / 500 | iteration 25 / 30 | Total Loss: 1.7821853160858154 | KNN Loss: 6.223984718322754 | BCE Loss: 1.7821853160858154\n",
      "Epoch 4 / 500 | iteration 0 / 30 | Total Loss: 1.7662348747253418 | KNN Loss: 6.224144458770752 | BCE Loss: 1.7662348747253418\n",
      "Epoch 4 / 500 | iteration 5 / 30 | Total Loss: 1.7429234981536865 | KNN Loss: 6.224311828613281 | BCE Loss: 1.7429234981536865\n",
      "Epoch 4 / 500 | iteration 10 / 30 | Total Loss: 1.7449922561645508 | KNN Loss: 6.223756790161133 | BCE Loss: 1.7449922561645508\n",
      "Epoch 4 / 500 | iteration 15 / 30 | Total Loss: 1.7583792209625244 | KNN Loss: 6.2241291999816895 | BCE Loss: 1.7583792209625244\n",
      "Epoch 4 / 500 | iteration 20 / 30 | Total Loss: 1.712770938873291 | KNN Loss: 6.2242584228515625 | BCE Loss: 1.712770938873291\n",
      "Epoch 4 / 500 | iteration 25 / 30 | Total Loss: 1.7117102146148682 | KNN Loss: 6.2244038581848145 | BCE Loss: 1.7117102146148682\n",
      "Epoch 5 / 500 | iteration 0 / 30 | Total Loss: 1.7272032499313354 | KNN Loss: 6.224339008331299 | BCE Loss: 1.7272032499313354\n",
      "Epoch 5 / 500 | iteration 5 / 30 | Total Loss: 1.6884875297546387 | KNN Loss: 6.223972320556641 | BCE Loss: 1.6884875297546387\n",
      "Epoch 5 / 500 | iteration 10 / 30 | Total Loss: 1.745563268661499 | KNN Loss: 6.224251747131348 | BCE Loss: 1.745563268661499\n",
      "Epoch 5 / 500 | iteration 15 / 30 | Total Loss: 1.719191312789917 | KNN Loss: 6.224238872528076 | BCE Loss: 1.719191312789917\n",
      "Epoch 5 / 500 | iteration 20 / 30 | Total Loss: 1.6502050161361694 | KNN Loss: 6.224181652069092 | BCE Loss: 1.6502050161361694\n",
      "Epoch 5 / 500 | iteration 25 / 30 | Total Loss: 1.6601191759109497 | KNN Loss: 6.224377632141113 | BCE Loss: 1.6601191759109497\n",
      "Epoch 6 / 500 | iteration 0 / 30 | Total Loss: 1.692429780960083 | KNN Loss: 6.224318504333496 | BCE Loss: 1.692429780960083\n",
      "Epoch 6 / 500 | iteration 5 / 30 | Total Loss: 1.6536228656768799 | KNN Loss: 6.224548816680908 | BCE Loss: 1.6536228656768799\n",
      "Epoch 6 / 500 | iteration 10 / 30 | Total Loss: 1.6350412368774414 | KNN Loss: 6.2248430252075195 | BCE Loss: 1.6350412368774414\n",
      "Epoch 6 / 500 | iteration 15 / 30 | Total Loss: 1.6587347984313965 | KNN Loss: 6.224356651306152 | BCE Loss: 1.6587347984313965\n",
      "Epoch 6 / 500 | iteration 20 / 30 | Total Loss: 1.5556310415267944 | KNN Loss: 6.224390029907227 | BCE Loss: 1.5556310415267944\n",
      "Epoch 6 / 500 | iteration 25 / 30 | Total Loss: 1.6231470108032227 | KNN Loss: 6.224667549133301 | BCE Loss: 1.6231470108032227\n",
      "Epoch 7 / 500 | iteration 0 / 30 | Total Loss: 1.5397710800170898 | KNN Loss: 6.22472620010376 | BCE Loss: 1.5397710800170898\n",
      "Epoch 7 / 500 | iteration 5 / 30 | Total Loss: 1.5855846405029297 | KNN Loss: 6.2250776290893555 | BCE Loss: 1.5855846405029297\n",
      "Epoch 7 / 500 | iteration 10 / 30 | Total Loss: 1.5505454540252686 | KNN Loss: 6.2247161865234375 | BCE Loss: 1.5505454540252686\n",
      "Epoch 7 / 500 | iteration 15 / 30 | Total Loss: 1.5150949954986572 | KNN Loss: 6.224693298339844 | BCE Loss: 1.5150949954986572\n",
      "Epoch 7 / 500 | iteration 20 / 30 | Total Loss: 1.5310571193695068 | KNN Loss: 6.224590301513672 | BCE Loss: 1.5310571193695068\n",
      "Epoch 7 / 500 | iteration 25 / 30 | Total Loss: 1.4760125875473022 | KNN Loss: 6.225123882293701 | BCE Loss: 1.4760125875473022\n",
      "Epoch 8 / 500 | iteration 0 / 30 | Total Loss: 1.496993064880371 | KNN Loss: 6.224740028381348 | BCE Loss: 1.496993064880371\n",
      "Epoch 8 / 500 | iteration 5 / 30 | Total Loss: 1.396730899810791 | KNN Loss: 6.225183010101318 | BCE Loss: 1.396730899810791\n",
      "Epoch 8 / 500 | iteration 10 / 30 | Total Loss: 1.4575891494750977 | KNN Loss: 6.2248053550720215 | BCE Loss: 1.4575891494750977\n",
      "Epoch 8 / 500 | iteration 15 / 30 | Total Loss: 1.419249176979065 | KNN Loss: 6.224903583526611 | BCE Loss: 1.419249176979065\n",
      "Epoch 8 / 500 | iteration 20 / 30 | Total Loss: 1.38283371925354 | KNN Loss: 6.2254438400268555 | BCE Loss: 1.38283371925354\n",
      "Epoch 8 / 500 | iteration 25 / 30 | Total Loss: 1.3726725578308105 | KNN Loss: 6.225303649902344 | BCE Loss: 1.3726725578308105\n",
      "Epoch 9 / 500 | iteration 0 / 30 | Total Loss: 1.345251202583313 | KNN Loss: 6.225180149078369 | BCE Loss: 1.345251202583313\n",
      "Epoch 9 / 500 | iteration 5 / 30 | Total Loss: 1.3371645212173462 | KNN Loss: 6.225563049316406 | BCE Loss: 1.3371645212173462\n",
      "Epoch 9 / 500 | iteration 10 / 30 | Total Loss: 1.2998437881469727 | KNN Loss: 6.2253098487854 | BCE Loss: 1.2998437881469727\n",
      "Epoch 9 / 500 | iteration 15 / 30 | Total Loss: 1.331136703491211 | KNN Loss: 6.225531578063965 | BCE Loss: 1.331136703491211\n",
      "Epoch 9 / 500 | iteration 20 / 30 | Total Loss: 1.337692379951477 | KNN Loss: 6.225596904754639 | BCE Loss: 1.337692379951477\n",
      "Epoch 9 / 500 | iteration 25 / 30 | Total Loss: 1.2829203605651855 | KNN Loss: 6.225632190704346 | BCE Loss: 1.2829203605651855\n",
      "Epoch 10 / 500 | iteration 0 / 30 | Total Loss: 1.2699790000915527 | KNN Loss: 6.225913047790527 | BCE Loss: 1.2699790000915527\n",
      "Epoch 10 / 500 | iteration 5 / 30 | Total Loss: 1.2338600158691406 | KNN Loss: 6.225788593292236 | BCE Loss: 1.2338600158691406\n",
      "Epoch 10 / 500 | iteration 10 / 30 | Total Loss: 1.199502944946289 | KNN Loss: 6.226059913635254 | BCE Loss: 1.199502944946289\n",
      "Epoch 10 / 500 | iteration 15 / 30 | Total Loss: 1.2058181762695312 | KNN Loss: 6.226090431213379 | BCE Loss: 1.2058181762695312\n",
      "Epoch 10 / 500 | iteration 20 / 30 | Total Loss: 1.2170839309692383 | KNN Loss: 6.225877285003662 | BCE Loss: 1.2170839309692383\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 / 500 | iteration 25 / 30 | Total Loss: 1.1807327270507812 | KNN Loss: 6.2258710861206055 | BCE Loss: 1.1807327270507812\n",
      "Epoch 11 / 500 | iteration 0 / 30 | Total Loss: 1.158316731452942 | KNN Loss: 6.226020336151123 | BCE Loss: 1.158316731452942\n",
      "Epoch 11 / 500 | iteration 5 / 30 | Total Loss: 1.1760101318359375 | KNN Loss: 6.225729465484619 | BCE Loss: 1.1760101318359375\n",
      "Epoch 11 / 500 | iteration 10 / 30 | Total Loss: 1.1324989795684814 | KNN Loss: 6.225797653198242 | BCE Loss: 1.1324989795684814\n",
      "Epoch 11 / 500 | iteration 15 / 30 | Total Loss: 1.1305344104766846 | KNN Loss: 6.226112365722656 | BCE Loss: 1.1305344104766846\n",
      "Epoch 11 / 500 | iteration 20 / 30 | Total Loss: 1.1277282238006592 | KNN Loss: 6.225693702697754 | BCE Loss: 1.1277282238006592\n",
      "Epoch 11 / 500 | iteration 25 / 30 | Total Loss: 1.1142151355743408 | KNN Loss: 6.226020336151123 | BCE Loss: 1.1142151355743408\n",
      "Epoch 12 / 500 | iteration 0 / 30 | Total Loss: 1.1002548933029175 | KNN Loss: 6.226152420043945 | BCE Loss: 1.1002548933029175\n",
      "Epoch 12 / 500 | iteration 5 / 30 | Total Loss: 1.1285310983657837 | KNN Loss: 6.226136684417725 | BCE Loss: 1.1285310983657837\n",
      "Epoch 12 / 500 | iteration 10 / 30 | Total Loss: 1.109360933303833 | KNN Loss: 6.226029396057129 | BCE Loss: 1.109360933303833\n",
      "Epoch 12 / 500 | iteration 15 / 30 | Total Loss: 1.1280925273895264 | KNN Loss: 6.225865364074707 | BCE Loss: 1.1280925273895264\n",
      "Epoch 12 / 500 | iteration 20 / 30 | Total Loss: 1.0992038249969482 | KNN Loss: 6.226052761077881 | BCE Loss: 1.0992038249969482\n",
      "Epoch 12 / 500 | iteration 25 / 30 | Total Loss: 1.0860073566436768 | KNN Loss: 6.225884914398193 | BCE Loss: 1.0860073566436768\n",
      "Epoch 13 / 500 | iteration 0 / 30 | Total Loss: 1.1005570888519287 | KNN Loss: 6.226100444793701 | BCE Loss: 1.1005570888519287\n",
      "Epoch 13 / 500 | iteration 5 / 30 | Total Loss: 1.0874766111373901 | KNN Loss: 6.226048946380615 | BCE Loss: 1.0874766111373901\n",
      "Epoch 13 / 500 | iteration 10 / 30 | Total Loss: 1.0820565223693848 | KNN Loss: 6.226027488708496 | BCE Loss: 1.0820565223693848\n",
      "Epoch 13 / 500 | iteration 15 / 30 | Total Loss: 1.0916763544082642 | KNN Loss: 6.225870132446289 | BCE Loss: 1.0916763544082642\n",
      "Epoch 13 / 500 | iteration 20 / 30 | Total Loss: 1.0994153022766113 | KNN Loss: 6.226062774658203 | BCE Loss: 1.0994153022766113\n",
      "Epoch 13 / 500 | iteration 25 / 30 | Total Loss: 1.0832710266113281 | KNN Loss: 6.225874900817871 | BCE Loss: 1.0832710266113281\n",
      "Epoch 14 / 500 | iteration 0 / 30 | Total Loss: 1.0782498121261597 | KNN Loss: 6.225944995880127 | BCE Loss: 1.0782498121261597\n",
      "Epoch 14 / 500 | iteration 5 / 30 | Total Loss: 1.0942425727844238 | KNN Loss: 6.22608757019043 | BCE Loss: 1.0942425727844238\n",
      "Epoch 14 / 500 | iteration 10 / 30 | Total Loss: 1.0639173984527588 | KNN Loss: 6.226134300231934 | BCE Loss: 1.0639173984527588\n",
      "Epoch 14 / 500 | iteration 15 / 30 | Total Loss: 1.0780267715454102 | KNN Loss: 6.225895404815674 | BCE Loss: 1.0780267715454102\n",
      "Epoch 14 / 500 | iteration 20 / 30 | Total Loss: 1.068556785583496 | KNN Loss: 6.226130485534668 | BCE Loss: 1.068556785583496\n",
      "Epoch 14 / 500 | iteration 25 / 30 | Total Loss: 1.0642316341400146 | KNN Loss: 6.2262396812438965 | BCE Loss: 1.0642316341400146\n",
      "Epoch 15 / 500 | iteration 0 / 30 | Total Loss: 1.092532753944397 | KNN Loss: 6.226193428039551 | BCE Loss: 1.092532753944397\n",
      "Epoch 15 / 500 | iteration 5 / 30 | Total Loss: 1.0493340492248535 | KNN Loss: 6.226132869720459 | BCE Loss: 1.0493340492248535\n",
      "Epoch 15 / 500 | iteration 10 / 30 | Total Loss: 1.0841689109802246 | KNN Loss: 6.2258219718933105 | BCE Loss: 1.0841689109802246\n",
      "Epoch 15 / 500 | iteration 15 / 30 | Total Loss: 1.0554077625274658 | KNN Loss: 6.225814342498779 | BCE Loss: 1.0554077625274658\n",
      "Epoch 15 / 500 | iteration 20 / 30 | Total Loss: 1.092984676361084 | KNN Loss: 6.22581148147583 | BCE Loss: 1.092984676361084\n",
      "Epoch 15 / 500 | iteration 25 / 30 | Total Loss: 1.0750725269317627 | KNN Loss: 6.226163387298584 | BCE Loss: 1.0750725269317627\n",
      "Epoch 16 / 500 | iteration 0 / 30 | Total Loss: 1.0825393199920654 | KNN Loss: 6.226169586181641 | BCE Loss: 1.0825393199920654\n",
      "Epoch 16 / 500 | iteration 5 / 30 | Total Loss: 1.082688570022583 | KNN Loss: 6.2258758544921875 | BCE Loss: 1.082688570022583\n",
      "Epoch 16 / 500 | iteration 10 / 30 | Total Loss: 1.0880361795425415 | KNN Loss: 6.225866317749023 | BCE Loss: 1.0880361795425415\n",
      "Epoch 16 / 500 | iteration 15 / 30 | Total Loss: 1.0644181966781616 | KNN Loss: 6.225936412811279 | BCE Loss: 1.0644181966781616\n",
      "Epoch 16 / 500 | iteration 20 / 30 | Total Loss: 1.074162483215332 | KNN Loss: 6.226133346557617 | BCE Loss: 1.074162483215332\n",
      "Epoch 16 / 500 | iteration 25 / 30 | Total Loss: 1.0763335227966309 | KNN Loss: 6.225895404815674 | BCE Loss: 1.0763335227966309\n",
      "Epoch 17 / 500 | iteration 0 / 30 | Total Loss: 1.0684967041015625 | KNN Loss: 6.226284980773926 | BCE Loss: 1.0684967041015625\n",
      "Epoch 17 / 500 | iteration 5 / 30 | Total Loss: 1.0831763744354248 | KNN Loss: 6.2260613441467285 | BCE Loss: 1.0831763744354248\n",
      "Epoch 17 / 500 | iteration 10 / 30 | Total Loss: 1.0250179767608643 | KNN Loss: 6.226022243499756 | BCE Loss: 1.0250179767608643\n",
      "Epoch 17 / 500 | iteration 15 / 30 | Total Loss: 1.090023159980774 | KNN Loss: 6.22604513168335 | BCE Loss: 1.090023159980774\n",
      "Epoch 17 / 500 | iteration 20 / 30 | Total Loss: 1.0828075408935547 | KNN Loss: 6.2257466316223145 | BCE Loss: 1.0828075408935547\n",
      "Epoch 17 / 500 | iteration 25 / 30 | Total Loss: 1.0631656646728516 | KNN Loss: 6.2255024909973145 | BCE Loss: 1.0631656646728516\n",
      "Epoch 18 / 500 | iteration 0 / 30 | Total Loss: 1.0502899885177612 | KNN Loss: 6.225918769836426 | BCE Loss: 1.0502899885177612\n",
      "Epoch 18 / 500 | iteration 5 / 30 | Total Loss: 1.0454168319702148 | KNN Loss: 6.225943088531494 | BCE Loss: 1.0454168319702148\n",
      "Epoch 18 / 500 | iteration 10 / 30 | Total Loss: 1.0552055835723877 | KNN Loss: 6.226284980773926 | BCE Loss: 1.0552055835723877\n",
      "Epoch 18 / 500 | iteration 15 / 30 | Total Loss: 1.0786501169204712 | KNN Loss: 6.2260308265686035 | BCE Loss: 1.0786501169204712\n",
      "Epoch 18 / 500 | iteration 20 / 30 | Total Loss: 1.0771052837371826 | KNN Loss: 6.225854873657227 | BCE Loss: 1.0771052837371826\n",
      "Epoch 18 / 500 | iteration 25 / 30 | Total Loss: 1.0801405906677246 | KNN Loss: 6.226291656494141 | BCE Loss: 1.0801405906677246\n",
      "Epoch 19 / 500 | iteration 0 / 30 | Total Loss: 1.0906933546066284 | KNN Loss: 6.226090431213379 | BCE Loss: 1.0906933546066284\n",
      "Epoch 19 / 500 | iteration 5 / 30 | Total Loss: 1.0623672008514404 | KNN Loss: 6.225945472717285 | BCE Loss: 1.0623672008514404\n",
      "Epoch 19 / 500 | iteration 10 / 30 | Total Loss: 1.0802299976348877 | KNN Loss: 6.2261505126953125 | BCE Loss: 1.0802299976348877\n",
      "Epoch 19 / 500 | iteration 15 / 30 | Total Loss: 1.0689125061035156 | KNN Loss: 6.226133823394775 | BCE Loss: 1.0689125061035156\n",
      "Epoch 19 / 500 | iteration 20 / 30 | Total Loss: 1.0246295928955078 | KNN Loss: 6.225924015045166 | BCE Loss: 1.0246295928955078\n",
      "Epoch 19 / 500 | iteration 25 / 30 | Total Loss: 1.0490224361419678 | KNN Loss: 6.225531578063965 | BCE Loss: 1.0490224361419678\n",
      "Epoch 20 / 500 | iteration 0 / 30 | Total Loss: 1.0877761840820312 | KNN Loss: 6.225663185119629 | BCE Loss: 1.0877761840820312\n",
      "Epoch 20 / 500 | iteration 5 / 30 | Total Loss: 1.0583994388580322 | KNN Loss: 6.226009845733643 | BCE Loss: 1.0583994388580322\n",
      "Epoch 20 / 500 | iteration 10 / 30 | Total Loss: 1.0521987676620483 | KNN Loss: 6.225919246673584 | BCE Loss: 1.0521987676620483\n",
      "Epoch 20 / 500 | iteration 15 / 30 | Total Loss: 1.067802906036377 | KNN Loss: 6.225635528564453 | BCE Loss: 1.067802906036377\n",
      "Epoch 20 / 500 | iteration 20 / 30 | Total Loss: 1.0465238094329834 | KNN Loss: 6.225806713104248 | BCE Loss: 1.0465238094329834\n",
      "Epoch 20 / 500 | iteration 25 / 30 | Total Loss: 1.0834181308746338 | KNN Loss: 6.226119041442871 | BCE Loss: 1.0834181308746338\n",
      "Epoch 21 / 500 | iteration 0 / 30 | Total Loss: 1.069685697555542 | KNN Loss: 6.226132392883301 | BCE Loss: 1.069685697555542\n",
      "Epoch 21 / 500 | iteration 5 / 30 | Total Loss: 1.1105422973632812 | KNN Loss: 6.225891590118408 | BCE Loss: 1.1105422973632812\n",
      "Epoch 21 / 500 | iteration 10 / 30 | Total Loss: 1.0817742347717285 | KNN Loss: 6.225868225097656 | BCE Loss: 1.0817742347717285\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 / 500 | iteration 15 / 30 | Total Loss: 1.0578217506408691 | KNN Loss: 6.225747108459473 | BCE Loss: 1.0578217506408691\n",
      "Epoch 21 / 500 | iteration 20 / 30 | Total Loss: 1.0658769607543945 | KNN Loss: 6.225818634033203 | BCE Loss: 1.0658769607543945\n",
      "Epoch 21 / 500 | iteration 25 / 30 | Total Loss: 1.0609794855117798 | KNN Loss: 6.225801467895508 | BCE Loss: 1.0609794855117798\n",
      "Epoch 22 / 500 | iteration 0 / 30 | Total Loss: 1.0351464748382568 | KNN Loss: 6.22580099105835 | BCE Loss: 1.0351464748382568\n",
      "Epoch 22 / 500 | iteration 5 / 30 | Total Loss: 1.0949946641921997 | KNN Loss: 6.2258172035217285 | BCE Loss: 1.0949946641921997\n",
      "Epoch 22 / 500 | iteration 10 / 30 | Total Loss: 1.0600221157073975 | KNN Loss: 6.225919723510742 | BCE Loss: 1.0600221157073975\n",
      "Epoch 22 / 500 | iteration 15 / 30 | Total Loss: 1.069531798362732 | KNN Loss: 6.225830078125 | BCE Loss: 1.069531798362732\n",
      "Epoch 22 / 500 | iteration 20 / 30 | Total Loss: 1.0701208114624023 | KNN Loss: 6.225661277770996 | BCE Loss: 1.0701208114624023\n",
      "Epoch 22 / 500 | iteration 25 / 30 | Total Loss: 1.0666136741638184 | KNN Loss: 6.225772857666016 | BCE Loss: 1.0666136741638184\n",
      "Epoch 23 / 500 | iteration 0 / 30 | Total Loss: 1.0362309217453003 | KNN Loss: 6.226132869720459 | BCE Loss: 1.0362309217453003\n",
      "Epoch 23 / 500 | iteration 5 / 30 | Total Loss: 1.0570266246795654 | KNN Loss: 6.225830078125 | BCE Loss: 1.0570266246795654\n",
      "Epoch 23 / 500 | iteration 10 / 30 | Total Loss: 1.0698752403259277 | KNN Loss: 6.225767612457275 | BCE Loss: 1.0698752403259277\n",
      "Epoch 23 / 500 | iteration 15 / 30 | Total Loss: 1.0399320125579834 | KNN Loss: 6.22567081451416 | BCE Loss: 1.0399320125579834\n",
      "Epoch 23 / 500 | iteration 20 / 30 | Total Loss: 1.0573794841766357 | KNN Loss: 6.225732326507568 | BCE Loss: 1.0573794841766357\n",
      "Epoch 23 / 500 | iteration 25 / 30 | Total Loss: 1.0531706809997559 | KNN Loss: 6.225953578948975 | BCE Loss: 1.0531706809997559\n",
      "Epoch 24 / 500 | iteration 0 / 30 | Total Loss: 1.028364896774292 | KNN Loss: 6.225948810577393 | BCE Loss: 1.028364896774292\n",
      "Epoch 24 / 500 | iteration 5 / 30 | Total Loss: 1.0662732124328613 | KNN Loss: 6.225896835327148 | BCE Loss: 1.0662732124328613\n",
      "Epoch 24 / 500 | iteration 10 / 30 | Total Loss: 1.073928713798523 | KNN Loss: 6.225864410400391 | BCE Loss: 1.073928713798523\n",
      "Epoch 24 / 500 | iteration 15 / 30 | Total Loss: 1.065292239189148 | KNN Loss: 6.22593355178833 | BCE Loss: 1.065292239189148\n",
      "Epoch 24 / 500 | iteration 20 / 30 | Total Loss: 1.0736474990844727 | KNN Loss: 6.225733757019043 | BCE Loss: 1.0736474990844727\n",
      "Epoch 24 / 500 | iteration 25 / 30 | Total Loss: 1.0671201944351196 | KNN Loss: 6.225682735443115 | BCE Loss: 1.0671201944351196\n",
      "Epoch 25 / 500 | iteration 0 / 30 | Total Loss: 1.0907330513000488 | KNN Loss: 6.225652694702148 | BCE Loss: 1.0907330513000488\n",
      "Epoch 25 / 500 | iteration 5 / 30 | Total Loss: 1.045606255531311 | KNN Loss: 6.225902557373047 | BCE Loss: 1.045606255531311\n",
      "Epoch 25 / 500 | iteration 10 / 30 | Total Loss: 1.0315985679626465 | KNN Loss: 6.225709438323975 | BCE Loss: 1.0315985679626465\n",
      "Epoch 25 / 500 | iteration 15 / 30 | Total Loss: 1.0675257444381714 | KNN Loss: 6.2260966300964355 | BCE Loss: 1.0675257444381714\n",
      "Epoch 25 / 500 | iteration 20 / 30 | Total Loss: 1.0791575908660889 | KNN Loss: 6.225635051727295 | BCE Loss: 1.0791575908660889\n",
      "Epoch 25 / 500 | iteration 25 / 30 | Total Loss: 1.0500049591064453 | KNN Loss: 6.226050853729248 | BCE Loss: 1.0500049591064453\n",
      "Epoch 26 / 500 | iteration 0 / 30 | Total Loss: 1.0674419403076172 | KNN Loss: 6.225521087646484 | BCE Loss: 1.0674419403076172\n",
      "Epoch 26 / 500 | iteration 5 / 30 | Total Loss: 1.060115098953247 | KNN Loss: 6.225679874420166 | BCE Loss: 1.060115098953247\n",
      "Epoch 26 / 500 | iteration 10 / 30 | Total Loss: 1.0930755138397217 | KNN Loss: 6.225668430328369 | BCE Loss: 1.0930755138397217\n",
      "Epoch 26 / 500 | iteration 15 / 30 | Total Loss: 1.0347774028778076 | KNN Loss: 6.2256622314453125 | BCE Loss: 1.0347774028778076\n",
      "Epoch 26 / 500 | iteration 20 / 30 | Total Loss: 1.0622576475143433 | KNN Loss: 6.225902080535889 | BCE Loss: 1.0622576475143433\n",
      "Epoch 26 / 500 | iteration 25 / 30 | Total Loss: 1.03599214553833 | KNN Loss: 6.225497245788574 | BCE Loss: 1.03599214553833\n",
      "Epoch 27 / 500 | iteration 0 / 30 | Total Loss: 1.06546151638031 | KNN Loss: 6.225745677947998 | BCE Loss: 1.06546151638031\n",
      "Epoch 27 / 500 | iteration 5 / 30 | Total Loss: 1.0511972904205322 | KNN Loss: 6.225949764251709 | BCE Loss: 1.0511972904205322\n",
      "Epoch 27 / 500 | iteration 10 / 30 | Total Loss: 1.0607280731201172 | KNN Loss: 6.225823879241943 | BCE Loss: 1.0607280731201172\n",
      "Epoch 27 / 500 | iteration 15 / 30 | Total Loss: 1.0921051502227783 | KNN Loss: 6.225781440734863 | BCE Loss: 1.0921051502227783\n",
      "Epoch 27 / 500 | iteration 20 / 30 | Total Loss: 1.0565004348754883 | KNN Loss: 6.225646018981934 | BCE Loss: 1.0565004348754883\n",
      "Epoch 27 / 500 | iteration 25 / 30 | Total Loss: 1.0726230144500732 | KNN Loss: 6.225880146026611 | BCE Loss: 1.0726230144500732\n",
      "Epoch 28 / 500 | iteration 0 / 30 | Total Loss: 1.0352658033370972 | KNN Loss: 6.225419998168945 | BCE Loss: 1.0352658033370972\n",
      "Epoch 28 / 500 | iteration 5 / 30 | Total Loss: 1.0573564767837524 | KNN Loss: 6.225793361663818 | BCE Loss: 1.0573564767837524\n",
      "Epoch 28 / 500 | iteration 10 / 30 | Total Loss: 1.0503082275390625 | KNN Loss: 6.225613594055176 | BCE Loss: 1.0503082275390625\n",
      "Epoch 28 / 500 | iteration 15 / 30 | Total Loss: 1.0476248264312744 | KNN Loss: 6.225663185119629 | BCE Loss: 1.0476248264312744\n",
      "Epoch 28 / 500 | iteration 20 / 30 | Total Loss: 1.0704169273376465 | KNN Loss: 6.225893974304199 | BCE Loss: 1.0704169273376465\n",
      "Epoch 28 / 500 | iteration 25 / 30 | Total Loss: 1.035905361175537 | KNN Loss: 6.225475788116455 | BCE Loss: 1.035905361175537\n",
      "Epoch 29 / 500 | iteration 0 / 30 | Total Loss: 1.0721461772918701 | KNN Loss: 6.225949764251709 | BCE Loss: 1.0721461772918701\n",
      "Epoch 29 / 500 | iteration 5 / 30 | Total Loss: 1.0518462657928467 | KNN Loss: 6.225438594818115 | BCE Loss: 1.0518462657928467\n",
      "Epoch 29 / 500 | iteration 10 / 30 | Total Loss: 1.049333930015564 | KNN Loss: 6.225443363189697 | BCE Loss: 1.049333930015564\n",
      "Epoch 29 / 500 | iteration 15 / 30 | Total Loss: 1.045795202255249 | KNN Loss: 6.2255096435546875 | BCE Loss: 1.045795202255249\n",
      "Epoch 29 / 500 | iteration 20 / 30 | Total Loss: 1.0560932159423828 | KNN Loss: 6.225734710693359 | BCE Loss: 1.0560932159423828\n",
      "Epoch 29 / 500 | iteration 25 / 30 | Total Loss: 1.072304606437683 | KNN Loss: 6.225774765014648 | BCE Loss: 1.072304606437683\n",
      "Epoch 30 / 500 | iteration 0 / 30 | Total Loss: 1.0516167879104614 | KNN Loss: 6.225450038909912 | BCE Loss: 1.0516167879104614\n",
      "Epoch 30 / 500 | iteration 5 / 30 | Total Loss: 1.0644996166229248 | KNN Loss: 6.225620746612549 | BCE Loss: 1.0644996166229248\n",
      "Epoch 30 / 500 | iteration 10 / 30 | Total Loss: 1.0647616386413574 | KNN Loss: 6.225705146789551 | BCE Loss: 1.0647616386413574\n",
      "Epoch 30 / 500 | iteration 15 / 30 | Total Loss: 1.0500202178955078 | KNN Loss: 6.22521448135376 | BCE Loss: 1.0500202178955078\n",
      "Epoch 30 / 500 | iteration 20 / 30 | Total Loss: 1.0450718402862549 | KNN Loss: 6.22538423538208 | BCE Loss: 1.0450718402862549\n",
      "Epoch 30 / 500 | iteration 25 / 30 | Total Loss: 1.039290189743042 | KNN Loss: 6.225577354431152 | BCE Loss: 1.039290189743042\n",
      "Epoch 31 / 500 | iteration 0 / 30 | Total Loss: 1.0716118812561035 | KNN Loss: 6.225371837615967 | BCE Loss: 1.0716118812561035\n",
      "Epoch 31 / 500 | iteration 5 / 30 | Total Loss: 1.0381898880004883 | KNN Loss: 6.2255024909973145 | BCE Loss: 1.0381898880004883\n",
      "Epoch 31 / 500 | iteration 10 / 30 | Total Loss: 1.0429892539978027 | KNN Loss: 6.225579738616943 | BCE Loss: 1.0429892539978027\n",
      "Epoch 31 / 500 | iteration 15 / 30 | Total Loss: 1.0749096870422363 | KNN Loss: 6.225398063659668 | BCE Loss: 1.0749096870422363\n",
      "Epoch 31 / 500 | iteration 20 / 30 | Total Loss: 1.063937783241272 | KNN Loss: 6.225354194641113 | BCE Loss: 1.063937783241272\n",
      "Epoch 31 / 500 | iteration 25 / 30 | Total Loss: 1.0384559631347656 | KNN Loss: 6.2254252433776855 | BCE Loss: 1.0384559631347656\n",
      "Epoch 32 / 500 | iteration 0 / 30 | Total Loss: 1.056316614151001 | KNN Loss: 6.22562313079834 | BCE Loss: 1.056316614151001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32 / 500 | iteration 5 / 30 | Total Loss: 1.0552157163619995 | KNN Loss: 6.225578308105469 | BCE Loss: 1.0552157163619995\n",
      "Epoch 32 / 500 | iteration 10 / 30 | Total Loss: 1.0483157634735107 | KNN Loss: 6.225674629211426 | BCE Loss: 1.0483157634735107\n",
      "Epoch 32 / 500 | iteration 15 / 30 | Total Loss: 1.036783218383789 | KNN Loss: 6.225553512573242 | BCE Loss: 1.036783218383789\n",
      "Epoch 32 / 500 | iteration 20 / 30 | Total Loss: 1.0388524532318115 | KNN Loss: 6.225253105163574 | BCE Loss: 1.0388524532318115\n",
      "Epoch 32 / 500 | iteration 25 / 30 | Total Loss: 1.0534021854400635 | KNN Loss: 6.225140571594238 | BCE Loss: 1.0534021854400635\n",
      "Epoch 33 / 500 | iteration 0 / 30 | Total Loss: 1.070055603981018 | KNN Loss: 6.225834846496582 | BCE Loss: 1.070055603981018\n",
      "Epoch 33 / 500 | iteration 5 / 30 | Total Loss: 1.0525712966918945 | KNN Loss: 6.225413799285889 | BCE Loss: 1.0525712966918945\n",
      "Epoch 33 / 500 | iteration 10 / 30 | Total Loss: 1.0483182668685913 | KNN Loss: 6.225648880004883 | BCE Loss: 1.0483182668685913\n",
      "Epoch 33 / 500 | iteration 15 / 30 | Total Loss: 1.0472407341003418 | KNN Loss: 6.225645065307617 | BCE Loss: 1.0472407341003418\n",
      "Epoch 33 / 500 | iteration 20 / 30 | Total Loss: 1.0290329456329346 | KNN Loss: 6.2251811027526855 | BCE Loss: 1.0290329456329346\n",
      "Epoch 33 / 500 | iteration 25 / 30 | Total Loss: 1.0367417335510254 | KNN Loss: 6.225243091583252 | BCE Loss: 1.0367417335510254\n",
      "Epoch 34 / 500 | iteration 0 / 30 | Total Loss: 1.0644617080688477 | KNN Loss: 6.225327968597412 | BCE Loss: 1.0644617080688477\n",
      "Epoch 34 / 500 | iteration 5 / 30 | Total Loss: 1.0505285263061523 | KNN Loss: 6.225395679473877 | BCE Loss: 1.0505285263061523\n",
      "Epoch 34 / 500 | iteration 10 / 30 | Total Loss: 1.0316717624664307 | KNN Loss: 6.225253582000732 | BCE Loss: 1.0316717624664307\n",
      "Epoch 34 / 500 | iteration 15 / 30 | Total Loss: 1.0289924144744873 | KNN Loss: 6.225581645965576 | BCE Loss: 1.0289924144744873\n",
      "Epoch 34 / 500 | iteration 20 / 30 | Total Loss: 1.0487496852874756 | KNN Loss: 6.225733757019043 | BCE Loss: 1.0487496852874756\n",
      "Epoch 34 / 500 | iteration 25 / 30 | Total Loss: 1.050879955291748 | KNN Loss: 6.225351333618164 | BCE Loss: 1.050879955291748\n",
      "Epoch 35 / 500 | iteration 0 / 30 | Total Loss: 1.047105312347412 | KNN Loss: 6.225255966186523 | BCE Loss: 1.047105312347412\n",
      "Epoch 35 / 500 | iteration 5 / 30 | Total Loss: 1.062925100326538 | KNN Loss: 6.2253899574279785 | BCE Loss: 1.062925100326538\n",
      "Epoch 35 / 500 | iteration 10 / 30 | Total Loss: 1.0724046230316162 | KNN Loss: 6.225103855133057 | BCE Loss: 1.0724046230316162\n",
      "Epoch 35 / 500 | iteration 15 / 30 | Total Loss: 1.0854841470718384 | KNN Loss: 6.225522994995117 | BCE Loss: 1.0854841470718384\n",
      "Epoch 35 / 500 | iteration 20 / 30 | Total Loss: 1.0386176109313965 | KNN Loss: 6.22529935836792 | BCE Loss: 1.0386176109313965\n",
      "Epoch 35 / 500 | iteration 25 / 30 | Total Loss: 1.0361417531967163 | KNN Loss: 6.22523307800293 | BCE Loss: 1.0361417531967163\n",
      "Epoch 36 / 500 | iteration 0 / 30 | Total Loss: 1.0727710723876953 | KNN Loss: 6.225436210632324 | BCE Loss: 1.0727710723876953\n",
      "Epoch 36 / 500 | iteration 5 / 30 | Total Loss: 1.050735592842102 | KNN Loss: 6.225419998168945 | BCE Loss: 1.050735592842102\n",
      "Epoch 36 / 500 | iteration 10 / 30 | Total Loss: 1.086733341217041 | KNN Loss: 6.2257184982299805 | BCE Loss: 1.086733341217041\n",
      "Epoch 36 / 500 | iteration 15 / 30 | Total Loss: 1.0245236158370972 | KNN Loss: 6.225391864776611 | BCE Loss: 1.0245236158370972\n",
      "Epoch 36 / 500 | iteration 20 / 30 | Total Loss: 1.0896039009094238 | KNN Loss: 6.225535869598389 | BCE Loss: 1.0896039009094238\n",
      "Epoch 36 / 500 | iteration 25 / 30 | Total Loss: 1.0674166679382324 | KNN Loss: 6.225812911987305 | BCE Loss: 1.0674166679382324\n",
      "Epoch 37 / 500 | iteration 0 / 30 | Total Loss: 1.056842565536499 | KNN Loss: 6.225721836090088 | BCE Loss: 1.056842565536499\n",
      "Epoch 37 / 500 | iteration 5 / 30 | Total Loss: 1.0386106967926025 | KNN Loss: 6.225665092468262 | BCE Loss: 1.0386106967926025\n",
      "Epoch 37 / 500 | iteration 10 / 30 | Total Loss: 1.0603008270263672 | KNN Loss: 6.225642204284668 | BCE Loss: 1.0603008270263672\n",
      "Epoch 37 / 500 | iteration 15 / 30 | Total Loss: 1.0460505485534668 | KNN Loss: 6.2256083488464355 | BCE Loss: 1.0460505485534668\n",
      "Epoch 37 / 500 | iteration 20 / 30 | Total Loss: 1.0738301277160645 | KNN Loss: 6.225478172302246 | BCE Loss: 1.0738301277160645\n",
      "Epoch 37 / 500 | iteration 25 / 30 | Total Loss: 1.018224835395813 | KNN Loss: 6.22527551651001 | BCE Loss: 1.018224835395813\n",
      "Epoch 38 / 500 | iteration 0 / 30 | Total Loss: 1.0576679706573486 | KNN Loss: 6.225536823272705 | BCE Loss: 1.0576679706573486\n",
      "Epoch 38 / 500 | iteration 5 / 30 | Total Loss: 1.0474821329116821 | KNN Loss: 6.225263595581055 | BCE Loss: 1.0474821329116821\n",
      "Epoch 38 / 500 | iteration 10 / 30 | Total Loss: 1.0394878387451172 | KNN Loss: 6.225183486938477 | BCE Loss: 1.0394878387451172\n",
      "Epoch 38 / 500 | iteration 15 / 30 | Total Loss: 1.0526018142700195 | KNN Loss: 6.225332260131836 | BCE Loss: 1.0526018142700195\n",
      "Epoch 38 / 500 | iteration 20 / 30 | Total Loss: 1.0568201541900635 | KNN Loss: 6.225245952606201 | BCE Loss: 1.0568201541900635\n",
      "Epoch 38 / 500 | iteration 25 / 30 | Total Loss: 1.0781614780426025 | KNN Loss: 6.225350379943848 | BCE Loss: 1.0781614780426025\n",
      "Epoch 39 / 500 | iteration 0 / 30 | Total Loss: 1.0251445770263672 | KNN Loss: 6.225275993347168 | BCE Loss: 1.0251445770263672\n",
      "Epoch 39 / 500 | iteration 5 / 30 | Total Loss: 1.0501477718353271 | KNN Loss: 6.225514888763428 | BCE Loss: 1.0501477718353271\n",
      "Epoch 39 / 500 | iteration 10 / 30 | Total Loss: 1.0180273056030273 | KNN Loss: 6.224967002868652 | BCE Loss: 1.0180273056030273\n",
      "Epoch 39 / 500 | iteration 15 / 30 | Total Loss: 1.0553240776062012 | KNN Loss: 6.225322246551514 | BCE Loss: 1.0553240776062012\n",
      "Epoch 39 / 500 | iteration 20 / 30 | Total Loss: 1.0549148321151733 | KNN Loss: 6.225650787353516 | BCE Loss: 1.0549148321151733\n",
      "Epoch 39 / 500 | iteration 25 / 30 | Total Loss: 1.064917802810669 | KNN Loss: 6.225199222564697 | BCE Loss: 1.064917802810669\n",
      "Epoch 40 / 500 | iteration 0 / 30 | Total Loss: 1.033233642578125 | KNN Loss: 6.225362300872803 | BCE Loss: 1.033233642578125\n",
      "Epoch 40 / 500 | iteration 5 / 30 | Total Loss: 1.040400505065918 | KNN Loss: 6.2254509925842285 | BCE Loss: 1.040400505065918\n",
      "Epoch 40 / 500 | iteration 10 / 30 | Total Loss: 1.0446540117263794 | KNN Loss: 6.225165843963623 | BCE Loss: 1.0446540117263794\n",
      "Epoch 40 / 500 | iteration 15 / 30 | Total Loss: 1.0718339681625366 | KNN Loss: 6.225345611572266 | BCE Loss: 1.0718339681625366\n",
      "Epoch 40 / 500 | iteration 20 / 30 | Total Loss: 1.0514514446258545 | KNN Loss: 6.225510120391846 | BCE Loss: 1.0514514446258545\n",
      "Epoch 40 / 500 | iteration 25 / 30 | Total Loss: 1.06711745262146 | KNN Loss: 6.225297927856445 | BCE Loss: 1.06711745262146\n",
      "Epoch 41 / 500 | iteration 0 / 30 | Total Loss: 1.077927827835083 | KNN Loss: 6.225302219390869 | BCE Loss: 1.077927827835083\n",
      "Epoch 41 / 500 | iteration 5 / 30 | Total Loss: 1.0584959983825684 | KNN Loss: 6.225200176239014 | BCE Loss: 1.0584959983825684\n",
      "Epoch 41 / 500 | iteration 10 / 30 | Total Loss: 1.0653036832809448 | KNN Loss: 6.225400924682617 | BCE Loss: 1.0653036832809448\n",
      "Epoch 41 / 500 | iteration 15 / 30 | Total Loss: 1.0588281154632568 | KNN Loss: 6.2251996994018555 | BCE Loss: 1.0588281154632568\n",
      "Epoch 41 / 500 | iteration 20 / 30 | Total Loss: 1.062569499015808 | KNN Loss: 6.225575923919678 | BCE Loss: 1.062569499015808\n",
      "Epoch 41 / 500 | iteration 25 / 30 | Total Loss: 1.0412466526031494 | KNN Loss: 6.225411415100098 | BCE Loss: 1.0412466526031494\n",
      "Epoch 42 / 500 | iteration 0 / 30 | Total Loss: 1.0476866960525513 | KNN Loss: 6.224946975708008 | BCE Loss: 1.0476866960525513\n",
      "Epoch 42 / 500 | iteration 5 / 30 | Total Loss: 1.0803847312927246 | KNN Loss: 6.225220203399658 | BCE Loss: 1.0803847312927246\n",
      "Epoch 42 / 500 | iteration 10 / 30 | Total Loss: 1.048235535621643 | KNN Loss: 6.225397109985352 | BCE Loss: 1.048235535621643\n",
      "Epoch 42 / 500 | iteration 15 / 30 | Total Loss: 1.0651495456695557 | KNN Loss: 6.225399971008301 | BCE Loss: 1.0651495456695557\n",
      "Epoch 42 / 500 | iteration 20 / 30 | Total Loss: 1.0591392517089844 | KNN Loss: 6.225222110748291 | BCE Loss: 1.0591392517089844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42 / 500 | iteration 25 / 30 | Total Loss: 1.0471668243408203 | KNN Loss: 6.225060939788818 | BCE Loss: 1.0471668243408203\n",
      "Epoch 43 / 500 | iteration 0 / 30 | Total Loss: 1.0459469556808472 | KNN Loss: 6.225045204162598 | BCE Loss: 1.0459469556808472\n",
      "Epoch 43 / 500 | iteration 5 / 30 | Total Loss: 1.0588915348052979 | KNN Loss: 6.225334644317627 | BCE Loss: 1.0588915348052979\n",
      "Epoch 43 / 500 | iteration 10 / 30 | Total Loss: 1.03245210647583 | KNN Loss: 6.2253241539001465 | BCE Loss: 1.03245210647583\n",
      "Epoch 43 / 500 | iteration 15 / 30 | Total Loss: 1.0404412746429443 | KNN Loss: 6.225056171417236 | BCE Loss: 1.0404412746429443\n",
      "Epoch 43 / 500 | iteration 20 / 30 | Total Loss: 1.077479362487793 | KNN Loss: 6.225242614746094 | BCE Loss: 1.077479362487793\n",
      "Epoch 43 / 500 | iteration 25 / 30 | Total Loss: 1.0450937747955322 | KNN Loss: 6.225175380706787 | BCE Loss: 1.0450937747955322\n",
      "Epoch 44 / 500 | iteration 0 / 30 | Total Loss: 1.0388567447662354 | KNN Loss: 6.225160121917725 | BCE Loss: 1.0388567447662354\n",
      "Epoch 44 / 500 | iteration 5 / 30 | Total Loss: 1.0705206394195557 | KNN Loss: 6.225459575653076 | BCE Loss: 1.0705206394195557\n",
      "Epoch 44 / 500 | iteration 10 / 30 | Total Loss: 1.0589959621429443 | KNN Loss: 6.224994659423828 | BCE Loss: 1.0589959621429443\n",
      "Epoch 44 / 500 | iteration 15 / 30 | Total Loss: 1.049099087715149 | KNN Loss: 6.225425720214844 | BCE Loss: 1.049099087715149\n",
      "Epoch 44 / 500 | iteration 20 / 30 | Total Loss: 1.0335807800292969 | KNN Loss: 6.225091457366943 | BCE Loss: 1.0335807800292969\n",
      "Epoch 44 / 500 | iteration 25 / 30 | Total Loss: 1.055119276046753 | KNN Loss: 6.225154876708984 | BCE Loss: 1.055119276046753\n",
      "Epoch 45 / 500 | iteration 0 / 30 | Total Loss: 1.0392204523086548 | KNN Loss: 6.225347518920898 | BCE Loss: 1.0392204523086548\n",
      "Epoch 45 / 500 | iteration 5 / 30 | Total Loss: 1.0574263334274292 | KNN Loss: 6.225368976593018 | BCE Loss: 1.0574263334274292\n",
      "Epoch 45 / 500 | iteration 10 / 30 | Total Loss: 1.0595862865447998 | KNN Loss: 6.225033283233643 | BCE Loss: 1.0595862865447998\n",
      "Epoch 45 / 500 | iteration 15 / 30 | Total Loss: 1.0625429153442383 | KNN Loss: 6.225017070770264 | BCE Loss: 1.0625429153442383\n",
      "Epoch 45 / 500 | iteration 20 / 30 | Total Loss: 1.0503090620040894 | KNN Loss: 6.224843978881836 | BCE Loss: 1.0503090620040894\n",
      "Epoch 45 / 500 | iteration 25 / 30 | Total Loss: 1.0694644451141357 | KNN Loss: 6.225138187408447 | BCE Loss: 1.0694644451141357\n",
      "Epoch 46 / 500 | iteration 0 / 30 | Total Loss: 1.041032314300537 | KNN Loss: 6.225074768066406 | BCE Loss: 1.041032314300537\n",
      "Epoch 46 / 500 | iteration 5 / 30 | Total Loss: 1.0702441930770874 | KNN Loss: 6.225413799285889 | BCE Loss: 1.0702441930770874\n",
      "Epoch 46 / 500 | iteration 10 / 30 | Total Loss: 1.063934564590454 | KNN Loss: 6.2250800132751465 | BCE Loss: 1.063934564590454\n",
      "Epoch 46 / 500 | iteration 15 / 30 | Total Loss: 1.0368196964263916 | KNN Loss: 6.225684642791748 | BCE Loss: 1.0368196964263916\n",
      "Epoch 46 / 500 | iteration 20 / 30 | Total Loss: 1.048997163772583 | KNN Loss: 6.22487735748291 | BCE Loss: 1.048997163772583\n",
      "Epoch 46 / 500 | iteration 25 / 30 | Total Loss: 1.0716867446899414 | KNN Loss: 6.225033760070801 | BCE Loss: 1.0716867446899414\n",
      "Epoch 47 / 500 | iteration 0 / 30 | Total Loss: 1.1043052673339844 | KNN Loss: 6.224801540374756 | BCE Loss: 1.1043052673339844\n",
      "Epoch 47 / 500 | iteration 5 / 30 | Total Loss: 1.0654010772705078 | KNN Loss: 6.225174427032471 | BCE Loss: 1.0654010772705078\n",
      "Epoch 47 / 500 | iteration 10 / 30 | Total Loss: 1.0818582773208618 | KNN Loss: 6.2252888679504395 | BCE Loss: 1.0818582773208618\n",
      "Epoch 47 / 500 | iteration 15 / 30 | Total Loss: 1.0524230003356934 | KNN Loss: 6.225080966949463 | BCE Loss: 1.0524230003356934\n",
      "Epoch 47 / 500 | iteration 20 / 30 | Total Loss: 1.0760728120803833 | KNN Loss: 6.225235462188721 | BCE Loss: 1.0760728120803833\n",
      "Epoch 47 / 500 | iteration 25 / 30 | Total Loss: 1.0285733938217163 | KNN Loss: 6.2252302169799805 | BCE Loss: 1.0285733938217163\n",
      "Epoch 48 / 500 | iteration 0 / 30 | Total Loss: 1.0803663730621338 | KNN Loss: 6.225118160247803 | BCE Loss: 1.0803663730621338\n",
      "Epoch 48 / 500 | iteration 5 / 30 | Total Loss: 1.05080246925354 | KNN Loss: 6.225430488586426 | BCE Loss: 1.05080246925354\n",
      "Epoch 48 / 500 | iteration 10 / 30 | Total Loss: 1.0624140501022339 | KNN Loss: 6.225404262542725 | BCE Loss: 1.0624140501022339\n",
      "Epoch 48 / 500 | iteration 15 / 30 | Total Loss: 1.0527496337890625 | KNN Loss: 6.2251200675964355 | BCE Loss: 1.0527496337890625\n",
      "Epoch 48 / 500 | iteration 20 / 30 | Total Loss: 1.0668463706970215 | KNN Loss: 6.225123882293701 | BCE Loss: 1.0668463706970215\n",
      "Epoch 48 / 500 | iteration 25 / 30 | Total Loss: 1.0562357902526855 | KNN Loss: 6.225123405456543 | BCE Loss: 1.0562357902526855\n",
      "Epoch 49 / 500 | iteration 0 / 30 | Total Loss: 1.0342180728912354 | KNN Loss: 6.2251176834106445 | BCE Loss: 1.0342180728912354\n",
      "Epoch 49 / 500 | iteration 5 / 30 | Total Loss: 1.0635192394256592 | KNN Loss: 6.225381374359131 | BCE Loss: 1.0635192394256592\n",
      "Epoch 49 / 500 | iteration 10 / 30 | Total Loss: 1.0570827722549438 | KNN Loss: 6.224981784820557 | BCE Loss: 1.0570827722549438\n",
      "Epoch 49 / 500 | iteration 15 / 30 | Total Loss: 1.0561935901641846 | KNN Loss: 6.225470066070557 | BCE Loss: 1.0561935901641846\n",
      "Epoch 49 / 500 | iteration 20 / 30 | Total Loss: 1.055777907371521 | KNN Loss: 6.224918842315674 | BCE Loss: 1.055777907371521\n",
      "Epoch 49 / 500 | iteration 25 / 30 | Total Loss: 1.0435490608215332 | KNN Loss: 6.224847793579102 | BCE Loss: 1.0435490608215332\n",
      "Epoch    50: reducing learning rate of group 0 to 3.5000e-03.\n",
      "Epoch 50 / 500 | iteration 0 / 30 | Total Loss: 1.0742294788360596 | KNN Loss: 6.224837779998779 | BCE Loss: 1.0742294788360596\n",
      "Epoch 50 / 500 | iteration 5 / 30 | Total Loss: 1.090294361114502 | KNN Loss: 6.2252702713012695 | BCE Loss: 1.090294361114502\n",
      "Epoch 50 / 500 | iteration 10 / 30 | Total Loss: 1.0952842235565186 | KNN Loss: 6.225174903869629 | BCE Loss: 1.0952842235565186\n",
      "Epoch 50 / 500 | iteration 15 / 30 | Total Loss: 1.0477534532546997 | KNN Loss: 6.225266933441162 | BCE Loss: 1.0477534532546997\n",
      "Epoch 50 / 500 | iteration 20 / 30 | Total Loss: 1.0539056062698364 | KNN Loss: 6.2252197265625 | BCE Loss: 1.0539056062698364\n",
      "Epoch 50 / 500 | iteration 25 / 30 | Total Loss: 1.0383899211883545 | KNN Loss: 6.2253570556640625 | BCE Loss: 1.0383899211883545\n",
      "Epoch 51 / 500 | iteration 0 / 30 | Total Loss: 1.058379054069519 | KNN Loss: 6.225413799285889 | BCE Loss: 1.058379054069519\n",
      "Epoch 51 / 500 | iteration 5 / 30 | Total Loss: 1.0425463914871216 | KNN Loss: 6.225198745727539 | BCE Loss: 1.0425463914871216\n",
      "Epoch 51 / 500 | iteration 10 / 30 | Total Loss: 1.0605676174163818 | KNN Loss: 6.22503137588501 | BCE Loss: 1.0605676174163818\n",
      "Epoch 51 / 500 | iteration 15 / 30 | Total Loss: 1.0697739124298096 | KNN Loss: 6.225189208984375 | BCE Loss: 1.0697739124298096\n",
      "Epoch 51 / 500 | iteration 20 / 30 | Total Loss: 1.038777470588684 | KNN Loss: 6.2250800132751465 | BCE Loss: 1.038777470588684\n",
      "Epoch 51 / 500 | iteration 25 / 30 | Total Loss: 1.0745439529418945 | KNN Loss: 6.2252960205078125 | BCE Loss: 1.0745439529418945\n",
      "Epoch 52 / 500 | iteration 0 / 30 | Total Loss: 1.0516400337219238 | KNN Loss: 6.225415229797363 | BCE Loss: 1.0516400337219238\n",
      "Epoch 52 / 500 | iteration 5 / 30 | Total Loss: 1.0490930080413818 | KNN Loss: 6.225074768066406 | BCE Loss: 1.0490930080413818\n",
      "Epoch 52 / 500 | iteration 10 / 30 | Total Loss: 1.045127511024475 | KNN Loss: 6.224925518035889 | BCE Loss: 1.045127511024475\n",
      "Epoch 52 / 500 | iteration 15 / 30 | Total Loss: 1.0718998908996582 | KNN Loss: 6.225318908691406 | BCE Loss: 1.0718998908996582\n",
      "Epoch 52 / 500 | iteration 20 / 30 | Total Loss: 1.0568443536758423 | KNN Loss: 6.225423336029053 | BCE Loss: 1.0568443536758423\n",
      "Epoch 52 / 500 | iteration 25 / 30 | Total Loss: 1.0417203903198242 | KNN Loss: 6.224474906921387 | BCE Loss: 1.0417203903198242\n",
      "Epoch 53 / 500 | iteration 0 / 30 | Total Loss: 1.047127366065979 | KNN Loss: 6.224961757659912 | BCE Loss: 1.047127366065979\n",
      "Epoch 53 / 500 | iteration 5 / 30 | Total Loss: 1.0605579614639282 | KNN Loss: 6.225217819213867 | BCE Loss: 1.0605579614639282\n",
      "Epoch 53 / 500 | iteration 10 / 30 | Total Loss: 1.0350863933563232 | KNN Loss: 6.2252421379089355 | BCE Loss: 1.0350863933563232\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53 / 500 | iteration 15 / 30 | Total Loss: 1.0646257400512695 | KNN Loss: 6.225299835205078 | BCE Loss: 1.0646257400512695\n",
      "Epoch 53 / 500 | iteration 20 / 30 | Total Loss: 1.0549118518829346 | KNN Loss: 6.224935054779053 | BCE Loss: 1.0549118518829346\n",
      "Epoch 53 / 500 | iteration 25 / 30 | Total Loss: 1.0352615118026733 | KNN Loss: 6.225063323974609 | BCE Loss: 1.0352615118026733\n",
      "Epoch 54 / 500 | iteration 0 / 30 | Total Loss: 1.0344198942184448 | KNN Loss: 6.225149631500244 | BCE Loss: 1.0344198942184448\n",
      "Epoch 54 / 500 | iteration 5 / 30 | Total Loss: 1.037271499633789 | KNN Loss: 6.224960803985596 | BCE Loss: 1.037271499633789\n",
      "Epoch 54 / 500 | iteration 10 / 30 | Total Loss: 1.0486021041870117 | KNN Loss: 6.225105285644531 | BCE Loss: 1.0486021041870117\n",
      "Epoch 54 / 500 | iteration 15 / 30 | Total Loss: 1.0494158267974854 | KNN Loss: 6.224609851837158 | BCE Loss: 1.0494158267974854\n",
      "Epoch 54 / 500 | iteration 20 / 30 | Total Loss: 1.0824624300003052 | KNN Loss: 6.225467205047607 | BCE Loss: 1.0824624300003052\n",
      "Epoch 54 / 500 | iteration 25 / 30 | Total Loss: 1.068197250366211 | KNN Loss: 6.225011825561523 | BCE Loss: 1.068197250366211\n",
      "Epoch 55 / 500 | iteration 0 / 30 | Total Loss: 1.0399378538131714 | KNN Loss: 6.224945545196533 | BCE Loss: 1.0399378538131714\n",
      "Epoch 55 / 500 | iteration 5 / 30 | Total Loss: 1.0629897117614746 | KNN Loss: 6.225096702575684 | BCE Loss: 1.0629897117614746\n",
      "Epoch 55 / 500 | iteration 10 / 30 | Total Loss: 1.066023826599121 | KNN Loss: 6.2251667976379395 | BCE Loss: 1.066023826599121\n",
      "Epoch 55 / 500 | iteration 15 / 30 | Total Loss: 1.0517655611038208 | KNN Loss: 6.224852561950684 | BCE Loss: 1.0517655611038208\n",
      "Epoch 55 / 500 | iteration 20 / 30 | Total Loss: 1.0467314720153809 | KNN Loss: 6.225015163421631 | BCE Loss: 1.0467314720153809\n",
      "Epoch 55 / 500 | iteration 25 / 30 | Total Loss: 1.0506303310394287 | KNN Loss: 6.2249226570129395 | BCE Loss: 1.0506303310394287\n",
      "Epoch 56 / 500 | iteration 0 / 30 | Total Loss: 1.0658066272735596 | KNN Loss: 6.225186824798584 | BCE Loss: 1.0658066272735596\n",
      "Epoch 56 / 500 | iteration 5 / 30 | Total Loss: 1.04396390914917 | KNN Loss: 6.225025177001953 | BCE Loss: 1.04396390914917\n",
      "Epoch 56 / 500 | iteration 10 / 30 | Total Loss: 1.070439100265503 | KNN Loss: 6.224839687347412 | BCE Loss: 1.070439100265503\n",
      "Epoch 56 / 500 | iteration 15 / 30 | Total Loss: 1.035733938217163 | KNN Loss: 6.2255401611328125 | BCE Loss: 1.035733938217163\n",
      "Epoch 56 / 500 | iteration 20 / 30 | Total Loss: 1.0448424816131592 | KNN Loss: 6.225537300109863 | BCE Loss: 1.0448424816131592\n",
      "Epoch 56 / 500 | iteration 25 / 30 | Total Loss: 1.0856482982635498 | KNN Loss: 6.224761962890625 | BCE Loss: 1.0856482982635498\n",
      "Epoch 57 / 500 | iteration 0 / 30 | Total Loss: 1.0242526531219482 | KNN Loss: 6.224950313568115 | BCE Loss: 1.0242526531219482\n",
      "Epoch 57 / 500 | iteration 5 / 30 | Total Loss: 1.073899507522583 | KNN Loss: 6.224969863891602 | BCE Loss: 1.073899507522583\n",
      "Epoch 57 / 500 | iteration 10 / 30 | Total Loss: 1.0744481086730957 | KNN Loss: 6.225179672241211 | BCE Loss: 1.0744481086730957\n",
      "Epoch 57 / 500 | iteration 15 / 30 | Total Loss: 1.0460875034332275 | KNN Loss: 6.225301265716553 | BCE Loss: 1.0460875034332275\n",
      "Epoch 57 / 500 | iteration 20 / 30 | Total Loss: 1.0600776672363281 | KNN Loss: 6.225086212158203 | BCE Loss: 1.0600776672363281\n",
      "Epoch 57 / 500 | iteration 25 / 30 | Total Loss: 1.0416454076766968 | KNN Loss: 6.225189685821533 | BCE Loss: 1.0416454076766968\n",
      "Epoch 58 / 500 | iteration 0 / 30 | Total Loss: 1.0709832906723022 | KNN Loss: 6.224852085113525 | BCE Loss: 1.0709832906723022\n",
      "Epoch 58 / 500 | iteration 5 / 30 | Total Loss: 1.0364766120910645 | KNN Loss: 6.22489595413208 | BCE Loss: 1.0364766120910645\n",
      "Epoch 58 / 500 | iteration 10 / 30 | Total Loss: 1.0309040546417236 | KNN Loss: 6.225101470947266 | BCE Loss: 1.0309040546417236\n",
      "Epoch 58 / 500 | iteration 15 / 30 | Total Loss: 1.0600134134292603 | KNN Loss: 6.225079536437988 | BCE Loss: 1.0600134134292603\n",
      "Epoch 58 / 500 | iteration 20 / 30 | Total Loss: 1.0504415035247803 | KNN Loss: 6.225092887878418 | BCE Loss: 1.0504415035247803\n",
      "Epoch 58 / 500 | iteration 25 / 30 | Total Loss: 1.0635063648223877 | KNN Loss: 6.224944591522217 | BCE Loss: 1.0635063648223877\n",
      "Epoch 59 / 500 | iteration 0 / 30 | Total Loss: 1.0272574424743652 | KNN Loss: 6.224688529968262 | BCE Loss: 1.0272574424743652\n",
      "Epoch 59 / 500 | iteration 5 / 30 | Total Loss: 1.0304009914398193 | KNN Loss: 6.224975109100342 | BCE Loss: 1.0304009914398193\n",
      "Epoch 59 / 500 | iteration 10 / 30 | Total Loss: 1.0544450283050537 | KNN Loss: 6.224978446960449 | BCE Loss: 1.0544450283050537\n",
      "Epoch 59 / 500 | iteration 15 / 30 | Total Loss: 1.080474853515625 | KNN Loss: 6.2248711585998535 | BCE Loss: 1.080474853515625\n",
      "Epoch 59 / 500 | iteration 20 / 30 | Total Loss: 1.0795350074768066 | KNN Loss: 6.224912643432617 | BCE Loss: 1.0795350074768066\n",
      "Epoch 59 / 500 | iteration 25 / 30 | Total Loss: 1.049145221710205 | KNN Loss: 6.225221633911133 | BCE Loss: 1.049145221710205\n",
      "Epoch 60 / 500 | iteration 0 / 30 | Total Loss: 1.0273058414459229 | KNN Loss: 6.224818229675293 | BCE Loss: 1.0273058414459229\n",
      "Epoch 60 / 500 | iteration 5 / 30 | Total Loss: 1.0955212116241455 | KNN Loss: 6.22493314743042 | BCE Loss: 1.0955212116241455\n",
      "Epoch 60 / 500 | iteration 10 / 30 | Total Loss: 1.0450692176818848 | KNN Loss: 6.2249250411987305 | BCE Loss: 1.0450692176818848\n",
      "Epoch 60 / 500 | iteration 15 / 30 | Total Loss: 1.0610381364822388 | KNN Loss: 6.224965572357178 | BCE Loss: 1.0610381364822388\n",
      "Epoch 60 / 500 | iteration 20 / 30 | Total Loss: 1.0382814407348633 | KNN Loss: 6.22505521774292 | BCE Loss: 1.0382814407348633\n",
      "Epoch 60 / 500 | iteration 25 / 30 | Total Loss: 1.0484259128570557 | KNN Loss: 6.225460052490234 | BCE Loss: 1.0484259128570557\n",
      "Epoch 61 / 500 | iteration 0 / 30 | Total Loss: 1.049566626548767 | KNN Loss: 6.225126266479492 | BCE Loss: 1.049566626548767\n",
      "Epoch 61 / 500 | iteration 5 / 30 | Total Loss: 1.066108226776123 | KNN Loss: 6.225024700164795 | BCE Loss: 1.066108226776123\n",
      "Epoch 61 / 500 | iteration 10 / 30 | Total Loss: 1.026613473892212 | KNN Loss: 6.225261211395264 | BCE Loss: 1.026613473892212\n",
      "Epoch 61 / 500 | iteration 15 / 30 | Total Loss: 1.0879857540130615 | KNN Loss: 6.225107192993164 | BCE Loss: 1.0879857540130615\n",
      "Epoch 61 / 500 | iteration 20 / 30 | Total Loss: 1.0776236057281494 | KNN Loss: 6.224890232086182 | BCE Loss: 1.0776236057281494\n",
      "Epoch 61 / 500 | iteration 25 / 30 | Total Loss: 1.0524390935897827 | KNN Loss: 6.2249755859375 | BCE Loss: 1.0524390935897827\n",
      "Epoch 62 / 500 | iteration 0 / 30 | Total Loss: 1.0673277378082275 | KNN Loss: 6.2252278327941895 | BCE Loss: 1.0673277378082275\n",
      "Epoch 62 / 500 | iteration 5 / 30 | Total Loss: 1.0587351322174072 | KNN Loss: 6.2249226570129395 | BCE Loss: 1.0587351322174072\n",
      "Epoch 62 / 500 | iteration 10 / 30 | Total Loss: 1.055916428565979 | KNN Loss: 6.224921226501465 | BCE Loss: 1.055916428565979\n",
      "Epoch 62 / 500 | iteration 15 / 30 | Total Loss: 1.0338973999023438 | KNN Loss: 6.22504186630249 | BCE Loss: 1.0338973999023438\n",
      "Epoch 62 / 500 | iteration 20 / 30 | Total Loss: 1.0344007015228271 | KNN Loss: 6.2251296043396 | BCE Loss: 1.0344007015228271\n",
      "Epoch 62 / 500 | iteration 25 / 30 | Total Loss: 1.038433313369751 | KNN Loss: 6.224746227264404 | BCE Loss: 1.038433313369751\n",
      "Epoch 63 / 500 | iteration 0 / 30 | Total Loss: 1.0429675579071045 | KNN Loss: 6.224935531616211 | BCE Loss: 1.0429675579071045\n",
      "Epoch 63 / 500 | iteration 5 / 30 | Total Loss: 1.0548834800720215 | KNN Loss: 6.225319862365723 | BCE Loss: 1.0548834800720215\n",
      "Epoch 63 / 500 | iteration 10 / 30 | Total Loss: 1.0549898147583008 | KNN Loss: 6.225220680236816 | BCE Loss: 1.0549898147583008\n",
      "Epoch 63 / 500 | iteration 15 / 30 | Total Loss: 1.0866972208023071 | KNN Loss: 6.22487211227417 | BCE Loss: 1.0866972208023071\n",
      "Epoch 63 / 500 | iteration 20 / 30 | Total Loss: 1.0634139776229858 | KNN Loss: 6.225176811218262 | BCE Loss: 1.0634139776229858\n",
      "Epoch 63 / 500 | iteration 25 / 30 | Total Loss: 1.0590084791183472 | KNN Loss: 6.224766254425049 | BCE Loss: 1.0590084791183472\n",
      "Epoch 64 / 500 | iteration 0 / 30 | Total Loss: 1.028387188911438 | KNN Loss: 6.224966526031494 | BCE Loss: 1.028387188911438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64 / 500 | iteration 5 / 30 | Total Loss: 1.0402413606643677 | KNN Loss: 6.224911689758301 | BCE Loss: 1.0402413606643677\n",
      "Epoch 64 / 500 | iteration 10 / 30 | Total Loss: 1.0289549827575684 | KNN Loss: 6.2247819900512695 | BCE Loss: 1.0289549827575684\n",
      "Epoch 64 / 500 | iteration 15 / 30 | Total Loss: 1.0621110200881958 | KNN Loss: 6.225223541259766 | BCE Loss: 1.0621110200881958\n",
      "Epoch 64 / 500 | iteration 20 / 30 | Total Loss: 1.061340570449829 | KNN Loss: 6.224773406982422 | BCE Loss: 1.061340570449829\n",
      "Epoch 64 / 500 | iteration 25 / 30 | Total Loss: 1.0504777431488037 | KNN Loss: 6.225216865539551 | BCE Loss: 1.0504777431488037\n",
      "Epoch 65 / 500 | iteration 0 / 30 | Total Loss: 1.056117057800293 | KNN Loss: 6.224765300750732 | BCE Loss: 1.056117057800293\n",
      "Epoch 65 / 500 | iteration 5 / 30 | Total Loss: 1.023073434829712 | KNN Loss: 6.224705696105957 | BCE Loss: 1.023073434829712\n",
      "Epoch 65 / 500 | iteration 10 / 30 | Total Loss: 1.0511085987091064 | KNN Loss: 6.225002765655518 | BCE Loss: 1.0511085987091064\n",
      "Epoch 65 / 500 | iteration 15 / 30 | Total Loss: 1.0536619424819946 | KNN Loss: 6.224891662597656 | BCE Loss: 1.0536619424819946\n",
      "Epoch 65 / 500 | iteration 20 / 30 | Total Loss: 1.0353243350982666 | KNN Loss: 6.224949359893799 | BCE Loss: 1.0353243350982666\n",
      "Epoch 65 / 500 | iteration 25 / 30 | Total Loss: 1.0371967554092407 | KNN Loss: 6.225017547607422 | BCE Loss: 1.0371967554092407\n",
      "Epoch 66 / 500 | iteration 0 / 30 | Total Loss: 1.0530940294265747 | KNN Loss: 6.225325107574463 | BCE Loss: 1.0530940294265747\n",
      "Epoch 66 / 500 | iteration 5 / 30 | Total Loss: 1.0747838020324707 | KNN Loss: 6.224958419799805 | BCE Loss: 1.0747838020324707\n",
      "Epoch 66 / 500 | iteration 10 / 30 | Total Loss: 1.0410902500152588 | KNN Loss: 6.225123405456543 | BCE Loss: 1.0410902500152588\n",
      "Epoch 66 / 500 | iteration 15 / 30 | Total Loss: 1.061508059501648 | KNN Loss: 6.224755764007568 | BCE Loss: 1.061508059501648\n",
      "Epoch 66 / 500 | iteration 20 / 30 | Total Loss: 1.0240757465362549 | KNN Loss: 6.224921226501465 | BCE Loss: 1.0240757465362549\n",
      "Epoch 66 / 500 | iteration 25 / 30 | Total Loss: 1.0344380140304565 | KNN Loss: 6.224900245666504 | BCE Loss: 1.0344380140304565\n",
      "Epoch 67 / 500 | iteration 0 / 30 | Total Loss: 1.0616943836212158 | KNN Loss: 6.22490930557251 | BCE Loss: 1.0616943836212158\n",
      "Epoch 67 / 500 | iteration 5 / 30 | Total Loss: 1.069075345993042 | KNN Loss: 6.225242614746094 | BCE Loss: 1.069075345993042\n",
      "Epoch 67 / 500 | iteration 10 / 30 | Total Loss: 1.0534864664077759 | KNN Loss: 6.224715709686279 | BCE Loss: 1.0534864664077759\n",
      "Epoch 67 / 500 | iteration 15 / 30 | Total Loss: 1.0565861463546753 | KNN Loss: 6.224969863891602 | BCE Loss: 1.0565861463546753\n",
      "Epoch 67 / 500 | iteration 20 / 30 | Total Loss: 1.0472891330718994 | KNN Loss: 6.224883556365967 | BCE Loss: 1.0472891330718994\n",
      "Epoch 67 / 500 | iteration 25 / 30 | Total Loss: 1.0124298334121704 | KNN Loss: 6.224613666534424 | BCE Loss: 1.0124298334121704\n",
      "Epoch 68 / 500 | iteration 0 / 30 | Total Loss: 1.0711026191711426 | KNN Loss: 6.22488260269165 | BCE Loss: 1.0711026191711426\n",
      "Epoch 68 / 500 | iteration 5 / 30 | Total Loss: 1.0386064052581787 | KNN Loss: 6.2248077392578125 | BCE Loss: 1.0386064052581787\n",
      "Epoch 68 / 500 | iteration 10 / 30 | Total Loss: 1.043013095855713 | KNN Loss: 6.225096225738525 | BCE Loss: 1.043013095855713\n",
      "Epoch 68 / 500 | iteration 15 / 30 | Total Loss: 1.0565447807312012 | KNN Loss: 6.225037574768066 | BCE Loss: 1.0565447807312012\n",
      "Epoch 68 / 500 | iteration 20 / 30 | Total Loss: 1.0505990982055664 | KNN Loss: 6.225035190582275 | BCE Loss: 1.0505990982055664\n",
      "Epoch 68 / 500 | iteration 25 / 30 | Total Loss: 1.0270254611968994 | KNN Loss: 6.2247633934021 | BCE Loss: 1.0270254611968994\n",
      "Epoch 69 / 500 | iteration 0 / 30 | Total Loss: 1.049255132675171 | KNN Loss: 6.224706172943115 | BCE Loss: 1.049255132675171\n",
      "Epoch 69 / 500 | iteration 5 / 30 | Total Loss: 1.0506746768951416 | KNN Loss: 6.224762439727783 | BCE Loss: 1.0506746768951416\n",
      "Epoch 69 / 500 | iteration 10 / 30 | Total Loss: 1.033752202987671 | KNN Loss: 6.2249274253845215 | BCE Loss: 1.033752202987671\n",
      "Epoch 69 / 500 | iteration 15 / 30 | Total Loss: 1.0866703987121582 | KNN Loss: 6.2248406410217285 | BCE Loss: 1.0866703987121582\n",
      "Epoch 69 / 500 | iteration 20 / 30 | Total Loss: 1.0423387289047241 | KNN Loss: 6.225034236907959 | BCE Loss: 1.0423387289047241\n",
      "Epoch 69 / 500 | iteration 25 / 30 | Total Loss: 1.0397837162017822 | KNN Loss: 6.224780082702637 | BCE Loss: 1.0397837162017822\n",
      "Epoch 70 / 500 | iteration 0 / 30 | Total Loss: 1.0507193803787231 | KNN Loss: 6.225131034851074 | BCE Loss: 1.0507193803787231\n",
      "Epoch 70 / 500 | iteration 5 / 30 | Total Loss: 1.055284023284912 | KNN Loss: 6.225035190582275 | BCE Loss: 1.055284023284912\n",
      "Epoch 70 / 500 | iteration 10 / 30 | Total Loss: 1.0364258289337158 | KNN Loss: 6.224982261657715 | BCE Loss: 1.0364258289337158\n",
      "Epoch 70 / 500 | iteration 15 / 30 | Total Loss: 1.054281234741211 | KNN Loss: 6.225047588348389 | BCE Loss: 1.054281234741211\n",
      "Epoch 70 / 500 | iteration 20 / 30 | Total Loss: 1.0309960842132568 | KNN Loss: 6.2249436378479 | BCE Loss: 1.0309960842132568\n",
      "Epoch 70 / 500 | iteration 25 / 30 | Total Loss: 1.0872869491577148 | KNN Loss: 6.224966526031494 | BCE Loss: 1.0872869491577148\n",
      "Epoch 71 / 500 | iteration 0 / 30 | Total Loss: 1.018292784690857 | KNN Loss: 6.225168704986572 | BCE Loss: 1.018292784690857\n",
      "Epoch 71 / 500 | iteration 5 / 30 | Total Loss: 1.0584232807159424 | KNN Loss: 6.224991321563721 | BCE Loss: 1.0584232807159424\n",
      "Epoch 71 / 500 | iteration 10 / 30 | Total Loss: 1.0641775131225586 | KNN Loss: 6.225142002105713 | BCE Loss: 1.0641775131225586\n",
      "Epoch 71 / 500 | iteration 15 / 30 | Total Loss: 1.0582484006881714 | KNN Loss: 6.225042819976807 | BCE Loss: 1.0582484006881714\n",
      "Epoch 71 / 500 | iteration 20 / 30 | Total Loss: 1.079744815826416 | KNN Loss: 6.2243523597717285 | BCE Loss: 1.079744815826416\n",
      "Epoch 71 / 500 | iteration 25 / 30 | Total Loss: 1.0464835166931152 | KNN Loss: 6.225331783294678 | BCE Loss: 1.0464835166931152\n",
      "Epoch 72 / 500 | iteration 0 / 30 | Total Loss: 1.0380384922027588 | KNN Loss: 6.224573612213135 | BCE Loss: 1.0380384922027588\n",
      "Epoch 72 / 500 | iteration 5 / 30 | Total Loss: 1.0446560382843018 | KNN Loss: 6.224945068359375 | BCE Loss: 1.0446560382843018\n",
      "Epoch 72 / 500 | iteration 10 / 30 | Total Loss: 1.0636498928070068 | KNN Loss: 6.2248992919921875 | BCE Loss: 1.0636498928070068\n",
      "Epoch 72 / 500 | iteration 15 / 30 | Total Loss: 1.0388562679290771 | KNN Loss: 6.2249627113342285 | BCE Loss: 1.0388562679290771\n",
      "Epoch 72 / 500 | iteration 20 / 30 | Total Loss: 1.0474870204925537 | KNN Loss: 6.225114345550537 | BCE Loss: 1.0474870204925537\n",
      "Epoch 72 / 500 | iteration 25 / 30 | Total Loss: 1.0589373111724854 | KNN Loss: 6.2247538566589355 | BCE Loss: 1.0589373111724854\n",
      "Epoch 73 / 500 | iteration 0 / 30 | Total Loss: 1.0295425653457642 | KNN Loss: 6.224739074707031 | BCE Loss: 1.0295425653457642\n",
      "Epoch 73 / 500 | iteration 5 / 30 | Total Loss: 1.0747883319854736 | KNN Loss: 6.224386215209961 | BCE Loss: 1.0747883319854736\n",
      "Epoch 73 / 500 | iteration 10 / 30 | Total Loss: 1.0358854532241821 | KNN Loss: 6.224447727203369 | BCE Loss: 1.0358854532241821\n",
      "Epoch 73 / 500 | iteration 15 / 30 | Total Loss: 1.015056848526001 | KNN Loss: 6.225022315979004 | BCE Loss: 1.015056848526001\n",
      "Epoch 73 / 500 | iteration 20 / 30 | Total Loss: 1.0773073434829712 | KNN Loss: 6.224998950958252 | BCE Loss: 1.0773073434829712\n",
      "Epoch 73 / 500 | iteration 25 / 30 | Total Loss: 1.005025863647461 | KNN Loss: 6.2248029708862305 | BCE Loss: 1.005025863647461\n",
      "Epoch 74 / 500 | iteration 0 / 30 | Total Loss: 1.0509424209594727 | KNN Loss: 6.224916458129883 | BCE Loss: 1.0509424209594727\n",
      "Epoch 74 / 500 | iteration 5 / 30 | Total Loss: 1.0771653652191162 | KNN Loss: 6.2250542640686035 | BCE Loss: 1.0771653652191162\n",
      "Epoch 74 / 500 | iteration 10 / 30 | Total Loss: 1.0590243339538574 | KNN Loss: 6.224844932556152 | BCE Loss: 1.0590243339538574\n",
      "Epoch 74 / 500 | iteration 15 / 30 | Total Loss: 1.0478163957595825 | KNN Loss: 6.225082874298096 | BCE Loss: 1.0478163957595825\n",
      "Epoch 74 / 500 | iteration 20 / 30 | Total Loss: 1.0589115619659424 | KNN Loss: 6.22469425201416 | BCE Loss: 1.0589115619659424\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74 / 500 | iteration 25 / 30 | Total Loss: 1.07102370262146 | KNN Loss: 6.225177764892578 | BCE Loss: 1.07102370262146\n",
      "Epoch 75 / 500 | iteration 0 / 30 | Total Loss: 1.053309440612793 | KNN Loss: 6.225134372711182 | BCE Loss: 1.053309440612793\n",
      "Epoch 75 / 500 | iteration 5 / 30 | Total Loss: 1.0440880060195923 | KNN Loss: 6.2250823974609375 | BCE Loss: 1.0440880060195923\n",
      "Epoch 75 / 500 | iteration 10 / 30 | Total Loss: 1.0464603900909424 | KNN Loss: 6.2247467041015625 | BCE Loss: 1.0464603900909424\n",
      "Epoch 75 / 500 | iteration 15 / 30 | Total Loss: 1.073265790939331 | KNN Loss: 6.224511623382568 | BCE Loss: 1.073265790939331\n",
      "Epoch 75 / 500 | iteration 20 / 30 | Total Loss: 1.067850112915039 | KNN Loss: 6.224900722503662 | BCE Loss: 1.067850112915039\n",
      "Epoch 75 / 500 | iteration 25 / 30 | Total Loss: 1.0611684322357178 | KNN Loss: 6.224756717681885 | BCE Loss: 1.0611684322357178\n",
      "Epoch 76 / 500 | iteration 0 / 30 | Total Loss: 1.048614501953125 | KNN Loss: 6.224401473999023 | BCE Loss: 1.048614501953125\n",
      "Epoch 76 / 500 | iteration 5 / 30 | Total Loss: 1.038612723350525 | KNN Loss: 6.224727153778076 | BCE Loss: 1.038612723350525\n",
      "Epoch 76 / 500 | iteration 10 / 30 | Total Loss: 1.0664000511169434 | KNN Loss: 6.224864482879639 | BCE Loss: 1.0664000511169434\n",
      "Epoch 76 / 500 | iteration 15 / 30 | Total Loss: 1.0410783290863037 | KNN Loss: 6.224864959716797 | BCE Loss: 1.0410783290863037\n",
      "Epoch 76 / 500 | iteration 20 / 30 | Total Loss: 1.0635721683502197 | KNN Loss: 6.224684715270996 | BCE Loss: 1.0635721683502197\n",
      "Epoch 76 / 500 | iteration 25 / 30 | Total Loss: 1.0645430088043213 | KNN Loss: 6.225022792816162 | BCE Loss: 1.0645430088043213\n",
      "Epoch 77 / 500 | iteration 0 / 30 | Total Loss: 1.0664088726043701 | KNN Loss: 6.224616527557373 | BCE Loss: 1.0664088726043701\n",
      "Epoch 77 / 500 | iteration 5 / 30 | Total Loss: 1.0229591131210327 | KNN Loss: 6.224941253662109 | BCE Loss: 1.0229591131210327\n",
      "Epoch 77 / 500 | iteration 10 / 30 | Total Loss: 1.060023307800293 | KNN Loss: 6.224835395812988 | BCE Loss: 1.060023307800293\n",
      "Epoch 77 / 500 | iteration 15 / 30 | Total Loss: 1.0631355047225952 | KNN Loss: 6.224509239196777 | BCE Loss: 1.0631355047225952\n",
      "Epoch 77 / 500 | iteration 20 / 30 | Total Loss: 1.0456030368804932 | KNN Loss: 6.224515438079834 | BCE Loss: 1.0456030368804932\n",
      "Epoch 77 / 500 | iteration 25 / 30 | Total Loss: 1.0360236167907715 | KNN Loss: 6.224646091461182 | BCE Loss: 1.0360236167907715\n",
      "Epoch 78 / 500 | iteration 0 / 30 | Total Loss: 1.0596444606781006 | KNN Loss: 6.224400997161865 | BCE Loss: 1.0596444606781006\n",
      "Epoch 78 / 500 | iteration 5 / 30 | Total Loss: 1.040992259979248 | KNN Loss: 6.224677562713623 | BCE Loss: 1.040992259979248\n",
      "Epoch 78 / 500 | iteration 10 / 30 | Total Loss: 1.0532525777816772 | KNN Loss: 6.224889755249023 | BCE Loss: 1.0532525777816772\n",
      "Epoch 78 / 500 | iteration 15 / 30 | Total Loss: 1.0136017799377441 | KNN Loss: 6.224880218505859 | BCE Loss: 1.0136017799377441\n",
      "Epoch 78 / 500 | iteration 20 / 30 | Total Loss: 1.0327479839324951 | KNN Loss: 6.2249932289123535 | BCE Loss: 1.0327479839324951\n",
      "Epoch 78 / 500 | iteration 25 / 30 | Total Loss: 1.0417401790618896 | KNN Loss: 6.2248406410217285 | BCE Loss: 1.0417401790618896\n",
      "Epoch    79: reducing learning rate of group 0 to 2.4500e-03.\n",
      "Epoch 79 / 500 | iteration 0 / 30 | Total Loss: 1.029198408126831 | KNN Loss: 6.224907875061035 | BCE Loss: 1.029198408126831\n",
      "Epoch 79 / 500 | iteration 5 / 30 | Total Loss: 1.0451726913452148 | KNN Loss: 6.22462797164917 | BCE Loss: 1.0451726913452148\n",
      "Epoch 79 / 500 | iteration 10 / 30 | Total Loss: 1.0750192403793335 | KNN Loss: 6.225040912628174 | BCE Loss: 1.0750192403793335\n",
      "Epoch 79 / 500 | iteration 15 / 30 | Total Loss: 1.0460258722305298 | KNN Loss: 6.224659442901611 | BCE Loss: 1.0460258722305298\n",
      "Epoch 79 / 500 | iteration 20 / 30 | Total Loss: 1.0298709869384766 | KNN Loss: 6.224454879760742 | BCE Loss: 1.0298709869384766\n",
      "Epoch 79 / 500 | iteration 25 / 30 | Total Loss: 1.0682945251464844 | KNN Loss: 6.22499418258667 | BCE Loss: 1.0682945251464844\n",
      "Epoch 80 / 500 | iteration 0 / 30 | Total Loss: 1.0557669401168823 | KNN Loss: 6.2248125076293945 | BCE Loss: 1.0557669401168823\n",
      "Epoch 80 / 500 | iteration 5 / 30 | Total Loss: 1.042881727218628 | KNN Loss: 6.22444486618042 | BCE Loss: 1.042881727218628\n",
      "Epoch 80 / 500 | iteration 10 / 30 | Total Loss: 1.0408381223678589 | KNN Loss: 6.2247633934021 | BCE Loss: 1.0408381223678589\n",
      "Epoch 80 / 500 | iteration 15 / 30 | Total Loss: 1.065845012664795 | KNN Loss: 6.2244367599487305 | BCE Loss: 1.065845012664795\n",
      "Epoch 80 / 500 | iteration 20 / 30 | Total Loss: 1.0541812181472778 | KNN Loss: 6.22484016418457 | BCE Loss: 1.0541812181472778\n",
      "Epoch 80 / 500 | iteration 25 / 30 | Total Loss: 1.0596222877502441 | KNN Loss: 6.224544525146484 | BCE Loss: 1.0596222877502441\n",
      "Epoch 81 / 500 | iteration 0 / 30 | Total Loss: 1.0396838188171387 | KNN Loss: 6.224793910980225 | BCE Loss: 1.0396838188171387\n",
      "Epoch 81 / 500 | iteration 5 / 30 | Total Loss: 1.0393673181533813 | KNN Loss: 6.22468900680542 | BCE Loss: 1.0393673181533813\n",
      "Epoch 81 / 500 | iteration 10 / 30 | Total Loss: 1.0394511222839355 | KNN Loss: 6.2247796058654785 | BCE Loss: 1.0394511222839355\n",
      "Epoch 81 / 500 | iteration 15 / 30 | Total Loss: 1.0659801959991455 | KNN Loss: 6.224579334259033 | BCE Loss: 1.0659801959991455\n",
      "Epoch 81 / 500 | iteration 20 / 30 | Total Loss: 1.0827499628067017 | KNN Loss: 6.224874019622803 | BCE Loss: 1.0827499628067017\n",
      "Epoch 81 / 500 | iteration 25 / 30 | Total Loss: 1.0246360301971436 | KNN Loss: 6.224666118621826 | BCE Loss: 1.0246360301971436\n",
      "Epoch 82 / 500 | iteration 0 / 30 | Total Loss: 1.0442761182785034 | KNN Loss: 6.224853992462158 | BCE Loss: 1.0442761182785034\n",
      "Epoch 82 / 500 | iteration 5 / 30 | Total Loss: 1.0496485233306885 | KNN Loss: 6.224879264831543 | BCE Loss: 1.0496485233306885\n",
      "Epoch 82 / 500 | iteration 10 / 30 | Total Loss: 1.0462374687194824 | KNN Loss: 6.224761486053467 | BCE Loss: 1.0462374687194824\n",
      "Epoch 82 / 500 | iteration 15 / 30 | Total Loss: 1.0351316928863525 | KNN Loss: 6.224903106689453 | BCE Loss: 1.0351316928863525\n",
      "Epoch 82 / 500 | iteration 20 / 30 | Total Loss: 1.0503941774368286 | KNN Loss: 6.224855899810791 | BCE Loss: 1.0503941774368286\n",
      "Epoch 82 / 500 | iteration 25 / 30 | Total Loss: 1.0441997051239014 | KNN Loss: 6.2250189781188965 | BCE Loss: 1.0441997051239014\n",
      "Epoch 83 / 500 | iteration 0 / 30 | Total Loss: 1.0554938316345215 | KNN Loss: 6.224597454071045 | BCE Loss: 1.0554938316345215\n",
      "Epoch 83 / 500 | iteration 5 / 30 | Total Loss: 1.0261600017547607 | KNN Loss: 6.224350452423096 | BCE Loss: 1.0261600017547607\n",
      "Epoch 83 / 500 | iteration 10 / 30 | Total Loss: 1.0718495845794678 | KNN Loss: 6.224494457244873 | BCE Loss: 1.0718495845794678\n",
      "Epoch 83 / 500 | iteration 15 / 30 | Total Loss: 1.0479158163070679 | KNN Loss: 6.224701881408691 | BCE Loss: 1.0479158163070679\n",
      "Epoch 83 / 500 | iteration 20 / 30 | Total Loss: 1.0512189865112305 | KNN Loss: 6.224508285522461 | BCE Loss: 1.0512189865112305\n",
      "Epoch 83 / 500 | iteration 25 / 30 | Total Loss: 1.0521187782287598 | KNN Loss: 6.224775314331055 | BCE Loss: 1.0521187782287598\n",
      "Epoch 84 / 500 | iteration 0 / 30 | Total Loss: 1.077591061592102 | KNN Loss: 6.224880218505859 | BCE Loss: 1.077591061592102\n",
      "Epoch 84 / 500 | iteration 5 / 30 | Total Loss: 1.0606369972229004 | KNN Loss: 6.2246880531311035 | BCE Loss: 1.0606369972229004\n",
      "Epoch 84 / 500 | iteration 10 / 30 | Total Loss: 1.059923768043518 | KNN Loss: 6.2246270179748535 | BCE Loss: 1.059923768043518\n",
      "Epoch 84 / 500 | iteration 15 / 30 | Total Loss: 1.051283359527588 | KNN Loss: 6.224858283996582 | BCE Loss: 1.051283359527588\n",
      "Epoch 84 / 500 | iteration 20 / 30 | Total Loss: 1.0386667251586914 | KNN Loss: 6.224735736846924 | BCE Loss: 1.0386667251586914\n",
      "Epoch 84 / 500 | iteration 25 / 30 | Total Loss: 1.0217989683151245 | KNN Loss: 6.224860668182373 | BCE Loss: 1.0217989683151245\n",
      "Epoch 85 / 500 | iteration 0 / 30 | Total Loss: 1.0094289779663086 | KNN Loss: 6.225166320800781 | BCE Loss: 1.0094289779663086\n",
      "Epoch 85 / 500 | iteration 5 / 30 | Total Loss: 1.0377769470214844 | KNN Loss: 6.224632740020752 | BCE Loss: 1.0377769470214844\n",
      "Epoch 85 / 500 | iteration 10 / 30 | Total Loss: 1.0452630519866943 | KNN Loss: 6.224660873413086 | BCE Loss: 1.0452630519866943\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85 / 500 | iteration 15 / 30 | Total Loss: 1.0765334367752075 | KNN Loss: 6.22444486618042 | BCE Loss: 1.0765334367752075\n",
      "Epoch 85 / 500 | iteration 20 / 30 | Total Loss: 1.0580940246582031 | KNN Loss: 6.224793910980225 | BCE Loss: 1.0580940246582031\n",
      "Epoch 85 / 500 | iteration 25 / 30 | Total Loss: 1.0606456995010376 | KNN Loss: 6.224514484405518 | BCE Loss: 1.0606456995010376\n",
      "Epoch 86 / 500 | iteration 0 / 30 | Total Loss: 1.044396162033081 | KNN Loss: 6.224551677703857 | BCE Loss: 1.044396162033081\n",
      "Epoch 86 / 500 | iteration 5 / 30 | Total Loss: 1.033923864364624 | KNN Loss: 6.224844932556152 | BCE Loss: 1.033923864364624\n",
      "Epoch 86 / 500 | iteration 10 / 30 | Total Loss: 1.0635799169540405 | KNN Loss: 6.224798202514648 | BCE Loss: 1.0635799169540405\n",
      "Epoch 86 / 500 | iteration 15 / 30 | Total Loss: 1.0358927249908447 | KNN Loss: 6.224663257598877 | BCE Loss: 1.0358927249908447\n",
      "Epoch 86 / 500 | iteration 20 / 30 | Total Loss: 1.0440495014190674 | KNN Loss: 6.224752902984619 | BCE Loss: 1.0440495014190674\n",
      "Epoch 86 / 500 | iteration 25 / 30 | Total Loss: 1.0605876445770264 | KNN Loss: 6.224630832672119 | BCE Loss: 1.0605876445770264\n",
      "Epoch 87 / 500 | iteration 0 / 30 | Total Loss: 1.0576059818267822 | KNN Loss: 6.224424839019775 | BCE Loss: 1.0576059818267822\n",
      "Epoch 87 / 500 | iteration 5 / 30 | Total Loss: 1.038632869720459 | KNN Loss: 6.224907875061035 | BCE Loss: 1.038632869720459\n",
      "Epoch 87 / 500 | iteration 10 / 30 | Total Loss: 1.0694105625152588 | KNN Loss: 6.224551200866699 | BCE Loss: 1.0694105625152588\n",
      "Epoch 87 / 500 | iteration 15 / 30 | Total Loss: 1.0357365608215332 | KNN Loss: 6.224708557128906 | BCE Loss: 1.0357365608215332\n",
      "Epoch 87 / 500 | iteration 20 / 30 | Total Loss: 1.023563265800476 | KNN Loss: 6.224968910217285 | BCE Loss: 1.023563265800476\n",
      "Epoch 87 / 500 | iteration 25 / 30 | Total Loss: 1.0355145931243896 | KNN Loss: 6.2249436378479 | BCE Loss: 1.0355145931243896\n",
      "Epoch 88 / 500 | iteration 0 / 30 | Total Loss: 1.0664777755737305 | KNN Loss: 6.224719047546387 | BCE Loss: 1.0664777755737305\n",
      "Epoch 88 / 500 | iteration 5 / 30 | Total Loss: 1.0735403299331665 | KNN Loss: 6.2248735427856445 | BCE Loss: 1.0735403299331665\n",
      "Epoch 88 / 500 | iteration 10 / 30 | Total Loss: 1.057064414024353 | KNN Loss: 6.224803447723389 | BCE Loss: 1.057064414024353\n",
      "Epoch 88 / 500 | iteration 15 / 30 | Total Loss: 1.0487520694732666 | KNN Loss: 6.2243971824646 | BCE Loss: 1.0487520694732666\n",
      "Epoch 88 / 500 | iteration 20 / 30 | Total Loss: 1.0549689531326294 | KNN Loss: 6.2243852615356445 | BCE Loss: 1.0549689531326294\n",
      "Epoch 88 / 500 | iteration 25 / 30 | Total Loss: 1.0365209579467773 | KNN Loss: 6.224510669708252 | BCE Loss: 1.0365209579467773\n",
      "Epoch 89 / 500 | iteration 0 / 30 | Total Loss: 1.0459281206130981 | KNN Loss: 6.22450590133667 | BCE Loss: 1.0459281206130981\n",
      "Epoch 89 / 500 | iteration 5 / 30 | Total Loss: 1.0424063205718994 | KNN Loss: 6.224681377410889 | BCE Loss: 1.0424063205718994\n",
      "Epoch 89 / 500 | iteration 10 / 30 | Total Loss: 1.0494766235351562 | KNN Loss: 6.224472999572754 | BCE Loss: 1.0494766235351562\n",
      "Epoch 89 / 500 | iteration 15 / 30 | Total Loss: 1.0507220029830933 | KNN Loss: 6.224706172943115 | BCE Loss: 1.0507220029830933\n",
      "Epoch 89 / 500 | iteration 20 / 30 | Total Loss: 1.054238200187683 | KNN Loss: 6.224874019622803 | BCE Loss: 1.054238200187683\n",
      "Epoch 89 / 500 | iteration 25 / 30 | Total Loss: 1.0489115715026855 | KNN Loss: 6.224790573120117 | BCE Loss: 1.0489115715026855\n",
      "Epoch    90: reducing learning rate of group 0 to 1.7150e-03.\n",
      "Epoch 90 / 500 | iteration 0 / 30 | Total Loss: 1.0698184967041016 | KNN Loss: 6.22482967376709 | BCE Loss: 1.0698184967041016\n",
      "Epoch 90 / 500 | iteration 5 / 30 | Total Loss: 1.0374518632888794 | KNN Loss: 6.224687576293945 | BCE Loss: 1.0374518632888794\n",
      "Epoch 90 / 500 | iteration 10 / 30 | Total Loss: 1.0617916584014893 | KNN Loss: 6.224978446960449 | BCE Loss: 1.0617916584014893\n",
      "Epoch 90 / 500 | iteration 15 / 30 | Total Loss: 1.0551848411560059 | KNN Loss: 6.22482442855835 | BCE Loss: 1.0551848411560059\n",
      "Epoch 90 / 500 | iteration 20 / 30 | Total Loss: 1.0491619110107422 | KNN Loss: 6.2245283126831055 | BCE Loss: 1.0491619110107422\n",
      "Epoch 90 / 500 | iteration 25 / 30 | Total Loss: 1.0823743343353271 | KNN Loss: 6.2247700691223145 | BCE Loss: 1.0823743343353271\n",
      "Epoch 91 / 500 | iteration 0 / 30 | Total Loss: 1.0586421489715576 | KNN Loss: 6.224758148193359 | BCE Loss: 1.0586421489715576\n",
      "Epoch 91 / 500 | iteration 5 / 30 | Total Loss: 1.0388152599334717 | KNN Loss: 6.224693298339844 | BCE Loss: 1.0388152599334717\n",
      "Epoch 91 / 500 | iteration 10 / 30 | Total Loss: 1.0469026565551758 | KNN Loss: 6.224484443664551 | BCE Loss: 1.0469026565551758\n",
      "Epoch 91 / 500 | iteration 15 / 30 | Total Loss: 1.0470095872879028 | KNN Loss: 6.224740505218506 | BCE Loss: 1.0470095872879028\n",
      "Epoch 91 / 500 | iteration 20 / 30 | Total Loss: 1.0494834184646606 | KNN Loss: 6.224893093109131 | BCE Loss: 1.0494834184646606\n",
      "Epoch 91 / 500 | iteration 25 / 30 | Total Loss: 1.0539979934692383 | KNN Loss: 6.224453926086426 | BCE Loss: 1.0539979934692383\n",
      "Epoch 92 / 500 | iteration 0 / 30 | Total Loss: 1.0543888807296753 | KNN Loss: 6.224650859832764 | BCE Loss: 1.0543888807296753\n",
      "Epoch 92 / 500 | iteration 5 / 30 | Total Loss: 1.049851655960083 | KNN Loss: 6.224872589111328 | BCE Loss: 1.049851655960083\n",
      "Epoch 92 / 500 | iteration 10 / 30 | Total Loss: 1.0517241954803467 | KNN Loss: 6.224327087402344 | BCE Loss: 1.0517241954803467\n",
      "Epoch 92 / 500 | iteration 15 / 30 | Total Loss: 1.0541640520095825 | KNN Loss: 6.224119186401367 | BCE Loss: 1.0541640520095825\n",
      "Epoch 92 / 500 | iteration 20 / 30 | Total Loss: 1.0549347400665283 | KNN Loss: 6.224547386169434 | BCE Loss: 1.0549347400665283\n",
      "Epoch 92 / 500 | iteration 25 / 30 | Total Loss: 1.0688393115997314 | KNN Loss: 6.224227428436279 | BCE Loss: 1.0688393115997314\n",
      "Epoch 93 / 500 | iteration 0 / 30 | Total Loss: 1.0337103605270386 | KNN Loss: 6.2248053550720215 | BCE Loss: 1.0337103605270386\n",
      "Epoch 93 / 500 | iteration 5 / 30 | Total Loss: 1.0893709659576416 | KNN Loss: 6.224820137023926 | BCE Loss: 1.0893709659576416\n",
      "Epoch 93 / 500 | iteration 10 / 30 | Total Loss: 1.043203592300415 | KNN Loss: 6.224706649780273 | BCE Loss: 1.043203592300415\n",
      "Epoch 93 / 500 | iteration 15 / 30 | Total Loss: 1.052740454673767 | KNN Loss: 6.224961280822754 | BCE Loss: 1.052740454673767\n",
      "Epoch 93 / 500 | iteration 20 / 30 | Total Loss: 1.0251522064208984 | KNN Loss: 6.2246994972229 | BCE Loss: 1.0251522064208984\n",
      "Epoch 93 / 500 | iteration 25 / 30 | Total Loss: 1.0319280624389648 | KNN Loss: 6.224478721618652 | BCE Loss: 1.0319280624389648\n",
      "Epoch 94 / 500 | iteration 0 / 30 | Total Loss: 1.045057773590088 | KNN Loss: 6.22490119934082 | BCE Loss: 1.045057773590088\n",
      "Epoch 94 / 500 | iteration 5 / 30 | Total Loss: 1.0724060535430908 | KNN Loss: 6.224666595458984 | BCE Loss: 1.0724060535430908\n",
      "Epoch 94 / 500 | iteration 10 / 30 | Total Loss: 1.0605309009552002 | KNN Loss: 6.224965572357178 | BCE Loss: 1.0605309009552002\n",
      "Epoch 94 / 500 | iteration 15 / 30 | Total Loss: 1.0608830451965332 | KNN Loss: 6.224699974060059 | BCE Loss: 1.0608830451965332\n",
      "Epoch 94 / 500 | iteration 20 / 30 | Total Loss: 1.0394788980484009 | KNN Loss: 6.22426176071167 | BCE Loss: 1.0394788980484009\n",
      "Epoch 94 / 500 | iteration 25 / 30 | Total Loss: 1.0545670986175537 | KNN Loss: 6.224303722381592 | BCE Loss: 1.0545670986175537\n",
      "Epoch 95 / 500 | iteration 0 / 30 | Total Loss: 1.0699996948242188 | KNN Loss: 6.224785804748535 | BCE Loss: 1.0699996948242188\n",
      "Epoch 95 / 500 | iteration 5 / 30 | Total Loss: 1.0887839794158936 | KNN Loss: 6.224287033081055 | BCE Loss: 1.0887839794158936\n",
      "Epoch 95 / 500 | iteration 10 / 30 | Total Loss: 1.061516284942627 | KNN Loss: 6.224092483520508 | BCE Loss: 1.061516284942627\n",
      "Epoch 95 / 500 | iteration 15 / 30 | Total Loss: 1.0385774374008179 | KNN Loss: 6.2248077392578125 | BCE Loss: 1.0385774374008179\n",
      "Epoch 95 / 500 | iteration 20 / 30 | Total Loss: 1.0419659614562988 | KNN Loss: 6.224455833435059 | BCE Loss: 1.0419659614562988\n",
      "Epoch 95 / 500 | iteration 25 / 30 | Total Loss: 1.0530130863189697 | KNN Loss: 6.22506046295166 | BCE Loss: 1.0530130863189697\n",
      "Epoch 96 / 500 | iteration 0 / 30 | Total Loss: 1.0470777750015259 | KNN Loss: 6.225019931793213 | BCE Loss: 1.0470777750015259\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96 / 500 | iteration 5 / 30 | Total Loss: 1.0582730770111084 | KNN Loss: 6.224781513214111 | BCE Loss: 1.0582730770111084\n",
      "Epoch 96 / 500 | iteration 10 / 30 | Total Loss: 1.0543168783187866 | KNN Loss: 6.224903106689453 | BCE Loss: 1.0543168783187866\n",
      "Epoch 96 / 500 | iteration 15 / 30 | Total Loss: 1.0624308586120605 | KNN Loss: 6.224381446838379 | BCE Loss: 1.0624308586120605\n",
      "Epoch 96 / 500 | iteration 20 / 30 | Total Loss: 1.0421669483184814 | KNN Loss: 6.224800109863281 | BCE Loss: 1.0421669483184814\n",
      "Epoch 96 / 500 | iteration 25 / 30 | Total Loss: 1.0481914281845093 | KNN Loss: 6.224433898925781 | BCE Loss: 1.0481914281845093\n",
      "Epoch 97 / 500 | iteration 0 / 30 | Total Loss: 1.0442655086517334 | KNN Loss: 6.224995136260986 | BCE Loss: 1.0442655086517334\n",
      "Epoch 97 / 500 | iteration 5 / 30 | Total Loss: 1.0551190376281738 | KNN Loss: 6.224913120269775 | BCE Loss: 1.0551190376281738\n",
      "Epoch 97 / 500 | iteration 10 / 30 | Total Loss: 1.0600216388702393 | KNN Loss: 6.224800109863281 | BCE Loss: 1.0600216388702393\n",
      "Epoch 97 / 500 | iteration 15 / 30 | Total Loss: 1.049729347229004 | KNN Loss: 6.224903106689453 | BCE Loss: 1.049729347229004\n",
      "Epoch 97 / 500 | iteration 20 / 30 | Total Loss: 1.0608603954315186 | KNN Loss: 6.224827289581299 | BCE Loss: 1.0608603954315186\n",
      "Epoch 97 / 500 | iteration 25 / 30 | Total Loss: 1.0649000406265259 | KNN Loss: 6.22488260269165 | BCE Loss: 1.0649000406265259\n",
      "Epoch 98 / 500 | iteration 0 / 30 | Total Loss: 1.0511232614517212 | KNN Loss: 6.2243733406066895 | BCE Loss: 1.0511232614517212\n",
      "Epoch 98 / 500 | iteration 5 / 30 | Total Loss: 1.0225247144699097 | KNN Loss: 6.224318504333496 | BCE Loss: 1.0225247144699097\n",
      "Epoch 98 / 500 | iteration 10 / 30 | Total Loss: 1.0240024328231812 | KNN Loss: 6.224447727203369 | BCE Loss: 1.0240024328231812\n",
      "Epoch 98 / 500 | iteration 15 / 30 | Total Loss: 1.0301287174224854 | KNN Loss: 6.224491119384766 | BCE Loss: 1.0301287174224854\n",
      "Epoch 98 / 500 | iteration 20 / 30 | Total Loss: 1.033005952835083 | KNN Loss: 6.224679946899414 | BCE Loss: 1.033005952835083\n",
      "Epoch 98 / 500 | iteration 25 / 30 | Total Loss: 1.0617347955703735 | KNN Loss: 6.224710941314697 | BCE Loss: 1.0617347955703735\n",
      "Epoch 99 / 500 | iteration 0 / 30 | Total Loss: 1.0698363780975342 | KNN Loss: 6.224852561950684 | BCE Loss: 1.0698363780975342\n",
      "Epoch 99 / 500 | iteration 5 / 30 | Total Loss: 1.0744807720184326 | KNN Loss: 6.224549770355225 | BCE Loss: 1.0744807720184326\n",
      "Epoch 99 / 500 | iteration 10 / 30 | Total Loss: 1.0480965375900269 | KNN Loss: 6.224926948547363 | BCE Loss: 1.0480965375900269\n",
      "Epoch 99 / 500 | iteration 15 / 30 | Total Loss: 1.0530357360839844 | KNN Loss: 6.224822998046875 | BCE Loss: 1.0530357360839844\n",
      "Epoch 99 / 500 | iteration 20 / 30 | Total Loss: 1.0296283960342407 | KNN Loss: 6.224831581115723 | BCE Loss: 1.0296283960342407\n",
      "Epoch 99 / 500 | iteration 25 / 30 | Total Loss: 1.0690884590148926 | KNN Loss: 6.224785327911377 | BCE Loss: 1.0690884590148926\n",
      "Epoch 100 / 500 | iteration 0 / 30 | Total Loss: 1.0346108675003052 | KNN Loss: 6.224296569824219 | BCE Loss: 1.0346108675003052\n",
      "Epoch 100 / 500 | iteration 5 / 30 | Total Loss: 1.0709381103515625 | KNN Loss: 6.224533557891846 | BCE Loss: 1.0709381103515625\n",
      "Epoch 100 / 500 | iteration 10 / 30 | Total Loss: 1.0515434741973877 | KNN Loss: 6.22520637512207 | BCE Loss: 1.0515434741973877\n",
      "Epoch 100 / 500 | iteration 15 / 30 | Total Loss: 1.090014934539795 | KNN Loss: 6.2248992919921875 | BCE Loss: 1.090014934539795\n",
      "Epoch 100 / 500 | iteration 20 / 30 | Total Loss: 1.0168843269348145 | KNN Loss: 6.224607944488525 | BCE Loss: 1.0168843269348145\n",
      "Epoch 100 / 500 | iteration 25 / 30 | Total Loss: 1.0607808828353882 | KNN Loss: 6.224815368652344 | BCE Loss: 1.0607808828353882\n",
      "Epoch   101: reducing learning rate of group 0 to 1.2005e-03.\n",
      "Epoch 101 / 500 | iteration 0 / 30 | Total Loss: 1.0731755495071411 | KNN Loss: 6.224627494812012 | BCE Loss: 1.0731755495071411\n",
      "Epoch 101 / 500 | iteration 5 / 30 | Total Loss: 1.020260214805603 | KNN Loss: 6.224607944488525 | BCE Loss: 1.020260214805603\n",
      "Epoch 101 / 500 | iteration 10 / 30 | Total Loss: 1.0459098815917969 | KNN Loss: 6.224651336669922 | BCE Loss: 1.0459098815917969\n",
      "Epoch 101 / 500 | iteration 15 / 30 | Total Loss: 1.0931994915008545 | KNN Loss: 6.224861145019531 | BCE Loss: 1.0931994915008545\n",
      "Epoch 101 / 500 | iteration 20 / 30 | Total Loss: 1.0512909889221191 | KNN Loss: 6.224625587463379 | BCE Loss: 1.0512909889221191\n",
      "Epoch 101 / 500 | iteration 25 / 30 | Total Loss: 1.075631856918335 | KNN Loss: 6.224671363830566 | BCE Loss: 1.075631856918335\n",
      "Epoch 102 / 500 | iteration 0 / 30 | Total Loss: 1.0592411756515503 | KNN Loss: 6.224610805511475 | BCE Loss: 1.0592411756515503\n",
      "Epoch 102 / 500 | iteration 5 / 30 | Total Loss: 1.0501289367675781 | KNN Loss: 6.224899768829346 | BCE Loss: 1.0501289367675781\n",
      "Epoch 102 / 500 | iteration 10 / 30 | Total Loss: 1.0823287963867188 | KNN Loss: 6.2242865562438965 | BCE Loss: 1.0823287963867188\n",
      "Epoch 102 / 500 | iteration 15 / 30 | Total Loss: 1.0396499633789062 | KNN Loss: 6.2249040603637695 | BCE Loss: 1.0396499633789062\n",
      "Epoch 102 / 500 | iteration 20 / 30 | Total Loss: 1.0443065166473389 | KNN Loss: 6.224490165710449 | BCE Loss: 1.0443065166473389\n",
      "Epoch 102 / 500 | iteration 25 / 30 | Total Loss: 1.0573539733886719 | KNN Loss: 6.224700450897217 | BCE Loss: 1.0573539733886719\n",
      "Epoch 103 / 500 | iteration 0 / 30 | Total Loss: 1.0521953105926514 | KNN Loss: 6.224449157714844 | BCE Loss: 1.0521953105926514\n",
      "Epoch 103 / 500 | iteration 5 / 30 | Total Loss: 1.0631872415542603 | KNN Loss: 6.22471284866333 | BCE Loss: 1.0631872415542603\n",
      "Epoch 103 / 500 | iteration 10 / 30 | Total Loss: 1.022956132888794 | KNN Loss: 6.224587917327881 | BCE Loss: 1.022956132888794\n",
      "Epoch 103 / 500 | iteration 15 / 30 | Total Loss: 1.0501766204833984 | KNN Loss: 6.224684715270996 | BCE Loss: 1.0501766204833984\n",
      "Epoch 103 / 500 | iteration 20 / 30 | Total Loss: 1.0536956787109375 | KNN Loss: 6.224368095397949 | BCE Loss: 1.0536956787109375\n",
      "Epoch 103 / 500 | iteration 25 / 30 | Total Loss: 1.0508041381835938 | KNN Loss: 6.224465847015381 | BCE Loss: 1.0508041381835938\n",
      "Epoch 104 / 500 | iteration 0 / 30 | Total Loss: 1.0464582443237305 | KNN Loss: 6.224795341491699 | BCE Loss: 1.0464582443237305\n",
      "Epoch 104 / 500 | iteration 5 / 30 | Total Loss: 1.0305289030075073 | KNN Loss: 6.224837779998779 | BCE Loss: 1.0305289030075073\n",
      "Epoch 104 / 500 | iteration 10 / 30 | Total Loss: 1.0650935173034668 | KNN Loss: 6.2247233390808105 | BCE Loss: 1.0650935173034668\n",
      "Epoch 104 / 500 | iteration 15 / 30 | Total Loss: 1.049686074256897 | KNN Loss: 6.224210739135742 | BCE Loss: 1.049686074256897\n",
      "Epoch 104 / 500 | iteration 20 / 30 | Total Loss: 1.0749695301055908 | KNN Loss: 6.2245941162109375 | BCE Loss: 1.0749695301055908\n",
      "Epoch 104 / 500 | iteration 25 / 30 | Total Loss: 1.0619745254516602 | KNN Loss: 6.224694728851318 | BCE Loss: 1.0619745254516602\n",
      "Epoch 105 / 500 | iteration 0 / 30 | Total Loss: 1.0640227794647217 | KNN Loss: 6.224236488342285 | BCE Loss: 1.0640227794647217\n",
      "Epoch 105 / 500 | iteration 5 / 30 | Total Loss: 1.046804666519165 | KNN Loss: 6.224221706390381 | BCE Loss: 1.046804666519165\n",
      "Epoch 105 / 500 | iteration 10 / 30 | Total Loss: 1.060300350189209 | KNN Loss: 6.2246222496032715 | BCE Loss: 1.060300350189209\n",
      "Epoch 105 / 500 | iteration 15 / 30 | Total Loss: 1.04878568649292 | KNN Loss: 6.224549293518066 | BCE Loss: 1.04878568649292\n",
      "Epoch 105 / 500 | iteration 20 / 30 | Total Loss: 1.0329080820083618 | KNN Loss: 6.22499418258667 | BCE Loss: 1.0329080820083618\n",
      "Epoch 105 / 500 | iteration 25 / 30 | Total Loss: 1.0683073997497559 | KNN Loss: 6.224541187286377 | BCE Loss: 1.0683073997497559\n",
      "Epoch 106 / 500 | iteration 0 / 30 | Total Loss: 1.032653570175171 | KNN Loss: 6.224798202514648 | BCE Loss: 1.032653570175171\n",
      "Epoch 106 / 500 | iteration 5 / 30 | Total Loss: 1.0745840072631836 | KNN Loss: 6.2242207527160645 | BCE Loss: 1.0745840072631836\n",
      "Epoch 106 / 500 | iteration 10 / 30 | Total Loss: 1.070096492767334 | KNN Loss: 6.22468376159668 | BCE Loss: 1.070096492767334\n",
      "Epoch 106 / 500 | iteration 15 / 30 | Total Loss: 1.0845271348953247 | KNN Loss: 6.224392414093018 | BCE Loss: 1.0845271348953247\n",
      "Epoch 106 / 500 | iteration 20 / 30 | Total Loss: 1.0306286811828613 | KNN Loss: 6.224738597869873 | BCE Loss: 1.0306286811828613\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 106 / 500 | iteration 25 / 30 | Total Loss: 1.0405049324035645 | KNN Loss: 6.2249298095703125 | BCE Loss: 1.0405049324035645\n",
      "Epoch 107 / 500 | iteration 0 / 30 | Total Loss: 1.0596915483474731 | KNN Loss: 6.224476337432861 | BCE Loss: 1.0596915483474731\n",
      "Epoch 107 / 500 | iteration 5 / 30 | Total Loss: 1.048195719718933 | KNN Loss: 6.22458553314209 | BCE Loss: 1.048195719718933\n",
      "Epoch 107 / 500 | iteration 10 / 30 | Total Loss: 1.0707423686981201 | KNN Loss: 6.224493980407715 | BCE Loss: 1.0707423686981201\n",
      "Epoch 107 / 500 | iteration 15 / 30 | Total Loss: 1.0522383451461792 | KNN Loss: 6.224863529205322 | BCE Loss: 1.0522383451461792\n",
      "Epoch 107 / 500 | iteration 20 / 30 | Total Loss: 1.078044056892395 | KNN Loss: 6.224877834320068 | BCE Loss: 1.078044056892395\n",
      "Epoch 107 / 500 | iteration 25 / 30 | Total Loss: 1.0453369617462158 | KNN Loss: 6.224391937255859 | BCE Loss: 1.0453369617462158\n",
      "Epoch 108 / 500 | iteration 0 / 30 | Total Loss: 1.0545514822006226 | KNN Loss: 6.224766254425049 | BCE Loss: 1.0545514822006226\n",
      "Epoch 108 / 500 | iteration 5 / 30 | Total Loss: 1.056136965751648 | KNN Loss: 6.224355220794678 | BCE Loss: 1.056136965751648\n",
      "Epoch 108 / 500 | iteration 10 / 30 | Total Loss: 1.0617443323135376 | KNN Loss: 6.224390029907227 | BCE Loss: 1.0617443323135376\n",
      "Epoch 108 / 500 | iteration 15 / 30 | Total Loss: 1.073859691619873 | KNN Loss: 6.224514961242676 | BCE Loss: 1.073859691619873\n",
      "Epoch 108 / 500 | iteration 20 / 30 | Total Loss: 1.0439939498901367 | KNN Loss: 6.224293231964111 | BCE Loss: 1.0439939498901367\n",
      "Epoch 108 / 500 | iteration 25 / 30 | Total Loss: 1.0724458694458008 | KNN Loss: 6.224784851074219 | BCE Loss: 1.0724458694458008\n",
      "Epoch 109 / 500 | iteration 0 / 30 | Total Loss: 1.058924913406372 | KNN Loss: 6.224885940551758 | BCE Loss: 1.058924913406372\n",
      "Epoch 109 / 500 | iteration 5 / 30 | Total Loss: 1.0498414039611816 | KNN Loss: 6.224276065826416 | BCE Loss: 1.0498414039611816\n",
      "Epoch 109 / 500 | iteration 10 / 30 | Total Loss: 1.0358161926269531 | KNN Loss: 6.22446870803833 | BCE Loss: 1.0358161926269531\n",
      "Epoch 109 / 500 | iteration 15 / 30 | Total Loss: 1.0424553155899048 | KNN Loss: 6.22459077835083 | BCE Loss: 1.0424553155899048\n",
      "Epoch 109 / 500 | iteration 20 / 30 | Total Loss: 1.0601345300674438 | KNN Loss: 6.22451114654541 | BCE Loss: 1.0601345300674438\n",
      "Epoch 109 / 500 | iteration 25 / 30 | Total Loss: 1.059212327003479 | KNN Loss: 6.224529266357422 | BCE Loss: 1.059212327003479\n",
      "Epoch 110 / 500 | iteration 0 / 30 | Total Loss: 1.0767543315887451 | KNN Loss: 6.224879264831543 | BCE Loss: 1.0767543315887451\n",
      "Epoch 110 / 500 | iteration 5 / 30 | Total Loss: 1.0382208824157715 | KNN Loss: 6.224536418914795 | BCE Loss: 1.0382208824157715\n",
      "Epoch 110 / 500 | iteration 10 / 30 | Total Loss: 1.0584951639175415 | KNN Loss: 6.224658012390137 | BCE Loss: 1.0584951639175415\n",
      "Epoch 110 / 500 | iteration 15 / 30 | Total Loss: 1.0455095767974854 | KNN Loss: 6.224728584289551 | BCE Loss: 1.0455095767974854\n",
      "Epoch 110 / 500 | iteration 20 / 30 | Total Loss: 0.9960467219352722 | KNN Loss: 6.224148273468018 | BCE Loss: 0.9960467219352722\n",
      "Epoch 110 / 500 | iteration 25 / 30 | Total Loss: 1.039135217666626 | KNN Loss: 6.224786281585693 | BCE Loss: 1.039135217666626\n",
      "Epoch 111 / 500 | iteration 0 / 30 | Total Loss: 1.0691678524017334 | KNN Loss: 6.224196434020996 | BCE Loss: 1.0691678524017334\n",
      "Epoch 111 / 500 | iteration 5 / 30 | Total Loss: 1.0246042013168335 | KNN Loss: 6.22462272644043 | BCE Loss: 1.0246042013168335\n",
      "Epoch 111 / 500 | iteration 10 / 30 | Total Loss: 1.0590600967407227 | KNN Loss: 6.2247843742370605 | BCE Loss: 1.0590600967407227\n",
      "Epoch 111 / 500 | iteration 15 / 30 | Total Loss: 1.0647025108337402 | KNN Loss: 6.224792957305908 | BCE Loss: 1.0647025108337402\n",
      "Epoch 111 / 500 | iteration 20 / 30 | Total Loss: 1.0327181816101074 | KNN Loss: 6.224616050720215 | BCE Loss: 1.0327181816101074\n",
      "Epoch 111 / 500 | iteration 25 / 30 | Total Loss: 1.0375851392745972 | KNN Loss: 6.224809646606445 | BCE Loss: 1.0375851392745972\n",
      "Epoch   112: reducing learning rate of group 0 to 8.4035e-04.\n",
      "Epoch 112 / 500 | iteration 0 / 30 | Total Loss: 1.024754524230957 | KNN Loss: 6.224435329437256 | BCE Loss: 1.024754524230957\n",
      "Epoch 112 / 500 | iteration 5 / 30 | Total Loss: 1.0762624740600586 | KNN Loss: 6.224886894226074 | BCE Loss: 1.0762624740600586\n",
      "Epoch 112 / 500 | iteration 10 / 30 | Total Loss: 1.0382683277130127 | KNN Loss: 6.224367141723633 | BCE Loss: 1.0382683277130127\n",
      "Epoch 112 / 500 | iteration 15 / 30 | Total Loss: 1.066335916519165 | KNN Loss: 6.224481582641602 | BCE Loss: 1.066335916519165\n",
      "Epoch 112 / 500 | iteration 20 / 30 | Total Loss: 1.0667591094970703 | KNN Loss: 6.22484827041626 | BCE Loss: 1.0667591094970703\n",
      "Epoch 112 / 500 | iteration 25 / 30 | Total Loss: 1.0520570278167725 | KNN Loss: 6.224266529083252 | BCE Loss: 1.0520570278167725\n",
      "Epoch 113 / 500 | iteration 0 / 30 | Total Loss: 1.0476826429367065 | KNN Loss: 6.224640846252441 | BCE Loss: 1.0476826429367065\n",
      "Epoch 113 / 500 | iteration 5 / 30 | Total Loss: 1.0539190769195557 | KNN Loss: 6.224222660064697 | BCE Loss: 1.0539190769195557\n",
      "Epoch 113 / 500 | iteration 10 / 30 | Total Loss: 1.0619964599609375 | KNN Loss: 6.224444389343262 | BCE Loss: 1.0619964599609375\n",
      "Epoch 113 / 500 | iteration 15 / 30 | Total Loss: 1.0686731338500977 | KNN Loss: 6.224363327026367 | BCE Loss: 1.0686731338500977\n",
      "Epoch 113 / 500 | iteration 20 / 30 | Total Loss: 1.0365087985992432 | KNN Loss: 6.225171089172363 | BCE Loss: 1.0365087985992432\n",
      "Epoch 113 / 500 | iteration 25 / 30 | Total Loss: 1.0383963584899902 | KNN Loss: 6.224858283996582 | BCE Loss: 1.0383963584899902\n",
      "Epoch 114 / 500 | iteration 0 / 30 | Total Loss: 1.072599172592163 | KNN Loss: 6.224695682525635 | BCE Loss: 1.072599172592163\n",
      "Epoch 114 / 500 | iteration 5 / 30 | Total Loss: 1.051990032196045 | KNN Loss: 6.224293231964111 | BCE Loss: 1.051990032196045\n",
      "Epoch 114 / 500 | iteration 10 / 30 | Total Loss: 1.0629639625549316 | KNN Loss: 6.2244367599487305 | BCE Loss: 1.0629639625549316\n",
      "Epoch 114 / 500 | iteration 15 / 30 | Total Loss: 1.0678188800811768 | KNN Loss: 6.224207878112793 | BCE Loss: 1.0678188800811768\n",
      "Epoch 114 / 500 | iteration 20 / 30 | Total Loss: 1.0660990476608276 | KNN Loss: 6.22431755065918 | BCE Loss: 1.0660990476608276\n",
      "Epoch 114 / 500 | iteration 25 / 30 | Total Loss: 1.0618404150009155 | KNN Loss: 6.224438190460205 | BCE Loss: 1.0618404150009155\n",
      "Epoch 115 / 500 | iteration 0 / 30 | Total Loss: 1.0611711740493774 | KNN Loss: 6.224107265472412 | BCE Loss: 1.0611711740493774\n",
      "Epoch 115 / 500 | iteration 5 / 30 | Total Loss: 1.0951383113861084 | KNN Loss: 6.224846363067627 | BCE Loss: 1.0951383113861084\n",
      "Epoch 115 / 500 | iteration 10 / 30 | Total Loss: 1.0439969301223755 | KNN Loss: 6.224214553833008 | BCE Loss: 1.0439969301223755\n",
      "Epoch 115 / 500 | iteration 15 / 30 | Total Loss: 1.028913974761963 | KNN Loss: 6.224623680114746 | BCE Loss: 1.028913974761963\n",
      "Epoch 115 / 500 | iteration 20 / 30 | Total Loss: 1.034879446029663 | KNN Loss: 6.224397659301758 | BCE Loss: 1.034879446029663\n",
      "Epoch 115 / 500 | iteration 25 / 30 | Total Loss: 1.0515793561935425 | KNN Loss: 6.224749565124512 | BCE Loss: 1.0515793561935425\n",
      "Epoch 116 / 500 | iteration 0 / 30 | Total Loss: 1.0380642414093018 | KNN Loss: 6.224409103393555 | BCE Loss: 1.0380642414093018\n",
      "Epoch 116 / 500 | iteration 5 / 30 | Total Loss: 1.0615603923797607 | KNN Loss: 6.224490165710449 | BCE Loss: 1.0615603923797607\n",
      "Epoch 116 / 500 | iteration 10 / 30 | Total Loss: 1.0394724607467651 | KNN Loss: 6.224458694458008 | BCE Loss: 1.0394724607467651\n",
      "Epoch 116 / 500 | iteration 15 / 30 | Total Loss: 1.0476456880569458 | KNN Loss: 6.2244720458984375 | BCE Loss: 1.0476456880569458\n",
      "Epoch 116 / 500 | iteration 20 / 30 | Total Loss: 1.0246150493621826 | KNN Loss: 6.22446870803833 | BCE Loss: 1.0246150493621826\n",
      "Epoch 116 / 500 | iteration 25 / 30 | Total Loss: 1.0511362552642822 | KNN Loss: 6.224786281585693 | BCE Loss: 1.0511362552642822\n",
      "Epoch 117 / 500 | iteration 0 / 30 | Total Loss: 1.0797209739685059 | KNN Loss: 6.224865436553955 | BCE Loss: 1.0797209739685059\n",
      "Epoch 117 / 500 | iteration 5 / 30 | Total Loss: 1.0846049785614014 | KNN Loss: 6.224470138549805 | BCE Loss: 1.0846049785614014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 117 / 500 | iteration 10 / 30 | Total Loss: 1.0205494165420532 | KNN Loss: 6.224244594573975 | BCE Loss: 1.0205494165420532\n",
      "Epoch 117 / 500 | iteration 15 / 30 | Total Loss: 1.0265007019042969 | KNN Loss: 6.224869728088379 | BCE Loss: 1.0265007019042969\n",
      "Epoch 117 / 500 | iteration 20 / 30 | Total Loss: 1.0465185642242432 | KNN Loss: 6.224089622497559 | BCE Loss: 1.0465185642242432\n",
      "Epoch 117 / 500 | iteration 25 / 30 | Total Loss: 1.0598138570785522 | KNN Loss: 6.22451639175415 | BCE Loss: 1.0598138570785522\n",
      "Epoch 118 / 500 | iteration 0 / 30 | Total Loss: 1.026871681213379 | KNN Loss: 6.224137783050537 | BCE Loss: 1.026871681213379\n",
      "Epoch 118 / 500 | iteration 5 / 30 | Total Loss: 1.0496935844421387 | KNN Loss: 6.224656105041504 | BCE Loss: 1.0496935844421387\n",
      "Epoch 118 / 500 | iteration 10 / 30 | Total Loss: 1.0394704341888428 | KNN Loss: 6.224643230438232 | BCE Loss: 1.0394704341888428\n",
      "Epoch 118 / 500 | iteration 15 / 30 | Total Loss: 1.037872552871704 | KNN Loss: 6.224328994750977 | BCE Loss: 1.037872552871704\n",
      "Epoch 118 / 500 | iteration 20 / 30 | Total Loss: 1.0682435035705566 | KNN Loss: 6.224429130554199 | BCE Loss: 1.0682435035705566\n",
      "Epoch 118 / 500 | iteration 25 / 30 | Total Loss: 1.05902898311615 | KNN Loss: 6.224211692810059 | BCE Loss: 1.05902898311615\n",
      "Epoch 119 / 500 | iteration 0 / 30 | Total Loss: 1.0404726266860962 | KNN Loss: 6.22461462020874 | BCE Loss: 1.0404726266860962\n",
      "Epoch 119 / 500 | iteration 5 / 30 | Total Loss: 1.0285148620605469 | KNN Loss: 6.224799633026123 | BCE Loss: 1.0285148620605469\n",
      "Epoch 119 / 500 | iteration 10 / 30 | Total Loss: 1.0723488330841064 | KNN Loss: 6.224606037139893 | BCE Loss: 1.0723488330841064\n",
      "Epoch 119 / 500 | iteration 15 / 30 | Total Loss: 1.0557515621185303 | KNN Loss: 6.224096298217773 | BCE Loss: 1.0557515621185303\n",
      "Epoch 119 / 500 | iteration 20 / 30 | Total Loss: 1.061043620109558 | KNN Loss: 6.224221229553223 | BCE Loss: 1.061043620109558\n",
      "Epoch 119 / 500 | iteration 25 / 30 | Total Loss: 1.052191972732544 | KNN Loss: 6.224818229675293 | BCE Loss: 1.052191972732544\n",
      "Epoch 120 / 500 | iteration 0 / 30 | Total Loss: 1.036980152130127 | KNN Loss: 6.224358081817627 | BCE Loss: 1.036980152130127\n",
      "Epoch 120 / 500 | iteration 5 / 30 | Total Loss: 1.0306172370910645 | KNN Loss: 6.224671840667725 | BCE Loss: 1.0306172370910645\n",
      "Epoch 120 / 500 | iteration 10 / 30 | Total Loss: 1.0626208782196045 | KNN Loss: 6.224485397338867 | BCE Loss: 1.0626208782196045\n",
      "Epoch 120 / 500 | iteration 15 / 30 | Total Loss: 1.0559419393539429 | KNN Loss: 6.224671840667725 | BCE Loss: 1.0559419393539429\n",
      "Epoch 120 / 500 | iteration 20 / 30 | Total Loss: 1.069342017173767 | KNN Loss: 6.224676609039307 | BCE Loss: 1.069342017173767\n",
      "Epoch 120 / 500 | iteration 25 / 30 | Total Loss: 1.0259588956832886 | KNN Loss: 6.224714279174805 | BCE Loss: 1.0259588956832886\n",
      "Epoch 121 / 500 | iteration 0 / 30 | Total Loss: 1.079322099685669 | KNN Loss: 6.2249274253845215 | BCE Loss: 1.079322099685669\n",
      "Epoch 121 / 500 | iteration 5 / 30 | Total Loss: 1.0442173480987549 | KNN Loss: 6.224747657775879 | BCE Loss: 1.0442173480987549\n",
      "Epoch 121 / 500 | iteration 10 / 30 | Total Loss: 1.0441029071807861 | KNN Loss: 6.2245612144470215 | BCE Loss: 1.0441029071807861\n",
      "Epoch 121 / 500 | iteration 15 / 30 | Total Loss: 1.0487967729568481 | KNN Loss: 6.224877834320068 | BCE Loss: 1.0487967729568481\n",
      "Epoch 121 / 500 | iteration 20 / 30 | Total Loss: 1.0808849334716797 | KNN Loss: 6.2243876457214355 | BCE Loss: 1.0808849334716797\n",
      "Epoch 121 / 500 | iteration 25 / 30 | Total Loss: 1.0254263877868652 | KNN Loss: 6.2245402336120605 | BCE Loss: 1.0254263877868652\n",
      "Epoch 122 / 500 | iteration 0 / 30 | Total Loss: 1.0919651985168457 | KNN Loss: 6.2243123054504395 | BCE Loss: 1.0919651985168457\n",
      "Epoch 122 / 500 | iteration 5 / 30 | Total Loss: 1.0351155996322632 | KNN Loss: 6.2247419357299805 | BCE Loss: 1.0351155996322632\n",
      "Epoch 122 / 500 | iteration 10 / 30 | Total Loss: 1.025433897972107 | KNN Loss: 6.2243475914001465 | BCE Loss: 1.025433897972107\n",
      "Epoch 122 / 500 | iteration 15 / 30 | Total Loss: 1.0239511728286743 | KNN Loss: 6.2242865562438965 | BCE Loss: 1.0239511728286743\n",
      "Epoch 122 / 500 | iteration 20 / 30 | Total Loss: 1.0292426347732544 | KNN Loss: 6.224222183227539 | BCE Loss: 1.0292426347732544\n",
      "Epoch 122 / 500 | iteration 25 / 30 | Total Loss: 1.0480554103851318 | KNN Loss: 6.224703311920166 | BCE Loss: 1.0480554103851318\n",
      "Epoch   123: reducing learning rate of group 0 to 5.8824e-04.\n",
      "Epoch 123 / 500 | iteration 0 / 30 | Total Loss: 1.0677363872528076 | KNN Loss: 6.224463939666748 | BCE Loss: 1.0677363872528076\n",
      "Epoch 123 / 500 | iteration 5 / 30 | Total Loss: 1.0365004539489746 | KNN Loss: 6.224795818328857 | BCE Loss: 1.0365004539489746\n",
      "Epoch 123 / 500 | iteration 10 / 30 | Total Loss: 1.0520570278167725 | KNN Loss: 6.224762439727783 | BCE Loss: 1.0520570278167725\n",
      "Epoch 123 / 500 | iteration 15 / 30 | Total Loss: 1.052371859550476 | KNN Loss: 6.224291801452637 | BCE Loss: 1.052371859550476\n",
      "Epoch 123 / 500 | iteration 20 / 30 | Total Loss: 1.0571391582489014 | KNN Loss: 6.224392890930176 | BCE Loss: 1.0571391582489014\n",
      "Epoch 123 / 500 | iteration 25 / 30 | Total Loss: 1.0482724905014038 | KNN Loss: 6.224545955657959 | BCE Loss: 1.0482724905014038\n",
      "Epoch 124 / 500 | iteration 0 / 30 | Total Loss: 1.0716623067855835 | KNN Loss: 6.224511623382568 | BCE Loss: 1.0716623067855835\n",
      "Epoch 124 / 500 | iteration 5 / 30 | Total Loss: 1.0387324094772339 | KNN Loss: 6.224374294281006 | BCE Loss: 1.0387324094772339\n",
      "Epoch 124 / 500 | iteration 10 / 30 | Total Loss: 1.038155198097229 | KNN Loss: 6.224637508392334 | BCE Loss: 1.038155198097229\n",
      "Epoch 124 / 500 | iteration 15 / 30 | Total Loss: 1.078108310699463 | KNN Loss: 6.224531650543213 | BCE Loss: 1.078108310699463\n",
      "Epoch 124 / 500 | iteration 20 / 30 | Total Loss: 1.0602319240570068 | KNN Loss: 6.2248077392578125 | BCE Loss: 1.0602319240570068\n",
      "Epoch 124 / 500 | iteration 25 / 30 | Total Loss: 1.0596849918365479 | KNN Loss: 6.224348545074463 | BCE Loss: 1.0596849918365479\n",
      "Epoch 125 / 500 | iteration 0 / 30 | Total Loss: 1.05558443069458 | KNN Loss: 6.224663257598877 | BCE Loss: 1.05558443069458\n",
      "Epoch 125 / 500 | iteration 5 / 30 | Total Loss: 1.0355169773101807 | KNN Loss: 6.224545955657959 | BCE Loss: 1.0355169773101807\n",
      "Epoch 125 / 500 | iteration 10 / 30 | Total Loss: 1.0561184883117676 | KNN Loss: 6.224321365356445 | BCE Loss: 1.0561184883117676\n",
      "Epoch 125 / 500 | iteration 15 / 30 | Total Loss: 1.054053783416748 | KNN Loss: 6.224420070648193 | BCE Loss: 1.054053783416748\n",
      "Epoch 125 / 500 | iteration 20 / 30 | Total Loss: 1.0285778045654297 | KNN Loss: 6.224365711212158 | BCE Loss: 1.0285778045654297\n",
      "Epoch 125 / 500 | iteration 25 / 30 | Total Loss: 1.045800805091858 | KNN Loss: 6.224245071411133 | BCE Loss: 1.045800805091858\n",
      "Epoch 126 / 500 | iteration 0 / 30 | Total Loss: 1.074474573135376 | KNN Loss: 6.224406719207764 | BCE Loss: 1.074474573135376\n",
      "Epoch 126 / 500 | iteration 5 / 30 | Total Loss: 1.0546867847442627 | KNN Loss: 6.224298000335693 | BCE Loss: 1.0546867847442627\n",
      "Epoch 126 / 500 | iteration 10 / 30 | Total Loss: 1.0557045936584473 | KNN Loss: 6.224636554718018 | BCE Loss: 1.0557045936584473\n",
      "Epoch 126 / 500 | iteration 15 / 30 | Total Loss: 1.0584728717803955 | KNN Loss: 6.224977970123291 | BCE Loss: 1.0584728717803955\n",
      "Epoch 126 / 500 | iteration 20 / 30 | Total Loss: 1.0426355600357056 | KNN Loss: 6.224659442901611 | BCE Loss: 1.0426355600357056\n",
      "Epoch 126 / 500 | iteration 25 / 30 | Total Loss: 1.050077199935913 | KNN Loss: 6.2242279052734375 | BCE Loss: 1.050077199935913\n",
      "Epoch 127 / 500 | iteration 0 / 30 | Total Loss: 1.0378190279006958 | KNN Loss: 6.224676609039307 | BCE Loss: 1.0378190279006958\n",
      "Epoch 127 / 500 | iteration 5 / 30 | Total Loss: 1.0765388011932373 | KNN Loss: 6.224522113800049 | BCE Loss: 1.0765388011932373\n",
      "Epoch 127 / 500 | iteration 10 / 30 | Total Loss: 1.0296597480773926 | KNN Loss: 6.224656105041504 | BCE Loss: 1.0296597480773926\n",
      "Epoch 127 / 500 | iteration 15 / 30 | Total Loss: 1.0482745170593262 | KNN Loss: 6.224600791931152 | BCE Loss: 1.0482745170593262\n",
      "Epoch 127 / 500 | iteration 20 / 30 | Total Loss: 1.0493512153625488 | KNN Loss: 6.2245283126831055 | BCE Loss: 1.0493512153625488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 127 / 500 | iteration 25 / 30 | Total Loss: 1.0717620849609375 | KNN Loss: 6.224358081817627 | BCE Loss: 1.0717620849609375\n",
      "Epoch 128 / 500 | iteration 0 / 30 | Total Loss: 1.0686118602752686 | KNN Loss: 6.22437858581543 | BCE Loss: 1.0686118602752686\n",
      "Epoch 128 / 500 | iteration 5 / 30 | Total Loss: 1.0324101448059082 | KNN Loss: 6.2245192527771 | BCE Loss: 1.0324101448059082\n",
      "Epoch 128 / 500 | iteration 10 / 30 | Total Loss: 1.072211742401123 | KNN Loss: 6.2246294021606445 | BCE Loss: 1.072211742401123\n",
      "Epoch 128 / 500 | iteration 15 / 30 | Total Loss: 1.0662367343902588 | KNN Loss: 6.224266052246094 | BCE Loss: 1.0662367343902588\n",
      "Epoch 128 / 500 | iteration 20 / 30 | Total Loss: 1.0588347911834717 | KNN Loss: 6.224305629730225 | BCE Loss: 1.0588347911834717\n",
      "Epoch 128 / 500 | iteration 25 / 30 | Total Loss: 1.0081779956817627 | KNN Loss: 6.224628448486328 | BCE Loss: 1.0081779956817627\n",
      "Epoch 129 / 500 | iteration 0 / 30 | Total Loss: 1.0564218759536743 | KNN Loss: 6.224445819854736 | BCE Loss: 1.0564218759536743\n",
      "Epoch 129 / 500 | iteration 5 / 30 | Total Loss: 1.0599539279937744 | KNN Loss: 6.224366664886475 | BCE Loss: 1.0599539279937744\n",
      "Epoch 129 / 500 | iteration 10 / 30 | Total Loss: 1.0396952629089355 | KNN Loss: 6.224398612976074 | BCE Loss: 1.0396952629089355\n",
      "Epoch 129 / 500 | iteration 15 / 30 | Total Loss: 1.0411591529846191 | KNN Loss: 6.224330425262451 | BCE Loss: 1.0411591529846191\n",
      "Epoch 129 / 500 | iteration 20 / 30 | Total Loss: 1.025901436805725 | KNN Loss: 6.224420547485352 | BCE Loss: 1.025901436805725\n",
      "Epoch 129 / 500 | iteration 25 / 30 | Total Loss: 1.0467873811721802 | KNN Loss: 6.224382400512695 | BCE Loss: 1.0467873811721802\n",
      "Epoch 130 / 500 | iteration 0 / 30 | Total Loss: 1.0750086307525635 | KNN Loss: 6.22467041015625 | BCE Loss: 1.0750086307525635\n",
      "Epoch 130 / 500 | iteration 5 / 30 | Total Loss: 1.0531446933746338 | KNN Loss: 6.224681377410889 | BCE Loss: 1.0531446933746338\n",
      "Epoch 130 / 500 | iteration 10 / 30 | Total Loss: 1.0169079303741455 | KNN Loss: 6.224330425262451 | BCE Loss: 1.0169079303741455\n",
      "Epoch 130 / 500 | iteration 15 / 30 | Total Loss: 1.0506938695907593 | KNN Loss: 6.224722385406494 | BCE Loss: 1.0506938695907593\n",
      "Epoch 130 / 500 | iteration 20 / 30 | Total Loss: 1.0642058849334717 | KNN Loss: 6.224633693695068 | BCE Loss: 1.0642058849334717\n",
      "Epoch 130 / 500 | iteration 25 / 30 | Total Loss: 1.059497594833374 | KNN Loss: 6.224400997161865 | BCE Loss: 1.059497594833374\n",
      "Epoch 131 / 500 | iteration 0 / 30 | Total Loss: 1.0383400917053223 | KNN Loss: 6.22441291809082 | BCE Loss: 1.0383400917053223\n",
      "Epoch 131 / 500 | iteration 5 / 30 | Total Loss: 1.0513931512832642 | KNN Loss: 6.224644184112549 | BCE Loss: 1.0513931512832642\n",
      "Epoch 131 / 500 | iteration 10 / 30 | Total Loss: 1.0615558624267578 | KNN Loss: 6.224545478820801 | BCE Loss: 1.0615558624267578\n",
      "Epoch 131 / 500 | iteration 15 / 30 | Total Loss: 1.0388665199279785 | KNN Loss: 6.224403381347656 | BCE Loss: 1.0388665199279785\n",
      "Epoch 131 / 500 | iteration 20 / 30 | Total Loss: 1.057340383529663 | KNN Loss: 6.2243332862854 | BCE Loss: 1.057340383529663\n",
      "Epoch 131 / 500 | iteration 25 / 30 | Total Loss: 1.0338451862335205 | KNN Loss: 6.224394798278809 | BCE Loss: 1.0338451862335205\n",
      "Epoch 132 / 500 | iteration 0 / 30 | Total Loss: 1.0707519054412842 | KNN Loss: 6.224483013153076 | BCE Loss: 1.0707519054412842\n",
      "Epoch 132 / 500 | iteration 5 / 30 | Total Loss: 1.0566749572753906 | KNN Loss: 6.224889278411865 | BCE Loss: 1.0566749572753906\n",
      "Epoch 132 / 500 | iteration 10 / 30 | Total Loss: 1.0126197338104248 | KNN Loss: 6.224664211273193 | BCE Loss: 1.0126197338104248\n",
      "Epoch 132 / 500 | iteration 15 / 30 | Total Loss: 1.0342907905578613 | KNN Loss: 6.224478244781494 | BCE Loss: 1.0342907905578613\n",
      "Epoch 132 / 500 | iteration 20 / 30 | Total Loss: 1.022890567779541 | KNN Loss: 6.224674701690674 | BCE Loss: 1.022890567779541\n",
      "Epoch 132 / 500 | iteration 25 / 30 | Total Loss: 1.046068787574768 | KNN Loss: 6.2241926193237305 | BCE Loss: 1.046068787574768\n",
      "Epoch 133 / 500 | iteration 0 / 30 | Total Loss: 1.0629522800445557 | KNN Loss: 6.22414493560791 | BCE Loss: 1.0629522800445557\n",
      "Epoch 133 / 500 | iteration 5 / 30 | Total Loss: 1.053945779800415 | KNN Loss: 6.224934101104736 | BCE Loss: 1.053945779800415\n",
      "Epoch 133 / 500 | iteration 10 / 30 | Total Loss: 1.0324609279632568 | KNN Loss: 6.224456310272217 | BCE Loss: 1.0324609279632568\n",
      "Epoch 133 / 500 | iteration 15 / 30 | Total Loss: 1.0398532152175903 | KNN Loss: 6.224363327026367 | BCE Loss: 1.0398532152175903\n",
      "Epoch 133 / 500 | iteration 20 / 30 | Total Loss: 1.0479415655136108 | KNN Loss: 6.22468376159668 | BCE Loss: 1.0479415655136108\n",
      "Epoch 133 / 500 | iteration 25 / 30 | Total Loss: 1.0380146503448486 | KNN Loss: 6.2244462966918945 | BCE Loss: 1.0380146503448486\n",
      "Epoch   134: reducing learning rate of group 0 to 4.1177e-04.\n",
      "Epoch 134 / 500 | iteration 0 / 30 | Total Loss: 1.0524908304214478 | KNN Loss: 6.224240779876709 | BCE Loss: 1.0524908304214478\n",
      "Epoch 134 / 500 | iteration 5 / 30 | Total Loss: 1.032111644744873 | KNN Loss: 6.224569797515869 | BCE Loss: 1.032111644744873\n",
      "Epoch 134 / 500 | iteration 10 / 30 | Total Loss: 1.0502538681030273 | KNN Loss: 6.224377632141113 | BCE Loss: 1.0502538681030273\n",
      "Epoch 134 / 500 | iteration 15 / 30 | Total Loss: 1.0530719757080078 | KNN Loss: 6.224635124206543 | BCE Loss: 1.0530719757080078\n",
      "Epoch 134 / 500 | iteration 20 / 30 | Total Loss: 1.0480436086654663 | KNN Loss: 6.224616050720215 | BCE Loss: 1.0480436086654663\n",
      "Epoch 134 / 500 | iteration 25 / 30 | Total Loss: 1.0374163389205933 | KNN Loss: 6.224608421325684 | BCE Loss: 1.0374163389205933\n",
      "Epoch 135 / 500 | iteration 0 / 30 | Total Loss: 1.0463283061981201 | KNN Loss: 6.224372386932373 | BCE Loss: 1.0463283061981201\n",
      "Epoch 135 / 500 | iteration 5 / 30 | Total Loss: 1.0313403606414795 | KNN Loss: 6.2247748374938965 | BCE Loss: 1.0313403606414795\n",
      "Epoch 135 / 500 | iteration 10 / 30 | Total Loss: 1.0741676092147827 | KNN Loss: 6.224603176116943 | BCE Loss: 1.0741676092147827\n",
      "Epoch 135 / 500 | iteration 15 / 30 | Total Loss: 1.072641372680664 | KNN Loss: 6.224812984466553 | BCE Loss: 1.072641372680664\n",
      "Epoch 135 / 500 | iteration 20 / 30 | Total Loss: 1.069046974182129 | KNN Loss: 6.2245588302612305 | BCE Loss: 1.069046974182129\n",
      "Epoch 135 / 500 | iteration 25 / 30 | Total Loss: 1.0521769523620605 | KNN Loss: 6.223959922790527 | BCE Loss: 1.0521769523620605\n",
      "Epoch 136 / 500 | iteration 0 / 30 | Total Loss: 1.037973403930664 | KNN Loss: 6.224724769592285 | BCE Loss: 1.037973403930664\n",
      "Epoch 136 / 500 | iteration 5 / 30 | Total Loss: 1.0641038417816162 | KNN Loss: 6.2245564460754395 | BCE Loss: 1.0641038417816162\n",
      "Epoch 136 / 500 | iteration 10 / 30 | Total Loss: 1.04548978805542 | KNN Loss: 6.224846839904785 | BCE Loss: 1.04548978805542\n",
      "Epoch 136 / 500 | iteration 15 / 30 | Total Loss: 1.0246142148971558 | KNN Loss: 6.224331855773926 | BCE Loss: 1.0246142148971558\n",
      "Epoch 136 / 500 | iteration 20 / 30 | Total Loss: 1.0602655410766602 | KNN Loss: 6.224192142486572 | BCE Loss: 1.0602655410766602\n",
      "Epoch 136 / 500 | iteration 25 / 30 | Total Loss: 1.0461339950561523 | KNN Loss: 6.224490642547607 | BCE Loss: 1.0461339950561523\n",
      "Epoch 137 / 500 | iteration 0 / 30 | Total Loss: 1.0546014308929443 | KNN Loss: 6.2245635986328125 | BCE Loss: 1.0546014308929443\n",
      "Epoch 137 / 500 | iteration 5 / 30 | Total Loss: 1.0636303424835205 | KNN Loss: 6.224570274353027 | BCE Loss: 1.0636303424835205\n",
      "Epoch 137 / 500 | iteration 10 / 30 | Total Loss: 1.043309211730957 | KNN Loss: 6.225004196166992 | BCE Loss: 1.043309211730957\n",
      "Epoch 137 / 500 | iteration 15 / 30 | Total Loss: 1.0299733877182007 | KNN Loss: 6.224229335784912 | BCE Loss: 1.0299733877182007\n",
      "Epoch 137 / 500 | iteration 20 / 30 | Total Loss: 1.0517518520355225 | KNN Loss: 6.224796295166016 | BCE Loss: 1.0517518520355225\n",
      "Epoch 137 / 500 | iteration 25 / 30 | Total Loss: 1.041395664215088 | KNN Loss: 6.224521160125732 | BCE Loss: 1.041395664215088\n",
      "Epoch 138 / 500 | iteration 0 / 30 | Total Loss: 1.026979684829712 | KNN Loss: 6.224634647369385 | BCE Loss: 1.026979684829712\n",
      "Epoch 138 / 500 | iteration 5 / 30 | Total Loss: 1.0637407302856445 | KNN Loss: 6.224090576171875 | BCE Loss: 1.0637407302856445\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 138 / 500 | iteration 10 / 30 | Total Loss: 1.0434868335723877 | KNN Loss: 6.224539756774902 | BCE Loss: 1.0434868335723877\n",
      "Epoch 138 / 500 | iteration 15 / 30 | Total Loss: 1.0644848346710205 | KNN Loss: 6.224668502807617 | BCE Loss: 1.0644848346710205\n",
      "Epoch 138 / 500 | iteration 20 / 30 | Total Loss: 1.0518426895141602 | KNN Loss: 6.224427700042725 | BCE Loss: 1.0518426895141602\n",
      "Epoch 138 / 500 | iteration 25 / 30 | Total Loss: 1.0558515787124634 | KNN Loss: 6.224489212036133 | BCE Loss: 1.0558515787124634\n",
      "Epoch 139 / 500 | iteration 0 / 30 | Total Loss: 1.0500545501708984 | KNN Loss: 6.2243733406066895 | BCE Loss: 1.0500545501708984\n",
      "Epoch 139 / 500 | iteration 5 / 30 | Total Loss: 1.0232995748519897 | KNN Loss: 6.224390506744385 | BCE Loss: 1.0232995748519897\n",
      "Epoch 139 / 500 | iteration 10 / 30 | Total Loss: 1.0811059474945068 | KNN Loss: 6.224963665008545 | BCE Loss: 1.0811059474945068\n",
      "Epoch 139 / 500 | iteration 15 / 30 | Total Loss: 1.0447630882263184 | KNN Loss: 6.224292755126953 | BCE Loss: 1.0447630882263184\n",
      "Epoch 139 / 500 | iteration 20 / 30 | Total Loss: 1.0632362365722656 | KNN Loss: 6.224374771118164 | BCE Loss: 1.0632362365722656\n",
      "Epoch 139 / 500 | iteration 25 / 30 | Total Loss: 1.0763881206512451 | KNN Loss: 6.224595069885254 | BCE Loss: 1.0763881206512451\n",
      "Epoch 140 / 500 | iteration 0 / 30 | Total Loss: 1.0309613943099976 | KNN Loss: 6.224298477172852 | BCE Loss: 1.0309613943099976\n",
      "Epoch 140 / 500 | iteration 5 / 30 | Total Loss: 1.0637081861495972 | KNN Loss: 6.224438667297363 | BCE Loss: 1.0637081861495972\n",
      "Epoch 140 / 500 | iteration 10 / 30 | Total Loss: 1.0551501512527466 | KNN Loss: 6.224542140960693 | BCE Loss: 1.0551501512527466\n",
      "Epoch 140 / 500 | iteration 15 / 30 | Total Loss: 1.0678493976593018 | KNN Loss: 6.2241644859313965 | BCE Loss: 1.0678493976593018\n",
      "Epoch 140 / 500 | iteration 20 / 30 | Total Loss: 1.047452449798584 | KNN Loss: 6.224606513977051 | BCE Loss: 1.047452449798584\n",
      "Epoch 140 / 500 | iteration 25 / 30 | Total Loss: 1.0763845443725586 | KNN Loss: 6.224189281463623 | BCE Loss: 1.0763845443725586\n",
      "Epoch 141 / 500 | iteration 0 / 30 | Total Loss: 1.057593822479248 | KNN Loss: 6.224347114562988 | BCE Loss: 1.057593822479248\n",
      "Epoch 141 / 500 | iteration 5 / 30 | Total Loss: 1.0554803609848022 | KNN Loss: 6.224601745605469 | BCE Loss: 1.0554803609848022\n",
      "Epoch 141 / 500 | iteration 10 / 30 | Total Loss: 1.0394644737243652 | KNN Loss: 6.224403381347656 | BCE Loss: 1.0394644737243652\n",
      "Epoch 141 / 500 | iteration 15 / 30 | Total Loss: 1.045546054840088 | KNN Loss: 6.224339962005615 | BCE Loss: 1.045546054840088\n",
      "Epoch 141 / 500 | iteration 20 / 30 | Total Loss: 1.0174050331115723 | KNN Loss: 6.224575996398926 | BCE Loss: 1.0174050331115723\n",
      "Epoch 141 / 500 | iteration 25 / 30 | Total Loss: 1.0617364645004272 | KNN Loss: 6.2247209548950195 | BCE Loss: 1.0617364645004272\n",
      "Epoch 142 / 500 | iteration 0 / 30 | Total Loss: 1.0497231483459473 | KNN Loss: 6.224298000335693 | BCE Loss: 1.0497231483459473\n",
      "Epoch 142 / 500 | iteration 5 / 30 | Total Loss: 1.0520780086517334 | KNN Loss: 6.224738597869873 | BCE Loss: 1.0520780086517334\n",
      "Epoch 142 / 500 | iteration 10 / 30 | Total Loss: 1.0715758800506592 | KNN Loss: 6.2244415283203125 | BCE Loss: 1.0715758800506592\n",
      "Epoch 142 / 500 | iteration 15 / 30 | Total Loss: 1.0590931177139282 | KNN Loss: 6.2247467041015625 | BCE Loss: 1.0590931177139282\n",
      "Epoch 142 / 500 | iteration 20 / 30 | Total Loss: 1.0274097919464111 | KNN Loss: 6.224297523498535 | BCE Loss: 1.0274097919464111\n",
      "Epoch 142 / 500 | iteration 25 / 30 | Total Loss: 1.0556206703186035 | KNN Loss: 6.224231719970703 | BCE Loss: 1.0556206703186035\n",
      "Epoch 143 / 500 | iteration 0 / 30 | Total Loss: 1.0348542928695679 | KNN Loss: 6.2245283126831055 | BCE Loss: 1.0348542928695679\n",
      "Epoch 143 / 500 | iteration 5 / 30 | Total Loss: 1.0830705165863037 | KNN Loss: 6.22412633895874 | BCE Loss: 1.0830705165863037\n",
      "Epoch 143 / 500 | iteration 10 / 30 | Total Loss: 1.0568861961364746 | KNN Loss: 6.224684715270996 | BCE Loss: 1.0568861961364746\n",
      "Epoch 143 / 500 | iteration 15 / 30 | Total Loss: 1.0736322402954102 | KNN Loss: 6.22456169128418 | BCE Loss: 1.0736322402954102\n",
      "Epoch 143 / 500 | iteration 20 / 30 | Total Loss: 1.0370855331420898 | KNN Loss: 6.224543571472168 | BCE Loss: 1.0370855331420898\n",
      "Epoch 143 / 500 | iteration 25 / 30 | Total Loss: 1.0759248733520508 | KNN Loss: 6.224423408508301 | BCE Loss: 1.0759248733520508\n",
      "Epoch 144 / 500 | iteration 0 / 30 | Total Loss: 1.0646408796310425 | KNN Loss: 6.2247209548950195 | BCE Loss: 1.0646408796310425\n",
      "Epoch 144 / 500 | iteration 5 / 30 | Total Loss: 1.0630767345428467 | KNN Loss: 6.224430561065674 | BCE Loss: 1.0630767345428467\n",
      "Epoch 144 / 500 | iteration 10 / 30 | Total Loss: 1.049548625946045 | KNN Loss: 6.224493503570557 | BCE Loss: 1.049548625946045\n",
      "Epoch 144 / 500 | iteration 15 / 30 | Total Loss: 1.021313190460205 | KNN Loss: 6.224850177764893 | BCE Loss: 1.021313190460205\n",
      "Epoch 144 / 500 | iteration 20 / 30 | Total Loss: 1.0342681407928467 | KNN Loss: 6.224380970001221 | BCE Loss: 1.0342681407928467\n",
      "Epoch 144 / 500 | iteration 25 / 30 | Total Loss: 1.0937919616699219 | KNN Loss: 6.224470138549805 | BCE Loss: 1.0937919616699219\n",
      "Epoch   145: reducing learning rate of group 0 to 2.8824e-04.\n",
      "Epoch 145 / 500 | iteration 0 / 30 | Total Loss: 1.0814028978347778 | KNN Loss: 6.224559307098389 | BCE Loss: 1.0814028978347778\n",
      "Epoch 145 / 500 | iteration 5 / 30 | Total Loss: 1.0366811752319336 | KNN Loss: 6.224569797515869 | BCE Loss: 1.0366811752319336\n",
      "Epoch 145 / 500 | iteration 10 / 30 | Total Loss: 1.0776602029800415 | KNN Loss: 6.2245073318481445 | BCE Loss: 1.0776602029800415\n",
      "Epoch 145 / 500 | iteration 15 / 30 | Total Loss: 1.0209922790527344 | KNN Loss: 6.224384784698486 | BCE Loss: 1.0209922790527344\n",
      "Epoch 145 / 500 | iteration 20 / 30 | Total Loss: 1.0680913925170898 | KNN Loss: 6.224781036376953 | BCE Loss: 1.0680913925170898\n",
      "Epoch 145 / 500 | iteration 25 / 30 | Total Loss: 1.0352519750595093 | KNN Loss: 6.224472999572754 | BCE Loss: 1.0352519750595093\n",
      "Epoch 146 / 500 | iteration 0 / 30 | Total Loss: 1.0714075565338135 | KNN Loss: 6.22453498840332 | BCE Loss: 1.0714075565338135\n",
      "Epoch 146 / 500 | iteration 5 / 30 | Total Loss: 1.0394110679626465 | KNN Loss: 6.224186897277832 | BCE Loss: 1.0394110679626465\n",
      "Epoch 146 / 500 | iteration 10 / 30 | Total Loss: 1.021317958831787 | KNN Loss: 6.224472522735596 | BCE Loss: 1.021317958831787\n",
      "Epoch 146 / 500 | iteration 15 / 30 | Total Loss: 1.0742735862731934 | KNN Loss: 6.224365711212158 | BCE Loss: 1.0742735862731934\n",
      "Epoch 146 / 500 | iteration 20 / 30 | Total Loss: 1.014559030532837 | KNN Loss: 6.224401473999023 | BCE Loss: 1.014559030532837\n",
      "Epoch 146 / 500 | iteration 25 / 30 | Total Loss: 1.030918836593628 | KNN Loss: 6.2246551513671875 | BCE Loss: 1.030918836593628\n",
      "Epoch 147 / 500 | iteration 0 / 30 | Total Loss: 1.0568866729736328 | KNN Loss: 6.225022315979004 | BCE Loss: 1.0568866729736328\n",
      "Epoch 147 / 500 | iteration 5 / 30 | Total Loss: 1.0342776775360107 | KNN Loss: 6.224911689758301 | BCE Loss: 1.0342776775360107\n",
      "Epoch 147 / 500 | iteration 10 / 30 | Total Loss: 1.048985242843628 | KNN Loss: 6.224819183349609 | BCE Loss: 1.048985242843628\n",
      "Epoch 147 / 500 | iteration 15 / 30 | Total Loss: 1.0717707872390747 | KNN Loss: 6.2243266105651855 | BCE Loss: 1.0717707872390747\n",
      "Epoch 147 / 500 | iteration 20 / 30 | Total Loss: 1.0419679880142212 | KNN Loss: 6.224451541900635 | BCE Loss: 1.0419679880142212\n",
      "Epoch 147 / 500 | iteration 25 / 30 | Total Loss: 1.0715070962905884 | KNN Loss: 6.224452972412109 | BCE Loss: 1.0715070962905884\n",
      "Epoch 148 / 500 | iteration 0 / 30 | Total Loss: 1.0742138624191284 | KNN Loss: 6.224234104156494 | BCE Loss: 1.0742138624191284\n",
      "Epoch 148 / 500 | iteration 5 / 30 | Total Loss: 1.0580456256866455 | KNN Loss: 6.2244954109191895 | BCE Loss: 1.0580456256866455\n",
      "Epoch 148 / 500 | iteration 10 / 30 | Total Loss: 1.0447077751159668 | KNN Loss: 6.224215507507324 | BCE Loss: 1.0447077751159668\n",
      "Epoch 148 / 500 | iteration 15 / 30 | Total Loss: 1.0587273836135864 | KNN Loss: 6.224412441253662 | BCE Loss: 1.0587273836135864\n",
      "Epoch 148 / 500 | iteration 20 / 30 | Total Loss: 1.0348594188690186 | KNN Loss: 6.224514484405518 | BCE Loss: 1.0348594188690186\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 148 / 500 | iteration 25 / 30 | Total Loss: 1.05637788772583 | KNN Loss: 6.224045753479004 | BCE Loss: 1.05637788772583\n",
      "Epoch 149 / 500 | iteration 0 / 30 | Total Loss: 1.0540565252304077 | KNN Loss: 6.224485397338867 | BCE Loss: 1.0540565252304077\n",
      "Epoch 149 / 500 | iteration 5 / 30 | Total Loss: 1.0433368682861328 | KNN Loss: 6.224481105804443 | BCE Loss: 1.0433368682861328\n",
      "Epoch 149 / 500 | iteration 10 / 30 | Total Loss: 1.062540054321289 | KNN Loss: 6.224402904510498 | BCE Loss: 1.062540054321289\n",
      "Epoch 149 / 500 | iteration 15 / 30 | Total Loss: 1.0731459856033325 | KNN Loss: 6.224174976348877 | BCE Loss: 1.0731459856033325\n",
      "Epoch 149 / 500 | iteration 20 / 30 | Total Loss: 1.0279265642166138 | KNN Loss: 6.224489688873291 | BCE Loss: 1.0279265642166138\n",
      "Epoch 149 / 500 | iteration 25 / 30 | Total Loss: 1.0626847743988037 | KNN Loss: 6.224423885345459 | BCE Loss: 1.0626847743988037\n",
      "Epoch 150 / 500 | iteration 0 / 30 | Total Loss: 1.0488964319229126 | KNN Loss: 6.224236488342285 | BCE Loss: 1.0488964319229126\n",
      "Epoch 150 / 500 | iteration 5 / 30 | Total Loss: 1.0507196187973022 | KNN Loss: 6.224455833435059 | BCE Loss: 1.0507196187973022\n",
      "Epoch 150 / 500 | iteration 10 / 30 | Total Loss: 1.0592228174209595 | KNN Loss: 6.22450590133667 | BCE Loss: 1.0592228174209595\n",
      "Epoch 150 / 500 | iteration 15 / 30 | Total Loss: 1.064518928527832 | KNN Loss: 6.224672317504883 | BCE Loss: 1.064518928527832\n",
      "Epoch 150 / 500 | iteration 20 / 30 | Total Loss: 1.0793688297271729 | KNN Loss: 6.224404811859131 | BCE Loss: 1.0793688297271729\n",
      "Epoch 150 / 500 | iteration 25 / 30 | Total Loss: 1.0154647827148438 | KNN Loss: 6.224494934082031 | BCE Loss: 1.0154647827148438\n",
      "Epoch 151 / 500 | iteration 0 / 30 | Total Loss: 1.043778419494629 | KNN Loss: 6.22422456741333 | BCE Loss: 1.043778419494629\n",
      "Epoch 151 / 500 | iteration 5 / 30 | Total Loss: 1.0512564182281494 | KNN Loss: 6.2247843742370605 | BCE Loss: 1.0512564182281494\n",
      "Epoch 151 / 500 | iteration 10 / 30 | Total Loss: 1.0479581356048584 | KNN Loss: 6.224400520324707 | BCE Loss: 1.0479581356048584\n",
      "Epoch 151 / 500 | iteration 15 / 30 | Total Loss: 1.0285704135894775 | KNN Loss: 6.224003791809082 | BCE Loss: 1.0285704135894775\n",
      "Epoch 151 / 500 | iteration 20 / 30 | Total Loss: 1.0568678379058838 | KNN Loss: 6.224401950836182 | BCE Loss: 1.0568678379058838\n",
      "Epoch 151 / 500 | iteration 25 / 30 | Total Loss: 1.0445371866226196 | KNN Loss: 6.22448205947876 | BCE Loss: 1.0445371866226196\n",
      "Epoch 152 / 500 | iteration 0 / 30 | Total Loss: 1.0346248149871826 | KNN Loss: 6.224893093109131 | BCE Loss: 1.0346248149871826\n",
      "Epoch 152 / 500 | iteration 5 / 30 | Total Loss: 1.0748445987701416 | KNN Loss: 6.224525451660156 | BCE Loss: 1.0748445987701416\n",
      "Epoch 152 / 500 | iteration 10 / 30 | Total Loss: 1.017629861831665 | KNN Loss: 6.224617958068848 | BCE Loss: 1.017629861831665\n",
      "Epoch 152 / 500 | iteration 15 / 30 | Total Loss: 1.046170949935913 | KNN Loss: 6.224529266357422 | BCE Loss: 1.046170949935913\n",
      "Epoch 152 / 500 | iteration 20 / 30 | Total Loss: 1.017594337463379 | KNN Loss: 6.224674701690674 | BCE Loss: 1.017594337463379\n",
      "Epoch 152 / 500 | iteration 25 / 30 | Total Loss: 1.0280816555023193 | KNN Loss: 6.224508285522461 | BCE Loss: 1.0280816555023193\n",
      "Epoch 153 / 500 | iteration 0 / 30 | Total Loss: 1.03653883934021 | KNN Loss: 6.224392890930176 | BCE Loss: 1.03653883934021\n",
      "Epoch 153 / 500 | iteration 5 / 30 | Total Loss: 1.0552005767822266 | KNN Loss: 6.224581718444824 | BCE Loss: 1.0552005767822266\n",
      "Epoch 153 / 500 | iteration 10 / 30 | Total Loss: 1.0526316165924072 | KNN Loss: 6.224934101104736 | BCE Loss: 1.0526316165924072\n",
      "Epoch 153 / 500 | iteration 15 / 30 | Total Loss: 1.063185691833496 | KNN Loss: 6.224474906921387 | BCE Loss: 1.063185691833496\n",
      "Epoch 153 / 500 | iteration 20 / 30 | Total Loss: 1.0380465984344482 | KNN Loss: 6.224456310272217 | BCE Loss: 1.0380465984344482\n",
      "Epoch 153 / 500 | iteration 25 / 30 | Total Loss: 1.0613596439361572 | KNN Loss: 6.2245612144470215 | BCE Loss: 1.0613596439361572\n",
      "Epoch 154 / 500 | iteration 0 / 30 | Total Loss: 1.0697987079620361 | KNN Loss: 6.2244462966918945 | BCE Loss: 1.0697987079620361\n",
      "Epoch 154 / 500 | iteration 5 / 30 | Total Loss: 1.0657424926757812 | KNN Loss: 6.224297523498535 | BCE Loss: 1.0657424926757812\n",
      "Epoch 154 / 500 | iteration 10 / 30 | Total Loss: 1.0604556798934937 | KNN Loss: 6.224087715148926 | BCE Loss: 1.0604556798934937\n",
      "Epoch 154 / 500 | iteration 15 / 30 | Total Loss: 1.0792150497436523 | KNN Loss: 6.2246527671813965 | BCE Loss: 1.0792150497436523\n",
      "Epoch 154 / 500 | iteration 20 / 30 | Total Loss: 1.082392692565918 | KNN Loss: 6.224250793457031 | BCE Loss: 1.082392692565918\n",
      "Epoch 154 / 500 | iteration 25 / 30 | Total Loss: 1.0433765649795532 | KNN Loss: 6.224419116973877 | BCE Loss: 1.0433765649795532\n",
      "Epoch 155 / 500 | iteration 0 / 30 | Total Loss: 1.046053409576416 | KNN Loss: 6.224297523498535 | BCE Loss: 1.046053409576416\n",
      "Epoch 155 / 500 | iteration 5 / 30 | Total Loss: 1.0501117706298828 | KNN Loss: 6.224536895751953 | BCE Loss: 1.0501117706298828\n",
      "Epoch 155 / 500 | iteration 10 / 30 | Total Loss: 1.0833582878112793 | KNN Loss: 6.224344253540039 | BCE Loss: 1.0833582878112793\n",
      "Epoch 155 / 500 | iteration 15 / 30 | Total Loss: 1.050625205039978 | KNN Loss: 6.2241058349609375 | BCE Loss: 1.050625205039978\n",
      "Epoch 155 / 500 | iteration 20 / 30 | Total Loss: 1.0418235063552856 | KNN Loss: 6.224525451660156 | BCE Loss: 1.0418235063552856\n",
      "Epoch 155 / 500 | iteration 25 / 30 | Total Loss: 1.0358197689056396 | KNN Loss: 6.224666118621826 | BCE Loss: 1.0358197689056396\n",
      "Epoch   156: reducing learning rate of group 0 to 2.0177e-04.\n",
      "Epoch 156 / 500 | iteration 0 / 30 | Total Loss: 1.075095295906067 | KNN Loss: 6.224271774291992 | BCE Loss: 1.075095295906067\n",
      "Epoch 156 / 500 | iteration 5 / 30 | Total Loss: 1.0238436460494995 | KNN Loss: 6.2245988845825195 | BCE Loss: 1.0238436460494995\n",
      "Epoch 156 / 500 | iteration 10 / 30 | Total Loss: 1.0596890449523926 | KNN Loss: 6.224551677703857 | BCE Loss: 1.0596890449523926\n",
      "Epoch 156 / 500 | iteration 15 / 30 | Total Loss: 1.0485332012176514 | KNN Loss: 6.224676132202148 | BCE Loss: 1.0485332012176514\n",
      "Epoch 156 / 500 | iteration 20 / 30 | Total Loss: 1.0507794618606567 | KNN Loss: 6.22469425201416 | BCE Loss: 1.0507794618606567\n",
      "Epoch 156 / 500 | iteration 25 / 30 | Total Loss: 1.0702791213989258 | KNN Loss: 6.224390029907227 | BCE Loss: 1.0702791213989258\n",
      "Epoch 157 / 500 | iteration 0 / 30 | Total Loss: 1.0586909055709839 | KNN Loss: 6.224486827850342 | BCE Loss: 1.0586909055709839\n",
      "Epoch 157 / 500 | iteration 5 / 30 | Total Loss: 1.041015386581421 | KNN Loss: 6.224701881408691 | BCE Loss: 1.041015386581421\n",
      "Epoch 157 / 500 | iteration 10 / 30 | Total Loss: 1.0428547859191895 | KNN Loss: 6.224747657775879 | BCE Loss: 1.0428547859191895\n",
      "Epoch 157 / 500 | iteration 15 / 30 | Total Loss: 1.06847083568573 | KNN Loss: 6.224560737609863 | BCE Loss: 1.06847083568573\n",
      "Epoch 157 / 500 | iteration 20 / 30 | Total Loss: 1.0521354675292969 | KNN Loss: 6.224666595458984 | BCE Loss: 1.0521354675292969\n",
      "Epoch 157 / 500 | iteration 25 / 30 | Total Loss: 1.0472179651260376 | KNN Loss: 6.224491596221924 | BCE Loss: 1.0472179651260376\n",
      "Epoch 158 / 500 | iteration 0 / 30 | Total Loss: 1.069054126739502 | KNN Loss: 6.22462272644043 | BCE Loss: 1.069054126739502\n",
      "Epoch 158 / 500 | iteration 5 / 30 | Total Loss: 1.0255887508392334 | KNN Loss: 6.224204063415527 | BCE Loss: 1.0255887508392334\n",
      "Epoch 158 / 500 | iteration 10 / 30 | Total Loss: 1.0526008605957031 | KNN Loss: 6.2245049476623535 | BCE Loss: 1.0526008605957031\n",
      "Epoch 158 / 500 | iteration 15 / 30 | Total Loss: 1.053412675857544 | KNN Loss: 6.2244343757629395 | BCE Loss: 1.053412675857544\n",
      "Epoch 158 / 500 | iteration 20 / 30 | Total Loss: 1.0315618515014648 | KNN Loss: 6.224843978881836 | BCE Loss: 1.0315618515014648\n",
      "Epoch 158 / 500 | iteration 25 / 30 | Total Loss: 1.0577868223190308 | KNN Loss: 6.2244415283203125 | BCE Loss: 1.0577868223190308\n",
      "Epoch 159 / 500 | iteration 0 / 30 | Total Loss: 1.0668354034423828 | KNN Loss: 6.224508285522461 | BCE Loss: 1.0668354034423828\n",
      "Epoch 159 / 500 | iteration 5 / 30 | Total Loss: 1.030861496925354 | KNN Loss: 6.224705219268799 | BCE Loss: 1.030861496925354\n",
      "Epoch 159 / 500 | iteration 10 / 30 | Total Loss: 1.0648747682571411 | KNN Loss: 6.224851131439209 | BCE Loss: 1.0648747682571411\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 159 / 500 | iteration 15 / 30 | Total Loss: 1.0526831150054932 | KNN Loss: 6.224292278289795 | BCE Loss: 1.0526831150054932\n",
      "Epoch 159 / 500 | iteration 20 / 30 | Total Loss: 1.0371780395507812 | KNN Loss: 6.224307537078857 | BCE Loss: 1.0371780395507812\n",
      "Epoch 159 / 500 | iteration 25 / 30 | Total Loss: 1.035780668258667 | KNN Loss: 6.224663257598877 | BCE Loss: 1.035780668258667\n",
      "Epoch 160 / 500 | iteration 0 / 30 | Total Loss: 1.0621693134307861 | KNN Loss: 6.224734306335449 | BCE Loss: 1.0621693134307861\n",
      "Epoch 160 / 500 | iteration 5 / 30 | Total Loss: 1.0234891176223755 | KNN Loss: 6.224315166473389 | BCE Loss: 1.0234891176223755\n",
      "Epoch 160 / 500 | iteration 10 / 30 | Total Loss: 1.0591609477996826 | KNN Loss: 6.224399566650391 | BCE Loss: 1.0591609477996826\n",
      "Epoch 160 / 500 | iteration 15 / 30 | Total Loss: 1.0416561365127563 | KNN Loss: 6.22465705871582 | BCE Loss: 1.0416561365127563\n",
      "Epoch 160 / 500 | iteration 20 / 30 | Total Loss: 1.0567348003387451 | KNN Loss: 6.223995685577393 | BCE Loss: 1.0567348003387451\n",
      "Epoch 160 / 500 | iteration 25 / 30 | Total Loss: 1.053572177886963 | KNN Loss: 6.22429895401001 | BCE Loss: 1.053572177886963\n",
      "Epoch 161 / 500 | iteration 0 / 30 | Total Loss: 1.0360664129257202 | KNN Loss: 6.224774360656738 | BCE Loss: 1.0360664129257202\n",
      "Epoch 161 / 500 | iteration 5 / 30 | Total Loss: 1.0731443166732788 | KNN Loss: 6.224540710449219 | BCE Loss: 1.0731443166732788\n",
      "Epoch 161 / 500 | iteration 10 / 30 | Total Loss: 1.068504810333252 | KNN Loss: 6.224452018737793 | BCE Loss: 1.068504810333252\n",
      "Epoch 161 / 500 | iteration 15 / 30 | Total Loss: 1.0654575824737549 | KNN Loss: 6.225000381469727 | BCE Loss: 1.0654575824737549\n",
      "Epoch 161 / 500 | iteration 20 / 30 | Total Loss: 1.0378644466400146 | KNN Loss: 6.224453449249268 | BCE Loss: 1.0378644466400146\n",
      "Epoch 161 / 500 | iteration 25 / 30 | Total Loss: 1.0410008430480957 | KNN Loss: 6.224257946014404 | BCE Loss: 1.0410008430480957\n",
      "Epoch 162 / 500 | iteration 0 / 30 | Total Loss: 1.0349268913269043 | KNN Loss: 6.224274158477783 | BCE Loss: 1.0349268913269043\n",
      "Epoch 162 / 500 | iteration 5 / 30 | Total Loss: 1.0438010692596436 | KNN Loss: 6.224400043487549 | BCE Loss: 1.0438010692596436\n",
      "Epoch 162 / 500 | iteration 10 / 30 | Total Loss: 1.0650863647460938 | KNN Loss: 6.224537372589111 | BCE Loss: 1.0650863647460938\n",
      "Epoch 162 / 500 | iteration 15 / 30 | Total Loss: 1.0477538108825684 | KNN Loss: 6.224557876586914 | BCE Loss: 1.0477538108825684\n",
      "Epoch 162 / 500 | iteration 20 / 30 | Total Loss: 1.034080147743225 | KNN Loss: 6.224371433258057 | BCE Loss: 1.034080147743225\n",
      "Epoch 162 / 500 | iteration 25 / 30 | Total Loss: 1.051737904548645 | KNN Loss: 6.224457263946533 | BCE Loss: 1.051737904548645\n",
      "Epoch 163 / 500 | iteration 0 / 30 | Total Loss: 1.0429701805114746 | KNN Loss: 6.224606513977051 | BCE Loss: 1.0429701805114746\n",
      "Epoch 163 / 500 | iteration 5 / 30 | Total Loss: 1.0515766143798828 | KNN Loss: 6.224717617034912 | BCE Loss: 1.0515766143798828\n",
      "Epoch 163 / 500 | iteration 10 / 30 | Total Loss: 1.017425298690796 | KNN Loss: 6.2246994972229 | BCE Loss: 1.017425298690796\n",
      "Epoch 163 / 500 | iteration 15 / 30 | Total Loss: 1.0191298723220825 | KNN Loss: 6.224623680114746 | BCE Loss: 1.0191298723220825\n",
      "Epoch 163 / 500 | iteration 20 / 30 | Total Loss: 1.0420101881027222 | KNN Loss: 6.224671363830566 | BCE Loss: 1.0420101881027222\n",
      "Epoch 163 / 500 | iteration 25 / 30 | Total Loss: 1.07668137550354 | KNN Loss: 6.224394798278809 | BCE Loss: 1.07668137550354\n",
      "Epoch 164 / 500 | iteration 0 / 30 | Total Loss: 1.053608775138855 | KNN Loss: 6.224379062652588 | BCE Loss: 1.053608775138855\n",
      "Epoch 164 / 500 | iteration 5 / 30 | Total Loss: 1.0512248277664185 | KNN Loss: 6.22411584854126 | BCE Loss: 1.0512248277664185\n",
      "Epoch 164 / 500 | iteration 10 / 30 | Total Loss: 1.0669806003570557 | KNN Loss: 6.224566459655762 | BCE Loss: 1.0669806003570557\n",
      "Epoch 164 / 500 | iteration 15 / 30 | Total Loss: 1.0524911880493164 | KNN Loss: 6.2241129875183105 | BCE Loss: 1.0524911880493164\n",
      "Epoch 164 / 500 | iteration 20 / 30 | Total Loss: 1.0510032176971436 | KNN Loss: 6.224974632263184 | BCE Loss: 1.0510032176971436\n",
      "Epoch 164 / 500 | iteration 25 / 30 | Total Loss: 1.0141339302062988 | KNN Loss: 6.224774360656738 | BCE Loss: 1.0141339302062988\n",
      "Epoch 165 / 500 | iteration 0 / 30 | Total Loss: 1.013296127319336 | KNN Loss: 6.224213123321533 | BCE Loss: 1.013296127319336\n",
      "Epoch 165 / 500 | iteration 5 / 30 | Total Loss: 1.0682010650634766 | KNN Loss: 6.224802494049072 | BCE Loss: 1.0682010650634766\n",
      "Epoch 165 / 500 | iteration 10 / 30 | Total Loss: 1.0521379709243774 | KNN Loss: 6.224122047424316 | BCE Loss: 1.0521379709243774\n",
      "Epoch 165 / 500 | iteration 15 / 30 | Total Loss: 1.0625243186950684 | KNN Loss: 6.224433422088623 | BCE Loss: 1.0625243186950684\n",
      "Epoch 165 / 500 | iteration 20 / 30 | Total Loss: 1.0642404556274414 | KNN Loss: 6.224703788757324 | BCE Loss: 1.0642404556274414\n",
      "Epoch 165 / 500 | iteration 25 / 30 | Total Loss: 1.0486490726470947 | KNN Loss: 6.224623203277588 | BCE Loss: 1.0486490726470947\n",
      "Epoch 166 / 500 | iteration 0 / 30 | Total Loss: 1.0501389503479004 | KNN Loss: 6.224491119384766 | BCE Loss: 1.0501389503479004\n",
      "Epoch 166 / 500 | iteration 5 / 30 | Total Loss: 1.0556139945983887 | KNN Loss: 6.224699020385742 | BCE Loss: 1.0556139945983887\n",
      "Epoch 166 / 500 | iteration 10 / 30 | Total Loss: 1.0671155452728271 | KNN Loss: 6.224308967590332 | BCE Loss: 1.0671155452728271\n",
      "Epoch 166 / 500 | iteration 15 / 30 | Total Loss: 1.0436196327209473 | KNN Loss: 6.224681377410889 | BCE Loss: 1.0436196327209473\n",
      "Epoch 166 / 500 | iteration 20 / 30 | Total Loss: 1.0272982120513916 | KNN Loss: 6.224457740783691 | BCE Loss: 1.0272982120513916\n",
      "Epoch 166 / 500 | iteration 25 / 30 | Total Loss: 1.055091142654419 | KNN Loss: 6.224715709686279 | BCE Loss: 1.055091142654419\n",
      "Epoch   167: reducing learning rate of group 0 to 1.4124e-04.\n",
      "Epoch 167 / 500 | iteration 0 / 30 | Total Loss: 1.0476577281951904 | KNN Loss: 6.224314212799072 | BCE Loss: 1.0476577281951904\n",
      "Epoch 167 / 500 | iteration 5 / 30 | Total Loss: 1.0300358533859253 | KNN Loss: 6.224314212799072 | BCE Loss: 1.0300358533859253\n",
      "Epoch 167 / 500 | iteration 10 / 30 | Total Loss: 1.0695805549621582 | KNN Loss: 6.224457740783691 | BCE Loss: 1.0695805549621582\n",
      "Epoch 167 / 500 | iteration 15 / 30 | Total Loss: 1.0502846240997314 | KNN Loss: 6.224708080291748 | BCE Loss: 1.0502846240997314\n",
      "Epoch 167 / 500 | iteration 20 / 30 | Total Loss: 1.0570224523544312 | KNN Loss: 6.224395751953125 | BCE Loss: 1.0570224523544312\n",
      "Epoch 167 / 500 | iteration 25 / 30 | Total Loss: 1.0480372905731201 | KNN Loss: 6.224522113800049 | BCE Loss: 1.0480372905731201\n",
      "Epoch 168 / 500 | iteration 0 / 30 | Total Loss: 1.0702658891677856 | KNN Loss: 6.224591255187988 | BCE Loss: 1.0702658891677856\n",
      "Epoch 168 / 500 | iteration 5 / 30 | Total Loss: 1.0256489515304565 | KNN Loss: 6.224404811859131 | BCE Loss: 1.0256489515304565\n",
      "Epoch 168 / 500 | iteration 10 / 30 | Total Loss: 1.0592494010925293 | KNN Loss: 6.2244181632995605 | BCE Loss: 1.0592494010925293\n",
      "Epoch 168 / 500 | iteration 15 / 30 | Total Loss: 1.0850530862808228 | KNN Loss: 6.224743843078613 | BCE Loss: 1.0850530862808228\n",
      "Epoch 168 / 500 | iteration 20 / 30 | Total Loss: 1.0610253810882568 | KNN Loss: 6.224625110626221 | BCE Loss: 1.0610253810882568\n",
      "Epoch 168 / 500 | iteration 25 / 30 | Total Loss: 1.0734584331512451 | KNN Loss: 6.224570274353027 | BCE Loss: 1.0734584331512451\n",
      "Epoch 169 / 500 | iteration 0 / 30 | Total Loss: 1.0626730918884277 | KNN Loss: 6.22476863861084 | BCE Loss: 1.0626730918884277\n",
      "Epoch 169 / 500 | iteration 5 / 30 | Total Loss: 1.0264670848846436 | KNN Loss: 6.224267482757568 | BCE Loss: 1.0264670848846436\n",
      "Epoch 169 / 500 | iteration 10 / 30 | Total Loss: 1.052983283996582 | KNN Loss: 6.224213600158691 | BCE Loss: 1.052983283996582\n",
      "Epoch 169 / 500 | iteration 15 / 30 | Total Loss: 1.05597984790802 | KNN Loss: 6.224465847015381 | BCE Loss: 1.05597984790802\n",
      "Epoch 169 / 500 | iteration 20 / 30 | Total Loss: 1.0539789199829102 | KNN Loss: 6.2243781089782715 | BCE Loss: 1.0539789199829102\n",
      "Epoch 169 / 500 | iteration 25 / 30 | Total Loss: 1.0638413429260254 | KNN Loss: 6.224472522735596 | BCE Loss: 1.0638413429260254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 170 / 500 | iteration 0 / 30 | Total Loss: 1.0331776142120361 | KNN Loss: 6.2244648933410645 | BCE Loss: 1.0331776142120361\n",
      "Epoch 170 / 500 | iteration 5 / 30 | Total Loss: 1.0659198760986328 | KNN Loss: 6.224669456481934 | BCE Loss: 1.0659198760986328\n",
      "Epoch 170 / 500 | iteration 10 / 30 | Total Loss: 1.040419340133667 | KNN Loss: 6.224400043487549 | BCE Loss: 1.040419340133667\n",
      "Epoch 170 / 500 | iteration 15 / 30 | Total Loss: 1.0286602973937988 | KNN Loss: 6.2245707511901855 | BCE Loss: 1.0286602973937988\n",
      "Epoch 170 / 500 | iteration 20 / 30 | Total Loss: 1.0602948665618896 | KNN Loss: 6.224560260772705 | BCE Loss: 1.0602948665618896\n",
      "Epoch 170 / 500 | iteration 25 / 30 | Total Loss: 1.049367070198059 | KNN Loss: 6.2246246337890625 | BCE Loss: 1.049367070198059\n",
      "Epoch 171 / 500 | iteration 0 / 30 | Total Loss: 1.0278263092041016 | KNN Loss: 6.224634647369385 | BCE Loss: 1.0278263092041016\n",
      "Epoch 171 / 500 | iteration 5 / 30 | Total Loss: 1.044264793395996 | KNN Loss: 6.224355220794678 | BCE Loss: 1.044264793395996\n",
      "Epoch 171 / 500 | iteration 10 / 30 | Total Loss: 1.062827706336975 | KNN Loss: 6.224668502807617 | BCE Loss: 1.062827706336975\n",
      "Epoch 171 / 500 | iteration 15 / 30 | Total Loss: 1.069401741027832 | KNN Loss: 6.224679946899414 | BCE Loss: 1.069401741027832\n",
      "Epoch 171 / 500 | iteration 20 / 30 | Total Loss: 1.0430843830108643 | KNN Loss: 6.224249839782715 | BCE Loss: 1.0430843830108643\n",
      "Epoch 171 / 500 | iteration 25 / 30 | Total Loss: 1.0721626281738281 | KNN Loss: 6.22451639175415 | BCE Loss: 1.0721626281738281\n",
      "Epoch 172 / 500 | iteration 0 / 30 | Total Loss: 1.0790455341339111 | KNN Loss: 6.224851131439209 | BCE Loss: 1.0790455341339111\n",
      "Epoch 172 / 500 | iteration 5 / 30 | Total Loss: 1.0724740028381348 | KNN Loss: 6.224769592285156 | BCE Loss: 1.0724740028381348\n",
      "Epoch 172 / 500 | iteration 10 / 30 | Total Loss: 1.030643105506897 | KNN Loss: 6.22458028793335 | BCE Loss: 1.030643105506897\n",
      "Epoch 172 / 500 | iteration 15 / 30 | Total Loss: 1.0323985815048218 | KNN Loss: 6.224294185638428 | BCE Loss: 1.0323985815048218\n",
      "Epoch 172 / 500 | iteration 20 / 30 | Total Loss: 1.0263803005218506 | KNN Loss: 6.22476053237915 | BCE Loss: 1.0263803005218506\n",
      "Epoch 172 / 500 | iteration 25 / 30 | Total Loss: 1.0245461463928223 | KNN Loss: 6.224631309509277 | BCE Loss: 1.0245461463928223\n",
      "Epoch 173 / 500 | iteration 0 / 30 | Total Loss: 1.0847885608673096 | KNN Loss: 6.224135398864746 | BCE Loss: 1.0847885608673096\n",
      "Epoch 173 / 500 | iteration 5 / 30 | Total Loss: 1.0495860576629639 | KNN Loss: 6.224484920501709 | BCE Loss: 1.0495860576629639\n",
      "Epoch 173 / 500 | iteration 10 / 30 | Total Loss: 1.036555528640747 | KNN Loss: 6.224447727203369 | BCE Loss: 1.036555528640747\n",
      "Epoch 173 / 500 | iteration 15 / 30 | Total Loss: 1.0600574016571045 | KNN Loss: 6.2244367599487305 | BCE Loss: 1.0600574016571045\n",
      "Epoch 173 / 500 | iteration 20 / 30 | Total Loss: 1.0474250316619873 | KNN Loss: 6.224643230438232 | BCE Loss: 1.0474250316619873\n",
      "Epoch 173 / 500 | iteration 25 / 30 | Total Loss: 1.0565820932388306 | KNN Loss: 6.224067687988281 | BCE Loss: 1.0565820932388306\n",
      "Epoch 174 / 500 | iteration 0 / 30 | Total Loss: 1.058915376663208 | KNN Loss: 6.2244181632995605 | BCE Loss: 1.058915376663208\n",
      "Epoch 174 / 500 | iteration 5 / 30 | Total Loss: 1.0623812675476074 | KNN Loss: 6.224123001098633 | BCE Loss: 1.0623812675476074\n",
      "Epoch 174 / 500 | iteration 10 / 30 | Total Loss: 1.0466899871826172 | KNN Loss: 6.22484016418457 | BCE Loss: 1.0466899871826172\n",
      "Epoch 174 / 500 | iteration 15 / 30 | Total Loss: 1.0361645221710205 | KNN Loss: 6.224404335021973 | BCE Loss: 1.0361645221710205\n",
      "Epoch 174 / 500 | iteration 20 / 30 | Total Loss: 1.0533039569854736 | KNN Loss: 6.22496223449707 | BCE Loss: 1.0533039569854736\n",
      "Epoch 174 / 500 | iteration 25 / 30 | Total Loss: 1.028136968612671 | KNN Loss: 6.2246527671813965 | BCE Loss: 1.028136968612671\n",
      "Epoch 175 / 500 | iteration 0 / 30 | Total Loss: 1.0374021530151367 | KNN Loss: 6.224358558654785 | BCE Loss: 1.0374021530151367\n",
      "Epoch 175 / 500 | iteration 5 / 30 | Total Loss: 1.0436652898788452 | KNN Loss: 6.224441051483154 | BCE Loss: 1.0436652898788452\n",
      "Epoch 175 / 500 | iteration 10 / 30 | Total Loss: 1.032839298248291 | KNN Loss: 6.224795341491699 | BCE Loss: 1.032839298248291\n",
      "Epoch 175 / 500 | iteration 15 / 30 | Total Loss: 1.0684138536453247 | KNN Loss: 6.22467041015625 | BCE Loss: 1.0684138536453247\n",
      "Epoch 175 / 500 | iteration 20 / 30 | Total Loss: 1.0743848085403442 | KNN Loss: 6.224735736846924 | BCE Loss: 1.0743848085403442\n",
      "Epoch 175 / 500 | iteration 25 / 30 | Total Loss: 1.04142165184021 | KNN Loss: 6.2244873046875 | BCE Loss: 1.04142165184021\n",
      "Epoch 176 / 500 | iteration 0 / 30 | Total Loss: 1.039089560508728 | KNN Loss: 6.2242655754089355 | BCE Loss: 1.039089560508728\n",
      "Epoch 176 / 500 | iteration 5 / 30 | Total Loss: 1.021371841430664 | KNN Loss: 6.224750518798828 | BCE Loss: 1.021371841430664\n",
      "Epoch 176 / 500 | iteration 10 / 30 | Total Loss: 1.047873616218567 | KNN Loss: 6.224664211273193 | BCE Loss: 1.047873616218567\n",
      "Epoch 176 / 500 | iteration 15 / 30 | Total Loss: 1.041582465171814 | KNN Loss: 6.224859714508057 | BCE Loss: 1.041582465171814\n",
      "Epoch 176 / 500 | iteration 20 / 30 | Total Loss: 1.0500396490097046 | KNN Loss: 6.224558353424072 | BCE Loss: 1.0500396490097046\n",
      "Epoch 176 / 500 | iteration 25 / 30 | Total Loss: 1.0356624126434326 | KNN Loss: 6.224170684814453 | BCE Loss: 1.0356624126434326\n",
      "Epoch 177 / 500 | iteration 0 / 30 | Total Loss: 1.052649736404419 | KNN Loss: 6.22374153137207 | BCE Loss: 1.052649736404419\n",
      "Epoch 177 / 500 | iteration 5 / 30 | Total Loss: 1.076432228088379 | KNN Loss: 6.224277496337891 | BCE Loss: 1.076432228088379\n",
      "Epoch 177 / 500 | iteration 10 / 30 | Total Loss: 1.0738017559051514 | KNN Loss: 6.22428560256958 | BCE Loss: 1.0738017559051514\n",
      "Epoch 177 / 500 | iteration 15 / 30 | Total Loss: 1.0388438701629639 | KNN Loss: 6.224634170532227 | BCE Loss: 1.0388438701629639\n",
      "Epoch 177 / 500 | iteration 20 / 30 | Total Loss: 1.0530649423599243 | KNN Loss: 6.223927974700928 | BCE Loss: 1.0530649423599243\n",
      "Epoch 177 / 500 | iteration 25 / 30 | Total Loss: 1.0270919799804688 | KNN Loss: 6.224729537963867 | BCE Loss: 1.0270919799804688\n",
      "Epoch   178: reducing learning rate of group 0 to 9.8866e-05.\n",
      "Epoch 178 / 500 | iteration 0 / 30 | Total Loss: 1.0513989925384521 | KNN Loss: 6.2242512702941895 | BCE Loss: 1.0513989925384521\n",
      "Epoch 178 / 500 | iteration 5 / 30 | Total Loss: 1.0495777130126953 | KNN Loss: 6.224344730377197 | BCE Loss: 1.0495777130126953\n",
      "Epoch 178 / 500 | iteration 10 / 30 | Total Loss: 1.0450152158737183 | KNN Loss: 6.224635601043701 | BCE Loss: 1.0450152158737183\n",
      "Epoch 178 / 500 | iteration 15 / 30 | Total Loss: 1.0487066507339478 | KNN Loss: 6.224462985992432 | BCE Loss: 1.0487066507339478\n",
      "Epoch 178 / 500 | iteration 20 / 30 | Total Loss: 1.0359952449798584 | KNN Loss: 6.2245988845825195 | BCE Loss: 1.0359952449798584\n",
      "Epoch 178 / 500 | iteration 25 / 30 | Total Loss: 1.0656816959381104 | KNN Loss: 6.224195957183838 | BCE Loss: 1.0656816959381104\n",
      "Epoch 179 / 500 | iteration 0 / 30 | Total Loss: 1.0429857969284058 | KNN Loss: 6.2246575355529785 | BCE Loss: 1.0429857969284058\n",
      "Epoch 179 / 500 | iteration 5 / 30 | Total Loss: 1.0456573963165283 | KNN Loss: 6.2245965003967285 | BCE Loss: 1.0456573963165283\n",
      "Epoch 179 / 500 | iteration 10 / 30 | Total Loss: 1.05099356174469 | KNN Loss: 6.224502086639404 | BCE Loss: 1.05099356174469\n",
      "Epoch 179 / 500 | iteration 15 / 30 | Total Loss: 1.0468192100524902 | KNN Loss: 6.224402904510498 | BCE Loss: 1.0468192100524902\n",
      "Epoch 179 / 500 | iteration 20 / 30 | Total Loss: 1.060714840888977 | KNN Loss: 6.224235534667969 | BCE Loss: 1.060714840888977\n",
      "Epoch 179 / 500 | iteration 25 / 30 | Total Loss: 1.0478644371032715 | KNN Loss: 6.224046230316162 | BCE Loss: 1.0478644371032715\n",
      "Epoch 180 / 500 | iteration 0 / 30 | Total Loss: 1.063619613647461 | KNN Loss: 6.2247843742370605 | BCE Loss: 1.063619613647461\n",
      "Epoch 180 / 500 | iteration 5 / 30 | Total Loss: 1.0532557964324951 | KNN Loss: 6.224266529083252 | BCE Loss: 1.0532557964324951\n",
      "Epoch 180 / 500 | iteration 10 / 30 | Total Loss: 1.0686712265014648 | KNN Loss: 6.2240986824035645 | BCE Loss: 1.0686712265014648\n",
      "Epoch 180 / 500 | iteration 15 / 30 | Total Loss: 1.0813640356063843 | KNN Loss: 6.2245683670043945 | BCE Loss: 1.0813640356063843\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 180 / 500 | iteration 20 / 30 | Total Loss: 1.0396220684051514 | KNN Loss: 6.224434852600098 | BCE Loss: 1.0396220684051514\n",
      "Epoch 180 / 500 | iteration 25 / 30 | Total Loss: 1.0584218502044678 | KNN Loss: 6.224247932434082 | BCE Loss: 1.0584218502044678\n",
      "Epoch 181 / 500 | iteration 0 / 30 | Total Loss: 1.037247896194458 | KNN Loss: 6.224360466003418 | BCE Loss: 1.037247896194458\n",
      "Epoch 181 / 500 | iteration 5 / 30 | Total Loss: 1.043837070465088 | KNN Loss: 6.2248125076293945 | BCE Loss: 1.043837070465088\n",
      "Epoch 181 / 500 | iteration 10 / 30 | Total Loss: 1.0840134620666504 | KNN Loss: 6.224400043487549 | BCE Loss: 1.0840134620666504\n",
      "Epoch 181 / 500 | iteration 15 / 30 | Total Loss: 1.030479073524475 | KNN Loss: 6.224012851715088 | BCE Loss: 1.030479073524475\n",
      "Epoch 181 / 500 | iteration 20 / 30 | Total Loss: 1.0713104009628296 | KNN Loss: 6.224464416503906 | BCE Loss: 1.0713104009628296\n",
      "Epoch 181 / 500 | iteration 25 / 30 | Total Loss: 1.0437517166137695 | KNN Loss: 6.224507808685303 | BCE Loss: 1.0437517166137695\n",
      "Epoch 182 / 500 | iteration 0 / 30 | Total Loss: 1.0401718616485596 | KNN Loss: 6.224471569061279 | BCE Loss: 1.0401718616485596\n",
      "Epoch 182 / 500 | iteration 5 / 30 | Total Loss: 1.050267219543457 | KNN Loss: 6.224508285522461 | BCE Loss: 1.050267219543457\n",
      "Epoch 182 / 500 | iteration 10 / 30 | Total Loss: 1.0550687313079834 | KNN Loss: 6.2244873046875 | BCE Loss: 1.0550687313079834\n",
      "Epoch 182 / 500 | iteration 15 / 30 | Total Loss: 1.0545039176940918 | KNN Loss: 6.224496364593506 | BCE Loss: 1.0545039176940918\n",
      "Epoch 182 / 500 | iteration 20 / 30 | Total Loss: 1.0530188083648682 | KNN Loss: 6.224384307861328 | BCE Loss: 1.0530188083648682\n",
      "Epoch 182 / 500 | iteration 25 / 30 | Total Loss: 1.0338075160980225 | KNN Loss: 6.224319934844971 | BCE Loss: 1.0338075160980225\n",
      "Epoch 183 / 500 | iteration 0 / 30 | Total Loss: 1.0451412200927734 | KNN Loss: 6.224256992340088 | BCE Loss: 1.0451412200927734\n",
      "Epoch 183 / 500 | iteration 5 / 30 | Total Loss: 1.0516687631607056 | KNN Loss: 6.224174976348877 | BCE Loss: 1.0516687631607056\n",
      "Epoch 183 / 500 | iteration 10 / 30 | Total Loss: 1.0300806760787964 | KNN Loss: 6.224283695220947 | BCE Loss: 1.0300806760787964\n",
      "Epoch 183 / 500 | iteration 15 / 30 | Total Loss: 1.0510004758834839 | KNN Loss: 6.224485397338867 | BCE Loss: 1.0510004758834839\n",
      "Epoch 183 / 500 | iteration 20 / 30 | Total Loss: 1.038315773010254 | KNN Loss: 6.224850177764893 | BCE Loss: 1.038315773010254\n",
      "Epoch 183 / 500 | iteration 25 / 30 | Total Loss: 1.0521836280822754 | KNN Loss: 6.224354267120361 | BCE Loss: 1.0521836280822754\n",
      "Epoch 184 / 500 | iteration 0 / 30 | Total Loss: 1.0484309196472168 | KNN Loss: 6.223991870880127 | BCE Loss: 1.0484309196472168\n",
      "Epoch 184 / 500 | iteration 5 / 30 | Total Loss: 1.059680700302124 | KNN Loss: 6.224443435668945 | BCE Loss: 1.059680700302124\n",
      "Epoch 184 / 500 | iteration 10 / 30 | Total Loss: 1.05792236328125 | KNN Loss: 6.224506378173828 | BCE Loss: 1.05792236328125\n",
      "Epoch 184 / 500 | iteration 15 / 30 | Total Loss: 1.062883973121643 | KNN Loss: 6.224155902862549 | BCE Loss: 1.062883973121643\n",
      "Epoch 184 / 500 | iteration 20 / 30 | Total Loss: 1.0462281703948975 | KNN Loss: 6.224377155303955 | BCE Loss: 1.0462281703948975\n",
      "Epoch 184 / 500 | iteration 25 / 30 | Total Loss: 1.0437145233154297 | KNN Loss: 6.224715232849121 | BCE Loss: 1.0437145233154297\n",
      "Epoch 185 / 500 | iteration 0 / 30 | Total Loss: 1.0230488777160645 | KNN Loss: 6.224364280700684 | BCE Loss: 1.0230488777160645\n",
      "Epoch 185 / 500 | iteration 5 / 30 | Total Loss: 1.0595035552978516 | KNN Loss: 6.2242279052734375 | BCE Loss: 1.0595035552978516\n",
      "Epoch 185 / 500 | iteration 10 / 30 | Total Loss: 1.0487096309661865 | KNN Loss: 6.224034786224365 | BCE Loss: 1.0487096309661865\n",
      "Epoch 185 / 500 | iteration 15 / 30 | Total Loss: 1.0535995960235596 | KNN Loss: 6.224234580993652 | BCE Loss: 1.0535995960235596\n",
      "Epoch 185 / 500 | iteration 20 / 30 | Total Loss: 1.0423583984375 | KNN Loss: 6.2243332862854 | BCE Loss: 1.0423583984375\n",
      "Epoch 185 / 500 | iteration 25 / 30 | Total Loss: 1.0723685026168823 | KNN Loss: 6.224554538726807 | BCE Loss: 1.0723685026168823\n",
      "Epoch 186 / 500 | iteration 0 / 30 | Total Loss: 1.0668082237243652 | KNN Loss: 6.224246025085449 | BCE Loss: 1.0668082237243652\n",
      "Epoch 186 / 500 | iteration 5 / 30 | Total Loss: 1.0525099039077759 | KNN Loss: 6.224244117736816 | BCE Loss: 1.0525099039077759\n",
      "Epoch 186 / 500 | iteration 10 / 30 | Total Loss: 1.0478814840316772 | KNN Loss: 6.224388122558594 | BCE Loss: 1.0478814840316772\n",
      "Epoch 186 / 500 | iteration 15 / 30 | Total Loss: 1.0678926706314087 | KNN Loss: 6.224642753601074 | BCE Loss: 1.0678926706314087\n",
      "Epoch 186 / 500 | iteration 20 / 30 | Total Loss: 1.0616703033447266 | KNN Loss: 6.22447395324707 | BCE Loss: 1.0616703033447266\n",
      "Epoch 186 / 500 | iteration 25 / 30 | Total Loss: 1.0552752017974854 | KNN Loss: 6.224319934844971 | BCE Loss: 1.0552752017974854\n",
      "Epoch 187 / 500 | iteration 0 / 30 | Total Loss: 1.05411958694458 | KNN Loss: 6.22450065612793 | BCE Loss: 1.05411958694458\n",
      "Epoch 187 / 500 | iteration 5 / 30 | Total Loss: 1.0522775650024414 | KNN Loss: 6.224391937255859 | BCE Loss: 1.0522775650024414\n",
      "Epoch 187 / 500 | iteration 10 / 30 | Total Loss: 1.0322511196136475 | KNN Loss: 6.223862648010254 | BCE Loss: 1.0322511196136475\n",
      "Epoch 187 / 500 | iteration 15 / 30 | Total Loss: 1.057466983795166 | KNN Loss: 6.224163055419922 | BCE Loss: 1.057466983795166\n",
      "Epoch 187 / 500 | iteration 20 / 30 | Total Loss: 1.0385186672210693 | KNN Loss: 6.224881649017334 | BCE Loss: 1.0385186672210693\n",
      "Epoch 187 / 500 | iteration 25 / 30 | Total Loss: 1.0260968208312988 | KNN Loss: 6.224880218505859 | BCE Loss: 1.0260968208312988\n",
      "Epoch 188 / 500 | iteration 0 / 30 | Total Loss: 1.0601989030838013 | KNN Loss: 6.224549770355225 | BCE Loss: 1.0601989030838013\n",
      "Epoch 188 / 500 | iteration 5 / 30 | Total Loss: 1.0562101602554321 | KNN Loss: 6.2247185707092285 | BCE Loss: 1.0562101602554321\n",
      "Epoch 188 / 500 | iteration 10 / 30 | Total Loss: 1.0620561838150024 | KNN Loss: 6.224400520324707 | BCE Loss: 1.0620561838150024\n",
      "Epoch 188 / 500 | iteration 15 / 30 | Total Loss: 1.0442731380462646 | KNN Loss: 6.224643230438232 | BCE Loss: 1.0442731380462646\n",
      "Epoch 188 / 500 | iteration 20 / 30 | Total Loss: 1.0538263320922852 | KNN Loss: 6.224653720855713 | BCE Loss: 1.0538263320922852\n",
      "Epoch 188 / 500 | iteration 25 / 30 | Total Loss: 1.0494097471237183 | KNN Loss: 6.224604606628418 | BCE Loss: 1.0494097471237183\n",
      "Epoch   189: reducing learning rate of group 0 to 6.9206e-05.\n",
      "Epoch 189 / 500 | iteration 0 / 30 | Total Loss: 1.0544407367706299 | KNN Loss: 6.224695205688477 | BCE Loss: 1.0544407367706299\n",
      "Epoch 189 / 500 | iteration 5 / 30 | Total Loss: 1.074435830116272 | KNN Loss: 6.224343776702881 | BCE Loss: 1.074435830116272\n",
      "Epoch 189 / 500 | iteration 10 / 30 | Total Loss: 1.0501539707183838 | KNN Loss: 6.224662780761719 | BCE Loss: 1.0501539707183838\n",
      "Epoch 189 / 500 | iteration 15 / 30 | Total Loss: 1.0204181671142578 | KNN Loss: 6.224215507507324 | BCE Loss: 1.0204181671142578\n",
      "Epoch 189 / 500 | iteration 20 / 30 | Total Loss: 1.0464105606079102 | KNN Loss: 6.224659442901611 | BCE Loss: 1.0464105606079102\n",
      "Epoch 189 / 500 | iteration 25 / 30 | Total Loss: 1.0800771713256836 | KNN Loss: 6.224611282348633 | BCE Loss: 1.0800771713256836\n",
      "Epoch 190 / 500 | iteration 0 / 30 | Total Loss: 1.0402508974075317 | KNN Loss: 6.224505424499512 | BCE Loss: 1.0402508974075317\n",
      "Epoch 190 / 500 | iteration 5 / 30 | Total Loss: 1.0433850288391113 | KNN Loss: 6.22476863861084 | BCE Loss: 1.0433850288391113\n",
      "Epoch 190 / 500 | iteration 10 / 30 | Total Loss: 1.0581659078598022 | KNN Loss: 6.224425315856934 | BCE Loss: 1.0581659078598022\n",
      "Epoch 190 / 500 | iteration 15 / 30 | Total Loss: 1.0627129077911377 | KNN Loss: 6.224191188812256 | BCE Loss: 1.0627129077911377\n",
      "Epoch 190 / 500 | iteration 20 / 30 | Total Loss: 1.042726993560791 | KNN Loss: 6.224232196807861 | BCE Loss: 1.042726993560791\n",
      "Epoch 190 / 500 | iteration 25 / 30 | Total Loss: 1.0671701431274414 | KNN Loss: 6.224623680114746 | BCE Loss: 1.0671701431274414\n",
      "Epoch 191 / 500 | iteration 0 / 30 | Total Loss: 1.0492148399353027 | KNN Loss: 6.224699020385742 | BCE Loss: 1.0492148399353027\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 191 / 500 | iteration 5 / 30 | Total Loss: 1.0423469543457031 | KNN Loss: 6.2241010665893555 | BCE Loss: 1.0423469543457031\n",
      "Epoch 191 / 500 | iteration 10 / 30 | Total Loss: 1.0488966703414917 | KNN Loss: 6.224429130554199 | BCE Loss: 1.0488966703414917\n",
      "Epoch 191 / 500 | iteration 15 / 30 | Total Loss: 1.029536247253418 | KNN Loss: 6.224433898925781 | BCE Loss: 1.029536247253418\n",
      "Epoch 191 / 500 | iteration 20 / 30 | Total Loss: 1.0639927387237549 | KNN Loss: 6.224366664886475 | BCE Loss: 1.0639927387237549\n",
      "Epoch 191 / 500 | iteration 25 / 30 | Total Loss: 1.047588586807251 | KNN Loss: 6.2245025634765625 | BCE Loss: 1.047588586807251\n",
      "Epoch 192 / 500 | iteration 0 / 30 | Total Loss: 1.0649235248565674 | KNN Loss: 6.224679946899414 | BCE Loss: 1.0649235248565674\n",
      "Epoch 192 / 500 | iteration 5 / 30 | Total Loss: 1.0534687042236328 | KNN Loss: 6.224210739135742 | BCE Loss: 1.0534687042236328\n",
      "Epoch 192 / 500 | iteration 10 / 30 | Total Loss: 1.0691789388656616 | KNN Loss: 6.2248969078063965 | BCE Loss: 1.0691789388656616\n",
      "Epoch 192 / 500 | iteration 15 / 30 | Total Loss: 1.0559303760528564 | KNN Loss: 6.224668979644775 | BCE Loss: 1.0559303760528564\n",
      "Epoch 192 / 500 | iteration 20 / 30 | Total Loss: 1.069197416305542 | KNN Loss: 6.224207878112793 | BCE Loss: 1.069197416305542\n",
      "Epoch 192 / 500 | iteration 25 / 30 | Total Loss: 1.0477350950241089 | KNN Loss: 6.224517822265625 | BCE Loss: 1.0477350950241089\n",
      "Epoch 193 / 500 | iteration 0 / 30 | Total Loss: 1.0507824420928955 | KNN Loss: 6.224419116973877 | BCE Loss: 1.0507824420928955\n",
      "Epoch 193 / 500 | iteration 5 / 30 | Total Loss: 1.0542796850204468 | KNN Loss: 6.224488735198975 | BCE Loss: 1.0542796850204468\n",
      "Epoch 193 / 500 | iteration 10 / 30 | Total Loss: 1.06150221824646 | KNN Loss: 6.224630355834961 | BCE Loss: 1.06150221824646\n",
      "Epoch 193 / 500 | iteration 15 / 30 | Total Loss: 1.0177289247512817 | KNN Loss: 6.224908828735352 | BCE Loss: 1.0177289247512817\n",
      "Epoch 193 / 500 | iteration 20 / 30 | Total Loss: 1.0723240375518799 | KNN Loss: 6.224878311157227 | BCE Loss: 1.0723240375518799\n",
      "Epoch 193 / 500 | iteration 25 / 30 | Total Loss: 1.0549802780151367 | KNN Loss: 6.224062919616699 | BCE Loss: 1.0549802780151367\n",
      "Epoch 194 / 500 | iteration 0 / 30 | Total Loss: 1.0144189596176147 | KNN Loss: 6.224644660949707 | BCE Loss: 1.0144189596176147\n",
      "Epoch 194 / 500 | iteration 5 / 30 | Total Loss: 1.0780537128448486 | KNN Loss: 6.224391460418701 | BCE Loss: 1.0780537128448486\n",
      "Epoch 194 / 500 | iteration 10 / 30 | Total Loss: 1.0147202014923096 | KNN Loss: 6.224421977996826 | BCE Loss: 1.0147202014923096\n",
      "Epoch 194 / 500 | iteration 15 / 30 | Total Loss: 1.0431268215179443 | KNN Loss: 6.224360942840576 | BCE Loss: 1.0431268215179443\n",
      "Epoch 194 / 500 | iteration 20 / 30 | Total Loss: 1.082334280014038 | KNN Loss: 6.22456169128418 | BCE Loss: 1.082334280014038\n",
      "Epoch 194 / 500 | iteration 25 / 30 | Total Loss: 1.0563610792160034 | KNN Loss: 6.224470138549805 | BCE Loss: 1.0563610792160034\n",
      "Epoch 195 / 500 | iteration 0 / 30 | Total Loss: 1.022576928138733 | KNN Loss: 6.224106311798096 | BCE Loss: 1.022576928138733\n",
      "Epoch 195 / 500 | iteration 5 / 30 | Total Loss: 1.048302412033081 | KNN Loss: 6.2244744300842285 | BCE Loss: 1.048302412033081\n",
      "Epoch 195 / 500 | iteration 10 / 30 | Total Loss: 1.061237096786499 | KNN Loss: 6.224696636199951 | BCE Loss: 1.061237096786499\n",
      "Epoch 195 / 500 | iteration 15 / 30 | Total Loss: 1.0712263584136963 | KNN Loss: 6.224832534790039 | BCE Loss: 1.0712263584136963\n",
      "Epoch 195 / 500 | iteration 20 / 30 | Total Loss: 1.0579077005386353 | KNN Loss: 6.224576950073242 | BCE Loss: 1.0579077005386353\n",
      "Epoch 195 / 500 | iteration 25 / 30 | Total Loss: 1.0555081367492676 | KNN Loss: 6.22434663772583 | BCE Loss: 1.0555081367492676\n",
      "Epoch 196 / 500 | iteration 0 / 30 | Total Loss: 1.0486184358596802 | KNN Loss: 6.2245330810546875 | BCE Loss: 1.0486184358596802\n",
      "Epoch 196 / 500 | iteration 5 / 30 | Total Loss: 1.046270728111267 | KNN Loss: 6.224368095397949 | BCE Loss: 1.046270728111267\n",
      "Epoch 196 / 500 | iteration 10 / 30 | Total Loss: 1.0549330711364746 | KNN Loss: 6.22473669052124 | BCE Loss: 1.0549330711364746\n",
      "Epoch 196 / 500 | iteration 15 / 30 | Total Loss: 1.067628026008606 | KNN Loss: 6.224401473999023 | BCE Loss: 1.067628026008606\n",
      "Epoch 196 / 500 | iteration 20 / 30 | Total Loss: 1.071362018585205 | KNN Loss: 6.224422454833984 | BCE Loss: 1.071362018585205\n",
      "Epoch 196 / 500 | iteration 25 / 30 | Total Loss: 1.0664165019989014 | KNN Loss: 6.224264144897461 | BCE Loss: 1.0664165019989014\n",
      "Epoch 197 / 500 | iteration 0 / 30 | Total Loss: 1.0333313941955566 | KNN Loss: 6.225011825561523 | BCE Loss: 1.0333313941955566\n",
      "Epoch 197 / 500 | iteration 5 / 30 | Total Loss: 1.0499372482299805 | KNN Loss: 6.224413871765137 | BCE Loss: 1.0499372482299805\n",
      "Epoch 197 / 500 | iteration 10 / 30 | Total Loss: 1.0530064105987549 | KNN Loss: 6.224375247955322 | BCE Loss: 1.0530064105987549\n",
      "Epoch 197 / 500 | iteration 15 / 30 | Total Loss: 1.0462987422943115 | KNN Loss: 6.224679946899414 | BCE Loss: 1.0462987422943115\n",
      "Epoch 197 / 500 | iteration 20 / 30 | Total Loss: 1.083864450454712 | KNN Loss: 6.224400520324707 | BCE Loss: 1.083864450454712\n",
      "Epoch 197 / 500 | iteration 25 / 30 | Total Loss: 1.0513381958007812 | KNN Loss: 6.224252700805664 | BCE Loss: 1.0513381958007812\n",
      "Epoch 198 / 500 | iteration 0 / 30 | Total Loss: 1.04823899269104 | KNN Loss: 6.2246503829956055 | BCE Loss: 1.04823899269104\n",
      "Epoch 198 / 500 | iteration 5 / 30 | Total Loss: 1.0413522720336914 | KNN Loss: 6.224637985229492 | BCE Loss: 1.0413522720336914\n",
      "Epoch 198 / 500 | iteration 10 / 30 | Total Loss: 1.0345382690429688 | KNN Loss: 6.224689960479736 | BCE Loss: 1.0345382690429688\n",
      "Epoch 198 / 500 | iteration 15 / 30 | Total Loss: 1.0266239643096924 | KNN Loss: 6.224491119384766 | BCE Loss: 1.0266239643096924\n",
      "Epoch 198 / 500 | iteration 20 / 30 | Total Loss: 1.065528392791748 | KNN Loss: 6.22440767288208 | BCE Loss: 1.065528392791748\n",
      "Epoch 198 / 500 | iteration 25 / 30 | Total Loss: 1.0396384000778198 | KNN Loss: 6.224608898162842 | BCE Loss: 1.0396384000778198\n",
      "Epoch 199 / 500 | iteration 0 / 30 | Total Loss: 1.0653462409973145 | KNN Loss: 6.224575042724609 | BCE Loss: 1.0653462409973145\n",
      "Epoch 199 / 500 | iteration 5 / 30 | Total Loss: 1.0567606687545776 | KNN Loss: 6.224667072296143 | BCE Loss: 1.0567606687545776\n",
      "Epoch 199 / 500 | iteration 10 / 30 | Total Loss: 1.0423424243927002 | KNN Loss: 6.224423408508301 | BCE Loss: 1.0423424243927002\n",
      "Epoch 199 / 500 | iteration 15 / 30 | Total Loss: 1.0248150825500488 | KNN Loss: 6.224197864532471 | BCE Loss: 1.0248150825500488\n",
      "Epoch 199 / 500 | iteration 20 / 30 | Total Loss: 1.0253782272338867 | KNN Loss: 6.224780082702637 | BCE Loss: 1.0253782272338867\n",
      "Epoch 199 / 500 | iteration 25 / 30 | Total Loss: 1.0503385066986084 | KNN Loss: 6.2240891456604 | BCE Loss: 1.0503385066986084\n",
      "Epoch   200: reducing learning rate of group 0 to 4.8445e-05.\n",
      "Epoch 200 / 500 | iteration 0 / 30 | Total Loss: 1.0378715991973877 | KNN Loss: 6.224660396575928 | BCE Loss: 1.0378715991973877\n",
      "Epoch 200 / 500 | iteration 5 / 30 | Total Loss: 1.0333847999572754 | KNN Loss: 6.224197864532471 | BCE Loss: 1.0333847999572754\n",
      "Epoch 200 / 500 | iteration 10 / 30 | Total Loss: 1.062268614768982 | KNN Loss: 6.224222660064697 | BCE Loss: 1.062268614768982\n",
      "Epoch 200 / 500 | iteration 15 / 30 | Total Loss: 1.0264360904693604 | KNN Loss: 6.224612712860107 | BCE Loss: 1.0264360904693604\n",
      "Epoch 200 / 500 | iteration 20 / 30 | Total Loss: 1.047340750694275 | KNN Loss: 6.22490119934082 | BCE Loss: 1.047340750694275\n",
      "Epoch 200 / 500 | iteration 25 / 30 | Total Loss: 1.061565637588501 | KNN Loss: 6.224460601806641 | BCE Loss: 1.061565637588501\n",
      "Epoch 201 / 500 | iteration 0 / 30 | Total Loss: 1.0529273748397827 | KNN Loss: 6.223987579345703 | BCE Loss: 1.0529273748397827\n",
      "Epoch 201 / 500 | iteration 5 / 30 | Total Loss: 1.0539103746414185 | KNN Loss: 6.224322319030762 | BCE Loss: 1.0539103746414185\n",
      "Epoch 201 / 500 | iteration 10 / 30 | Total Loss: 1.037607192993164 | KNN Loss: 6.224305629730225 | BCE Loss: 1.037607192993164\n",
      "Epoch 201 / 500 | iteration 15 / 30 | Total Loss: 1.0084785223007202 | KNN Loss: 6.22433614730835 | BCE Loss: 1.0084785223007202\n",
      "Epoch 201 / 500 | iteration 20 / 30 | Total Loss: 1.0451321601867676 | KNN Loss: 6.2243852615356445 | BCE Loss: 1.0451321601867676\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 201 / 500 | iteration 25 / 30 | Total Loss: 1.0267778635025024 | KNN Loss: 6.224354267120361 | BCE Loss: 1.0267778635025024\n",
      "Epoch 202 / 500 | iteration 0 / 30 | Total Loss: 1.0787684917449951 | KNN Loss: 6.224696636199951 | BCE Loss: 1.0787684917449951\n",
      "Epoch 202 / 500 | iteration 5 / 30 | Total Loss: 1.078385829925537 | KNN Loss: 6.224146842956543 | BCE Loss: 1.078385829925537\n",
      "Epoch 202 / 500 | iteration 10 / 30 | Total Loss: 1.0522165298461914 | KNN Loss: 6.224628448486328 | BCE Loss: 1.0522165298461914\n",
      "Epoch 202 / 500 | iteration 15 / 30 | Total Loss: 1.0645942687988281 | KNN Loss: 6.2244133949279785 | BCE Loss: 1.0645942687988281\n",
      "Epoch 202 / 500 | iteration 20 / 30 | Total Loss: 1.0524063110351562 | KNN Loss: 6.224375247955322 | BCE Loss: 1.0524063110351562\n",
      "Epoch 202 / 500 | iteration 25 / 30 | Total Loss: 1.0243499279022217 | KNN Loss: 6.224273681640625 | BCE Loss: 1.0243499279022217\n",
      "Epoch 203 / 500 | iteration 0 / 30 | Total Loss: 1.0561286211013794 | KNN Loss: 6.224851131439209 | BCE Loss: 1.0561286211013794\n",
      "Epoch 203 / 500 | iteration 5 / 30 | Total Loss: 1.0657505989074707 | KNN Loss: 6.224399566650391 | BCE Loss: 1.0657505989074707\n",
      "Epoch 203 / 500 | iteration 10 / 30 | Total Loss: 1.0495573282241821 | KNN Loss: 6.224413871765137 | BCE Loss: 1.0495573282241821\n",
      "Epoch 203 / 500 | iteration 15 / 30 | Total Loss: 1.0538012981414795 | KNN Loss: 6.2246551513671875 | BCE Loss: 1.0538012981414795\n",
      "Epoch 203 / 500 | iteration 20 / 30 | Total Loss: 1.0557491779327393 | KNN Loss: 6.224379539489746 | BCE Loss: 1.0557491779327393\n",
      "Epoch 203 / 500 | iteration 25 / 30 | Total Loss: 1.0702183246612549 | KNN Loss: 6.224641799926758 | BCE Loss: 1.0702183246612549\n",
      "Epoch 204 / 500 | iteration 0 / 30 | Total Loss: 1.0546321868896484 | KNN Loss: 6.224375247955322 | BCE Loss: 1.0546321868896484\n",
      "Epoch 204 / 500 | iteration 5 / 30 | Total Loss: 1.0414903163909912 | KNN Loss: 6.2244062423706055 | BCE Loss: 1.0414903163909912\n",
      "Epoch 204 / 500 | iteration 10 / 30 | Total Loss: 1.0488970279693604 | KNN Loss: 6.224315643310547 | BCE Loss: 1.0488970279693604\n",
      "Epoch 204 / 500 | iteration 15 / 30 | Total Loss: 1.0601365566253662 | KNN Loss: 6.224480628967285 | BCE Loss: 1.0601365566253662\n",
      "Epoch 204 / 500 | iteration 20 / 30 | Total Loss: 1.0668175220489502 | KNN Loss: 6.224117279052734 | BCE Loss: 1.0668175220489502\n",
      "Epoch 204 / 500 | iteration 25 / 30 | Total Loss: 1.0574853420257568 | KNN Loss: 6.22443151473999 | BCE Loss: 1.0574853420257568\n",
      "Epoch 205 / 500 | iteration 0 / 30 | Total Loss: 1.0491923093795776 | KNN Loss: 6.2242021560668945 | BCE Loss: 1.0491923093795776\n",
      "Epoch 205 / 500 | iteration 5 / 30 | Total Loss: 1.0380438566207886 | KNN Loss: 6.224489212036133 | BCE Loss: 1.0380438566207886\n",
      "Epoch 205 / 500 | iteration 10 / 30 | Total Loss: 1.0540270805358887 | KNN Loss: 6.224557399749756 | BCE Loss: 1.0540270805358887\n",
      "Epoch 205 / 500 | iteration 15 / 30 | Total Loss: 1.0481191873550415 | KNN Loss: 6.2247090339660645 | BCE Loss: 1.0481191873550415\n",
      "Epoch 205 / 500 | iteration 20 / 30 | Total Loss: 1.055719017982483 | KNN Loss: 6.224603652954102 | BCE Loss: 1.055719017982483\n",
      "Epoch 205 / 500 | iteration 25 / 30 | Total Loss: 1.0640088319778442 | KNN Loss: 6.224392414093018 | BCE Loss: 1.0640088319778442\n",
      "Epoch 206 / 500 | iteration 0 / 30 | Total Loss: 1.0390251874923706 | KNN Loss: 6.224257946014404 | BCE Loss: 1.0390251874923706\n",
      "Epoch 206 / 500 | iteration 5 / 30 | Total Loss: 1.06991446018219 | KNN Loss: 6.224727153778076 | BCE Loss: 1.06991446018219\n",
      "Epoch 206 / 500 | iteration 10 / 30 | Total Loss: 1.0512511730194092 | KNN Loss: 6.22451114654541 | BCE Loss: 1.0512511730194092\n",
      "Epoch 206 / 500 | iteration 15 / 30 | Total Loss: 1.061760663986206 | KNN Loss: 6.224240779876709 | BCE Loss: 1.061760663986206\n",
      "Epoch 206 / 500 | iteration 20 / 30 | Total Loss: 1.035275936126709 | KNN Loss: 6.224454402923584 | BCE Loss: 1.035275936126709\n",
      "Epoch 206 / 500 | iteration 25 / 30 | Total Loss: 1.0430171489715576 | KNN Loss: 6.224551200866699 | BCE Loss: 1.0430171489715576\n",
      "Epoch 207 / 500 | iteration 0 / 30 | Total Loss: 1.0442043542861938 | KNN Loss: 6.2243428230285645 | BCE Loss: 1.0442043542861938\n",
      "Epoch 207 / 500 | iteration 5 / 30 | Total Loss: 1.0688672065734863 | KNN Loss: 6.224576950073242 | BCE Loss: 1.0688672065734863\n",
      "Epoch 207 / 500 | iteration 10 / 30 | Total Loss: 1.0208394527435303 | KNN Loss: 6.224621295928955 | BCE Loss: 1.0208394527435303\n",
      "Epoch 207 / 500 | iteration 15 / 30 | Total Loss: 1.0641772747039795 | KNN Loss: 6.224483013153076 | BCE Loss: 1.0641772747039795\n",
      "Epoch 207 / 500 | iteration 20 / 30 | Total Loss: 1.0504999160766602 | KNN Loss: 6.224273681640625 | BCE Loss: 1.0504999160766602\n",
      "Epoch 207 / 500 | iteration 25 / 30 | Total Loss: 1.0352280139923096 | KNN Loss: 6.224632263183594 | BCE Loss: 1.0352280139923096\n",
      "Epoch 208 / 500 | iteration 0 / 30 | Total Loss: 1.0453667640686035 | KNN Loss: 6.22456169128418 | BCE Loss: 1.0453667640686035\n",
      "Epoch 208 / 500 | iteration 5 / 30 | Total Loss: 1.0784375667572021 | KNN Loss: 6.224139213562012 | BCE Loss: 1.0784375667572021\n",
      "Epoch 208 / 500 | iteration 10 / 30 | Total Loss: 1.0558500289916992 | KNN Loss: 6.224308967590332 | BCE Loss: 1.0558500289916992\n",
      "Epoch 208 / 500 | iteration 15 / 30 | Total Loss: 1.0626355409622192 | KNN Loss: 6.2244873046875 | BCE Loss: 1.0626355409622192\n",
      "Epoch 208 / 500 | iteration 20 / 30 | Total Loss: 1.0526628494262695 | KNN Loss: 6.224644184112549 | BCE Loss: 1.0526628494262695\n",
      "Epoch 208 / 500 | iteration 25 / 30 | Total Loss: 1.0294508934020996 | KNN Loss: 6.224432945251465 | BCE Loss: 1.0294508934020996\n",
      "Epoch 209 / 500 | iteration 0 / 30 | Total Loss: 1.0529468059539795 | KNN Loss: 6.224432945251465 | BCE Loss: 1.0529468059539795\n",
      "Epoch 209 / 500 | iteration 5 / 30 | Total Loss: 1.0416967868804932 | KNN Loss: 6.224320888519287 | BCE Loss: 1.0416967868804932\n",
      "Epoch 209 / 500 | iteration 10 / 30 | Total Loss: 1.026672124862671 | KNN Loss: 6.224310874938965 | BCE Loss: 1.026672124862671\n",
      "Epoch 209 / 500 | iteration 15 / 30 | Total Loss: 1.043926477432251 | KNN Loss: 6.2245988845825195 | BCE Loss: 1.043926477432251\n",
      "Epoch 209 / 500 | iteration 20 / 30 | Total Loss: 1.0434343814849854 | KNN Loss: 6.224595069885254 | BCE Loss: 1.0434343814849854\n",
      "Epoch 209 / 500 | iteration 25 / 30 | Total Loss: 1.054753065109253 | KNN Loss: 6.224549770355225 | BCE Loss: 1.054753065109253\n",
      "Epoch 210 / 500 | iteration 0 / 30 | Total Loss: 1.0509183406829834 | KNN Loss: 6.22427225112915 | BCE Loss: 1.0509183406829834\n",
      "Epoch 210 / 500 | iteration 5 / 30 | Total Loss: 1.0460398197174072 | KNN Loss: 6.224623203277588 | BCE Loss: 1.0460398197174072\n",
      "Epoch 210 / 500 | iteration 10 / 30 | Total Loss: 1.0683798789978027 | KNN Loss: 6.224114894866943 | BCE Loss: 1.0683798789978027\n",
      "Epoch 210 / 500 | iteration 15 / 30 | Total Loss: 1.0282566547393799 | KNN Loss: 6.224549293518066 | BCE Loss: 1.0282566547393799\n",
      "Epoch 210 / 500 | iteration 20 / 30 | Total Loss: 1.0607317686080933 | KNN Loss: 6.224216938018799 | BCE Loss: 1.0607317686080933\n",
      "Epoch 210 / 500 | iteration 25 / 30 | Total Loss: 1.0372593402862549 | KNN Loss: 6.224494934082031 | BCE Loss: 1.0372593402862549\n",
      "Epoch   211: reducing learning rate of group 0 to 3.3911e-05.\n",
      "Epoch 211 / 500 | iteration 0 / 30 | Total Loss: 1.0620596408843994 | KNN Loss: 6.224836349487305 | BCE Loss: 1.0620596408843994\n",
      "Epoch 211 / 500 | iteration 5 / 30 | Total Loss: 1.0552682876586914 | KNN Loss: 6.224217891693115 | BCE Loss: 1.0552682876586914\n",
      "Epoch 211 / 500 | iteration 10 / 30 | Total Loss: 1.0740020275115967 | KNN Loss: 6.224155426025391 | BCE Loss: 1.0740020275115967\n",
      "Epoch 211 / 500 | iteration 15 / 30 | Total Loss: 1.0660724639892578 | KNN Loss: 6.22416877746582 | BCE Loss: 1.0660724639892578\n",
      "Epoch 211 / 500 | iteration 20 / 30 | Total Loss: 1.0335438251495361 | KNN Loss: 6.224515914916992 | BCE Loss: 1.0335438251495361\n",
      "Epoch 211 / 500 | iteration 25 / 30 | Total Loss: 1.04721200466156 | KNN Loss: 6.224299907684326 | BCE Loss: 1.04721200466156\n",
      "Epoch 212 / 500 | iteration 0 / 30 | Total Loss: 1.0637258291244507 | KNN Loss: 6.2245564460754395 | BCE Loss: 1.0637258291244507\n",
      "Epoch 212 / 500 | iteration 5 / 30 | Total Loss: 1.0371594429016113 | KNN Loss: 6.224409103393555 | BCE Loss: 1.0371594429016113\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 212 / 500 | iteration 10 / 30 | Total Loss: 1.0614166259765625 | KNN Loss: 6.224542140960693 | BCE Loss: 1.0614166259765625\n",
      "Epoch 212 / 500 | iteration 15 / 30 | Total Loss: 1.0532965660095215 | KNN Loss: 6.224286079406738 | BCE Loss: 1.0532965660095215\n",
      "Epoch 212 / 500 | iteration 20 / 30 | Total Loss: 1.0444316864013672 | KNN Loss: 6.224583625793457 | BCE Loss: 1.0444316864013672\n",
      "Epoch 212 / 500 | iteration 25 / 30 | Total Loss: 1.06974196434021 | KNN Loss: 6.224419593811035 | BCE Loss: 1.06974196434021\n",
      "Epoch 213 / 500 | iteration 0 / 30 | Total Loss: 1.0598831176757812 | KNN Loss: 6.2248406410217285 | BCE Loss: 1.0598831176757812\n",
      "Epoch 213 / 500 | iteration 5 / 30 | Total Loss: 1.0313644409179688 | KNN Loss: 6.224653244018555 | BCE Loss: 1.0313644409179688\n",
      "Epoch 213 / 500 | iteration 10 / 30 | Total Loss: 1.0352468490600586 | KNN Loss: 6.224531173706055 | BCE Loss: 1.0352468490600586\n",
      "Epoch 213 / 500 | iteration 15 / 30 | Total Loss: 1.0814759731292725 | KNN Loss: 6.224477291107178 | BCE Loss: 1.0814759731292725\n",
      "Epoch 213 / 500 | iteration 20 / 30 | Total Loss: 1.0867418050765991 | KNN Loss: 6.224633693695068 | BCE Loss: 1.0867418050765991\n",
      "Epoch 213 / 500 | iteration 25 / 30 | Total Loss: 1.0274841785430908 | KNN Loss: 6.224839210510254 | BCE Loss: 1.0274841785430908\n",
      "Epoch 214 / 500 | iteration 0 / 30 | Total Loss: 1.0656967163085938 | KNN Loss: 6.224208831787109 | BCE Loss: 1.0656967163085938\n",
      "Epoch 214 / 500 | iteration 5 / 30 | Total Loss: 1.0860750675201416 | KNN Loss: 6.224527835845947 | BCE Loss: 1.0860750675201416\n",
      "Epoch 214 / 500 | iteration 10 / 30 | Total Loss: 1.0525667667388916 | KNN Loss: 6.224386692047119 | BCE Loss: 1.0525667667388916\n",
      "Epoch 214 / 500 | iteration 15 / 30 | Total Loss: 1.0453451871871948 | KNN Loss: 6.224297046661377 | BCE Loss: 1.0453451871871948\n",
      "Epoch 214 / 500 | iteration 20 / 30 | Total Loss: 1.0602450370788574 | KNN Loss: 6.224937438964844 | BCE Loss: 1.0602450370788574\n",
      "Epoch 214 / 500 | iteration 25 / 30 | Total Loss: 1.0376396179199219 | KNN Loss: 6.224542617797852 | BCE Loss: 1.0376396179199219\n",
      "Epoch 215 / 500 | iteration 0 / 30 | Total Loss: 1.0356695652008057 | KNN Loss: 6.22426176071167 | BCE Loss: 1.0356695652008057\n",
      "Epoch 215 / 500 | iteration 5 / 30 | Total Loss: 1.0731722116470337 | KNN Loss: 6.22456693649292 | BCE Loss: 1.0731722116470337\n",
      "Epoch 215 / 500 | iteration 10 / 30 | Total Loss: 1.0700291395187378 | KNN Loss: 6.224316596984863 | BCE Loss: 1.0700291395187378\n",
      "Epoch 215 / 500 | iteration 15 / 30 | Total Loss: 1.060481309890747 | KNN Loss: 6.224603652954102 | BCE Loss: 1.060481309890747\n",
      "Epoch 215 / 500 | iteration 20 / 30 | Total Loss: 1.0439062118530273 | KNN Loss: 6.224532127380371 | BCE Loss: 1.0439062118530273\n",
      "Epoch 215 / 500 | iteration 25 / 30 | Total Loss: 1.033345341682434 | KNN Loss: 6.224487781524658 | BCE Loss: 1.033345341682434\n",
      "Epoch 216 / 500 | iteration 0 / 30 | Total Loss: 1.042154312133789 | KNN Loss: 6.224729537963867 | BCE Loss: 1.042154312133789\n",
      "Epoch 216 / 500 | iteration 5 / 30 | Total Loss: 1.0496768951416016 | KNN Loss: 6.224151611328125 | BCE Loss: 1.0496768951416016\n",
      "Epoch 216 / 500 | iteration 10 / 30 | Total Loss: 1.0813443660736084 | KNN Loss: 6.224793910980225 | BCE Loss: 1.0813443660736084\n",
      "Epoch 216 / 500 | iteration 15 / 30 | Total Loss: 1.0710046291351318 | KNN Loss: 6.224819183349609 | BCE Loss: 1.0710046291351318\n",
      "Epoch 216 / 500 | iteration 20 / 30 | Total Loss: 1.0353158712387085 | KNN Loss: 6.223728179931641 | BCE Loss: 1.0353158712387085\n",
      "Epoch 216 / 500 | iteration 25 / 30 | Total Loss: 1.0417635440826416 | KNN Loss: 6.224411964416504 | BCE Loss: 1.0417635440826416\n",
      "Epoch 217 / 500 | iteration 0 / 30 | Total Loss: 1.052253246307373 | KNN Loss: 6.224656581878662 | BCE Loss: 1.052253246307373\n",
      "Epoch 217 / 500 | iteration 5 / 30 | Total Loss: 1.0588765144348145 | KNN Loss: 6.224343299865723 | BCE Loss: 1.0588765144348145\n",
      "Epoch 217 / 500 | iteration 10 / 30 | Total Loss: 1.0784144401550293 | KNN Loss: 6.224270820617676 | BCE Loss: 1.0784144401550293\n",
      "Epoch 217 / 500 | iteration 15 / 30 | Total Loss: 1.0683999061584473 | KNN Loss: 6.22456693649292 | BCE Loss: 1.0683999061584473\n",
      "Epoch 217 / 500 | iteration 20 / 30 | Total Loss: 1.0490717887878418 | KNN Loss: 6.224441051483154 | BCE Loss: 1.0490717887878418\n",
      "Epoch 217 / 500 | iteration 25 / 30 | Total Loss: 1.0463898181915283 | KNN Loss: 6.224574089050293 | BCE Loss: 1.0463898181915283\n",
      "Epoch 218 / 500 | iteration 0 / 30 | Total Loss: 1.0671027898788452 | KNN Loss: 6.2245869636535645 | BCE Loss: 1.0671027898788452\n",
      "Epoch 218 / 500 | iteration 5 / 30 | Total Loss: 1.0288543701171875 | KNN Loss: 6.224213600158691 | BCE Loss: 1.0288543701171875\n",
      "Epoch 218 / 500 | iteration 10 / 30 | Total Loss: 1.0561729669570923 | KNN Loss: 6.224388599395752 | BCE Loss: 1.0561729669570923\n",
      "Epoch 218 / 500 | iteration 15 / 30 | Total Loss: 1.0423133373260498 | KNN Loss: 6.224374294281006 | BCE Loss: 1.0423133373260498\n",
      "Epoch 218 / 500 | iteration 20 / 30 | Total Loss: 1.0611358880996704 | KNN Loss: 6.2243266105651855 | BCE Loss: 1.0611358880996704\n",
      "Epoch 218 / 500 | iteration 25 / 30 | Total Loss: 1.0469812154769897 | KNN Loss: 6.224419116973877 | BCE Loss: 1.0469812154769897\n",
      "Epoch 219 / 500 | iteration 0 / 30 | Total Loss: 1.0454211235046387 | KNN Loss: 6.224486827850342 | BCE Loss: 1.0454211235046387\n",
      "Epoch 219 / 500 | iteration 5 / 30 | Total Loss: 1.0690370798110962 | KNN Loss: 6.2246174812316895 | BCE Loss: 1.0690370798110962\n",
      "Epoch 219 / 500 | iteration 10 / 30 | Total Loss: 1.076758623123169 | KNN Loss: 6.224782466888428 | BCE Loss: 1.076758623123169\n",
      "Epoch 219 / 500 | iteration 15 / 30 | Total Loss: 1.0574932098388672 | KNN Loss: 6.224074363708496 | BCE Loss: 1.0574932098388672\n",
      "Epoch 219 / 500 | iteration 20 / 30 | Total Loss: 1.0478734970092773 | KNN Loss: 6.224573612213135 | BCE Loss: 1.0478734970092773\n",
      "Epoch 219 / 500 | iteration 25 / 30 | Total Loss: 1.0627837181091309 | KNN Loss: 6.224781513214111 | BCE Loss: 1.0627837181091309\n",
      "Epoch 220 / 500 | iteration 0 / 30 | Total Loss: 1.0468535423278809 | KNN Loss: 6.224794387817383 | BCE Loss: 1.0468535423278809\n",
      "Epoch 220 / 500 | iteration 5 / 30 | Total Loss: 1.0280401706695557 | KNN Loss: 6.224193572998047 | BCE Loss: 1.0280401706695557\n",
      "Epoch 220 / 500 | iteration 10 / 30 | Total Loss: 1.0360949039459229 | KNN Loss: 6.224521160125732 | BCE Loss: 1.0360949039459229\n",
      "Epoch 220 / 500 | iteration 15 / 30 | Total Loss: 1.0691344738006592 | KNN Loss: 6.224302291870117 | BCE Loss: 1.0691344738006592\n",
      "Epoch 220 / 500 | iteration 20 / 30 | Total Loss: 1.019878625869751 | KNN Loss: 6.224853038787842 | BCE Loss: 1.019878625869751\n",
      "Epoch 220 / 500 | iteration 25 / 30 | Total Loss: 1.0551633834838867 | KNN Loss: 6.224399566650391 | BCE Loss: 1.0551633834838867\n",
      "Epoch 221 / 500 | iteration 0 / 30 | Total Loss: 1.0388405323028564 | KNN Loss: 6.224424839019775 | BCE Loss: 1.0388405323028564\n",
      "Epoch 221 / 500 | iteration 5 / 30 | Total Loss: 1.0447461605072021 | KNN Loss: 6.224449634552002 | BCE Loss: 1.0447461605072021\n",
      "Epoch 221 / 500 | iteration 10 / 30 | Total Loss: 1.0314934253692627 | KNN Loss: 6.224488258361816 | BCE Loss: 1.0314934253692627\n",
      "Epoch 221 / 500 | iteration 15 / 30 | Total Loss: 1.0677446126937866 | KNN Loss: 6.2245378494262695 | BCE Loss: 1.0677446126937866\n",
      "Epoch 221 / 500 | iteration 20 / 30 | Total Loss: 1.04811429977417 | KNN Loss: 6.224342346191406 | BCE Loss: 1.04811429977417\n",
      "Epoch 221 / 500 | iteration 25 / 30 | Total Loss: 1.0509241819381714 | KNN Loss: 6.224331855773926 | BCE Loss: 1.0509241819381714\n",
      "Epoch   222: reducing learning rate of group 0 to 2.3738e-05.\n",
      "Epoch 222 / 500 | iteration 0 / 30 | Total Loss: 1.0549038648605347 | KNN Loss: 6.224542140960693 | BCE Loss: 1.0549038648605347\n",
      "Epoch 222 / 500 | iteration 5 / 30 | Total Loss: 1.0422585010528564 | KNN Loss: 6.224265098571777 | BCE Loss: 1.0422585010528564\n",
      "Epoch 222 / 500 | iteration 10 / 30 | Total Loss: 1.0568592548370361 | KNN Loss: 6.224274635314941 | BCE Loss: 1.0568592548370361\n",
      "Epoch 222 / 500 | iteration 15 / 30 | Total Loss: 1.0308525562286377 | KNN Loss: 6.224384784698486 | BCE Loss: 1.0308525562286377\n",
      "Epoch 222 / 500 | iteration 20 / 30 | Total Loss: 1.0203380584716797 | KNN Loss: 6.224653244018555 | BCE Loss: 1.0203380584716797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 222 / 500 | iteration 25 / 30 | Total Loss: 1.0172940492630005 | KNN Loss: 6.224638938903809 | BCE Loss: 1.0172940492630005\n",
      "Epoch 223 / 500 | iteration 0 / 30 | Total Loss: 1.060800313949585 | KNN Loss: 6.224909782409668 | BCE Loss: 1.060800313949585\n",
      "Epoch 223 / 500 | iteration 5 / 30 | Total Loss: 1.047761082649231 | KNN Loss: 6.224462985992432 | BCE Loss: 1.047761082649231\n",
      "Epoch 223 / 500 | iteration 10 / 30 | Total Loss: 1.0428802967071533 | KNN Loss: 6.224353313446045 | BCE Loss: 1.0428802967071533\n",
      "Epoch 223 / 500 | iteration 15 / 30 | Total Loss: 1.0442523956298828 | KNN Loss: 6.224369049072266 | BCE Loss: 1.0442523956298828\n",
      "Epoch 223 / 500 | iteration 20 / 30 | Total Loss: 1.027963399887085 | KNN Loss: 6.224691867828369 | BCE Loss: 1.027963399887085\n",
      "Epoch 223 / 500 | iteration 25 / 30 | Total Loss: 1.020859956741333 | KNN Loss: 6.224618911743164 | BCE Loss: 1.020859956741333\n",
      "Epoch 224 / 500 | iteration 0 / 30 | Total Loss: 1.0474138259887695 | KNN Loss: 6.224839687347412 | BCE Loss: 1.0474138259887695\n",
      "Epoch 224 / 500 | iteration 5 / 30 | Total Loss: 1.0674480199813843 | KNN Loss: 6.224574565887451 | BCE Loss: 1.0674480199813843\n",
      "Epoch 224 / 500 | iteration 10 / 30 | Total Loss: 1.0329407453536987 | KNN Loss: 6.224449634552002 | BCE Loss: 1.0329407453536987\n",
      "Epoch 224 / 500 | iteration 15 / 30 | Total Loss: 1.067903757095337 | KNN Loss: 6.224550247192383 | BCE Loss: 1.067903757095337\n",
      "Epoch 224 / 500 | iteration 20 / 30 | Total Loss: 1.0434340238571167 | KNN Loss: 6.224437236785889 | BCE Loss: 1.0434340238571167\n",
      "Epoch 224 / 500 | iteration 25 / 30 | Total Loss: 1.0120261907577515 | KNN Loss: 6.2246294021606445 | BCE Loss: 1.0120261907577515\n",
      "Epoch 225 / 500 | iteration 0 / 30 | Total Loss: 1.0618457794189453 | KNN Loss: 6.224523544311523 | BCE Loss: 1.0618457794189453\n",
      "Epoch 225 / 500 | iteration 5 / 30 | Total Loss: 1.055161476135254 | KNN Loss: 6.224344253540039 | BCE Loss: 1.055161476135254\n",
      "Epoch 225 / 500 | iteration 10 / 30 | Total Loss: 1.0356931686401367 | KNN Loss: 6.224658012390137 | BCE Loss: 1.0356931686401367\n",
      "Epoch 225 / 500 | iteration 15 / 30 | Total Loss: 1.0627095699310303 | KNN Loss: 6.2248125076293945 | BCE Loss: 1.0627095699310303\n",
      "Epoch 225 / 500 | iteration 20 / 30 | Total Loss: 1.0452133417129517 | KNN Loss: 6.2243781089782715 | BCE Loss: 1.0452133417129517\n",
      "Epoch 225 / 500 | iteration 25 / 30 | Total Loss: 1.0459725856781006 | KNN Loss: 6.224031925201416 | BCE Loss: 1.0459725856781006\n",
      "Epoch 226 / 500 | iteration 0 / 30 | Total Loss: 1.0729436874389648 | KNN Loss: 6.224222183227539 | BCE Loss: 1.0729436874389648\n",
      "Epoch 226 / 500 | iteration 5 / 30 | Total Loss: 1.0379345417022705 | KNN Loss: 6.224018573760986 | BCE Loss: 1.0379345417022705\n",
      "Epoch 226 / 500 | iteration 10 / 30 | Total Loss: 1.0370984077453613 | KNN Loss: 6.224757671356201 | BCE Loss: 1.0370984077453613\n",
      "Epoch 226 / 500 | iteration 15 / 30 | Total Loss: 1.0396311283111572 | KNN Loss: 6.224086284637451 | BCE Loss: 1.0396311283111572\n",
      "Epoch 226 / 500 | iteration 20 / 30 | Total Loss: 1.0300676822662354 | KNN Loss: 6.224384784698486 | BCE Loss: 1.0300676822662354\n",
      "Epoch 226 / 500 | iteration 25 / 30 | Total Loss: 1.053550362586975 | KNN Loss: 6.224621772766113 | BCE Loss: 1.053550362586975\n",
      "Epoch 227 / 500 | iteration 0 / 30 | Total Loss: 1.021918535232544 | KNN Loss: 6.224359512329102 | BCE Loss: 1.021918535232544\n",
      "Epoch 227 / 500 | iteration 5 / 30 | Total Loss: 1.0214787721633911 | KNN Loss: 6.224588394165039 | BCE Loss: 1.0214787721633911\n",
      "Epoch 227 / 500 | iteration 10 / 30 | Total Loss: 1.0488865375518799 | KNN Loss: 6.224191188812256 | BCE Loss: 1.0488865375518799\n",
      "Epoch 227 / 500 | iteration 15 / 30 | Total Loss: 1.0549037456512451 | KNN Loss: 6.224478721618652 | BCE Loss: 1.0549037456512451\n",
      "Epoch 227 / 500 | iteration 20 / 30 | Total Loss: 1.0523600578308105 | KNN Loss: 6.224725246429443 | BCE Loss: 1.0523600578308105\n",
      "Epoch 227 / 500 | iteration 25 / 30 | Total Loss: 1.0756103992462158 | KNN Loss: 6.224477291107178 | BCE Loss: 1.0756103992462158\n",
      "Epoch 228 / 500 | iteration 0 / 30 | Total Loss: 1.0480413436889648 | KNN Loss: 6.224606513977051 | BCE Loss: 1.0480413436889648\n",
      "Epoch 228 / 500 | iteration 5 / 30 | Total Loss: 1.0356311798095703 | KNN Loss: 6.224398136138916 | BCE Loss: 1.0356311798095703\n",
      "Epoch 228 / 500 | iteration 10 / 30 | Total Loss: 1.0778940916061401 | KNN Loss: 6.224331855773926 | BCE Loss: 1.0778940916061401\n",
      "Epoch 228 / 500 | iteration 15 / 30 | Total Loss: 1.0292627811431885 | KNN Loss: 6.224163055419922 | BCE Loss: 1.0292627811431885\n",
      "Epoch 228 / 500 | iteration 20 / 30 | Total Loss: 1.0611568689346313 | KNN Loss: 6.224158763885498 | BCE Loss: 1.0611568689346313\n",
      "Epoch 228 / 500 | iteration 25 / 30 | Total Loss: 1.0566602945327759 | KNN Loss: 6.224452972412109 | BCE Loss: 1.0566602945327759\n",
      "Epoch 229 / 500 | iteration 0 / 30 | Total Loss: 1.0484132766723633 | KNN Loss: 6.224417209625244 | BCE Loss: 1.0484132766723633\n",
      "Epoch 229 / 500 | iteration 5 / 30 | Total Loss: 1.0130436420440674 | KNN Loss: 6.224138259887695 | BCE Loss: 1.0130436420440674\n",
      "Epoch 229 / 500 | iteration 10 / 30 | Total Loss: 1.0754653215408325 | KNN Loss: 6.224341869354248 | BCE Loss: 1.0754653215408325\n",
      "Epoch 229 / 500 | iteration 15 / 30 | Total Loss: 1.0570006370544434 | KNN Loss: 6.224301815032959 | BCE Loss: 1.0570006370544434\n",
      "Epoch 229 / 500 | iteration 20 / 30 | Total Loss: 1.04825758934021 | KNN Loss: 6.224488735198975 | BCE Loss: 1.04825758934021\n",
      "Epoch 229 / 500 | iteration 25 / 30 | Total Loss: 1.0512303113937378 | KNN Loss: 6.224596977233887 | BCE Loss: 1.0512303113937378\n",
      "Epoch 230 / 500 | iteration 0 / 30 | Total Loss: 1.07826566696167 | KNN Loss: 6.224679946899414 | BCE Loss: 1.07826566696167\n",
      "Epoch 230 / 500 | iteration 5 / 30 | Total Loss: 1.0427820682525635 | KNN Loss: 6.224242687225342 | BCE Loss: 1.0427820682525635\n",
      "Epoch 230 / 500 | iteration 10 / 30 | Total Loss: 1.0599511861801147 | KNN Loss: 6.224143981933594 | BCE Loss: 1.0599511861801147\n",
      "Epoch 230 / 500 | iteration 15 / 30 | Total Loss: 1.0395303964614868 | KNN Loss: 6.224781036376953 | BCE Loss: 1.0395303964614868\n",
      "Epoch 230 / 500 | iteration 20 / 30 | Total Loss: 1.0803735256195068 | KNN Loss: 6.224691390991211 | BCE Loss: 1.0803735256195068\n",
      "Epoch 230 / 500 | iteration 25 / 30 | Total Loss: 1.065122127532959 | KNN Loss: 6.2246479988098145 | BCE Loss: 1.065122127532959\n",
      "Epoch 231 / 500 | iteration 0 / 30 | Total Loss: 1.0397590398788452 | KNN Loss: 6.224522590637207 | BCE Loss: 1.0397590398788452\n",
      "Epoch 231 / 500 | iteration 5 / 30 | Total Loss: 1.0478620529174805 | KNN Loss: 6.22471809387207 | BCE Loss: 1.0478620529174805\n",
      "Epoch 231 / 500 | iteration 10 / 30 | Total Loss: 1.0320067405700684 | KNN Loss: 6.224393367767334 | BCE Loss: 1.0320067405700684\n",
      "Epoch 231 / 500 | iteration 15 / 30 | Total Loss: 1.0774589776992798 | KNN Loss: 6.224975109100342 | BCE Loss: 1.0774589776992798\n",
      "Epoch 231 / 500 | iteration 20 / 30 | Total Loss: 1.020952582359314 | KNN Loss: 6.224482536315918 | BCE Loss: 1.020952582359314\n",
      "Epoch 231 / 500 | iteration 25 / 30 | Total Loss: 1.026136040687561 | KNN Loss: 6.224658966064453 | BCE Loss: 1.026136040687561\n",
      "Epoch 232 / 500 | iteration 0 / 30 | Total Loss: 1.0620920658111572 | KNN Loss: 6.223968505859375 | BCE Loss: 1.0620920658111572\n",
      "Epoch 232 / 500 | iteration 5 / 30 | Total Loss: 1.0316848754882812 | KNN Loss: 6.224399566650391 | BCE Loss: 1.0316848754882812\n",
      "Epoch 232 / 500 | iteration 10 / 30 | Total Loss: 1.0691547393798828 | KNN Loss: 6.224201679229736 | BCE Loss: 1.0691547393798828\n",
      "Epoch 232 / 500 | iteration 15 / 30 | Total Loss: 1.045148253440857 | KNN Loss: 6.224491596221924 | BCE Loss: 1.045148253440857\n",
      "Epoch 232 / 500 | iteration 20 / 30 | Total Loss: 1.0550508499145508 | KNN Loss: 6.224661350250244 | BCE Loss: 1.0550508499145508\n",
      "Epoch 232 / 500 | iteration 25 / 30 | Total Loss: 1.0504409074783325 | KNN Loss: 6.224669456481934 | BCE Loss: 1.0504409074783325\n",
      "Epoch   233: reducing learning rate of group 0 to 1.6616e-05.\n",
      "Epoch 233 / 500 | iteration 0 / 30 | Total Loss: 1.0666522979736328 | KNN Loss: 6.224721431732178 | BCE Loss: 1.0666522979736328\n",
      "Epoch 233 / 500 | iteration 5 / 30 | Total Loss: 1.064416527748108 | KNN Loss: 6.224697589874268 | BCE Loss: 1.064416527748108\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 233 / 500 | iteration 10 / 30 | Total Loss: 1.0416216850280762 | KNN Loss: 6.224234104156494 | BCE Loss: 1.0416216850280762\n",
      "Epoch 233 / 500 | iteration 15 / 30 | Total Loss: 1.051363468170166 | KNN Loss: 6.224732398986816 | BCE Loss: 1.051363468170166\n",
      "Epoch 233 / 500 | iteration 20 / 30 | Total Loss: 1.0091228485107422 | KNN Loss: 6.22451639175415 | BCE Loss: 1.0091228485107422\n",
      "Epoch 233 / 500 | iteration 25 / 30 | Total Loss: 1.049600601196289 | KNN Loss: 6.224287986755371 | BCE Loss: 1.049600601196289\n",
      "Epoch 234 / 500 | iteration 0 / 30 | Total Loss: 1.055942416191101 | KNN Loss: 6.224703311920166 | BCE Loss: 1.055942416191101\n",
      "Epoch 234 / 500 | iteration 5 / 30 | Total Loss: 1.0682318210601807 | KNN Loss: 6.224812984466553 | BCE Loss: 1.0682318210601807\n",
      "Epoch 234 / 500 | iteration 10 / 30 | Total Loss: 0.9930659532546997 | KNN Loss: 6.224452972412109 | BCE Loss: 0.9930659532546997\n",
      "Epoch 234 / 500 | iteration 15 / 30 | Total Loss: 1.0639606714248657 | KNN Loss: 6.224636554718018 | BCE Loss: 1.0639606714248657\n",
      "Epoch 234 / 500 | iteration 20 / 30 | Total Loss: 1.0589349269866943 | KNN Loss: 6.224637031555176 | BCE Loss: 1.0589349269866943\n",
      "Epoch 234 / 500 | iteration 25 / 30 | Total Loss: 1.0437946319580078 | KNN Loss: 6.224493503570557 | BCE Loss: 1.0437946319580078\n",
      "Epoch 235 / 500 | iteration 0 / 30 | Total Loss: 1.0386667251586914 | KNN Loss: 6.224419593811035 | BCE Loss: 1.0386667251586914\n",
      "Epoch 235 / 500 | iteration 5 / 30 | Total Loss: 1.0388376712799072 | KNN Loss: 6.224216938018799 | BCE Loss: 1.0388376712799072\n",
      "Epoch 235 / 500 | iteration 10 / 30 | Total Loss: 1.063999056816101 | KNN Loss: 6.2239556312561035 | BCE Loss: 1.063999056816101\n",
      "Epoch 235 / 500 | iteration 15 / 30 | Total Loss: 1.0592526197433472 | KNN Loss: 6.224323749542236 | BCE Loss: 1.0592526197433472\n",
      "Epoch 235 / 500 | iteration 20 / 30 | Total Loss: 1.0578393936157227 | KNN Loss: 6.224243640899658 | BCE Loss: 1.0578393936157227\n",
      "Epoch 235 / 500 | iteration 25 / 30 | Total Loss: 1.0369091033935547 | KNN Loss: 6.224233150482178 | BCE Loss: 1.0369091033935547\n",
      "Epoch 236 / 500 | iteration 0 / 30 | Total Loss: 1.0837609767913818 | KNN Loss: 6.224502086639404 | BCE Loss: 1.0837609767913818\n",
      "Epoch 236 / 500 | iteration 5 / 30 | Total Loss: 1.035370111465454 | KNN Loss: 6.225067138671875 | BCE Loss: 1.035370111465454\n",
      "Epoch 236 / 500 | iteration 10 / 30 | Total Loss: 1.0338339805603027 | KNN Loss: 6.224850177764893 | BCE Loss: 1.0338339805603027\n",
      "Epoch 236 / 500 | iteration 15 / 30 | Total Loss: 1.044346570968628 | KNN Loss: 6.224152088165283 | BCE Loss: 1.044346570968628\n",
      "Epoch 236 / 500 | iteration 20 / 30 | Total Loss: 1.0596356391906738 | KNN Loss: 6.224530220031738 | BCE Loss: 1.0596356391906738\n",
      "Epoch 236 / 500 | iteration 25 / 30 | Total Loss: 1.0512871742248535 | KNN Loss: 6.224442005157471 | BCE Loss: 1.0512871742248535\n",
      "Epoch 237 / 500 | iteration 0 / 30 | Total Loss: 1.0674529075622559 | KNN Loss: 6.224734306335449 | BCE Loss: 1.0674529075622559\n",
      "Epoch 237 / 500 | iteration 5 / 30 | Total Loss: 1.045719861984253 | KNN Loss: 6.224002838134766 | BCE Loss: 1.045719861984253\n",
      "Epoch 237 / 500 | iteration 10 / 30 | Total Loss: 1.044417142868042 | KNN Loss: 6.224510192871094 | BCE Loss: 1.044417142868042\n",
      "Epoch 237 / 500 | iteration 15 / 30 | Total Loss: 1.0610711574554443 | KNN Loss: 6.224556922912598 | BCE Loss: 1.0610711574554443\n",
      "Epoch 237 / 500 | iteration 20 / 30 | Total Loss: 1.090329885482788 | KNN Loss: 6.224511623382568 | BCE Loss: 1.090329885482788\n",
      "Epoch 237 / 500 | iteration 25 / 30 | Total Loss: 1.044416904449463 | KNN Loss: 6.224148750305176 | BCE Loss: 1.044416904449463\n",
      "Epoch 238 / 500 | iteration 0 / 30 | Total Loss: 1.0561153888702393 | KNN Loss: 6.2245330810546875 | BCE Loss: 1.0561153888702393\n",
      "Epoch 238 / 500 | iteration 5 / 30 | Total Loss: 1.038741946220398 | KNN Loss: 6.224546909332275 | BCE Loss: 1.038741946220398\n",
      "Epoch 238 / 500 | iteration 10 / 30 | Total Loss: 1.0446621179580688 | KNN Loss: 6.224630355834961 | BCE Loss: 1.0446621179580688\n",
      "Epoch 238 / 500 | iteration 15 / 30 | Total Loss: 1.0626251697540283 | KNN Loss: 6.224630832672119 | BCE Loss: 1.0626251697540283\n",
      "Epoch 238 / 500 | iteration 20 / 30 | Total Loss: 1.049892544746399 | KNN Loss: 6.2244720458984375 | BCE Loss: 1.049892544746399\n",
      "Epoch 238 / 500 | iteration 25 / 30 | Total Loss: 1.048196792602539 | KNN Loss: 6.224337577819824 | BCE Loss: 1.048196792602539\n",
      "Epoch 239 / 500 | iteration 0 / 30 | Total Loss: 1.0672881603240967 | KNN Loss: 6.224178791046143 | BCE Loss: 1.0672881603240967\n",
      "Epoch 239 / 500 | iteration 5 / 30 | Total Loss: 1.0629158020019531 | KNN Loss: 6.224602222442627 | BCE Loss: 1.0629158020019531\n",
      "Epoch 239 / 500 | iteration 10 / 30 | Total Loss: 1.0718581676483154 | KNN Loss: 6.224405765533447 | BCE Loss: 1.0718581676483154\n",
      "Epoch 239 / 500 | iteration 15 / 30 | Total Loss: 1.046877384185791 | KNN Loss: 6.22460412979126 | BCE Loss: 1.046877384185791\n",
      "Epoch 239 / 500 | iteration 20 / 30 | Total Loss: 1.0683003664016724 | KNN Loss: 6.224424362182617 | BCE Loss: 1.0683003664016724\n",
      "Epoch 239 / 500 | iteration 25 / 30 | Total Loss: 1.0457139015197754 | KNN Loss: 6.224587440490723 | BCE Loss: 1.0457139015197754\n",
      "Epoch 240 / 500 | iteration 0 / 30 | Total Loss: 1.0432406663894653 | KNN Loss: 6.224365711212158 | BCE Loss: 1.0432406663894653\n",
      "Epoch 240 / 500 | iteration 5 / 30 | Total Loss: 1.077361822128296 | KNN Loss: 6.224110126495361 | BCE Loss: 1.077361822128296\n",
      "Epoch 240 / 500 | iteration 10 / 30 | Total Loss: 1.076395034790039 | KNN Loss: 6.224549770355225 | BCE Loss: 1.076395034790039\n",
      "Epoch 240 / 500 | iteration 15 / 30 | Total Loss: 1.0781508684158325 | KNN Loss: 6.224393367767334 | BCE Loss: 1.0781508684158325\n",
      "Epoch 240 / 500 | iteration 20 / 30 | Total Loss: 1.054443120956421 | KNN Loss: 6.224483966827393 | BCE Loss: 1.054443120956421\n",
      "Epoch 240 / 500 | iteration 25 / 30 | Total Loss: 1.0665075778961182 | KNN Loss: 6.224837303161621 | BCE Loss: 1.0665075778961182\n",
      "Epoch 241 / 500 | iteration 0 / 30 | Total Loss: 1.0421640872955322 | KNN Loss: 6.2242631912231445 | BCE Loss: 1.0421640872955322\n",
      "Epoch 241 / 500 | iteration 5 / 30 | Total Loss: 1.0371949672698975 | KNN Loss: 6.224539279937744 | BCE Loss: 1.0371949672698975\n",
      "Epoch 241 / 500 | iteration 10 / 30 | Total Loss: 1.071434497833252 | KNN Loss: 6.22477912902832 | BCE Loss: 1.071434497833252\n",
      "Epoch 241 / 500 | iteration 15 / 30 | Total Loss: 1.0642832517623901 | KNN Loss: 6.224741458892822 | BCE Loss: 1.0642832517623901\n",
      "Epoch 241 / 500 | iteration 20 / 30 | Total Loss: 1.0555691719055176 | KNN Loss: 6.224275588989258 | BCE Loss: 1.0555691719055176\n",
      "Epoch 241 / 500 | iteration 25 / 30 | Total Loss: 1.0431280136108398 | KNN Loss: 6.224363327026367 | BCE Loss: 1.0431280136108398\n",
      "Epoch 242 / 500 | iteration 0 / 30 | Total Loss: 1.040215253829956 | KNN Loss: 6.224458694458008 | BCE Loss: 1.040215253829956\n",
      "Epoch 242 / 500 | iteration 5 / 30 | Total Loss: 1.0762720108032227 | KNN Loss: 6.224512577056885 | BCE Loss: 1.0762720108032227\n",
      "Epoch 242 / 500 | iteration 10 / 30 | Total Loss: 1.061288833618164 | KNN Loss: 6.224762916564941 | BCE Loss: 1.061288833618164\n",
      "Epoch 242 / 500 | iteration 15 / 30 | Total Loss: 1.0428948402404785 | KNN Loss: 6.224763870239258 | BCE Loss: 1.0428948402404785\n",
      "Epoch 242 / 500 | iteration 20 / 30 | Total Loss: 1.0627912282943726 | KNN Loss: 6.224123001098633 | BCE Loss: 1.0627912282943726\n",
      "Epoch 242 / 500 | iteration 25 / 30 | Total Loss: 1.034423589706421 | KNN Loss: 6.224384784698486 | BCE Loss: 1.034423589706421\n",
      "Epoch 243 / 500 | iteration 0 / 30 | Total Loss: 1.0672377347946167 | KNN Loss: 6.22452449798584 | BCE Loss: 1.0672377347946167\n",
      "Epoch 243 / 500 | iteration 5 / 30 | Total Loss: 1.0040699243545532 | KNN Loss: 6.224515438079834 | BCE Loss: 1.0040699243545532\n",
      "Epoch 243 / 500 | iteration 10 / 30 | Total Loss: 1.055080771446228 | KNN Loss: 6.22463846206665 | BCE Loss: 1.055080771446228\n",
      "Epoch 243 / 500 | iteration 15 / 30 | Total Loss: 1.0677618980407715 | KNN Loss: 6.224282264709473 | BCE Loss: 1.0677618980407715\n",
      "Epoch 243 / 500 | iteration 20 / 30 | Total Loss: 1.0759159326553345 | KNN Loss: 6.224653720855713 | BCE Loss: 1.0759159326553345\n",
      "Epoch 243 / 500 | iteration 25 / 30 | Total Loss: 1.0632755756378174 | KNN Loss: 6.224352836608887 | BCE Loss: 1.0632755756378174\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   244: reducing learning rate of group 0 to 1.1632e-05.\n",
      "Epoch 244 / 500 | iteration 0 / 30 | Total Loss: 1.0352814197540283 | KNN Loss: 6.224630355834961 | BCE Loss: 1.0352814197540283\n",
      "Epoch 244 / 500 | iteration 5 / 30 | Total Loss: 1.031891107559204 | KNN Loss: 6.2245354652404785 | BCE Loss: 1.031891107559204\n",
      "Epoch 244 / 500 | iteration 10 / 30 | Total Loss: 1.0435283184051514 | KNN Loss: 6.22435188293457 | BCE Loss: 1.0435283184051514\n",
      "Epoch 244 / 500 | iteration 15 / 30 | Total Loss: 1.056640863418579 | KNN Loss: 6.224337100982666 | BCE Loss: 1.056640863418579\n",
      "Epoch 244 / 500 | iteration 20 / 30 | Total Loss: 1.0300850868225098 | KNN Loss: 6.22428035736084 | BCE Loss: 1.0300850868225098\n",
      "Epoch 244 / 500 | iteration 25 / 30 | Total Loss: 1.0617735385894775 | KNN Loss: 6.224442958831787 | BCE Loss: 1.0617735385894775\n",
      "Epoch 245 / 500 | iteration 0 / 30 | Total Loss: 1.079470157623291 | KNN Loss: 6.224175453186035 | BCE Loss: 1.079470157623291\n",
      "Epoch 245 / 500 | iteration 5 / 30 | Total Loss: 1.0470060110092163 | KNN Loss: 6.224812984466553 | BCE Loss: 1.0470060110092163\n",
      "Epoch 245 / 500 | iteration 10 / 30 | Total Loss: 1.0526704788208008 | KNN Loss: 6.224255561828613 | BCE Loss: 1.0526704788208008\n",
      "Epoch 245 / 500 | iteration 15 / 30 | Total Loss: 1.0455503463745117 | KNN Loss: 6.224390983581543 | BCE Loss: 1.0455503463745117\n",
      "Epoch 245 / 500 | iteration 20 / 30 | Total Loss: 1.0544867515563965 | KNN Loss: 6.224742412567139 | BCE Loss: 1.0544867515563965\n",
      "Epoch 245 / 500 | iteration 25 / 30 | Total Loss: 1.0659523010253906 | KNN Loss: 6.224490165710449 | BCE Loss: 1.0659523010253906\n",
      "Epoch 246 / 500 | iteration 0 / 30 | Total Loss: 1.0847465991973877 | KNN Loss: 6.224433898925781 | BCE Loss: 1.0847465991973877\n",
      "Epoch 246 / 500 | iteration 5 / 30 | Total Loss: 1.0604418516159058 | KNN Loss: 6.224252223968506 | BCE Loss: 1.0604418516159058\n",
      "Epoch 246 / 500 | iteration 10 / 30 | Total Loss: 1.0668985843658447 | KNN Loss: 6.224362373352051 | BCE Loss: 1.0668985843658447\n",
      "Epoch 246 / 500 | iteration 15 / 30 | Total Loss: 1.0513670444488525 | KNN Loss: 6.224289894104004 | BCE Loss: 1.0513670444488525\n",
      "Epoch 246 / 500 | iteration 20 / 30 | Total Loss: 1.029280185699463 | KNN Loss: 6.224201202392578 | BCE Loss: 1.029280185699463\n",
      "Epoch 246 / 500 | iteration 25 / 30 | Total Loss: 1.0539238452911377 | KNN Loss: 6.224549293518066 | BCE Loss: 1.0539238452911377\n",
      "Epoch 247 / 500 | iteration 0 / 30 | Total Loss: 1.0139356851577759 | KNN Loss: 6.224589824676514 | BCE Loss: 1.0139356851577759\n",
      "Epoch 247 / 500 | iteration 5 / 30 | Total Loss: 1.0327072143554688 | KNN Loss: 6.2241082191467285 | BCE Loss: 1.0327072143554688\n",
      "Epoch 247 / 500 | iteration 10 / 30 | Total Loss: 1.0584943294525146 | KNN Loss: 6.224225044250488 | BCE Loss: 1.0584943294525146\n",
      "Epoch 247 / 500 | iteration 15 / 30 | Total Loss: 1.0501117706298828 | KNN Loss: 6.224765300750732 | BCE Loss: 1.0501117706298828\n",
      "Epoch 247 / 500 | iteration 20 / 30 | Total Loss: 1.0522414445877075 | KNN Loss: 6.22443962097168 | BCE Loss: 1.0522414445877075\n",
      "Epoch 247 / 500 | iteration 25 / 30 | Total Loss: 1.0295958518981934 | KNN Loss: 6.224391937255859 | BCE Loss: 1.0295958518981934\n",
      "Epoch 248 / 500 | iteration 0 / 30 | Total Loss: 1.0396603345870972 | KNN Loss: 6.224889755249023 | BCE Loss: 1.0396603345870972\n",
      "Epoch 248 / 500 | iteration 5 / 30 | Total Loss: 1.046299695968628 | KNN Loss: 6.224172592163086 | BCE Loss: 1.046299695968628\n",
      "Epoch 248 / 500 | iteration 10 / 30 | Total Loss: 1.0380851030349731 | KNN Loss: 6.224431037902832 | BCE Loss: 1.0380851030349731\n",
      "Epoch 248 / 500 | iteration 15 / 30 | Total Loss: 1.0148544311523438 | KNN Loss: 6.2247419357299805 | BCE Loss: 1.0148544311523438\n",
      "Epoch 248 / 500 | iteration 20 / 30 | Total Loss: 1.0269807577133179 | KNN Loss: 6.224599838256836 | BCE Loss: 1.0269807577133179\n",
      "Epoch 248 / 500 | iteration 25 / 30 | Total Loss: 1.0761889219284058 | KNN Loss: 6.22421932220459 | BCE Loss: 1.0761889219284058\n",
      "Epoch 249 / 500 | iteration 0 / 30 | Total Loss: 1.03817880153656 | KNN Loss: 6.224607944488525 | BCE Loss: 1.03817880153656\n",
      "Epoch 249 / 500 | iteration 5 / 30 | Total Loss: 1.0391418933868408 | KNN Loss: 6.224071502685547 | BCE Loss: 1.0391418933868408\n",
      "Epoch 249 / 500 | iteration 10 / 30 | Total Loss: 1.0174283981323242 | KNN Loss: 6.224637031555176 | BCE Loss: 1.0174283981323242\n",
      "Epoch 249 / 500 | iteration 15 / 30 | Total Loss: 1.0496344566345215 | KNN Loss: 6.224724292755127 | BCE Loss: 1.0496344566345215\n",
      "Epoch 249 / 500 | iteration 20 / 30 | Total Loss: 1.0538996458053589 | KNN Loss: 6.224687099456787 | BCE Loss: 1.0538996458053589\n",
      "Epoch 249 / 500 | iteration 25 / 30 | Total Loss: 1.0651291608810425 | KNN Loss: 6.224402904510498 | BCE Loss: 1.0651291608810425\n",
      "Epoch 250 / 500 | iteration 0 / 30 | Total Loss: 1.0619248151779175 | KNN Loss: 6.224652290344238 | BCE Loss: 1.0619248151779175\n",
      "Epoch 250 / 500 | iteration 5 / 30 | Total Loss: 1.0569982528686523 | KNN Loss: 6.2242326736450195 | BCE Loss: 1.0569982528686523\n",
      "Epoch 250 / 500 | iteration 10 / 30 | Total Loss: 1.0216277837753296 | KNN Loss: 6.224593162536621 | BCE Loss: 1.0216277837753296\n",
      "Epoch 250 / 500 | iteration 15 / 30 | Total Loss: 1.084479570388794 | KNN Loss: 6.224733352661133 | BCE Loss: 1.084479570388794\n",
      "Epoch 250 / 500 | iteration 20 / 30 | Total Loss: 1.057517647743225 | KNN Loss: 6.224254608154297 | BCE Loss: 1.057517647743225\n",
      "Epoch 250 / 500 | iteration 25 / 30 | Total Loss: 1.0799686908721924 | KNN Loss: 6.2243266105651855 | BCE Loss: 1.0799686908721924\n",
      "Epoch 251 / 500 | iteration 0 / 30 | Total Loss: 1.0480570793151855 | KNN Loss: 6.224691390991211 | BCE Loss: 1.0480570793151855\n",
      "Epoch 251 / 500 | iteration 5 / 30 | Total Loss: 1.051558017730713 | KNN Loss: 6.224338054656982 | BCE Loss: 1.051558017730713\n",
      "Epoch 251 / 500 | iteration 10 / 30 | Total Loss: 1.0665388107299805 | KNN Loss: 6.224285125732422 | BCE Loss: 1.0665388107299805\n",
      "Epoch 251 / 500 | iteration 15 / 30 | Total Loss: 1.0364097356796265 | KNN Loss: 6.224490165710449 | BCE Loss: 1.0364097356796265\n",
      "Epoch 251 / 500 | iteration 20 / 30 | Total Loss: 1.0397570133209229 | KNN Loss: 6.22466516494751 | BCE Loss: 1.0397570133209229\n",
      "Epoch 251 / 500 | iteration 25 / 30 | Total Loss: 1.053507924079895 | KNN Loss: 6.224344253540039 | BCE Loss: 1.053507924079895\n",
      "Epoch 252 / 500 | iteration 0 / 30 | Total Loss: 1.0543277263641357 | KNN Loss: 6.224232196807861 | BCE Loss: 1.0543277263641357\n",
      "Epoch 252 / 500 | iteration 5 / 30 | Total Loss: 1.0667288303375244 | KNN Loss: 6.224280834197998 | BCE Loss: 1.0667288303375244\n",
      "Epoch 252 / 500 | iteration 10 / 30 | Total Loss: 1.0541183948516846 | KNN Loss: 6.224430084228516 | BCE Loss: 1.0541183948516846\n",
      "Epoch 252 / 500 | iteration 15 / 30 | Total Loss: 1.069730281829834 | KNN Loss: 6.224848747253418 | BCE Loss: 1.069730281829834\n",
      "Epoch 252 / 500 | iteration 20 / 30 | Total Loss: 1.0759036540985107 | KNN Loss: 6.224480152130127 | BCE Loss: 1.0759036540985107\n",
      "Epoch 252 / 500 | iteration 25 / 30 | Total Loss: 1.0307451486587524 | KNN Loss: 6.224293231964111 | BCE Loss: 1.0307451486587524\n",
      "Epoch 253 / 500 | iteration 0 / 30 | Total Loss: 1.0342572927474976 | KNN Loss: 6.224234104156494 | BCE Loss: 1.0342572927474976\n",
      "Epoch 253 / 500 | iteration 5 / 30 | Total Loss: 1.0399606227874756 | KNN Loss: 6.224597454071045 | BCE Loss: 1.0399606227874756\n",
      "Epoch 253 / 500 | iteration 10 / 30 | Total Loss: 1.057725191116333 | KNN Loss: 6.224210739135742 | BCE Loss: 1.057725191116333\n",
      "Epoch 253 / 500 | iteration 15 / 30 | Total Loss: 1.023797869682312 | KNN Loss: 6.2244133949279785 | BCE Loss: 1.023797869682312\n",
      "Epoch 253 / 500 | iteration 20 / 30 | Total Loss: 1.0801464319229126 | KNN Loss: 6.2245354652404785 | BCE Loss: 1.0801464319229126\n",
      "Epoch 253 / 500 | iteration 25 / 30 | Total Loss: 1.0519695281982422 | KNN Loss: 6.224179744720459 | BCE Loss: 1.0519695281982422\n",
      "Epoch 254 / 500 | iteration 0 / 30 | Total Loss: 1.0379129648208618 | KNN Loss: 6.224311828613281 | BCE Loss: 1.0379129648208618\n",
      "Epoch 254 / 500 | iteration 5 / 30 | Total Loss: 1.0636590719223022 | KNN Loss: 6.223981857299805 | BCE Loss: 1.0636590719223022\n",
      "Epoch 254 / 500 | iteration 10 / 30 | Total Loss: 1.0336976051330566 | KNN Loss: 6.224745273590088 | BCE Loss: 1.0336976051330566\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 254 / 500 | iteration 15 / 30 | Total Loss: 1.0573482513427734 | KNN Loss: 6.224456310272217 | BCE Loss: 1.0573482513427734\n",
      "Epoch 254 / 500 | iteration 20 / 30 | Total Loss: 1.0567731857299805 | KNN Loss: 6.224266052246094 | BCE Loss: 1.0567731857299805\n",
      "Epoch 254 / 500 | iteration 25 / 30 | Total Loss: 1.073794960975647 | KNN Loss: 6.2245612144470215 | BCE Loss: 1.073794960975647\n",
      "Epoch 255 / 500 | iteration 0 / 30 | Total Loss: 1.0609829425811768 | KNN Loss: 6.224381446838379 | BCE Loss: 1.0609829425811768\n",
      "Epoch 255 / 500 | iteration 5 / 30 | Total Loss: 1.0225276947021484 | KNN Loss: 6.22417688369751 | BCE Loss: 1.0225276947021484\n",
      "Epoch 255 / 500 | iteration 10 / 30 | Total Loss: 1.0486847162246704 | KNN Loss: 6.224466800689697 | BCE Loss: 1.0486847162246704\n",
      "Epoch 255 / 500 | iteration 15 / 30 | Total Loss: 1.0587266683578491 | KNN Loss: 6.224488258361816 | BCE Loss: 1.0587266683578491\n",
      "Epoch 255 / 500 | iteration 20 / 30 | Total Loss: 1.0480461120605469 | KNN Loss: 6.224249362945557 | BCE Loss: 1.0480461120605469\n",
      "Epoch 255 / 500 | iteration 25 / 30 | Total Loss: 1.036880373954773 | KNN Loss: 6.224489688873291 | BCE Loss: 1.036880373954773\n",
      "Epoch 256 / 500 | iteration 0 / 30 | Total Loss: 1.0566775798797607 | KNN Loss: 6.224147319793701 | BCE Loss: 1.0566775798797607\n",
      "Epoch 256 / 500 | iteration 5 / 30 | Total Loss: 1.0565686225891113 | KNN Loss: 6.224344253540039 | BCE Loss: 1.0565686225891113\n",
      "Epoch 256 / 500 | iteration 10 / 30 | Total Loss: 1.0348305702209473 | KNN Loss: 6.224282741546631 | BCE Loss: 1.0348305702209473\n",
      "Epoch 256 / 500 | iteration 15 / 30 | Total Loss: 1.028240442276001 | KNN Loss: 6.224353790283203 | BCE Loss: 1.028240442276001\n",
      "Epoch 256 / 500 | iteration 20 / 30 | Total Loss: 1.0421652793884277 | KNN Loss: 6.224477767944336 | BCE Loss: 1.0421652793884277\n",
      "Epoch 256 / 500 | iteration 25 / 30 | Total Loss: 1.0706355571746826 | KNN Loss: 6.224431037902832 | BCE Loss: 1.0706355571746826\n",
      "Epoch 257 / 500 | iteration 0 / 30 | Total Loss: 1.044234275817871 | KNN Loss: 6.224485397338867 | BCE Loss: 1.044234275817871\n",
      "Epoch 257 / 500 | iteration 5 / 30 | Total Loss: 1.0333634614944458 | KNN Loss: 6.224544048309326 | BCE Loss: 1.0333634614944458\n",
      "Epoch 257 / 500 | iteration 10 / 30 | Total Loss: 1.061635971069336 | KNN Loss: 6.224371910095215 | BCE Loss: 1.061635971069336\n",
      "Epoch 257 / 500 | iteration 15 / 30 | Total Loss: 1.0577565431594849 | KNN Loss: 6.224618911743164 | BCE Loss: 1.0577565431594849\n",
      "Epoch 257 / 500 | iteration 20 / 30 | Total Loss: 1.073503851890564 | KNN Loss: 6.2246904373168945 | BCE Loss: 1.073503851890564\n",
      "Epoch 257 / 500 | iteration 25 / 30 | Total Loss: 1.038298487663269 | KNN Loss: 6.2242560386657715 | BCE Loss: 1.038298487663269\n",
      "Epoch   258: reducing learning rate of group 0 to 8.1421e-06.\n",
      "Epoch 258 / 500 | iteration 0 / 30 | Total Loss: 1.0135955810546875 | KNN Loss: 6.224465370178223 | BCE Loss: 1.0135955810546875\n",
      "Epoch 258 / 500 | iteration 5 / 30 | Total Loss: 1.043484091758728 | KNN Loss: 6.2245869636535645 | BCE Loss: 1.043484091758728\n",
      "Epoch 258 / 500 | iteration 10 / 30 | Total Loss: 1.048276424407959 | KNN Loss: 6.22478723526001 | BCE Loss: 1.048276424407959\n",
      "Epoch 258 / 500 | iteration 15 / 30 | Total Loss: 1.0566372871398926 | KNN Loss: 6.224292278289795 | BCE Loss: 1.0566372871398926\n",
      "Epoch 258 / 500 | iteration 20 / 30 | Total Loss: 1.0604877471923828 | KNN Loss: 6.224618911743164 | BCE Loss: 1.0604877471923828\n",
      "Epoch 258 / 500 | iteration 25 / 30 | Total Loss: 1.0685577392578125 | KNN Loss: 6.224256992340088 | BCE Loss: 1.0685577392578125\n",
      "Epoch 259 / 500 | iteration 0 / 30 | Total Loss: 1.0546486377716064 | KNN Loss: 6.223937511444092 | BCE Loss: 1.0546486377716064\n",
      "Epoch 259 / 500 | iteration 5 / 30 | Total Loss: 1.0212678909301758 | KNN Loss: 6.225025177001953 | BCE Loss: 1.0212678909301758\n",
      "Epoch 259 / 500 | iteration 10 / 30 | Total Loss: 1.063934564590454 | KNN Loss: 6.223796367645264 | BCE Loss: 1.063934564590454\n",
      "Epoch 259 / 500 | iteration 15 / 30 | Total Loss: 1.0436079502105713 | KNN Loss: 6.224313735961914 | BCE Loss: 1.0436079502105713\n",
      "Epoch 259 / 500 | iteration 20 / 30 | Total Loss: 1.0481572151184082 | KNN Loss: 6.224482536315918 | BCE Loss: 1.0481572151184082\n",
      "Epoch 259 / 500 | iteration 25 / 30 | Total Loss: 1.0397098064422607 | KNN Loss: 6.2240681648254395 | BCE Loss: 1.0397098064422607\n",
      "Epoch 260 / 500 | iteration 0 / 30 | Total Loss: 1.0725233554840088 | KNN Loss: 6.224541664123535 | BCE Loss: 1.0725233554840088\n",
      "Epoch 260 / 500 | iteration 5 / 30 | Total Loss: 1.0282135009765625 | KNN Loss: 6.2242865562438965 | BCE Loss: 1.0282135009765625\n",
      "Epoch 260 / 500 | iteration 10 / 30 | Total Loss: 1.0336530208587646 | KNN Loss: 6.224515914916992 | BCE Loss: 1.0336530208587646\n",
      "Epoch 260 / 500 | iteration 15 / 30 | Total Loss: 1.0518531799316406 | KNN Loss: 6.224423885345459 | BCE Loss: 1.0518531799316406\n",
      "Epoch 260 / 500 | iteration 20 / 30 | Total Loss: 1.049141526222229 | KNN Loss: 6.224560260772705 | BCE Loss: 1.049141526222229\n",
      "Epoch 260 / 500 | iteration 25 / 30 | Total Loss: 1.0699036121368408 | KNN Loss: 6.224735736846924 | BCE Loss: 1.0699036121368408\n",
      "Epoch 261 / 500 | iteration 0 / 30 | Total Loss: 1.0739974975585938 | KNN Loss: 6.224313259124756 | BCE Loss: 1.0739974975585938\n",
      "Epoch 261 / 500 | iteration 5 / 30 | Total Loss: 1.043588638305664 | KNN Loss: 6.224512100219727 | BCE Loss: 1.043588638305664\n",
      "Epoch 261 / 500 | iteration 10 / 30 | Total Loss: 1.041994333267212 | KNN Loss: 6.224493980407715 | BCE Loss: 1.041994333267212\n",
      "Epoch 261 / 500 | iteration 15 / 30 | Total Loss: 1.0439226627349854 | KNN Loss: 6.224458694458008 | BCE Loss: 1.0439226627349854\n",
      "Epoch 261 / 500 | iteration 20 / 30 | Total Loss: 1.0542123317718506 | KNN Loss: 6.224489212036133 | BCE Loss: 1.0542123317718506\n",
      "Epoch 261 / 500 | iteration 25 / 30 | Total Loss: 1.030555009841919 | KNN Loss: 6.224427223205566 | BCE Loss: 1.030555009841919\n",
      "Epoch 262 / 500 | iteration 0 / 30 | Total Loss: 1.0285718441009521 | KNN Loss: 6.224457740783691 | BCE Loss: 1.0285718441009521\n",
      "Epoch 262 / 500 | iteration 5 / 30 | Total Loss: 1.069122552871704 | KNN Loss: 6.2243547439575195 | BCE Loss: 1.069122552871704\n",
      "Epoch 262 / 500 | iteration 10 / 30 | Total Loss: 1.0802751779556274 | KNN Loss: 6.224736213684082 | BCE Loss: 1.0802751779556274\n",
      "Epoch 262 / 500 | iteration 15 / 30 | Total Loss: 1.063212275505066 | KNN Loss: 6.2239179611206055 | BCE Loss: 1.063212275505066\n",
      "Epoch 262 / 500 | iteration 20 / 30 | Total Loss: 1.0495805740356445 | KNN Loss: 6.224234580993652 | BCE Loss: 1.0495805740356445\n",
      "Epoch 262 / 500 | iteration 25 / 30 | Total Loss: 1.060166835784912 | KNN Loss: 6.224338054656982 | BCE Loss: 1.060166835784912\n",
      "Epoch 263 / 500 | iteration 0 / 30 | Total Loss: 1.050696611404419 | KNN Loss: 6.224368572235107 | BCE Loss: 1.050696611404419\n",
      "Epoch 263 / 500 | iteration 5 / 30 | Total Loss: 1.0357942581176758 | KNN Loss: 6.224335670471191 | BCE Loss: 1.0357942581176758\n",
      "Epoch 263 / 500 | iteration 10 / 30 | Total Loss: 1.0341291427612305 | KNN Loss: 6.224311828613281 | BCE Loss: 1.0341291427612305\n",
      "Epoch 263 / 500 | iteration 15 / 30 | Total Loss: 1.0320091247558594 | KNN Loss: 6.224767208099365 | BCE Loss: 1.0320091247558594\n",
      "Epoch 263 / 500 | iteration 20 / 30 | Total Loss: 1.023552417755127 | KNN Loss: 6.224063873291016 | BCE Loss: 1.023552417755127\n",
      "Epoch 263 / 500 | iteration 25 / 30 | Total Loss: 1.0545791387557983 | KNN Loss: 6.2243781089782715 | BCE Loss: 1.0545791387557983\n",
      "Epoch 264 / 500 | iteration 0 / 30 | Total Loss: 1.0229768753051758 | KNN Loss: 6.224151611328125 | BCE Loss: 1.0229768753051758\n",
      "Epoch 264 / 500 | iteration 5 / 30 | Total Loss: 1.058309555053711 | KNN Loss: 6.224517345428467 | BCE Loss: 1.058309555053711\n",
      "Epoch 264 / 500 | iteration 10 / 30 | Total Loss: 1.0406103134155273 | KNN Loss: 6.224049091339111 | BCE Loss: 1.0406103134155273\n",
      "Epoch 264 / 500 | iteration 15 / 30 | Total Loss: 1.045182228088379 | KNN Loss: 6.224335193634033 | BCE Loss: 1.045182228088379\n",
      "Epoch 264 / 500 | iteration 20 / 30 | Total Loss: 1.0624051094055176 | KNN Loss: 6.2245988845825195 | BCE Loss: 1.0624051094055176\n",
      "Epoch 264 / 500 | iteration 25 / 30 | Total Loss: 1.0586681365966797 | KNN Loss: 6.22434139251709 | BCE Loss: 1.0586681365966797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 265 / 500 | iteration 0 / 30 | Total Loss: 1.043273687362671 | KNN Loss: 6.2243733406066895 | BCE Loss: 1.043273687362671\n",
      "Epoch 265 / 500 | iteration 5 / 30 | Total Loss: 1.0259876251220703 | KNN Loss: 6.224399566650391 | BCE Loss: 1.0259876251220703\n",
      "Epoch 265 / 500 | iteration 10 / 30 | Total Loss: 1.0471856594085693 | KNN Loss: 6.224631309509277 | BCE Loss: 1.0471856594085693\n",
      "Epoch 265 / 500 | iteration 15 / 30 | Total Loss: 1.024215817451477 | KNN Loss: 6.2240376472473145 | BCE Loss: 1.024215817451477\n",
      "Epoch 265 / 500 | iteration 20 / 30 | Total Loss: 1.0617256164550781 | KNN Loss: 6.224730491638184 | BCE Loss: 1.0617256164550781\n",
      "Epoch 265 / 500 | iteration 25 / 30 | Total Loss: 1.0659470558166504 | KNN Loss: 6.224471092224121 | BCE Loss: 1.0659470558166504\n",
      "Epoch 266 / 500 | iteration 0 / 30 | Total Loss: 1.0728554725646973 | KNN Loss: 6.224879264831543 | BCE Loss: 1.0728554725646973\n",
      "Epoch 266 / 500 | iteration 5 / 30 | Total Loss: 1.0661587715148926 | KNN Loss: 6.224401473999023 | BCE Loss: 1.0661587715148926\n",
      "Epoch 266 / 500 | iteration 10 / 30 | Total Loss: 1.029848575592041 | KNN Loss: 6.224305152893066 | BCE Loss: 1.029848575592041\n",
      "Epoch 266 / 500 | iteration 15 / 30 | Total Loss: 1.0353248119354248 | KNN Loss: 6.2245097160339355 | BCE Loss: 1.0353248119354248\n",
      "Epoch 266 / 500 | iteration 20 / 30 | Total Loss: 1.016394853591919 | KNN Loss: 6.224598407745361 | BCE Loss: 1.016394853591919\n",
      "Epoch 266 / 500 | iteration 25 / 30 | Total Loss: 1.039605736732483 | KNN Loss: 6.2245869636535645 | BCE Loss: 1.039605736732483\n",
      "Epoch 267 / 500 | iteration 0 / 30 | Total Loss: 1.0474498271942139 | KNN Loss: 6.2247233390808105 | BCE Loss: 1.0474498271942139\n",
      "Epoch 267 / 500 | iteration 5 / 30 | Total Loss: 1.0362529754638672 | KNN Loss: 6.224524021148682 | BCE Loss: 1.0362529754638672\n",
      "Epoch 267 / 500 | iteration 10 / 30 | Total Loss: 1.057440161705017 | KNN Loss: 6.224318027496338 | BCE Loss: 1.057440161705017\n",
      "Epoch 267 / 500 | iteration 15 / 30 | Total Loss: 1.0686734914779663 | KNN Loss: 6.224459171295166 | BCE Loss: 1.0686734914779663\n",
      "Epoch 267 / 500 | iteration 20 / 30 | Total Loss: 1.0453914403915405 | KNN Loss: 6.224599838256836 | BCE Loss: 1.0453914403915405\n",
      "Epoch 267 / 500 | iteration 25 / 30 | Total Loss: 1.0556395053863525 | KNN Loss: 6.224639892578125 | BCE Loss: 1.0556395053863525\n",
      "Epoch 268 / 500 | iteration 0 / 30 | Total Loss: 1.0466160774230957 | KNN Loss: 6.224372386932373 | BCE Loss: 1.0466160774230957\n",
      "Epoch 268 / 500 | iteration 5 / 30 | Total Loss: 1.0242669582366943 | KNN Loss: 6.224574565887451 | BCE Loss: 1.0242669582366943\n",
      "Epoch 268 / 500 | iteration 10 / 30 | Total Loss: 1.0408556461334229 | KNN Loss: 6.224396705627441 | BCE Loss: 1.0408556461334229\n",
      "Epoch 268 / 500 | iteration 15 / 30 | Total Loss: 1.0776046514511108 | KNN Loss: 6.224542140960693 | BCE Loss: 1.0776046514511108\n",
      "Epoch 268 / 500 | iteration 20 / 30 | Total Loss: 1.0552537441253662 | KNN Loss: 6.224322319030762 | BCE Loss: 1.0552537441253662\n",
      "Epoch 268 / 500 | iteration 25 / 30 | Total Loss: 1.0540845394134521 | KNN Loss: 6.224435329437256 | BCE Loss: 1.0540845394134521\n",
      "Epoch   269: reducing learning rate of group 0 to 5.6994e-06.\n",
      "Epoch 269 / 500 | iteration 0 / 30 | Total Loss: 1.059253454208374 | KNN Loss: 6.224587440490723 | BCE Loss: 1.059253454208374\n",
      "Epoch 269 / 500 | iteration 5 / 30 | Total Loss: 1.0508663654327393 | KNN Loss: 6.224576950073242 | BCE Loss: 1.0508663654327393\n",
      "Epoch 269 / 500 | iteration 10 / 30 | Total Loss: 1.0913312435150146 | KNN Loss: 6.224514961242676 | BCE Loss: 1.0913312435150146\n",
      "Epoch 269 / 500 | iteration 15 / 30 | Total Loss: 1.0583443641662598 | KNN Loss: 6.224647521972656 | BCE Loss: 1.0583443641662598\n",
      "Epoch 269 / 500 | iteration 20 / 30 | Total Loss: 1.0548186302185059 | KNN Loss: 6.2245917320251465 | BCE Loss: 1.0548186302185059\n",
      "Epoch 269 / 500 | iteration 25 / 30 | Total Loss: 1.0648428201675415 | KNN Loss: 6.22458553314209 | BCE Loss: 1.0648428201675415\n",
      "Epoch 270 / 500 | iteration 0 / 30 | Total Loss: 1.0818411111831665 | KNN Loss: 6.224491596221924 | BCE Loss: 1.0818411111831665\n",
      "Epoch 270 / 500 | iteration 5 / 30 | Total Loss: 1.052599549293518 | KNN Loss: 6.224780559539795 | BCE Loss: 1.052599549293518\n",
      "Epoch 270 / 500 | iteration 10 / 30 | Total Loss: 1.0681642293930054 | KNN Loss: 6.224368095397949 | BCE Loss: 1.0681642293930054\n",
      "Epoch 270 / 500 | iteration 15 / 30 | Total Loss: 1.0471599102020264 | KNN Loss: 6.224489688873291 | BCE Loss: 1.0471599102020264\n",
      "Epoch 270 / 500 | iteration 20 / 30 | Total Loss: 1.0282338857650757 | KNN Loss: 6.224442958831787 | BCE Loss: 1.0282338857650757\n",
      "Epoch 270 / 500 | iteration 25 / 30 | Total Loss: 1.0121732950210571 | KNN Loss: 6.224517345428467 | BCE Loss: 1.0121732950210571\n",
      "Epoch 271 / 500 | iteration 0 / 30 | Total Loss: 1.0584015846252441 | KNN Loss: 6.224180221557617 | BCE Loss: 1.0584015846252441\n",
      "Epoch 271 / 500 | iteration 5 / 30 | Total Loss: 1.0722222328186035 | KNN Loss: 6.224301338195801 | BCE Loss: 1.0722222328186035\n",
      "Epoch 271 / 500 | iteration 10 / 30 | Total Loss: 1.0520381927490234 | KNN Loss: 6.224061489105225 | BCE Loss: 1.0520381927490234\n",
      "Epoch 271 / 500 | iteration 15 / 30 | Total Loss: 1.0458253622055054 | KNN Loss: 6.2241530418396 | BCE Loss: 1.0458253622055054\n",
      "Epoch 271 / 500 | iteration 20 / 30 | Total Loss: 1.0198582410812378 | KNN Loss: 6.2245192527771 | BCE Loss: 1.0198582410812378\n",
      "Epoch 271 / 500 | iteration 25 / 30 | Total Loss: 1.0922532081604004 | KNN Loss: 6.224528789520264 | BCE Loss: 1.0922532081604004\n",
      "Epoch 272 / 500 | iteration 0 / 30 | Total Loss: 1.0475070476531982 | KNN Loss: 6.224668025970459 | BCE Loss: 1.0475070476531982\n",
      "Epoch 272 / 500 | iteration 5 / 30 | Total Loss: 1.038100004196167 | KNN Loss: 6.224318504333496 | BCE Loss: 1.038100004196167\n",
      "Epoch 272 / 500 | iteration 10 / 30 | Total Loss: 1.0464251041412354 | KNN Loss: 6.224132537841797 | BCE Loss: 1.0464251041412354\n",
      "Epoch 272 / 500 | iteration 15 / 30 | Total Loss: 1.0510127544403076 | KNN Loss: 6.224578857421875 | BCE Loss: 1.0510127544403076\n",
      "Epoch 272 / 500 | iteration 20 / 30 | Total Loss: 1.072021484375 | KNN Loss: 6.224786758422852 | BCE Loss: 1.072021484375\n",
      "Epoch 272 / 500 | iteration 25 / 30 | Total Loss: 1.0341753959655762 | KNN Loss: 6.224598407745361 | BCE Loss: 1.0341753959655762\n",
      "Epoch 273 / 500 | iteration 0 / 30 | Total Loss: 1.0408532619476318 | KNN Loss: 6.224494457244873 | BCE Loss: 1.0408532619476318\n",
      "Epoch 273 / 500 | iteration 5 / 30 | Total Loss: 1.058290719985962 | KNN Loss: 6.224768161773682 | BCE Loss: 1.058290719985962\n",
      "Epoch 273 / 500 | iteration 10 / 30 | Total Loss: 1.0439718961715698 | KNN Loss: 6.224120616912842 | BCE Loss: 1.0439718961715698\n",
      "Epoch 273 / 500 | iteration 15 / 30 | Total Loss: 1.036094307899475 | KNN Loss: 6.224289894104004 | BCE Loss: 1.036094307899475\n",
      "Epoch 273 / 500 | iteration 20 / 30 | Total Loss: 1.0348879098892212 | KNN Loss: 6.2247209548950195 | BCE Loss: 1.0348879098892212\n",
      "Epoch 273 / 500 | iteration 25 / 30 | Total Loss: 1.0452210903167725 | KNN Loss: 6.224261283874512 | BCE Loss: 1.0452210903167725\n",
      "Epoch 274 / 500 | iteration 0 / 30 | Total Loss: 1.0619385242462158 | KNN Loss: 6.224702835083008 | BCE Loss: 1.0619385242462158\n",
      "Epoch 274 / 500 | iteration 5 / 30 | Total Loss: 1.0327140092849731 | KNN Loss: 6.224800109863281 | BCE Loss: 1.0327140092849731\n",
      "Epoch 274 / 500 | iteration 10 / 30 | Total Loss: 1.0342719554901123 | KNN Loss: 6.224074363708496 | BCE Loss: 1.0342719554901123\n",
      "Epoch 274 / 500 | iteration 15 / 30 | Total Loss: 1.0512568950653076 | KNN Loss: 6.224704265594482 | BCE Loss: 1.0512568950653076\n",
      "Epoch 274 / 500 | iteration 20 / 30 | Total Loss: 1.0438910722732544 | KNN Loss: 6.224321365356445 | BCE Loss: 1.0438910722732544\n",
      "Epoch 274 / 500 | iteration 25 / 30 | Total Loss: 1.0552582740783691 | KNN Loss: 6.223871231079102 | BCE Loss: 1.0552582740783691\n",
      "Epoch 275 / 500 | iteration 0 / 30 | Total Loss: 1.032046914100647 | KNN Loss: 6.224520683288574 | BCE Loss: 1.032046914100647\n",
      "Epoch 275 / 500 | iteration 5 / 30 | Total Loss: 1.0535751581192017 | KNN Loss: 6.224578380584717 | BCE Loss: 1.0535751581192017\n",
      "Epoch 275 / 500 | iteration 10 / 30 | Total Loss: 1.067169189453125 | KNN Loss: 6.22445821762085 | BCE Loss: 1.067169189453125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 275 / 500 | iteration 15 / 30 | Total Loss: 1.0248640775680542 | KNN Loss: 6.224447250366211 | BCE Loss: 1.0248640775680542\n",
      "Epoch 275 / 500 | iteration 20 / 30 | Total Loss: 1.0563808679580688 | KNN Loss: 6.224633693695068 | BCE Loss: 1.0563808679580688\n",
      "Epoch 275 / 500 | iteration 25 / 30 | Total Loss: 1.064805507659912 | KNN Loss: 6.224029064178467 | BCE Loss: 1.064805507659912\n",
      "Epoch 276 / 500 | iteration 0 / 30 | Total Loss: 1.0296330451965332 | KNN Loss: 6.224772930145264 | BCE Loss: 1.0296330451965332\n",
      "Epoch 276 / 500 | iteration 5 / 30 | Total Loss: 1.026968240737915 | KNN Loss: 6.224341869354248 | BCE Loss: 1.026968240737915\n",
      "Epoch 276 / 500 | iteration 10 / 30 | Total Loss: 1.03910493850708 | KNN Loss: 6.2240681648254395 | BCE Loss: 1.03910493850708\n",
      "Epoch 276 / 500 | iteration 15 / 30 | Total Loss: 1.0757595300674438 | KNN Loss: 6.224577903747559 | BCE Loss: 1.0757595300674438\n",
      "Epoch 276 / 500 | iteration 20 / 30 | Total Loss: 1.052459478378296 | KNN Loss: 6.224550247192383 | BCE Loss: 1.052459478378296\n",
      "Epoch 276 / 500 | iteration 25 / 30 | Total Loss: 1.0367861986160278 | KNN Loss: 6.22470760345459 | BCE Loss: 1.0367861986160278\n",
      "Epoch 277 / 500 | iteration 0 / 30 | Total Loss: 1.0305393934249878 | KNN Loss: 6.224039554595947 | BCE Loss: 1.0305393934249878\n",
      "Epoch 277 / 500 | iteration 5 / 30 | Total Loss: 1.0546514987945557 | KNN Loss: 6.224388122558594 | BCE Loss: 1.0546514987945557\n",
      "Epoch 277 / 500 | iteration 10 / 30 | Total Loss: 1.0702235698699951 | KNN Loss: 6.224697589874268 | BCE Loss: 1.0702235698699951\n",
      "Epoch 277 / 500 | iteration 15 / 30 | Total Loss: 1.0492768287658691 | KNN Loss: 6.224538803100586 | BCE Loss: 1.0492768287658691\n",
      "Epoch 277 / 500 | iteration 20 / 30 | Total Loss: 1.0646514892578125 | KNN Loss: 6.224340915679932 | BCE Loss: 1.0646514892578125\n",
      "Epoch 277 / 500 | iteration 25 / 30 | Total Loss: 1.0671498775482178 | KNN Loss: 6.224522113800049 | BCE Loss: 1.0671498775482178\n",
      "Epoch 278 / 500 | iteration 0 / 30 | Total Loss: 1.0780487060546875 | KNN Loss: 6.224106311798096 | BCE Loss: 1.0780487060546875\n",
      "Epoch 278 / 500 | iteration 5 / 30 | Total Loss: 1.0558385848999023 | KNN Loss: 6.224300384521484 | BCE Loss: 1.0558385848999023\n",
      "Epoch 278 / 500 | iteration 10 / 30 | Total Loss: 1.066886305809021 | KNN Loss: 6.224248886108398 | BCE Loss: 1.066886305809021\n",
      "Epoch 278 / 500 | iteration 15 / 30 | Total Loss: 1.0481915473937988 | KNN Loss: 6.224903106689453 | BCE Loss: 1.0481915473937988\n",
      "Epoch 278 / 500 | iteration 20 / 30 | Total Loss: 1.0597071647644043 | KNN Loss: 6.224348068237305 | BCE Loss: 1.0597071647644043\n",
      "Epoch 278 / 500 | iteration 25 / 30 | Total Loss: 1.021223783493042 | KNN Loss: 6.22453498840332 | BCE Loss: 1.021223783493042\n",
      "Epoch 279 / 500 | iteration 0 / 30 | Total Loss: 1.0712629556655884 | KNN Loss: 6.224674701690674 | BCE Loss: 1.0712629556655884\n",
      "Epoch 279 / 500 | iteration 5 / 30 | Total Loss: 1.0572017431259155 | KNN Loss: 6.224523067474365 | BCE Loss: 1.0572017431259155\n",
      "Epoch 279 / 500 | iteration 10 / 30 | Total Loss: 1.0593292713165283 | KNN Loss: 6.224818706512451 | BCE Loss: 1.0593292713165283\n",
      "Epoch 279 / 500 | iteration 15 / 30 | Total Loss: 1.0802218914031982 | KNN Loss: 6.224164962768555 | BCE Loss: 1.0802218914031982\n",
      "Epoch 279 / 500 | iteration 20 / 30 | Total Loss: 1.0258331298828125 | KNN Loss: 6.224715232849121 | BCE Loss: 1.0258331298828125\n",
      "Epoch 279 / 500 | iteration 25 / 30 | Total Loss: 1.0446451902389526 | KNN Loss: 6.224277496337891 | BCE Loss: 1.0446451902389526\n",
      "Epoch   280: reducing learning rate of group 0 to 3.9896e-06.\n",
      "Epoch 280 / 500 | iteration 0 / 30 | Total Loss: 1.0296494960784912 | KNN Loss: 6.224632263183594 | BCE Loss: 1.0296494960784912\n",
      "Epoch 280 / 500 | iteration 5 / 30 | Total Loss: 1.0498521327972412 | KNN Loss: 6.224776268005371 | BCE Loss: 1.0498521327972412\n",
      "Epoch 280 / 500 | iteration 10 / 30 | Total Loss: 1.0530369281768799 | KNN Loss: 6.224462985992432 | BCE Loss: 1.0530369281768799\n",
      "Epoch 280 / 500 | iteration 15 / 30 | Total Loss: 1.0356049537658691 | KNN Loss: 6.224377632141113 | BCE Loss: 1.0356049537658691\n",
      "Epoch 280 / 500 | iteration 20 / 30 | Total Loss: 1.0430347919464111 | KNN Loss: 6.2246246337890625 | BCE Loss: 1.0430347919464111\n",
      "Epoch 280 / 500 | iteration 25 / 30 | Total Loss: 1.0622150897979736 | KNN Loss: 6.224614143371582 | BCE Loss: 1.0622150897979736\n",
      "Epoch 281 / 500 | iteration 0 / 30 | Total Loss: 1.043455719947815 | KNN Loss: 6.224586009979248 | BCE Loss: 1.043455719947815\n",
      "Epoch 281 / 500 | iteration 5 / 30 | Total Loss: 1.06199312210083 | KNN Loss: 6.224356651306152 | BCE Loss: 1.06199312210083\n",
      "Epoch 281 / 500 | iteration 10 / 30 | Total Loss: 1.0631132125854492 | KNN Loss: 6.224587917327881 | BCE Loss: 1.0631132125854492\n",
      "Epoch 281 / 500 | iteration 15 / 30 | Total Loss: 1.044771671295166 | KNN Loss: 6.2246317863464355 | BCE Loss: 1.044771671295166\n",
      "Epoch 281 / 500 | iteration 20 / 30 | Total Loss: 1.068993330001831 | KNN Loss: 6.22470760345459 | BCE Loss: 1.068993330001831\n",
      "Epoch 281 / 500 | iteration 25 / 30 | Total Loss: 1.0402214527130127 | KNN Loss: 6.224465847015381 | BCE Loss: 1.0402214527130127\n",
      "Epoch 282 / 500 | iteration 0 / 30 | Total Loss: 1.0659575462341309 | KNN Loss: 6.224505424499512 | BCE Loss: 1.0659575462341309\n",
      "Epoch 282 / 500 | iteration 5 / 30 | Total Loss: 1.0388762950897217 | KNN Loss: 6.224486827850342 | BCE Loss: 1.0388762950897217\n",
      "Epoch 282 / 500 | iteration 10 / 30 | Total Loss: 1.0443663597106934 | KNN Loss: 6.224339008331299 | BCE Loss: 1.0443663597106934\n",
      "Epoch 282 / 500 | iteration 15 / 30 | Total Loss: 1.0829405784606934 | KNN Loss: 6.224428653717041 | BCE Loss: 1.0829405784606934\n",
      "Epoch 282 / 500 | iteration 20 / 30 | Total Loss: 1.05667245388031 | KNN Loss: 6.224532604217529 | BCE Loss: 1.05667245388031\n",
      "Epoch 282 / 500 | iteration 25 / 30 | Total Loss: 1.0273525714874268 | KNN Loss: 6.224415302276611 | BCE Loss: 1.0273525714874268\n",
      "Epoch 283 / 500 | iteration 0 / 30 | Total Loss: 1.0366511344909668 | KNN Loss: 6.224396228790283 | BCE Loss: 1.0366511344909668\n",
      "Epoch 283 / 500 | iteration 5 / 30 | Total Loss: 1.0273964405059814 | KNN Loss: 6.224342346191406 | BCE Loss: 1.0273964405059814\n",
      "Epoch 283 / 500 | iteration 10 / 30 | Total Loss: 1.054178237915039 | KNN Loss: 6.224606037139893 | BCE Loss: 1.054178237915039\n",
      "Epoch 283 / 500 | iteration 15 / 30 | Total Loss: 1.0342804193496704 | KNN Loss: 6.224693298339844 | BCE Loss: 1.0342804193496704\n",
      "Epoch 283 / 500 | iteration 20 / 30 | Total Loss: 1.0486878156661987 | KNN Loss: 6.224533557891846 | BCE Loss: 1.0486878156661987\n",
      "Epoch 283 / 500 | iteration 25 / 30 | Total Loss: 1.0839754343032837 | KNN Loss: 6.224780082702637 | BCE Loss: 1.0839754343032837\n",
      "Epoch 284 / 500 | iteration 0 / 30 | Total Loss: 1.0588918924331665 | KNN Loss: 6.22441291809082 | BCE Loss: 1.0588918924331665\n",
      "Epoch 284 / 500 | iteration 5 / 30 | Total Loss: 1.0759981870651245 | KNN Loss: 6.224277019500732 | BCE Loss: 1.0759981870651245\n",
      "Epoch 284 / 500 | iteration 10 / 30 | Total Loss: 1.042781949043274 | KNN Loss: 6.224404335021973 | BCE Loss: 1.042781949043274\n",
      "Epoch 284 / 500 | iteration 15 / 30 | Total Loss: 1.0696110725402832 | KNN Loss: 6.224640369415283 | BCE Loss: 1.0696110725402832\n",
      "Epoch 284 / 500 | iteration 20 / 30 | Total Loss: 1.0705997943878174 | KNN Loss: 6.224328994750977 | BCE Loss: 1.0705997943878174\n",
      "Epoch 284 / 500 | iteration 25 / 30 | Total Loss: 1.0629312992095947 | KNN Loss: 6.224445343017578 | BCE Loss: 1.0629312992095947\n",
      "Epoch 285 / 500 | iteration 0 / 30 | Total Loss: 1.0598866939544678 | KNN Loss: 6.224254608154297 | BCE Loss: 1.0598866939544678\n",
      "Epoch 285 / 500 | iteration 5 / 30 | Total Loss: 1.0672483444213867 | KNN Loss: 6.224675178527832 | BCE Loss: 1.0672483444213867\n",
      "Epoch 285 / 500 | iteration 10 / 30 | Total Loss: 1.0624608993530273 | KNN Loss: 6.224484920501709 | BCE Loss: 1.0624608993530273\n",
      "Epoch 285 / 500 | iteration 15 / 30 | Total Loss: 1.0602657794952393 | KNN Loss: 6.224226951599121 | BCE Loss: 1.0602657794952393\n",
      "Epoch 285 / 500 | iteration 20 / 30 | Total Loss: 1.060166597366333 | KNN Loss: 6.224273204803467 | BCE Loss: 1.060166597366333\n",
      "Epoch 285 / 500 | iteration 25 / 30 | Total Loss: 1.072390079498291 | KNN Loss: 6.224432468414307 | BCE Loss: 1.072390079498291\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 286 / 500 | iteration 0 / 30 | Total Loss: 1.0380558967590332 | KNN Loss: 6.22443151473999 | BCE Loss: 1.0380558967590332\n",
      "Epoch 286 / 500 | iteration 5 / 30 | Total Loss: 1.0746194124221802 | KNN Loss: 6.224595546722412 | BCE Loss: 1.0746194124221802\n",
      "Epoch 286 / 500 | iteration 10 / 30 | Total Loss: 1.0416345596313477 | KNN Loss: 6.224277496337891 | BCE Loss: 1.0416345596313477\n",
      "Epoch 286 / 500 | iteration 15 / 30 | Total Loss: 1.052182912826538 | KNN Loss: 6.224298477172852 | BCE Loss: 1.052182912826538\n",
      "Epoch 286 / 500 | iteration 20 / 30 | Total Loss: 1.071049690246582 | KNN Loss: 6.224433422088623 | BCE Loss: 1.071049690246582\n",
      "Epoch 286 / 500 | iteration 25 / 30 | Total Loss: 1.0638822317123413 | KNN Loss: 6.224143028259277 | BCE Loss: 1.0638822317123413\n",
      "Epoch 287 / 500 | iteration 0 / 30 | Total Loss: 1.0355387926101685 | KNN Loss: 6.224236488342285 | BCE Loss: 1.0355387926101685\n",
      "Epoch 287 / 500 | iteration 5 / 30 | Total Loss: 1.078221082687378 | KNN Loss: 6.224668502807617 | BCE Loss: 1.078221082687378\n",
      "Epoch 287 / 500 | iteration 10 / 30 | Total Loss: 1.037527322769165 | KNN Loss: 6.224299430847168 | BCE Loss: 1.037527322769165\n",
      "Epoch 287 / 500 | iteration 15 / 30 | Total Loss: 1.0496243238449097 | KNN Loss: 6.224777698516846 | BCE Loss: 1.0496243238449097\n",
      "Epoch 287 / 500 | iteration 20 / 30 | Total Loss: 1.1040621995925903 | KNN Loss: 6.224499702453613 | BCE Loss: 1.1040621995925903\n",
      "Epoch 287 / 500 | iteration 25 / 30 | Total Loss: 1.0563297271728516 | KNN Loss: 6.22448205947876 | BCE Loss: 1.0563297271728516\n",
      "Epoch 288 / 500 | iteration 0 / 30 | Total Loss: 1.0559370517730713 | KNN Loss: 6.2246012687683105 | BCE Loss: 1.0559370517730713\n",
      "Epoch 288 / 500 | iteration 5 / 30 | Total Loss: 1.0573054552078247 | KNN Loss: 6.22419548034668 | BCE Loss: 1.0573054552078247\n",
      "Epoch 288 / 500 | iteration 10 / 30 | Total Loss: 1.071866750717163 | KNN Loss: 6.22458028793335 | BCE Loss: 1.071866750717163\n",
      "Epoch 288 / 500 | iteration 15 / 30 | Total Loss: 1.0098814964294434 | KNN Loss: 6.224360942840576 | BCE Loss: 1.0098814964294434\n",
      "Epoch 288 / 500 | iteration 20 / 30 | Total Loss: 1.0508804321289062 | KNN Loss: 6.224584579467773 | BCE Loss: 1.0508804321289062\n",
      "Epoch 288 / 500 | iteration 25 / 30 | Total Loss: 1.0673623085021973 | KNN Loss: 6.224030494689941 | BCE Loss: 1.0673623085021973\n",
      "Epoch 289 / 500 | iteration 0 / 30 | Total Loss: 1.0294585227966309 | KNN Loss: 6.224263668060303 | BCE Loss: 1.0294585227966309\n",
      "Epoch 289 / 500 | iteration 5 / 30 | Total Loss: 1.055919885635376 | KNN Loss: 6.224515438079834 | BCE Loss: 1.055919885635376\n",
      "Epoch 289 / 500 | iteration 10 / 30 | Total Loss: 1.0547536611557007 | KNN Loss: 6.224555969238281 | BCE Loss: 1.0547536611557007\n",
      "Epoch 289 / 500 | iteration 15 / 30 | Total Loss: 1.0310778617858887 | KNN Loss: 6.224114418029785 | BCE Loss: 1.0310778617858887\n",
      "Epoch 289 / 500 | iteration 20 / 30 | Total Loss: 1.047400712966919 | KNN Loss: 6.224449634552002 | BCE Loss: 1.047400712966919\n",
      "Epoch 289 / 500 | iteration 25 / 30 | Total Loss: 1.0715110301971436 | KNN Loss: 6.224210739135742 | BCE Loss: 1.0715110301971436\n",
      "Epoch 290 / 500 | iteration 0 / 30 | Total Loss: 1.0293751955032349 | KNN Loss: 6.224725246429443 | BCE Loss: 1.0293751955032349\n",
      "Epoch 290 / 500 | iteration 5 / 30 | Total Loss: 1.0463597774505615 | KNN Loss: 6.224552154541016 | BCE Loss: 1.0463597774505615\n",
      "Epoch 290 / 500 | iteration 10 / 30 | Total Loss: 1.0334556102752686 | KNN Loss: 6.224525451660156 | BCE Loss: 1.0334556102752686\n",
      "Epoch 290 / 500 | iteration 15 / 30 | Total Loss: 1.0401318073272705 | KNN Loss: 6.224712371826172 | BCE Loss: 1.0401318073272705\n",
      "Epoch 290 / 500 | iteration 20 / 30 | Total Loss: 1.04136061668396 | KNN Loss: 6.224535942077637 | BCE Loss: 1.04136061668396\n",
      "Epoch 290 / 500 | iteration 25 / 30 | Total Loss: 1.067955732345581 | KNN Loss: 6.224215507507324 | BCE Loss: 1.067955732345581\n",
      "Epoch   291: reducing learning rate of group 0 to 2.7927e-06.\n",
      "Epoch 291 / 500 | iteration 0 / 30 | Total Loss: 1.058727502822876 | KNN Loss: 6.224349498748779 | BCE Loss: 1.058727502822876\n",
      "Epoch 291 / 500 | iteration 5 / 30 | Total Loss: 1.0217074155807495 | KNN Loss: 6.22475528717041 | BCE Loss: 1.0217074155807495\n",
      "Epoch 291 / 500 | iteration 10 / 30 | Total Loss: 1.0436909198760986 | KNN Loss: 6.224487781524658 | BCE Loss: 1.0436909198760986\n",
      "Epoch 291 / 500 | iteration 15 / 30 | Total Loss: 1.0521022081375122 | KNN Loss: 6.22477388381958 | BCE Loss: 1.0521022081375122\n",
      "Epoch 291 / 500 | iteration 20 / 30 | Total Loss: 1.0708930492401123 | KNN Loss: 6.2247467041015625 | BCE Loss: 1.0708930492401123\n",
      "Epoch 291 / 500 | iteration 25 / 30 | Total Loss: 1.0549871921539307 | KNN Loss: 6.224447250366211 | BCE Loss: 1.0549871921539307\n",
      "Epoch 292 / 500 | iteration 0 / 30 | Total Loss: 1.0366688966751099 | KNN Loss: 6.22427225112915 | BCE Loss: 1.0366688966751099\n",
      "Epoch 292 / 500 | iteration 5 / 30 | Total Loss: 1.0396943092346191 | KNN Loss: 6.224560260772705 | BCE Loss: 1.0396943092346191\n",
      "Epoch 292 / 500 | iteration 10 / 30 | Total Loss: 1.0502305030822754 | KNN Loss: 6.2247724533081055 | BCE Loss: 1.0502305030822754\n",
      "Epoch 292 / 500 | iteration 15 / 30 | Total Loss: 1.033519983291626 | KNN Loss: 6.224234104156494 | BCE Loss: 1.033519983291626\n",
      "Epoch 292 / 500 | iteration 20 / 30 | Total Loss: 1.0499136447906494 | KNN Loss: 6.224328994750977 | BCE Loss: 1.0499136447906494\n",
      "Epoch 292 / 500 | iteration 25 / 30 | Total Loss: 1.045032262802124 | KNN Loss: 6.224512577056885 | BCE Loss: 1.045032262802124\n",
      "Epoch 293 / 500 | iteration 0 / 30 | Total Loss: 1.0826668739318848 | KNN Loss: 6.224214553833008 | BCE Loss: 1.0826668739318848\n",
      "Epoch 293 / 500 | iteration 5 / 30 | Total Loss: 1.0635180473327637 | KNN Loss: 6.224730014801025 | BCE Loss: 1.0635180473327637\n",
      "Epoch 293 / 500 | iteration 10 / 30 | Total Loss: 1.0720775127410889 | KNN Loss: 6.224934101104736 | BCE Loss: 1.0720775127410889\n",
      "Epoch 293 / 500 | iteration 15 / 30 | Total Loss: 1.0648633241653442 | KNN Loss: 6.224618434906006 | BCE Loss: 1.0648633241653442\n",
      "Epoch 293 / 500 | iteration 20 / 30 | Total Loss: 1.0778722763061523 | KNN Loss: 6.224450588226318 | BCE Loss: 1.0778722763061523\n",
      "Epoch 293 / 500 | iteration 25 / 30 | Total Loss: 1.0502915382385254 | KNN Loss: 6.2246174812316895 | BCE Loss: 1.0502915382385254\n",
      "Epoch 294 / 500 | iteration 0 / 30 | Total Loss: 1.0234630107879639 | KNN Loss: 6.224085807800293 | BCE Loss: 1.0234630107879639\n",
      "Epoch 294 / 500 | iteration 5 / 30 | Total Loss: 1.0406622886657715 | KNN Loss: 6.224614143371582 | BCE Loss: 1.0406622886657715\n",
      "Epoch 294 / 500 | iteration 10 / 30 | Total Loss: 1.0695550441741943 | KNN Loss: 6.224352836608887 | BCE Loss: 1.0695550441741943\n",
      "Epoch 294 / 500 | iteration 15 / 30 | Total Loss: 1.064594030380249 | KNN Loss: 6.224355697631836 | BCE Loss: 1.064594030380249\n",
      "Epoch 294 / 500 | iteration 20 / 30 | Total Loss: 1.048527479171753 | KNN Loss: 6.224638938903809 | BCE Loss: 1.048527479171753\n",
      "Epoch 294 / 500 | iteration 25 / 30 | Total Loss: 1.0772955417633057 | KNN Loss: 6.224323749542236 | BCE Loss: 1.0772955417633057\n",
      "Epoch 295 / 500 | iteration 0 / 30 | Total Loss: 1.0803790092468262 | KNN Loss: 6.224302291870117 | BCE Loss: 1.0803790092468262\n",
      "Epoch 295 / 500 | iteration 5 / 30 | Total Loss: 1.070853590965271 | KNN Loss: 6.2241034507751465 | BCE Loss: 1.070853590965271\n",
      "Epoch 295 / 500 | iteration 10 / 30 | Total Loss: 1.0343343019485474 | KNN Loss: 6.224441051483154 | BCE Loss: 1.0343343019485474\n",
      "Epoch 295 / 500 | iteration 15 / 30 | Total Loss: 1.0410490036010742 | KNN Loss: 6.224576473236084 | BCE Loss: 1.0410490036010742\n",
      "Epoch 295 / 500 | iteration 20 / 30 | Total Loss: 1.0371007919311523 | KNN Loss: 6.224432945251465 | BCE Loss: 1.0371007919311523\n",
      "Epoch 295 / 500 | iteration 25 / 30 | Total Loss: 1.0618802309036255 | KNN Loss: 6.224507808685303 | BCE Loss: 1.0618802309036255\n",
      "Epoch 296 / 500 | iteration 0 / 30 | Total Loss: 1.0318747758865356 | KNN Loss: 6.224172592163086 | BCE Loss: 1.0318747758865356\n",
      "Epoch 296 / 500 | iteration 5 / 30 | Total Loss: 1.0629377365112305 | KNN Loss: 6.224531173706055 | BCE Loss: 1.0629377365112305\n",
      "Epoch 296 / 500 | iteration 10 / 30 | Total Loss: 1.0534884929656982 | KNN Loss: 6.22422456741333 | BCE Loss: 1.0534884929656982\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 296 / 500 | iteration 15 / 30 | Total Loss: 1.0803128480911255 | KNN Loss: 6.224167823791504 | BCE Loss: 1.0803128480911255\n",
      "Epoch 296 / 500 | iteration 20 / 30 | Total Loss: 1.0536699295043945 | KNN Loss: 6.224960803985596 | BCE Loss: 1.0536699295043945\n",
      "Epoch 296 / 500 | iteration 25 / 30 | Total Loss: 1.0311559438705444 | KNN Loss: 6.2247538566589355 | BCE Loss: 1.0311559438705444\n",
      "Epoch 297 / 500 | iteration 0 / 30 | Total Loss: 1.0729475021362305 | KNN Loss: 6.224485397338867 | BCE Loss: 1.0729475021362305\n",
      "Epoch 297 / 500 | iteration 5 / 30 | Total Loss: 1.0430800914764404 | KNN Loss: 6.224111080169678 | BCE Loss: 1.0430800914764404\n",
      "Epoch 297 / 500 | iteration 10 / 30 | Total Loss: 1.042130708694458 | KNN Loss: 6.224938869476318 | BCE Loss: 1.042130708694458\n",
      "Epoch 297 / 500 | iteration 15 / 30 | Total Loss: 1.0647881031036377 | KNN Loss: 6.224634647369385 | BCE Loss: 1.0647881031036377\n",
      "Epoch 297 / 500 | iteration 20 / 30 | Total Loss: 1.0589311122894287 | KNN Loss: 6.224695682525635 | BCE Loss: 1.0589311122894287\n",
      "Epoch 297 / 500 | iteration 25 / 30 | Total Loss: 1.0432250499725342 | KNN Loss: 6.224411964416504 | BCE Loss: 1.0432250499725342\n",
      "Epoch 298 / 500 | iteration 0 / 30 | Total Loss: 1.0276246070861816 | KNN Loss: 6.22466516494751 | BCE Loss: 1.0276246070861816\n",
      "Epoch 298 / 500 | iteration 5 / 30 | Total Loss: 1.0893529653549194 | KNN Loss: 6.224247455596924 | BCE Loss: 1.0893529653549194\n",
      "Epoch 298 / 500 | iteration 10 / 30 | Total Loss: 1.0726624727249146 | KNN Loss: 6.224704265594482 | BCE Loss: 1.0726624727249146\n",
      "Epoch 298 / 500 | iteration 15 / 30 | Total Loss: 1.0532078742980957 | KNN Loss: 6.2240777015686035 | BCE Loss: 1.0532078742980957\n",
      "Epoch 298 / 500 | iteration 20 / 30 | Total Loss: 1.0555660724639893 | KNN Loss: 6.224778652191162 | BCE Loss: 1.0555660724639893\n",
      "Epoch 298 / 500 | iteration 25 / 30 | Total Loss: 1.0559954643249512 | KNN Loss: 6.224328994750977 | BCE Loss: 1.0559954643249512\n",
      "Epoch 299 / 500 | iteration 0 / 30 | Total Loss: 1.0478800535202026 | KNN Loss: 6.2242431640625 | BCE Loss: 1.0478800535202026\n",
      "Epoch 299 / 500 | iteration 5 / 30 | Total Loss: 1.0421333312988281 | KNN Loss: 6.224360466003418 | BCE Loss: 1.0421333312988281\n",
      "Epoch 299 / 500 | iteration 10 / 30 | Total Loss: 1.093752145767212 | KNN Loss: 6.224454402923584 | BCE Loss: 1.093752145767212\n",
      "Epoch 299 / 500 | iteration 15 / 30 | Total Loss: 1.0415366888046265 | KNN Loss: 6.224676132202148 | BCE Loss: 1.0415366888046265\n",
      "Epoch 299 / 500 | iteration 20 / 30 | Total Loss: 1.0699430704116821 | KNN Loss: 6.2246012687683105 | BCE Loss: 1.0699430704116821\n",
      "Epoch 299 / 500 | iteration 25 / 30 | Total Loss: 1.0647797584533691 | KNN Loss: 6.22418212890625 | BCE Loss: 1.0647797584533691\n",
      "Epoch 300 / 500 | iteration 0 / 30 | Total Loss: 1.030117154121399 | KNN Loss: 6.22476863861084 | BCE Loss: 1.030117154121399\n",
      "Epoch 300 / 500 | iteration 5 / 30 | Total Loss: 1.0429198741912842 | KNN Loss: 6.22431755065918 | BCE Loss: 1.0429198741912842\n",
      "Epoch 300 / 500 | iteration 10 / 30 | Total Loss: 1.0855168104171753 | KNN Loss: 6.224278926849365 | BCE Loss: 1.0855168104171753\n",
      "Epoch 300 / 500 | iteration 15 / 30 | Total Loss: 1.053127408027649 | KNN Loss: 6.224379539489746 | BCE Loss: 1.053127408027649\n",
      "Epoch 300 / 500 | iteration 20 / 30 | Total Loss: 1.071678876876831 | KNN Loss: 6.224273681640625 | BCE Loss: 1.071678876876831\n",
      "Epoch 300 / 500 | iteration 25 / 30 | Total Loss: 1.066413402557373 | KNN Loss: 6.22412633895874 | BCE Loss: 1.066413402557373\n",
      "Epoch 301 / 500 | iteration 0 / 30 | Total Loss: 1.0718607902526855 | KNN Loss: 6.224503517150879 | BCE Loss: 1.0718607902526855\n",
      "Epoch 301 / 500 | iteration 5 / 30 | Total Loss: 1.0520223379135132 | KNN Loss: 6.224639892578125 | BCE Loss: 1.0520223379135132\n",
      "Epoch 301 / 500 | iteration 10 / 30 | Total Loss: 1.0392370223999023 | KNN Loss: 6.224442958831787 | BCE Loss: 1.0392370223999023\n",
      "Epoch 301 / 500 | iteration 15 / 30 | Total Loss: 1.0386855602264404 | KNN Loss: 6.224180221557617 | BCE Loss: 1.0386855602264404\n",
      "Epoch 301 / 500 | iteration 20 / 30 | Total Loss: 1.0910546779632568 | KNN Loss: 6.2245259284973145 | BCE Loss: 1.0910546779632568\n",
      "Epoch 301 / 500 | iteration 25 / 30 | Total Loss: 1.0580241680145264 | KNN Loss: 6.224668502807617 | BCE Loss: 1.0580241680145264\n",
      "Epoch   302: reducing learning rate of group 0 to 1.9549e-06.\n",
      "Epoch 302 / 500 | iteration 0 / 30 | Total Loss: 1.0580081939697266 | KNN Loss: 6.224421977996826 | BCE Loss: 1.0580081939697266\n",
      "Epoch 302 / 500 | iteration 5 / 30 | Total Loss: 1.0483007431030273 | KNN Loss: 6.224264621734619 | BCE Loss: 1.0483007431030273\n",
      "Epoch 302 / 500 | iteration 10 / 30 | Total Loss: 1.0665278434753418 | KNN Loss: 6.224358081817627 | BCE Loss: 1.0665278434753418\n",
      "Epoch 302 / 500 | iteration 15 / 30 | Total Loss: 1.0342950820922852 | KNN Loss: 6.2245774269104 | BCE Loss: 1.0342950820922852\n",
      "Epoch 302 / 500 | iteration 20 / 30 | Total Loss: 1.0563101768493652 | KNN Loss: 6.2246479988098145 | BCE Loss: 1.0563101768493652\n",
      "Epoch 302 / 500 | iteration 25 / 30 | Total Loss: 1.0793209075927734 | KNN Loss: 6.224516868591309 | BCE Loss: 1.0793209075927734\n",
      "Epoch 303 / 500 | iteration 0 / 30 | Total Loss: 1.0498859882354736 | KNN Loss: 6.224234104156494 | BCE Loss: 1.0498859882354736\n",
      "Epoch 303 / 500 | iteration 5 / 30 | Total Loss: 1.049691915512085 | KNN Loss: 6.224112510681152 | BCE Loss: 1.049691915512085\n",
      "Epoch 303 / 500 | iteration 10 / 30 | Total Loss: 1.037808895111084 | KNN Loss: 6.224377632141113 | BCE Loss: 1.037808895111084\n",
      "Epoch 303 / 500 | iteration 15 / 30 | Total Loss: 1.0673308372497559 | KNN Loss: 6.224587440490723 | BCE Loss: 1.0673308372497559\n",
      "Epoch 303 / 500 | iteration 20 / 30 | Total Loss: 1.0316963195800781 | KNN Loss: 6.224605560302734 | BCE Loss: 1.0316963195800781\n",
      "Epoch 303 / 500 | iteration 25 / 30 | Total Loss: 1.0395033359527588 | KNN Loss: 6.224491596221924 | BCE Loss: 1.0395033359527588\n",
      "Epoch 304 / 500 | iteration 0 / 30 | Total Loss: 1.0196387767791748 | KNN Loss: 6.224696636199951 | BCE Loss: 1.0196387767791748\n",
      "Epoch 304 / 500 | iteration 5 / 30 | Total Loss: 1.0872533321380615 | KNN Loss: 6.224102020263672 | BCE Loss: 1.0872533321380615\n",
      "Epoch 304 / 500 | iteration 10 / 30 | Total Loss: 1.0554895401000977 | KNN Loss: 6.224417686462402 | BCE Loss: 1.0554895401000977\n",
      "Epoch 304 / 500 | iteration 15 / 30 | Total Loss: 1.066831111907959 | KNN Loss: 6.224639415740967 | BCE Loss: 1.066831111907959\n",
      "Epoch 304 / 500 | iteration 20 / 30 | Total Loss: 1.0429060459136963 | KNN Loss: 6.224415302276611 | BCE Loss: 1.0429060459136963\n",
      "Epoch 304 / 500 | iteration 25 / 30 | Total Loss: 1.0335382223129272 | KNN Loss: 6.224371910095215 | BCE Loss: 1.0335382223129272\n",
      "Epoch 305 / 500 | iteration 0 / 30 | Total Loss: 1.051531195640564 | KNN Loss: 6.22457218170166 | BCE Loss: 1.051531195640564\n",
      "Epoch 305 / 500 | iteration 5 / 30 | Total Loss: 1.0380592346191406 | KNN Loss: 6.22441291809082 | BCE Loss: 1.0380592346191406\n",
      "Epoch 305 / 500 | iteration 10 / 30 | Total Loss: 1.0034099817276 | KNN Loss: 6.224785327911377 | BCE Loss: 1.0034099817276\n",
      "Epoch 305 / 500 | iteration 15 / 30 | Total Loss: 1.0326734781265259 | KNN Loss: 6.22428035736084 | BCE Loss: 1.0326734781265259\n",
      "Epoch 305 / 500 | iteration 20 / 30 | Total Loss: 1.023532509803772 | KNN Loss: 6.224384784698486 | BCE Loss: 1.023532509803772\n",
      "Epoch 305 / 500 | iteration 25 / 30 | Total Loss: 1.0452697277069092 | KNN Loss: 6.224181652069092 | BCE Loss: 1.0452697277069092\n",
      "Epoch 306 / 500 | iteration 0 / 30 | Total Loss: 1.051406741142273 | KNN Loss: 6.224493026733398 | BCE Loss: 1.051406741142273\n",
      "Epoch 306 / 500 | iteration 5 / 30 | Total Loss: 1.0697287321090698 | KNN Loss: 6.224515438079834 | BCE Loss: 1.0697287321090698\n",
      "Epoch 306 / 500 | iteration 10 / 30 | Total Loss: 1.052828073501587 | KNN Loss: 6.2247538566589355 | BCE Loss: 1.052828073501587\n",
      "Epoch 306 / 500 | iteration 15 / 30 | Total Loss: 1.079469919204712 | KNN Loss: 6.224774360656738 | BCE Loss: 1.079469919204712\n",
      "Epoch 306 / 500 | iteration 20 / 30 | Total Loss: 1.0179798603057861 | KNN Loss: 6.224132061004639 | BCE Loss: 1.0179798603057861\n",
      "Epoch 306 / 500 | iteration 25 / 30 | Total Loss: 1.0324912071228027 | KNN Loss: 6.224620819091797 | BCE Loss: 1.0324912071228027\n",
      "Epoch 307 / 500 | iteration 0 / 30 | Total Loss: 1.0438199043273926 | KNN Loss: 6.224421977996826 | BCE Loss: 1.0438199043273926\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 307 / 500 | iteration 5 / 30 | Total Loss: 1.0271050930023193 | KNN Loss: 6.224339008331299 | BCE Loss: 1.0271050930023193\n",
      "Epoch 307 / 500 | iteration 10 / 30 | Total Loss: 1.041398525238037 | KNN Loss: 6.22435188293457 | BCE Loss: 1.041398525238037\n",
      "Epoch 307 / 500 | iteration 15 / 30 | Total Loss: 1.0410473346710205 | KNN Loss: 6.224340438842773 | BCE Loss: 1.0410473346710205\n",
      "Epoch 307 / 500 | iteration 20 / 30 | Total Loss: 1.061018943786621 | KNN Loss: 6.224172115325928 | BCE Loss: 1.061018943786621\n",
      "Epoch 307 / 500 | iteration 25 / 30 | Total Loss: 1.0444198846817017 | KNN Loss: 6.224419116973877 | BCE Loss: 1.0444198846817017\n",
      "Epoch 308 / 500 | iteration 0 / 30 | Total Loss: 1.0373451709747314 | KNN Loss: 6.2242536544799805 | BCE Loss: 1.0373451709747314\n",
      "Epoch 308 / 500 | iteration 5 / 30 | Total Loss: 1.091111660003662 | KNN Loss: 6.224403381347656 | BCE Loss: 1.091111660003662\n",
      "Epoch 308 / 500 | iteration 10 / 30 | Total Loss: 1.0396782159805298 | KNN Loss: 6.2241010665893555 | BCE Loss: 1.0396782159805298\n",
      "Epoch 308 / 500 | iteration 15 / 30 | Total Loss: 1.0585633516311646 | KNN Loss: 6.2243757247924805 | BCE Loss: 1.0585633516311646\n",
      "Epoch 308 / 500 | iteration 20 / 30 | Total Loss: 1.0366183519363403 | KNN Loss: 6.224730014801025 | BCE Loss: 1.0366183519363403\n",
      "Epoch 308 / 500 | iteration 25 / 30 | Total Loss: 1.0253984928131104 | KNN Loss: 6.224462509155273 | BCE Loss: 1.0253984928131104\n",
      "Epoch 309 / 500 | iteration 0 / 30 | Total Loss: 1.0445327758789062 | KNN Loss: 6.224198341369629 | BCE Loss: 1.0445327758789062\n",
      "Epoch 309 / 500 | iteration 5 / 30 | Total Loss: 1.06355881690979 | KNN Loss: 6.2244367599487305 | BCE Loss: 1.06355881690979\n",
      "Epoch 309 / 500 | iteration 10 / 30 | Total Loss: 1.1063120365142822 | KNN Loss: 6.2244133949279785 | BCE Loss: 1.1063120365142822\n",
      "Epoch 309 / 500 | iteration 15 / 30 | Total Loss: 1.0574126243591309 | KNN Loss: 6.224281311035156 | BCE Loss: 1.0574126243591309\n",
      "Epoch 309 / 500 | iteration 20 / 30 | Total Loss: 1.0740172863006592 | KNN Loss: 6.224277973175049 | BCE Loss: 1.0740172863006592\n",
      "Epoch 309 / 500 | iteration 25 / 30 | Total Loss: 1.0394773483276367 | KNN Loss: 6.224295139312744 | BCE Loss: 1.0394773483276367\n",
      "Epoch 310 / 500 | iteration 0 / 30 | Total Loss: 1.0434937477111816 | KNN Loss: 6.224551677703857 | BCE Loss: 1.0434937477111816\n",
      "Epoch 310 / 500 | iteration 5 / 30 | Total Loss: 1.0537023544311523 | KNN Loss: 6.224562168121338 | BCE Loss: 1.0537023544311523\n",
      "Epoch 310 / 500 | iteration 10 / 30 | Total Loss: 1.0589702129364014 | KNN Loss: 6.224476337432861 | BCE Loss: 1.0589702129364014\n",
      "Epoch 310 / 500 | iteration 15 / 30 | Total Loss: 1.060433268547058 | KNN Loss: 6.224684238433838 | BCE Loss: 1.060433268547058\n",
      "Epoch 310 / 500 | iteration 20 / 30 | Total Loss: 1.0446786880493164 | KNN Loss: 6.2241973876953125 | BCE Loss: 1.0446786880493164\n",
      "Epoch 310 / 500 | iteration 25 / 30 | Total Loss: 1.0588817596435547 | KNN Loss: 6.2244696617126465 | BCE Loss: 1.0588817596435547\n",
      "Epoch 311 / 500 | iteration 0 / 30 | Total Loss: 1.0535857677459717 | KNN Loss: 6.224734783172607 | BCE Loss: 1.0535857677459717\n",
      "Epoch 311 / 500 | iteration 5 / 30 | Total Loss: 1.0503476858139038 | KNN Loss: 6.22432804107666 | BCE Loss: 1.0503476858139038\n",
      "Epoch 311 / 500 | iteration 10 / 30 | Total Loss: 1.0798368453979492 | KNN Loss: 6.224579334259033 | BCE Loss: 1.0798368453979492\n",
      "Epoch 311 / 500 | iteration 15 / 30 | Total Loss: 1.0601327419281006 | KNN Loss: 6.224562168121338 | BCE Loss: 1.0601327419281006\n",
      "Epoch 311 / 500 | iteration 20 / 30 | Total Loss: 1.0474779605865479 | KNN Loss: 6.224632263183594 | BCE Loss: 1.0474779605865479\n",
      "Epoch 311 / 500 | iteration 25 / 30 | Total Loss: 1.0297486782073975 | KNN Loss: 6.224339962005615 | BCE Loss: 1.0297486782073975\n",
      "Epoch 312 / 500 | iteration 0 / 30 | Total Loss: 1.0838199853897095 | KNN Loss: 6.224596977233887 | BCE Loss: 1.0838199853897095\n",
      "Epoch 312 / 500 | iteration 5 / 30 | Total Loss: 1.0394518375396729 | KNN Loss: 6.224681377410889 | BCE Loss: 1.0394518375396729\n",
      "Epoch 312 / 500 | iteration 10 / 30 | Total Loss: 1.0423318147659302 | KNN Loss: 6.224289894104004 | BCE Loss: 1.0423318147659302\n",
      "Epoch 312 / 500 | iteration 15 / 30 | Total Loss: 1.0302362442016602 | KNN Loss: 6.22429895401001 | BCE Loss: 1.0302362442016602\n",
      "Epoch 312 / 500 | iteration 20 / 30 | Total Loss: 1.097825050354004 | KNN Loss: 6.224583625793457 | BCE Loss: 1.097825050354004\n",
      "Epoch 312 / 500 | iteration 25 / 30 | Total Loss: 1.0406866073608398 | KNN Loss: 6.224569797515869 | BCE Loss: 1.0406866073608398\n",
      "Epoch   313: reducing learning rate of group 0 to 1.3684e-06.\n",
      "Epoch 313 / 500 | iteration 0 / 30 | Total Loss: 1.0532042980194092 | KNN Loss: 6.224440574645996 | BCE Loss: 1.0532042980194092\n",
      "Epoch 313 / 500 | iteration 5 / 30 | Total Loss: 1.0312042236328125 | KNN Loss: 6.224730014801025 | BCE Loss: 1.0312042236328125\n",
      "Epoch 313 / 500 | iteration 10 / 30 | Total Loss: 1.0693223476409912 | KNN Loss: 6.224457263946533 | BCE Loss: 1.0693223476409912\n",
      "Epoch 313 / 500 | iteration 15 / 30 | Total Loss: 1.0390148162841797 | KNN Loss: 6.224051475524902 | BCE Loss: 1.0390148162841797\n",
      "Epoch 313 / 500 | iteration 20 / 30 | Total Loss: 1.0668047666549683 | KNN Loss: 6.224580764770508 | BCE Loss: 1.0668047666549683\n",
      "Epoch 313 / 500 | iteration 25 / 30 | Total Loss: 1.0445955991744995 | KNN Loss: 6.223918914794922 | BCE Loss: 1.0445955991744995\n",
      "Epoch 314 / 500 | iteration 0 / 30 | Total Loss: 1.0683172941207886 | KNN Loss: 6.224508285522461 | BCE Loss: 1.0683172941207886\n",
      "Epoch 314 / 500 | iteration 5 / 30 | Total Loss: 1.0373833179473877 | KNN Loss: 6.224140167236328 | BCE Loss: 1.0373833179473877\n",
      "Epoch 314 / 500 | iteration 10 / 30 | Total Loss: 1.0643160343170166 | KNN Loss: 6.2245049476623535 | BCE Loss: 1.0643160343170166\n",
      "Epoch 314 / 500 | iteration 15 / 30 | Total Loss: 1.0532386302947998 | KNN Loss: 6.224301815032959 | BCE Loss: 1.0532386302947998\n",
      "Epoch 314 / 500 | iteration 20 / 30 | Total Loss: 1.0640214681625366 | KNN Loss: 6.22407865524292 | BCE Loss: 1.0640214681625366\n",
      "Epoch 314 / 500 | iteration 25 / 30 | Total Loss: 1.0322136878967285 | KNN Loss: 6.224774360656738 | BCE Loss: 1.0322136878967285\n",
      "Epoch 315 / 500 | iteration 0 / 30 | Total Loss: 1.0616676807403564 | KNN Loss: 6.224781513214111 | BCE Loss: 1.0616676807403564\n",
      "Epoch 315 / 500 | iteration 5 / 30 | Total Loss: 1.042754888534546 | KNN Loss: 6.224314212799072 | BCE Loss: 1.042754888534546\n",
      "Epoch 315 / 500 | iteration 10 / 30 | Total Loss: 1.0704810619354248 | KNN Loss: 6.224313735961914 | BCE Loss: 1.0704810619354248\n",
      "Epoch 315 / 500 | iteration 15 / 30 | Total Loss: 1.0525500774383545 | KNN Loss: 6.224616050720215 | BCE Loss: 1.0525500774383545\n",
      "Epoch 315 / 500 | iteration 20 / 30 | Total Loss: 1.0599026679992676 | KNN Loss: 6.224267959594727 | BCE Loss: 1.0599026679992676\n",
      "Epoch 315 / 500 | iteration 25 / 30 | Total Loss: 1.0489286184310913 | KNN Loss: 6.224769592285156 | BCE Loss: 1.0489286184310913\n",
      "Epoch 316 / 500 | iteration 0 / 30 | Total Loss: 1.0403308868408203 | KNN Loss: 6.224823951721191 | BCE Loss: 1.0403308868408203\n",
      "Epoch 316 / 500 | iteration 5 / 30 | Total Loss: 1.0799299478530884 | KNN Loss: 6.22448205947876 | BCE Loss: 1.0799299478530884\n",
      "Epoch 316 / 500 | iteration 10 / 30 | Total Loss: 1.029527187347412 | KNN Loss: 6.224570274353027 | BCE Loss: 1.029527187347412\n",
      "Epoch 316 / 500 | iteration 15 / 30 | Total Loss: 1.0438867807388306 | KNN Loss: 6.224380970001221 | BCE Loss: 1.0438867807388306\n",
      "Epoch 316 / 500 | iteration 20 / 30 | Total Loss: 1.049511194229126 | KNN Loss: 6.224626541137695 | BCE Loss: 1.049511194229126\n",
      "Epoch 316 / 500 | iteration 25 / 30 | Total Loss: 1.0463924407958984 | KNN Loss: 6.225018501281738 | BCE Loss: 1.0463924407958984\n",
      "Epoch 317 / 500 | iteration 0 / 30 | Total Loss: 1.061894178390503 | KNN Loss: 6.224592208862305 | BCE Loss: 1.061894178390503\n",
      "Epoch 317 / 500 | iteration 5 / 30 | Total Loss: 1.0513921976089478 | KNN Loss: 6.224751949310303 | BCE Loss: 1.0513921976089478\n",
      "Epoch 317 / 500 | iteration 10 / 30 | Total Loss: 1.0391091108322144 | KNN Loss: 6.224119186401367 | BCE Loss: 1.0391091108322144\n",
      "Epoch 317 / 500 | iteration 15 / 30 | Total Loss: 1.0411019325256348 | KNN Loss: 6.2241621017456055 | BCE Loss: 1.0411019325256348\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 317 / 500 | iteration 20 / 30 | Total Loss: 1.0304913520812988 | KNN Loss: 6.2244086265563965 | BCE Loss: 1.0304913520812988\n",
      "Epoch 317 / 500 | iteration 25 / 30 | Total Loss: 1.022512674331665 | KNN Loss: 6.224454402923584 | BCE Loss: 1.022512674331665\n",
      "Epoch 318 / 500 | iteration 0 / 30 | Total Loss: 1.042155146598816 | KNN Loss: 6.224437713623047 | BCE Loss: 1.042155146598816\n",
      "Epoch 318 / 500 | iteration 5 / 30 | Total Loss: 1.051027774810791 | KNN Loss: 6.224869728088379 | BCE Loss: 1.051027774810791\n",
      "Epoch 318 / 500 | iteration 10 / 30 | Total Loss: 1.1020526885986328 | KNN Loss: 6.224310874938965 | BCE Loss: 1.1020526885986328\n",
      "Epoch 318 / 500 | iteration 15 / 30 | Total Loss: 1.0528674125671387 | KNN Loss: 6.2241740226745605 | BCE Loss: 1.0528674125671387\n",
      "Epoch 318 / 500 | iteration 20 / 30 | Total Loss: 1.0457926988601685 | KNN Loss: 6.224430561065674 | BCE Loss: 1.0457926988601685\n",
      "Epoch 318 / 500 | iteration 25 / 30 | Total Loss: 1.0376588106155396 | KNN Loss: 6.224541187286377 | BCE Loss: 1.0376588106155396\n",
      "Epoch 319 / 500 | iteration 0 / 30 | Total Loss: 1.0333969593048096 | KNN Loss: 6.224698066711426 | BCE Loss: 1.0333969593048096\n",
      "Epoch 319 / 500 | iteration 5 / 30 | Total Loss: 1.0819543600082397 | KNN Loss: 6.224020957946777 | BCE Loss: 1.0819543600082397\n",
      "Epoch 319 / 500 | iteration 10 / 30 | Total Loss: 1.047417402267456 | KNN Loss: 6.2242560386657715 | BCE Loss: 1.047417402267456\n",
      "Epoch 319 / 500 | iteration 15 / 30 | Total Loss: 1.0476043224334717 | KNN Loss: 6.22422456741333 | BCE Loss: 1.0476043224334717\n",
      "Epoch 319 / 500 | iteration 20 / 30 | Total Loss: 1.0648200511932373 | KNN Loss: 6.2241997718811035 | BCE Loss: 1.0648200511932373\n",
      "Epoch 319 / 500 | iteration 25 / 30 | Total Loss: 1.053885579109192 | KNN Loss: 6.224127292633057 | BCE Loss: 1.053885579109192\n",
      "Epoch 320 / 500 | iteration 0 / 30 | Total Loss: 1.0620914697647095 | KNN Loss: 6.224516868591309 | BCE Loss: 1.0620914697647095\n",
      "Epoch 320 / 500 | iteration 5 / 30 | Total Loss: 1.0414366722106934 | KNN Loss: 6.224499702453613 | BCE Loss: 1.0414366722106934\n",
      "Epoch 320 / 500 | iteration 10 / 30 | Total Loss: 1.058616042137146 | KNN Loss: 6.2248311042785645 | BCE Loss: 1.058616042137146\n",
      "Epoch 320 / 500 | iteration 15 / 30 | Total Loss: 1.0334415435791016 | KNN Loss: 6.224470615386963 | BCE Loss: 1.0334415435791016\n",
      "Epoch 320 / 500 | iteration 20 / 30 | Total Loss: 1.0299080610275269 | KNN Loss: 6.224462509155273 | BCE Loss: 1.0299080610275269\n",
      "Epoch 320 / 500 | iteration 25 / 30 | Total Loss: 1.0631353855133057 | KNN Loss: 6.224217891693115 | BCE Loss: 1.0631353855133057\n",
      "Epoch 321 / 500 | iteration 0 / 30 | Total Loss: 1.0303682088851929 | KNN Loss: 6.224475383758545 | BCE Loss: 1.0303682088851929\n",
      "Epoch 321 / 500 | iteration 5 / 30 | Total Loss: 1.03444242477417 | KNN Loss: 6.224477767944336 | BCE Loss: 1.03444242477417\n",
      "Epoch 321 / 500 | iteration 10 / 30 | Total Loss: 1.0286645889282227 | KNN Loss: 6.224611759185791 | BCE Loss: 1.0286645889282227\n",
      "Epoch 321 / 500 | iteration 15 / 30 | Total Loss: 1.076902151107788 | KNN Loss: 6.224308490753174 | BCE Loss: 1.076902151107788\n",
      "Epoch 321 / 500 | iteration 20 / 30 | Total Loss: 1.0482802391052246 | KNN Loss: 6.224516868591309 | BCE Loss: 1.0482802391052246\n",
      "Epoch 321 / 500 | iteration 25 / 30 | Total Loss: 1.046069622039795 | KNN Loss: 6.224371910095215 | BCE Loss: 1.046069622039795\n",
      "Epoch 322 / 500 | iteration 0 / 30 | Total Loss: 1.029191255569458 | KNN Loss: 6.224617004394531 | BCE Loss: 1.029191255569458\n",
      "Epoch 322 / 500 | iteration 5 / 30 | Total Loss: 1.0785560607910156 | KNN Loss: 6.224673271179199 | BCE Loss: 1.0785560607910156\n",
      "Epoch 322 / 500 | iteration 10 / 30 | Total Loss: 1.0700926780700684 | KNN Loss: 6.22437858581543 | BCE Loss: 1.0700926780700684\n",
      "Epoch 322 / 500 | iteration 15 / 30 | Total Loss: 1.0338785648345947 | KNN Loss: 6.22445011138916 | BCE Loss: 1.0338785648345947\n",
      "Epoch 322 / 500 | iteration 20 / 30 | Total Loss: 1.050957202911377 | KNN Loss: 6.22416877746582 | BCE Loss: 1.050957202911377\n",
      "Epoch 322 / 500 | iteration 25 / 30 | Total Loss: 1.0588204860687256 | KNN Loss: 6.224349498748779 | BCE Loss: 1.0588204860687256\n",
      "Epoch 323 / 500 | iteration 0 / 30 | Total Loss: 1.0591332912445068 | KNN Loss: 6.224514961242676 | BCE Loss: 1.0591332912445068\n",
      "Epoch 323 / 500 | iteration 5 / 30 | Total Loss: 1.057187557220459 | KNN Loss: 6.224377155303955 | BCE Loss: 1.057187557220459\n",
      "Epoch 323 / 500 | iteration 10 / 30 | Total Loss: 1.0263551473617554 | KNN Loss: 6.224316120147705 | BCE Loss: 1.0263551473617554\n",
      "Epoch 323 / 500 | iteration 15 / 30 | Total Loss: 1.0317726135253906 | KNN Loss: 6.224366664886475 | BCE Loss: 1.0317726135253906\n",
      "Epoch 323 / 500 | iteration 20 / 30 | Total Loss: 1.0306826829910278 | KNN Loss: 6.2245965003967285 | BCE Loss: 1.0306826829910278\n",
      "Epoch 323 / 500 | iteration 25 / 30 | Total Loss: 1.0263066291809082 | KNN Loss: 6.224636554718018 | BCE Loss: 1.0263066291809082\n",
      "Epoch   324: reducing learning rate of group 0 to 9.5791e-07.\n",
      "Epoch 324 / 500 | iteration 0 / 30 | Total Loss: 1.114159107208252 | KNN Loss: 6.224682331085205 | BCE Loss: 1.114159107208252\n",
      "Epoch 324 / 500 | iteration 5 / 30 | Total Loss: 1.0497243404388428 | KNN Loss: 6.224368572235107 | BCE Loss: 1.0497243404388428\n",
      "Epoch 324 / 500 | iteration 10 / 30 | Total Loss: 1.006654143333435 | KNN Loss: 6.224293231964111 | BCE Loss: 1.006654143333435\n",
      "Epoch 324 / 500 | iteration 15 / 30 | Total Loss: 1.057976484298706 | KNN Loss: 6.224593162536621 | BCE Loss: 1.057976484298706\n",
      "Epoch 324 / 500 | iteration 20 / 30 | Total Loss: 1.0346959829330444 | KNN Loss: 6.2246174812316895 | BCE Loss: 1.0346959829330444\n",
      "Epoch 324 / 500 | iteration 25 / 30 | Total Loss: 1.0335594415664673 | KNN Loss: 6.224698066711426 | BCE Loss: 1.0335594415664673\n",
      "Epoch 325 / 500 | iteration 0 / 30 | Total Loss: 1.059952974319458 | KNN Loss: 6.224273681640625 | BCE Loss: 1.059952974319458\n",
      "Epoch 325 / 500 | iteration 5 / 30 | Total Loss: 1.0540356636047363 | KNN Loss: 6.224540710449219 | BCE Loss: 1.0540356636047363\n",
      "Epoch 325 / 500 | iteration 10 / 30 | Total Loss: 1.0328583717346191 | KNN Loss: 6.2243733406066895 | BCE Loss: 1.0328583717346191\n",
      "Epoch 325 / 500 | iteration 15 / 30 | Total Loss: 1.079545021057129 | KNN Loss: 6.224606990814209 | BCE Loss: 1.079545021057129\n",
      "Epoch 325 / 500 | iteration 20 / 30 | Total Loss: 1.0503040552139282 | KNN Loss: 6.224215030670166 | BCE Loss: 1.0503040552139282\n",
      "Epoch 325 / 500 | iteration 25 / 30 | Total Loss: 1.0550971031188965 | KNN Loss: 6.22466516494751 | BCE Loss: 1.0550971031188965\n",
      "Epoch 326 / 500 | iteration 0 / 30 | Total Loss: 1.0467256307601929 | KNN Loss: 6.224344253540039 | BCE Loss: 1.0467256307601929\n",
      "Epoch 326 / 500 | iteration 5 / 30 | Total Loss: 1.03475022315979 | KNN Loss: 6.2246012687683105 | BCE Loss: 1.03475022315979\n",
      "Epoch 326 / 500 | iteration 10 / 30 | Total Loss: 1.1026945114135742 | KNN Loss: 6.224646091461182 | BCE Loss: 1.1026945114135742\n",
      "Epoch 326 / 500 | iteration 15 / 30 | Total Loss: 1.0341973304748535 | KNN Loss: 6.224249362945557 | BCE Loss: 1.0341973304748535\n",
      "Epoch 326 / 500 | iteration 20 / 30 | Total Loss: 1.0297845602035522 | KNN Loss: 6.224463939666748 | BCE Loss: 1.0297845602035522\n",
      "Epoch 326 / 500 | iteration 25 / 30 | Total Loss: 1.0581268072128296 | KNN Loss: 6.2248311042785645 | BCE Loss: 1.0581268072128296\n",
      "Epoch 327 / 500 | iteration 0 / 30 | Total Loss: 1.0689876079559326 | KNN Loss: 6.2245683670043945 | BCE Loss: 1.0689876079559326\n",
      "Epoch 327 / 500 | iteration 5 / 30 | Total Loss: 1.046205759048462 | KNN Loss: 6.224719047546387 | BCE Loss: 1.046205759048462\n",
      "Epoch 327 / 500 | iteration 10 / 30 | Total Loss: 1.0464961528778076 | KNN Loss: 6.2242960929870605 | BCE Loss: 1.0464961528778076\n",
      "Epoch 327 / 500 | iteration 15 / 30 | Total Loss: 1.040212869644165 | KNN Loss: 6.224371433258057 | BCE Loss: 1.040212869644165\n",
      "Epoch 327 / 500 | iteration 20 / 30 | Total Loss: 1.060725212097168 | KNN Loss: 6.224873065948486 | BCE Loss: 1.060725212097168\n",
      "Epoch 327 / 500 | iteration 25 / 30 | Total Loss: 1.0481722354888916 | KNN Loss: 6.224414348602295 | BCE Loss: 1.0481722354888916\n",
      "Epoch 328 / 500 | iteration 0 / 30 | Total Loss: 1.085890531539917 | KNN Loss: 6.224733829498291 | BCE Loss: 1.085890531539917\n",
      "Epoch 328 / 500 | iteration 5 / 30 | Total Loss: 1.021176815032959 | KNN Loss: 6.2243475914001465 | BCE Loss: 1.021176815032959\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 328 / 500 | iteration 10 / 30 | Total Loss: 1.0698760747909546 | KNN Loss: 6.224327087402344 | BCE Loss: 1.0698760747909546\n",
      "Epoch 328 / 500 | iteration 15 / 30 | Total Loss: 1.0441166162490845 | KNN Loss: 6.224541187286377 | BCE Loss: 1.0441166162490845\n",
      "Epoch 328 / 500 | iteration 20 / 30 | Total Loss: 1.031194806098938 | KNN Loss: 6.224257946014404 | BCE Loss: 1.031194806098938\n",
      "Epoch 328 / 500 | iteration 25 / 30 | Total Loss: 1.0331920385360718 | KNN Loss: 6.224069595336914 | BCE Loss: 1.0331920385360718\n",
      "Epoch 329 / 500 | iteration 0 / 30 | Total Loss: 1.0517688989639282 | KNN Loss: 6.224905490875244 | BCE Loss: 1.0517688989639282\n",
      "Epoch 329 / 500 | iteration 5 / 30 | Total Loss: 1.0591871738433838 | KNN Loss: 6.224545478820801 | BCE Loss: 1.0591871738433838\n",
      "Epoch 329 / 500 | iteration 10 / 30 | Total Loss: 1.0593271255493164 | KNN Loss: 6.224287986755371 | BCE Loss: 1.0593271255493164\n",
      "Epoch 329 / 500 | iteration 15 / 30 | Total Loss: 1.0481394529342651 | KNN Loss: 6.224233150482178 | BCE Loss: 1.0481394529342651\n",
      "Epoch 329 / 500 | iteration 20 / 30 | Total Loss: 1.0245583057403564 | KNN Loss: 6.224514961242676 | BCE Loss: 1.0245583057403564\n",
      "Epoch 329 / 500 | iteration 25 / 30 | Total Loss: 1.0814282894134521 | KNN Loss: 6.224496364593506 | BCE Loss: 1.0814282894134521\n",
      "Epoch 330 / 500 | iteration 0 / 30 | Total Loss: 1.041684627532959 | KNN Loss: 6.224455833435059 | BCE Loss: 1.041684627532959\n",
      "Epoch 330 / 500 | iteration 5 / 30 | Total Loss: 1.076994776725769 | KNN Loss: 6.224456310272217 | BCE Loss: 1.076994776725769\n",
      "Epoch 330 / 500 | iteration 10 / 30 | Total Loss: 1.0388102531433105 | KNN Loss: 6.224655628204346 | BCE Loss: 1.0388102531433105\n",
      "Epoch 330 / 500 | iteration 15 / 30 | Total Loss: 1.0283361673355103 | KNN Loss: 6.224481582641602 | BCE Loss: 1.0283361673355103\n",
      "Epoch 330 / 500 | iteration 20 / 30 | Total Loss: 1.059256672859192 | KNN Loss: 6.2247138023376465 | BCE Loss: 1.059256672859192\n",
      "Epoch 330 / 500 | iteration 25 / 30 | Total Loss: 1.0507655143737793 | KNN Loss: 6.224254131317139 | BCE Loss: 1.0507655143737793\n",
      "Epoch 331 / 500 | iteration 0 / 30 | Total Loss: 1.0702980756759644 | KNN Loss: 6.224612712860107 | BCE Loss: 1.0702980756759644\n",
      "Epoch 331 / 500 | iteration 5 / 30 | Total Loss: 1.029587745666504 | KNN Loss: 6.224844932556152 | BCE Loss: 1.029587745666504\n",
      "Epoch 331 / 500 | iteration 10 / 30 | Total Loss: 1.0358688831329346 | KNN Loss: 6.224544525146484 | BCE Loss: 1.0358688831329346\n",
      "Epoch 331 / 500 | iteration 15 / 30 | Total Loss: 1.0405243635177612 | KNN Loss: 6.224146366119385 | BCE Loss: 1.0405243635177612\n",
      "Epoch 331 / 500 | iteration 20 / 30 | Total Loss: 1.0342340469360352 | KNN Loss: 6.224593639373779 | BCE Loss: 1.0342340469360352\n",
      "Epoch 331 / 500 | iteration 25 / 30 | Total Loss: 1.0586256980895996 | KNN Loss: 6.224525451660156 | BCE Loss: 1.0586256980895996\n",
      "Epoch 332 / 500 | iteration 0 / 30 | Total Loss: 1.0344072580337524 | KNN Loss: 6.224335670471191 | BCE Loss: 1.0344072580337524\n",
      "Epoch 332 / 500 | iteration 5 / 30 | Total Loss: 1.0397639274597168 | KNN Loss: 6.224634647369385 | BCE Loss: 1.0397639274597168\n",
      "Epoch 332 / 500 | iteration 10 / 30 | Total Loss: 1.019293189048767 | KNN Loss: 6.224736213684082 | BCE Loss: 1.019293189048767\n",
      "Epoch 332 / 500 | iteration 15 / 30 | Total Loss: 1.0688743591308594 | KNN Loss: 6.2245330810546875 | BCE Loss: 1.0688743591308594\n",
      "Epoch 332 / 500 | iteration 20 / 30 | Total Loss: 1.0473406314849854 | KNN Loss: 6.224427223205566 | BCE Loss: 1.0473406314849854\n",
      "Epoch 332 / 500 | iteration 25 / 30 | Total Loss: 1.08480966091156 | KNN Loss: 6.224435806274414 | BCE Loss: 1.08480966091156\n",
      "Epoch 333 / 500 | iteration 0 / 30 | Total Loss: 1.0179853439331055 | KNN Loss: 6.224268436431885 | BCE Loss: 1.0179853439331055\n",
      "Epoch 333 / 500 | iteration 5 / 30 | Total Loss: 1.0142786502838135 | KNN Loss: 6.224447250366211 | BCE Loss: 1.0142786502838135\n",
      "Epoch 333 / 500 | iteration 10 / 30 | Total Loss: 1.0776875019073486 | KNN Loss: 6.224569797515869 | BCE Loss: 1.0776875019073486\n",
      "Epoch 333 / 500 | iteration 15 / 30 | Total Loss: 1.0505082607269287 | KNN Loss: 6.224456310272217 | BCE Loss: 1.0505082607269287\n",
      "Epoch 333 / 500 | iteration 20 / 30 | Total Loss: 1.0956873893737793 | KNN Loss: 6.224582195281982 | BCE Loss: 1.0956873893737793\n",
      "Epoch 333 / 500 | iteration 25 / 30 | Total Loss: 1.0282188653945923 | KNN Loss: 6.2246198654174805 | BCE Loss: 1.0282188653945923\n",
      "Epoch 334 / 500 | iteration 0 / 30 | Total Loss: 1.0742380619049072 | KNN Loss: 6.224246025085449 | BCE Loss: 1.0742380619049072\n",
      "Epoch 334 / 500 | iteration 5 / 30 | Total Loss: 1.0619783401489258 | KNN Loss: 6.224382400512695 | BCE Loss: 1.0619783401489258\n",
      "Epoch 334 / 500 | iteration 10 / 30 | Total Loss: 1.0235825777053833 | KNN Loss: 6.2245659828186035 | BCE Loss: 1.0235825777053833\n",
      "Epoch 334 / 500 | iteration 15 / 30 | Total Loss: 1.0588008165359497 | KNN Loss: 6.224366664886475 | BCE Loss: 1.0588008165359497\n",
      "Epoch 334 / 500 | iteration 20 / 30 | Total Loss: 1.0458204746246338 | KNN Loss: 6.224569320678711 | BCE Loss: 1.0458204746246338\n",
      "Epoch 334 / 500 | iteration 25 / 30 | Total Loss: 1.0659103393554688 | KNN Loss: 6.224353790283203 | BCE Loss: 1.0659103393554688\n",
      "Epoch   335: reducing learning rate of group 0 to 6.7053e-07.\n",
      "Epoch 335 / 500 | iteration 0 / 30 | Total Loss: 1.0250253677368164 | KNN Loss: 6.224372863769531 | BCE Loss: 1.0250253677368164\n",
      "Epoch 335 / 500 | iteration 5 / 30 | Total Loss: 1.0500068664550781 | KNN Loss: 6.224234104156494 | BCE Loss: 1.0500068664550781\n",
      "Epoch 335 / 500 | iteration 10 / 30 | Total Loss: 1.0651944875717163 | KNN Loss: 6.2245588302612305 | BCE Loss: 1.0651944875717163\n",
      "Epoch 335 / 500 | iteration 15 / 30 | Total Loss: 1.0380752086639404 | KNN Loss: 6.224759578704834 | BCE Loss: 1.0380752086639404\n",
      "Epoch 335 / 500 | iteration 20 / 30 | Total Loss: 1.072167158126831 | KNN Loss: 6.224727153778076 | BCE Loss: 1.072167158126831\n",
      "Epoch 335 / 500 | iteration 25 / 30 | Total Loss: 1.0535521507263184 | KNN Loss: 6.224083423614502 | BCE Loss: 1.0535521507263184\n",
      "Epoch 336 / 500 | iteration 0 / 30 | Total Loss: 1.059718370437622 | KNN Loss: 6.224660873413086 | BCE Loss: 1.059718370437622\n",
      "Epoch 336 / 500 | iteration 5 / 30 | Total Loss: 1.0818724632263184 | KNN Loss: 6.224318027496338 | BCE Loss: 1.0818724632263184\n",
      "Epoch 336 / 500 | iteration 10 / 30 | Total Loss: 1.0573058128356934 | KNN Loss: 6.224111080169678 | BCE Loss: 1.0573058128356934\n",
      "Epoch 336 / 500 | iteration 15 / 30 | Total Loss: 1.069956660270691 | KNN Loss: 6.224638938903809 | BCE Loss: 1.069956660270691\n",
      "Epoch 336 / 500 | iteration 20 / 30 | Total Loss: 1.0457695722579956 | KNN Loss: 6.224391937255859 | BCE Loss: 1.0457695722579956\n",
      "Epoch 336 / 500 | iteration 25 / 30 | Total Loss: 1.0845587253570557 | KNN Loss: 6.2245330810546875 | BCE Loss: 1.0845587253570557\n",
      "Epoch 337 / 500 | iteration 0 / 30 | Total Loss: 1.019099473953247 | KNN Loss: 6.224544525146484 | BCE Loss: 1.019099473953247\n",
      "Epoch 337 / 500 | iteration 5 / 30 | Total Loss: 1.0497101545333862 | KNN Loss: 6.2244873046875 | BCE Loss: 1.0497101545333862\n",
      "Epoch 337 / 500 | iteration 10 / 30 | Total Loss: 1.0661745071411133 | KNN Loss: 6.2246294021606445 | BCE Loss: 1.0661745071411133\n",
      "Epoch 337 / 500 | iteration 15 / 30 | Total Loss: 1.075817346572876 | KNN Loss: 6.224711894989014 | BCE Loss: 1.075817346572876\n",
      "Epoch 337 / 500 | iteration 20 / 30 | Total Loss: 1.0593903064727783 | KNN Loss: 6.2246575355529785 | BCE Loss: 1.0593903064727783\n",
      "Epoch 337 / 500 | iteration 25 / 30 | Total Loss: 1.0578252077102661 | KNN Loss: 6.224723815917969 | BCE Loss: 1.0578252077102661\n",
      "Epoch 338 / 500 | iteration 0 / 30 | Total Loss: 1.0763061046600342 | KNN Loss: 6.2240681648254395 | BCE Loss: 1.0763061046600342\n",
      "Epoch 338 / 500 | iteration 5 / 30 | Total Loss: 1.055553913116455 | KNN Loss: 6.224408149719238 | BCE Loss: 1.055553913116455\n",
      "Epoch 338 / 500 | iteration 10 / 30 | Total Loss: 1.0637598037719727 | KNN Loss: 6.224189281463623 | BCE Loss: 1.0637598037719727\n",
      "Epoch 338 / 500 | iteration 15 / 30 | Total Loss: 1.0636721849441528 | KNN Loss: 6.224493980407715 | BCE Loss: 1.0636721849441528\n",
      "Epoch 338 / 500 | iteration 20 / 30 | Total Loss: 1.0351027250289917 | KNN Loss: 6.224259376525879 | BCE Loss: 1.0351027250289917\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 338 / 500 | iteration 25 / 30 | Total Loss: 1.0476807355880737 | KNN Loss: 6.22468900680542 | BCE Loss: 1.0476807355880737\n",
      "Epoch 339 / 500 | iteration 0 / 30 | Total Loss: 1.0425565242767334 | KNN Loss: 6.224487781524658 | BCE Loss: 1.0425565242767334\n",
      "Epoch 339 / 500 | iteration 5 / 30 | Total Loss: 1.0701920986175537 | KNN Loss: 6.224555015563965 | BCE Loss: 1.0701920986175537\n",
      "Epoch 339 / 500 | iteration 10 / 30 | Total Loss: 1.0560481548309326 | KNN Loss: 6.224308013916016 | BCE Loss: 1.0560481548309326\n",
      "Epoch 339 / 500 | iteration 15 / 30 | Total Loss: 1.0609067678451538 | KNN Loss: 6.224414348602295 | BCE Loss: 1.0609067678451538\n",
      "Epoch 339 / 500 | iteration 20 / 30 | Total Loss: 1.0209779739379883 | KNN Loss: 6.224145889282227 | BCE Loss: 1.0209779739379883\n",
      "Epoch 339 / 500 | iteration 25 / 30 | Total Loss: 1.042926549911499 | KNN Loss: 6.224164009094238 | BCE Loss: 1.042926549911499\n",
      "Epoch 340 / 500 | iteration 0 / 30 | Total Loss: 1.0528521537780762 | KNN Loss: 6.224648952484131 | BCE Loss: 1.0528521537780762\n",
      "Epoch 340 / 500 | iteration 5 / 30 | Total Loss: 1.0665695667266846 | KNN Loss: 6.224752902984619 | BCE Loss: 1.0665695667266846\n",
      "Epoch 340 / 500 | iteration 10 / 30 | Total Loss: 1.0344276428222656 | KNN Loss: 6.224287986755371 | BCE Loss: 1.0344276428222656\n",
      "Epoch 340 / 500 | iteration 15 / 30 | Total Loss: 1.0647532939910889 | KNN Loss: 6.224475860595703 | BCE Loss: 1.0647532939910889\n",
      "Epoch 340 / 500 | iteration 20 / 30 | Total Loss: 1.084902048110962 | KNN Loss: 6.224826335906982 | BCE Loss: 1.084902048110962\n",
      "Epoch 340 / 500 | iteration 25 / 30 | Total Loss: 1.0435054302215576 | KNN Loss: 6.224455833435059 | BCE Loss: 1.0435054302215576\n",
      "Epoch 341 / 500 | iteration 0 / 30 | Total Loss: 1.064199686050415 | KNN Loss: 6.224308967590332 | BCE Loss: 1.064199686050415\n",
      "Epoch 341 / 500 | iteration 5 / 30 | Total Loss: 1.045312762260437 | KNN Loss: 6.224596977233887 | BCE Loss: 1.045312762260437\n",
      "Epoch 341 / 500 | iteration 10 / 30 | Total Loss: 1.0250548124313354 | KNN Loss: 6.224544525146484 | BCE Loss: 1.0250548124313354\n",
      "Epoch 341 / 500 | iteration 15 / 30 | Total Loss: 1.053673267364502 | KNN Loss: 6.224517822265625 | BCE Loss: 1.053673267364502\n",
      "Epoch 341 / 500 | iteration 20 / 30 | Total Loss: 1.061560869216919 | KNN Loss: 6.224813461303711 | BCE Loss: 1.061560869216919\n",
      "Epoch 341 / 500 | iteration 25 / 30 | Total Loss: 1.0193203687667847 | KNN Loss: 6.224414348602295 | BCE Loss: 1.0193203687667847\n",
      "Epoch 342 / 500 | iteration 0 / 30 | Total Loss: 1.0480256080627441 | KNN Loss: 6.224578380584717 | BCE Loss: 1.0480256080627441\n",
      "Epoch 342 / 500 | iteration 5 / 30 | Total Loss: 1.034867763519287 | KNN Loss: 6.224414348602295 | BCE Loss: 1.034867763519287\n",
      "Epoch 342 / 500 | iteration 10 / 30 | Total Loss: 1.0364993810653687 | KNN Loss: 6.2247161865234375 | BCE Loss: 1.0364993810653687\n",
      "Epoch 342 / 500 | iteration 15 / 30 | Total Loss: 1.0450412034988403 | KNN Loss: 6.224201679229736 | BCE Loss: 1.0450412034988403\n",
      "Epoch 342 / 500 | iteration 20 / 30 | Total Loss: 1.0475351810455322 | KNN Loss: 6.224593639373779 | BCE Loss: 1.0475351810455322\n",
      "Epoch 342 / 500 | iteration 25 / 30 | Total Loss: 1.0379102230072021 | KNN Loss: 6.224219799041748 | BCE Loss: 1.0379102230072021\n",
      "Epoch 343 / 500 | iteration 0 / 30 | Total Loss: 1.042766809463501 | KNN Loss: 6.2246880531311035 | BCE Loss: 1.042766809463501\n",
      "Epoch 343 / 500 | iteration 5 / 30 | Total Loss: 1.071892499923706 | KNN Loss: 6.224746227264404 | BCE Loss: 1.071892499923706\n",
      "Epoch 343 / 500 | iteration 10 / 30 | Total Loss: 1.036164402961731 | KNN Loss: 6.224493026733398 | BCE Loss: 1.036164402961731\n",
      "Epoch 343 / 500 | iteration 15 / 30 | Total Loss: 1.0335334539413452 | KNN Loss: 6.22459602355957 | BCE Loss: 1.0335334539413452\n",
      "Epoch 343 / 500 | iteration 20 / 30 | Total Loss: 1.0450102090835571 | KNN Loss: 6.22438383102417 | BCE Loss: 1.0450102090835571\n",
      "Epoch 343 / 500 | iteration 25 / 30 | Total Loss: 1.045201301574707 | KNN Loss: 6.224598407745361 | BCE Loss: 1.045201301574707\n",
      "Epoch 344 / 500 | iteration 0 / 30 | Total Loss: 1.0381741523742676 | KNN Loss: 6.224419116973877 | BCE Loss: 1.0381741523742676\n",
      "Epoch 344 / 500 | iteration 5 / 30 | Total Loss: 1.0372414588928223 | KNN Loss: 6.22475004196167 | BCE Loss: 1.0372414588928223\n",
      "Epoch 344 / 500 | iteration 10 / 30 | Total Loss: 1.0428389310836792 | KNN Loss: 6.22454833984375 | BCE Loss: 1.0428389310836792\n",
      "Epoch 344 / 500 | iteration 15 / 30 | Total Loss: 1.0529121160507202 | KNN Loss: 6.224626064300537 | BCE Loss: 1.0529121160507202\n",
      "Epoch 344 / 500 | iteration 20 / 30 | Total Loss: 1.0453293323516846 | KNN Loss: 6.224405288696289 | BCE Loss: 1.0453293323516846\n",
      "Epoch 344 / 500 | iteration 25 / 30 | Total Loss: 1.0507373809814453 | KNN Loss: 6.224442481994629 | BCE Loss: 1.0507373809814453\n",
      "Epoch 345 / 500 | iteration 0 / 30 | Total Loss: 1.0149917602539062 | KNN Loss: 6.224381446838379 | BCE Loss: 1.0149917602539062\n",
      "Epoch 345 / 500 | iteration 5 / 30 | Total Loss: 1.0618284940719604 | KNN Loss: 6.224488258361816 | BCE Loss: 1.0618284940719604\n",
      "Epoch 345 / 500 | iteration 10 / 30 | Total Loss: 1.0319926738739014 | KNN Loss: 6.224454879760742 | BCE Loss: 1.0319926738739014\n",
      "Epoch 345 / 500 | iteration 15 / 30 | Total Loss: 1.036205530166626 | KNN Loss: 6.2247633934021 | BCE Loss: 1.036205530166626\n",
      "Epoch 345 / 500 | iteration 20 / 30 | Total Loss: 1.0451586246490479 | KNN Loss: 6.224728584289551 | BCE Loss: 1.0451586246490479\n",
      "Epoch 345 / 500 | iteration 25 / 30 | Total Loss: 1.0615575313568115 | KNN Loss: 6.224517822265625 | BCE Loss: 1.0615575313568115\n",
      "Epoch   346: reducing learning rate of group 0 to 4.6937e-07.\n",
      "Epoch 346 / 500 | iteration 0 / 30 | Total Loss: 1.0247262716293335 | KNN Loss: 6.22406530380249 | BCE Loss: 1.0247262716293335\n",
      "Epoch 346 / 500 | iteration 5 / 30 | Total Loss: 1.0527071952819824 | KNN Loss: 6.224447250366211 | BCE Loss: 1.0527071952819824\n",
      "Epoch 346 / 500 | iteration 10 / 30 | Total Loss: 1.0340781211853027 | KNN Loss: 6.224113464355469 | BCE Loss: 1.0340781211853027\n",
      "Epoch 346 / 500 | iteration 15 / 30 | Total Loss: 1.073032259941101 | KNN Loss: 6.224316120147705 | BCE Loss: 1.073032259941101\n",
      "Epoch 346 / 500 | iteration 20 / 30 | Total Loss: 1.0380032062530518 | KNN Loss: 6.224258899688721 | BCE Loss: 1.0380032062530518\n",
      "Epoch 346 / 500 | iteration 25 / 30 | Total Loss: 1.0211228132247925 | KNN Loss: 6.224658966064453 | BCE Loss: 1.0211228132247925\n",
      "Epoch 347 / 500 | iteration 0 / 30 | Total Loss: 1.0249435901641846 | KNN Loss: 6.223968982696533 | BCE Loss: 1.0249435901641846\n",
      "Epoch 347 / 500 | iteration 5 / 30 | Total Loss: 1.0789481401443481 | KNN Loss: 6.224477291107178 | BCE Loss: 1.0789481401443481\n",
      "Epoch 347 / 500 | iteration 10 / 30 | Total Loss: 1.0458483695983887 | KNN Loss: 6.224140644073486 | BCE Loss: 1.0458483695983887\n",
      "Epoch 347 / 500 | iteration 15 / 30 | Total Loss: 1.0620131492614746 | KNN Loss: 6.2247490882873535 | BCE Loss: 1.0620131492614746\n",
      "Epoch 347 / 500 | iteration 20 / 30 | Total Loss: 1.0592997074127197 | KNN Loss: 6.22467041015625 | BCE Loss: 1.0592997074127197\n",
      "Epoch 347 / 500 | iteration 25 / 30 | Total Loss: 1.0476341247558594 | KNN Loss: 6.224750995635986 | BCE Loss: 1.0476341247558594\n",
      "Epoch 348 / 500 | iteration 0 / 30 | Total Loss: 1.0281002521514893 | KNN Loss: 6.224088668823242 | BCE Loss: 1.0281002521514893\n",
      "Epoch 348 / 500 | iteration 5 / 30 | Total Loss: 1.0742928981781006 | KNN Loss: 6.224714756011963 | BCE Loss: 1.0742928981781006\n",
      "Epoch 348 / 500 | iteration 10 / 30 | Total Loss: 1.0772809982299805 | KNN Loss: 6.224554061889648 | BCE Loss: 1.0772809982299805\n",
      "Epoch 348 / 500 | iteration 15 / 30 | Total Loss: 1.0584115982055664 | KNN Loss: 6.2246599197387695 | BCE Loss: 1.0584115982055664\n",
      "Epoch 348 / 500 | iteration 20 / 30 | Total Loss: 1.0633145570755005 | KNN Loss: 6.224544048309326 | BCE Loss: 1.0633145570755005\n",
      "Epoch 348 / 500 | iteration 25 / 30 | Total Loss: 1.003585696220398 | KNN Loss: 6.22447395324707 | BCE Loss: 1.003585696220398\n",
      "Epoch 349 / 500 | iteration 0 / 30 | Total Loss: 1.0420325994491577 | KNN Loss: 6.224814414978027 | BCE Loss: 1.0420325994491577\n",
      "Epoch 349 / 500 | iteration 5 / 30 | Total Loss: 1.0530825853347778 | KNN Loss: 6.2245049476623535 | BCE Loss: 1.0530825853347778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 349 / 500 | iteration 10 / 30 | Total Loss: 1.0152555704116821 | KNN Loss: 6.224185466766357 | BCE Loss: 1.0152555704116821\n",
      "Epoch 349 / 500 | iteration 15 / 30 | Total Loss: 1.0581552982330322 | KNN Loss: 6.224181652069092 | BCE Loss: 1.0581552982330322\n",
      "Epoch 349 / 500 | iteration 20 / 30 | Total Loss: 1.0157139301300049 | KNN Loss: 6.2242960929870605 | BCE Loss: 1.0157139301300049\n",
      "Epoch 349 / 500 | iteration 25 / 30 | Total Loss: 1.057373046875 | KNN Loss: 6.22415828704834 | BCE Loss: 1.057373046875\n",
      "Epoch 350 / 500 | iteration 0 / 30 | Total Loss: 1.0690217018127441 | KNN Loss: 6.224431991577148 | BCE Loss: 1.0690217018127441\n",
      "Epoch 350 / 500 | iteration 5 / 30 | Total Loss: 1.0632871389389038 | KNN Loss: 6.224470615386963 | BCE Loss: 1.0632871389389038\n",
      "Epoch 350 / 500 | iteration 10 / 30 | Total Loss: 1.0469751358032227 | KNN Loss: 6.224776268005371 | BCE Loss: 1.0469751358032227\n",
      "Epoch 350 / 500 | iteration 15 / 30 | Total Loss: 1.0382843017578125 | KNN Loss: 6.224638938903809 | BCE Loss: 1.0382843017578125\n",
      "Epoch 350 / 500 | iteration 20 / 30 | Total Loss: 1.0578203201293945 | KNN Loss: 6.224228858947754 | BCE Loss: 1.0578203201293945\n",
      "Epoch 350 / 500 | iteration 25 / 30 | Total Loss: 1.0412030220031738 | KNN Loss: 6.224257469177246 | BCE Loss: 1.0412030220031738\n",
      "Epoch 351 / 500 | iteration 0 / 30 | Total Loss: 1.0476312637329102 | KNN Loss: 6.224252223968506 | BCE Loss: 1.0476312637329102\n",
      "Epoch 351 / 500 | iteration 5 / 30 | Total Loss: 1.0580978393554688 | KNN Loss: 6.22408390045166 | BCE Loss: 1.0580978393554688\n",
      "Epoch 351 / 500 | iteration 10 / 30 | Total Loss: 1.0669294595718384 | KNN Loss: 6.22446870803833 | BCE Loss: 1.0669294595718384\n",
      "Epoch 351 / 500 | iteration 15 / 30 | Total Loss: 1.0549906492233276 | KNN Loss: 6.224587917327881 | BCE Loss: 1.0549906492233276\n",
      "Epoch 351 / 500 | iteration 20 / 30 | Total Loss: 1.082251787185669 | KNN Loss: 6.224442958831787 | BCE Loss: 1.082251787185669\n",
      "Epoch 351 / 500 | iteration 25 / 30 | Total Loss: 1.069403886795044 | KNN Loss: 6.223801612854004 | BCE Loss: 1.069403886795044\n",
      "Epoch 352 / 500 | iteration 0 / 30 | Total Loss: 1.0354392528533936 | KNN Loss: 6.224238872528076 | BCE Loss: 1.0354392528533936\n",
      "Epoch 352 / 500 | iteration 5 / 30 | Total Loss: 1.0571824312210083 | KNN Loss: 6.224122047424316 | BCE Loss: 1.0571824312210083\n",
      "Epoch 352 / 500 | iteration 10 / 30 | Total Loss: 1.0564870834350586 | KNN Loss: 6.224359035491943 | BCE Loss: 1.0564870834350586\n",
      "Epoch 352 / 500 | iteration 15 / 30 | Total Loss: 1.0331089496612549 | KNN Loss: 6.224309921264648 | BCE Loss: 1.0331089496612549\n",
      "Epoch 352 / 500 | iteration 20 / 30 | Total Loss: 1.0695061683654785 | KNN Loss: 6.224252223968506 | BCE Loss: 1.0695061683654785\n",
      "Epoch 352 / 500 | iteration 25 / 30 | Total Loss: 1.0250964164733887 | KNN Loss: 6.224257469177246 | BCE Loss: 1.0250964164733887\n",
      "Epoch 353 / 500 | iteration 0 / 30 | Total Loss: 1.0304415225982666 | KNN Loss: 6.224185466766357 | BCE Loss: 1.0304415225982666\n",
      "Epoch 353 / 500 | iteration 5 / 30 | Total Loss: 1.0273698568344116 | KNN Loss: 6.224425792694092 | BCE Loss: 1.0273698568344116\n",
      "Epoch 353 / 500 | iteration 10 / 30 | Total Loss: 1.0418756008148193 | KNN Loss: 6.224480152130127 | BCE Loss: 1.0418756008148193\n",
      "Epoch 353 / 500 | iteration 15 / 30 | Total Loss: 1.0312285423278809 | KNN Loss: 6.224275588989258 | BCE Loss: 1.0312285423278809\n",
      "Epoch 353 / 500 | iteration 20 / 30 | Total Loss: 1.0687124729156494 | KNN Loss: 6.224710941314697 | BCE Loss: 1.0687124729156494\n",
      "Epoch 353 / 500 | iteration 25 / 30 | Total Loss: 1.0585238933563232 | KNN Loss: 6.224824905395508 | BCE Loss: 1.0585238933563232\n",
      "Epoch 354 / 500 | iteration 0 / 30 | Total Loss: 1.0567026138305664 | KNN Loss: 6.224630355834961 | BCE Loss: 1.0567026138305664\n",
      "Epoch 354 / 500 | iteration 5 / 30 | Total Loss: 1.0102641582489014 | KNN Loss: 6.224642753601074 | BCE Loss: 1.0102641582489014\n",
      "Epoch 354 / 500 | iteration 10 / 30 | Total Loss: 1.0734196901321411 | KNN Loss: 6.224398612976074 | BCE Loss: 1.0734196901321411\n",
      "Epoch 354 / 500 | iteration 15 / 30 | Total Loss: 1.0831719636917114 | KNN Loss: 6.224350452423096 | BCE Loss: 1.0831719636917114\n",
      "Epoch 354 / 500 | iteration 20 / 30 | Total Loss: 1.069358229637146 | KNN Loss: 6.224264144897461 | BCE Loss: 1.069358229637146\n",
      "Epoch 354 / 500 | iteration 25 / 30 | Total Loss: 1.038317084312439 | KNN Loss: 6.224488735198975 | BCE Loss: 1.038317084312439\n",
      "Epoch 355 / 500 | iteration 0 / 30 | Total Loss: 1.0600342750549316 | KNN Loss: 6.224130630493164 | BCE Loss: 1.0600342750549316\n",
      "Epoch 355 / 500 | iteration 5 / 30 | Total Loss: 1.0501368045806885 | KNN Loss: 6.224672794342041 | BCE Loss: 1.0501368045806885\n",
      "Epoch 355 / 500 | iteration 10 / 30 | Total Loss: 1.0699472427368164 | KNN Loss: 6.224381446838379 | BCE Loss: 1.0699472427368164\n",
      "Epoch 355 / 500 | iteration 15 / 30 | Total Loss: 1.0447986125946045 | KNN Loss: 6.2243242263793945 | BCE Loss: 1.0447986125946045\n",
      "Epoch 355 / 500 | iteration 20 / 30 | Total Loss: 1.0735020637512207 | KNN Loss: 6.224328517913818 | BCE Loss: 1.0735020637512207\n",
      "Epoch 355 / 500 | iteration 25 / 30 | Total Loss: 1.0622634887695312 | KNN Loss: 6.224372386932373 | BCE Loss: 1.0622634887695312\n",
      "Epoch 356 / 500 | iteration 0 / 30 | Total Loss: 1.0298339128494263 | KNN Loss: 6.224206924438477 | BCE Loss: 1.0298339128494263\n",
      "Epoch 356 / 500 | iteration 5 / 30 | Total Loss: 1.0508702993392944 | KNN Loss: 6.224605083465576 | BCE Loss: 1.0508702993392944\n",
      "Epoch 356 / 500 | iteration 10 / 30 | Total Loss: 1.0584404468536377 | KNN Loss: 6.224588871002197 | BCE Loss: 1.0584404468536377\n",
      "Epoch 356 / 500 | iteration 15 / 30 | Total Loss: 1.0674149990081787 | KNN Loss: 6.224303722381592 | BCE Loss: 1.0674149990081787\n",
      "Epoch 356 / 500 | iteration 20 / 30 | Total Loss: 1.044318437576294 | KNN Loss: 6.224709510803223 | BCE Loss: 1.044318437576294\n",
      "Epoch 356 / 500 | iteration 25 / 30 | Total Loss: 1.0867747068405151 | KNN Loss: 6.224584102630615 | BCE Loss: 1.0867747068405151\n",
      "Epoch   357: reducing learning rate of group 0 to 3.2856e-07.\n",
      "Epoch 357 / 500 | iteration 0 / 30 | Total Loss: 1.0614402294158936 | KNN Loss: 6.225022792816162 | BCE Loss: 1.0614402294158936\n",
      "Epoch 357 / 500 | iteration 5 / 30 | Total Loss: 1.044018030166626 | KNN Loss: 6.22430944442749 | BCE Loss: 1.044018030166626\n",
      "Epoch 357 / 500 | iteration 10 / 30 | Total Loss: 1.0686578750610352 | KNN Loss: 6.224569797515869 | BCE Loss: 1.0686578750610352\n",
      "Epoch 357 / 500 | iteration 15 / 30 | Total Loss: 1.0372769832611084 | KNN Loss: 6.2243876457214355 | BCE Loss: 1.0372769832611084\n",
      "Epoch 357 / 500 | iteration 20 / 30 | Total Loss: 1.0481661558151245 | KNN Loss: 6.2247772216796875 | BCE Loss: 1.0481661558151245\n",
      "Epoch 357 / 500 | iteration 25 / 30 | Total Loss: 1.0431954860687256 | KNN Loss: 6.224215030670166 | BCE Loss: 1.0431954860687256\n",
      "Epoch 358 / 500 | iteration 0 / 30 | Total Loss: 1.0299389362335205 | KNN Loss: 6.2245707511901855 | BCE Loss: 1.0299389362335205\n",
      "Epoch 358 / 500 | iteration 5 / 30 | Total Loss: 1.0725982189178467 | KNN Loss: 6.224539279937744 | BCE Loss: 1.0725982189178467\n",
      "Epoch 358 / 500 | iteration 10 / 30 | Total Loss: 1.0484774112701416 | KNN Loss: 6.224691390991211 | BCE Loss: 1.0484774112701416\n",
      "Epoch 358 / 500 | iteration 15 / 30 | Total Loss: 1.045058012008667 | KNN Loss: 6.224537372589111 | BCE Loss: 1.045058012008667\n",
      "Epoch 358 / 500 | iteration 20 / 30 | Total Loss: 1.033105731010437 | KNN Loss: 6.224289894104004 | BCE Loss: 1.033105731010437\n",
      "Epoch 358 / 500 | iteration 25 / 30 | Total Loss: 1.0430030822753906 | KNN Loss: 6.22460412979126 | BCE Loss: 1.0430030822753906\n",
      "Epoch 359 / 500 | iteration 0 / 30 | Total Loss: 1.0391845703125 | KNN Loss: 6.2243781089782715 | BCE Loss: 1.0391845703125\n",
      "Epoch 359 / 500 | iteration 5 / 30 | Total Loss: 1.0477240085601807 | KNN Loss: 6.223829746246338 | BCE Loss: 1.0477240085601807\n",
      "Epoch 359 / 500 | iteration 10 / 30 | Total Loss: 1.0333918333053589 | KNN Loss: 6.22463846206665 | BCE Loss: 1.0333918333053589\n",
      "Epoch 359 / 500 | iteration 15 / 30 | Total Loss: 1.0474414825439453 | KNN Loss: 6.224323272705078 | BCE Loss: 1.0474414825439453\n",
      "Epoch 359 / 500 | iteration 20 / 30 | Total Loss: 1.0372071266174316 | KNN Loss: 6.224140167236328 | BCE Loss: 1.0372071266174316\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 359 / 500 | iteration 25 / 30 | Total Loss: 1.0313165187835693 | KNN Loss: 6.224365711212158 | BCE Loss: 1.0313165187835693\n",
      "Epoch 360 / 500 | iteration 0 / 30 | Total Loss: 1.0807781219482422 | KNN Loss: 6.2242350578308105 | BCE Loss: 1.0807781219482422\n",
      "Epoch 360 / 500 | iteration 5 / 30 | Total Loss: 1.0295381546020508 | KNN Loss: 6.224366188049316 | BCE Loss: 1.0295381546020508\n",
      "Epoch 360 / 500 | iteration 10 / 30 | Total Loss: 1.0438203811645508 | KNN Loss: 6.224514484405518 | BCE Loss: 1.0438203811645508\n",
      "Epoch 360 / 500 | iteration 15 / 30 | Total Loss: 1.0498225688934326 | KNN Loss: 6.224639892578125 | BCE Loss: 1.0498225688934326\n",
      "Epoch 360 / 500 | iteration 20 / 30 | Total Loss: 1.0330480337142944 | KNN Loss: 6.224452018737793 | BCE Loss: 1.0330480337142944\n",
      "Epoch 360 / 500 | iteration 25 / 30 | Total Loss: 1.0579442977905273 | KNN Loss: 6.224433898925781 | BCE Loss: 1.0579442977905273\n",
      "Epoch 361 / 500 | iteration 0 / 30 | Total Loss: 1.027134895324707 | KNN Loss: 6.224895000457764 | BCE Loss: 1.027134895324707\n",
      "Epoch 361 / 500 | iteration 5 / 30 | Total Loss: 1.0366809368133545 | KNN Loss: 6.224623203277588 | BCE Loss: 1.0366809368133545\n",
      "Epoch 361 / 500 | iteration 10 / 30 | Total Loss: 1.053924560546875 | KNN Loss: 6.224719047546387 | BCE Loss: 1.053924560546875\n",
      "Epoch 361 / 500 | iteration 15 / 30 | Total Loss: 1.011263132095337 | KNN Loss: 6.224300861358643 | BCE Loss: 1.011263132095337\n",
      "Epoch 361 / 500 | iteration 20 / 30 | Total Loss: 1.0499937534332275 | KNN Loss: 6.224465370178223 | BCE Loss: 1.0499937534332275\n",
      "Epoch 361 / 500 | iteration 25 / 30 | Total Loss: 1.0337975025177002 | KNN Loss: 6.224299907684326 | BCE Loss: 1.0337975025177002\n",
      "Epoch 362 / 500 | iteration 0 / 30 | Total Loss: 1.0340230464935303 | KNN Loss: 6.223909378051758 | BCE Loss: 1.0340230464935303\n",
      "Epoch 362 / 500 | iteration 5 / 30 | Total Loss: 1.0728137493133545 | KNN Loss: 6.224549293518066 | BCE Loss: 1.0728137493133545\n",
      "Epoch 362 / 500 | iteration 10 / 30 | Total Loss: 1.0766732692718506 | KNN Loss: 6.22390604019165 | BCE Loss: 1.0766732692718506\n",
      "Epoch 362 / 500 | iteration 15 / 30 | Total Loss: 1.0763781070709229 | KNN Loss: 6.224282741546631 | BCE Loss: 1.0763781070709229\n",
      "Epoch 362 / 500 | iteration 20 / 30 | Total Loss: 1.0505074262619019 | KNN Loss: 6.224522590637207 | BCE Loss: 1.0505074262619019\n",
      "Epoch 362 / 500 | iteration 25 / 30 | Total Loss: 1.057563066482544 | KNN Loss: 6.2245354652404785 | BCE Loss: 1.057563066482544\n",
      "Epoch 363 / 500 | iteration 0 / 30 | Total Loss: 1.0489035844802856 | KNN Loss: 6.224318027496338 | BCE Loss: 1.0489035844802856\n",
      "Epoch 363 / 500 | iteration 5 / 30 | Total Loss: 1.0764508247375488 | KNN Loss: 6.224484920501709 | BCE Loss: 1.0764508247375488\n",
      "Epoch 363 / 500 | iteration 10 / 30 | Total Loss: 1.0463767051696777 | KNN Loss: 6.224471092224121 | BCE Loss: 1.0463767051696777\n",
      "Epoch 363 / 500 | iteration 15 / 30 | Total Loss: 1.0289242267608643 | KNN Loss: 6.224391937255859 | BCE Loss: 1.0289242267608643\n",
      "Epoch 363 / 500 | iteration 20 / 30 | Total Loss: 1.0246188640594482 | KNN Loss: 6.224613189697266 | BCE Loss: 1.0246188640594482\n",
      "Epoch 363 / 500 | iteration 25 / 30 | Total Loss: 1.031375765800476 | KNN Loss: 6.224275588989258 | BCE Loss: 1.031375765800476\n",
      "Epoch 364 / 500 | iteration 0 / 30 | Total Loss: 1.0236740112304688 | KNN Loss: 6.224268913269043 | BCE Loss: 1.0236740112304688\n",
      "Epoch 364 / 500 | iteration 5 / 30 | Total Loss: 1.0692145824432373 | KNN Loss: 6.224331855773926 | BCE Loss: 1.0692145824432373\n",
      "Epoch 364 / 500 | iteration 10 / 30 | Total Loss: 1.0552325248718262 | KNN Loss: 6.2240214347839355 | BCE Loss: 1.0552325248718262\n",
      "Epoch 364 / 500 | iteration 15 / 30 | Total Loss: 1.0292809009552002 | KNN Loss: 6.224745750427246 | BCE Loss: 1.0292809009552002\n",
      "Epoch 364 / 500 | iteration 20 / 30 | Total Loss: 1.0671169757843018 | KNN Loss: 6.224424362182617 | BCE Loss: 1.0671169757843018\n",
      "Epoch 364 / 500 | iteration 25 / 30 | Total Loss: 1.058319091796875 | KNN Loss: 6.224803924560547 | BCE Loss: 1.058319091796875\n",
      "Epoch 365 / 500 | iteration 0 / 30 | Total Loss: 1.0483461618423462 | KNN Loss: 6.224335670471191 | BCE Loss: 1.0483461618423462\n",
      "Epoch 365 / 500 | iteration 5 / 30 | Total Loss: 1.0665308237075806 | KNN Loss: 6.2241692543029785 | BCE Loss: 1.0665308237075806\n",
      "Epoch 365 / 500 | iteration 10 / 30 | Total Loss: 1.0769891738891602 | KNN Loss: 6.224674224853516 | BCE Loss: 1.0769891738891602\n",
      "Epoch 365 / 500 | iteration 15 / 30 | Total Loss: 1.0314630270004272 | KNN Loss: 6.22462272644043 | BCE Loss: 1.0314630270004272\n",
      "Epoch 365 / 500 | iteration 20 / 30 | Total Loss: 1.0693973302841187 | KNN Loss: 6.2242608070373535 | BCE Loss: 1.0693973302841187\n",
      "Epoch 365 / 500 | iteration 25 / 30 | Total Loss: 1.0811376571655273 | KNN Loss: 6.224417209625244 | BCE Loss: 1.0811376571655273\n",
      "Epoch 366 / 500 | iteration 0 / 30 | Total Loss: 1.0495413541793823 | KNN Loss: 6.224705219268799 | BCE Loss: 1.0495413541793823\n",
      "Epoch 366 / 500 | iteration 5 / 30 | Total Loss: 1.0500719547271729 | KNN Loss: 6.22470235824585 | BCE Loss: 1.0500719547271729\n",
      "Epoch 366 / 500 | iteration 10 / 30 | Total Loss: 1.0483487844467163 | KNN Loss: 6.224287509918213 | BCE Loss: 1.0483487844467163\n",
      "Epoch 366 / 500 | iteration 15 / 30 | Total Loss: 1.0393257141113281 | KNN Loss: 6.22437047958374 | BCE Loss: 1.0393257141113281\n",
      "Epoch 366 / 500 | iteration 20 / 30 | Total Loss: 1.099682092666626 | KNN Loss: 6.2245965003967285 | BCE Loss: 1.099682092666626\n",
      "Epoch 366 / 500 | iteration 25 / 30 | Total Loss: 1.055367350578308 | KNN Loss: 6.224091053009033 | BCE Loss: 1.055367350578308\n",
      "Epoch 367 / 500 | iteration 0 / 30 | Total Loss: 1.0498685836791992 | KNN Loss: 6.224510192871094 | BCE Loss: 1.0498685836791992\n",
      "Epoch 367 / 500 | iteration 5 / 30 | Total Loss: 1.0410215854644775 | KNN Loss: 6.224432468414307 | BCE Loss: 1.0410215854644775\n",
      "Epoch 367 / 500 | iteration 10 / 30 | Total Loss: 1.0566530227661133 | KNN Loss: 6.224557876586914 | BCE Loss: 1.0566530227661133\n",
      "Epoch 367 / 500 | iteration 15 / 30 | Total Loss: 1.0412174463272095 | KNN Loss: 6.224609851837158 | BCE Loss: 1.0412174463272095\n",
      "Epoch 367 / 500 | iteration 20 / 30 | Total Loss: 1.0658090114593506 | KNN Loss: 6.224372863769531 | BCE Loss: 1.0658090114593506\n",
      "Epoch 367 / 500 | iteration 25 / 30 | Total Loss: 1.044241189956665 | KNN Loss: 6.22499942779541 | BCE Loss: 1.044241189956665\n",
      "Epoch 368 / 500 | iteration 0 / 30 | Total Loss: 1.0580785274505615 | KNN Loss: 6.224627494812012 | BCE Loss: 1.0580785274505615\n",
      "Epoch 368 / 500 | iteration 5 / 30 | Total Loss: 1.0428669452667236 | KNN Loss: 6.224245548248291 | BCE Loss: 1.0428669452667236\n",
      "Epoch 368 / 500 | iteration 10 / 30 | Total Loss: 1.0534803867340088 | KNN Loss: 6.224277496337891 | BCE Loss: 1.0534803867340088\n",
      "Epoch 368 / 500 | iteration 15 / 30 | Total Loss: 1.0629565715789795 | KNN Loss: 6.224574089050293 | BCE Loss: 1.0629565715789795\n",
      "Epoch 368 / 500 | iteration 20 / 30 | Total Loss: 1.0492854118347168 | KNN Loss: 6.224584579467773 | BCE Loss: 1.0492854118347168\n",
      "Epoch 368 / 500 | iteration 25 / 30 | Total Loss: 1.0595943927764893 | KNN Loss: 6.224849224090576 | BCE Loss: 1.0595943927764893\n",
      "Epoch   369: reducing learning rate of group 0 to 2.2999e-07.\n",
      "Epoch 369 / 500 | iteration 0 / 30 | Total Loss: 1.0466201305389404 | KNN Loss: 6.224525451660156 | BCE Loss: 1.0466201305389404\n",
      "Epoch 369 / 500 | iteration 5 / 30 | Total Loss: 1.0445129871368408 | KNN Loss: 6.224709987640381 | BCE Loss: 1.0445129871368408\n",
      "Epoch 369 / 500 | iteration 10 / 30 | Total Loss: 1.0651168823242188 | KNN Loss: 6.22458553314209 | BCE Loss: 1.0651168823242188\n",
      "Epoch 369 / 500 | iteration 15 / 30 | Total Loss: 1.0434300899505615 | KNN Loss: 6.224568843841553 | BCE Loss: 1.0434300899505615\n",
      "Epoch 369 / 500 | iteration 20 / 30 | Total Loss: 1.03497314453125 | KNN Loss: 6.224394798278809 | BCE Loss: 1.03497314453125\n",
      "Epoch 369 / 500 | iteration 25 / 30 | Total Loss: 1.04780113697052 | KNN Loss: 6.224488735198975 | BCE Loss: 1.04780113697052\n",
      "Epoch 370 / 500 | iteration 0 / 30 | Total Loss: 1.0242514610290527 | KNN Loss: 6.224684715270996 | BCE Loss: 1.0242514610290527\n",
      "Epoch 370 / 500 | iteration 5 / 30 | Total Loss: 1.0389173030853271 | KNN Loss: 6.223999977111816 | BCE Loss: 1.0389173030853271\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 370 / 500 | iteration 10 / 30 | Total Loss: 1.0368140935897827 | KNN Loss: 6.2245283126831055 | BCE Loss: 1.0368140935897827\n",
      "Epoch 370 / 500 | iteration 15 / 30 | Total Loss: 1.059569001197815 | KNN Loss: 6.224441051483154 | BCE Loss: 1.059569001197815\n",
      "Epoch 370 / 500 | iteration 20 / 30 | Total Loss: 1.0367376804351807 | KNN Loss: 6.2246785163879395 | BCE Loss: 1.0367376804351807\n",
      "Epoch 370 / 500 | iteration 25 / 30 | Total Loss: 1.0563509464263916 | KNN Loss: 6.224264621734619 | BCE Loss: 1.0563509464263916\n",
      "Epoch 371 / 500 | iteration 0 / 30 | Total Loss: 1.0409603118896484 | KNN Loss: 6.224153995513916 | BCE Loss: 1.0409603118896484\n",
      "Epoch 371 / 500 | iteration 5 / 30 | Total Loss: 1.0537822246551514 | KNN Loss: 6.224803447723389 | BCE Loss: 1.0537822246551514\n",
      "Epoch 371 / 500 | iteration 10 / 30 | Total Loss: 1.0504586696624756 | KNN Loss: 6.224382400512695 | BCE Loss: 1.0504586696624756\n",
      "Epoch 371 / 500 | iteration 15 / 30 | Total Loss: 1.0511090755462646 | KNN Loss: 6.224651336669922 | BCE Loss: 1.0511090755462646\n",
      "Epoch 371 / 500 | iteration 20 / 30 | Total Loss: 1.071661114692688 | KNN Loss: 6.2245306968688965 | BCE Loss: 1.071661114692688\n",
      "Epoch 371 / 500 | iteration 25 / 30 | Total Loss: 1.0518468618392944 | KNN Loss: 6.224649429321289 | BCE Loss: 1.0518468618392944\n",
      "Epoch 372 / 500 | iteration 0 / 30 | Total Loss: 1.0726802349090576 | KNN Loss: 6.224714279174805 | BCE Loss: 1.0726802349090576\n",
      "Epoch 372 / 500 | iteration 5 / 30 | Total Loss: 1.027379035949707 | KNN Loss: 6.224393844604492 | BCE Loss: 1.027379035949707\n",
      "Epoch 372 / 500 | iteration 10 / 30 | Total Loss: 1.0364494323730469 | KNN Loss: 6.224318027496338 | BCE Loss: 1.0364494323730469\n",
      "Epoch 372 / 500 | iteration 15 / 30 | Total Loss: 1.0462698936462402 | KNN Loss: 6.224690914154053 | BCE Loss: 1.0462698936462402\n",
      "Epoch 372 / 500 | iteration 20 / 30 | Total Loss: 1.046470284461975 | KNN Loss: 6.225048065185547 | BCE Loss: 1.046470284461975\n",
      "Epoch 372 / 500 | iteration 25 / 30 | Total Loss: 1.0721231698989868 | KNN Loss: 6.224794864654541 | BCE Loss: 1.0721231698989868\n",
      "Epoch 373 / 500 | iteration 0 / 30 | Total Loss: 1.0564377307891846 | KNN Loss: 6.224486351013184 | BCE Loss: 1.0564377307891846\n",
      "Epoch 373 / 500 | iteration 5 / 30 | Total Loss: 1.0516979694366455 | KNN Loss: 6.2246599197387695 | BCE Loss: 1.0516979694366455\n",
      "Epoch 373 / 500 | iteration 10 / 30 | Total Loss: 1.035395860671997 | KNN Loss: 6.2242512702941895 | BCE Loss: 1.035395860671997\n",
      "Epoch 373 / 500 | iteration 15 / 30 | Total Loss: 1.0301302671432495 | KNN Loss: 6.224377632141113 | BCE Loss: 1.0301302671432495\n",
      "Epoch 373 / 500 | iteration 20 / 30 | Total Loss: 1.0468497276306152 | KNN Loss: 6.224700927734375 | BCE Loss: 1.0468497276306152\n",
      "Epoch 373 / 500 | iteration 25 / 30 | Total Loss: 1.064358115196228 | KNN Loss: 6.224883079528809 | BCE Loss: 1.064358115196228\n",
      "Epoch 374 / 500 | iteration 0 / 30 | Total Loss: 1.0837783813476562 | KNN Loss: 6.224295139312744 | BCE Loss: 1.0837783813476562\n",
      "Epoch 374 / 500 | iteration 5 / 30 | Total Loss: 1.0405495166778564 | KNN Loss: 6.224531650543213 | BCE Loss: 1.0405495166778564\n",
      "Epoch 374 / 500 | iteration 10 / 30 | Total Loss: 1.0461928844451904 | KNN Loss: 6.224405288696289 | BCE Loss: 1.0461928844451904\n",
      "Epoch 374 / 500 | iteration 15 / 30 | Total Loss: 1.0644924640655518 | KNN Loss: 6.224310398101807 | BCE Loss: 1.0644924640655518\n",
      "Epoch 374 / 500 | iteration 20 / 30 | Total Loss: 1.0486257076263428 | KNN Loss: 6.224381923675537 | BCE Loss: 1.0486257076263428\n",
      "Epoch 374 / 500 | iteration 25 / 30 | Total Loss: 1.0457656383514404 | KNN Loss: 6.22404670715332 | BCE Loss: 1.0457656383514404\n",
      "Epoch 375 / 500 | iteration 0 / 30 | Total Loss: 1.0798722505569458 | KNN Loss: 6.224255084991455 | BCE Loss: 1.0798722505569458\n",
      "Epoch 375 / 500 | iteration 5 / 30 | Total Loss: 1.0791239738464355 | KNN Loss: 6.224644660949707 | BCE Loss: 1.0791239738464355\n",
      "Epoch 375 / 500 | iteration 10 / 30 | Total Loss: 1.0624202489852905 | KNN Loss: 6.224334239959717 | BCE Loss: 1.0624202489852905\n",
      "Epoch 375 / 500 | iteration 15 / 30 | Total Loss: 1.0337802171707153 | KNN Loss: 6.224240303039551 | BCE Loss: 1.0337802171707153\n",
      "Epoch 375 / 500 | iteration 20 / 30 | Total Loss: 1.0600824356079102 | KNN Loss: 6.224203109741211 | BCE Loss: 1.0600824356079102\n",
      "Epoch 375 / 500 | iteration 25 / 30 | Total Loss: 1.0478346347808838 | KNN Loss: 6.224010467529297 | BCE Loss: 1.0478346347808838\n",
      "Epoch 376 / 500 | iteration 0 / 30 | Total Loss: 1.049396276473999 | KNN Loss: 6.224358558654785 | BCE Loss: 1.049396276473999\n",
      "Epoch 376 / 500 | iteration 5 / 30 | Total Loss: 1.0175977945327759 | KNN Loss: 6.2241435050964355 | BCE Loss: 1.0175977945327759\n",
      "Epoch 376 / 500 | iteration 10 / 30 | Total Loss: 1.0656766891479492 | KNN Loss: 6.22432279586792 | BCE Loss: 1.0656766891479492\n",
      "Epoch 376 / 500 | iteration 15 / 30 | Total Loss: 1.0563923120498657 | KNN Loss: 6.22430944442749 | BCE Loss: 1.0563923120498657\n",
      "Epoch 376 / 500 | iteration 20 / 30 | Total Loss: 1.0403025150299072 | KNN Loss: 6.2245588302612305 | BCE Loss: 1.0403025150299072\n",
      "Epoch 376 / 500 | iteration 25 / 30 | Total Loss: 1.0573362112045288 | KNN Loss: 6.22438383102417 | BCE Loss: 1.0573362112045288\n",
      "Epoch 377 / 500 | iteration 0 / 30 | Total Loss: 1.0180017948150635 | KNN Loss: 6.2241740226745605 | BCE Loss: 1.0180017948150635\n",
      "Epoch 377 / 500 | iteration 5 / 30 | Total Loss: 1.0414724349975586 | KNN Loss: 6.224560260772705 | BCE Loss: 1.0414724349975586\n",
      "Epoch 377 / 500 | iteration 10 / 30 | Total Loss: 1.0559501647949219 | KNN Loss: 6.224362850189209 | BCE Loss: 1.0559501647949219\n",
      "Epoch 377 / 500 | iteration 15 / 30 | Total Loss: 1.0494458675384521 | KNN Loss: 6.224617004394531 | BCE Loss: 1.0494458675384521\n",
      "Epoch 377 / 500 | iteration 20 / 30 | Total Loss: 1.031009554862976 | KNN Loss: 6.224171161651611 | BCE Loss: 1.031009554862976\n",
      "Epoch 377 / 500 | iteration 25 / 30 | Total Loss: 1.0526994466781616 | KNN Loss: 6.2242021560668945 | BCE Loss: 1.0526994466781616\n",
      "Epoch 378 / 500 | iteration 0 / 30 | Total Loss: 1.0428779125213623 | KNN Loss: 6.224764347076416 | BCE Loss: 1.0428779125213623\n",
      "Epoch 378 / 500 | iteration 5 / 30 | Total Loss: 1.0443155765533447 | KNN Loss: 6.224900722503662 | BCE Loss: 1.0443155765533447\n",
      "Epoch 378 / 500 | iteration 10 / 30 | Total Loss: 1.0609393119812012 | KNN Loss: 6.224336624145508 | BCE Loss: 1.0609393119812012\n",
      "Epoch 378 / 500 | iteration 15 / 30 | Total Loss: 1.065462350845337 | KNN Loss: 6.224173069000244 | BCE Loss: 1.065462350845337\n",
      "Epoch 378 / 500 | iteration 20 / 30 | Total Loss: 1.081956148147583 | KNN Loss: 6.224362850189209 | BCE Loss: 1.081956148147583\n",
      "Epoch 378 / 500 | iteration 25 / 30 | Total Loss: 1.0683174133300781 | KNN Loss: 6.224278926849365 | BCE Loss: 1.0683174133300781\n",
      "Epoch 379 / 500 | iteration 0 / 30 | Total Loss: 1.0530178546905518 | KNN Loss: 6.224459648132324 | BCE Loss: 1.0530178546905518\n",
      "Epoch 379 / 500 | iteration 5 / 30 | Total Loss: 1.0458881855010986 | KNN Loss: 6.224212646484375 | BCE Loss: 1.0458881855010986\n",
      "Epoch 379 / 500 | iteration 10 / 30 | Total Loss: 1.0849469900131226 | KNN Loss: 6.223902702331543 | BCE Loss: 1.0849469900131226\n",
      "Epoch 379 / 500 | iteration 15 / 30 | Total Loss: 1.0743623971939087 | KNN Loss: 6.224551677703857 | BCE Loss: 1.0743623971939087\n",
      "Epoch 379 / 500 | iteration 20 / 30 | Total Loss: 1.054706335067749 | KNN Loss: 6.22445011138916 | BCE Loss: 1.054706335067749\n",
      "Epoch 379 / 500 | iteration 25 / 30 | Total Loss: 1.0482215881347656 | KNN Loss: 6.224233627319336 | BCE Loss: 1.0482215881347656\n",
      "Epoch   380: reducing learning rate of group 0 to 1.6100e-07.\n",
      "Epoch 380 / 500 | iteration 0 / 30 | Total Loss: 1.061765193939209 | KNN Loss: 6.2245025634765625 | BCE Loss: 1.061765193939209\n",
      "Epoch 380 / 500 | iteration 5 / 30 | Total Loss: 1.0495272874832153 | KNN Loss: 6.224311828613281 | BCE Loss: 1.0495272874832153\n",
      "Epoch 380 / 500 | iteration 10 / 30 | Total Loss: 1.0541658401489258 | KNN Loss: 6.224368572235107 | BCE Loss: 1.0541658401489258\n",
      "Epoch 380 / 500 | iteration 15 / 30 | Total Loss: 1.058714747428894 | KNN Loss: 6.224063396453857 | BCE Loss: 1.058714747428894\n",
      "Epoch 380 / 500 | iteration 20 / 30 | Total Loss: 1.0223424434661865 | KNN Loss: 6.224724292755127 | BCE Loss: 1.0223424434661865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 380 / 500 | iteration 25 / 30 | Total Loss: 1.086454153060913 | KNN Loss: 6.224172115325928 | BCE Loss: 1.086454153060913\n",
      "Epoch 381 / 500 | iteration 0 / 30 | Total Loss: 1.0659332275390625 | KNN Loss: 6.224648475646973 | BCE Loss: 1.0659332275390625\n",
      "Epoch 381 / 500 | iteration 5 / 30 | Total Loss: 1.036665678024292 | KNN Loss: 6.224678039550781 | BCE Loss: 1.036665678024292\n",
      "Epoch 381 / 500 | iteration 10 / 30 | Total Loss: 1.0537291765213013 | KNN Loss: 6.22409200668335 | BCE Loss: 1.0537291765213013\n",
      "Epoch 381 / 500 | iteration 15 / 30 | Total Loss: 1.0460681915283203 | KNN Loss: 6.224681854248047 | BCE Loss: 1.0460681915283203\n",
      "Epoch 381 / 500 | iteration 20 / 30 | Total Loss: 1.090316891670227 | KNN Loss: 6.224748134613037 | BCE Loss: 1.090316891670227\n",
      "Epoch 381 / 500 | iteration 25 / 30 | Total Loss: 1.0223219394683838 | KNN Loss: 6.224374294281006 | BCE Loss: 1.0223219394683838\n",
      "Epoch 382 / 500 | iteration 0 / 30 | Total Loss: 1.0401746034622192 | KNN Loss: 6.224605560302734 | BCE Loss: 1.0401746034622192\n",
      "Epoch 382 / 500 | iteration 5 / 30 | Total Loss: 1.0552949905395508 | KNN Loss: 6.224431991577148 | BCE Loss: 1.0552949905395508\n",
      "Epoch 382 / 500 | iteration 10 / 30 | Total Loss: 1.0463109016418457 | KNN Loss: 6.224240779876709 | BCE Loss: 1.0463109016418457\n",
      "Epoch 382 / 500 | iteration 15 / 30 | Total Loss: 1.0398904085159302 | KNN Loss: 6.224240779876709 | BCE Loss: 1.0398904085159302\n",
      "Epoch 382 / 500 | iteration 20 / 30 | Total Loss: 1.0322788953781128 | KNN Loss: 6.224399089813232 | BCE Loss: 1.0322788953781128\n",
      "Epoch 382 / 500 | iteration 25 / 30 | Total Loss: 1.0525672435760498 | KNN Loss: 6.224681377410889 | BCE Loss: 1.0525672435760498\n",
      "Epoch 383 / 500 | iteration 0 / 30 | Total Loss: 1.056593656539917 | KNN Loss: 6.224251747131348 | BCE Loss: 1.056593656539917\n",
      "Epoch 383 / 500 | iteration 5 / 30 | Total Loss: 1.0782828330993652 | KNN Loss: 6.2248921394348145 | BCE Loss: 1.0782828330993652\n",
      "Epoch 383 / 500 | iteration 10 / 30 | Total Loss: 1.0432167053222656 | KNN Loss: 6.224116325378418 | BCE Loss: 1.0432167053222656\n",
      "Epoch 383 / 500 | iteration 15 / 30 | Total Loss: 1.0649522542953491 | KNN Loss: 6.2246904373168945 | BCE Loss: 1.0649522542953491\n",
      "Epoch 383 / 500 | iteration 20 / 30 | Total Loss: 1.0460439920425415 | KNN Loss: 6.224252700805664 | BCE Loss: 1.0460439920425415\n",
      "Epoch 383 / 500 | iteration 25 / 30 | Total Loss: 1.0765910148620605 | KNN Loss: 6.2243781089782715 | BCE Loss: 1.0765910148620605\n",
      "Epoch 384 / 500 | iteration 0 / 30 | Total Loss: 1.0410701036453247 | KNN Loss: 6.224478721618652 | BCE Loss: 1.0410701036453247\n",
      "Epoch 384 / 500 | iteration 5 / 30 | Total Loss: 1.0617446899414062 | KNN Loss: 6.224515914916992 | BCE Loss: 1.0617446899414062\n",
      "Epoch 384 / 500 | iteration 10 / 30 | Total Loss: 1.0438542366027832 | KNN Loss: 6.224453449249268 | BCE Loss: 1.0438542366027832\n",
      "Epoch 384 / 500 | iteration 15 / 30 | Total Loss: 1.02410089969635 | KNN Loss: 6.224326133728027 | BCE Loss: 1.02410089969635\n",
      "Epoch 384 / 500 | iteration 20 / 30 | Total Loss: 1.0486750602722168 | KNN Loss: 6.224358081817627 | BCE Loss: 1.0486750602722168\n",
      "Epoch 384 / 500 | iteration 25 / 30 | Total Loss: 1.0341541767120361 | KNN Loss: 6.223984241485596 | BCE Loss: 1.0341541767120361\n",
      "Epoch 385 / 500 | iteration 0 / 30 | Total Loss: 1.0475640296936035 | KNN Loss: 6.224467754364014 | BCE Loss: 1.0475640296936035\n",
      "Epoch 385 / 500 | iteration 5 / 30 | Total Loss: 1.0226128101348877 | KNN Loss: 6.224146842956543 | BCE Loss: 1.0226128101348877\n",
      "Epoch 385 / 500 | iteration 10 / 30 | Total Loss: 1.0361549854278564 | KNN Loss: 6.224416732788086 | BCE Loss: 1.0361549854278564\n",
      "Epoch 385 / 500 | iteration 15 / 30 | Total Loss: 1.0500770807266235 | KNN Loss: 6.224816799163818 | BCE Loss: 1.0500770807266235\n",
      "Epoch 385 / 500 | iteration 20 / 30 | Total Loss: 1.029041051864624 | KNN Loss: 6.2244343757629395 | BCE Loss: 1.029041051864624\n",
      "Epoch 385 / 500 | iteration 25 / 30 | Total Loss: 1.033743977546692 | KNN Loss: 6.224159240722656 | BCE Loss: 1.033743977546692\n",
      "Epoch 386 / 500 | iteration 0 / 30 | Total Loss: 1.0452806949615479 | KNN Loss: 6.224360942840576 | BCE Loss: 1.0452806949615479\n",
      "Epoch 386 / 500 | iteration 5 / 30 | Total Loss: 1.0266233682632446 | KNN Loss: 6.224597930908203 | BCE Loss: 1.0266233682632446\n",
      "Epoch 386 / 500 | iteration 10 / 30 | Total Loss: 1.0478984117507935 | KNN Loss: 6.224367141723633 | BCE Loss: 1.0478984117507935\n",
      "Epoch 386 / 500 | iteration 15 / 30 | Total Loss: 1.0369608402252197 | KNN Loss: 6.224452018737793 | BCE Loss: 1.0369608402252197\n",
      "Epoch 386 / 500 | iteration 20 / 30 | Total Loss: 1.0224769115447998 | KNN Loss: 6.224355697631836 | BCE Loss: 1.0224769115447998\n",
      "Epoch 386 / 500 | iteration 25 / 30 | Total Loss: 1.0749943256378174 | KNN Loss: 6.2244157791137695 | BCE Loss: 1.0749943256378174\n",
      "Epoch 387 / 500 | iteration 0 / 30 | Total Loss: 1.0453581809997559 | KNN Loss: 6.224382400512695 | BCE Loss: 1.0453581809997559\n",
      "Epoch 387 / 500 | iteration 5 / 30 | Total Loss: 1.0294668674468994 | KNN Loss: 6.224331855773926 | BCE Loss: 1.0294668674468994\n",
      "Epoch 387 / 500 | iteration 10 / 30 | Total Loss: 1.0607408285140991 | KNN Loss: 6.2245869636535645 | BCE Loss: 1.0607408285140991\n",
      "Epoch 387 / 500 | iteration 15 / 30 | Total Loss: 1.0443657636642456 | KNN Loss: 6.2247419357299805 | BCE Loss: 1.0443657636642456\n",
      "Epoch 387 / 500 | iteration 20 / 30 | Total Loss: 1.0643607378005981 | KNN Loss: 6.224563121795654 | BCE Loss: 1.0643607378005981\n",
      "Epoch 387 / 500 | iteration 25 / 30 | Total Loss: 1.0633883476257324 | KNN Loss: 6.224476337432861 | BCE Loss: 1.0633883476257324\n",
      "Epoch 388 / 500 | iteration 0 / 30 | Total Loss: 1.0804424285888672 | KNN Loss: 6.2242841720581055 | BCE Loss: 1.0804424285888672\n",
      "Epoch 388 / 500 | iteration 5 / 30 | Total Loss: 1.0404372215270996 | KNN Loss: 6.224453926086426 | BCE Loss: 1.0404372215270996\n",
      "Epoch 388 / 500 | iteration 10 / 30 | Total Loss: 1.0371382236480713 | KNN Loss: 6.224595069885254 | BCE Loss: 1.0371382236480713\n",
      "Epoch 388 / 500 | iteration 15 / 30 | Total Loss: 1.0708662271499634 | KNN Loss: 6.224656105041504 | BCE Loss: 1.0708662271499634\n",
      "Epoch 388 / 500 | iteration 20 / 30 | Total Loss: 1.0718423128128052 | KNN Loss: 6.224306106567383 | BCE Loss: 1.0718423128128052\n",
      "Epoch 388 / 500 | iteration 25 / 30 | Total Loss: 1.0949461460113525 | KNN Loss: 6.224274635314941 | BCE Loss: 1.0949461460113525\n",
      "Epoch 389 / 500 | iteration 0 / 30 | Total Loss: 1.039251685142517 | KNN Loss: 6.2237982749938965 | BCE Loss: 1.039251685142517\n",
      "Epoch 389 / 500 | iteration 5 / 30 | Total Loss: 1.0547025203704834 | KNN Loss: 6.22458028793335 | BCE Loss: 1.0547025203704834\n",
      "Epoch 389 / 500 | iteration 10 / 30 | Total Loss: 1.0382778644561768 | KNN Loss: 6.224677562713623 | BCE Loss: 1.0382778644561768\n",
      "Epoch 389 / 500 | iteration 15 / 30 | Total Loss: 1.061217188835144 | KNN Loss: 6.224254608154297 | BCE Loss: 1.061217188835144\n",
      "Epoch 389 / 500 | iteration 20 / 30 | Total Loss: 1.0532433986663818 | KNN Loss: 6.224706172943115 | BCE Loss: 1.0532433986663818\n",
      "Epoch 389 / 500 | iteration 25 / 30 | Total Loss: 1.0221288204193115 | KNN Loss: 6.224699020385742 | BCE Loss: 1.0221288204193115\n",
      "Epoch 390 / 500 | iteration 0 / 30 | Total Loss: 1.0476062297821045 | KNN Loss: 6.224360466003418 | BCE Loss: 1.0476062297821045\n",
      "Epoch 390 / 500 | iteration 5 / 30 | Total Loss: 1.0283589363098145 | KNN Loss: 6.22432279586792 | BCE Loss: 1.0283589363098145\n",
      "Epoch 390 / 500 | iteration 10 / 30 | Total Loss: 1.0480746030807495 | KNN Loss: 6.224142074584961 | BCE Loss: 1.0480746030807495\n",
      "Epoch 390 / 500 | iteration 15 / 30 | Total Loss: 1.0626813173294067 | KNN Loss: 6.224410057067871 | BCE Loss: 1.0626813173294067\n",
      "Epoch 390 / 500 | iteration 20 / 30 | Total Loss: 1.0329842567443848 | KNN Loss: 6.22442626953125 | BCE Loss: 1.0329842567443848\n",
      "Epoch 390 / 500 | iteration 25 / 30 | Total Loss: 1.0525912046432495 | KNN Loss: 6.2241435050964355 | BCE Loss: 1.0525912046432495\n",
      "Epoch   391: reducing learning rate of group 0 to 1.1270e-07.\n",
      "Epoch 391 / 500 | iteration 0 / 30 | Total Loss: 1.0506343841552734 | KNN Loss: 6.224447727203369 | BCE Loss: 1.0506343841552734\n",
      "Epoch 391 / 500 | iteration 5 / 30 | Total Loss: 1.0685679912567139 | KNN Loss: 6.224536895751953 | BCE Loss: 1.0685679912567139\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 391 / 500 | iteration 10 / 30 | Total Loss: 1.0403945446014404 | KNN Loss: 6.224269390106201 | BCE Loss: 1.0403945446014404\n",
      "Epoch 391 / 500 | iteration 15 / 30 | Total Loss: 1.0430713891983032 | KNN Loss: 6.224405288696289 | BCE Loss: 1.0430713891983032\n",
      "Epoch 391 / 500 | iteration 20 / 30 | Total Loss: 1.0586649179458618 | KNN Loss: 6.224427223205566 | BCE Loss: 1.0586649179458618\n",
      "Epoch 391 / 500 | iteration 25 / 30 | Total Loss: 1.07962167263031 | KNN Loss: 6.224656105041504 | BCE Loss: 1.07962167263031\n",
      "Epoch 392 / 500 | iteration 0 / 30 | Total Loss: 1.071535348892212 | KNN Loss: 6.2246503829956055 | BCE Loss: 1.071535348892212\n",
      "Epoch 392 / 500 | iteration 5 / 30 | Total Loss: 1.0645420551300049 | KNN Loss: 6.224515914916992 | BCE Loss: 1.0645420551300049\n",
      "Epoch 392 / 500 | iteration 10 / 30 | Total Loss: 1.0727369785308838 | KNN Loss: 6.224330425262451 | BCE Loss: 1.0727369785308838\n",
      "Epoch 392 / 500 | iteration 15 / 30 | Total Loss: 1.0274819135665894 | KNN Loss: 6.224188804626465 | BCE Loss: 1.0274819135665894\n",
      "Epoch 392 / 500 | iteration 20 / 30 | Total Loss: 1.0326621532440186 | KNN Loss: 6.224257469177246 | BCE Loss: 1.0326621532440186\n",
      "Epoch 392 / 500 | iteration 25 / 30 | Total Loss: 1.0425840616226196 | KNN Loss: 6.2241997718811035 | BCE Loss: 1.0425840616226196\n",
      "Epoch 393 / 500 | iteration 0 / 30 | Total Loss: 1.039597511291504 | KNN Loss: 6.224129676818848 | BCE Loss: 1.039597511291504\n",
      "Epoch 393 / 500 | iteration 5 / 30 | Total Loss: 1.0534820556640625 | KNN Loss: 6.224635601043701 | BCE Loss: 1.0534820556640625\n",
      "Epoch 393 / 500 | iteration 10 / 30 | Total Loss: 1.0481191873550415 | KNN Loss: 6.224554061889648 | BCE Loss: 1.0481191873550415\n",
      "Epoch 393 / 500 | iteration 15 / 30 | Total Loss: 1.0815916061401367 | KNN Loss: 6.2246413230896 | BCE Loss: 1.0815916061401367\n",
      "Epoch 393 / 500 | iteration 20 / 30 | Total Loss: 1.0181130170822144 | KNN Loss: 6.22431755065918 | BCE Loss: 1.0181130170822144\n",
      "Epoch 393 / 500 | iteration 25 / 30 | Total Loss: 1.0384016036987305 | KNN Loss: 6.224301815032959 | BCE Loss: 1.0384016036987305\n",
      "Epoch 394 / 500 | iteration 0 / 30 | Total Loss: 1.0559260845184326 | KNN Loss: 6.224493980407715 | BCE Loss: 1.0559260845184326\n",
      "Epoch 394 / 500 | iteration 5 / 30 | Total Loss: 1.0664441585540771 | KNN Loss: 6.224180698394775 | BCE Loss: 1.0664441585540771\n",
      "Epoch 394 / 500 | iteration 10 / 30 | Total Loss: 1.0467453002929688 | KNN Loss: 6.224625110626221 | BCE Loss: 1.0467453002929688\n",
      "Epoch 394 / 500 | iteration 15 / 30 | Total Loss: 1.0563600063323975 | KNN Loss: 6.2244391441345215 | BCE Loss: 1.0563600063323975\n",
      "Epoch 394 / 500 | iteration 20 / 30 | Total Loss: 1.0461866855621338 | KNN Loss: 6.224477291107178 | BCE Loss: 1.0461866855621338\n",
      "Epoch 394 / 500 | iteration 25 / 30 | Total Loss: 1.0454381704330444 | KNN Loss: 6.22434139251709 | BCE Loss: 1.0454381704330444\n",
      "Epoch 395 / 500 | iteration 0 / 30 | Total Loss: 1.0421745777130127 | KNN Loss: 6.224350452423096 | BCE Loss: 1.0421745777130127\n",
      "Epoch 395 / 500 | iteration 5 / 30 | Total Loss: 1.073998212814331 | KNN Loss: 6.224498748779297 | BCE Loss: 1.073998212814331\n",
      "Epoch 395 / 500 | iteration 10 / 30 | Total Loss: 1.045048713684082 | KNN Loss: 6.224545478820801 | BCE Loss: 1.045048713684082\n",
      "Epoch 395 / 500 | iteration 15 / 30 | Total Loss: 1.074404001235962 | KNN Loss: 6.224606037139893 | BCE Loss: 1.074404001235962\n",
      "Epoch 395 / 500 | iteration 20 / 30 | Total Loss: 1.0898501873016357 | KNN Loss: 6.22459602355957 | BCE Loss: 1.0898501873016357\n",
      "Epoch 395 / 500 | iteration 25 / 30 | Total Loss: 1.0465549230575562 | KNN Loss: 6.224218368530273 | BCE Loss: 1.0465549230575562\n",
      "Epoch 396 / 500 | iteration 0 / 30 | Total Loss: 1.051623821258545 | KNN Loss: 6.224391460418701 | BCE Loss: 1.051623821258545\n",
      "Epoch 396 / 500 | iteration 5 / 30 | Total Loss: 1.0769191980361938 | KNN Loss: 6.224420070648193 | BCE Loss: 1.0769191980361938\n",
      "Epoch 396 / 500 | iteration 10 / 30 | Total Loss: 1.0359433889389038 | KNN Loss: 6.22446346282959 | BCE Loss: 1.0359433889389038\n",
      "Epoch 396 / 500 | iteration 15 / 30 | Total Loss: 1.0391476154327393 | KNN Loss: 6.2243332862854 | BCE Loss: 1.0391476154327393\n",
      "Epoch 396 / 500 | iteration 20 / 30 | Total Loss: 1.03793466091156 | KNN Loss: 6.224583148956299 | BCE Loss: 1.03793466091156\n",
      "Epoch 396 / 500 | iteration 25 / 30 | Total Loss: 1.0533695220947266 | KNN Loss: 6.224305629730225 | BCE Loss: 1.0533695220947266\n",
      "Epoch 397 / 500 | iteration 0 / 30 | Total Loss: 1.0649590492248535 | KNN Loss: 6.223990440368652 | BCE Loss: 1.0649590492248535\n",
      "Epoch 397 / 500 | iteration 5 / 30 | Total Loss: 1.0876944065093994 | KNN Loss: 6.224506378173828 | BCE Loss: 1.0876944065093994\n",
      "Epoch 397 / 500 | iteration 10 / 30 | Total Loss: 1.0656192302703857 | KNN Loss: 6.224486827850342 | BCE Loss: 1.0656192302703857\n",
      "Epoch 397 / 500 | iteration 15 / 30 | Total Loss: 1.0404431819915771 | KNN Loss: 6.22450590133667 | BCE Loss: 1.0404431819915771\n",
      "Epoch 397 / 500 | iteration 20 / 30 | Total Loss: 1.0487462282180786 | KNN Loss: 6.224316596984863 | BCE Loss: 1.0487462282180786\n",
      "Epoch 397 / 500 | iteration 25 / 30 | Total Loss: 1.0707218647003174 | KNN Loss: 6.224252223968506 | BCE Loss: 1.0707218647003174\n",
      "Epoch 398 / 500 | iteration 0 / 30 | Total Loss: 1.0465495586395264 | KNN Loss: 6.224447250366211 | BCE Loss: 1.0465495586395264\n",
      "Epoch 398 / 500 | iteration 5 / 30 | Total Loss: 1.0659735202789307 | KNN Loss: 6.224394798278809 | BCE Loss: 1.0659735202789307\n",
      "Epoch 398 / 500 | iteration 10 / 30 | Total Loss: 1.027998447418213 | KNN Loss: 6.2245192527771 | BCE Loss: 1.027998447418213\n",
      "Epoch 398 / 500 | iteration 15 / 30 | Total Loss: 1.0525166988372803 | KNN Loss: 6.224429607391357 | BCE Loss: 1.0525166988372803\n",
      "Epoch 398 / 500 | iteration 20 / 30 | Total Loss: 1.0466935634613037 | KNN Loss: 6.224699974060059 | BCE Loss: 1.0466935634613037\n",
      "Epoch 398 / 500 | iteration 25 / 30 | Total Loss: 1.0516003370285034 | KNN Loss: 6.224459648132324 | BCE Loss: 1.0516003370285034\n",
      "Epoch 399 / 500 | iteration 0 / 30 | Total Loss: 1.0612156391143799 | KNN Loss: 6.224602699279785 | BCE Loss: 1.0612156391143799\n",
      "Epoch 399 / 500 | iteration 5 / 30 | Total Loss: 1.0601391792297363 | KNN Loss: 6.224244594573975 | BCE Loss: 1.0601391792297363\n",
      "Epoch 399 / 500 | iteration 10 / 30 | Total Loss: 1.0336188077926636 | KNN Loss: 6.224636077880859 | BCE Loss: 1.0336188077926636\n",
      "Epoch 399 / 500 | iteration 15 / 30 | Total Loss: 1.0961164236068726 | KNN Loss: 6.224826335906982 | BCE Loss: 1.0961164236068726\n",
      "Epoch 399 / 500 | iteration 20 / 30 | Total Loss: 1.0331599712371826 | KNN Loss: 6.224403381347656 | BCE Loss: 1.0331599712371826\n",
      "Epoch 399 / 500 | iteration 25 / 30 | Total Loss: 1.0477890968322754 | KNN Loss: 6.224611282348633 | BCE Loss: 1.0477890968322754\n",
      "Epoch 400 / 500 | iteration 0 / 30 | Total Loss: 1.048525094985962 | KNN Loss: 6.224135398864746 | BCE Loss: 1.048525094985962\n",
      "Epoch 400 / 500 | iteration 5 / 30 | Total Loss: 1.038217306137085 | KNN Loss: 6.224752902984619 | BCE Loss: 1.038217306137085\n",
      "Epoch 400 / 500 | iteration 10 / 30 | Total Loss: 1.0622614622116089 | KNN Loss: 6.224691390991211 | BCE Loss: 1.0622614622116089\n",
      "Epoch 400 / 500 | iteration 15 / 30 | Total Loss: 1.0542340278625488 | KNN Loss: 6.224085330963135 | BCE Loss: 1.0542340278625488\n",
      "Epoch 400 / 500 | iteration 20 / 30 | Total Loss: 1.0796129703521729 | KNN Loss: 6.224221229553223 | BCE Loss: 1.0796129703521729\n",
      "Epoch 400 / 500 | iteration 25 / 30 | Total Loss: 1.0351600646972656 | KNN Loss: 6.2248311042785645 | BCE Loss: 1.0351600646972656\n",
      "Epoch 401 / 500 | iteration 0 / 30 | Total Loss: 1.0143402814865112 | KNN Loss: 6.224604606628418 | BCE Loss: 1.0143402814865112\n",
      "Epoch 401 / 500 | iteration 5 / 30 | Total Loss: 1.0700563192367554 | KNN Loss: 6.22455358505249 | BCE Loss: 1.0700563192367554\n",
      "Epoch 401 / 500 | iteration 10 / 30 | Total Loss: 1.0891567468643188 | KNN Loss: 6.224414348602295 | BCE Loss: 1.0891567468643188\n",
      "Epoch 401 / 500 | iteration 15 / 30 | Total Loss: 1.0586755275726318 | KNN Loss: 6.224538803100586 | BCE Loss: 1.0586755275726318\n",
      "Epoch 401 / 500 | iteration 20 / 30 | Total Loss: 1.0447537899017334 | KNN Loss: 6.2245683670043945 | BCE Loss: 1.0447537899017334\n",
      "Epoch 401 / 500 | iteration 25 / 30 | Total Loss: 1.028425931930542 | KNN Loss: 6.224109649658203 | BCE Loss: 1.028425931930542\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   402: reducing learning rate of group 0 to 7.8888e-08.\n",
      "Epoch 402 / 500 | iteration 0 / 30 | Total Loss: 1.0489351749420166 | KNN Loss: 6.224586009979248 | BCE Loss: 1.0489351749420166\n",
      "Epoch 402 / 500 | iteration 5 / 30 | Total Loss: 1.0453919172286987 | KNN Loss: 6.224823474884033 | BCE Loss: 1.0453919172286987\n",
      "Epoch 402 / 500 | iteration 10 / 30 | Total Loss: 1.053794026374817 | KNN Loss: 6.224209308624268 | BCE Loss: 1.053794026374817\n",
      "Epoch 402 / 500 | iteration 15 / 30 | Total Loss: 1.0373785495758057 | KNN Loss: 6.224127769470215 | BCE Loss: 1.0373785495758057\n",
      "Epoch 402 / 500 | iteration 20 / 30 | Total Loss: 1.0833402872085571 | KNN Loss: 6.224686622619629 | BCE Loss: 1.0833402872085571\n",
      "Epoch 402 / 500 | iteration 25 / 30 | Total Loss: 1.0511353015899658 | KNN Loss: 6.224090576171875 | BCE Loss: 1.0511353015899658\n",
      "Epoch 403 / 500 | iteration 0 / 30 | Total Loss: 1.047454595565796 | KNN Loss: 6.224590301513672 | BCE Loss: 1.047454595565796\n",
      "Epoch 403 / 500 | iteration 5 / 30 | Total Loss: 1.0589566230773926 | KNN Loss: 6.224555015563965 | BCE Loss: 1.0589566230773926\n",
      "Epoch 403 / 500 | iteration 10 / 30 | Total Loss: 1.0581594705581665 | KNN Loss: 6.224597454071045 | BCE Loss: 1.0581594705581665\n",
      "Epoch 403 / 500 | iteration 15 / 30 | Total Loss: 1.028496503829956 | KNN Loss: 6.224523067474365 | BCE Loss: 1.028496503829956\n",
      "Epoch 403 / 500 | iteration 20 / 30 | Total Loss: 1.0345168113708496 | KNN Loss: 6.2243852615356445 | BCE Loss: 1.0345168113708496\n",
      "Epoch 403 / 500 | iteration 25 / 30 | Total Loss: 1.0611809492111206 | KNN Loss: 6.224113464355469 | BCE Loss: 1.0611809492111206\n",
      "Epoch 404 / 500 | iteration 0 / 30 | Total Loss: 1.0178465843200684 | KNN Loss: 6.224213600158691 | BCE Loss: 1.0178465843200684\n",
      "Epoch 404 / 500 | iteration 5 / 30 | Total Loss: 1.019873857498169 | KNN Loss: 6.224693775177002 | BCE Loss: 1.019873857498169\n",
      "Epoch 404 / 500 | iteration 10 / 30 | Total Loss: 1.0601747035980225 | KNN Loss: 6.224536895751953 | BCE Loss: 1.0601747035980225\n",
      "Epoch 404 / 500 | iteration 15 / 30 | Total Loss: 1.0415794849395752 | KNN Loss: 6.224959850311279 | BCE Loss: 1.0415794849395752\n",
      "Epoch 404 / 500 | iteration 20 / 30 | Total Loss: 1.0471336841583252 | KNN Loss: 6.224275588989258 | BCE Loss: 1.0471336841583252\n",
      "Epoch 404 / 500 | iteration 25 / 30 | Total Loss: 1.0779030323028564 | KNN Loss: 6.224660396575928 | BCE Loss: 1.0779030323028564\n",
      "Epoch 405 / 500 | iteration 0 / 30 | Total Loss: 1.05531644821167 | KNN Loss: 6.2247233390808105 | BCE Loss: 1.05531644821167\n",
      "Epoch 405 / 500 | iteration 5 / 30 | Total Loss: 1.062563419342041 | KNN Loss: 6.224503517150879 | BCE Loss: 1.062563419342041\n",
      "Epoch 405 / 500 | iteration 10 / 30 | Total Loss: 1.0445480346679688 | KNN Loss: 6.224767684936523 | BCE Loss: 1.0445480346679688\n",
      "Epoch 405 / 500 | iteration 15 / 30 | Total Loss: 1.035574197769165 | KNN Loss: 6.2242255210876465 | BCE Loss: 1.035574197769165\n",
      "Epoch 405 / 500 | iteration 20 / 30 | Total Loss: 1.0515201091766357 | KNN Loss: 6.224279880523682 | BCE Loss: 1.0515201091766357\n",
      "Epoch 405 / 500 | iteration 25 / 30 | Total Loss: 1.0556604862213135 | KNN Loss: 6.224761486053467 | BCE Loss: 1.0556604862213135\n",
      "Epoch 406 / 500 | iteration 0 / 30 | Total Loss: 1.04011070728302 | KNN Loss: 6.224116802215576 | BCE Loss: 1.04011070728302\n",
      "Epoch 406 / 500 | iteration 5 / 30 | Total Loss: 1.0426294803619385 | KNN Loss: 6.224159240722656 | BCE Loss: 1.0426294803619385\n",
      "Epoch 406 / 500 | iteration 10 / 30 | Total Loss: 1.0470921993255615 | KNN Loss: 6.224301815032959 | BCE Loss: 1.0470921993255615\n",
      "Epoch 406 / 500 | iteration 15 / 30 | Total Loss: 1.0821661949157715 | KNN Loss: 6.224184989929199 | BCE Loss: 1.0821661949157715\n",
      "Epoch 406 / 500 | iteration 20 / 30 | Total Loss: 1.0529272556304932 | KNN Loss: 6.22442102432251 | BCE Loss: 1.0529272556304932\n",
      "Epoch 406 / 500 | iteration 25 / 30 | Total Loss: 1.0723810195922852 | KNN Loss: 6.224456310272217 | BCE Loss: 1.0723810195922852\n",
      "Epoch 407 / 500 | iteration 0 / 30 | Total Loss: 1.0612094402313232 | KNN Loss: 6.224755764007568 | BCE Loss: 1.0612094402313232\n",
      "Epoch 407 / 500 | iteration 5 / 30 | Total Loss: 1.0467755794525146 | KNN Loss: 6.224391937255859 | BCE Loss: 1.0467755794525146\n",
      "Epoch 407 / 500 | iteration 10 / 30 | Total Loss: 1.0649962425231934 | KNN Loss: 6.223982334136963 | BCE Loss: 1.0649962425231934\n",
      "Epoch 407 / 500 | iteration 15 / 30 | Total Loss: 1.0540138483047485 | KNN Loss: 6.224629878997803 | BCE Loss: 1.0540138483047485\n",
      "Epoch 407 / 500 | iteration 20 / 30 | Total Loss: 1.0450470447540283 | KNN Loss: 6.224602222442627 | BCE Loss: 1.0450470447540283\n",
      "Epoch 407 / 500 | iteration 25 / 30 | Total Loss: 1.0493957996368408 | KNN Loss: 6.224369525909424 | BCE Loss: 1.0493957996368408\n",
      "Epoch 408 / 500 | iteration 0 / 30 | Total Loss: 1.0740959644317627 | KNN Loss: 6.224567413330078 | BCE Loss: 1.0740959644317627\n",
      "Epoch 408 / 500 | iteration 5 / 30 | Total Loss: 1.0607250928878784 | KNN Loss: 6.224584102630615 | BCE Loss: 1.0607250928878784\n",
      "Epoch 408 / 500 | iteration 10 / 30 | Total Loss: 1.058058261871338 | KNN Loss: 6.22421932220459 | BCE Loss: 1.058058261871338\n",
      "Epoch 408 / 500 | iteration 15 / 30 | Total Loss: 1.070465087890625 | KNN Loss: 6.224594593048096 | BCE Loss: 1.070465087890625\n",
      "Epoch 408 / 500 | iteration 20 / 30 | Total Loss: 1.0195350646972656 | KNN Loss: 6.22468900680542 | BCE Loss: 1.0195350646972656\n",
      "Epoch 408 / 500 | iteration 25 / 30 | Total Loss: 1.0644522905349731 | KNN Loss: 6.224380970001221 | BCE Loss: 1.0644522905349731\n",
      "Epoch 409 / 500 | iteration 0 / 30 | Total Loss: 1.0227241516113281 | KNN Loss: 6.224542140960693 | BCE Loss: 1.0227241516113281\n",
      "Epoch 409 / 500 | iteration 5 / 30 | Total Loss: 1.0450130701065063 | KNN Loss: 6.224432945251465 | BCE Loss: 1.0450130701065063\n",
      "Epoch 409 / 500 | iteration 10 / 30 | Total Loss: 1.0623704195022583 | KNN Loss: 6.22451639175415 | BCE Loss: 1.0623704195022583\n",
      "Epoch 409 / 500 | iteration 15 / 30 | Total Loss: 1.037773609161377 | KNN Loss: 6.224452972412109 | BCE Loss: 1.037773609161377\n",
      "Epoch 409 / 500 | iteration 20 / 30 | Total Loss: 1.025280475616455 | KNN Loss: 6.224456787109375 | BCE Loss: 1.025280475616455\n",
      "Epoch 409 / 500 | iteration 25 / 30 | Total Loss: 1.057812213897705 | KNN Loss: 6.224359512329102 | BCE Loss: 1.057812213897705\n",
      "Epoch 410 / 500 | iteration 0 / 30 | Total Loss: 1.0384125709533691 | KNN Loss: 6.224395275115967 | BCE Loss: 1.0384125709533691\n",
      "Epoch 410 / 500 | iteration 5 / 30 | Total Loss: 1.0402424335479736 | KNN Loss: 6.224702835083008 | BCE Loss: 1.0402424335479736\n",
      "Epoch 410 / 500 | iteration 10 / 30 | Total Loss: 1.0958755016326904 | KNN Loss: 6.224328994750977 | BCE Loss: 1.0958755016326904\n",
      "Epoch 410 / 500 | iteration 15 / 30 | Total Loss: 1.0265854597091675 | KNN Loss: 6.224551200866699 | BCE Loss: 1.0265854597091675\n",
      "Epoch 410 / 500 | iteration 20 / 30 | Total Loss: 1.047973394393921 | KNN Loss: 6.22469425201416 | BCE Loss: 1.047973394393921\n",
      "Epoch 410 / 500 | iteration 25 / 30 | Total Loss: 1.0662145614624023 | KNN Loss: 6.224695205688477 | BCE Loss: 1.0662145614624023\n",
      "Epoch 411 / 500 | iteration 0 / 30 | Total Loss: 1.0367052555084229 | KNN Loss: 6.224400997161865 | BCE Loss: 1.0367052555084229\n",
      "Epoch 411 / 500 | iteration 5 / 30 | Total Loss: 1.0461976528167725 | KNN Loss: 6.224047660827637 | BCE Loss: 1.0461976528167725\n",
      "Epoch 411 / 500 | iteration 10 / 30 | Total Loss: 1.0110797882080078 | KNN Loss: 6.2243499755859375 | BCE Loss: 1.0110797882080078\n",
      "Epoch 411 / 500 | iteration 15 / 30 | Total Loss: 1.0374677181243896 | KNN Loss: 6.224534511566162 | BCE Loss: 1.0374677181243896\n",
      "Epoch 411 / 500 | iteration 20 / 30 | Total Loss: 1.0478246212005615 | KNN Loss: 6.224527835845947 | BCE Loss: 1.0478246212005615\n",
      "Epoch 411 / 500 | iteration 25 / 30 | Total Loss: 1.054548740386963 | KNN Loss: 6.224433898925781 | BCE Loss: 1.054548740386963\n",
      "Epoch 412 / 500 | iteration 0 / 30 | Total Loss: 1.0507222414016724 | KNN Loss: 6.224128246307373 | BCE Loss: 1.0507222414016724\n",
      "Epoch 412 / 500 | iteration 5 / 30 | Total Loss: 1.0624600648880005 | KNN Loss: 6.2243876457214355 | BCE Loss: 1.0624600648880005\n",
      "Epoch 412 / 500 | iteration 10 / 30 | Total Loss: 1.0723929405212402 | KNN Loss: 6.224484443664551 | BCE Loss: 1.0723929405212402\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 412 / 500 | iteration 15 / 30 | Total Loss: 1.0403016805648804 | KNN Loss: 6.22496223449707 | BCE Loss: 1.0403016805648804\n",
      "Epoch 412 / 500 | iteration 20 / 30 | Total Loss: 1.0390263795852661 | KNN Loss: 6.224148750305176 | BCE Loss: 1.0390263795852661\n",
      "Epoch 412 / 500 | iteration 25 / 30 | Total Loss: 1.0590797662734985 | KNN Loss: 6.224576473236084 | BCE Loss: 1.0590797662734985\n",
      "Epoch   413: reducing learning rate of group 0 to 5.5221e-08.\n",
      "Epoch 413 / 500 | iteration 0 / 30 | Total Loss: 1.071245551109314 | KNN Loss: 6.224816799163818 | BCE Loss: 1.071245551109314\n",
      "Epoch 413 / 500 | iteration 5 / 30 | Total Loss: 1.0546715259552002 | KNN Loss: 6.224415302276611 | BCE Loss: 1.0546715259552002\n",
      "Epoch 413 / 500 | iteration 10 / 30 | Total Loss: 1.034571886062622 | KNN Loss: 6.224503517150879 | BCE Loss: 1.034571886062622\n",
      "Epoch 413 / 500 | iteration 15 / 30 | Total Loss: 1.034019947052002 | KNN Loss: 6.224691867828369 | BCE Loss: 1.034019947052002\n",
      "Epoch 413 / 500 | iteration 20 / 30 | Total Loss: 1.046774983406067 | KNN Loss: 6.224556922912598 | BCE Loss: 1.046774983406067\n",
      "Epoch 413 / 500 | iteration 25 / 30 | Total Loss: 1.0561304092407227 | KNN Loss: 6.224583148956299 | BCE Loss: 1.0561304092407227\n",
      "Epoch 414 / 500 | iteration 0 / 30 | Total Loss: 1.0506199598312378 | KNN Loss: 6.224442481994629 | BCE Loss: 1.0506199598312378\n",
      "Epoch 414 / 500 | iteration 5 / 30 | Total Loss: 1.0397593975067139 | KNN Loss: 6.224461555480957 | BCE Loss: 1.0397593975067139\n",
      "Epoch 414 / 500 | iteration 10 / 30 | Total Loss: 1.0593639612197876 | KNN Loss: 6.22440767288208 | BCE Loss: 1.0593639612197876\n",
      "Epoch 414 / 500 | iteration 15 / 30 | Total Loss: 1.084824800491333 | KNN Loss: 6.2247161865234375 | BCE Loss: 1.084824800491333\n",
      "Epoch 414 / 500 | iteration 20 / 30 | Total Loss: 1.0461643934249878 | KNN Loss: 6.2241435050964355 | BCE Loss: 1.0461643934249878\n",
      "Epoch 414 / 500 | iteration 25 / 30 | Total Loss: 1.031994342803955 | KNN Loss: 6.224515438079834 | BCE Loss: 1.031994342803955\n",
      "Epoch 415 / 500 | iteration 0 / 30 | Total Loss: 1.0648808479309082 | KNN Loss: 6.224269390106201 | BCE Loss: 1.0648808479309082\n",
      "Epoch 415 / 500 | iteration 5 / 30 | Total Loss: 1.0353307723999023 | KNN Loss: 6.224273204803467 | BCE Loss: 1.0353307723999023\n",
      "Epoch 415 / 500 | iteration 10 / 30 | Total Loss: 1.0678790807724 | KNN Loss: 6.224640369415283 | BCE Loss: 1.0678790807724\n",
      "Epoch 415 / 500 | iteration 15 / 30 | Total Loss: 1.0548810958862305 | KNN Loss: 6.224647521972656 | BCE Loss: 1.0548810958862305\n",
      "Epoch 415 / 500 | iteration 20 / 30 | Total Loss: 1.0409586429595947 | KNN Loss: 6.224709987640381 | BCE Loss: 1.0409586429595947\n",
      "Epoch 415 / 500 | iteration 25 / 30 | Total Loss: 1.0122179985046387 | KNN Loss: 6.224542617797852 | BCE Loss: 1.0122179985046387\n",
      "Epoch 416 / 500 | iteration 0 / 30 | Total Loss: 1.047135353088379 | KNN Loss: 6.2247419357299805 | BCE Loss: 1.047135353088379\n",
      "Epoch 416 / 500 | iteration 5 / 30 | Total Loss: 1.0481257438659668 | KNN Loss: 6.224725723266602 | BCE Loss: 1.0481257438659668\n",
      "Epoch 416 / 500 | iteration 10 / 30 | Total Loss: 1.038438320159912 | KNN Loss: 6.224833011627197 | BCE Loss: 1.038438320159912\n",
      "Epoch 416 / 500 | iteration 15 / 30 | Total Loss: 1.0585447549819946 | KNN Loss: 6.2245635986328125 | BCE Loss: 1.0585447549819946\n",
      "Epoch 416 / 500 | iteration 20 / 30 | Total Loss: 1.05629301071167 | KNN Loss: 6.22477388381958 | BCE Loss: 1.05629301071167\n",
      "Epoch 416 / 500 | iteration 25 / 30 | Total Loss: 1.0542316436767578 | KNN Loss: 6.224472999572754 | BCE Loss: 1.0542316436767578\n",
      "Epoch 417 / 500 | iteration 0 / 30 | Total Loss: 1.029797077178955 | KNN Loss: 6.224354267120361 | BCE Loss: 1.029797077178955\n",
      "Epoch 417 / 500 | iteration 5 / 30 | Total Loss: 1.0374460220336914 | KNN Loss: 6.22453498840332 | BCE Loss: 1.0374460220336914\n",
      "Epoch 417 / 500 | iteration 10 / 30 | Total Loss: 1.0548295974731445 | KNN Loss: 6.224237442016602 | BCE Loss: 1.0548295974731445\n",
      "Epoch 417 / 500 | iteration 15 / 30 | Total Loss: 1.0993828773498535 | KNN Loss: 6.2246623039245605 | BCE Loss: 1.0993828773498535\n",
      "Epoch 417 / 500 | iteration 20 / 30 | Total Loss: 1.0528091192245483 | KNN Loss: 6.2241902351379395 | BCE Loss: 1.0528091192245483\n",
      "Epoch 417 / 500 | iteration 25 / 30 | Total Loss: 1.052314043045044 | KNN Loss: 6.224478721618652 | BCE Loss: 1.052314043045044\n",
      "Epoch 418 / 500 | iteration 0 / 30 | Total Loss: 1.0751374959945679 | KNN Loss: 6.224845886230469 | BCE Loss: 1.0751374959945679\n",
      "Epoch 418 / 500 | iteration 5 / 30 | Total Loss: 1.0440518856048584 | KNN Loss: 6.22464656829834 | BCE Loss: 1.0440518856048584\n",
      "Epoch 418 / 500 | iteration 10 / 30 | Total Loss: 1.0312550067901611 | KNN Loss: 6.224518299102783 | BCE Loss: 1.0312550067901611\n",
      "Epoch 418 / 500 | iteration 15 / 30 | Total Loss: 1.0621274709701538 | KNN Loss: 6.224349021911621 | BCE Loss: 1.0621274709701538\n",
      "Epoch 418 / 500 | iteration 20 / 30 | Total Loss: 1.052358865737915 | KNN Loss: 6.224292755126953 | BCE Loss: 1.052358865737915\n",
      "Epoch 418 / 500 | iteration 25 / 30 | Total Loss: 1.0836067199707031 | KNN Loss: 6.224201202392578 | BCE Loss: 1.0836067199707031\n",
      "Epoch 419 / 500 | iteration 0 / 30 | Total Loss: 1.0693100690841675 | KNN Loss: 6.224516868591309 | BCE Loss: 1.0693100690841675\n",
      "Epoch 419 / 500 | iteration 5 / 30 | Total Loss: 1.0589734315872192 | KNN Loss: 6.224598407745361 | BCE Loss: 1.0589734315872192\n",
      "Epoch 419 / 500 | iteration 10 / 30 | Total Loss: 1.0430450439453125 | KNN Loss: 6.224147319793701 | BCE Loss: 1.0430450439453125\n",
      "Epoch 419 / 500 | iteration 15 / 30 | Total Loss: 1.0606584548950195 | KNN Loss: 6.224182605743408 | BCE Loss: 1.0606584548950195\n",
      "Epoch 419 / 500 | iteration 20 / 30 | Total Loss: 1.0548827648162842 | KNN Loss: 6.224174499511719 | BCE Loss: 1.0548827648162842\n",
      "Epoch 419 / 500 | iteration 25 / 30 | Total Loss: 1.049339771270752 | KNN Loss: 6.224386215209961 | BCE Loss: 1.049339771270752\n",
      "Epoch 420 / 500 | iteration 0 / 30 | Total Loss: 1.0412770509719849 | KNN Loss: 6.224799156188965 | BCE Loss: 1.0412770509719849\n",
      "Epoch 420 / 500 | iteration 5 / 30 | Total Loss: 1.0432337522506714 | KNN Loss: 6.224349021911621 | BCE Loss: 1.0432337522506714\n",
      "Epoch 420 / 500 | iteration 10 / 30 | Total Loss: 1.0378953218460083 | KNN Loss: 6.224271774291992 | BCE Loss: 1.0378953218460083\n",
      "Epoch 420 / 500 | iteration 15 / 30 | Total Loss: 1.0096396207809448 | KNN Loss: 6.224239826202393 | BCE Loss: 1.0096396207809448\n",
      "Epoch 420 / 500 | iteration 20 / 30 | Total Loss: 1.0324413776397705 | KNN Loss: 6.2245659828186035 | BCE Loss: 1.0324413776397705\n",
      "Epoch 420 / 500 | iteration 25 / 30 | Total Loss: 1.0425336360931396 | KNN Loss: 6.224539279937744 | BCE Loss: 1.0425336360931396\n",
      "Epoch 421 / 500 | iteration 0 / 30 | Total Loss: 1.0523576736450195 | KNN Loss: 6.224748134613037 | BCE Loss: 1.0523576736450195\n",
      "Epoch 421 / 500 | iteration 5 / 30 | Total Loss: 1.0558216571807861 | KNN Loss: 6.224697113037109 | BCE Loss: 1.0558216571807861\n",
      "Epoch 421 / 500 | iteration 10 / 30 | Total Loss: 1.048412561416626 | KNN Loss: 6.224811553955078 | BCE Loss: 1.048412561416626\n",
      "Epoch 421 / 500 | iteration 15 / 30 | Total Loss: 1.0347919464111328 | KNN Loss: 6.224262237548828 | BCE Loss: 1.0347919464111328\n",
      "Epoch 421 / 500 | iteration 20 / 30 | Total Loss: 1.0542744398117065 | KNN Loss: 6.224610805511475 | BCE Loss: 1.0542744398117065\n",
      "Epoch 421 / 500 | iteration 25 / 30 | Total Loss: 1.066838026046753 | KNN Loss: 6.224398136138916 | BCE Loss: 1.066838026046753\n",
      "Epoch 422 / 500 | iteration 0 / 30 | Total Loss: 1.0670744180679321 | KNN Loss: 6.224297523498535 | BCE Loss: 1.0670744180679321\n",
      "Epoch 422 / 500 | iteration 5 / 30 | Total Loss: 1.0571191310882568 | KNN Loss: 6.224472522735596 | BCE Loss: 1.0571191310882568\n",
      "Epoch 422 / 500 | iteration 10 / 30 | Total Loss: 1.0233430862426758 | KNN Loss: 6.224475383758545 | BCE Loss: 1.0233430862426758\n",
      "Epoch 422 / 500 | iteration 15 / 30 | Total Loss: 1.0719637870788574 | KNN Loss: 6.224245071411133 | BCE Loss: 1.0719637870788574\n",
      "Epoch 422 / 500 | iteration 20 / 30 | Total Loss: 1.0432778596878052 | KNN Loss: 6.224460124969482 | BCE Loss: 1.0432778596878052\n",
      "Epoch 422 / 500 | iteration 25 / 30 | Total Loss: 1.057408332824707 | KNN Loss: 6.224699974060059 | BCE Loss: 1.057408332824707\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 423 / 500 | iteration 0 / 30 | Total Loss: 1.0519174337387085 | KNN Loss: 6.2244672775268555 | BCE Loss: 1.0519174337387085\n",
      "Epoch 423 / 500 | iteration 5 / 30 | Total Loss: 1.0655481815338135 | KNN Loss: 6.2244415283203125 | BCE Loss: 1.0655481815338135\n",
      "Epoch 423 / 500 | iteration 10 / 30 | Total Loss: 1.0739216804504395 | KNN Loss: 6.224446773529053 | BCE Loss: 1.0739216804504395\n",
      "Epoch 423 / 500 | iteration 15 / 30 | Total Loss: 1.022998332977295 | KNN Loss: 6.224568843841553 | BCE Loss: 1.022998332977295\n",
      "Epoch 423 / 500 | iteration 20 / 30 | Total Loss: 1.0214616060256958 | KNN Loss: 6.224155426025391 | BCE Loss: 1.0214616060256958\n",
      "Epoch 423 / 500 | iteration 25 / 30 | Total Loss: 1.0822575092315674 | KNN Loss: 6.224652290344238 | BCE Loss: 1.0822575092315674\n",
      "Epoch   424: reducing learning rate of group 0 to 3.8655e-08.\n",
      "Epoch 424 / 500 | iteration 0 / 30 | Total Loss: 1.0564122200012207 | KNN Loss: 6.224653244018555 | BCE Loss: 1.0564122200012207\n",
      "Epoch 424 / 500 | iteration 5 / 30 | Total Loss: 1.052987813949585 | KNN Loss: 6.224745273590088 | BCE Loss: 1.052987813949585\n",
      "Epoch 424 / 500 | iteration 10 / 30 | Total Loss: 1.044025182723999 | KNN Loss: 6.224054336547852 | BCE Loss: 1.044025182723999\n",
      "Epoch 424 / 500 | iteration 15 / 30 | Total Loss: 1.0403625965118408 | KNN Loss: 6.224698066711426 | BCE Loss: 1.0403625965118408\n",
      "Epoch 424 / 500 | iteration 20 / 30 | Total Loss: 1.057763934135437 | KNN Loss: 6.224608898162842 | BCE Loss: 1.057763934135437\n",
      "Epoch 424 / 500 | iteration 25 / 30 | Total Loss: 1.082448959350586 | KNN Loss: 6.224308967590332 | BCE Loss: 1.082448959350586\n",
      "Epoch 425 / 500 | iteration 0 / 30 | Total Loss: 1.0362966060638428 | KNN Loss: 6.224236965179443 | BCE Loss: 1.0362966060638428\n",
      "Epoch 425 / 500 | iteration 5 / 30 | Total Loss: 1.0718311071395874 | KNN Loss: 6.224567890167236 | BCE Loss: 1.0718311071395874\n",
      "Epoch 425 / 500 | iteration 10 / 30 | Total Loss: 1.0508683919906616 | KNN Loss: 6.224540710449219 | BCE Loss: 1.0508683919906616\n",
      "Epoch 425 / 500 | iteration 15 / 30 | Total Loss: 1.039737582206726 | KNN Loss: 6.224222660064697 | BCE Loss: 1.039737582206726\n",
      "Epoch 425 / 500 | iteration 20 / 30 | Total Loss: 1.0561426877975464 | KNN Loss: 6.224496364593506 | BCE Loss: 1.0561426877975464\n",
      "Epoch 425 / 500 | iteration 25 / 30 | Total Loss: 1.0566308498382568 | KNN Loss: 6.224429130554199 | BCE Loss: 1.0566308498382568\n",
      "Epoch 426 / 500 | iteration 0 / 30 | Total Loss: 1.044966697692871 | KNN Loss: 6.224740028381348 | BCE Loss: 1.044966697692871\n",
      "Epoch 426 / 500 | iteration 5 / 30 | Total Loss: 1.038623571395874 | KNN Loss: 6.224554538726807 | BCE Loss: 1.038623571395874\n",
      "Epoch 426 / 500 | iteration 10 / 30 | Total Loss: 1.0234637260437012 | KNN Loss: 6.224358081817627 | BCE Loss: 1.0234637260437012\n",
      "Epoch 426 / 500 | iteration 15 / 30 | Total Loss: 1.0409642457962036 | KNN Loss: 6.224640369415283 | BCE Loss: 1.0409642457962036\n",
      "Epoch 426 / 500 | iteration 20 / 30 | Total Loss: 1.0564343929290771 | KNN Loss: 6.224588871002197 | BCE Loss: 1.0564343929290771\n",
      "Epoch 426 / 500 | iteration 25 / 30 | Total Loss: 1.0200178623199463 | KNN Loss: 6.22434139251709 | BCE Loss: 1.0200178623199463\n",
      "Epoch 427 / 500 | iteration 0 / 30 | Total Loss: 1.0735230445861816 | KNN Loss: 6.224062919616699 | BCE Loss: 1.0735230445861816\n",
      "Epoch 427 / 500 | iteration 5 / 30 | Total Loss: 1.057861328125 | KNN Loss: 6.224465370178223 | BCE Loss: 1.057861328125\n",
      "Epoch 427 / 500 | iteration 10 / 30 | Total Loss: 1.0374424457550049 | KNN Loss: 6.22406530380249 | BCE Loss: 1.0374424457550049\n",
      "Epoch 427 / 500 | iteration 15 / 30 | Total Loss: 1.0490930080413818 | KNN Loss: 6.224479675292969 | BCE Loss: 1.0490930080413818\n",
      "Epoch 427 / 500 | iteration 20 / 30 | Total Loss: 1.070716381072998 | KNN Loss: 6.2247419357299805 | BCE Loss: 1.070716381072998\n",
      "Epoch 427 / 500 | iteration 25 / 30 | Total Loss: 1.0345176458358765 | KNN Loss: 6.2246832847595215 | BCE Loss: 1.0345176458358765\n",
      "Epoch 428 / 500 | iteration 0 / 30 | Total Loss: 1.0476974248886108 | KNN Loss: 6.224479675292969 | BCE Loss: 1.0476974248886108\n",
      "Epoch 428 / 500 | iteration 5 / 30 | Total Loss: 1.071955919265747 | KNN Loss: 6.2242751121521 | BCE Loss: 1.071955919265747\n",
      "Epoch 428 / 500 | iteration 10 / 30 | Total Loss: 1.0613386631011963 | KNN Loss: 6.224950313568115 | BCE Loss: 1.0613386631011963\n",
      "Epoch 428 / 500 | iteration 15 / 30 | Total Loss: 1.0499593019485474 | KNN Loss: 6.224580764770508 | BCE Loss: 1.0499593019485474\n",
      "Epoch 428 / 500 | iteration 20 / 30 | Total Loss: 1.065075397491455 | KNN Loss: 6.224266052246094 | BCE Loss: 1.065075397491455\n",
      "Epoch 428 / 500 | iteration 25 / 30 | Total Loss: 1.0128850936889648 | KNN Loss: 6.224285125732422 | BCE Loss: 1.0128850936889648\n",
      "Epoch 429 / 500 | iteration 0 / 30 | Total Loss: 1.0714787244796753 | KNN Loss: 6.224156856536865 | BCE Loss: 1.0714787244796753\n",
      "Epoch 429 / 500 | iteration 5 / 30 | Total Loss: 1.042924165725708 | KNN Loss: 6.224651336669922 | BCE Loss: 1.042924165725708\n",
      "Epoch 429 / 500 | iteration 10 / 30 | Total Loss: 1.045857548713684 | KNN Loss: 6.224118232727051 | BCE Loss: 1.045857548713684\n",
      "Epoch 429 / 500 | iteration 15 / 30 | Total Loss: 1.0534998178482056 | KNN Loss: 6.224460601806641 | BCE Loss: 1.0534998178482056\n",
      "Epoch 429 / 500 | iteration 20 / 30 | Total Loss: 1.0342004299163818 | KNN Loss: 6.224673748016357 | BCE Loss: 1.0342004299163818\n",
      "Epoch 429 / 500 | iteration 25 / 30 | Total Loss: 1.0681257247924805 | KNN Loss: 6.224431991577148 | BCE Loss: 1.0681257247924805\n",
      "Epoch 430 / 500 | iteration 0 / 30 | Total Loss: 1.0765783786773682 | KNN Loss: 6.224710941314697 | BCE Loss: 1.0765783786773682\n",
      "Epoch 430 / 500 | iteration 5 / 30 | Total Loss: 1.081683874130249 | KNN Loss: 6.224401950836182 | BCE Loss: 1.081683874130249\n",
      "Epoch 430 / 500 | iteration 10 / 30 | Total Loss: 1.035888433456421 | KNN Loss: 6.224159240722656 | BCE Loss: 1.035888433456421\n",
      "Epoch 430 / 500 | iteration 15 / 30 | Total Loss: 1.0812387466430664 | KNN Loss: 6.224221706390381 | BCE Loss: 1.0812387466430664\n",
      "Epoch 430 / 500 | iteration 20 / 30 | Total Loss: 1.038564920425415 | KNN Loss: 6.2242207527160645 | BCE Loss: 1.038564920425415\n",
      "Epoch 430 / 500 | iteration 25 / 30 | Total Loss: 1.0353295803070068 | KNN Loss: 6.224043846130371 | BCE Loss: 1.0353295803070068\n",
      "Epoch 431 / 500 | iteration 0 / 30 | Total Loss: 1.0390278100967407 | KNN Loss: 6.224289417266846 | BCE Loss: 1.0390278100967407\n",
      "Epoch 431 / 500 | iteration 5 / 30 | Total Loss: 1.0665239095687866 | KNN Loss: 6.224471092224121 | BCE Loss: 1.0665239095687866\n",
      "Epoch 431 / 500 | iteration 10 / 30 | Total Loss: 1.0372118949890137 | KNN Loss: 6.224394798278809 | BCE Loss: 1.0372118949890137\n",
      "Epoch 431 / 500 | iteration 15 / 30 | Total Loss: 1.0576446056365967 | KNN Loss: 6.224273681640625 | BCE Loss: 1.0576446056365967\n",
      "Epoch 431 / 500 | iteration 20 / 30 | Total Loss: 1.0489012002944946 | KNN Loss: 6.224642276763916 | BCE Loss: 1.0489012002944946\n",
      "Epoch 431 / 500 | iteration 25 / 30 | Total Loss: 1.0840909481048584 | KNN Loss: 6.224307060241699 | BCE Loss: 1.0840909481048584\n",
      "Epoch 432 / 500 | iteration 0 / 30 | Total Loss: 1.0668615102767944 | KNN Loss: 6.224371910095215 | BCE Loss: 1.0668615102767944\n",
      "Epoch 432 / 500 | iteration 5 / 30 | Total Loss: 1.0262508392333984 | KNN Loss: 6.224339962005615 | BCE Loss: 1.0262508392333984\n",
      "Epoch 432 / 500 | iteration 10 / 30 | Total Loss: 1.0457420349121094 | KNN Loss: 6.224470138549805 | BCE Loss: 1.0457420349121094\n",
      "Epoch 432 / 500 | iteration 15 / 30 | Total Loss: 1.062491536140442 | KNN Loss: 6.224801540374756 | BCE Loss: 1.062491536140442\n",
      "Epoch 432 / 500 | iteration 20 / 30 | Total Loss: 1.0683709383010864 | KNN Loss: 6.224407196044922 | BCE Loss: 1.0683709383010864\n",
      "Epoch 432 / 500 | iteration 25 / 30 | Total Loss: 1.0731468200683594 | KNN Loss: 6.22442102432251 | BCE Loss: 1.0731468200683594\n",
      "Epoch 433 / 500 | iteration 0 / 30 | Total Loss: 1.043013095855713 | KNN Loss: 6.224881649017334 | BCE Loss: 1.043013095855713\n",
      "Epoch 433 / 500 | iteration 5 / 30 | Total Loss: 1.0418285131454468 | KNN Loss: 6.2247090339660645 | BCE Loss: 1.0418285131454468\n",
      "Epoch 433 / 500 | iteration 10 / 30 | Total Loss: 1.0374245643615723 | KNN Loss: 6.2246575355529785 | BCE Loss: 1.0374245643615723\n",
      "Epoch 433 / 500 | iteration 15 / 30 | Total Loss: 1.072335958480835 | KNN Loss: 6.224347114562988 | BCE Loss: 1.072335958480835\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 433 / 500 | iteration 20 / 30 | Total Loss: 1.0382535457611084 | KNN Loss: 6.2244343757629395 | BCE Loss: 1.0382535457611084\n",
      "Epoch 433 / 500 | iteration 25 / 30 | Total Loss: 1.0269207954406738 | KNN Loss: 6.224828720092773 | BCE Loss: 1.0269207954406738\n",
      "Epoch 434 / 500 | iteration 0 / 30 | Total Loss: 1.0532808303833008 | KNN Loss: 6.224363327026367 | BCE Loss: 1.0532808303833008\n",
      "Epoch 434 / 500 | iteration 5 / 30 | Total Loss: 1.0783848762512207 | KNN Loss: 6.224705219268799 | BCE Loss: 1.0783848762512207\n",
      "Epoch 434 / 500 | iteration 10 / 30 | Total Loss: 1.0590641498565674 | KNN Loss: 6.224483966827393 | BCE Loss: 1.0590641498565674\n",
      "Epoch 434 / 500 | iteration 15 / 30 | Total Loss: 1.0717170238494873 | KNN Loss: 6.223955154418945 | BCE Loss: 1.0717170238494873\n",
      "Epoch 434 / 500 | iteration 20 / 30 | Total Loss: 1.0330395698547363 | KNN Loss: 6.224670886993408 | BCE Loss: 1.0330395698547363\n",
      "Epoch 434 / 500 | iteration 25 / 30 | Total Loss: 1.003517746925354 | KNN Loss: 6.2239789962768555 | BCE Loss: 1.003517746925354\n",
      "Epoch   435: reducing learning rate of group 0 to 2.7058e-08.\n",
      "Epoch 435 / 500 | iteration 0 / 30 | Total Loss: 1.057068109512329 | KNN Loss: 6.224352836608887 | BCE Loss: 1.057068109512329\n",
      "Epoch 435 / 500 | iteration 5 / 30 | Total Loss: 1.0174119472503662 | KNN Loss: 6.224388122558594 | BCE Loss: 1.0174119472503662\n",
      "Epoch 435 / 500 | iteration 10 / 30 | Total Loss: 1.0508441925048828 | KNN Loss: 6.224590301513672 | BCE Loss: 1.0508441925048828\n",
      "Epoch 435 / 500 | iteration 15 / 30 | Total Loss: 1.0238220691680908 | KNN Loss: 6.224487781524658 | BCE Loss: 1.0238220691680908\n",
      "Epoch 435 / 500 | iteration 20 / 30 | Total Loss: 1.0614556074142456 | KNN Loss: 6.224637508392334 | BCE Loss: 1.0614556074142456\n",
      "Epoch 435 / 500 | iteration 25 / 30 | Total Loss: 1.0406229496002197 | KNN Loss: 6.224430084228516 | BCE Loss: 1.0406229496002197\n",
      "Epoch 436 / 500 | iteration 0 / 30 | Total Loss: 1.0658005475997925 | KNN Loss: 6.224507808685303 | BCE Loss: 1.0658005475997925\n",
      "Epoch 436 / 500 | iteration 5 / 30 | Total Loss: 1.025429606437683 | KNN Loss: 6.224449157714844 | BCE Loss: 1.025429606437683\n",
      "Epoch 436 / 500 | iteration 10 / 30 | Total Loss: 1.034196138381958 | KNN Loss: 6.224478721618652 | BCE Loss: 1.034196138381958\n",
      "Epoch 436 / 500 | iteration 15 / 30 | Total Loss: 1.05403733253479 | KNN Loss: 6.224353790283203 | BCE Loss: 1.05403733253479\n",
      "Epoch 436 / 500 | iteration 20 / 30 | Total Loss: 1.0720937252044678 | KNN Loss: 6.224352836608887 | BCE Loss: 1.0720937252044678\n",
      "Epoch 436 / 500 | iteration 25 / 30 | Total Loss: 1.0220059156417847 | KNN Loss: 6.224503517150879 | BCE Loss: 1.0220059156417847\n",
      "Epoch 437 / 500 | iteration 0 / 30 | Total Loss: 1.0351815223693848 | KNN Loss: 6.224037170410156 | BCE Loss: 1.0351815223693848\n",
      "Epoch 437 / 500 | iteration 5 / 30 | Total Loss: 1.0767498016357422 | KNN Loss: 6.224406719207764 | BCE Loss: 1.0767498016357422\n",
      "Epoch 437 / 500 | iteration 10 / 30 | Total Loss: 1.058619499206543 | KNN Loss: 6.2245869636535645 | BCE Loss: 1.058619499206543\n",
      "Epoch 437 / 500 | iteration 15 / 30 | Total Loss: 1.048461675643921 | KNN Loss: 6.224282741546631 | BCE Loss: 1.048461675643921\n",
      "Epoch 437 / 500 | iteration 20 / 30 | Total Loss: 1.0702393054962158 | KNN Loss: 6.224372863769531 | BCE Loss: 1.0702393054962158\n",
      "Epoch 437 / 500 | iteration 25 / 30 | Total Loss: 1.1010098457336426 | KNN Loss: 6.224575519561768 | BCE Loss: 1.1010098457336426\n",
      "Epoch 438 / 500 | iteration 0 / 30 | Total Loss: 1.060126781463623 | KNN Loss: 6.224269866943359 | BCE Loss: 1.060126781463623\n",
      "Epoch 438 / 500 | iteration 5 / 30 | Total Loss: 1.0837236642837524 | KNN Loss: 6.224452972412109 | BCE Loss: 1.0837236642837524\n",
      "Epoch 438 / 500 | iteration 10 / 30 | Total Loss: 1.0691440105438232 | KNN Loss: 6.224541187286377 | BCE Loss: 1.0691440105438232\n",
      "Epoch 438 / 500 | iteration 15 / 30 | Total Loss: 1.0593596696853638 | KNN Loss: 6.224533557891846 | BCE Loss: 1.0593596696853638\n",
      "Epoch 438 / 500 | iteration 20 / 30 | Total Loss: 1.022698163986206 | KNN Loss: 6.224510192871094 | BCE Loss: 1.022698163986206\n",
      "Epoch 438 / 500 | iteration 25 / 30 | Total Loss: 1.0554063320159912 | KNN Loss: 6.22456693649292 | BCE Loss: 1.0554063320159912\n",
      "Epoch 439 / 500 | iteration 0 / 30 | Total Loss: 1.0798676013946533 | KNN Loss: 6.224829196929932 | BCE Loss: 1.0798676013946533\n",
      "Epoch 439 / 500 | iteration 5 / 30 | Total Loss: 1.0696808099746704 | KNN Loss: 6.224541187286377 | BCE Loss: 1.0696808099746704\n",
      "Epoch 439 / 500 | iteration 10 / 30 | Total Loss: 1.0516088008880615 | KNN Loss: 6.224620819091797 | BCE Loss: 1.0516088008880615\n",
      "Epoch 439 / 500 | iteration 15 / 30 | Total Loss: 1.0492470264434814 | KNN Loss: 6.224400043487549 | BCE Loss: 1.0492470264434814\n",
      "Epoch 439 / 500 | iteration 20 / 30 | Total Loss: 1.04496169090271 | KNN Loss: 6.2248711585998535 | BCE Loss: 1.04496169090271\n",
      "Epoch 439 / 500 | iteration 25 / 30 | Total Loss: 1.0450044870376587 | KNN Loss: 6.223997116088867 | BCE Loss: 1.0450044870376587\n",
      "Epoch 440 / 500 | iteration 0 / 30 | Total Loss: 1.0603008270263672 | KNN Loss: 6.224093437194824 | BCE Loss: 1.0603008270263672\n",
      "Epoch 440 / 500 | iteration 5 / 30 | Total Loss: 1.0354856252670288 | KNN Loss: 6.224634170532227 | BCE Loss: 1.0354856252670288\n",
      "Epoch 440 / 500 | iteration 10 / 30 | Total Loss: 1.0558491945266724 | KNN Loss: 6.224445343017578 | BCE Loss: 1.0558491945266724\n",
      "Epoch 440 / 500 | iteration 15 / 30 | Total Loss: 1.0397536754608154 | KNN Loss: 6.224308967590332 | BCE Loss: 1.0397536754608154\n",
      "Epoch 440 / 500 | iteration 20 / 30 | Total Loss: 1.0688443183898926 | KNN Loss: 6.224743366241455 | BCE Loss: 1.0688443183898926\n",
      "Epoch 440 / 500 | iteration 25 / 30 | Total Loss: 1.0193414688110352 | KNN Loss: 6.224469184875488 | BCE Loss: 1.0193414688110352\n",
      "Epoch 441 / 500 | iteration 0 / 30 | Total Loss: 1.0463112592697144 | KNN Loss: 6.224313259124756 | BCE Loss: 1.0463112592697144\n",
      "Epoch 441 / 500 | iteration 5 / 30 | Total Loss: 1.0729362964630127 | KNN Loss: 6.225037574768066 | BCE Loss: 1.0729362964630127\n",
      "Epoch 441 / 500 | iteration 10 / 30 | Total Loss: 1.0476657152175903 | KNN Loss: 6.224809646606445 | BCE Loss: 1.0476657152175903\n",
      "Epoch 441 / 500 | iteration 15 / 30 | Total Loss: 1.0463756322860718 | KNN Loss: 6.224189281463623 | BCE Loss: 1.0463756322860718\n",
      "Epoch 441 / 500 | iteration 20 / 30 | Total Loss: 1.034523844718933 | KNN Loss: 6.224506378173828 | BCE Loss: 1.034523844718933\n",
      "Epoch 441 / 500 | iteration 25 / 30 | Total Loss: 1.0636963844299316 | KNN Loss: 6.224376678466797 | BCE Loss: 1.0636963844299316\n",
      "Epoch 442 / 500 | iteration 0 / 30 | Total Loss: 1.0533947944641113 | KNN Loss: 6.2248215675354 | BCE Loss: 1.0533947944641113\n",
      "Epoch 442 / 500 | iteration 5 / 30 | Total Loss: 1.0487428903579712 | KNN Loss: 6.2243757247924805 | BCE Loss: 1.0487428903579712\n",
      "Epoch 442 / 500 | iteration 10 / 30 | Total Loss: 1.073068380355835 | KNN Loss: 6.2245049476623535 | BCE Loss: 1.073068380355835\n",
      "Epoch 442 / 500 | iteration 15 / 30 | Total Loss: 1.0720784664154053 | KNN Loss: 6.224783897399902 | BCE Loss: 1.0720784664154053\n",
      "Epoch 442 / 500 | iteration 20 / 30 | Total Loss: 1.06728196144104 | KNN Loss: 6.224442958831787 | BCE Loss: 1.06728196144104\n",
      "Epoch 442 / 500 | iteration 25 / 30 | Total Loss: 1.0688965320587158 | KNN Loss: 6.224061965942383 | BCE Loss: 1.0688965320587158\n",
      "Epoch 443 / 500 | iteration 0 / 30 | Total Loss: 1.031588077545166 | KNN Loss: 6.224199295043945 | BCE Loss: 1.031588077545166\n",
      "Epoch 443 / 500 | iteration 5 / 30 | Total Loss: 1.0814836025238037 | KNN Loss: 6.224485874176025 | BCE Loss: 1.0814836025238037\n",
      "Epoch 443 / 500 | iteration 10 / 30 | Total Loss: 1.0549707412719727 | KNN Loss: 6.224532127380371 | BCE Loss: 1.0549707412719727\n",
      "Epoch 443 / 500 | iteration 15 / 30 | Total Loss: 1.0522606372833252 | KNN Loss: 6.224118709564209 | BCE Loss: 1.0522606372833252\n",
      "Epoch 443 / 500 | iteration 20 / 30 | Total Loss: 1.03346586227417 | KNN Loss: 6.224267482757568 | BCE Loss: 1.03346586227417\n",
      "Epoch 443 / 500 | iteration 25 / 30 | Total Loss: 1.046558141708374 | KNN Loss: 6.224360942840576 | BCE Loss: 1.046558141708374\n",
      "Epoch 444 / 500 | iteration 0 / 30 | Total Loss: 1.0728797912597656 | KNN Loss: 6.224654674530029 | BCE Loss: 1.0728797912597656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 444 / 500 | iteration 5 / 30 | Total Loss: 1.0181970596313477 | KNN Loss: 6.2242560386657715 | BCE Loss: 1.0181970596313477\n",
      "Epoch 444 / 500 | iteration 10 / 30 | Total Loss: 1.0399465560913086 | KNN Loss: 6.2241902351379395 | BCE Loss: 1.0399465560913086\n",
      "Epoch 444 / 500 | iteration 15 / 30 | Total Loss: 1.040428638458252 | KNN Loss: 6.224527359008789 | BCE Loss: 1.040428638458252\n",
      "Epoch 444 / 500 | iteration 20 / 30 | Total Loss: 1.0321407318115234 | KNN Loss: 6.223783493041992 | BCE Loss: 1.0321407318115234\n",
      "Epoch 444 / 500 | iteration 25 / 30 | Total Loss: 1.0559033155441284 | KNN Loss: 6.224629878997803 | BCE Loss: 1.0559033155441284\n",
      "Epoch 445 / 500 | iteration 0 / 30 | Total Loss: 1.0751242637634277 | KNN Loss: 6.224888801574707 | BCE Loss: 1.0751242637634277\n",
      "Epoch 445 / 500 | iteration 5 / 30 | Total Loss: 1.0568912029266357 | KNN Loss: 6.224323272705078 | BCE Loss: 1.0568912029266357\n",
      "Epoch 445 / 500 | iteration 10 / 30 | Total Loss: 1.0513652563095093 | KNN Loss: 6.224563121795654 | BCE Loss: 1.0513652563095093\n",
      "Epoch 445 / 500 | iteration 15 / 30 | Total Loss: 1.0545408725738525 | KNN Loss: 6.224161148071289 | BCE Loss: 1.0545408725738525\n",
      "Epoch 445 / 500 | iteration 20 / 30 | Total Loss: 1.0458632707595825 | KNN Loss: 6.224160671234131 | BCE Loss: 1.0458632707595825\n",
      "Epoch 445 / 500 | iteration 25 / 30 | Total Loss: 1.0305513143539429 | KNN Loss: 6.224758148193359 | BCE Loss: 1.0305513143539429\n",
      "Epoch 446 / 500 | iteration 0 / 30 | Total Loss: 1.0750210285186768 | KNN Loss: 6.224182605743408 | BCE Loss: 1.0750210285186768\n",
      "Epoch 446 / 500 | iteration 5 / 30 | Total Loss: 1.0517805814743042 | KNN Loss: 6.224814414978027 | BCE Loss: 1.0517805814743042\n",
      "Epoch 446 / 500 | iteration 10 / 30 | Total Loss: 1.0396043062210083 | KNN Loss: 6.224534511566162 | BCE Loss: 1.0396043062210083\n",
      "Epoch 446 / 500 | iteration 15 / 30 | Total Loss: 1.0355031490325928 | KNN Loss: 6.224145889282227 | BCE Loss: 1.0355031490325928\n",
      "Epoch 446 / 500 | iteration 20 / 30 | Total Loss: 1.0459258556365967 | KNN Loss: 6.22413969039917 | BCE Loss: 1.0459258556365967\n",
      "Epoch 446 / 500 | iteration 25 / 30 | Total Loss: 1.0283534526824951 | KNN Loss: 6.224382400512695 | BCE Loss: 1.0283534526824951\n",
      "Epoch 447 / 500 | iteration 0 / 30 | Total Loss: 1.0365114212036133 | KNN Loss: 6.224697589874268 | BCE Loss: 1.0365114212036133\n",
      "Epoch 447 / 500 | iteration 5 / 30 | Total Loss: 1.0268062353134155 | KNN Loss: 6.224081993103027 | BCE Loss: 1.0268062353134155\n",
      "Epoch 447 / 500 | iteration 10 / 30 | Total Loss: 1.0588752031326294 | KNN Loss: 6.224639892578125 | BCE Loss: 1.0588752031326294\n",
      "Epoch 447 / 500 | iteration 15 / 30 | Total Loss: 1.0297459363937378 | KNN Loss: 6.224255084991455 | BCE Loss: 1.0297459363937378\n",
      "Epoch 447 / 500 | iteration 20 / 30 | Total Loss: 1.0620534420013428 | KNN Loss: 6.224459648132324 | BCE Loss: 1.0620534420013428\n",
      "Epoch 447 / 500 | iteration 25 / 30 | Total Loss: 1.0545055866241455 | KNN Loss: 6.224364280700684 | BCE Loss: 1.0545055866241455\n",
      "Epoch 448 / 500 | iteration 0 / 30 | Total Loss: 1.0555737018585205 | KNN Loss: 6.224466800689697 | BCE Loss: 1.0555737018585205\n",
      "Epoch 448 / 500 | iteration 5 / 30 | Total Loss: 1.0712685585021973 | KNN Loss: 6.2248101234436035 | BCE Loss: 1.0712685585021973\n",
      "Epoch 448 / 500 | iteration 10 / 30 | Total Loss: 1.045568585395813 | KNN Loss: 6.22434139251709 | BCE Loss: 1.045568585395813\n",
      "Epoch 448 / 500 | iteration 15 / 30 | Total Loss: 1.0453784465789795 | KNN Loss: 6.224481105804443 | BCE Loss: 1.0453784465789795\n",
      "Epoch 448 / 500 | iteration 20 / 30 | Total Loss: 1.0758706331253052 | KNN Loss: 6.224322319030762 | BCE Loss: 1.0758706331253052\n",
      "Epoch 448 / 500 | iteration 25 / 30 | Total Loss: 1.0449788570404053 | KNN Loss: 6.224158763885498 | BCE Loss: 1.0449788570404053\n",
      "Epoch 449 / 500 | iteration 0 / 30 | Total Loss: 1.0698890686035156 | KNN Loss: 6.2247467041015625 | BCE Loss: 1.0698890686035156\n",
      "Epoch 449 / 500 | iteration 5 / 30 | Total Loss: 1.0524179935455322 | KNN Loss: 6.224879264831543 | BCE Loss: 1.0524179935455322\n",
      "Epoch 449 / 500 | iteration 10 / 30 | Total Loss: 1.045873999595642 | KNN Loss: 6.224433898925781 | BCE Loss: 1.045873999595642\n",
      "Epoch 449 / 500 | iteration 15 / 30 | Total Loss: 1.042359471321106 | KNN Loss: 6.22422981262207 | BCE Loss: 1.042359471321106\n",
      "Epoch 449 / 500 | iteration 20 / 30 | Total Loss: 1.061261534690857 | KNN Loss: 6.224484443664551 | BCE Loss: 1.061261534690857\n",
      "Epoch 449 / 500 | iteration 25 / 30 | Total Loss: 1.0423504114151 | KNN Loss: 6.22452974319458 | BCE Loss: 1.0423504114151\n",
      "Epoch 450 / 500 | iteration 0 / 30 | Total Loss: 1.014857292175293 | KNN Loss: 6.224617958068848 | BCE Loss: 1.014857292175293\n",
      "Epoch 450 / 500 | iteration 5 / 30 | Total Loss: 1.0742461681365967 | KNN Loss: 6.224436283111572 | BCE Loss: 1.0742461681365967\n",
      "Epoch 450 / 500 | iteration 10 / 30 | Total Loss: 1.0676674842834473 | KNN Loss: 6.224270343780518 | BCE Loss: 1.0676674842834473\n",
      "Epoch 450 / 500 | iteration 15 / 30 | Total Loss: 1.0624061822891235 | KNN Loss: 6.2247090339660645 | BCE Loss: 1.0624061822891235\n",
      "Epoch 450 / 500 | iteration 20 / 30 | Total Loss: 1.0510225296020508 | KNN Loss: 6.2245917320251465 | BCE Loss: 1.0510225296020508\n",
      "Epoch 450 / 500 | iteration 25 / 30 | Total Loss: 1.0605201721191406 | KNN Loss: 6.224116325378418 | BCE Loss: 1.0605201721191406\n",
      "Epoch 451 / 500 | iteration 0 / 30 | Total Loss: 1.0571069717407227 | KNN Loss: 6.224371910095215 | BCE Loss: 1.0571069717407227\n",
      "Epoch 451 / 500 | iteration 5 / 30 | Total Loss: 1.0426820516586304 | KNN Loss: 6.224480152130127 | BCE Loss: 1.0426820516586304\n",
      "Epoch 451 / 500 | iteration 10 / 30 | Total Loss: 1.0658230781555176 | KNN Loss: 6.224465370178223 | BCE Loss: 1.0658230781555176\n",
      "Epoch 451 / 500 | iteration 15 / 30 | Total Loss: 1.0471999645233154 | KNN Loss: 6.224099159240723 | BCE Loss: 1.0471999645233154\n",
      "Epoch 451 / 500 | iteration 20 / 30 | Total Loss: 1.0652129650115967 | KNN Loss: 6.224466323852539 | BCE Loss: 1.0652129650115967\n",
      "Epoch 451 / 500 | iteration 25 / 30 | Total Loss: 1.0417158603668213 | KNN Loss: 6.224529266357422 | BCE Loss: 1.0417158603668213\n",
      "Epoch 452 / 500 | iteration 0 / 30 | Total Loss: 1.0653995275497437 | KNN Loss: 6.224788188934326 | BCE Loss: 1.0653995275497437\n",
      "Epoch 452 / 500 | iteration 5 / 30 | Total Loss: 1.0587480068206787 | KNN Loss: 6.224564075469971 | BCE Loss: 1.0587480068206787\n",
      "Epoch 452 / 500 | iteration 10 / 30 | Total Loss: 1.0269291400909424 | KNN Loss: 6.224517345428467 | BCE Loss: 1.0269291400909424\n",
      "Epoch 452 / 500 | iteration 15 / 30 | Total Loss: 1.0453684329986572 | KNN Loss: 6.224227428436279 | BCE Loss: 1.0453684329986572\n",
      "Epoch 452 / 500 | iteration 20 / 30 | Total Loss: 1.032914400100708 | KNN Loss: 6.224134922027588 | BCE Loss: 1.032914400100708\n",
      "Epoch 452 / 500 | iteration 25 / 30 | Total Loss: 1.0490957498550415 | KNN Loss: 6.224324703216553 | BCE Loss: 1.0490957498550415\n",
      "Epoch 453 / 500 | iteration 0 / 30 | Total Loss: 1.0597014427185059 | KNN Loss: 6.224548816680908 | BCE Loss: 1.0597014427185059\n",
      "Epoch 453 / 500 | iteration 5 / 30 | Total Loss: 1.05625319480896 | KNN Loss: 6.223830699920654 | BCE Loss: 1.05625319480896\n",
      "Epoch 453 / 500 | iteration 10 / 30 | Total Loss: 1.038424015045166 | KNN Loss: 6.224451541900635 | BCE Loss: 1.038424015045166\n",
      "Epoch 453 / 500 | iteration 15 / 30 | Total Loss: 1.0536139011383057 | KNN Loss: 6.224273681640625 | BCE Loss: 1.0536139011383057\n",
      "Epoch 453 / 500 | iteration 20 / 30 | Total Loss: 1.036481499671936 | KNN Loss: 6.224400520324707 | BCE Loss: 1.036481499671936\n",
      "Epoch 453 / 500 | iteration 25 / 30 | Total Loss: 1.0770971775054932 | KNN Loss: 6.224253177642822 | BCE Loss: 1.0770971775054932\n",
      "Epoch 454 / 500 | iteration 0 / 30 | Total Loss: 1.034333348274231 | KNN Loss: 6.224437713623047 | BCE Loss: 1.034333348274231\n",
      "Epoch 454 / 500 | iteration 5 / 30 | Total Loss: 1.051283359527588 | KNN Loss: 6.224528789520264 | BCE Loss: 1.051283359527588\n",
      "Epoch 454 / 500 | iteration 10 / 30 | Total Loss: 1.0219510793685913 | KNN Loss: 6.224111080169678 | BCE Loss: 1.0219510793685913\n",
      "Epoch 454 / 500 | iteration 15 / 30 | Total Loss: 1.0469763278961182 | KNN Loss: 6.224333763122559 | BCE Loss: 1.0469763278961182\n",
      "Epoch 454 / 500 | iteration 20 / 30 | Total Loss: 1.0894198417663574 | KNN Loss: 6.224761486053467 | BCE Loss: 1.0894198417663574\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 454 / 500 | iteration 25 / 30 | Total Loss: 1.0386836528778076 | KNN Loss: 6.2243499755859375 | BCE Loss: 1.0386836528778076\n",
      "Epoch 455 / 500 | iteration 0 / 30 | Total Loss: 1.0423836708068848 | KNN Loss: 6.224418640136719 | BCE Loss: 1.0423836708068848\n",
      "Epoch 455 / 500 | iteration 5 / 30 | Total Loss: 1.0410542488098145 | KNN Loss: 6.2241291999816895 | BCE Loss: 1.0410542488098145\n",
      "Epoch 455 / 500 | iteration 10 / 30 | Total Loss: 1.0504595041275024 | KNN Loss: 6.224740505218506 | BCE Loss: 1.0504595041275024\n",
      "Epoch 455 / 500 | iteration 15 / 30 | Total Loss: 1.0450432300567627 | KNN Loss: 6.224768161773682 | BCE Loss: 1.0450432300567627\n",
      "Epoch 455 / 500 | iteration 20 / 30 | Total Loss: 1.0475773811340332 | KNN Loss: 6.224482536315918 | BCE Loss: 1.0475773811340332\n",
      "Epoch 455 / 500 | iteration 25 / 30 | Total Loss: 1.0611717700958252 | KNN Loss: 6.224587440490723 | BCE Loss: 1.0611717700958252\n",
      "Epoch 456 / 500 | iteration 0 / 30 | Total Loss: 1.07692289352417 | KNN Loss: 6.224618911743164 | BCE Loss: 1.07692289352417\n",
      "Epoch 456 / 500 | iteration 5 / 30 | Total Loss: 1.0494199991226196 | KNN Loss: 6.224424362182617 | BCE Loss: 1.0494199991226196\n",
      "Epoch 456 / 500 | iteration 10 / 30 | Total Loss: 1.0306369066238403 | KNN Loss: 6.224552631378174 | BCE Loss: 1.0306369066238403\n",
      "Epoch 456 / 500 | iteration 15 / 30 | Total Loss: 1.0354753732681274 | KNN Loss: 6.2245869636535645 | BCE Loss: 1.0354753732681274\n",
      "Epoch 456 / 500 | iteration 20 / 30 | Total Loss: 1.053625226020813 | KNN Loss: 6.223944664001465 | BCE Loss: 1.053625226020813\n",
      "Epoch 456 / 500 | iteration 25 / 30 | Total Loss: 1.0556552410125732 | KNN Loss: 6.2245025634765625 | BCE Loss: 1.0556552410125732\n",
      "Epoch 457 / 500 | iteration 0 / 30 | Total Loss: 1.0437824726104736 | KNN Loss: 6.224869728088379 | BCE Loss: 1.0437824726104736\n",
      "Epoch 457 / 500 | iteration 5 / 30 | Total Loss: 1.037862777709961 | KNN Loss: 6.224575519561768 | BCE Loss: 1.037862777709961\n",
      "Epoch 457 / 500 | iteration 10 / 30 | Total Loss: 1.072535514831543 | KNN Loss: 6.224417209625244 | BCE Loss: 1.072535514831543\n",
      "Epoch 457 / 500 | iteration 15 / 30 | Total Loss: 1.0707237720489502 | KNN Loss: 6.224203109741211 | BCE Loss: 1.0707237720489502\n",
      "Epoch 457 / 500 | iteration 20 / 30 | Total Loss: 1.028998613357544 | KNN Loss: 6.224354267120361 | BCE Loss: 1.028998613357544\n",
      "Epoch 457 / 500 | iteration 25 / 30 | Total Loss: 1.0484035015106201 | KNN Loss: 6.224766731262207 | BCE Loss: 1.0484035015106201\n",
      "Epoch 458 / 500 | iteration 0 / 30 | Total Loss: 1.0256307125091553 | KNN Loss: 6.22422456741333 | BCE Loss: 1.0256307125091553\n",
      "Epoch 458 / 500 | iteration 5 / 30 | Total Loss: 1.059519648551941 | KNN Loss: 6.224130153656006 | BCE Loss: 1.059519648551941\n",
      "Epoch 458 / 500 | iteration 10 / 30 | Total Loss: 1.0123798847198486 | KNN Loss: 6.224490642547607 | BCE Loss: 1.0123798847198486\n",
      "Epoch 458 / 500 | iteration 15 / 30 | Total Loss: 1.049267053604126 | KNN Loss: 6.224518299102783 | BCE Loss: 1.049267053604126\n",
      "Epoch 458 / 500 | iteration 20 / 30 | Total Loss: 1.0658618211746216 | KNN Loss: 6.22401762008667 | BCE Loss: 1.0658618211746216\n",
      "Epoch 458 / 500 | iteration 25 / 30 | Total Loss: 1.0632240772247314 | KNN Loss: 6.224784851074219 | BCE Loss: 1.0632240772247314\n",
      "Epoch 459 / 500 | iteration 0 / 30 | Total Loss: 1.0314888954162598 | KNN Loss: 6.224381446838379 | BCE Loss: 1.0314888954162598\n",
      "Epoch 459 / 500 | iteration 5 / 30 | Total Loss: 1.02305006980896 | KNN Loss: 6.2244486808776855 | BCE Loss: 1.02305006980896\n",
      "Epoch 459 / 500 | iteration 10 / 30 | Total Loss: 1.0366264581680298 | KNN Loss: 6.2248148918151855 | BCE Loss: 1.0366264581680298\n",
      "Epoch 459 / 500 | iteration 15 / 30 | Total Loss: 1.0690184831619263 | KNN Loss: 6.2241716384887695 | BCE Loss: 1.0690184831619263\n",
      "Epoch 459 / 500 | iteration 20 / 30 | Total Loss: 1.0491979122161865 | KNN Loss: 6.224423408508301 | BCE Loss: 1.0491979122161865\n",
      "Epoch 459 / 500 | iteration 25 / 30 | Total Loss: 1.0369970798492432 | KNN Loss: 6.224153518676758 | BCE Loss: 1.0369970798492432\n",
      "Epoch 460 / 500 | iteration 0 / 30 | Total Loss: 1.0551540851593018 | KNN Loss: 6.224654197692871 | BCE Loss: 1.0551540851593018\n",
      "Epoch 460 / 500 | iteration 5 / 30 | Total Loss: 1.0517759323120117 | KNN Loss: 6.224126815795898 | BCE Loss: 1.0517759323120117\n",
      "Epoch 460 / 500 | iteration 10 / 30 | Total Loss: 1.069504976272583 | KNN Loss: 6.224545478820801 | BCE Loss: 1.069504976272583\n",
      "Epoch 460 / 500 | iteration 15 / 30 | Total Loss: 1.0451936721801758 | KNN Loss: 6.22437047958374 | BCE Loss: 1.0451936721801758\n",
      "Epoch 460 / 500 | iteration 20 / 30 | Total Loss: 1.0725600719451904 | KNN Loss: 6.224560737609863 | BCE Loss: 1.0725600719451904\n",
      "Epoch 460 / 500 | iteration 25 / 30 | Total Loss: 1.0830227136611938 | KNN Loss: 6.224362373352051 | BCE Loss: 1.0830227136611938\n",
      "Epoch 461 / 500 | iteration 0 / 30 | Total Loss: 1.0599513053894043 | KNN Loss: 6.224271297454834 | BCE Loss: 1.0599513053894043\n",
      "Epoch 461 / 500 | iteration 5 / 30 | Total Loss: 1.04634690284729 | KNN Loss: 6.224565505981445 | BCE Loss: 1.04634690284729\n",
      "Epoch 461 / 500 | iteration 10 / 30 | Total Loss: 1.0531474351882935 | KNN Loss: 6.2246270179748535 | BCE Loss: 1.0531474351882935\n",
      "Epoch 461 / 500 | iteration 15 / 30 | Total Loss: 1.0127308368682861 | KNN Loss: 6.224721908569336 | BCE Loss: 1.0127308368682861\n",
      "Epoch 461 / 500 | iteration 20 / 30 | Total Loss: 1.078526496887207 | KNN Loss: 6.224189758300781 | BCE Loss: 1.078526496887207\n",
      "Epoch 461 / 500 | iteration 25 / 30 | Total Loss: 1.010080337524414 | KNN Loss: 6.22440767288208 | BCE Loss: 1.010080337524414\n",
      "Epoch 462 / 500 | iteration 0 / 30 | Total Loss: 1.0955770015716553 | KNN Loss: 6.224889755249023 | BCE Loss: 1.0955770015716553\n",
      "Epoch 462 / 500 | iteration 5 / 30 | Total Loss: 1.0667083263397217 | KNN Loss: 6.224510669708252 | BCE Loss: 1.0667083263397217\n",
      "Epoch 462 / 500 | iteration 10 / 30 | Total Loss: 1.054422378540039 | KNN Loss: 6.224470138549805 | BCE Loss: 1.054422378540039\n",
      "Epoch 462 / 500 | iteration 15 / 30 | Total Loss: 1.0468230247497559 | KNN Loss: 6.224471092224121 | BCE Loss: 1.0468230247497559\n",
      "Epoch 462 / 500 | iteration 20 / 30 | Total Loss: 1.0327156782150269 | KNN Loss: 6.224971294403076 | BCE Loss: 1.0327156782150269\n",
      "Epoch 462 / 500 | iteration 25 / 30 | Total Loss: 1.0637649297714233 | KNN Loss: 6.224511623382568 | BCE Loss: 1.0637649297714233\n",
      "Epoch 463 / 500 | iteration 0 / 30 | Total Loss: 1.0636792182922363 | KNN Loss: 6.22403621673584 | BCE Loss: 1.0636792182922363\n",
      "Epoch 463 / 500 | iteration 5 / 30 | Total Loss: 1.0348678827285767 | KNN Loss: 6.224379062652588 | BCE Loss: 1.0348678827285767\n",
      "Epoch 463 / 500 | iteration 10 / 30 | Total Loss: 1.0487862825393677 | KNN Loss: 6.223971366882324 | BCE Loss: 1.0487862825393677\n",
      "Epoch 463 / 500 | iteration 15 / 30 | Total Loss: 1.0595862865447998 | KNN Loss: 6.22390604019165 | BCE Loss: 1.0595862865447998\n",
      "Epoch 463 / 500 | iteration 20 / 30 | Total Loss: 1.0624291896820068 | KNN Loss: 6.22463321685791 | BCE Loss: 1.0624291896820068\n",
      "Epoch 463 / 500 | iteration 25 / 30 | Total Loss: 1.0618798732757568 | KNN Loss: 6.224400997161865 | BCE Loss: 1.0618798732757568\n",
      "Epoch 464 / 500 | iteration 0 / 30 | Total Loss: 1.063629388809204 | KNN Loss: 6.224676609039307 | BCE Loss: 1.063629388809204\n",
      "Epoch 464 / 500 | iteration 5 / 30 | Total Loss: 1.0308161973953247 | KNN Loss: 6.225068092346191 | BCE Loss: 1.0308161973953247\n",
      "Epoch 464 / 500 | iteration 10 / 30 | Total Loss: 1.048980474472046 | KNN Loss: 6.224391460418701 | BCE Loss: 1.048980474472046\n",
      "Epoch 464 / 500 | iteration 15 / 30 | Total Loss: 1.0388599634170532 | KNN Loss: 6.224242687225342 | BCE Loss: 1.0388599634170532\n",
      "Epoch 464 / 500 | iteration 20 / 30 | Total Loss: 1.0662174224853516 | KNN Loss: 6.2243428230285645 | BCE Loss: 1.0662174224853516\n",
      "Epoch 464 / 500 | iteration 25 / 30 | Total Loss: 1.0552873611450195 | KNN Loss: 6.224526882171631 | BCE Loss: 1.0552873611450195\n",
      "Epoch 465 / 500 | iteration 0 / 30 | Total Loss: 1.0586268901824951 | KNN Loss: 6.224400043487549 | BCE Loss: 1.0586268901824951\n",
      "Epoch 465 / 500 | iteration 5 / 30 | Total Loss: 1.0350046157836914 | KNN Loss: 6.224693298339844 | BCE Loss: 1.0350046157836914\n",
      "Epoch 465 / 500 | iteration 10 / 30 | Total Loss: 1.0433787107467651 | KNN Loss: 6.224363803863525 | BCE Loss: 1.0433787107467651\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 465 / 500 | iteration 15 / 30 | Total Loss: 1.0223348140716553 | KNN Loss: 6.224613666534424 | BCE Loss: 1.0223348140716553\n",
      "Epoch 465 / 500 | iteration 20 / 30 | Total Loss: 1.067919373512268 | KNN Loss: 6.224935531616211 | BCE Loss: 1.067919373512268\n",
      "Epoch 465 / 500 | iteration 25 / 30 | Total Loss: 1.043061375617981 | KNN Loss: 6.224202632904053 | BCE Loss: 1.043061375617981\n",
      "Epoch 466 / 500 | iteration 0 / 30 | Total Loss: 1.045883059501648 | KNN Loss: 6.2244791984558105 | BCE Loss: 1.045883059501648\n",
      "Epoch 466 / 500 | iteration 5 / 30 | Total Loss: 1.0668728351593018 | KNN Loss: 6.224584102630615 | BCE Loss: 1.0668728351593018\n",
      "Epoch 466 / 500 | iteration 10 / 30 | Total Loss: 1.0705169439315796 | KNN Loss: 6.224330425262451 | BCE Loss: 1.0705169439315796\n",
      "Epoch 466 / 500 | iteration 15 / 30 | Total Loss: 1.0422401428222656 | KNN Loss: 6.224624156951904 | BCE Loss: 1.0422401428222656\n",
      "Epoch 466 / 500 | iteration 20 / 30 | Total Loss: 1.044601559638977 | KNN Loss: 6.224548816680908 | BCE Loss: 1.044601559638977\n",
      "Epoch 466 / 500 | iteration 25 / 30 | Total Loss: 1.0302362442016602 | KNN Loss: 6.224501609802246 | BCE Loss: 1.0302362442016602\n",
      "Epoch 467 / 500 | iteration 0 / 30 | Total Loss: 1.044759750366211 | KNN Loss: 6.224296569824219 | BCE Loss: 1.044759750366211\n",
      "Epoch 467 / 500 | iteration 5 / 30 | Total Loss: 1.0669838190078735 | KNN Loss: 6.224294185638428 | BCE Loss: 1.0669838190078735\n",
      "Epoch 467 / 500 | iteration 10 / 30 | Total Loss: 1.0580682754516602 | KNN Loss: 6.224235534667969 | BCE Loss: 1.0580682754516602\n",
      "Epoch 467 / 500 | iteration 15 / 30 | Total Loss: 1.0491787195205688 | KNN Loss: 6.224583625793457 | BCE Loss: 1.0491787195205688\n",
      "Epoch 467 / 500 | iteration 20 / 30 | Total Loss: 1.0738108158111572 | KNN Loss: 6.224661827087402 | BCE Loss: 1.0738108158111572\n",
      "Epoch 467 / 500 | iteration 25 / 30 | Total Loss: 1.0504664182662964 | KNN Loss: 6.224612236022949 | BCE Loss: 1.0504664182662964\n",
      "Epoch 468 / 500 | iteration 0 / 30 | Total Loss: 1.009266972541809 | KNN Loss: 6.224462509155273 | BCE Loss: 1.009266972541809\n",
      "Epoch 468 / 500 | iteration 5 / 30 | Total Loss: 1.049814224243164 | KNN Loss: 6.224424839019775 | BCE Loss: 1.049814224243164\n",
      "Epoch 468 / 500 | iteration 10 / 30 | Total Loss: 1.0771229267120361 | KNN Loss: 6.224442481994629 | BCE Loss: 1.0771229267120361\n",
      "Epoch 468 / 500 | iteration 15 / 30 | Total Loss: 1.0568749904632568 | KNN Loss: 6.224271774291992 | BCE Loss: 1.0568749904632568\n",
      "Epoch 468 / 500 | iteration 20 / 30 | Total Loss: 1.048745036125183 | KNN Loss: 6.2249321937561035 | BCE Loss: 1.048745036125183\n",
      "Epoch 468 / 500 | iteration 25 / 30 | Total Loss: 1.0415723323822021 | KNN Loss: 6.224205493927002 | BCE Loss: 1.0415723323822021\n",
      "Epoch 469 / 500 | iteration 0 / 30 | Total Loss: 1.0773348808288574 | KNN Loss: 6.224596977233887 | BCE Loss: 1.0773348808288574\n",
      "Epoch 469 / 500 | iteration 5 / 30 | Total Loss: 1.0390269756317139 | KNN Loss: 6.224414348602295 | BCE Loss: 1.0390269756317139\n",
      "Epoch 469 / 500 | iteration 10 / 30 | Total Loss: 1.0221285820007324 | KNN Loss: 6.224696636199951 | BCE Loss: 1.0221285820007324\n",
      "Epoch 469 / 500 | iteration 15 / 30 | Total Loss: 1.022531509399414 | KNN Loss: 6.224164009094238 | BCE Loss: 1.022531509399414\n",
      "Epoch 469 / 500 | iteration 20 / 30 | Total Loss: 1.0504423379898071 | KNN Loss: 6.224582195281982 | BCE Loss: 1.0504423379898071\n",
      "Epoch 469 / 500 | iteration 25 / 30 | Total Loss: 1.064366340637207 | KNN Loss: 6.224451065063477 | BCE Loss: 1.064366340637207\n",
      "Epoch 470 / 500 | iteration 0 / 30 | Total Loss: 1.0134161710739136 | KNN Loss: 6.224056243896484 | BCE Loss: 1.0134161710739136\n",
      "Epoch 470 / 500 | iteration 5 / 30 | Total Loss: 1.0940616130828857 | KNN Loss: 6.224137306213379 | BCE Loss: 1.0940616130828857\n",
      "Epoch 470 / 500 | iteration 10 / 30 | Total Loss: 1.060669183731079 | KNN Loss: 6.2245073318481445 | BCE Loss: 1.060669183731079\n",
      "Epoch 470 / 500 | iteration 15 / 30 | Total Loss: 1.0388727188110352 | KNN Loss: 6.224279880523682 | BCE Loss: 1.0388727188110352\n",
      "Epoch 470 / 500 | iteration 20 / 30 | Total Loss: 1.047562599182129 | KNN Loss: 6.224447250366211 | BCE Loss: 1.047562599182129\n",
      "Epoch 470 / 500 | iteration 25 / 30 | Total Loss: 1.0303819179534912 | KNN Loss: 6.224580764770508 | BCE Loss: 1.0303819179534912\n",
      "Epoch 471 / 500 | iteration 0 / 30 | Total Loss: 1.0620050430297852 | KNN Loss: 6.224649429321289 | BCE Loss: 1.0620050430297852\n",
      "Epoch 471 / 500 | iteration 5 / 30 | Total Loss: 1.0647120475769043 | KNN Loss: 6.224700450897217 | BCE Loss: 1.0647120475769043\n",
      "Epoch 471 / 500 | iteration 10 / 30 | Total Loss: 1.044830083847046 | KNN Loss: 6.224576950073242 | BCE Loss: 1.044830083847046\n",
      "Epoch 471 / 500 | iteration 15 / 30 | Total Loss: 1.0461007356643677 | KNN Loss: 6.224542617797852 | BCE Loss: 1.0461007356643677\n",
      "Epoch 471 / 500 | iteration 20 / 30 | Total Loss: 1.056959867477417 | KNN Loss: 6.224149227142334 | BCE Loss: 1.056959867477417\n",
      "Epoch 471 / 500 | iteration 25 / 30 | Total Loss: 1.034456729888916 | KNN Loss: 6.22453498840332 | BCE Loss: 1.034456729888916\n",
      "Epoch 472 / 500 | iteration 0 / 30 | Total Loss: 1.058185338973999 | KNN Loss: 6.224520206451416 | BCE Loss: 1.058185338973999\n",
      "Epoch 472 / 500 | iteration 5 / 30 | Total Loss: 1.1005786657333374 | KNN Loss: 6.224225997924805 | BCE Loss: 1.1005786657333374\n",
      "Epoch 472 / 500 | iteration 10 / 30 | Total Loss: 1.0529488325119019 | KNN Loss: 6.223998546600342 | BCE Loss: 1.0529488325119019\n",
      "Epoch 472 / 500 | iteration 15 / 30 | Total Loss: 1.050654411315918 | KNN Loss: 6.224509239196777 | BCE Loss: 1.050654411315918\n",
      "Epoch 472 / 500 | iteration 20 / 30 | Total Loss: 1.0343482494354248 | KNN Loss: 6.22443962097168 | BCE Loss: 1.0343482494354248\n",
      "Epoch 472 / 500 | iteration 25 / 30 | Total Loss: 1.0367467403411865 | KNN Loss: 6.224667072296143 | BCE Loss: 1.0367467403411865\n",
      "Epoch 473 / 500 | iteration 0 / 30 | Total Loss: 1.0443546772003174 | KNN Loss: 6.224405288696289 | BCE Loss: 1.0443546772003174\n",
      "Epoch 473 / 500 | iteration 5 / 30 | Total Loss: 1.0481274127960205 | KNN Loss: 6.224704742431641 | BCE Loss: 1.0481274127960205\n",
      "Epoch 473 / 500 | iteration 10 / 30 | Total Loss: 1.0562796592712402 | KNN Loss: 6.224490642547607 | BCE Loss: 1.0562796592712402\n",
      "Epoch 473 / 500 | iteration 15 / 30 | Total Loss: 1.077500581741333 | KNN Loss: 6.224768161773682 | BCE Loss: 1.077500581741333\n",
      "Epoch 473 / 500 | iteration 20 / 30 | Total Loss: 1.0462286472320557 | KNN Loss: 6.2244086265563965 | BCE Loss: 1.0462286472320557\n",
      "Epoch 473 / 500 | iteration 25 / 30 | Total Loss: 1.0457037687301636 | KNN Loss: 6.224514961242676 | BCE Loss: 1.0457037687301636\n",
      "Epoch 474 / 500 | iteration 0 / 30 | Total Loss: 1.0832104682922363 | KNN Loss: 6.224262714385986 | BCE Loss: 1.0832104682922363\n",
      "Epoch 474 / 500 | iteration 5 / 30 | Total Loss: 1.0412218570709229 | KNN Loss: 6.224719524383545 | BCE Loss: 1.0412218570709229\n",
      "Epoch 474 / 500 | iteration 10 / 30 | Total Loss: 1.0343997478485107 | KNN Loss: 6.224707126617432 | BCE Loss: 1.0343997478485107\n",
      "Epoch 474 / 500 | iteration 15 / 30 | Total Loss: 1.069139838218689 | KNN Loss: 6.224321365356445 | BCE Loss: 1.069139838218689\n",
      "Epoch 474 / 500 | iteration 20 / 30 | Total Loss: 1.0526742935180664 | KNN Loss: 6.2245306968688965 | BCE Loss: 1.0526742935180664\n",
      "Epoch 474 / 500 | iteration 25 / 30 | Total Loss: 1.0660840272903442 | KNN Loss: 6.224873065948486 | BCE Loss: 1.0660840272903442\n",
      "Epoch 475 / 500 | iteration 0 / 30 | Total Loss: 1.088830828666687 | KNN Loss: 6.2244062423706055 | BCE Loss: 1.088830828666687\n",
      "Epoch 475 / 500 | iteration 5 / 30 | Total Loss: 1.056335687637329 | KNN Loss: 6.224828720092773 | BCE Loss: 1.056335687637329\n",
      "Epoch 475 / 500 | iteration 10 / 30 | Total Loss: 1.0546536445617676 | KNN Loss: 6.2247161865234375 | BCE Loss: 1.0546536445617676\n",
      "Epoch 475 / 500 | iteration 15 / 30 | Total Loss: 1.0390684604644775 | KNN Loss: 6.224845886230469 | BCE Loss: 1.0390684604644775\n",
      "Epoch 475 / 500 | iteration 20 / 30 | Total Loss: 1.0390057563781738 | KNN Loss: 6.224478721618652 | BCE Loss: 1.0390057563781738\n",
      "Epoch 475 / 500 | iteration 25 / 30 | Total Loss: 1.0399858951568604 | KNN Loss: 6.224559783935547 | BCE Loss: 1.0399858951568604\n",
      "Epoch 476 / 500 | iteration 0 / 30 | Total Loss: 1.0526429414749146 | KNN Loss: 6.224547863006592 | BCE Loss: 1.0526429414749146\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 476 / 500 | iteration 5 / 30 | Total Loss: 1.0375841856002808 | KNN Loss: 6.224062919616699 | BCE Loss: 1.0375841856002808\n",
      "Epoch 476 / 500 | iteration 10 / 30 | Total Loss: 1.052534818649292 | KNN Loss: 6.224465847015381 | BCE Loss: 1.052534818649292\n",
      "Epoch 476 / 500 | iteration 15 / 30 | Total Loss: 1.045093059539795 | KNN Loss: 6.224832057952881 | BCE Loss: 1.045093059539795\n",
      "Epoch 476 / 500 | iteration 20 / 30 | Total Loss: 1.063495397567749 | KNN Loss: 6.224554538726807 | BCE Loss: 1.063495397567749\n",
      "Epoch 476 / 500 | iteration 25 / 30 | Total Loss: 1.0082381963729858 | KNN Loss: 6.22435188293457 | BCE Loss: 1.0082381963729858\n",
      "Epoch 477 / 500 | iteration 0 / 30 | Total Loss: 1.0363152027130127 | KNN Loss: 6.224386215209961 | BCE Loss: 1.0363152027130127\n",
      "Epoch 477 / 500 | iteration 5 / 30 | Total Loss: 1.0466156005859375 | KNN Loss: 6.224661350250244 | BCE Loss: 1.0466156005859375\n",
      "Epoch 477 / 500 | iteration 10 / 30 | Total Loss: 1.0606563091278076 | KNN Loss: 6.22452449798584 | BCE Loss: 1.0606563091278076\n",
      "Epoch 477 / 500 | iteration 15 / 30 | Total Loss: 1.0722951889038086 | KNN Loss: 6.224356651306152 | BCE Loss: 1.0722951889038086\n",
      "Epoch 477 / 500 | iteration 20 / 30 | Total Loss: 1.0199639797210693 | KNN Loss: 6.224480628967285 | BCE Loss: 1.0199639797210693\n",
      "Epoch 477 / 500 | iteration 25 / 30 | Total Loss: 1.045365571975708 | KNN Loss: 6.224542617797852 | BCE Loss: 1.045365571975708\n",
      "Epoch 478 / 500 | iteration 0 / 30 | Total Loss: 1.03902006149292 | KNN Loss: 6.224416732788086 | BCE Loss: 1.03902006149292\n",
      "Epoch 478 / 500 | iteration 5 / 30 | Total Loss: 1.0203216075897217 | KNN Loss: 6.224274158477783 | BCE Loss: 1.0203216075897217\n",
      "Epoch 478 / 500 | iteration 10 / 30 | Total Loss: 1.0411008596420288 | KNN Loss: 6.224350452423096 | BCE Loss: 1.0411008596420288\n",
      "Epoch 478 / 500 | iteration 15 / 30 | Total Loss: 1.0471887588500977 | KNN Loss: 6.224203109741211 | BCE Loss: 1.0471887588500977\n",
      "Epoch 478 / 500 | iteration 20 / 30 | Total Loss: 1.0928187370300293 | KNN Loss: 6.224490165710449 | BCE Loss: 1.0928187370300293\n",
      "Epoch 478 / 500 | iteration 25 / 30 | Total Loss: 1.0684638023376465 | KNN Loss: 6.22476053237915 | BCE Loss: 1.0684638023376465\n",
      "Epoch 479 / 500 | iteration 0 / 30 | Total Loss: 1.0414423942565918 | KNN Loss: 6.22435998916626 | BCE Loss: 1.0414423942565918\n",
      "Epoch 479 / 500 | iteration 5 / 30 | Total Loss: 1.0570038557052612 | KNN Loss: 6.224324703216553 | BCE Loss: 1.0570038557052612\n",
      "Epoch 479 / 500 | iteration 10 / 30 | Total Loss: 1.0767796039581299 | KNN Loss: 6.224503993988037 | BCE Loss: 1.0767796039581299\n",
      "Epoch 479 / 500 | iteration 15 / 30 | Total Loss: 1.0640010833740234 | KNN Loss: 6.224299907684326 | BCE Loss: 1.0640010833740234\n",
      "Epoch 479 / 500 | iteration 20 / 30 | Total Loss: 1.0591989755630493 | KNN Loss: 6.224435329437256 | BCE Loss: 1.0591989755630493\n",
      "Epoch 479 / 500 | iteration 25 / 30 | Total Loss: 1.0501527786254883 | KNN Loss: 6.224104881286621 | BCE Loss: 1.0501527786254883\n",
      "Epoch 480 / 500 | iteration 0 / 30 | Total Loss: 1.027580738067627 | KNN Loss: 6.224494934082031 | BCE Loss: 1.027580738067627\n",
      "Epoch 480 / 500 | iteration 5 / 30 | Total Loss: 1.0542794466018677 | KNN Loss: 6.2243781089782715 | BCE Loss: 1.0542794466018677\n",
      "Epoch 480 / 500 | iteration 10 / 30 | Total Loss: 1.064712405204773 | KNN Loss: 6.224781513214111 | BCE Loss: 1.064712405204773\n",
      "Epoch 480 / 500 | iteration 15 / 30 | Total Loss: 1.0601089000701904 | KNN Loss: 6.224519729614258 | BCE Loss: 1.0601089000701904\n",
      "Epoch 480 / 500 | iteration 20 / 30 | Total Loss: 1.0387718677520752 | KNN Loss: 6.224625587463379 | BCE Loss: 1.0387718677520752\n",
      "Epoch 480 / 500 | iteration 25 / 30 | Total Loss: 1.0575644969940186 | KNN Loss: 6.224313735961914 | BCE Loss: 1.0575644969940186\n",
      "Epoch 481 / 500 | iteration 0 / 30 | Total Loss: 1.02243971824646 | KNN Loss: 6.223904609680176 | BCE Loss: 1.02243971824646\n",
      "Epoch 481 / 500 | iteration 5 / 30 | Total Loss: 1.0802595615386963 | KNN Loss: 6.224356651306152 | BCE Loss: 1.0802595615386963\n",
      "Epoch 481 / 500 | iteration 10 / 30 | Total Loss: 1.0644124746322632 | KNN Loss: 6.22482442855835 | BCE Loss: 1.0644124746322632\n",
      "Epoch 481 / 500 | iteration 15 / 30 | Total Loss: 1.0295236110687256 | KNN Loss: 6.224696636199951 | BCE Loss: 1.0295236110687256\n",
      "Epoch 481 / 500 | iteration 20 / 30 | Total Loss: 1.0592069625854492 | KNN Loss: 6.2243571281433105 | BCE Loss: 1.0592069625854492\n",
      "Epoch 481 / 500 | iteration 25 / 30 | Total Loss: 1.040442705154419 | KNN Loss: 6.224210262298584 | BCE Loss: 1.040442705154419\n",
      "Epoch 482 / 500 | iteration 0 / 30 | Total Loss: 1.0142936706542969 | KNN Loss: 6.224589824676514 | BCE Loss: 1.0142936706542969\n",
      "Epoch 482 / 500 | iteration 5 / 30 | Total Loss: 1.0310933589935303 | KNN Loss: 6.224698066711426 | BCE Loss: 1.0310933589935303\n",
      "Epoch 482 / 500 | iteration 10 / 30 | Total Loss: 1.0560745000839233 | KNN Loss: 6.224416732788086 | BCE Loss: 1.0560745000839233\n",
      "Epoch 482 / 500 | iteration 15 / 30 | Total Loss: 1.0234014987945557 | KNN Loss: 6.224757194519043 | BCE Loss: 1.0234014987945557\n",
      "Epoch 482 / 500 | iteration 20 / 30 | Total Loss: 1.0110450983047485 | KNN Loss: 6.224564075469971 | BCE Loss: 1.0110450983047485\n",
      "Epoch 482 / 500 | iteration 25 / 30 | Total Loss: 1.0564409494400024 | KNN Loss: 6.2246832847595215 | BCE Loss: 1.0564409494400024\n",
      "Epoch 483 / 500 | iteration 0 / 30 | Total Loss: 1.0415332317352295 | KNN Loss: 6.224706172943115 | BCE Loss: 1.0415332317352295\n",
      "Epoch 483 / 500 | iteration 5 / 30 | Total Loss: 1.0598433017730713 | KNN Loss: 6.224068641662598 | BCE Loss: 1.0598433017730713\n",
      "Epoch 483 / 500 | iteration 10 / 30 | Total Loss: 1.0641974210739136 | KNN Loss: 6.224618434906006 | BCE Loss: 1.0641974210739136\n",
      "Epoch 483 / 500 | iteration 15 / 30 | Total Loss: 1.0142865180969238 | KNN Loss: 6.224715232849121 | BCE Loss: 1.0142865180969238\n",
      "Epoch 483 / 500 | iteration 20 / 30 | Total Loss: 1.0653035640716553 | KNN Loss: 6.224591255187988 | BCE Loss: 1.0653035640716553\n",
      "Epoch 483 / 500 | iteration 25 / 30 | Total Loss: 1.06644868850708 | KNN Loss: 6.224837303161621 | BCE Loss: 1.06644868850708\n",
      "Epoch 484 / 500 | iteration 0 / 30 | Total Loss: 1.0525120496749878 | KNN Loss: 6.224145412445068 | BCE Loss: 1.0525120496749878\n",
      "Epoch 484 / 500 | iteration 5 / 30 | Total Loss: 1.0717976093292236 | KNN Loss: 6.2242512702941895 | BCE Loss: 1.0717976093292236\n",
      "Epoch 484 / 500 | iteration 10 / 30 | Total Loss: 1.028711199760437 | KNN Loss: 6.224062442779541 | BCE Loss: 1.028711199760437\n",
      "Epoch 484 / 500 | iteration 15 / 30 | Total Loss: 1.0392932891845703 | KNN Loss: 6.225090026855469 | BCE Loss: 1.0392932891845703\n",
      "Epoch 484 / 500 | iteration 20 / 30 | Total Loss: 1.0140867233276367 | KNN Loss: 6.22456693649292 | BCE Loss: 1.0140867233276367\n",
      "Epoch 484 / 500 | iteration 25 / 30 | Total Loss: 1.0582571029663086 | KNN Loss: 6.22459077835083 | BCE Loss: 1.0582571029663086\n",
      "Epoch 485 / 500 | iteration 0 / 30 | Total Loss: 1.0415363311767578 | KNN Loss: 6.223977565765381 | BCE Loss: 1.0415363311767578\n",
      "Epoch 485 / 500 | iteration 5 / 30 | Total Loss: 1.0631556510925293 | KNN Loss: 6.22409200668335 | BCE Loss: 1.0631556510925293\n",
      "Epoch 485 / 500 | iteration 10 / 30 | Total Loss: 1.0649696588516235 | KNN Loss: 6.224462985992432 | BCE Loss: 1.0649696588516235\n",
      "Epoch 485 / 500 | iteration 15 / 30 | Total Loss: 1.069075584411621 | KNN Loss: 6.22477388381958 | BCE Loss: 1.069075584411621\n",
      "Epoch 485 / 500 | iteration 20 / 30 | Total Loss: 1.0517561435699463 | KNN Loss: 6.224245548248291 | BCE Loss: 1.0517561435699463\n",
      "Epoch 485 / 500 | iteration 25 / 30 | Total Loss: 1.0903446674346924 | KNN Loss: 6.224512100219727 | BCE Loss: 1.0903446674346924\n",
      "Epoch 486 / 500 | iteration 0 / 30 | Total Loss: 1.0466073751449585 | KNN Loss: 6.224420547485352 | BCE Loss: 1.0466073751449585\n",
      "Epoch 486 / 500 | iteration 5 / 30 | Total Loss: 1.0399566888809204 | KNN Loss: 6.224767208099365 | BCE Loss: 1.0399566888809204\n",
      "Epoch 486 / 500 | iteration 10 / 30 | Total Loss: 1.0695298910140991 | KNN Loss: 6.224457263946533 | BCE Loss: 1.0695298910140991\n",
      "Epoch 486 / 500 | iteration 15 / 30 | Total Loss: 1.0316182374954224 | KNN Loss: 6.224422931671143 | BCE Loss: 1.0316182374954224\n",
      "Epoch 486 / 500 | iteration 20 / 30 | Total Loss: 1.003293752670288 | KNN Loss: 6.2243428230285645 | BCE Loss: 1.003293752670288\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 486 / 500 | iteration 25 / 30 | Total Loss: 1.055832862854004 | KNN Loss: 6.22460412979126 | BCE Loss: 1.055832862854004\n",
      "Epoch 487 / 500 | iteration 0 / 30 | Total Loss: 1.053954839706421 | KNN Loss: 6.224515438079834 | BCE Loss: 1.053954839706421\n",
      "Epoch 487 / 500 | iteration 5 / 30 | Total Loss: 1.0651004314422607 | KNN Loss: 6.224644660949707 | BCE Loss: 1.0651004314422607\n",
      "Epoch 487 / 500 | iteration 10 / 30 | Total Loss: 1.057348370552063 | KNN Loss: 6.224420547485352 | BCE Loss: 1.057348370552063\n",
      "Epoch 487 / 500 | iteration 15 / 30 | Total Loss: 1.0303072929382324 | KNN Loss: 6.224255084991455 | BCE Loss: 1.0303072929382324\n",
      "Epoch 487 / 500 | iteration 20 / 30 | Total Loss: 1.054123878479004 | KNN Loss: 6.224785327911377 | BCE Loss: 1.054123878479004\n",
      "Epoch 487 / 500 | iteration 25 / 30 | Total Loss: 1.0397799015045166 | KNN Loss: 6.224265098571777 | BCE Loss: 1.0397799015045166\n",
      "Epoch 488 / 500 | iteration 0 / 30 | Total Loss: 1.0715899467468262 | KNN Loss: 6.2243733406066895 | BCE Loss: 1.0715899467468262\n",
      "Epoch 488 / 500 | iteration 5 / 30 | Total Loss: 1.055432677268982 | KNN Loss: 6.224528789520264 | BCE Loss: 1.055432677268982\n",
      "Epoch 488 / 500 | iteration 10 / 30 | Total Loss: 1.059959053993225 | KNN Loss: 6.224583625793457 | BCE Loss: 1.059959053993225\n",
      "Epoch 488 / 500 | iteration 15 / 30 | Total Loss: 1.059669852256775 | KNN Loss: 6.224645614624023 | BCE Loss: 1.059669852256775\n",
      "Epoch 488 / 500 | iteration 20 / 30 | Total Loss: 1.0271934270858765 | KNN Loss: 6.22448205947876 | BCE Loss: 1.0271934270858765\n",
      "Epoch 488 / 500 | iteration 25 / 30 | Total Loss: 1.0566375255584717 | KNN Loss: 6.224373817443848 | BCE Loss: 1.0566375255584717\n",
      "Epoch 489 / 500 | iteration 0 / 30 | Total Loss: 1.0366430282592773 | KNN Loss: 6.224794387817383 | BCE Loss: 1.0366430282592773\n",
      "Epoch 489 / 500 | iteration 5 / 30 | Total Loss: 1.0865658521652222 | KNN Loss: 6.224470138549805 | BCE Loss: 1.0865658521652222\n",
      "Epoch 489 / 500 | iteration 10 / 30 | Total Loss: 1.0756289958953857 | KNN Loss: 6.224790573120117 | BCE Loss: 1.0756289958953857\n",
      "Epoch 489 / 500 | iteration 15 / 30 | Total Loss: 1.0485987663269043 | KNN Loss: 6.224767208099365 | BCE Loss: 1.0485987663269043\n",
      "Epoch 489 / 500 | iteration 20 / 30 | Total Loss: 1.0488669872283936 | KNN Loss: 6.224459648132324 | BCE Loss: 1.0488669872283936\n",
      "Epoch 489 / 500 | iteration 25 / 30 | Total Loss: 1.0506709814071655 | KNN Loss: 6.224323749542236 | BCE Loss: 1.0506709814071655\n",
      "Epoch 490 / 500 | iteration 0 / 30 | Total Loss: 1.0576469898223877 | KNN Loss: 6.2245941162109375 | BCE Loss: 1.0576469898223877\n",
      "Epoch 490 / 500 | iteration 5 / 30 | Total Loss: 1.0182493925094604 | KNN Loss: 6.224348068237305 | BCE Loss: 1.0182493925094604\n",
      "Epoch 490 / 500 | iteration 10 / 30 | Total Loss: 1.0270260572433472 | KNN Loss: 6.224311351776123 | BCE Loss: 1.0270260572433472\n",
      "Epoch 490 / 500 | iteration 15 / 30 | Total Loss: 1.0428225994110107 | KNN Loss: 6.224812984466553 | BCE Loss: 1.0428225994110107\n",
      "Epoch 490 / 500 | iteration 20 / 30 | Total Loss: 1.0803029537200928 | KNN Loss: 6.224402904510498 | BCE Loss: 1.0803029537200928\n",
      "Epoch 490 / 500 | iteration 25 / 30 | Total Loss: 1.0699026584625244 | KNN Loss: 6.224250793457031 | BCE Loss: 1.0699026584625244\n",
      "Epoch 491 / 500 | iteration 0 / 30 | Total Loss: 1.0544562339782715 | KNN Loss: 6.224144458770752 | BCE Loss: 1.0544562339782715\n",
      "Epoch 491 / 500 | iteration 5 / 30 | Total Loss: 1.0392309427261353 | KNN Loss: 6.224542140960693 | BCE Loss: 1.0392309427261353\n",
      "Epoch 491 / 500 | iteration 10 / 30 | Total Loss: 1.0686161518096924 | KNN Loss: 6.224470138549805 | BCE Loss: 1.0686161518096924\n",
      "Epoch 491 / 500 | iteration 15 / 30 | Total Loss: 1.0572476387023926 | KNN Loss: 6.2242937088012695 | BCE Loss: 1.0572476387023926\n",
      "Epoch 491 / 500 | iteration 20 / 30 | Total Loss: 1.0463755130767822 | KNN Loss: 6.224055290222168 | BCE Loss: 1.0463755130767822\n",
      "Epoch 491 / 500 | iteration 25 / 30 | Total Loss: 1.0238001346588135 | KNN Loss: 6.224564552307129 | BCE Loss: 1.0238001346588135\n",
      "Epoch 492 / 500 | iteration 0 / 30 | Total Loss: 1.0472060441970825 | KNN Loss: 6.2239603996276855 | BCE Loss: 1.0472060441970825\n",
      "Epoch 492 / 500 | iteration 5 / 30 | Total Loss: 1.0507490634918213 | KNN Loss: 6.224310874938965 | BCE Loss: 1.0507490634918213\n",
      "Epoch 492 / 500 | iteration 10 / 30 | Total Loss: 1.0356957912445068 | KNN Loss: 6.224234580993652 | BCE Loss: 1.0356957912445068\n",
      "Epoch 492 / 500 | iteration 15 / 30 | Total Loss: 1.057957410812378 | KNN Loss: 6.224649906158447 | BCE Loss: 1.057957410812378\n",
      "Epoch 492 / 500 | iteration 20 / 30 | Total Loss: 1.050474762916565 | KNN Loss: 6.224438190460205 | BCE Loss: 1.050474762916565\n",
      "Epoch 492 / 500 | iteration 25 / 30 | Total Loss: 1.0648499727249146 | KNN Loss: 6.224522113800049 | BCE Loss: 1.0648499727249146\n",
      "Epoch 493 / 500 | iteration 0 / 30 | Total Loss: 1.0347790718078613 | KNN Loss: 6.224308013916016 | BCE Loss: 1.0347790718078613\n",
      "Epoch 493 / 500 | iteration 5 / 30 | Total Loss: 1.0547796487808228 | KNN Loss: 6.224282741546631 | BCE Loss: 1.0547796487808228\n",
      "Epoch 493 / 500 | iteration 10 / 30 | Total Loss: 1.062143087387085 | KNN Loss: 6.224493980407715 | BCE Loss: 1.062143087387085\n",
      "Epoch 493 / 500 | iteration 15 / 30 | Total Loss: 1.045548915863037 | KNN Loss: 6.224379062652588 | BCE Loss: 1.045548915863037\n",
      "Epoch 493 / 500 | iteration 20 / 30 | Total Loss: 1.0750679969787598 | KNN Loss: 6.224301338195801 | BCE Loss: 1.0750679969787598\n",
      "Epoch 493 / 500 | iteration 25 / 30 | Total Loss: 1.017296552658081 | KNN Loss: 6.224663734436035 | BCE Loss: 1.017296552658081\n",
      "Epoch 494 / 500 | iteration 0 / 30 | Total Loss: 1.055897831916809 | KNN Loss: 6.224462985992432 | BCE Loss: 1.055897831916809\n",
      "Epoch 494 / 500 | iteration 5 / 30 | Total Loss: 1.06158447265625 | KNN Loss: 6.224514484405518 | BCE Loss: 1.06158447265625\n",
      "Epoch 494 / 500 | iteration 10 / 30 | Total Loss: 1.052874207496643 | KNN Loss: 6.224451541900635 | BCE Loss: 1.052874207496643\n",
      "Epoch 494 / 500 | iteration 15 / 30 | Total Loss: 1.0378140211105347 | KNN Loss: 6.224129676818848 | BCE Loss: 1.0378140211105347\n",
      "Epoch 494 / 500 | iteration 20 / 30 | Total Loss: 1.0432250499725342 | KNN Loss: 6.224431991577148 | BCE Loss: 1.0432250499725342\n",
      "Epoch 494 / 500 | iteration 25 / 30 | Total Loss: 1.0473413467407227 | KNN Loss: 6.224518299102783 | BCE Loss: 1.0473413467407227\n",
      "Epoch 495 / 500 | iteration 0 / 30 | Total Loss: 1.036861777305603 | KNN Loss: 6.224282741546631 | BCE Loss: 1.036861777305603\n",
      "Epoch 495 / 500 | iteration 5 / 30 | Total Loss: 1.0645060539245605 | KNN Loss: 6.224782466888428 | BCE Loss: 1.0645060539245605\n",
      "Epoch 495 / 500 | iteration 10 / 30 | Total Loss: 1.0641462802886963 | KNN Loss: 6.2244086265563965 | BCE Loss: 1.0641462802886963\n",
      "Epoch 495 / 500 | iteration 15 / 30 | Total Loss: 1.0262669324874878 | KNN Loss: 6.224226474761963 | BCE Loss: 1.0262669324874878\n",
      "Epoch 495 / 500 | iteration 20 / 30 | Total Loss: 1.0466339588165283 | KNN Loss: 6.224761009216309 | BCE Loss: 1.0466339588165283\n",
      "Epoch 495 / 500 | iteration 25 / 30 | Total Loss: 1.0593507289886475 | KNN Loss: 6.22407865524292 | BCE Loss: 1.0593507289886475\n",
      "Epoch 496 / 500 | iteration 0 / 30 | Total Loss: 1.0660912990570068 | KNN Loss: 6.224166393280029 | BCE Loss: 1.0660912990570068\n",
      "Epoch 496 / 500 | iteration 5 / 30 | Total Loss: 1.0764997005462646 | KNN Loss: 6.224884986877441 | BCE Loss: 1.0764997005462646\n",
      "Epoch 496 / 500 | iteration 10 / 30 | Total Loss: 1.075552225112915 | KNN Loss: 6.224493980407715 | BCE Loss: 1.075552225112915\n",
      "Epoch 496 / 500 | iteration 15 / 30 | Total Loss: 1.0373268127441406 | KNN Loss: 6.224247932434082 | BCE Loss: 1.0373268127441406\n",
      "Epoch 496 / 500 | iteration 20 / 30 | Total Loss: 1.0549910068511963 | KNN Loss: 6.224308490753174 | BCE Loss: 1.0549910068511963\n",
      "Epoch 496 / 500 | iteration 25 / 30 | Total Loss: 1.0789002180099487 | KNN Loss: 6.22416877746582 | BCE Loss: 1.0789002180099487\n",
      "Epoch 497 / 500 | iteration 0 / 30 | Total Loss: 1.0507653951644897 | KNN Loss: 6.224900722503662 | BCE Loss: 1.0507653951644897\n",
      "Epoch 497 / 500 | iteration 5 / 30 | Total Loss: 1.0805625915527344 | KNN Loss: 6.2244157791137695 | BCE Loss: 1.0805625915527344\n",
      "Epoch 497 / 500 | iteration 10 / 30 | Total Loss: 1.0265698432922363 | KNN Loss: 6.224449634552002 | BCE Loss: 1.0265698432922363\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 497 / 500 | iteration 15 / 30 | Total Loss: 1.0636194944381714 | KNN Loss: 6.224575996398926 | BCE Loss: 1.0636194944381714\n",
      "Epoch 497 / 500 | iteration 20 / 30 | Total Loss: 1.0476899147033691 | KNN Loss: 6.2240729331970215 | BCE Loss: 1.0476899147033691\n",
      "Epoch 497 / 500 | iteration 25 / 30 | Total Loss: 1.036081314086914 | KNN Loss: 6.224581241607666 | BCE Loss: 1.036081314086914\n",
      "Epoch 498 / 500 | iteration 0 / 30 | Total Loss: 1.066345453262329 | KNN Loss: 6.224164009094238 | BCE Loss: 1.066345453262329\n",
      "Epoch 498 / 500 | iteration 5 / 30 | Total Loss: 1.0668036937713623 | KNN Loss: 6.224806785583496 | BCE Loss: 1.0668036937713623\n",
      "Epoch 498 / 500 | iteration 10 / 30 | Total Loss: 1.0526536703109741 | KNN Loss: 6.224017143249512 | BCE Loss: 1.0526536703109741\n",
      "Epoch 498 / 500 | iteration 15 / 30 | Total Loss: 1.026498794555664 | KNN Loss: 6.224737167358398 | BCE Loss: 1.026498794555664\n",
      "Epoch 498 / 500 | iteration 20 / 30 | Total Loss: 1.048725962638855 | KNN Loss: 6.22463321685791 | BCE Loss: 1.048725962638855\n",
      "Epoch 498 / 500 | iteration 25 / 30 | Total Loss: 1.0474793910980225 | KNN Loss: 6.224341869354248 | BCE Loss: 1.0474793910980225\n",
      "Epoch 499 / 500 | iteration 0 / 30 | Total Loss: 1.0491541624069214 | KNN Loss: 6.224906921386719 | BCE Loss: 1.0491541624069214\n",
      "Epoch 499 / 500 | iteration 5 / 30 | Total Loss: 1.034808874130249 | KNN Loss: 6.224677085876465 | BCE Loss: 1.034808874130249\n",
      "Epoch 499 / 500 | iteration 10 / 30 | Total Loss: 1.076812982559204 | KNN Loss: 6.224250316619873 | BCE Loss: 1.076812982559204\n",
      "Epoch 499 / 500 | iteration 15 / 30 | Total Loss: 1.0578176975250244 | KNN Loss: 6.22405481338501 | BCE Loss: 1.0578176975250244\n",
      "Epoch 499 / 500 | iteration 20 / 30 | Total Loss: 1.0519464015960693 | KNN Loss: 6.224578380584717 | BCE Loss: 1.0519464015960693\n",
      "Epoch 499 / 500 | iteration 25 / 30 | Total Loss: 1.0441774129867554 | KNN Loss: 6.224592208862305 | BCE Loss: 1.0441774129867554\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "data_iter = torch.utils.data.DataLoader(dataset,\n",
    "                                     batch_size=batch_size,\n",
    "                                     shuffle=True,\n",
    "                                     num_workers=1,\n",
    "                                     pin_memory=True)\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', verbose=True, factor=0.7, threshold=1e-4)\n",
    "knn_crt = KNNLoss(k=k).to(device)\n",
    "losses = []\n",
    "alpha = 10/170\n",
    "gamma = 2\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for iteration, (batch, target) in enumerate(data_iter):\n",
    "        batch = batch.to(device)\n",
    "        target = target.to(device)\n",
    "        outputs, iterm = model(batch, return_intermidiate=True)\n",
    "        mse_loss = F.binary_cross_entropy_with_logits(outputs, target, reduction='none')\n",
    "        mask = torch.ones_like(mse_loss)\n",
    "        mask[target == 0] = alpha ** gamma\n",
    "        mask[target == 1] = (1 - alpha) ** gamma\n",
    "        mse_loss = (mse_loss * mask).sum(dim=-1).mean()\n",
    "        try:\n",
    "            knn_loss = knn_crt(iterm)\n",
    "            if torch.isinf(knn_loss):\n",
    "                knn_loss = 0\n",
    "        except ValueError:\n",
    "            knn_loss = torch.tensor(0)\n",
    "        loss = mse_loss + knn_loss\n",
    "        loss = mse_loss\n",
    "        total_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if iteration % log_every == 0:\n",
    "            print(f\"Epoch {epoch} / {epochs} | iteration {iteration} / {len(data_iter)} | Total Loss: {loss.item()} | KNN Loss: {knn_loss.item()} | BCE Loss: {mse_loss.item()}\")\n",
    "    \n",
    "    scheduler.step(total_loss / (iteration + 1))\n",
    "    losses.append(total_loss / (iteration + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.9231,  3.8722,  2.5812,  3.5745,  3.4543,  0.7017,  2.6651,  2.1970,\n",
      "          2.3083,  1.9925,  2.2344,  2.1979,  0.7876,  1.8203,  1.2891,  1.5216,\n",
      "          2.8064,  3.1788,  2.7991,  2.3030,  1.7450,  2.9516,  2.2904,  2.6379,\n",
      "          2.5332,  1.7390,  2.1229,  1.4155,  1.4934,  0.3250, -0.2396,  0.9983,\n",
      "          0.2212,  0.9240,  1.5308,  1.4778,  1.0049,  3.3144,  0.8001,  1.3198,\n",
      "          0.9675, -0.7032, -0.2357,  2.3364,  2.1887,  0.7335, -0.2012,  0.0970,\n",
      "          1.4607,  2.4952,  1.8230,  0.1381,  1.4258,  0.5204, -0.6363,  1.1095,\n",
      "          1.4810,  1.3720,  1.3400,  1.8270,  0.5738,  0.8433,  0.1389,  1.7235,\n",
      "          1.3158,  1.6659, -1.8257,  0.3069,  2.2878,  2.1419,  2.5500,  0.4267,\n",
      "          1.3524,  2.4587,  1.9961,  1.2923,  0.2264,  0.7375,  0.2173,  1.5898,\n",
      "          0.0257,  0.3779,  1.8381, -0.3734,  0.2388, -1.0732, -2.2649, -0.2561,\n",
      "          0.5456, -1.8580,  0.4615, -0.1291, -0.5688, -0.9364,  0.5635,  1.2683,\n",
      "         -0.6937, -0.7033,  0.3464,  1.1352,  0.6856, -1.2365,  0.8952,  1.1243,\n",
      "         -1.2352, -1.1404, -0.1560,  0.0262, -1.0285, -1.6657, -0.4297, -2.5551,\n",
      "         -0.3807,  1.7576,  1.5741, -0.2836, -0.5959,  0.0245,  1.5415, -2.4229,\n",
      "          0.1550, -0.1913,  0.4610, -0.7066,  0.0115, -0.7768, -0.9856,  0.9555,\n",
      "          0.2542, -0.5711,  0.3443, -0.6563, -1.3020, -0.3326, -0.5114,  0.8507,\n",
      "         -0.4546,  0.1265, -1.9411, -0.9865, -1.3692,  0.6011, -1.8337, -0.9706,\n",
      "         -1.0133, -0.6127, -1.5804, -1.0726, -2.2838, -0.9799, -1.2998, -0.3544,\n",
      "         -1.7549,  0.4606, -1.5011, -0.5358, -2.9129,  0.1758, -0.1748, -0.7738,\n",
      "         -2.1404, -1.6598, -1.2479, -1.3717, -2.2958, -2.2745, -2.8898]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor(-2.9129, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "tensor(3.8722, device='cuda:0', grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "outputs, iterm = model(dataset[67][0].unsqueeze(0).to(device), return_intermidiate=True)\n",
    "print(outputs)\n",
    "print(outputs.min())\n",
    "print(outputs.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48eee9abbdd94c13ae42800af39d409e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.transform = torchvision.transforms.Compose([\n",
    "    BinaryEncodingTransform(mapping=dataset.items_to_idx),\n",
    "]\n",
    ")\n",
    "dataset.target_transform = torchvision.transforms.Compose([\n",
    "    BinaryEncodingTransform(mapping=dataset.items_to_idx),\n",
    "]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_ = [d[0].cpu() for d in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:00<00:00, 72.73it/s]\n"
     ]
    }
   ],
   "source": [
    "model = model.eval().cpu()\n",
    "projections = model.calculate_intermidiate(dataset_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5a8d022c424491c8ae897dd2548a40a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f71786eeca5f47ada3c08d4858c546a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "distances = pairwise_distances(projections)\n",
    "# distances = np.triu(distances)\n",
    "distances_f = distances.flatten()\n",
    "\n",
    "plt.matshow(distances)\n",
    "plt.colorbar()\n",
    "plt.figure()\n",
    "plt.hist(distances_f[distances_f > 0], bins=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit DBSCAN and calculate indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = DBSCAN(eps=0.01, min_samples=80).fit_predict(projections)\n",
    "# scores = []\n",
    "# best_score = float('inf')\n",
    "# clusters = None\n",
    "# range_ = list(range(5, 20))\n",
    "# for k in tqdm(range_):\n",
    "#     y = GaussianMixture(n_components=k).fit_predict(projections)\n",
    "#     cur_score = davies_bouldin_score(projections, y)\n",
    "#     scores.append(cur_score)\n",
    "    \n",
    "#     if cur_score < best_score:\n",
    "#         best_score = cur_score\n",
    "#         clusters = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58f12c7688e841e29a8c4b53ff96960e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "perplexity = 100\n",
    "\n",
    "p = reduce_dims_and_plot(projections[clusters != -1],\n",
    "                         y=clusters[clusters != -1],\n",
    "                         title=f'perplexity: {perplexity}',\n",
    "                         file_name=None,\n",
    "                         perplexity=perplexity,\n",
    "                         library='Multicore-TSNE',\n",
    "                         perform_PCA=False,\n",
    "                         projected=None,\n",
    "                         figure_type='2d',\n",
    "                         show_figure=True,\n",
    "                         close_figure=False,\n",
    "                         text=None)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn import tree\n",
    "# from sklearn.tree import _tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_dataset = torch.stack(dataset_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = DecisionTreeClassifier(max_depth=200, min_samples_leaf=5)\n",
    "# clf = clf.fit(tensor_dataset[clusters!=-1], clusters[clusters != -1])\n",
    "# print(clf.score(tensor_dataset[clusters!=-1], clusters[clusters != -1]))\n",
    "# print(clf.get_depth())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores = []\n",
    "# for min_samples in range(1,50, 1):\n",
    "#     clf = DecisionTreeClassifier(max_depth=200, min_samples_leaf=min_samples)\n",
    "#     clf = clf.fit(tensor_dataset[clusters!=-1], clusters[clusters != -1])\n",
    "#     scores.append(clf.score(tensor_dataset[clusters!=-1], clusters[clusters != -1]))\n",
    "    \n",
    "# plt.figure()\n",
    "# plt.plot(list(range(1,50, 1)), scores)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rules(tree, feature_names, class_names):\n",
    "    tree_ = tree.tree_\n",
    "    feature_name = [\n",
    "        feature_names[i] if i != _tree.TREE_UNDEFINED else \"undefined!\"\n",
    "        for i in tree_.feature\n",
    "    ]\n",
    "\n",
    "    paths = []\n",
    "    path = []\n",
    "    \n",
    "    def recurse(node, path, paths):\n",
    "        if tree_.feature[node] != _tree.TREE_UNDEFINED:\n",
    "            name = feature_name[node]\n",
    "            threshold = tree_.threshold[node]\n",
    "            p1, p2 = list(path), list(path)\n",
    "#             p1 += [f\"({name} <= {np.round(threshold, 3)})\"]\n",
    "            p1 += [(name, '<=', np.round(threshold, 3))]\n",
    "            recurse(tree_.children_left[node], p1, paths)\n",
    "            p2 += [(name, '>', np.round(threshold, 3))]\n",
    "            recurse(tree_.children_right[node], p2, paths)\n",
    "        else:\n",
    "            path += [(tree_.value[node], tree_.n_node_samples[node])]\n",
    "            paths += [path]\n",
    "            \n",
    "    recurse(0, path, paths)\n",
    "\n",
    "    # sort by samples count\n",
    "    samples_count = [p[-1][1] for p in paths]\n",
    "    ii = list(np.argsort(samples_count))\n",
    "    paths = [paths[i] for i in reversed(ii)]\n",
    "    \n",
    "    rules = []\n",
    "    for path in paths:\n",
    "        rule = []\n",
    "        \n",
    "        for p in path[:-1]:\n",
    "            rule += [p]\n",
    "        target = \" then \"\n",
    "        if class_names is None:\n",
    "            target += \"response: \"+str(np.round(path[-1][0][0][0],3))\n",
    "        else:\n",
    "            classes = path[-1][0][0]\n",
    "            l = np.argmax(classes)\n",
    "            target += f\"class: {class_names[l]} (proba: {np.round(100.0*classes[l]/np.sum(classes),2)}%)\"\n",
    "           \n",
    "        proba = np.round(100.0*classes[l]/np.sum(classes),2)\n",
    "        target += f\" | based on {path[-1][1]:,} samples\"\n",
    "        rule_wrapper = {'target': target, 'rule': rule, 'proba': proba}\n",
    "        rules += [rule_wrapper]\n",
    "        \n",
    "    return rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rules = get_rules(clf, dataset.items, clusters[clusters != -1])\n",
    "\n",
    "# for rule in rules:\n",
    "#     n_pos = 0\n",
    "#     for c,p,v in rule['rule']:\n",
    "#         if p == '>':\n",
    "#             n_pos += 1\n",
    "#     rule['pos'] = n_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure()\n",
    "# probs = [r['proba'] for r in rules]\n",
    "# plt.hist(probs, bins = 100)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rules = sorted(rules, key=lambda x:x['pos'])\n",
    "# rules = [r for r in rules if r['proba'] > 50]\n",
    "# print(len(rules))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(17):\n",
    "#     r_i = rules[i]\n",
    "#     print(f\"------------- rule {i} length {len(r_i)} -------------\")\n",
    "#     print(r_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Soft-Decision-Tree given the self-labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_dataset = list(zip(tensor_dataset[clusters!=-1], clusters[clusters != -1]))\n",
    "batch_size = 512\n",
    "tree_loader = torch.utils.data.DataLoader(tree_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define how we prune the weights of a node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_node(node_weights, factor=1):\n",
    "    w = node_weights.cpu().detach().numpy()\n",
    "    mean_ = np.mean(w)\n",
    "    std_ = np.std(w)\n",
    "    node_weights[((mean_ - std_ * factor) < node_weights) & (node_weights < (mean_ + std_ * factor))] = 0\n",
    "    return node_weights\n",
    "\n",
    "def prune_node_keep(node_weights, keep=4):\n",
    "    w = node_weights.cpu().detach().numpy()\n",
    "    throw_idx = np.argsort(abs(w))[:-keep]\n",
    "    node_weights[throw_idx] = 0\n",
    "    return node_weights\n",
    "\n",
    "def prune_tree(tree_, factor):\n",
    "    new_weights = tree_.inner_nodes.weight.clone()\n",
    "    for i in range(new_weights.shape[0]):\n",
    "        res = prune_node_keep(new_weights[i, :], factor)\n",
    "        new_weights[i, :] = res\n",
    "\n",
    "    with torch.no_grad():\n",
    "        tree_.inner_nodes.weight.copy_(new_weights)\n",
    "        \n",
    "def sparseness(x):\n",
    "    s = []\n",
    "    for i in range(x.shape[0]):\n",
    "        x_ = x[i, :]\n",
    "        sp = (len(x_) - torch.norm(x_, 0).item()) / len(x_)\n",
    "        s.append(sp)\n",
    "    return np.mean(s)\n",
    "\n",
    "def compute_regularization_by_level(tree):\n",
    "    total_reg = 0\n",
    "    for i in range(tree.inner_nodes.weight.shape[0]):\n",
    "        cur_level = np.floor(np.log2(i+1))\n",
    "        node_reg = torch.norm(tree.inner_nodes.weight[i].view(-1), 2)\n",
    "        total_reg += 2**(-cur_level) * node_reg\n",
    "    return total_reg\n",
    "\n",
    "def show_sparseness(tree):\n",
    "    avg_sp = sparseness(tree.inner_nodes.weight)\n",
    "    print(f\"Average sparseness: {avg_sp}\")\n",
    "    layer = 0\n",
    "    sps = []\n",
    "    for i in range(tree.inner_nodes.weight.shape[0]):\n",
    "        cur_layer = int(np.floor(np.log2(i+1)))\n",
    "        if cur_layer != layer:\n",
    "            print(f\"layer {layer}: {np.mean(sps)}\")\n",
    "            sps = []\n",
    "            layer = cur_layer\n",
    "\n",
    "        x_ = tree.inner_nodes.weight[i, :]\n",
    "        sp = (len(x_) - torch.norm(x_, 0).item()) / len(x_)\n",
    "        sps.append(sp)\n",
    "        \n",
    "    return avg_sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_epoch(model, loader, device, log_interval, losses, accs, epoch, iteration):\n",
    "    model = model.train()\n",
    "    for batch_idx, (data, target) in enumerate(loader):\n",
    "        iteration += 1\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        output, penalty = tree.forward(data)\n",
    "\n",
    "        # Loss\n",
    "        loss_tree = criterion(output, target.view(-1))\n",
    "\n",
    "        # Penalty\n",
    "        loss_tree += penalty\n",
    "\n",
    "        # Sparse regularization\n",
    "#         fc_params = torch.cat([x.view(-1) for x in tree.inner_nodes.parameters()])\n",
    "#         regularization = sparsity_lamda * torch.norm(fc_params, 2)\n",
    "        regularization = sparsity_lamda * compute_regularization_by_level(tree)\n",
    "        loss = loss_tree\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        pred = output.data.max(1)[1]\n",
    "        correct = pred.eq(target.view(-1).data).sum()\n",
    "        accs.append(correct.item() / data.size()[0])\n",
    "\n",
    "        # Print training status\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(f\"Epoch: {epoch:02d} | Batch: {batch_idx:03d} / {len(loader):03d} | Total loss: {loss.item():.3f} | Reg loss: {regularization.item():.3f} | Tree loss: {loss_tree.item():.3f} | Accuracy: {correct.item() / data.size()[0]:03f} | {round((time.time() - start_time) / iteration, 3)} sec/iter\")\n",
    "            \n",
    "    return iteration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 5e-3\n",
    "weight_decay = 5e-4\n",
    "sparsity_lamda = 2e-3\n",
    "epochs = 100\n",
    "output_dim = len(set(clusters))\n",
    "log_interval = 1\n",
    "use_cuda = device != 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = SDT(input_dim=tensor_dataset.shape[1], output_dim=len(clusters - 1), depth=tree_depth, lamda=1e-3, use_cuda=use_cuda)\n",
    "optimizer = torch.optim.Adam(tree.parameters(),\n",
    "                                 lr=lr,\n",
    "                                 weight_decay=weight_decay)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "tree = tree.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "accs = []\n",
    "sparsity = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.0\n",
      "layer 0: 0.0\n",
      "layer 1: 0.0\n",
      "layer 2: 0.0\n",
      "layer 3: 0.0\n",
      "layer 4: 0.0\n",
      "Epoch: 00 | Batch: 000 / 025 | Total loss: 9.612 | Reg loss: 0.007 | Tree loss: 9.612 | Accuracy: 0.000000 | 0.095 sec/iter\n",
      "Epoch: 00 | Batch: 001 / 025 | Total loss: 9.592 | Reg loss: 0.007 | Tree loss: 9.592 | Accuracy: 0.000000 | 0.082 sec/iter\n",
      "Epoch: 00 | Batch: 002 / 025 | Total loss: 9.572 | Reg loss: 0.007 | Tree loss: 9.572 | Accuracy: 0.000000 | 0.08 sec/iter\n",
      "Epoch: 00 | Batch: 003 / 025 | Total loss: 9.553 | Reg loss: 0.007 | Tree loss: 9.553 | Accuracy: 0.000000 | 0.076 sec/iter\n",
      "Epoch: 00 | Batch: 004 / 025 | Total loss: 9.531 | Reg loss: 0.007 | Tree loss: 9.531 | Accuracy: 0.000000 | 0.073 sec/iter\n",
      "Epoch: 00 | Batch: 005 / 025 | Total loss: 9.512 | Reg loss: 0.007 | Tree loss: 9.512 | Accuracy: 0.000000 | 0.071 sec/iter\n",
      "Epoch: 00 | Batch: 006 / 025 | Total loss: 9.492 | Reg loss: 0.007 | Tree loss: 9.492 | Accuracy: 0.000000 | 0.069 sec/iter\n",
      "Epoch: 00 | Batch: 007 / 025 | Total loss: 9.472 | Reg loss: 0.007 | Tree loss: 9.472 | Accuracy: 0.000000 | 0.068 sec/iter\n",
      "Epoch: 00 | Batch: 008 / 025 | Total loss: 9.453 | Reg loss: 0.007 | Tree loss: 9.453 | Accuracy: 0.000000 | 0.068 sec/iter\n",
      "Epoch: 00 | Batch: 009 / 025 | Total loss: 9.433 | Reg loss: 0.008 | Tree loss: 9.433 | Accuracy: 0.000000 | 0.067 sec/iter\n",
      "Epoch: 00 | Batch: 010 / 025 | Total loss: 9.415 | Reg loss: 0.008 | Tree loss: 9.415 | Accuracy: 0.000000 | 0.068 sec/iter\n",
      "Epoch: 00 | Batch: 011 / 025 | Total loss: 9.394 | Reg loss: 0.008 | Tree loss: 9.394 | Accuracy: 0.000000 | 0.068 sec/iter\n",
      "Epoch: 00 | Batch: 012 / 025 | Total loss: 9.375 | Reg loss: 0.009 | Tree loss: 9.375 | Accuracy: 0.001953 | 0.067 sec/iter\n",
      "Epoch: 00 | Batch: 013 / 025 | Total loss: 9.359 | Reg loss: 0.009 | Tree loss: 9.359 | Accuracy: 0.001953 | 0.066 sec/iter\n",
      "Epoch: 00 | Batch: 014 / 025 | Total loss: 9.337 | Reg loss: 0.009 | Tree loss: 9.337 | Accuracy: 0.007812 | 0.066 sec/iter\n",
      "Epoch: 00 | Batch: 015 / 025 | Total loss: 9.322 | Reg loss: 0.010 | Tree loss: 9.322 | Accuracy: 0.015625 | 0.066 sec/iter\n",
      "Epoch: 00 | Batch: 016 / 025 | Total loss: 9.298 | Reg loss: 0.010 | Tree loss: 9.298 | Accuracy: 0.023438 | 0.065 sec/iter\n",
      "Epoch: 00 | Batch: 017 / 025 | Total loss: 9.280 | Reg loss: 0.010 | Tree loss: 9.280 | Accuracy: 0.035156 | 0.065 sec/iter\n",
      "Epoch: 00 | Batch: 018 / 025 | Total loss: 9.258 | Reg loss: 0.011 | Tree loss: 9.258 | Accuracy: 0.037109 | 0.065 sec/iter\n",
      "Epoch: 00 | Batch: 019 / 025 | Total loss: 9.244 | Reg loss: 0.011 | Tree loss: 9.244 | Accuracy: 0.054688 | 0.065 sec/iter\n",
      "Epoch: 00 | Batch: 020 / 025 | Total loss: 9.230 | Reg loss: 0.012 | Tree loss: 9.230 | Accuracy: 0.072266 | 0.065 sec/iter\n",
      "Epoch: 00 | Batch: 021 / 025 | Total loss: 9.209 | Reg loss: 0.012 | Tree loss: 9.209 | Accuracy: 0.109375 | 0.065 sec/iter\n",
      "Epoch: 00 | Batch: 022 / 025 | Total loss: 9.193 | Reg loss: 0.012 | Tree loss: 9.193 | Accuracy: 0.119141 | 0.065 sec/iter\n",
      "Epoch: 00 | Batch: 023 / 025 | Total loss: 9.173 | Reg loss: 0.013 | Tree loss: 9.173 | Accuracy: 0.212891 | 0.065 sec/iter\n",
      "Epoch: 00 | Batch: 024 / 025 | Total loss: 9.155 | Reg loss: 0.013 | Tree loss: 9.155 | Accuracy: 0.250000 | 0.065 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 01 | Batch: 000 / 025 | Total loss: 9.451 | Reg loss: 0.004 | Tree loss: 9.451 | Accuracy: 0.000000 | 0.067 sec/iter\n",
      "Epoch: 01 | Batch: 001 / 025 | Total loss: 9.434 | Reg loss: 0.004 | Tree loss: 9.434 | Accuracy: 0.000000 | 0.067 sec/iter\n",
      "Epoch: 01 | Batch: 002 / 025 | Total loss: 9.415 | Reg loss: 0.004 | Tree loss: 9.415 | Accuracy: 0.000000 | 0.067 sec/iter\n",
      "Epoch: 01 | Batch: 003 / 025 | Total loss: 9.394 | Reg loss: 0.004 | Tree loss: 9.394 | Accuracy: 0.000000 | 0.067 sec/iter\n",
      "Epoch: 01 | Batch: 004 / 025 | Total loss: 9.377 | Reg loss: 0.005 | Tree loss: 9.377 | Accuracy: 0.000000 | 0.067 sec/iter\n",
      "Epoch: 01 | Batch: 005 / 025 | Total loss: 9.353 | Reg loss: 0.005 | Tree loss: 9.353 | Accuracy: 0.000000 | 0.067 sec/iter\n",
      "Epoch: 01 | Batch: 006 / 025 | Total loss: 9.338 | Reg loss: 0.005 | Tree loss: 9.338 | Accuracy: 0.001953 | 0.066 sec/iter\n",
      "Epoch: 01 | Batch: 007 / 025 | Total loss: 9.318 | Reg loss: 0.006 | Tree loss: 9.318 | Accuracy: 0.013672 | 0.066 sec/iter\n",
      "Epoch: 01 | Batch: 008 / 025 | Total loss: 9.300 | Reg loss: 0.006 | Tree loss: 9.300 | Accuracy: 0.017578 | 0.066 sec/iter\n",
      "Epoch: 01 | Batch: 009 / 025 | Total loss: 9.279 | Reg loss: 0.006 | Tree loss: 9.279 | Accuracy: 0.039062 | 0.066 sec/iter\n",
      "Epoch: 01 | Batch: 010 / 025 | Total loss: 9.253 | Reg loss: 0.007 | Tree loss: 9.253 | Accuracy: 0.095703 | 0.066 sec/iter\n",
      "Epoch: 01 | Batch: 011 / 025 | Total loss: 9.245 | Reg loss: 0.007 | Tree loss: 9.245 | Accuracy: 0.099609 | 0.066 sec/iter\n",
      "Epoch: 01 | Batch: 012 / 025 | Total loss: 9.223 | Reg loss: 0.008 | Tree loss: 9.223 | Accuracy: 0.173828 | 0.066 sec/iter\n",
      "Epoch: 01 | Batch: 013 / 025 | Total loss: 9.210 | Reg loss: 0.008 | Tree loss: 9.210 | Accuracy: 0.214844 | 0.066 sec/iter\n",
      "Epoch: 01 | Batch: 014 / 025 | Total loss: 9.185 | Reg loss: 0.009 | Tree loss: 9.185 | Accuracy: 0.341797 | 0.066 sec/iter\n",
      "Epoch: 01 | Batch: 015 / 025 | Total loss: 9.164 | Reg loss: 0.009 | Tree loss: 9.164 | Accuracy: 0.619141 | 0.066 sec/iter\n",
      "Epoch: 01 | Batch: 016 / 025 | Total loss: 9.144 | Reg loss: 0.009 | Tree loss: 9.144 | Accuracy: 0.908203 | 0.066 sec/iter\n",
      "Epoch: 01 | Batch: 017 / 025 | Total loss: 9.129 | Reg loss: 0.010 | Tree loss: 9.129 | Accuracy: 0.962891 | 0.066 sec/iter\n",
      "Epoch: 01 | Batch: 018 / 025 | Total loss: 9.111 | Reg loss: 0.010 | Tree loss: 9.111 | Accuracy: 0.982422 | 0.066 sec/iter\n",
      "Epoch: 01 | Batch: 019 / 025 | Total loss: 9.083 | Reg loss: 0.011 | Tree loss: 9.083 | Accuracy: 0.996094 | 0.066 sec/iter\n",
      "Epoch: 01 | Batch: 020 / 025 | Total loss: 9.075 | Reg loss: 0.011 | Tree loss: 9.075 | Accuracy: 1.000000 | 0.066 sec/iter\n",
      "Epoch: 01 | Batch: 021 / 025 | Total loss: 9.055 | Reg loss: 0.012 | Tree loss: 9.055 | Accuracy: 1.000000 | 0.066 sec/iter\n",
      "Epoch: 01 | Batch: 022 / 025 | Total loss: 9.038 | Reg loss: 0.012 | Tree loss: 9.038 | Accuracy: 1.000000 | 0.066 sec/iter\n",
      "Epoch: 01 | Batch: 023 / 025 | Total loss: 9.020 | Reg loss: 0.012 | Tree loss: 9.020 | Accuracy: 1.000000 | 0.066 sec/iter\n",
      "Epoch: 01 | Batch: 024 / 025 | Total loss: 8.996 | Reg loss: 0.013 | Tree loss: 8.996 | Accuracy: 1.000000 | 0.066 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 02 | Batch: 000 / 025 | Total loss: 9.309 | Reg loss: 0.006 | Tree loss: 9.309 | Accuracy: 0.017578 | 0.066 sec/iter\n",
      "Epoch: 02 | Batch: 001 / 025 | Total loss: 9.291 | Reg loss: 0.006 | Tree loss: 9.291 | Accuracy: 0.031250 | 0.066 sec/iter\n",
      "Epoch: 02 | Batch: 002 / 025 | Total loss: 9.274 | Reg loss: 0.006 | Tree loss: 9.274 | Accuracy: 0.074219 | 0.066 sec/iter\n",
      "Epoch: 02 | Batch: 003 / 025 | Total loss: 9.253 | Reg loss: 0.006 | Tree loss: 9.253 | Accuracy: 0.195312 | 0.067 sec/iter\n",
      "Epoch: 02 | Batch: 004 / 025 | Total loss: 9.230 | Reg loss: 0.006 | Tree loss: 9.230 | Accuracy: 0.437500 | 0.066 sec/iter\n",
      "Epoch: 02 | Batch: 005 / 025 | Total loss: 9.212 | Reg loss: 0.006 | Tree loss: 9.212 | Accuracy: 0.693359 | 0.066 sec/iter\n",
      "Epoch: 02 | Batch: 006 / 025 | Total loss: 9.193 | Reg loss: 0.007 | Tree loss: 9.193 | Accuracy: 0.937500 | 0.066 sec/iter\n",
      "Epoch: 02 | Batch: 007 / 025 | Total loss: 9.180 | Reg loss: 0.007 | Tree loss: 9.180 | Accuracy: 0.992188 | 0.066 sec/iter\n",
      "Epoch: 02 | Batch: 008 / 025 | Total loss: 9.152 | Reg loss: 0.007 | Tree loss: 9.152 | Accuracy: 1.000000 | 0.066 sec/iter\n",
      "Epoch: 02 | Batch: 009 / 025 | Total loss: 9.134 | Reg loss: 0.008 | Tree loss: 9.134 | Accuracy: 1.000000 | 0.066 sec/iter\n",
      "Epoch: 02 | Batch: 010 / 025 | Total loss: 9.119 | Reg loss: 0.008 | Tree loss: 9.119 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 02 | Batch: 011 / 025 | Total loss: 9.100 | Reg loss: 0.009 | Tree loss: 9.100 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 02 | Batch: 012 / 025 | Total loss: 9.083 | Reg loss: 0.009 | Tree loss: 9.083 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 02 | Batch: 013 / 025 | Total loss: 9.064 | Reg loss: 0.009 | Tree loss: 9.064 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 02 | Batch: 014 / 025 | Total loss: 9.047 | Reg loss: 0.010 | Tree loss: 9.047 | Accuracy: 1.000000 | 0.066 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02 | Batch: 015 / 025 | Total loss: 9.022 | Reg loss: 0.010 | Tree loss: 9.022 | Accuracy: 1.000000 | 0.066 sec/iter\n",
      "Epoch: 02 | Batch: 016 / 025 | Total loss: 9.010 | Reg loss: 0.011 | Tree loss: 9.010 | Accuracy: 1.000000 | 0.066 sec/iter\n",
      "Epoch: 02 | Batch: 017 / 025 | Total loss: 8.991 | Reg loss: 0.011 | Tree loss: 8.991 | Accuracy: 1.000000 | 0.066 sec/iter\n",
      "Epoch: 02 | Batch: 018 / 025 | Total loss: 8.972 | Reg loss: 0.011 | Tree loss: 8.972 | Accuracy: 1.000000 | 0.066 sec/iter\n",
      "Epoch: 02 | Batch: 019 / 025 | Total loss: 8.947 | Reg loss: 0.012 | Tree loss: 8.947 | Accuracy: 1.000000 | 0.066 sec/iter\n",
      "Epoch: 02 | Batch: 020 / 025 | Total loss: 8.933 | Reg loss: 0.012 | Tree loss: 8.933 | Accuracy: 1.000000 | 0.066 sec/iter\n",
      "Epoch: 02 | Batch: 021 / 025 | Total loss: 8.911 | Reg loss: 0.013 | Tree loss: 8.911 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 02 | Batch: 022 / 025 | Total loss: 8.882 | Reg loss: 0.013 | Tree loss: 8.882 | Accuracy: 1.000000 | 0.066 sec/iter\n",
      "Epoch: 02 | Batch: 023 / 025 | Total loss: 8.875 | Reg loss: 0.014 | Tree loss: 8.875 | Accuracy: 1.000000 | 0.066 sec/iter\n",
      "Epoch: 02 | Batch: 024 / 025 | Total loss: 8.854 | Reg loss: 0.014 | Tree loss: 8.854 | Accuracy: 1.000000 | 0.066 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 03 | Batch: 000 / 025 | Total loss: 9.169 | Reg loss: 0.008 | Tree loss: 9.169 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 03 | Batch: 001 / 025 | Total loss: 9.150 | Reg loss: 0.008 | Tree loss: 9.150 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 03 | Batch: 002 / 025 | Total loss: 9.134 | Reg loss: 0.008 | Tree loss: 9.134 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 03 | Batch: 003 / 025 | Total loss: 9.112 | Reg loss: 0.008 | Tree loss: 9.112 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 03 | Batch: 004 / 025 | Total loss: 9.093 | Reg loss: 0.008 | Tree loss: 9.093 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 03 | Batch: 005 / 025 | Total loss: 9.075 | Reg loss: 0.009 | Tree loss: 9.075 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 03 | Batch: 006 / 025 | Total loss: 9.054 | Reg loss: 0.009 | Tree loss: 9.054 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 03 | Batch: 007 / 025 | Total loss: 9.040 | Reg loss: 0.009 | Tree loss: 9.040 | Accuracy: 1.000000 | 0.066 sec/iter\n",
      "Epoch: 03 | Batch: 008 / 025 | Total loss: 9.016 | Reg loss: 0.009 | Tree loss: 9.016 | Accuracy: 1.000000 | 0.066 sec/iter\n",
      "Epoch: 03 | Batch: 009 / 025 | Total loss: 8.997 | Reg loss: 0.010 | Tree loss: 8.997 | Accuracy: 1.000000 | 0.066 sec/iter\n",
      "Epoch: 03 | Batch: 010 / 025 | Total loss: 8.982 | Reg loss: 0.010 | Tree loss: 8.982 | Accuracy: 1.000000 | 0.066 sec/iter\n",
      "Epoch: 03 | Batch: 011 / 025 | Total loss: 8.952 | Reg loss: 0.010 | Tree loss: 8.952 | Accuracy: 1.000000 | 0.066 sec/iter\n",
      "Epoch: 03 | Batch: 012 / 025 | Total loss: 8.937 | Reg loss: 0.011 | Tree loss: 8.937 | Accuracy: 1.000000 | 0.066 sec/iter\n",
      "Epoch: 03 | Batch: 013 / 025 | Total loss: 8.914 | Reg loss: 0.011 | Tree loss: 8.914 | Accuracy: 1.000000 | 0.066 sec/iter\n",
      "Epoch: 03 | Batch: 014 / 025 | Total loss: 8.900 | Reg loss: 0.011 | Tree loss: 8.900 | Accuracy: 1.000000 | 0.066 sec/iter\n",
      "Epoch: 03 | Batch: 015 / 025 | Total loss: 8.886 | Reg loss: 0.012 | Tree loss: 8.886 | Accuracy: 1.000000 | 0.066 sec/iter\n",
      "Epoch: 03 | Batch: 016 / 025 | Total loss: 8.867 | Reg loss: 0.012 | Tree loss: 8.867 | Accuracy: 1.000000 | 0.066 sec/iter\n",
      "Epoch: 03 | Batch: 017 / 025 | Total loss: 8.843 | Reg loss: 0.013 | Tree loss: 8.843 | Accuracy: 1.000000 | 0.066 sec/iter\n",
      "Epoch: 03 | Batch: 018 / 025 | Total loss: 8.825 | Reg loss: 0.013 | Tree loss: 8.825 | Accuracy: 1.000000 | 0.066 sec/iter\n",
      "Epoch: 03 | Batch: 019 / 025 | Total loss: 8.810 | Reg loss: 0.014 | Tree loss: 8.810 | Accuracy: 1.000000 | 0.066 sec/iter\n",
      "Epoch: 03 | Batch: 020 / 025 | Total loss: 8.790 | Reg loss: 0.014 | Tree loss: 8.790 | Accuracy: 1.000000 | 0.066 sec/iter\n",
      "Epoch: 03 | Batch: 021 / 025 | Total loss: 8.768 | Reg loss: 0.014 | Tree loss: 8.768 | Accuracy: 1.000000 | 0.066 sec/iter\n",
      "Epoch: 03 | Batch: 022 / 025 | Total loss: 8.745 | Reg loss: 0.015 | Tree loss: 8.745 | Accuracy: 1.000000 | 0.066 sec/iter\n",
      "Epoch: 03 | Batch: 023 / 025 | Total loss: 8.735 | Reg loss: 0.015 | Tree loss: 8.735 | Accuracy: 1.000000 | 0.066 sec/iter\n",
      "Epoch: 03 | Batch: 024 / 025 | Total loss: 8.707 | Reg loss: 0.016 | Tree loss: 8.707 | Accuracy: 1.000000 | 0.066 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 04 | Batch: 000 / 025 | Total loss: 9.030 | Reg loss: 0.010 | Tree loss: 9.030 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 04 | Batch: 001 / 025 | Total loss: 9.008 | Reg loss: 0.010 | Tree loss: 9.008 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 04 | Batch: 002 / 025 | Total loss: 8.990 | Reg loss: 0.010 | Tree loss: 8.990 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 04 | Batch: 003 / 025 | Total loss: 8.971 | Reg loss: 0.010 | Tree loss: 8.971 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 04 | Batch: 004 / 025 | Total loss: 8.951 | Reg loss: 0.010 | Tree loss: 8.951 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 04 | Batch: 005 / 025 | Total loss: 8.933 | Reg loss: 0.010 | Tree loss: 8.933 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 04 | Batch: 006 / 025 | Total loss: 8.918 | Reg loss: 0.011 | Tree loss: 8.918 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 04 | Batch: 007 / 025 | Total loss: 8.896 | Reg loss: 0.011 | Tree loss: 8.896 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 04 | Batch: 008 / 025 | Total loss: 8.881 | Reg loss: 0.011 | Tree loss: 8.881 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 04 | Batch: 009 / 025 | Total loss: 8.857 | Reg loss: 0.012 | Tree loss: 8.857 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 04 | Batch: 010 / 025 | Total loss: 8.838 | Reg loss: 0.012 | Tree loss: 8.838 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 04 | Batch: 011 / 025 | Total loss: 8.819 | Reg loss: 0.012 | Tree loss: 8.819 | Accuracy: 1.000000 | 0.066 sec/iter\n",
      "Epoch: 04 | Batch: 012 / 025 | Total loss: 8.800 | Reg loss: 0.013 | Tree loss: 8.800 | Accuracy: 1.000000 | 0.066 sec/iter\n",
      "Epoch: 04 | Batch: 013 / 025 | Total loss: 8.777 | Reg loss: 0.013 | Tree loss: 8.777 | Accuracy: 1.000000 | 0.066 sec/iter\n",
      "Epoch: 04 | Batch: 014 / 025 | Total loss: 8.760 | Reg loss: 0.013 | Tree loss: 8.760 | Accuracy: 1.000000 | 0.066 sec/iter\n",
      "Epoch: 04 | Batch: 015 / 025 | Total loss: 8.747 | Reg loss: 0.014 | Tree loss: 8.747 | Accuracy: 1.000000 | 0.066 sec/iter\n",
      "Epoch: 04 | Batch: 016 / 025 | Total loss: 8.722 | Reg loss: 0.014 | Tree loss: 8.722 | Accuracy: 1.000000 | 0.066 sec/iter\n",
      "Epoch: 04 | Batch: 017 / 025 | Total loss: 8.700 | Reg loss: 0.014 | Tree loss: 8.700 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 04 | Batch: 018 / 025 | Total loss: 8.690 | Reg loss: 0.015 | Tree loss: 8.690 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 04 | Batch: 019 / 025 | Total loss: 8.657 | Reg loss: 0.015 | Tree loss: 8.657 | Accuracy: 1.000000 | 0.066 sec/iter\n",
      "Epoch: 04 | Batch: 020 / 025 | Total loss: 8.646 | Reg loss: 0.016 | Tree loss: 8.646 | Accuracy: 1.000000 | 0.066 sec/iter\n",
      "Epoch: 04 | Batch: 021 / 025 | Total loss: 8.625 | Reg loss: 0.016 | Tree loss: 8.625 | Accuracy: 1.000000 | 0.066 sec/iter\n",
      "Epoch: 04 | Batch: 022 / 025 | Total loss: 8.600 | Reg loss: 0.016 | Tree loss: 8.600 | Accuracy: 1.000000 | 0.066 sec/iter\n",
      "Epoch: 04 | Batch: 023 / 025 | Total loss: 8.591 | Reg loss: 0.017 | Tree loss: 8.591 | Accuracy: 1.000000 | 0.066 sec/iter\n",
      "Epoch: 04 | Batch: 024 / 025 | Total loss: 8.572 | Reg loss: 0.017 | Tree loss: 8.572 | Accuracy: 1.000000 | 0.066 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 05 | Batch: 000 / 025 | Total loss: 8.890 | Reg loss: 0.012 | Tree loss: 8.890 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 05 | Batch: 001 / 025 | Total loss: 8.867 | Reg loss: 0.012 | Tree loss: 8.867 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 05 | Batch: 002 / 025 | Total loss: 8.854 | Reg loss: 0.012 | Tree loss: 8.854 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 05 | Batch: 003 / 025 | Total loss: 8.834 | Reg loss: 0.012 | Tree loss: 8.834 | Accuracy: 1.000000 | 0.067 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 05 | Batch: 004 / 025 | Total loss: 8.814 | Reg loss: 0.012 | Tree loss: 8.814 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 05 | Batch: 005 / 025 | Total loss: 8.799 | Reg loss: 0.012 | Tree loss: 8.799 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 05 | Batch: 006 / 025 | Total loss: 8.774 | Reg loss: 0.013 | Tree loss: 8.774 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 05 | Batch: 007 / 025 | Total loss: 8.757 | Reg loss: 0.013 | Tree loss: 8.757 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 05 | Batch: 008 / 025 | Total loss: 8.737 | Reg loss: 0.013 | Tree loss: 8.737 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 05 | Batch: 009 / 025 | Total loss: 8.723 | Reg loss: 0.013 | Tree loss: 8.723 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 05 | Batch: 010 / 025 | Total loss: 8.691 | Reg loss: 0.014 | Tree loss: 8.691 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 05 | Batch: 011 / 025 | Total loss: 8.679 | Reg loss: 0.014 | Tree loss: 8.679 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 05 | Batch: 012 / 025 | Total loss: 8.660 | Reg loss: 0.014 | Tree loss: 8.660 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 05 | Batch: 013 / 025 | Total loss: 8.640 | Reg loss: 0.015 | Tree loss: 8.640 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 05 | Batch: 014 / 025 | Total loss: 8.619 | Reg loss: 0.015 | Tree loss: 8.619 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 05 | Batch: 015 / 025 | Total loss: 8.597 | Reg loss: 0.015 | Tree loss: 8.597 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 05 | Batch: 016 / 025 | Total loss: 8.580 | Reg loss: 0.016 | Tree loss: 8.580 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 05 | Batch: 017 / 025 | Total loss: 8.560 | Reg loss: 0.016 | Tree loss: 8.560 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 05 | Batch: 018 / 025 | Total loss: 8.546 | Reg loss: 0.017 | Tree loss: 8.546 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 05 | Batch: 019 / 025 | Total loss: 8.524 | Reg loss: 0.017 | Tree loss: 8.524 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 05 | Batch: 020 / 025 | Total loss: 8.502 | Reg loss: 0.017 | Tree loss: 8.502 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 05 | Batch: 021 / 025 | Total loss: 8.487 | Reg loss: 0.018 | Tree loss: 8.487 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 05 | Batch: 022 / 025 | Total loss: 8.453 | Reg loss: 0.018 | Tree loss: 8.453 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 05 | Batch: 023 / 025 | Total loss: 8.443 | Reg loss: 0.019 | Tree loss: 8.443 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 05 | Batch: 024 / 025 | Total loss: 8.423 | Reg loss: 0.019 | Tree loss: 8.423 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 06 | Batch: 000 / 025 | Total loss: 8.752 | Reg loss: 0.013 | Tree loss: 8.752 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 06 | Batch: 001 / 025 | Total loss: 8.731 | Reg loss: 0.013 | Tree loss: 8.731 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 06 | Batch: 002 / 025 | Total loss: 8.712 | Reg loss: 0.014 | Tree loss: 8.712 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 06 | Batch: 003 / 025 | Total loss: 8.701 | Reg loss: 0.014 | Tree loss: 8.701 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 06 | Batch: 004 / 025 | Total loss: 8.674 | Reg loss: 0.014 | Tree loss: 8.674 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 06 | Batch: 005 / 025 | Total loss: 8.656 | Reg loss: 0.014 | Tree loss: 8.656 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 06 | Batch: 006 / 025 | Total loss: 8.637 | Reg loss: 0.014 | Tree loss: 8.637 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 06 | Batch: 007 / 025 | Total loss: 8.620 | Reg loss: 0.014 | Tree loss: 8.620 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 06 | Batch: 008 / 025 | Total loss: 8.592 | Reg loss: 0.015 | Tree loss: 8.592 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 06 | Batch: 009 / 025 | Total loss: 8.571 | Reg loss: 0.015 | Tree loss: 8.571 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 06 | Batch: 010 / 025 | Total loss: 8.562 | Reg loss: 0.015 | Tree loss: 8.562 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 06 | Batch: 011 / 025 | Total loss: 8.540 | Reg loss: 0.016 | Tree loss: 8.540 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 06 | Batch: 012 / 025 | Total loss: 8.523 | Reg loss: 0.016 | Tree loss: 8.523 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 06 | Batch: 013 / 025 | Total loss: 8.499 | Reg loss: 0.016 | Tree loss: 8.499 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 06 | Batch: 014 / 025 | Total loss: 8.487 | Reg loss: 0.017 | Tree loss: 8.487 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 06 | Batch: 015 / 025 | Total loss: 8.457 | Reg loss: 0.017 | Tree loss: 8.457 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 06 | Batch: 016 / 025 | Total loss: 8.436 | Reg loss: 0.017 | Tree loss: 8.436 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 06 | Batch: 017 / 025 | Total loss: 8.413 | Reg loss: 0.018 | Tree loss: 8.413 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 06 | Batch: 018 / 025 | Total loss: 8.385 | Reg loss: 0.018 | Tree loss: 8.385 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 06 | Batch: 019 / 025 | Total loss: 8.382 | Reg loss: 0.019 | Tree loss: 8.382 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 06 | Batch: 020 / 025 | Total loss: 8.359 | Reg loss: 0.019 | Tree loss: 8.359 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 06 | Batch: 021 / 025 | Total loss: 8.340 | Reg loss: 0.019 | Tree loss: 8.340 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 06 | Batch: 022 / 025 | Total loss: 8.323 | Reg loss: 0.020 | Tree loss: 8.323 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 06 | Batch: 023 / 025 | Total loss: 8.303 | Reg loss: 0.020 | Tree loss: 8.303 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 06 | Batch: 024 / 025 | Total loss: 8.283 | Reg loss: 0.021 | Tree loss: 8.283 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 07 | Batch: 000 / 025 | Total loss: 8.618 | Reg loss: 0.015 | Tree loss: 8.618 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 07 | Batch: 001 / 025 | Total loss: 8.597 | Reg loss: 0.015 | Tree loss: 8.597 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 07 | Batch: 002 / 025 | Total loss: 8.575 | Reg loss: 0.015 | Tree loss: 8.575 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 07 | Batch: 003 / 025 | Total loss: 8.561 | Reg loss: 0.015 | Tree loss: 8.561 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 07 | Batch: 004 / 025 | Total loss: 8.539 | Reg loss: 0.016 | Tree loss: 8.539 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 07 | Batch: 005 / 025 | Total loss: 8.520 | Reg loss: 0.016 | Tree loss: 8.520 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 07 | Batch: 006 / 025 | Total loss: 8.501 | Reg loss: 0.016 | Tree loss: 8.501 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 07 | Batch: 007 / 025 | Total loss: 8.474 | Reg loss: 0.016 | Tree loss: 8.474 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 07 | Batch: 008 / 025 | Total loss: 8.457 | Reg loss: 0.016 | Tree loss: 8.457 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 07 | Batch: 009 / 025 | Total loss: 8.435 | Reg loss: 0.017 | Tree loss: 8.435 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 07 | Batch: 010 / 025 | Total loss: 8.413 | Reg loss: 0.017 | Tree loss: 8.413 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 07 | Batch: 011 / 025 | Total loss: 8.409 | Reg loss: 0.017 | Tree loss: 8.409 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 07 | Batch: 012 / 025 | Total loss: 8.378 | Reg loss: 0.018 | Tree loss: 8.378 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 07 | Batch: 013 / 025 | Total loss: 8.355 | Reg loss: 0.018 | Tree loss: 8.355 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 07 | Batch: 014 / 025 | Total loss: 8.336 | Reg loss: 0.018 | Tree loss: 8.336 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 07 | Batch: 015 / 025 | Total loss: 8.318 | Reg loss: 0.019 | Tree loss: 8.318 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 07 | Batch: 016 / 025 | Total loss: 8.302 | Reg loss: 0.019 | Tree loss: 8.302 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 07 | Batch: 017 / 025 | Total loss: 8.276 | Reg loss: 0.019 | Tree loss: 8.276 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 07 | Batch: 018 / 025 | Total loss: 8.257 | Reg loss: 0.020 | Tree loss: 8.257 | Accuracy: 1.000000 | 0.067 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 07 | Batch: 019 / 025 | Total loss: 8.229 | Reg loss: 0.020 | Tree loss: 8.229 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 07 | Batch: 020 / 025 | Total loss: 8.218 | Reg loss: 0.021 | Tree loss: 8.218 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 07 | Batch: 021 / 025 | Total loss: 8.197 | Reg loss: 0.021 | Tree loss: 8.197 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 07 | Batch: 022 / 025 | Total loss: 8.172 | Reg loss: 0.021 | Tree loss: 8.172 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 07 | Batch: 023 / 025 | Total loss: 8.163 | Reg loss: 0.022 | Tree loss: 8.163 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 07 | Batch: 024 / 025 | Total loss: 8.136 | Reg loss: 0.022 | Tree loss: 8.136 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 08 | Batch: 000 / 025 | Total loss: 8.483 | Reg loss: 0.017 | Tree loss: 8.483 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 08 | Batch: 001 / 025 | Total loss: 8.461 | Reg loss: 0.017 | Tree loss: 8.461 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 08 | Batch: 002 / 025 | Total loss: 8.442 | Reg loss: 0.017 | Tree loss: 8.442 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 08 | Batch: 003 / 025 | Total loss: 8.425 | Reg loss: 0.017 | Tree loss: 8.425 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 08 | Batch: 004 / 025 | Total loss: 8.403 | Reg loss: 0.017 | Tree loss: 8.403 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 08 | Batch: 005 / 025 | Total loss: 8.376 | Reg loss: 0.017 | Tree loss: 8.376 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 08 | Batch: 006 / 025 | Total loss: 8.362 | Reg loss: 0.017 | Tree loss: 8.362 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 08 | Batch: 007 / 025 | Total loss: 8.351 | Reg loss: 0.018 | Tree loss: 8.351 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 08 | Batch: 008 / 025 | Total loss: 8.323 | Reg loss: 0.018 | Tree loss: 8.323 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 08 | Batch: 009 / 025 | Total loss: 8.304 | Reg loss: 0.018 | Tree loss: 8.304 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 08 | Batch: 010 / 025 | Total loss: 8.276 | Reg loss: 0.018 | Tree loss: 8.276 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 08 | Batch: 011 / 025 | Total loss: 8.254 | Reg loss: 0.019 | Tree loss: 8.254 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 08 | Batch: 012 / 025 | Total loss: 8.236 | Reg loss: 0.019 | Tree loss: 8.236 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 08 | Batch: 013 / 025 | Total loss: 8.218 | Reg loss: 0.019 | Tree loss: 8.218 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 08 | Batch: 014 / 025 | Total loss: 8.196 | Reg loss: 0.020 | Tree loss: 8.196 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 08 | Batch: 015 / 025 | Total loss: 8.173 | Reg loss: 0.020 | Tree loss: 8.173 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 08 | Batch: 016 / 025 | Total loss: 8.162 | Reg loss: 0.020 | Tree loss: 8.162 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 08 | Batch: 017 / 025 | Total loss: 8.134 | Reg loss: 0.021 | Tree loss: 8.134 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 08 | Batch: 018 / 025 | Total loss: 8.121 | Reg loss: 0.021 | Tree loss: 8.121 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 08 | Batch: 019 / 025 | Total loss: 8.096 | Reg loss: 0.021 | Tree loss: 8.096 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 08 | Batch: 020 / 025 | Total loss: 8.081 | Reg loss: 0.022 | Tree loss: 8.081 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 08 | Batch: 021 / 025 | Total loss: 8.053 | Reg loss: 0.022 | Tree loss: 8.053 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 08 | Batch: 022 / 025 | Total loss: 8.035 | Reg loss: 0.023 | Tree loss: 8.035 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 08 | Batch: 023 / 025 | Total loss: 8.014 | Reg loss: 0.023 | Tree loss: 8.014 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 08 | Batch: 024 / 025 | Total loss: 7.993 | Reg loss: 0.023 | Tree loss: 7.993 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 09 | Batch: 000 / 025 | Total loss: 8.343 | Reg loss: 0.018 | Tree loss: 8.343 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 09 | Batch: 001 / 025 | Total loss: 8.328 | Reg loss: 0.018 | Tree loss: 8.328 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 09 | Batch: 002 / 025 | Total loss: 8.308 | Reg loss: 0.018 | Tree loss: 8.308 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 09 | Batch: 003 / 025 | Total loss: 8.285 | Reg loss: 0.018 | Tree loss: 8.285 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 09 | Batch: 004 / 025 | Total loss: 8.266 | Reg loss: 0.018 | Tree loss: 8.266 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 09 | Batch: 005 / 025 | Total loss: 8.246 | Reg loss: 0.019 | Tree loss: 8.246 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 09 | Batch: 006 / 025 | Total loss: 8.228 | Reg loss: 0.019 | Tree loss: 8.228 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 09 | Batch: 007 / 025 | Total loss: 8.205 | Reg loss: 0.019 | Tree loss: 8.205 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 09 | Batch: 008 / 025 | Total loss: 8.175 | Reg loss: 0.019 | Tree loss: 8.175 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 09 | Batch: 009 / 025 | Total loss: 8.166 | Reg loss: 0.019 | Tree loss: 8.166 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 09 | Batch: 010 / 025 | Total loss: 8.134 | Reg loss: 0.020 | Tree loss: 8.134 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 09 | Batch: 011 / 025 | Total loss: 8.115 | Reg loss: 0.020 | Tree loss: 8.115 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 09 | Batch: 012 / 025 | Total loss: 8.107 | Reg loss: 0.020 | Tree loss: 8.107 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 09 | Batch: 013 / 025 | Total loss: 8.084 | Reg loss: 0.021 | Tree loss: 8.084 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 09 | Batch: 014 / 025 | Total loss: 8.063 | Reg loss: 0.021 | Tree loss: 8.063 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 09 | Batch: 015 / 025 | Total loss: 8.043 | Reg loss: 0.021 | Tree loss: 8.043 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 09 | Batch: 016 / 025 | Total loss: 8.020 | Reg loss: 0.022 | Tree loss: 8.020 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 09 | Batch: 017 / 025 | Total loss: 7.995 | Reg loss: 0.022 | Tree loss: 7.995 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 09 | Batch: 018 / 025 | Total loss: 7.981 | Reg loss: 0.022 | Tree loss: 7.981 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 09 | Batch: 019 / 025 | Total loss: 7.954 | Reg loss: 0.023 | Tree loss: 7.954 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 09 | Batch: 020 / 025 | Total loss: 7.935 | Reg loss: 0.023 | Tree loss: 7.935 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 09 | Batch: 021 / 025 | Total loss: 7.919 | Reg loss: 0.023 | Tree loss: 7.919 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 09 | Batch: 022 / 025 | Total loss: 7.894 | Reg loss: 0.024 | Tree loss: 7.894 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 09 | Batch: 023 / 025 | Total loss: 7.871 | Reg loss: 0.024 | Tree loss: 7.871 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 09 | Batch: 024 / 025 | Total loss: 7.867 | Reg loss: 0.025 | Tree loss: 7.867 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 10 | Batch: 000 / 025 | Total loss: 8.218 | Reg loss: 0.019 | Tree loss: 8.218 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 10 | Batch: 001 / 025 | Total loss: 8.185 | Reg loss: 0.019 | Tree loss: 8.185 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 10 | Batch: 002 / 025 | Total loss: 8.169 | Reg loss: 0.019 | Tree loss: 8.169 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 10 | Batch: 003 / 025 | Total loss: 8.146 | Reg loss: 0.020 | Tree loss: 8.146 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 10 | Batch: 004 / 025 | Total loss: 8.131 | Reg loss: 0.020 | Tree loss: 8.131 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 10 | Batch: 005 / 025 | Total loss: 8.113 | Reg loss: 0.020 | Tree loss: 8.113 | Accuracy: 1.000000 | 0.067 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 | Batch: 006 / 025 | Total loss: 8.088 | Reg loss: 0.020 | Tree loss: 8.088 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 10 | Batch: 007 / 025 | Total loss: 8.066 | Reg loss: 0.020 | Tree loss: 8.066 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 10 | Batch: 008 / 025 | Total loss: 8.040 | Reg loss: 0.020 | Tree loss: 8.040 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 10 | Batch: 009 / 025 | Total loss: 8.032 | Reg loss: 0.021 | Tree loss: 8.032 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 10 | Batch: 010 / 025 | Total loss: 7.996 | Reg loss: 0.021 | Tree loss: 7.996 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 10 | Batch: 011 / 025 | Total loss: 7.989 | Reg loss: 0.021 | Tree loss: 7.989 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 10 | Batch: 012 / 025 | Total loss: 7.953 | Reg loss: 0.021 | Tree loss: 7.953 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 10 | Batch: 013 / 025 | Total loss: 7.943 | Reg loss: 0.022 | Tree loss: 7.943 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 10 | Batch: 014 / 025 | Total loss: 7.929 | Reg loss: 0.022 | Tree loss: 7.929 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 10 | Batch: 015 / 025 | Total loss: 7.908 | Reg loss: 0.022 | Tree loss: 7.908 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 10 | Batch: 016 / 025 | Total loss: 7.880 | Reg loss: 0.023 | Tree loss: 7.880 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 10 | Batch: 017 / 025 | Total loss: 7.860 | Reg loss: 0.023 | Tree loss: 7.860 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 10 | Batch: 018 / 025 | Total loss: 7.844 | Reg loss: 0.023 | Tree loss: 7.844 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 10 | Batch: 019 / 025 | Total loss: 7.824 | Reg loss: 0.024 | Tree loss: 7.824 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 10 | Batch: 020 / 025 | Total loss: 7.805 | Reg loss: 0.024 | Tree loss: 7.805 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 10 | Batch: 021 / 025 | Total loss: 7.777 | Reg loss: 0.024 | Tree loss: 7.777 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 10 | Batch: 022 / 025 | Total loss: 7.764 | Reg loss: 0.025 | Tree loss: 7.764 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 10 | Batch: 023 / 025 | Total loss: 7.739 | Reg loss: 0.025 | Tree loss: 7.739 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 10 | Batch: 024 / 025 | Total loss: 7.703 | Reg loss: 0.026 | Tree loss: 7.703 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 11 | Batch: 000 / 025 | Total loss: 8.066 | Reg loss: 0.021 | Tree loss: 8.066 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 11 | Batch: 001 / 025 | Total loss: 8.048 | Reg loss: 0.021 | Tree loss: 8.048 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 11 | Batch: 002 / 025 | Total loss: 8.032 | Reg loss: 0.021 | Tree loss: 8.032 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 11 | Batch: 003 / 025 | Total loss: 7.998 | Reg loss: 0.021 | Tree loss: 7.998 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 11 | Batch: 004 / 025 | Total loss: 7.988 | Reg loss: 0.021 | Tree loss: 7.988 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 11 | Batch: 005 / 025 | Total loss: 7.967 | Reg loss: 0.021 | Tree loss: 7.967 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 11 | Batch: 006 / 025 | Total loss: 7.946 | Reg loss: 0.021 | Tree loss: 7.946 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 11 | Batch: 007 / 025 | Total loss: 7.914 | Reg loss: 0.021 | Tree loss: 7.914 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 11 | Batch: 008 / 025 | Total loss: 7.906 | Reg loss: 0.022 | Tree loss: 7.906 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 11 | Batch: 009 / 025 | Total loss: 7.868 | Reg loss: 0.022 | Tree loss: 7.868 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 11 | Batch: 010 / 025 | Total loss: 7.856 | Reg loss: 0.022 | Tree loss: 7.856 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 11 | Batch: 011 / 025 | Total loss: 7.834 | Reg loss: 0.022 | Tree loss: 7.834 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 11 | Batch: 012 / 025 | Total loss: 7.813 | Reg loss: 0.022 | Tree loss: 7.813 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 11 | Batch: 013 / 025 | Total loss: 7.807 | Reg loss: 0.023 | Tree loss: 7.807 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 11 | Batch: 014 / 025 | Total loss: 7.768 | Reg loss: 0.023 | Tree loss: 7.768 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 11 | Batch: 015 / 025 | Total loss: 7.745 | Reg loss: 0.023 | Tree loss: 7.745 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 11 | Batch: 016 / 025 | Total loss: 7.728 | Reg loss: 0.024 | Tree loss: 7.728 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 11 | Batch: 017 / 025 | Total loss: 7.708 | Reg loss: 0.024 | Tree loss: 7.708 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 11 | Batch: 018 / 025 | Total loss: 7.685 | Reg loss: 0.024 | Tree loss: 7.685 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 11 | Batch: 019 / 025 | Total loss: 7.680 | Reg loss: 0.025 | Tree loss: 7.680 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 11 | Batch: 020 / 025 | Total loss: 7.635 | Reg loss: 0.025 | Tree loss: 7.635 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 11 | Batch: 021 / 025 | Total loss: 7.614 | Reg loss: 0.025 | Tree loss: 7.614 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 11 | Batch: 022 / 025 | Total loss: 7.584 | Reg loss: 0.026 | Tree loss: 7.584 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 11 | Batch: 023 / 025 | Total loss: 7.578 | Reg loss: 0.026 | Tree loss: 7.578 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 11 | Batch: 024 / 025 | Total loss: 7.552 | Reg loss: 0.026 | Tree loss: 7.552 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 12 | Batch: 000 / 025 | Total loss: 7.917 | Reg loss: 0.022 | Tree loss: 7.917 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 12 | Batch: 001 / 025 | Total loss: 7.895 | Reg loss: 0.022 | Tree loss: 7.895 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 12 | Batch: 002 / 025 | Total loss: 7.882 | Reg loss: 0.022 | Tree loss: 7.882 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 12 | Batch: 003 / 025 | Total loss: 7.856 | Reg loss: 0.022 | Tree loss: 7.856 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 12 | Batch: 004 / 025 | Total loss: 7.840 | Reg loss: 0.022 | Tree loss: 7.840 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 12 | Batch: 005 / 025 | Total loss: 7.820 | Reg loss: 0.022 | Tree loss: 7.820 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 12 | Batch: 006 / 025 | Total loss: 7.799 | Reg loss: 0.022 | Tree loss: 7.799 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 12 | Batch: 007 / 025 | Total loss: 7.756 | Reg loss: 0.022 | Tree loss: 7.756 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 12 | Batch: 008 / 025 | Total loss: 7.745 | Reg loss: 0.023 | Tree loss: 7.745 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 12 | Batch: 009 / 025 | Total loss: 7.719 | Reg loss: 0.023 | Tree loss: 7.719 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 12 | Batch: 010 / 025 | Total loss: 7.705 | Reg loss: 0.023 | Tree loss: 7.705 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 12 | Batch: 011 / 025 | Total loss: 7.681 | Reg loss: 0.023 | Tree loss: 7.681 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 12 | Batch: 012 / 025 | Total loss: 7.657 | Reg loss: 0.023 | Tree loss: 7.657 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 12 | Batch: 013 / 025 | Total loss: 7.619 | Reg loss: 0.024 | Tree loss: 7.619 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 12 | Batch: 014 / 025 | Total loss: 7.611 | Reg loss: 0.024 | Tree loss: 7.611 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 12 | Batch: 015 / 025 | Total loss: 7.576 | Reg loss: 0.024 | Tree loss: 7.576 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 12 | Batch: 016 / 025 | Total loss: 7.579 | Reg loss: 0.025 | Tree loss: 7.579 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 12 | Batch: 017 / 025 | Total loss: 7.548 | Reg loss: 0.025 | Tree loss: 7.548 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 12 | Batch: 018 / 025 | Total loss: 7.525 | Reg loss: 0.025 | Tree loss: 7.525 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 12 | Batch: 019 / 025 | Total loss: 7.492 | Reg loss: 0.026 | Tree loss: 7.492 | Accuracy: 1.000000 | 0.067 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12 | Batch: 020 / 025 | Total loss: 7.477 | Reg loss: 0.026 | Tree loss: 7.477 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 12 | Batch: 021 / 025 | Total loss: 7.455 | Reg loss: 0.026 | Tree loss: 7.455 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 12 | Batch: 022 / 025 | Total loss: 7.425 | Reg loss: 0.027 | Tree loss: 7.425 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 12 | Batch: 023 / 025 | Total loss: 7.400 | Reg loss: 0.027 | Tree loss: 7.400 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 12 | Batch: 024 / 025 | Total loss: 7.368 | Reg loss: 0.027 | Tree loss: 7.368 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 13 | Batch: 000 / 025 | Total loss: 7.773 | Reg loss: 0.023 | Tree loss: 7.773 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 13 | Batch: 001 / 025 | Total loss: 7.740 | Reg loss: 0.023 | Tree loss: 7.740 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 13 | Batch: 002 / 025 | Total loss: 7.727 | Reg loss: 0.023 | Tree loss: 7.727 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 13 | Batch: 003 / 025 | Total loss: 7.698 | Reg loss: 0.023 | Tree loss: 7.698 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 13 | Batch: 004 / 025 | Total loss: 7.677 | Reg loss: 0.023 | Tree loss: 7.677 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 13 | Batch: 005 / 025 | Total loss: 7.651 | Reg loss: 0.023 | Tree loss: 7.651 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 13 | Batch: 006 / 025 | Total loss: 7.638 | Reg loss: 0.023 | Tree loss: 7.638 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 13 | Batch: 007 / 025 | Total loss: 7.600 | Reg loss: 0.023 | Tree loss: 7.600 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 13 | Batch: 008 / 025 | Total loss: 7.576 | Reg loss: 0.023 | Tree loss: 7.576 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 13 | Batch: 009 / 025 | Total loss: 7.564 | Reg loss: 0.024 | Tree loss: 7.564 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 13 | Batch: 010 / 025 | Total loss: 7.527 | Reg loss: 0.024 | Tree loss: 7.527 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 13 | Batch: 011 / 025 | Total loss: 7.521 | Reg loss: 0.024 | Tree loss: 7.521 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 13 | Batch: 012 / 025 | Total loss: 7.497 | Reg loss: 0.024 | Tree loss: 7.497 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 13 | Batch: 013 / 025 | Total loss: 7.466 | Reg loss: 0.025 | Tree loss: 7.466 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 13 | Batch: 014 / 025 | Total loss: 7.456 | Reg loss: 0.025 | Tree loss: 7.456 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 13 | Batch: 015 / 025 | Total loss: 7.406 | Reg loss: 0.025 | Tree loss: 7.406 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 13 | Batch: 016 / 025 | Total loss: 7.393 | Reg loss: 0.025 | Tree loss: 7.393 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 13 | Batch: 017 / 025 | Total loss: 7.371 | Reg loss: 0.026 | Tree loss: 7.371 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 13 | Batch: 018 / 025 | Total loss: 7.332 | Reg loss: 0.026 | Tree loss: 7.332 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 13 | Batch: 019 / 025 | Total loss: 7.322 | Reg loss: 0.026 | Tree loss: 7.322 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 13 | Batch: 020 / 025 | Total loss: 7.306 | Reg loss: 0.027 | Tree loss: 7.306 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 13 | Batch: 021 / 025 | Total loss: 7.267 | Reg loss: 0.027 | Tree loss: 7.267 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 13 | Batch: 022 / 025 | Total loss: 7.247 | Reg loss: 0.027 | Tree loss: 7.247 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 13 | Batch: 023 / 025 | Total loss: 7.240 | Reg loss: 0.028 | Tree loss: 7.240 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 13 | Batch: 024 / 025 | Total loss: 7.212 | Reg loss: 0.028 | Tree loss: 7.212 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 14 | Batch: 000 / 025 | Total loss: 7.612 | Reg loss: 0.023 | Tree loss: 7.612 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 14 | Batch: 001 / 025 | Total loss: 7.594 | Reg loss: 0.023 | Tree loss: 7.594 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 14 | Batch: 002 / 025 | Total loss: 7.560 | Reg loss: 0.023 | Tree loss: 7.560 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 14 | Batch: 003 / 025 | Total loss: 7.552 | Reg loss: 0.023 | Tree loss: 7.552 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 14 | Batch: 004 / 025 | Total loss: 7.517 | Reg loss: 0.024 | Tree loss: 7.517 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 14 | Batch: 005 / 025 | Total loss: 7.494 | Reg loss: 0.024 | Tree loss: 7.494 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 14 | Batch: 006 / 025 | Total loss: 7.471 | Reg loss: 0.024 | Tree loss: 7.471 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 14 | Batch: 007 / 025 | Total loss: 7.431 | Reg loss: 0.024 | Tree loss: 7.431 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 14 | Batch: 008 / 025 | Total loss: 7.422 | Reg loss: 0.024 | Tree loss: 7.422 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 14 | Batch: 009 / 025 | Total loss: 7.400 | Reg loss: 0.024 | Tree loss: 7.400 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 14 | Batch: 010 / 025 | Total loss: 7.365 | Reg loss: 0.025 | Tree loss: 7.365 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 14 | Batch: 011 / 025 | Total loss: 7.337 | Reg loss: 0.025 | Tree loss: 7.337 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 14 | Batch: 012 / 025 | Total loss: 7.323 | Reg loss: 0.025 | Tree loss: 7.323 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 14 | Batch: 013 / 025 | Total loss: 7.290 | Reg loss: 0.025 | Tree loss: 7.290 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 14 | Batch: 014 / 025 | Total loss: 7.264 | Reg loss: 0.025 | Tree loss: 7.264 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 14 | Batch: 015 / 025 | Total loss: 7.237 | Reg loss: 0.026 | Tree loss: 7.237 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 14 | Batch: 016 / 025 | Total loss: 7.205 | Reg loss: 0.026 | Tree loss: 7.205 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 14 | Batch: 017 / 025 | Total loss: 7.196 | Reg loss: 0.026 | Tree loss: 7.196 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 14 | Batch: 018 / 025 | Total loss: 7.184 | Reg loss: 0.027 | Tree loss: 7.184 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 14 | Batch: 019 / 025 | Total loss: 7.146 | Reg loss: 0.027 | Tree loss: 7.146 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 14 | Batch: 020 / 025 | Total loss: 7.116 | Reg loss: 0.027 | Tree loss: 7.116 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 14 | Batch: 021 / 025 | Total loss: 7.082 | Reg loss: 0.027 | Tree loss: 7.082 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 14 | Batch: 022 / 025 | Total loss: 7.069 | Reg loss: 0.028 | Tree loss: 7.069 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 14 | Batch: 023 / 025 | Total loss: 7.036 | Reg loss: 0.028 | Tree loss: 7.036 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 14 | Batch: 024 / 025 | Total loss: 7.012 | Reg loss: 0.028 | Tree loss: 7.012 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 15 | Batch: 000 / 025 | Total loss: 7.447 | Reg loss: 0.024 | Tree loss: 7.447 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 15 | Batch: 001 / 025 | Total loss: 7.433 | Reg loss: 0.024 | Tree loss: 7.433 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 15 | Batch: 002 / 025 | Total loss: 7.397 | Reg loss: 0.024 | Tree loss: 7.397 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 15 | Batch: 003 / 025 | Total loss: 7.370 | Reg loss: 0.024 | Tree loss: 7.370 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 15 | Batch: 004 / 025 | Total loss: 7.360 | Reg loss: 0.024 | Tree loss: 7.360 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 15 | Batch: 005 / 025 | Total loss: 7.325 | Reg loss: 0.024 | Tree loss: 7.325 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 15 | Batch: 006 / 025 | Total loss: 7.280 | Reg loss: 0.024 | Tree loss: 7.280 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 15 | Batch: 007 / 025 | Total loss: 7.279 | Reg loss: 0.025 | Tree loss: 7.279 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 15 | Batch: 008 / 025 | Total loss: 7.252 | Reg loss: 0.025 | Tree loss: 7.252 | Accuracy: 1.000000 | 0.067 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15 | Batch: 009 / 025 | Total loss: 7.226 | Reg loss: 0.025 | Tree loss: 7.226 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 15 | Batch: 010 / 025 | Total loss: 7.192 | Reg loss: 0.025 | Tree loss: 7.192 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 15 | Batch: 011 / 025 | Total loss: 7.162 | Reg loss: 0.025 | Tree loss: 7.162 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 15 | Batch: 012 / 025 | Total loss: 7.148 | Reg loss: 0.025 | Tree loss: 7.148 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 15 | Batch: 013 / 025 | Total loss: 7.104 | Reg loss: 0.026 | Tree loss: 7.104 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 15 | Batch: 014 / 025 | Total loss: 7.105 | Reg loss: 0.026 | Tree loss: 7.105 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 15 | Batch: 015 / 025 | Total loss: 7.060 | Reg loss: 0.026 | Tree loss: 7.060 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 15 | Batch: 016 / 025 | Total loss: 7.046 | Reg loss: 0.026 | Tree loss: 7.046 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 15 | Batch: 017 / 025 | Total loss: 7.013 | Reg loss: 0.027 | Tree loss: 7.013 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 15 | Batch: 018 / 025 | Total loss: 6.982 | Reg loss: 0.027 | Tree loss: 6.982 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 15 | Batch: 019 / 025 | Total loss: 6.960 | Reg loss: 0.027 | Tree loss: 6.960 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 15 | Batch: 020 / 025 | Total loss: 6.927 | Reg loss: 0.028 | Tree loss: 6.927 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 15 | Batch: 021 / 025 | Total loss: 6.915 | Reg loss: 0.028 | Tree loss: 6.915 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 15 | Batch: 022 / 025 | Total loss: 6.885 | Reg loss: 0.028 | Tree loss: 6.885 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 15 | Batch: 023 / 025 | Total loss: 6.863 | Reg loss: 0.028 | Tree loss: 6.863 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 15 | Batch: 024 / 025 | Total loss: 6.809 | Reg loss: 0.029 | Tree loss: 6.809 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 16 | Batch: 000 / 025 | Total loss: 7.269 | Reg loss: 0.025 | Tree loss: 7.269 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 16 | Batch: 001 / 025 | Total loss: 7.246 | Reg loss: 0.025 | Tree loss: 7.246 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 16 | Batch: 002 / 025 | Total loss: 7.228 | Reg loss: 0.025 | Tree loss: 7.228 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 16 | Batch: 003 / 025 | Total loss: 7.201 | Reg loss: 0.025 | Tree loss: 7.201 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 16 | Batch: 004 / 025 | Total loss: 7.174 | Reg loss: 0.025 | Tree loss: 7.174 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 16 | Batch: 005 / 025 | Total loss: 7.156 | Reg loss: 0.025 | Tree loss: 7.156 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 16 | Batch: 006 / 025 | Total loss: 7.117 | Reg loss: 0.025 | Tree loss: 7.117 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 16 | Batch: 007 / 025 | Total loss: 7.110 | Reg loss: 0.025 | Tree loss: 7.110 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 16 | Batch: 008 / 025 | Total loss: 7.069 | Reg loss: 0.025 | Tree loss: 7.069 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 16 | Batch: 009 / 025 | Total loss: 7.058 | Reg loss: 0.025 | Tree loss: 7.058 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 16 | Batch: 010 / 025 | Total loss: 7.017 | Reg loss: 0.026 | Tree loss: 7.017 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 16 | Batch: 011 / 025 | Total loss: 6.990 | Reg loss: 0.026 | Tree loss: 6.990 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 16 | Batch: 012 / 025 | Total loss: 6.977 | Reg loss: 0.026 | Tree loss: 6.977 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 16 | Batch: 013 / 025 | Total loss: 6.935 | Reg loss: 0.026 | Tree loss: 6.935 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 16 | Batch: 014 / 025 | Total loss: 6.904 | Reg loss: 0.026 | Tree loss: 6.904 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 16 | Batch: 015 / 025 | Total loss: 6.902 | Reg loss: 0.027 | Tree loss: 6.902 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 16 | Batch: 016 / 025 | Total loss: 6.869 | Reg loss: 0.027 | Tree loss: 6.869 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 16 | Batch: 017 / 025 | Total loss: 6.837 | Reg loss: 0.027 | Tree loss: 6.837 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 16 | Batch: 018 / 025 | Total loss: 6.805 | Reg loss: 0.027 | Tree loss: 6.805 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 16 | Batch: 019 / 025 | Total loss: 6.788 | Reg loss: 0.028 | Tree loss: 6.788 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 16 | Batch: 020 / 025 | Total loss: 6.758 | Reg loss: 0.028 | Tree loss: 6.758 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 16 | Batch: 021 / 025 | Total loss: 6.743 | Reg loss: 0.028 | Tree loss: 6.743 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 16 | Batch: 022 / 025 | Total loss: 6.693 | Reg loss: 0.028 | Tree loss: 6.693 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 16 | Batch: 023 / 025 | Total loss: 6.671 | Reg loss: 0.029 | Tree loss: 6.671 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 16 | Batch: 024 / 025 | Total loss: 6.636 | Reg loss: 0.029 | Tree loss: 6.636 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 17 | Batch: 000 / 025 | Total loss: 7.107 | Reg loss: 0.025 | Tree loss: 7.107 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 17 | Batch: 001 / 025 | Total loss: 7.082 | Reg loss: 0.025 | Tree loss: 7.082 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 17 | Batch: 002 / 025 | Total loss: 7.051 | Reg loss: 0.025 | Tree loss: 7.051 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 17 | Batch: 003 / 025 | Total loss: 7.036 | Reg loss: 0.025 | Tree loss: 7.036 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 17 | Batch: 004 / 025 | Total loss: 7.001 | Reg loss: 0.025 | Tree loss: 7.001 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 17 | Batch: 005 / 025 | Total loss: 6.979 | Reg loss: 0.025 | Tree loss: 6.979 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 17 | Batch: 006 / 025 | Total loss: 6.930 | Reg loss: 0.025 | Tree loss: 6.930 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 17 | Batch: 007 / 025 | Total loss: 6.924 | Reg loss: 0.025 | Tree loss: 6.924 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 17 | Batch: 008 / 025 | Total loss: 6.887 | Reg loss: 0.026 | Tree loss: 6.887 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 17 | Batch: 009 / 025 | Total loss: 6.855 | Reg loss: 0.026 | Tree loss: 6.855 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 17 | Batch: 010 / 025 | Total loss: 6.834 | Reg loss: 0.026 | Tree loss: 6.834 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 17 | Batch: 011 / 025 | Total loss: 6.817 | Reg loss: 0.026 | Tree loss: 6.817 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 17 | Batch: 012 / 025 | Total loss: 6.792 | Reg loss: 0.026 | Tree loss: 6.792 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 17 | Batch: 013 / 025 | Total loss: 6.764 | Reg loss: 0.026 | Tree loss: 6.764 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 17 | Batch: 014 / 025 | Total loss: 6.730 | Reg loss: 0.027 | Tree loss: 6.730 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 17 | Batch: 015 / 025 | Total loss: 6.707 | Reg loss: 0.027 | Tree loss: 6.707 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 17 | Batch: 016 / 025 | Total loss: 6.694 | Reg loss: 0.027 | Tree loss: 6.694 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 17 | Batch: 017 / 025 | Total loss: 6.663 | Reg loss: 0.027 | Tree loss: 6.663 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 17 | Batch: 018 / 025 | Total loss: 6.628 | Reg loss: 0.028 | Tree loss: 6.628 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 17 | Batch: 019 / 025 | Total loss: 6.604 | Reg loss: 0.028 | Tree loss: 6.604 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 17 | Batch: 020 / 025 | Total loss: 6.599 | Reg loss: 0.028 | Tree loss: 6.599 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 17 | Batch: 021 / 025 | Total loss: 6.564 | Reg loss: 0.028 | Tree loss: 6.564 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 17 | Batch: 022 / 025 | Total loss: 6.542 | Reg loss: 0.028 | Tree loss: 6.542 | Accuracy: 1.000000 | 0.067 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17 | Batch: 023 / 025 | Total loss: 6.509 | Reg loss: 0.029 | Tree loss: 6.509 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 17 | Batch: 024 / 025 | Total loss: 6.478 | Reg loss: 0.029 | Tree loss: 6.478 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 18 | Batch: 000 / 025 | Total loss: 6.925 | Reg loss: 0.025 | Tree loss: 6.925 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 18 | Batch: 001 / 025 | Total loss: 6.903 | Reg loss: 0.025 | Tree loss: 6.903 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 18 | Batch: 002 / 025 | Total loss: 6.883 | Reg loss: 0.025 | Tree loss: 6.883 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 18 | Batch: 003 / 025 | Total loss: 6.857 | Reg loss: 0.025 | Tree loss: 6.857 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 18 | Batch: 004 / 025 | Total loss: 6.816 | Reg loss: 0.026 | Tree loss: 6.816 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 18 | Batch: 005 / 025 | Total loss: 6.819 | Reg loss: 0.026 | Tree loss: 6.819 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 18 | Batch: 006 / 025 | Total loss: 6.772 | Reg loss: 0.026 | Tree loss: 6.772 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 18 | Batch: 007 / 025 | Total loss: 6.739 | Reg loss: 0.026 | Tree loss: 6.739 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 18 | Batch: 008 / 025 | Total loss: 6.728 | Reg loss: 0.026 | Tree loss: 6.728 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 18 | Batch: 009 / 025 | Total loss: 6.695 | Reg loss: 0.026 | Tree loss: 6.695 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 18 | Batch: 010 / 025 | Total loss: 6.663 | Reg loss: 0.026 | Tree loss: 6.663 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 18 | Batch: 011 / 025 | Total loss: 6.654 | Reg loss: 0.026 | Tree loss: 6.654 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 18 | Batch: 012 / 025 | Total loss: 6.628 | Reg loss: 0.027 | Tree loss: 6.628 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 18 | Batch: 013 / 025 | Total loss: 6.580 | Reg loss: 0.027 | Tree loss: 6.580 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 18 | Batch: 014 / 025 | Total loss: 6.566 | Reg loss: 0.027 | Tree loss: 6.566 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 18 | Batch: 015 / 025 | Total loss: 6.546 | Reg loss: 0.027 | Tree loss: 6.546 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 18 | Batch: 016 / 025 | Total loss: 6.532 | Reg loss: 0.027 | Tree loss: 6.532 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 18 | Batch: 017 / 025 | Total loss: 6.481 | Reg loss: 0.027 | Tree loss: 6.481 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 18 | Batch: 018 / 025 | Total loss: 6.458 | Reg loss: 0.028 | Tree loss: 6.458 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 18 | Batch: 019 / 025 | Total loss: 6.447 | Reg loss: 0.028 | Tree loss: 6.447 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 18 | Batch: 020 / 025 | Total loss: 6.403 | Reg loss: 0.028 | Tree loss: 6.403 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 18 | Batch: 021 / 025 | Total loss: 6.376 | Reg loss: 0.028 | Tree loss: 6.376 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 18 | Batch: 022 / 025 | Total loss: 6.358 | Reg loss: 0.029 | Tree loss: 6.358 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 18 | Batch: 023 / 025 | Total loss: 6.302 | Reg loss: 0.029 | Tree loss: 6.302 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 18 | Batch: 024 / 025 | Total loss: 6.306 | Reg loss: 0.029 | Tree loss: 6.306 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 19 | Batch: 000 / 025 | Total loss: 6.755 | Reg loss: 0.026 | Tree loss: 6.755 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 19 | Batch: 001 / 025 | Total loss: 6.743 | Reg loss: 0.026 | Tree loss: 6.743 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 19 | Batch: 002 / 025 | Total loss: 6.699 | Reg loss: 0.026 | Tree loss: 6.699 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 19 | Batch: 003 / 025 | Total loss: 6.690 | Reg loss: 0.026 | Tree loss: 6.690 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 19 | Batch: 004 / 025 | Total loss: 6.661 | Reg loss: 0.026 | Tree loss: 6.661 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 19 | Batch: 005 / 025 | Total loss: 6.605 | Reg loss: 0.026 | Tree loss: 6.605 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 19 | Batch: 006 / 025 | Total loss: 6.608 | Reg loss: 0.026 | Tree loss: 6.608 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 19 | Batch: 007 / 025 | Total loss: 6.569 | Reg loss: 0.026 | Tree loss: 6.569 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 19 | Batch: 008 / 025 | Total loss: 6.550 | Reg loss: 0.026 | Tree loss: 6.550 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 19 | Batch: 009 / 025 | Total loss: 6.528 | Reg loss: 0.026 | Tree loss: 6.528 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 19 | Batch: 010 / 025 | Total loss: 6.492 | Reg loss: 0.026 | Tree loss: 6.492 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 19 | Batch: 011 / 025 | Total loss: 6.480 | Reg loss: 0.027 | Tree loss: 6.480 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 19 | Batch: 012 / 025 | Total loss: 6.441 | Reg loss: 0.027 | Tree loss: 6.441 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 19 | Batch: 013 / 025 | Total loss: 6.416 | Reg loss: 0.027 | Tree loss: 6.416 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 19 | Batch: 014 / 025 | Total loss: 6.385 | Reg loss: 0.027 | Tree loss: 6.385 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 19 | Batch: 015 / 025 | Total loss: 6.363 | Reg loss: 0.027 | Tree loss: 6.363 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 19 | Batch: 016 / 025 | Total loss: 6.339 | Reg loss: 0.027 | Tree loss: 6.339 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 19 | Batch: 017 / 025 | Total loss: 6.304 | Reg loss: 0.028 | Tree loss: 6.304 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 19 | Batch: 018 / 025 | Total loss: 6.293 | Reg loss: 0.028 | Tree loss: 6.293 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 19 | Batch: 019 / 025 | Total loss: 6.266 | Reg loss: 0.028 | Tree loss: 6.266 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 19 | Batch: 020 / 025 | Total loss: 6.239 | Reg loss: 0.028 | Tree loss: 6.239 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 19 | Batch: 021 / 025 | Total loss: 6.202 | Reg loss: 0.028 | Tree loss: 6.202 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 19 | Batch: 022 / 025 | Total loss: 6.180 | Reg loss: 0.029 | Tree loss: 6.180 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 19 | Batch: 023 / 025 | Total loss: 6.183 | Reg loss: 0.029 | Tree loss: 6.183 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 19 | Batch: 024 / 025 | Total loss: 6.134 | Reg loss: 0.029 | Tree loss: 6.134 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 20 | Batch: 000 / 025 | Total loss: 6.590 | Reg loss: 0.026 | Tree loss: 6.590 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 20 | Batch: 001 / 025 | Total loss: 6.571 | Reg loss: 0.026 | Tree loss: 6.571 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 20 | Batch: 002 / 025 | Total loss: 6.556 | Reg loss: 0.026 | Tree loss: 6.556 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 20 | Batch: 003 / 025 | Total loss: 6.521 | Reg loss: 0.026 | Tree loss: 6.521 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 20 | Batch: 004 / 025 | Total loss: 6.479 | Reg loss: 0.026 | Tree loss: 6.479 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 20 | Batch: 005 / 025 | Total loss: 6.459 | Reg loss: 0.026 | Tree loss: 6.459 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 20 | Batch: 006 / 025 | Total loss: 6.427 | Reg loss: 0.026 | Tree loss: 6.427 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 20 | Batch: 007 / 025 | Total loss: 6.398 | Reg loss: 0.026 | Tree loss: 6.398 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 20 | Batch: 008 / 025 | Total loss: 6.386 | Reg loss: 0.026 | Tree loss: 6.386 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 20 | Batch: 009 / 025 | Total loss: 6.346 | Reg loss: 0.027 | Tree loss: 6.346 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 20 | Batch: 010 / 025 | Total loss: 6.327 | Reg loss: 0.027 | Tree loss: 6.327 | Accuracy: 1.000000 | 0.067 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20 | Batch: 011 / 025 | Total loss: 6.296 | Reg loss: 0.027 | Tree loss: 6.296 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 20 | Batch: 012 / 025 | Total loss: 6.271 | Reg loss: 0.027 | Tree loss: 6.271 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 20 | Batch: 013 / 025 | Total loss: 6.249 | Reg loss: 0.027 | Tree loss: 6.249 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 20 | Batch: 014 / 025 | Total loss: 6.209 | Reg loss: 0.027 | Tree loss: 6.209 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 20 | Batch: 015 / 025 | Total loss: 6.193 | Reg loss: 0.027 | Tree loss: 6.193 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 20 | Batch: 016 / 025 | Total loss: 6.185 | Reg loss: 0.028 | Tree loss: 6.185 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 20 | Batch: 017 / 025 | Total loss: 6.140 | Reg loss: 0.028 | Tree loss: 6.140 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 20 | Batch: 018 / 025 | Total loss: 6.093 | Reg loss: 0.028 | Tree loss: 6.093 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 20 | Batch: 019 / 025 | Total loss: 6.091 | Reg loss: 0.028 | Tree loss: 6.091 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 20 | Batch: 020 / 025 | Total loss: 6.076 | Reg loss: 0.028 | Tree loss: 6.076 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 20 | Batch: 021 / 025 | Total loss: 6.047 | Reg loss: 0.028 | Tree loss: 6.047 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 20 | Batch: 022 / 025 | Total loss: 6.016 | Reg loss: 0.029 | Tree loss: 6.016 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 20 | Batch: 023 / 025 | Total loss: 5.992 | Reg loss: 0.029 | Tree loss: 5.992 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 20 | Batch: 024 / 025 | Total loss: 5.974 | Reg loss: 0.029 | Tree loss: 5.974 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 21 | Batch: 000 / 025 | Total loss: 6.411 | Reg loss: 0.026 | Tree loss: 6.411 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 21 | Batch: 001 / 025 | Total loss: 6.407 | Reg loss: 0.026 | Tree loss: 6.407 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 21 | Batch: 002 / 025 | Total loss: 6.360 | Reg loss: 0.026 | Tree loss: 6.360 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 21 | Batch: 003 / 025 | Total loss: 6.332 | Reg loss: 0.026 | Tree loss: 6.332 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 21 | Batch: 004 / 025 | Total loss: 6.323 | Reg loss: 0.026 | Tree loss: 6.323 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 21 | Batch: 005 / 025 | Total loss: 6.281 | Reg loss: 0.026 | Tree loss: 6.281 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 21 | Batch: 006 / 025 | Total loss: 6.268 | Reg loss: 0.026 | Tree loss: 6.268 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 21 | Batch: 007 / 025 | Total loss: 6.228 | Reg loss: 0.027 | Tree loss: 6.228 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 21 | Batch: 008 / 025 | Total loss: 6.197 | Reg loss: 0.027 | Tree loss: 6.197 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 21 | Batch: 009 / 025 | Total loss: 6.199 | Reg loss: 0.027 | Tree loss: 6.199 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 21 | Batch: 010 / 025 | Total loss: 6.158 | Reg loss: 0.027 | Tree loss: 6.158 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 21 | Batch: 011 / 025 | Total loss: 6.124 | Reg loss: 0.027 | Tree loss: 6.124 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 21 | Batch: 012 / 025 | Total loss: 6.101 | Reg loss: 0.027 | Tree loss: 6.101 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 21 | Batch: 013 / 025 | Total loss: 6.087 | Reg loss: 0.027 | Tree loss: 6.087 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 21 | Batch: 014 / 025 | Total loss: 6.051 | Reg loss: 0.027 | Tree loss: 6.051 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 21 | Batch: 015 / 025 | Total loss: 6.024 | Reg loss: 0.028 | Tree loss: 6.024 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 21 | Batch: 016 / 025 | Total loss: 6.007 | Reg loss: 0.028 | Tree loss: 6.007 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 21 | Batch: 017 / 025 | Total loss: 5.978 | Reg loss: 0.028 | Tree loss: 5.978 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 21 | Batch: 018 / 025 | Total loss: 5.948 | Reg loss: 0.028 | Tree loss: 5.948 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 21 | Batch: 019 / 025 | Total loss: 5.917 | Reg loss: 0.028 | Tree loss: 5.917 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 21 | Batch: 020 / 025 | Total loss: 5.920 | Reg loss: 0.028 | Tree loss: 5.920 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 21 | Batch: 021 / 025 | Total loss: 5.879 | Reg loss: 0.029 | Tree loss: 5.879 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 21 | Batch: 022 / 025 | Total loss: 5.874 | Reg loss: 0.029 | Tree loss: 5.874 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 21 | Batch: 023 / 025 | Total loss: 5.836 | Reg loss: 0.029 | Tree loss: 5.836 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 21 | Batch: 024 / 025 | Total loss: 5.818 | Reg loss: 0.029 | Tree loss: 5.818 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 22 | Batch: 000 / 025 | Total loss: 6.260 | Reg loss: 0.026 | Tree loss: 6.260 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 22 | Batch: 001 / 025 | Total loss: 6.221 | Reg loss: 0.026 | Tree loss: 6.221 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 22 | Batch: 002 / 025 | Total loss: 6.196 | Reg loss: 0.026 | Tree loss: 6.196 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 22 | Batch: 003 / 025 | Total loss: 6.173 | Reg loss: 0.026 | Tree loss: 6.173 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 22 | Batch: 004 / 025 | Total loss: 6.145 | Reg loss: 0.026 | Tree loss: 6.145 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 22 | Batch: 005 / 025 | Total loss: 6.137 | Reg loss: 0.027 | Tree loss: 6.137 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 22 | Batch: 006 / 025 | Total loss: 6.083 | Reg loss: 0.027 | Tree loss: 6.083 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 22 | Batch: 007 / 025 | Total loss: 6.059 | Reg loss: 0.027 | Tree loss: 6.059 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 22 | Batch: 008 / 025 | Total loss: 6.040 | Reg loss: 0.027 | Tree loss: 6.040 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 22 | Batch: 009 / 025 | Total loss: 6.008 | Reg loss: 0.027 | Tree loss: 6.008 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 22 | Batch: 010 / 025 | Total loss: 5.990 | Reg loss: 0.027 | Tree loss: 5.990 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 22 | Batch: 011 / 025 | Total loss: 5.986 | Reg loss: 0.027 | Tree loss: 5.986 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 22 | Batch: 012 / 025 | Total loss: 5.942 | Reg loss: 0.027 | Tree loss: 5.942 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 22 | Batch: 013 / 025 | Total loss: 5.919 | Reg loss: 0.027 | Tree loss: 5.919 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 22 | Batch: 014 / 025 | Total loss: 5.896 | Reg loss: 0.027 | Tree loss: 5.896 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 22 | Batch: 015 / 025 | Total loss: 5.873 | Reg loss: 0.028 | Tree loss: 5.873 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 22 | Batch: 016 / 025 | Total loss: 5.841 | Reg loss: 0.028 | Tree loss: 5.841 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 22 | Batch: 017 / 025 | Total loss: 5.813 | Reg loss: 0.028 | Tree loss: 5.813 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 22 | Batch: 018 / 025 | Total loss: 5.800 | Reg loss: 0.028 | Tree loss: 5.800 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 22 | Batch: 019 / 025 | Total loss: 5.759 | Reg loss: 0.028 | Tree loss: 5.759 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 22 | Batch: 020 / 025 | Total loss: 5.750 | Reg loss: 0.028 | Tree loss: 5.750 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 22 | Batch: 021 / 025 | Total loss: 5.712 | Reg loss: 0.029 | Tree loss: 5.712 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 22 | Batch: 022 / 025 | Total loss: 5.676 | Reg loss: 0.029 | Tree loss: 5.676 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 22 | Batch: 023 / 025 | Total loss: 5.682 | Reg loss: 0.029 | Tree loss: 5.682 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 22 | Batch: 024 / 025 | Total loss: 5.639 | Reg loss: 0.029 | Tree loss: 5.639 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23 | Batch: 000 / 025 | Total loss: 6.078 | Reg loss: 0.027 | Tree loss: 6.078 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 23 | Batch: 001 / 025 | Total loss: 6.063 | Reg loss: 0.027 | Tree loss: 6.063 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 23 | Batch: 002 / 025 | Total loss: 6.042 | Reg loss: 0.027 | Tree loss: 6.042 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 23 | Batch: 003 / 025 | Total loss: 6.013 | Reg loss: 0.027 | Tree loss: 6.013 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 23 | Batch: 004 / 025 | Total loss: 5.976 | Reg loss: 0.027 | Tree loss: 5.976 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 23 | Batch: 005 / 025 | Total loss: 5.967 | Reg loss: 0.027 | Tree loss: 5.967 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 23 | Batch: 006 / 025 | Total loss: 5.935 | Reg loss: 0.027 | Tree loss: 5.935 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 23 | Batch: 007 / 025 | Total loss: 5.904 | Reg loss: 0.027 | Tree loss: 5.904 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 23 | Batch: 008 / 025 | Total loss: 5.877 | Reg loss: 0.027 | Tree loss: 5.877 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 23 | Batch: 009 / 025 | Total loss: 5.850 | Reg loss: 0.027 | Tree loss: 5.850 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 23 | Batch: 010 / 025 | Total loss: 5.829 | Reg loss: 0.027 | Tree loss: 5.829 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 23 | Batch: 011 / 025 | Total loss: 5.820 | Reg loss: 0.027 | Tree loss: 5.820 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 23 | Batch: 012 / 025 | Total loss: 5.787 | Reg loss: 0.027 | Tree loss: 5.787 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 23 | Batch: 013 / 025 | Total loss: 5.756 | Reg loss: 0.027 | Tree loss: 5.756 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 23 | Batch: 014 / 025 | Total loss: 5.726 | Reg loss: 0.028 | Tree loss: 5.726 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 23 | Batch: 015 / 025 | Total loss: 5.700 | Reg loss: 0.028 | Tree loss: 5.700 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 23 | Batch: 016 / 025 | Total loss: 5.679 | Reg loss: 0.028 | Tree loss: 5.679 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 23 | Batch: 017 / 025 | Total loss: 5.648 | Reg loss: 0.028 | Tree loss: 5.648 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 23 | Batch: 018 / 025 | Total loss: 5.632 | Reg loss: 0.028 | Tree loss: 5.632 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 23 | Batch: 019 / 025 | Total loss: 5.599 | Reg loss: 0.028 | Tree loss: 5.599 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 23 | Batch: 020 / 025 | Total loss: 5.572 | Reg loss: 0.028 | Tree loss: 5.572 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 23 | Batch: 021 / 025 | Total loss: 5.555 | Reg loss: 0.029 | Tree loss: 5.555 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 23 | Batch: 022 / 025 | Total loss: 5.541 | Reg loss: 0.029 | Tree loss: 5.541 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 23 | Batch: 023 / 025 | Total loss: 5.509 | Reg loss: 0.029 | Tree loss: 5.509 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 23 | Batch: 024 / 025 | Total loss: 5.493 | Reg loss: 0.029 | Tree loss: 5.493 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 24 | Batch: 000 / 025 | Total loss: 5.910 | Reg loss: 0.027 | Tree loss: 5.910 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 24 | Batch: 001 / 025 | Total loss: 5.895 | Reg loss: 0.027 | Tree loss: 5.895 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 24 | Batch: 002 / 025 | Total loss: 5.865 | Reg loss: 0.027 | Tree loss: 5.865 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 24 | Batch: 003 / 025 | Total loss: 5.850 | Reg loss: 0.027 | Tree loss: 5.850 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 24 | Batch: 004 / 025 | Total loss: 5.813 | Reg loss: 0.027 | Tree loss: 5.813 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 24 | Batch: 005 / 025 | Total loss: 5.795 | Reg loss: 0.027 | Tree loss: 5.795 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 24 | Batch: 006 / 025 | Total loss: 5.774 | Reg loss: 0.027 | Tree loss: 5.774 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 24 | Batch: 007 / 025 | Total loss: 5.749 | Reg loss: 0.027 | Tree loss: 5.749 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 24 | Batch: 008 / 025 | Total loss: 5.723 | Reg loss: 0.027 | Tree loss: 5.723 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 24 | Batch: 009 / 025 | Total loss: 5.693 | Reg loss: 0.027 | Tree loss: 5.693 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 24 | Batch: 010 / 025 | Total loss: 5.677 | Reg loss: 0.027 | Tree loss: 5.677 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 24 | Batch: 011 / 025 | Total loss: 5.639 | Reg loss: 0.027 | Tree loss: 5.639 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 24 | Batch: 012 / 025 | Total loss: 5.619 | Reg loss: 0.027 | Tree loss: 5.619 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 24 | Batch: 013 / 025 | Total loss: 5.586 | Reg loss: 0.027 | Tree loss: 5.586 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 24 | Batch: 014 / 025 | Total loss: 5.570 | Reg loss: 0.028 | Tree loss: 5.570 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 24 | Batch: 015 / 025 | Total loss: 5.557 | Reg loss: 0.028 | Tree loss: 5.557 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 24 | Batch: 016 / 025 | Total loss: 5.525 | Reg loss: 0.028 | Tree loss: 5.525 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 24 | Batch: 017 / 025 | Total loss: 5.515 | Reg loss: 0.028 | Tree loss: 5.515 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 24 | Batch: 018 / 025 | Total loss: 5.476 | Reg loss: 0.028 | Tree loss: 5.476 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 24 | Batch: 019 / 025 | Total loss: 5.451 | Reg loss: 0.028 | Tree loss: 5.451 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 24 | Batch: 020 / 025 | Total loss: 5.408 | Reg loss: 0.028 | Tree loss: 5.408 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 24 | Batch: 021 / 025 | Total loss: 5.414 | Reg loss: 0.029 | Tree loss: 5.414 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 24 | Batch: 022 / 025 | Total loss: 5.361 | Reg loss: 0.029 | Tree loss: 5.361 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 24 | Batch: 023 / 025 | Total loss: 5.352 | Reg loss: 0.029 | Tree loss: 5.352 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 24 | Batch: 024 / 025 | Total loss: 5.344 | Reg loss: 0.029 | Tree loss: 5.344 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 25 | Batch: 000 / 025 | Total loss: 5.748 | Reg loss: 0.027 | Tree loss: 5.748 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 25 | Batch: 001 / 025 | Total loss: 5.741 | Reg loss: 0.027 | Tree loss: 5.741 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 25 | Batch: 002 / 025 | Total loss: 5.716 | Reg loss: 0.027 | Tree loss: 5.716 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 25 | Batch: 003 / 025 | Total loss: 5.678 | Reg loss: 0.027 | Tree loss: 5.678 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 25 | Batch: 004 / 025 | Total loss: 5.670 | Reg loss: 0.027 | Tree loss: 5.670 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 25 | Batch: 005 / 025 | Total loss: 5.635 | Reg loss: 0.027 | Tree loss: 5.635 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 25 | Batch: 006 / 025 | Total loss: 5.605 | Reg loss: 0.027 | Tree loss: 5.605 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 25 | Batch: 007 / 025 | Total loss: 5.601 | Reg loss: 0.027 | Tree loss: 5.601 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 25 | Batch: 008 / 025 | Total loss: 5.571 | Reg loss: 0.027 | Tree loss: 5.571 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 25 | Batch: 009 / 025 | Total loss: 5.534 | Reg loss: 0.027 | Tree loss: 5.534 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 25 | Batch: 010 / 025 | Total loss: 5.508 | Reg loss: 0.027 | Tree loss: 5.508 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 25 | Batch: 011 / 025 | Total loss: 5.478 | Reg loss: 0.027 | Tree loss: 5.478 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 25 | Batch: 012 / 025 | Total loss: 5.460 | Reg loss: 0.027 | Tree loss: 5.460 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 25 | Batch: 013 / 025 | Total loss: 5.437 | Reg loss: 0.028 | Tree loss: 5.437 | Accuracy: 1.000000 | 0.067 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25 | Batch: 014 / 025 | Total loss: 5.418 | Reg loss: 0.028 | Tree loss: 5.418 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 25 | Batch: 015 / 025 | Total loss: 5.394 | Reg loss: 0.028 | Tree loss: 5.394 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 25 | Batch: 016 / 025 | Total loss: 5.383 | Reg loss: 0.028 | Tree loss: 5.383 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 25 | Batch: 017 / 025 | Total loss: 5.339 | Reg loss: 0.028 | Tree loss: 5.339 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 25 | Batch: 018 / 025 | Total loss: 5.309 | Reg loss: 0.028 | Tree loss: 5.309 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 25 | Batch: 019 / 025 | Total loss: 5.279 | Reg loss: 0.028 | Tree loss: 5.279 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 25 | Batch: 020 / 025 | Total loss: 5.269 | Reg loss: 0.028 | Tree loss: 5.269 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 25 | Batch: 021 / 025 | Total loss: 5.247 | Reg loss: 0.029 | Tree loss: 5.247 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 25 | Batch: 022 / 025 | Total loss: 5.215 | Reg loss: 0.029 | Tree loss: 5.215 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 25 | Batch: 023 / 025 | Total loss: 5.209 | Reg loss: 0.029 | Tree loss: 5.209 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 25 | Batch: 024 / 025 | Total loss: 5.180 | Reg loss: 0.029 | Tree loss: 5.180 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 26 | Batch: 000 / 025 | Total loss: 5.592 | Reg loss: 0.027 | Tree loss: 5.592 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 26 | Batch: 001 / 025 | Total loss: 5.580 | Reg loss: 0.027 | Tree loss: 5.580 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 26 | Batch: 002 / 025 | Total loss: 5.549 | Reg loss: 0.027 | Tree loss: 5.549 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 26 | Batch: 003 / 025 | Total loss: 5.527 | Reg loss: 0.027 | Tree loss: 5.527 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 26 | Batch: 004 / 025 | Total loss: 5.490 | Reg loss: 0.027 | Tree loss: 5.490 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 26 | Batch: 005 / 025 | Total loss: 5.482 | Reg loss: 0.027 | Tree loss: 5.482 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 26 | Batch: 006 / 025 | Total loss: 5.455 | Reg loss: 0.027 | Tree loss: 5.455 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 26 | Batch: 007 / 025 | Total loss: 5.442 | Reg loss: 0.027 | Tree loss: 5.442 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 26 | Batch: 008 / 025 | Total loss: 5.404 | Reg loss: 0.027 | Tree loss: 5.404 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 26 | Batch: 009 / 025 | Total loss: 5.355 | Reg loss: 0.027 | Tree loss: 5.355 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 26 | Batch: 010 / 025 | Total loss: 5.366 | Reg loss: 0.027 | Tree loss: 5.366 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 26 | Batch: 011 / 025 | Total loss: 5.330 | Reg loss: 0.027 | Tree loss: 5.330 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 26 | Batch: 012 / 025 | Total loss: 5.297 | Reg loss: 0.028 | Tree loss: 5.297 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 26 | Batch: 013 / 025 | Total loss: 5.296 | Reg loss: 0.028 | Tree loss: 5.296 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 26 | Batch: 014 / 025 | Total loss: 5.268 | Reg loss: 0.028 | Tree loss: 5.268 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 26 | Batch: 015 / 025 | Total loss: 5.242 | Reg loss: 0.028 | Tree loss: 5.242 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 26 | Batch: 016 / 025 | Total loss: 5.223 | Reg loss: 0.028 | Tree loss: 5.223 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 26 | Batch: 017 / 025 | Total loss: 5.185 | Reg loss: 0.028 | Tree loss: 5.185 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 26 | Batch: 018 / 025 | Total loss: 5.169 | Reg loss: 0.028 | Tree loss: 5.169 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 26 | Batch: 019 / 025 | Total loss: 5.137 | Reg loss: 0.028 | Tree loss: 5.137 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 26 | Batch: 020 / 025 | Total loss: 5.114 | Reg loss: 0.028 | Tree loss: 5.114 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 26 | Batch: 021 / 025 | Total loss: 5.099 | Reg loss: 0.029 | Tree loss: 5.099 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 26 | Batch: 022 / 025 | Total loss: 5.065 | Reg loss: 0.029 | Tree loss: 5.065 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 26 | Batch: 023 / 025 | Total loss: 5.055 | Reg loss: 0.029 | Tree loss: 5.055 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 26 | Batch: 024 / 025 | Total loss: 5.012 | Reg loss: 0.029 | Tree loss: 5.012 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 27 | Batch: 000 / 025 | Total loss: 5.437 | Reg loss: 0.027 | Tree loss: 5.437 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 27 | Batch: 001 / 025 | Total loss: 5.414 | Reg loss: 0.027 | Tree loss: 5.414 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 27 | Batch: 002 / 025 | Total loss: 5.382 | Reg loss: 0.027 | Tree loss: 5.382 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 27 | Batch: 003 / 025 | Total loss: 5.381 | Reg loss: 0.027 | Tree loss: 5.381 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 27 | Batch: 004 / 025 | Total loss: 5.341 | Reg loss: 0.027 | Tree loss: 5.341 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 27 | Batch: 005 / 025 | Total loss: 5.343 | Reg loss: 0.027 | Tree loss: 5.343 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 27 | Batch: 006 / 025 | Total loss: 5.314 | Reg loss: 0.027 | Tree loss: 5.314 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 27 | Batch: 007 / 025 | Total loss: 5.289 | Reg loss: 0.027 | Tree loss: 5.289 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 27 | Batch: 008 / 025 | Total loss: 5.245 | Reg loss: 0.027 | Tree loss: 5.245 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 27 | Batch: 009 / 025 | Total loss: 5.232 | Reg loss: 0.027 | Tree loss: 5.232 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 27 | Batch: 010 / 025 | Total loss: 5.187 | Reg loss: 0.027 | Tree loss: 5.187 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 27 | Batch: 011 / 025 | Total loss: 5.169 | Reg loss: 0.028 | Tree loss: 5.169 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 27 | Batch: 012 / 025 | Total loss: 5.142 | Reg loss: 0.028 | Tree loss: 5.142 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 27 | Batch: 013 / 025 | Total loss: 5.122 | Reg loss: 0.028 | Tree loss: 5.122 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 27 | Batch: 014 / 025 | Total loss: 5.105 | Reg loss: 0.028 | Tree loss: 5.105 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 27 | Batch: 015 / 025 | Total loss: 5.088 | Reg loss: 0.028 | Tree loss: 5.088 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 27 | Batch: 016 / 025 | Total loss: 5.060 | Reg loss: 0.028 | Tree loss: 5.060 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 27 | Batch: 017 / 025 | Total loss: 5.039 | Reg loss: 0.028 | Tree loss: 5.039 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 27 | Batch: 018 / 025 | Total loss: 5.008 | Reg loss: 0.028 | Tree loss: 5.008 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 27 | Batch: 019 / 025 | Total loss: 4.984 | Reg loss: 0.028 | Tree loss: 4.984 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 27 | Batch: 020 / 025 | Total loss: 4.962 | Reg loss: 0.029 | Tree loss: 4.962 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 27 | Batch: 021 / 025 | Total loss: 4.967 | Reg loss: 0.029 | Tree loss: 4.967 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 27 | Batch: 022 / 025 | Total loss: 4.925 | Reg loss: 0.029 | Tree loss: 4.925 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 27 | Batch: 023 / 025 | Total loss: 4.903 | Reg loss: 0.029 | Tree loss: 4.903 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 27 | Batch: 024 / 025 | Total loss: 4.892 | Reg loss: 0.029 | Tree loss: 4.892 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 28 | Batch: 000 / 025 | Total loss: 5.299 | Reg loss: 0.027 | Tree loss: 5.299 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 28 | Batch: 001 / 025 | Total loss: 5.259 | Reg loss: 0.027 | Tree loss: 5.259 | Accuracy: 1.000000 | 0.067 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 28 | Batch: 002 / 025 | Total loss: 5.240 | Reg loss: 0.027 | Tree loss: 5.240 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 28 | Batch: 003 / 025 | Total loss: 5.228 | Reg loss: 0.027 | Tree loss: 5.228 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 28 | Batch: 004 / 025 | Total loss: 5.187 | Reg loss: 0.027 | Tree loss: 5.187 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 28 | Batch: 005 / 025 | Total loss: 5.173 | Reg loss: 0.027 | Tree loss: 5.173 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 28 | Batch: 006 / 025 | Total loss: 5.150 | Reg loss: 0.027 | Tree loss: 5.150 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 28 | Batch: 007 / 025 | Total loss: 5.108 | Reg loss: 0.027 | Tree loss: 5.108 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 28 | Batch: 008 / 025 | Total loss: 5.099 | Reg loss: 0.027 | Tree loss: 5.099 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 28 | Batch: 009 / 025 | Total loss: 5.069 | Reg loss: 0.027 | Tree loss: 5.069 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 28 | Batch: 010 / 025 | Total loss: 5.052 | Reg loss: 0.027 | Tree loss: 5.052 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 28 | Batch: 011 / 025 | Total loss: 5.041 | Reg loss: 0.028 | Tree loss: 5.041 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 28 | Batch: 012 / 025 | Total loss: 4.998 | Reg loss: 0.028 | Tree loss: 4.998 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 28 | Batch: 013 / 025 | Total loss: 4.983 | Reg loss: 0.028 | Tree loss: 4.983 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 28 | Batch: 014 / 025 | Total loss: 4.947 | Reg loss: 0.028 | Tree loss: 4.947 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 28 | Batch: 015 / 025 | Total loss: 4.953 | Reg loss: 0.028 | Tree loss: 4.953 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 28 | Batch: 016 / 025 | Total loss: 4.917 | Reg loss: 0.028 | Tree loss: 4.917 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 28 | Batch: 017 / 025 | Total loss: 4.881 | Reg loss: 0.028 | Tree loss: 4.881 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 28 | Batch: 018 / 025 | Total loss: 4.843 | Reg loss: 0.028 | Tree loss: 4.843 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 28 | Batch: 019 / 025 | Total loss: 4.840 | Reg loss: 0.028 | Tree loss: 4.840 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 28 | Batch: 020 / 025 | Total loss: 4.815 | Reg loss: 0.029 | Tree loss: 4.815 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 28 | Batch: 021 / 025 | Total loss: 4.808 | Reg loss: 0.029 | Tree loss: 4.808 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 28 | Batch: 022 / 025 | Total loss: 4.788 | Reg loss: 0.029 | Tree loss: 4.788 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 28 | Batch: 023 / 025 | Total loss: 4.754 | Reg loss: 0.029 | Tree loss: 4.754 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 28 | Batch: 024 / 025 | Total loss: 4.736 | Reg loss: 0.029 | Tree loss: 4.736 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 29 | Batch: 000 / 025 | Total loss: 5.150 | Reg loss: 0.027 | Tree loss: 5.150 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 29 | Batch: 001 / 025 | Total loss: 5.116 | Reg loss: 0.027 | Tree loss: 5.116 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 29 | Batch: 002 / 025 | Total loss: 5.091 | Reg loss: 0.027 | Tree loss: 5.091 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 29 | Batch: 003 / 025 | Total loss: 5.065 | Reg loss: 0.027 | Tree loss: 5.065 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 29 | Batch: 004 / 025 | Total loss: 5.049 | Reg loss: 0.027 | Tree loss: 5.049 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 29 | Batch: 005 / 025 | Total loss: 5.017 | Reg loss: 0.027 | Tree loss: 5.017 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 29 | Batch: 006 / 025 | Total loss: 4.993 | Reg loss: 0.027 | Tree loss: 4.993 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 29 | Batch: 007 / 025 | Total loss: 4.979 | Reg loss: 0.027 | Tree loss: 4.979 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 29 | Batch: 008 / 025 | Total loss: 4.948 | Reg loss: 0.027 | Tree loss: 4.948 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 29 | Batch: 009 / 025 | Total loss: 4.928 | Reg loss: 0.027 | Tree loss: 4.928 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 29 | Batch: 010 / 025 | Total loss: 4.901 | Reg loss: 0.028 | Tree loss: 4.901 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 29 | Batch: 011 / 025 | Total loss: 4.872 | Reg loss: 0.028 | Tree loss: 4.872 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 29 | Batch: 012 / 025 | Total loss: 4.862 | Reg loss: 0.028 | Tree loss: 4.862 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 29 | Batch: 013 / 025 | Total loss: 4.813 | Reg loss: 0.028 | Tree loss: 4.813 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 29 | Batch: 014 / 025 | Total loss: 4.793 | Reg loss: 0.028 | Tree loss: 4.793 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 29 | Batch: 015 / 025 | Total loss: 4.776 | Reg loss: 0.028 | Tree loss: 4.776 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 29 | Batch: 016 / 025 | Total loss: 4.773 | Reg loss: 0.028 | Tree loss: 4.773 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 29 | Batch: 017 / 025 | Total loss: 4.745 | Reg loss: 0.028 | Tree loss: 4.745 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 29 | Batch: 018 / 025 | Total loss: 4.727 | Reg loss: 0.028 | Tree loss: 4.727 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 29 | Batch: 019 / 025 | Total loss: 4.694 | Reg loss: 0.028 | Tree loss: 4.694 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 29 | Batch: 020 / 025 | Total loss: 4.678 | Reg loss: 0.029 | Tree loss: 4.678 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 29 | Batch: 021 / 025 | Total loss: 4.644 | Reg loss: 0.029 | Tree loss: 4.644 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 29 | Batch: 022 / 025 | Total loss: 4.649 | Reg loss: 0.029 | Tree loss: 4.649 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 29 | Batch: 023 / 025 | Total loss: 4.598 | Reg loss: 0.029 | Tree loss: 4.598 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 29 | Batch: 024 / 025 | Total loss: 4.588 | Reg loss: 0.029 | Tree loss: 4.588 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 30 | Batch: 000 / 025 | Total loss: 4.987 | Reg loss: 0.027 | Tree loss: 4.987 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 30 | Batch: 001 / 025 | Total loss: 4.982 | Reg loss: 0.027 | Tree loss: 4.982 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 30 | Batch: 002 / 025 | Total loss: 4.936 | Reg loss: 0.027 | Tree loss: 4.936 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 30 | Batch: 003 / 025 | Total loss: 4.931 | Reg loss: 0.027 | Tree loss: 4.931 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 30 | Batch: 004 / 025 | Total loss: 4.889 | Reg loss: 0.027 | Tree loss: 4.889 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 30 | Batch: 005 / 025 | Total loss: 4.876 | Reg loss: 0.027 | Tree loss: 4.876 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 30 | Batch: 006 / 025 | Total loss: 4.838 | Reg loss: 0.027 | Tree loss: 4.838 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 30 | Batch: 007 / 025 | Total loss: 4.815 | Reg loss: 0.027 | Tree loss: 4.815 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 30 | Batch: 008 / 025 | Total loss: 4.802 | Reg loss: 0.027 | Tree loss: 4.802 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 30 | Batch: 009 / 025 | Total loss: 4.765 | Reg loss: 0.028 | Tree loss: 4.765 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 30 | Batch: 010 / 025 | Total loss: 4.750 | Reg loss: 0.028 | Tree loss: 4.750 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 30 | Batch: 011 / 025 | Total loss: 4.736 | Reg loss: 0.028 | Tree loss: 4.736 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 30 | Batch: 012 / 025 | Total loss: 4.713 | Reg loss: 0.028 | Tree loss: 4.713 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 30 | Batch: 013 / 025 | Total loss: 4.682 | Reg loss: 0.028 | Tree loss: 4.682 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 30 | Batch: 014 / 025 | Total loss: 4.667 | Reg loss: 0.028 | Tree loss: 4.667 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 30 | Batch: 015 / 025 | Total loss: 4.633 | Reg loss: 0.028 | Tree loss: 4.633 | Accuracy: 1.000000 | 0.067 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30 | Batch: 016 / 025 | Total loss: 4.611 | Reg loss: 0.028 | Tree loss: 4.611 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 30 | Batch: 017 / 025 | Total loss: 4.600 | Reg loss: 0.028 | Tree loss: 4.600 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 30 | Batch: 018 / 025 | Total loss: 4.585 | Reg loss: 0.028 | Tree loss: 4.585 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 30 | Batch: 019 / 025 | Total loss: 4.552 | Reg loss: 0.028 | Tree loss: 4.552 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 30 | Batch: 020 / 025 | Total loss: 4.537 | Reg loss: 0.029 | Tree loss: 4.537 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 30 | Batch: 021 / 025 | Total loss: 4.503 | Reg loss: 0.029 | Tree loss: 4.503 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 30 | Batch: 022 / 025 | Total loss: 4.485 | Reg loss: 0.029 | Tree loss: 4.485 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 30 | Batch: 023 / 025 | Total loss: 4.463 | Reg loss: 0.029 | Tree loss: 4.463 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 30 | Batch: 024 / 025 | Total loss: 4.438 | Reg loss: 0.029 | Tree loss: 4.438 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 31 | Batch: 000 / 025 | Total loss: 4.843 | Reg loss: 0.027 | Tree loss: 4.843 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 31 | Batch: 001 / 025 | Total loss: 4.829 | Reg loss: 0.027 | Tree loss: 4.829 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 31 | Batch: 002 / 025 | Total loss: 4.797 | Reg loss: 0.027 | Tree loss: 4.797 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 31 | Batch: 003 / 025 | Total loss: 4.761 | Reg loss: 0.027 | Tree loss: 4.761 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 31 | Batch: 004 / 025 | Total loss: 4.758 | Reg loss: 0.027 | Tree loss: 4.758 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 31 | Batch: 005 / 025 | Total loss: 4.723 | Reg loss: 0.027 | Tree loss: 4.723 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 31 | Batch: 006 / 025 | Total loss: 4.704 | Reg loss: 0.027 | Tree loss: 4.704 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 31 | Batch: 007 / 025 | Total loss: 4.681 | Reg loss: 0.027 | Tree loss: 4.681 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 31 | Batch: 008 / 025 | Total loss: 4.652 | Reg loss: 0.028 | Tree loss: 4.652 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 31 | Batch: 009 / 025 | Total loss: 4.634 | Reg loss: 0.028 | Tree loss: 4.634 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 31 | Batch: 010 / 025 | Total loss: 4.607 | Reg loss: 0.028 | Tree loss: 4.607 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 31 | Batch: 011 / 025 | Total loss: 4.593 | Reg loss: 0.028 | Tree loss: 4.593 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 31 | Batch: 012 / 025 | Total loss: 4.573 | Reg loss: 0.028 | Tree loss: 4.573 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 31 | Batch: 013 / 025 | Total loss: 4.538 | Reg loss: 0.028 | Tree loss: 4.538 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 31 | Batch: 014 / 025 | Total loss: 4.509 | Reg loss: 0.028 | Tree loss: 4.509 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 31 | Batch: 015 / 025 | Total loss: 4.482 | Reg loss: 0.028 | Tree loss: 4.482 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 31 | Batch: 016 / 025 | Total loss: 4.473 | Reg loss: 0.028 | Tree loss: 4.473 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 31 | Batch: 017 / 025 | Total loss: 4.449 | Reg loss: 0.028 | Tree loss: 4.449 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 31 | Batch: 018 / 025 | Total loss: 4.442 | Reg loss: 0.028 | Tree loss: 4.442 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 31 | Batch: 019 / 025 | Total loss: 4.412 | Reg loss: 0.028 | Tree loss: 4.412 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 31 | Batch: 020 / 025 | Total loss: 4.379 | Reg loss: 0.029 | Tree loss: 4.379 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 31 | Batch: 021 / 025 | Total loss: 4.356 | Reg loss: 0.029 | Tree loss: 4.356 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 31 | Batch: 022 / 025 | Total loss: 4.352 | Reg loss: 0.029 | Tree loss: 4.352 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 31 | Batch: 023 / 025 | Total loss: 4.304 | Reg loss: 0.029 | Tree loss: 4.304 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 31 | Batch: 024 / 025 | Total loss: 4.314 | Reg loss: 0.029 | Tree loss: 4.314 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 32 | Batch: 000 / 025 | Total loss: 4.680 | Reg loss: 0.027 | Tree loss: 4.680 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 32 | Batch: 001 / 025 | Total loss: 4.671 | Reg loss: 0.027 | Tree loss: 4.671 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 32 | Batch: 002 / 025 | Total loss: 4.647 | Reg loss: 0.027 | Tree loss: 4.647 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 32 | Batch: 003 / 025 | Total loss: 4.622 | Reg loss: 0.027 | Tree loss: 4.622 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 32 | Batch: 004 / 025 | Total loss: 4.598 | Reg loss: 0.027 | Tree loss: 4.598 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 32 | Batch: 005 / 025 | Total loss: 4.589 | Reg loss: 0.027 | Tree loss: 4.589 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 32 | Batch: 006 / 025 | Total loss: 4.571 | Reg loss: 0.027 | Tree loss: 4.571 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 32 | Batch: 007 / 025 | Total loss: 4.529 | Reg loss: 0.028 | Tree loss: 4.529 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 32 | Batch: 008 / 025 | Total loss: 4.498 | Reg loss: 0.028 | Tree loss: 4.498 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 32 | Batch: 009 / 025 | Total loss: 4.493 | Reg loss: 0.028 | Tree loss: 4.493 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 32 | Batch: 010 / 025 | Total loss: 4.471 | Reg loss: 0.028 | Tree loss: 4.471 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 32 | Batch: 011 / 025 | Total loss: 4.449 | Reg loss: 0.028 | Tree loss: 4.449 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 32 | Batch: 012 / 025 | Total loss: 4.414 | Reg loss: 0.028 | Tree loss: 4.414 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 32 | Batch: 013 / 025 | Total loss: 4.386 | Reg loss: 0.028 | Tree loss: 4.386 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 32 | Batch: 014 / 025 | Total loss: 4.387 | Reg loss: 0.028 | Tree loss: 4.387 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 32 | Batch: 015 / 025 | Total loss: 4.359 | Reg loss: 0.028 | Tree loss: 4.359 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 32 | Batch: 016 / 025 | Total loss: 4.343 | Reg loss: 0.028 | Tree loss: 4.343 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 32 | Batch: 017 / 025 | Total loss: 4.311 | Reg loss: 0.028 | Tree loss: 4.311 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 32 | Batch: 018 / 025 | Total loss: 4.288 | Reg loss: 0.028 | Tree loss: 4.288 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 32 | Batch: 019 / 025 | Total loss: 4.274 | Reg loss: 0.028 | Tree loss: 4.274 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 32 | Batch: 020 / 025 | Total loss: 4.242 | Reg loss: 0.029 | Tree loss: 4.242 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 32 | Batch: 021 / 025 | Total loss: 4.214 | Reg loss: 0.029 | Tree loss: 4.214 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 32 | Batch: 022 / 025 | Total loss: 4.217 | Reg loss: 0.029 | Tree loss: 4.217 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 32 | Batch: 023 / 025 | Total loss: 4.169 | Reg loss: 0.029 | Tree loss: 4.169 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 32 | Batch: 024 / 025 | Total loss: 4.146 | Reg loss: 0.029 | Tree loss: 4.146 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 33 | Batch: 000 / 025 | Total loss: 4.554 | Reg loss: 0.027 | Tree loss: 4.554 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 33 | Batch: 001 / 025 | Total loss: 4.525 | Reg loss: 0.027 | Tree loss: 4.525 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 33 | Batch: 002 / 025 | Total loss: 4.505 | Reg loss: 0.027 | Tree loss: 4.505 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 33 | Batch: 003 / 025 | Total loss: 4.489 | Reg loss: 0.027 | Tree loss: 4.489 | Accuracy: 1.000000 | 0.067 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 33 | Batch: 004 / 025 | Total loss: 4.462 | Reg loss: 0.027 | Tree loss: 4.462 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 33 | Batch: 005 / 025 | Total loss: 4.424 | Reg loss: 0.027 | Tree loss: 4.424 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 33 | Batch: 006 / 025 | Total loss: 4.405 | Reg loss: 0.028 | Tree loss: 4.405 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 33 | Batch: 007 / 025 | Total loss: 4.407 | Reg loss: 0.028 | Tree loss: 4.407 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 33 | Batch: 008 / 025 | Total loss: 4.366 | Reg loss: 0.028 | Tree loss: 4.366 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 33 | Batch: 009 / 025 | Total loss: 4.345 | Reg loss: 0.028 | Tree loss: 4.345 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 33 | Batch: 010 / 025 | Total loss: 4.330 | Reg loss: 0.028 | Tree loss: 4.330 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 33 | Batch: 011 / 025 | Total loss: 4.303 | Reg loss: 0.028 | Tree loss: 4.303 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 33 | Batch: 012 / 025 | Total loss: 4.282 | Reg loss: 0.028 | Tree loss: 4.282 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 33 | Batch: 013 / 025 | Total loss: 4.247 | Reg loss: 0.028 | Tree loss: 4.247 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 33 | Batch: 014 / 025 | Total loss: 4.244 | Reg loss: 0.028 | Tree loss: 4.244 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 33 | Batch: 015 / 025 | Total loss: 4.206 | Reg loss: 0.028 | Tree loss: 4.206 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 33 | Batch: 016 / 025 | Total loss: 4.192 | Reg loss: 0.028 | Tree loss: 4.192 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 33 | Batch: 017 / 025 | Total loss: 4.170 | Reg loss: 0.028 | Tree loss: 4.170 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 33 | Batch: 018 / 025 | Total loss: 4.132 | Reg loss: 0.028 | Tree loss: 4.132 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 33 | Batch: 019 / 025 | Total loss: 4.122 | Reg loss: 0.028 | Tree loss: 4.122 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 33 | Batch: 020 / 025 | Total loss: 4.118 | Reg loss: 0.029 | Tree loss: 4.118 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 33 | Batch: 021 / 025 | Total loss: 4.073 | Reg loss: 0.029 | Tree loss: 4.073 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 33 | Batch: 022 / 025 | Total loss: 4.058 | Reg loss: 0.029 | Tree loss: 4.058 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 33 | Batch: 023 / 025 | Total loss: 4.064 | Reg loss: 0.029 | Tree loss: 4.064 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 33 | Batch: 024 / 025 | Total loss: 4.022 | Reg loss: 0.029 | Tree loss: 4.022 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 34 | Batch: 000 / 025 | Total loss: 4.413 | Reg loss: 0.027 | Tree loss: 4.413 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 34 | Batch: 001 / 025 | Total loss: 4.396 | Reg loss: 0.027 | Tree loss: 4.396 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 34 | Batch: 002 / 025 | Total loss: 4.364 | Reg loss: 0.027 | Tree loss: 4.364 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 34 | Batch: 003 / 025 | Total loss: 4.329 | Reg loss: 0.027 | Tree loss: 4.329 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 34 | Batch: 004 / 025 | Total loss: 4.324 | Reg loss: 0.027 | Tree loss: 4.324 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 34 | Batch: 005 / 025 | Total loss: 4.297 | Reg loss: 0.028 | Tree loss: 4.297 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 34 | Batch: 006 / 025 | Total loss: 4.270 | Reg loss: 0.028 | Tree loss: 4.270 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 34 | Batch: 007 / 025 | Total loss: 4.247 | Reg loss: 0.028 | Tree loss: 4.247 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 34 | Batch: 008 / 025 | Total loss: 4.216 | Reg loss: 0.028 | Tree loss: 4.216 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 34 | Batch: 009 / 025 | Total loss: 4.203 | Reg loss: 0.028 | Tree loss: 4.203 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 34 | Batch: 010 / 025 | Total loss: 4.193 | Reg loss: 0.028 | Tree loss: 4.193 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 34 | Batch: 011 / 025 | Total loss: 4.144 | Reg loss: 0.028 | Tree loss: 4.144 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 34 | Batch: 012 / 025 | Total loss: 4.138 | Reg loss: 0.028 | Tree loss: 4.138 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 34 | Batch: 013 / 025 | Total loss: 4.128 | Reg loss: 0.028 | Tree loss: 4.128 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 34 | Batch: 014 / 025 | Total loss: 4.086 | Reg loss: 0.028 | Tree loss: 4.086 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 34 | Batch: 015 / 025 | Total loss: 4.081 | Reg loss: 0.028 | Tree loss: 4.081 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 34 | Batch: 016 / 025 | Total loss: 4.065 | Reg loss: 0.028 | Tree loss: 4.065 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 34 | Batch: 017 / 025 | Total loss: 4.023 | Reg loss: 0.028 | Tree loss: 4.023 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 34 | Batch: 018 / 025 | Total loss: 4.009 | Reg loss: 0.028 | Tree loss: 4.009 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 34 | Batch: 019 / 025 | Total loss: 3.978 | Reg loss: 0.028 | Tree loss: 3.978 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 34 | Batch: 020 / 025 | Total loss: 3.970 | Reg loss: 0.029 | Tree loss: 3.970 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 34 | Batch: 021 / 025 | Total loss: 3.943 | Reg loss: 0.029 | Tree loss: 3.943 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 34 | Batch: 022 / 025 | Total loss: 3.949 | Reg loss: 0.029 | Tree loss: 3.949 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 34 | Batch: 023 / 025 | Total loss: 3.897 | Reg loss: 0.029 | Tree loss: 3.897 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 34 | Batch: 024 / 025 | Total loss: 3.888 | Reg loss: 0.029 | Tree loss: 3.888 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 35 | Batch: 000 / 025 | Total loss: 4.279 | Reg loss: 0.028 | Tree loss: 4.279 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 35 | Batch: 001 / 025 | Total loss: 4.236 | Reg loss: 0.028 | Tree loss: 4.236 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 35 | Batch: 002 / 025 | Total loss: 4.212 | Reg loss: 0.028 | Tree loss: 4.212 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 35 | Batch: 003 / 025 | Total loss: 4.207 | Reg loss: 0.028 | Tree loss: 4.207 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 35 | Batch: 004 / 025 | Total loss: 4.173 | Reg loss: 0.028 | Tree loss: 4.173 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 35 | Batch: 005 / 025 | Total loss: 4.165 | Reg loss: 0.028 | Tree loss: 4.165 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 35 | Batch: 006 / 025 | Total loss: 4.128 | Reg loss: 0.028 | Tree loss: 4.128 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 35 | Batch: 007 / 025 | Total loss: 4.110 | Reg loss: 0.028 | Tree loss: 4.110 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 35 | Batch: 008 / 025 | Total loss: 4.100 | Reg loss: 0.028 | Tree loss: 4.100 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 35 | Batch: 009 / 025 | Total loss: 4.054 | Reg loss: 0.028 | Tree loss: 4.054 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 35 | Batch: 010 / 025 | Total loss: 4.052 | Reg loss: 0.028 | Tree loss: 4.052 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 35 | Batch: 011 / 025 | Total loss: 4.026 | Reg loss: 0.028 | Tree loss: 4.026 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 35 | Batch: 012 / 025 | Total loss: 4.003 | Reg loss: 0.028 | Tree loss: 4.003 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 35 | Batch: 013 / 025 | Total loss: 3.981 | Reg loss: 0.028 | Tree loss: 3.981 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 35 | Batch: 014 / 025 | Total loss: 3.980 | Reg loss: 0.028 | Tree loss: 3.980 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 35 | Batch: 015 / 025 | Total loss: 3.924 | Reg loss: 0.028 | Tree loss: 3.924 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 35 | Batch: 016 / 025 | Total loss: 3.910 | Reg loss: 0.028 | Tree loss: 3.910 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 35 | Batch: 017 / 025 | Total loss: 3.892 | Reg loss: 0.028 | Tree loss: 3.892 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 35 | Batch: 018 / 025 | Total loss: 3.871 | Reg loss: 0.028 | Tree loss: 3.871 | Accuracy: 1.000000 | 0.067 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 35 | Batch: 019 / 025 | Total loss: 3.835 | Reg loss: 0.029 | Tree loss: 3.835 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 35 | Batch: 020 / 025 | Total loss: 3.822 | Reg loss: 0.029 | Tree loss: 3.822 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 35 | Batch: 021 / 025 | Total loss: 3.804 | Reg loss: 0.029 | Tree loss: 3.804 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 35 | Batch: 022 / 025 | Total loss: 3.811 | Reg loss: 0.029 | Tree loss: 3.811 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 35 | Batch: 023 / 025 | Total loss: 3.778 | Reg loss: 0.029 | Tree loss: 3.778 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 35 | Batch: 024 / 025 | Total loss: 3.752 | Reg loss: 0.029 | Tree loss: 3.752 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 36 | Batch: 000 / 025 | Total loss: 4.118 | Reg loss: 0.028 | Tree loss: 4.118 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 36 | Batch: 001 / 025 | Total loss: 4.096 | Reg loss: 0.028 | Tree loss: 4.096 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 36 | Batch: 002 / 025 | Total loss: 4.082 | Reg loss: 0.028 | Tree loss: 4.082 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 36 | Batch: 003 / 025 | Total loss: 4.056 | Reg loss: 0.028 | Tree loss: 4.056 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 36 | Batch: 004 / 025 | Total loss: 4.055 | Reg loss: 0.028 | Tree loss: 4.055 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 36 | Batch: 005 / 025 | Total loss: 4.039 | Reg loss: 0.028 | Tree loss: 4.039 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 36 | Batch: 006 / 025 | Total loss: 4.001 | Reg loss: 0.028 | Tree loss: 4.001 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 36 | Batch: 007 / 025 | Total loss: 3.980 | Reg loss: 0.028 | Tree loss: 3.980 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 36 | Batch: 008 / 025 | Total loss: 3.966 | Reg loss: 0.028 | Tree loss: 3.966 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 36 | Batch: 009 / 025 | Total loss: 3.942 | Reg loss: 0.028 | Tree loss: 3.942 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 36 | Batch: 010 / 025 | Total loss: 3.916 | Reg loss: 0.028 | Tree loss: 3.916 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 36 | Batch: 011 / 025 | Total loss: 3.879 | Reg loss: 0.028 | Tree loss: 3.879 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 36 | Batch: 012 / 025 | Total loss: 3.846 | Reg loss: 0.028 | Tree loss: 3.846 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 36 | Batch: 013 / 025 | Total loss: 3.844 | Reg loss: 0.028 | Tree loss: 3.844 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 36 | Batch: 014 / 025 | Total loss: 3.809 | Reg loss: 0.028 | Tree loss: 3.809 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 36 | Batch: 015 / 025 | Total loss: 3.800 | Reg loss: 0.028 | Tree loss: 3.800 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 36 | Batch: 016 / 025 | Total loss: 3.777 | Reg loss: 0.028 | Tree loss: 3.777 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 36 | Batch: 017 / 025 | Total loss: 3.752 | Reg loss: 0.028 | Tree loss: 3.752 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 36 | Batch: 018 / 025 | Total loss: 3.739 | Reg loss: 0.028 | Tree loss: 3.739 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 36 | Batch: 019 / 025 | Total loss: 3.712 | Reg loss: 0.029 | Tree loss: 3.712 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 36 | Batch: 020 / 025 | Total loss: 3.687 | Reg loss: 0.029 | Tree loss: 3.687 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 36 | Batch: 021 / 025 | Total loss: 3.681 | Reg loss: 0.029 | Tree loss: 3.681 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 36 | Batch: 022 / 025 | Total loss: 3.657 | Reg loss: 0.029 | Tree loss: 3.657 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 36 | Batch: 023 / 025 | Total loss: 3.641 | Reg loss: 0.029 | Tree loss: 3.641 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 36 | Batch: 024 / 025 | Total loss: 3.636 | Reg loss: 0.029 | Tree loss: 3.636 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 37 | Batch: 000 / 025 | Total loss: 3.979 | Reg loss: 0.028 | Tree loss: 3.979 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 37 | Batch: 001 / 025 | Total loss: 3.974 | Reg loss: 0.028 | Tree loss: 3.974 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 37 | Batch: 002 / 025 | Total loss: 3.937 | Reg loss: 0.028 | Tree loss: 3.937 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 37 | Batch: 003 / 025 | Total loss: 3.934 | Reg loss: 0.028 | Tree loss: 3.934 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 37 | Batch: 004 / 025 | Total loss: 3.913 | Reg loss: 0.028 | Tree loss: 3.913 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 37 | Batch: 005 / 025 | Total loss: 3.876 | Reg loss: 0.028 | Tree loss: 3.876 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 37 | Batch: 006 / 025 | Total loss: 3.860 | Reg loss: 0.028 | Tree loss: 3.860 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 37 | Batch: 007 / 025 | Total loss: 3.840 | Reg loss: 0.028 | Tree loss: 3.840 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 37 | Batch: 008 / 025 | Total loss: 3.803 | Reg loss: 0.028 | Tree loss: 3.803 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 37 | Batch: 009 / 025 | Total loss: 3.805 | Reg loss: 0.028 | Tree loss: 3.805 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 37 | Batch: 010 / 025 | Total loss: 3.779 | Reg loss: 0.028 | Tree loss: 3.779 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 37 | Batch: 011 / 025 | Total loss: 3.747 | Reg loss: 0.028 | Tree loss: 3.747 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 37 | Batch: 012 / 025 | Total loss: 3.737 | Reg loss: 0.028 | Tree loss: 3.737 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 37 | Batch: 013 / 025 | Total loss: 3.708 | Reg loss: 0.028 | Tree loss: 3.708 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 37 | Batch: 014 / 025 | Total loss: 3.683 | Reg loss: 0.028 | Tree loss: 3.683 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 37 | Batch: 015 / 025 | Total loss: 3.665 | Reg loss: 0.028 | Tree loss: 3.665 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 37 | Batch: 016 / 025 | Total loss: 3.653 | Reg loss: 0.028 | Tree loss: 3.653 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 37 | Batch: 017 / 025 | Total loss: 3.623 | Reg loss: 0.028 | Tree loss: 3.623 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 37 | Batch: 018 / 025 | Total loss: 3.607 | Reg loss: 0.029 | Tree loss: 3.607 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 37 | Batch: 019 / 025 | Total loss: 3.585 | Reg loss: 0.029 | Tree loss: 3.585 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 37 | Batch: 020 / 025 | Total loss: 3.561 | Reg loss: 0.029 | Tree loss: 3.561 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 37 | Batch: 021 / 025 | Total loss: 3.558 | Reg loss: 0.029 | Tree loss: 3.558 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 37 | Batch: 022 / 025 | Total loss: 3.514 | Reg loss: 0.029 | Tree loss: 3.514 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 37 | Batch: 023 / 025 | Total loss: 3.502 | Reg loss: 0.029 | Tree loss: 3.502 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 37 | Batch: 024 / 025 | Total loss: 3.504 | Reg loss: 0.029 | Tree loss: 3.504 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 38 | Batch: 000 / 025 | Total loss: 3.851 | Reg loss: 0.028 | Tree loss: 3.851 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 38 | Batch: 001 / 025 | Total loss: 3.830 | Reg loss: 0.028 | Tree loss: 3.830 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 38 | Batch: 002 / 025 | Total loss: 3.799 | Reg loss: 0.028 | Tree loss: 3.799 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 38 | Batch: 003 / 025 | Total loss: 3.780 | Reg loss: 0.028 | Tree loss: 3.780 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 38 | Batch: 004 / 025 | Total loss: 3.769 | Reg loss: 0.028 | Tree loss: 3.769 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 38 | Batch: 005 / 025 | Total loss: 3.747 | Reg loss: 0.028 | Tree loss: 3.747 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 38 | Batch: 006 / 025 | Total loss: 3.729 | Reg loss: 0.028 | Tree loss: 3.729 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 38 | Batch: 007 / 025 | Total loss: 3.696 | Reg loss: 0.028 | Tree loss: 3.696 | Accuracy: 1.000000 | 0.067 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 38 | Batch: 008 / 025 | Total loss: 3.679 | Reg loss: 0.028 | Tree loss: 3.679 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 38 | Batch: 009 / 025 | Total loss: 3.672 | Reg loss: 0.028 | Tree loss: 3.672 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 38 | Batch: 010 / 025 | Total loss: 3.642 | Reg loss: 0.028 | Tree loss: 3.642 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 38 | Batch: 011 / 025 | Total loss: 3.617 | Reg loss: 0.028 | Tree loss: 3.617 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 38 | Batch: 012 / 025 | Total loss: 3.599 | Reg loss: 0.028 | Tree loss: 3.599 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 38 | Batch: 013 / 025 | Total loss: 3.581 | Reg loss: 0.028 | Tree loss: 3.581 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 38 | Batch: 014 / 025 | Total loss: 3.547 | Reg loss: 0.028 | Tree loss: 3.547 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 38 | Batch: 015 / 025 | Total loss: 3.534 | Reg loss: 0.028 | Tree loss: 3.534 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 38 | Batch: 016 / 025 | Total loss: 3.509 | Reg loss: 0.028 | Tree loss: 3.509 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 38 | Batch: 017 / 025 | Total loss: 3.501 | Reg loss: 0.028 | Tree loss: 3.501 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 38 | Batch: 018 / 025 | Total loss: 3.487 | Reg loss: 0.029 | Tree loss: 3.487 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 38 | Batch: 019 / 025 | Total loss: 3.444 | Reg loss: 0.029 | Tree loss: 3.444 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 38 | Batch: 020 / 025 | Total loss: 3.433 | Reg loss: 0.029 | Tree loss: 3.433 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 38 | Batch: 021 / 025 | Total loss: 3.433 | Reg loss: 0.029 | Tree loss: 3.433 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 38 | Batch: 022 / 025 | Total loss: 3.390 | Reg loss: 0.029 | Tree loss: 3.390 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 38 | Batch: 023 / 025 | Total loss: 3.374 | Reg loss: 0.029 | Tree loss: 3.374 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 38 | Batch: 024 / 025 | Total loss: 3.366 | Reg loss: 0.029 | Tree loss: 3.366 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 39 | Batch: 000 / 025 | Total loss: 3.705 | Reg loss: 0.028 | Tree loss: 3.705 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 39 | Batch: 001 / 025 | Total loss: 3.687 | Reg loss: 0.028 | Tree loss: 3.687 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 39 | Batch: 002 / 025 | Total loss: 3.684 | Reg loss: 0.028 | Tree loss: 3.684 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 39 | Batch: 003 / 025 | Total loss: 3.645 | Reg loss: 0.028 | Tree loss: 3.645 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 39 | Batch: 004 / 025 | Total loss: 3.638 | Reg loss: 0.028 | Tree loss: 3.638 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 39 | Batch: 005 / 025 | Total loss: 3.599 | Reg loss: 0.028 | Tree loss: 3.599 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 39 | Batch: 006 / 025 | Total loss: 3.604 | Reg loss: 0.028 | Tree loss: 3.604 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 39 | Batch: 007 / 025 | Total loss: 3.564 | Reg loss: 0.028 | Tree loss: 3.564 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 39 | Batch: 008 / 025 | Total loss: 3.579 | Reg loss: 0.028 | Tree loss: 3.579 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 39 | Batch: 009 / 025 | Total loss: 3.534 | Reg loss: 0.028 | Tree loss: 3.534 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 39 | Batch: 010 / 025 | Total loss: 3.510 | Reg loss: 0.028 | Tree loss: 3.510 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 39 | Batch: 011 / 025 | Total loss: 3.471 | Reg loss: 0.028 | Tree loss: 3.471 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 39 | Batch: 012 / 025 | Total loss: 3.455 | Reg loss: 0.028 | Tree loss: 3.455 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 39 | Batch: 013 / 025 | Total loss: 3.433 | Reg loss: 0.028 | Tree loss: 3.433 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 39 | Batch: 014 / 025 | Total loss: 3.433 | Reg loss: 0.028 | Tree loss: 3.433 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 39 | Batch: 015 / 025 | Total loss: 3.388 | Reg loss: 0.028 | Tree loss: 3.388 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 39 | Batch: 016 / 025 | Total loss: 3.387 | Reg loss: 0.028 | Tree loss: 3.387 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 39 | Batch: 017 / 025 | Total loss: 3.362 | Reg loss: 0.028 | Tree loss: 3.362 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 39 | Batch: 018 / 025 | Total loss: 3.359 | Reg loss: 0.029 | Tree loss: 3.359 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 39 | Batch: 019 / 025 | Total loss: 3.317 | Reg loss: 0.029 | Tree loss: 3.317 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 39 | Batch: 020 / 025 | Total loss: 3.305 | Reg loss: 0.029 | Tree loss: 3.305 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 39 | Batch: 021 / 025 | Total loss: 3.290 | Reg loss: 0.029 | Tree loss: 3.290 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 39 | Batch: 022 / 025 | Total loss: 3.273 | Reg loss: 0.029 | Tree loss: 3.273 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 39 | Batch: 023 / 025 | Total loss: 3.261 | Reg loss: 0.029 | Tree loss: 3.261 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 39 | Batch: 024 / 025 | Total loss: 3.245 | Reg loss: 0.029 | Tree loss: 3.245 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 40 | Batch: 000 / 025 | Total loss: 3.587 | Reg loss: 0.028 | Tree loss: 3.587 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 40 | Batch: 001 / 025 | Total loss: 3.557 | Reg loss: 0.028 | Tree loss: 3.557 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 40 | Batch: 002 / 025 | Total loss: 3.541 | Reg loss: 0.028 | Tree loss: 3.541 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 40 | Batch: 003 / 025 | Total loss: 3.522 | Reg loss: 0.028 | Tree loss: 3.522 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 40 | Batch: 004 / 025 | Total loss: 3.506 | Reg loss: 0.028 | Tree loss: 3.506 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 40 | Batch: 005 / 025 | Total loss: 3.475 | Reg loss: 0.028 | Tree loss: 3.475 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 40 | Batch: 006 / 025 | Total loss: 3.472 | Reg loss: 0.028 | Tree loss: 3.472 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 40 | Batch: 007 / 025 | Total loss: 3.453 | Reg loss: 0.028 | Tree loss: 3.453 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 40 | Batch: 008 / 025 | Total loss: 3.407 | Reg loss: 0.028 | Tree loss: 3.407 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 40 | Batch: 009 / 025 | Total loss: 3.409 | Reg loss: 0.028 | Tree loss: 3.409 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 40 | Batch: 010 / 025 | Total loss: 3.374 | Reg loss: 0.028 | Tree loss: 3.374 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 40 | Batch: 011 / 025 | Total loss: 3.349 | Reg loss: 0.028 | Tree loss: 3.349 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 40 | Batch: 012 / 025 | Total loss: 3.343 | Reg loss: 0.028 | Tree loss: 3.343 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 40 | Batch: 013 / 025 | Total loss: 3.314 | Reg loss: 0.028 | Tree loss: 3.314 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 40 | Batch: 014 / 025 | Total loss: 3.296 | Reg loss: 0.028 | Tree loss: 3.296 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 40 | Batch: 015 / 025 | Total loss: 3.267 | Reg loss: 0.028 | Tree loss: 3.267 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 40 | Batch: 016 / 025 | Total loss: 3.269 | Reg loss: 0.028 | Tree loss: 3.269 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 40 | Batch: 017 / 025 | Total loss: 3.234 | Reg loss: 0.029 | Tree loss: 3.234 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 40 | Batch: 018 / 025 | Total loss: 3.206 | Reg loss: 0.029 | Tree loss: 3.206 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 40 | Batch: 019 / 025 | Total loss: 3.207 | Reg loss: 0.029 | Tree loss: 3.207 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 40 | Batch: 020 / 025 | Total loss: 3.192 | Reg loss: 0.029 | Tree loss: 3.192 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 40 | Batch: 021 / 025 | Total loss: 3.158 | Reg loss: 0.029 | Tree loss: 3.158 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 40 | Batch: 022 / 025 | Total loss: 3.140 | Reg loss: 0.029 | Tree loss: 3.140 | Accuracy: 1.000000 | 0.067 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 40 | Batch: 023 / 025 | Total loss: 3.109 | Reg loss: 0.029 | Tree loss: 3.109 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 40 | Batch: 024 / 025 | Total loss: 3.100 | Reg loss: 0.029 | Tree loss: 3.100 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 41 | Batch: 000 / 025 | Total loss: 3.457 | Reg loss: 0.028 | Tree loss: 3.457 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 41 | Batch: 001 / 025 | Total loss: 3.427 | Reg loss: 0.028 | Tree loss: 3.427 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 41 | Batch: 002 / 025 | Total loss: 3.410 | Reg loss: 0.028 | Tree loss: 3.410 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 41 | Batch: 003 / 025 | Total loss: 3.396 | Reg loss: 0.028 | Tree loss: 3.396 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 41 | Batch: 004 / 025 | Total loss: 3.365 | Reg loss: 0.028 | Tree loss: 3.365 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 41 | Batch: 005 / 025 | Total loss: 3.355 | Reg loss: 0.028 | Tree loss: 3.355 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 41 | Batch: 006 / 025 | Total loss: 3.324 | Reg loss: 0.028 | Tree loss: 3.324 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 41 | Batch: 007 / 025 | Total loss: 3.314 | Reg loss: 0.028 | Tree loss: 3.314 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 41 | Batch: 008 / 025 | Total loss: 3.287 | Reg loss: 0.028 | Tree loss: 3.287 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 41 | Batch: 009 / 025 | Total loss: 3.268 | Reg loss: 0.028 | Tree loss: 3.268 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 41 | Batch: 010 / 025 | Total loss: 3.239 | Reg loss: 0.028 | Tree loss: 3.239 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 41 | Batch: 011 / 025 | Total loss: 3.236 | Reg loss: 0.028 | Tree loss: 3.236 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 41 | Batch: 012 / 025 | Total loss: 3.197 | Reg loss: 0.028 | Tree loss: 3.197 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 41 | Batch: 013 / 025 | Total loss: 3.199 | Reg loss: 0.028 | Tree loss: 3.199 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 41 | Batch: 014 / 025 | Total loss: 3.187 | Reg loss: 0.028 | Tree loss: 3.187 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 41 | Batch: 015 / 025 | Total loss: 3.152 | Reg loss: 0.028 | Tree loss: 3.152 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 41 | Batch: 016 / 025 | Total loss: 3.137 | Reg loss: 0.028 | Tree loss: 3.137 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 41 | Batch: 017 / 025 | Total loss: 3.120 | Reg loss: 0.029 | Tree loss: 3.120 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 41 | Batch: 018 / 025 | Total loss: 3.084 | Reg loss: 0.029 | Tree loss: 3.084 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 41 | Batch: 019 / 025 | Total loss: 3.074 | Reg loss: 0.029 | Tree loss: 3.074 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 41 | Batch: 020 / 025 | Total loss: 3.041 | Reg loss: 0.029 | Tree loss: 3.041 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 41 | Batch: 021 / 025 | Total loss: 3.022 | Reg loss: 0.029 | Tree loss: 3.022 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 41 | Batch: 022 / 025 | Total loss: 3.027 | Reg loss: 0.029 | Tree loss: 3.027 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 41 | Batch: 023 / 025 | Total loss: 3.008 | Reg loss: 0.029 | Tree loss: 3.008 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 41 | Batch: 024 / 025 | Total loss: 2.960 | Reg loss: 0.029 | Tree loss: 2.960 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 42 | Batch: 000 / 025 | Total loss: 3.328 | Reg loss: 0.028 | Tree loss: 3.328 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 42 | Batch: 001 / 025 | Total loss: 3.315 | Reg loss: 0.028 | Tree loss: 3.315 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 42 | Batch: 002 / 025 | Total loss: 3.290 | Reg loss: 0.028 | Tree loss: 3.290 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 42 | Batch: 003 / 025 | Total loss: 3.268 | Reg loss: 0.028 | Tree loss: 3.268 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 42 | Batch: 004 / 025 | Total loss: 3.224 | Reg loss: 0.028 | Tree loss: 3.224 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 42 | Batch: 005 / 025 | Total loss: 3.196 | Reg loss: 0.028 | Tree loss: 3.196 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 42 | Batch: 006 / 025 | Total loss: 3.203 | Reg loss: 0.028 | Tree loss: 3.203 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 42 | Batch: 007 / 025 | Total loss: 3.177 | Reg loss: 0.028 | Tree loss: 3.177 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 42 | Batch: 008 / 025 | Total loss: 3.160 | Reg loss: 0.028 | Tree loss: 3.160 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 42 | Batch: 009 / 025 | Total loss: 3.134 | Reg loss: 0.028 | Tree loss: 3.134 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 42 | Batch: 010 / 025 | Total loss: 3.132 | Reg loss: 0.028 | Tree loss: 3.132 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 42 | Batch: 011 / 025 | Total loss: 3.096 | Reg loss: 0.028 | Tree loss: 3.096 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 42 | Batch: 012 / 025 | Total loss: 3.083 | Reg loss: 0.028 | Tree loss: 3.083 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 42 | Batch: 013 / 025 | Total loss: 3.073 | Reg loss: 0.028 | Tree loss: 3.073 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 42 | Batch: 014 / 025 | Total loss: 3.051 | Reg loss: 0.028 | Tree loss: 3.051 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 42 | Batch: 015 / 025 | Total loss: 3.028 | Reg loss: 0.028 | Tree loss: 3.028 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 42 | Batch: 016 / 025 | Total loss: 2.997 | Reg loss: 0.029 | Tree loss: 2.997 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 42 | Batch: 017 / 025 | Total loss: 2.993 | Reg loss: 0.029 | Tree loss: 2.993 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 42 | Batch: 018 / 025 | Total loss: 2.961 | Reg loss: 0.029 | Tree loss: 2.961 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 42 | Batch: 019 / 025 | Total loss: 2.944 | Reg loss: 0.029 | Tree loss: 2.944 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 42 | Batch: 020 / 025 | Total loss: 2.930 | Reg loss: 0.029 | Tree loss: 2.930 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 42 | Batch: 021 / 025 | Total loss: 2.914 | Reg loss: 0.029 | Tree loss: 2.914 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 42 | Batch: 022 / 025 | Total loss: 2.901 | Reg loss: 0.029 | Tree loss: 2.901 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 42 | Batch: 023 / 025 | Total loss: 2.877 | Reg loss: 0.029 | Tree loss: 2.877 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 42 | Batch: 024 / 025 | Total loss: 2.862 | Reg loss: 0.029 | Tree loss: 2.862 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 43 | Batch: 000 / 025 | Total loss: 3.189 | Reg loss: 0.028 | Tree loss: 3.189 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 43 | Batch: 001 / 025 | Total loss: 3.167 | Reg loss: 0.028 | Tree loss: 3.167 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 43 | Batch: 002 / 025 | Total loss: 3.155 | Reg loss: 0.028 | Tree loss: 3.155 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 43 | Batch: 003 / 025 | Total loss: 3.138 | Reg loss: 0.028 | Tree loss: 3.138 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 43 | Batch: 004 / 025 | Total loss: 3.104 | Reg loss: 0.028 | Tree loss: 3.104 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 43 | Batch: 005 / 025 | Total loss: 3.108 | Reg loss: 0.028 | Tree loss: 3.108 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 43 | Batch: 006 / 025 | Total loss: 3.086 | Reg loss: 0.028 | Tree loss: 3.086 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 43 | Batch: 007 / 025 | Total loss: 3.066 | Reg loss: 0.028 | Tree loss: 3.066 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 43 | Batch: 008 / 025 | Total loss: 3.040 | Reg loss: 0.028 | Tree loss: 3.040 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 43 | Batch: 009 / 025 | Total loss: 3.030 | Reg loss: 0.028 | Tree loss: 3.030 | Accuracy: 1.000000 | 0.067 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 43 | Batch: 010 / 025 | Total loss: 3.004 | Reg loss: 0.028 | Tree loss: 3.004 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 43 | Batch: 011 / 025 | Total loss: 2.981 | Reg loss: 0.028 | Tree loss: 2.981 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 43 | Batch: 012 / 025 | Total loss: 2.942 | Reg loss: 0.028 | Tree loss: 2.942 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 43 | Batch: 013 / 025 | Total loss: 2.951 | Reg loss: 0.028 | Tree loss: 2.951 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 43 | Batch: 014 / 025 | Total loss: 2.924 | Reg loss: 0.028 | Tree loss: 2.924 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 43 | Batch: 015 / 025 | Total loss: 2.900 | Reg loss: 0.028 | Tree loss: 2.900 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 43 | Batch: 016 / 025 | Total loss: 2.873 | Reg loss: 0.029 | Tree loss: 2.873 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 43 | Batch: 017 / 025 | Total loss: 2.866 | Reg loss: 0.029 | Tree loss: 2.866 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 43 | Batch: 018 / 025 | Total loss: 2.839 | Reg loss: 0.029 | Tree loss: 2.839 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 43 | Batch: 019 / 025 | Total loss: 2.821 | Reg loss: 0.029 | Tree loss: 2.821 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 43 | Batch: 020 / 025 | Total loss: 2.813 | Reg loss: 0.029 | Tree loss: 2.813 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 43 | Batch: 021 / 025 | Total loss: 2.795 | Reg loss: 0.029 | Tree loss: 2.795 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 43 | Batch: 022 / 025 | Total loss: 2.760 | Reg loss: 0.029 | Tree loss: 2.760 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 43 | Batch: 023 / 025 | Total loss: 2.768 | Reg loss: 0.029 | Tree loss: 2.768 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 43 | Batch: 024 / 025 | Total loss: 2.724 | Reg loss: 0.029 | Tree loss: 2.724 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 44 | Batch: 000 / 025 | Total loss: 3.077 | Reg loss: 0.028 | Tree loss: 3.077 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 44 | Batch: 001 / 025 | Total loss: 3.055 | Reg loss: 0.028 | Tree loss: 3.055 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 44 | Batch: 002 / 025 | Total loss: 3.035 | Reg loss: 0.028 | Tree loss: 3.035 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 44 | Batch: 003 / 025 | Total loss: 3.022 | Reg loss: 0.028 | Tree loss: 3.022 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 44 | Batch: 004 / 025 | Total loss: 2.992 | Reg loss: 0.028 | Tree loss: 2.992 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 44 | Batch: 005 / 025 | Total loss: 2.953 | Reg loss: 0.028 | Tree loss: 2.953 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 44 | Batch: 006 / 025 | Total loss: 2.959 | Reg loss: 0.028 | Tree loss: 2.959 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 44 | Batch: 007 / 025 | Total loss: 2.933 | Reg loss: 0.028 | Tree loss: 2.933 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 44 | Batch: 008 / 025 | Total loss: 2.926 | Reg loss: 0.028 | Tree loss: 2.926 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 44 | Batch: 009 / 025 | Total loss: 2.891 | Reg loss: 0.028 | Tree loss: 2.891 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 44 | Batch: 010 / 025 | Total loss: 2.876 | Reg loss: 0.028 | Tree loss: 2.876 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 44 | Batch: 011 / 025 | Total loss: 2.849 | Reg loss: 0.028 | Tree loss: 2.849 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 44 | Batch: 012 / 025 | Total loss: 2.826 | Reg loss: 0.028 | Tree loss: 2.826 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 44 | Batch: 013 / 025 | Total loss: 2.816 | Reg loss: 0.028 | Tree loss: 2.816 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 44 | Batch: 014 / 025 | Total loss: 2.786 | Reg loss: 0.028 | Tree loss: 2.786 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 44 | Batch: 015 / 025 | Total loss: 2.769 | Reg loss: 0.028 | Tree loss: 2.769 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 44 | Batch: 016 / 025 | Total loss: 2.761 | Reg loss: 0.029 | Tree loss: 2.761 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 44 | Batch: 017 / 025 | Total loss: 2.751 | Reg loss: 0.029 | Tree loss: 2.751 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 44 | Batch: 018 / 025 | Total loss: 2.733 | Reg loss: 0.029 | Tree loss: 2.733 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 44 | Batch: 019 / 025 | Total loss: 2.705 | Reg loss: 0.029 | Tree loss: 2.705 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 44 | Batch: 020 / 025 | Total loss: 2.677 | Reg loss: 0.029 | Tree loss: 2.677 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 44 | Batch: 021 / 025 | Total loss: 2.687 | Reg loss: 0.029 | Tree loss: 2.687 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 44 | Batch: 022 / 025 | Total loss: 2.656 | Reg loss: 0.029 | Tree loss: 2.656 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 44 | Batch: 023 / 025 | Total loss: 2.647 | Reg loss: 0.029 | Tree loss: 2.647 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 44 | Batch: 024 / 025 | Total loss: 2.625 | Reg loss: 0.029 | Tree loss: 2.625 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 45 | Batch: 000 / 025 | Total loss: 2.967 | Reg loss: 0.028 | Tree loss: 2.967 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 45 | Batch: 001 / 025 | Total loss: 2.938 | Reg loss: 0.028 | Tree loss: 2.938 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 45 | Batch: 002 / 025 | Total loss: 2.907 | Reg loss: 0.028 | Tree loss: 2.907 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 45 | Batch: 003 / 025 | Total loss: 2.904 | Reg loss: 0.028 | Tree loss: 2.904 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 45 | Batch: 004 / 025 | Total loss: 2.868 | Reg loss: 0.028 | Tree loss: 2.868 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 45 | Batch: 005 / 025 | Total loss: 2.857 | Reg loss: 0.028 | Tree loss: 2.857 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 45 | Batch: 006 / 025 | Total loss: 2.833 | Reg loss: 0.028 | Tree loss: 2.833 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 45 | Batch: 007 / 025 | Total loss: 2.826 | Reg loss: 0.028 | Tree loss: 2.826 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 45 | Batch: 008 / 025 | Total loss: 2.794 | Reg loss: 0.028 | Tree loss: 2.794 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 45 | Batch: 009 / 025 | Total loss: 2.769 | Reg loss: 0.028 | Tree loss: 2.769 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 45 | Batch: 010 / 025 | Total loss: 2.749 | Reg loss: 0.028 | Tree loss: 2.749 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 45 | Batch: 011 / 025 | Total loss: 2.733 | Reg loss: 0.028 | Tree loss: 2.733 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 45 | Batch: 012 / 025 | Total loss: 2.720 | Reg loss: 0.028 | Tree loss: 2.720 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 45 | Batch: 013 / 025 | Total loss: 2.702 | Reg loss: 0.028 | Tree loss: 2.702 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 45 | Batch: 014 / 025 | Total loss: 2.682 | Reg loss: 0.028 | Tree loss: 2.682 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 45 | Batch: 015 / 025 | Total loss: 2.657 | Reg loss: 0.028 | Tree loss: 2.657 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 45 | Batch: 016 / 025 | Total loss: 2.636 | Reg loss: 0.029 | Tree loss: 2.636 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 45 | Batch: 017 / 025 | Total loss: 2.626 | Reg loss: 0.029 | Tree loss: 2.626 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 45 | Batch: 018 / 025 | Total loss: 2.607 | Reg loss: 0.029 | Tree loss: 2.607 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 45 | Batch: 019 / 025 | Total loss: 2.581 | Reg loss: 0.029 | Tree loss: 2.581 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 45 | Batch: 020 / 025 | Total loss: 2.560 | Reg loss: 0.029 | Tree loss: 2.560 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 45 | Batch: 021 / 025 | Total loss: 2.561 | Reg loss: 0.029 | Tree loss: 2.561 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 45 | Batch: 022 / 025 | Total loss: 2.535 | Reg loss: 0.029 | Tree loss: 2.535 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 45 | Batch: 023 / 025 | Total loss: 2.513 | Reg loss: 0.029 | Tree loss: 2.513 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 45 | Batch: 024 / 025 | Total loss: 2.502 | Reg loss: 0.029 | Tree loss: 2.502 | Accuracy: 1.000000 | 0.067 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 46 | Batch: 000 / 025 | Total loss: 2.830 | Reg loss: 0.028 | Tree loss: 2.830 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 46 | Batch: 001 / 025 | Total loss: 2.813 | Reg loss: 0.028 | Tree loss: 2.813 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 46 | Batch: 002 / 025 | Total loss: 2.804 | Reg loss: 0.028 | Tree loss: 2.804 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 46 | Batch: 003 / 025 | Total loss: 2.776 | Reg loss: 0.028 | Tree loss: 2.776 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 46 | Batch: 004 / 025 | Total loss: 2.771 | Reg loss: 0.028 | Tree loss: 2.771 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 46 | Batch: 005 / 025 | Total loss: 2.722 | Reg loss: 0.028 | Tree loss: 2.722 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 46 | Batch: 006 / 025 | Total loss: 2.716 | Reg loss: 0.028 | Tree loss: 2.716 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 46 | Batch: 007 / 025 | Total loss: 2.691 | Reg loss: 0.028 | Tree loss: 2.691 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 46 | Batch: 008 / 025 | Total loss: 2.665 | Reg loss: 0.028 | Tree loss: 2.665 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 46 | Batch: 009 / 025 | Total loss: 2.667 | Reg loss: 0.028 | Tree loss: 2.667 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 46 | Batch: 010 / 025 | Total loss: 2.633 | Reg loss: 0.028 | Tree loss: 2.633 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 46 | Batch: 011 / 025 | Total loss: 2.624 | Reg loss: 0.028 | Tree loss: 2.624 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 46 | Batch: 012 / 025 | Total loss: 2.600 | Reg loss: 0.028 | Tree loss: 2.600 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 46 | Batch: 013 / 025 | Total loss: 2.588 | Reg loss: 0.028 | Tree loss: 2.588 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 46 | Batch: 014 / 025 | Total loss: 2.544 | Reg loss: 0.028 | Tree loss: 2.544 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 46 | Batch: 015 / 025 | Total loss: 2.546 | Reg loss: 0.028 | Tree loss: 2.546 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 46 | Batch: 016 / 025 | Total loss: 2.521 | Reg loss: 0.029 | Tree loss: 2.521 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 46 | Batch: 017 / 025 | Total loss: 2.507 | Reg loss: 0.029 | Tree loss: 2.507 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 46 | Batch: 018 / 025 | Total loss: 2.486 | Reg loss: 0.029 | Tree loss: 2.486 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 46 | Batch: 019 / 025 | Total loss: 2.464 | Reg loss: 0.029 | Tree loss: 2.464 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 46 | Batch: 020 / 025 | Total loss: 2.462 | Reg loss: 0.029 | Tree loss: 2.462 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 46 | Batch: 021 / 025 | Total loss: 2.458 | Reg loss: 0.029 | Tree loss: 2.458 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 46 | Batch: 022 / 025 | Total loss: 2.426 | Reg loss: 0.029 | Tree loss: 2.426 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 46 | Batch: 023 / 025 | Total loss: 2.406 | Reg loss: 0.029 | Tree loss: 2.406 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 46 | Batch: 024 / 025 | Total loss: 2.385 | Reg loss: 0.029 | Tree loss: 2.385 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 47 | Batch: 000 / 025 | Total loss: 2.707 | Reg loss: 0.028 | Tree loss: 2.707 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 47 | Batch: 001 / 025 | Total loss: 2.687 | Reg loss: 0.028 | Tree loss: 2.687 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 47 | Batch: 002 / 025 | Total loss: 2.682 | Reg loss: 0.028 | Tree loss: 2.682 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 47 | Batch: 003 / 025 | Total loss: 2.659 | Reg loss: 0.028 | Tree loss: 2.659 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 47 | Batch: 004 / 025 | Total loss: 2.650 | Reg loss: 0.028 | Tree loss: 2.650 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 47 | Batch: 005 / 025 | Total loss: 2.619 | Reg loss: 0.028 | Tree loss: 2.619 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 47 | Batch: 006 / 025 | Total loss: 2.600 | Reg loss: 0.028 | Tree loss: 2.600 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 47 | Batch: 007 / 025 | Total loss: 2.583 | Reg loss: 0.028 | Tree loss: 2.583 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 47 | Batch: 008 / 025 | Total loss: 2.555 | Reg loss: 0.028 | Tree loss: 2.555 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 47 | Batch: 009 / 025 | Total loss: 2.529 | Reg loss: 0.028 | Tree loss: 2.529 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 47 | Batch: 010 / 025 | Total loss: 2.519 | Reg loss: 0.028 | Tree loss: 2.519 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 47 | Batch: 011 / 025 | Total loss: 2.510 | Reg loss: 0.028 | Tree loss: 2.510 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 47 | Batch: 012 / 025 | Total loss: 2.484 | Reg loss: 0.028 | Tree loss: 2.484 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 47 | Batch: 013 / 025 | Total loss: 2.467 | Reg loss: 0.028 | Tree loss: 2.467 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 47 | Batch: 014 / 025 | Total loss: 2.437 | Reg loss: 0.028 | Tree loss: 2.437 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 47 | Batch: 015 / 025 | Total loss: 2.431 | Reg loss: 0.028 | Tree loss: 2.431 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 47 | Batch: 016 / 025 | Total loss: 2.420 | Reg loss: 0.029 | Tree loss: 2.420 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 47 | Batch: 017 / 025 | Total loss: 2.407 | Reg loss: 0.029 | Tree loss: 2.407 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 47 | Batch: 018 / 025 | Total loss: 2.378 | Reg loss: 0.029 | Tree loss: 2.378 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 47 | Batch: 019 / 025 | Total loss: 2.367 | Reg loss: 0.029 | Tree loss: 2.367 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 47 | Batch: 020 / 025 | Total loss: 2.361 | Reg loss: 0.029 | Tree loss: 2.361 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 47 | Batch: 021 / 025 | Total loss: 2.331 | Reg loss: 0.029 | Tree loss: 2.331 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 47 | Batch: 022 / 025 | Total loss: 2.304 | Reg loss: 0.029 | Tree loss: 2.304 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 47 | Batch: 023 / 025 | Total loss: 2.289 | Reg loss: 0.029 | Tree loss: 2.289 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 47 | Batch: 024 / 025 | Total loss: 2.278 | Reg loss: 0.029 | Tree loss: 2.278 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 48 | Batch: 000 / 025 | Total loss: 2.607 | Reg loss: 0.028 | Tree loss: 2.607 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 48 | Batch: 001 / 025 | Total loss: 2.578 | Reg loss: 0.028 | Tree loss: 2.578 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 48 | Batch: 002 / 025 | Total loss: 2.562 | Reg loss: 0.028 | Tree loss: 2.562 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 48 | Batch: 003 / 025 | Total loss: 2.543 | Reg loss: 0.028 | Tree loss: 2.543 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 48 | Batch: 004 / 025 | Total loss: 2.529 | Reg loss: 0.028 | Tree loss: 2.529 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 48 | Batch: 005 / 025 | Total loss: 2.509 | Reg loss: 0.028 | Tree loss: 2.509 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 48 | Batch: 006 / 025 | Total loss: 2.484 | Reg loss: 0.028 | Tree loss: 2.484 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 48 | Batch: 007 / 025 | Total loss: 2.458 | Reg loss: 0.028 | Tree loss: 2.458 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 48 | Batch: 008 / 025 | Total loss: 2.451 | Reg loss: 0.028 | Tree loss: 2.451 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 48 | Batch: 009 / 025 | Total loss: 2.425 | Reg loss: 0.028 | Tree loss: 2.425 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 48 | Batch: 010 / 025 | Total loss: 2.409 | Reg loss: 0.028 | Tree loss: 2.409 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 48 | Batch: 011 / 025 | Total loss: 2.392 | Reg loss: 0.028 | Tree loss: 2.392 | Accuracy: 1.000000 | 0.067 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 48 | Batch: 012 / 025 | Total loss: 2.380 | Reg loss: 0.028 | Tree loss: 2.380 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 48 | Batch: 013 / 025 | Total loss: 2.351 | Reg loss: 0.028 | Tree loss: 2.351 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 48 | Batch: 014 / 025 | Total loss: 2.348 | Reg loss: 0.028 | Tree loss: 2.348 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 48 | Batch: 015 / 025 | Total loss: 2.320 | Reg loss: 0.028 | Tree loss: 2.320 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 48 | Batch: 016 / 025 | Total loss: 2.314 | Reg loss: 0.029 | Tree loss: 2.314 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 48 | Batch: 017 / 025 | Total loss: 2.281 | Reg loss: 0.029 | Tree loss: 2.281 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 48 | Batch: 018 / 025 | Total loss: 2.271 | Reg loss: 0.029 | Tree loss: 2.271 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 48 | Batch: 019 / 025 | Total loss: 2.241 | Reg loss: 0.029 | Tree loss: 2.241 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 48 | Batch: 020 / 025 | Total loss: 2.229 | Reg loss: 0.029 | Tree loss: 2.229 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 48 | Batch: 021 / 025 | Total loss: 2.225 | Reg loss: 0.029 | Tree loss: 2.225 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 48 | Batch: 022 / 025 | Total loss: 2.205 | Reg loss: 0.029 | Tree loss: 2.205 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 48 | Batch: 023 / 025 | Total loss: 2.187 | Reg loss: 0.029 | Tree loss: 2.187 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 48 | Batch: 024 / 025 | Total loss: 2.183 | Reg loss: 0.029 | Tree loss: 2.183 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 49 | Batch: 000 / 025 | Total loss: 2.475 | Reg loss: 0.028 | Tree loss: 2.475 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 49 | Batch: 001 / 025 | Total loss: 2.476 | Reg loss: 0.028 | Tree loss: 2.476 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 49 | Batch: 002 / 025 | Total loss: 2.462 | Reg loss: 0.028 | Tree loss: 2.462 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 49 | Batch: 003 / 025 | Total loss: 2.433 | Reg loss: 0.028 | Tree loss: 2.433 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 49 | Batch: 004 / 025 | Total loss: 2.411 | Reg loss: 0.028 | Tree loss: 2.411 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 49 | Batch: 005 / 025 | Total loss: 2.398 | Reg loss: 0.028 | Tree loss: 2.398 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 49 | Batch: 006 / 025 | Total loss: 2.367 | Reg loss: 0.028 | Tree loss: 2.367 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 49 | Batch: 007 / 025 | Total loss: 2.363 | Reg loss: 0.028 | Tree loss: 2.363 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 49 | Batch: 008 / 025 | Total loss: 2.331 | Reg loss: 0.028 | Tree loss: 2.331 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 49 | Batch: 009 / 025 | Total loss: 2.315 | Reg loss: 0.028 | Tree loss: 2.315 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 49 | Batch: 010 / 025 | Total loss: 2.308 | Reg loss: 0.028 | Tree loss: 2.308 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 49 | Batch: 011 / 025 | Total loss: 2.288 | Reg loss: 0.028 | Tree loss: 2.288 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 49 | Batch: 012 / 025 | Total loss: 2.267 | Reg loss: 0.028 | Tree loss: 2.267 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 49 | Batch: 013 / 025 | Total loss: 2.252 | Reg loss: 0.028 | Tree loss: 2.252 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 49 | Batch: 014 / 025 | Total loss: 2.238 | Reg loss: 0.028 | Tree loss: 2.238 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 49 | Batch: 015 / 025 | Total loss: 2.220 | Reg loss: 0.028 | Tree loss: 2.220 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 49 | Batch: 016 / 025 | Total loss: 2.193 | Reg loss: 0.029 | Tree loss: 2.193 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 49 | Batch: 017 / 025 | Total loss: 2.176 | Reg loss: 0.029 | Tree loss: 2.176 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 49 | Batch: 018 / 025 | Total loss: 2.160 | Reg loss: 0.029 | Tree loss: 2.160 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 49 | Batch: 019 / 025 | Total loss: 2.146 | Reg loss: 0.029 | Tree loss: 2.146 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 49 | Batch: 020 / 025 | Total loss: 2.128 | Reg loss: 0.029 | Tree loss: 2.128 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 49 | Batch: 021 / 025 | Total loss: 2.110 | Reg loss: 0.029 | Tree loss: 2.110 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 49 | Batch: 022 / 025 | Total loss: 2.098 | Reg loss: 0.029 | Tree loss: 2.098 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 49 | Batch: 023 / 025 | Total loss: 2.079 | Reg loss: 0.029 | Tree loss: 2.079 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 49 | Batch: 024 / 025 | Total loss: 2.078 | Reg loss: 0.029 | Tree loss: 2.078 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 50 | Batch: 000 / 025 | Total loss: 2.371 | Reg loss: 0.028 | Tree loss: 2.371 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 50 | Batch: 001 / 025 | Total loss: 2.350 | Reg loss: 0.028 | Tree loss: 2.350 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 50 | Batch: 002 / 025 | Total loss: 2.331 | Reg loss: 0.028 | Tree loss: 2.331 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 50 | Batch: 003 / 025 | Total loss: 2.308 | Reg loss: 0.028 | Tree loss: 2.308 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 50 | Batch: 004 / 025 | Total loss: 2.301 | Reg loss: 0.028 | Tree loss: 2.301 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 50 | Batch: 005 / 025 | Total loss: 2.304 | Reg loss: 0.028 | Tree loss: 2.304 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 50 | Batch: 006 / 025 | Total loss: 2.272 | Reg loss: 0.028 | Tree loss: 2.272 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 50 | Batch: 007 / 025 | Total loss: 2.247 | Reg loss: 0.028 | Tree loss: 2.247 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 50 | Batch: 008 / 025 | Total loss: 2.238 | Reg loss: 0.028 | Tree loss: 2.238 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 50 | Batch: 009 / 025 | Total loss: 2.210 | Reg loss: 0.028 | Tree loss: 2.210 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 50 | Batch: 010 / 025 | Total loss: 2.203 | Reg loss: 0.028 | Tree loss: 2.203 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 50 | Batch: 011 / 025 | Total loss: 2.191 | Reg loss: 0.028 | Tree loss: 2.191 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 50 | Batch: 012 / 025 | Total loss: 2.146 | Reg loss: 0.028 | Tree loss: 2.146 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 50 | Batch: 013 / 025 | Total loss: 2.146 | Reg loss: 0.028 | Tree loss: 2.146 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 50 | Batch: 014 / 025 | Total loss: 2.129 | Reg loss: 0.028 | Tree loss: 2.129 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 50 | Batch: 015 / 025 | Total loss: 2.114 | Reg loss: 0.028 | Tree loss: 2.114 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 50 | Batch: 016 / 025 | Total loss: 2.088 | Reg loss: 0.028 | Tree loss: 2.088 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 50 | Batch: 017 / 025 | Total loss: 2.074 | Reg loss: 0.029 | Tree loss: 2.074 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 50 | Batch: 018 / 025 | Total loss: 2.051 | Reg loss: 0.029 | Tree loss: 2.051 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 50 | Batch: 019 / 025 | Total loss: 2.049 | Reg loss: 0.029 | Tree loss: 2.049 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 50 | Batch: 020 / 025 | Total loss: 2.042 | Reg loss: 0.029 | Tree loss: 2.042 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 50 | Batch: 021 / 025 | Total loss: 2.014 | Reg loss: 0.029 | Tree loss: 2.014 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 50 | Batch: 022 / 025 | Total loss: 2.002 | Reg loss: 0.029 | Tree loss: 2.002 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 50 | Batch: 023 / 025 | Total loss: 1.985 | Reg loss: 0.029 | Tree loss: 1.985 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 50 | Batch: 024 / 025 | Total loss: 1.973 | Reg loss: 0.029 | Tree loss: 1.973 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 51 | Batch: 000 / 025 | Total loss: 2.255 | Reg loss: 0.028 | Tree loss: 2.255 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 51 | Batch: 001 / 025 | Total loss: 2.241 | Reg loss: 0.028 | Tree loss: 2.241 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 51 | Batch: 002 / 025 | Total loss: 2.217 | Reg loss: 0.028 | Tree loss: 2.217 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 51 | Batch: 003 / 025 | Total loss: 2.212 | Reg loss: 0.028 | Tree loss: 2.212 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 51 | Batch: 004 / 025 | Total loss: 2.200 | Reg loss: 0.028 | Tree loss: 2.200 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 51 | Batch: 005 / 025 | Total loss: 2.198 | Reg loss: 0.028 | Tree loss: 2.198 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 51 | Batch: 006 / 025 | Total loss: 2.178 | Reg loss: 0.028 | Tree loss: 2.178 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 51 | Batch: 007 / 025 | Total loss: 2.138 | Reg loss: 0.028 | Tree loss: 2.138 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 51 | Batch: 008 / 025 | Total loss: 2.131 | Reg loss: 0.028 | Tree loss: 2.131 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 51 | Batch: 009 / 025 | Total loss: 2.118 | Reg loss: 0.028 | Tree loss: 2.118 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 51 | Batch: 010 / 025 | Total loss: 2.085 | Reg loss: 0.028 | Tree loss: 2.085 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 51 | Batch: 011 / 025 | Total loss: 2.072 | Reg loss: 0.028 | Tree loss: 2.072 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 51 | Batch: 012 / 025 | Total loss: 2.053 | Reg loss: 0.028 | Tree loss: 2.053 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 51 | Batch: 013 / 025 | Total loss: 2.038 | Reg loss: 0.028 | Tree loss: 2.038 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 51 | Batch: 014 / 025 | Total loss: 2.019 | Reg loss: 0.028 | Tree loss: 2.019 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 51 | Batch: 015 / 025 | Total loss: 2.012 | Reg loss: 0.028 | Tree loss: 2.012 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 51 | Batch: 016 / 025 | Total loss: 1.999 | Reg loss: 0.028 | Tree loss: 1.999 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 51 | Batch: 017 / 025 | Total loss: 1.970 | Reg loss: 0.028 | Tree loss: 1.970 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 51 | Batch: 018 / 025 | Total loss: 1.966 | Reg loss: 0.029 | Tree loss: 1.966 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 51 | Batch: 019 / 025 | Total loss: 1.960 | Reg loss: 0.029 | Tree loss: 1.960 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 51 | Batch: 020 / 025 | Total loss: 1.935 | Reg loss: 0.029 | Tree loss: 1.935 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 51 | Batch: 021 / 025 | Total loss: 1.925 | Reg loss: 0.029 | Tree loss: 1.925 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 51 | Batch: 022 / 025 | Total loss: 1.907 | Reg loss: 0.029 | Tree loss: 1.907 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 51 | Batch: 023 / 025 | Total loss: 1.893 | Reg loss: 0.029 | Tree loss: 1.893 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 51 | Batch: 024 / 025 | Total loss: 1.870 | Reg loss: 0.029 | Tree loss: 1.870 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 52 | Batch: 000 / 025 | Total loss: 2.170 | Reg loss: 0.028 | Tree loss: 2.170 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 52 | Batch: 001 / 025 | Total loss: 2.156 | Reg loss: 0.028 | Tree loss: 2.156 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 52 | Batch: 002 / 025 | Total loss: 2.125 | Reg loss: 0.028 | Tree loss: 2.125 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 52 | Batch: 003 / 025 | Total loss: 2.105 | Reg loss: 0.028 | Tree loss: 2.105 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 52 | Batch: 004 / 025 | Total loss: 2.094 | Reg loss: 0.028 | Tree loss: 2.094 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 52 | Batch: 005 / 025 | Total loss: 2.085 | Reg loss: 0.028 | Tree loss: 2.085 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 52 | Batch: 006 / 025 | Total loss: 2.063 | Reg loss: 0.028 | Tree loss: 2.063 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 52 | Batch: 007 / 025 | Total loss: 2.036 | Reg loss: 0.028 | Tree loss: 2.036 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 52 | Batch: 008 / 025 | Total loss: 2.035 | Reg loss: 0.028 | Tree loss: 2.035 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 52 | Batch: 009 / 025 | Total loss: 2.015 | Reg loss: 0.028 | Tree loss: 2.015 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 52 | Batch: 010 / 025 | Total loss: 1.996 | Reg loss: 0.028 | Tree loss: 1.996 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 52 | Batch: 011 / 025 | Total loss: 1.974 | Reg loss: 0.028 | Tree loss: 1.974 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 52 | Batch: 012 / 025 | Total loss: 1.952 | Reg loss: 0.028 | Tree loss: 1.952 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 52 | Batch: 013 / 025 | Total loss: 1.946 | Reg loss: 0.028 | Tree loss: 1.946 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 52 | Batch: 014 / 025 | Total loss: 1.918 | Reg loss: 0.028 | Tree loss: 1.918 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 52 | Batch: 015 / 025 | Total loss: 1.915 | Reg loss: 0.028 | Tree loss: 1.915 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 52 | Batch: 016 / 025 | Total loss: 1.902 | Reg loss: 0.028 | Tree loss: 1.902 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 52 | Batch: 017 / 025 | Total loss: 1.885 | Reg loss: 0.028 | Tree loss: 1.885 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 52 | Batch: 018 / 025 | Total loss: 1.858 | Reg loss: 0.028 | Tree loss: 1.858 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 52 | Batch: 019 / 025 | Total loss: 1.848 | Reg loss: 0.029 | Tree loss: 1.848 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 52 | Batch: 020 / 025 | Total loss: 1.830 | Reg loss: 0.029 | Tree loss: 1.830 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 52 | Batch: 021 / 025 | Total loss: 1.823 | Reg loss: 0.029 | Tree loss: 1.823 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 52 | Batch: 022 / 025 | Total loss: 1.811 | Reg loss: 0.029 | Tree loss: 1.811 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 52 | Batch: 023 / 025 | Total loss: 1.805 | Reg loss: 0.029 | Tree loss: 1.805 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 52 | Batch: 024 / 025 | Total loss: 1.783 | Reg loss: 0.029 | Tree loss: 1.783 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 53 | Batch: 000 / 025 | Total loss: 2.064 | Reg loss: 0.028 | Tree loss: 2.064 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 53 | Batch: 001 / 025 | Total loss: 2.041 | Reg loss: 0.028 | Tree loss: 2.041 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 53 | Batch: 002 / 025 | Total loss: 2.021 | Reg loss: 0.028 | Tree loss: 2.021 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 53 | Batch: 003 / 025 | Total loss: 2.017 | Reg loss: 0.028 | Tree loss: 2.017 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 53 | Batch: 004 / 025 | Total loss: 1.998 | Reg loss: 0.028 | Tree loss: 1.998 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 53 | Batch: 005 / 025 | Total loss: 1.975 | Reg loss: 0.028 | Tree loss: 1.975 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 53 | Batch: 006 / 025 | Total loss: 1.966 | Reg loss: 0.028 | Tree loss: 1.966 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 53 | Batch: 007 / 025 | Total loss: 1.947 | Reg loss: 0.028 | Tree loss: 1.947 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 53 | Batch: 008 / 025 | Total loss: 1.936 | Reg loss: 0.028 | Tree loss: 1.936 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 53 | Batch: 009 / 025 | Total loss: 1.918 | Reg loss: 0.028 | Tree loss: 1.918 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 53 | Batch: 010 / 025 | Total loss: 1.904 | Reg loss: 0.028 | Tree loss: 1.904 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 53 | Batch: 011 / 025 | Total loss: 1.872 | Reg loss: 0.028 | Tree loss: 1.872 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 53 | Batch: 012 / 025 | Total loss: 1.866 | Reg loss: 0.028 | Tree loss: 1.866 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 53 | Batch: 013 / 025 | Total loss: 1.846 | Reg loss: 0.028 | Tree loss: 1.846 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 53 | Batch: 014 / 025 | Total loss: 1.835 | Reg loss: 0.028 | Tree loss: 1.835 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 53 | Batch: 015 / 025 | Total loss: 1.818 | Reg loss: 0.028 | Tree loss: 1.818 | Accuracy: 1.000000 | 0.067 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 53 | Batch: 016 / 025 | Total loss: 1.803 | Reg loss: 0.028 | Tree loss: 1.803 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 53 | Batch: 017 / 025 | Total loss: 1.800 | Reg loss: 0.028 | Tree loss: 1.800 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 53 | Batch: 018 / 025 | Total loss: 1.776 | Reg loss: 0.028 | Tree loss: 1.776 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 53 | Batch: 019 / 025 | Total loss: 1.745 | Reg loss: 0.028 | Tree loss: 1.745 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 53 | Batch: 020 / 025 | Total loss: 1.754 | Reg loss: 0.029 | Tree loss: 1.754 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 53 | Batch: 021 / 025 | Total loss: 1.740 | Reg loss: 0.029 | Tree loss: 1.740 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 53 | Batch: 022 / 025 | Total loss: 1.721 | Reg loss: 0.029 | Tree loss: 1.721 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 53 | Batch: 023 / 025 | Total loss: 1.701 | Reg loss: 0.029 | Tree loss: 1.701 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 53 | Batch: 024 / 025 | Total loss: 1.695 | Reg loss: 0.029 | Tree loss: 1.695 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 54 | Batch: 000 / 025 | Total loss: 1.965 | Reg loss: 0.028 | Tree loss: 1.965 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 54 | Batch: 001 / 025 | Total loss: 1.959 | Reg loss: 0.028 | Tree loss: 1.959 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 54 | Batch: 002 / 025 | Total loss: 1.935 | Reg loss: 0.028 | Tree loss: 1.935 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 54 | Batch: 003 / 025 | Total loss: 1.907 | Reg loss: 0.028 | Tree loss: 1.907 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 54 | Batch: 004 / 025 | Total loss: 1.900 | Reg loss: 0.028 | Tree loss: 1.900 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 54 | Batch: 005 / 025 | Total loss: 1.884 | Reg loss: 0.028 | Tree loss: 1.884 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 54 | Batch: 006 / 025 | Total loss: 1.871 | Reg loss: 0.028 | Tree loss: 1.871 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 54 | Batch: 007 / 025 | Total loss: 1.848 | Reg loss: 0.028 | Tree loss: 1.848 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 54 | Batch: 008 / 025 | Total loss: 1.849 | Reg loss: 0.028 | Tree loss: 1.849 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 54 | Batch: 009 / 025 | Total loss: 1.822 | Reg loss: 0.028 | Tree loss: 1.822 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 54 | Batch: 010 / 025 | Total loss: 1.801 | Reg loss: 0.028 | Tree loss: 1.801 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 54 | Batch: 011 / 025 | Total loss: 1.796 | Reg loss: 0.028 | Tree loss: 1.796 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 54 | Batch: 012 / 025 | Total loss: 1.769 | Reg loss: 0.028 | Tree loss: 1.769 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 54 | Batch: 013 / 025 | Total loss: 1.753 | Reg loss: 0.028 | Tree loss: 1.753 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 54 | Batch: 014 / 025 | Total loss: 1.742 | Reg loss: 0.028 | Tree loss: 1.742 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 54 | Batch: 015 / 025 | Total loss: 1.741 | Reg loss: 0.028 | Tree loss: 1.741 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 54 | Batch: 016 / 025 | Total loss: 1.701 | Reg loss: 0.028 | Tree loss: 1.701 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 54 | Batch: 017 / 025 | Total loss: 1.698 | Reg loss: 0.028 | Tree loss: 1.698 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 54 | Batch: 018 / 025 | Total loss: 1.689 | Reg loss: 0.028 | Tree loss: 1.689 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 54 | Batch: 019 / 025 | Total loss: 1.679 | Reg loss: 0.028 | Tree loss: 1.679 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 54 | Batch: 020 / 025 | Total loss: 1.653 | Reg loss: 0.028 | Tree loss: 1.653 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 54 | Batch: 021 / 025 | Total loss: 1.645 | Reg loss: 0.028 | Tree loss: 1.645 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 54 | Batch: 022 / 025 | Total loss: 1.634 | Reg loss: 0.029 | Tree loss: 1.634 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 54 | Batch: 023 / 025 | Total loss: 1.624 | Reg loss: 0.029 | Tree loss: 1.624 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 54 | Batch: 024 / 025 | Total loss: 1.616 | Reg loss: 0.029 | Tree loss: 1.616 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 55 | Batch: 000 / 025 | Total loss: 1.864 | Reg loss: 0.028 | Tree loss: 1.864 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 55 | Batch: 001 / 025 | Total loss: 1.849 | Reg loss: 0.028 | Tree loss: 1.849 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 55 | Batch: 002 / 025 | Total loss: 1.834 | Reg loss: 0.028 | Tree loss: 1.834 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 55 | Batch: 003 / 025 | Total loss: 1.837 | Reg loss: 0.028 | Tree loss: 1.837 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 55 | Batch: 004 / 025 | Total loss: 1.803 | Reg loss: 0.028 | Tree loss: 1.803 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 55 | Batch: 005 / 025 | Total loss: 1.795 | Reg loss: 0.028 | Tree loss: 1.795 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 55 | Batch: 006 / 025 | Total loss: 1.775 | Reg loss: 0.028 | Tree loss: 1.775 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 55 | Batch: 007 / 025 | Total loss: 1.759 | Reg loss: 0.028 | Tree loss: 1.759 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 55 | Batch: 008 / 025 | Total loss: 1.754 | Reg loss: 0.028 | Tree loss: 1.754 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 55 | Batch: 009 / 025 | Total loss: 1.735 | Reg loss: 0.028 | Tree loss: 1.735 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 55 | Batch: 010 / 025 | Total loss: 1.713 | Reg loss: 0.028 | Tree loss: 1.713 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 55 | Batch: 011 / 025 | Total loss: 1.702 | Reg loss: 0.028 | Tree loss: 1.702 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 55 | Batch: 012 / 025 | Total loss: 1.682 | Reg loss: 0.028 | Tree loss: 1.682 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 55 | Batch: 013 / 025 | Total loss: 1.675 | Reg loss: 0.028 | Tree loss: 1.675 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 55 | Batch: 014 / 025 | Total loss: 1.659 | Reg loss: 0.028 | Tree loss: 1.659 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 55 | Batch: 015 / 025 | Total loss: 1.652 | Reg loss: 0.028 | Tree loss: 1.652 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 55 | Batch: 016 / 025 | Total loss: 1.630 | Reg loss: 0.028 | Tree loss: 1.630 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 55 | Batch: 017 / 025 | Total loss: 1.619 | Reg loss: 0.028 | Tree loss: 1.619 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 55 | Batch: 018 / 025 | Total loss: 1.596 | Reg loss: 0.028 | Tree loss: 1.596 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 55 | Batch: 019 / 025 | Total loss: 1.585 | Reg loss: 0.028 | Tree loss: 1.585 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 55 | Batch: 020 / 025 | Total loss: 1.578 | Reg loss: 0.028 | Tree loss: 1.578 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 55 | Batch: 021 / 025 | Total loss: 1.563 | Reg loss: 0.028 | Tree loss: 1.563 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 55 | Batch: 022 / 025 | Total loss: 1.549 | Reg loss: 0.028 | Tree loss: 1.549 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 55 | Batch: 023 / 025 | Total loss: 1.535 | Reg loss: 0.028 | Tree loss: 1.535 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 55 | Batch: 024 / 025 | Total loss: 1.545 | Reg loss: 0.029 | Tree loss: 1.545 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 56 | Batch: 000 / 025 | Total loss: 1.775 | Reg loss: 0.028 | Tree loss: 1.775 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 56 | Batch: 001 / 025 | Total loss: 1.773 | Reg loss: 0.028 | Tree loss: 1.773 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 56 | Batch: 002 / 025 | Total loss: 1.750 | Reg loss: 0.028 | Tree loss: 1.750 | Accuracy: 1.000000 | 0.067 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 56 | Batch: 003 / 025 | Total loss: 1.738 | Reg loss: 0.028 | Tree loss: 1.738 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 56 | Batch: 004 / 025 | Total loss: 1.716 | Reg loss: 0.028 | Tree loss: 1.716 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 56 | Batch: 005 / 025 | Total loss: 1.696 | Reg loss: 0.028 | Tree loss: 1.696 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 56 | Batch: 006 / 025 | Total loss: 1.686 | Reg loss: 0.028 | Tree loss: 1.686 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 56 | Batch: 007 / 025 | Total loss: 1.673 | Reg loss: 0.028 | Tree loss: 1.673 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 56 | Batch: 008 / 025 | Total loss: 1.660 | Reg loss: 0.028 | Tree loss: 1.660 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 56 | Batch: 009 / 025 | Total loss: 1.655 | Reg loss: 0.028 | Tree loss: 1.655 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 56 | Batch: 010 / 025 | Total loss: 1.648 | Reg loss: 0.028 | Tree loss: 1.648 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 56 | Batch: 011 / 025 | Total loss: 1.620 | Reg loss: 0.028 | Tree loss: 1.620 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 56 | Batch: 012 / 025 | Total loss: 1.602 | Reg loss: 0.028 | Tree loss: 1.602 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 56 | Batch: 013 / 025 | Total loss: 1.591 | Reg loss: 0.028 | Tree loss: 1.591 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 56 | Batch: 014 / 025 | Total loss: 1.578 | Reg loss: 0.028 | Tree loss: 1.578 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 56 | Batch: 015 / 025 | Total loss: 1.554 | Reg loss: 0.028 | Tree loss: 1.554 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 56 | Batch: 016 / 025 | Total loss: 1.549 | Reg loss: 0.028 | Tree loss: 1.549 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 56 | Batch: 017 / 025 | Total loss: 1.527 | Reg loss: 0.028 | Tree loss: 1.527 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 56 | Batch: 018 / 025 | Total loss: 1.522 | Reg loss: 0.028 | Tree loss: 1.522 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 56 | Batch: 019 / 025 | Total loss: 1.508 | Reg loss: 0.028 | Tree loss: 1.508 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 56 | Batch: 020 / 025 | Total loss: 1.508 | Reg loss: 0.028 | Tree loss: 1.508 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 56 | Batch: 021 / 025 | Total loss: 1.483 | Reg loss: 0.028 | Tree loss: 1.483 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 56 | Batch: 022 / 025 | Total loss: 1.474 | Reg loss: 0.028 | Tree loss: 1.474 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 56 | Batch: 023 / 025 | Total loss: 1.442 | Reg loss: 0.028 | Tree loss: 1.442 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Epoch: 56 | Batch: 024 / 025 | Total loss: 1.459 | Reg loss: 0.028 | Tree loss: 1.459 | Accuracy: 1.000000 | 0.067 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 57 | Batch: 000 / 025 | Total loss: 1.701 | Reg loss: 0.027 | Tree loss: 1.701 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 57 | Batch: 001 / 025 | Total loss: 1.678 | Reg loss: 0.027 | Tree loss: 1.678 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 57 | Batch: 002 / 025 | Total loss: 1.681 | Reg loss: 0.027 | Tree loss: 1.681 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 57 | Batch: 003 / 025 | Total loss: 1.652 | Reg loss: 0.027 | Tree loss: 1.652 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 57 | Batch: 004 / 025 | Total loss: 1.636 | Reg loss: 0.027 | Tree loss: 1.636 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 57 | Batch: 005 / 025 | Total loss: 1.618 | Reg loss: 0.027 | Tree loss: 1.618 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 57 | Batch: 006 / 025 | Total loss: 1.591 | Reg loss: 0.027 | Tree loss: 1.591 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 57 | Batch: 007 / 025 | Total loss: 1.584 | Reg loss: 0.027 | Tree loss: 1.584 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 57 | Batch: 008 / 025 | Total loss: 1.588 | Reg loss: 0.028 | Tree loss: 1.588 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 57 | Batch: 009 / 025 | Total loss: 1.560 | Reg loss: 0.028 | Tree loss: 1.560 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 57 | Batch: 010 / 025 | Total loss: 1.553 | Reg loss: 0.028 | Tree loss: 1.553 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 57 | Batch: 011 / 025 | Total loss: 1.542 | Reg loss: 0.028 | Tree loss: 1.542 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 57 | Batch: 012 / 025 | Total loss: 1.506 | Reg loss: 0.028 | Tree loss: 1.506 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 57 | Batch: 013 / 025 | Total loss: 1.511 | Reg loss: 0.028 | Tree loss: 1.511 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 57 | Batch: 014 / 025 | Total loss: 1.494 | Reg loss: 0.028 | Tree loss: 1.494 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 57 | Batch: 015 / 025 | Total loss: 1.485 | Reg loss: 0.028 | Tree loss: 1.485 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 57 | Batch: 016 / 025 | Total loss: 1.478 | Reg loss: 0.028 | Tree loss: 1.478 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 57 | Batch: 017 / 025 | Total loss: 1.464 | Reg loss: 0.028 | Tree loss: 1.464 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 57 | Batch: 018 / 025 | Total loss: 1.442 | Reg loss: 0.028 | Tree loss: 1.442 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 57 | Batch: 019 / 025 | Total loss: 1.434 | Reg loss: 0.028 | Tree loss: 1.434 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 57 | Batch: 020 / 025 | Total loss: 1.410 | Reg loss: 0.028 | Tree loss: 1.410 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 57 | Batch: 021 / 025 | Total loss: 1.413 | Reg loss: 0.028 | Tree loss: 1.413 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 57 | Batch: 022 / 025 | Total loss: 1.403 | Reg loss: 0.028 | Tree loss: 1.403 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 57 | Batch: 023 / 025 | Total loss: 1.389 | Reg loss: 0.028 | Tree loss: 1.389 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 57 | Batch: 024 / 025 | Total loss: 1.344 | Reg loss: 0.028 | Tree loss: 1.344 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 58 | Batch: 000 / 025 | Total loss: 1.612 | Reg loss: 0.027 | Tree loss: 1.612 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 58 | Batch: 001 / 025 | Total loss: 1.613 | Reg loss: 0.027 | Tree loss: 1.613 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 58 | Batch: 002 / 025 | Total loss: 1.593 | Reg loss: 0.027 | Tree loss: 1.593 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 58 | Batch: 003 / 025 | Total loss: 1.554 | Reg loss: 0.027 | Tree loss: 1.554 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 58 | Batch: 004 / 025 | Total loss: 1.577 | Reg loss: 0.027 | Tree loss: 1.577 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 58 | Batch: 005 / 025 | Total loss: 1.546 | Reg loss: 0.027 | Tree loss: 1.546 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 58 | Batch: 006 / 025 | Total loss: 1.527 | Reg loss: 0.027 | Tree loss: 1.527 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 58 | Batch: 007 / 025 | Total loss: 1.502 | Reg loss: 0.027 | Tree loss: 1.502 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 58 | Batch: 008 / 025 | Total loss: 1.505 | Reg loss: 0.027 | Tree loss: 1.505 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 58 | Batch: 009 / 025 | Total loss: 1.487 | Reg loss: 0.027 | Tree loss: 1.487 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 58 | Batch: 010 / 025 | Total loss: 1.464 | Reg loss: 0.027 | Tree loss: 1.464 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 58 | Batch: 011 / 025 | Total loss: 1.459 | Reg loss: 0.027 | Tree loss: 1.459 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 58 | Batch: 012 / 025 | Total loss: 1.434 | Reg loss: 0.028 | Tree loss: 1.434 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 58 | Batch: 013 / 025 | Total loss: 1.431 | Reg loss: 0.028 | Tree loss: 1.431 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 58 | Batch: 014 / 025 | Total loss: 1.414 | Reg loss: 0.028 | Tree loss: 1.414 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 58 | Batch: 015 / 025 | Total loss: 1.404 | Reg loss: 0.028 | Tree loss: 1.404 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 58 | Batch: 016 / 025 | Total loss: 1.392 | Reg loss: 0.028 | Tree loss: 1.392 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 58 | Batch: 017 / 025 | Total loss: 1.387 | Reg loss: 0.028 | Tree loss: 1.387 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 58 | Batch: 018 / 025 | Total loss: 1.380 | Reg loss: 0.028 | Tree loss: 1.380 | Accuracy: 1.000000 | 0.068 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 58 | Batch: 019 / 025 | Total loss: 1.352 | Reg loss: 0.028 | Tree loss: 1.352 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 58 | Batch: 020 / 025 | Total loss: 1.349 | Reg loss: 0.028 | Tree loss: 1.349 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 58 | Batch: 021 / 025 | Total loss: 1.332 | Reg loss: 0.028 | Tree loss: 1.332 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 58 | Batch: 022 / 025 | Total loss: 1.323 | Reg loss: 0.028 | Tree loss: 1.323 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 58 | Batch: 023 / 025 | Total loss: 1.317 | Reg loss: 0.028 | Tree loss: 1.317 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 58 | Batch: 024 / 025 | Total loss: 1.299 | Reg loss: 0.028 | Tree loss: 1.299 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 59 | Batch: 000 / 025 | Total loss: 1.539 | Reg loss: 0.027 | Tree loss: 1.539 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 59 | Batch: 001 / 025 | Total loss: 1.516 | Reg loss: 0.027 | Tree loss: 1.516 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 59 | Batch: 002 / 025 | Total loss: 1.511 | Reg loss: 0.027 | Tree loss: 1.511 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 59 | Batch: 003 / 025 | Total loss: 1.486 | Reg loss: 0.027 | Tree loss: 1.486 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 59 | Batch: 004 / 025 | Total loss: 1.472 | Reg loss: 0.027 | Tree loss: 1.472 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 59 | Batch: 005 / 025 | Total loss: 1.469 | Reg loss: 0.027 | Tree loss: 1.469 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 59 | Batch: 006 / 025 | Total loss: 1.451 | Reg loss: 0.027 | Tree loss: 1.451 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 59 | Batch: 007 / 025 | Total loss: 1.434 | Reg loss: 0.027 | Tree loss: 1.434 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 59 | Batch: 008 / 025 | Total loss: 1.414 | Reg loss: 0.027 | Tree loss: 1.414 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 59 | Batch: 009 / 025 | Total loss: 1.411 | Reg loss: 0.027 | Tree loss: 1.411 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 59 | Batch: 010 / 025 | Total loss: 1.400 | Reg loss: 0.027 | Tree loss: 1.400 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 59 | Batch: 011 / 025 | Total loss: 1.382 | Reg loss: 0.027 | Tree loss: 1.382 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 59 | Batch: 012 / 025 | Total loss: 1.380 | Reg loss: 0.027 | Tree loss: 1.380 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 59 | Batch: 013 / 025 | Total loss: 1.362 | Reg loss: 0.027 | Tree loss: 1.362 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 59 | Batch: 014 / 025 | Total loss: 1.350 | Reg loss: 0.027 | Tree loss: 1.350 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 59 | Batch: 015 / 025 | Total loss: 1.334 | Reg loss: 0.027 | Tree loss: 1.334 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 59 | Batch: 016 / 025 | Total loss: 1.328 | Reg loss: 0.028 | Tree loss: 1.328 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 59 | Batch: 017 / 025 | Total loss: 1.307 | Reg loss: 0.028 | Tree loss: 1.307 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 59 | Batch: 018 / 025 | Total loss: 1.294 | Reg loss: 0.028 | Tree loss: 1.294 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 59 | Batch: 019 / 025 | Total loss: 1.294 | Reg loss: 0.028 | Tree loss: 1.294 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 59 | Batch: 020 / 025 | Total loss: 1.279 | Reg loss: 0.028 | Tree loss: 1.279 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 59 | Batch: 021 / 025 | Total loss: 1.262 | Reg loss: 0.028 | Tree loss: 1.262 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 59 | Batch: 022 / 025 | Total loss: 1.267 | Reg loss: 0.028 | Tree loss: 1.267 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 59 | Batch: 023 / 025 | Total loss: 1.251 | Reg loss: 0.028 | Tree loss: 1.251 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 59 | Batch: 024 / 025 | Total loss: 1.241 | Reg loss: 0.028 | Tree loss: 1.241 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 60 | Batch: 000 / 025 | Total loss: 1.451 | Reg loss: 0.027 | Tree loss: 1.451 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 60 | Batch: 001 / 025 | Total loss: 1.439 | Reg loss: 0.027 | Tree loss: 1.439 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 60 | Batch: 002 / 025 | Total loss: 1.432 | Reg loss: 0.027 | Tree loss: 1.432 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 60 | Batch: 003 / 025 | Total loss: 1.425 | Reg loss: 0.027 | Tree loss: 1.425 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 60 | Batch: 004 / 025 | Total loss: 1.410 | Reg loss: 0.027 | Tree loss: 1.410 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 60 | Batch: 005 / 025 | Total loss: 1.395 | Reg loss: 0.027 | Tree loss: 1.395 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 60 | Batch: 006 / 025 | Total loss: 1.381 | Reg loss: 0.027 | Tree loss: 1.381 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 60 | Batch: 007 / 025 | Total loss: 1.377 | Reg loss: 0.027 | Tree loss: 1.377 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 60 | Batch: 008 / 025 | Total loss: 1.351 | Reg loss: 0.027 | Tree loss: 1.351 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 60 | Batch: 009 / 025 | Total loss: 1.351 | Reg loss: 0.027 | Tree loss: 1.351 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 60 | Batch: 010 / 025 | Total loss: 1.321 | Reg loss: 0.027 | Tree loss: 1.321 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 60 | Batch: 011 / 025 | Total loss: 1.314 | Reg loss: 0.027 | Tree loss: 1.314 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 60 | Batch: 012 / 025 | Total loss: 1.291 | Reg loss: 0.027 | Tree loss: 1.291 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 60 | Batch: 013 / 025 | Total loss: 1.288 | Reg loss: 0.027 | Tree loss: 1.288 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 60 | Batch: 014 / 025 | Total loss: 1.282 | Reg loss: 0.027 | Tree loss: 1.282 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 60 | Batch: 015 / 025 | Total loss: 1.262 | Reg loss: 0.027 | Tree loss: 1.262 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 60 | Batch: 016 / 025 | Total loss: 1.254 | Reg loss: 0.027 | Tree loss: 1.254 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 60 | Batch: 017 / 025 | Total loss: 1.254 | Reg loss: 0.027 | Tree loss: 1.254 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 60 | Batch: 018 / 025 | Total loss: 1.230 | Reg loss: 0.027 | Tree loss: 1.230 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 60 | Batch: 019 / 025 | Total loss: 1.237 | Reg loss: 0.028 | Tree loss: 1.237 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 60 | Batch: 020 / 025 | Total loss: 1.207 | Reg loss: 0.028 | Tree loss: 1.207 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 60 | Batch: 021 / 025 | Total loss: 1.204 | Reg loss: 0.028 | Tree loss: 1.204 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 60 | Batch: 022 / 025 | Total loss: 1.185 | Reg loss: 0.028 | Tree loss: 1.185 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 60 | Batch: 023 / 025 | Total loss: 1.176 | Reg loss: 0.028 | Tree loss: 1.176 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 60 | Batch: 024 / 025 | Total loss: 1.176 | Reg loss: 0.028 | Tree loss: 1.176 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 61 | Batch: 000 / 025 | Total loss: 1.381 | Reg loss: 0.027 | Tree loss: 1.381 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 61 | Batch: 001 / 025 | Total loss: 1.370 | Reg loss: 0.027 | Tree loss: 1.370 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 61 | Batch: 002 / 025 | Total loss: 1.355 | Reg loss: 0.027 | Tree loss: 1.355 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 61 | Batch: 003 / 025 | Total loss: 1.337 | Reg loss: 0.027 | Tree loss: 1.337 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 61 | Batch: 004 / 025 | Total loss: 1.334 | Reg loss: 0.027 | Tree loss: 1.334 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 61 | Batch: 005 / 025 | Total loss: 1.319 | Reg loss: 0.027 | Tree loss: 1.319 | Accuracy: 1.000000 | 0.068 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 61 | Batch: 006 / 025 | Total loss: 1.312 | Reg loss: 0.027 | Tree loss: 1.312 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 61 | Batch: 007 / 025 | Total loss: 1.316 | Reg loss: 0.027 | Tree loss: 1.316 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 61 | Batch: 008 / 025 | Total loss: 1.286 | Reg loss: 0.027 | Tree loss: 1.286 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 61 | Batch: 009 / 025 | Total loss: 1.271 | Reg loss: 0.027 | Tree loss: 1.271 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 61 | Batch: 010 / 025 | Total loss: 1.264 | Reg loss: 0.027 | Tree loss: 1.264 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 61 | Batch: 011 / 025 | Total loss: 1.255 | Reg loss: 0.027 | Tree loss: 1.255 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 61 | Batch: 012 / 025 | Total loss: 1.238 | Reg loss: 0.027 | Tree loss: 1.238 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 61 | Batch: 013 / 025 | Total loss: 1.226 | Reg loss: 0.027 | Tree loss: 1.226 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 61 | Batch: 014 / 025 | Total loss: 1.204 | Reg loss: 0.027 | Tree loss: 1.204 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 61 | Batch: 015 / 025 | Total loss: 1.209 | Reg loss: 0.027 | Tree loss: 1.209 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 61 | Batch: 016 / 025 | Total loss: 1.194 | Reg loss: 0.027 | Tree loss: 1.194 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 61 | Batch: 017 / 025 | Total loss: 1.184 | Reg loss: 0.027 | Tree loss: 1.184 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 61 | Batch: 018 / 025 | Total loss: 1.173 | Reg loss: 0.027 | Tree loss: 1.173 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 61 | Batch: 019 / 025 | Total loss: 1.154 | Reg loss: 0.027 | Tree loss: 1.154 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 61 | Batch: 020 / 025 | Total loss: 1.146 | Reg loss: 0.027 | Tree loss: 1.146 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 61 | Batch: 021 / 025 | Total loss: 1.138 | Reg loss: 0.027 | Tree loss: 1.138 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 61 | Batch: 022 / 025 | Total loss: 1.135 | Reg loss: 0.027 | Tree loss: 1.135 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 61 | Batch: 023 / 025 | Total loss: 1.121 | Reg loss: 0.027 | Tree loss: 1.121 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 61 | Batch: 024 / 025 | Total loss: 1.119 | Reg loss: 0.028 | Tree loss: 1.119 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 62 | Batch: 000 / 025 | Total loss: 1.313 | Reg loss: 0.027 | Tree loss: 1.313 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 62 | Batch: 001 / 025 | Total loss: 1.295 | Reg loss: 0.027 | Tree loss: 1.295 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 62 | Batch: 002 / 025 | Total loss: 1.286 | Reg loss: 0.027 | Tree loss: 1.286 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 62 | Batch: 003 / 025 | Total loss: 1.280 | Reg loss: 0.027 | Tree loss: 1.280 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 62 | Batch: 004 / 025 | Total loss: 1.270 | Reg loss: 0.027 | Tree loss: 1.270 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 62 | Batch: 005 / 025 | Total loss: 1.262 | Reg loss: 0.027 | Tree loss: 1.262 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 62 | Batch: 006 / 025 | Total loss: 1.242 | Reg loss: 0.027 | Tree loss: 1.242 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 62 | Batch: 007 / 025 | Total loss: 1.236 | Reg loss: 0.027 | Tree loss: 1.236 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 62 | Batch: 008 / 025 | Total loss: 1.223 | Reg loss: 0.027 | Tree loss: 1.223 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 62 | Batch: 009 / 025 | Total loss: 1.211 | Reg loss: 0.027 | Tree loss: 1.211 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 62 | Batch: 010 / 025 | Total loss: 1.198 | Reg loss: 0.027 | Tree loss: 1.198 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 62 | Batch: 011 / 025 | Total loss: 1.185 | Reg loss: 0.027 | Tree loss: 1.185 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 62 | Batch: 012 / 025 | Total loss: 1.177 | Reg loss: 0.027 | Tree loss: 1.177 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 62 | Batch: 013 / 025 | Total loss: 1.164 | Reg loss: 0.027 | Tree loss: 1.164 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 62 | Batch: 014 / 025 | Total loss: 1.160 | Reg loss: 0.027 | Tree loss: 1.160 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 62 | Batch: 015 / 025 | Total loss: 1.143 | Reg loss: 0.027 | Tree loss: 1.143 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 62 | Batch: 016 / 025 | Total loss: 1.139 | Reg loss: 0.027 | Tree loss: 1.139 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 62 | Batch: 017 / 025 | Total loss: 1.115 | Reg loss: 0.027 | Tree loss: 1.115 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 62 | Batch: 018 / 025 | Total loss: 1.116 | Reg loss: 0.027 | Tree loss: 1.116 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 62 | Batch: 019 / 025 | Total loss: 1.112 | Reg loss: 0.027 | Tree loss: 1.112 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 62 | Batch: 020 / 025 | Total loss: 1.088 | Reg loss: 0.027 | Tree loss: 1.088 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 62 | Batch: 021 / 025 | Total loss: 1.078 | Reg loss: 0.027 | Tree loss: 1.078 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 62 | Batch: 022 / 025 | Total loss: 1.074 | Reg loss: 0.027 | Tree loss: 1.074 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 62 | Batch: 023 / 025 | Total loss: 1.061 | Reg loss: 0.027 | Tree loss: 1.061 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 62 | Batch: 024 / 025 | Total loss: 1.045 | Reg loss: 0.027 | Tree loss: 1.045 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 63 | Batch: 000 / 025 | Total loss: 1.238 | Reg loss: 0.027 | Tree loss: 1.238 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 63 | Batch: 001 / 025 | Total loss: 1.236 | Reg loss: 0.027 | Tree loss: 1.236 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 63 | Batch: 002 / 025 | Total loss: 1.232 | Reg loss: 0.027 | Tree loss: 1.232 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 63 | Batch: 003 / 025 | Total loss: 1.220 | Reg loss: 0.027 | Tree loss: 1.220 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 63 | Batch: 004 / 025 | Total loss: 1.204 | Reg loss: 0.027 | Tree loss: 1.204 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 63 | Batch: 005 / 025 | Total loss: 1.194 | Reg loss: 0.027 | Tree loss: 1.194 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 63 | Batch: 006 / 025 | Total loss: 1.180 | Reg loss: 0.027 | Tree loss: 1.180 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 63 | Batch: 007 / 025 | Total loss: 1.177 | Reg loss: 0.027 | Tree loss: 1.177 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 63 | Batch: 008 / 025 | Total loss: 1.152 | Reg loss: 0.027 | Tree loss: 1.152 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 63 | Batch: 009 / 025 | Total loss: 1.156 | Reg loss: 0.027 | Tree loss: 1.156 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 63 | Batch: 010 / 025 | Total loss: 1.135 | Reg loss: 0.027 | Tree loss: 1.135 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 63 | Batch: 011 / 025 | Total loss: 1.122 | Reg loss: 0.027 | Tree loss: 1.122 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 63 | Batch: 012 / 025 | Total loss: 1.118 | Reg loss: 0.027 | Tree loss: 1.118 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 63 | Batch: 013 / 025 | Total loss: 1.112 | Reg loss: 0.027 | Tree loss: 1.112 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 63 | Batch: 014 / 025 | Total loss: 1.098 | Reg loss: 0.027 | Tree loss: 1.098 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 63 | Batch: 015 / 025 | Total loss: 1.089 | Reg loss: 0.027 | Tree loss: 1.089 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 63 | Batch: 016 / 025 | Total loss: 1.073 | Reg loss: 0.027 | Tree loss: 1.073 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 63 | Batch: 017 / 025 | Total loss: 1.069 | Reg loss: 0.027 | Tree loss: 1.069 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 63 | Batch: 018 / 025 | Total loss: 1.051 | Reg loss: 0.027 | Tree loss: 1.051 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 63 | Batch: 019 / 025 | Total loss: 1.039 | Reg loss: 0.027 | Tree loss: 1.039 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 63 | Batch: 020 / 025 | Total loss: 1.037 | Reg loss: 0.027 | Tree loss: 1.037 | Accuracy: 1.000000 | 0.068 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 63 | Batch: 021 / 025 | Total loss: 1.032 | Reg loss: 0.027 | Tree loss: 1.032 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 63 | Batch: 022 / 025 | Total loss: 1.017 | Reg loss: 0.027 | Tree loss: 1.017 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 63 | Batch: 023 / 025 | Total loss: 1.012 | Reg loss: 0.027 | Tree loss: 1.012 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 63 | Batch: 024 / 025 | Total loss: 1.000 | Reg loss: 0.027 | Tree loss: 1.000 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 64 | Batch: 000 / 025 | Total loss: 1.175 | Reg loss: 0.026 | Tree loss: 1.175 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 64 | Batch: 001 / 025 | Total loss: 1.159 | Reg loss: 0.026 | Tree loss: 1.159 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 64 | Batch: 002 / 025 | Total loss: 1.167 | Reg loss: 0.026 | Tree loss: 1.167 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 64 | Batch: 003 / 025 | Total loss: 1.152 | Reg loss: 0.026 | Tree loss: 1.152 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 64 | Batch: 004 / 025 | Total loss: 1.147 | Reg loss: 0.026 | Tree loss: 1.147 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 64 | Batch: 005 / 025 | Total loss: 1.129 | Reg loss: 0.026 | Tree loss: 1.129 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 64 | Batch: 006 / 025 | Total loss: 1.117 | Reg loss: 0.026 | Tree loss: 1.117 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 64 | Batch: 007 / 025 | Total loss: 1.108 | Reg loss: 0.026 | Tree loss: 1.108 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 64 | Batch: 008 / 025 | Total loss: 1.104 | Reg loss: 0.026 | Tree loss: 1.104 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 64 | Batch: 009 / 025 | Total loss: 1.085 | Reg loss: 0.026 | Tree loss: 1.085 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 64 | Batch: 010 / 025 | Total loss: 1.086 | Reg loss: 0.027 | Tree loss: 1.086 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 64 | Batch: 011 / 025 | Total loss: 1.082 | Reg loss: 0.027 | Tree loss: 1.082 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 64 | Batch: 012 / 025 | Total loss: 1.064 | Reg loss: 0.027 | Tree loss: 1.064 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 64 | Batch: 013 / 025 | Total loss: 1.044 | Reg loss: 0.027 | Tree loss: 1.044 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 64 | Batch: 014 / 025 | Total loss: 1.045 | Reg loss: 0.027 | Tree loss: 1.045 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 64 | Batch: 015 / 025 | Total loss: 1.035 | Reg loss: 0.027 | Tree loss: 1.035 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 64 | Batch: 016 / 025 | Total loss: 1.022 | Reg loss: 0.027 | Tree loss: 1.022 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 64 | Batch: 017 / 025 | Total loss: 1.019 | Reg loss: 0.027 | Tree loss: 1.019 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 64 | Batch: 018 / 025 | Total loss: 1.006 | Reg loss: 0.027 | Tree loss: 1.006 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 64 | Batch: 019 / 025 | Total loss: 0.995 | Reg loss: 0.027 | Tree loss: 0.995 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 64 | Batch: 020 / 025 | Total loss: 0.985 | Reg loss: 0.027 | Tree loss: 0.985 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 64 | Batch: 021 / 025 | Total loss: 0.982 | Reg loss: 0.027 | Tree loss: 0.982 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 64 | Batch: 022 / 025 | Total loss: 0.975 | Reg loss: 0.027 | Tree loss: 0.975 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 64 | Batch: 023 / 025 | Total loss: 0.962 | Reg loss: 0.027 | Tree loss: 0.962 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 64 | Batch: 024 / 025 | Total loss: 0.946 | Reg loss: 0.027 | Tree loss: 0.946 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 65 | Batch: 000 / 025 | Total loss: 1.130 | Reg loss: 0.026 | Tree loss: 1.130 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 65 | Batch: 001 / 025 | Total loss: 1.118 | Reg loss: 0.026 | Tree loss: 1.118 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 65 | Batch: 002 / 025 | Total loss: 1.108 | Reg loss: 0.026 | Tree loss: 1.108 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 65 | Batch: 003 / 025 | Total loss: 1.102 | Reg loss: 0.026 | Tree loss: 1.102 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 65 | Batch: 004 / 025 | Total loss: 1.088 | Reg loss: 0.026 | Tree loss: 1.088 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 65 | Batch: 005 / 025 | Total loss: 1.075 | Reg loss: 0.026 | Tree loss: 1.075 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 65 | Batch: 006 / 025 | Total loss: 1.058 | Reg loss: 0.026 | Tree loss: 1.058 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 65 | Batch: 007 / 025 | Total loss: 1.048 | Reg loss: 0.026 | Tree loss: 1.048 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 65 | Batch: 008 / 025 | Total loss: 1.053 | Reg loss: 0.026 | Tree loss: 1.053 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 65 | Batch: 009 / 025 | Total loss: 1.032 | Reg loss: 0.026 | Tree loss: 1.032 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 65 | Batch: 010 / 025 | Total loss: 1.031 | Reg loss: 0.026 | Tree loss: 1.031 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 65 | Batch: 011 / 025 | Total loss: 1.018 | Reg loss: 0.026 | Tree loss: 1.018 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 65 | Batch: 012 / 025 | Total loss: 1.009 | Reg loss: 0.026 | Tree loss: 1.009 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 65 | Batch: 013 / 025 | Total loss: 1.001 | Reg loss: 0.026 | Tree loss: 1.001 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 65 | Batch: 014 / 025 | Total loss: 0.989 | Reg loss: 0.026 | Tree loss: 0.989 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 65 | Batch: 015 / 025 | Total loss: 0.979 | Reg loss: 0.026 | Tree loss: 0.979 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 65 | Batch: 016 / 025 | Total loss: 0.972 | Reg loss: 0.027 | Tree loss: 0.972 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 65 | Batch: 017 / 025 | Total loss: 0.968 | Reg loss: 0.027 | Tree loss: 0.968 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 65 | Batch: 018 / 025 | Total loss: 0.955 | Reg loss: 0.027 | Tree loss: 0.955 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 65 | Batch: 019 / 025 | Total loss: 0.948 | Reg loss: 0.027 | Tree loss: 0.948 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 65 | Batch: 020 / 025 | Total loss: 0.932 | Reg loss: 0.027 | Tree loss: 0.932 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 65 | Batch: 021 / 025 | Total loss: 0.928 | Reg loss: 0.027 | Tree loss: 0.928 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 65 | Batch: 022 / 025 | Total loss: 0.916 | Reg loss: 0.027 | Tree loss: 0.916 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 65 | Batch: 023 / 025 | Total loss: 0.908 | Reg loss: 0.027 | Tree loss: 0.908 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 65 | Batch: 024 / 025 | Total loss: 0.905 | Reg loss: 0.027 | Tree loss: 0.905 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 66 | Batch: 000 / 025 | Total loss: 1.062 | Reg loss: 0.026 | Tree loss: 1.062 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 66 | Batch: 001 / 025 | Total loss: 1.067 | Reg loss: 0.026 | Tree loss: 1.067 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 66 | Batch: 002 / 025 | Total loss: 1.046 | Reg loss: 0.026 | Tree loss: 1.046 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 66 | Batch: 003 / 025 | Total loss: 1.049 | Reg loss: 0.026 | Tree loss: 1.049 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 66 | Batch: 004 / 025 | Total loss: 1.026 | Reg loss: 0.026 | Tree loss: 1.026 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 66 | Batch: 005 / 025 | Total loss: 1.026 | Reg loss: 0.026 | Tree loss: 1.026 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 66 | Batch: 006 / 025 | Total loss: 1.019 | Reg loss: 0.026 | Tree loss: 1.019 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 66 | Batch: 007 / 025 | Total loss: 1.001 | Reg loss: 0.026 | Tree loss: 1.001 | Accuracy: 1.000000 | 0.068 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 66 | Batch: 008 / 025 | Total loss: 0.999 | Reg loss: 0.026 | Tree loss: 0.999 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 66 | Batch: 009 / 025 | Total loss: 0.989 | Reg loss: 0.026 | Tree loss: 0.989 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 66 | Batch: 010 / 025 | Total loss: 0.972 | Reg loss: 0.026 | Tree loss: 0.972 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 66 | Batch: 011 / 025 | Total loss: 0.977 | Reg loss: 0.026 | Tree loss: 0.977 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 66 | Batch: 012 / 025 | Total loss: 0.959 | Reg loss: 0.026 | Tree loss: 0.959 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 66 | Batch: 013 / 025 | Total loss: 0.950 | Reg loss: 0.026 | Tree loss: 0.950 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 66 | Batch: 014 / 025 | Total loss: 0.947 | Reg loss: 0.026 | Tree loss: 0.947 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 66 | Batch: 015 / 025 | Total loss: 0.928 | Reg loss: 0.026 | Tree loss: 0.928 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 66 | Batch: 016 / 025 | Total loss: 0.918 | Reg loss: 0.026 | Tree loss: 0.918 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 66 | Batch: 017 / 025 | Total loss: 0.916 | Reg loss: 0.026 | Tree loss: 0.916 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 66 | Batch: 018 / 025 | Total loss: 0.912 | Reg loss: 0.026 | Tree loss: 0.912 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 66 | Batch: 019 / 025 | Total loss: 0.894 | Reg loss: 0.026 | Tree loss: 0.894 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 66 | Batch: 020 / 025 | Total loss: 0.884 | Reg loss: 0.026 | Tree loss: 0.884 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 66 | Batch: 021 / 025 | Total loss: 0.882 | Reg loss: 0.027 | Tree loss: 0.882 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 66 | Batch: 022 / 025 | Total loss: 0.880 | Reg loss: 0.027 | Tree loss: 0.880 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 66 | Batch: 023 / 025 | Total loss: 0.859 | Reg loss: 0.027 | Tree loss: 0.859 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 66 | Batch: 024 / 025 | Total loss: 0.866 | Reg loss: 0.027 | Tree loss: 0.866 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 67 | Batch: 000 / 025 | Total loss: 1.024 | Reg loss: 0.026 | Tree loss: 1.024 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 67 | Batch: 001 / 025 | Total loss: 1.017 | Reg loss: 0.026 | Tree loss: 1.017 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 67 | Batch: 002 / 025 | Total loss: 0.996 | Reg loss: 0.026 | Tree loss: 0.996 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 67 | Batch: 003 / 025 | Total loss: 0.987 | Reg loss: 0.026 | Tree loss: 0.987 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 67 | Batch: 004 / 025 | Total loss: 0.983 | Reg loss: 0.026 | Tree loss: 0.983 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 67 | Batch: 005 / 025 | Total loss: 0.981 | Reg loss: 0.026 | Tree loss: 0.981 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 67 | Batch: 006 / 025 | Total loss: 0.960 | Reg loss: 0.026 | Tree loss: 0.960 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 67 | Batch: 007 / 025 | Total loss: 0.956 | Reg loss: 0.026 | Tree loss: 0.956 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 67 | Batch: 008 / 025 | Total loss: 0.945 | Reg loss: 0.026 | Tree loss: 0.945 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 67 | Batch: 009 / 025 | Total loss: 0.938 | Reg loss: 0.026 | Tree loss: 0.938 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 67 | Batch: 010 / 025 | Total loss: 0.932 | Reg loss: 0.026 | Tree loss: 0.932 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 67 | Batch: 011 / 025 | Total loss: 0.913 | Reg loss: 0.026 | Tree loss: 0.913 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 67 | Batch: 012 / 025 | Total loss: 0.911 | Reg loss: 0.026 | Tree loss: 0.911 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 67 | Batch: 013 / 025 | Total loss: 0.903 | Reg loss: 0.026 | Tree loss: 0.903 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 67 | Batch: 014 / 025 | Total loss: 0.894 | Reg loss: 0.026 | Tree loss: 0.894 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 67 | Batch: 015 / 025 | Total loss: 0.892 | Reg loss: 0.026 | Tree loss: 0.892 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 67 | Batch: 016 / 025 | Total loss: 0.869 | Reg loss: 0.026 | Tree loss: 0.869 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 67 | Batch: 017 / 025 | Total loss: 0.868 | Reg loss: 0.026 | Tree loss: 0.868 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 67 | Batch: 018 / 025 | Total loss: 0.863 | Reg loss: 0.026 | Tree loss: 0.863 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 67 | Batch: 019 / 025 | Total loss: 0.854 | Reg loss: 0.026 | Tree loss: 0.854 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 67 | Batch: 020 / 025 | Total loss: 0.854 | Reg loss: 0.026 | Tree loss: 0.854 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 67 | Batch: 021 / 025 | Total loss: 0.840 | Reg loss: 0.026 | Tree loss: 0.840 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 67 | Batch: 022 / 025 | Total loss: 0.835 | Reg loss: 0.026 | Tree loss: 0.835 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 67 | Batch: 023 / 025 | Total loss: 0.830 | Reg loss: 0.026 | Tree loss: 0.830 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 67 | Batch: 024 / 025 | Total loss: 0.807 | Reg loss: 0.026 | Tree loss: 0.807 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 68 | Batch: 000 / 025 | Total loss: 0.970 | Reg loss: 0.026 | Tree loss: 0.970 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 68 | Batch: 001 / 025 | Total loss: 0.960 | Reg loss: 0.026 | Tree loss: 0.960 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 68 | Batch: 002 / 025 | Total loss: 0.955 | Reg loss: 0.026 | Tree loss: 0.955 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 68 | Batch: 003 / 025 | Total loss: 0.945 | Reg loss: 0.026 | Tree loss: 0.945 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 68 | Batch: 004 / 025 | Total loss: 0.938 | Reg loss: 0.026 | Tree loss: 0.938 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 68 | Batch: 005 / 025 | Total loss: 0.921 | Reg loss: 0.026 | Tree loss: 0.921 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 68 | Batch: 006 / 025 | Total loss: 0.916 | Reg loss: 0.026 | Tree loss: 0.916 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 68 | Batch: 007 / 025 | Total loss: 0.909 | Reg loss: 0.026 | Tree loss: 0.909 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 68 | Batch: 008 / 025 | Total loss: 0.900 | Reg loss: 0.026 | Tree loss: 0.900 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 68 | Batch: 009 / 025 | Total loss: 0.894 | Reg loss: 0.026 | Tree loss: 0.894 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 68 | Batch: 010 / 025 | Total loss: 0.880 | Reg loss: 0.026 | Tree loss: 0.880 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 68 | Batch: 011 / 025 | Total loss: 0.871 | Reg loss: 0.026 | Tree loss: 0.871 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 68 | Batch: 012 / 025 | Total loss: 0.869 | Reg loss: 0.026 | Tree loss: 0.869 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 68 | Batch: 013 / 025 | Total loss: 0.855 | Reg loss: 0.026 | Tree loss: 0.855 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 68 | Batch: 014 / 025 | Total loss: 0.852 | Reg loss: 0.026 | Tree loss: 0.852 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 68 | Batch: 015 / 025 | Total loss: 0.841 | Reg loss: 0.026 | Tree loss: 0.841 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 68 | Batch: 016 / 025 | Total loss: 0.841 | Reg loss: 0.026 | Tree loss: 0.841 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 68 | Batch: 017 / 025 | Total loss: 0.828 | Reg loss: 0.026 | Tree loss: 0.828 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 68 | Batch: 018 / 025 | Total loss: 0.820 | Reg loss: 0.026 | Tree loss: 0.820 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 68 | Batch: 019 / 025 | Total loss: 0.807 | Reg loss: 0.026 | Tree loss: 0.807 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 68 | Batch: 020 / 025 | Total loss: 0.804 | Reg loss: 0.026 | Tree loss: 0.804 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 68 | Batch: 021 / 025 | Total loss: 0.806 | Reg loss: 0.026 | Tree loss: 0.806 | Accuracy: 1.000000 | 0.068 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 68 | Batch: 022 / 025 | Total loss: 0.784 | Reg loss: 0.026 | Tree loss: 0.784 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 68 | Batch: 023 / 025 | Total loss: 0.785 | Reg loss: 0.026 | Tree loss: 0.785 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 68 | Batch: 024 / 025 | Total loss: 0.794 | Reg loss: 0.026 | Tree loss: 0.794 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 69 | Batch: 000 / 025 | Total loss: 0.921 | Reg loss: 0.026 | Tree loss: 0.921 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 69 | Batch: 001 / 025 | Total loss: 0.920 | Reg loss: 0.026 | Tree loss: 0.920 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 69 | Batch: 002 / 025 | Total loss: 0.906 | Reg loss: 0.026 | Tree loss: 0.906 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 69 | Batch: 003 / 025 | Total loss: 0.891 | Reg loss: 0.026 | Tree loss: 0.891 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 69 | Batch: 004 / 025 | Total loss: 0.881 | Reg loss: 0.026 | Tree loss: 0.881 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 69 | Batch: 005 / 025 | Total loss: 0.880 | Reg loss: 0.026 | Tree loss: 0.880 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 69 | Batch: 006 / 025 | Total loss: 0.865 | Reg loss: 0.026 | Tree loss: 0.865 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 69 | Batch: 007 / 025 | Total loss: 0.867 | Reg loss: 0.026 | Tree loss: 0.867 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 69 | Batch: 008 / 025 | Total loss: 0.859 | Reg loss: 0.026 | Tree loss: 0.859 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 69 | Batch: 009 / 025 | Total loss: 0.843 | Reg loss: 0.026 | Tree loss: 0.843 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 69 | Batch: 010 / 025 | Total loss: 0.843 | Reg loss: 0.026 | Tree loss: 0.843 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 69 | Batch: 011 / 025 | Total loss: 0.830 | Reg loss: 0.026 | Tree loss: 0.830 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 69 | Batch: 012 / 025 | Total loss: 0.826 | Reg loss: 0.026 | Tree loss: 0.826 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 69 | Batch: 013 / 025 | Total loss: 0.818 | Reg loss: 0.026 | Tree loss: 0.818 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 69 | Batch: 014 / 025 | Total loss: 0.812 | Reg loss: 0.026 | Tree loss: 0.812 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 69 | Batch: 015 / 025 | Total loss: 0.804 | Reg loss: 0.026 | Tree loss: 0.804 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 69 | Batch: 016 / 025 | Total loss: 0.791 | Reg loss: 0.026 | Tree loss: 0.791 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 69 | Batch: 017 / 025 | Total loss: 0.787 | Reg loss: 0.026 | Tree loss: 0.787 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 69 | Batch: 018 / 025 | Total loss: 0.782 | Reg loss: 0.026 | Tree loss: 0.782 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 69 | Batch: 019 / 025 | Total loss: 0.780 | Reg loss: 0.026 | Tree loss: 0.780 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 69 | Batch: 020 / 025 | Total loss: 0.774 | Reg loss: 0.026 | Tree loss: 0.774 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 69 | Batch: 021 / 025 | Total loss: 0.765 | Reg loss: 0.026 | Tree loss: 0.765 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 69 | Batch: 022 / 025 | Total loss: 0.754 | Reg loss: 0.026 | Tree loss: 0.754 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 69 | Batch: 023 / 025 | Total loss: 0.753 | Reg loss: 0.026 | Tree loss: 0.753 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 69 | Batch: 024 / 025 | Total loss: 0.735 | Reg loss: 0.026 | Tree loss: 0.735 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 70 | Batch: 000 / 025 | Total loss: 0.878 | Reg loss: 0.026 | Tree loss: 0.878 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 70 | Batch: 001 / 025 | Total loss: 0.865 | Reg loss: 0.026 | Tree loss: 0.865 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 70 | Batch: 002 / 025 | Total loss: 0.855 | Reg loss: 0.026 | Tree loss: 0.855 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 70 | Batch: 003 / 025 | Total loss: 0.856 | Reg loss: 0.026 | Tree loss: 0.856 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 70 | Batch: 004 / 025 | Total loss: 0.844 | Reg loss: 0.026 | Tree loss: 0.844 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 70 | Batch: 005 / 025 | Total loss: 0.841 | Reg loss: 0.026 | Tree loss: 0.841 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 70 | Batch: 006 / 025 | Total loss: 0.832 | Reg loss: 0.026 | Tree loss: 0.832 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 70 | Batch: 007 / 025 | Total loss: 0.818 | Reg loss: 0.026 | Tree loss: 0.818 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 70 | Batch: 008 / 025 | Total loss: 0.817 | Reg loss: 0.026 | Tree loss: 0.817 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 70 | Batch: 009 / 025 | Total loss: 0.810 | Reg loss: 0.026 | Tree loss: 0.810 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 70 | Batch: 010 / 025 | Total loss: 0.801 | Reg loss: 0.026 | Tree loss: 0.801 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 70 | Batch: 011 / 025 | Total loss: 0.799 | Reg loss: 0.026 | Tree loss: 0.799 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 70 | Batch: 012 / 025 | Total loss: 0.786 | Reg loss: 0.026 | Tree loss: 0.786 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 70 | Batch: 013 / 025 | Total loss: 0.783 | Reg loss: 0.026 | Tree loss: 0.783 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 70 | Batch: 014 / 025 | Total loss: 0.778 | Reg loss: 0.026 | Tree loss: 0.778 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 70 | Batch: 015 / 025 | Total loss: 0.770 | Reg loss: 0.026 | Tree loss: 0.770 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 70 | Batch: 016 / 025 | Total loss: 0.754 | Reg loss: 0.026 | Tree loss: 0.754 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 70 | Batch: 017 / 025 | Total loss: 0.756 | Reg loss: 0.026 | Tree loss: 0.756 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 70 | Batch: 018 / 025 | Total loss: 0.740 | Reg loss: 0.026 | Tree loss: 0.740 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 70 | Batch: 019 / 025 | Total loss: 0.742 | Reg loss: 0.026 | Tree loss: 0.742 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 70 | Batch: 020 / 025 | Total loss: 0.730 | Reg loss: 0.026 | Tree loss: 0.730 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 70 | Batch: 021 / 025 | Total loss: 0.722 | Reg loss: 0.026 | Tree loss: 0.722 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 70 | Batch: 022 / 025 | Total loss: 0.713 | Reg loss: 0.026 | Tree loss: 0.713 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 70 | Batch: 023 / 025 | Total loss: 0.712 | Reg loss: 0.026 | Tree loss: 0.712 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 70 | Batch: 024 / 025 | Total loss: 0.707 | Reg loss: 0.026 | Tree loss: 0.707 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 71 | Batch: 000 / 025 | Total loss: 0.842 | Reg loss: 0.025 | Tree loss: 0.842 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 71 | Batch: 001 / 025 | Total loss: 0.828 | Reg loss: 0.025 | Tree loss: 0.828 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 71 | Batch: 002 / 025 | Total loss: 0.824 | Reg loss: 0.025 | Tree loss: 0.824 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 71 | Batch: 003 / 025 | Total loss: 0.827 | Reg loss: 0.025 | Tree loss: 0.827 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 71 | Batch: 004 / 025 | Total loss: 0.805 | Reg loss: 0.025 | Tree loss: 0.805 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 71 | Batch: 005 / 025 | Total loss: 0.802 | Reg loss: 0.025 | Tree loss: 0.802 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 71 | Batch: 006 / 025 | Total loss: 0.795 | Reg loss: 0.025 | Tree loss: 0.795 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 71 | Batch: 007 / 025 | Total loss: 0.790 | Reg loss: 0.025 | Tree loss: 0.790 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 71 | Batch: 008 / 025 | Total loss: 0.770 | Reg loss: 0.025 | Tree loss: 0.770 | Accuracy: 1.000000 | 0.069 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 71 | Batch: 009 / 025 | Total loss: 0.765 | Reg loss: 0.026 | Tree loss: 0.765 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 71 | Batch: 010 / 025 | Total loss: 0.760 | Reg loss: 0.026 | Tree loss: 0.760 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 71 | Batch: 011 / 025 | Total loss: 0.763 | Reg loss: 0.026 | Tree loss: 0.763 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 71 | Batch: 012 / 025 | Total loss: 0.748 | Reg loss: 0.026 | Tree loss: 0.748 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 71 | Batch: 013 / 025 | Total loss: 0.740 | Reg loss: 0.026 | Tree loss: 0.740 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 71 | Batch: 014 / 025 | Total loss: 0.738 | Reg loss: 0.026 | Tree loss: 0.738 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 71 | Batch: 015 / 025 | Total loss: 0.724 | Reg loss: 0.026 | Tree loss: 0.724 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 71 | Batch: 016 / 025 | Total loss: 0.715 | Reg loss: 0.026 | Tree loss: 0.715 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 71 | Batch: 017 / 025 | Total loss: 0.713 | Reg loss: 0.026 | Tree loss: 0.713 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 71 | Batch: 018 / 025 | Total loss: 0.707 | Reg loss: 0.026 | Tree loss: 0.707 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 71 | Batch: 019 / 025 | Total loss: 0.706 | Reg loss: 0.026 | Tree loss: 0.706 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 71 | Batch: 020 / 025 | Total loss: 0.697 | Reg loss: 0.026 | Tree loss: 0.697 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 71 | Batch: 021 / 025 | Total loss: 0.698 | Reg loss: 0.026 | Tree loss: 0.698 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 71 | Batch: 022 / 025 | Total loss: 0.684 | Reg loss: 0.026 | Tree loss: 0.684 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 71 | Batch: 023 / 025 | Total loss: 0.679 | Reg loss: 0.026 | Tree loss: 0.679 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 71 | Batch: 024 / 025 | Total loss: 0.662 | Reg loss: 0.026 | Tree loss: 0.662 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 72 | Batch: 000 / 025 | Total loss: 0.801 | Reg loss: 0.025 | Tree loss: 0.801 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 72 | Batch: 001 / 025 | Total loss: 0.784 | Reg loss: 0.025 | Tree loss: 0.784 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 72 | Batch: 002 / 025 | Total loss: 0.786 | Reg loss: 0.025 | Tree loss: 0.786 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 72 | Batch: 003 / 025 | Total loss: 0.773 | Reg loss: 0.025 | Tree loss: 0.773 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 72 | Batch: 004 / 025 | Total loss: 0.772 | Reg loss: 0.025 | Tree loss: 0.772 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 72 | Batch: 005 / 025 | Total loss: 0.760 | Reg loss: 0.025 | Tree loss: 0.760 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 72 | Batch: 006 / 025 | Total loss: 0.759 | Reg loss: 0.025 | Tree loss: 0.759 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 72 | Batch: 007 / 025 | Total loss: 0.750 | Reg loss: 0.025 | Tree loss: 0.750 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 72 | Batch: 008 / 025 | Total loss: 0.742 | Reg loss: 0.025 | Tree loss: 0.742 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 72 | Batch: 009 / 025 | Total loss: 0.732 | Reg loss: 0.025 | Tree loss: 0.732 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 72 | Batch: 010 / 025 | Total loss: 0.729 | Reg loss: 0.025 | Tree loss: 0.729 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 72 | Batch: 011 / 025 | Total loss: 0.721 | Reg loss: 0.025 | Tree loss: 0.721 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 72 | Batch: 012 / 025 | Total loss: 0.714 | Reg loss: 0.025 | Tree loss: 0.714 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 72 | Batch: 013 / 025 | Total loss: 0.712 | Reg loss: 0.025 | Tree loss: 0.712 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 72 | Batch: 014 / 025 | Total loss: 0.691 | Reg loss: 0.026 | Tree loss: 0.691 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 72 | Batch: 015 / 025 | Total loss: 0.697 | Reg loss: 0.026 | Tree loss: 0.697 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 72 | Batch: 016 / 025 | Total loss: 0.688 | Reg loss: 0.026 | Tree loss: 0.688 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 72 | Batch: 017 / 025 | Total loss: 0.687 | Reg loss: 0.026 | Tree loss: 0.687 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 72 | Batch: 018 / 025 | Total loss: 0.678 | Reg loss: 0.026 | Tree loss: 0.678 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 72 | Batch: 019 / 025 | Total loss: 0.666 | Reg loss: 0.026 | Tree loss: 0.666 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 72 | Batch: 020 / 025 | Total loss: 0.669 | Reg loss: 0.026 | Tree loss: 0.669 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 72 | Batch: 021 / 025 | Total loss: 0.655 | Reg loss: 0.026 | Tree loss: 0.655 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 72 | Batch: 022 / 025 | Total loss: 0.661 | Reg loss: 0.026 | Tree loss: 0.661 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 72 | Batch: 023 / 025 | Total loss: 0.647 | Reg loss: 0.026 | Tree loss: 0.647 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 72 | Batch: 024 / 025 | Total loss: 0.640 | Reg loss: 0.026 | Tree loss: 0.640 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 73 | Batch: 000 / 025 | Total loss: 0.759 | Reg loss: 0.025 | Tree loss: 0.759 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 73 | Batch: 001 / 025 | Total loss: 0.752 | Reg loss: 0.025 | Tree loss: 0.752 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 73 | Batch: 002 / 025 | Total loss: 0.752 | Reg loss: 0.025 | Tree loss: 0.752 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 73 | Batch: 003 / 025 | Total loss: 0.741 | Reg loss: 0.025 | Tree loss: 0.741 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 73 | Batch: 004 / 025 | Total loss: 0.735 | Reg loss: 0.025 | Tree loss: 0.735 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 73 | Batch: 005 / 025 | Total loss: 0.728 | Reg loss: 0.025 | Tree loss: 0.728 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 73 | Batch: 006 / 025 | Total loss: 0.712 | Reg loss: 0.025 | Tree loss: 0.712 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 73 | Batch: 007 / 025 | Total loss: 0.713 | Reg loss: 0.025 | Tree loss: 0.713 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 73 | Batch: 008 / 025 | Total loss: 0.707 | Reg loss: 0.025 | Tree loss: 0.707 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 73 | Batch: 009 / 025 | Total loss: 0.704 | Reg loss: 0.025 | Tree loss: 0.704 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 73 | Batch: 010 / 025 | Total loss: 0.697 | Reg loss: 0.025 | Tree loss: 0.697 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 73 | Batch: 011 / 025 | Total loss: 0.692 | Reg loss: 0.025 | Tree loss: 0.692 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 73 | Batch: 012 / 025 | Total loss: 0.679 | Reg loss: 0.025 | Tree loss: 0.679 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 73 | Batch: 013 / 025 | Total loss: 0.672 | Reg loss: 0.025 | Tree loss: 0.672 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 73 | Batch: 014 / 025 | Total loss: 0.671 | Reg loss: 0.025 | Tree loss: 0.671 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 73 | Batch: 015 / 025 | Total loss: 0.666 | Reg loss: 0.025 | Tree loss: 0.666 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 73 | Batch: 016 / 025 | Total loss: 0.663 | Reg loss: 0.025 | Tree loss: 0.663 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 73 | Batch: 017 / 025 | Total loss: 0.654 | Reg loss: 0.025 | Tree loss: 0.654 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 73 | Batch: 018 / 025 | Total loss: 0.654 | Reg loss: 0.026 | Tree loss: 0.654 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 73 | Batch: 019 / 025 | Total loss: 0.641 | Reg loss: 0.026 | Tree loss: 0.641 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 73 | Batch: 020 / 025 | Total loss: 0.638 | Reg loss: 0.026 | Tree loss: 0.638 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 73 | Batch: 021 / 025 | Total loss: 0.628 | Reg loss: 0.026 | Tree loss: 0.628 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 73 | Batch: 022 / 025 | Total loss: 0.622 | Reg loss: 0.026 | Tree loss: 0.622 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 73 | Batch: 023 / 025 | Total loss: 0.617 | Reg loss: 0.026 | Tree loss: 0.617 | Accuracy: 1.000000 | 0.069 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 73 | Batch: 024 / 025 | Total loss: 0.608 | Reg loss: 0.026 | Tree loss: 0.608 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 74 | Batch: 000 / 025 | Total loss: 0.723 | Reg loss: 0.025 | Tree loss: 0.723 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 74 | Batch: 001 / 025 | Total loss: 0.727 | Reg loss: 0.025 | Tree loss: 0.727 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 74 | Batch: 002 / 025 | Total loss: 0.710 | Reg loss: 0.025 | Tree loss: 0.710 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 74 | Batch: 003 / 025 | Total loss: 0.707 | Reg loss: 0.025 | Tree loss: 0.707 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 74 | Batch: 004 / 025 | Total loss: 0.701 | Reg loss: 0.025 | Tree loss: 0.701 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 74 | Batch: 005 / 025 | Total loss: 0.699 | Reg loss: 0.025 | Tree loss: 0.699 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 74 | Batch: 006 / 025 | Total loss: 0.687 | Reg loss: 0.025 | Tree loss: 0.687 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 74 | Batch: 007 / 025 | Total loss: 0.686 | Reg loss: 0.025 | Tree loss: 0.686 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 74 | Batch: 008 / 025 | Total loss: 0.674 | Reg loss: 0.025 | Tree loss: 0.674 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 74 | Batch: 009 / 025 | Total loss: 0.670 | Reg loss: 0.025 | Tree loss: 0.670 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 74 | Batch: 010 / 025 | Total loss: 0.665 | Reg loss: 0.025 | Tree loss: 0.665 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 74 | Batch: 011 / 025 | Total loss: 0.655 | Reg loss: 0.025 | Tree loss: 0.655 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 74 | Batch: 012 / 025 | Total loss: 0.655 | Reg loss: 0.025 | Tree loss: 0.655 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 74 | Batch: 013 / 025 | Total loss: 0.650 | Reg loss: 0.025 | Tree loss: 0.650 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 74 | Batch: 014 / 025 | Total loss: 0.637 | Reg loss: 0.025 | Tree loss: 0.637 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 74 | Batch: 015 / 025 | Total loss: 0.632 | Reg loss: 0.025 | Tree loss: 0.632 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 74 | Batch: 016 / 025 | Total loss: 0.630 | Reg loss: 0.025 | Tree loss: 0.630 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 74 | Batch: 017 / 025 | Total loss: 0.621 | Reg loss: 0.025 | Tree loss: 0.621 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 74 | Batch: 018 / 025 | Total loss: 0.620 | Reg loss: 0.025 | Tree loss: 0.620 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 74 | Batch: 019 / 025 | Total loss: 0.614 | Reg loss: 0.025 | Tree loss: 0.614 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 74 | Batch: 020 / 025 | Total loss: 0.603 | Reg loss: 0.025 | Tree loss: 0.603 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 74 | Batch: 021 / 025 | Total loss: 0.608 | Reg loss: 0.026 | Tree loss: 0.608 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 74 | Batch: 022 / 025 | Total loss: 0.604 | Reg loss: 0.026 | Tree loss: 0.604 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 74 | Batch: 023 / 025 | Total loss: 0.588 | Reg loss: 0.026 | Tree loss: 0.588 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 74 | Batch: 024 / 025 | Total loss: 0.586 | Reg loss: 0.026 | Tree loss: 0.586 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 75 | Batch: 000 / 025 | Total loss: 0.692 | Reg loss: 0.025 | Tree loss: 0.692 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 75 | Batch: 001 / 025 | Total loss: 0.690 | Reg loss: 0.025 | Tree loss: 0.690 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 75 | Batch: 002 / 025 | Total loss: 0.683 | Reg loss: 0.025 | Tree loss: 0.683 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 75 | Batch: 003 / 025 | Total loss: 0.675 | Reg loss: 0.025 | Tree loss: 0.675 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 75 | Batch: 004 / 025 | Total loss: 0.671 | Reg loss: 0.025 | Tree loss: 0.671 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 75 | Batch: 005 / 025 | Total loss: 0.661 | Reg loss: 0.025 | Tree loss: 0.661 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 75 | Batch: 006 / 025 | Total loss: 0.660 | Reg loss: 0.025 | Tree loss: 0.660 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 75 | Batch: 007 / 025 | Total loss: 0.660 | Reg loss: 0.025 | Tree loss: 0.660 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 75 | Batch: 008 / 025 | Total loss: 0.646 | Reg loss: 0.025 | Tree loss: 0.646 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 75 | Batch: 009 / 025 | Total loss: 0.639 | Reg loss: 0.025 | Tree loss: 0.639 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 75 | Batch: 010 / 025 | Total loss: 0.631 | Reg loss: 0.025 | Tree loss: 0.631 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 75 | Batch: 011 / 025 | Total loss: 0.633 | Reg loss: 0.025 | Tree loss: 0.633 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 75 | Batch: 012 / 025 | Total loss: 0.628 | Reg loss: 0.025 | Tree loss: 0.628 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 75 | Batch: 013 / 025 | Total loss: 0.614 | Reg loss: 0.025 | Tree loss: 0.614 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 75 | Batch: 014 / 025 | Total loss: 0.619 | Reg loss: 0.025 | Tree loss: 0.619 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 75 | Batch: 015 / 025 | Total loss: 0.611 | Reg loss: 0.025 | Tree loss: 0.611 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 75 | Batch: 016 / 025 | Total loss: 0.603 | Reg loss: 0.025 | Tree loss: 0.603 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 75 | Batch: 017 / 025 | Total loss: 0.598 | Reg loss: 0.025 | Tree loss: 0.598 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 75 | Batch: 018 / 025 | Total loss: 0.595 | Reg loss: 0.025 | Tree loss: 0.595 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 75 | Batch: 019 / 025 | Total loss: 0.581 | Reg loss: 0.025 | Tree loss: 0.581 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 75 | Batch: 020 / 025 | Total loss: 0.586 | Reg loss: 0.025 | Tree loss: 0.586 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 75 | Batch: 021 / 025 | Total loss: 0.577 | Reg loss: 0.025 | Tree loss: 0.577 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 75 | Batch: 022 / 025 | Total loss: 0.570 | Reg loss: 0.025 | Tree loss: 0.570 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 75 | Batch: 023 / 025 | Total loss: 0.566 | Reg loss: 0.025 | Tree loss: 0.566 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 75 | Batch: 024 / 025 | Total loss: 0.556 | Reg loss: 0.025 | Tree loss: 0.556 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 76 | Batch: 000 / 025 | Total loss: 0.671 | Reg loss: 0.025 | Tree loss: 0.671 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 76 | Batch: 001 / 025 | Total loss: 0.655 | Reg loss: 0.025 | Tree loss: 0.655 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 76 | Batch: 002 / 025 | Total loss: 0.656 | Reg loss: 0.025 | Tree loss: 0.656 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 76 | Batch: 003 / 025 | Total loss: 0.648 | Reg loss: 0.025 | Tree loss: 0.648 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 76 | Batch: 004 / 025 | Total loss: 0.637 | Reg loss: 0.025 | Tree loss: 0.637 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 76 | Batch: 005 / 025 | Total loss: 0.633 | Reg loss: 0.025 | Tree loss: 0.633 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 76 | Batch: 006 / 025 | Total loss: 0.633 | Reg loss: 0.025 | Tree loss: 0.633 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 76 | Batch: 007 / 025 | Total loss: 0.625 | Reg loss: 0.025 | Tree loss: 0.625 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 76 | Batch: 008 / 025 | Total loss: 0.623 | Reg loss: 0.025 | Tree loss: 0.623 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 76 | Batch: 009 / 025 | Total loss: 0.615 | Reg loss: 0.025 | Tree loss: 0.615 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 76 | Batch: 010 / 025 | Total loss: 0.606 | Reg loss: 0.025 | Tree loss: 0.606 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 76 | Batch: 011 / 025 | Total loss: 0.603 | Reg loss: 0.025 | Tree loss: 0.603 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 76 | Batch: 012 / 025 | Total loss: 0.602 | Reg loss: 0.025 | Tree loss: 0.602 | Accuracy: 1.000000 | 0.069 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 76 | Batch: 013 / 025 | Total loss: 0.600 | Reg loss: 0.025 | Tree loss: 0.600 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 76 | Batch: 014 / 025 | Total loss: 0.586 | Reg loss: 0.025 | Tree loss: 0.586 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 76 | Batch: 015 / 025 | Total loss: 0.575 | Reg loss: 0.025 | Tree loss: 0.575 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 76 | Batch: 016 / 025 | Total loss: 0.576 | Reg loss: 0.025 | Tree loss: 0.576 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 76 | Batch: 017 / 025 | Total loss: 0.574 | Reg loss: 0.025 | Tree loss: 0.574 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 76 | Batch: 018 / 025 | Total loss: 0.568 | Reg loss: 0.025 | Tree loss: 0.568 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 76 | Batch: 019 / 025 | Total loss: 0.559 | Reg loss: 0.025 | Tree loss: 0.559 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 76 | Batch: 020 / 025 | Total loss: 0.560 | Reg loss: 0.025 | Tree loss: 0.560 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 76 | Batch: 021 / 025 | Total loss: 0.553 | Reg loss: 0.025 | Tree loss: 0.553 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 76 | Batch: 022 / 025 | Total loss: 0.549 | Reg loss: 0.025 | Tree loss: 0.549 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 76 | Batch: 023 / 025 | Total loss: 0.540 | Reg loss: 0.025 | Tree loss: 0.540 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 76 | Batch: 024 / 025 | Total loss: 0.544 | Reg loss: 0.025 | Tree loss: 0.544 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 77 | Batch: 000 / 025 | Total loss: 0.639 | Reg loss: 0.025 | Tree loss: 0.639 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 77 | Batch: 001 / 025 | Total loss: 0.638 | Reg loss: 0.025 | Tree loss: 0.638 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 77 | Batch: 002 / 025 | Total loss: 0.623 | Reg loss: 0.025 | Tree loss: 0.623 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 77 | Batch: 003 / 025 | Total loss: 0.620 | Reg loss: 0.025 | Tree loss: 0.620 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 77 | Batch: 004 / 025 | Total loss: 0.616 | Reg loss: 0.025 | Tree loss: 0.616 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 77 | Batch: 005 / 025 | Total loss: 0.609 | Reg loss: 0.025 | Tree loss: 0.609 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 77 | Batch: 006 / 025 | Total loss: 0.602 | Reg loss: 0.025 | Tree loss: 0.602 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 77 | Batch: 007 / 025 | Total loss: 0.594 | Reg loss: 0.025 | Tree loss: 0.594 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 77 | Batch: 008 / 025 | Total loss: 0.593 | Reg loss: 0.025 | Tree loss: 0.593 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 77 | Batch: 009 / 025 | Total loss: 0.587 | Reg loss: 0.025 | Tree loss: 0.587 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 77 | Batch: 010 / 025 | Total loss: 0.584 | Reg loss: 0.025 | Tree loss: 0.584 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 77 | Batch: 011 / 025 | Total loss: 0.577 | Reg loss: 0.025 | Tree loss: 0.577 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 77 | Batch: 012 / 025 | Total loss: 0.573 | Reg loss: 0.025 | Tree loss: 0.573 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 77 | Batch: 013 / 025 | Total loss: 0.570 | Reg loss: 0.025 | Tree loss: 0.570 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 77 | Batch: 014 / 025 | Total loss: 0.562 | Reg loss: 0.025 | Tree loss: 0.562 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 77 | Batch: 015 / 025 | Total loss: 0.558 | Reg loss: 0.025 | Tree loss: 0.558 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 77 | Batch: 016 / 025 | Total loss: 0.555 | Reg loss: 0.025 | Tree loss: 0.555 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 77 | Batch: 017 / 025 | Total loss: 0.550 | Reg loss: 0.025 | Tree loss: 0.550 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 77 | Batch: 018 / 025 | Total loss: 0.545 | Reg loss: 0.025 | Tree loss: 0.545 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 77 | Batch: 019 / 025 | Total loss: 0.540 | Reg loss: 0.025 | Tree loss: 0.540 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 77 | Batch: 020 / 025 | Total loss: 0.537 | Reg loss: 0.025 | Tree loss: 0.537 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 77 | Batch: 021 / 025 | Total loss: 0.532 | Reg loss: 0.025 | Tree loss: 0.532 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 77 | Batch: 022 / 025 | Total loss: 0.525 | Reg loss: 0.025 | Tree loss: 0.525 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 77 | Batch: 023 / 025 | Total loss: 0.518 | Reg loss: 0.025 | Tree loss: 0.518 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 77 | Batch: 024 / 025 | Total loss: 0.520 | Reg loss: 0.025 | Tree loss: 0.520 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 78 | Batch: 000 / 025 | Total loss: 0.611 | Reg loss: 0.025 | Tree loss: 0.611 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 78 | Batch: 001 / 025 | Total loss: 0.608 | Reg loss: 0.025 | Tree loss: 0.608 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 78 | Batch: 002 / 025 | Total loss: 0.598 | Reg loss: 0.025 | Tree loss: 0.598 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 78 | Batch: 003 / 025 | Total loss: 0.593 | Reg loss: 0.025 | Tree loss: 0.593 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 78 | Batch: 004 / 025 | Total loss: 0.591 | Reg loss: 0.025 | Tree loss: 0.591 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 78 | Batch: 005 / 025 | Total loss: 0.590 | Reg loss: 0.025 | Tree loss: 0.590 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 78 | Batch: 006 / 025 | Total loss: 0.585 | Reg loss: 0.025 | Tree loss: 0.585 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 78 | Batch: 007 / 025 | Total loss: 0.571 | Reg loss: 0.025 | Tree loss: 0.571 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 78 | Batch: 008 / 025 | Total loss: 0.574 | Reg loss: 0.025 | Tree loss: 0.574 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 78 | Batch: 009 / 025 | Total loss: 0.567 | Reg loss: 0.025 | Tree loss: 0.567 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 78 | Batch: 010 / 025 | Total loss: 0.562 | Reg loss: 0.025 | Tree loss: 0.562 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 78 | Batch: 011 / 025 | Total loss: 0.555 | Reg loss: 0.025 | Tree loss: 0.555 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 78 | Batch: 012 / 025 | Total loss: 0.545 | Reg loss: 0.025 | Tree loss: 0.545 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 78 | Batch: 013 / 025 | Total loss: 0.543 | Reg loss: 0.025 | Tree loss: 0.543 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 78 | Batch: 014 / 025 | Total loss: 0.543 | Reg loss: 0.025 | Tree loss: 0.543 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 78 | Batch: 015 / 025 | Total loss: 0.533 | Reg loss: 0.025 | Tree loss: 0.533 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 78 | Batch: 016 / 025 | Total loss: 0.528 | Reg loss: 0.025 | Tree loss: 0.528 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 78 | Batch: 017 / 025 | Total loss: 0.526 | Reg loss: 0.025 | Tree loss: 0.526 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 78 | Batch: 018 / 025 | Total loss: 0.520 | Reg loss: 0.025 | Tree loss: 0.520 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 78 | Batch: 019 / 025 | Total loss: 0.516 | Reg loss: 0.025 | Tree loss: 0.516 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 78 | Batch: 020 / 025 | Total loss: 0.515 | Reg loss: 0.025 | Tree loss: 0.515 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 78 | Batch: 021 / 025 | Total loss: 0.506 | Reg loss: 0.025 | Tree loss: 0.506 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 78 | Batch: 022 / 025 | Total loss: 0.500 | Reg loss: 0.025 | Tree loss: 0.500 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 78 | Batch: 023 / 025 | Total loss: 0.506 | Reg loss: 0.025 | Tree loss: 0.506 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 78 | Batch: 024 / 025 | Total loss: 0.499 | Reg loss: 0.025 | Tree loss: 0.499 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 79 | Batch: 000 / 025 | Total loss: 0.579 | Reg loss: 0.025 | Tree loss: 0.579 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 79 | Batch: 001 / 025 | Total loss: 0.581 | Reg loss: 0.025 | Tree loss: 0.581 | Accuracy: 1.000000 | 0.069 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 79 | Batch: 002 / 025 | Total loss: 0.582 | Reg loss: 0.025 | Tree loss: 0.582 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 79 | Batch: 003 / 025 | Total loss: 0.570 | Reg loss: 0.025 | Tree loss: 0.570 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 79 | Batch: 004 / 025 | Total loss: 0.559 | Reg loss: 0.025 | Tree loss: 0.559 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 79 | Batch: 005 / 025 | Total loss: 0.570 | Reg loss: 0.025 | Tree loss: 0.570 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 79 | Batch: 006 / 025 | Total loss: 0.557 | Reg loss: 0.025 | Tree loss: 0.557 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 79 | Batch: 007 / 025 | Total loss: 0.548 | Reg loss: 0.025 | Tree loss: 0.548 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 79 | Batch: 008 / 025 | Total loss: 0.548 | Reg loss: 0.025 | Tree loss: 0.548 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 79 | Batch: 009 / 025 | Total loss: 0.542 | Reg loss: 0.025 | Tree loss: 0.542 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 79 | Batch: 010 / 025 | Total loss: 0.542 | Reg loss: 0.025 | Tree loss: 0.542 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 79 | Batch: 011 / 025 | Total loss: 0.532 | Reg loss: 0.025 | Tree loss: 0.532 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 79 | Batch: 012 / 025 | Total loss: 0.532 | Reg loss: 0.025 | Tree loss: 0.532 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 79 | Batch: 013 / 025 | Total loss: 0.525 | Reg loss: 0.025 | Tree loss: 0.525 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 79 | Batch: 014 / 025 | Total loss: 0.513 | Reg loss: 0.025 | Tree loss: 0.513 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 79 | Batch: 015 / 025 | Total loss: 0.513 | Reg loss: 0.025 | Tree loss: 0.513 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 79 | Batch: 016 / 025 | Total loss: 0.507 | Reg loss: 0.025 | Tree loss: 0.507 | Accuracy: 1.000000 | 0.069 sec/iter\n",
      "Epoch: 79 | Batch: 017 / 025 | Total loss: 0.506 | Reg loss: 0.025 | Tree loss: 0.506 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 79 | Batch: 018 / 025 | Total loss: 0.497 | Reg loss: 0.025 | Tree loss: 0.497 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 79 | Batch: 019 / 025 | Total loss: 0.500 | Reg loss: 0.025 | Tree loss: 0.500 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 79 | Batch: 020 / 025 | Total loss: 0.490 | Reg loss: 0.025 | Tree loss: 0.490 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 79 | Batch: 021 / 025 | Total loss: 0.493 | Reg loss: 0.025 | Tree loss: 0.493 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 79 | Batch: 022 / 025 | Total loss: 0.485 | Reg loss: 0.025 | Tree loss: 0.485 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 79 | Batch: 023 / 025 | Total loss: 0.484 | Reg loss: 0.025 | Tree loss: 0.484 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 79 | Batch: 024 / 025 | Total loss: 0.483 | Reg loss: 0.025 | Tree loss: 0.483 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 80 | Batch: 000 / 025 | Total loss: 0.563 | Reg loss: 0.025 | Tree loss: 0.563 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 80 | Batch: 001 / 025 | Total loss: 0.558 | Reg loss: 0.025 | Tree loss: 0.558 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 80 | Batch: 002 / 025 | Total loss: 0.546 | Reg loss: 0.025 | Tree loss: 0.546 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 80 | Batch: 003 / 025 | Total loss: 0.544 | Reg loss: 0.025 | Tree loss: 0.544 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 80 | Batch: 004 / 025 | Total loss: 0.552 | Reg loss: 0.025 | Tree loss: 0.552 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 80 | Batch: 005 / 025 | Total loss: 0.539 | Reg loss: 0.025 | Tree loss: 0.539 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 80 | Batch: 006 / 025 | Total loss: 0.537 | Reg loss: 0.025 | Tree loss: 0.537 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 80 | Batch: 007 / 025 | Total loss: 0.532 | Reg loss: 0.025 | Tree loss: 0.532 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 80 | Batch: 008 / 025 | Total loss: 0.524 | Reg loss: 0.025 | Tree loss: 0.524 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 80 | Batch: 009 / 025 | Total loss: 0.524 | Reg loss: 0.025 | Tree loss: 0.524 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 80 | Batch: 010 / 025 | Total loss: 0.518 | Reg loss: 0.025 | Tree loss: 0.518 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 80 | Batch: 011 / 025 | Total loss: 0.513 | Reg loss: 0.025 | Tree loss: 0.513 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 80 | Batch: 012 / 025 | Total loss: 0.506 | Reg loss: 0.025 | Tree loss: 0.506 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 80 | Batch: 013 / 025 | Total loss: 0.503 | Reg loss: 0.025 | Tree loss: 0.503 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 80 | Batch: 014 / 025 | Total loss: 0.505 | Reg loss: 0.025 | Tree loss: 0.505 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 80 | Batch: 015 / 025 | Total loss: 0.492 | Reg loss: 0.025 | Tree loss: 0.492 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 80 | Batch: 016 / 025 | Total loss: 0.491 | Reg loss: 0.025 | Tree loss: 0.491 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 80 | Batch: 017 / 025 | Total loss: 0.482 | Reg loss: 0.025 | Tree loss: 0.482 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 80 | Batch: 018 / 025 | Total loss: 0.479 | Reg loss: 0.025 | Tree loss: 0.479 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 80 | Batch: 019 / 025 | Total loss: 0.475 | Reg loss: 0.025 | Tree loss: 0.475 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 80 | Batch: 020 / 025 | Total loss: 0.481 | Reg loss: 0.025 | Tree loss: 0.481 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 80 | Batch: 021 / 025 | Total loss: 0.467 | Reg loss: 0.025 | Tree loss: 0.467 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 80 | Batch: 022 / 025 | Total loss: 0.470 | Reg loss: 0.025 | Tree loss: 0.470 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 80 | Batch: 023 / 025 | Total loss: 0.464 | Reg loss: 0.025 | Tree loss: 0.464 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 80 | Batch: 024 / 025 | Total loss: 0.456 | Reg loss: 0.025 | Tree loss: 0.456 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 81 | Batch: 000 / 025 | Total loss: 0.541 | Reg loss: 0.024 | Tree loss: 0.541 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 81 | Batch: 001 / 025 | Total loss: 0.540 | Reg loss: 0.024 | Tree loss: 0.540 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 81 | Batch: 002 / 025 | Total loss: 0.529 | Reg loss: 0.024 | Tree loss: 0.529 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 81 | Batch: 003 / 025 | Total loss: 0.524 | Reg loss: 0.024 | Tree loss: 0.524 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 81 | Batch: 004 / 025 | Total loss: 0.521 | Reg loss: 0.024 | Tree loss: 0.521 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 81 | Batch: 005 / 025 | Total loss: 0.519 | Reg loss: 0.024 | Tree loss: 0.519 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 81 | Batch: 006 / 025 | Total loss: 0.515 | Reg loss: 0.024 | Tree loss: 0.515 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 81 | Batch: 007 / 025 | Total loss: 0.512 | Reg loss: 0.024 | Tree loss: 0.512 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 81 | Batch: 008 / 025 | Total loss: 0.501 | Reg loss: 0.024 | Tree loss: 0.501 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 81 | Batch: 009 / 025 | Total loss: 0.502 | Reg loss: 0.024 | Tree loss: 0.502 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 81 | Batch: 010 / 025 | Total loss: 0.499 | Reg loss: 0.024 | Tree loss: 0.499 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 81 | Batch: 011 / 025 | Total loss: 0.494 | Reg loss: 0.024 | Tree loss: 0.494 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 81 | Batch: 012 / 025 | Total loss: 0.488 | Reg loss: 0.024 | Tree loss: 0.488 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 81 | Batch: 013 / 025 | Total loss: 0.480 | Reg loss: 0.024 | Tree loss: 0.480 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 81 | Batch: 014 / 025 | Total loss: 0.486 | Reg loss: 0.025 | Tree loss: 0.486 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 81 | Batch: 015 / 025 | Total loss: 0.477 | Reg loss: 0.025 | Tree loss: 0.477 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 81 | Batch: 016 / 025 | Total loss: 0.478 | Reg loss: 0.025 | Tree loss: 0.478 | Accuracy: 1.000000 | 0.068 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 81 | Batch: 017 / 025 | Total loss: 0.463 | Reg loss: 0.025 | Tree loss: 0.463 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 81 | Batch: 018 / 025 | Total loss: 0.465 | Reg loss: 0.025 | Tree loss: 0.465 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 81 | Batch: 019 / 025 | Total loss: 0.463 | Reg loss: 0.025 | Tree loss: 0.463 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 81 | Batch: 020 / 025 | Total loss: 0.453 | Reg loss: 0.025 | Tree loss: 0.453 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 81 | Batch: 021 / 025 | Total loss: 0.454 | Reg loss: 0.025 | Tree loss: 0.454 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 81 | Batch: 022 / 025 | Total loss: 0.453 | Reg loss: 0.025 | Tree loss: 0.453 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 81 | Batch: 023 / 025 | Total loss: 0.444 | Reg loss: 0.025 | Tree loss: 0.444 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 81 | Batch: 024 / 025 | Total loss: 0.441 | Reg loss: 0.025 | Tree loss: 0.441 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 82 | Batch: 000 / 025 | Total loss: 0.521 | Reg loss: 0.024 | Tree loss: 0.521 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 82 | Batch: 001 / 025 | Total loss: 0.515 | Reg loss: 0.024 | Tree loss: 0.515 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 82 | Batch: 002 / 025 | Total loss: 0.511 | Reg loss: 0.024 | Tree loss: 0.511 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 82 | Batch: 003 / 025 | Total loss: 0.505 | Reg loss: 0.024 | Tree loss: 0.505 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 82 | Batch: 004 / 025 | Total loss: 0.504 | Reg loss: 0.024 | Tree loss: 0.504 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 82 | Batch: 005 / 025 | Total loss: 0.498 | Reg loss: 0.024 | Tree loss: 0.498 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 82 | Batch: 006 / 025 | Total loss: 0.492 | Reg loss: 0.024 | Tree loss: 0.492 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 82 | Batch: 007 / 025 | Total loss: 0.494 | Reg loss: 0.024 | Tree loss: 0.494 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 82 | Batch: 008 / 025 | Total loss: 0.483 | Reg loss: 0.024 | Tree loss: 0.483 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 82 | Batch: 009 / 025 | Total loss: 0.485 | Reg loss: 0.024 | Tree loss: 0.485 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 82 | Batch: 010 / 025 | Total loss: 0.475 | Reg loss: 0.024 | Tree loss: 0.475 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 82 | Batch: 011 / 025 | Total loss: 0.480 | Reg loss: 0.024 | Tree loss: 0.480 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 82 | Batch: 012 / 025 | Total loss: 0.470 | Reg loss: 0.024 | Tree loss: 0.470 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 82 | Batch: 013 / 025 | Total loss: 0.472 | Reg loss: 0.024 | Tree loss: 0.472 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 82 | Batch: 014 / 025 | Total loss: 0.461 | Reg loss: 0.024 | Tree loss: 0.461 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 82 | Batch: 015 / 025 | Total loss: 0.459 | Reg loss: 0.024 | Tree loss: 0.459 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 82 | Batch: 016 / 025 | Total loss: 0.457 | Reg loss: 0.024 | Tree loss: 0.457 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 82 | Batch: 017 / 025 | Total loss: 0.450 | Reg loss: 0.024 | Tree loss: 0.450 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 82 | Batch: 018 / 025 | Total loss: 0.443 | Reg loss: 0.024 | Tree loss: 0.443 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 82 | Batch: 019 / 025 | Total loss: 0.442 | Reg loss: 0.025 | Tree loss: 0.442 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 82 | Batch: 020 / 025 | Total loss: 0.441 | Reg loss: 0.025 | Tree loss: 0.441 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 82 | Batch: 021 / 025 | Total loss: 0.437 | Reg loss: 0.025 | Tree loss: 0.437 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 82 | Batch: 022 / 025 | Total loss: 0.439 | Reg loss: 0.025 | Tree loss: 0.439 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 82 | Batch: 023 / 025 | Total loss: 0.430 | Reg loss: 0.025 | Tree loss: 0.430 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 82 | Batch: 024 / 025 | Total loss: 0.423 | Reg loss: 0.025 | Tree loss: 0.423 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 83 | Batch: 000 / 025 | Total loss: 0.494 | Reg loss: 0.024 | Tree loss: 0.494 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 83 | Batch: 001 / 025 | Total loss: 0.497 | Reg loss: 0.024 | Tree loss: 0.497 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 83 | Batch: 002 / 025 | Total loss: 0.494 | Reg loss: 0.024 | Tree loss: 0.494 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 83 | Batch: 003 / 025 | Total loss: 0.491 | Reg loss: 0.024 | Tree loss: 0.491 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 83 | Batch: 004 / 025 | Total loss: 0.484 | Reg loss: 0.024 | Tree loss: 0.484 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 83 | Batch: 005 / 025 | Total loss: 0.486 | Reg loss: 0.024 | Tree loss: 0.486 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 83 | Batch: 006 / 025 | Total loss: 0.485 | Reg loss: 0.024 | Tree loss: 0.485 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 83 | Batch: 007 / 025 | Total loss: 0.474 | Reg loss: 0.024 | Tree loss: 0.474 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 83 | Batch: 008 / 025 | Total loss: 0.468 | Reg loss: 0.024 | Tree loss: 0.468 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 83 | Batch: 009 / 025 | Total loss: 0.466 | Reg loss: 0.024 | Tree loss: 0.466 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 83 | Batch: 010 / 025 | Total loss: 0.457 | Reg loss: 0.024 | Tree loss: 0.457 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 83 | Batch: 011 / 025 | Total loss: 0.460 | Reg loss: 0.024 | Tree loss: 0.460 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 83 | Batch: 012 / 025 | Total loss: 0.450 | Reg loss: 0.024 | Tree loss: 0.450 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 83 | Batch: 013 / 025 | Total loss: 0.448 | Reg loss: 0.024 | Tree loss: 0.448 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 83 | Batch: 014 / 025 | Total loss: 0.440 | Reg loss: 0.024 | Tree loss: 0.440 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 83 | Batch: 015 / 025 | Total loss: 0.438 | Reg loss: 0.024 | Tree loss: 0.438 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 83 | Batch: 016 / 025 | Total loss: 0.444 | Reg loss: 0.024 | Tree loss: 0.444 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 83 | Batch: 017 / 025 | Total loss: 0.434 | Reg loss: 0.024 | Tree loss: 0.434 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 83 | Batch: 018 / 025 | Total loss: 0.431 | Reg loss: 0.024 | Tree loss: 0.431 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 83 | Batch: 019 / 025 | Total loss: 0.430 | Reg loss: 0.024 | Tree loss: 0.430 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 83 | Batch: 020 / 025 | Total loss: 0.426 | Reg loss: 0.024 | Tree loss: 0.426 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 83 | Batch: 021 / 025 | Total loss: 0.422 | Reg loss: 0.024 | Tree loss: 0.422 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 83 | Batch: 022 / 025 | Total loss: 0.419 | Reg loss: 0.024 | Tree loss: 0.419 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 83 | Batch: 023 / 025 | Total loss: 0.416 | Reg loss: 0.024 | Tree loss: 0.416 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 83 | Batch: 024 / 025 | Total loss: 0.408 | Reg loss: 0.025 | Tree loss: 0.408 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 84 | Batch: 000 / 025 | Total loss: 0.483 | Reg loss: 0.024 | Tree loss: 0.483 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 84 | Batch: 001 / 025 | Total loss: 0.475 | Reg loss: 0.024 | Tree loss: 0.475 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 84 | Batch: 002 / 025 | Total loss: 0.477 | Reg loss: 0.024 | Tree loss: 0.477 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 84 | Batch: 003 / 025 | Total loss: 0.477 | Reg loss: 0.024 | Tree loss: 0.477 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 84 | Batch: 004 / 025 | Total loss: 0.471 | Reg loss: 0.024 | Tree loss: 0.471 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 84 | Batch: 005 / 025 | Total loss: 0.461 | Reg loss: 0.024 | Tree loss: 0.461 | Accuracy: 1.000000 | 0.068 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 84 | Batch: 006 / 025 | Total loss: 0.457 | Reg loss: 0.024 | Tree loss: 0.457 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 84 | Batch: 007 / 025 | Total loss: 0.457 | Reg loss: 0.024 | Tree loss: 0.457 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 84 | Batch: 008 / 025 | Total loss: 0.451 | Reg loss: 0.024 | Tree loss: 0.451 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 84 | Batch: 009 / 025 | Total loss: 0.451 | Reg loss: 0.024 | Tree loss: 0.451 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 84 | Batch: 010 / 025 | Total loss: 0.444 | Reg loss: 0.024 | Tree loss: 0.444 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 84 | Batch: 011 / 025 | Total loss: 0.440 | Reg loss: 0.024 | Tree loss: 0.440 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 84 | Batch: 012 / 025 | Total loss: 0.439 | Reg loss: 0.024 | Tree loss: 0.439 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 84 | Batch: 013 / 025 | Total loss: 0.436 | Reg loss: 0.024 | Tree loss: 0.436 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 84 | Batch: 014 / 025 | Total loss: 0.427 | Reg loss: 0.024 | Tree loss: 0.427 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 84 | Batch: 015 / 025 | Total loss: 0.432 | Reg loss: 0.024 | Tree loss: 0.432 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 84 | Batch: 016 / 025 | Total loss: 0.425 | Reg loss: 0.024 | Tree loss: 0.425 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 84 | Batch: 017 / 025 | Total loss: 0.426 | Reg loss: 0.024 | Tree loss: 0.426 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 84 | Batch: 018 / 025 | Total loss: 0.407 | Reg loss: 0.024 | Tree loss: 0.407 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 84 | Batch: 019 / 025 | Total loss: 0.412 | Reg loss: 0.024 | Tree loss: 0.412 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 84 | Batch: 020 / 025 | Total loss: 0.412 | Reg loss: 0.024 | Tree loss: 0.412 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 84 | Batch: 021 / 025 | Total loss: 0.409 | Reg loss: 0.024 | Tree loss: 0.409 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 84 | Batch: 022 / 025 | Total loss: 0.398 | Reg loss: 0.024 | Tree loss: 0.398 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 84 | Batch: 023 / 025 | Total loss: 0.397 | Reg loss: 0.024 | Tree loss: 0.397 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 84 | Batch: 024 / 025 | Total loss: 0.394 | Reg loss: 0.024 | Tree loss: 0.394 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 85 | Batch: 000 / 025 | Total loss: 0.468 | Reg loss: 0.024 | Tree loss: 0.468 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 85 | Batch: 001 / 025 | Total loss: 0.458 | Reg loss: 0.024 | Tree loss: 0.458 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 85 | Batch: 002 / 025 | Total loss: 0.456 | Reg loss: 0.024 | Tree loss: 0.456 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 85 | Batch: 003 / 025 | Total loss: 0.457 | Reg loss: 0.024 | Tree loss: 0.457 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 85 | Batch: 004 / 025 | Total loss: 0.453 | Reg loss: 0.024 | Tree loss: 0.453 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 85 | Batch: 005 / 025 | Total loss: 0.446 | Reg loss: 0.024 | Tree loss: 0.446 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 85 | Batch: 006 / 025 | Total loss: 0.442 | Reg loss: 0.024 | Tree loss: 0.442 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 85 | Batch: 007 / 025 | Total loss: 0.436 | Reg loss: 0.024 | Tree loss: 0.436 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 85 | Batch: 008 / 025 | Total loss: 0.440 | Reg loss: 0.024 | Tree loss: 0.440 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 85 | Batch: 009 / 025 | Total loss: 0.430 | Reg loss: 0.024 | Tree loss: 0.430 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 85 | Batch: 010 / 025 | Total loss: 0.440 | Reg loss: 0.024 | Tree loss: 0.440 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 85 | Batch: 011 / 025 | Total loss: 0.422 | Reg loss: 0.024 | Tree loss: 0.422 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 85 | Batch: 012 / 025 | Total loss: 0.424 | Reg loss: 0.024 | Tree loss: 0.424 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 85 | Batch: 013 / 025 | Total loss: 0.416 | Reg loss: 0.024 | Tree loss: 0.416 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 85 | Batch: 014 / 025 | Total loss: 0.417 | Reg loss: 0.024 | Tree loss: 0.417 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 85 | Batch: 015 / 025 | Total loss: 0.411 | Reg loss: 0.024 | Tree loss: 0.411 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 85 | Batch: 016 / 025 | Total loss: 0.408 | Reg loss: 0.024 | Tree loss: 0.408 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 85 | Batch: 017 / 025 | Total loss: 0.409 | Reg loss: 0.024 | Tree loss: 0.409 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 85 | Batch: 018 / 025 | Total loss: 0.402 | Reg loss: 0.024 | Tree loss: 0.402 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 85 | Batch: 019 / 025 | Total loss: 0.394 | Reg loss: 0.024 | Tree loss: 0.394 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 85 | Batch: 020 / 025 | Total loss: 0.399 | Reg loss: 0.024 | Tree loss: 0.399 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 85 | Batch: 021 / 025 | Total loss: 0.390 | Reg loss: 0.024 | Tree loss: 0.390 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 85 | Batch: 022 / 025 | Total loss: 0.391 | Reg loss: 0.024 | Tree loss: 0.391 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 85 | Batch: 023 / 025 | Total loss: 0.385 | Reg loss: 0.024 | Tree loss: 0.385 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 85 | Batch: 024 / 025 | Total loss: 0.390 | Reg loss: 0.024 | Tree loss: 0.390 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 86 | Batch: 000 / 025 | Total loss: 0.454 | Reg loss: 0.024 | Tree loss: 0.454 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 86 | Batch: 001 / 025 | Total loss: 0.448 | Reg loss: 0.024 | Tree loss: 0.448 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 86 | Batch: 002 / 025 | Total loss: 0.443 | Reg loss: 0.024 | Tree loss: 0.443 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 86 | Batch: 003 / 025 | Total loss: 0.443 | Reg loss: 0.024 | Tree loss: 0.443 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 86 | Batch: 004 / 025 | Total loss: 0.433 | Reg loss: 0.024 | Tree loss: 0.433 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 86 | Batch: 005 / 025 | Total loss: 0.429 | Reg loss: 0.024 | Tree loss: 0.429 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 86 | Batch: 006 / 025 | Total loss: 0.431 | Reg loss: 0.024 | Tree loss: 0.431 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 86 | Batch: 007 / 025 | Total loss: 0.426 | Reg loss: 0.024 | Tree loss: 0.426 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 86 | Batch: 008 / 025 | Total loss: 0.425 | Reg loss: 0.024 | Tree loss: 0.425 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 86 | Batch: 009 / 025 | Total loss: 0.414 | Reg loss: 0.024 | Tree loss: 0.414 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 86 | Batch: 010 / 025 | Total loss: 0.416 | Reg loss: 0.024 | Tree loss: 0.416 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 86 | Batch: 011 / 025 | Total loss: 0.411 | Reg loss: 0.024 | Tree loss: 0.411 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 86 | Batch: 012 / 025 | Total loss: 0.405 | Reg loss: 0.024 | Tree loss: 0.405 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 86 | Batch: 013 / 025 | Total loss: 0.405 | Reg loss: 0.024 | Tree loss: 0.405 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 86 | Batch: 014 / 025 | Total loss: 0.404 | Reg loss: 0.024 | Tree loss: 0.404 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 86 | Batch: 015 / 025 | Total loss: 0.401 | Reg loss: 0.024 | Tree loss: 0.401 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 86 | Batch: 016 / 025 | Total loss: 0.394 | Reg loss: 0.024 | Tree loss: 0.394 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 86 | Batch: 017 / 025 | Total loss: 0.393 | Reg loss: 0.024 | Tree loss: 0.393 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 86 | Batch: 018 / 025 | Total loss: 0.388 | Reg loss: 0.024 | Tree loss: 0.388 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 86 | Batch: 019 / 025 | Total loss: 0.387 | Reg loss: 0.024 | Tree loss: 0.387 | Accuracy: 1.000000 | 0.068 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 86 | Batch: 020 / 025 | Total loss: 0.380 | Reg loss: 0.024 | Tree loss: 0.380 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 86 | Batch: 021 / 025 | Total loss: 0.381 | Reg loss: 0.024 | Tree loss: 0.381 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 86 | Batch: 022 / 025 | Total loss: 0.374 | Reg loss: 0.024 | Tree loss: 0.374 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 86 | Batch: 023 / 025 | Total loss: 0.374 | Reg loss: 0.024 | Tree loss: 0.374 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 86 | Batch: 024 / 025 | Total loss: 0.372 | Reg loss: 0.024 | Tree loss: 0.372 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 87 | Batch: 000 / 025 | Total loss: 0.434 | Reg loss: 0.024 | Tree loss: 0.434 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 87 | Batch: 001 / 025 | Total loss: 0.433 | Reg loss: 0.024 | Tree loss: 0.433 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 87 | Batch: 002 / 025 | Total loss: 0.439 | Reg loss: 0.024 | Tree loss: 0.439 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 87 | Batch: 003 / 025 | Total loss: 0.422 | Reg loss: 0.024 | Tree loss: 0.422 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 87 | Batch: 004 / 025 | Total loss: 0.425 | Reg loss: 0.024 | Tree loss: 0.425 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 87 | Batch: 005 / 025 | Total loss: 0.415 | Reg loss: 0.024 | Tree loss: 0.415 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 87 | Batch: 006 / 025 | Total loss: 0.420 | Reg loss: 0.024 | Tree loss: 0.420 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 87 | Batch: 007 / 025 | Total loss: 0.410 | Reg loss: 0.024 | Tree loss: 0.410 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 87 | Batch: 008 / 025 | Total loss: 0.408 | Reg loss: 0.024 | Tree loss: 0.408 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 87 | Batch: 009 / 025 | Total loss: 0.399 | Reg loss: 0.024 | Tree loss: 0.399 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 87 | Batch: 010 / 025 | Total loss: 0.406 | Reg loss: 0.024 | Tree loss: 0.406 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 87 | Batch: 011 / 025 | Total loss: 0.401 | Reg loss: 0.024 | Tree loss: 0.401 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 87 | Batch: 012 / 025 | Total loss: 0.394 | Reg loss: 0.024 | Tree loss: 0.394 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 87 | Batch: 013 / 025 | Total loss: 0.388 | Reg loss: 0.024 | Tree loss: 0.388 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 87 | Batch: 014 / 025 | Total loss: 0.390 | Reg loss: 0.024 | Tree loss: 0.390 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 87 | Batch: 015 / 025 | Total loss: 0.385 | Reg loss: 0.024 | Tree loss: 0.385 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 87 | Batch: 016 / 025 | Total loss: 0.384 | Reg loss: 0.024 | Tree loss: 0.384 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 87 | Batch: 017 / 025 | Total loss: 0.384 | Reg loss: 0.024 | Tree loss: 0.384 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 87 | Batch: 018 / 025 | Total loss: 0.373 | Reg loss: 0.024 | Tree loss: 0.373 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 87 | Batch: 019 / 025 | Total loss: 0.370 | Reg loss: 0.024 | Tree loss: 0.370 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 87 | Batch: 020 / 025 | Total loss: 0.369 | Reg loss: 0.024 | Tree loss: 0.369 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 87 | Batch: 021 / 025 | Total loss: 0.369 | Reg loss: 0.024 | Tree loss: 0.369 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 87 | Batch: 022 / 025 | Total loss: 0.363 | Reg loss: 0.024 | Tree loss: 0.363 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 87 | Batch: 023 / 025 | Total loss: 0.360 | Reg loss: 0.024 | Tree loss: 0.360 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 87 | Batch: 024 / 025 | Total loss: 0.357 | Reg loss: 0.024 | Tree loss: 0.357 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 88 | Batch: 000 / 025 | Total loss: 0.414 | Reg loss: 0.024 | Tree loss: 0.414 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 88 | Batch: 001 / 025 | Total loss: 0.422 | Reg loss: 0.024 | Tree loss: 0.422 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 88 | Batch: 002 / 025 | Total loss: 0.417 | Reg loss: 0.024 | Tree loss: 0.417 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 88 | Batch: 003 / 025 | Total loss: 0.409 | Reg loss: 0.024 | Tree loss: 0.409 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 88 | Batch: 004 / 025 | Total loss: 0.405 | Reg loss: 0.024 | Tree loss: 0.405 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 88 | Batch: 005 / 025 | Total loss: 0.407 | Reg loss: 0.024 | Tree loss: 0.407 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 88 | Batch: 006 / 025 | Total loss: 0.404 | Reg loss: 0.024 | Tree loss: 0.404 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 88 | Batch: 007 / 025 | Total loss: 0.397 | Reg loss: 0.024 | Tree loss: 0.397 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 88 | Batch: 008 / 025 | Total loss: 0.398 | Reg loss: 0.024 | Tree loss: 0.398 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 88 | Batch: 009 / 025 | Total loss: 0.389 | Reg loss: 0.024 | Tree loss: 0.389 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 88 | Batch: 010 / 025 | Total loss: 0.391 | Reg loss: 0.024 | Tree loss: 0.391 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 88 | Batch: 011 / 025 | Total loss: 0.383 | Reg loss: 0.024 | Tree loss: 0.383 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 88 | Batch: 012 / 025 | Total loss: 0.381 | Reg loss: 0.024 | Tree loss: 0.381 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 88 | Batch: 013 / 025 | Total loss: 0.377 | Reg loss: 0.024 | Tree loss: 0.377 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 88 | Batch: 014 / 025 | Total loss: 0.376 | Reg loss: 0.024 | Tree loss: 0.376 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 88 | Batch: 015 / 025 | Total loss: 0.379 | Reg loss: 0.024 | Tree loss: 0.379 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 88 | Batch: 016 / 025 | Total loss: 0.373 | Reg loss: 0.024 | Tree loss: 0.373 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 88 | Batch: 017 / 025 | Total loss: 0.369 | Reg loss: 0.024 | Tree loss: 0.369 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 88 | Batch: 018 / 025 | Total loss: 0.361 | Reg loss: 0.024 | Tree loss: 0.361 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 88 | Batch: 019 / 025 | Total loss: 0.367 | Reg loss: 0.024 | Tree loss: 0.367 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 88 | Batch: 020 / 025 | Total loss: 0.361 | Reg loss: 0.024 | Tree loss: 0.361 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 88 | Batch: 021 / 025 | Total loss: 0.357 | Reg loss: 0.024 | Tree loss: 0.357 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 88 | Batch: 022 / 025 | Total loss: 0.352 | Reg loss: 0.024 | Tree loss: 0.352 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 88 | Batch: 023 / 025 | Total loss: 0.348 | Reg loss: 0.024 | Tree loss: 0.348 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 88 | Batch: 024 / 025 | Total loss: 0.346 | Reg loss: 0.024 | Tree loss: 0.346 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 89 | Batch: 000 / 025 | Total loss: 0.406 | Reg loss: 0.024 | Tree loss: 0.406 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 89 | Batch: 001 / 025 | Total loss: 0.403 | Reg loss: 0.024 | Tree loss: 0.403 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 89 | Batch: 002 / 025 | Total loss: 0.400 | Reg loss: 0.024 | Tree loss: 0.400 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 89 | Batch: 003 / 025 | Total loss: 0.402 | Reg loss: 0.024 | Tree loss: 0.402 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 89 | Batch: 004 / 025 | Total loss: 0.396 | Reg loss: 0.024 | Tree loss: 0.396 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 89 | Batch: 005 / 025 | Total loss: 0.388 | Reg loss: 0.024 | Tree loss: 0.388 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 89 | Batch: 006 / 025 | Total loss: 0.389 | Reg loss: 0.024 | Tree loss: 0.389 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 89 | Batch: 007 / 025 | Total loss: 0.387 | Reg loss: 0.024 | Tree loss: 0.387 | Accuracy: 1.000000 | 0.068 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 89 | Batch: 008 / 025 | Total loss: 0.389 | Reg loss: 0.024 | Tree loss: 0.389 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 89 | Batch: 009 / 025 | Total loss: 0.384 | Reg loss: 0.024 | Tree loss: 0.384 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 89 | Batch: 010 / 025 | Total loss: 0.372 | Reg loss: 0.024 | Tree loss: 0.372 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 89 | Batch: 011 / 025 | Total loss: 0.371 | Reg loss: 0.024 | Tree loss: 0.371 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 89 | Batch: 012 / 025 | Total loss: 0.368 | Reg loss: 0.024 | Tree loss: 0.368 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 89 | Batch: 013 / 025 | Total loss: 0.363 | Reg loss: 0.024 | Tree loss: 0.363 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 89 | Batch: 014 / 025 | Total loss: 0.369 | Reg loss: 0.024 | Tree loss: 0.369 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 89 | Batch: 015 / 025 | Total loss: 0.360 | Reg loss: 0.024 | Tree loss: 0.360 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 89 | Batch: 016 / 025 | Total loss: 0.360 | Reg loss: 0.024 | Tree loss: 0.360 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 89 | Batch: 017 / 025 | Total loss: 0.354 | Reg loss: 0.024 | Tree loss: 0.354 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 89 | Batch: 018 / 025 | Total loss: 0.356 | Reg loss: 0.024 | Tree loss: 0.356 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 89 | Batch: 019 / 025 | Total loss: 0.349 | Reg loss: 0.024 | Tree loss: 0.349 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 89 | Batch: 020 / 025 | Total loss: 0.352 | Reg loss: 0.024 | Tree loss: 0.352 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 89 | Batch: 021 / 025 | Total loss: 0.346 | Reg loss: 0.024 | Tree loss: 0.346 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 89 | Batch: 022 / 025 | Total loss: 0.342 | Reg loss: 0.024 | Tree loss: 0.342 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 89 | Batch: 023 / 025 | Total loss: 0.338 | Reg loss: 0.024 | Tree loss: 0.338 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 89 | Batch: 024 / 025 | Total loss: 0.342 | Reg loss: 0.024 | Tree loss: 0.342 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 90 | Batch: 000 / 025 | Total loss: 0.402 | Reg loss: 0.024 | Tree loss: 0.402 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 90 | Batch: 001 / 025 | Total loss: 0.385 | Reg loss: 0.024 | Tree loss: 0.385 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 90 | Batch: 002 / 025 | Total loss: 0.389 | Reg loss: 0.024 | Tree loss: 0.389 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 90 | Batch: 003 / 025 | Total loss: 0.384 | Reg loss: 0.024 | Tree loss: 0.384 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 90 | Batch: 004 / 025 | Total loss: 0.388 | Reg loss: 0.024 | Tree loss: 0.388 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 90 | Batch: 005 / 025 | Total loss: 0.378 | Reg loss: 0.024 | Tree loss: 0.378 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 90 | Batch: 006 / 025 | Total loss: 0.375 | Reg loss: 0.024 | Tree loss: 0.375 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 90 | Batch: 007 / 025 | Total loss: 0.376 | Reg loss: 0.024 | Tree loss: 0.376 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 90 | Batch: 008 / 025 | Total loss: 0.373 | Reg loss: 0.024 | Tree loss: 0.373 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 90 | Batch: 009 / 025 | Total loss: 0.368 | Reg loss: 0.024 | Tree loss: 0.368 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 90 | Batch: 010 / 025 | Total loss: 0.370 | Reg loss: 0.024 | Tree loss: 0.370 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 90 | Batch: 011 / 025 | Total loss: 0.362 | Reg loss: 0.024 | Tree loss: 0.362 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 90 | Batch: 012 / 025 | Total loss: 0.360 | Reg loss: 0.024 | Tree loss: 0.360 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 90 | Batch: 013 / 025 | Total loss: 0.358 | Reg loss: 0.024 | Tree loss: 0.358 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 90 | Batch: 014 / 025 | Total loss: 0.356 | Reg loss: 0.024 | Tree loss: 0.356 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 90 | Batch: 015 / 025 | Total loss: 0.350 | Reg loss: 0.024 | Tree loss: 0.350 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 90 | Batch: 016 / 025 | Total loss: 0.347 | Reg loss: 0.024 | Tree loss: 0.347 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 90 | Batch: 017 / 025 | Total loss: 0.348 | Reg loss: 0.024 | Tree loss: 0.348 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 90 | Batch: 018 / 025 | Total loss: 0.342 | Reg loss: 0.024 | Tree loss: 0.342 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 90 | Batch: 019 / 025 | Total loss: 0.340 | Reg loss: 0.024 | Tree loss: 0.340 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 90 | Batch: 020 / 025 | Total loss: 0.335 | Reg loss: 0.024 | Tree loss: 0.335 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 90 | Batch: 021 / 025 | Total loss: 0.334 | Reg loss: 0.024 | Tree loss: 0.334 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 90 | Batch: 022 / 025 | Total loss: 0.334 | Reg loss: 0.024 | Tree loss: 0.334 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 90 | Batch: 023 / 025 | Total loss: 0.328 | Reg loss: 0.024 | Tree loss: 0.328 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 90 | Batch: 024 / 025 | Total loss: 0.322 | Reg loss: 0.024 | Tree loss: 0.322 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 91 | Batch: 000 / 025 | Total loss: 0.385 | Reg loss: 0.024 | Tree loss: 0.385 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 91 | Batch: 001 / 025 | Total loss: 0.383 | Reg loss: 0.024 | Tree loss: 0.383 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 91 | Batch: 002 / 025 | Total loss: 0.378 | Reg loss: 0.024 | Tree loss: 0.378 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 91 | Batch: 003 / 025 | Total loss: 0.375 | Reg loss: 0.024 | Tree loss: 0.375 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 91 | Batch: 004 / 025 | Total loss: 0.378 | Reg loss: 0.024 | Tree loss: 0.378 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 91 | Batch: 005 / 025 | Total loss: 0.369 | Reg loss: 0.024 | Tree loss: 0.369 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 91 | Batch: 006 / 025 | Total loss: 0.365 | Reg loss: 0.024 | Tree loss: 0.365 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 91 | Batch: 007 / 025 | Total loss: 0.365 | Reg loss: 0.024 | Tree loss: 0.365 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 91 | Batch: 008 / 025 | Total loss: 0.357 | Reg loss: 0.024 | Tree loss: 0.357 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 91 | Batch: 009 / 025 | Total loss: 0.357 | Reg loss: 0.024 | Tree loss: 0.357 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 91 | Batch: 010 / 025 | Total loss: 0.354 | Reg loss: 0.024 | Tree loss: 0.354 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 91 | Batch: 011 / 025 | Total loss: 0.350 | Reg loss: 0.024 | Tree loss: 0.350 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 91 | Batch: 012 / 025 | Total loss: 0.345 | Reg loss: 0.024 | Tree loss: 0.345 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 91 | Batch: 013 / 025 | Total loss: 0.350 | Reg loss: 0.024 | Tree loss: 0.350 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 91 | Batch: 014 / 025 | Total loss: 0.346 | Reg loss: 0.024 | Tree loss: 0.346 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 91 | Batch: 015 / 025 | Total loss: 0.341 | Reg loss: 0.024 | Tree loss: 0.341 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 91 | Batch: 016 / 025 | Total loss: 0.336 | Reg loss: 0.024 | Tree loss: 0.336 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 91 | Batch: 017 / 025 | Total loss: 0.331 | Reg loss: 0.024 | Tree loss: 0.331 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 91 | Batch: 018 / 025 | Total loss: 0.332 | Reg loss: 0.024 | Tree loss: 0.332 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 91 | Batch: 019 / 025 | Total loss: 0.338 | Reg loss: 0.024 | Tree loss: 0.338 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 91 | Batch: 020 / 025 | Total loss: 0.331 | Reg loss: 0.024 | Tree loss: 0.331 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 91 | Batch: 021 / 025 | Total loss: 0.322 | Reg loss: 0.024 | Tree loss: 0.322 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 91 | Batch: 022 / 025 | Total loss: 0.325 | Reg loss: 0.024 | Tree loss: 0.325 | Accuracy: 1.000000 | 0.068 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 91 | Batch: 023 / 025 | Total loss: 0.317 | Reg loss: 0.024 | Tree loss: 0.317 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 91 | Batch: 024 / 025 | Total loss: 0.310 | Reg loss: 0.024 | Tree loss: 0.310 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 92 | Batch: 000 / 025 | Total loss: 0.377 | Reg loss: 0.023 | Tree loss: 0.377 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 92 | Batch: 001 / 025 | Total loss: 0.373 | Reg loss: 0.023 | Tree loss: 0.373 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 92 | Batch: 002 / 025 | Total loss: 0.370 | Reg loss: 0.023 | Tree loss: 0.370 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 92 | Batch: 003 / 025 | Total loss: 0.361 | Reg loss: 0.023 | Tree loss: 0.361 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 92 | Batch: 004 / 025 | Total loss: 0.361 | Reg loss: 0.023 | Tree loss: 0.361 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 92 | Batch: 005 / 025 | Total loss: 0.353 | Reg loss: 0.023 | Tree loss: 0.353 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 92 | Batch: 006 / 025 | Total loss: 0.354 | Reg loss: 0.023 | Tree loss: 0.354 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 92 | Batch: 007 / 025 | Total loss: 0.352 | Reg loss: 0.023 | Tree loss: 0.352 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 92 | Batch: 008 / 025 | Total loss: 0.353 | Reg loss: 0.023 | Tree loss: 0.353 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 92 | Batch: 009 / 025 | Total loss: 0.348 | Reg loss: 0.024 | Tree loss: 0.348 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 92 | Batch: 010 / 025 | Total loss: 0.343 | Reg loss: 0.024 | Tree loss: 0.343 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 92 | Batch: 011 / 025 | Total loss: 0.345 | Reg loss: 0.024 | Tree loss: 0.345 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 92 | Batch: 012 / 025 | Total loss: 0.341 | Reg loss: 0.024 | Tree loss: 0.341 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 92 | Batch: 013 / 025 | Total loss: 0.336 | Reg loss: 0.024 | Tree loss: 0.336 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 92 | Batch: 014 / 025 | Total loss: 0.335 | Reg loss: 0.024 | Tree loss: 0.335 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 92 | Batch: 015 / 025 | Total loss: 0.332 | Reg loss: 0.024 | Tree loss: 0.332 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 92 | Batch: 016 / 025 | Total loss: 0.330 | Reg loss: 0.024 | Tree loss: 0.330 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 92 | Batch: 017 / 025 | Total loss: 0.325 | Reg loss: 0.024 | Tree loss: 0.325 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 92 | Batch: 018 / 025 | Total loss: 0.320 | Reg loss: 0.024 | Tree loss: 0.320 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 92 | Batch: 019 / 025 | Total loss: 0.319 | Reg loss: 0.024 | Tree loss: 0.319 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 92 | Batch: 020 / 025 | Total loss: 0.316 | Reg loss: 0.024 | Tree loss: 0.316 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 92 | Batch: 021 / 025 | Total loss: 0.316 | Reg loss: 0.024 | Tree loss: 0.316 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 92 | Batch: 022 / 025 | Total loss: 0.312 | Reg loss: 0.024 | Tree loss: 0.312 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 92 | Batch: 023 / 025 | Total loss: 0.310 | Reg loss: 0.024 | Tree loss: 0.310 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 92 | Batch: 024 / 025 | Total loss: 0.313 | Reg loss: 0.024 | Tree loss: 0.313 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 93 | Batch: 000 / 025 | Total loss: 0.356 | Reg loss: 0.023 | Tree loss: 0.356 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 93 | Batch: 001 / 025 | Total loss: 0.359 | Reg loss: 0.023 | Tree loss: 0.359 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 93 | Batch: 002 / 025 | Total loss: 0.354 | Reg loss: 0.023 | Tree loss: 0.354 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 93 | Batch: 003 / 025 | Total loss: 0.356 | Reg loss: 0.023 | Tree loss: 0.356 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 93 | Batch: 004 / 025 | Total loss: 0.354 | Reg loss: 0.023 | Tree loss: 0.354 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 93 | Batch: 005 / 025 | Total loss: 0.353 | Reg loss: 0.023 | Tree loss: 0.353 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 93 | Batch: 006 / 025 | Total loss: 0.347 | Reg loss: 0.023 | Tree loss: 0.347 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 93 | Batch: 007 / 025 | Total loss: 0.341 | Reg loss: 0.023 | Tree loss: 0.341 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 93 | Batch: 008 / 025 | Total loss: 0.337 | Reg loss: 0.023 | Tree loss: 0.337 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 93 | Batch: 009 / 025 | Total loss: 0.339 | Reg loss: 0.023 | Tree loss: 0.339 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 93 | Batch: 010 / 025 | Total loss: 0.338 | Reg loss: 0.023 | Tree loss: 0.338 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 93 | Batch: 011 / 025 | Total loss: 0.332 | Reg loss: 0.023 | Tree loss: 0.332 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 93 | Batch: 012 / 025 | Total loss: 0.330 | Reg loss: 0.024 | Tree loss: 0.330 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 93 | Batch: 013 / 025 | Total loss: 0.326 | Reg loss: 0.024 | Tree loss: 0.326 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 93 | Batch: 014 / 025 | Total loss: 0.323 | Reg loss: 0.024 | Tree loss: 0.323 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 93 | Batch: 015 / 025 | Total loss: 0.326 | Reg loss: 0.024 | Tree loss: 0.326 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 93 | Batch: 016 / 025 | Total loss: 0.322 | Reg loss: 0.024 | Tree loss: 0.322 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 93 | Batch: 017 / 025 | Total loss: 0.315 | Reg loss: 0.024 | Tree loss: 0.315 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 93 | Batch: 018 / 025 | Total loss: 0.316 | Reg loss: 0.024 | Tree loss: 0.316 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 93 | Batch: 019 / 025 | Total loss: 0.314 | Reg loss: 0.024 | Tree loss: 0.314 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 93 | Batch: 020 / 025 | Total loss: 0.308 | Reg loss: 0.024 | Tree loss: 0.308 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 93 | Batch: 021 / 025 | Total loss: 0.308 | Reg loss: 0.024 | Tree loss: 0.308 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 93 | Batch: 022 / 025 | Total loss: 0.304 | Reg loss: 0.024 | Tree loss: 0.304 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 93 | Batch: 023 / 025 | Total loss: 0.300 | Reg loss: 0.024 | Tree loss: 0.300 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 93 | Batch: 024 / 025 | Total loss: 0.300 | Reg loss: 0.024 | Tree loss: 0.300 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 94 | Batch: 000 / 025 | Total loss: 0.351 | Reg loss: 0.023 | Tree loss: 0.351 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 94 | Batch: 001 / 025 | Total loss: 0.347 | Reg loss: 0.023 | Tree loss: 0.347 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 94 | Batch: 002 / 025 | Total loss: 0.351 | Reg loss: 0.023 | Tree loss: 0.351 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 94 | Batch: 003 / 025 | Total loss: 0.344 | Reg loss: 0.023 | Tree loss: 0.344 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 94 | Batch: 004 / 025 | Total loss: 0.339 | Reg loss: 0.023 | Tree loss: 0.339 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 94 | Batch: 005 / 025 | Total loss: 0.343 | Reg loss: 0.023 | Tree loss: 0.343 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 94 | Batch: 006 / 025 | Total loss: 0.338 | Reg loss: 0.023 | Tree loss: 0.338 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 94 | Batch: 007 / 025 | Total loss: 0.330 | Reg loss: 0.023 | Tree loss: 0.330 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 94 | Batch: 008 / 025 | Total loss: 0.332 | Reg loss: 0.023 | Tree loss: 0.332 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 94 | Batch: 009 / 025 | Total loss: 0.329 | Reg loss: 0.023 | Tree loss: 0.329 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 94 | Batch: 010 / 025 | Total loss: 0.329 | Reg loss: 0.023 | Tree loss: 0.329 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 94 | Batch: 011 / 025 | Total loss: 0.327 | Reg loss: 0.023 | Tree loss: 0.327 | Accuracy: 1.000000 | 0.068 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 94 | Batch: 012 / 025 | Total loss: 0.319 | Reg loss: 0.023 | Tree loss: 0.319 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 94 | Batch: 013 / 025 | Total loss: 0.320 | Reg loss: 0.023 | Tree loss: 0.320 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 94 | Batch: 014 / 025 | Total loss: 0.318 | Reg loss: 0.023 | Tree loss: 0.318 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 94 | Batch: 015 / 025 | Total loss: 0.310 | Reg loss: 0.024 | Tree loss: 0.310 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 94 | Batch: 016 / 025 | Total loss: 0.313 | Reg loss: 0.024 | Tree loss: 0.313 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 94 | Batch: 017 / 025 | Total loss: 0.306 | Reg loss: 0.024 | Tree loss: 0.306 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 94 | Batch: 018 / 025 | Total loss: 0.305 | Reg loss: 0.024 | Tree loss: 0.305 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 94 | Batch: 019 / 025 | Total loss: 0.303 | Reg loss: 0.024 | Tree loss: 0.303 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 94 | Batch: 020 / 025 | Total loss: 0.304 | Reg loss: 0.024 | Tree loss: 0.304 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 94 | Batch: 021 / 025 | Total loss: 0.296 | Reg loss: 0.024 | Tree loss: 0.296 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 94 | Batch: 022 / 025 | Total loss: 0.296 | Reg loss: 0.024 | Tree loss: 0.296 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 94 | Batch: 023 / 025 | Total loss: 0.297 | Reg loss: 0.024 | Tree loss: 0.297 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 94 | Batch: 024 / 025 | Total loss: 0.290 | Reg loss: 0.024 | Tree loss: 0.290 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 95 | Batch: 000 / 025 | Total loss: 0.343 | Reg loss: 0.023 | Tree loss: 0.343 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 95 | Batch: 001 / 025 | Total loss: 0.340 | Reg loss: 0.023 | Tree loss: 0.340 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 95 | Batch: 002 / 025 | Total loss: 0.333 | Reg loss: 0.023 | Tree loss: 0.333 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 95 | Batch: 003 / 025 | Total loss: 0.337 | Reg loss: 0.023 | Tree loss: 0.337 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 95 | Batch: 004 / 025 | Total loss: 0.333 | Reg loss: 0.023 | Tree loss: 0.333 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 95 | Batch: 005 / 025 | Total loss: 0.325 | Reg loss: 0.023 | Tree loss: 0.325 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 95 | Batch: 006 / 025 | Total loss: 0.335 | Reg loss: 0.023 | Tree loss: 0.335 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 95 | Batch: 007 / 025 | Total loss: 0.330 | Reg loss: 0.023 | Tree loss: 0.330 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 95 | Batch: 008 / 025 | Total loss: 0.323 | Reg loss: 0.023 | Tree loss: 0.323 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 95 | Batch: 009 / 025 | Total loss: 0.320 | Reg loss: 0.023 | Tree loss: 0.320 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 95 | Batch: 010 / 025 | Total loss: 0.318 | Reg loss: 0.023 | Tree loss: 0.318 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 95 | Batch: 011 / 025 | Total loss: 0.313 | Reg loss: 0.023 | Tree loss: 0.313 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 95 | Batch: 012 / 025 | Total loss: 0.313 | Reg loss: 0.023 | Tree loss: 0.313 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 95 | Batch: 013 / 025 | Total loss: 0.306 | Reg loss: 0.023 | Tree loss: 0.306 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 95 | Batch: 014 / 025 | Total loss: 0.311 | Reg loss: 0.023 | Tree loss: 0.311 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 95 | Batch: 015 / 025 | Total loss: 0.305 | Reg loss: 0.023 | Tree loss: 0.305 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 95 | Batch: 016 / 025 | Total loss: 0.300 | Reg loss: 0.023 | Tree loss: 0.300 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 95 | Batch: 017 / 025 | Total loss: 0.303 | Reg loss: 0.023 | Tree loss: 0.303 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 95 | Batch: 018 / 025 | Total loss: 0.294 | Reg loss: 0.024 | Tree loss: 0.294 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 95 | Batch: 019 / 025 | Total loss: 0.296 | Reg loss: 0.024 | Tree loss: 0.296 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 95 | Batch: 020 / 025 | Total loss: 0.295 | Reg loss: 0.024 | Tree loss: 0.295 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 95 | Batch: 021 / 025 | Total loss: 0.291 | Reg loss: 0.024 | Tree loss: 0.291 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 95 | Batch: 022 / 025 | Total loss: 0.291 | Reg loss: 0.024 | Tree loss: 0.291 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 95 | Batch: 023 / 025 | Total loss: 0.288 | Reg loss: 0.024 | Tree loss: 0.288 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 95 | Batch: 024 / 025 | Total loss: 0.286 | Reg loss: 0.024 | Tree loss: 0.286 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 96 | Batch: 000 / 025 | Total loss: 0.342 | Reg loss: 0.023 | Tree loss: 0.342 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 96 | Batch: 001 / 025 | Total loss: 0.334 | Reg loss: 0.023 | Tree loss: 0.334 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 96 | Batch: 002 / 025 | Total loss: 0.326 | Reg loss: 0.023 | Tree loss: 0.326 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 96 | Batch: 003 / 025 | Total loss: 0.326 | Reg loss: 0.023 | Tree loss: 0.326 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 96 | Batch: 004 / 025 | Total loss: 0.326 | Reg loss: 0.023 | Tree loss: 0.326 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 96 | Batch: 005 / 025 | Total loss: 0.326 | Reg loss: 0.023 | Tree loss: 0.326 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 96 | Batch: 006 / 025 | Total loss: 0.316 | Reg loss: 0.023 | Tree loss: 0.316 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 96 | Batch: 007 / 025 | Total loss: 0.315 | Reg loss: 0.023 | Tree loss: 0.315 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 96 | Batch: 008 / 025 | Total loss: 0.314 | Reg loss: 0.023 | Tree loss: 0.314 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 96 | Batch: 009 / 025 | Total loss: 0.318 | Reg loss: 0.023 | Tree loss: 0.318 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 96 | Batch: 010 / 025 | Total loss: 0.305 | Reg loss: 0.023 | Tree loss: 0.305 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 96 | Batch: 011 / 025 | Total loss: 0.308 | Reg loss: 0.023 | Tree loss: 0.308 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 96 | Batch: 012 / 025 | Total loss: 0.303 | Reg loss: 0.023 | Tree loss: 0.303 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 96 | Batch: 013 / 025 | Total loss: 0.303 | Reg loss: 0.023 | Tree loss: 0.303 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 96 | Batch: 014 / 025 | Total loss: 0.304 | Reg loss: 0.023 | Tree loss: 0.304 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 96 | Batch: 015 / 025 | Total loss: 0.297 | Reg loss: 0.023 | Tree loss: 0.297 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 96 | Batch: 016 / 025 | Total loss: 0.292 | Reg loss: 0.023 | Tree loss: 0.292 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 96 | Batch: 017 / 025 | Total loss: 0.294 | Reg loss: 0.023 | Tree loss: 0.294 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 96 | Batch: 018 / 025 | Total loss: 0.288 | Reg loss: 0.023 | Tree loss: 0.288 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 96 | Batch: 019 / 025 | Total loss: 0.288 | Reg loss: 0.023 | Tree loss: 0.288 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 96 | Batch: 020 / 025 | Total loss: 0.286 | Reg loss: 0.024 | Tree loss: 0.286 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 96 | Batch: 021 / 025 | Total loss: 0.282 | Reg loss: 0.024 | Tree loss: 0.282 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 96 | Batch: 022 / 025 | Total loss: 0.284 | Reg loss: 0.024 | Tree loss: 0.284 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 96 | Batch: 023 / 025 | Total loss: 0.279 | Reg loss: 0.024 | Tree loss: 0.279 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 96 | Batch: 024 / 025 | Total loss: 0.272 | Reg loss: 0.024 | Tree loss: 0.272 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 97 | Batch: 000 / 025 | Total loss: 0.334 | Reg loss: 0.023 | Tree loss: 0.334 | Accuracy: 1.000000 | 0.068 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 97 | Batch: 001 / 025 | Total loss: 0.325 | Reg loss: 0.023 | Tree loss: 0.325 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 97 | Batch: 002 / 025 | Total loss: 0.322 | Reg loss: 0.023 | Tree loss: 0.322 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 97 | Batch: 003 / 025 | Total loss: 0.321 | Reg loss: 0.023 | Tree loss: 0.321 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 97 | Batch: 004 / 025 | Total loss: 0.316 | Reg loss: 0.023 | Tree loss: 0.316 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 97 | Batch: 005 / 025 | Total loss: 0.315 | Reg loss: 0.023 | Tree loss: 0.315 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 97 | Batch: 006 / 025 | Total loss: 0.313 | Reg loss: 0.023 | Tree loss: 0.313 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 97 | Batch: 007 / 025 | Total loss: 0.313 | Reg loss: 0.023 | Tree loss: 0.313 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 97 | Batch: 008 / 025 | Total loss: 0.306 | Reg loss: 0.023 | Tree loss: 0.306 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 97 | Batch: 009 / 025 | Total loss: 0.303 | Reg loss: 0.023 | Tree loss: 0.303 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 97 | Batch: 010 / 025 | Total loss: 0.303 | Reg loss: 0.023 | Tree loss: 0.303 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 97 | Batch: 011 / 025 | Total loss: 0.300 | Reg loss: 0.023 | Tree loss: 0.300 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 97 | Batch: 012 / 025 | Total loss: 0.295 | Reg loss: 0.023 | Tree loss: 0.295 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 97 | Batch: 013 / 025 | Total loss: 0.299 | Reg loss: 0.023 | Tree loss: 0.299 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 97 | Batch: 014 / 025 | Total loss: 0.290 | Reg loss: 0.023 | Tree loss: 0.290 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 97 | Batch: 015 / 025 | Total loss: 0.292 | Reg loss: 0.023 | Tree loss: 0.292 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 97 | Batch: 016 / 025 | Total loss: 0.289 | Reg loss: 0.023 | Tree loss: 0.289 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 97 | Batch: 017 / 025 | Total loss: 0.285 | Reg loss: 0.023 | Tree loss: 0.285 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 97 | Batch: 018 / 025 | Total loss: 0.284 | Reg loss: 0.023 | Tree loss: 0.284 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 97 | Batch: 019 / 025 | Total loss: 0.276 | Reg loss: 0.023 | Tree loss: 0.276 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 97 | Batch: 020 / 025 | Total loss: 0.277 | Reg loss: 0.023 | Tree loss: 0.277 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 97 | Batch: 021 / 025 | Total loss: 0.277 | Reg loss: 0.023 | Tree loss: 0.277 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 97 | Batch: 022 / 025 | Total loss: 0.273 | Reg loss: 0.024 | Tree loss: 0.273 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 97 | Batch: 023 / 025 | Total loss: 0.270 | Reg loss: 0.024 | Tree loss: 0.270 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 97 | Batch: 024 / 025 | Total loss: 0.270 | Reg loss: 0.024 | Tree loss: 0.270 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 98 | Batch: 000 / 025 | Total loss: 0.320 | Reg loss: 0.023 | Tree loss: 0.320 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 98 | Batch: 001 / 025 | Total loss: 0.319 | Reg loss: 0.023 | Tree loss: 0.319 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 98 | Batch: 002 / 025 | Total loss: 0.310 | Reg loss: 0.023 | Tree loss: 0.310 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 98 | Batch: 003 / 025 | Total loss: 0.312 | Reg loss: 0.023 | Tree loss: 0.312 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 98 | Batch: 004 / 025 | Total loss: 0.309 | Reg loss: 0.023 | Tree loss: 0.309 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 98 | Batch: 005 / 025 | Total loss: 0.307 | Reg loss: 0.023 | Tree loss: 0.307 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 98 | Batch: 006 / 025 | Total loss: 0.304 | Reg loss: 0.023 | Tree loss: 0.304 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 98 | Batch: 007 / 025 | Total loss: 0.305 | Reg loss: 0.023 | Tree loss: 0.305 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 98 | Batch: 008 / 025 | Total loss: 0.301 | Reg loss: 0.023 | Tree loss: 0.301 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 98 | Batch: 009 / 025 | Total loss: 0.300 | Reg loss: 0.023 | Tree loss: 0.300 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 98 | Batch: 010 / 025 | Total loss: 0.291 | Reg loss: 0.023 | Tree loss: 0.291 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 98 | Batch: 011 / 025 | Total loss: 0.291 | Reg loss: 0.023 | Tree loss: 0.291 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 98 | Batch: 012 / 025 | Total loss: 0.288 | Reg loss: 0.023 | Tree loss: 0.288 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 98 | Batch: 013 / 025 | Total loss: 0.292 | Reg loss: 0.023 | Tree loss: 0.292 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 98 | Batch: 014 / 025 | Total loss: 0.284 | Reg loss: 0.023 | Tree loss: 0.284 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 98 | Batch: 015 / 025 | Total loss: 0.285 | Reg loss: 0.023 | Tree loss: 0.285 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 98 | Batch: 016 / 025 | Total loss: 0.284 | Reg loss: 0.023 | Tree loss: 0.284 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 98 | Batch: 017 / 025 | Total loss: 0.278 | Reg loss: 0.023 | Tree loss: 0.278 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 98 | Batch: 018 / 025 | Total loss: 0.274 | Reg loss: 0.023 | Tree loss: 0.274 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 98 | Batch: 019 / 025 | Total loss: 0.275 | Reg loss: 0.023 | Tree loss: 0.275 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 98 | Batch: 020 / 025 | Total loss: 0.274 | Reg loss: 0.023 | Tree loss: 0.274 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 98 | Batch: 021 / 025 | Total loss: 0.268 | Reg loss: 0.023 | Tree loss: 0.268 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 98 | Batch: 022 / 025 | Total loss: 0.274 | Reg loss: 0.023 | Tree loss: 0.274 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 98 | Batch: 023 / 025 | Total loss: 0.261 | Reg loss: 0.023 | Tree loss: 0.261 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 98 | Batch: 024 / 025 | Total loss: 0.265 | Reg loss: 0.024 | Tree loss: 0.265 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Average sparseness: 0.9821428571428567\n",
      "layer 0: 0.9821428571428571\n",
      "layer 1: 0.9821428571428571\n",
      "layer 2: 0.9821428571428571\n",
      "layer 3: 0.9821428571428571\n",
      "layer 4: 0.9821428571428571\n",
      "Epoch: 99 | Batch: 000 / 025 | Total loss: 0.314 | Reg loss: 0.023 | Tree loss: 0.314 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 99 | Batch: 001 / 025 | Total loss: 0.310 | Reg loss: 0.023 | Tree loss: 0.310 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 99 | Batch: 002 / 025 | Total loss: 0.310 | Reg loss: 0.023 | Tree loss: 0.310 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 99 | Batch: 003 / 025 | Total loss: 0.307 | Reg loss: 0.023 | Tree loss: 0.307 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 99 | Batch: 004 / 025 | Total loss: 0.305 | Reg loss: 0.023 | Tree loss: 0.305 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 99 | Batch: 005 / 025 | Total loss: 0.297 | Reg loss: 0.023 | Tree loss: 0.297 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 99 | Batch: 006 / 025 | Total loss: 0.288 | Reg loss: 0.023 | Tree loss: 0.288 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 99 | Batch: 007 / 025 | Total loss: 0.297 | Reg loss: 0.023 | Tree loss: 0.297 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 99 | Batch: 008 / 025 | Total loss: 0.292 | Reg loss: 0.023 | Tree loss: 0.292 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 99 | Batch: 009 / 025 | Total loss: 0.292 | Reg loss: 0.023 | Tree loss: 0.292 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 99 | Batch: 010 / 025 | Total loss: 0.287 | Reg loss: 0.023 | Tree loss: 0.287 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 99 | Batch: 011 / 025 | Total loss: 0.286 | Reg loss: 0.023 | Tree loss: 0.286 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 99 | Batch: 012 / 025 | Total loss: 0.290 | Reg loss: 0.023 | Tree loss: 0.290 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 99 | Batch: 013 / 025 | Total loss: 0.280 | Reg loss: 0.023 | Tree loss: 0.280 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 99 | Batch: 014 / 025 | Total loss: 0.275 | Reg loss: 0.023 | Tree loss: 0.275 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 99 | Batch: 015 / 025 | Total loss: 0.276 | Reg loss: 0.023 | Tree loss: 0.276 | Accuracy: 1.000000 | 0.068 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 99 | Batch: 016 / 025 | Total loss: 0.276 | Reg loss: 0.023 | Tree loss: 0.276 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 99 | Batch: 017 / 025 | Total loss: 0.272 | Reg loss: 0.023 | Tree loss: 0.272 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 99 | Batch: 018 / 025 | Total loss: 0.271 | Reg loss: 0.023 | Tree loss: 0.271 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 99 | Batch: 019 / 025 | Total loss: 0.267 | Reg loss: 0.023 | Tree loss: 0.267 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 99 | Batch: 020 / 025 | Total loss: 0.264 | Reg loss: 0.023 | Tree loss: 0.264 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 99 | Batch: 021 / 025 | Total loss: 0.261 | Reg loss: 0.023 | Tree loss: 0.261 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 99 | Batch: 022 / 025 | Total loss: 0.263 | Reg loss: 0.023 | Tree loss: 0.263 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 99 | Batch: 023 / 025 | Total loss: 0.262 | Reg loss: 0.023 | Tree loss: 0.262 | Accuracy: 1.000000 | 0.068 sec/iter\n",
      "Epoch: 99 | Batch: 024 / 025 | Total loss: 0.259 | Reg loss: 0.023 | Tree loss: 0.259 | Accuracy: 1.000000 | 0.068 sec/iter\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "iteration = 0\n",
    "for epoch in range(epochs):\n",
    "    # Training\n",
    "    avg_sp = show_sparseness(tree)\n",
    "    sparsity.append(avg_sp)\n",
    "    iteration = do_epoch(tree, tree_loader, device, log_interval, losses, accs, epoch, iteration)\n",
    "    \n",
    "    if epoch % 1 == 0:\n",
    "        prune_tree(tree, factor=3)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eitan.k/.local/lib/python3.6/site-packages/ipykernel_launcher.py:1: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1946b908391b49e8b945be4c3d9190cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel('Iteration')\n",
    "plt.plot(accs, label='Accuracy vs iteration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bfb116242ee4b979d6c7b98694e0575",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7814403b4874e828738e08a8e622555",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel('Iteration')\n",
    "plt.plot(losses, label='Loss vs iteration')\n",
    "plt.yscale(\"log\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "weights = tree.inner_nodes.weight.cpu().detach().numpy().flatten()\n",
    "plt.hist(weights, bins=500)\n",
    "weights_std = np.std(weights)\n",
    "weights_mean = np.mean(weights)\n",
    "plt.axvline(weights_mean + weights_std, color='r')\n",
    "plt.axvline(weights_mean - weights_std, color='r')\n",
    "plt.title(f\"Mean: {weights_mean}   |   STD: {weights_std}\")\n",
    "plt.yscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81c8b17c4c9344b887576cfd7762a4f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average height: 0.0\n"
     ]
    }
   ],
   "source": [
    "plt.figure(figsize=(15, 10), dpi=80)\n",
    "avg_height, root = tree.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accumulate samples in the leaves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of patterns: 1\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of patterns: {len(root.get_leaves())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = 'greedy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "root.clear_leaves_samples()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (data, target) in enumerate(tree_loader):\n",
    "        root.accumulate_samples(data, method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tighten boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12584\n",
      "============== Pattern 1 ==============\n",
      "Average comprehensibility: 0.0\n",
      "std comprehensibility: 0.0\n",
      "var comprehensibility: 0.0\n",
      "minimum comprehensibility: 0\n",
      "maximum comprehensibility: 0\n"
     ]
    }
   ],
   "source": [
    "attr_names = dataset.items\n",
    "\n",
    "# print(attr_names)\n",
    "leaves = root.get_leaves()\n",
    "sum_comprehensibility = 0\n",
    "comprehensibilities = []\n",
    "for pattern_counter, leaf in enumerate(leaves):\n",
    "    leaf.reset_path()\n",
    "    leaf.tighten_with_accumulated_samples()\n",
    "    conds = leaf.get_path_conditions(attr_names)\n",
    "    for cond in conds:\n",
    "        cond.weights = cond.weights / normalizers\n",
    "    print(f\"============== Pattern {pattern_counter + 1} ==============\")\n",
    "    comprehensibilities.append(sum([cond.comprehensibility for cond in conds]))\n",
    "    \n",
    "print(f\"Average comprehensibility: {np.mean(comprehensibilities)}\")\n",
    "print(f\"std comprehensibility: {np.std(comprehensibilities)}\")\n",
    "print(f\"var comprehensibility: {np.var(comprehensibilities)}\")\n",
    "print(f\"minimum comprehensibility: {np.min(comprehensibilities)}\")\n",
    "print(f\"maximum comprehensibility: {np.max(comprehensibilities)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
