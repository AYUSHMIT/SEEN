{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "%matplotlib inline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "import time\n",
    "import copy\n",
    "import os\n",
    "from queue import LifoQueue\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "from scipy.stats import kurtosis\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import network.cpc\n",
    "from network.cpc import CDCK2\n",
    "from utils.MatplotlibUtils import reduce_dims_and_plot\n",
    "from utils.ClassificationUtiols import onehot_coding\n",
    "from soft_decision_tree.sdt_model import SDT\n",
    "from sklearn import tree as tt\n",
    "\n",
    "# IPython.OutputArea.auto_scroll_threshold = 9999;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the model and the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load the model from: /home/eitan.k/EntangledExplainableClustering/knn_loss_batch_512_k_8/models/epoch_26.pt\n",
      "sensor names: (18 total)\n",
      "- speed\n",
      "- steering_angle\n",
      "- wheel_speed_0\n",
      "- wheel_speed_1\n",
      "- wheel_speed_2\n",
      "- wheel_speed_3\n",
      "- accelerometer_0\n",
      "- accelerometer_1\n",
      "- accelerometer_2\n",
      "- gyro_0\n",
      "- gyro_1\n",
      "- gyro_2\n",
      "- gyro_bias_0\n",
      "- gyro_bias_1\n",
      "- gyro_bias_2\n",
      "- gyro_uncalibrated_0\n",
      "- gyro_uncalibrated_1\n",
      "- gyro_uncalibrated_2\n",
      "Multihorizon size of the model: 30\n",
      "Test split ratio: 0.2\n",
      "Total number of windows in the dataset (without splitting): 101465\n"
     ]
    }
   ],
   "source": [
    "model_path = r''  # Insert path of the cpc model\n",
    "dataset_path = r''  # Insert path of the test dataset that was created using the run_cpc.py script\n",
    "\n",
    "print(f\"Load the model from: {model_path}\")\n",
    "model = torch.load(model_path, map_location='cpu')\n",
    "\n",
    "with open(dataset_path, 'rb') as fp:\n",
    "    dataset = pickle.load(fp)\n",
    "\n",
    "all_sensors = dataset.dataset.all_signals    \n",
    "print(f\"sensor names: ({len(all_sensors)} total)\")\n",
    "\n",
    "for s in all_sensors:\n",
    "    print(f\"- {s}\")\n",
    "    \n",
    "print(f\"Multihorizon size of the model: {model.timestep}\")\n",
    "print(f\"Test split ratio: {len(dataset) / len(dataset.dataset)}\")\n",
    "print(f\"Total number of windows in the dataset (without splitting): {len(dataset.dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 500\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Extract representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a884b3fccb67410d93f6da091d2cc67e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20293 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "projections = torch.tensor([])\n",
    "samples = torch.tensor([])\n",
    "device = 'cuda'\n",
    "model = model.to(device).eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    bar = tqdm(total=len(loader.dataset))\n",
    "    for batch in loader:\n",
    "        hidden = CDCK2.init_hidden(len(batch))\n",
    "        batch = batch.to(device)\n",
    "        hidden = hidden.to(device)\n",
    "\n",
    "        y = model.predict(batch, hidden).detach().cpu()\n",
    "        projections = torch.cat([projections, y.detach().cpu()])\n",
    "        samples = torch.cat([samples, batch.detach().cpu()])\n",
    "        bar.update(y.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit GMM and calculate indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23d547ca2a3844dfba8c2d4f673cc5d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scores = []\n",
    "best_score = float('inf')\n",
    "clusters = None\n",
    "range_ = list(range(5, 40))\n",
    "for k in tqdm(range_):\n",
    "    y = GaussianMixture(n_components=k).fit_predict(projections)\n",
    "    cur_score = davies_bouldin_score(projections, y)\n",
    "    scores.append(cur_score)\n",
    "    \n",
    "    if cur_score < best_score:\n",
    "        best_score = cur_score\n",
    "        clusters = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26}\n"
     ]
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('DB Score')\n",
    "plt.plot(range_, scores)\n",
    "best_k = range_[np.argmin(scores)]\n",
    "plt.axvline(best_k, color='r')\n",
    "plt.show()\n",
    "\n",
    "labels = set(clusters)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAREAAADzCAYAAAChbyKzAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAEAAElEQVR4nOz9d5gk13XfD39u5erq3NOTw85sBhaLxSInEgRJgFmkROUs/SxLtiXZem1Lshxky35fWbbkIFnB/FmBMilRpAIpkWKESJDIC2AXWGya3dnZyaGnc3fluu8f1VjCkiguSAAEHu/3eerp6lvVVadu3Xv6nHNPEFJKruIqruIqvlYo32gCruIqruK1jatM5Cqu4iq+LlxlIldxFVfxdeEqE7mKq7iKrwtXmchVXMVVfF24ykSu4iqu4uuC9o0m4Cqu4iquDPe/wZE79fiKz3/yGf9TUsq3vIwkAVeZyFVcxWsGtXrMY5+avOLz9bELQy8jOZdxlYlcxVW8ZiCJZfKNJuJv4CoTuYqreI1AAgmvPg/zq0zkKq7iNQKJJJRXbhN5pXCViVzFVbyG8GqURF5zS7xCiLcIIc4KIc4LIX7mG0zLohDiWSHEcSHEsUFbWQjxGSHE/OCzNGgXQoj/PqD7GSHE0Rdc5/sH588LIb7/JaTvt4UQW0KIky9oe8noE0LcOHj+84PfipeB3p8XQqwO+vi4EOJtLzj2s4N7nxVC3P+C9r91jAghZoUQjw3aPySEML5OeqeEEH8lhDglhHhOCPGTg/aXpY8lECOveHvFIKV8zWyAClwA5gADOAFc8w2kZxEY+mttvwT8zGD/Z4D/ONh/G/CXgABuAx4btJeBhcFnabBfeonoex1wFDj5ctAHPD44Vwx++9aXgd6fB/7p33LuNYP3bwKzg3Gh/l1jBPgj4DsG+78J/NjXSe8YcHSwnwPODeh6Wfr4+sO63Fodv+INOPZKzIPXmiRyC3BeSrkgpQyAPwS+6RtM01/HNwG/N9j/PeDdL2h/v0zxKFAUQowB9wOfkVLWpZQN4DPAS7K2L6V8EKi/HPQNjuWllI/KdLS//wXXeinp/Ur4JuAPpZS+lPIicJ50fPytY2QgJd0LfORvefavld51KeVTg/0OcBqY4GXqYwnEUl7x9krhtcZEJoDlF3xfGbR9oyCBTwshnhRC/MigbURKuT7Y3wBGBvtfifZX+pleKvomBvt/vf3lwD8aiP+//bxq8DXQWwGaUsro5aBXCLELuAF4jJevj0lexPZK4bXGRF5tuEtKeRR4K/APhRCve+HBwZ/Hq88SNsCrnb4BfgPYDRwB1oFf/oZS87dACJEF/hj4x1LK9guPvZR9LF+EPeSVtIm81pjIKjD1gu+Tg7ZvCKSUq4PPLeBPSUXpzYEYyuBza3D6V6L9lX6ml4q+1cH+X29/SSGl3JRSxlLKBHgfaR9/LfTukKoP2l9r/7oghNBJGcgHpJR/Mmh+WfpYSghfxPZK4bXGRJ4A9g6s7AbwHcDHvhGECCEcIUTu+X3gPuDkgJ7nrevfD3x0sP8x4PsGFvrbgNZA5P0UcJ8QojQQ1e8btL1ceEnoGxxrCyFuG9gbvu8F13rJ8PxkHOA9pH38PL3fIYQwhRCzwF5SI+TfOkYGEsFfAe/9W579a6VNAP8LOC2l/JUXHHqZ+lgQv4jtFcMrYb19KTdSC/c5Ugv8z30D6ZgjtfyfAJ57nhZS3ftzwDzwWaA8aBfA/xjQ/Sxw0wuu9UOkhsHzwA++hDT+AakKEJLq1j/8UtIH3EQ6qS8AvwaIl4He3x/Q8wzpJBx7wfk/N7j3WV6wMvSVxsjgnT0+eI4PA+bXSe9dpKrKM8Dxwfa2l6uPr71Ol2eWxq544xVanREDQq/iKq7iVY5Dhw35Rx+vXvH5106vPSmlvOllJAm46rF6FVfxmkHqbPYKqilXiKtM5Cqu4jWERF5lIldxFVfxNeKqJHIVV3EVXxckglCq32gy/gZeNUu8Xylo6u84/0e+2jmvJlyl9+XF/w30Pi+JvNqWeF8VTEQIoZIufb2VNIDpO4UQ13yVn72mBg1X6X258X8BvYJYKle8vVJ4tagzl4OmAIQQzwfWnfqGUnUVV/EqQprZ7FXxv/9/4NXCRP62AKRb//pJAxEw5eCqemNelGVSzKB2fLxxCwBr3SfOmSBA8ROCvILUQHMh0cBoxZBIvHEFRZEkoZIGj0sQikQGCuip74yhRQSBhlAlQkhsPaTXNxGRAAlSBd0OkQ0dKSAxQAlA9SRBQSAGUVBGO8FwSmSqUzLRQXFitHVBlFFQfUmYE5QKXXqRgaYkGEoMEiKpIAEv0pGAlAJTi3A9A0VLyBseCYJ2zybvuPRDgyhWyBghbtdEahKhJVhahCIkPddE02OieKBXhwKUgZ+QFAg9QQhJEimopRLm3KScdnZYalcwrIgwUpFSIMSgnxSZuk/FAtWKiD0N9ASkACFBChQ1Sfs5EWlfJwIEiBjKxQ792CCn+Wx1cxALRDpTkCpp/9kxMkp/J/QEU4vwPB2UwfuKFECilkpY41NScWI0Jcb39XTWqRK1oxBbpPfVEyYyTVZ6RZTBsytCEkUqal+QZBOkBOErKJmYRIJMnlcNRBrZFgvMTIDvGph2QCwVpATZ1kgM0vEzOA89wdwGOR4T+BqmGSIRmCM5cvtHpbveJmr3r1j3eCnVFCHEbwPvALaklIf+2rH/D/CfgaqUsvZ3XefVwkSuCFLK/wn8T4BMdUoevusnsD/6OO67b6E9pSEklE/7eGWN4uNrrL1jCs2VRBnB0LMuG7faDB/zac0aNO7xSHwVIgWRiTAWTcK8JLESRCZC0RJGy202j4/AtMtIuc3B0iaPrO7Cn88DkBiSa48u4v7LMVQ3ZOe6HM5WRKIL+lWV51fjcisR1mYfhGDt9Xk6+0L2/l5AbGtsX2+i9yS7v/ccFxoVpvIt8oZLPzKouVlarkVzPQ9Ckh/pEsUK8ngBb6/H4ZlVEgTnPzPHvjdf4JmLk9DROHx4kYWP7iZywJ2IGN9VI6sHzK8OM1TpsHOugjQkIhQoniAe99GXTMLJAN0OCX0NGSooZsyv3/5+/sEHf4TqzZts1tPnjusm9qpK5EhiW6J3FLwZHzwVZ6RHr2WlzEJCsdrFMQOGMx3O1YbpNeyU8YSC+244yWq/yFuHT/JfP/YOAKJCjAgURAKKL7j+znmePD8DnsrM7i1urCzxxY3dSCmw9ZCVc8OoFZ9k08JoKcy87hIz2ToPfP4ImX1NhnNdLj0xiQSiUoTiqvynt/9v/u2pd1DMuFxar2BYIboe417Io8/08NczSE2SGemRs30KZsqso0RhcbmKUCSHZld5dmGC8bEGM7kGj5zaQ3Zep7s7RHgqxliPYN3BHO+hPZYj+8ZNer5BNdvjSGmFk81xgkTl2I9+4EWMf/FSqym/S+pp/P4XNgohpkhd75eu5CKvFtnoRQehJSZkH5zHffct2H/2OJ3dCa29CdZCDakKGrdPMPSMS3da0NoXU99vkaigehHDX9gAAdlKHxSJUCT+VIAx0UP4gy6RAj/SiCpp5HjbM2mFFr3tDEZLYNbTf9StXpbGAYvNW/M090NzTkfxJc0Dks4u6OwCEUtqR/Js3ZQjykBupEtsadQPmpTORbTnYMpukDMDAMpGn1Grw2S2yf6hLfKjHbLDvbRjii3cXQGGFTKZaVI1u7hTIXnDpVjuQj4kSFS8qsSdCrEqLlk9wNZCaOt4oYYshZALUV1BlI+hYZDooBoxlhkiAwXRU0n6Gs3YISwlRIlCkggiV0MUAoKSJCwkxLmY2JTkSn30ppoyEF9F6Ako4Po6Pd/gzNYI7lIOfUtHbWjYKzpF3SWMVWphjqgakhgSo65ibaXvQInguY0xCBVEJFjeKvFUfYrt5RK1jTzrO4VUYmsaKKFA9eDSTpnTjVGiakB7PceFlSrZS6lUo7Y19PEe7diiuZNltVYkm3fJOx7dpo2zqqAoCcqQj7WhYWgxrZ7N/MowF1arlxmIqOsEsYroaOQMnzM7w5RHW5CA0lOhFODXbRCSMFQZfcwlawR0NrNoIuHR7V0kCBQhkS9SskgQV7x9NcivnL/lvwD/nCuMPn61SCKXg6ZImcd3AN/1d/3ArIWc+c/7yM+rrP7X29jzjx9FmCZn//1RkkxC/qzK1rskrEpKJxW8IYF/qM/FUoZoSGe00mBjpUxxpMNYvs2Fh2aIVYPxGzbpeCZj+TbbPQe0hFKhx7WVDXbZO4ze2ObPlSOQQHa4x92jF/iL8WGcFYlVE/THJO6ohrMkCAqpVL98n4a9KRAxeHs8yh8vsnJvqvp4r3cRF7L86XNHkJFCq9Ll7NYwgaeRdHT0lkr+AkQZwR3f+xSf+asbyG4LursVPtE/BIrEXtF5qjqFeKSArUFtyEF1Bbt+N2DrxhwX71AwrRDpRJhaTHDJxB+OiK100oZTPsYZCzdjkiu3Kc/0UYTk4toQO3EWe1UlmFZJPBWhSZzjNiKG2FKJbBWRgHy4RHggwMoGeNLEWDRJdMiPtLC0iKMjKywWy2y0cggBQaDymeX9FGwPP9HQt3XCkYBwSBJrCaqQJFLwY9d8kd86dTeGHnH9yCq3Fi4yXx6hF5k4ms9H3SPsmt5m8VKVRNe4fWKZou6y9uwId9/1HAqSMyPDtJ8bJs4kJFsZ+onJxHgdRw84v15FOj7vvf4pHhvfxXSuwcNP7UceSsdF2exjqyGKSIgSlWd3xnAmAt44fAbj+hhNxNw4ssJnn74WrgkoDXVoXipy3eFLnN8e4p6Z8zzw43s5ZLjced08Q2aXXVaNZzuT5HSPE1p4xZNEIgjki5qyQ2KQtnOA/zmQ5r8ihBDfBKxKKU+IK8x2+apgIlLKSAjxj0ijGVXgt6WUz/3dv0ofcKB6I0wT6fuDC3KZh4pkoFtL0hNfyFtl6gEYJ6n4LJX0sCIkUgqUgb6vCImCRH3eyKFIQAzOkak9JUn/OUWSkqbEL7gvwIAOmQiel0hFAolM7QNCSKRIbS9CDGwNL1DFpYAwUb/8fM8fEqn+LYRMrytS+oUEqSqDtuev+/x56fWlkm5C+fJvXwhFlagk6TlCgipBkciBDUkK0mshLl9HCIlQE6SaXlsREkVINCVGE6l9RAiJoijp52B74fO8kAyVQX8AqkjpAVBEku4PnpfBs+hKahNBChS+fG2pDN5bmF5dU5L/o08gHQvPe4QqSsrEFJGgK2mG9UR82YaSSCU9V6S0MHjXz787BTlQP778NIpISKRAF/Flul6MHPI1GFZrLyZ2RgiRAf4FqSpzxXhVMBEAKeUngE9c6fl+RePAf7iIf2iK8Y/XOPvv05y3u//ZI/Teeysijin/tsrmzYL2Hkn1qYSWtJn5g2XC8TLnf7SAVfLoLOXpFGzkcIySDdl8doSkGtCsO+yb3mTHL7HTzHI8HsevaDz03F6MLQ0Rg7+T5/HcDMXzCYkqCHOQX4DSmT6X3pZB66VDZO/vt+nO5YgsQTRv4r65ze5/HbBzUwXjuQzbN8C7rznB6fYoecNjym6QSMG6VyBBcPpgmhjr2MYUe2++xJmTU+RGO7xuYgE/0Xigdh33Ty7wpdtn6ew4XFeo84wc5vx36+iFHocn1shoAV+s7cPSIrYnQpCgdFSifIIMVPyhBGu0R72bwd3OoPYVxJjHbmMLbyRm2unhhzpu38C/oYdcyhDbCdJMUDsq2nUdWMkS2yFCkcS7POJASRm0kDyzM87GegnRU5GaRGup3H//Cc73qowZTcJijLFmkGig9UVqa+kKfid7G27TImjoHBcSP9Z45NwcmhljZ3xET2Xh4ghKR8VoKZxrVpnItoizMQ+cPIBd9DC+kIfZBBEpiHLAuN5gq53FMkI0PVVX/+TUEZynbZ6+zUHaMeGFHOtazEqzmBpNpSCOFYKGRb2n8IWjEWePTzN17Qad0MIZ7WF8Jk9rroIshZxYmETbMngg3M/4Bw22fjzH8kqFuZktlnplOqFJnCi40YubgvHL6/a+mzRf7fNSyCTwlBDiFinlxlf60avFJvKiIVVovW4Wr6zRuHWMJJOQ2Am9996K85HHUCJJ5KiUz8ZEtiQyBeNf7NO8ZZzujI0MVLyugczEGIsWWltFJiI1rKoJJIK6m8FaU4lDBS/Q6Uc6xILYILX2A13fJLvkEeRSy33+UsDWjQ5KKEh0SHTo7MkRmelqzdgjHuVsn9ahMkYn/VfNLkFW82n7Fo4akFVTiapouLSD9EaJFHTPlmj7FtKO6XUs8ppLUe+T2Ak132Es1yFXSW0nXjVB2BGVYpetfo6al8Uq+NQ6DpoVodoRyYiPzMTpvpkQBhq+qyPsmDiT0taTaR/t9B2SRFDI91G1hNhJkNkYNRciVRgrtJGGJGMFyEQwVOpQHupQbzrUug5ZI0AxYqSZgCqJKhGmErHRy5NIBfSEoBwTj/l4YxFhKcIbicmaAcJIiEshXqDTDi3snE8cKfT7JtJOpQRpSiJb0vXMVEIwE+yih2MFqEEqPYlCgNwx2Y7yRJFCwfZSyQjQLlqICOLzWRQjff7OpQJSQs72yWc8shkP9ISkEnJ6MU1zstPLYKoRvaaNXxTElRBhJOCrRIUYdT6DV1bZbOawCx6L6xU0JaYf6ixtlEmSK5+CEkGMcsXbi55TUj4rpRyWUu6SUu4iXSU9+ncxEHgVSSIvFqoHUhWUHlmlcfsE+bNqKk7GMd47b8H688fZ/PE7yK7F5BYVMtsh2zdkyK7G9MsCzQmJejpCT/AnA8wVg7ilI50YTUtQ9XR5L7YluhVRzfUYtTtkR7q43TwSQeIkTORa+B2VoecErVkLkUiMtkTvpnYMJEhFkF32UN2QrZvzNFbLzHRj9F7Exs022dWE59pjNHo223YWP9FohxadwGSrnaW/6QCgT/fYauQw13T8McGp9hgJAnNTY2ePw6XNCnFXo11oYW0pyLrFRqRQHOqSSEHgadhOQNwwkWYMgYLiKSRDCUZdJTA1rHw6QWM1ST+lglbTSUbA93SiSCHatjG3VaKsQmyrmC2FhfW07GsQqchQYauWBwlmJsTQInK6h2UHuFE6uGUsWHLLAIRSRWlpSE0iwy8btpEwnm2xul1EKlDJ9ZjL1mh4NoEVYGoRq+0hjJJHuG2jRFB2+uR0H9HRyI74FCyPxVnQXEHoaqiBQBUJlpXaIsJAQ1UT5N4evUwGRn2Sjg5GgpYLKTkuWSNl6mGi0jQcFFUyWW1wSa1QsD0yWgCBQqKTqnJ9Davi4u3YRHtc+j2bUq5PnCiUs33G7RZerKMOS7bVF5cNNXkJV2eEEH8A3ENqO1kB/o2U8n+92Ou8ZplI7Eiyl1zW3jHF0DNuakQFyr+tEjkqmz9+ByO/+jCX/u0dhHMuUrFRXRCJZOTz27Ren2VorEVtrQCRINrbZ6jQo/HcEMouH79jkq+0qA/FWIqk3rdpOjb+mQIYEhGD2ldY6+YJ7y7hDYFfjfGLFnpXsnNLhHBVEJLcA5LNWzIkemo3uX7PMlv5WXau0Zn6/fOc++dzfE/lLAAVs8chZ41QqnRii/6QwemhUaJE4czSKG/Yf45HjRmqVsA9Q2fpxyZn9o1wXWmNqt3lQmOIstnn9FREcazNTK7DkNXD0XzWtwtkzABzukEUq7S3suiTPfyeQTjtM1TuEkQq7UYWtaVh7OqiigR1rstQpo+qSLquyei+LVYKJQwrJGNEtB2HG6ZXePrULHGsYOQCyvkefqgRRBq6mjC/U8VdzYIKUk/Q6hpHcstsejkArJkO/S0H4apoHYWwGKO6CscuTaf2CU9hY6fAU8D6fBVpJAg7RnFCgoYFZkKUEWx3sgCY4z22l0rUMhG7Phtw8dsVEBJzro1CkkoxUjBSbqOrMStnxxk9lrD6VpX8aIfe+QKi4FPvZljrF4GU8SEF1DR2Mhn0JRNzPOJis8L4rhrt86P4HZ3cZJvOWg4hBUnNZPpDy7Tu1dlcLlEeb/HIxixlu0/PN0iSK1dPUrf3l46JSCm/86sc33Ul13nNMhFiwfKbHbIrkpU32rAqEQls3iwon43JrsVc+rd3MPNvHmbxF27H2pFYzZjGPo2NW6uYZpvaShG94KOcc1DWNLYnDJQxj5LjsRNoKEJSOq7C2wImCy12ZXY4sa9NGGipjrxtcaC8yebjFrUjWbJLCpUTLS6+J0/1IQ23mjpPNecgs5VgthKMdsjqGwoEYwrDT/tc/LE97P/vy5x9/SgLjQqlEZclv0wzzNCJTE5tj9DvmwBoqyZPFKbwlnN4Qz7PDU2kxrpVm2er46nTVKygiAQRCpq1VO/vBiZFy2V6tE4vMPACHVVNKI226bkmMxM7LG+W6HkGUgqy1R5BQaOa7wIQhRoJAkuLKJf7qZ0gVEh05bKNtxNaFMfaTBRarHdy5A2fQIuo9zJYWsTrx85zojBBJ0htAWJKMu8O0w1MCmoft2eiZENGqy3argW+RpxV+eYDJ/jS5hxNPcP146vckF/mRG6SZmCjCMlys0hmqE2rZ+OZBkdGV9nrbPG7K7dz83UXKBl9PvfW61EzPkOlDr3PD9M5aHNgfJNdTp2H1maxjJAoH7N+l4KxptOzLHJ7m3Tmi+y7+SK7sjtpPyQqn5w/SHmkRa2WgymfRt/mLVOn+fCn74RhSXG6SbdvUp1psFPPoqxbnP3xCcxehxsOLnLi0iQ/esODPNaY5XyniqpcuSRyNQDvJYbmgl9J/338ckxhHgrzEBYkq68XdKZVwjmXxV+4nV3/6hFEDCvviYhsiMYDNDWdaPG6jXJtG60PlWMqiafR9w2SUOFoeZnuNBhazJjdwlQiZsoNso5H1vFQIjjgbLJ6bw5nK6Z4wWPlTQWMlsDZCBl50mPkmEtkQ6IJ+kMq579bo/fIELEB9f0m0b4+F79nise3Zqg4fRqBzTONCRY6FY4tztDvmyQ7JnFX56bXnwFg/69tMlFt8tj6NE9sTLPnAx1G7A6XHp/EP1bGVGKmP5HgnDNoPjLCXKHGXLZGEKvcPXoBdyVH91KBVjtDWLPJ6KkdI2v7JImgW88Q+hrDmQ5VtQPrJkdKKxQtl5Zv4fYMtJpOvG7T2cghtISLT01eFu3Hch3mV4ZZXK7y+snzvH5knmlzh+uKa1TtHsNOl6wRcLY1wh3VBXKqi7FoYp6xabsWQaAhLzkoqxbDRps4UfDXM9xbPsMbsqcoG31uLS9yd+U8Qagx5rQRQqIZEW8qn+KQvULpMQNFSGasOpndLSw7IIwVYhOOWJc4UlzB0XxaSwU21ku89eZnsKY6jNyygX3cpuz0GT20RVb32WXV2GXVmLLqJDWTTt/iXYeewbBDwlhlt7WFkKC3Bc2mA4sO11Y2UFctrrvtPNp0D8cKCBKNa6fXOWovkkjBwcmNyys/VwIpuRo781IiUaFyQlC84KIEFt5QqkNXn0qITEFmO0QqNtaOpPm9t1P8/UfoTdzB8DGf7b5J57BEGgl6W8PbcLDN1DFM39FwbQOlpdMMM5hNQbNrc8kqYyoR59aHSYLUCOtsK8z3hxl9zENvePjDGcYedfFLOmFWRa0nSEUw8aCLfnoJKiX80jCdQwF73xeitVyWs0NYNcm15Q22/Sw53WfU6lx2RvIjjTUKqGrCM5vjDGV7rN8/jhlvcWAoTSK+cOM+5owLJDMu3o6JrsT0hzW84YS4EKEOllE7nsmmn0+NkbFAeqm6VXczyEhBSkHoayiddPVpZ8zBEjGJDu3Iwo80/FDDtEMCxUIaEvQE6asw4bLdcdDV1GhoZkLCUGWpV8a1DBphhtOtUbYG6obn67x57iztyEZFEtlg9aHXspF9FdMTqD6caE/R6tpoPYUnOzMAPFMfJ2+mDMvv61yoD9Fvp05uz/YmmTCbeFXBfD2103inisRTHpH55ZCGc91hLDVMDctmzIPLuwnP5lmZNNHKkuXtEgiJrsYoYppEKkRSQe0reHWLE8UJwjUHZbLLBW+YyEkonVLwRnUiW/LczihaT/DMygT6mQz9IwlLYRFdjXnK3UUrsKn3MgTJi5mCV+ZE9krjtctEMgnusMAbsklU8A/1QQpa0mb8i322b8igumA1Y1beE9GbuIOJX3yYlZ+9AyUmVTCNBLm7R/6hLJ25BH2iR9AxydgBblHh2MYUIoYoUlltFcgbHnHLAC31DXFHEk7WxtBHDHbemDpXVZ/W2D6avuhET0u9ZtYV5B37UUKY+tAiuW/xePaeA1j1DEZTIgXszmzzXH2UyUyTKatOPXI4Wl7m2eY41VKHKFaJP14heXcf754O7Z0c75g8SSwVnr53ioVOhTfuOcvyWIl6kKF+n4fjeBysbnKmMUzWCMhbPo9fmqE60URKQSLBD3X8UEOzIzp9E0VL0MZ7BK5OvZfhuDeJPdnhiY1pNDVhONtNDb4TGtmMj6VHbG/lecPueT5/YS9hrFLvOFw3tkYQa5xYmMR0Aq4Z3WC1USCKFGSioBsRo2aLP754hBm7hr6rSzAlKDkeXduCIYkfatT9DLoeE+3u8tDKHMulEl6ksbaTLnubmZBO20bRE5JQ4YGVfRwc2oTr2xhazGY/h70paI8pOEWfVsbhr7rXcHp7hL2VbUbHG6hC0vvzUYQDzqpF/OYG3baNfdqifmuIocQIIQkTFTnt4lghq0+MI/Op38v5XhVpJewcFthjXVQ1YXu9gHqwT/EBG78MrmtQLbfZOlvlyfIMfqTRWiwSxy9mdYZXVMK4Urz6KLpCCF9h6i8bDB/zmfhCD7FiI1YsZv5gmd64SXY1Jrca0dirYV6wGD7ms/KzdzD5/3uYynMhMhHoVgSLDq3rA6QCfsvCWNNxOxbSVdlb2aY/lvJ+xwxwtABUidJRU8NjU6HqdBGxpPp0gr0pkAL2fLCF0RTkzyvkLyhM/d55Ro75FBciFr9vF0+vTDLzvrPYtQSrKemPCXZCh1GnQzu0qIVZEim42KuQNzy8QCdOBPItDcpWn2ghS6XcZdkrseSWURZsDhQ2eWhljtPnJrDVEOeYTXsjx7HFGfYWt9mf32J9u8C+sS22NwvU1go0z5fpr2fp9iyiloFpRChKQrjioK2ZSGCfsYm7lmVfZRtFSBa2KjR7NnLVpruSZ3u9gOhrPLQ8S9TXaPRtFCXh+PIkzyxOsGuyxjWjG/RCkyhSCHsGcaDirTt0YoujIyuU1R7+Vgb9mSzN+TLirIN4Lof9tE3dzdBvW8izWaq5Lnty29SbWWw7oFrq4NXsdAWsYaDVNaaKTXK6R3Apy+ZmkbZnpkF9vkp9o0BcjLjdmccxA2pulu1Gjp2OQ+fOPiKG3t1duqt5aOkotzQBaHg2O32Hei+D3LTwz+eZu20JtaeQGYQq6DmfoeMQzOfpbORQMxH62QzNe12yywlDpQ4bG0UmDm5iqyGakjC2fwvlRdhEgJd1ifdrxWtWEpGaRLgBrdkiw1+oEw3pAITjZaQq6JcFI5/fZuPWKtF4wHbfRInBf/vNmB9/AvW7j1DIudSGdYSaoFY9ivk+O/0yqhEjjZiLzQpxJcRQEzquSS8yEG7q1KSE4I7GbHRyJGMqQQ688RipqUg1T39XiOulL9K5a5b2jEqig9RgvNyi9YY9tOZUxr/YY/1Om5Lex4s1LC0kq/qEUmXcbhEkGjnLJ5aCesdBU2LU2S5932DUbNOPDeI5l15kMlZosyZhx3fo7InRCz6VYpe67+DFOklX51KjhJVLlyyDrgNORFwzEQLCSCXneNRLOrKj0dlxWIrKkItYbJWptxyiQEXXY+JsDFaMaiTIrsZwvsvqhRxtNQORglX0AFjdKbBjZXD7JizZ6AnElsTcUSioLk/1ppi2y2hlj75iorVURCTwxiK0tkq/nkNKCKoxi5eqtD0TuWXS0w26ziCYr61DItBcwanVUdbzedTJPsmmTT0oMLqd0O2l3rvZ2RaLQZXNlRJqNqRa6mCqMUvzIxhtiS/AGe8QPV1ETEk8X6e9lapgxAJlyCepmfRDg0SXDDtdntscZbTUYWcqS5yJsYf6eOsO3lgEnkb50XWC/ydh01PR1ZgnNyYZy3WA1EB/xWMecTXH6ksJRU84+6/ygEfjDUVGKw0Azv9oARnEaE5I6/VZTLONrSZ0DqcqSPegQP3uI+z+ruMs/fwdiHKMtARy26DWMhCKZNdvCM5/t87Ne8/wl0uHmfvlHs3DReaLBcQdLmFkEjupq7P/aAX/VhchJI4d4BZNeocEpWKPnmsihGT17SaKEaBqMXGYhtOvvz1CMXwWrheUcnUUJNcV1xgzWmQUn1qUI5GCDS9P0XJp+xYZyydKVDJWgKlHZJSArOph2wEHnXUWu2WKjsu9Q2c4UxlG0xJ6vsGh8gaO5nPjoQUyWsATnzpEZl2SeccO3sNDVO5ZZ61WxOsbVH8vQ/MNClKF6miLO6xVjEsmo7MdbD1kKtvg5PYYYUuFlgoJhKWYjB4QVUPuu/YUX1qeo5rroasxLc9iPNvmmpl1nhiZ4fyFUfS6hjzSYURvcdfQBe7PPcufPH4P8XSCMtsjTgTacoY4m5DNeth/UqR2RFLc26Tq9PCmddyVHGpdJ7uooAaS2BR0pyTVUodvnjrO+z52H0z43LN/ni86e4j7Grt2bXFpvcI9mfP83kyNo0PLPPi+m4l8KLy7wU7GQYsVouNFgnKCv5KnfFxh9HsW05WvROHMs1OgSmodh8RKWP6zWb7vhz/Dbz7+etgdYGzomOfyaAZ09kSYWZ8L/zGP/ac5uCZiYX6Un3zdp/nYP3kTS+9Q0bZenMTwSkoYV4rXLBNJQoVM1qe7kyFb6bOxUgYJVsnDSwRRT0/9QFaKiFAgjQSMBN2KKORcln7+DqZ//mHO/a+bEK7KxOcT+lWVnXt8Ft5jghax0KmQn9dY/OYy3mSIMAOGyx0aeoylx/S2HKLru8RNE6SgVzeReoJR9GleKiLVQcxHISTuaSSD7l5aq6DoCXHbYGiySW2lyIeTG/BDjUOj62y7WXZ6mdTN+kKe2JQoIWjTPZ65uAuppXEsfxjcmC7hrmf53+rNdHsWQpE8npsl7BtEXRWZiXk43kXGDJkp1FnulkhUSX9UkBUQFCQZPQAhUTcsOhMgh3yEIilYHl9wZ0hMyXK7QN7y8WKddtdO/T3UQYyPnnCpXqJcbRMlKo4V0HItLCOk5xnUtQxjQy3mcjXcaZ1W1WI41+V/r9yKpiQM6R1EDEk2ppzvIaVgs6+j2hG3ji3x2buyCCvmSHWN3ZltTlljLDplglhlO1tE2DHSU1HsiAOlLTJKgNYX7JneYHdmm2fyY7RFJr3XAyZ/fuMhpnMNCpqLVAVhNlVXo6JCt5HBCKAw18B9qkL9dR73FVZRhCSUKqeNSfKjHVzXQC0GKJFGQe0z9wHJxR+QxHMuyU6G/k19RKAiBIwUOrSjLBNzNTaeG2aXUWPpPg0RQVB4cZLI1SXelxKqpLuRhUihu+1QHO6QH+7ir2fS5DZ6Qm2tgF7wEaHAqGnodgiXMtS28vjlmHP/6yb2/fAxpCJZvl/QPAD7fjVAGpJdH4bvGn+M3rjEm/XJDXcZGW7ReLJKtdAla/ns/lDErqF6miAnEIgwdUZSnssijYTMsoa9qqFdsFDbKiJU0LIhphOgLFuIQFBbKWKUPG4aXeZ79j2BowUcLG7yjpnnGC10eOu9x7jtprPcdscZ5qo7fMfdj6C1Vd578zF2l2uMOW3sNY0f3vMwQpGEmza7MjvknzEY3rOD0BN++tpP8XP7P8FGL893Tj5BstslKCTs1HLIaZey2Wff2BY333mGzhwc+IUmu/9Hwq2VRfYaGyTTLv9gz4O8bewky50i9+09gzrdQ5/sYc902DO7SRwrTOVbhFLh9WPnSaRgp5nl2/Y8zfdMP8a8O4wb65TtPhOFFo2+za5cne+eeIyi2qe9J2bvbwdsrJaonRpi/294zP2q5EtLc5hFjwP/rsGB7Dp7rA0euTDLqNNmrriDWfKwHZ9MpU+x2OPW/AJFtY/mwpmTUzzVnKKxUObgxAbdwGD7rpA3O6c5kN1gxS3RnZb0JiWWFhGcKmDnPZQYbCPEONJAKJLFfoUznRHmO1Wu+Q9rmH9aZKraYOgTFt1pST3KsviDCRN/plP9C4veRMJQscvBn10lY/ksrZdpXB+z9dQIykSfs94YcTmCqp8mcbpCSBgE/l3Z9krhNSuJmHqEcNLgKaFIxvKpP0GnYGMuWviTAUQC5ZyDdm0bb8Mh/1CW1vVB6ppsCYSrcu7Xb2HfP3ic879yG/Goz4X35rBGO2wdzfPp+iGGTkhqb00IAo2i7aF6X44CXb/NwuhmUVsqctQn6WkYNY3wQB/6Gv2pKM2mlYjLUkn2oQyv/6HH+WjrCGpDh1AiGw4Hr13n0eYss84OY0aTfmxy+9BFzveqKEgiqdD8rWmWfqKPtb/FE7UZ3jR6Bl3EPHPNBF+o7+Nb9z/NwuQQ/cQguLPDtYUdbqou84XmAYp6n2vL6/zZxhEOTmywXXKYyjVp+Ta2GuKp6UpIbEpO/5MhVFdhT+jwhDvH7pEaf9U4wIjZ5ubqErqImR5q4GgBGS1g28vynfuf5OnmFEXdZd0r8M5dJ3FjnY+vXMuw0+VwYZWVgednLBWmC02GjC5/uHYzb6ieozDT4vyPZbhh1yXWe3nmfzKPjBV2l7dwQ53T/6JCvjXDsllmYrjJpVYZN9CZKLdYaxQo59KYoYdbu9nnbOHd1GO81KFkuJgNBT/S2FvcpvHEMB9o3Mq6X2DarmPvbxLHCo0/nkArCqZ/PmLhX3Ux1Jjc/84TfHufstFHEQl+ovG5n9yNGHfRf2uM7lyaVS2UKtRM1u4GddQll/Gptxw2f77EzK9D/Vsl5raKc8MOmd8pUf9XDrodMvJhi7UX4bEKV0tGvKQIugYyUjBXdPypgAsPzaSh9sMxsS0xVwyivX2UNQ3t4XzqBzKXIHoaatVDbhtMfD5h+X7B+V+5jT0/9SjB/Tex+M0RUS0DUxH9SEeJIKmZqJMBiRRk79hmq5lFUSRxSTKccfGbZZSajdaDMAdh3aR4RiWy0lQF/fEEcyeN7elNST76xFHUroJZFwTXuegnM/zJyhH6gY4mEk63R2l6No2+TW8thwgGjOs9fdZP7SFzwWDpoMVfRIcw1Bjz2Qw7ww4fvHAzMla495ozJOeyHJs/QFiMGdlVJ2f6mGrKdM88NIuIoXvIxDtXgJtgfnkEGSg46wq9vRGxHTPfrvJt5cf574++A25bZr2fx9EDnlsaQ1uySHSITYm0Yi5uVshnXUpmn4vtMic2x9GUJI2UFQkTZoPtIMfZ5jC9QCc0Vc4yQtF0mTTqyM+WUXYlXCqViGIV2TaQWsLR8jIffvYoRIJhs8MhZ5VeZLLu5sGB0wvj2AWPtdUyQpXcXF1iTG/ifMlh+NvXmM3UeOiaDhc2qrhDOhOfD7jxOxb5C/96AOSXSkgTjHdu0WlmOTObo/AlndabfTo3K2T0iKrRAdIYH6mBbQe0vyug37awzluMGU3m/sTn4rvT4L/4wTLW3Q2iHYu173dxjJj4kkE543L+/gKvy53hw8FRVu5P4MkrH/PpsvyrT3l49VH0ImBmfcK8xMoGSDXNoapkQ+JRn6AcUy70cCcijJYkykj0iR4yE1PM94md1Aai9hVkJSC4/yaMTx1LYyMUiZCC3dka7Rkl/Y3jYmoROdOnnO9TyfeQimQs08YbjonNdOXFG06Qauo8pUSkkaFOQpiTBAVJPOmhN1TiTEJQlFSKXdzRmH6gU8q4APRCg1gK3H5qY5GGJLET9o5so2UisquSSrlL1zPp+gaZDUnBcBENA6WlYashRjNlPEZdZTrfYNppYCgRB/MbaZYvf5ArVZVUrB5GJsAo+EQ24Kc5TSedJrqIkCrsztcYy7RRkGhGTGwMcokIiZ4PiNsGw9kuOc1jItsiCDS8QOdAaYv9+U2Kap8Ze4fxbIuRbJei6dILDWYydXQRIWLQugqqIlGVBLWrIDyVjBKgm2kc0v7MBvuM1Ei8J7fNntw2ihlTyvbRMyGZvMcBe50JvUF2PUZTEkpaj0q+RyHfw9ZCWnMGFbXL7sw2WdVD70k0F64tb5DLuuSGu2S2Un+YZNwja/kM6R3KWo8hvYPiC6JIZbZcT31TdElF7dKZMdM8u4GK6sF4vo3aV5it1qnmuoS5VG0xij5VtYOiyVTVfpEBeFc9Vl9CCDsmWnZInAR/zWH8hk0ksPnsCNJKkE6cBtONeezcJtF3NIKOibGhsdMvIxTJzj0++3414MJ7cyx+cwTvuYV9P/o4537zFvZ80Gfs9S38kgQtod5xyGU84o8OobxrBzfQ2fOBDhevLSMNiTcW4Y2mqou1rtG71kdfMZAK2CsafjlNVYCvMnrDJrUnRwizkvqJKtquPm+YmMdWQxZ6Q1xbXGfY6HDMnmF/fpOan0URkqe2Jvj2a57kA+3befvIJXqxgR9rPH6oxHeWL7C2r8DmRhGAyIG525ZYa+e5t3yGnOrxX+ffyPWFVTLXNWhu5RDNDJm51DdlrrpD3vA4tryPA7/VQazVKHzSpZlkMPe3mLW36RsmH1s6xDv2nOSz5n5UJcHQYnKmT6eQBrQt9ipMZpqUcn06romjBuRUjz/ePErDz1xe0lxo5rhz+iJlrcdaWKJ5s8/Yp3TWR4vodY3RxxP0XszvZu/ALnhMf8Tn9B3jtKIMf/HoUcb2bKMrCXYmYKuex7RCdDVmya9Qj7Js3qyw+aV9LF9fpP3gCDP3LbLRydG4IyRGsOEXWOkXaVwfQwIPLc1ifz5H56YA96jA7eRQtYS19RIfVm4kShRiKdj9R23ae3MsfkfE3K9KFt4r+VzrGnbe5jH5QQO3YrB9Z0jQKDL3Jz3Oj1ehZiItyernpwjmfH5r8w0koULc0ZEvKhUAVz1WX0okiSCpBmmXZqDjmWnGqWpq89C0BGWXT8nx6PsGrm2knqiqRDVidv2GYOE9Jud+QMca7aQqjCI595spIzn/+zdwujeGnHbZ/T6F9kyWMJMjuL9FcmyIMJtw9idC7IcLmNd1iCOVTMYnjFRcy6JabdOy0yAxr2+gGOmKjqbFtF2LcCLAdFJHpZFCB01Jde792U10EdONTcYzLZb7JXY8h25oMFNocLI1Tnaky5O1Sd4wOg/A8Zk0UK4f6Fg5n3GziT/rMb86jJSCJzu7sNUAxwi40B8i+asy42sJW+8OMT6dZ+ktCZ2tLOgJxYsKZ3+oAFqexYs9fvLmzyO+WOQLpX2sd3IMZfr8xflDaCeyxAmEEdTGEgr76sw/N8HRGy7w6XMHMcwQTYs5tj3FqFNkMtOk5mZZe2YUa0fg7ws4klumFdvcZC1Qetxg+0j6bsNKxNaNGpEjsAtdJv6bzuI7LTZX56g4fbQhl43Tw0gFJj4v8YoKumvR2K/wKeUA75o+ieoLvLGQiWyL9k0WZ56dIjfdJlvqM6e1eGxzhoOVDUon0vSO1W+vs/A6wWihR3t+BO/pMuzvUf2CQfKdAk1JIFE4+/cc1C7M5rrM/8gw+rqgqPVR1ISldydkFjQqj2v4pQLz3xszPbzJlpUlms/hlxJMJ2Cfs8nZv7iWjW/20dovhim85ImaXxK8ZpmIrqWBS4qWligYy7eRUtCsO8hARdVj/I7JTqCRhApKS8ctKkhXRRox579bBy1i14dh62gepiJErLDngz7nf/8G9nzv09x9ZoWHTl3P4t/r42SaZC0f/nyU2W+fJ5IqwU9V8H+xxuLKEEJAu5VLVaFQYXsrj76eSiLCkMQZlVhP0/kdnF3j9EYWr2+DAiuhyutHQjJKwHpQoKT1GTbaLLllJuzmZcPeZy4e4Lv3P8GJc9O886aTtCMbAH8pi34wZrLQYnGnnMbdbJkcvf0cG7089xTPUFR7bHh5bi1cZOltZVZqRUr5Pua729xUXcId11GF5LObN7Drz0NEAu/+1S+yHGfJ37fBe0efRBlN+Nj2Ef6fax/iE+VDaEqCqUaUzR513+HW25ewlYCbD1/iieYMrcDm1soik0adlaDMwdIG+1+3hRvrbLtZ5t1h9mc22IiKdO52yX4pQ7S/R7dpk10GpMCdAvfnWgy93+Tb3/4Io1qLf117FzfclK4ANa7JELsWWdtjtx7wrpET5BWXMJuAJrHUiGA+z5vvPc6O7/DUM7tZvj7Ld88+znpQ5ItHI4gF3e0K+oksm2M2oppw3Y0XOXFumub9fd5Q2iBKVEKpkP0tm9U3FVhYG2L3+yVrdwmGjTZB16DymI5XgfrrfEaqLQr/7xCXzCFUO0YZlBiJlh04CGvviNAu2UT5F7c682pc4hVSXlFC51cdnH1j8sZf/x6iJLW8JxIUAWW7T93NkEjIWz6KkBwtL9MMM2l6wco2F5sVbh5ZYqFT4bvGH+PT9UP0I53d2RpjRovTvTHuLpzlAwcmKXypAkAQa+zNbRFKlY8+cRThK/z9ez/HZ7cOkEhBxeoxbrdY6pWw1Iii4TJitEkQXOhVUUSCF+sstUuMZ1sYaoyCZNvLoisx6+088nNlWkcC8uUe7brDrqltFleGwE1tN7tvWOHiY1P8wDse4M9+5V6s79wgloJvmTzO//uht3D0raew1ZAHP3uY733nX/GhC0e5fmSNS/95P6qbcOndcPCX6yS/6XGgsMmnLx5gpNBh64EJIkdiNAXf+X2fY8assRkW+P3feAu9u3qMltvY/ypL7UiW5us8lFWLzP4mthGiKgmbJ4c5ets8xx/cB4LU/f+eBo4ZoP9WBXvD49zf19G2DaJ8moYSCaPVFtEHR9i6J+Sea85yU36RDy3fhK2FvGvsBN3YYtUvcrY1wreNH+OXPvIegmrM3def4fHlGVQ1oZLt0wt0kkShuZEjs6gT5iQ//M7Pcsmr8MDFvewZrjH/pV2EhYRbjs7z+PldFB63aF4X8Q/u+hyWiPjQ8o0MZ1KJ8LbiRf7H8dczPVKnYvU4uT6GaUTEicLPXvtJznsjnO2OsD+7yaRR5z985ps4esMF9mW3MJWIbmzyqaUD/KP9X+DPNo7Q9i3ePHaGfmLQiSw+t7CP7z/4GBtBnj/87k9TO127InFk9Nqy/L4PvvGK58h/OvKRJ19MjtWvFa9ZJmLOTsrRf/ETaI1BWQctSf1DfDVNaWhLgqGY0nGV7jSYzTTben8sIa6E4Krk5zV645KhExIlgvaMgl+SyGkX41SGQ285S+uuHc79zo0Uyz3G8m1On5+gNJJKPfGDZapvXWH7LycJChJzR6BE0LomJjevXs7C61bTfKEiAaMl0d5Ro3FyCGdV0N4fU3pG4dofeI6F1hD7i1toSkwvMql5Dou18mU7gjiTxby+QW++iDnX5uDwJpqS8PQD+9l91yX8WKPWdbhtfJHPPnQ96lifQtajkulRNvskCJq+zcXtCqGnMT7SZKuZ5ejUCud2qoSxSvBcgbCQIGLB7bec4duqj/NTT3wbR6ZWyGgBUaKy6eZY3KhgmBGGnk6uiUKLBMFcrsamm0cRCUGi0fBsqnaPOyvnebYzQc3L4scauhJTNF28SOdtw8/yiw+8EyLB9MGNtNbOalrj5t6jp3hqc5Lmap6333yCPZlNHtzZSz8yCBOVWtdJbQUD28KbZs5ywF7nlz/2LuZuXuZwaZU/fugWKrMNRrId3H83zrf+2id5srOLSavB7z5816CYVlqDJ3dRoTMjGb1uk+7HR1HevMNbpk6nEdWJxsc+cRtyTw95yUHINGP/j37LX/IHv/hWWnsE0R6XuKOTG+3Qv1BAbysEe1yMeZvKHRvUvzjKP/6eP+NXnn0jyokcFz/wn/EXVq+YiXzPB998xXPkl4/80SvCRF6z6oyiJuj5AJkL0YBSoYciJDvNLF5WQbciLEXC2wLKWkyzaxNFKgIw1IS5X+6x+M1l4qGA2lsTkpqJzISgJex+n8Li3+sDcO53bmTfDz6J//abaRSKZL6lQ/dkmSibIG5wUT46SXB3B1VNsG2fjmuiRyq53V36gZ6ugHQyJAN7SNfXqCgJYrpPtDfCjBWs9/QYMTvoxYRJu4EpIjaCPGUj9X3wY422Z2Lc0kNXEnLXbdLs2+zLbqEIyeKNZa4rrvHQ5hxZy2fUbOPMtnD7Jn1fZyIXXWYgmpKgHc/iNCWt+yysJ7KctYdp1rIIT2XklGTjnnSF6cnVKf7p2KcwTmVYL+fp+wazpR1WdopoFy2khDAU+KWE6HCHhXOjlA73eXZpnHKph62H1DsOqpBccoe40Bpifb6KtakSXtPnH17/eRbcKhN6ndx5lfbBkJVa8cuZ0DMxj67OUPpglub9CZ9b3MeT2Uma3QzRhSxKDLmLoAYQD1I5fEbux5yLSAzJ/PIIBdPFHO2zs1BC3ZNQ/5GQm+2L/Nb83UQjCsXnNFRPorynRrOdIZyNUM7l2TwxQnRNxPBHyjzyvbNAusQaTATQMsnvb9IZRPqu+iW27g3SgM6VDIVlgXKiSHK7j7HbRZMC+jarF4dgNmArzDPxWwYL3xZibl/5mE/ziVw1rL5kMLUI2w7IWT5tz+TaygYKkuPxOF6gU831qPdtJgstxuwWl6wyq60CjhnQcU2ah4t4kyG5Up8g0FAnA4qOS73j0J7J4mSaBLFGsdy7HLRn3H49256BUAEVCvk+1ccigne1yRk+I1aHpW6JXmhwuLJ2OfvWeXWIouViqRGrnQKHyhucEiOULJdLjRK7CzV2W1vAMAXVZURvoSsROSUNYvMTjR3LYbVbYDrXoGz0eHRzFwfsNQC2h7LkVI/DlbX0X17EHBjaouFnyOkehhpjKBGGGuNHGv3ZEL+jMmJ7rM/GOImCmfPxFYP2jImW9S5nLGtLE3dXQDZWUZQES40o53tsTKYBj4RpOc1ri+sskKZxRMBItkNW91nfKeCGOhtejmbPRvHS4MU4VJjS65zujdFLTNr7InLDXXw/Zbx62cOyQrqreayKglFw8eoWG20TYkFxWaD3JL1xwfBTIdtHdCI7IahnWB4tYc10cNeybPXT9IvSStCUhNlqnU5i0XMNWoFFa1+ahtH007QNqpLArh7KBQf0hMK8z3Izj6Ikada3okt3J0MQaihagj+UsO4VGB1togjJ+o5Fez9MfyJBfXsXN9BRlAS/JFG7KnE54VxvmCijPu+C+qLG/dUAvJcQhhJzbXUDRwtohRa77B1UkeBXNPqRzqjdoemkLuCmkmYWzxsejhbQi4w0mM4McMyAou2RDOrc5jIeYSZH1vLZm9siSFQahSLG7dcjHjlB4V/sozEqMDNhGgw2UuD60hlMJWJI75LXXOqhw77MBltaHkVIdCWmrPcGNLjkNZcDxS3KRo+s7nNtdp2M4jNt1imoPRzFp6C6WCKgoqfSiKlEZLSAqtElr3lM5po4Srq6U9F7DOttGmEGWwkY05tsWbm0pi8wYTfRRUxRT/1Q5q1hEl9hzGmzlikzVWzSDw22lCyJbmKYIaqaUM32qCguuhMymWuSSIWpTAMv1mjnLeJYIQw07Eyaod4cctmdrbFWLlA2e+Q1n5Fym/Fsiz3ONkGi8VzfwNVMKpUuOcVlT2aLUa0FZkLO8nHMAHWQQKmUcenmbPySRcYKsEbCdLUr0OlOaWh9QZSVNHfreBWJGPHJOx77s5uc1kZwcxG78zXypseCWmE610BTYqpqj6mhJruzNY6LOYSEfUNbtAIbU41YjMq4oyFW3sev2EyW11GQJAg6vomXMSjneriBTjuxOZDdoOY55A2PNasMsaC+X6dqeYzl2mQ1n2NKkdiJUcyYa7PrrPb3IiyQ2ovL9v4SJ2r+G7V4hRD/CXgnEAAXgB+UUjb/zuu8Vm0ilYNVmf0n/xxnuEdvO8M7bzwOwJ8/fQRiQXaki3+mgL6vzUy5wbn14TShkCoRroqo+FTLHRpPVlG91BM1Z/o0/2gC7/42xgOF1D39qRvIlPv4nkEh32PonefofXIuzfL1wBDezT0QkkLWQ1Njths5YldjbmaLtUYBRUlwN7IY1T66HtOtOXzfLQ/z/idvR7dD1DMO2Ztr/OjuB1kKKqx5RQ5nVyioPVaCCiN6i1acIUbw6w/dy0/f/Qk+tnk9c9kd9mQ28ROdP1s5zNsnnsNLdI43Jxmz2zy6NsM3zT5LNzbZbW2TUXx++9IdHCimKtBCp0K9l2H/0BY7nsOubB1FJHz27AEyz9pYNcmtP/YU42aTU90xJuwmOdXj0+sH+fapYzzU2IOthphqRDNImfVTjSliqbA3v40fazQCm0iqVMweJ2tjdF2Tcq6HALaaWb7twNOsewVsNeR8Z4izZybQyx5h3ULxFVQ3tSNVb9pk+4kRpu9YQVdiFr44QzjngYS9E1ucWx5heiytBlk0XTQl5lKrTPt4hXjOpfRZm9kfOsdyp4iUgndMnuRYY4a6l0FVErxIo/HEMNWnEzZuV6gc2sYPNZqbORBgFjykFMhEoJ1yUP3UC3n2zwIWfkAwO5nmWll+aBKpwPTtK5xfGkZfN9ICZRpEwyHWJYMwl5Db26TvGcgLDsu/9Sv4F1euSLwYvqYi3/u/33rFc+Q3bvzA32kTEUK8DugC738BE7kPeGBQUO4/Akgpf/rvus9rVhLp9iwqmxp+K0+mJdLSlorE2NKIDXC7eTAkYaClOT8CFTR5uchRGJk09JjqzZtAOqgVIVHetUNybIjZb5/no08cpTTeonuyjFChMSqwPzmH85YFsjdeS+c/bNA7NUzixNS6BqXhDkmgIroaTdfCHiSscQG/Y+IrEmHEfHj+BkRHI9ZjomqMHql8obmPk9tjqaekFOz4DpYWstE7yGqtSORpHNy3yn977l5GCh0++eANvOGuZ/ETFUuLeLyxi4v1MlGk0s5bSCn4g8/fiRj2kLFAqBK5YbFmDWHWVMKsRPUFp2plRALdzQlauxX06zoU7t3ADTUe+/Wj/MOf/mM++NE38shYQvaiQvvakF///DvRu5AYqYNv+UzEiR8cZ+oXFS78QIbuQxNs3imRZoxe04ktiRj2UC/aZP9cIGJJ75Ys4X6V49vj3Dsxz/yJKazJHjyXA0siZntICZoe431kBPMdTTY+NUVvKoYZn+zTNmEOWp+YopQTsFLFeW6Dc9+zG3mkQ5Kk9gsRqOy8LqDwL2do3mNRvX2dMb3JxT+7l/ZhH82M0fSY3NEd1uYc9oxts1grM/tLMcrPt2g/VyEIMkhFImLB7D1LXNgc4uaZJRb3ldlnuVzcrlAtdJH7ewQ9g/Pnxiic0tDvr2HrIesnRpkYr7OmFjm8a5WNXo522+baOy+y/BsvLoo3Sl66JV4p5YNCiF1/re3TL/j6KPDer3ad1ywTgdStPCYtQ5lWV0xXYNLKlum+lOLyv0hal0akOrmTYOkxUaKgKWmaO1VJcAOdMJsQSRXhp3lHo2wCapqKz480sjdei3zyOcJ4L0kmuVwjJR6sECiBIEmUL+uvkYDnA60ERKGa0pYIRJTS1wwyuL7BFlkMNaYTmLiRfrmQNsmgzGOSVqcH2PEz6SpFrNIJTMJQJQw06r0MYaimbumJIIkURAxmU8GvkCaOziRY2xphFrJLaa6VRAPbDIkShTBWyQSSlaBMlEn7WIlBbaVDJsqkhbmkCqGTqjUiDNGbSjrhAgFxOuBFBHHbwOoIpCLoj9noPcm6l8cPdVbdIomdICWoPpAIYglxpKIoEr8skImSZiiTIAN1UEBM4lUUIgv6wxrWdhEAv69jZ32CyARfQZoJUQaMVvoKNsMCeleitHSsGRdDi4gTkf7RQJp0ydbRVZ/ElCiuIDFA8dM8q0mkoCAJIpWs7hN0DJSiJAoHE9yKCbMaIlaItXS8ZQ0fpEBT0iLhViZAE/GgJOuV4xX2WP0h4ENf7aTXLBMRekJQSGu+umOS7HCah8LfyacTzklQ+wrxtkVn3cLZVnBHEvSuwB1NXZ17Ww6jHzJZv80iLkk2FYc9H+hw9idCgp+q8Pd/93N84HffjLjBTY2oTo/1j0/T+Q8bhPFeSm+fp/S5SRaOTYEAb6OIDqiBoDNfxFlNs2k5KkQZhdiUJIakfLBF64wDLYvYlPRWc8xNn2Nvdot66FDQXQqay9PNKaazjcv+C8eXJ3nL3lN84vM3cvddzxFLQUYLufSFGb77Wx7g49G1bG4WedPMWT7zkVsYev0GHc/kB/Y8RkHt8/vjt3HX8AU+fulamjtZwsM9TCtk9s2btAOLKcPj6Yf3Ufh8RNaNueGXH+N2Z54/vO5GfnDv43iJzuc29nOovM4XV+aw1BhTj7DvDqmEOvp/63Gjtk75vj4nG2O0XIvbxxcZMdqs+QU2D+aJ3qKgSUG/m0UXCd+x50mG9TaP6Lsp/EWWnftdkoZB9eMOaiBpf3uAftcOmd8r8rp/+SV2WTV+6an7mbkvzfGx1s5DrBKpMTvvlXzP9OMUtD6/+pF3wGjENYeWuPjpWTL/bJHQt1g/Mcqtez5G48cy1IIsnz9+EAA1F1J8yuB8exK1L+j/y002V8qYEz0Oj68RJQqRVPH/WZXsXRbH9Gkmf0fnxL1lvvP+h/iDx25j158m7Bwy6OwLse6oUfyvORa+NY/IJcwfnwIVnjw5x3fe9igfPn2U48fn0n+8K8TXsDrzogt6X55fQvwcEAEf+KrnvlZtIkMHh+S+//bDlO0+W70sd49eQBGSx2szdH2TiVyLtW6eA+VNDjibzPeHOVkbo+p02ejk8B+tEF3fZddQna1ulmLGZSzT5mK7TOvhEUZet4qmpAWYdz46ychjHbwRm/Xv8onXbZJMwr69a/DGFXJfHKJi9hg3W5ztjuDFGjcVl2jFNioJz7QmqFpdDCXibHOEu4YvcLI1TsXs8UxtnDvHFjjqLHKiN82UVWdKr7MclhnVWjzTnyJGYcvPcaYxzKHyBvucDf546QZ+Zu8nATjlTtBPDFSRECbqZZ+GZbdE2eiz5aWqmhfr1N0M9W4G39PZNbrDpa0yhhGhqgn9rolsGFTmGkSxQt8z+I9H/4Sfe+ab0NQERUiuG15jtVdks51LkxeHKqGv8X2HH+N3H76Law8uc3p5lFvnFsnpHp85c5BM1mei0GJhcwi5ZmE0FLx9Hr965wf5wObt3FM+y385+UZmKnVWWwWEkOQsn4wecvH4BEZToXTnBhunhhEJxNmE4YdV7O2IlTepjDwq2bpRSR3ZPIW561dRRcK505NUZhr4kUqn7jA1scOu/A5vKz/Lv3jsPRyY2uDC1hChp2E5QZqdzkrDA1aeHCce85n5oMLy98WXV2emh+ssbZVRtRhdj+m1bK7ZtZYWLRcxxy9OYdghI++3KfyzJRYbJXQ1pr5cRIQKFAMOTG+w9ie7aN0QsPNTv0a3vnxFnGHo4JB8++990xXPkfff+ttf1U9koM78xfM2kUHbDwB/H3ijlLL/1e7zmpVEui0b/49GcOc9ogMWfzE+DBKK5xOKSx5+RyW8u8Tm4xZP3XsoLeswYtCKSyRjKv6tLnHT5Fx7DLWl4jfLXBoeRRoS87oOiytDzE1tp45kd3cI3tXm+tIZ1s8fJHHSqvMLx6a4/osenbtr1N5+MyeKKpUHFll/9yyr3T1EGUCCXU8I1jzUrk8yl+cj35Vj+IM2wakdgrcO8ezJHMEvajy1Pcm+UpZFY4heZPKgv5fldol6wwEBSVfHC3S+cOowybTHn+8cIUHw8GcOcfB1Cyy1irQ7GW6bvcixzx4kKEiMsR5jpTYFw2OxUSJjhHi9NA3k4nqFxNXQhn066zn0kocc8qk3HZJAJV/uUVT69Dcd9uxfZ6eX4fTOKI12BrlhkTgxwo6hYfBni4dRCwFNz8awIk7XRogShUKhTzHjMuk0aRRs2lpMMg1arPCZ1iHqfgZDRHjbNuc2JtGGPEJPo+PnEKHgrjtO8cTKDGurZQ7esMTR0jIPbc8R7lIRaozcKNP8joCoaSN6KtffdIHd2Rp//PAtjO/ZZjjT4dRf7YWRiI1Gjq1Hx3jbdzzL5EiDrO4jJTh5j249g76pU8slbJsJyoSHdDX4p1sctdPYpCDWOHF+Ct0J0LSE7pZDcbTDtNPgkw8fwd5QENe6+G2TnR/qsfTcNACZZY3sLWnYRLtvcbS0TPIeQdgoUqtEVzzmX4kcq0KItwD/HHj9lTAQeA2nAnjewUh1Q2JD4KxIssuSRBXUDmeIihbeENSOZCmfidAbHjuHUmt/kAMhJEiBcBXkqI8Skeb8INXFhYCK1SMoSFQ1IWf4mEpEIeuBmqYnRKRlL5/3I8lshnRuncbekdj1KK0DvJoOEq3WRUQJtes0whUH1ZME4wW8YUlrzmCpV0JVEvqRwYaXZ9vLcqlVou/ryLZB0tMY31UjShTyF8C0Aha7ZZa6JXKXwFAjGqsFxIpFIgWF82BvKETLDo6eJg8KQ41Rp4OoG8imQdLV04xrgAgUVFUilCS1Dci0aJclQoyGSsXqoakJfV8n7muYDQW9rkHDSKv8recvO4kZekSr4dCtZxjLt5lwWjiaT8ly0fX0HxwhqfnZQXqBtBawWVMJPQ18Fa2lorcVslpAHAtET2XKaTBrbmOqEaNOm6lsg8RXsYzwss1p2mkwpHexNlOJLKv7+NUIFEkSq0hNUtXalK1+mvKwa+C5BlOTO4SlGGeyg76joWoJ+eEuphpR0D1ymk/ecCFQiEOVQyPrCDtGUxOKWj9NL+lD0tdQmxrFjIvWVSlPNQkKEscMkFIwWWwxorfRlITxYhvla7CJXOn21TCoxfsIsF8IsSKE+GHg14Ac8BkhxHEhxG9+teu8ZiWRRIPuNKheju6UxKqlLudhDkigNWvhV2OySwpmI8QfTmvhtmdUvPEYxw7o1U1ELEh6GlovNep5o5DJ+LRbOcbtFmd3BLadOpIN6V00NaY03CFOFLyNIuNmixNFFeVNN6J/9kn8b7uNzpTC1B+tkFTyiDBm7d4KWr+MEqQ2nMROaM7pFBcgKMfozympS7mhUDJcEinQRIKuJniAtGIUI8YNdCw9wqonmI6LrqQ5M6LNGEsNEZmIOBYMWx3OK2n+1NhJqFpdynqPzVKO8UyLE4UozUeb9/Esg9lSnZP9dPl1u5HjeU+z2eIOGSUkqMRMWE2ivELLslkDXFdFWglqJmWSioSJoSYTTou2bXEJiGKFMbvNkNll2kxLUdpaSILAjXQUkVDWe1S0LlFGovWgPNSh75kEfQepCGasHarFLus7NgeddfaaG8zldshpHrqIyQ31mMq3CCKNINDYa28yqrUQEUxmm+xztviSswcn75G3PXqP2ugiZn9+E0sJeUyZA0BXU6nKdQ3UGMaHmqzVCwzbHQ466wB4ic5D/jVYwwHtwEqTTHkGu60tjIaKVEHLhsSBwmSuyaYcIYpVouGQfqCzq9Rgq5dlr7nBF9S9bPWyqOLFpkd86SSRr1CL9/+egt5SA2dZ4GyFhFmd/lhaize/APlLASKR+EWLyokWK28qMPaoS/VpDSkkUlNxi4OEP1qafzXMpQmFRCIIIxUUyVKvhBJBxzVZ6pbIay7bjdyXrfjA2e4IlQcW6dw6jf9tt5H9o0fp/fgd1O+eJDLTWrzORozRClF7AcNPqtR/qIvzuRyZk2vkD86QXXWZshs85U6hiISq0aMbm+wqKCwrRcIwlYyay0VG5mps3aigdjMcmk4H94M3zjKRqAxX29QNBzfWac8KomJEdqTLju8QS0EvMJhvV1GtCKRI/+EFbPazJIFKxzNJEgWhJchAZa1boJ/ooCVc6pfZ8Zy0jq4gzSurJ2mdmo7ByGSDrXYWU43Y7jlkzIAgUjnTHKZiZ3EdnWcb42ntFtJYlyOlFS72KnSyqY0pVBVc38B3dWQmQQSCc70R2q6FNBIebc7hJzoX2kNEUkFXYjzX4CJl+l2TxNU41t7F7sw27njMej+VjoxLJr2ZVPqMClCPszzXGmPE6mA4AYYRsXBxJHUPsCRBNWKtXiCoW6wPFVAHElaYqCRWQhiqLGxXkD0NJedzzhslzCfovXQlTWZiztaGiTMJnUsFrB2FMOen9qiWw1P9XfQjI3WzfzFLtvKlXeJ9qfCaVWdEDKVzPokuKJ4PUyO3gNKZPjvXmLSnTfSu5OJ78ogE/JLO9lFB8XQbZy0hiQVG0cda04hnXdzxmMRMMDdV3JaFCBUsNaJ1TUwUqfRCg3roELtaWtho20ANBF6ssf7uWWJDoTOlsPnjdzDyqw/TmlNIDIgsyF7q4w6b1K8rsHa3SnfbIXe+w+ZbplFCWHyrxapXpGB4XOqWWexX6EQWK50iI5kOhhFjmiG7D6xRzfRQPUE522e1X2SxU0HxYcjssrlWRCxk2PayFM+lk7C3mqNqdinqLpvLJQqGS9wySOoGytksoqGztZOHnoYqJLoRoa2ZmGs6Hc+knVioTQ1LjXBDneXlCmGoorUVlJpOtJP21eZKiX49w6WdMn6os3GpQvN8mYLpkdV8ju9MslYv0N5x6LZt3Es5Nv0cw1aXtbCE4ioU5hWSUzmckxb5sxrlEwrHt8fpNjI4F3Savk0tzDK/PELbswgTlWjHotezLhev6kUGl9wKekthcWGYE5vjKEFaMrS7ksfb65ERPjtuhuPb44Sehu9rlEbaqJ6gsK+O2lUJt20mZmtsdbKc2Brn+OZEmvKxqaKeznLT5DJaV6WQcTndHsWc6pK/GGOeSW0zfqjhXFIpzjawtyUjhQ6ri0PsG9viZGccN9KZHd55UWP++aREL5U681LhNSuJIGHpLTrFs4LmAYmzJFBiuPS2DEoIehd2bomoPqThbASE2ZSDL72tSH9XSKnYS8s6TITQ19KcqDb0rvWpVttsb+UpGi65+TSY7nBljX2ZDZZmSjRdiyRR6MwXuam4xGp3D3ovZuqPVqjfPcnyv7yDqX//MOF9NyEiyeq9OYrnY+ydCK2vE+UEq28sUjwfsfzOhIm/VPFu16l7GfYWtgmlQi8yiBKFZ1cmiFoGGAnqoPZJ4UKCe4uGo6fFkPKXElb6RYSREFRixu02C2UFvSMJhiNaoYUiJHv2bJDT/dQYqki0iS5B2+Lw1BonxTi6FuMFOnLGJRGSvZVtKmqPOJtQ1F1GnQ6FWY/FWpkolyCtGDUTIROBImBmZIexTJu6n2FdjYmlYMjqUjb63FM+y6mhcZZ6JRKpEEypRIlKLAV7zA2kAu25hNFrtuh6Ju3NLCSCf7r7ET5s3siSUuEtI89xs70Ah0EXMboS0+jbzJV2uJAdwvV1Xl8+x6jW4ovJYe674SR7Mlu8z7sLx4woZlx6HxuFO+GukQWyqs/7m7eiqpKq02PhoE3YySBiwe6Dq1zaLvGGuXkOZ1cA8BOdX91+I6WJJs3ARkz36Homf2/Xl/ilz76XnUOgXddCbjkcHV/h4dn9FKSgfacHnsmdh89xoVXhn0x9mt/ZvJuL7fLl0IQrxasxduY1K4kgQHXTspWKLwgK4BdB66WOX1EmzebuVgVqkGDVQxJdYrRAeAo910SqksyiDkHqsKSEoK8YtLo2+rrBiNEGkWYMawY2W0GetUZhkDBX4KwqtGKbKANSESSVPJEpsLcl4X03oX/6GMbDz2HVJKonia3UVwRIa/CqAtFX6UypWFrIZLYJgC4SCnoafDdU6qAXfTJFF8sIGc+3MRsxI9luahwGcosu004DGQvUroKtBhQvhESOhFAw6+wwa9fwI41hs4NMBLKn4Xt66oQHaHpEwfLwXR2WbZKVDBktwBLpIK8aHUpmHzfSyWa81JFNpiUydCMiib48uLO6n+Z58XWKusuQ3qUV2wSJNthUuoGJpsSMm6kHmBKkYfjNnp1Gxy7pOJc05t0ReoGBuWgCEEqNi70K/cTAS9I6wp3AIogGKuaAXmtbcL5dpRbmCNsGhhalTnR5KKp9dBHTTwySno7XNQgSNWXWgNEQuKGOrsdsunkakcNWmKcWZrGXdLY3CiRSEG3Z9PomXqITZiWFC5LepoPSV+hHOtmFlEkm/TTf7PnmEEGk0owdtt0sfd94UX4fz9tErnR7pfB1MREhxKIQ4tmBFffYoK0shPiMEGJ+8FkatAshxH8XQpwXQjwjhDj6gut8/+D8eSHE939NtAy8GS9Dpo1CgnhhpOTzjqODCSiVtO2yN6QyOKZ82TtQCJm6xAv5f9ROlQqoJKknbCIRYcxg0QcRSRTLIvG8wfVlSod4AR0DFex521qCuLzCcfkez3vcDgZFgvhb/ZMuD5q/fv3Bsb9x7a8wxoSQaZ98lVUD8YJBeiWlINUBMc/349+G52PLhEj7SYqUKaQ0Pf/75PLv1dRNGSEkQvxNOjSRpOe8cJT/LbcWIqXrhX3y/D2UwctRSOl+vm+UwRgRz58rIVFJGwbXk+r/2c3qC8ZOSvOL99F6NTKRl0KdeYOUsvaC7z8DfE5K+YtCiJ8ZfP9p4K3A3sF2K/AbwK1CiDLwb4CbSF/xk0KIj0kpG3/XTaUC059yCfM6+UuS5fvSR9n7+206e3JIRZB7QNKcg9W7bCYedMmsK4x9+DzOXbOsvt1ELYQkNRWRCPrjCbGTYK9oeH0DYUgu9Kq41TQfyHl1CF2JcTeyuACRwFHhmdYEdj0hshXW7q3gbMSUTvVZvTeHNXdDugz8vkeI33CUIK9h7gi0PX1GPtmge8MEYw8Kmntgym5wqV/GT1TGrDYqCbsKJgqSME6zoHdci4lsi2dep1H1La6vrBEmKl968yzXKxG5Up9OmKUZZmju1knyAZodXU6juForMm9V07o7qiCum6AnnNuu4ndMmhkboaT2Jq2ncKY+Qm9UAwkX+kMsd0tst7NpTFAkQIMkEsQ9k2y1x8W1IdpliyhWiCKVOFI53Rply8rRDU0WahV8V0coEHd07rr1Amt+gRkzS+zExJaGu5pF7yggQXPhS5tz7NSzGJHgLzcOsVws89SlaeazLhkzoF/PMO+my9WKq/DZiYOMWm16U2k+kYZnU3pKo3FzDqFI5GxEJ7H40tZudDXNsqbpMYvrFQrPaXRuCXHHYpYvVrEqLhfqQ2z205KmUawSZyTapkF3wsSoK5iTPp+v70dOu5hPmOgNlbAU88zKBLoKvZbNyIMq/rdGrK2VGRpu84n6YbpBKlm9GEPpq7UW78uhznwT8HuD/d8D3v2C9vfLFI8CRSHEGHA/8BkpZX3AOD4DvOWr3URE0J6zsTb7dCZ17E0Fe12hO5cj1gWZNY/mnEZmKyG7KtFPLyEF+IemaM+oKEZM3NOIMmnZBHNHQW+o+OUExYhJMgmKSN3kNT2maLmU9R5GtY+0YmQmJspA1eqSWfPIP1OjeCHEWfPpT2Qono9xNmKyaxHxG46i/tVT5I6tpIXAWxbReBnnXJ32rIqzJqkFWVp+Otk7kcV2kGPHc7jYLtNsOtRbDkJI1nt57C2BG+jUfIdWaGHVoBlm6PdSIyeA5kq0mk7U0+lEJm5iYFppKL10NYgF0kwQoZIyhVBJVQIhiYoxfjVGV2MMEhQ3vaaUIi3b2bPQXIHwVKSvQpyWt9DNiKzpoyhJGu8zSINgKBG7nDojhQ6ZrI9lB+gFn26cTqSM4qP2VGITyEcE5ZgoK/FLsLe4jarHhIWEcafFnL3NZLXBWL7NmNMGLa3Xi5Ygdcmw2WXCbKJ6MFRNq+R1Z9Ixo1khxo6KLmLKVp8Ru0MSKshEkMn69CYlQgHVU1DzAVLCULaX5nsp7DBb3CG2JNFwGr3sV1PVaSrTIGmYuEMKYSlGBIJCziXKpuU12rNp/xUrqYvAgew6jh4MvIBfRMkICZFUrnh7pfD13kkCnxZCPCmE+JFB24iUcn2wvwGMDPYngOUX/HZl0PaV2v8GhBA/IoQ4JoQ4Fru9dNAJQaJ/OUAsslJxX3VDEh3MVoIUQKWEEkKYU0l0UAeJnhNbXn4SKVIfDl2PQU9zoooENC3GUtOcJLo+CJpSJLEpMZQIteuDpqIECWovIMgqaP2ExBAkuiDIa2gT40SraygBoEjCoonoeyQaWK3UCOlG+uVnfX4Q+KFGEqokoUIYqnihRqJBEH1ZiEx06EVG6pqtShIpCDMDtUdL8CKd7uC4F+upo5yepBNPkyiDP7dEClQ1QVgx0oxJpCAgLe4dJBpRoqQMQqa2KFQJWirKK4MgRkiZjaqmy79xohAkGtrAp0URg7oyWhrk2ItMEqkgNUmckShGDEZCbEtiW5LVgjQgLpOgINFFWjzceP56elq2QmjycpVBgNgAxwgwlIhEG2RKE6CEgn6SMi9NiVGNBN2IUjuXLkliQWymDoZJrGBrIaYSY6shlhqSWAmqFaXvR0/Q1DhVmSJB6Ii073SJpsZEtrx8XUVJyAwcznQRY2spQxcvQrB4tdpEvl515i4p5aoQYpjUw+3MCw9KKaX4WhS/r4BB8ND/hDTHan9EsPb6PFEGvD0eMhFE8yZjj3hs3ZxHicBohyy9U+KXhpn60CKL37cLqUE8iLhUSz7ZhzL0piTJpJd6S2oxCFhqlzBakq6vsdopkDdcujUHYaTHE0NytjlCMpendp2GVEn9QK4Dra+nRlQB5o6gNbsLJdjF8K8/zOyDVc4d3YszOpWG1GuC3Zka5xrDZLWAcbNJI8xQKLqcSCYQVZnmEP10GeudPVo3uMi+wb7sFgDHjuyi7me4c3aBlV4RP9HoHPWwsz6Hqlts9XMEiUo12+PiTpmhsRaJBFWR9H0DTY3Rix5xrBBHKkYmIPB0pBQsBMNYE10utUvoSsJYroOlR2wC2YyPpUfUajnunF3gsZUZTDU1YB4aXSeINU6ujOM4HvZoSNO1L0ciW0ZIWe9xbGuKjUIBbbxPEitUSx3apkWUVYlClXqQQVMTzIrLqcYIoVRwI52lTgkAOxPQ6lpoZkSYCE42xkhKArErTZfZDmysmoI7ElPK9WkqDifcaTZ6OfKGS6nQQ1Njug+MYBpgnrfw720TRQr6Mw5b+SwZLUARkiBWUbIhmYzP0rEJRC7B9Q22gzTLvzsqcQouMi/SZfNhn9wXbdxhQbPlMD1SZ+nMMOerIzR9m635IcL4xf2PvxrVma+LiUgpVwefW0KIPwVuATaFEGNSyvWBurI1OH0VmHrBzycHbavAPX+t/fNflfCOwLy5TnMjR26kS/njRaQC7pvbeK/r01gtc/2eZVbfUMB+ZIjOoYDct3hEK13Gyy2kFCytVdD0OE0+9MRR9GWL0Rs2absWB2fXsNSQlXcIKkrCofIGec3l+255mA/P30AUqpQPtrhr+AIf+a4c4YpDYifUf6hLtO0Q5b78srU9fbotCxTJ7INVeq/bxvvgBP5huHFmibO1Yf7wT+8hPtDlLzevRd1JJZLsJYXutMTeFkQ2/Juf+AN+9nPfir2qodzY4k8+cjdSAc2SbJWzKD9mE+wt89QPFDAXLLILJt6xAPlfYvqhwaWFYe48fI6nPnENfjlV1RINdmY1klUbtxQxM1WjFxjYhS5d3+CsN4Z8Nk/13hoXahVW5ofRmwp6KIjDDB0N5GjE2k/sQv4zWNiqYFshC7+7D6nAbT9wGlsNcWOdvaXty5O/0bP5wKO3c8+R05zoTCEvOETlmI12JZWSBAgtYY+zzcnNMZJE8PaJ5zhorfEX4nrurC5Q0Pr8xpOv59CuNU6tjiLUhDeMnEvVpEUHe3SbvOFy5zc/zWfPHmBroYJxoMuQ1mFXoc5Ca4jaegE1E/Gff+T9fHTnBq7PLfNrn3wLQwdr3Pnek6x6RXKanyaeVrVUdQJ+9b2/ze9u3kkQqyRSoJQDNCPCdQ3khsW9dz7L5y/s5R//5If52PYRMlrAer/Aj77pM1giohY4vPXe5/jlXw+ufL69Sm0iXzMTEUI4gCKl7Az27wP+HfAx4PuBXxx8fnTwk48B/0gI8YekhtXWgNF8Cvj/Pr+KM7jOz361+yc6VP+jSZWA2Mqwcm/avvtfB7QODTPTjdnKzxKMKSgG7H1fyLP3HGD3+87SesMe1t8eoegJ8YVsWly7qxAW4rQy3UTA6Y0stx4+T+PkEGK6zykxwoHiFn9y4iiioyFiaJ1xOHlvh+EP2qheTHNOx/lcjsnzHVbfWMRoppLIyCcbRONlwqLJuaN78T44we7vOo7/9ptZGNpPOCZ483ufYKlXJqv7DJlpwNdiNy1XMV8bQlMkv3T2Pg4eXGGhNoNjhBx+xzOEUuHYJw5x1/gCn/6FA/g1uGfyEsf/6jq2b03YflOOuwrnyWs+K7kS7dDCnU5jTUJFQaoSXZHEmYR8pcfaToF4w0YJBNkDDd6aP8H7i68jq/nkMx5RVUUMS6KFLHE5rcMrQoWLPyUIOwaloQ5BpNG71yUJVNb7eUpmnyDRuFCr4HZSZiq7Gu+97QmaYYY7C/N8wbmWzCUNbyRB21FRAoHWhy+N7qbfttBXDL5Y3UOnbPHQ4izZjE/WDJB9jeeWx5BNA7Wn8Gx7nHG7hRRwemWU4Uqb7gMjJLsjsGP8us1uY4vf6dyBqUUoVoSqxfz0U9+MdjzLw0dmSTTYXCzzaT9dGrb1KK35Eqt0FwoA/PfCGzn35DSlA3Uy5RCA/Mey1A9BnIt5cHE32tkM/168ncJnMohvqVHbyfHn0WEOldfZ7Of4WOc63OjpFzXvrmQl7JXG12MTGQG+JIQ4ATwOfFxK+UlS5vFmIcQ88KbBd4BPAAvAeeB9wD8AkFLWgV8Anhhs/27Q9ndCCqgdzhDbGvVrTJRAoHqCnZsqKKFE70V0JhWGnvHRO6C1XKy6pP36PbTmBobVjk6USVAbOuaOgtZJM36ZTpDq+UicVYFhRpQsl7LRQ7dDRNlHVgb5Tc0e2VM7qF5McSGk8OgK2zcVKJ6PsBsJ9k5C94YJ1JaLfXoDZ00i4HLQnjsiqB4PiaRKJzSx1XRABolGgmCzn8X3DHxPp1nP0vBsVFfgR2kxJQCtD83QTu01dmrIjDJAAqqeBvX5iUYhn+ZrFVaMsCMY8pGZmHK+B3acVhBMFJJcRFSIKQ5qAyd2asvIGgHlfA9VTW0WMhOjZiOkmWCaEdlSn+Fsl6zlk8+6OAUXTaQJkvdmtxgvtilVOhT//+z9d5Qk13nfjX9u5a7OPdOT087mvNiARSRBRAIEcxJJiSJFRZpUeC1LryzJyhIlWZb0kyyLskkziEHMOYAEkfNigc15wk7u6Z7O3ZXv+0cNIP5sklrYOrTgo3tOn9NTU9Nd011167nP830+31yb5EAbX6q4kYqtuKDGXsXqQAdvyMMtRDhFyebsKslsFz8fMZKsMWZW2FBcYyjToDfRQs+55LJtRM4jKAQMWE1GrTWkAqN9VTbnVvFyEiXpk8x3ScxpOFJnINlgIrWGUOP8Rxio+BmJtxZPclaxS6tqU0y22Z5fYUd+hW2FFaQes1yXm2mihMTxNSbsCnLFxM0Lwn4PNEk21cVPxQ1+rTFBxzXoL9ZZa9tsSpRIaD61duIFl3n/r1KsSimngL3fY3sF+J8cdmQMLvk33+e1Pgh88AW9vxWRnQlY3WuSPx/gvLRLJME4ZQOwfCjB6EcvMv1zmwi2dJhL9T4v8Bp6qM3UXkHvSI3yfA58ibe7S0+uxdqxYvwGCqw6KRpbQ8xQYbaaJ6W7qGeTBMUQEcQJuOPlIbw7e3H6JF4hJLN9HMWHuVdGiE4sFBh8UDD36j4iLVbSHhi/zFTvViq/ch1Df/Iol/7sGt6SukzZTaIrIZsTK0RSYcis0ckYnE/1EUQqx86OcVXvAg9dbZI0PQ5lZnGkxuOHJigaLQ4OznE+UaQb6rS2+PQPVxlINknrcQdys5WgpEYM9tVwfI1qJU2q0GF1LYOe8LFNjyBSaDVTaDWVtbbNcpAl1d9izbXxQ5W2azBeqHLR07ASHpYesEaKvf2LPHJuI/Prd8r+TJOOr7PYyNDyDc6s9tNcyCD1CPQIpaqzedsKX2/uYtbrJTXSoLmchpqF1lAJMiG4Kvdd3IKmB0gheWh6IxcKRebP98WVJTNEM0LWlrIII0J4Cg8uTFJM9WNONpi91MdcKs+mL7a48B6dTsvEuqrBcpDlxOIQtuUx0ltDVSIWHx6h/wmf2VcKsmN16rNZlILH5bU8F5f6kBLkekVLlkzaWoi5opLf1OXbi1sZ2FGiuTAAbY38aI3Vy3mEFaFVNDZ8ukL5UMTyXIFkb4dPzBwkl+jG3kUvxItX/l+YE/k/OnyFhZs00tOSxRtVxKUUIoTVqyB1GVILEed/ZZKt/785pn90FKsskQKqWxSWrk+QT69Rns9h5B1kNYl+0qY0YKJNdOjPNpn3VXQlNpayXhuX+Xamlrh0qBc9UJEy7ku5fnCKEyfT1CcN9FMKqYUuM3daDH89VqKKCGqbILkoseohkSY4V+7DH4wjkEt/dg0b/+3jPHXDBi5UimQHHM51Bqj5CRpegnOlPtyOjlAkSlvlwbmNuBcyOKNdnshuQFdCmE5ytGcUKQVNx0TNSURHZWU+TzikUHMT5Mwuk/1lOr5BtZNAV0N6iw1aXZMtQytcXC7SdEyEkOQGG3hFleFsHVVEOF0DrRBhaT4T+ZhLErganhpL8WUE860cw4NVhlJ1Sp00moiw9bgCkbe6vGr4BMeKI9S8RHzhjMGzzTHcUKOoNWk3LdS0z3h/hYZj0eqaBBmVn9j1GPetbmFJi7h57DwHU9M8XthEKzBQheRstY9sv0OplaKdMLlt9BwTVoX/NHcrt+4/RdFo8omfuAYr0aU/26T6tSGcHTqHx2YYtBp8e34LCT3AnXSYHVLRqirNVoKRrSXmz/Vx8NAltiTjtJ4vVT5x+iBDE3UWK1mCbR2cQOON40f5b1+8nWA4on+yTLVps2XLIpdWegmFyZn3Zsn4TW7cfY7Hpif55R3f4oHaVmZX8xjqC5G9v7BJ54c1XrRkM3NyRP7S5w9zsV1kNFHl86f2IYTkNTuOkdJcTjUGeVnPOc51BniyNM7OwjIb7VUqfpK83kFB8unpqzg4MMf25BKfm99Hx9N52fAFNCXCUny+NLOb3X2L9JtNNlolbMXFlxoP1LZQ82wmU2X2J2d4vLmJy+08tuYxmqiy4ORwAh1Li5cmo4kqZS8Vu8rbZT75+Zu47e6nCKTKvtRlnmps4PLhNpd/6zrsJUn/A6vIhEGYNFi8wUaqMTvlfW/9CP/+I28ndc0q5XKabb9fI8rauH/UZPGJIdSuIEhLjKog+7JlFucLaBWdIB/E0skIhscrdDydsWyN4ycmGNq0StMxY5JZxeZd1zxE2U+hEnGmMcDZuQF6e5o4vkbGclk814couPQUWviBihCShOFTe2CADXdMc3p2kB3jS0zduyEWBL7kMgBepDKRXiOpepiKz6n6IN0/H2blx7p4JZvxrcsUEy1OrQxgaCHbe1eIEFyVmWPJy9IOTPak5hk1KjzU2MLu5DxpxeHz5f1sTpaY6fZQdW2uLUzxRHUCJ9QZTVZZ6ma5u+84n13cz/npASYnSvihyubsKo/NT7BncJGk6vHW4mOccEYpqC0+t3KAseQaPXobW3U50RxGFRI/UsnpXdY8m3f0P8yRziSqiPjG8g4ajsnhgTiadEKd23rPcLQ5xmt7nmbRz9OMLC52+rgqdZnTnSF22IvoIuQPXv8sMyebVxRepLYMyl1/9Y4rvkaeePn7/tUB7wcPydHqKKvtJKVOGhkoSCE50xig4VpU27Fwa6raQ0+yw6qb4tTaAAPJJk6osTu3iOtrbEiUeby2gY6nk7e7JFQfN9KwFQ95b4Gpuzz0XAT0MWauUQ5SnFwdpOsabE6VONYe4+jqCKoSERgKR7ujZA2HNcdmJFUjQjDbKVB3E3QDnfPVPsJtLS63CzR9k7Kb5EKliPNbBcZ+51Fmfv9a2m/tI7koqe6MyJ+UhJbAaMAHF28gfy6i/7Ya0Rd6ufCufqQuuSY5j/GVLAu/ElKwXDr39rEhs8aiUsDeWiP3gTSRJpi/O6T+nQF6bl2kaLVQCy66GtJYTCMiQXJOxblaZ8xcoxlanDk/HPfG9IU49xZZGosg78GqiZ/poqmxTmJxsUDiQIPzS33Q1Dl1fgSxyUEzQmYeG0UJBPaBMo9VJ+jNrDvjdRI0r1cpfDFJ6bAkbbhsSFYopdOYasBksowbaSgiou4n2GyX+Pjlg4xnqozba3ynug1TCekEOhfafZSdJPO1HFPVAp22xe1bzpDTOlxsFHm4tokLc/0QKJRbSZqzWRYKOVQ9ZMBqkFJdPrD8Ekw1oOmbTKQqnKoN0vV1ru+fYs1NYqk+XqSxI7VERuvyD+XDDFr12IS9Glt+KkjG7TXKXoon6hvYmFzlk6XDeJHKhmSFAbPBE41JVp0UmxMljrdHceXJF3DG/8tczvzLi42ucCiKJGd2Gc3UyZldcj0t8j0tMobD1lyJjb0Vesw2B/rnyRpdesw2+4vzpHSXnNll0Kiza2CJC50+NiQrbOsp0Z9oMtXuJaW6LHlZ6vs8tuZKjCSqDBp1smqbRSdHX6pFwvRY85OMWmtsyZcYTDYYTdbYki3hhBqbs6skVJ+05mKpPv12g4lMhf3FefyuTkp3mUxX6DXbHByYw16SzPz+tUz8xmMUnw2RArJnVarXedR3BNT2+NzUc57F22KRVe0mh8wU9D0Vn1hTr0nSXklSmuqh/4kOZSdJrqdFq2Vx+bUR83dFpE8ZuPtbFKw2Sc1FP2PTY7URUqAXu3R2OowYsSvdmdYAmf4W1rJGMdGmc7CDOdKi8JCJPtKmP92kkOiQNR0GBqu4cylSSQd7TqU4XMO8ZKEfT5Lcs4a2t0b9fAFFkSR0n6LVotlK0HNcsnKrT88zCv1Wk4VujtmpPs5fGGKq3cuZxgD3rW4lq3c53RpkPFOl12hz3+JmikaLDYkyWcMhp3fpSzQpplts6VkldFUUJN9Z2sLUhQFMJWRwoEpiTmP/wDxq0SGb7aCdSpHXOqRVh13puIVgf3YOXYSYasAtg+fQRcj5pT6OzY1wemGAWafAgyubuC57kQUnx4hRwV9IcnBkjpKb4kRtiILeZjhR40Krj72ZebK6w057gQdLm9ibnuPswgAFrcV2ewlTXDkeERnnRa708cMaL9pIRErBuL3GkpOhYHQ4V+pDCMlookpKdXEjjV3JRS67MULwuax9WU2RUl1sxWW1m6I3t8KgUeNMY4C2b7Azt4QuQvJah0yhjaaEmCKgX6+TVFz2pOaJpMBQQ7J6l1F9jRmjN66mSEHRaNM0LXypoItY3TloNWgGFgBDZg21oj9fxt2cWOFcZ4D+B1Zpv7WPzmsPY3/+CawbryLSFdojRtwMqEsmzRJaRaPq2iAk/Y+sQRTRCkyUEPRqrMat7E6Qj1TaHRNZN8AKQZW0JkKGci3qXoKm0Y0rIKGOPdhCVaIYTQhktC5JzaWVMznVl6Yb6KSTDtmEw+LGNAoQSgU30FCVOPdh1BVGsnVOTGbYlGpwThRj8V2qhZSC0qaIrb0leo02Ga3LSjGNcllg2BGNjQZ138JQQoy8g6pGmGpACpdNyVVUEdENdbbaK4wYFdxIY3dynpzaYd7NM2qtseDmcBI6B7KXqY8nqPkJtuRLaErEvvRlVpw0iyNxpJFJd9jZu8wjW2wqfpKU6vKS9DlUETGoV5nu9rIpvUpKdbBEwK7h2K40kgoJ1Y8rO8Yq84kCvtTIbqyy0k2zORMb69Z8m8PZKcpuiu3WAqYSL2sn0xVyaoeDE7OoRGTV9guTvfNDt4y4ovEinkTiO3AnMMhoLp6jxSpSKWgEFg3fwpcqNd+OvUIQrAVJIE6QlYM0lbZNrrdDJzSpOQlCKegzmrRCkz6jQWMtSbvPZNnLoCsBWbVLVm1TcZM0PZOs1mXOL9AOTJxARxMRrdDEVAPagYG53s6vElPYA6lQ9e3n/4d44lGo+QlkwiC5KGn3K1g3XoXy0DM033YN9pIgtGJV67KfJbkgUA5J5IpFZ1wnSCj0iRqJZUFrTBJmAqSik9B8/I4OQmIs6SDA6wlZWcuQy3RoWiZRIqLhWrhOLG5TVg3KQZq8HvN5H2lNggINz6S2msLNaTFHpG5RsxOEkYhpYaFKYElqTryEXGxliAxJpMNcNXady9gOLd9EW58QIinojFl4lQjVhLqbYGOmHNtlGj5J1SOpeqRVhzmnQI/eZsHNoYiInN5hNUjTjkxqXoKkmqLspqi6NlPdXpquSY/Vps9ox34vQKmdAgFVd93fp5tG1SKagRV/B6GNqfjUwyRupBL5Fgvk6dVbMR1NCfEilbzWIbBUloMcBa1NPYy/z0rbZn/BwY9Uql6CJS9Hr9liLUwRSYUVP0tad1gLUhhKQGV9eyBfSAPe/306kf+zI1RY85KUuylqfoKoqRM1dZacLN3IoOmZNEOLZmAyu9LDhUZcup3tFGiFJp3QIAwVnqhM0IniisVKJcuR6ji1wOZIfYKJ0VXKTjzxpBUHS3jMez1Ymo+hhDxTG2VAq1NxbeYaWS7Weim5aS7Uilxu5DlRGeREZZCpTi8zrQLz7RwhCqlZhZlWD/OdHPUwrsKESYPqzijmkugK9bddQ/Zjj+P0rtsw2pKC1qI1Jrm4WEQfbmNfqJA5vsqppUHqOwOyFyB7UqczIDm7MIDoqCRnNZStLbRtDZCwfXiZVtdkuZ1Bq8dIxHSqi6aHREWPepDgSHWco7VRRjNVMgNNml2LyYkSo/kaakdBSQR0XJ1216TrGiQMn+wFyJgOSjJgLFPFqAnsRcHugSX2D82TNl2G7RpJzSWnx5BkxZeMbyoR9bmstpMcXR2h0Uiwupbh2cowT5dH6EQGvoy/p93JefZZl6n5Nv1anVG9ghNq2KpHhEAVEQfSs+wsLHO+UmTZyaApEfXAJms6ICS1rsWewUW2ZEoYZsBiO8tiN0NO7bDkxZPU+WofipBktS4KkqzuYKoBlhrwWGUDJ6uD9KgtTraHWPEzDKSbuL7Gs2sjnKv1sdzOUNDaPFbagK24z7NPFrtZFBHRbzZxo3jiFt+LTfB9x5X3zVxJ7kQI8UEhREkIcfK7tn1PlMcPGi/eSSQCXyrUu1bM26yr6DWVCMFMq0CpkaITGZxe7QchcQONE7WhuAci0pjq9OJdyrAzt8TFdpH2YpqwrbE1s0LNSzBo1ZmZ72WmXOB8o4/TnSGm3T769TrL7Qxt36DfanK8M8pcI4/j6TS7JnPNHH12k0ot9byQSEHS8gwqbZtja8O0xuLtq50kndDgXKmPxRts8icV6le7zN1q4BQUZn/nOsZ/61GKx336j4R8q7qLyc91MKwA7ek0M28e4MK7+rEtj8wZjbV9Ia1rungjHgM9dcyKSmcsRD2SJjqRZWhDmZPTw7htAyfQSG2r0u6YaGqEU7NInLU41+xnV3aR3dlFZut5BtJNOi2TqYsDzJQL9O1ZQSxZdBpWjBbs6qysZKkcCqk5CYaLNc6V+2jvdOle0+LJixM8MTPBDb2XAGj6FnPdPMVEi5XDgstnYx/dZivBplyZ3WOLXD0xw/b8Cgd657k2eZFVJ8W+/Dwn2iPM+L3clDvLZa+X490x6l6CRSfHSifNajvJ081x0rqDqkhuLpzlpb3n4+R2JU9+sMHaXI6pWjyBHx6ape5aVF2bY90xbMXjSGMCgG6o8/X5HdTDBDtTi+xPX2Z/5jKb0mVuHTjLt+o7GbWqXGr1sju3iH8hw1iqyr6eeRqOybnOAHsKizzd3oAvVZ6qjnNj/iKP1yY5lJrislvgaHOMTmS8sNM+Elf8uILxIf7njvnnUB6bgXvXf/6B40W7nFE9OFMZoLaU4YwUZC4BAs5sj5uGOytJzvQO0OmYRBWTRbIU802WGhnSlkvO6hKasW2BwrrtIwplN0XFSVIwOtBVkbbADTXcKP6o6qHNQjlHFAn67CYFo81aNYlsGEgrxPdV6p0EQd2gIlJIKfBDhVotSeSriGLcC3Oh3IvrxEIyt6OTUCG0YhYoCoQWqM4/KlvVTRtIqB5ewcCpQdqPy76KD41mAr0A5qqG5ytgRqysZchPS6RQiTSQimSplEO6CsJRadgWbs2CULDqaFjzOpoTLwdrvh0v/8pptm5Z5WIlNkL3ywmWPZXUvKATGkgVpC7RHYHiwVIyi6JKgraOXtEIEjoyHRAS80iWOlnqToxq9AKVyJJIM0JXQ4K2zkI7S6VtkzQ98lYXRUiW01mW2hksNSBCMGDWiaRCyUtjKgFLtQxp3WW5lsFpmqxkMhhqQKNtMe8VWPXiZYNXsnENCwSMZarM1gusGUlcX0NTIkKpMO/kSag+za7JRXrJWg7NwKIdmCgi7jqOl8kKk4kyJS+NpQbMdgqEpmSmVSBrdOl2TNY8Gy9S2ZhyOVEfIqW7zHt5+swms14vkRQU9PYLWp7ECdN/Vtr7/+TFS4zsuGn9+YeJ+9h+oKH3izYSkQKidXpZJAWBHbe/wz+WwZ7zrJVmhKpGBGGsawiloOFaKH5MoHouWSXVmFjV8g0UEdtwAjQck4qbpOInCREEjkbkqGhKRMmNneOlFnNIhIjb4llvMdeNAFWRCFUi1IgoUggScXVJUSKCSEUoktBcJ5wpMm5JX1/CRJpA3bSB8OI0bqShNwKURIC1JgktSWSCYQYIGXvTSjOK8yCmH9tTmBK9BVon/h+VTtyXEgSxmlZxFAgUVBesisQN48lSQUI3Xq8rrkBrKSgdBaGA5khUL57AFFegtdfzNt34b0VHjQlgmkQ0NWRbI4hUur5O2zFodk2CSInVq/76RREKtHWXOSnj55qIcCKdtmugCEnVsSl5GZxIp+SmKXspPDcGOHuOhuiqrDk2XqjFHr7rOadAKqjt57BpMQio7RhEUhDK2Nu4E8U/u6GGENByTLq+zppn0w113EjDDbVY3AfPRxBepMbnmQRdCZ/vUq44STQlev4YOuvMXDfS8CONZmDhRjrhC+R+/BBQAN8P5fF9x4s2EkkVOgShSqY/rnJc92NH8SOVI8ujdM7n0MfanL08gLZgcvVLz3J8ZYjwqz3Il1dZayaxLRdtrM2aa1N7/xjaazts7l/laGmY8WyVb01vY+NV88w+PIZxdZuFVhZb8/jy0zezfcsCkRQ8OzdCT65F1NIZmijT9XRqcznGtlVRhYxNlYBm1yKR8PA1Fe4p8Fs//wn+5NzttN0Ex86OobRV3vfWj/DBxRv4sZ7zTJollv0sBa3Ft27fRUL1cKMEM1d3GXn8IpNKwLHCMH+06R50EfDJ0mHWemx+cezbzHi9XHL6WPOS9L+3wR57jk+tHCRrOIwl1vjK5Z3cMXKWTmRgKx6t0Iw9ZndrLHUzLNSzqCKi5iS4/eAJhsw6vbe2ON/oY1t2hWGzSmeXyaKbJad3sRWPZ2qj/Ozw/Xxk5TqGEzXmxvO8svcYvlT5o2MvJ5N02JyKS98j6RpOoFO0WuzZPM9/vXAdLy1eYNCuE0QqhwqzLLlZ3FDDCTWeamxgV3EZJ4wrQXPdPAqSS9Ve2o7BSzde4NTaAHvH53HWE7YTqQoZo0vNt0moHl955AC7rplma3qFzzx9EEMN2DWwxL7MPLNOgSBSeeg3r2XuVoVN/9Dl8H+6RMVNsvqfN7D60wGjhTlMJcCNNJ5YnWAg2eDCb+xk5m0RqWyXu8dP8ezACCutFJsLZW7efJ5H5jZgFgPO/sw2pv6tSuCpFLe2OPrRPejveJqVbpqFL03gBN95Qef9Cyzd/i978cbvdWUojxevYnVsVN72sddwqdzDSK7O1NMxZWDzoVkarkWpmubGDZd4ajneXrBjU6iC1UFTQoJI5fixCX7kxse43C3wyOlNaHbAm3c8zcn6EAfzs3zkKy8jsaNGNuEwlq7SbzbYnFjhL0/dTBQJ7th4hm5ocGR5lCBSsPQATQ0p2m2m1woMZRpECLJGl6V2BsfXsPSAxaletm+fp+okuKp3gQfnNhI9kyV/Lop1IBWN5EJcbZn8XAevYKA3Akb+9CKL1zSpf20Tyod7Ke8VRKZkfO8i1c8OUz3kY6VdnNUEW7ctMPvgOM6ox+C3NLykwHlFA+98Br/oUxyo0+xYuB0dRZOIeYvCKRj+yYsczF3Glyofevo6xkfKzM71kpg2cPpDUqMNwifzdMaC5+FMal0jzAdoVoBp+bRLSdAiNDuA+QSRJrn5+hNcbuVp+fEd39ICZs8OYFQV0vsr1FsWL5u8wFSzl7TuMJRooCkhh1LT/NfZG7muOMWztRF2ZpcYM9c42hxDQfLs6hCD6SZztRytlsVwsUbB6lD3LO4aPMmym0URks88cQg95xK4GrlCiyCKS9Id38DWPfbl5uOqnZeKmSyBhheq7Cwsk9YdTCXmpJhKDKdqBBY9epsLnT66oc6Tz2zm5YePoYmQhxcnGcvW2Jwu4UYagVQpOSkO5WY51Rpkd3qB6W6RUAo++2NfZ+3M6hWFDYlNQ3LiT3/miq+Rs6/77RfsxSuEOAfc9F0oj/ullFt/0Gu8aCMRxYUT50dJXNY5P5EgtRpT3s+eHEUmQsxFnceNcZy5NFv/eoWlO4Zwbmqy9OwA6oYWtuUhNcmnv3k91tY69iWD1ILOxxrXkupvcez8GD9193f44DdvJr17hYLRJqM5fGllL/3ZJkGk8LX7D/AzL/8WD5zeQ+YSWGsRpQMKdUeQvRThVpMICcdfopEoxeyO+lVdEgsaU+Vx1K7goatN3AsZ8tes0n9bjWElouraKIckzcUic/9PhFMDJQGTSkD9a5vI3nUR84EWjUoPqhJhvUty4FPHeeDePYhLBq94zdOc/rVdyBshMWNw9b97kqzW5eHVjdxy1xH+20M3Ub5UQOvrIl2Vg5su8aw+jL2/wamlQZ45vQGMiFt3nuGn+u7nR869hx99071c6PRxqd7L6lVNxJINqkBaIcZ4C286zfar5+kx24SDgkcubST0VH78jgcoaG1sxWVDIsuFTh9+pNLyTTZsX+L2/jNsMEv85qffyiPPXoVxXYV5N8ep85uQAoZfVSWIFP7hvuv4k7s/zm5zib8pv5TdqVh/cWR5lIzuoKkhqZTDL09+E0v4/Pof/iTPvKvB1blpPj5ziIGJCqqQrB7p529f+vc83N5KM7T48P0vQSZCdh5c4ttzW+lNtSl9aZQNr7tE0zfxpcIOO9aJ+FLlz7/+CtSRDq/ZcpwPnzmMqkb89p4vc6S7lfu/tB9no4u6bDB26xRf+uo1HLrlDKfL/bEFaRAvy27sP89Dlc3PL7evdEjED6PE+/1QHt93vHgjkYkRuekv30W7aWFYPu5arFFID8TNXJGjURyqUammGC7W8EKV1UqankKLjmuQTjgsXy7whkNHeKo8zuXlAj2FFof7Z3m6PMLLBi7w7b+8nvYrGwgBtukxkq4xlGjwjQevAuDG609xZq2fcjWNaXnkkl2qLZtCqkPX1+hPxUutumvR9XS8QKPbMUjYHgnDxw1UkqZHuZom9BUKD5vUbopLkXLFQh9uoz2dRvHBWpMob1pF+XAvve+ewX3pMtOf3IOiSHYNLnHma1uwry+T0H3mpotctWOaY09vJLWhTt+fW3gZnfk3+/R91WTl5R69vU2q9SSBo6HUNOxlhb4jLuZvLrE/P0cjsLj3c4cYumWOi9P9jH1BYW2bRnC4ifZkmvZYiDQiUCWpswbGS8s0T/Xg93ukT5p09nexEh65j6cILIH19mXmSnmiTnzf0tMehhmgPpAlcUeJejvBdaPTPLs6REIP2J5fAaBgtPnSxd3cMXmGLz59FfmBBocGLvPQ5Y0oisTpGqhaiNcy0Mo6oR2RHGkymqsxbNe53MoTIVj+5iitTT5qyieddGheyJHcVGcw08BSA2puAkvzcQKdjOlw4tIIesJnoNBAVSIMJSSQCvsLc7RDk5lWgdFklW6oc6YyQHk1zeEt0xhKwNm1fmoNm7u2nOLZyghOoJGzukymK1xq9JK3OgSRghPqPPiTn6J1fvmKZgZr07Ac/5Mrj0TOv/63fmAksu7FexPQC6wQA9O/AHwKGANmgTf9U2iOF+8ksmFEvvtz13OpVWTErvG1U7sAuGvnSTJal9ONQW7qPcep1jBPLI2xrbfEnswCc06eAbOBrXh8cuoAGwtl9mXn+cr8LlqOyaGhywyadRpBguNrQ/TZTbakSmxLLJJUPOb8Aqdaw1Rcm5Tu8rreo3y5so+ZVgFdCRlPrbHQyeGvJ3EhViqW3VhvsiVV4nOfuZFr14FChzKzPFHfwNqPF7jwrn4yU9D/yBqd8Qz2hQozbx5AKnHS9I/e8DF+7TNvw9jWwPNUNvzIcbSJMZb/yqJ+voBVil33ALQDVZrlJHpJJxp3YlFYS2f75gVW2ykG0w1OPjvByLYVnEDD9TVaTYt37n2MS50iCpJ2aHBkdozeXAtDDUkZLueOjaEOdChkOniBiqpIUqZL5RvDbHvtOY7OjnH1xAzP3LMdEcCuO8+hCEnFSbI1U8JUfFKay9PVMbq/N0T1F1pUF7JMbFrBVANmygVUNWJjb4UgUrih5xL1MMF0u4e7e48xpq/xueoBbsicJ6d0+ET5GrYll5nq9lJxk9zZe5KH65uZaRaYTFeouElu6T3LveVtHLs0yvDQGgndZ2duiXsvb2Fn3zJJ1ePNvU9wpDNJr97kC8v72J5ZJqM5ZLUOZ9qDRFIhkiLGLAQm7x6+j/ua28mqXR4sb2KlmWZrb4mWH/NbX9v/DF8u7eUdQ48w5/XgSI2zrUH2peeYcXrYai8TScGfvuEI0ydbVzaJbByWY3/8s1d8jVx443/41wa8f2p0IyM2hw51UGIfDzfS8GWsF+mEcWlOW/f7CKVCEKl0QiOWNBs+0To411BDDC14vjoR7y+ehws/N9xIx41UOoGBvd6l+1x157n38cPYme658T/6rUqF54FCjowz/lHWRuoSswFEsQUFgJ+SKH7sPayLgMiMQceKoqBNjBHMXEZXNxGZ6x47QQysBmKQMnG1SCgSpROj/ISQKEjUbuxF4/hazHBd93l57nidQIf1/SMp8CMVrfU/y6OkFIgIOoFB2FXxIhXVias3EFcU6o5FwzZJqAqaEtHyTBRVoKkRwv/HyozvagRq7PTnhhqt0Iyrb5FCJzJpS4NmYMWGUUpsDuXL2E0viFQ6kUnTNzGUkGZgUnMTdCIjBis7CgndRxUR7cCk2zUwlABTjftXOpGBIQI0JaIbGQyocTm56tnPVzwMNaQb6OgiTrRauk/TM2Pv4VDFD1USmk/IPxLXm6FFVuvQDg1sxaXm2xgi7qx+oVCif4mK1RdtJJIYHJV9v/PzJOY1uqM+iXkdInB7I6JEhLmiEW1pEy0k2PSxJuUDGeo3d1GmEoSTXRIJj9ZSisSihrejg3nCxl6WVHdJjPEW7uUU/+b2e3j/5+8gf2CVvb0L9Oht7lvejKUF+KFK5bEBfuz19/LhL95MehaSKyGlAxqKC5nZiPRMTAabuy2JVY6Rjs19LtqigdaN8X/OoRZMJxk+uMhwsk4kBa3ARBMhp5YGsS2PRjOBYQZcNTTPSjeN9S5J4u+7TNcK6GpE9q6LbD2i89UHD2DUFe541ZOc+sXdzLzSQgTwhjsfIa+3uWdlB7f3n+ZvHr4F4Qm0vi5+2+D67Rd5dnmYkVyNi8tFWLKIDMld1zzLO3sf4g3feA/vvfHbXOz2cbo6QMMxqS5k44k7EZJMO7Tn0+zZM0PRipdwD0xvJAxU3rnnMXq1JoqQlPwM59t9RFKh4VsEkcKtxbOMG2V+7bNvw14UiFvXaHcNtJMpAN74xgf49tJWyk/183tv/jg7jWX+YuVWNiTKWIrPhy8eZnPPKtO1AlGk8Ds7vkRScfm13/1pBn5imusLl/johaspJGMpf+nxQT729r/goc4W6oEd50SskNuvOsn905vJpzs4X+9j7A1TVLo2w6k6Ly2cJ5IKvlT5u8++HG/C4bbtZ7jn5E40K+D3DnyR3/jSj5BcFDS2+Wg1jZtf9iyPfPYqJu6cZmatgKJEFNZJcb+98Uv80cxddAOdk+/9ELWzpSuOREb+6Oeu+Bq59Obf/KFEIi/aScTePCjf/slbWezEFPajS6MIIblxZIqym6TiJNmdX+REdYj+RJOc0WWq2cO27ArtwGR7com/nzrEuzY9ygNrW6g4SbJGlxsKsbJSFyH/5ZOvYOLmGXbnFkmrDn16g5Kf4cnqBE3P5Jb+cziRzsn6EIYaYKk+QaTSa7aY7+QYS1aJpMBUAmq+TTswWHNtSq0UNwxNUfMTFI0WR9dGWXxiiImvtJl6TRIlhMSyoL4zIHNGwy3EDn3jL5ml9OkxDrzjOI9+YW88YZqSu685yrmDPo2vbyRrOszfM84db3ycb8xsp5hu0/qHQRDQur0FZ1Psu+UcW1MrfOXyTrb3lHh8ZiKOtmZsfvaV3ySrdmhGFn976kbkhRR7X3qeE9/aijvpYNo+bkdn29gyhhKiiIgLlSJJ00NVIpaW8xi2h6ZFmLpP7UIBJRAM7l3GC1UmsjFTZMVJc+GhCXpPSFb3C176suNsTy7xZH0CTUQczM7gSzVutfdyjBhrfGllL4OJBluSK5T9VKzzQEEXIY3AYqbZQ9s3KNVSvGX706RUh0cqm9iYWuU781tod0yK+Sb1+wZQr4290d48eRRL8TlSH2fQqnOpVeTq/AzT3V5OV/t5zcgxjjVGSag+gVQ4mJnBjXQerU5yKDeLqfh88MK1COCNk8+gi5C1IIkbaWy0VnmktpEBq0FW65LX2jxa3YgXaryu/2nuq23n3nd+lvlT9SuaRMyNw3LkD999xdfI1I/8xr9OIj9o2H2jcvuH3k6lnCZXaOE/1INUQLm2ymC6yaXlIgcnZqk4SWafHCEa73LLpnM8Mj/JYLYBwGypgFAkb9z6DB8/dghRNShuKdPxdEaydQpmh6VOBjfQ2NOzSELxSKg+X5zeje+r5NMdbh86y5dnd1FdyCLsgL5ig5XF2FxbhvG5kc536LQtFCXi+g1TrPx4P1O/FzNRDw7OMd3oofTAEMqBOu2V5PPduNkLsLYvxFzVCC3JX73+g/zcgz+GvqyT3VOh+1AvQkJnIKJv+yqZOy8R3XgVs++OEBdt7BXB4L2r+H/toCshZ4+Pcdu1x3jg61fh5UPsBRW3IBFjHYJVC3I+W0ZWWGmmsQyfMFJ42dAFvvDVa9l703nOlfvonMvFYXgAqiuQqsQdDNjxm5eZ+c+9OF2DQq6N8eE8CMGuXz6GqQTMtHtIaS7zrRwAq40U7rLNXdc8y7KT5tknNhGmIpSOgjQl0ow7j+/cdYqH5ifpdg1+YtdjbLWW+OjStezNzZNVu/z10ZvYNb7I2aU+wkDlXXseZdnL8LUHDjCws0Qh0WHMrvLNC9uJli1ym9f4hc3f4Z61XZxaHaC2lkRP+Pz1wU/wD+XDXJe9yB98+9X0baxw98hJjjeGY0Hi+pLq4lovnqfxgYMf5i8Xb0MTERGCkyuDhKGC2zagqXHb4ePce3Er//HQZ/hU6RA5o8vR1RHeMfEY5SDN2dYA1+Yu8buvO8bq6cqVTSKTw3LkD78nYfR7jqm3/Pq/5kR+4BCwVk1CU6MmkiS0eFuzEicww5bGpWovQaigdQROxWRuME971WZRQi7ZRSgSfyXB1EgvMlQQElaWc1hpl5lKgeHROuVWkpTl4oYaCcXj2doIQaDiexorKzn8AZVG00ZrqIShYM1IYqzoeD0haivOMzT9FMJXCFXJfDuHt7mAWwY3EXI+EWMJg7SMgUJhikiHMBMQmjpq1sfzFaQZMeP1YqVdxCWDhO7TNuKL2agrZE2HcL37l3fvxV4ReBlwhjP0mmtYqo/qiBhPYMZOXXoTnGIMjpaGxErE1YlaKY3QI4YG4vKq1CCtuaQsF68rcAZ99Kr2vLIWLcKfHAACZKBgagG+pWDVQlQRG06tdW0iS+AEGroaxl3XMjaQcsK4y9gsqYggNt0KbAURwuUNeaJIIJct/J0qjtSZa2SZSFVi824J3UAnihRkGAOKA6miuILVeoogUmh6Jooaq2NdX8eXMbah666bhUnBBW+A09V++s0GiivoeDplPxVDrsPYuCuMFJqrKYSjMOMXOVvuYyK//hlJcBeTKEHsR9QNdVi0OOcMMlXvYUu+RKWWYt4rsOYn8SKVBTf/gnMc/xLv+S/aSCS5eVDe8aHXEERxIq/cSaIIyUQ2rkY1PIuC2UEREaYSoisha55NQvWpuElu7j3Lk7UNTNgVOpFB07eeJ60PmTGR7KNffhk33fYsA2YDXYQM6jUeb2zkXK2PtbbNrePnSKkuM50eIinos5p0Q51VJ8VQokFCjYnwtfX2/0gK3Ejj6OIIh0dmMZSAbqijCslTn9+NEkD/Ex0quxNIRdAZkHgjHrixveUbDh3hM48f4hWHjvHVp/eSWNBQXbjlzU/y7U9fjbc3prlveMsx+h/LcHatHz9QaZ7PowSQKAla4xHXXn2WCbvCp75xAwdvPMuRB7cRjXcJmzq/8ZIvc8npoxEk+PrZHaSOJtj+hrM89dQWoqxP9hmT9jUdtg6txJ24SGZreeo1GyPhk7w3RfNlbeTlJFJA2Bsfv7mi4Q7F8Oi+ZIuTs0OknrVweyR9T0cc+PWnUYliSw414lW7jlP3E8y3c+zMLaES8cjKJGnTpe5YbMmvktGd+GIF3FBjtpmn6+m0ThZ4yyse5N6lrSyd6ePaw2c5URokfDzP9a97hnue3o2wA1LHLd7xzm/EuMb2MGuezbi9RkFrc8/KdnbmlkioPp86ejAW1kWCtx98jKerY9zYc5E5p8BLsmf57Q+/jWtfeRxFSJqBydbUCotOjoy2nhPr5rmr9wSfXDzEy4rnef+DN/OHt36KOb/Af37To1w+2bjiSGT49688Epl+2w8nEnnRTiKJgVE58gu/hFkROMW40iAkIMEpRlglhe5ogPAFY1+L6PRprN3ukDySoLkpROvp4ncMMscNvOubROdTGDVBkAR3g4NSMnnHHffx3++9ieSGOtt6SwxYDe6b2xxXEXwV7dkUd73xMb78lWvIXoyrLo0Ngtz5CKegkLvkg4DaRh2tK/FtQXO/gzkV++cGNrS2+IiOyuDWEhsya5SdJH4UZ/jPLgww0FNnZS2DYfpcMzzLQieL94cD5P7DZS6u9QLQ/2cWe/7iGF/52mHsFcHut51k5doGU++7FqMhePUbHyaluvz9+UO8cfMz/P19N6I4gtCOQArGti8zv5pntG+N+dU82jmbyJDsfel53jN0Lz/xuZ/lHbffz+nmIEcXRtD1kO6lDAgIExFazkPMJCjsWyVrOiQ0n2NnxlE6Cm942eMUtDZLXpY1z+Zys4AQknIryUCmye39p8mqXf7sM6+m94Rk8ZYQpaOSPy0QAWz8yXMcXxpCfzjDj//kN9hlzfH7l+5mV2GJtObw6RP7GemvsljJIiPBv9t/D7oIef8fv5buq+tcNTDPwxc2kcp0cRydYDXBJ+/+a/5h7TBLTpYnntxKlIi4ad8Z7j+9lYnRVapfHka5tUIx2caLVLbnVgilwIs0nvzibtyC5O6bn+JLjx/A6m/z5s1H+dCT19P3kEZtM6DA9hummPvYJKNvm+LYpVGK/XVqDZstgyXu7jvOJ+cPYWk+T/zMJ6ifW7nCSWREDv3ulU8iMz/27/91OfODht6K8AZ8VMfA7wmY+JCHVBUuvk1HJALkmkVusEGtnKK8x8Dpi0gmHRpbdfSsi6ZFBC2VxO0ldmYrHLmwDTcvmbzmMhcW+th/7Xn+4dJ+1MEO3Y5J1bUxlJBXbzjBJ+6/HqlIel+6zFw3j5eVtIYVvKwkyAWU8gK9KWluVEFClPHQyjpCShIpl9SUyerhCCLoH66yMp9ncb7AolIg19Oi3THxOzqio1K5MBB346oJ+t/b4NFv7EHeCLNPbyRKB6BKuq80WJzZ/vwS5uxaP9X37WTy/32MtZ+4lk8eP4iqRwSuyhem95A7LWhOgAgEmSmFWbtI7lmdpb5h7L1VBl+6iqGGPHVyI6cLZ8mfEnx09Gr8ZRtyHtkvp0gFcXNgkFAJLRspoHSph1p/B2/Fxl5SCWzJ12Z2YGgB+4qLLLRzzJXyyFCg6hEV1eari7u5rjiF0RQsv9JBhLGZdms49vY9kJ3liVMbSd5a5sHKZpo5i30988x3ctTUBMqKST1jwaKF5gi+MrKHsWSV6na4dmAeUwnZNb7Ihfsm8ftiA6uj3QnuW9hMT7ITe+okfTbbJSobk0wkK3x1YhBZTTKSrUMQYwEU4sbN4ssW0dWQjdYq1111Lma7OnkIBfW7W9imT+dEntVukugVVXZklljoy9Jrt5nMVRi2aoQomGrAzuwSj70QUtk/cxfvP9d40XbxhpZCIuvQHQ6w8g6lA0lK+xPoWZf+vjrOmEd/usnAUJXQhDAbsL24gpr26cm10LQQaYeUVmO8op8LEREsNjJIKVhuZ9jbv0g25WCYPul1SlkrNBF9DqLg0XRMCkYHY7CN2xsR5EJS/S1QwOsLiIyIyIzQEgFBIcDv99haLNFzpAxpH7XXZSDZpHeojlbRyRTaNJoJgjULXAV7XsUthjQmBfUtsMeewxn1UAJIbaijl3T0JQMRQDHdZvDeVfqfdPEDFaMhWPuJayl88DE2jqyydWgFXJWrBy8TJASqKzDXFLw06CmP5oYIdziusMyUC8xW8+SH6nQiE7cg2Dm4RHKsAU2dtZ3QGlVwCgI/JWhsDhl4ooXW6yAEpEYbJBckmSnY27/I4YHLtEODHqtNNtMhm4vLrbW1FHsKC9QCm0gDsWKilHWCioW3bNNdtbnY6UOYIeWVDLuyi2yyVphp9zCUaDBhV4gSEWnLJcyGeH0B+3LzpDUHvSU4X+2j7ltkdAd3wkWEAivrUtBiFmvH11G6ClFHY5O1jKX6TFgV9JYgmXGYTJUJpEI31GmHBp3AYHEtw/RyL+PGauzFE8XaGzXtE/gataUMIhQMJuMb2JbEMv2pJoYScq7cR17v4EY6gVTIaA7qC8QjIl/A44c0XrzLmU1D8rb//jpWu0lSusd0qQeEZM/wIqVOmqZjsrO4zEyjwGS2/LxHyebcKmtuksFEg0fnJ/jVnd/kgdo2Tq0NMJapcnPhLE83J7gpd5a//bU3kHzPPJYaYGk+4/YaI0aVvzh6M1Gg8N5D9/FodZKKkySpexStFhU3SdFsUfctNiQr67jGBM11hGK5m0QCG7NlOoFBWneZbvQwO1Nk7MuCy6+NQIKxpKNsbcVAoXXTq42vv8Dy32zk6n93hFP/z26mfzoWkr1h2zPc89fXk3/rPHmzw9HHtvCmWx/hk8cPsnFkFeWWObSRYS7+WQ+9n0kQvrPC9sIKx0pDaGpEeaqA1lTInYOX/eJjHEhOUwuT/KdPvxp1ZwNTD8j+TYbVfTricI3gWA65o4mux6557Qs5RvctMnOxH8wI67KBsrtOOuESfroYl6x/dInlWhohQFWj2GaiYdPzNYu1V3TZNLDKnX2n+FZ5OynN5fr8RXypYgmfL63s5U2DR/i9p15BX2+DVw6f5Fsr21CEJIwUVCWi4VhUymmkq6BnXV695QS9eovH1iYZsWt84/79RP0ug301lko5xJpBZkONm0fOk1B9Zjo9Mc7Qsxmxanz98g6Spsfm3CpV18bWPLxI5bV9z7AWpDjZHmJXchFHanz80iEiKZ4v8Z5qDTLT6OHHxx7j4fpm/Eil32wwaq1xqjVMOzDYmlqh4if55NvuoXymfGXLmQ0jcvB33nvF18jsj/+//7qc+UFDCImteaT0mCdqWv4/bjNcIiliFJ/VJad3n7eBzGguTqiT1Fxs0yejOuT0DmnTJa25pFWHhOqRU9uo3YiC2SFCYCgBugixFTdmg4SQVWPMX9ZwsDWPgt4mXH9fRUjyWud5NauuhLQUAy9S6fgxF9ZUQkwlIGd2mRUxR5VQxErTdS5JmIiBQmEoyBoOl5OCrNbFy+gI4SEUSV5vx2pTJcRS/ThSUV1UPQYey5FhgvkFZBSXhC0tIKl6CCGxtAARiBgGbUBadUgqLpFUEGGc+8kluyh+hAjicDpSJaYRoKshqiJp6TGDFAkEcSOkEMQ2GioIX+KGKlGooKhRzFsBhBIhhSAMFFK6S1rtookQTQlJKi7eehXFUOLPXYZKLLUXsYpYERJtXW1qaAFCjZCRSuirZLUuuoj7XZKai1TjxGjWdFiSIHyBF6hkNAdL8ePzQfGI9PjzVZSIlOGiiYhgnUkSRCpppUtbMUmp8fHqkY7raxRSHbJqnEjNaLEVR1rtklQ9XKHFUQeSjNal4trYqosv1ResWP1hRhhXOl60kYg5MSIHfv3nEYkQGjoyGSAEyCAOWb11cvnYwBpeGLNEM5bL0mqWqKVzYNcUAMvtDDsLSyx0chhKwFwjT9Lw6LObPHVqksO7LlFzExhqyESywtPlUZZP92HUFPpvWCRjxo5wvq8xlK/T9gxW5vJs2rSMG2goQrJQzmFaPooSUUy1mbo4gJr2yWbaNFsJJvvLnLswBJokfcqgNREiDQkShjaUWSrlAPixvU/w2Uv76M80mVnuIepoKB2Vyd0LzFVyeEtJVEdgLwv86xt0mya4auzyFwk2vOUYFz50gETaIZfsYv1JnrmfCih+PkF5j8DrC3j1gWc4sjpGx9NpHy+gbG0R+Cqhp6IuG2z+wApnf74I6QCERCgSuWbGCty2YOwbTS6+KUXuLCgBrN3RRYaCnnstOv0CLy/x8wE9T2lUrvEx0h7Bkk3f1lU6roH5+RyhBd7L6zhdg6CrYee6GFpIbTWFmXEJ5pJoo23y6dgCVNMDPEdHWTUICz7WjMnIS+aYOjFM7oyge1sTbzbF1r9aZOCTa1TcJGcenmT4AZ+B37pEUvMIpMLjcxO8dtMxTjaGOP3UBK+9+QkeK22g/u0BIG4nGHzlLOcuDfHL13+Dj18+xOtHnuXv//oODrzzOI/MbcBpG2wYLrPaSqKrIdcPTvPlY3v52asf4G8fvYn3Xn8v7//CHdx95xMMmHX+9PVHXlgk8lsvIBJ5579GIj9waEZIcaQGgJPRMLXw+TtruZkkkfSwTY+2Z3DjwCVW3AxPzo6zfXSZ2WrsVjfXyvP28cf5wvI+ADYkK+zNLnCp08vh7DStP8tR+9tETKgK4o9qW67EotWL2wM39F3i/uXN2IbPQH6NIbvOhUaRkR3TpHWXPrMJwAWrGPeohDrTlQLX7zlPw48tJEpqRMc3GB6vUP/OAM7VLYZyLVbWMmwfXubk9DDSVVA6Kl+5vBPvfIZb7jrClz/wMvI/fZlICm7pO8sHHrmd217+LLqI+PojV/Gjm5/hC9N7uHrwMqf/4y6EhAsfOsDmdzyN+M4wm9JlHvp5kyG7y/LOJFKTpM/p2Ic83j7+OCU/w6e/czPB9oj+QpvUb9gsX2cy9QdJxIIgmetgGz6qElFa6uf6m07yyH27uPDWJKlZgfb6EknDI/dbBbSVGud+K4dYtgjTIcIKqd4UsGVwlcZ/G6H8SoeN2Qr7M5f50tv2oKshd/afohlaLLpZztf7eN3Qs/zN0Vfg9qlcfc15jsyMU66m6e+p03RMdD2k1dFInTbp9kleWrzAxLVrPDw8STHdZqWT5vSv9aO6HidnhkiXBNNvkdyaXsRSfD54/lqG8nWeqExwe/8ZTvYNce/8FoYyDSqHmxh6QBgpvG7wGWbyc3xhaR/XFGdQRER1X8hCJ8vtG86SUl1aock31rbzUzsf5WOzh8gXm5S8NK8++AyP1zbA5jYJ1edYY5RQHr3yk16CvDJ26g91vGgjkcTAqOz/j+9GVAxk3seYNZGKxBv20ayAsGqSH6vieDrd+TQyEVIcrrG6ksVKu3AqTaRKoo1dtg8vc/aRDYgI7N1VovsK5O9axNJ8pld70J5N0dngI6yQW7ae46Fv7iFIQHpLFdfXcNoGYs0gygaoVkBYNxCJ8PkvXKgRsquBKukdrNN+vJfuWPx6g301Gl0LXQ3JWC4Fq03dSzzvR+u2DajrKJ7gTbc+wieOXo1wVEj59PS0EEKyOpfn8O6LPPvtbUSm5OqXnOGxJ7eROy0IEoLEHSUsLWC1mWQ0X0PevIB3x0FmX6Wy/U+XOPeHBcwTNkEyRi4aEy1MPaBeTfKhl3yAP775Vaz+Z5NyOU0y4xA9lWPkOy3UhgN+wMJdA3T7JWFCMrxjhbnpIlpNJUxI8pNrFOwue3ILnGv2c+rsKFpDJRpyODBxmbzR4frMBT76U6/k0hsNpCERVojsqGBF3LzjLEc/sof6YYfNIyW2Zle43C4wXS1g6gHKx3soHZYUjis4PYKRO2a5tmeaz37kJtK3LbOrsMRcO8/0/RN4m7og4GPX/Td+6dybGE3XuPixLfgpwavf9hBPVcYZS1V58pN7aY1HjO1cYuZykWu3xa0QEYLjS0NEkeAtW5/m4fJGLi0Ueeuep/jYE9egpX2Cps7YVwRzrw8RWsRrth/jkZVJyrUU6aTD5p5V7uo9we9++7Xs33uJr771y7iz81cWiUyMyIHf/PkrvkYu/+Sv/lAikRdtdWadmxyH/YrE7QvwivG6XALSDAlCFVVdz36H60CXIP6+7CWJEgiiZYvVThIRguoIaqU0qcWI+XKObdkVfEfDrEnUpgp1nalmD35KEqZDapUUrhObbUvtuTcW6zkGCa4CnoL01rt4FUkkwS1EEMW5A8ePFZxj2Rqbs6uMJatMpNbYkKtQzLToLTYwBtsw3KUTGRQH6hBCb2+TwXSDoVQD4Qm2plbw8iFBQjJhV1CcuIzr5WB7YYVd+SVyyS6b0mW8Ow5ifPMI0g7obOsnm+7QHQ7x+gOSCwrOUpJaKW4nGFVbNPcOMJmr0Fds0JPs0JnwqW1JUttdoLG3SGdI4hdComzAQLJBordDkA+QGZ/BdJOJ1Bp5vUNGdxCJgNCK0PSQ0USVTXaJYb2K2vFILKmgSGQksOc1tJLOipNG9SXaokne6pDRHGpuzI4JI0GkCxRXIbQEXlbSl2gyaNRILkasNZMkVJ+6a+H2hdgpFxkKhrQu/XaLPrNF7oJH7lJIKzRZ69pUXJvMbEhkRrRcE7WmYaoBmhLGzNeVJG45waqXZq6SQ1006dWbIAVBw8Bc1kmeX0P6CmLFpObbhJGCLFnUazaLrSy24mItq8w3c4jwBUYWUlz544c0XrSRiDUyKod/8ZeIjFhoFloy7p3xBFG/i2xpoEnyAw3qDZvIUSn0N6hdLICE3OY1FAGVcpqDm2Y4s9qPEJJWzUY1QnKZDm3HoJDqUO9aZBIOg8kGl9Z6aZ/OozcE/p42o8UqM0s9RC0dq7dLGAqUcym03fXnTaHCNTMGKGsRxb4Gjad78TMSel1koNBbbFCeLsTeuGds3ELciazVVVLbqtRWYhj0Ww4+wecv7iUMBTJSCFetWGQ30SaddHAf7kVvQntMEtgRIojLuNbVFYSQ5P48TfnnOzQXMkg7YMs7n+biX1xDakbBy4IzGLB1ywLn5/uRgYJoamzYucjMiSGssSadlSQbPx0w/SoDxY8nQanEgOnkgsDNx5NzdU/E2Ncj9FbAyi85qELiPF0gSMUJ6SAVUTiuUDkUMDxWYXktQ+iqiLbG5Gd9/KTG3B0CxVEIc+s2k4HA7OnSm2mz8mw/2sYWuVSH0rkiMu8hqgZ6Q6DtatCuJlATAep0ArUj8HZ2EPMJxr7hofx6iT25BT731EGypzTkLVUShs/2wgoPXtjEK7af5J6pbUQXU7z0luM8tjCBfyqDiAQihHBHC3Epyc+8+pu8/9QN3DA+xYMP7Gbi4DwXzw3GtiXjDroR4FQt7rzqBF9/eg+37z/Bd+7bxxtvf4RPHDnM7i1zbE2v8F/f/ACNKxWbTYzIwV//hSu+RmZ/+lf+NRL5QUOEEGRCFEfgZ0O0dpzYixIRMlBQHAUz59DurltJegquryMNiSz4OI/2UjvZg2KE1N0EzvksnYtZ7IxD+sEEphbQn21SqqUQD+dYuljk6QsTbO0tYZUFkQGm5TNbKhB1NdS2gtMyCBwdPxPRaVgEjkbgaEg9QrgKuCod14gtHFRJ5Koks11aXZOhTauMFKvsuP0826+aZXTD6vO+MIQCpaXSCk3cjo7fMAkcjZFtKwzvX8JvG2zvKeEWJM3JiIM3ngUZC8lECJoakTR85n4qIG932f6nS0z8g8LFv7iGTb/4OI09Hnor7l25+PQYhuWTzHWRZsTvTn6BbX88jaGFoEkW3u1jrSps/vAaW96/xNa/vEz2IrTGIyJdoryqgggECzdqXHqdSRQp2KbHdXcdp2/PClIFq6RSORhw056zbM6t8hv7v8bEJ+MIbuo1Bpfvjr/jMBNwYNs0/Q+oYEakbZexdJWRA4sEvkq9nWD0myHaosnQg5LkAqQsl9ftO4p13MbaWWPXnefoLzSwVgVTbxXMrPTwxvxTJPvaDL1uBu7L03yoj6Tq0dfbYM1Loh9JowRwptpPeznJhusvM3HjLGMvvYyqSsIJhxU/QzHb4v6Lm7nj5qNcvNyHSAUEmYiBzxu4JRthxWyWRG+Hey9uRZtscbbZz7uv/Q4nj41zodmH230BvjMvRCPyrzqRf3qYEyNy4N//AsIOoGpALi53Rp6KmgiIgrgha3y4gq17rHXtmN7VtgjLJiNbS9i6R8HskFB95ts5eqw2Dd/ici3HzaMX+NZnrmb33Wc5V+4jiBRGczX8SKX0lVGMmmTDu85zcnkQTQsRwIb8GiudFKVKht2ji88f6/nVIgnTQxGgqWFsbKVICpk2q2sZtgytsFDP0lhMI6TAHmzhOnrsTKdGrJYyECjcue8E95zfzsGJWZ56Ygs9Wys4vsaevqV/bOcHmLEZOrDE7GwRPeURlC1EIBh4DCo7FYLNHbLpDsG9vTT2eGz5iSOsvPc6OoOS628+STeMbRiafzuK+IkSC0t5rGmT1JwkSAicPnD6Y6gOQmLP6Dg7ugx+wUDrRrQHNJrjgsiQiCiODq0K2CshbkahMyDInw8R71plYboXJe2TzXToSXZY/PYoQVIydHiRrq/H6EhHZfvkIgtfmqBbjJdOalshTIcQiDhK8AX2koII4hvM6GunOXVpmNRZg+DqJsbDaQpnPSZ+5xwVN0n1L8bxUgqjP30BSw1Yc20uV/PcPHae+U6Oi1/YzJ43nObJy2MkHk8RWqB4cO2PPMPDc5O8YdOzPF0d41B+li+8/yb2vf0EF2pFap0EObvLwnQve3fMkjG6PHRsG286/CSfPX0Vb9v1JJ/4+kvYf+M5NiVX+as3Pkrz3JXhEc3xUTn4719AJPKz/+5fqzM/aCiOQOkqaGWTSAfjrIVUYihR6CoYayr+mMvcSh4ZCWSgxMrRuoEQsFjOgZBsGSzhqBoX5vqZtT0mixWapRTdIZ0gKTlfKVIrpzDTLh3fYHN2ldbKMMi4yU9VI5pLaYSncLJjEnkqtDVOiiE0PQ7F3aZJd70JT885RAsJQjtiuaOjJ3wuLhfRjQARCfRiF3VdR9F1DZyahTWvo7rg7tYQ81YMVV5WcCdjItmzy3G7erBqIQ2JOt5lfjVP7lmd5gYVzYt1IOU9IDWJecKmOmxiZiF9ymDlvdfR/1ePsvpz13K22sdgsoGmRKhexA39F/nq12/Ay0GyFDB/s8rkZ7rUttqEJkS6oP+ROhf60wgpWbhJY+jBkPagRmTES/MwIfFvbrC4kiRzTqB6sLpfYVgNUTM++8cvc/SxLQSbFZz+iCgRUu9ahJHCNRumefKB7bRGTJoTEYmRJuO5Bhcv96GZIeYxm85ISOqyQu5CwPwtCuZoi7MLA4yOVKj1WKT1gMKjMH9LmtrKMHeOneaL2ybpbHGpLQ5hmT47i8tUE7ER+MmFIUwFVrqxOM6/obHuW6RQcZPkkl38SMXWPC60+3BvbvD08ggQa2MWpnvjnik3QUp3EYmAS61eRopVKl6KbddOM9/M0Wu0CcIXuBj4F3jPf9FOIpEBxkgbt22gGiFd2wQB1kAb39PwTI3eQou2Y5BKuEgpaHZM7MEmfqDidAzUZYvceJc110Z6CjIhyBgO6BGqkBg1EbNSHRVXMSgpKbbmVqhvVIg0GDUcLrT60PMOqioppNs0HRM1L9G1kKwVS+VrdiJ2fJOCMFTo5gMyPW0UIbHNWD7fLKVIzql0Mhq+o6GsGrhFD3sqdqazKpKlbobCKbD3N1COGMzsjh3dNg+VmHpiDIbduJ1/zWJissRS3zBhLqDwkE5kQOVQQPqcjpuPTa6cQYksxQ55qz93LcX/8hjzt+5kc26VIAqY3aJxsjGE2yPJnZdUt+hERYfKHptun0CqQATl/RlkwWVtq0WQ82mM6zh9ETIV0POYjp8UmLs9OgmL9rCKEoDfE1BqpFAuWyz2ZImsiKFMgws9FpYVUEy20dVw3ZhM0mc3mTOLCAGmGmuCwkDBLUhQwOmVlC0NqcQ9N+lUF1MLSFsuKd2jdKCX7kCE1zF5am0ctyAxkx5hoOJ6Gq3AJGs61Dw7pt4NJ1ioZglDBdvy0NSIUBXYmkfWdJjq9GKpPpebBSzDp1pKs3PTAlUnQUNPQqAwkGzQ8k16e5tMVXvoSXa42OwlYzjUuxZTrZ4rtbv8rhP/n/9a+t8d/+RyRgjxQeBuoPRd3hQF4B+ACWCGmAhdFUII4C+Bu4AO8A4p40K4EOLHgd9Yf9nfl1J+eH37AWJP0ATwNeAX5BWssezNQ3Lyz34S19WxTJ+0Fbu/r7Vs3K6OYQUYeoDna0RRzO5UtFgtmU46pP4uR3NYpTkJoSmxl2JnuiAZkZ5WaI9K3nHXd/jwl26mcFrSGFeIdPA2d9HNgITp0zqXJ7Qi1F4XocQdsVGkoBvrEUhXRwiJUAAhUdWIMFAZKcZdp1GkkLBdhJC8afIZnEhnxIhRBuUgTT1IcK7Z/7wzW6mZYmOhzKmlQTb2lbmuZwpdhPzXE9fz7j0Pck9pO06g82Ojj/O+Z16ObbuoSsRto+dIqw5lP4WteHzq3utILigMvnKWi0+PceONJzlb7aPRsRh5/Smm3nctUpNcd/1p/nj4a9zwuV/mzuufwV/v9zhRG+Lc+eH4i5Bg9XbZ2lei3E3y8qHTPFmdQEFiqAFOqDORrLA3OcczrXGeqQxTrqcY663y+sGjOFLnJvscb/+rX6IzHNG/s4QfqlRP9RJkQiY3LTP31DCRIXnp9ScZSVR5sLSJUiOF72mEKwlECJEuUXtdbt50jttzp/gPH/pRtKurvHHyGf7+7CESps/uvkUefXQH33rDf+TPSrew1V7mk79zJ3ozpPOeGuVKGhkKMsdNopfUcM5nUTx41Ssef56i9vVvHMLr91HtALlisfHTXX7yQ1/gNz/9Vry+AKWtkj0nqO2MIBToAx0KmTap92XiNoU5i//4+g/zR//h7aztECz++Z/TLs9d2XJmbFQO/uovXsmuAMy+55f/Kdr7LwE/GX+LnADeKaV0rvgN1seVxFIf4spNf+8ENq8/fhr4L+sHWyDG0R8GrgZ+67vcxv8L8FPf9Xf/43t939FcSeG7Go1yrA7U1ZDuqo0MFcJAoVFKoa3bCYg1A00LCRdt1spp5l+mULvOZfN/XUHqEe3NHm4xYPNH69S3RIx822PcLONnI5ZvCvF2d1D21rFPJOjNtDH1gOH7A3omq/Ek1THwOzpRIAgvpgh8FXXJRCxbiDmLsGHgd3U0PaDtGYTLCcKuSmslhSokZT9FQWvzdHOCC93YufBCs4892QUmkmtszZQYzDQ4mLuMV7XYn5/jUqfIieYwLFlk1Q4rzTSzs0UuOX1o52wGMw06jsGB5DRX2TMcWR1jwipjTLRoToacn+9HG2/RDXUGkw0ODM0x9b5rmfx/H2PLBypssCtMBTbmSIurUrNsTy7xaGkDe/MLpAeapAea5IfrbCyWuVzP0Ztoc7Y1wNb0CgvNLCeWhrgqN8cOe5FH65tYdtKkDZehQp3lRprj7VFyaofj7jDNrT4j3wlZnO2heqqXia86TH46ZGq2Dza02fKBNUYSVQb1GjOX+hnINtnQV8EYbsOgQ2K4RT7bZqu9QiVMERnQPZPjodVNBHNJNuQrXKgVYdChKTV6jBZP1DewfA3M36xhqCHmJQsr5eHmJQnDJ7G1hlcMOVUf5GRtiBPVITb+fZnhr6v0FxoMPSiZeZXN8c4YYnOLvgc1hh6U1HZEJIebbPlIE9vyWF7OcekNBtaJBOGIw+OtTSzfGOEN+QTpF7Y+EfLKHz/wdYQYBn4eOLgeHKjAj7ygg1kf/+Ry5gWa/r4a+Mh6JPG4ECK37qJ1E/Ct5/wrhBDfAl4uhLgfyEgpH1/f/hHgNcDX/+lDX6eXaxLRUZ9PKqodhRAI1Qi1ruFlNYQeIULwujqaK5BNDamCUCRBbxq1qxAmQjAjxGIZtAwighU/iwhjBKAkbhwzy5Kur+GHKqluSBAqcR5k/UuTXux1GwlJpACKRGsrcVk2EHiOTiLbivcJFLSaildUUYlohhbNwCSpxVGVIiLUdfyegqTmJPClCkZEI7Biv1xiklYzsrAMH6FHNIIEkSHjC8MIqIXJ2A/W0yn5GUw9oGtFEChoWkTNjVW5QRQgNYm6fTPhmQtUfZtKGE/Ea2GSTmjG1SUpUEXsJayrEX6oYuoBTd/Ei9RYDyJi+v6ql0YXIStOmpqTQBLDmZyugR+prAZpAIQV4mYNCCMUF9y8jtqNEI6KSEGQt1l0coRSQW0pNBwLVYliEr2vgAl+oLLkZbHVBIEt0dqC1XYSsxJ7vDheXHJfDtIsuxnKTpLIjpEMlWYSaxXqXR3FlnQ9Hd+P9T1rXRs/VIgihWLORHUlNcdkYKlLpNssOVkURaI5EaEeG3q5rkZoqzTbEqWuI/V1an+gMNPpAV0+r1l6QeOfNyeiAQkhhA/YwOI/sf/3HFdUnfkeVns1KWVu/bkAqlLKnBDiK8D7pJQPr//uXuLJ5SbAklL+/vr23wS6xJPP+6SUt65vvxH4VSnl3d/nOH6aOMJB7ckdGPmLX4krMh0NYYUoqny+oSkMFMyETzHTos9uUnGSrLVtJDFCsThQJ2s5HO6ZYc1PxnL1ZI2s3uWe6W383PaH+ODfvIKdbzvN0wuxFedQvs727ApP/M1+VE9y4799gi9d2I1l+hhayIZchcVWlqZjsrlnFVuLyWZn1/qfX99LGTd+SSnI2V3W2jbD2ToAZ84Pk+lvMZqrsdJKM5qpMlvPs1ZOQ1fl9oMnuOfkTm7deYZHv7iX3a84ixPojNg1vj29hUzSwVBDllazHNhwmadObozb+Z8tIEJBpEussqC5zyVXaFGbziPNiKHvKKheRHWLxlWvPckGu0LVtzl30GfiyQT3HN1N4RmV/u/EfTP5Ewr1rbHxOAL6HxO4P1Kl8OdJFq+32PDxRU7/ch/CDkk/Y6L4UNvrk5jTsZckXlbgp+GGu47x7Wd2smXzIouNDAcH57j/xDZUO+C6yUt4kUbLNzl1ZpRXHDzGIx86QHNCMrB7haUzfTELpKMgDYleV0hPxdYa1R2S/fsvUu6muLzYw8jgGup/6mH1KoPDrzmOpoSc/w+7uHy7ylWHLmJrHi3fpOlbbM8uU3LTnP7sNra89jynlgdx1iww4uXJT139EEdqY+zILNMKTQaNOu9/8GZu2H8GJ9TpBAaVrs3KpV5ede3T1P0ED09t5DXbjnG8Osyhnlk+f2kPW4sldmSW+cs3PEb7/NIVL2eG/t0vXsmuAMz8/C/PAuXv2vT/58UrhPgF4A+Ir8V7pJRvu+IX/67xv51YvVLT33+Osf4B/B3AxK6U/JvrPoYu4jttJUyhErHRKNGWRnzHWmc1FNUmlgh51hlhi7HC5aDAddYCD3TH2Wws81R3kjcVnoxfK7L5hUP3MxemaN/Q5k3FJ/nlwW/SkCY9SpcvNvbxb371s8x7Ba5NXuD6/RfIKR0s4WMrPp1IpxFZ9KhtLBEbwLQHNAwiPBSmvD7OOYPcmTkGwHIQG2i/99G3ghR4z+Y51ZeOmSQDKgPpJlu3rAIwZNYZHynzU333M3NLgSOzYyAFv3ndl/nWPft52V0nCCKFb3/zGt5z/b2cLpylE5l8wL8W31dRVUmwPeJDV32MUbXFT9pv5Xcnv8CvTryBG/ovcrIxxB8Pf42pII5A3Cf3MnN1lw9P/R3vGXgLve8oo05nuOPnjvKS9DmU9SzfyZeNMtUtkvlPXT7x7CE2f3oe2fBi+4sBk7ZrIGo2oy9b4s6BkwzpVb5V3cUD9+1BUyD67SIf++h/ed6LJ5SC9wzcy1qY4t7GDn7/5V+gLXW0nwip+wnSmsPelz9KiMI+a5blIEtO7TDl9fFAbSv3P7udPx77Ah+pHeYzzSQ/Mf4Iq3+W5uOXDvHG3qf47d9+JyO/fpHZS+P81fgXUIXgT1dvYKNVohOZ/ErffXznp4+zEmT5uaH7eN/MXagi9sW5NX2SrNah5Ge4JnWJlybm+Fv1Zby651l61BaVMMV+c5GjG4cY1qp8p7WDuw8eY4exzNeNXegipNu0+Nurv8AXWpsZtyov6Bp4gVda+fvlRNbTCa8GNgA14NNCiB+VUv79C3oH/tcjke9p+iuEeP/68098937PPaSUP7O+/f3EUcj9wH1Sym3r29/y3fv9oGH3j8ri7/48RlnFz0ckFlSkAk5/DBvSyjrqZIvA12ApLgMnRpp0F1OQDjBmTSJTEo112dhfZubxUaQK5tY64qEcmduXUYRksZLFOG3TnfDQkz5Xj8/y7Bd3ENig7a4jJXRWkhhVFa8nBC1CrWmEqe9Ko0tQukostBpuIU9k8HKxKjXV38LpGuSzcTWimGjTDXQankmza9FpmYiKgeIK7rz1CF8+thfRUZGJkP6hGkJIlmd6OLT7Esfu34LUYP+N5zjy2BbypwRuQaC9tELC8CmtZegvNMj+bEBz7wALNyls++NpTv/RCJlnTdweiZ+RmCMxtKm5nObDt/8dfzC5j9IXt1FbTmPmHfSn0ox9ag7puIiExcotwzQ2AxFk91SoXCqgdWIVsT3RIJNw2FVY4kx1gIXT/WhtgTfgc+32S2hKyEtz5/nYL9zN7F1avHQ0I4SnII2Im/ac5fh/38XaNR5Dg1W25UosdTPMVOLeGfvvsyxfKyg+DV5GkHrNMoeLM3zjE9eSuGmVLflVphsFGvcO0NrqIYyIj97w3/i5429jIl9l5ouT+Cm47VVP8XR5lA2ZNY59fgduQTKwf5m5+R72bpoDIJAKU+UegkDhTdue4cGVTSxVsty55RRffmYfQo+QjsrQdxQWbw8QRsRt287w0OWNhKGCZfps7S1xXf4Sf/nwbRzYMc1X3/Zl3Jkr7J0ZG5XD//aXrmRXAKZ/8d9+38SqEOKNwMullO9a//ntwDVSyiv3pFgf/6uRyPcz/f0S8B4hxCeJk6j19Ynmm8Afflcy9Xbg16SUa0KIhhDiGuAJ4O3AX13JAYhMwMSmFfwNKkGk4I3FbIaxZJtKJ0nUD712zPPYt32eRmDx1PIYu/dfYKZeYGBDk7lGlndvepD7qtvgmjk2ZspsSKzyQH4Lbxh4mk+8/Q72/dk8S4UMqVBlJF1jOFHjscE4x/LOzU/y8UsH2bR1iR6rzbBVY7ZTwFIDcnqXohF38V7qxCxUL9KYbeQp3lwmtc4cWXNttELETDWPc2+RlYMd0kmH2mqKyYkSU6UkqBIRCc43+khMG/zom+7lm//+pcj3xkuk9974bf7rp1/O3tvOkdZc7n98F++8/X4+Ono1OweXqP7hBIofEf4YpH7DZvU/u0zmLlJZHmTx/Xmsp028HOTOSw7+wjNclZplLUzyya/fznsG3oL6RUnfq8+ivutaWneAl5Fc/JM8uh5HWsFJ2HXdRc58ZzOVSwUKJwXOnQ2ydpfUb6dQGxr3/eYWwpUEUpN4vRHCiFhsZ2n9wyBP3D7Bxl9b5N8Uz/LZ+X2kdI9b+8/gRxrzbh75ijV+edOj/PWnXsl3+nq5ft95znYGcIVO8NYmqqtRGYjbAPxvDfLZ0X7e+raHWHKyPLU8ypaeVWr6APgKezbP8c4n3kniiSQn99m86m1PYioBT5bHGU3XCKTC6370AT5y5Fq8UGV0pML51SKmHhBECr+y6x4uOv2cb/Vxy8A51MGIDzx1A5sml7mud4pOaLC0J8vK9AZ++ap7+NrqbgZzDbZmSyhIGoHJRy4d5tZ9p2NPItO/8qtO8s9Z4r0MXCOEsImXM7cAR/5XXuifnES+2/RXCDFPXGV5H/ApIcS7WDf9Xd/9a8Tl3YvEJd53AqxPFr8HPLW+3+9+l0nwu/nHEu/XuaKkKgS+ynItQ+CrRJEgclRQJa6vE0UC19FRlRgNcK7ZjxtoaGrETL3AWj1JQvfJWC5rYZJ+s8FSJ0PDt+gYJkvNNMpARHlfin7No+MaKEpEJBXSqkNqWkEJwYn02Ge2bVN3LIKMQsVJ0vV1BpJN2mEsaZ5rxdYAQaSgKxGXyj1kbIeU4eGHKpYWH8vSWIRpBGQTDm5OQxMRRt7BLyeIdNiWXeF8/wgXOn2sbdMYNVz8SOVitw930uFcuY+U5RJlfU43B/GXbS4YReQ+PfaJWZYsX2fSKDsoQtJZSdLRJL1zkmQpoLpFx5cKzciiE5r0f2eF3neUeej4VtR3XUvPBx6jcuBqBk9JVpI2vh5rNPrOSc7v6qNwOmJth0LxsTXO7c3SSVmkrjNRvRRh2EUEkFhSCRPgZRX6JptcvqqfDT015utZplJFVipZGrbLbKb3+eVNtZJiaSyHCABV4oRa3NtjhLSWUpAIUao6mRmF0ARpRFxsF2ONybo+p/dEwKqqkdnX5ZqJac5/ZQeNrsqykyGtuUymK7RDg3F7jUvtIsaizuS2CheqRYJAQQiVMFRoRyarXop9mXkiBCnVAQk9Vpv5bh430phpFAg9hSU/x0SywkMLGykWm0x3etiSLHGhVsSNVLYlV5AvhLHKC17OfN8hpXxCCPEZ4CgQAM+wnip4oeNKqjNv+T6/uuV77CuB74mjllJ+EPjg99h+BNj1Tx3H9xpuV0fVIiJXRWhxN2+3Y5DNdAgChVbXpFDosNpJ4voafakWU6UeAk9lNFXFCXU+N7ePQ8XLJPU4ufaly7votTt8aXUftZc4BJHKhnwFSw0Ytavcs7Sdxk4fta5x7/JWdvctcqYyQNsxqFsJwkhhZTFHdoPDmhNHIKuNFJYRQ4kG003mL/QRFFXIQNs1mMg7LJ7rg7yH/Y00ixvTBOmIqak0fXtWWPZUyMGwWSU12uBSvZfgcJNzx8bQWgL/ahXT9umcy+F1Bdk1OJocgZxHeyFN8nANKQWbf7XL1B8kSeohlhaw8dMBC+/2CRJp5m9WiYoOGc3h03P76bgG9Z/Pok5nMPMOrTugcuBqtrz7Sc6//xBqKohV70KycpOOdTzLyiHJyH0+Z9+bpviogt7Vab21ih8pZL+VoTUO7fEANetjP5PgabGZ1ESdy0eH6d+zwoMLk+TvtYh0iwdevpFu1yDyFQq9Tb40swu5t0l/ssvRo5sobqrQa7eZNnuwLZeWbVG3LexiG3UqQ8VJcun0EOmLKjO3+FTvlmz6aAf9FRGrborSdZKRbwm0vRGKiEioAU8tj7J9wzIN38IrhmxOlag4SVpHepFh3Jj9hZ59XLw0wG/c+GU+OHsdbxw9SuqsQXq3+zyUaPNIidGhNb40s5s7x07T7pjktTYfOXkdh66bZfVkHy+79QK26j5vU3LF458x+yil/C3ioOB/a7xoG/AUR6AZAeGaiWKEpE6YJE/GMvSuaxCsJuhNt5mv5SiVMzieTtOL4TWJlMvJ1UGOXh7lqt6FmIt5eZAzy/3cNHSRmXKBw/lplAWLlW6aU0uDTDcKXGgWefPoEexpHXNNsKuwxEI7R7Vh06kmWKxlWGvbGGmPmXKBtqfT9nQSpke7Y1Jv2lSdBHpNQQgZi7uydWaqeUTBRVQMWre3UDa2kGZENOSwcqKf1JEE2Qfj6CB8Ms9qI4X2ZBp1oIPc1KbhxI15CHAGfdrXdND1kOK3TdSWQnAsh38qw9mfL+IvJImeyjE7U4y7cZ9J4/TB5Ge65B8xOVEb4ob+KW4dPUf+hMIbtx9FfyqNPJdi8EGF8+8/xJafeYrMQxapxxPYT9qMfx4CW5JcFMy+UtD7uEZrTLByNbRmsnTm0uRfv4C6sYXaVmHJpLXDY2BbiW7H5PANZ1g608dgukn1Fgfv5XXG81V2DC/zM/sfYm0hx4GBeXxfJW26vOqGI4SRoO5aBFMpmq0E8lKS7GmNbtOiuKvE/FqO2w4fZ/x1U1zVt8DgfSqzdyc4URlkV2YRxRFUf7TFM0vDHCsPM2A0ODx4mQjBpXIPiqNwodVH2zcYf8ksm26dYuKWGfbl57l5zxlCFG4fPMtUt0j+tiUeXxxnU7HMjvElzk8NUm4m2d23iK16DBQanGkPcvvekzRDi7tvforTjUHcSH/eN+eKx7/ABrwXrexdhBAt2CTKCl7HQoTEremXbdxkhLmqMp/NI30FrazjKRalYQ25kCBMhfh1FVS4V93CWG8V7bJFaMC3za1ox1J8rbALe2uNmeUetGmL5RGdRsbCUgP0VuwZ89D8JFLG7myJqkK3q4ICWkMhSEesKRZinWGidQVCwAqg+4JgKoWXkFz0NAJXo7fYwM906U83CaVCzU7QcXU6vQqd0ED1YNHN0hkLEEs27lhIXyampq/MFti+bZ6LC+PoVY0tW+c5e2ScVCCx1hT8g01MI8Cv2SRzHQb/Qae2JcnaLhj/YoUz78lS2xpL2c+dH2ZxIIMqJM2tkpekz3HsU+Nc/JM8K0kbNRWw+rPXUvzbx9BGholyaRrbs4gI/CTYfW3qmzOIaF2309/FSnj0J5pUOwnCcF3LYysMp+r0JDoczk4z88hWLujDEApaps6plRRoET272mRPaTyY2kQq5dBjtTlTG6DVsfCMgOIzkhU9Qd/J2I0vV2ixq7DMI4/s5VhymIFkk2dXh5A5QZCOKJfTvGTrOT49uJ9iusXyQ8NUkpLpnl6OloZpFkzks1kMFWabeZZWcmweKeEGGoFU+Oblbfi+Rq/Z4rHSBiotmwNDc8zNbeFELQGhoOcJjcqhBI+6G2Ey7tNaa9sYWkC9J8Gu9CJfmt2DoQZ43vErP+evQET2f2K8aCOR54yTgqQkTESEFrE1RCJC2iFBSmJYPrrtx3aPiYiE7RIlIrDihKBc15UkNY9IJ5amKxEiAk2JieSGGTz/xYWhQkL1iQyIdNDVGMkYJUMCWyKtCJIBkS6RVgi6RBoR0ophQWEywrZdFD9uSpN2iJXwMGwPP1DR1LiM6AYaYRSv54UikWoMYcrp3TimVuN1vxeoeEEM8jGUENWNDZ8UEes3Ik2guKDrsZoXIWOkYcNBdWOIkui68WdpEvfCrA9FWW/tJ0I6LroeIvV1cZsu0Nbhz0qlhm8rKF6s0VAUSaSD4set/boRYGrx562pEVJZfx8Fgkh5XkuzXg3nf3RQCKV4/gZh6gHa+g6qGsWffxS/llTi/8HQYrdDEUEYKSgixiCElkAKiapH6CJAXT8mZb3rN6d3SBo+Wb0LEUSqJKV7z8N9xDoYWgBRJMiqcW+OEKCLCKFHKHoMm1J8QIuv+JzexTDjpWwYKSRVDwWJDAXWC13KwL9IKNGLdhKRCmBG8cVoRnHfix0/V62AMBEnKa2ER2RHyESIpQdIK0Q1YpUiAjQtwtY8QlPGKk8tRARxk5eqRBh6EAN4fAXfiylXUsQXgqkH+L6KSMSTiGoHaGZAZMXPn3sIK0Qm4tKzpQcxT8QKUe0AS4/7cISQaOuT0nNKTFWN4klEjycSW/Fi8pcVm12rSvwQiRBFRDGjRCM2WkrEn4lU40lFVWKosqpE4AcoYQxxwvNBSCJdxJ+JjCdQXY14LucnEjEPFiW+mKQKUS6NNjhAsLT8/CQH8e9Zn8SeO7ueuwCf605+bqLQlAhNxDkJRNxh/BxeAEUi9DiZrXoSocqYNL9+cUSRiO0ivJhCrwQQPQeQQ8YTvYz/d1v34v9VAcP00UWIosTHhIwnEVMJSGg+CdWPJ7l1Wr1cV5U+ZxplrLN8dSWIz4X191A0+XzPVHyAAk0LSag+ph48f+yKkLF+yVfQRfTClx3/upz5ZxwC1KqG3lJQPLF+YgrUporsqJh1hUYyNvcWdoB0VVZLGYSrIlsafj4EPWIw22DVSSGtED3jkTZdyoMRBbPN2SPjWJNN3HwEiiRhe9S8BIWzAX5SIXGjT8nVoGo8H634zRg1IKP4LiylIGzHYCECQbmcRg4EscF3R2ONFDKCgWKdxcUCDFaxdZ8gVMnYDitNE92JgUvP1EZR6xrGeAv18Qyp8bg72Uk7XKgUcQcD0CJma3m0nEdo2bQ2hCgXcrR0GbvULfXTukvQGYpZHyt3jmHPxO385f0Z2N1lKNOIu5cfE5x82SgrtwwTnIyrMCs36eTP+zS2Z/HtPFLdQOGDj1H+u0P0fVNhoT/N2P0RizcIwkREdCJLzZKc3w2rCzlyswLFlwSJuFXhydlxCkab1b0qdrEe63gUSPW2EULiRSpOr6CYb1I6U+ToiMlgvoG3auPqEeqAGkd2BvSccpje1MtjWxXcDQ65/4+9/46yJLvqfPFP+Ljep7dVmVkuy7Yp01bdrXZqNZJADkmAkIR7iBF6IBAPDSBghnkPNGKEF0ggIYNA3jRq31J3V5f3Niu9d9eb8Of3R6RK/GYNTPUMP37q9Thr3VWZkTejIm/E2Wefvb9G9ZmuZHE8hZ6vLTL+Ex1IbfB8fQviXJK57TLuFhvV8DhZ6mWtGWVCzWMPWaSOmCzXw2tZqCaRJUEgJAazRRQ5zuV6JwC9mTKnV7tRNQ9VDVByTYo7EyhllXxvgwuVTsrrcXq713EDmYprcqraSzTXpOYZL3sZf7leV/8W4xUbRKQAkiMlSutxEpkm4sUMQgZ1Z43OVJWJxTx7++aouSaTJ3ugu8WrNo/xwuwgbck6Uc1huphhdj3NW7ecYHI5h1sxqKUMUiNFinaMfQfGqNgRvF01dqQXiSshp+XMO7twHZWcq/Fju47wlaldlBeTyALae0JbTFkKuToA8UKDVtNAlgNuG5xg4RcGmHy/hGF47G5fYK6eZvWZLiI3VVm5XECvyHimQBsDbvGRHXATgp/pfpafHe/FmUyQumuN9X/sRgqgscmnbfM6fR9ax93UwbWfjqLOGwgJhj/TwvntCrrsM3msN1Rlf3oUL+WTuqhS3hog2izG2hOIrM2uthVmKmkMzcN+S4mJVoHqcIgDuTrahnk2xfzd3xMbCjOQtb+4hZGfOsb0F3ZC1aD6njrdf5ZAErDpNy9gKB4XSx3s3jLDcm8cTQ6orqc4dnSE+247w5VKO07GR6xFURsygSdRV6IgCRIdNsqBEmulBG+/97uMmEt8ev4AP3rbi2TVBn8cvYudvQtc2tTOmqPw9l3fZaaV5bnp7YiCRNywGWlb4akPb0GehkTEol2rsOO+q5xb6ELWfQzT5Rd6n+Kzq/u5J3OZc5d7UR9a40f6T3Em20PVNfECGSEkZstpGi2d92x9jo8s3E9qwxnRDyRato7d1BAJn4f2nePxsa384s1P8jnlVvqjRb6zOMQDuQtcbHaR6LG5JTnJd83mjT/0P6A1kVeusll/j+h+3y/iRwO0soLb7iLJAqpqKN68MdKdVVIRi9VaDMfW8Joq6rqGV3DJFqr0JkPeykwlTVu8jhASYxe6efDgGZ58Yi/9+0PtTAAj32IgX4RfySK5PtofFjl3sQ8l5SBLgu58mZVqnGYxyqbB5evXMLmQRzM8ZDmswQghYdUM4pkm9XKE7s4SabPF1cU24jGLnlSFshUhaViUrUjoldNSObjjGsem+0N1+sOD7Ln9Kk1PR5UCVlsxyo0NAWNfJh1vsTKeQ81buGUDBEQWVbxoqOgepDzMaZ1AExROBUgCilsUMnctkY80qLkG0u/mufUjx/mHx25DcSSyFwOWb5GILUi4sbAGApA757P25ib9bzrHzG8covMFm+mHNQIzwFxSQQY77xOfVOh6uoLVFaXRprLnZ87y3MQQo90LnLrWT3tHmcrRNryoILNjDfd7NZ/jKQqvWqD2xU5q/eB120QvmThJQWRZIjDAKAkyVy3WRiOURz0KfSXW1hNQ0dDaWvT+qcraLpOBHx5nb3qWJz58B8v7ZdLb1zFUD1UOWKnG2dM5z0QlR+wPUvDBVSYvdqI0ZYQmkFyJ/Xde4tRiN3f1jTNRy9EdrfD0ye3s2D7LYi1BrWESBDLG+Sg9986gKT4Xz/exb/c4Z2Z7uHvzGM9NbsY0XW7pmOWzb3kce2L+hgoYZnev6Pu599/wHBn79ff/u7LZvzQkVUCHTSzi0DBDCrckCfyIS9R0cLwQHNSdqhBVHTTFD3UqmhGqSpT7d1zECxRcIZPWWmSMJgnVYqqRY9/ecSKyAxJsSqyR2dXEC2Q2x9c4Xeph/CeiaGWZm9RFdmybpWyFk7c7VsFQPKYDmc5o9fq1VrMmcSPMYgzFY2IlRyZfoy1eZ05IdMUrHB/vh5qGczrGuU1JAOSYR3ehjKwI5LhLd6TMWbOLnNHAbXc4Od2H31K4b+clzk72IKmhvmzmsE7qTSXK7c1QmcsIwAuNpcZ+NEb39mU6YlXGXxpBfnQd9cUU83ereGmXt3Zd5HK9AydQmLrN5HOnbyG/AWUvbpfpecZl+rUS0bZGWGeRBPPtCagazPzGIfp+60Um/9NBzLWwKNLsd5Fsme6noNkmGH9rEi/vkjol8fTVEYKGxoWJIfbecQ1PKNScNlQk8tEGVdukUknTfucyScNidqsgiPtIdQ17V5N4zCLYHopWNwOJ9X1RZCsgf0xh974FnlzcRu60jPtal8nXRdn8hQbDP7HCY/PbaWxS2Pz3NXb++RhRxeFSrYNKy2Qguk7eqPON1+/jh7OzyDsEK1/vDYvUHiQ3Mo/ByCpzzTR3pS9z7bHt7Di4SMPVEUJitLDI6Wg3Cd2izaxzKd3BfblLnJntYUd8npdO7OZN73iauGJdF6664fEDuOa/YoOI8CV8R6bhmGArWMJA2ihEVqwowpXREw6LtQSdiRpNV6NYi4UWjp7M87ObiJkOd3VeY9FKMVnN0h2v0BMt8/jVbdyya5rIssRyK8m5mS6QYCGbYl9hjvoL3QhZkL2/yZGJgesCSNWIyWojhqIEFO0ocS0MHJ4vU7WM66jViOnieOr14LPSTLC9f5ELV3uI3llkKF5loZ6kL1kK9V0bGlJTYbY/Q2Mlht8pkThvMPqGSzgb1UQ96pCI2hiqx8qr4vSqLs5ylHhvFWYSIODam1Ti0xKzkQJr+RjWrgDpWhY6JLq+41Pt1zi6ZYAtieWQzv9ZneG/n+Nrz99M9rxE4XCRy+9NkH9JoTKcvN7R6ns2oPqeOpnPx5j8TwcZ/LXDXP2zW5FjLpkXwqLswhss1BmT1Bj40zrNTsEDWy/x2Ild7LjjGufnu9jXN0tzs4Ma8YhrNlHVId/f4NyJQUZvW2J6SqY+AL07lpg710FFN1DrclgQL8m0TwU4cYm1/R7zzRT9fWssxFIUTJuOzzaYeTBBupllX2GO0zN5rrwrgl/qIaK6eIFMOtpi0U6xasVJjCuc3tbD5HKOYI+NrAUEvkRBrzGcX2O6lacnWuZCq4eZ1wdkqu2kdAtVDji93E1zPEVb9yRrdgxJFhyrDjLQvs61ZjvmXWs8szLCjvQiNcd4Wc/9D+J25hUbRGRbQjF8/JaKFPXQpwyEAv6ARSFXZWUtSTbZIKnbXJzuxIi67Oxc4PRsD2baopBoUGmZPD6zldcOnOfMchdrlTiZRBPdcDlW7kfcXUKWArKZBu3xGlmjge2rLN8mkByJ86VO9m+a4tJaO5VSjGkgajgU59MsKj5eEFbNPC+E5itKwGjHIhN/PULjnha66tGerKFKARNPDSINWdSOFLgiFQh0wZVygcYOG21dRcjw2vwZjqibeGF8M9K+Fqce34ZigbO3jq776H+TwTVlxE44U+4nuqhgHk/jP1pBkiD2zSTqD6/QPFugJUXofyxg/o5QVLnRqWK1BfQjeHZhONQC+aUMouqgNkMuzJXdKQovytT7JKRAoNUlkCQWbpfo/rME0w8rmGsSV//sVkZ+5ijy6FZK/3ctbIPPZgkGW6y3aUh6qND+jy/uYdOOBS6vtONWDI5eGyB9SsdO6Rz3+iGQ6OwoIfIOz01upvuReXbHSxyd7WfznjlyZoOXTo8Q765SW0rgZGR6dy1gV+NcmQuFnQJXIdfV5MJb2sieE9Rdg22JJdZe30SbjDFXTBMxHH5q6Hm+srSHm5NTfHTiXrztLq/LT2B7KnPLGXwnvJdPLm5hcSnDz9z+LH8y/yr6oqVw4XLMcCu7HgVV0Du6zLPTQ7xvx9PMVDOktSbPLQ3zw50nOS730h6psSW6RET7X2jz/oCNV26LVyIUoxGh0EughUU+35Fx/fC47ao4gYLwZVxXwfFVPEfB92Q0xcfUXVotnZavoW5oc9ZaBqrqU3EixAwHJ1CJaC5xzSap2pScCMLwEYqg0jJJaFZYdPMlvA3Dabkl4wsJx1FxHDWU8fNlPE/G8cOAEDgKrq/QdDU8IYdFYd1HdjeAdBrILqimex1j4goFNeohPBkz4oStTRd8T8HQXJAkzLKPkEBuymHLW4FExCYVbSF7ENMd/IgAPfSF8SNha9uLC0TcQ1c86i2DestAioYEP98UpKItiHtoLYGTCvBNgRcRYY0lFtZUAjNsHcgxF3l0K8H5y6QMi5RhhWZdEAYQI1SKlx2JmOaEOqN6yICVHYFWB9EKu2yWq6LqHp6lEdNCQ3bPVYmoLjHVQRh+iEMxfAJTkNBt1A3DssBRwJGpOUYoPvRPhq6HWBJd8zA0D0to1B0DO9iQtNQCZASG6n1/CxFI1CwD0VJwhEKxFUWVQ7yJLvtYTR3JkZF1n6jmYDX1EJXqaBiyh+/I+ITPSERxCYR8XVjqxh/8l/H6Nxqv2MJqbKRTDP7+e1AkQcvWSMZCUpkfyBTLMXTDQ1ECFDngrp5rzDSynJnoYaBnjfn1FOlEi4al86ahU3xzbgeKHLA1s0JMcTi+2st9XVf4zm8exP+ZNYq1GJ4bSuJlI00uHx4E4L57T/GPl7aTSjXpTFbpjFS5XG4jZVjkzXoIDgMuVTrQZB8/kBlfKnBgcJLFZhJVClioJklGLOKaw9ThXmK7irTF68yW0uzsWOTotQGEHxpJaR1NgukYb3/gOZ747Tvoet81AHYl5/nkk3dz3+1nUCTBY+d38KY9J/jW1HZ2ty9w5ePbEAqU72mx6Y9h/VebdCZqzJTTBIGMeymJkCA5AV3vmGRvepZVJ8ELf7uPxENLVFomXf9FZfFQHOnOEvWpFEp76zouIjiX4tCDZ3nxW7uwul0yp1T0R1fD4HHvHMrIZq79VhwxGw0xULLAT3vcNDLFxGeHkR5eJx2xuDk3w9PzI+iqx4HCFK5QmGrkuLzYxo9sOc3XP3M7rXbB4L45xs93I4kQEetWdVAF6rpGYhLKowEP33qaqUaW8dU8pu7iHs7SGHIY6F9lqZxEfyFB45YWd2y+hozg2WvDZNOh+vod3eN84/A+SLmYUQfHUdE0H9+XeNu248y0shxf6mVn2yIRxeWpS1vp7AhlClwhU3UinD07wDvufJ4vXN2HLAf0ZsrENZv5eopq06Q/W6Jqm5z6uU/TurZwY4XVrl4x8FM3Xli98lv/Xlj9F4eMIGY4KBubRFP1QkShJLBiKrrqoSkBpuqR0Zq0TB0j5pAxmqybUbriVYpqlE69TFusjioFtBk1EopFRyxNj14ksmQRiTTCQOVqdMUrxFQH3xRIHrTrVaJxm3S0RdZokjfq5CJx4qpNVm+S1+oArJgJdNnDCVRiMYuI4pIxmqhyQN3VyZgtXF9B9kJ1ebGhgKZKAYoW4ANCC0jGLIpqlKzawDOl69iFvFpD9iQM2UOTfLAVsmoDXfVIahayD5IboiTV5TLZqEdnpMp8JUUi2qLopEJbh5jEQGydTq0cArLckCCYjFgoVRXFieMGMkICM+JcN1EvmwJD8UIwmh0mt9+TR5RHNuNfHcdz96H6EEQEgRHibgAUG+qWwXB2jU69jKF6GKpHt1GmGegUnRipuEWnHnbRpIAwA4mEGVcyalMXEpIc4DUVpEBGqcl0GWXKbgTfl0hFLJZ1IJAoROrMrmaIWALfVuiLFNEknyPRftKRMOh3G2WEEZBMtoibNsvF5MbfBD16kbpvkI01aTeq5LQGoqVQiDTIG3UCIRFXHc4mPNq0KqlYC9eX2ZRYx98AnK2X4wzEi6xpMc4pLxP48QO45r9itzOa4tO0ddqiNWxXZTi1ypb0Mpan0pGs0Z8q0bB19hemaNOqRBSH7R1LNFyDVtNge3KRN/ac5Hyjm12peV5duMiu6Cwny730RMvMOVmu/rTGbblr7Mou0J8qMhRb5fxaJ1KbhR8PWLBTdKcqbEqsk9Ub9BnrDMTWmaun2R5dwJBdonLIDk6qNh1mlX0dc7R8DSdQ6TbL1C2DO3LXcAKF6E1rtCaSrNTjIdCsleDtO45y+8g1Du4e4+HeC9xz27nQy/XHlli3YkxVssiSoHP3ElONHEdWBzCWVRadFHsKCxSdKJG3LyK9bZXcUyZXfiPNrvQ8/ZF1rBNZtqZXMNdBH6li3L/K7tgsx6qDvFQcpLzbpVKOMppd5NqHIlQPtNCfSDK4Y4HRwhLDmVD0J71zjYulDuy8T/czULnNYmk2y5Wr3Vz7rThjn9rH8I+dJHsh9CzW0jaJcwbz9RS1B+v443H2Jme51mxn/aUO5l/o4Vy9mxfXN/HSC9voTFQ5UhnEyocz6MpTm0EVGCmLzmQVM+LQnatgdjaoDAvaTgSk1CYvHd7K5t9xWKvHkHZVyZ5QeUPhJD35Mq12iW1/UGPQWKVPX2MgW2SpmuCuwhh5tQay4M2bTmKqHj1/q1H4TISOT5tctTr42thOHu48z6VqB/uiU2z7WJl3dB5mrFbgucUhRqJLRJMWZ+s97CvMUalFuTd1kcfP7uDBwgX6/lxhJLrEPdnLRBXnhp/574F5/zWEmv81xys2E3F8hepigquBjD0XZyqdRZUClhYzyHrISWnNxzmT6iZIS1yqdDBfSuF5MsxEONbez6bEGi1fY66ZZtVJ0B9Zp2RHWWvF2ZZZQl3VOVfrZrySp9yIhJqfLQNlMoJZk1jelmRiOU8pFSFjhqvYuVIXy+UEF/NdOBtY8Im1HA1XR5UDyq0Iw5lVxtdytDyN2nySM4UeBhJFDpcG0PvrbMmvUHcNuqNlFuwUi80ULVfD8lUarsFgJMXsSoaHtlykGjVYcZM4vkJctQlMifkul6ITZX7D1W+pnCDwZfR2CWnJ5EpvO0nNwosLLpfbiC77LCzHaEZMTrX1U3FNylaEyKxG76tCRTJ/OYLkQb0fWqUUpWYEdWO7uDqfZveWGYqTCs02gTpjEgyGn4eYjaL6UHn7AVJ/+xKt/CFa9SjxhYCduQUeX9hBZKjGuVo3npDxjRBZu27HqDsGQhVcXS4w0r5K7rzAysi02gSRcR03pTEuJNylKI20ibymk74i4cYkZuwcQoHS7jSNlVCLVTckzjZ7yZoNnCsBKweyXG11ADC5nqVZjHK+rQsrphGZ1Dm9tYeZlSwdhoydktEaAUUnhhBwsd5FsRXlnNVDcV+W860eVpoJWo7GpUYnrakEpbYoK80E8kyEU9v7QUhcbHZR3GpwptZLQrVoepdf3oP/A5iJvGKDiOspKE2ZRimC1pJYqiRCMFdDIfAkWp4MCtQcg7FaGyu1OJ4n4zZ0tACujXfQ6tPIRkLE4JVyGzUvbMMunO1gy50reEmfNSvO4lgB2ZK50NTpKFSIf11CyBLegzJiwaSq+rQcjYjqUmxEceo6M40MTqAiSwK7pbGmxK5vP2ZqGVo1k6LuIrSAshNhU3ydfLJBRHPJ643rRLOaa1KxTBqWTk+izHJTZ6zZRtBUMWSXiCJztdHGQKrIXD2N5am0d5eYqWWZXclQSkaQJJCVACcj8BM+Fy73IkU8VB9WzraTSkLyikSjW+HUejcJ3UYQmnM/1HGejz3+IEIVRBYVGv0ewVQc39/gLwHpaYnl3jhdT1cYf2uS1BjXuzCyFG5hVg4IWvlDdHz0RTiwi3pflC6jQqGjwu78As9cGyGfqaHWw0LufDWJ6ysEaRfjcIL1Bxp4hoSTBjcVkJiWsXwJbSaOYgskzyC24lHepFHZ73B4ZRChCtb2Ssgxl/wLJvVemGlluT9/kU+YQ5RGBc8sDaPJAcmohWOHCOPvLA8x+Nl5KvdEkKYjrI+GIteyq9DwdAJfCQWjFJ+L9S6W7/AZa7TRdDQaNZNnJ4dJjsuUdkcx1ZAAerHaAUF4P6t3WByeGeDW3mkc52VMwR9QxOordjsDoVUCUvivJG2Qw9Tvf8pCC5mTwQaTTAQykhIiNrWiSqVl4vgKLS/U/ag6IabBXJdo+Rpy3MX2VczlsHXJmhGmlL6g1W4SCAm99P2PMEBCwHXiWLCxB5bk7xO4rjueySFylY1rNGSXiOZSMOsk1RaG7BFXbDwhX7fDsDyNQEi4G9iQuGqT1loEImSHQrjNa4vVw8/CD2srihKg6x5uxguDR1WBioYXD9BqEs0OKewGebBWiWP7KoGQcFISXVoJtSEhNIEfASUVEvaUpoTSCjk9sivQ5ACrK4qXd/F16ftdGDmsgag5i1aHgAO74KWzNNplUmqTqObSY5bwaxqOp4Ss7KjA9cO/UVYD4vMBjqfipCW82IYGqw9qEyKrAdFVn/iih+SDG4dopkXd1hGqIMi7KGqAJAR2NsDyVbJqnUanBHmbYjXGajWOEBKyEtD0dGqWgZ+OU7Ii6GUJOxvgpL7/+0iCmmcQCImFRopIrkXVNVFkgXBlnJpOZD24fq/8SMBiI6yrrNpxsuk6djFC0Y69fNuIH8DuzCs3iAgJrSyDK6GXZRxHwXFU1IoSMkJ9CbWoIkmClqdh2VrYTaipGOsyyuY6bYk6K/U4MdUhYTjoss9SOUFjxGG1FQcBmuzjbm/S3G6RHS6yUo6zfGsCOyWzXI9jjVgEvoxtq7Q8jSCQkdZ1nECh7hhUbTOcILaK46roqk+pEUHUVRxPRS6Fq9+FSifFZoTjM30cWR9gopbjdKkHWRI4noKiBBTMOqbqUXcNtITDiVIfR1YHqLomy1aC1WqcpdUU56e7WKvHULSAesMkorsYqk/umIqkCIIuC629RfasTKvHJXPVx84K3JxHX77EWj3GWimBm4AnSqM4HS5S1MNJBURORfCzHlaHh93uYXV6VDfB4nqKRptK6lQIJFMXDJiK4ac9MH3MYzHypwX1vuh1798XS5tpODqPL24lWmjQaBm0Hw/InZZoNkxqxRjKrMnyrVCpRfBMCFRB4qpGo0eiNhiwtldidY/K4iGV9R0aXlzgXEsS0V1ikyqZF3W8okllSKbtGJiKx4VWT4g+PRKhM1OlJ1tGlQOcoknOaJAwbSrbEgykijQGPNqPQNsx6HhRsNJMwIJJt1lmrRqjL16CM0nSepNiOYZcUUnkG6ztlliuJkhoNuaKynB6FWMpdAMsn8vTO7hKX6yEvCFLcaNDCm789W81XrEt3uhwp/g/v7SfJTtJWmvxxOwWJEnwQM9lDNljppVlT2KWsVYbVyrtDCdX6TAq1HyTlNKiXavwt3P7GUgUyet1rtTaabg6A/Hi9d87sdZLb6LMgfQEvVqRhNziO/WtuEJh0UqiSQFvyB/nicooa3YcWQpoN2os2wm8QLmOIegxy9R9A08oZLUGn3npID9y6zFcoTAcWeZ0rY+rv7ODxdsUcmcF8RmLep+J7AqW90sEpkBoAe+//XH+69MPMrhtkeVqgvY/NhGKhPnBBcaf78dNh9Ty+JRC+2tmWW9EKRfjKLqPJAe4dZ2RwSVSRoveSIkvnbiJu3ddZqxcQFN8Vqpx3rvtWc42eq9nO889s4ub77jMQiNFW7TGiePDdGxdoTtewQvkEF8jCY4dHeGeQ+d4+uoID2y9xD++uAfZkdhzYAyA+XqKXbkFuowKKbXJi6XNVG5fZ+rvdqGcjXPwtWdxhcyZ5W4EcFPHHA1PZ1tiiYza4FythzvSVymoVb5Z2sO9qYvEZJvDjSE6tTLLboplJ8mu2CzfWt1JzmggI5iqZ3mw4wKX6508OzbMcPcKY/NtvG77Gb41sYNHh84RlUO3w0m7gCb5LNhp2vUqdd9ga2SR5yvDG52VUE9m3Y5yf/4iF5tdbIku8dmZW1AkwebUGrYfChbtT0zw2PpODqbHCYSMJnmcqvfRZVS4VO/gjYXjXLE6+eu3Ps21c80bSkcinb1i04/feIv34n/59xbvvzhUOeBMuQdPyEz5OVKRECdyrVFgaSN1XLYS1B2DQ4UJql6EL07uYV/7HCcbvdyeH0eVA+5KX+HzC7eQNlpsSy6RVRtU/Ahboksc+ezNWO9qMNEqcKnRyVB0hUUrxenVLmxX4y1DJ/jM8kGKdpTOaJWs1mCykaPNrOMLiS4jbEsu2CkAGp7B8ZVe7t5zibIbxQ4UHquNhijYd7TIfjXG0qtd9GiAsx7QP7RCcLkdYQTgSnx87BB6Seb+9kt89uuvpvQf1lGVgEcKl6mc62PvL59GlX0eq93C/e0X+ebCTm4fneDoR29CSBLlRxpU/7KHgV88Q4dRobsv9DyZn8yjJF3kGRNrq8atiQlWvQR/8vy9qDKosk/97zqZ2dtOfKDCSjFJLtIkqjqoUsDR6X7uu+0Mz4yHXJjHTuxi044FYprDxGeHUWyoPVjn8YUdFDoqRDWXhqNT/btuBt58lmsfOUDeqDNorGJtyAXuT07QDAzm7TTrTpxDqWt8+OgjmFGHu/uv8UfTr8IPZLakVzhX6UKVA8ZLOR6b34Xcknn/A99kyspzqtnNc2sjXJ7vQJLDTCR6NsKXajejph02myvoksdHLt/LpkyRohXlx/sO89n5Wyk2org9CpdLbZiqh+srvLP/RZbdFH8zfYA7O8bDoDOXZcumRfoixVDzBfj0/AHe0HmKv5y4jWykyf7cFB1GlWOlfibWctyTjXO62kPNN2/8of9X3qZIkpQG/pJQ41gAPymEOPxyz/OKDSIx1eHO3BiG7LLmJrA3ipidepkgI4d2k0BKaZJQWigI+iNrZJUGfZEsDyTOkddqaJLPqwpX6dGLaJLHgpvhZnOCJS/Nyt0u72o7R7dWpBEYdKgVplt57ukeY76Vpk2r0patokseMgE5tU4tHmHBzTBkLF2/1n4jTlS2CYTMUirFmVovt6XGiMo2006eglrjw5dez8p+Qf47OtXNOooBM4tZ+rYuh6pkwF2FMb6q7mLQWCHywArL01kkV6J/eI3VfRJLVgLL18IW5xtaHCpMUPaiFF/TwvdkWIyy9lqL25JjdGsl/rx4B+8eeJ7vJIbY1z/DQi7F3dErnLVDs+6R4QWC3yxw12uvcuT+AQZzZWZOdrP/9kvsT00CodVnVm9wpdLOaPcCFyaG2HHHNS6vtBMEErGH16lbBsF4nMhQjd35BXrMEo8vbkU5G+faRw4w9P6XuPPaZWp+hMlyDoBo3qYZGIzV23h752GagcG+wRksX+NKpY3bChOYskufvsaylyKltOiPtnMh3snY0X62Ggs8tbaV0lyKhw9dpDta5vFTo7yx4zi/q2+m0F+icjrPTQenUBDc3XMNQ/Zoy1XZYcyzLztLW0eVrNLgW5XtSBu5QkJpcaI+wOt6zuILmQORCYwFjTfedgJLaKy5Ce6KX0Zr89Elj9f0XECRAu6LX+DjK3fxQOEiHz31EAdunoAcnFVehhQA/+qF1T8E/lEI8SOSJOmEVpove7xitzNmV69o/92fR10Naf3aariCuelQbEiuqJj9NVoNA33KwIuANlDHXomiZi0iR2NIPlSHfFL9FcSTWSQfyrfYZI7q1O5ocfumcZ47sZ3ENYXqiAdGwJaBRcbO9IYyi1qAmbSxViPoRQUvKgiiQWhUJYewbgA/5qM0FIQqULuaiPEYXkyAIoj3VGnUTPo6iiR0m3azRsU1qdgRVhsxavUIXkMDX+KO3Zd5aXoAaTyGPFKnI11FlQOmj/Zw1z1neea7O0GCR+4+zmPfugW9JhGo0P/qKeKazWwtzebUOsu/MojSdLjy8yYDn5eo/B81qleyBGZAfFKhtsVFMn1iSYvP7P0E7/upn8f84AJzlVRobXGpjc4XRChbKMHqbiXUA4n77B2a5vx8F27FAD1gU98KhUidvclZztW6eWlyEL+mES00ONA9Td6oc2fiMv9taCsz//EQ9mYLSRGokyZeTHDnoQuc+PudNHoC9t96he2JRY6WBliqJ1DkAOsb7VS2+8THFeycYP89FziUGudvP/QI3rvWuavzGs8tDrF6JU+kv0ZrJsGXX/dRfn36dfRFS1z61VGsnMb2//McJ5Z7yMWaOB/tZOZRQTTbpLkW5TX7zhIg4QuJx0/uRIp43LVljKPzfbhjSd772m/xsa8+jJv2kW2ZrR+Z49LvtEFZ5+abxhhbL1C7lsYYqBEzHX5t5DE+8MV3kNixzsWf+RT27OyNbWc6esXmd9z4dubC7//z2xlJklLAaWCT+N8MAq/YwqpQQSmGgUOuqLjtDm67g74e8iWEKmiuxELUpC1hrkv4vkRkXsEtG9T7Aio7XYY+Z1GpRKkPBLTaofPbGrV+iD8f5ebkFHgS1W0uic4aXV1FrlzuRu9pYGRbtD2n0Z8rIvkSsht2KiRbJjkuI/kQn5GIzUnEplW0moRiSwS+jJv1iSyFxtS1pQSSDIVInW3JJVq+hi77bE6uoas++wemGBhYYWh4ES9QeNWmMbSaxKHeSQzFQ0YQXZDYFlvEjweoDQmFgPw5QXNnCzsb8FDbBR7On6Np6+xLzjD+Rp3Z+1NIDZWZh1RysSbR4TIjO+Zodgf0fw3av61zc+csrpCZfljlvsJl7uyeoGoZDO2aY+FumL8X5u4XKKMVhBHQ3lHGEwr7+maRIh64EjfnZjiQnmDWyuIJmXymRqargueFMgyDxio1P8LMfzxE34dfhLKOOm0y9OczjPx1mZNLPdS2OYz8dYUt8WX69DUuzXfQEa/RmyhT3umhZi1qOxy8foudiVC9343KLE+FCv3LM1nSQ8XQnyjuYUqhHUTRibJ4yGBtp8S6HaUyngnb6W0KkUyLZNRCjnms2HGWWgkWWyn6vgHZ542we3ckFQpyezG8HpueJyXaX4Kpt/eRydbZ8okK61aM8mqcIOfiXEuGejV2O16nHarH6S+vAvoywWZ5SZKO/5PXT/2TUw0Cq8AnJUk6JUnSX0qSFPtfmYuv2O0MkkD2wU356EUFNx/CrAMVpJiHcGWklkJHoUI1btOoRMjELMr5KGpFQR5skE02GHtXhr0D00xnMiiyYLEjDYC3pcHfzd5M37Yl5tbS2LZGzHDQshZcSKDYsP5Ai2YlhZq3sOIq2XyNlq3TqiTo2L5CuT+k+rfm45D0kHWfQqbGUjWH1R6gdDShbNLfvs6F5Q5WEgmmJ9rQMxZR06FajdARq7HeiCKExC3ZaY4V+9EPrXN6tYtGy8C1VZL3FTlaGUBuykiexJfO7IN7fSRfRgKeWNuGKvkYX07ztbftQuiC5qDL5s/5TLxOZ+HJXqz2gLGcSceOFabjOfADls5tBUJB6y/O7WF5PUXmKZPxe03wQ0lKyZNoLcRRGzKVxTZqThvNzQ7pUzqyI3i6ewRD9Vh/qQPfEKj1sE3bfjzgzI91Y/kak+Uc9maLsT88wPB/eAlp7w6uvK83zOQugmwKrv5Eiqtn9xON20SORznX6EVL2GglBd8KsUKKrfOX2m14joL8UIv0C1HGJoZhm0NpIkvbMRh6zxTfqu/gSydvIj6mIR+oYCg+uuITxH1ubpvlqfsj6EeTbH3DNEsLGS5/eUsoviyg/tYWkdMquxLzHB0d4OZN03zq6TsZ3TfF+Xv6ic3IWNtbNJdSFH9G4p74LFO1Lm659SoXz27ltvsm+JPn76Wjf52b8nNMyP8/hb3/s168hHN/H/DeDSOrPwR+FfjQy7ugV3AmgkS48jsySlNCVgNkRaA2JURTBVdGrclUWyaOoyKaCvWmiVaXwvcEGxiKsspiI4nnK3i+jFZUQYJ6OUJEdam0zOucHEUSuEUzVAbTICjpSJLAtVSwFZqWgdXU0eoSdcug1dj4viYjWgp+S6XaNEELMwa3paFWFaqWia76If+EUMnc1F0kGaKqQ8wIX4t2ioRm0bJ1IlpIMJQUQaMVqpsJQ4SrkBIgN5XrotFx1Sau2fjmhkK96YMqcGNqKHIdEwQRH90Mi4dqRUVfV1CiHr6QEEZAXHNCtXwNIlE79MWJBAQRH2EGBEbI6A10UCMedgq8iIS+wYX5ngC5bwoCQ+DE/r/5q5ISSgtIe3cgTl1Aq8sYJQnFkhC6QK/IKGqAKgfote//pm8IhCzwjRD/o+sesiIIfAmtEco4KiUVYfq4MYmGGzJrJd0PF5yNJXupkURdV1lopgh8GV+HimOCCNnhgRaKXvsVjciqYMFOI63p1F0DoQgcX0Evyqh1kDZmlVZWmG+mQiyOp6M1BGt2HIIQN7Rix7muhn0jj/zLyEJuoHYyB8wJIY5sfP8PhEHlZY9XbhABnFRo72Dng+sT3Y98/xN00z62rRJMxTBWQ8yI5EtYnR5iNsryYpqhz9dZWUtSm09Su5Sl/WiAvqKSfUnn0c4zlOeTuCWDYIMGLtsy0mADf3udwhGZhGmDraBWFJzZGKKpIrtQXY5jXjExrkRQmhJaWUGuqXiecp0GL1U1vKRPvWWwLb/MLblpDu4cY3/PNDfl5+jIhYS/jNmiLVbH9lW6IlWCq3G2ZZbZnF9npHMF9Xycm1NTCCNUnX909CyZiyF4igBuy1zjnsxlnAcrPNR+AdFUiF7TmX1AQnIluvYvkOms0pctUbqQZ+CbFu0nfA5tGufnO55CcmTua7/EXT3j142lrg8pFFUOUh6ZHWsM3jXF3r5ZrC0W1e0uBwpTvKbjPAcfOMe2g5PE96wT3Vxh/UGLmzrmuCd7mfcOPY06aTL0pzNce1uCmd88RN9vvkjvN9fwtzUQqmDTJ2d5/dYzvHv4BdYPuGwaXGaofS2sTaVdgoSP32Px5s0ned+ep0i9YFLaDp2vmcGPBuS7KzTvqzO1lOPRxBke2XGOLQ+O4Z1K0zqdJWlYCAUW6kniL0bxRuss1FMgJHoemKb94Vmyr5mn/+sC1RIsWQni0zKXz/fyxjuOML5UIHtRkB530C9FiOWabP58GdtXsTs9zl3roVWQuLjWzo/d9gLLcxkuLHfg+///AZsJIZaAWUmStmwcuhe4+PIuJhyv3O0MoDZCroXscd1KQKtLeLFQT0RpyfhxJbRqsUN9kUgD1KqCHw9QIh6BFuqNCDXANyW0ho8XC5fN+kb7TYr6mKZLJtqi1UojRKjhoTiCqOYiuaF0npAlJEdCq4rQf8QPV1+1BYEuIZTQQ1hSA9Qm4f9jK3hJhQAJO1CpuiZxbGKKgy9CZq4sCWRCD1pV9q9biniBjO2Ht9AVoRex5EPFDXku8obdhCsUHKFitfSwpWgGOEmBbMn4SY+Wq+EHocaKl/QJVBmlFeAEKkU/jtAD3EDFFTKtlo6cFaBu2D9owXWbCNdTqNpm6CUTSOBLuEKhGegst8J2+/eQqL4dQsibgUEzMMJsKJtAdiTUBijbR/AvXsX394LhI0yDmmfS1HSwZfwgpDWgCmRF4KsBIpBo+jq+kHHjEpInQs2PqkIgwLFVJAkqQXiukh3FSQcIVYTuiDUJP5BwEhAEcmjdYfjUHAM/CHVqYroUevYAqiUQMZ8VO4Gq+URWXbyogpsQROQAyfFwfAW5pqB0tgAdVQlYspOocRdVCa53fW50/Ct3Z94LfGajMzPBhnf2yx2v2CCSiTWRd1QZbV/iwlInP7v9uygIPhk/QN5wQt3S6T7esPUMbXurnKn2UrSjFDNRmsUE8bjF/s4Znn//JjZnV9iXnSUqO/x1/BCRVJ1WL8zbae7Zd5GX5vupzyepJyJoAaha6FtSfbPDyulubj90kbjq0G+uc7XRzunBLn5p82HGWu1oks/zy5vYnV4lrjoUnShDsVWe79jMcGqVZ66N8JOjh1E2/FcuiE6GYqskFItcV52DsWssJVJYgcax6iC3xCfpfrTEmptgMLJG3TfQ3jhOVHZ4aPQCM4MZ5hppNr/7CjelprnW14YpuaG5dUtlwU5xz/bLLG9KcGGsh5u2TnJ6ppcDg5MEQmLT0BITP9qGZCnEXIOnqtu5e9dl5uwMEApB3ZYdJzfauA7AcgKFRIfNS/P9VCpp8v0NOjtKWK7KVCNH0Ylx9Xh/CENPu8hqgLakse2mJebtNGP1Nu48dIGTm3rgIngRuPahCL6/l6G3n2Luizu4/GtJrlzeRuCMsuUTTSZ/qJNAg/zoOqVLOdIjJVQl4Etjuwl8mbveeIbJX9nCYi2P1+/RHq9TXEny3v1Pc9bu5cRSD/XpFIcOXKbuGszXUtgFn93ZNVKvn+Xxszvo7y+xdryd+omOcIsQwNo7qihHkrQDpZ0BOzfP8ezpbbz39if56/fuxz6XZv8dl3jphW1M/06VLq1KkHHZ1rlC7TO9pF5d48nnd2P0hYjpqy83KPwrBhEhxGngfxuM9opt8RqDPaLjQ+8FRYArY2asEOJeNpH0UGtVlgX5TA0/kKnUI2iaT7NqIgS0PaOzcruHkbbIJxssrKTRNoSMuv9Qo/V/VYhpTgiL/1SKVk7GzkikX7WE9Q/t2FkJ7fZ1rGM5xK4avi9RSNeptkzqpSj9PWs0nLBmsl6Mo2g+muajKiFXplk1iaVauK5Cb67MjvQiFTdCSmuRUlusO3FcIbNqxVlsJGnYOqOFJRYaKbxAZr0W49GhcwRC4sWVQV7VMcZXJ3cSBBKvHrjC45NbaRUjSIbP1r4ldNlnbC1PW7JO+cvdKK5g/VaP9ucUll/loa1oeNGwpsRgA0kCp2jy5Qc/xrt+732I1xQprcfJ5msU59OkLqhIPiiOwMpLKAdKeEcyJO9cZuVSAZF3UDdEi1Jxi85ElavLBYzDCeLzAcu3ws8/9I9ca7ZzZ+oyv/Pnb6O2zUHeUOsXqgDDJ5q06PnhC1z95E3gS5gpG9dVUMeiBLpg6FNrVEazJK9UWL01Q/EOm7u2jPHc8e0IPaC3f41yy8Q/kqG1zaKnvcQfbfkcr3vu5xjsXmP+hR6ELNh8+zSz5TT9mRKXTveTuSDhP1KCb2ex76mGRftARtM8astx9o+Oc2m1nVolwoGhScZKBaoNk2AqhrkqoTaheVed4fZVriy041kqUkuhZ2iFXdkFvvP5m4jdt8y5H/sMzeUba/FG23vF8JtvvMV79mP/jlj9l4ckkOsqgREgeRK6Fj6wTknDz7ihBKElU9ai2Iuhn4m3uY42q+MUfNb2CCTTZ9Ov1bn0aznwJNySTt8/2Ey91iT/KYM3fegr/M4zj1J+uu0EUwAAeN5JREFUIEBPtYiaDqvH2jEeKSMCmejfpIm8Z4mF+SxSQ2FxPYLQA2LjGjNyDmPKQMgC3ZNwUwHNaICRaxEEEtqcTqOhIiTBohowEC8yHF3hYr2Tlq+R0xocWR9gT2YOUwm3NJavcqgwwd89c4jX3nWcyUYOL5BZO9ZOzw8fodXSEUsmykCA9nyS2H1rrC0neVPncaKyzX8uP8Qbuk7zX/fnQm6LJ7F8j8e2wQXqPQZt0Rqnjw4x8mELLxNl5KNnaAiN4gGHXxp6kcW+NF+bGuXuPZf4TnwoZJUqgkKmxlopQferFkgaFqO3LfHc5GY8S+Mtu4/RqVc4UhlkpH2V9QcaIWeoFuFcrYdDqWs0A4NGT8DWjzW4+hMpjDWFTZ+cRZgGl38tydVP3sTIO09w06mAbqPE73/nIfpvm8NQPC53diKpDktE0I06795ylIRicXxqJ3ZWRtvk05hM0XfPPGv1GIvrKRQErx89zWQjx6whEIAbKDgXUizs8oksyrQerBBTfVb3uoxkyjiBgh/ImB9OkclK1LcY6F9Nw96A7kiZUws9FP4+gl52mL1Px9xSYfMvtyj9UQSvroEsiM4qlLoixNpsaltd7GoMP/YyujM/oCzeV2wmktnaJvK//V46M1VmVzLctnkcRRKcXu7GcjRyiQZL6yluHpjmnuxlTtT6eWFuE4VEnanpAtn2KnsKC2yNL3Ki0k+bEQrnXmp08Z35Tfzk8GH+9EsP8eoHT/LU1AhW0STZHqagS9/uBQGv/dHn+btnDrFl7wy9sRLbYou8VN5E2Y7wYPuF69f62NIoXbEKMoKLpXZe032B764O0RMr893JzTw4fJHhyAqfnbmZ/mSJW1NTzNtpdsbmONfoYd5KU7Ki120o39X9PL/07Jv5rTu/TDMwyKp1/nr+Nm7PjeMKhcfmt/OmvpN8Z32Y0dQCnzt3c1j38STMWZ2+O2fImE1Oz3eTiNo4T+WpDQQII+CePRfpiZRYsNIc/9Ru7vrJoxxb7aP4UgeSB2J3DddViEXt6/aQxatZ3n7vd/nGn9xJeasgPiWTfWSemOYw/Y1QStLKC3LnxQadX8Iz4QPv+Ac+fPQR9g3OoMs+W+LLfOrsfhQ14PVbz1DzTL59eRuBrfDWm45yYq+Me99NTL5Vou1ZDTcOpVtczBkdO++jlWSGPrHA5Nu7+fE3PcFfnT8EU1HkzXWk8wnsgs+773qWBTvNxQ+OMv2wxptf9SI+Ml8fH0VRArYVlrkpNcNnP/FqOl87zZVrXcQmNHwTJA9+6q3f4rGlURK6hal47EzM81dfv4+feORpJlt5FltJOiNVXvribt789qd5fHEbs3M5fmjPaZ6eHWZ/5wzPPreLgZvmGIgX+cxbnsAavzHfmWhbrxh5041nImf++N8zkX9xxBSbzW1rbEsuIUmC/alJFAJsPyxOboqvcRLYm5xljzkNwGwmw1BilaplUIg12BxdpV9fY9bIMhqbZ0RfouJFycWadKgVnILPUHSZE/EelqoGsiTQZJ9Grw8CBsw1pAD2ZWYZNFYZNpawA401N84tkQlcoSJLAbPpLJsiqyEISshsMxeoZU36jHXGsgVujk+SVCz6kyXyeoMefR1ZChjQ1iAGHUaFlUiS2VaG3kiJncYimY4qfVqRhtAZUEt0RqpsMRexhEbC2MyoOUstbTJkLtOWr+J4Co2Wgd2msCW1TFK1mEum6UuUOF7IEemphfJ/kRKdWhlfyNQGBBU3wtb0Ck+35UERtMdaJAybnBlqngRC4mSPwYi5RK0fgrhPfQB2x0skVZvL7QPXGaVWRsZJExZRVUFBrWJGHSxfY09qjj59jWjcRpUD+ox1mppO4Ixipmy6jRIv3Xc/2pMn4K034284LSSyDVprGmrOwolqNEcKyC5sMReRlYBAgkyiyVK/gWz4jEZmGW8WKA/rCCNgs7mCJTRU1ScbbZE3GmwxF2m1C7JGE9RQJEnIAlSJXq1IymghS4Ks3qBdqyAJ2B6ZR5M97EBhW2yR56O7GTRWiWqbSefrtGk1NMUnptqoTYmEZhFRHGT55S3iP4g2mq/YTCS7rSASv/gBCr0lVmcz/NAtJwH46tndRBI2mXiTxbECB/ZdJas3OVvswvJUiuU4YsXA7Kuxt3Oew+ODdLeVGU6tElNtvvHSPtR8C4TEgYFJ6q7BpeUOrJKJmbEQl+I4/TbC2cBR1DQGNi9jKB6bEuuMV/OMzbbzw7tOMtnIIUuCk9N99BRKRDWHlqcxkCjywtQgg4UiYxe7efDAGSKygyoHPLMwzO78AmmtSdmNcnf6MnNOFivQuFJv52B6gnGrQMsPFcRrnokqBQxFVzhc3MRsNYUiC0zVY09ujqlGjgOZSRQp4OOP38vNB65ieRplO8L8yU56blpgarydoeFFDMWj4epMjbej1GU6RlfYl5/lWq1A1mhi+SonTw7x6O3HuVTuuH4vnEDBUDyuzrZDXaNveJmlcgLPVRnsWCOmOlx5ajOBJnBTYcaTuKpx+1vCe3al0kZMc7g030HkeBS9Jlg/4IIts+UTTa69X8Wv6qCFM2jkXceZ/vBBvJhA62vgX4vDQMhBEULCd2X2DM5i/WyW5duzVDdBbucqa5fyvOmeF+k2Snz09L3IExF23HkNJ1CZLmWwL6XYftsEuuJx4ugwN906xqkXRkhfJtSpDQTrD1skvhNh5B1XOHZkhL23XOPU8SHefs93+fSpA0SuGAw+MMmVIwP4nTY97SUWznawa/81Sr/bT+7Xpzj73WFiO0p0pyo8/c4vvqxMZOsP33gmcurP/j0T+ReHEBKS6YfCPmpAwzOQpQDVCKX7HdNB6KFq2JZ46KC2sN5OJGrT0HRacwmmYln2DswyXcmy2EoylFilc2iVpUtt7L35Gkdn+xnIF/HG46RnJeq9KsEmi/ipCL4O/ffPs/7JftwBhZzZIKFaeEImlw+JfT3RMgBj8RZRLdQrmalluK0wQTxqoyk+wgioezq3ZiZ4urSVOzrG2RmbY9VLsDM6y6RdYMVJsGInGC/lMRSPnfF5/vLyIX5z19exAo05J8eaG2d3eo6B+DprdpykZjHXTNMXLfHE8lZkSaD2Njg+1U8sFrquqZvrzK1mUBoy12bakCSIJCw2DS1RtUwWL7Wx+8EXeXJqC5ebHQhPpjC0zncXNlFvmihKQBBIOKtRfvS2F5l7vB97V5O5cx1s3jNHRHU5e3oQEfFhwCEyrpOYlpF8mUYP3Ju6yB9Nv4rbChM8Pr+Vbd1LnGv0ArCpfwU/kJn8oU7UMYn+2+ao/003vgHTHz5I/388jLxrK9felqHrBZ8VO0RsRxcFpYMOuuJz7sey+JEAoQvWLhQI2mzW3RgD5hrahSj21hYVJ4LtqdRLUSh4LDVCHZHMRYkL/R3EpyUaPWxACWT29s1y8o4+xop5ooNVKk4EkXOYa2Xo715jVsuy3ooy+JUm8f+ywMWlDoJIwGw1Q/FBlUY1g9vphN7NcgDWy4Bq/RuLDd3oeMWCzRQpQIuEamBq1COm2sQVm0jURlYDDNVDivjIksCU3RC7AMRNGxHzkL1Qp1WVA1qOdv28mhyERdkNRTA3UJB90BobnQsBbgLclECWBJFVD03xQ6tOyUeTfZKmhSb7GLJHVHaIGg5R1cFUQ6OilNokbjjosodk+CiSICFbGLJPTmuQVppEZYf0BsPTkD0iikvD0kMvFNlFlgVpuUlOrWPKYQs3pbTIa3WSmkVCDffsSTVMvWVJkEk0kSSBoXmoik863kTTPfyEj2r4IAtcRw1V5uUQkeojY2heyEHSffLRBqbmoesepu5i6B5CC8iqDZykCK0tdUHObFAw62EhUAIjbuOmwk6OmwjVwmKyHRYrZRdFDjAVN4SyJ2wMxSOqOSEyWA/V5N14iBz1YgJ511aCs5fxIwFeRMIzBW5M4BsSiu6jy15YtEx6oAUEZkA0YeMFCnXfxEsIYgkr9NUREjhyuBjZ4Wds5SRaDQPZF9iZADcdYGdCUKOme1RrUSRJUHd0zKiDHajENAdV877vQKB4uLaKiISAwsAMwi6h6SHZMrV65OVvT34Alc3+p5mIJEmfAB4BVoQQoxvHfhN4DyGBB+DXhBDf2vjZB4F3AT7wC0KIb28cf5CQeqwAfymE+L2N44PA54EccAJ4hxDifyqBXWtEiK9GmFuJgICvtvaABFJDQUR85qt55JjLbDnNX68ewG5qGFGXpalciLKcklmNp6k2TLqzFS5NdHHFaCcSdeh+VlDaHiUXb7JWj5GYhEaXhBcXDHevUPlWL1ZOZqGapHqfgljKMmUXSOQbWC0db92k1Ixgu+HH2yxGWVAzyFpAJOrwpyfuQjRVltJJVN3ncqkNN9hH09PwRBtzdoayE8Hyt1JxIiyWkzi2xl2bxzi92sXxpV6sls7n1g7gC4kzy928qneMPzp5Nwi4a8sY/3hpO/KywUuRgL6RZVTFY7WYpD1Xwf90G2gSK3sEvd/2mX6thHEmip0N4eMzaiYMGk2ZPeY00b9N4f1ojfpinEkjhzcRp3BqA1LuBCgdCn8cvYv4skSwXUKty7x0egRh+GjtLZJRm85klXERaqJGVgOaXRKHG0NsSa/Qp69hfaOdYztzIRfGEFwq9YAqyI+uU/hlweXOTsQtLolsA81Vufa2DP679zP8C0dYf89BRv5ohvLBHpYedTg4MMWLL26HnEtXR4maZSCdzlDRo1wwO3hv+1N8rO0e2kybxcNdBBrcdPsYV9cLjORWOfriVrLLgvhdVaqFNiIDoa9yEEhcWm3Hqhg8tPs8x1d6KVZj7O6e58JqB7V6BBYNanMJynfB1EwfewdmOTXdGyJTVcFw+yoDsSJPjO9jcOcacy9jGf+e2vsP2riR7cxfA38EfOq/O/5fhRC//08PSJK0HXgLsAPoAp6UJGlk48d/DLyaELN/TJKkrwkhLgL/ZeNcn5ck6c8IA9Cf/s8uSjG8kFqfdPDKOgN9q8iSYGKyHYSEnrFwSibRfJXN2TXGi3lq1QhS1IOqhuIIpIiPogQslFJEUhaZeJOVYhIrLeO3zFCnFFAcaDvpUt6scbW9nUwi7C7gK7S/JCi/xcFMu/QmK0ySpZGR2ZRZp+aE5xhr6cTjFrrqU6mbjA4scGG2k0S8RXExRardYji2wlijjZji0GsWiSlxoorDgpUmodmU7QgXih10JmokNYsTdi9bY0shIjSno0k+o/0LtLwwq+ppL1FJmiRM+3pXR9U8apZBfX/IbBYZh4U7DaQgoNnjgwyyJeG6SuguqAuWvBRLByUUW4WIT9S0qfXJLGuR0GfGk5Fd2Nm7wPjZzYgg9MaNd1cxVJ+1+RR1ITEbyLhLURRboNoBiqXSqZU5V+li2UtR2e6jpWx8K4KQBVIs5MCULuXQRwMk1cG4FqG1piH50PWCjxeRWH/PQXIfP8zajx3EykoosyZXswW0vgbiUpxFJUXQUjEKAskIiOkOM14GuaJSTkewOzzwJM7OdeNWdSbkALm3gXwmylopQd9Rh6lCIvTJCSSUvipSQ+XIUh/lcgwkmCznSJg2lqPh+Sa1YZ+tf1xi5laNi8sdBDWNQFMw5zUmMjnWW1GMksTESg7FvoEZ+E/HKzGICCG+I0nSwA2e74eAzwshbGBSkqRrwK0bP7smhJgAkCTp88APSZJ0CbgH+NGN9/wN8JvcQBDxbRWlIRO0TFRXYmq6EE6CmoIwBO5qBIyASiNCtWnSrJrIWoAo6RBI+IaEsBSIQTbRYGE+i+cqGKaL1jKJRyzKzQhBIOMbsLpHw8oJ+jqLMFeg2abiKT4rN8l45QjNIIrjqTTrBqKkMx7P43gKkgRBXaNqKUiqQDU8Ls53IMo6ZUDSA1bqcaaiOdasGLIUMG+nWbPjBEgsNxMslZM4lsru/jnGi3lUxcep60y08niBwmQ5S85ocHmxjSCQKfTXWVhPwYJJNeWT76ygqx6OpaFpPtmzMr4pUdV02o8FLNwhEZ+RsfICNxEQMVwwwJszSCtNCidgvUNGLmnUoyZMRmk7L0LNFA98HS5taidREjSDULy6tpSgZoREP0kOyMcbNNImkmegWCFpbtlNocoBKaVFfFyhZmpoLQnfgMBW8NWA9EiJ5KdhiQh23kfNhfWcFTuGZwpG/miGtR87SPpTh3EevIWprRId8Rprk1lEr0M23aCumyQOq6zlQ7JjWmkiNEFEd7FLKkIV9I6UmFdStMXrlKfTKLYgFrNYuiWD2h6akIlARlc96gmPLdlVrgBNy6AQqzNfSeE6KoEZ4kEW7svjeTV6c2WulSJoMRerB4YyFTqjFV4qtNGdqzCn3+DM2hjSD2Aj5H+nJvLzkiSdlSTpE5IkZTaOdQOz/+Q9cxvH/rnjOaAshPD+u+P/wyFJ0k99TxshqDXQyzJ6RcYoSsg1FammhmzPmoxWlpFrClZTx3UVsBWCZhh4jKJMvVcgR8J9PYSgKd0I6xulLTIxzaG8lACgNgCttgC6wgc4dmGJ7IU6iizwkj5SQ0FqyTiOStBSUZoyLVvD3/AAllsykqUgWgpuSyNw5Y0AqEJDpdEyKNlR5sppZusZlqwkq1acsh2hahlYNQNR1bF8jXrdxPMV1DWNdTvGuh0jCGSmajl8L8wgpmsZRBAyYBGwvpZgaTWFvKpTX45j5SQaXQKtKtEsyEQXZdJjHmpdQsnbxEwHXfVJTMCE04aTlPBXTRJTcpjdLUkEqoQXkbDSEslJC89RyFy1aK1GSU4FKA0Zqamirmt4qxGWKwnkNZ3YSnir9YrEspNkvJRjrNWOnRPIho9iSyiOhBzxQnEiJWD11kx4b0oybuv79SuhQPlgD1ZWwnnwFvR/PIZWUpgtpyHuIWsB1VoU11bJXmpgrqj4gcSY3UFkXqFYiuG1OdBphZ7IrhoW6pMuRslDkQOcbIDb1PEsDa+lEtNdFCNUmlNkQVemwmw5jSwJhAAR87AKAV4UNM3H9RWUikIuXUeJ+MRUh/lGGr/HIqY5CO1lBAXxgynU/L/anflT4LcJk6vfBv4A+Ml/rYv654YQ4i+AvwDIbC0I4+Yi7Yka0+tZDnbPosk+V8sF6lZoc7hai7OnY577shc51+jh6bkRereUuTjfQSFTY2tmhf3JCV6sbOaWwgxbI4vM2Dm+LW/l0fYzTH57kPtuvsITYgtOMUoyZpE2Wlx9+2YA3t53lL+6+ip23zxOX6zEcGSZ49UBGp7OXdmroaUl8GT3NtqMcDU7X+rkVe1XOVftosOs8Z35Tby69wpteo2JUpac2eCm1AwTrTw3JaY5UetnOZmkaEUJhER3ocwvbfo275t9Bw/lz9MMDPq7Vvm/xx/kXbteJEDiS1O7+eV9j/ONnl3sSc/xmSMHEIECWZf4RYPuh6Zpi9Q4udiLdrON+2Qbc/fKCDng1UNX2BJdZtFJ8bR6gOfKW4i/bgn3iU58A6KFBtV9JulsHV0N/77JoTxv3/VdvjZ6F7IV4MQlenctkNBt5j4/iBTIVOw46SsS5U0hSMyLC3bFZnlsfhcX4p3sv+cCOxPz/KV2G4bu8ebNJ2n6Ol8a241zh827txzlhQ9spTlSYO7HJaKLYRF16VEHZdZkaquEdtdBBj94mLkPHuIX3v4tPjF2kMZEisxwkdn78tgFn58YPMkTxe20H7OZ6jB4210vAPDVyZ0UslVyZoOHbn6aj195mK7YCqVMAmNWD61TA/jxQ4f5dmwHlq9yc9ssg5FVPn70ft78yDNUvAizrQxZvcmzX9nHz468wJcW9iB3t9jfNs0R+imYdc6dGGR4zyydkSqS9sq30fxfCiJCiOXvfS1J0seBb2x8Ow/0/pO39mwc4585vg6kJUlSN7KRf/r+f/nC5YD2RI3+eJGmq5PWWqiyT3e8QhCTSGjhZnM4tkJCtug2ymzLL5PQLBaTSd7Qe5qo7JBQLEZiK3RqZbq1EkUvzqN950nKLdyEYGtkEWOTx2xHhi3xZc5VuxB7athNjZTaZNPueTbH18hrdTrUCpujq0y3cnSolevX2mFW6TbKAAQZibpv0BWp0GsWKcTbGTDXeWZ9hGbD5ERxgEp/hJpt4AUKCc1CVzyyZpOB+DpT9Rym5BLrqfF8ZZiaa/CzXXOslOMs5ZN4QqF+Pos27NMXK+EKJaw1uAr6tQitNsHB3CSdepnvXhzhgT2XeNJvw+itI8uC+9MXWPfjRJUIpe2CZ09v40duPcYXe9sReoAykaR9dIXR7BKa7CMjOLxFZqaVpTzqkT+msLbfw67GKSpRaqMBSk2m/Ujo9VvZ7xDNtAiuJfnW6k7klszY0X4efe3X0CQfz1EIfJmM2sAXckim2zJGQrGYfHs3sgu+G1A66KDoPgcHpriaLdARrzFbTjP3wUP0/OcXGX7XEkEQqswld9o0dtUwLiTYYi7yVxcOYuw1yJ8UHHzNGAqCI8kBZARb4stsN+ZxdzbYn5sipbe4en4EzwmZ2gkldPbbnlzCDlTuiF7lya/dxugb5zjT7COltbgtOcbhWweIyjZDyTWqlsGrU+c5vtrHnsQMl14Y5S0PH8MKNL5ltG5ozn1v/CAWVm8IbLZRE/nGP+nOdAohFje+/kVgvxDiLZIk7QA+S1gH6QKeAoYJC8tXCTUL5oFjwI8KIS5IkvT3wBf/SWH1rBDiT/5n12R294r23/n5MF0uOCjrGggJP+6DESDVVIyuBlZdJ3NExypIsLuKMx1H6Ql1TtWmhNoC6+YGsedjxBd9lm+RUWwJNx7wngee4i+/eR+BLjD7a+hqSKCrNkxU1ce/kGTg9hmuXuzBXFaQPGh1+WgVGSkAczXEEDV6BYoV1g6kgQZMxTZQkGBsqtKsmmwbWGQgXkRGUHYjeELm6noBRRZUGyayLLilZ5rZeob657oovGOalhdKKZY+1ctD7/8On3nyDmRb4q2v+Q6P/dc7KW0LpRFe8/rDpNQWz60Oc1dhjC/8zT3EFgJWX2thno0y8OAkl+c7SMRbOEeyBDp4UcHug2P8l76v8Lo/+gA/9Lbvcq1RYN2KMVdMI51JIAUQ6GAPWlDTyA8W2Z1fYL6Z4spcOwAPbb1Il1EmpTaZsXMcXhmkbutEdJeR9Co3J6fYaizwGx94N25UpvhQi8CXSL1g4sYlDrzxDE8dGyU2pfD2H3+CLeYin1o8iK6EbdwXX9yO1tfAXo5C3OMXbnmKYWOJ/za0lcq3hnik5zx/P7EX11PIxJusnmznsbf9P/xF8XZM2eWp374dOyGz72dP89zUELlkg/Q7m8z9aQYBWJbG27Yd39BYlfny391Bq9vn0L4rvHB+GGNJ5UNv/gIf/oc3oTRD1bbOF33snyvSeryNwdePM13OUJ1MY/TU0VSfj+z8Au9+/F3E2huMv/8vsSfmbghsFsv3ih2v+cUbeSsAxz71f/5ggM0kSfoccDehXuMc8BvA3ZIk7SFMrqaAnwbYCApfIBQ38YD/Qwjhb5zn54FvE7Z4PyGE+B655FeAz0uS9DvAKeCvbuTCjbgDrkx0pEx1McEdt19ARvD0+a1EkhbxdpvVmQy37BxHHhGMFfPoqs9yziBYjkC3zVDfEpfP99KVqdH25gVUOWD5+RGsThdUwbSVY9Mts4zNttNaiNNKeGhzOk63g+MZ0OFx9VIPXUOryMOCnniZxWaSqYk27t97nmvVAqoUMDbbTqZQJaY7IbakY5VLcx30tpWYHm/jvn0XkKWAtNrk6cURRjIrtOkNgqzEPdkQsWoHKmU3ysOd5zn1riox1SEQEjXPwPjJSeKKRceOFVYrcZ5a3ELrhyoc7JjjaqmNvFZHk3wmznUzcLBI4tVLFGsxlKtxlAMlLox309uzjqF6rNwq0bqURm1IrLXifKq8n8jdqyxaKQIhMX6xi1fvP8uZWHfoLiggrfqIgsTKWpInF7fR37cGQOAoTDWylN0ILx3eilBAqCFD15lUkV+/wpSV56m1rXjvWmdlKkv6hShaQ1DaDpInmPyVLYgfDbCzMn91/hCyErDpt5wQSBYLIOciLsURvQ6yGvCJsYMEgUz8Wzaph6/xuQ/eg9UWkB0usnSpjYdefZJjVi/fnNxBaypB7p3r+L7M2fUu/Ik47YeWWPxEkvqlFHtvucaFJ0f44rG7kbywztDc4dH9pIR8U4BWVGnbv8SvP/96XvvAcb5+fC/5Iwr1ny5TPV/A3+zT8jQqU2n6ti8R/TkJ9a9avPuJdyEnXDpTVcZfTmbxA0rAu5HuzFv/B4f/2YkuhPhd4Hf/B8e/BXzrf3B8gu93cG54yJJASTq0JerUKhFkNsBfaYuY6ZAyLdaiHhm9Sb9ZBGC5mSCTq1F0Uty9ZYzN0VXM3S4ZvcVgdI2M2mB2d5rueAVT8Xh6cpjXDp0nZbRYaSbYnFzjOX0IyVHAltk+OsNyPUFbtEZcsxmJrSBLgrW2GEPRFdJaC4WAkhVhU3odXfaoOhGSeou2XJW+RJHZeIaCXuN8tYtr1QLL01lUOSCuhxySWmCy6sQ3jJMcluwUt6YneWxplIjqUrYjPNR5gRfWh8hGmniBzOKlNm47cBFD9ulLljhc3IQnZNKXJJ7v3sSd/deIFFweP30rW/IrXHphC+Vc2A5+46ZTfDcxxGojxsxCjn+oxdjbOc+xpV48TyFxTWFxV4qOWA1ZCsFX05UsccOGikbutMxCLEXgKuDKjK/m8X2J4U+WKe1Os7ZXIsh7GCWFqXqWU81uSnMp3nTwKGPxAmMTw/iGxKabZii2oizW8vT2L6Jt8pk91k0gwfLt4EcCiHt0dZRYVFJk0w2qtSiNiRTJcZlHfvppPvfBe+j5zy9y9ZM30bB0kuMy2x5Y4AvLN+NcTjLwlMNr7j8BwBfn9uC1O/REyxzMTvCnY/cxGFtnfF+Rtt/VEIqM7Ppsf8s8h6UhbjOrxLaV+PG+w3z6v72WfbdNc2Kgl9VUgrf2XeDTpf1s71vEVFyEEfDGnhN89Kce4cczz1D7cg+3/t4porLDKfNl9nh/AIPIK5Y7Ex3uElvu/SVqmyA+DZEfCss0jW92oDiC6iD0Pukw85BGdHMF62KayLKEJEL/1tVHbFLJBqWJLEZJRmyvkUs2KD/XATdXcMaSbDkwxcVzfRgdIXJ0uG2Vxb8ZZP1OB+HKRCc1lFtLOGcy2AUPKeahTxvIjoS3o4Fb1UGGzEmVen9oAWmuydz2hlO8+Pd7cdKCga/WufqTET541zd4vjyEIfvsScwAUPGiBEhMt3J4Qua5Izv44TuO8NziECnT4tGOszQDnU+P3coD/ZewA40L5Q66YxVevLaJ0f4FkppFWmsRU22+Pj5KPtEgrttUbJPlYpL2bJViLUYiapExW0yvZ/BmYxjrMrm7FvnJ/hf4+NTtdMaqBEJiqpxlb9s8p1e7iOkuUc1hsZrkQNcUz04OYxoucdMmF2lScwzKzQipiMVaPUZjJYYcC71xvaLJz971JM+tjbAnPceTC1tYnsmCFqCUVPxogFpV8NI+iY4ajckUWneDTKLJ2tk2vFRoDZLMNaiuxNFTNq6tksnUSZo2xUaU+mSKIO0y8s4TzH9pB46touke79n6Ap8YO4giCdLRsCax/lg3vV+c5erP9ZDesc7aYiq0UU04eG6oxiZ8CbmiobQk3LxH9riK91AZQwu7erUnOmj0BrRtWWVpModsyRjrMm2nXFZ+soU1H4eUSyzVIhmxWJjPsvw7/+2GuTPxXK8YfejGtzNHPvNvs515xQYRo6dXdL//fUheqDoeGCFuARG2/tSWhJvzUKIeZsSh1dAJPBlsBaUh4yd91JjLtu4lbE9lfKlAKtmgPV7n8rleXn3gLM8+vofknnXWJzIIMyCWb7K9bYnir/fjRRWivzzPpcmuEDItC+KFBo2qibAUYoUmuhq2M0trievXLWthRyNoaMgxFyEkjIhLX7bE2Gw7nR0lcpEmK404KcNiej2DsxJFachsOzDJuYt9dAysU3+mnc2vGccTMlXbDEFkDRNZCTANN9TIOJ3BHrCRV3WEEoo4K00J2ZOw23yUhoy5KiE70PlijZWb4oiHSgxm1rF8jdbvdfHwHzzDp/7mgVBF7JzHzCOCzmcUrLSEb0oIGXq+tsjVD6cY+AuZydfpDH22wbW3xAiiAbEpNawF7apiz8TJn5KQhKAyJHPowbM8Nz7MvcOXefKF3aSHipQmsgjTJ99dIRDQHq8z/fgAbffMs/Tdbqx+h/bOcsiFMQOSVxWsgiAxCdlLDWbvixPsqiHLgpjphFQBWdD9hgvM/MYh0geWebDrEl/+y7up3mKhmyF0Px9vsFaPMZgpMraWJ/qNJMobV1leSKNEfGQ5ZCzf3D/DVCXLjuwSq3YcU3E5t9hFPGLj+TJeIFNfiaGvqfQdmEORAq6MdbFleIFiK8pQeo1VK85yLc5oYYkvv+2b2BM3HkR2PvC+G54jL33ul/5NgsgrljuDvMGjyHgEGvjRgCDqh4rnORs354EUKpsZmotmeGTzNVDCYDMwsMJA+zqrzRjt0Spd+TL5aJOlWoJEX5V1O4abCmiP12gbWqe7b51d7QvM1tLM321S2qJRsU16u9fR0hZKzCMZsYinWkiuTDraQld9dNVHkgVa1EGPORRyNUQlZKRGYiG6vz1V4+pkB8JSKH23g3MX+1ieyXL1WifRDSKhl/HYklhGS9sokqA+5HJmvJcLF8OmV6NpECyFylr+SxksS8NqCw3Mg3YbkXXZ8rEFnDYPZ6hFrKtG3z861Lc4ZC87zN2boLJVsLNtgcVGksVqktW9Op8dv4X6Fodmr8fqHpWhT7us7ZEo7wyojnjURjzGf6IDeTrC2i6TzV9oMfNgguw5ifwRhcaQg9XjEnkiQWJKpt4rsbYHMpcCnh0bBuDxU6OhsZSt03YMsidV6i2DciXGpYkuWtvCTMYu+MiGz9qlPEGbTbS7TmXUxelzWLvFZ+bBOFanj3QhQSbepDiWxXg2iWOrzPzGIfp+60V6E2XOVbtwktDzJZWBfJHh3CqFSJ2WpTEQX6ctWafeL7E7P08y3yB6IoJxMkbsaJR1K8byXIaR2DIrjTgj8RWM7yQYTK/TtHTqxSjZrgrBYIu1eozeWBkCid2ZeVZnwg7f1LEebuqYYzi+QmQDp3RD4+XwZv4Nc4NXbibS3yP6fv+n8RajaF0NnJUoBCBlHcR6uKUwNlWRj6RCBKQWWhr4aY9kvkGtGiH/tMHq7S7Ggkb3sw6VTTrrh1zimSaNhQS37hlj9UODzP6Uy2ChSHu0ytVSG/qGreXimQ4O3nGBo4+PIlRBZEnCSYE1bJE6YuImAQGNQQ99XUF2pVCXYmsduxghMqvC3ira80nyj8yxVo+xr2OOmhuCz8otk+JseoM0QSjErAVoczr+phYd2SoRzWX2+V5y+5ewXBXb1bitZ4InjuxCRHzMlE020SBlWHRGqqzacS7MdiJ8ic09q0wt57h90zinl7vDQLQQgc4QVHfXpmu8MX+Mn3vxbezqnyept9CkgHPrnaytJVC0AN1wkSRIRCzao3WGEytMN7PU3VDwo+VpFCJ13lA4ydlmLzOtLJavYioea1YMU/F4Y8dx/uNjbySIe+wcngvlCJZySBL83O7n+Mr8bhbXU7xz9DCjkVleqg+x7sbwAoULxQ5iukPVMvEDiTcPnmSLucgH/v4d3P/qk2yLLfAnl+4kGbXoTZSp3L7OD18KC7rdRok/ePphhCpIdtRoWRrBXBTJl9h2YJKxZzYxeNcUr+s4DYRi2L//3EP0blplfjUNAuRFk9993Wf5xI++lqs/ESPTX6I0nWF42zxXpzpAFhTaqpTO5zl45wWeP7aNT7zm4/zypR9hfSrD8kc+Quvawo1lItlesevV77vhOXL4C//zTESSJAU4DswLIR654ZP/03O8UoPIwGhC/D9fGUGRAqq+STMIH9ourcSqlwyFjwmoBZHrokTP1LdzMDbGlFPg7ug1vl4f5dWxS3ymtJ+bYlPklDo+EpvUCrN+nJ/47k/ygVu+zS2RSWqBSUFp8OXq3usu9Ptj11j1QvPwglpFk3yKfpyoFBbLvsfCrQUmmuTTDAzOtPrIqzU266EYzpIXijB/dvZW5i63oxQskokmLVtnV+cCE+UcfckSXqCgKx7Xinn+bOff8tNn30E+3kCRAn538Mv8+J+9jw+88wu4QuV3nn2Uv3vwjznZGiCr1vnVl34YBOwamOfsVDefvePjdKkt/nD1Tt6YOcZ/mn0No8kFjhX7+Yuhz1ETKktegi8Wb+bMf9vNH3z4T3jnkXdyYGCSshNlNLnAnYkraJIXClHXt9CuVZhzsjw2v519hTkKeg2AL1zdh+sq9OTLZM0G9+cvklXrXGj18LdnbyV6NpRV+Px7PoIp+XyrvgM70Hg0cYZKYHDW7uVAZAIFwZ+u3k3L19iXnEaTfOq+yT2xS8x4GdJKkzG7gyeK2zk528NjB/+EY1YvX1i+mTtzY1S8KOeqXdyfv8gXt7WReSHLcjPBZ7d8BkWS+PnpRzmQnuRaq41fa3+S51u9PFHawTsKL/IPxVswZBcfmZ/MPc+Um+VTS4f4kbYT7DPnePPpd/HjQy+xLzKFQkCX0uSI1ct2Y5E/XLqPQ6lr7DDmWfETPFnZwUvLA3x15yf52PohvvKOb3D5rH3jQeS+993wHDn89zcURN5PKNac/F8NIq9YPZFVJ86vHH8DiZhFeT1Od1cRVQ5YqcbxPBnTdGk2DbZ2LbOYTnO13sal1Xa+YYyyPJfhb/rX6EuUqPsmi3aKb9i72RxdZclOcWS5n7cNHiV11OTE1gH+fOwOGi2d3nyZhGYz+ZV70OqC0s9G+eL5vfS0l8iaTbYkl7lQ6WS9FeX29onriNXnVzaTNcOA8j29ik/WDtERq3JuoYv9fVMMp1aZz6ZJxFvsyC+x0krQZtRwEgrTlSwNS2e0YxEvkHm+sYXaWJq7XnWNhmfw3eYIysESjxdHCYSEFPX4u+J+npkfZiS3ilTUkVyJS3ObSKxI/GLbm2iP1jlzqZ9vt20j8+k4X926CTsr+IPkveT0Okt2kqv/cZSe/+saP3v2bUSOxLj6je2sHBJctAb4+859KGqoei/OJdlx31WWPrqZxiaF0zN51l7fRNd99BcSRCzBcnsU50rAJ8whGp0hcOt1b3mJL9VuptBf4tenX8dwfIUvnbwJSfeZ35Gm5pmcWOrh9yoP8vrR01z84CjlYZ3v3DGEdiGKlxB8rO0e5IoacmHmFdqP2Rh7Df5iy+18c3IHzuUk17bnkZ/K4CRh5EdWyLyQpXRbkeUPbeUjuTvQJJ+ZaobxUo5dhUX+ZP0Q3/j07Qy9boz3HPkx1MtR/IhAdqH7DSWeWd1Cb6zEt0ujrCaSOC9lWelL8heVu6m6JknN4sjz23jjq19g2UrwOxcf4W37jvDkwhbu6Bhn/UqOX0w/Sm+kxKyVfVnP/b9mi1eSpB7gNYTd1BtXO/rvxiu2JiIhMAyXdLSFFnGJaaFeh6m7dOcq5GJNIhGHgVgRVyiYistwbhVd8VHiLvvys2yOrTLZzNMXKdIXKWLILnPNNNtySyw6aco7PXrMEnva5xntWmRfdpaiFaW6y6a0Q7DmxNnau0RHrBr+37JLu1nD8RTiio2PjI98XW8kqbcYyawwV0tjqB4pLbTL7DSrHJ4bQNF8rGM5Xri2mYnlPE/PjJDRW8R0h3yiwZ7kHD2pCjXfJDZU4amZEZ4ZH6bihWbuF1Y7ODnfQ/ysyaKVIhdrMltLkxwsY2yq0v2cS3WPTW+iTE+0TOqCSl+mhBOXaY7YqP11tkSXGG8UmK5nmblf4eR4PwOZErU9Nsu3B3Q/BeZgjZ5Cic5Mlc50lWB7nXMLXSzvl+l+tsbiqwKYjOFeStK4pUXpVpfBf1jHiUms7xZY21uoLfjWxA7UtEPldJ6+aIlFK0V8TCNyxWSumWaqlqU+nWKwe43JRo7phzXKowHyRAR7awtza5nuriKR/hq5gRLW9hZTr9WIrIQaMq2pBL1POSiSoHqLRf6CR7dRYrmZYPZDh+j97Rdp06u06VWG0msEgczW2BJxxaY24rE/M0VPoUTmSkDunCB7QTBrZbm2VGBvfIZ1O0avvk73sw36jHWKdpTpUobuSJnISJmpZo5tySW0iMtoZI7l+QxbI4vkT0vsTc6yNbJIXH0ZLV4BIUHnBl//shcvwEeBDwD/W0ybV+x2xhjoEUN/+C4aNZN4skWjbiJJId1d3hDXac9Wadg6lZkUIurT0VVitZSgkKlhf7UNoUjU+wSRLWXE85kQ5LTbJ3NGobTP4+cOPs2ffPde0hdUKiM+SNC/dYm51QxmxKG+HEfPWAgBbj1s5+oxB9dSkWRB0NgwHI+7IelOD8ikGqwtppDNkGDWky9Ts3WGs2t0mFUyapN1N0bNM1lopKjYJrar4guJg11TXCm3M3Opg5Gds6SNFrrs8fzR7bz7Vc/w8RO3A/DeW57hj5+4nyASILdkXnfnUZKqxZV6O6OJBf7hT+8hPeaw/vNNeCbD8I9c5dxCF4Ev0/YVk6UDEEQDbtoxwcf6v8IDH/0A97ztKEtWElUKOLXYTXAmhewBAlpbbGTdJ5NscnfXGKdLPcwV0+iax76OOfoiRQaNVa62OnhmaZhiNUZnpsr+whSbzRVuMqf4wLt/lsVDBvLeCpIk8E6lcdIBBw9c5vgT2wkMwY888AKbzRW+ubqTihMhEBKLh7uwOzzUkorX5vC2vUc5GB/jP//qj+O8s8hb+k/w9cWdLJaSDOSLXL3YwwuP/gEfWbuDNr3Kk6MJ1MF+Nv/9AkdX+kkYNv5/bmPl5yx8X8ZeifKO258PIfhC4gtP3oafdTm4dZyj0/0kXojy6//hb/ngF98WdgMrKsOfqXDtAzpiPsKthy6zbsWYf6IPb18NRQn4i72f5mf+7Odxbqoz+4t/gTV3Y5YR8Uyv2POq/3DDc+SFL//yP7udkSTpEeBhIcTPSZJ0N/BL/6/bziALTM3DNV1MzUPE7Ou+qhKhn62m+Ji6Sznmo2woiGmaj6H4eDa48fBUvi8jDJACKSzOBoQeJ5IHEiiWQGnJSAIsT0XV/OvtW9dSiSUtRBAmdbruIQQoisDy5I3A5iOUAE0PFcWUqIeihlomihwQ0TxiikNcsTFll7hiEwiZmGbjBKH6mhfIeIFCVHPCgqkS/o6heAjTx5RdtIgLQsKQ3TCAxFwCSSWihFlSTHUwZRc3LuEmVSK6Sy0aKnCZhovtqGg1H8lXIQh9gBVJwo2H6moJ1UaWAhIRm/VY6LYn+aAaHobpYqgeUcUhorpEDAdD85ARaJKPvLHYaXKALIswO5MddMlDQWDlNHxdYGwUrd0NdGvdDW03BOAjYwkNJ1CxvTCwBhrgSQhVfJ9di8BOyPj+9xNtQ/eIa3b4PkkKVegkH3WwH29yGk+YBELC9RXcjBoyeG0VuSVdDyCw8WwEobeu8CX0mkCRAgKVUPZAgN0WxfdCn2QnUNAUH7UBlqfgeUqYnVYFDTv07rnR8a8sSnQb8KgkSQ8DJpCUJOlvhRBvf7knesVmIm3bcyLyK79MPN2iXo7wI7tD0d8vXdyDOmkihhuIqRhe0uehW87yndnNaE+lqN3WJCgZpHoqRA0HU/UofbEb/bWr7Mgu8cLMIAP5IpOrOQqpOgvjBfIDRZq2zkh+hYvPDpHYt44fSFRrUXTDpVUx6e1ZR1N8JibbybRXKcQaOIGCLAmmFnNE43aoC3s4xW//1Kf4lZNvwPcUtLEI9iaLj9/2N/zV0p2MJhbYF52i7EdJK03OtPrwhUwz0Pnuhw5y24dfwhUKx9b6+bVN3wTgi8WbqbkmP9H+PGNOBxca3bR8jeHoCkPmEl9b20tMtXEClWMLfbxu01nqvkHFjRBTHCbqOdJ6i7pnsFRPoCs+67UY29qX2Bxfww5UzpW62JRYJ6badOhVJlt50loTQ/Y4WerlF3qf4vcmHqItWqPuGjzafgZLaHzs1KuIRB0GskUm17MkoxZCSKhywLsHnucjl+/l7p5rtHyddTuKrvgsNZIkDYuaY1C3Q70ON1CYL6VQ1VBXt16KgiNz044Jzs5101so0XQ16pZBR7LGpsQaZ9e7kCVB7fEOCg/PUYjUubTazpb8CjPVDEPpNbJ6A08ojN9isfCBQ8QWAvb9h9O8tNiP+90c6u1F0tHW9UDu+AqBkAg+28bq/oC2Tevkow2qtslqNU5XpsK21DLfPLOTTQMraL+a5Npb4qh9DfLJBsUXOkgeXCFttph+rp/pT/3+DYPNEukesefuG89Env/qB24IJ/L/2kyk7up0nDKo92kk52WOdA0QCInYqdCHthGN0nE8YPF2medmNuNeSSLFIHoiil4VrEdjeGmZtYsp1LRErRznlNdN5NkEE3dKaGfitD20yJLVRrka1hwqToT/T3vvHWXZVd3rfjueHOtUztVVnVtqtTopS60sBBISwgRLgEEYY6KNweZhX+wLGLCfLNuAAWNsiWAQGBRAObSEUqtzDtVVXblOhZPjjuv9sUsC34dxC2Ffafh8Y5zRVbv3XnvVDvOsudacv9m812VmIIRrKsT36JiXmGhzGpNmM1LAQZ9XqU80MboqgF3QQYLYYZVKlx9XE/h0uCdzFuq+MCIqaN1hMd6hcLDejU+xOVJuR5FcfLLF3movQdlkqp7AFRKTl8lYQuHRyRW0Rkrsqg5QdXW2nxri2sFDfH9xC0dyrfRGc+w4PkBmWYh9ShcdgQJB2eTHJ8+kI1FgZ6aXbC1INheiJVWkVPOTCwSI+eosZiL4Rvz4F6B0Y55l/nm+NbGF7kieiqOzM93NlvYJ9sx3EtItAqrFYjXIdxe2MF8MU6j5iQdr3J1eT9n0kYxXiAdqjOcSVLNBTENDVlzMrJ9T7c0MJLL4ZJunZwYojCRwww5qRmVKAa0kYTQ71LtUzMMx9DUFksEaM7vaodkGn8OJTDNWUWdaiWFbKs3JIjKCJ8cGcUbD2K0mq/9tkqkLw0wuxgHYGj/FSK6Jwwtt+DQbV0jUPt5Mxxef5dTnzuGpyWVUCn4YsJEKQYrFAADCkdEmdZS6RO1cL4dmvskTiLYcGf/2CJOdYfLr/GDJTD/biXKFRPNul/k2lbn9rdidNoqhUzM1jBaHl/sd/prMnXm14ldtjAtKaIDd79ITyeEKib1bQ14JgTaD6asV9BmNpnCVqS4foWk/lQvKGBKojkw5FySwokTPp22O9UewgwaljSZtsQpz7QFU2SVySsbq9wRqfIrNxDkyg+2etOzJYhdNfpPFiEu4rUyt5ulOhM/IYJWCqDEvmKy02UKSvbBp30k/Z0YmeXZ9P1bWz/jrJdScQlIpU7J8bIhN0q7lKDghenwZdhX7CCgWhqMy+P0ai1vCpMIV6rZGSiuhSzaJSJWRcjOvb9lPq6+Ig8xcd4S+UIY+f4Z9pS5cTeKNg/vZkemjJ5wjpBmc03aKrBki2VwhbwXIm0GEI8HaEoWaxgWxNFXXR380iy1keoNZVvWncZEoJX3EtBoBxWJUTbEtcQzbVegLZpg1YmyMjmG4Gt8a8dKifmvZHg61dABQtXWaeivekng9SEtTkaZQFXW5y8aWSWaqMWbKURxX4szkImXLx8wZDoPJRVK+CvHzaqQrESqGzvKmBUZll5ZwGSEkmvwVVoTnKFsraT03TVcwzwPv38iaxCh94Qw/eWIjJ5e1cEbzLCtDaR6dX4nlKMgzLqc+dw79n3yO2kP99CWzlL/YReX9ZbZ1etowhqvyI3U9yaYiHV+IMfY6H05J59o1O/inZy9E2lKnvSVPRDeopnworS5dfylz8qMKpP2s2DJG/u97uPBTh3hwchVtT0nMvtwH/7/AiAghtgPbf93jX7NGpFLyEzNURFZHThk8u2cFACLgIHfVcUsa0bYSFb+f3KPtqEmBc3kOezpKqKOEvS+OboLs6Iz+aZnY0xrB+Ri1DRLF4VakZpet8VPs6V2OfCIKfRXG7CRNaxcYW0yiaQ5KVSKkmyz4XIxjMRQHzGabwokkkiOh57xRaq3dQa7L4BMY24p86cGrcFWQZEGsp0CpHOBHc2fTF86Qs4OcqqUwXIUTuRYASjUfkgRbbhthopJg/t5uem8Y5e70elTZpf5AC5tvPchnH70O2ZC49crHyN3XyU/72tHKEje8/hliao1nssu4ovUo3/3m5UTHHY68pYq2K8LQtcMcmu4gEDCJHvBhJHTkoGB+WYSPtzzBP99xFTf89pOMVJopWn5GFpsQ+2LgeikGxmCdg8e6ae3OkfKVWaiHuX30UiRJcNXQETp9eVJqiXpI46m5QUp1H3l/gIBi8Y6e51jjm+Ynt1+K26Lw2BUBXEcm/GwQMwKxN06yc+dyArMyZ/+2F0j2XWMLfTFPNuGFZ1cid1fIj8chanH1xsdZ7ZvmhXf1MPvNKOckR4mvyTC8mKJg+pEciU+2PspXMufiCBnnL1uwEiobPr6PpyaXUXuon8CVp5i8exXurRVqhRCxnhou3jyK72iAdJfGeZ87wciRQYKjOqsunkHPKLCgsDDdSuBhg/Aflqk/mSJ82wlSpTiZyQCji02oNxd5XWwf399+LsabS/DcaXkyL/FqHIm8ZudEfP1doufWP8BMOvjTKmKtly9hjURwgq4noFNSiAzlSYaqTC4kvESqgoZvQcFMusQGcgR0C11xKNT8tITLzJUi1PcmWXHJCAcnO2hPFZjb3+qVmGiziKfKtP6FghPQqH6qyPy+VtzOOorq0pnKM5ONYS0EWLZqhpqlIUuCyVPNKFETRXERApLRKnNjSfzNNYzZIF0r5lnfNMXRQhs+xWYwskDJ8iNLgpqjcTKfolz3Mdi0SLYeJOmvsn+kmxvO3EPN1Zmqxmn1lziYaadqajSHK+SqAfK5EKFoHVV2kWWXwvEkbouJlNVxfS5aTkG2Qc97OTDVTpdgX5GAblEzNaSfxfnYe+/iMz++CavFQp/RMJs9g6jnZVxFgATBWQn16kWifxPh1BtVIiMKxdWWp9pVUhE+F2RB4JRO/3enceJhCqsiXPWJp3hqYZANyUl+uHsjgUQN5YUojg722rKnb1tVadqhUbuqiPp0jFqrwNEFiSMS9SaJ4JxAtgWKIfDlbOY2+7DWVQgETMqjMU96IOCSelal3CsxdMkob2vbwee+9lZKy21CLRUU2ZvgzmXCrOmfYTIfp+36o1QeHCDzXBuuJhCKV0az59wpTo62cd6aYQ4vtNEbz3FwooM1PbMcT7dg5vxIfofkMzriDRlCukV6dxu9W6YYOdzBhrNGOJxux6hpnNE7zUO/fQ/G+OmtzkRiXWLD+R867Xfkqfs/8erQE3m14tctgutyqLaC3unQHvWyTGdVh9J4DDViIcUMSsNxQmtNkASBo37kzXmkboExFaW2pwlnfY7It6OUNskUOoIoqgsrKuw/0UNf3zy5ezuxV3u1S/xRg/xcBPnTBTTFYG4qidRuIGoq4ZYyM9kYZtZP5+AC4wsJtKVkO39TDSHAdWS0AyHOe9MhHjZ0yrkgctJk6ngLl150nJqlcW5qlLBSZ5oEMbXGA1Orifk9Q7Dw5X6cdy5SsnxoAYuoWqdNKbA/00GmFuTarkMsWmFcIXFcaqUrVmAgvMiuhR7CusG2bcM8NrWcWHuGsuFj/YZpjuZa6YtmmatFmM7FqJ+IIa3IY1kKa994gjk7RtuGNKajMLAyw1B4nuFyC+OlBGHNRJFd5sph3tS7l4f/ZBU3JifZt6qL61OjyAju2L+VaLTGbw3sYd/KLgrbAuTqJn2xYcqOj2wlSEtbkWCySjRYZ+UN4xRMPzPlGIrs0tub42hzKyHVIfn6cZK+KraQOdzbRq3iI3xRkcVchFCoTk126QjNs6VpjHvH1nLWppP0hzL8cNdGlJsWuDA1zVP3n8UjV5UZvH6YLYkx/uXYVkxDxbcnBAM25S924d5aofLgAKGrRkl/N8obV+5HlgS2K3Pf8Fp6exeY+swQ+TdIVI4kePfrHufOH1+KsqZIR98ixZqfzFaJJGB8pw3/DXnSD3Sz8nVj5D7dy9v/5hn+adf55P6692VHaLway2i+Zo0IgGGpRIN1CpUASV8VWXKZyscJdJVIhGpky0GWbzpFWPOqzWW3eMlOdUMjuU8me2Eds+zH/K0qQc0m7DeYmU3Q/KRO/soqTf4KhcsztPwwSWzYwGgKMHGdoHi4Cdcn8HVWaP8nH3xsHp9i0xIoMZuKMV8Kc8nAMHO1KLLkMpJNkQpXCKgW89Ew0/U4umqzom+WiWyCjZtGCCoG57WO4ggZv2ST0srICC7pGKZk+8maQRbea9MXzmIJGctRiKlVXCHTGS6wKT7GznwfVVtnTWwW01XAhoO5DobiC6iSy3Pz/XREixwe6UTJqzxnalTSIeaTYSTJW+pWTCgtCVQfltr5vY4n+MrUJXR3ZRjOecpmFUtndi4OQkLYEsiwP9nFqSPtyGsEp+aaMGwVn2rjD5qE/QaPza9gYj6JNB5Az0vs7Etw5cWHsboUkkqF6mKQekgnPZNYCqqSkHwOi7taCc5ILJxlsSDioArUBY3wuITfERSbW+h5wSS9KUEl6ZJLRIjpNep1jcOPLmdkQxYtYjI3E+d5U6P/ojFubn6WW3fcQs4IYswHkWsS6vlZpEKQyvvL1Aoh7H1x0t+Nsuxt+/i3OzcgKwLXkZAVwfSedoLvzcNCGDvk8kKuj7ZzZlgsh8jsaMNosel8TGL+xgChG3NYpobZ4TL5QB+Vd9SJ5btpf0hl4job7Wcv86F/FXoOr1kjIksupqkSS9SZX4gSUCw02ZvtjgQMwrrBTDVOXzhDn38RWerxpATrAYrzYdp+e4wrYtOMVZtI6lWa9RIprcQP5LNx3ypxSSLNk2OD3DC0n+du7mcyH6UrOYsvE8c0vQfvjI4Z9tzSw4ZAmZhWZ1VoFkUSZCpBzghPkfN7M/dz1QidoTw+2YtcjagGAc2mO5Tn5GwLy0PzHCx1kjVCnJhtYW3nDLIkiGl11oRnqNg+rx5NchJLKKwOzjBZTnC00k7ODHJR8gTP5ZchSwLDUblrz0auPuMQNUej5mjkjCC2kCk82kZmS4lzVo7gU2yefWQtK86bYPbeXqzziwT9Jpe9bj+HC+1ka0HmJhN8fuwazhyc5MRCM7YtU96VovfCcYa65l+6FzPFKEXLj1KVmb+vG3e9wdScZwxkzWUuG6Xr2xptPpnMWolam0v7kzJPnzXEsVwL9xdW87oNB5g3whz78QpcFbquHPeWeHe3YWwrsjyRZ/qRHoQEwbSg0gVGQhDoKzLWHEFtLeNUdXyTOicOLeft79jOv+28mJbPagx/QEEJOIinElx/65P8MLsJ9VgQ4x4/N3/yZzhC5pmFAYrFANs6TxDrqXHHqW28ceV+/u3ODQzdsgdlaAAWMrQ/6PB4ZRXX9x/gIX0Vb+/dybc/fw3v/eSP+d7MJk4N6ly38iD3mhvZ0DWNX7F55sBy3nf5Y3zz7st4z/pnuPeLlzD44WNcGZrntuaXkcXLq3NO5DUb9v4iLhJInlEBr0bvL91PyF4xqqX/f7G0pBfL4b60j+3KS0NXBZ9uIy/dNVl2kfGOF7LA1b3hrSy/eOzPz/viOSyhvBSo5Ar5F87rvjTJ/uIXiyIJTwVr6Vhddl6azJMlF3WpLKdPtn/+/7/QdkDxSmnarqdt4ggJeeksQdV8qYyornnHO0LygusAx+8F56mKp5nhuDKWI4PuoixdG59mo2kOksNLWcwvBve9ONQXmsBVPcMhHAnhSmiag6Y5uLqEEZO9+QUZXNXrg1+1vdo8eH2XraVaNkLGcb0AP2npPjl+vBwWx6uNK/SliycvBRnKAscvsANee5INQpFfun/C0xbykukC3jzHi/dHXbqPmuRdd6Es3StFoAwN4AyPIiXiGK4KjndfnKUAQ8fvtWu7Mq4jec+C4j1DmvzzaDLZkVAkF9snIUvetZVeznLLq1QK4DU7EhFIuLb30gvbi+Z0JeHVenFkLFdBOBK2q2C4GrbwttmODI73wFrCCxwyXNV74Vmqy+rKWEsPseF6tUiEkLyHy5WQHAnZkLGFghASpqNiKRZ1V/PO4coYSz/LksB2lJeMlekomIqK5SgYS320hILlKpiuisAzZi8GqllCwRbK0t+helGVQsF+yYD8/Hfb9fqMK2G6XkSnvfSg20s1hR1XxsUzPJLjGTzZBNuRcRQJB9lr2/WukxDSz9t2ZGThvSy2kF8yUu7S9ZEsL7HOdZYMqivhOBKS5E1KahUX2VKQXG8i1F1yy8AzKLZQPB1RxzuH5XiC1+7S3yXZgCohuS6y7fXPdSVwJYQrIxxPoOpFIyS5IFsOwlFxVQnZWjLseMZKcn5u/G1XRjje/dYkB8n+uVFgIfNSZKsrvBGWJRQc17v2suVF01qOAu7SF4YLtpAxHBXJ8aJecb3jZMe7x5ZQEJz+6owXsfrqG4q8Zldn/IMdovWTH/ZKOcqCVLNXL3VhMgGai6w7uJaC4rdxF30oVRnRU0PM+SFlIHI6Qhes/uwMxz/chVBBNiSW3VXk+K0hVnytws3fe5C/+MGbMTtNwvEaIZ9J8bkWui+ewBYy6idj2J8rMDreAqaMbMi4fhc1r2AnbQITGkIGJ+h9Q7p+FzlsEQ7XvZUDTSB8LlrE5IrBY3T48ozXkwQUi4Ra5blMP4ORRfJWAE12GCmkOL9lhLseP5c3XrKD6VocF4l9j67kgzfdx9dOnE9pIcwtm5/lh9+7iOZLZpjJRvlfZ/2UiFzjK5OXcEP7Xm4/vI36XAi1qYaiCC7sPUnGCBFUTXY8voZl317Ejge5+GvPc1nkEO/c/S4+vvZhKq6Pu2fXsz4xxUMTK5EAXXVojxSZzMdZ05wmqtVp1ks8OruCUt3Hjf376dKznKi3kTVDVGyduqMyX42wKjHH1ugIEaXGJx7/LXp+AtNvtXAKGr33CVxdYvHmKqrq0PZXPq75+na6tSx/vOsGzuqZRJYERxdacYWErtqEdG/JOKLU+PN/fjvVXptz1g2zY+cKNm06QaYe4uSJdu65+u94qLyGyXqS+5/YiORCy5lzLO5sxe6v4zsaoHPbJOMLCVxH4aJlwy8Z8Mx5ORbedw7KtRn0OxJk1iq898YH+fsd22h7VMUKS2TXClqXLxD7pJ/JT4FR1yHtQ3Ik7JTFh7c8yt8+dxnYMunP337aymbRaJfYuOkDp/2OPPH4nzRWZ34VEqAUVNwmCymrEer0lNSzFRm3yUFWBCyqJFsLlDSHetZPyG9hWAHcRR8onhDNwrZupI4agYCJbSsUhyIoZZi+LMbJeitisAIFH+VMkHpQx2/AyFwK15YJn++nNq+ghUwcXcHfYmJZCspsmERnngUlBrJAndNxozaK3yYY/HnWppw0EfM+OvoKZM0gUbXG4Ww7baEitl+hbmu0+QpYS65QW6iIT7ZRuqpUHB9l20fN1jD76hiuhmmqSHWZ3bkejKSXn+I6Clk7TEX2cXykg7HEJK4rISSBNBLC6qvz9OQA8VCNmK+O2WpRXJ1EMQS78j3E1Cq2LXOy3sqCGebkSBs9Z+SwLBXXlTBMFUUOU6np7J31ihcOpRaZTScQNYWJtiRlx8e9w+uWVqgUz7Gf8dOyucScFWN3uQ8pYFNp9xHYpxJYEJgxb7Si7IhSGrRIJCUeSK8l5qsReSrAngt60HSbesGHVFEpR2wKPoeHQmsAqHU6dD4q8Zw0iFaTGCskmZtK0D2wwJiV5ImFFZxMN+MkraXRg1cxMNlUJN2lcXK0jd7eBab3tPN4ZRU4XjXBxPsUmr/6HL63tjGdaMJaVuMHk2cx0DvPYksXHU/myWwJkp5O4K7TiIfmSE9EULuqdP6jRukjJb5zajO4EuHWMvwH7vd/+Ny/Cr/0X7Mjka41MbHqS7+DT7UxHYVLW47hCpknF4c4OtZOX9cimUqQciHAG9YeYH+2k+mdHQxsnaBq6SyWQtiWQndzjvrX2im+rUR/MstYLkFrpMzoTIotA2PsfHolwRV5TEslGamQPtbCpk0nkBHsmuhBVlxU1WVt6yxF08/oQhMbuybJL2WZypKgbPrwKTa2kJnY1cnfv+mb/N3kpaRLESo1H8KV+NKm7/L9xS2cGztJn75A2o7TpJR5pLCGFr3k5c780Tmc9Zd7UGWXg/kOPtT9GJpkc1dmMxkjxEc7H2bMambKTDJrxljmX6BXX+CnuTMJKwYDgQXunl3P+akRFswIA4EF5qyol5TnKoxWU+wa76E1WaRc9/H6vkMokosjZE6UW1gfnaJVK+AgM1JvIabU0GSbY+V2bm15kjsz59EfWGC8luLaxD5MofCnh68jGapyTfshjpQ7UCWXku2j059nKDDHHeNbub7rAMcqbdQcjTMi08wYcdL1n+vSqrJL2fLhV6yX5i6GsymKpSCXDx1jR7qHFUkvirjuqKyOphmrNiFLLu3+Inft2sS2dUdZHprj6/vP56zeSZp8Fc4KT/BUbjlVW2f2GwPMn+uw7Ps2bZ8bBWDqM0Pk3lvm+v4DaJKDJRTun1xDe6SIcVGaE1/fBKrgfZue5J8On0NLvExLsERCr/H0+ADLWhapfb6DyocKZE40cfamYRY/3cfZX9zDwxMriX8jwgsH/xbj1OnVnYlGOsWms3//tN+Rx5/8fxpCzb+K8PI20fz7f4Qbt5BKKmvPHMcVEkf39gIgmky0CR9Wt4EesLBmQl4GZ8Wb3HP9LkrcJHW/n3KnTHV1HVlzGfh7wfB7VZbdKbjg9uf5zgMXYXcYyKpLNFwjeVuI8seLmLZC/EsRFn6vSnk+hBRwPJHoiopaVpB6KtjzAU8eICtjNHvq5JKhsHn9MLueW44bEPjmFOyVVd619jmqrs50PU5fIENSrXCo0kG3P0fO8nJ3fnxoPe856xnuOLqFi/uHiWs1DFflnv3red+mJ9ld6OHYYgsXdo5y/wvrOfes42SNIFe3HCai1PjHsfPZ2jzGgXwnk5k4Qkg0x8rE/TWCqolfsXjm+dV0PCUIztZY9eUjbA2P8JWxi7m07TiK5PLAzGquaD/GY+kV+FQbn+JN1AZVk6LpR1ccuoJ5pqpxsrUgfdEsrb4iR4ttZGtegp0rJBaLIa4YOEZAsYgpNb4zvBF5R4zK2jrSok54XEatC3LrXCJdRfR74qy49ShJvcJPnzqbYH8RSfIyuTPZMPF4BUUWbGyZJKQY/HDXRrSsSmhVDvn+BJ1vO8V8JcxCJsJnN9/NQ7m1ZIyQV1zdkRAFnY4nIL1VxklYxJvL5GejoAhaO3M4rozjSgS+FaeekMlutFn+3p2M/PVWzj3vCDunegk/EMaMSdS3lnFsheQjfrLrBJIt4eoCPSdjRV3WbjrFgVNdqDM6k1+9DWPs9I3I5g2nb0Qee+q/x4i8Zt0ZWRIMnDFN1dKI9Bqokqf30b0mTaYSJBao4+uwyVUDXm5El1cmMugzaQmVmby7H9lWyZzhTWX7T3oJcqNvEmizEjPnS3TpWWQTAkf9GCmXohsg/06T5f4aYc1g/7YkTiFAvK2EqriesnjEIBasUa77qKRkJMDXZeADVMWhZuiYjkJiZZa6pZIYrFG3VRTJJSibXBI/iiVUCk6QuFZjX6ELv2JjugrhWI3haguK4lJzNHr9Wfyaheq38ckWquTSl8hxYewYj7cOsVgPYbkKdaGiuRo3du1Dllx+PL0eZcbHhRcfZPvJIS5fd4zhSgsTpSTLflBj7A1BXC3INr3ARYFJ/jQTQ2l3CSt1burew2itmUw5iCR5q0vdiTwxrU5ENbgofozDtS56gjkGwotUbB9NWoUPdT/GwXoXR8odzFRirOqdY0UwjSY5bA2M8i/D27B7HTYOjFPu9nFM60aEHNYtm+LQWAec5bIuMk2rVmBmU4yCGaBs6mSLIZCgWvfRkSjQH1jgguAJ7ktvpWVLmnf0PMffiUvwKxaXdRzn+4fOZ4N/ioVIlO6mDH9+1y3oJYH0lgXmm0I4JZ3gqE7vUI7KkQR2yOXtvTsBb1L062uvwlpWA1Nh5K+3suxjz/Oe0X288MjvkbnIQJgK/iNhxMoKi2e7qKk6qzvSGL+fYPIvFOSdMf64+35+764P4vi9ebiXxavwS/81OxJpWtUsBm9/N/3RLMcyLZzdOoUsuYyWUvgUm6BqcirfxOWdx1jmn2ek3sLJSjMAh+fauGX5C8SUKlk7jCUU2vU8TUqZxwqriatVWvQif/fYlXzo0oeYNhLM1mOsDKd5Yn45U5k4ZknnrWe/wIFCJz2hHHG1yjL/PCfqbRwttnFD6x7qrhf2vj27gu5gDgWXBTPy0opQXzDDozMruLFnL/fPrmU2F8WaDhFblgOgLVJiXXyG8WoS25VZHp5ntJrihubd/NWJK2kKViiZPj7Y/wSfO3oVq5vnKJp+xh7t47fevP2l7N89c10Ylor/0Qi59Q5v3/ocKa3El39yNVdu28Mz/3w2xrYift3ij1c+yIFqD7P1GE/sXoNQBK/fuJf79p8JAsLHdBKXz9IXzaAtFa/at9BJU7DC8WOd9DwAE290keSfP1eiprDq7/NkNySZu8Ah0FSD/VES56WZmUrim9H4/RvvJ2uHuPPxCxGK4KYLdjBvRNi+bxXnrBumM5Dnnge3IgmvxKdoMvEHTda2zXIq30RzqMxkPo5xNEb/vRVuueOnfOrpN9L7I4nq+/OUqn58T0X45Ae/w18evRrz+SSd2yv87h0/RpFcvjF9AYvVENd2HWKVf4aP77iRd5/5LC/k+pj9xoD3wlvwlo89xA8mz+L6rgMcLrfzntan+OzAen5v+CQP5NYxXY3zxta9fHn4Il7fe4gFM8L28UH+av0P+cM9N/FH6x7hX99/DW/9ilfH7c9uOMTs4dzpjUTCnWLL+vef9jvy6DOfargzvwpfX5dY/eV3kE3HSLYVyE7HwYVQW4VKPgCmTEffIvP7W70HL+Qi/C5axKAtUWJyqomB7wjG3uXCoo+BHxmUen1krql7qeplnbOXjzH59SHmt5m0teWJ+eo4Qv53OTHrVk1waF8fSAI9p2BFXXzdZdgfxQoLz8XpqeHmfGBLIAvkpBezIeZ8tK2eZ3F3K+EzMlRqPjZ2TTJXi5CpBDEsFWs4iuPz2pHa6rhZH3JNxkmZxJMVgj6TuYOthIbyOK6MELC1c5zHD3qTgUrEIhSqE/EbrErMMV2NcfREJwiJwaFZTk60cPGqE+xOd1FaDKHPaUhD3qhtY+cE1zXt42NP/haDA2ma/BUiqsHzM72UM0EkzUVWBapmE/SbdESLrInNcrzYSsH0o8sOZUunOVDh5vbnOFTrYrjSQtHyE9erjBWbCGomN7Xv5gs/fiN2l8Ha3hlMR2Ek3YyqOdy66hn+dWwj5ZqPm1e8wOrANHurvUzVEhiuyuGFNiJ+g0LNyzW6sX8fawNTfOKum7nyyl1sCI/zj2PnY9oq/fEMxY+0c9UdTzNvRunxZbjtnjfgqtBxRpqKqZGdjqNnFFacf4oT2wdoO2eGm7ufB7xl3L989hoGeueZXIyjKAJxNMzn3/ot/mFokOE7N9CULJM/3ETXhhkm55MoqkN/c4bxp3rZdOUhnt65iu9e+xXetfudGPNB0n/9N6etJxINd4otZ/7eab8jjz77pw135lciJOxHU4R9YB5PwWoTBOiPRBFxT/GqeLINWgRqSSJxRCZzhkTyySCZ7jAsMzn1TkHnj3VmLoBT1/sQEnR9V2fiepemHRrLN8xzaHA5mt8LOovqdQ48vhyxooJtKfT92EVd7RBIyyiGl9GqVWSiz4bIrIXkYXAV8O30UUvJWCGJWptA1W2i94YxEhKl6TbsTpctbRPICOaNMEPRBTYk6+zLdtFzyRhj5SSa7DBXDrNl6Bjb793AGeeeAsB0FMozbdx05V6+c3wTxkwIuUvQ8jOVwrVlbEvlpoG9xJQa/3D0Aq7oP8ZwpAW7qHPyeDtS2GY4743Q1gxOc1h00XJfGLXuUv+wRpNSRtJczk2NMlVL8MxkP4PNixzMB5AUz4CoqkvN0JktRahYOjG9znwxTL2qc/HQMClfme+ktzBfjVA1NRRZcDjfzoWDJ+kJZKkLDSvu0HW3xqFtvehZmeYjgsCCxb98cAuGodH8gwCn/iiFJtt8a+9WejsXCWkmpXKAuqlhmSpCQMEOsL/ag1KVuG/XWezu66b0SBvKhVn2T3divTPAJwNjfL1wMfvyXV59IlWwUAzj3x5B2lKHBYXj6RaUNUUWyyG+N7NpKW5Foe1RlcWWLlquTmPc1UrmIoMHcusYvnMVQ7fsIf2Rc3GGHPLVAD3/LDPxO4KJx3uxV1d5+oXVBLpK3Jk5D79u4aZqLwXsnf5z/xt+j34DvHaNiOJS6XFxwg5yRSGRKiFJgsJAE06TiaS4GCWNeE+efD5EvU0j0F4mY0dxgg56WsMZqGH7JZS2mhcwZSrUmnSCoyr1Jk8S0B6swVSQ2YyfGX8SXQazogOQWaszfqobaU0Nt6qihi1cV8LRA6jrCsy3eP66llOwEg6S3yEUq1Gr6WTXgtNqQEWldWCRRSNEbzBL0fTCHy1XoWZrBBSTmO71byi5iCo51JcZ6LJN1ghhOQrFlZand1HRkW0o2T7yQxD0WeSzgZdU5+sVnbBiYJc0fHOqV4vHlciHA14Waz2AXFHQKy6O5kkAZpwwoq5QdXQMV6Ve0XFTkhfoZakYVRWlqYpR1XCWgtNU2aWaCSKZXtCeKySmSnFqpkal5EdYMnJBxehXCcomi1YE2ZBxNAhNyKhl8OUs7KCCcTCOq4OeN5mtRTFchcBxH5NaElWzYdaH7fhx/QIRspmsJYhpNeyQILVDYSEWwel2CboyZlkn2eu5lUXLz3QhhlJQkQR09Mwz2RmmvSXPwnQrVs5PR98imR1tnBrUvaAzVyIZluh4Mk/gxhIjsTaEqTBdjdOULJP+yLm03f4s1S+cQ342iq9VpiuVYaLdB7ZM1xMulXd7YuDFkTiBvpJXs/hl8Gpc4n3Nhr1LpozbbCLVFUTKJD8eJzeewElYSKrrZdZ2FSlXfagzPm9or7jYcQd/R4VgWiK0M8jCBggFDYK7g8R2+lg4zyYwL6iurlN2fDgljfCkF1qPI9FzzhQYMkiC0nILPWDh1hWUooKT9eGaCvVWx6s7W5ORa7JnQEwJUfMiXEXajx1xEKZMvDtPrhSk7mgsmmHWxGaJqAa5pSXi7VOD7J/sYv9EFxHV4OmZAZS0zrFsKwAB1ULNq2TtkFftThesCM+BDNWDCXxzKofL7RyvttHfuUjZ8dHzE4mB787j9tbp2A7xYI3ibISZqSSx4xLTl8DsZQ6ZWpANvhk6HpeZrccYKyYZ6prn8HAXTc9rND+j0vyMinEqgqgruK7M2uZZ5kphT280blI0A1QcH2/r28m5nafQfDYICA0USPnKAFwUPsbK26YoDMhU19UonVdjbpPOzPkKmy49in9BYvIynfZAkfWRKfqvPEVzskgqWiE0JeOEXYKzMkpBJalXuTB2gvZnHdwbMrx19S5aVixQng+RbCmSG0/QoVSJanWu6j3K0HcKdD9qsSo2R2Rdhohu0POwgeR3KNb8GC021648yLWrD3LNmkNk1wqOfSBIQq9R31rGP6Xxxta95A83URpyGP3COQx84jmkoM3ieomOUIFUXxZ1xsfcW72iYO/vfAKt5FUBeFlZvAJwxOl//pt4zc6JRJa3ieTH/wBfvI6RDbBulecO7B/tAkPB31SjvhCguTfHmqY0hzNtLMzGUII2bsZH+JRCdWOVVLxMthDCH/B8+slcHPn5GOFtc1QMb8ShPhQnOmGTXaFRWmcQ2+PDCoP/3EXCX4uT+Z0K8WCNrkie44stGJbKho4pqrY3d3JgqpNYpIaqOMxnomwbOsFTY8uIhWssTCRYvnyGa9sOsqPQT2fAkyScNeMk1QrHq21kTa9842QuzvLmBXpCWR4cXs0n1j+Eg8zuUi8BxXop4c4nOyzUwyzUQrSHisxXI2iKw0I5hGmqmFUdYcn443WM+aBXlW/JK5dsiVBnCcNQsRcD/NUV/8rHnnozSsDBMWW6O7IslkLUMgFQBbgSSl7liov38uSPN6BszlEdidGxdo6gZnLicBdEbILROrWxCNERmUDGZfFMif91w118a3orV7Qc5Uu7LyaRLJNNxwAINVWRZZfqcJzIKQn78jw8F8cOgh0Q9N/t1fGZuShEz72LzFyWwg4uuY+bs/g0m8yhZpw2A1FW0XMKbn+NgdZF3tX1DH96z1sILM9Tq/pwbBlhymDJ+FM1wgEDcU8Tma0WnQ8ozFzg5cLgQuvQIunpBHrExLEVZNklHKoTC9TJVwPkZ6NIQZuhW/ZQfGAZ84tRREFHBBzUjAZdNSLhGvUXmjBW1pj/8D9QWTg9PZFYqENsXf27p/2OPLzr0405kV+FaanIZQWzFkKSBCcXUt5Qel7HjjnUMwEkIZHJhnnm4FrUioSyqop2PEi93aY0aCOZCk3vLjP36QR2xs9INc7AjyoM3+wQ+0aKD3zmPr54/xtwzzFQXlem2V+n/kIn2pWLSI5M/PYIoT+bYOJwD0Y5ypxoxQm6hMYVnu1fQXhU8eZJFCiGg9gBAS0G20eGUI8HyYZDSH6XkbkUe0I9LAstMFxuYdEIk/KVeWxqBWckZzBdhYS/iq/ZZigyz70/3co11+zkvvkzsYXM2AP9/O4tP+Vv918CM36uu+QF9nznDNzX5dg93cenzv0JEaXGF45fya1rnuX2vduQMxp14UeKWJzZO03eCNAWKvLCruV0fkbCCSqsuX03nWoOSXf52FkPM2vFuXdsHRs7J3jWWAaSQFUdUt0VHh5eybJLJ4jodVo6T7F9fJB6NcHNFz5Ni1bkQLmLXEuQ3JlBL0y9GOGBzDpuaN+LLtmQ12m5TSH7Pgktr9D9JRfJtBn/TJFqj8KyP6px7o/20O9b4E93XEf4CzP4FZuxiR4mNmvYdglNc/i95c8QlA3+7ms34CxzWN0zy9E9vfRs9cqUnhhrY/WyWW66/BnGqk288OxKJEXQf8aMp4na6lJ/MoX+hkWSwPyNATZ0TWO7S/lJH03irtNoetcMi3f0sni2y+tX7+I7hzbT888yvlaZxfU+ig8sI3r1CM5Ph1ioxUHytEB0v8V1vQf5VmkLiuLgxF6G3Ds0lnh/k6RWpUT0z/4ATbexLIUrho7hCInHT65AGQ5iD9a8nBlDYt3WkxyY6iT+eID8thpOXcUXNpAkCPoNIl+JMfMOg/7mLCdnm+luyTE+kWLVshlObe9DP8sLNhpoynD8Z/20bkrjCImZsRSRthKlmQjJ7jy2o1AajxHvzyGEl8wnAaVCAFX3VMojPwvwkQ//gM/sfR1WWUfNqDhhl9uv/Bbfm9/CmdEpVvmnvXKcssHuSj8Ahqty7HdXsvyrx8maIabKcT7S/ygA35i+gLBm8JaWFzheb2fejDBSbmZ1dJblgTRP5lcQUkxkyeWF+V7Oax0lbwXxKTaW6yUCukKmbPk4mm4l6DcpVfxcMDDCUHCe8XqS2VqMvlCGsGIQVExOVFqIL2msHi60866OZ/jnmfPoCeVYNEJc1nQUw9X4ypELiYVqbGie4kiuDb/qRcdGNIPLm49wx6mtvK7rMIdL7WTqIfrCWaarMQxHxXQUgpqFT7HJ1QMokiComVQtnflCGMtQOatvkiNzbXQmCliOgiQJBqOLLNTD1GyNiF7nyIPL6bl0nO5QnkePrGTbquPM1SOsiqaZrsUxXYXyR9uYvCJK9yMlwrfNMl+NYHynDfPGHGua02iyg+Go7E93EA/V0P8myeQVKnJHjcsGj/PY6HIcR6YrlacjVGDHaB9NyTKJ1w1z4qubQRYMLkuTvauLoVuOM1mKU7unlaMP/L+nnTsTC3WIrStvPe135OE9f/Gr6s50A3cCrXiO0teFEH972o3/Aq/ZkUjJ8HHmVyG9JULXjhqPf9CrMN/xXZ16UlCtBOj5/iTHP9jJkXQb2rEgRhKa7/OTfH6WkS9EaY2VmJhNkr1JENIdT8hn0ce8P4wScCgafszBGqqQvGGrauCqXiFv2QIp4tW2CU6qFItN2C0W/owMx5IUz6vjVr3L2/qUQrHfi5SttUjcu7Ce2CNByj0S/T/IcPSDMWasBKarcKLS6tWNETJTbhK/bHGw0IErZEb/UGFAKBxZbEVXHSbNJkqOn7Fskst6j3PX/CZGC17t3v0j3Uy3xDgQ7iSu1zAklUcPrSLRXOKZuQEcV2ZxKk4gVaW2GEQK2KRSJZLRCul0HLmg8TTLuHbjfr59YhPt8SI/m15GpeqjLVlkZjGO7rPwaTb5TJh/VTZz5FAPR+NtSLJgopigZmrIsovlyDw8vBJ5IoCrCZyAi39e5dKbjpEMVFEkl+FMM/mFMGOlDk/lvd1GLim4CQvV52CXNVAE8VSZ0okEbsBFBBz2jnfjljRO5gIoBQW5s0ax7sN1ZQpjcYTPpXevxfGeDo67nTT35Dg3dpLPHLmWE4EW+lu8Uh8n3xKmebfLyY8qpEpxsvua8d+QxzI1njmwHMDL3o6bpCciNH0og3QoxeqONNvHB+lvzjDxeC8T7T6qfRqioLNQi7Pw1c0sf98LSI93cvx4J903pDkw28FAKgPXzaE8/DImRQSvsFbdv8MG/lAIsUeSpAiwW5KkR4QQR15uQ69ZI+LXbOw/yxK2VbjaZK1eA2D+gxHy+QiJSJXCNg1fpUTIb1Jd71Kr6YQuLmG+xyXw4whFO4w408G3oOCM60xGIgi/wB6OIAu4fO0OvvfsxVANYCQEu+Q4dqtFZ0eWsG4wvK+b7GSc8OY8TT6TqqlhRQyisRLUfdT9XgU84yYbH56mRb7gZctKNy6iGDqLm1yiVomS66c/lGFNcBqAOSuGI7xkurBmULUVbFNhvh5GVVzi/hp1oRJTvbmDmFrD1FWWJ+bZljjGWCFJKlhBlVxafUWiap33bX6SeTPCPcfPQMz7ueLc/Tx2cgVv3vICI+UUo7kmYl8MI73Jq217/cr9rNbTOI7Mitg8zc0lEmqFo5V2spUg8lItlu7ODL3BLJwJlzUdZWex/9/VpBmIZLh0xRH2ru7lSLGN2UqUobMW0CSbLU1jXBY+zL+cvBipyWLT5hNUbZ2DJ7tQ2musap/n8GgnyILr1u+jRSuxK9HDZDFBuebDcSRcTUELWTR1ldnSMs7lsUN86CfvpGd1mpu6dvPl1EWsSOY4MzHNjx4+hzWrpnn7hh2sDUzxmX9+K2oF1AtKzLepkPaTmQzQt2WK9APdmB0u77v8McCTF/jmI5egdlXJnGgCXWD8foK/uvuHfOzbv4O9ugq2TP5gCtFsefNMNkiPdyK2TRO5O0rp3nZu/+g/8onbbkW2BRRe3iv4m1qdEULMgic2L4QoSZJ0FOgEXrYRec26M9EVrSLxR39IpKVMaS7MeeuGkSWXnx1ZTiBWJ+Q3WZyOcdaqMUxXZSIfJ+QzSafjUPd0KzoHFpnf00pkXYZk0DNC09u7MRKegM1NFz7PM3MDTJ9KoZQVnJCDf07FGKh72ZcFz0i0Ll9ACImWUJlsLcj0WIrzzjjByXwKRXaZmUkSbyoT9JnoioMqu4xMN9PaXCA9meSCdccJKBZtviJPzQ8yEMkQ0erM1GJckDjJlJnAdmXyVpDloTmeXByiL5zFcFQqjk66EuX6zn18b3wjmXyYZW0LjM6l2NAzyfHFFt4x+DwKgtuevoLrNu7lyalBCvkgStqHOlDGMlW6mnNoisOpdAr/wQCyBe3XTHBl6xHuOLmFCzpHKdo+fnZoBVeceYid6R5Pm0RIhP0GtqO8VJ+nrzXDqXQKx5Q5e3CcJl+Fhw+s8a7Z0jepL61x0dV7afMVmaoleH66F/NklMCchFYR1Jq9EX7zPpvJqyWCkwryVq+P0S9FmbhKxfW7oAr80xr1LhMl4NCcLKJIgmw5SN8fVxh+bxuOT5AazLAwkeCC9cd4U2oXnxu+hrnpBL5YHdtWaEsWmdvfyootY4wuNlGfCbFy3SSTD/RhRgSy4/XdDgu6HzZJfXqMgw+vQFlfwLZlNneP8/QLq+l6wmXurXXc8RCSC72bpjh5op1Ie4n2649iPNzHTCaGlffR3JPj4PvuxBg/vdyZWLBDnDP07tN+Rx468JnTLV7VBzwFrBVCFE/7BC8e/58Zkf/Id5IkKQl8H+gDxoA3CyFykiRJwN8C1wBV4J1CiD1Lbb0D+NRS058RQtyxtP1s4F+AAHA/8GHxn3SseXWTOOPL7/BUqSSXVfE0rpA4nG9nbLaJ9uYCNUslnw+xpmeW6WKU/EiSzlVzXqW64TaUqozcWaX1rgBTV7rocQOzquELmdiTIa64cB/b795Ard/01NN8DlLax9pNp1Blh92HBpCCNoGwQVe8QMHwky2EWN4+T6YWxLS9GABZ8vJmhJBYOJHifZc9wn3TZ5CteC+dUdf4w/WPsKM4wJmRSeJL4fiy5PJ8foAWXwnDVXnhjrNY99uHXooPubbtIEHZ4FuTW2kJltiWPMaUmSSi1PlZZpAVkTkSWpXxWhNRtUanL8/zeW+OZaYc48LWkxwrtbIuNkPGDHOylGL0hR6crjrClnnb+hdo1Yr8LDeIX7FYHponptQoOX4OlDqX5lkEBcvPlU2Heb64jDXhaU5WW1kbmsJB5tvjm2kPFbmq+TBHqh2ULD8LRpi+UIYmrcLOXC9XNh/hW2ObEULivPZRFo0wRxZbURWXtnCJfD1Arhpga8c4IdVgqhpnopigUA4w1LrA6GITnYkCIdWk2V9mfWSCr584n95Ejs2JMb655zy2Do2yIjzHHU9ewLXn7sEn26wMzHLbkUsxDA39aJBap03nIxLFm4ssb1og9+leTr1D8J71z6BILpZQ+Kdd55NqKRK5PcLEVRq+jMwf3PIjPvvIdQQ6y/h1T+6wWAp6pUUfihK6IU3p3nZi183gu2KMaw/nuO3JK1n1v07xhH4fxuRprs4E2sU5g79zOrsC8NChz40Di7+w6etCiK//4j6SJIWBJ4HPCiF+dNqN/2Ibp2FE2oH2X/SdgOuBdwJZIcTnJUn6YyAhhPjEUm3PD+IZkS3A3wohtiwZnV3ARjxjtBs4e8nwvAB8CNiBZ0T+TgjxwK/qV+eauBj8u/ewITnJ8wt93NS9B01yeGRhNars0BEo8Fy6nzf37WZDYIw9tT52F7yl0N3pLn5n6Dn69EWO19vJ2iEujByjWSnxtblLWB6aA+CbR87hD894lHkryolKC2vCs3xv9Gx8mk2hEuCNg/vZl+9iQ2KSVq3IkC/Nnmofh0odvLvtKfKOp7F6f/YMVoZn0SSHk9VWVgTTHK22MxiY51/HNvL7g9vZU+5ltJzi2HQbG/vG0WWbVl+JTeFRxs0UlqsyY8TpDSxyQfAEt89eTqc/T94KcnPzs9w+dTkbE+PMGHEefOFMPn7JT3DwFNZ2FXqpOyqHnxqEoQp/cuaDBGWDTzz5Zt5/zuP840+uYOU5p/ArFr/d+hzPlwcZqzaxP91BreTnQ5se486RLfg0m4VDLVy7bedLQtKK5LK32E2nP899w2vx7Q7jv2jRq2VrK17ovqnS8zWF7EofxQvqJONl8gdTbLjgOAdmOzAmw/y/136bYaOVrzx9Kbhwy3nPkDaiPPr0mVx9wV5CqsGPHjkHtSrh6AKr3UTz21wxeIzdi10MxReYrsQZO9BB+zOCP//iN3jPI++m78cui++r0hYtMbazi3+86Wt8/NiNZI43kdoncdunv4yDzMePvYmKofP6vkO8LraP9+x+B29fsZM9+W5mvr7MkzR04G1/9ADfObWZizuGGS618Mfd9/Opd9/KZ/7pH7kzcx5T1Tjv73yCTx65nut6D3K03MaB2Q5uX/99PrDzbXzwjO38ZE2CW45PsmBH+Os37WTmcP70jcjAyzAiRz73K0cikiRpwE+Ah4QQt512w/9nOy/XnZEk6R7gS0ufi4UQs0uGZrsQYoUkSV9b+vlfl/Y/Dlz84kcI8btL27+GV3VrO/CEEGLl0va3/uJ+/xHxlS3i7H/4bWQELhI9oRyyJBgtNVG1NJoCVSxHIaLXcYVEwQxg2CrqUiU76wttTFyh4iRttICFbSrIqsC1ZDp+ojJzrc17NjzNncc20/k1HTuooFYdpt7r7esPmpiGRui5IG1vHEeVXfyKRdXWqdkanaECC7Uw0pKeSEjzCl2/uJQ6V40QWFqp6Arl2RIdxRIqSbWMgkvGCWO4GhNGEldIlGw/c7UIfeEsU9U4umxzReoIumTzr9ObeXvn8zycXYvpKtzQvIevjl/0kobJhc0nCSoGaSNGQLH47tPn4k8rDF4+yqH9vWw+e5ipUpxCzU/4rijpC1zQBGetGOOr/Xez5f6PcNn6IxiuQpc/z5FiOwfGO71qeZZMsKnKuV1j7Jjt4c0De3lifjmtgRIBxWK2FqUvnGV5MM3+UjfPTfRhZAN09y/w4f7HyDhhtgZGedP3PordbtDaUkAIibmpBGrY8oqgPxeltNJicCBNRKtzKtfkxWMYMlpJwpeTqDcLnK46Qx3zvKVjJ3/+xPXIEYs3rdnL03MDlOo+zm6b4tBiO/ee8U0+OvkGzopO8u1vXY5WFLS8eYLZYhRFdimMJBBNJsKWaX9IZeDDx5AlF1fIPHNkEFyJaEuZUt4LUPzAB37EXx+6HL9uURyJo5U8+UVzVY3WpgIJf43pu/rJr/MC7T5z6b9x54puTt6+lcwnb6NYPs3VmUC7OLf/Xaf9rj549C9/1eqMBNyBNxD4yGk3+kt4WbM6S77TWXgjhtalyRmANJ67A97kzOQvHDa1tO1XbZ/6Jdt/2fnfC7wXINwWekk/VJYEEc2LBnRcmbmFGEqLoGLonCw1s6orTbYSpDAWp33FPKAyca2CZIOsObT+wM/UlS5yzMApaaRvMFDHA6TNKPL+CKNvtrwEOD+oIyHWnHcKVXLYt2+AwlkmVi5OR7zIfCVMvhikvyXDqWKSqqG/lBthOgoFyc/8cIqrtx3m3tI65othhJAYX0hw/pnDHKh0syo4S0yp4AoZTXIoWAGSWoWQYjJ9bx/db8152p9C8/RBJajZGk/kV3FOfIRpI8GklcSvWqyJzRJV62SsEJZQaPMV2F/sZsOZI0wNxFkRmUM+UzAYWiClVxgtNzGyOgY+F2yJ1dE0d5eHOHv1KVwhsTI0R1AxiCVr6IqNX7HQlkSGNkW9fcJKnTXxWVYE07hC5nj+bBa1EGeELCJqnc3d42RbQvSEchyvt7Ov2AVNEFmTwXIUzk5NMW+EKTX5UBWXlkiZ8mU1jKK3/BtQTExXRZZdSuUA/esWGZ1vorOpQEgzaQ8UqbsaodYK7bEiQdlkZjrJOStH6AlkeXJsNX/feS7dgRwptYR5dpmKoVJ7shejxaHtKQnjzSWGUovk/rqXietsrgzNv+TOPGMvJ9xaJv6NCLVzlZdEmo35IG6qRqCvhKY4FEsBFMWhdk8rXDeHbAuae3Kk3pln4aIIJ2/fyuBHnme242UGjf/m5jDPA24GDkqStG9p2yeFEPe/3IZO24gs+U7/BnxECFH0DJmHEEJILzuT6OWz5M99HSC2slXkqwE01cGyFeYDEWRJUKz7UHUbZylOo7M57wnu6BaV1io1U6PkyvTdazN5hQ6KYOoaB0kVqKqLHbRpeiDA4hV1SrYfY2WNwa8JlLqNkQow/laDQ4d6EUEbNWXQfI8f+xYTV0h0hIteH0wfneECJc2PJAnmShF0xUFTHPydZU5WWzBtlUSoRr4aYLDVc1vjapWgbOCXLQoOaJKNLtsU7QA5M4BzQYGS5Seu1xgvJQjKBrrkkAqUWRlKc7jSieGotOsFL0U/10VMrxHT69hCYbYeo+6o7D48gJZTOOivceJQF4VVfjKVIJWyn6YRWOgENMGjMyv4+upv85dHX8fgsjRj5SRr4rMczHUwNtnsyRzaMpLPcx+3Dw8RWGmxfXKQxdYwIdUkXw3gU22Ga60cyHYyPpFCXdQYX5Hgf6+9l4wVYpk+R344idpZ5bl0L64rU6/q6H6bU+kU4R0B7E01fjY+QMBnUanpyCdC6BWJsWCI2Jgg3R2m3m4z2lyleVmJatnHSLGFZn8ZPWyyY7SPao9OpKvIG2J7+cix38JqVeBkiEBFwllfQjJV5t8ATi7Agdk+5IslOh+QeKpvEFkSnlJd2KKcC1J5k4OwXXx5nVNGM2pTDb/PojQdRanIJI9B5ZoSxQtqYGqgwMJEguwnmthkz7Ds+zVOff4c9E889jJeAMD9ja3OPA0vQyX6V3Ba7swv851edFP+b7kzkiSVgOMv9w/+v0iKfz/J9Wqn0d//Wl7sb68Qovl0Doj528S5Pe847RM8OPzFV0fY+5Lv9E/A0f9j8uVe4B3A55f+vecXtn9AkqTv4U2sFpYMzUPA5yRJSiztdwXwJ0KIrCRJRUmStuK5SbcAf38afT/+33GBflNIkrSr0d//Ov7H9PdVGJJxOu7ML/Wd8IzHXZIkvRsYB9689H/3463MnMRb4n0XwJKx+N/AzqX9/kIIkV36+f38fIn3gaVPgwYNfhEBOK++Yrz/qRH5T3ynS3/J/gL4pWqyQohvAt/8Jdt3AWv/s740aPA/GwHiNWhEXsV8/T/f5VVFo7//tfzP6O9r1J15VfJ/Rt692mn097+W/xH9/Q2uzvwmec0akQYN/kfSGIk0aNDgFdEwIg0aNPi1EQKcl6mE9t9Aw4g0aPBaojESadCgwSuiYUQaNGjw6yMaqzMNGjR4BQgQjWCzBg0avCIaI5EGDRq8IhpzIg0aNPi1aSzxNmjQ4JUi3MacSIMGDX5tRMOdadCgwSvgVZqA9zJVYhs0aPB/FeGe/uc/QZKkqyRJOi5J0smlsi+/Fo2RSIMGrxEEIH5DIxFJkhTgy8DleBUWdkqSdO+vU4u3MRJp0OC1ghC/yZHIZuCkEGJUCGEC3wOu+3W61RiJNGjwGkL85pZ4f1kdqC2/TkMNI9KgwWuEErmHHhU/TL2MQ/ySJO36hd//f7V4fxM0jEiDBq8RhBBX/Qabmwa6f+H3rqVtL5vGnEiDBv8z2QkMSZLUL0mSDrwFr2bUy6YxEmnQ4H8gQghbkqQPAA8BCvBNIcThX6et0yqj2aBBgwb/EQ13pkGDBq+IhhFp0KDBK6JhRBo0aPCKaBiRBg0avCIaRqRBgwaviIYRadCgwSuiYUQaNGjwimgYkQYNGrwi/j8l9MiCALwwzwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 288x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "distances = pairwise_distances(projections)\n",
    "# distances = np.triu(distances)\n",
    "distances_f = distances.flatten()\n",
    "\n",
    "plt.matshow(distances)\n",
    "plt.colorbar()\n",
    "plt.figure()\n",
    "plt.hist(distances_f[distances_f > 0], bins=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize with T-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbwAAAG7CAYAAAC4tSBnAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAACPaklEQVR4nOydd3xUVfqHnzMzaYTQS6ihRnpR2lCDQRAFURAry1p2A5ZdyyrFXVf9uRZwFXVXhSiusmBbUVQsYCKhyCC9B0LvoQdCQjKZmfP7495JJskkmcnMJJPkPHyGufXck0nmfu/7nve8r5BSolAoFApFdcdQ2R1QKBQKhaIiUIKnUCgUihqBEjyFQqFQ1AiU4CkUCoWiRqAET6FQKBQ1AiV4CoVCoagRKMFTKEpBCBEnhDjuh3bmCiGe9UefFApF+VCCp1BUAFLKqVLKF8F/Iqq3dbMQYo0QIkMIkS6E+EAIEeWyP0wI8aEQ4rK+/8ki58cLIfYIIbKFECuEEDH+6JdCEYwowVPUWIQQpsrugx+oC/wDaA50BloAr7nsfx7oCMQAw4FpQogbAYQQjYCvgGeBBsBG4POK6rhCUdEowVNUOYQQh4UQM4UQu4UQF4UQ/xFChLvsHyOE2KpbPWuFED2KnDtdCLEdyBJCmMpqr8i1mwshFgshzgohDgkh/qxvbyCEOC6EGKuv1xZC7BdCTNbXPxJC/EMIEQn8CDQXQlzRX811C6uhy3Wu1a8RUtpnIaX8REr5k5QyW0p5EXgfGORyyO+BF6WUF6WUqfr++/R944FdUsr/SSlz0MSxpxCikwe/BoWiyqEET1FVuRcYBbQHYoG/AQghegMfAlOAhsA84FshRJjLuXcDNwP1pJS20tpzRQhhAL4DtqFZUvHA40KIUVLKC8ADwPtCiCbAHGCrlHKBaxtSyixgNHBSSllbf50EUoA7XA79HfCZlDJPF+7BHn4uQ4Fden/rA830/jrZBnTVl7u67tP7dsBlv0JRrVCCp6iq/FtKeUwXmpfQRAwgAZgnpfxNSmmXUn4M5AIDXM59Wz/3qgftudIXaCyl/D8ppVVKeRDNYroLQEq5HPgfkAzchCa6nvIxMAlACGHUr/9fvd16Uso1ZTUghLgBzaL7u76ptv5+yeWwS0CUy37XfUX3KxTVCiV4iqrKMZflI2hjWKCNVf1Ft4oyhBAZQCuX/UXPLas9V2LQXJGubT8DNHU5JhHoBnwkpTzvxc/zDdBFCNEWuAG4JKVc7+nJQogBwCfA7VLKNH3zFf29jsuhdYBMl/2u+4ruVyiqFUrwFFWVVi7LrYGT+vIx4CXdKnK+akkpP3U53l2JkJLac+UYcKhI21FSypsg3zJLBBYADwshOpTQ92LX18fQvkCz8n6Hbt15gu7G/RZ4QEqZ7NLmReAU0NPl8J7oLk/9vadLO5FoLt1dKBTVECV4iqrKI0KIlkKIBsBfKYgufB+YKoToLzQi9dD9stx0JbXnynogUw96iRBCGIUQ3YQQffX9z6CJ2QNokZILdBEsymmgoRCibpHtC9ACSm7BQ8ETQnQDfgL+JKX8zs0hC4C/CSHq68EofwQ+0vd9DXQTQkzQg3T+DmyXUu7x5NoKRVVDCZ6iqvIJsBw4iBZo8Q8AKeVGtJv6v4GLwH4KohK9bs8VKaUdGAP0Ag4B54APgLpCiOuAJ4HJ+nGz0MRvhpt29gCfAgd112hzffuvgAPYLKU84jxej+QcUkK//wI0Bua7RH26WmjP6T/PEWAl8JqU8if9emeBCWhjlheB/ujjkQpFdUSoArCKqoYQ4jDwByllUjC252NffgE+kVJ+UNl9USiqG9Vh4q1CUS3QXaPXAuMquy8KRXVEuTQViiBACPExkAQ8LqVUUZIKRQBQLk2FQqFQ1AiUhadQKBSKGoESPIVCoVDUCKpF0EqjRo1kmzZtKrsbCoVCUaXYtGnTOSll48ruR0VRLQSvTZs2bNy4sbK7oVAoFFUKIcSRso+qPiiXpkKhUChqBErwFAqFQlEjUIKnUCgUihpBtRjDUygUCoV/2LRpUxOTyfQBWpmrqmYUOYCdNpvtD9ddd92ZojuV4CkUCoUiH5PJ9EF0dHTnxo0bXzQYDFUqM4nD4RBnz57tkp6e/gFa1ZFCVDX1VigUCkVg6da4cePLVU3sAAwGg2zcuPElNOu0+P4K7k8x9JpiW4QQS/X1tkKI34QQ+4UQnwshQiu7jwrFyVFwqBYc7aItX06s7B4pFAHDUBXFzoned7faVumCBzwGpLqszwLmSCk7oNXoerBSeqWolpyeBIcbasJ1pBWcGAY5Fu11uBUcNMChuoUF7eQoyFkO8irYUrXlc1OU6CkUgeTLL7+s06ZNm26tW7fu9swzz0T7o81KFTwhREvgZrQimgghBHA98KV+yMfArZXSOUW1IceiCdtBAVmLwHFBEy77cchdBScHai/HcUCCvKwJ2vnp+vkp7tu9PL+ifgKFomZhs9l44oknWv/www9paWlpuxYvXtxg06ZN4b62W9kW3pvANLTIGoCGQIaU0qavHwdaVEK/FNWEHAucHKwJm7dcmq1ZccYSEi9Z12uWokJR40myRDLzjWiSLJH+aC4lJSUyJiYmt0uXLtbw8HA5fvz4C19++WU9X9utNMETQowBzkgpN5Xz/AQhxEYhxMazZ8/6uXeK6kLmAgoep8rBuSlgP1HyfltqgejlWODsQ5B+m/aeYyn/dRWKKkOSJZIxD8Uy+8MWjHko1h+id+zYsdAWLVpYnestW7a0njhxwud4jsqcljAIuEUIcRMQDtQB3gLqCSFMupXXEnB7u5FSJgKJAH369KmyA6yKwGJPD/w1bKna2GDWosLbM+eCoQlE3QcNZwW+HwpFpZBsiSLPZsDhAJvNQLIlihHmrMruljsqzcKTUs6UUraUUrYB7gJ+kVLeC6wAbtcP+z3wTSV1UVENMPplqLtsioqdE8cZzTV6clTF9EOhqHDizZmEmBwYDWAyOYg3Z/raZKtWrQpZdMePHy9k8ZWXyh7Dc8d04EkhxH60MT0VGqAoN1GTAWNl90KL7HS6Pp2RoidHwcVXlOtTUcUZYc5i6XtpPP3ACZa+l+YP627YsGFZhw8fDt+zZ09oTk6O+OqrrxpMmDAhw9d2gyLTipQyBUjRlw8C/SqzP4rqQ7gZmq+GjNmQlwaObLAfASQg9PcKwpaqRYo6ubpce2GC9D9Ay8kQY664/igUfmOEOcufbsyQkBBef/31ozfeeGOs3W7nnnvuOdenT58cX9sNCsFTKAJJuBmiv9aWcyxwKh6kFUQohA3RrK/KQgDSBhfmwtL5cOu/oX9C5fVHoQgW7rzzzkt33nnnJX+2GYwuTYUiYISboVky1H9Re7cdq/g+OI1KUWS7Iw+WPAJHlItToQgIysJT1DjCzdoLQBRVHV8w4NEUCNdLSv21T193OOBginJtKhSBQFl4ihpN3cf80074SGj0Hl4FyDgtvcvAaX05JAzaxfmnTwqFojDKwlPUaOro42Xnn9ZSinmNAUQYNHi+wGo89zCapRcCjf4F9vNw8Rn3p9uBzwEEdB0Hw6Yp606hCBTKwlPUeOokQNtL0HwtEOLdueEjtLFAp9jVSdCiQuu/BM1TtPX6M6GdBFNn7RinGxPggP7eVELoN7B5KKwywid14ePb1HieQuFPlOApFDrhZmi+Euq/rIlfo3llnGAqbNm5tlN/ZvHtrXdr7eY0AiuQhjYXpylaBvW+EnrYoIUD+l8GxxJ4d6ASPYXCXyiXpkLhgmtAS7gZQrvD+Rlg3Q6GehA+CPL2gak51JtWXNQ8ab/rWXizN5zaqm1rhjb055wW6HxvB+wBProFhj4JVzNgw3zIPq+dJ4zwqq3oFRSKqs/EiRPbJCcn123YsKFt3759u/zVrrLwFIFnfiL06gq9u2jLVYhwM7RYCW0vQswhaLoQWv6mzevzVuxcaT2gYPkU2lieM8DT6e48qL9nn4Of/gorZxeIHYC0w/QgyCKjUPibBx544Ny33367r+wjvUMJniKwXNMGHpkCe3ZDaqq2HC5gXc320103mfxv32nge2ADsBWtJtYqNOsun5IywjiUy1MRBPySFMnfZkbzS5JfygONHj36SuPGjf3uv1CCpwgM6yzQsgkcOeJ+f9zAKmft+ZMYMzy8Bpr1grA60OxeTezWAz9QROzKYN5wJXqKSuSXpEhuGxPLG7NbcNuYWH+JXiBQY3gK/7POAqOGQ25u6cf96SHt/cGamUsrxgyPbylYzzoL+8qR5syeC+8O1gRUTWlQVDi/JEeRl6eVB8qzGfglOYrrR6jyQIoawDoLPPV42WIHWlqRR6bUePemk/ZxPpysZ2hRwD4sfMsr/EIiD9OE3yH4HYKniGUf6m/N71wfn0lIiAODEUJMDq6P97k8UKBQFp7CP6yzwOuz4ftvNSHzhuGD4N9zteUXn4PLl2HcbfCfhf7vZxBTq2Hlnl9V2YeFeUzmNPsBEPpzvCyS5+00+3jb8iTdZ79MyLprCLFH0O3++vRSxXl94/oRWXy9NE2z7OIzg9W6AyV4Cn+wzgIj48BazvqMUmqWniuf6hVVa5DouUZglocTW8o+prqxDwv/xyBco3qKCp2TepYBmIetROQVZBdIna1NBFGi5yPXj8jyp9CNHTu27bp166IuXrxoatq0aY8ZM2acfOKJJ8752q4SPIXvrEqBvDz/t/vN1/5vM8j4LRG+fwqsWdDoGnyq0bfhAy36s7qN402lIVlcwICJ3/MO11Mw5vs9s/H0A2uYEofIMyFc0ndLJMe/QglekPHdd98dCkS7agxP4TtD4yDEy5xcnnD1arUe3/stEb6aArmZIB1wNhXqtaZ43SAPcdiq1jiec6zNdVxtHxb+w0P8h4fYh4XfYyKLCwA4sPEfpvAozfgFLcL3FHs9vt75uBRkiA3p8g+g5Xg//lCKoEZZeArfGWCG5SnaVAN/sypFa78asmNx8W0ZR8EUrkVehtWBnAzv2gz2SgufMZ0VJJJLNnYKu8AjaUAuWdjQAp5+Ya7bNi6Rzn+Ywl5WeSV4GeZ1WFYOo93sp6m3rh/CbiL8/t30mnV9+X8gRZVCCZ7CPwwwQ1QUZPoxQEtKaFhCJMY6iyaGQ+OqrCB2n+BmGoIE21VtMScDGnSEzFOQd6Xs9nrdG9zuzM+Yrrsg3eO05DxlLYu87kOGeR2bv56AkVBu5HHuQvkyaxLKpanwH6/+0/9tbnUTibHOAqPj4YVntfcq6vbsn6CVAyqNC/vKFrvwelo7dwd5fM9GvqrsLuTjIE+JXQ1ECZ7CfzyYAO+UUGKgfcfytZm6u/i2VSmcvjyXw9mnOH15rmbpVVFumgUPr4UbX9ayrpQHqbcT7PQheAbLZHkjgxRVGiV4Cv/yYALkyOKv4fHla29vajEL7vQrXchy/A4Hjchy/I7TXwXPjbQ8xJhh+Ey47V3K9Y3MzagaqcXuYhY3U4ZJW0G0o19ld0FRCSjBU1QMkyaX77yzZ7VgmN5dtPWxo8jOGKrv1MIZs7fG+N6/SuaIRauGUMIUsjKpKtGZdzGrsNjIIu8VQDhRvMBvFXdBhdfs378/pH///rHt27fv2qFDh64vvvhiE3+0qwRPUTEMMMMNI8t/fmoqDOkPPy/HwJlCuwyNy6kSQcKnk+DdQbBrSfnb2PRfv3Un4LzAb4QTVVD8D0qdihFFE3y7VRU+924CMNas8CshISG8/vrrxw8cOLBrw4YNqfPnz2+yadOmcF/bVYKnqDi+WwaRtct//ob1ANQy/FJoc60xtXzpVaXyw3TYugifLZxzaX7pToVxd66eONyDn3so9/F31jCRlxnIvdSiLmHUpjW9MFByQcBIGvBfJP/Fzv3MoxsjuZ95hSauK/zEhqRI3p0ZzQb/VEqIiYnJGzx4cDZA/fr1He3bt7969OjRUF/bVdMSFBXL98vLnq8nhDYloQSijAvIdNwPhAB5RC2eAA26wUtVIHKjCDv9FLhoCtPcosE8LcGV6zPr8UVdyDJRIHpFrLwoGjOU+/OjKTtS/If7Dw/xC/MAicDA7fyDW5hZ/HokKKELFBuSInlqTCx5eQY+n+Pgn0vT6Ou/NGN79+4N3b17d61hw4Z5MDmndJSFp6hYBpghZS2MvRWiozVxcyIEmExaIukcqc3rc0O4YR3NQ4ZT3/g3mocMJ/zyD/D6bCxTR/AKSVg4XCE/ij/o5pd4G0letoP3465y5MN3/NFg4KkVx9z9ETSy4tbKC6UW73KmzKkDg5lMKOEYMBJCGJ2JC0h3FaWwQS8PJB1gsxnYkOz+i1sOLl26ZBg/fnz7V1999ViDBg18HrtQFp6i4hlghv/peTKdE8gbNoTz5wtPJD97WSsSqyeWtvRvxowXB/HrgGbYTQYan77AN3cdwfwb9E+5k/X9osHxAwaD4D1uJ8GNRRBsOKcTbFkEVzMgz+PnYtcBMAADNpuJgz8cJ6ZJfxgT5EEZtcwQk8yczBSoFccvtXbwEQ8hcRBOFO9z2aNmOmJmBsmkkkJn4txagYoA0zc+k8/nOLDZDJhMDvr6pzxQbm6uuPnmm9tPnDjxwu9///sMf7QpZCmuo6pCnz595MaNGyu7G4pAMT8Ry7dvMWTxCOzGIk4Ju53o9GzSW+gPlS4W4zwmVgnRc/LVQ/Cb+2xahQipZSMv20AhwRN2QkKs/PHpeGI6rINu06BPKdbRGQukp0B0HDSpOp+Rwr8IITZJKfu4btu2bdvhnj17eleZYENSJBuSo+gbn+kPd6bD4WDChAlt6tevb//www+PeXv+tm3bGvXs2bNN0e3KwlMENdNZylcPnqP2nXdgN2YUdoECGI1uxQ5gCv/jAOeZxZiK6ayPXDcZNv0HbPm1c4tacdCsJzz+7p848sNWNv2qTfVoEbOZ7CuNaNcpRRM7gCNflSx4exNh3cMg7Vr77e6BoUGepkUR3PQdkeXPcbuff/659pIlSxp27NjxaqdOnboAvPDCCyfuvPPOS760qwRPEbRMZymz0SMyawOylNj1okKoM5tf+JwtPMOIoLf2YsyQsAI2LYBNH17FZg2nsOhJzuzRlmM6rCsQt6iOkLmvcGON+xe/wBkL7JwNR5e4bJRwcBEcXgz934JrVGCHovIZNWrUFSnlJn+3qwRPEbR8xfbCGwru+y7zt8qupXOEi0zhfwBBL3oAmz506GIHRS08e64DGvYGYSyw0Go1g4hmcGZVwYH1umrvZyywfwFcTYcT34OjhLqFjhywTNFeZblDFYoqihK8akiOBa6mQEQchAf//b1ExtOjwMJzpZz14h6qAi7OgylgywP36g4hoVc0UcpHwmld6AyhWlE8IeD4T3B2fekiVxI7Z8Ox7+A2N3lMFYoqjJqWUEWxcLhQCP5WLPyZ27jHMpwjw3I5/4yDo8Os/GBZQo4FLr6iCWFV4la6+bU9B5qLsxnPMZ2lQTmFoV0cmEw2NKFzzbulLY+9+y8ln+ywAQ7N8juzCo4t8V7snFxKhaVu3KIKRRVGWXhVkOks5Z+swIEDA3b6s44rbMaBnRmzF2PKC0UgEHkhWGfU4/CGq4TmRnAREPVAmCDqPmgY5F6rFPYHpN10MvMtxwhCSOYhzLQJyLW8Jaa9hYS//oNNKZoVmpsTyb5dIwkNzWL4mFfpH/dBKWf7OcXaufXwkYCwxhD/jYrmVFR5lOBVMRKx6DdrzdXlwMABoDF2ABqebF7o+GYH22HMDdEe/JFIvcbmpdmamyyYRS+ODhgxYPf3jdyFq+Qxg6Ws5NGAXcMj9ibClucgJ52YGIj5/Q+V2x9Xcs/CDwOh6VC47lUlfIoqi3JpVjHe4QcKxnU0N1ckx/P3//zgfKCg3tfKexYhjQ4kUrP69H8SSVbw1ON0i5k2rOZROtIooNdZxUGmszSg1yiRMxb4cZg2LpeTXjl98ABLxgCafPoZhsf7EXpPLl2egsTkyu6VQuEdlWbhCSFaAQuApmh37kQp5VtCiAbA50Ab4DBwh5TyYmX1MxjYioUNpFCPhpznW+B6nGLXkI3U4lT+scsTNJfXwMUTWDthMcsTPiC9/UEenqIVZnWKHUBkFSgjZ6YNaTzDKOaxnL0Bu85XbK/4YJal/TW3YZBjyRjAwPWrQU/UnEcoqSclU+ZrXoKEcpY6VChKIjs7W/Tv37+T1WoVdrtdjB079uKcOXNO+tpuZbo0bcBfpJSbhRBRwCYhxM/AfUCylPJVIcQMYAYwvRL7WalsxcIfiMeKFQMG6pGHAweX6Ugd9tGAncXOWZ7wQb7wOdejD7RjwuwZ+WK3Z+Sv3DxrcIX9HL4T2IxA4+mhTXJnO+PpEXjxWz6qSogdQMqFODRnUPHw2MXrleAp/E94eLhcs2bN3rp16zpyc3NF3759r0lOTr4UHx/v0+T2SnNpSilPSSk368uZQCrQAhgHfKwf9jFwa6V0MEjYQApWrDiw48COAQMN2EkbvnYrdiWxYNYzvDMvgS0jl/HOvARaLCu5rEowMoGeAWu7H63Zyglm8wutE9vT3XAjnwgHn4XYOBeIyNa9iXAyKQANB4a4BikUjholf3mCKhyuANibFMl3M6PZ65/yQAaDgbp16zoArFarsNlsQngw57bMdn1uwQ8IIdoAvYHfgKZSSqePLh3N5enunAQhxEYhxMazZ89WTEcrgb7EEUooRoyYCKEN15S7reUJH/D8stH8kvAxvarABGxXEjAzj4m0pK5f272X63iQ/ixnL9cn9ucPU+7AKI2a69dmJGmg9K/o7U3U59H5MRCn+UgwhPivPYDwaG0CurEW5nrrmNd5qr6jQPjuHaSsOwWa2CWOiSV5dgsSx8T6S/RsNhudOnXq0rRp057Dhg27fP311/ucuqzSBU8IURtYDDwupSyUIl1qma3d+rKklIlSyj5Syj6NGzeugJ5WDr0w8wHJTOCP2LFxkFRAG4szYqRTGZZPAzcBH5N5IiB9DTQJmPmC3xNBCEYExvLOQAeMCOYxkYXcy2K2AdBvsfZZFg7ugTMpfui8k91v+rExneg4uHElxE7VXt2m+d6mwahlW/ldFtTtTEKr+awdOIpb226lX+tM5j0oWPiI75dRVAPSkqOw6+WB7DYDaf4pD2QymdizZ8/uo0ePbt+8eXPkhg0bfK54XqnTEoQQIWhit0hK6YwZPC2EaCalPCWEaAacqbweBg8bWYVdn3oA0IwYBnMjnenNPP6PdE4UO6cR0dzLn+lLHL+whCS+YgTjebKMGmPBjJk2JPMQKewnjg4sYSdfsZ12NGQTxzhPdpltTMXMZPrmz72bQE+Wk8b6Cdvosfya/HFO0EatmsT5qfMbp2sTuv2JCCmoeOA6XaD1rfDL+PJHfra7t2BZz7hiBr4ubz8V1ZfY+ExS5jiw2wwYTQ5i/VMeyEmjRo3sQ4YMyfzuu+/q9u3bN8eXtiozSlMA84FUKeUbLru+BX4PvKq/f1MJ3QsanEErOVwttP0kR/iCkmvJhBDKOdJ5i2cIJ4IPSK7SQueKmTb5YmWmTbEAk0Qs+bkzXelIIz7mnmKTzJ35Necn/MZGfuW6qQNAGjCYHIxYZaKRP7y/Zyyw8zU/NOSKEQb82/28uCZmTfTSivyNhDYAhxVsRYtHCwitq2Ul6PiAyqWp8JxrRmSRsDSNtOQoYuMzucb3qgknT540hYaGykaNGtmvXLkiVqxYUeepp57yed5OZVp4g4DfATuEEFv1bc+gCd0XQogHgSPAHZXTveDAGbRSnNKjFvNczsnhKhtIqXLjduXFKWCPsBg7DowYeIcJpSaOTsCs7U9AewF+9finp+C3SNP6PaGxGTpMLn0SeIfJsP9DTeAQ0O1pTcjOWOCnYS5pxwxgDIMRPwTVpPKdJHKAxbRnAt1QVRyCmmtGZPlD6JwcO3Ys5L777mtrt9uRUopx48ZduPvuu30qDQSVKHhSyjWUnAZYDYXr9CUOh4sr05d2ahIJmOlOs3zXZ2WlDrPwGymsJq5lE8ybDfglWMWWBQPfK/u4Jma4MaV4odcmZm3MLz0FwhpC7vmgKgQ7idUsYj8QShi38hpT2MqbTKJ4MutTWDhBCi2Io1kNeaCrCfTv3/9qamqq37OXq9RiQY4/rLJQwmqMdeeKq+sz0Fj4jQV8AsBk7sFMfyz8RjxjsGIltEEoycOnY17xiu8Xi/EiY0DRsb2ytlcSiaTxFqns5hKaJaxZ17nU4mne5TUeZgmjuJVlgCZ0a5nBSVYVaieWexmFKmarcI8SvBqAkao1564qYeE3ZjOHb/kBh269fch/SeFHUliNFSt27FixkhLTAHPToQXlfLzFGAGd/1TtxtcSSWMK64psLSiPlEsEACdI4X2akEPJ05DSWASgRE/hlkqflqAom7o08On8eG7zSz9yLHCoCRw0wNEufmmyypLIhzSkFQOJZwlL88UOII88zY3JkPw5lKGEEscQLfmyKMdzZrdp8Lvsaid2AIs5UmRL8ZGOF5nF19xSqtg5OcKPfuqZorqhLLwqQFf6sJbl5TrXiJFX/fC0m2OB4wMLbkV5qZAWCtH/hjrVNJ7AOf62i1SWk0wkkczkLxzgELOZU+J5IYQQxxDM9CeZpfniZ6Y/NAFGr9KqkF/aDad/hbLGaKM6VkuhczKBGJa75IPVcA3wEZylKcl6NO44viy1vVwu8C8EYCCKVtjJJYz69OJxFfxSw1GCVwW4gQnlFrxeDCq0/j8S+ZnF3MAEJpbw5Xcmq+5LXP7Y36+ztWzernW4jXlwdgqsfRqavAbXVqN7yXSe5Z+8icPlxnuWc0zhzyWeI4BxjGEaT2jiBpjpn7+cj+v42RkLWB6Gi1tL7ky3p8r5U1QNEogF4C1S2cslXf4FWoCP8y9Oqw6ynb5lCl4BDjJ16zGbdFYwhf0szh8HVNQ8lOBVASaSwDEO8CGzvTxT8ASvAprQvc/LnNRvAGtZztv8jd4MohHRZJPJBlZSj0bsZwfZlr5cXVCfnumx1KUh55bBOxSkEHYVvo6XwToF/r0KHq0GQyeJfFiqBVcSc3mbBB7w7qQmZhi3BVZNguM/QsvRcOWYVrEcAIMWRVnNSSA2X/gsnCWFdJZwjPWc04/QHjx6sMGn6xxjeaHgF0XNQgleFeFJZrGBFHbgWYb9+jTmX3xDL8zMYBJL9cF8Vy5yll9Ykr+eaxnA9hmvYN3YB7JrAYI1Lse/BPzNZd1V+EzAgUXQfhk0FXBdczC11fb3ioZhkyEmeIICS2Wxl7kOalOb5XxT3JLzhqEuTwpnLLAsXps/ZwjVpgxUEZxiFUc0ZrxP+WfhLPEsx+rGzdudg1xLGkmMx0w4kXpUrLccYzk7SVTuzSDHZrPRvXv3LtHR0dYVK1bs90ebSvCqEJ/yG7fQJT+fJsAY7uUI+0hjB7lcJZQwZvJ2vrvyDaa7Fbui5FoGcGbISrC7JiEuHDzwNXAArV5TJwqPstiAjYDxHAwAjGfBoaWoZAew5z8wZUXVEL0JjGM5nlc3HcfNvoldUZqYYVRy8flzQc50NvFPdiGBcIwkM9Jr0UshXY9rLcAAPEU3buUm4onFip0fyeVRfqMtB8rV1wMsVoIX5PzjH/9o2qFDh6tXrlzxW5i5Erwqxrfs9mgcDrSxuLLcoLmWAeSmxGE72hrsJgqLnKSo6G0H7gH+BIwGMoCdwFJ9X2+0MqFO16dzRpXdCgdTKlbwnEEnGVzSpwjkkksejWlIA+oDEE1T6lCHrWynMY3YzDaucpVedCeXPPZzgDzySr3O2Xy3mx8JsnlyZTGdTcxmV/56DnZSSPda8OKIJhQjOdjzH6gEgnqEkkJ6/nY7oXzFPYznk3KJXnsmeH2OOzzNBrOTRHYzHzu5GAmjCw9WL8E9mRTJqeQomsVn0tz3jCsHDhwIWbZsWd2ZM2eemjNnjtuKOeVBCV4VZCIJpQqdkw2kuN1+ftLHXP1xNKF9NmBdHYe0hoLRBgYHOIqHhBsMYB4M9RvAhQuwdhX8C+1VlJMUxBy65hSRwBv/hSMN4b4AfM/XW2BRyjGWxf0fe82flnqsn9M3M4Fxfm6xaqFlRjlUbHtDwhjGT6Ryic7U5VWuK1MAzTQmmZEs4AAfsh87klAMxBHN82zVRVD7/wjteZsZ/JlXvRK9a5nmF7HZSSIrmALAUZazl0XUphVHWU4O5yktq85p1nOJAwyqDvltTyZFkjQmFkeegV1zHIxYmuar6D3yyCOtZs+effzSpUt+nUSsBK8a4y6d2PlJH5O96HcA5C4frW/VbLHaCR9g292NvM29iTCFctPNIXTqCkPioJ+LsfHcdHizBMPxNJq11xxN/HAuSzidCqu0+4NfRa+H5WEOx/8TrM0g9C2MyQewmyummngvunsfqFKNmM4mt2InodBk8rOcYQg/sprRHomemcZMpn3+eOAOLrpMXSh4KHNgYh+dPRa8a5nmN5HZzfxC60WzvpTFZmZzlOXE8W7VTot2KjkKR54BHOCwGTiVHOWL4H366ad1GzVqZBsyZEj20qVL/VJqyImaeF6N6YWZhawttO3qj64i50SCyc6syYM4vXIwFzIjOXExhPcXwl9mFhY7gBdmwVvzKDET6mlgi/7uuuxkwXx3Z5WP/sSxN6UxWEMRdhNYQzCmDPHfBUrBhIl3ebNCrhWsuLoxy8KONkbnKWYaM5PumGnsZnK6VirTgI2OHtjsUbRhOPOKiV3nbT8QuvoKnbf94HG/nETS3OtzinKOrXzJQHaS6HNblUaz+EwMIQ4wgsHkoJlv5YHWrFlT++eff67XokWL7vfdd1+7devWRY0bN66tP7qqBK+a0wszO5EMYiQAEaOdWSgKRkjAwS33X2SSubvH7d6XAG/NBWM5HA7Rvt8n8tnIZuxxqyHUijTmQWieth5gBIJ3eMO/wSrVHCPaGF1JWDjLK+zA4iabygRiimyRdGdjMXemwEQ3pjKcebTjVprSj+HM4z4OFXNjdt72A3syRpPniGRPxmivRe9a/FBoV2cFU6uu6DUfkcWIpWl0f/qEP9yZ77zzzonTp09vP3HixI6PPvro4IABAzK/+eab4m6EcqBcmjWEefq8o/cXvsJTi5zBKM6wEiPfzm3GR729czXelwDbtsCHJZflK4bJBI/77z5BbSK5bF5PdvIYjClDsMetrhB3pkSyRa+UrvAMI4IhetqvtkTRjXpEE8Fk2rOEo6VGeDrn6M1nP82JYBrdiMLKAdJoxASsXAYglDqcYyuN6c3NZZSr3XfJWZRF+x4UrHvGQZcpPb4jWcEUfmU6t/BD1XNxNh+R5Y9glUAjpHRfo0sI0R14H2gB/AhMl1Je1Petl1L2q7BelkGfPn3kxo0bK7sbVYKtWBgm+lEQR1mYt+Z5J3rrLXDjELB7UMEoNBS+TynuIvUFI3UK5bGsSEIIYSU/1XgrT7DAr+0ZgH/Qm5mU7nGYSx3ycHrPnA9vBQxnHht5mUyOEkVr7uNwof2Ra86QbW+cf24t41myBjfxuJ8L6Mgl/DI9rAiC2/m1QkRPCLFJStnHddu2bdsO9+zZMwChxxXHtm3bGvXs2bNN0e2luTTfA54HugNpwBohRHt9X0hJJymCm16Y6dzP+SBW/GHnsSlQV0A/D5ND9zNDbQ+Hla1W/4pdf+IqTexASxJdnows1Q3JZKIJ82N7xV2fp7DwPbexkK68TxP+hXARO+dZhVnFY3pqMUkmR/ioSKmoOe2c4iaLrHtGe7wo0+QVkhMlRFgrfKM0l2aUlPInffmfQohNwE9CiN/ht9LNispg3W91aNMQLl4o+Zi9qdAwBBo2gnr14eHHNcvvo0Qt6MSaC6Fh0L4jXMrw7LoGP48Yb2arfxssB0tYSiIf1uhITYBT3AnAKH52kwjaO0zAAg4wm50AHOMw5zjMAC4yyE0R2JKwk1NoPZOjhdYTmgMIFp+DCY2c657jDIBJ5T9c5Rz+vC2G09BvbSkKKM2luQ0YKqW85LKtB7AYaCClDJrfiHJplo+6JdWbL4E77oUvyk7aUiI33wqflD6s4hX9iWM9Jf/eo4km3YuowPIykniWeZmOrDrjD9ErTME96k7+wyBWlnlGK0aSwd785NEAUcQUc2v6C2fldQt/xR/CJzDxaBkJD/yBcmkWMAvo7LpBSrkdiAe+8mvvFJXCJS+/lz/7UGbMaITISE1ktZdDe4VfxcJv5WrzN1LoRx+3+zrTiVPsZx5v04VO5e+4B9T0iedFWcYNrGV02Qd6TEHW1m30LfU4E1FcyzRuZRn3cZgoYgARULEDaIaZPszkTzgIwfepYxIbc6njh54pXClR8KSUn0gpi5YhRkp5VEr5x8B2S1FRXJKeR03eUMI9TJRiKQqhWXYT7nK1DiUSob1ywxkR3o3pPEsiH9KWrhipg6A2tWlCIh+W2qffSGEebxfadi93slu3/BJ4gF1s5F7d5VYSQxnEVB4s1hZoUxDcVY0PJZR55amQUAMw05i1jGYo3o2LuUfitJr6c5BwGgMGBCZaMRIzL3M7a/kTDh7icqG5dvdxmD/hCKjYFWUql7mdtXRjKt2Yyu2s5XbW0oheXrVTeIxS4Q/UtAQFL+j3h4/eh6wrgIDGTSDEBDk5hcfwBg0tPIY3+UHo0h2efAh2FInSHzgUXnhVC1Tp3dF1j3ApLyQx5Ea4Df7IIju//lxpouLct5hvmMA4t8cuZD4taM4b/As7dmpTG4GgHTG8y5uFIi0rQ8AsJyHlOMS1BLMf5ylWJmYas5IbSSStUNYVbxhKE6xc5jKXuI+WPM1iP/cyMDTDXCzK8m62AJr780sGU1rqMQ2/ZtVSUMoYXlVCjeEFDx8lwjeLYdyEwtMbCqcjK0hKLZHYjVaybSUPCVenMTJXYdtxDhbvh8YR8MnegpGfNlEwsy8k9KjUrvoV76cuOHiEbYzmMLWIphOTq97ctBJwzcFZGsOZF/AE08E8hteiRYvukZGRdoPBgMlkkjt37vQ4DW5JY3hlWnhCiEFSyl/L2lbt6J8JmxzQDPgiEszKGPaE+xLcz+NzWpGa6GnznrQbvIPc1aWP9wTrGJmzGkMcQ/ItxOmr4Z+btGf36FrwwgBN1Ho1hstWmLuj7HYPZ8KUX7Tl6iJ6Bsq2Z5wIbNzBAq5hJQf1bal8yG2kVAvRK5qD00gY7bmdNJcyXv5KcF3VWblyZVqzZs1s/mrPk0Bxd0nx3W2rFlhOwvGumcj1Di3533FgYBaEXQKL3z73GskLs7QxwzvuhagoQaOepwhdO5FW5tPM423m8TZtiMGg/1lGUitox8gs/EY8Y3iWFxn842HqvpdHl49h9qaCG3t6tiZcy49q2z0RO1ee88ILaDkJr6zX3vPZYYGPX9HeK5l/0NvDI+08zivFIjHt5FWbuWlFc3DGMJpRLOR21uaPR5aU4PoUFjbyCqeo/N9pIa4kRXJ6ZjRXkiIruyulUaLZIoQwAwOBxkKIJ1121aE6OpctNo58a2Om1cTPqW6eRa1owueknwHqCZgQAgn+m3RbE3g/v7h3c4oG/AajuLlDq69nxf7efshtxGXgcq5/r5GeXfp+y0mYvRFWnYALxa7tAEd/oD8xSw5xGAt0rzzrKI5owjCQ68G41Twe41X+VGRrCC3cVP+oilzLNA6zFAc2DJjyc3K6jvudwsJaZnCSXwE7AgMtGcExuTy/ndiDRkatDYc6HcH8buXVT7ySFMmxMbHIPAMX5jhotTSN2v5JMxYfH99RCMH9999/9qmnnvLZzVqany4UqK0f4xpnexm43dcLBxM7vrXRbVwWrYEV5JIeCdFl/brW61/c5XpOLSV6NYo4hmD/NAlyG+lbvJzUWE6cIpd8DDJLnaYl8rt0pHY7mn0bxqn5o+DNZcUPPWMJbHV1y1bMKesxPh3uUZhcNnXoxlSy9TmU1W0MrxlmxrOKE6QQTkM2M5uzbMFEJG0Zg5XL7OR9cKn7LnEUiJ2eRS2tnZ3aWVkM2rwVfhgMN62pHNHLSo5C6uWBpM1AVnKUPwRvzZo1e9q2bZt34sQJ0/XXXx/btWvXnNGjR1/xpc0S//yklCuBlUKIj6SURWtzVBssJ6HneO1348zGV/+q9qdmxMPb2FvWAsHrnwmbHXCtAX4r53wciw1SbBBnUmOHQYqZ/nDaaa1UnNgN+xLyPBoMKxA8pCQ9ojmW/ZcxD68Nj70Bt+rjQ2cssCxeK0lvDIVRyf69ad7UAs6fZPrDt5NtHKPHK5X+eUVgYDjv+a8PQcpp1hdLQH3R00wy+s3qQAwM2gzggP0LKkfwIuMzuTDHgbQZECYHkb6VB3LStm3bPIAWLVrYbr755gyLxRLpq+B5MoYXJoRIFEIsF0L84nz5ctFgIuU4hBdJfBzq8ELsAPY4oNkliLikWX42tPfIS3DblbLH/ppdAqG/mlyCYVnw11wYnqXGDYOUST9CSQm4/cW91xReTznuqdg50eM+hSZ+KU3iICcLZk2Bd6Zr+9JTNLHDDg6rtu4vbmsDF0+CgNmTbiroC4CU2ssNyYzyXx+CkFNY+Jrh3ldbKJjLk/+rbR8MpkjtEVm0WppGw6dP+MudefnyZcPFixcNzuUVK1bU6dGjx1Vf2/XEfPgfMBf4AFcbu5oQ1xKuGqGWvSAs3OuUjw5wm8EqG1hihyX677+zgN1Fsic0u1T4XNdSYLnAQ1dhq1+L/ip8pP+nsP502cf5wr3XwMIiwatxLcEgwOHNTCJdVEIcNuLOpBRsX/QaDL1Vc2MaQzWxM4Rq6/4iXc9dKShu1bkKnxA0IYxO1OVVriuzInpV5wQp2PF+wPfQpQHsuxRHx7optK2zjlrZTutOp8Nkv/XRa2qPyPLXuB3A8ePHTbfddlsHALvdLiZMmHD+9ttvv+xru54Ink1KWW39C+YjNqQu4wF3TKVKMOmpSa/TXZ5lpXrc5oDpV2FWBFZCgTwghFCsAe6swh2jvgq82BlEcbEDbUL6e8Nh6i+lZGuUBVadkA5aZJ+gz8WNTNvzGubz6woflzBIy+g97Dq461b/j+FFt4ZTmglisDtwmIyF+oeU4JDMM5rz693VBMqTGPrQpQG8tTUFByGAgzBDJg5HKP9uvpd3zz+C2bGn8oJWAkCXLl2se/fu9TxTuId4Ysx8J4R4WAjRTAjRwPnyd0cqg/UWsA0sGL+rEOz6a70DWl2CMj5JicTx1W6sCMhPJpuni5+ioll9IvDXKM2CS+gBc68Hk6Ho36wsJiZGh50vLHfy9a8TCoud6zkOO6xYDx+m+P+G+fVhiKgLEuxDHsRgsxf0UX+f9q+tfhO7SaxG2BcgHB9R56p/a/T5kxzOe31O0rGncRCKdss2kuuoR56sxdbQ3gyOXoXFENh8sdUFTwTv98DTwFpgk/6q8mlN1lvg2SG2yp1fcRy4ilu1lfo/AEe7g8UPIA8rRqwIJX4VSN0SgnGNQOPwiulDQg9YdTvc0NrlC+wUSV3shHTwzuZHShA6N6z/GWY/5P85eysyYOyLEN4H+6yTSMN9zHtiJSOXHWLeEyuZ9fibfrnMJFazSB4Cg+YizQyX1LnyIbySCJatfrmGv2hBHCYiyjxOuNyeD1waUmiPFpSkvRzCSEr4cP93tBpSpktTStm2IjpS0axOgYF2LSCkwqw7d1xFm9N31AHpBflHnNkmtTTLJcmyM4IhDysCI/MwquwMAcVawii2HTiX436ft3jy92huDs8PgNXHbeTYBRJDgZsQeHrPbBIOfuD5RaWEr+fC1/Ng0tPwiPuJz+Xib3/TXjoJb36o/ZXe6L9LLOEoIAuNDWbWMsDf3oawUEjWk5CnrIdd+2HlBmhUHwb0gMm3grmX/zpTBs0wcyvJnCCFzy4tY87Wn3TrDTrV+4mHe47FRBhDeJMcznOcFLJtRVxBLgE/BmknroF66PWEMi08IUQtIcTfhBCJ+npHIcSYwHctsAyJg7VGU37AU6VmFF1fIHauQucUPznBszySdqZgJ9Hn7lg4zCskYanADPNVhT5NS97nr78hx+OeHWc2WUiOiGNKaCJhIhfhsGGQdqalvsqsHc+U8+oSFs6Gh4YFRYYWT4lyfXbXxSAqIxccDi0D+oIlEP8APPMmLFoKx0/D1j0w9wsYeA+E9IDwXjDJw9IhPtIMM3mXZvL61hU4CMNpte3JGM3cTUe4lWS6kcDmkzO5beUyCt+qtZ/PKG30sm5ljeElzMOer5B+V3U8CVr5D5obc6C+fgItcnNpoDpVEfQzw4urTbwwI5JnVmUVdwrGgEvtyApD5t827cjOaTgen4sjwfPxCAeLfbLyfmUp8fyCFQjFxAoexkybMq5pQZKCIA5DNZkcXBJxLbVUYf7GJOCd4V7mzkxPwWxaiznkVyZbF5ByJI64bSmeuzFLY+sqSBhYsB7VAJZ7P/ZUUbxAb6ZgyRe7sOw8Ljd8S9spgQXfQW4pgV42mzadaJF+W1s4u+Rjy4nlEizQA55614bF58CdPb/7SksOX2rJ0ydgUX7UtutxggYmOD84BOitvxSe4IngtZdS3imEuBtASpktRBkzR6sI/czQb6UJqKtFQn6SB+0EvBqhTfiOvQT7AnRxI8UmeQiXP2opBI7f/c8rsQMwMKHcXXJg4WP+RS5dAEEuNj4ikf6MLVHI8piEdEl6CyMJxU02j2pCXMvSEyGHGSDXAbWM0K0RbD6j3aryyjD/nry2HImio+O0qQT2XMxiHeZd6yhHPIRnZF6AkQ2DVvQSiIUnXmXxja2ZsHgPCR9sL3xAthdTuBYt9bvgJZ6Eqfs89wIM3FryvlDg/GA/dKoG4knQilUIEYH+uxJCtIdyTCLxEiHEjUKIvUKI/UKIGYG+HrMi4FgdWBlVkN3k4wDlQQ0B4rVxOenmKyCRICQybo1Xzfo6hidJobAKSyTrsTEch5tktVb6FxE7gOVYqe/2eE9xYCGP27BSFyshWIn1qT1/Ym4Oa+6AXo00q8yVtXdAzp9BPg5Zf4Lf7oa8x8D6GMREacIXE6Xtb6AHvxiAadfBrCF4TxMzjF4Bh5rB90CAp0uQeQGWJGqvx0Zp70FEwpsfsmz0F8XFrjyE94KovhDaA3qPx/JrGq8c1aw0b7Fc8k7syuJfHcs+pjpw7tw544033tiubdu2Xdu1a9c1Kcn3xNSeWHjPAT8BrYQQi4BBwH2+Xrg0hBBG4B3gBrRYxg1CiG+llH6fl1EqZpM2beCCj+00AJoKGBuiJZx2pgwTJX97HLf8iDRvKKPhKMCIgQRMJWRX9wQ7iThYjKAXk9jHx3QhDwMhOJjEbiAXG0OA6zByK4I4HCwB1pfQYgY2BmJkHnbeQzOTm2MgHgOTMWDGgQUHC3CwDs1L3hkj9yLZgqNIHkHYh42BmFgbFC5Tc3PYMsm7cw4/WHj9/EN+6kwTM0SNg9Nz/dRgGcxyqeO2Xs/teGvBQ5aFnaSwhTh6Y6ZbxfQp/+Jb/ddWrjXfBWqxhhF/pRXWQ5JQgyC5B5jret5UyiX/iZ0RSKgmBYLLIiEhodXIkSMv//TTTwdzcnLElStXvM4JUhRPojR/FkJsBgagPaQ+JqUMdHHAfsB+KeVBACHEZ8A48DTRnB85X7cgG0o08FVkQZ5LgNm5sMUGkQZ4LFTLqek83gA8FapZj+5wJu/Uybf2jHk4pr1dQoeMQHdMvFvum38iFhazjXH8lz+w1uX6yxlAbX7mS1bRkqEcZwCn9L12YD32EkWuOPZCRS734WAfDhLRCm5kFDn6LHZWldqejZsI5aLH168xxFbiGM6KxfmCZ2En8TyGlTxCCSGZtwIrepat8ND/wa59WnCKVyloPCelVz+sISHYEVgdmoB5I3gN/ZgON1jLdTtIinSQHGUgPtOA7xlXzp8/b/ztt9+ivvzyy8MA4eHhMjy8aBJI7/H0VxEOXNSP7yKEQEpZ+t3JN1oAx1zWj4NeYbMyOFXkr9s1ofPXbj7CoseXxNOhiNnWQm5Ne6/tyHefQpq3o33ctdFG000+W3IA81jEVH0a5XL6AZf5AztdjrjCAK64CJ2/cVBc7DwlAzuJXrttTd3Arn9V+nWH374o5+WDlUsBGlerFQXJl2H6bbBqiftjhheMGaewBSt52HFgJY8UtgRO8CxbYcgksHuVXLRcxG1dT2heHlYJRpOBozkGLB6InjNIJdGPX6U+tf3Xlr9wkBRpY0ws5BkczHGYWJrmq+jt3bs3tEGDBraJEye22b17d60ePXpkvf/++8fq1Knj0y/ck2kJs4Bfgb+iTUB/GnjKl4v6AyFEghBioxBi49mzZ8s+IRiZFQHTQhGNBaKNAce8jcgtL2Ew/4FQcgglj1AuEkomoVz0XuzWWWD2K/x64C3+wThW0Y4vWazv1AagvqZqDAis2zKAiY8uxnzHQBI/9/w8V7EDWL8D+t9RsG7Zos9N3uK/vlY418YFpl1nKaFJ08AUoi2bQrT1fiNh+rxC7sy4CyYt8boUhBJCXCCjB1PWV4jYAZh3byP5yQf449IvkTY7805Jhm9zP55nuQSvHIVJu7XAk7mnPK/07o5etaCxSfPr9KsNv13nQ2MBwkFyFOjlgbAZtHXfsNlsIjU1tdYjjzxyNjU1dXetWrUczz77bLSv7Xpi4d0KXCOlDHigigsngFYu6y31bflIKRNBm3TWp0+fYLX0y2ZWRL7L08gIjIzwT7vrLDA6nl/vqc8N7cZjlW0JFa15mC0kEYPTOXJbwMJQ/ce6LQOIn5yCzaZNHtmgVw5PuLPsc+1unCDrd4DoDPOeh8dfBasVQvW5yeaqGOFdkvXlK4/EwTspWuHYd1fC5hRNXN0VkrUkYt77KMmNw5ndtSXrmjXnlrAZPMDNtE8byVup+5GdN/N4bFcSuMX3vsX1A6MhcKJnMGhuUh3z7m0sGDWOPKMJEORKyYLTopCVZ7kE8dshx+E/1+PWbJjXMbjH7QzEZzqY4wCbAUwOA76XB2rTpo21adOm1uuvvz4L4M4777z46quv+ix4ngwCHkSLK6xINgAdhRBthRChwF3AtxXchyqNY/8C7H/OYcVfWmDFiF0YsGKkHlbeIYkRHOEdkoq4M0uj8mairNoQh80WQn5KJeC1//hulU15HnJztXumNQ9SyooRClaWfeLd8cIAtT1wu9vyNJEDTeR+P9O92O2wwCePgCOPHfVqsSSmPumhVzknM5idtpUpL7dg9//iSX35CaakfY+JoST6+nU294LVC6FnJy3btr9JmAgvPw73joE6mh8xvZ5L0mcJS84VWHmWS/D8EW1Kir+fvhcHOmLCRwyMyNLcmE+f8Ic7E6B169a26Oho67Zt28IAli9fXueaa67xOZeRJxZeNrBVCJGMy3QEKeWffb14SUgpbUKIR4FlaNb8h1LKXYG6XnXDgQXb3f8BJEMNxwnFrk8kt+cHongudFAgNL58lYue70l7BiCcoX1XYjLZXEQP9h+BwZPgqfthVgkOdssWLatUafONhUH7AwsNgbi+nv4sQUaLdnD2uOfHSwdc8SC+3mDSSvzssLgXOtD2ffA8HLNBD3iibxttu3Oq7u7eYDOBw6QNRaf2xh67iynM5kX+wxe8WP5xPnMv2PqVtpz4BTz3b0j3kzr07gxbdsMXP0GeloIwOqPwWGm6VTJwqyA6BM7ZwBYgP9OERoFp158YGJHlD6Fz5V//+tfRe++9t53VahWtW7fO/fTTTw/72qYnFt63wIsUTh69ydcLl4WU8gcpZayUsr2U8qVAX686IUkBYx4YYQCn+Eks5nnW8hOLyxmMEoPB52Fb12qDEQju8eAcBwYmM7T3WlYtCKVjTOEneYcDZs+HXrfBQ88Xt/hSNmiWW2nY7drLmldF3ZkAD7/q3/Z6DdXe7XmwZC78Kd59mrEdFm3fhp+1OYDfQ7apyC2lyxYw2cBg0947F/ySjnOWgUylP3/M3zad9+jIXUQSj2AwUdyAxZOHs4Q74NQqzSLzBw/9H8z7X77YARxsqvsV9Rp+TlFPzwuc2HWuFdzuzEAycODAqzt37kxNS0vbnZSUdKBx48aBj9KUUn6suxWdNTz2SinLuI0oKhNBHJrd4gChiZ4vUZeCJpiYhZ32OFgMNEbyHVCeeowOoA2CqGL23botA1j4jVbEctK4BQzoXZAiy9wbLpUwMrBtj/Z6/0t499mCsb24vp7bpXY7hHYH6w5vf54goLsZ+o+E35b7p72tRQKwc69qrs2iVt7mFMiz5qfzsjSOopjrO3YXPPMYpPbWxC62uKNmPakYGExrojlSpEDkFa4ykKmsZa5nluDC2dor8QuYvxhOnIUz5woJl0c4Co8NTv/jEyzvr2cHqMBEU4+3qLBL1Qg8idKMQ5s5/A7wLpAmhBga2G4pfMGAGSP/5gMmcjOJfMBEn9qT1ANA0B1BOwRRmPgJE2sR9CtHi6k4KDxR2hmY8v7nU3n/86kMn7SaDz5/CAMFVZwNZfy12u3wyD8KLD1zbxjsRVRbni0IozXfmQ5mA5iF9nqwhNk5by6DUfeCsYRnWOHjnF13kaDXxkFIKBgMWLpF8fyDrdxXN4/dBeMWuhU7JxKKiZ0rKXj5i0m4A377HI7/Atbt3lt+BkPBH5wQfDX0hvzlQBMuNMuuaLBK4knougG6bNCWFd7jybfgdWCklHKYlHIoMAqYE9huKXzlJm7kEV4gicE8wgt8QEEs/jp6MZs/so5eHraWhJU62BiMg7k4mIuNOACMvOmX/i78ZrIehamNFzocRh55/t+Y7yiwKgb0LLsdm61w8MmrT4LRi6KHw+8LHtE78pd3WDHLwJFMF5HbvV4TvoEGGNUI7u5SkOLr+YWwJk+bNhBVD0yh2vukabDWDvXKORhkMLofw+tuhn8lY/nrX4h/rzdJ/epr26X//Xu+THGwsJNXFnbH8u9bsZhNvDIjHMsA7cEgcexE+n/+I7f953MsPa/VBnPvHQNrFsI//qyF8r70GONjwnV3gX9qq5R2432rA+zuq6UjEyu11/QDMGUf7M6G1GxtWYme9whZxh+nEGK7lLJHWdsqkz59+siNG6t8TVq/MZ0zzC6SD60vZ1jDMNbRixv4iDxMhGDjZ+5jAFvLcRWBkZcwMlMvQOubl/vR59/l/c+nUtglJgHByEGw7ANNiAaWMfRnMGj3KtfxuMTP4c0FkOqujm4JNKgL/ig6UF6OWOD9wdnYHKGYDFb+2DmemKhSOmQ0adbHdcML5s8VZYcFHhoKdluxsPtSSVxbctAK8Ar/5Vnex44DIV3kwE/GUGdi2F0sZ6tnuGZ/MWJE5OZhM0pCrfCnGTcwe/xz+ceGGAQre5Y8oXz6AfjkDCDhuI+DOvP06a/z0zWLDuCsDc5atQAYTxhZH5b5eBcWQmySUvZx3bZt27bDPXv2DPLY0NLZtm1bo549e7Yput0TC2+jEOIDIUSc/nqfalDxvDrziZuxtRa0w8A0FjIeK6FIjFgJZWG550SF6GOFEIoVfKwdP2mcsypE0UhOWPGbtmbuDWs/0R7CS+Kp+wuL3fR/wtQXvBM7gAuXoOEA787xJwdTwOYIRWLC5gjh4OW40k+w27RpBL8t1yzAd6bDxI4wNAweH6Ud090M762CqS/DDXdrlltZjLq3VLEDzfoKJQQjBsJFGP1EZ7/OYknlCE0YQyLf8gr/zX/3JJjFNftLHjasJondJLCGwFdDR2oH6QEoeVJLG1YSs9rDMTMcGwhre8HQOoX3e/MjT90Hi07D/mxYnwm/XtYsN0/FDqpG9Gaw4cm0hIeARwDnNITVaGN5iiDlStG6Q8A0GmJiFnspHJWRSnsvW++AgRH5SaCdmFiNjXi0mSvuLQdn8mcrowDXAIsoBvReR9/uFjbsMOO07PL3uuRIN/eG3O2atff7mbDPpWbhvOcLT0ZP/FyL4iwvF8qRGd9ftIvTPJI2ax4mQx7t6qR414BreRunCIKWHeVKBixzsZiEAZq2gtPHtCkLoKUVG/+QR5XPzXQjmbcKJY2uzQ1k4UVJHmAtc/PbGMjUQvvOksEUCpfsCcHIE9xFPWqXmKzaKcb5Fp7dhs3hIDQPxq9azuzx5nwXbIhBEOdhVkBzXVjZW3MrLj6nic8UL3I4SGCVJzFf6cApoBnc2x22ZGnnPt6i5kZv+kKZLk0APUqzM9qdbK+UspSZTRWPcmkWJpy95LpYSiGAlU4AdOEgqeTijF8MI4fLXFtKayMRNEbyI4LRhLCwxCOdhWChIXA+3wL0pDis89yWQx4l/ZwzYaB2ky4qZJ7SJh6O+DDOEREG2VvLf76vHLHAwVd+pN3B/yvdnekttevDFZck3C07wP/2aS7P0rKpeMF03mN2CW7IKGrRmRjWkwpADNEc5sv8/QLvi70ZMbCad92KnmsFB4CUD58l7sODmNc5SBx/N/PvuY/mHZozrZV3SaGLUmc1ZPoj8YtT5MLQkjo6AANMnQLvlaeMVCkEq0tz27ZtYXfeeWf+0/jx48fDpk2bduLvf//7GQ/Pd+vSLNPCE0LcDMwFDqDdgdoKIaZIKX/0uPeKCmUYESwnO399OLXyl68hlFQKnldyCWcwn7DGzbw4wb2lClxRNEFzd6Ms++bpPPfUapj0NHydrCW4eOGR8omdZYtvYgfw5kzfzveVGDPEvFQPEvw8mOgqdgBx47X37mafhc7JLLT6R2/zP2zYuZ7rWMYbZZ7n0Zw7N9hxMIP3WMk7xfaZ6VZICM0PfA0PaMsJ+ssfXB7iB9FLB5ZS4CRxPrc6IH0n4GfBC1Z69uyZu2fPnt0ANpuN6OjonnfddVeGr+164tJ8HRgupdwP+QVgvweU4AUpy2jNKI6ymqsMIYJltM7fdzI/uMTpNpRsoHj4Y2glFiJZ+JrvbfgjTdiU52HVRli6Uks/dm1nLeqzQieob06B05PhzO8gNB2avw1RfvjhOvaCq1c0sfPAbVkeZvFQvvA5cbXeJIULHFvYWcyV6TFpXVmT2pPEzgdIiPXWTe8/LhcRpGa/QnoJ43K1BGQX/ZqdQhM7N1+/6FA/dDBAZJAUmUFyVD3iM+v5OePKt99+W6d169a5sbGxPnsWPRG8TKfY6RwEfE4OqggsriLnZBRHWZ+fHa7wELugHwYe9Kliuqecs8CZFGgSB40CVM81rq82HcFd8mhvWLS0YHnVRhjyO1j93woUvYO3Ig8105azQFwcCV3H+C56XQfA9Pd8758XFHVVCgbTj85sYi/NaMAYBpWv4bSu8PJbOGwmpphs8Ezlip4rpwYVlAnanQU5Eh6M1sbfLJdg8NYiI97NIMQEDruWItSONqwaYoTJQWrdZZAUuYsxsZI8wwnmOLqyNM2fovfpp582uP322/1SA8sTwdsohPgB+ALtuWMiWgXy8QBSyq/80RFFYOnCAVJLnDrgYBa9GcqHDKZ7QKuKn7PAiniwW8EYCsOTAyN65t7wx9thrhelhDzBbtesx4oSvJwvWxKGI//xRBKK3LUMA7kQtQkafQG2hlBnjeciGBIKN00u+7gKwDmOd5xzzOWb8jWSWjhf5+LUyyTEln1aRWGu635s0FwX1vTSIkMbmuC8DeJ6Ab0hJRXiOmvHOZfNQfQzuZJBcpTUywNJbAbN0vOP4OXk5IikpKS6b7zxhhfJYkvGE8ELR8uUN0xfPwtEAGPRBFAJXhCTSAYPk+4mbtOJJII8nudRQsljOesZHEDBO5OiiR12cFi19UBZeZPHwcffwFUvcqzXuQpNL2l/2GfqwuUixeqNxopNMn1ZChrry5qXSyAQSMIRmYMgcxDgAEMudB5XSPQSb2nK4uEN6ZWWRVrrCNZ1jcJRuzb3ZQ5kVpPA/Y4rnM56vk4bYLIxoXOdMk8JFtyKYd3C4hasQuekHvGZJ5jjkNgMApOjnh/KAzn58ssv63bp0iW7VatWXuaGc48nuTTv98eFFBWLhavM5jxLuFLqcQ2QXCAMMGAFVtGvHDFyntMkTrPsHFYwhGrrgcLcW6txl7IBGtbVa9/lafP4cq3F513XuQo9j4LQx0+iM6D7I/DJ+sobw7M/Ho5jSlb+hNkCR3SBzQdGcITC5cFMmp3BZzc0wm4gv2zO8v71Cs4RVmZHpADvFRtfCzSSNYXcmv3onG/h+YRLvs5pnbuSEBukvr9qSj1GZHVlaVogxvA+++yzBnfccceFso/0DE+iNNsCfwLauB4vpfRDFUeFv7FwlQVc4kMyKGmENwJogJFT2LmAEe2mKREYWUkdXieN0USyEP9nrm1k1tyYgR7Dc2LuXSBQ3WM18YvrW7DNtSJ63WxN7JxSYgA62CFjfWD7WBrNEkycXBVJvUVXicDhEssg9X7q6bEFPP/USRaNbkwhMRSiILu/C1+xssIFT+tR4UCV/vyRDexB+hokFbuLabG9mFVTwhiDjHqMyPJ3sMrly5cNa9asqfPxxx8fKftoz/DEpbkEmA98h2/V6hUBZhIn+ITMMm8dbQjhd9TjGc7qW7SbYR0M+dMZFpEJnAiY6AVa6NzhKn5ObHoUvGULjB4PUv9ICkrNVj7NF5q4PDSKzOeuEnnGipBgMOoRDQBGAe/U5l8JlynUa+m6rEfl6pvG549QVC6/8T4WdjKUR7CV6HhvB1wD7EWLmSuMEQPv8pR/KqkrgoY6deo4MjIytvqzTU8EL0dK+bY/L6rwP9M5o4tU2aSS5yJ2BWQXeZ75tgx3aHXC3Bt+/ApefBKyVgISQsJgVHDEdlAnAUiIQLPPdSw2SLFBnAnMJkYzgEWuGWxEwUKbrEhyaofhwMF93Fwp1l1JmOnGKt7hDv7GcYrOd24HPIWWus4O/BM4yDymKYFTeI0ngveWEOI5tFxQrhXPNwesVwqvsHCVD8jwqQ0DUDS2ox2lJK2shph7ww8rYJcFtqZArzjoGsyxHWZN6Jws5O8AfMrPhZyfIcLIJ7VfL39l8QrATDeOscTN1IVOSIw4c7VG0pOfmRbUP4siePFE8LoDvwOup/D8/+sD1SmF50zihMeWXWkU9VUL4D2a+dxuVaSrOciFrhQW8ncW8ncs7GQBPwKCydxYZQRCsoZQhpGHnRCMrOQJhnMYK5JQDPzMw5iJKLshhcINngjeRKBdsOXPVEAz0kgP0LCqj+VCFZVM0XRaVQkrKwutr6ANKWQTRy0ldgqf8OS+thP0kteKoKE/h4ib3oA3Orblrun+rxNiBwZyhOl4lKtVoQgYZiKYSUMldgqf8cTCqwfsEUJsoPAYnhoxrkQ6TI9izOyGANyiv382y/8JzmdzgfaEkqCeeRQKRRXHE8F7ruxDFBXN4K+0bBJa1g1Jv6+iAiJ4AG9yQQmeQqGoUF544YUm//3vfxsLIejUqVP2559/frhWrVo+Tdgs06UppVwJ7AGi9Feqvk1RiZjHa6nTnRN2148vCFwxAlOpx71E+eVaqVixeFnMU6FQKMrLoUOHQhITE5tu3bp19759+3bZ7XbxwQcfNPC1XU8yrdwBvAakoAXv/UsI8bSU8stST1QElF6zAATHvwIxPo9vZp3HgCZ2T9CANKycxEYDBBc8zGIRAiWml04hu2aNofyaCOvmQ93mED8N2lbRsM0AYOEwKeynIZH8kxUcdVxgWJ6BZfbboJb6nGoix0iKPEZyVCviM1v5KeOK3W4XWVlZhrCwMPvVq1cNLVu2LOn25DGeuDT/CvSVUp4BEEI0BpIAJXiVTK9ZTuELIZnWpJBNQ4xlJIsumdL+mt7hAhnYmUWT8nW2qvBrIiydCdku6ft2LIGIelCnGTS5psYJ4HSW8iG/kYuNLKyF5vhpZRUly0Pt9M9+m9+yUaJXwzhGUuR3jIm1k2fYyhzHWJam+Sp6bdu2zXvkkUfS27Zt2yMsLMwxZMiQy+PHj7/sa189idI0OMVO57yH5ykqEGck2xZyyiV2ZXECO7O5UL2jNn9NhC+mFBY7J1cz4HSqJn5vDoRDloruXYVj4TBRzGA2v3COLDLJLSx2oOdg01K6rK8VzW3iUywcrvC+KiqPYyRH2fXyQA5shmMk+zyWcvbsWeP3339fb//+/TvS09O3Z2dnG959912fXZqeCNdPQohlQoj7hBD3oaqd12i+qs61f7cvLvOQXacGsGjDDHa9NpddFvjbbfBQf/gusQL6V4FMYhEDeZsrJaYgL4IuekvCjQzkbU30si1w7hXt3RMuJMKRUdq7t/hyrsInWhGfaSTEITBiwORo5YfyQN99912d1q1b5zZv3twWFhYmb7311oy1a9fW9rVdT8oDPa0Xe3Xm/EmUUn7t64UVgWEydZlPRr570gAkUI/NXGUDub7mpGe8nwJhKp1fEzWBO7EdMtM9OmXXqQE8/tUKbPYQsEikS8Hw1PUw92m4ZSrUruc+LVlVSVk2inksZ6/nJzirMbhUZBhof5t+Ip0HrTtIOPM3iLobZBbYTkK9ByG8O2SnQK047YSTD4N1q7actRyu/AhZq0C6WtsmqHU9kAPZv+EyS6qArOVgPQDRs7z6mRXlpxUjssayNM2fY3ht2rSxbt68uXZmZqYhMjLS8csvv0Rdd9112b62K6R0fwsUQnQAmkopfy2yfTBwSkp5wNeL+4s+ffrIjRs3VnY3ggZniSDQBNAZbNKQNC54kJlFAHURZCAxAdcSxgUcjCeqeozhOV2XXvK3pYtZc/A28kvylFJPISQM3lyhCdtdbSD9SPH9P3tRmLaiSMTCFP7n3Umut5D8ykQFGyMcebx5eiXnjRHEZR/HfPUU2qOYQz/BAP52xEfPgwYJ/m2zGiKE2CSl7OO6bdu2bYd79uwZmDlOXvDEE080X7JkSX2TyUTXrl2zP/3008MREREePbNv27atUc+ePdsU3V6ahfcmMNPN9kv6vrGeXFhR8ZiJcBtROZrIUvNuhqDdgkIR/EDr6huV6YHrsii7Tg1gzUHPcy3k5cJfbwW7DTLdDAnm5UKcgL/Mg7FBdF9ezDbPDpT5/2kUqbeXvy4lVw0hTGk2ApBESDvJRxbroudsIwCjzun6A40SvSrLnDlzTs6ZM+ekP9ssTfCaSil3FN0opdwhhGjjz04oKgattt0JfiSLPoTTjlB2k0sOkgepR3fCqkbOwndHwd7lbnYIeMuD3KI9JsAed+eXzNbjcWiWiOdV8jI8iO95fQqcPABTgsQDN4GeLCfNgyMLxG5k5mGWR7UpLnpQSPgQBnKAlFotXQQvgKQ/rLlOVdSoQqc0watXyr4gvhsqSqOsgq5BLXQAr/eHoyWVIJfwmID6MfD84ZLbGKQ/9W9frEVb5pY9xn74QmcKuzJdqor7WCr209mwax1MebXyx/YS0DrwMF9iL23E10XIfq7dms4550iNaFz68VKr0h6Xfdx/HS4Vuz5OqARPoVFalOZGIcQfi24UQvwB2BS4LikUJXDIUorYuXDxCDwdVfrUgUEJcOPz4LB5dOnd6QP0paLiVrbYhYSW3f72VfDIQM3NOW+6R10KGAmYuYtryz5QH6eTwkBqeAkJzJ1jefr7U+c2Vox158QZFKNQULrgPQ7cL4RIEUK8rr9WAg8Cj1VI7xQVhoWrvML54EwhdsgCP78C6xd4fo71Cvx7eOmitz8FbG4i/dwwtINz3E+6vEqmQTR07qeN0b2ZAiYvaul+OlsLdKlMutK09ANcIzOLRGgWwnV7CQFyAcMYA6ceh91Ce+1pqKYt1HBKdGlKKU8DA4UQwyG/sNb3UspfKqRnigrDwlXiOaoX2RQkB0vAijO91/HN2s3SYARTmMcihS1XE7WSsqJ0iANZ9pjfrlMDqB16mRuu+S9Je+/RK3CXbtlFRMGjb8KyBbBvCwy4GdYs8azboEV1Pj0KXlvm+Tn+JI4OGBEluzULCRklfxyycGDL7EZ9AZh19tcSTvAThmiwnwW7S3is44IWzJK1ClotDOz1FUGJJ/PwVgArKqAvikrgnAV2LBDcQ2NWTb7MQXMOKWjTXZypys5jr7hAlkMWTaRO7YJNiwrvc0ho0glO7y5+XqMOcG5/8e21GmrWYYe44sLX1pw/tlQSu04N4Mmvk8mzhxJitBJizMFqjyzzxzixT3NR+sL21XofKmH+npk2rOZPPMyX7ESbpyiRBQLotPD0PfmK54zeLCmARUo+qdvJRfAiwN9eheh5cPZloIRpW5mL4MJQFcFZA/Ekl6aimnLOAr8Mhzq5YcQTxtAP6/FayjEamo0kWM7QISWCXXGZHDDnEFYRlt+vifDlo/q4mhsRkg73YgeQV8JN86s/a3MDTKHwSHJx0avTAi6VHESx9XgcefZQHNJEnl1SKyIHa7ZvQSqeEllXEztX4bx7WsVFdJppwxaeyl+3cJgh/Bu70yp2Pii4ipvQ/5O6CLoRvnZ52hxRao2EOhMKphD4A1EPzr4I9jICYzLmK8ELcl588cUmCxYsaCylZPLkyWf//ve/+5zXsFJyYgohXhNC7BFCbBdCfC2EqOeyb6YQYr8QYq8QYlRl9K+mcCYFHFbQquoJQvIEr6U0J6R/LWYObM3EZxrz7LDWxCXWZdQr9bFYPEwz5Q2HLPDBbTCzoTYZ3JFHWeNjbrly1v12Wy5Iu/b+0/PFx/Tu/6LUZnu1TCHEaMUg8ggx2Rn7aMVlmsnJKm4lfjq78tKYaVbfo4wUsRSIWRFBK2Qty4K9UttnxMGrZ36F+lOhzTJNdIzRXvSiNoR2KXm3zChb7ABMzb24pqKi2bBhQ/iCBQsab968OTU1NXXXTz/9VG/nzp1hvrZbWUmgfwa6SSl7AGnoE9yFEF2Au4CuwI3Au0IIYyX1sdrTJA4MLhGEBpMg45kQItaH6PkvBKY8wYMPRXP7s41pEV+HcxYfA1wOWWBWb3jcBI8ZtETMO5a4T9jsKcIAjdqXfox0aHPv3hqsWZJO2prh8bXQvJfb07o2W8cbt8Xz4ISveGNlOFNmaYEofUdCiw7l77InZJcwW2KV9/Pm/YaZNjzPjUSIEIwIjEIQiv4VLfKc0tJu4iVuYh4TmWprx9SM3aw+vBhzTgbUnVxwYOMXSrlimGYJhnSABtOgSyY0/wBEBOWfDhICjaaV81yFO3aSFPk5M6N3klS2v98DduzYEdG7d+8rUVFRjpCQEAYNGpT52Wef1fO13UpxaUopXWf9rgNu15fHAZ9JKXOBQ0KI/UA/oPqnpq9AzlkgdTZc3gsN+mu3jZyzkJnqPEIUupUIhyZ+WGFjSh7jzEfIQ8vMkkJMyW7OQxZY+Hu4cAjC60CrPiVMGPcR6dAqGXh6rDOtmHM+XlszTN+iCeHyFyGjsIXQtcspuj5/Z/762ATttcsCfx6qeUwrkqETKvZ6RTHThmQeIoX9xNEBM22wcJgFYgNzpUWfbyf4wvQIZtpoJ4WYIew6iEqBpnGF58Y5XYtnZmqBJdoJEP1v927HWmaISYZLC+DiPDz3CIRAkxe0qQpqbp7f2ElS5BuMibWRZ1jGHMeTLE3r5mM+zV69el39v//7vxbp6enGyMhI+fPPP9ft2bOnzzk6g2EM7wHgc325BZoAOjmubyuGECIBSABo3bp1IPsXFJyzwLYZcOUAxNzrrINXnP2JcHwxtJwAHdzcK/YnwsaHwJlSM7MMnciXPgHCBPvW25j0UFNWT77EPnMOC7iE+dBWLdCkVkPIPq8FiAC8OYj8m1H2hcCIXXn5YgqcOwDjXD7IQQnQvDu8E18wJti0MzzjftywqxneXqW5GX/9puKi7tt1r5jrlIaZNgVi5rI+WfQtJISFqGUuWWgaJGivbEtBUunSRMnZlqgDF2Z71ukGT0Ajd9kSFb6wi+QoG3kGiQMbNsMukqN8Fbxrr70257HHHkuPj4+PjYiIcHTt2jXbaPTd2Vdi8mifGxYiCXDnnP+rlPIb/Zi/An2A8VJKKYT4N7BOSrlQ3z8f+LGs6urVPXn08v5woch86+iREFckZH1/ImwsMv4/Yi00Mrvsn0q5hsgi20DWMZB27WQHkqyGDsKEg55t36KXeTr50Xoh4dBplOaqDFIS+/6Bxd0nMCEsgoROwwrvdEaKuovsLIFdFk34zp/U8mTu9zAlZXnoMRTeXhm49qscFxIhczFETYD0p8Bdvtioe9VUBDf4I3l0gYVnM5gw+cXCK8qjjz7aomXLltYZM2aUMFhfmPIkj/YJKeWI0vbrtfXGAPGyQHVPAK1cDmupb6txnLPAqnFgLeHXm74ctk4vbOkddzO2kzRQEz2ATQ9TLrEDyDqineu0+AxA1HkjAiN7zj0FDhu9Bj2jHZR3NejFbsp4bRxvOUAeJLhODG9r9rqieVcz/EMvmrXLAk/GQ55Vi+soy+UphPZradFBm85QFieDpk5JkOC0Dp3LAMcmQdZ3YGoHzd9VLswA0o0RWU+yNG0XyVFdic/0l9idOHHC1KJFC9u+fftCv//++3obNmzY42ubleLSFELcCEwDhkkpXSfLfAt8IoR4A2gOdAQ8yCVVvThngSQXb2BJ7JkNLW8tsOCyjrk/LmkgBdVYykuxQtei0M7j+yfoghdohBakIsufYX9+3wf1prTw+fn2IoLnI13N8EZywdy5gztgzsPgKNJlgxGeeBcuny+YY7fLAs+Mg0vntJRkdlvx82641399rbYoa65C6caILH9bdbfcckv7jIwMk8lkkm+++ebRRo0a+VxWo7LG8P4NhAE/C22ezjop5VQp5S4hxBfAbsAGPCKlD3e2KsqZFDy2xM6kFAjeldKsA1/ErkQKOtmyQ3Hz0tKmFykd+hK3fwPmw1v9d9lGbeGsm0nmHtL8skvFESFoHoBpdV3NBZPEu5q1cbetKXAlQ3tv2FybU1d0InlXM3zjMttol0XL1rJrHVw+ByPuCZ7KCgpFINm0aZMXVYg9o7KiNEsM6JZSvgS8VIHdCTqaxHl+7PZn4OCHMGABhNQBqw/R/d6hiZ3BmEVsr38Vs+4sbXoR/8iHWI0hhNrzSH7nAf+InikUuo+HXzwMVACmj3qZr7pNYPzOxcxa8RLTTm/n+27j8yNNp1XAt8BVACviPIVCUZxgiNJUFKGRGUQoSA/neV/Zr7kthQdZ+ctH0ZLWWvLkmGsWYh71e7dnpHToi9UYgt1owqqvuxc8Z3ulIPQ6dN3GQvy0gvE1D0Rv+qiXmR03A0B7D6/LrOsfZqUdUhwQZwCzmumpUNQIlOAFKe0egANzvTvHU4H0nsLjdSDpc/0UOnT7QNvUaaRWascl0COOq4Q6DmO12wi15xG3f0MJbZcldka4+cXiEZPjZmmTzb+YUuKpAP8e+Ce9HW287t/X/Z5ZaCKnhE6hqFkowQtS2k72XvACS4EwxVyzsEDsDMZiYgdaIdlkQxtSLuwhbsu3mMsrxrUawA0lzJ1yzpt7fxxkFYSzWloPYPbQpzlZpzlXTeGFTskx+pydSKFQVFEqK7WYogwamaFez8ruRVGsdLruVRc3poDb3y0xhN9MBDMb9MYc/5yWyeT6UtI5FRGmfLLOlV7TDrTad3r0pqXNYIYlrGRJ19tY36o/0lDYjHMYTVgCGAaVmAf9c+C2XNxeZ5cFFr2ivSsUiopFCV4Q0+c9gug3JIgekk2vqeuhaRfofis8/mtBei5PGDcLrhnpft/4t0rIZynh43vci96vibBoMuTl4CxJk9J5LHlGk/uipPp6SkAiVjWxm2KD9RKWOGCgFUblFOx3zs+b/6z2/l2iEj+FoiIJmtupojiNzDBiDTQeWnZASkgDqNUGancMXH9aTqoPf/gantmlvXs5ORuAh5fBHfMgujNERWviecc8TTjveFcPUNFINE9k1NREEjv11ZJMf3BbgfD9msj0S+fpOPl7po96CRBgDCUj9gYKytPgVvhetLm3vnzleTcTzJdLmKTXq92aAlarNq8u9yq8MRU++Ksmfkr0SiDbAude0d4VNYaJEye2adCgQc+OHTt2dW47ffq0ceDAgR1jYmK6DRw4sOPZs2e9HoUPWGqxiqS6pxZz5TM3c8aMkTDxSsH61ulwIBHsuXqshgMiWkG9rpB7AXLPQtZhcLgpdiDCQLorKG6AuypiRuQhC7wTT2KfMUy5U8+ib6tHnSx4benfSdj4H5j4LtNNdZnd/Y780/qd3MpvxhxGNTez3F5CAdIiRAK3GmBhCcN6liKRnEXXXZmUC4tKsBzDgaECNq6DLjeByAHh1GM0jf/DP+BeleaxMNkWOBKvRWOJUC1htMqY4lf8kVosEPz444+1o6KiHPfff3/bffv27QKYOnVqywYNGthefvnl9GeeeSb64sWLxvfee89tJq4KTy2mCAzRI7W0Yq60vLXweq9ZJSeXLsrW6VrGFifXvQ07X4ScIiXFRqzxuqvlo60ZbnuTxQ10wbLVA1s0l8NgyvhEptw2F+GQSINuCerRl+ub92KSUTDBAMsdns0kz0ITqR+vwvkiBR8sdoi3ghUIBd40weO2gvXk0ALRs9hLFjuAHDRLj/6w/QfoPAnCTrhM8JBappVAY9kCKRugbSjYT1VsBfWinCKR8yymIRNoRmG3+B4mkcGP1HPUopPUn8qkVU8qrQQvGLGQFLmO5KgBxGea/ZBxZfTo0Vf27t1byK/1008/1Vu5cuVegClTppwfNmzYNXiZelIJXhWj5YTCgtegH5h9yKLUaxbUbl+4wkK97rAiXrcQDXDdOwXZXCqEQQlMWPE8yzsB9nr6RqGbRQZk0b9a3Zr7xAGPmGCaEWZ7YY1eALpchd0uopfi0MTNjva+2KEJlwSuAjPyYKWx4FhPudwfMq/VBM9JxsDAC49lC8Q/YCc0A7ofNmDEAQjqxhxixuH+1GUILZlGHcyFxCiS7lwihbrEUQf/dPIUiRxAm06SoWUzzRe9HYziEstBwrnIC+xpBp1OARi0CgqKoMNCUuQjenmg/zLH8Q5L0/whekU5f/68KSYmJg+gVatWeefPn/dav5TgVTGcJX9KKwFUnjZd22lkhuHJWtqyJnEVLHY6CcOfZ/65layPagHSGcEp3NT81Cs06EspDpgVCh9dhTNFDy2FVLTIymkmzXKLM2iWnNOiQxaeMbhKguEq9AXeDAUjmjh6wvEnoeFPQB7IEDj8Dy86Wk5+2nCEXGsLWmSYMCDReiy5dKQdr3f5hcd39+QCSwqd4xQjJ0YaUJeh+cLoyh4mcYElGIgihhdoRgKXsbgVy/MUTkN3nsU0I4E9TNLEDvLN34tRwCkgvLey7oKUdXp5IIdeHmgdyVGBEDxXDAYDwoNhi6IowauCFBWoQNDIXDlC58pvjYZRx36ETGsUBXJT9I+88PocmyZW93lp5YEWWbnUCqt0d2VyqCagDYGZbgJSJFpm88etWh0sT30rl/vDtuVQdxVcGgqm/t71szz07ruSkNDb9ZuEMzBIU5Xze7t41IadC1xgCRdYgpF6RJNAW2axh0mcYxEADrI4wBQO8CiQB4CBCLqRnC96DZlQSEwzWMmaor9X/ddtN8CaTtAitx5ty/mzKwLLAOIz/8sch7M80ADi3dRn8p2GDRvajhw5EhITE5N35MiRkAYNGnhdellFaSqCmsvGGKJDj+M+I0vxbWfRpgO8Vc4AGxsQZ4Xp1gJL73Gb5vYsifV4X8Pqcn849rT2/koFPHZe37sjb344kgF/TERgx5kxB6DhNe4L3JaGnQxOMJs1iHyxK0xe/pKDq2xnIOv1grDNSKA984igs36EmygpUfh1ImI5azCxBsGvRHrdX0XgMDMi6x2Wpt3P0ycC5c4EGDVqVMa8efMaAsybN6/hjTfemOFtGypKUxH0WLjKwKshaG44J86QjwCUOtCJQCvpkRGwK0BtIDOizMP8wiGmc4LZHLUMYOG4r8g+14RGnXbx+O6Ky3AQSgz9OAzAOpphI72cLYUxmJyyD1OUSrBGaY4dO7btunXroi5evGhq2LChbcaMGSfvvvvui7fddlv7kydPhrZo0cL69ddfH2jatKnbR1sVpamoUoziKKu5SndC2YYVIiRc7YjmlAi82IEWnOJm5oZfebgC83maqAdAa/M6njnTvOIu7IKVIxzjFWxk+CB2ALnsYRKdUHXvqiPffffdIXfbLRZLmi/tKpemIugYxVGWk81VJOvJJdfpujRedjkqsGJXUcwKWIWL4tQlDkEgLyhozzwEtUo96gh/5QSel3dyi4RzchFbchv61o6iRqEETxF0rC7JrjJeomDsyfWl8IQ6mOlOCtFMpQG3Es1UerAWz24Dglr0IoQWRbYbgFAE4TTiHs6xGEl2GW354XemP+9khV7g0GXPgm4UCuXSVAQdQ4hguctNsx9hXEsE6UYbu0JPccDaCEkIjXAAJpx1EgzACAHLwrXJ4DPyYLX0/vZqRPtiuEs440/urYTHzTqY86MlT5HIPh5E+4lLnkzYgFvdTkWAgnFBCXrwSum3FEEIEhve/1YMxfuoe7bPh+1VEZwKj1CCpwg6ltE6fwxvCBEso3XBTiNaNAlQmmViNmoTw53pwFLserYTD7Dj+Zw6V+aZ4J822Id2Lw7FvWgK4J5SUppVBN9tWcLSDYfo3bcu3XrnlXBUKBG0L7Wd83xVaL1A0IpTl5HE8DyXSMFGBhdYipV07KXEwPZgbf5cvpO8o4lqkd9jw9xrtOgihaIMVJSmosbgrGbgpDPanOaMUs4pqx77PBMkhBTeVjTn5nQrfOWA8YaKHbNzxy4LfPsJvL4slzrnjTS+Iuh99yImfeS+cn0BBtrzXrE0YE4Lz0kLpnGFrQUTyPVzW/AUbSme7+4yFrYz0O0VG3FvsaCUy1jYQTxSTwTb4kpn2tbxflqFQiNYozR9RUVpKmo83Q3aH7xT8w4A7Sld8Mp6HFzsgKI5AIpWU58ViptbfcXjLE+UmwNdZChCl/M9H/+OZU1PMGrWM6Wc7eAADxNJ90KuTaeInecrGjLeraiVxiVSSthjdBuBWQczg8guMO7reHU5RQ1HBa0oagwpjsKuSitaSjFfmFCFvkFbUyDPilY6EACpix5s/OAPHLUMKKMFO2dYUGxrW2bRh31eix1okaMGCk9ENBDF4BLcooqagbvyQB9++GH9Dh06dDUYDNetWrWq9FDgEqhCX1eFwjec+TH9xTRjcXdmMNMrDkJCwWAEo9E1AylcvdiQ/8QneyB6/qUOZrqRTAwv04O1DEYykMtln6io1jzwwAPnvv32232u23r16nV18eLF+/v06XOlpPPKQrk0FTUGsxFWhMIC3czbbNfSgpWHacbKH4/zlq5meCNZs/R6xQl+WrKFpA9acvViQ5AGbNYQDqXE0dq8rsiZQv8/lCZM9nu/XCNHFVWTJFZEJrMiKp7hmSMYHpDyQNdee63PqXWU4ClqFEXH10blFI/eNABPGaGegAwJb9i1cT8BtAaecROoUlXoai4oRdTV3Jsht+7gr/GR2KwhmELzaBuXkn9sbfrRlAcDUiJIUX1IYkXkGG6PzSPPMId3HEv5Ms0fohcIlOApajTLwss+JhgCTgJFP3N33kyGJSkzaRuXQmvzOgzUphvLC4mbEjpFSSSzIiovvzxQniGZFVFK8BQKRVCiWX2vVHY3FFWUeIZnzuEdh408g4kQRzzDA1IeyB8owVMoFApFuRnB8KylfJnmzzG8QKEET6FQKBQ+MYLhWf4UOtfyQE2bNu0xY8aMkw0bNrQ9/fTTrS9evGi67bbbOnbu3Dl7zZo1+8purQAleAqFQqEIKkoqDzR58uQMX9pV8/AUCoVCUSNQgqdQKBSKGoESPIVCoVDUCJTgKRQKhaJGoARPoVAoFDUCJXgKhUKhqBEowVMoFApFUOGuPNCUKVNatm3btmtsbGyXG264of25c+eMpbXhjkoVPCHEX4QQUgjRSF8XQoi3hRD7hRDbhRDXVmb/FAqFQlHxuCsPNGrUqMtpaWm70tLSdnfo0CHn2Wefjfa23UoTPCFEK2AkcNRl82igo/5KAN6rhK4pFAqFwguS2Bg5k7nRSWyM9Ed7o0ePvtK4ceNCVYDHjx9/OSREK1NiNpuzTpw44XWBrsq08OYA03BWoNQYByyQGuuAekKIZpXSO4VCoVCUSRIbI8cwLXY2i1qMYVqsv0SvND766KNGN9544yVvz6sUwRNCjANOSCm3FdnVAjjmsn5c36ZQVBksafDKN9p7VWxfofCGZDbq5YEkNmyGZDZGBfJ606dPjzYajXLq1KkXvD03YLk0hRBJgDsf61+BZ9Dcmb60n4Dm9qR169a+NKVQ+AVLGixYDR+uBLsDQk2Q/AyYY31vd/ZSOHkR4jrDmz9Bnh2QEGKE0BB4eATMuscvP4ZC4RXx9MmcwxcOGzaDCZMjnj4BKw/09ttvN1y2bFm91atXpxkM3ttrARM8KeUId9uFEN2BtsA2IQRAS2CzEKIfcAJo5XJ4S32bu/YTgUSAPn36SHfHKBQVhWXlDuLfv4arDhNabXRBjhVSUjXBm/4JfPIrtGsKr97luQha0mDg8wXr6w8U3m+1a6/ZS7V1JXqKimYEfbKWMjstmY1R8fTJHEGfgJQH+vLLL+u89dZb0atXr94bFRXlKE8bFV4tQUq5A2jiXBdCHAb6SCnPCSG+BR4VQnwG9AcuSSlPVXQfFQqvOGQhZdH3XHW8iCZ2EpBIBK98o1l8+9O1Q49f1ATMpD+cdmsF795fsgCO9KIua+IKJXiKymEEfbL8KXTuygPNmTMn2mq1Gq6//vpYgGuvvfbKJ598crSstlwJtvJAPwA3AfuBbOD+yu2OQuEB+1P455U/6ysSTfQ0MnMgM734KTb9+XTrEU0AR3aHZTMLH2NJgyu5nnfDZiv7GIWiKuCuPNATTzxxztd2K33iuZSyjZTynL4spZSPSCnbSym7Syk3Vnb/FIoy6RDHBRrpK04LzzuW74BWjxYORIl70bs2rm3r9WUVihpFpQueQlHlaWsmxOA6pCBKPLQ0jl+AIf+niZ4lTRub84Z7B5XrsgpFjUEJnkLhB/59vzNYpWw6N4dQo/uj7Q64420t2MVbHv1ITVVQKEpDCZ5C4QcS4j0/9sRFSHkWHJ9Av3bF9x+/AH//0vs+5NnLJ5QKRU1BCZ5C4SfWPu/ZcZevQvzLMOkd2FhsaF7D5qU7E7Q5eXGdvT9PoagpKMFTKPyEOVYTvQ5Nyz72qhUW/QoOP8wgDTHCrX1g5bO+T3JXKKozSvAUCj9ijoV9czThCw3gpB+DgHkPgvwErP+Fr59UYqeoPrgrD/TYY481j42N7dKpU6cugwYN6nj48OEQb9tVgqdQ+BlLmjaW5i/3YkwjiAgBo4CWDWBqPKx5zrtxQ4WiKuGuPNBzzz2XnpaWtnvPnj27R48efemZZ57xurBAsE08VyiqNJY0bXzOavOfhXf8gha96VyuE6GsOUVwkURaZDJpUfHEZo4g1ueMK6NHj76yd+/eQuV/GjRokD/3Jysry6CnpvQKZeEpFH4kJVUTO7tDeze5+YYZDZo78tY+WpRmad/bqfEFYufkqw3+7bNC4QtJpEWO4YPY2axoMYYPYpNIC1h5oD/96U8toqOje3z55ZcNX3vttZPenq8ET6HwI3GdNcvOaNDe3ymSHO/WPrD675o78usn4bd/wA3d3Le19nmYPERry5XxfQPSdYWiXCSTFpWHXS8PZDckkxaw8kD/+te/TqSnp2+//fbbz7/22mtNyj6jMMqlqVD4EXOsVhLIOYZnjoXurQqvF2XZTBj1CqzeC0OuKZ5Tc/XfYcZncPA03DNIJYhWBBfxxGbOYZXDht1gwuiIJzZg5YGcPPDAAxduuummjnPmzPHKylOCp1D4GXNsYWEruu6OoiJXtL2Vf/dP3xQKfzOC2Kyl/CHNn2N47tixY0dY9+7dcwG++OKLeu3bt7/qbRtK8BQKhULhEyOIzfKn0LkrD/TTTz/VPXjwYLgQQrZs2dI6f/78I962qwRPoVAoFEFFtS0PpFAoFApFRaAET6FQKBQ1AiV4CoVCoagRKMFTKBQKRY2gWgStbNq06ZwQwuuIHQ9oBPg8UFoDUJ+TZ6jPyXPUZ+UZvn5OMf7qSFWgWgielLJxINoVQmyUUvYJRNvVCfU5eYb6nDxHfVaeoT4n71AuTYVCoVAEFe7KAzl57rnnmgohrjt16pTXBpsSPIVCoVAEFe7KAwHs378/JDk5uU6zZs2s5WlXCV7pJFZ2B6oI6nPyDPU5eY76rDwjKD6nJE5FzmRzdBKn/FIpYfTo0VcaN25sK7r90UcfbfXaa68dL09pIKgmY3iBQkoZFH9MwY76nDxDfU6eoz4rzwiGzymJU5FjSI7Nw2GYw27HUuLTRtDM7/k0Fy5cWK9Zs2Z5ZrPZ6xyaTpTgKRQKhaLcJHMqKg+HwQHYcBiSORXlb8HLzMw0zJ49O3rFihXF3JzeoFyaCoVCoSg38TTLDMHgMAImDI54mvm9PFBqamrY8ePHw3r06NGlRYsW3U+fPh167bXXdj569KhXRpsSvFIQQvxFCCGFEI30dSGEeFsIsV8IsV0IcW1l97EyEUK8JoTYo38WXwsh6rnsm6l/TnuFEKMqsZtBgRDiRv2z2C+EmFHZ/QkWhBCthBArhBC7hRC7hBCP6dsbCCF+FkLs09/rV3ZfgwEhhFEIsUUIsVRfbyuE+E3/u/pcCBFa0X0aQbOspcSnPU23E4FyZ/br1+/qhQsXtp04cWLHiRMndjRt2tS6efPm1NatWxcb5ysNJXglIIRoBYwEjrpsHg101F8JwHuV0LVg4megm5SyB5AGzAQQQnQB7gK6AjcC7wohjJXWy0pG/9nfQfv76QLcrX9GCrABf5FSdgEGAI/on80MIFlK2RFI1tcV8BiQ6rI+C5gjpewAXAQerIxOjaBZ1itcm+4vsRs7dmzbwYMHdzp06FBY06ZNe8yZM6eRP9pVY3glMweYBnzjsm0csEBKKYF1Qoh6QohmUspTldLDSkZKudxldR1wu748DvhMSpkLHBJC7Af6AZYK7mKw0A/YL6U8CCCE+AztM9pdqb0KAvTvzil9OVMIkQq0QPt84vTDPgZSgOmV0MWgQQjRErgZeAl4UmihitcD9+iHfAw8TzV4EHdXHsiVEydO7ChPu8rCc4MQYhxwQkq5rciuFsAxl/Xj+jYFPAD8qC+rz6kw6vPwACFEG6A38BvQ1OVBMh1oWln9CiLeRHsId+jrDYEMKaXTraf+rsqgxlp4QogkINrNrr8Cz6C5M2s8pX1OUspv9GP+iuaaWlSRfVNUH4QQtYHFwONSysuu86yklFIIISutc0GAEGIMcEZKuUkIEVfJ3amy1FjBk1KOcLddCNEdaAts0790LYHNQoh+wAmglcvhLfVt1ZaSPicnQoj7gDFAvO7qhRr4OZWB+jxKQQgRgiZ2i6SUX+mbTzuHC4QQzYAzldfDoGAQcIsQ4iYgHKgDvAXUE0KYdCtP/V2VgXJpFkFKuUNK2URK2UZK2QbNTXCtlDId+BaYrEdrDgAu1dTxO9AiD9FcLLdIKbNddn0L3CWECBNCtEUL8llfGX0MEjYAHfWIulC0gJ5vK7lPQYE+DjUfSJVSvuGy61vg9/ry7yk8ll7jkFLOlFK21O9JdwG/SCnvBVZQMHZe4z+nsqixFl45+QG4CdgPZAP3V253Kp1/A2HAz7o1vE5KOVVKuUsI8QVaUIYNeERKaa/EflYqUkqbEOJRYBlgBD6UUu6q5G4FC4OA3wE7hBBb9W3PAK8CXwghHgSOAHdUTveCnunAZ0KIfwBb0B4eFCUgCrxQCoVCoajpbNu27XDPnj2rdC3Cbdu2NerZs2ebotuVS1OhUCgUQYW78kBPPvlk8yZNmvTo1KlTl06dOnX5/PPP63rbrhI8hUKhUAQVJZUHmjp16uk9e/bs3rNnz+4777zzkrftKsFTKBQKhU8kkRU5kzPRSWQFtDyQryjBUygUCkW5SSIrcgzHY2dzocUYjsf6S/TcMX/+/CaxsbFdJk6c2Obs2bNepytUgqdQKBSKcpNMVlQeUi8PJA3JZEUF4jpPPPHEmSNHjuxITU3dHR0dnffwww+3KvuswijBU1RrhBDRQojPhBAHhBCbhBA/CCFiK7tfviCEiBNCDCxhXychhEUIkSuEeKqi+6aoecQTmRmC0MsDCUc8kX4vDwTQqlUrm8lkwmg08uijj57dunWr15akmoenqLbok5q/Bj6WUt6lb+uJlpcxrTL75iNxwBVgrZt9F4A/A7dWYH8UNZgRRGYtpWVaMllR8URmjiDS7+WBAI4cORISExOTB/DZZ5/Vu+aaa7yufK4ET1GdGQ7kSSnnOjc4E4LrYjgbrWSPBP4hpfxcz1P4ApABdAe+AHaglWWJAG6VUh4QQnwE5AB90NI8PSmlXCqECEfLVt8HbdL9k1LKFXoKtluAWkB74Gsp5TS9LyP1a4YBB4D7pZRXhBCH0TLgjwVCgIn6NacCdiHEJOBPUsrVLj/fGeCMEOJm/3yECkXZjCAyy59CN3bs2Lbr1q2Lunjxoqlp06Y9ZsyYcXLlypVRu3fvjgBo2bKl9T//+c8Rb9tVgqeoznQDNpWwbzzQC+gJNAI2CCFW6ft6Ap3RrKWDwAdSyn56cdI/AY/rx7VBK/3THlghhOgAPIKW77i7EKITsNzFhdoLrRpALrBXCPEv4CrwN2CElDJLCDEdeBL4P/2cc1LKa4UQDwNPSSn/IISYC1yRUv6z/B+NQhG8uCsP9MQTT/g8GV4JnqKmMhj4VE95dloIsRLoC1wGNjhzpAohDgDOun870KxGJ19IKR3APiHEQaCT3u6/AKSUe4QQRwCn4CVLKS/p7e4GYoB6aEVhf9XTs4VSuG6gM5nyJjSRVigU5UQJnqI6s4uCxLrekOuy7HBZd1D4O1M0L19Zefpc27XrbQngZynl3WWc4zxeoVCUExWlqajO/AKECSESnBuEED2EEEOA1cCdQgijEKIxMBTvKzpMFEIYhBDtgXbAXr3de/VrxQKt9e0lsQ4YpLtDEUJEehBFmgkEJPRboajOKMFTVFv0+ny3ASP0aQm7gFfQKmh/DWwHtqEJ4zS9BJQ3HEUTyR+BqVLKHOBdwCCE2AF8DtwnpcwtqQEp5VngPuBTIcR2NHdmpzKu+x1wmxBiqy7e+ejTMI6jjQP+TQhxXAhRx8ufS6GolqhqCQpFOdCjNJdKKb+s7L4oFP5EVUtQKBQKhaKKowRPoSgHUsr7lHWnUAQGd+WBAF566aUmbdu27dqhQ4euU6dObeltuyrqS6FQKBRBxQMPPHDuscceO3P//fe3dW777rvvor7//vt6u3fv3h0RESFPnDjhtX4pC0+hUCgUPpFkJ3JmHtFJdgJWHui9995rPG3atFMRERESoEWLFl6XD1KCp1AoFIpyk2QncoyV2Nk2WoyxEusv0SvKwYMHw1euXBnVo0ePTn379r1m5cqVtbxtQwmeQqFQKMpNsoOoPNDLA2FIdgRmjqjdbhcXLlwwbt26dc/s2bOP3XPPPe0dDodXbSjBUygUCkW5iTeQGQJ6eSAc8QYCUh4oOjraevvtt2cYDAaGDx+ebTAYZHp6ulfjeErwFAqFQlFuRhjJWhpK2tMmTiwNJW2EkYCUBxo7dmxGcnJyFMD27dvD8vLyDNHR0V6N46koTYVCoVD4xAgjWf4UOnflgf785z+fu/POO9t07Nixa0hIiCMxMfGQweCdzaYET6FQKBRBhbvyQADffPON2+2eolyaCoVCoagRKMFTKBQKRY1ACZ5CoVAoagRK8BQKhUJRI1CCp1AoFIoagRI8hUKhUNQI1LQEhUKhUAQVEydObJOcnFy3YcOGtn379u0CuPnmm9sdOHAgHCAzM9MYFRVl37Nnz25v2lWCp1AoFIqgwl15oO+///6gc/mPf/xjy7p169q9bVe5NBUKhULhE0kXiZx5kOiki4ErD+TE4XDw3XffNfj9739/wdt2lYWnUCgUinKTdJHIMTuIzZMY5hzHsbQ7aSPqByafJsCyZctqN2rUKK979+653p6rLDyFQqFQlJvki0TlSb08kMSQfDEw5YGcLFy4sMGECRO8tu5ACZ5CoVAofCC+PpkhQi8PJHDE1w9MeSCAvLw8fvrpp/qTJ08ul+Apl6ZCoVAoys2I+mQt7U5a8kWi4uuTGUh35jfffFOnXbt2Oe3bt88rz/nKwlMoFAqFT4yoT9Yr7Uj3l9iNHTu27eDBgzsdOnQorGnTpj3mzJnTCODTTz9tMHHixHJZd6AsPIVCoVAEGSWVB1q8ePFhX9pVFp5CoVAoagRK8BQKhUJRI1CCp1AoFIoagRI8hUKhUNQIlOApFAqFokagBE+hUCgUNQIleAqFQqEIKiZOnNimQYMGPTt27NjVuW3t2rURPXv27NSpU6cu3bp167xixYpa3rarBE+hUCgUQcUDDzxw7ttvv93nuu3pp59u+de//vXknj17dj/77LMnp0+f3srbdpXgKRQKhcInko4SOXMN0UlHA1ceSAjBpUuXjAAZGRnGpk2bWr1tV2VaUSgUCkW5STpK5JhviM2zY5izBcfScaSNaO3/fJpvv/32sZtvvrnjs88+28rhcLBmzZo93rahLDyFQqFQlJvko0Tl2fXyQA4MyUcDUx7o7bffbvzKK68cS09P3/7yyy8fu++++9p424YSPIVCoVCUm/jWZIYYcRgFmAw44lsHpjzQ4sWLG06ePDkD4IEHHri4fft2r92nSvAUCoVCUW5GtCZr6TjSnr6OE4FyZwI0btw474cffogC+O6776JiYmJyvG1DjeEpFAqFwidGtCbLn0I3duzYtuvWrYu6ePGiqWnTpj1mzJhx8r333jvy5JNPtvrLX/4iwsLCHHPnzj3ibbtK8BQKhUIRVJRUHmjXrl2pvrSrXJoKhUKhqBEowVMoFApFjUAJnkKhUChqBErwFAqFQlEjUIKnUCgUihqBEjyFQqFQ1AiU4CkUCoUiqHBXHshisUT06tWrU2xsbJfrr7++w4ULF7zWLyV4CoVCoQgq3JUH+uMf/9jmpZdeOp6Wlrb7lltuufjCCy9Ee9uuEjyFQqFQ+ETSTiJnfkZ00s7AlQc6cuRI2OjRo68AjBkz5vLSpUvre9uuEjyFQqFQlJuknUSOeY3Y2d/RYsxrxPpL9IrSoUOHnEWLFtUDWLhwYYP09PRQb9tQgqdQKBSKcpO8k6g8GwaHBJsNQ/LOwJQH+vDDDw/PnTu3cdeuXTtnZmYaQkJCpLdtqFyaCoVCoSg38d3InPMjDpsNg8mEI75bYMoD9e7dO+fXX3/dB7B9+/aw5cuX1/O2DSV4CoVCoSg3I7qRtfRp0pJ3EhXfjcwR3QJTHujEiROmFi1a2Ox2O88991yzBx988Iy3bSjBUygUCoVPjOhGlj+Fzl15oCtXrhjmz5/fBOCmm266+Oc///m8t+0qwVMoFApFUFFSeaBnn33Wa6vOFRW0olAoFIoagRI8hUKhUNQIlOApFAqFokagBE+hUCgUNQIleAqFQqGoESjBUygUCkWNQAmeQqFQKIKK/fv3h/Tv3z+2ffv2XTt06ND1xRdfbAJw+vRp48CBAzvGxMR0GzhwYMezZ88avWlXCZ5CoVAogoqQkBBef/314wcOHNi1YcOG1Pnz5zfZtGlT+HPPPdcsLi4u88iRIzvj4uIy//73v3tVIkhNPFcoFAqFTyRZiEy2EBVvJnOE2feMKzExMXkxMTF5APXr13e0b9/+6tGjR0N/+umneitXrtwLMGXKlPPDhg27BjjhabtK8BQKhUJRbpIsRI6ZSmyeDcOcj3EsnUuaP0TPyd69e0N3795da9iwYVfOnz9vcgphq1at8s6fP++VhimXpkKhUCjKTbJFLw/k0MsDWfxXHujSpUuG8ePHt3/11VePNWjQwOG6z2AwIITwqj0leAqFQqEoN/FmMkNMOIwGMJn+v737j22yTuA4/umzVpy1ihuBzo6bZGsp4xjK1GSRGLQ1mzr/wMBh/EONZ+JiUKPJ/JUYnInJZfxh1ORy4IH8YUTxFyYzF0IbGZL4j0sYeBsWDhnHpLtxYzLL3No9vT/YLjuOmj1PWyx53q8/V/ik/33yPN8+z0dmpKkw80ATExOu+++/v3b9+vUjjz766KgkVVZWZgYGBjySNDAw4KmoqMj8ashFKDwAgG3RJqW6/qJE+x81WKjbmaZp6qGHHqoJhUK/vPbaa0Mzf29ubh7dsmVLpSRt2bKlsqWlZdRKLmd4AIC8RJuUKuS53d69e6/dvXt3ZTAYHA+Hw/WS1NHRMdjR0XF67dq1tTU1NQsCgcDk559//g8ruRQeAKCkNDc3/5zNZnsu9dk333yTsJvLLU0AgCNQeAAAR6DwAACOQOEBAByBwgMAOAKFBwBwBAoPAFBScs0Dbd++/Ya6urrlhmE07t+//xqruRQeAKCk5JoHuvnmm8c//fTTY7feeuvPdnJ58BwAkJd9MXn3xeVbE9HYmmjx5oHWrl17Lp9cCg8AYNu+mLx/aFUonZbx5zdl7upSohClN2P2PFC+WdzSBADYti8uXzp9YR4onZGxL3555oHsoPAAALatiWjM45FplEket8w1keLNA+WLW5oAANvWRJXa1aVEIc/wcs0D5cuVzWYLlQUAuML19vaeWLly5Znf8jvs2bPn2paWlqXBYHDcMC7ciOzo6BicmJhwtbe3/+7s2bNun883tWzZsvMHDhw4evH/7+3tXbBy5cqbLv47V3gAgJLya/NAjzzyyKjdXM7wAACOQOEBAByBwgMAOAKFBwBwBAoPAOAIFB4AwBEoPABASck1D/Tkk09WL1myZHkoFKq/5557as+cOVNmJZfCAwCUlFzzQM3NzecSicTfE4lEX11d3S+vvvqq30ouhQcAyEtPTN6tL8vfE5O3EHk1NTXp1atXn5f+dx7owQcfPOfxeCRJTU1NqcHBwaus5FJ4AADbemLyvtSq0M5OBV5qVahQpTcj1zzQjh07FrS0tPxkJYvCAwDY1hOXL5OWkTWlqYyMnsswD/Tiiy/6y8rKsm1tbSNW8ig8AIBtjRGNuafngcrcMhuLPA/09ttvV+7Zs2f+Z5999sPMi6XnipdHAwBsa4wq9acuJXri8jVGNNZYxHmgTz755Lq33nrL//XXX3/v8/ksD8JSeACAvDRGlSpE0c3Yu3fvtbt3764MBoPj4XC4XrowD9Te3r54cnLSuPvuu0OStGrVqp8/+OCDk3PNpfAAACUl1zzQhg0bLP1I5WKc4QEAHIHCAwA4AoUHAHAECg8A4AgUHgDAESg8AIAjUHgAgJKSax7o2WefvTEUCtWHw+H6O+64I3jixAmPlVwKDwBQUnLNA23atCmZSCT6jhw50nfvvff+9Morr1RZyaXwAAB5ORqT928vy3+0yPNAs18gnUqlDJfLZSmXN60AAGw7GpN3R6tCU2kZB96U+ViXEsECvmbs4nmgp59+OvDxxx9X+ny+qe7u7u+tZHGFBwCw7VhcvqlZ80DHijwP9M477wwmk8lD69at+/fmzZsXWsmj8AAAttVFNFbmkemangeqK/I80IzHH398pKur6wYrmdzSBADYFowq9ViXEsfi8tVFNFaI25m55oEOHz48b8WKFROStGvXrvm1tbXjVnIpPABAXoJRpQp5bpdrHmj79u0Ljh8/frXL5cpWV1dPbtu2bcBKLoUHACgpzAMBAJAHCg8A4AgUHgDAESg8AIAjUHgAAEeg8AAAjkDhAQBKSq55oBmbNm1a5HK5Gk+fPm3p0ToKDwBQUnLNA0kXyjAej19XVVU1aTWXwgMA5CUZk7f3ZfmTRZ4HkqSNGzcu3rx58ymr00ASb1oBAOQhGZN3f6tCZlrG92/KvLNLCX+R5oHef//9+VVVVemmpiZL79CcQeEBAGwbistnpmXIlMyMjKG4fIUqvNnzQB6PR52dnf6vvvrqqN08bmkCAGxbFNGY4ZGpMslwy1xUpHmg/v7+eadOnZrX0NBQHwgEVgwNDV21atWqZSdPnpzzhRtXeAAA2/xRpe7sUmIoLt+iiMYKcXV3qXmg22+/fXxkZKR35t8EAoEV3377bX9VVVVmrrkUHgAgL/6oUoU8t8s1D5TvWgKFBwAoKbnmgWYbHBw8bDWXMzwAgCNQeAAAR6DwAACOQOEBAByBwgMAOAKFBwBwBAoPAFBScs0DPf/88zcuXLiwIRwO14fD4fqPPvroeiu5PIcHACgpM/NAq1evPn/27Fnjlltuqb/vvvvOSVJbW9vQ66+/PmQnl8IDAORlPCbveFy+8ojGygvwxpWampp0TU1NWvr/eaB8cEsTAGDbeEzeZKtCo50KJFsVGi/QJt6M2fNAkrRt27aFoVCofv369TcNDw+XWcmi8AAAto3H5ctOzwNlMzLG4/IVKnv2PFBFRYX53HPP/WtgYOBwf39/n9/vTz/11FOLreRReAAA28ojGnNNzwO53DLLizQPJEmLFy/OuN1ulZWVaePGjcMHDx60dDXJGR4AwLbyqFL+LiUKeYZ3qXkgSRoYGPDMnO19+OGH85cuXWpp+ZzCAwDkpTyqVCGKbkaueaCdO3dW9PX1lUtSdXX15HvvvTdgJZfCAwCUlFzzQPnu4XGGBwBwBAoPAOAIFB4AwBEoPACAI1B4AABHoPAAAI5A4QEASkqueSBJeuONNxYuWbJkeV1d3fK2trZqK7k8hwcAKCm55oF+/PFHz5dffjm/r6+vr7y8PDs4OGipwyg8AEB+Yhmv4hmfIu4xRd1Fmwd69913F7zwwguny8vLs5IUCAQyVnK5pQkAsC+W8ao1FVLnRECtqZBimaLNAx0/fvzq7u5uX0NDQ/i2225b2t3dfY2VLAoPAGBfPOPT9DyQMjIUzxRtHmhqaso1MjJSdvDgwSOdnZ3/fPjhh2tN05xzHoUHALAv4h7T9DyQ3DIVcRdtHsjv90+uW7du1DAM3XXXXecNw8gmk8k5H81ReAAA+6LulLq8CbXPG1SXN1GIM7xc80APPPDAaDwe90nSoUOH5qXTacPv98/5HI8frQAA8hN1pwpRdDNyzQM988wzZzZs2HBTMBhc7vF4zK1bt/5gGHO/bqPwAAAlJdc8kCR98cUXP9jN5ZYmAMARKDwAgCNQeAAAR6DwAACzmaZpun7rL2HX9He/5MN5FB4AYLbvhoeHr78SS880Tdfw8PD1kr671Of8ShMA8F+ZTOaJZDL512Qy+XtdeRdFpqTvMpnME5f60JXNZi/z9wEA4PK70tobAABbKDwAgCNQeAAAR6DwAACOQOEBABzhP1z1VNpuIz2+AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "perplexity = 200\n",
    "\n",
    "p = reduce_dims_and_plot(projections,\n",
    "                         y=clusters,\n",
    "                         title=f'perplexity: {perplexity}',\n",
    "                         file_name=None,\n",
    "                         perplexity=perplexity,\n",
    "                         library='Multicore-TSNE',\n",
    "                         perform_PCA=False,\n",
    "                         projected=None,\n",
    "                         figure_type='2d',\n",
    "                         show_figure=True,\n",
    "                         close_figure=False,\n",
    "                         text=None)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Soft-Decision-Tree given the self-labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unnormalized_samples = samples.clone()\n",
    "\n",
    "# for col, sensor in enumerate(tqdm(dataset.dataset.all_signals)):\n",
    "#     denormalizer = dataset.dataset.get_denormalization_for_sensor(sensor)\n",
    "#     unnormalized_samples[:, col, :] = denormalizer(unnormalized_samples[:, col, :])\n",
    "\n",
    "sampled = samples[..., range(0, samples.shape[-1], 200)]\n",
    "\n",
    "samples_f = sampled.flatten(1)\n",
    "tree_dataset = list(zip(samples_f, clusters))\n",
    "batch_size = 2000\n",
    "tree_loader = torch.utils.data.DataLoader(tree_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define how we prune the weights of a node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_node(node_weights, factor=1):\n",
    "    w = node_weights.cpu().detach().numpy()\n",
    "    mean_ = np.mean(w)\n",
    "    std_ = np.std(w)\n",
    "    node_weights[((mean_ - std_ * factor) < node_weights) & (node_weights < (mean_ + std_ * factor))] = 0\n",
    "    return node_weights\n",
    "\n",
    "def prune_node_keep(node_weights, keep=4):\n",
    "    w = node_weights.cpu().detach().numpy()\n",
    "    throw_idx = np.argsort(abs(w))[:-keep]\n",
    "    node_weights[throw_idx] = 0\n",
    "    return node_weights\n",
    "\n",
    "def prune_tree(tree_, factor):\n",
    "    new_weights = tree_.inner_nodes.weight.clone()\n",
    "    for i in range(new_weights.shape[0]):\n",
    "        res = prune_node_keep(new_weights[i, :], factor)\n",
    "        new_weights[i, :] = res\n",
    "\n",
    "    with torch.no_grad():\n",
    "        tree_.inner_nodes.weight.copy_(new_weights)\n",
    "        \n",
    "def sparseness(x):\n",
    "    s = []\n",
    "    for i in range(x.shape[0]):\n",
    "        x_ = x[i, :]\n",
    "        sp = (len(x_) - torch.norm(x_, 0).item()) / len(x_)\n",
    "        s.append(sp)\n",
    "    return np.mean(s)\n",
    "\n",
    "def compute_regularization_by_level(tree):\n",
    "    total_reg = 0\n",
    "    for i in range(tree.inner_nodes.weight.shape[0]):\n",
    "        cur_level = np.floor(np.log2(i+1))\n",
    "        node_reg = torch.norm(tree.inner_nodes.weight[i].view(-1), 2)\n",
    "        total_reg += 2**(-cur_level) * node_reg\n",
    "    return total_reg\n",
    "\n",
    "def show_sparseness(tree):\n",
    "    avg_sp = sparseness(tree.inner_nodes.weight)\n",
    "    print(f\"Average sparseness: {avg_sp}\")\n",
    "    layer = 0\n",
    "    sps = []\n",
    "    for i in range(tree.inner_nodes.weight.shape[0]):\n",
    "        cur_layer = int(np.floor(np.log2(i+1)))\n",
    "        if cur_layer != layer:\n",
    "            print(f\"layer {layer}: {np.mean(sps)}\")\n",
    "            sps = []\n",
    "            layer = cur_layer\n",
    "\n",
    "        x_ = tree.inner_nodes.weight[i, :]\n",
    "        sp = (len(x_) - torch.norm(x_, 0).item()) / len(x_)\n",
    "        sps.append(sp)\n",
    "        \n",
    "    return avg_sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_epoch(model, loader, device, log_interval, losses, accs, epoch, iteration):\n",
    "    model = model.train()\n",
    "    for batch_idx, (data, target) in enumerate(loader):\n",
    "        iteration += 1\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        output, penalty = tree.forward(data)\n",
    "\n",
    "        # Loss\n",
    "        loss_tree = criterion(output, target.view(-1))\n",
    "\n",
    "        # Penalty\n",
    "        loss_tree += penalty\n",
    "\n",
    "        # Sparse regularization\n",
    "#         fc_params = torch.cat([x.view(-1) for x in tree.inner_nodes.parameters()])\n",
    "#         regularization = sparsity_lamda * torch.norm(fc_params, 2)\n",
    "        regularization = sparsity_lamda * compute_regularization_by_level(tree)\n",
    "        loss = loss_tree\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        pred = output.data.max(1)[1]\n",
    "        correct = pred.eq(target.view(-1).data).sum()\n",
    "        accs.append(correct.item() / data.size()[0])\n",
    "\n",
    "        # Print training status\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(f\"Epoch: {epoch:02d} | Batch: {batch_idx:03d} / {len(loader):03d} | Total loss: {loss.item():.3f} | Reg loss: {regularization.item():.3f} | Tree loss: {loss_tree.item():.3f} | Accuracy: {correct.item() / data.size()[0]:03f} | {round((time.time() - start_time) / iteration, 3)} sec/iter\")\n",
    "            \n",
    "    return iteration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 5e-3\n",
    "weight_decay = 5e-4\n",
    "sparsity_lamda = 2e-3\n",
    "epochs = 500\n",
    "output_dim = len(set(clusters))\n",
    "log_interval = 1\n",
    "tree_depth = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT accuracy: 0.9663923520425762\n"
     ]
    }
   ],
   "source": [
    "tree = SDT(input_dim=samples_f.shape[1], output_dim=len(labels), depth=tree_depth, lamda=1e-3, use_cuda=True)\n",
    "optimizer = torch.optim.Adam(tree.parameters(),\n",
    "                                 lr=lr,\n",
    "                                 weight_decay=weight_decay)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "tree = tree.to(device)\n",
    "clf = DecisionTreeClassifier(max_depth=tree_depth).fit(samples_f, clusters)\n",
    "print(f\"DT accuracy: {clf.score(samples_f, clusters)}\")\n",
    "tree.initialize_from_decision_tree(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "accs = []\n",
    "sparsity = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.23297851082559695\n",
      "layer 0: 0.988950276243094\n",
      "layer 1: 0.988950276243094\n",
      "layer 2: 0.988950276243094\n",
      "layer 3: 0.988950276243094\n",
      "layer 4: 0.9271408839779005\n",
      "layer 5: 0.7726174033149171\n",
      "layer 6: 0.5253798342541437\n",
      "layer 7: 0.31677313535911605\n",
      "layer 8: 0.20474361187845302\n",
      "Epoch: 00 | Batch: 000 / 011 | Total loss: 3.302 | Reg loss: 0.019 | Tree loss: 3.302 | Accuracy: 0.045000 | 1.508 sec/iter\n",
      "Epoch: 00 | Batch: 001 / 011 | Total loss: 3.295 | Reg loss: 0.018 | Tree loss: 3.295 | Accuracy: 0.075500 | 1.287 sec/iter\n",
      "Epoch: 00 | Batch: 002 / 011 | Total loss: 3.289 | Reg loss: 0.018 | Tree loss: 3.289 | Accuracy: 0.128500 | 1.216 sec/iter\n",
      "Epoch: 00 | Batch: 003 / 011 | Total loss: 3.283 | Reg loss: 0.018 | Tree loss: 3.283 | Accuracy: 0.136500 | 1.146 sec/iter\n",
      "Epoch: 00 | Batch: 004 / 011 | Total loss: 3.275 | Reg loss: 0.017 | Tree loss: 3.275 | Accuracy: 0.150500 | 1.131 sec/iter\n",
      "Epoch: 00 | Batch: 005 / 011 | Total loss: 3.273 | Reg loss: 0.017 | Tree loss: 3.273 | Accuracy: 0.137000 | 1.124 sec/iter\n",
      "Epoch: 00 | Batch: 006 / 011 | Total loss: 3.266 | Reg loss: 0.017 | Tree loss: 3.266 | Accuracy: 0.135500 | 1.1 sec/iter\n",
      "Epoch: 00 | Batch: 007 / 011 | Total loss: 3.262 | Reg loss: 0.017 | Tree loss: 3.262 | Accuracy: 0.142000 | 1.096 sec/iter\n",
      "Epoch: 00 | Batch: 008 / 011 | Total loss: 3.251 | Reg loss: 0.017 | Tree loss: 3.251 | Accuracy: 0.145000 | 1.096 sec/iter\n",
      "Epoch: 00 | Batch: 009 / 011 | Total loss: 3.248 | Reg loss: 0.017 | Tree loss: 3.248 | Accuracy: 0.135000 | 1.082 sec/iter\n",
      "Epoch: 00 | Batch: 010 / 011 | Total loss: 3.236 | Reg loss: 0.017 | Tree loss: 3.236 | Accuracy: 0.156997 | 1.06 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 01 | Batch: 000 / 011 | Total loss: 3.290 | Reg loss: 0.015 | Tree loss: 3.290 | Accuracy: 0.069000 | 1.09 sec/iter\n",
      "Epoch: 01 | Batch: 001 / 011 | Total loss: 3.283 | Reg loss: 0.015 | Tree loss: 3.283 | Accuracy: 0.128500 | 1.089 sec/iter\n",
      "Epoch: 01 | Batch: 002 / 011 | Total loss: 3.275 | Reg loss: 0.015 | Tree loss: 3.275 | Accuracy: 0.195000 | 1.088 sec/iter\n",
      "Epoch: 01 | Batch: 003 / 011 | Total loss: 3.271 | Reg loss: 0.015 | Tree loss: 3.271 | Accuracy: 0.178000 | 1.081 sec/iter\n",
      "Epoch: 01 | Batch: 004 / 011 | Total loss: 3.261 | Reg loss: 0.015 | Tree loss: 3.261 | Accuracy: 0.173000 | 1.08 sec/iter\n",
      "Epoch: 01 | Batch: 005 / 011 | Total loss: 3.251 | Reg loss: 0.016 | Tree loss: 3.251 | Accuracy: 0.174000 | 1.081 sec/iter\n",
      "Epoch: 01 | Batch: 006 / 011 | Total loss: 3.248 | Reg loss: 0.016 | Tree loss: 3.248 | Accuracy: 0.147000 | 1.074 sec/iter\n",
      "Epoch: 01 | Batch: 007 / 011 | Total loss: 3.239 | Reg loss: 0.016 | Tree loss: 3.239 | Accuracy: 0.144500 | 1.076 sec/iter\n",
      "Epoch: 01 | Batch: 008 / 011 | Total loss: 3.231 | Reg loss: 0.016 | Tree loss: 3.231 | Accuracy: 0.149500 | 1.075 sec/iter\n",
      "Epoch: 01 | Batch: 009 / 011 | Total loss: 3.227 | Reg loss: 0.016 | Tree loss: 3.227 | Accuracy: 0.137500 | 1.076 sec/iter\n",
      "Epoch: 01 | Batch: 010 / 011 | Total loss: 3.212 | Reg loss: 0.016 | Tree loss: 3.212 | Accuracy: 0.174061 | 1.065 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 02 | Batch: 000 / 011 | Total loss: 3.279 | Reg loss: 0.015 | Tree loss: 3.279 | Accuracy: 0.153500 | 1.076 sec/iter\n",
      "Epoch: 02 | Batch: 001 / 011 | Total loss: 3.271 | Reg loss: 0.015 | Tree loss: 3.271 | Accuracy: 0.170000 | 1.075 sec/iter\n",
      "Epoch: 02 | Batch: 002 / 011 | Total loss: 3.267 | Reg loss: 0.015 | Tree loss: 3.267 | Accuracy: 0.173500 | 1.075 sec/iter\n",
      "Epoch: 02 | Batch: 003 / 011 | Total loss: 3.255 | Reg loss: 0.015 | Tree loss: 3.255 | Accuracy: 0.172000 | 1.069 sec/iter\n",
      "Epoch: 02 | Batch: 004 / 011 | Total loss: 3.244 | Reg loss: 0.015 | Tree loss: 3.244 | Accuracy: 0.163000 | 1.07 sec/iter\n",
      "Epoch: 02 | Batch: 005 / 011 | Total loss: 3.234 | Reg loss: 0.015 | Tree loss: 3.234 | Accuracy: 0.165000 | 1.07 sec/iter\n",
      "Epoch: 02 | Batch: 006 / 011 | Total loss: 3.227 | Reg loss: 0.015 | Tree loss: 3.227 | Accuracy: 0.159000 | 1.066 sec/iter\n",
      "Epoch: 02 | Batch: 007 / 011 | Total loss: 3.215 | Reg loss: 0.015 | Tree loss: 3.215 | Accuracy: 0.171000 | 1.066 sec/iter\n",
      "Epoch: 02 | Batch: 008 / 011 | Total loss: 3.210 | Reg loss: 0.016 | Tree loss: 3.210 | Accuracy: 0.160500 | 1.067 sec/iter\n",
      "Epoch: 02 | Batch: 009 / 011 | Total loss: 3.208 | Reg loss: 0.016 | Tree loss: 3.208 | Accuracy: 0.146500 | 1.068 sec/iter\n",
      "Epoch: 02 | Batch: 010 / 011 | Total loss: 3.184 | Reg loss: 0.016 | Tree loss: 3.184 | Accuracy: 0.187713 | 1.061 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 03 | Batch: 000 / 011 | Total loss: 3.272 | Reg loss: 0.015 | Tree loss: 3.272 | Accuracy: 0.149000 | 1.068 sec/iter\n",
      "Epoch: 03 | Batch: 001 / 011 | Total loss: 3.263 | Reg loss: 0.015 | Tree loss: 3.263 | Accuracy: 0.149500 | 1.068 sec/iter\n",
      "Epoch: 03 | Batch: 002 / 011 | Total loss: 3.254 | Reg loss: 0.015 | Tree loss: 3.254 | Accuracy: 0.172500 | 1.068 sec/iter\n",
      "Epoch: 03 | Batch: 003 / 011 | Total loss: 3.243 | Reg loss: 0.015 | Tree loss: 3.243 | Accuracy: 0.190500 | 1.065 sec/iter\n",
      "Epoch: 03 | Batch: 004 / 011 | Total loss: 3.228 | Reg loss: 0.015 | Tree loss: 3.228 | Accuracy: 0.197000 | 1.065 sec/iter\n",
      "Epoch: 03 | Batch: 005 / 011 | Total loss: 3.215 | Reg loss: 0.015 | Tree loss: 3.215 | Accuracy: 0.197500 | 1.066 sec/iter\n",
      "Epoch: 03 | Batch: 006 / 011 | Total loss: 3.202 | Reg loss: 0.015 | Tree loss: 3.202 | Accuracy: 0.192500 | 1.063 sec/iter\n",
      "Epoch: 03 | Batch: 007 / 011 | Total loss: 3.188 | Reg loss: 0.016 | Tree loss: 3.188 | Accuracy: 0.212500 | 1.064 sec/iter\n",
      "Epoch: 03 | Batch: 008 / 011 | Total loss: 3.178 | Reg loss: 0.016 | Tree loss: 3.178 | Accuracy: 0.228500 | 1.065 sec/iter\n",
      "Epoch: 03 | Batch: 009 / 011 | Total loss: 3.163 | Reg loss: 0.016 | Tree loss: 3.163 | Accuracy: 0.248000 | 1.066 sec/iter\n",
      "Epoch: 03 | Batch: 010 / 011 | Total loss: 3.157 | Reg loss: 0.016 | Tree loss: 3.157 | Accuracy: 0.238908 | 1.06 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 04 | Batch: 000 / 011 | Total loss: 3.266 | Reg loss: 0.015 | Tree loss: 3.266 | Accuracy: 0.132500 | 1.066 sec/iter\n",
      "Epoch: 04 | Batch: 001 / 011 | Total loss: 3.251 | Reg loss: 0.015 | Tree loss: 3.251 | Accuracy: 0.195500 | 1.066 sec/iter\n",
      "Epoch: 04 | Batch: 002 / 011 | Total loss: 3.239 | Reg loss: 0.015 | Tree loss: 3.239 | Accuracy: 0.199500 | 1.066 sec/iter\n",
      "Epoch: 04 | Batch: 003 / 011 | Total loss: 3.222 | Reg loss: 0.015 | Tree loss: 3.222 | Accuracy: 0.206500 | 1.063 sec/iter\n",
      "Epoch: 04 | Batch: 004 / 011 | Total loss: 3.205 | Reg loss: 0.015 | Tree loss: 3.205 | Accuracy: 0.207500 | 1.063 sec/iter\n",
      "Epoch: 04 | Batch: 005 / 011 | Total loss: 3.193 | Reg loss: 0.015 | Tree loss: 3.193 | Accuracy: 0.184500 | 1.064 sec/iter\n",
      "Epoch: 04 | Batch: 006 / 011 | Total loss: 3.170 | Reg loss: 0.015 | Tree loss: 3.170 | Accuracy: 0.203000 | 1.062 sec/iter\n",
      "Epoch: 04 | Batch: 007 / 011 | Total loss: 3.153 | Reg loss: 0.016 | Tree loss: 3.153 | Accuracy: 0.215000 | 1.062 sec/iter\n",
      "Epoch: 04 | Batch: 008 / 011 | Total loss: 3.146 | Reg loss: 0.016 | Tree loss: 3.146 | Accuracy: 0.218000 | 1.062 sec/iter\n",
      "Epoch: 04 | Batch: 009 / 011 | Total loss: 3.146 | Reg loss: 0.016 | Tree loss: 3.146 | Accuracy: 0.177000 | 1.061 sec/iter\n",
      "Epoch: 04 | Batch: 010 / 011 | Total loss: 3.112 | Reg loss: 0.017 | Tree loss: 3.112 | Accuracy: 0.215017 | 1.057 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 05 | Batch: 000 / 011 | Total loss: 3.255 | Reg loss: 0.015 | Tree loss: 3.255 | Accuracy: 0.166500 | 1.063 sec/iter\n",
      "Epoch: 05 | Batch: 001 / 011 | Total loss: 3.240 | Reg loss: 0.015 | Tree loss: 3.240 | Accuracy: 0.220500 | 1.063 sec/iter\n",
      "Epoch: 05 | Batch: 002 / 011 | Total loss: 3.220 | Reg loss: 0.015 | Tree loss: 3.220 | Accuracy: 0.219500 | 1.063 sec/iter\n",
      "Epoch: 05 | Batch: 003 / 011 | Total loss: 3.200 | Reg loss: 0.015 | Tree loss: 3.200 | Accuracy: 0.223500 | 1.062 sec/iter\n",
      "Epoch: 05 | Batch: 004 / 011 | Total loss: 3.183 | Reg loss: 0.015 | Tree loss: 3.183 | Accuracy: 0.197000 | 1.062 sec/iter\n",
      "Epoch: 05 | Batch: 005 / 011 | Total loss: 3.165 | Reg loss: 0.015 | Tree loss: 3.165 | Accuracy: 0.204000 | 1.062 sec/iter\n",
      "Epoch: 05 | Batch: 006 / 011 | Total loss: 3.143 | Reg loss: 0.016 | Tree loss: 3.143 | Accuracy: 0.212000 | 1.062 sec/iter\n",
      "Epoch: 05 | Batch: 007 / 011 | Total loss: 3.124 | Reg loss: 0.016 | Tree loss: 3.124 | Accuracy: 0.240000 | 1.062 sec/iter\n",
      "Epoch: 05 | Batch: 008 / 011 | Total loss: 3.103 | Reg loss: 0.016 | Tree loss: 3.103 | Accuracy: 0.254500 | 1.062 sec/iter\n",
      "Epoch: 05 | Batch: 009 / 011 | Total loss: 3.095 | Reg loss: 0.017 | Tree loss: 3.095 | Accuracy: 0.242000 | 1.063 sec/iter\n",
      "Epoch: 05 | Batch: 010 / 011 | Total loss: 3.067 | Reg loss: 0.017 | Tree loss: 3.067 | Accuracy: 0.232082 | 1.059 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 06 | Batch: 000 / 011 | Total loss: 3.243 | Reg loss: 0.015 | Tree loss: 3.243 | Accuracy: 0.180000 | 1.065 sec/iter\n",
      "Epoch: 06 | Batch: 001 / 011 | Total loss: 3.228 | Reg loss: 0.015 | Tree loss: 3.228 | Accuracy: 0.211000 | 1.065 sec/iter\n",
      "Epoch: 06 | Batch: 002 / 011 | Total loss: 3.203 | Reg loss: 0.015 | Tree loss: 3.203 | Accuracy: 0.229000 | 1.064 sec/iter\n",
      "Epoch: 06 | Batch: 003 / 011 | Total loss: 3.179 | Reg loss: 0.015 | Tree loss: 3.179 | Accuracy: 0.199500 | 1.064 sec/iter\n",
      "Epoch: 06 | Batch: 004 / 011 | Total loss: 3.150 | Reg loss: 0.016 | Tree loss: 3.150 | Accuracy: 0.209500 | 1.064 sec/iter\n",
      "Epoch: 06 | Batch: 005 / 011 | Total loss: 3.122 | Reg loss: 0.016 | Tree loss: 3.122 | Accuracy: 0.215500 | 1.063 sec/iter\n",
      "Epoch: 06 | Batch: 006 / 011 | Total loss: 3.096 | Reg loss: 0.016 | Tree loss: 3.096 | Accuracy: 0.249000 | 1.063 sec/iter\n",
      "Epoch: 06 | Batch: 007 / 011 | Total loss: 3.075 | Reg loss: 0.016 | Tree loss: 3.075 | Accuracy: 0.218500 | 1.063 sec/iter\n",
      "Epoch: 06 | Batch: 008 / 011 | Total loss: 3.068 | Reg loss: 0.017 | Tree loss: 3.068 | Accuracy: 0.206000 | 1.061 sec/iter\n",
      "Epoch: 06 | Batch: 009 / 011 | Total loss: 3.049 | Reg loss: 0.017 | Tree loss: 3.049 | Accuracy: 0.206500 | 1.061 sec/iter\n",
      "Epoch: 06 | Batch: 010 / 011 | Total loss: 3.063 | Reg loss: 0.017 | Tree loss: 3.063 | Accuracy: 0.156997 | 1.058 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 07 | Batch: 000 / 011 | Total loss: 3.232 | Reg loss: 0.015 | Tree loss: 3.232 | Accuracy: 0.215500 | 1.063 sec/iter\n",
      "Epoch: 07 | Batch: 001 / 011 | Total loss: 3.207 | Reg loss: 0.016 | Tree loss: 3.207 | Accuracy: 0.224500 | 1.063 sec/iter\n",
      "Epoch: 07 | Batch: 002 / 011 | Total loss: 3.176 | Reg loss: 0.016 | Tree loss: 3.176 | Accuracy: 0.222500 | 1.062 sec/iter\n",
      "Epoch: 07 | Batch: 003 / 011 | Total loss: 3.147 | Reg loss: 0.016 | Tree loss: 3.147 | Accuracy: 0.226500 | 1.062 sec/iter\n",
      "Epoch: 07 | Batch: 004 / 011 | Total loss: 3.113 | Reg loss: 0.016 | Tree loss: 3.113 | Accuracy: 0.217500 | 1.063 sec/iter\n",
      "Epoch: 07 | Batch: 005 / 011 | Total loss: 3.084 | Reg loss: 0.016 | Tree loss: 3.084 | Accuracy: 0.224500 | 1.061 sec/iter\n",
      "Epoch: 07 | Batch: 006 / 011 | Total loss: 3.052 | Reg loss: 0.016 | Tree loss: 3.052 | Accuracy: 0.264000 | 1.061 sec/iter\n",
      "Epoch: 07 | Batch: 007 / 011 | Total loss: 3.031 | Reg loss: 0.017 | Tree loss: 3.031 | Accuracy: 0.249000 | 1.062 sec/iter\n",
      "Epoch: 07 | Batch: 008 / 011 | Total loss: 3.012 | Reg loss: 0.017 | Tree loss: 3.012 | Accuracy: 0.243000 | 1.061 sec/iter\n",
      "Epoch: 07 | Batch: 009 / 011 | Total loss: 3.010 | Reg loss: 0.017 | Tree loss: 3.010 | Accuracy: 0.225500 | 1.061 sec/iter\n",
      "Epoch: 07 | Batch: 010 / 011 | Total loss: 3.001 | Reg loss: 0.018 | Tree loss: 3.001 | Accuracy: 0.211604 | 1.059 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 08 | Batch: 000 / 011 | Total loss: 3.217 | Reg loss: 0.016 | Tree loss: 3.217 | Accuracy: 0.214500 | 1.062 sec/iter\n",
      "Epoch: 08 | Batch: 001 / 011 | Total loss: 3.180 | Reg loss: 0.016 | Tree loss: 3.180 | Accuracy: 0.240000 | 1.062 sec/iter\n",
      "Epoch: 08 | Batch: 002 / 011 | Total loss: 3.150 | Reg loss: 0.016 | Tree loss: 3.150 | Accuracy: 0.231000 | 1.061 sec/iter\n",
      "Epoch: 08 | Batch: 003 / 011 | Total loss: 3.111 | Reg loss: 0.016 | Tree loss: 3.111 | Accuracy: 0.228500 | 1.061 sec/iter\n",
      "Epoch: 08 | Batch: 004 / 011 | Total loss: 3.073 | Reg loss: 0.016 | Tree loss: 3.073 | Accuracy: 0.245500 | 1.062 sec/iter\n",
      "Epoch: 08 | Batch: 005 / 011 | Total loss: 3.024 | Reg loss: 0.017 | Tree loss: 3.024 | Accuracy: 0.258500 | 1.061 sec/iter\n",
      "Epoch: 08 | Batch: 006 / 011 | Total loss: 3.000 | Reg loss: 0.017 | Tree loss: 3.000 | Accuracy: 0.252500 | 1.061 sec/iter\n",
      "Epoch: 08 | Batch: 007 / 011 | Total loss: 2.985 | Reg loss: 0.017 | Tree loss: 2.985 | Accuracy: 0.247000 | 1.061 sec/iter\n",
      "Epoch: 08 | Batch: 008 / 011 | Total loss: 2.965 | Reg loss: 0.018 | Tree loss: 2.965 | Accuracy: 0.243500 | 1.06 sec/iter\n",
      "Epoch: 08 | Batch: 009 / 011 | Total loss: 2.957 | Reg loss: 0.018 | Tree loss: 2.957 | Accuracy: 0.238000 | 1.06 sec/iter\n",
      "Epoch: 08 | Batch: 010 / 011 | Total loss: 2.928 | Reg loss: 0.018 | Tree loss: 2.928 | Accuracy: 0.276451 | 1.058 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 09 | Batch: 000 / 011 | Total loss: 3.190 | Reg loss: 0.016 | Tree loss: 3.190 | Accuracy: 0.222000 | 1.061 sec/iter\n",
      "Epoch: 09 | Batch: 001 / 011 | Total loss: 3.153 | Reg loss: 0.017 | Tree loss: 3.153 | Accuracy: 0.255500 | 1.062 sec/iter\n",
      "Epoch: 09 | Batch: 002 / 011 | Total loss: 3.113 | Reg loss: 0.017 | Tree loss: 3.113 | Accuracy: 0.252000 | 1.061 sec/iter\n",
      "Epoch: 09 | Batch: 003 / 011 | Total loss: 3.067 | Reg loss: 0.017 | Tree loss: 3.067 | Accuracy: 0.231500 | 1.061 sec/iter\n",
      "Epoch: 09 | Batch: 004 / 011 | Total loss: 3.030 | Reg loss: 0.017 | Tree loss: 3.030 | Accuracy: 0.228000 | 1.061 sec/iter\n",
      "Epoch: 09 | Batch: 005 / 011 | Total loss: 2.984 | Reg loss: 0.017 | Tree loss: 2.984 | Accuracy: 0.231000 | 1.06 sec/iter\n",
      "Epoch: 09 | Batch: 006 / 011 | Total loss: 2.950 | Reg loss: 0.018 | Tree loss: 2.950 | Accuracy: 0.234500 | 1.06 sec/iter\n",
      "Epoch: 09 | Batch: 007 / 011 | Total loss: 2.934 | Reg loss: 0.018 | Tree loss: 2.934 | Accuracy: 0.249000 | 1.06 sec/iter\n",
      "Epoch: 09 | Batch: 008 / 011 | Total loss: 2.904 | Reg loss: 0.018 | Tree loss: 2.904 | Accuracy: 0.256500 | 1.059 sec/iter\n",
      "Epoch: 09 | Batch: 009 / 011 | Total loss: 2.885 | Reg loss: 0.018 | Tree loss: 2.885 | Accuracy: 0.269500 | 1.059 sec/iter\n",
      "Epoch: 09 | Batch: 010 / 011 | Total loss: 2.881 | Reg loss: 0.019 | Tree loss: 2.881 | Accuracy: 0.225256 | 1.057 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 10 | Batch: 000 / 011 | Total loss: 3.154 | Reg loss: 0.017 | Tree loss: 3.154 | Accuracy: 0.265000 | 1.061 sec/iter\n",
      "Epoch: 10 | Batch: 001 / 011 | Total loss: 3.112 | Reg loss: 0.017 | Tree loss: 3.112 | Accuracy: 0.265000 | 1.06 sec/iter\n",
      "Epoch: 10 | Batch: 002 / 011 | Total loss: 3.064 | Reg loss: 0.017 | Tree loss: 3.064 | Accuracy: 0.267500 | 1.06 sec/iter\n",
      "Epoch: 10 | Batch: 003 / 011 | Total loss: 3.009 | Reg loss: 0.017 | Tree loss: 3.009 | Accuracy: 0.247500 | 1.06 sec/iter\n",
      "Epoch: 10 | Batch: 004 / 011 | Total loss: 2.980 | Reg loss: 0.018 | Tree loss: 2.980 | Accuracy: 0.242000 | 1.06 sec/iter\n",
      "Epoch: 10 | Batch: 005 / 011 | Total loss: 2.932 | Reg loss: 0.018 | Tree loss: 2.932 | Accuracy: 0.246500 | 1.059 sec/iter\n",
      "Epoch: 10 | Batch: 006 / 011 | Total loss: 2.886 | Reg loss: 0.018 | Tree loss: 2.886 | Accuracy: 0.237000 | 1.059 sec/iter\n",
      "Epoch: 10 | Batch: 007 / 011 | Total loss: 2.875 | Reg loss: 0.018 | Tree loss: 2.875 | Accuracy: 0.237000 | 1.06 sec/iter\n",
      "Epoch: 10 | Batch: 008 / 011 | Total loss: 2.874 | Reg loss: 0.019 | Tree loss: 2.874 | Accuracy: 0.213500 | 1.059 sec/iter\n",
      "Epoch: 10 | Batch: 009 / 011 | Total loss: 2.854 | Reg loss: 0.019 | Tree loss: 2.854 | Accuracy: 0.234000 | 1.059 sec/iter\n",
      "Epoch: 10 | Batch: 010 / 011 | Total loss: 2.842 | Reg loss: 0.019 | Tree loss: 2.842 | Accuracy: 0.211604 | 1.057 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 11 | Batch: 000 / 011 | Total loss: 3.121 | Reg loss: 0.018 | Tree loss: 3.121 | Accuracy: 0.276000 | 1.06 sec/iter\n",
      "Epoch: 11 | Batch: 001 / 011 | Total loss: 3.069 | Reg loss: 0.018 | Tree loss: 3.069 | Accuracy: 0.289000 | 1.06 sec/iter\n",
      "Epoch: 11 | Batch: 002 / 011 | Total loss: 3.017 | Reg loss: 0.018 | Tree loss: 3.017 | Accuracy: 0.265000 | 1.059 sec/iter\n",
      "Epoch: 11 | Batch: 003 / 011 | Total loss: 2.965 | Reg loss: 0.018 | Tree loss: 2.965 | Accuracy: 0.260000 | 1.059 sec/iter\n",
      "Epoch: 11 | Batch: 004 / 011 | Total loss: 2.926 | Reg loss: 0.018 | Tree loss: 2.926 | Accuracy: 0.237000 | 1.06 sec/iter\n",
      "Epoch: 11 | Batch: 005 / 011 | Total loss: 2.880 | Reg loss: 0.019 | Tree loss: 2.880 | Accuracy: 0.239000 | 1.059 sec/iter\n",
      "Epoch: 11 | Batch: 006 / 011 | Total loss: 2.851 | Reg loss: 0.019 | Tree loss: 2.851 | Accuracy: 0.227000 | 1.059 sec/iter\n",
      "Epoch: 11 | Batch: 007 / 011 | Total loss: 2.831 | Reg loss: 0.019 | Tree loss: 2.831 | Accuracy: 0.243000 | 1.059 sec/iter\n",
      "Epoch: 11 | Batch: 008 / 011 | Total loss: 2.805 | Reg loss: 0.019 | Tree loss: 2.805 | Accuracy: 0.254500 | 1.059 sec/iter\n",
      "Epoch: 11 | Batch: 009 / 011 | Total loss: 2.789 | Reg loss: 0.020 | Tree loss: 2.789 | Accuracy: 0.245500 | 1.059 sec/iter\n",
      "Epoch: 11 | Batch: 010 / 011 | Total loss: 2.794 | Reg loss: 0.020 | Tree loss: 2.794 | Accuracy: 0.242321 | 1.058 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 12 | Batch: 000 / 011 | Total loss: 3.074 | Reg loss: 0.019 | Tree loss: 3.074 | Accuracy: 0.277500 | 1.06 sec/iter\n",
      "Epoch: 12 | Batch: 001 / 011 | Total loss: 3.026 | Reg loss: 0.019 | Tree loss: 3.026 | Accuracy: 0.278000 | 1.06 sec/iter\n",
      "Epoch: 12 | Batch: 002 / 011 | Total loss: 2.971 | Reg loss: 0.019 | Tree loss: 2.971 | Accuracy: 0.288000 | 1.06 sec/iter\n",
      "Epoch: 12 | Batch: 003 / 011 | Total loss: 2.915 | Reg loss: 0.019 | Tree loss: 2.915 | Accuracy: 0.270000 | 1.06 sec/iter\n",
      "Epoch: 12 | Batch: 004 / 011 | Total loss: 2.868 | Reg loss: 0.019 | Tree loss: 2.868 | Accuracy: 0.271500 | 1.06 sec/iter\n",
      "Epoch: 12 | Batch: 005 / 011 | Total loss: 2.834 | Reg loss: 0.019 | Tree loss: 2.834 | Accuracy: 0.245500 | 1.06 sec/iter\n",
      "Epoch: 12 | Batch: 006 / 011 | Total loss: 2.783 | Reg loss: 0.020 | Tree loss: 2.783 | Accuracy: 0.246500 | 1.06 sec/iter\n",
      "Epoch: 12 | Batch: 007 / 011 | Total loss: 2.778 | Reg loss: 0.020 | Tree loss: 2.778 | Accuracy: 0.231000 | 1.059 sec/iter\n",
      "Epoch: 12 | Batch: 008 / 011 | Total loss: 2.762 | Reg loss: 0.020 | Tree loss: 2.762 | Accuracy: 0.215500 | 1.059 sec/iter\n",
      "Epoch: 12 | Batch: 009 / 011 | Total loss: 2.742 | Reg loss: 0.020 | Tree loss: 2.742 | Accuracy: 0.214500 | 1.059 sec/iter\n",
      "Epoch: 12 | Batch: 010 / 011 | Total loss: 2.720 | Reg loss: 0.021 | Tree loss: 2.720 | Accuracy: 0.191126 | 1.058 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 13 | Batch: 000 / 011 | Total loss: 3.025 | Reg loss: 0.019 | Tree loss: 3.025 | Accuracy: 0.295500 | 1.06 sec/iter\n",
      "Epoch: 13 | Batch: 001 / 011 | Total loss: 2.977 | Reg loss: 0.019 | Tree loss: 2.977 | Accuracy: 0.272500 | 1.06 sec/iter\n",
      "Epoch: 13 | Batch: 002 / 011 | Total loss: 2.928 | Reg loss: 0.020 | Tree loss: 2.928 | Accuracy: 0.263500 | 1.06 sec/iter\n",
      "Epoch: 13 | Batch: 003 / 011 | Total loss: 2.869 | Reg loss: 0.020 | Tree loss: 2.869 | Accuracy: 0.253000 | 1.06 sec/iter\n",
      "Epoch: 13 | Batch: 004 / 011 | Total loss: 2.819 | Reg loss: 0.020 | Tree loss: 2.819 | Accuracy: 0.251500 | 1.059 sec/iter\n",
      "Epoch: 13 | Batch: 005 / 011 | Total loss: 2.795 | Reg loss: 0.020 | Tree loss: 2.795 | Accuracy: 0.245500 | 1.059 sec/iter\n",
      "Epoch: 13 | Batch: 006 / 011 | Total loss: 2.763 | Reg loss: 0.020 | Tree loss: 2.763 | Accuracy: 0.252000 | 1.059 sec/iter\n",
      "Epoch: 13 | Batch: 007 / 011 | Total loss: 2.717 | Reg loss: 0.021 | Tree loss: 2.717 | Accuracy: 0.255000 | 1.06 sec/iter\n",
      "Epoch: 13 | Batch: 008 / 011 | Total loss: 2.704 | Reg loss: 0.021 | Tree loss: 2.704 | Accuracy: 0.249500 | 1.06 sec/iter\n",
      "Epoch: 13 | Batch: 009 / 011 | Total loss: 2.687 | Reg loss: 0.021 | Tree loss: 2.687 | Accuracy: 0.234000 | 1.059 sec/iter\n",
      "Epoch: 13 | Batch: 010 / 011 | Total loss: 2.631 | Reg loss: 0.021 | Tree loss: 2.631 | Accuracy: 0.215017 | 1.058 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 14 | Batch: 000 / 011 | Total loss: 2.984 | Reg loss: 0.020 | Tree loss: 2.984 | Accuracy: 0.281000 | 1.06 sec/iter\n",
      "Epoch: 14 | Batch: 001 / 011 | Total loss: 2.922 | Reg loss: 0.020 | Tree loss: 2.922 | Accuracy: 0.289500 | 1.06 sec/iter\n",
      "Epoch: 14 | Batch: 002 / 011 | Total loss: 2.868 | Reg loss: 0.020 | Tree loss: 2.868 | Accuracy: 0.264500 | 1.06 sec/iter\n",
      "Epoch: 14 | Batch: 003 / 011 | Total loss: 2.824 | Reg loss: 0.020 | Tree loss: 2.824 | Accuracy: 0.259000 | 1.06 sec/iter\n",
      "Epoch: 14 | Batch: 004 / 011 | Total loss: 2.772 | Reg loss: 0.021 | Tree loss: 2.772 | Accuracy: 0.271500 | 1.06 sec/iter\n",
      "Epoch: 14 | Batch: 005 / 011 | Total loss: 2.722 | Reg loss: 0.021 | Tree loss: 2.722 | Accuracy: 0.257500 | 1.06 sec/iter\n",
      "Epoch: 14 | Batch: 006 / 011 | Total loss: 2.717 | Reg loss: 0.021 | Tree loss: 2.717 | Accuracy: 0.235000 | 1.059 sec/iter\n",
      "Epoch: 14 | Batch: 007 / 011 | Total loss: 2.667 | Reg loss: 0.021 | Tree loss: 2.667 | Accuracy: 0.262500 | 1.06 sec/iter\n",
      "Epoch: 14 | Batch: 008 / 011 | Total loss: 2.657 | Reg loss: 0.021 | Tree loss: 2.657 | Accuracy: 0.236500 | 1.06 sec/iter\n",
      "Epoch: 14 | Batch: 009 / 011 | Total loss: 2.643 | Reg loss: 0.022 | Tree loss: 2.643 | Accuracy: 0.222000 | 1.059 sec/iter\n",
      "Epoch: 14 | Batch: 010 / 011 | Total loss: 2.585 | Reg loss: 0.022 | Tree loss: 2.585 | Accuracy: 0.255973 | 1.058 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 15 | Batch: 000 / 011 | Total loss: 2.917 | Reg loss: 0.021 | Tree loss: 2.917 | Accuracy: 0.295500 | 1.06 sec/iter\n",
      "Epoch: 15 | Batch: 001 / 011 | Total loss: 2.884 | Reg loss: 0.021 | Tree loss: 2.884 | Accuracy: 0.271000 | 1.06 sec/iter\n",
      "Epoch: 15 | Batch: 002 / 011 | Total loss: 2.811 | Reg loss: 0.021 | Tree loss: 2.811 | Accuracy: 0.276000 | 1.06 sec/iter\n",
      "Epoch: 15 | Batch: 003 / 011 | Total loss: 2.781 | Reg loss: 0.021 | Tree loss: 2.781 | Accuracy: 0.253000 | 1.06 sec/iter\n",
      "Epoch: 15 | Batch: 004 / 011 | Total loss: 2.736 | Reg loss: 0.021 | Tree loss: 2.736 | Accuracy: 0.252000 | 1.06 sec/iter\n",
      "Epoch: 15 | Batch: 005 / 011 | Total loss: 2.708 | Reg loss: 0.021 | Tree loss: 2.708 | Accuracy: 0.252500 | 1.06 sec/iter\n",
      "Epoch: 15 | Batch: 006 / 011 | Total loss: 2.645 | Reg loss: 0.022 | Tree loss: 2.645 | Accuracy: 0.266500 | 1.059 sec/iter\n",
      "Epoch: 15 | Batch: 007 / 011 | Total loss: 2.622 | Reg loss: 0.022 | Tree loss: 2.622 | Accuracy: 0.282500 | 1.059 sec/iter\n",
      "Epoch: 15 | Batch: 008 / 011 | Total loss: 2.614 | Reg loss: 0.022 | Tree loss: 2.614 | Accuracy: 0.235500 | 1.059 sec/iter\n",
      "Epoch: 15 | Batch: 009 / 011 | Total loss: 2.608 | Reg loss: 0.022 | Tree loss: 2.608 | Accuracy: 0.239000 | 1.06 sec/iter\n",
      "Epoch: 15 | Batch: 010 / 011 | Total loss: 2.601 | Reg loss: 0.022 | Tree loss: 2.601 | Accuracy: 0.235495 | 1.058 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 16 | Batch: 000 / 011 | Total loss: 2.877 | Reg loss: 0.022 | Tree loss: 2.877 | Accuracy: 0.288000 | 1.06 sec/iter\n",
      "Epoch: 16 | Batch: 001 / 011 | Total loss: 2.834 | Reg loss: 0.022 | Tree loss: 2.834 | Accuracy: 0.289000 | 1.06 sec/iter\n",
      "Epoch: 16 | Batch: 002 / 011 | Total loss: 2.785 | Reg loss: 0.022 | Tree loss: 2.785 | Accuracy: 0.274000 | 1.06 sec/iter\n",
      "Epoch: 16 | Batch: 003 / 011 | Total loss: 2.711 | Reg loss: 0.022 | Tree loss: 2.711 | Accuracy: 0.271000 | 1.059 sec/iter\n",
      "Epoch: 16 | Batch: 004 / 011 | Total loss: 2.691 | Reg loss: 0.022 | Tree loss: 2.691 | Accuracy: 0.262500 | 1.059 sec/iter\n",
      "Epoch: 16 | Batch: 005 / 011 | Total loss: 2.654 | Reg loss: 0.022 | Tree loss: 2.654 | Accuracy: 0.252000 | 1.06 sec/iter\n",
      "Epoch: 16 | Batch: 006 / 011 | Total loss: 2.607 | Reg loss: 0.022 | Tree loss: 2.607 | Accuracy: 0.285000 | 1.059 sec/iter\n",
      "Epoch: 16 | Batch: 007 / 011 | Total loss: 2.598 | Reg loss: 0.022 | Tree loss: 2.598 | Accuracy: 0.261500 | 1.059 sec/iter\n",
      "Epoch: 16 | Batch: 008 / 011 | Total loss: 2.565 | Reg loss: 0.023 | Tree loss: 2.565 | Accuracy: 0.238500 | 1.06 sec/iter\n",
      "Epoch: 16 | Batch: 009 / 011 | Total loss: 2.558 | Reg loss: 0.023 | Tree loss: 2.558 | Accuracy: 0.245500 | 1.06 sec/iter\n",
      "Epoch: 16 | Batch: 010 / 011 | Total loss: 2.546 | Reg loss: 0.023 | Tree loss: 2.546 | Accuracy: 0.245734 | 1.058 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 17 | Batch: 000 / 011 | Total loss: 2.844 | Reg loss: 0.022 | Tree loss: 2.844 | Accuracy: 0.304000 | 1.06 sec/iter\n",
      "Epoch: 17 | Batch: 001 / 011 | Total loss: 2.778 | Reg loss: 0.022 | Tree loss: 2.778 | Accuracy: 0.278000 | 1.06 sec/iter\n",
      "Epoch: 17 | Batch: 002 / 011 | Total loss: 2.729 | Reg loss: 0.022 | Tree loss: 2.729 | Accuracy: 0.279500 | 1.06 sec/iter\n",
      "Epoch: 17 | Batch: 003 / 011 | Total loss: 2.683 | Reg loss: 0.022 | Tree loss: 2.683 | Accuracy: 0.273500 | 1.059 sec/iter\n",
      "Epoch: 17 | Batch: 004 / 011 | Total loss: 2.649 | Reg loss: 0.023 | Tree loss: 2.649 | Accuracy: 0.271500 | 1.059 sec/iter\n",
      "Epoch: 17 | Batch: 005 / 011 | Total loss: 2.605 | Reg loss: 0.023 | Tree loss: 2.605 | Accuracy: 0.268000 | 1.059 sec/iter\n",
      "Epoch: 17 | Batch: 006 / 011 | Total loss: 2.579 | Reg loss: 0.023 | Tree loss: 2.579 | Accuracy: 0.257000 | 1.059 sec/iter\n",
      "Epoch: 17 | Batch: 007 / 011 | Total loss: 2.555 | Reg loss: 0.023 | Tree loss: 2.555 | Accuracy: 0.280000 | 1.059 sec/iter\n",
      "Epoch: 17 | Batch: 008 / 011 | Total loss: 2.525 | Reg loss: 0.023 | Tree loss: 2.525 | Accuracy: 0.260500 | 1.059 sec/iter\n",
      "Epoch: 17 | Batch: 009 / 011 | Total loss: 2.536 | Reg loss: 0.023 | Tree loss: 2.536 | Accuracy: 0.246000 | 1.058 sec/iter\n",
      "Epoch: 17 | Batch: 010 / 011 | Total loss: 2.554 | Reg loss: 0.024 | Tree loss: 2.554 | Accuracy: 0.204778 | 1.057 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 18 | Batch: 000 / 011 | Total loss: 2.780 | Reg loss: 0.023 | Tree loss: 2.780 | Accuracy: 0.311500 | 1.059 sec/iter\n",
      "Epoch: 18 | Batch: 001 / 011 | Total loss: 2.735 | Reg loss: 0.023 | Tree loss: 2.735 | Accuracy: 0.294500 | 1.059 sec/iter\n",
      "Epoch: 18 | Batch: 002 / 011 | Total loss: 2.701 | Reg loss: 0.023 | Tree loss: 2.701 | Accuracy: 0.291000 | 1.059 sec/iter\n",
      "Epoch: 18 | Batch: 003 / 011 | Total loss: 2.651 | Reg loss: 0.023 | Tree loss: 2.651 | Accuracy: 0.280000 | 1.058 sec/iter\n",
      "Epoch: 18 | Batch: 004 / 011 | Total loss: 2.604 | Reg loss: 0.023 | Tree loss: 2.604 | Accuracy: 0.267500 | 1.058 sec/iter\n",
      "Epoch: 18 | Batch: 005 / 011 | Total loss: 2.585 | Reg loss: 0.023 | Tree loss: 2.585 | Accuracy: 0.256500 | 1.059 sec/iter\n",
      "Epoch: 18 | Batch: 006 / 011 | Total loss: 2.538 | Reg loss: 0.024 | Tree loss: 2.538 | Accuracy: 0.279000 | 1.059 sec/iter\n",
      "Epoch: 18 | Batch: 007 / 011 | Total loss: 2.507 | Reg loss: 0.024 | Tree loss: 2.507 | Accuracy: 0.281000 | 1.059 sec/iter\n",
      "Epoch: 18 | Batch: 008 / 011 | Total loss: 2.504 | Reg loss: 0.024 | Tree loss: 2.504 | Accuracy: 0.249500 | 1.059 sec/iter\n",
      "Epoch: 18 | Batch: 009 / 011 | Total loss: 2.490 | Reg loss: 0.024 | Tree loss: 2.490 | Accuracy: 0.238500 | 1.059 sec/iter\n",
      "Epoch: 18 | Batch: 010 / 011 | Total loss: 2.432 | Reg loss: 0.024 | Tree loss: 2.432 | Accuracy: 0.262799 | 1.058 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 19 | Batch: 000 / 011 | Total loss: 2.755 | Reg loss: 0.023 | Tree loss: 2.755 | Accuracy: 0.289000 | 1.059 sec/iter\n",
      "Epoch: 19 | Batch: 001 / 011 | Total loss: 2.700 | Reg loss: 0.024 | Tree loss: 2.700 | Accuracy: 0.303500 | 1.059 sec/iter\n",
      "Epoch: 19 | Batch: 002 / 011 | Total loss: 2.665 | Reg loss: 0.024 | Tree loss: 2.665 | Accuracy: 0.279000 | 1.059 sec/iter\n",
      "Epoch: 19 | Batch: 003 / 011 | Total loss: 2.615 | Reg loss: 0.024 | Tree loss: 2.615 | Accuracy: 0.276000 | 1.059 sec/iter\n",
      "Epoch: 19 | Batch: 004 / 011 | Total loss: 2.570 | Reg loss: 0.024 | Tree loss: 2.570 | Accuracy: 0.277500 | 1.059 sec/iter\n",
      "Epoch: 19 | Batch: 005 / 011 | Total loss: 2.526 | Reg loss: 0.024 | Tree loss: 2.526 | Accuracy: 0.276500 | 1.059 sec/iter\n",
      "Epoch: 19 | Batch: 006 / 011 | Total loss: 2.509 | Reg loss: 0.024 | Tree loss: 2.509 | Accuracy: 0.302000 | 1.059 sec/iter\n",
      "Epoch: 19 | Batch: 007 / 011 | Total loss: 2.493 | Reg loss: 0.024 | Tree loss: 2.493 | Accuracy: 0.268500 | 1.059 sec/iter\n",
      "Epoch: 19 | Batch: 008 / 011 | Total loss: 2.453 | Reg loss: 0.024 | Tree loss: 2.453 | Accuracy: 0.273000 | 1.058 sec/iter\n",
      "Epoch: 19 | Batch: 009 / 011 | Total loss: 2.441 | Reg loss: 0.025 | Tree loss: 2.441 | Accuracy: 0.257000 | 1.058 sec/iter\n",
      "Epoch: 19 | Batch: 010 / 011 | Total loss: 2.375 | Reg loss: 0.025 | Tree loss: 2.375 | Accuracy: 0.259386 | 1.057 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 20 | Batch: 000 / 011 | Total loss: 2.707 | Reg loss: 0.024 | Tree loss: 2.707 | Accuracy: 0.305500 | 1.059 sec/iter\n",
      "Epoch: 20 | Batch: 001 / 011 | Total loss: 2.658 | Reg loss: 0.024 | Tree loss: 2.658 | Accuracy: 0.289500 | 1.059 sec/iter\n",
      "Epoch: 20 | Batch: 002 / 011 | Total loss: 2.633 | Reg loss: 0.024 | Tree loss: 2.633 | Accuracy: 0.297500 | 1.059 sec/iter\n",
      "Epoch: 20 | Batch: 003 / 011 | Total loss: 2.563 | Reg loss: 0.024 | Tree loss: 2.563 | Accuracy: 0.291000 | 1.059 sec/iter\n",
      "Epoch: 20 | Batch: 004 / 011 | Total loss: 2.546 | Reg loss: 0.024 | Tree loss: 2.546 | Accuracy: 0.287500 | 1.059 sec/iter\n",
      "Epoch: 20 | Batch: 005 / 011 | Total loss: 2.491 | Reg loss: 0.024 | Tree loss: 2.491 | Accuracy: 0.280500 | 1.059 sec/iter\n",
      "Epoch: 20 | Batch: 006 / 011 | Total loss: 2.476 | Reg loss: 0.025 | Tree loss: 2.476 | Accuracy: 0.286500 | 1.059 sec/iter\n",
      "Epoch: 20 | Batch: 007 / 011 | Total loss: 2.445 | Reg loss: 0.025 | Tree loss: 2.445 | Accuracy: 0.277500 | 1.059 sec/iter\n",
      "Epoch: 20 | Batch: 008 / 011 | Total loss: 2.447 | Reg loss: 0.025 | Tree loss: 2.447 | Accuracy: 0.237500 | 1.058 sec/iter\n",
      "Epoch: 20 | Batch: 009 / 011 | Total loss: 2.406 | Reg loss: 0.025 | Tree loss: 2.406 | Accuracy: 0.262500 | 1.058 sec/iter\n",
      "Epoch: 20 | Batch: 010 / 011 | Total loss: 2.403 | Reg loss: 0.025 | Tree loss: 2.403 | Accuracy: 0.273038 | 1.057 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 21 | Batch: 000 / 011 | Total loss: 2.654 | Reg loss: 0.025 | Tree loss: 2.654 | Accuracy: 0.321500 | 1.059 sec/iter\n",
      "Epoch: 21 | Batch: 001 / 011 | Total loss: 2.620 | Reg loss: 0.025 | Tree loss: 2.620 | Accuracy: 0.301500 | 1.059 sec/iter\n",
      "Epoch: 21 | Batch: 002 / 011 | Total loss: 2.582 | Reg loss: 0.025 | Tree loss: 2.582 | Accuracy: 0.280000 | 1.059 sec/iter\n",
      "Epoch: 21 | Batch: 003 / 011 | Total loss: 2.547 | Reg loss: 0.025 | Tree loss: 2.547 | Accuracy: 0.295500 | 1.059 sec/iter\n",
      "Epoch: 21 | Batch: 004 / 011 | Total loss: 2.502 | Reg loss: 0.025 | Tree loss: 2.502 | Accuracy: 0.296500 | 1.059 sec/iter\n",
      "Epoch: 21 | Batch: 005 / 011 | Total loss: 2.476 | Reg loss: 0.025 | Tree loss: 2.476 | Accuracy: 0.271000 | 1.058 sec/iter\n",
      "Epoch: 21 | Batch: 006 / 011 | Total loss: 2.422 | Reg loss: 0.025 | Tree loss: 2.422 | Accuracy: 0.315500 | 1.059 sec/iter\n",
      "Epoch: 21 | Batch: 007 / 011 | Total loss: 2.413 | Reg loss: 0.025 | Tree loss: 2.413 | Accuracy: 0.283500 | 1.059 sec/iter\n",
      "Epoch: 21 | Batch: 008 / 011 | Total loss: 2.416 | Reg loss: 0.025 | Tree loss: 2.416 | Accuracy: 0.270000 | 1.058 sec/iter\n",
      "Epoch: 21 | Batch: 009 / 011 | Total loss: 2.402 | Reg loss: 0.026 | Tree loss: 2.402 | Accuracy: 0.251000 | 1.058 sec/iter\n",
      "Epoch: 21 | Batch: 010 / 011 | Total loss: 2.380 | Reg loss: 0.026 | Tree loss: 2.380 | Accuracy: 0.249147 | 1.057 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 22 | Batch: 000 / 011 | Total loss: 2.648 | Reg loss: 0.025 | Tree loss: 2.648 | Accuracy: 0.293000 | 1.059 sec/iter\n",
      "Epoch: 22 | Batch: 001 / 011 | Total loss: 2.600 | Reg loss: 0.025 | Tree loss: 2.600 | Accuracy: 0.294500 | 1.059 sec/iter\n",
      "Epoch: 22 | Batch: 002 / 011 | Total loss: 2.554 | Reg loss: 0.025 | Tree loss: 2.554 | Accuracy: 0.284000 | 1.059 sec/iter\n",
      "Epoch: 22 | Batch: 003 / 011 | Total loss: 2.511 | Reg loss: 0.025 | Tree loss: 2.511 | Accuracy: 0.285500 | 1.059 sec/iter\n",
      "Epoch: 22 | Batch: 004 / 011 | Total loss: 2.463 | Reg loss: 0.025 | Tree loss: 2.463 | Accuracy: 0.299000 | 1.059 sec/iter\n",
      "Epoch: 22 | Batch: 005 / 011 | Total loss: 2.430 | Reg loss: 0.026 | Tree loss: 2.430 | Accuracy: 0.297500 | 1.058 sec/iter\n",
      "Epoch: 22 | Batch: 006 / 011 | Total loss: 2.400 | Reg loss: 0.026 | Tree loss: 2.400 | Accuracy: 0.304000 | 1.058 sec/iter\n",
      "Epoch: 22 | Batch: 007 / 011 | Total loss: 2.376 | Reg loss: 0.026 | Tree loss: 2.376 | Accuracy: 0.289500 | 1.058 sec/iter\n",
      "Epoch: 22 | Batch: 008 / 011 | Total loss: 2.373 | Reg loss: 0.026 | Tree loss: 2.373 | Accuracy: 0.283000 | 1.059 sec/iter\n",
      "Epoch: 22 | Batch: 009 / 011 | Total loss: 2.380 | Reg loss: 0.026 | Tree loss: 2.380 | Accuracy: 0.273000 | 1.059 sec/iter\n",
      "Epoch: 22 | Batch: 010 / 011 | Total loss: 2.350 | Reg loss: 0.026 | Tree loss: 2.350 | Accuracy: 0.273038 | 1.058 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 23 | Batch: 000 / 011 | Total loss: 2.618 | Reg loss: 0.026 | Tree loss: 2.618 | Accuracy: 0.301000 | 1.059 sec/iter\n",
      "Epoch: 23 | Batch: 001 / 011 | Total loss: 2.542 | Reg loss: 0.026 | Tree loss: 2.542 | Accuracy: 0.295000 | 1.059 sec/iter\n",
      "Epoch: 23 | Batch: 002 / 011 | Total loss: 2.515 | Reg loss: 0.026 | Tree loss: 2.515 | Accuracy: 0.288500 | 1.059 sec/iter\n",
      "Epoch: 23 | Batch: 003 / 011 | Total loss: 2.516 | Reg loss: 0.026 | Tree loss: 2.516 | Accuracy: 0.276500 | 1.059 sec/iter\n",
      "Epoch: 23 | Batch: 004 / 011 | Total loss: 2.459 | Reg loss: 0.026 | Tree loss: 2.459 | Accuracy: 0.305500 | 1.059 sec/iter\n",
      "Epoch: 23 | Batch: 005 / 011 | Total loss: 2.408 | Reg loss: 0.026 | Tree loss: 2.408 | Accuracy: 0.311000 | 1.059 sec/iter\n",
      "Epoch: 23 | Batch: 006 / 011 | Total loss: 2.391 | Reg loss: 0.026 | Tree loss: 2.391 | Accuracy: 0.302500 | 1.059 sec/iter\n",
      "Epoch: 23 | Batch: 007 / 011 | Total loss: 2.354 | Reg loss: 0.026 | Tree loss: 2.354 | Accuracy: 0.293500 | 1.059 sec/iter\n",
      "Epoch: 23 | Batch: 008 / 011 | Total loss: 2.337 | Reg loss: 0.026 | Tree loss: 2.337 | Accuracy: 0.287500 | 1.059 sec/iter\n",
      "Epoch: 23 | Batch: 009 / 011 | Total loss: 2.323 | Reg loss: 0.026 | Tree loss: 2.323 | Accuracy: 0.292500 | 1.059 sec/iter\n",
      "Epoch: 23 | Batch: 010 / 011 | Total loss: 2.316 | Reg loss: 0.027 | Tree loss: 2.316 | Accuracy: 0.296928 | 1.058 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 24 | Batch: 000 / 011 | Total loss: 2.582 | Reg loss: 0.026 | Tree loss: 2.582 | Accuracy: 0.298500 | 1.06 sec/iter\n",
      "Epoch: 24 | Batch: 001 / 011 | Total loss: 2.532 | Reg loss: 0.026 | Tree loss: 2.532 | Accuracy: 0.284500 | 1.059 sec/iter\n",
      "Epoch: 24 | Batch: 002 / 011 | Total loss: 2.492 | Reg loss: 0.026 | Tree loss: 2.492 | Accuracy: 0.296500 | 1.059 sec/iter\n",
      "Epoch: 24 | Batch: 003 / 011 | Total loss: 2.473 | Reg loss: 0.026 | Tree loss: 2.473 | Accuracy: 0.277500 | 1.059 sec/iter\n",
      "Epoch: 24 | Batch: 004 / 011 | Total loss: 2.404 | Reg loss: 0.026 | Tree loss: 2.404 | Accuracy: 0.304000 | 1.059 sec/iter\n",
      "Epoch: 24 | Batch: 005 / 011 | Total loss: 2.389 | Reg loss: 0.026 | Tree loss: 2.389 | Accuracy: 0.294500 | 1.059 sec/iter\n",
      "Epoch: 24 | Batch: 006 / 011 | Total loss: 2.360 | Reg loss: 0.027 | Tree loss: 2.360 | Accuracy: 0.319500 | 1.059 sec/iter\n",
      "Epoch: 24 | Batch: 007 / 011 | Total loss: 2.347 | Reg loss: 0.027 | Tree loss: 2.347 | Accuracy: 0.301000 | 1.059 sec/iter\n",
      "Epoch: 24 | Batch: 008 / 011 | Total loss: 2.322 | Reg loss: 0.027 | Tree loss: 2.322 | Accuracy: 0.298000 | 1.06 sec/iter\n",
      "Epoch: 24 | Batch: 009 / 011 | Total loss: 2.297 | Reg loss: 0.027 | Tree loss: 2.297 | Accuracy: 0.290000 | 1.059 sec/iter\n",
      "Epoch: 24 | Batch: 010 / 011 | Total loss: 2.314 | Reg loss: 0.027 | Tree loss: 2.314 | Accuracy: 0.303754 | 1.058 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 25 | Batch: 000 / 011 | Total loss: 2.526 | Reg loss: 0.026 | Tree loss: 2.526 | Accuracy: 0.331500 | 1.06 sec/iter\n",
      "Epoch: 25 | Batch: 001 / 011 | Total loss: 2.507 | Reg loss: 0.027 | Tree loss: 2.507 | Accuracy: 0.282500 | 1.06 sec/iter\n",
      "Epoch: 25 | Batch: 002 / 011 | Total loss: 2.471 | Reg loss: 0.027 | Tree loss: 2.471 | Accuracy: 0.292000 | 1.06 sec/iter\n",
      "Epoch: 25 | Batch: 003 / 011 | Total loss: 2.431 | Reg loss: 0.027 | Tree loss: 2.431 | Accuracy: 0.287500 | 1.06 sec/iter\n",
      "Epoch: 25 | Batch: 004 / 011 | Total loss: 2.389 | Reg loss: 0.027 | Tree loss: 2.389 | Accuracy: 0.302000 | 1.06 sec/iter\n",
      "Epoch: 25 | Batch: 005 / 011 | Total loss: 2.357 | Reg loss: 0.027 | Tree loss: 2.357 | Accuracy: 0.296500 | 1.06 sec/iter\n",
      "Epoch: 25 | Batch: 006 / 011 | Total loss: 2.328 | Reg loss: 0.027 | Tree loss: 2.328 | Accuracy: 0.319500 | 1.059 sec/iter\n",
      "Epoch: 25 | Batch: 007 / 011 | Total loss: 2.307 | Reg loss: 0.027 | Tree loss: 2.307 | Accuracy: 0.317000 | 1.06 sec/iter\n",
      "Epoch: 25 | Batch: 008 / 011 | Total loss: 2.306 | Reg loss: 0.027 | Tree loss: 2.306 | Accuracy: 0.289500 | 1.06 sec/iter\n",
      "Epoch: 25 | Batch: 009 / 011 | Total loss: 2.285 | Reg loss: 0.027 | Tree loss: 2.285 | Accuracy: 0.303500 | 1.059 sec/iter\n",
      "Epoch: 25 | Batch: 010 / 011 | Total loss: 2.289 | Reg loss: 0.027 | Tree loss: 2.289 | Accuracy: 0.300341 | 1.059 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 26 | Batch: 000 / 011 | Total loss: 2.517 | Reg loss: 0.027 | Tree loss: 2.517 | Accuracy: 0.312000 | 1.06 sec/iter\n",
      "Epoch: 26 | Batch: 001 / 011 | Total loss: 2.494 | Reg loss: 0.027 | Tree loss: 2.494 | Accuracy: 0.279500 | 1.059 sec/iter\n",
      "Epoch: 26 | Batch: 002 / 011 | Total loss: 2.450 | Reg loss: 0.027 | Tree loss: 2.450 | Accuracy: 0.285000 | 1.059 sec/iter\n",
      "Epoch: 26 | Batch: 003 / 011 | Total loss: 2.413 | Reg loss: 0.027 | Tree loss: 2.413 | Accuracy: 0.298500 | 1.059 sec/iter\n",
      "Epoch: 26 | Batch: 004 / 011 | Total loss: 2.358 | Reg loss: 0.027 | Tree loss: 2.358 | Accuracy: 0.305000 | 1.059 sec/iter\n",
      "Epoch: 26 | Batch: 005 / 011 | Total loss: 2.321 | Reg loss: 0.027 | Tree loss: 2.321 | Accuracy: 0.323000 | 1.059 sec/iter\n",
      "Epoch: 26 | Batch: 006 / 011 | Total loss: 2.295 | Reg loss: 0.027 | Tree loss: 2.295 | Accuracy: 0.318000 | 1.059 sec/iter\n",
      "Epoch: 26 | Batch: 007 / 011 | Total loss: 2.278 | Reg loss: 0.027 | Tree loss: 2.278 | Accuracy: 0.300500 | 1.059 sec/iter\n",
      "Epoch: 26 | Batch: 008 / 011 | Total loss: 2.268 | Reg loss: 0.027 | Tree loss: 2.268 | Accuracy: 0.315000 | 1.059 sec/iter\n",
      "Epoch: 26 | Batch: 009 / 011 | Total loss: 2.280 | Reg loss: 0.028 | Tree loss: 2.280 | Accuracy: 0.293000 | 1.059 sec/iter\n",
      "Epoch: 26 | Batch: 010 / 011 | Total loss: 2.291 | Reg loss: 0.028 | Tree loss: 2.291 | Accuracy: 0.317406 | 1.058 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 27 | Batch: 000 / 011 | Total loss: 2.491 | Reg loss: 0.027 | Tree loss: 2.491 | Accuracy: 0.286500 | 1.059 sec/iter\n",
      "Epoch: 27 | Batch: 001 / 011 | Total loss: 2.457 | Reg loss: 0.027 | Tree loss: 2.457 | Accuracy: 0.286500 | 1.059 sec/iter\n",
      "Epoch: 27 | Batch: 002 / 011 | Total loss: 2.424 | Reg loss: 0.027 | Tree loss: 2.424 | Accuracy: 0.294000 | 1.059 sec/iter\n",
      "Epoch: 27 | Batch: 003 / 011 | Total loss: 2.373 | Reg loss: 0.027 | Tree loss: 2.373 | Accuracy: 0.317500 | 1.059 sec/iter\n",
      "Epoch: 27 | Batch: 004 / 011 | Total loss: 2.348 | Reg loss: 0.027 | Tree loss: 2.348 | Accuracy: 0.317000 | 1.059 sec/iter\n",
      "Epoch: 27 | Batch: 005 / 011 | Total loss: 2.323 | Reg loss: 0.028 | Tree loss: 2.323 | Accuracy: 0.307500 | 1.059 sec/iter\n",
      "Epoch: 27 | Batch: 006 / 011 | Total loss: 2.280 | Reg loss: 0.028 | Tree loss: 2.280 | Accuracy: 0.312500 | 1.059 sec/iter\n",
      "Epoch: 27 | Batch: 007 / 011 | Total loss: 2.253 | Reg loss: 0.028 | Tree loss: 2.253 | Accuracy: 0.318000 | 1.059 sec/iter\n",
      "Epoch: 27 | Batch: 008 / 011 | Total loss: 2.262 | Reg loss: 0.028 | Tree loss: 2.262 | Accuracy: 0.320000 | 1.059 sec/iter\n",
      "Epoch: 27 | Batch: 009 / 011 | Total loss: 2.243 | Reg loss: 0.028 | Tree loss: 2.243 | Accuracy: 0.312000 | 1.059 sec/iter\n",
      "Epoch: 27 | Batch: 010 / 011 | Total loss: 2.180 | Reg loss: 0.028 | Tree loss: 2.180 | Accuracy: 0.361775 | 1.058 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 28 | Batch: 000 / 011 | Total loss: 2.513 | Reg loss: 0.028 | Tree loss: 2.513 | Accuracy: 0.262000 | 1.06 sec/iter\n",
      "Epoch: 28 | Batch: 001 / 011 | Total loss: 2.457 | Reg loss: 0.028 | Tree loss: 2.457 | Accuracy: 0.295500 | 1.06 sec/iter\n",
      "Epoch: 28 | Batch: 002 / 011 | Total loss: 2.402 | Reg loss: 0.028 | Tree loss: 2.402 | Accuracy: 0.301000 | 1.06 sec/iter\n",
      "Epoch: 28 | Batch: 003 / 011 | Total loss: 2.370 | Reg loss: 0.028 | Tree loss: 2.370 | Accuracy: 0.313500 | 1.059 sec/iter\n",
      "Epoch: 28 | Batch: 004 / 011 | Total loss: 2.306 | Reg loss: 0.028 | Tree loss: 2.306 | Accuracy: 0.318000 | 1.06 sec/iter\n",
      "Epoch: 28 | Batch: 005 / 011 | Total loss: 2.274 | Reg loss: 0.028 | Tree loss: 2.274 | Accuracy: 0.313000 | 1.06 sec/iter\n",
      "Epoch: 28 | Batch: 006 / 011 | Total loss: 2.253 | Reg loss: 0.028 | Tree loss: 2.253 | Accuracy: 0.310500 | 1.059 sec/iter\n",
      "Epoch: 28 | Batch: 007 / 011 | Total loss: 2.229 | Reg loss: 0.028 | Tree loss: 2.229 | Accuracy: 0.313000 | 1.059 sec/iter\n",
      "Epoch: 28 | Batch: 008 / 011 | Total loss: 2.219 | Reg loss: 0.028 | Tree loss: 2.219 | Accuracy: 0.321500 | 1.059 sec/iter\n",
      "Epoch: 28 | Batch: 009 / 011 | Total loss: 2.215 | Reg loss: 0.028 | Tree loss: 2.215 | Accuracy: 0.310500 | 1.06 sec/iter\n",
      "Epoch: 28 | Batch: 010 / 011 | Total loss: 2.171 | Reg loss: 0.028 | Tree loss: 2.171 | Accuracy: 0.375427 | 1.059 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 29 | Batch: 000 / 011 | Total loss: 2.468 | Reg loss: 0.028 | Tree loss: 2.468 | Accuracy: 0.298500 | 1.06 sec/iter\n",
      "Epoch: 29 | Batch: 001 / 011 | Total loss: 2.424 | Reg loss: 0.028 | Tree loss: 2.424 | Accuracy: 0.294500 | 1.06 sec/iter\n",
      "Epoch: 29 | Batch: 002 / 011 | Total loss: 2.383 | Reg loss: 0.028 | Tree loss: 2.383 | Accuracy: 0.290500 | 1.06 sec/iter\n",
      "Epoch: 29 | Batch: 003 / 011 | Total loss: 2.329 | Reg loss: 0.028 | Tree loss: 2.329 | Accuracy: 0.317000 | 1.059 sec/iter\n",
      "Epoch: 29 | Batch: 004 / 011 | Total loss: 2.298 | Reg loss: 0.028 | Tree loss: 2.298 | Accuracy: 0.330500 | 1.059 sec/iter\n",
      "Epoch: 29 | Batch: 005 / 011 | Total loss: 2.266 | Reg loss: 0.028 | Tree loss: 2.266 | Accuracy: 0.328000 | 1.06 sec/iter\n",
      "Epoch: 29 | Batch: 006 / 011 | Total loss: 2.254 | Reg loss: 0.028 | Tree loss: 2.254 | Accuracy: 0.307000 | 1.06 sec/iter\n",
      "Epoch: 29 | Batch: 007 / 011 | Total loss: 2.200 | Reg loss: 0.028 | Tree loss: 2.200 | Accuracy: 0.333500 | 1.06 sec/iter\n",
      "Epoch: 29 | Batch: 008 / 011 | Total loss: 2.206 | Reg loss: 0.028 | Tree loss: 2.206 | Accuracy: 0.314000 | 1.06 sec/iter\n",
      "Epoch: 29 | Batch: 009 / 011 | Total loss: 2.217 | Reg loss: 0.029 | Tree loss: 2.217 | Accuracy: 0.311500 | 1.06 sec/iter\n",
      "Epoch: 29 | Batch: 010 / 011 | Total loss: 2.203 | Reg loss: 0.029 | Tree loss: 2.203 | Accuracy: 0.293515 | 1.059 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 30 | Batch: 000 / 011 | Total loss: 2.450 | Reg loss: 0.028 | Tree loss: 2.450 | Accuracy: 0.288500 | 1.06 sec/iter\n",
      "Epoch: 30 | Batch: 001 / 011 | Total loss: 2.400 | Reg loss: 0.028 | Tree loss: 2.400 | Accuracy: 0.277500 | 1.06 sec/iter\n",
      "Epoch: 30 | Batch: 002 / 011 | Total loss: 2.359 | Reg loss: 0.028 | Tree loss: 2.359 | Accuracy: 0.302500 | 1.06 sec/iter\n",
      "Epoch: 30 | Batch: 003 / 011 | Total loss: 2.312 | Reg loss: 0.028 | Tree loss: 2.312 | Accuracy: 0.309000 | 1.06 sec/iter\n",
      "Epoch: 30 | Batch: 004 / 011 | Total loss: 2.274 | Reg loss: 0.028 | Tree loss: 2.274 | Accuracy: 0.332500 | 1.06 sec/iter\n",
      "Epoch: 30 | Batch: 005 / 011 | Total loss: 2.249 | Reg loss: 0.029 | Tree loss: 2.249 | Accuracy: 0.310000 | 1.06 sec/iter\n",
      "Epoch: 30 | Batch: 006 / 011 | Total loss: 2.213 | Reg loss: 0.029 | Tree loss: 2.213 | Accuracy: 0.340500 | 1.06 sec/iter\n",
      "Epoch: 30 | Batch: 007 / 011 | Total loss: 2.220 | Reg loss: 0.029 | Tree loss: 2.220 | Accuracy: 0.314000 | 1.06 sec/iter\n",
      "Epoch: 30 | Batch: 008 / 011 | Total loss: 2.179 | Reg loss: 0.029 | Tree loss: 2.179 | Accuracy: 0.326000 | 1.06 sec/iter\n",
      "Epoch: 30 | Batch: 009 / 011 | Total loss: 2.188 | Reg loss: 0.029 | Tree loss: 2.188 | Accuracy: 0.334500 | 1.06 sec/iter\n",
      "Epoch: 30 | Batch: 010 / 011 | Total loss: 2.175 | Reg loss: 0.029 | Tree loss: 2.175 | Accuracy: 0.327645 | 1.059 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 31 | Batch: 000 / 011 | Total loss: 2.412 | Reg loss: 0.029 | Tree loss: 2.412 | Accuracy: 0.293000 | 1.06 sec/iter\n",
      "Epoch: 31 | Batch: 001 / 011 | Total loss: 2.379 | Reg loss: 0.029 | Tree loss: 2.379 | Accuracy: 0.294000 | 1.06 sec/iter\n",
      "Epoch: 31 | Batch: 002 / 011 | Total loss: 2.317 | Reg loss: 0.029 | Tree loss: 2.317 | Accuracy: 0.323000 | 1.06 sec/iter\n",
      "Epoch: 31 | Batch: 003 / 011 | Total loss: 2.294 | Reg loss: 0.029 | Tree loss: 2.294 | Accuracy: 0.301500 | 1.06 sec/iter\n",
      "Epoch: 31 | Batch: 004 / 011 | Total loss: 2.266 | Reg loss: 0.029 | Tree loss: 2.266 | Accuracy: 0.338500 | 1.06 sec/iter\n",
      "Epoch: 31 | Batch: 005 / 011 | Total loss: 2.223 | Reg loss: 0.029 | Tree loss: 2.223 | Accuracy: 0.315000 | 1.06 sec/iter\n",
      "Epoch: 31 | Batch: 006 / 011 | Total loss: 2.222 | Reg loss: 0.029 | Tree loss: 2.222 | Accuracy: 0.312000 | 1.06 sec/iter\n",
      "Epoch: 31 | Batch: 007 / 011 | Total loss: 2.195 | Reg loss: 0.029 | Tree loss: 2.195 | Accuracy: 0.318000 | 1.06 sec/iter\n",
      "Epoch: 31 | Batch: 008 / 011 | Total loss: 2.154 | Reg loss: 0.029 | Tree loss: 2.154 | Accuracy: 0.349500 | 1.06 sec/iter\n",
      "Epoch: 31 | Batch: 009 / 011 | Total loss: 2.181 | Reg loss: 0.029 | Tree loss: 2.181 | Accuracy: 0.329000 | 1.06 sec/iter\n",
      "Epoch: 31 | Batch: 010 / 011 | Total loss: 2.225 | Reg loss: 0.029 | Tree loss: 2.225 | Accuracy: 0.317406 | 1.059 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 32 | Batch: 000 / 011 | Total loss: 2.385 | Reg loss: 0.029 | Tree loss: 2.385 | Accuracy: 0.305500 | 1.06 sec/iter\n",
      "Epoch: 32 | Batch: 001 / 011 | Total loss: 2.357 | Reg loss: 0.029 | Tree loss: 2.357 | Accuracy: 0.311500 | 1.06 sec/iter\n",
      "Epoch: 32 | Batch: 002 / 011 | Total loss: 2.321 | Reg loss: 0.029 | Tree loss: 2.321 | Accuracy: 0.304500 | 1.06 sec/iter\n",
      "Epoch: 32 | Batch: 003 / 011 | Total loss: 2.291 | Reg loss: 0.029 | Tree loss: 2.291 | Accuracy: 0.304000 | 1.06 sec/iter\n",
      "Epoch: 32 | Batch: 004 / 011 | Total loss: 2.239 | Reg loss: 0.029 | Tree loss: 2.239 | Accuracy: 0.319000 | 1.06 sec/iter\n",
      "Epoch: 32 | Batch: 005 / 011 | Total loss: 2.224 | Reg loss: 0.029 | Tree loss: 2.224 | Accuracy: 0.320000 | 1.06 sec/iter\n",
      "Epoch: 32 | Batch: 006 / 011 | Total loss: 2.151 | Reg loss: 0.029 | Tree loss: 2.151 | Accuracy: 0.336000 | 1.06 sec/iter\n",
      "Epoch: 32 | Batch: 007 / 011 | Total loss: 2.168 | Reg loss: 0.029 | Tree loss: 2.168 | Accuracy: 0.319000 | 1.06 sec/iter\n",
      "Epoch: 32 | Batch: 008 / 011 | Total loss: 2.166 | Reg loss: 0.029 | Tree loss: 2.166 | Accuracy: 0.317500 | 1.06 sec/iter\n",
      "Epoch: 32 | Batch: 009 / 011 | Total loss: 2.174 | Reg loss: 0.029 | Tree loss: 2.174 | Accuracy: 0.322000 | 1.06 sec/iter\n",
      "Epoch: 32 | Batch: 010 / 011 | Total loss: 2.167 | Reg loss: 0.029 | Tree loss: 2.167 | Accuracy: 0.337884 | 1.059 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 33 | Batch: 000 / 011 | Total loss: 2.371 | Reg loss: 0.029 | Tree loss: 2.371 | Accuracy: 0.293500 | 1.06 sec/iter\n",
      "Epoch: 33 | Batch: 001 / 011 | Total loss: 2.340 | Reg loss: 0.029 | Tree loss: 2.340 | Accuracy: 0.300000 | 1.06 sec/iter\n",
      "Epoch: 33 | Batch: 002 / 011 | Total loss: 2.313 | Reg loss: 0.029 | Tree loss: 2.313 | Accuracy: 0.282000 | 1.06 sec/iter\n",
      "Epoch: 33 | Batch: 003 / 011 | Total loss: 2.258 | Reg loss: 0.029 | Tree loss: 2.258 | Accuracy: 0.308000 | 1.06 sec/iter\n",
      "Epoch: 33 | Batch: 004 / 011 | Total loss: 2.232 | Reg loss: 0.029 | Tree loss: 2.232 | Accuracy: 0.320500 | 1.06 sec/iter\n",
      "Epoch: 33 | Batch: 005 / 011 | Total loss: 2.187 | Reg loss: 0.029 | Tree loss: 2.187 | Accuracy: 0.339000 | 1.06 sec/iter\n",
      "Epoch: 33 | Batch: 006 / 011 | Total loss: 2.174 | Reg loss: 0.029 | Tree loss: 2.174 | Accuracy: 0.320500 | 1.06 sec/iter\n",
      "Epoch: 33 | Batch: 007 / 011 | Total loss: 2.160 | Reg loss: 0.030 | Tree loss: 2.160 | Accuracy: 0.329000 | 1.06 sec/iter\n",
      "Epoch: 33 | Batch: 008 / 011 | Total loss: 2.137 | Reg loss: 0.030 | Tree loss: 2.137 | Accuracy: 0.340500 | 1.06 sec/iter\n",
      "Epoch: 33 | Batch: 009 / 011 | Total loss: 2.117 | Reg loss: 0.030 | Tree loss: 2.117 | Accuracy: 0.360000 | 1.06 sec/iter\n",
      "Epoch: 33 | Batch: 010 / 011 | Total loss: 2.105 | Reg loss: 0.030 | Tree loss: 2.105 | Accuracy: 0.351536 | 1.059 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 34 | Batch: 000 / 011 | Total loss: 2.359 | Reg loss: 0.029 | Tree loss: 2.359 | Accuracy: 0.302000 | 1.06 sec/iter\n",
      "Epoch: 34 | Batch: 001 / 011 | Total loss: 2.345 | Reg loss: 0.029 | Tree loss: 2.345 | Accuracy: 0.280000 | 1.06 sec/iter\n",
      "Epoch: 34 | Batch: 002 / 011 | Total loss: 2.303 | Reg loss: 0.029 | Tree loss: 2.303 | Accuracy: 0.293000 | 1.06 sec/iter\n",
      "Epoch: 34 | Batch: 003 / 011 | Total loss: 2.262 | Reg loss: 0.030 | Tree loss: 2.262 | Accuracy: 0.311500 | 1.06 sec/iter\n",
      "Epoch: 34 | Batch: 004 / 011 | Total loss: 2.210 | Reg loss: 0.030 | Tree loss: 2.210 | Accuracy: 0.329000 | 1.06 sec/iter\n",
      "Epoch: 34 | Batch: 005 / 011 | Total loss: 2.145 | Reg loss: 0.030 | Tree loss: 2.145 | Accuracy: 0.352000 | 1.06 sec/iter\n",
      "Epoch: 34 | Batch: 006 / 011 | Total loss: 2.165 | Reg loss: 0.030 | Tree loss: 2.165 | Accuracy: 0.330500 | 1.06 sec/iter\n",
      "Epoch: 34 | Batch: 007 / 011 | Total loss: 2.142 | Reg loss: 0.030 | Tree loss: 2.142 | Accuracy: 0.328000 | 1.059 sec/iter\n",
      "Epoch: 34 | Batch: 008 / 011 | Total loss: 2.121 | Reg loss: 0.030 | Tree loss: 2.121 | Accuracy: 0.324000 | 1.06 sec/iter\n",
      "Epoch: 34 | Batch: 009 / 011 | Total loss: 2.094 | Reg loss: 0.030 | Tree loss: 2.094 | Accuracy: 0.355000 | 1.06 sec/iter\n",
      "Epoch: 34 | Batch: 010 / 011 | Total loss: 2.098 | Reg loss: 0.030 | Tree loss: 2.098 | Accuracy: 0.351536 | 1.059 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 35 | Batch: 000 / 011 | Total loss: 2.383 | Reg loss: 0.030 | Tree loss: 2.383 | Accuracy: 0.277500 | 1.06 sec/iter\n",
      "Epoch: 35 | Batch: 001 / 011 | Total loss: 2.288 | Reg loss: 0.030 | Tree loss: 2.288 | Accuracy: 0.321000 | 1.06 sec/iter\n",
      "Epoch: 35 | Batch: 002 / 011 | Total loss: 2.271 | Reg loss: 0.030 | Tree loss: 2.271 | Accuracy: 0.306000 | 1.06 sec/iter\n",
      "Epoch: 35 | Batch: 003 / 011 | Total loss: 2.237 | Reg loss: 0.030 | Tree loss: 2.237 | Accuracy: 0.312000 | 1.06 sec/iter\n",
      "Epoch: 35 | Batch: 004 / 011 | Total loss: 2.209 | Reg loss: 0.030 | Tree loss: 2.209 | Accuracy: 0.320500 | 1.059 sec/iter\n",
      "Epoch: 35 | Batch: 005 / 011 | Total loss: 2.153 | Reg loss: 0.030 | Tree loss: 2.153 | Accuracy: 0.337500 | 1.059 sec/iter\n",
      "Epoch: 35 | Batch: 006 / 011 | Total loss: 2.132 | Reg loss: 0.030 | Tree loss: 2.132 | Accuracy: 0.327500 | 1.059 sec/iter\n",
      "Epoch: 35 | Batch: 007 / 011 | Total loss: 2.124 | Reg loss: 0.030 | Tree loss: 2.124 | Accuracy: 0.329000 | 1.06 sec/iter\n",
      "Epoch: 35 | Batch: 008 / 011 | Total loss: 2.104 | Reg loss: 0.030 | Tree loss: 2.104 | Accuracy: 0.344000 | 1.06 sec/iter\n",
      "Epoch: 35 | Batch: 009 / 011 | Total loss: 2.078 | Reg loss: 0.030 | Tree loss: 2.078 | Accuracy: 0.380500 | 1.06 sec/iter\n",
      "Epoch: 35 | Batch: 010 / 011 | Total loss: 2.067 | Reg loss: 0.030 | Tree loss: 2.067 | Accuracy: 0.372014 | 1.059 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 36 | Batch: 000 / 011 | Total loss: 2.327 | Reg loss: 0.030 | Tree loss: 2.327 | Accuracy: 0.283000 | 1.06 sec/iter\n",
      "Epoch: 36 | Batch: 001 / 011 | Total loss: 2.296 | Reg loss: 0.030 | Tree loss: 2.296 | Accuracy: 0.303000 | 1.06 sec/iter\n",
      "Epoch: 36 | Batch: 002 / 011 | Total loss: 2.275 | Reg loss: 0.030 | Tree loss: 2.275 | Accuracy: 0.311500 | 1.06 sec/iter\n",
      "Epoch: 36 | Batch: 003 / 011 | Total loss: 2.235 | Reg loss: 0.030 | Tree loss: 2.235 | Accuracy: 0.306000 | 1.06 sec/iter\n",
      "Epoch: 36 | Batch: 004 / 011 | Total loss: 2.174 | Reg loss: 0.030 | Tree loss: 2.174 | Accuracy: 0.327500 | 1.06 sec/iter\n",
      "Epoch: 36 | Batch: 005 / 011 | Total loss: 2.156 | Reg loss: 0.030 | Tree loss: 2.156 | Accuracy: 0.329000 | 1.06 sec/iter\n",
      "Epoch: 36 | Batch: 006 / 011 | Total loss: 2.133 | Reg loss: 0.030 | Tree loss: 2.133 | Accuracy: 0.330000 | 1.06 sec/iter\n",
      "Epoch: 36 | Batch: 007 / 011 | Total loss: 2.082 | Reg loss: 0.030 | Tree loss: 2.082 | Accuracy: 0.339500 | 1.06 sec/iter\n",
      "Epoch: 36 | Batch: 008 / 011 | Total loss: 2.081 | Reg loss: 0.030 | Tree loss: 2.081 | Accuracy: 0.349500 | 1.06 sec/iter\n",
      "Epoch: 36 | Batch: 009 / 011 | Total loss: 2.069 | Reg loss: 0.030 | Tree loss: 2.069 | Accuracy: 0.349000 | 1.06 sec/iter\n",
      "Epoch: 36 | Batch: 010 / 011 | Total loss: 2.114 | Reg loss: 0.030 | Tree loss: 2.114 | Accuracy: 0.303754 | 1.059 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 37 | Batch: 000 / 011 | Total loss: 2.343 | Reg loss: 0.030 | Tree loss: 2.343 | Accuracy: 0.295500 | 1.06 sec/iter\n",
      "Epoch: 37 | Batch: 001 / 011 | Total loss: 2.281 | Reg loss: 0.030 | Tree loss: 2.281 | Accuracy: 0.311500 | 1.06 sec/iter\n",
      "Epoch: 37 | Batch: 002 / 011 | Total loss: 2.273 | Reg loss: 0.030 | Tree loss: 2.273 | Accuracy: 0.283000 | 1.06 sec/iter\n",
      "Epoch: 37 | Batch: 003 / 011 | Total loss: 2.207 | Reg loss: 0.030 | Tree loss: 2.207 | Accuracy: 0.321500 | 1.06 sec/iter\n",
      "Epoch: 37 | Batch: 004 / 011 | Total loss: 2.156 | Reg loss: 0.030 | Tree loss: 2.156 | Accuracy: 0.338000 | 1.06 sec/iter\n",
      "Epoch: 37 | Batch: 005 / 011 | Total loss: 2.116 | Reg loss: 0.030 | Tree loss: 2.116 | Accuracy: 0.340000 | 1.06 sec/iter\n",
      "Epoch: 37 | Batch: 006 / 011 | Total loss: 2.124 | Reg loss: 0.030 | Tree loss: 2.124 | Accuracy: 0.321500 | 1.06 sec/iter\n",
      "Epoch: 37 | Batch: 007 / 011 | Total loss: 2.074 | Reg loss: 0.030 | Tree loss: 2.074 | Accuracy: 0.329500 | 1.06 sec/iter\n",
      "Epoch: 37 | Batch: 008 / 011 | Total loss: 2.059 | Reg loss: 0.031 | Tree loss: 2.059 | Accuracy: 0.365000 | 1.06 sec/iter\n",
      "Epoch: 37 | Batch: 009 / 011 | Total loss: 2.050 | Reg loss: 0.031 | Tree loss: 2.050 | Accuracy: 0.383500 | 1.06 sec/iter\n",
      "Epoch: 37 | Batch: 010 / 011 | Total loss: 2.110 | Reg loss: 0.031 | Tree loss: 2.110 | Accuracy: 0.354949 | 1.059 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 38 | Batch: 000 / 011 | Total loss: 2.316 | Reg loss: 0.030 | Tree loss: 2.316 | Accuracy: 0.312000 | 1.06 sec/iter\n",
      "Epoch: 38 | Batch: 001 / 011 | Total loss: 2.272 | Reg loss: 0.030 | Tree loss: 2.272 | Accuracy: 0.307500 | 1.06 sec/iter\n",
      "Epoch: 38 | Batch: 002 / 011 | Total loss: 2.265 | Reg loss: 0.030 | Tree loss: 2.265 | Accuracy: 0.290500 | 1.06 sec/iter\n",
      "Epoch: 38 | Batch: 003 / 011 | Total loss: 2.209 | Reg loss: 0.030 | Tree loss: 2.209 | Accuracy: 0.310500 | 1.06 sec/iter\n",
      "Epoch: 38 | Batch: 004 / 011 | Total loss: 2.152 | Reg loss: 0.031 | Tree loss: 2.152 | Accuracy: 0.322000 | 1.06 sec/iter\n",
      "Epoch: 38 | Batch: 005 / 011 | Total loss: 2.102 | Reg loss: 0.031 | Tree loss: 2.102 | Accuracy: 0.342000 | 1.06 sec/iter\n",
      "Epoch: 38 | Batch: 006 / 011 | Total loss: 2.086 | Reg loss: 0.031 | Tree loss: 2.086 | Accuracy: 0.328500 | 1.06 sec/iter\n",
      "Epoch: 38 | Batch: 007 / 011 | Total loss: 2.068 | Reg loss: 0.031 | Tree loss: 2.068 | Accuracy: 0.343500 | 1.06 sec/iter\n",
      "Epoch: 38 | Batch: 008 / 011 | Total loss: 2.052 | Reg loss: 0.031 | Tree loss: 2.052 | Accuracy: 0.345500 | 1.06 sec/iter\n",
      "Epoch: 38 | Batch: 009 / 011 | Total loss: 2.048 | Reg loss: 0.031 | Tree loss: 2.048 | Accuracy: 0.354000 | 1.06 sec/iter\n",
      "Epoch: 38 | Batch: 010 / 011 | Total loss: 2.074 | Reg loss: 0.031 | Tree loss: 2.074 | Accuracy: 0.358362 | 1.059 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 39 | Batch: 000 / 011 | Total loss: 2.311 | Reg loss: 0.031 | Tree loss: 2.311 | Accuracy: 0.295000 | 1.06 sec/iter\n",
      "Epoch: 39 | Batch: 001 / 011 | Total loss: 2.263 | Reg loss: 0.031 | Tree loss: 2.263 | Accuracy: 0.309000 | 1.06 sec/iter\n",
      "Epoch: 39 | Batch: 002 / 011 | Total loss: 2.235 | Reg loss: 0.031 | Tree loss: 2.235 | Accuracy: 0.315500 | 1.06 sec/iter\n",
      "Epoch: 39 | Batch: 003 / 011 | Total loss: 2.179 | Reg loss: 0.031 | Tree loss: 2.179 | Accuracy: 0.329500 | 1.06 sec/iter\n",
      "Epoch: 39 | Batch: 004 / 011 | Total loss: 2.141 | Reg loss: 0.031 | Tree loss: 2.141 | Accuracy: 0.330500 | 1.06 sec/iter\n",
      "Epoch: 39 | Batch: 005 / 011 | Total loss: 2.098 | Reg loss: 0.031 | Tree loss: 2.098 | Accuracy: 0.343500 | 1.06 sec/iter\n",
      "Epoch: 39 | Batch: 006 / 011 | Total loss: 2.086 | Reg loss: 0.031 | Tree loss: 2.086 | Accuracy: 0.342500 | 1.06 sec/iter\n",
      "Epoch: 39 | Batch: 007 / 011 | Total loss: 2.045 | Reg loss: 0.031 | Tree loss: 2.045 | Accuracy: 0.331500 | 1.06 sec/iter\n",
      "Epoch: 39 | Batch: 008 / 011 | Total loss: 2.050 | Reg loss: 0.031 | Tree loss: 2.050 | Accuracy: 0.336500 | 1.06 sec/iter\n",
      "Epoch: 39 | Batch: 009 / 011 | Total loss: 2.036 | Reg loss: 0.031 | Tree loss: 2.036 | Accuracy: 0.349500 | 1.06 sec/iter\n",
      "Epoch: 39 | Batch: 010 / 011 | Total loss: 2.045 | Reg loss: 0.031 | Tree loss: 2.045 | Accuracy: 0.337884 | 1.059 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 40 | Batch: 000 / 011 | Total loss: 2.300 | Reg loss: 0.031 | Tree loss: 2.300 | Accuracy: 0.297500 | 1.06 sec/iter\n",
      "Epoch: 40 | Batch: 001 / 011 | Total loss: 2.256 | Reg loss: 0.031 | Tree loss: 2.256 | Accuracy: 0.288000 | 1.06 sec/iter\n",
      "Epoch: 40 | Batch: 002 / 011 | Total loss: 2.193 | Reg loss: 0.031 | Tree loss: 2.193 | Accuracy: 0.320500 | 1.06 sec/iter\n",
      "Epoch: 40 | Batch: 003 / 011 | Total loss: 2.180 | Reg loss: 0.031 | Tree loss: 2.180 | Accuracy: 0.315000 | 1.06 sec/iter\n",
      "Epoch: 40 | Batch: 004 / 011 | Total loss: 2.138 | Reg loss: 0.031 | Tree loss: 2.138 | Accuracy: 0.332000 | 1.06 sec/iter\n",
      "Epoch: 40 | Batch: 005 / 011 | Total loss: 2.082 | Reg loss: 0.031 | Tree loss: 2.082 | Accuracy: 0.348500 | 1.06 sec/iter\n",
      "Epoch: 40 | Batch: 006 / 011 | Total loss: 2.068 | Reg loss: 0.031 | Tree loss: 2.068 | Accuracy: 0.342500 | 1.06 sec/iter\n",
      "Epoch: 40 | Batch: 007 / 011 | Total loss: 2.019 | Reg loss: 0.031 | Tree loss: 2.019 | Accuracy: 0.364500 | 1.06 sec/iter\n",
      "Epoch: 40 | Batch: 008 / 011 | Total loss: 2.034 | Reg loss: 0.031 | Tree loss: 2.034 | Accuracy: 0.347000 | 1.06 sec/iter\n",
      "Epoch: 40 | Batch: 009 / 011 | Total loss: 2.041 | Reg loss: 0.031 | Tree loss: 2.041 | Accuracy: 0.362500 | 1.06 sec/iter\n",
      "Epoch: 40 | Batch: 010 / 011 | Total loss: 2.058 | Reg loss: 0.031 | Tree loss: 2.058 | Accuracy: 0.341297 | 1.059 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 41 | Batch: 000 / 011 | Total loss: 2.269 | Reg loss: 0.031 | Tree loss: 2.269 | Accuracy: 0.314500 | 1.06 sec/iter\n",
      "Epoch: 41 | Batch: 001 / 011 | Total loss: 2.249 | Reg loss: 0.031 | Tree loss: 2.249 | Accuracy: 0.310000 | 1.06 sec/iter\n",
      "Epoch: 41 | Batch: 002 / 011 | Total loss: 2.182 | Reg loss: 0.031 | Tree loss: 2.182 | Accuracy: 0.309500 | 1.06 sec/iter\n",
      "Epoch: 41 | Batch: 003 / 011 | Total loss: 2.154 | Reg loss: 0.031 | Tree loss: 2.154 | Accuracy: 0.335000 | 1.06 sec/iter\n",
      "Epoch: 41 | Batch: 004 / 011 | Total loss: 2.143 | Reg loss: 0.031 | Tree loss: 2.143 | Accuracy: 0.308500 | 1.06 sec/iter\n",
      "Epoch: 41 | Batch: 005 / 011 | Total loss: 2.073 | Reg loss: 0.031 | Tree loss: 2.073 | Accuracy: 0.333500 | 1.06 sec/iter\n",
      "Epoch: 41 | Batch: 006 / 011 | Total loss: 2.033 | Reg loss: 0.031 | Tree loss: 2.033 | Accuracy: 0.365000 | 1.06 sec/iter\n",
      "Epoch: 41 | Batch: 007 / 011 | Total loss: 2.032 | Reg loss: 0.031 | Tree loss: 2.032 | Accuracy: 0.322500 | 1.06 sec/iter\n",
      "Epoch: 41 | Batch: 008 / 011 | Total loss: 2.036 | Reg loss: 0.031 | Tree loss: 2.036 | Accuracy: 0.344000 | 1.06 sec/iter\n",
      "Epoch: 41 | Batch: 009 / 011 | Total loss: 2.021 | Reg loss: 0.031 | Tree loss: 2.021 | Accuracy: 0.362000 | 1.06 sec/iter\n",
      "Epoch: 41 | Batch: 010 / 011 | Total loss: 2.037 | Reg loss: 0.031 | Tree loss: 2.037 | Accuracy: 0.365188 | 1.059 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 42 | Batch: 000 / 011 | Total loss: 2.290 | Reg loss: 0.031 | Tree loss: 2.290 | Accuracy: 0.280000 | 1.06 sec/iter\n",
      "Epoch: 42 | Batch: 001 / 011 | Total loss: 2.228 | Reg loss: 0.031 | Tree loss: 2.228 | Accuracy: 0.309500 | 1.06 sec/iter\n",
      "Epoch: 42 | Batch: 002 / 011 | Total loss: 2.218 | Reg loss: 0.031 | Tree loss: 2.218 | Accuracy: 0.292500 | 1.06 sec/iter\n",
      "Epoch: 42 | Batch: 003 / 011 | Total loss: 2.126 | Reg loss: 0.031 | Tree loss: 2.126 | Accuracy: 0.339000 | 1.06 sec/iter\n",
      "Epoch: 42 | Batch: 004 / 011 | Total loss: 2.096 | Reg loss: 0.031 | Tree loss: 2.096 | Accuracy: 0.345500 | 1.06 sec/iter\n",
      "Epoch: 42 | Batch: 005 / 011 | Total loss: 2.058 | Reg loss: 0.031 | Tree loss: 2.058 | Accuracy: 0.348000 | 1.06 sec/iter\n",
      "Epoch: 42 | Batch: 006 / 011 | Total loss: 2.044 | Reg loss: 0.031 | Tree loss: 2.044 | Accuracy: 0.336500 | 1.06 sec/iter\n",
      "Epoch: 42 | Batch: 007 / 011 | Total loss: 2.017 | Reg loss: 0.031 | Tree loss: 2.017 | Accuracy: 0.355500 | 1.06 sec/iter\n",
      "Epoch: 42 | Batch: 008 / 011 | Total loss: 2.001 | Reg loss: 0.032 | Tree loss: 2.001 | Accuracy: 0.363000 | 1.06 sec/iter\n",
      "Epoch: 42 | Batch: 009 / 011 | Total loss: 2.022 | Reg loss: 0.032 | Tree loss: 2.022 | Accuracy: 0.347000 | 1.06 sec/iter\n",
      "Epoch: 42 | Batch: 010 / 011 | Total loss: 1.947 | Reg loss: 0.032 | Tree loss: 1.947 | Accuracy: 0.354949 | 1.06 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 43 | Batch: 000 / 011 | Total loss: 2.278 | Reg loss: 0.031 | Tree loss: 2.278 | Accuracy: 0.288500 | 1.06 sec/iter\n",
      "Epoch: 43 | Batch: 001 / 011 | Total loss: 2.224 | Reg loss: 0.031 | Tree loss: 2.224 | Accuracy: 0.287000 | 1.06 sec/iter\n",
      "Epoch: 43 | Batch: 002 / 011 | Total loss: 2.184 | Reg loss: 0.031 | Tree loss: 2.184 | Accuracy: 0.315500 | 1.06 sec/iter\n",
      "Epoch: 43 | Batch: 003 / 011 | Total loss: 2.126 | Reg loss: 0.031 | Tree loss: 2.126 | Accuracy: 0.344000 | 1.06 sec/iter\n",
      "Epoch: 43 | Batch: 004 / 011 | Total loss: 2.077 | Reg loss: 0.032 | Tree loss: 2.077 | Accuracy: 0.352500 | 1.06 sec/iter\n",
      "Epoch: 43 | Batch: 005 / 011 | Total loss: 2.066 | Reg loss: 0.032 | Tree loss: 2.066 | Accuracy: 0.330000 | 1.06 sec/iter\n",
      "Epoch: 43 | Batch: 006 / 011 | Total loss: 2.038 | Reg loss: 0.032 | Tree loss: 2.038 | Accuracy: 0.336000 | 1.06 sec/iter\n",
      "Epoch: 43 | Batch: 007 / 011 | Total loss: 2.016 | Reg loss: 0.032 | Tree loss: 2.016 | Accuracy: 0.350500 | 1.06 sec/iter\n",
      "Epoch: 43 | Batch: 008 / 011 | Total loss: 2.000 | Reg loss: 0.032 | Tree loss: 2.000 | Accuracy: 0.353000 | 1.06 sec/iter\n",
      "Epoch: 43 | Batch: 009 / 011 | Total loss: 1.992 | Reg loss: 0.032 | Tree loss: 1.992 | Accuracy: 0.367500 | 1.06 sec/iter\n",
      "Epoch: 43 | Batch: 010 / 011 | Total loss: 2.046 | Reg loss: 0.032 | Tree loss: 2.046 | Accuracy: 0.334471 | 1.059 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 44 | Batch: 000 / 011 | Total loss: 2.231 | Reg loss: 0.032 | Tree loss: 2.231 | Accuracy: 0.312500 | 1.06 sec/iter\n",
      "Epoch: 44 | Batch: 001 / 011 | Total loss: 2.215 | Reg loss: 0.032 | Tree loss: 2.215 | Accuracy: 0.314500 | 1.06 sec/iter\n",
      "Epoch: 44 | Batch: 002 / 011 | Total loss: 2.162 | Reg loss: 0.032 | Tree loss: 2.162 | Accuracy: 0.323000 | 1.06 sec/iter\n",
      "Epoch: 44 | Batch: 003 / 011 | Total loss: 2.123 | Reg loss: 0.032 | Tree loss: 2.123 | Accuracy: 0.315500 | 1.06 sec/iter\n",
      "Epoch: 44 | Batch: 004 / 011 | Total loss: 2.117 | Reg loss: 0.032 | Tree loss: 2.117 | Accuracy: 0.304500 | 1.06 sec/iter\n",
      "Epoch: 44 | Batch: 005 / 011 | Total loss: 2.046 | Reg loss: 0.032 | Tree loss: 2.046 | Accuracy: 0.348500 | 1.06 sec/iter\n",
      "Epoch: 44 | Batch: 006 / 011 | Total loss: 2.021 | Reg loss: 0.032 | Tree loss: 2.021 | Accuracy: 0.342500 | 1.06 sec/iter\n",
      "Epoch: 44 | Batch: 007 / 011 | Total loss: 2.031 | Reg loss: 0.032 | Tree loss: 2.031 | Accuracy: 0.347000 | 1.06 sec/iter\n",
      "Epoch: 44 | Batch: 008 / 011 | Total loss: 1.994 | Reg loss: 0.032 | Tree loss: 1.994 | Accuracy: 0.355500 | 1.06 sec/iter\n",
      "Epoch: 44 | Batch: 009 / 011 | Total loss: 1.976 | Reg loss: 0.032 | Tree loss: 1.976 | Accuracy: 0.374000 | 1.06 sec/iter\n",
      "Epoch: 44 | Batch: 010 / 011 | Total loss: 1.935 | Reg loss: 0.032 | Tree loss: 1.935 | Accuracy: 0.382253 | 1.059 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 45 | Batch: 000 / 011 | Total loss: 2.234 | Reg loss: 0.032 | Tree loss: 2.234 | Accuracy: 0.317500 | 1.06 sec/iter\n",
      "Epoch: 45 | Batch: 001 / 011 | Total loss: 2.187 | Reg loss: 0.032 | Tree loss: 2.187 | Accuracy: 0.320000 | 1.06 sec/iter\n",
      "Epoch: 45 | Batch: 002 / 011 | Total loss: 2.146 | Reg loss: 0.032 | Tree loss: 2.146 | Accuracy: 0.315000 | 1.06 sec/iter\n",
      "Epoch: 45 | Batch: 003 / 011 | Total loss: 2.120 | Reg loss: 0.032 | Tree loss: 2.120 | Accuracy: 0.332000 | 1.06 sec/iter\n",
      "Epoch: 45 | Batch: 004 / 011 | Total loss: 2.114 | Reg loss: 0.032 | Tree loss: 2.114 | Accuracy: 0.320500 | 1.06 sec/iter\n",
      "Epoch: 45 | Batch: 005 / 011 | Total loss: 2.035 | Reg loss: 0.032 | Tree loss: 2.035 | Accuracy: 0.354500 | 1.06 sec/iter\n",
      "Epoch: 45 | Batch: 006 / 011 | Total loss: 2.026 | Reg loss: 0.032 | Tree loss: 2.026 | Accuracy: 0.332500 | 1.06 sec/iter\n",
      "Epoch: 45 | Batch: 007 / 011 | Total loss: 1.999 | Reg loss: 0.032 | Tree loss: 1.999 | Accuracy: 0.358000 | 1.06 sec/iter\n",
      "Epoch: 45 | Batch: 008 / 011 | Total loss: 1.973 | Reg loss: 0.032 | Tree loss: 1.973 | Accuracy: 0.359000 | 1.06 sec/iter\n",
      "Epoch: 45 | Batch: 009 / 011 | Total loss: 1.993 | Reg loss: 0.032 | Tree loss: 1.993 | Accuracy: 0.369000 | 1.06 sec/iter\n",
      "Epoch: 45 | Batch: 010 / 011 | Total loss: 1.993 | Reg loss: 0.032 | Tree loss: 1.993 | Accuracy: 0.351536 | 1.059 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 46 | Batch: 000 / 011 | Total loss: 2.258 | Reg loss: 0.032 | Tree loss: 2.258 | Accuracy: 0.300000 | 1.06 sec/iter\n",
      "Epoch: 46 | Batch: 001 / 011 | Total loss: 2.176 | Reg loss: 0.032 | Tree loss: 2.176 | Accuracy: 0.325000 | 1.06 sec/iter\n",
      "Epoch: 46 | Batch: 002 / 011 | Total loss: 2.140 | Reg loss: 0.032 | Tree loss: 2.140 | Accuracy: 0.338000 | 1.06 sec/iter\n",
      "Epoch: 46 | Batch: 003 / 011 | Total loss: 2.068 | Reg loss: 0.032 | Tree loss: 2.068 | Accuracy: 0.352500 | 1.06 sec/iter\n",
      "Epoch: 46 | Batch: 004 / 011 | Total loss: 2.081 | Reg loss: 0.032 | Tree loss: 2.081 | Accuracy: 0.335000 | 1.06 sec/iter\n",
      "Epoch: 46 | Batch: 005 / 011 | Total loss: 2.061 | Reg loss: 0.032 | Tree loss: 2.061 | Accuracy: 0.324500 | 1.06 sec/iter\n",
      "Epoch: 46 | Batch: 006 / 011 | Total loss: 2.028 | Reg loss: 0.032 | Tree loss: 2.028 | Accuracy: 0.335500 | 1.06 sec/iter\n",
      "Epoch: 46 | Batch: 007 / 011 | Total loss: 1.964 | Reg loss: 0.032 | Tree loss: 1.964 | Accuracy: 0.378500 | 1.059 sec/iter\n",
      "Epoch: 46 | Batch: 008 / 011 | Total loss: 2.002 | Reg loss: 0.032 | Tree loss: 2.002 | Accuracy: 0.347500 | 1.059 sec/iter\n",
      "Epoch: 46 | Batch: 009 / 011 | Total loss: 1.965 | Reg loss: 0.032 | Tree loss: 1.965 | Accuracy: 0.388500 | 1.06 sec/iter\n",
      "Epoch: 46 | Batch: 010 / 011 | Total loss: 1.978 | Reg loss: 0.032 | Tree loss: 1.978 | Accuracy: 0.344710 | 1.059 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 47 | Batch: 000 / 011 | Total loss: 2.223 | Reg loss: 0.032 | Tree loss: 2.223 | Accuracy: 0.302500 | 1.06 sec/iter\n",
      "Epoch: 47 | Batch: 001 / 011 | Total loss: 2.210 | Reg loss: 0.032 | Tree loss: 2.210 | Accuracy: 0.308000 | 1.06 sec/iter\n",
      "Epoch: 47 | Batch: 002 / 011 | Total loss: 2.159 | Reg loss: 0.032 | Tree loss: 2.159 | Accuracy: 0.326500 | 1.06 sec/iter\n",
      "Epoch: 47 | Batch: 003 / 011 | Total loss: 2.100 | Reg loss: 0.032 | Tree loss: 2.100 | Accuracy: 0.341000 | 1.06 sec/iter\n",
      "Epoch: 47 | Batch: 004 / 011 | Total loss: 2.048 | Reg loss: 0.032 | Tree loss: 2.048 | Accuracy: 0.341000 | 1.06 sec/iter\n",
      "Epoch: 47 | Batch: 005 / 011 | Total loss: 2.028 | Reg loss: 0.032 | Tree loss: 2.028 | Accuracy: 0.334500 | 1.06 sec/iter\n",
      "Epoch: 47 | Batch: 006 / 011 | Total loss: 2.004 | Reg loss: 0.032 | Tree loss: 2.004 | Accuracy: 0.319000 | 1.06 sec/iter\n",
      "Epoch: 47 | Batch: 007 / 011 | Total loss: 1.993 | Reg loss: 0.032 | Tree loss: 1.993 | Accuracy: 0.353000 | 1.059 sec/iter\n",
      "Epoch: 47 | Batch: 008 / 011 | Total loss: 1.966 | Reg loss: 0.032 | Tree loss: 1.966 | Accuracy: 0.365500 | 1.059 sec/iter\n",
      "Epoch: 47 | Batch: 009 / 011 | Total loss: 1.937 | Reg loss: 0.032 | Tree loss: 1.937 | Accuracy: 0.400500 | 1.059 sec/iter\n",
      "Epoch: 47 | Batch: 010 / 011 | Total loss: 1.939 | Reg loss: 0.033 | Tree loss: 1.939 | Accuracy: 0.395904 | 1.059 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 48 | Batch: 000 / 011 | Total loss: 2.232 | Reg loss: 0.032 | Tree loss: 2.232 | Accuracy: 0.309000 | 1.06 sec/iter\n",
      "Epoch: 48 | Batch: 001 / 011 | Total loss: 2.163 | Reg loss: 0.032 | Tree loss: 2.163 | Accuracy: 0.317000 | 1.06 sec/iter\n",
      "Epoch: 48 | Batch: 002 / 011 | Total loss: 2.149 | Reg loss: 0.032 | Tree loss: 2.149 | Accuracy: 0.323500 | 1.06 sec/iter\n",
      "Epoch: 48 | Batch: 003 / 011 | Total loss: 2.114 | Reg loss: 0.032 | Tree loss: 2.114 | Accuracy: 0.328000 | 1.06 sec/iter\n",
      "Epoch: 48 | Batch: 004 / 011 | Total loss: 2.079 | Reg loss: 0.032 | Tree loss: 2.079 | Accuracy: 0.325500 | 1.059 sec/iter\n",
      "Epoch: 48 | Batch: 005 / 011 | Total loss: 2.015 | Reg loss: 0.032 | Tree loss: 2.015 | Accuracy: 0.347500 | 1.059 sec/iter\n",
      "Epoch: 48 | Batch: 006 / 011 | Total loss: 1.992 | Reg loss: 0.032 | Tree loss: 1.992 | Accuracy: 0.341500 | 1.06 sec/iter\n",
      "Epoch: 48 | Batch: 007 / 011 | Total loss: 1.948 | Reg loss: 0.033 | Tree loss: 1.948 | Accuracy: 0.366500 | 1.059 sec/iter\n",
      "Epoch: 48 | Batch: 008 / 011 | Total loss: 1.927 | Reg loss: 0.033 | Tree loss: 1.927 | Accuracy: 0.386500 | 1.059 sec/iter\n",
      "Epoch: 48 | Batch: 009 / 011 | Total loss: 1.959 | Reg loss: 0.033 | Tree loss: 1.959 | Accuracy: 0.382500 | 1.059 sec/iter\n",
      "Epoch: 48 | Batch: 010 / 011 | Total loss: 1.967 | Reg loss: 0.033 | Tree loss: 1.967 | Accuracy: 0.368601 | 1.059 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 49 | Batch: 000 / 011 | Total loss: 2.210 | Reg loss: 0.032 | Tree loss: 2.210 | Accuracy: 0.307500 | 1.06 sec/iter\n",
      "Epoch: 49 | Batch: 001 / 011 | Total loss: 2.190 | Reg loss: 0.032 | Tree loss: 2.190 | Accuracy: 0.299000 | 1.06 sec/iter\n",
      "Epoch: 49 | Batch: 002 / 011 | Total loss: 2.127 | Reg loss: 0.033 | Tree loss: 2.127 | Accuracy: 0.334500 | 1.06 sec/iter\n",
      "Epoch: 49 | Batch: 003 / 011 | Total loss: 2.075 | Reg loss: 0.033 | Tree loss: 2.075 | Accuracy: 0.339000 | 1.06 sec/iter\n",
      "Epoch: 49 | Batch: 004 / 011 | Total loss: 2.042 | Reg loss: 0.033 | Tree loss: 2.042 | Accuracy: 0.350500 | 1.059 sec/iter\n",
      "Epoch: 49 | Batch: 005 / 011 | Total loss: 2.021 | Reg loss: 0.033 | Tree loss: 2.021 | Accuracy: 0.333000 | 1.059 sec/iter\n",
      "Epoch: 49 | Batch: 006 / 011 | Total loss: 2.004 | Reg loss: 0.033 | Tree loss: 2.004 | Accuracy: 0.341000 | 1.059 sec/iter\n",
      "Epoch: 49 | Batch: 007 / 011 | Total loss: 1.960 | Reg loss: 0.033 | Tree loss: 1.960 | Accuracy: 0.361500 | 1.059 sec/iter\n",
      "Epoch: 49 | Batch: 008 / 011 | Total loss: 1.936 | Reg loss: 0.033 | Tree loss: 1.936 | Accuracy: 0.387500 | 1.059 sec/iter\n",
      "Epoch: 49 | Batch: 009 / 011 | Total loss: 1.959 | Reg loss: 0.033 | Tree loss: 1.959 | Accuracy: 0.378000 | 1.059 sec/iter\n",
      "Epoch: 49 | Batch: 010 / 011 | Total loss: 1.937 | Reg loss: 0.033 | Tree loss: 1.937 | Accuracy: 0.416382 | 1.058 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50 | Batch: 000 / 011 | Total loss: 2.229 | Reg loss: 0.033 | Tree loss: 2.229 | Accuracy: 0.312000 | 1.058 sec/iter\n",
      "Epoch: 50 | Batch: 001 / 011 | Total loss: 2.158 | Reg loss: 0.033 | Tree loss: 2.158 | Accuracy: 0.315500 | 1.058 sec/iter\n",
      "Epoch: 50 | Batch: 002 / 011 | Total loss: 2.138 | Reg loss: 0.033 | Tree loss: 2.138 | Accuracy: 0.322000 | 1.058 sec/iter\n",
      "Epoch: 50 | Batch: 003 / 011 | Total loss: 2.052 | Reg loss: 0.033 | Tree loss: 2.052 | Accuracy: 0.347000 | 1.058 sec/iter\n",
      "Epoch: 50 | Batch: 004 / 011 | Total loss: 2.045 | Reg loss: 0.033 | Tree loss: 2.045 | Accuracy: 0.338000 | 1.057 sec/iter\n",
      "Epoch: 50 | Batch: 005 / 011 | Total loss: 1.994 | Reg loss: 0.033 | Tree loss: 1.994 | Accuracy: 0.347500 | 1.057 sec/iter\n",
      "Epoch: 50 | Batch: 006 / 011 | Total loss: 1.971 | Reg loss: 0.033 | Tree loss: 1.971 | Accuracy: 0.358000 | 1.057 sec/iter\n",
      "Epoch: 50 | Batch: 007 / 011 | Total loss: 1.967 | Reg loss: 0.033 | Tree loss: 1.967 | Accuracy: 0.357500 | 1.057 sec/iter\n",
      "Epoch: 50 | Batch: 008 / 011 | Total loss: 1.964 | Reg loss: 0.033 | Tree loss: 1.964 | Accuracy: 0.362500 | 1.056 sec/iter\n",
      "Epoch: 50 | Batch: 009 / 011 | Total loss: 1.948 | Reg loss: 0.033 | Tree loss: 1.948 | Accuracy: 0.369500 | 1.056 sec/iter\n",
      "Epoch: 50 | Batch: 010 / 011 | Total loss: 1.965 | Reg loss: 0.033 | Tree loss: 1.965 | Accuracy: 0.368601 | 1.056 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 51 | Batch: 000 / 011 | Total loss: 2.192 | Reg loss: 0.033 | Tree loss: 2.192 | Accuracy: 0.311000 | 1.056 sec/iter\n",
      "Epoch: 51 | Batch: 001 / 011 | Total loss: 2.148 | Reg loss: 0.033 | Tree loss: 2.148 | Accuracy: 0.311000 | 1.055 sec/iter\n",
      "Epoch: 51 | Batch: 002 / 011 | Total loss: 2.121 | Reg loss: 0.033 | Tree loss: 2.121 | Accuracy: 0.327500 | 1.055 sec/iter\n",
      "Epoch: 51 | Batch: 003 / 011 | Total loss: 2.089 | Reg loss: 0.033 | Tree loss: 2.089 | Accuracy: 0.323000 | 1.055 sec/iter\n",
      "Epoch: 51 | Batch: 004 / 011 | Total loss: 2.048 | Reg loss: 0.033 | Tree loss: 2.048 | Accuracy: 0.342000 | 1.055 sec/iter\n",
      "Epoch: 51 | Batch: 005 / 011 | Total loss: 2.002 | Reg loss: 0.033 | Tree loss: 2.002 | Accuracy: 0.334000 | 1.054 sec/iter\n",
      "Epoch: 51 | Batch: 006 / 011 | Total loss: 1.971 | Reg loss: 0.033 | Tree loss: 1.971 | Accuracy: 0.367500 | 1.054 sec/iter\n",
      "Epoch: 51 | Batch: 007 / 011 | Total loss: 1.961 | Reg loss: 0.033 | Tree loss: 1.961 | Accuracy: 0.367500 | 1.054 sec/iter\n",
      "Epoch: 51 | Batch: 008 / 011 | Total loss: 1.925 | Reg loss: 0.033 | Tree loss: 1.925 | Accuracy: 0.393000 | 1.054 sec/iter\n",
      "Epoch: 51 | Batch: 009 / 011 | Total loss: 1.948 | Reg loss: 0.033 | Tree loss: 1.948 | Accuracy: 0.375500 | 1.053 sec/iter\n",
      "Epoch: 51 | Batch: 010 / 011 | Total loss: 1.948 | Reg loss: 0.033 | Tree loss: 1.948 | Accuracy: 0.395904 | 1.053 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 52 | Batch: 000 / 011 | Total loss: 2.168 | Reg loss: 0.033 | Tree loss: 2.168 | Accuracy: 0.322000 | 1.053 sec/iter\n",
      "Epoch: 52 | Batch: 001 / 011 | Total loss: 2.154 | Reg loss: 0.033 | Tree loss: 2.154 | Accuracy: 0.316500 | 1.053 sec/iter\n",
      "Epoch: 52 | Batch: 002 / 011 | Total loss: 2.114 | Reg loss: 0.033 | Tree loss: 2.114 | Accuracy: 0.320500 | 1.052 sec/iter\n",
      "Epoch: 52 | Batch: 003 / 011 | Total loss: 2.076 | Reg loss: 0.033 | Tree loss: 2.076 | Accuracy: 0.338000 | 1.052 sec/iter\n",
      "Epoch: 52 | Batch: 004 / 011 | Total loss: 2.005 | Reg loss: 0.033 | Tree loss: 2.005 | Accuracy: 0.371000 | 1.052 sec/iter\n",
      "Epoch: 52 | Batch: 005 / 011 | Total loss: 2.005 | Reg loss: 0.033 | Tree loss: 2.005 | Accuracy: 0.335500 | 1.052 sec/iter\n",
      "Epoch: 52 | Batch: 006 / 011 | Total loss: 1.979 | Reg loss: 0.033 | Tree loss: 1.979 | Accuracy: 0.346000 | 1.051 sec/iter\n",
      "Epoch: 52 | Batch: 007 / 011 | Total loss: 1.962 | Reg loss: 0.033 | Tree loss: 1.962 | Accuracy: 0.358500 | 1.051 sec/iter\n",
      "Epoch: 52 | Batch: 008 / 011 | Total loss: 1.939 | Reg loss: 0.033 | Tree loss: 1.939 | Accuracy: 0.374500 | 1.051 sec/iter\n",
      "Epoch: 52 | Batch: 009 / 011 | Total loss: 1.940 | Reg loss: 0.033 | Tree loss: 1.940 | Accuracy: 0.376500 | 1.051 sec/iter\n",
      "Epoch: 52 | Batch: 010 / 011 | Total loss: 1.940 | Reg loss: 0.033 | Tree loss: 1.940 | Accuracy: 0.402730 | 1.05 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 53 | Batch: 000 / 011 | Total loss: 2.195 | Reg loss: 0.033 | Tree loss: 2.195 | Accuracy: 0.298000 | 1.05 sec/iter\n",
      "Epoch: 53 | Batch: 001 / 011 | Total loss: 2.149 | Reg loss: 0.033 | Tree loss: 2.149 | Accuracy: 0.324500 | 1.05 sec/iter\n",
      "Epoch: 53 | Batch: 002 / 011 | Total loss: 2.093 | Reg loss: 0.033 | Tree loss: 2.093 | Accuracy: 0.334500 | 1.05 sec/iter\n",
      "Epoch: 53 | Batch: 003 / 011 | Total loss: 2.069 | Reg loss: 0.033 | Tree loss: 2.069 | Accuracy: 0.330500 | 1.05 sec/iter\n",
      "Epoch: 53 | Batch: 004 / 011 | Total loss: 2.012 | Reg loss: 0.033 | Tree loss: 2.012 | Accuracy: 0.356500 | 1.049 sec/iter\n",
      "Epoch: 53 | Batch: 005 / 011 | Total loss: 1.984 | Reg loss: 0.033 | Tree loss: 1.984 | Accuracy: 0.342500 | 1.049 sec/iter\n",
      "Epoch: 53 | Batch: 006 / 011 | Total loss: 1.951 | Reg loss: 0.033 | Tree loss: 1.951 | Accuracy: 0.376000 | 1.049 sec/iter\n",
      "Epoch: 53 | Batch: 007 / 011 | Total loss: 1.940 | Reg loss: 0.033 | Tree loss: 1.940 | Accuracy: 0.361500 | 1.049 sec/iter\n",
      "Epoch: 53 | Batch: 008 / 011 | Total loss: 1.949 | Reg loss: 0.033 | Tree loss: 1.949 | Accuracy: 0.380000 | 1.049 sec/iter\n",
      "Epoch: 53 | Batch: 009 / 011 | Total loss: 1.931 | Reg loss: 0.033 | Tree loss: 1.931 | Accuracy: 0.384000 | 1.048 sec/iter\n",
      "Epoch: 53 | Batch: 010 / 011 | Total loss: 1.893 | Reg loss: 0.034 | Tree loss: 1.893 | Accuracy: 0.395904 | 1.048 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 54 | Batch: 000 / 011 | Total loss: 2.170 | Reg loss: 0.033 | Tree loss: 2.170 | Accuracy: 0.318000 | 1.048 sec/iter\n",
      "Epoch: 54 | Batch: 001 / 011 | Total loss: 2.126 | Reg loss: 0.033 | Tree loss: 2.126 | Accuracy: 0.333500 | 1.048 sec/iter\n",
      "Epoch: 54 | Batch: 002 / 011 | Total loss: 2.104 | Reg loss: 0.033 | Tree loss: 2.104 | Accuracy: 0.316000 | 1.047 sec/iter\n",
      "Epoch: 54 | Batch: 003 / 011 | Total loss: 2.056 | Reg loss: 0.033 | Tree loss: 2.056 | Accuracy: 0.341000 | 1.047 sec/iter\n",
      "Epoch: 54 | Batch: 004 / 011 | Total loss: 2.011 | Reg loss: 0.033 | Tree loss: 2.011 | Accuracy: 0.352500 | 1.047 sec/iter\n",
      "Epoch: 54 | Batch: 005 / 011 | Total loss: 1.991 | Reg loss: 0.033 | Tree loss: 1.991 | Accuracy: 0.360000 | 1.047 sec/iter\n",
      "Epoch: 54 | Batch: 006 / 011 | Total loss: 1.962 | Reg loss: 0.033 | Tree loss: 1.962 | Accuracy: 0.345500 | 1.047 sec/iter\n",
      "Epoch: 54 | Batch: 007 / 011 | Total loss: 1.938 | Reg loss: 0.034 | Tree loss: 1.938 | Accuracy: 0.371500 | 1.046 sec/iter\n",
      "Epoch: 54 | Batch: 008 / 011 | Total loss: 1.930 | Reg loss: 0.034 | Tree loss: 1.930 | Accuracy: 0.381000 | 1.046 sec/iter\n",
      "Epoch: 54 | Batch: 009 / 011 | Total loss: 1.923 | Reg loss: 0.034 | Tree loss: 1.923 | Accuracy: 0.382000 | 1.046 sec/iter\n",
      "Epoch: 54 | Batch: 010 / 011 | Total loss: 1.944 | Reg loss: 0.034 | Tree loss: 1.944 | Accuracy: 0.327645 | 1.046 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 55 | Batch: 000 / 011 | Total loss: 2.176 | Reg loss: 0.033 | Tree loss: 2.176 | Accuracy: 0.329500 | 1.046 sec/iter\n",
      "Epoch: 55 | Batch: 001 / 011 | Total loss: 2.130 | Reg loss: 0.033 | Tree loss: 2.130 | Accuracy: 0.329000 | 1.045 sec/iter\n",
      "Epoch: 55 | Batch: 002 / 011 | Total loss: 2.098 | Reg loss: 0.033 | Tree loss: 2.098 | Accuracy: 0.336000 | 1.045 sec/iter\n",
      "Epoch: 55 | Batch: 003 / 011 | Total loss: 2.045 | Reg loss: 0.034 | Tree loss: 2.045 | Accuracy: 0.341500 | 1.045 sec/iter\n",
      "Epoch: 55 | Batch: 004 / 011 | Total loss: 2.013 | Reg loss: 0.034 | Tree loss: 2.013 | Accuracy: 0.351000 | 1.045 sec/iter\n",
      "Epoch: 55 | Batch: 005 / 011 | Total loss: 1.955 | Reg loss: 0.034 | Tree loss: 1.955 | Accuracy: 0.364500 | 1.045 sec/iter\n",
      "Epoch: 55 | Batch: 006 / 011 | Total loss: 1.953 | Reg loss: 0.034 | Tree loss: 1.953 | Accuracy: 0.359000 | 1.044 sec/iter\n",
      "Epoch: 55 | Batch: 007 / 011 | Total loss: 1.915 | Reg loss: 0.034 | Tree loss: 1.915 | Accuracy: 0.383500 | 1.044 sec/iter\n",
      "Epoch: 55 | Batch: 008 / 011 | Total loss: 1.934 | Reg loss: 0.034 | Tree loss: 1.934 | Accuracy: 0.374000 | 1.044 sec/iter\n",
      "Epoch: 55 | Batch: 009 / 011 | Total loss: 1.937 | Reg loss: 0.034 | Tree loss: 1.937 | Accuracy: 0.372000 | 1.044 sec/iter\n",
      "Epoch: 55 | Batch: 010 / 011 | Total loss: 1.974 | Reg loss: 0.034 | Tree loss: 1.974 | Accuracy: 0.351536 | 1.043 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 56 | Batch: 000 / 011 | Total loss: 2.170 | Reg loss: 0.034 | Tree loss: 2.170 | Accuracy: 0.320000 | 1.043 sec/iter\n",
      "Epoch: 56 | Batch: 001 / 011 | Total loss: 2.125 | Reg loss: 0.034 | Tree loss: 2.125 | Accuracy: 0.339500 | 1.043 sec/iter\n",
      "Epoch: 56 | Batch: 002 / 011 | Total loss: 2.079 | Reg loss: 0.034 | Tree loss: 2.079 | Accuracy: 0.334500 | 1.043 sec/iter\n",
      "Epoch: 56 | Batch: 003 / 011 | Total loss: 2.044 | Reg loss: 0.034 | Tree loss: 2.044 | Accuracy: 0.329000 | 1.043 sec/iter\n",
      "Epoch: 56 | Batch: 004 / 011 | Total loss: 2.025 | Reg loss: 0.034 | Tree loss: 2.025 | Accuracy: 0.347500 | 1.042 sec/iter\n",
      "Epoch: 56 | Batch: 005 / 011 | Total loss: 1.970 | Reg loss: 0.034 | Tree loss: 1.970 | Accuracy: 0.359000 | 1.042 sec/iter\n",
      "Epoch: 56 | Batch: 006 / 011 | Total loss: 1.947 | Reg loss: 0.034 | Tree loss: 1.947 | Accuracy: 0.365500 | 1.042 sec/iter\n",
      "Epoch: 56 | Batch: 007 / 011 | Total loss: 1.914 | Reg loss: 0.034 | Tree loss: 1.914 | Accuracy: 0.395000 | 1.042 sec/iter\n",
      "Epoch: 56 | Batch: 008 / 011 | Total loss: 1.908 | Reg loss: 0.034 | Tree loss: 1.908 | Accuracy: 0.390500 | 1.042 sec/iter\n",
      "Epoch: 56 | Batch: 009 / 011 | Total loss: 1.925 | Reg loss: 0.034 | Tree loss: 1.925 | Accuracy: 0.378000 | 1.041 sec/iter\n",
      "Epoch: 56 | Batch: 010 / 011 | Total loss: 1.924 | Reg loss: 0.034 | Tree loss: 1.924 | Accuracy: 0.354949 | 1.041 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 57 | Batch: 000 / 011 | Total loss: 2.141 | Reg loss: 0.034 | Tree loss: 2.141 | Accuracy: 0.301500 | 1.041 sec/iter\n",
      "Epoch: 57 | Batch: 001 / 011 | Total loss: 2.118 | Reg loss: 0.034 | Tree loss: 2.118 | Accuracy: 0.321000 | 1.041 sec/iter\n",
      "Epoch: 57 | Batch: 002 / 011 | Total loss: 2.113 | Reg loss: 0.034 | Tree loss: 2.113 | Accuracy: 0.320000 | 1.041 sec/iter\n",
      "Epoch: 57 | Batch: 003 / 011 | Total loss: 2.028 | Reg loss: 0.034 | Tree loss: 2.028 | Accuracy: 0.355000 | 1.04 sec/iter\n",
      "Epoch: 57 | Batch: 004 / 011 | Total loss: 2.009 | Reg loss: 0.034 | Tree loss: 2.009 | Accuracy: 0.367500 | 1.04 sec/iter\n",
      "Epoch: 57 | Batch: 005 / 011 | Total loss: 1.955 | Reg loss: 0.034 | Tree loss: 1.955 | Accuracy: 0.374500 | 1.04 sec/iter\n",
      "Epoch: 57 | Batch: 006 / 011 | Total loss: 1.933 | Reg loss: 0.034 | Tree loss: 1.933 | Accuracy: 0.396000 | 1.04 sec/iter\n",
      "Epoch: 57 | Batch: 007 / 011 | Total loss: 1.907 | Reg loss: 0.034 | Tree loss: 1.907 | Accuracy: 0.394000 | 1.04 sec/iter\n",
      "Epoch: 57 | Batch: 008 / 011 | Total loss: 1.944 | Reg loss: 0.034 | Tree loss: 1.944 | Accuracy: 0.361000 | 1.039 sec/iter\n",
      "Epoch: 57 | Batch: 009 / 011 | Total loss: 1.897 | Reg loss: 0.034 | Tree loss: 1.897 | Accuracy: 0.393500 | 1.039 sec/iter\n",
      "Epoch: 57 | Batch: 010 / 011 | Total loss: 1.911 | Reg loss: 0.034 | Tree loss: 1.911 | Accuracy: 0.389078 | 1.039 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 58 | Batch: 000 / 011 | Total loss: 2.159 | Reg loss: 0.034 | Tree loss: 2.159 | Accuracy: 0.328500 | 1.039 sec/iter\n",
      "Epoch: 58 | Batch: 001 / 011 | Total loss: 2.126 | Reg loss: 0.034 | Tree loss: 2.126 | Accuracy: 0.321500 | 1.039 sec/iter\n",
      "Epoch: 58 | Batch: 002 / 011 | Total loss: 2.064 | Reg loss: 0.034 | Tree loss: 2.064 | Accuracy: 0.328000 | 1.038 sec/iter\n",
      "Epoch: 58 | Batch: 003 / 011 | Total loss: 2.027 | Reg loss: 0.034 | Tree loss: 2.027 | Accuracy: 0.340500 | 1.038 sec/iter\n",
      "Epoch: 58 | Batch: 004 / 011 | Total loss: 1.988 | Reg loss: 0.034 | Tree loss: 1.988 | Accuracy: 0.355000 | 1.038 sec/iter\n",
      "Epoch: 58 | Batch: 005 / 011 | Total loss: 1.955 | Reg loss: 0.034 | Tree loss: 1.955 | Accuracy: 0.378000 | 1.038 sec/iter\n",
      "Epoch: 58 | Batch: 006 / 011 | Total loss: 1.930 | Reg loss: 0.034 | Tree loss: 1.930 | Accuracy: 0.385000 | 1.038 sec/iter\n",
      "Epoch: 58 | Batch: 007 / 011 | Total loss: 1.920 | Reg loss: 0.034 | Tree loss: 1.920 | Accuracy: 0.393500 | 1.037 sec/iter\n",
      "Epoch: 58 | Batch: 008 / 011 | Total loss: 1.929 | Reg loss: 0.034 | Tree loss: 1.929 | Accuracy: 0.374000 | 1.037 sec/iter\n",
      "Epoch: 58 | Batch: 009 / 011 | Total loss: 1.916 | Reg loss: 0.034 | Tree loss: 1.916 | Accuracy: 0.378500 | 1.037 sec/iter\n",
      "Epoch: 58 | Batch: 010 / 011 | Total loss: 1.835 | Reg loss: 0.034 | Tree loss: 1.835 | Accuracy: 0.440273 | 1.037 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 59 | Batch: 000 / 011 | Total loss: 2.142 | Reg loss: 0.034 | Tree loss: 2.142 | Accuracy: 0.340000 | 1.037 sec/iter\n",
      "Epoch: 59 | Batch: 001 / 011 | Total loss: 2.095 | Reg loss: 0.034 | Tree loss: 2.095 | Accuracy: 0.324500 | 1.037 sec/iter\n",
      "Epoch: 59 | Batch: 002 / 011 | Total loss: 2.059 | Reg loss: 0.034 | Tree loss: 2.059 | Accuracy: 0.332500 | 1.036 sec/iter\n",
      "Epoch: 59 | Batch: 003 / 011 | Total loss: 2.025 | Reg loss: 0.034 | Tree loss: 2.025 | Accuracy: 0.348500 | 1.036 sec/iter\n",
      "Epoch: 59 | Batch: 004 / 011 | Total loss: 1.992 | Reg loss: 0.034 | Tree loss: 1.992 | Accuracy: 0.359000 | 1.036 sec/iter\n",
      "Epoch: 59 | Batch: 005 / 011 | Total loss: 1.964 | Reg loss: 0.034 | Tree loss: 1.964 | Accuracy: 0.353000 | 1.036 sec/iter\n",
      "Epoch: 59 | Batch: 006 / 011 | Total loss: 1.931 | Reg loss: 0.034 | Tree loss: 1.931 | Accuracy: 0.367000 | 1.036 sec/iter\n",
      "Epoch: 59 | Batch: 007 / 011 | Total loss: 1.939 | Reg loss: 0.034 | Tree loss: 1.939 | Accuracy: 0.373000 | 1.035 sec/iter\n",
      "Epoch: 59 | Batch: 008 / 011 | Total loss: 1.895 | Reg loss: 0.034 | Tree loss: 1.895 | Accuracy: 0.398500 | 1.035 sec/iter\n",
      "Epoch: 59 | Batch: 009 / 011 | Total loss: 1.920 | Reg loss: 0.034 | Tree loss: 1.920 | Accuracy: 0.372000 | 1.035 sec/iter\n",
      "Epoch: 59 | Batch: 010 / 011 | Total loss: 1.869 | Reg loss: 0.034 | Tree loss: 1.869 | Accuracy: 0.419795 | 1.035 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 60 | Batch: 000 / 011 | Total loss: 2.149 | Reg loss: 0.034 | Tree loss: 2.149 | Accuracy: 0.326500 | 1.035 sec/iter\n",
      "Epoch: 60 | Batch: 001 / 011 | Total loss: 2.088 | Reg loss: 0.034 | Tree loss: 2.088 | Accuracy: 0.338500 | 1.034 sec/iter\n",
      "Epoch: 60 | Batch: 002 / 011 | Total loss: 2.072 | Reg loss: 0.034 | Tree loss: 2.072 | Accuracy: 0.339000 | 1.034 sec/iter\n",
      "Epoch: 60 | Batch: 003 / 011 | Total loss: 2.027 | Reg loss: 0.034 | Tree loss: 2.027 | Accuracy: 0.345500 | 1.034 sec/iter\n",
      "Epoch: 60 | Batch: 004 / 011 | Total loss: 1.993 | Reg loss: 0.034 | Tree loss: 1.993 | Accuracy: 0.354000 | 1.034 sec/iter\n",
      "Epoch: 60 | Batch: 005 / 011 | Total loss: 1.955 | Reg loss: 0.034 | Tree loss: 1.955 | Accuracy: 0.352500 | 1.034 sec/iter\n",
      "Epoch: 60 | Batch: 006 / 011 | Total loss: 1.929 | Reg loss: 0.034 | Tree loss: 1.929 | Accuracy: 0.380000 | 1.034 sec/iter\n",
      "Epoch: 60 | Batch: 007 / 011 | Total loss: 1.926 | Reg loss: 0.034 | Tree loss: 1.926 | Accuracy: 0.393500 | 1.033 sec/iter\n",
      "Epoch: 60 | Batch: 008 / 011 | Total loss: 1.875 | Reg loss: 0.034 | Tree loss: 1.875 | Accuracy: 0.389500 | 1.033 sec/iter\n",
      "Epoch: 60 | Batch: 009 / 011 | Total loss: 1.898 | Reg loss: 0.034 | Tree loss: 1.898 | Accuracy: 0.387500 | 1.033 sec/iter\n",
      "Epoch: 60 | Batch: 010 / 011 | Total loss: 1.873 | Reg loss: 0.035 | Tree loss: 1.873 | Accuracy: 0.457338 | 1.033 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 61 | Batch: 000 / 011 | Total loss: 2.131 | Reg loss: 0.034 | Tree loss: 2.131 | Accuracy: 0.335500 | 1.033 sec/iter\n",
      "Epoch: 61 | Batch: 001 / 011 | Total loss: 2.112 | Reg loss: 0.034 | Tree loss: 2.112 | Accuracy: 0.310000 | 1.033 sec/iter\n",
      "Epoch: 61 | Batch: 002 / 011 | Total loss: 2.050 | Reg loss: 0.034 | Tree loss: 2.050 | Accuracy: 0.342500 | 1.032 sec/iter\n",
      "Epoch: 61 | Batch: 003 / 011 | Total loss: 1.990 | Reg loss: 0.034 | Tree loss: 1.990 | Accuracy: 0.353000 | 1.032 sec/iter\n",
      "Epoch: 61 | Batch: 004 / 011 | Total loss: 1.998 | Reg loss: 0.034 | Tree loss: 1.998 | Accuracy: 0.341500 | 1.032 sec/iter\n",
      "Epoch: 61 | Batch: 005 / 011 | Total loss: 1.918 | Reg loss: 0.034 | Tree loss: 1.918 | Accuracy: 0.378500 | 1.032 sec/iter\n",
      "Epoch: 61 | Batch: 006 / 011 | Total loss: 1.946 | Reg loss: 0.034 | Tree loss: 1.946 | Accuracy: 0.384000 | 1.032 sec/iter\n",
      "Epoch: 61 | Batch: 007 / 011 | Total loss: 1.918 | Reg loss: 0.035 | Tree loss: 1.918 | Accuracy: 0.362000 | 1.031 sec/iter\n",
      "Epoch: 61 | Batch: 008 / 011 | Total loss: 1.908 | Reg loss: 0.035 | Tree loss: 1.908 | Accuracy: 0.380500 | 1.031 sec/iter\n",
      "Epoch: 61 | Batch: 009 / 011 | Total loss: 1.914 | Reg loss: 0.035 | Tree loss: 1.914 | Accuracy: 0.367000 | 1.031 sec/iter\n",
      "Epoch: 61 | Batch: 010 / 011 | Total loss: 1.858 | Reg loss: 0.035 | Tree loss: 1.858 | Accuracy: 0.399317 | 1.031 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 62 | Batch: 000 / 011 | Total loss: 2.150 | Reg loss: 0.034 | Tree loss: 2.150 | Accuracy: 0.314000 | 1.031 sec/iter\n",
      "Epoch: 62 | Batch: 001 / 011 | Total loss: 2.088 | Reg loss: 0.034 | Tree loss: 2.088 | Accuracy: 0.343000 | 1.031 sec/iter\n",
      "Epoch: 62 | Batch: 002 / 011 | Total loss: 2.069 | Reg loss: 0.034 | Tree loss: 2.069 | Accuracy: 0.332000 | 1.03 sec/iter\n",
      "Epoch: 62 | Batch: 003 / 011 | Total loss: 2.022 | Reg loss: 0.035 | Tree loss: 2.022 | Accuracy: 0.346500 | 1.03 sec/iter\n",
      "Epoch: 62 | Batch: 004 / 011 | Total loss: 1.955 | Reg loss: 0.035 | Tree loss: 1.955 | Accuracy: 0.359000 | 1.03 sec/iter\n",
      "Epoch: 62 | Batch: 005 / 011 | Total loss: 1.965 | Reg loss: 0.035 | Tree loss: 1.965 | Accuracy: 0.358500 | 1.03 sec/iter\n",
      "Epoch: 62 | Batch: 006 / 011 | Total loss: 1.928 | Reg loss: 0.035 | Tree loss: 1.928 | Accuracy: 0.377000 | 1.03 sec/iter\n",
      "Epoch: 62 | Batch: 007 / 011 | Total loss: 1.883 | Reg loss: 0.035 | Tree loss: 1.883 | Accuracy: 0.400500 | 1.029 sec/iter\n",
      "Epoch: 62 | Batch: 008 / 011 | Total loss: 1.879 | Reg loss: 0.035 | Tree loss: 1.879 | Accuracy: 0.422000 | 1.029 sec/iter\n",
      "Epoch: 62 | Batch: 009 / 011 | Total loss: 1.871 | Reg loss: 0.035 | Tree loss: 1.871 | Accuracy: 0.385500 | 1.029 sec/iter\n",
      "Epoch: 62 | Batch: 010 / 011 | Total loss: 1.915 | Reg loss: 0.035 | Tree loss: 1.915 | Accuracy: 0.382253 | 1.029 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 63 | Batch: 000 / 011 | Total loss: 2.136 | Reg loss: 0.035 | Tree loss: 2.136 | Accuracy: 0.328500 | 1.029 sec/iter\n",
      "Epoch: 63 | Batch: 001 / 011 | Total loss: 2.093 | Reg loss: 0.035 | Tree loss: 2.093 | Accuracy: 0.328000 | 1.029 sec/iter\n",
      "Epoch: 63 | Batch: 002 / 011 | Total loss: 2.041 | Reg loss: 0.035 | Tree loss: 2.041 | Accuracy: 0.347000 | 1.028 sec/iter\n",
      "Epoch: 63 | Batch: 003 / 011 | Total loss: 1.995 | Reg loss: 0.035 | Tree loss: 1.995 | Accuracy: 0.351500 | 1.028 sec/iter\n",
      "Epoch: 63 | Batch: 004 / 011 | Total loss: 1.978 | Reg loss: 0.035 | Tree loss: 1.978 | Accuracy: 0.353500 | 1.028 sec/iter\n",
      "Epoch: 63 | Batch: 005 / 011 | Total loss: 1.956 | Reg loss: 0.035 | Tree loss: 1.956 | Accuracy: 0.379500 | 1.028 sec/iter\n",
      "Epoch: 63 | Batch: 006 / 011 | Total loss: 1.920 | Reg loss: 0.035 | Tree loss: 1.920 | Accuracy: 0.380500 | 1.028 sec/iter\n",
      "Epoch: 63 | Batch: 007 / 011 | Total loss: 1.877 | Reg loss: 0.035 | Tree loss: 1.877 | Accuracy: 0.407500 | 1.028 sec/iter\n",
      "Epoch: 63 | Batch: 008 / 011 | Total loss: 1.878 | Reg loss: 0.035 | Tree loss: 1.878 | Accuracy: 0.386000 | 1.027 sec/iter\n",
      "Epoch: 63 | Batch: 009 / 011 | Total loss: 1.877 | Reg loss: 0.035 | Tree loss: 1.877 | Accuracy: 0.382500 | 1.027 sec/iter\n",
      "Epoch: 63 | Batch: 010 / 011 | Total loss: 1.857 | Reg loss: 0.035 | Tree loss: 1.857 | Accuracy: 0.382253 | 1.027 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 64 | Batch: 000 / 011 | Total loss: 2.131 | Reg loss: 0.035 | Tree loss: 2.131 | Accuracy: 0.328000 | 1.027 sec/iter\n",
      "Epoch: 64 | Batch: 001 / 011 | Total loss: 2.089 | Reg loss: 0.035 | Tree loss: 2.089 | Accuracy: 0.316000 | 1.027 sec/iter\n",
      "Epoch: 64 | Batch: 002 / 011 | Total loss: 2.044 | Reg loss: 0.035 | Tree loss: 2.044 | Accuracy: 0.353500 | 1.027 sec/iter\n",
      "Epoch: 64 | Batch: 003 / 011 | Total loss: 2.019 | Reg loss: 0.035 | Tree loss: 2.019 | Accuracy: 0.351000 | 1.026 sec/iter\n",
      "Epoch: 64 | Batch: 004 / 011 | Total loss: 1.956 | Reg loss: 0.035 | Tree loss: 1.956 | Accuracy: 0.370500 | 1.026 sec/iter\n",
      "Epoch: 64 | Batch: 005 / 011 | Total loss: 1.932 | Reg loss: 0.035 | Tree loss: 1.932 | Accuracy: 0.360000 | 1.026 sec/iter\n",
      "Epoch: 64 | Batch: 006 / 011 | Total loss: 1.894 | Reg loss: 0.035 | Tree loss: 1.894 | Accuracy: 0.390500 | 1.026 sec/iter\n",
      "Epoch: 64 | Batch: 007 / 011 | Total loss: 1.867 | Reg loss: 0.035 | Tree loss: 1.867 | Accuracy: 0.412500 | 1.026 sec/iter\n",
      "Epoch: 64 | Batch: 008 / 011 | Total loss: 1.888 | Reg loss: 0.035 | Tree loss: 1.888 | Accuracy: 0.398000 | 1.026 sec/iter\n",
      "Epoch: 64 | Batch: 009 / 011 | Total loss: 1.880 | Reg loss: 0.035 | Tree loss: 1.880 | Accuracy: 0.377000 | 1.025 sec/iter\n",
      "Epoch: 64 | Batch: 010 / 011 | Total loss: 1.941 | Reg loss: 0.035 | Tree loss: 1.941 | Accuracy: 0.361775 | 1.025 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 65 | Batch: 000 / 011 | Total loss: 2.112 | Reg loss: 0.035 | Tree loss: 2.112 | Accuracy: 0.338000 | 1.025 sec/iter\n",
      "Epoch: 65 | Batch: 001 / 011 | Total loss: 2.066 | Reg loss: 0.035 | Tree loss: 2.066 | Accuracy: 0.327500 | 1.025 sec/iter\n",
      "Epoch: 65 | Batch: 002 / 011 | Total loss: 2.033 | Reg loss: 0.035 | Tree loss: 2.033 | Accuracy: 0.351000 | 1.025 sec/iter\n",
      "Epoch: 65 | Batch: 003 / 011 | Total loss: 1.995 | Reg loss: 0.035 | Tree loss: 1.995 | Accuracy: 0.352500 | 1.025 sec/iter\n",
      "Epoch: 65 | Batch: 004 / 011 | Total loss: 1.961 | Reg loss: 0.035 | Tree loss: 1.961 | Accuracy: 0.358500 | 1.024 sec/iter\n",
      "Epoch: 65 | Batch: 005 / 011 | Total loss: 1.945 | Reg loss: 0.035 | Tree loss: 1.945 | Accuracy: 0.366500 | 1.024 sec/iter\n",
      "Epoch: 65 | Batch: 006 / 011 | Total loss: 1.911 | Reg loss: 0.035 | Tree loss: 1.911 | Accuracy: 0.399500 | 1.024 sec/iter\n",
      "Epoch: 65 | Batch: 007 / 011 | Total loss: 1.888 | Reg loss: 0.035 | Tree loss: 1.888 | Accuracy: 0.386000 | 1.024 sec/iter\n",
      "Epoch: 65 | Batch: 008 / 011 | Total loss: 1.884 | Reg loss: 0.035 | Tree loss: 1.884 | Accuracy: 0.396500 | 1.024 sec/iter\n",
      "Epoch: 65 | Batch: 009 / 011 | Total loss: 1.874 | Reg loss: 0.035 | Tree loss: 1.874 | Accuracy: 0.389000 | 1.024 sec/iter\n",
      "Epoch: 65 | Batch: 010 / 011 | Total loss: 1.871 | Reg loss: 0.035 | Tree loss: 1.871 | Accuracy: 0.416382 | 1.023 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 66 | Batch: 000 / 011 | Total loss: 2.112 | Reg loss: 0.035 | Tree loss: 2.112 | Accuracy: 0.329000 | 1.023 sec/iter\n",
      "Epoch: 66 | Batch: 001 / 011 | Total loss: 2.064 | Reg loss: 0.035 | Tree loss: 2.064 | Accuracy: 0.341500 | 1.023 sec/iter\n",
      "Epoch: 66 | Batch: 002 / 011 | Total loss: 2.020 | Reg loss: 0.035 | Tree loss: 2.020 | Accuracy: 0.349000 | 1.023 sec/iter\n",
      "Epoch: 66 | Batch: 003 / 011 | Total loss: 1.984 | Reg loss: 0.035 | Tree loss: 1.984 | Accuracy: 0.373000 | 1.023 sec/iter\n",
      "Epoch: 66 | Batch: 004 / 011 | Total loss: 1.962 | Reg loss: 0.035 | Tree loss: 1.962 | Accuracy: 0.352500 | 1.023 sec/iter\n",
      "Epoch: 66 | Batch: 005 / 011 | Total loss: 1.916 | Reg loss: 0.035 | Tree loss: 1.916 | Accuracy: 0.384500 | 1.023 sec/iter\n",
      "Epoch: 66 | Batch: 006 / 011 | Total loss: 1.926 | Reg loss: 0.035 | Tree loss: 1.926 | Accuracy: 0.396000 | 1.023 sec/iter\n",
      "Epoch: 66 | Batch: 007 / 011 | Total loss: 1.881 | Reg loss: 0.035 | Tree loss: 1.881 | Accuracy: 0.412500 | 1.022 sec/iter\n",
      "Epoch: 66 | Batch: 008 / 011 | Total loss: 1.873 | Reg loss: 0.035 | Tree loss: 1.873 | Accuracy: 0.386500 | 1.022 sec/iter\n",
      "Epoch: 66 | Batch: 009 / 011 | Total loss: 1.892 | Reg loss: 0.035 | Tree loss: 1.892 | Accuracy: 0.381500 | 1.022 sec/iter\n",
      "Epoch: 66 | Batch: 010 / 011 | Total loss: 1.855 | Reg loss: 0.035 | Tree loss: 1.855 | Accuracy: 0.406143 | 1.022 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 67 | Batch: 000 / 011 | Total loss: 2.115 | Reg loss: 0.035 | Tree loss: 2.115 | Accuracy: 0.329500 | 1.022 sec/iter\n",
      "Epoch: 67 | Batch: 001 / 011 | Total loss: 2.074 | Reg loss: 0.035 | Tree loss: 2.074 | Accuracy: 0.327500 | 1.022 sec/iter\n",
      "Epoch: 67 | Batch: 002 / 011 | Total loss: 1.996 | Reg loss: 0.035 | Tree loss: 1.996 | Accuracy: 0.356000 | 1.021 sec/iter\n",
      "Epoch: 67 | Batch: 003 / 011 | Total loss: 1.988 | Reg loss: 0.035 | Tree loss: 1.988 | Accuracy: 0.343500 | 1.021 sec/iter\n",
      "Epoch: 67 | Batch: 004 / 011 | Total loss: 1.964 | Reg loss: 0.035 | Tree loss: 1.964 | Accuracy: 0.344000 | 1.021 sec/iter\n",
      "Epoch: 67 | Batch: 005 / 011 | Total loss: 1.921 | Reg loss: 0.035 | Tree loss: 1.921 | Accuracy: 0.382000 | 1.021 sec/iter\n",
      "Epoch: 67 | Batch: 006 / 011 | Total loss: 1.914 | Reg loss: 0.035 | Tree loss: 1.914 | Accuracy: 0.370500 | 1.021 sec/iter\n",
      "Epoch: 67 | Batch: 007 / 011 | Total loss: 1.881 | Reg loss: 0.035 | Tree loss: 1.881 | Accuracy: 0.396500 | 1.021 sec/iter\n",
      "Epoch: 67 | Batch: 008 / 011 | Total loss: 1.865 | Reg loss: 0.035 | Tree loss: 1.865 | Accuracy: 0.396500 | 1.021 sec/iter\n",
      "Epoch: 67 | Batch: 009 / 011 | Total loss: 1.835 | Reg loss: 0.035 | Tree loss: 1.835 | Accuracy: 0.395500 | 1.02 sec/iter\n",
      "Epoch: 67 | Batch: 010 / 011 | Total loss: 1.938 | Reg loss: 0.035 | Tree loss: 1.938 | Accuracy: 0.385666 | 1.02 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 68 | Batch: 000 / 011 | Total loss: 2.087 | Reg loss: 0.035 | Tree loss: 2.087 | Accuracy: 0.353000 | 1.02 sec/iter\n",
      "Epoch: 68 | Batch: 001 / 011 | Total loss: 2.053 | Reg loss: 0.035 | Tree loss: 2.053 | Accuracy: 0.343000 | 1.02 sec/iter\n",
      "Epoch: 68 | Batch: 002 / 011 | Total loss: 2.020 | Reg loss: 0.035 | Tree loss: 2.020 | Accuracy: 0.344000 | 1.02 sec/iter\n",
      "Epoch: 68 | Batch: 003 / 011 | Total loss: 1.975 | Reg loss: 0.035 | Tree loss: 1.975 | Accuracy: 0.369500 | 1.02 sec/iter\n",
      "Epoch: 68 | Batch: 004 / 011 | Total loss: 1.956 | Reg loss: 0.035 | Tree loss: 1.956 | Accuracy: 0.359000 | 1.02 sec/iter\n",
      "Epoch: 68 | Batch: 005 / 011 | Total loss: 1.910 | Reg loss: 0.035 | Tree loss: 1.910 | Accuracy: 0.389500 | 1.019 sec/iter\n",
      "Epoch: 68 | Batch: 006 / 011 | Total loss: 1.904 | Reg loss: 0.035 | Tree loss: 1.904 | Accuracy: 0.372000 | 1.019 sec/iter\n",
      "Epoch: 68 | Batch: 007 / 011 | Total loss: 1.886 | Reg loss: 0.035 | Tree loss: 1.886 | Accuracy: 0.387000 | 1.019 sec/iter\n",
      "Epoch: 68 | Batch: 008 / 011 | Total loss: 1.867 | Reg loss: 0.035 | Tree loss: 1.867 | Accuracy: 0.392500 | 1.019 sec/iter\n",
      "Epoch: 68 | Batch: 009 / 011 | Total loss: 1.861 | Reg loss: 0.036 | Tree loss: 1.861 | Accuracy: 0.400500 | 1.019 sec/iter\n",
      "Epoch: 68 | Batch: 010 / 011 | Total loss: 1.821 | Reg loss: 0.036 | Tree loss: 1.821 | Accuracy: 0.409556 | 1.019 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 69 | Batch: 000 / 011 | Total loss: 2.103 | Reg loss: 0.035 | Tree loss: 2.103 | Accuracy: 0.347500 | 1.019 sec/iter\n",
      "Epoch: 69 | Batch: 001 / 011 | Total loss: 2.052 | Reg loss: 0.035 | Tree loss: 2.052 | Accuracy: 0.357500 | 1.019 sec/iter\n",
      "Epoch: 69 | Batch: 002 / 011 | Total loss: 1.999 | Reg loss: 0.035 | Tree loss: 1.999 | Accuracy: 0.363000 | 1.018 sec/iter\n",
      "Epoch: 69 | Batch: 003 / 011 | Total loss: 1.993 | Reg loss: 0.035 | Tree loss: 1.993 | Accuracy: 0.335000 | 1.018 sec/iter\n",
      "Epoch: 69 | Batch: 004 / 011 | Total loss: 1.956 | Reg loss: 0.035 | Tree loss: 1.956 | Accuracy: 0.362000 | 1.018 sec/iter\n",
      "Epoch: 69 | Batch: 005 / 011 | Total loss: 1.906 | Reg loss: 0.036 | Tree loss: 1.906 | Accuracy: 0.382500 | 1.018 sec/iter\n",
      "Epoch: 69 | Batch: 006 / 011 | Total loss: 1.876 | Reg loss: 0.036 | Tree loss: 1.876 | Accuracy: 0.394000 | 1.018 sec/iter\n",
      "Epoch: 69 | Batch: 007 / 011 | Total loss: 1.884 | Reg loss: 0.036 | Tree loss: 1.884 | Accuracy: 0.397000 | 1.018 sec/iter\n",
      "Epoch: 69 | Batch: 008 / 011 | Total loss: 1.885 | Reg loss: 0.036 | Tree loss: 1.885 | Accuracy: 0.357500 | 1.018 sec/iter\n",
      "Epoch: 69 | Batch: 009 / 011 | Total loss: 1.846 | Reg loss: 0.036 | Tree loss: 1.846 | Accuracy: 0.394500 | 1.018 sec/iter\n",
      "Epoch: 69 | Batch: 010 / 011 | Total loss: 1.827 | Reg loss: 0.036 | Tree loss: 1.827 | Accuracy: 0.385666 | 1.017 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 70 | Batch: 000 / 011 | Total loss: 2.100 | Reg loss: 0.036 | Tree loss: 2.100 | Accuracy: 0.332500 | 1.017 sec/iter\n",
      "Epoch: 70 | Batch: 001 / 011 | Total loss: 2.051 | Reg loss: 0.036 | Tree loss: 2.051 | Accuracy: 0.347000 | 1.017 sec/iter\n",
      "Epoch: 70 | Batch: 002 / 011 | Total loss: 2.005 | Reg loss: 0.036 | Tree loss: 2.005 | Accuracy: 0.363000 | 1.017 sec/iter\n",
      "Epoch: 70 | Batch: 003 / 011 | Total loss: 1.958 | Reg loss: 0.036 | Tree loss: 1.958 | Accuracy: 0.363000 | 1.017 sec/iter\n",
      "Epoch: 70 | Batch: 004 / 011 | Total loss: 1.942 | Reg loss: 0.036 | Tree loss: 1.942 | Accuracy: 0.369000 | 1.017 sec/iter\n",
      "Epoch: 70 | Batch: 005 / 011 | Total loss: 1.906 | Reg loss: 0.036 | Tree loss: 1.906 | Accuracy: 0.389000 | 1.017 sec/iter\n",
      "Epoch: 70 | Batch: 006 / 011 | Total loss: 1.883 | Reg loss: 0.036 | Tree loss: 1.883 | Accuracy: 0.386000 | 1.016 sec/iter\n",
      "Epoch: 70 | Batch: 007 / 011 | Total loss: 1.861 | Reg loss: 0.036 | Tree loss: 1.861 | Accuracy: 0.399000 | 1.016 sec/iter\n",
      "Epoch: 70 | Batch: 008 / 011 | Total loss: 1.868 | Reg loss: 0.036 | Tree loss: 1.868 | Accuracy: 0.384000 | 1.016 sec/iter\n",
      "Epoch: 70 | Batch: 009 / 011 | Total loss: 1.837 | Reg loss: 0.036 | Tree loss: 1.837 | Accuracy: 0.404000 | 1.016 sec/iter\n",
      "Epoch: 70 | Batch: 010 / 011 | Total loss: 1.902 | Reg loss: 0.036 | Tree loss: 1.902 | Accuracy: 0.399317 | 1.016 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 71 | Batch: 000 / 011 | Total loss: 2.092 | Reg loss: 0.036 | Tree loss: 2.092 | Accuracy: 0.343000 | 1.016 sec/iter\n",
      "Epoch: 71 | Batch: 001 / 011 | Total loss: 2.060 | Reg loss: 0.036 | Tree loss: 2.060 | Accuracy: 0.339000 | 1.016 sec/iter\n",
      "Epoch: 71 | Batch: 002 / 011 | Total loss: 1.999 | Reg loss: 0.036 | Tree loss: 1.999 | Accuracy: 0.356000 | 1.016 sec/iter\n",
      "Epoch: 71 | Batch: 003 / 011 | Total loss: 1.980 | Reg loss: 0.036 | Tree loss: 1.980 | Accuracy: 0.354500 | 1.015 sec/iter\n",
      "Epoch: 71 | Batch: 004 / 011 | Total loss: 1.919 | Reg loss: 0.036 | Tree loss: 1.919 | Accuracy: 0.367500 | 1.015 sec/iter\n",
      "Epoch: 71 | Batch: 005 / 011 | Total loss: 1.894 | Reg loss: 0.036 | Tree loss: 1.894 | Accuracy: 0.403000 | 1.015 sec/iter\n",
      "Epoch: 71 | Batch: 006 / 011 | Total loss: 1.885 | Reg loss: 0.036 | Tree loss: 1.885 | Accuracy: 0.393000 | 1.015 sec/iter\n",
      "Epoch: 71 | Batch: 007 / 011 | Total loss: 1.879 | Reg loss: 0.036 | Tree loss: 1.879 | Accuracy: 0.391500 | 1.015 sec/iter\n",
      "Epoch: 71 | Batch: 008 / 011 | Total loss: 1.850 | Reg loss: 0.036 | Tree loss: 1.850 | Accuracy: 0.391500 | 1.015 sec/iter\n",
      "Epoch: 71 | Batch: 009 / 011 | Total loss: 1.844 | Reg loss: 0.036 | Tree loss: 1.844 | Accuracy: 0.401000 | 1.015 sec/iter\n",
      "Epoch: 71 | Batch: 010 / 011 | Total loss: 1.808 | Reg loss: 0.036 | Tree loss: 1.808 | Accuracy: 0.412969 | 1.014 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 72 | Batch: 000 / 011 | Total loss: 2.110 | Reg loss: 0.036 | Tree loss: 2.110 | Accuracy: 0.320000 | 1.014 sec/iter\n",
      "Epoch: 72 | Batch: 001 / 011 | Total loss: 2.050 | Reg loss: 0.036 | Tree loss: 2.050 | Accuracy: 0.339500 | 1.014 sec/iter\n",
      "Epoch: 72 | Batch: 002 / 011 | Total loss: 1.986 | Reg loss: 0.036 | Tree loss: 1.986 | Accuracy: 0.361500 | 1.014 sec/iter\n",
      "Epoch: 72 | Batch: 003 / 011 | Total loss: 1.961 | Reg loss: 0.036 | Tree loss: 1.961 | Accuracy: 0.358500 | 1.014 sec/iter\n",
      "Epoch: 72 | Batch: 004 / 011 | Total loss: 1.913 | Reg loss: 0.036 | Tree loss: 1.913 | Accuracy: 0.365000 | 1.014 sec/iter\n",
      "Epoch: 72 | Batch: 005 / 011 | Total loss: 1.903 | Reg loss: 0.036 | Tree loss: 1.903 | Accuracy: 0.382500 | 1.014 sec/iter\n",
      "Epoch: 72 | Batch: 006 / 011 | Total loss: 1.859 | Reg loss: 0.036 | Tree loss: 1.859 | Accuracy: 0.405000 | 1.014 sec/iter\n",
      "Epoch: 72 | Batch: 007 / 011 | Total loss: 1.874 | Reg loss: 0.036 | Tree loss: 1.874 | Accuracy: 0.374000 | 1.013 sec/iter\n",
      "Epoch: 72 | Batch: 008 / 011 | Total loss: 1.854 | Reg loss: 0.036 | Tree loss: 1.854 | Accuracy: 0.400000 | 1.013 sec/iter\n",
      "Epoch: 72 | Batch: 009 / 011 | Total loss: 1.826 | Reg loss: 0.036 | Tree loss: 1.826 | Accuracy: 0.412000 | 1.013 sec/iter\n",
      "Epoch: 72 | Batch: 010 / 011 | Total loss: 1.846 | Reg loss: 0.036 | Tree loss: 1.846 | Accuracy: 0.412969 | 1.013 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 73 | Batch: 000 / 011 | Total loss: 2.068 | Reg loss: 0.036 | Tree loss: 2.068 | Accuracy: 0.335500 | 1.013 sec/iter\n",
      "Epoch: 73 | Batch: 001 / 011 | Total loss: 2.051 | Reg loss: 0.036 | Tree loss: 2.051 | Accuracy: 0.338000 | 1.013 sec/iter\n",
      "Epoch: 73 | Batch: 002 / 011 | Total loss: 1.976 | Reg loss: 0.036 | Tree loss: 1.976 | Accuracy: 0.370500 | 1.013 sec/iter\n",
      "Epoch: 73 | Batch: 003 / 011 | Total loss: 1.971 | Reg loss: 0.036 | Tree loss: 1.971 | Accuracy: 0.354500 | 1.013 sec/iter\n",
      "Epoch: 73 | Batch: 004 / 011 | Total loss: 1.922 | Reg loss: 0.036 | Tree loss: 1.922 | Accuracy: 0.371000 | 1.013 sec/iter\n",
      "Epoch: 73 | Batch: 005 / 011 | Total loss: 1.909 | Reg loss: 0.036 | Tree loss: 1.909 | Accuracy: 0.380500 | 1.012 sec/iter\n",
      "Epoch: 73 | Batch: 006 / 011 | Total loss: 1.858 | Reg loss: 0.036 | Tree loss: 1.858 | Accuracy: 0.421000 | 1.012 sec/iter\n",
      "Epoch: 73 | Batch: 007 / 011 | Total loss: 1.873 | Reg loss: 0.036 | Tree loss: 1.873 | Accuracy: 0.380000 | 1.012 sec/iter\n",
      "Epoch: 73 | Batch: 008 / 011 | Total loss: 1.843 | Reg loss: 0.036 | Tree loss: 1.843 | Accuracy: 0.396000 | 1.012 sec/iter\n",
      "Epoch: 73 | Batch: 009 / 011 | Total loss: 1.868 | Reg loss: 0.036 | Tree loss: 1.868 | Accuracy: 0.402000 | 1.012 sec/iter\n",
      "Epoch: 73 | Batch: 010 / 011 | Total loss: 1.798 | Reg loss: 0.036 | Tree loss: 1.798 | Accuracy: 0.399317 | 1.012 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 74 | Batch: 000 / 011 | Total loss: 2.060 | Reg loss: 0.036 | Tree loss: 2.060 | Accuracy: 0.330500 | 1.012 sec/iter\n",
      "Epoch: 74 | Batch: 001 / 011 | Total loss: 2.047 | Reg loss: 0.036 | Tree loss: 2.047 | Accuracy: 0.326000 | 1.012 sec/iter\n",
      "Epoch: 74 | Batch: 002 / 011 | Total loss: 1.987 | Reg loss: 0.036 | Tree loss: 1.987 | Accuracy: 0.352500 | 1.011 sec/iter\n",
      "Epoch: 74 | Batch: 003 / 011 | Total loss: 1.943 | Reg loss: 0.036 | Tree loss: 1.943 | Accuracy: 0.372500 | 1.011 sec/iter\n",
      "Epoch: 74 | Batch: 004 / 011 | Total loss: 1.937 | Reg loss: 0.036 | Tree loss: 1.937 | Accuracy: 0.365500 | 1.011 sec/iter\n",
      "Epoch: 74 | Batch: 005 / 011 | Total loss: 1.909 | Reg loss: 0.036 | Tree loss: 1.909 | Accuracy: 0.368500 | 1.011 sec/iter\n",
      "Epoch: 74 | Batch: 006 / 011 | Total loss: 1.873 | Reg loss: 0.036 | Tree loss: 1.873 | Accuracy: 0.399000 | 1.011 sec/iter\n",
      "Epoch: 74 | Batch: 007 / 011 | Total loss: 1.847 | Reg loss: 0.036 | Tree loss: 1.847 | Accuracy: 0.400500 | 1.011 sec/iter\n",
      "Epoch: 74 | Batch: 008 / 011 | Total loss: 1.837 | Reg loss: 0.036 | Tree loss: 1.837 | Accuracy: 0.393000 | 1.011 sec/iter\n",
      "Epoch: 74 | Batch: 009 / 011 | Total loss: 1.850 | Reg loss: 0.036 | Tree loss: 1.850 | Accuracy: 0.390000 | 1.011 sec/iter\n",
      "Epoch: 74 | Batch: 010 / 011 | Total loss: 1.794 | Reg loss: 0.036 | Tree loss: 1.794 | Accuracy: 0.416382 | 1.01 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 75 | Batch: 000 / 011 | Total loss: 2.091 | Reg loss: 0.036 | Tree loss: 2.091 | Accuracy: 0.324000 | 1.01 sec/iter\n",
      "Epoch: 75 | Batch: 001 / 011 | Total loss: 2.050 | Reg loss: 0.036 | Tree loss: 2.050 | Accuracy: 0.346000 | 1.01 sec/iter\n",
      "Epoch: 75 | Batch: 002 / 011 | Total loss: 1.965 | Reg loss: 0.036 | Tree loss: 1.965 | Accuracy: 0.366000 | 1.01 sec/iter\n",
      "Epoch: 75 | Batch: 003 / 011 | Total loss: 1.953 | Reg loss: 0.036 | Tree loss: 1.953 | Accuracy: 0.337000 | 1.01 sec/iter\n",
      "Epoch: 75 | Batch: 004 / 011 | Total loss: 1.888 | Reg loss: 0.036 | Tree loss: 1.888 | Accuracy: 0.398500 | 1.01 sec/iter\n",
      "Epoch: 75 | Batch: 005 / 011 | Total loss: 1.894 | Reg loss: 0.036 | Tree loss: 1.894 | Accuracy: 0.389000 | 1.01 sec/iter\n",
      "Epoch: 75 | Batch: 006 / 011 | Total loss: 1.887 | Reg loss: 0.036 | Tree loss: 1.887 | Accuracy: 0.382500 | 1.01 sec/iter\n",
      "Epoch: 75 | Batch: 007 / 011 | Total loss: 1.837 | Reg loss: 0.036 | Tree loss: 1.837 | Accuracy: 0.408500 | 1.01 sec/iter\n",
      "Epoch: 75 | Batch: 008 / 011 | Total loss: 1.844 | Reg loss: 0.036 | Tree loss: 1.844 | Accuracy: 0.400500 | 1.009 sec/iter\n",
      "Epoch: 75 | Batch: 009 / 011 | Total loss: 1.831 | Reg loss: 0.036 | Tree loss: 1.831 | Accuracy: 0.402000 | 1.009 sec/iter\n",
      "Epoch: 75 | Batch: 010 / 011 | Total loss: 1.835 | Reg loss: 0.036 | Tree loss: 1.835 | Accuracy: 0.416382 | 1.009 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 76 | Batch: 000 / 011 | Total loss: 2.082 | Reg loss: 0.036 | Tree loss: 2.082 | Accuracy: 0.324000 | 1.009 sec/iter\n",
      "Epoch: 76 | Batch: 001 / 011 | Total loss: 2.013 | Reg loss: 0.036 | Tree loss: 2.013 | Accuracy: 0.362000 | 1.009 sec/iter\n",
      "Epoch: 76 | Batch: 002 / 011 | Total loss: 1.975 | Reg loss: 0.036 | Tree loss: 1.975 | Accuracy: 0.358000 | 1.009 sec/iter\n",
      "Epoch: 76 | Batch: 003 / 011 | Total loss: 1.943 | Reg loss: 0.036 | Tree loss: 1.943 | Accuracy: 0.344500 | 1.009 sec/iter\n",
      "Epoch: 76 | Batch: 004 / 011 | Total loss: 1.938 | Reg loss: 0.036 | Tree loss: 1.938 | Accuracy: 0.370500 | 1.009 sec/iter\n",
      "Epoch: 76 | Batch: 005 / 011 | Total loss: 1.853 | Reg loss: 0.036 | Tree loss: 1.853 | Accuracy: 0.406500 | 1.009 sec/iter\n",
      "Epoch: 76 | Batch: 006 / 011 | Total loss: 1.858 | Reg loss: 0.036 | Tree loss: 1.858 | Accuracy: 0.406000 | 1.008 sec/iter\n",
      "Epoch: 76 | Batch: 007 / 011 | Total loss: 1.864 | Reg loss: 0.036 | Tree loss: 1.864 | Accuracy: 0.380000 | 1.008 sec/iter\n",
      "Epoch: 76 | Batch: 008 / 011 | Total loss: 1.823 | Reg loss: 0.036 | Tree loss: 1.823 | Accuracy: 0.397500 | 1.008 sec/iter\n",
      "Epoch: 76 | Batch: 009 / 011 | Total loss: 1.860 | Reg loss: 0.036 | Tree loss: 1.860 | Accuracy: 0.385000 | 1.008 sec/iter\n",
      "Epoch: 76 | Batch: 010 / 011 | Total loss: 1.827 | Reg loss: 0.037 | Tree loss: 1.827 | Accuracy: 0.402730 | 1.008 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 77 | Batch: 000 / 011 | Total loss: 2.067 | Reg loss: 0.036 | Tree loss: 2.067 | Accuracy: 0.340000 | 1.008 sec/iter\n",
      "Epoch: 77 | Batch: 001 / 011 | Total loss: 2.009 | Reg loss: 0.036 | Tree loss: 2.009 | Accuracy: 0.352500 | 1.008 sec/iter\n",
      "Epoch: 77 | Batch: 002 / 011 | Total loss: 1.988 | Reg loss: 0.036 | Tree loss: 1.988 | Accuracy: 0.347500 | 1.008 sec/iter\n",
      "Epoch: 77 | Batch: 003 / 011 | Total loss: 1.937 | Reg loss: 0.036 | Tree loss: 1.937 | Accuracy: 0.351000 | 1.007 sec/iter\n",
      "Epoch: 77 | Batch: 004 / 011 | Total loss: 1.911 | Reg loss: 0.036 | Tree loss: 1.911 | Accuracy: 0.378000 | 1.007 sec/iter\n",
      "Epoch: 77 | Batch: 005 / 011 | Total loss: 1.876 | Reg loss: 0.036 | Tree loss: 1.876 | Accuracy: 0.403500 | 1.007 sec/iter\n",
      "Epoch: 77 | Batch: 006 / 011 | Total loss: 1.855 | Reg loss: 0.037 | Tree loss: 1.855 | Accuracy: 0.416000 | 1.007 sec/iter\n",
      "Epoch: 77 | Batch: 007 / 011 | Total loss: 1.855 | Reg loss: 0.037 | Tree loss: 1.855 | Accuracy: 0.401000 | 1.007 sec/iter\n",
      "Epoch: 77 | Batch: 008 / 011 | Total loss: 1.850 | Reg loss: 0.037 | Tree loss: 1.850 | Accuracy: 0.399000 | 1.007 sec/iter\n",
      "Epoch: 77 | Batch: 009 / 011 | Total loss: 1.837 | Reg loss: 0.037 | Tree loss: 1.837 | Accuracy: 0.398500 | 1.007 sec/iter\n",
      "Epoch: 77 | Batch: 010 / 011 | Total loss: 1.857 | Reg loss: 0.037 | Tree loss: 1.857 | Accuracy: 0.365188 | 1.007 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 78 | Batch: 000 / 011 | Total loss: 2.043 | Reg loss: 0.036 | Tree loss: 2.043 | Accuracy: 0.337500 | 1.007 sec/iter\n",
      "Epoch: 78 | Batch: 001 / 011 | Total loss: 2.026 | Reg loss: 0.036 | Tree loss: 2.026 | Accuracy: 0.341000 | 1.007 sec/iter\n",
      "Epoch: 78 | Batch: 002 / 011 | Total loss: 1.982 | Reg loss: 0.037 | Tree loss: 1.982 | Accuracy: 0.355000 | 1.006 sec/iter\n",
      "Epoch: 78 | Batch: 003 / 011 | Total loss: 1.946 | Reg loss: 0.037 | Tree loss: 1.946 | Accuracy: 0.345500 | 1.006 sec/iter\n",
      "Epoch: 78 | Batch: 004 / 011 | Total loss: 1.909 | Reg loss: 0.037 | Tree loss: 1.909 | Accuracy: 0.377500 | 1.006 sec/iter\n",
      "Epoch: 78 | Batch: 005 / 011 | Total loss: 1.868 | Reg loss: 0.037 | Tree loss: 1.868 | Accuracy: 0.399500 | 1.006 sec/iter\n",
      "Epoch: 78 | Batch: 006 / 011 | Total loss: 1.883 | Reg loss: 0.037 | Tree loss: 1.883 | Accuracy: 0.392000 | 1.006 sec/iter\n",
      "Epoch: 78 | Batch: 007 / 011 | Total loss: 1.829 | Reg loss: 0.037 | Tree loss: 1.829 | Accuracy: 0.391500 | 1.006 sec/iter\n",
      "Epoch: 78 | Batch: 008 / 011 | Total loss: 1.841 | Reg loss: 0.037 | Tree loss: 1.841 | Accuracy: 0.399500 | 1.006 sec/iter\n",
      "Epoch: 78 | Batch: 009 / 011 | Total loss: 1.819 | Reg loss: 0.037 | Tree loss: 1.819 | Accuracy: 0.415500 | 1.006 sec/iter\n",
      "Epoch: 78 | Batch: 010 / 011 | Total loss: 1.833 | Reg loss: 0.037 | Tree loss: 1.833 | Accuracy: 0.447099 | 1.006 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 79 | Batch: 000 / 011 | Total loss: 2.061 | Reg loss: 0.037 | Tree loss: 2.061 | Accuracy: 0.362500 | 1.006 sec/iter\n",
      "Epoch: 79 | Batch: 001 / 011 | Total loss: 2.018 | Reg loss: 0.037 | Tree loss: 2.018 | Accuracy: 0.348500 | 1.005 sec/iter\n",
      "Epoch: 79 | Batch: 002 / 011 | Total loss: 1.984 | Reg loss: 0.037 | Tree loss: 1.984 | Accuracy: 0.350000 | 1.005 sec/iter\n",
      "Epoch: 79 | Batch: 003 / 011 | Total loss: 1.936 | Reg loss: 0.037 | Tree loss: 1.936 | Accuracy: 0.362500 | 1.005 sec/iter\n",
      "Epoch: 79 | Batch: 004 / 011 | Total loss: 1.908 | Reg loss: 0.037 | Tree loss: 1.908 | Accuracy: 0.384000 | 1.005 sec/iter\n",
      "Epoch: 79 | Batch: 005 / 011 | Total loss: 1.875 | Reg loss: 0.037 | Tree loss: 1.875 | Accuracy: 0.395000 | 1.005 sec/iter\n",
      "Epoch: 79 | Batch: 006 / 011 | Total loss: 1.863 | Reg loss: 0.037 | Tree loss: 1.863 | Accuracy: 0.411500 | 1.005 sec/iter\n",
      "Epoch: 79 | Batch: 007 / 011 | Total loss: 1.839 | Reg loss: 0.037 | Tree loss: 1.839 | Accuracy: 0.410500 | 1.005 sec/iter\n",
      "Epoch: 79 | Batch: 008 / 011 | Total loss: 1.821 | Reg loss: 0.037 | Tree loss: 1.821 | Accuracy: 0.410000 | 1.005 sec/iter\n",
      "Epoch: 79 | Batch: 009 / 011 | Total loss: 1.812 | Reg loss: 0.037 | Tree loss: 1.812 | Accuracy: 0.406500 | 1.005 sec/iter\n",
      "Epoch: 79 | Batch: 010 / 011 | Total loss: 1.857 | Reg loss: 0.037 | Tree loss: 1.857 | Accuracy: 0.382253 | 1.004 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 80 | Batch: 000 / 011 | Total loss: 2.034 | Reg loss: 0.037 | Tree loss: 2.034 | Accuracy: 0.350000 | 1.004 sec/iter\n",
      "Epoch: 80 | Batch: 001 / 011 | Total loss: 1.993 | Reg loss: 0.037 | Tree loss: 1.993 | Accuracy: 0.354000 | 1.004 sec/iter\n",
      "Epoch: 80 | Batch: 002 / 011 | Total loss: 1.967 | Reg loss: 0.037 | Tree loss: 1.967 | Accuracy: 0.349000 | 1.004 sec/iter\n",
      "Epoch: 80 | Batch: 003 / 011 | Total loss: 1.947 | Reg loss: 0.037 | Tree loss: 1.947 | Accuracy: 0.344000 | 1.004 sec/iter\n",
      "Epoch: 80 | Batch: 004 / 011 | Total loss: 1.892 | Reg loss: 0.037 | Tree loss: 1.892 | Accuracy: 0.380500 | 1.004 sec/iter\n",
      "Epoch: 80 | Batch: 005 / 011 | Total loss: 1.852 | Reg loss: 0.037 | Tree loss: 1.852 | Accuracy: 0.393000 | 1.004 sec/iter\n",
      "Epoch: 80 | Batch: 006 / 011 | Total loss: 1.872 | Reg loss: 0.037 | Tree loss: 1.872 | Accuracy: 0.397000 | 1.004 sec/iter\n",
      "Epoch: 80 | Batch: 007 / 011 | Total loss: 1.853 | Reg loss: 0.037 | Tree loss: 1.853 | Accuracy: 0.405500 | 1.004 sec/iter\n",
      "Epoch: 80 | Batch: 008 / 011 | Total loss: 1.819 | Reg loss: 0.037 | Tree loss: 1.819 | Accuracy: 0.405500 | 1.004 sec/iter\n",
      "Epoch: 80 | Batch: 009 / 011 | Total loss: 1.829 | Reg loss: 0.037 | Tree loss: 1.829 | Accuracy: 0.402000 | 1.004 sec/iter\n",
      "Epoch: 80 | Batch: 010 / 011 | Total loss: 1.877 | Reg loss: 0.037 | Tree loss: 1.877 | Accuracy: 0.378840 | 1.003 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 81 | Batch: 000 / 011 | Total loss: 2.042 | Reg loss: 0.037 | Tree loss: 2.042 | Accuracy: 0.350000 | 1.003 sec/iter\n",
      "Epoch: 81 | Batch: 001 / 011 | Total loss: 2.007 | Reg loss: 0.037 | Tree loss: 2.007 | Accuracy: 0.347000 | 1.003 sec/iter\n",
      "Epoch: 81 | Batch: 002 / 011 | Total loss: 1.989 | Reg loss: 0.037 | Tree loss: 1.989 | Accuracy: 0.346500 | 1.003 sec/iter\n",
      "Epoch: 81 | Batch: 003 / 011 | Total loss: 1.933 | Reg loss: 0.037 | Tree loss: 1.933 | Accuracy: 0.368500 | 1.003 sec/iter\n",
      "Epoch: 81 | Batch: 004 / 011 | Total loss: 1.885 | Reg loss: 0.037 | Tree loss: 1.885 | Accuracy: 0.395500 | 1.003 sec/iter\n",
      "Epoch: 81 | Batch: 005 / 011 | Total loss: 1.849 | Reg loss: 0.037 | Tree loss: 1.849 | Accuracy: 0.412500 | 1.003 sec/iter\n",
      "Epoch: 81 | Batch: 006 / 011 | Total loss: 1.871 | Reg loss: 0.037 | Tree loss: 1.871 | Accuracy: 0.402000 | 1.003 sec/iter\n",
      "Epoch: 81 | Batch: 007 / 011 | Total loss: 1.835 | Reg loss: 0.037 | Tree loss: 1.835 | Accuracy: 0.398000 | 1.003 sec/iter\n",
      "Epoch: 81 | Batch: 008 / 011 | Total loss: 1.815 | Reg loss: 0.037 | Tree loss: 1.815 | Accuracy: 0.417500 | 1.002 sec/iter\n",
      "Epoch: 81 | Batch: 009 / 011 | Total loss: 1.831 | Reg loss: 0.037 | Tree loss: 1.831 | Accuracy: 0.393500 | 1.002 sec/iter\n",
      "Epoch: 81 | Batch: 010 / 011 | Total loss: 1.757 | Reg loss: 0.037 | Tree loss: 1.757 | Accuracy: 0.412969 | 1.002 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 82 | Batch: 000 / 011 | Total loss: 2.050 | Reg loss: 0.037 | Tree loss: 2.050 | Accuracy: 0.352000 | 1.002 sec/iter\n",
      "Epoch: 82 | Batch: 001 / 011 | Total loss: 2.018 | Reg loss: 0.037 | Tree loss: 2.018 | Accuracy: 0.339500 | 1.002 sec/iter\n",
      "Epoch: 82 | Batch: 002 / 011 | Total loss: 1.957 | Reg loss: 0.037 | Tree loss: 1.957 | Accuracy: 0.346500 | 1.002 sec/iter\n",
      "Epoch: 82 | Batch: 003 / 011 | Total loss: 1.922 | Reg loss: 0.037 | Tree loss: 1.922 | Accuracy: 0.356000 | 1.002 sec/iter\n",
      "Epoch: 82 | Batch: 004 / 011 | Total loss: 1.884 | Reg loss: 0.037 | Tree loss: 1.884 | Accuracy: 0.382000 | 1.002 sec/iter\n",
      "Epoch: 82 | Batch: 005 / 011 | Total loss: 1.870 | Reg loss: 0.037 | Tree loss: 1.870 | Accuracy: 0.382000 | 1.002 sec/iter\n",
      "Epoch: 82 | Batch: 006 / 011 | Total loss: 1.846 | Reg loss: 0.037 | Tree loss: 1.846 | Accuracy: 0.393500 | 1.002 sec/iter\n",
      "Epoch: 82 | Batch: 007 / 011 | Total loss: 1.806 | Reg loss: 0.037 | Tree loss: 1.806 | Accuracy: 0.428000 | 1.001 sec/iter\n",
      "Epoch: 82 | Batch: 008 / 011 | Total loss: 1.821 | Reg loss: 0.037 | Tree loss: 1.821 | Accuracy: 0.402000 | 1.001 sec/iter\n",
      "Epoch: 82 | Batch: 009 / 011 | Total loss: 1.805 | Reg loss: 0.037 | Tree loss: 1.805 | Accuracy: 0.418000 | 1.001 sec/iter\n",
      "Epoch: 82 | Batch: 010 / 011 | Total loss: 1.869 | Reg loss: 0.037 | Tree loss: 1.869 | Accuracy: 0.399317 | 1.001 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 83 | Batch: 000 / 011 | Total loss: 2.047 | Reg loss: 0.037 | Tree loss: 2.047 | Accuracy: 0.354500 | 1.001 sec/iter\n",
      "Epoch: 83 | Batch: 001 / 011 | Total loss: 2.025 | Reg loss: 0.037 | Tree loss: 2.025 | Accuracy: 0.345500 | 1.001 sec/iter\n",
      "Epoch: 83 | Batch: 002 / 011 | Total loss: 1.981 | Reg loss: 0.037 | Tree loss: 1.981 | Accuracy: 0.339500 | 1.001 sec/iter\n",
      "Epoch: 83 | Batch: 003 / 011 | Total loss: 1.899 | Reg loss: 0.037 | Tree loss: 1.899 | Accuracy: 0.360500 | 1.001 sec/iter\n",
      "Epoch: 83 | Batch: 004 / 011 | Total loss: 1.897 | Reg loss: 0.037 | Tree loss: 1.897 | Accuracy: 0.367500 | 1.001 sec/iter\n",
      "Epoch: 83 | Batch: 005 / 011 | Total loss: 1.851 | Reg loss: 0.037 | Tree loss: 1.851 | Accuracy: 0.419500 | 1.001 sec/iter\n",
      "Epoch: 83 | Batch: 006 / 011 | Total loss: 1.836 | Reg loss: 0.037 | Tree loss: 1.836 | Accuracy: 0.417000 | 1.001 sec/iter\n",
      "Epoch: 83 | Batch: 007 / 011 | Total loss: 1.813 | Reg loss: 0.037 | Tree loss: 1.813 | Accuracy: 0.412000 | 1.0 sec/iter\n",
      "Epoch: 83 | Batch: 008 / 011 | Total loss: 1.806 | Reg loss: 0.037 | Tree loss: 1.806 | Accuracy: 0.416000 | 1.0 sec/iter\n",
      "Epoch: 83 | Batch: 009 / 011 | Total loss: 1.839 | Reg loss: 0.037 | Tree loss: 1.839 | Accuracy: 0.391000 | 1.0 sec/iter\n",
      "Epoch: 83 | Batch: 010 / 011 | Total loss: 1.778 | Reg loss: 0.037 | Tree loss: 1.778 | Accuracy: 0.430034 | 1.0 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 84 | Batch: 000 / 011 | Total loss: 2.056 | Reg loss: 0.037 | Tree loss: 2.056 | Accuracy: 0.323000 | 1.0 sec/iter\n",
      "Epoch: 84 | Batch: 001 / 011 | Total loss: 1.994 | Reg loss: 0.037 | Tree loss: 1.994 | Accuracy: 0.354000 | 1.0 sec/iter\n",
      "Epoch: 84 | Batch: 002 / 011 | Total loss: 1.928 | Reg loss: 0.037 | Tree loss: 1.928 | Accuracy: 0.370000 | 1.0 sec/iter\n",
      "Epoch: 84 | Batch: 003 / 011 | Total loss: 1.903 | Reg loss: 0.037 | Tree loss: 1.903 | Accuracy: 0.378000 | 1.0 sec/iter\n",
      "Epoch: 84 | Batch: 004 / 011 | Total loss: 1.901 | Reg loss: 0.037 | Tree loss: 1.901 | Accuracy: 0.372000 | 1.0 sec/iter\n",
      "Epoch: 84 | Batch: 005 / 011 | Total loss: 1.852 | Reg loss: 0.037 | Tree loss: 1.852 | Accuracy: 0.388000 | 1.0 sec/iter\n",
      "Epoch: 84 | Batch: 006 / 011 | Total loss: 1.868 | Reg loss: 0.037 | Tree loss: 1.868 | Accuracy: 0.383500 | 1.0 sec/iter\n",
      "Epoch: 84 | Batch: 007 / 011 | Total loss: 1.814 | Reg loss: 0.037 | Tree loss: 1.814 | Accuracy: 0.401500 | 0.999 sec/iter\n",
      "Epoch: 84 | Batch: 008 / 011 | Total loss: 1.799 | Reg loss: 0.037 | Tree loss: 1.799 | Accuracy: 0.412500 | 0.999 sec/iter\n",
      "Epoch: 84 | Batch: 009 / 011 | Total loss: 1.781 | Reg loss: 0.037 | Tree loss: 1.781 | Accuracy: 0.436000 | 0.999 sec/iter\n",
      "Epoch: 84 | Batch: 010 / 011 | Total loss: 1.867 | Reg loss: 0.037 | Tree loss: 1.867 | Accuracy: 0.385666 | 0.999 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 85 | Batch: 000 / 011 | Total loss: 2.053 | Reg loss: 0.037 | Tree loss: 2.053 | Accuracy: 0.341500 | 0.999 sec/iter\n",
      "Epoch: 85 | Batch: 001 / 011 | Total loss: 1.975 | Reg loss: 0.037 | Tree loss: 1.975 | Accuracy: 0.370000 | 0.999 sec/iter\n",
      "Epoch: 85 | Batch: 002 / 011 | Total loss: 1.944 | Reg loss: 0.037 | Tree loss: 1.944 | Accuracy: 0.357500 | 0.999 sec/iter\n",
      "Epoch: 85 | Batch: 003 / 011 | Total loss: 1.932 | Reg loss: 0.037 | Tree loss: 1.932 | Accuracy: 0.366000 | 0.999 sec/iter\n",
      "Epoch: 85 | Batch: 004 / 011 | Total loss: 1.874 | Reg loss: 0.037 | Tree loss: 1.874 | Accuracy: 0.385000 | 0.999 sec/iter\n",
      "Epoch: 85 | Batch: 005 / 011 | Total loss: 1.847 | Reg loss: 0.037 | Tree loss: 1.847 | Accuracy: 0.403500 | 0.999 sec/iter\n",
      "Epoch: 85 | Batch: 006 / 011 | Total loss: 1.827 | Reg loss: 0.037 | Tree loss: 1.827 | Accuracy: 0.423000 | 0.999 sec/iter\n",
      "Epoch: 85 | Batch: 007 / 011 | Total loss: 1.836 | Reg loss: 0.037 | Tree loss: 1.836 | Accuracy: 0.399000 | 0.998 sec/iter\n",
      "Epoch: 85 | Batch: 008 / 011 | Total loss: 1.808 | Reg loss: 0.037 | Tree loss: 1.808 | Accuracy: 0.402500 | 0.998 sec/iter\n",
      "Epoch: 85 | Batch: 009 / 011 | Total loss: 1.808 | Reg loss: 0.037 | Tree loss: 1.808 | Accuracy: 0.405000 | 0.998 sec/iter\n",
      "Epoch: 85 | Batch: 010 / 011 | Total loss: 1.804 | Reg loss: 0.037 | Tree loss: 1.804 | Accuracy: 0.423208 | 0.998 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 86 | Batch: 000 / 011 | Total loss: 2.046 | Reg loss: 0.037 | Tree loss: 2.046 | Accuracy: 0.317500 | 0.998 sec/iter\n",
      "Epoch: 86 | Batch: 001 / 011 | Total loss: 1.989 | Reg loss: 0.037 | Tree loss: 1.989 | Accuracy: 0.371500 | 0.998 sec/iter\n",
      "Epoch: 86 | Batch: 002 / 011 | Total loss: 1.966 | Reg loss: 0.037 | Tree loss: 1.966 | Accuracy: 0.346000 | 0.998 sec/iter\n",
      "Epoch: 86 | Batch: 003 / 011 | Total loss: 1.894 | Reg loss: 0.037 | Tree loss: 1.894 | Accuracy: 0.385000 | 0.998 sec/iter\n",
      "Epoch: 86 | Batch: 004 / 011 | Total loss: 1.862 | Reg loss: 0.037 | Tree loss: 1.862 | Accuracy: 0.391000 | 0.998 sec/iter\n",
      "Epoch: 86 | Batch: 005 / 011 | Total loss: 1.840 | Reg loss: 0.037 | Tree loss: 1.840 | Accuracy: 0.398500 | 0.998 sec/iter\n",
      "Epoch: 86 | Batch: 006 / 011 | Total loss: 1.811 | Reg loss: 0.037 | Tree loss: 1.811 | Accuracy: 0.419500 | 0.998 sec/iter\n",
      "Epoch: 86 | Batch: 007 / 011 | Total loss: 1.828 | Reg loss: 0.037 | Tree loss: 1.828 | Accuracy: 0.398500 | 0.997 sec/iter\n",
      "Epoch: 86 | Batch: 008 / 011 | Total loss: 1.816 | Reg loss: 0.037 | Tree loss: 1.816 | Accuracy: 0.406000 | 0.997 sec/iter\n",
      "Epoch: 86 | Batch: 009 / 011 | Total loss: 1.833 | Reg loss: 0.037 | Tree loss: 1.833 | Accuracy: 0.391000 | 0.997 sec/iter\n",
      "Epoch: 86 | Batch: 010 / 011 | Total loss: 1.814 | Reg loss: 0.038 | Tree loss: 1.814 | Accuracy: 0.409556 | 0.997 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 87 | Batch: 000 / 011 | Total loss: 2.037 | Reg loss: 0.037 | Tree loss: 2.037 | Accuracy: 0.331000 | 0.997 sec/iter\n",
      "Epoch: 87 | Batch: 001 / 011 | Total loss: 1.978 | Reg loss: 0.037 | Tree loss: 1.978 | Accuracy: 0.354500 | 0.997 sec/iter\n",
      "Epoch: 87 | Batch: 002 / 011 | Total loss: 1.938 | Reg loss: 0.037 | Tree loss: 1.938 | Accuracy: 0.358000 | 0.997 sec/iter\n",
      "Epoch: 87 | Batch: 003 / 011 | Total loss: 1.894 | Reg loss: 0.037 | Tree loss: 1.894 | Accuracy: 0.374000 | 0.997 sec/iter\n",
      "Epoch: 87 | Batch: 004 / 011 | Total loss: 1.875 | Reg loss: 0.037 | Tree loss: 1.875 | Accuracy: 0.382000 | 0.997 sec/iter\n",
      "Epoch: 87 | Batch: 005 / 011 | Total loss: 1.845 | Reg loss: 0.037 | Tree loss: 1.845 | Accuracy: 0.400500 | 0.997 sec/iter\n",
      "Epoch: 87 | Batch: 006 / 011 | Total loss: 1.817 | Reg loss: 0.037 | Tree loss: 1.817 | Accuracy: 0.430000 | 0.997 sec/iter\n",
      "Epoch: 87 | Batch: 007 / 011 | Total loss: 1.827 | Reg loss: 0.038 | Tree loss: 1.827 | Accuracy: 0.407500 | 0.996 sec/iter\n",
      "Epoch: 87 | Batch: 008 / 011 | Total loss: 1.829 | Reg loss: 0.038 | Tree loss: 1.829 | Accuracy: 0.396000 | 0.996 sec/iter\n",
      "Epoch: 87 | Batch: 009 / 011 | Total loss: 1.798 | Reg loss: 0.038 | Tree loss: 1.798 | Accuracy: 0.400500 | 0.996 sec/iter\n",
      "Epoch: 87 | Batch: 010 / 011 | Total loss: 1.829 | Reg loss: 0.038 | Tree loss: 1.829 | Accuracy: 0.382253 | 0.996 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 88 | Batch: 000 / 011 | Total loss: 2.043 | Reg loss: 0.037 | Tree loss: 2.043 | Accuracy: 0.348000 | 0.996 sec/iter\n",
      "Epoch: 88 | Batch: 001 / 011 | Total loss: 2.007 | Reg loss: 0.037 | Tree loss: 2.007 | Accuracy: 0.339000 | 0.996 sec/iter\n",
      "Epoch: 88 | Batch: 002 / 011 | Total loss: 1.927 | Reg loss: 0.037 | Tree loss: 1.927 | Accuracy: 0.359500 | 0.996 sec/iter\n",
      "Epoch: 88 | Batch: 003 / 011 | Total loss: 1.898 | Reg loss: 0.037 | Tree loss: 1.898 | Accuracy: 0.369500 | 0.996 sec/iter\n",
      "Epoch: 88 | Batch: 004 / 011 | Total loss: 1.854 | Reg loss: 0.038 | Tree loss: 1.854 | Accuracy: 0.385500 | 0.996 sec/iter\n",
      "Epoch: 88 | Batch: 005 / 011 | Total loss: 1.867 | Reg loss: 0.038 | Tree loss: 1.867 | Accuracy: 0.391000 | 0.996 sec/iter\n",
      "Epoch: 88 | Batch: 006 / 011 | Total loss: 1.833 | Reg loss: 0.038 | Tree loss: 1.833 | Accuracy: 0.392500 | 0.996 sec/iter\n",
      "Epoch: 88 | Batch: 007 / 011 | Total loss: 1.801 | Reg loss: 0.038 | Tree loss: 1.801 | Accuracy: 0.421500 | 0.996 sec/iter\n",
      "Epoch: 88 | Batch: 008 / 011 | Total loss: 1.795 | Reg loss: 0.038 | Tree loss: 1.795 | Accuracy: 0.414000 | 0.995 sec/iter\n",
      "Epoch: 88 | Batch: 009 / 011 | Total loss: 1.801 | Reg loss: 0.038 | Tree loss: 1.801 | Accuracy: 0.422500 | 0.995 sec/iter\n",
      "Epoch: 88 | Batch: 010 / 011 | Total loss: 1.797 | Reg loss: 0.038 | Tree loss: 1.797 | Accuracy: 0.430034 | 0.995 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 89 | Batch: 000 / 011 | Total loss: 2.034 | Reg loss: 0.038 | Tree loss: 2.034 | Accuracy: 0.342000 | 0.995 sec/iter\n",
      "Epoch: 89 | Batch: 001 / 011 | Total loss: 1.982 | Reg loss: 0.038 | Tree loss: 1.982 | Accuracy: 0.360000 | 0.995 sec/iter\n",
      "Epoch: 89 | Batch: 002 / 011 | Total loss: 1.969 | Reg loss: 0.038 | Tree loss: 1.969 | Accuracy: 0.337000 | 0.995 sec/iter\n",
      "Epoch: 89 | Batch: 003 / 011 | Total loss: 1.904 | Reg loss: 0.038 | Tree loss: 1.904 | Accuracy: 0.357000 | 0.995 sec/iter\n",
      "Epoch: 89 | Batch: 004 / 011 | Total loss: 1.885 | Reg loss: 0.038 | Tree loss: 1.885 | Accuracy: 0.368500 | 0.995 sec/iter\n",
      "Epoch: 89 | Batch: 005 / 011 | Total loss: 1.852 | Reg loss: 0.038 | Tree loss: 1.852 | Accuracy: 0.405500 | 0.995 sec/iter\n",
      "Epoch: 89 | Batch: 006 / 011 | Total loss: 1.828 | Reg loss: 0.038 | Tree loss: 1.828 | Accuracy: 0.419500 | 0.995 sec/iter\n",
      "Epoch: 89 | Batch: 007 / 011 | Total loss: 1.810 | Reg loss: 0.038 | Tree loss: 1.810 | Accuracy: 0.400500 | 0.995 sec/iter\n",
      "Epoch: 89 | Batch: 008 / 011 | Total loss: 1.792 | Reg loss: 0.038 | Tree loss: 1.792 | Accuracy: 0.417000 | 0.994 sec/iter\n",
      "Epoch: 89 | Batch: 009 / 011 | Total loss: 1.764 | Reg loss: 0.038 | Tree loss: 1.764 | Accuracy: 0.434000 | 0.994 sec/iter\n",
      "Epoch: 89 | Batch: 010 / 011 | Total loss: 1.789 | Reg loss: 0.038 | Tree loss: 1.789 | Accuracy: 0.416382 | 0.994 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 90 | Batch: 000 / 011 | Total loss: 2.038 | Reg loss: 0.038 | Tree loss: 2.038 | Accuracy: 0.335500 | 0.994 sec/iter\n",
      "Epoch: 90 | Batch: 001 / 011 | Total loss: 2.005 | Reg loss: 0.038 | Tree loss: 2.005 | Accuracy: 0.339000 | 0.994 sec/iter\n",
      "Epoch: 90 | Batch: 002 / 011 | Total loss: 1.947 | Reg loss: 0.038 | Tree loss: 1.947 | Accuracy: 0.351000 | 0.994 sec/iter\n",
      "Epoch: 90 | Batch: 003 / 011 | Total loss: 1.915 | Reg loss: 0.038 | Tree loss: 1.915 | Accuracy: 0.357000 | 0.994 sec/iter\n",
      "Epoch: 90 | Batch: 004 / 011 | Total loss: 1.863 | Reg loss: 0.038 | Tree loss: 1.863 | Accuracy: 0.391500 | 0.994 sec/iter\n",
      "Epoch: 90 | Batch: 005 / 011 | Total loss: 1.838 | Reg loss: 0.038 | Tree loss: 1.838 | Accuracy: 0.404500 | 0.994 sec/iter\n",
      "Epoch: 90 | Batch: 006 / 011 | Total loss: 1.796 | Reg loss: 0.038 | Tree loss: 1.796 | Accuracy: 0.424000 | 0.994 sec/iter\n",
      "Epoch: 90 | Batch: 007 / 011 | Total loss: 1.809 | Reg loss: 0.038 | Tree loss: 1.809 | Accuracy: 0.415500 | 0.994 sec/iter\n",
      "Epoch: 90 | Batch: 008 / 011 | Total loss: 1.789 | Reg loss: 0.038 | Tree loss: 1.789 | Accuracy: 0.405000 | 0.994 sec/iter\n",
      "Epoch: 90 | Batch: 009 / 011 | Total loss: 1.789 | Reg loss: 0.038 | Tree loss: 1.789 | Accuracy: 0.413000 | 0.993 sec/iter\n",
      "Epoch: 90 | Batch: 010 / 011 | Total loss: 1.835 | Reg loss: 0.038 | Tree loss: 1.835 | Accuracy: 0.416382 | 0.993 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 91 | Batch: 000 / 011 | Total loss: 2.009 | Reg loss: 0.038 | Tree loss: 2.009 | Accuracy: 0.345500 | 0.993 sec/iter\n",
      "Epoch: 91 | Batch: 001 / 011 | Total loss: 1.998 | Reg loss: 0.038 | Tree loss: 1.998 | Accuracy: 0.349500 | 0.993 sec/iter\n",
      "Epoch: 91 | Batch: 002 / 011 | Total loss: 1.931 | Reg loss: 0.038 | Tree loss: 1.931 | Accuracy: 0.354500 | 0.993 sec/iter\n",
      "Epoch: 91 | Batch: 003 / 011 | Total loss: 1.899 | Reg loss: 0.038 | Tree loss: 1.899 | Accuracy: 0.355000 | 0.993 sec/iter\n",
      "Epoch: 91 | Batch: 004 / 011 | Total loss: 1.865 | Reg loss: 0.038 | Tree loss: 1.865 | Accuracy: 0.399500 | 0.993 sec/iter\n",
      "Epoch: 91 | Batch: 005 / 011 | Total loss: 1.831 | Reg loss: 0.038 | Tree loss: 1.831 | Accuracy: 0.402000 | 0.993 sec/iter\n",
      "Epoch: 91 | Batch: 006 / 011 | Total loss: 1.828 | Reg loss: 0.038 | Tree loss: 1.828 | Accuracy: 0.405000 | 0.993 sec/iter\n",
      "Epoch: 91 | Batch: 007 / 011 | Total loss: 1.791 | Reg loss: 0.038 | Tree loss: 1.791 | Accuracy: 0.424500 | 0.993 sec/iter\n",
      "Epoch: 91 | Batch: 008 / 011 | Total loss: 1.798 | Reg loss: 0.038 | Tree loss: 1.798 | Accuracy: 0.408500 | 0.993 sec/iter\n",
      "Epoch: 91 | Batch: 009 / 011 | Total loss: 1.801 | Reg loss: 0.038 | Tree loss: 1.801 | Accuracy: 0.418500 | 0.993 sec/iter\n",
      "Epoch: 91 | Batch: 010 / 011 | Total loss: 1.808 | Reg loss: 0.038 | Tree loss: 1.808 | Accuracy: 0.389078 | 0.992 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 92 | Batch: 000 / 011 | Total loss: 2.039 | Reg loss: 0.038 | Tree loss: 2.039 | Accuracy: 0.346000 | 0.992 sec/iter\n",
      "Epoch: 92 | Batch: 001 / 011 | Total loss: 1.992 | Reg loss: 0.038 | Tree loss: 1.992 | Accuracy: 0.368000 | 0.992 sec/iter\n",
      "Epoch: 92 | Batch: 002 / 011 | Total loss: 1.912 | Reg loss: 0.038 | Tree loss: 1.912 | Accuracy: 0.371000 | 0.992 sec/iter\n",
      "Epoch: 92 | Batch: 003 / 011 | Total loss: 1.877 | Reg loss: 0.038 | Tree loss: 1.877 | Accuracy: 0.395000 | 0.992 sec/iter\n",
      "Epoch: 92 | Batch: 004 / 011 | Total loss: 1.871 | Reg loss: 0.038 | Tree loss: 1.871 | Accuracy: 0.387000 | 0.992 sec/iter\n",
      "Epoch: 92 | Batch: 005 / 011 | Total loss: 1.815 | Reg loss: 0.038 | Tree loss: 1.815 | Accuracy: 0.420500 | 0.992 sec/iter\n",
      "Epoch: 92 | Batch: 006 / 011 | Total loss: 1.819 | Reg loss: 0.038 | Tree loss: 1.819 | Accuracy: 0.408500 | 0.992 sec/iter\n",
      "Epoch: 92 | Batch: 007 / 011 | Total loss: 1.810 | Reg loss: 0.038 | Tree loss: 1.810 | Accuracy: 0.423500 | 0.992 sec/iter\n",
      "Epoch: 92 | Batch: 008 / 011 | Total loss: 1.808 | Reg loss: 0.038 | Tree loss: 1.808 | Accuracy: 0.411500 | 0.992 sec/iter\n",
      "Epoch: 92 | Batch: 009 / 011 | Total loss: 1.777 | Reg loss: 0.038 | Tree loss: 1.777 | Accuracy: 0.414500 | 0.992 sec/iter\n",
      "Epoch: 92 | Batch: 010 / 011 | Total loss: 1.788 | Reg loss: 0.038 | Tree loss: 1.788 | Accuracy: 0.392491 | 0.992 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 93 | Batch: 000 / 011 | Total loss: 1.986 | Reg loss: 0.038 | Tree loss: 1.986 | Accuracy: 0.354000 | 0.992 sec/iter\n",
      "Epoch: 93 | Batch: 001 / 011 | Total loss: 1.982 | Reg loss: 0.038 | Tree loss: 1.982 | Accuracy: 0.344500 | 0.992 sec/iter\n",
      "Epoch: 93 | Batch: 002 / 011 | Total loss: 1.917 | Reg loss: 0.038 | Tree loss: 1.917 | Accuracy: 0.380000 | 0.991 sec/iter\n",
      "Epoch: 93 | Batch: 003 / 011 | Total loss: 1.905 | Reg loss: 0.038 | Tree loss: 1.905 | Accuracy: 0.363000 | 0.991 sec/iter\n",
      "Epoch: 93 | Batch: 004 / 011 | Total loss: 1.866 | Reg loss: 0.038 | Tree loss: 1.866 | Accuracy: 0.379000 | 0.991 sec/iter\n",
      "Epoch: 93 | Batch: 005 / 011 | Total loss: 1.850 | Reg loss: 0.038 | Tree loss: 1.850 | Accuracy: 0.395000 | 0.991 sec/iter\n",
      "Epoch: 93 | Batch: 006 / 011 | Total loss: 1.806 | Reg loss: 0.038 | Tree loss: 1.806 | Accuracy: 0.402500 | 0.991 sec/iter\n",
      "Epoch: 93 | Batch: 007 / 011 | Total loss: 1.803 | Reg loss: 0.038 | Tree loss: 1.803 | Accuracy: 0.412500 | 0.991 sec/iter\n",
      "Epoch: 93 | Batch: 008 / 011 | Total loss: 1.774 | Reg loss: 0.038 | Tree loss: 1.774 | Accuracy: 0.426500 | 0.991 sec/iter\n",
      "Epoch: 93 | Batch: 009 / 011 | Total loss: 1.778 | Reg loss: 0.038 | Tree loss: 1.778 | Accuracy: 0.428500 | 0.991 sec/iter\n",
      "Epoch: 93 | Batch: 010 / 011 | Total loss: 1.836 | Reg loss: 0.038 | Tree loss: 1.836 | Accuracy: 0.358362 | 0.991 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 94 | Batch: 000 / 011 | Total loss: 1.987 | Reg loss: 0.038 | Tree loss: 1.987 | Accuracy: 0.357000 | 0.991 sec/iter\n",
      "Epoch: 94 | Batch: 001 / 011 | Total loss: 1.976 | Reg loss: 0.038 | Tree loss: 1.976 | Accuracy: 0.341000 | 0.991 sec/iter\n",
      "Epoch: 94 | Batch: 002 / 011 | Total loss: 1.918 | Reg loss: 0.038 | Tree loss: 1.918 | Accuracy: 0.368000 | 0.991 sec/iter\n",
      "Epoch: 94 | Batch: 003 / 011 | Total loss: 1.893 | Reg loss: 0.038 | Tree loss: 1.893 | Accuracy: 0.373000 | 0.991 sec/iter\n",
      "Epoch: 94 | Batch: 004 / 011 | Total loss: 1.871 | Reg loss: 0.038 | Tree loss: 1.871 | Accuracy: 0.384500 | 0.99 sec/iter\n",
      "Epoch: 94 | Batch: 005 / 011 | Total loss: 1.851 | Reg loss: 0.038 | Tree loss: 1.851 | Accuracy: 0.413500 | 0.99 sec/iter\n",
      "Epoch: 94 | Batch: 006 / 011 | Total loss: 1.812 | Reg loss: 0.038 | Tree loss: 1.812 | Accuracy: 0.419500 | 0.99 sec/iter\n",
      "Epoch: 94 | Batch: 007 / 011 | Total loss: 1.788 | Reg loss: 0.038 | Tree loss: 1.788 | Accuracy: 0.432000 | 0.99 sec/iter\n",
      "Epoch: 94 | Batch: 008 / 011 | Total loss: 1.807 | Reg loss: 0.038 | Tree loss: 1.807 | Accuracy: 0.404500 | 0.99 sec/iter\n",
      "Epoch: 94 | Batch: 009 / 011 | Total loss: 1.785 | Reg loss: 0.038 | Tree loss: 1.785 | Accuracy: 0.403500 | 0.99 sec/iter\n",
      "Epoch: 94 | Batch: 010 / 011 | Total loss: 1.770 | Reg loss: 0.038 | Tree loss: 1.770 | Accuracy: 0.460751 | 0.99 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 95 | Batch: 000 / 011 | Total loss: 1.999 | Reg loss: 0.038 | Tree loss: 1.999 | Accuracy: 0.370500 | 0.99 sec/iter\n",
      "Epoch: 95 | Batch: 001 / 011 | Total loss: 1.968 | Reg loss: 0.038 | Tree loss: 1.968 | Accuracy: 0.354500 | 0.99 sec/iter\n",
      "Epoch: 95 | Batch: 002 / 011 | Total loss: 1.915 | Reg loss: 0.038 | Tree loss: 1.915 | Accuracy: 0.367000 | 0.99 sec/iter\n",
      "Epoch: 95 | Batch: 003 / 011 | Total loss: 1.894 | Reg loss: 0.038 | Tree loss: 1.894 | Accuracy: 0.371500 | 0.99 sec/iter\n",
      "Epoch: 95 | Batch: 004 / 011 | Total loss: 1.870 | Reg loss: 0.038 | Tree loss: 1.870 | Accuracy: 0.375500 | 0.99 sec/iter\n",
      "Epoch: 95 | Batch: 005 / 011 | Total loss: 1.843 | Reg loss: 0.038 | Tree loss: 1.843 | Accuracy: 0.384000 | 0.99 sec/iter\n",
      "Epoch: 95 | Batch: 006 / 011 | Total loss: 1.809 | Reg loss: 0.038 | Tree loss: 1.809 | Accuracy: 0.428000 | 0.99 sec/iter\n",
      "Epoch: 95 | Batch: 007 / 011 | Total loss: 1.792 | Reg loss: 0.038 | Tree loss: 1.792 | Accuracy: 0.406000 | 0.989 sec/iter\n",
      "Epoch: 95 | Batch: 008 / 011 | Total loss: 1.783 | Reg loss: 0.038 | Tree loss: 1.783 | Accuracy: 0.412500 | 0.989 sec/iter\n",
      "Epoch: 95 | Batch: 009 / 011 | Total loss: 1.780 | Reg loss: 0.038 | Tree loss: 1.780 | Accuracy: 0.420000 | 0.989 sec/iter\n",
      "Epoch: 95 | Batch: 010 / 011 | Total loss: 1.781 | Reg loss: 0.038 | Tree loss: 1.781 | Accuracy: 0.395904 | 0.989 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 96 | Batch: 000 / 011 | Total loss: 2.016 | Reg loss: 0.038 | Tree loss: 2.016 | Accuracy: 0.355000 | 0.989 sec/iter\n",
      "Epoch: 96 | Batch: 001 / 011 | Total loss: 1.950 | Reg loss: 0.038 | Tree loss: 1.950 | Accuracy: 0.353500 | 0.989 sec/iter\n",
      "Epoch: 96 | Batch: 002 / 011 | Total loss: 1.913 | Reg loss: 0.038 | Tree loss: 1.913 | Accuracy: 0.366500 | 0.989 sec/iter\n",
      "Epoch: 96 | Batch: 003 / 011 | Total loss: 1.886 | Reg loss: 0.038 | Tree loss: 1.886 | Accuracy: 0.375500 | 0.989 sec/iter\n",
      "Epoch: 96 | Batch: 004 / 011 | Total loss: 1.858 | Reg loss: 0.038 | Tree loss: 1.858 | Accuracy: 0.395000 | 0.989 sec/iter\n",
      "Epoch: 96 | Batch: 005 / 011 | Total loss: 1.840 | Reg loss: 0.038 | Tree loss: 1.840 | Accuracy: 0.416000 | 0.989 sec/iter\n",
      "Epoch: 96 | Batch: 006 / 011 | Total loss: 1.775 | Reg loss: 0.038 | Tree loss: 1.775 | Accuracy: 0.434500 | 0.989 sec/iter\n",
      "Epoch: 96 | Batch: 007 / 011 | Total loss: 1.808 | Reg loss: 0.038 | Tree loss: 1.808 | Accuracy: 0.401500 | 0.989 sec/iter\n",
      "Epoch: 96 | Batch: 008 / 011 | Total loss: 1.793 | Reg loss: 0.038 | Tree loss: 1.793 | Accuracy: 0.420500 | 0.989 sec/iter\n",
      "Epoch: 96 | Batch: 009 / 011 | Total loss: 1.785 | Reg loss: 0.038 | Tree loss: 1.785 | Accuracy: 0.416000 | 0.989 sec/iter\n",
      "Epoch: 96 | Batch: 010 / 011 | Total loss: 1.790 | Reg loss: 0.038 | Tree loss: 1.790 | Accuracy: 0.399317 | 0.988 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 97 | Batch: 000 / 011 | Total loss: 1.974 | Reg loss: 0.038 | Tree loss: 1.974 | Accuracy: 0.361500 | 0.988 sec/iter\n",
      "Epoch: 97 | Batch: 001 / 011 | Total loss: 1.976 | Reg loss: 0.038 | Tree loss: 1.976 | Accuracy: 0.343000 | 0.988 sec/iter\n",
      "Epoch: 97 | Batch: 002 / 011 | Total loss: 1.916 | Reg loss: 0.038 | Tree loss: 1.916 | Accuracy: 0.359500 | 0.988 sec/iter\n",
      "Epoch: 97 | Batch: 003 / 011 | Total loss: 1.881 | Reg loss: 0.038 | Tree loss: 1.881 | Accuracy: 0.380500 | 0.988 sec/iter\n",
      "Epoch: 97 | Batch: 004 / 011 | Total loss: 1.870 | Reg loss: 0.038 | Tree loss: 1.870 | Accuracy: 0.389000 | 0.988 sec/iter\n",
      "Epoch: 97 | Batch: 005 / 011 | Total loss: 1.835 | Reg loss: 0.038 | Tree loss: 1.835 | Accuracy: 0.400500 | 0.988 sec/iter\n",
      "Epoch: 97 | Batch: 006 / 011 | Total loss: 1.795 | Reg loss: 0.038 | Tree loss: 1.795 | Accuracy: 0.418500 | 0.988 sec/iter\n",
      "Epoch: 97 | Batch: 007 / 011 | Total loss: 1.786 | Reg loss: 0.038 | Tree loss: 1.786 | Accuracy: 0.403000 | 0.988 sec/iter\n",
      "Epoch: 97 | Batch: 008 / 011 | Total loss: 1.799 | Reg loss: 0.038 | Tree loss: 1.799 | Accuracy: 0.397000 | 0.988 sec/iter\n",
      "Epoch: 97 | Batch: 009 / 011 | Total loss: 1.780 | Reg loss: 0.038 | Tree loss: 1.780 | Accuracy: 0.426500 | 0.988 sec/iter\n",
      "Epoch: 97 | Batch: 010 / 011 | Total loss: 1.754 | Reg loss: 0.038 | Tree loss: 1.754 | Accuracy: 0.470990 | 0.988 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 98 | Batch: 000 / 011 | Total loss: 1.999 | Reg loss: 0.038 | Tree loss: 1.999 | Accuracy: 0.342000 | 0.988 sec/iter\n",
      "Epoch: 98 | Batch: 001 / 011 | Total loss: 1.938 | Reg loss: 0.038 | Tree loss: 1.938 | Accuracy: 0.376000 | 0.987 sec/iter\n",
      "Epoch: 98 | Batch: 002 / 011 | Total loss: 1.940 | Reg loss: 0.038 | Tree loss: 1.940 | Accuracy: 0.340500 | 0.987 sec/iter\n",
      "Epoch: 98 | Batch: 003 / 011 | Total loss: 1.912 | Reg loss: 0.038 | Tree loss: 1.912 | Accuracy: 0.363000 | 0.987 sec/iter\n",
      "Epoch: 98 | Batch: 004 / 011 | Total loss: 1.835 | Reg loss: 0.038 | Tree loss: 1.835 | Accuracy: 0.406000 | 0.987 sec/iter\n",
      "Epoch: 98 | Batch: 005 / 011 | Total loss: 1.840 | Reg loss: 0.038 | Tree loss: 1.840 | Accuracy: 0.402500 | 0.987 sec/iter\n",
      "Epoch: 98 | Batch: 006 / 011 | Total loss: 1.815 | Reg loss: 0.038 | Tree loss: 1.815 | Accuracy: 0.417500 | 0.987 sec/iter\n",
      "Epoch: 98 | Batch: 007 / 011 | Total loss: 1.771 | Reg loss: 0.038 | Tree loss: 1.771 | Accuracy: 0.417000 | 0.987 sec/iter\n",
      "Epoch: 98 | Batch: 008 / 011 | Total loss: 1.774 | Reg loss: 0.038 | Tree loss: 1.774 | Accuracy: 0.429500 | 0.987 sec/iter\n",
      "Epoch: 98 | Batch: 009 / 011 | Total loss: 1.776 | Reg loss: 0.038 | Tree loss: 1.776 | Accuracy: 0.427500 | 0.987 sec/iter\n",
      "Epoch: 98 | Batch: 010 / 011 | Total loss: 1.740 | Reg loss: 0.038 | Tree loss: 1.740 | Accuracy: 0.443686 | 0.987 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 99 | Batch: 000 / 011 | Total loss: 1.988 | Reg loss: 0.038 | Tree loss: 1.988 | Accuracy: 0.368500 | 0.987 sec/iter\n",
      "Epoch: 99 | Batch: 001 / 011 | Total loss: 1.969 | Reg loss: 0.038 | Tree loss: 1.969 | Accuracy: 0.353500 | 0.987 sec/iter\n",
      "Epoch: 99 | Batch: 002 / 011 | Total loss: 1.920 | Reg loss: 0.038 | Tree loss: 1.920 | Accuracy: 0.356000 | 0.987 sec/iter\n",
      "Epoch: 99 | Batch: 003 / 011 | Total loss: 1.866 | Reg loss: 0.038 | Tree loss: 1.866 | Accuracy: 0.362500 | 0.987 sec/iter\n",
      "Epoch: 99 | Batch: 004 / 011 | Total loss: 1.834 | Reg loss: 0.038 | Tree loss: 1.834 | Accuracy: 0.398500 | 0.986 sec/iter\n",
      "Epoch: 99 | Batch: 005 / 011 | Total loss: 1.832 | Reg loss: 0.038 | Tree loss: 1.832 | Accuracy: 0.396000 | 0.986 sec/iter\n",
      "Epoch: 99 | Batch: 006 / 011 | Total loss: 1.811 | Reg loss: 0.038 | Tree loss: 1.811 | Accuracy: 0.418500 | 0.986 sec/iter\n",
      "Epoch: 99 | Batch: 007 / 011 | Total loss: 1.814 | Reg loss: 0.038 | Tree loss: 1.814 | Accuracy: 0.401000 | 0.986 sec/iter\n",
      "Epoch: 99 | Batch: 008 / 011 | Total loss: 1.770 | Reg loss: 0.038 | Tree loss: 1.770 | Accuracy: 0.425000 | 0.986 sec/iter\n",
      "Epoch: 99 | Batch: 009 / 011 | Total loss: 1.764 | Reg loss: 0.038 | Tree loss: 1.764 | Accuracy: 0.424500 | 0.986 sec/iter\n",
      "Epoch: 99 | Batch: 010 / 011 | Total loss: 1.808 | Reg loss: 0.039 | Tree loss: 1.808 | Accuracy: 0.409556 | 0.986 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100 | Batch: 000 / 011 | Total loss: 1.962 | Reg loss: 0.038 | Tree loss: 1.962 | Accuracy: 0.361000 | 0.986 sec/iter\n",
      "Epoch: 100 | Batch: 001 / 011 | Total loss: 1.955 | Reg loss: 0.038 | Tree loss: 1.955 | Accuracy: 0.351500 | 0.986 sec/iter\n",
      "Epoch: 100 | Batch: 002 / 011 | Total loss: 1.912 | Reg loss: 0.038 | Tree loss: 1.912 | Accuracy: 0.358500 | 0.986 sec/iter\n",
      "Epoch: 100 | Batch: 003 / 011 | Total loss: 1.889 | Reg loss: 0.038 | Tree loss: 1.889 | Accuracy: 0.381000 | 0.986 sec/iter\n",
      "Epoch: 100 | Batch: 004 / 011 | Total loss: 1.857 | Reg loss: 0.038 | Tree loss: 1.857 | Accuracy: 0.395500 | 0.986 sec/iter\n",
      "Epoch: 100 | Batch: 005 / 011 | Total loss: 1.813 | Reg loss: 0.038 | Tree loss: 1.813 | Accuracy: 0.421000 | 0.986 sec/iter\n",
      "Epoch: 100 | Batch: 006 / 011 | Total loss: 1.805 | Reg loss: 0.038 | Tree loss: 1.805 | Accuracy: 0.422000 | 0.986 sec/iter\n",
      "Epoch: 100 | Batch: 007 / 011 | Total loss: 1.787 | Reg loss: 0.039 | Tree loss: 1.787 | Accuracy: 0.431500 | 0.985 sec/iter\n",
      "Epoch: 100 | Batch: 008 / 011 | Total loss: 1.790 | Reg loss: 0.039 | Tree loss: 1.790 | Accuracy: 0.412000 | 0.985 sec/iter\n",
      "Epoch: 100 | Batch: 009 / 011 | Total loss: 1.781 | Reg loss: 0.039 | Tree loss: 1.781 | Accuracy: 0.403000 | 0.985 sec/iter\n",
      "Epoch: 100 | Batch: 010 / 011 | Total loss: 1.804 | Reg loss: 0.039 | Tree loss: 1.804 | Accuracy: 0.436860 | 0.985 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 101 | Batch: 000 / 011 | Total loss: 2.010 | Reg loss: 0.038 | Tree loss: 2.010 | Accuracy: 0.346500 | 0.985 sec/iter\n",
      "Epoch: 101 | Batch: 001 / 011 | Total loss: 1.943 | Reg loss: 0.038 | Tree loss: 1.943 | Accuracy: 0.373500 | 0.985 sec/iter\n",
      "Epoch: 101 | Batch: 002 / 011 | Total loss: 1.916 | Reg loss: 0.038 | Tree loss: 1.916 | Accuracy: 0.369000 | 0.985 sec/iter\n",
      "Epoch: 101 | Batch: 003 / 011 | Total loss: 1.868 | Reg loss: 0.038 | Tree loss: 1.868 | Accuracy: 0.387000 | 0.985 sec/iter\n",
      "Epoch: 101 | Batch: 004 / 011 | Total loss: 1.845 | Reg loss: 0.038 | Tree loss: 1.845 | Accuracy: 0.394000 | 0.985 sec/iter\n",
      "Epoch: 101 | Batch: 005 / 011 | Total loss: 1.813 | Reg loss: 0.039 | Tree loss: 1.813 | Accuracy: 0.416000 | 0.985 sec/iter\n",
      "Epoch: 101 | Batch: 006 / 011 | Total loss: 1.795 | Reg loss: 0.039 | Tree loss: 1.795 | Accuracy: 0.417000 | 0.985 sec/iter\n",
      "Epoch: 101 | Batch: 007 / 011 | Total loss: 1.806 | Reg loss: 0.039 | Tree loss: 1.806 | Accuracy: 0.395000 | 0.985 sec/iter\n",
      "Epoch: 101 | Batch: 008 / 011 | Total loss: 1.772 | Reg loss: 0.039 | Tree loss: 1.772 | Accuracy: 0.407500 | 0.985 sec/iter\n",
      "Epoch: 101 | Batch: 009 / 011 | Total loss: 1.758 | Reg loss: 0.039 | Tree loss: 1.758 | Accuracy: 0.420000 | 0.985 sec/iter\n",
      "Epoch: 101 | Batch: 010 / 011 | Total loss: 1.725 | Reg loss: 0.039 | Tree loss: 1.725 | Accuracy: 0.453925 | 0.984 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 102 | Batch: 000 / 011 | Total loss: 1.972 | Reg loss: 0.038 | Tree loss: 1.972 | Accuracy: 0.373000 | 0.984 sec/iter\n",
      "Epoch: 102 | Batch: 001 / 011 | Total loss: 1.981 | Reg loss: 0.039 | Tree loss: 1.981 | Accuracy: 0.321000 | 0.984 sec/iter\n",
      "Epoch: 102 | Batch: 002 / 011 | Total loss: 1.919 | Reg loss: 0.039 | Tree loss: 1.919 | Accuracy: 0.348000 | 0.984 sec/iter\n",
      "Epoch: 102 | Batch: 003 / 011 | Total loss: 1.880 | Reg loss: 0.039 | Tree loss: 1.880 | Accuracy: 0.367500 | 0.984 sec/iter\n",
      "Epoch: 102 | Batch: 004 / 011 | Total loss: 1.833 | Reg loss: 0.039 | Tree loss: 1.833 | Accuracy: 0.405500 | 0.984 sec/iter\n",
      "Epoch: 102 | Batch: 005 / 011 | Total loss: 1.817 | Reg loss: 0.039 | Tree loss: 1.817 | Accuracy: 0.411500 | 0.984 sec/iter\n",
      "Epoch: 102 | Batch: 006 / 011 | Total loss: 1.762 | Reg loss: 0.039 | Tree loss: 1.762 | Accuracy: 0.435000 | 0.984 sec/iter\n",
      "Epoch: 102 | Batch: 007 / 011 | Total loss: 1.785 | Reg loss: 0.039 | Tree loss: 1.785 | Accuracy: 0.421500 | 0.984 sec/iter\n",
      "Epoch: 102 | Batch: 008 / 011 | Total loss: 1.764 | Reg loss: 0.039 | Tree loss: 1.764 | Accuracy: 0.405000 | 0.984 sec/iter\n",
      "Epoch: 102 | Batch: 009 / 011 | Total loss: 1.798 | Reg loss: 0.039 | Tree loss: 1.798 | Accuracy: 0.401500 | 0.984 sec/iter\n",
      "Epoch: 102 | Batch: 010 / 011 | Total loss: 1.718 | Reg loss: 0.039 | Tree loss: 1.718 | Accuracy: 0.464164 | 0.984 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 103 | Batch: 000 / 011 | Total loss: 1.984 | Reg loss: 0.039 | Tree loss: 1.984 | Accuracy: 0.346500 | 0.984 sec/iter\n",
      "Epoch: 103 | Batch: 001 / 011 | Total loss: 1.925 | Reg loss: 0.039 | Tree loss: 1.925 | Accuracy: 0.369000 | 0.984 sec/iter\n",
      "Epoch: 103 | Batch: 002 / 011 | Total loss: 1.886 | Reg loss: 0.039 | Tree loss: 1.886 | Accuracy: 0.372000 | 0.984 sec/iter\n",
      "Epoch: 103 | Batch: 003 / 011 | Total loss: 1.862 | Reg loss: 0.039 | Tree loss: 1.862 | Accuracy: 0.385500 | 0.984 sec/iter\n",
      "Epoch: 103 | Batch: 004 / 011 | Total loss: 1.851 | Reg loss: 0.039 | Tree loss: 1.851 | Accuracy: 0.395000 | 0.984 sec/iter\n",
      "Epoch: 103 | Batch: 005 / 011 | Total loss: 1.832 | Reg loss: 0.039 | Tree loss: 1.832 | Accuracy: 0.412000 | 0.983 sec/iter\n",
      "Epoch: 103 | Batch: 006 / 011 | Total loss: 1.789 | Reg loss: 0.039 | Tree loss: 1.789 | Accuracy: 0.423000 | 0.983 sec/iter\n",
      "Epoch: 103 | Batch: 007 / 011 | Total loss: 1.785 | Reg loss: 0.039 | Tree loss: 1.785 | Accuracy: 0.408000 | 0.983 sec/iter\n",
      "Epoch: 103 | Batch: 008 / 011 | Total loss: 1.791 | Reg loss: 0.039 | Tree loss: 1.791 | Accuracy: 0.413500 | 0.983 sec/iter\n",
      "Epoch: 103 | Batch: 009 / 011 | Total loss: 1.767 | Reg loss: 0.039 | Tree loss: 1.767 | Accuracy: 0.412500 | 0.983 sec/iter\n",
      "Epoch: 103 | Batch: 010 / 011 | Total loss: 1.700 | Reg loss: 0.039 | Tree loss: 1.700 | Accuracy: 0.477816 | 0.983 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 104 | Batch: 000 / 011 | Total loss: 1.960 | Reg loss: 0.039 | Tree loss: 1.960 | Accuracy: 0.369500 | 0.983 sec/iter\n",
      "Epoch: 104 | Batch: 001 / 011 | Total loss: 1.956 | Reg loss: 0.039 | Tree loss: 1.956 | Accuracy: 0.371500 | 0.983 sec/iter\n",
      "Epoch: 104 | Batch: 002 / 011 | Total loss: 1.906 | Reg loss: 0.039 | Tree loss: 1.906 | Accuracy: 0.364500 | 0.983 sec/iter\n",
      "Epoch: 104 | Batch: 003 / 011 | Total loss: 1.842 | Reg loss: 0.039 | Tree loss: 1.842 | Accuracy: 0.405000 | 0.983 sec/iter\n",
      "Epoch: 104 | Batch: 004 / 011 | Total loss: 1.843 | Reg loss: 0.039 | Tree loss: 1.843 | Accuracy: 0.391500 | 0.983 sec/iter\n",
      "Epoch: 104 | Batch: 005 / 011 | Total loss: 1.808 | Reg loss: 0.039 | Tree loss: 1.808 | Accuracy: 0.383000 | 0.983 sec/iter\n",
      "Epoch: 104 | Batch: 006 / 011 | Total loss: 1.813 | Reg loss: 0.039 | Tree loss: 1.813 | Accuracy: 0.406500 | 0.983 sec/iter\n",
      "Epoch: 104 | Batch: 007 / 011 | Total loss: 1.764 | Reg loss: 0.039 | Tree loss: 1.764 | Accuracy: 0.412500 | 0.983 sec/iter\n",
      "Epoch: 104 | Batch: 008 / 011 | Total loss: 1.782 | Reg loss: 0.039 | Tree loss: 1.782 | Accuracy: 0.411000 | 0.983 sec/iter\n",
      "Epoch: 104 | Batch: 009 / 011 | Total loss: 1.772 | Reg loss: 0.039 | Tree loss: 1.772 | Accuracy: 0.413500 | 0.983 sec/iter\n",
      "Epoch: 104 | Batch: 010 / 011 | Total loss: 1.740 | Reg loss: 0.039 | Tree loss: 1.740 | Accuracy: 0.406143 | 0.982 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 105 | Batch: 000 / 011 | Total loss: 1.958 | Reg loss: 0.039 | Tree loss: 1.958 | Accuracy: 0.373500 | 0.982 sec/iter\n",
      "Epoch: 105 | Batch: 001 / 011 | Total loss: 1.945 | Reg loss: 0.039 | Tree loss: 1.945 | Accuracy: 0.344000 | 0.982 sec/iter\n",
      "Epoch: 105 | Batch: 002 / 011 | Total loss: 1.923 | Reg loss: 0.039 | Tree loss: 1.923 | Accuracy: 0.368000 | 0.982 sec/iter\n",
      "Epoch: 105 | Batch: 003 / 011 | Total loss: 1.873 | Reg loss: 0.039 | Tree loss: 1.873 | Accuracy: 0.380500 | 0.982 sec/iter\n",
      "Epoch: 105 | Batch: 004 / 011 | Total loss: 1.831 | Reg loss: 0.039 | Tree loss: 1.831 | Accuracy: 0.411000 | 0.982 sec/iter\n",
      "Epoch: 105 | Batch: 005 / 011 | Total loss: 1.812 | Reg loss: 0.039 | Tree loss: 1.812 | Accuracy: 0.414000 | 0.982 sec/iter\n",
      "Epoch: 105 | Batch: 006 / 011 | Total loss: 1.799 | Reg loss: 0.039 | Tree loss: 1.799 | Accuracy: 0.421000 | 0.982 sec/iter\n",
      "Epoch: 105 | Batch: 007 / 011 | Total loss: 1.772 | Reg loss: 0.039 | Tree loss: 1.772 | Accuracy: 0.420500 | 0.982 sec/iter\n",
      "Epoch: 105 | Batch: 008 / 011 | Total loss: 1.772 | Reg loss: 0.039 | Tree loss: 1.772 | Accuracy: 0.405500 | 0.982 sec/iter\n",
      "Epoch: 105 | Batch: 009 / 011 | Total loss: 1.765 | Reg loss: 0.039 | Tree loss: 1.765 | Accuracy: 0.410000 | 0.982 sec/iter\n",
      "Epoch: 105 | Batch: 010 / 011 | Total loss: 1.763 | Reg loss: 0.039 | Tree loss: 1.763 | Accuracy: 0.436860 | 0.982 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 106 | Batch: 000 / 011 | Total loss: 1.999 | Reg loss: 0.039 | Tree loss: 1.999 | Accuracy: 0.347500 | 0.982 sec/iter\n",
      "Epoch: 106 | Batch: 001 / 011 | Total loss: 1.951 | Reg loss: 0.039 | Tree loss: 1.951 | Accuracy: 0.351500 | 0.982 sec/iter\n",
      "Epoch: 106 | Batch: 002 / 011 | Total loss: 1.906 | Reg loss: 0.039 | Tree loss: 1.906 | Accuracy: 0.374500 | 0.982 sec/iter\n",
      "Epoch: 106 | Batch: 003 / 011 | Total loss: 1.847 | Reg loss: 0.039 | Tree loss: 1.847 | Accuracy: 0.393500 | 0.982 sec/iter\n",
      "Epoch: 106 | Batch: 004 / 011 | Total loss: 1.796 | Reg loss: 0.039 | Tree loss: 1.796 | Accuracy: 0.405000 | 0.982 sec/iter\n",
      "Epoch: 106 | Batch: 005 / 011 | Total loss: 1.806 | Reg loss: 0.039 | Tree loss: 1.806 | Accuracy: 0.401000 | 0.981 sec/iter\n",
      "Epoch: 106 | Batch: 006 / 011 | Total loss: 1.795 | Reg loss: 0.039 | Tree loss: 1.795 | Accuracy: 0.414000 | 0.981 sec/iter\n",
      "Epoch: 106 | Batch: 007 / 011 | Total loss: 1.774 | Reg loss: 0.039 | Tree loss: 1.774 | Accuracy: 0.418000 | 0.981 sec/iter\n",
      "Epoch: 106 | Batch: 008 / 011 | Total loss: 1.770 | Reg loss: 0.039 | Tree loss: 1.770 | Accuracy: 0.413500 | 0.981 sec/iter\n",
      "Epoch: 106 | Batch: 009 / 011 | Total loss: 1.764 | Reg loss: 0.039 | Tree loss: 1.764 | Accuracy: 0.426500 | 0.981 sec/iter\n",
      "Epoch: 106 | Batch: 010 / 011 | Total loss: 1.770 | Reg loss: 0.039 | Tree loss: 1.770 | Accuracy: 0.409556 | 0.981 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 107 | Batch: 000 / 011 | Total loss: 1.966 | Reg loss: 0.039 | Tree loss: 1.966 | Accuracy: 0.355000 | 0.981 sec/iter\n",
      "Epoch: 107 | Batch: 001 / 011 | Total loss: 1.923 | Reg loss: 0.039 | Tree loss: 1.923 | Accuracy: 0.355500 | 0.981 sec/iter\n",
      "Epoch: 107 | Batch: 002 / 011 | Total loss: 1.888 | Reg loss: 0.039 | Tree loss: 1.888 | Accuracy: 0.366000 | 0.981 sec/iter\n",
      "Epoch: 107 | Batch: 003 / 011 | Total loss: 1.866 | Reg loss: 0.039 | Tree loss: 1.866 | Accuracy: 0.393500 | 0.981 sec/iter\n",
      "Epoch: 107 | Batch: 004 / 011 | Total loss: 1.801 | Reg loss: 0.039 | Tree loss: 1.801 | Accuracy: 0.414500 | 0.981 sec/iter\n",
      "Epoch: 107 | Batch: 005 / 011 | Total loss: 1.842 | Reg loss: 0.039 | Tree loss: 1.842 | Accuracy: 0.396500 | 0.981 sec/iter\n",
      "Epoch: 107 | Batch: 006 / 011 | Total loss: 1.793 | Reg loss: 0.039 | Tree loss: 1.793 | Accuracy: 0.414500 | 0.981 sec/iter\n",
      "Epoch: 107 | Batch: 007 / 011 | Total loss: 1.771 | Reg loss: 0.039 | Tree loss: 1.771 | Accuracy: 0.419500 | 0.981 sec/iter\n",
      "Epoch: 107 | Batch: 008 / 011 | Total loss: 1.756 | Reg loss: 0.039 | Tree loss: 1.756 | Accuracy: 0.411500 | 0.981 sec/iter\n",
      "Epoch: 107 | Batch: 009 / 011 | Total loss: 1.787 | Reg loss: 0.039 | Tree loss: 1.787 | Accuracy: 0.396500 | 0.981 sec/iter\n",
      "Epoch: 107 | Batch: 010 / 011 | Total loss: 1.767 | Reg loss: 0.039 | Tree loss: 1.767 | Accuracy: 0.450512 | 0.98 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 108 | Batch: 000 / 011 | Total loss: 1.965 | Reg loss: 0.039 | Tree loss: 1.965 | Accuracy: 0.367500 | 0.981 sec/iter\n",
      "Epoch: 108 | Batch: 001 / 011 | Total loss: 1.924 | Reg loss: 0.039 | Tree loss: 1.924 | Accuracy: 0.352500 | 0.98 sec/iter\n",
      "Epoch: 108 | Batch: 002 / 011 | Total loss: 1.892 | Reg loss: 0.039 | Tree loss: 1.892 | Accuracy: 0.375000 | 0.98 sec/iter\n",
      "Epoch: 108 | Batch: 003 / 011 | Total loss: 1.874 | Reg loss: 0.039 | Tree loss: 1.874 | Accuracy: 0.385500 | 0.98 sec/iter\n",
      "Epoch: 108 | Batch: 004 / 011 | Total loss: 1.816 | Reg loss: 0.039 | Tree loss: 1.816 | Accuracy: 0.417000 | 0.98 sec/iter\n",
      "Epoch: 108 | Batch: 005 / 011 | Total loss: 1.797 | Reg loss: 0.039 | Tree loss: 1.797 | Accuracy: 0.419500 | 0.98 sec/iter\n",
      "Epoch: 108 | Batch: 006 / 011 | Total loss: 1.781 | Reg loss: 0.039 | Tree loss: 1.781 | Accuracy: 0.427000 | 0.98 sec/iter\n",
      "Epoch: 108 | Batch: 007 / 011 | Total loss: 1.777 | Reg loss: 0.039 | Tree loss: 1.777 | Accuracy: 0.419500 | 0.98 sec/iter\n",
      "Epoch: 108 | Batch: 008 / 011 | Total loss: 1.777 | Reg loss: 0.039 | Tree loss: 1.777 | Accuracy: 0.414500 | 0.98 sec/iter\n",
      "Epoch: 108 | Batch: 009 / 011 | Total loss: 1.773 | Reg loss: 0.039 | Tree loss: 1.773 | Accuracy: 0.415500 | 0.98 sec/iter\n",
      "Epoch: 108 | Batch: 010 / 011 | Total loss: 1.757 | Reg loss: 0.039 | Tree loss: 1.757 | Accuracy: 0.406143 | 0.98 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 109 | Batch: 000 / 011 | Total loss: 1.977 | Reg loss: 0.039 | Tree loss: 1.977 | Accuracy: 0.357000 | 0.98 sec/iter\n",
      "Epoch: 109 | Batch: 001 / 011 | Total loss: 1.950 | Reg loss: 0.039 | Tree loss: 1.950 | Accuracy: 0.343500 | 0.98 sec/iter\n",
      "Epoch: 109 | Batch: 002 / 011 | Total loss: 1.895 | Reg loss: 0.039 | Tree loss: 1.895 | Accuracy: 0.379500 | 0.98 sec/iter\n",
      "Epoch: 109 | Batch: 003 / 011 | Total loss: 1.837 | Reg loss: 0.039 | Tree loss: 1.837 | Accuracy: 0.414500 | 0.98 sec/iter\n",
      "Epoch: 109 | Batch: 004 / 011 | Total loss: 1.811 | Reg loss: 0.039 | Tree loss: 1.811 | Accuracy: 0.433000 | 0.98 sec/iter\n",
      "Epoch: 109 | Batch: 005 / 011 | Total loss: 1.797 | Reg loss: 0.039 | Tree loss: 1.797 | Accuracy: 0.418000 | 0.979 sec/iter\n",
      "Epoch: 109 | Batch: 006 / 011 | Total loss: 1.806 | Reg loss: 0.039 | Tree loss: 1.806 | Accuracy: 0.407500 | 0.979 sec/iter\n",
      "Epoch: 109 | Batch: 007 / 011 | Total loss: 1.779 | Reg loss: 0.039 | Tree loss: 1.779 | Accuracy: 0.418000 | 0.979 sec/iter\n",
      "Epoch: 109 | Batch: 008 / 011 | Total loss: 1.754 | Reg loss: 0.039 | Tree loss: 1.754 | Accuracy: 0.414500 | 0.979 sec/iter\n",
      "Epoch: 109 | Batch: 009 / 011 | Total loss: 1.772 | Reg loss: 0.039 | Tree loss: 1.772 | Accuracy: 0.415500 | 0.979 sec/iter\n",
      "Epoch: 109 | Batch: 010 / 011 | Total loss: 1.715 | Reg loss: 0.039 | Tree loss: 1.715 | Accuracy: 0.426621 | 0.979 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 110 | Batch: 000 / 011 | Total loss: 1.950 | Reg loss: 0.039 | Tree loss: 1.950 | Accuracy: 0.370500 | 0.979 sec/iter\n",
      "Epoch: 110 | Batch: 001 / 011 | Total loss: 1.917 | Reg loss: 0.039 | Tree loss: 1.917 | Accuracy: 0.374500 | 0.979 sec/iter\n",
      "Epoch: 110 | Batch: 002 / 011 | Total loss: 1.906 | Reg loss: 0.039 | Tree loss: 1.906 | Accuracy: 0.355000 | 0.979 sec/iter\n",
      "Epoch: 110 | Batch: 003 / 011 | Total loss: 1.868 | Reg loss: 0.039 | Tree loss: 1.868 | Accuracy: 0.375500 | 0.979 sec/iter\n",
      "Epoch: 110 | Batch: 004 / 011 | Total loss: 1.824 | Reg loss: 0.039 | Tree loss: 1.824 | Accuracy: 0.384500 | 0.979 sec/iter\n",
      "Epoch: 110 | Batch: 005 / 011 | Total loss: 1.805 | Reg loss: 0.039 | Tree loss: 1.805 | Accuracy: 0.410000 | 0.979 sec/iter\n",
      "Epoch: 110 | Batch: 006 / 011 | Total loss: 1.778 | Reg loss: 0.039 | Tree loss: 1.778 | Accuracy: 0.399000 | 0.979 sec/iter\n",
      "Epoch: 110 | Batch: 007 / 011 | Total loss: 1.786 | Reg loss: 0.039 | Tree loss: 1.786 | Accuracy: 0.415500 | 0.979 sec/iter\n",
      "Epoch: 110 | Batch: 008 / 011 | Total loss: 1.751 | Reg loss: 0.039 | Tree loss: 1.751 | Accuracy: 0.427500 | 0.979 sec/iter\n",
      "Epoch: 110 | Batch: 009 / 011 | Total loss: 1.770 | Reg loss: 0.039 | Tree loss: 1.770 | Accuracy: 0.424000 | 0.979 sec/iter\n",
      "Epoch: 110 | Batch: 010 / 011 | Total loss: 1.683 | Reg loss: 0.039 | Tree loss: 1.683 | Accuracy: 0.460751 | 0.978 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 111 | Batch: 000 / 011 | Total loss: 1.957 | Reg loss: 0.039 | Tree loss: 1.957 | Accuracy: 0.372000 | 0.979 sec/iter\n",
      "Epoch: 111 | Batch: 001 / 011 | Total loss: 1.932 | Reg loss: 0.039 | Tree loss: 1.932 | Accuracy: 0.367000 | 0.978 sec/iter\n",
      "Epoch: 111 | Batch: 002 / 011 | Total loss: 1.893 | Reg loss: 0.039 | Tree loss: 1.893 | Accuracy: 0.374000 | 0.978 sec/iter\n",
      "Epoch: 111 | Batch: 003 / 011 | Total loss: 1.840 | Reg loss: 0.039 | Tree loss: 1.840 | Accuracy: 0.395500 | 0.978 sec/iter\n",
      "Epoch: 111 | Batch: 004 / 011 | Total loss: 1.804 | Reg loss: 0.039 | Tree loss: 1.804 | Accuracy: 0.420000 | 0.978 sec/iter\n",
      "Epoch: 111 | Batch: 005 / 011 | Total loss: 1.813 | Reg loss: 0.039 | Tree loss: 1.813 | Accuracy: 0.411000 | 0.978 sec/iter\n",
      "Epoch: 111 | Batch: 006 / 011 | Total loss: 1.782 | Reg loss: 0.039 | Tree loss: 1.782 | Accuracy: 0.425500 | 0.978 sec/iter\n",
      "Epoch: 111 | Batch: 007 / 011 | Total loss: 1.794 | Reg loss: 0.039 | Tree loss: 1.794 | Accuracy: 0.394500 | 0.978 sec/iter\n",
      "Epoch: 111 | Batch: 008 / 011 | Total loss: 1.749 | Reg loss: 0.039 | Tree loss: 1.749 | Accuracy: 0.428500 | 0.978 sec/iter\n",
      "Epoch: 111 | Batch: 009 / 011 | Total loss: 1.763 | Reg loss: 0.039 | Tree loss: 1.763 | Accuracy: 0.420000 | 0.978 sec/iter\n",
      "Epoch: 111 | Batch: 010 / 011 | Total loss: 1.752 | Reg loss: 0.039 | Tree loss: 1.752 | Accuracy: 0.440273 | 0.978 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 112 | Batch: 000 / 011 | Total loss: 1.943 | Reg loss: 0.039 | Tree loss: 1.943 | Accuracy: 0.374500 | 0.978 sec/iter\n",
      "Epoch: 112 | Batch: 001 / 011 | Total loss: 1.922 | Reg loss: 0.039 | Tree loss: 1.922 | Accuracy: 0.377500 | 0.978 sec/iter\n",
      "Epoch: 112 | Batch: 002 / 011 | Total loss: 1.870 | Reg loss: 0.039 | Tree loss: 1.870 | Accuracy: 0.383000 | 0.978 sec/iter\n",
      "Epoch: 112 | Batch: 003 / 011 | Total loss: 1.855 | Reg loss: 0.039 | Tree loss: 1.855 | Accuracy: 0.382500 | 0.978 sec/iter\n",
      "Epoch: 112 | Batch: 004 / 011 | Total loss: 1.832 | Reg loss: 0.039 | Tree loss: 1.832 | Accuracy: 0.406000 | 0.978 sec/iter\n",
      "Epoch: 112 | Batch: 005 / 011 | Total loss: 1.803 | Reg loss: 0.039 | Tree loss: 1.803 | Accuracy: 0.419000 | 0.978 sec/iter\n",
      "Epoch: 112 | Batch: 006 / 011 | Total loss: 1.766 | Reg loss: 0.039 | Tree loss: 1.766 | Accuracy: 0.411000 | 0.978 sec/iter\n",
      "Epoch: 112 | Batch: 007 / 011 | Total loss: 1.777 | Reg loss: 0.039 | Tree loss: 1.777 | Accuracy: 0.417000 | 0.978 sec/iter\n",
      "Epoch: 112 | Batch: 008 / 011 | Total loss: 1.766 | Reg loss: 0.039 | Tree loss: 1.766 | Accuracy: 0.417000 | 0.978 sec/iter\n",
      "Epoch: 112 | Batch: 009 / 011 | Total loss: 1.762 | Reg loss: 0.039 | Tree loss: 1.762 | Accuracy: 0.422500 | 0.977 sec/iter\n",
      "Epoch: 112 | Batch: 010 / 011 | Total loss: 1.762 | Reg loss: 0.039 | Tree loss: 1.762 | Accuracy: 0.430034 | 0.977 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 113 | Batch: 000 / 011 | Total loss: 1.942 | Reg loss: 0.039 | Tree loss: 1.942 | Accuracy: 0.357500 | 0.977 sec/iter\n",
      "Epoch: 113 | Batch: 001 / 011 | Total loss: 1.911 | Reg loss: 0.039 | Tree loss: 1.911 | Accuracy: 0.380500 | 0.977 sec/iter\n",
      "Epoch: 113 | Batch: 002 / 011 | Total loss: 1.913 | Reg loss: 0.039 | Tree loss: 1.913 | Accuracy: 0.378500 | 0.977 sec/iter\n",
      "Epoch: 113 | Batch: 003 / 011 | Total loss: 1.842 | Reg loss: 0.039 | Tree loss: 1.842 | Accuracy: 0.398000 | 0.977 sec/iter\n",
      "Epoch: 113 | Batch: 004 / 011 | Total loss: 1.827 | Reg loss: 0.039 | Tree loss: 1.827 | Accuracy: 0.430000 | 0.977 sec/iter\n",
      "Epoch: 113 | Batch: 005 / 011 | Total loss: 1.795 | Reg loss: 0.039 | Tree loss: 1.795 | Accuracy: 0.420000 | 0.977 sec/iter\n",
      "Epoch: 113 | Batch: 006 / 011 | Total loss: 1.778 | Reg loss: 0.039 | Tree loss: 1.778 | Accuracy: 0.429500 | 0.977 sec/iter\n",
      "Epoch: 113 | Batch: 007 / 011 | Total loss: 1.760 | Reg loss: 0.039 | Tree loss: 1.760 | Accuracy: 0.408000 | 0.977 sec/iter\n",
      "Epoch: 113 | Batch: 008 / 011 | Total loss: 1.769 | Reg loss: 0.039 | Tree loss: 1.769 | Accuracy: 0.417500 | 0.977 sec/iter\n",
      "Epoch: 113 | Batch: 009 / 011 | Total loss: 1.768 | Reg loss: 0.039 | Tree loss: 1.768 | Accuracy: 0.413000 | 0.977 sec/iter\n",
      "Epoch: 113 | Batch: 010 / 011 | Total loss: 1.697 | Reg loss: 0.039 | Tree loss: 1.697 | Accuracy: 0.443686 | 0.977 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 114 | Batch: 000 / 011 | Total loss: 1.949 | Reg loss: 0.039 | Tree loss: 1.949 | Accuracy: 0.371000 | 0.977 sec/iter\n",
      "Epoch: 114 | Batch: 001 / 011 | Total loss: 1.913 | Reg loss: 0.039 | Tree loss: 1.913 | Accuracy: 0.369000 | 0.977 sec/iter\n",
      "Epoch: 114 | Batch: 002 / 011 | Total loss: 1.881 | Reg loss: 0.039 | Tree loss: 1.881 | Accuracy: 0.378500 | 0.977 sec/iter\n",
      "Epoch: 114 | Batch: 003 / 011 | Total loss: 1.851 | Reg loss: 0.039 | Tree loss: 1.851 | Accuracy: 0.380000 | 0.977 sec/iter\n",
      "Epoch: 114 | Batch: 004 / 011 | Total loss: 1.819 | Reg loss: 0.039 | Tree loss: 1.819 | Accuracy: 0.394500 | 0.977 sec/iter\n",
      "Epoch: 114 | Batch: 005 / 011 | Total loss: 1.812 | Reg loss: 0.039 | Tree loss: 1.812 | Accuracy: 0.394500 | 0.977 sec/iter\n",
      "Epoch: 114 | Batch: 006 / 011 | Total loss: 1.791 | Reg loss: 0.039 | Tree loss: 1.791 | Accuracy: 0.413500 | 0.976 sec/iter\n",
      "Epoch: 114 | Batch: 007 / 011 | Total loss: 1.778 | Reg loss: 0.039 | Tree loss: 1.778 | Accuracy: 0.411000 | 0.976 sec/iter\n",
      "Epoch: 114 | Batch: 008 / 011 | Total loss: 1.747 | Reg loss: 0.039 | Tree loss: 1.747 | Accuracy: 0.423000 | 0.976 sec/iter\n",
      "Epoch: 114 | Batch: 009 / 011 | Total loss: 1.739 | Reg loss: 0.039 | Tree loss: 1.739 | Accuracy: 0.426000 | 0.976 sec/iter\n",
      "Epoch: 114 | Batch: 010 / 011 | Total loss: 1.815 | Reg loss: 0.039 | Tree loss: 1.815 | Accuracy: 0.385666 | 0.976 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 115 | Batch: 000 / 011 | Total loss: 1.946 | Reg loss: 0.039 | Tree loss: 1.946 | Accuracy: 0.364500 | 0.976 sec/iter\n",
      "Epoch: 115 | Batch: 001 / 011 | Total loss: 1.919 | Reg loss: 0.039 | Tree loss: 1.919 | Accuracy: 0.363500 | 0.976 sec/iter\n",
      "Epoch: 115 | Batch: 002 / 011 | Total loss: 1.888 | Reg loss: 0.039 | Tree loss: 1.888 | Accuracy: 0.381500 | 0.976 sec/iter\n",
      "Epoch: 115 | Batch: 003 / 011 | Total loss: 1.870 | Reg loss: 0.039 | Tree loss: 1.870 | Accuracy: 0.372000 | 0.976 sec/iter\n",
      "Epoch: 115 | Batch: 004 / 011 | Total loss: 1.809 | Reg loss: 0.039 | Tree loss: 1.809 | Accuracy: 0.425500 | 0.976 sec/iter\n",
      "Epoch: 115 | Batch: 005 / 011 | Total loss: 1.795 | Reg loss: 0.039 | Tree loss: 1.795 | Accuracy: 0.427000 | 0.976 sec/iter\n",
      "Epoch: 115 | Batch: 006 / 011 | Total loss: 1.779 | Reg loss: 0.039 | Tree loss: 1.779 | Accuracy: 0.417500 | 0.976 sec/iter\n",
      "Epoch: 115 | Batch: 007 / 011 | Total loss: 1.772 | Reg loss: 0.039 | Tree loss: 1.772 | Accuracy: 0.404000 | 0.976 sec/iter\n",
      "Epoch: 115 | Batch: 008 / 011 | Total loss: 1.769 | Reg loss: 0.039 | Tree loss: 1.769 | Accuracy: 0.414500 | 0.976 sec/iter\n",
      "Epoch: 115 | Batch: 009 / 011 | Total loss: 1.725 | Reg loss: 0.039 | Tree loss: 1.725 | Accuracy: 0.445000 | 0.976 sec/iter\n",
      "Epoch: 115 | Batch: 010 / 011 | Total loss: 1.675 | Reg loss: 0.039 | Tree loss: 1.675 | Accuracy: 0.522184 | 0.976 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 116 | Batch: 000 / 011 | Total loss: 1.949 | Reg loss: 0.039 | Tree loss: 1.949 | Accuracy: 0.369000 | 0.976 sec/iter\n",
      "Epoch: 116 | Batch: 001 / 011 | Total loss: 1.924 | Reg loss: 0.039 | Tree loss: 1.924 | Accuracy: 0.370000 | 0.976 sec/iter\n",
      "Epoch: 116 | Batch: 002 / 011 | Total loss: 1.879 | Reg loss: 0.039 | Tree loss: 1.879 | Accuracy: 0.372500 | 0.976 sec/iter\n",
      "Epoch: 116 | Batch: 003 / 011 | Total loss: 1.847 | Reg loss: 0.039 | Tree loss: 1.847 | Accuracy: 0.408500 | 0.976 sec/iter\n",
      "Epoch: 116 | Batch: 004 / 011 | Total loss: 1.825 | Reg loss: 0.039 | Tree loss: 1.825 | Accuracy: 0.407000 | 0.975 sec/iter\n",
      "Epoch: 116 | Batch: 005 / 011 | Total loss: 1.791 | Reg loss: 0.039 | Tree loss: 1.791 | Accuracy: 0.427500 | 0.975 sec/iter\n",
      "Epoch: 116 | Batch: 006 / 011 | Total loss: 1.766 | Reg loss: 0.039 | Tree loss: 1.766 | Accuracy: 0.423000 | 0.975 sec/iter\n",
      "Epoch: 116 | Batch: 007 / 011 | Total loss: 1.767 | Reg loss: 0.039 | Tree loss: 1.767 | Accuracy: 0.411500 | 0.975 sec/iter\n",
      "Epoch: 116 | Batch: 008 / 011 | Total loss: 1.745 | Reg loss: 0.039 | Tree loss: 1.745 | Accuracy: 0.407500 | 0.975 sec/iter\n",
      "Epoch: 116 | Batch: 009 / 011 | Total loss: 1.757 | Reg loss: 0.039 | Tree loss: 1.757 | Accuracy: 0.404500 | 0.975 sec/iter\n",
      "Epoch: 116 | Batch: 010 / 011 | Total loss: 1.827 | Reg loss: 0.040 | Tree loss: 1.827 | Accuracy: 0.399317 | 0.975 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 117 | Batch: 000 / 011 | Total loss: 1.936 | Reg loss: 0.039 | Tree loss: 1.936 | Accuracy: 0.376000 | 0.975 sec/iter\n",
      "Epoch: 117 | Batch: 001 / 011 | Total loss: 1.923 | Reg loss: 0.039 | Tree loss: 1.923 | Accuracy: 0.372000 | 0.975 sec/iter\n",
      "Epoch: 117 | Batch: 002 / 011 | Total loss: 1.889 | Reg loss: 0.039 | Tree loss: 1.889 | Accuracy: 0.382000 | 0.975 sec/iter\n",
      "Epoch: 117 | Batch: 003 / 011 | Total loss: 1.863 | Reg loss: 0.039 | Tree loss: 1.863 | Accuracy: 0.381500 | 0.975 sec/iter\n",
      "Epoch: 117 | Batch: 004 / 011 | Total loss: 1.806 | Reg loss: 0.039 | Tree loss: 1.806 | Accuracy: 0.411000 | 0.975 sec/iter\n",
      "Epoch: 117 | Batch: 005 / 011 | Total loss: 1.799 | Reg loss: 0.039 | Tree loss: 1.799 | Accuracy: 0.411000 | 0.975 sec/iter\n",
      "Epoch: 117 | Batch: 006 / 011 | Total loss: 1.762 | Reg loss: 0.039 | Tree loss: 1.762 | Accuracy: 0.421500 | 0.975 sec/iter\n",
      "Epoch: 117 | Batch: 007 / 011 | Total loss: 1.758 | Reg loss: 0.039 | Tree loss: 1.758 | Accuracy: 0.420500 | 0.975 sec/iter\n",
      "Epoch: 117 | Batch: 008 / 011 | Total loss: 1.731 | Reg loss: 0.040 | Tree loss: 1.731 | Accuracy: 0.423000 | 0.975 sec/iter\n",
      "Epoch: 117 | Batch: 009 / 011 | Total loss: 1.758 | Reg loss: 0.040 | Tree loss: 1.758 | Accuracy: 0.420500 | 0.975 sec/iter\n",
      "Epoch: 117 | Batch: 010 / 011 | Total loss: 1.749 | Reg loss: 0.040 | Tree loss: 1.749 | Accuracy: 0.464164 | 0.975 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 118 | Batch: 000 / 011 | Total loss: 1.952 | Reg loss: 0.039 | Tree loss: 1.952 | Accuracy: 0.360000 | 0.975 sec/iter\n",
      "Epoch: 118 | Batch: 001 / 011 | Total loss: 1.929 | Reg loss: 0.039 | Tree loss: 1.929 | Accuracy: 0.374500 | 0.975 sec/iter\n",
      "Epoch: 118 | Batch: 002 / 011 | Total loss: 1.886 | Reg loss: 0.039 | Tree loss: 1.886 | Accuracy: 0.376000 | 0.975 sec/iter\n",
      "Epoch: 118 | Batch: 003 / 011 | Total loss: 1.842 | Reg loss: 0.039 | Tree loss: 1.842 | Accuracy: 0.403500 | 0.974 sec/iter\n",
      "Epoch: 118 | Batch: 004 / 011 | Total loss: 1.803 | Reg loss: 0.039 | Tree loss: 1.803 | Accuracy: 0.417000 | 0.974 sec/iter\n",
      "Epoch: 118 | Batch: 005 / 011 | Total loss: 1.805 | Reg loss: 0.039 | Tree loss: 1.805 | Accuracy: 0.421000 | 0.974 sec/iter\n",
      "Epoch: 118 | Batch: 006 / 011 | Total loss: 1.752 | Reg loss: 0.040 | Tree loss: 1.752 | Accuracy: 0.422000 | 0.974 sec/iter\n",
      "Epoch: 118 | Batch: 007 / 011 | Total loss: 1.772 | Reg loss: 0.040 | Tree loss: 1.772 | Accuracy: 0.410000 | 0.974 sec/iter\n",
      "Epoch: 118 | Batch: 008 / 011 | Total loss: 1.747 | Reg loss: 0.040 | Tree loss: 1.747 | Accuracy: 0.419000 | 0.974 sec/iter\n",
      "Epoch: 118 | Batch: 009 / 011 | Total loss: 1.735 | Reg loss: 0.040 | Tree loss: 1.735 | Accuracy: 0.431000 | 0.974 sec/iter\n",
      "Epoch: 118 | Batch: 010 / 011 | Total loss: 1.707 | Reg loss: 0.040 | Tree loss: 1.707 | Accuracy: 0.433447 | 0.974 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 119 | Batch: 000 / 011 | Total loss: 1.951 | Reg loss: 0.039 | Tree loss: 1.951 | Accuracy: 0.360500 | 0.974 sec/iter\n",
      "Epoch: 119 | Batch: 001 / 011 | Total loss: 1.906 | Reg loss: 0.039 | Tree loss: 1.906 | Accuracy: 0.371000 | 0.974 sec/iter\n",
      "Epoch: 119 | Batch: 002 / 011 | Total loss: 1.864 | Reg loss: 0.039 | Tree loss: 1.864 | Accuracy: 0.406500 | 0.974 sec/iter\n",
      "Epoch: 119 | Batch: 003 / 011 | Total loss: 1.825 | Reg loss: 0.039 | Tree loss: 1.825 | Accuracy: 0.406500 | 0.974 sec/iter\n",
      "Epoch: 119 | Batch: 004 / 011 | Total loss: 1.823 | Reg loss: 0.040 | Tree loss: 1.823 | Accuracy: 0.416000 | 0.974 sec/iter\n",
      "Epoch: 119 | Batch: 005 / 011 | Total loss: 1.795 | Reg loss: 0.040 | Tree loss: 1.795 | Accuracy: 0.417500 | 0.974 sec/iter\n",
      "Epoch: 119 | Batch: 006 / 011 | Total loss: 1.761 | Reg loss: 0.040 | Tree loss: 1.761 | Accuracy: 0.428500 | 0.974 sec/iter\n",
      "Epoch: 119 | Batch: 007 / 011 | Total loss: 1.765 | Reg loss: 0.040 | Tree loss: 1.765 | Accuracy: 0.419500 | 0.974 sec/iter\n",
      "Epoch: 119 | Batch: 008 / 011 | Total loss: 1.770 | Reg loss: 0.040 | Tree loss: 1.770 | Accuracy: 0.406000 | 0.974 sec/iter\n",
      "Epoch: 119 | Batch: 009 / 011 | Total loss: 1.742 | Reg loss: 0.040 | Tree loss: 1.742 | Accuracy: 0.428000 | 0.974 sec/iter\n",
      "Epoch: 119 | Batch: 010 / 011 | Total loss: 1.737 | Reg loss: 0.040 | Tree loss: 1.737 | Accuracy: 0.406143 | 0.974 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 120 | Batch: 000 / 011 | Total loss: 1.966 | Reg loss: 0.040 | Tree loss: 1.966 | Accuracy: 0.344500 | 0.974 sec/iter\n",
      "Epoch: 120 | Batch: 001 / 011 | Total loss: 1.904 | Reg loss: 0.040 | Tree loss: 1.904 | Accuracy: 0.370500 | 0.974 sec/iter\n",
      "Epoch: 120 | Batch: 002 / 011 | Total loss: 1.877 | Reg loss: 0.040 | Tree loss: 1.877 | Accuracy: 0.369500 | 0.973 sec/iter\n",
      "Epoch: 120 | Batch: 003 / 011 | Total loss: 1.831 | Reg loss: 0.040 | Tree loss: 1.831 | Accuracy: 0.396000 | 0.973 sec/iter\n",
      "Epoch: 120 | Batch: 004 / 011 | Total loss: 1.823 | Reg loss: 0.040 | Tree loss: 1.823 | Accuracy: 0.391500 | 0.973 sec/iter\n",
      "Epoch: 120 | Batch: 005 / 011 | Total loss: 1.781 | Reg loss: 0.040 | Tree loss: 1.781 | Accuracy: 0.411500 | 0.973 sec/iter\n",
      "Epoch: 120 | Batch: 006 / 011 | Total loss: 1.783 | Reg loss: 0.040 | Tree loss: 1.783 | Accuracy: 0.410500 | 0.973 sec/iter\n",
      "Epoch: 120 | Batch: 007 / 011 | Total loss: 1.730 | Reg loss: 0.040 | Tree loss: 1.730 | Accuracy: 0.426000 | 0.973 sec/iter\n",
      "Epoch: 120 | Batch: 008 / 011 | Total loss: 1.750 | Reg loss: 0.040 | Tree loss: 1.750 | Accuracy: 0.436500 | 0.973 sec/iter\n",
      "Epoch: 120 | Batch: 009 / 011 | Total loss: 1.739 | Reg loss: 0.040 | Tree loss: 1.739 | Accuracy: 0.428500 | 0.973 sec/iter\n",
      "Epoch: 120 | Batch: 010 / 011 | Total loss: 1.791 | Reg loss: 0.040 | Tree loss: 1.791 | Accuracy: 0.385666 | 0.973 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 121 | Batch: 000 / 011 | Total loss: 1.955 | Reg loss: 0.040 | Tree loss: 1.955 | Accuracy: 0.354000 | 0.973 sec/iter\n",
      "Epoch: 121 | Batch: 001 / 011 | Total loss: 1.903 | Reg loss: 0.040 | Tree loss: 1.903 | Accuracy: 0.362000 | 0.973 sec/iter\n",
      "Epoch: 121 | Batch: 002 / 011 | Total loss: 1.880 | Reg loss: 0.040 | Tree loss: 1.880 | Accuracy: 0.387000 | 0.973 sec/iter\n",
      "Epoch: 121 | Batch: 003 / 011 | Total loss: 1.835 | Reg loss: 0.040 | Tree loss: 1.835 | Accuracy: 0.396000 | 0.973 sec/iter\n",
      "Epoch: 121 | Batch: 004 / 011 | Total loss: 1.795 | Reg loss: 0.040 | Tree loss: 1.795 | Accuracy: 0.414000 | 0.973 sec/iter\n",
      "Epoch: 121 | Batch: 005 / 011 | Total loss: 1.779 | Reg loss: 0.040 | Tree loss: 1.779 | Accuracy: 0.421500 | 0.973 sec/iter\n",
      "Epoch: 121 | Batch: 006 / 011 | Total loss: 1.743 | Reg loss: 0.040 | Tree loss: 1.743 | Accuracy: 0.434000 | 0.973 sec/iter\n",
      "Epoch: 121 | Batch: 007 / 011 | Total loss: 1.767 | Reg loss: 0.040 | Tree loss: 1.767 | Accuracy: 0.413500 | 0.973 sec/iter\n",
      "Epoch: 121 | Batch: 008 / 011 | Total loss: 1.747 | Reg loss: 0.040 | Tree loss: 1.747 | Accuracy: 0.431000 | 0.973 sec/iter\n",
      "Epoch: 121 | Batch: 009 / 011 | Total loss: 1.762 | Reg loss: 0.040 | Tree loss: 1.762 | Accuracy: 0.419500 | 0.973 sec/iter\n",
      "Epoch: 121 | Batch: 010 / 011 | Total loss: 1.717 | Reg loss: 0.040 | Tree loss: 1.717 | Accuracy: 0.457338 | 0.973 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 122 | Batch: 000 / 011 | Total loss: 1.970 | Reg loss: 0.040 | Tree loss: 1.970 | Accuracy: 0.369500 | 0.973 sec/iter\n",
      "Epoch: 122 | Batch: 001 / 011 | Total loss: 1.916 | Reg loss: 0.040 | Tree loss: 1.916 | Accuracy: 0.353500 | 0.973 sec/iter\n",
      "Epoch: 122 | Batch: 002 / 011 | Total loss: 1.872 | Reg loss: 0.040 | Tree loss: 1.872 | Accuracy: 0.382500 | 0.972 sec/iter\n",
      "Epoch: 122 | Batch: 003 / 011 | Total loss: 1.864 | Reg loss: 0.040 | Tree loss: 1.864 | Accuracy: 0.394500 | 0.972 sec/iter\n",
      "Epoch: 122 | Batch: 004 / 011 | Total loss: 1.815 | Reg loss: 0.040 | Tree loss: 1.815 | Accuracy: 0.380000 | 0.972 sec/iter\n",
      "Epoch: 122 | Batch: 005 / 011 | Total loss: 1.787 | Reg loss: 0.040 | Tree loss: 1.787 | Accuracy: 0.418500 | 0.972 sec/iter\n",
      "Epoch: 122 | Batch: 006 / 011 | Total loss: 1.774 | Reg loss: 0.040 | Tree loss: 1.774 | Accuracy: 0.410500 | 0.972 sec/iter\n",
      "Epoch: 122 | Batch: 007 / 011 | Total loss: 1.740 | Reg loss: 0.040 | Tree loss: 1.740 | Accuracy: 0.424500 | 0.972 sec/iter\n",
      "Epoch: 122 | Batch: 008 / 011 | Total loss: 1.722 | Reg loss: 0.040 | Tree loss: 1.722 | Accuracy: 0.417000 | 0.972 sec/iter\n",
      "Epoch: 122 | Batch: 009 / 011 | Total loss: 1.729 | Reg loss: 0.040 | Tree loss: 1.729 | Accuracy: 0.449000 | 0.972 sec/iter\n",
      "Epoch: 122 | Batch: 010 / 011 | Total loss: 1.824 | Reg loss: 0.040 | Tree loss: 1.824 | Accuracy: 0.382253 | 0.972 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 123 | Batch: 000 / 011 | Total loss: 1.928 | Reg loss: 0.040 | Tree loss: 1.928 | Accuracy: 0.390000 | 0.972 sec/iter\n",
      "Epoch: 123 | Batch: 001 / 011 | Total loss: 1.876 | Reg loss: 0.040 | Tree loss: 1.876 | Accuracy: 0.379500 | 0.972 sec/iter\n",
      "Epoch: 123 | Batch: 002 / 011 | Total loss: 1.871 | Reg loss: 0.040 | Tree loss: 1.871 | Accuracy: 0.399500 | 0.972 sec/iter\n",
      "Epoch: 123 | Batch: 003 / 011 | Total loss: 1.843 | Reg loss: 0.040 | Tree loss: 1.843 | Accuracy: 0.413000 | 0.972 sec/iter\n",
      "Epoch: 123 | Batch: 004 / 011 | Total loss: 1.829 | Reg loss: 0.040 | Tree loss: 1.829 | Accuracy: 0.406500 | 0.972 sec/iter\n",
      "Epoch: 123 | Batch: 005 / 011 | Total loss: 1.793 | Reg loss: 0.040 | Tree loss: 1.793 | Accuracy: 0.409000 | 0.972 sec/iter\n",
      "Epoch: 123 | Batch: 006 / 011 | Total loss: 1.757 | Reg loss: 0.040 | Tree loss: 1.757 | Accuracy: 0.425500 | 0.972 sec/iter\n",
      "Epoch: 123 | Batch: 007 / 011 | Total loss: 1.779 | Reg loss: 0.040 | Tree loss: 1.779 | Accuracy: 0.418000 | 0.972 sec/iter\n",
      "Epoch: 123 | Batch: 008 / 011 | Total loss: 1.727 | Reg loss: 0.040 | Tree loss: 1.727 | Accuracy: 0.429000 | 0.972 sec/iter\n",
      "Epoch: 123 | Batch: 009 / 011 | Total loss: 1.770 | Reg loss: 0.040 | Tree loss: 1.770 | Accuracy: 0.415500 | 0.972 sec/iter\n",
      "Epoch: 123 | Batch: 010 / 011 | Total loss: 1.723 | Reg loss: 0.040 | Tree loss: 1.723 | Accuracy: 0.426621 | 0.972 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 124 | Batch: 000 / 011 | Total loss: 1.934 | Reg loss: 0.040 | Tree loss: 1.934 | Accuracy: 0.374000 | 0.972 sec/iter\n",
      "Epoch: 124 | Batch: 001 / 011 | Total loss: 1.920 | Reg loss: 0.040 | Tree loss: 1.920 | Accuracy: 0.369500 | 0.972 sec/iter\n",
      "Epoch: 124 | Batch: 002 / 011 | Total loss: 1.891 | Reg loss: 0.040 | Tree loss: 1.891 | Accuracy: 0.375500 | 0.971 sec/iter\n",
      "Epoch: 124 | Batch: 003 / 011 | Total loss: 1.852 | Reg loss: 0.040 | Tree loss: 1.852 | Accuracy: 0.385000 | 0.971 sec/iter\n",
      "Epoch: 124 | Batch: 004 / 011 | Total loss: 1.793 | Reg loss: 0.040 | Tree loss: 1.793 | Accuracy: 0.422500 | 0.971 sec/iter\n",
      "Epoch: 124 | Batch: 005 / 011 | Total loss: 1.786 | Reg loss: 0.040 | Tree loss: 1.786 | Accuracy: 0.423500 | 0.971 sec/iter\n",
      "Epoch: 124 | Batch: 006 / 011 | Total loss: 1.768 | Reg loss: 0.040 | Tree loss: 1.768 | Accuracy: 0.410000 | 0.971 sec/iter\n",
      "Epoch: 124 | Batch: 007 / 011 | Total loss: 1.730 | Reg loss: 0.040 | Tree loss: 1.730 | Accuracy: 0.436000 | 0.971 sec/iter\n",
      "Epoch: 124 | Batch: 008 / 011 | Total loss: 1.742 | Reg loss: 0.040 | Tree loss: 1.742 | Accuracy: 0.418500 | 0.971 sec/iter\n",
      "Epoch: 124 | Batch: 009 / 011 | Total loss: 1.733 | Reg loss: 0.040 | Tree loss: 1.733 | Accuracy: 0.432000 | 0.971 sec/iter\n",
      "Epoch: 124 | Batch: 010 / 011 | Total loss: 1.725 | Reg loss: 0.040 | Tree loss: 1.725 | Accuracy: 0.467577 | 0.971 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 125 | Batch: 000 / 011 | Total loss: 1.938 | Reg loss: 0.040 | Tree loss: 1.938 | Accuracy: 0.372500 | 0.971 sec/iter\n",
      "Epoch: 125 | Batch: 001 / 011 | Total loss: 1.908 | Reg loss: 0.040 | Tree loss: 1.908 | Accuracy: 0.374000 | 0.971 sec/iter\n",
      "Epoch: 125 | Batch: 002 / 011 | Total loss: 1.864 | Reg loss: 0.040 | Tree loss: 1.864 | Accuracy: 0.398000 | 0.971 sec/iter\n",
      "Epoch: 125 | Batch: 003 / 011 | Total loss: 1.833 | Reg loss: 0.040 | Tree loss: 1.833 | Accuracy: 0.402500 | 0.971 sec/iter\n",
      "Epoch: 125 | Batch: 004 / 011 | Total loss: 1.810 | Reg loss: 0.040 | Tree loss: 1.810 | Accuracy: 0.420000 | 0.971 sec/iter\n",
      "Epoch: 125 | Batch: 005 / 011 | Total loss: 1.766 | Reg loss: 0.040 | Tree loss: 1.766 | Accuracy: 0.423000 | 0.971 sec/iter\n",
      "Epoch: 125 | Batch: 006 / 011 | Total loss: 1.774 | Reg loss: 0.040 | Tree loss: 1.774 | Accuracy: 0.405000 | 0.971 sec/iter\n",
      "Epoch: 125 | Batch: 007 / 011 | Total loss: 1.735 | Reg loss: 0.040 | Tree loss: 1.735 | Accuracy: 0.425500 | 0.971 sec/iter\n",
      "Epoch: 125 | Batch: 008 / 011 | Total loss: 1.755 | Reg loss: 0.040 | Tree loss: 1.755 | Accuracy: 0.418500 | 0.971 sec/iter\n",
      "Epoch: 125 | Batch: 009 / 011 | Total loss: 1.757 | Reg loss: 0.040 | Tree loss: 1.757 | Accuracy: 0.412500 | 0.971 sec/iter\n",
      "Epoch: 125 | Batch: 010 / 011 | Total loss: 1.723 | Reg loss: 0.040 | Tree loss: 1.723 | Accuracy: 0.416382 | 0.971 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 126 | Batch: 000 / 011 | Total loss: 1.914 | Reg loss: 0.040 | Tree loss: 1.914 | Accuracy: 0.391500 | 0.971 sec/iter\n",
      "Epoch: 126 | Batch: 001 / 011 | Total loss: 1.918 | Reg loss: 0.040 | Tree loss: 1.918 | Accuracy: 0.358500 | 0.971 sec/iter\n",
      "Epoch: 126 | Batch: 002 / 011 | Total loss: 1.836 | Reg loss: 0.040 | Tree loss: 1.836 | Accuracy: 0.403500 | 0.97 sec/iter\n",
      "Epoch: 126 | Batch: 003 / 011 | Total loss: 1.854 | Reg loss: 0.040 | Tree loss: 1.854 | Accuracy: 0.390500 | 0.97 sec/iter\n",
      "Epoch: 126 | Batch: 004 / 011 | Total loss: 1.818 | Reg loss: 0.040 | Tree loss: 1.818 | Accuracy: 0.403000 | 0.97 sec/iter\n",
      "Epoch: 126 | Batch: 005 / 011 | Total loss: 1.795 | Reg loss: 0.040 | Tree loss: 1.795 | Accuracy: 0.399000 | 0.97 sec/iter\n",
      "Epoch: 126 | Batch: 006 / 011 | Total loss: 1.761 | Reg loss: 0.040 | Tree loss: 1.761 | Accuracy: 0.415000 | 0.97 sec/iter\n",
      "Epoch: 126 | Batch: 007 / 011 | Total loss: 1.760 | Reg loss: 0.040 | Tree loss: 1.760 | Accuracy: 0.408500 | 0.97 sec/iter\n",
      "Epoch: 126 | Batch: 008 / 011 | Total loss: 1.713 | Reg loss: 0.040 | Tree loss: 1.713 | Accuracy: 0.442000 | 0.97 sec/iter\n",
      "Epoch: 126 | Batch: 009 / 011 | Total loss: 1.765 | Reg loss: 0.040 | Tree loss: 1.765 | Accuracy: 0.405000 | 0.97 sec/iter\n",
      "Epoch: 126 | Batch: 010 / 011 | Total loss: 1.763 | Reg loss: 0.040 | Tree loss: 1.763 | Accuracy: 0.430034 | 0.97 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 127 | Batch: 000 / 011 | Total loss: 1.951 | Reg loss: 0.040 | Tree loss: 1.951 | Accuracy: 0.358000 | 0.97 sec/iter\n",
      "Epoch: 127 | Batch: 001 / 011 | Total loss: 1.902 | Reg loss: 0.040 | Tree loss: 1.902 | Accuracy: 0.373000 | 0.97 sec/iter\n",
      "Epoch: 127 | Batch: 002 / 011 | Total loss: 1.851 | Reg loss: 0.040 | Tree loss: 1.851 | Accuracy: 0.410000 | 0.97 sec/iter\n",
      "Epoch: 127 | Batch: 003 / 011 | Total loss: 1.816 | Reg loss: 0.040 | Tree loss: 1.816 | Accuracy: 0.399500 | 0.97 sec/iter\n",
      "Epoch: 127 | Batch: 004 / 011 | Total loss: 1.795 | Reg loss: 0.040 | Tree loss: 1.795 | Accuracy: 0.417500 | 0.97 sec/iter\n",
      "Epoch: 127 | Batch: 005 / 011 | Total loss: 1.791 | Reg loss: 0.040 | Tree loss: 1.791 | Accuracy: 0.426500 | 0.97 sec/iter\n",
      "Epoch: 127 | Batch: 006 / 011 | Total loss: 1.740 | Reg loss: 0.040 | Tree loss: 1.740 | Accuracy: 0.429000 | 0.97 sec/iter\n",
      "Epoch: 127 | Batch: 007 / 011 | Total loss: 1.775 | Reg loss: 0.040 | Tree loss: 1.775 | Accuracy: 0.398500 | 0.97 sec/iter\n",
      "Epoch: 127 | Batch: 008 / 011 | Total loss: 1.764 | Reg loss: 0.040 | Tree loss: 1.764 | Accuracy: 0.406000 | 0.97 sec/iter\n",
      "Epoch: 127 | Batch: 009 / 011 | Total loss: 1.731 | Reg loss: 0.040 | Tree loss: 1.731 | Accuracy: 0.431000 | 0.97 sec/iter\n",
      "Epoch: 127 | Batch: 010 / 011 | Total loss: 1.703 | Reg loss: 0.040 | Tree loss: 1.703 | Accuracy: 0.406143 | 0.97 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 128 | Batch: 000 / 011 | Total loss: 1.910 | Reg loss: 0.040 | Tree loss: 1.910 | Accuracy: 0.383000 | 0.97 sec/iter\n",
      "Epoch: 128 | Batch: 001 / 011 | Total loss: 1.921 | Reg loss: 0.040 | Tree loss: 1.921 | Accuracy: 0.360000 | 0.97 sec/iter\n",
      "Epoch: 128 | Batch: 002 / 011 | Total loss: 1.860 | Reg loss: 0.040 | Tree loss: 1.860 | Accuracy: 0.395500 | 0.97 sec/iter\n",
      "Epoch: 128 | Batch: 003 / 011 | Total loss: 1.843 | Reg loss: 0.040 | Tree loss: 1.843 | Accuracy: 0.384000 | 0.969 sec/iter\n",
      "Epoch: 128 | Batch: 004 / 011 | Total loss: 1.810 | Reg loss: 0.040 | Tree loss: 1.810 | Accuracy: 0.400000 | 0.969 sec/iter\n",
      "Epoch: 128 | Batch: 005 / 011 | Total loss: 1.761 | Reg loss: 0.040 | Tree loss: 1.761 | Accuracy: 0.420000 | 0.969 sec/iter\n",
      "Epoch: 128 | Batch: 006 / 011 | Total loss: 1.754 | Reg loss: 0.040 | Tree loss: 1.754 | Accuracy: 0.402000 | 0.969 sec/iter\n",
      "Epoch: 128 | Batch: 007 / 011 | Total loss: 1.749 | Reg loss: 0.040 | Tree loss: 1.749 | Accuracy: 0.418500 | 0.969 sec/iter\n",
      "Epoch: 128 | Batch: 008 / 011 | Total loss: 1.763 | Reg loss: 0.040 | Tree loss: 1.763 | Accuracy: 0.407500 | 0.969 sec/iter\n",
      "Epoch: 128 | Batch: 009 / 011 | Total loss: 1.742 | Reg loss: 0.040 | Tree loss: 1.742 | Accuracy: 0.421000 | 0.969 sec/iter\n",
      "Epoch: 128 | Batch: 010 / 011 | Total loss: 1.715 | Reg loss: 0.040 | Tree loss: 1.715 | Accuracy: 0.453925 | 0.969 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 129 | Batch: 000 / 011 | Total loss: 1.938 | Reg loss: 0.040 | Tree loss: 1.938 | Accuracy: 0.357000 | 0.969 sec/iter\n",
      "Epoch: 129 | Batch: 001 / 011 | Total loss: 1.898 | Reg loss: 0.040 | Tree loss: 1.898 | Accuracy: 0.385000 | 0.969 sec/iter\n",
      "Epoch: 129 | Batch: 002 / 011 | Total loss: 1.886 | Reg loss: 0.040 | Tree loss: 1.886 | Accuracy: 0.392000 | 0.969 sec/iter\n",
      "Epoch: 129 | Batch: 003 / 011 | Total loss: 1.830 | Reg loss: 0.040 | Tree loss: 1.830 | Accuracy: 0.402000 | 0.969 sec/iter\n",
      "Epoch: 129 | Batch: 004 / 011 | Total loss: 1.788 | Reg loss: 0.040 | Tree loss: 1.788 | Accuracy: 0.431000 | 0.969 sec/iter\n",
      "Epoch: 129 | Batch: 005 / 011 | Total loss: 1.750 | Reg loss: 0.040 | Tree loss: 1.750 | Accuracy: 0.423500 | 0.969 sec/iter\n",
      "Epoch: 129 | Batch: 006 / 011 | Total loss: 1.758 | Reg loss: 0.040 | Tree loss: 1.758 | Accuracy: 0.427000 | 0.969 sec/iter\n",
      "Epoch: 129 | Batch: 007 / 011 | Total loss: 1.748 | Reg loss: 0.040 | Tree loss: 1.748 | Accuracy: 0.406500 | 0.969 sec/iter\n",
      "Epoch: 129 | Batch: 008 / 011 | Total loss: 1.751 | Reg loss: 0.040 | Tree loss: 1.751 | Accuracy: 0.400000 | 0.969 sec/iter\n",
      "Epoch: 129 | Batch: 009 / 011 | Total loss: 1.736 | Reg loss: 0.040 | Tree loss: 1.736 | Accuracy: 0.437000 | 0.969 sec/iter\n",
      "Epoch: 129 | Batch: 010 / 011 | Total loss: 1.697 | Reg loss: 0.040 | Tree loss: 1.697 | Accuracy: 0.440273 | 0.969 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 130 | Batch: 000 / 011 | Total loss: 1.937 | Reg loss: 0.040 | Tree loss: 1.937 | Accuracy: 0.377500 | 0.969 sec/iter\n",
      "Epoch: 130 | Batch: 001 / 011 | Total loss: 1.893 | Reg loss: 0.040 | Tree loss: 1.893 | Accuracy: 0.371000 | 0.969 sec/iter\n",
      "Epoch: 130 | Batch: 002 / 011 | Total loss: 1.861 | Reg loss: 0.040 | Tree loss: 1.861 | Accuracy: 0.390500 | 0.969 sec/iter\n",
      "Epoch: 130 | Batch: 003 / 011 | Total loss: 1.811 | Reg loss: 0.040 | Tree loss: 1.811 | Accuracy: 0.403500 | 0.969 sec/iter\n",
      "Epoch: 130 | Batch: 004 / 011 | Total loss: 1.815 | Reg loss: 0.040 | Tree loss: 1.815 | Accuracy: 0.391000 | 0.969 sec/iter\n",
      "Epoch: 130 | Batch: 005 / 011 | Total loss: 1.791 | Reg loss: 0.040 | Tree loss: 1.791 | Accuracy: 0.418500 | 0.969 sec/iter\n",
      "Epoch: 130 | Batch: 006 / 011 | Total loss: 1.764 | Reg loss: 0.040 | Tree loss: 1.764 | Accuracy: 0.417000 | 0.969 sec/iter\n",
      "Epoch: 130 | Batch: 007 / 011 | Total loss: 1.745 | Reg loss: 0.040 | Tree loss: 1.745 | Accuracy: 0.418000 | 0.968 sec/iter\n",
      "Epoch: 130 | Batch: 008 / 011 | Total loss: 1.740 | Reg loss: 0.040 | Tree loss: 1.740 | Accuracy: 0.431000 | 0.968 sec/iter\n",
      "Epoch: 130 | Batch: 009 / 011 | Total loss: 1.739 | Reg loss: 0.040 | Tree loss: 1.739 | Accuracy: 0.417500 | 0.968 sec/iter\n",
      "Epoch: 130 | Batch: 010 / 011 | Total loss: 1.763 | Reg loss: 0.040 | Tree loss: 1.763 | Accuracy: 0.389078 | 0.968 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 131 | Batch: 000 / 011 | Total loss: 1.931 | Reg loss: 0.040 | Tree loss: 1.931 | Accuracy: 0.371000 | 0.968 sec/iter\n",
      "Epoch: 131 | Batch: 001 / 011 | Total loss: 1.870 | Reg loss: 0.040 | Tree loss: 1.870 | Accuracy: 0.385500 | 0.968 sec/iter\n",
      "Epoch: 131 | Batch: 002 / 011 | Total loss: 1.846 | Reg loss: 0.040 | Tree loss: 1.846 | Accuracy: 0.413500 | 0.968 sec/iter\n",
      "Epoch: 131 | Batch: 003 / 011 | Total loss: 1.839 | Reg loss: 0.040 | Tree loss: 1.839 | Accuracy: 0.403000 | 0.968 sec/iter\n",
      "Epoch: 131 | Batch: 004 / 011 | Total loss: 1.804 | Reg loss: 0.040 | Tree loss: 1.804 | Accuracy: 0.427000 | 0.968 sec/iter\n",
      "Epoch: 131 | Batch: 005 / 011 | Total loss: 1.780 | Reg loss: 0.040 | Tree loss: 1.780 | Accuracy: 0.412000 | 0.968 sec/iter\n",
      "Epoch: 131 | Batch: 006 / 011 | Total loss: 1.741 | Reg loss: 0.040 | Tree loss: 1.741 | Accuracy: 0.417500 | 0.968 sec/iter\n",
      "Epoch: 131 | Batch: 007 / 011 | Total loss: 1.768 | Reg loss: 0.040 | Tree loss: 1.768 | Accuracy: 0.405000 | 0.968 sec/iter\n",
      "Epoch: 131 | Batch: 008 / 011 | Total loss: 1.749 | Reg loss: 0.040 | Tree loss: 1.749 | Accuracy: 0.420000 | 0.968 sec/iter\n",
      "Epoch: 131 | Batch: 009 / 011 | Total loss: 1.744 | Reg loss: 0.040 | Tree loss: 1.744 | Accuracy: 0.416500 | 0.968 sec/iter\n",
      "Epoch: 131 | Batch: 010 / 011 | Total loss: 1.700 | Reg loss: 0.040 | Tree loss: 1.700 | Accuracy: 0.453925 | 0.968 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 132 | Batch: 000 / 011 | Total loss: 1.930 | Reg loss: 0.040 | Tree loss: 1.930 | Accuracy: 0.375000 | 0.968 sec/iter\n",
      "Epoch: 132 | Batch: 001 / 011 | Total loss: 1.878 | Reg loss: 0.040 | Tree loss: 1.878 | Accuracy: 0.390500 | 0.968 sec/iter\n",
      "Epoch: 132 | Batch: 002 / 011 | Total loss: 1.833 | Reg loss: 0.040 | Tree loss: 1.833 | Accuracy: 0.409500 | 0.968 sec/iter\n",
      "Epoch: 132 | Batch: 003 / 011 | Total loss: 1.809 | Reg loss: 0.040 | Tree loss: 1.809 | Accuracy: 0.395500 | 0.968 sec/iter\n",
      "Epoch: 132 | Batch: 004 / 011 | Total loss: 1.808 | Reg loss: 0.040 | Tree loss: 1.808 | Accuracy: 0.406500 | 0.968 sec/iter\n",
      "Epoch: 132 | Batch: 005 / 011 | Total loss: 1.779 | Reg loss: 0.040 | Tree loss: 1.779 | Accuracy: 0.401000 | 0.968 sec/iter\n",
      "Epoch: 132 | Batch: 006 / 011 | Total loss: 1.755 | Reg loss: 0.040 | Tree loss: 1.755 | Accuracy: 0.432500 | 0.968 sec/iter\n",
      "Epoch: 132 | Batch: 007 / 011 | Total loss: 1.755 | Reg loss: 0.040 | Tree loss: 1.755 | Accuracy: 0.419000 | 0.968 sec/iter\n",
      "Epoch: 132 | Batch: 008 / 011 | Total loss: 1.746 | Reg loss: 0.040 | Tree loss: 1.746 | Accuracy: 0.414500 | 0.968 sec/iter\n",
      "Epoch: 132 | Batch: 009 / 011 | Total loss: 1.758 | Reg loss: 0.040 | Tree loss: 1.758 | Accuracy: 0.427500 | 0.968 sec/iter\n",
      "Epoch: 132 | Batch: 010 / 011 | Total loss: 1.794 | Reg loss: 0.040 | Tree loss: 1.794 | Accuracy: 0.372014 | 0.968 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 133 | Batch: 000 / 011 | Total loss: 1.925 | Reg loss: 0.040 | Tree loss: 1.925 | Accuracy: 0.387500 | 0.968 sec/iter\n",
      "Epoch: 133 | Batch: 001 / 011 | Total loss: 1.890 | Reg loss: 0.040 | Tree loss: 1.890 | Accuracy: 0.384000 | 0.968 sec/iter\n",
      "Epoch: 133 | Batch: 002 / 011 | Total loss: 1.884 | Reg loss: 0.040 | Tree loss: 1.884 | Accuracy: 0.381000 | 0.967 sec/iter\n",
      "Epoch: 133 | Batch: 003 / 011 | Total loss: 1.850 | Reg loss: 0.040 | Tree loss: 1.850 | Accuracy: 0.392000 | 0.967 sec/iter\n",
      "Epoch: 133 | Batch: 004 / 011 | Total loss: 1.809 | Reg loss: 0.040 | Tree loss: 1.809 | Accuracy: 0.417500 | 0.967 sec/iter\n",
      "Epoch: 133 | Batch: 005 / 011 | Total loss: 1.770 | Reg loss: 0.040 | Tree loss: 1.770 | Accuracy: 0.418000 | 0.967 sec/iter\n",
      "Epoch: 133 | Batch: 006 / 011 | Total loss: 1.736 | Reg loss: 0.040 | Tree loss: 1.736 | Accuracy: 0.426000 | 0.967 sec/iter\n",
      "Epoch: 133 | Batch: 007 / 011 | Total loss: 1.742 | Reg loss: 0.040 | Tree loss: 1.742 | Accuracy: 0.424000 | 0.967 sec/iter\n",
      "Epoch: 133 | Batch: 008 / 011 | Total loss: 1.733 | Reg loss: 0.040 | Tree loss: 1.733 | Accuracy: 0.422000 | 0.967 sec/iter\n",
      "Epoch: 133 | Batch: 009 / 011 | Total loss: 1.708 | Reg loss: 0.040 | Tree loss: 1.708 | Accuracy: 0.430000 | 0.967 sec/iter\n",
      "Epoch: 133 | Batch: 010 / 011 | Total loss: 1.772 | Reg loss: 0.040 | Tree loss: 1.772 | Accuracy: 0.399317 | 0.967 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 134 | Batch: 000 / 011 | Total loss: 1.944 | Reg loss: 0.040 | Tree loss: 1.944 | Accuracy: 0.372000 | 0.967 sec/iter\n",
      "Epoch: 134 | Batch: 001 / 011 | Total loss: 1.883 | Reg loss: 0.040 | Tree loss: 1.883 | Accuracy: 0.370000 | 0.967 sec/iter\n",
      "Epoch: 134 | Batch: 002 / 011 | Total loss: 1.842 | Reg loss: 0.040 | Tree loss: 1.842 | Accuracy: 0.395500 | 0.967 sec/iter\n",
      "Epoch: 134 | Batch: 003 / 011 | Total loss: 1.825 | Reg loss: 0.040 | Tree loss: 1.825 | Accuracy: 0.391500 | 0.967 sec/iter\n",
      "Epoch: 134 | Batch: 004 / 011 | Total loss: 1.821 | Reg loss: 0.040 | Tree loss: 1.821 | Accuracy: 0.399000 | 0.967 sec/iter\n",
      "Epoch: 134 | Batch: 005 / 011 | Total loss: 1.758 | Reg loss: 0.040 | Tree loss: 1.758 | Accuracy: 0.426000 | 0.967 sec/iter\n",
      "Epoch: 134 | Batch: 006 / 011 | Total loss: 1.756 | Reg loss: 0.040 | Tree loss: 1.756 | Accuracy: 0.415000 | 0.967 sec/iter\n",
      "Epoch: 134 | Batch: 007 / 011 | Total loss: 1.743 | Reg loss: 0.040 | Tree loss: 1.743 | Accuracy: 0.421000 | 0.967 sec/iter\n",
      "Epoch: 134 | Batch: 008 / 011 | Total loss: 1.725 | Reg loss: 0.040 | Tree loss: 1.725 | Accuracy: 0.436000 | 0.967 sec/iter\n",
      "Epoch: 134 | Batch: 009 / 011 | Total loss: 1.740 | Reg loss: 0.040 | Tree loss: 1.740 | Accuracy: 0.430500 | 0.967 sec/iter\n",
      "Epoch: 134 | Batch: 010 / 011 | Total loss: 1.678 | Reg loss: 0.040 | Tree loss: 1.678 | Accuracy: 0.419795 | 0.967 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 135 | Batch: 000 / 011 | Total loss: 1.916 | Reg loss: 0.040 | Tree loss: 1.916 | Accuracy: 0.389000 | 0.967 sec/iter\n",
      "Epoch: 135 | Batch: 001 / 011 | Total loss: 1.887 | Reg loss: 0.040 | Tree loss: 1.887 | Accuracy: 0.397500 | 0.967 sec/iter\n",
      "Epoch: 135 | Batch: 002 / 011 | Total loss: 1.853 | Reg loss: 0.040 | Tree loss: 1.853 | Accuracy: 0.387000 | 0.967 sec/iter\n",
      "Epoch: 135 | Batch: 003 / 011 | Total loss: 1.822 | Reg loss: 0.040 | Tree loss: 1.822 | Accuracy: 0.405000 | 0.967 sec/iter\n",
      "Epoch: 135 | Batch: 004 / 011 | Total loss: 1.770 | Reg loss: 0.040 | Tree loss: 1.770 | Accuracy: 0.422000 | 0.967 sec/iter\n",
      "Epoch: 135 | Batch: 005 / 011 | Total loss: 1.806 | Reg loss: 0.040 | Tree loss: 1.806 | Accuracy: 0.409500 | 0.967 sec/iter\n",
      "Epoch: 135 | Batch: 006 / 011 | Total loss: 1.741 | Reg loss: 0.040 | Tree loss: 1.741 | Accuracy: 0.431000 | 0.967 sec/iter\n",
      "Epoch: 135 | Batch: 007 / 011 | Total loss: 1.762 | Reg loss: 0.040 | Tree loss: 1.762 | Accuracy: 0.415500 | 0.967 sec/iter\n",
      "Epoch: 135 | Batch: 008 / 011 | Total loss: 1.757 | Reg loss: 0.040 | Tree loss: 1.757 | Accuracy: 0.402500 | 0.966 sec/iter\n",
      "Epoch: 135 | Batch: 009 / 011 | Total loss: 1.727 | Reg loss: 0.040 | Tree loss: 1.727 | Accuracy: 0.427000 | 0.966 sec/iter\n",
      "Epoch: 135 | Batch: 010 / 011 | Total loss: 1.668 | Reg loss: 0.040 | Tree loss: 1.668 | Accuracy: 0.419795 | 0.966 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 136 | Batch: 000 / 011 | Total loss: 1.908 | Reg loss: 0.040 | Tree loss: 1.908 | Accuracy: 0.393000 | 0.966 sec/iter\n",
      "Epoch: 136 | Batch: 001 / 011 | Total loss: 1.895 | Reg loss: 0.040 | Tree loss: 1.895 | Accuracy: 0.373500 | 0.966 sec/iter\n",
      "Epoch: 136 | Batch: 002 / 011 | Total loss: 1.861 | Reg loss: 0.040 | Tree loss: 1.861 | Accuracy: 0.402000 | 0.966 sec/iter\n",
      "Epoch: 136 | Batch: 003 / 011 | Total loss: 1.802 | Reg loss: 0.040 | Tree loss: 1.802 | Accuracy: 0.411500 | 0.966 sec/iter\n",
      "Epoch: 136 | Batch: 004 / 011 | Total loss: 1.828 | Reg loss: 0.040 | Tree loss: 1.828 | Accuracy: 0.411000 | 0.966 sec/iter\n",
      "Epoch: 136 | Batch: 005 / 011 | Total loss: 1.795 | Reg loss: 0.040 | Tree loss: 1.795 | Accuracy: 0.395500 | 0.966 sec/iter\n",
      "Epoch: 136 | Batch: 006 / 011 | Total loss: 1.739 | Reg loss: 0.040 | Tree loss: 1.739 | Accuracy: 0.424000 | 0.966 sec/iter\n",
      "Epoch: 136 | Batch: 007 / 011 | Total loss: 1.730 | Reg loss: 0.040 | Tree loss: 1.730 | Accuracy: 0.429500 | 0.966 sec/iter\n",
      "Epoch: 136 | Batch: 008 / 011 | Total loss: 1.730 | Reg loss: 0.040 | Tree loss: 1.730 | Accuracy: 0.415500 | 0.966 sec/iter\n",
      "Epoch: 136 | Batch: 009 / 011 | Total loss: 1.738 | Reg loss: 0.040 | Tree loss: 1.738 | Accuracy: 0.416500 | 0.966 sec/iter\n",
      "Epoch: 136 | Batch: 010 / 011 | Total loss: 1.692 | Reg loss: 0.040 | Tree loss: 1.692 | Accuracy: 0.430034 | 0.966 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 137 | Batch: 000 / 011 | Total loss: 1.924 | Reg loss: 0.040 | Tree loss: 1.924 | Accuracy: 0.390500 | 0.966 sec/iter\n",
      "Epoch: 137 | Batch: 001 / 011 | Total loss: 1.895 | Reg loss: 0.040 | Tree loss: 1.895 | Accuracy: 0.376000 | 0.966 sec/iter\n",
      "Epoch: 137 | Batch: 002 / 011 | Total loss: 1.863 | Reg loss: 0.040 | Tree loss: 1.863 | Accuracy: 0.401000 | 0.966 sec/iter\n",
      "Epoch: 137 | Batch: 003 / 011 | Total loss: 1.799 | Reg loss: 0.040 | Tree loss: 1.799 | Accuracy: 0.425000 | 0.966 sec/iter\n",
      "Epoch: 137 | Batch: 004 / 011 | Total loss: 1.778 | Reg loss: 0.040 | Tree loss: 1.778 | Accuracy: 0.426500 | 0.966 sec/iter\n",
      "Epoch: 137 | Batch: 005 / 011 | Total loss: 1.776 | Reg loss: 0.040 | Tree loss: 1.776 | Accuracy: 0.433500 | 0.966 sec/iter\n",
      "Epoch: 137 | Batch: 006 / 011 | Total loss: 1.737 | Reg loss: 0.040 | Tree loss: 1.737 | Accuracy: 0.430500 | 0.966 sec/iter\n",
      "Epoch: 137 | Batch: 007 / 011 | Total loss: 1.765 | Reg loss: 0.040 | Tree loss: 1.765 | Accuracy: 0.398500 | 0.966 sec/iter\n",
      "Epoch: 137 | Batch: 008 / 011 | Total loss: 1.726 | Reg loss: 0.040 | Tree loss: 1.726 | Accuracy: 0.429000 | 0.966 sec/iter\n",
      "Epoch: 137 | Batch: 009 / 011 | Total loss: 1.743 | Reg loss: 0.040 | Tree loss: 1.743 | Accuracy: 0.421000 | 0.966 sec/iter\n",
      "Epoch: 137 | Batch: 010 / 011 | Total loss: 1.770 | Reg loss: 0.040 | Tree loss: 1.770 | Accuracy: 0.395904 | 0.966 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 138 | Batch: 000 / 011 | Total loss: 1.901 | Reg loss: 0.040 | Tree loss: 1.901 | Accuracy: 0.374500 | 0.966 sec/iter\n",
      "Epoch: 138 | Batch: 001 / 011 | Total loss: 1.910 | Reg loss: 0.040 | Tree loss: 1.910 | Accuracy: 0.387000 | 0.966 sec/iter\n",
      "Epoch: 138 | Batch: 002 / 011 | Total loss: 1.853 | Reg loss: 0.040 | Tree loss: 1.853 | Accuracy: 0.383500 | 0.966 sec/iter\n",
      "Epoch: 138 | Batch: 003 / 011 | Total loss: 1.846 | Reg loss: 0.040 | Tree loss: 1.846 | Accuracy: 0.382000 | 0.965 sec/iter\n",
      "Epoch: 138 | Batch: 004 / 011 | Total loss: 1.801 | Reg loss: 0.040 | Tree loss: 1.801 | Accuracy: 0.403000 | 0.965 sec/iter\n",
      "Epoch: 138 | Batch: 005 / 011 | Total loss: 1.762 | Reg loss: 0.040 | Tree loss: 1.762 | Accuracy: 0.429500 | 0.965 sec/iter\n",
      "Epoch: 138 | Batch: 006 / 011 | Total loss: 1.741 | Reg loss: 0.040 | Tree loss: 1.741 | Accuracy: 0.428500 | 0.965 sec/iter\n",
      "Epoch: 138 | Batch: 007 / 011 | Total loss: 1.729 | Reg loss: 0.040 | Tree loss: 1.729 | Accuracy: 0.433000 | 0.965 sec/iter\n",
      "Epoch: 138 | Batch: 008 / 011 | Total loss: 1.727 | Reg loss: 0.040 | Tree loss: 1.727 | Accuracy: 0.418000 | 0.965 sec/iter\n",
      "Epoch: 138 | Batch: 009 / 011 | Total loss: 1.754 | Reg loss: 0.040 | Tree loss: 1.754 | Accuracy: 0.417000 | 0.965 sec/iter\n",
      "Epoch: 138 | Batch: 010 / 011 | Total loss: 1.697 | Reg loss: 0.040 | Tree loss: 1.697 | Accuracy: 0.464164 | 0.965 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 139 | Batch: 000 / 011 | Total loss: 1.893 | Reg loss: 0.040 | Tree loss: 1.893 | Accuracy: 0.388500 | 0.965 sec/iter\n",
      "Epoch: 139 | Batch: 001 / 011 | Total loss: 1.865 | Reg loss: 0.040 | Tree loss: 1.865 | Accuracy: 0.378000 | 0.965 sec/iter\n",
      "Epoch: 139 | Batch: 002 / 011 | Total loss: 1.873 | Reg loss: 0.040 | Tree loss: 1.873 | Accuracy: 0.394500 | 0.965 sec/iter\n",
      "Epoch: 139 | Batch: 003 / 011 | Total loss: 1.811 | Reg loss: 0.040 | Tree loss: 1.811 | Accuracy: 0.424000 | 0.965 sec/iter\n",
      "Epoch: 139 | Batch: 004 / 011 | Total loss: 1.774 | Reg loss: 0.040 | Tree loss: 1.774 | Accuracy: 0.435000 | 0.965 sec/iter\n",
      "Epoch: 139 | Batch: 005 / 011 | Total loss: 1.745 | Reg loss: 0.040 | Tree loss: 1.745 | Accuracy: 0.438000 | 0.965 sec/iter\n",
      "Epoch: 139 | Batch: 006 / 011 | Total loss: 1.771 | Reg loss: 0.040 | Tree loss: 1.771 | Accuracy: 0.414500 | 0.965 sec/iter\n",
      "Epoch: 139 | Batch: 007 / 011 | Total loss: 1.756 | Reg loss: 0.040 | Tree loss: 1.756 | Accuracy: 0.406500 | 0.965 sec/iter\n",
      "Epoch: 139 | Batch: 008 / 011 | Total loss: 1.746 | Reg loss: 0.040 | Tree loss: 1.746 | Accuracy: 0.405000 | 0.965 sec/iter\n",
      "Epoch: 139 | Batch: 009 / 011 | Total loss: 1.755 | Reg loss: 0.040 | Tree loss: 1.755 | Accuracy: 0.417000 | 0.965 sec/iter\n",
      "Epoch: 139 | Batch: 010 / 011 | Total loss: 1.750 | Reg loss: 0.040 | Tree loss: 1.750 | Accuracy: 0.399317 | 0.965 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 140 | Batch: 000 / 011 | Total loss: 1.930 | Reg loss: 0.040 | Tree loss: 1.930 | Accuracy: 0.373500 | 0.965 sec/iter\n",
      "Epoch: 140 | Batch: 001 / 011 | Total loss: 1.899 | Reg loss: 0.040 | Tree loss: 1.899 | Accuracy: 0.385500 | 0.965 sec/iter\n",
      "Epoch: 140 | Batch: 002 / 011 | Total loss: 1.856 | Reg loss: 0.040 | Tree loss: 1.856 | Accuracy: 0.374500 | 0.965 sec/iter\n",
      "Epoch: 140 | Batch: 003 / 011 | Total loss: 1.825 | Reg loss: 0.040 | Tree loss: 1.825 | Accuracy: 0.403500 | 0.965 sec/iter\n",
      "Epoch: 140 | Batch: 004 / 011 | Total loss: 1.790 | Reg loss: 0.040 | Tree loss: 1.790 | Accuracy: 0.405000 | 0.965 sec/iter\n",
      "Epoch: 140 | Batch: 005 / 011 | Total loss: 1.758 | Reg loss: 0.040 | Tree loss: 1.758 | Accuracy: 0.429000 | 0.965 sec/iter\n",
      "Epoch: 140 | Batch: 006 / 011 | Total loss: 1.744 | Reg loss: 0.040 | Tree loss: 1.744 | Accuracy: 0.418000 | 0.965 sec/iter\n",
      "Epoch: 140 | Batch: 007 / 011 | Total loss: 1.732 | Reg loss: 0.040 | Tree loss: 1.732 | Accuracy: 0.418500 | 0.965 sec/iter\n",
      "Epoch: 140 | Batch: 008 / 011 | Total loss: 1.737 | Reg loss: 0.040 | Tree loss: 1.737 | Accuracy: 0.421500 | 0.965 sec/iter\n",
      "Epoch: 140 | Batch: 009 / 011 | Total loss: 1.709 | Reg loss: 0.040 | Tree loss: 1.709 | Accuracy: 0.441500 | 0.964 sec/iter\n",
      "Epoch: 140 | Batch: 010 / 011 | Total loss: 1.707 | Reg loss: 0.040 | Tree loss: 1.707 | Accuracy: 0.423208 | 0.964 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 141 | Batch: 000 / 011 | Total loss: 1.908 | Reg loss: 0.040 | Tree loss: 1.908 | Accuracy: 0.392500 | 0.964 sec/iter\n",
      "Epoch: 141 | Batch: 001 / 011 | Total loss: 1.890 | Reg loss: 0.040 | Tree loss: 1.890 | Accuracy: 0.390500 | 0.964 sec/iter\n",
      "Epoch: 141 | Batch: 002 / 011 | Total loss: 1.874 | Reg loss: 0.040 | Tree loss: 1.874 | Accuracy: 0.394500 | 0.964 sec/iter\n",
      "Epoch: 141 | Batch: 003 / 011 | Total loss: 1.800 | Reg loss: 0.040 | Tree loss: 1.800 | Accuracy: 0.407000 | 0.964 sec/iter\n",
      "Epoch: 141 | Batch: 004 / 011 | Total loss: 1.816 | Reg loss: 0.040 | Tree loss: 1.816 | Accuracy: 0.417000 | 0.964 sec/iter\n",
      "Epoch: 141 | Batch: 005 / 011 | Total loss: 1.745 | Reg loss: 0.040 | Tree loss: 1.745 | Accuracy: 0.423500 | 0.964 sec/iter\n",
      "Epoch: 141 | Batch: 006 / 011 | Total loss: 1.765 | Reg loss: 0.040 | Tree loss: 1.765 | Accuracy: 0.423000 | 0.964 sec/iter\n",
      "Epoch: 141 | Batch: 007 / 011 | Total loss: 1.732 | Reg loss: 0.040 | Tree loss: 1.732 | Accuracy: 0.412000 | 0.964 sec/iter\n",
      "Epoch: 141 | Batch: 008 / 011 | Total loss: 1.700 | Reg loss: 0.040 | Tree loss: 1.700 | Accuracy: 0.437000 | 0.964 sec/iter\n",
      "Epoch: 141 | Batch: 009 / 011 | Total loss: 1.754 | Reg loss: 0.040 | Tree loss: 1.754 | Accuracy: 0.403000 | 0.964 sec/iter\n",
      "Epoch: 141 | Batch: 010 / 011 | Total loss: 1.628 | Reg loss: 0.040 | Tree loss: 1.628 | Accuracy: 0.474403 | 0.964 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 142 | Batch: 000 / 011 | Total loss: 1.885 | Reg loss: 0.040 | Tree loss: 1.885 | Accuracy: 0.397500 | 0.964 sec/iter\n",
      "Epoch: 142 | Batch: 001 / 011 | Total loss: 1.884 | Reg loss: 0.040 | Tree loss: 1.884 | Accuracy: 0.379500 | 0.964 sec/iter\n",
      "Epoch: 142 | Batch: 002 / 011 | Total loss: 1.816 | Reg loss: 0.040 | Tree loss: 1.816 | Accuracy: 0.391500 | 0.964 sec/iter\n",
      "Epoch: 142 | Batch: 003 / 011 | Total loss: 1.850 | Reg loss: 0.040 | Tree loss: 1.850 | Accuracy: 0.380500 | 0.964 sec/iter\n",
      "Epoch: 142 | Batch: 004 / 011 | Total loss: 1.796 | Reg loss: 0.040 | Tree loss: 1.796 | Accuracy: 0.410000 | 0.964 sec/iter\n",
      "Epoch: 142 | Batch: 005 / 011 | Total loss: 1.785 | Reg loss: 0.040 | Tree loss: 1.785 | Accuracy: 0.417500 | 0.964 sec/iter\n",
      "Epoch: 142 | Batch: 006 / 011 | Total loss: 1.738 | Reg loss: 0.040 | Tree loss: 1.738 | Accuracy: 0.417000 | 0.964 sec/iter\n",
      "Epoch: 142 | Batch: 007 / 011 | Total loss: 1.743 | Reg loss: 0.040 | Tree loss: 1.743 | Accuracy: 0.424500 | 0.964 sec/iter\n",
      "Epoch: 142 | Batch: 008 / 011 | Total loss: 1.739 | Reg loss: 0.040 | Tree loss: 1.739 | Accuracy: 0.420500 | 0.964 sec/iter\n",
      "Epoch: 142 | Batch: 009 / 011 | Total loss: 1.726 | Reg loss: 0.040 | Tree loss: 1.726 | Accuracy: 0.418000 | 0.964 sec/iter\n",
      "Epoch: 142 | Batch: 010 / 011 | Total loss: 1.730 | Reg loss: 0.041 | Tree loss: 1.730 | Accuracy: 0.430034 | 0.964 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 143 | Batch: 000 / 011 | Total loss: 1.915 | Reg loss: 0.040 | Tree loss: 1.915 | Accuracy: 0.388500 | 0.964 sec/iter\n",
      "Epoch: 143 | Batch: 001 / 011 | Total loss: 1.866 | Reg loss: 0.040 | Tree loss: 1.866 | Accuracy: 0.386500 | 0.964 sec/iter\n",
      "Epoch: 143 | Batch: 002 / 011 | Total loss: 1.854 | Reg loss: 0.040 | Tree loss: 1.854 | Accuracy: 0.394500 | 0.964 sec/iter\n",
      "Epoch: 143 | Batch: 003 / 011 | Total loss: 1.820 | Reg loss: 0.040 | Tree loss: 1.820 | Accuracy: 0.406000 | 0.964 sec/iter\n",
      "Epoch: 143 | Batch: 004 / 011 | Total loss: 1.788 | Reg loss: 0.040 | Tree loss: 1.788 | Accuracy: 0.412000 | 0.964 sec/iter\n",
      "Epoch: 143 | Batch: 005 / 011 | Total loss: 1.762 | Reg loss: 0.040 | Tree loss: 1.762 | Accuracy: 0.423000 | 0.964 sec/iter\n",
      "Epoch: 143 | Batch: 006 / 011 | Total loss: 1.734 | Reg loss: 0.040 | Tree loss: 1.734 | Accuracy: 0.440500 | 0.963 sec/iter\n",
      "Epoch: 143 | Batch: 007 / 011 | Total loss: 1.731 | Reg loss: 0.040 | Tree loss: 1.731 | Accuracy: 0.423500 | 0.963 sec/iter\n",
      "Epoch: 143 | Batch: 008 / 011 | Total loss: 1.753 | Reg loss: 0.041 | Tree loss: 1.753 | Accuracy: 0.400500 | 0.963 sec/iter\n",
      "Epoch: 143 | Batch: 009 / 011 | Total loss: 1.727 | Reg loss: 0.041 | Tree loss: 1.727 | Accuracy: 0.427000 | 0.963 sec/iter\n",
      "Epoch: 143 | Batch: 010 / 011 | Total loss: 1.746 | Reg loss: 0.041 | Tree loss: 1.746 | Accuracy: 0.416382 | 0.963 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 144 | Batch: 000 / 011 | Total loss: 1.908 | Reg loss: 0.040 | Tree loss: 1.908 | Accuracy: 0.383500 | 0.963 sec/iter\n",
      "Epoch: 144 | Batch: 001 / 011 | Total loss: 1.900 | Reg loss: 0.040 | Tree loss: 1.900 | Accuracy: 0.381500 | 0.963 sec/iter\n",
      "Epoch: 144 | Batch: 002 / 011 | Total loss: 1.856 | Reg loss: 0.040 | Tree loss: 1.856 | Accuracy: 0.374000 | 0.963 sec/iter\n",
      "Epoch: 144 | Batch: 003 / 011 | Total loss: 1.803 | Reg loss: 0.040 | Tree loss: 1.803 | Accuracy: 0.410000 | 0.963 sec/iter\n",
      "Epoch: 144 | Batch: 004 / 011 | Total loss: 1.784 | Reg loss: 0.040 | Tree loss: 1.784 | Accuracy: 0.418000 | 0.963 sec/iter\n",
      "Epoch: 144 | Batch: 005 / 011 | Total loss: 1.760 | Reg loss: 0.040 | Tree loss: 1.760 | Accuracy: 0.436500 | 0.963 sec/iter\n",
      "Epoch: 144 | Batch: 006 / 011 | Total loss: 1.753 | Reg loss: 0.040 | Tree loss: 1.753 | Accuracy: 0.418000 | 0.963 sec/iter\n",
      "Epoch: 144 | Batch: 007 / 011 | Total loss: 1.738 | Reg loss: 0.041 | Tree loss: 1.738 | Accuracy: 0.427000 | 0.963 sec/iter\n",
      "Epoch: 144 | Batch: 008 / 011 | Total loss: 1.741 | Reg loss: 0.041 | Tree loss: 1.741 | Accuracy: 0.407500 | 0.963 sec/iter\n",
      "Epoch: 144 | Batch: 009 / 011 | Total loss: 1.709 | Reg loss: 0.041 | Tree loss: 1.709 | Accuracy: 0.437000 | 0.963 sec/iter\n",
      "Epoch: 144 | Batch: 010 / 011 | Total loss: 1.753 | Reg loss: 0.041 | Tree loss: 1.753 | Accuracy: 0.406143 | 0.963 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 145 | Batch: 000 / 011 | Total loss: 1.904 | Reg loss: 0.040 | Tree loss: 1.904 | Accuracy: 0.374000 | 0.963 sec/iter\n",
      "Epoch: 145 | Batch: 001 / 011 | Total loss: 1.888 | Reg loss: 0.040 | Tree loss: 1.888 | Accuracy: 0.390500 | 0.963 sec/iter\n",
      "Epoch: 145 | Batch: 002 / 011 | Total loss: 1.856 | Reg loss: 0.040 | Tree loss: 1.856 | Accuracy: 0.404000 | 0.963 sec/iter\n",
      "Epoch: 145 | Batch: 003 / 011 | Total loss: 1.814 | Reg loss: 0.040 | Tree loss: 1.814 | Accuracy: 0.407500 | 0.963 sec/iter\n",
      "Epoch: 145 | Batch: 004 / 011 | Total loss: 1.771 | Reg loss: 0.040 | Tree loss: 1.771 | Accuracy: 0.426000 | 0.963 sec/iter\n",
      "Epoch: 145 | Batch: 005 / 011 | Total loss: 1.776 | Reg loss: 0.040 | Tree loss: 1.776 | Accuracy: 0.409000 | 0.963 sec/iter\n",
      "Epoch: 145 | Batch: 006 / 011 | Total loss: 1.745 | Reg loss: 0.041 | Tree loss: 1.745 | Accuracy: 0.423500 | 0.963 sec/iter\n",
      "Epoch: 145 | Batch: 007 / 011 | Total loss: 1.740 | Reg loss: 0.041 | Tree loss: 1.740 | Accuracy: 0.419500 | 0.963 sec/iter\n",
      "Epoch: 145 | Batch: 008 / 011 | Total loss: 1.737 | Reg loss: 0.041 | Tree loss: 1.737 | Accuracy: 0.424500 | 0.963 sec/iter\n",
      "Epoch: 145 | Batch: 009 / 011 | Total loss: 1.731 | Reg loss: 0.041 | Tree loss: 1.731 | Accuracy: 0.418000 | 0.963 sec/iter\n",
      "Epoch: 145 | Batch: 010 / 011 | Total loss: 1.665 | Reg loss: 0.041 | Tree loss: 1.665 | Accuracy: 0.443686 | 0.963 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 146 | Batch: 000 / 011 | Total loss: 1.891 | Reg loss: 0.040 | Tree loss: 1.891 | Accuracy: 0.391500 | 0.963 sec/iter\n",
      "Epoch: 146 | Batch: 001 / 011 | Total loss: 1.880 | Reg loss: 0.040 | Tree loss: 1.880 | Accuracy: 0.381000 | 0.963 sec/iter\n",
      "Epoch: 146 | Batch: 002 / 011 | Total loss: 1.846 | Reg loss: 0.040 | Tree loss: 1.846 | Accuracy: 0.392000 | 0.963 sec/iter\n",
      "Epoch: 146 | Batch: 003 / 011 | Total loss: 1.829 | Reg loss: 0.040 | Tree loss: 1.829 | Accuracy: 0.408000 | 0.963 sec/iter\n",
      "Epoch: 146 | Batch: 004 / 011 | Total loss: 1.768 | Reg loss: 0.041 | Tree loss: 1.768 | Accuracy: 0.416500 | 0.962 sec/iter\n",
      "Epoch: 146 | Batch: 005 / 011 | Total loss: 1.769 | Reg loss: 0.041 | Tree loss: 1.769 | Accuracy: 0.398000 | 0.962 sec/iter\n",
      "Epoch: 146 | Batch: 006 / 011 | Total loss: 1.744 | Reg loss: 0.041 | Tree loss: 1.744 | Accuracy: 0.413500 | 0.962 sec/iter\n",
      "Epoch: 146 | Batch: 007 / 011 | Total loss: 1.741 | Reg loss: 0.041 | Tree loss: 1.741 | Accuracy: 0.409500 | 0.962 sec/iter\n",
      "Epoch: 146 | Batch: 008 / 011 | Total loss: 1.740 | Reg loss: 0.041 | Tree loss: 1.740 | Accuracy: 0.417000 | 0.962 sec/iter\n",
      "Epoch: 146 | Batch: 009 / 011 | Total loss: 1.734 | Reg loss: 0.041 | Tree loss: 1.734 | Accuracy: 0.431500 | 0.962 sec/iter\n",
      "Epoch: 146 | Batch: 010 / 011 | Total loss: 1.725 | Reg loss: 0.041 | Tree loss: 1.725 | Accuracy: 0.433447 | 0.962 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 147 | Batch: 000 / 011 | Total loss: 1.904 | Reg loss: 0.040 | Tree loss: 1.904 | Accuracy: 0.367000 | 0.962 sec/iter\n",
      "Epoch: 147 | Batch: 001 / 011 | Total loss: 1.852 | Reg loss: 0.040 | Tree loss: 1.852 | Accuracy: 0.411000 | 0.962 sec/iter\n",
      "Epoch: 147 | Batch: 002 / 011 | Total loss: 1.853 | Reg loss: 0.040 | Tree loss: 1.853 | Accuracy: 0.389500 | 0.962 sec/iter\n",
      "Epoch: 147 | Batch: 003 / 011 | Total loss: 1.809 | Reg loss: 0.041 | Tree loss: 1.809 | Accuracy: 0.423000 | 0.962 sec/iter\n",
      "Epoch: 147 | Batch: 004 / 011 | Total loss: 1.789 | Reg loss: 0.041 | Tree loss: 1.789 | Accuracy: 0.419000 | 0.962 sec/iter\n",
      "Epoch: 147 | Batch: 005 / 011 | Total loss: 1.774 | Reg loss: 0.041 | Tree loss: 1.774 | Accuracy: 0.425500 | 0.962 sec/iter\n",
      "Epoch: 147 | Batch: 006 / 011 | Total loss: 1.773 | Reg loss: 0.041 | Tree loss: 1.773 | Accuracy: 0.412500 | 0.962 sec/iter\n",
      "Epoch: 147 | Batch: 007 / 011 | Total loss: 1.725 | Reg loss: 0.041 | Tree loss: 1.725 | Accuracy: 0.425000 | 0.962 sec/iter\n",
      "Epoch: 147 | Batch: 008 / 011 | Total loss: 1.707 | Reg loss: 0.041 | Tree loss: 1.707 | Accuracy: 0.429000 | 0.962 sec/iter\n",
      "Epoch: 147 | Batch: 009 / 011 | Total loss: 1.734 | Reg loss: 0.041 | Tree loss: 1.734 | Accuracy: 0.414500 | 0.962 sec/iter\n",
      "Epoch: 147 | Batch: 010 / 011 | Total loss: 1.721 | Reg loss: 0.041 | Tree loss: 1.721 | Accuracy: 0.433447 | 0.962 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 148 | Batch: 000 / 011 | Total loss: 1.905 | Reg loss: 0.041 | Tree loss: 1.905 | Accuracy: 0.382000 | 0.962 sec/iter\n",
      "Epoch: 148 | Batch: 001 / 011 | Total loss: 1.879 | Reg loss: 0.041 | Tree loss: 1.879 | Accuracy: 0.387000 | 0.962 sec/iter\n",
      "Epoch: 148 | Batch: 002 / 011 | Total loss: 1.850 | Reg loss: 0.041 | Tree loss: 1.850 | Accuracy: 0.393500 | 0.962 sec/iter\n",
      "Epoch: 148 | Batch: 003 / 011 | Total loss: 1.818 | Reg loss: 0.041 | Tree loss: 1.818 | Accuracy: 0.397000 | 0.962 sec/iter\n",
      "Epoch: 148 | Batch: 004 / 011 | Total loss: 1.774 | Reg loss: 0.041 | Tree loss: 1.774 | Accuracy: 0.424500 | 0.962 sec/iter\n",
      "Epoch: 148 | Batch: 005 / 011 | Total loss: 1.776 | Reg loss: 0.041 | Tree loss: 1.776 | Accuracy: 0.404000 | 0.962 sec/iter\n",
      "Epoch: 148 | Batch: 006 / 011 | Total loss: 1.743 | Reg loss: 0.041 | Tree loss: 1.743 | Accuracy: 0.427000 | 0.962 sec/iter\n",
      "Epoch: 148 | Batch: 007 / 011 | Total loss: 1.729 | Reg loss: 0.041 | Tree loss: 1.729 | Accuracy: 0.416500 | 0.962 sec/iter\n",
      "Epoch: 148 | Batch: 008 / 011 | Total loss: 1.701 | Reg loss: 0.041 | Tree loss: 1.701 | Accuracy: 0.432000 | 0.962 sec/iter\n",
      "Epoch: 148 | Batch: 009 / 011 | Total loss: 1.729 | Reg loss: 0.041 | Tree loss: 1.729 | Accuracy: 0.415000 | 0.962 sec/iter\n",
      "Epoch: 148 | Batch: 010 / 011 | Total loss: 1.716 | Reg loss: 0.041 | Tree loss: 1.716 | Accuracy: 0.433447 | 0.962 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 149 | Batch: 000 / 011 | Total loss: 1.900 | Reg loss: 0.041 | Tree loss: 1.900 | Accuracy: 0.394000 | 0.962 sec/iter\n",
      "Epoch: 149 | Batch: 001 / 011 | Total loss: 1.849 | Reg loss: 0.041 | Tree loss: 1.849 | Accuracy: 0.411500 | 0.962 sec/iter\n",
      "Epoch: 149 | Batch: 002 / 011 | Total loss: 1.854 | Reg loss: 0.041 | Tree loss: 1.854 | Accuracy: 0.398500 | 0.962 sec/iter\n",
      "Epoch: 149 | Batch: 003 / 011 | Total loss: 1.816 | Reg loss: 0.041 | Tree loss: 1.816 | Accuracy: 0.421000 | 0.962 sec/iter\n",
      "Epoch: 149 | Batch: 004 / 011 | Total loss: 1.809 | Reg loss: 0.041 | Tree loss: 1.809 | Accuracy: 0.398500 | 0.962 sec/iter\n",
      "Epoch: 149 | Batch: 005 / 011 | Total loss: 1.750 | Reg loss: 0.041 | Tree loss: 1.750 | Accuracy: 0.425500 | 0.961 sec/iter\n",
      "Epoch: 149 | Batch: 006 / 011 | Total loss: 1.744 | Reg loss: 0.041 | Tree loss: 1.744 | Accuracy: 0.424000 | 0.961 sec/iter\n",
      "Epoch: 149 | Batch: 007 / 011 | Total loss: 1.724 | Reg loss: 0.041 | Tree loss: 1.724 | Accuracy: 0.438500 | 0.961 sec/iter\n",
      "Epoch: 149 | Batch: 008 / 011 | Total loss: 1.728 | Reg loss: 0.041 | Tree loss: 1.728 | Accuracy: 0.422000 | 0.961 sec/iter\n",
      "Epoch: 149 | Batch: 009 / 011 | Total loss: 1.726 | Reg loss: 0.041 | Tree loss: 1.726 | Accuracy: 0.423000 | 0.961 sec/iter\n",
      "Epoch: 149 | Batch: 010 / 011 | Total loss: 1.769 | Reg loss: 0.041 | Tree loss: 1.769 | Accuracy: 0.375427 | 0.961 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 150 | Batch: 000 / 011 | Total loss: 1.901 | Reg loss: 0.041 | Tree loss: 1.901 | Accuracy: 0.375000 | 0.961 sec/iter\n",
      "Epoch: 150 | Batch: 001 / 011 | Total loss: 1.853 | Reg loss: 0.041 | Tree loss: 1.853 | Accuracy: 0.394500 | 0.961 sec/iter\n",
      "Epoch: 150 | Batch: 002 / 011 | Total loss: 1.845 | Reg loss: 0.041 | Tree loss: 1.845 | Accuracy: 0.406500 | 0.961 sec/iter\n",
      "Epoch: 150 | Batch: 003 / 011 | Total loss: 1.837 | Reg loss: 0.041 | Tree loss: 1.837 | Accuracy: 0.378500 | 0.961 sec/iter\n",
      "Epoch: 150 | Batch: 004 / 011 | Total loss: 1.811 | Reg loss: 0.041 | Tree loss: 1.811 | Accuracy: 0.397000 | 0.961 sec/iter\n",
      "Epoch: 150 | Batch: 005 / 011 | Total loss: 1.758 | Reg loss: 0.041 | Tree loss: 1.758 | Accuracy: 0.423500 | 0.961 sec/iter\n",
      "Epoch: 150 | Batch: 006 / 011 | Total loss: 1.738 | Reg loss: 0.041 | Tree loss: 1.738 | Accuracy: 0.425500 | 0.961 sec/iter\n",
      "Epoch: 150 | Batch: 007 / 011 | Total loss: 1.732 | Reg loss: 0.041 | Tree loss: 1.732 | Accuracy: 0.421000 | 0.961 sec/iter\n",
      "Epoch: 150 | Batch: 008 / 011 | Total loss: 1.715 | Reg loss: 0.041 | Tree loss: 1.715 | Accuracy: 0.435000 | 0.961 sec/iter\n",
      "Epoch: 150 | Batch: 009 / 011 | Total loss: 1.734 | Reg loss: 0.041 | Tree loss: 1.734 | Accuracy: 0.417500 | 0.961 sec/iter\n",
      "Epoch: 150 | Batch: 010 / 011 | Total loss: 1.707 | Reg loss: 0.041 | Tree loss: 1.707 | Accuracy: 0.443686 | 0.961 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 151 | Batch: 000 / 011 | Total loss: 1.929 | Reg loss: 0.041 | Tree loss: 1.929 | Accuracy: 0.386500 | 0.961 sec/iter\n",
      "Epoch: 151 | Batch: 001 / 011 | Total loss: 1.871 | Reg loss: 0.041 | Tree loss: 1.871 | Accuracy: 0.415500 | 0.961 sec/iter\n",
      "Epoch: 151 | Batch: 002 / 011 | Total loss: 1.837 | Reg loss: 0.041 | Tree loss: 1.837 | Accuracy: 0.399000 | 0.961 sec/iter\n",
      "Epoch: 151 | Batch: 003 / 011 | Total loss: 1.777 | Reg loss: 0.041 | Tree loss: 1.777 | Accuracy: 0.418500 | 0.961 sec/iter\n",
      "Epoch: 151 | Batch: 004 / 011 | Total loss: 1.804 | Reg loss: 0.041 | Tree loss: 1.804 | Accuracy: 0.403500 | 0.961 sec/iter\n",
      "Epoch: 151 | Batch: 005 / 011 | Total loss: 1.789 | Reg loss: 0.041 | Tree loss: 1.789 | Accuracy: 0.418000 | 0.961 sec/iter\n",
      "Epoch: 151 | Batch: 006 / 011 | Total loss: 1.753 | Reg loss: 0.041 | Tree loss: 1.753 | Accuracy: 0.416500 | 0.961 sec/iter\n",
      "Epoch: 151 | Batch: 007 / 011 | Total loss: 1.752 | Reg loss: 0.041 | Tree loss: 1.752 | Accuracy: 0.416000 | 0.961 sec/iter\n",
      "Epoch: 151 | Batch: 008 / 011 | Total loss: 1.706 | Reg loss: 0.041 | Tree loss: 1.706 | Accuracy: 0.442500 | 0.961 sec/iter\n",
      "Epoch: 151 | Batch: 009 / 011 | Total loss: 1.692 | Reg loss: 0.041 | Tree loss: 1.692 | Accuracy: 0.424000 | 0.961 sec/iter\n",
      "Epoch: 151 | Batch: 010 / 011 | Total loss: 1.638 | Reg loss: 0.041 | Tree loss: 1.638 | Accuracy: 0.464164 | 0.961 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 152 | Batch: 000 / 011 | Total loss: 1.884 | Reg loss: 0.041 | Tree loss: 1.884 | Accuracy: 0.410500 | 0.961 sec/iter\n",
      "Epoch: 152 | Batch: 001 / 011 | Total loss: 1.878 | Reg loss: 0.041 | Tree loss: 1.878 | Accuracy: 0.392500 | 0.961 sec/iter\n",
      "Epoch: 152 | Batch: 002 / 011 | Total loss: 1.848 | Reg loss: 0.041 | Tree loss: 1.848 | Accuracy: 0.384500 | 0.961 sec/iter\n",
      "Epoch: 152 | Batch: 003 / 011 | Total loss: 1.825 | Reg loss: 0.041 | Tree loss: 1.825 | Accuracy: 0.374500 | 0.961 sec/iter\n",
      "Epoch: 152 | Batch: 004 / 011 | Total loss: 1.771 | Reg loss: 0.041 | Tree loss: 1.771 | Accuracy: 0.411500 | 0.961 sec/iter\n",
      "Epoch: 152 | Batch: 005 / 011 | Total loss: 1.757 | Reg loss: 0.041 | Tree loss: 1.757 | Accuracy: 0.414500 | 0.961 sec/iter\n",
      "Epoch: 152 | Batch: 006 / 011 | Total loss: 1.740 | Reg loss: 0.041 | Tree loss: 1.740 | Accuracy: 0.418500 | 0.961 sec/iter\n",
      "Epoch: 152 | Batch: 007 / 011 | Total loss: 1.736 | Reg loss: 0.041 | Tree loss: 1.736 | Accuracy: 0.426500 | 0.96 sec/iter\n",
      "Epoch: 152 | Batch: 008 / 011 | Total loss: 1.746 | Reg loss: 0.041 | Tree loss: 1.746 | Accuracy: 0.420000 | 0.96 sec/iter\n",
      "Epoch: 152 | Batch: 009 / 011 | Total loss: 1.722 | Reg loss: 0.041 | Tree loss: 1.722 | Accuracy: 0.435500 | 0.96 sec/iter\n",
      "Epoch: 152 | Batch: 010 / 011 | Total loss: 1.755 | Reg loss: 0.041 | Tree loss: 1.755 | Accuracy: 0.412969 | 0.96 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 153 | Batch: 000 / 011 | Total loss: 1.915 | Reg loss: 0.041 | Tree loss: 1.915 | Accuracy: 0.399000 | 0.96 sec/iter\n",
      "Epoch: 153 | Batch: 001 / 011 | Total loss: 1.874 | Reg loss: 0.041 | Tree loss: 1.874 | Accuracy: 0.400500 | 0.96 sec/iter\n",
      "Epoch: 153 | Batch: 002 / 011 | Total loss: 1.826 | Reg loss: 0.041 | Tree loss: 1.826 | Accuracy: 0.406500 | 0.96 sec/iter\n",
      "Epoch: 153 | Batch: 003 / 011 | Total loss: 1.827 | Reg loss: 0.041 | Tree loss: 1.827 | Accuracy: 0.395500 | 0.96 sec/iter\n",
      "Epoch: 153 | Batch: 004 / 011 | Total loss: 1.788 | Reg loss: 0.041 | Tree loss: 1.788 | Accuracy: 0.434500 | 0.96 sec/iter\n",
      "Epoch: 153 | Batch: 005 / 011 | Total loss: 1.758 | Reg loss: 0.041 | Tree loss: 1.758 | Accuracy: 0.425500 | 0.96 sec/iter\n",
      "Epoch: 153 | Batch: 006 / 011 | Total loss: 1.727 | Reg loss: 0.041 | Tree loss: 1.727 | Accuracy: 0.428500 | 0.96 sec/iter\n",
      "Epoch: 153 | Batch: 007 / 011 | Total loss: 1.710 | Reg loss: 0.041 | Tree loss: 1.710 | Accuracy: 0.439500 | 0.96 sec/iter\n",
      "Epoch: 153 | Batch: 008 / 011 | Total loss: 1.733 | Reg loss: 0.041 | Tree loss: 1.733 | Accuracy: 0.416500 | 0.96 sec/iter\n",
      "Epoch: 153 | Batch: 009 / 011 | Total loss: 1.732 | Reg loss: 0.041 | Tree loss: 1.732 | Accuracy: 0.406000 | 0.96 sec/iter\n",
      "Epoch: 153 | Batch: 010 / 011 | Total loss: 1.728 | Reg loss: 0.041 | Tree loss: 1.728 | Accuracy: 0.426621 | 0.96 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 154 | Batch: 000 / 011 | Total loss: 1.890 | Reg loss: 0.041 | Tree loss: 1.890 | Accuracy: 0.412500 | 0.96 sec/iter\n",
      "Epoch: 154 | Batch: 001 / 011 | Total loss: 1.878 | Reg loss: 0.041 | Tree loss: 1.878 | Accuracy: 0.391000 | 0.96 sec/iter\n",
      "Epoch: 154 | Batch: 002 / 011 | Total loss: 1.842 | Reg loss: 0.041 | Tree loss: 1.842 | Accuracy: 0.394500 | 0.96 sec/iter\n",
      "Epoch: 154 | Batch: 003 / 011 | Total loss: 1.803 | Reg loss: 0.041 | Tree loss: 1.803 | Accuracy: 0.406000 | 0.96 sec/iter\n",
      "Epoch: 154 | Batch: 004 / 011 | Total loss: 1.793 | Reg loss: 0.041 | Tree loss: 1.793 | Accuracy: 0.417500 | 0.96 sec/iter\n",
      "Epoch: 154 | Batch: 005 / 011 | Total loss: 1.771 | Reg loss: 0.041 | Tree loss: 1.771 | Accuracy: 0.389000 | 0.96 sec/iter\n",
      "Epoch: 154 | Batch: 006 / 011 | Total loss: 1.722 | Reg loss: 0.041 | Tree loss: 1.722 | Accuracy: 0.422500 | 0.96 sec/iter\n",
      "Epoch: 154 | Batch: 007 / 011 | Total loss: 1.727 | Reg loss: 0.041 | Tree loss: 1.727 | Accuracy: 0.434500 | 0.96 sec/iter\n",
      "Epoch: 154 | Batch: 008 / 011 | Total loss: 1.731 | Reg loss: 0.041 | Tree loss: 1.731 | Accuracy: 0.427000 | 0.96 sec/iter\n",
      "Epoch: 154 | Batch: 009 / 011 | Total loss: 1.720 | Reg loss: 0.041 | Tree loss: 1.720 | Accuracy: 0.421500 | 0.96 sec/iter\n",
      "Epoch: 154 | Batch: 010 / 011 | Total loss: 1.795 | Reg loss: 0.041 | Tree loss: 1.795 | Accuracy: 0.365188 | 0.96 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 155 | Batch: 000 / 011 | Total loss: 1.884 | Reg loss: 0.041 | Tree loss: 1.884 | Accuracy: 0.388500 | 0.96 sec/iter\n",
      "Epoch: 155 | Batch: 001 / 011 | Total loss: 1.863 | Reg loss: 0.041 | Tree loss: 1.863 | Accuracy: 0.402500 | 0.96 sec/iter\n",
      "Epoch: 155 | Batch: 002 / 011 | Total loss: 1.837 | Reg loss: 0.041 | Tree loss: 1.837 | Accuracy: 0.400500 | 0.96 sec/iter\n",
      "Epoch: 155 | Batch: 003 / 011 | Total loss: 1.812 | Reg loss: 0.041 | Tree loss: 1.812 | Accuracy: 0.412000 | 0.96 sec/iter\n",
      "Epoch: 155 | Batch: 004 / 011 | Total loss: 1.790 | Reg loss: 0.041 | Tree loss: 1.790 | Accuracy: 0.405000 | 0.96 sec/iter\n",
      "Epoch: 155 | Batch: 005 / 011 | Total loss: 1.748 | Reg loss: 0.041 | Tree loss: 1.748 | Accuracy: 0.435500 | 0.96 sec/iter\n",
      "Epoch: 155 | Batch: 006 / 011 | Total loss: 1.758 | Reg loss: 0.041 | Tree loss: 1.758 | Accuracy: 0.417500 | 0.96 sec/iter\n",
      "Epoch: 155 | Batch: 007 / 011 | Total loss: 1.732 | Reg loss: 0.041 | Tree loss: 1.732 | Accuracy: 0.410000 | 0.96 sec/iter\n",
      "Epoch: 155 | Batch: 008 / 011 | Total loss: 1.729 | Reg loss: 0.041 | Tree loss: 1.729 | Accuracy: 0.411000 | 0.96 sec/iter\n",
      "Epoch: 155 | Batch: 009 / 011 | Total loss: 1.707 | Reg loss: 0.041 | Tree loss: 1.707 | Accuracy: 0.440000 | 0.96 sec/iter\n",
      "Epoch: 155 | Batch: 010 / 011 | Total loss: 1.726 | Reg loss: 0.041 | Tree loss: 1.726 | Accuracy: 0.423208 | 0.959 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 156 | Batch: 000 / 011 | Total loss: 1.903 | Reg loss: 0.041 | Tree loss: 1.903 | Accuracy: 0.380000 | 0.96 sec/iter\n",
      "Epoch: 156 | Batch: 001 / 011 | Total loss: 1.857 | Reg loss: 0.041 | Tree loss: 1.857 | Accuracy: 0.398000 | 0.959 sec/iter\n",
      "Epoch: 156 | Batch: 002 / 011 | Total loss: 1.868 | Reg loss: 0.041 | Tree loss: 1.868 | Accuracy: 0.392500 | 0.959 sec/iter\n",
      "Epoch: 156 | Batch: 003 / 011 | Total loss: 1.808 | Reg loss: 0.041 | Tree loss: 1.808 | Accuracy: 0.403000 | 0.959 sec/iter\n",
      "Epoch: 156 | Batch: 004 / 011 | Total loss: 1.804 | Reg loss: 0.041 | Tree loss: 1.804 | Accuracy: 0.418500 | 0.959 sec/iter\n",
      "Epoch: 156 | Batch: 005 / 011 | Total loss: 1.763 | Reg loss: 0.041 | Tree loss: 1.763 | Accuracy: 0.412000 | 0.959 sec/iter\n",
      "Epoch: 156 | Batch: 006 / 011 | Total loss: 1.715 | Reg loss: 0.041 | Tree loss: 1.715 | Accuracy: 0.436000 | 0.959 sec/iter\n",
      "Epoch: 156 | Batch: 007 / 011 | Total loss: 1.710 | Reg loss: 0.041 | Tree loss: 1.710 | Accuracy: 0.413000 | 0.959 sec/iter\n",
      "Epoch: 156 | Batch: 008 / 011 | Total loss: 1.722 | Reg loss: 0.041 | Tree loss: 1.722 | Accuracy: 0.415500 | 0.959 sec/iter\n",
      "Epoch: 156 | Batch: 009 / 011 | Total loss: 1.714 | Reg loss: 0.041 | Tree loss: 1.714 | Accuracy: 0.442500 | 0.959 sec/iter\n",
      "Epoch: 156 | Batch: 010 / 011 | Total loss: 1.691 | Reg loss: 0.041 | Tree loss: 1.691 | Accuracy: 0.447099 | 0.959 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 157 | Batch: 000 / 011 | Total loss: 1.894 | Reg loss: 0.041 | Tree loss: 1.894 | Accuracy: 0.383500 | 0.959 sec/iter\n",
      "Epoch: 157 | Batch: 001 / 011 | Total loss: 1.847 | Reg loss: 0.041 | Tree loss: 1.847 | Accuracy: 0.396500 | 0.959 sec/iter\n",
      "Epoch: 157 | Batch: 002 / 011 | Total loss: 1.829 | Reg loss: 0.041 | Tree loss: 1.829 | Accuracy: 0.403500 | 0.959 sec/iter\n",
      "Epoch: 157 | Batch: 003 / 011 | Total loss: 1.826 | Reg loss: 0.041 | Tree loss: 1.826 | Accuracy: 0.401000 | 0.959 sec/iter\n",
      "Epoch: 157 | Batch: 004 / 011 | Total loss: 1.813 | Reg loss: 0.041 | Tree loss: 1.813 | Accuracy: 0.401500 | 0.959 sec/iter\n",
      "Epoch: 157 | Batch: 005 / 011 | Total loss: 1.778 | Reg loss: 0.041 | Tree loss: 1.778 | Accuracy: 0.415000 | 0.959 sec/iter\n",
      "Epoch: 157 | Batch: 006 / 011 | Total loss: 1.726 | Reg loss: 0.041 | Tree loss: 1.726 | Accuracy: 0.421500 | 0.959 sec/iter\n",
      "Epoch: 157 | Batch: 007 / 011 | Total loss: 1.740 | Reg loss: 0.041 | Tree loss: 1.740 | Accuracy: 0.405000 | 0.959 sec/iter\n",
      "Epoch: 157 | Batch: 008 / 011 | Total loss: 1.728 | Reg loss: 0.041 | Tree loss: 1.728 | Accuracy: 0.413000 | 0.959 sec/iter\n",
      "Epoch: 157 | Batch: 009 / 011 | Total loss: 1.692 | Reg loss: 0.041 | Tree loss: 1.692 | Accuracy: 0.445000 | 0.959 sec/iter\n",
      "Epoch: 157 | Batch: 010 / 011 | Total loss: 1.696 | Reg loss: 0.041 | Tree loss: 1.696 | Accuracy: 0.457338 | 0.959 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 158 | Batch: 000 / 011 | Total loss: 1.874 | Reg loss: 0.041 | Tree loss: 1.874 | Accuracy: 0.392000 | 0.959 sec/iter\n",
      "Epoch: 158 | Batch: 001 / 011 | Total loss: 1.898 | Reg loss: 0.041 | Tree loss: 1.898 | Accuracy: 0.373500 | 0.959 sec/iter\n",
      "Epoch: 158 | Batch: 002 / 011 | Total loss: 1.850 | Reg loss: 0.041 | Tree loss: 1.850 | Accuracy: 0.409000 | 0.959 sec/iter\n",
      "Epoch: 158 | Batch: 003 / 011 | Total loss: 1.799 | Reg loss: 0.041 | Tree loss: 1.799 | Accuracy: 0.435000 | 0.959 sec/iter\n",
      "Epoch: 158 | Batch: 004 / 011 | Total loss: 1.785 | Reg loss: 0.041 | Tree loss: 1.785 | Accuracy: 0.413500 | 0.959 sec/iter\n",
      "Epoch: 158 | Batch: 005 / 011 | Total loss: 1.759 | Reg loss: 0.041 | Tree loss: 1.759 | Accuracy: 0.421500 | 0.959 sec/iter\n",
      "Epoch: 158 | Batch: 006 / 011 | Total loss: 1.740 | Reg loss: 0.041 | Tree loss: 1.740 | Accuracy: 0.423000 | 0.959 sec/iter\n",
      "Epoch: 158 | Batch: 007 / 011 | Total loss: 1.722 | Reg loss: 0.041 | Tree loss: 1.722 | Accuracy: 0.417500 | 0.959 sec/iter\n",
      "Epoch: 158 | Batch: 008 / 011 | Total loss: 1.714 | Reg loss: 0.041 | Tree loss: 1.714 | Accuracy: 0.422000 | 0.959 sec/iter\n",
      "Epoch: 158 | Batch: 009 / 011 | Total loss: 1.724 | Reg loss: 0.041 | Tree loss: 1.724 | Accuracy: 0.422500 | 0.959 sec/iter\n",
      "Epoch: 158 | Batch: 010 / 011 | Total loss: 1.671 | Reg loss: 0.041 | Tree loss: 1.671 | Accuracy: 0.433447 | 0.959 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 159 | Batch: 000 / 011 | Total loss: 1.899 | Reg loss: 0.041 | Tree loss: 1.899 | Accuracy: 0.386500 | 0.959 sec/iter\n",
      "Epoch: 159 | Batch: 001 / 011 | Total loss: 1.887 | Reg loss: 0.041 | Tree loss: 1.887 | Accuracy: 0.389000 | 0.959 sec/iter\n",
      "Epoch: 159 | Batch: 002 / 011 | Total loss: 1.823 | Reg loss: 0.041 | Tree loss: 1.823 | Accuracy: 0.414000 | 0.959 sec/iter\n",
      "Epoch: 159 | Batch: 003 / 011 | Total loss: 1.806 | Reg loss: 0.041 | Tree loss: 1.806 | Accuracy: 0.409000 | 0.959 sec/iter\n",
      "Epoch: 159 | Batch: 004 / 011 | Total loss: 1.753 | Reg loss: 0.041 | Tree loss: 1.753 | Accuracy: 0.433500 | 0.958 sec/iter\n",
      "Epoch: 159 | Batch: 005 / 011 | Total loss: 1.782 | Reg loss: 0.041 | Tree loss: 1.782 | Accuracy: 0.408500 | 0.958 sec/iter\n",
      "Epoch: 159 | Batch: 006 / 011 | Total loss: 1.732 | Reg loss: 0.041 | Tree loss: 1.732 | Accuracy: 0.428500 | 0.958 sec/iter\n",
      "Epoch: 159 | Batch: 007 / 011 | Total loss: 1.727 | Reg loss: 0.041 | Tree loss: 1.727 | Accuracy: 0.425000 | 0.958 sec/iter\n",
      "Epoch: 159 | Batch: 008 / 011 | Total loss: 1.735 | Reg loss: 0.041 | Tree loss: 1.735 | Accuracy: 0.410000 | 0.958 sec/iter\n",
      "Epoch: 159 | Batch: 009 / 011 | Total loss: 1.693 | Reg loss: 0.041 | Tree loss: 1.693 | Accuracy: 0.438500 | 0.958 sec/iter\n",
      "Epoch: 159 | Batch: 010 / 011 | Total loss: 1.740 | Reg loss: 0.041 | Tree loss: 1.740 | Accuracy: 0.436860 | 0.958 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 160 | Batch: 000 / 011 | Total loss: 1.875 | Reg loss: 0.041 | Tree loss: 1.875 | Accuracy: 0.400000 | 0.958 sec/iter\n",
      "Epoch: 160 | Batch: 001 / 011 | Total loss: 1.863 | Reg loss: 0.041 | Tree loss: 1.863 | Accuracy: 0.403500 | 0.958 sec/iter\n",
      "Epoch: 160 | Batch: 002 / 011 | Total loss: 1.813 | Reg loss: 0.041 | Tree loss: 1.813 | Accuracy: 0.416500 | 0.958 sec/iter\n",
      "Epoch: 160 | Batch: 003 / 011 | Total loss: 1.814 | Reg loss: 0.041 | Tree loss: 1.814 | Accuracy: 0.402500 | 0.958 sec/iter\n",
      "Epoch: 160 | Batch: 004 / 011 | Total loss: 1.807 | Reg loss: 0.041 | Tree loss: 1.807 | Accuracy: 0.394500 | 0.958 sec/iter\n",
      "Epoch: 160 | Batch: 005 / 011 | Total loss: 1.751 | Reg loss: 0.041 | Tree loss: 1.751 | Accuracy: 0.441000 | 0.958 sec/iter\n",
      "Epoch: 160 | Batch: 006 / 011 | Total loss: 1.744 | Reg loss: 0.041 | Tree loss: 1.744 | Accuracy: 0.412000 | 0.958 sec/iter\n",
      "Epoch: 160 | Batch: 007 / 011 | Total loss: 1.715 | Reg loss: 0.041 | Tree loss: 1.715 | Accuracy: 0.408500 | 0.958 sec/iter\n",
      "Epoch: 160 | Batch: 008 / 011 | Total loss: 1.723 | Reg loss: 0.041 | Tree loss: 1.723 | Accuracy: 0.422000 | 0.958 sec/iter\n",
      "Epoch: 160 | Batch: 009 / 011 | Total loss: 1.734 | Reg loss: 0.041 | Tree loss: 1.734 | Accuracy: 0.417000 | 0.958 sec/iter\n",
      "Epoch: 160 | Batch: 010 / 011 | Total loss: 1.726 | Reg loss: 0.041 | Tree loss: 1.726 | Accuracy: 0.406143 | 0.958 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 161 | Batch: 000 / 011 | Total loss: 1.912 | Reg loss: 0.041 | Tree loss: 1.912 | Accuracy: 0.383000 | 0.958 sec/iter\n",
      "Epoch: 161 | Batch: 001 / 011 | Total loss: 1.874 | Reg loss: 0.041 | Tree loss: 1.874 | Accuracy: 0.389500 | 0.958 sec/iter\n",
      "Epoch: 161 | Batch: 002 / 011 | Total loss: 1.803 | Reg loss: 0.041 | Tree loss: 1.803 | Accuracy: 0.412000 | 0.958 sec/iter\n",
      "Epoch: 161 | Batch: 003 / 011 | Total loss: 1.819 | Reg loss: 0.041 | Tree loss: 1.819 | Accuracy: 0.405000 | 0.958 sec/iter\n",
      "Epoch: 161 | Batch: 004 / 011 | Total loss: 1.786 | Reg loss: 0.041 | Tree loss: 1.786 | Accuracy: 0.407500 | 0.958 sec/iter\n",
      "Epoch: 161 | Batch: 005 / 011 | Total loss: 1.758 | Reg loss: 0.041 | Tree loss: 1.758 | Accuracy: 0.424500 | 0.958 sec/iter\n",
      "Epoch: 161 | Batch: 006 / 011 | Total loss: 1.739 | Reg loss: 0.041 | Tree loss: 1.739 | Accuracy: 0.403500 | 0.958 sec/iter\n",
      "Epoch: 161 | Batch: 007 / 011 | Total loss: 1.725 | Reg loss: 0.041 | Tree loss: 1.725 | Accuracy: 0.432500 | 0.958 sec/iter\n",
      "Epoch: 161 | Batch: 008 / 011 | Total loss: 1.724 | Reg loss: 0.041 | Tree loss: 1.724 | Accuracy: 0.425500 | 0.958 sec/iter\n",
      "Epoch: 161 | Batch: 009 / 011 | Total loss: 1.701 | Reg loss: 0.041 | Tree loss: 1.701 | Accuracy: 0.429500 | 0.958 sec/iter\n",
      "Epoch: 161 | Batch: 010 / 011 | Total loss: 1.663 | Reg loss: 0.041 | Tree loss: 1.663 | Accuracy: 0.453925 | 0.958 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 162 | Batch: 000 / 011 | Total loss: 1.883 | Reg loss: 0.041 | Tree loss: 1.883 | Accuracy: 0.381000 | 0.958 sec/iter\n",
      "Epoch: 162 | Batch: 001 / 011 | Total loss: 1.872 | Reg loss: 0.041 | Tree loss: 1.872 | Accuracy: 0.380000 | 0.958 sec/iter\n",
      "Epoch: 162 | Batch: 002 / 011 | Total loss: 1.812 | Reg loss: 0.041 | Tree loss: 1.812 | Accuracy: 0.421500 | 0.958 sec/iter\n",
      "Epoch: 162 | Batch: 003 / 011 | Total loss: 1.804 | Reg loss: 0.041 | Tree loss: 1.804 | Accuracy: 0.421500 | 0.958 sec/iter\n",
      "Epoch: 162 | Batch: 004 / 011 | Total loss: 1.791 | Reg loss: 0.041 | Tree loss: 1.791 | Accuracy: 0.400500 | 0.958 sec/iter\n",
      "Epoch: 162 | Batch: 005 / 011 | Total loss: 1.745 | Reg loss: 0.041 | Tree loss: 1.745 | Accuracy: 0.424000 | 0.958 sec/iter\n",
      "Epoch: 162 | Batch: 006 / 011 | Total loss: 1.771 | Reg loss: 0.041 | Tree loss: 1.771 | Accuracy: 0.402000 | 0.958 sec/iter\n",
      "Epoch: 162 | Batch: 007 / 011 | Total loss: 1.716 | Reg loss: 0.041 | Tree loss: 1.716 | Accuracy: 0.413500 | 0.958 sec/iter\n",
      "Epoch: 162 | Batch: 008 / 011 | Total loss: 1.717 | Reg loss: 0.041 | Tree loss: 1.717 | Accuracy: 0.415500 | 0.958 sec/iter\n",
      "Epoch: 162 | Batch: 009 / 011 | Total loss: 1.724 | Reg loss: 0.041 | Tree loss: 1.724 | Accuracy: 0.425500 | 0.957 sec/iter\n",
      "Epoch: 162 | Batch: 010 / 011 | Total loss: 1.673 | Reg loss: 0.041 | Tree loss: 1.673 | Accuracy: 0.470990 | 0.957 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 163 | Batch: 000 / 011 | Total loss: 1.889 | Reg loss: 0.041 | Tree loss: 1.889 | Accuracy: 0.398000 | 0.957 sec/iter\n",
      "Epoch: 163 | Batch: 001 / 011 | Total loss: 1.877 | Reg loss: 0.041 | Tree loss: 1.877 | Accuracy: 0.394000 | 0.957 sec/iter\n",
      "Epoch: 163 | Batch: 002 / 011 | Total loss: 1.836 | Reg loss: 0.041 | Tree loss: 1.836 | Accuracy: 0.393500 | 0.957 sec/iter\n",
      "Epoch: 163 | Batch: 003 / 011 | Total loss: 1.781 | Reg loss: 0.041 | Tree loss: 1.781 | Accuracy: 0.427000 | 0.957 sec/iter\n",
      "Epoch: 163 | Batch: 004 / 011 | Total loss: 1.775 | Reg loss: 0.041 | Tree loss: 1.775 | Accuracy: 0.405000 | 0.957 sec/iter\n",
      "Epoch: 163 | Batch: 005 / 011 | Total loss: 1.723 | Reg loss: 0.041 | Tree loss: 1.723 | Accuracy: 0.427500 | 0.957 sec/iter\n",
      "Epoch: 163 | Batch: 006 / 011 | Total loss: 1.745 | Reg loss: 0.041 | Tree loss: 1.745 | Accuracy: 0.421500 | 0.957 sec/iter\n",
      "Epoch: 163 | Batch: 007 / 011 | Total loss: 1.758 | Reg loss: 0.041 | Tree loss: 1.758 | Accuracy: 0.393500 | 0.957 sec/iter\n",
      "Epoch: 163 | Batch: 008 / 011 | Total loss: 1.716 | Reg loss: 0.041 | Tree loss: 1.716 | Accuracy: 0.437500 | 0.957 sec/iter\n",
      "Epoch: 163 | Batch: 009 / 011 | Total loss: 1.718 | Reg loss: 0.041 | Tree loss: 1.718 | Accuracy: 0.421500 | 0.957 sec/iter\n",
      "Epoch: 163 | Batch: 010 / 011 | Total loss: 1.645 | Reg loss: 0.041 | Tree loss: 1.645 | Accuracy: 0.433447 | 0.957 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 164 | Batch: 000 / 011 | Total loss: 1.876 | Reg loss: 0.041 | Tree loss: 1.876 | Accuracy: 0.397500 | 0.957 sec/iter\n",
      "Epoch: 164 | Batch: 001 / 011 | Total loss: 1.852 | Reg loss: 0.041 | Tree loss: 1.852 | Accuracy: 0.403000 | 0.957 sec/iter\n",
      "Epoch: 164 | Batch: 002 / 011 | Total loss: 1.834 | Reg loss: 0.041 | Tree loss: 1.834 | Accuracy: 0.390000 | 0.957 sec/iter\n",
      "Epoch: 164 | Batch: 003 / 011 | Total loss: 1.814 | Reg loss: 0.041 | Tree loss: 1.814 | Accuracy: 0.404000 | 0.957 sec/iter\n",
      "Epoch: 164 | Batch: 004 / 011 | Total loss: 1.787 | Reg loss: 0.041 | Tree loss: 1.787 | Accuracy: 0.413000 | 0.957 sec/iter\n",
      "Epoch: 164 | Batch: 005 / 011 | Total loss: 1.765 | Reg loss: 0.041 | Tree loss: 1.765 | Accuracy: 0.407500 | 0.957 sec/iter\n",
      "Epoch: 164 | Batch: 006 / 011 | Total loss: 1.743 | Reg loss: 0.041 | Tree loss: 1.743 | Accuracy: 0.409500 | 0.957 sec/iter\n",
      "Epoch: 164 | Batch: 007 / 011 | Total loss: 1.729 | Reg loss: 0.041 | Tree loss: 1.729 | Accuracy: 0.414000 | 0.957 sec/iter\n",
      "Epoch: 164 | Batch: 008 / 011 | Total loss: 1.709 | Reg loss: 0.041 | Tree loss: 1.709 | Accuracy: 0.418500 | 0.957 sec/iter\n",
      "Epoch: 164 | Batch: 009 / 011 | Total loss: 1.726 | Reg loss: 0.041 | Tree loss: 1.726 | Accuracy: 0.425000 | 0.957 sec/iter\n",
      "Epoch: 164 | Batch: 010 / 011 | Total loss: 1.682 | Reg loss: 0.041 | Tree loss: 1.682 | Accuracy: 0.426621 | 0.957 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 165 | Batch: 000 / 011 | Total loss: 1.853 | Reg loss: 0.041 | Tree loss: 1.853 | Accuracy: 0.399000 | 0.957 sec/iter\n",
      "Epoch: 165 | Batch: 001 / 011 | Total loss: 1.864 | Reg loss: 0.041 | Tree loss: 1.864 | Accuracy: 0.400500 | 0.957 sec/iter\n",
      "Epoch: 165 | Batch: 002 / 011 | Total loss: 1.840 | Reg loss: 0.041 | Tree loss: 1.840 | Accuracy: 0.383500 | 0.957 sec/iter\n",
      "Epoch: 165 | Batch: 003 / 011 | Total loss: 1.796 | Reg loss: 0.041 | Tree loss: 1.796 | Accuracy: 0.410500 | 0.957 sec/iter\n",
      "Epoch: 165 | Batch: 004 / 011 | Total loss: 1.780 | Reg loss: 0.041 | Tree loss: 1.780 | Accuracy: 0.421000 | 0.957 sec/iter\n",
      "Epoch: 165 | Batch: 005 / 011 | Total loss: 1.753 | Reg loss: 0.041 | Tree loss: 1.753 | Accuracy: 0.421000 | 0.957 sec/iter\n",
      "Epoch: 165 | Batch: 006 / 011 | Total loss: 1.745 | Reg loss: 0.041 | Tree loss: 1.745 | Accuracy: 0.409500 | 0.957 sec/iter\n",
      "Epoch: 165 | Batch: 007 / 011 | Total loss: 1.710 | Reg loss: 0.041 | Tree loss: 1.710 | Accuracy: 0.425000 | 0.957 sec/iter\n",
      "Epoch: 165 | Batch: 008 / 011 | Total loss: 1.715 | Reg loss: 0.041 | Tree loss: 1.715 | Accuracy: 0.426500 | 0.957 sec/iter\n",
      "Epoch: 165 | Batch: 009 / 011 | Total loss: 1.732 | Reg loss: 0.041 | Tree loss: 1.732 | Accuracy: 0.428500 | 0.957 sec/iter\n",
      "Epoch: 165 | Batch: 010 / 011 | Total loss: 1.787 | Reg loss: 0.041 | Tree loss: 1.787 | Accuracy: 0.385666 | 0.957 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 166 | Batch: 000 / 011 | Total loss: 1.918 | Reg loss: 0.041 | Tree loss: 1.918 | Accuracy: 0.383500 | 0.957 sec/iter\n",
      "Epoch: 166 | Batch: 001 / 011 | Total loss: 1.870 | Reg loss: 0.041 | Tree loss: 1.870 | Accuracy: 0.391500 | 0.957 sec/iter\n",
      "Epoch: 166 | Batch: 002 / 011 | Total loss: 1.820 | Reg loss: 0.041 | Tree loss: 1.820 | Accuracy: 0.424000 | 0.957 sec/iter\n",
      "Epoch: 166 | Batch: 003 / 011 | Total loss: 1.776 | Reg loss: 0.041 | Tree loss: 1.776 | Accuracy: 0.413500 | 0.957 sec/iter\n",
      "Epoch: 166 | Batch: 004 / 011 | Total loss: 1.765 | Reg loss: 0.041 | Tree loss: 1.765 | Accuracy: 0.416500 | 0.957 sec/iter\n",
      "Epoch: 166 | Batch: 005 / 011 | Total loss: 1.773 | Reg loss: 0.041 | Tree loss: 1.773 | Accuracy: 0.406000 | 0.956 sec/iter\n",
      "Epoch: 166 | Batch: 006 / 011 | Total loss: 1.749 | Reg loss: 0.041 | Tree loss: 1.749 | Accuracy: 0.410500 | 0.956 sec/iter\n",
      "Epoch: 166 | Batch: 007 / 011 | Total loss: 1.715 | Reg loss: 0.041 | Tree loss: 1.715 | Accuracy: 0.420000 | 0.956 sec/iter\n",
      "Epoch: 166 | Batch: 008 / 011 | Total loss: 1.701 | Reg loss: 0.041 | Tree loss: 1.701 | Accuracy: 0.425500 | 0.956 sec/iter\n",
      "Epoch: 166 | Batch: 009 / 011 | Total loss: 1.714 | Reg loss: 0.041 | Tree loss: 1.714 | Accuracy: 0.431500 | 0.956 sec/iter\n",
      "Epoch: 166 | Batch: 010 / 011 | Total loss: 1.683 | Reg loss: 0.041 | Tree loss: 1.683 | Accuracy: 0.453925 | 0.956 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 167 | Batch: 000 / 011 | Total loss: 1.903 | Reg loss: 0.041 | Tree loss: 1.903 | Accuracy: 0.374500 | 0.956 sec/iter\n",
      "Epoch: 167 | Batch: 001 / 011 | Total loss: 1.870 | Reg loss: 0.041 | Tree loss: 1.870 | Accuracy: 0.395000 | 0.956 sec/iter\n",
      "Epoch: 167 | Batch: 002 / 011 | Total loss: 1.831 | Reg loss: 0.041 | Tree loss: 1.831 | Accuracy: 0.409000 | 0.956 sec/iter\n",
      "Epoch: 167 | Batch: 003 / 011 | Total loss: 1.796 | Reg loss: 0.041 | Tree loss: 1.796 | Accuracy: 0.411500 | 0.956 sec/iter\n",
      "Epoch: 167 | Batch: 004 / 011 | Total loss: 1.756 | Reg loss: 0.041 | Tree loss: 1.756 | Accuracy: 0.416500 | 0.956 sec/iter\n",
      "Epoch: 167 | Batch: 005 / 011 | Total loss: 1.755 | Reg loss: 0.041 | Tree loss: 1.755 | Accuracy: 0.409500 | 0.956 sec/iter\n",
      "Epoch: 167 | Batch: 006 / 011 | Total loss: 1.738 | Reg loss: 0.041 | Tree loss: 1.738 | Accuracy: 0.414000 | 0.956 sec/iter\n",
      "Epoch: 167 | Batch: 007 / 011 | Total loss: 1.725 | Reg loss: 0.041 | Tree loss: 1.725 | Accuracy: 0.425500 | 0.956 sec/iter\n",
      "Epoch: 167 | Batch: 008 / 011 | Total loss: 1.711 | Reg loss: 0.041 | Tree loss: 1.711 | Accuracy: 0.429500 | 0.956 sec/iter\n",
      "Epoch: 167 | Batch: 009 / 011 | Total loss: 1.713 | Reg loss: 0.041 | Tree loss: 1.713 | Accuracy: 0.432500 | 0.956 sec/iter\n",
      "Epoch: 167 | Batch: 010 / 011 | Total loss: 1.725 | Reg loss: 0.041 | Tree loss: 1.725 | Accuracy: 0.457338 | 0.956 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 168 | Batch: 000 / 011 | Total loss: 1.883 | Reg loss: 0.041 | Tree loss: 1.883 | Accuracy: 0.400500 | 0.956 sec/iter\n",
      "Epoch: 168 | Batch: 001 / 011 | Total loss: 1.870 | Reg loss: 0.041 | Tree loss: 1.870 | Accuracy: 0.393500 | 0.956 sec/iter\n",
      "Epoch: 168 | Batch: 002 / 011 | Total loss: 1.814 | Reg loss: 0.041 | Tree loss: 1.814 | Accuracy: 0.404000 | 0.956 sec/iter\n",
      "Epoch: 168 | Batch: 003 / 011 | Total loss: 1.806 | Reg loss: 0.041 | Tree loss: 1.806 | Accuracy: 0.399000 | 0.956 sec/iter\n",
      "Epoch: 168 | Batch: 004 / 011 | Total loss: 1.793 | Reg loss: 0.041 | Tree loss: 1.793 | Accuracy: 0.395000 | 0.956 sec/iter\n",
      "Epoch: 168 | Batch: 005 / 011 | Total loss: 1.772 | Reg loss: 0.041 | Tree loss: 1.772 | Accuracy: 0.414000 | 0.956 sec/iter\n",
      "Epoch: 168 | Batch: 006 / 011 | Total loss: 1.704 | Reg loss: 0.041 | Tree loss: 1.704 | Accuracy: 0.447000 | 0.956 sec/iter\n",
      "Epoch: 168 | Batch: 007 / 011 | Total loss: 1.727 | Reg loss: 0.041 | Tree loss: 1.727 | Accuracy: 0.419500 | 0.956 sec/iter\n",
      "Epoch: 168 | Batch: 008 / 011 | Total loss: 1.725 | Reg loss: 0.041 | Tree loss: 1.725 | Accuracy: 0.411500 | 0.956 sec/iter\n",
      "Epoch: 168 | Batch: 009 / 011 | Total loss: 1.703 | Reg loss: 0.041 | Tree loss: 1.703 | Accuracy: 0.429000 | 0.956 sec/iter\n",
      "Epoch: 168 | Batch: 010 / 011 | Total loss: 1.681 | Reg loss: 0.041 | Tree loss: 1.681 | Accuracy: 0.447099 | 0.956 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 169 | Batch: 000 / 011 | Total loss: 1.877 | Reg loss: 0.041 | Tree loss: 1.877 | Accuracy: 0.408000 | 0.956 sec/iter\n",
      "Epoch: 169 | Batch: 001 / 011 | Total loss: 1.868 | Reg loss: 0.041 | Tree loss: 1.868 | Accuracy: 0.407500 | 0.956 sec/iter\n",
      "Epoch: 169 | Batch: 002 / 011 | Total loss: 1.828 | Reg loss: 0.041 | Tree loss: 1.828 | Accuracy: 0.416500 | 0.956 sec/iter\n",
      "Epoch: 169 | Batch: 003 / 011 | Total loss: 1.790 | Reg loss: 0.041 | Tree loss: 1.790 | Accuracy: 0.401500 | 0.956 sec/iter\n",
      "Epoch: 169 | Batch: 004 / 011 | Total loss: 1.793 | Reg loss: 0.041 | Tree loss: 1.793 | Accuracy: 0.396000 | 0.956 sec/iter\n",
      "Epoch: 169 | Batch: 005 / 011 | Total loss: 1.740 | Reg loss: 0.041 | Tree loss: 1.740 | Accuracy: 0.404500 | 0.956 sec/iter\n",
      "Epoch: 169 | Batch: 006 / 011 | Total loss: 1.720 | Reg loss: 0.041 | Tree loss: 1.720 | Accuracy: 0.416000 | 0.956 sec/iter\n",
      "Epoch: 169 | Batch: 007 / 011 | Total loss: 1.727 | Reg loss: 0.041 | Tree loss: 1.727 | Accuracy: 0.421500 | 0.956 sec/iter\n",
      "Epoch: 169 | Batch: 008 / 011 | Total loss: 1.709 | Reg loss: 0.041 | Tree loss: 1.709 | Accuracy: 0.418500 | 0.956 sec/iter\n",
      "Epoch: 169 | Batch: 009 / 011 | Total loss: 1.726 | Reg loss: 0.041 | Tree loss: 1.726 | Accuracy: 0.424500 | 0.956 sec/iter\n",
      "Epoch: 169 | Batch: 010 / 011 | Total loss: 1.706 | Reg loss: 0.041 | Tree loss: 1.706 | Accuracy: 0.426621 | 0.956 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 170 | Batch: 000 / 011 | Total loss: 1.909 | Reg loss: 0.041 | Tree loss: 1.909 | Accuracy: 0.381000 | 0.956 sec/iter\n",
      "Epoch: 170 | Batch: 001 / 011 | Total loss: 1.868 | Reg loss: 0.041 | Tree loss: 1.868 | Accuracy: 0.393500 | 0.956 sec/iter\n",
      "Epoch: 170 | Batch: 002 / 011 | Total loss: 1.799 | Reg loss: 0.041 | Tree loss: 1.799 | Accuracy: 0.417000 | 0.956 sec/iter\n",
      "Epoch: 170 | Batch: 003 / 011 | Total loss: 1.815 | Reg loss: 0.041 | Tree loss: 1.815 | Accuracy: 0.382500 | 0.956 sec/iter\n",
      "Epoch: 170 | Batch: 004 / 011 | Total loss: 1.749 | Reg loss: 0.041 | Tree loss: 1.749 | Accuracy: 0.417000 | 0.955 sec/iter\n",
      "Epoch: 170 | Batch: 005 / 011 | Total loss: 1.757 | Reg loss: 0.041 | Tree loss: 1.757 | Accuracy: 0.418000 | 0.955 sec/iter\n",
      "Epoch: 170 | Batch: 006 / 011 | Total loss: 1.705 | Reg loss: 0.041 | Tree loss: 1.705 | Accuracy: 0.428000 | 0.955 sec/iter\n",
      "Epoch: 170 | Batch: 007 / 011 | Total loss: 1.730 | Reg loss: 0.041 | Tree loss: 1.730 | Accuracy: 0.413000 | 0.955 sec/iter\n",
      "Epoch: 170 | Batch: 008 / 011 | Total loss: 1.719 | Reg loss: 0.041 | Tree loss: 1.719 | Accuracy: 0.424500 | 0.955 sec/iter\n",
      "Epoch: 170 | Batch: 009 / 011 | Total loss: 1.731 | Reg loss: 0.041 | Tree loss: 1.731 | Accuracy: 0.422000 | 0.955 sec/iter\n",
      "Epoch: 170 | Batch: 010 / 011 | Total loss: 1.700 | Reg loss: 0.041 | Tree loss: 1.700 | Accuracy: 0.450512 | 0.955 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 171 | Batch: 000 / 011 | Total loss: 1.873 | Reg loss: 0.041 | Tree loss: 1.873 | Accuracy: 0.404500 | 0.955 sec/iter\n",
      "Epoch: 171 | Batch: 001 / 011 | Total loss: 1.882 | Reg loss: 0.041 | Tree loss: 1.882 | Accuracy: 0.398500 | 0.955 sec/iter\n",
      "Epoch: 171 | Batch: 002 / 011 | Total loss: 1.789 | Reg loss: 0.041 | Tree loss: 1.789 | Accuracy: 0.415500 | 0.955 sec/iter\n",
      "Epoch: 171 | Batch: 003 / 011 | Total loss: 1.804 | Reg loss: 0.041 | Tree loss: 1.804 | Accuracy: 0.405000 | 0.955 sec/iter\n",
      "Epoch: 171 | Batch: 004 / 011 | Total loss: 1.785 | Reg loss: 0.041 | Tree loss: 1.785 | Accuracy: 0.408500 | 0.955 sec/iter\n",
      "Epoch: 171 | Batch: 005 / 011 | Total loss: 1.773 | Reg loss: 0.041 | Tree loss: 1.773 | Accuracy: 0.400500 | 0.955 sec/iter\n",
      "Epoch: 171 | Batch: 006 / 011 | Total loss: 1.713 | Reg loss: 0.041 | Tree loss: 1.713 | Accuracy: 0.427500 | 0.955 sec/iter\n",
      "Epoch: 171 | Batch: 007 / 011 | Total loss: 1.726 | Reg loss: 0.041 | Tree loss: 1.726 | Accuracy: 0.413500 | 0.955 sec/iter\n",
      "Epoch: 171 | Batch: 008 / 011 | Total loss: 1.703 | Reg loss: 0.041 | Tree loss: 1.703 | Accuracy: 0.429500 | 0.955 sec/iter\n",
      "Epoch: 171 | Batch: 009 / 011 | Total loss: 1.708 | Reg loss: 0.041 | Tree loss: 1.708 | Accuracy: 0.423000 | 0.955 sec/iter\n",
      "Epoch: 171 | Batch: 010 / 011 | Total loss: 1.726 | Reg loss: 0.041 | Tree loss: 1.726 | Accuracy: 0.440273 | 0.955 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 172 | Batch: 000 / 011 | Total loss: 1.887 | Reg loss: 0.041 | Tree loss: 1.887 | Accuracy: 0.405500 | 0.955 sec/iter\n",
      "Epoch: 172 | Batch: 001 / 011 | Total loss: 1.874 | Reg loss: 0.041 | Tree loss: 1.874 | Accuracy: 0.375000 | 0.955 sec/iter\n",
      "Epoch: 172 | Batch: 002 / 011 | Total loss: 1.829 | Reg loss: 0.041 | Tree loss: 1.829 | Accuracy: 0.391500 | 0.955 sec/iter\n",
      "Epoch: 172 | Batch: 003 / 011 | Total loss: 1.820 | Reg loss: 0.041 | Tree loss: 1.820 | Accuracy: 0.402500 | 0.955 sec/iter\n",
      "Epoch: 172 | Batch: 004 / 011 | Total loss: 1.783 | Reg loss: 0.041 | Tree loss: 1.783 | Accuracy: 0.395500 | 0.955 sec/iter\n",
      "Epoch: 172 | Batch: 005 / 011 | Total loss: 1.746 | Reg loss: 0.041 | Tree loss: 1.746 | Accuracy: 0.419500 | 0.955 sec/iter\n",
      "Epoch: 172 | Batch: 006 / 011 | Total loss: 1.712 | Reg loss: 0.041 | Tree loss: 1.712 | Accuracy: 0.440500 | 0.955 sec/iter\n",
      "Epoch: 172 | Batch: 007 / 011 | Total loss: 1.716 | Reg loss: 0.041 | Tree loss: 1.716 | Accuracy: 0.425000 | 0.955 sec/iter\n",
      "Epoch: 172 | Batch: 008 / 011 | Total loss: 1.702 | Reg loss: 0.041 | Tree loss: 1.702 | Accuracy: 0.446500 | 0.955 sec/iter\n",
      "Epoch: 172 | Batch: 009 / 011 | Total loss: 1.697 | Reg loss: 0.041 | Tree loss: 1.697 | Accuracy: 0.429500 | 0.955 sec/iter\n",
      "Epoch: 172 | Batch: 010 / 011 | Total loss: 1.734 | Reg loss: 0.041 | Tree loss: 1.734 | Accuracy: 0.423208 | 0.955 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 173 | Batch: 000 / 011 | Total loss: 1.882 | Reg loss: 0.041 | Tree loss: 1.882 | Accuracy: 0.400500 | 0.955 sec/iter\n",
      "Epoch: 173 | Batch: 001 / 011 | Total loss: 1.863 | Reg loss: 0.041 | Tree loss: 1.863 | Accuracy: 0.395000 | 0.955 sec/iter\n",
      "Epoch: 173 | Batch: 002 / 011 | Total loss: 1.832 | Reg loss: 0.041 | Tree loss: 1.832 | Accuracy: 0.406000 | 0.955 sec/iter\n",
      "Epoch: 173 | Batch: 003 / 011 | Total loss: 1.794 | Reg loss: 0.041 | Tree loss: 1.794 | Accuracy: 0.411500 | 0.955 sec/iter\n",
      "Epoch: 173 | Batch: 004 / 011 | Total loss: 1.764 | Reg loss: 0.041 | Tree loss: 1.764 | Accuracy: 0.435500 | 0.955 sec/iter\n",
      "Epoch: 173 | Batch: 005 / 011 | Total loss: 1.744 | Reg loss: 0.041 | Tree loss: 1.744 | Accuracy: 0.411000 | 0.955 sec/iter\n",
      "Epoch: 173 | Batch: 006 / 011 | Total loss: 1.768 | Reg loss: 0.041 | Tree loss: 1.768 | Accuracy: 0.415000 | 0.955 sec/iter\n",
      "Epoch: 173 | Batch: 007 / 011 | Total loss: 1.739 | Reg loss: 0.041 | Tree loss: 1.739 | Accuracy: 0.421500 | 0.955 sec/iter\n",
      "Epoch: 173 | Batch: 008 / 011 | Total loss: 1.701 | Reg loss: 0.041 | Tree loss: 1.701 | Accuracy: 0.426500 | 0.955 sec/iter\n",
      "Epoch: 173 | Batch: 009 / 011 | Total loss: 1.695 | Reg loss: 0.041 | Tree loss: 1.695 | Accuracy: 0.436000 | 0.955 sec/iter\n",
      "Epoch: 173 | Batch: 010 / 011 | Total loss: 1.666 | Reg loss: 0.041 | Tree loss: 1.666 | Accuracy: 0.447099 | 0.955 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 174 | Batch: 000 / 011 | Total loss: 1.870 | Reg loss: 0.041 | Tree loss: 1.870 | Accuracy: 0.390000 | 0.955 sec/iter\n",
      "Epoch: 174 | Batch: 001 / 011 | Total loss: 1.877 | Reg loss: 0.041 | Tree loss: 1.877 | Accuracy: 0.384000 | 0.955 sec/iter\n",
      "Epoch: 174 | Batch: 002 / 011 | Total loss: 1.823 | Reg loss: 0.041 | Tree loss: 1.823 | Accuracy: 0.404000 | 0.955 sec/iter\n",
      "Epoch: 174 | Batch: 003 / 011 | Total loss: 1.787 | Reg loss: 0.041 | Tree loss: 1.787 | Accuracy: 0.421000 | 0.954 sec/iter\n",
      "Epoch: 174 | Batch: 004 / 011 | Total loss: 1.758 | Reg loss: 0.041 | Tree loss: 1.758 | Accuracy: 0.416500 | 0.954 sec/iter\n",
      "Epoch: 174 | Batch: 005 / 011 | Total loss: 1.732 | Reg loss: 0.041 | Tree loss: 1.732 | Accuracy: 0.425500 | 0.954 sec/iter\n",
      "Epoch: 174 | Batch: 006 / 011 | Total loss: 1.738 | Reg loss: 0.041 | Tree loss: 1.738 | Accuracy: 0.421000 | 0.954 sec/iter\n",
      "Epoch: 174 | Batch: 007 / 011 | Total loss: 1.719 | Reg loss: 0.041 | Tree loss: 1.719 | Accuracy: 0.423000 | 0.954 sec/iter\n",
      "Epoch: 174 | Batch: 008 / 011 | Total loss: 1.739 | Reg loss: 0.041 | Tree loss: 1.739 | Accuracy: 0.415500 | 0.954 sec/iter\n",
      "Epoch: 174 | Batch: 009 / 011 | Total loss: 1.711 | Reg loss: 0.041 | Tree loss: 1.711 | Accuracy: 0.423500 | 0.954 sec/iter\n",
      "Epoch: 174 | Batch: 010 / 011 | Total loss: 1.726 | Reg loss: 0.041 | Tree loss: 1.726 | Accuracy: 0.412969 | 0.954 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 175 | Batch: 000 / 011 | Total loss: 1.894 | Reg loss: 0.041 | Tree loss: 1.894 | Accuracy: 0.395500 | 0.954 sec/iter\n",
      "Epoch: 175 | Batch: 001 / 011 | Total loss: 1.840 | Reg loss: 0.041 | Tree loss: 1.840 | Accuracy: 0.424500 | 0.954 sec/iter\n",
      "Epoch: 175 | Batch: 002 / 011 | Total loss: 1.811 | Reg loss: 0.041 | Tree loss: 1.811 | Accuracy: 0.410000 | 0.954 sec/iter\n",
      "Epoch: 175 | Batch: 003 / 011 | Total loss: 1.787 | Reg loss: 0.041 | Tree loss: 1.787 | Accuracy: 0.424500 | 0.954 sec/iter\n",
      "Epoch: 175 | Batch: 004 / 011 | Total loss: 1.806 | Reg loss: 0.041 | Tree loss: 1.806 | Accuracy: 0.403500 | 0.954 sec/iter\n",
      "Epoch: 175 | Batch: 005 / 011 | Total loss: 1.752 | Reg loss: 0.041 | Tree loss: 1.752 | Accuracy: 0.421000 | 0.954 sec/iter\n",
      "Epoch: 175 | Batch: 006 / 011 | Total loss: 1.751 | Reg loss: 0.041 | Tree loss: 1.751 | Accuracy: 0.408500 | 0.954 sec/iter\n",
      "Epoch: 175 | Batch: 007 / 011 | Total loss: 1.723 | Reg loss: 0.041 | Tree loss: 1.723 | Accuracy: 0.408000 | 0.954 sec/iter\n",
      "Epoch: 175 | Batch: 008 / 011 | Total loss: 1.702 | Reg loss: 0.041 | Tree loss: 1.702 | Accuracy: 0.433000 | 0.954 sec/iter\n",
      "Epoch: 175 | Batch: 009 / 011 | Total loss: 1.719 | Reg loss: 0.041 | Tree loss: 1.719 | Accuracy: 0.409500 | 0.954 sec/iter\n",
      "Epoch: 175 | Batch: 010 / 011 | Total loss: 1.677 | Reg loss: 0.041 | Tree loss: 1.677 | Accuracy: 0.481229 | 0.954 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 176 | Batch: 000 / 011 | Total loss: 1.869 | Reg loss: 0.041 | Tree loss: 1.869 | Accuracy: 0.406500 | 0.954 sec/iter\n",
      "Epoch: 176 | Batch: 001 / 011 | Total loss: 1.873 | Reg loss: 0.041 | Tree loss: 1.873 | Accuracy: 0.385500 | 0.954 sec/iter\n",
      "Epoch: 176 | Batch: 002 / 011 | Total loss: 1.793 | Reg loss: 0.041 | Tree loss: 1.793 | Accuracy: 0.432000 | 0.954 sec/iter\n",
      "Epoch: 176 | Batch: 003 / 011 | Total loss: 1.805 | Reg loss: 0.041 | Tree loss: 1.805 | Accuracy: 0.400000 | 0.954 sec/iter\n",
      "Epoch: 176 | Batch: 004 / 011 | Total loss: 1.779 | Reg loss: 0.041 | Tree loss: 1.779 | Accuracy: 0.397500 | 0.954 sec/iter\n",
      "Epoch: 176 | Batch: 005 / 011 | Total loss: 1.707 | Reg loss: 0.041 | Tree loss: 1.707 | Accuracy: 0.420500 | 0.954 sec/iter\n",
      "Epoch: 176 | Batch: 006 / 011 | Total loss: 1.748 | Reg loss: 0.041 | Tree loss: 1.748 | Accuracy: 0.420000 | 0.954 sec/iter\n",
      "Epoch: 176 | Batch: 007 / 011 | Total loss: 1.718 | Reg loss: 0.041 | Tree loss: 1.718 | Accuracy: 0.439500 | 0.954 sec/iter\n",
      "Epoch: 176 | Batch: 008 / 011 | Total loss: 1.731 | Reg loss: 0.041 | Tree loss: 1.731 | Accuracy: 0.411500 | 0.954 sec/iter\n",
      "Epoch: 176 | Batch: 009 / 011 | Total loss: 1.700 | Reg loss: 0.041 | Tree loss: 1.700 | Accuracy: 0.427500 | 0.954 sec/iter\n",
      "Epoch: 176 | Batch: 010 / 011 | Total loss: 1.728 | Reg loss: 0.041 | Tree loss: 1.728 | Accuracy: 0.430034 | 0.954 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 177 | Batch: 000 / 011 | Total loss: 1.897 | Reg loss: 0.041 | Tree loss: 1.897 | Accuracy: 0.392500 | 0.954 sec/iter\n",
      "Epoch: 177 | Batch: 001 / 011 | Total loss: 1.856 | Reg loss: 0.041 | Tree loss: 1.856 | Accuracy: 0.407000 | 0.954 sec/iter\n",
      "Epoch: 177 | Batch: 002 / 011 | Total loss: 1.822 | Reg loss: 0.041 | Tree loss: 1.822 | Accuracy: 0.408000 | 0.954 sec/iter\n",
      "Epoch: 177 | Batch: 003 / 011 | Total loss: 1.797 | Reg loss: 0.041 | Tree loss: 1.797 | Accuracy: 0.419000 | 0.954 sec/iter\n",
      "Epoch: 177 | Batch: 004 / 011 | Total loss: 1.758 | Reg loss: 0.041 | Tree loss: 1.758 | Accuracy: 0.418000 | 0.954 sec/iter\n",
      "Epoch: 177 | Batch: 005 / 011 | Total loss: 1.771 | Reg loss: 0.041 | Tree loss: 1.771 | Accuracy: 0.405000 | 0.954 sec/iter\n",
      "Epoch: 177 | Batch: 006 / 011 | Total loss: 1.713 | Reg loss: 0.041 | Tree loss: 1.713 | Accuracy: 0.437000 | 0.954 sec/iter\n",
      "Epoch: 177 | Batch: 007 / 011 | Total loss: 1.719 | Reg loss: 0.041 | Tree loss: 1.719 | Accuracy: 0.432500 | 0.954 sec/iter\n",
      "Epoch: 177 | Batch: 008 / 011 | Total loss: 1.693 | Reg loss: 0.041 | Tree loss: 1.693 | Accuracy: 0.433000 | 0.954 sec/iter\n",
      "Epoch: 177 | Batch: 009 / 011 | Total loss: 1.708 | Reg loss: 0.041 | Tree loss: 1.708 | Accuracy: 0.419000 | 0.954 sec/iter\n",
      "Epoch: 177 | Batch: 010 / 011 | Total loss: 1.696 | Reg loss: 0.041 | Tree loss: 1.696 | Accuracy: 0.440273 | 0.954 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 178 | Batch: 000 / 011 | Total loss: 1.860 | Reg loss: 0.041 | Tree loss: 1.860 | Accuracy: 0.405000 | 0.954 sec/iter\n",
      "Epoch: 178 | Batch: 001 / 011 | Total loss: 1.867 | Reg loss: 0.041 | Tree loss: 1.867 | Accuracy: 0.382000 | 0.954 sec/iter\n",
      "Epoch: 178 | Batch: 002 / 011 | Total loss: 1.823 | Reg loss: 0.041 | Tree loss: 1.823 | Accuracy: 0.401500 | 0.954 sec/iter\n",
      "Epoch: 178 | Batch: 003 / 011 | Total loss: 1.771 | Reg loss: 0.041 | Tree loss: 1.771 | Accuracy: 0.415500 | 0.954 sec/iter\n",
      "Epoch: 178 | Batch: 004 / 011 | Total loss: 1.789 | Reg loss: 0.041 | Tree loss: 1.789 | Accuracy: 0.408000 | 0.954 sec/iter\n",
      "Epoch: 178 | Batch: 005 / 011 | Total loss: 1.748 | Reg loss: 0.041 | Tree loss: 1.748 | Accuracy: 0.412500 | 0.954 sec/iter\n",
      "Epoch: 178 | Batch: 006 / 011 | Total loss: 1.728 | Reg loss: 0.041 | Tree loss: 1.728 | Accuracy: 0.439500 | 0.953 sec/iter\n",
      "Epoch: 178 | Batch: 007 / 011 | Total loss: 1.703 | Reg loss: 0.041 | Tree loss: 1.703 | Accuracy: 0.427000 | 0.953 sec/iter\n",
      "Epoch: 178 | Batch: 008 / 011 | Total loss: 1.726 | Reg loss: 0.041 | Tree loss: 1.726 | Accuracy: 0.416500 | 0.953 sec/iter\n",
      "Epoch: 178 | Batch: 009 / 011 | Total loss: 1.717 | Reg loss: 0.041 | Tree loss: 1.717 | Accuracy: 0.411500 | 0.953 sec/iter\n",
      "Epoch: 178 | Batch: 010 / 011 | Total loss: 1.669 | Reg loss: 0.041 | Tree loss: 1.669 | Accuracy: 0.447099 | 0.953 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 179 | Batch: 000 / 011 | Total loss: 1.892 | Reg loss: 0.041 | Tree loss: 1.892 | Accuracy: 0.385500 | 0.953 sec/iter\n",
      "Epoch: 179 | Batch: 001 / 011 | Total loss: 1.846 | Reg loss: 0.041 | Tree loss: 1.846 | Accuracy: 0.399000 | 0.953 sec/iter\n",
      "Epoch: 179 | Batch: 002 / 011 | Total loss: 1.819 | Reg loss: 0.041 | Tree loss: 1.819 | Accuracy: 0.421500 | 0.953 sec/iter\n",
      "Epoch: 179 | Batch: 003 / 011 | Total loss: 1.782 | Reg loss: 0.041 | Tree loss: 1.782 | Accuracy: 0.437000 | 0.953 sec/iter\n",
      "Epoch: 179 | Batch: 004 / 011 | Total loss: 1.776 | Reg loss: 0.041 | Tree loss: 1.776 | Accuracy: 0.421000 | 0.953 sec/iter\n",
      "Epoch: 179 | Batch: 005 / 011 | Total loss: 1.733 | Reg loss: 0.041 | Tree loss: 1.733 | Accuracy: 0.433500 | 0.953 sec/iter\n",
      "Epoch: 179 | Batch: 006 / 011 | Total loss: 1.731 | Reg loss: 0.041 | Tree loss: 1.731 | Accuracy: 0.423000 | 0.953 sec/iter\n",
      "Epoch: 179 | Batch: 007 / 011 | Total loss: 1.722 | Reg loss: 0.041 | Tree loss: 1.722 | Accuracy: 0.414500 | 0.953 sec/iter\n",
      "Epoch: 179 | Batch: 008 / 011 | Total loss: 1.690 | Reg loss: 0.041 | Tree loss: 1.690 | Accuracy: 0.424500 | 0.953 sec/iter\n",
      "Epoch: 179 | Batch: 009 / 011 | Total loss: 1.726 | Reg loss: 0.041 | Tree loss: 1.726 | Accuracy: 0.416500 | 0.953 sec/iter\n",
      "Epoch: 179 | Batch: 010 / 011 | Total loss: 1.760 | Reg loss: 0.041 | Tree loss: 1.760 | Accuracy: 0.406143 | 0.953 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 180 | Batch: 000 / 011 | Total loss: 1.879 | Reg loss: 0.041 | Tree loss: 1.879 | Accuracy: 0.399500 | 0.953 sec/iter\n",
      "Epoch: 180 | Batch: 001 / 011 | Total loss: 1.853 | Reg loss: 0.041 | Tree loss: 1.853 | Accuracy: 0.376500 | 0.953 sec/iter\n",
      "Epoch: 180 | Batch: 002 / 011 | Total loss: 1.809 | Reg loss: 0.041 | Tree loss: 1.809 | Accuracy: 0.414000 | 0.953 sec/iter\n",
      "Epoch: 180 | Batch: 003 / 011 | Total loss: 1.814 | Reg loss: 0.041 | Tree loss: 1.814 | Accuracy: 0.386500 | 0.953 sec/iter\n",
      "Epoch: 180 | Batch: 004 / 011 | Total loss: 1.759 | Reg loss: 0.041 | Tree loss: 1.759 | Accuracy: 0.424500 | 0.953 sec/iter\n",
      "Epoch: 180 | Batch: 005 / 011 | Total loss: 1.738 | Reg loss: 0.041 | Tree loss: 1.738 | Accuracy: 0.430000 | 0.953 sec/iter\n",
      "Epoch: 180 | Batch: 006 / 011 | Total loss: 1.735 | Reg loss: 0.041 | Tree loss: 1.735 | Accuracy: 0.426000 | 0.953 sec/iter\n",
      "Epoch: 180 | Batch: 007 / 011 | Total loss: 1.719 | Reg loss: 0.041 | Tree loss: 1.719 | Accuracy: 0.433000 | 0.953 sec/iter\n",
      "Epoch: 180 | Batch: 008 / 011 | Total loss: 1.714 | Reg loss: 0.041 | Tree loss: 1.714 | Accuracy: 0.422500 | 0.953 sec/iter\n",
      "Epoch: 180 | Batch: 009 / 011 | Total loss: 1.701 | Reg loss: 0.041 | Tree loss: 1.701 | Accuracy: 0.430000 | 0.953 sec/iter\n",
      "Epoch: 180 | Batch: 010 / 011 | Total loss: 1.697 | Reg loss: 0.041 | Tree loss: 1.697 | Accuracy: 0.416382 | 0.953 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 181 | Batch: 000 / 011 | Total loss: 1.885 | Reg loss: 0.041 | Tree loss: 1.885 | Accuracy: 0.398500 | 0.953 sec/iter\n",
      "Epoch: 181 | Batch: 001 / 011 | Total loss: 1.864 | Reg loss: 0.041 | Tree loss: 1.864 | Accuracy: 0.388000 | 0.953 sec/iter\n",
      "Epoch: 181 | Batch: 002 / 011 | Total loss: 1.819 | Reg loss: 0.041 | Tree loss: 1.819 | Accuracy: 0.399500 | 0.953 sec/iter\n",
      "Epoch: 181 | Batch: 003 / 011 | Total loss: 1.802 | Reg loss: 0.041 | Tree loss: 1.802 | Accuracy: 0.398000 | 0.953 sec/iter\n",
      "Epoch: 181 | Batch: 004 / 011 | Total loss: 1.780 | Reg loss: 0.041 | Tree loss: 1.780 | Accuracy: 0.415500 | 0.953 sec/iter\n",
      "Epoch: 181 | Batch: 005 / 011 | Total loss: 1.732 | Reg loss: 0.041 | Tree loss: 1.732 | Accuracy: 0.449500 | 0.953 sec/iter\n",
      "Epoch: 181 | Batch: 006 / 011 | Total loss: 1.717 | Reg loss: 0.041 | Tree loss: 1.717 | Accuracy: 0.421500 | 0.953 sec/iter\n",
      "Epoch: 181 | Batch: 007 / 011 | Total loss: 1.717 | Reg loss: 0.041 | Tree loss: 1.717 | Accuracy: 0.419000 | 0.953 sec/iter\n",
      "Epoch: 181 | Batch: 008 / 011 | Total loss: 1.692 | Reg loss: 0.041 | Tree loss: 1.692 | Accuracy: 0.428500 | 0.953 sec/iter\n",
      "Epoch: 181 | Batch: 009 / 011 | Total loss: 1.709 | Reg loss: 0.041 | Tree loss: 1.709 | Accuracy: 0.413000 | 0.953 sec/iter\n",
      "Epoch: 181 | Batch: 010 / 011 | Total loss: 1.721 | Reg loss: 0.041 | Tree loss: 1.721 | Accuracy: 0.385666 | 0.953 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 182 | Batch: 000 / 011 | Total loss: 1.863 | Reg loss: 0.041 | Tree loss: 1.863 | Accuracy: 0.401500 | 0.953 sec/iter\n",
      "Epoch: 182 | Batch: 001 / 011 | Total loss: 1.833 | Reg loss: 0.041 | Tree loss: 1.833 | Accuracy: 0.419000 | 0.953 sec/iter\n",
      "Epoch: 182 | Batch: 002 / 011 | Total loss: 1.829 | Reg loss: 0.041 | Tree loss: 1.829 | Accuracy: 0.386500 | 0.953 sec/iter\n",
      "Epoch: 182 | Batch: 003 / 011 | Total loss: 1.792 | Reg loss: 0.041 | Tree loss: 1.792 | Accuracy: 0.402000 | 0.953 sec/iter\n",
      "Epoch: 182 | Batch: 004 / 011 | Total loss: 1.792 | Reg loss: 0.041 | Tree loss: 1.792 | Accuracy: 0.403500 | 0.953 sec/iter\n",
      "Epoch: 182 | Batch: 005 / 011 | Total loss: 1.734 | Reg loss: 0.041 | Tree loss: 1.734 | Accuracy: 0.422500 | 0.953 sec/iter\n",
      "Epoch: 182 | Batch: 006 / 011 | Total loss: 1.728 | Reg loss: 0.041 | Tree loss: 1.728 | Accuracy: 0.414500 | 0.953 sec/iter\n",
      "Epoch: 182 | Batch: 007 / 011 | Total loss: 1.707 | Reg loss: 0.041 | Tree loss: 1.707 | Accuracy: 0.433000 | 0.953 sec/iter\n",
      "Epoch: 182 | Batch: 008 / 011 | Total loss: 1.704 | Reg loss: 0.041 | Tree loss: 1.704 | Accuracy: 0.434000 | 0.953 sec/iter\n",
      "Epoch: 182 | Batch: 009 / 011 | Total loss: 1.732 | Reg loss: 0.041 | Tree loss: 1.732 | Accuracy: 0.410500 | 0.953 sec/iter\n",
      "Epoch: 182 | Batch: 010 / 011 | Total loss: 1.646 | Reg loss: 0.042 | Tree loss: 1.646 | Accuracy: 0.464164 | 0.953 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 183 | Batch: 000 / 011 | Total loss: 1.873 | Reg loss: 0.041 | Tree loss: 1.873 | Accuracy: 0.406000 | 0.953 sec/iter\n",
      "Epoch: 183 | Batch: 001 / 011 | Total loss: 1.852 | Reg loss: 0.041 | Tree loss: 1.852 | Accuracy: 0.397500 | 0.953 sec/iter\n",
      "Epoch: 183 | Batch: 002 / 011 | Total loss: 1.832 | Reg loss: 0.041 | Tree loss: 1.832 | Accuracy: 0.401000 | 0.953 sec/iter\n",
      "Epoch: 183 | Batch: 003 / 011 | Total loss: 1.813 | Reg loss: 0.041 | Tree loss: 1.813 | Accuracy: 0.407500 | 0.952 sec/iter\n",
      "Epoch: 183 | Batch: 004 / 011 | Total loss: 1.778 | Reg loss: 0.041 | Tree loss: 1.778 | Accuracy: 0.404500 | 0.952 sec/iter\n",
      "Epoch: 183 | Batch: 005 / 011 | Total loss: 1.746 | Reg loss: 0.041 | Tree loss: 1.746 | Accuracy: 0.401500 | 0.952 sec/iter\n",
      "Epoch: 183 | Batch: 006 / 011 | Total loss: 1.712 | Reg loss: 0.041 | Tree loss: 1.712 | Accuracy: 0.442500 | 0.952 sec/iter\n",
      "Epoch: 183 | Batch: 007 / 011 | Total loss: 1.711 | Reg loss: 0.041 | Tree loss: 1.711 | Accuracy: 0.434000 | 0.952 sec/iter\n",
      "Epoch: 183 | Batch: 008 / 011 | Total loss: 1.707 | Reg loss: 0.041 | Tree loss: 1.707 | Accuracy: 0.430500 | 0.952 sec/iter\n",
      "Epoch: 183 | Batch: 009 / 011 | Total loss: 1.699 | Reg loss: 0.042 | Tree loss: 1.699 | Accuracy: 0.430500 | 0.952 sec/iter\n",
      "Epoch: 183 | Batch: 010 / 011 | Total loss: 1.703 | Reg loss: 0.042 | Tree loss: 1.703 | Accuracy: 0.443686 | 0.952 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 184 | Batch: 000 / 011 | Total loss: 1.882 | Reg loss: 0.041 | Tree loss: 1.882 | Accuracy: 0.379500 | 0.952 sec/iter\n",
      "Epoch: 184 | Batch: 001 / 011 | Total loss: 1.866 | Reg loss: 0.041 | Tree loss: 1.866 | Accuracy: 0.413000 | 0.952 sec/iter\n",
      "Epoch: 184 | Batch: 002 / 011 | Total loss: 1.798 | Reg loss: 0.041 | Tree loss: 1.798 | Accuracy: 0.425500 | 0.952 sec/iter\n",
      "Epoch: 184 | Batch: 003 / 011 | Total loss: 1.792 | Reg loss: 0.041 | Tree loss: 1.792 | Accuracy: 0.417000 | 0.952 sec/iter\n",
      "Epoch: 184 | Batch: 004 / 011 | Total loss: 1.763 | Reg loss: 0.041 | Tree loss: 1.763 | Accuracy: 0.425000 | 0.952 sec/iter\n",
      "Epoch: 184 | Batch: 005 / 011 | Total loss: 1.759 | Reg loss: 0.041 | Tree loss: 1.759 | Accuracy: 0.423000 | 0.952 sec/iter\n",
      "Epoch: 184 | Batch: 006 / 011 | Total loss: 1.757 | Reg loss: 0.041 | Tree loss: 1.757 | Accuracy: 0.406500 | 0.952 sec/iter\n",
      "Epoch: 184 | Batch: 007 / 011 | Total loss: 1.742 | Reg loss: 0.041 | Tree loss: 1.742 | Accuracy: 0.404500 | 0.952 sec/iter\n",
      "Epoch: 184 | Batch: 008 / 011 | Total loss: 1.684 | Reg loss: 0.041 | Tree loss: 1.684 | Accuracy: 0.431000 | 0.952 sec/iter\n",
      "Epoch: 184 | Batch: 009 / 011 | Total loss: 1.682 | Reg loss: 0.042 | Tree loss: 1.682 | Accuracy: 0.436000 | 0.952 sec/iter\n",
      "Epoch: 184 | Batch: 010 / 011 | Total loss: 1.638 | Reg loss: 0.042 | Tree loss: 1.638 | Accuracy: 0.470990 | 0.952 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 185 | Batch: 000 / 011 | Total loss: 1.892 | Reg loss: 0.041 | Tree loss: 1.892 | Accuracy: 0.394500 | 0.952 sec/iter\n",
      "Epoch: 185 | Batch: 001 / 011 | Total loss: 1.865 | Reg loss: 0.041 | Tree loss: 1.865 | Accuracy: 0.387500 | 0.952 sec/iter\n",
      "Epoch: 185 | Batch: 002 / 011 | Total loss: 1.801 | Reg loss: 0.041 | Tree loss: 1.801 | Accuracy: 0.409500 | 0.952 sec/iter\n",
      "Epoch: 185 | Batch: 003 / 011 | Total loss: 1.789 | Reg loss: 0.041 | Tree loss: 1.789 | Accuracy: 0.409000 | 0.952 sec/iter\n",
      "Epoch: 185 | Batch: 004 / 011 | Total loss: 1.765 | Reg loss: 0.041 | Tree loss: 1.765 | Accuracy: 0.407500 | 0.952 sec/iter\n",
      "Epoch: 185 | Batch: 005 / 011 | Total loss: 1.718 | Reg loss: 0.041 | Tree loss: 1.718 | Accuracy: 0.428000 | 0.952 sec/iter\n",
      "Epoch: 185 | Batch: 006 / 011 | Total loss: 1.744 | Reg loss: 0.041 | Tree loss: 1.744 | Accuracy: 0.415000 | 0.952 sec/iter\n",
      "Epoch: 185 | Batch: 007 / 011 | Total loss: 1.741 | Reg loss: 0.041 | Tree loss: 1.741 | Accuracy: 0.412000 | 0.952 sec/iter\n",
      "Epoch: 185 | Batch: 008 / 011 | Total loss: 1.701 | Reg loss: 0.042 | Tree loss: 1.701 | Accuracy: 0.429000 | 0.952 sec/iter\n",
      "Epoch: 185 | Batch: 009 / 011 | Total loss: 1.693 | Reg loss: 0.042 | Tree loss: 1.693 | Accuracy: 0.433500 | 0.952 sec/iter\n",
      "Epoch: 185 | Batch: 010 / 011 | Total loss: 1.648 | Reg loss: 0.042 | Tree loss: 1.648 | Accuracy: 0.447099 | 0.952 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 186 | Batch: 000 / 011 | Total loss: 1.870 | Reg loss: 0.041 | Tree loss: 1.870 | Accuracy: 0.408500 | 0.952 sec/iter\n",
      "Epoch: 186 | Batch: 001 / 011 | Total loss: 1.863 | Reg loss: 0.041 | Tree loss: 1.863 | Accuracy: 0.405000 | 0.952 sec/iter\n",
      "Epoch: 186 | Batch: 002 / 011 | Total loss: 1.808 | Reg loss: 0.041 | Tree loss: 1.808 | Accuracy: 0.404000 | 0.952 sec/iter\n",
      "Epoch: 186 | Batch: 003 / 011 | Total loss: 1.787 | Reg loss: 0.041 | Tree loss: 1.787 | Accuracy: 0.422000 | 0.952 sec/iter\n",
      "Epoch: 186 | Batch: 004 / 011 | Total loss: 1.757 | Reg loss: 0.041 | Tree loss: 1.757 | Accuracy: 0.428500 | 0.952 sec/iter\n",
      "Epoch: 186 | Batch: 005 / 011 | Total loss: 1.736 | Reg loss: 0.041 | Tree loss: 1.736 | Accuracy: 0.423000 | 0.952 sec/iter\n",
      "Epoch: 186 | Batch: 006 / 011 | Total loss: 1.740 | Reg loss: 0.041 | Tree loss: 1.740 | Accuracy: 0.410500 | 0.952 sec/iter\n",
      "Epoch: 186 | Batch: 007 / 011 | Total loss: 1.709 | Reg loss: 0.042 | Tree loss: 1.709 | Accuracy: 0.423500 | 0.952 sec/iter\n",
      "Epoch: 186 | Batch: 008 / 011 | Total loss: 1.721 | Reg loss: 0.042 | Tree loss: 1.721 | Accuracy: 0.414500 | 0.952 sec/iter\n",
      "Epoch: 186 | Batch: 009 / 011 | Total loss: 1.691 | Reg loss: 0.042 | Tree loss: 1.691 | Accuracy: 0.428000 | 0.952 sec/iter\n",
      "Epoch: 186 | Batch: 010 / 011 | Total loss: 1.747 | Reg loss: 0.042 | Tree loss: 1.747 | Accuracy: 0.392491 | 0.952 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 187 | Batch: 000 / 011 | Total loss: 1.890 | Reg loss: 0.041 | Tree loss: 1.890 | Accuracy: 0.392000 | 0.952 sec/iter\n",
      "Epoch: 187 | Batch: 001 / 011 | Total loss: 1.843 | Reg loss: 0.041 | Tree loss: 1.843 | Accuracy: 0.417500 | 0.952 sec/iter\n",
      "Epoch: 187 | Batch: 002 / 011 | Total loss: 1.806 | Reg loss: 0.041 | Tree loss: 1.806 | Accuracy: 0.410500 | 0.952 sec/iter\n",
      "Epoch: 187 | Batch: 003 / 011 | Total loss: 1.816 | Reg loss: 0.041 | Tree loss: 1.816 | Accuracy: 0.389000 | 0.952 sec/iter\n",
      "Epoch: 187 | Batch: 004 / 011 | Total loss: 1.796 | Reg loss: 0.041 | Tree loss: 1.796 | Accuracy: 0.383000 | 0.952 sec/iter\n",
      "Epoch: 187 | Batch: 005 / 011 | Total loss: 1.726 | Reg loss: 0.041 | Tree loss: 1.726 | Accuracy: 0.428500 | 0.952 sec/iter\n",
      "Epoch: 187 | Batch: 006 / 011 | Total loss: 1.693 | Reg loss: 0.041 | Tree loss: 1.693 | Accuracy: 0.435000 | 0.952 sec/iter\n",
      "Epoch: 187 | Batch: 007 / 011 | Total loss: 1.713 | Reg loss: 0.042 | Tree loss: 1.713 | Accuracy: 0.429500 | 0.952 sec/iter\n",
      "Epoch: 187 | Batch: 008 / 011 | Total loss: 1.704 | Reg loss: 0.042 | Tree loss: 1.704 | Accuracy: 0.435500 | 0.952 sec/iter\n",
      "Epoch: 187 | Batch: 009 / 011 | Total loss: 1.720 | Reg loss: 0.042 | Tree loss: 1.720 | Accuracy: 0.408000 | 0.952 sec/iter\n",
      "Epoch: 187 | Batch: 010 / 011 | Total loss: 1.702 | Reg loss: 0.042 | Tree loss: 1.702 | Accuracy: 0.419795 | 0.951 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 188 | Batch: 000 / 011 | Total loss: 1.864 | Reg loss: 0.041 | Tree loss: 1.864 | Accuracy: 0.411500 | 0.952 sec/iter\n",
      "Epoch: 188 | Batch: 001 / 011 | Total loss: 1.858 | Reg loss: 0.041 | Tree loss: 1.858 | Accuracy: 0.384000 | 0.951 sec/iter\n",
      "Epoch: 188 | Batch: 002 / 011 | Total loss: 1.796 | Reg loss: 0.041 | Tree loss: 1.796 | Accuracy: 0.432500 | 0.951 sec/iter\n",
      "Epoch: 188 | Batch: 003 / 011 | Total loss: 1.824 | Reg loss: 0.041 | Tree loss: 1.824 | Accuracy: 0.388500 | 0.951 sec/iter\n",
      "Epoch: 188 | Batch: 004 / 011 | Total loss: 1.739 | Reg loss: 0.041 | Tree loss: 1.739 | Accuracy: 0.429000 | 0.951 sec/iter\n",
      "Epoch: 188 | Batch: 005 / 011 | Total loss: 1.734 | Reg loss: 0.041 | Tree loss: 1.734 | Accuracy: 0.422500 | 0.951 sec/iter\n",
      "Epoch: 188 | Batch: 006 / 011 | Total loss: 1.711 | Reg loss: 0.042 | Tree loss: 1.711 | Accuracy: 0.433000 | 0.951 sec/iter\n",
      "Epoch: 188 | Batch: 007 / 011 | Total loss: 1.722 | Reg loss: 0.042 | Tree loss: 1.722 | Accuracy: 0.409500 | 0.951 sec/iter\n",
      "Epoch: 188 | Batch: 008 / 011 | Total loss: 1.732 | Reg loss: 0.042 | Tree loss: 1.732 | Accuracy: 0.423000 | 0.951 sec/iter\n",
      "Epoch: 188 | Batch: 009 / 011 | Total loss: 1.724 | Reg loss: 0.042 | Tree loss: 1.724 | Accuracy: 0.411500 | 0.951 sec/iter\n",
      "Epoch: 188 | Batch: 010 / 011 | Total loss: 1.702 | Reg loss: 0.042 | Tree loss: 1.702 | Accuracy: 0.467577 | 0.951 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 189 | Batch: 000 / 011 | Total loss: 1.878 | Reg loss: 0.041 | Tree loss: 1.878 | Accuracy: 0.404000 | 0.951 sec/iter\n",
      "Epoch: 189 | Batch: 001 / 011 | Total loss: 1.854 | Reg loss: 0.041 | Tree loss: 1.854 | Accuracy: 0.401000 | 0.951 sec/iter\n",
      "Epoch: 189 | Batch: 002 / 011 | Total loss: 1.806 | Reg loss: 0.041 | Tree loss: 1.806 | Accuracy: 0.404500 | 0.951 sec/iter\n",
      "Epoch: 189 | Batch: 003 / 011 | Total loss: 1.767 | Reg loss: 0.041 | Tree loss: 1.767 | Accuracy: 0.423000 | 0.951 sec/iter\n",
      "Epoch: 189 | Batch: 004 / 011 | Total loss: 1.786 | Reg loss: 0.041 | Tree loss: 1.786 | Accuracy: 0.406000 | 0.951 sec/iter\n",
      "Epoch: 189 | Batch: 005 / 011 | Total loss: 1.726 | Reg loss: 0.042 | Tree loss: 1.726 | Accuracy: 0.442500 | 0.951 sec/iter\n",
      "Epoch: 189 | Batch: 006 / 011 | Total loss: 1.719 | Reg loss: 0.042 | Tree loss: 1.719 | Accuracy: 0.430500 | 0.951 sec/iter\n",
      "Epoch: 189 | Batch: 007 / 011 | Total loss: 1.726 | Reg loss: 0.042 | Tree loss: 1.726 | Accuracy: 0.411500 | 0.951 sec/iter\n",
      "Epoch: 189 | Batch: 008 / 011 | Total loss: 1.709 | Reg loss: 0.042 | Tree loss: 1.709 | Accuracy: 0.413500 | 0.951 sec/iter\n",
      "Epoch: 189 | Batch: 009 / 011 | Total loss: 1.714 | Reg loss: 0.042 | Tree loss: 1.714 | Accuracy: 0.411000 | 0.951 sec/iter\n",
      "Epoch: 189 | Batch: 010 / 011 | Total loss: 1.760 | Reg loss: 0.042 | Tree loss: 1.760 | Accuracy: 0.412969 | 0.951 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 190 | Batch: 000 / 011 | Total loss: 1.872 | Reg loss: 0.041 | Tree loss: 1.872 | Accuracy: 0.418500 | 0.951 sec/iter\n",
      "Epoch: 190 | Batch: 001 / 011 | Total loss: 1.855 | Reg loss: 0.041 | Tree loss: 1.855 | Accuracy: 0.379500 | 0.951 sec/iter\n",
      "Epoch: 190 | Batch: 002 / 011 | Total loss: 1.780 | Reg loss: 0.041 | Tree loss: 1.780 | Accuracy: 0.422500 | 0.951 sec/iter\n",
      "Epoch: 190 | Batch: 003 / 011 | Total loss: 1.768 | Reg loss: 0.041 | Tree loss: 1.768 | Accuracy: 0.428000 | 0.951 sec/iter\n",
      "Epoch: 190 | Batch: 004 / 011 | Total loss: 1.776 | Reg loss: 0.041 | Tree loss: 1.776 | Accuracy: 0.417500 | 0.951 sec/iter\n",
      "Epoch: 190 | Batch: 005 / 011 | Total loss: 1.764 | Reg loss: 0.042 | Tree loss: 1.764 | Accuracy: 0.408000 | 0.951 sec/iter\n",
      "Epoch: 190 | Batch: 006 / 011 | Total loss: 1.739 | Reg loss: 0.042 | Tree loss: 1.739 | Accuracy: 0.421500 | 0.951 sec/iter\n",
      "Epoch: 190 | Batch: 007 / 011 | Total loss: 1.734 | Reg loss: 0.042 | Tree loss: 1.734 | Accuracy: 0.403000 | 0.951 sec/iter\n",
      "Epoch: 190 | Batch: 008 / 011 | Total loss: 1.677 | Reg loss: 0.042 | Tree loss: 1.677 | Accuracy: 0.436000 | 0.951 sec/iter\n",
      "Epoch: 190 | Batch: 009 / 011 | Total loss: 1.723 | Reg loss: 0.042 | Tree loss: 1.723 | Accuracy: 0.412000 | 0.951 sec/iter\n",
      "Epoch: 190 | Batch: 010 / 011 | Total loss: 1.651 | Reg loss: 0.042 | Tree loss: 1.651 | Accuracy: 0.453925 | 0.951 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 191 | Batch: 000 / 011 | Total loss: 1.874 | Reg loss: 0.041 | Tree loss: 1.874 | Accuracy: 0.398000 | 0.951 sec/iter\n",
      "Epoch: 191 | Batch: 001 / 011 | Total loss: 1.882 | Reg loss: 0.041 | Tree loss: 1.882 | Accuracy: 0.374500 | 0.951 sec/iter\n",
      "Epoch: 191 | Batch: 002 / 011 | Total loss: 1.842 | Reg loss: 0.041 | Tree loss: 1.842 | Accuracy: 0.409500 | 0.951 sec/iter\n",
      "Epoch: 191 | Batch: 003 / 011 | Total loss: 1.792 | Reg loss: 0.041 | Tree loss: 1.792 | Accuracy: 0.398500 | 0.951 sec/iter\n",
      "Epoch: 191 | Batch: 004 / 011 | Total loss: 1.766 | Reg loss: 0.042 | Tree loss: 1.766 | Accuracy: 0.410500 | 0.951 sec/iter\n",
      "Epoch: 191 | Batch: 005 / 011 | Total loss: 1.723 | Reg loss: 0.042 | Tree loss: 1.723 | Accuracy: 0.426500 | 0.951 sec/iter\n",
      "Epoch: 191 | Batch: 006 / 011 | Total loss: 1.721 | Reg loss: 0.042 | Tree loss: 1.721 | Accuracy: 0.409000 | 0.951 sec/iter\n",
      "Epoch: 191 | Batch: 007 / 011 | Total loss: 1.710 | Reg loss: 0.042 | Tree loss: 1.710 | Accuracy: 0.430000 | 0.951 sec/iter\n",
      "Epoch: 191 | Batch: 008 / 011 | Total loss: 1.716 | Reg loss: 0.042 | Tree loss: 1.716 | Accuracy: 0.437000 | 0.951 sec/iter\n",
      "Epoch: 191 | Batch: 009 / 011 | Total loss: 1.666 | Reg loss: 0.042 | Tree loss: 1.666 | Accuracy: 0.442500 | 0.951 sec/iter\n",
      "Epoch: 191 | Batch: 010 / 011 | Total loss: 1.652 | Reg loss: 0.042 | Tree loss: 1.652 | Accuracy: 0.450512 | 0.951 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 192 | Batch: 000 / 011 | Total loss: 1.867 | Reg loss: 0.041 | Tree loss: 1.867 | Accuracy: 0.418000 | 0.951 sec/iter\n",
      "Epoch: 192 | Batch: 001 / 011 | Total loss: 1.816 | Reg loss: 0.041 | Tree loss: 1.816 | Accuracy: 0.409000 | 0.951 sec/iter\n",
      "Epoch: 192 | Batch: 002 / 011 | Total loss: 1.825 | Reg loss: 0.042 | Tree loss: 1.825 | Accuracy: 0.380000 | 0.951 sec/iter\n",
      "Epoch: 192 | Batch: 003 / 011 | Total loss: 1.793 | Reg loss: 0.042 | Tree loss: 1.793 | Accuracy: 0.398500 | 0.951 sec/iter\n",
      "Epoch: 192 | Batch: 004 / 011 | Total loss: 1.772 | Reg loss: 0.042 | Tree loss: 1.772 | Accuracy: 0.425500 | 0.951 sec/iter\n",
      "Epoch: 192 | Batch: 005 / 011 | Total loss: 1.744 | Reg loss: 0.042 | Tree loss: 1.744 | Accuracy: 0.409000 | 0.951 sec/iter\n",
      "Epoch: 192 | Batch: 006 / 011 | Total loss: 1.713 | Reg loss: 0.042 | Tree loss: 1.713 | Accuracy: 0.425000 | 0.951 sec/iter\n",
      "Epoch: 192 | Batch: 007 / 011 | Total loss: 1.706 | Reg loss: 0.042 | Tree loss: 1.706 | Accuracy: 0.424500 | 0.951 sec/iter\n",
      "Epoch: 192 | Batch: 008 / 011 | Total loss: 1.719 | Reg loss: 0.042 | Tree loss: 1.719 | Accuracy: 0.409000 | 0.95 sec/iter\n",
      "Epoch: 192 | Batch: 009 / 011 | Total loss: 1.733 | Reg loss: 0.042 | Tree loss: 1.733 | Accuracy: 0.417500 | 0.95 sec/iter\n",
      "Epoch: 192 | Batch: 010 / 011 | Total loss: 1.687 | Reg loss: 0.042 | Tree loss: 1.687 | Accuracy: 0.419795 | 0.95 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 193 | Batch: 000 / 011 | Total loss: 1.875 | Reg loss: 0.042 | Tree loss: 1.875 | Accuracy: 0.398000 | 0.95 sec/iter\n",
      "Epoch: 193 | Batch: 001 / 011 | Total loss: 1.835 | Reg loss: 0.042 | Tree loss: 1.835 | Accuracy: 0.410500 | 0.95 sec/iter\n",
      "Epoch: 193 | Batch: 002 / 011 | Total loss: 1.799 | Reg loss: 0.042 | Tree loss: 1.799 | Accuracy: 0.419500 | 0.95 sec/iter\n",
      "Epoch: 193 | Batch: 003 / 011 | Total loss: 1.812 | Reg loss: 0.042 | Tree loss: 1.812 | Accuracy: 0.401000 | 0.95 sec/iter\n",
      "Epoch: 193 | Batch: 004 / 011 | Total loss: 1.773 | Reg loss: 0.042 | Tree loss: 1.773 | Accuracy: 0.401000 | 0.95 sec/iter\n",
      "Epoch: 193 | Batch: 005 / 011 | Total loss: 1.723 | Reg loss: 0.042 | Tree loss: 1.723 | Accuracy: 0.429000 | 0.95 sec/iter\n",
      "Epoch: 193 | Batch: 006 / 011 | Total loss: 1.716 | Reg loss: 0.042 | Tree loss: 1.716 | Accuracy: 0.422000 | 0.95 sec/iter\n",
      "Epoch: 193 | Batch: 007 / 011 | Total loss: 1.728 | Reg loss: 0.042 | Tree loss: 1.728 | Accuracy: 0.403000 | 0.95 sec/iter\n",
      "Epoch: 193 | Batch: 008 / 011 | Total loss: 1.685 | Reg loss: 0.042 | Tree loss: 1.685 | Accuracy: 0.435500 | 0.95 sec/iter\n",
      "Epoch: 193 | Batch: 009 / 011 | Total loss: 1.741 | Reg loss: 0.042 | Tree loss: 1.741 | Accuracy: 0.420500 | 0.95 sec/iter\n",
      "Epoch: 193 | Batch: 010 / 011 | Total loss: 1.610 | Reg loss: 0.042 | Tree loss: 1.610 | Accuracy: 0.488055 | 0.95 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 194 | Batch: 000 / 011 | Total loss: 1.847 | Reg loss: 0.042 | Tree loss: 1.847 | Accuracy: 0.411000 | 0.95 sec/iter\n",
      "Epoch: 194 | Batch: 001 / 011 | Total loss: 1.823 | Reg loss: 0.042 | Tree loss: 1.823 | Accuracy: 0.425500 | 0.95 sec/iter\n",
      "Epoch: 194 | Batch: 002 / 011 | Total loss: 1.818 | Reg loss: 0.042 | Tree loss: 1.818 | Accuracy: 0.408500 | 0.95 sec/iter\n",
      "Epoch: 194 | Batch: 003 / 011 | Total loss: 1.825 | Reg loss: 0.042 | Tree loss: 1.825 | Accuracy: 0.398000 | 0.95 sec/iter\n",
      "Epoch: 194 | Batch: 004 / 011 | Total loss: 1.762 | Reg loss: 0.042 | Tree loss: 1.762 | Accuracy: 0.419000 | 0.95 sec/iter\n",
      "Epoch: 194 | Batch: 005 / 011 | Total loss: 1.725 | Reg loss: 0.042 | Tree loss: 1.725 | Accuracy: 0.429500 | 0.95 sec/iter\n",
      "Epoch: 194 | Batch: 006 / 011 | Total loss: 1.741 | Reg loss: 0.042 | Tree loss: 1.741 | Accuracy: 0.401000 | 0.95 sec/iter\n",
      "Epoch: 194 | Batch: 007 / 011 | Total loss: 1.714 | Reg loss: 0.042 | Tree loss: 1.714 | Accuracy: 0.413000 | 0.95 sec/iter\n",
      "Epoch: 194 | Batch: 008 / 011 | Total loss: 1.704 | Reg loss: 0.042 | Tree loss: 1.704 | Accuracy: 0.425500 | 0.95 sec/iter\n",
      "Epoch: 194 | Batch: 009 / 011 | Total loss: 1.698 | Reg loss: 0.042 | Tree loss: 1.698 | Accuracy: 0.434500 | 0.95 sec/iter\n",
      "Epoch: 194 | Batch: 010 / 011 | Total loss: 1.698 | Reg loss: 0.042 | Tree loss: 1.698 | Accuracy: 0.406143 | 0.95 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 195 | Batch: 000 / 011 | Total loss: 1.834 | Reg loss: 0.042 | Tree loss: 1.834 | Accuracy: 0.412500 | 0.95 sec/iter\n",
      "Epoch: 195 | Batch: 001 / 011 | Total loss: 1.856 | Reg loss: 0.042 | Tree loss: 1.856 | Accuracy: 0.389500 | 0.95 sec/iter\n",
      "Epoch: 195 | Batch: 002 / 011 | Total loss: 1.820 | Reg loss: 0.042 | Tree loss: 1.820 | Accuracy: 0.402000 | 0.95 sec/iter\n",
      "Epoch: 195 | Batch: 003 / 011 | Total loss: 1.748 | Reg loss: 0.042 | Tree loss: 1.748 | Accuracy: 0.423000 | 0.95 sec/iter\n",
      "Epoch: 195 | Batch: 004 / 011 | Total loss: 1.757 | Reg loss: 0.042 | Tree loss: 1.757 | Accuracy: 0.409500 | 0.95 sec/iter\n",
      "Epoch: 195 | Batch: 005 / 011 | Total loss: 1.739 | Reg loss: 0.042 | Tree loss: 1.739 | Accuracy: 0.410000 | 0.95 sec/iter\n",
      "Epoch: 195 | Batch: 006 / 011 | Total loss: 1.740 | Reg loss: 0.042 | Tree loss: 1.740 | Accuracy: 0.410500 | 0.95 sec/iter\n",
      "Epoch: 195 | Batch: 007 / 011 | Total loss: 1.745 | Reg loss: 0.042 | Tree loss: 1.745 | Accuracy: 0.424000 | 0.95 sec/iter\n",
      "Epoch: 195 | Batch: 008 / 011 | Total loss: 1.700 | Reg loss: 0.042 | Tree loss: 1.700 | Accuracy: 0.424500 | 0.95 sec/iter\n",
      "Epoch: 195 | Batch: 009 / 011 | Total loss: 1.707 | Reg loss: 0.042 | Tree loss: 1.707 | Accuracy: 0.427000 | 0.95 sec/iter\n",
      "Epoch: 195 | Batch: 010 / 011 | Total loss: 1.769 | Reg loss: 0.042 | Tree loss: 1.769 | Accuracy: 0.409556 | 0.95 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 196 | Batch: 000 / 011 | Total loss: 1.884 | Reg loss: 0.042 | Tree loss: 1.884 | Accuracy: 0.390500 | 0.95 sec/iter\n",
      "Epoch: 196 | Batch: 001 / 011 | Total loss: 1.850 | Reg loss: 0.042 | Tree loss: 1.850 | Accuracy: 0.410500 | 0.95 sec/iter\n",
      "Epoch: 196 | Batch: 002 / 011 | Total loss: 1.796 | Reg loss: 0.042 | Tree loss: 1.796 | Accuracy: 0.414000 | 0.95 sec/iter\n",
      "Epoch: 196 | Batch: 003 / 011 | Total loss: 1.775 | Reg loss: 0.042 | Tree loss: 1.775 | Accuracy: 0.423000 | 0.95 sec/iter\n",
      "Epoch: 196 | Batch: 004 / 011 | Total loss: 1.770 | Reg loss: 0.042 | Tree loss: 1.770 | Accuracy: 0.412000 | 0.95 sec/iter\n",
      "Epoch: 196 | Batch: 005 / 011 | Total loss: 1.740 | Reg loss: 0.042 | Tree loss: 1.740 | Accuracy: 0.422500 | 0.95 sec/iter\n",
      "Epoch: 196 | Batch: 006 / 011 | Total loss: 1.745 | Reg loss: 0.042 | Tree loss: 1.745 | Accuracy: 0.415000 | 0.95 sec/iter\n",
      "Epoch: 196 | Batch: 007 / 011 | Total loss: 1.713 | Reg loss: 0.042 | Tree loss: 1.713 | Accuracy: 0.418000 | 0.95 sec/iter\n",
      "Epoch: 196 | Batch: 008 / 011 | Total loss: 1.708 | Reg loss: 0.042 | Tree loss: 1.708 | Accuracy: 0.413500 | 0.95 sec/iter\n",
      "Epoch: 196 | Batch: 009 / 011 | Total loss: 1.686 | Reg loss: 0.042 | Tree loss: 1.686 | Accuracy: 0.436500 | 0.95 sec/iter\n",
      "Epoch: 196 | Batch: 010 / 011 | Total loss: 1.670 | Reg loss: 0.042 | Tree loss: 1.670 | Accuracy: 0.416382 | 0.95 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 197 | Batch: 000 / 011 | Total loss: 1.858 | Reg loss: 0.042 | Tree loss: 1.858 | Accuracy: 0.384500 | 0.95 sec/iter\n",
      "Epoch: 197 | Batch: 001 / 011 | Total loss: 1.848 | Reg loss: 0.042 | Tree loss: 1.848 | Accuracy: 0.392000 | 0.95 sec/iter\n",
      "Epoch: 197 | Batch: 002 / 011 | Total loss: 1.802 | Reg loss: 0.042 | Tree loss: 1.802 | Accuracy: 0.414000 | 0.95 sec/iter\n",
      "Epoch: 197 | Batch: 003 / 011 | Total loss: 1.806 | Reg loss: 0.042 | Tree loss: 1.806 | Accuracy: 0.417000 | 0.95 sec/iter\n",
      "Epoch: 197 | Batch: 004 / 011 | Total loss: 1.758 | Reg loss: 0.042 | Tree loss: 1.758 | Accuracy: 0.416500 | 0.95 sec/iter\n",
      "Epoch: 197 | Batch: 005 / 011 | Total loss: 1.754 | Reg loss: 0.042 | Tree loss: 1.754 | Accuracy: 0.413500 | 0.95 sec/iter\n",
      "Epoch: 197 | Batch: 006 / 011 | Total loss: 1.715 | Reg loss: 0.042 | Tree loss: 1.715 | Accuracy: 0.423500 | 0.95 sec/iter\n",
      "Epoch: 197 | Batch: 007 / 011 | Total loss: 1.708 | Reg loss: 0.042 | Tree loss: 1.708 | Accuracy: 0.409000 | 0.95 sec/iter\n",
      "Epoch: 197 | Batch: 008 / 011 | Total loss: 1.709 | Reg loss: 0.042 | Tree loss: 1.709 | Accuracy: 0.414000 | 0.95 sec/iter\n",
      "Epoch: 197 | Batch: 009 / 011 | Total loss: 1.692 | Reg loss: 0.042 | Tree loss: 1.692 | Accuracy: 0.441000 | 0.949 sec/iter\n",
      "Epoch: 197 | Batch: 010 / 011 | Total loss: 1.702 | Reg loss: 0.042 | Tree loss: 1.702 | Accuracy: 0.433447 | 0.949 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 198 | Batch: 000 / 011 | Total loss: 1.850 | Reg loss: 0.042 | Tree loss: 1.850 | Accuracy: 0.415000 | 0.949 sec/iter\n",
      "Epoch: 198 | Batch: 001 / 011 | Total loss: 1.844 | Reg loss: 0.042 | Tree loss: 1.844 | Accuracy: 0.390000 | 0.949 sec/iter\n",
      "Epoch: 198 | Batch: 002 / 011 | Total loss: 1.818 | Reg loss: 0.042 | Tree loss: 1.818 | Accuracy: 0.402500 | 0.949 sec/iter\n",
      "Epoch: 198 | Batch: 003 / 011 | Total loss: 1.785 | Reg loss: 0.042 | Tree loss: 1.785 | Accuracy: 0.415500 | 0.949 sec/iter\n",
      "Epoch: 198 | Batch: 004 / 011 | Total loss: 1.785 | Reg loss: 0.042 | Tree loss: 1.785 | Accuracy: 0.401000 | 0.949 sec/iter\n",
      "Epoch: 198 | Batch: 005 / 011 | Total loss: 1.713 | Reg loss: 0.042 | Tree loss: 1.713 | Accuracy: 0.440000 | 0.949 sec/iter\n",
      "Epoch: 198 | Batch: 006 / 011 | Total loss: 1.706 | Reg loss: 0.042 | Tree loss: 1.706 | Accuracy: 0.436500 | 0.949 sec/iter\n",
      "Epoch: 198 | Batch: 007 / 011 | Total loss: 1.714 | Reg loss: 0.042 | Tree loss: 1.714 | Accuracy: 0.424500 | 0.949 sec/iter\n",
      "Epoch: 198 | Batch: 008 / 011 | Total loss: 1.707 | Reg loss: 0.042 | Tree loss: 1.707 | Accuracy: 0.430000 | 0.949 sec/iter\n",
      "Epoch: 198 | Batch: 009 / 011 | Total loss: 1.737 | Reg loss: 0.042 | Tree loss: 1.737 | Accuracy: 0.413500 | 0.949 sec/iter\n",
      "Epoch: 198 | Batch: 010 / 011 | Total loss: 1.730 | Reg loss: 0.042 | Tree loss: 1.730 | Accuracy: 0.375427 | 0.949 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 199 | Batch: 000 / 011 | Total loss: 1.869 | Reg loss: 0.042 | Tree loss: 1.869 | Accuracy: 0.401000 | 0.949 sec/iter\n",
      "Epoch: 199 | Batch: 001 / 011 | Total loss: 1.853 | Reg loss: 0.042 | Tree loss: 1.853 | Accuracy: 0.418500 | 0.949 sec/iter\n",
      "Epoch: 199 | Batch: 002 / 011 | Total loss: 1.789 | Reg loss: 0.042 | Tree loss: 1.789 | Accuracy: 0.416000 | 0.949 sec/iter\n",
      "Epoch: 199 | Batch: 003 / 011 | Total loss: 1.785 | Reg loss: 0.042 | Tree loss: 1.785 | Accuracy: 0.421000 | 0.949 sec/iter\n",
      "Epoch: 199 | Batch: 004 / 011 | Total loss: 1.767 | Reg loss: 0.042 | Tree loss: 1.767 | Accuracy: 0.409500 | 0.949 sec/iter\n",
      "Epoch: 199 | Batch: 005 / 011 | Total loss: 1.737 | Reg loss: 0.042 | Tree loss: 1.737 | Accuracy: 0.411000 | 0.949 sec/iter\n",
      "Epoch: 199 | Batch: 006 / 011 | Total loss: 1.705 | Reg loss: 0.042 | Tree loss: 1.705 | Accuracy: 0.417500 | 0.949 sec/iter\n",
      "Epoch: 199 | Batch: 007 / 011 | Total loss: 1.695 | Reg loss: 0.042 | Tree loss: 1.695 | Accuracy: 0.420000 | 0.949 sec/iter\n",
      "Epoch: 199 | Batch: 008 / 011 | Total loss: 1.716 | Reg loss: 0.042 | Tree loss: 1.716 | Accuracy: 0.406500 | 0.949 sec/iter\n",
      "Epoch: 199 | Batch: 009 / 011 | Total loss: 1.728 | Reg loss: 0.042 | Tree loss: 1.728 | Accuracy: 0.423000 | 0.949 sec/iter\n",
      "Epoch: 199 | Batch: 010 / 011 | Total loss: 1.745 | Reg loss: 0.042 | Tree loss: 1.745 | Accuracy: 0.389078 | 0.949 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 200 | Batch: 000 / 011 | Total loss: 1.898 | Reg loss: 0.042 | Tree loss: 1.898 | Accuracy: 0.389500 | 0.949 sec/iter\n",
      "Epoch: 200 | Batch: 001 / 011 | Total loss: 1.836 | Reg loss: 0.042 | Tree loss: 1.836 | Accuracy: 0.416500 | 0.949 sec/iter\n",
      "Epoch: 200 | Batch: 002 / 011 | Total loss: 1.823 | Reg loss: 0.042 | Tree loss: 1.823 | Accuracy: 0.404500 | 0.949 sec/iter\n",
      "Epoch: 200 | Batch: 003 / 011 | Total loss: 1.781 | Reg loss: 0.042 | Tree loss: 1.781 | Accuracy: 0.404500 | 0.949 sec/iter\n",
      "Epoch: 200 | Batch: 004 / 011 | Total loss: 1.768 | Reg loss: 0.042 | Tree loss: 1.768 | Accuracy: 0.402000 | 0.949 sec/iter\n",
      "Epoch: 200 | Batch: 005 / 011 | Total loss: 1.755 | Reg loss: 0.042 | Tree loss: 1.755 | Accuracy: 0.422000 | 0.949 sec/iter\n",
      "Epoch: 200 | Batch: 006 / 011 | Total loss: 1.690 | Reg loss: 0.042 | Tree loss: 1.690 | Accuracy: 0.444000 | 0.949 sec/iter\n",
      "Epoch: 200 | Batch: 007 / 011 | Total loss: 1.724 | Reg loss: 0.042 | Tree loss: 1.724 | Accuracy: 0.416000 | 0.949 sec/iter\n",
      "Epoch: 200 | Batch: 008 / 011 | Total loss: 1.676 | Reg loss: 0.042 | Tree loss: 1.676 | Accuracy: 0.434500 | 0.949 sec/iter\n",
      "Epoch: 200 | Batch: 009 / 011 | Total loss: 1.689 | Reg loss: 0.042 | Tree loss: 1.689 | Accuracy: 0.427500 | 0.949 sec/iter\n",
      "Epoch: 200 | Batch: 010 / 011 | Total loss: 1.735 | Reg loss: 0.042 | Tree loss: 1.735 | Accuracy: 0.378840 | 0.949 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 201 | Batch: 000 / 011 | Total loss: 1.877 | Reg loss: 0.042 | Tree loss: 1.877 | Accuracy: 0.393000 | 0.949 sec/iter\n",
      "Epoch: 201 | Batch: 001 / 011 | Total loss: 1.849 | Reg loss: 0.042 | Tree loss: 1.849 | Accuracy: 0.412000 | 0.949 sec/iter\n",
      "Epoch: 201 | Batch: 002 / 011 | Total loss: 1.824 | Reg loss: 0.042 | Tree loss: 1.824 | Accuracy: 0.410500 | 0.949 sec/iter\n",
      "Epoch: 201 | Batch: 003 / 011 | Total loss: 1.776 | Reg loss: 0.042 | Tree loss: 1.776 | Accuracy: 0.409000 | 0.949 sec/iter\n",
      "Epoch: 201 | Batch: 004 / 011 | Total loss: 1.742 | Reg loss: 0.042 | Tree loss: 1.742 | Accuracy: 0.415000 | 0.949 sec/iter\n",
      "Epoch: 201 | Batch: 005 / 011 | Total loss: 1.730 | Reg loss: 0.042 | Tree loss: 1.730 | Accuracy: 0.415000 | 0.949 sec/iter\n",
      "Epoch: 201 | Batch: 006 / 011 | Total loss: 1.748 | Reg loss: 0.042 | Tree loss: 1.748 | Accuracy: 0.406000 | 0.949 sec/iter\n",
      "Epoch: 201 | Batch: 007 / 011 | Total loss: 1.717 | Reg loss: 0.042 | Tree loss: 1.717 | Accuracy: 0.411500 | 0.949 sec/iter\n",
      "Epoch: 201 | Batch: 008 / 011 | Total loss: 1.692 | Reg loss: 0.042 | Tree loss: 1.692 | Accuracy: 0.430500 | 0.949 sec/iter\n",
      "Epoch: 201 | Batch: 009 / 011 | Total loss: 1.688 | Reg loss: 0.042 | Tree loss: 1.688 | Accuracy: 0.440500 | 0.949 sec/iter\n",
      "Epoch: 201 | Batch: 010 / 011 | Total loss: 1.715 | Reg loss: 0.042 | Tree loss: 1.715 | Accuracy: 0.378840 | 0.949 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 202 | Batch: 000 / 011 | Total loss: 1.879 | Reg loss: 0.042 | Tree loss: 1.879 | Accuracy: 0.383500 | 0.949 sec/iter\n",
      "Epoch: 202 | Batch: 001 / 011 | Total loss: 1.844 | Reg loss: 0.042 | Tree loss: 1.844 | Accuracy: 0.406000 | 0.949 sec/iter\n",
      "Epoch: 202 | Batch: 002 / 011 | Total loss: 1.821 | Reg loss: 0.042 | Tree loss: 1.821 | Accuracy: 0.411500 | 0.949 sec/iter\n",
      "Epoch: 202 | Batch: 003 / 011 | Total loss: 1.759 | Reg loss: 0.042 | Tree loss: 1.759 | Accuracy: 0.418000 | 0.949 sec/iter\n",
      "Epoch: 202 | Batch: 004 / 011 | Total loss: 1.746 | Reg loss: 0.042 | Tree loss: 1.746 | Accuracy: 0.419000 | 0.949 sec/iter\n",
      "Epoch: 202 | Batch: 005 / 011 | Total loss: 1.752 | Reg loss: 0.042 | Tree loss: 1.752 | Accuracy: 0.425000 | 0.949 sec/iter\n",
      "Epoch: 202 | Batch: 006 / 011 | Total loss: 1.734 | Reg loss: 0.042 | Tree loss: 1.734 | Accuracy: 0.413000 | 0.949 sec/iter\n",
      "Epoch: 202 | Batch: 007 / 011 | Total loss: 1.714 | Reg loss: 0.042 | Tree loss: 1.714 | Accuracy: 0.414500 | 0.949 sec/iter\n",
      "Epoch: 202 | Batch: 008 / 011 | Total loss: 1.711 | Reg loss: 0.042 | Tree loss: 1.711 | Accuracy: 0.424000 | 0.949 sec/iter\n",
      "Epoch: 202 | Batch: 009 / 011 | Total loss: 1.699 | Reg loss: 0.042 | Tree loss: 1.699 | Accuracy: 0.436500 | 0.949 sec/iter\n",
      "Epoch: 202 | Batch: 010 / 011 | Total loss: 1.720 | Reg loss: 0.042 | Tree loss: 1.720 | Accuracy: 0.412969 | 0.949 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 203 | Batch: 000 / 011 | Total loss: 1.845 | Reg loss: 0.042 | Tree loss: 1.845 | Accuracy: 0.416000 | 0.949 sec/iter\n",
      "Epoch: 203 | Batch: 001 / 011 | Total loss: 1.846 | Reg loss: 0.042 | Tree loss: 1.846 | Accuracy: 0.408000 | 0.949 sec/iter\n",
      "Epoch: 203 | Batch: 002 / 011 | Total loss: 1.798 | Reg loss: 0.042 | Tree loss: 1.798 | Accuracy: 0.411000 | 0.948 sec/iter\n",
      "Epoch: 203 | Batch: 003 / 011 | Total loss: 1.793 | Reg loss: 0.042 | Tree loss: 1.793 | Accuracy: 0.406500 | 0.948 sec/iter\n",
      "Epoch: 203 | Batch: 004 / 011 | Total loss: 1.765 | Reg loss: 0.042 | Tree loss: 1.765 | Accuracy: 0.406500 | 0.948 sec/iter\n",
      "Epoch: 203 | Batch: 005 / 011 | Total loss: 1.749 | Reg loss: 0.042 | Tree loss: 1.749 | Accuracy: 0.417500 | 0.948 sec/iter\n",
      "Epoch: 203 | Batch: 006 / 011 | Total loss: 1.734 | Reg loss: 0.042 | Tree loss: 1.734 | Accuracy: 0.408500 | 0.948 sec/iter\n",
      "Epoch: 203 | Batch: 007 / 011 | Total loss: 1.711 | Reg loss: 0.042 | Tree loss: 1.711 | Accuracy: 0.420000 | 0.948 sec/iter\n",
      "Epoch: 203 | Batch: 008 / 011 | Total loss: 1.708 | Reg loss: 0.042 | Tree loss: 1.708 | Accuracy: 0.423500 | 0.948 sec/iter\n",
      "Epoch: 203 | Batch: 009 / 011 | Total loss: 1.695 | Reg loss: 0.042 | Tree loss: 1.695 | Accuracy: 0.424000 | 0.948 sec/iter\n",
      "Epoch: 203 | Batch: 010 / 011 | Total loss: 1.737 | Reg loss: 0.042 | Tree loss: 1.737 | Accuracy: 0.443686 | 0.948 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 204 | Batch: 000 / 011 | Total loss: 1.843 | Reg loss: 0.042 | Tree loss: 1.843 | Accuracy: 0.422500 | 0.948 sec/iter\n",
      "Epoch: 204 | Batch: 001 / 011 | Total loss: 1.833 | Reg loss: 0.042 | Tree loss: 1.833 | Accuracy: 0.408500 | 0.948 sec/iter\n",
      "Epoch: 204 | Batch: 002 / 011 | Total loss: 1.800 | Reg loss: 0.042 | Tree loss: 1.800 | Accuracy: 0.417000 | 0.948 sec/iter\n",
      "Epoch: 204 | Batch: 003 / 011 | Total loss: 1.771 | Reg loss: 0.042 | Tree loss: 1.771 | Accuracy: 0.415500 | 0.948 sec/iter\n",
      "Epoch: 204 | Batch: 004 / 011 | Total loss: 1.763 | Reg loss: 0.042 | Tree loss: 1.763 | Accuracy: 0.425000 | 0.948 sec/iter\n",
      "Epoch: 204 | Batch: 005 / 011 | Total loss: 1.744 | Reg loss: 0.042 | Tree loss: 1.744 | Accuracy: 0.412500 | 0.948 sec/iter\n",
      "Epoch: 204 | Batch: 006 / 011 | Total loss: 1.720 | Reg loss: 0.042 | Tree loss: 1.720 | Accuracy: 0.422000 | 0.948 sec/iter\n",
      "Epoch: 204 | Batch: 007 / 011 | Total loss: 1.720 | Reg loss: 0.042 | Tree loss: 1.720 | Accuracy: 0.419500 | 0.948 sec/iter\n",
      "Epoch: 204 | Batch: 008 / 011 | Total loss: 1.707 | Reg loss: 0.042 | Tree loss: 1.707 | Accuracy: 0.417000 | 0.948 sec/iter\n",
      "Epoch: 204 | Batch: 009 / 011 | Total loss: 1.731 | Reg loss: 0.042 | Tree loss: 1.731 | Accuracy: 0.409500 | 0.948 sec/iter\n",
      "Epoch: 204 | Batch: 010 / 011 | Total loss: 1.706 | Reg loss: 0.042 | Tree loss: 1.706 | Accuracy: 0.402730 | 0.948 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 205 | Batch: 000 / 011 | Total loss: 1.843 | Reg loss: 0.042 | Tree loss: 1.843 | Accuracy: 0.415500 | 0.948 sec/iter\n",
      "Epoch: 205 | Batch: 001 / 011 | Total loss: 1.836 | Reg loss: 0.042 | Tree loss: 1.836 | Accuracy: 0.398500 | 0.948 sec/iter\n",
      "Epoch: 205 | Batch: 002 / 011 | Total loss: 1.810 | Reg loss: 0.042 | Tree loss: 1.810 | Accuracy: 0.405000 | 0.948 sec/iter\n",
      "Epoch: 205 | Batch: 003 / 011 | Total loss: 1.761 | Reg loss: 0.042 | Tree loss: 1.761 | Accuracy: 0.431500 | 0.948 sec/iter\n",
      "Epoch: 205 | Batch: 004 / 011 | Total loss: 1.764 | Reg loss: 0.042 | Tree loss: 1.764 | Accuracy: 0.424000 | 0.948 sec/iter\n",
      "Epoch: 205 | Batch: 005 / 011 | Total loss: 1.742 | Reg loss: 0.042 | Tree loss: 1.742 | Accuracy: 0.415500 | 0.948 sec/iter\n",
      "Epoch: 205 | Batch: 006 / 011 | Total loss: 1.706 | Reg loss: 0.042 | Tree loss: 1.706 | Accuracy: 0.424500 | 0.948 sec/iter\n",
      "Epoch: 205 | Batch: 007 / 011 | Total loss: 1.730 | Reg loss: 0.042 | Tree loss: 1.730 | Accuracy: 0.406500 | 0.948 sec/iter\n",
      "Epoch: 205 | Batch: 008 / 011 | Total loss: 1.739 | Reg loss: 0.042 | Tree loss: 1.739 | Accuracy: 0.415500 | 0.948 sec/iter\n",
      "Epoch: 205 | Batch: 009 / 011 | Total loss: 1.722 | Reg loss: 0.042 | Tree loss: 1.722 | Accuracy: 0.424500 | 0.948 sec/iter\n",
      "Epoch: 205 | Batch: 010 / 011 | Total loss: 1.663 | Reg loss: 0.042 | Tree loss: 1.663 | Accuracy: 0.460751 | 0.948 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 206 | Batch: 000 / 011 | Total loss: 1.864 | Reg loss: 0.042 | Tree loss: 1.864 | Accuracy: 0.409500 | 0.948 sec/iter\n",
      "Epoch: 206 | Batch: 001 / 011 | Total loss: 1.855 | Reg loss: 0.042 | Tree loss: 1.855 | Accuracy: 0.392500 | 0.948 sec/iter\n",
      "Epoch: 206 | Batch: 002 / 011 | Total loss: 1.800 | Reg loss: 0.042 | Tree loss: 1.800 | Accuracy: 0.403000 | 0.948 sec/iter\n",
      "Epoch: 206 | Batch: 003 / 011 | Total loss: 1.786 | Reg loss: 0.042 | Tree loss: 1.786 | Accuracy: 0.424000 | 0.948 sec/iter\n",
      "Epoch: 206 | Batch: 004 / 011 | Total loss: 1.775 | Reg loss: 0.042 | Tree loss: 1.775 | Accuracy: 0.410500 | 0.948 sec/iter\n",
      "Epoch: 206 | Batch: 005 / 011 | Total loss: 1.729 | Reg loss: 0.042 | Tree loss: 1.729 | Accuracy: 0.419500 | 0.948 sec/iter\n",
      "Epoch: 206 | Batch: 006 / 011 | Total loss: 1.702 | Reg loss: 0.042 | Tree loss: 1.702 | Accuracy: 0.419500 | 0.948 sec/iter\n",
      "Epoch: 206 | Batch: 007 / 011 | Total loss: 1.712 | Reg loss: 0.042 | Tree loss: 1.712 | Accuracy: 0.437000 | 0.948 sec/iter\n",
      "Epoch: 206 | Batch: 008 / 011 | Total loss: 1.703 | Reg loss: 0.042 | Tree loss: 1.703 | Accuracy: 0.421500 | 0.948 sec/iter\n",
      "Epoch: 206 | Batch: 009 / 011 | Total loss: 1.706 | Reg loss: 0.042 | Tree loss: 1.706 | Accuracy: 0.416000 | 0.948 sec/iter\n",
      "Epoch: 206 | Batch: 010 / 011 | Total loss: 1.702 | Reg loss: 0.042 | Tree loss: 1.702 | Accuracy: 0.419795 | 0.948 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 207 | Batch: 000 / 011 | Total loss: 1.865 | Reg loss: 0.042 | Tree loss: 1.865 | Accuracy: 0.416000 | 0.948 sec/iter\n",
      "Epoch: 207 | Batch: 001 / 011 | Total loss: 1.847 | Reg loss: 0.042 | Tree loss: 1.847 | Accuracy: 0.404500 | 0.948 sec/iter\n",
      "Epoch: 207 | Batch: 002 / 011 | Total loss: 1.785 | Reg loss: 0.042 | Tree loss: 1.785 | Accuracy: 0.422000 | 0.948 sec/iter\n",
      "Epoch: 207 | Batch: 003 / 011 | Total loss: 1.793 | Reg loss: 0.042 | Tree loss: 1.793 | Accuracy: 0.410500 | 0.948 sec/iter\n",
      "Epoch: 207 | Batch: 004 / 011 | Total loss: 1.748 | Reg loss: 0.042 | Tree loss: 1.748 | Accuracy: 0.425000 | 0.948 sec/iter\n",
      "Epoch: 207 | Batch: 005 / 011 | Total loss: 1.765 | Reg loss: 0.042 | Tree loss: 1.765 | Accuracy: 0.410000 | 0.948 sec/iter\n",
      "Epoch: 207 | Batch: 006 / 011 | Total loss: 1.724 | Reg loss: 0.042 | Tree loss: 1.724 | Accuracy: 0.408500 | 0.948 sec/iter\n",
      "Epoch: 207 | Batch: 007 / 011 | Total loss: 1.717 | Reg loss: 0.042 | Tree loss: 1.717 | Accuracy: 0.424500 | 0.948 sec/iter\n",
      "Epoch: 207 | Batch: 008 / 011 | Total loss: 1.670 | Reg loss: 0.042 | Tree loss: 1.670 | Accuracy: 0.438000 | 0.948 sec/iter\n",
      "Epoch: 207 | Batch: 009 / 011 | Total loss: 1.718 | Reg loss: 0.042 | Tree loss: 1.718 | Accuracy: 0.422500 | 0.948 sec/iter\n",
      "Epoch: 207 | Batch: 010 / 011 | Total loss: 1.650 | Reg loss: 0.042 | Tree loss: 1.650 | Accuracy: 0.433447 | 0.948 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 208 | Batch: 000 / 011 | Total loss: 1.887 | Reg loss: 0.042 | Tree loss: 1.887 | Accuracy: 0.392000 | 0.948 sec/iter\n",
      "Epoch: 208 | Batch: 001 / 011 | Total loss: 1.856 | Reg loss: 0.042 | Tree loss: 1.856 | Accuracy: 0.404000 | 0.948 sec/iter\n",
      "Epoch: 208 | Batch: 002 / 011 | Total loss: 1.798 | Reg loss: 0.042 | Tree loss: 1.798 | Accuracy: 0.404000 | 0.948 sec/iter\n",
      "Epoch: 208 | Batch: 003 / 011 | Total loss: 1.775 | Reg loss: 0.042 | Tree loss: 1.775 | Accuracy: 0.415500 | 0.948 sec/iter\n",
      "Epoch: 208 | Batch: 004 / 011 | Total loss: 1.745 | Reg loss: 0.042 | Tree loss: 1.745 | Accuracy: 0.406000 | 0.948 sec/iter\n",
      "Epoch: 208 | Batch: 005 / 011 | Total loss: 1.753 | Reg loss: 0.042 | Tree loss: 1.753 | Accuracy: 0.422500 | 0.948 sec/iter\n",
      "Epoch: 208 | Batch: 006 / 011 | Total loss: 1.714 | Reg loss: 0.042 | Tree loss: 1.714 | Accuracy: 0.428000 | 0.948 sec/iter\n",
      "Epoch: 208 | Batch: 007 / 011 | Total loss: 1.708 | Reg loss: 0.042 | Tree loss: 1.708 | Accuracy: 0.418000 | 0.948 sec/iter\n",
      "Epoch: 208 | Batch: 008 / 011 | Total loss: 1.703 | Reg loss: 0.042 | Tree loss: 1.703 | Accuracy: 0.419000 | 0.948 sec/iter\n",
      "Epoch: 208 | Batch: 009 / 011 | Total loss: 1.709 | Reg loss: 0.042 | Tree loss: 1.709 | Accuracy: 0.427000 | 0.948 sec/iter\n",
      "Epoch: 208 | Batch: 010 / 011 | Total loss: 1.675 | Reg loss: 0.042 | Tree loss: 1.675 | Accuracy: 0.443686 | 0.948 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 209 | Batch: 000 / 011 | Total loss: 1.887 | Reg loss: 0.042 | Tree loss: 1.887 | Accuracy: 0.396500 | 0.948 sec/iter\n",
      "Epoch: 209 | Batch: 001 / 011 | Total loss: 1.850 | Reg loss: 0.042 | Tree loss: 1.850 | Accuracy: 0.409500 | 0.948 sec/iter\n",
      "Epoch: 209 | Batch: 002 / 011 | Total loss: 1.812 | Reg loss: 0.042 | Tree loss: 1.812 | Accuracy: 0.407500 | 0.948 sec/iter\n",
      "Epoch: 209 | Batch: 003 / 011 | Total loss: 1.778 | Reg loss: 0.042 | Tree loss: 1.778 | Accuracy: 0.423500 | 0.948 sec/iter\n",
      "Epoch: 209 | Batch: 004 / 011 | Total loss: 1.750 | Reg loss: 0.042 | Tree loss: 1.750 | Accuracy: 0.423500 | 0.948 sec/iter\n",
      "Epoch: 209 | Batch: 005 / 011 | Total loss: 1.750 | Reg loss: 0.042 | Tree loss: 1.750 | Accuracy: 0.422500 | 0.948 sec/iter\n",
      "Epoch: 209 | Batch: 006 / 011 | Total loss: 1.706 | Reg loss: 0.042 | Tree loss: 1.706 | Accuracy: 0.436000 | 0.947 sec/iter\n",
      "Epoch: 209 | Batch: 007 / 011 | Total loss: 1.724 | Reg loss: 0.042 | Tree loss: 1.724 | Accuracy: 0.410000 | 0.947 sec/iter\n",
      "Epoch: 209 | Batch: 008 / 011 | Total loss: 1.694 | Reg loss: 0.042 | Tree loss: 1.694 | Accuracy: 0.419000 | 0.947 sec/iter\n",
      "Epoch: 209 | Batch: 009 / 011 | Total loss: 1.685 | Reg loss: 0.042 | Tree loss: 1.685 | Accuracy: 0.434500 | 0.947 sec/iter\n",
      "Epoch: 209 | Batch: 010 / 011 | Total loss: 1.667 | Reg loss: 0.042 | Tree loss: 1.667 | Accuracy: 0.453925 | 0.947 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 210 | Batch: 000 / 011 | Total loss: 1.864 | Reg loss: 0.042 | Tree loss: 1.864 | Accuracy: 0.390500 | 0.947 sec/iter\n",
      "Epoch: 210 | Batch: 001 / 011 | Total loss: 1.856 | Reg loss: 0.042 | Tree loss: 1.856 | Accuracy: 0.400000 | 0.947 sec/iter\n",
      "Epoch: 210 | Batch: 002 / 011 | Total loss: 1.801 | Reg loss: 0.042 | Tree loss: 1.801 | Accuracy: 0.403000 | 0.947 sec/iter\n",
      "Epoch: 210 | Batch: 003 / 011 | Total loss: 1.776 | Reg loss: 0.042 | Tree loss: 1.776 | Accuracy: 0.426500 | 0.947 sec/iter\n",
      "Epoch: 210 | Batch: 004 / 011 | Total loss: 1.751 | Reg loss: 0.042 | Tree loss: 1.751 | Accuracy: 0.419500 | 0.947 sec/iter\n",
      "Epoch: 210 | Batch: 005 / 011 | Total loss: 1.704 | Reg loss: 0.042 | Tree loss: 1.704 | Accuracy: 0.433500 | 0.947 sec/iter\n",
      "Epoch: 210 | Batch: 006 / 011 | Total loss: 1.728 | Reg loss: 0.042 | Tree loss: 1.728 | Accuracy: 0.407000 | 0.947 sec/iter\n",
      "Epoch: 210 | Batch: 007 / 011 | Total loss: 1.709 | Reg loss: 0.042 | Tree loss: 1.709 | Accuracy: 0.426000 | 0.947 sec/iter\n",
      "Epoch: 210 | Batch: 008 / 011 | Total loss: 1.723 | Reg loss: 0.042 | Tree loss: 1.723 | Accuracy: 0.410500 | 0.947 sec/iter\n",
      "Epoch: 210 | Batch: 009 / 011 | Total loss: 1.718 | Reg loss: 0.042 | Tree loss: 1.718 | Accuracy: 0.417500 | 0.947 sec/iter\n",
      "Epoch: 210 | Batch: 010 / 011 | Total loss: 1.707 | Reg loss: 0.042 | Tree loss: 1.707 | Accuracy: 0.450512 | 0.947 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 211 | Batch: 000 / 011 | Total loss: 1.857 | Reg loss: 0.042 | Tree loss: 1.857 | Accuracy: 0.417500 | 0.947 sec/iter\n",
      "Epoch: 211 | Batch: 001 / 011 | Total loss: 1.810 | Reg loss: 0.042 | Tree loss: 1.810 | Accuracy: 0.429500 | 0.947 sec/iter\n",
      "Epoch: 211 | Batch: 002 / 011 | Total loss: 1.793 | Reg loss: 0.042 | Tree loss: 1.793 | Accuracy: 0.431500 | 0.947 sec/iter\n",
      "Epoch: 211 | Batch: 003 / 011 | Total loss: 1.815 | Reg loss: 0.042 | Tree loss: 1.815 | Accuracy: 0.400500 | 0.947 sec/iter\n",
      "Epoch: 211 | Batch: 004 / 011 | Total loss: 1.763 | Reg loss: 0.042 | Tree loss: 1.763 | Accuracy: 0.407500 | 0.947 sec/iter\n",
      "Epoch: 211 | Batch: 005 / 011 | Total loss: 1.757 | Reg loss: 0.042 | Tree loss: 1.757 | Accuracy: 0.405500 | 0.947 sec/iter\n",
      "Epoch: 211 | Batch: 006 / 011 | Total loss: 1.730 | Reg loss: 0.042 | Tree loss: 1.730 | Accuracy: 0.415000 | 0.947 sec/iter\n",
      "Epoch: 211 | Batch: 007 / 011 | Total loss: 1.690 | Reg loss: 0.042 | Tree loss: 1.690 | Accuracy: 0.438000 | 0.947 sec/iter\n",
      "Epoch: 211 | Batch: 008 / 011 | Total loss: 1.726 | Reg loss: 0.042 | Tree loss: 1.726 | Accuracy: 0.409500 | 0.947 sec/iter\n",
      "Epoch: 211 | Batch: 009 / 011 | Total loss: 1.697 | Reg loss: 0.042 | Tree loss: 1.697 | Accuracy: 0.418000 | 0.947 sec/iter\n",
      "Epoch: 211 | Batch: 010 / 011 | Total loss: 1.667 | Reg loss: 0.042 | Tree loss: 1.667 | Accuracy: 0.423208 | 0.947 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 212 | Batch: 000 / 011 | Total loss: 1.870 | Reg loss: 0.042 | Tree loss: 1.870 | Accuracy: 0.412500 | 0.947 sec/iter\n",
      "Epoch: 212 | Batch: 001 / 011 | Total loss: 1.863 | Reg loss: 0.042 | Tree loss: 1.863 | Accuracy: 0.384000 | 0.947 sec/iter\n",
      "Epoch: 212 | Batch: 002 / 011 | Total loss: 1.810 | Reg loss: 0.042 | Tree loss: 1.810 | Accuracy: 0.396500 | 0.947 sec/iter\n",
      "Epoch: 212 | Batch: 003 / 011 | Total loss: 1.775 | Reg loss: 0.042 | Tree loss: 1.775 | Accuracy: 0.410500 | 0.947 sec/iter\n",
      "Epoch: 212 | Batch: 004 / 011 | Total loss: 1.744 | Reg loss: 0.042 | Tree loss: 1.744 | Accuracy: 0.418500 | 0.947 sec/iter\n",
      "Epoch: 212 | Batch: 005 / 011 | Total loss: 1.723 | Reg loss: 0.042 | Tree loss: 1.723 | Accuracy: 0.422500 | 0.947 sec/iter\n",
      "Epoch: 212 | Batch: 006 / 011 | Total loss: 1.732 | Reg loss: 0.042 | Tree loss: 1.732 | Accuracy: 0.417000 | 0.947 sec/iter\n",
      "Epoch: 212 | Batch: 007 / 011 | Total loss: 1.716 | Reg loss: 0.042 | Tree loss: 1.716 | Accuracy: 0.426500 | 0.947 sec/iter\n",
      "Epoch: 212 | Batch: 008 / 011 | Total loss: 1.714 | Reg loss: 0.042 | Tree loss: 1.714 | Accuracy: 0.417500 | 0.947 sec/iter\n",
      "Epoch: 212 | Batch: 009 / 011 | Total loss: 1.696 | Reg loss: 0.042 | Tree loss: 1.696 | Accuracy: 0.424000 | 0.947 sec/iter\n",
      "Epoch: 212 | Batch: 010 / 011 | Total loss: 1.667 | Reg loss: 0.042 | Tree loss: 1.667 | Accuracy: 0.412969 | 0.947 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 213 | Batch: 000 / 011 | Total loss: 1.853 | Reg loss: 0.042 | Tree loss: 1.853 | Accuracy: 0.394500 | 0.947 sec/iter\n",
      "Epoch: 213 | Batch: 001 / 011 | Total loss: 1.829 | Reg loss: 0.042 | Tree loss: 1.829 | Accuracy: 0.412000 | 0.947 sec/iter\n",
      "Epoch: 213 | Batch: 002 / 011 | Total loss: 1.823 | Reg loss: 0.042 | Tree loss: 1.823 | Accuracy: 0.410000 | 0.947 sec/iter\n",
      "Epoch: 213 | Batch: 003 / 011 | Total loss: 1.788 | Reg loss: 0.042 | Tree loss: 1.788 | Accuracy: 0.416500 | 0.947 sec/iter\n",
      "Epoch: 213 | Batch: 004 / 011 | Total loss: 1.749 | Reg loss: 0.042 | Tree loss: 1.749 | Accuracy: 0.419000 | 0.947 sec/iter\n",
      "Epoch: 213 | Batch: 005 / 011 | Total loss: 1.725 | Reg loss: 0.042 | Tree loss: 1.725 | Accuracy: 0.432000 | 0.947 sec/iter\n",
      "Epoch: 213 | Batch: 006 / 011 | Total loss: 1.723 | Reg loss: 0.042 | Tree loss: 1.723 | Accuracy: 0.415500 | 0.947 sec/iter\n",
      "Epoch: 213 | Batch: 007 / 011 | Total loss: 1.683 | Reg loss: 0.042 | Tree loss: 1.683 | Accuracy: 0.435500 | 0.947 sec/iter\n",
      "Epoch: 213 | Batch: 008 / 011 | Total loss: 1.698 | Reg loss: 0.042 | Tree loss: 1.698 | Accuracy: 0.420500 | 0.947 sec/iter\n",
      "Epoch: 213 | Batch: 009 / 011 | Total loss: 1.740 | Reg loss: 0.042 | Tree loss: 1.740 | Accuracy: 0.406500 | 0.947 sec/iter\n",
      "Epoch: 213 | Batch: 010 / 011 | Total loss: 1.729 | Reg loss: 0.042 | Tree loss: 1.729 | Accuracy: 0.406143 | 0.947 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 214 | Batch: 000 / 011 | Total loss: 1.854 | Reg loss: 0.042 | Tree loss: 1.854 | Accuracy: 0.399500 | 0.947 sec/iter\n",
      "Epoch: 214 | Batch: 001 / 011 | Total loss: 1.842 | Reg loss: 0.042 | Tree loss: 1.842 | Accuracy: 0.415500 | 0.947 sec/iter\n",
      "Epoch: 214 | Batch: 002 / 011 | Total loss: 1.816 | Reg loss: 0.042 | Tree loss: 1.816 | Accuracy: 0.410500 | 0.947 sec/iter\n",
      "Epoch: 214 | Batch: 003 / 011 | Total loss: 1.779 | Reg loss: 0.042 | Tree loss: 1.779 | Accuracy: 0.442500 | 0.947 sec/iter\n",
      "Epoch: 214 | Batch: 004 / 011 | Total loss: 1.747 | Reg loss: 0.042 | Tree loss: 1.747 | Accuracy: 0.423500 | 0.947 sec/iter\n",
      "Epoch: 214 | Batch: 005 / 011 | Total loss: 1.710 | Reg loss: 0.042 | Tree loss: 1.710 | Accuracy: 0.428500 | 0.947 sec/iter\n",
      "Epoch: 214 | Batch: 006 / 011 | Total loss: 1.723 | Reg loss: 0.042 | Tree loss: 1.723 | Accuracy: 0.408000 | 0.947 sec/iter\n",
      "Epoch: 214 | Batch: 007 / 011 | Total loss: 1.710 | Reg loss: 0.042 | Tree loss: 1.710 | Accuracy: 0.423000 | 0.947 sec/iter\n",
      "Epoch: 214 | Batch: 008 / 011 | Total loss: 1.709 | Reg loss: 0.042 | Tree loss: 1.709 | Accuracy: 0.428500 | 0.947 sec/iter\n",
      "Epoch: 214 | Batch: 009 / 011 | Total loss: 1.717 | Reg loss: 0.042 | Tree loss: 1.717 | Accuracy: 0.407500 | 0.947 sec/iter\n",
      "Epoch: 214 | Batch: 010 / 011 | Total loss: 1.777 | Reg loss: 0.042 | Tree loss: 1.777 | Accuracy: 0.416382 | 0.947 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 215 | Batch: 000 / 011 | Total loss: 1.868 | Reg loss: 0.042 | Tree loss: 1.868 | Accuracy: 0.393500 | 0.947 sec/iter\n",
      "Epoch: 215 | Batch: 001 / 011 | Total loss: 1.852 | Reg loss: 0.042 | Tree loss: 1.852 | Accuracy: 0.398500 | 0.947 sec/iter\n",
      "Epoch: 215 | Batch: 002 / 011 | Total loss: 1.805 | Reg loss: 0.042 | Tree loss: 1.805 | Accuracy: 0.423000 | 0.947 sec/iter\n",
      "Epoch: 215 | Batch: 003 / 011 | Total loss: 1.791 | Reg loss: 0.042 | Tree loss: 1.791 | Accuracy: 0.420000 | 0.947 sec/iter\n",
      "Epoch: 215 | Batch: 004 / 011 | Total loss: 1.764 | Reg loss: 0.042 | Tree loss: 1.764 | Accuracy: 0.412500 | 0.947 sec/iter\n",
      "Epoch: 215 | Batch: 005 / 011 | Total loss: 1.735 | Reg loss: 0.042 | Tree loss: 1.735 | Accuracy: 0.417000 | 0.947 sec/iter\n",
      "Epoch: 215 | Batch: 006 / 011 | Total loss: 1.705 | Reg loss: 0.042 | Tree loss: 1.705 | Accuracy: 0.418000 | 0.947 sec/iter\n",
      "Epoch: 215 | Batch: 007 / 011 | Total loss: 1.704 | Reg loss: 0.042 | Tree loss: 1.704 | Accuracy: 0.437500 | 0.947 sec/iter\n",
      "Epoch: 215 | Batch: 008 / 011 | Total loss: 1.706 | Reg loss: 0.042 | Tree loss: 1.706 | Accuracy: 0.424500 | 0.947 sec/iter\n",
      "Epoch: 215 | Batch: 009 / 011 | Total loss: 1.680 | Reg loss: 0.042 | Tree loss: 1.680 | Accuracy: 0.433000 | 0.947 sec/iter\n",
      "Epoch: 215 | Batch: 010 / 011 | Total loss: 1.681 | Reg loss: 0.042 | Tree loss: 1.681 | Accuracy: 0.436860 | 0.946 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 216 | Batch: 000 / 011 | Total loss: 1.848 | Reg loss: 0.042 | Tree loss: 1.848 | Accuracy: 0.427500 | 0.946 sec/iter\n",
      "Epoch: 216 | Batch: 001 / 011 | Total loss: 1.838 | Reg loss: 0.042 | Tree loss: 1.838 | Accuracy: 0.410000 | 0.946 sec/iter\n",
      "Epoch: 216 | Batch: 002 / 011 | Total loss: 1.802 | Reg loss: 0.042 | Tree loss: 1.802 | Accuracy: 0.410500 | 0.946 sec/iter\n",
      "Epoch: 216 | Batch: 003 / 011 | Total loss: 1.765 | Reg loss: 0.042 | Tree loss: 1.765 | Accuracy: 0.403500 | 0.946 sec/iter\n",
      "Epoch: 216 | Batch: 004 / 011 | Total loss: 1.748 | Reg loss: 0.042 | Tree loss: 1.748 | Accuracy: 0.421000 | 0.946 sec/iter\n",
      "Epoch: 216 | Batch: 005 / 011 | Total loss: 1.751 | Reg loss: 0.042 | Tree loss: 1.751 | Accuracy: 0.401500 | 0.946 sec/iter\n",
      "Epoch: 216 | Batch: 006 / 011 | Total loss: 1.705 | Reg loss: 0.042 | Tree loss: 1.705 | Accuracy: 0.428500 | 0.946 sec/iter\n",
      "Epoch: 216 | Batch: 007 / 011 | Total loss: 1.707 | Reg loss: 0.042 | Tree loss: 1.707 | Accuracy: 0.414000 | 0.946 sec/iter\n",
      "Epoch: 216 | Batch: 008 / 011 | Total loss: 1.730 | Reg loss: 0.042 | Tree loss: 1.730 | Accuracy: 0.414000 | 0.946 sec/iter\n",
      "Epoch: 216 | Batch: 009 / 011 | Total loss: 1.711 | Reg loss: 0.042 | Tree loss: 1.711 | Accuracy: 0.409500 | 0.946 sec/iter\n",
      "Epoch: 216 | Batch: 010 / 011 | Total loss: 1.671 | Reg loss: 0.042 | Tree loss: 1.671 | Accuracy: 0.467577 | 0.946 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 217 | Batch: 000 / 011 | Total loss: 1.876 | Reg loss: 0.042 | Tree loss: 1.876 | Accuracy: 0.401500 | 0.946 sec/iter\n",
      "Epoch: 217 | Batch: 001 / 011 | Total loss: 1.842 | Reg loss: 0.042 | Tree loss: 1.842 | Accuracy: 0.403000 | 0.946 sec/iter\n",
      "Epoch: 217 | Batch: 002 / 011 | Total loss: 1.790 | Reg loss: 0.042 | Tree loss: 1.790 | Accuracy: 0.425500 | 0.946 sec/iter\n",
      "Epoch: 217 | Batch: 003 / 011 | Total loss: 1.771 | Reg loss: 0.042 | Tree loss: 1.771 | Accuracy: 0.432500 | 0.946 sec/iter\n",
      "Epoch: 217 | Batch: 004 / 011 | Total loss: 1.747 | Reg loss: 0.042 | Tree loss: 1.747 | Accuracy: 0.429500 | 0.946 sec/iter\n",
      "Epoch: 217 | Batch: 005 / 011 | Total loss: 1.739 | Reg loss: 0.042 | Tree loss: 1.739 | Accuracy: 0.399000 | 0.946 sec/iter\n",
      "Epoch: 217 | Batch: 006 / 011 | Total loss: 1.707 | Reg loss: 0.042 | Tree loss: 1.707 | Accuracy: 0.413500 | 0.946 sec/iter\n",
      "Epoch: 217 | Batch: 007 / 011 | Total loss: 1.714 | Reg loss: 0.042 | Tree loss: 1.714 | Accuracy: 0.425500 | 0.946 sec/iter\n",
      "Epoch: 217 | Batch: 008 / 011 | Total loss: 1.701 | Reg loss: 0.042 | Tree loss: 1.701 | Accuracy: 0.427500 | 0.946 sec/iter\n",
      "Epoch: 217 | Batch: 009 / 011 | Total loss: 1.724 | Reg loss: 0.042 | Tree loss: 1.724 | Accuracy: 0.415500 | 0.946 sec/iter\n",
      "Epoch: 217 | Batch: 010 / 011 | Total loss: 1.698 | Reg loss: 0.042 | Tree loss: 1.698 | Accuracy: 0.426621 | 0.946 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 218 | Batch: 000 / 011 | Total loss: 1.873 | Reg loss: 0.042 | Tree loss: 1.873 | Accuracy: 0.402500 | 0.946 sec/iter\n",
      "Epoch: 218 | Batch: 001 / 011 | Total loss: 1.816 | Reg loss: 0.042 | Tree loss: 1.816 | Accuracy: 0.416000 | 0.946 sec/iter\n",
      "Epoch: 218 | Batch: 002 / 011 | Total loss: 1.793 | Reg loss: 0.042 | Tree loss: 1.793 | Accuracy: 0.438000 | 0.946 sec/iter\n",
      "Epoch: 218 | Batch: 003 / 011 | Total loss: 1.795 | Reg loss: 0.042 | Tree loss: 1.795 | Accuracy: 0.403500 | 0.946 sec/iter\n",
      "Epoch: 218 | Batch: 004 / 011 | Total loss: 1.774 | Reg loss: 0.042 | Tree loss: 1.774 | Accuracy: 0.389000 | 0.946 sec/iter\n",
      "Epoch: 218 | Batch: 005 / 011 | Total loss: 1.735 | Reg loss: 0.042 | Tree loss: 1.735 | Accuracy: 0.429000 | 0.946 sec/iter\n",
      "Epoch: 218 | Batch: 006 / 011 | Total loss: 1.722 | Reg loss: 0.042 | Tree loss: 1.722 | Accuracy: 0.416500 | 0.946 sec/iter\n",
      "Epoch: 218 | Batch: 007 / 011 | Total loss: 1.701 | Reg loss: 0.042 | Tree loss: 1.701 | Accuracy: 0.426000 | 0.946 sec/iter\n",
      "Epoch: 218 | Batch: 008 / 011 | Total loss: 1.706 | Reg loss: 0.042 | Tree loss: 1.706 | Accuracy: 0.404500 | 0.946 sec/iter\n",
      "Epoch: 218 | Batch: 009 / 011 | Total loss: 1.698 | Reg loss: 0.042 | Tree loss: 1.698 | Accuracy: 0.414000 | 0.946 sec/iter\n",
      "Epoch: 218 | Batch: 010 / 011 | Total loss: 1.635 | Reg loss: 0.042 | Tree loss: 1.635 | Accuracy: 0.453925 | 0.946 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 219 | Batch: 000 / 011 | Total loss: 1.842 | Reg loss: 0.042 | Tree loss: 1.842 | Accuracy: 0.409000 | 0.946 sec/iter\n",
      "Epoch: 219 | Batch: 001 / 011 | Total loss: 1.871 | Reg loss: 0.042 | Tree loss: 1.871 | Accuracy: 0.383500 | 0.946 sec/iter\n",
      "Epoch: 219 | Batch: 002 / 011 | Total loss: 1.788 | Reg loss: 0.042 | Tree loss: 1.788 | Accuracy: 0.425000 | 0.946 sec/iter\n",
      "Epoch: 219 | Batch: 003 / 011 | Total loss: 1.775 | Reg loss: 0.042 | Tree loss: 1.775 | Accuracy: 0.411500 | 0.946 sec/iter\n",
      "Epoch: 219 | Batch: 004 / 011 | Total loss: 1.742 | Reg loss: 0.042 | Tree loss: 1.742 | Accuracy: 0.428000 | 0.946 sec/iter\n",
      "Epoch: 219 | Batch: 005 / 011 | Total loss: 1.753 | Reg loss: 0.042 | Tree loss: 1.753 | Accuracy: 0.413500 | 0.946 sec/iter\n",
      "Epoch: 219 | Batch: 006 / 011 | Total loss: 1.720 | Reg loss: 0.042 | Tree loss: 1.720 | Accuracy: 0.420500 | 0.946 sec/iter\n",
      "Epoch: 219 | Batch: 007 / 011 | Total loss: 1.707 | Reg loss: 0.042 | Tree loss: 1.707 | Accuracy: 0.436000 | 0.946 sec/iter\n",
      "Epoch: 219 | Batch: 008 / 011 | Total loss: 1.693 | Reg loss: 0.042 | Tree loss: 1.693 | Accuracy: 0.423000 | 0.946 sec/iter\n",
      "Epoch: 219 | Batch: 009 / 011 | Total loss: 1.715 | Reg loss: 0.042 | Tree loss: 1.715 | Accuracy: 0.423000 | 0.946 sec/iter\n",
      "Epoch: 219 | Batch: 010 / 011 | Total loss: 1.738 | Reg loss: 0.042 | Tree loss: 1.738 | Accuracy: 0.395904 | 0.946 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 220 | Batch: 000 / 011 | Total loss: 1.850 | Reg loss: 0.042 | Tree loss: 1.850 | Accuracy: 0.401500 | 0.946 sec/iter\n",
      "Epoch: 220 | Batch: 001 / 011 | Total loss: 1.828 | Reg loss: 0.042 | Tree loss: 1.828 | Accuracy: 0.415500 | 0.946 sec/iter\n",
      "Epoch: 220 | Batch: 002 / 011 | Total loss: 1.814 | Reg loss: 0.042 | Tree loss: 1.814 | Accuracy: 0.399000 | 0.946 sec/iter\n",
      "Epoch: 220 | Batch: 003 / 011 | Total loss: 1.799 | Reg loss: 0.042 | Tree loss: 1.799 | Accuracy: 0.412500 | 0.946 sec/iter\n",
      "Epoch: 220 | Batch: 004 / 011 | Total loss: 1.741 | Reg loss: 0.042 | Tree loss: 1.741 | Accuracy: 0.429000 | 0.946 sec/iter\n",
      "Epoch: 220 | Batch: 005 / 011 | Total loss: 1.745 | Reg loss: 0.042 | Tree loss: 1.745 | Accuracy: 0.408000 | 0.946 sec/iter\n",
      "Epoch: 220 | Batch: 006 / 011 | Total loss: 1.729 | Reg loss: 0.042 | Tree loss: 1.729 | Accuracy: 0.415000 | 0.946 sec/iter\n",
      "Epoch: 220 | Batch: 007 / 011 | Total loss: 1.705 | Reg loss: 0.042 | Tree loss: 1.705 | Accuracy: 0.427000 | 0.946 sec/iter\n",
      "Epoch: 220 | Batch: 008 / 011 | Total loss: 1.687 | Reg loss: 0.042 | Tree loss: 1.687 | Accuracy: 0.426000 | 0.946 sec/iter\n",
      "Epoch: 220 | Batch: 009 / 011 | Total loss: 1.720 | Reg loss: 0.042 | Tree loss: 1.720 | Accuracy: 0.423500 | 0.946 sec/iter\n",
      "Epoch: 220 | Batch: 010 / 011 | Total loss: 1.668 | Reg loss: 0.042 | Tree loss: 1.668 | Accuracy: 0.453925 | 0.946 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 221 | Batch: 000 / 011 | Total loss: 1.848 | Reg loss: 0.042 | Tree loss: 1.848 | Accuracy: 0.420500 | 0.946 sec/iter\n",
      "Epoch: 221 | Batch: 001 / 011 | Total loss: 1.824 | Reg loss: 0.042 | Tree loss: 1.824 | Accuracy: 0.411500 | 0.946 sec/iter\n",
      "Epoch: 221 | Batch: 002 / 011 | Total loss: 1.821 | Reg loss: 0.042 | Tree loss: 1.821 | Accuracy: 0.393500 | 0.946 sec/iter\n",
      "Epoch: 221 | Batch: 003 / 011 | Total loss: 1.793 | Reg loss: 0.042 | Tree loss: 1.793 | Accuracy: 0.411500 | 0.946 sec/iter\n",
      "Epoch: 221 | Batch: 004 / 011 | Total loss: 1.749 | Reg loss: 0.042 | Tree loss: 1.749 | Accuracy: 0.421000 | 0.946 sec/iter\n",
      "Epoch: 221 | Batch: 005 / 011 | Total loss: 1.722 | Reg loss: 0.042 | Tree loss: 1.722 | Accuracy: 0.418500 | 0.946 sec/iter\n",
      "Epoch: 221 | Batch: 006 / 011 | Total loss: 1.728 | Reg loss: 0.042 | Tree loss: 1.728 | Accuracy: 0.404500 | 0.946 sec/iter\n",
      "Epoch: 221 | Batch: 007 / 011 | Total loss: 1.706 | Reg loss: 0.042 | Tree loss: 1.706 | Accuracy: 0.430500 | 0.946 sec/iter\n",
      "Epoch: 221 | Batch: 008 / 011 | Total loss: 1.698 | Reg loss: 0.042 | Tree loss: 1.698 | Accuracy: 0.438500 | 0.946 sec/iter\n",
      "Epoch: 221 | Batch: 009 / 011 | Total loss: 1.708 | Reg loss: 0.042 | Tree loss: 1.708 | Accuracy: 0.408000 | 0.946 sec/iter\n",
      "Epoch: 221 | Batch: 010 / 011 | Total loss: 1.679 | Reg loss: 0.042 | Tree loss: 1.679 | Accuracy: 0.416382 | 0.946 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 222 | Batch: 000 / 011 | Total loss: 1.856 | Reg loss: 0.042 | Tree loss: 1.856 | Accuracy: 0.415500 | 0.946 sec/iter\n",
      "Epoch: 222 | Batch: 001 / 011 | Total loss: 1.830 | Reg loss: 0.042 | Tree loss: 1.830 | Accuracy: 0.418000 | 0.946 sec/iter\n",
      "Epoch: 222 | Batch: 002 / 011 | Total loss: 1.827 | Reg loss: 0.042 | Tree loss: 1.827 | Accuracy: 0.398000 | 0.946 sec/iter\n",
      "Epoch: 222 | Batch: 003 / 011 | Total loss: 1.768 | Reg loss: 0.042 | Tree loss: 1.768 | Accuracy: 0.422500 | 0.946 sec/iter\n",
      "Epoch: 222 | Batch: 004 / 011 | Total loss: 1.778 | Reg loss: 0.042 | Tree loss: 1.778 | Accuracy: 0.402000 | 0.946 sec/iter\n",
      "Epoch: 222 | Batch: 005 / 011 | Total loss: 1.750 | Reg loss: 0.042 | Tree loss: 1.750 | Accuracy: 0.412000 | 0.946 sec/iter\n",
      "Epoch: 222 | Batch: 006 / 011 | Total loss: 1.718 | Reg loss: 0.042 | Tree loss: 1.718 | Accuracy: 0.414500 | 0.946 sec/iter\n",
      "Epoch: 222 | Batch: 007 / 011 | Total loss: 1.693 | Reg loss: 0.042 | Tree loss: 1.693 | Accuracy: 0.424500 | 0.946 sec/iter\n",
      "Epoch: 222 | Batch: 008 / 011 | Total loss: 1.700 | Reg loss: 0.042 | Tree loss: 1.700 | Accuracy: 0.430500 | 0.946 sec/iter\n",
      "Epoch: 222 | Batch: 009 / 011 | Total loss: 1.698 | Reg loss: 0.042 | Tree loss: 1.698 | Accuracy: 0.428000 | 0.946 sec/iter\n",
      "Epoch: 222 | Batch: 010 / 011 | Total loss: 1.664 | Reg loss: 0.042 | Tree loss: 1.664 | Accuracy: 0.457338 | 0.945 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 223 | Batch: 000 / 011 | Total loss: 1.878 | Reg loss: 0.042 | Tree loss: 1.878 | Accuracy: 0.403000 | 0.946 sec/iter\n",
      "Epoch: 223 | Batch: 001 / 011 | Total loss: 1.831 | Reg loss: 0.042 | Tree loss: 1.831 | Accuracy: 0.423500 | 0.945 sec/iter\n",
      "Epoch: 223 | Batch: 002 / 011 | Total loss: 1.819 | Reg loss: 0.042 | Tree loss: 1.819 | Accuracy: 0.405000 | 0.945 sec/iter\n",
      "Epoch: 223 | Batch: 003 / 011 | Total loss: 1.765 | Reg loss: 0.042 | Tree loss: 1.765 | Accuracy: 0.415000 | 0.945 sec/iter\n",
      "Epoch: 223 | Batch: 004 / 011 | Total loss: 1.756 | Reg loss: 0.042 | Tree loss: 1.756 | Accuracy: 0.406500 | 0.945 sec/iter\n",
      "Epoch: 223 | Batch: 005 / 011 | Total loss: 1.761 | Reg loss: 0.042 | Tree loss: 1.761 | Accuracy: 0.392000 | 0.945 sec/iter\n",
      "Epoch: 223 | Batch: 006 / 011 | Total loss: 1.733 | Reg loss: 0.042 | Tree loss: 1.733 | Accuracy: 0.411000 | 0.945 sec/iter\n",
      "Epoch: 223 | Batch: 007 / 011 | Total loss: 1.696 | Reg loss: 0.042 | Tree loss: 1.696 | Accuracy: 0.430000 | 0.945 sec/iter\n",
      "Epoch: 223 | Batch: 008 / 011 | Total loss: 1.650 | Reg loss: 0.042 | Tree loss: 1.650 | Accuracy: 0.454500 | 0.945 sec/iter\n",
      "Epoch: 223 | Batch: 009 / 011 | Total loss: 1.700 | Reg loss: 0.042 | Tree loss: 1.700 | Accuracy: 0.414500 | 0.945 sec/iter\n",
      "Epoch: 223 | Batch: 010 / 011 | Total loss: 1.682 | Reg loss: 0.042 | Tree loss: 1.682 | Accuracy: 0.436860 | 0.945 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 224 | Batch: 000 / 011 | Total loss: 1.847 | Reg loss: 0.042 | Tree loss: 1.847 | Accuracy: 0.414000 | 0.945 sec/iter\n",
      "Epoch: 224 | Batch: 001 / 011 | Total loss: 1.826 | Reg loss: 0.042 | Tree loss: 1.826 | Accuracy: 0.418000 | 0.945 sec/iter\n",
      "Epoch: 224 | Batch: 002 / 011 | Total loss: 1.798 | Reg loss: 0.042 | Tree loss: 1.798 | Accuracy: 0.419500 | 0.945 sec/iter\n",
      "Epoch: 224 | Batch: 003 / 011 | Total loss: 1.788 | Reg loss: 0.042 | Tree loss: 1.788 | Accuracy: 0.398500 | 0.945 sec/iter\n",
      "Epoch: 224 | Batch: 004 / 011 | Total loss: 1.779 | Reg loss: 0.042 | Tree loss: 1.779 | Accuracy: 0.417500 | 0.945 sec/iter\n",
      "Epoch: 224 | Batch: 005 / 011 | Total loss: 1.734 | Reg loss: 0.042 | Tree loss: 1.734 | Accuracy: 0.413000 | 0.945 sec/iter\n",
      "Epoch: 224 | Batch: 006 / 011 | Total loss: 1.717 | Reg loss: 0.042 | Tree loss: 1.717 | Accuracy: 0.428500 | 0.945 sec/iter\n",
      "Epoch: 224 | Batch: 007 / 011 | Total loss: 1.693 | Reg loss: 0.042 | Tree loss: 1.693 | Accuracy: 0.444000 | 0.945 sec/iter\n",
      "Epoch: 224 | Batch: 008 / 011 | Total loss: 1.701 | Reg loss: 0.042 | Tree loss: 1.701 | Accuracy: 0.424000 | 0.945 sec/iter\n",
      "Epoch: 224 | Batch: 009 / 011 | Total loss: 1.693 | Reg loss: 0.042 | Tree loss: 1.693 | Accuracy: 0.425000 | 0.945 sec/iter\n",
      "Epoch: 224 | Batch: 010 / 011 | Total loss: 1.790 | Reg loss: 0.042 | Tree loss: 1.790 | Accuracy: 0.385666 | 0.945 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 225 | Batch: 000 / 011 | Total loss: 1.864 | Reg loss: 0.042 | Tree loss: 1.864 | Accuracy: 0.409000 | 0.945 sec/iter\n",
      "Epoch: 225 | Batch: 001 / 011 | Total loss: 1.839 | Reg loss: 0.042 | Tree loss: 1.839 | Accuracy: 0.410000 | 0.945 sec/iter\n",
      "Epoch: 225 | Batch: 002 / 011 | Total loss: 1.815 | Reg loss: 0.042 | Tree loss: 1.815 | Accuracy: 0.415500 | 0.945 sec/iter\n",
      "Epoch: 225 | Batch: 003 / 011 | Total loss: 1.779 | Reg loss: 0.042 | Tree loss: 1.779 | Accuracy: 0.418000 | 0.945 sec/iter\n",
      "Epoch: 225 | Batch: 004 / 011 | Total loss: 1.754 | Reg loss: 0.042 | Tree loss: 1.754 | Accuracy: 0.420500 | 0.945 sec/iter\n",
      "Epoch: 225 | Batch: 005 / 011 | Total loss: 1.729 | Reg loss: 0.042 | Tree loss: 1.729 | Accuracy: 0.429000 | 0.945 sec/iter\n",
      "Epoch: 225 | Batch: 006 / 011 | Total loss: 1.743 | Reg loss: 0.042 | Tree loss: 1.743 | Accuracy: 0.406500 | 0.945 sec/iter\n",
      "Epoch: 225 | Batch: 007 / 011 | Total loss: 1.718 | Reg loss: 0.042 | Tree loss: 1.718 | Accuracy: 0.404000 | 0.945 sec/iter\n",
      "Epoch: 225 | Batch: 008 / 011 | Total loss: 1.691 | Reg loss: 0.042 | Tree loss: 1.691 | Accuracy: 0.434000 | 0.945 sec/iter\n",
      "Epoch: 225 | Batch: 009 / 011 | Total loss: 1.677 | Reg loss: 0.042 | Tree loss: 1.677 | Accuracy: 0.434000 | 0.945 sec/iter\n",
      "Epoch: 225 | Batch: 010 / 011 | Total loss: 1.698 | Reg loss: 0.042 | Tree loss: 1.698 | Accuracy: 0.426621 | 0.945 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 226 | Batch: 000 / 011 | Total loss: 1.865 | Reg loss: 0.042 | Tree loss: 1.865 | Accuracy: 0.413000 | 0.945 sec/iter\n",
      "Epoch: 226 | Batch: 001 / 011 | Total loss: 1.801 | Reg loss: 0.042 | Tree loss: 1.801 | Accuracy: 0.434000 | 0.945 sec/iter\n",
      "Epoch: 226 | Batch: 002 / 011 | Total loss: 1.816 | Reg loss: 0.042 | Tree loss: 1.816 | Accuracy: 0.406500 | 0.945 sec/iter\n",
      "Epoch: 226 | Batch: 003 / 011 | Total loss: 1.801 | Reg loss: 0.042 | Tree loss: 1.801 | Accuracy: 0.401500 | 0.945 sec/iter\n",
      "Epoch: 226 | Batch: 004 / 011 | Total loss: 1.754 | Reg loss: 0.042 | Tree loss: 1.754 | Accuracy: 0.405500 | 0.945 sec/iter\n",
      "Epoch: 226 | Batch: 005 / 011 | Total loss: 1.716 | Reg loss: 0.042 | Tree loss: 1.716 | Accuracy: 0.432000 | 0.945 sec/iter\n",
      "Epoch: 226 | Batch: 006 / 011 | Total loss: 1.726 | Reg loss: 0.042 | Tree loss: 1.726 | Accuracy: 0.422000 | 0.945 sec/iter\n",
      "Epoch: 226 | Batch: 007 / 011 | Total loss: 1.721 | Reg loss: 0.042 | Tree loss: 1.721 | Accuracy: 0.416000 | 0.945 sec/iter\n",
      "Epoch: 226 | Batch: 008 / 011 | Total loss: 1.692 | Reg loss: 0.042 | Tree loss: 1.692 | Accuracy: 0.428500 | 0.945 sec/iter\n",
      "Epoch: 226 | Batch: 009 / 011 | Total loss: 1.709 | Reg loss: 0.042 | Tree loss: 1.709 | Accuracy: 0.421000 | 0.945 sec/iter\n",
      "Epoch: 226 | Batch: 010 / 011 | Total loss: 1.653 | Reg loss: 0.042 | Tree loss: 1.653 | Accuracy: 0.430034 | 0.945 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 227 | Batch: 000 / 011 | Total loss: 1.852 | Reg loss: 0.042 | Tree loss: 1.852 | Accuracy: 0.433000 | 0.945 sec/iter\n",
      "Epoch: 227 | Batch: 001 / 011 | Total loss: 1.829 | Reg loss: 0.042 | Tree loss: 1.829 | Accuracy: 0.408000 | 0.945 sec/iter\n",
      "Epoch: 227 | Batch: 002 / 011 | Total loss: 1.804 | Reg loss: 0.042 | Tree loss: 1.804 | Accuracy: 0.396000 | 0.945 sec/iter\n",
      "Epoch: 227 | Batch: 003 / 011 | Total loss: 1.785 | Reg loss: 0.042 | Tree loss: 1.785 | Accuracy: 0.400000 | 0.945 sec/iter\n",
      "Epoch: 227 | Batch: 004 / 011 | Total loss: 1.769 | Reg loss: 0.042 | Tree loss: 1.769 | Accuracy: 0.417500 | 0.945 sec/iter\n",
      "Epoch: 227 | Batch: 005 / 011 | Total loss: 1.727 | Reg loss: 0.042 | Tree loss: 1.727 | Accuracy: 0.416000 | 0.945 sec/iter\n",
      "Epoch: 227 | Batch: 006 / 011 | Total loss: 1.716 | Reg loss: 0.042 | Tree loss: 1.716 | Accuracy: 0.428000 | 0.945 sec/iter\n",
      "Epoch: 227 | Batch: 007 / 011 | Total loss: 1.706 | Reg loss: 0.042 | Tree loss: 1.706 | Accuracy: 0.417500 | 0.945 sec/iter\n",
      "Epoch: 227 | Batch: 008 / 011 | Total loss: 1.703 | Reg loss: 0.042 | Tree loss: 1.703 | Accuracy: 0.415500 | 0.945 sec/iter\n",
      "Epoch: 227 | Batch: 009 / 011 | Total loss: 1.710 | Reg loss: 0.042 | Tree loss: 1.710 | Accuracy: 0.432500 | 0.945 sec/iter\n",
      "Epoch: 227 | Batch: 010 / 011 | Total loss: 1.657 | Reg loss: 0.042 | Tree loss: 1.657 | Accuracy: 0.419795 | 0.945 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 228 | Batch: 000 / 011 | Total loss: 1.848 | Reg loss: 0.042 | Tree loss: 1.848 | Accuracy: 0.406500 | 0.945 sec/iter\n",
      "Epoch: 228 | Batch: 001 / 011 | Total loss: 1.821 | Reg loss: 0.042 | Tree loss: 1.821 | Accuracy: 0.411500 | 0.945 sec/iter\n",
      "Epoch: 228 | Batch: 002 / 011 | Total loss: 1.789 | Reg loss: 0.042 | Tree loss: 1.789 | Accuracy: 0.422500 | 0.945 sec/iter\n",
      "Epoch: 228 | Batch: 003 / 011 | Total loss: 1.760 | Reg loss: 0.042 | Tree loss: 1.760 | Accuracy: 0.438000 | 0.945 sec/iter\n",
      "Epoch: 228 | Batch: 004 / 011 | Total loss: 1.747 | Reg loss: 0.042 | Tree loss: 1.747 | Accuracy: 0.411500 | 0.945 sec/iter\n",
      "Epoch: 228 | Batch: 005 / 011 | Total loss: 1.728 | Reg loss: 0.042 | Tree loss: 1.728 | Accuracy: 0.413000 | 0.945 sec/iter\n",
      "Epoch: 228 | Batch: 006 / 011 | Total loss: 1.723 | Reg loss: 0.042 | Tree loss: 1.723 | Accuracy: 0.413500 | 0.945 sec/iter\n",
      "Epoch: 228 | Batch: 007 / 011 | Total loss: 1.732 | Reg loss: 0.042 | Tree loss: 1.732 | Accuracy: 0.426000 | 0.945 sec/iter\n",
      "Epoch: 228 | Batch: 008 / 011 | Total loss: 1.725 | Reg loss: 0.042 | Tree loss: 1.725 | Accuracy: 0.393500 | 0.945 sec/iter\n",
      "Epoch: 228 | Batch: 009 / 011 | Total loss: 1.698 | Reg loss: 0.042 | Tree loss: 1.698 | Accuracy: 0.420000 | 0.945 sec/iter\n",
      "Epoch: 228 | Batch: 010 / 011 | Total loss: 1.654 | Reg loss: 0.042 | Tree loss: 1.654 | Accuracy: 0.453925 | 0.945 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 229 | Batch: 000 / 011 | Total loss: 1.860 | Reg loss: 0.042 | Tree loss: 1.860 | Accuracy: 0.408000 | 0.945 sec/iter\n",
      "Epoch: 229 | Batch: 001 / 011 | Total loss: 1.841 | Reg loss: 0.042 | Tree loss: 1.841 | Accuracy: 0.398000 | 0.945 sec/iter\n",
      "Epoch: 229 | Batch: 002 / 011 | Total loss: 1.796 | Reg loss: 0.042 | Tree loss: 1.796 | Accuracy: 0.413000 | 0.945 sec/iter\n",
      "Epoch: 229 | Batch: 003 / 011 | Total loss: 1.781 | Reg loss: 0.042 | Tree loss: 1.781 | Accuracy: 0.421500 | 0.945 sec/iter\n",
      "Epoch: 229 | Batch: 004 / 011 | Total loss: 1.740 | Reg loss: 0.042 | Tree loss: 1.740 | Accuracy: 0.424500 | 0.945 sec/iter\n",
      "Epoch: 229 | Batch: 005 / 011 | Total loss: 1.725 | Reg loss: 0.042 | Tree loss: 1.725 | Accuracy: 0.415500 | 0.945 sec/iter\n",
      "Epoch: 229 | Batch: 006 / 011 | Total loss: 1.708 | Reg loss: 0.042 | Tree loss: 1.708 | Accuracy: 0.412500 | 0.945 sec/iter\n",
      "Epoch: 229 | Batch: 007 / 011 | Total loss: 1.719 | Reg loss: 0.042 | Tree loss: 1.719 | Accuracy: 0.395500 | 0.945 sec/iter\n",
      "Epoch: 229 | Batch: 008 / 011 | Total loss: 1.681 | Reg loss: 0.042 | Tree loss: 1.681 | Accuracy: 0.433000 | 0.945 sec/iter\n",
      "Epoch: 229 | Batch: 009 / 011 | Total loss: 1.719 | Reg loss: 0.042 | Tree loss: 1.719 | Accuracy: 0.428500 | 0.945 sec/iter\n",
      "Epoch: 229 | Batch: 010 / 011 | Total loss: 1.650 | Reg loss: 0.042 | Tree loss: 1.650 | Accuracy: 0.447099 | 0.945 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 230 | Batch: 000 / 011 | Total loss: 1.833 | Reg loss: 0.042 | Tree loss: 1.833 | Accuracy: 0.421500 | 0.945 sec/iter\n",
      "Epoch: 230 | Batch: 001 / 011 | Total loss: 1.831 | Reg loss: 0.042 | Tree loss: 1.831 | Accuracy: 0.404000 | 0.945 sec/iter\n",
      "Epoch: 230 | Batch: 002 / 011 | Total loss: 1.798 | Reg loss: 0.042 | Tree loss: 1.798 | Accuracy: 0.428000 | 0.945 sec/iter\n",
      "Epoch: 230 | Batch: 003 / 011 | Total loss: 1.800 | Reg loss: 0.042 | Tree loss: 1.800 | Accuracy: 0.408500 | 0.945 sec/iter\n",
      "Epoch: 230 | Batch: 004 / 011 | Total loss: 1.761 | Reg loss: 0.042 | Tree loss: 1.761 | Accuracy: 0.409500 | 0.945 sec/iter\n",
      "Epoch: 230 | Batch: 005 / 011 | Total loss: 1.734 | Reg loss: 0.042 | Tree loss: 1.734 | Accuracy: 0.429000 | 0.945 sec/iter\n",
      "Epoch: 230 | Batch: 006 / 011 | Total loss: 1.689 | Reg loss: 0.042 | Tree loss: 1.689 | Accuracy: 0.435500 | 0.945 sec/iter\n",
      "Epoch: 230 | Batch: 007 / 011 | Total loss: 1.686 | Reg loss: 0.042 | Tree loss: 1.686 | Accuracy: 0.430000 | 0.944 sec/iter\n",
      "Epoch: 230 | Batch: 008 / 011 | Total loss: 1.719 | Reg loss: 0.042 | Tree loss: 1.719 | Accuracy: 0.403500 | 0.944 sec/iter\n",
      "Epoch: 230 | Batch: 009 / 011 | Total loss: 1.704 | Reg loss: 0.042 | Tree loss: 1.704 | Accuracy: 0.429500 | 0.944 sec/iter\n",
      "Epoch: 230 | Batch: 010 / 011 | Total loss: 1.719 | Reg loss: 0.042 | Tree loss: 1.719 | Accuracy: 0.443686 | 0.944 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 231 | Batch: 000 / 011 | Total loss: 1.851 | Reg loss: 0.042 | Tree loss: 1.851 | Accuracy: 0.404500 | 0.944 sec/iter\n",
      "Epoch: 231 | Batch: 001 / 011 | Total loss: 1.829 | Reg loss: 0.042 | Tree loss: 1.829 | Accuracy: 0.422500 | 0.944 sec/iter\n",
      "Epoch: 231 | Batch: 002 / 011 | Total loss: 1.797 | Reg loss: 0.042 | Tree loss: 1.797 | Accuracy: 0.402000 | 0.944 sec/iter\n",
      "Epoch: 231 | Batch: 003 / 011 | Total loss: 1.774 | Reg loss: 0.042 | Tree loss: 1.774 | Accuracy: 0.425500 | 0.944 sec/iter\n",
      "Epoch: 231 | Batch: 004 / 011 | Total loss: 1.769 | Reg loss: 0.042 | Tree loss: 1.769 | Accuracy: 0.425500 | 0.944 sec/iter\n",
      "Epoch: 231 | Batch: 005 / 011 | Total loss: 1.722 | Reg loss: 0.042 | Tree loss: 1.722 | Accuracy: 0.418000 | 0.944 sec/iter\n",
      "Epoch: 231 | Batch: 006 / 011 | Total loss: 1.718 | Reg loss: 0.042 | Tree loss: 1.718 | Accuracy: 0.415500 | 0.944 sec/iter\n",
      "Epoch: 231 | Batch: 007 / 011 | Total loss: 1.723 | Reg loss: 0.042 | Tree loss: 1.723 | Accuracy: 0.418500 | 0.944 sec/iter\n",
      "Epoch: 231 | Batch: 008 / 011 | Total loss: 1.686 | Reg loss: 0.042 | Tree loss: 1.686 | Accuracy: 0.433000 | 0.944 sec/iter\n",
      "Epoch: 231 | Batch: 009 / 011 | Total loss: 1.699 | Reg loss: 0.042 | Tree loss: 1.699 | Accuracy: 0.427000 | 0.944 sec/iter\n",
      "Epoch: 231 | Batch: 010 / 011 | Total loss: 1.694 | Reg loss: 0.042 | Tree loss: 1.694 | Accuracy: 0.433447 | 0.944 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 232 | Batch: 000 / 011 | Total loss: 1.844 | Reg loss: 0.042 | Tree loss: 1.844 | Accuracy: 0.403500 | 0.944 sec/iter\n",
      "Epoch: 232 | Batch: 001 / 011 | Total loss: 1.844 | Reg loss: 0.042 | Tree loss: 1.844 | Accuracy: 0.401000 | 0.944 sec/iter\n",
      "Epoch: 232 | Batch: 002 / 011 | Total loss: 1.781 | Reg loss: 0.042 | Tree loss: 1.781 | Accuracy: 0.427000 | 0.944 sec/iter\n",
      "Epoch: 232 | Batch: 003 / 011 | Total loss: 1.793 | Reg loss: 0.042 | Tree loss: 1.793 | Accuracy: 0.408500 | 0.944 sec/iter\n",
      "Epoch: 232 | Batch: 004 / 011 | Total loss: 1.739 | Reg loss: 0.042 | Tree loss: 1.739 | Accuracy: 0.427500 | 0.944 sec/iter\n",
      "Epoch: 232 | Batch: 005 / 011 | Total loss: 1.717 | Reg loss: 0.042 | Tree loss: 1.717 | Accuracy: 0.415000 | 0.944 sec/iter\n",
      "Epoch: 232 | Batch: 006 / 011 | Total loss: 1.708 | Reg loss: 0.042 | Tree loss: 1.708 | Accuracy: 0.425500 | 0.944 sec/iter\n",
      "Epoch: 232 | Batch: 007 / 011 | Total loss: 1.718 | Reg loss: 0.042 | Tree loss: 1.718 | Accuracy: 0.411500 | 0.944 sec/iter\n",
      "Epoch: 232 | Batch: 008 / 011 | Total loss: 1.738 | Reg loss: 0.042 | Tree loss: 1.738 | Accuracy: 0.406000 | 0.944 sec/iter\n",
      "Epoch: 232 | Batch: 009 / 011 | Total loss: 1.701 | Reg loss: 0.042 | Tree loss: 1.701 | Accuracy: 0.427500 | 0.944 sec/iter\n",
      "Epoch: 232 | Batch: 010 / 011 | Total loss: 1.649 | Reg loss: 0.042 | Tree loss: 1.649 | Accuracy: 0.450512 | 0.944 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 233 | Batch: 000 / 011 | Total loss: 1.863 | Reg loss: 0.042 | Tree loss: 1.863 | Accuracy: 0.402000 | 0.944 sec/iter\n",
      "Epoch: 233 | Batch: 001 / 011 | Total loss: 1.838 | Reg loss: 0.042 | Tree loss: 1.838 | Accuracy: 0.406500 | 0.944 sec/iter\n",
      "Epoch: 233 | Batch: 002 / 011 | Total loss: 1.826 | Reg loss: 0.042 | Tree loss: 1.826 | Accuracy: 0.411000 | 0.944 sec/iter\n",
      "Epoch: 233 | Batch: 003 / 011 | Total loss: 1.779 | Reg loss: 0.042 | Tree loss: 1.779 | Accuracy: 0.411500 | 0.944 sec/iter\n",
      "Epoch: 233 | Batch: 004 / 011 | Total loss: 1.725 | Reg loss: 0.042 | Tree loss: 1.725 | Accuracy: 0.433500 | 0.944 sec/iter\n",
      "Epoch: 233 | Batch: 005 / 011 | Total loss: 1.716 | Reg loss: 0.042 | Tree loss: 1.716 | Accuracy: 0.422500 | 0.944 sec/iter\n",
      "Epoch: 233 | Batch: 006 / 011 | Total loss: 1.707 | Reg loss: 0.042 | Tree loss: 1.707 | Accuracy: 0.421500 | 0.944 sec/iter\n",
      "Epoch: 233 | Batch: 007 / 011 | Total loss: 1.678 | Reg loss: 0.042 | Tree loss: 1.678 | Accuracy: 0.447000 | 0.944 sec/iter\n",
      "Epoch: 233 | Batch: 008 / 011 | Total loss: 1.732 | Reg loss: 0.042 | Tree loss: 1.732 | Accuracy: 0.408500 | 0.944 sec/iter\n",
      "Epoch: 233 | Batch: 009 / 011 | Total loss: 1.702 | Reg loss: 0.042 | Tree loss: 1.702 | Accuracy: 0.430500 | 0.944 sec/iter\n",
      "Epoch: 233 | Batch: 010 / 011 | Total loss: 1.671 | Reg loss: 0.042 | Tree loss: 1.671 | Accuracy: 0.453925 | 0.944 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 234 | Batch: 000 / 011 | Total loss: 1.845 | Reg loss: 0.042 | Tree loss: 1.845 | Accuracy: 0.407500 | 0.944 sec/iter\n",
      "Epoch: 234 | Batch: 001 / 011 | Total loss: 1.838 | Reg loss: 0.042 | Tree loss: 1.838 | Accuracy: 0.388000 | 0.944 sec/iter\n",
      "Epoch: 234 | Batch: 002 / 011 | Total loss: 1.799 | Reg loss: 0.042 | Tree loss: 1.799 | Accuracy: 0.412000 | 0.944 sec/iter\n",
      "Epoch: 234 | Batch: 003 / 011 | Total loss: 1.791 | Reg loss: 0.042 | Tree loss: 1.791 | Accuracy: 0.424000 | 0.944 sec/iter\n",
      "Epoch: 234 | Batch: 004 / 011 | Total loss: 1.751 | Reg loss: 0.042 | Tree loss: 1.751 | Accuracy: 0.429500 | 0.944 sec/iter\n",
      "Epoch: 234 | Batch: 005 / 011 | Total loss: 1.707 | Reg loss: 0.042 | Tree loss: 1.707 | Accuracy: 0.433500 | 0.944 sec/iter\n",
      "Epoch: 234 | Batch: 006 / 011 | Total loss: 1.745 | Reg loss: 0.042 | Tree loss: 1.745 | Accuracy: 0.415000 | 0.944 sec/iter\n",
      "Epoch: 234 | Batch: 007 / 011 | Total loss: 1.712 | Reg loss: 0.042 | Tree loss: 1.712 | Accuracy: 0.413500 | 0.944 sec/iter\n",
      "Epoch: 234 | Batch: 008 / 011 | Total loss: 1.689 | Reg loss: 0.042 | Tree loss: 1.689 | Accuracy: 0.437000 | 0.944 sec/iter\n",
      "Epoch: 234 | Batch: 009 / 011 | Total loss: 1.708 | Reg loss: 0.042 | Tree loss: 1.708 | Accuracy: 0.406000 | 0.944 sec/iter\n",
      "Epoch: 234 | Batch: 010 / 011 | Total loss: 1.670 | Reg loss: 0.042 | Tree loss: 1.670 | Accuracy: 0.470990 | 0.944 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 235 | Batch: 000 / 011 | Total loss: 1.860 | Reg loss: 0.042 | Tree loss: 1.860 | Accuracy: 0.410000 | 0.944 sec/iter\n",
      "Epoch: 235 | Batch: 001 / 011 | Total loss: 1.826 | Reg loss: 0.042 | Tree loss: 1.826 | Accuracy: 0.411000 | 0.944 sec/iter\n",
      "Epoch: 235 | Batch: 002 / 011 | Total loss: 1.807 | Reg loss: 0.042 | Tree loss: 1.807 | Accuracy: 0.424000 | 0.944 sec/iter\n",
      "Epoch: 235 | Batch: 003 / 011 | Total loss: 1.771 | Reg loss: 0.042 | Tree loss: 1.771 | Accuracy: 0.417500 | 0.944 sec/iter\n",
      "Epoch: 235 | Batch: 004 / 011 | Total loss: 1.763 | Reg loss: 0.042 | Tree loss: 1.763 | Accuracy: 0.429000 | 0.944 sec/iter\n",
      "Epoch: 235 | Batch: 005 / 011 | Total loss: 1.721 | Reg loss: 0.042 | Tree loss: 1.721 | Accuracy: 0.419500 | 0.944 sec/iter\n",
      "Epoch: 235 | Batch: 006 / 011 | Total loss: 1.703 | Reg loss: 0.042 | Tree loss: 1.703 | Accuracy: 0.424500 | 0.944 sec/iter\n",
      "Epoch: 235 | Batch: 007 / 011 | Total loss: 1.708 | Reg loss: 0.042 | Tree loss: 1.708 | Accuracy: 0.417000 | 0.944 sec/iter\n",
      "Epoch: 235 | Batch: 008 / 011 | Total loss: 1.689 | Reg loss: 0.042 | Tree loss: 1.689 | Accuracy: 0.422000 | 0.944 sec/iter\n",
      "Epoch: 235 | Batch: 009 / 011 | Total loss: 1.717 | Reg loss: 0.042 | Tree loss: 1.717 | Accuracy: 0.409500 | 0.944 sec/iter\n",
      "Epoch: 235 | Batch: 010 / 011 | Total loss: 1.716 | Reg loss: 0.042 | Tree loss: 1.716 | Accuracy: 0.447099 | 0.944 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 236 | Batch: 000 / 011 | Total loss: 1.849 | Reg loss: 0.042 | Tree loss: 1.849 | Accuracy: 0.413500 | 0.944 sec/iter\n",
      "Epoch: 236 | Batch: 001 / 011 | Total loss: 1.814 | Reg loss: 0.042 | Tree loss: 1.814 | Accuracy: 0.428000 | 0.944 sec/iter\n",
      "Epoch: 236 | Batch: 002 / 011 | Total loss: 1.822 | Reg loss: 0.042 | Tree loss: 1.822 | Accuracy: 0.391500 | 0.944 sec/iter\n",
      "Epoch: 236 | Batch: 003 / 011 | Total loss: 1.781 | Reg loss: 0.042 | Tree loss: 1.781 | Accuracy: 0.415000 | 0.944 sec/iter\n",
      "Epoch: 236 | Batch: 004 / 011 | Total loss: 1.753 | Reg loss: 0.042 | Tree loss: 1.753 | Accuracy: 0.424500 | 0.944 sec/iter\n",
      "Epoch: 236 | Batch: 005 / 011 | Total loss: 1.745 | Reg loss: 0.042 | Tree loss: 1.745 | Accuracy: 0.412000 | 0.944 sec/iter\n",
      "Epoch: 236 | Batch: 006 / 011 | Total loss: 1.698 | Reg loss: 0.042 | Tree loss: 1.698 | Accuracy: 0.430000 | 0.944 sec/iter\n",
      "Epoch: 236 | Batch: 007 / 011 | Total loss: 1.733 | Reg loss: 0.042 | Tree loss: 1.733 | Accuracy: 0.420000 | 0.944 sec/iter\n",
      "Epoch: 236 | Batch: 008 / 011 | Total loss: 1.682 | Reg loss: 0.042 | Tree loss: 1.682 | Accuracy: 0.425000 | 0.944 sec/iter\n",
      "Epoch: 236 | Batch: 009 / 011 | Total loss: 1.668 | Reg loss: 0.042 | Tree loss: 1.668 | Accuracy: 0.438500 | 0.944 sec/iter\n",
      "Epoch: 236 | Batch: 010 / 011 | Total loss: 1.700 | Reg loss: 0.042 | Tree loss: 1.700 | Accuracy: 0.423208 | 0.944 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 237 | Batch: 000 / 011 | Total loss: 1.857 | Reg loss: 0.042 | Tree loss: 1.857 | Accuracy: 0.419000 | 0.944 sec/iter\n",
      "Epoch: 237 | Batch: 001 / 011 | Total loss: 1.818 | Reg loss: 0.042 | Tree loss: 1.818 | Accuracy: 0.429500 | 0.944 sec/iter\n",
      "Epoch: 237 | Batch: 002 / 011 | Total loss: 1.808 | Reg loss: 0.042 | Tree loss: 1.808 | Accuracy: 0.406500 | 0.944 sec/iter\n",
      "Epoch: 237 | Batch: 003 / 011 | Total loss: 1.782 | Reg loss: 0.042 | Tree loss: 1.782 | Accuracy: 0.400500 | 0.944 sec/iter\n",
      "Epoch: 237 | Batch: 004 / 011 | Total loss: 1.750 | Reg loss: 0.042 | Tree loss: 1.750 | Accuracy: 0.407000 | 0.944 sec/iter\n",
      "Epoch: 237 | Batch: 005 / 011 | Total loss: 1.746 | Reg loss: 0.042 | Tree loss: 1.746 | Accuracy: 0.413500 | 0.944 sec/iter\n",
      "Epoch: 237 | Batch: 006 / 011 | Total loss: 1.719 | Reg loss: 0.042 | Tree loss: 1.719 | Accuracy: 0.412500 | 0.944 sec/iter\n",
      "Epoch: 237 | Batch: 007 / 011 | Total loss: 1.686 | Reg loss: 0.042 | Tree loss: 1.686 | Accuracy: 0.426500 | 0.944 sec/iter\n",
      "Epoch: 237 | Batch: 008 / 011 | Total loss: 1.687 | Reg loss: 0.042 | Tree loss: 1.687 | Accuracy: 0.422500 | 0.944 sec/iter\n",
      "Epoch: 237 | Batch: 009 / 011 | Total loss: 1.709 | Reg loss: 0.042 | Tree loss: 1.709 | Accuracy: 0.413500 | 0.944 sec/iter\n",
      "Epoch: 237 | Batch: 010 / 011 | Total loss: 1.703 | Reg loss: 0.042 | Tree loss: 1.703 | Accuracy: 0.430034 | 0.944 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 238 | Batch: 000 / 011 | Total loss: 1.851 | Reg loss: 0.042 | Tree loss: 1.851 | Accuracy: 0.397500 | 0.944 sec/iter\n",
      "Epoch: 238 | Batch: 001 / 011 | Total loss: 1.842 | Reg loss: 0.042 | Tree loss: 1.842 | Accuracy: 0.428500 | 0.944 sec/iter\n",
      "Epoch: 238 | Batch: 002 / 011 | Total loss: 1.792 | Reg loss: 0.042 | Tree loss: 1.792 | Accuracy: 0.416000 | 0.944 sec/iter\n",
      "Epoch: 238 | Batch: 003 / 011 | Total loss: 1.756 | Reg loss: 0.042 | Tree loss: 1.756 | Accuracy: 0.430000 | 0.944 sec/iter\n",
      "Epoch: 238 | Batch: 004 / 011 | Total loss: 1.744 | Reg loss: 0.042 | Tree loss: 1.744 | Accuracy: 0.407500 | 0.944 sec/iter\n",
      "Epoch: 238 | Batch: 005 / 011 | Total loss: 1.758 | Reg loss: 0.042 | Tree loss: 1.758 | Accuracy: 0.416000 | 0.944 sec/iter\n",
      "Epoch: 238 | Batch: 006 / 011 | Total loss: 1.712 | Reg loss: 0.042 | Tree loss: 1.712 | Accuracy: 0.429500 | 0.944 sec/iter\n",
      "Epoch: 238 | Batch: 007 / 011 | Total loss: 1.709 | Reg loss: 0.042 | Tree loss: 1.709 | Accuracy: 0.438500 | 0.944 sec/iter\n",
      "Epoch: 238 | Batch: 008 / 011 | Total loss: 1.714 | Reg loss: 0.042 | Tree loss: 1.714 | Accuracy: 0.417500 | 0.943 sec/iter\n",
      "Epoch: 238 | Batch: 009 / 011 | Total loss: 1.675 | Reg loss: 0.042 | Tree loss: 1.675 | Accuracy: 0.427500 | 0.943 sec/iter\n",
      "Epoch: 238 | Batch: 010 / 011 | Total loss: 1.740 | Reg loss: 0.042 | Tree loss: 1.740 | Accuracy: 0.426621 | 0.943 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 239 | Batch: 000 / 011 | Total loss: 1.848 | Reg loss: 0.042 | Tree loss: 1.848 | Accuracy: 0.414000 | 0.943 sec/iter\n",
      "Epoch: 239 | Batch: 001 / 011 | Total loss: 1.814 | Reg loss: 0.042 | Tree loss: 1.814 | Accuracy: 0.416500 | 0.943 sec/iter\n",
      "Epoch: 239 | Batch: 002 / 011 | Total loss: 1.815 | Reg loss: 0.042 | Tree loss: 1.815 | Accuracy: 0.401500 | 0.943 sec/iter\n",
      "Epoch: 239 | Batch: 003 / 011 | Total loss: 1.796 | Reg loss: 0.042 | Tree loss: 1.796 | Accuracy: 0.414500 | 0.943 sec/iter\n",
      "Epoch: 239 | Batch: 004 / 011 | Total loss: 1.782 | Reg loss: 0.042 | Tree loss: 1.782 | Accuracy: 0.413000 | 0.943 sec/iter\n",
      "Epoch: 239 | Batch: 005 / 011 | Total loss: 1.723 | Reg loss: 0.042 | Tree loss: 1.723 | Accuracy: 0.429000 | 0.943 sec/iter\n",
      "Epoch: 239 | Batch: 006 / 011 | Total loss: 1.719 | Reg loss: 0.042 | Tree loss: 1.719 | Accuracy: 0.415000 | 0.943 sec/iter\n",
      "Epoch: 239 | Batch: 007 / 011 | Total loss: 1.693 | Reg loss: 0.042 | Tree loss: 1.693 | Accuracy: 0.415500 | 0.943 sec/iter\n",
      "Epoch: 239 | Batch: 008 / 011 | Total loss: 1.687 | Reg loss: 0.042 | Tree loss: 1.687 | Accuracy: 0.438000 | 0.943 sec/iter\n",
      "Epoch: 239 | Batch: 009 / 011 | Total loss: 1.682 | Reg loss: 0.042 | Tree loss: 1.682 | Accuracy: 0.405500 | 0.943 sec/iter\n",
      "Epoch: 239 | Batch: 010 / 011 | Total loss: 1.660 | Reg loss: 0.042 | Tree loss: 1.660 | Accuracy: 0.436860 | 0.943 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 240 | Batch: 000 / 011 | Total loss: 1.850 | Reg loss: 0.042 | Tree loss: 1.850 | Accuracy: 0.434000 | 0.943 sec/iter\n",
      "Epoch: 240 | Batch: 001 / 011 | Total loss: 1.837 | Reg loss: 0.042 | Tree loss: 1.837 | Accuracy: 0.412500 | 0.943 sec/iter\n",
      "Epoch: 240 | Batch: 002 / 011 | Total loss: 1.813 | Reg loss: 0.042 | Tree loss: 1.813 | Accuracy: 0.404000 | 0.943 sec/iter\n",
      "Epoch: 240 | Batch: 003 / 011 | Total loss: 1.790 | Reg loss: 0.042 | Tree loss: 1.790 | Accuracy: 0.405500 | 0.943 sec/iter\n",
      "Epoch: 240 | Batch: 004 / 011 | Total loss: 1.728 | Reg loss: 0.042 | Tree loss: 1.728 | Accuracy: 0.410500 | 0.943 sec/iter\n",
      "Epoch: 240 | Batch: 005 / 011 | Total loss: 1.721 | Reg loss: 0.042 | Tree loss: 1.721 | Accuracy: 0.423500 | 0.943 sec/iter\n",
      "Epoch: 240 | Batch: 006 / 011 | Total loss: 1.709 | Reg loss: 0.042 | Tree loss: 1.709 | Accuracy: 0.426500 | 0.943 sec/iter\n",
      "Epoch: 240 | Batch: 007 / 011 | Total loss: 1.697 | Reg loss: 0.042 | Tree loss: 1.697 | Accuracy: 0.423000 | 0.943 sec/iter\n",
      "Epoch: 240 | Batch: 008 / 011 | Total loss: 1.680 | Reg loss: 0.042 | Tree loss: 1.680 | Accuracy: 0.437500 | 0.943 sec/iter\n",
      "Epoch: 240 | Batch: 009 / 011 | Total loss: 1.716 | Reg loss: 0.042 | Tree loss: 1.716 | Accuracy: 0.397000 | 0.943 sec/iter\n",
      "Epoch: 240 | Batch: 010 / 011 | Total loss: 1.746 | Reg loss: 0.042 | Tree loss: 1.746 | Accuracy: 0.423208 | 0.943 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 241 | Batch: 000 / 011 | Total loss: 1.857 | Reg loss: 0.042 | Tree loss: 1.857 | Accuracy: 0.411000 | 0.943 sec/iter\n",
      "Epoch: 241 | Batch: 001 / 011 | Total loss: 1.842 | Reg loss: 0.042 | Tree loss: 1.842 | Accuracy: 0.419500 | 0.943 sec/iter\n",
      "Epoch: 241 | Batch: 002 / 011 | Total loss: 1.800 | Reg loss: 0.042 | Tree loss: 1.800 | Accuracy: 0.407000 | 0.943 sec/iter\n",
      "Epoch: 241 | Batch: 003 / 011 | Total loss: 1.778 | Reg loss: 0.042 | Tree loss: 1.778 | Accuracy: 0.429000 | 0.943 sec/iter\n",
      "Epoch: 241 | Batch: 004 / 011 | Total loss: 1.749 | Reg loss: 0.042 | Tree loss: 1.749 | Accuracy: 0.413000 | 0.943 sec/iter\n",
      "Epoch: 241 | Batch: 005 / 011 | Total loss: 1.708 | Reg loss: 0.042 | Tree loss: 1.708 | Accuracy: 0.431500 | 0.943 sec/iter\n",
      "Epoch: 241 | Batch: 006 / 011 | Total loss: 1.733 | Reg loss: 0.042 | Tree loss: 1.733 | Accuracy: 0.419000 | 0.943 sec/iter\n",
      "Epoch: 241 | Batch: 007 / 011 | Total loss: 1.698 | Reg loss: 0.042 | Tree loss: 1.698 | Accuracy: 0.412500 | 0.943 sec/iter\n",
      "Epoch: 241 | Batch: 008 / 011 | Total loss: 1.703 | Reg loss: 0.042 | Tree loss: 1.703 | Accuracy: 0.424000 | 0.943 sec/iter\n",
      "Epoch: 241 | Batch: 009 / 011 | Total loss: 1.688 | Reg loss: 0.042 | Tree loss: 1.688 | Accuracy: 0.420500 | 0.943 sec/iter\n",
      "Epoch: 241 | Batch: 010 / 011 | Total loss: 1.700 | Reg loss: 0.042 | Tree loss: 1.700 | Accuracy: 0.419795 | 0.943 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 242 | Batch: 000 / 011 | Total loss: 1.869 | Reg loss: 0.042 | Tree loss: 1.869 | Accuracy: 0.408500 | 0.943 sec/iter\n",
      "Epoch: 242 | Batch: 001 / 011 | Total loss: 1.842 | Reg loss: 0.042 | Tree loss: 1.842 | Accuracy: 0.404500 | 0.943 sec/iter\n",
      "Epoch: 242 | Batch: 002 / 011 | Total loss: 1.810 | Reg loss: 0.042 | Tree loss: 1.810 | Accuracy: 0.417500 | 0.943 sec/iter\n",
      "Epoch: 242 | Batch: 003 / 011 | Total loss: 1.782 | Reg loss: 0.042 | Tree loss: 1.782 | Accuracy: 0.406000 | 0.943 sec/iter\n",
      "Epoch: 242 | Batch: 004 / 011 | Total loss: 1.749 | Reg loss: 0.042 | Tree loss: 1.749 | Accuracy: 0.424000 | 0.943 sec/iter\n",
      "Epoch: 242 | Batch: 005 / 011 | Total loss: 1.742 | Reg loss: 0.042 | Tree loss: 1.742 | Accuracy: 0.405000 | 0.943 sec/iter\n",
      "Epoch: 242 | Batch: 006 / 011 | Total loss: 1.702 | Reg loss: 0.042 | Tree loss: 1.702 | Accuracy: 0.425000 | 0.943 sec/iter\n",
      "Epoch: 242 | Batch: 007 / 011 | Total loss: 1.679 | Reg loss: 0.042 | Tree loss: 1.679 | Accuracy: 0.434000 | 0.943 sec/iter\n",
      "Epoch: 242 | Batch: 008 / 011 | Total loss: 1.682 | Reg loss: 0.042 | Tree loss: 1.682 | Accuracy: 0.423500 | 0.943 sec/iter\n",
      "Epoch: 242 | Batch: 009 / 011 | Total loss: 1.680 | Reg loss: 0.042 | Tree loss: 1.680 | Accuracy: 0.442500 | 0.943 sec/iter\n",
      "Epoch: 242 | Batch: 010 / 011 | Total loss: 1.721 | Reg loss: 0.042 | Tree loss: 1.721 | Accuracy: 0.392491 | 0.943 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 243 | Batch: 000 / 011 | Total loss: 1.841 | Reg loss: 0.042 | Tree loss: 1.841 | Accuracy: 0.411000 | 0.943 sec/iter\n",
      "Epoch: 243 | Batch: 001 / 011 | Total loss: 1.835 | Reg loss: 0.042 | Tree loss: 1.835 | Accuracy: 0.407000 | 0.943 sec/iter\n",
      "Epoch: 243 | Batch: 002 / 011 | Total loss: 1.788 | Reg loss: 0.042 | Tree loss: 1.788 | Accuracy: 0.420000 | 0.943 sec/iter\n",
      "Epoch: 243 | Batch: 003 / 011 | Total loss: 1.791 | Reg loss: 0.042 | Tree loss: 1.791 | Accuracy: 0.400000 | 0.943 sec/iter\n",
      "Epoch: 243 | Batch: 004 / 011 | Total loss: 1.751 | Reg loss: 0.042 | Tree loss: 1.751 | Accuracy: 0.438500 | 0.943 sec/iter\n",
      "Epoch: 243 | Batch: 005 / 011 | Total loss: 1.740 | Reg loss: 0.042 | Tree loss: 1.740 | Accuracy: 0.409000 | 0.943 sec/iter\n",
      "Epoch: 243 | Batch: 006 / 011 | Total loss: 1.694 | Reg loss: 0.042 | Tree loss: 1.694 | Accuracy: 0.430000 | 0.943 sec/iter\n",
      "Epoch: 243 | Batch: 007 / 011 | Total loss: 1.702 | Reg loss: 0.042 | Tree loss: 1.702 | Accuracy: 0.431000 | 0.943 sec/iter\n",
      "Epoch: 243 | Batch: 008 / 011 | Total loss: 1.744 | Reg loss: 0.042 | Tree loss: 1.744 | Accuracy: 0.393000 | 0.943 sec/iter\n",
      "Epoch: 243 | Batch: 009 / 011 | Total loss: 1.685 | Reg loss: 0.042 | Tree loss: 1.685 | Accuracy: 0.423000 | 0.943 sec/iter\n",
      "Epoch: 243 | Batch: 010 / 011 | Total loss: 1.684 | Reg loss: 0.042 | Tree loss: 1.684 | Accuracy: 0.447099 | 0.943 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 244 | Batch: 000 / 011 | Total loss: 1.807 | Reg loss: 0.042 | Tree loss: 1.807 | Accuracy: 0.428500 | 0.943 sec/iter\n",
      "Epoch: 244 | Batch: 001 / 011 | Total loss: 1.831 | Reg loss: 0.042 | Tree loss: 1.831 | Accuracy: 0.413000 | 0.943 sec/iter\n",
      "Epoch: 244 | Batch: 002 / 011 | Total loss: 1.815 | Reg loss: 0.042 | Tree loss: 1.815 | Accuracy: 0.411500 | 0.943 sec/iter\n",
      "Epoch: 244 | Batch: 003 / 011 | Total loss: 1.742 | Reg loss: 0.042 | Tree loss: 1.742 | Accuracy: 0.441000 | 0.943 sec/iter\n",
      "Epoch: 244 | Batch: 004 / 011 | Total loss: 1.762 | Reg loss: 0.042 | Tree loss: 1.762 | Accuracy: 0.424500 | 0.943 sec/iter\n",
      "Epoch: 244 | Batch: 005 / 011 | Total loss: 1.739 | Reg loss: 0.042 | Tree loss: 1.739 | Accuracy: 0.404500 | 0.943 sec/iter\n",
      "Epoch: 244 | Batch: 006 / 011 | Total loss: 1.730 | Reg loss: 0.042 | Tree loss: 1.730 | Accuracy: 0.415500 | 0.943 sec/iter\n",
      "Epoch: 244 | Batch: 007 / 011 | Total loss: 1.711 | Reg loss: 0.042 | Tree loss: 1.711 | Accuracy: 0.420000 | 0.943 sec/iter\n",
      "Epoch: 244 | Batch: 008 / 011 | Total loss: 1.712 | Reg loss: 0.042 | Tree loss: 1.712 | Accuracy: 0.407000 | 0.943 sec/iter\n",
      "Epoch: 244 | Batch: 009 / 011 | Total loss: 1.698 | Reg loss: 0.042 | Tree loss: 1.698 | Accuracy: 0.423500 | 0.943 sec/iter\n",
      "Epoch: 244 | Batch: 010 / 011 | Total loss: 1.739 | Reg loss: 0.042 | Tree loss: 1.739 | Accuracy: 0.392491 | 0.943 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 245 | Batch: 000 / 011 | Total loss: 1.862 | Reg loss: 0.042 | Tree loss: 1.862 | Accuracy: 0.400000 | 0.943 sec/iter\n",
      "Epoch: 245 | Batch: 001 / 011 | Total loss: 1.830 | Reg loss: 0.042 | Tree loss: 1.830 | Accuracy: 0.413000 | 0.943 sec/iter\n",
      "Epoch: 245 | Batch: 002 / 011 | Total loss: 1.806 | Reg loss: 0.042 | Tree loss: 1.806 | Accuracy: 0.406000 | 0.943 sec/iter\n",
      "Epoch: 245 | Batch: 003 / 011 | Total loss: 1.758 | Reg loss: 0.042 | Tree loss: 1.758 | Accuracy: 0.431500 | 0.943 sec/iter\n",
      "Epoch: 245 | Batch: 004 / 011 | Total loss: 1.745 | Reg loss: 0.042 | Tree loss: 1.745 | Accuracy: 0.419500 | 0.943 sec/iter\n",
      "Epoch: 245 | Batch: 005 / 011 | Total loss: 1.739 | Reg loss: 0.042 | Tree loss: 1.739 | Accuracy: 0.427000 | 0.943 sec/iter\n",
      "Epoch: 245 | Batch: 006 / 011 | Total loss: 1.714 | Reg loss: 0.042 | Tree loss: 1.714 | Accuracy: 0.410500 | 0.943 sec/iter\n",
      "Epoch: 245 | Batch: 007 / 011 | Total loss: 1.707 | Reg loss: 0.042 | Tree loss: 1.707 | Accuracy: 0.426500 | 0.943 sec/iter\n",
      "Epoch: 245 | Batch: 008 / 011 | Total loss: 1.690 | Reg loss: 0.042 | Tree loss: 1.690 | Accuracy: 0.429000 | 0.943 sec/iter\n",
      "Epoch: 245 | Batch: 009 / 011 | Total loss: 1.688 | Reg loss: 0.042 | Tree loss: 1.688 | Accuracy: 0.417000 | 0.943 sec/iter\n",
      "Epoch: 245 | Batch: 010 / 011 | Total loss: 1.706 | Reg loss: 0.042 | Tree loss: 1.706 | Accuracy: 0.416382 | 0.943 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 246 | Batch: 000 / 011 | Total loss: 1.871 | Reg loss: 0.042 | Tree loss: 1.871 | Accuracy: 0.402000 | 0.943 sec/iter\n",
      "Epoch: 246 | Batch: 001 / 011 | Total loss: 1.825 | Reg loss: 0.042 | Tree loss: 1.825 | Accuracy: 0.414500 | 0.943 sec/iter\n",
      "Epoch: 246 | Batch: 002 / 011 | Total loss: 1.818 | Reg loss: 0.042 | Tree loss: 1.818 | Accuracy: 0.413000 | 0.943 sec/iter\n",
      "Epoch: 246 | Batch: 003 / 011 | Total loss: 1.760 | Reg loss: 0.042 | Tree loss: 1.760 | Accuracy: 0.428500 | 0.943 sec/iter\n",
      "Epoch: 246 | Batch: 004 / 011 | Total loss: 1.772 | Reg loss: 0.042 | Tree loss: 1.772 | Accuracy: 0.404500 | 0.943 sec/iter\n",
      "Epoch: 246 | Batch: 005 / 011 | Total loss: 1.691 | Reg loss: 0.042 | Tree loss: 1.691 | Accuracy: 0.432000 | 0.943 sec/iter\n",
      "Epoch: 246 | Batch: 006 / 011 | Total loss: 1.719 | Reg loss: 0.042 | Tree loss: 1.719 | Accuracy: 0.415000 | 0.943 sec/iter\n",
      "Epoch: 246 | Batch: 007 / 011 | Total loss: 1.701 | Reg loss: 0.042 | Tree loss: 1.701 | Accuracy: 0.428000 | 0.943 sec/iter\n",
      "Epoch: 246 | Batch: 008 / 011 | Total loss: 1.701 | Reg loss: 0.042 | Tree loss: 1.701 | Accuracy: 0.421500 | 0.943 sec/iter\n",
      "Epoch: 246 | Batch: 009 / 011 | Total loss: 1.684 | Reg loss: 0.042 | Tree loss: 1.684 | Accuracy: 0.423500 | 0.943 sec/iter\n",
      "Epoch: 246 | Batch: 010 / 011 | Total loss: 1.617 | Reg loss: 0.042 | Tree loss: 1.617 | Accuracy: 0.464164 | 0.942 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 247 | Batch: 000 / 011 | Total loss: 1.868 | Reg loss: 0.042 | Tree loss: 1.868 | Accuracy: 0.400000 | 0.943 sec/iter\n",
      "Epoch: 247 | Batch: 001 / 011 | Total loss: 1.825 | Reg loss: 0.042 | Tree loss: 1.825 | Accuracy: 0.413000 | 0.942 sec/iter\n",
      "Epoch: 247 | Batch: 002 / 011 | Total loss: 1.774 | Reg loss: 0.042 | Tree loss: 1.774 | Accuracy: 0.429000 | 0.942 sec/iter\n",
      "Epoch: 247 | Batch: 003 / 011 | Total loss: 1.768 | Reg loss: 0.042 | Tree loss: 1.768 | Accuracy: 0.422500 | 0.942 sec/iter\n",
      "Epoch: 247 | Batch: 004 / 011 | Total loss: 1.754 | Reg loss: 0.042 | Tree loss: 1.754 | Accuracy: 0.416000 | 0.942 sec/iter\n",
      "Epoch: 247 | Batch: 005 / 011 | Total loss: 1.726 | Reg loss: 0.042 | Tree loss: 1.726 | Accuracy: 0.410500 | 0.942 sec/iter\n",
      "Epoch: 247 | Batch: 006 / 011 | Total loss: 1.696 | Reg loss: 0.042 | Tree loss: 1.696 | Accuracy: 0.429500 | 0.942 sec/iter\n",
      "Epoch: 247 | Batch: 007 / 011 | Total loss: 1.700 | Reg loss: 0.042 | Tree loss: 1.700 | Accuracy: 0.427500 | 0.942 sec/iter\n",
      "Epoch: 247 | Batch: 008 / 011 | Total loss: 1.732 | Reg loss: 0.042 | Tree loss: 1.732 | Accuracy: 0.418000 | 0.942 sec/iter\n",
      "Epoch: 247 | Batch: 009 / 011 | Total loss: 1.702 | Reg loss: 0.042 | Tree loss: 1.702 | Accuracy: 0.415500 | 0.942 sec/iter\n",
      "Epoch: 247 | Batch: 010 / 011 | Total loss: 1.689 | Reg loss: 0.042 | Tree loss: 1.689 | Accuracy: 0.433447 | 0.942 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 248 | Batch: 000 / 011 | Total loss: 1.856 | Reg loss: 0.042 | Tree loss: 1.856 | Accuracy: 0.400000 | 0.942 sec/iter\n",
      "Epoch: 248 | Batch: 001 / 011 | Total loss: 1.835 | Reg loss: 0.042 | Tree loss: 1.835 | Accuracy: 0.400000 | 0.942 sec/iter\n",
      "Epoch: 248 | Batch: 002 / 011 | Total loss: 1.793 | Reg loss: 0.042 | Tree loss: 1.793 | Accuracy: 0.423500 | 0.942 sec/iter\n",
      "Epoch: 248 | Batch: 003 / 011 | Total loss: 1.788 | Reg loss: 0.042 | Tree loss: 1.788 | Accuracy: 0.418500 | 0.942 sec/iter\n",
      "Epoch: 248 | Batch: 004 / 011 | Total loss: 1.740 | Reg loss: 0.042 | Tree loss: 1.740 | Accuracy: 0.424000 | 0.942 sec/iter\n",
      "Epoch: 248 | Batch: 005 / 011 | Total loss: 1.733 | Reg loss: 0.042 | Tree loss: 1.733 | Accuracy: 0.423500 | 0.942 sec/iter\n",
      "Epoch: 248 | Batch: 006 / 011 | Total loss: 1.712 | Reg loss: 0.042 | Tree loss: 1.712 | Accuracy: 0.420500 | 0.942 sec/iter\n",
      "Epoch: 248 | Batch: 007 / 011 | Total loss: 1.724 | Reg loss: 0.042 | Tree loss: 1.724 | Accuracy: 0.423000 | 0.942 sec/iter\n",
      "Epoch: 248 | Batch: 008 / 011 | Total loss: 1.702 | Reg loss: 0.042 | Tree loss: 1.702 | Accuracy: 0.421500 | 0.942 sec/iter\n",
      "Epoch: 248 | Batch: 009 / 011 | Total loss: 1.663 | Reg loss: 0.042 | Tree loss: 1.663 | Accuracy: 0.424500 | 0.942 sec/iter\n",
      "Epoch: 248 | Batch: 010 / 011 | Total loss: 1.704 | Reg loss: 0.042 | Tree loss: 1.704 | Accuracy: 0.436860 | 0.942 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 249 | Batch: 000 / 011 | Total loss: 1.888 | Reg loss: 0.042 | Tree loss: 1.888 | Accuracy: 0.405000 | 0.942 sec/iter\n",
      "Epoch: 249 | Batch: 001 / 011 | Total loss: 1.858 | Reg loss: 0.042 | Tree loss: 1.858 | Accuracy: 0.390000 | 0.942 sec/iter\n",
      "Epoch: 249 | Batch: 002 / 011 | Total loss: 1.770 | Reg loss: 0.042 | Tree loss: 1.770 | Accuracy: 0.427000 | 0.942 sec/iter\n",
      "Epoch: 249 | Batch: 003 / 011 | Total loss: 1.788 | Reg loss: 0.042 | Tree loss: 1.788 | Accuracy: 0.402000 | 0.942 sec/iter\n",
      "Epoch: 249 | Batch: 004 / 011 | Total loss: 1.758 | Reg loss: 0.042 | Tree loss: 1.758 | Accuracy: 0.415000 | 0.942 sec/iter\n",
      "Epoch: 249 | Batch: 005 / 011 | Total loss: 1.714 | Reg loss: 0.042 | Tree loss: 1.714 | Accuracy: 0.422000 | 0.942 sec/iter\n",
      "Epoch: 249 | Batch: 006 / 011 | Total loss: 1.696 | Reg loss: 0.042 | Tree loss: 1.696 | Accuracy: 0.433000 | 0.942 sec/iter\n",
      "Epoch: 249 | Batch: 007 / 011 | Total loss: 1.700 | Reg loss: 0.042 | Tree loss: 1.700 | Accuracy: 0.441000 | 0.942 sec/iter\n",
      "Epoch: 249 | Batch: 008 / 011 | Total loss: 1.699 | Reg loss: 0.042 | Tree loss: 1.699 | Accuracy: 0.419500 | 0.942 sec/iter\n",
      "Epoch: 249 | Batch: 009 / 011 | Total loss: 1.675 | Reg loss: 0.042 | Tree loss: 1.675 | Accuracy: 0.428000 | 0.942 sec/iter\n",
      "Epoch: 249 | Batch: 010 / 011 | Total loss: 1.676 | Reg loss: 0.042 | Tree loss: 1.676 | Accuracy: 0.402730 | 0.942 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 250 | Batch: 000 / 011 | Total loss: 1.845 | Reg loss: 0.042 | Tree loss: 1.845 | Accuracy: 0.414000 | 0.942 sec/iter\n",
      "Epoch: 250 | Batch: 001 / 011 | Total loss: 1.822 | Reg loss: 0.042 | Tree loss: 1.822 | Accuracy: 0.418000 | 0.942 sec/iter\n",
      "Epoch: 250 | Batch: 002 / 011 | Total loss: 1.812 | Reg loss: 0.042 | Tree loss: 1.812 | Accuracy: 0.401000 | 0.942 sec/iter\n",
      "Epoch: 250 | Batch: 003 / 011 | Total loss: 1.747 | Reg loss: 0.042 | Tree loss: 1.747 | Accuracy: 0.445000 | 0.942 sec/iter\n",
      "Epoch: 250 | Batch: 004 / 011 | Total loss: 1.762 | Reg loss: 0.042 | Tree loss: 1.762 | Accuracy: 0.403000 | 0.942 sec/iter\n",
      "Epoch: 250 | Batch: 005 / 011 | Total loss: 1.718 | Reg loss: 0.042 | Tree loss: 1.718 | Accuracy: 0.438500 | 0.942 sec/iter\n",
      "Epoch: 250 | Batch: 006 / 011 | Total loss: 1.722 | Reg loss: 0.042 | Tree loss: 1.722 | Accuracy: 0.404000 | 0.942 sec/iter\n",
      "Epoch: 250 | Batch: 007 / 011 | Total loss: 1.701 | Reg loss: 0.042 | Tree loss: 1.701 | Accuracy: 0.417000 | 0.942 sec/iter\n",
      "Epoch: 250 | Batch: 008 / 011 | Total loss: 1.726 | Reg loss: 0.042 | Tree loss: 1.726 | Accuracy: 0.408000 | 0.942 sec/iter\n",
      "Epoch: 250 | Batch: 009 / 011 | Total loss: 1.683 | Reg loss: 0.042 | Tree loss: 1.683 | Accuracy: 0.431500 | 0.942 sec/iter\n",
      "Epoch: 250 | Batch: 010 / 011 | Total loss: 1.691 | Reg loss: 0.042 | Tree loss: 1.691 | Accuracy: 0.416382 | 0.942 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 251 | Batch: 000 / 011 | Total loss: 1.846 | Reg loss: 0.042 | Tree loss: 1.846 | Accuracy: 0.418000 | 0.942 sec/iter\n",
      "Epoch: 251 | Batch: 001 / 011 | Total loss: 1.828 | Reg loss: 0.042 | Tree loss: 1.828 | Accuracy: 0.398000 | 0.942 sec/iter\n",
      "Epoch: 251 | Batch: 002 / 011 | Total loss: 1.794 | Reg loss: 0.042 | Tree loss: 1.794 | Accuracy: 0.411500 | 0.942 sec/iter\n",
      "Epoch: 251 | Batch: 003 / 011 | Total loss: 1.748 | Reg loss: 0.042 | Tree loss: 1.748 | Accuracy: 0.434500 | 0.942 sec/iter\n",
      "Epoch: 251 | Batch: 004 / 011 | Total loss: 1.746 | Reg loss: 0.042 | Tree loss: 1.746 | Accuracy: 0.409500 | 0.942 sec/iter\n",
      "Epoch: 251 | Batch: 005 / 011 | Total loss: 1.743 | Reg loss: 0.042 | Tree loss: 1.743 | Accuracy: 0.409500 | 0.942 sec/iter\n",
      "Epoch: 251 | Batch: 006 / 011 | Total loss: 1.737 | Reg loss: 0.042 | Tree loss: 1.737 | Accuracy: 0.399000 | 0.942 sec/iter\n",
      "Epoch: 251 | Batch: 007 / 011 | Total loss: 1.691 | Reg loss: 0.042 | Tree loss: 1.691 | Accuracy: 0.424000 | 0.942 sec/iter\n",
      "Epoch: 251 | Batch: 008 / 011 | Total loss: 1.705 | Reg loss: 0.042 | Tree loss: 1.705 | Accuracy: 0.427500 | 0.942 sec/iter\n",
      "Epoch: 251 | Batch: 009 / 011 | Total loss: 1.689 | Reg loss: 0.042 | Tree loss: 1.689 | Accuracy: 0.438000 | 0.942 sec/iter\n",
      "Epoch: 251 | Batch: 010 / 011 | Total loss: 1.708 | Reg loss: 0.042 | Tree loss: 1.708 | Accuracy: 0.389078 | 0.942 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 252 | Batch: 000 / 011 | Total loss: 1.853 | Reg loss: 0.042 | Tree loss: 1.853 | Accuracy: 0.416000 | 0.942 sec/iter\n",
      "Epoch: 252 | Batch: 001 / 011 | Total loss: 1.827 | Reg loss: 0.042 | Tree loss: 1.827 | Accuracy: 0.416500 | 0.942 sec/iter\n",
      "Epoch: 252 | Batch: 002 / 011 | Total loss: 1.792 | Reg loss: 0.042 | Tree loss: 1.792 | Accuracy: 0.423000 | 0.942 sec/iter\n",
      "Epoch: 252 | Batch: 003 / 011 | Total loss: 1.800 | Reg loss: 0.042 | Tree loss: 1.800 | Accuracy: 0.405500 | 0.942 sec/iter\n",
      "Epoch: 252 | Batch: 004 / 011 | Total loss: 1.725 | Reg loss: 0.042 | Tree loss: 1.725 | Accuracy: 0.430000 | 0.942 sec/iter\n",
      "Epoch: 252 | Batch: 005 / 011 | Total loss: 1.722 | Reg loss: 0.042 | Tree loss: 1.722 | Accuracy: 0.427000 | 0.942 sec/iter\n",
      "Epoch: 252 | Batch: 006 / 011 | Total loss: 1.736 | Reg loss: 0.042 | Tree loss: 1.736 | Accuracy: 0.409000 | 0.942 sec/iter\n",
      "Epoch: 252 | Batch: 007 / 011 | Total loss: 1.689 | Reg loss: 0.042 | Tree loss: 1.689 | Accuracy: 0.429500 | 0.942 sec/iter\n",
      "Epoch: 252 | Batch: 008 / 011 | Total loss: 1.700 | Reg loss: 0.042 | Tree loss: 1.700 | Accuracy: 0.412000 | 0.942 sec/iter\n",
      "Epoch: 252 | Batch: 009 / 011 | Total loss: 1.697 | Reg loss: 0.042 | Tree loss: 1.697 | Accuracy: 0.423000 | 0.942 sec/iter\n",
      "Epoch: 252 | Batch: 010 / 011 | Total loss: 1.657 | Reg loss: 0.042 | Tree loss: 1.657 | Accuracy: 0.457338 | 0.942 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 253 | Batch: 000 / 011 | Total loss: 1.843 | Reg loss: 0.042 | Tree loss: 1.843 | Accuracy: 0.424000 | 0.942 sec/iter\n",
      "Epoch: 253 | Batch: 001 / 011 | Total loss: 1.832 | Reg loss: 0.042 | Tree loss: 1.832 | Accuracy: 0.408500 | 0.942 sec/iter\n",
      "Epoch: 253 | Batch: 002 / 011 | Total loss: 1.817 | Reg loss: 0.042 | Tree loss: 1.817 | Accuracy: 0.395500 | 0.942 sec/iter\n",
      "Epoch: 253 | Batch: 003 / 011 | Total loss: 1.761 | Reg loss: 0.042 | Tree loss: 1.761 | Accuracy: 0.411000 | 0.942 sec/iter\n",
      "Epoch: 253 | Batch: 004 / 011 | Total loss: 1.782 | Reg loss: 0.042 | Tree loss: 1.782 | Accuracy: 0.409000 | 0.942 sec/iter\n",
      "Epoch: 253 | Batch: 005 / 011 | Total loss: 1.722 | Reg loss: 0.042 | Tree loss: 1.722 | Accuracy: 0.432500 | 0.942 sec/iter\n",
      "Epoch: 253 | Batch: 006 / 011 | Total loss: 1.688 | Reg loss: 0.042 | Tree loss: 1.688 | Accuracy: 0.431500 | 0.942 sec/iter\n",
      "Epoch: 253 | Batch: 007 / 011 | Total loss: 1.719 | Reg loss: 0.042 | Tree loss: 1.719 | Accuracy: 0.418500 | 0.942 sec/iter\n",
      "Epoch: 253 | Batch: 008 / 011 | Total loss: 1.698 | Reg loss: 0.042 | Tree loss: 1.698 | Accuracy: 0.412500 | 0.942 sec/iter\n",
      "Epoch: 253 | Batch: 009 / 011 | Total loss: 1.687 | Reg loss: 0.042 | Tree loss: 1.687 | Accuracy: 0.428500 | 0.942 sec/iter\n",
      "Epoch: 253 | Batch: 010 / 011 | Total loss: 1.631 | Reg loss: 0.042 | Tree loss: 1.631 | Accuracy: 0.440273 | 0.942 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 254 | Batch: 000 / 011 | Total loss: 1.843 | Reg loss: 0.042 | Tree loss: 1.843 | Accuracy: 0.416000 | 0.942 sec/iter\n",
      "Epoch: 254 | Batch: 001 / 011 | Total loss: 1.840 | Reg loss: 0.042 | Tree loss: 1.840 | Accuracy: 0.408500 | 0.942 sec/iter\n",
      "Epoch: 254 | Batch: 002 / 011 | Total loss: 1.814 | Reg loss: 0.042 | Tree loss: 1.814 | Accuracy: 0.391000 | 0.942 sec/iter\n",
      "Epoch: 254 | Batch: 003 / 011 | Total loss: 1.779 | Reg loss: 0.042 | Tree loss: 1.779 | Accuracy: 0.412500 | 0.942 sec/iter\n",
      "Epoch: 254 | Batch: 004 / 011 | Total loss: 1.755 | Reg loss: 0.042 | Tree loss: 1.755 | Accuracy: 0.420500 | 0.942 sec/iter\n",
      "Epoch: 254 | Batch: 005 / 011 | Total loss: 1.751 | Reg loss: 0.042 | Tree loss: 1.751 | Accuracy: 0.419500 | 0.942 sec/iter\n",
      "Epoch: 254 | Batch: 006 / 011 | Total loss: 1.676 | Reg loss: 0.042 | Tree loss: 1.676 | Accuracy: 0.431500 | 0.942 sec/iter\n",
      "Epoch: 254 | Batch: 007 / 011 | Total loss: 1.705 | Reg loss: 0.042 | Tree loss: 1.705 | Accuracy: 0.415500 | 0.942 sec/iter\n",
      "Epoch: 254 | Batch: 008 / 011 | Total loss: 1.709 | Reg loss: 0.042 | Tree loss: 1.709 | Accuracy: 0.432500 | 0.942 sec/iter\n",
      "Epoch: 254 | Batch: 009 / 011 | Total loss: 1.676 | Reg loss: 0.042 | Tree loss: 1.676 | Accuracy: 0.435500 | 0.942 sec/iter\n",
      "Epoch: 254 | Batch: 010 / 011 | Total loss: 1.640 | Reg loss: 0.042 | Tree loss: 1.640 | Accuracy: 0.430034 | 0.942 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 255 | Batch: 000 / 011 | Total loss: 1.835 | Reg loss: 0.042 | Tree loss: 1.835 | Accuracy: 0.416500 | 0.942 sec/iter\n",
      "Epoch: 255 | Batch: 001 / 011 | Total loss: 1.814 | Reg loss: 0.042 | Tree loss: 1.814 | Accuracy: 0.421000 | 0.942 sec/iter\n",
      "Epoch: 255 | Batch: 002 / 011 | Total loss: 1.813 | Reg loss: 0.042 | Tree loss: 1.813 | Accuracy: 0.407000 | 0.942 sec/iter\n",
      "Epoch: 255 | Batch: 003 / 011 | Total loss: 1.768 | Reg loss: 0.042 | Tree loss: 1.768 | Accuracy: 0.422000 | 0.942 sec/iter\n",
      "Epoch: 255 | Batch: 004 / 011 | Total loss: 1.770 | Reg loss: 0.042 | Tree loss: 1.770 | Accuracy: 0.414000 | 0.942 sec/iter\n",
      "Epoch: 255 | Batch: 005 / 011 | Total loss: 1.734 | Reg loss: 0.042 | Tree loss: 1.734 | Accuracy: 0.411500 | 0.942 sec/iter\n",
      "Epoch: 255 | Batch: 006 / 011 | Total loss: 1.689 | Reg loss: 0.042 | Tree loss: 1.689 | Accuracy: 0.421500 | 0.942 sec/iter\n",
      "Epoch: 255 | Batch: 007 / 011 | Total loss: 1.708 | Reg loss: 0.042 | Tree loss: 1.708 | Accuracy: 0.423500 | 0.941 sec/iter\n",
      "Epoch: 255 | Batch: 008 / 011 | Total loss: 1.713 | Reg loss: 0.042 | Tree loss: 1.713 | Accuracy: 0.419500 | 0.941 sec/iter\n",
      "Epoch: 255 | Batch: 009 / 011 | Total loss: 1.682 | Reg loss: 0.042 | Tree loss: 1.682 | Accuracy: 0.420500 | 0.941 sec/iter\n",
      "Epoch: 255 | Batch: 010 / 011 | Total loss: 1.733 | Reg loss: 0.042 | Tree loss: 1.733 | Accuracy: 0.416382 | 0.941 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 256 | Batch: 000 / 011 | Total loss: 1.838 | Reg loss: 0.042 | Tree loss: 1.838 | Accuracy: 0.427500 | 0.941 sec/iter\n",
      "Epoch: 256 | Batch: 001 / 011 | Total loss: 1.830 | Reg loss: 0.042 | Tree loss: 1.830 | Accuracy: 0.413000 | 0.941 sec/iter\n",
      "Epoch: 256 | Batch: 002 / 011 | Total loss: 1.796 | Reg loss: 0.042 | Tree loss: 1.796 | Accuracy: 0.427000 | 0.941 sec/iter\n",
      "Epoch: 256 | Batch: 003 / 011 | Total loss: 1.748 | Reg loss: 0.042 | Tree loss: 1.748 | Accuracy: 0.433500 | 0.941 sec/iter\n",
      "Epoch: 256 | Batch: 004 / 011 | Total loss: 1.765 | Reg loss: 0.042 | Tree loss: 1.765 | Accuracy: 0.417500 | 0.941 sec/iter\n",
      "Epoch: 256 | Batch: 005 / 011 | Total loss: 1.742 | Reg loss: 0.042 | Tree loss: 1.742 | Accuracy: 0.411000 | 0.941 sec/iter\n",
      "Epoch: 256 | Batch: 006 / 011 | Total loss: 1.714 | Reg loss: 0.042 | Tree loss: 1.714 | Accuracy: 0.429500 | 0.941 sec/iter\n",
      "Epoch: 256 | Batch: 007 / 011 | Total loss: 1.717 | Reg loss: 0.042 | Tree loss: 1.717 | Accuracy: 0.417500 | 0.941 sec/iter\n",
      "Epoch: 256 | Batch: 008 / 011 | Total loss: 1.699 | Reg loss: 0.042 | Tree loss: 1.699 | Accuracy: 0.411000 | 0.941 sec/iter\n",
      "Epoch: 256 | Batch: 009 / 011 | Total loss: 1.690 | Reg loss: 0.042 | Tree loss: 1.690 | Accuracy: 0.414000 | 0.941 sec/iter\n",
      "Epoch: 256 | Batch: 010 / 011 | Total loss: 1.637 | Reg loss: 0.042 | Tree loss: 1.637 | Accuracy: 0.433447 | 0.941 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 257 | Batch: 000 / 011 | Total loss: 1.842 | Reg loss: 0.042 | Tree loss: 1.842 | Accuracy: 0.412500 | 0.941 sec/iter\n",
      "Epoch: 257 | Batch: 001 / 011 | Total loss: 1.825 | Reg loss: 0.042 | Tree loss: 1.825 | Accuracy: 0.413500 | 0.941 sec/iter\n",
      "Epoch: 257 | Batch: 002 / 011 | Total loss: 1.789 | Reg loss: 0.042 | Tree loss: 1.789 | Accuracy: 0.424000 | 0.941 sec/iter\n",
      "Epoch: 257 | Batch: 003 / 011 | Total loss: 1.781 | Reg loss: 0.042 | Tree loss: 1.781 | Accuracy: 0.412500 | 0.941 sec/iter\n",
      "Epoch: 257 | Batch: 004 / 011 | Total loss: 1.756 | Reg loss: 0.042 | Tree loss: 1.756 | Accuracy: 0.406500 | 0.941 sec/iter\n",
      "Epoch: 257 | Batch: 005 / 011 | Total loss: 1.748 | Reg loss: 0.042 | Tree loss: 1.748 | Accuracy: 0.420500 | 0.941 sec/iter\n",
      "Epoch: 257 | Batch: 006 / 011 | Total loss: 1.721 | Reg loss: 0.042 | Tree loss: 1.721 | Accuracy: 0.412000 | 0.941 sec/iter\n",
      "Epoch: 257 | Batch: 007 / 011 | Total loss: 1.683 | Reg loss: 0.042 | Tree loss: 1.683 | Accuracy: 0.433000 | 0.941 sec/iter\n",
      "Epoch: 257 | Batch: 008 / 011 | Total loss: 1.686 | Reg loss: 0.042 | Tree loss: 1.686 | Accuracy: 0.425000 | 0.941 sec/iter\n",
      "Epoch: 257 | Batch: 009 / 011 | Total loss: 1.697 | Reg loss: 0.042 | Tree loss: 1.697 | Accuracy: 0.409500 | 0.941 sec/iter\n",
      "Epoch: 257 | Batch: 010 / 011 | Total loss: 1.685 | Reg loss: 0.042 | Tree loss: 1.685 | Accuracy: 0.430034 | 0.941 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 258 | Batch: 000 / 011 | Total loss: 1.848 | Reg loss: 0.042 | Tree loss: 1.848 | Accuracy: 0.413500 | 0.941 sec/iter\n",
      "Epoch: 258 | Batch: 001 / 011 | Total loss: 1.823 | Reg loss: 0.042 | Tree loss: 1.823 | Accuracy: 0.420500 | 0.941 sec/iter\n",
      "Epoch: 258 | Batch: 002 / 011 | Total loss: 1.817 | Reg loss: 0.042 | Tree loss: 1.817 | Accuracy: 0.414500 | 0.941 sec/iter\n",
      "Epoch: 258 | Batch: 003 / 011 | Total loss: 1.770 | Reg loss: 0.042 | Tree loss: 1.770 | Accuracy: 0.402500 | 0.941 sec/iter\n",
      "Epoch: 258 | Batch: 004 / 011 | Total loss: 1.735 | Reg loss: 0.042 | Tree loss: 1.735 | Accuracy: 0.444500 | 0.941 sec/iter\n",
      "Epoch: 258 | Batch: 005 / 011 | Total loss: 1.714 | Reg loss: 0.042 | Tree loss: 1.714 | Accuracy: 0.418500 | 0.941 sec/iter\n",
      "Epoch: 258 | Batch: 006 / 011 | Total loss: 1.718 | Reg loss: 0.042 | Tree loss: 1.718 | Accuracy: 0.419500 | 0.941 sec/iter\n",
      "Epoch: 258 | Batch: 007 / 011 | Total loss: 1.714 | Reg loss: 0.042 | Tree loss: 1.714 | Accuracy: 0.438500 | 0.941 sec/iter\n",
      "Epoch: 258 | Batch: 008 / 011 | Total loss: 1.690 | Reg loss: 0.042 | Tree loss: 1.690 | Accuracy: 0.433500 | 0.941 sec/iter\n",
      "Epoch: 258 | Batch: 009 / 011 | Total loss: 1.700 | Reg loss: 0.042 | Tree loss: 1.700 | Accuracy: 0.406500 | 0.941 sec/iter\n",
      "Epoch: 258 | Batch: 010 / 011 | Total loss: 1.744 | Reg loss: 0.042 | Tree loss: 1.744 | Accuracy: 0.402730 | 0.941 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 259 | Batch: 000 / 011 | Total loss: 1.864 | Reg loss: 0.042 | Tree loss: 1.864 | Accuracy: 0.413500 | 0.941 sec/iter\n",
      "Epoch: 259 | Batch: 001 / 011 | Total loss: 1.815 | Reg loss: 0.042 | Tree loss: 1.815 | Accuracy: 0.426500 | 0.941 sec/iter\n",
      "Epoch: 259 | Batch: 002 / 011 | Total loss: 1.835 | Reg loss: 0.042 | Tree loss: 1.835 | Accuracy: 0.394500 | 0.941 sec/iter\n",
      "Epoch: 259 | Batch: 003 / 011 | Total loss: 1.761 | Reg loss: 0.042 | Tree loss: 1.761 | Accuracy: 0.418500 | 0.941 sec/iter\n",
      "Epoch: 259 | Batch: 004 / 011 | Total loss: 1.747 | Reg loss: 0.042 | Tree loss: 1.747 | Accuracy: 0.397500 | 0.941 sec/iter\n",
      "Epoch: 259 | Batch: 005 / 011 | Total loss: 1.741 | Reg loss: 0.042 | Tree loss: 1.741 | Accuracy: 0.425000 | 0.941 sec/iter\n",
      "Epoch: 259 | Batch: 006 / 011 | Total loss: 1.713 | Reg loss: 0.042 | Tree loss: 1.713 | Accuracy: 0.423500 | 0.941 sec/iter\n",
      "Epoch: 259 | Batch: 007 / 011 | Total loss: 1.695 | Reg loss: 0.042 | Tree loss: 1.695 | Accuracy: 0.433500 | 0.941 sec/iter\n",
      "Epoch: 259 | Batch: 008 / 011 | Total loss: 1.691 | Reg loss: 0.042 | Tree loss: 1.691 | Accuracy: 0.453500 | 0.941 sec/iter\n",
      "Epoch: 259 | Batch: 009 / 011 | Total loss: 1.670 | Reg loss: 0.042 | Tree loss: 1.670 | Accuracy: 0.417500 | 0.941 sec/iter\n",
      "Epoch: 259 | Batch: 010 / 011 | Total loss: 1.673 | Reg loss: 0.042 | Tree loss: 1.673 | Accuracy: 0.419795 | 0.941 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 260 | Batch: 000 / 011 | Total loss: 1.815 | Reg loss: 0.042 | Tree loss: 1.815 | Accuracy: 0.432000 | 0.941 sec/iter\n",
      "Epoch: 260 | Batch: 001 / 011 | Total loss: 1.824 | Reg loss: 0.042 | Tree loss: 1.824 | Accuracy: 0.419000 | 0.941 sec/iter\n",
      "Epoch: 260 | Batch: 002 / 011 | Total loss: 1.812 | Reg loss: 0.042 | Tree loss: 1.812 | Accuracy: 0.405500 | 0.941 sec/iter\n",
      "Epoch: 260 | Batch: 003 / 011 | Total loss: 1.779 | Reg loss: 0.042 | Tree loss: 1.779 | Accuracy: 0.418500 | 0.941 sec/iter\n",
      "Epoch: 260 | Batch: 004 / 011 | Total loss: 1.730 | Reg loss: 0.042 | Tree loss: 1.730 | Accuracy: 0.414000 | 0.941 sec/iter\n",
      "Epoch: 260 | Batch: 005 / 011 | Total loss: 1.742 | Reg loss: 0.042 | Tree loss: 1.742 | Accuracy: 0.417500 | 0.941 sec/iter\n",
      "Epoch: 260 | Batch: 006 / 011 | Total loss: 1.725 | Reg loss: 0.042 | Tree loss: 1.725 | Accuracy: 0.407500 | 0.941 sec/iter\n",
      "Epoch: 260 | Batch: 007 / 011 | Total loss: 1.708 | Reg loss: 0.042 | Tree loss: 1.708 | Accuracy: 0.433500 | 0.941 sec/iter\n",
      "Epoch: 260 | Batch: 008 / 011 | Total loss: 1.693 | Reg loss: 0.042 | Tree loss: 1.693 | Accuracy: 0.421000 | 0.941 sec/iter\n",
      "Epoch: 260 | Batch: 009 / 011 | Total loss: 1.682 | Reg loss: 0.042 | Tree loss: 1.682 | Accuracy: 0.422000 | 0.941 sec/iter\n",
      "Epoch: 260 | Batch: 010 / 011 | Total loss: 1.759 | Reg loss: 0.042 | Tree loss: 1.759 | Accuracy: 0.409556 | 0.941 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 261 | Batch: 000 / 011 | Total loss: 1.842 | Reg loss: 0.042 | Tree loss: 1.842 | Accuracy: 0.424000 | 0.941 sec/iter\n",
      "Epoch: 261 | Batch: 001 / 011 | Total loss: 1.858 | Reg loss: 0.042 | Tree loss: 1.858 | Accuracy: 0.389500 | 0.941 sec/iter\n",
      "Epoch: 261 | Batch: 002 / 011 | Total loss: 1.849 | Reg loss: 0.042 | Tree loss: 1.849 | Accuracy: 0.390000 | 0.941 sec/iter\n",
      "Epoch: 261 | Batch: 003 / 011 | Total loss: 1.771 | Reg loss: 0.042 | Tree loss: 1.771 | Accuracy: 0.402500 | 0.941 sec/iter\n",
      "Epoch: 261 | Batch: 004 / 011 | Total loss: 1.742 | Reg loss: 0.042 | Tree loss: 1.742 | Accuracy: 0.421500 | 0.941 sec/iter\n",
      "Epoch: 261 | Batch: 005 / 011 | Total loss: 1.737 | Reg loss: 0.042 | Tree loss: 1.737 | Accuracy: 0.412500 | 0.941 sec/iter\n",
      "Epoch: 261 | Batch: 006 / 011 | Total loss: 1.704 | Reg loss: 0.042 | Tree loss: 1.704 | Accuracy: 0.424000 | 0.941 sec/iter\n",
      "Epoch: 261 | Batch: 007 / 011 | Total loss: 1.691 | Reg loss: 0.042 | Tree loss: 1.691 | Accuracy: 0.423500 | 0.941 sec/iter\n",
      "Epoch: 261 | Batch: 008 / 011 | Total loss: 1.683 | Reg loss: 0.042 | Tree loss: 1.683 | Accuracy: 0.439500 | 0.941 sec/iter\n",
      "Epoch: 261 | Batch: 009 / 011 | Total loss: 1.675 | Reg loss: 0.042 | Tree loss: 1.675 | Accuracy: 0.429500 | 0.941 sec/iter\n",
      "Epoch: 261 | Batch: 010 / 011 | Total loss: 1.618 | Reg loss: 0.042 | Tree loss: 1.618 | Accuracy: 0.511945 | 0.941 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 262 | Batch: 000 / 011 | Total loss: 1.860 | Reg loss: 0.042 | Tree loss: 1.860 | Accuracy: 0.419500 | 0.941 sec/iter\n",
      "Epoch: 262 | Batch: 001 / 011 | Total loss: 1.837 | Reg loss: 0.042 | Tree loss: 1.837 | Accuracy: 0.402500 | 0.941 sec/iter\n",
      "Epoch: 262 | Batch: 002 / 011 | Total loss: 1.788 | Reg loss: 0.042 | Tree loss: 1.788 | Accuracy: 0.414500 | 0.941 sec/iter\n",
      "Epoch: 262 | Batch: 003 / 011 | Total loss: 1.769 | Reg loss: 0.042 | Tree loss: 1.769 | Accuracy: 0.414500 | 0.941 sec/iter\n",
      "Epoch: 262 | Batch: 004 / 011 | Total loss: 1.757 | Reg loss: 0.042 | Tree loss: 1.757 | Accuracy: 0.410000 | 0.941 sec/iter\n",
      "Epoch: 262 | Batch: 005 / 011 | Total loss: 1.727 | Reg loss: 0.042 | Tree loss: 1.727 | Accuracy: 0.411500 | 0.941 sec/iter\n",
      "Epoch: 262 | Batch: 006 / 011 | Total loss: 1.713 | Reg loss: 0.042 | Tree loss: 1.713 | Accuracy: 0.418000 | 0.941 sec/iter\n",
      "Epoch: 262 | Batch: 007 / 011 | Total loss: 1.696 | Reg loss: 0.042 | Tree loss: 1.696 | Accuracy: 0.424500 | 0.941 sec/iter\n",
      "Epoch: 262 | Batch: 008 / 011 | Total loss: 1.690 | Reg loss: 0.042 | Tree loss: 1.690 | Accuracy: 0.426500 | 0.941 sec/iter\n",
      "Epoch: 262 | Batch: 009 / 011 | Total loss: 1.695 | Reg loss: 0.042 | Tree loss: 1.695 | Accuracy: 0.421000 | 0.941 sec/iter\n",
      "Epoch: 262 | Batch: 010 / 011 | Total loss: 1.698 | Reg loss: 0.042 | Tree loss: 1.698 | Accuracy: 0.426621 | 0.941 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 263 | Batch: 000 / 011 | Total loss: 1.856 | Reg loss: 0.042 | Tree loss: 1.856 | Accuracy: 0.406000 | 0.941 sec/iter\n",
      "Epoch: 263 | Batch: 001 / 011 | Total loss: 1.829 | Reg loss: 0.042 | Tree loss: 1.829 | Accuracy: 0.422000 | 0.941 sec/iter\n",
      "Epoch: 263 | Batch: 002 / 011 | Total loss: 1.799 | Reg loss: 0.042 | Tree loss: 1.799 | Accuracy: 0.422000 | 0.941 sec/iter\n",
      "Epoch: 263 | Batch: 003 / 011 | Total loss: 1.800 | Reg loss: 0.042 | Tree loss: 1.800 | Accuracy: 0.394000 | 0.941 sec/iter\n",
      "Epoch: 263 | Batch: 004 / 011 | Total loss: 1.722 | Reg loss: 0.042 | Tree loss: 1.722 | Accuracy: 0.421500 | 0.941 sec/iter\n",
      "Epoch: 263 | Batch: 005 / 011 | Total loss: 1.731 | Reg loss: 0.042 | Tree loss: 1.731 | Accuracy: 0.404000 | 0.941 sec/iter\n",
      "Epoch: 263 | Batch: 006 / 011 | Total loss: 1.678 | Reg loss: 0.042 | Tree loss: 1.678 | Accuracy: 0.443000 | 0.941 sec/iter\n",
      "Epoch: 263 | Batch: 007 / 011 | Total loss: 1.708 | Reg loss: 0.042 | Tree loss: 1.708 | Accuracy: 0.437500 | 0.941 sec/iter\n",
      "Epoch: 263 | Batch: 008 / 011 | Total loss: 1.700 | Reg loss: 0.042 | Tree loss: 1.700 | Accuracy: 0.427000 | 0.941 sec/iter\n",
      "Epoch: 263 | Batch: 009 / 011 | Total loss: 1.701 | Reg loss: 0.042 | Tree loss: 1.701 | Accuracy: 0.408000 | 0.941 sec/iter\n",
      "Epoch: 263 | Batch: 010 / 011 | Total loss: 1.757 | Reg loss: 0.042 | Tree loss: 1.757 | Accuracy: 0.389078 | 0.941 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 264 | Batch: 000 / 011 | Total loss: 1.846 | Reg loss: 0.042 | Tree loss: 1.846 | Accuracy: 0.416500 | 0.941 sec/iter\n",
      "Epoch: 264 | Batch: 001 / 011 | Total loss: 1.816 | Reg loss: 0.042 | Tree loss: 1.816 | Accuracy: 0.420000 | 0.941 sec/iter\n",
      "Epoch: 264 | Batch: 002 / 011 | Total loss: 1.791 | Reg loss: 0.042 | Tree loss: 1.791 | Accuracy: 0.422000 | 0.941 sec/iter\n",
      "Epoch: 264 | Batch: 003 / 011 | Total loss: 1.779 | Reg loss: 0.042 | Tree loss: 1.779 | Accuracy: 0.429500 | 0.941 sec/iter\n",
      "Epoch: 264 | Batch: 004 / 011 | Total loss: 1.749 | Reg loss: 0.042 | Tree loss: 1.749 | Accuracy: 0.413000 | 0.941 sec/iter\n",
      "Epoch: 264 | Batch: 005 / 011 | Total loss: 1.727 | Reg loss: 0.042 | Tree loss: 1.727 | Accuracy: 0.422000 | 0.941 sec/iter\n",
      "Epoch: 264 | Batch: 006 / 011 | Total loss: 1.713 | Reg loss: 0.042 | Tree loss: 1.713 | Accuracy: 0.418000 | 0.941 sec/iter\n",
      "Epoch: 264 | Batch: 007 / 011 | Total loss: 1.712 | Reg loss: 0.042 | Tree loss: 1.712 | Accuracy: 0.407000 | 0.941 sec/iter\n",
      "Epoch: 264 | Batch: 008 / 011 | Total loss: 1.705 | Reg loss: 0.042 | Tree loss: 1.705 | Accuracy: 0.418000 | 0.941 sec/iter\n",
      "Epoch: 264 | Batch: 009 / 011 | Total loss: 1.672 | Reg loss: 0.042 | Tree loss: 1.672 | Accuracy: 0.445500 | 0.941 sec/iter\n",
      "Epoch: 264 | Batch: 010 / 011 | Total loss: 1.707 | Reg loss: 0.042 | Tree loss: 1.707 | Accuracy: 0.416382 | 0.941 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 265 | Batch: 000 / 011 | Total loss: 1.862 | Reg loss: 0.042 | Tree loss: 1.862 | Accuracy: 0.416000 | 0.941 sec/iter\n",
      "Epoch: 265 | Batch: 001 / 011 | Total loss: 1.830 | Reg loss: 0.042 | Tree loss: 1.830 | Accuracy: 0.410500 | 0.941 sec/iter\n",
      "Epoch: 265 | Batch: 002 / 011 | Total loss: 1.784 | Reg loss: 0.042 | Tree loss: 1.784 | Accuracy: 0.407500 | 0.941 sec/iter\n",
      "Epoch: 265 | Batch: 003 / 011 | Total loss: 1.772 | Reg loss: 0.042 | Tree loss: 1.772 | Accuracy: 0.403500 | 0.941 sec/iter\n",
      "Epoch: 265 | Batch: 004 / 011 | Total loss: 1.737 | Reg loss: 0.042 | Tree loss: 1.737 | Accuracy: 0.428500 | 0.941 sec/iter\n",
      "Epoch: 265 | Batch: 005 / 011 | Total loss: 1.686 | Reg loss: 0.042 | Tree loss: 1.686 | Accuracy: 0.435500 | 0.941 sec/iter\n",
      "Epoch: 265 | Batch: 006 / 011 | Total loss: 1.728 | Reg loss: 0.042 | Tree loss: 1.728 | Accuracy: 0.418000 | 0.941 sec/iter\n",
      "Epoch: 265 | Batch: 007 / 011 | Total loss: 1.713 | Reg loss: 0.042 | Tree loss: 1.713 | Accuracy: 0.421000 | 0.941 sec/iter\n",
      "Epoch: 265 | Batch: 008 / 011 | Total loss: 1.711 | Reg loss: 0.042 | Tree loss: 1.711 | Accuracy: 0.417000 | 0.94 sec/iter\n",
      "Epoch: 265 | Batch: 009 / 011 | Total loss: 1.702 | Reg loss: 0.042 | Tree loss: 1.702 | Accuracy: 0.418000 | 0.94 sec/iter\n",
      "Epoch: 265 | Batch: 010 / 011 | Total loss: 1.680 | Reg loss: 0.042 | Tree loss: 1.680 | Accuracy: 0.443686 | 0.94 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 266 | Batch: 000 / 011 | Total loss: 1.864 | Reg loss: 0.042 | Tree loss: 1.864 | Accuracy: 0.394000 | 0.94 sec/iter\n",
      "Epoch: 266 | Batch: 001 / 011 | Total loss: 1.823 | Reg loss: 0.042 | Tree loss: 1.823 | Accuracy: 0.412500 | 0.94 sec/iter\n",
      "Epoch: 266 | Batch: 002 / 011 | Total loss: 1.802 | Reg loss: 0.042 | Tree loss: 1.802 | Accuracy: 0.421000 | 0.94 sec/iter\n",
      "Epoch: 266 | Batch: 003 / 011 | Total loss: 1.777 | Reg loss: 0.042 | Tree loss: 1.777 | Accuracy: 0.406000 | 0.94 sec/iter\n",
      "Epoch: 266 | Batch: 004 / 011 | Total loss: 1.748 | Reg loss: 0.042 | Tree loss: 1.748 | Accuracy: 0.424500 | 0.94 sec/iter\n",
      "Epoch: 266 | Batch: 005 / 011 | Total loss: 1.722 | Reg loss: 0.042 | Tree loss: 1.722 | Accuracy: 0.424500 | 0.94 sec/iter\n",
      "Epoch: 266 | Batch: 006 / 011 | Total loss: 1.670 | Reg loss: 0.042 | Tree loss: 1.670 | Accuracy: 0.441000 | 0.94 sec/iter\n",
      "Epoch: 266 | Batch: 007 / 011 | Total loss: 1.693 | Reg loss: 0.042 | Tree loss: 1.693 | Accuracy: 0.437000 | 0.94 sec/iter\n",
      "Epoch: 266 | Batch: 008 / 011 | Total loss: 1.701 | Reg loss: 0.042 | Tree loss: 1.701 | Accuracy: 0.413500 | 0.94 sec/iter\n",
      "Epoch: 266 | Batch: 009 / 011 | Total loss: 1.685 | Reg loss: 0.042 | Tree loss: 1.685 | Accuracy: 0.420500 | 0.94 sec/iter\n",
      "Epoch: 266 | Batch: 010 / 011 | Total loss: 1.743 | Reg loss: 0.042 | Tree loss: 1.743 | Accuracy: 0.344710 | 0.94 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 267 | Batch: 000 / 011 | Total loss: 1.855 | Reg loss: 0.042 | Tree loss: 1.855 | Accuracy: 0.402500 | 0.94 sec/iter\n",
      "Epoch: 267 | Batch: 001 / 011 | Total loss: 1.833 | Reg loss: 0.042 | Tree loss: 1.833 | Accuracy: 0.403500 | 0.94 sec/iter\n",
      "Epoch: 267 | Batch: 002 / 011 | Total loss: 1.798 | Reg loss: 0.042 | Tree loss: 1.798 | Accuracy: 0.399000 | 0.94 sec/iter\n",
      "Epoch: 267 | Batch: 003 / 011 | Total loss: 1.764 | Reg loss: 0.042 | Tree loss: 1.764 | Accuracy: 0.425000 | 0.94 sec/iter\n",
      "Epoch: 267 | Batch: 004 / 011 | Total loss: 1.750 | Reg loss: 0.042 | Tree loss: 1.750 | Accuracy: 0.427500 | 0.94 sec/iter\n",
      "Epoch: 267 | Batch: 005 / 011 | Total loss: 1.707 | Reg loss: 0.042 | Tree loss: 1.707 | Accuracy: 0.440500 | 0.94 sec/iter\n",
      "Epoch: 267 | Batch: 006 / 011 | Total loss: 1.705 | Reg loss: 0.042 | Tree loss: 1.705 | Accuracy: 0.425000 | 0.94 sec/iter\n",
      "Epoch: 267 | Batch: 007 / 011 | Total loss: 1.690 | Reg loss: 0.042 | Tree loss: 1.690 | Accuracy: 0.423000 | 0.94 sec/iter\n",
      "Epoch: 267 | Batch: 008 / 011 | Total loss: 1.713 | Reg loss: 0.042 | Tree loss: 1.713 | Accuracy: 0.418000 | 0.94 sec/iter\n",
      "Epoch: 267 | Batch: 009 / 011 | Total loss: 1.705 | Reg loss: 0.042 | Tree loss: 1.705 | Accuracy: 0.415000 | 0.94 sec/iter\n",
      "Epoch: 267 | Batch: 010 / 011 | Total loss: 1.723 | Reg loss: 0.042 | Tree loss: 1.723 | Accuracy: 0.399317 | 0.94 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 268 | Batch: 000 / 011 | Total loss: 1.851 | Reg loss: 0.042 | Tree loss: 1.851 | Accuracy: 0.417500 | 0.94 sec/iter\n",
      "Epoch: 268 | Batch: 001 / 011 | Total loss: 1.845 | Reg loss: 0.042 | Tree loss: 1.845 | Accuracy: 0.396500 | 0.94 sec/iter\n",
      "Epoch: 268 | Batch: 002 / 011 | Total loss: 1.813 | Reg loss: 0.042 | Tree loss: 1.813 | Accuracy: 0.409000 | 0.94 sec/iter\n",
      "Epoch: 268 | Batch: 003 / 011 | Total loss: 1.765 | Reg loss: 0.042 | Tree loss: 1.765 | Accuracy: 0.417500 | 0.94 sec/iter\n",
      "Epoch: 268 | Batch: 004 / 011 | Total loss: 1.742 | Reg loss: 0.042 | Tree loss: 1.742 | Accuracy: 0.411500 | 0.94 sec/iter\n",
      "Epoch: 268 | Batch: 005 / 011 | Total loss: 1.690 | Reg loss: 0.042 | Tree loss: 1.690 | Accuracy: 0.438000 | 0.94 sec/iter\n",
      "Epoch: 268 | Batch: 006 / 011 | Total loss: 1.709 | Reg loss: 0.042 | Tree loss: 1.709 | Accuracy: 0.421500 | 0.94 sec/iter\n",
      "Epoch: 268 | Batch: 007 / 011 | Total loss: 1.698 | Reg loss: 0.042 | Tree loss: 1.698 | Accuracy: 0.439000 | 0.94 sec/iter\n",
      "Epoch: 268 | Batch: 008 / 011 | Total loss: 1.705 | Reg loss: 0.042 | Tree loss: 1.705 | Accuracy: 0.414000 | 0.94 sec/iter\n",
      "Epoch: 268 | Batch: 009 / 011 | Total loss: 1.688 | Reg loss: 0.042 | Tree loss: 1.688 | Accuracy: 0.418500 | 0.94 sec/iter\n",
      "Epoch: 268 | Batch: 010 / 011 | Total loss: 1.709 | Reg loss: 0.042 | Tree loss: 1.709 | Accuracy: 0.416382 | 0.94 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 269 | Batch: 000 / 011 | Total loss: 1.850 | Reg loss: 0.042 | Tree loss: 1.850 | Accuracy: 0.415500 | 0.94 sec/iter\n",
      "Epoch: 269 | Batch: 001 / 011 | Total loss: 1.825 | Reg loss: 0.042 | Tree loss: 1.825 | Accuracy: 0.415500 | 0.94 sec/iter\n",
      "Epoch: 269 | Batch: 002 / 011 | Total loss: 1.784 | Reg loss: 0.042 | Tree loss: 1.784 | Accuracy: 0.430000 | 0.94 sec/iter\n",
      "Epoch: 269 | Batch: 003 / 011 | Total loss: 1.761 | Reg loss: 0.042 | Tree loss: 1.761 | Accuracy: 0.430000 | 0.94 sec/iter\n",
      "Epoch: 269 | Batch: 004 / 011 | Total loss: 1.743 | Reg loss: 0.042 | Tree loss: 1.743 | Accuracy: 0.425000 | 0.94 sec/iter\n",
      "Epoch: 269 | Batch: 005 / 011 | Total loss: 1.743 | Reg loss: 0.042 | Tree loss: 1.743 | Accuracy: 0.400500 | 0.94 sec/iter\n",
      "Epoch: 269 | Batch: 006 / 011 | Total loss: 1.699 | Reg loss: 0.042 | Tree loss: 1.699 | Accuracy: 0.432000 | 0.94 sec/iter\n",
      "Epoch: 269 | Batch: 007 / 011 | Total loss: 1.694 | Reg loss: 0.042 | Tree loss: 1.694 | Accuracy: 0.441000 | 0.94 sec/iter\n",
      "Epoch: 269 | Batch: 008 / 011 | Total loss: 1.703 | Reg loss: 0.042 | Tree loss: 1.703 | Accuracy: 0.420000 | 0.94 sec/iter\n",
      "Epoch: 269 | Batch: 009 / 011 | Total loss: 1.694 | Reg loss: 0.042 | Tree loss: 1.694 | Accuracy: 0.407500 | 0.94 sec/iter\n",
      "Epoch: 269 | Batch: 010 / 011 | Total loss: 1.739 | Reg loss: 0.042 | Tree loss: 1.739 | Accuracy: 0.365188 | 0.94 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 270 | Batch: 000 / 011 | Total loss: 1.880 | Reg loss: 0.042 | Tree loss: 1.880 | Accuracy: 0.392500 | 0.94 sec/iter\n",
      "Epoch: 270 | Batch: 001 / 011 | Total loss: 1.824 | Reg loss: 0.042 | Tree loss: 1.824 | Accuracy: 0.424500 | 0.94 sec/iter\n",
      "Epoch: 270 | Batch: 002 / 011 | Total loss: 1.810 | Reg loss: 0.042 | Tree loss: 1.810 | Accuracy: 0.428000 | 0.94 sec/iter\n",
      "Epoch: 270 | Batch: 003 / 011 | Total loss: 1.763 | Reg loss: 0.042 | Tree loss: 1.763 | Accuracy: 0.422000 | 0.94 sec/iter\n",
      "Epoch: 270 | Batch: 004 / 011 | Total loss: 1.767 | Reg loss: 0.042 | Tree loss: 1.767 | Accuracy: 0.395500 | 0.94 sec/iter\n",
      "Epoch: 270 | Batch: 005 / 011 | Total loss: 1.733 | Reg loss: 0.042 | Tree loss: 1.733 | Accuracy: 0.427500 | 0.94 sec/iter\n",
      "Epoch: 270 | Batch: 006 / 011 | Total loss: 1.685 | Reg loss: 0.042 | Tree loss: 1.685 | Accuracy: 0.430000 | 0.94 sec/iter\n",
      "Epoch: 270 | Batch: 007 / 011 | Total loss: 1.716 | Reg loss: 0.042 | Tree loss: 1.716 | Accuracy: 0.405000 | 0.94 sec/iter\n",
      "Epoch: 270 | Batch: 008 / 011 | Total loss: 1.678 | Reg loss: 0.042 | Tree loss: 1.678 | Accuracy: 0.434500 | 0.94 sec/iter\n",
      "Epoch: 270 | Batch: 009 / 011 | Total loss: 1.678 | Reg loss: 0.042 | Tree loss: 1.678 | Accuracy: 0.428000 | 0.94 sec/iter\n",
      "Epoch: 270 | Batch: 010 / 011 | Total loss: 1.683 | Reg loss: 0.042 | Tree loss: 1.683 | Accuracy: 0.423208 | 0.94 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 271 | Batch: 000 / 011 | Total loss: 1.878 | Reg loss: 0.042 | Tree loss: 1.878 | Accuracy: 0.401000 | 0.94 sec/iter\n",
      "Epoch: 271 | Batch: 001 / 011 | Total loss: 1.843 | Reg loss: 0.042 | Tree loss: 1.843 | Accuracy: 0.422500 | 0.94 sec/iter\n",
      "Epoch: 271 | Batch: 002 / 011 | Total loss: 1.789 | Reg loss: 0.042 | Tree loss: 1.789 | Accuracy: 0.425000 | 0.94 sec/iter\n",
      "Epoch: 271 | Batch: 003 / 011 | Total loss: 1.776 | Reg loss: 0.042 | Tree loss: 1.776 | Accuracy: 0.403000 | 0.94 sec/iter\n",
      "Epoch: 271 | Batch: 004 / 011 | Total loss: 1.734 | Reg loss: 0.042 | Tree loss: 1.734 | Accuracy: 0.419500 | 0.94 sec/iter\n",
      "Epoch: 271 | Batch: 005 / 011 | Total loss: 1.733 | Reg loss: 0.042 | Tree loss: 1.733 | Accuracy: 0.421500 | 0.94 sec/iter\n",
      "Epoch: 271 | Batch: 006 / 011 | Total loss: 1.737 | Reg loss: 0.042 | Tree loss: 1.737 | Accuracy: 0.410000 | 0.94 sec/iter\n",
      "Epoch: 271 | Batch: 007 / 011 | Total loss: 1.673 | Reg loss: 0.042 | Tree loss: 1.673 | Accuracy: 0.428500 | 0.94 sec/iter\n",
      "Epoch: 271 | Batch: 008 / 011 | Total loss: 1.690 | Reg loss: 0.042 | Tree loss: 1.690 | Accuracy: 0.431500 | 0.94 sec/iter\n",
      "Epoch: 271 | Batch: 009 / 011 | Total loss: 1.678 | Reg loss: 0.042 | Tree loss: 1.678 | Accuracy: 0.432500 | 0.94 sec/iter\n",
      "Epoch: 271 | Batch: 010 / 011 | Total loss: 1.731 | Reg loss: 0.042 | Tree loss: 1.731 | Accuracy: 0.430034 | 0.94 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 272 | Batch: 000 / 011 | Total loss: 1.870 | Reg loss: 0.042 | Tree loss: 1.870 | Accuracy: 0.420000 | 0.94 sec/iter\n",
      "Epoch: 272 | Batch: 001 / 011 | Total loss: 1.835 | Reg loss: 0.042 | Tree loss: 1.835 | Accuracy: 0.406500 | 0.94 sec/iter\n",
      "Epoch: 272 | Batch: 002 / 011 | Total loss: 1.787 | Reg loss: 0.042 | Tree loss: 1.787 | Accuracy: 0.423000 | 0.94 sec/iter\n",
      "Epoch: 272 | Batch: 003 / 011 | Total loss: 1.760 | Reg loss: 0.042 | Tree loss: 1.760 | Accuracy: 0.410000 | 0.94 sec/iter\n",
      "Epoch: 272 | Batch: 004 / 011 | Total loss: 1.734 | Reg loss: 0.042 | Tree loss: 1.734 | Accuracy: 0.425000 | 0.94 sec/iter\n",
      "Epoch: 272 | Batch: 005 / 011 | Total loss: 1.746 | Reg loss: 0.042 | Tree loss: 1.746 | Accuracy: 0.405000 | 0.94 sec/iter\n",
      "Epoch: 272 | Batch: 006 / 011 | Total loss: 1.705 | Reg loss: 0.042 | Tree loss: 1.705 | Accuracy: 0.426000 | 0.94 sec/iter\n",
      "Epoch: 272 | Batch: 007 / 011 | Total loss: 1.672 | Reg loss: 0.042 | Tree loss: 1.672 | Accuracy: 0.431000 | 0.94 sec/iter\n",
      "Epoch: 272 | Batch: 008 / 011 | Total loss: 1.710 | Reg loss: 0.042 | Tree loss: 1.710 | Accuracy: 0.430000 | 0.94 sec/iter\n",
      "Epoch: 272 | Batch: 009 / 011 | Total loss: 1.693 | Reg loss: 0.042 | Tree loss: 1.693 | Accuracy: 0.424000 | 0.94 sec/iter\n",
      "Epoch: 272 | Batch: 010 / 011 | Total loss: 1.718 | Reg loss: 0.042 | Tree loss: 1.718 | Accuracy: 0.423208 | 0.94 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 273 | Batch: 000 / 011 | Total loss: 1.869 | Reg loss: 0.042 | Tree loss: 1.869 | Accuracy: 0.403500 | 0.94 sec/iter\n",
      "Epoch: 273 | Batch: 001 / 011 | Total loss: 1.858 | Reg loss: 0.042 | Tree loss: 1.858 | Accuracy: 0.414000 | 0.94 sec/iter\n",
      "Epoch: 273 | Batch: 002 / 011 | Total loss: 1.812 | Reg loss: 0.042 | Tree loss: 1.812 | Accuracy: 0.393500 | 0.94 sec/iter\n",
      "Epoch: 273 | Batch: 003 / 011 | Total loss: 1.758 | Reg loss: 0.042 | Tree loss: 1.758 | Accuracy: 0.421000 | 0.94 sec/iter\n",
      "Epoch: 273 | Batch: 004 / 011 | Total loss: 1.740 | Reg loss: 0.042 | Tree loss: 1.740 | Accuracy: 0.431500 | 0.94 sec/iter\n",
      "Epoch: 273 | Batch: 005 / 011 | Total loss: 1.742 | Reg loss: 0.042 | Tree loss: 1.742 | Accuracy: 0.412500 | 0.94 sec/iter\n",
      "Epoch: 273 | Batch: 006 / 011 | Total loss: 1.710 | Reg loss: 0.042 | Tree loss: 1.710 | Accuracy: 0.428500 | 0.94 sec/iter\n",
      "Epoch: 273 | Batch: 007 / 011 | Total loss: 1.676 | Reg loss: 0.042 | Tree loss: 1.676 | Accuracy: 0.449000 | 0.94 sec/iter\n",
      "Epoch: 273 | Batch: 008 / 011 | Total loss: 1.704 | Reg loss: 0.042 | Tree loss: 1.704 | Accuracy: 0.417000 | 0.94 sec/iter\n",
      "Epoch: 273 | Batch: 009 / 011 | Total loss: 1.660 | Reg loss: 0.042 | Tree loss: 1.660 | Accuracy: 0.424500 | 0.94 sec/iter\n",
      "Epoch: 273 | Batch: 010 / 011 | Total loss: 1.771 | Reg loss: 0.042 | Tree loss: 1.771 | Accuracy: 0.375427 | 0.94 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 274 | Batch: 000 / 011 | Total loss: 1.838 | Reg loss: 0.042 | Tree loss: 1.838 | Accuracy: 0.413500 | 0.94 sec/iter\n",
      "Epoch: 274 | Batch: 001 / 011 | Total loss: 1.829 | Reg loss: 0.042 | Tree loss: 1.829 | Accuracy: 0.406000 | 0.94 sec/iter\n",
      "Epoch: 274 | Batch: 002 / 011 | Total loss: 1.813 | Reg loss: 0.042 | Tree loss: 1.813 | Accuracy: 0.420500 | 0.94 sec/iter\n",
      "Epoch: 274 | Batch: 003 / 011 | Total loss: 1.773 | Reg loss: 0.042 | Tree loss: 1.773 | Accuracy: 0.422500 | 0.94 sec/iter\n",
      "Epoch: 274 | Batch: 004 / 011 | Total loss: 1.721 | Reg loss: 0.042 | Tree loss: 1.721 | Accuracy: 0.428500 | 0.94 sec/iter\n",
      "Epoch: 274 | Batch: 005 / 011 | Total loss: 1.754 | Reg loss: 0.042 | Tree loss: 1.754 | Accuracy: 0.427500 | 0.94 sec/iter\n",
      "Epoch: 274 | Batch: 006 / 011 | Total loss: 1.686 | Reg loss: 0.042 | Tree loss: 1.686 | Accuracy: 0.437000 | 0.94 sec/iter\n",
      "Epoch: 274 | Batch: 007 / 011 | Total loss: 1.695 | Reg loss: 0.042 | Tree loss: 1.695 | Accuracy: 0.428000 | 0.94 sec/iter\n",
      "Epoch: 274 | Batch: 008 / 011 | Total loss: 1.725 | Reg loss: 0.042 | Tree loss: 1.725 | Accuracy: 0.421000 | 0.94 sec/iter\n",
      "Epoch: 274 | Batch: 009 / 011 | Total loss: 1.697 | Reg loss: 0.042 | Tree loss: 1.697 | Accuracy: 0.415500 | 0.94 sec/iter\n",
      "Epoch: 274 | Batch: 010 / 011 | Total loss: 1.688 | Reg loss: 0.042 | Tree loss: 1.688 | Accuracy: 0.433447 | 0.94 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 275 | Batch: 000 / 011 | Total loss: 1.857 | Reg loss: 0.042 | Tree loss: 1.857 | Accuracy: 0.412500 | 0.94 sec/iter\n",
      "Epoch: 275 | Batch: 001 / 011 | Total loss: 1.826 | Reg loss: 0.042 | Tree loss: 1.826 | Accuracy: 0.424500 | 0.94 sec/iter\n",
      "Epoch: 275 | Batch: 002 / 011 | Total loss: 1.786 | Reg loss: 0.042 | Tree loss: 1.786 | Accuracy: 0.410000 | 0.94 sec/iter\n",
      "Epoch: 275 | Batch: 003 / 011 | Total loss: 1.790 | Reg loss: 0.042 | Tree loss: 1.790 | Accuracy: 0.414000 | 0.94 sec/iter\n",
      "Epoch: 275 | Batch: 004 / 011 | Total loss: 1.762 | Reg loss: 0.042 | Tree loss: 1.762 | Accuracy: 0.422500 | 0.94 sec/iter\n",
      "Epoch: 275 | Batch: 005 / 011 | Total loss: 1.730 | Reg loss: 0.042 | Tree loss: 1.730 | Accuracy: 0.420500 | 0.94 sec/iter\n",
      "Epoch: 275 | Batch: 006 / 011 | Total loss: 1.710 | Reg loss: 0.042 | Tree loss: 1.710 | Accuracy: 0.424500 | 0.94 sec/iter\n",
      "Epoch: 275 | Batch: 007 / 011 | Total loss: 1.674 | Reg loss: 0.042 | Tree loss: 1.674 | Accuracy: 0.428500 | 0.94 sec/iter\n",
      "Epoch: 275 | Batch: 008 / 011 | Total loss: 1.680 | Reg loss: 0.042 | Tree loss: 1.680 | Accuracy: 0.417500 | 0.939 sec/iter\n",
      "Epoch: 275 | Batch: 009 / 011 | Total loss: 1.698 | Reg loss: 0.042 | Tree loss: 1.698 | Accuracy: 0.423500 | 0.939 sec/iter\n",
      "Epoch: 275 | Batch: 010 / 011 | Total loss: 1.694 | Reg loss: 0.042 | Tree loss: 1.694 | Accuracy: 0.423208 | 0.939 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 276 | Batch: 000 / 011 | Total loss: 1.861 | Reg loss: 0.042 | Tree loss: 1.861 | Accuracy: 0.409500 | 0.939 sec/iter\n",
      "Epoch: 276 | Batch: 001 / 011 | Total loss: 1.863 | Reg loss: 0.042 | Tree loss: 1.863 | Accuracy: 0.393000 | 0.939 sec/iter\n",
      "Epoch: 276 | Batch: 002 / 011 | Total loss: 1.802 | Reg loss: 0.042 | Tree loss: 1.802 | Accuracy: 0.404000 | 0.939 sec/iter\n",
      "Epoch: 276 | Batch: 003 / 011 | Total loss: 1.780 | Reg loss: 0.042 | Tree loss: 1.780 | Accuracy: 0.420000 | 0.939 sec/iter\n",
      "Epoch: 276 | Batch: 004 / 011 | Total loss: 1.749 | Reg loss: 0.042 | Tree loss: 1.749 | Accuracy: 0.419500 | 0.939 sec/iter\n",
      "Epoch: 276 | Batch: 005 / 011 | Total loss: 1.732 | Reg loss: 0.042 | Tree loss: 1.732 | Accuracy: 0.427500 | 0.939 sec/iter\n",
      "Epoch: 276 | Batch: 006 / 011 | Total loss: 1.684 | Reg loss: 0.042 | Tree loss: 1.684 | Accuracy: 0.432500 | 0.939 sec/iter\n",
      "Epoch: 276 | Batch: 007 / 011 | Total loss: 1.702 | Reg loss: 0.042 | Tree loss: 1.702 | Accuracy: 0.408500 | 0.939 sec/iter\n",
      "Epoch: 276 | Batch: 008 / 011 | Total loss: 1.654 | Reg loss: 0.042 | Tree loss: 1.654 | Accuracy: 0.447500 | 0.939 sec/iter\n",
      "Epoch: 276 | Batch: 009 / 011 | Total loss: 1.691 | Reg loss: 0.042 | Tree loss: 1.691 | Accuracy: 0.422500 | 0.939 sec/iter\n",
      "Epoch: 276 | Batch: 010 / 011 | Total loss: 1.679 | Reg loss: 0.042 | Tree loss: 1.679 | Accuracy: 0.436860 | 0.939 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 277 | Batch: 000 / 011 | Total loss: 1.857 | Reg loss: 0.042 | Tree loss: 1.857 | Accuracy: 0.408500 | 0.939 sec/iter\n",
      "Epoch: 277 | Batch: 001 / 011 | Total loss: 1.840 | Reg loss: 0.042 | Tree loss: 1.840 | Accuracy: 0.415000 | 0.939 sec/iter\n",
      "Epoch: 277 | Batch: 002 / 011 | Total loss: 1.797 | Reg loss: 0.042 | Tree loss: 1.797 | Accuracy: 0.432500 | 0.939 sec/iter\n",
      "Epoch: 277 | Batch: 003 / 011 | Total loss: 1.797 | Reg loss: 0.042 | Tree loss: 1.797 | Accuracy: 0.412000 | 0.939 sec/iter\n",
      "Epoch: 277 | Batch: 004 / 011 | Total loss: 1.761 | Reg loss: 0.042 | Tree loss: 1.761 | Accuracy: 0.421500 | 0.939 sec/iter\n",
      "Epoch: 277 | Batch: 005 / 011 | Total loss: 1.695 | Reg loss: 0.042 | Tree loss: 1.695 | Accuracy: 0.439500 | 0.939 sec/iter\n",
      "Epoch: 277 | Batch: 006 / 011 | Total loss: 1.720 | Reg loss: 0.042 | Tree loss: 1.720 | Accuracy: 0.411000 | 0.939 sec/iter\n",
      "Epoch: 277 | Batch: 007 / 011 | Total loss: 1.702 | Reg loss: 0.042 | Tree loss: 1.702 | Accuracy: 0.418500 | 0.939 sec/iter\n",
      "Epoch: 277 | Batch: 008 / 011 | Total loss: 1.682 | Reg loss: 0.042 | Tree loss: 1.682 | Accuracy: 0.409000 | 0.939 sec/iter\n",
      "Epoch: 277 | Batch: 009 / 011 | Total loss: 1.660 | Reg loss: 0.042 | Tree loss: 1.660 | Accuracy: 0.434500 | 0.939 sec/iter\n",
      "Epoch: 277 | Batch: 010 / 011 | Total loss: 1.695 | Reg loss: 0.042 | Tree loss: 1.695 | Accuracy: 0.419795 | 0.939 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 278 | Batch: 000 / 011 | Total loss: 1.846 | Reg loss: 0.042 | Tree loss: 1.846 | Accuracy: 0.405500 | 0.939 sec/iter\n",
      "Epoch: 278 | Batch: 001 / 011 | Total loss: 1.851 | Reg loss: 0.042 | Tree loss: 1.851 | Accuracy: 0.404500 | 0.939 sec/iter\n",
      "Epoch: 278 | Batch: 002 / 011 | Total loss: 1.807 | Reg loss: 0.042 | Tree loss: 1.807 | Accuracy: 0.416500 | 0.939 sec/iter\n",
      "Epoch: 278 | Batch: 003 / 011 | Total loss: 1.808 | Reg loss: 0.042 | Tree loss: 1.808 | Accuracy: 0.388000 | 0.939 sec/iter\n",
      "Epoch: 278 | Batch: 004 / 011 | Total loss: 1.741 | Reg loss: 0.042 | Tree loss: 1.741 | Accuracy: 0.417000 | 0.939 sec/iter\n",
      "Epoch: 278 | Batch: 005 / 011 | Total loss: 1.716 | Reg loss: 0.042 | Tree loss: 1.716 | Accuracy: 0.436500 | 0.939 sec/iter\n",
      "Epoch: 278 | Batch: 006 / 011 | Total loss: 1.706 | Reg loss: 0.042 | Tree loss: 1.706 | Accuracy: 0.412000 | 0.939 sec/iter\n",
      "Epoch: 278 | Batch: 007 / 011 | Total loss: 1.678 | Reg loss: 0.042 | Tree loss: 1.678 | Accuracy: 0.441000 | 0.939 sec/iter\n",
      "Epoch: 278 | Batch: 008 / 011 | Total loss: 1.709 | Reg loss: 0.042 | Tree loss: 1.709 | Accuracy: 0.418500 | 0.939 sec/iter\n",
      "Epoch: 278 | Batch: 009 / 011 | Total loss: 1.672 | Reg loss: 0.042 | Tree loss: 1.672 | Accuracy: 0.439000 | 0.939 sec/iter\n",
      "Epoch: 278 | Batch: 010 / 011 | Total loss: 1.686 | Reg loss: 0.043 | Tree loss: 1.686 | Accuracy: 0.419795 | 0.939 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 279 | Batch: 000 / 011 | Total loss: 1.850 | Reg loss: 0.042 | Tree loss: 1.850 | Accuracy: 0.421500 | 0.939 sec/iter\n",
      "Epoch: 279 | Batch: 001 / 011 | Total loss: 1.840 | Reg loss: 0.042 | Tree loss: 1.840 | Accuracy: 0.406000 | 0.939 sec/iter\n",
      "Epoch: 279 | Batch: 002 / 011 | Total loss: 1.787 | Reg loss: 0.042 | Tree loss: 1.787 | Accuracy: 0.415500 | 0.939 sec/iter\n",
      "Epoch: 279 | Batch: 003 / 011 | Total loss: 1.758 | Reg loss: 0.042 | Tree loss: 1.758 | Accuracy: 0.417500 | 0.939 sec/iter\n",
      "Epoch: 279 | Batch: 004 / 011 | Total loss: 1.755 | Reg loss: 0.042 | Tree loss: 1.755 | Accuracy: 0.422500 | 0.939 sec/iter\n",
      "Epoch: 279 | Batch: 005 / 011 | Total loss: 1.732 | Reg loss: 0.042 | Tree loss: 1.732 | Accuracy: 0.415000 | 0.939 sec/iter\n",
      "Epoch: 279 | Batch: 006 / 011 | Total loss: 1.698 | Reg loss: 0.042 | Tree loss: 1.698 | Accuracy: 0.431000 | 0.939 sec/iter\n",
      "Epoch: 279 | Batch: 007 / 011 | Total loss: 1.695 | Reg loss: 0.042 | Tree loss: 1.695 | Accuracy: 0.428500 | 0.939 sec/iter\n",
      "Epoch: 279 | Batch: 008 / 011 | Total loss: 1.714 | Reg loss: 0.042 | Tree loss: 1.714 | Accuracy: 0.406500 | 0.939 sec/iter\n",
      "Epoch: 279 | Batch: 009 / 011 | Total loss: 1.698 | Reg loss: 0.042 | Tree loss: 1.698 | Accuracy: 0.420000 | 0.939 sec/iter\n",
      "Epoch: 279 | Batch: 010 / 011 | Total loss: 1.592 | Reg loss: 0.043 | Tree loss: 1.592 | Accuracy: 0.464164 | 0.939 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 280 | Batch: 000 / 011 | Total loss: 1.869 | Reg loss: 0.042 | Tree loss: 1.869 | Accuracy: 0.407000 | 0.939 sec/iter\n",
      "Epoch: 280 | Batch: 001 / 011 | Total loss: 1.839 | Reg loss: 0.042 | Tree loss: 1.839 | Accuracy: 0.412500 | 0.939 sec/iter\n",
      "Epoch: 280 | Batch: 002 / 011 | Total loss: 1.808 | Reg loss: 0.042 | Tree loss: 1.808 | Accuracy: 0.419500 | 0.939 sec/iter\n",
      "Epoch: 280 | Batch: 003 / 011 | Total loss: 1.761 | Reg loss: 0.042 | Tree loss: 1.761 | Accuracy: 0.417500 | 0.939 sec/iter\n",
      "Epoch: 280 | Batch: 004 / 011 | Total loss: 1.767 | Reg loss: 0.042 | Tree loss: 1.767 | Accuracy: 0.424500 | 0.939 sec/iter\n",
      "Epoch: 280 | Batch: 005 / 011 | Total loss: 1.724 | Reg loss: 0.042 | Tree loss: 1.724 | Accuracy: 0.417000 | 0.939 sec/iter\n",
      "Epoch: 280 | Batch: 006 / 011 | Total loss: 1.704 | Reg loss: 0.042 | Tree loss: 1.704 | Accuracy: 0.414000 | 0.939 sec/iter\n",
      "Epoch: 280 | Batch: 007 / 011 | Total loss: 1.690 | Reg loss: 0.042 | Tree loss: 1.690 | Accuracy: 0.420000 | 0.939 sec/iter\n",
      "Epoch: 280 | Batch: 008 / 011 | Total loss: 1.701 | Reg loss: 0.042 | Tree loss: 1.701 | Accuracy: 0.419500 | 0.939 sec/iter\n",
      "Epoch: 280 | Batch: 009 / 011 | Total loss: 1.685 | Reg loss: 0.042 | Tree loss: 1.685 | Accuracy: 0.421000 | 0.939 sec/iter\n",
      "Epoch: 280 | Batch: 010 / 011 | Total loss: 1.593 | Reg loss: 0.043 | Tree loss: 1.593 | Accuracy: 0.453925 | 0.939 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 281 | Batch: 000 / 011 | Total loss: 1.870 | Reg loss: 0.042 | Tree loss: 1.870 | Accuracy: 0.400500 | 0.939 sec/iter\n",
      "Epoch: 281 | Batch: 001 / 011 | Total loss: 1.821 | Reg loss: 0.042 | Tree loss: 1.821 | Accuracy: 0.407500 | 0.939 sec/iter\n",
      "Epoch: 281 | Batch: 002 / 011 | Total loss: 1.783 | Reg loss: 0.042 | Tree loss: 1.783 | Accuracy: 0.427000 | 0.939 sec/iter\n",
      "Epoch: 281 | Batch: 003 / 011 | Total loss: 1.762 | Reg loss: 0.042 | Tree loss: 1.762 | Accuracy: 0.423000 | 0.939 sec/iter\n",
      "Epoch: 281 | Batch: 004 / 011 | Total loss: 1.720 | Reg loss: 0.042 | Tree loss: 1.720 | Accuracy: 0.430500 | 0.939 sec/iter\n",
      "Epoch: 281 | Batch: 005 / 011 | Total loss: 1.727 | Reg loss: 0.042 | Tree loss: 1.727 | Accuracy: 0.416000 | 0.939 sec/iter\n",
      "Epoch: 281 | Batch: 006 / 011 | Total loss: 1.708 | Reg loss: 0.042 | Tree loss: 1.708 | Accuracy: 0.426000 | 0.939 sec/iter\n",
      "Epoch: 281 | Batch: 007 / 011 | Total loss: 1.716 | Reg loss: 0.042 | Tree loss: 1.716 | Accuracy: 0.420500 | 0.939 sec/iter\n",
      "Epoch: 281 | Batch: 008 / 011 | Total loss: 1.703 | Reg loss: 0.042 | Tree loss: 1.703 | Accuracy: 0.417000 | 0.939 sec/iter\n",
      "Epoch: 281 | Batch: 009 / 011 | Total loss: 1.701 | Reg loss: 0.043 | Tree loss: 1.701 | Accuracy: 0.431000 | 0.939 sec/iter\n",
      "Epoch: 281 | Batch: 010 / 011 | Total loss: 1.631 | Reg loss: 0.043 | Tree loss: 1.631 | Accuracy: 0.474403 | 0.939 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 282 | Batch: 000 / 011 | Total loss: 1.851 | Reg loss: 0.042 | Tree loss: 1.851 | Accuracy: 0.408000 | 0.939 sec/iter\n",
      "Epoch: 282 | Batch: 001 / 011 | Total loss: 1.834 | Reg loss: 0.042 | Tree loss: 1.834 | Accuracy: 0.412500 | 0.939 sec/iter\n",
      "Epoch: 282 | Batch: 002 / 011 | Total loss: 1.803 | Reg loss: 0.042 | Tree loss: 1.803 | Accuracy: 0.413000 | 0.939 sec/iter\n",
      "Epoch: 282 | Batch: 003 / 011 | Total loss: 1.770 | Reg loss: 0.042 | Tree loss: 1.770 | Accuracy: 0.420000 | 0.939 sec/iter\n",
      "Epoch: 282 | Batch: 004 / 011 | Total loss: 1.753 | Reg loss: 0.042 | Tree loss: 1.753 | Accuracy: 0.422000 | 0.939 sec/iter\n",
      "Epoch: 282 | Batch: 005 / 011 | Total loss: 1.730 | Reg loss: 0.042 | Tree loss: 1.730 | Accuracy: 0.414000 | 0.939 sec/iter\n",
      "Epoch: 282 | Batch: 006 / 011 | Total loss: 1.715 | Reg loss: 0.042 | Tree loss: 1.715 | Accuracy: 0.409000 | 0.939 sec/iter\n",
      "Epoch: 282 | Batch: 007 / 011 | Total loss: 1.691 | Reg loss: 0.042 | Tree loss: 1.691 | Accuracy: 0.426500 | 0.939 sec/iter\n",
      "Epoch: 282 | Batch: 008 / 011 | Total loss: 1.698 | Reg loss: 0.042 | Tree loss: 1.698 | Accuracy: 0.426000 | 0.939 sec/iter\n",
      "Epoch: 282 | Batch: 009 / 011 | Total loss: 1.686 | Reg loss: 0.043 | Tree loss: 1.686 | Accuracy: 0.425000 | 0.939 sec/iter\n",
      "Epoch: 282 | Batch: 010 / 011 | Total loss: 1.626 | Reg loss: 0.043 | Tree loss: 1.626 | Accuracy: 0.457338 | 0.939 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 283 | Batch: 000 / 011 | Total loss: 1.842 | Reg loss: 0.042 | Tree loss: 1.842 | Accuracy: 0.436500 | 0.939 sec/iter\n",
      "Epoch: 283 | Batch: 001 / 011 | Total loss: 1.824 | Reg loss: 0.042 | Tree loss: 1.824 | Accuracy: 0.419000 | 0.939 sec/iter\n",
      "Epoch: 283 | Batch: 002 / 011 | Total loss: 1.803 | Reg loss: 0.042 | Tree loss: 1.803 | Accuracy: 0.400000 | 0.939 sec/iter\n",
      "Epoch: 283 | Batch: 003 / 011 | Total loss: 1.752 | Reg loss: 0.042 | Tree loss: 1.752 | Accuracy: 0.421000 | 0.939 sec/iter\n",
      "Epoch: 283 | Batch: 004 / 011 | Total loss: 1.761 | Reg loss: 0.042 | Tree loss: 1.761 | Accuracy: 0.407000 | 0.939 sec/iter\n",
      "Epoch: 283 | Batch: 005 / 011 | Total loss: 1.726 | Reg loss: 0.042 | Tree loss: 1.726 | Accuracy: 0.399500 | 0.939 sec/iter\n",
      "Epoch: 283 | Batch: 006 / 011 | Total loss: 1.713 | Reg loss: 0.042 | Tree loss: 1.713 | Accuracy: 0.433000 | 0.939 sec/iter\n",
      "Epoch: 283 | Batch: 007 / 011 | Total loss: 1.729 | Reg loss: 0.042 | Tree loss: 1.729 | Accuracy: 0.409500 | 0.939 sec/iter\n",
      "Epoch: 283 | Batch: 008 / 011 | Total loss: 1.683 | Reg loss: 0.042 | Tree loss: 1.683 | Accuracy: 0.437000 | 0.939 sec/iter\n",
      "Epoch: 283 | Batch: 009 / 011 | Total loss: 1.673 | Reg loss: 0.043 | Tree loss: 1.673 | Accuracy: 0.426000 | 0.939 sec/iter\n",
      "Epoch: 283 | Batch: 010 / 011 | Total loss: 1.684 | Reg loss: 0.043 | Tree loss: 1.684 | Accuracy: 0.426621 | 0.939 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 284 | Batch: 000 / 011 | Total loss: 1.860 | Reg loss: 0.042 | Tree loss: 1.860 | Accuracy: 0.409000 | 0.939 sec/iter\n",
      "Epoch: 284 | Batch: 001 / 011 | Total loss: 1.823 | Reg loss: 0.042 | Tree loss: 1.823 | Accuracy: 0.414000 | 0.939 sec/iter\n",
      "Epoch: 284 | Batch: 002 / 011 | Total loss: 1.777 | Reg loss: 0.042 | Tree loss: 1.777 | Accuracy: 0.432000 | 0.939 sec/iter\n",
      "Epoch: 284 | Batch: 003 / 011 | Total loss: 1.766 | Reg loss: 0.042 | Tree loss: 1.766 | Accuracy: 0.417500 | 0.939 sec/iter\n",
      "Epoch: 284 | Batch: 004 / 011 | Total loss: 1.744 | Reg loss: 0.042 | Tree loss: 1.744 | Accuracy: 0.414000 | 0.939 sec/iter\n",
      "Epoch: 284 | Batch: 005 / 011 | Total loss: 1.724 | Reg loss: 0.042 | Tree loss: 1.724 | Accuracy: 0.427000 | 0.939 sec/iter\n",
      "Epoch: 284 | Batch: 006 / 011 | Total loss: 1.701 | Reg loss: 0.042 | Tree loss: 1.701 | Accuracy: 0.414500 | 0.939 sec/iter\n",
      "Epoch: 284 | Batch: 007 / 011 | Total loss: 1.727 | Reg loss: 0.042 | Tree loss: 1.727 | Accuracy: 0.411000 | 0.939 sec/iter\n",
      "Epoch: 284 | Batch: 008 / 011 | Total loss: 1.723 | Reg loss: 0.042 | Tree loss: 1.723 | Accuracy: 0.407000 | 0.939 sec/iter\n",
      "Epoch: 284 | Batch: 009 / 011 | Total loss: 1.676 | Reg loss: 0.043 | Tree loss: 1.676 | Accuracy: 0.413000 | 0.939 sec/iter\n",
      "Epoch: 284 | Batch: 010 / 011 | Total loss: 1.696 | Reg loss: 0.043 | Tree loss: 1.696 | Accuracy: 0.399317 | 0.939 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 285 | Batch: 000 / 011 | Total loss: 1.861 | Reg loss: 0.042 | Tree loss: 1.861 | Accuracy: 0.397500 | 0.939 sec/iter\n",
      "Epoch: 285 | Batch: 001 / 011 | Total loss: 1.831 | Reg loss: 0.042 | Tree loss: 1.831 | Accuracy: 0.423000 | 0.939 sec/iter\n",
      "Epoch: 285 | Batch: 002 / 011 | Total loss: 1.802 | Reg loss: 0.042 | Tree loss: 1.802 | Accuracy: 0.418500 | 0.939 sec/iter\n",
      "Epoch: 285 | Batch: 003 / 011 | Total loss: 1.785 | Reg loss: 0.042 | Tree loss: 1.785 | Accuracy: 0.408000 | 0.939 sec/iter\n",
      "Epoch: 285 | Batch: 004 / 011 | Total loss: 1.748 | Reg loss: 0.042 | Tree loss: 1.748 | Accuracy: 0.428000 | 0.939 sec/iter\n",
      "Epoch: 285 | Batch: 005 / 011 | Total loss: 1.733 | Reg loss: 0.042 | Tree loss: 1.733 | Accuracy: 0.407500 | 0.939 sec/iter\n",
      "Epoch: 285 | Batch: 006 / 011 | Total loss: 1.690 | Reg loss: 0.042 | Tree loss: 1.690 | Accuracy: 0.440000 | 0.939 sec/iter\n",
      "Epoch: 285 | Batch: 007 / 011 | Total loss: 1.694 | Reg loss: 0.042 | Tree loss: 1.694 | Accuracy: 0.424500 | 0.939 sec/iter\n",
      "Epoch: 285 | Batch: 008 / 011 | Total loss: 1.688 | Reg loss: 0.042 | Tree loss: 1.688 | Accuracy: 0.421500 | 0.939 sec/iter\n",
      "Epoch: 285 | Batch: 009 / 011 | Total loss: 1.693 | Reg loss: 0.043 | Tree loss: 1.693 | Accuracy: 0.421500 | 0.939 sec/iter\n",
      "Epoch: 285 | Batch: 010 / 011 | Total loss: 1.687 | Reg loss: 0.043 | Tree loss: 1.687 | Accuracy: 0.382253 | 0.938 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 286 | Batch: 000 / 011 | Total loss: 1.858 | Reg loss: 0.042 | Tree loss: 1.858 | Accuracy: 0.405000 | 0.939 sec/iter\n",
      "Epoch: 286 | Batch: 001 / 011 | Total loss: 1.828 | Reg loss: 0.042 | Tree loss: 1.828 | Accuracy: 0.424500 | 0.939 sec/iter\n",
      "Epoch: 286 | Batch: 002 / 011 | Total loss: 1.827 | Reg loss: 0.042 | Tree loss: 1.827 | Accuracy: 0.411000 | 0.939 sec/iter\n",
      "Epoch: 286 | Batch: 003 / 011 | Total loss: 1.778 | Reg loss: 0.042 | Tree loss: 1.778 | Accuracy: 0.420500 | 0.938 sec/iter\n",
      "Epoch: 286 | Batch: 004 / 011 | Total loss: 1.731 | Reg loss: 0.042 | Tree loss: 1.731 | Accuracy: 0.428000 | 0.938 sec/iter\n",
      "Epoch: 286 | Batch: 005 / 011 | Total loss: 1.724 | Reg loss: 0.042 | Tree loss: 1.724 | Accuracy: 0.415500 | 0.938 sec/iter\n",
      "Epoch: 286 | Batch: 006 / 011 | Total loss: 1.736 | Reg loss: 0.042 | Tree loss: 1.736 | Accuracy: 0.394500 | 0.938 sec/iter\n",
      "Epoch: 286 | Batch: 007 / 011 | Total loss: 1.684 | Reg loss: 0.042 | Tree loss: 1.684 | Accuracy: 0.426000 | 0.938 sec/iter\n",
      "Epoch: 286 | Batch: 008 / 011 | Total loss: 1.690 | Reg loss: 0.042 | Tree loss: 1.690 | Accuracy: 0.412500 | 0.938 sec/iter\n",
      "Epoch: 286 | Batch: 009 / 011 | Total loss: 1.680 | Reg loss: 0.043 | Tree loss: 1.680 | Accuracy: 0.426500 | 0.938 sec/iter\n",
      "Epoch: 286 | Batch: 010 / 011 | Total loss: 1.704 | Reg loss: 0.043 | Tree loss: 1.704 | Accuracy: 0.402730 | 0.938 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 287 | Batch: 000 / 011 | Total loss: 1.874 | Reg loss: 0.042 | Tree loss: 1.874 | Accuracy: 0.395000 | 0.938 sec/iter\n",
      "Epoch: 287 | Batch: 001 / 011 | Total loss: 1.840 | Reg loss: 0.042 | Tree loss: 1.840 | Accuracy: 0.398000 | 0.938 sec/iter\n",
      "Epoch: 287 | Batch: 002 / 011 | Total loss: 1.777 | Reg loss: 0.042 | Tree loss: 1.777 | Accuracy: 0.433500 | 0.938 sec/iter\n",
      "Epoch: 287 | Batch: 003 / 011 | Total loss: 1.760 | Reg loss: 0.042 | Tree loss: 1.760 | Accuracy: 0.426500 | 0.938 sec/iter\n",
      "Epoch: 287 | Batch: 004 / 011 | Total loss: 1.764 | Reg loss: 0.042 | Tree loss: 1.764 | Accuracy: 0.404500 | 0.938 sec/iter\n",
      "Epoch: 287 | Batch: 005 / 011 | Total loss: 1.745 | Reg loss: 0.042 | Tree loss: 1.745 | Accuracy: 0.403000 | 0.938 sec/iter\n",
      "Epoch: 287 | Batch: 006 / 011 | Total loss: 1.721 | Reg loss: 0.042 | Tree loss: 1.721 | Accuracy: 0.417500 | 0.938 sec/iter\n",
      "Epoch: 287 | Batch: 007 / 011 | Total loss: 1.693 | Reg loss: 0.042 | Tree loss: 1.693 | Accuracy: 0.423000 | 0.938 sec/iter\n",
      "Epoch: 287 | Batch: 008 / 011 | Total loss: 1.663 | Reg loss: 0.042 | Tree loss: 1.663 | Accuracy: 0.447000 | 0.938 sec/iter\n",
      "Epoch: 287 | Batch: 009 / 011 | Total loss: 1.701 | Reg loss: 0.043 | Tree loss: 1.701 | Accuracy: 0.425500 | 0.938 sec/iter\n",
      "Epoch: 287 | Batch: 010 / 011 | Total loss: 1.654 | Reg loss: 0.043 | Tree loss: 1.654 | Accuracy: 0.467577 | 0.938 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 288 | Batch: 000 / 011 | Total loss: 1.871 | Reg loss: 0.042 | Tree loss: 1.871 | Accuracy: 0.392000 | 0.938 sec/iter\n",
      "Epoch: 288 | Batch: 001 / 011 | Total loss: 1.806 | Reg loss: 0.042 | Tree loss: 1.806 | Accuracy: 0.422000 | 0.938 sec/iter\n",
      "Epoch: 288 | Batch: 002 / 011 | Total loss: 1.806 | Reg loss: 0.042 | Tree loss: 1.806 | Accuracy: 0.409500 | 0.938 sec/iter\n",
      "Epoch: 288 | Batch: 003 / 011 | Total loss: 1.775 | Reg loss: 0.042 | Tree loss: 1.775 | Accuracy: 0.408500 | 0.938 sec/iter\n",
      "Epoch: 288 | Batch: 004 / 011 | Total loss: 1.751 | Reg loss: 0.042 | Tree loss: 1.751 | Accuracy: 0.420500 | 0.938 sec/iter\n",
      "Epoch: 288 | Batch: 005 / 011 | Total loss: 1.709 | Reg loss: 0.042 | Tree loss: 1.709 | Accuracy: 0.424500 | 0.938 sec/iter\n",
      "Epoch: 288 | Batch: 006 / 011 | Total loss: 1.696 | Reg loss: 0.042 | Tree loss: 1.696 | Accuracy: 0.432500 | 0.938 sec/iter\n",
      "Epoch: 288 | Batch: 007 / 011 | Total loss: 1.721 | Reg loss: 0.042 | Tree loss: 1.721 | Accuracy: 0.415000 | 0.938 sec/iter\n",
      "Epoch: 288 | Batch: 008 / 011 | Total loss: 1.680 | Reg loss: 0.042 | Tree loss: 1.680 | Accuracy: 0.436500 | 0.938 sec/iter\n",
      "Epoch: 288 | Batch: 009 / 011 | Total loss: 1.682 | Reg loss: 0.043 | Tree loss: 1.682 | Accuracy: 0.426500 | 0.938 sec/iter\n",
      "Epoch: 288 | Batch: 010 / 011 | Total loss: 1.730 | Reg loss: 0.043 | Tree loss: 1.730 | Accuracy: 0.399317 | 0.938 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 289 | Batch: 000 / 011 | Total loss: 1.878 | Reg loss: 0.042 | Tree loss: 1.878 | Accuracy: 0.383000 | 0.938 sec/iter\n",
      "Epoch: 289 | Batch: 001 / 011 | Total loss: 1.845 | Reg loss: 0.042 | Tree loss: 1.845 | Accuracy: 0.397500 | 0.938 sec/iter\n",
      "Epoch: 289 | Batch: 002 / 011 | Total loss: 1.800 | Reg loss: 0.042 | Tree loss: 1.800 | Accuracy: 0.427000 | 0.938 sec/iter\n",
      "Epoch: 289 | Batch: 003 / 011 | Total loss: 1.786 | Reg loss: 0.042 | Tree loss: 1.786 | Accuracy: 0.429000 | 0.938 sec/iter\n",
      "Epoch: 289 | Batch: 004 / 011 | Total loss: 1.730 | Reg loss: 0.042 | Tree loss: 1.730 | Accuracy: 0.427000 | 0.938 sec/iter\n",
      "Epoch: 289 | Batch: 005 / 011 | Total loss: 1.702 | Reg loss: 0.042 | Tree loss: 1.702 | Accuracy: 0.429500 | 0.938 sec/iter\n",
      "Epoch: 289 | Batch: 006 / 011 | Total loss: 1.727 | Reg loss: 0.042 | Tree loss: 1.727 | Accuracy: 0.426000 | 0.938 sec/iter\n",
      "Epoch: 289 | Batch: 007 / 011 | Total loss: 1.715 | Reg loss: 0.042 | Tree loss: 1.715 | Accuracy: 0.394000 | 0.938 sec/iter\n",
      "Epoch: 289 | Batch: 008 / 011 | Total loss: 1.695 | Reg loss: 0.043 | Tree loss: 1.695 | Accuracy: 0.430000 | 0.938 sec/iter\n",
      "Epoch: 289 | Batch: 009 / 011 | Total loss: 1.661 | Reg loss: 0.043 | Tree loss: 1.661 | Accuracy: 0.443000 | 0.938 sec/iter\n",
      "Epoch: 289 | Batch: 010 / 011 | Total loss: 1.657 | Reg loss: 0.043 | Tree loss: 1.657 | Accuracy: 0.433447 | 0.938 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 290 | Batch: 000 / 011 | Total loss: 1.849 | Reg loss: 0.042 | Tree loss: 1.849 | Accuracy: 0.400500 | 0.938 sec/iter\n",
      "Epoch: 290 | Batch: 001 / 011 | Total loss: 1.822 | Reg loss: 0.042 | Tree loss: 1.822 | Accuracy: 0.402500 | 0.938 sec/iter\n",
      "Epoch: 290 | Batch: 002 / 011 | Total loss: 1.805 | Reg loss: 0.042 | Tree loss: 1.805 | Accuracy: 0.436000 | 0.938 sec/iter\n",
      "Epoch: 290 | Batch: 003 / 011 | Total loss: 1.776 | Reg loss: 0.042 | Tree loss: 1.776 | Accuracy: 0.410500 | 0.938 sec/iter\n",
      "Epoch: 290 | Batch: 004 / 011 | Total loss: 1.709 | Reg loss: 0.042 | Tree loss: 1.709 | Accuracy: 0.428000 | 0.938 sec/iter\n",
      "Epoch: 290 | Batch: 005 / 011 | Total loss: 1.723 | Reg loss: 0.042 | Tree loss: 1.723 | Accuracy: 0.435000 | 0.938 sec/iter\n",
      "Epoch: 290 | Batch: 006 / 011 | Total loss: 1.715 | Reg loss: 0.042 | Tree loss: 1.715 | Accuracy: 0.417500 | 0.938 sec/iter\n",
      "Epoch: 290 | Batch: 007 / 011 | Total loss: 1.711 | Reg loss: 0.042 | Tree loss: 1.711 | Accuracy: 0.426500 | 0.938 sec/iter\n",
      "Epoch: 290 | Batch: 008 / 011 | Total loss: 1.692 | Reg loss: 0.043 | Tree loss: 1.692 | Accuracy: 0.415500 | 0.938 sec/iter\n",
      "Epoch: 290 | Batch: 009 / 011 | Total loss: 1.711 | Reg loss: 0.043 | Tree loss: 1.711 | Accuracy: 0.413000 | 0.938 sec/iter\n",
      "Epoch: 290 | Batch: 010 / 011 | Total loss: 1.708 | Reg loss: 0.043 | Tree loss: 1.708 | Accuracy: 0.406143 | 0.938 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 291 | Batch: 000 / 011 | Total loss: 1.870 | Reg loss: 0.042 | Tree loss: 1.870 | Accuracy: 0.401000 | 0.938 sec/iter\n",
      "Epoch: 291 | Batch: 001 / 011 | Total loss: 1.830 | Reg loss: 0.042 | Tree loss: 1.830 | Accuracy: 0.420500 | 0.938 sec/iter\n",
      "Epoch: 291 | Batch: 002 / 011 | Total loss: 1.802 | Reg loss: 0.042 | Tree loss: 1.802 | Accuracy: 0.417000 | 0.938 sec/iter\n",
      "Epoch: 291 | Batch: 003 / 011 | Total loss: 1.774 | Reg loss: 0.042 | Tree loss: 1.774 | Accuracy: 0.418000 | 0.938 sec/iter\n",
      "Epoch: 291 | Batch: 004 / 011 | Total loss: 1.762 | Reg loss: 0.042 | Tree loss: 1.762 | Accuracy: 0.416000 | 0.938 sec/iter\n",
      "Epoch: 291 | Batch: 005 / 011 | Total loss: 1.720 | Reg loss: 0.042 | Tree loss: 1.720 | Accuracy: 0.415500 | 0.938 sec/iter\n",
      "Epoch: 291 | Batch: 006 / 011 | Total loss: 1.684 | Reg loss: 0.042 | Tree loss: 1.684 | Accuracy: 0.430500 | 0.938 sec/iter\n",
      "Epoch: 291 | Batch: 007 / 011 | Total loss: 1.686 | Reg loss: 0.042 | Tree loss: 1.686 | Accuracy: 0.422000 | 0.938 sec/iter\n",
      "Epoch: 291 | Batch: 008 / 011 | Total loss: 1.692 | Reg loss: 0.043 | Tree loss: 1.692 | Accuracy: 0.431000 | 0.938 sec/iter\n",
      "Epoch: 291 | Batch: 009 / 011 | Total loss: 1.696 | Reg loss: 0.043 | Tree loss: 1.696 | Accuracy: 0.418500 | 0.938 sec/iter\n",
      "Epoch: 291 | Batch: 010 / 011 | Total loss: 1.623 | Reg loss: 0.043 | Tree loss: 1.623 | Accuracy: 0.440273 | 0.938 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 292 | Batch: 000 / 011 | Total loss: 1.860 | Reg loss: 0.042 | Tree loss: 1.860 | Accuracy: 0.405000 | 0.938 sec/iter\n",
      "Epoch: 292 | Batch: 001 / 011 | Total loss: 1.826 | Reg loss: 0.042 | Tree loss: 1.826 | Accuracy: 0.410500 | 0.938 sec/iter\n",
      "Epoch: 292 | Batch: 002 / 011 | Total loss: 1.818 | Reg loss: 0.042 | Tree loss: 1.818 | Accuracy: 0.403500 | 0.938 sec/iter\n",
      "Epoch: 292 | Batch: 003 / 011 | Total loss: 1.782 | Reg loss: 0.042 | Tree loss: 1.782 | Accuracy: 0.407500 | 0.938 sec/iter\n",
      "Epoch: 292 | Batch: 004 / 011 | Total loss: 1.721 | Reg loss: 0.042 | Tree loss: 1.721 | Accuracy: 0.434500 | 0.938 sec/iter\n",
      "Epoch: 292 | Batch: 005 / 011 | Total loss: 1.728 | Reg loss: 0.042 | Tree loss: 1.728 | Accuracy: 0.405000 | 0.938 sec/iter\n",
      "Epoch: 292 | Batch: 006 / 011 | Total loss: 1.706 | Reg loss: 0.042 | Tree loss: 1.706 | Accuracy: 0.430000 | 0.938 sec/iter\n",
      "Epoch: 292 | Batch: 007 / 011 | Total loss: 1.681 | Reg loss: 0.042 | Tree loss: 1.681 | Accuracy: 0.424500 | 0.938 sec/iter\n",
      "Epoch: 292 | Batch: 008 / 011 | Total loss: 1.698 | Reg loss: 0.043 | Tree loss: 1.698 | Accuracy: 0.413500 | 0.938 sec/iter\n",
      "Epoch: 292 | Batch: 009 / 011 | Total loss: 1.686 | Reg loss: 0.043 | Tree loss: 1.686 | Accuracy: 0.410000 | 0.938 sec/iter\n",
      "Epoch: 292 | Batch: 010 / 011 | Total loss: 1.769 | Reg loss: 0.043 | Tree loss: 1.769 | Accuracy: 0.385666 | 0.938 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 293 | Batch: 000 / 011 | Total loss: 1.851 | Reg loss: 0.042 | Tree loss: 1.851 | Accuracy: 0.397500 | 0.938 sec/iter\n",
      "Epoch: 293 | Batch: 001 / 011 | Total loss: 1.845 | Reg loss: 0.042 | Tree loss: 1.845 | Accuracy: 0.419000 | 0.938 sec/iter\n",
      "Epoch: 293 | Batch: 002 / 011 | Total loss: 1.787 | Reg loss: 0.042 | Tree loss: 1.787 | Accuracy: 0.435500 | 0.938 sec/iter\n",
      "Epoch: 293 | Batch: 003 / 011 | Total loss: 1.786 | Reg loss: 0.042 | Tree loss: 1.786 | Accuracy: 0.415500 | 0.938 sec/iter\n",
      "Epoch: 293 | Batch: 004 / 011 | Total loss: 1.771 | Reg loss: 0.042 | Tree loss: 1.771 | Accuracy: 0.406000 | 0.938 sec/iter\n",
      "Epoch: 293 | Batch: 005 / 011 | Total loss: 1.721 | Reg loss: 0.042 | Tree loss: 1.721 | Accuracy: 0.419500 | 0.938 sec/iter\n",
      "Epoch: 293 | Batch: 006 / 011 | Total loss: 1.691 | Reg loss: 0.042 | Tree loss: 1.691 | Accuracy: 0.429500 | 0.938 sec/iter\n",
      "Epoch: 293 | Batch: 007 / 011 | Total loss: 1.706 | Reg loss: 0.042 | Tree loss: 1.706 | Accuracy: 0.428000 | 0.938 sec/iter\n",
      "Epoch: 293 | Batch: 008 / 011 | Total loss: 1.690 | Reg loss: 0.043 | Tree loss: 1.690 | Accuracy: 0.423500 | 0.938 sec/iter\n",
      "Epoch: 293 | Batch: 009 / 011 | Total loss: 1.677 | Reg loss: 0.043 | Tree loss: 1.677 | Accuracy: 0.426500 | 0.938 sec/iter\n",
      "Epoch: 293 | Batch: 010 / 011 | Total loss: 1.715 | Reg loss: 0.043 | Tree loss: 1.715 | Accuracy: 0.372014 | 0.938 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 294 | Batch: 000 / 011 | Total loss: 1.844 | Reg loss: 0.042 | Tree loss: 1.844 | Accuracy: 0.422000 | 0.938 sec/iter\n",
      "Epoch: 294 | Batch: 001 / 011 | Total loss: 1.835 | Reg loss: 0.042 | Tree loss: 1.835 | Accuracy: 0.405000 | 0.938 sec/iter\n",
      "Epoch: 294 | Batch: 002 / 011 | Total loss: 1.799 | Reg loss: 0.042 | Tree loss: 1.799 | Accuracy: 0.417000 | 0.938 sec/iter\n",
      "Epoch: 294 | Batch: 003 / 011 | Total loss: 1.772 | Reg loss: 0.042 | Tree loss: 1.772 | Accuracy: 0.422500 | 0.938 sec/iter\n",
      "Epoch: 294 | Batch: 004 / 011 | Total loss: 1.745 | Reg loss: 0.042 | Tree loss: 1.745 | Accuracy: 0.402000 | 0.938 sec/iter\n",
      "Epoch: 294 | Batch: 005 / 011 | Total loss: 1.703 | Reg loss: 0.042 | Tree loss: 1.703 | Accuracy: 0.434000 | 0.938 sec/iter\n",
      "Epoch: 294 | Batch: 006 / 011 | Total loss: 1.708 | Reg loss: 0.042 | Tree loss: 1.708 | Accuracy: 0.424000 | 0.938 sec/iter\n",
      "Epoch: 294 | Batch: 007 / 011 | Total loss: 1.725 | Reg loss: 0.042 | Tree loss: 1.725 | Accuracy: 0.408000 | 0.938 sec/iter\n",
      "Epoch: 294 | Batch: 008 / 011 | Total loss: 1.693 | Reg loss: 0.042 | Tree loss: 1.693 | Accuracy: 0.414000 | 0.938 sec/iter\n",
      "Epoch: 294 | Batch: 009 / 011 | Total loss: 1.692 | Reg loss: 0.043 | Tree loss: 1.692 | Accuracy: 0.427500 | 0.938 sec/iter\n",
      "Epoch: 294 | Batch: 010 / 011 | Total loss: 1.753 | Reg loss: 0.043 | Tree loss: 1.753 | Accuracy: 0.399317 | 0.938 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 295 | Batch: 000 / 011 | Total loss: 1.850 | Reg loss: 0.042 | Tree loss: 1.850 | Accuracy: 0.416000 | 0.938 sec/iter\n",
      "Epoch: 295 | Batch: 001 / 011 | Total loss: 1.830 | Reg loss: 0.042 | Tree loss: 1.830 | Accuracy: 0.412000 | 0.938 sec/iter\n",
      "Epoch: 295 | Batch: 002 / 011 | Total loss: 1.796 | Reg loss: 0.042 | Tree loss: 1.796 | Accuracy: 0.432500 | 0.938 sec/iter\n",
      "Epoch: 295 | Batch: 003 / 011 | Total loss: 1.786 | Reg loss: 0.042 | Tree loss: 1.786 | Accuracy: 0.430000 | 0.938 sec/iter\n",
      "Epoch: 295 | Batch: 004 / 011 | Total loss: 1.743 | Reg loss: 0.042 | Tree loss: 1.743 | Accuracy: 0.427500 | 0.938 sec/iter\n",
      "Epoch: 295 | Batch: 005 / 011 | Total loss: 1.738 | Reg loss: 0.042 | Tree loss: 1.738 | Accuracy: 0.422000 | 0.938 sec/iter\n",
      "Epoch: 295 | Batch: 006 / 011 | Total loss: 1.715 | Reg loss: 0.042 | Tree loss: 1.715 | Accuracy: 0.414500 | 0.938 sec/iter\n",
      "Epoch: 295 | Batch: 007 / 011 | Total loss: 1.679 | Reg loss: 0.042 | Tree loss: 1.679 | Accuracy: 0.432500 | 0.938 sec/iter\n",
      "Epoch: 295 | Batch: 008 / 011 | Total loss: 1.702 | Reg loss: 0.043 | Tree loss: 1.702 | Accuracy: 0.418500 | 0.938 sec/iter\n",
      "Epoch: 295 | Batch: 009 / 011 | Total loss: 1.675 | Reg loss: 0.043 | Tree loss: 1.675 | Accuracy: 0.431000 | 0.938 sec/iter\n",
      "Epoch: 295 | Batch: 010 / 011 | Total loss: 1.740 | Reg loss: 0.043 | Tree loss: 1.740 | Accuracy: 0.372014 | 0.938 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 296 | Batch: 000 / 011 | Total loss: 1.865 | Reg loss: 0.042 | Tree loss: 1.865 | Accuracy: 0.402000 | 0.938 sec/iter\n",
      "Epoch: 296 | Batch: 001 / 011 | Total loss: 1.816 | Reg loss: 0.042 | Tree loss: 1.816 | Accuracy: 0.411500 | 0.938 sec/iter\n",
      "Epoch: 296 | Batch: 002 / 011 | Total loss: 1.812 | Reg loss: 0.042 | Tree loss: 1.812 | Accuracy: 0.408500 | 0.938 sec/iter\n",
      "Epoch: 296 | Batch: 003 / 011 | Total loss: 1.776 | Reg loss: 0.042 | Tree loss: 1.776 | Accuracy: 0.415000 | 0.938 sec/iter\n",
      "Epoch: 296 | Batch: 004 / 011 | Total loss: 1.743 | Reg loss: 0.042 | Tree loss: 1.743 | Accuracy: 0.419500 | 0.938 sec/iter\n",
      "Epoch: 296 | Batch: 005 / 011 | Total loss: 1.724 | Reg loss: 0.042 | Tree loss: 1.724 | Accuracy: 0.416000 | 0.938 sec/iter\n",
      "Epoch: 296 | Batch: 006 / 011 | Total loss: 1.711 | Reg loss: 0.042 | Tree loss: 1.711 | Accuracy: 0.411500 | 0.938 sec/iter\n",
      "Epoch: 296 | Batch: 007 / 011 | Total loss: 1.686 | Reg loss: 0.042 | Tree loss: 1.686 | Accuracy: 0.428500 | 0.938 sec/iter\n",
      "Epoch: 296 | Batch: 008 / 011 | Total loss: 1.708 | Reg loss: 0.043 | Tree loss: 1.708 | Accuracy: 0.412500 | 0.938 sec/iter\n",
      "Epoch: 296 | Batch: 009 / 011 | Total loss: 1.685 | Reg loss: 0.043 | Tree loss: 1.685 | Accuracy: 0.427500 | 0.938 sec/iter\n",
      "Epoch: 296 | Batch: 010 / 011 | Total loss: 1.599 | Reg loss: 0.043 | Tree loss: 1.599 | Accuracy: 0.494881 | 0.938 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 297 | Batch: 000 / 011 | Total loss: 1.853 | Reg loss: 0.042 | Tree loss: 1.853 | Accuracy: 0.404000 | 0.938 sec/iter\n",
      "Epoch: 297 | Batch: 001 / 011 | Total loss: 1.854 | Reg loss: 0.042 | Tree loss: 1.854 | Accuracy: 0.390500 | 0.938 sec/iter\n",
      "Epoch: 297 | Batch: 002 / 011 | Total loss: 1.800 | Reg loss: 0.042 | Tree loss: 1.800 | Accuracy: 0.421500 | 0.938 sec/iter\n",
      "Epoch: 297 | Batch: 003 / 011 | Total loss: 1.771 | Reg loss: 0.042 | Tree loss: 1.771 | Accuracy: 0.409000 | 0.938 sec/iter\n",
      "Epoch: 297 | Batch: 004 / 011 | Total loss: 1.760 | Reg loss: 0.042 | Tree loss: 1.760 | Accuracy: 0.410000 | 0.938 sec/iter\n",
      "Epoch: 297 | Batch: 005 / 011 | Total loss: 1.718 | Reg loss: 0.042 | Tree loss: 1.718 | Accuracy: 0.444000 | 0.938 sec/iter\n",
      "Epoch: 297 | Batch: 006 / 011 | Total loss: 1.706 | Reg loss: 0.042 | Tree loss: 1.706 | Accuracy: 0.424000 | 0.938 sec/iter\n",
      "Epoch: 297 | Batch: 007 / 011 | Total loss: 1.662 | Reg loss: 0.042 | Tree loss: 1.662 | Accuracy: 0.425000 | 0.938 sec/iter\n",
      "Epoch: 297 | Batch: 008 / 011 | Total loss: 1.712 | Reg loss: 0.043 | Tree loss: 1.712 | Accuracy: 0.419500 | 0.938 sec/iter\n",
      "Epoch: 297 | Batch: 009 / 011 | Total loss: 1.673 | Reg loss: 0.043 | Tree loss: 1.673 | Accuracy: 0.432500 | 0.938 sec/iter\n",
      "Epoch: 297 | Batch: 010 / 011 | Total loss: 1.700 | Reg loss: 0.043 | Tree loss: 1.700 | Accuracy: 0.440273 | 0.937 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 298 | Batch: 000 / 011 | Total loss: 1.829 | Reg loss: 0.042 | Tree loss: 1.829 | Accuracy: 0.428000 | 0.938 sec/iter\n",
      "Epoch: 298 | Batch: 001 / 011 | Total loss: 1.778 | Reg loss: 0.042 | Tree loss: 1.778 | Accuracy: 0.435500 | 0.938 sec/iter\n",
      "Epoch: 298 | Batch: 002 / 011 | Total loss: 1.794 | Reg loss: 0.042 | Tree loss: 1.794 | Accuracy: 0.426000 | 0.938 sec/iter\n",
      "Epoch: 298 | Batch: 003 / 011 | Total loss: 1.779 | Reg loss: 0.042 | Tree loss: 1.779 | Accuracy: 0.409500 | 0.937 sec/iter\n",
      "Epoch: 298 | Batch: 004 / 011 | Total loss: 1.744 | Reg loss: 0.042 | Tree loss: 1.744 | Accuracy: 0.413500 | 0.937 sec/iter\n",
      "Epoch: 298 | Batch: 005 / 011 | Total loss: 1.732 | Reg loss: 0.042 | Tree loss: 1.732 | Accuracy: 0.405500 | 0.937 sec/iter\n",
      "Epoch: 298 | Batch: 006 / 011 | Total loss: 1.714 | Reg loss: 0.042 | Tree loss: 1.714 | Accuracy: 0.416500 | 0.937 sec/iter\n",
      "Epoch: 298 | Batch: 007 / 011 | Total loss: 1.725 | Reg loss: 0.042 | Tree loss: 1.725 | Accuracy: 0.402500 | 0.937 sec/iter\n",
      "Epoch: 298 | Batch: 008 / 011 | Total loss: 1.706 | Reg loss: 0.043 | Tree loss: 1.706 | Accuracy: 0.433000 | 0.937 sec/iter\n",
      "Epoch: 298 | Batch: 009 / 011 | Total loss: 1.688 | Reg loss: 0.043 | Tree loss: 1.688 | Accuracy: 0.420000 | 0.937 sec/iter\n",
      "Epoch: 298 | Batch: 010 / 011 | Total loss: 1.702 | Reg loss: 0.043 | Tree loss: 1.702 | Accuracy: 0.430034 | 0.937 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 299 | Batch: 000 / 011 | Total loss: 1.841 | Reg loss: 0.042 | Tree loss: 1.841 | Accuracy: 0.418500 | 0.937 sec/iter\n",
      "Epoch: 299 | Batch: 001 / 011 | Total loss: 1.839 | Reg loss: 0.042 | Tree loss: 1.839 | Accuracy: 0.400000 | 0.937 sec/iter\n",
      "Epoch: 299 | Batch: 002 / 011 | Total loss: 1.790 | Reg loss: 0.042 | Tree loss: 1.790 | Accuracy: 0.426500 | 0.937 sec/iter\n",
      "Epoch: 299 | Batch: 003 / 011 | Total loss: 1.773 | Reg loss: 0.042 | Tree loss: 1.773 | Accuracy: 0.413500 | 0.937 sec/iter\n",
      "Epoch: 299 | Batch: 004 / 011 | Total loss: 1.740 | Reg loss: 0.042 | Tree loss: 1.740 | Accuracy: 0.422500 | 0.937 sec/iter\n",
      "Epoch: 299 | Batch: 005 / 011 | Total loss: 1.719 | Reg loss: 0.042 | Tree loss: 1.719 | Accuracy: 0.416500 | 0.937 sec/iter\n",
      "Epoch: 299 | Batch: 006 / 011 | Total loss: 1.752 | Reg loss: 0.042 | Tree loss: 1.752 | Accuracy: 0.396500 | 0.937 sec/iter\n",
      "Epoch: 299 | Batch: 007 / 011 | Total loss: 1.666 | Reg loss: 0.042 | Tree loss: 1.666 | Accuracy: 0.444000 | 0.937 sec/iter\n",
      "Epoch: 299 | Batch: 008 / 011 | Total loss: 1.704 | Reg loss: 0.043 | Tree loss: 1.704 | Accuracy: 0.412500 | 0.937 sec/iter\n",
      "Epoch: 299 | Batch: 009 / 011 | Total loss: 1.709 | Reg loss: 0.043 | Tree loss: 1.709 | Accuracy: 0.434000 | 0.937 sec/iter\n",
      "Epoch: 299 | Batch: 010 / 011 | Total loss: 1.593 | Reg loss: 0.043 | Tree loss: 1.593 | Accuracy: 0.467577 | 0.937 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 300 | Batch: 000 / 011 | Total loss: 1.867 | Reg loss: 0.042 | Tree loss: 1.867 | Accuracy: 0.405500 | 0.937 sec/iter\n",
      "Epoch: 300 | Batch: 001 / 011 | Total loss: 1.834 | Reg loss: 0.042 | Tree loss: 1.834 | Accuracy: 0.422000 | 0.937 sec/iter\n",
      "Epoch: 300 | Batch: 002 / 011 | Total loss: 1.786 | Reg loss: 0.042 | Tree loss: 1.786 | Accuracy: 0.424500 | 0.937 sec/iter\n",
      "Epoch: 300 | Batch: 003 / 011 | Total loss: 1.773 | Reg loss: 0.042 | Tree loss: 1.773 | Accuracy: 0.417500 | 0.937 sec/iter\n",
      "Epoch: 300 | Batch: 004 / 011 | Total loss: 1.772 | Reg loss: 0.042 | Tree loss: 1.772 | Accuracy: 0.414000 | 0.937 sec/iter\n",
      "Epoch: 300 | Batch: 005 / 011 | Total loss: 1.728 | Reg loss: 0.042 | Tree loss: 1.728 | Accuracy: 0.420500 | 0.937 sec/iter\n",
      "Epoch: 300 | Batch: 006 / 011 | Total loss: 1.693 | Reg loss: 0.042 | Tree loss: 1.693 | Accuracy: 0.429500 | 0.937 sec/iter\n",
      "Epoch: 300 | Batch: 007 / 011 | Total loss: 1.695 | Reg loss: 0.042 | Tree loss: 1.695 | Accuracy: 0.423000 | 0.937 sec/iter\n",
      "Epoch: 300 | Batch: 008 / 011 | Total loss: 1.678 | Reg loss: 0.043 | Tree loss: 1.678 | Accuracy: 0.423000 | 0.937 sec/iter\n",
      "Epoch: 300 | Batch: 009 / 011 | Total loss: 1.672 | Reg loss: 0.043 | Tree loss: 1.672 | Accuracy: 0.424500 | 0.937 sec/iter\n",
      "Epoch: 300 | Batch: 010 / 011 | Total loss: 1.676 | Reg loss: 0.043 | Tree loss: 1.676 | Accuracy: 0.392491 | 0.937 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 301 | Batch: 000 / 011 | Total loss: 1.875 | Reg loss: 0.042 | Tree loss: 1.875 | Accuracy: 0.397500 | 0.937 sec/iter\n",
      "Epoch: 301 | Batch: 001 / 011 | Total loss: 1.825 | Reg loss: 0.042 | Tree loss: 1.825 | Accuracy: 0.416500 | 0.937 sec/iter\n",
      "Epoch: 301 | Batch: 002 / 011 | Total loss: 1.809 | Reg loss: 0.042 | Tree loss: 1.809 | Accuracy: 0.417000 | 0.937 sec/iter\n",
      "Epoch: 301 | Batch: 003 / 011 | Total loss: 1.766 | Reg loss: 0.042 | Tree loss: 1.766 | Accuracy: 0.413500 | 0.937 sec/iter\n",
      "Epoch: 301 | Batch: 004 / 011 | Total loss: 1.720 | Reg loss: 0.042 | Tree loss: 1.720 | Accuracy: 0.426500 | 0.937 sec/iter\n",
      "Epoch: 301 | Batch: 005 / 011 | Total loss: 1.710 | Reg loss: 0.042 | Tree loss: 1.710 | Accuracy: 0.428500 | 0.937 sec/iter\n",
      "Epoch: 301 | Batch: 006 / 011 | Total loss: 1.729 | Reg loss: 0.042 | Tree loss: 1.729 | Accuracy: 0.415000 | 0.937 sec/iter\n",
      "Epoch: 301 | Batch: 007 / 011 | Total loss: 1.711 | Reg loss: 0.042 | Tree loss: 1.711 | Accuracy: 0.418000 | 0.937 sec/iter\n",
      "Epoch: 301 | Batch: 008 / 011 | Total loss: 1.680 | Reg loss: 0.043 | Tree loss: 1.680 | Accuracy: 0.420000 | 0.937 sec/iter\n",
      "Epoch: 301 | Batch: 009 / 011 | Total loss: 1.693 | Reg loss: 0.043 | Tree loss: 1.693 | Accuracy: 0.426500 | 0.937 sec/iter\n",
      "Epoch: 301 | Batch: 010 / 011 | Total loss: 1.559 | Reg loss: 0.043 | Tree loss: 1.559 | Accuracy: 0.515358 | 0.937 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 302 | Batch: 000 / 011 | Total loss: 1.847 | Reg loss: 0.042 | Tree loss: 1.847 | Accuracy: 0.416500 | 0.937 sec/iter\n",
      "Epoch: 302 | Batch: 001 / 011 | Total loss: 1.816 | Reg loss: 0.042 | Tree loss: 1.816 | Accuracy: 0.409000 | 0.937 sec/iter\n",
      "Epoch: 302 | Batch: 002 / 011 | Total loss: 1.821 | Reg loss: 0.042 | Tree loss: 1.821 | Accuracy: 0.411500 | 0.937 sec/iter\n",
      "Epoch: 302 | Batch: 003 / 011 | Total loss: 1.780 | Reg loss: 0.042 | Tree loss: 1.780 | Accuracy: 0.410000 | 0.937 sec/iter\n",
      "Epoch: 302 | Batch: 004 / 011 | Total loss: 1.732 | Reg loss: 0.042 | Tree loss: 1.732 | Accuracy: 0.409000 | 0.937 sec/iter\n",
      "Epoch: 302 | Batch: 005 / 011 | Total loss: 1.725 | Reg loss: 0.042 | Tree loss: 1.725 | Accuracy: 0.421500 | 0.937 sec/iter\n",
      "Epoch: 302 | Batch: 006 / 011 | Total loss: 1.725 | Reg loss: 0.042 | Tree loss: 1.725 | Accuracy: 0.416500 | 0.937 sec/iter\n",
      "Epoch: 302 | Batch: 007 / 011 | Total loss: 1.692 | Reg loss: 0.043 | Tree loss: 1.692 | Accuracy: 0.424000 | 0.937 sec/iter\n",
      "Epoch: 302 | Batch: 008 / 011 | Total loss: 1.688 | Reg loss: 0.043 | Tree loss: 1.688 | Accuracy: 0.428500 | 0.937 sec/iter\n",
      "Epoch: 302 | Batch: 009 / 011 | Total loss: 1.680 | Reg loss: 0.043 | Tree loss: 1.680 | Accuracy: 0.419000 | 0.937 sec/iter\n",
      "Epoch: 302 | Batch: 010 / 011 | Total loss: 1.699 | Reg loss: 0.043 | Tree loss: 1.699 | Accuracy: 0.419795 | 0.937 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 303 | Batch: 000 / 011 | Total loss: 1.847 | Reg loss: 0.042 | Tree loss: 1.847 | Accuracy: 0.412500 | 0.937 sec/iter\n",
      "Epoch: 303 | Batch: 001 / 011 | Total loss: 1.825 | Reg loss: 0.042 | Tree loss: 1.825 | Accuracy: 0.418000 | 0.937 sec/iter\n",
      "Epoch: 303 | Batch: 002 / 011 | Total loss: 1.780 | Reg loss: 0.042 | Tree loss: 1.780 | Accuracy: 0.431500 | 0.937 sec/iter\n",
      "Epoch: 303 | Batch: 003 / 011 | Total loss: 1.778 | Reg loss: 0.042 | Tree loss: 1.778 | Accuracy: 0.421500 | 0.937 sec/iter\n",
      "Epoch: 303 | Batch: 004 / 011 | Total loss: 1.754 | Reg loss: 0.042 | Tree loss: 1.754 | Accuracy: 0.411000 | 0.937 sec/iter\n",
      "Epoch: 303 | Batch: 005 / 011 | Total loss: 1.731 | Reg loss: 0.042 | Tree loss: 1.731 | Accuracy: 0.417500 | 0.937 sec/iter\n",
      "Epoch: 303 | Batch: 006 / 011 | Total loss: 1.713 | Reg loss: 0.042 | Tree loss: 1.713 | Accuracy: 0.425000 | 0.937 sec/iter\n",
      "Epoch: 303 | Batch: 007 / 011 | Total loss: 1.703 | Reg loss: 0.043 | Tree loss: 1.703 | Accuracy: 0.410000 | 0.937 sec/iter\n",
      "Epoch: 303 | Batch: 008 / 011 | Total loss: 1.682 | Reg loss: 0.043 | Tree loss: 1.682 | Accuracy: 0.422500 | 0.937 sec/iter\n",
      "Epoch: 303 | Batch: 009 / 011 | Total loss: 1.711 | Reg loss: 0.043 | Tree loss: 1.711 | Accuracy: 0.422000 | 0.937 sec/iter\n",
      "Epoch: 303 | Batch: 010 / 011 | Total loss: 1.721 | Reg loss: 0.043 | Tree loss: 1.721 | Accuracy: 0.412969 | 0.937 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 304 | Batch: 000 / 011 | Total loss: 1.840 | Reg loss: 0.042 | Tree loss: 1.840 | Accuracy: 0.416500 | 0.937 sec/iter\n",
      "Epoch: 304 | Batch: 001 / 011 | Total loss: 1.824 | Reg loss: 0.042 | Tree loss: 1.824 | Accuracy: 0.420500 | 0.937 sec/iter\n",
      "Epoch: 304 | Batch: 002 / 011 | Total loss: 1.827 | Reg loss: 0.042 | Tree loss: 1.827 | Accuracy: 0.397000 | 0.937 sec/iter\n",
      "Epoch: 304 | Batch: 003 / 011 | Total loss: 1.760 | Reg loss: 0.042 | Tree loss: 1.760 | Accuracy: 0.408500 | 0.937 sec/iter\n",
      "Epoch: 304 | Batch: 004 / 011 | Total loss: 1.742 | Reg loss: 0.042 | Tree loss: 1.742 | Accuracy: 0.415000 | 0.937 sec/iter\n",
      "Epoch: 304 | Batch: 005 / 011 | Total loss: 1.711 | Reg loss: 0.042 | Tree loss: 1.711 | Accuracy: 0.438500 | 0.937 sec/iter\n",
      "Epoch: 304 | Batch: 006 / 011 | Total loss: 1.728 | Reg loss: 0.042 | Tree loss: 1.728 | Accuracy: 0.405000 | 0.937 sec/iter\n",
      "Epoch: 304 | Batch: 007 / 011 | Total loss: 1.677 | Reg loss: 0.043 | Tree loss: 1.677 | Accuracy: 0.439500 | 0.937 sec/iter\n",
      "Epoch: 304 | Batch: 008 / 011 | Total loss: 1.704 | Reg loss: 0.043 | Tree loss: 1.704 | Accuracy: 0.413500 | 0.937 sec/iter\n",
      "Epoch: 304 | Batch: 009 / 011 | Total loss: 1.703 | Reg loss: 0.043 | Tree loss: 1.703 | Accuracy: 0.411500 | 0.937 sec/iter\n",
      "Epoch: 304 | Batch: 010 / 011 | Total loss: 1.675 | Reg loss: 0.043 | Tree loss: 1.675 | Accuracy: 0.443686 | 0.937 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 305 | Batch: 000 / 011 | Total loss: 1.838 | Reg loss: 0.042 | Tree loss: 1.838 | Accuracy: 0.435000 | 0.937 sec/iter\n",
      "Epoch: 305 | Batch: 001 / 011 | Total loss: 1.833 | Reg loss: 0.042 | Tree loss: 1.833 | Accuracy: 0.405000 | 0.937 sec/iter\n",
      "Epoch: 305 | Batch: 002 / 011 | Total loss: 1.762 | Reg loss: 0.042 | Tree loss: 1.762 | Accuracy: 0.434000 | 0.937 sec/iter\n",
      "Epoch: 305 | Batch: 003 / 011 | Total loss: 1.790 | Reg loss: 0.042 | Tree loss: 1.790 | Accuracy: 0.414500 | 0.937 sec/iter\n",
      "Epoch: 305 | Batch: 004 / 011 | Total loss: 1.758 | Reg loss: 0.042 | Tree loss: 1.758 | Accuracy: 0.403000 | 0.937 sec/iter\n",
      "Epoch: 305 | Batch: 005 / 011 | Total loss: 1.736 | Reg loss: 0.042 | Tree loss: 1.736 | Accuracy: 0.411500 | 0.937 sec/iter\n",
      "Epoch: 305 | Batch: 006 / 011 | Total loss: 1.703 | Reg loss: 0.042 | Tree loss: 1.703 | Accuracy: 0.422000 | 0.937 sec/iter\n",
      "Epoch: 305 | Batch: 007 / 011 | Total loss: 1.701 | Reg loss: 0.043 | Tree loss: 1.701 | Accuracy: 0.420000 | 0.937 sec/iter\n",
      "Epoch: 305 | Batch: 008 / 011 | Total loss: 1.685 | Reg loss: 0.043 | Tree loss: 1.685 | Accuracy: 0.424000 | 0.937 sec/iter\n",
      "Epoch: 305 | Batch: 009 / 011 | Total loss: 1.682 | Reg loss: 0.043 | Tree loss: 1.682 | Accuracy: 0.432500 | 0.937 sec/iter\n",
      "Epoch: 305 | Batch: 010 / 011 | Total loss: 1.715 | Reg loss: 0.043 | Tree loss: 1.715 | Accuracy: 0.399317 | 0.937 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 306 | Batch: 000 / 011 | Total loss: 1.862 | Reg loss: 0.042 | Tree loss: 1.862 | Accuracy: 0.404000 | 0.937 sec/iter\n",
      "Epoch: 306 | Batch: 001 / 011 | Total loss: 1.830 | Reg loss: 0.042 | Tree loss: 1.830 | Accuracy: 0.418000 | 0.937 sec/iter\n",
      "Epoch: 306 | Batch: 002 / 011 | Total loss: 1.808 | Reg loss: 0.042 | Tree loss: 1.808 | Accuracy: 0.406500 | 0.937 sec/iter\n",
      "Epoch: 306 | Batch: 003 / 011 | Total loss: 1.794 | Reg loss: 0.042 | Tree loss: 1.794 | Accuracy: 0.387000 | 0.937 sec/iter\n",
      "Epoch: 306 | Batch: 004 / 011 | Total loss: 1.736 | Reg loss: 0.042 | Tree loss: 1.736 | Accuracy: 0.434000 | 0.937 sec/iter\n",
      "Epoch: 306 | Batch: 005 / 011 | Total loss: 1.734 | Reg loss: 0.042 | Tree loss: 1.734 | Accuracy: 0.407500 | 0.937 sec/iter\n",
      "Epoch: 306 | Batch: 006 / 011 | Total loss: 1.709 | Reg loss: 0.042 | Tree loss: 1.709 | Accuracy: 0.426500 | 0.937 sec/iter\n",
      "Epoch: 306 | Batch: 007 / 011 | Total loss: 1.671 | Reg loss: 0.043 | Tree loss: 1.671 | Accuracy: 0.438500 | 0.937 sec/iter\n",
      "Epoch: 306 | Batch: 008 / 011 | Total loss: 1.659 | Reg loss: 0.043 | Tree loss: 1.659 | Accuracy: 0.428000 | 0.937 sec/iter\n",
      "Epoch: 306 | Batch: 009 / 011 | Total loss: 1.700 | Reg loss: 0.043 | Tree loss: 1.700 | Accuracy: 0.420000 | 0.937 sec/iter\n",
      "Epoch: 306 | Batch: 010 / 011 | Total loss: 1.673 | Reg loss: 0.043 | Tree loss: 1.673 | Accuracy: 0.433447 | 0.937 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 307 | Batch: 000 / 011 | Total loss: 1.840 | Reg loss: 0.042 | Tree loss: 1.840 | Accuracy: 0.407500 | 0.937 sec/iter\n",
      "Epoch: 307 | Batch: 001 / 011 | Total loss: 1.805 | Reg loss: 0.042 | Tree loss: 1.805 | Accuracy: 0.426500 | 0.937 sec/iter\n",
      "Epoch: 307 | Batch: 002 / 011 | Total loss: 1.789 | Reg loss: 0.042 | Tree loss: 1.789 | Accuracy: 0.429500 | 0.937 sec/iter\n",
      "Epoch: 307 | Batch: 003 / 011 | Total loss: 1.778 | Reg loss: 0.042 | Tree loss: 1.778 | Accuracy: 0.416000 | 0.937 sec/iter\n",
      "Epoch: 307 | Batch: 004 / 011 | Total loss: 1.765 | Reg loss: 0.042 | Tree loss: 1.765 | Accuracy: 0.402000 | 0.937 sec/iter\n",
      "Epoch: 307 | Batch: 005 / 011 | Total loss: 1.710 | Reg loss: 0.042 | Tree loss: 1.710 | Accuracy: 0.436500 | 0.937 sec/iter\n",
      "Epoch: 307 | Batch: 006 / 011 | Total loss: 1.699 | Reg loss: 0.042 | Tree loss: 1.699 | Accuracy: 0.423000 | 0.937 sec/iter\n",
      "Epoch: 307 | Batch: 007 / 011 | Total loss: 1.677 | Reg loss: 0.042 | Tree loss: 1.677 | Accuracy: 0.425500 | 0.937 sec/iter\n",
      "Epoch: 307 | Batch: 008 / 011 | Total loss: 1.701 | Reg loss: 0.043 | Tree loss: 1.701 | Accuracy: 0.429500 | 0.937 sec/iter\n",
      "Epoch: 307 | Batch: 009 / 011 | Total loss: 1.711 | Reg loss: 0.043 | Tree loss: 1.711 | Accuracy: 0.401500 | 0.937 sec/iter\n",
      "Epoch: 307 | Batch: 010 / 011 | Total loss: 1.725 | Reg loss: 0.043 | Tree loss: 1.725 | Accuracy: 0.416382 | 0.937 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 308 | Batch: 000 / 011 | Total loss: 1.900 | Reg loss: 0.042 | Tree loss: 1.900 | Accuracy: 0.380500 | 0.937 sec/iter\n",
      "Epoch: 308 | Batch: 001 / 011 | Total loss: 1.830 | Reg loss: 0.042 | Tree loss: 1.830 | Accuracy: 0.431500 | 0.937 sec/iter\n",
      "Epoch: 308 | Batch: 002 / 011 | Total loss: 1.784 | Reg loss: 0.042 | Tree loss: 1.784 | Accuracy: 0.420000 | 0.937 sec/iter\n",
      "Epoch: 308 | Batch: 003 / 011 | Total loss: 1.779 | Reg loss: 0.042 | Tree loss: 1.779 | Accuracy: 0.413000 | 0.937 sec/iter\n",
      "Epoch: 308 | Batch: 004 / 011 | Total loss: 1.723 | Reg loss: 0.042 | Tree loss: 1.723 | Accuracy: 0.423000 | 0.937 sec/iter\n",
      "Epoch: 308 | Batch: 005 / 011 | Total loss: 1.723 | Reg loss: 0.042 | Tree loss: 1.723 | Accuracy: 0.421500 | 0.936 sec/iter\n",
      "Epoch: 308 | Batch: 006 / 011 | Total loss: 1.726 | Reg loss: 0.042 | Tree loss: 1.726 | Accuracy: 0.411000 | 0.936 sec/iter\n",
      "Epoch: 308 | Batch: 007 / 011 | Total loss: 1.684 | Reg loss: 0.043 | Tree loss: 1.684 | Accuracy: 0.426500 | 0.936 sec/iter\n",
      "Epoch: 308 | Batch: 008 / 011 | Total loss: 1.661 | Reg loss: 0.043 | Tree loss: 1.661 | Accuracy: 0.444500 | 0.936 sec/iter\n",
      "Epoch: 308 | Batch: 009 / 011 | Total loss: 1.709 | Reg loss: 0.043 | Tree loss: 1.709 | Accuracy: 0.421500 | 0.936 sec/iter\n",
      "Epoch: 308 | Batch: 010 / 011 | Total loss: 1.691 | Reg loss: 0.043 | Tree loss: 1.691 | Accuracy: 0.423208 | 0.936 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 309 | Batch: 000 / 011 | Total loss: 1.861 | Reg loss: 0.042 | Tree loss: 1.861 | Accuracy: 0.403500 | 0.936 sec/iter\n",
      "Epoch: 309 | Batch: 001 / 011 | Total loss: 1.842 | Reg loss: 0.042 | Tree loss: 1.842 | Accuracy: 0.395000 | 0.936 sec/iter\n",
      "Epoch: 309 | Batch: 002 / 011 | Total loss: 1.802 | Reg loss: 0.042 | Tree loss: 1.802 | Accuracy: 0.417500 | 0.936 sec/iter\n",
      "Epoch: 309 | Batch: 003 / 011 | Total loss: 1.763 | Reg loss: 0.042 | Tree loss: 1.763 | Accuracy: 0.423500 | 0.936 sec/iter\n",
      "Epoch: 309 | Batch: 004 / 011 | Total loss: 1.736 | Reg loss: 0.042 | Tree loss: 1.736 | Accuracy: 0.422500 | 0.936 sec/iter\n",
      "Epoch: 309 | Batch: 005 / 011 | Total loss: 1.741 | Reg loss: 0.042 | Tree loss: 1.741 | Accuracy: 0.415000 | 0.936 sec/iter\n",
      "Epoch: 309 | Batch: 006 / 011 | Total loss: 1.697 | Reg loss: 0.042 | Tree loss: 1.697 | Accuracy: 0.423000 | 0.936 sec/iter\n",
      "Epoch: 309 | Batch: 007 / 011 | Total loss: 1.700 | Reg loss: 0.043 | Tree loss: 1.700 | Accuracy: 0.426000 | 0.936 sec/iter\n",
      "Epoch: 309 | Batch: 008 / 011 | Total loss: 1.673 | Reg loss: 0.043 | Tree loss: 1.673 | Accuracy: 0.439000 | 0.936 sec/iter\n",
      "Epoch: 309 | Batch: 009 / 011 | Total loss: 1.682 | Reg loss: 0.043 | Tree loss: 1.682 | Accuracy: 0.416500 | 0.936 sec/iter\n",
      "Epoch: 309 | Batch: 010 / 011 | Total loss: 1.707 | Reg loss: 0.043 | Tree loss: 1.707 | Accuracy: 0.402730 | 0.936 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 310 | Batch: 000 / 011 | Total loss: 1.846 | Reg loss: 0.042 | Tree loss: 1.846 | Accuracy: 0.409500 | 0.936 sec/iter\n",
      "Epoch: 310 | Batch: 001 / 011 | Total loss: 1.824 | Reg loss: 0.042 | Tree loss: 1.824 | Accuracy: 0.416000 | 0.936 sec/iter\n",
      "Epoch: 310 | Batch: 002 / 011 | Total loss: 1.811 | Reg loss: 0.042 | Tree loss: 1.811 | Accuracy: 0.414000 | 0.936 sec/iter\n",
      "Epoch: 310 | Batch: 003 / 011 | Total loss: 1.772 | Reg loss: 0.042 | Tree loss: 1.772 | Accuracy: 0.420500 | 0.936 sec/iter\n",
      "Epoch: 310 | Batch: 004 / 011 | Total loss: 1.768 | Reg loss: 0.042 | Tree loss: 1.768 | Accuracy: 0.409000 | 0.936 sec/iter\n",
      "Epoch: 310 | Batch: 005 / 011 | Total loss: 1.723 | Reg loss: 0.042 | Tree loss: 1.723 | Accuracy: 0.440500 | 0.936 sec/iter\n",
      "Epoch: 310 | Batch: 006 / 011 | Total loss: 1.737 | Reg loss: 0.042 | Tree loss: 1.737 | Accuracy: 0.396500 | 0.936 sec/iter\n",
      "Epoch: 310 | Batch: 007 / 011 | Total loss: 1.681 | Reg loss: 0.043 | Tree loss: 1.681 | Accuracy: 0.434500 | 0.936 sec/iter\n",
      "Epoch: 310 | Batch: 008 / 011 | Total loss: 1.673 | Reg loss: 0.043 | Tree loss: 1.673 | Accuracy: 0.426000 | 0.936 sec/iter\n",
      "Epoch: 310 | Batch: 009 / 011 | Total loss: 1.659 | Reg loss: 0.043 | Tree loss: 1.659 | Accuracy: 0.430000 | 0.936 sec/iter\n",
      "Epoch: 310 | Batch: 010 / 011 | Total loss: 1.678 | Reg loss: 0.043 | Tree loss: 1.678 | Accuracy: 0.385666 | 0.936 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 311 | Batch: 000 / 011 | Total loss: 1.860 | Reg loss: 0.042 | Tree loss: 1.860 | Accuracy: 0.394500 | 0.936 sec/iter\n",
      "Epoch: 311 | Batch: 001 / 011 | Total loss: 1.838 | Reg loss: 0.042 | Tree loss: 1.838 | Accuracy: 0.407500 | 0.936 sec/iter\n",
      "Epoch: 311 | Batch: 002 / 011 | Total loss: 1.772 | Reg loss: 0.042 | Tree loss: 1.772 | Accuracy: 0.427000 | 0.936 sec/iter\n",
      "Epoch: 311 | Batch: 003 / 011 | Total loss: 1.801 | Reg loss: 0.042 | Tree loss: 1.801 | Accuracy: 0.405500 | 0.936 sec/iter\n",
      "Epoch: 311 | Batch: 004 / 011 | Total loss: 1.745 | Reg loss: 0.042 | Tree loss: 1.745 | Accuracy: 0.417500 | 0.936 sec/iter\n",
      "Epoch: 311 | Batch: 005 / 011 | Total loss: 1.705 | Reg loss: 0.042 | Tree loss: 1.705 | Accuracy: 0.433000 | 0.936 sec/iter\n",
      "Epoch: 311 | Batch: 006 / 011 | Total loss: 1.675 | Reg loss: 0.043 | Tree loss: 1.675 | Accuracy: 0.451000 | 0.936 sec/iter\n",
      "Epoch: 311 | Batch: 007 / 011 | Total loss: 1.713 | Reg loss: 0.043 | Tree loss: 1.713 | Accuracy: 0.409500 | 0.936 sec/iter\n",
      "Epoch: 311 | Batch: 008 / 011 | Total loss: 1.693 | Reg loss: 0.043 | Tree loss: 1.693 | Accuracy: 0.437500 | 0.936 sec/iter\n",
      "Epoch: 311 | Batch: 009 / 011 | Total loss: 1.689 | Reg loss: 0.043 | Tree loss: 1.689 | Accuracy: 0.418000 | 0.936 sec/iter\n",
      "Epoch: 311 | Batch: 010 / 011 | Total loss: 1.717 | Reg loss: 0.043 | Tree loss: 1.717 | Accuracy: 0.406143 | 0.936 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 312 | Batch: 000 / 011 | Total loss: 1.830 | Reg loss: 0.042 | Tree loss: 1.830 | Accuracy: 0.419500 | 0.936 sec/iter\n",
      "Epoch: 312 | Batch: 001 / 011 | Total loss: 1.856 | Reg loss: 0.042 | Tree loss: 1.856 | Accuracy: 0.404000 | 0.936 sec/iter\n",
      "Epoch: 312 | Batch: 002 / 011 | Total loss: 1.816 | Reg loss: 0.042 | Tree loss: 1.816 | Accuracy: 0.411500 | 0.936 sec/iter\n",
      "Epoch: 312 | Batch: 003 / 011 | Total loss: 1.762 | Reg loss: 0.042 | Tree loss: 1.762 | Accuracy: 0.427000 | 0.936 sec/iter\n",
      "Epoch: 312 | Batch: 004 / 011 | Total loss: 1.743 | Reg loss: 0.042 | Tree loss: 1.743 | Accuracy: 0.429000 | 0.936 sec/iter\n",
      "Epoch: 312 | Batch: 005 / 011 | Total loss: 1.708 | Reg loss: 0.042 | Tree loss: 1.708 | Accuracy: 0.436000 | 0.936 sec/iter\n",
      "Epoch: 312 | Batch: 006 / 011 | Total loss: 1.687 | Reg loss: 0.043 | Tree loss: 1.687 | Accuracy: 0.436000 | 0.936 sec/iter\n",
      "Epoch: 312 | Batch: 007 / 011 | Total loss: 1.685 | Reg loss: 0.043 | Tree loss: 1.685 | Accuracy: 0.407000 | 0.936 sec/iter\n",
      "Epoch: 312 | Batch: 008 / 011 | Total loss: 1.694 | Reg loss: 0.043 | Tree loss: 1.694 | Accuracy: 0.420000 | 0.936 sec/iter\n",
      "Epoch: 312 | Batch: 009 / 011 | Total loss: 1.727 | Reg loss: 0.043 | Tree loss: 1.727 | Accuracy: 0.410500 | 0.936 sec/iter\n",
      "Epoch: 312 | Batch: 010 / 011 | Total loss: 1.700 | Reg loss: 0.043 | Tree loss: 1.700 | Accuracy: 0.399317 | 0.936 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 313 | Batch: 000 / 011 | Total loss: 1.854 | Reg loss: 0.042 | Tree loss: 1.854 | Accuracy: 0.419500 | 0.936 sec/iter\n",
      "Epoch: 313 | Batch: 001 / 011 | Total loss: 1.815 | Reg loss: 0.042 | Tree loss: 1.815 | Accuracy: 0.421500 | 0.936 sec/iter\n",
      "Epoch: 313 | Batch: 002 / 011 | Total loss: 1.783 | Reg loss: 0.042 | Tree loss: 1.783 | Accuracy: 0.415500 | 0.936 sec/iter\n",
      "Epoch: 313 | Batch: 003 / 011 | Total loss: 1.771 | Reg loss: 0.042 | Tree loss: 1.771 | Accuracy: 0.422000 | 0.936 sec/iter\n",
      "Epoch: 313 | Batch: 004 / 011 | Total loss: 1.735 | Reg loss: 0.042 | Tree loss: 1.735 | Accuracy: 0.422000 | 0.936 sec/iter\n",
      "Epoch: 313 | Batch: 005 / 011 | Total loss: 1.719 | Reg loss: 0.043 | Tree loss: 1.719 | Accuracy: 0.430500 | 0.936 sec/iter\n",
      "Epoch: 313 | Batch: 006 / 011 | Total loss: 1.696 | Reg loss: 0.043 | Tree loss: 1.696 | Accuracy: 0.431500 | 0.936 sec/iter\n",
      "Epoch: 313 | Batch: 007 / 011 | Total loss: 1.717 | Reg loss: 0.043 | Tree loss: 1.717 | Accuracy: 0.410000 | 0.936 sec/iter\n",
      "Epoch: 313 | Batch: 008 / 011 | Total loss: 1.710 | Reg loss: 0.043 | Tree loss: 1.710 | Accuracy: 0.398000 | 0.936 sec/iter\n",
      "Epoch: 313 | Batch: 009 / 011 | Total loss: 1.682 | Reg loss: 0.043 | Tree loss: 1.682 | Accuracy: 0.436000 | 0.936 sec/iter\n",
      "Epoch: 313 | Batch: 010 / 011 | Total loss: 1.634 | Reg loss: 0.043 | Tree loss: 1.634 | Accuracy: 0.436860 | 0.936 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 314 | Batch: 000 / 011 | Total loss: 1.847 | Reg loss: 0.042 | Tree loss: 1.847 | Accuracy: 0.421000 | 0.936 sec/iter\n",
      "Epoch: 314 | Batch: 001 / 011 | Total loss: 1.846 | Reg loss: 0.042 | Tree loss: 1.846 | Accuracy: 0.382000 | 0.936 sec/iter\n",
      "Epoch: 314 | Batch: 002 / 011 | Total loss: 1.800 | Reg loss: 0.042 | Tree loss: 1.800 | Accuracy: 0.415000 | 0.936 sec/iter\n",
      "Epoch: 314 | Batch: 003 / 011 | Total loss: 1.754 | Reg loss: 0.042 | Tree loss: 1.754 | Accuracy: 0.424000 | 0.936 sec/iter\n",
      "Epoch: 314 | Batch: 004 / 011 | Total loss: 1.723 | Reg loss: 0.042 | Tree loss: 1.723 | Accuracy: 0.425500 | 0.936 sec/iter\n",
      "Epoch: 314 | Batch: 005 / 011 | Total loss: 1.749 | Reg loss: 0.043 | Tree loss: 1.749 | Accuracy: 0.397000 | 0.936 sec/iter\n",
      "Epoch: 314 | Batch: 006 / 011 | Total loss: 1.689 | Reg loss: 0.043 | Tree loss: 1.689 | Accuracy: 0.431500 | 0.936 sec/iter\n",
      "Epoch: 314 | Batch: 007 / 011 | Total loss: 1.723 | Reg loss: 0.043 | Tree loss: 1.723 | Accuracy: 0.406500 | 0.936 sec/iter\n",
      "Epoch: 314 | Batch: 008 / 011 | Total loss: 1.666 | Reg loss: 0.043 | Tree loss: 1.666 | Accuracy: 0.443000 | 0.936 sec/iter\n",
      "Epoch: 314 | Batch: 009 / 011 | Total loss: 1.669 | Reg loss: 0.043 | Tree loss: 1.669 | Accuracy: 0.437500 | 0.936 sec/iter\n",
      "Epoch: 314 | Batch: 010 / 011 | Total loss: 1.697 | Reg loss: 0.043 | Tree loss: 1.697 | Accuracy: 0.426621 | 0.935 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 315 | Batch: 000 / 011 | Total loss: 1.842 | Reg loss: 0.042 | Tree loss: 1.842 | Accuracy: 0.422000 | 0.936 sec/iter\n",
      "Epoch: 315 | Batch: 001 / 011 | Total loss: 1.836 | Reg loss: 0.042 | Tree loss: 1.836 | Accuracy: 0.413500 | 0.935 sec/iter\n",
      "Epoch: 315 | Batch: 002 / 011 | Total loss: 1.793 | Reg loss: 0.042 | Tree loss: 1.793 | Accuracy: 0.435500 | 0.935 sec/iter\n",
      "Epoch: 315 | Batch: 003 / 011 | Total loss: 1.768 | Reg loss: 0.042 | Tree loss: 1.768 | Accuracy: 0.423500 | 0.935 sec/iter\n",
      "Epoch: 315 | Batch: 004 / 011 | Total loss: 1.733 | Reg loss: 0.042 | Tree loss: 1.733 | Accuracy: 0.416000 | 0.935 sec/iter\n",
      "Epoch: 315 | Batch: 005 / 011 | Total loss: 1.719 | Reg loss: 0.043 | Tree loss: 1.719 | Accuracy: 0.415000 | 0.935 sec/iter\n",
      "Epoch: 315 | Batch: 006 / 011 | Total loss: 1.718 | Reg loss: 0.043 | Tree loss: 1.718 | Accuracy: 0.418500 | 0.935 sec/iter\n",
      "Epoch: 315 | Batch: 007 / 011 | Total loss: 1.706 | Reg loss: 0.043 | Tree loss: 1.706 | Accuracy: 0.408000 | 0.935 sec/iter\n",
      "Epoch: 315 | Batch: 008 / 011 | Total loss: 1.704 | Reg loss: 0.043 | Tree loss: 1.704 | Accuracy: 0.440500 | 0.935 sec/iter\n",
      "Epoch: 315 | Batch: 009 / 011 | Total loss: 1.690 | Reg loss: 0.043 | Tree loss: 1.690 | Accuracy: 0.425000 | 0.935 sec/iter\n",
      "Epoch: 315 | Batch: 010 / 011 | Total loss: 1.614 | Reg loss: 0.043 | Tree loss: 1.614 | Accuracy: 0.436860 | 0.935 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 316 | Batch: 000 / 011 | Total loss: 1.851 | Reg loss: 0.042 | Tree loss: 1.851 | Accuracy: 0.418000 | 0.935 sec/iter\n",
      "Epoch: 316 | Batch: 001 / 011 | Total loss: 1.808 | Reg loss: 0.042 | Tree loss: 1.808 | Accuracy: 0.431500 | 0.935 sec/iter\n",
      "Epoch: 316 | Batch: 002 / 011 | Total loss: 1.803 | Reg loss: 0.042 | Tree loss: 1.803 | Accuracy: 0.418000 | 0.935 sec/iter\n",
      "Epoch: 316 | Batch: 003 / 011 | Total loss: 1.754 | Reg loss: 0.042 | Tree loss: 1.754 | Accuracy: 0.428000 | 0.935 sec/iter\n",
      "Epoch: 316 | Batch: 004 / 011 | Total loss: 1.749 | Reg loss: 0.043 | Tree loss: 1.749 | Accuracy: 0.418000 | 0.935 sec/iter\n",
      "Epoch: 316 | Batch: 005 / 011 | Total loss: 1.727 | Reg loss: 0.043 | Tree loss: 1.727 | Accuracy: 0.421000 | 0.935 sec/iter\n",
      "Epoch: 316 | Batch: 006 / 011 | Total loss: 1.703 | Reg loss: 0.043 | Tree loss: 1.703 | Accuracy: 0.419500 | 0.935 sec/iter\n",
      "Epoch: 316 | Batch: 007 / 011 | Total loss: 1.699 | Reg loss: 0.043 | Tree loss: 1.699 | Accuracy: 0.419500 | 0.935 sec/iter\n",
      "Epoch: 316 | Batch: 008 / 011 | Total loss: 1.705 | Reg loss: 0.043 | Tree loss: 1.705 | Accuracy: 0.417000 | 0.935 sec/iter\n",
      "Epoch: 316 | Batch: 009 / 011 | Total loss: 1.707 | Reg loss: 0.043 | Tree loss: 1.707 | Accuracy: 0.413000 | 0.935 sec/iter\n",
      "Epoch: 316 | Batch: 010 / 011 | Total loss: 1.682 | Reg loss: 0.043 | Tree loss: 1.682 | Accuracy: 0.426621 | 0.935 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 317 | Batch: 000 / 011 | Total loss: 1.830 | Reg loss: 0.042 | Tree loss: 1.830 | Accuracy: 0.414000 | 0.935 sec/iter\n",
      "Epoch: 317 | Batch: 001 / 011 | Total loss: 1.823 | Reg loss: 0.042 | Tree loss: 1.823 | Accuracy: 0.416500 | 0.935 sec/iter\n",
      "Epoch: 317 | Batch: 002 / 011 | Total loss: 1.787 | Reg loss: 0.042 | Tree loss: 1.787 | Accuracy: 0.431000 | 0.935 sec/iter\n",
      "Epoch: 317 | Batch: 003 / 011 | Total loss: 1.746 | Reg loss: 0.042 | Tree loss: 1.746 | Accuracy: 0.425000 | 0.935 sec/iter\n",
      "Epoch: 317 | Batch: 004 / 011 | Total loss: 1.750 | Reg loss: 0.043 | Tree loss: 1.750 | Accuracy: 0.414000 | 0.935 sec/iter\n",
      "Epoch: 317 | Batch: 005 / 011 | Total loss: 1.756 | Reg loss: 0.043 | Tree loss: 1.756 | Accuracy: 0.412000 | 0.935 sec/iter\n",
      "Epoch: 317 | Batch: 006 / 011 | Total loss: 1.705 | Reg loss: 0.043 | Tree loss: 1.705 | Accuracy: 0.412000 | 0.935 sec/iter\n",
      "Epoch: 317 | Batch: 007 / 011 | Total loss: 1.698 | Reg loss: 0.043 | Tree loss: 1.698 | Accuracy: 0.427500 | 0.935 sec/iter\n",
      "Epoch: 317 | Batch: 008 / 011 | Total loss: 1.714 | Reg loss: 0.043 | Tree loss: 1.714 | Accuracy: 0.413000 | 0.935 sec/iter\n",
      "Epoch: 317 | Batch: 009 / 011 | Total loss: 1.688 | Reg loss: 0.043 | Tree loss: 1.688 | Accuracy: 0.420000 | 0.935 sec/iter\n",
      "Epoch: 317 | Batch: 010 / 011 | Total loss: 1.774 | Reg loss: 0.043 | Tree loss: 1.774 | Accuracy: 0.354949 | 0.935 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 318 | Batch: 000 / 011 | Total loss: 1.856 | Reg loss: 0.042 | Tree loss: 1.856 | Accuracy: 0.404000 | 0.935 sec/iter\n",
      "Epoch: 318 | Batch: 001 / 011 | Total loss: 1.837 | Reg loss: 0.042 | Tree loss: 1.837 | Accuracy: 0.416000 | 0.935 sec/iter\n",
      "Epoch: 318 | Batch: 002 / 011 | Total loss: 1.809 | Reg loss: 0.042 | Tree loss: 1.809 | Accuracy: 0.408500 | 0.935 sec/iter\n",
      "Epoch: 318 | Batch: 003 / 011 | Total loss: 1.732 | Reg loss: 0.042 | Tree loss: 1.732 | Accuracy: 0.446000 | 0.935 sec/iter\n",
      "Epoch: 318 | Batch: 004 / 011 | Total loss: 1.736 | Reg loss: 0.042 | Tree loss: 1.736 | Accuracy: 0.432500 | 0.935 sec/iter\n",
      "Epoch: 318 | Batch: 005 / 011 | Total loss: 1.722 | Reg loss: 0.043 | Tree loss: 1.722 | Accuracy: 0.432500 | 0.935 sec/iter\n",
      "Epoch: 318 | Batch: 006 / 011 | Total loss: 1.731 | Reg loss: 0.043 | Tree loss: 1.731 | Accuracy: 0.405000 | 0.935 sec/iter\n",
      "Epoch: 318 | Batch: 007 / 011 | Total loss: 1.681 | Reg loss: 0.043 | Tree loss: 1.681 | Accuracy: 0.427000 | 0.935 sec/iter\n",
      "Epoch: 318 | Batch: 008 / 011 | Total loss: 1.713 | Reg loss: 0.043 | Tree loss: 1.713 | Accuracy: 0.403500 | 0.935 sec/iter\n",
      "Epoch: 318 | Batch: 009 / 011 | Total loss: 1.701 | Reg loss: 0.043 | Tree loss: 1.701 | Accuracy: 0.416500 | 0.935 sec/iter\n",
      "Epoch: 318 | Batch: 010 / 011 | Total loss: 1.664 | Reg loss: 0.043 | Tree loss: 1.664 | Accuracy: 0.419795 | 0.935 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 319 | Batch: 000 / 011 | Total loss: 1.846 | Reg loss: 0.042 | Tree loss: 1.846 | Accuracy: 0.403500 | 0.935 sec/iter\n",
      "Epoch: 319 | Batch: 001 / 011 | Total loss: 1.840 | Reg loss: 0.042 | Tree loss: 1.840 | Accuracy: 0.411000 | 0.935 sec/iter\n",
      "Epoch: 319 | Batch: 002 / 011 | Total loss: 1.791 | Reg loss: 0.042 | Tree loss: 1.791 | Accuracy: 0.423500 | 0.935 sec/iter\n",
      "Epoch: 319 | Batch: 003 / 011 | Total loss: 1.772 | Reg loss: 0.042 | Tree loss: 1.772 | Accuracy: 0.423500 | 0.935 sec/iter\n",
      "Epoch: 319 | Batch: 004 / 011 | Total loss: 1.760 | Reg loss: 0.042 | Tree loss: 1.760 | Accuracy: 0.414000 | 0.935 sec/iter\n",
      "Epoch: 319 | Batch: 005 / 011 | Total loss: 1.700 | Reg loss: 0.043 | Tree loss: 1.700 | Accuracy: 0.437000 | 0.935 sec/iter\n",
      "Epoch: 319 | Batch: 006 / 011 | Total loss: 1.709 | Reg loss: 0.043 | Tree loss: 1.709 | Accuracy: 0.417000 | 0.935 sec/iter\n",
      "Epoch: 319 | Batch: 007 / 011 | Total loss: 1.698 | Reg loss: 0.043 | Tree loss: 1.698 | Accuracy: 0.423000 | 0.935 sec/iter\n",
      "Epoch: 319 | Batch: 008 / 011 | Total loss: 1.681 | Reg loss: 0.043 | Tree loss: 1.681 | Accuracy: 0.433000 | 0.935 sec/iter\n",
      "Epoch: 319 | Batch: 009 / 011 | Total loss: 1.693 | Reg loss: 0.043 | Tree loss: 1.693 | Accuracy: 0.412500 | 0.935 sec/iter\n",
      "Epoch: 319 | Batch: 010 / 011 | Total loss: 1.729 | Reg loss: 0.043 | Tree loss: 1.729 | Accuracy: 0.416382 | 0.935 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 320 | Batch: 000 / 011 | Total loss: 1.855 | Reg loss: 0.042 | Tree loss: 1.855 | Accuracy: 0.413500 | 0.935 sec/iter\n",
      "Epoch: 320 | Batch: 001 / 011 | Total loss: 1.841 | Reg loss: 0.042 | Tree loss: 1.841 | Accuracy: 0.401000 | 0.935 sec/iter\n",
      "Epoch: 320 | Batch: 002 / 011 | Total loss: 1.777 | Reg loss: 0.042 | Tree loss: 1.777 | Accuracy: 0.442000 | 0.935 sec/iter\n",
      "Epoch: 320 | Batch: 003 / 011 | Total loss: 1.767 | Reg loss: 0.042 | Tree loss: 1.767 | Accuracy: 0.435500 | 0.935 sec/iter\n",
      "Epoch: 320 | Batch: 004 / 011 | Total loss: 1.742 | Reg loss: 0.042 | Tree loss: 1.742 | Accuracy: 0.412000 | 0.935 sec/iter\n",
      "Epoch: 320 | Batch: 005 / 011 | Total loss: 1.712 | Reg loss: 0.043 | Tree loss: 1.712 | Accuracy: 0.420000 | 0.935 sec/iter\n",
      "Epoch: 320 | Batch: 006 / 011 | Total loss: 1.719 | Reg loss: 0.043 | Tree loss: 1.719 | Accuracy: 0.416000 | 0.935 sec/iter\n",
      "Epoch: 320 | Batch: 007 / 011 | Total loss: 1.691 | Reg loss: 0.043 | Tree loss: 1.691 | Accuracy: 0.415500 | 0.935 sec/iter\n",
      "Epoch: 320 | Batch: 008 / 011 | Total loss: 1.709 | Reg loss: 0.043 | Tree loss: 1.709 | Accuracy: 0.423000 | 0.935 sec/iter\n",
      "Epoch: 320 | Batch: 009 / 011 | Total loss: 1.681 | Reg loss: 0.043 | Tree loss: 1.681 | Accuracy: 0.415000 | 0.935 sec/iter\n",
      "Epoch: 320 | Batch: 010 / 011 | Total loss: 1.691 | Reg loss: 0.043 | Tree loss: 1.691 | Accuracy: 0.433447 | 0.935 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 321 | Batch: 000 / 011 | Total loss: 1.861 | Reg loss: 0.042 | Tree loss: 1.861 | Accuracy: 0.406500 | 0.935 sec/iter\n",
      "Epoch: 321 | Batch: 001 / 011 | Total loss: 1.823 | Reg loss: 0.042 | Tree loss: 1.823 | Accuracy: 0.396000 | 0.935 sec/iter\n",
      "Epoch: 321 | Batch: 002 / 011 | Total loss: 1.790 | Reg loss: 0.042 | Tree loss: 1.790 | Accuracy: 0.422500 | 0.935 sec/iter\n",
      "Epoch: 321 | Batch: 003 / 011 | Total loss: 1.742 | Reg loss: 0.042 | Tree loss: 1.742 | Accuracy: 0.442000 | 0.935 sec/iter\n",
      "Epoch: 321 | Batch: 004 / 011 | Total loss: 1.729 | Reg loss: 0.043 | Tree loss: 1.729 | Accuracy: 0.440500 | 0.935 sec/iter\n",
      "Epoch: 321 | Batch: 005 / 011 | Total loss: 1.779 | Reg loss: 0.043 | Tree loss: 1.779 | Accuracy: 0.390000 | 0.935 sec/iter\n",
      "Epoch: 321 | Batch: 006 / 011 | Total loss: 1.716 | Reg loss: 0.043 | Tree loss: 1.716 | Accuracy: 0.419000 | 0.935 sec/iter\n",
      "Epoch: 321 | Batch: 007 / 011 | Total loss: 1.665 | Reg loss: 0.043 | Tree loss: 1.665 | Accuracy: 0.424500 | 0.935 sec/iter\n",
      "Epoch: 321 | Batch: 008 / 011 | Total loss: 1.671 | Reg loss: 0.043 | Tree loss: 1.671 | Accuracy: 0.437500 | 0.935 sec/iter\n",
      "Epoch: 321 | Batch: 009 / 011 | Total loss: 1.708 | Reg loss: 0.043 | Tree loss: 1.708 | Accuracy: 0.425000 | 0.935 sec/iter\n",
      "Epoch: 321 | Batch: 010 / 011 | Total loss: 1.661 | Reg loss: 0.043 | Tree loss: 1.661 | Accuracy: 0.436860 | 0.934 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 322 | Batch: 000 / 011 | Total loss: 1.849 | Reg loss: 0.042 | Tree loss: 1.849 | Accuracy: 0.429000 | 0.935 sec/iter\n",
      "Epoch: 322 | Batch: 001 / 011 | Total loss: 1.823 | Reg loss: 0.042 | Tree loss: 1.823 | Accuracy: 0.414000 | 0.935 sec/iter\n",
      "Epoch: 322 | Batch: 002 / 011 | Total loss: 1.828 | Reg loss: 0.042 | Tree loss: 1.828 | Accuracy: 0.397500 | 0.934 sec/iter\n",
      "Epoch: 322 | Batch: 003 / 011 | Total loss: 1.752 | Reg loss: 0.042 | Tree loss: 1.752 | Accuracy: 0.414000 | 0.934 sec/iter\n",
      "Epoch: 322 | Batch: 004 / 011 | Total loss: 1.750 | Reg loss: 0.043 | Tree loss: 1.750 | Accuracy: 0.408500 | 0.934 sec/iter\n",
      "Epoch: 322 | Batch: 005 / 011 | Total loss: 1.718 | Reg loss: 0.043 | Tree loss: 1.718 | Accuracy: 0.432500 | 0.934 sec/iter\n",
      "Epoch: 322 | Batch: 006 / 011 | Total loss: 1.726 | Reg loss: 0.043 | Tree loss: 1.726 | Accuracy: 0.406500 | 0.934 sec/iter\n",
      "Epoch: 322 | Batch: 007 / 011 | Total loss: 1.713 | Reg loss: 0.043 | Tree loss: 1.713 | Accuracy: 0.415500 | 0.934 sec/iter\n",
      "Epoch: 322 | Batch: 008 / 011 | Total loss: 1.658 | Reg loss: 0.043 | Tree loss: 1.658 | Accuracy: 0.438000 | 0.934 sec/iter\n",
      "Epoch: 322 | Batch: 009 / 011 | Total loss: 1.677 | Reg loss: 0.043 | Tree loss: 1.677 | Accuracy: 0.445500 | 0.934 sec/iter\n",
      "Epoch: 322 | Batch: 010 / 011 | Total loss: 1.694 | Reg loss: 0.043 | Tree loss: 1.694 | Accuracy: 0.416382 | 0.934 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 323 | Batch: 000 / 011 | Total loss: 1.866 | Reg loss: 0.042 | Tree loss: 1.866 | Accuracy: 0.404000 | 0.934 sec/iter\n",
      "Epoch: 323 | Batch: 001 / 011 | Total loss: 1.830 | Reg loss: 0.042 | Tree loss: 1.830 | Accuracy: 0.413500 | 0.934 sec/iter\n",
      "Epoch: 323 | Batch: 002 / 011 | Total loss: 1.804 | Reg loss: 0.042 | Tree loss: 1.804 | Accuracy: 0.428000 | 0.934 sec/iter\n",
      "Epoch: 323 | Batch: 003 / 011 | Total loss: 1.749 | Reg loss: 0.042 | Tree loss: 1.749 | Accuracy: 0.431000 | 0.934 sec/iter\n",
      "Epoch: 323 | Batch: 004 / 011 | Total loss: 1.742 | Reg loss: 0.043 | Tree loss: 1.742 | Accuracy: 0.417000 | 0.934 sec/iter\n",
      "Epoch: 323 | Batch: 005 / 011 | Total loss: 1.702 | Reg loss: 0.043 | Tree loss: 1.702 | Accuracy: 0.425000 | 0.934 sec/iter\n",
      "Epoch: 323 | Batch: 006 / 011 | Total loss: 1.718 | Reg loss: 0.043 | Tree loss: 1.718 | Accuracy: 0.425000 | 0.934 sec/iter\n",
      "Epoch: 323 | Batch: 007 / 011 | Total loss: 1.698 | Reg loss: 0.043 | Tree loss: 1.698 | Accuracy: 0.422000 | 0.934 sec/iter\n",
      "Epoch: 323 | Batch: 008 / 011 | Total loss: 1.699 | Reg loss: 0.043 | Tree loss: 1.699 | Accuracy: 0.418500 | 0.934 sec/iter\n",
      "Epoch: 323 | Batch: 009 / 011 | Total loss: 1.664 | Reg loss: 0.043 | Tree loss: 1.664 | Accuracy: 0.435000 | 0.934 sec/iter\n",
      "Epoch: 323 | Batch: 010 / 011 | Total loss: 1.728 | Reg loss: 0.043 | Tree loss: 1.728 | Accuracy: 0.399317 | 0.934 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 324 | Batch: 000 / 011 | Total loss: 1.862 | Reg loss: 0.042 | Tree loss: 1.862 | Accuracy: 0.409500 | 0.934 sec/iter\n",
      "Epoch: 324 | Batch: 001 / 011 | Total loss: 1.847 | Reg loss: 0.042 | Tree loss: 1.847 | Accuracy: 0.410000 | 0.934 sec/iter\n",
      "Epoch: 324 | Batch: 002 / 011 | Total loss: 1.780 | Reg loss: 0.042 | Tree loss: 1.780 | Accuracy: 0.433000 | 0.934 sec/iter\n",
      "Epoch: 324 | Batch: 003 / 011 | Total loss: 1.782 | Reg loss: 0.043 | Tree loss: 1.782 | Accuracy: 0.412500 | 0.934 sec/iter\n",
      "Epoch: 324 | Batch: 004 / 011 | Total loss: 1.762 | Reg loss: 0.043 | Tree loss: 1.762 | Accuracy: 0.403000 | 0.934 sec/iter\n",
      "Epoch: 324 | Batch: 005 / 011 | Total loss: 1.738 | Reg loss: 0.043 | Tree loss: 1.738 | Accuracy: 0.420500 | 0.934 sec/iter\n",
      "Epoch: 324 | Batch: 006 / 011 | Total loss: 1.729 | Reg loss: 0.043 | Tree loss: 1.729 | Accuracy: 0.401500 | 0.934 sec/iter\n",
      "Epoch: 324 | Batch: 007 / 011 | Total loss: 1.643 | Reg loss: 0.043 | Tree loss: 1.643 | Accuracy: 0.458000 | 0.934 sec/iter\n",
      "Epoch: 324 | Batch: 008 / 011 | Total loss: 1.689 | Reg loss: 0.043 | Tree loss: 1.689 | Accuracy: 0.422000 | 0.934 sec/iter\n",
      "Epoch: 324 | Batch: 009 / 011 | Total loss: 1.689 | Reg loss: 0.043 | Tree loss: 1.689 | Accuracy: 0.418500 | 0.934 sec/iter\n",
      "Epoch: 324 | Batch: 010 / 011 | Total loss: 1.626 | Reg loss: 0.043 | Tree loss: 1.626 | Accuracy: 0.419795 | 0.934 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 325 | Batch: 000 / 011 | Total loss: 1.841 | Reg loss: 0.042 | Tree loss: 1.841 | Accuracy: 0.423000 | 0.934 sec/iter\n",
      "Epoch: 325 | Batch: 001 / 011 | Total loss: 1.832 | Reg loss: 0.042 | Tree loss: 1.832 | Accuracy: 0.404000 | 0.934 sec/iter\n",
      "Epoch: 325 | Batch: 002 / 011 | Total loss: 1.776 | Reg loss: 0.042 | Tree loss: 1.776 | Accuracy: 0.435000 | 0.934 sec/iter\n",
      "Epoch: 325 | Batch: 003 / 011 | Total loss: 1.797 | Reg loss: 0.043 | Tree loss: 1.797 | Accuracy: 0.407500 | 0.934 sec/iter\n",
      "Epoch: 325 | Batch: 004 / 011 | Total loss: 1.744 | Reg loss: 0.043 | Tree loss: 1.744 | Accuracy: 0.424000 | 0.934 sec/iter\n",
      "Epoch: 325 | Batch: 005 / 011 | Total loss: 1.719 | Reg loss: 0.043 | Tree loss: 1.719 | Accuracy: 0.430500 | 0.934 sec/iter\n",
      "Epoch: 325 | Batch: 006 / 011 | Total loss: 1.690 | Reg loss: 0.043 | Tree loss: 1.690 | Accuracy: 0.419000 | 0.934 sec/iter\n",
      "Epoch: 325 | Batch: 007 / 011 | Total loss: 1.702 | Reg loss: 0.043 | Tree loss: 1.702 | Accuracy: 0.421000 | 0.934 sec/iter\n",
      "Epoch: 325 | Batch: 008 / 011 | Total loss: 1.690 | Reg loss: 0.043 | Tree loss: 1.690 | Accuracy: 0.423000 | 0.934 sec/iter\n",
      "Epoch: 325 | Batch: 009 / 011 | Total loss: 1.683 | Reg loss: 0.043 | Tree loss: 1.683 | Accuracy: 0.416000 | 0.934 sec/iter\n",
      "Epoch: 325 | Batch: 010 / 011 | Total loss: 1.710 | Reg loss: 0.043 | Tree loss: 1.710 | Accuracy: 0.409556 | 0.934 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 326 | Batch: 000 / 011 | Total loss: 1.876 | Reg loss: 0.042 | Tree loss: 1.876 | Accuracy: 0.411000 | 0.934 sec/iter\n",
      "Epoch: 326 | Batch: 001 / 011 | Total loss: 1.819 | Reg loss: 0.042 | Tree loss: 1.819 | Accuracy: 0.406000 | 0.934 sec/iter\n",
      "Epoch: 326 | Batch: 002 / 011 | Total loss: 1.802 | Reg loss: 0.042 | Tree loss: 1.802 | Accuracy: 0.411000 | 0.934 sec/iter\n",
      "Epoch: 326 | Batch: 003 / 011 | Total loss: 1.757 | Reg loss: 0.043 | Tree loss: 1.757 | Accuracy: 0.428500 | 0.934 sec/iter\n",
      "Epoch: 326 | Batch: 004 / 011 | Total loss: 1.744 | Reg loss: 0.043 | Tree loss: 1.744 | Accuracy: 0.422500 | 0.934 sec/iter\n",
      "Epoch: 326 | Batch: 005 / 011 | Total loss: 1.731 | Reg loss: 0.043 | Tree loss: 1.731 | Accuracy: 0.425500 | 0.934 sec/iter\n",
      "Epoch: 326 | Batch: 006 / 011 | Total loss: 1.708 | Reg loss: 0.043 | Tree loss: 1.708 | Accuracy: 0.420500 | 0.934 sec/iter\n",
      "Epoch: 326 | Batch: 007 / 011 | Total loss: 1.686 | Reg loss: 0.043 | Tree loss: 1.686 | Accuracy: 0.423000 | 0.934 sec/iter\n",
      "Epoch: 326 | Batch: 008 / 011 | Total loss: 1.708 | Reg loss: 0.043 | Tree loss: 1.708 | Accuracy: 0.406500 | 0.934 sec/iter\n",
      "Epoch: 326 | Batch: 009 / 011 | Total loss: 1.665 | Reg loss: 0.043 | Tree loss: 1.665 | Accuracy: 0.435500 | 0.934 sec/iter\n",
      "Epoch: 326 | Batch: 010 / 011 | Total loss: 1.713 | Reg loss: 0.043 | Tree loss: 1.713 | Accuracy: 0.399317 | 0.934 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 327 | Batch: 000 / 011 | Total loss: 1.849 | Reg loss: 0.042 | Tree loss: 1.849 | Accuracy: 0.415000 | 0.934 sec/iter\n",
      "Epoch: 327 | Batch: 001 / 011 | Total loss: 1.824 | Reg loss: 0.042 | Tree loss: 1.824 | Accuracy: 0.428000 | 0.934 sec/iter\n",
      "Epoch: 327 | Batch: 002 / 011 | Total loss: 1.795 | Reg loss: 0.042 | Tree loss: 1.795 | Accuracy: 0.427000 | 0.934 sec/iter\n",
      "Epoch: 327 | Batch: 003 / 011 | Total loss: 1.785 | Reg loss: 0.042 | Tree loss: 1.785 | Accuracy: 0.398500 | 0.934 sec/iter\n",
      "Epoch: 327 | Batch: 004 / 011 | Total loss: 1.758 | Reg loss: 0.043 | Tree loss: 1.758 | Accuracy: 0.403500 | 0.934 sec/iter\n",
      "Epoch: 327 | Batch: 005 / 011 | Total loss: 1.705 | Reg loss: 0.043 | Tree loss: 1.705 | Accuracy: 0.418000 | 0.934 sec/iter\n",
      "Epoch: 327 | Batch: 006 / 011 | Total loss: 1.703 | Reg loss: 0.043 | Tree loss: 1.703 | Accuracy: 0.409000 | 0.934 sec/iter\n",
      "Epoch: 327 | Batch: 007 / 011 | Total loss: 1.711 | Reg loss: 0.043 | Tree loss: 1.711 | Accuracy: 0.426000 | 0.934 sec/iter\n",
      "Epoch: 327 | Batch: 008 / 011 | Total loss: 1.689 | Reg loss: 0.043 | Tree loss: 1.689 | Accuracy: 0.431000 | 0.934 sec/iter\n",
      "Epoch: 327 | Batch: 009 / 011 | Total loss: 1.695 | Reg loss: 0.043 | Tree loss: 1.695 | Accuracy: 0.430000 | 0.934 sec/iter\n",
      "Epoch: 327 | Batch: 010 / 011 | Total loss: 1.678 | Reg loss: 0.043 | Tree loss: 1.678 | Accuracy: 0.399317 | 0.934 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 328 | Batch: 000 / 011 | Total loss: 1.844 | Reg loss: 0.042 | Tree loss: 1.844 | Accuracy: 0.423000 | 0.934 sec/iter\n",
      "Epoch: 328 | Batch: 001 / 011 | Total loss: 1.844 | Reg loss: 0.042 | Tree loss: 1.844 | Accuracy: 0.395500 | 0.934 sec/iter\n",
      "Epoch: 328 | Batch: 002 / 011 | Total loss: 1.794 | Reg loss: 0.042 | Tree loss: 1.794 | Accuracy: 0.414000 | 0.934 sec/iter\n",
      "Epoch: 328 | Batch: 003 / 011 | Total loss: 1.765 | Reg loss: 0.043 | Tree loss: 1.765 | Accuracy: 0.419500 | 0.934 sec/iter\n",
      "Epoch: 328 | Batch: 004 / 011 | Total loss: 1.757 | Reg loss: 0.043 | Tree loss: 1.757 | Accuracy: 0.424000 | 0.934 sec/iter\n",
      "Epoch: 328 | Batch: 005 / 011 | Total loss: 1.717 | Reg loss: 0.043 | Tree loss: 1.717 | Accuracy: 0.427000 | 0.934 sec/iter\n",
      "Epoch: 328 | Batch: 006 / 011 | Total loss: 1.718 | Reg loss: 0.043 | Tree loss: 1.718 | Accuracy: 0.416000 | 0.934 sec/iter\n",
      "Epoch: 328 | Batch: 007 / 011 | Total loss: 1.682 | Reg loss: 0.043 | Tree loss: 1.682 | Accuracy: 0.435500 | 0.934 sec/iter\n",
      "Epoch: 328 | Batch: 008 / 011 | Total loss: 1.674 | Reg loss: 0.043 | Tree loss: 1.674 | Accuracy: 0.430000 | 0.934 sec/iter\n",
      "Epoch: 328 | Batch: 009 / 011 | Total loss: 1.688 | Reg loss: 0.043 | Tree loss: 1.688 | Accuracy: 0.423000 | 0.934 sec/iter\n",
      "Epoch: 328 | Batch: 010 / 011 | Total loss: 1.671 | Reg loss: 0.043 | Tree loss: 1.671 | Accuracy: 0.453925 | 0.934 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 329 | Batch: 000 / 011 | Total loss: 1.853 | Reg loss: 0.042 | Tree loss: 1.853 | Accuracy: 0.404500 | 0.934 sec/iter\n",
      "Epoch: 329 | Batch: 001 / 011 | Total loss: 1.840 | Reg loss: 0.042 | Tree loss: 1.840 | Accuracy: 0.407500 | 0.934 sec/iter\n",
      "Epoch: 329 | Batch: 002 / 011 | Total loss: 1.806 | Reg loss: 0.042 | Tree loss: 1.806 | Accuracy: 0.413000 | 0.934 sec/iter\n",
      "Epoch: 329 | Batch: 003 / 011 | Total loss: 1.777 | Reg loss: 0.043 | Tree loss: 1.777 | Accuracy: 0.410500 | 0.934 sec/iter\n",
      "Epoch: 329 | Batch: 004 / 011 | Total loss: 1.738 | Reg loss: 0.043 | Tree loss: 1.738 | Accuracy: 0.421500 | 0.933 sec/iter\n",
      "Epoch: 329 | Batch: 005 / 011 | Total loss: 1.710 | Reg loss: 0.043 | Tree loss: 1.710 | Accuracy: 0.428500 | 0.933 sec/iter\n",
      "Epoch: 329 | Batch: 006 / 011 | Total loss: 1.728 | Reg loss: 0.043 | Tree loss: 1.728 | Accuracy: 0.412500 | 0.933 sec/iter\n",
      "Epoch: 329 | Batch: 007 / 011 | Total loss: 1.685 | Reg loss: 0.043 | Tree loss: 1.685 | Accuracy: 0.432000 | 0.933 sec/iter\n",
      "Epoch: 329 | Batch: 008 / 011 | Total loss: 1.678 | Reg loss: 0.043 | Tree loss: 1.678 | Accuracy: 0.428500 | 0.933 sec/iter\n",
      "Epoch: 329 | Batch: 009 / 011 | Total loss: 1.677 | Reg loss: 0.043 | Tree loss: 1.677 | Accuracy: 0.429000 | 0.933 sec/iter\n",
      "Epoch: 329 | Batch: 010 / 011 | Total loss: 1.674 | Reg loss: 0.043 | Tree loss: 1.674 | Accuracy: 0.385666 | 0.933 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 330 | Batch: 000 / 011 | Total loss: 1.847 | Reg loss: 0.042 | Tree loss: 1.847 | Accuracy: 0.415500 | 0.933 sec/iter\n",
      "Epoch: 330 | Batch: 001 / 011 | Total loss: 1.819 | Reg loss: 0.042 | Tree loss: 1.819 | Accuracy: 0.404000 | 0.933 sec/iter\n",
      "Epoch: 330 | Batch: 002 / 011 | Total loss: 1.798 | Reg loss: 0.042 | Tree loss: 1.798 | Accuracy: 0.416500 | 0.933 sec/iter\n",
      "Epoch: 330 | Batch: 003 / 011 | Total loss: 1.746 | Reg loss: 0.043 | Tree loss: 1.746 | Accuracy: 0.444500 | 0.933 sec/iter\n",
      "Epoch: 330 | Batch: 004 / 011 | Total loss: 1.754 | Reg loss: 0.043 | Tree loss: 1.754 | Accuracy: 0.411500 | 0.933 sec/iter\n",
      "Epoch: 330 | Batch: 005 / 011 | Total loss: 1.717 | Reg loss: 0.043 | Tree loss: 1.717 | Accuracy: 0.421000 | 0.933 sec/iter\n",
      "Epoch: 330 | Batch: 006 / 011 | Total loss: 1.737 | Reg loss: 0.043 | Tree loss: 1.737 | Accuracy: 0.407500 | 0.933 sec/iter\n",
      "Epoch: 330 | Batch: 007 / 011 | Total loss: 1.710 | Reg loss: 0.043 | Tree loss: 1.710 | Accuracy: 0.413500 | 0.933 sec/iter\n",
      "Epoch: 330 | Batch: 008 / 011 | Total loss: 1.687 | Reg loss: 0.043 | Tree loss: 1.687 | Accuracy: 0.410000 | 0.933 sec/iter\n",
      "Epoch: 330 | Batch: 009 / 011 | Total loss: 1.679 | Reg loss: 0.043 | Tree loss: 1.679 | Accuracy: 0.417500 | 0.933 sec/iter\n",
      "Epoch: 330 | Batch: 010 / 011 | Total loss: 1.775 | Reg loss: 0.043 | Tree loss: 1.775 | Accuracy: 0.433447 | 0.933 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 331 | Batch: 000 / 011 | Total loss: 1.880 | Reg loss: 0.042 | Tree loss: 1.880 | Accuracy: 0.402000 | 0.933 sec/iter\n",
      "Epoch: 331 | Batch: 001 / 011 | Total loss: 1.832 | Reg loss: 0.042 | Tree loss: 1.832 | Accuracy: 0.410500 | 0.933 sec/iter\n",
      "Epoch: 331 | Batch: 002 / 011 | Total loss: 1.792 | Reg loss: 0.042 | Tree loss: 1.792 | Accuracy: 0.423000 | 0.933 sec/iter\n",
      "Epoch: 331 | Batch: 003 / 011 | Total loss: 1.763 | Reg loss: 0.043 | Tree loss: 1.763 | Accuracy: 0.427000 | 0.933 sec/iter\n",
      "Epoch: 331 | Batch: 004 / 011 | Total loss: 1.748 | Reg loss: 0.043 | Tree loss: 1.748 | Accuracy: 0.425500 | 0.933 sec/iter\n",
      "Epoch: 331 | Batch: 005 / 011 | Total loss: 1.739 | Reg loss: 0.043 | Tree loss: 1.739 | Accuracy: 0.407000 | 0.933 sec/iter\n",
      "Epoch: 331 | Batch: 006 / 011 | Total loss: 1.705 | Reg loss: 0.043 | Tree loss: 1.705 | Accuracy: 0.416000 | 0.933 sec/iter\n",
      "Epoch: 331 | Batch: 007 / 011 | Total loss: 1.682 | Reg loss: 0.043 | Tree loss: 1.682 | Accuracy: 0.433000 | 0.933 sec/iter\n",
      "Epoch: 331 | Batch: 008 / 011 | Total loss: 1.671 | Reg loss: 0.043 | Tree loss: 1.671 | Accuracy: 0.440500 | 0.933 sec/iter\n",
      "Epoch: 331 | Batch: 009 / 011 | Total loss: 1.690 | Reg loss: 0.043 | Tree loss: 1.690 | Accuracy: 0.421500 | 0.933 sec/iter\n",
      "Epoch: 331 | Batch: 010 / 011 | Total loss: 1.637 | Reg loss: 0.043 | Tree loss: 1.637 | Accuracy: 0.433447 | 0.933 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 332 | Batch: 000 / 011 | Total loss: 1.859 | Reg loss: 0.042 | Tree loss: 1.859 | Accuracy: 0.418500 | 0.933 sec/iter\n",
      "Epoch: 332 | Batch: 001 / 011 | Total loss: 1.847 | Reg loss: 0.042 | Tree loss: 1.847 | Accuracy: 0.408500 | 0.933 sec/iter\n",
      "Epoch: 332 | Batch: 002 / 011 | Total loss: 1.789 | Reg loss: 0.043 | Tree loss: 1.789 | Accuracy: 0.436500 | 0.933 sec/iter\n",
      "Epoch: 332 | Batch: 003 / 011 | Total loss: 1.790 | Reg loss: 0.043 | Tree loss: 1.790 | Accuracy: 0.427000 | 0.933 sec/iter\n",
      "Epoch: 332 | Batch: 004 / 011 | Total loss: 1.725 | Reg loss: 0.043 | Tree loss: 1.725 | Accuracy: 0.432500 | 0.933 sec/iter\n",
      "Epoch: 332 | Batch: 005 / 011 | Total loss: 1.704 | Reg loss: 0.043 | Tree loss: 1.704 | Accuracy: 0.432500 | 0.933 sec/iter\n",
      "Epoch: 332 | Batch: 006 / 011 | Total loss: 1.709 | Reg loss: 0.043 | Tree loss: 1.709 | Accuracy: 0.404000 | 0.933 sec/iter\n",
      "Epoch: 332 | Batch: 007 / 011 | Total loss: 1.675 | Reg loss: 0.043 | Tree loss: 1.675 | Accuracy: 0.429000 | 0.933 sec/iter\n",
      "Epoch: 332 | Batch: 008 / 011 | Total loss: 1.705 | Reg loss: 0.043 | Tree loss: 1.705 | Accuracy: 0.412500 | 0.933 sec/iter\n",
      "Epoch: 332 | Batch: 009 / 011 | Total loss: 1.693 | Reg loss: 0.043 | Tree loss: 1.693 | Accuracy: 0.414000 | 0.933 sec/iter\n",
      "Epoch: 332 | Batch: 010 / 011 | Total loss: 1.632 | Reg loss: 0.043 | Tree loss: 1.632 | Accuracy: 0.436860 | 0.933 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 333 | Batch: 000 / 011 | Total loss: 1.852 | Reg loss: 0.042 | Tree loss: 1.852 | Accuracy: 0.420500 | 0.933 sec/iter\n",
      "Epoch: 333 | Batch: 001 / 011 | Total loss: 1.809 | Reg loss: 0.042 | Tree loss: 1.809 | Accuracy: 0.419500 | 0.933 sec/iter\n",
      "Epoch: 333 | Batch: 002 / 011 | Total loss: 1.799 | Reg loss: 0.043 | Tree loss: 1.799 | Accuracy: 0.406000 | 0.933 sec/iter\n",
      "Epoch: 333 | Batch: 003 / 011 | Total loss: 1.782 | Reg loss: 0.043 | Tree loss: 1.782 | Accuracy: 0.401500 | 0.933 sec/iter\n",
      "Epoch: 333 | Batch: 004 / 011 | Total loss: 1.730 | Reg loss: 0.043 | Tree loss: 1.730 | Accuracy: 0.426500 | 0.933 sec/iter\n",
      "Epoch: 333 | Batch: 005 / 011 | Total loss: 1.709 | Reg loss: 0.043 | Tree loss: 1.709 | Accuracy: 0.426000 | 0.933 sec/iter\n",
      "Epoch: 333 | Batch: 006 / 011 | Total loss: 1.731 | Reg loss: 0.043 | Tree loss: 1.731 | Accuracy: 0.406500 | 0.933 sec/iter\n",
      "Epoch: 333 | Batch: 007 / 011 | Total loss: 1.699 | Reg loss: 0.043 | Tree loss: 1.699 | Accuracy: 0.428000 | 0.933 sec/iter\n",
      "Epoch: 333 | Batch: 008 / 011 | Total loss: 1.678 | Reg loss: 0.043 | Tree loss: 1.678 | Accuracy: 0.434000 | 0.933 sec/iter\n",
      "Epoch: 333 | Batch: 009 / 011 | Total loss: 1.698 | Reg loss: 0.043 | Tree loss: 1.698 | Accuracy: 0.427500 | 0.933 sec/iter\n",
      "Epoch: 333 | Batch: 010 / 011 | Total loss: 1.738 | Reg loss: 0.043 | Tree loss: 1.738 | Accuracy: 0.419795 | 0.933 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 334 | Batch: 000 / 011 | Total loss: 1.836 | Reg loss: 0.042 | Tree loss: 1.836 | Accuracy: 0.415500 | 0.933 sec/iter\n",
      "Epoch: 334 | Batch: 001 / 011 | Total loss: 1.854 | Reg loss: 0.042 | Tree loss: 1.854 | Accuracy: 0.396000 | 0.933 sec/iter\n",
      "Epoch: 334 | Batch: 002 / 011 | Total loss: 1.780 | Reg loss: 0.042 | Tree loss: 1.780 | Accuracy: 0.431500 | 0.933 sec/iter\n",
      "Epoch: 334 | Batch: 003 / 011 | Total loss: 1.771 | Reg loss: 0.043 | Tree loss: 1.771 | Accuracy: 0.424000 | 0.933 sec/iter\n",
      "Epoch: 334 | Batch: 004 / 011 | Total loss: 1.758 | Reg loss: 0.043 | Tree loss: 1.758 | Accuracy: 0.406000 | 0.933 sec/iter\n",
      "Epoch: 334 | Batch: 005 / 011 | Total loss: 1.709 | Reg loss: 0.043 | Tree loss: 1.709 | Accuracy: 0.431500 | 0.933 sec/iter\n",
      "Epoch: 334 | Batch: 006 / 011 | Total loss: 1.715 | Reg loss: 0.043 | Tree loss: 1.715 | Accuracy: 0.429000 | 0.933 sec/iter\n",
      "Epoch: 334 | Batch: 007 / 011 | Total loss: 1.684 | Reg loss: 0.043 | Tree loss: 1.684 | Accuracy: 0.430000 | 0.933 sec/iter\n",
      "Epoch: 334 | Batch: 008 / 011 | Total loss: 1.725 | Reg loss: 0.043 | Tree loss: 1.725 | Accuracy: 0.414500 | 0.933 sec/iter\n",
      "Epoch: 334 | Batch: 009 / 011 | Total loss: 1.678 | Reg loss: 0.043 | Tree loss: 1.678 | Accuracy: 0.425000 | 0.933 sec/iter\n",
      "Epoch: 334 | Batch: 010 / 011 | Total loss: 1.627 | Reg loss: 0.043 | Tree loss: 1.627 | Accuracy: 0.457338 | 0.933 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 335 | Batch: 000 / 011 | Total loss: 1.857 | Reg loss: 0.042 | Tree loss: 1.857 | Accuracy: 0.410500 | 0.933 sec/iter\n",
      "Epoch: 335 | Batch: 001 / 011 | Total loss: 1.832 | Reg loss: 0.042 | Tree loss: 1.832 | Accuracy: 0.404000 | 0.933 sec/iter\n",
      "Epoch: 335 | Batch: 002 / 011 | Total loss: 1.799 | Reg loss: 0.042 | Tree loss: 1.799 | Accuracy: 0.423500 | 0.933 sec/iter\n",
      "Epoch: 335 | Batch: 003 / 011 | Total loss: 1.727 | Reg loss: 0.043 | Tree loss: 1.727 | Accuracy: 0.438000 | 0.933 sec/iter\n",
      "Epoch: 335 | Batch: 004 / 011 | Total loss: 1.742 | Reg loss: 0.043 | Tree loss: 1.742 | Accuracy: 0.438500 | 0.933 sec/iter\n",
      "Epoch: 335 | Batch: 005 / 011 | Total loss: 1.718 | Reg loss: 0.043 | Tree loss: 1.718 | Accuracy: 0.419500 | 0.933 sec/iter\n",
      "Epoch: 335 | Batch: 006 / 011 | Total loss: 1.749 | Reg loss: 0.043 | Tree loss: 1.749 | Accuracy: 0.387500 | 0.933 sec/iter\n",
      "Epoch: 335 | Batch: 007 / 011 | Total loss: 1.690 | Reg loss: 0.043 | Tree loss: 1.690 | Accuracy: 0.420000 | 0.933 sec/iter\n",
      "Epoch: 335 | Batch: 008 / 011 | Total loss: 1.696 | Reg loss: 0.043 | Tree loss: 1.696 | Accuracy: 0.416500 | 0.933 sec/iter\n",
      "Epoch: 335 | Batch: 009 / 011 | Total loss: 1.680 | Reg loss: 0.043 | Tree loss: 1.680 | Accuracy: 0.436500 | 0.933 sec/iter\n",
      "Epoch: 335 | Batch: 010 / 011 | Total loss: 1.720 | Reg loss: 0.043 | Tree loss: 1.720 | Accuracy: 0.382253 | 0.933 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 336 | Batch: 000 / 011 | Total loss: 1.869 | Reg loss: 0.042 | Tree loss: 1.869 | Accuracy: 0.393500 | 0.933 sec/iter\n",
      "Epoch: 336 | Batch: 001 / 011 | Total loss: 1.863 | Reg loss: 0.042 | Tree loss: 1.863 | Accuracy: 0.406500 | 0.933 sec/iter\n",
      "Epoch: 336 | Batch: 002 / 011 | Total loss: 1.789 | Reg loss: 0.042 | Tree loss: 1.789 | Accuracy: 0.406000 | 0.933 sec/iter\n",
      "Epoch: 336 | Batch: 003 / 011 | Total loss: 1.803 | Reg loss: 0.043 | Tree loss: 1.803 | Accuracy: 0.414500 | 0.933 sec/iter\n",
      "Epoch: 336 | Batch: 004 / 011 | Total loss: 1.756 | Reg loss: 0.043 | Tree loss: 1.756 | Accuracy: 0.428500 | 0.933 sec/iter\n",
      "Epoch: 336 | Batch: 005 / 011 | Total loss: 1.686 | Reg loss: 0.043 | Tree loss: 1.686 | Accuracy: 0.438500 | 0.933 sec/iter\n",
      "Epoch: 336 | Batch: 006 / 011 | Total loss: 1.686 | Reg loss: 0.043 | Tree loss: 1.686 | Accuracy: 0.427500 | 0.933 sec/iter\n",
      "Epoch: 336 | Batch: 007 / 011 | Total loss: 1.662 | Reg loss: 0.043 | Tree loss: 1.662 | Accuracy: 0.426500 | 0.933 sec/iter\n",
      "Epoch: 336 | Batch: 008 / 011 | Total loss: 1.684 | Reg loss: 0.043 | Tree loss: 1.684 | Accuracy: 0.421000 | 0.932 sec/iter\n",
      "Epoch: 336 | Batch: 009 / 011 | Total loss: 1.700 | Reg loss: 0.043 | Tree loss: 1.700 | Accuracy: 0.423000 | 0.932 sec/iter\n",
      "Epoch: 336 | Batch: 010 / 011 | Total loss: 1.776 | Reg loss: 0.043 | Tree loss: 1.776 | Accuracy: 0.406143 | 0.932 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 337 | Batch: 000 / 011 | Total loss: 1.867 | Reg loss: 0.042 | Tree loss: 1.867 | Accuracy: 0.393000 | 0.932 sec/iter\n",
      "Epoch: 337 | Batch: 001 / 011 | Total loss: 1.832 | Reg loss: 0.042 | Tree loss: 1.832 | Accuracy: 0.426500 | 0.932 sec/iter\n",
      "Epoch: 337 | Batch: 002 / 011 | Total loss: 1.803 | Reg loss: 0.042 | Tree loss: 1.803 | Accuracy: 0.412500 | 0.932 sec/iter\n",
      "Epoch: 337 | Batch: 003 / 011 | Total loss: 1.792 | Reg loss: 0.043 | Tree loss: 1.792 | Accuracy: 0.414000 | 0.932 sec/iter\n",
      "Epoch: 337 | Batch: 004 / 011 | Total loss: 1.735 | Reg loss: 0.043 | Tree loss: 1.735 | Accuracy: 0.440000 | 0.932 sec/iter\n",
      "Epoch: 337 | Batch: 005 / 011 | Total loss: 1.725 | Reg loss: 0.043 | Tree loss: 1.725 | Accuracy: 0.416000 | 0.932 sec/iter\n",
      "Epoch: 337 | Batch: 006 / 011 | Total loss: 1.690 | Reg loss: 0.043 | Tree loss: 1.690 | Accuracy: 0.429000 | 0.932 sec/iter\n",
      "Epoch: 337 | Batch: 007 / 011 | Total loss: 1.680 | Reg loss: 0.043 | Tree loss: 1.680 | Accuracy: 0.429500 | 0.932 sec/iter\n",
      "Epoch: 337 | Batch: 008 / 011 | Total loss: 1.681 | Reg loss: 0.043 | Tree loss: 1.681 | Accuracy: 0.433000 | 0.932 sec/iter\n",
      "Epoch: 337 | Batch: 009 / 011 | Total loss: 1.688 | Reg loss: 0.043 | Tree loss: 1.688 | Accuracy: 0.422500 | 0.932 sec/iter\n",
      "Epoch: 337 | Batch: 010 / 011 | Total loss: 1.687 | Reg loss: 0.043 | Tree loss: 1.687 | Accuracy: 0.392491 | 0.932 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 338 | Batch: 000 / 011 | Total loss: 1.867 | Reg loss: 0.042 | Tree loss: 1.867 | Accuracy: 0.391500 | 0.932 sec/iter\n",
      "Epoch: 338 | Batch: 001 / 011 | Total loss: 1.823 | Reg loss: 0.042 | Tree loss: 1.823 | Accuracy: 0.395500 | 0.932 sec/iter\n",
      "Epoch: 338 | Batch: 002 / 011 | Total loss: 1.793 | Reg loss: 0.042 | Tree loss: 1.793 | Accuracy: 0.424500 | 0.932 sec/iter\n",
      "Epoch: 338 | Batch: 003 / 011 | Total loss: 1.782 | Reg loss: 0.043 | Tree loss: 1.782 | Accuracy: 0.423500 | 0.932 sec/iter\n",
      "Epoch: 338 | Batch: 004 / 011 | Total loss: 1.751 | Reg loss: 0.043 | Tree loss: 1.751 | Accuracy: 0.422500 | 0.932 sec/iter\n",
      "Epoch: 338 | Batch: 005 / 011 | Total loss: 1.729 | Reg loss: 0.043 | Tree loss: 1.729 | Accuracy: 0.428500 | 0.932 sec/iter\n",
      "Epoch: 338 | Batch: 006 / 011 | Total loss: 1.717 | Reg loss: 0.043 | Tree loss: 1.717 | Accuracy: 0.417000 | 0.932 sec/iter\n",
      "Epoch: 338 | Batch: 007 / 011 | Total loss: 1.694 | Reg loss: 0.043 | Tree loss: 1.694 | Accuracy: 0.428000 | 0.932 sec/iter\n",
      "Epoch: 338 | Batch: 008 / 011 | Total loss: 1.684 | Reg loss: 0.043 | Tree loss: 1.684 | Accuracy: 0.421500 | 0.932 sec/iter\n",
      "Epoch: 338 | Batch: 009 / 011 | Total loss: 1.662 | Reg loss: 0.043 | Tree loss: 1.662 | Accuracy: 0.431500 | 0.932 sec/iter\n",
      "Epoch: 338 | Batch: 010 / 011 | Total loss: 1.661 | Reg loss: 0.043 | Tree loss: 1.661 | Accuracy: 0.457338 | 0.932 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 339 | Batch: 000 / 011 | Total loss: 1.847 | Reg loss: 0.042 | Tree loss: 1.847 | Accuracy: 0.411000 | 0.932 sec/iter\n",
      "Epoch: 339 | Batch: 001 / 011 | Total loss: 1.821 | Reg loss: 0.042 | Tree loss: 1.821 | Accuracy: 0.411500 | 0.932 sec/iter\n",
      "Epoch: 339 | Batch: 002 / 011 | Total loss: 1.798 | Reg loss: 0.043 | Tree loss: 1.798 | Accuracy: 0.421500 | 0.932 sec/iter\n",
      "Epoch: 339 | Batch: 003 / 011 | Total loss: 1.785 | Reg loss: 0.043 | Tree loss: 1.785 | Accuracy: 0.412500 | 0.932 sec/iter\n",
      "Epoch: 339 | Batch: 004 / 011 | Total loss: 1.739 | Reg loss: 0.043 | Tree loss: 1.739 | Accuracy: 0.432500 | 0.932 sec/iter\n",
      "Epoch: 339 | Batch: 005 / 011 | Total loss: 1.710 | Reg loss: 0.043 | Tree loss: 1.710 | Accuracy: 0.428500 | 0.932 sec/iter\n",
      "Epoch: 339 | Batch: 006 / 011 | Total loss: 1.722 | Reg loss: 0.043 | Tree loss: 1.722 | Accuracy: 0.406500 | 0.932 sec/iter\n",
      "Epoch: 339 | Batch: 007 / 011 | Total loss: 1.674 | Reg loss: 0.043 | Tree loss: 1.674 | Accuracy: 0.430500 | 0.932 sec/iter\n",
      "Epoch: 339 | Batch: 008 / 011 | Total loss: 1.684 | Reg loss: 0.043 | Tree loss: 1.684 | Accuracy: 0.428000 | 0.932 sec/iter\n",
      "Epoch: 339 | Batch: 009 / 011 | Total loss: 1.711 | Reg loss: 0.043 | Tree loss: 1.711 | Accuracy: 0.398500 | 0.932 sec/iter\n",
      "Epoch: 339 | Batch: 010 / 011 | Total loss: 1.690 | Reg loss: 0.043 | Tree loss: 1.690 | Accuracy: 0.443686 | 0.932 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 340 | Batch: 000 / 011 | Total loss: 1.845 | Reg loss: 0.042 | Tree loss: 1.845 | Accuracy: 0.415500 | 0.932 sec/iter\n",
      "Epoch: 340 | Batch: 001 / 011 | Total loss: 1.836 | Reg loss: 0.042 | Tree loss: 1.836 | Accuracy: 0.409000 | 0.932 sec/iter\n",
      "Epoch: 340 | Batch: 002 / 011 | Total loss: 1.815 | Reg loss: 0.043 | Tree loss: 1.815 | Accuracy: 0.401000 | 0.932 sec/iter\n",
      "Epoch: 340 | Batch: 003 / 011 | Total loss: 1.753 | Reg loss: 0.043 | Tree loss: 1.753 | Accuracy: 0.419000 | 0.932 sec/iter\n",
      "Epoch: 340 | Batch: 004 / 011 | Total loss: 1.751 | Reg loss: 0.043 | Tree loss: 1.751 | Accuracy: 0.423000 | 0.932 sec/iter\n",
      "Epoch: 340 | Batch: 005 / 011 | Total loss: 1.742 | Reg loss: 0.043 | Tree loss: 1.742 | Accuracy: 0.415000 | 0.932 sec/iter\n",
      "Epoch: 340 | Batch: 006 / 011 | Total loss: 1.675 | Reg loss: 0.043 | Tree loss: 1.675 | Accuracy: 0.429500 | 0.932 sec/iter\n",
      "Epoch: 340 | Batch: 007 / 011 | Total loss: 1.676 | Reg loss: 0.043 | Tree loss: 1.676 | Accuracy: 0.435500 | 0.932 sec/iter\n",
      "Epoch: 340 | Batch: 008 / 011 | Total loss: 1.712 | Reg loss: 0.043 | Tree loss: 1.712 | Accuracy: 0.415500 | 0.932 sec/iter\n",
      "Epoch: 340 | Batch: 009 / 011 | Total loss: 1.703 | Reg loss: 0.043 | Tree loss: 1.703 | Accuracy: 0.414500 | 0.932 sec/iter\n",
      "Epoch: 340 | Batch: 010 / 011 | Total loss: 1.640 | Reg loss: 0.043 | Tree loss: 1.640 | Accuracy: 0.470990 | 0.932 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 341 | Batch: 000 / 011 | Total loss: 1.865 | Reg loss: 0.042 | Tree loss: 1.865 | Accuracy: 0.401500 | 0.932 sec/iter\n",
      "Epoch: 341 | Batch: 001 / 011 | Total loss: 1.829 | Reg loss: 0.042 | Tree loss: 1.829 | Accuracy: 0.420000 | 0.932 sec/iter\n",
      "Epoch: 341 | Batch: 002 / 011 | Total loss: 1.792 | Reg loss: 0.043 | Tree loss: 1.792 | Accuracy: 0.425500 | 0.932 sec/iter\n",
      "Epoch: 341 | Batch: 003 / 011 | Total loss: 1.771 | Reg loss: 0.043 | Tree loss: 1.771 | Accuracy: 0.412500 | 0.932 sec/iter\n",
      "Epoch: 341 | Batch: 004 / 011 | Total loss: 1.720 | Reg loss: 0.043 | Tree loss: 1.720 | Accuracy: 0.433000 | 0.932 sec/iter\n",
      "Epoch: 341 | Batch: 005 / 011 | Total loss: 1.722 | Reg loss: 0.043 | Tree loss: 1.722 | Accuracy: 0.401000 | 0.932 sec/iter\n",
      "Epoch: 341 | Batch: 006 / 011 | Total loss: 1.716 | Reg loss: 0.043 | Tree loss: 1.716 | Accuracy: 0.419000 | 0.932 sec/iter\n",
      "Epoch: 341 | Batch: 007 / 011 | Total loss: 1.705 | Reg loss: 0.043 | Tree loss: 1.705 | Accuracy: 0.420000 | 0.932 sec/iter\n",
      "Epoch: 341 | Batch: 008 / 011 | Total loss: 1.668 | Reg loss: 0.043 | Tree loss: 1.668 | Accuracy: 0.435500 | 0.932 sec/iter\n",
      "Epoch: 341 | Batch: 009 / 011 | Total loss: 1.707 | Reg loss: 0.043 | Tree loss: 1.707 | Accuracy: 0.406500 | 0.932 sec/iter\n",
      "Epoch: 341 | Batch: 010 / 011 | Total loss: 1.759 | Reg loss: 0.043 | Tree loss: 1.759 | Accuracy: 0.399317 | 0.932 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 342 | Batch: 000 / 011 | Total loss: 1.864 | Reg loss: 0.042 | Tree loss: 1.864 | Accuracy: 0.392000 | 0.932 sec/iter\n",
      "Epoch: 342 | Batch: 001 / 011 | Total loss: 1.826 | Reg loss: 0.042 | Tree loss: 1.826 | Accuracy: 0.429000 | 0.932 sec/iter\n",
      "Epoch: 342 | Batch: 002 / 011 | Total loss: 1.789 | Reg loss: 0.043 | Tree loss: 1.789 | Accuracy: 0.434000 | 0.932 sec/iter\n",
      "Epoch: 342 | Batch: 003 / 011 | Total loss: 1.774 | Reg loss: 0.043 | Tree loss: 1.774 | Accuracy: 0.425000 | 0.932 sec/iter\n",
      "Epoch: 342 | Batch: 004 / 011 | Total loss: 1.738 | Reg loss: 0.043 | Tree loss: 1.738 | Accuracy: 0.434500 | 0.932 sec/iter\n",
      "Epoch: 342 | Batch: 005 / 011 | Total loss: 1.731 | Reg loss: 0.043 | Tree loss: 1.731 | Accuracy: 0.413500 | 0.932 sec/iter\n",
      "Epoch: 342 | Batch: 006 / 011 | Total loss: 1.732 | Reg loss: 0.043 | Tree loss: 1.732 | Accuracy: 0.413500 | 0.932 sec/iter\n",
      "Epoch: 342 | Batch: 007 / 011 | Total loss: 1.702 | Reg loss: 0.043 | Tree loss: 1.702 | Accuracy: 0.416500 | 0.932 sec/iter\n",
      "Epoch: 342 | Batch: 008 / 011 | Total loss: 1.684 | Reg loss: 0.043 | Tree loss: 1.684 | Accuracy: 0.423000 | 0.932 sec/iter\n",
      "Epoch: 342 | Batch: 009 / 011 | Total loss: 1.647 | Reg loss: 0.043 | Tree loss: 1.647 | Accuracy: 0.450500 | 0.932 sec/iter\n",
      "Epoch: 342 | Batch: 010 / 011 | Total loss: 1.711 | Reg loss: 0.043 | Tree loss: 1.711 | Accuracy: 0.399317 | 0.932 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 343 | Batch: 000 / 011 | Total loss: 1.863 | Reg loss: 0.042 | Tree loss: 1.863 | Accuracy: 0.407000 | 0.932 sec/iter\n",
      "Epoch: 343 | Batch: 001 / 011 | Total loss: 1.842 | Reg loss: 0.042 | Tree loss: 1.842 | Accuracy: 0.410000 | 0.932 sec/iter\n",
      "Epoch: 343 | Batch: 002 / 011 | Total loss: 1.767 | Reg loss: 0.043 | Tree loss: 1.767 | Accuracy: 0.430500 | 0.932 sec/iter\n",
      "Epoch: 343 | Batch: 003 / 011 | Total loss: 1.764 | Reg loss: 0.043 | Tree loss: 1.764 | Accuracy: 0.425500 | 0.932 sec/iter\n",
      "Epoch: 343 | Batch: 004 / 011 | Total loss: 1.748 | Reg loss: 0.043 | Tree loss: 1.748 | Accuracy: 0.416000 | 0.932 sec/iter\n",
      "Epoch: 343 | Batch: 005 / 011 | Total loss: 1.750 | Reg loss: 0.043 | Tree loss: 1.750 | Accuracy: 0.408500 | 0.932 sec/iter\n",
      "Epoch: 343 | Batch: 006 / 011 | Total loss: 1.676 | Reg loss: 0.043 | Tree loss: 1.676 | Accuracy: 0.429000 | 0.932 sec/iter\n",
      "Epoch: 343 | Batch: 007 / 011 | Total loss: 1.704 | Reg loss: 0.043 | Tree loss: 1.704 | Accuracy: 0.413000 | 0.932 sec/iter\n",
      "Epoch: 343 | Batch: 008 / 011 | Total loss: 1.672 | Reg loss: 0.043 | Tree loss: 1.672 | Accuracy: 0.428000 | 0.932 sec/iter\n",
      "Epoch: 343 | Batch: 009 / 011 | Total loss: 1.709 | Reg loss: 0.043 | Tree loss: 1.709 | Accuracy: 0.407500 | 0.932 sec/iter\n",
      "Epoch: 343 | Batch: 010 / 011 | Total loss: 1.658 | Reg loss: 0.043 | Tree loss: 1.658 | Accuracy: 0.440273 | 0.932 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 344 | Batch: 000 / 011 | Total loss: 1.866 | Reg loss: 0.042 | Tree loss: 1.866 | Accuracy: 0.400500 | 0.932 sec/iter\n",
      "Epoch: 344 | Batch: 001 / 011 | Total loss: 1.836 | Reg loss: 0.042 | Tree loss: 1.836 | Accuracy: 0.411000 | 0.932 sec/iter\n",
      "Epoch: 344 | Batch: 002 / 011 | Total loss: 1.762 | Reg loss: 0.043 | Tree loss: 1.762 | Accuracy: 0.433500 | 0.932 sec/iter\n",
      "Epoch: 344 | Batch: 003 / 011 | Total loss: 1.764 | Reg loss: 0.043 | Tree loss: 1.764 | Accuracy: 0.420000 | 0.932 sec/iter\n",
      "Epoch: 344 | Batch: 004 / 011 | Total loss: 1.719 | Reg loss: 0.043 | Tree loss: 1.719 | Accuracy: 0.427000 | 0.932 sec/iter\n",
      "Epoch: 344 | Batch: 005 / 011 | Total loss: 1.714 | Reg loss: 0.043 | Tree loss: 1.714 | Accuracy: 0.420000 | 0.931 sec/iter\n",
      "Epoch: 344 | Batch: 006 / 011 | Total loss: 1.732 | Reg loss: 0.043 | Tree loss: 1.732 | Accuracy: 0.429500 | 0.931 sec/iter\n",
      "Epoch: 344 | Batch: 007 / 011 | Total loss: 1.695 | Reg loss: 0.043 | Tree loss: 1.695 | Accuracy: 0.413500 | 0.931 sec/iter\n",
      "Epoch: 344 | Batch: 008 / 011 | Total loss: 1.685 | Reg loss: 0.043 | Tree loss: 1.685 | Accuracy: 0.431000 | 0.931 sec/iter\n",
      "Epoch: 344 | Batch: 009 / 011 | Total loss: 1.698 | Reg loss: 0.043 | Tree loss: 1.698 | Accuracy: 0.413000 | 0.931 sec/iter\n",
      "Epoch: 344 | Batch: 010 / 011 | Total loss: 1.669 | Reg loss: 0.043 | Tree loss: 1.669 | Accuracy: 0.426621 | 0.931 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 345 | Batch: 000 / 011 | Total loss: 1.856 | Reg loss: 0.042 | Tree loss: 1.856 | Accuracy: 0.387500 | 0.931 sec/iter\n",
      "Epoch: 345 | Batch: 001 / 011 | Total loss: 1.841 | Reg loss: 0.043 | Tree loss: 1.841 | Accuracy: 0.407500 | 0.931 sec/iter\n",
      "Epoch: 345 | Batch: 002 / 011 | Total loss: 1.783 | Reg loss: 0.043 | Tree loss: 1.783 | Accuracy: 0.439000 | 0.931 sec/iter\n",
      "Epoch: 345 | Batch: 003 / 011 | Total loss: 1.789 | Reg loss: 0.043 | Tree loss: 1.789 | Accuracy: 0.400000 | 0.931 sec/iter\n",
      "Epoch: 345 | Batch: 004 / 011 | Total loss: 1.745 | Reg loss: 0.043 | Tree loss: 1.745 | Accuracy: 0.420500 | 0.931 sec/iter\n",
      "Epoch: 345 | Batch: 005 / 011 | Total loss: 1.703 | Reg loss: 0.043 | Tree loss: 1.703 | Accuracy: 0.432500 | 0.931 sec/iter\n",
      "Epoch: 345 | Batch: 006 / 011 | Total loss: 1.696 | Reg loss: 0.043 | Tree loss: 1.696 | Accuracy: 0.425500 | 0.931 sec/iter\n",
      "Epoch: 345 | Batch: 007 / 011 | Total loss: 1.691 | Reg loss: 0.043 | Tree loss: 1.691 | Accuracy: 0.421500 | 0.931 sec/iter\n",
      "Epoch: 345 | Batch: 008 / 011 | Total loss: 1.687 | Reg loss: 0.043 | Tree loss: 1.687 | Accuracy: 0.423500 | 0.931 sec/iter\n",
      "Epoch: 345 | Batch: 009 / 011 | Total loss: 1.707 | Reg loss: 0.043 | Tree loss: 1.707 | Accuracy: 0.421000 | 0.931 sec/iter\n",
      "Epoch: 345 | Batch: 010 / 011 | Total loss: 1.685 | Reg loss: 0.043 | Tree loss: 1.685 | Accuracy: 0.419795 | 0.931 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 346 | Batch: 000 / 011 | Total loss: 1.856 | Reg loss: 0.043 | Tree loss: 1.856 | Accuracy: 0.417500 | 0.931 sec/iter\n",
      "Epoch: 346 | Batch: 001 / 011 | Total loss: 1.835 | Reg loss: 0.043 | Tree loss: 1.835 | Accuracy: 0.395500 | 0.931 sec/iter\n",
      "Epoch: 346 | Batch: 002 / 011 | Total loss: 1.788 | Reg loss: 0.043 | Tree loss: 1.788 | Accuracy: 0.432500 | 0.931 sec/iter\n",
      "Epoch: 346 | Batch: 003 / 011 | Total loss: 1.780 | Reg loss: 0.043 | Tree loss: 1.780 | Accuracy: 0.405000 | 0.931 sec/iter\n",
      "Epoch: 346 | Batch: 004 / 011 | Total loss: 1.740 | Reg loss: 0.043 | Tree loss: 1.740 | Accuracy: 0.429000 | 0.931 sec/iter\n",
      "Epoch: 346 | Batch: 005 / 011 | Total loss: 1.735 | Reg loss: 0.043 | Tree loss: 1.735 | Accuracy: 0.418500 | 0.931 sec/iter\n",
      "Epoch: 346 | Batch: 006 / 011 | Total loss: 1.686 | Reg loss: 0.043 | Tree loss: 1.686 | Accuracy: 0.438500 | 0.931 sec/iter\n",
      "Epoch: 346 | Batch: 007 / 011 | Total loss: 1.709 | Reg loss: 0.043 | Tree loss: 1.709 | Accuracy: 0.411000 | 0.931 sec/iter\n",
      "Epoch: 346 | Batch: 008 / 011 | Total loss: 1.669 | Reg loss: 0.043 | Tree loss: 1.669 | Accuracy: 0.443500 | 0.931 sec/iter\n",
      "Epoch: 346 | Batch: 009 / 011 | Total loss: 1.671 | Reg loss: 0.043 | Tree loss: 1.671 | Accuracy: 0.429000 | 0.931 sec/iter\n",
      "Epoch: 346 | Batch: 010 / 011 | Total loss: 1.739 | Reg loss: 0.043 | Tree loss: 1.739 | Accuracy: 0.402730 | 0.931 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 347 | Batch: 000 / 011 | Total loss: 1.855 | Reg loss: 0.043 | Tree loss: 1.855 | Accuracy: 0.417000 | 0.931 sec/iter\n",
      "Epoch: 347 | Batch: 001 / 011 | Total loss: 1.843 | Reg loss: 0.043 | Tree loss: 1.843 | Accuracy: 0.407000 | 0.931 sec/iter\n",
      "Epoch: 347 | Batch: 002 / 011 | Total loss: 1.806 | Reg loss: 0.043 | Tree loss: 1.806 | Accuracy: 0.409500 | 0.931 sec/iter\n",
      "Epoch: 347 | Batch: 003 / 011 | Total loss: 1.768 | Reg loss: 0.043 | Tree loss: 1.768 | Accuracy: 0.416000 | 0.931 sec/iter\n",
      "Epoch: 347 | Batch: 004 / 011 | Total loss: 1.733 | Reg loss: 0.043 | Tree loss: 1.733 | Accuracy: 0.410500 | 0.931 sec/iter\n",
      "Epoch: 347 | Batch: 005 / 011 | Total loss: 1.712 | Reg loss: 0.043 | Tree loss: 1.712 | Accuracy: 0.426000 | 0.931 sec/iter\n",
      "Epoch: 347 | Batch: 006 / 011 | Total loss: 1.689 | Reg loss: 0.043 | Tree loss: 1.689 | Accuracy: 0.427500 | 0.931 sec/iter\n",
      "Epoch: 347 | Batch: 007 / 011 | Total loss: 1.682 | Reg loss: 0.043 | Tree loss: 1.682 | Accuracy: 0.432500 | 0.931 sec/iter\n",
      "Epoch: 347 | Batch: 008 / 011 | Total loss: 1.704 | Reg loss: 0.043 | Tree loss: 1.704 | Accuracy: 0.422000 | 0.931 sec/iter\n",
      "Epoch: 347 | Batch: 009 / 011 | Total loss: 1.707 | Reg loss: 0.043 | Tree loss: 1.707 | Accuracy: 0.415500 | 0.931 sec/iter\n",
      "Epoch: 347 | Batch: 010 / 011 | Total loss: 1.717 | Reg loss: 0.043 | Tree loss: 1.717 | Accuracy: 0.389078 | 0.931 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 348 | Batch: 000 / 011 | Total loss: 1.854 | Reg loss: 0.042 | Tree loss: 1.854 | Accuracy: 0.402000 | 0.931 sec/iter\n",
      "Epoch: 348 | Batch: 001 / 011 | Total loss: 1.805 | Reg loss: 0.043 | Tree loss: 1.805 | Accuracy: 0.430500 | 0.931 sec/iter\n",
      "Epoch: 348 | Batch: 002 / 011 | Total loss: 1.789 | Reg loss: 0.043 | Tree loss: 1.789 | Accuracy: 0.421500 | 0.931 sec/iter\n",
      "Epoch: 348 | Batch: 003 / 011 | Total loss: 1.772 | Reg loss: 0.043 | Tree loss: 1.772 | Accuracy: 0.430500 | 0.931 sec/iter\n",
      "Epoch: 348 | Batch: 004 / 011 | Total loss: 1.758 | Reg loss: 0.043 | Tree loss: 1.758 | Accuracy: 0.428500 | 0.931 sec/iter\n",
      "Epoch: 348 | Batch: 005 / 011 | Total loss: 1.728 | Reg loss: 0.043 | Tree loss: 1.728 | Accuracy: 0.416500 | 0.931 sec/iter\n",
      "Epoch: 348 | Batch: 006 / 011 | Total loss: 1.699 | Reg loss: 0.043 | Tree loss: 1.699 | Accuracy: 0.418500 | 0.931 sec/iter\n",
      "Epoch: 348 | Batch: 007 / 011 | Total loss: 1.720 | Reg loss: 0.043 | Tree loss: 1.720 | Accuracy: 0.418500 | 0.931 sec/iter\n",
      "Epoch: 348 | Batch: 008 / 011 | Total loss: 1.692 | Reg loss: 0.043 | Tree loss: 1.692 | Accuracy: 0.416000 | 0.931 sec/iter\n",
      "Epoch: 348 | Batch: 009 / 011 | Total loss: 1.704 | Reg loss: 0.043 | Tree loss: 1.704 | Accuracy: 0.413500 | 0.931 sec/iter\n",
      "Epoch: 348 | Batch: 010 / 011 | Total loss: 1.571 | Reg loss: 0.043 | Tree loss: 1.571 | Accuracy: 0.470990 | 0.931 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 349 | Batch: 000 / 011 | Total loss: 1.866 | Reg loss: 0.043 | Tree loss: 1.866 | Accuracy: 0.411500 | 0.931 sec/iter\n",
      "Epoch: 349 | Batch: 001 / 011 | Total loss: 1.838 | Reg loss: 0.043 | Tree loss: 1.838 | Accuracy: 0.408000 | 0.931 sec/iter\n",
      "Epoch: 349 | Batch: 002 / 011 | Total loss: 1.794 | Reg loss: 0.043 | Tree loss: 1.794 | Accuracy: 0.422500 | 0.931 sec/iter\n",
      "Epoch: 349 | Batch: 003 / 011 | Total loss: 1.771 | Reg loss: 0.043 | Tree loss: 1.771 | Accuracy: 0.404000 | 0.931 sec/iter\n",
      "Epoch: 349 | Batch: 004 / 011 | Total loss: 1.750 | Reg loss: 0.043 | Tree loss: 1.750 | Accuracy: 0.426000 | 0.931 sec/iter\n",
      "Epoch: 349 | Batch: 005 / 011 | Total loss: 1.703 | Reg loss: 0.043 | Tree loss: 1.703 | Accuracy: 0.430500 | 0.931 sec/iter\n",
      "Epoch: 349 | Batch: 006 / 011 | Total loss: 1.685 | Reg loss: 0.043 | Tree loss: 1.685 | Accuracy: 0.441500 | 0.931 sec/iter\n",
      "Epoch: 349 | Batch: 007 / 011 | Total loss: 1.704 | Reg loss: 0.043 | Tree loss: 1.704 | Accuracy: 0.412500 | 0.931 sec/iter\n",
      "Epoch: 349 | Batch: 008 / 011 | Total loss: 1.697 | Reg loss: 0.043 | Tree loss: 1.697 | Accuracy: 0.409500 | 0.931 sec/iter\n",
      "Epoch: 349 | Batch: 009 / 011 | Total loss: 1.673 | Reg loss: 0.043 | Tree loss: 1.673 | Accuracy: 0.435000 | 0.931 sec/iter\n",
      "Epoch: 349 | Batch: 010 / 011 | Total loss: 1.755 | Reg loss: 0.043 | Tree loss: 1.755 | Accuracy: 0.389078 | 0.931 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 350 | Batch: 000 / 011 | Total loss: 1.874 | Reg loss: 0.043 | Tree loss: 1.874 | Accuracy: 0.395500 | 0.931 sec/iter\n",
      "Epoch: 350 | Batch: 001 / 011 | Total loss: 1.837 | Reg loss: 0.043 | Tree loss: 1.837 | Accuracy: 0.401000 | 0.931 sec/iter\n",
      "Epoch: 350 | Batch: 002 / 011 | Total loss: 1.787 | Reg loss: 0.043 | Tree loss: 1.787 | Accuracy: 0.439000 | 0.931 sec/iter\n",
      "Epoch: 350 | Batch: 003 / 011 | Total loss: 1.783 | Reg loss: 0.043 | Tree loss: 1.783 | Accuracy: 0.428000 | 0.931 sec/iter\n",
      "Epoch: 350 | Batch: 004 / 011 | Total loss: 1.744 | Reg loss: 0.043 | Tree loss: 1.744 | Accuracy: 0.410000 | 0.931 sec/iter\n",
      "Epoch: 350 | Batch: 005 / 011 | Total loss: 1.710 | Reg loss: 0.043 | Tree loss: 1.710 | Accuracy: 0.441500 | 0.931 sec/iter\n",
      "Epoch: 350 | Batch: 006 / 011 | Total loss: 1.724 | Reg loss: 0.043 | Tree loss: 1.724 | Accuracy: 0.405000 | 0.931 sec/iter\n",
      "Epoch: 350 | Batch: 007 / 011 | Total loss: 1.686 | Reg loss: 0.043 | Tree loss: 1.686 | Accuracy: 0.429500 | 0.931 sec/iter\n",
      "Epoch: 350 | Batch: 008 / 011 | Total loss: 1.680 | Reg loss: 0.043 | Tree loss: 1.680 | Accuracy: 0.421000 | 0.931 sec/iter\n",
      "Epoch: 350 | Batch: 009 / 011 | Total loss: 1.694 | Reg loss: 0.043 | Tree loss: 1.694 | Accuracy: 0.410000 | 0.931 sec/iter\n",
      "Epoch: 350 | Batch: 010 / 011 | Total loss: 1.635 | Reg loss: 0.043 | Tree loss: 1.635 | Accuracy: 0.450512 | 0.931 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 351 | Batch: 000 / 011 | Total loss: 1.843 | Reg loss: 0.043 | Tree loss: 1.843 | Accuracy: 0.415500 | 0.931 sec/iter\n",
      "Epoch: 351 | Batch: 001 / 011 | Total loss: 1.826 | Reg loss: 0.043 | Tree loss: 1.826 | Accuracy: 0.413000 | 0.931 sec/iter\n",
      "Epoch: 351 | Batch: 002 / 011 | Total loss: 1.790 | Reg loss: 0.043 | Tree loss: 1.790 | Accuracy: 0.428500 | 0.931 sec/iter\n",
      "Epoch: 351 | Batch: 003 / 011 | Total loss: 1.804 | Reg loss: 0.043 | Tree loss: 1.804 | Accuracy: 0.398000 | 0.931 sec/iter\n",
      "Epoch: 351 | Batch: 004 / 011 | Total loss: 1.730 | Reg loss: 0.043 | Tree loss: 1.730 | Accuracy: 0.431500 | 0.931 sec/iter\n",
      "Epoch: 351 | Batch: 005 / 011 | Total loss: 1.707 | Reg loss: 0.043 | Tree loss: 1.707 | Accuracy: 0.435000 | 0.931 sec/iter\n",
      "Epoch: 351 | Batch: 006 / 011 | Total loss: 1.703 | Reg loss: 0.043 | Tree loss: 1.703 | Accuracy: 0.428000 | 0.931 sec/iter\n",
      "Epoch: 351 | Batch: 007 / 011 | Total loss: 1.694 | Reg loss: 0.043 | Tree loss: 1.694 | Accuracy: 0.411000 | 0.931 sec/iter\n",
      "Epoch: 351 | Batch: 008 / 011 | Total loss: 1.699 | Reg loss: 0.043 | Tree loss: 1.699 | Accuracy: 0.411500 | 0.931 sec/iter\n",
      "Epoch: 351 | Batch: 009 / 011 | Total loss: 1.705 | Reg loss: 0.043 | Tree loss: 1.705 | Accuracy: 0.403500 | 0.931 sec/iter\n",
      "Epoch: 351 | Batch: 010 / 011 | Total loss: 1.649 | Reg loss: 0.043 | Tree loss: 1.649 | Accuracy: 0.436860 | 0.931 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 352 | Batch: 000 / 011 | Total loss: 1.866 | Reg loss: 0.043 | Tree loss: 1.866 | Accuracy: 0.408000 | 0.931 sec/iter\n",
      "Epoch: 352 | Batch: 001 / 011 | Total loss: 1.834 | Reg loss: 0.043 | Tree loss: 1.834 | Accuracy: 0.413000 | 0.931 sec/iter\n",
      "Epoch: 352 | Batch: 002 / 011 | Total loss: 1.837 | Reg loss: 0.043 | Tree loss: 1.837 | Accuracy: 0.410500 | 0.931 sec/iter\n",
      "Epoch: 352 | Batch: 003 / 011 | Total loss: 1.769 | Reg loss: 0.043 | Tree loss: 1.769 | Accuracy: 0.397500 | 0.931 sec/iter\n",
      "Epoch: 352 | Batch: 004 / 011 | Total loss: 1.756 | Reg loss: 0.043 | Tree loss: 1.756 | Accuracy: 0.416000 | 0.931 sec/iter\n",
      "Epoch: 352 | Batch: 005 / 011 | Total loss: 1.705 | Reg loss: 0.043 | Tree loss: 1.705 | Accuracy: 0.424000 | 0.931 sec/iter\n",
      "Epoch: 352 | Batch: 006 / 011 | Total loss: 1.703 | Reg loss: 0.043 | Tree loss: 1.703 | Accuracy: 0.418000 | 0.931 sec/iter\n",
      "Epoch: 352 | Batch: 007 / 011 | Total loss: 1.715 | Reg loss: 0.043 | Tree loss: 1.715 | Accuracy: 0.417000 | 0.93 sec/iter\n",
      "Epoch: 352 | Batch: 008 / 011 | Total loss: 1.672 | Reg loss: 0.043 | Tree loss: 1.672 | Accuracy: 0.433500 | 0.93 sec/iter\n",
      "Epoch: 352 | Batch: 009 / 011 | Total loss: 1.664 | Reg loss: 0.043 | Tree loss: 1.664 | Accuracy: 0.438500 | 0.93 sec/iter\n",
      "Epoch: 352 | Batch: 010 / 011 | Total loss: 1.697 | Reg loss: 0.043 | Tree loss: 1.697 | Accuracy: 0.423208 | 0.93 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 353 | Batch: 000 / 011 | Total loss: 1.858 | Reg loss: 0.043 | Tree loss: 1.858 | Accuracy: 0.398000 | 0.93 sec/iter\n",
      "Epoch: 353 | Batch: 001 / 011 | Total loss: 1.848 | Reg loss: 0.043 | Tree loss: 1.848 | Accuracy: 0.393000 | 0.93 sec/iter\n",
      "Epoch: 353 | Batch: 002 / 011 | Total loss: 1.795 | Reg loss: 0.043 | Tree loss: 1.795 | Accuracy: 0.438000 | 0.93 sec/iter\n",
      "Epoch: 353 | Batch: 003 / 011 | Total loss: 1.746 | Reg loss: 0.043 | Tree loss: 1.746 | Accuracy: 0.431000 | 0.93 sec/iter\n",
      "Epoch: 353 | Batch: 004 / 011 | Total loss: 1.729 | Reg loss: 0.043 | Tree loss: 1.729 | Accuracy: 0.427500 | 0.93 sec/iter\n",
      "Epoch: 353 | Batch: 005 / 011 | Total loss: 1.722 | Reg loss: 0.043 | Tree loss: 1.722 | Accuracy: 0.437500 | 0.93 sec/iter\n",
      "Epoch: 353 | Batch: 006 / 011 | Total loss: 1.716 | Reg loss: 0.043 | Tree loss: 1.716 | Accuracy: 0.409000 | 0.93 sec/iter\n",
      "Epoch: 353 | Batch: 007 / 011 | Total loss: 1.715 | Reg loss: 0.043 | Tree loss: 1.715 | Accuracy: 0.409500 | 0.93 sec/iter\n",
      "Epoch: 353 | Batch: 008 / 011 | Total loss: 1.687 | Reg loss: 0.043 | Tree loss: 1.687 | Accuracy: 0.424500 | 0.93 sec/iter\n",
      "Epoch: 353 | Batch: 009 / 011 | Total loss: 1.688 | Reg loss: 0.043 | Tree loss: 1.688 | Accuracy: 0.418500 | 0.93 sec/iter\n",
      "Epoch: 353 | Batch: 010 / 011 | Total loss: 1.660 | Reg loss: 0.043 | Tree loss: 1.660 | Accuracy: 0.416382 | 0.93 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 354 | Batch: 000 / 011 | Total loss: 1.857 | Reg loss: 0.043 | Tree loss: 1.857 | Accuracy: 0.396000 | 0.93 sec/iter\n",
      "Epoch: 354 | Batch: 001 / 011 | Total loss: 1.817 | Reg loss: 0.043 | Tree loss: 1.817 | Accuracy: 0.422500 | 0.93 sec/iter\n",
      "Epoch: 354 | Batch: 002 / 011 | Total loss: 1.795 | Reg loss: 0.043 | Tree loss: 1.795 | Accuracy: 0.428000 | 0.93 sec/iter\n",
      "Epoch: 354 | Batch: 003 / 011 | Total loss: 1.772 | Reg loss: 0.043 | Tree loss: 1.772 | Accuracy: 0.429500 | 0.93 sec/iter\n",
      "Epoch: 354 | Batch: 004 / 011 | Total loss: 1.742 | Reg loss: 0.043 | Tree loss: 1.742 | Accuracy: 0.427500 | 0.93 sec/iter\n",
      "Epoch: 354 | Batch: 005 / 011 | Total loss: 1.739 | Reg loss: 0.043 | Tree loss: 1.739 | Accuracy: 0.411000 | 0.93 sec/iter\n",
      "Epoch: 354 | Batch: 006 / 011 | Total loss: 1.695 | Reg loss: 0.043 | Tree loss: 1.695 | Accuracy: 0.428000 | 0.93 sec/iter\n",
      "Epoch: 354 | Batch: 007 / 011 | Total loss: 1.705 | Reg loss: 0.043 | Tree loss: 1.705 | Accuracy: 0.400500 | 0.93 sec/iter\n",
      "Epoch: 354 | Batch: 008 / 011 | Total loss: 1.688 | Reg loss: 0.043 | Tree loss: 1.688 | Accuracy: 0.428500 | 0.93 sec/iter\n",
      "Epoch: 354 | Batch: 009 / 011 | Total loss: 1.704 | Reg loss: 0.043 | Tree loss: 1.704 | Accuracy: 0.409500 | 0.93 sec/iter\n",
      "Epoch: 354 | Batch: 010 / 011 | Total loss: 1.643 | Reg loss: 0.043 | Tree loss: 1.643 | Accuracy: 0.412969 | 0.93 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 355 | Batch: 000 / 011 | Total loss: 1.856 | Reg loss: 0.043 | Tree loss: 1.856 | Accuracy: 0.406500 | 0.93 sec/iter\n",
      "Epoch: 355 | Batch: 001 / 011 | Total loss: 1.814 | Reg loss: 0.043 | Tree loss: 1.814 | Accuracy: 0.421500 | 0.93 sec/iter\n",
      "Epoch: 355 | Batch: 002 / 011 | Total loss: 1.834 | Reg loss: 0.043 | Tree loss: 1.834 | Accuracy: 0.409000 | 0.93 sec/iter\n",
      "Epoch: 355 | Batch: 003 / 011 | Total loss: 1.749 | Reg loss: 0.043 | Tree loss: 1.749 | Accuracy: 0.429500 | 0.93 sec/iter\n",
      "Epoch: 355 | Batch: 004 / 011 | Total loss: 1.756 | Reg loss: 0.043 | Tree loss: 1.756 | Accuracy: 0.415000 | 0.93 sec/iter\n",
      "Epoch: 355 | Batch: 005 / 011 | Total loss: 1.721 | Reg loss: 0.043 | Tree loss: 1.721 | Accuracy: 0.418500 | 0.93 sec/iter\n",
      "Epoch: 355 | Batch: 006 / 011 | Total loss: 1.704 | Reg loss: 0.043 | Tree loss: 1.704 | Accuracy: 0.430500 | 0.93 sec/iter\n",
      "Epoch: 355 | Batch: 007 / 011 | Total loss: 1.696 | Reg loss: 0.043 | Tree loss: 1.696 | Accuracy: 0.407000 | 0.93 sec/iter\n",
      "Epoch: 355 | Batch: 008 / 011 | Total loss: 1.654 | Reg loss: 0.043 | Tree loss: 1.654 | Accuracy: 0.444500 | 0.93 sec/iter\n",
      "Epoch: 355 | Batch: 009 / 011 | Total loss: 1.700 | Reg loss: 0.043 | Tree loss: 1.700 | Accuracy: 0.419500 | 0.93 sec/iter\n",
      "Epoch: 355 | Batch: 010 / 011 | Total loss: 1.667 | Reg loss: 0.043 | Tree loss: 1.667 | Accuracy: 0.436860 | 0.93 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 356 | Batch: 000 / 011 | Total loss: 1.848 | Reg loss: 0.043 | Tree loss: 1.848 | Accuracy: 0.407500 | 0.93 sec/iter\n",
      "Epoch: 356 | Batch: 001 / 011 | Total loss: 1.846 | Reg loss: 0.043 | Tree loss: 1.846 | Accuracy: 0.408500 | 0.93 sec/iter\n",
      "Epoch: 356 | Batch: 002 / 011 | Total loss: 1.804 | Reg loss: 0.043 | Tree loss: 1.804 | Accuracy: 0.419000 | 0.93 sec/iter\n",
      "Epoch: 356 | Batch: 003 / 011 | Total loss: 1.772 | Reg loss: 0.043 | Tree loss: 1.772 | Accuracy: 0.434500 | 0.93 sec/iter\n",
      "Epoch: 356 | Batch: 004 / 011 | Total loss: 1.742 | Reg loss: 0.043 | Tree loss: 1.742 | Accuracy: 0.430000 | 0.93 sec/iter\n",
      "Epoch: 356 | Batch: 005 / 011 | Total loss: 1.723 | Reg loss: 0.043 | Tree loss: 1.723 | Accuracy: 0.416500 | 0.93 sec/iter\n",
      "Epoch: 356 | Batch: 006 / 011 | Total loss: 1.696 | Reg loss: 0.043 | Tree loss: 1.696 | Accuracy: 0.418500 | 0.93 sec/iter\n",
      "Epoch: 356 | Batch: 007 / 011 | Total loss: 1.689 | Reg loss: 0.043 | Tree loss: 1.689 | Accuracy: 0.417500 | 0.93 sec/iter\n",
      "Epoch: 356 | Batch: 008 / 011 | Total loss: 1.677 | Reg loss: 0.043 | Tree loss: 1.677 | Accuracy: 0.415500 | 0.93 sec/iter\n",
      "Epoch: 356 | Batch: 009 / 011 | Total loss: 1.702 | Reg loss: 0.043 | Tree loss: 1.702 | Accuracy: 0.422500 | 0.93 sec/iter\n",
      "Epoch: 356 | Batch: 010 / 011 | Total loss: 1.624 | Reg loss: 0.043 | Tree loss: 1.624 | Accuracy: 0.433447 | 0.93 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 357 | Batch: 000 / 011 | Total loss: 1.870 | Reg loss: 0.043 | Tree loss: 1.870 | Accuracy: 0.387500 | 0.93 sec/iter\n",
      "Epoch: 357 | Batch: 001 / 011 | Total loss: 1.805 | Reg loss: 0.043 | Tree loss: 1.805 | Accuracy: 0.431500 | 0.93 sec/iter\n",
      "Epoch: 357 | Batch: 002 / 011 | Total loss: 1.785 | Reg loss: 0.043 | Tree loss: 1.785 | Accuracy: 0.421500 | 0.93 sec/iter\n",
      "Epoch: 357 | Batch: 003 / 011 | Total loss: 1.800 | Reg loss: 0.043 | Tree loss: 1.800 | Accuracy: 0.397500 | 0.93 sec/iter\n",
      "Epoch: 357 | Batch: 004 / 011 | Total loss: 1.733 | Reg loss: 0.043 | Tree loss: 1.733 | Accuracy: 0.424500 | 0.93 sec/iter\n",
      "Epoch: 357 | Batch: 005 / 011 | Total loss: 1.704 | Reg loss: 0.043 | Tree loss: 1.704 | Accuracy: 0.425000 | 0.93 sec/iter\n",
      "Epoch: 357 | Batch: 006 / 011 | Total loss: 1.714 | Reg loss: 0.043 | Tree loss: 1.714 | Accuracy: 0.419000 | 0.93 sec/iter\n",
      "Epoch: 357 | Batch: 007 / 011 | Total loss: 1.683 | Reg loss: 0.043 | Tree loss: 1.683 | Accuracy: 0.414500 | 0.93 sec/iter\n",
      "Epoch: 357 | Batch: 008 / 011 | Total loss: 1.700 | Reg loss: 0.043 | Tree loss: 1.700 | Accuracy: 0.429000 | 0.93 sec/iter\n",
      "Epoch: 357 | Batch: 009 / 011 | Total loss: 1.693 | Reg loss: 0.043 | Tree loss: 1.693 | Accuracy: 0.416500 | 0.93 sec/iter\n",
      "Epoch: 357 | Batch: 010 / 011 | Total loss: 1.673 | Reg loss: 0.043 | Tree loss: 1.673 | Accuracy: 0.470990 | 0.93 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 358 | Batch: 000 / 011 | Total loss: 1.851 | Reg loss: 0.043 | Tree loss: 1.851 | Accuracy: 0.402500 | 0.93 sec/iter\n",
      "Epoch: 358 | Batch: 001 / 011 | Total loss: 1.802 | Reg loss: 0.043 | Tree loss: 1.802 | Accuracy: 0.419500 | 0.93 sec/iter\n",
      "Epoch: 358 | Batch: 002 / 011 | Total loss: 1.796 | Reg loss: 0.043 | Tree loss: 1.796 | Accuracy: 0.434500 | 0.93 sec/iter\n",
      "Epoch: 358 | Batch: 003 / 011 | Total loss: 1.786 | Reg loss: 0.043 | Tree loss: 1.786 | Accuracy: 0.408000 | 0.93 sec/iter\n",
      "Epoch: 358 | Batch: 004 / 011 | Total loss: 1.732 | Reg loss: 0.043 | Tree loss: 1.732 | Accuracy: 0.445500 | 0.93 sec/iter\n",
      "Epoch: 358 | Batch: 005 / 011 | Total loss: 1.695 | Reg loss: 0.043 | Tree loss: 1.695 | Accuracy: 0.426000 | 0.93 sec/iter\n",
      "Epoch: 358 | Batch: 006 / 011 | Total loss: 1.737 | Reg loss: 0.043 | Tree loss: 1.737 | Accuracy: 0.396000 | 0.93 sec/iter\n",
      "Epoch: 358 | Batch: 007 / 011 | Total loss: 1.701 | Reg loss: 0.043 | Tree loss: 1.701 | Accuracy: 0.433000 | 0.93 sec/iter\n",
      "Epoch: 358 | Batch: 008 / 011 | Total loss: 1.686 | Reg loss: 0.043 | Tree loss: 1.686 | Accuracy: 0.420000 | 0.93 sec/iter\n",
      "Epoch: 358 | Batch: 009 / 011 | Total loss: 1.707 | Reg loss: 0.043 | Tree loss: 1.707 | Accuracy: 0.420000 | 0.93 sec/iter\n",
      "Epoch: 358 | Batch: 010 / 011 | Total loss: 1.733 | Reg loss: 0.043 | Tree loss: 1.733 | Accuracy: 0.375427 | 0.93 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 359 | Batch: 000 / 011 | Total loss: 1.847 | Reg loss: 0.043 | Tree loss: 1.847 | Accuracy: 0.402500 | 0.93 sec/iter\n",
      "Epoch: 359 | Batch: 001 / 011 | Total loss: 1.823 | Reg loss: 0.043 | Tree loss: 1.823 | Accuracy: 0.434500 | 0.93 sec/iter\n",
      "Epoch: 359 | Batch: 002 / 011 | Total loss: 1.818 | Reg loss: 0.043 | Tree loss: 1.818 | Accuracy: 0.416500 | 0.93 sec/iter\n",
      "Epoch: 359 | Batch: 003 / 011 | Total loss: 1.790 | Reg loss: 0.043 | Tree loss: 1.790 | Accuracy: 0.417500 | 0.93 sec/iter\n",
      "Epoch: 359 | Batch: 004 / 011 | Total loss: 1.733 | Reg loss: 0.043 | Tree loss: 1.733 | Accuracy: 0.441500 | 0.93 sec/iter\n",
      "Epoch: 359 | Batch: 005 / 011 | Total loss: 1.738 | Reg loss: 0.043 | Tree loss: 1.738 | Accuracy: 0.417000 | 0.93 sec/iter\n",
      "Epoch: 359 | Batch: 006 / 011 | Total loss: 1.705 | Reg loss: 0.043 | Tree loss: 1.705 | Accuracy: 0.418500 | 0.93 sec/iter\n",
      "Epoch: 359 | Batch: 007 / 011 | Total loss: 1.684 | Reg loss: 0.043 | Tree loss: 1.684 | Accuracy: 0.433000 | 0.93 sec/iter\n",
      "Epoch: 359 | Batch: 008 / 011 | Total loss: 1.696 | Reg loss: 0.043 | Tree loss: 1.696 | Accuracy: 0.420000 | 0.93 sec/iter\n",
      "Epoch: 359 | Batch: 009 / 011 | Total loss: 1.678 | Reg loss: 0.043 | Tree loss: 1.678 | Accuracy: 0.412500 | 0.93 sec/iter\n",
      "Epoch: 359 | Batch: 010 / 011 | Total loss: 1.689 | Reg loss: 0.043 | Tree loss: 1.689 | Accuracy: 0.447099 | 0.93 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 360 | Batch: 000 / 011 | Total loss: 1.843 | Reg loss: 0.043 | Tree loss: 1.843 | Accuracy: 0.408500 | 0.93 sec/iter\n",
      "Epoch: 360 | Batch: 001 / 011 | Total loss: 1.836 | Reg loss: 0.043 | Tree loss: 1.836 | Accuracy: 0.409500 | 0.93 sec/iter\n",
      "Epoch: 360 | Batch: 002 / 011 | Total loss: 1.800 | Reg loss: 0.043 | Tree loss: 1.800 | Accuracy: 0.415000 | 0.93 sec/iter\n",
      "Epoch: 360 | Batch: 003 / 011 | Total loss: 1.797 | Reg loss: 0.043 | Tree loss: 1.797 | Accuracy: 0.417500 | 0.93 sec/iter\n",
      "Epoch: 360 | Batch: 004 / 011 | Total loss: 1.748 | Reg loss: 0.043 | Tree loss: 1.748 | Accuracy: 0.428000 | 0.93 sec/iter\n",
      "Epoch: 360 | Batch: 005 / 011 | Total loss: 1.719 | Reg loss: 0.043 | Tree loss: 1.719 | Accuracy: 0.415500 | 0.93 sec/iter\n",
      "Epoch: 360 | Batch: 006 / 011 | Total loss: 1.709 | Reg loss: 0.043 | Tree loss: 1.709 | Accuracy: 0.413000 | 0.93 sec/iter\n",
      "Epoch: 360 | Batch: 007 / 011 | Total loss: 1.686 | Reg loss: 0.043 | Tree loss: 1.686 | Accuracy: 0.423500 | 0.93 sec/iter\n",
      "Epoch: 360 | Batch: 008 / 011 | Total loss: 1.702 | Reg loss: 0.043 | Tree loss: 1.702 | Accuracy: 0.408500 | 0.93 sec/iter\n",
      "Epoch: 360 | Batch: 009 / 011 | Total loss: 1.671 | Reg loss: 0.043 | Tree loss: 1.671 | Accuracy: 0.437500 | 0.93 sec/iter\n",
      "Epoch: 360 | Batch: 010 / 011 | Total loss: 1.694 | Reg loss: 0.043 | Tree loss: 1.694 | Accuracy: 0.443686 | 0.93 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 361 | Batch: 000 / 011 | Total loss: 1.869 | Reg loss: 0.043 | Tree loss: 1.869 | Accuracy: 0.405000 | 0.93 sec/iter\n",
      "Epoch: 361 | Batch: 001 / 011 | Total loss: 1.813 | Reg loss: 0.043 | Tree loss: 1.813 | Accuracy: 0.406000 | 0.93 sec/iter\n",
      "Epoch: 361 | Batch: 002 / 011 | Total loss: 1.776 | Reg loss: 0.043 | Tree loss: 1.776 | Accuracy: 0.424500 | 0.93 sec/iter\n",
      "Epoch: 361 | Batch: 003 / 011 | Total loss: 1.772 | Reg loss: 0.043 | Tree loss: 1.772 | Accuracy: 0.437000 | 0.93 sec/iter\n",
      "Epoch: 361 | Batch: 004 / 011 | Total loss: 1.734 | Reg loss: 0.043 | Tree loss: 1.734 | Accuracy: 0.432000 | 0.93 sec/iter\n",
      "Epoch: 361 | Batch: 005 / 011 | Total loss: 1.739 | Reg loss: 0.043 | Tree loss: 1.739 | Accuracy: 0.403500 | 0.93 sec/iter\n",
      "Epoch: 361 | Batch: 006 / 011 | Total loss: 1.727 | Reg loss: 0.043 | Tree loss: 1.727 | Accuracy: 0.408000 | 0.93 sec/iter\n",
      "Epoch: 361 | Batch: 007 / 011 | Total loss: 1.682 | Reg loss: 0.043 | Tree loss: 1.682 | Accuracy: 0.433500 | 0.93 sec/iter\n",
      "Epoch: 361 | Batch: 008 / 011 | Total loss: 1.691 | Reg loss: 0.043 | Tree loss: 1.691 | Accuracy: 0.428000 | 0.93 sec/iter\n",
      "Epoch: 361 | Batch: 009 / 011 | Total loss: 1.681 | Reg loss: 0.043 | Tree loss: 1.681 | Accuracy: 0.428000 | 0.929 sec/iter\n",
      "Epoch: 361 | Batch: 010 / 011 | Total loss: 1.732 | Reg loss: 0.043 | Tree loss: 1.732 | Accuracy: 0.409556 | 0.929 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 362 | Batch: 000 / 011 | Total loss: 1.864 | Reg loss: 0.043 | Tree loss: 1.864 | Accuracy: 0.407500 | 0.929 sec/iter\n",
      "Epoch: 362 | Batch: 001 / 011 | Total loss: 1.846 | Reg loss: 0.043 | Tree loss: 1.846 | Accuracy: 0.403500 | 0.929 sec/iter\n",
      "Epoch: 362 | Batch: 002 / 011 | Total loss: 1.820 | Reg loss: 0.043 | Tree loss: 1.820 | Accuracy: 0.396000 | 0.929 sec/iter\n",
      "Epoch: 362 | Batch: 003 / 011 | Total loss: 1.788 | Reg loss: 0.043 | Tree loss: 1.788 | Accuracy: 0.426000 | 0.929 sec/iter\n",
      "Epoch: 362 | Batch: 004 / 011 | Total loss: 1.757 | Reg loss: 0.043 | Tree loss: 1.757 | Accuracy: 0.413000 | 0.929 sec/iter\n",
      "Epoch: 362 | Batch: 005 / 011 | Total loss: 1.711 | Reg loss: 0.043 | Tree loss: 1.711 | Accuracy: 0.435500 | 0.929 sec/iter\n",
      "Epoch: 362 | Batch: 006 / 011 | Total loss: 1.696 | Reg loss: 0.043 | Tree loss: 1.696 | Accuracy: 0.415500 | 0.929 sec/iter\n",
      "Epoch: 362 | Batch: 007 / 011 | Total loss: 1.686 | Reg loss: 0.043 | Tree loss: 1.686 | Accuracy: 0.426000 | 0.929 sec/iter\n",
      "Epoch: 362 | Batch: 008 / 011 | Total loss: 1.663 | Reg loss: 0.043 | Tree loss: 1.663 | Accuracy: 0.430500 | 0.929 sec/iter\n",
      "Epoch: 362 | Batch: 009 / 011 | Total loss: 1.679 | Reg loss: 0.043 | Tree loss: 1.679 | Accuracy: 0.433500 | 0.929 sec/iter\n",
      "Epoch: 362 | Batch: 010 / 011 | Total loss: 1.597 | Reg loss: 0.043 | Tree loss: 1.597 | Accuracy: 0.447099 | 0.929 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 363 | Batch: 000 / 011 | Total loss: 1.882 | Reg loss: 0.043 | Tree loss: 1.882 | Accuracy: 0.392500 | 0.929 sec/iter\n",
      "Epoch: 363 | Batch: 001 / 011 | Total loss: 1.827 | Reg loss: 0.043 | Tree loss: 1.827 | Accuracy: 0.417000 | 0.929 sec/iter\n",
      "Epoch: 363 | Batch: 002 / 011 | Total loss: 1.817 | Reg loss: 0.043 | Tree loss: 1.817 | Accuracy: 0.411500 | 0.929 sec/iter\n",
      "Epoch: 363 | Batch: 003 / 011 | Total loss: 1.752 | Reg loss: 0.043 | Tree loss: 1.752 | Accuracy: 0.430500 | 0.929 sec/iter\n",
      "Epoch: 363 | Batch: 004 / 011 | Total loss: 1.737 | Reg loss: 0.043 | Tree loss: 1.737 | Accuracy: 0.418000 | 0.929 sec/iter\n",
      "Epoch: 363 | Batch: 005 / 011 | Total loss: 1.721 | Reg loss: 0.043 | Tree loss: 1.721 | Accuracy: 0.411000 | 0.929 sec/iter\n",
      "Epoch: 363 | Batch: 006 / 011 | Total loss: 1.696 | Reg loss: 0.043 | Tree loss: 1.696 | Accuracy: 0.429000 | 0.929 sec/iter\n",
      "Epoch: 363 | Batch: 007 / 011 | Total loss: 1.693 | Reg loss: 0.043 | Tree loss: 1.693 | Accuracy: 0.423500 | 0.929 sec/iter\n",
      "Epoch: 363 | Batch: 008 / 011 | Total loss: 1.666 | Reg loss: 0.043 | Tree loss: 1.666 | Accuracy: 0.452000 | 0.929 sec/iter\n",
      "Epoch: 363 | Batch: 009 / 011 | Total loss: 1.699 | Reg loss: 0.043 | Tree loss: 1.699 | Accuracy: 0.412000 | 0.929 sec/iter\n",
      "Epoch: 363 | Batch: 010 / 011 | Total loss: 1.726 | Reg loss: 0.043 | Tree loss: 1.726 | Accuracy: 0.409556 | 0.929 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 364 | Batch: 000 / 011 | Total loss: 1.881 | Reg loss: 0.043 | Tree loss: 1.881 | Accuracy: 0.408500 | 0.929 sec/iter\n",
      "Epoch: 364 | Batch: 001 / 011 | Total loss: 1.848 | Reg loss: 0.043 | Tree loss: 1.848 | Accuracy: 0.403000 | 0.929 sec/iter\n",
      "Epoch: 364 | Batch: 002 / 011 | Total loss: 1.793 | Reg loss: 0.043 | Tree loss: 1.793 | Accuracy: 0.408500 | 0.929 sec/iter\n",
      "Epoch: 364 | Batch: 003 / 011 | Total loss: 1.773 | Reg loss: 0.043 | Tree loss: 1.773 | Accuracy: 0.404500 | 0.929 sec/iter\n",
      "Epoch: 364 | Batch: 004 / 011 | Total loss: 1.763 | Reg loss: 0.043 | Tree loss: 1.763 | Accuracy: 0.410500 | 0.929 sec/iter\n",
      "Epoch: 364 | Batch: 005 / 011 | Total loss: 1.713 | Reg loss: 0.043 | Tree loss: 1.713 | Accuracy: 0.426000 | 0.929 sec/iter\n",
      "Epoch: 364 | Batch: 006 / 011 | Total loss: 1.710 | Reg loss: 0.043 | Tree loss: 1.710 | Accuracy: 0.410000 | 0.929 sec/iter\n",
      "Epoch: 364 | Batch: 007 / 011 | Total loss: 1.665 | Reg loss: 0.043 | Tree loss: 1.665 | Accuracy: 0.451000 | 0.929 sec/iter\n",
      "Epoch: 364 | Batch: 008 / 011 | Total loss: 1.687 | Reg loss: 0.043 | Tree loss: 1.687 | Accuracy: 0.426500 | 0.929 sec/iter\n",
      "Epoch: 364 | Batch: 009 / 011 | Total loss: 1.673 | Reg loss: 0.043 | Tree loss: 1.673 | Accuracy: 0.421000 | 0.929 sec/iter\n",
      "Epoch: 364 | Batch: 010 / 011 | Total loss: 1.676 | Reg loss: 0.043 | Tree loss: 1.676 | Accuracy: 0.447099 | 0.929 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 365 | Batch: 000 / 011 | Total loss: 1.843 | Reg loss: 0.043 | Tree loss: 1.843 | Accuracy: 0.423500 | 0.929 sec/iter\n",
      "Epoch: 365 | Batch: 001 / 011 | Total loss: 1.827 | Reg loss: 0.043 | Tree loss: 1.827 | Accuracy: 0.417500 | 0.929 sec/iter\n",
      "Epoch: 365 | Batch: 002 / 011 | Total loss: 1.780 | Reg loss: 0.043 | Tree loss: 1.780 | Accuracy: 0.428000 | 0.929 sec/iter\n",
      "Epoch: 365 | Batch: 003 / 011 | Total loss: 1.782 | Reg loss: 0.043 | Tree loss: 1.782 | Accuracy: 0.408000 | 0.929 sec/iter\n",
      "Epoch: 365 | Batch: 004 / 011 | Total loss: 1.761 | Reg loss: 0.043 | Tree loss: 1.761 | Accuracy: 0.423000 | 0.929 sec/iter\n",
      "Epoch: 365 | Batch: 005 / 011 | Total loss: 1.721 | Reg loss: 0.043 | Tree loss: 1.721 | Accuracy: 0.424000 | 0.929 sec/iter\n",
      "Epoch: 365 | Batch: 006 / 011 | Total loss: 1.702 | Reg loss: 0.043 | Tree loss: 1.702 | Accuracy: 0.412000 | 0.929 sec/iter\n",
      "Epoch: 365 | Batch: 007 / 011 | Total loss: 1.695 | Reg loss: 0.043 | Tree loss: 1.695 | Accuracy: 0.421000 | 0.929 sec/iter\n",
      "Epoch: 365 | Batch: 008 / 011 | Total loss: 1.682 | Reg loss: 0.043 | Tree loss: 1.682 | Accuracy: 0.413500 | 0.929 sec/iter\n",
      "Epoch: 365 | Batch: 009 / 011 | Total loss: 1.679 | Reg loss: 0.043 | Tree loss: 1.679 | Accuracy: 0.427500 | 0.929 sec/iter\n",
      "Epoch: 365 | Batch: 010 / 011 | Total loss: 1.710 | Reg loss: 0.043 | Tree loss: 1.710 | Accuracy: 0.450512 | 0.929 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 366 | Batch: 000 / 011 | Total loss: 1.846 | Reg loss: 0.043 | Tree loss: 1.846 | Accuracy: 0.406500 | 0.929 sec/iter\n",
      "Epoch: 366 | Batch: 001 / 011 | Total loss: 1.825 | Reg loss: 0.043 | Tree loss: 1.825 | Accuracy: 0.412000 | 0.929 sec/iter\n",
      "Epoch: 366 | Batch: 002 / 011 | Total loss: 1.819 | Reg loss: 0.043 | Tree loss: 1.819 | Accuracy: 0.400000 | 0.929 sec/iter\n",
      "Epoch: 366 | Batch: 003 / 011 | Total loss: 1.757 | Reg loss: 0.043 | Tree loss: 1.757 | Accuracy: 0.422500 | 0.929 sec/iter\n",
      "Epoch: 366 | Batch: 004 / 011 | Total loss: 1.733 | Reg loss: 0.043 | Tree loss: 1.733 | Accuracy: 0.427000 | 0.929 sec/iter\n",
      "Epoch: 366 | Batch: 005 / 011 | Total loss: 1.738 | Reg loss: 0.043 | Tree loss: 1.738 | Accuracy: 0.416000 | 0.929 sec/iter\n",
      "Epoch: 366 | Batch: 006 / 011 | Total loss: 1.701 | Reg loss: 0.043 | Tree loss: 1.701 | Accuracy: 0.429500 | 0.929 sec/iter\n",
      "Epoch: 366 | Batch: 007 / 011 | Total loss: 1.677 | Reg loss: 0.043 | Tree loss: 1.677 | Accuracy: 0.426000 | 0.929 sec/iter\n",
      "Epoch: 366 | Batch: 008 / 011 | Total loss: 1.701 | Reg loss: 0.043 | Tree loss: 1.701 | Accuracy: 0.410000 | 0.929 sec/iter\n",
      "Epoch: 366 | Batch: 009 / 011 | Total loss: 1.701 | Reg loss: 0.043 | Tree loss: 1.701 | Accuracy: 0.423500 | 0.929 sec/iter\n",
      "Epoch: 366 | Batch: 010 / 011 | Total loss: 1.681 | Reg loss: 0.043 | Tree loss: 1.681 | Accuracy: 0.447099 | 0.929 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 367 | Batch: 000 / 011 | Total loss: 1.859 | Reg loss: 0.043 | Tree loss: 1.859 | Accuracy: 0.405000 | 0.929 sec/iter\n",
      "Epoch: 367 | Batch: 001 / 011 | Total loss: 1.820 | Reg loss: 0.043 | Tree loss: 1.820 | Accuracy: 0.416500 | 0.929 sec/iter\n",
      "Epoch: 367 | Batch: 002 / 011 | Total loss: 1.793 | Reg loss: 0.043 | Tree loss: 1.793 | Accuracy: 0.418500 | 0.929 sec/iter\n",
      "Epoch: 367 | Batch: 003 / 011 | Total loss: 1.791 | Reg loss: 0.043 | Tree loss: 1.791 | Accuracy: 0.414500 | 0.929 sec/iter\n",
      "Epoch: 367 | Batch: 004 / 011 | Total loss: 1.758 | Reg loss: 0.043 | Tree loss: 1.758 | Accuracy: 0.408500 | 0.929 sec/iter\n",
      "Epoch: 367 | Batch: 005 / 011 | Total loss: 1.717 | Reg loss: 0.043 | Tree loss: 1.717 | Accuracy: 0.431000 | 0.929 sec/iter\n",
      "Epoch: 367 | Batch: 006 / 011 | Total loss: 1.692 | Reg loss: 0.043 | Tree loss: 1.692 | Accuracy: 0.415000 | 0.929 sec/iter\n",
      "Epoch: 367 | Batch: 007 / 011 | Total loss: 1.681 | Reg loss: 0.043 | Tree loss: 1.681 | Accuracy: 0.427500 | 0.929 sec/iter\n",
      "Epoch: 367 | Batch: 008 / 011 | Total loss: 1.683 | Reg loss: 0.043 | Tree loss: 1.683 | Accuracy: 0.432500 | 0.929 sec/iter\n",
      "Epoch: 367 | Batch: 009 / 011 | Total loss: 1.707 | Reg loss: 0.043 | Tree loss: 1.707 | Accuracy: 0.425500 | 0.929 sec/iter\n",
      "Epoch: 367 | Batch: 010 / 011 | Total loss: 1.712 | Reg loss: 0.043 | Tree loss: 1.712 | Accuracy: 0.430034 | 0.929 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 368 | Batch: 000 / 011 | Total loss: 1.859 | Reg loss: 0.043 | Tree loss: 1.859 | Accuracy: 0.398500 | 0.929 sec/iter\n",
      "Epoch: 368 | Batch: 001 / 011 | Total loss: 1.839 | Reg loss: 0.043 | Tree loss: 1.839 | Accuracy: 0.403500 | 0.929 sec/iter\n",
      "Epoch: 368 | Batch: 002 / 011 | Total loss: 1.798 | Reg loss: 0.043 | Tree loss: 1.798 | Accuracy: 0.420000 | 0.929 sec/iter\n",
      "Epoch: 368 | Batch: 003 / 011 | Total loss: 1.781 | Reg loss: 0.043 | Tree loss: 1.781 | Accuracy: 0.421000 | 0.929 sec/iter\n",
      "Epoch: 368 | Batch: 004 / 011 | Total loss: 1.755 | Reg loss: 0.043 | Tree loss: 1.755 | Accuracy: 0.422500 | 0.929 sec/iter\n",
      "Epoch: 368 | Batch: 005 / 011 | Total loss: 1.713 | Reg loss: 0.043 | Tree loss: 1.713 | Accuracy: 0.427000 | 0.929 sec/iter\n",
      "Epoch: 368 | Batch: 006 / 011 | Total loss: 1.704 | Reg loss: 0.043 | Tree loss: 1.704 | Accuracy: 0.410500 | 0.929 sec/iter\n",
      "Epoch: 368 | Batch: 007 / 011 | Total loss: 1.701 | Reg loss: 0.043 | Tree loss: 1.701 | Accuracy: 0.426500 | 0.929 sec/iter\n",
      "Epoch: 368 | Batch: 008 / 011 | Total loss: 1.691 | Reg loss: 0.043 | Tree loss: 1.691 | Accuracy: 0.430500 | 0.929 sec/iter\n",
      "Epoch: 368 | Batch: 009 / 011 | Total loss: 1.672 | Reg loss: 0.043 | Tree loss: 1.672 | Accuracy: 0.424500 | 0.929 sec/iter\n",
      "Epoch: 368 | Batch: 010 / 011 | Total loss: 1.604 | Reg loss: 0.043 | Tree loss: 1.604 | Accuracy: 0.436860 | 0.929 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 369 | Batch: 000 / 011 | Total loss: 1.858 | Reg loss: 0.043 | Tree loss: 1.858 | Accuracy: 0.410000 | 0.929 sec/iter\n",
      "Epoch: 369 | Batch: 001 / 011 | Total loss: 1.828 | Reg loss: 0.043 | Tree loss: 1.828 | Accuracy: 0.422500 | 0.929 sec/iter\n",
      "Epoch: 369 | Batch: 002 / 011 | Total loss: 1.782 | Reg loss: 0.043 | Tree loss: 1.782 | Accuracy: 0.425500 | 0.929 sec/iter\n",
      "Epoch: 369 | Batch: 003 / 011 | Total loss: 1.780 | Reg loss: 0.043 | Tree loss: 1.780 | Accuracy: 0.417500 | 0.929 sec/iter\n",
      "Epoch: 369 | Batch: 004 / 011 | Total loss: 1.752 | Reg loss: 0.043 | Tree loss: 1.752 | Accuracy: 0.408000 | 0.929 sec/iter\n",
      "Epoch: 369 | Batch: 005 / 011 | Total loss: 1.713 | Reg loss: 0.043 | Tree loss: 1.713 | Accuracy: 0.413500 | 0.929 sec/iter\n",
      "Epoch: 369 | Batch: 006 / 011 | Total loss: 1.721 | Reg loss: 0.043 | Tree loss: 1.721 | Accuracy: 0.422000 | 0.929 sec/iter\n",
      "Epoch: 369 | Batch: 007 / 011 | Total loss: 1.700 | Reg loss: 0.043 | Tree loss: 1.700 | Accuracy: 0.423500 | 0.929 sec/iter\n",
      "Epoch: 369 | Batch: 008 / 011 | Total loss: 1.712 | Reg loss: 0.043 | Tree loss: 1.712 | Accuracy: 0.412500 | 0.929 sec/iter\n",
      "Epoch: 369 | Batch: 009 / 011 | Total loss: 1.657 | Reg loss: 0.043 | Tree loss: 1.657 | Accuracy: 0.428500 | 0.929 sec/iter\n",
      "Epoch: 369 | Batch: 010 / 011 | Total loss: 1.602 | Reg loss: 0.043 | Tree loss: 1.602 | Accuracy: 0.498294 | 0.929 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 370 | Batch: 000 / 011 | Total loss: 1.857 | Reg loss: 0.043 | Tree loss: 1.857 | Accuracy: 0.423500 | 0.929 sec/iter\n",
      "Epoch: 370 | Batch: 001 / 011 | Total loss: 1.821 | Reg loss: 0.043 | Tree loss: 1.821 | Accuracy: 0.432500 | 0.929 sec/iter\n",
      "Epoch: 370 | Batch: 002 / 011 | Total loss: 1.804 | Reg loss: 0.043 | Tree loss: 1.804 | Accuracy: 0.419000 | 0.929 sec/iter\n",
      "Epoch: 370 | Batch: 003 / 011 | Total loss: 1.767 | Reg loss: 0.043 | Tree loss: 1.767 | Accuracy: 0.415500 | 0.929 sec/iter\n",
      "Epoch: 370 | Batch: 004 / 011 | Total loss: 1.726 | Reg loss: 0.043 | Tree loss: 1.726 | Accuracy: 0.425500 | 0.929 sec/iter\n",
      "Epoch: 370 | Batch: 005 / 011 | Total loss: 1.740 | Reg loss: 0.043 | Tree loss: 1.740 | Accuracy: 0.419000 | 0.928 sec/iter\n",
      "Epoch: 370 | Batch: 006 / 011 | Total loss: 1.699 | Reg loss: 0.043 | Tree loss: 1.699 | Accuracy: 0.433000 | 0.928 sec/iter\n",
      "Epoch: 370 | Batch: 007 / 011 | Total loss: 1.708 | Reg loss: 0.043 | Tree loss: 1.708 | Accuracy: 0.400000 | 0.928 sec/iter\n",
      "Epoch: 370 | Batch: 008 / 011 | Total loss: 1.661 | Reg loss: 0.043 | Tree loss: 1.661 | Accuracy: 0.437500 | 0.928 sec/iter\n",
      "Epoch: 370 | Batch: 009 / 011 | Total loss: 1.709 | Reg loss: 0.043 | Tree loss: 1.709 | Accuracy: 0.396500 | 0.928 sec/iter\n",
      "Epoch: 370 | Batch: 010 / 011 | Total loss: 1.627 | Reg loss: 0.043 | Tree loss: 1.627 | Accuracy: 0.423208 | 0.928 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 371 | Batch: 000 / 011 | Total loss: 1.854 | Reg loss: 0.043 | Tree loss: 1.854 | Accuracy: 0.400500 | 0.928 sec/iter\n",
      "Epoch: 371 | Batch: 001 / 011 | Total loss: 1.843 | Reg loss: 0.043 | Tree loss: 1.843 | Accuracy: 0.404500 | 0.928 sec/iter\n",
      "Epoch: 371 | Batch: 002 / 011 | Total loss: 1.818 | Reg loss: 0.043 | Tree loss: 1.818 | Accuracy: 0.401000 | 0.928 sec/iter\n",
      "Epoch: 371 | Batch: 003 / 011 | Total loss: 1.752 | Reg loss: 0.043 | Tree loss: 1.752 | Accuracy: 0.430000 | 0.928 sec/iter\n",
      "Epoch: 371 | Batch: 004 / 011 | Total loss: 1.725 | Reg loss: 0.043 | Tree loss: 1.725 | Accuracy: 0.421000 | 0.928 sec/iter\n",
      "Epoch: 371 | Batch: 005 / 011 | Total loss: 1.715 | Reg loss: 0.043 | Tree loss: 1.715 | Accuracy: 0.419500 | 0.928 sec/iter\n",
      "Epoch: 371 | Batch: 006 / 011 | Total loss: 1.725 | Reg loss: 0.043 | Tree loss: 1.725 | Accuracy: 0.417000 | 0.928 sec/iter\n",
      "Epoch: 371 | Batch: 007 / 011 | Total loss: 1.681 | Reg loss: 0.043 | Tree loss: 1.681 | Accuracy: 0.423000 | 0.928 sec/iter\n",
      "Epoch: 371 | Batch: 008 / 011 | Total loss: 1.704 | Reg loss: 0.043 | Tree loss: 1.704 | Accuracy: 0.425000 | 0.928 sec/iter\n",
      "Epoch: 371 | Batch: 009 / 011 | Total loss: 1.676 | Reg loss: 0.043 | Tree loss: 1.676 | Accuracy: 0.429500 | 0.928 sec/iter\n",
      "Epoch: 371 | Batch: 010 / 011 | Total loss: 1.634 | Reg loss: 0.043 | Tree loss: 1.634 | Accuracy: 0.457338 | 0.928 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 372 | Batch: 000 / 011 | Total loss: 1.867 | Reg loss: 0.043 | Tree loss: 1.867 | Accuracy: 0.398000 | 0.928 sec/iter\n",
      "Epoch: 372 | Batch: 001 / 011 | Total loss: 1.832 | Reg loss: 0.043 | Tree loss: 1.832 | Accuracy: 0.411000 | 0.928 sec/iter\n",
      "Epoch: 372 | Batch: 002 / 011 | Total loss: 1.797 | Reg loss: 0.043 | Tree loss: 1.797 | Accuracy: 0.418500 | 0.928 sec/iter\n",
      "Epoch: 372 | Batch: 003 / 011 | Total loss: 1.760 | Reg loss: 0.043 | Tree loss: 1.760 | Accuracy: 0.423500 | 0.928 sec/iter\n",
      "Epoch: 372 | Batch: 004 / 011 | Total loss: 1.739 | Reg loss: 0.043 | Tree loss: 1.739 | Accuracy: 0.437000 | 0.928 sec/iter\n",
      "Epoch: 372 | Batch: 005 / 011 | Total loss: 1.729 | Reg loss: 0.043 | Tree loss: 1.729 | Accuracy: 0.430000 | 0.928 sec/iter\n",
      "Epoch: 372 | Batch: 006 / 011 | Total loss: 1.708 | Reg loss: 0.043 | Tree loss: 1.708 | Accuracy: 0.420000 | 0.928 sec/iter\n",
      "Epoch: 372 | Batch: 007 / 011 | Total loss: 1.696 | Reg loss: 0.043 | Tree loss: 1.696 | Accuracy: 0.424000 | 0.928 sec/iter\n",
      "Epoch: 372 | Batch: 008 / 011 | Total loss: 1.677 | Reg loss: 0.043 | Tree loss: 1.677 | Accuracy: 0.419500 | 0.928 sec/iter\n",
      "Epoch: 372 | Batch: 009 / 011 | Total loss: 1.701 | Reg loss: 0.043 | Tree loss: 1.701 | Accuracy: 0.410500 | 0.928 sec/iter\n",
      "Epoch: 372 | Batch: 010 / 011 | Total loss: 1.592 | Reg loss: 0.043 | Tree loss: 1.592 | Accuracy: 0.481229 | 0.928 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 373 | Batch: 000 / 011 | Total loss: 1.861 | Reg loss: 0.043 | Tree loss: 1.861 | Accuracy: 0.405500 | 0.928 sec/iter\n",
      "Epoch: 373 | Batch: 001 / 011 | Total loss: 1.831 | Reg loss: 0.043 | Tree loss: 1.831 | Accuracy: 0.407000 | 0.928 sec/iter\n",
      "Epoch: 373 | Batch: 002 / 011 | Total loss: 1.813 | Reg loss: 0.043 | Tree loss: 1.813 | Accuracy: 0.414500 | 0.928 sec/iter\n",
      "Epoch: 373 | Batch: 003 / 011 | Total loss: 1.803 | Reg loss: 0.043 | Tree loss: 1.803 | Accuracy: 0.399500 | 0.928 sec/iter\n",
      "Epoch: 373 | Batch: 004 / 011 | Total loss: 1.723 | Reg loss: 0.043 | Tree loss: 1.723 | Accuracy: 0.439000 | 0.928 sec/iter\n",
      "Epoch: 373 | Batch: 005 / 011 | Total loss: 1.725 | Reg loss: 0.043 | Tree loss: 1.725 | Accuracy: 0.415000 | 0.928 sec/iter\n",
      "Epoch: 373 | Batch: 006 / 011 | Total loss: 1.669 | Reg loss: 0.043 | Tree loss: 1.669 | Accuracy: 0.443000 | 0.928 sec/iter\n",
      "Epoch: 373 | Batch: 007 / 011 | Total loss: 1.697 | Reg loss: 0.043 | Tree loss: 1.697 | Accuracy: 0.422500 | 0.928 sec/iter\n",
      "Epoch: 373 | Batch: 008 / 011 | Total loss: 1.699 | Reg loss: 0.043 | Tree loss: 1.699 | Accuracy: 0.421500 | 0.928 sec/iter\n",
      "Epoch: 373 | Batch: 009 / 011 | Total loss: 1.685 | Reg loss: 0.043 | Tree loss: 1.685 | Accuracy: 0.417000 | 0.928 sec/iter\n",
      "Epoch: 373 | Batch: 010 / 011 | Total loss: 1.668 | Reg loss: 0.043 | Tree loss: 1.668 | Accuracy: 0.430034 | 0.928 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 374 | Batch: 000 / 011 | Total loss: 1.869 | Reg loss: 0.043 | Tree loss: 1.869 | Accuracy: 0.408500 | 0.928 sec/iter\n",
      "Epoch: 374 | Batch: 001 / 011 | Total loss: 1.799 | Reg loss: 0.043 | Tree loss: 1.799 | Accuracy: 0.422500 | 0.928 sec/iter\n",
      "Epoch: 374 | Batch: 002 / 011 | Total loss: 1.798 | Reg loss: 0.043 | Tree loss: 1.798 | Accuracy: 0.398500 | 0.928 sec/iter\n",
      "Epoch: 374 | Batch: 003 / 011 | Total loss: 1.784 | Reg loss: 0.043 | Tree loss: 1.784 | Accuracy: 0.407500 | 0.928 sec/iter\n",
      "Epoch: 374 | Batch: 004 / 011 | Total loss: 1.745 | Reg loss: 0.043 | Tree loss: 1.745 | Accuracy: 0.418500 | 0.928 sec/iter\n",
      "Epoch: 374 | Batch: 005 / 011 | Total loss: 1.720 | Reg loss: 0.043 | Tree loss: 1.720 | Accuracy: 0.426500 | 0.928 sec/iter\n",
      "Epoch: 374 | Batch: 006 / 011 | Total loss: 1.704 | Reg loss: 0.043 | Tree loss: 1.704 | Accuracy: 0.418500 | 0.928 sec/iter\n",
      "Epoch: 374 | Batch: 007 / 011 | Total loss: 1.676 | Reg loss: 0.043 | Tree loss: 1.676 | Accuracy: 0.436000 | 0.928 sec/iter\n",
      "Epoch: 374 | Batch: 008 / 011 | Total loss: 1.702 | Reg loss: 0.043 | Tree loss: 1.702 | Accuracy: 0.421000 | 0.928 sec/iter\n",
      "Epoch: 374 | Batch: 009 / 011 | Total loss: 1.704 | Reg loss: 0.043 | Tree loss: 1.704 | Accuracy: 0.415500 | 0.928 sec/iter\n",
      "Epoch: 374 | Batch: 010 / 011 | Total loss: 1.691 | Reg loss: 0.043 | Tree loss: 1.691 | Accuracy: 0.392491 | 0.928 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 375 | Batch: 000 / 011 | Total loss: 1.871 | Reg loss: 0.043 | Tree loss: 1.871 | Accuracy: 0.402500 | 0.928 sec/iter\n",
      "Epoch: 375 | Batch: 001 / 011 | Total loss: 1.799 | Reg loss: 0.043 | Tree loss: 1.799 | Accuracy: 0.435000 | 0.928 sec/iter\n",
      "Epoch: 375 | Batch: 002 / 011 | Total loss: 1.810 | Reg loss: 0.043 | Tree loss: 1.810 | Accuracy: 0.408500 | 0.928 sec/iter\n",
      "Epoch: 375 | Batch: 003 / 011 | Total loss: 1.798 | Reg loss: 0.043 | Tree loss: 1.798 | Accuracy: 0.405000 | 0.928 sec/iter\n",
      "Epoch: 375 | Batch: 004 / 011 | Total loss: 1.765 | Reg loss: 0.043 | Tree loss: 1.765 | Accuracy: 0.410000 | 0.928 sec/iter\n",
      "Epoch: 375 | Batch: 005 / 011 | Total loss: 1.719 | Reg loss: 0.043 | Tree loss: 1.719 | Accuracy: 0.426500 | 0.928 sec/iter\n",
      "Epoch: 375 | Batch: 006 / 011 | Total loss: 1.698 | Reg loss: 0.043 | Tree loss: 1.698 | Accuracy: 0.426500 | 0.928 sec/iter\n",
      "Epoch: 375 | Batch: 007 / 011 | Total loss: 1.696 | Reg loss: 0.043 | Tree loss: 1.696 | Accuracy: 0.425500 | 0.928 sec/iter\n",
      "Epoch: 375 | Batch: 008 / 011 | Total loss: 1.665 | Reg loss: 0.043 | Tree loss: 1.665 | Accuracy: 0.430500 | 0.928 sec/iter\n",
      "Epoch: 375 | Batch: 009 / 011 | Total loss: 1.671 | Reg loss: 0.043 | Tree loss: 1.671 | Accuracy: 0.436000 | 0.928 sec/iter\n",
      "Epoch: 375 | Batch: 010 / 011 | Total loss: 1.674 | Reg loss: 0.043 | Tree loss: 1.674 | Accuracy: 0.419795 | 0.928 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 376 | Batch: 000 / 011 | Total loss: 1.865 | Reg loss: 0.043 | Tree loss: 1.865 | Accuracy: 0.412000 | 0.928 sec/iter\n",
      "Epoch: 376 | Batch: 001 / 011 | Total loss: 1.835 | Reg loss: 0.043 | Tree loss: 1.835 | Accuracy: 0.415500 | 0.928 sec/iter\n",
      "Epoch: 376 | Batch: 002 / 011 | Total loss: 1.796 | Reg loss: 0.043 | Tree loss: 1.796 | Accuracy: 0.410000 | 0.928 sec/iter\n",
      "Epoch: 376 | Batch: 003 / 011 | Total loss: 1.740 | Reg loss: 0.043 | Tree loss: 1.740 | Accuracy: 0.441000 | 0.928 sec/iter\n",
      "Epoch: 376 | Batch: 004 / 011 | Total loss: 1.774 | Reg loss: 0.043 | Tree loss: 1.774 | Accuracy: 0.402500 | 0.928 sec/iter\n",
      "Epoch: 376 | Batch: 005 / 011 | Total loss: 1.725 | Reg loss: 0.043 | Tree loss: 1.725 | Accuracy: 0.410000 | 0.928 sec/iter\n",
      "Epoch: 376 | Batch: 006 / 011 | Total loss: 1.701 | Reg loss: 0.043 | Tree loss: 1.701 | Accuracy: 0.416500 | 0.928 sec/iter\n",
      "Epoch: 376 | Batch: 007 / 011 | Total loss: 1.687 | Reg loss: 0.043 | Tree loss: 1.687 | Accuracy: 0.432500 | 0.928 sec/iter\n",
      "Epoch: 376 | Batch: 008 / 011 | Total loss: 1.694 | Reg loss: 0.043 | Tree loss: 1.694 | Accuracy: 0.420500 | 0.928 sec/iter\n",
      "Epoch: 376 | Batch: 009 / 011 | Total loss: 1.694 | Reg loss: 0.043 | Tree loss: 1.694 | Accuracy: 0.428000 | 0.928 sec/iter\n",
      "Epoch: 376 | Batch: 010 / 011 | Total loss: 1.682 | Reg loss: 0.043 | Tree loss: 1.682 | Accuracy: 0.399317 | 0.928 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 377 | Batch: 000 / 011 | Total loss: 1.866 | Reg loss: 0.043 | Tree loss: 1.866 | Accuracy: 0.405000 | 0.928 sec/iter\n",
      "Epoch: 377 | Batch: 001 / 011 | Total loss: 1.812 | Reg loss: 0.043 | Tree loss: 1.812 | Accuracy: 0.411500 | 0.928 sec/iter\n",
      "Epoch: 377 | Batch: 002 / 011 | Total loss: 1.772 | Reg loss: 0.043 | Tree loss: 1.772 | Accuracy: 0.435000 | 0.928 sec/iter\n",
      "Epoch: 377 | Batch: 003 / 011 | Total loss: 1.792 | Reg loss: 0.043 | Tree loss: 1.792 | Accuracy: 0.414000 | 0.928 sec/iter\n",
      "Epoch: 377 | Batch: 004 / 011 | Total loss: 1.752 | Reg loss: 0.043 | Tree loss: 1.752 | Accuracy: 0.421500 | 0.928 sec/iter\n",
      "Epoch: 377 | Batch: 005 / 011 | Total loss: 1.731 | Reg loss: 0.043 | Tree loss: 1.731 | Accuracy: 0.420500 | 0.928 sec/iter\n",
      "Epoch: 377 | Batch: 006 / 011 | Total loss: 1.693 | Reg loss: 0.043 | Tree loss: 1.693 | Accuracy: 0.440000 | 0.928 sec/iter\n",
      "Epoch: 377 | Batch: 007 / 011 | Total loss: 1.693 | Reg loss: 0.043 | Tree loss: 1.693 | Accuracy: 0.413500 | 0.928 sec/iter\n",
      "Epoch: 377 | Batch: 008 / 011 | Total loss: 1.701 | Reg loss: 0.043 | Tree loss: 1.701 | Accuracy: 0.422000 | 0.928 sec/iter\n",
      "Epoch: 377 | Batch: 009 / 011 | Total loss: 1.708 | Reg loss: 0.043 | Tree loss: 1.708 | Accuracy: 0.420500 | 0.928 sec/iter\n",
      "Epoch: 377 | Batch: 010 / 011 | Total loss: 1.640 | Reg loss: 0.043 | Tree loss: 1.640 | Accuracy: 0.453925 | 0.928 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 378 | Batch: 000 / 011 | Total loss: 1.870 | Reg loss: 0.043 | Tree loss: 1.870 | Accuracy: 0.416000 | 0.928 sec/iter\n",
      "Epoch: 378 | Batch: 001 / 011 | Total loss: 1.830 | Reg loss: 0.043 | Tree loss: 1.830 | Accuracy: 0.407500 | 0.928 sec/iter\n",
      "Epoch: 378 | Batch: 002 / 011 | Total loss: 1.799 | Reg loss: 0.043 | Tree loss: 1.799 | Accuracy: 0.423500 | 0.928 sec/iter\n",
      "Epoch: 378 | Batch: 003 / 011 | Total loss: 1.770 | Reg loss: 0.043 | Tree loss: 1.770 | Accuracy: 0.402000 | 0.928 sec/iter\n",
      "Epoch: 378 | Batch: 004 / 011 | Total loss: 1.728 | Reg loss: 0.043 | Tree loss: 1.728 | Accuracy: 0.428000 | 0.928 sec/iter\n",
      "Epoch: 378 | Batch: 005 / 011 | Total loss: 1.750 | Reg loss: 0.043 | Tree loss: 1.750 | Accuracy: 0.403000 | 0.928 sec/iter\n",
      "Epoch: 378 | Batch: 006 / 011 | Total loss: 1.678 | Reg loss: 0.043 | Tree loss: 1.678 | Accuracy: 0.447500 | 0.928 sec/iter\n",
      "Epoch: 378 | Batch: 007 / 011 | Total loss: 1.684 | Reg loss: 0.043 | Tree loss: 1.684 | Accuracy: 0.420500 | 0.928 sec/iter\n",
      "Epoch: 378 | Batch: 008 / 011 | Total loss: 1.687 | Reg loss: 0.043 | Tree loss: 1.687 | Accuracy: 0.412500 | 0.928 sec/iter\n",
      "Epoch: 378 | Batch: 009 / 011 | Total loss: 1.702 | Reg loss: 0.043 | Tree loss: 1.702 | Accuracy: 0.411500 | 0.928 sec/iter\n",
      "Epoch: 378 | Batch: 010 / 011 | Total loss: 1.647 | Reg loss: 0.043 | Tree loss: 1.647 | Accuracy: 0.440273 | 0.928 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 379 | Batch: 000 / 011 | Total loss: 1.849 | Reg loss: 0.043 | Tree loss: 1.849 | Accuracy: 0.414500 | 0.928 sec/iter\n",
      "Epoch: 379 | Batch: 001 / 011 | Total loss: 1.818 | Reg loss: 0.043 | Tree loss: 1.818 | Accuracy: 0.424500 | 0.928 sec/iter\n",
      "Epoch: 379 | Batch: 002 / 011 | Total loss: 1.778 | Reg loss: 0.043 | Tree loss: 1.778 | Accuracy: 0.429500 | 0.928 sec/iter\n",
      "Epoch: 379 | Batch: 003 / 011 | Total loss: 1.748 | Reg loss: 0.043 | Tree loss: 1.748 | Accuracy: 0.447500 | 0.928 sec/iter\n",
      "Epoch: 379 | Batch: 004 / 011 | Total loss: 1.766 | Reg loss: 0.043 | Tree loss: 1.766 | Accuracy: 0.401000 | 0.928 sec/iter\n",
      "Epoch: 379 | Batch: 005 / 011 | Total loss: 1.728 | Reg loss: 0.043 | Tree loss: 1.728 | Accuracy: 0.420500 | 0.928 sec/iter\n",
      "Epoch: 379 | Batch: 006 / 011 | Total loss: 1.708 | Reg loss: 0.043 | Tree loss: 1.708 | Accuracy: 0.418000 | 0.928 sec/iter\n",
      "Epoch: 379 | Batch: 007 / 011 | Total loss: 1.705 | Reg loss: 0.043 | Tree loss: 1.705 | Accuracy: 0.408500 | 0.928 sec/iter\n",
      "Epoch: 379 | Batch: 008 / 011 | Total loss: 1.686 | Reg loss: 0.043 | Tree loss: 1.686 | Accuracy: 0.423000 | 0.928 sec/iter\n",
      "Epoch: 379 | Batch: 009 / 011 | Total loss: 1.695 | Reg loss: 0.043 | Tree loss: 1.695 | Accuracy: 0.415500 | 0.928 sec/iter\n",
      "Epoch: 379 | Batch: 010 / 011 | Total loss: 1.712 | Reg loss: 0.043 | Tree loss: 1.712 | Accuracy: 0.426621 | 0.927 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 380 | Batch: 000 / 011 | Total loss: 1.864 | Reg loss: 0.043 | Tree loss: 1.864 | Accuracy: 0.402500 | 0.928 sec/iter\n",
      "Epoch: 380 | Batch: 001 / 011 | Total loss: 1.827 | Reg loss: 0.043 | Tree loss: 1.827 | Accuracy: 0.416500 | 0.927 sec/iter\n",
      "Epoch: 380 | Batch: 002 / 011 | Total loss: 1.806 | Reg loss: 0.043 | Tree loss: 1.806 | Accuracy: 0.415000 | 0.927 sec/iter\n",
      "Epoch: 380 | Batch: 003 / 011 | Total loss: 1.759 | Reg loss: 0.043 | Tree loss: 1.759 | Accuracy: 0.435500 | 0.927 sec/iter\n",
      "Epoch: 380 | Batch: 004 / 011 | Total loss: 1.722 | Reg loss: 0.043 | Tree loss: 1.722 | Accuracy: 0.433500 | 0.927 sec/iter\n",
      "Epoch: 380 | Batch: 005 / 011 | Total loss: 1.716 | Reg loss: 0.043 | Tree loss: 1.716 | Accuracy: 0.426000 | 0.927 sec/iter\n",
      "Epoch: 380 | Batch: 006 / 011 | Total loss: 1.703 | Reg loss: 0.043 | Tree loss: 1.703 | Accuracy: 0.425000 | 0.927 sec/iter\n",
      "Epoch: 380 | Batch: 007 / 011 | Total loss: 1.693 | Reg loss: 0.043 | Tree loss: 1.693 | Accuracy: 0.410500 | 0.927 sec/iter\n",
      "Epoch: 380 | Batch: 008 / 011 | Total loss: 1.702 | Reg loss: 0.043 | Tree loss: 1.702 | Accuracy: 0.415000 | 0.927 sec/iter\n",
      "Epoch: 380 | Batch: 009 / 011 | Total loss: 1.696 | Reg loss: 0.043 | Tree loss: 1.696 | Accuracy: 0.420000 | 0.927 sec/iter\n",
      "Epoch: 380 | Batch: 010 / 011 | Total loss: 1.674 | Reg loss: 0.043 | Tree loss: 1.674 | Accuracy: 0.402730 | 0.927 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 381 | Batch: 000 / 011 | Total loss: 1.842 | Reg loss: 0.043 | Tree loss: 1.842 | Accuracy: 0.407000 | 0.927 sec/iter\n",
      "Epoch: 381 | Batch: 001 / 011 | Total loss: 1.807 | Reg loss: 0.043 | Tree loss: 1.807 | Accuracy: 0.427500 | 0.927 sec/iter\n",
      "Epoch: 381 | Batch: 002 / 011 | Total loss: 1.790 | Reg loss: 0.043 | Tree loss: 1.790 | Accuracy: 0.426000 | 0.927 sec/iter\n",
      "Epoch: 381 | Batch: 003 / 011 | Total loss: 1.778 | Reg loss: 0.043 | Tree loss: 1.778 | Accuracy: 0.417500 | 0.927 sec/iter\n",
      "Epoch: 381 | Batch: 004 / 011 | Total loss: 1.731 | Reg loss: 0.043 | Tree loss: 1.731 | Accuracy: 0.435000 | 0.927 sec/iter\n",
      "Epoch: 381 | Batch: 005 / 011 | Total loss: 1.745 | Reg loss: 0.043 | Tree loss: 1.745 | Accuracy: 0.409000 | 0.927 sec/iter\n",
      "Epoch: 381 | Batch: 006 / 011 | Total loss: 1.700 | Reg loss: 0.043 | Tree loss: 1.700 | Accuracy: 0.428000 | 0.927 sec/iter\n",
      "Epoch: 381 | Batch: 007 / 011 | Total loss: 1.721 | Reg loss: 0.043 | Tree loss: 1.721 | Accuracy: 0.408000 | 0.927 sec/iter\n",
      "Epoch: 381 | Batch: 008 / 011 | Total loss: 1.686 | Reg loss: 0.043 | Tree loss: 1.686 | Accuracy: 0.425000 | 0.927 sec/iter\n",
      "Epoch: 381 | Batch: 009 / 011 | Total loss: 1.691 | Reg loss: 0.043 | Tree loss: 1.691 | Accuracy: 0.407500 | 0.927 sec/iter\n",
      "Epoch: 381 | Batch: 010 / 011 | Total loss: 1.682 | Reg loss: 0.043 | Tree loss: 1.682 | Accuracy: 0.430034 | 0.927 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 382 | Batch: 000 / 011 | Total loss: 1.847 | Reg loss: 0.043 | Tree loss: 1.847 | Accuracy: 0.414500 | 0.927 sec/iter\n",
      "Epoch: 382 | Batch: 001 / 011 | Total loss: 1.847 | Reg loss: 0.043 | Tree loss: 1.847 | Accuracy: 0.416000 | 0.927 sec/iter\n",
      "Epoch: 382 | Batch: 002 / 011 | Total loss: 1.799 | Reg loss: 0.043 | Tree loss: 1.799 | Accuracy: 0.434500 | 0.927 sec/iter\n",
      "Epoch: 382 | Batch: 003 / 011 | Total loss: 1.781 | Reg loss: 0.043 | Tree loss: 1.781 | Accuracy: 0.403500 | 0.927 sec/iter\n",
      "Epoch: 382 | Batch: 004 / 011 | Total loss: 1.749 | Reg loss: 0.043 | Tree loss: 1.749 | Accuracy: 0.425000 | 0.927 sec/iter\n",
      "Epoch: 382 | Batch: 005 / 011 | Total loss: 1.758 | Reg loss: 0.043 | Tree loss: 1.758 | Accuracy: 0.388000 | 0.927 sec/iter\n",
      "Epoch: 382 | Batch: 006 / 011 | Total loss: 1.687 | Reg loss: 0.043 | Tree loss: 1.687 | Accuracy: 0.438000 | 0.927 sec/iter\n",
      "Epoch: 382 | Batch: 007 / 011 | Total loss: 1.664 | Reg loss: 0.043 | Tree loss: 1.664 | Accuracy: 0.440000 | 0.927 sec/iter\n",
      "Epoch: 382 | Batch: 008 / 011 | Total loss: 1.683 | Reg loss: 0.043 | Tree loss: 1.683 | Accuracy: 0.431500 | 0.927 sec/iter\n",
      "Epoch: 382 | Batch: 009 / 011 | Total loss: 1.683 | Reg loss: 0.043 | Tree loss: 1.683 | Accuracy: 0.423500 | 0.927 sec/iter\n",
      "Epoch: 382 | Batch: 010 / 011 | Total loss: 1.664 | Reg loss: 0.043 | Tree loss: 1.664 | Accuracy: 0.433447 | 0.927 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 383 | Batch: 000 / 011 | Total loss: 1.861 | Reg loss: 0.043 | Tree loss: 1.861 | Accuracy: 0.409000 | 0.927 sec/iter\n",
      "Epoch: 383 | Batch: 001 / 011 | Total loss: 1.831 | Reg loss: 0.043 | Tree loss: 1.831 | Accuracy: 0.415500 | 0.927 sec/iter\n",
      "Epoch: 383 | Batch: 002 / 011 | Total loss: 1.788 | Reg loss: 0.043 | Tree loss: 1.788 | Accuracy: 0.423500 | 0.927 sec/iter\n",
      "Epoch: 383 | Batch: 003 / 011 | Total loss: 1.791 | Reg loss: 0.043 | Tree loss: 1.791 | Accuracy: 0.395000 | 0.927 sec/iter\n",
      "Epoch: 383 | Batch: 004 / 011 | Total loss: 1.738 | Reg loss: 0.043 | Tree loss: 1.738 | Accuracy: 0.416000 | 0.927 sec/iter\n",
      "Epoch: 383 | Batch: 005 / 011 | Total loss: 1.712 | Reg loss: 0.043 | Tree loss: 1.712 | Accuracy: 0.428500 | 0.927 sec/iter\n",
      "Epoch: 383 | Batch: 006 / 011 | Total loss: 1.698 | Reg loss: 0.043 | Tree loss: 1.698 | Accuracy: 0.416500 | 0.927 sec/iter\n",
      "Epoch: 383 | Batch: 007 / 011 | Total loss: 1.696 | Reg loss: 0.043 | Tree loss: 1.696 | Accuracy: 0.428500 | 0.927 sec/iter\n",
      "Epoch: 383 | Batch: 008 / 011 | Total loss: 1.680 | Reg loss: 0.043 | Tree loss: 1.680 | Accuracy: 0.434500 | 0.927 sec/iter\n",
      "Epoch: 383 | Batch: 009 / 011 | Total loss: 1.683 | Reg loss: 0.043 | Tree loss: 1.683 | Accuracy: 0.440500 | 0.927 sec/iter\n",
      "Epoch: 383 | Batch: 010 / 011 | Total loss: 1.768 | Reg loss: 0.043 | Tree loss: 1.768 | Accuracy: 0.392491 | 0.927 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 384 | Batch: 000 / 011 | Total loss: 1.866 | Reg loss: 0.043 | Tree loss: 1.866 | Accuracy: 0.403500 | 0.927 sec/iter\n",
      "Epoch: 384 | Batch: 001 / 011 | Total loss: 1.828 | Reg loss: 0.043 | Tree loss: 1.828 | Accuracy: 0.414500 | 0.927 sec/iter\n",
      "Epoch: 384 | Batch: 002 / 011 | Total loss: 1.786 | Reg loss: 0.043 | Tree loss: 1.786 | Accuracy: 0.413500 | 0.927 sec/iter\n",
      "Epoch: 384 | Batch: 003 / 011 | Total loss: 1.791 | Reg loss: 0.043 | Tree loss: 1.791 | Accuracy: 0.407500 | 0.927 sec/iter\n",
      "Epoch: 384 | Batch: 004 / 011 | Total loss: 1.739 | Reg loss: 0.043 | Tree loss: 1.739 | Accuracy: 0.423500 | 0.927 sec/iter\n",
      "Epoch: 384 | Batch: 005 / 011 | Total loss: 1.710 | Reg loss: 0.043 | Tree loss: 1.710 | Accuracy: 0.434500 | 0.927 sec/iter\n",
      "Epoch: 384 | Batch: 006 / 011 | Total loss: 1.712 | Reg loss: 0.043 | Tree loss: 1.712 | Accuracy: 0.432000 | 0.927 sec/iter\n",
      "Epoch: 384 | Batch: 007 / 011 | Total loss: 1.697 | Reg loss: 0.043 | Tree loss: 1.697 | Accuracy: 0.402500 | 0.927 sec/iter\n",
      "Epoch: 384 | Batch: 008 / 011 | Total loss: 1.704 | Reg loss: 0.043 | Tree loss: 1.704 | Accuracy: 0.422000 | 0.927 sec/iter\n",
      "Epoch: 384 | Batch: 009 / 011 | Total loss: 1.682 | Reg loss: 0.043 | Tree loss: 1.682 | Accuracy: 0.435500 | 0.927 sec/iter\n",
      "Epoch: 384 | Batch: 010 / 011 | Total loss: 1.624 | Reg loss: 0.043 | Tree loss: 1.624 | Accuracy: 0.443686 | 0.927 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 385 | Batch: 000 / 011 | Total loss: 1.861 | Reg loss: 0.043 | Tree loss: 1.861 | Accuracy: 0.421000 | 0.927 sec/iter\n",
      "Epoch: 385 | Batch: 001 / 011 | Total loss: 1.820 | Reg loss: 0.043 | Tree loss: 1.820 | Accuracy: 0.409500 | 0.927 sec/iter\n",
      "Epoch: 385 | Batch: 002 / 011 | Total loss: 1.804 | Reg loss: 0.043 | Tree loss: 1.804 | Accuracy: 0.416000 | 0.927 sec/iter\n",
      "Epoch: 385 | Batch: 003 / 011 | Total loss: 1.755 | Reg loss: 0.043 | Tree loss: 1.755 | Accuracy: 0.422500 | 0.927 sec/iter\n",
      "Epoch: 385 | Batch: 004 / 011 | Total loss: 1.749 | Reg loss: 0.043 | Tree loss: 1.749 | Accuracy: 0.407000 | 0.927 sec/iter\n",
      "Epoch: 385 | Batch: 005 / 011 | Total loss: 1.720 | Reg loss: 0.043 | Tree loss: 1.720 | Accuracy: 0.427000 | 0.927 sec/iter\n",
      "Epoch: 385 | Batch: 006 / 011 | Total loss: 1.716 | Reg loss: 0.043 | Tree loss: 1.716 | Accuracy: 0.417000 | 0.927 sec/iter\n",
      "Epoch: 385 | Batch: 007 / 011 | Total loss: 1.706 | Reg loss: 0.043 | Tree loss: 1.706 | Accuracy: 0.420500 | 0.927 sec/iter\n",
      "Epoch: 385 | Batch: 008 / 011 | Total loss: 1.649 | Reg loss: 0.043 | Tree loss: 1.649 | Accuracy: 0.439500 | 0.927 sec/iter\n",
      "Epoch: 385 | Batch: 009 / 011 | Total loss: 1.686 | Reg loss: 0.043 | Tree loss: 1.686 | Accuracy: 0.416000 | 0.927 sec/iter\n",
      "Epoch: 385 | Batch: 010 / 011 | Total loss: 1.754 | Reg loss: 0.043 | Tree loss: 1.754 | Accuracy: 0.392491 | 0.927 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 386 | Batch: 000 / 011 | Total loss: 1.860 | Reg loss: 0.043 | Tree loss: 1.860 | Accuracy: 0.409500 | 0.927 sec/iter\n",
      "Epoch: 386 | Batch: 001 / 011 | Total loss: 1.849 | Reg loss: 0.043 | Tree loss: 1.849 | Accuracy: 0.405000 | 0.927 sec/iter\n",
      "Epoch: 386 | Batch: 002 / 011 | Total loss: 1.788 | Reg loss: 0.043 | Tree loss: 1.788 | Accuracy: 0.431500 | 0.927 sec/iter\n",
      "Epoch: 386 | Batch: 003 / 011 | Total loss: 1.762 | Reg loss: 0.043 | Tree loss: 1.762 | Accuracy: 0.427000 | 0.927 sec/iter\n",
      "Epoch: 386 | Batch: 004 / 011 | Total loss: 1.760 | Reg loss: 0.043 | Tree loss: 1.760 | Accuracy: 0.426000 | 0.927 sec/iter\n",
      "Epoch: 386 | Batch: 005 / 011 | Total loss: 1.727 | Reg loss: 0.043 | Tree loss: 1.727 | Accuracy: 0.434000 | 0.927 sec/iter\n",
      "Epoch: 386 | Batch: 006 / 011 | Total loss: 1.711 | Reg loss: 0.043 | Tree loss: 1.711 | Accuracy: 0.415500 | 0.927 sec/iter\n",
      "Epoch: 386 | Batch: 007 / 011 | Total loss: 1.718 | Reg loss: 0.043 | Tree loss: 1.718 | Accuracy: 0.409000 | 0.927 sec/iter\n",
      "Epoch: 386 | Batch: 008 / 011 | Total loss: 1.687 | Reg loss: 0.043 | Tree loss: 1.687 | Accuracy: 0.427500 | 0.927 sec/iter\n",
      "Epoch: 386 | Batch: 009 / 011 | Total loss: 1.645 | Reg loss: 0.043 | Tree loss: 1.645 | Accuracy: 0.438000 | 0.927 sec/iter\n",
      "Epoch: 386 | Batch: 010 / 011 | Total loss: 1.705 | Reg loss: 0.043 | Tree loss: 1.705 | Accuracy: 0.375427 | 0.927 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 387 | Batch: 000 / 011 | Total loss: 1.822 | Reg loss: 0.043 | Tree loss: 1.822 | Accuracy: 0.417000 | 0.927 sec/iter\n",
      "Epoch: 387 | Batch: 001 / 011 | Total loss: 1.803 | Reg loss: 0.043 | Tree loss: 1.803 | Accuracy: 0.412500 | 0.927 sec/iter\n",
      "Epoch: 387 | Batch: 002 / 011 | Total loss: 1.791 | Reg loss: 0.043 | Tree loss: 1.791 | Accuracy: 0.414500 | 0.927 sec/iter\n",
      "Epoch: 387 | Batch: 003 / 011 | Total loss: 1.739 | Reg loss: 0.043 | Tree loss: 1.739 | Accuracy: 0.421000 | 0.927 sec/iter\n",
      "Epoch: 387 | Batch: 004 / 011 | Total loss: 1.771 | Reg loss: 0.043 | Tree loss: 1.771 | Accuracy: 0.395500 | 0.927 sec/iter\n",
      "Epoch: 387 | Batch: 005 / 011 | Total loss: 1.736 | Reg loss: 0.043 | Tree loss: 1.736 | Accuracy: 0.406500 | 0.927 sec/iter\n",
      "Epoch: 387 | Batch: 006 / 011 | Total loss: 1.723 | Reg loss: 0.043 | Tree loss: 1.723 | Accuracy: 0.431000 | 0.927 sec/iter\n",
      "Epoch: 387 | Batch: 007 / 011 | Total loss: 1.663 | Reg loss: 0.043 | Tree loss: 1.663 | Accuracy: 0.426500 | 0.927 sec/iter\n",
      "Epoch: 387 | Batch: 008 / 011 | Total loss: 1.719 | Reg loss: 0.043 | Tree loss: 1.719 | Accuracy: 0.405000 | 0.927 sec/iter\n",
      "Epoch: 387 | Batch: 009 / 011 | Total loss: 1.728 | Reg loss: 0.043 | Tree loss: 1.728 | Accuracy: 0.422500 | 0.927 sec/iter\n",
      "Epoch: 387 | Batch: 010 / 011 | Total loss: 1.748 | Reg loss: 0.043 | Tree loss: 1.748 | Accuracy: 0.392491 | 0.927 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 388 | Batch: 000 / 011 | Total loss: 1.865 | Reg loss: 0.043 | Tree loss: 1.865 | Accuracy: 0.419500 | 0.927 sec/iter\n",
      "Epoch: 388 | Batch: 001 / 011 | Total loss: 1.832 | Reg loss: 0.043 | Tree loss: 1.832 | Accuracy: 0.405500 | 0.927 sec/iter\n",
      "Epoch: 388 | Batch: 002 / 011 | Total loss: 1.797 | Reg loss: 0.043 | Tree loss: 1.797 | Accuracy: 0.418000 | 0.927 sec/iter\n",
      "Epoch: 388 | Batch: 003 / 011 | Total loss: 1.752 | Reg loss: 0.043 | Tree loss: 1.752 | Accuracy: 0.416500 | 0.927 sec/iter\n",
      "Epoch: 388 | Batch: 004 / 011 | Total loss: 1.731 | Reg loss: 0.043 | Tree loss: 1.731 | Accuracy: 0.426000 | 0.927 sec/iter\n",
      "Epoch: 388 | Batch: 005 / 011 | Total loss: 1.726 | Reg loss: 0.043 | Tree loss: 1.726 | Accuracy: 0.418500 | 0.927 sec/iter\n",
      "Epoch: 388 | Batch: 006 / 011 | Total loss: 1.721 | Reg loss: 0.043 | Tree loss: 1.721 | Accuracy: 0.410000 | 0.927 sec/iter\n",
      "Epoch: 388 | Batch: 007 / 011 | Total loss: 1.691 | Reg loss: 0.043 | Tree loss: 1.691 | Accuracy: 0.417000 | 0.927 sec/iter\n",
      "Epoch: 388 | Batch: 008 / 011 | Total loss: 1.692 | Reg loss: 0.043 | Tree loss: 1.692 | Accuracy: 0.424000 | 0.927 sec/iter\n",
      "Epoch: 388 | Batch: 009 / 011 | Total loss: 1.688 | Reg loss: 0.043 | Tree loss: 1.688 | Accuracy: 0.437000 | 0.927 sec/iter\n",
      "Epoch: 388 | Batch: 010 / 011 | Total loss: 1.680 | Reg loss: 0.043 | Tree loss: 1.680 | Accuracy: 0.447099 | 0.927 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 389 | Batch: 000 / 011 | Total loss: 1.873 | Reg loss: 0.043 | Tree loss: 1.873 | Accuracy: 0.405500 | 0.927 sec/iter\n",
      "Epoch: 389 | Batch: 001 / 011 | Total loss: 1.796 | Reg loss: 0.043 | Tree loss: 1.796 | Accuracy: 0.412000 | 0.927 sec/iter\n",
      "Epoch: 389 | Batch: 002 / 011 | Total loss: 1.809 | Reg loss: 0.043 | Tree loss: 1.809 | Accuracy: 0.417000 | 0.927 sec/iter\n",
      "Epoch: 389 | Batch: 003 / 011 | Total loss: 1.769 | Reg loss: 0.043 | Tree loss: 1.769 | Accuracy: 0.428500 | 0.927 sec/iter\n",
      "Epoch: 389 | Batch: 004 / 011 | Total loss: 1.742 | Reg loss: 0.043 | Tree loss: 1.742 | Accuracy: 0.425500 | 0.927 sec/iter\n",
      "Epoch: 389 | Batch: 005 / 011 | Total loss: 1.704 | Reg loss: 0.043 | Tree loss: 1.704 | Accuracy: 0.409500 | 0.927 sec/iter\n",
      "Epoch: 389 | Batch: 006 / 011 | Total loss: 1.724 | Reg loss: 0.043 | Tree loss: 1.724 | Accuracy: 0.411500 | 0.927 sec/iter\n",
      "Epoch: 389 | Batch: 007 / 011 | Total loss: 1.697 | Reg loss: 0.043 | Tree loss: 1.697 | Accuracy: 0.444000 | 0.927 sec/iter\n",
      "Epoch: 389 | Batch: 008 / 011 | Total loss: 1.679 | Reg loss: 0.043 | Tree loss: 1.679 | Accuracy: 0.420000 | 0.927 sec/iter\n",
      "Epoch: 389 | Batch: 009 / 011 | Total loss: 1.696 | Reg loss: 0.043 | Tree loss: 1.696 | Accuracy: 0.413000 | 0.927 sec/iter\n",
      "Epoch: 389 | Batch: 010 / 011 | Total loss: 1.730 | Reg loss: 0.043 | Tree loss: 1.730 | Accuracy: 0.402730 | 0.926 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 390 | Batch: 000 / 011 | Total loss: 1.868 | Reg loss: 0.043 | Tree loss: 1.868 | Accuracy: 0.401000 | 0.927 sec/iter\n",
      "Epoch: 390 | Batch: 001 / 011 | Total loss: 1.833 | Reg loss: 0.043 | Tree loss: 1.833 | Accuracy: 0.422500 | 0.927 sec/iter\n",
      "Epoch: 390 | Batch: 002 / 011 | Total loss: 1.815 | Reg loss: 0.043 | Tree loss: 1.815 | Accuracy: 0.410500 | 0.926 sec/iter\n",
      "Epoch: 390 | Batch: 003 / 011 | Total loss: 1.748 | Reg loss: 0.043 | Tree loss: 1.748 | Accuracy: 0.433000 | 0.926 sec/iter\n",
      "Epoch: 390 | Batch: 004 / 011 | Total loss: 1.725 | Reg loss: 0.043 | Tree loss: 1.725 | Accuracy: 0.441500 | 0.926 sec/iter\n",
      "Epoch: 390 | Batch: 005 / 011 | Total loss: 1.728 | Reg loss: 0.043 | Tree loss: 1.728 | Accuracy: 0.419000 | 0.926 sec/iter\n",
      "Epoch: 390 | Batch: 006 / 011 | Total loss: 1.687 | Reg loss: 0.043 | Tree loss: 1.687 | Accuracy: 0.434000 | 0.926 sec/iter\n",
      "Epoch: 390 | Batch: 007 / 011 | Total loss: 1.709 | Reg loss: 0.043 | Tree loss: 1.709 | Accuracy: 0.401000 | 0.926 sec/iter\n",
      "Epoch: 390 | Batch: 008 / 011 | Total loss: 1.691 | Reg loss: 0.043 | Tree loss: 1.691 | Accuracy: 0.421500 | 0.926 sec/iter\n",
      "Epoch: 390 | Batch: 009 / 011 | Total loss: 1.697 | Reg loss: 0.043 | Tree loss: 1.697 | Accuracy: 0.422500 | 0.926 sec/iter\n",
      "Epoch: 390 | Batch: 010 / 011 | Total loss: 1.702 | Reg loss: 0.043 | Tree loss: 1.702 | Accuracy: 0.395904 | 0.926 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 391 | Batch: 000 / 011 | Total loss: 1.826 | Reg loss: 0.043 | Tree loss: 1.826 | Accuracy: 0.426500 | 0.926 sec/iter\n",
      "Epoch: 391 | Batch: 001 / 011 | Total loss: 1.836 | Reg loss: 0.043 | Tree loss: 1.836 | Accuracy: 0.403000 | 0.926 sec/iter\n",
      "Epoch: 391 | Batch: 002 / 011 | Total loss: 1.807 | Reg loss: 0.043 | Tree loss: 1.807 | Accuracy: 0.411000 | 0.926 sec/iter\n",
      "Epoch: 391 | Batch: 003 / 011 | Total loss: 1.795 | Reg loss: 0.043 | Tree loss: 1.795 | Accuracy: 0.399500 | 0.926 sec/iter\n",
      "Epoch: 391 | Batch: 004 / 011 | Total loss: 1.746 | Reg loss: 0.043 | Tree loss: 1.746 | Accuracy: 0.426500 | 0.926 sec/iter\n",
      "Epoch: 391 | Batch: 005 / 011 | Total loss: 1.688 | Reg loss: 0.043 | Tree loss: 1.688 | Accuracy: 0.434000 | 0.926 sec/iter\n",
      "Epoch: 391 | Batch: 006 / 011 | Total loss: 1.711 | Reg loss: 0.043 | Tree loss: 1.711 | Accuracy: 0.422000 | 0.926 sec/iter\n",
      "Epoch: 391 | Batch: 007 / 011 | Total loss: 1.707 | Reg loss: 0.043 | Tree loss: 1.707 | Accuracy: 0.431000 | 0.926 sec/iter\n",
      "Epoch: 391 | Batch: 008 / 011 | Total loss: 1.683 | Reg loss: 0.043 | Tree loss: 1.683 | Accuracy: 0.417500 | 0.926 sec/iter\n",
      "Epoch: 391 | Batch: 009 / 011 | Total loss: 1.691 | Reg loss: 0.043 | Tree loss: 1.691 | Accuracy: 0.427000 | 0.926 sec/iter\n",
      "Epoch: 391 | Batch: 010 / 011 | Total loss: 1.668 | Reg loss: 0.043 | Tree loss: 1.668 | Accuracy: 0.385666 | 0.926 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 392 | Batch: 000 / 011 | Total loss: 1.848 | Reg loss: 0.043 | Tree loss: 1.848 | Accuracy: 0.420500 | 0.926 sec/iter\n",
      "Epoch: 392 | Batch: 001 / 011 | Total loss: 1.826 | Reg loss: 0.043 | Tree loss: 1.826 | Accuracy: 0.411000 | 0.926 sec/iter\n",
      "Epoch: 392 | Batch: 002 / 011 | Total loss: 1.806 | Reg loss: 0.043 | Tree loss: 1.806 | Accuracy: 0.411000 | 0.926 sec/iter\n",
      "Epoch: 392 | Batch: 003 / 011 | Total loss: 1.739 | Reg loss: 0.043 | Tree loss: 1.739 | Accuracy: 0.429500 | 0.926 sec/iter\n",
      "Epoch: 392 | Batch: 004 / 011 | Total loss: 1.755 | Reg loss: 0.043 | Tree loss: 1.755 | Accuracy: 0.404500 | 0.926 sec/iter\n",
      "Epoch: 392 | Batch: 005 / 011 | Total loss: 1.745 | Reg loss: 0.043 | Tree loss: 1.745 | Accuracy: 0.428500 | 0.926 sec/iter\n",
      "Epoch: 392 | Batch: 006 / 011 | Total loss: 1.707 | Reg loss: 0.043 | Tree loss: 1.707 | Accuracy: 0.413500 | 0.926 sec/iter\n",
      "Epoch: 392 | Batch: 007 / 011 | Total loss: 1.708 | Reg loss: 0.043 | Tree loss: 1.708 | Accuracy: 0.409000 | 0.926 sec/iter\n",
      "Epoch: 392 | Batch: 008 / 011 | Total loss: 1.656 | Reg loss: 0.043 | Tree loss: 1.656 | Accuracy: 0.436000 | 0.926 sec/iter\n",
      "Epoch: 392 | Batch: 009 / 011 | Total loss: 1.693 | Reg loss: 0.043 | Tree loss: 1.693 | Accuracy: 0.425500 | 0.926 sec/iter\n",
      "Epoch: 392 | Batch: 010 / 011 | Total loss: 1.663 | Reg loss: 0.043 | Tree loss: 1.663 | Accuracy: 0.464164 | 0.926 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 393 | Batch: 000 / 011 | Total loss: 1.857 | Reg loss: 0.043 | Tree loss: 1.857 | Accuracy: 0.408500 | 0.926 sec/iter\n",
      "Epoch: 393 | Batch: 001 / 011 | Total loss: 1.850 | Reg loss: 0.043 | Tree loss: 1.850 | Accuracy: 0.418000 | 0.926 sec/iter\n",
      "Epoch: 393 | Batch: 002 / 011 | Total loss: 1.782 | Reg loss: 0.043 | Tree loss: 1.782 | Accuracy: 0.420500 | 0.926 sec/iter\n",
      "Epoch: 393 | Batch: 003 / 011 | Total loss: 1.751 | Reg loss: 0.043 | Tree loss: 1.751 | Accuracy: 0.436000 | 0.926 sec/iter\n",
      "Epoch: 393 | Batch: 004 / 011 | Total loss: 1.718 | Reg loss: 0.043 | Tree loss: 1.718 | Accuracy: 0.438500 | 0.926 sec/iter\n",
      "Epoch: 393 | Batch: 005 / 011 | Total loss: 1.724 | Reg loss: 0.043 | Tree loss: 1.724 | Accuracy: 0.403000 | 0.926 sec/iter\n",
      "Epoch: 393 | Batch: 006 / 011 | Total loss: 1.706 | Reg loss: 0.043 | Tree loss: 1.706 | Accuracy: 0.410000 | 0.926 sec/iter\n",
      "Epoch: 393 | Batch: 007 / 011 | Total loss: 1.689 | Reg loss: 0.043 | Tree loss: 1.689 | Accuracy: 0.431500 | 0.926 sec/iter\n",
      "Epoch: 393 | Batch: 008 / 011 | Total loss: 1.689 | Reg loss: 0.043 | Tree loss: 1.689 | Accuracy: 0.426000 | 0.926 sec/iter\n",
      "Epoch: 393 | Batch: 009 / 011 | Total loss: 1.721 | Reg loss: 0.043 | Tree loss: 1.721 | Accuracy: 0.401000 | 0.926 sec/iter\n",
      "Epoch: 393 | Batch: 010 / 011 | Total loss: 1.722 | Reg loss: 0.043 | Tree loss: 1.722 | Accuracy: 0.409556 | 0.926 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 394 | Batch: 000 / 011 | Total loss: 1.884 | Reg loss: 0.043 | Tree loss: 1.884 | Accuracy: 0.397500 | 0.926 sec/iter\n",
      "Epoch: 394 | Batch: 001 / 011 | Total loss: 1.823 | Reg loss: 0.043 | Tree loss: 1.823 | Accuracy: 0.421500 | 0.926 sec/iter\n",
      "Epoch: 394 | Batch: 002 / 011 | Total loss: 1.806 | Reg loss: 0.043 | Tree loss: 1.806 | Accuracy: 0.402500 | 0.926 sec/iter\n",
      "Epoch: 394 | Batch: 003 / 011 | Total loss: 1.752 | Reg loss: 0.043 | Tree loss: 1.752 | Accuracy: 0.430500 | 0.926 sec/iter\n",
      "Epoch: 394 | Batch: 004 / 011 | Total loss: 1.730 | Reg loss: 0.043 | Tree loss: 1.730 | Accuracy: 0.432500 | 0.926 sec/iter\n",
      "Epoch: 394 | Batch: 005 / 011 | Total loss: 1.746 | Reg loss: 0.043 | Tree loss: 1.746 | Accuracy: 0.405000 | 0.926 sec/iter\n",
      "Epoch: 394 | Batch: 006 / 011 | Total loss: 1.709 | Reg loss: 0.043 | Tree loss: 1.709 | Accuracy: 0.421500 | 0.926 sec/iter\n",
      "Epoch: 394 | Batch: 007 / 011 | Total loss: 1.705 | Reg loss: 0.043 | Tree loss: 1.705 | Accuracy: 0.413500 | 0.926 sec/iter\n",
      "Epoch: 394 | Batch: 008 / 011 | Total loss: 1.661 | Reg loss: 0.043 | Tree loss: 1.661 | Accuracy: 0.432000 | 0.926 sec/iter\n",
      "Epoch: 394 | Batch: 009 / 011 | Total loss: 1.691 | Reg loss: 0.043 | Tree loss: 1.691 | Accuracy: 0.420500 | 0.926 sec/iter\n",
      "Epoch: 394 | Batch: 010 / 011 | Total loss: 1.644 | Reg loss: 0.043 | Tree loss: 1.644 | Accuracy: 0.440273 | 0.926 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 395 | Batch: 000 / 011 | Total loss: 1.832 | Reg loss: 0.043 | Tree loss: 1.832 | Accuracy: 0.437000 | 0.926 sec/iter\n",
      "Epoch: 395 | Batch: 001 / 011 | Total loss: 1.796 | Reg loss: 0.043 | Tree loss: 1.796 | Accuracy: 0.429500 | 0.926 sec/iter\n",
      "Epoch: 395 | Batch: 002 / 011 | Total loss: 1.814 | Reg loss: 0.043 | Tree loss: 1.814 | Accuracy: 0.407000 | 0.926 sec/iter\n",
      "Epoch: 395 | Batch: 003 / 011 | Total loss: 1.799 | Reg loss: 0.043 | Tree loss: 1.799 | Accuracy: 0.406500 | 0.926 sec/iter\n",
      "Epoch: 395 | Batch: 004 / 011 | Total loss: 1.766 | Reg loss: 0.043 | Tree loss: 1.766 | Accuracy: 0.418500 | 0.926 sec/iter\n",
      "Epoch: 395 | Batch: 005 / 011 | Total loss: 1.716 | Reg loss: 0.043 | Tree loss: 1.716 | Accuracy: 0.432000 | 0.926 sec/iter\n",
      "Epoch: 395 | Batch: 006 / 011 | Total loss: 1.698 | Reg loss: 0.043 | Tree loss: 1.698 | Accuracy: 0.428500 | 0.926 sec/iter\n",
      "Epoch: 395 | Batch: 007 / 011 | Total loss: 1.695 | Reg loss: 0.043 | Tree loss: 1.695 | Accuracy: 0.416500 | 0.926 sec/iter\n",
      "Epoch: 395 | Batch: 008 / 011 | Total loss: 1.695 | Reg loss: 0.043 | Tree loss: 1.695 | Accuracy: 0.409000 | 0.926 sec/iter\n",
      "Epoch: 395 | Batch: 009 / 011 | Total loss: 1.680 | Reg loss: 0.043 | Tree loss: 1.680 | Accuracy: 0.419500 | 0.926 sec/iter\n",
      "Epoch: 395 | Batch: 010 / 011 | Total loss: 1.672 | Reg loss: 0.043 | Tree loss: 1.672 | Accuracy: 0.436860 | 0.926 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 396 | Batch: 000 / 011 | Total loss: 1.838 | Reg loss: 0.043 | Tree loss: 1.838 | Accuracy: 0.415500 | 0.926 sec/iter\n",
      "Epoch: 396 | Batch: 001 / 011 | Total loss: 1.810 | Reg loss: 0.043 | Tree loss: 1.810 | Accuracy: 0.430000 | 0.926 sec/iter\n",
      "Epoch: 396 | Batch: 002 / 011 | Total loss: 1.789 | Reg loss: 0.043 | Tree loss: 1.789 | Accuracy: 0.434500 | 0.926 sec/iter\n",
      "Epoch: 396 | Batch: 003 / 011 | Total loss: 1.785 | Reg loss: 0.043 | Tree loss: 1.785 | Accuracy: 0.411000 | 0.926 sec/iter\n",
      "Epoch: 396 | Batch: 004 / 011 | Total loss: 1.741 | Reg loss: 0.043 | Tree loss: 1.741 | Accuracy: 0.429000 | 0.926 sec/iter\n",
      "Epoch: 396 | Batch: 005 / 011 | Total loss: 1.733 | Reg loss: 0.043 | Tree loss: 1.733 | Accuracy: 0.414000 | 0.926 sec/iter\n",
      "Epoch: 396 | Batch: 006 / 011 | Total loss: 1.715 | Reg loss: 0.043 | Tree loss: 1.715 | Accuracy: 0.395000 | 0.926 sec/iter\n",
      "Epoch: 396 | Batch: 007 / 011 | Total loss: 1.711 | Reg loss: 0.043 | Tree loss: 1.711 | Accuracy: 0.405000 | 0.926 sec/iter\n",
      "Epoch: 396 | Batch: 008 / 011 | Total loss: 1.665 | Reg loss: 0.043 | Tree loss: 1.665 | Accuracy: 0.431000 | 0.926 sec/iter\n",
      "Epoch: 396 | Batch: 009 / 011 | Total loss: 1.686 | Reg loss: 0.043 | Tree loss: 1.686 | Accuracy: 0.417000 | 0.926 sec/iter\n",
      "Epoch: 396 | Batch: 010 / 011 | Total loss: 1.694 | Reg loss: 0.043 | Tree loss: 1.694 | Accuracy: 0.426621 | 0.926 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 397 | Batch: 000 / 011 | Total loss: 1.871 | Reg loss: 0.043 | Tree loss: 1.871 | Accuracy: 0.392000 | 0.926 sec/iter\n",
      "Epoch: 397 | Batch: 001 / 011 | Total loss: 1.856 | Reg loss: 0.043 | Tree loss: 1.856 | Accuracy: 0.392000 | 0.926 sec/iter\n",
      "Epoch: 397 | Batch: 002 / 011 | Total loss: 1.801 | Reg loss: 0.043 | Tree loss: 1.801 | Accuracy: 0.422000 | 0.926 sec/iter\n",
      "Epoch: 397 | Batch: 003 / 011 | Total loss: 1.776 | Reg loss: 0.043 | Tree loss: 1.776 | Accuracy: 0.426000 | 0.926 sec/iter\n",
      "Epoch: 397 | Batch: 004 / 011 | Total loss: 1.750 | Reg loss: 0.043 | Tree loss: 1.750 | Accuracy: 0.415000 | 0.926 sec/iter\n",
      "Epoch: 397 | Batch: 005 / 011 | Total loss: 1.697 | Reg loss: 0.043 | Tree loss: 1.697 | Accuracy: 0.441000 | 0.926 sec/iter\n",
      "Epoch: 397 | Batch: 006 / 011 | Total loss: 1.706 | Reg loss: 0.043 | Tree loss: 1.706 | Accuracy: 0.426500 | 0.926 sec/iter\n",
      "Epoch: 397 | Batch: 007 / 011 | Total loss: 1.681 | Reg loss: 0.043 | Tree loss: 1.681 | Accuracy: 0.402500 | 0.926 sec/iter\n",
      "Epoch: 397 | Batch: 008 / 011 | Total loss: 1.678 | Reg loss: 0.043 | Tree loss: 1.678 | Accuracy: 0.429000 | 0.926 sec/iter\n",
      "Epoch: 397 | Batch: 009 / 011 | Total loss: 1.679 | Reg loss: 0.043 | Tree loss: 1.679 | Accuracy: 0.428000 | 0.926 sec/iter\n",
      "Epoch: 397 | Batch: 010 / 011 | Total loss: 1.579 | Reg loss: 0.043 | Tree loss: 1.579 | Accuracy: 0.481229 | 0.926 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 398 | Batch: 000 / 011 | Total loss: 1.887 | Reg loss: 0.043 | Tree loss: 1.887 | Accuracy: 0.392000 | 0.926 sec/iter\n",
      "Epoch: 398 | Batch: 001 / 011 | Total loss: 1.837 | Reg loss: 0.043 | Tree loss: 1.837 | Accuracy: 0.406500 | 0.926 sec/iter\n",
      "Epoch: 398 | Batch: 002 / 011 | Total loss: 1.797 | Reg loss: 0.043 | Tree loss: 1.797 | Accuracy: 0.419500 | 0.926 sec/iter\n",
      "Epoch: 398 | Batch: 003 / 011 | Total loss: 1.752 | Reg loss: 0.043 | Tree loss: 1.752 | Accuracy: 0.427500 | 0.926 sec/iter\n",
      "Epoch: 398 | Batch: 004 / 011 | Total loss: 1.741 | Reg loss: 0.043 | Tree loss: 1.741 | Accuracy: 0.411500 | 0.926 sec/iter\n",
      "Epoch: 398 | Batch: 005 / 011 | Total loss: 1.747 | Reg loss: 0.043 | Tree loss: 1.747 | Accuracy: 0.418000 | 0.926 sec/iter\n",
      "Epoch: 398 | Batch: 006 / 011 | Total loss: 1.714 | Reg loss: 0.043 | Tree loss: 1.714 | Accuracy: 0.410500 | 0.926 sec/iter\n",
      "Epoch: 398 | Batch: 007 / 011 | Total loss: 1.705 | Reg loss: 0.043 | Tree loss: 1.705 | Accuracy: 0.415000 | 0.926 sec/iter\n",
      "Epoch: 398 | Batch: 008 / 011 | Total loss: 1.654 | Reg loss: 0.043 | Tree loss: 1.654 | Accuracy: 0.438000 | 0.926 sec/iter\n",
      "Epoch: 398 | Batch: 009 / 011 | Total loss: 1.655 | Reg loss: 0.043 | Tree loss: 1.655 | Accuracy: 0.450000 | 0.926 sec/iter\n",
      "Epoch: 398 | Batch: 010 / 011 | Total loss: 1.632 | Reg loss: 0.043 | Tree loss: 1.632 | Accuracy: 0.474403 | 0.926 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 399 | Batch: 000 / 011 | Total loss: 1.856 | Reg loss: 0.043 | Tree loss: 1.856 | Accuracy: 0.410000 | 0.926 sec/iter\n",
      "Epoch: 399 | Batch: 001 / 011 | Total loss: 1.844 | Reg loss: 0.043 | Tree loss: 1.844 | Accuracy: 0.388500 | 0.926 sec/iter\n",
      "Epoch: 399 | Batch: 002 / 011 | Total loss: 1.805 | Reg loss: 0.043 | Tree loss: 1.805 | Accuracy: 0.424500 | 0.926 sec/iter\n",
      "Epoch: 399 | Batch: 003 / 011 | Total loss: 1.774 | Reg loss: 0.043 | Tree loss: 1.774 | Accuracy: 0.395000 | 0.926 sec/iter\n",
      "Epoch: 399 | Batch: 004 / 011 | Total loss: 1.747 | Reg loss: 0.043 | Tree loss: 1.747 | Accuracy: 0.423000 | 0.926 sec/iter\n",
      "Epoch: 399 | Batch: 005 / 011 | Total loss: 1.724 | Reg loss: 0.043 | Tree loss: 1.724 | Accuracy: 0.424000 | 0.926 sec/iter\n",
      "Epoch: 399 | Batch: 006 / 011 | Total loss: 1.704 | Reg loss: 0.043 | Tree loss: 1.704 | Accuracy: 0.418000 | 0.926 sec/iter\n",
      "Epoch: 399 | Batch: 007 / 011 | Total loss: 1.701 | Reg loss: 0.043 | Tree loss: 1.701 | Accuracy: 0.407000 | 0.926 sec/iter\n",
      "Epoch: 399 | Batch: 008 / 011 | Total loss: 1.659 | Reg loss: 0.043 | Tree loss: 1.659 | Accuracy: 0.440500 | 0.926 sec/iter\n",
      "Epoch: 399 | Batch: 009 / 011 | Total loss: 1.684 | Reg loss: 0.043 | Tree loss: 1.684 | Accuracy: 0.430500 | 0.926 sec/iter\n",
      "Epoch: 399 | Batch: 010 / 011 | Total loss: 1.621 | Reg loss: 0.043 | Tree loss: 1.621 | Accuracy: 0.460751 | 0.926 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 400 | Batch: 000 / 011 | Total loss: 1.860 | Reg loss: 0.043 | Tree loss: 1.860 | Accuracy: 0.407500 | 0.926 sec/iter\n",
      "Epoch: 400 | Batch: 001 / 011 | Total loss: 1.846 | Reg loss: 0.043 | Tree loss: 1.846 | Accuracy: 0.404000 | 0.926 sec/iter\n",
      "Epoch: 400 | Batch: 002 / 011 | Total loss: 1.762 | Reg loss: 0.043 | Tree loss: 1.762 | Accuracy: 0.421000 | 0.926 sec/iter\n",
      "Epoch: 400 | Batch: 003 / 011 | Total loss: 1.783 | Reg loss: 0.043 | Tree loss: 1.783 | Accuracy: 0.411500 | 0.926 sec/iter\n",
      "Epoch: 400 | Batch: 004 / 011 | Total loss: 1.761 | Reg loss: 0.043 | Tree loss: 1.761 | Accuracy: 0.427000 | 0.926 sec/iter\n",
      "Epoch: 400 | Batch: 005 / 011 | Total loss: 1.713 | Reg loss: 0.043 | Tree loss: 1.713 | Accuracy: 0.425500 | 0.926 sec/iter\n",
      "Epoch: 400 | Batch: 006 / 011 | Total loss: 1.698 | Reg loss: 0.043 | Tree loss: 1.698 | Accuracy: 0.437500 | 0.926 sec/iter\n",
      "Epoch: 400 | Batch: 007 / 011 | Total loss: 1.694 | Reg loss: 0.043 | Tree loss: 1.694 | Accuracy: 0.424500 | 0.926 sec/iter\n",
      "Epoch: 400 | Batch: 008 / 011 | Total loss: 1.681 | Reg loss: 0.043 | Tree loss: 1.681 | Accuracy: 0.415000 | 0.926 sec/iter\n",
      "Epoch: 400 | Batch: 009 / 011 | Total loss: 1.681 | Reg loss: 0.043 | Tree loss: 1.681 | Accuracy: 0.414500 | 0.926 sec/iter\n",
      "Epoch: 400 | Batch: 010 / 011 | Total loss: 1.614 | Reg loss: 0.043 | Tree loss: 1.614 | Accuracy: 0.470990 | 0.925 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 401 | Batch: 000 / 011 | Total loss: 1.851 | Reg loss: 0.043 | Tree loss: 1.851 | Accuracy: 0.410000 | 0.926 sec/iter\n",
      "Epoch: 401 | Batch: 001 / 011 | Total loss: 1.829 | Reg loss: 0.043 | Tree loss: 1.829 | Accuracy: 0.410500 | 0.926 sec/iter\n",
      "Epoch: 401 | Batch: 002 / 011 | Total loss: 1.818 | Reg loss: 0.043 | Tree loss: 1.818 | Accuracy: 0.392000 | 0.925 sec/iter\n",
      "Epoch: 401 | Batch: 003 / 011 | Total loss: 1.761 | Reg loss: 0.043 | Tree loss: 1.761 | Accuracy: 0.428500 | 0.925 sec/iter\n",
      "Epoch: 401 | Batch: 004 / 011 | Total loss: 1.744 | Reg loss: 0.043 | Tree loss: 1.744 | Accuracy: 0.413000 | 0.925 sec/iter\n",
      "Epoch: 401 | Batch: 005 / 011 | Total loss: 1.747 | Reg loss: 0.043 | Tree loss: 1.747 | Accuracy: 0.407000 | 0.925 sec/iter\n",
      "Epoch: 401 | Batch: 006 / 011 | Total loss: 1.678 | Reg loss: 0.043 | Tree loss: 1.678 | Accuracy: 0.440000 | 0.925 sec/iter\n",
      "Epoch: 401 | Batch: 007 / 011 | Total loss: 1.689 | Reg loss: 0.043 | Tree loss: 1.689 | Accuracy: 0.433000 | 0.925 sec/iter\n",
      "Epoch: 401 | Batch: 008 / 011 | Total loss: 1.665 | Reg loss: 0.043 | Tree loss: 1.665 | Accuracy: 0.447000 | 0.925 sec/iter\n",
      "Epoch: 401 | Batch: 009 / 011 | Total loss: 1.680 | Reg loss: 0.043 | Tree loss: 1.680 | Accuracy: 0.425500 | 0.925 sec/iter\n",
      "Epoch: 401 | Batch: 010 / 011 | Total loss: 1.697 | Reg loss: 0.043 | Tree loss: 1.697 | Accuracy: 0.440273 | 0.925 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 402 | Batch: 000 / 011 | Total loss: 1.855 | Reg loss: 0.043 | Tree loss: 1.855 | Accuracy: 0.403000 | 0.925 sec/iter\n",
      "Epoch: 402 | Batch: 001 / 011 | Total loss: 1.837 | Reg loss: 0.043 | Tree loss: 1.837 | Accuracy: 0.416000 | 0.925 sec/iter\n",
      "Epoch: 402 | Batch: 002 / 011 | Total loss: 1.804 | Reg loss: 0.043 | Tree loss: 1.804 | Accuracy: 0.423000 | 0.925 sec/iter\n",
      "Epoch: 402 | Batch: 003 / 011 | Total loss: 1.769 | Reg loss: 0.043 | Tree loss: 1.769 | Accuracy: 0.426500 | 0.925 sec/iter\n",
      "Epoch: 402 | Batch: 004 / 011 | Total loss: 1.734 | Reg loss: 0.043 | Tree loss: 1.734 | Accuracy: 0.416500 | 0.925 sec/iter\n",
      "Epoch: 402 | Batch: 005 / 011 | Total loss: 1.691 | Reg loss: 0.043 | Tree loss: 1.691 | Accuracy: 0.439500 | 0.925 sec/iter\n",
      "Epoch: 402 | Batch: 006 / 011 | Total loss: 1.703 | Reg loss: 0.043 | Tree loss: 1.703 | Accuracy: 0.430000 | 0.925 sec/iter\n",
      "Epoch: 402 | Batch: 007 / 011 | Total loss: 1.709 | Reg loss: 0.043 | Tree loss: 1.709 | Accuracy: 0.408000 | 0.925 sec/iter\n",
      "Epoch: 402 | Batch: 008 / 011 | Total loss: 1.693 | Reg loss: 0.043 | Tree loss: 1.693 | Accuracy: 0.421500 | 0.925 sec/iter\n",
      "Epoch: 402 | Batch: 009 / 011 | Total loss: 1.693 | Reg loss: 0.043 | Tree loss: 1.693 | Accuracy: 0.420000 | 0.925 sec/iter\n",
      "Epoch: 402 | Batch: 010 / 011 | Total loss: 1.633 | Reg loss: 0.043 | Tree loss: 1.633 | Accuracy: 0.412969 | 0.925 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 403 | Batch: 000 / 011 | Total loss: 1.857 | Reg loss: 0.043 | Tree loss: 1.857 | Accuracy: 0.417500 | 0.925 sec/iter\n",
      "Epoch: 403 | Batch: 001 / 011 | Total loss: 1.832 | Reg loss: 0.043 | Tree loss: 1.832 | Accuracy: 0.393000 | 0.925 sec/iter\n",
      "Epoch: 403 | Batch: 002 / 011 | Total loss: 1.801 | Reg loss: 0.043 | Tree loss: 1.801 | Accuracy: 0.440500 | 0.925 sec/iter\n",
      "Epoch: 403 | Batch: 003 / 011 | Total loss: 1.757 | Reg loss: 0.043 | Tree loss: 1.757 | Accuracy: 0.438500 | 0.925 sec/iter\n",
      "Epoch: 403 | Batch: 004 / 011 | Total loss: 1.776 | Reg loss: 0.043 | Tree loss: 1.776 | Accuracy: 0.402500 | 0.925 sec/iter\n",
      "Epoch: 403 | Batch: 005 / 011 | Total loss: 1.718 | Reg loss: 0.043 | Tree loss: 1.718 | Accuracy: 0.410500 | 0.925 sec/iter\n",
      "Epoch: 403 | Batch: 006 / 011 | Total loss: 1.730 | Reg loss: 0.043 | Tree loss: 1.730 | Accuracy: 0.400500 | 0.925 sec/iter\n",
      "Epoch: 403 | Batch: 007 / 011 | Total loss: 1.663 | Reg loss: 0.043 | Tree loss: 1.663 | Accuracy: 0.445000 | 0.925 sec/iter\n",
      "Epoch: 403 | Batch: 008 / 011 | Total loss: 1.685 | Reg loss: 0.043 | Tree loss: 1.685 | Accuracy: 0.424500 | 0.925 sec/iter\n",
      "Epoch: 403 | Batch: 009 / 011 | Total loss: 1.685 | Reg loss: 0.043 | Tree loss: 1.685 | Accuracy: 0.417500 | 0.925 sec/iter\n",
      "Epoch: 403 | Batch: 010 / 011 | Total loss: 1.682 | Reg loss: 0.043 | Tree loss: 1.682 | Accuracy: 0.406143 | 0.925 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 404 | Batch: 000 / 011 | Total loss: 1.856 | Reg loss: 0.043 | Tree loss: 1.856 | Accuracy: 0.419500 | 0.925 sec/iter\n",
      "Epoch: 404 | Batch: 001 / 011 | Total loss: 1.833 | Reg loss: 0.043 | Tree loss: 1.833 | Accuracy: 0.411000 | 0.925 sec/iter\n",
      "Epoch: 404 | Batch: 002 / 011 | Total loss: 1.796 | Reg loss: 0.043 | Tree loss: 1.796 | Accuracy: 0.414000 | 0.925 sec/iter\n",
      "Epoch: 404 | Batch: 003 / 011 | Total loss: 1.779 | Reg loss: 0.043 | Tree loss: 1.779 | Accuracy: 0.428000 | 0.925 sec/iter\n",
      "Epoch: 404 | Batch: 004 / 011 | Total loss: 1.746 | Reg loss: 0.043 | Tree loss: 1.746 | Accuracy: 0.421500 | 0.925 sec/iter\n",
      "Epoch: 404 | Batch: 005 / 011 | Total loss: 1.739 | Reg loss: 0.043 | Tree loss: 1.739 | Accuracy: 0.406500 | 0.925 sec/iter\n",
      "Epoch: 404 | Batch: 006 / 011 | Total loss: 1.686 | Reg loss: 0.043 | Tree loss: 1.686 | Accuracy: 0.428500 | 0.925 sec/iter\n",
      "Epoch: 404 | Batch: 007 / 011 | Total loss: 1.693 | Reg loss: 0.043 | Tree loss: 1.693 | Accuracy: 0.417000 | 0.925 sec/iter\n",
      "Epoch: 404 | Batch: 008 / 011 | Total loss: 1.690 | Reg loss: 0.043 | Tree loss: 1.690 | Accuracy: 0.416000 | 0.925 sec/iter\n",
      "Epoch: 404 | Batch: 009 / 011 | Total loss: 1.675 | Reg loss: 0.043 | Tree loss: 1.675 | Accuracy: 0.424000 | 0.925 sec/iter\n",
      "Epoch: 404 | Batch: 010 / 011 | Total loss: 1.622 | Reg loss: 0.043 | Tree loss: 1.622 | Accuracy: 0.433447 | 0.925 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 405 | Batch: 000 / 011 | Total loss: 1.858 | Reg loss: 0.043 | Tree loss: 1.858 | Accuracy: 0.407000 | 0.925 sec/iter\n",
      "Epoch: 405 | Batch: 001 / 011 | Total loss: 1.828 | Reg loss: 0.043 | Tree loss: 1.828 | Accuracy: 0.404500 | 0.925 sec/iter\n",
      "Epoch: 405 | Batch: 002 / 011 | Total loss: 1.759 | Reg loss: 0.043 | Tree loss: 1.759 | Accuracy: 0.443500 | 0.925 sec/iter\n",
      "Epoch: 405 | Batch: 003 / 011 | Total loss: 1.763 | Reg loss: 0.043 | Tree loss: 1.763 | Accuracy: 0.423500 | 0.925 sec/iter\n",
      "Epoch: 405 | Batch: 004 / 011 | Total loss: 1.739 | Reg loss: 0.043 | Tree loss: 1.739 | Accuracy: 0.419500 | 0.925 sec/iter\n",
      "Epoch: 405 | Batch: 005 / 011 | Total loss: 1.704 | Reg loss: 0.043 | Tree loss: 1.704 | Accuracy: 0.424000 | 0.925 sec/iter\n",
      "Epoch: 405 | Batch: 006 / 011 | Total loss: 1.703 | Reg loss: 0.043 | Tree loss: 1.703 | Accuracy: 0.415000 | 0.925 sec/iter\n",
      "Epoch: 405 | Batch: 007 / 011 | Total loss: 1.715 | Reg loss: 0.043 | Tree loss: 1.715 | Accuracy: 0.413500 | 0.925 sec/iter\n",
      "Epoch: 405 | Batch: 008 / 011 | Total loss: 1.711 | Reg loss: 0.043 | Tree loss: 1.711 | Accuracy: 0.417000 | 0.925 sec/iter\n",
      "Epoch: 405 | Batch: 009 / 011 | Total loss: 1.705 | Reg loss: 0.043 | Tree loss: 1.705 | Accuracy: 0.419500 | 0.925 sec/iter\n",
      "Epoch: 405 | Batch: 010 / 011 | Total loss: 1.695 | Reg loss: 0.043 | Tree loss: 1.695 | Accuracy: 0.423208 | 0.925 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 406 | Batch: 000 / 011 | Total loss: 1.834 | Reg loss: 0.043 | Tree loss: 1.834 | Accuracy: 0.422000 | 0.925 sec/iter\n",
      "Epoch: 406 | Batch: 001 / 011 | Total loss: 1.830 | Reg loss: 0.043 | Tree loss: 1.830 | Accuracy: 0.412000 | 0.925 sec/iter\n",
      "Epoch: 406 | Batch: 002 / 011 | Total loss: 1.814 | Reg loss: 0.043 | Tree loss: 1.814 | Accuracy: 0.418000 | 0.925 sec/iter\n",
      "Epoch: 406 | Batch: 003 / 011 | Total loss: 1.771 | Reg loss: 0.043 | Tree loss: 1.771 | Accuracy: 0.423000 | 0.925 sec/iter\n",
      "Epoch: 406 | Batch: 004 / 011 | Total loss: 1.751 | Reg loss: 0.043 | Tree loss: 1.751 | Accuracy: 0.415000 | 0.925 sec/iter\n",
      "Epoch: 406 | Batch: 005 / 011 | Total loss: 1.724 | Reg loss: 0.043 | Tree loss: 1.724 | Accuracy: 0.413500 | 0.925 sec/iter\n",
      "Epoch: 406 | Batch: 006 / 011 | Total loss: 1.728 | Reg loss: 0.043 | Tree loss: 1.728 | Accuracy: 0.405500 | 0.925 sec/iter\n",
      "Epoch: 406 | Batch: 007 / 011 | Total loss: 1.686 | Reg loss: 0.043 | Tree loss: 1.686 | Accuracy: 0.414000 | 0.925 sec/iter\n",
      "Epoch: 406 | Batch: 008 / 011 | Total loss: 1.678 | Reg loss: 0.043 | Tree loss: 1.678 | Accuracy: 0.432000 | 0.925 sec/iter\n",
      "Epoch: 406 | Batch: 009 / 011 | Total loss: 1.698 | Reg loss: 0.043 | Tree loss: 1.698 | Accuracy: 0.424500 | 0.925 sec/iter\n",
      "Epoch: 406 | Batch: 010 / 011 | Total loss: 1.686 | Reg loss: 0.043 | Tree loss: 1.686 | Accuracy: 0.385666 | 0.925 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 407 | Batch: 000 / 011 | Total loss: 1.852 | Reg loss: 0.043 | Tree loss: 1.852 | Accuracy: 0.412500 | 0.925 sec/iter\n",
      "Epoch: 407 | Batch: 001 / 011 | Total loss: 1.815 | Reg loss: 0.043 | Tree loss: 1.815 | Accuracy: 0.419500 | 0.925 sec/iter\n",
      "Epoch: 407 | Batch: 002 / 011 | Total loss: 1.795 | Reg loss: 0.043 | Tree loss: 1.795 | Accuracy: 0.434000 | 0.925 sec/iter\n",
      "Epoch: 407 | Batch: 003 / 011 | Total loss: 1.793 | Reg loss: 0.043 | Tree loss: 1.793 | Accuracy: 0.401500 | 0.925 sec/iter\n",
      "Epoch: 407 | Batch: 004 / 011 | Total loss: 1.764 | Reg loss: 0.043 | Tree loss: 1.764 | Accuracy: 0.403500 | 0.925 sec/iter\n",
      "Epoch: 407 | Batch: 005 / 011 | Total loss: 1.727 | Reg loss: 0.043 | Tree loss: 1.727 | Accuracy: 0.433000 | 0.925 sec/iter\n",
      "Epoch: 407 | Batch: 006 / 011 | Total loss: 1.701 | Reg loss: 0.043 | Tree loss: 1.701 | Accuracy: 0.418500 | 0.925 sec/iter\n",
      "Epoch: 407 | Batch: 007 / 011 | Total loss: 1.701 | Reg loss: 0.043 | Tree loss: 1.701 | Accuracy: 0.426500 | 0.925 sec/iter\n",
      "Epoch: 407 | Batch: 008 / 011 | Total loss: 1.682 | Reg loss: 0.043 | Tree loss: 1.682 | Accuracy: 0.420000 | 0.925 sec/iter\n",
      "Epoch: 407 | Batch: 009 / 011 | Total loss: 1.668 | Reg loss: 0.043 | Tree loss: 1.668 | Accuracy: 0.418500 | 0.925 sec/iter\n",
      "Epoch: 407 | Batch: 010 / 011 | Total loss: 1.672 | Reg loss: 0.043 | Tree loss: 1.672 | Accuracy: 0.436860 | 0.925 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 408 | Batch: 000 / 011 | Total loss: 1.845 | Reg loss: 0.043 | Tree loss: 1.845 | Accuracy: 0.420000 | 0.925 sec/iter\n",
      "Epoch: 408 | Batch: 001 / 011 | Total loss: 1.833 | Reg loss: 0.043 | Tree loss: 1.833 | Accuracy: 0.406500 | 0.925 sec/iter\n",
      "Epoch: 408 | Batch: 002 / 011 | Total loss: 1.786 | Reg loss: 0.043 | Tree loss: 1.786 | Accuracy: 0.423000 | 0.925 sec/iter\n",
      "Epoch: 408 | Batch: 003 / 011 | Total loss: 1.766 | Reg loss: 0.043 | Tree loss: 1.766 | Accuracy: 0.410500 | 0.925 sec/iter\n",
      "Epoch: 408 | Batch: 004 / 011 | Total loss: 1.746 | Reg loss: 0.043 | Tree loss: 1.746 | Accuracy: 0.430500 | 0.925 sec/iter\n",
      "Epoch: 408 | Batch: 005 / 011 | Total loss: 1.729 | Reg loss: 0.043 | Tree loss: 1.729 | Accuracy: 0.430500 | 0.925 sec/iter\n",
      "Epoch: 408 | Batch: 006 / 011 | Total loss: 1.693 | Reg loss: 0.043 | Tree loss: 1.693 | Accuracy: 0.436000 | 0.925 sec/iter\n",
      "Epoch: 408 | Batch: 007 / 011 | Total loss: 1.684 | Reg loss: 0.043 | Tree loss: 1.684 | Accuracy: 0.427500 | 0.925 sec/iter\n",
      "Epoch: 408 | Batch: 008 / 011 | Total loss: 1.686 | Reg loss: 0.043 | Tree loss: 1.686 | Accuracy: 0.425000 | 0.925 sec/iter\n",
      "Epoch: 408 | Batch: 009 / 011 | Total loss: 1.721 | Reg loss: 0.043 | Tree loss: 1.721 | Accuracy: 0.395500 | 0.925 sec/iter\n",
      "Epoch: 408 | Batch: 010 / 011 | Total loss: 1.675 | Reg loss: 0.043 | Tree loss: 1.675 | Accuracy: 0.402730 | 0.925 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 409 | Batch: 000 / 011 | Total loss: 1.855 | Reg loss: 0.043 | Tree loss: 1.855 | Accuracy: 0.400000 | 0.925 sec/iter\n",
      "Epoch: 409 | Batch: 001 / 011 | Total loss: 1.819 | Reg loss: 0.043 | Tree loss: 1.819 | Accuracy: 0.431000 | 0.925 sec/iter\n",
      "Epoch: 409 | Batch: 002 / 011 | Total loss: 1.794 | Reg loss: 0.043 | Tree loss: 1.794 | Accuracy: 0.430000 | 0.925 sec/iter\n",
      "Epoch: 409 | Batch: 003 / 011 | Total loss: 1.810 | Reg loss: 0.043 | Tree loss: 1.810 | Accuracy: 0.396500 | 0.925 sec/iter\n",
      "Epoch: 409 | Batch: 004 / 011 | Total loss: 1.746 | Reg loss: 0.043 | Tree loss: 1.746 | Accuracy: 0.409500 | 0.925 sec/iter\n",
      "Epoch: 409 | Batch: 005 / 011 | Total loss: 1.741 | Reg loss: 0.043 | Tree loss: 1.741 | Accuracy: 0.414500 | 0.925 sec/iter\n",
      "Epoch: 409 | Batch: 006 / 011 | Total loss: 1.711 | Reg loss: 0.043 | Tree loss: 1.711 | Accuracy: 0.427500 | 0.925 sec/iter\n",
      "Epoch: 409 | Batch: 007 / 011 | Total loss: 1.695 | Reg loss: 0.043 | Tree loss: 1.695 | Accuracy: 0.422000 | 0.925 sec/iter\n",
      "Epoch: 409 | Batch: 008 / 011 | Total loss: 1.677 | Reg loss: 0.043 | Tree loss: 1.677 | Accuracy: 0.426500 | 0.925 sec/iter\n",
      "Epoch: 409 | Batch: 009 / 011 | Total loss: 1.648 | Reg loss: 0.043 | Tree loss: 1.648 | Accuracy: 0.442500 | 0.925 sec/iter\n",
      "Epoch: 409 | Batch: 010 / 011 | Total loss: 1.625 | Reg loss: 0.043 | Tree loss: 1.625 | Accuracy: 0.450512 | 0.925 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 410 | Batch: 000 / 011 | Total loss: 1.851 | Reg loss: 0.043 | Tree loss: 1.851 | Accuracy: 0.400000 | 0.925 sec/iter\n",
      "Epoch: 410 | Batch: 001 / 011 | Total loss: 1.808 | Reg loss: 0.043 | Tree loss: 1.808 | Accuracy: 0.428500 | 0.925 sec/iter\n",
      "Epoch: 410 | Batch: 002 / 011 | Total loss: 1.824 | Reg loss: 0.043 | Tree loss: 1.824 | Accuracy: 0.408500 | 0.925 sec/iter\n",
      "Epoch: 410 | Batch: 003 / 011 | Total loss: 1.758 | Reg loss: 0.043 | Tree loss: 1.758 | Accuracy: 0.414500 | 0.925 sec/iter\n",
      "Epoch: 410 | Batch: 004 / 011 | Total loss: 1.765 | Reg loss: 0.043 | Tree loss: 1.765 | Accuracy: 0.409000 | 0.925 sec/iter\n",
      "Epoch: 410 | Batch: 005 / 011 | Total loss: 1.706 | Reg loss: 0.043 | Tree loss: 1.706 | Accuracy: 0.439500 | 0.925 sec/iter\n",
      "Epoch: 410 | Batch: 006 / 011 | Total loss: 1.688 | Reg loss: 0.043 | Tree loss: 1.688 | Accuracy: 0.418000 | 0.925 sec/iter\n",
      "Epoch: 410 | Batch: 007 / 011 | Total loss: 1.695 | Reg loss: 0.043 | Tree loss: 1.695 | Accuracy: 0.436500 | 0.925 sec/iter\n",
      "Epoch: 410 | Batch: 008 / 011 | Total loss: 1.688 | Reg loss: 0.043 | Tree loss: 1.688 | Accuracy: 0.416000 | 0.925 sec/iter\n",
      "Epoch: 410 | Batch: 009 / 011 | Total loss: 1.688 | Reg loss: 0.043 | Tree loss: 1.688 | Accuracy: 0.429000 | 0.925 sec/iter\n",
      "Epoch: 410 | Batch: 010 / 011 | Total loss: 1.764 | Reg loss: 0.043 | Tree loss: 1.764 | Accuracy: 0.402730 | 0.925 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 411 | Batch: 000 / 011 | Total loss: 1.862 | Reg loss: 0.043 | Tree loss: 1.862 | Accuracy: 0.413500 | 0.925 sec/iter\n",
      "Epoch: 411 | Batch: 001 / 011 | Total loss: 1.835 | Reg loss: 0.043 | Tree loss: 1.835 | Accuracy: 0.424000 | 0.925 sec/iter\n",
      "Epoch: 411 | Batch: 002 / 011 | Total loss: 1.798 | Reg loss: 0.043 | Tree loss: 1.798 | Accuracy: 0.410500 | 0.925 sec/iter\n",
      "Epoch: 411 | Batch: 003 / 011 | Total loss: 1.761 | Reg loss: 0.043 | Tree loss: 1.761 | Accuracy: 0.426000 | 0.925 sec/iter\n",
      "Epoch: 411 | Batch: 004 / 011 | Total loss: 1.756 | Reg loss: 0.043 | Tree loss: 1.756 | Accuracy: 0.418500 | 0.925 sec/iter\n",
      "Epoch: 411 | Batch: 005 / 011 | Total loss: 1.715 | Reg loss: 0.043 | Tree loss: 1.715 | Accuracy: 0.430500 | 0.925 sec/iter\n",
      "Epoch: 411 | Batch: 006 / 011 | Total loss: 1.708 | Reg loss: 0.043 | Tree loss: 1.708 | Accuracy: 0.431500 | 0.925 sec/iter\n",
      "Epoch: 411 | Batch: 007 / 011 | Total loss: 1.721 | Reg loss: 0.043 | Tree loss: 1.721 | Accuracy: 0.397500 | 0.925 sec/iter\n",
      "Epoch: 411 | Batch: 008 / 011 | Total loss: 1.666 | Reg loss: 0.043 | Tree loss: 1.666 | Accuracy: 0.422500 | 0.925 sec/iter\n",
      "Epoch: 411 | Batch: 009 / 011 | Total loss: 1.685 | Reg loss: 0.043 | Tree loss: 1.685 | Accuracy: 0.427500 | 0.925 sec/iter\n",
      "Epoch: 411 | Batch: 010 / 011 | Total loss: 1.630 | Reg loss: 0.043 | Tree loss: 1.630 | Accuracy: 0.443686 | 0.925 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 412 | Batch: 000 / 011 | Total loss: 1.879 | Reg loss: 0.043 | Tree loss: 1.879 | Accuracy: 0.415000 | 0.925 sec/iter\n",
      "Epoch: 412 | Batch: 001 / 011 | Total loss: 1.831 | Reg loss: 0.043 | Tree loss: 1.831 | Accuracy: 0.418000 | 0.925 sec/iter\n",
      "Epoch: 412 | Batch: 002 / 011 | Total loss: 1.816 | Reg loss: 0.043 | Tree loss: 1.816 | Accuracy: 0.403000 | 0.925 sec/iter\n",
      "Epoch: 412 | Batch: 003 / 011 | Total loss: 1.756 | Reg loss: 0.043 | Tree loss: 1.756 | Accuracy: 0.415500 | 0.925 sec/iter\n",
      "Epoch: 412 | Batch: 004 / 011 | Total loss: 1.737 | Reg loss: 0.043 | Tree loss: 1.737 | Accuracy: 0.426000 | 0.924 sec/iter\n",
      "Epoch: 412 | Batch: 005 / 011 | Total loss: 1.740 | Reg loss: 0.043 | Tree loss: 1.740 | Accuracy: 0.418500 | 0.924 sec/iter\n",
      "Epoch: 412 | Batch: 006 / 011 | Total loss: 1.696 | Reg loss: 0.043 | Tree loss: 1.696 | Accuracy: 0.409000 | 0.924 sec/iter\n",
      "Epoch: 412 | Batch: 007 / 011 | Total loss: 1.686 | Reg loss: 0.043 | Tree loss: 1.686 | Accuracy: 0.425000 | 0.924 sec/iter\n",
      "Epoch: 412 | Batch: 008 / 011 | Total loss: 1.672 | Reg loss: 0.043 | Tree loss: 1.672 | Accuracy: 0.424500 | 0.924 sec/iter\n",
      "Epoch: 412 | Batch: 009 / 011 | Total loss: 1.677 | Reg loss: 0.043 | Tree loss: 1.677 | Accuracy: 0.426000 | 0.924 sec/iter\n",
      "Epoch: 412 | Batch: 010 / 011 | Total loss: 1.692 | Reg loss: 0.043 | Tree loss: 1.692 | Accuracy: 0.382253 | 0.924 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 413 | Batch: 000 / 011 | Total loss: 1.833 | Reg loss: 0.043 | Tree loss: 1.833 | Accuracy: 0.433000 | 0.924 sec/iter\n",
      "Epoch: 413 | Batch: 001 / 011 | Total loss: 1.837 | Reg loss: 0.043 | Tree loss: 1.837 | Accuracy: 0.407000 | 0.924 sec/iter\n",
      "Epoch: 413 | Batch: 002 / 011 | Total loss: 1.795 | Reg loss: 0.043 | Tree loss: 1.795 | Accuracy: 0.420000 | 0.924 sec/iter\n",
      "Epoch: 413 | Batch: 003 / 011 | Total loss: 1.766 | Reg loss: 0.043 | Tree loss: 1.766 | Accuracy: 0.410000 | 0.924 sec/iter\n",
      "Epoch: 413 | Batch: 004 / 011 | Total loss: 1.738 | Reg loss: 0.043 | Tree loss: 1.738 | Accuracy: 0.431500 | 0.924 sec/iter\n",
      "Epoch: 413 | Batch: 005 / 011 | Total loss: 1.711 | Reg loss: 0.043 | Tree loss: 1.711 | Accuracy: 0.425000 | 0.924 sec/iter\n",
      "Epoch: 413 | Batch: 006 / 011 | Total loss: 1.698 | Reg loss: 0.043 | Tree loss: 1.698 | Accuracy: 0.431000 | 0.924 sec/iter\n",
      "Epoch: 413 | Batch: 007 / 011 | Total loss: 1.714 | Reg loss: 0.043 | Tree loss: 1.714 | Accuracy: 0.404000 | 0.924 sec/iter\n",
      "Epoch: 413 | Batch: 008 / 011 | Total loss: 1.697 | Reg loss: 0.043 | Tree loss: 1.697 | Accuracy: 0.406000 | 0.924 sec/iter\n",
      "Epoch: 413 | Batch: 009 / 011 | Total loss: 1.685 | Reg loss: 0.043 | Tree loss: 1.685 | Accuracy: 0.426500 | 0.924 sec/iter\n",
      "Epoch: 413 | Batch: 010 / 011 | Total loss: 1.699 | Reg loss: 0.043 | Tree loss: 1.699 | Accuracy: 0.426621 | 0.924 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 414 | Batch: 000 / 011 | Total loss: 1.846 | Reg loss: 0.043 | Tree loss: 1.846 | Accuracy: 0.422000 | 0.924 sec/iter\n",
      "Epoch: 414 | Batch: 001 / 011 | Total loss: 1.826 | Reg loss: 0.043 | Tree loss: 1.826 | Accuracy: 0.389000 | 0.924 sec/iter\n",
      "Epoch: 414 | Batch: 002 / 011 | Total loss: 1.814 | Reg loss: 0.043 | Tree loss: 1.814 | Accuracy: 0.422500 | 0.924 sec/iter\n",
      "Epoch: 414 | Batch: 003 / 011 | Total loss: 1.783 | Reg loss: 0.043 | Tree loss: 1.783 | Accuracy: 0.427000 | 0.924 sec/iter\n",
      "Epoch: 414 | Batch: 004 / 011 | Total loss: 1.761 | Reg loss: 0.043 | Tree loss: 1.761 | Accuracy: 0.423000 | 0.924 sec/iter\n",
      "Epoch: 414 | Batch: 005 / 011 | Total loss: 1.717 | Reg loss: 0.043 | Tree loss: 1.717 | Accuracy: 0.421000 | 0.924 sec/iter\n",
      "Epoch: 414 | Batch: 006 / 011 | Total loss: 1.701 | Reg loss: 0.043 | Tree loss: 1.701 | Accuracy: 0.425500 | 0.924 sec/iter\n",
      "Epoch: 414 | Batch: 007 / 011 | Total loss: 1.703 | Reg loss: 0.043 | Tree loss: 1.703 | Accuracy: 0.413000 | 0.924 sec/iter\n",
      "Epoch: 414 | Batch: 008 / 011 | Total loss: 1.652 | Reg loss: 0.043 | Tree loss: 1.652 | Accuracy: 0.442000 | 0.924 sec/iter\n",
      "Epoch: 414 | Batch: 009 / 011 | Total loss: 1.689 | Reg loss: 0.043 | Tree loss: 1.689 | Accuracy: 0.422000 | 0.924 sec/iter\n",
      "Epoch: 414 | Batch: 010 / 011 | Total loss: 1.674 | Reg loss: 0.043 | Tree loss: 1.674 | Accuracy: 0.426621 | 0.924 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 415 | Batch: 000 / 011 | Total loss: 1.877 | Reg loss: 0.043 | Tree loss: 1.877 | Accuracy: 0.385500 | 0.924 sec/iter\n",
      "Epoch: 415 | Batch: 001 / 011 | Total loss: 1.803 | Reg loss: 0.043 | Tree loss: 1.803 | Accuracy: 0.427500 | 0.924 sec/iter\n",
      "Epoch: 415 | Batch: 002 / 011 | Total loss: 1.792 | Reg loss: 0.043 | Tree loss: 1.792 | Accuracy: 0.418000 | 0.924 sec/iter\n",
      "Epoch: 415 | Batch: 003 / 011 | Total loss: 1.777 | Reg loss: 0.043 | Tree loss: 1.777 | Accuracy: 0.422500 | 0.924 sec/iter\n",
      "Epoch: 415 | Batch: 004 / 011 | Total loss: 1.730 | Reg loss: 0.043 | Tree loss: 1.730 | Accuracy: 0.428500 | 0.924 sec/iter\n",
      "Epoch: 415 | Batch: 005 / 011 | Total loss: 1.711 | Reg loss: 0.043 | Tree loss: 1.711 | Accuracy: 0.423000 | 0.924 sec/iter\n",
      "Epoch: 415 | Batch: 006 / 011 | Total loss: 1.732 | Reg loss: 0.043 | Tree loss: 1.732 | Accuracy: 0.391500 | 0.924 sec/iter\n",
      "Epoch: 415 | Batch: 007 / 011 | Total loss: 1.663 | Reg loss: 0.043 | Tree loss: 1.663 | Accuracy: 0.437000 | 0.924 sec/iter\n",
      "Epoch: 415 | Batch: 008 / 011 | Total loss: 1.696 | Reg loss: 0.043 | Tree loss: 1.696 | Accuracy: 0.424000 | 0.924 sec/iter\n",
      "Epoch: 415 | Batch: 009 / 011 | Total loss: 1.696 | Reg loss: 0.043 | Tree loss: 1.696 | Accuracy: 0.424500 | 0.924 sec/iter\n",
      "Epoch: 415 | Batch: 010 / 011 | Total loss: 1.763 | Reg loss: 0.043 | Tree loss: 1.763 | Accuracy: 0.402730 | 0.924 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 416 | Batch: 000 / 011 | Total loss: 1.849 | Reg loss: 0.043 | Tree loss: 1.849 | Accuracy: 0.425000 | 0.924 sec/iter\n",
      "Epoch: 416 | Batch: 001 / 011 | Total loss: 1.805 | Reg loss: 0.043 | Tree loss: 1.805 | Accuracy: 0.401500 | 0.924 sec/iter\n",
      "Epoch: 416 | Batch: 002 / 011 | Total loss: 1.808 | Reg loss: 0.043 | Tree loss: 1.808 | Accuracy: 0.390500 | 0.924 sec/iter\n",
      "Epoch: 416 | Batch: 003 / 011 | Total loss: 1.796 | Reg loss: 0.043 | Tree loss: 1.796 | Accuracy: 0.397500 | 0.924 sec/iter\n",
      "Epoch: 416 | Batch: 004 / 011 | Total loss: 1.769 | Reg loss: 0.043 | Tree loss: 1.769 | Accuracy: 0.416000 | 0.924 sec/iter\n",
      "Epoch: 416 | Batch: 005 / 011 | Total loss: 1.707 | Reg loss: 0.043 | Tree loss: 1.707 | Accuracy: 0.445500 | 0.924 sec/iter\n",
      "Epoch: 416 | Batch: 006 / 011 | Total loss: 1.722 | Reg loss: 0.043 | Tree loss: 1.722 | Accuracy: 0.412000 | 0.924 sec/iter\n",
      "Epoch: 416 | Batch: 007 / 011 | Total loss: 1.700 | Reg loss: 0.043 | Tree loss: 1.700 | Accuracy: 0.416500 | 0.924 sec/iter\n",
      "Epoch: 416 | Batch: 008 / 011 | Total loss: 1.698 | Reg loss: 0.043 | Tree loss: 1.698 | Accuracy: 0.406500 | 0.924 sec/iter\n",
      "Epoch: 416 | Batch: 009 / 011 | Total loss: 1.659 | Reg loss: 0.043 | Tree loss: 1.659 | Accuracy: 0.433500 | 0.924 sec/iter\n",
      "Epoch: 416 | Batch: 010 / 011 | Total loss: 1.710 | Reg loss: 0.043 | Tree loss: 1.710 | Accuracy: 0.423208 | 0.924 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 417 | Batch: 000 / 011 | Total loss: 1.878 | Reg loss: 0.043 | Tree loss: 1.878 | Accuracy: 0.404500 | 0.924 sec/iter\n",
      "Epoch: 417 | Batch: 001 / 011 | Total loss: 1.839 | Reg loss: 0.043 | Tree loss: 1.839 | Accuracy: 0.404000 | 0.924 sec/iter\n",
      "Epoch: 417 | Batch: 002 / 011 | Total loss: 1.780 | Reg loss: 0.043 | Tree loss: 1.780 | Accuracy: 0.416500 | 0.924 sec/iter\n",
      "Epoch: 417 | Batch: 003 / 011 | Total loss: 1.747 | Reg loss: 0.043 | Tree loss: 1.747 | Accuracy: 0.437500 | 0.924 sec/iter\n",
      "Epoch: 417 | Batch: 004 / 011 | Total loss: 1.743 | Reg loss: 0.043 | Tree loss: 1.743 | Accuracy: 0.411000 | 0.924 sec/iter\n",
      "Epoch: 417 | Batch: 005 / 011 | Total loss: 1.699 | Reg loss: 0.043 | Tree loss: 1.699 | Accuracy: 0.428000 | 0.924 sec/iter\n",
      "Epoch: 417 | Batch: 006 / 011 | Total loss: 1.702 | Reg loss: 0.043 | Tree loss: 1.702 | Accuracy: 0.415500 | 0.924 sec/iter\n",
      "Epoch: 417 | Batch: 007 / 011 | Total loss: 1.727 | Reg loss: 0.043 | Tree loss: 1.727 | Accuracy: 0.411000 | 0.924 sec/iter\n",
      "Epoch: 417 | Batch: 008 / 011 | Total loss: 1.687 | Reg loss: 0.043 | Tree loss: 1.687 | Accuracy: 0.414500 | 0.924 sec/iter\n",
      "Epoch: 417 | Batch: 009 / 011 | Total loss: 1.704 | Reg loss: 0.043 | Tree loss: 1.704 | Accuracy: 0.434000 | 0.924 sec/iter\n",
      "Epoch: 417 | Batch: 010 / 011 | Total loss: 1.617 | Reg loss: 0.043 | Tree loss: 1.617 | Accuracy: 0.436860 | 0.924 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 418 | Batch: 000 / 011 | Total loss: 1.849 | Reg loss: 0.043 | Tree loss: 1.849 | Accuracy: 0.414000 | 0.924 sec/iter\n",
      "Epoch: 418 | Batch: 001 / 011 | Total loss: 1.828 | Reg loss: 0.043 | Tree loss: 1.828 | Accuracy: 0.428500 | 0.924 sec/iter\n",
      "Epoch: 418 | Batch: 002 / 011 | Total loss: 1.786 | Reg loss: 0.043 | Tree loss: 1.786 | Accuracy: 0.431500 | 0.924 sec/iter\n",
      "Epoch: 418 | Batch: 003 / 011 | Total loss: 1.766 | Reg loss: 0.043 | Tree loss: 1.766 | Accuracy: 0.415000 | 0.924 sec/iter\n",
      "Epoch: 418 | Batch: 004 / 011 | Total loss: 1.744 | Reg loss: 0.043 | Tree loss: 1.744 | Accuracy: 0.403000 | 0.924 sec/iter\n",
      "Epoch: 418 | Batch: 005 / 011 | Total loss: 1.726 | Reg loss: 0.043 | Tree loss: 1.726 | Accuracy: 0.419000 | 0.924 sec/iter\n",
      "Epoch: 418 | Batch: 006 / 011 | Total loss: 1.695 | Reg loss: 0.043 | Tree loss: 1.695 | Accuracy: 0.429500 | 0.924 sec/iter\n",
      "Epoch: 418 | Batch: 007 / 011 | Total loss: 1.695 | Reg loss: 0.043 | Tree loss: 1.695 | Accuracy: 0.424500 | 0.924 sec/iter\n",
      "Epoch: 418 | Batch: 008 / 011 | Total loss: 1.713 | Reg loss: 0.043 | Tree loss: 1.713 | Accuracy: 0.412500 | 0.924 sec/iter\n",
      "Epoch: 418 | Batch: 009 / 011 | Total loss: 1.695 | Reg loss: 0.043 | Tree loss: 1.695 | Accuracy: 0.407500 | 0.924 sec/iter\n",
      "Epoch: 418 | Batch: 010 / 011 | Total loss: 1.683 | Reg loss: 0.043 | Tree loss: 1.683 | Accuracy: 0.389078 | 0.924 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 419 | Batch: 000 / 011 | Total loss: 1.849 | Reg loss: 0.043 | Tree loss: 1.849 | Accuracy: 0.415500 | 0.924 sec/iter\n",
      "Epoch: 419 | Batch: 001 / 011 | Total loss: 1.852 | Reg loss: 0.043 | Tree loss: 1.852 | Accuracy: 0.402000 | 0.924 sec/iter\n",
      "Epoch: 419 | Batch: 002 / 011 | Total loss: 1.778 | Reg loss: 0.043 | Tree loss: 1.778 | Accuracy: 0.435000 | 0.924 sec/iter\n",
      "Epoch: 419 | Batch: 003 / 011 | Total loss: 1.758 | Reg loss: 0.043 | Tree loss: 1.758 | Accuracy: 0.423500 | 0.924 sec/iter\n",
      "Epoch: 419 | Batch: 004 / 011 | Total loss: 1.740 | Reg loss: 0.043 | Tree loss: 1.740 | Accuracy: 0.426000 | 0.924 sec/iter\n",
      "Epoch: 419 | Batch: 005 / 011 | Total loss: 1.740 | Reg loss: 0.043 | Tree loss: 1.740 | Accuracy: 0.415500 | 0.924 sec/iter\n",
      "Epoch: 419 | Batch: 006 / 011 | Total loss: 1.673 | Reg loss: 0.043 | Tree loss: 1.673 | Accuracy: 0.438500 | 0.924 sec/iter\n",
      "Epoch: 419 | Batch: 007 / 011 | Total loss: 1.696 | Reg loss: 0.043 | Tree loss: 1.696 | Accuracy: 0.423000 | 0.924 sec/iter\n",
      "Epoch: 419 | Batch: 008 / 011 | Total loss: 1.704 | Reg loss: 0.043 | Tree loss: 1.704 | Accuracy: 0.417500 | 0.924 sec/iter\n",
      "Epoch: 419 | Batch: 009 / 011 | Total loss: 1.679 | Reg loss: 0.043 | Tree loss: 1.679 | Accuracy: 0.407500 | 0.924 sec/iter\n",
      "Epoch: 419 | Batch: 010 / 011 | Total loss: 1.746 | Reg loss: 0.043 | Tree loss: 1.746 | Accuracy: 0.440273 | 0.924 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 420 | Batch: 000 / 011 | Total loss: 1.880 | Reg loss: 0.043 | Tree loss: 1.880 | Accuracy: 0.395000 | 0.924 sec/iter\n",
      "Epoch: 420 | Batch: 001 / 011 | Total loss: 1.831 | Reg loss: 0.043 | Tree loss: 1.831 | Accuracy: 0.428500 | 0.924 sec/iter\n",
      "Epoch: 420 | Batch: 002 / 011 | Total loss: 1.806 | Reg loss: 0.043 | Tree loss: 1.806 | Accuracy: 0.417000 | 0.924 sec/iter\n",
      "Epoch: 420 | Batch: 003 / 011 | Total loss: 1.784 | Reg loss: 0.043 | Tree loss: 1.784 | Accuracy: 0.399500 | 0.924 sec/iter\n",
      "Epoch: 420 | Batch: 004 / 011 | Total loss: 1.741 | Reg loss: 0.043 | Tree loss: 1.741 | Accuracy: 0.420500 | 0.924 sec/iter\n",
      "Epoch: 420 | Batch: 005 / 011 | Total loss: 1.729 | Reg loss: 0.043 | Tree loss: 1.729 | Accuracy: 0.423500 | 0.924 sec/iter\n",
      "Epoch: 420 | Batch: 006 / 011 | Total loss: 1.669 | Reg loss: 0.043 | Tree loss: 1.669 | Accuracy: 0.437500 | 0.924 sec/iter\n",
      "Epoch: 420 | Batch: 007 / 011 | Total loss: 1.687 | Reg loss: 0.043 | Tree loss: 1.687 | Accuracy: 0.422000 | 0.924 sec/iter\n",
      "Epoch: 420 | Batch: 008 / 011 | Total loss: 1.704 | Reg loss: 0.043 | Tree loss: 1.704 | Accuracy: 0.412000 | 0.924 sec/iter\n",
      "Epoch: 420 | Batch: 009 / 011 | Total loss: 1.665 | Reg loss: 0.043 | Tree loss: 1.665 | Accuracy: 0.430000 | 0.924 sec/iter\n",
      "Epoch: 420 | Batch: 010 / 011 | Total loss: 1.677 | Reg loss: 0.043 | Tree loss: 1.677 | Accuracy: 0.436860 | 0.924 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 421 | Batch: 000 / 011 | Total loss: 1.857 | Reg loss: 0.043 | Tree loss: 1.857 | Accuracy: 0.403500 | 0.924 sec/iter\n",
      "Epoch: 421 | Batch: 001 / 011 | Total loss: 1.823 | Reg loss: 0.043 | Tree loss: 1.823 | Accuracy: 0.417500 | 0.924 sec/iter\n",
      "Epoch: 421 | Batch: 002 / 011 | Total loss: 1.794 | Reg loss: 0.043 | Tree loss: 1.794 | Accuracy: 0.423000 | 0.924 sec/iter\n",
      "Epoch: 421 | Batch: 003 / 011 | Total loss: 1.751 | Reg loss: 0.043 | Tree loss: 1.751 | Accuracy: 0.438000 | 0.924 sec/iter\n",
      "Epoch: 421 | Batch: 004 / 011 | Total loss: 1.728 | Reg loss: 0.043 | Tree loss: 1.728 | Accuracy: 0.433500 | 0.924 sec/iter\n",
      "Epoch: 421 | Batch: 005 / 011 | Total loss: 1.713 | Reg loss: 0.043 | Tree loss: 1.713 | Accuracy: 0.420000 | 0.924 sec/iter\n",
      "Epoch: 421 | Batch: 006 / 011 | Total loss: 1.704 | Reg loss: 0.043 | Tree loss: 1.704 | Accuracy: 0.432000 | 0.924 sec/iter\n",
      "Epoch: 421 | Batch: 007 / 011 | Total loss: 1.706 | Reg loss: 0.043 | Tree loss: 1.706 | Accuracy: 0.414500 | 0.924 sec/iter\n",
      "Epoch: 421 | Batch: 008 / 011 | Total loss: 1.707 | Reg loss: 0.043 | Tree loss: 1.707 | Accuracy: 0.420000 | 0.924 sec/iter\n",
      "Epoch: 421 | Batch: 009 / 011 | Total loss: 1.710 | Reg loss: 0.043 | Tree loss: 1.710 | Accuracy: 0.410500 | 0.924 sec/iter\n",
      "Epoch: 421 | Batch: 010 / 011 | Total loss: 1.697 | Reg loss: 0.043 | Tree loss: 1.697 | Accuracy: 0.385666 | 0.924 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 422 | Batch: 000 / 011 | Total loss: 1.894 | Reg loss: 0.043 | Tree loss: 1.894 | Accuracy: 0.404000 | 0.924 sec/iter\n",
      "Epoch: 422 | Batch: 001 / 011 | Total loss: 1.815 | Reg loss: 0.043 | Tree loss: 1.815 | Accuracy: 0.421500 | 0.924 sec/iter\n",
      "Epoch: 422 | Batch: 002 / 011 | Total loss: 1.809 | Reg loss: 0.043 | Tree loss: 1.809 | Accuracy: 0.414000 | 0.924 sec/iter\n",
      "Epoch: 422 | Batch: 003 / 011 | Total loss: 1.751 | Reg loss: 0.043 | Tree loss: 1.751 | Accuracy: 0.420000 | 0.924 sec/iter\n",
      "Epoch: 422 | Batch: 004 / 011 | Total loss: 1.747 | Reg loss: 0.043 | Tree loss: 1.747 | Accuracy: 0.427500 | 0.924 sec/iter\n",
      "Epoch: 422 | Batch: 005 / 011 | Total loss: 1.712 | Reg loss: 0.043 | Tree loss: 1.712 | Accuracy: 0.412500 | 0.924 sec/iter\n",
      "Epoch: 422 | Batch: 006 / 011 | Total loss: 1.718 | Reg loss: 0.043 | Tree loss: 1.718 | Accuracy: 0.414500 | 0.924 sec/iter\n",
      "Epoch: 422 | Batch: 007 / 011 | Total loss: 1.674 | Reg loss: 0.043 | Tree loss: 1.674 | Accuracy: 0.427000 | 0.924 sec/iter\n",
      "Epoch: 422 | Batch: 008 / 011 | Total loss: 1.670 | Reg loss: 0.043 | Tree loss: 1.670 | Accuracy: 0.427000 | 0.924 sec/iter\n",
      "Epoch: 422 | Batch: 009 / 011 | Total loss: 1.666 | Reg loss: 0.043 | Tree loss: 1.666 | Accuracy: 0.434500 | 0.924 sec/iter\n",
      "Epoch: 422 | Batch: 010 / 011 | Total loss: 1.725 | Reg loss: 0.043 | Tree loss: 1.725 | Accuracy: 0.395904 | 0.924 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 423 | Batch: 000 / 011 | Total loss: 1.833 | Reg loss: 0.043 | Tree loss: 1.833 | Accuracy: 0.435500 | 0.924 sec/iter\n",
      "Epoch: 423 | Batch: 001 / 011 | Total loss: 1.854 | Reg loss: 0.043 | Tree loss: 1.854 | Accuracy: 0.419000 | 0.924 sec/iter\n",
      "Epoch: 423 | Batch: 002 / 011 | Total loss: 1.779 | Reg loss: 0.043 | Tree loss: 1.779 | Accuracy: 0.427000 | 0.924 sec/iter\n",
      "Epoch: 423 | Batch: 003 / 011 | Total loss: 1.780 | Reg loss: 0.043 | Tree loss: 1.780 | Accuracy: 0.421000 | 0.924 sec/iter\n",
      "Epoch: 423 | Batch: 004 / 011 | Total loss: 1.724 | Reg loss: 0.043 | Tree loss: 1.724 | Accuracy: 0.416000 | 0.924 sec/iter\n",
      "Epoch: 423 | Batch: 005 / 011 | Total loss: 1.712 | Reg loss: 0.043 | Tree loss: 1.712 | Accuracy: 0.428000 | 0.924 sec/iter\n",
      "Epoch: 423 | Batch: 006 / 011 | Total loss: 1.712 | Reg loss: 0.043 | Tree loss: 1.712 | Accuracy: 0.421500 | 0.924 sec/iter\n",
      "Epoch: 423 | Batch: 007 / 011 | Total loss: 1.671 | Reg loss: 0.043 | Tree loss: 1.671 | Accuracy: 0.429000 | 0.924 sec/iter\n",
      "Epoch: 423 | Batch: 008 / 011 | Total loss: 1.685 | Reg loss: 0.043 | Tree loss: 1.685 | Accuracy: 0.437500 | 0.924 sec/iter\n",
      "Epoch: 423 | Batch: 009 / 011 | Total loss: 1.704 | Reg loss: 0.043 | Tree loss: 1.704 | Accuracy: 0.399500 | 0.924 sec/iter\n",
      "Epoch: 423 | Batch: 010 / 011 | Total loss: 1.714 | Reg loss: 0.043 | Tree loss: 1.714 | Accuracy: 0.365188 | 0.924 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 424 | Batch: 000 / 011 | Total loss: 1.858 | Reg loss: 0.043 | Tree loss: 1.858 | Accuracy: 0.403000 | 0.924 sec/iter\n",
      "Epoch: 424 | Batch: 001 / 011 | Total loss: 1.835 | Reg loss: 0.043 | Tree loss: 1.835 | Accuracy: 0.396500 | 0.924 sec/iter\n",
      "Epoch: 424 | Batch: 002 / 011 | Total loss: 1.769 | Reg loss: 0.043 | Tree loss: 1.769 | Accuracy: 0.426500 | 0.924 sec/iter\n",
      "Epoch: 424 | Batch: 003 / 011 | Total loss: 1.802 | Reg loss: 0.043 | Tree loss: 1.802 | Accuracy: 0.405500 | 0.924 sec/iter\n",
      "Epoch: 424 | Batch: 004 / 011 | Total loss: 1.755 | Reg loss: 0.043 | Tree loss: 1.755 | Accuracy: 0.404500 | 0.923 sec/iter\n",
      "Epoch: 424 | Batch: 005 / 011 | Total loss: 1.731 | Reg loss: 0.043 | Tree loss: 1.731 | Accuracy: 0.406500 | 0.923 sec/iter\n",
      "Epoch: 424 | Batch: 006 / 011 | Total loss: 1.685 | Reg loss: 0.043 | Tree loss: 1.685 | Accuracy: 0.422000 | 0.923 sec/iter\n",
      "Epoch: 424 | Batch: 007 / 011 | Total loss: 1.671 | Reg loss: 0.043 | Tree loss: 1.671 | Accuracy: 0.421000 | 0.923 sec/iter\n",
      "Epoch: 424 | Batch: 008 / 011 | Total loss: 1.679 | Reg loss: 0.043 | Tree loss: 1.679 | Accuracy: 0.435000 | 0.923 sec/iter\n",
      "Epoch: 424 | Batch: 009 / 011 | Total loss: 1.702 | Reg loss: 0.043 | Tree loss: 1.702 | Accuracy: 0.412000 | 0.923 sec/iter\n",
      "Epoch: 424 | Batch: 010 / 011 | Total loss: 1.664 | Reg loss: 0.043 | Tree loss: 1.664 | Accuracy: 0.464164 | 0.923 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 425 | Batch: 000 / 011 | Total loss: 1.837 | Reg loss: 0.043 | Tree loss: 1.837 | Accuracy: 0.426000 | 0.923 sec/iter\n",
      "Epoch: 425 | Batch: 001 / 011 | Total loss: 1.833 | Reg loss: 0.043 | Tree loss: 1.833 | Accuracy: 0.408500 | 0.923 sec/iter\n",
      "Epoch: 425 | Batch: 002 / 011 | Total loss: 1.794 | Reg loss: 0.043 | Tree loss: 1.794 | Accuracy: 0.420500 | 0.923 sec/iter\n",
      "Epoch: 425 | Batch: 003 / 011 | Total loss: 1.753 | Reg loss: 0.043 | Tree loss: 1.753 | Accuracy: 0.416500 | 0.923 sec/iter\n",
      "Epoch: 425 | Batch: 004 / 011 | Total loss: 1.742 | Reg loss: 0.043 | Tree loss: 1.742 | Accuracy: 0.436000 | 0.923 sec/iter\n",
      "Epoch: 425 | Batch: 005 / 011 | Total loss: 1.746 | Reg loss: 0.043 | Tree loss: 1.746 | Accuracy: 0.412000 | 0.923 sec/iter\n",
      "Epoch: 425 | Batch: 006 / 011 | Total loss: 1.720 | Reg loss: 0.043 | Tree loss: 1.720 | Accuracy: 0.417500 | 0.923 sec/iter\n",
      "Epoch: 425 | Batch: 007 / 011 | Total loss: 1.706 | Reg loss: 0.043 | Tree loss: 1.706 | Accuracy: 0.414500 | 0.923 sec/iter\n",
      "Epoch: 425 | Batch: 008 / 011 | Total loss: 1.691 | Reg loss: 0.043 | Tree loss: 1.691 | Accuracy: 0.430500 | 0.923 sec/iter\n",
      "Epoch: 425 | Batch: 009 / 011 | Total loss: 1.670 | Reg loss: 0.043 | Tree loss: 1.670 | Accuracy: 0.435500 | 0.923 sec/iter\n",
      "Epoch: 425 | Batch: 010 / 011 | Total loss: 1.641 | Reg loss: 0.043 | Tree loss: 1.641 | Accuracy: 0.436860 | 0.923 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 426 | Batch: 000 / 011 | Total loss: 1.866 | Reg loss: 0.043 | Tree loss: 1.866 | Accuracy: 0.396500 | 0.923 sec/iter\n",
      "Epoch: 426 | Batch: 001 / 011 | Total loss: 1.821 | Reg loss: 0.043 | Tree loss: 1.821 | Accuracy: 0.409500 | 0.923 sec/iter\n",
      "Epoch: 426 | Batch: 002 / 011 | Total loss: 1.793 | Reg loss: 0.043 | Tree loss: 1.793 | Accuracy: 0.408000 | 0.923 sec/iter\n",
      "Epoch: 426 | Batch: 003 / 011 | Total loss: 1.765 | Reg loss: 0.043 | Tree loss: 1.765 | Accuracy: 0.419500 | 0.923 sec/iter\n",
      "Epoch: 426 | Batch: 004 / 011 | Total loss: 1.761 | Reg loss: 0.043 | Tree loss: 1.761 | Accuracy: 0.427500 | 0.923 sec/iter\n",
      "Epoch: 426 | Batch: 005 / 011 | Total loss: 1.682 | Reg loss: 0.043 | Tree loss: 1.682 | Accuracy: 0.442500 | 0.923 sec/iter\n",
      "Epoch: 426 | Batch: 006 / 011 | Total loss: 1.717 | Reg loss: 0.043 | Tree loss: 1.717 | Accuracy: 0.406000 | 0.923 sec/iter\n",
      "Epoch: 426 | Batch: 007 / 011 | Total loss: 1.696 | Reg loss: 0.043 | Tree loss: 1.696 | Accuracy: 0.412000 | 0.923 sec/iter\n",
      "Epoch: 426 | Batch: 008 / 011 | Total loss: 1.696 | Reg loss: 0.043 | Tree loss: 1.696 | Accuracy: 0.410500 | 0.923 sec/iter\n",
      "Epoch: 426 | Batch: 009 / 011 | Total loss: 1.696 | Reg loss: 0.043 | Tree loss: 1.696 | Accuracy: 0.430000 | 0.923 sec/iter\n",
      "Epoch: 426 | Batch: 010 / 011 | Total loss: 1.666 | Reg loss: 0.043 | Tree loss: 1.666 | Accuracy: 0.464164 | 0.923 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 427 | Batch: 000 / 011 | Total loss: 1.859 | Reg loss: 0.043 | Tree loss: 1.859 | Accuracy: 0.407500 | 0.923 sec/iter\n",
      "Epoch: 427 | Batch: 001 / 011 | Total loss: 1.833 | Reg loss: 0.043 | Tree loss: 1.833 | Accuracy: 0.418000 | 0.923 sec/iter\n",
      "Epoch: 427 | Batch: 002 / 011 | Total loss: 1.792 | Reg loss: 0.043 | Tree loss: 1.792 | Accuracy: 0.418000 | 0.923 sec/iter\n",
      "Epoch: 427 | Batch: 003 / 011 | Total loss: 1.771 | Reg loss: 0.043 | Tree loss: 1.771 | Accuracy: 0.441000 | 0.923 sec/iter\n",
      "Epoch: 427 | Batch: 004 / 011 | Total loss: 1.778 | Reg loss: 0.043 | Tree loss: 1.778 | Accuracy: 0.405000 | 0.923 sec/iter\n",
      "Epoch: 427 | Batch: 005 / 011 | Total loss: 1.711 | Reg loss: 0.043 | Tree loss: 1.711 | Accuracy: 0.425000 | 0.923 sec/iter\n",
      "Epoch: 427 | Batch: 006 / 011 | Total loss: 1.682 | Reg loss: 0.043 | Tree loss: 1.682 | Accuracy: 0.428500 | 0.923 sec/iter\n",
      "Epoch: 427 | Batch: 007 / 011 | Total loss: 1.708 | Reg loss: 0.043 | Tree loss: 1.708 | Accuracy: 0.405000 | 0.923 sec/iter\n",
      "Epoch: 427 | Batch: 008 / 011 | Total loss: 1.683 | Reg loss: 0.043 | Tree loss: 1.683 | Accuracy: 0.432000 | 0.923 sec/iter\n",
      "Epoch: 427 | Batch: 009 / 011 | Total loss: 1.670 | Reg loss: 0.043 | Tree loss: 1.670 | Accuracy: 0.437500 | 0.923 sec/iter\n",
      "Epoch: 427 | Batch: 010 / 011 | Total loss: 1.668 | Reg loss: 0.043 | Tree loss: 1.668 | Accuracy: 0.440273 | 0.923 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 428 | Batch: 000 / 011 | Total loss: 1.858 | Reg loss: 0.043 | Tree loss: 1.858 | Accuracy: 0.410500 | 0.923 sec/iter\n",
      "Epoch: 428 | Batch: 001 / 011 | Total loss: 1.827 | Reg loss: 0.043 | Tree loss: 1.827 | Accuracy: 0.416500 | 0.923 sec/iter\n",
      "Epoch: 428 | Batch: 002 / 011 | Total loss: 1.790 | Reg loss: 0.043 | Tree loss: 1.790 | Accuracy: 0.399500 | 0.923 sec/iter\n",
      "Epoch: 428 | Batch: 003 / 011 | Total loss: 1.788 | Reg loss: 0.043 | Tree loss: 1.788 | Accuracy: 0.417000 | 0.923 sec/iter\n",
      "Epoch: 428 | Batch: 004 / 011 | Total loss: 1.721 | Reg loss: 0.043 | Tree loss: 1.721 | Accuracy: 0.432500 | 0.923 sec/iter\n",
      "Epoch: 428 | Batch: 005 / 011 | Total loss: 1.702 | Reg loss: 0.043 | Tree loss: 1.702 | Accuracy: 0.431500 | 0.923 sec/iter\n",
      "Epoch: 428 | Batch: 006 / 011 | Total loss: 1.688 | Reg loss: 0.043 | Tree loss: 1.688 | Accuracy: 0.430500 | 0.923 sec/iter\n",
      "Epoch: 428 | Batch: 007 / 011 | Total loss: 1.718 | Reg loss: 0.043 | Tree loss: 1.718 | Accuracy: 0.413000 | 0.923 sec/iter\n",
      "Epoch: 428 | Batch: 008 / 011 | Total loss: 1.677 | Reg loss: 0.043 | Tree loss: 1.677 | Accuracy: 0.423500 | 0.923 sec/iter\n",
      "Epoch: 428 | Batch: 009 / 011 | Total loss: 1.727 | Reg loss: 0.043 | Tree loss: 1.727 | Accuracy: 0.410000 | 0.923 sec/iter\n",
      "Epoch: 428 | Batch: 010 / 011 | Total loss: 1.648 | Reg loss: 0.043 | Tree loss: 1.648 | Accuracy: 0.419795 | 0.923 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 429 | Batch: 000 / 011 | Total loss: 1.847 | Reg loss: 0.043 | Tree loss: 1.847 | Accuracy: 0.412500 | 0.923 sec/iter\n",
      "Epoch: 429 | Batch: 001 / 011 | Total loss: 1.814 | Reg loss: 0.043 | Tree loss: 1.814 | Accuracy: 0.417000 | 0.923 sec/iter\n",
      "Epoch: 429 | Batch: 002 / 011 | Total loss: 1.809 | Reg loss: 0.043 | Tree loss: 1.809 | Accuracy: 0.402500 | 0.923 sec/iter\n",
      "Epoch: 429 | Batch: 003 / 011 | Total loss: 1.757 | Reg loss: 0.043 | Tree loss: 1.757 | Accuracy: 0.424000 | 0.923 sec/iter\n",
      "Epoch: 429 | Batch: 004 / 011 | Total loss: 1.725 | Reg loss: 0.043 | Tree loss: 1.725 | Accuracy: 0.447000 | 0.923 sec/iter\n",
      "Epoch: 429 | Batch: 005 / 011 | Total loss: 1.721 | Reg loss: 0.043 | Tree loss: 1.721 | Accuracy: 0.421000 | 0.923 sec/iter\n",
      "Epoch: 429 | Batch: 006 / 011 | Total loss: 1.690 | Reg loss: 0.043 | Tree loss: 1.690 | Accuracy: 0.417000 | 0.923 sec/iter\n",
      "Epoch: 429 | Batch: 007 / 011 | Total loss: 1.711 | Reg loss: 0.043 | Tree loss: 1.711 | Accuracy: 0.418500 | 0.923 sec/iter\n",
      "Epoch: 429 | Batch: 008 / 011 | Total loss: 1.720 | Reg loss: 0.043 | Tree loss: 1.720 | Accuracy: 0.404500 | 0.923 sec/iter\n",
      "Epoch: 429 | Batch: 009 / 011 | Total loss: 1.702 | Reg loss: 0.043 | Tree loss: 1.702 | Accuracy: 0.430500 | 0.923 sec/iter\n",
      "Epoch: 429 | Batch: 010 / 011 | Total loss: 1.615 | Reg loss: 0.043 | Tree loss: 1.615 | Accuracy: 0.447099 | 0.923 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 430 | Batch: 000 / 011 | Total loss: 1.844 | Reg loss: 0.043 | Tree loss: 1.844 | Accuracy: 0.399000 | 0.923 sec/iter\n",
      "Epoch: 430 | Batch: 001 / 011 | Total loss: 1.829 | Reg loss: 0.043 | Tree loss: 1.829 | Accuracy: 0.404000 | 0.923 sec/iter\n",
      "Epoch: 430 | Batch: 002 / 011 | Total loss: 1.803 | Reg loss: 0.043 | Tree loss: 1.803 | Accuracy: 0.426500 | 0.923 sec/iter\n",
      "Epoch: 430 | Batch: 003 / 011 | Total loss: 1.771 | Reg loss: 0.043 | Tree loss: 1.771 | Accuracy: 0.408500 | 0.923 sec/iter\n",
      "Epoch: 430 | Batch: 004 / 011 | Total loss: 1.753 | Reg loss: 0.043 | Tree loss: 1.753 | Accuracy: 0.427500 | 0.923 sec/iter\n",
      "Epoch: 430 | Batch: 005 / 011 | Total loss: 1.726 | Reg loss: 0.043 | Tree loss: 1.726 | Accuracy: 0.421000 | 0.923 sec/iter\n",
      "Epoch: 430 | Batch: 006 / 011 | Total loss: 1.718 | Reg loss: 0.043 | Tree loss: 1.718 | Accuracy: 0.418500 | 0.923 sec/iter\n",
      "Epoch: 430 | Batch: 007 / 011 | Total loss: 1.659 | Reg loss: 0.043 | Tree loss: 1.659 | Accuracy: 0.427500 | 0.923 sec/iter\n",
      "Epoch: 430 | Batch: 008 / 011 | Total loss: 1.707 | Reg loss: 0.043 | Tree loss: 1.707 | Accuracy: 0.408000 | 0.923 sec/iter\n",
      "Epoch: 430 | Batch: 009 / 011 | Total loss: 1.646 | Reg loss: 0.043 | Tree loss: 1.646 | Accuracy: 0.457500 | 0.923 sec/iter\n",
      "Epoch: 430 | Batch: 010 / 011 | Total loss: 1.720 | Reg loss: 0.043 | Tree loss: 1.720 | Accuracy: 0.365188 | 0.923 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 431 | Batch: 000 / 011 | Total loss: 1.865 | Reg loss: 0.043 | Tree loss: 1.865 | Accuracy: 0.404500 | 0.923 sec/iter\n",
      "Epoch: 431 | Batch: 001 / 011 | Total loss: 1.800 | Reg loss: 0.043 | Tree loss: 1.800 | Accuracy: 0.423000 | 0.923 sec/iter\n",
      "Epoch: 431 | Batch: 002 / 011 | Total loss: 1.801 | Reg loss: 0.043 | Tree loss: 1.801 | Accuracy: 0.419000 | 0.923 sec/iter\n",
      "Epoch: 431 | Batch: 003 / 011 | Total loss: 1.780 | Reg loss: 0.043 | Tree loss: 1.780 | Accuracy: 0.421500 | 0.923 sec/iter\n",
      "Epoch: 431 | Batch: 004 / 011 | Total loss: 1.728 | Reg loss: 0.043 | Tree loss: 1.728 | Accuracy: 0.428000 | 0.923 sec/iter\n",
      "Epoch: 431 | Batch: 005 / 011 | Total loss: 1.734 | Reg loss: 0.043 | Tree loss: 1.734 | Accuracy: 0.414000 | 0.923 sec/iter\n",
      "Epoch: 431 | Batch: 006 / 011 | Total loss: 1.731 | Reg loss: 0.043 | Tree loss: 1.731 | Accuracy: 0.391500 | 0.923 sec/iter\n",
      "Epoch: 431 | Batch: 007 / 011 | Total loss: 1.686 | Reg loss: 0.043 | Tree loss: 1.686 | Accuracy: 0.427000 | 0.923 sec/iter\n",
      "Epoch: 431 | Batch: 008 / 011 | Total loss: 1.674 | Reg loss: 0.043 | Tree loss: 1.674 | Accuracy: 0.418000 | 0.923 sec/iter\n",
      "Epoch: 431 | Batch: 009 / 011 | Total loss: 1.684 | Reg loss: 0.043 | Tree loss: 1.684 | Accuracy: 0.432500 | 0.923 sec/iter\n",
      "Epoch: 431 | Batch: 010 / 011 | Total loss: 1.672 | Reg loss: 0.043 | Tree loss: 1.672 | Accuracy: 0.470990 | 0.923 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 432 | Batch: 000 / 011 | Total loss: 1.844 | Reg loss: 0.043 | Tree loss: 1.844 | Accuracy: 0.410500 | 0.923 sec/iter\n",
      "Epoch: 432 | Batch: 001 / 011 | Total loss: 1.820 | Reg loss: 0.043 | Tree loss: 1.820 | Accuracy: 0.422000 | 0.923 sec/iter\n",
      "Epoch: 432 | Batch: 002 / 011 | Total loss: 1.789 | Reg loss: 0.043 | Tree loss: 1.789 | Accuracy: 0.414500 | 0.923 sec/iter\n",
      "Epoch: 432 | Batch: 003 / 011 | Total loss: 1.779 | Reg loss: 0.043 | Tree loss: 1.779 | Accuracy: 0.415500 | 0.923 sec/iter\n",
      "Epoch: 432 | Batch: 004 / 011 | Total loss: 1.748 | Reg loss: 0.043 | Tree loss: 1.748 | Accuracy: 0.423500 | 0.923 sec/iter\n",
      "Epoch: 432 | Batch: 005 / 011 | Total loss: 1.679 | Reg loss: 0.043 | Tree loss: 1.679 | Accuracy: 0.434500 | 0.923 sec/iter\n",
      "Epoch: 432 | Batch: 006 / 011 | Total loss: 1.730 | Reg loss: 0.043 | Tree loss: 1.730 | Accuracy: 0.411000 | 0.923 sec/iter\n",
      "Epoch: 432 | Batch: 007 / 011 | Total loss: 1.697 | Reg loss: 0.043 | Tree loss: 1.697 | Accuracy: 0.402000 | 0.923 sec/iter\n",
      "Epoch: 432 | Batch: 008 / 011 | Total loss: 1.708 | Reg loss: 0.043 | Tree loss: 1.708 | Accuracy: 0.414000 | 0.923 sec/iter\n",
      "Epoch: 432 | Batch: 009 / 011 | Total loss: 1.683 | Reg loss: 0.043 | Tree loss: 1.683 | Accuracy: 0.427500 | 0.923 sec/iter\n",
      "Epoch: 432 | Batch: 010 / 011 | Total loss: 1.715 | Reg loss: 0.043 | Tree loss: 1.715 | Accuracy: 0.409556 | 0.923 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 433 | Batch: 000 / 011 | Total loss: 1.848 | Reg loss: 0.043 | Tree loss: 1.848 | Accuracy: 0.416500 | 0.923 sec/iter\n",
      "Epoch: 433 | Batch: 001 / 011 | Total loss: 1.837 | Reg loss: 0.043 | Tree loss: 1.837 | Accuracy: 0.402000 | 0.923 sec/iter\n",
      "Epoch: 433 | Batch: 002 / 011 | Total loss: 1.814 | Reg loss: 0.043 | Tree loss: 1.814 | Accuracy: 0.399000 | 0.923 sec/iter\n",
      "Epoch: 433 | Batch: 003 / 011 | Total loss: 1.768 | Reg loss: 0.043 | Tree loss: 1.768 | Accuracy: 0.422500 | 0.923 sec/iter\n",
      "Epoch: 433 | Batch: 004 / 011 | Total loss: 1.745 | Reg loss: 0.043 | Tree loss: 1.745 | Accuracy: 0.425500 | 0.923 sec/iter\n",
      "Epoch: 433 | Batch: 005 / 011 | Total loss: 1.721 | Reg loss: 0.043 | Tree loss: 1.721 | Accuracy: 0.417000 | 0.923 sec/iter\n",
      "Epoch: 433 | Batch: 006 / 011 | Total loss: 1.715 | Reg loss: 0.043 | Tree loss: 1.715 | Accuracy: 0.412000 | 0.923 sec/iter\n",
      "Epoch: 433 | Batch: 007 / 011 | Total loss: 1.659 | Reg loss: 0.043 | Tree loss: 1.659 | Accuracy: 0.431000 | 0.923 sec/iter\n",
      "Epoch: 433 | Batch: 008 / 011 | Total loss: 1.698 | Reg loss: 0.043 | Tree loss: 1.698 | Accuracy: 0.414500 | 0.923 sec/iter\n",
      "Epoch: 433 | Batch: 009 / 011 | Total loss: 1.689 | Reg loss: 0.043 | Tree loss: 1.689 | Accuracy: 0.436000 | 0.923 sec/iter\n",
      "Epoch: 433 | Batch: 010 / 011 | Total loss: 1.670 | Reg loss: 0.043 | Tree loss: 1.670 | Accuracy: 0.426621 | 0.923 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 434 | Batch: 000 / 011 | Total loss: 1.858 | Reg loss: 0.043 | Tree loss: 1.858 | Accuracy: 0.404500 | 0.923 sec/iter\n",
      "Epoch: 434 | Batch: 001 / 011 | Total loss: 1.847 | Reg loss: 0.043 | Tree loss: 1.847 | Accuracy: 0.397500 | 0.923 sec/iter\n",
      "Epoch: 434 | Batch: 002 / 011 | Total loss: 1.790 | Reg loss: 0.043 | Tree loss: 1.790 | Accuracy: 0.435000 | 0.923 sec/iter\n",
      "Epoch: 434 | Batch: 003 / 011 | Total loss: 1.759 | Reg loss: 0.043 | Tree loss: 1.759 | Accuracy: 0.419000 | 0.923 sec/iter\n",
      "Epoch: 434 | Batch: 004 / 011 | Total loss: 1.721 | Reg loss: 0.043 | Tree loss: 1.721 | Accuracy: 0.444000 | 0.923 sec/iter\n",
      "Epoch: 434 | Batch: 005 / 011 | Total loss: 1.731 | Reg loss: 0.043 | Tree loss: 1.731 | Accuracy: 0.413500 | 0.923 sec/iter\n",
      "Epoch: 434 | Batch: 006 / 011 | Total loss: 1.717 | Reg loss: 0.043 | Tree loss: 1.717 | Accuracy: 0.404500 | 0.923 sec/iter\n",
      "Epoch: 434 | Batch: 007 / 011 | Total loss: 1.654 | Reg loss: 0.043 | Tree loss: 1.654 | Accuracy: 0.431500 | 0.923 sec/iter\n",
      "Epoch: 434 | Batch: 008 / 011 | Total loss: 1.709 | Reg loss: 0.043 | Tree loss: 1.709 | Accuracy: 0.405500 | 0.923 sec/iter\n",
      "Epoch: 434 | Batch: 009 / 011 | Total loss: 1.699 | Reg loss: 0.043 | Tree loss: 1.699 | Accuracy: 0.420000 | 0.923 sec/iter\n",
      "Epoch: 434 | Batch: 010 / 011 | Total loss: 1.654 | Reg loss: 0.043 | Tree loss: 1.654 | Accuracy: 0.385666 | 0.923 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 435 | Batch: 000 / 011 | Total loss: 1.875 | Reg loss: 0.043 | Tree loss: 1.875 | Accuracy: 0.411500 | 0.923 sec/iter\n",
      "Epoch: 435 | Batch: 001 / 011 | Total loss: 1.804 | Reg loss: 0.043 | Tree loss: 1.804 | Accuracy: 0.420000 | 0.923 sec/iter\n",
      "Epoch: 435 | Batch: 002 / 011 | Total loss: 1.805 | Reg loss: 0.043 | Tree loss: 1.805 | Accuracy: 0.415500 | 0.923 sec/iter\n",
      "Epoch: 435 | Batch: 003 / 011 | Total loss: 1.761 | Reg loss: 0.043 | Tree loss: 1.761 | Accuracy: 0.424500 | 0.923 sec/iter\n",
      "Epoch: 435 | Batch: 004 / 011 | Total loss: 1.725 | Reg loss: 0.043 | Tree loss: 1.725 | Accuracy: 0.419500 | 0.923 sec/iter\n",
      "Epoch: 435 | Batch: 005 / 011 | Total loss: 1.730 | Reg loss: 0.043 | Tree loss: 1.730 | Accuracy: 0.414000 | 0.923 sec/iter\n",
      "Epoch: 435 | Batch: 006 / 011 | Total loss: 1.693 | Reg loss: 0.043 | Tree loss: 1.693 | Accuracy: 0.420500 | 0.923 sec/iter\n",
      "Epoch: 435 | Batch: 007 / 011 | Total loss: 1.687 | Reg loss: 0.043 | Tree loss: 1.687 | Accuracy: 0.421000 | 0.923 sec/iter\n",
      "Epoch: 435 | Batch: 008 / 011 | Total loss: 1.686 | Reg loss: 0.043 | Tree loss: 1.686 | Accuracy: 0.426000 | 0.923 sec/iter\n",
      "Epoch: 435 | Batch: 009 / 011 | Total loss: 1.690 | Reg loss: 0.043 | Tree loss: 1.690 | Accuracy: 0.426500 | 0.923 sec/iter\n",
      "Epoch: 435 | Batch: 010 / 011 | Total loss: 1.676 | Reg loss: 0.043 | Tree loss: 1.676 | Accuracy: 0.426621 | 0.923 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 436 | Batch: 000 / 011 | Total loss: 1.860 | Reg loss: 0.043 | Tree loss: 1.860 | Accuracy: 0.391500 | 0.923 sec/iter\n",
      "Epoch: 436 | Batch: 001 / 011 | Total loss: 1.838 | Reg loss: 0.043 | Tree loss: 1.838 | Accuracy: 0.425000 | 0.923 sec/iter\n",
      "Epoch: 436 | Batch: 002 / 011 | Total loss: 1.774 | Reg loss: 0.043 | Tree loss: 1.774 | Accuracy: 0.428000 | 0.923 sec/iter\n",
      "Epoch: 436 | Batch: 003 / 011 | Total loss: 1.765 | Reg loss: 0.043 | Tree loss: 1.765 | Accuracy: 0.438500 | 0.923 sec/iter\n",
      "Epoch: 436 | Batch: 004 / 011 | Total loss: 1.731 | Reg loss: 0.043 | Tree loss: 1.731 | Accuracy: 0.425000 | 0.923 sec/iter\n",
      "Epoch: 436 | Batch: 005 / 011 | Total loss: 1.701 | Reg loss: 0.043 | Tree loss: 1.701 | Accuracy: 0.419000 | 0.923 sec/iter\n",
      "Epoch: 436 | Batch: 006 / 011 | Total loss: 1.705 | Reg loss: 0.043 | Tree loss: 1.705 | Accuracy: 0.417000 | 0.923 sec/iter\n",
      "Epoch: 436 | Batch: 007 / 011 | Total loss: 1.718 | Reg loss: 0.043 | Tree loss: 1.718 | Accuracy: 0.395000 | 0.923 sec/iter\n",
      "Epoch: 436 | Batch: 008 / 011 | Total loss: 1.684 | Reg loss: 0.043 | Tree loss: 1.684 | Accuracy: 0.422500 | 0.923 sec/iter\n",
      "Epoch: 436 | Batch: 009 / 011 | Total loss: 1.673 | Reg loss: 0.043 | Tree loss: 1.673 | Accuracy: 0.436000 | 0.923 sec/iter\n",
      "Epoch: 436 | Batch: 010 / 011 | Total loss: 1.674 | Reg loss: 0.043 | Tree loss: 1.674 | Accuracy: 0.395904 | 0.922 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 437 | Batch: 000 / 011 | Total loss: 1.840 | Reg loss: 0.043 | Tree loss: 1.840 | Accuracy: 0.414500 | 0.923 sec/iter\n",
      "Epoch: 437 | Batch: 001 / 011 | Total loss: 1.840 | Reg loss: 0.043 | Tree loss: 1.840 | Accuracy: 0.398500 | 0.922 sec/iter\n",
      "Epoch: 437 | Batch: 002 / 011 | Total loss: 1.783 | Reg loss: 0.043 | Tree loss: 1.783 | Accuracy: 0.429000 | 0.922 sec/iter\n",
      "Epoch: 437 | Batch: 003 / 011 | Total loss: 1.793 | Reg loss: 0.043 | Tree loss: 1.793 | Accuracy: 0.416000 | 0.922 sec/iter\n",
      "Epoch: 437 | Batch: 004 / 011 | Total loss: 1.743 | Reg loss: 0.043 | Tree loss: 1.743 | Accuracy: 0.405500 | 0.922 sec/iter\n",
      "Epoch: 437 | Batch: 005 / 011 | Total loss: 1.712 | Reg loss: 0.043 | Tree loss: 1.712 | Accuracy: 0.439500 | 0.922 sec/iter\n",
      "Epoch: 437 | Batch: 006 / 011 | Total loss: 1.685 | Reg loss: 0.043 | Tree loss: 1.685 | Accuracy: 0.427000 | 0.922 sec/iter\n",
      "Epoch: 437 | Batch: 007 / 011 | Total loss: 1.698 | Reg loss: 0.043 | Tree loss: 1.698 | Accuracy: 0.414500 | 0.922 sec/iter\n",
      "Epoch: 437 | Batch: 008 / 011 | Total loss: 1.699 | Reg loss: 0.043 | Tree loss: 1.699 | Accuracy: 0.422000 | 0.922 sec/iter\n",
      "Epoch: 437 | Batch: 009 / 011 | Total loss: 1.694 | Reg loss: 0.043 | Tree loss: 1.694 | Accuracy: 0.413500 | 0.922 sec/iter\n",
      "Epoch: 437 | Batch: 010 / 011 | Total loss: 1.699 | Reg loss: 0.043 | Tree loss: 1.699 | Accuracy: 0.419795 | 0.922 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 438 | Batch: 000 / 011 | Total loss: 1.849 | Reg loss: 0.043 | Tree loss: 1.849 | Accuracy: 0.389000 | 0.922 sec/iter\n",
      "Epoch: 438 | Batch: 001 / 011 | Total loss: 1.833 | Reg loss: 0.043 | Tree loss: 1.833 | Accuracy: 0.433000 | 0.922 sec/iter\n",
      "Epoch: 438 | Batch: 002 / 011 | Total loss: 1.798 | Reg loss: 0.043 | Tree loss: 1.798 | Accuracy: 0.411500 | 0.922 sec/iter\n",
      "Epoch: 438 | Batch: 003 / 011 | Total loss: 1.759 | Reg loss: 0.043 | Tree loss: 1.759 | Accuracy: 0.431000 | 0.922 sec/iter\n",
      "Epoch: 438 | Batch: 004 / 011 | Total loss: 1.732 | Reg loss: 0.043 | Tree loss: 1.732 | Accuracy: 0.414500 | 0.922 sec/iter\n",
      "Epoch: 438 | Batch: 005 / 011 | Total loss: 1.768 | Reg loss: 0.043 | Tree loss: 1.768 | Accuracy: 0.401000 | 0.922 sec/iter\n",
      "Epoch: 438 | Batch: 006 / 011 | Total loss: 1.700 | Reg loss: 0.043 | Tree loss: 1.700 | Accuracy: 0.422500 | 0.922 sec/iter\n",
      "Epoch: 438 | Batch: 007 / 011 | Total loss: 1.674 | Reg loss: 0.043 | Tree loss: 1.674 | Accuracy: 0.439000 | 0.922 sec/iter\n",
      "Epoch: 438 | Batch: 008 / 011 | Total loss: 1.682 | Reg loss: 0.043 | Tree loss: 1.682 | Accuracy: 0.423500 | 0.922 sec/iter\n",
      "Epoch: 438 | Batch: 009 / 011 | Total loss: 1.682 | Reg loss: 0.043 | Tree loss: 1.682 | Accuracy: 0.424000 | 0.922 sec/iter\n",
      "Epoch: 438 | Batch: 010 / 011 | Total loss: 1.738 | Reg loss: 0.043 | Tree loss: 1.738 | Accuracy: 0.409556 | 0.922 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 439 | Batch: 000 / 011 | Total loss: 1.828 | Reg loss: 0.043 | Tree loss: 1.828 | Accuracy: 0.407000 | 0.922 sec/iter\n",
      "Epoch: 439 | Batch: 001 / 011 | Total loss: 1.800 | Reg loss: 0.043 | Tree loss: 1.800 | Accuracy: 0.425500 | 0.922 sec/iter\n",
      "Epoch: 439 | Batch: 002 / 011 | Total loss: 1.790 | Reg loss: 0.043 | Tree loss: 1.790 | Accuracy: 0.424500 | 0.922 sec/iter\n",
      "Epoch: 439 | Batch: 003 / 011 | Total loss: 1.732 | Reg loss: 0.043 | Tree loss: 1.732 | Accuracy: 0.445000 | 0.922 sec/iter\n",
      "Epoch: 439 | Batch: 004 / 011 | Total loss: 1.754 | Reg loss: 0.043 | Tree loss: 1.754 | Accuracy: 0.420500 | 0.922 sec/iter\n",
      "Epoch: 439 | Batch: 005 / 011 | Total loss: 1.708 | Reg loss: 0.043 | Tree loss: 1.708 | Accuracy: 0.422500 | 0.922 sec/iter\n",
      "Epoch: 439 | Batch: 006 / 011 | Total loss: 1.717 | Reg loss: 0.043 | Tree loss: 1.717 | Accuracy: 0.415500 | 0.922 sec/iter\n",
      "Epoch: 439 | Batch: 007 / 011 | Total loss: 1.738 | Reg loss: 0.043 | Tree loss: 1.738 | Accuracy: 0.388000 | 0.922 sec/iter\n",
      "Epoch: 439 | Batch: 008 / 011 | Total loss: 1.687 | Reg loss: 0.043 | Tree loss: 1.687 | Accuracy: 0.418000 | 0.922 sec/iter\n",
      "Epoch: 439 | Batch: 009 / 011 | Total loss: 1.723 | Reg loss: 0.043 | Tree loss: 1.723 | Accuracy: 0.407000 | 0.922 sec/iter\n",
      "Epoch: 439 | Batch: 010 / 011 | Total loss: 1.667 | Reg loss: 0.043 | Tree loss: 1.667 | Accuracy: 0.409556 | 0.922 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 440 | Batch: 000 / 011 | Total loss: 1.806 | Reg loss: 0.043 | Tree loss: 1.806 | Accuracy: 0.439500 | 0.922 sec/iter\n",
      "Epoch: 440 | Batch: 001 / 011 | Total loss: 1.835 | Reg loss: 0.043 | Tree loss: 1.835 | Accuracy: 0.406000 | 0.922 sec/iter\n",
      "Epoch: 440 | Batch: 002 / 011 | Total loss: 1.807 | Reg loss: 0.043 | Tree loss: 1.807 | Accuracy: 0.419500 | 0.922 sec/iter\n",
      "Epoch: 440 | Batch: 003 / 011 | Total loss: 1.782 | Reg loss: 0.043 | Tree loss: 1.782 | Accuracy: 0.406000 | 0.922 sec/iter\n",
      "Epoch: 440 | Batch: 004 / 011 | Total loss: 1.737 | Reg loss: 0.043 | Tree loss: 1.737 | Accuracy: 0.422000 | 0.922 sec/iter\n",
      "Epoch: 440 | Batch: 005 / 011 | Total loss: 1.754 | Reg loss: 0.043 | Tree loss: 1.754 | Accuracy: 0.408500 | 0.922 sec/iter\n",
      "Epoch: 440 | Batch: 006 / 011 | Total loss: 1.716 | Reg loss: 0.043 | Tree loss: 1.716 | Accuracy: 0.410500 | 0.922 sec/iter\n",
      "Epoch: 440 | Batch: 007 / 011 | Total loss: 1.670 | Reg loss: 0.043 | Tree loss: 1.670 | Accuracy: 0.430500 | 0.922 sec/iter\n",
      "Epoch: 440 | Batch: 008 / 011 | Total loss: 1.688 | Reg loss: 0.043 | Tree loss: 1.688 | Accuracy: 0.412000 | 0.922 sec/iter\n",
      "Epoch: 440 | Batch: 009 / 011 | Total loss: 1.682 | Reg loss: 0.043 | Tree loss: 1.682 | Accuracy: 0.425500 | 0.922 sec/iter\n",
      "Epoch: 440 | Batch: 010 / 011 | Total loss: 1.677 | Reg loss: 0.043 | Tree loss: 1.677 | Accuracy: 0.416382 | 0.922 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 441 | Batch: 000 / 011 | Total loss: 1.874 | Reg loss: 0.043 | Tree loss: 1.874 | Accuracy: 0.410500 | 0.922 sec/iter\n",
      "Epoch: 441 | Batch: 001 / 011 | Total loss: 1.842 | Reg loss: 0.043 | Tree loss: 1.842 | Accuracy: 0.402000 | 0.922 sec/iter\n",
      "Epoch: 441 | Batch: 002 / 011 | Total loss: 1.786 | Reg loss: 0.043 | Tree loss: 1.786 | Accuracy: 0.449000 | 0.922 sec/iter\n",
      "Epoch: 441 | Batch: 003 / 011 | Total loss: 1.752 | Reg loss: 0.043 | Tree loss: 1.752 | Accuracy: 0.431500 | 0.922 sec/iter\n",
      "Epoch: 441 | Batch: 004 / 011 | Total loss: 1.747 | Reg loss: 0.043 | Tree loss: 1.747 | Accuracy: 0.413000 | 0.922 sec/iter\n",
      "Epoch: 441 | Batch: 005 / 011 | Total loss: 1.754 | Reg loss: 0.043 | Tree loss: 1.754 | Accuracy: 0.407000 | 0.922 sec/iter\n",
      "Epoch: 441 | Batch: 006 / 011 | Total loss: 1.683 | Reg loss: 0.043 | Tree loss: 1.683 | Accuracy: 0.436000 | 0.922 sec/iter\n",
      "Epoch: 441 | Batch: 007 / 011 | Total loss: 1.667 | Reg loss: 0.043 | Tree loss: 1.667 | Accuracy: 0.432500 | 0.922 sec/iter\n",
      "Epoch: 441 | Batch: 008 / 011 | Total loss: 1.685 | Reg loss: 0.043 | Tree loss: 1.685 | Accuracy: 0.414500 | 0.922 sec/iter\n",
      "Epoch: 441 | Batch: 009 / 011 | Total loss: 1.702 | Reg loss: 0.043 | Tree loss: 1.702 | Accuracy: 0.407000 | 0.922 sec/iter\n",
      "Epoch: 441 | Batch: 010 / 011 | Total loss: 1.669 | Reg loss: 0.043 | Tree loss: 1.669 | Accuracy: 0.385666 | 0.922 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 442 | Batch: 000 / 011 | Total loss: 1.863 | Reg loss: 0.043 | Tree loss: 1.863 | Accuracy: 0.411000 | 0.922 sec/iter\n",
      "Epoch: 442 | Batch: 001 / 011 | Total loss: 1.820 | Reg loss: 0.043 | Tree loss: 1.820 | Accuracy: 0.421000 | 0.922 sec/iter\n",
      "Epoch: 442 | Batch: 002 / 011 | Total loss: 1.790 | Reg loss: 0.043 | Tree loss: 1.790 | Accuracy: 0.423000 | 0.922 sec/iter\n",
      "Epoch: 442 | Batch: 003 / 011 | Total loss: 1.767 | Reg loss: 0.043 | Tree loss: 1.767 | Accuracy: 0.414000 | 0.922 sec/iter\n",
      "Epoch: 442 | Batch: 004 / 011 | Total loss: 1.712 | Reg loss: 0.043 | Tree loss: 1.712 | Accuracy: 0.454500 | 0.922 sec/iter\n",
      "Epoch: 442 | Batch: 005 / 011 | Total loss: 1.755 | Reg loss: 0.043 | Tree loss: 1.755 | Accuracy: 0.395500 | 0.922 sec/iter\n",
      "Epoch: 442 | Batch: 006 / 011 | Total loss: 1.706 | Reg loss: 0.043 | Tree loss: 1.706 | Accuracy: 0.414000 | 0.922 sec/iter\n",
      "Epoch: 442 | Batch: 007 / 011 | Total loss: 1.703 | Reg loss: 0.043 | Tree loss: 1.703 | Accuracy: 0.408500 | 0.922 sec/iter\n",
      "Epoch: 442 | Batch: 008 / 011 | Total loss: 1.668 | Reg loss: 0.043 | Tree loss: 1.668 | Accuracy: 0.418000 | 0.922 sec/iter\n",
      "Epoch: 442 | Batch: 009 / 011 | Total loss: 1.693 | Reg loss: 0.043 | Tree loss: 1.693 | Accuracy: 0.414000 | 0.922 sec/iter\n",
      "Epoch: 442 | Batch: 010 / 011 | Total loss: 1.665 | Reg loss: 0.043 | Tree loss: 1.665 | Accuracy: 0.433447 | 0.922 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 443 | Batch: 000 / 011 | Total loss: 1.843 | Reg loss: 0.043 | Tree loss: 1.843 | Accuracy: 0.420000 | 0.922 sec/iter\n",
      "Epoch: 443 | Batch: 001 / 011 | Total loss: 1.838 | Reg loss: 0.043 | Tree loss: 1.838 | Accuracy: 0.408000 | 0.922 sec/iter\n",
      "Epoch: 443 | Batch: 002 / 011 | Total loss: 1.804 | Reg loss: 0.043 | Tree loss: 1.804 | Accuracy: 0.414500 | 0.922 sec/iter\n",
      "Epoch: 443 | Batch: 003 / 011 | Total loss: 1.742 | Reg loss: 0.043 | Tree loss: 1.742 | Accuracy: 0.437500 | 0.922 sec/iter\n",
      "Epoch: 443 | Batch: 004 / 011 | Total loss: 1.722 | Reg loss: 0.043 | Tree loss: 1.722 | Accuracy: 0.435000 | 0.922 sec/iter\n",
      "Epoch: 443 | Batch: 005 / 011 | Total loss: 1.737 | Reg loss: 0.043 | Tree loss: 1.737 | Accuracy: 0.410500 | 0.922 sec/iter\n",
      "Epoch: 443 | Batch: 006 / 011 | Total loss: 1.719 | Reg loss: 0.043 | Tree loss: 1.719 | Accuracy: 0.414000 | 0.922 sec/iter\n",
      "Epoch: 443 | Batch: 007 / 011 | Total loss: 1.698 | Reg loss: 0.043 | Tree loss: 1.698 | Accuracy: 0.418000 | 0.922 sec/iter\n",
      "Epoch: 443 | Batch: 008 / 011 | Total loss: 1.687 | Reg loss: 0.043 | Tree loss: 1.687 | Accuracy: 0.421000 | 0.922 sec/iter\n",
      "Epoch: 443 | Batch: 009 / 011 | Total loss: 1.688 | Reg loss: 0.043 | Tree loss: 1.688 | Accuracy: 0.407500 | 0.922 sec/iter\n",
      "Epoch: 443 | Batch: 010 / 011 | Total loss: 1.652 | Reg loss: 0.043 | Tree loss: 1.652 | Accuracy: 0.443686 | 0.922 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 444 | Batch: 000 / 011 | Total loss: 1.853 | Reg loss: 0.043 | Tree loss: 1.853 | Accuracy: 0.405500 | 0.922 sec/iter\n",
      "Epoch: 444 | Batch: 001 / 011 | Total loss: 1.830 | Reg loss: 0.043 | Tree loss: 1.830 | Accuracy: 0.412500 | 0.922 sec/iter\n",
      "Epoch: 444 | Batch: 002 / 011 | Total loss: 1.803 | Reg loss: 0.043 | Tree loss: 1.803 | Accuracy: 0.414000 | 0.922 sec/iter\n",
      "Epoch: 444 | Batch: 003 / 011 | Total loss: 1.785 | Reg loss: 0.043 | Tree loss: 1.785 | Accuracy: 0.413000 | 0.922 sec/iter\n",
      "Epoch: 444 | Batch: 004 / 011 | Total loss: 1.739 | Reg loss: 0.043 | Tree loss: 1.739 | Accuracy: 0.420500 | 0.922 sec/iter\n",
      "Epoch: 444 | Batch: 005 / 011 | Total loss: 1.739 | Reg loss: 0.043 | Tree loss: 1.739 | Accuracy: 0.423000 | 0.922 sec/iter\n",
      "Epoch: 444 | Batch: 006 / 011 | Total loss: 1.694 | Reg loss: 0.043 | Tree loss: 1.694 | Accuracy: 0.424500 | 0.922 sec/iter\n",
      "Epoch: 444 | Batch: 007 / 011 | Total loss: 1.682 | Reg loss: 0.043 | Tree loss: 1.682 | Accuracy: 0.424500 | 0.922 sec/iter\n",
      "Epoch: 444 | Batch: 008 / 011 | Total loss: 1.685 | Reg loss: 0.043 | Tree loss: 1.685 | Accuracy: 0.410000 | 0.922 sec/iter\n",
      "Epoch: 444 | Batch: 009 / 011 | Total loss: 1.674 | Reg loss: 0.043 | Tree loss: 1.674 | Accuracy: 0.424000 | 0.922 sec/iter\n",
      "Epoch: 444 | Batch: 010 / 011 | Total loss: 1.663 | Reg loss: 0.043 | Tree loss: 1.663 | Accuracy: 0.440273 | 0.922 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 445 | Batch: 000 / 011 | Total loss: 1.862 | Reg loss: 0.043 | Tree loss: 1.862 | Accuracy: 0.418500 | 0.922 sec/iter\n",
      "Epoch: 445 | Batch: 001 / 011 | Total loss: 1.804 | Reg loss: 0.043 | Tree loss: 1.804 | Accuracy: 0.432000 | 0.922 sec/iter\n",
      "Epoch: 445 | Batch: 002 / 011 | Total loss: 1.780 | Reg loss: 0.043 | Tree loss: 1.780 | Accuracy: 0.425500 | 0.922 sec/iter\n",
      "Epoch: 445 | Batch: 003 / 011 | Total loss: 1.775 | Reg loss: 0.043 | Tree loss: 1.775 | Accuracy: 0.415500 | 0.922 sec/iter\n",
      "Epoch: 445 | Batch: 004 / 011 | Total loss: 1.735 | Reg loss: 0.043 | Tree loss: 1.735 | Accuracy: 0.421500 | 0.922 sec/iter\n",
      "Epoch: 445 | Batch: 005 / 011 | Total loss: 1.702 | Reg loss: 0.043 | Tree loss: 1.702 | Accuracy: 0.429000 | 0.922 sec/iter\n",
      "Epoch: 445 | Batch: 006 / 011 | Total loss: 1.722 | Reg loss: 0.043 | Tree loss: 1.722 | Accuracy: 0.406000 | 0.922 sec/iter\n",
      "Epoch: 445 | Batch: 007 / 011 | Total loss: 1.694 | Reg loss: 0.043 | Tree loss: 1.694 | Accuracy: 0.418000 | 0.922 sec/iter\n",
      "Epoch: 445 | Batch: 008 / 011 | Total loss: 1.708 | Reg loss: 0.043 | Tree loss: 1.708 | Accuracy: 0.413500 | 0.922 sec/iter\n",
      "Epoch: 445 | Batch: 009 / 011 | Total loss: 1.678 | Reg loss: 0.043 | Tree loss: 1.678 | Accuracy: 0.424500 | 0.922 sec/iter\n",
      "Epoch: 445 | Batch: 010 / 011 | Total loss: 1.672 | Reg loss: 0.043 | Tree loss: 1.672 | Accuracy: 0.402730 | 0.922 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 446 | Batch: 000 / 011 | Total loss: 1.850 | Reg loss: 0.043 | Tree loss: 1.850 | Accuracy: 0.420000 | 0.922 sec/iter\n",
      "Epoch: 446 | Batch: 001 / 011 | Total loss: 1.822 | Reg loss: 0.043 | Tree loss: 1.822 | Accuracy: 0.424000 | 0.922 sec/iter\n",
      "Epoch: 446 | Batch: 002 / 011 | Total loss: 1.777 | Reg loss: 0.043 | Tree loss: 1.777 | Accuracy: 0.421500 | 0.922 sec/iter\n",
      "Epoch: 446 | Batch: 003 / 011 | Total loss: 1.753 | Reg loss: 0.043 | Tree loss: 1.753 | Accuracy: 0.420000 | 0.922 sec/iter\n",
      "Epoch: 446 | Batch: 004 / 011 | Total loss: 1.740 | Reg loss: 0.043 | Tree loss: 1.740 | Accuracy: 0.415000 | 0.922 sec/iter\n",
      "Epoch: 446 | Batch: 005 / 011 | Total loss: 1.725 | Reg loss: 0.043 | Tree loss: 1.725 | Accuracy: 0.422500 | 0.922 sec/iter\n",
      "Epoch: 446 | Batch: 006 / 011 | Total loss: 1.695 | Reg loss: 0.043 | Tree loss: 1.695 | Accuracy: 0.432500 | 0.922 sec/iter\n",
      "Epoch: 446 | Batch: 007 / 011 | Total loss: 1.730 | Reg loss: 0.043 | Tree loss: 1.730 | Accuracy: 0.393000 | 0.922 sec/iter\n",
      "Epoch: 446 | Batch: 008 / 011 | Total loss: 1.687 | Reg loss: 0.043 | Tree loss: 1.687 | Accuracy: 0.415000 | 0.922 sec/iter\n",
      "Epoch: 446 | Batch: 009 / 011 | Total loss: 1.683 | Reg loss: 0.043 | Tree loss: 1.683 | Accuracy: 0.432500 | 0.922 sec/iter\n",
      "Epoch: 446 | Batch: 010 / 011 | Total loss: 1.742 | Reg loss: 0.043 | Tree loss: 1.742 | Accuracy: 0.358362 | 0.922 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 447 | Batch: 000 / 011 | Total loss: 1.845 | Reg loss: 0.043 | Tree loss: 1.845 | Accuracy: 0.404000 | 0.922 sec/iter\n",
      "Epoch: 447 | Batch: 001 / 011 | Total loss: 1.826 | Reg loss: 0.043 | Tree loss: 1.826 | Accuracy: 0.432500 | 0.922 sec/iter\n",
      "Epoch: 447 | Batch: 002 / 011 | Total loss: 1.804 | Reg loss: 0.043 | Tree loss: 1.804 | Accuracy: 0.412500 | 0.922 sec/iter\n",
      "Epoch: 447 | Batch: 003 / 011 | Total loss: 1.780 | Reg loss: 0.043 | Tree loss: 1.780 | Accuracy: 0.404500 | 0.922 sec/iter\n",
      "Epoch: 447 | Batch: 004 / 011 | Total loss: 1.739 | Reg loss: 0.043 | Tree loss: 1.739 | Accuracy: 0.402000 | 0.922 sec/iter\n",
      "Epoch: 447 | Batch: 005 / 011 | Total loss: 1.716 | Reg loss: 0.043 | Tree loss: 1.716 | Accuracy: 0.435500 | 0.922 sec/iter\n",
      "Epoch: 447 | Batch: 006 / 011 | Total loss: 1.688 | Reg loss: 0.043 | Tree loss: 1.688 | Accuracy: 0.440500 | 0.922 sec/iter\n",
      "Epoch: 447 | Batch: 007 / 011 | Total loss: 1.695 | Reg loss: 0.043 | Tree loss: 1.695 | Accuracy: 0.426000 | 0.922 sec/iter\n",
      "Epoch: 447 | Batch: 008 / 011 | Total loss: 1.700 | Reg loss: 0.043 | Tree loss: 1.700 | Accuracy: 0.417000 | 0.922 sec/iter\n",
      "Epoch: 447 | Batch: 009 / 011 | Total loss: 1.666 | Reg loss: 0.043 | Tree loss: 1.666 | Accuracy: 0.426000 | 0.922 sec/iter\n",
      "Epoch: 447 | Batch: 010 / 011 | Total loss: 1.667 | Reg loss: 0.043 | Tree loss: 1.667 | Accuracy: 0.419795 | 0.922 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 448 | Batch: 000 / 011 | Total loss: 1.853 | Reg loss: 0.043 | Tree loss: 1.853 | Accuracy: 0.412000 | 0.922 sec/iter\n",
      "Epoch: 448 | Batch: 001 / 011 | Total loss: 1.835 | Reg loss: 0.043 | Tree loss: 1.835 | Accuracy: 0.403500 | 0.922 sec/iter\n",
      "Epoch: 448 | Batch: 002 / 011 | Total loss: 1.781 | Reg loss: 0.043 | Tree loss: 1.781 | Accuracy: 0.423500 | 0.922 sec/iter\n",
      "Epoch: 448 | Batch: 003 / 011 | Total loss: 1.771 | Reg loss: 0.043 | Tree loss: 1.771 | Accuracy: 0.415500 | 0.922 sec/iter\n",
      "Epoch: 448 | Batch: 004 / 011 | Total loss: 1.748 | Reg loss: 0.043 | Tree loss: 1.748 | Accuracy: 0.423500 | 0.922 sec/iter\n",
      "Epoch: 448 | Batch: 005 / 011 | Total loss: 1.713 | Reg loss: 0.043 | Tree loss: 1.713 | Accuracy: 0.414000 | 0.922 sec/iter\n",
      "Epoch: 448 | Batch: 006 / 011 | Total loss: 1.712 | Reg loss: 0.043 | Tree loss: 1.712 | Accuracy: 0.407000 | 0.922 sec/iter\n",
      "Epoch: 448 | Batch: 007 / 011 | Total loss: 1.693 | Reg loss: 0.043 | Tree loss: 1.693 | Accuracy: 0.417500 | 0.922 sec/iter\n",
      "Epoch: 448 | Batch: 008 / 011 | Total loss: 1.702 | Reg loss: 0.043 | Tree loss: 1.702 | Accuracy: 0.432500 | 0.922 sec/iter\n",
      "Epoch: 448 | Batch: 009 / 011 | Total loss: 1.658 | Reg loss: 0.043 | Tree loss: 1.658 | Accuracy: 0.443000 | 0.922 sec/iter\n",
      "Epoch: 448 | Batch: 010 / 011 | Total loss: 1.720 | Reg loss: 0.043 | Tree loss: 1.720 | Accuracy: 0.406143 | 0.922 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 449 | Batch: 000 / 011 | Total loss: 1.875 | Reg loss: 0.043 | Tree loss: 1.875 | Accuracy: 0.388000 | 0.922 sec/iter\n",
      "Epoch: 449 | Batch: 001 / 011 | Total loss: 1.803 | Reg loss: 0.043 | Tree loss: 1.803 | Accuracy: 0.426500 | 0.922 sec/iter\n",
      "Epoch: 449 | Batch: 002 / 011 | Total loss: 1.797 | Reg loss: 0.043 | Tree loss: 1.797 | Accuracy: 0.428000 | 0.922 sec/iter\n",
      "Epoch: 449 | Batch: 003 / 011 | Total loss: 1.774 | Reg loss: 0.043 | Tree loss: 1.774 | Accuracy: 0.412500 | 0.922 sec/iter\n",
      "Epoch: 449 | Batch: 004 / 011 | Total loss: 1.762 | Reg loss: 0.043 | Tree loss: 1.762 | Accuracy: 0.407000 | 0.922 sec/iter\n",
      "Epoch: 449 | Batch: 005 / 011 | Total loss: 1.723 | Reg loss: 0.043 | Tree loss: 1.723 | Accuracy: 0.426000 | 0.922 sec/iter\n",
      "Epoch: 449 | Batch: 006 / 011 | Total loss: 1.696 | Reg loss: 0.043 | Tree loss: 1.696 | Accuracy: 0.421000 | 0.922 sec/iter\n",
      "Epoch: 449 | Batch: 007 / 011 | Total loss: 1.678 | Reg loss: 0.043 | Tree loss: 1.678 | Accuracy: 0.420000 | 0.922 sec/iter\n",
      "Epoch: 449 | Batch: 008 / 011 | Total loss: 1.698 | Reg loss: 0.043 | Tree loss: 1.698 | Accuracy: 0.439500 | 0.922 sec/iter\n",
      "Epoch: 449 | Batch: 009 / 011 | Total loss: 1.674 | Reg loss: 0.043 | Tree loss: 1.674 | Accuracy: 0.432500 | 0.922 sec/iter\n",
      "Epoch: 449 | Batch: 010 / 011 | Total loss: 1.729 | Reg loss: 0.043 | Tree loss: 1.729 | Accuracy: 0.389078 | 0.922 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 450 | Batch: 000 / 011 | Total loss: 1.851 | Reg loss: 0.043 | Tree loss: 1.851 | Accuracy: 0.425500 | 0.922 sec/iter\n",
      "Epoch: 450 | Batch: 001 / 011 | Total loss: 1.837 | Reg loss: 0.043 | Tree loss: 1.837 | Accuracy: 0.410500 | 0.922 sec/iter\n",
      "Epoch: 450 | Batch: 002 / 011 | Total loss: 1.798 | Reg loss: 0.043 | Tree loss: 1.798 | Accuracy: 0.413000 | 0.922 sec/iter\n",
      "Epoch: 450 | Batch: 003 / 011 | Total loss: 1.736 | Reg loss: 0.043 | Tree loss: 1.736 | Accuracy: 0.433000 | 0.922 sec/iter\n",
      "Epoch: 450 | Batch: 004 / 011 | Total loss: 1.761 | Reg loss: 0.043 | Tree loss: 1.761 | Accuracy: 0.418000 | 0.922 sec/iter\n",
      "Epoch: 450 | Batch: 005 / 011 | Total loss: 1.725 | Reg loss: 0.043 | Tree loss: 1.725 | Accuracy: 0.423500 | 0.922 sec/iter\n",
      "Epoch: 450 | Batch: 006 / 011 | Total loss: 1.680 | Reg loss: 0.043 | Tree loss: 1.680 | Accuracy: 0.417000 | 0.922 sec/iter\n",
      "Epoch: 450 | Batch: 007 / 011 | Total loss: 1.703 | Reg loss: 0.043 | Tree loss: 1.703 | Accuracy: 0.422500 | 0.921 sec/iter\n",
      "Epoch: 450 | Batch: 008 / 011 | Total loss: 1.714 | Reg loss: 0.043 | Tree loss: 1.714 | Accuracy: 0.392500 | 0.921 sec/iter\n",
      "Epoch: 450 | Batch: 009 / 011 | Total loss: 1.658 | Reg loss: 0.043 | Tree loss: 1.658 | Accuracy: 0.445500 | 0.921 sec/iter\n",
      "Epoch: 450 | Batch: 010 / 011 | Total loss: 1.774 | Reg loss: 0.043 | Tree loss: 1.774 | Accuracy: 0.372014 | 0.921 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 451 | Batch: 000 / 011 | Total loss: 1.860 | Reg loss: 0.043 | Tree loss: 1.860 | Accuracy: 0.405000 | 0.921 sec/iter\n",
      "Epoch: 451 | Batch: 001 / 011 | Total loss: 1.829 | Reg loss: 0.043 | Tree loss: 1.829 | Accuracy: 0.416500 | 0.921 sec/iter\n",
      "Epoch: 451 | Batch: 002 / 011 | Total loss: 1.791 | Reg loss: 0.043 | Tree loss: 1.791 | Accuracy: 0.424000 | 0.921 sec/iter\n",
      "Epoch: 451 | Batch: 003 / 011 | Total loss: 1.763 | Reg loss: 0.043 | Tree loss: 1.763 | Accuracy: 0.415500 | 0.921 sec/iter\n",
      "Epoch: 451 | Batch: 004 / 011 | Total loss: 1.748 | Reg loss: 0.043 | Tree loss: 1.748 | Accuracy: 0.412500 | 0.921 sec/iter\n",
      "Epoch: 451 | Batch: 005 / 011 | Total loss: 1.723 | Reg loss: 0.043 | Tree loss: 1.723 | Accuracy: 0.432500 | 0.921 sec/iter\n",
      "Epoch: 451 | Batch: 006 / 011 | Total loss: 1.699 | Reg loss: 0.043 | Tree loss: 1.699 | Accuracy: 0.418000 | 0.921 sec/iter\n",
      "Epoch: 451 | Batch: 007 / 011 | Total loss: 1.697 | Reg loss: 0.043 | Tree loss: 1.697 | Accuracy: 0.406500 | 0.921 sec/iter\n",
      "Epoch: 451 | Batch: 008 / 011 | Total loss: 1.690 | Reg loss: 0.043 | Tree loss: 1.690 | Accuracy: 0.415500 | 0.921 sec/iter\n",
      "Epoch: 451 | Batch: 009 / 011 | Total loss: 1.659 | Reg loss: 0.043 | Tree loss: 1.659 | Accuracy: 0.438000 | 0.921 sec/iter\n",
      "Epoch: 451 | Batch: 010 / 011 | Total loss: 1.653 | Reg loss: 0.043 | Tree loss: 1.653 | Accuracy: 0.423208 | 0.921 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 452 | Batch: 000 / 011 | Total loss: 1.866 | Reg loss: 0.043 | Tree loss: 1.866 | Accuracy: 0.394500 | 0.921 sec/iter\n",
      "Epoch: 452 | Batch: 001 / 011 | Total loss: 1.814 | Reg loss: 0.043 | Tree loss: 1.814 | Accuracy: 0.417500 | 0.921 sec/iter\n",
      "Epoch: 452 | Batch: 002 / 011 | Total loss: 1.800 | Reg loss: 0.043 | Tree loss: 1.800 | Accuracy: 0.418000 | 0.921 sec/iter\n",
      "Epoch: 452 | Batch: 003 / 011 | Total loss: 1.777 | Reg loss: 0.043 | Tree loss: 1.777 | Accuracy: 0.413500 | 0.921 sec/iter\n",
      "Epoch: 452 | Batch: 004 / 011 | Total loss: 1.753 | Reg loss: 0.043 | Tree loss: 1.753 | Accuracy: 0.421500 | 0.921 sec/iter\n",
      "Epoch: 452 | Batch: 005 / 011 | Total loss: 1.718 | Reg loss: 0.043 | Tree loss: 1.718 | Accuracy: 0.424000 | 0.921 sec/iter\n",
      "Epoch: 452 | Batch: 006 / 011 | Total loss: 1.713 | Reg loss: 0.043 | Tree loss: 1.713 | Accuracy: 0.408500 | 0.921 sec/iter\n",
      "Epoch: 452 | Batch: 007 / 011 | Total loss: 1.663 | Reg loss: 0.043 | Tree loss: 1.663 | Accuracy: 0.445000 | 0.921 sec/iter\n",
      "Epoch: 452 | Batch: 008 / 011 | Total loss: 1.686 | Reg loss: 0.043 | Tree loss: 1.686 | Accuracy: 0.427500 | 0.921 sec/iter\n",
      "Epoch: 452 | Batch: 009 / 011 | Total loss: 1.656 | Reg loss: 0.043 | Tree loss: 1.656 | Accuracy: 0.433500 | 0.921 sec/iter\n",
      "Epoch: 452 | Batch: 010 / 011 | Total loss: 1.715 | Reg loss: 0.043 | Tree loss: 1.715 | Accuracy: 0.402730 | 0.921 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 453 | Batch: 000 / 011 | Total loss: 1.850 | Reg loss: 0.043 | Tree loss: 1.850 | Accuracy: 0.422500 | 0.921 sec/iter\n",
      "Epoch: 453 | Batch: 001 / 011 | Total loss: 1.835 | Reg loss: 0.043 | Tree loss: 1.835 | Accuracy: 0.433000 | 0.921 sec/iter\n",
      "Epoch: 453 | Batch: 002 / 011 | Total loss: 1.793 | Reg loss: 0.043 | Tree loss: 1.793 | Accuracy: 0.403000 | 0.921 sec/iter\n",
      "Epoch: 453 | Batch: 003 / 011 | Total loss: 1.755 | Reg loss: 0.043 | Tree loss: 1.755 | Accuracy: 0.420000 | 0.921 sec/iter\n",
      "Epoch: 453 | Batch: 004 / 011 | Total loss: 1.742 | Reg loss: 0.043 | Tree loss: 1.742 | Accuracy: 0.406500 | 0.921 sec/iter\n",
      "Epoch: 453 | Batch: 005 / 011 | Total loss: 1.724 | Reg loss: 0.043 | Tree loss: 1.724 | Accuracy: 0.421500 | 0.921 sec/iter\n",
      "Epoch: 453 | Batch: 006 / 011 | Total loss: 1.687 | Reg loss: 0.043 | Tree loss: 1.687 | Accuracy: 0.421500 | 0.921 sec/iter\n",
      "Epoch: 453 | Batch: 007 / 011 | Total loss: 1.699 | Reg loss: 0.043 | Tree loss: 1.699 | Accuracy: 0.417500 | 0.921 sec/iter\n",
      "Epoch: 453 | Batch: 008 / 011 | Total loss: 1.681 | Reg loss: 0.043 | Tree loss: 1.681 | Accuracy: 0.433000 | 0.921 sec/iter\n",
      "Epoch: 453 | Batch: 009 / 011 | Total loss: 1.702 | Reg loss: 0.043 | Tree loss: 1.702 | Accuracy: 0.415000 | 0.921 sec/iter\n",
      "Epoch: 453 | Batch: 010 / 011 | Total loss: 1.662 | Reg loss: 0.043 | Tree loss: 1.662 | Accuracy: 0.436860 | 0.921 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 454 | Batch: 000 / 011 | Total loss: 1.826 | Reg loss: 0.043 | Tree loss: 1.826 | Accuracy: 0.413000 | 0.921 sec/iter\n",
      "Epoch: 454 | Batch: 001 / 011 | Total loss: 1.819 | Reg loss: 0.043 | Tree loss: 1.819 | Accuracy: 0.421500 | 0.921 sec/iter\n",
      "Epoch: 454 | Batch: 002 / 011 | Total loss: 1.811 | Reg loss: 0.043 | Tree loss: 1.811 | Accuracy: 0.398000 | 0.921 sec/iter\n",
      "Epoch: 454 | Batch: 003 / 011 | Total loss: 1.766 | Reg loss: 0.043 | Tree loss: 1.766 | Accuracy: 0.438500 | 0.921 sec/iter\n",
      "Epoch: 454 | Batch: 004 / 011 | Total loss: 1.774 | Reg loss: 0.043 | Tree loss: 1.774 | Accuracy: 0.414500 | 0.921 sec/iter\n",
      "Epoch: 454 | Batch: 005 / 011 | Total loss: 1.705 | Reg loss: 0.043 | Tree loss: 1.705 | Accuracy: 0.433000 | 0.921 sec/iter\n",
      "Epoch: 454 | Batch: 006 / 011 | Total loss: 1.712 | Reg loss: 0.043 | Tree loss: 1.712 | Accuracy: 0.402500 | 0.921 sec/iter\n",
      "Epoch: 454 | Batch: 007 / 011 | Total loss: 1.713 | Reg loss: 0.043 | Tree loss: 1.713 | Accuracy: 0.420000 | 0.921 sec/iter\n",
      "Epoch: 454 | Batch: 008 / 011 | Total loss: 1.667 | Reg loss: 0.043 | Tree loss: 1.667 | Accuracy: 0.438000 | 0.921 sec/iter\n",
      "Epoch: 454 | Batch: 009 / 011 | Total loss: 1.675 | Reg loss: 0.043 | Tree loss: 1.675 | Accuracy: 0.426000 | 0.921 sec/iter\n",
      "Epoch: 454 | Batch: 010 / 011 | Total loss: 1.747 | Reg loss: 0.043 | Tree loss: 1.747 | Accuracy: 0.409556 | 0.921 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 455 | Batch: 000 / 011 | Total loss: 1.844 | Reg loss: 0.043 | Tree loss: 1.844 | Accuracy: 0.412000 | 0.921 sec/iter\n",
      "Epoch: 455 | Batch: 001 / 011 | Total loss: 1.849 | Reg loss: 0.043 | Tree loss: 1.849 | Accuracy: 0.404000 | 0.921 sec/iter\n",
      "Epoch: 455 | Batch: 002 / 011 | Total loss: 1.784 | Reg loss: 0.043 | Tree loss: 1.784 | Accuracy: 0.435000 | 0.921 sec/iter\n",
      "Epoch: 455 | Batch: 003 / 011 | Total loss: 1.773 | Reg loss: 0.043 | Tree loss: 1.773 | Accuracy: 0.433500 | 0.921 sec/iter\n",
      "Epoch: 455 | Batch: 004 / 011 | Total loss: 1.748 | Reg loss: 0.043 | Tree loss: 1.748 | Accuracy: 0.424000 | 0.921 sec/iter\n",
      "Epoch: 455 | Batch: 005 / 011 | Total loss: 1.700 | Reg loss: 0.043 | Tree loss: 1.700 | Accuracy: 0.427000 | 0.921 sec/iter\n",
      "Epoch: 455 | Batch: 006 / 011 | Total loss: 1.735 | Reg loss: 0.043 | Tree loss: 1.735 | Accuracy: 0.394000 | 0.921 sec/iter\n",
      "Epoch: 455 | Batch: 007 / 011 | Total loss: 1.665 | Reg loss: 0.043 | Tree loss: 1.665 | Accuracy: 0.432500 | 0.921 sec/iter\n",
      "Epoch: 455 | Batch: 008 / 011 | Total loss: 1.700 | Reg loss: 0.043 | Tree loss: 1.700 | Accuracy: 0.417500 | 0.921 sec/iter\n",
      "Epoch: 455 | Batch: 009 / 011 | Total loss: 1.699 | Reg loss: 0.043 | Tree loss: 1.699 | Accuracy: 0.419500 | 0.921 sec/iter\n",
      "Epoch: 455 | Batch: 010 / 011 | Total loss: 1.716 | Reg loss: 0.043 | Tree loss: 1.716 | Accuracy: 0.399317 | 0.921 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 456 | Batch: 000 / 011 | Total loss: 1.863 | Reg loss: 0.043 | Tree loss: 1.863 | Accuracy: 0.400000 | 0.921 sec/iter\n",
      "Epoch: 456 | Batch: 001 / 011 | Total loss: 1.839 | Reg loss: 0.043 | Tree loss: 1.839 | Accuracy: 0.427000 | 0.921 sec/iter\n",
      "Epoch: 456 | Batch: 002 / 011 | Total loss: 1.789 | Reg loss: 0.043 | Tree loss: 1.789 | Accuracy: 0.425000 | 0.921 sec/iter\n",
      "Epoch: 456 | Batch: 003 / 011 | Total loss: 1.785 | Reg loss: 0.043 | Tree loss: 1.785 | Accuracy: 0.409500 | 0.921 sec/iter\n",
      "Epoch: 456 | Batch: 004 / 011 | Total loss: 1.718 | Reg loss: 0.043 | Tree loss: 1.718 | Accuracy: 0.429000 | 0.921 sec/iter\n",
      "Epoch: 456 | Batch: 005 / 011 | Total loss: 1.709 | Reg loss: 0.043 | Tree loss: 1.709 | Accuracy: 0.422500 | 0.921 sec/iter\n",
      "Epoch: 456 | Batch: 006 / 011 | Total loss: 1.689 | Reg loss: 0.043 | Tree loss: 1.689 | Accuracy: 0.441000 | 0.921 sec/iter\n",
      "Epoch: 456 | Batch: 007 / 011 | Total loss: 1.700 | Reg loss: 0.043 | Tree loss: 1.700 | Accuracy: 0.423500 | 0.921 sec/iter\n",
      "Epoch: 456 | Batch: 008 / 011 | Total loss: 1.685 | Reg loss: 0.043 | Tree loss: 1.685 | Accuracy: 0.421000 | 0.921 sec/iter\n",
      "Epoch: 456 | Batch: 009 / 011 | Total loss: 1.692 | Reg loss: 0.043 | Tree loss: 1.692 | Accuracy: 0.407000 | 0.921 sec/iter\n",
      "Epoch: 456 | Batch: 010 / 011 | Total loss: 1.673 | Reg loss: 0.043 | Tree loss: 1.673 | Accuracy: 0.382253 | 0.921 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 457 | Batch: 000 / 011 | Total loss: 1.851 | Reg loss: 0.043 | Tree loss: 1.851 | Accuracy: 0.406500 | 0.921 sec/iter\n",
      "Epoch: 457 | Batch: 001 / 011 | Total loss: 1.829 | Reg loss: 0.043 | Tree loss: 1.829 | Accuracy: 0.410500 | 0.921 sec/iter\n",
      "Epoch: 457 | Batch: 002 / 011 | Total loss: 1.795 | Reg loss: 0.043 | Tree loss: 1.795 | Accuracy: 0.427500 | 0.921 sec/iter\n",
      "Epoch: 457 | Batch: 003 / 011 | Total loss: 1.770 | Reg loss: 0.043 | Tree loss: 1.770 | Accuracy: 0.410500 | 0.921 sec/iter\n",
      "Epoch: 457 | Batch: 004 / 011 | Total loss: 1.763 | Reg loss: 0.043 | Tree loss: 1.763 | Accuracy: 0.426000 | 0.921 sec/iter\n",
      "Epoch: 457 | Batch: 005 / 011 | Total loss: 1.729 | Reg loss: 0.043 | Tree loss: 1.729 | Accuracy: 0.429000 | 0.921 sec/iter\n",
      "Epoch: 457 | Batch: 006 / 011 | Total loss: 1.723 | Reg loss: 0.043 | Tree loss: 1.723 | Accuracy: 0.411500 | 0.921 sec/iter\n",
      "Epoch: 457 | Batch: 007 / 011 | Total loss: 1.698 | Reg loss: 0.043 | Tree loss: 1.698 | Accuracy: 0.414000 | 0.921 sec/iter\n",
      "Epoch: 457 | Batch: 008 / 011 | Total loss: 1.670 | Reg loss: 0.043 | Tree loss: 1.670 | Accuracy: 0.428500 | 0.921 sec/iter\n",
      "Epoch: 457 | Batch: 009 / 011 | Total loss: 1.671 | Reg loss: 0.043 | Tree loss: 1.671 | Accuracy: 0.431000 | 0.921 sec/iter\n",
      "Epoch: 457 | Batch: 010 / 011 | Total loss: 1.644 | Reg loss: 0.043 | Tree loss: 1.644 | Accuracy: 0.494881 | 0.921 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 458 | Batch: 000 / 011 | Total loss: 1.864 | Reg loss: 0.043 | Tree loss: 1.864 | Accuracy: 0.394500 | 0.921 sec/iter\n",
      "Epoch: 458 | Batch: 001 / 011 | Total loss: 1.842 | Reg loss: 0.043 | Tree loss: 1.842 | Accuracy: 0.410000 | 0.921 sec/iter\n",
      "Epoch: 458 | Batch: 002 / 011 | Total loss: 1.772 | Reg loss: 0.043 | Tree loss: 1.772 | Accuracy: 0.432000 | 0.921 sec/iter\n",
      "Epoch: 458 | Batch: 003 / 011 | Total loss: 1.781 | Reg loss: 0.043 | Tree loss: 1.781 | Accuracy: 0.424500 | 0.921 sec/iter\n",
      "Epoch: 458 | Batch: 004 / 011 | Total loss: 1.753 | Reg loss: 0.043 | Tree loss: 1.753 | Accuracy: 0.406000 | 0.921 sec/iter\n",
      "Epoch: 458 | Batch: 005 / 011 | Total loss: 1.723 | Reg loss: 0.043 | Tree loss: 1.723 | Accuracy: 0.430500 | 0.921 sec/iter\n",
      "Epoch: 458 | Batch: 006 / 011 | Total loss: 1.707 | Reg loss: 0.043 | Tree loss: 1.707 | Accuracy: 0.424500 | 0.921 sec/iter\n",
      "Epoch: 458 | Batch: 007 / 011 | Total loss: 1.681 | Reg loss: 0.043 | Tree loss: 1.681 | Accuracy: 0.413000 | 0.921 sec/iter\n",
      "Epoch: 458 | Batch: 008 / 011 | Total loss: 1.680 | Reg loss: 0.043 | Tree loss: 1.680 | Accuracy: 0.417000 | 0.921 sec/iter\n",
      "Epoch: 458 | Batch: 009 / 011 | Total loss: 1.669 | Reg loss: 0.043 | Tree loss: 1.669 | Accuracy: 0.432000 | 0.921 sec/iter\n",
      "Epoch: 458 | Batch: 010 / 011 | Total loss: 1.654 | Reg loss: 0.043 | Tree loss: 1.654 | Accuracy: 0.481229 | 0.921 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 459 | Batch: 000 / 011 | Total loss: 1.839 | Reg loss: 0.043 | Tree loss: 1.839 | Accuracy: 0.421500 | 0.921 sec/iter\n",
      "Epoch: 459 | Batch: 001 / 011 | Total loss: 1.819 | Reg loss: 0.043 | Tree loss: 1.819 | Accuracy: 0.415500 | 0.921 sec/iter\n",
      "Epoch: 459 | Batch: 002 / 011 | Total loss: 1.790 | Reg loss: 0.043 | Tree loss: 1.790 | Accuracy: 0.414000 | 0.921 sec/iter\n",
      "Epoch: 459 | Batch: 003 / 011 | Total loss: 1.787 | Reg loss: 0.043 | Tree loss: 1.787 | Accuracy: 0.395000 | 0.921 sec/iter\n",
      "Epoch: 459 | Batch: 004 / 011 | Total loss: 1.763 | Reg loss: 0.043 | Tree loss: 1.763 | Accuracy: 0.408000 | 0.921 sec/iter\n",
      "Epoch: 459 | Batch: 005 / 011 | Total loss: 1.712 | Reg loss: 0.043 | Tree loss: 1.712 | Accuracy: 0.428000 | 0.921 sec/iter\n",
      "Epoch: 459 | Batch: 006 / 011 | Total loss: 1.700 | Reg loss: 0.043 | Tree loss: 1.700 | Accuracy: 0.433500 | 0.921 sec/iter\n",
      "Epoch: 459 | Batch: 007 / 011 | Total loss: 1.682 | Reg loss: 0.043 | Tree loss: 1.682 | Accuracy: 0.431000 | 0.921 sec/iter\n",
      "Epoch: 459 | Batch: 008 / 011 | Total loss: 1.673 | Reg loss: 0.043 | Tree loss: 1.673 | Accuracy: 0.427000 | 0.921 sec/iter\n",
      "Epoch: 459 | Batch: 009 / 011 | Total loss: 1.718 | Reg loss: 0.043 | Tree loss: 1.718 | Accuracy: 0.415000 | 0.921 sec/iter\n",
      "Epoch: 459 | Batch: 010 / 011 | Total loss: 1.671 | Reg loss: 0.043 | Tree loss: 1.671 | Accuracy: 0.481229 | 0.921 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 460 | Batch: 000 / 011 | Total loss: 1.836 | Reg loss: 0.043 | Tree loss: 1.836 | Accuracy: 0.429500 | 0.921 sec/iter\n",
      "Epoch: 460 | Batch: 001 / 011 | Total loss: 1.832 | Reg loss: 0.043 | Tree loss: 1.832 | Accuracy: 0.397500 | 0.921 sec/iter\n",
      "Epoch: 460 | Batch: 002 / 011 | Total loss: 1.802 | Reg loss: 0.043 | Tree loss: 1.802 | Accuracy: 0.418000 | 0.921 sec/iter\n",
      "Epoch: 460 | Batch: 003 / 011 | Total loss: 1.780 | Reg loss: 0.043 | Tree loss: 1.780 | Accuracy: 0.416000 | 0.921 sec/iter\n",
      "Epoch: 460 | Batch: 004 / 011 | Total loss: 1.723 | Reg loss: 0.043 | Tree loss: 1.723 | Accuracy: 0.434000 | 0.921 sec/iter\n",
      "Epoch: 460 | Batch: 005 / 011 | Total loss: 1.716 | Reg loss: 0.043 | Tree loss: 1.716 | Accuracy: 0.420000 | 0.921 sec/iter\n",
      "Epoch: 460 | Batch: 006 / 011 | Total loss: 1.728 | Reg loss: 0.043 | Tree loss: 1.728 | Accuracy: 0.411000 | 0.921 sec/iter\n",
      "Epoch: 460 | Batch: 007 / 011 | Total loss: 1.687 | Reg loss: 0.043 | Tree loss: 1.687 | Accuracy: 0.424000 | 0.921 sec/iter\n",
      "Epoch: 460 | Batch: 008 / 011 | Total loss: 1.691 | Reg loss: 0.043 | Tree loss: 1.691 | Accuracy: 0.421000 | 0.921 sec/iter\n",
      "Epoch: 460 | Batch: 009 / 011 | Total loss: 1.707 | Reg loss: 0.043 | Tree loss: 1.707 | Accuracy: 0.418000 | 0.921 sec/iter\n",
      "Epoch: 460 | Batch: 010 / 011 | Total loss: 1.643 | Reg loss: 0.043 | Tree loss: 1.643 | Accuracy: 0.409556 | 0.921 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 461 | Batch: 000 / 011 | Total loss: 1.876 | Reg loss: 0.043 | Tree loss: 1.876 | Accuracy: 0.399000 | 0.921 sec/iter\n",
      "Epoch: 461 | Batch: 001 / 011 | Total loss: 1.820 | Reg loss: 0.043 | Tree loss: 1.820 | Accuracy: 0.415000 | 0.921 sec/iter\n",
      "Epoch: 461 | Batch: 002 / 011 | Total loss: 1.815 | Reg loss: 0.043 | Tree loss: 1.815 | Accuracy: 0.416000 | 0.921 sec/iter\n",
      "Epoch: 461 | Batch: 003 / 011 | Total loss: 1.769 | Reg loss: 0.043 | Tree loss: 1.769 | Accuracy: 0.412000 | 0.921 sec/iter\n",
      "Epoch: 461 | Batch: 004 / 011 | Total loss: 1.744 | Reg loss: 0.043 | Tree loss: 1.744 | Accuracy: 0.426000 | 0.921 sec/iter\n",
      "Epoch: 461 | Batch: 005 / 011 | Total loss: 1.697 | Reg loss: 0.043 | Tree loss: 1.697 | Accuracy: 0.414500 | 0.921 sec/iter\n",
      "Epoch: 461 | Batch: 006 / 011 | Total loss: 1.691 | Reg loss: 0.043 | Tree loss: 1.691 | Accuracy: 0.436500 | 0.921 sec/iter\n",
      "Epoch: 461 | Batch: 007 / 011 | Total loss: 1.678 | Reg loss: 0.043 | Tree loss: 1.678 | Accuracy: 0.413500 | 0.921 sec/iter\n",
      "Epoch: 461 | Batch: 008 / 011 | Total loss: 1.670 | Reg loss: 0.043 | Tree loss: 1.670 | Accuracy: 0.439500 | 0.921 sec/iter\n",
      "Epoch: 461 | Batch: 009 / 011 | Total loss: 1.699 | Reg loss: 0.043 | Tree loss: 1.699 | Accuracy: 0.420500 | 0.921 sec/iter\n",
      "Epoch: 461 | Batch: 010 / 011 | Total loss: 1.732 | Reg loss: 0.043 | Tree loss: 1.732 | Accuracy: 0.378840 | 0.921 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 462 | Batch: 000 / 011 | Total loss: 1.847 | Reg loss: 0.043 | Tree loss: 1.847 | Accuracy: 0.407000 | 0.921 sec/iter\n",
      "Epoch: 462 | Batch: 001 / 011 | Total loss: 1.815 | Reg loss: 0.043 | Tree loss: 1.815 | Accuracy: 0.405000 | 0.921 sec/iter\n",
      "Epoch: 462 | Batch: 002 / 011 | Total loss: 1.819 | Reg loss: 0.043 | Tree loss: 1.819 | Accuracy: 0.404500 | 0.921 sec/iter\n",
      "Epoch: 462 | Batch: 003 / 011 | Total loss: 1.779 | Reg loss: 0.043 | Tree loss: 1.779 | Accuracy: 0.419500 | 0.921 sec/iter\n",
      "Epoch: 462 | Batch: 004 / 011 | Total loss: 1.731 | Reg loss: 0.043 | Tree loss: 1.731 | Accuracy: 0.444000 | 0.921 sec/iter\n",
      "Epoch: 462 | Batch: 005 / 011 | Total loss: 1.726 | Reg loss: 0.043 | Tree loss: 1.726 | Accuracy: 0.428000 | 0.921 sec/iter\n",
      "Epoch: 462 | Batch: 006 / 011 | Total loss: 1.715 | Reg loss: 0.043 | Tree loss: 1.715 | Accuracy: 0.420500 | 0.921 sec/iter\n",
      "Epoch: 462 | Batch: 007 / 011 | Total loss: 1.687 | Reg loss: 0.043 | Tree loss: 1.687 | Accuracy: 0.433500 | 0.921 sec/iter\n",
      "Epoch: 462 | Batch: 008 / 011 | Total loss: 1.685 | Reg loss: 0.043 | Tree loss: 1.685 | Accuracy: 0.424500 | 0.921 sec/iter\n",
      "Epoch: 462 | Batch: 009 / 011 | Total loss: 1.711 | Reg loss: 0.043 | Tree loss: 1.711 | Accuracy: 0.419000 | 0.921 sec/iter\n",
      "Epoch: 462 | Batch: 010 / 011 | Total loss: 1.685 | Reg loss: 0.043 | Tree loss: 1.685 | Accuracy: 0.433447 | 0.921 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 463 | Batch: 000 / 011 | Total loss: 1.873 | Reg loss: 0.043 | Tree loss: 1.873 | Accuracy: 0.413500 | 0.921 sec/iter\n",
      "Epoch: 463 | Batch: 001 / 011 | Total loss: 1.822 | Reg loss: 0.043 | Tree loss: 1.822 | Accuracy: 0.422000 | 0.921 sec/iter\n",
      "Epoch: 463 | Batch: 002 / 011 | Total loss: 1.787 | Reg loss: 0.043 | Tree loss: 1.787 | Accuracy: 0.424500 | 0.921 sec/iter\n",
      "Epoch: 463 | Batch: 003 / 011 | Total loss: 1.740 | Reg loss: 0.043 | Tree loss: 1.740 | Accuracy: 0.419500 | 0.921 sec/iter\n",
      "Epoch: 463 | Batch: 004 / 011 | Total loss: 1.766 | Reg loss: 0.043 | Tree loss: 1.766 | Accuracy: 0.404500 | 0.921 sec/iter\n",
      "Epoch: 463 | Batch: 005 / 011 | Total loss: 1.698 | Reg loss: 0.043 | Tree loss: 1.698 | Accuracy: 0.422000 | 0.921 sec/iter\n",
      "Epoch: 463 | Batch: 006 / 011 | Total loss: 1.698 | Reg loss: 0.043 | Tree loss: 1.698 | Accuracy: 0.418500 | 0.921 sec/iter\n",
      "Epoch: 463 | Batch: 007 / 011 | Total loss: 1.713 | Reg loss: 0.043 | Tree loss: 1.713 | Accuracy: 0.399500 | 0.921 sec/iter\n",
      "Epoch: 463 | Batch: 008 / 011 | Total loss: 1.686 | Reg loss: 0.043 | Tree loss: 1.686 | Accuracy: 0.424000 | 0.921 sec/iter\n",
      "Epoch: 463 | Batch: 009 / 011 | Total loss: 1.685 | Reg loss: 0.043 | Tree loss: 1.685 | Accuracy: 0.433000 | 0.921 sec/iter\n",
      "Epoch: 463 | Batch: 010 / 011 | Total loss: 1.660 | Reg loss: 0.043 | Tree loss: 1.660 | Accuracy: 0.419795 | 0.921 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 464 | Batch: 000 / 011 | Total loss: 1.843 | Reg loss: 0.043 | Tree loss: 1.843 | Accuracy: 0.428000 | 0.921 sec/iter\n",
      "Epoch: 464 | Batch: 001 / 011 | Total loss: 1.842 | Reg loss: 0.043 | Tree loss: 1.842 | Accuracy: 0.404500 | 0.921 sec/iter\n",
      "Epoch: 464 | Batch: 002 / 011 | Total loss: 1.788 | Reg loss: 0.043 | Tree loss: 1.788 | Accuracy: 0.414000 | 0.921 sec/iter\n",
      "Epoch: 464 | Batch: 003 / 011 | Total loss: 1.739 | Reg loss: 0.043 | Tree loss: 1.739 | Accuracy: 0.444000 | 0.921 sec/iter\n",
      "Epoch: 464 | Batch: 004 / 011 | Total loss: 1.764 | Reg loss: 0.043 | Tree loss: 1.764 | Accuracy: 0.403000 | 0.921 sec/iter\n",
      "Epoch: 464 | Batch: 005 / 011 | Total loss: 1.722 | Reg loss: 0.043 | Tree loss: 1.722 | Accuracy: 0.427000 | 0.921 sec/iter\n",
      "Epoch: 464 | Batch: 006 / 011 | Total loss: 1.715 | Reg loss: 0.043 | Tree loss: 1.715 | Accuracy: 0.414000 | 0.92 sec/iter\n",
      "Epoch: 464 | Batch: 007 / 011 | Total loss: 1.688 | Reg loss: 0.043 | Tree loss: 1.688 | Accuracy: 0.431500 | 0.92 sec/iter\n",
      "Epoch: 464 | Batch: 008 / 011 | Total loss: 1.681 | Reg loss: 0.043 | Tree loss: 1.681 | Accuracy: 0.428000 | 0.92 sec/iter\n",
      "Epoch: 464 | Batch: 009 / 011 | Total loss: 1.696 | Reg loss: 0.043 | Tree loss: 1.696 | Accuracy: 0.420500 | 0.92 sec/iter\n",
      "Epoch: 464 | Batch: 010 / 011 | Total loss: 1.729 | Reg loss: 0.043 | Tree loss: 1.729 | Accuracy: 0.382253 | 0.92 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 465 | Batch: 000 / 011 | Total loss: 1.842 | Reg loss: 0.043 | Tree loss: 1.842 | Accuracy: 0.435500 | 0.92 sec/iter\n",
      "Epoch: 465 | Batch: 001 / 011 | Total loss: 1.824 | Reg loss: 0.043 | Tree loss: 1.824 | Accuracy: 0.432500 | 0.92 sec/iter\n",
      "Epoch: 465 | Batch: 002 / 011 | Total loss: 1.801 | Reg loss: 0.043 | Tree loss: 1.801 | Accuracy: 0.410500 | 0.92 sec/iter\n",
      "Epoch: 465 | Batch: 003 / 011 | Total loss: 1.776 | Reg loss: 0.043 | Tree loss: 1.776 | Accuracy: 0.402500 | 0.92 sec/iter\n",
      "Epoch: 465 | Batch: 004 / 011 | Total loss: 1.763 | Reg loss: 0.043 | Tree loss: 1.763 | Accuracy: 0.416500 | 0.92 sec/iter\n",
      "Epoch: 465 | Batch: 005 / 011 | Total loss: 1.717 | Reg loss: 0.043 | Tree loss: 1.717 | Accuracy: 0.423500 | 0.92 sec/iter\n",
      "Epoch: 465 | Batch: 006 / 011 | Total loss: 1.735 | Reg loss: 0.043 | Tree loss: 1.735 | Accuracy: 0.395000 | 0.92 sec/iter\n",
      "Epoch: 465 | Batch: 007 / 011 | Total loss: 1.673 | Reg loss: 0.043 | Tree loss: 1.673 | Accuracy: 0.431000 | 0.92 sec/iter\n",
      "Epoch: 465 | Batch: 008 / 011 | Total loss: 1.681 | Reg loss: 0.043 | Tree loss: 1.681 | Accuracy: 0.430500 | 0.92 sec/iter\n",
      "Epoch: 465 | Batch: 009 / 011 | Total loss: 1.672 | Reg loss: 0.043 | Tree loss: 1.672 | Accuracy: 0.424000 | 0.92 sec/iter\n",
      "Epoch: 465 | Batch: 010 / 011 | Total loss: 1.754 | Reg loss: 0.043 | Tree loss: 1.754 | Accuracy: 0.348123 | 0.92 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 466 | Batch: 000 / 011 | Total loss: 1.849 | Reg loss: 0.043 | Tree loss: 1.849 | Accuracy: 0.424500 | 0.92 sec/iter\n",
      "Epoch: 466 | Batch: 001 / 011 | Total loss: 1.841 | Reg loss: 0.043 | Tree loss: 1.841 | Accuracy: 0.416000 | 0.92 sec/iter\n",
      "Epoch: 466 | Batch: 002 / 011 | Total loss: 1.790 | Reg loss: 0.043 | Tree loss: 1.790 | Accuracy: 0.416500 | 0.92 sec/iter\n",
      "Epoch: 466 | Batch: 003 / 011 | Total loss: 1.771 | Reg loss: 0.043 | Tree loss: 1.771 | Accuracy: 0.413000 | 0.92 sec/iter\n",
      "Epoch: 466 | Batch: 004 / 011 | Total loss: 1.721 | Reg loss: 0.043 | Tree loss: 1.721 | Accuracy: 0.424000 | 0.92 sec/iter\n",
      "Epoch: 466 | Batch: 005 / 011 | Total loss: 1.734 | Reg loss: 0.043 | Tree loss: 1.734 | Accuracy: 0.430500 | 0.92 sec/iter\n",
      "Epoch: 466 | Batch: 006 / 011 | Total loss: 1.699 | Reg loss: 0.043 | Tree loss: 1.699 | Accuracy: 0.414000 | 0.92 sec/iter\n",
      "Epoch: 466 | Batch: 007 / 011 | Total loss: 1.671 | Reg loss: 0.043 | Tree loss: 1.671 | Accuracy: 0.424000 | 0.92 sec/iter\n",
      "Epoch: 466 | Batch: 008 / 011 | Total loss: 1.721 | Reg loss: 0.043 | Tree loss: 1.721 | Accuracy: 0.408500 | 0.92 sec/iter\n",
      "Epoch: 466 | Batch: 009 / 011 | Total loss: 1.687 | Reg loss: 0.043 | Tree loss: 1.687 | Accuracy: 0.425500 | 0.92 sec/iter\n",
      "Epoch: 466 | Batch: 010 / 011 | Total loss: 1.672 | Reg loss: 0.043 | Tree loss: 1.672 | Accuracy: 0.416382 | 0.92 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 467 | Batch: 000 / 011 | Total loss: 1.872 | Reg loss: 0.043 | Tree loss: 1.872 | Accuracy: 0.408000 | 0.92 sec/iter\n",
      "Epoch: 467 | Batch: 001 / 011 | Total loss: 1.827 | Reg loss: 0.043 | Tree loss: 1.827 | Accuracy: 0.416000 | 0.92 sec/iter\n",
      "Epoch: 467 | Batch: 002 / 011 | Total loss: 1.798 | Reg loss: 0.043 | Tree loss: 1.798 | Accuracy: 0.412000 | 0.92 sec/iter\n",
      "Epoch: 467 | Batch: 003 / 011 | Total loss: 1.757 | Reg loss: 0.043 | Tree loss: 1.757 | Accuracy: 0.422000 | 0.92 sec/iter\n",
      "Epoch: 467 | Batch: 004 / 011 | Total loss: 1.732 | Reg loss: 0.043 | Tree loss: 1.732 | Accuracy: 0.430000 | 0.92 sec/iter\n",
      "Epoch: 467 | Batch: 005 / 011 | Total loss: 1.690 | Reg loss: 0.043 | Tree loss: 1.690 | Accuracy: 0.444000 | 0.92 sec/iter\n",
      "Epoch: 467 | Batch: 006 / 011 | Total loss: 1.706 | Reg loss: 0.043 | Tree loss: 1.706 | Accuracy: 0.410500 | 0.92 sec/iter\n",
      "Epoch: 467 | Batch: 007 / 011 | Total loss: 1.714 | Reg loss: 0.043 | Tree loss: 1.714 | Accuracy: 0.410000 | 0.92 sec/iter\n",
      "Epoch: 467 | Batch: 008 / 011 | Total loss: 1.696 | Reg loss: 0.043 | Tree loss: 1.696 | Accuracy: 0.415500 | 0.92 sec/iter\n",
      "Epoch: 467 | Batch: 009 / 011 | Total loss: 1.670 | Reg loss: 0.043 | Tree loss: 1.670 | Accuracy: 0.420500 | 0.92 sec/iter\n",
      "Epoch: 467 | Batch: 010 / 011 | Total loss: 1.704 | Reg loss: 0.043 | Tree loss: 1.704 | Accuracy: 0.375427 | 0.92 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 468 | Batch: 000 / 011 | Total loss: 1.849 | Reg loss: 0.043 | Tree loss: 1.849 | Accuracy: 0.418000 | 0.92 sec/iter\n",
      "Epoch: 468 | Batch: 001 / 011 | Total loss: 1.836 | Reg loss: 0.043 | Tree loss: 1.836 | Accuracy: 0.430500 | 0.92 sec/iter\n",
      "Epoch: 468 | Batch: 002 / 011 | Total loss: 1.794 | Reg loss: 0.043 | Tree loss: 1.794 | Accuracy: 0.412000 | 0.92 sec/iter\n",
      "Epoch: 468 | Batch: 003 / 011 | Total loss: 1.768 | Reg loss: 0.043 | Tree loss: 1.768 | Accuracy: 0.410500 | 0.92 sec/iter\n",
      "Epoch: 468 | Batch: 004 / 011 | Total loss: 1.726 | Reg loss: 0.043 | Tree loss: 1.726 | Accuracy: 0.425500 | 0.92 sec/iter\n",
      "Epoch: 468 | Batch: 005 / 011 | Total loss: 1.721 | Reg loss: 0.043 | Tree loss: 1.721 | Accuracy: 0.437000 | 0.92 sec/iter\n",
      "Epoch: 468 | Batch: 006 / 011 | Total loss: 1.722 | Reg loss: 0.043 | Tree loss: 1.722 | Accuracy: 0.401500 | 0.92 sec/iter\n",
      "Epoch: 468 | Batch: 007 / 011 | Total loss: 1.696 | Reg loss: 0.043 | Tree loss: 1.696 | Accuracy: 0.413000 | 0.92 sec/iter\n",
      "Epoch: 468 | Batch: 008 / 011 | Total loss: 1.668 | Reg loss: 0.043 | Tree loss: 1.668 | Accuracy: 0.416500 | 0.92 sec/iter\n",
      "Epoch: 468 | Batch: 009 / 011 | Total loss: 1.697 | Reg loss: 0.043 | Tree loss: 1.697 | Accuracy: 0.411000 | 0.92 sec/iter\n",
      "Epoch: 468 | Batch: 010 / 011 | Total loss: 1.665 | Reg loss: 0.043 | Tree loss: 1.665 | Accuracy: 0.426621 | 0.92 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 469 | Batch: 000 / 011 | Total loss: 1.834 | Reg loss: 0.043 | Tree loss: 1.834 | Accuracy: 0.408500 | 0.92 sec/iter\n",
      "Epoch: 469 | Batch: 001 / 011 | Total loss: 1.867 | Reg loss: 0.043 | Tree loss: 1.867 | Accuracy: 0.405500 | 0.92 sec/iter\n",
      "Epoch: 469 | Batch: 002 / 011 | Total loss: 1.799 | Reg loss: 0.043 | Tree loss: 1.799 | Accuracy: 0.412000 | 0.92 sec/iter\n",
      "Epoch: 469 | Batch: 003 / 011 | Total loss: 1.782 | Reg loss: 0.043 | Tree loss: 1.782 | Accuracy: 0.414500 | 0.92 sec/iter\n",
      "Epoch: 469 | Batch: 004 / 011 | Total loss: 1.733 | Reg loss: 0.043 | Tree loss: 1.733 | Accuracy: 0.442500 | 0.92 sec/iter\n",
      "Epoch: 469 | Batch: 005 / 011 | Total loss: 1.718 | Reg loss: 0.043 | Tree loss: 1.718 | Accuracy: 0.423000 | 0.92 sec/iter\n",
      "Epoch: 469 | Batch: 006 / 011 | Total loss: 1.705 | Reg loss: 0.043 | Tree loss: 1.705 | Accuracy: 0.418000 | 0.92 sec/iter\n",
      "Epoch: 469 | Batch: 007 / 011 | Total loss: 1.651 | Reg loss: 0.043 | Tree loss: 1.651 | Accuracy: 0.448000 | 0.92 sec/iter\n",
      "Epoch: 469 | Batch: 008 / 011 | Total loss: 1.707 | Reg loss: 0.043 | Tree loss: 1.707 | Accuracy: 0.418000 | 0.92 sec/iter\n",
      "Epoch: 469 | Batch: 009 / 011 | Total loss: 1.674 | Reg loss: 0.043 | Tree loss: 1.674 | Accuracy: 0.426000 | 0.92 sec/iter\n",
      "Epoch: 469 | Batch: 010 / 011 | Total loss: 1.763 | Reg loss: 0.043 | Tree loss: 1.763 | Accuracy: 0.348123 | 0.92 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 470 | Batch: 000 / 011 | Total loss: 1.868 | Reg loss: 0.043 | Tree loss: 1.868 | Accuracy: 0.409000 | 0.92 sec/iter\n",
      "Epoch: 470 | Batch: 001 / 011 | Total loss: 1.835 | Reg loss: 0.043 | Tree loss: 1.835 | Accuracy: 0.409000 | 0.92 sec/iter\n",
      "Epoch: 470 | Batch: 002 / 011 | Total loss: 1.791 | Reg loss: 0.043 | Tree loss: 1.791 | Accuracy: 0.419000 | 0.92 sec/iter\n",
      "Epoch: 470 | Batch: 003 / 011 | Total loss: 1.790 | Reg loss: 0.043 | Tree loss: 1.790 | Accuracy: 0.416000 | 0.92 sec/iter\n",
      "Epoch: 470 | Batch: 004 / 011 | Total loss: 1.761 | Reg loss: 0.043 | Tree loss: 1.761 | Accuracy: 0.393000 | 0.92 sec/iter\n",
      "Epoch: 470 | Batch: 005 / 011 | Total loss: 1.700 | Reg loss: 0.043 | Tree loss: 1.700 | Accuracy: 0.423500 | 0.92 sec/iter\n",
      "Epoch: 470 | Batch: 006 / 011 | Total loss: 1.697 | Reg loss: 0.043 | Tree loss: 1.697 | Accuracy: 0.427500 | 0.92 sec/iter\n",
      "Epoch: 470 | Batch: 007 / 011 | Total loss: 1.679 | Reg loss: 0.043 | Tree loss: 1.679 | Accuracy: 0.437000 | 0.92 sec/iter\n",
      "Epoch: 470 | Batch: 008 / 011 | Total loss: 1.693 | Reg loss: 0.043 | Tree loss: 1.693 | Accuracy: 0.415500 | 0.92 sec/iter\n",
      "Epoch: 470 | Batch: 009 / 011 | Total loss: 1.663 | Reg loss: 0.043 | Tree loss: 1.663 | Accuracy: 0.431000 | 0.92 sec/iter\n",
      "Epoch: 470 | Batch: 010 / 011 | Total loss: 1.691 | Reg loss: 0.043 | Tree loss: 1.691 | Accuracy: 0.440273 | 0.92 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 471 | Batch: 000 / 011 | Total loss: 1.855 | Reg loss: 0.043 | Tree loss: 1.855 | Accuracy: 0.413000 | 0.92 sec/iter\n",
      "Epoch: 471 | Batch: 001 / 011 | Total loss: 1.823 | Reg loss: 0.043 | Tree loss: 1.823 | Accuracy: 0.405500 | 0.92 sec/iter\n",
      "Epoch: 471 | Batch: 002 / 011 | Total loss: 1.777 | Reg loss: 0.043 | Tree loss: 1.777 | Accuracy: 0.434500 | 0.92 sec/iter\n",
      "Epoch: 471 | Batch: 003 / 011 | Total loss: 1.759 | Reg loss: 0.043 | Tree loss: 1.759 | Accuracy: 0.423000 | 0.92 sec/iter\n",
      "Epoch: 471 | Batch: 004 / 011 | Total loss: 1.763 | Reg loss: 0.043 | Tree loss: 1.763 | Accuracy: 0.417500 | 0.92 sec/iter\n",
      "Epoch: 471 | Batch: 005 / 011 | Total loss: 1.737 | Reg loss: 0.043 | Tree loss: 1.737 | Accuracy: 0.416500 | 0.92 sec/iter\n",
      "Epoch: 471 | Batch: 006 / 011 | Total loss: 1.699 | Reg loss: 0.043 | Tree loss: 1.699 | Accuracy: 0.433500 | 0.92 sec/iter\n",
      "Epoch: 471 | Batch: 007 / 011 | Total loss: 1.718 | Reg loss: 0.043 | Tree loss: 1.718 | Accuracy: 0.408500 | 0.92 sec/iter\n",
      "Epoch: 471 | Batch: 008 / 011 | Total loss: 1.713 | Reg loss: 0.043 | Tree loss: 1.713 | Accuracy: 0.410500 | 0.92 sec/iter\n",
      "Epoch: 471 | Batch: 009 / 011 | Total loss: 1.649 | Reg loss: 0.043 | Tree loss: 1.649 | Accuracy: 0.440000 | 0.92 sec/iter\n",
      "Epoch: 471 | Batch: 010 / 011 | Total loss: 1.692 | Reg loss: 0.043 | Tree loss: 1.692 | Accuracy: 0.436860 | 0.92 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 472 | Batch: 000 / 011 | Total loss: 1.853 | Reg loss: 0.043 | Tree loss: 1.853 | Accuracy: 0.402500 | 0.92 sec/iter\n",
      "Epoch: 472 | Batch: 001 / 011 | Total loss: 1.833 | Reg loss: 0.043 | Tree loss: 1.833 | Accuracy: 0.422500 | 0.92 sec/iter\n",
      "Epoch: 472 | Batch: 002 / 011 | Total loss: 1.813 | Reg loss: 0.043 | Tree loss: 1.813 | Accuracy: 0.413000 | 0.92 sec/iter\n",
      "Epoch: 472 | Batch: 003 / 011 | Total loss: 1.735 | Reg loss: 0.043 | Tree loss: 1.735 | Accuracy: 0.440000 | 0.92 sec/iter\n",
      "Epoch: 472 | Batch: 004 / 011 | Total loss: 1.738 | Reg loss: 0.043 | Tree loss: 1.738 | Accuracy: 0.428500 | 0.92 sec/iter\n",
      "Epoch: 472 | Batch: 005 / 011 | Total loss: 1.722 | Reg loss: 0.043 | Tree loss: 1.722 | Accuracy: 0.411000 | 0.92 sec/iter\n",
      "Epoch: 472 | Batch: 006 / 011 | Total loss: 1.741 | Reg loss: 0.043 | Tree loss: 1.741 | Accuracy: 0.399500 | 0.92 sec/iter\n",
      "Epoch: 472 | Batch: 007 / 011 | Total loss: 1.684 | Reg loss: 0.043 | Tree loss: 1.684 | Accuracy: 0.419500 | 0.92 sec/iter\n",
      "Epoch: 472 | Batch: 008 / 011 | Total loss: 1.682 | Reg loss: 0.043 | Tree loss: 1.682 | Accuracy: 0.420000 | 0.92 sec/iter\n",
      "Epoch: 472 | Batch: 009 / 011 | Total loss: 1.678 | Reg loss: 0.043 | Tree loss: 1.678 | Accuracy: 0.428500 | 0.92 sec/iter\n",
      "Epoch: 472 | Batch: 010 / 011 | Total loss: 1.673 | Reg loss: 0.043 | Tree loss: 1.673 | Accuracy: 0.426621 | 0.92 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 473 | Batch: 000 / 011 | Total loss: 1.879 | Reg loss: 0.043 | Tree loss: 1.879 | Accuracy: 0.401500 | 0.92 sec/iter\n",
      "Epoch: 473 | Batch: 001 / 011 | Total loss: 1.848 | Reg loss: 0.043 | Tree loss: 1.848 | Accuracy: 0.407000 | 0.92 sec/iter\n",
      "Epoch: 473 | Batch: 002 / 011 | Total loss: 1.812 | Reg loss: 0.043 | Tree loss: 1.812 | Accuracy: 0.414000 | 0.92 sec/iter\n",
      "Epoch: 473 | Batch: 003 / 011 | Total loss: 1.729 | Reg loss: 0.043 | Tree loss: 1.729 | Accuracy: 0.440500 | 0.92 sec/iter\n",
      "Epoch: 473 | Batch: 004 / 011 | Total loss: 1.749 | Reg loss: 0.043 | Tree loss: 1.749 | Accuracy: 0.422500 | 0.92 sec/iter\n",
      "Epoch: 473 | Batch: 005 / 011 | Total loss: 1.722 | Reg loss: 0.043 | Tree loss: 1.722 | Accuracy: 0.433500 | 0.92 sec/iter\n",
      "Epoch: 473 | Batch: 006 / 011 | Total loss: 1.709 | Reg loss: 0.043 | Tree loss: 1.709 | Accuracy: 0.416500 | 0.92 sec/iter\n",
      "Epoch: 473 | Batch: 007 / 011 | Total loss: 1.690 | Reg loss: 0.043 | Tree loss: 1.690 | Accuracy: 0.424500 | 0.92 sec/iter\n",
      "Epoch: 473 | Batch: 008 / 011 | Total loss: 1.644 | Reg loss: 0.043 | Tree loss: 1.644 | Accuracy: 0.434500 | 0.92 sec/iter\n",
      "Epoch: 473 | Batch: 009 / 011 | Total loss: 1.682 | Reg loss: 0.043 | Tree loss: 1.682 | Accuracy: 0.423000 | 0.92 sec/iter\n",
      "Epoch: 473 | Batch: 010 / 011 | Total loss: 1.739 | Reg loss: 0.043 | Tree loss: 1.739 | Accuracy: 0.372014 | 0.92 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 474 | Batch: 000 / 011 | Total loss: 1.853 | Reg loss: 0.043 | Tree loss: 1.853 | Accuracy: 0.427500 | 0.92 sec/iter\n",
      "Epoch: 474 | Batch: 001 / 011 | Total loss: 1.841 | Reg loss: 0.043 | Tree loss: 1.841 | Accuracy: 0.408000 | 0.92 sec/iter\n",
      "Epoch: 474 | Batch: 002 / 011 | Total loss: 1.784 | Reg loss: 0.043 | Tree loss: 1.784 | Accuracy: 0.407500 | 0.92 sec/iter\n",
      "Epoch: 474 | Batch: 003 / 011 | Total loss: 1.778 | Reg loss: 0.043 | Tree loss: 1.778 | Accuracy: 0.419000 | 0.92 sec/iter\n",
      "Epoch: 474 | Batch: 004 / 011 | Total loss: 1.736 | Reg loss: 0.043 | Tree loss: 1.736 | Accuracy: 0.427500 | 0.92 sec/iter\n",
      "Epoch: 474 | Batch: 005 / 011 | Total loss: 1.734 | Reg loss: 0.043 | Tree loss: 1.734 | Accuracy: 0.422000 | 0.92 sec/iter\n",
      "Epoch: 474 | Batch: 006 / 011 | Total loss: 1.696 | Reg loss: 0.043 | Tree loss: 1.696 | Accuracy: 0.422000 | 0.92 sec/iter\n",
      "Epoch: 474 | Batch: 007 / 011 | Total loss: 1.701 | Reg loss: 0.043 | Tree loss: 1.701 | Accuracy: 0.420500 | 0.92 sec/iter\n",
      "Epoch: 474 | Batch: 008 / 011 | Total loss: 1.686 | Reg loss: 0.043 | Tree loss: 1.686 | Accuracy: 0.429000 | 0.92 sec/iter\n",
      "Epoch: 474 | Batch: 009 / 011 | Total loss: 1.663 | Reg loss: 0.043 | Tree loss: 1.663 | Accuracy: 0.433000 | 0.92 sec/iter\n",
      "Epoch: 474 | Batch: 010 / 011 | Total loss: 1.665 | Reg loss: 0.043 | Tree loss: 1.665 | Accuracy: 0.406143 | 0.92 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 475 | Batch: 000 / 011 | Total loss: 1.857 | Reg loss: 0.043 | Tree loss: 1.857 | Accuracy: 0.406000 | 0.92 sec/iter\n",
      "Epoch: 475 | Batch: 001 / 011 | Total loss: 1.826 | Reg loss: 0.043 | Tree loss: 1.826 | Accuracy: 0.415500 | 0.92 sec/iter\n",
      "Epoch: 475 | Batch: 002 / 011 | Total loss: 1.806 | Reg loss: 0.043 | Tree loss: 1.806 | Accuracy: 0.411000 | 0.92 sec/iter\n",
      "Epoch: 475 | Batch: 003 / 011 | Total loss: 1.740 | Reg loss: 0.043 | Tree loss: 1.740 | Accuracy: 0.428500 | 0.92 sec/iter\n",
      "Epoch: 475 | Batch: 004 / 011 | Total loss: 1.748 | Reg loss: 0.043 | Tree loss: 1.748 | Accuracy: 0.428500 | 0.92 sec/iter\n",
      "Epoch: 475 | Batch: 005 / 011 | Total loss: 1.711 | Reg loss: 0.043 | Tree loss: 1.711 | Accuracy: 0.440000 | 0.92 sec/iter\n",
      "Epoch: 475 | Batch: 006 / 011 | Total loss: 1.695 | Reg loss: 0.043 | Tree loss: 1.695 | Accuracy: 0.439000 | 0.92 sec/iter\n",
      "Epoch: 475 | Batch: 007 / 011 | Total loss: 1.695 | Reg loss: 0.043 | Tree loss: 1.695 | Accuracy: 0.401000 | 0.92 sec/iter\n",
      "Epoch: 475 | Batch: 008 / 011 | Total loss: 1.677 | Reg loss: 0.043 | Tree loss: 1.677 | Accuracy: 0.417500 | 0.92 sec/iter\n",
      "Epoch: 475 | Batch: 009 / 011 | Total loss: 1.695 | Reg loss: 0.043 | Tree loss: 1.695 | Accuracy: 0.415500 | 0.92 sec/iter\n",
      "Epoch: 475 | Batch: 010 / 011 | Total loss: 1.606 | Reg loss: 0.043 | Tree loss: 1.606 | Accuracy: 0.484642 | 0.92 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 476 | Batch: 000 / 011 | Total loss: 1.883 | Reg loss: 0.043 | Tree loss: 1.883 | Accuracy: 0.398000 | 0.92 sec/iter\n",
      "Epoch: 476 | Batch: 001 / 011 | Total loss: 1.836 | Reg loss: 0.043 | Tree loss: 1.836 | Accuracy: 0.429500 | 0.92 sec/iter\n",
      "Epoch: 476 | Batch: 002 / 011 | Total loss: 1.765 | Reg loss: 0.043 | Tree loss: 1.765 | Accuracy: 0.424000 | 0.92 sec/iter\n",
      "Epoch: 476 | Batch: 003 / 011 | Total loss: 1.749 | Reg loss: 0.043 | Tree loss: 1.749 | Accuracy: 0.424500 | 0.92 sec/iter\n",
      "Epoch: 476 | Batch: 004 / 011 | Total loss: 1.742 | Reg loss: 0.043 | Tree loss: 1.742 | Accuracy: 0.403500 | 0.92 sec/iter\n",
      "Epoch: 476 | Batch: 005 / 011 | Total loss: 1.709 | Reg loss: 0.043 | Tree loss: 1.709 | Accuracy: 0.438500 | 0.92 sec/iter\n",
      "Epoch: 476 | Batch: 006 / 011 | Total loss: 1.692 | Reg loss: 0.043 | Tree loss: 1.692 | Accuracy: 0.432500 | 0.92 sec/iter\n",
      "Epoch: 476 | Batch: 007 / 011 | Total loss: 1.708 | Reg loss: 0.043 | Tree loss: 1.708 | Accuracy: 0.412000 | 0.92 sec/iter\n",
      "Epoch: 476 | Batch: 008 / 011 | Total loss: 1.698 | Reg loss: 0.043 | Tree loss: 1.698 | Accuracy: 0.429000 | 0.92 sec/iter\n",
      "Epoch: 476 | Batch: 009 / 011 | Total loss: 1.695 | Reg loss: 0.043 | Tree loss: 1.695 | Accuracy: 0.414000 | 0.92 sec/iter\n",
      "Epoch: 476 | Batch: 010 / 011 | Total loss: 1.659 | Reg loss: 0.043 | Tree loss: 1.659 | Accuracy: 0.409556 | 0.92 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 477 | Batch: 000 / 011 | Total loss: 1.863 | Reg loss: 0.043 | Tree loss: 1.863 | Accuracy: 0.414000 | 0.92 sec/iter\n",
      "Epoch: 477 | Batch: 001 / 011 | Total loss: 1.835 | Reg loss: 0.043 | Tree loss: 1.835 | Accuracy: 0.423500 | 0.92 sec/iter\n",
      "Epoch: 477 | Batch: 002 / 011 | Total loss: 1.763 | Reg loss: 0.043 | Tree loss: 1.763 | Accuracy: 0.420000 | 0.92 sec/iter\n",
      "Epoch: 477 | Batch: 003 / 011 | Total loss: 1.784 | Reg loss: 0.043 | Tree loss: 1.784 | Accuracy: 0.407500 | 0.92 sec/iter\n",
      "Epoch: 477 | Batch: 004 / 011 | Total loss: 1.748 | Reg loss: 0.043 | Tree loss: 1.748 | Accuracy: 0.406500 | 0.92 sec/iter\n",
      "Epoch: 477 | Batch: 005 / 011 | Total loss: 1.699 | Reg loss: 0.043 | Tree loss: 1.699 | Accuracy: 0.438000 | 0.92 sec/iter\n",
      "Epoch: 477 | Batch: 006 / 011 | Total loss: 1.703 | Reg loss: 0.043 | Tree loss: 1.703 | Accuracy: 0.432500 | 0.92 sec/iter\n",
      "Epoch: 477 | Batch: 007 / 011 | Total loss: 1.724 | Reg loss: 0.043 | Tree loss: 1.724 | Accuracy: 0.404500 | 0.92 sec/iter\n",
      "Epoch: 477 | Batch: 008 / 011 | Total loss: 1.682 | Reg loss: 0.043 | Tree loss: 1.682 | Accuracy: 0.420500 | 0.92 sec/iter\n",
      "Epoch: 477 | Batch: 009 / 011 | Total loss: 1.650 | Reg loss: 0.043 | Tree loss: 1.650 | Accuracy: 0.449500 | 0.92 sec/iter\n",
      "Epoch: 477 | Batch: 010 / 011 | Total loss: 1.713 | Reg loss: 0.043 | Tree loss: 1.713 | Accuracy: 0.423208 | 0.92 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 478 | Batch: 000 / 011 | Total loss: 1.864 | Reg loss: 0.043 | Tree loss: 1.864 | Accuracy: 0.401000 | 0.92 sec/iter\n",
      "Epoch: 478 | Batch: 001 / 011 | Total loss: 1.806 | Reg loss: 0.043 | Tree loss: 1.806 | Accuracy: 0.431000 | 0.92 sec/iter\n",
      "Epoch: 478 | Batch: 002 / 011 | Total loss: 1.805 | Reg loss: 0.043 | Tree loss: 1.805 | Accuracy: 0.417000 | 0.92 sec/iter\n",
      "Epoch: 478 | Batch: 003 / 011 | Total loss: 1.771 | Reg loss: 0.043 | Tree loss: 1.771 | Accuracy: 0.403500 | 0.92 sec/iter\n",
      "Epoch: 478 | Batch: 004 / 011 | Total loss: 1.751 | Reg loss: 0.043 | Tree loss: 1.751 | Accuracy: 0.404000 | 0.92 sec/iter\n",
      "Epoch: 478 | Batch: 005 / 011 | Total loss: 1.730 | Reg loss: 0.043 | Tree loss: 1.730 | Accuracy: 0.418500 | 0.92 sec/iter\n",
      "Epoch: 478 | Batch: 006 / 011 | Total loss: 1.712 | Reg loss: 0.043 | Tree loss: 1.712 | Accuracy: 0.429500 | 0.92 sec/iter\n",
      "Epoch: 478 | Batch: 007 / 011 | Total loss: 1.685 | Reg loss: 0.043 | Tree loss: 1.685 | Accuracy: 0.436500 | 0.92 sec/iter\n",
      "Epoch: 478 | Batch: 008 / 011 | Total loss: 1.661 | Reg loss: 0.043 | Tree loss: 1.661 | Accuracy: 0.431000 | 0.92 sec/iter\n",
      "Epoch: 478 | Batch: 009 / 011 | Total loss: 1.694 | Reg loss: 0.043 | Tree loss: 1.694 | Accuracy: 0.417500 | 0.92 sec/iter\n",
      "Epoch: 478 | Batch: 010 / 011 | Total loss: 1.707 | Reg loss: 0.043 | Tree loss: 1.707 | Accuracy: 0.395904 | 0.92 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 479 | Batch: 000 / 011 | Total loss: 1.872 | Reg loss: 0.043 | Tree loss: 1.872 | Accuracy: 0.403500 | 0.92 sec/iter\n",
      "Epoch: 479 | Batch: 001 / 011 | Total loss: 1.839 | Reg loss: 0.043 | Tree loss: 1.839 | Accuracy: 0.400000 | 0.92 sec/iter\n",
      "Epoch: 479 | Batch: 002 / 011 | Total loss: 1.790 | Reg loss: 0.043 | Tree loss: 1.790 | Accuracy: 0.424500 | 0.92 sec/iter\n",
      "Epoch: 479 | Batch: 003 / 011 | Total loss: 1.744 | Reg loss: 0.043 | Tree loss: 1.744 | Accuracy: 0.422000 | 0.92 sec/iter\n",
      "Epoch: 479 | Batch: 004 / 011 | Total loss: 1.745 | Reg loss: 0.043 | Tree loss: 1.745 | Accuracy: 0.416000 | 0.92 sec/iter\n",
      "Epoch: 479 | Batch: 005 / 011 | Total loss: 1.714 | Reg loss: 0.043 | Tree loss: 1.714 | Accuracy: 0.416500 | 0.92 sec/iter\n",
      "Epoch: 479 | Batch: 006 / 011 | Total loss: 1.695 | Reg loss: 0.043 | Tree loss: 1.695 | Accuracy: 0.426000 | 0.92 sec/iter\n",
      "Epoch: 479 | Batch: 007 / 011 | Total loss: 1.687 | Reg loss: 0.043 | Tree loss: 1.687 | Accuracy: 0.440000 | 0.92 sec/iter\n",
      "Epoch: 479 | Batch: 008 / 011 | Total loss: 1.699 | Reg loss: 0.043 | Tree loss: 1.699 | Accuracy: 0.409500 | 0.92 sec/iter\n",
      "Epoch: 479 | Batch: 009 / 011 | Total loss: 1.685 | Reg loss: 0.043 | Tree loss: 1.685 | Accuracy: 0.426500 | 0.92 sec/iter\n",
      "Epoch: 479 | Batch: 010 / 011 | Total loss: 1.668 | Reg loss: 0.043 | Tree loss: 1.668 | Accuracy: 0.447099 | 0.92 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 480 | Batch: 000 / 011 | Total loss: 1.862 | Reg loss: 0.043 | Tree loss: 1.862 | Accuracy: 0.395500 | 0.92 sec/iter\n",
      "Epoch: 480 | Batch: 001 / 011 | Total loss: 1.816 | Reg loss: 0.043 | Tree loss: 1.816 | Accuracy: 0.427500 | 0.92 sec/iter\n",
      "Epoch: 480 | Batch: 002 / 011 | Total loss: 1.755 | Reg loss: 0.043 | Tree loss: 1.755 | Accuracy: 0.439500 | 0.92 sec/iter\n",
      "Epoch: 480 | Batch: 003 / 011 | Total loss: 1.810 | Reg loss: 0.043 | Tree loss: 1.810 | Accuracy: 0.406000 | 0.92 sec/iter\n",
      "Epoch: 480 | Batch: 004 / 011 | Total loss: 1.754 | Reg loss: 0.043 | Tree loss: 1.754 | Accuracy: 0.425000 | 0.92 sec/iter\n",
      "Epoch: 480 | Batch: 005 / 011 | Total loss: 1.728 | Reg loss: 0.043 | Tree loss: 1.728 | Accuracy: 0.423500 | 0.92 sec/iter\n",
      "Epoch: 480 | Batch: 006 / 011 | Total loss: 1.694 | Reg loss: 0.043 | Tree loss: 1.694 | Accuracy: 0.439000 | 0.92 sec/iter\n",
      "Epoch: 480 | Batch: 007 / 011 | Total loss: 1.686 | Reg loss: 0.043 | Tree loss: 1.686 | Accuracy: 0.407500 | 0.92 sec/iter\n",
      "Epoch: 480 | Batch: 008 / 011 | Total loss: 1.708 | Reg loss: 0.043 | Tree loss: 1.708 | Accuracy: 0.416500 | 0.92 sec/iter\n",
      "Epoch: 480 | Batch: 009 / 011 | Total loss: 1.674 | Reg loss: 0.043 | Tree loss: 1.674 | Accuracy: 0.425000 | 0.92 sec/iter\n",
      "Epoch: 480 | Batch: 010 / 011 | Total loss: 1.603 | Reg loss: 0.043 | Tree loss: 1.603 | Accuracy: 0.477816 | 0.919 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 481 | Batch: 000 / 011 | Total loss: 1.855 | Reg loss: 0.043 | Tree loss: 1.855 | Accuracy: 0.412000 | 0.919 sec/iter\n",
      "Epoch: 481 | Batch: 001 / 011 | Total loss: 1.830 | Reg loss: 0.043 | Tree loss: 1.830 | Accuracy: 0.406500 | 0.919 sec/iter\n",
      "Epoch: 481 | Batch: 002 / 011 | Total loss: 1.794 | Reg loss: 0.043 | Tree loss: 1.794 | Accuracy: 0.423000 | 0.919 sec/iter\n",
      "Epoch: 481 | Batch: 003 / 011 | Total loss: 1.757 | Reg loss: 0.043 | Tree loss: 1.757 | Accuracy: 0.417500 | 0.919 sec/iter\n",
      "Epoch: 481 | Batch: 004 / 011 | Total loss: 1.731 | Reg loss: 0.043 | Tree loss: 1.731 | Accuracy: 0.423000 | 0.919 sec/iter\n",
      "Epoch: 481 | Batch: 005 / 011 | Total loss: 1.735 | Reg loss: 0.043 | Tree loss: 1.735 | Accuracy: 0.424000 | 0.919 sec/iter\n",
      "Epoch: 481 | Batch: 006 / 011 | Total loss: 1.680 | Reg loss: 0.043 | Tree loss: 1.680 | Accuracy: 0.426000 | 0.919 sec/iter\n",
      "Epoch: 481 | Batch: 007 / 011 | Total loss: 1.716 | Reg loss: 0.043 | Tree loss: 1.716 | Accuracy: 0.422500 | 0.919 sec/iter\n",
      "Epoch: 481 | Batch: 008 / 011 | Total loss: 1.681 | Reg loss: 0.043 | Tree loss: 1.681 | Accuracy: 0.418500 | 0.919 sec/iter\n",
      "Epoch: 481 | Batch: 009 / 011 | Total loss: 1.684 | Reg loss: 0.043 | Tree loss: 1.684 | Accuracy: 0.422500 | 0.919 sec/iter\n",
      "Epoch: 481 | Batch: 010 / 011 | Total loss: 1.680 | Reg loss: 0.043 | Tree loss: 1.680 | Accuracy: 0.368601 | 0.919 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 482 | Batch: 000 / 011 | Total loss: 1.844 | Reg loss: 0.043 | Tree loss: 1.844 | Accuracy: 0.407500 | 0.919 sec/iter\n",
      "Epoch: 482 | Batch: 001 / 011 | Total loss: 1.847 | Reg loss: 0.043 | Tree loss: 1.847 | Accuracy: 0.413000 | 0.919 sec/iter\n",
      "Epoch: 482 | Batch: 002 / 011 | Total loss: 1.803 | Reg loss: 0.043 | Tree loss: 1.803 | Accuracy: 0.412000 | 0.919 sec/iter\n",
      "Epoch: 482 | Batch: 003 / 011 | Total loss: 1.745 | Reg loss: 0.043 | Tree loss: 1.745 | Accuracy: 0.434500 | 0.919 sec/iter\n",
      "Epoch: 482 | Batch: 004 / 011 | Total loss: 1.738 | Reg loss: 0.043 | Tree loss: 1.738 | Accuracy: 0.433500 | 0.919 sec/iter\n",
      "Epoch: 482 | Batch: 005 / 011 | Total loss: 1.743 | Reg loss: 0.043 | Tree loss: 1.743 | Accuracy: 0.403000 | 0.919 sec/iter\n",
      "Epoch: 482 | Batch: 006 / 011 | Total loss: 1.680 | Reg loss: 0.043 | Tree loss: 1.680 | Accuracy: 0.420500 | 0.919 sec/iter\n",
      "Epoch: 482 | Batch: 007 / 011 | Total loss: 1.685 | Reg loss: 0.043 | Tree loss: 1.685 | Accuracy: 0.420500 | 0.919 sec/iter\n",
      "Epoch: 482 | Batch: 008 / 011 | Total loss: 1.689 | Reg loss: 0.043 | Tree loss: 1.689 | Accuracy: 0.425500 | 0.919 sec/iter\n",
      "Epoch: 482 | Batch: 009 / 011 | Total loss: 1.686 | Reg loss: 0.043 | Tree loss: 1.686 | Accuracy: 0.423500 | 0.919 sec/iter\n",
      "Epoch: 482 | Batch: 010 / 011 | Total loss: 1.723 | Reg loss: 0.043 | Tree loss: 1.723 | Accuracy: 0.354949 | 0.919 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 483 | Batch: 000 / 011 | Total loss: 1.849 | Reg loss: 0.043 | Tree loss: 1.849 | Accuracy: 0.416500 | 0.919 sec/iter\n",
      "Epoch: 483 | Batch: 001 / 011 | Total loss: 1.828 | Reg loss: 0.043 | Tree loss: 1.828 | Accuracy: 0.426000 | 0.919 sec/iter\n",
      "Epoch: 483 | Batch: 002 / 011 | Total loss: 1.793 | Reg loss: 0.043 | Tree loss: 1.793 | Accuracy: 0.426000 | 0.919 sec/iter\n",
      "Epoch: 483 | Batch: 003 / 011 | Total loss: 1.792 | Reg loss: 0.043 | Tree loss: 1.792 | Accuracy: 0.404000 | 0.919 sec/iter\n",
      "Epoch: 483 | Batch: 004 / 011 | Total loss: 1.746 | Reg loss: 0.043 | Tree loss: 1.746 | Accuracy: 0.412000 | 0.919 sec/iter\n",
      "Epoch: 483 | Batch: 005 / 011 | Total loss: 1.708 | Reg loss: 0.043 | Tree loss: 1.708 | Accuracy: 0.428500 | 0.919 sec/iter\n",
      "Epoch: 483 | Batch: 006 / 011 | Total loss: 1.682 | Reg loss: 0.043 | Tree loss: 1.682 | Accuracy: 0.427500 | 0.919 sec/iter\n",
      "Epoch: 483 | Batch: 007 / 011 | Total loss: 1.705 | Reg loss: 0.043 | Tree loss: 1.705 | Accuracy: 0.415500 | 0.919 sec/iter\n",
      "Epoch: 483 | Batch: 008 / 011 | Total loss: 1.692 | Reg loss: 0.043 | Tree loss: 1.692 | Accuracy: 0.425000 | 0.919 sec/iter\n",
      "Epoch: 483 | Batch: 009 / 011 | Total loss: 1.674 | Reg loss: 0.043 | Tree loss: 1.674 | Accuracy: 0.431000 | 0.919 sec/iter\n",
      "Epoch: 483 | Batch: 010 / 011 | Total loss: 1.656 | Reg loss: 0.043 | Tree loss: 1.656 | Accuracy: 0.457338 | 0.919 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 484 | Batch: 000 / 011 | Total loss: 1.880 | Reg loss: 0.043 | Tree loss: 1.880 | Accuracy: 0.391500 | 0.919 sec/iter\n",
      "Epoch: 484 | Batch: 001 / 011 | Total loss: 1.792 | Reg loss: 0.043 | Tree loss: 1.792 | Accuracy: 0.418000 | 0.919 sec/iter\n",
      "Epoch: 484 | Batch: 002 / 011 | Total loss: 1.804 | Reg loss: 0.043 | Tree loss: 1.804 | Accuracy: 0.416000 | 0.919 sec/iter\n",
      "Epoch: 484 | Batch: 003 / 011 | Total loss: 1.766 | Reg loss: 0.043 | Tree loss: 1.766 | Accuracy: 0.422500 | 0.919 sec/iter\n",
      "Epoch: 484 | Batch: 004 / 011 | Total loss: 1.743 | Reg loss: 0.043 | Tree loss: 1.743 | Accuracy: 0.418500 | 0.919 sec/iter\n",
      "Epoch: 484 | Batch: 005 / 011 | Total loss: 1.722 | Reg loss: 0.043 | Tree loss: 1.722 | Accuracy: 0.432000 | 0.919 sec/iter\n",
      "Epoch: 484 | Batch: 006 / 011 | Total loss: 1.705 | Reg loss: 0.043 | Tree loss: 1.705 | Accuracy: 0.407500 | 0.919 sec/iter\n",
      "Epoch: 484 | Batch: 007 / 011 | Total loss: 1.697 | Reg loss: 0.043 | Tree loss: 1.697 | Accuracy: 0.426000 | 0.919 sec/iter\n",
      "Epoch: 484 | Batch: 008 / 011 | Total loss: 1.688 | Reg loss: 0.043 | Tree loss: 1.688 | Accuracy: 0.416500 | 0.919 sec/iter\n",
      "Epoch: 484 | Batch: 009 / 011 | Total loss: 1.678 | Reg loss: 0.043 | Tree loss: 1.678 | Accuracy: 0.438000 | 0.919 sec/iter\n",
      "Epoch: 484 | Batch: 010 / 011 | Total loss: 1.723 | Reg loss: 0.043 | Tree loss: 1.723 | Accuracy: 0.361775 | 0.919 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 485 | Batch: 000 / 011 | Total loss: 1.858 | Reg loss: 0.043 | Tree loss: 1.858 | Accuracy: 0.423000 | 0.919 sec/iter\n",
      "Epoch: 485 | Batch: 001 / 011 | Total loss: 1.830 | Reg loss: 0.043 | Tree loss: 1.830 | Accuracy: 0.430000 | 0.919 sec/iter\n",
      "Epoch: 485 | Batch: 002 / 011 | Total loss: 1.782 | Reg loss: 0.043 | Tree loss: 1.782 | Accuracy: 0.413500 | 0.919 sec/iter\n",
      "Epoch: 485 | Batch: 003 / 011 | Total loss: 1.764 | Reg loss: 0.043 | Tree loss: 1.764 | Accuracy: 0.416000 | 0.919 sec/iter\n",
      "Epoch: 485 | Batch: 004 / 011 | Total loss: 1.749 | Reg loss: 0.043 | Tree loss: 1.749 | Accuracy: 0.420000 | 0.919 sec/iter\n",
      "Epoch: 485 | Batch: 005 / 011 | Total loss: 1.742 | Reg loss: 0.043 | Tree loss: 1.742 | Accuracy: 0.406000 | 0.919 sec/iter\n",
      "Epoch: 485 | Batch: 006 / 011 | Total loss: 1.714 | Reg loss: 0.043 | Tree loss: 1.714 | Accuracy: 0.425000 | 0.919 sec/iter\n",
      "Epoch: 485 | Batch: 007 / 011 | Total loss: 1.671 | Reg loss: 0.043 | Tree loss: 1.671 | Accuracy: 0.427000 | 0.919 sec/iter\n",
      "Epoch: 485 | Batch: 008 / 011 | Total loss: 1.691 | Reg loss: 0.043 | Tree loss: 1.691 | Accuracy: 0.403000 | 0.919 sec/iter\n",
      "Epoch: 485 | Batch: 009 / 011 | Total loss: 1.658 | Reg loss: 0.043 | Tree loss: 1.658 | Accuracy: 0.427000 | 0.919 sec/iter\n",
      "Epoch: 485 | Batch: 010 / 011 | Total loss: 1.674 | Reg loss: 0.043 | Tree loss: 1.674 | Accuracy: 0.419795 | 0.919 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 486 | Batch: 000 / 011 | Total loss: 1.859 | Reg loss: 0.043 | Tree loss: 1.859 | Accuracy: 0.407500 | 0.919 sec/iter\n",
      "Epoch: 486 | Batch: 001 / 011 | Total loss: 1.849 | Reg loss: 0.043 | Tree loss: 1.849 | Accuracy: 0.400000 | 0.919 sec/iter\n",
      "Epoch: 486 | Batch: 002 / 011 | Total loss: 1.792 | Reg loss: 0.043 | Tree loss: 1.792 | Accuracy: 0.428500 | 0.919 sec/iter\n",
      "Epoch: 486 | Batch: 003 / 011 | Total loss: 1.760 | Reg loss: 0.043 | Tree loss: 1.760 | Accuracy: 0.426000 | 0.919 sec/iter\n",
      "Epoch: 486 | Batch: 004 / 011 | Total loss: 1.764 | Reg loss: 0.043 | Tree loss: 1.764 | Accuracy: 0.419000 | 0.919 sec/iter\n",
      "Epoch: 486 | Batch: 005 / 011 | Total loss: 1.714 | Reg loss: 0.043 | Tree loss: 1.714 | Accuracy: 0.417500 | 0.919 sec/iter\n",
      "Epoch: 486 | Batch: 006 / 011 | Total loss: 1.701 | Reg loss: 0.043 | Tree loss: 1.701 | Accuracy: 0.425000 | 0.919 sec/iter\n",
      "Epoch: 486 | Batch: 007 / 011 | Total loss: 1.678 | Reg loss: 0.043 | Tree loss: 1.678 | Accuracy: 0.416000 | 0.919 sec/iter\n",
      "Epoch: 486 | Batch: 008 / 011 | Total loss: 1.689 | Reg loss: 0.043 | Tree loss: 1.689 | Accuracy: 0.416500 | 0.919 sec/iter\n",
      "Epoch: 486 | Batch: 009 / 011 | Total loss: 1.665 | Reg loss: 0.043 | Tree loss: 1.665 | Accuracy: 0.428500 | 0.919 sec/iter\n",
      "Epoch: 486 | Batch: 010 / 011 | Total loss: 1.644 | Reg loss: 0.043 | Tree loss: 1.644 | Accuracy: 0.412969 | 0.919 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 487 | Batch: 000 / 011 | Total loss: 1.833 | Reg loss: 0.043 | Tree loss: 1.833 | Accuracy: 0.420500 | 0.919 sec/iter\n",
      "Epoch: 487 | Batch: 001 / 011 | Total loss: 1.854 | Reg loss: 0.043 | Tree loss: 1.854 | Accuracy: 0.402000 | 0.919 sec/iter\n",
      "Epoch: 487 | Batch: 002 / 011 | Total loss: 1.797 | Reg loss: 0.043 | Tree loss: 1.797 | Accuracy: 0.422500 | 0.919 sec/iter\n",
      "Epoch: 487 | Batch: 003 / 011 | Total loss: 1.780 | Reg loss: 0.043 | Tree loss: 1.780 | Accuracy: 0.419500 | 0.919 sec/iter\n",
      "Epoch: 487 | Batch: 004 / 011 | Total loss: 1.758 | Reg loss: 0.043 | Tree loss: 1.758 | Accuracy: 0.424500 | 0.919 sec/iter\n",
      "Epoch: 487 | Batch: 005 / 011 | Total loss: 1.721 | Reg loss: 0.043 | Tree loss: 1.721 | Accuracy: 0.420500 | 0.919 sec/iter\n",
      "Epoch: 487 | Batch: 006 / 011 | Total loss: 1.700 | Reg loss: 0.043 | Tree loss: 1.700 | Accuracy: 0.413500 | 0.919 sec/iter\n",
      "Epoch: 487 | Batch: 007 / 011 | Total loss: 1.675 | Reg loss: 0.043 | Tree loss: 1.675 | Accuracy: 0.414500 | 0.919 sec/iter\n",
      "Epoch: 487 | Batch: 008 / 011 | Total loss: 1.688 | Reg loss: 0.043 | Tree loss: 1.688 | Accuracy: 0.427000 | 0.919 sec/iter\n",
      "Epoch: 487 | Batch: 009 / 011 | Total loss: 1.678 | Reg loss: 0.043 | Tree loss: 1.678 | Accuracy: 0.419000 | 0.919 sec/iter\n",
      "Epoch: 487 | Batch: 010 / 011 | Total loss: 1.667 | Reg loss: 0.043 | Tree loss: 1.667 | Accuracy: 0.447099 | 0.919 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 488 | Batch: 000 / 011 | Total loss: 1.853 | Reg loss: 0.043 | Tree loss: 1.853 | Accuracy: 0.405500 | 0.919 sec/iter\n",
      "Epoch: 488 | Batch: 001 / 011 | Total loss: 1.831 | Reg loss: 0.043 | Tree loss: 1.831 | Accuracy: 0.420000 | 0.919 sec/iter\n",
      "Epoch: 488 | Batch: 002 / 011 | Total loss: 1.788 | Reg loss: 0.043 | Tree loss: 1.788 | Accuracy: 0.419500 | 0.919 sec/iter\n",
      "Epoch: 488 | Batch: 003 / 011 | Total loss: 1.755 | Reg loss: 0.043 | Tree loss: 1.755 | Accuracy: 0.424500 | 0.919 sec/iter\n",
      "Epoch: 488 | Batch: 004 / 011 | Total loss: 1.716 | Reg loss: 0.043 | Tree loss: 1.716 | Accuracy: 0.438500 | 0.919 sec/iter\n",
      "Epoch: 488 | Batch: 005 / 011 | Total loss: 1.715 | Reg loss: 0.043 | Tree loss: 1.715 | Accuracy: 0.425000 | 0.919 sec/iter\n",
      "Epoch: 488 | Batch: 006 / 011 | Total loss: 1.722 | Reg loss: 0.043 | Tree loss: 1.722 | Accuracy: 0.414000 | 0.919 sec/iter\n",
      "Epoch: 488 | Batch: 007 / 011 | Total loss: 1.697 | Reg loss: 0.043 | Tree loss: 1.697 | Accuracy: 0.424000 | 0.919 sec/iter\n",
      "Epoch: 488 | Batch: 008 / 011 | Total loss: 1.715 | Reg loss: 0.043 | Tree loss: 1.715 | Accuracy: 0.406000 | 0.919 sec/iter\n",
      "Epoch: 488 | Batch: 009 / 011 | Total loss: 1.690 | Reg loss: 0.043 | Tree loss: 1.690 | Accuracy: 0.419500 | 0.919 sec/iter\n",
      "Epoch: 488 | Batch: 010 / 011 | Total loss: 1.649 | Reg loss: 0.043 | Tree loss: 1.649 | Accuracy: 0.430034 | 0.919 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 489 | Batch: 000 / 011 | Total loss: 1.865 | Reg loss: 0.043 | Tree loss: 1.865 | Accuracy: 0.401000 | 0.919 sec/iter\n",
      "Epoch: 489 | Batch: 001 / 011 | Total loss: 1.835 | Reg loss: 0.043 | Tree loss: 1.835 | Accuracy: 0.417000 | 0.919 sec/iter\n",
      "Epoch: 489 | Batch: 002 / 011 | Total loss: 1.795 | Reg loss: 0.043 | Tree loss: 1.795 | Accuracy: 0.413500 | 0.919 sec/iter\n",
      "Epoch: 489 | Batch: 003 / 011 | Total loss: 1.746 | Reg loss: 0.043 | Tree loss: 1.746 | Accuracy: 0.434500 | 0.919 sec/iter\n",
      "Epoch: 489 | Batch: 004 / 011 | Total loss: 1.755 | Reg loss: 0.043 | Tree loss: 1.755 | Accuracy: 0.424500 | 0.919 sec/iter\n",
      "Epoch: 489 | Batch: 005 / 011 | Total loss: 1.700 | Reg loss: 0.043 | Tree loss: 1.700 | Accuracy: 0.429500 | 0.919 sec/iter\n",
      "Epoch: 489 | Batch: 006 / 011 | Total loss: 1.730 | Reg loss: 0.043 | Tree loss: 1.730 | Accuracy: 0.391500 | 0.919 sec/iter\n",
      "Epoch: 489 | Batch: 007 / 011 | Total loss: 1.689 | Reg loss: 0.043 | Tree loss: 1.689 | Accuracy: 0.430000 | 0.919 sec/iter\n",
      "Epoch: 489 | Batch: 008 / 011 | Total loss: 1.706 | Reg loss: 0.043 | Tree loss: 1.706 | Accuracy: 0.409000 | 0.919 sec/iter\n",
      "Epoch: 489 | Batch: 009 / 011 | Total loss: 1.661 | Reg loss: 0.043 | Tree loss: 1.661 | Accuracy: 0.433500 | 0.919 sec/iter\n",
      "Epoch: 489 | Batch: 010 / 011 | Total loss: 1.642 | Reg loss: 0.043 | Tree loss: 1.642 | Accuracy: 0.477816 | 0.919 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 490 | Batch: 000 / 011 | Total loss: 1.859 | Reg loss: 0.043 | Tree loss: 1.859 | Accuracy: 0.422500 | 0.919 sec/iter\n",
      "Epoch: 490 | Batch: 001 / 011 | Total loss: 1.846 | Reg loss: 0.043 | Tree loss: 1.846 | Accuracy: 0.391000 | 0.919 sec/iter\n",
      "Epoch: 490 | Batch: 002 / 011 | Total loss: 1.775 | Reg loss: 0.043 | Tree loss: 1.775 | Accuracy: 0.417500 | 0.919 sec/iter\n",
      "Epoch: 490 | Batch: 003 / 011 | Total loss: 1.773 | Reg loss: 0.043 | Tree loss: 1.773 | Accuracy: 0.423000 | 0.919 sec/iter\n",
      "Epoch: 490 | Batch: 004 / 011 | Total loss: 1.761 | Reg loss: 0.043 | Tree loss: 1.761 | Accuracy: 0.403500 | 0.919 sec/iter\n",
      "Epoch: 490 | Batch: 005 / 011 | Total loss: 1.697 | Reg loss: 0.043 | Tree loss: 1.697 | Accuracy: 0.421500 | 0.919 sec/iter\n",
      "Epoch: 490 | Batch: 006 / 011 | Total loss: 1.686 | Reg loss: 0.043 | Tree loss: 1.686 | Accuracy: 0.431500 | 0.919 sec/iter\n",
      "Epoch: 490 | Batch: 007 / 011 | Total loss: 1.686 | Reg loss: 0.043 | Tree loss: 1.686 | Accuracy: 0.433000 | 0.919 sec/iter\n",
      "Epoch: 490 | Batch: 008 / 011 | Total loss: 1.694 | Reg loss: 0.043 | Tree loss: 1.694 | Accuracy: 0.423000 | 0.919 sec/iter\n",
      "Epoch: 490 | Batch: 009 / 011 | Total loss: 1.689 | Reg loss: 0.043 | Tree loss: 1.689 | Accuracy: 0.431500 | 0.919 sec/iter\n",
      "Epoch: 490 | Batch: 010 / 011 | Total loss: 1.765 | Reg loss: 0.043 | Tree loss: 1.765 | Accuracy: 0.406143 | 0.919 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 491 | Batch: 000 / 011 | Total loss: 1.879 | Reg loss: 0.043 | Tree loss: 1.879 | Accuracy: 0.397000 | 0.919 sec/iter\n",
      "Epoch: 491 | Batch: 001 / 011 | Total loss: 1.831 | Reg loss: 0.043 | Tree loss: 1.831 | Accuracy: 0.416500 | 0.919 sec/iter\n",
      "Epoch: 491 | Batch: 002 / 011 | Total loss: 1.799 | Reg loss: 0.043 | Tree loss: 1.799 | Accuracy: 0.421500 | 0.919 sec/iter\n",
      "Epoch: 491 | Batch: 003 / 011 | Total loss: 1.763 | Reg loss: 0.043 | Tree loss: 1.763 | Accuracy: 0.432000 | 0.919 sec/iter\n",
      "Epoch: 491 | Batch: 004 / 011 | Total loss: 1.753 | Reg loss: 0.043 | Tree loss: 1.753 | Accuracy: 0.400000 | 0.919 sec/iter\n",
      "Epoch: 491 | Batch: 005 / 011 | Total loss: 1.721 | Reg loss: 0.043 | Tree loss: 1.721 | Accuracy: 0.422000 | 0.919 sec/iter\n",
      "Epoch: 491 | Batch: 006 / 011 | Total loss: 1.695 | Reg loss: 0.043 | Tree loss: 1.695 | Accuracy: 0.432000 | 0.919 sec/iter\n",
      "Epoch: 491 | Batch: 007 / 011 | Total loss: 1.703 | Reg loss: 0.043 | Tree loss: 1.703 | Accuracy: 0.421500 | 0.919 sec/iter\n",
      "Epoch: 491 | Batch: 008 / 011 | Total loss: 1.667 | Reg loss: 0.043 | Tree loss: 1.667 | Accuracy: 0.430500 | 0.919 sec/iter\n",
      "Epoch: 491 | Batch: 009 / 011 | Total loss: 1.672 | Reg loss: 0.043 | Tree loss: 1.672 | Accuracy: 0.432000 | 0.919 sec/iter\n",
      "Epoch: 491 | Batch: 010 / 011 | Total loss: 1.684 | Reg loss: 0.043 | Tree loss: 1.684 | Accuracy: 0.419795 | 0.919 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 492 | Batch: 000 / 011 | Total loss: 1.859 | Reg loss: 0.043 | Tree loss: 1.859 | Accuracy: 0.426500 | 0.919 sec/iter\n",
      "Epoch: 492 | Batch: 001 / 011 | Total loss: 1.805 | Reg loss: 0.043 | Tree loss: 1.805 | Accuracy: 0.435500 | 0.919 sec/iter\n",
      "Epoch: 492 | Batch: 002 / 011 | Total loss: 1.807 | Reg loss: 0.043 | Tree loss: 1.807 | Accuracy: 0.421500 | 0.919 sec/iter\n",
      "Epoch: 492 | Batch: 003 / 011 | Total loss: 1.798 | Reg loss: 0.043 | Tree loss: 1.798 | Accuracy: 0.416000 | 0.919 sec/iter\n",
      "Epoch: 492 | Batch: 004 / 011 | Total loss: 1.739 | Reg loss: 0.043 | Tree loss: 1.739 | Accuracy: 0.404500 | 0.919 sec/iter\n",
      "Epoch: 492 | Batch: 005 / 011 | Total loss: 1.730 | Reg loss: 0.043 | Tree loss: 1.730 | Accuracy: 0.419000 | 0.919 sec/iter\n",
      "Epoch: 492 | Batch: 006 / 011 | Total loss: 1.702 | Reg loss: 0.043 | Tree loss: 1.702 | Accuracy: 0.413000 | 0.919 sec/iter\n",
      "Epoch: 492 | Batch: 007 / 011 | Total loss: 1.704 | Reg loss: 0.043 | Tree loss: 1.704 | Accuracy: 0.408500 | 0.919 sec/iter\n",
      "Epoch: 492 | Batch: 008 / 011 | Total loss: 1.671 | Reg loss: 0.043 | Tree loss: 1.671 | Accuracy: 0.426500 | 0.919 sec/iter\n",
      "Epoch: 492 | Batch: 009 / 011 | Total loss: 1.677 | Reg loss: 0.043 | Tree loss: 1.677 | Accuracy: 0.427000 | 0.919 sec/iter\n",
      "Epoch: 492 | Batch: 010 / 011 | Total loss: 1.609 | Reg loss: 0.043 | Tree loss: 1.609 | Accuracy: 0.457338 | 0.919 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 493 | Batch: 000 / 011 | Total loss: 1.852 | Reg loss: 0.043 | Tree loss: 1.852 | Accuracy: 0.408500 | 0.919 sec/iter\n",
      "Epoch: 493 | Batch: 001 / 011 | Total loss: 1.793 | Reg loss: 0.043 | Tree loss: 1.793 | Accuracy: 0.444000 | 0.919 sec/iter\n",
      "Epoch: 493 | Batch: 002 / 011 | Total loss: 1.802 | Reg loss: 0.043 | Tree loss: 1.802 | Accuracy: 0.412500 | 0.919 sec/iter\n",
      "Epoch: 493 | Batch: 003 / 011 | Total loss: 1.758 | Reg loss: 0.043 | Tree loss: 1.758 | Accuracy: 0.421500 | 0.919 sec/iter\n",
      "Epoch: 493 | Batch: 004 / 011 | Total loss: 1.747 | Reg loss: 0.043 | Tree loss: 1.747 | Accuracy: 0.415500 | 0.919 sec/iter\n",
      "Epoch: 493 | Batch: 005 / 011 | Total loss: 1.700 | Reg loss: 0.043 | Tree loss: 1.700 | Accuracy: 0.424500 | 0.919 sec/iter\n",
      "Epoch: 493 | Batch: 006 / 011 | Total loss: 1.676 | Reg loss: 0.043 | Tree loss: 1.676 | Accuracy: 0.433000 | 0.919 sec/iter\n",
      "Epoch: 493 | Batch: 007 / 011 | Total loss: 1.729 | Reg loss: 0.043 | Tree loss: 1.729 | Accuracy: 0.408500 | 0.919 sec/iter\n",
      "Epoch: 493 | Batch: 008 / 011 | Total loss: 1.696 | Reg loss: 0.043 | Tree loss: 1.696 | Accuracy: 0.417500 | 0.919 sec/iter\n",
      "Epoch: 493 | Batch: 009 / 011 | Total loss: 1.712 | Reg loss: 0.043 | Tree loss: 1.712 | Accuracy: 0.406500 | 0.919 sec/iter\n",
      "Epoch: 493 | Batch: 010 / 011 | Total loss: 1.730 | Reg loss: 0.043 | Tree loss: 1.730 | Accuracy: 0.399317 | 0.919 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 494 | Batch: 000 / 011 | Total loss: 1.857 | Reg loss: 0.043 | Tree loss: 1.857 | Accuracy: 0.402500 | 0.919 sec/iter\n",
      "Epoch: 494 | Batch: 001 / 011 | Total loss: 1.833 | Reg loss: 0.043 | Tree loss: 1.833 | Accuracy: 0.418500 | 0.919 sec/iter\n",
      "Epoch: 494 | Batch: 002 / 011 | Total loss: 1.824 | Reg loss: 0.043 | Tree loss: 1.824 | Accuracy: 0.398000 | 0.919 sec/iter\n",
      "Epoch: 494 | Batch: 003 / 011 | Total loss: 1.768 | Reg loss: 0.043 | Tree loss: 1.768 | Accuracy: 0.428500 | 0.919 sec/iter\n",
      "Epoch: 494 | Batch: 004 / 011 | Total loss: 1.700 | Reg loss: 0.043 | Tree loss: 1.700 | Accuracy: 0.438000 | 0.919 sec/iter\n",
      "Epoch: 494 | Batch: 005 / 011 | Total loss: 1.714 | Reg loss: 0.043 | Tree loss: 1.714 | Accuracy: 0.436000 | 0.919 sec/iter\n",
      "Epoch: 494 | Batch: 006 / 011 | Total loss: 1.704 | Reg loss: 0.043 | Tree loss: 1.704 | Accuracy: 0.404000 | 0.919 sec/iter\n",
      "Epoch: 494 | Batch: 007 / 011 | Total loss: 1.714 | Reg loss: 0.043 | Tree loss: 1.714 | Accuracy: 0.409000 | 0.919 sec/iter\n",
      "Epoch: 494 | Batch: 008 / 011 | Total loss: 1.664 | Reg loss: 0.043 | Tree loss: 1.664 | Accuracy: 0.433500 | 0.919 sec/iter\n",
      "Epoch: 494 | Batch: 009 / 011 | Total loss: 1.690 | Reg loss: 0.043 | Tree loss: 1.690 | Accuracy: 0.426500 | 0.919 sec/iter\n",
      "Epoch: 494 | Batch: 010 / 011 | Total loss: 1.687 | Reg loss: 0.043 | Tree loss: 1.687 | Accuracy: 0.406143 | 0.919 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 495 | Batch: 000 / 011 | Total loss: 1.888 | Reg loss: 0.043 | Tree loss: 1.888 | Accuracy: 0.416500 | 0.919 sec/iter\n",
      "Epoch: 495 | Batch: 001 / 011 | Total loss: 1.831 | Reg loss: 0.043 | Tree loss: 1.831 | Accuracy: 0.421500 | 0.919 sec/iter\n",
      "Epoch: 495 | Batch: 002 / 011 | Total loss: 1.796 | Reg loss: 0.043 | Tree loss: 1.796 | Accuracy: 0.414500 | 0.919 sec/iter\n",
      "Epoch: 495 | Batch: 003 / 011 | Total loss: 1.760 | Reg loss: 0.043 | Tree loss: 1.760 | Accuracy: 0.426000 | 0.919 sec/iter\n",
      "Epoch: 495 | Batch: 004 / 011 | Total loss: 1.740 | Reg loss: 0.043 | Tree loss: 1.740 | Accuracy: 0.419500 | 0.919 sec/iter\n",
      "Epoch: 495 | Batch: 005 / 011 | Total loss: 1.704 | Reg loss: 0.043 | Tree loss: 1.704 | Accuracy: 0.422000 | 0.919 sec/iter\n",
      "Epoch: 495 | Batch: 006 / 011 | Total loss: 1.740 | Reg loss: 0.043 | Tree loss: 1.740 | Accuracy: 0.402500 | 0.919 sec/iter\n",
      "Epoch: 495 | Batch: 007 / 011 | Total loss: 1.684 | Reg loss: 0.043 | Tree loss: 1.684 | Accuracy: 0.418000 | 0.919 sec/iter\n",
      "Epoch: 495 | Batch: 008 / 011 | Total loss: 1.669 | Reg loss: 0.043 | Tree loss: 1.669 | Accuracy: 0.419500 | 0.919 sec/iter\n",
      "Epoch: 495 | Batch: 009 / 011 | Total loss: 1.659 | Reg loss: 0.043 | Tree loss: 1.659 | Accuracy: 0.427000 | 0.919 sec/iter\n",
      "Epoch: 495 | Batch: 010 / 011 | Total loss: 1.650 | Reg loss: 0.043 | Tree loss: 1.650 | Accuracy: 0.433447 | 0.919 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 496 | Batch: 000 / 011 | Total loss: 1.861 | Reg loss: 0.043 | Tree loss: 1.861 | Accuracy: 0.416500 | 0.919 sec/iter\n",
      "Epoch: 496 | Batch: 001 / 011 | Total loss: 1.800 | Reg loss: 0.043 | Tree loss: 1.800 | Accuracy: 0.430500 | 0.919 sec/iter\n",
      "Epoch: 496 | Batch: 002 / 011 | Total loss: 1.812 | Reg loss: 0.043 | Tree loss: 1.812 | Accuracy: 0.419500 | 0.919 sec/iter\n",
      "Epoch: 496 | Batch: 003 / 011 | Total loss: 1.752 | Reg loss: 0.043 | Tree loss: 1.752 | Accuracy: 0.423000 | 0.919 sec/iter\n",
      "Epoch: 496 | Batch: 004 / 011 | Total loss: 1.744 | Reg loss: 0.043 | Tree loss: 1.744 | Accuracy: 0.431000 | 0.919 sec/iter\n",
      "Epoch: 496 | Batch: 005 / 011 | Total loss: 1.733 | Reg loss: 0.043 | Tree loss: 1.733 | Accuracy: 0.407000 | 0.919 sec/iter\n",
      "Epoch: 496 | Batch: 006 / 011 | Total loss: 1.692 | Reg loss: 0.043 | Tree loss: 1.692 | Accuracy: 0.415500 | 0.919 sec/iter\n",
      "Epoch: 496 | Batch: 007 / 011 | Total loss: 1.686 | Reg loss: 0.043 | Tree loss: 1.686 | Accuracy: 0.419500 | 0.919 sec/iter\n",
      "Epoch: 496 | Batch: 008 / 011 | Total loss: 1.700 | Reg loss: 0.043 | Tree loss: 1.700 | Accuracy: 0.409000 | 0.919 sec/iter\n",
      "Epoch: 496 | Batch: 009 / 011 | Total loss: 1.687 | Reg loss: 0.043 | Tree loss: 1.687 | Accuracy: 0.431500 | 0.919 sec/iter\n",
      "Epoch: 496 | Batch: 010 / 011 | Total loss: 1.658 | Reg loss: 0.043 | Tree loss: 1.658 | Accuracy: 0.399317 | 0.918 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 497 | Batch: 000 / 011 | Total loss: 1.867 | Reg loss: 0.043 | Tree loss: 1.867 | Accuracy: 0.408000 | 0.919 sec/iter\n",
      "Epoch: 497 | Batch: 001 / 011 | Total loss: 1.825 | Reg loss: 0.043 | Tree loss: 1.825 | Accuracy: 0.420000 | 0.919 sec/iter\n",
      "Epoch: 497 | Batch: 002 / 011 | Total loss: 1.790 | Reg loss: 0.043 | Tree loss: 1.790 | Accuracy: 0.404500 | 0.919 sec/iter\n",
      "Epoch: 497 | Batch: 003 / 011 | Total loss: 1.788 | Reg loss: 0.043 | Tree loss: 1.788 | Accuracy: 0.396500 | 0.918 sec/iter\n",
      "Epoch: 497 | Batch: 004 / 011 | Total loss: 1.742 | Reg loss: 0.043 | Tree loss: 1.742 | Accuracy: 0.417500 | 0.918 sec/iter\n",
      "Epoch: 497 | Batch: 005 / 011 | Total loss: 1.691 | Reg loss: 0.043 | Tree loss: 1.691 | Accuracy: 0.438500 | 0.918 sec/iter\n",
      "Epoch: 497 | Batch: 006 / 011 | Total loss: 1.711 | Reg loss: 0.043 | Tree loss: 1.711 | Accuracy: 0.416000 | 0.918 sec/iter\n",
      "Epoch: 497 | Batch: 007 / 011 | Total loss: 1.702 | Reg loss: 0.043 | Tree loss: 1.702 | Accuracy: 0.411500 | 0.918 sec/iter\n",
      "Epoch: 497 | Batch: 008 / 011 | Total loss: 1.713 | Reg loss: 0.043 | Tree loss: 1.713 | Accuracy: 0.414000 | 0.918 sec/iter\n",
      "Epoch: 497 | Batch: 009 / 011 | Total loss: 1.668 | Reg loss: 0.043 | Tree loss: 1.668 | Accuracy: 0.437500 | 0.918 sec/iter\n",
      "Epoch: 497 | Batch: 010 / 011 | Total loss: 1.709 | Reg loss: 0.043 | Tree loss: 1.709 | Accuracy: 0.389078 | 0.918 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 498 | Batch: 000 / 011 | Total loss: 1.832 | Reg loss: 0.043 | Tree loss: 1.832 | Accuracy: 0.418500 | 0.918 sec/iter\n",
      "Epoch: 498 | Batch: 001 / 011 | Total loss: 1.834 | Reg loss: 0.043 | Tree loss: 1.834 | Accuracy: 0.412000 | 0.918 sec/iter\n",
      "Epoch: 498 | Batch: 002 / 011 | Total loss: 1.825 | Reg loss: 0.043 | Tree loss: 1.825 | Accuracy: 0.403500 | 0.918 sec/iter\n",
      "Epoch: 498 | Batch: 003 / 011 | Total loss: 1.763 | Reg loss: 0.043 | Tree loss: 1.763 | Accuracy: 0.428000 | 0.918 sec/iter\n",
      "Epoch: 498 | Batch: 004 / 011 | Total loss: 1.726 | Reg loss: 0.043 | Tree loss: 1.726 | Accuracy: 0.430000 | 0.918 sec/iter\n",
      "Epoch: 498 | Batch: 005 / 011 | Total loss: 1.707 | Reg loss: 0.043 | Tree loss: 1.707 | Accuracy: 0.440000 | 0.918 sec/iter\n",
      "Epoch: 498 | Batch: 006 / 011 | Total loss: 1.700 | Reg loss: 0.043 | Tree loss: 1.700 | Accuracy: 0.422500 | 0.918 sec/iter\n",
      "Epoch: 498 | Batch: 007 / 011 | Total loss: 1.690 | Reg loss: 0.043 | Tree loss: 1.690 | Accuracy: 0.418000 | 0.918 sec/iter\n",
      "Epoch: 498 | Batch: 008 / 011 | Total loss: 1.699 | Reg loss: 0.043 | Tree loss: 1.699 | Accuracy: 0.411000 | 0.918 sec/iter\n",
      "Epoch: 498 | Batch: 009 / 011 | Total loss: 1.683 | Reg loss: 0.043 | Tree loss: 1.683 | Accuracy: 0.417000 | 0.918 sec/iter\n",
      "Epoch: 498 | Batch: 010 / 011 | Total loss: 1.676 | Reg loss: 0.043 | Tree loss: 1.676 | Accuracy: 0.399317 | 0.918 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 499 | Batch: 000 / 011 | Total loss: 1.848 | Reg loss: 0.043 | Tree loss: 1.848 | Accuracy: 0.407500 | 0.918 sec/iter\n",
      "Epoch: 499 | Batch: 001 / 011 | Total loss: 1.834 | Reg loss: 0.043 | Tree loss: 1.834 | Accuracy: 0.406500 | 0.918 sec/iter\n",
      "Epoch: 499 | Batch: 002 / 011 | Total loss: 1.792 | Reg loss: 0.043 | Tree loss: 1.792 | Accuracy: 0.420000 | 0.918 sec/iter\n",
      "Epoch: 499 | Batch: 003 / 011 | Total loss: 1.752 | Reg loss: 0.043 | Tree loss: 1.752 | Accuracy: 0.435500 | 0.918 sec/iter\n",
      "Epoch: 499 | Batch: 004 / 011 | Total loss: 1.729 | Reg loss: 0.043 | Tree loss: 1.729 | Accuracy: 0.423500 | 0.918 sec/iter\n",
      "Epoch: 499 | Batch: 005 / 011 | Total loss: 1.730 | Reg loss: 0.043 | Tree loss: 1.730 | Accuracy: 0.414500 | 0.918 sec/iter\n",
      "Epoch: 499 | Batch: 006 / 011 | Total loss: 1.696 | Reg loss: 0.043 | Tree loss: 1.696 | Accuracy: 0.433000 | 0.918 sec/iter\n",
      "Epoch: 499 | Batch: 007 / 011 | Total loss: 1.715 | Reg loss: 0.043 | Tree loss: 1.715 | Accuracy: 0.405500 | 0.918 sec/iter\n",
      "Epoch: 499 | Batch: 008 / 011 | Total loss: 1.692 | Reg loss: 0.043 | Tree loss: 1.692 | Accuracy: 0.433500 | 0.918 sec/iter\n",
      "Epoch: 499 | Batch: 009 / 011 | Total loss: 1.700 | Reg loss: 0.043 | Tree loss: 1.700 | Accuracy: 0.415500 | 0.918 sec/iter\n",
      "Epoch: 499 | Batch: 010 / 011 | Total loss: 1.667 | Reg loss: 0.043 | Tree loss: 1.667 | Accuracy: 0.426621 | 0.918 sec/iter\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "iteration = 0\n",
    "for epoch in range(epochs):\n",
    "    # Training\n",
    "    avg_sp = show_sparseness(tree)\n",
    "    sparsity.append(avg_sp)\n",
    "    iteration = do_epoch(tree, tree_loader, device, log_interval, losses, accs, epoch, iteration)\n",
    "    \n",
    "    if epoch % 1 == 0:\n",
    "        prune_tree(tree, factor=5)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel('Iteration')\n",
    "plt.plot(accs, label='Accuracy vs iteration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel('Iteration')\n",
    "plt.plot(losses, label='Loss vs iteration')\n",
    "plt.yscale(\"log\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "weights = tree.inner_nodes.weight.cpu().detach().numpy().flatten()\n",
    "plt.hist(weights, bins=500)\n",
    "weights_std = np.std(weights)\n",
    "weights_mean = np.mean(weights)\n",
    "plt.axvline(weights_mean + weights_std, color='r')\n",
    "plt.axvline(weights_mean - weights_std, color='r')\n",
    "plt.title(f\"Mean: {weights_mean}   |   STD: {weights_std}\")\n",
    "plt.yscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/usr/bin/python3'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average height: 7.763440860215054\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1200x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(15, 10), dpi=80)\n",
    "avg_height, root = tree.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accumulate samples in the leaves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of patterns: 93\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of patterns: {len(root.get_leaves())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = 'MLE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "root.clear_leaves_samples()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (data, target) in enumerate(tree_loader):\n",
    "        root.accumulate_samples(data, method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tighten boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Pattern 1 ==============\n",
      "============== Pattern 2 ==============\n",
      "============== Pattern 3 ==============\n",
      "============== Pattern 4 ==============\n",
      "============== Pattern 5 ==============\n",
      "============== Pattern 6 ==============\n",
      "============== Pattern 7 ==============\n",
      "131\n",
      "============== Pattern 8 ==============\n",
      "66\n",
      "============== Pattern 9 ==============\n",
      "============== Pattern 10 ==============\n",
      "3\n",
      "============== Pattern 11 ==============\n",
      "============== Pattern 12 ==============\n",
      "============== Pattern 13 ==============\n",
      "============== Pattern 14 ==============\n",
      "============== Pattern 15 ==============\n",
      "============== Pattern 16 ==============\n",
      "============== Pattern 17 ==============\n",
      "============== Pattern 18 ==============\n",
      "============== Pattern 19 ==============\n",
      "============== Pattern 20 ==============\n",
      "============== Pattern 21 ==============\n",
      "============== Pattern 22 ==============\n",
      "============== Pattern 23 ==============\n",
      "============== Pattern 24 ==============\n",
      "============== Pattern 25 ==============\n",
      "============== Pattern 26 ==============\n",
      "============== Pattern 27 ==============\n",
      "============== Pattern 28 ==============\n",
      "16329\n",
      "============== Pattern 29 ==============\n",
      "============== Pattern 30 ==============\n",
      "============== Pattern 31 ==============\n",
      "============== Pattern 32 ==============\n",
      "============== Pattern 33 ==============\n",
      "============== Pattern 34 ==============\n",
      "============== Pattern 35 ==============\n",
      "============== Pattern 36 ==============\n",
      "============== Pattern 37 ==============\n",
      "============== Pattern 38 ==============\n",
      "============== Pattern 39 ==============\n",
      "============== Pattern 40 ==============\n",
      "============== Pattern 41 ==============\n",
      "============== Pattern 42 ==============\n",
      "============== Pattern 43 ==============\n",
      "============== Pattern 44 ==============\n",
      "============== Pattern 45 ==============\n",
      "============== Pattern 46 ==============\n",
      "============== Pattern 47 ==============\n",
      "============== Pattern 48 ==============\n",
      "============== Pattern 49 ==============\n",
      "============== Pattern 50 ==============\n",
      "3232\n",
      "============== Pattern 51 ==============\n",
      "============== Pattern 52 ==============\n",
      "============== Pattern 53 ==============\n",
      "============== Pattern 54 ==============\n",
      "============== Pattern 55 ==============\n",
      "============== Pattern 56 ==============\n",
      "============== Pattern 57 ==============\n",
      "============== Pattern 58 ==============\n",
      "============== Pattern 59 ==============\n",
      "============== Pattern 60 ==============\n",
      "============== Pattern 61 ==============\n",
      "============== Pattern 62 ==============\n",
      "532\n",
      "============== Pattern 63 ==============\n",
      "============== Pattern 64 ==============\n",
      "============== Pattern 65 ==============\n",
      "============== Pattern 66 ==============\n",
      "============== Pattern 67 ==============\n",
      "============== Pattern 68 ==============\n",
      "============== Pattern 69 ==============\n",
      "============== Pattern 70 ==============\n",
      "============== Pattern 71 ==============\n",
      "============== Pattern 72 ==============\n",
      "============== Pattern 73 ==============\n",
      "============== Pattern 74 ==============\n",
      "============== Pattern 75 ==============\n",
      "============== Pattern 76 ==============\n",
      "============== Pattern 77 ==============\n",
      "============== Pattern 78 ==============\n",
      "============== Pattern 79 ==============\n",
      "============== Pattern 80 ==============\n",
      "============== Pattern 81 ==============\n",
      "============== Pattern 82 ==============\n",
      "============== Pattern 83 ==============\n",
      "============== Pattern 84 ==============\n",
      "============== Pattern 85 ==============\n",
      "============== Pattern 86 ==============\n",
      "============== Pattern 87 ==============\n",
      "============== Pattern 88 ==============\n",
      "============== Pattern 89 ==============\n",
      "============== Pattern 90 ==============\n",
      "============== Pattern 91 ==============\n",
      "============== Pattern 92 ==============\n",
      "============== Pattern 93 ==============\n",
      "Average comprehensibility: 73.91397849462365\n",
      "std comprehensibility: 17.43044914194296\n",
      "var comprehensibility: 303.8205572898601\n",
      "minimum comprehensibility: 30\n",
      "maximum comprehensibility: 100\n"
     ]
    }
   ],
   "source": [
    "signal_names = dataset.dataset.all_signals\n",
    "normalizers = torch.tensor([])\n",
    "attr_names = []\n",
    "for signal_name in signal_names:\n",
    "    attr_names += [f\"T{i}.{signal_name}\" for i in range(sampled.shape[-1])]\n",
    "    sensor_norm = torch.tensor([torch.tensor(dataset.dataset.sensor_maxs[signal_name]) for _ in range(sampled.shape[-1])])\n",
    "    normalizers = torch.cat([normalizers, sensor_norm])\n",
    "    \n",
    "\n",
    "# print(attr_names)\n",
    "leaves = root.get_leaves()\n",
    "sum_comprehensibility = 0\n",
    "comprehensibilities = []\n",
    "for pattern_counter, leaf in enumerate(leaves):\n",
    "    leaf.reset_path()\n",
    "    leaf.tighten_with_accumulated_samples()\n",
    "    conds = leaf.get_path_conditions(attr_names)\n",
    "    for cond in conds:\n",
    "        cond.weights = cond.weights / normalizers\n",
    "    print(f\"============== Pattern {pattern_counter + 1} ==============\")\n",
    "    comprehensibilities.append(sum([cond.comprehensibility for cond in conds]))\n",
    "    \n",
    "print(f\"Average comprehensibility: {np.mean(comprehensibilities)}\")\n",
    "print(f\"std comprehensibility: {np.std(comprehensibilities)}\")\n",
    "print(f\"var comprehensibility: {np.var(comprehensibilities)}\")\n",
    "print(f\"minimum comprehensibility: {np.min(comprehensibilities)}\")\n",
    "print(f\"maximum comprehensibility: {np.max(comprehensibilities)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
