{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "%matplotlib inline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "import time\n",
    "import copy\n",
    "import os\n",
    "from queue import LifoQueue\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "from scipy.stats import kurtosis\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import network.cpc\n",
    "from network.cpc import CDCK2\n",
    "from utils.MatplotlibUtils import reduce_dims_and_plot\n",
    "from utils.ClassificationUtiols import onehot_coding\n",
    "from soft_decision_tree.sdt_model import SDT\n",
    "from sklearn import tree as tt\n",
    "\n",
    "# IPython.OutputArea.auto_scroll_threshold = 9999;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the model and the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load the model from: /home/eitan.k/EntangledExplainableClustering/knn_loss_batch_512_k_128/models/epoch_40.pt\n",
      "sensor names: (18 total)\n",
      "- speed\n",
      "- steering_angle\n",
      "- wheel_speed_0\n",
      "- wheel_speed_1\n",
      "- wheel_speed_2\n",
      "- wheel_speed_3\n",
      "- accelerometer_0\n",
      "- accelerometer_1\n",
      "- accelerometer_2\n",
      "- gyro_0\n",
      "- gyro_1\n",
      "- gyro_2\n",
      "- gyro_bias_0\n",
      "- gyro_bias_1\n",
      "- gyro_bias_2\n",
      "- gyro_uncalibrated_0\n",
      "- gyro_uncalibrated_1\n",
      "- gyro_uncalibrated_2\n",
      "Multihorizon size of the model: 30\n",
      "Test split ratio: 0.2\n",
      "Total number of windows in the dataset (without splitting): 101465\n"
     ]
    }
   ],
   "source": [
    "model_path = r'/home/eitan.k/EntangledExplainableClustering/knn_loss_batch_512_k_128/models/epoch_40.pt'\n",
    "dataset_path = r'/home/eitan.k/EntangledExplainableClustering/knn_loss_batch_512_k_128/data/test_data_protocol_4.file'\n",
    "\n",
    "print(f\"Load the model from: {model_path}\")\n",
    "model = torch.load(model_path, map_location='cpu')\n",
    "\n",
    "with open(dataset_path, 'rb') as fp:\n",
    "    dataset = pickle.load(fp)\n",
    "\n",
    "all_sensors = dataset.dataset.all_signals    \n",
    "print(f\"sensor names: ({len(all_sensors)} total)\")\n",
    "\n",
    "for s in all_sensors:\n",
    "    print(f\"- {s}\")\n",
    "    \n",
    "print(f\"Multihorizon size of the model: {model.timestep}\")\n",
    "print(f\"Test split ratio: {len(dataset) / len(dataset.dataset)}\")\n",
    "print(f\"Total number of windows in the dataset (without splitting): {len(dataset.dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 500\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Extract representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ebb7dee363e482f93e785e788b771cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=20293.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "projections = torch.tensor([])\n",
    "samples = torch.tensor([])\n",
    "device = 'cuda'\n",
    "model = model.to(device).eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    bar = tqdm(total=len(loader.dataset))\n",
    "    for batch in loader:\n",
    "        hidden = CDCK2.init_hidden(len(batch))\n",
    "        batch = batch.to(device)\n",
    "        hidden = hidden.to(device)\n",
    "\n",
    "        y = model.predict(batch, hidden).detach().cpu()\n",
    "        projections = torch.cat([projections, y.detach().cpu()])\n",
    "        samples = torch.cat([samples, batch.detach().cpu()])\n",
    "        bar.update(y.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit GMM and calculate indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0be00269ab16455283f71ac1861702ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=15.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "best_score = float('inf')\n",
    "clusters = None\n",
    "range_ = list(range(5, 20))\n",
    "for k in tqdm(range_):\n",
    "    y = GaussianMixture(n_components=k).fit_predict(projections)\n",
    "    cur_score = davies_bouldin_score(projections, y)\n",
    "    scores.append(cur_score)\n",
    "    \n",
    "    if cur_score < best_score:\n",
    "        best_score = cur_score\n",
    "        clusters = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1, 2, 3, 4}\n"
     ]
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('DB Score')\n",
    "plt.plot(range_, scores)\n",
    "best_k = range_[np.argmin(scores)]\n",
    "plt.axvline(best_k, color='r')\n",
    "plt.show()\n",
    "\n",
    "labels = set(clusters)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "distances = pairwise_distances(projections)\n",
    "# distances = np.triu(distances)\n",
    "distances_f = distances.flatten()\n",
    "\n",
    "plt.matshow(distances)\n",
    "plt.colorbar()\n",
    "plt.figure()\n",
    "plt.hist(distances_f[distances_f > 0], bins=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize with T-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "perplexity = 200\n",
    "\n",
    "p = reduce_dims_and_plot(projections,\n",
    "                         y=clusters,\n",
    "                         title=f'perplexity: {perplexity}',\n",
    "                         file_name=None,\n",
    "                         perplexity=perplexity,\n",
    "                         library='Multicore-TSNE',\n",
    "                         perform_PCA=False,\n",
    "                         projected=None,\n",
    "                         figure_type='2d',\n",
    "                         show_figure=True,\n",
    "                         close_figure=False,\n",
    "                         text=None)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Soft-Decision-Tree given the self-labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unnormalized_samples = samples.clone()\n",
    "\n",
    "# for col, sensor in enumerate(tqdm(dataset.dataset.all_signals)):\n",
    "#     denormalizer = dataset.dataset.get_denormalization_for_sensor(sensor)\n",
    "#     unnormalized_samples[:, col, :] = denormalizer(unnormalized_samples[:, col, :])\n",
    "\n",
    "sampled = samples[..., range(0, samples.shape[-1], 200)]\n",
    "\n",
    "samples_f = sampled.flatten(1)\n",
    "tree_dataset = list(zip(samples_f, clusters))\n",
    "batch_size = 2000\n",
    "tree_loader = torch.utils.data.DataLoader(tree_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define how we prune the weights of a node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_node(node_weights, factor=1):\n",
    "    w = node_weights.cpu().detach().numpy()\n",
    "    mean_ = np.mean(w)\n",
    "    std_ = np.std(w)\n",
    "    node_weights[((mean_ - std_ * factor) < node_weights) & (node_weights < (mean_ + std_ * factor))] = 0\n",
    "    return node_weights\n",
    "\n",
    "def prune_node_keep(node_weights, keep=4):\n",
    "    w = node_weights.cpu().detach().numpy()\n",
    "    throw_idx = np.argsort(abs(w))[:-keep]\n",
    "    node_weights[throw_idx] = 0\n",
    "    return node_weights\n",
    "\n",
    "def prune_tree(tree_, factor):\n",
    "    new_weights = tree_.inner_nodes.weight.clone()\n",
    "    for i in range(new_weights.shape[0]):\n",
    "        res = prune_node_keep(new_weights[i, :], factor)\n",
    "        new_weights[i, :] = res\n",
    "\n",
    "    with torch.no_grad():\n",
    "        tree_.inner_nodes.weight.copy_(new_weights)\n",
    "        \n",
    "def sparseness(x):\n",
    "    s = []\n",
    "    for i in range(x.shape[0]):\n",
    "        x_ = x[i, :]\n",
    "        sp = (len(x_) - torch.norm(x_, 0).item()) / len(x_)\n",
    "        s.append(sp)\n",
    "    return np.mean(s)\n",
    "\n",
    "def compute_regularization_by_level(tree):\n",
    "    total_reg = 0\n",
    "    for i in range(tree.inner_nodes.weight.shape[0]):\n",
    "        cur_level = np.floor(np.log2(i+1))\n",
    "        node_reg = torch.norm(tree.inner_nodes.weight[i].view(-1), 2)\n",
    "        total_reg += 2**(-cur_level) * node_reg\n",
    "    return total_reg\n",
    "\n",
    "def show_sparseness(tree):\n",
    "    avg_sp = sparseness(tree.inner_nodes.weight)\n",
    "    print(f\"Average sparseness: {avg_sp}\")\n",
    "    layer = 0\n",
    "    sps = []\n",
    "    for i in range(tree.inner_nodes.weight.shape[0]):\n",
    "        cur_layer = int(np.floor(np.log2(i+1)))\n",
    "        if cur_layer != layer:\n",
    "            print(f\"layer {layer}: {np.mean(sps)}\")\n",
    "            sps = []\n",
    "            layer = cur_layer\n",
    "\n",
    "        x_ = tree.inner_nodes.weight[i, :]\n",
    "        sp = (len(x_) - torch.norm(x_, 0).item()) / len(x_)\n",
    "        sps.append(sp)\n",
    "        \n",
    "    return avg_sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_epoch(model, loader, device, log_interval, losses, accs, epoch, iteration):\n",
    "    model = model.train()\n",
    "    for batch_idx, (data, target) in enumerate(loader):\n",
    "        iteration += 1\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        output, penalty = tree.forward(data)\n",
    "\n",
    "        # Loss\n",
    "        loss_tree = criterion(output, target.view(-1))\n",
    "\n",
    "        # Penalty\n",
    "        loss_tree += penalty\n",
    "\n",
    "        # Sparse regularization\n",
    "#         fc_params = torch.cat([x.view(-1) for x in tree.inner_nodes.parameters()])\n",
    "#         regularization = sparsity_lamda * torch.norm(fc_params, 2)\n",
    "        regularization = sparsity_lamda * compute_regularization_by_level(tree)\n",
    "        loss = loss_tree\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        pred = output.data.max(1)[1]\n",
    "        correct = pred.eq(target.view(-1).data).sum()\n",
    "        accs.append(correct.item() / data.size()[0])\n",
    "\n",
    "        # Print training status\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(f\"Epoch: {epoch:02d} | Batch: {batch_idx:03d} / {len(loader):03d} | Total loss: {loss.item():.3f} | Reg loss: {regularization.item():.3f} | Tree loss: {loss_tree.item():.3f} | Accuracy: {correct.item() / data.size()[0]:03f} | {round((time.time() - start_time) / iteration, 3)} sec/iter\")\n",
    "            \n",
    "    return iteration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 5e-3\n",
    "weight_decay = 5e-4\n",
    "sparsity_lamda = 2e-3\n",
    "epochs = 500\n",
    "output_dim = len(set(clusters))\n",
    "log_interval = 1\n",
    "tree_depth = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT accuracy: 0.9809786625930124\n"
     ]
    }
   ],
   "source": [
    "tree = SDT(input_dim=samples_f.shape[1], output_dim=len(labels), depth=tree_depth, lamda=1e-3, use_cuda=True)\n",
    "optimizer = torch.optim.Adam(tree.parameters(),\n",
    "                                 lr=lr,\n",
    "                                 weight_decay=weight_decay)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "tree = tree.to(device)\n",
    "clf = DecisionTreeClassifier(max_depth=tree_depth).fit(samples_f, clusters)\n",
    "print(f\"DT accuracy: {clf.score(samples_f, clusters)}\")\n",
    "tree.initialize_from_decision_tree(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "accs = []\n",
    "sparsity = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.21654434201217304\n",
      "layer 0: 0.988950276243094\n",
      "layer 1: 0.988950276243094\n",
      "layer 2: 0.988950276243094\n",
      "layer 3: 0.988950276243094\n",
      "layer 4: 0.988950276243094\n",
      "layer 5: 0.9271408839779005\n",
      "layer 6: 0.6180939226519337\n",
      "layer 7: 0.3399516574585636\n",
      "layer 8: 0.158386567679558\n",
      "Epoch: 00 | Batch: 000 / 011 | Total loss: 1.610 | Reg loss: 0.019 | Tree loss: 1.610 | Accuracy: 0.289000 | 5.613 sec/iter\n",
      "Epoch: 00 | Batch: 001 / 011 | Total loss: 1.592 | Reg loss: 0.018 | Tree loss: 1.592 | Accuracy: 0.286500 | 3.261 sec/iter\n",
      "Epoch: 00 | Batch: 002 / 011 | Total loss: 1.578 | Reg loss: 0.018 | Tree loss: 1.578 | Accuracy: 0.393000 | 2.622 sec/iter\n",
      "Epoch: 00 | Batch: 003 / 011 | Total loss: 1.556 | Reg loss: 0.018 | Tree loss: 1.556 | Accuracy: 0.399000 | 2.268 sec/iter\n",
      "Epoch: 00 | Batch: 004 / 011 | Total loss: 1.540 | Reg loss: 0.018 | Tree loss: 1.540 | Accuracy: 0.398500 | 2.066 sec/iter\n",
      "Epoch: 00 | Batch: 005 / 011 | Total loss: 1.537 | Reg loss: 0.018 | Tree loss: 1.537 | Accuracy: 0.370000 | 1.923 sec/iter\n",
      "Epoch: 00 | Batch: 006 / 011 | Total loss: 1.530 | Reg loss: 0.018 | Tree loss: 1.530 | Accuracy: 0.374500 | 1.814 sec/iter\n",
      "Epoch: 00 | Batch: 007 / 011 | Total loss: 1.522 | Reg loss: 0.018 | Tree loss: 1.522 | Accuracy: 0.363500 | 1.732 sec/iter\n",
      "Epoch: 00 | Batch: 008 / 011 | Total loss: 1.515 | Reg loss: 0.017 | Tree loss: 1.515 | Accuracy: 0.375000 | 1.682 sec/iter\n",
      "Epoch: 00 | Batch: 009 / 011 | Total loss: 1.506 | Reg loss: 0.017 | Tree loss: 1.506 | Accuracy: 0.385000 | 1.652 sec/iter\n",
      "Epoch: 00 | Batch: 010 / 011 | Total loss: 1.512 | Reg loss: 0.017 | Tree loss: 1.512 | Accuracy: 0.354949 | 1.611 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 01 | Batch: 000 / 011 | Total loss: 1.589 | Reg loss: 0.016 | Tree loss: 1.589 | Accuracy: 0.331000 | 1.709 sec/iter\n",
      "Epoch: 01 | Batch: 001 / 011 | Total loss: 1.581 | Reg loss: 0.016 | Tree loss: 1.581 | Accuracy: 0.328500 | 1.647 sec/iter\n",
      "Epoch: 01 | Batch: 002 / 011 | Total loss: 1.568 | Reg loss: 0.016 | Tree loss: 1.568 | Accuracy: 0.397000 | 1.593 sec/iter\n",
      "Epoch: 01 | Batch: 003 / 011 | Total loss: 1.555 | Reg loss: 0.016 | Tree loss: 1.555 | Accuracy: 0.458500 | 1.547 sec/iter\n",
      "Epoch: 01 | Batch: 004 / 011 | Total loss: 1.539 | Reg loss: 0.016 | Tree loss: 1.539 | Accuracy: 0.471000 | 1.508 sec/iter\n",
      "Epoch: 01 | Batch: 005 / 011 | Total loss: 1.533 | Reg loss: 0.016 | Tree loss: 1.533 | Accuracy: 0.446500 | 1.472 sec/iter\n",
      "Epoch: 01 | Batch: 006 / 011 | Total loss: 1.513 | Reg loss: 0.016 | Tree loss: 1.513 | Accuracy: 0.459000 | 1.44 sec/iter\n",
      "Epoch: 01 | Batch: 007 / 011 | Total loss: 1.504 | Reg loss: 0.017 | Tree loss: 1.504 | Accuracy: 0.432500 | 1.417 sec/iter\n",
      "Epoch: 01 | Batch: 008 / 011 | Total loss: 1.503 | Reg loss: 0.017 | Tree loss: 1.503 | Accuracy: 0.401500 | 1.4 sec/iter\n",
      "Epoch: 01 | Batch: 009 / 011 | Total loss: 1.500 | Reg loss: 0.017 | Tree loss: 1.500 | Accuracy: 0.386500 | 1.389 sec/iter\n",
      "Epoch: 01 | Batch: 010 / 011 | Total loss: 1.491 | Reg loss: 0.017 | Tree loss: 1.491 | Accuracy: 0.402730 | 1.367 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 02 | Batch: 000 / 011 | Total loss: 1.580 | Reg loss: 0.016 | Tree loss: 1.580 | Accuracy: 0.330000 | 1.421 sec/iter\n",
      "Epoch: 02 | Batch: 001 / 011 | Total loss: 1.563 | Reg loss: 0.016 | Tree loss: 1.563 | Accuracy: 0.435000 | 1.421 sec/iter\n",
      "Epoch: 02 | Batch: 002 / 011 | Total loss: 1.553 | Reg loss: 0.016 | Tree loss: 1.553 | Accuracy: 0.436500 | 1.42 sec/iter\n",
      "Epoch: 02 | Batch: 003 / 011 | Total loss: 1.540 | Reg loss: 0.016 | Tree loss: 1.540 | Accuracy: 0.432000 | 1.413 sec/iter\n",
      "Epoch: 02 | Batch: 004 / 011 | Total loss: 1.533 | Reg loss: 0.016 | Tree loss: 1.533 | Accuracy: 0.406500 | 1.41 sec/iter\n",
      "Epoch: 02 | Batch: 005 / 011 | Total loss: 1.513 | Reg loss: 0.016 | Tree loss: 1.513 | Accuracy: 0.428500 | 1.402 sec/iter\n",
      "Epoch: 02 | Batch: 006 / 011 | Total loss: 1.503 | Reg loss: 0.016 | Tree loss: 1.503 | Accuracy: 0.402000 | 1.399 sec/iter\n",
      "Epoch: 02 | Batch: 007 / 011 | Total loss: 1.491 | Reg loss: 0.016 | Tree loss: 1.491 | Accuracy: 0.397500 | 1.395 sec/iter\n",
      "Epoch: 02 | Batch: 008 / 011 | Total loss: 1.484 | Reg loss: 0.016 | Tree loss: 1.484 | Accuracy: 0.393500 | 1.394 sec/iter\n",
      "Epoch: 02 | Batch: 009 / 011 | Total loss: 1.478 | Reg loss: 0.017 | Tree loss: 1.478 | Accuracy: 0.396000 | 1.392 sec/iter\n",
      "Epoch: 02 | Batch: 010 / 011 | Total loss: 1.498 | Reg loss: 0.017 | Tree loss: 1.498 | Accuracy: 0.337884 | 1.385 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 03 | Batch: 000 / 011 | Total loss: 1.565 | Reg loss: 0.015 | Tree loss: 1.565 | Accuracy: 0.367000 | 1.466 sec/iter\n",
      "Epoch: 03 | Batch: 001 / 011 | Total loss: 1.554 | Reg loss: 0.016 | Tree loss: 1.554 | Accuracy: 0.387500 | 1.461 sec/iter\n",
      "Epoch: 03 | Batch: 002 / 011 | Total loss: 1.545 | Reg loss: 0.016 | Tree loss: 1.545 | Accuracy: 0.401000 | 1.454 sec/iter\n",
      "Epoch: 03 | Batch: 003 / 011 | Total loss: 1.528 | Reg loss: 0.016 | Tree loss: 1.528 | Accuracy: 0.438500 | 1.457 sec/iter\n",
      "Epoch: 03 | Batch: 004 / 011 | Total loss: 1.513 | Reg loss: 0.016 | Tree loss: 1.513 | Accuracy: 0.447000 | 1.455 sec/iter\n",
      "Epoch: 03 | Batch: 005 / 011 | Total loss: 1.505 | Reg loss: 0.016 | Tree loss: 1.505 | Accuracy: 0.417000 | 1.455 sec/iter\n",
      "Epoch: 03 | Batch: 006 / 011 | Total loss: 1.489 | Reg loss: 0.016 | Tree loss: 1.489 | Accuracy: 0.414000 | 1.449 sec/iter\n",
      "Epoch: 03 | Batch: 007 / 011 | Total loss: 1.480 | Reg loss: 0.016 | Tree loss: 1.480 | Accuracy: 0.392500 | 1.44 sec/iter\n",
      "Epoch: 03 | Batch: 008 / 011 | Total loss: 1.484 | Reg loss: 0.016 | Tree loss: 1.484 | Accuracy: 0.374500 | 1.435 sec/iter\n",
      "Epoch: 03 | Batch: 009 / 011 | Total loss: 1.490 | Reg loss: 0.017 | Tree loss: 1.490 | Accuracy: 0.351500 | 1.428 sec/iter\n",
      "Epoch: 03 | Batch: 010 / 011 | Total loss: 1.486 | Reg loss: 0.017 | Tree loss: 1.486 | Accuracy: 0.361775 | 1.419 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 04 | Batch: 000 / 011 | Total loss: 1.556 | Reg loss: 0.016 | Tree loss: 1.556 | Accuracy: 0.384500 | 1.475 sec/iter\n",
      "Epoch: 04 | Batch: 001 / 011 | Total loss: 1.543 | Reg loss: 0.016 | Tree loss: 1.543 | Accuracy: 0.415000 | 1.47 sec/iter\n",
      "Epoch: 04 | Batch: 002 / 011 | Total loss: 1.533 | Reg loss: 0.016 | Tree loss: 1.533 | Accuracy: 0.410500 | 1.465 sec/iter\n",
      "Epoch: 04 | Batch: 003 / 011 | Total loss: 1.517 | Reg loss: 0.016 | Tree loss: 1.517 | Accuracy: 0.433000 | 1.46 sec/iter\n",
      "Epoch: 04 | Batch: 004 / 011 | Total loss: 1.512 | Reg loss: 0.016 | Tree loss: 1.512 | Accuracy: 0.424000 | 1.46 sec/iter\n",
      "Epoch: 04 | Batch: 005 / 011 | Total loss: 1.490 | Reg loss: 0.016 | Tree loss: 1.490 | Accuracy: 0.443000 | 1.459 sec/iter\n",
      "Epoch: 04 | Batch: 006 / 011 | Total loss: 1.497 | Reg loss: 0.016 | Tree loss: 1.497 | Accuracy: 0.393000 | 1.46 sec/iter\n",
      "Epoch: 04 | Batch: 007 / 011 | Total loss: 1.478 | Reg loss: 0.016 | Tree loss: 1.478 | Accuracy: 0.391500 | 1.456 sec/iter\n",
      "Epoch: 04 | Batch: 008 / 011 | Total loss: 1.464 | Reg loss: 0.017 | Tree loss: 1.464 | Accuracy: 0.385000 | 1.451 sec/iter\n",
      "Epoch: 04 | Batch: 009 / 011 | Total loss: 1.450 | Reg loss: 0.017 | Tree loss: 1.450 | Accuracy: 0.394500 | 1.445 sec/iter\n",
      "Epoch: 04 | Batch: 010 / 011 | Total loss: 1.447 | Reg loss: 0.017 | Tree loss: 1.447 | Accuracy: 0.378840 | 1.435 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 05 | Batch: 000 / 011 | Total loss: 1.550 | Reg loss: 0.016 | Tree loss: 1.550 | Accuracy: 0.379500 | 1.491 sec/iter\n",
      "Epoch: 05 | Batch: 001 / 011 | Total loss: 1.543 | Reg loss: 0.016 | Tree loss: 1.543 | Accuracy: 0.368500 | 1.487 sec/iter\n",
      "Epoch: 05 | Batch: 002 / 011 | Total loss: 1.518 | Reg loss: 0.016 | Tree loss: 1.518 | Accuracy: 0.431000 | 1.485 sec/iter\n",
      "Epoch: 05 | Batch: 003 / 011 | Total loss: 1.514 | Reg loss: 0.016 | Tree loss: 1.514 | Accuracy: 0.403000 | 1.484 sec/iter\n",
      "Epoch: 05 | Batch: 004 / 011 | Total loss: 1.501 | Reg loss: 0.016 | Tree loss: 1.501 | Accuracy: 0.406000 | 1.482 sec/iter\n",
      "Epoch: 05 | Batch: 005 / 011 | Total loss: 1.477 | Reg loss: 0.016 | Tree loss: 1.477 | Accuracy: 0.440500 | 1.478 sec/iter\n",
      "Epoch: 05 | Batch: 006 / 011 | Total loss: 1.462 | Reg loss: 0.016 | Tree loss: 1.462 | Accuracy: 0.420000 | 1.474 sec/iter\n",
      "Epoch: 05 | Batch: 007 / 011 | Total loss: 1.449 | Reg loss: 0.017 | Tree loss: 1.449 | Accuracy: 0.411500 | 1.469 sec/iter\n",
      "Epoch: 05 | Batch: 008 / 011 | Total loss: 1.453 | Reg loss: 0.017 | Tree loss: 1.453 | Accuracy: 0.385000 | 1.466 sec/iter\n",
      "Epoch: 05 | Batch: 009 / 011 | Total loss: 1.429 | Reg loss: 0.017 | Tree loss: 1.429 | Accuracy: 0.401500 | 1.464 sec/iter\n",
      "Epoch: 05 | Batch: 010 / 011 | Total loss: 1.399 | Reg loss: 0.017 | Tree loss: 1.399 | Accuracy: 0.443686 | 1.457 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 06 | Batch: 000 / 011 | Total loss: 1.542 | Reg loss: 0.016 | Tree loss: 1.542 | Accuracy: 0.371500 | 1.495 sec/iter\n",
      "Epoch: 06 | Batch: 001 / 011 | Total loss: 1.529 | Reg loss: 0.016 | Tree loss: 1.529 | Accuracy: 0.366000 | 1.491 sec/iter\n",
      "Epoch: 06 | Batch: 002 / 011 | Total loss: 1.516 | Reg loss: 0.016 | Tree loss: 1.516 | Accuracy: 0.380000 | 1.487 sec/iter\n",
      "Epoch: 06 | Batch: 003 / 011 | Total loss: 1.492 | Reg loss: 0.016 | Tree loss: 1.492 | Accuracy: 0.422500 | 1.484 sec/iter\n",
      "Epoch: 06 | Batch: 004 / 011 | Total loss: 1.478 | Reg loss: 0.016 | Tree loss: 1.478 | Accuracy: 0.434500 | 1.481 sec/iter\n",
      "Epoch: 06 | Batch: 005 / 011 | Total loss: 1.463 | Reg loss: 0.016 | Tree loss: 1.463 | Accuracy: 0.450500 | 1.479 sec/iter\n",
      "Epoch: 06 | Batch: 006 / 011 | Total loss: 1.452 | Reg loss: 0.017 | Tree loss: 1.452 | Accuracy: 0.417500 | 1.477 sec/iter\n",
      "Epoch: 06 | Batch: 007 / 011 | Total loss: 1.436 | Reg loss: 0.017 | Tree loss: 1.436 | Accuracy: 0.415500 | 1.474 sec/iter\n",
      "Epoch: 06 | Batch: 008 / 011 | Total loss: 1.422 | Reg loss: 0.017 | Tree loss: 1.422 | Accuracy: 0.416000 | 1.47 sec/iter\n",
      "Epoch: 06 | Batch: 009 / 011 | Total loss: 1.418 | Reg loss: 0.017 | Tree loss: 1.418 | Accuracy: 0.406500 | 1.468 sec/iter\n",
      "Epoch: 06 | Batch: 010 / 011 | Total loss: 1.340 | Reg loss: 0.018 | Tree loss: 1.340 | Accuracy: 0.498294 | 1.462 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 07 | Batch: 000 / 011 | Total loss: 1.539 | Reg loss: 0.016 | Tree loss: 1.539 | Accuracy: 0.347000 | 1.502 sec/iter\n",
      "Epoch: 07 | Batch: 001 / 011 | Total loss: 1.516 | Reg loss: 0.016 | Tree loss: 1.516 | Accuracy: 0.372000 | 1.499 sec/iter\n",
      "Epoch: 07 | Batch: 002 / 011 | Total loss: 1.498 | Reg loss: 0.016 | Tree loss: 1.498 | Accuracy: 0.394500 | 1.494 sec/iter\n",
      "Epoch: 07 | Batch: 003 / 011 | Total loss: 1.478 | Reg loss: 0.016 | Tree loss: 1.478 | Accuracy: 0.413000 | 1.488 sec/iter\n",
      "Epoch: 07 | Batch: 004 / 011 | Total loss: 1.466 | Reg loss: 0.017 | Tree loss: 1.466 | Accuracy: 0.409500 | 1.483 sec/iter\n",
      "Epoch: 07 | Batch: 005 / 011 | Total loss: 1.433 | Reg loss: 0.017 | Tree loss: 1.433 | Accuracy: 0.453000 | 1.48 sec/iter\n",
      "Epoch: 07 | Batch: 006 / 011 | Total loss: 1.421 | Reg loss: 0.017 | Tree loss: 1.421 | Accuracy: 0.450500 | 1.477 sec/iter\n",
      "Epoch: 07 | Batch: 007 / 011 | Total loss: 1.420 | Reg loss: 0.017 | Tree loss: 1.420 | Accuracy: 0.425000 | 1.472 sec/iter\n",
      "Epoch: 07 | Batch: 008 / 011 | Total loss: 1.388 | Reg loss: 0.017 | Tree loss: 1.388 | Accuracy: 0.443500 | 1.467 sec/iter\n",
      "Epoch: 07 | Batch: 009 / 011 | Total loss: 1.394 | Reg loss: 0.018 | Tree loss: 1.394 | Accuracy: 0.412500 | 1.461 sec/iter\n",
      "Epoch: 07 | Batch: 010 / 011 | Total loss: 1.380 | Reg loss: 0.018 | Tree loss: 1.380 | Accuracy: 0.433447 | 1.456 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 08 | Batch: 000 / 011 | Total loss: 1.530 | Reg loss: 0.016 | Tree loss: 1.530 | Accuracy: 0.344000 | 1.477 sec/iter\n",
      "Epoch: 08 | Batch: 001 / 011 | Total loss: 1.507 | Reg loss: 0.017 | Tree loss: 1.507 | Accuracy: 0.372500 | 1.475 sec/iter\n",
      "Epoch: 08 | Batch: 002 / 011 | Total loss: 1.490 | Reg loss: 0.017 | Tree loss: 1.490 | Accuracy: 0.393000 | 1.472 sec/iter\n",
      "Epoch: 08 | Batch: 003 / 011 | Total loss: 1.468 | Reg loss: 0.017 | Tree loss: 1.468 | Accuracy: 0.420000 | 1.47 sec/iter\n",
      "Epoch: 08 | Batch: 004 / 011 | Total loss: 1.437 | Reg loss: 0.017 | Tree loss: 1.437 | Accuracy: 0.442000 | 1.467 sec/iter\n",
      "Epoch: 08 | Batch: 005 / 011 | Total loss: 1.415 | Reg loss: 0.017 | Tree loss: 1.415 | Accuracy: 0.439000 | 1.465 sec/iter\n",
      "Epoch: 08 | Batch: 006 / 011 | Total loss: 1.391 | Reg loss: 0.017 | Tree loss: 1.391 | Accuracy: 0.472000 | 1.464 sec/iter\n",
      "Epoch: 08 | Batch: 007 / 011 | Total loss: 1.378 | Reg loss: 0.017 | Tree loss: 1.378 | Accuracy: 0.451500 | 1.459 sec/iter\n",
      "Epoch: 08 | Batch: 008 / 011 | Total loss: 1.381 | Reg loss: 0.018 | Tree loss: 1.381 | Accuracy: 0.416000 | 1.454 sec/iter\n",
      "Epoch: 08 | Batch: 009 / 011 | Total loss: 1.370 | Reg loss: 0.018 | Tree loss: 1.370 | Accuracy: 0.417000 | 1.451 sec/iter\n",
      "Epoch: 08 | Batch: 010 / 011 | Total loss: 1.389 | Reg loss: 0.018 | Tree loss: 1.389 | Accuracy: 0.344710 | 1.447 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 09 | Batch: 000 / 011 | Total loss: 1.517 | Reg loss: 0.017 | Tree loss: 1.517 | Accuracy: 0.346500 | 1.481 sec/iter\n",
      "Epoch: 09 | Batch: 001 / 011 | Total loss: 1.492 | Reg loss: 0.017 | Tree loss: 1.492 | Accuracy: 0.369000 | 1.478 sec/iter\n",
      "Epoch: 09 | Batch: 002 / 011 | Total loss: 1.474 | Reg loss: 0.017 | Tree loss: 1.474 | Accuracy: 0.408000 | 1.475 sec/iter\n",
      "Epoch: 09 | Batch: 003 / 011 | Total loss: 1.448 | Reg loss: 0.017 | Tree loss: 1.448 | Accuracy: 0.419000 | 1.473 sec/iter\n",
      "Epoch: 09 | Batch: 004 / 011 | Total loss: 1.423 | Reg loss: 0.017 | Tree loss: 1.423 | Accuracy: 0.465000 | 1.471 sec/iter\n",
      "Epoch: 09 | Batch: 005 / 011 | Total loss: 1.398 | Reg loss: 0.017 | Tree loss: 1.398 | Accuracy: 0.468500 | 1.469 sec/iter\n",
      "Epoch: 09 | Batch: 006 / 011 | Total loss: 1.376 | Reg loss: 0.018 | Tree loss: 1.376 | Accuracy: 0.463000 | 1.467 sec/iter\n",
      "Epoch: 09 | Batch: 007 / 011 | Total loss: 1.362 | Reg loss: 0.018 | Tree loss: 1.362 | Accuracy: 0.482500 | 1.463 sec/iter\n",
      "Epoch: 09 | Batch: 008 / 011 | Total loss: 1.349 | Reg loss: 0.018 | Tree loss: 1.349 | Accuracy: 0.475000 | 1.459 sec/iter\n",
      "Epoch: 09 | Batch: 009 / 011 | Total loss: 1.352 | Reg loss: 0.018 | Tree loss: 1.352 | Accuracy: 0.423000 | 1.456 sec/iter\n",
      "Epoch: 09 | Batch: 010 / 011 | Total loss: 1.287 | Reg loss: 0.018 | Tree loss: 1.287 | Accuracy: 0.470990 | 1.452 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 10 | Batch: 000 / 011 | Total loss: 1.497 | Reg loss: 0.017 | Tree loss: 1.497 | Accuracy: 0.380500 | 1.478 sec/iter\n",
      "Epoch: 10 | Batch: 001 / 011 | Total loss: 1.473 | Reg loss: 0.017 | Tree loss: 1.473 | Accuracy: 0.405500 | 1.476 sec/iter\n",
      "Epoch: 10 | Batch: 002 / 011 | Total loss: 1.440 | Reg loss: 0.017 | Tree loss: 1.440 | Accuracy: 0.438000 | 1.473 sec/iter\n",
      "Epoch: 10 | Batch: 003 / 011 | Total loss: 1.421 | Reg loss: 0.017 | Tree loss: 1.421 | Accuracy: 0.437500 | 1.471 sec/iter\n",
      "Epoch: 10 | Batch: 004 / 011 | Total loss: 1.396 | Reg loss: 0.018 | Tree loss: 1.396 | Accuracy: 0.454000 | 1.47 sec/iter\n",
      "Epoch: 10 | Batch: 005 / 011 | Total loss: 1.370 | Reg loss: 0.018 | Tree loss: 1.370 | Accuracy: 0.453000 | 1.468 sec/iter\n",
      "Epoch: 10 | Batch: 006 / 011 | Total loss: 1.354 | Reg loss: 0.018 | Tree loss: 1.354 | Accuracy: 0.461500 | 1.467 sec/iter\n",
      "Epoch: 10 | Batch: 007 / 011 | Total loss: 1.323 | Reg loss: 0.018 | Tree loss: 1.323 | Accuracy: 0.469000 | 1.465 sec/iter\n",
      "Epoch: 10 | Batch: 008 / 011 | Total loss: 1.330 | Reg loss: 0.018 | Tree loss: 1.330 | Accuracy: 0.442500 | 1.462 sec/iter\n",
      "Epoch: 10 | Batch: 009 / 011 | Total loss: 1.326 | Reg loss: 0.019 | Tree loss: 1.326 | Accuracy: 0.420500 | 1.461 sec/iter\n",
      "Epoch: 10 | Batch: 010 / 011 | Total loss: 1.322 | Reg loss: 0.019 | Tree loss: 1.322 | Accuracy: 0.406143 | 1.458 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 11 | Batch: 000 / 011 | Total loss: 1.486 | Reg loss: 0.018 | Tree loss: 1.486 | Accuracy: 0.416000 | 1.482 sec/iter\n",
      "Epoch: 11 | Batch: 001 / 011 | Total loss: 1.457 | Reg loss: 0.018 | Tree loss: 1.457 | Accuracy: 0.425000 | 1.48 sec/iter\n",
      "Epoch: 11 | Batch: 002 / 011 | Total loss: 1.439 | Reg loss: 0.018 | Tree loss: 1.439 | Accuracy: 0.447500 | 1.478 sec/iter\n",
      "Epoch: 11 | Batch: 003 / 011 | Total loss: 1.412 | Reg loss: 0.018 | Tree loss: 1.412 | Accuracy: 0.458500 | 1.476 sec/iter\n",
      "Epoch: 11 | Batch: 004 / 011 | Total loss: 1.383 | Reg loss: 0.018 | Tree loss: 1.383 | Accuracy: 0.448000 | 1.474 sec/iter\n",
      "Epoch: 11 | Batch: 005 / 011 | Total loss: 1.360 | Reg loss: 0.018 | Tree loss: 1.360 | Accuracy: 0.479500 | 1.472 sec/iter\n",
      "Epoch: 11 | Batch: 006 / 011 | Total loss: 1.336 | Reg loss: 0.018 | Tree loss: 1.336 | Accuracy: 0.478000 | 1.471 sec/iter\n",
      "Epoch: 11 | Batch: 007 / 011 | Total loss: 1.314 | Reg loss: 0.018 | Tree loss: 1.314 | Accuracy: 0.486500 | 1.468 sec/iter\n",
      "Epoch: 11 | Batch: 008 / 011 | Total loss: 1.307 | Reg loss: 0.019 | Tree loss: 1.307 | Accuracy: 0.471000 | 1.465 sec/iter\n",
      "Epoch: 11 | Batch: 009 / 011 | Total loss: 1.281 | Reg loss: 0.019 | Tree loss: 1.281 | Accuracy: 0.494500 | 1.463 sec/iter\n",
      "Epoch: 11 | Batch: 010 / 011 | Total loss: 1.312 | Reg loss: 0.019 | Tree loss: 1.312 | Accuracy: 0.430034 | 1.46 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 12 | Batch: 000 / 011 | Total loss: 1.468 | Reg loss: 0.018 | Tree loss: 1.468 | Accuracy: 0.444000 | 1.485 sec/iter\n",
      "Epoch: 12 | Batch: 001 / 011 | Total loss: 1.444 | Reg loss: 0.018 | Tree loss: 1.444 | Accuracy: 0.462500 | 1.483 sec/iter\n",
      "Epoch: 12 | Batch: 002 / 011 | Total loss: 1.417 | Reg loss: 0.018 | Tree loss: 1.417 | Accuracy: 0.489500 | 1.482 sec/iter\n",
      "Epoch: 12 | Batch: 003 / 011 | Total loss: 1.381 | Reg loss: 0.018 | Tree loss: 1.381 | Accuracy: 0.483500 | 1.481 sec/iter\n",
      "Epoch: 12 | Batch: 004 / 011 | Total loss: 1.340 | Reg loss: 0.018 | Tree loss: 1.340 | Accuracy: 0.508500 | 1.479 sec/iter\n",
      "Epoch: 12 | Batch: 005 / 011 | Total loss: 1.313 | Reg loss: 0.018 | Tree loss: 1.313 | Accuracy: 0.486500 | 1.477 sec/iter\n",
      "Epoch: 12 | Batch: 006 / 011 | Total loss: 1.308 | Reg loss: 0.019 | Tree loss: 1.308 | Accuracy: 0.484000 | 1.476 sec/iter\n",
      "Epoch: 12 | Batch: 007 / 011 | Total loss: 1.293 | Reg loss: 0.019 | Tree loss: 1.293 | Accuracy: 0.493500 | 1.473 sec/iter\n",
      "Epoch: 12 | Batch: 008 / 011 | Total loss: 1.278 | Reg loss: 0.019 | Tree loss: 1.278 | Accuracy: 0.489500 | 1.47 sec/iter\n",
      "Epoch: 12 | Batch: 009 / 011 | Total loss: 1.267 | Reg loss: 0.019 | Tree loss: 1.267 | Accuracy: 0.471000 | 1.467 sec/iter\n",
      "Epoch: 12 | Batch: 010 / 011 | Total loss: 1.228 | Reg loss: 0.019 | Tree loss: 1.228 | Accuracy: 0.498294 | 1.463 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 13 | Batch: 000 / 011 | Total loss: 1.445 | Reg loss: 0.018 | Tree loss: 1.445 | Accuracy: 0.490500 | 1.489 sec/iter\n",
      "Epoch: 13 | Batch: 001 / 011 | Total loss: 1.415 | Reg loss: 0.018 | Tree loss: 1.415 | Accuracy: 0.516000 | 1.488 sec/iter\n",
      "Epoch: 13 | Batch: 002 / 011 | Total loss: 1.386 | Reg loss: 0.018 | Tree loss: 1.386 | Accuracy: 0.512000 | 1.485 sec/iter\n",
      "Epoch: 13 | Batch: 003 / 011 | Total loss: 1.368 | Reg loss: 0.019 | Tree loss: 1.368 | Accuracy: 0.455500 | 1.482 sec/iter\n",
      "Epoch: 13 | Batch: 004 / 011 | Total loss: 1.343 | Reg loss: 0.019 | Tree loss: 1.343 | Accuracy: 0.465000 | 1.478 sec/iter\n",
      "Epoch: 13 | Batch: 005 / 011 | Total loss: 1.320 | Reg loss: 0.019 | Tree loss: 1.320 | Accuracy: 0.488000 | 1.474 sec/iter\n",
      "Epoch: 13 | Batch: 006 / 011 | Total loss: 1.290 | Reg loss: 0.019 | Tree loss: 1.290 | Accuracy: 0.528000 | 1.471 sec/iter\n",
      "Epoch: 13 | Batch: 007 / 011 | Total loss: 1.264 | Reg loss: 0.019 | Tree loss: 1.264 | Accuracy: 0.520000 | 1.468 sec/iter\n",
      "Epoch: 13 | Batch: 008 / 011 | Total loss: 1.248 | Reg loss: 0.019 | Tree loss: 1.248 | Accuracy: 0.515500 | 1.464 sec/iter\n",
      "Epoch: 13 | Batch: 009 / 011 | Total loss: 1.255 | Reg loss: 0.020 | Tree loss: 1.255 | Accuracy: 0.512000 | 1.461 sec/iter\n",
      "Epoch: 13 | Batch: 010 / 011 | Total loss: 1.247 | Reg loss: 0.020 | Tree loss: 1.247 | Accuracy: 0.501706 | 1.457 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 14 | Batch: 000 / 011 | Total loss: 1.431 | Reg loss: 0.019 | Tree loss: 1.431 | Accuracy: 0.528500 | 1.465 sec/iter\n",
      "Epoch: 14 | Batch: 001 / 011 | Total loss: 1.401 | Reg loss: 0.019 | Tree loss: 1.401 | Accuracy: 0.542000 | 1.462 sec/iter\n",
      "Epoch: 14 | Batch: 002 / 011 | Total loss: 1.374 | Reg loss: 0.019 | Tree loss: 1.374 | Accuracy: 0.556000 | 1.459 sec/iter\n",
      "Epoch: 14 | Batch: 003 / 011 | Total loss: 1.334 | Reg loss: 0.019 | Tree loss: 1.334 | Accuracy: 0.576000 | 1.457 sec/iter\n",
      "Epoch: 14 | Batch: 004 / 011 | Total loss: 1.301 | Reg loss: 0.019 | Tree loss: 1.301 | Accuracy: 0.567500 | 1.456 sec/iter\n",
      "Epoch: 14 | Batch: 005 / 011 | Total loss: 1.292 | Reg loss: 0.019 | Tree loss: 1.292 | Accuracy: 0.518000 | 1.454 sec/iter\n",
      "Epoch: 14 | Batch: 006 / 011 | Total loss: 1.261 | Reg loss: 0.019 | Tree loss: 1.261 | Accuracy: 0.541000 | 1.453 sec/iter\n",
      "Epoch: 14 | Batch: 007 / 011 | Total loss: 1.241 | Reg loss: 0.020 | Tree loss: 1.241 | Accuracy: 0.531500 | 1.453 sec/iter\n",
      "Epoch: 14 | Batch: 008 / 011 | Total loss: 1.227 | Reg loss: 0.020 | Tree loss: 1.227 | Accuracy: 0.548500 | 1.452 sec/iter\n",
      "Epoch: 14 | Batch: 009 / 011 | Total loss: 1.205 | Reg loss: 0.020 | Tree loss: 1.205 | Accuracy: 0.522500 | 1.45 sec/iter\n",
      "Epoch: 14 | Batch: 010 / 011 | Total loss: 1.180 | Reg loss: 0.020 | Tree loss: 1.180 | Accuracy: 0.556314 | 1.447 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 15 | Batch: 000 / 011 | Total loss: 1.404 | Reg loss: 0.019 | Tree loss: 1.404 | Accuracy: 0.553000 | 1.489 sec/iter\n",
      "Epoch: 15 | Batch: 001 / 011 | Total loss: 1.383 | Reg loss: 0.019 | Tree loss: 1.383 | Accuracy: 0.530000 | 1.487 sec/iter\n",
      "Epoch: 15 | Batch: 002 / 011 | Total loss: 1.349 | Reg loss: 0.019 | Tree loss: 1.349 | Accuracy: 0.563000 | 1.486 sec/iter\n",
      "Epoch: 15 | Batch: 003 / 011 | Total loss: 1.306 | Reg loss: 0.019 | Tree loss: 1.306 | Accuracy: 0.576500 | 1.483 sec/iter\n",
      "Epoch: 15 | Batch: 004 / 011 | Total loss: 1.295 | Reg loss: 0.019 | Tree loss: 1.295 | Accuracy: 0.536000 | 1.481 sec/iter\n",
      "Epoch: 15 | Batch: 005 / 011 | Total loss: 1.265 | Reg loss: 0.020 | Tree loss: 1.265 | Accuracy: 0.549500 | 1.479 sec/iter\n",
      "Epoch: 15 | Batch: 006 / 011 | Total loss: 1.246 | Reg loss: 0.020 | Tree loss: 1.246 | Accuracy: 0.530000 | 1.477 sec/iter\n",
      "Epoch: 15 | Batch: 007 / 011 | Total loss: 1.226 | Reg loss: 0.020 | Tree loss: 1.226 | Accuracy: 0.533000 | 1.474 sec/iter\n",
      "Epoch: 15 | Batch: 008 / 011 | Total loss: 1.191 | Reg loss: 0.020 | Tree loss: 1.191 | Accuracy: 0.554500 | 1.473 sec/iter\n",
      "Epoch: 15 | Batch: 009 / 011 | Total loss: 1.191 | Reg loss: 0.020 | Tree loss: 1.191 | Accuracy: 0.561500 | 1.472 sec/iter\n",
      "Epoch: 15 | Batch: 010 / 011 | Total loss: 1.166 | Reg loss: 0.020 | Tree loss: 1.166 | Accuracy: 0.556314 | 1.469 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 16 | Batch: 000 / 011 | Total loss: 1.385 | Reg loss: 0.020 | Tree loss: 1.385 | Accuracy: 0.557000 | 1.494 sec/iter\n",
      "Epoch: 16 | Batch: 001 / 011 | Total loss: 1.357 | Reg loss: 0.020 | Tree loss: 1.357 | Accuracy: 0.570500 | 1.493 sec/iter\n",
      "Epoch: 16 | Batch: 002 / 011 | Total loss: 1.319 | Reg loss: 0.020 | Tree loss: 1.319 | Accuracy: 0.585000 | 1.49 sec/iter\n",
      "Epoch: 16 | Batch: 003 / 011 | Total loss: 1.302 | Reg loss: 0.020 | Tree loss: 1.302 | Accuracy: 0.577500 | 1.488 sec/iter\n",
      "Epoch: 16 | Batch: 004 / 011 | Total loss: 1.254 | Reg loss: 0.020 | Tree loss: 1.254 | Accuracy: 0.580500 | 1.486 sec/iter\n",
      "Epoch: 16 | Batch: 005 / 011 | Total loss: 1.226 | Reg loss: 0.020 | Tree loss: 1.226 | Accuracy: 0.573000 | 1.483 sec/iter\n",
      "Epoch: 16 | Batch: 006 / 011 | Total loss: 1.226 | Reg loss: 0.020 | Tree loss: 1.226 | Accuracy: 0.538500 | 1.48 sec/iter\n",
      "Epoch: 16 | Batch: 007 / 011 | Total loss: 1.206 | Reg loss: 0.020 | Tree loss: 1.206 | Accuracy: 0.552000 | 1.478 sec/iter\n",
      "Epoch: 16 | Batch: 008 / 011 | Total loss: 1.192 | Reg loss: 0.020 | Tree loss: 1.192 | Accuracy: 0.548000 | 1.478 sec/iter\n",
      "Epoch: 16 | Batch: 009 / 011 | Total loss: 1.167 | Reg loss: 0.021 | Tree loss: 1.167 | Accuracy: 0.560500 | 1.477 sec/iter\n",
      "Epoch: 16 | Batch: 010 / 011 | Total loss: 1.162 | Reg loss: 0.021 | Tree loss: 1.162 | Accuracy: 0.580205 | 1.474 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 17 | Batch: 000 / 011 | Total loss: 1.356 | Reg loss: 0.020 | Tree loss: 1.356 | Accuracy: 0.572000 | 1.498 sec/iter\n",
      "Epoch: 17 | Batch: 001 / 011 | Total loss: 1.340 | Reg loss: 0.020 | Tree loss: 1.340 | Accuracy: 0.564000 | 1.497 sec/iter\n",
      "Epoch: 17 | Batch: 002 / 011 | Total loss: 1.295 | Reg loss: 0.020 | Tree loss: 1.295 | Accuracy: 0.583500 | 1.495 sec/iter\n",
      "Epoch: 17 | Batch: 003 / 011 | Total loss: 1.282 | Reg loss: 0.020 | Tree loss: 1.282 | Accuracy: 0.583000 | 1.493 sec/iter\n",
      "Epoch: 17 | Batch: 004 / 011 | Total loss: 1.245 | Reg loss: 0.020 | Tree loss: 1.245 | Accuracy: 0.588500 | 1.491 sec/iter\n",
      "Epoch: 17 | Batch: 005 / 011 | Total loss: 1.231 | Reg loss: 0.020 | Tree loss: 1.231 | Accuracy: 0.573500 | 1.489 sec/iter\n",
      "Epoch: 17 | Batch: 006 / 011 | Total loss: 1.199 | Reg loss: 0.020 | Tree loss: 1.199 | Accuracy: 0.586000 | 1.487 sec/iter\n",
      "Epoch: 17 | Batch: 007 / 011 | Total loss: 1.169 | Reg loss: 0.021 | Tree loss: 1.169 | Accuracy: 0.584000 | 1.486 sec/iter\n",
      "Epoch: 17 | Batch: 008 / 011 | Total loss: 1.166 | Reg loss: 0.021 | Tree loss: 1.166 | Accuracy: 0.562500 | 1.485 sec/iter\n",
      "Epoch: 17 | Batch: 009 / 011 | Total loss: 1.151 | Reg loss: 0.021 | Tree loss: 1.151 | Accuracy: 0.587500 | 1.484 sec/iter\n",
      "Epoch: 17 | Batch: 010 / 011 | Total loss: 1.167 | Reg loss: 0.021 | Tree loss: 1.167 | Accuracy: 0.546075 | 1.481 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 18 | Batch: 000 / 011 | Total loss: 1.343 | Reg loss: 0.020 | Tree loss: 1.343 | Accuracy: 0.549500 | 1.501 sec/iter\n",
      "Epoch: 18 | Batch: 001 / 011 | Total loss: 1.310 | Reg loss: 0.020 | Tree loss: 1.310 | Accuracy: 0.576000 | 1.499 sec/iter\n",
      "Epoch: 18 | Batch: 002 / 011 | Total loss: 1.275 | Reg loss: 0.020 | Tree loss: 1.275 | Accuracy: 0.586000 | 1.497 sec/iter\n",
      "Epoch: 18 | Batch: 003 / 011 | Total loss: 1.236 | Reg loss: 0.020 | Tree loss: 1.236 | Accuracy: 0.607500 | 1.495 sec/iter\n",
      "Epoch: 18 | Batch: 004 / 011 | Total loss: 1.224 | Reg loss: 0.021 | Tree loss: 1.224 | Accuracy: 0.566500 | 1.493 sec/iter\n",
      "Epoch: 18 | Batch: 005 / 011 | Total loss: 1.195 | Reg loss: 0.021 | Tree loss: 1.195 | Accuracy: 0.565500 | 1.491 sec/iter\n",
      "Epoch: 18 | Batch: 006 / 011 | Total loss: 1.181 | Reg loss: 0.021 | Tree loss: 1.181 | Accuracy: 0.558000 | 1.489 sec/iter\n",
      "Epoch: 18 | Batch: 007 / 011 | Total loss: 1.155 | Reg loss: 0.021 | Tree loss: 1.155 | Accuracy: 0.571500 | 1.487 sec/iter\n",
      "Epoch: 18 | Batch: 008 / 011 | Total loss: 1.149 | Reg loss: 0.021 | Tree loss: 1.149 | Accuracy: 0.582000 | 1.485 sec/iter\n",
      "Epoch: 18 | Batch: 009 / 011 | Total loss: 1.160 | Reg loss: 0.021 | Tree loss: 1.160 | Accuracy: 0.563500 | 1.484 sec/iter\n",
      "Epoch: 18 | Batch: 010 / 011 | Total loss: 1.131 | Reg loss: 0.021 | Tree loss: 1.131 | Accuracy: 0.587031 | 1.482 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 19 | Batch: 000 / 011 | Total loss: 1.327 | Reg loss: 0.021 | Tree loss: 1.327 | Accuracy: 0.552500 | 1.504 sec/iter\n",
      "Epoch: 19 | Batch: 001 / 011 | Total loss: 1.289 | Reg loss: 0.021 | Tree loss: 1.289 | Accuracy: 0.581000 | 1.502 sec/iter\n",
      "Epoch: 19 | Batch: 002 / 011 | Total loss: 1.252 | Reg loss: 0.021 | Tree loss: 1.252 | Accuracy: 0.581000 | 1.499 sec/iter\n",
      "Epoch: 19 | Batch: 003 / 011 | Total loss: 1.225 | Reg loss: 0.021 | Tree loss: 1.225 | Accuracy: 0.580500 | 1.496 sec/iter\n",
      "Epoch: 19 | Batch: 004 / 011 | Total loss: 1.214 | Reg loss: 0.021 | Tree loss: 1.214 | Accuracy: 0.568500 | 1.494 sec/iter\n",
      "Epoch: 19 | Batch: 005 / 011 | Total loss: 1.185 | Reg loss: 0.021 | Tree loss: 1.185 | Accuracy: 0.600500 | 1.491 sec/iter\n",
      "Epoch: 19 | Batch: 006 / 011 | Total loss: 1.165 | Reg loss: 0.021 | Tree loss: 1.165 | Accuracy: 0.587000 | 1.489 sec/iter\n",
      "Epoch: 19 | Batch: 007 / 011 | Total loss: 1.141 | Reg loss: 0.021 | Tree loss: 1.141 | Accuracy: 0.586000 | 1.488 sec/iter\n",
      "Epoch: 19 | Batch: 008 / 011 | Total loss: 1.126 | Reg loss: 0.021 | Tree loss: 1.126 | Accuracy: 0.577000 | 1.486 sec/iter\n",
      "Epoch: 19 | Batch: 009 / 011 | Total loss: 1.128 | Reg loss: 0.021 | Tree loss: 1.128 | Accuracy: 0.588000 | 1.484 sec/iter\n",
      "Epoch: 19 | Batch: 010 / 011 | Total loss: 1.099 | Reg loss: 0.022 | Tree loss: 1.099 | Accuracy: 0.600683 | 1.482 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 20 | Batch: 000 / 011 | Total loss: 1.304 | Reg loss: 0.021 | Tree loss: 1.304 | Accuracy: 0.564500 | 1.491 sec/iter\n",
      "Epoch: 20 | Batch: 001 / 011 | Total loss: 1.278 | Reg loss: 0.021 | Tree loss: 1.278 | Accuracy: 0.564500 | 1.49 sec/iter\n",
      "Epoch: 20 | Batch: 002 / 011 | Total loss: 1.243 | Reg loss: 0.021 | Tree loss: 1.243 | Accuracy: 0.569000 | 1.488 sec/iter\n",
      "Epoch: 20 | Batch: 003 / 011 | Total loss: 1.209 | Reg loss: 0.021 | Tree loss: 1.209 | Accuracy: 0.579000 | 1.487 sec/iter\n",
      "Epoch: 20 | Batch: 004 / 011 | Total loss: 1.168 | Reg loss: 0.021 | Tree loss: 1.168 | Accuracy: 0.610500 | 1.485 sec/iter\n",
      "Epoch: 20 | Batch: 005 / 011 | Total loss: 1.152 | Reg loss: 0.021 | Tree loss: 1.152 | Accuracy: 0.590000 | 1.483 sec/iter\n",
      "Epoch: 20 | Batch: 006 / 011 | Total loss: 1.143 | Reg loss: 0.021 | Tree loss: 1.143 | Accuracy: 0.588000 | 1.48 sec/iter\n",
      "Epoch: 20 | Batch: 007 / 011 | Total loss: 1.129 | Reg loss: 0.021 | Tree loss: 1.129 | Accuracy: 0.591500 | 1.479 sec/iter\n",
      "Epoch: 20 | Batch: 008 / 011 | Total loss: 1.102 | Reg loss: 0.022 | Tree loss: 1.102 | Accuracy: 0.614500 | 1.478 sec/iter\n",
      "Epoch: 20 | Batch: 009 / 011 | Total loss: 1.110 | Reg loss: 0.022 | Tree loss: 1.110 | Accuracy: 0.575500 | 1.478 sec/iter\n",
      "Epoch: 20 | Batch: 010 / 011 | Total loss: 1.108 | Reg loss: 0.022 | Tree loss: 1.108 | Accuracy: 0.580205 | 1.476 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 21 | Batch: 000 / 011 | Total loss: 1.260 | Reg loss: 0.021 | Tree loss: 1.260 | Accuracy: 0.591000 | 1.491 sec/iter\n",
      "Epoch: 21 | Batch: 001 / 011 | Total loss: 1.249 | Reg loss: 0.021 | Tree loss: 1.249 | Accuracy: 0.566500 | 1.49 sec/iter\n",
      "Epoch: 21 | Batch: 002 / 011 | Total loss: 1.229 | Reg loss: 0.021 | Tree loss: 1.229 | Accuracy: 0.557500 | 1.488 sec/iter\n",
      "Epoch: 21 | Batch: 003 / 011 | Total loss: 1.191 | Reg loss: 0.021 | Tree loss: 1.191 | Accuracy: 0.574500 | 1.486 sec/iter\n",
      "Epoch: 21 | Batch: 004 / 011 | Total loss: 1.168 | Reg loss: 0.021 | Tree loss: 1.168 | Accuracy: 0.588500 | 1.484 sec/iter\n",
      "Epoch: 21 | Batch: 005 / 011 | Total loss: 1.154 | Reg loss: 0.022 | Tree loss: 1.154 | Accuracy: 0.595000 | 1.483 sec/iter\n",
      "Epoch: 21 | Batch: 006 / 011 | Total loss: 1.120 | Reg loss: 0.022 | Tree loss: 1.120 | Accuracy: 0.591500 | 1.48 sec/iter\n",
      "Epoch: 21 | Batch: 007 / 011 | Total loss: 1.112 | Reg loss: 0.022 | Tree loss: 1.112 | Accuracy: 0.589500 | 1.479 sec/iter\n",
      "Epoch: 21 | Batch: 008 / 011 | Total loss: 1.118 | Reg loss: 0.022 | Tree loss: 1.118 | Accuracy: 0.586500 | 1.479 sec/iter\n",
      "Epoch: 21 | Batch: 009 / 011 | Total loss: 1.094 | Reg loss: 0.022 | Tree loss: 1.094 | Accuracy: 0.595500 | 1.479 sec/iter\n",
      "Epoch: 21 | Batch: 010 / 011 | Total loss: 1.104 | Reg loss: 0.022 | Tree loss: 1.104 | Accuracy: 0.583618 | 1.477 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 22 | Batch: 000 / 011 | Total loss: 1.253 | Reg loss: 0.021 | Tree loss: 1.253 | Accuracy: 0.580000 | 1.494 sec/iter\n",
      "Epoch: 22 | Batch: 001 / 011 | Total loss: 1.231 | Reg loss: 0.022 | Tree loss: 1.231 | Accuracy: 0.568000 | 1.493 sec/iter\n",
      "Epoch: 22 | Batch: 002 / 011 | Total loss: 1.208 | Reg loss: 0.022 | Tree loss: 1.208 | Accuracy: 0.569000 | 1.492 sec/iter\n",
      "Epoch: 22 | Batch: 003 / 011 | Total loss: 1.168 | Reg loss: 0.022 | Tree loss: 1.168 | Accuracy: 0.590500 | 1.49 sec/iter\n",
      "Epoch: 22 | Batch: 004 / 011 | Total loss: 1.154 | Reg loss: 0.022 | Tree loss: 1.154 | Accuracy: 0.580500 | 1.489 sec/iter\n",
      "Epoch: 22 | Batch: 005 / 011 | Total loss: 1.127 | Reg loss: 0.022 | Tree loss: 1.127 | Accuracy: 0.586000 | 1.487 sec/iter\n",
      "Epoch: 22 | Batch: 006 / 011 | Total loss: 1.104 | Reg loss: 0.022 | Tree loss: 1.104 | Accuracy: 0.591000 | 1.485 sec/iter\n",
      "Epoch: 22 | Batch: 007 / 011 | Total loss: 1.094 | Reg loss: 0.022 | Tree loss: 1.094 | Accuracy: 0.607000 | 1.484 sec/iter\n",
      "Epoch: 22 | Batch: 008 / 011 | Total loss: 1.088 | Reg loss: 0.022 | Tree loss: 1.088 | Accuracy: 0.594000 | 1.484 sec/iter\n",
      "Epoch: 22 | Batch: 009 / 011 | Total loss: 1.072 | Reg loss: 0.022 | Tree loss: 1.072 | Accuracy: 0.598000 | 1.483 sec/iter\n",
      "Epoch: 22 | Batch: 010 / 011 | Total loss: 1.115 | Reg loss: 0.022 | Tree loss: 1.115 | Accuracy: 0.576792 | 1.481 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 23 | Batch: 000 / 011 | Total loss: 1.242 | Reg loss: 0.022 | Tree loss: 1.242 | Accuracy: 0.566000 | 1.496 sec/iter\n",
      "Epoch: 23 | Batch: 001 / 011 | Total loss: 1.204 | Reg loss: 0.022 | Tree loss: 1.204 | Accuracy: 0.575500 | 1.495 sec/iter\n",
      "Epoch: 23 | Batch: 002 / 011 | Total loss: 1.186 | Reg loss: 0.022 | Tree loss: 1.186 | Accuracy: 0.577000 | 1.493 sec/iter\n",
      "Epoch: 23 | Batch: 003 / 011 | Total loss: 1.155 | Reg loss: 0.022 | Tree loss: 1.155 | Accuracy: 0.592000 | 1.492 sec/iter\n",
      "Epoch: 23 | Batch: 004 / 011 | Total loss: 1.150 | Reg loss: 0.022 | Tree loss: 1.150 | Accuracy: 0.594000 | 1.49 sec/iter\n",
      "Epoch: 23 | Batch: 005 / 011 | Total loss: 1.112 | Reg loss: 0.022 | Tree loss: 1.112 | Accuracy: 0.589000 | 1.489 sec/iter\n",
      "Epoch: 23 | Batch: 006 / 011 | Total loss: 1.112 | Reg loss: 0.022 | Tree loss: 1.112 | Accuracy: 0.575000 | 1.487 sec/iter\n",
      "Epoch: 23 | Batch: 007 / 011 | Total loss: 1.068 | Reg loss: 0.022 | Tree loss: 1.068 | Accuracy: 0.618500 | 1.485 sec/iter\n",
      "Epoch: 23 | Batch: 008 / 011 | Total loss: 1.078 | Reg loss: 0.022 | Tree loss: 1.078 | Accuracy: 0.608000 | 1.484 sec/iter\n",
      "Epoch: 23 | Batch: 009 / 011 | Total loss: 1.068 | Reg loss: 0.022 | Tree loss: 1.068 | Accuracy: 0.593500 | 1.483 sec/iter\n",
      "Epoch: 23 | Batch: 010 / 011 | Total loss: 1.076 | Reg loss: 0.022 | Tree loss: 1.076 | Accuracy: 0.580205 | 1.481 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 24 | Batch: 000 / 011 | Total loss: 1.230 | Reg loss: 0.022 | Tree loss: 1.230 | Accuracy: 0.566500 | 1.498 sec/iter\n",
      "Epoch: 24 | Batch: 001 / 011 | Total loss: 1.192 | Reg loss: 0.022 | Tree loss: 1.192 | Accuracy: 0.570000 | 1.497 sec/iter\n",
      "Epoch: 24 | Batch: 002 / 011 | Total loss: 1.181 | Reg loss: 0.022 | Tree loss: 1.181 | Accuracy: 0.555000 | 1.496 sec/iter\n",
      "Epoch: 24 | Batch: 003 / 011 | Total loss: 1.141 | Reg loss: 0.022 | Tree loss: 1.141 | Accuracy: 0.606500 | 1.494 sec/iter\n",
      "Epoch: 24 | Batch: 004 / 011 | Total loss: 1.127 | Reg loss: 0.022 | Tree loss: 1.127 | Accuracy: 0.578500 | 1.492 sec/iter\n",
      "Epoch: 24 | Batch: 005 / 011 | Total loss: 1.097 | Reg loss: 0.022 | Tree loss: 1.097 | Accuracy: 0.592500 | 1.491 sec/iter\n",
      "Epoch: 24 | Batch: 006 / 011 | Total loss: 1.080 | Reg loss: 0.022 | Tree loss: 1.080 | Accuracy: 0.598000 | 1.49 sec/iter\n",
      "Epoch: 24 | Batch: 007 / 011 | Total loss: 1.081 | Reg loss: 0.022 | Tree loss: 1.081 | Accuracy: 0.598500 | 1.488 sec/iter\n",
      "Epoch: 24 | Batch: 008 / 011 | Total loss: 1.057 | Reg loss: 0.022 | Tree loss: 1.057 | Accuracy: 0.602000 | 1.488 sec/iter\n",
      "Epoch: 24 | Batch: 009 / 011 | Total loss: 1.049 | Reg loss: 0.023 | Tree loss: 1.049 | Accuracy: 0.583000 | 1.487 sec/iter\n",
      "Epoch: 24 | Batch: 010 / 011 | Total loss: 1.036 | Reg loss: 0.023 | Tree loss: 1.036 | Accuracy: 0.583618 | 1.485 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 25 | Batch: 000 / 011 | Total loss: 1.206 | Reg loss: 0.022 | Tree loss: 1.206 | Accuracy: 0.572500 | 1.501 sec/iter\n",
      "Epoch: 25 | Batch: 001 / 011 | Total loss: 1.181 | Reg loss: 0.022 | Tree loss: 1.181 | Accuracy: 0.584500 | 1.5 sec/iter\n",
      "Epoch: 25 | Batch: 002 / 011 | Total loss: 1.160 | Reg loss: 0.022 | Tree loss: 1.160 | Accuracy: 0.577000 | 1.498 sec/iter\n",
      "Epoch: 25 | Batch: 003 / 011 | Total loss: 1.132 | Reg loss: 0.022 | Tree loss: 1.132 | Accuracy: 0.586000 | 1.496 sec/iter\n",
      "Epoch: 25 | Batch: 004 / 011 | Total loss: 1.110 | Reg loss: 0.022 | Tree loss: 1.110 | Accuracy: 0.605000 | 1.495 sec/iter\n",
      "Epoch: 25 | Batch: 005 / 011 | Total loss: 1.087 | Reg loss: 0.022 | Tree loss: 1.087 | Accuracy: 0.591000 | 1.493 sec/iter\n",
      "Epoch: 25 | Batch: 006 / 011 | Total loss: 1.061 | Reg loss: 0.022 | Tree loss: 1.061 | Accuracy: 0.593000 | 1.491 sec/iter\n",
      "Epoch: 25 | Batch: 007 / 011 | Total loss: 1.051 | Reg loss: 0.023 | Tree loss: 1.051 | Accuracy: 0.612000 | 1.49 sec/iter\n",
      "Epoch: 25 | Batch: 008 / 011 | Total loss: 1.047 | Reg loss: 0.023 | Tree loss: 1.047 | Accuracy: 0.592000 | 1.488 sec/iter\n",
      "Epoch: 25 | Batch: 009 / 011 | Total loss: 1.066 | Reg loss: 0.023 | Tree loss: 1.066 | Accuracy: 0.564500 | 1.486 sec/iter\n",
      "Epoch: 25 | Batch: 010 / 011 | Total loss: 1.043 | Reg loss: 0.023 | Tree loss: 1.043 | Accuracy: 0.569966 | 1.484 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 26 | Batch: 000 / 011 | Total loss: 1.199 | Reg loss: 0.022 | Tree loss: 1.199 | Accuracy: 0.577500 | 1.491 sec/iter\n",
      "Epoch: 26 | Batch: 001 / 011 | Total loss: 1.183 | Reg loss: 0.022 | Tree loss: 1.183 | Accuracy: 0.585000 | 1.489 sec/iter\n",
      "Epoch: 26 | Batch: 002 / 011 | Total loss: 1.144 | Reg loss: 0.022 | Tree loss: 1.144 | Accuracy: 0.584500 | 1.488 sec/iter\n",
      "Epoch: 26 | Batch: 003 / 011 | Total loss: 1.106 | Reg loss: 0.022 | Tree loss: 1.106 | Accuracy: 0.592500 | 1.486 sec/iter\n",
      "Epoch: 26 | Batch: 004 / 011 | Total loss: 1.116 | Reg loss: 0.023 | Tree loss: 1.116 | Accuracy: 0.583000 | 1.485 sec/iter\n",
      "Epoch: 26 | Batch: 005 / 011 | Total loss: 1.082 | Reg loss: 0.023 | Tree loss: 1.082 | Accuracy: 0.597000 | 1.484 sec/iter\n",
      "Epoch: 26 | Batch: 006 / 011 | Total loss: 1.057 | Reg loss: 0.023 | Tree loss: 1.057 | Accuracy: 0.589000 | 1.482 sec/iter\n",
      "Epoch: 26 | Batch: 007 / 011 | Total loss: 1.039 | Reg loss: 0.023 | Tree loss: 1.039 | Accuracy: 0.616500 | 1.482 sec/iter\n",
      "Epoch: 26 | Batch: 008 / 011 | Total loss: 1.037 | Reg loss: 0.023 | Tree loss: 1.037 | Accuracy: 0.606500 | 1.481 sec/iter\n",
      "Epoch: 26 | Batch: 009 / 011 | Total loss: 1.036 | Reg loss: 0.023 | Tree loss: 1.036 | Accuracy: 0.583500 | 1.481 sec/iter\n",
      "Epoch: 26 | Batch: 010 / 011 | Total loss: 0.983 | Reg loss: 0.023 | Tree loss: 0.983 | Accuracy: 0.634812 | 1.479 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 27 | Batch: 000 / 011 | Total loss: 1.184 | Reg loss: 0.023 | Tree loss: 1.184 | Accuracy: 0.572500 | 1.492 sec/iter\n",
      "Epoch: 27 | Batch: 001 / 011 | Total loss: 1.178 | Reg loss: 0.023 | Tree loss: 1.178 | Accuracy: 0.560500 | 1.491 sec/iter\n",
      "Epoch: 27 | Batch: 002 / 011 | Total loss: 1.138 | Reg loss: 0.023 | Tree loss: 1.138 | Accuracy: 0.561500 | 1.49 sec/iter\n",
      "Epoch: 27 | Batch: 003 / 011 | Total loss: 1.102 | Reg loss: 0.023 | Tree loss: 1.102 | Accuracy: 0.595500 | 1.489 sec/iter\n",
      "Epoch: 27 | Batch: 004 / 011 | Total loss: 1.083 | Reg loss: 0.023 | Tree loss: 1.083 | Accuracy: 0.604000 | 1.487 sec/iter\n",
      "Epoch: 27 | Batch: 005 / 011 | Total loss: 1.058 | Reg loss: 0.023 | Tree loss: 1.058 | Accuracy: 0.605500 | 1.486 sec/iter\n",
      "Epoch: 27 | Batch: 006 / 011 | Total loss: 1.054 | Reg loss: 0.023 | Tree loss: 1.054 | Accuracy: 0.587500 | 1.484 sec/iter\n",
      "Epoch: 27 | Batch: 007 / 011 | Total loss: 1.022 | Reg loss: 0.023 | Tree loss: 1.022 | Accuracy: 0.630500 | 1.483 sec/iter\n",
      "Epoch: 27 | Batch: 008 / 011 | Total loss: 1.035 | Reg loss: 0.023 | Tree loss: 1.035 | Accuracy: 0.583000 | 1.483 sec/iter\n",
      "Epoch: 27 | Batch: 009 / 011 | Total loss: 1.016 | Reg loss: 0.023 | Tree loss: 1.016 | Accuracy: 0.607000 | 1.483 sec/iter\n",
      "Epoch: 27 | Batch: 010 / 011 | Total loss: 1.017 | Reg loss: 0.023 | Tree loss: 1.017 | Accuracy: 0.559727 | 1.482 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 28 | Batch: 000 / 011 | Total loss: 1.184 | Reg loss: 0.023 | Tree loss: 1.184 | Accuracy: 0.568000 | 1.494 sec/iter\n",
      "Epoch: 28 | Batch: 001 / 011 | Total loss: 1.143 | Reg loss: 0.023 | Tree loss: 1.143 | Accuracy: 0.583500 | 1.494 sec/iter\n",
      "Epoch: 28 | Batch: 002 / 011 | Total loss: 1.128 | Reg loss: 0.023 | Tree loss: 1.128 | Accuracy: 0.556000 | 1.493 sec/iter\n",
      "Epoch: 28 | Batch: 003 / 011 | Total loss: 1.098 | Reg loss: 0.023 | Tree loss: 1.098 | Accuracy: 0.589000 | 1.492 sec/iter\n",
      "Epoch: 28 | Batch: 004 / 011 | Total loss: 1.074 | Reg loss: 0.023 | Tree loss: 1.074 | Accuracy: 0.589500 | 1.491 sec/iter\n",
      "Epoch: 28 | Batch: 005 / 011 | Total loss: 1.055 | Reg loss: 0.023 | Tree loss: 1.055 | Accuracy: 0.594000 | 1.489 sec/iter\n",
      "Epoch: 28 | Batch: 006 / 011 | Total loss: 1.046 | Reg loss: 0.023 | Tree loss: 1.046 | Accuracy: 0.602000 | 1.488 sec/iter\n",
      "Epoch: 28 | Batch: 007 / 011 | Total loss: 1.019 | Reg loss: 0.023 | Tree loss: 1.019 | Accuracy: 0.633500 | 1.487 sec/iter\n",
      "Epoch: 28 | Batch: 008 / 011 | Total loss: 1.019 | Reg loss: 0.023 | Tree loss: 1.019 | Accuracy: 0.607500 | 1.487 sec/iter\n",
      "Epoch: 28 | Batch: 009 / 011 | Total loss: 1.017 | Reg loss: 0.023 | Tree loss: 1.017 | Accuracy: 0.585500 | 1.486 sec/iter\n",
      "Epoch: 28 | Batch: 010 / 011 | Total loss: 0.986 | Reg loss: 0.023 | Tree loss: 0.986 | Accuracy: 0.576792 | 1.485 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 29 | Batch: 000 / 011 | Total loss: 1.151 | Reg loss: 0.023 | Tree loss: 1.151 | Accuracy: 0.577000 | 1.498 sec/iter\n",
      "Epoch: 29 | Batch: 001 / 011 | Total loss: 1.137 | Reg loss: 0.023 | Tree loss: 1.137 | Accuracy: 0.575000 | 1.497 sec/iter\n",
      "Epoch: 29 | Batch: 002 / 011 | Total loss: 1.118 | Reg loss: 0.023 | Tree loss: 1.118 | Accuracy: 0.567000 | 1.496 sec/iter\n",
      "Epoch: 29 | Batch: 003 / 011 | Total loss: 1.090 | Reg loss: 0.023 | Tree loss: 1.090 | Accuracy: 0.582000 | 1.495 sec/iter\n",
      "Epoch: 29 | Batch: 004 / 011 | Total loss: 1.057 | Reg loss: 0.023 | Tree loss: 1.057 | Accuracy: 0.585000 | 1.493 sec/iter\n",
      "Epoch: 29 | Batch: 005 / 011 | Total loss: 1.039 | Reg loss: 0.023 | Tree loss: 1.039 | Accuracy: 0.589500 | 1.492 sec/iter\n",
      "Epoch: 29 | Batch: 006 / 011 | Total loss: 1.022 | Reg loss: 0.023 | Tree loss: 1.022 | Accuracy: 0.625000 | 1.491 sec/iter\n",
      "Epoch: 29 | Batch: 007 / 011 | Total loss: 1.017 | Reg loss: 0.023 | Tree loss: 1.017 | Accuracy: 0.612000 | 1.49 sec/iter\n",
      "Epoch: 29 | Batch: 008 / 011 | Total loss: 1.007 | Reg loss: 0.023 | Tree loss: 1.007 | Accuracy: 0.596500 | 1.49 sec/iter\n",
      "Epoch: 29 | Batch: 009 / 011 | Total loss: 1.011 | Reg loss: 0.023 | Tree loss: 1.011 | Accuracy: 0.584500 | 1.49 sec/iter\n",
      "Epoch: 29 | Batch: 010 / 011 | Total loss: 1.079 | Reg loss: 0.023 | Tree loss: 1.079 | Accuracy: 0.522184 | 1.488 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 30 | Batch: 000 / 011 | Total loss: 1.141 | Reg loss: 0.023 | Tree loss: 1.141 | Accuracy: 0.575500 | 1.499 sec/iter\n",
      "Epoch: 30 | Batch: 001 / 011 | Total loss: 1.127 | Reg loss: 0.023 | Tree loss: 1.127 | Accuracy: 0.562000 | 1.498 sec/iter\n",
      "Epoch: 30 | Batch: 002 / 011 | Total loss: 1.121 | Reg loss: 0.023 | Tree loss: 1.121 | Accuracy: 0.561000 | 1.497 sec/iter\n",
      "Epoch: 30 | Batch: 003 / 011 | Total loss: 1.064 | Reg loss: 0.023 | Tree loss: 1.064 | Accuracy: 0.591500 | 1.495 sec/iter\n",
      "Epoch: 30 | Batch: 004 / 011 | Total loss: 1.055 | Reg loss: 0.023 | Tree loss: 1.055 | Accuracy: 0.586500 | 1.494 sec/iter\n",
      "Epoch: 30 | Batch: 005 / 011 | Total loss: 1.050 | Reg loss: 0.023 | Tree loss: 1.050 | Accuracy: 0.592000 | 1.493 sec/iter\n",
      "Epoch: 30 | Batch: 006 / 011 | Total loss: 1.028 | Reg loss: 0.023 | Tree loss: 1.028 | Accuracy: 0.594000 | 1.491 sec/iter\n",
      "Epoch: 30 | Batch: 007 / 011 | Total loss: 1.014 | Reg loss: 0.023 | Tree loss: 1.014 | Accuracy: 0.617000 | 1.491 sec/iter\n",
      "Epoch: 30 | Batch: 008 / 011 | Total loss: 1.001 | Reg loss: 0.023 | Tree loss: 1.001 | Accuracy: 0.610500 | 1.49 sec/iter\n",
      "Epoch: 30 | Batch: 009 / 011 | Total loss: 0.999 | Reg loss: 0.023 | Tree loss: 0.999 | Accuracy: 0.591500 | 1.49 sec/iter\n",
      "Epoch: 30 | Batch: 010 / 011 | Total loss: 1.004 | Reg loss: 0.024 | Tree loss: 1.004 | Accuracy: 0.597270 | 1.489 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 31 | Batch: 000 / 011 | Total loss: 1.139 | Reg loss: 0.023 | Tree loss: 1.139 | Accuracy: 0.576500 | 1.499 sec/iter\n",
      "Epoch: 31 | Batch: 001 / 011 | Total loss: 1.111 | Reg loss: 0.023 | Tree loss: 1.111 | Accuracy: 0.586000 | 1.498 sec/iter\n",
      "Epoch: 31 | Batch: 002 / 011 | Total loss: 1.083 | Reg loss: 0.023 | Tree loss: 1.083 | Accuracy: 0.589000 | 1.496 sec/iter\n",
      "Epoch: 31 | Batch: 003 / 011 | Total loss: 1.060 | Reg loss: 0.023 | Tree loss: 1.060 | Accuracy: 0.599500 | 1.494 sec/iter\n",
      "Epoch: 31 | Batch: 004 / 011 | Total loss: 1.029 | Reg loss: 0.023 | Tree loss: 1.029 | Accuracy: 0.616000 | 1.493 sec/iter\n",
      "Epoch: 31 | Batch: 005 / 011 | Total loss: 1.027 | Reg loss: 0.023 | Tree loss: 1.027 | Accuracy: 0.618000 | 1.492 sec/iter\n",
      "Epoch: 31 | Batch: 006 / 011 | Total loss: 1.010 | Reg loss: 0.023 | Tree loss: 1.010 | Accuracy: 0.600000 | 1.491 sec/iter\n",
      "Epoch: 31 | Batch: 007 / 011 | Total loss: 1.009 | Reg loss: 0.023 | Tree loss: 1.009 | Accuracy: 0.589000 | 1.49 sec/iter\n",
      "Epoch: 31 | Batch: 008 / 011 | Total loss: 1.017 | Reg loss: 0.024 | Tree loss: 1.017 | Accuracy: 0.569500 | 1.489 sec/iter\n",
      "Epoch: 31 | Batch: 009 / 011 | Total loss: 0.997 | Reg loss: 0.024 | Tree loss: 0.997 | Accuracy: 0.568000 | 1.487 sec/iter\n",
      "Epoch: 31 | Batch: 010 / 011 | Total loss: 0.956 | Reg loss: 0.024 | Tree loss: 0.956 | Accuracy: 0.645051 | 1.486 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 32 | Batch: 000 / 011 | Total loss: 1.131 | Reg loss: 0.023 | Tree loss: 1.131 | Accuracy: 0.582000 | 1.49 sec/iter\n",
      "Epoch: 32 | Batch: 001 / 011 | Total loss: 1.108 | Reg loss: 0.023 | Tree loss: 1.108 | Accuracy: 0.600000 | 1.489 sec/iter\n",
      "Epoch: 32 | Batch: 002 / 011 | Total loss: 1.087 | Reg loss: 0.023 | Tree loss: 1.087 | Accuracy: 0.589500 | 1.488 sec/iter\n",
      "Epoch: 32 | Batch: 003 / 011 | Total loss: 1.067 | Reg loss: 0.023 | Tree loss: 1.067 | Accuracy: 0.585500 | 1.487 sec/iter\n",
      "Epoch: 32 | Batch: 004 / 011 | Total loss: 1.053 | Reg loss: 0.023 | Tree loss: 1.053 | Accuracy: 0.587500 | 1.486 sec/iter\n",
      "Epoch: 32 | Batch: 005 / 011 | Total loss: 1.028 | Reg loss: 0.024 | Tree loss: 1.028 | Accuracy: 0.596000 | 1.485 sec/iter\n",
      "Epoch: 32 | Batch: 006 / 011 | Total loss: 1.018 | Reg loss: 0.024 | Tree loss: 1.018 | Accuracy: 0.587000 | 1.483 sec/iter\n",
      "Epoch: 32 | Batch: 007 / 011 | Total loss: 1.003 | Reg loss: 0.024 | Tree loss: 1.003 | Accuracy: 0.613000 | 1.482 sec/iter\n",
      "Epoch: 32 | Batch: 008 / 011 | Total loss: 1.001 | Reg loss: 0.024 | Tree loss: 1.001 | Accuracy: 0.593500 | 1.482 sec/iter\n",
      "Epoch: 32 | Batch: 009 / 011 | Total loss: 0.976 | Reg loss: 0.024 | Tree loss: 0.976 | Accuracy: 0.596500 | 1.482 sec/iter\n",
      "Epoch: 32 | Batch: 010 / 011 | Total loss: 0.920 | Reg loss: 0.024 | Tree loss: 0.920 | Accuracy: 0.600683 | 1.48 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 33 | Batch: 000 / 011 | Total loss: 1.111 | Reg loss: 0.023 | Tree loss: 1.111 | Accuracy: 0.606500 | 1.493 sec/iter\n",
      "Epoch: 33 | Batch: 001 / 011 | Total loss: 1.076 | Reg loss: 0.023 | Tree loss: 1.076 | Accuracy: 0.610000 | 1.492 sec/iter\n",
      "Epoch: 33 | Batch: 002 / 011 | Total loss: 1.061 | Reg loss: 0.024 | Tree loss: 1.061 | Accuracy: 0.611500 | 1.491 sec/iter\n",
      "Epoch: 33 | Batch: 003 / 011 | Total loss: 1.042 | Reg loss: 0.024 | Tree loss: 1.042 | Accuracy: 0.607000 | 1.49 sec/iter\n",
      "Epoch: 33 | Batch: 004 / 011 | Total loss: 1.006 | Reg loss: 0.024 | Tree loss: 1.006 | Accuracy: 0.605500 | 1.489 sec/iter\n",
      "Epoch: 33 | Batch: 005 / 011 | Total loss: 0.995 | Reg loss: 0.024 | Tree loss: 0.995 | Accuracy: 0.618500 | 1.487 sec/iter\n",
      "Epoch: 33 | Batch: 006 / 011 | Total loss: 1.022 | Reg loss: 0.024 | Tree loss: 1.022 | Accuracy: 0.586000 | 1.486 sec/iter\n",
      "Epoch: 33 | Batch: 007 / 011 | Total loss: 1.001 | Reg loss: 0.024 | Tree loss: 1.001 | Accuracy: 0.573000 | 1.485 sec/iter\n",
      "Epoch: 33 | Batch: 008 / 011 | Total loss: 1.011 | Reg loss: 0.024 | Tree loss: 1.011 | Accuracy: 0.565000 | 1.485 sec/iter\n",
      "Epoch: 33 | Batch: 009 / 011 | Total loss: 0.980 | Reg loss: 0.024 | Tree loss: 0.980 | Accuracy: 0.573500 | 1.484 sec/iter\n",
      "Epoch: 33 | Batch: 010 / 011 | Total loss: 0.944 | Reg loss: 0.024 | Tree loss: 0.944 | Accuracy: 0.624573 | 1.483 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 34 | Batch: 000 / 011 | Total loss: 1.094 | Reg loss: 0.024 | Tree loss: 1.094 | Accuracy: 0.607000 | 1.493 sec/iter\n",
      "Epoch: 34 | Batch: 001 / 011 | Total loss: 1.086 | Reg loss: 0.024 | Tree loss: 1.086 | Accuracy: 0.603500 | 1.493 sec/iter\n",
      "Epoch: 34 | Batch: 002 / 011 | Total loss: 1.110 | Reg loss: 0.024 | Tree loss: 1.110 | Accuracy: 0.572500 | 1.492 sec/iter\n",
      "Epoch: 34 | Batch: 003 / 011 | Total loss: 1.084 | Reg loss: 0.024 | Tree loss: 1.084 | Accuracy: 0.583000 | 1.491 sec/iter\n",
      "Epoch: 34 | Batch: 004 / 011 | Total loss: 1.040 | Reg loss: 0.024 | Tree loss: 1.040 | Accuracy: 0.576500 | 1.49 sec/iter\n",
      "Epoch: 34 | Batch: 005 / 011 | Total loss: 1.007 | Reg loss: 0.024 | Tree loss: 1.007 | Accuracy: 0.617000 | 1.489 sec/iter\n",
      "Epoch: 34 | Batch: 006 / 011 | Total loss: 0.995 | Reg loss: 0.024 | Tree loss: 0.995 | Accuracy: 0.612500 | 1.487 sec/iter\n",
      "Epoch: 34 | Batch: 007 / 011 | Total loss: 0.988 | Reg loss: 0.024 | Tree loss: 0.988 | Accuracy: 0.608000 | 1.487 sec/iter\n",
      "Epoch: 34 | Batch: 008 / 011 | Total loss: 0.985 | Reg loss: 0.024 | Tree loss: 0.985 | Accuracy: 0.599000 | 1.487 sec/iter\n",
      "Epoch: 34 | Batch: 009 / 011 | Total loss: 0.981 | Reg loss: 0.024 | Tree loss: 0.981 | Accuracy: 0.579000 | 1.486 sec/iter\n",
      "Epoch: 34 | Batch: 010 / 011 | Total loss: 0.979 | Reg loss: 0.024 | Tree loss: 0.979 | Accuracy: 0.573379 | 1.485 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 35 | Batch: 000 / 011 | Total loss: 1.093 | Reg loss: 0.024 | Tree loss: 1.093 | Accuracy: 0.611000 | 1.495 sec/iter\n",
      "Epoch: 35 | Batch: 001 / 011 | Total loss: 1.094 | Reg loss: 0.024 | Tree loss: 1.094 | Accuracy: 0.574000 | 1.494 sec/iter\n",
      "Epoch: 35 | Batch: 002 / 011 | Total loss: 1.042 | Reg loss: 0.024 | Tree loss: 1.042 | Accuracy: 0.607000 | 1.494 sec/iter\n",
      "Epoch: 35 | Batch: 003 / 011 | Total loss: 1.029 | Reg loss: 0.024 | Tree loss: 1.029 | Accuracy: 0.615000 | 1.493 sec/iter\n",
      "Epoch: 35 | Batch: 004 / 011 | Total loss: 1.015 | Reg loss: 0.024 | Tree loss: 1.015 | Accuracy: 0.603500 | 1.492 sec/iter\n",
      "Epoch: 35 | Batch: 005 / 011 | Total loss: 0.988 | Reg loss: 0.024 | Tree loss: 0.988 | Accuracy: 0.629000 | 1.491 sec/iter\n",
      "Epoch: 35 | Batch: 006 / 011 | Total loss: 0.979 | Reg loss: 0.024 | Tree loss: 0.979 | Accuracy: 0.601000 | 1.489 sec/iter\n",
      "Epoch: 35 | Batch: 007 / 011 | Total loss: 0.999 | Reg loss: 0.024 | Tree loss: 0.999 | Accuracy: 0.562000 | 1.489 sec/iter\n",
      "Epoch: 35 | Batch: 008 / 011 | Total loss: 0.987 | Reg loss: 0.024 | Tree loss: 0.987 | Accuracy: 0.572500 | 1.488 sec/iter\n",
      "Epoch: 35 | Batch: 009 / 011 | Total loss: 0.984 | Reg loss: 0.024 | Tree loss: 0.984 | Accuracy: 0.573500 | 1.488 sec/iter\n",
      "Epoch: 35 | Batch: 010 / 011 | Total loss: 0.953 | Reg loss: 0.024 | Tree loss: 0.953 | Accuracy: 0.552901 | 1.487 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 36 | Batch: 000 / 011 | Total loss: 1.092 | Reg loss: 0.024 | Tree loss: 1.092 | Accuracy: 0.596500 | 1.497 sec/iter\n",
      "Epoch: 36 | Batch: 001 / 011 | Total loss: 1.096 | Reg loss: 0.024 | Tree loss: 1.096 | Accuracy: 0.579000 | 1.496 sec/iter\n",
      "Epoch: 36 | Batch: 002 / 011 | Total loss: 1.081 | Reg loss: 0.024 | Tree loss: 1.081 | Accuracy: 0.592000 | 1.495 sec/iter\n",
      "Epoch: 36 | Batch: 003 / 011 | Total loss: 1.060 | Reg loss: 0.024 | Tree loss: 1.060 | Accuracy: 0.592500 | 1.494 sec/iter\n",
      "Epoch: 36 | Batch: 004 / 011 | Total loss: 1.051 | Reg loss: 0.024 | Tree loss: 1.051 | Accuracy: 0.583500 | 1.493 sec/iter\n",
      "Epoch: 36 | Batch: 005 / 011 | Total loss: 1.031 | Reg loss: 0.024 | Tree loss: 1.031 | Accuracy: 0.588500 | 1.492 sec/iter\n",
      "Epoch: 36 | Batch: 006 / 011 | Total loss: 0.984 | Reg loss: 0.024 | Tree loss: 0.984 | Accuracy: 0.596500 | 1.491 sec/iter\n",
      "Epoch: 36 | Batch: 007 / 011 | Total loss: 0.970 | Reg loss: 0.024 | Tree loss: 0.970 | Accuracy: 0.634500 | 1.491 sec/iter\n",
      "Epoch: 36 | Batch: 008 / 011 | Total loss: 0.990 | Reg loss: 0.024 | Tree loss: 0.990 | Accuracy: 0.588500 | 1.491 sec/iter\n",
      "Epoch: 36 | Batch: 009 / 011 | Total loss: 0.969 | Reg loss: 0.024 | Tree loss: 0.969 | Accuracy: 0.599000 | 1.49 sec/iter\n",
      "Epoch: 36 | Batch: 010 / 011 | Total loss: 0.990 | Reg loss: 0.024 | Tree loss: 0.990 | Accuracy: 0.600683 | 1.489 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 37 | Batch: 000 / 011 | Total loss: 1.081 | Reg loss: 0.024 | Tree loss: 1.081 | Accuracy: 0.605500 | 1.498 sec/iter\n",
      "Epoch: 37 | Batch: 001 / 011 | Total loss: 1.075 | Reg loss: 0.024 | Tree loss: 1.075 | Accuracy: 0.579000 | 1.497 sec/iter\n",
      "Epoch: 37 | Batch: 002 / 011 | Total loss: 1.049 | Reg loss: 0.024 | Tree loss: 1.049 | Accuracy: 0.596500 | 1.496 sec/iter\n",
      "Epoch: 37 | Batch: 003 / 011 | Total loss: 0.998 | Reg loss: 0.024 | Tree loss: 0.998 | Accuracy: 0.624000 | 1.495 sec/iter\n",
      "Epoch: 37 | Batch: 004 / 011 | Total loss: 0.982 | Reg loss: 0.024 | Tree loss: 0.982 | Accuracy: 0.631500 | 1.494 sec/iter\n",
      "Epoch: 37 | Batch: 005 / 011 | Total loss: 0.972 | Reg loss: 0.024 | Tree loss: 0.972 | Accuracy: 0.607000 | 1.493 sec/iter\n",
      "Epoch: 37 | Batch: 006 / 011 | Total loss: 0.981 | Reg loss: 0.024 | Tree loss: 0.981 | Accuracy: 0.587000 | 1.492 sec/iter\n",
      "Epoch: 37 | Batch: 007 / 011 | Total loss: 0.987 | Reg loss: 0.024 | Tree loss: 0.987 | Accuracy: 0.573500 | 1.49 sec/iter\n",
      "Epoch: 37 | Batch: 008 / 011 | Total loss: 0.985 | Reg loss: 0.024 | Tree loss: 0.985 | Accuracy: 0.566000 | 1.489 sec/iter\n",
      "Epoch: 37 | Batch: 009 / 011 | Total loss: 0.963 | Reg loss: 0.024 | Tree loss: 0.963 | Accuracy: 0.582000 | 1.488 sec/iter\n",
      "Epoch: 37 | Batch: 010 / 011 | Total loss: 0.918 | Reg loss: 0.024 | Tree loss: 0.918 | Accuracy: 0.655290 | 1.486 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 38 | Batch: 000 / 011 | Total loss: 1.082 | Reg loss: 0.024 | Tree loss: 1.082 | Accuracy: 0.589500 | 1.491 sec/iter\n",
      "Epoch: 38 | Batch: 001 / 011 | Total loss: 1.073 | Reg loss: 0.024 | Tree loss: 1.073 | Accuracy: 0.594000 | 1.49 sec/iter\n",
      "Epoch: 38 | Batch: 002 / 011 | Total loss: 1.053 | Reg loss: 0.024 | Tree loss: 1.053 | Accuracy: 0.608000 | 1.489 sec/iter\n",
      "Epoch: 38 | Batch: 003 / 011 | Total loss: 1.059 | Reg loss: 0.024 | Tree loss: 1.059 | Accuracy: 0.588000 | 1.488 sec/iter\n",
      "Epoch: 38 | Batch: 004 / 011 | Total loss: 1.027 | Reg loss: 0.024 | Tree loss: 1.027 | Accuracy: 0.597500 | 1.487 sec/iter\n",
      "Epoch: 38 | Batch: 005 / 011 | Total loss: 1.006 | Reg loss: 0.024 | Tree loss: 1.006 | Accuracy: 0.598000 | 1.486 sec/iter\n",
      "Epoch: 38 | Batch: 006 / 011 | Total loss: 0.975 | Reg loss: 0.024 | Tree loss: 0.975 | Accuracy: 0.625000 | 1.485 sec/iter\n",
      "Epoch: 38 | Batch: 007 / 011 | Total loss: 0.982 | Reg loss: 0.024 | Tree loss: 0.982 | Accuracy: 0.614500 | 1.485 sec/iter\n",
      "Epoch: 38 | Batch: 008 / 011 | Total loss: 0.961 | Reg loss: 0.024 | Tree loss: 0.961 | Accuracy: 0.615000 | 1.484 sec/iter\n",
      "Epoch: 38 | Batch: 009 / 011 | Total loss: 0.981 | Reg loss: 0.024 | Tree loss: 0.981 | Accuracy: 0.580500 | 1.484 sec/iter\n",
      "Epoch: 38 | Batch: 010 / 011 | Total loss: 0.903 | Reg loss: 0.024 | Tree loss: 0.903 | Accuracy: 0.638225 | 1.483 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 39 | Batch: 000 / 011 | Total loss: 1.079 | Reg loss: 0.024 | Tree loss: 1.079 | Accuracy: 0.584500 | 1.494 sec/iter\n",
      "Epoch: 39 | Batch: 001 / 011 | Total loss: 1.041 | Reg loss: 0.024 | Tree loss: 1.041 | Accuracy: 0.610500 | 1.494 sec/iter\n",
      "Epoch: 39 | Batch: 002 / 011 | Total loss: 1.033 | Reg loss: 0.024 | Tree loss: 1.033 | Accuracy: 0.593500 | 1.493 sec/iter\n",
      "Epoch: 39 | Batch: 003 / 011 | Total loss: 1.008 | Reg loss: 0.024 | Tree loss: 1.008 | Accuracy: 0.627500 | 1.492 sec/iter\n",
      "Epoch: 39 | Batch: 004 / 011 | Total loss: 0.987 | Reg loss: 0.024 | Tree loss: 0.987 | Accuracy: 0.638500 | 1.491 sec/iter\n",
      "Epoch: 39 | Batch: 005 / 011 | Total loss: 0.959 | Reg loss: 0.024 | Tree loss: 0.959 | Accuracy: 0.619000 | 1.489 sec/iter\n",
      "Epoch: 39 | Batch: 006 / 011 | Total loss: 0.965 | Reg loss: 0.024 | Tree loss: 0.965 | Accuracy: 0.601000 | 1.49 sec/iter\n",
      "Epoch: 39 | Batch: 007 / 011 | Total loss: 0.980 | Reg loss: 0.024 | Tree loss: 0.980 | Accuracy: 0.574500 | 1.489 sec/iter\n",
      "Epoch: 39 | Batch: 008 / 011 | Total loss: 0.967 | Reg loss: 0.024 | Tree loss: 0.967 | Accuracy: 0.580000 | 1.489 sec/iter\n",
      "Epoch: 39 | Batch: 009 / 011 | Total loss: 0.948 | Reg loss: 0.024 | Tree loss: 0.948 | Accuracy: 0.597500 | 1.488 sec/iter\n",
      "Epoch: 39 | Batch: 010 / 011 | Total loss: 0.961 | Reg loss: 0.024 | Tree loss: 0.961 | Accuracy: 0.563140 | 1.488 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 40 | Batch: 000 / 011 | Total loss: 1.076 | Reg loss: 0.024 | Tree loss: 1.076 | Accuracy: 0.614500 | 1.497 sec/iter\n",
      "Epoch: 40 | Batch: 001 / 011 | Total loss: 1.079 | Reg loss: 0.024 | Tree loss: 1.079 | Accuracy: 0.578000 | 1.496 sec/iter\n",
      "Epoch: 40 | Batch: 002 / 011 | Total loss: 1.038 | Reg loss: 0.024 | Tree loss: 1.038 | Accuracy: 0.612500 | 1.495 sec/iter\n",
      "Epoch: 40 | Batch: 003 / 011 | Total loss: 1.054 | Reg loss: 0.024 | Tree loss: 1.054 | Accuracy: 0.572500 | 1.495 sec/iter\n",
      "Epoch: 40 | Batch: 004 / 011 | Total loss: 1.016 | Reg loss: 0.024 | Tree loss: 1.016 | Accuracy: 0.600500 | 1.493 sec/iter\n",
      "Epoch: 40 | Batch: 005 / 011 | Total loss: 0.989 | Reg loss: 0.024 | Tree loss: 0.989 | Accuracy: 0.601000 | 1.492 sec/iter\n",
      "Epoch: 40 | Batch: 006 / 011 | Total loss: 0.973 | Reg loss: 0.024 | Tree loss: 0.973 | Accuracy: 0.625500 | 1.492 sec/iter\n",
      "Epoch: 40 | Batch: 007 / 011 | Total loss: 0.971 | Reg loss: 0.024 | Tree loss: 0.971 | Accuracy: 0.602500 | 1.492 sec/iter\n",
      "Epoch: 40 | Batch: 008 / 011 | Total loss: 0.949 | Reg loss: 0.024 | Tree loss: 0.949 | Accuracy: 0.612500 | 1.491 sec/iter\n",
      "Epoch: 40 | Batch: 009 / 011 | Total loss: 0.920 | Reg loss: 0.024 | Tree loss: 0.920 | Accuracy: 0.629000 | 1.491 sec/iter\n",
      "Epoch: 40 | Batch: 010 / 011 | Total loss: 0.944 | Reg loss: 0.024 | Tree loss: 0.944 | Accuracy: 0.576792 | 1.49 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 41 | Batch: 000 / 011 | Total loss: 1.076 | Reg loss: 0.024 | Tree loss: 1.076 | Accuracy: 0.600500 | 1.5 sec/iter\n",
      "Epoch: 41 | Batch: 001 / 011 | Total loss: 1.034 | Reg loss: 0.024 | Tree loss: 1.034 | Accuracy: 0.622500 | 1.5 sec/iter\n",
      "Epoch: 41 | Batch: 002 / 011 | Total loss: 1.016 | Reg loss: 0.024 | Tree loss: 1.016 | Accuracy: 0.616500 | 1.499 sec/iter\n",
      "Epoch: 41 | Batch: 003 / 011 | Total loss: 1.007 | Reg loss: 0.024 | Tree loss: 1.007 | Accuracy: 0.612000 | 1.498 sec/iter\n",
      "Epoch: 41 | Batch: 004 / 011 | Total loss: 0.972 | Reg loss: 0.024 | Tree loss: 0.972 | Accuracy: 0.632500 | 1.498 sec/iter\n",
      "Epoch: 41 | Batch: 005 / 011 | Total loss: 0.926 | Reg loss: 0.024 | Tree loss: 0.926 | Accuracy: 0.646500 | 1.497 sec/iter\n",
      "Epoch: 41 | Batch: 006 / 011 | Total loss: 0.960 | Reg loss: 0.024 | Tree loss: 0.960 | Accuracy: 0.592500 | 1.497 sec/iter\n",
      "Epoch: 41 | Batch: 007 / 011 | Total loss: 0.977 | Reg loss: 0.024 | Tree loss: 0.977 | Accuracy: 0.574000 | 1.496 sec/iter\n",
      "Epoch: 41 | Batch: 008 / 011 | Total loss: 0.964 | Reg loss: 0.024 | Tree loss: 0.964 | Accuracy: 0.569500 | 1.496 sec/iter\n",
      "Epoch: 41 | Batch: 009 / 011 | Total loss: 0.956 | Reg loss: 0.024 | Tree loss: 0.956 | Accuracy: 0.584000 | 1.495 sec/iter\n",
      "Epoch: 41 | Batch: 010 / 011 | Total loss: 0.888 | Reg loss: 0.024 | Tree loss: 0.888 | Accuracy: 0.631399 | 1.494 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 42 | Batch: 000 / 011 | Total loss: 1.055 | Reg loss: 0.024 | Tree loss: 1.055 | Accuracy: 0.620500 | 1.503 sec/iter\n",
      "Epoch: 42 | Batch: 001 / 011 | Total loss: 1.054 | Reg loss: 0.024 | Tree loss: 1.054 | Accuracy: 0.601500 | 1.502 sec/iter\n",
      "Epoch: 42 | Batch: 002 / 011 | Total loss: 1.051 | Reg loss: 0.024 | Tree loss: 1.051 | Accuracy: 0.587000 | 1.501 sec/iter\n",
      "Epoch: 42 | Batch: 003 / 011 | Total loss: 1.021 | Reg loss: 0.024 | Tree loss: 1.021 | Accuracy: 0.598000 | 1.5 sec/iter\n",
      "Epoch: 42 | Batch: 004 / 011 | Total loss: 0.973 | Reg loss: 0.024 | Tree loss: 0.973 | Accuracy: 0.613500 | 1.5 sec/iter\n",
      "Epoch: 42 | Batch: 005 / 011 | Total loss: 0.995 | Reg loss: 0.024 | Tree loss: 0.995 | Accuracy: 0.612000 | 1.499 sec/iter\n",
      "Epoch: 42 | Batch: 006 / 011 | Total loss: 0.980 | Reg loss: 0.024 | Tree loss: 0.980 | Accuracy: 0.631000 | 1.499 sec/iter\n",
      "Epoch: 42 | Batch: 007 / 011 | Total loss: 0.958 | Reg loss: 0.024 | Tree loss: 0.958 | Accuracy: 0.594000 | 1.499 sec/iter\n",
      "Epoch: 42 | Batch: 008 / 011 | Total loss: 0.935 | Reg loss: 0.024 | Tree loss: 0.935 | Accuracy: 0.610000 | 1.498 sec/iter\n",
      "Epoch: 42 | Batch: 009 / 011 | Total loss: 0.925 | Reg loss: 0.024 | Tree loss: 0.925 | Accuracy: 0.604500 | 1.498 sec/iter\n",
      "Epoch: 42 | Batch: 010 / 011 | Total loss: 1.029 | Reg loss: 0.024 | Tree loss: 1.029 | Accuracy: 0.522184 | 1.497 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 43 | Batch: 000 / 011 | Total loss: 1.060 | Reg loss: 0.024 | Tree loss: 1.060 | Accuracy: 0.609000 | 1.501 sec/iter\n",
      "Epoch: 43 | Batch: 001 / 011 | Total loss: 1.029 | Reg loss: 0.024 | Tree loss: 1.029 | Accuracy: 0.617000 | 1.5 sec/iter\n",
      "Epoch: 43 | Batch: 002 / 011 | Total loss: 1.035 | Reg loss: 0.024 | Tree loss: 1.035 | Accuracy: 0.591500 | 1.499 sec/iter\n",
      "Epoch: 43 | Batch: 003 / 011 | Total loss: 0.980 | Reg loss: 0.024 | Tree loss: 0.980 | Accuracy: 0.614500 | 1.498 sec/iter\n",
      "Epoch: 43 | Batch: 004 / 011 | Total loss: 0.978 | Reg loss: 0.024 | Tree loss: 0.978 | Accuracy: 0.615000 | 1.497 sec/iter\n",
      "Epoch: 43 | Batch: 005 / 011 | Total loss: 0.940 | Reg loss: 0.024 | Tree loss: 0.940 | Accuracy: 0.655000 | 1.496 sec/iter\n",
      "Epoch: 43 | Batch: 006 / 011 | Total loss: 0.950 | Reg loss: 0.024 | Tree loss: 0.950 | Accuracy: 0.605000 | 1.495 sec/iter\n",
      "Epoch: 43 | Batch: 007 / 011 | Total loss: 0.954 | Reg loss: 0.024 | Tree loss: 0.954 | Accuracy: 0.587500 | 1.495 sec/iter\n",
      "Epoch: 43 | Batch: 008 / 011 | Total loss: 0.940 | Reg loss: 0.024 | Tree loss: 0.940 | Accuracy: 0.592000 | 1.494 sec/iter\n",
      "Epoch: 43 | Batch: 009 / 011 | Total loss: 0.948 | Reg loss: 0.024 | Tree loss: 0.948 | Accuracy: 0.581000 | 1.493 sec/iter\n",
      "Epoch: 43 | Batch: 010 / 011 | Total loss: 0.948 | Reg loss: 0.024 | Tree loss: 0.948 | Accuracy: 0.566553 | 1.492 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 44 | Batch: 000 / 011 | Total loss: 1.027 | Reg loss: 0.024 | Tree loss: 1.027 | Accuracy: 0.628000 | 1.495 sec/iter\n",
      "Epoch: 44 | Batch: 001 / 011 | Total loss: 1.042 | Reg loss: 0.024 | Tree loss: 1.042 | Accuracy: 0.601000 | 1.495 sec/iter\n",
      "Epoch: 44 | Batch: 002 / 011 | Total loss: 1.034 | Reg loss: 0.024 | Tree loss: 1.034 | Accuracy: 0.598500 | 1.494 sec/iter\n",
      "Epoch: 44 | Batch: 003 / 011 | Total loss: 1.025 | Reg loss: 0.024 | Tree loss: 1.025 | Accuracy: 0.598500 | 1.494 sec/iter\n",
      "Epoch: 44 | Batch: 004 / 011 | Total loss: 0.985 | Reg loss: 0.024 | Tree loss: 0.985 | Accuracy: 0.613500 | 1.494 sec/iter\n",
      "Epoch: 44 | Batch: 005 / 011 | Total loss: 0.979 | Reg loss: 0.024 | Tree loss: 0.979 | Accuracy: 0.613500 | 1.493 sec/iter\n",
      "Epoch: 44 | Batch: 006 / 011 | Total loss: 0.937 | Reg loss: 0.024 | Tree loss: 0.937 | Accuracy: 0.648000 | 1.493 sec/iter\n",
      "Epoch: 44 | Batch: 007 / 011 | Total loss: 0.971 | Reg loss: 0.024 | Tree loss: 0.971 | Accuracy: 0.606000 | 1.493 sec/iter\n",
      "Epoch: 44 | Batch: 008 / 011 | Total loss: 0.938 | Reg loss: 0.024 | Tree loss: 0.938 | Accuracy: 0.602500 | 1.492 sec/iter\n",
      "Epoch: 44 | Batch: 009 / 011 | Total loss: 0.919 | Reg loss: 0.025 | Tree loss: 0.919 | Accuracy: 0.596500 | 1.492 sec/iter\n",
      "Epoch: 44 | Batch: 010 / 011 | Total loss: 0.942 | Reg loss: 0.025 | Tree loss: 0.942 | Accuracy: 0.556314 | 1.491 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 45 | Batch: 000 / 011 | Total loss: 1.053 | Reg loss: 0.024 | Tree loss: 1.053 | Accuracy: 0.611000 | 1.495 sec/iter\n",
      "Epoch: 45 | Batch: 001 / 011 | Total loss: 1.018 | Reg loss: 0.024 | Tree loss: 1.018 | Accuracy: 0.625000 | 1.495 sec/iter\n",
      "Epoch: 45 | Batch: 002 / 011 | Total loss: 1.017 | Reg loss: 0.024 | Tree loss: 1.017 | Accuracy: 0.611000 | 1.495 sec/iter\n",
      "Epoch: 45 | Batch: 003 / 011 | Total loss: 0.979 | Reg loss: 0.024 | Tree loss: 0.979 | Accuracy: 0.630000 | 1.494 sec/iter\n",
      "Epoch: 45 | Batch: 004 / 011 | Total loss: 0.959 | Reg loss: 0.024 | Tree loss: 0.959 | Accuracy: 0.628000 | 1.494 sec/iter\n",
      "Epoch: 45 | Batch: 005 / 011 | Total loss: 0.921 | Reg loss: 0.024 | Tree loss: 0.921 | Accuracy: 0.654500 | 1.494 sec/iter\n",
      "Epoch: 45 | Batch: 006 / 011 | Total loss: 0.948 | Reg loss: 0.024 | Tree loss: 0.948 | Accuracy: 0.603500 | 1.493 sec/iter\n",
      "Epoch: 45 | Batch: 007 / 011 | Total loss: 0.938 | Reg loss: 0.024 | Tree loss: 0.938 | Accuracy: 0.592500 | 1.492 sec/iter\n",
      "Epoch: 45 | Batch: 008 / 011 | Total loss: 0.954 | Reg loss: 0.025 | Tree loss: 0.954 | Accuracy: 0.571500 | 1.492 sec/iter\n",
      "Epoch: 45 | Batch: 009 / 011 | Total loss: 0.941 | Reg loss: 0.025 | Tree loss: 0.941 | Accuracy: 0.582500 | 1.491 sec/iter\n",
      "Epoch: 45 | Batch: 010 / 011 | Total loss: 0.901 | Reg loss: 0.025 | Tree loss: 0.901 | Accuracy: 0.614334 | 1.49 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 46 | Batch: 000 / 011 | Total loss: 1.047 | Reg loss: 0.024 | Tree loss: 1.047 | Accuracy: 0.609500 | 1.496 sec/iter\n",
      "Epoch: 46 | Batch: 001 / 011 | Total loss: 1.045 | Reg loss: 0.024 | Tree loss: 1.045 | Accuracy: 0.603000 | 1.496 sec/iter\n",
      "Epoch: 46 | Batch: 002 / 011 | Total loss: 1.020 | Reg loss: 0.024 | Tree loss: 1.020 | Accuracy: 0.618000 | 1.495 sec/iter\n",
      "Epoch: 46 | Batch: 003 / 011 | Total loss: 0.987 | Reg loss: 0.024 | Tree loss: 0.987 | Accuracy: 0.629500 | 1.495 sec/iter\n",
      "Epoch: 46 | Batch: 004 / 011 | Total loss: 0.995 | Reg loss: 0.024 | Tree loss: 0.995 | Accuracy: 0.600500 | 1.494 sec/iter\n",
      "Epoch: 46 | Batch: 005 / 011 | Total loss: 0.976 | Reg loss: 0.025 | Tree loss: 0.976 | Accuracy: 0.597500 | 1.494 sec/iter\n",
      "Epoch: 46 | Batch: 006 / 011 | Total loss: 0.948 | Reg loss: 0.025 | Tree loss: 0.948 | Accuracy: 0.647000 | 1.494 sec/iter\n",
      "Epoch: 46 | Batch: 007 / 011 | Total loss: 0.953 | Reg loss: 0.025 | Tree loss: 0.953 | Accuracy: 0.610500 | 1.493 sec/iter\n",
      "Epoch: 46 | Batch: 008 / 011 | Total loss: 0.924 | Reg loss: 0.025 | Tree loss: 0.924 | Accuracy: 0.603000 | 1.492 sec/iter\n",
      "Epoch: 46 | Batch: 009 / 011 | Total loss: 0.909 | Reg loss: 0.025 | Tree loss: 0.909 | Accuracy: 0.606500 | 1.492 sec/iter\n",
      "Epoch: 46 | Batch: 010 / 011 | Total loss: 0.894 | Reg loss: 0.025 | Tree loss: 0.894 | Accuracy: 0.607509 | 1.491 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 47 | Batch: 000 / 011 | Total loss: 1.022 | Reg loss: 0.024 | Tree loss: 1.022 | Accuracy: 0.640500 | 1.497 sec/iter\n",
      "Epoch: 47 | Batch: 001 / 011 | Total loss: 1.011 | Reg loss: 0.024 | Tree loss: 1.011 | Accuracy: 0.618000 | 1.496 sec/iter\n",
      "Epoch: 47 | Batch: 002 / 011 | Total loss: 0.996 | Reg loss: 0.024 | Tree loss: 0.996 | Accuracy: 0.622000 | 1.496 sec/iter\n",
      "Epoch: 47 | Batch: 003 / 011 | Total loss: 0.999 | Reg loss: 0.024 | Tree loss: 0.999 | Accuracy: 0.611500 | 1.495 sec/iter\n",
      "Epoch: 47 | Batch: 004 / 011 | Total loss: 0.967 | Reg loss: 0.025 | Tree loss: 0.967 | Accuracy: 0.642000 | 1.495 sec/iter\n",
      "Epoch: 47 | Batch: 005 / 011 | Total loss: 0.931 | Reg loss: 0.025 | Tree loss: 0.931 | Accuracy: 0.632000 | 1.495 sec/iter\n",
      "Epoch: 47 | Batch: 006 / 011 | Total loss: 0.919 | Reg loss: 0.025 | Tree loss: 0.919 | Accuracy: 0.609000 | 1.495 sec/iter\n",
      "Epoch: 47 | Batch: 007 / 011 | Total loss: 0.926 | Reg loss: 0.025 | Tree loss: 0.926 | Accuracy: 0.590000 | 1.494 sec/iter\n",
      "Epoch: 47 | Batch: 008 / 011 | Total loss: 0.944 | Reg loss: 0.025 | Tree loss: 0.944 | Accuracy: 0.576500 | 1.493 sec/iter\n",
      "Epoch: 47 | Batch: 009 / 011 | Total loss: 0.927 | Reg loss: 0.025 | Tree loss: 0.927 | Accuracy: 0.593000 | 1.493 sec/iter\n",
      "Epoch: 47 | Batch: 010 / 011 | Total loss: 0.874 | Reg loss: 0.025 | Tree loss: 0.874 | Accuracy: 0.604096 | 1.492 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 48 | Batch: 000 / 011 | Total loss: 1.056 | Reg loss: 0.025 | Tree loss: 1.056 | Accuracy: 0.598000 | 1.497 sec/iter\n",
      "Epoch: 48 | Batch: 001 / 011 | Total loss: 1.034 | Reg loss: 0.025 | Tree loss: 1.034 | Accuracy: 0.617500 | 1.497 sec/iter\n",
      "Epoch: 48 | Batch: 002 / 011 | Total loss: 1.016 | Reg loss: 0.025 | Tree loss: 1.016 | Accuracy: 0.610500 | 1.497 sec/iter\n",
      "Epoch: 48 | Batch: 003 / 011 | Total loss: 1.015 | Reg loss: 0.025 | Tree loss: 1.015 | Accuracy: 0.619000 | 1.496 sec/iter\n",
      "Epoch: 48 | Batch: 004 / 011 | Total loss: 0.993 | Reg loss: 0.025 | Tree loss: 0.993 | Accuracy: 0.609000 | 1.496 sec/iter\n",
      "Epoch: 48 | Batch: 005 / 011 | Total loss: 0.967 | Reg loss: 0.025 | Tree loss: 0.967 | Accuracy: 0.618000 | 1.496 sec/iter\n",
      "Epoch: 48 | Batch: 006 / 011 | Total loss: 0.925 | Reg loss: 0.025 | Tree loss: 0.925 | Accuracy: 0.672000 | 1.495 sec/iter\n",
      "Epoch: 48 | Batch: 007 / 011 | Total loss: 0.928 | Reg loss: 0.025 | Tree loss: 0.928 | Accuracy: 0.632500 | 1.495 sec/iter\n",
      "Epoch: 48 | Batch: 008 / 011 | Total loss: 0.914 | Reg loss: 0.025 | Tree loss: 0.914 | Accuracy: 0.619500 | 1.494 sec/iter\n",
      "Epoch: 48 | Batch: 009 / 011 | Total loss: 0.907 | Reg loss: 0.025 | Tree loss: 0.907 | Accuracy: 0.598000 | 1.493 sec/iter\n",
      "Epoch: 48 | Batch: 010 / 011 | Total loss: 0.927 | Reg loss: 0.025 | Tree loss: 0.927 | Accuracy: 0.597270 | 1.492 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 49 | Batch: 000 / 011 | Total loss: 1.039 | Reg loss: 0.025 | Tree loss: 1.039 | Accuracy: 0.628000 | 1.498 sec/iter\n",
      "Epoch: 49 | Batch: 001 / 011 | Total loss: 1.022 | Reg loss: 0.025 | Tree loss: 1.022 | Accuracy: 0.617500 | 1.498 sec/iter\n",
      "Epoch: 49 | Batch: 002 / 011 | Total loss: 0.988 | Reg loss: 0.025 | Tree loss: 0.988 | Accuracy: 0.617000 | 1.497 sec/iter\n",
      "Epoch: 49 | Batch: 003 / 011 | Total loss: 0.959 | Reg loss: 0.025 | Tree loss: 0.959 | Accuracy: 0.636000 | 1.496 sec/iter\n",
      "Epoch: 49 | Batch: 004 / 011 | Total loss: 0.943 | Reg loss: 0.025 | Tree loss: 0.943 | Accuracy: 0.655500 | 1.495 sec/iter\n",
      "Epoch: 49 | Batch: 005 / 011 | Total loss: 0.928 | Reg loss: 0.025 | Tree loss: 0.928 | Accuracy: 0.624000 | 1.494 sec/iter\n",
      "Epoch: 49 | Batch: 006 / 011 | Total loss: 0.929 | Reg loss: 0.025 | Tree loss: 0.929 | Accuracy: 0.585000 | 1.493 sec/iter\n",
      "Epoch: 49 | Batch: 007 / 011 | Total loss: 0.911 | Reg loss: 0.025 | Tree loss: 0.911 | Accuracy: 0.597000 | 1.492 sec/iter\n",
      "Epoch: 49 | Batch: 008 / 011 | Total loss: 0.928 | Reg loss: 0.025 | Tree loss: 0.928 | Accuracy: 0.590500 | 1.491 sec/iter\n",
      "Epoch: 49 | Batch: 009 / 011 | Total loss: 0.924 | Reg loss: 0.025 | Tree loss: 0.924 | Accuracy: 0.594000 | 1.49 sec/iter\n",
      "Epoch: 49 | Batch: 010 / 011 | Total loss: 0.910 | Reg loss: 0.025 | Tree loss: 0.910 | Accuracy: 0.604096 | 1.489 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 50 | Batch: 000 / 011 | Total loss: 1.021 | Reg loss: 0.025 | Tree loss: 1.021 | Accuracy: 0.619500 | 1.491 sec/iter\n",
      "Epoch: 50 | Batch: 001 / 011 | Total loss: 1.033 | Reg loss: 0.025 | Tree loss: 1.033 | Accuracy: 0.612500 | 1.49 sec/iter\n",
      "Epoch: 50 | Batch: 002 / 011 | Total loss: 1.035 | Reg loss: 0.025 | Tree loss: 1.035 | Accuracy: 0.598000 | 1.489 sec/iter\n",
      "Epoch: 50 | Batch: 003 / 011 | Total loss: 0.980 | Reg loss: 0.025 | Tree loss: 0.980 | Accuracy: 0.627000 | 1.489 sec/iter\n",
      "Epoch: 50 | Batch: 004 / 011 | Total loss: 0.959 | Reg loss: 0.025 | Tree loss: 0.959 | Accuracy: 0.613500 | 1.488 sec/iter\n",
      "Epoch: 50 | Batch: 005 / 011 | Total loss: 0.956 | Reg loss: 0.025 | Tree loss: 0.956 | Accuracy: 0.621000 | 1.488 sec/iter\n",
      "Epoch: 50 | Batch: 006 / 011 | Total loss: 0.918 | Reg loss: 0.025 | Tree loss: 0.918 | Accuracy: 0.660500 | 1.487 sec/iter\n",
      "Epoch: 50 | Batch: 007 / 011 | Total loss: 0.926 | Reg loss: 0.025 | Tree loss: 0.926 | Accuracy: 0.633000 | 1.487 sec/iter\n",
      "Epoch: 50 | Batch: 008 / 011 | Total loss: 0.906 | Reg loss: 0.025 | Tree loss: 0.906 | Accuracy: 0.598500 | 1.487 sec/iter\n",
      "Epoch: 50 | Batch: 009 / 011 | Total loss: 0.907 | Reg loss: 0.025 | Tree loss: 0.907 | Accuracy: 0.584000 | 1.486 sec/iter\n",
      "Epoch: 50 | Batch: 010 / 011 | Total loss: 0.834 | Reg loss: 0.025 | Tree loss: 0.834 | Accuracy: 0.648464 | 1.485 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 51 | Batch: 000 / 011 | Total loss: 1.028 | Reg loss: 0.025 | Tree loss: 1.028 | Accuracy: 0.640000 | 1.496 sec/iter\n",
      "Epoch: 51 | Batch: 001 / 011 | Total loss: 1.007 | Reg loss: 0.025 | Tree loss: 1.007 | Accuracy: 0.615000 | 1.496 sec/iter\n",
      "Epoch: 51 | Batch: 002 / 011 | Total loss: 0.986 | Reg loss: 0.025 | Tree loss: 0.986 | Accuracy: 0.623500 | 1.495 sec/iter\n",
      "Epoch: 51 | Batch: 003 / 011 | Total loss: 0.963 | Reg loss: 0.025 | Tree loss: 0.963 | Accuracy: 0.614500 | 1.495 sec/iter\n",
      "Epoch: 51 | Batch: 004 / 011 | Total loss: 0.927 | Reg loss: 0.025 | Tree loss: 0.927 | Accuracy: 0.676500 | 1.494 sec/iter\n",
      "Epoch: 51 | Batch: 005 / 011 | Total loss: 0.926 | Reg loss: 0.025 | Tree loss: 0.926 | Accuracy: 0.633500 | 1.494 sec/iter\n",
      "Epoch: 51 | Batch: 006 / 011 | Total loss: 0.905 | Reg loss: 0.025 | Tree loss: 0.905 | Accuracy: 0.607000 | 1.493 sec/iter\n",
      "Epoch: 51 | Batch: 007 / 011 | Total loss: 0.949 | Reg loss: 0.025 | Tree loss: 0.949 | Accuracy: 0.565500 | 1.492 sec/iter\n",
      "Epoch: 51 | Batch: 008 / 011 | Total loss: 0.919 | Reg loss: 0.025 | Tree loss: 0.919 | Accuracy: 0.582500 | 1.491 sec/iter\n",
      "Epoch: 51 | Batch: 009 / 011 | Total loss: 0.924 | Reg loss: 0.025 | Tree loss: 0.924 | Accuracy: 0.593000 | 1.491 sec/iter\n",
      "Epoch: 51 | Batch: 010 / 011 | Total loss: 0.888 | Reg loss: 0.025 | Tree loss: 0.888 | Accuracy: 0.604096 | 1.491 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 52 | Batch: 000 / 011 | Total loss: 1.033 | Reg loss: 0.025 | Tree loss: 1.033 | Accuracy: 0.624500 | 1.5 sec/iter\n",
      "Epoch: 52 | Batch: 001 / 011 | Total loss: 1.029 | Reg loss: 0.025 | Tree loss: 1.029 | Accuracy: 0.603000 | 1.499 sec/iter\n",
      "Epoch: 52 | Batch: 002 / 011 | Total loss: 0.994 | Reg loss: 0.025 | Tree loss: 0.994 | Accuracy: 0.617000 | 1.499 sec/iter\n",
      "Epoch: 52 | Batch: 003 / 011 | Total loss: 1.010 | Reg loss: 0.025 | Tree loss: 1.010 | Accuracy: 0.607500 | 1.498 sec/iter\n",
      "Epoch: 52 | Batch: 004 / 011 | Total loss: 0.965 | Reg loss: 0.025 | Tree loss: 0.965 | Accuracy: 0.616500 | 1.497 sec/iter\n",
      "Epoch: 52 | Batch: 005 / 011 | Total loss: 0.945 | Reg loss: 0.025 | Tree loss: 0.945 | Accuracy: 0.631000 | 1.497 sec/iter\n",
      "Epoch: 52 | Batch: 006 / 011 | Total loss: 0.911 | Reg loss: 0.025 | Tree loss: 0.911 | Accuracy: 0.671000 | 1.496 sec/iter\n",
      "Epoch: 52 | Batch: 007 / 011 | Total loss: 0.937 | Reg loss: 0.025 | Tree loss: 0.937 | Accuracy: 0.626000 | 1.496 sec/iter\n",
      "Epoch: 52 | Batch: 008 / 011 | Total loss: 0.927 | Reg loss: 0.025 | Tree loss: 0.927 | Accuracy: 0.598000 | 1.495 sec/iter\n",
      "Epoch: 52 | Batch: 009 / 011 | Total loss: 0.905 | Reg loss: 0.025 | Tree loss: 0.905 | Accuracy: 0.606500 | 1.495 sec/iter\n",
      "Epoch: 52 | Batch: 010 / 011 | Total loss: 0.947 | Reg loss: 0.025 | Tree loss: 0.947 | Accuracy: 0.556314 | 1.494 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 53 | Batch: 000 / 011 | Total loss: 1.039 | Reg loss: 0.025 | Tree loss: 1.039 | Accuracy: 0.612000 | 1.5 sec/iter\n",
      "Epoch: 53 | Batch: 001 / 011 | Total loss: 0.993 | Reg loss: 0.025 | Tree loss: 0.993 | Accuracy: 0.629500 | 1.5 sec/iter\n",
      "Epoch: 53 | Batch: 002 / 011 | Total loss: 0.976 | Reg loss: 0.025 | Tree loss: 0.976 | Accuracy: 0.635000 | 1.5 sec/iter\n",
      "Epoch: 53 | Batch: 003 / 011 | Total loss: 0.948 | Reg loss: 0.025 | Tree loss: 0.948 | Accuracy: 0.633000 | 1.499 sec/iter\n",
      "Epoch: 53 | Batch: 004 / 011 | Total loss: 0.958 | Reg loss: 0.025 | Tree loss: 0.958 | Accuracy: 0.621000 | 1.498 sec/iter\n",
      "Epoch: 53 | Batch: 005 / 011 | Total loss: 0.924 | Reg loss: 0.025 | Tree loss: 0.924 | Accuracy: 0.645000 | 1.498 sec/iter\n",
      "Epoch: 53 | Batch: 006 / 011 | Total loss: 0.927 | Reg loss: 0.025 | Tree loss: 0.927 | Accuracy: 0.609500 | 1.497 sec/iter\n",
      "Epoch: 53 | Batch: 007 / 011 | Total loss: 0.909 | Reg loss: 0.025 | Tree loss: 0.909 | Accuracy: 0.595500 | 1.497 sec/iter\n",
      "Epoch: 53 | Batch: 008 / 011 | Total loss: 0.906 | Reg loss: 0.025 | Tree loss: 0.906 | Accuracy: 0.594500 | 1.496 sec/iter\n",
      "Epoch: 53 | Batch: 009 / 011 | Total loss: 0.904 | Reg loss: 0.025 | Tree loss: 0.904 | Accuracy: 0.594500 | 1.496 sec/iter\n",
      "Epoch: 53 | Batch: 010 / 011 | Total loss: 0.871 | Reg loss: 0.025 | Tree loss: 0.871 | Accuracy: 0.638225 | 1.495 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 54 | Batch: 000 / 011 | Total loss: 1.017 | Reg loss: 0.025 | Tree loss: 1.017 | Accuracy: 0.631500 | 1.502 sec/iter\n",
      "Epoch: 54 | Batch: 001 / 011 | Total loss: 0.999 | Reg loss: 0.025 | Tree loss: 0.999 | Accuracy: 0.635500 | 1.502 sec/iter\n",
      "Epoch: 54 | Batch: 002 / 011 | Total loss: 1.004 | Reg loss: 0.025 | Tree loss: 1.004 | Accuracy: 0.609500 | 1.501 sec/iter\n",
      "Epoch: 54 | Batch: 003 / 011 | Total loss: 0.976 | Reg loss: 0.025 | Tree loss: 0.976 | Accuracy: 0.612500 | 1.5 sec/iter\n",
      "Epoch: 54 | Batch: 004 / 011 | Total loss: 0.959 | Reg loss: 0.025 | Tree loss: 0.959 | Accuracy: 0.621000 | 1.5 sec/iter\n",
      "Epoch: 54 | Batch: 005 / 011 | Total loss: 0.935 | Reg loss: 0.025 | Tree loss: 0.935 | Accuracy: 0.668500 | 1.499 sec/iter\n",
      "Epoch: 54 | Batch: 006 / 011 | Total loss: 0.906 | Reg loss: 0.025 | Tree loss: 0.906 | Accuracy: 0.658500 | 1.498 sec/iter\n",
      "Epoch: 54 | Batch: 007 / 011 | Total loss: 0.904 | Reg loss: 0.025 | Tree loss: 0.904 | Accuracy: 0.612500 | 1.498 sec/iter\n",
      "Epoch: 54 | Batch: 008 / 011 | Total loss: 0.918 | Reg loss: 0.025 | Tree loss: 0.918 | Accuracy: 0.597000 | 1.498 sec/iter\n",
      "Epoch: 54 | Batch: 009 / 011 | Total loss: 0.938 | Reg loss: 0.025 | Tree loss: 0.938 | Accuracy: 0.570000 | 1.497 sec/iter\n",
      "Epoch: 54 | Batch: 010 / 011 | Total loss: 0.951 | Reg loss: 0.025 | Tree loss: 0.951 | Accuracy: 0.566553 | 1.497 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 55 | Batch: 000 / 011 | Total loss: 0.997 | Reg loss: 0.025 | Tree loss: 0.997 | Accuracy: 0.636000 | 1.503 sec/iter\n",
      "Epoch: 55 | Batch: 001 / 011 | Total loss: 1.000 | Reg loss: 0.025 | Tree loss: 1.000 | Accuracy: 0.632000 | 1.503 sec/iter\n",
      "Epoch: 55 | Batch: 002 / 011 | Total loss: 0.991 | Reg loss: 0.025 | Tree loss: 0.991 | Accuracy: 0.635500 | 1.502 sec/iter\n",
      "Epoch: 55 | Batch: 003 / 011 | Total loss: 0.961 | Reg loss: 0.025 | Tree loss: 0.961 | Accuracy: 0.634500 | 1.501 sec/iter\n",
      "Epoch: 55 | Batch: 004 / 011 | Total loss: 0.936 | Reg loss: 0.025 | Tree loss: 0.936 | Accuracy: 0.634000 | 1.5 sec/iter\n",
      "Epoch: 55 | Batch: 005 / 011 | Total loss: 0.905 | Reg loss: 0.025 | Tree loss: 0.905 | Accuracy: 0.668500 | 1.499 sec/iter\n",
      "Epoch: 55 | Batch: 006 / 011 | Total loss: 0.919 | Reg loss: 0.025 | Tree loss: 0.919 | Accuracy: 0.638500 | 1.499 sec/iter\n",
      "Epoch: 55 | Batch: 007 / 011 | Total loss: 0.913 | Reg loss: 0.025 | Tree loss: 0.913 | Accuracy: 0.615000 | 1.498 sec/iter\n",
      "Epoch: 55 | Batch: 008 / 011 | Total loss: 0.926 | Reg loss: 0.025 | Tree loss: 0.926 | Accuracy: 0.582500 | 1.497 sec/iter\n",
      "Epoch: 55 | Batch: 009 / 011 | Total loss: 0.916 | Reg loss: 0.025 | Tree loss: 0.916 | Accuracy: 0.587500 | 1.496 sec/iter\n",
      "Epoch: 55 | Batch: 010 / 011 | Total loss: 0.905 | Reg loss: 0.025 | Tree loss: 0.905 | Accuracy: 0.593857 | 1.495 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 56 | Batch: 000 / 011 | Total loss: 1.023 | Reg loss: 0.025 | Tree loss: 1.023 | Accuracy: 0.614500 | 1.498 sec/iter\n",
      "Epoch: 56 | Batch: 001 / 011 | Total loss: 0.988 | Reg loss: 0.025 | Tree loss: 0.988 | Accuracy: 0.644000 | 1.497 sec/iter\n",
      "Epoch: 56 | Batch: 002 / 011 | Total loss: 0.991 | Reg loss: 0.025 | Tree loss: 0.991 | Accuracy: 0.652500 | 1.496 sec/iter\n",
      "Epoch: 56 | Batch: 003 / 011 | Total loss: 0.974 | Reg loss: 0.025 | Tree loss: 0.974 | Accuracy: 0.628000 | 1.496 sec/iter\n",
      "Epoch: 56 | Batch: 004 / 011 | Total loss: 0.969 | Reg loss: 0.025 | Tree loss: 0.969 | Accuracy: 0.629500 | 1.495 sec/iter\n",
      "Epoch: 56 | Batch: 005 / 011 | Total loss: 0.928 | Reg loss: 0.025 | Tree loss: 0.928 | Accuracy: 0.653500 | 1.494 sec/iter\n",
      "Epoch: 56 | Batch: 006 / 011 | Total loss: 0.931 | Reg loss: 0.025 | Tree loss: 0.931 | Accuracy: 0.648500 | 1.494 sec/iter\n",
      "Epoch: 56 | Batch: 007 / 011 | Total loss: 0.919 | Reg loss: 0.025 | Tree loss: 0.919 | Accuracy: 0.610500 | 1.493 sec/iter\n",
      "Epoch: 56 | Batch: 008 / 011 | Total loss: 0.913 | Reg loss: 0.025 | Tree loss: 0.913 | Accuracy: 0.594500 | 1.493 sec/iter\n",
      "Epoch: 56 | Batch: 009 / 011 | Total loss: 0.902 | Reg loss: 0.025 | Tree loss: 0.902 | Accuracy: 0.584500 | 1.493 sec/iter\n",
      "Epoch: 56 | Batch: 010 / 011 | Total loss: 0.891 | Reg loss: 0.025 | Tree loss: 0.891 | Accuracy: 0.583618 | 1.492 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 57 | Batch: 000 / 011 | Total loss: 1.015 | Reg loss: 0.025 | Tree loss: 1.015 | Accuracy: 0.626500 | 1.499 sec/iter\n",
      "Epoch: 57 | Batch: 001 / 011 | Total loss: 1.001 | Reg loss: 0.025 | Tree loss: 1.001 | Accuracy: 0.635000 | 1.499 sec/iter\n",
      "Epoch: 57 | Batch: 002 / 011 | Total loss: 0.967 | Reg loss: 0.025 | Tree loss: 0.967 | Accuracy: 0.635500 | 1.498 sec/iter\n",
      "Epoch: 57 | Batch: 003 / 011 | Total loss: 0.948 | Reg loss: 0.025 | Tree loss: 0.948 | Accuracy: 0.629000 | 1.498 sec/iter\n",
      "Epoch: 57 | Batch: 004 / 011 | Total loss: 0.926 | Reg loss: 0.025 | Tree loss: 0.926 | Accuracy: 0.656000 | 1.497 sec/iter\n",
      "Epoch: 57 | Batch: 005 / 011 | Total loss: 0.919 | Reg loss: 0.025 | Tree loss: 0.919 | Accuracy: 0.660000 | 1.497 sec/iter\n",
      "Epoch: 57 | Batch: 006 / 011 | Total loss: 0.884 | Reg loss: 0.025 | Tree loss: 0.884 | Accuracy: 0.648500 | 1.496 sec/iter\n",
      "Epoch: 57 | Batch: 007 / 011 | Total loss: 0.921 | Reg loss: 0.025 | Tree loss: 0.921 | Accuracy: 0.594500 | 1.495 sec/iter\n",
      "Epoch: 57 | Batch: 008 / 011 | Total loss: 0.919 | Reg loss: 0.025 | Tree loss: 0.919 | Accuracy: 0.596000 | 1.495 sec/iter\n",
      "Epoch: 57 | Batch: 009 / 011 | Total loss: 0.923 | Reg loss: 0.025 | Tree loss: 0.923 | Accuracy: 0.597000 | 1.495 sec/iter\n",
      "Epoch: 57 | Batch: 010 / 011 | Total loss: 0.895 | Reg loss: 0.025 | Tree loss: 0.895 | Accuracy: 0.641638 | 1.494 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 58 | Batch: 000 / 011 | Total loss: 0.998 | Reg loss: 0.025 | Tree loss: 0.998 | Accuracy: 0.647500 | 1.5 sec/iter\n",
      "Epoch: 58 | Batch: 001 / 011 | Total loss: 0.987 | Reg loss: 0.025 | Tree loss: 0.987 | Accuracy: 0.628500 | 1.499 sec/iter\n",
      "Epoch: 58 | Batch: 002 / 011 | Total loss: 0.991 | Reg loss: 0.025 | Tree loss: 0.991 | Accuracy: 0.621500 | 1.498 sec/iter\n",
      "Epoch: 58 | Batch: 003 / 011 | Total loss: 0.953 | Reg loss: 0.025 | Tree loss: 0.953 | Accuracy: 0.620500 | 1.498 sec/iter\n",
      "Epoch: 58 | Batch: 004 / 011 | Total loss: 0.959 | Reg loss: 0.025 | Tree loss: 0.959 | Accuracy: 0.637000 | 1.497 sec/iter\n",
      "Epoch: 58 | Batch: 005 / 011 | Total loss: 0.905 | Reg loss: 0.025 | Tree loss: 0.905 | Accuracy: 0.675500 | 1.496 sec/iter\n",
      "Epoch: 58 | Batch: 006 / 011 | Total loss: 0.906 | Reg loss: 0.025 | Tree loss: 0.906 | Accuracy: 0.634500 | 1.496 sec/iter\n",
      "Epoch: 58 | Batch: 007 / 011 | Total loss: 0.926 | Reg loss: 0.025 | Tree loss: 0.926 | Accuracy: 0.596500 | 1.495 sec/iter\n",
      "Epoch: 58 | Batch: 008 / 011 | Total loss: 0.887 | Reg loss: 0.025 | Tree loss: 0.887 | Accuracy: 0.607000 | 1.494 sec/iter\n",
      "Epoch: 58 | Batch: 009 / 011 | Total loss: 0.918 | Reg loss: 0.025 | Tree loss: 0.918 | Accuracy: 0.574500 | 1.494 sec/iter\n",
      "Epoch: 58 | Batch: 010 / 011 | Total loss: 0.881 | Reg loss: 0.025 | Tree loss: 0.881 | Accuracy: 0.634812 | 1.493 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 59 | Batch: 000 / 011 | Total loss: 0.996 | Reg loss: 0.025 | Tree loss: 0.996 | Accuracy: 0.648500 | 1.501 sec/iter\n",
      "Epoch: 59 | Batch: 001 / 011 | Total loss: 0.975 | Reg loss: 0.025 | Tree loss: 0.975 | Accuracy: 0.651500 | 1.501 sec/iter\n",
      "Epoch: 59 | Batch: 002 / 011 | Total loss: 0.980 | Reg loss: 0.025 | Tree loss: 0.980 | Accuracy: 0.635000 | 1.5 sec/iter\n",
      "Epoch: 59 | Batch: 003 / 011 | Total loss: 0.963 | Reg loss: 0.025 | Tree loss: 0.963 | Accuracy: 0.630500 | 1.499 sec/iter\n",
      "Epoch: 59 | Batch: 004 / 011 | Total loss: 0.946 | Reg loss: 0.025 | Tree loss: 0.946 | Accuracy: 0.631500 | 1.499 sec/iter\n",
      "Epoch: 59 | Batch: 005 / 011 | Total loss: 0.914 | Reg loss: 0.025 | Tree loss: 0.914 | Accuracy: 0.659000 | 1.498 sec/iter\n",
      "Epoch: 59 | Batch: 006 / 011 | Total loss: 0.890 | Reg loss: 0.025 | Tree loss: 0.890 | Accuracy: 0.642500 | 1.497 sec/iter\n",
      "Epoch: 59 | Batch: 007 / 011 | Total loss: 0.901 | Reg loss: 0.025 | Tree loss: 0.901 | Accuracy: 0.605500 | 1.497 sec/iter\n",
      "Epoch: 59 | Batch: 008 / 011 | Total loss: 0.907 | Reg loss: 0.025 | Tree loss: 0.907 | Accuracy: 0.597500 | 1.496 sec/iter\n",
      "Epoch: 59 | Batch: 009 / 011 | Total loss: 0.905 | Reg loss: 0.025 | Tree loss: 0.905 | Accuracy: 0.591000 | 1.496 sec/iter\n",
      "Epoch: 59 | Batch: 010 / 011 | Total loss: 0.843 | Reg loss: 0.025 | Tree loss: 0.843 | Accuracy: 0.662116 | 1.495 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 60 | Batch: 000 / 011 | Total loss: 1.031 | Reg loss: 0.025 | Tree loss: 1.031 | Accuracy: 0.617500 | 1.501 sec/iter\n",
      "Epoch: 60 | Batch: 001 / 011 | Total loss: 0.981 | Reg loss: 0.025 | Tree loss: 0.981 | Accuracy: 0.638000 | 1.501 sec/iter\n",
      "Epoch: 60 | Batch: 002 / 011 | Total loss: 0.966 | Reg loss: 0.025 | Tree loss: 0.966 | Accuracy: 0.628500 | 1.501 sec/iter\n",
      "Epoch: 60 | Batch: 003 / 011 | Total loss: 0.966 | Reg loss: 0.025 | Tree loss: 0.966 | Accuracy: 0.628000 | 1.5 sec/iter\n",
      "Epoch: 60 | Batch: 004 / 011 | Total loss: 0.933 | Reg loss: 0.025 | Tree loss: 0.933 | Accuracy: 0.652500 | 1.499 sec/iter\n",
      "Epoch: 60 | Batch: 005 / 011 | Total loss: 0.914 | Reg loss: 0.025 | Tree loss: 0.914 | Accuracy: 0.659500 | 1.499 sec/iter\n",
      "Epoch: 60 | Batch: 006 / 011 | Total loss: 0.887 | Reg loss: 0.025 | Tree loss: 0.887 | Accuracy: 0.641000 | 1.498 sec/iter\n",
      "Epoch: 60 | Batch: 007 / 011 | Total loss: 0.905 | Reg loss: 0.025 | Tree loss: 0.905 | Accuracy: 0.601000 | 1.498 sec/iter\n",
      "Epoch: 60 | Batch: 008 / 011 | Total loss: 0.883 | Reg loss: 0.025 | Tree loss: 0.883 | Accuracy: 0.612500 | 1.498 sec/iter\n",
      "Epoch: 60 | Batch: 009 / 011 | Total loss: 0.900 | Reg loss: 0.025 | Tree loss: 0.900 | Accuracy: 0.583500 | 1.497 sec/iter\n",
      "Epoch: 60 | Batch: 010 / 011 | Total loss: 0.887 | Reg loss: 0.025 | Tree loss: 0.887 | Accuracy: 0.587031 | 1.497 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 61 | Batch: 000 / 011 | Total loss: 0.984 | Reg loss: 0.025 | Tree loss: 0.984 | Accuracy: 0.649000 | 1.502 sec/iter\n",
      "Epoch: 61 | Batch: 001 / 011 | Total loss: 0.983 | Reg loss: 0.025 | Tree loss: 0.983 | Accuracy: 0.633500 | 1.502 sec/iter\n",
      "Epoch: 61 | Batch: 002 / 011 | Total loss: 0.996 | Reg loss: 0.025 | Tree loss: 0.996 | Accuracy: 0.621500 | 1.501 sec/iter\n",
      "Epoch: 61 | Batch: 003 / 011 | Total loss: 0.961 | Reg loss: 0.025 | Tree loss: 0.961 | Accuracy: 0.637000 | 1.5 sec/iter\n",
      "Epoch: 61 | Batch: 004 / 011 | Total loss: 0.932 | Reg loss: 0.025 | Tree loss: 0.932 | Accuracy: 0.641500 | 1.499 sec/iter\n",
      "Epoch: 61 | Batch: 005 / 011 | Total loss: 0.894 | Reg loss: 0.025 | Tree loss: 0.894 | Accuracy: 0.671500 | 1.499 sec/iter\n",
      "Epoch: 61 | Batch: 006 / 011 | Total loss: 0.910 | Reg loss: 0.025 | Tree loss: 0.910 | Accuracy: 0.652000 | 1.498 sec/iter\n",
      "Epoch: 61 | Batch: 007 / 011 | Total loss: 0.909 | Reg loss: 0.025 | Tree loss: 0.909 | Accuracy: 0.611500 | 1.498 sec/iter\n",
      "Epoch: 61 | Batch: 008 / 011 | Total loss: 0.912 | Reg loss: 0.025 | Tree loss: 0.912 | Accuracy: 0.590000 | 1.497 sec/iter\n",
      "Epoch: 61 | Batch: 009 / 011 | Total loss: 0.899 | Reg loss: 0.025 | Tree loss: 0.899 | Accuracy: 0.591000 | 1.496 sec/iter\n",
      "Epoch: 61 | Batch: 010 / 011 | Total loss: 0.883 | Reg loss: 0.025 | Tree loss: 0.883 | Accuracy: 0.580205 | 1.496 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 62 | Batch: 000 / 011 | Total loss: 0.997 | Reg loss: 0.025 | Tree loss: 0.997 | Accuracy: 0.628000 | 1.498 sec/iter\n",
      "Epoch: 62 | Batch: 001 / 011 | Total loss: 0.990 | Reg loss: 0.025 | Tree loss: 0.990 | Accuracy: 0.630500 | 1.497 sec/iter\n",
      "Epoch: 62 | Batch: 002 / 011 | Total loss: 0.965 | Reg loss: 0.025 | Tree loss: 0.965 | Accuracy: 0.648000 | 1.497 sec/iter\n",
      "Epoch: 62 | Batch: 003 / 011 | Total loss: 0.954 | Reg loss: 0.025 | Tree loss: 0.954 | Accuracy: 0.647000 | 1.496 sec/iter\n",
      "Epoch: 62 | Batch: 004 / 011 | Total loss: 0.953 | Reg loss: 0.025 | Tree loss: 0.953 | Accuracy: 0.633500 | 1.496 sec/iter\n",
      "Epoch: 62 | Batch: 005 / 011 | Total loss: 0.894 | Reg loss: 0.025 | Tree loss: 0.894 | Accuracy: 0.674500 | 1.495 sec/iter\n",
      "Epoch: 62 | Batch: 006 / 011 | Total loss: 0.889 | Reg loss: 0.025 | Tree loss: 0.889 | Accuracy: 0.641500 | 1.495 sec/iter\n",
      "Epoch: 62 | Batch: 007 / 011 | Total loss: 0.884 | Reg loss: 0.025 | Tree loss: 0.884 | Accuracy: 0.612500 | 1.494 sec/iter\n",
      "Epoch: 62 | Batch: 008 / 011 | Total loss: 0.893 | Reg loss: 0.025 | Tree loss: 0.893 | Accuracy: 0.594000 | 1.494 sec/iter\n",
      "Epoch: 62 | Batch: 009 / 011 | Total loss: 0.904 | Reg loss: 0.025 | Tree loss: 0.904 | Accuracy: 0.579500 | 1.494 sec/iter\n",
      "Epoch: 62 | Batch: 010 / 011 | Total loss: 0.884 | Reg loss: 0.025 | Tree loss: 0.884 | Accuracy: 0.597270 | 1.493 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 63 | Batch: 000 / 011 | Total loss: 1.003 | Reg loss: 0.025 | Tree loss: 1.003 | Accuracy: 0.629500 | 1.499 sec/iter\n",
      "Epoch: 63 | Batch: 001 / 011 | Total loss: 0.976 | Reg loss: 0.025 | Tree loss: 0.976 | Accuracy: 0.636000 | 1.498 sec/iter\n",
      "Epoch: 63 | Batch: 002 / 011 | Total loss: 0.973 | Reg loss: 0.025 | Tree loss: 0.973 | Accuracy: 0.630000 | 1.498 sec/iter\n",
      "Epoch: 63 | Batch: 003 / 011 | Total loss: 0.961 | Reg loss: 0.025 | Tree loss: 0.961 | Accuracy: 0.622000 | 1.497 sec/iter\n",
      "Epoch: 63 | Batch: 004 / 011 | Total loss: 0.912 | Reg loss: 0.025 | Tree loss: 0.912 | Accuracy: 0.647000 | 1.497 sec/iter\n",
      "Epoch: 63 | Batch: 005 / 011 | Total loss: 0.909 | Reg loss: 0.025 | Tree loss: 0.909 | Accuracy: 0.665500 | 1.496 sec/iter\n",
      "Epoch: 63 | Batch: 006 / 011 | Total loss: 0.886 | Reg loss: 0.025 | Tree loss: 0.886 | Accuracy: 0.662500 | 1.495 sec/iter\n",
      "Epoch: 63 | Batch: 007 / 011 | Total loss: 0.893 | Reg loss: 0.025 | Tree loss: 0.893 | Accuracy: 0.610500 | 1.495 sec/iter\n",
      "Epoch: 63 | Batch: 008 / 011 | Total loss: 0.889 | Reg loss: 0.025 | Tree loss: 0.889 | Accuracy: 0.599500 | 1.494 sec/iter\n",
      "Epoch: 63 | Batch: 009 / 011 | Total loss: 0.900 | Reg loss: 0.025 | Tree loss: 0.900 | Accuracy: 0.590000 | 1.494 sec/iter\n",
      "Epoch: 63 | Batch: 010 / 011 | Total loss: 0.864 | Reg loss: 0.025 | Tree loss: 0.864 | Accuracy: 0.614334 | 1.493 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 64 | Batch: 000 / 011 | Total loss: 0.985 | Reg loss: 0.025 | Tree loss: 0.985 | Accuracy: 0.643500 | 1.5 sec/iter\n",
      "Epoch: 64 | Batch: 001 / 011 | Total loss: 0.992 | Reg loss: 0.025 | Tree loss: 0.992 | Accuracy: 0.622500 | 1.499 sec/iter\n",
      "Epoch: 64 | Batch: 002 / 011 | Total loss: 0.968 | Reg loss: 0.025 | Tree loss: 0.968 | Accuracy: 0.635000 | 1.499 sec/iter\n",
      "Epoch: 64 | Batch: 003 / 011 | Total loss: 0.942 | Reg loss: 0.025 | Tree loss: 0.942 | Accuracy: 0.631000 | 1.498 sec/iter\n",
      "Epoch: 64 | Batch: 004 / 011 | Total loss: 0.930 | Reg loss: 0.025 | Tree loss: 0.930 | Accuracy: 0.642500 | 1.497 sec/iter\n",
      "Epoch: 64 | Batch: 005 / 011 | Total loss: 0.878 | Reg loss: 0.025 | Tree loss: 0.878 | Accuracy: 0.691000 | 1.497 sec/iter\n",
      "Epoch: 64 | Batch: 006 / 011 | Total loss: 0.892 | Reg loss: 0.025 | Tree loss: 0.892 | Accuracy: 0.656000 | 1.496 sec/iter\n",
      "Epoch: 64 | Batch: 007 / 011 | Total loss: 0.889 | Reg loss: 0.025 | Tree loss: 0.889 | Accuracy: 0.615500 | 1.495 sec/iter\n",
      "Epoch: 64 | Batch: 008 / 011 | Total loss: 0.910 | Reg loss: 0.025 | Tree loss: 0.910 | Accuracy: 0.582500 | 1.495 sec/iter\n",
      "Epoch: 64 | Batch: 009 / 011 | Total loss: 0.922 | Reg loss: 0.025 | Tree loss: 0.922 | Accuracy: 0.561500 | 1.495 sec/iter\n",
      "Epoch: 64 | Batch: 010 / 011 | Total loss: 0.869 | Reg loss: 0.025 | Tree loss: 0.869 | Accuracy: 0.593857 | 1.494 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 65 | Batch: 000 / 011 | Total loss: 0.990 | Reg loss: 0.025 | Tree loss: 0.990 | Accuracy: 0.641000 | 1.5 sec/iter\n",
      "Epoch: 65 | Batch: 001 / 011 | Total loss: 0.978 | Reg loss: 0.025 | Tree loss: 0.978 | Accuracy: 0.643000 | 1.5 sec/iter\n",
      "Epoch: 65 | Batch: 002 / 011 | Total loss: 0.966 | Reg loss: 0.025 | Tree loss: 0.966 | Accuracy: 0.651500 | 1.5 sec/iter\n",
      "Epoch: 65 | Batch: 003 / 011 | Total loss: 0.927 | Reg loss: 0.025 | Tree loss: 0.927 | Accuracy: 0.671500 | 1.499 sec/iter\n",
      "Epoch: 65 | Batch: 004 / 011 | Total loss: 0.914 | Reg loss: 0.025 | Tree loss: 0.914 | Accuracy: 0.648000 | 1.499 sec/iter\n",
      "Epoch: 65 | Batch: 005 / 011 | Total loss: 0.907 | Reg loss: 0.025 | Tree loss: 0.907 | Accuracy: 0.656500 | 1.498 sec/iter\n",
      "Epoch: 65 | Batch: 006 / 011 | Total loss: 0.919 | Reg loss: 0.025 | Tree loss: 0.919 | Accuracy: 0.625000 | 1.497 sec/iter\n",
      "Epoch: 65 | Batch: 007 / 011 | Total loss: 0.887 | Reg loss: 0.025 | Tree loss: 0.887 | Accuracy: 0.600500 | 1.497 sec/iter\n",
      "Epoch: 65 | Batch: 008 / 011 | Total loss: 0.905 | Reg loss: 0.025 | Tree loss: 0.905 | Accuracy: 0.575500 | 1.497 sec/iter\n",
      "Epoch: 65 | Batch: 009 / 011 | Total loss: 0.875 | Reg loss: 0.025 | Tree loss: 0.875 | Accuracy: 0.612500 | 1.496 sec/iter\n",
      "Epoch: 65 | Batch: 010 / 011 | Total loss: 0.885 | Reg loss: 0.025 | Tree loss: 0.885 | Accuracy: 0.590444 | 1.496 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 66 | Batch: 000 / 011 | Total loss: 0.969 | Reg loss: 0.025 | Tree loss: 0.969 | Accuracy: 0.650000 | 1.501 sec/iter\n",
      "Epoch: 66 | Batch: 001 / 011 | Total loss: 0.987 | Reg loss: 0.025 | Tree loss: 0.987 | Accuracy: 0.641500 | 1.501 sec/iter\n",
      "Epoch: 66 | Batch: 002 / 011 | Total loss: 0.943 | Reg loss: 0.025 | Tree loss: 0.943 | Accuracy: 0.654500 | 1.5 sec/iter\n",
      "Epoch: 66 | Batch: 003 / 011 | Total loss: 0.954 | Reg loss: 0.025 | Tree loss: 0.954 | Accuracy: 0.642000 | 1.5 sec/iter\n",
      "Epoch: 66 | Batch: 004 / 011 | Total loss: 0.929 | Reg loss: 0.025 | Tree loss: 0.929 | Accuracy: 0.638500 | 1.499 sec/iter\n",
      "Epoch: 66 | Batch: 005 / 011 | Total loss: 0.929 | Reg loss: 0.025 | Tree loss: 0.929 | Accuracy: 0.651500 | 1.498 sec/iter\n",
      "Epoch: 66 | Batch: 006 / 011 | Total loss: 0.900 | Reg loss: 0.025 | Tree loss: 0.900 | Accuracy: 0.662500 | 1.498 sec/iter\n",
      "Epoch: 66 | Batch: 007 / 011 | Total loss: 0.889 | Reg loss: 0.025 | Tree loss: 0.889 | Accuracy: 0.639500 | 1.497 sec/iter\n",
      "Epoch: 66 | Batch: 008 / 011 | Total loss: 0.917 | Reg loss: 0.025 | Tree loss: 0.917 | Accuracy: 0.589500 | 1.497 sec/iter\n",
      "Epoch: 66 | Batch: 009 / 011 | Total loss: 0.879 | Reg loss: 0.025 | Tree loss: 0.879 | Accuracy: 0.597000 | 1.496 sec/iter\n",
      "Epoch: 66 | Batch: 010 / 011 | Total loss: 0.892 | Reg loss: 0.025 | Tree loss: 0.892 | Accuracy: 0.638225 | 1.496 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 67 | Batch: 000 / 011 | Total loss: 0.999 | Reg loss: 0.025 | Tree loss: 0.999 | Accuracy: 0.633000 | 1.502 sec/iter\n",
      "Epoch: 67 | Batch: 001 / 011 | Total loss: 0.980 | Reg loss: 0.025 | Tree loss: 0.980 | Accuracy: 0.625500 | 1.501 sec/iter\n",
      "Epoch: 67 | Batch: 002 / 011 | Total loss: 0.971 | Reg loss: 0.025 | Tree loss: 0.971 | Accuracy: 0.642000 | 1.5 sec/iter\n",
      "Epoch: 67 | Batch: 003 / 011 | Total loss: 0.919 | Reg loss: 0.025 | Tree loss: 0.919 | Accuracy: 0.648500 | 1.5 sec/iter\n",
      "Epoch: 67 | Batch: 004 / 011 | Total loss: 0.906 | Reg loss: 0.025 | Tree loss: 0.906 | Accuracy: 0.671000 | 1.499 sec/iter\n",
      "Epoch: 67 | Batch: 005 / 011 | Total loss: 0.880 | Reg loss: 0.025 | Tree loss: 0.880 | Accuracy: 0.668000 | 1.498 sec/iter\n",
      "Epoch: 67 | Batch: 006 / 011 | Total loss: 0.879 | Reg loss: 0.025 | Tree loss: 0.879 | Accuracy: 0.628500 | 1.498 sec/iter\n",
      "Epoch: 67 | Batch: 007 / 011 | Total loss: 0.893 | Reg loss: 0.025 | Tree loss: 0.893 | Accuracy: 0.589000 | 1.497 sec/iter\n",
      "Epoch: 67 | Batch: 008 / 011 | Total loss: 0.913 | Reg loss: 0.025 | Tree loss: 0.913 | Accuracy: 0.584000 | 1.497 sec/iter\n",
      "Epoch: 67 | Batch: 009 / 011 | Total loss: 0.879 | Reg loss: 0.025 | Tree loss: 0.879 | Accuracy: 0.602500 | 1.496 sec/iter\n",
      "Epoch: 67 | Batch: 010 / 011 | Total loss: 0.880 | Reg loss: 0.025 | Tree loss: 0.880 | Accuracy: 0.624573 | 1.495 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 68 | Batch: 000 / 011 | Total loss: 0.971 | Reg loss: 0.025 | Tree loss: 0.971 | Accuracy: 0.637000 | 1.501 sec/iter\n",
      "Epoch: 68 | Batch: 001 / 011 | Total loss: 0.969 | Reg loss: 0.025 | Tree loss: 0.969 | Accuracy: 0.635000 | 1.5 sec/iter\n",
      "Epoch: 68 | Batch: 002 / 011 | Total loss: 0.975 | Reg loss: 0.025 | Tree loss: 0.975 | Accuracy: 0.620000 | 1.5 sec/iter\n",
      "Epoch: 68 | Batch: 003 / 011 | Total loss: 0.940 | Reg loss: 0.025 | Tree loss: 0.940 | Accuracy: 0.658500 | 1.499 sec/iter\n",
      "Epoch: 68 | Batch: 004 / 011 | Total loss: 0.927 | Reg loss: 0.025 | Tree loss: 0.927 | Accuracy: 0.646000 | 1.499 sec/iter\n",
      "Epoch: 68 | Batch: 005 / 011 | Total loss: 0.926 | Reg loss: 0.025 | Tree loss: 0.926 | Accuracy: 0.650500 | 1.498 sec/iter\n",
      "Epoch: 68 | Batch: 006 / 011 | Total loss: 0.904 | Reg loss: 0.025 | Tree loss: 0.904 | Accuracy: 0.648000 | 1.498 sec/iter\n",
      "Epoch: 68 | Batch: 007 / 011 | Total loss: 0.892 | Reg loss: 0.025 | Tree loss: 0.892 | Accuracy: 0.618500 | 1.498 sec/iter\n",
      "Epoch: 68 | Batch: 008 / 011 | Total loss: 0.879 | Reg loss: 0.025 | Tree loss: 0.879 | Accuracy: 0.603000 | 1.498 sec/iter\n",
      "Epoch: 68 | Batch: 009 / 011 | Total loss: 0.875 | Reg loss: 0.025 | Tree loss: 0.875 | Accuracy: 0.593500 | 1.497 sec/iter\n",
      "Epoch: 68 | Batch: 010 / 011 | Total loss: 0.871 | Reg loss: 0.025 | Tree loss: 0.871 | Accuracy: 0.607509 | 1.497 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 69 | Batch: 000 / 011 | Total loss: 0.964 | Reg loss: 0.025 | Tree loss: 0.964 | Accuracy: 0.657000 | 1.504 sec/iter\n",
      "Epoch: 69 | Batch: 001 / 011 | Total loss: 0.981 | Reg loss: 0.025 | Tree loss: 0.981 | Accuracy: 0.640000 | 1.503 sec/iter\n",
      "Epoch: 69 | Batch: 002 / 011 | Total loss: 0.973 | Reg loss: 0.025 | Tree loss: 0.973 | Accuracy: 0.646000 | 1.502 sec/iter\n",
      "Epoch: 69 | Batch: 003 / 011 | Total loss: 0.942 | Reg loss: 0.025 | Tree loss: 0.942 | Accuracy: 0.661000 | 1.502 sec/iter\n",
      "Epoch: 69 | Batch: 004 / 011 | Total loss: 0.930 | Reg loss: 0.025 | Tree loss: 0.930 | Accuracy: 0.654000 | 1.502 sec/iter\n",
      "Epoch: 69 | Batch: 005 / 011 | Total loss: 0.904 | Reg loss: 0.025 | Tree loss: 0.904 | Accuracy: 0.643000 | 1.502 sec/iter\n",
      "Epoch: 69 | Batch: 006 / 011 | Total loss: 0.874 | Reg loss: 0.025 | Tree loss: 0.874 | Accuracy: 0.637000 | 1.501 sec/iter\n",
      "Epoch: 69 | Batch: 007 / 011 | Total loss: 0.867 | Reg loss: 0.025 | Tree loss: 0.867 | Accuracy: 0.623500 | 1.501 sec/iter\n",
      "Epoch: 69 | Batch: 008 / 011 | Total loss: 0.897 | Reg loss: 0.025 | Tree loss: 0.897 | Accuracy: 0.586500 | 1.501 sec/iter\n",
      "Epoch: 69 | Batch: 009 / 011 | Total loss: 0.899 | Reg loss: 0.025 | Tree loss: 0.899 | Accuracy: 0.571500 | 1.5 sec/iter\n",
      "Epoch: 69 | Batch: 010 / 011 | Total loss: 0.875 | Reg loss: 0.025 | Tree loss: 0.875 | Accuracy: 0.617747 | 1.5 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 70 | Batch: 000 / 011 | Total loss: 0.999 | Reg loss: 0.025 | Tree loss: 0.999 | Accuracy: 0.631500 | 1.505 sec/iter\n",
      "Epoch: 70 | Batch: 001 / 011 | Total loss: 0.980 | Reg loss: 0.025 | Tree loss: 0.980 | Accuracy: 0.637000 | 1.505 sec/iter\n",
      "Epoch: 70 | Batch: 002 / 011 | Total loss: 0.975 | Reg loss: 0.025 | Tree loss: 0.975 | Accuracy: 0.614000 | 1.505 sec/iter\n",
      "Epoch: 70 | Batch: 003 / 011 | Total loss: 0.925 | Reg loss: 0.025 | Tree loss: 0.925 | Accuracy: 0.645000 | 1.505 sec/iter\n",
      "Epoch: 70 | Batch: 004 / 011 | Total loss: 0.902 | Reg loss: 0.025 | Tree loss: 0.902 | Accuracy: 0.669500 | 1.505 sec/iter\n",
      "Epoch: 70 | Batch: 005 / 011 | Total loss: 0.905 | Reg loss: 0.025 | Tree loss: 0.905 | Accuracy: 0.679000 | 1.505 sec/iter\n",
      "Epoch: 70 | Batch: 006 / 011 | Total loss: 0.869 | Reg loss: 0.025 | Tree loss: 0.869 | Accuracy: 0.674000 | 1.504 sec/iter\n",
      "Epoch: 70 | Batch: 007 / 011 | Total loss: 0.884 | Reg loss: 0.025 | Tree loss: 0.884 | Accuracy: 0.632000 | 1.504 sec/iter\n",
      "Epoch: 70 | Batch: 008 / 011 | Total loss: 0.897 | Reg loss: 0.025 | Tree loss: 0.897 | Accuracy: 0.588000 | 1.503 sec/iter\n",
      "Epoch: 70 | Batch: 009 / 011 | Total loss: 0.882 | Reg loss: 0.025 | Tree loss: 0.882 | Accuracy: 0.587500 | 1.503 sec/iter\n",
      "Epoch: 70 | Batch: 010 / 011 | Total loss: 0.870 | Reg loss: 0.025 | Tree loss: 0.870 | Accuracy: 0.597270 | 1.502 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 71 | Batch: 000 / 011 | Total loss: 0.979 | Reg loss: 0.025 | Tree loss: 0.979 | Accuracy: 0.638500 | 1.506 sec/iter\n",
      "Epoch: 71 | Batch: 001 / 011 | Total loss: 0.974 | Reg loss: 0.025 | Tree loss: 0.974 | Accuracy: 0.650000 | 1.506 sec/iter\n",
      "Epoch: 71 | Batch: 002 / 011 | Total loss: 0.961 | Reg loss: 0.025 | Tree loss: 0.961 | Accuracy: 0.639500 | 1.506 sec/iter\n",
      "Epoch: 71 | Batch: 003 / 011 | Total loss: 0.925 | Reg loss: 0.025 | Tree loss: 0.925 | Accuracy: 0.661000 | 1.505 sec/iter\n",
      "Epoch: 71 | Batch: 004 / 011 | Total loss: 0.929 | Reg loss: 0.025 | Tree loss: 0.929 | Accuracy: 0.637000 | 1.505 sec/iter\n",
      "Epoch: 71 | Batch: 005 / 011 | Total loss: 0.883 | Reg loss: 0.025 | Tree loss: 0.883 | Accuracy: 0.689000 | 1.505 sec/iter\n",
      "Epoch: 71 | Batch: 006 / 011 | Total loss: 0.885 | Reg loss: 0.025 | Tree loss: 0.885 | Accuracy: 0.633000 | 1.505 sec/iter\n",
      "Epoch: 71 | Batch: 007 / 011 | Total loss: 0.882 | Reg loss: 0.025 | Tree loss: 0.882 | Accuracy: 0.599000 | 1.504 sec/iter\n",
      "Epoch: 71 | Batch: 008 / 011 | Total loss: 0.879 | Reg loss: 0.025 | Tree loss: 0.879 | Accuracy: 0.591500 | 1.504 sec/iter\n",
      "Epoch: 71 | Batch: 009 / 011 | Total loss: 0.880 | Reg loss: 0.025 | Tree loss: 0.880 | Accuracy: 0.596500 | 1.504 sec/iter\n",
      "Epoch: 71 | Batch: 010 / 011 | Total loss: 0.859 | Reg loss: 0.025 | Tree loss: 0.859 | Accuracy: 0.634812 | 1.503 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 72 | Batch: 000 / 011 | Total loss: 0.999 | Reg loss: 0.025 | Tree loss: 0.999 | Accuracy: 0.620000 | 1.506 sec/iter\n",
      "Epoch: 72 | Batch: 001 / 011 | Total loss: 0.968 | Reg loss: 0.025 | Tree loss: 0.968 | Accuracy: 0.634000 | 1.506 sec/iter\n",
      "Epoch: 72 | Batch: 002 / 011 | Total loss: 0.945 | Reg loss: 0.025 | Tree loss: 0.945 | Accuracy: 0.658000 | 1.505 sec/iter\n",
      "Epoch: 72 | Batch: 003 / 011 | Total loss: 0.927 | Reg loss: 0.025 | Tree loss: 0.927 | Accuracy: 0.654500 | 1.505 sec/iter\n",
      "Epoch: 72 | Batch: 004 / 011 | Total loss: 0.907 | Reg loss: 0.025 | Tree loss: 0.907 | Accuracy: 0.645500 | 1.505 sec/iter\n",
      "Epoch: 72 | Batch: 005 / 011 | Total loss: 0.896 | Reg loss: 0.025 | Tree loss: 0.896 | Accuracy: 0.666000 | 1.505 sec/iter\n",
      "Epoch: 72 | Batch: 006 / 011 | Total loss: 0.873 | Reg loss: 0.025 | Tree loss: 0.873 | Accuracy: 0.647000 | 1.505 sec/iter\n",
      "Epoch: 72 | Batch: 007 / 011 | Total loss: 0.885 | Reg loss: 0.025 | Tree loss: 0.885 | Accuracy: 0.627000 | 1.504 sec/iter\n",
      "Epoch: 72 | Batch: 008 / 011 | Total loss: 0.891 | Reg loss: 0.025 | Tree loss: 0.891 | Accuracy: 0.584500 | 1.504 sec/iter\n",
      "Epoch: 72 | Batch: 009 / 011 | Total loss: 0.910 | Reg loss: 0.025 | Tree loss: 0.910 | Accuracy: 0.576000 | 1.504 sec/iter\n",
      "Epoch: 72 | Batch: 010 / 011 | Total loss: 0.869 | Reg loss: 0.025 | Tree loss: 0.869 | Accuracy: 0.597270 | 1.503 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 73 | Batch: 000 / 011 | Total loss: 0.970 | Reg loss: 0.025 | Tree loss: 0.970 | Accuracy: 0.653500 | 1.504 sec/iter\n",
      "Epoch: 73 | Batch: 001 / 011 | Total loss: 0.970 | Reg loss: 0.025 | Tree loss: 0.970 | Accuracy: 0.646500 | 1.504 sec/iter\n",
      "Epoch: 73 | Batch: 002 / 011 | Total loss: 0.955 | Reg loss: 0.025 | Tree loss: 0.955 | Accuracy: 0.640500 | 1.504 sec/iter\n",
      "Epoch: 73 | Batch: 003 / 011 | Total loss: 0.933 | Reg loss: 0.025 | Tree loss: 0.933 | Accuracy: 0.646000 | 1.503 sec/iter\n",
      "Epoch: 73 | Batch: 004 / 011 | Total loss: 0.877 | Reg loss: 0.025 | Tree loss: 0.877 | Accuracy: 0.673000 | 1.502 sec/iter\n",
      "Epoch: 73 | Batch: 005 / 011 | Total loss: 0.866 | Reg loss: 0.025 | Tree loss: 0.866 | Accuracy: 0.682500 | 1.502 sec/iter\n",
      "Epoch: 73 | Batch: 006 / 011 | Total loss: 0.893 | Reg loss: 0.025 | Tree loss: 0.893 | Accuracy: 0.639000 | 1.501 sec/iter\n",
      "Epoch: 73 | Batch: 007 / 011 | Total loss: 0.878 | Reg loss: 0.025 | Tree loss: 0.878 | Accuracy: 0.632000 | 1.501 sec/iter\n",
      "Epoch: 73 | Batch: 008 / 011 | Total loss: 0.903 | Reg loss: 0.025 | Tree loss: 0.903 | Accuracy: 0.585500 | 1.5 sec/iter\n",
      "Epoch: 73 | Batch: 009 / 011 | Total loss: 0.889 | Reg loss: 0.025 | Tree loss: 0.889 | Accuracy: 0.602000 | 1.5 sec/iter\n",
      "Epoch: 73 | Batch: 010 / 011 | Total loss: 0.865 | Reg loss: 0.025 | Tree loss: 0.865 | Accuracy: 0.614334 | 1.499 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 74 | Batch: 000 / 011 | Total loss: 0.956 | Reg loss: 0.025 | Tree loss: 0.956 | Accuracy: 0.654000 | 1.504 sec/iter\n",
      "Epoch: 74 | Batch: 001 / 011 | Total loss: 0.931 | Reg loss: 0.025 | Tree loss: 0.931 | Accuracy: 0.667000 | 1.503 sec/iter\n",
      "Epoch: 74 | Batch: 002 / 011 | Total loss: 0.940 | Reg loss: 0.025 | Tree loss: 0.940 | Accuracy: 0.665500 | 1.503 sec/iter\n",
      "Epoch: 74 | Batch: 003 / 011 | Total loss: 0.929 | Reg loss: 0.025 | Tree loss: 0.929 | Accuracy: 0.667000 | 1.503 sec/iter\n",
      "Epoch: 74 | Batch: 004 / 011 | Total loss: 0.911 | Reg loss: 0.025 | Tree loss: 0.911 | Accuracy: 0.650500 | 1.503 sec/iter\n",
      "Epoch: 74 | Batch: 005 / 011 | Total loss: 0.900 | Reg loss: 0.025 | Tree loss: 0.900 | Accuracy: 0.671500 | 1.502 sec/iter\n",
      "Epoch: 74 | Batch: 006 / 011 | Total loss: 0.893 | Reg loss: 0.025 | Tree loss: 0.893 | Accuracy: 0.624500 | 1.502 sec/iter\n",
      "Epoch: 74 | Batch: 007 / 011 | Total loss: 0.909 | Reg loss: 0.025 | Tree loss: 0.909 | Accuracy: 0.602500 | 1.502 sec/iter\n",
      "Epoch: 74 | Batch: 008 / 011 | Total loss: 0.889 | Reg loss: 0.025 | Tree loss: 0.889 | Accuracy: 0.580500 | 1.501 sec/iter\n",
      "Epoch: 74 | Batch: 009 / 011 | Total loss: 0.875 | Reg loss: 0.025 | Tree loss: 0.875 | Accuracy: 0.600000 | 1.501 sec/iter\n",
      "Epoch: 74 | Batch: 010 / 011 | Total loss: 0.818 | Reg loss: 0.025 | Tree loss: 0.818 | Accuracy: 0.658703 | 1.5 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 75 | Batch: 000 / 011 | Total loss: 1.004 | Reg loss: 0.025 | Tree loss: 1.004 | Accuracy: 0.620000 | 1.503 sec/iter\n",
      "Epoch: 75 | Batch: 001 / 011 | Total loss: 0.950 | Reg loss: 0.025 | Tree loss: 0.950 | Accuracy: 0.661000 | 1.503 sec/iter\n",
      "Epoch: 75 | Batch: 002 / 011 | Total loss: 0.927 | Reg loss: 0.025 | Tree loss: 0.927 | Accuracy: 0.669500 | 1.503 sec/iter\n",
      "Epoch: 75 | Batch: 003 / 011 | Total loss: 0.929 | Reg loss: 0.025 | Tree loss: 0.929 | Accuracy: 0.653000 | 1.502 sec/iter\n",
      "Epoch: 75 | Batch: 004 / 011 | Total loss: 0.905 | Reg loss: 0.025 | Tree loss: 0.905 | Accuracy: 0.659000 | 1.502 sec/iter\n",
      "Epoch: 75 | Batch: 005 / 011 | Total loss: 0.915 | Reg loss: 0.025 | Tree loss: 0.915 | Accuracy: 0.659500 | 1.502 sec/iter\n",
      "Epoch: 75 | Batch: 006 / 011 | Total loss: 0.869 | Reg loss: 0.025 | Tree loss: 0.869 | Accuracy: 0.643500 | 1.501 sec/iter\n",
      "Epoch: 75 | Batch: 007 / 011 | Total loss: 0.867 | Reg loss: 0.025 | Tree loss: 0.867 | Accuracy: 0.623000 | 1.501 sec/iter\n",
      "Epoch: 75 | Batch: 008 / 011 | Total loss: 0.866 | Reg loss: 0.025 | Tree loss: 0.866 | Accuracy: 0.607000 | 1.5 sec/iter\n",
      "Epoch: 75 | Batch: 009 / 011 | Total loss: 0.889 | Reg loss: 0.025 | Tree loss: 0.889 | Accuracy: 0.580500 | 1.5 sec/iter\n",
      "Epoch: 75 | Batch: 010 / 011 | Total loss: 0.927 | Reg loss: 0.025 | Tree loss: 0.927 | Accuracy: 0.569966 | 1.499 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 76 | Batch: 000 / 011 | Total loss: 0.992 | Reg loss: 0.025 | Tree loss: 0.992 | Accuracy: 0.614000 | 1.504 sec/iter\n",
      "Epoch: 76 | Batch: 001 / 011 | Total loss: 0.960 | Reg loss: 0.025 | Tree loss: 0.960 | Accuracy: 0.645500 | 1.503 sec/iter\n",
      "Epoch: 76 | Batch: 002 / 011 | Total loss: 0.922 | Reg loss: 0.025 | Tree loss: 0.922 | Accuracy: 0.672000 | 1.503 sec/iter\n",
      "Epoch: 76 | Batch: 003 / 011 | Total loss: 0.927 | Reg loss: 0.025 | Tree loss: 0.927 | Accuracy: 0.650500 | 1.503 sec/iter\n",
      "Epoch: 76 | Batch: 004 / 011 | Total loss: 0.910 | Reg loss: 0.025 | Tree loss: 0.910 | Accuracy: 0.663500 | 1.503 sec/iter\n",
      "Epoch: 76 | Batch: 005 / 011 | Total loss: 0.893 | Reg loss: 0.025 | Tree loss: 0.893 | Accuracy: 0.647500 | 1.502 sec/iter\n",
      "Epoch: 76 | Batch: 006 / 011 | Total loss: 0.871 | Reg loss: 0.025 | Tree loss: 0.871 | Accuracy: 0.654500 | 1.502 sec/iter\n",
      "Epoch: 76 | Batch: 007 / 011 | Total loss: 0.891 | Reg loss: 0.025 | Tree loss: 0.891 | Accuracy: 0.605000 | 1.501 sec/iter\n",
      "Epoch: 76 | Batch: 008 / 011 | Total loss: 0.874 | Reg loss: 0.025 | Tree loss: 0.874 | Accuracy: 0.604500 | 1.501 sec/iter\n",
      "Epoch: 76 | Batch: 009 / 011 | Total loss: 0.884 | Reg loss: 0.025 | Tree loss: 0.884 | Accuracy: 0.599500 | 1.501 sec/iter\n",
      "Epoch: 76 | Batch: 010 / 011 | Total loss: 0.848 | Reg loss: 0.025 | Tree loss: 0.848 | Accuracy: 0.634812 | 1.5 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 77 | Batch: 000 / 011 | Total loss: 0.964 | Reg loss: 0.025 | Tree loss: 0.964 | Accuracy: 0.651000 | 1.504 sec/iter\n",
      "Epoch: 77 | Batch: 001 / 011 | Total loss: 0.969 | Reg loss: 0.025 | Tree loss: 0.969 | Accuracy: 0.650500 | 1.503 sec/iter\n",
      "Epoch: 77 | Batch: 002 / 011 | Total loss: 0.931 | Reg loss: 0.025 | Tree loss: 0.931 | Accuracy: 0.666000 | 1.503 sec/iter\n",
      "Epoch: 77 | Batch: 003 / 011 | Total loss: 0.933 | Reg loss: 0.025 | Tree loss: 0.933 | Accuracy: 0.649500 | 1.503 sec/iter\n",
      "Epoch: 77 | Batch: 004 / 011 | Total loss: 0.898 | Reg loss: 0.025 | Tree loss: 0.898 | Accuracy: 0.661500 | 1.503 sec/iter\n",
      "Epoch: 77 | Batch: 005 / 011 | Total loss: 0.896 | Reg loss: 0.025 | Tree loss: 0.896 | Accuracy: 0.657000 | 1.503 sec/iter\n",
      "Epoch: 77 | Batch: 006 / 011 | Total loss: 0.886 | Reg loss: 0.025 | Tree loss: 0.886 | Accuracy: 0.639000 | 1.502 sec/iter\n",
      "Epoch: 77 | Batch: 007 / 011 | Total loss: 0.889 | Reg loss: 0.025 | Tree loss: 0.889 | Accuracy: 0.594500 | 1.502 sec/iter\n",
      "Epoch: 77 | Batch: 008 / 011 | Total loss: 0.874 | Reg loss: 0.025 | Tree loss: 0.874 | Accuracy: 0.608500 | 1.502 sec/iter\n",
      "Epoch: 77 | Batch: 009 / 011 | Total loss: 0.859 | Reg loss: 0.025 | Tree loss: 0.859 | Accuracy: 0.615500 | 1.501 sec/iter\n",
      "Epoch: 77 | Batch: 010 / 011 | Total loss: 0.823 | Reg loss: 0.025 | Tree loss: 0.823 | Accuracy: 0.627986 | 1.501 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 78 | Batch: 000 / 011 | Total loss: 0.974 | Reg loss: 0.025 | Tree loss: 0.974 | Accuracy: 0.644000 | 1.504 sec/iter\n",
      "Epoch: 78 | Batch: 001 / 011 | Total loss: 0.955 | Reg loss: 0.025 | Tree loss: 0.955 | Accuracy: 0.662000 | 1.504 sec/iter\n",
      "Epoch: 78 | Batch: 002 / 011 | Total loss: 0.920 | Reg loss: 0.025 | Tree loss: 0.920 | Accuracy: 0.674500 | 1.504 sec/iter\n",
      "Epoch: 78 | Batch: 003 / 011 | Total loss: 0.929 | Reg loss: 0.025 | Tree loss: 0.929 | Accuracy: 0.653000 | 1.503 sec/iter\n",
      "Epoch: 78 | Batch: 004 / 011 | Total loss: 0.909 | Reg loss: 0.025 | Tree loss: 0.909 | Accuracy: 0.636500 | 1.503 sec/iter\n",
      "Epoch: 78 | Batch: 005 / 011 | Total loss: 0.885 | Reg loss: 0.025 | Tree loss: 0.885 | Accuracy: 0.667500 | 1.503 sec/iter\n",
      "Epoch: 78 | Batch: 006 / 011 | Total loss: 0.876 | Reg loss: 0.025 | Tree loss: 0.876 | Accuracy: 0.669000 | 1.502 sec/iter\n",
      "Epoch: 78 | Batch: 007 / 011 | Total loss: 0.860 | Reg loss: 0.025 | Tree loss: 0.860 | Accuracy: 0.635000 | 1.502 sec/iter\n",
      "Epoch: 78 | Batch: 008 / 011 | Total loss: 0.881 | Reg loss: 0.025 | Tree loss: 0.881 | Accuracy: 0.586500 | 1.501 sec/iter\n",
      "Epoch: 78 | Batch: 009 / 011 | Total loss: 0.886 | Reg loss: 0.026 | Tree loss: 0.886 | Accuracy: 0.581500 | 1.501 sec/iter\n",
      "Epoch: 78 | Batch: 010 / 011 | Total loss: 0.925 | Reg loss: 0.026 | Tree loss: 0.925 | Accuracy: 0.549488 | 1.5 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 79 | Batch: 000 / 011 | Total loss: 0.980 | Reg loss: 0.025 | Tree loss: 0.980 | Accuracy: 0.652000 | 1.501 sec/iter\n",
      "Epoch: 79 | Batch: 001 / 011 | Total loss: 0.958 | Reg loss: 0.025 | Tree loss: 0.958 | Accuracy: 0.644500 | 1.501 sec/iter\n",
      "Epoch: 79 | Batch: 002 / 011 | Total loss: 0.923 | Reg loss: 0.025 | Tree loss: 0.923 | Accuracy: 0.663000 | 1.5 sec/iter\n",
      "Epoch: 79 | Batch: 003 / 011 | Total loss: 0.934 | Reg loss: 0.025 | Tree loss: 0.934 | Accuracy: 0.654000 | 1.5 sec/iter\n",
      "Epoch: 79 | Batch: 004 / 011 | Total loss: 0.906 | Reg loss: 0.025 | Tree loss: 0.906 | Accuracy: 0.664000 | 1.499 sec/iter\n",
      "Epoch: 79 | Batch: 005 / 011 | Total loss: 0.873 | Reg loss: 0.025 | Tree loss: 0.873 | Accuracy: 0.665500 | 1.498 sec/iter\n",
      "Epoch: 79 | Batch: 006 / 011 | Total loss: 0.891 | Reg loss: 0.025 | Tree loss: 0.891 | Accuracy: 0.608500 | 1.498 sec/iter\n",
      "Epoch: 79 | Batch: 007 / 011 | Total loss: 0.882 | Reg loss: 0.025 | Tree loss: 0.882 | Accuracy: 0.606500 | 1.498 sec/iter\n",
      "Epoch: 79 | Batch: 008 / 011 | Total loss: 0.875 | Reg loss: 0.026 | Tree loss: 0.875 | Accuracy: 0.612000 | 1.497 sec/iter\n",
      "Epoch: 79 | Batch: 009 / 011 | Total loss: 0.880 | Reg loss: 0.026 | Tree loss: 0.880 | Accuracy: 0.612500 | 1.497 sec/iter\n",
      "Epoch: 79 | Batch: 010 / 011 | Total loss: 0.834 | Reg loss: 0.026 | Tree loss: 0.834 | Accuracy: 0.631399 | 1.496 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 80 | Batch: 000 / 011 | Total loss: 0.952 | Reg loss: 0.025 | Tree loss: 0.952 | Accuracy: 0.647000 | 1.503 sec/iter\n",
      "Epoch: 80 | Batch: 001 / 011 | Total loss: 0.955 | Reg loss: 0.025 | Tree loss: 0.955 | Accuracy: 0.658500 | 1.503 sec/iter\n",
      "Epoch: 80 | Batch: 002 / 011 | Total loss: 0.935 | Reg loss: 0.025 | Tree loss: 0.935 | Accuracy: 0.650500 | 1.503 sec/iter\n",
      "Epoch: 80 | Batch: 003 / 011 | Total loss: 0.919 | Reg loss: 0.025 | Tree loss: 0.919 | Accuracy: 0.668500 | 1.502 sec/iter\n",
      "Epoch: 80 | Batch: 004 / 011 | Total loss: 0.910 | Reg loss: 0.025 | Tree loss: 0.910 | Accuracy: 0.660500 | 1.502 sec/iter\n",
      "Epoch: 80 | Batch: 005 / 011 | Total loss: 0.887 | Reg loss: 0.026 | Tree loss: 0.887 | Accuracy: 0.674500 | 1.501 sec/iter\n",
      "Epoch: 80 | Batch: 006 / 011 | Total loss: 0.891 | Reg loss: 0.026 | Tree loss: 0.891 | Accuracy: 0.648500 | 1.501 sec/iter\n",
      "Epoch: 80 | Batch: 007 / 011 | Total loss: 0.887 | Reg loss: 0.026 | Tree loss: 0.887 | Accuracy: 0.637500 | 1.5 sec/iter\n",
      "Epoch: 80 | Batch: 008 / 011 | Total loss: 0.873 | Reg loss: 0.026 | Tree loss: 0.873 | Accuracy: 0.614500 | 1.5 sec/iter\n",
      "Epoch: 80 | Batch: 009 / 011 | Total loss: 0.865 | Reg loss: 0.026 | Tree loss: 0.865 | Accuracy: 0.608500 | 1.5 sec/iter\n",
      "Epoch: 80 | Batch: 010 / 011 | Total loss: 0.858 | Reg loss: 0.026 | Tree loss: 0.858 | Accuracy: 0.610922 | 1.499 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 81 | Batch: 000 / 011 | Total loss: 0.965 | Reg loss: 0.025 | Tree loss: 0.965 | Accuracy: 0.654500 | 1.505 sec/iter\n",
      "Epoch: 81 | Batch: 001 / 011 | Total loss: 0.953 | Reg loss: 0.025 | Tree loss: 0.953 | Accuracy: 0.650500 | 1.505 sec/iter\n",
      "Epoch: 81 | Batch: 002 / 011 | Total loss: 0.932 | Reg loss: 0.025 | Tree loss: 0.932 | Accuracy: 0.670500 | 1.504 sec/iter\n",
      "Epoch: 81 | Batch: 003 / 011 | Total loss: 0.899 | Reg loss: 0.025 | Tree loss: 0.899 | Accuracy: 0.659500 | 1.504 sec/iter\n",
      "Epoch: 81 | Batch: 004 / 011 | Total loss: 0.893 | Reg loss: 0.025 | Tree loss: 0.893 | Accuracy: 0.670000 | 1.503 sec/iter\n",
      "Epoch: 81 | Batch: 005 / 011 | Total loss: 0.893 | Reg loss: 0.026 | Tree loss: 0.893 | Accuracy: 0.642000 | 1.503 sec/iter\n",
      "Epoch: 81 | Batch: 006 / 011 | Total loss: 0.881 | Reg loss: 0.026 | Tree loss: 0.881 | Accuracy: 0.640000 | 1.503 sec/iter\n",
      "Epoch: 81 | Batch: 007 / 011 | Total loss: 0.850 | Reg loss: 0.026 | Tree loss: 0.850 | Accuracy: 0.634000 | 1.502 sec/iter\n",
      "Epoch: 81 | Batch: 008 / 011 | Total loss: 0.867 | Reg loss: 0.026 | Tree loss: 0.867 | Accuracy: 0.601000 | 1.502 sec/iter\n",
      "Epoch: 81 | Batch: 009 / 011 | Total loss: 0.881 | Reg loss: 0.026 | Tree loss: 0.881 | Accuracy: 0.594000 | 1.502 sec/iter\n",
      "Epoch: 81 | Batch: 010 / 011 | Total loss: 0.889 | Reg loss: 0.026 | Tree loss: 0.889 | Accuracy: 0.566553 | 1.501 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 82 | Batch: 000 / 011 | Total loss: 0.957 | Reg loss: 0.025 | Tree loss: 0.957 | Accuracy: 0.666000 | 1.506 sec/iter\n",
      "Epoch: 82 | Batch: 001 / 011 | Total loss: 0.942 | Reg loss: 0.026 | Tree loss: 0.942 | Accuracy: 0.659000 | 1.506 sec/iter\n",
      "Epoch: 82 | Batch: 002 / 011 | Total loss: 0.947 | Reg loss: 0.026 | Tree loss: 0.947 | Accuracy: 0.639500 | 1.505 sec/iter\n",
      "Epoch: 82 | Batch: 003 / 011 | Total loss: 0.910 | Reg loss: 0.026 | Tree loss: 0.910 | Accuracy: 0.653500 | 1.505 sec/iter\n",
      "Epoch: 82 | Batch: 004 / 011 | Total loss: 0.918 | Reg loss: 0.026 | Tree loss: 0.918 | Accuracy: 0.652000 | 1.504 sec/iter\n",
      "Epoch: 82 | Batch: 005 / 011 | Total loss: 0.890 | Reg loss: 0.026 | Tree loss: 0.890 | Accuracy: 0.654000 | 1.504 sec/iter\n",
      "Epoch: 82 | Batch: 006 / 011 | Total loss: 0.879 | Reg loss: 0.026 | Tree loss: 0.879 | Accuracy: 0.629000 | 1.503 sec/iter\n",
      "Epoch: 82 | Batch: 007 / 011 | Total loss: 0.882 | Reg loss: 0.026 | Tree loss: 0.882 | Accuracy: 0.612000 | 1.502 sec/iter\n",
      "Epoch: 82 | Batch: 008 / 011 | Total loss: 0.869 | Reg loss: 0.026 | Tree loss: 0.869 | Accuracy: 0.601500 | 1.502 sec/iter\n",
      "Epoch: 82 | Batch: 009 / 011 | Total loss: 0.848 | Reg loss: 0.026 | Tree loss: 0.848 | Accuracy: 0.627000 | 1.502 sec/iter\n",
      "Epoch: 82 | Batch: 010 / 011 | Total loss: 0.881 | Reg loss: 0.026 | Tree loss: 0.881 | Accuracy: 0.627986 | 1.501 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 83 | Batch: 000 / 011 | Total loss: 0.957 | Reg loss: 0.026 | Tree loss: 0.957 | Accuracy: 0.659000 | 1.507 sec/iter\n",
      "Epoch: 83 | Batch: 001 / 011 | Total loss: 0.947 | Reg loss: 0.026 | Tree loss: 0.947 | Accuracy: 0.656000 | 1.506 sec/iter\n",
      "Epoch: 83 | Batch: 002 / 011 | Total loss: 0.951 | Reg loss: 0.026 | Tree loss: 0.951 | Accuracy: 0.662000 | 1.506 sec/iter\n",
      "Epoch: 83 | Batch: 003 / 011 | Total loss: 0.886 | Reg loss: 0.026 | Tree loss: 0.886 | Accuracy: 0.698000 | 1.505 sec/iter\n",
      "Epoch: 83 | Batch: 004 / 011 | Total loss: 0.883 | Reg loss: 0.026 | Tree loss: 0.883 | Accuracy: 0.671000 | 1.505 sec/iter\n",
      "Epoch: 83 | Batch: 005 / 011 | Total loss: 0.889 | Reg loss: 0.026 | Tree loss: 0.889 | Accuracy: 0.639000 | 1.504 sec/iter\n",
      "Epoch: 83 | Batch: 006 / 011 | Total loss: 0.880 | Reg loss: 0.026 | Tree loss: 0.880 | Accuracy: 0.614000 | 1.504 sec/iter\n",
      "Epoch: 83 | Batch: 007 / 011 | Total loss: 0.871 | Reg loss: 0.026 | Tree loss: 0.871 | Accuracy: 0.602500 | 1.503 sec/iter\n",
      "Epoch: 83 | Batch: 008 / 011 | Total loss: 0.850 | Reg loss: 0.026 | Tree loss: 0.850 | Accuracy: 0.617500 | 1.503 sec/iter\n",
      "Epoch: 83 | Batch: 009 / 011 | Total loss: 0.889 | Reg loss: 0.026 | Tree loss: 0.889 | Accuracy: 0.595000 | 1.503 sec/iter\n",
      "Epoch: 83 | Batch: 010 / 011 | Total loss: 0.882 | Reg loss: 0.026 | Tree loss: 0.882 | Accuracy: 0.590444 | 1.502 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 84 | Batch: 000 / 011 | Total loss: 0.953 | Reg loss: 0.026 | Tree loss: 0.953 | Accuracy: 0.659500 | 1.507 sec/iter\n",
      "Epoch: 84 | Batch: 001 / 011 | Total loss: 0.954 | Reg loss: 0.026 | Tree loss: 0.954 | Accuracy: 0.651000 | 1.506 sec/iter\n",
      "Epoch: 84 | Batch: 002 / 011 | Total loss: 0.948 | Reg loss: 0.026 | Tree loss: 0.948 | Accuracy: 0.636500 | 1.506 sec/iter\n",
      "Epoch: 84 | Batch: 003 / 011 | Total loss: 0.929 | Reg loss: 0.026 | Tree loss: 0.929 | Accuracy: 0.643000 | 1.506 sec/iter\n",
      "Epoch: 84 | Batch: 004 / 011 | Total loss: 0.892 | Reg loss: 0.026 | Tree loss: 0.892 | Accuracy: 0.656500 | 1.505 sec/iter\n",
      "Epoch: 84 | Batch: 005 / 011 | Total loss: 0.885 | Reg loss: 0.026 | Tree loss: 0.885 | Accuracy: 0.665000 | 1.505 sec/iter\n",
      "Epoch: 84 | Batch: 006 / 011 | Total loss: 0.873 | Reg loss: 0.026 | Tree loss: 0.873 | Accuracy: 0.657500 | 1.504 sec/iter\n",
      "Epoch: 84 | Batch: 007 / 011 | Total loss: 0.851 | Reg loss: 0.026 | Tree loss: 0.851 | Accuracy: 0.641000 | 1.503 sec/iter\n",
      "Epoch: 84 | Batch: 008 / 011 | Total loss: 0.855 | Reg loss: 0.026 | Tree loss: 0.855 | Accuracy: 0.616500 | 1.503 sec/iter\n",
      "Epoch: 84 | Batch: 009 / 011 | Total loss: 0.895 | Reg loss: 0.026 | Tree loss: 0.895 | Accuracy: 0.589500 | 1.503 sec/iter\n",
      "Epoch: 84 | Batch: 010 / 011 | Total loss: 0.855 | Reg loss: 0.026 | Tree loss: 0.855 | Accuracy: 0.600683 | 1.502 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 85 | Batch: 000 / 011 | Total loss: 0.960 | Reg loss: 0.026 | Tree loss: 0.960 | Accuracy: 0.662500 | 1.504 sec/iter\n",
      "Epoch: 85 | Batch: 001 / 011 | Total loss: 0.951 | Reg loss: 0.026 | Tree loss: 0.951 | Accuracy: 0.657500 | 1.504 sec/iter\n",
      "Epoch: 85 | Batch: 002 / 011 | Total loss: 0.914 | Reg loss: 0.026 | Tree loss: 0.914 | Accuracy: 0.662500 | 1.503 sec/iter\n",
      "Epoch: 85 | Batch: 003 / 011 | Total loss: 0.952 | Reg loss: 0.026 | Tree loss: 0.952 | Accuracy: 0.661000 | 1.503 sec/iter\n",
      "Epoch: 85 | Batch: 004 / 011 | Total loss: 0.919 | Reg loss: 0.026 | Tree loss: 0.919 | Accuracy: 0.640000 | 1.502 sec/iter\n",
      "Epoch: 85 | Batch: 005 / 011 | Total loss: 0.882 | Reg loss: 0.026 | Tree loss: 0.882 | Accuracy: 0.658500 | 1.501 sec/iter\n",
      "Epoch: 85 | Batch: 006 / 011 | Total loss: 0.873 | Reg loss: 0.026 | Tree loss: 0.873 | Accuracy: 0.639000 | 1.501 sec/iter\n",
      "Epoch: 85 | Batch: 007 / 011 | Total loss: 0.845 | Reg loss: 0.026 | Tree loss: 0.845 | Accuracy: 0.612500 | 1.5 sec/iter\n",
      "Epoch: 85 | Batch: 008 / 011 | Total loss: 0.855 | Reg loss: 0.026 | Tree loss: 0.855 | Accuracy: 0.615500 | 1.5 sec/iter\n",
      "Epoch: 85 | Batch: 009 / 011 | Total loss: 0.865 | Reg loss: 0.026 | Tree loss: 0.865 | Accuracy: 0.603500 | 1.5 sec/iter\n",
      "Epoch: 85 | Batch: 010 / 011 | Total loss: 0.884 | Reg loss: 0.026 | Tree loss: 0.884 | Accuracy: 0.587031 | 1.499 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 86 | Batch: 000 / 011 | Total loss: 0.954 | Reg loss: 0.026 | Tree loss: 0.954 | Accuracy: 0.651500 | 1.504 sec/iter\n",
      "Epoch: 86 | Batch: 001 / 011 | Total loss: 0.972 | Reg loss: 0.026 | Tree loss: 0.972 | Accuracy: 0.644500 | 1.504 sec/iter\n",
      "Epoch: 86 | Batch: 002 / 011 | Total loss: 0.912 | Reg loss: 0.026 | Tree loss: 0.912 | Accuracy: 0.667500 | 1.504 sec/iter\n",
      "Epoch: 86 | Batch: 003 / 011 | Total loss: 0.904 | Reg loss: 0.026 | Tree loss: 0.904 | Accuracy: 0.672500 | 1.503 sec/iter\n",
      "Epoch: 86 | Batch: 004 / 011 | Total loss: 0.871 | Reg loss: 0.026 | Tree loss: 0.871 | Accuracy: 0.689500 | 1.503 sec/iter\n",
      "Epoch: 86 | Batch: 005 / 011 | Total loss: 0.876 | Reg loss: 0.026 | Tree loss: 0.876 | Accuracy: 0.662000 | 1.502 sec/iter\n",
      "Epoch: 86 | Batch: 006 / 011 | Total loss: 0.870 | Reg loss: 0.026 | Tree loss: 0.870 | Accuracy: 0.647500 | 1.502 sec/iter\n",
      "Epoch: 86 | Batch: 007 / 011 | Total loss: 0.894 | Reg loss: 0.026 | Tree loss: 0.894 | Accuracy: 0.598500 | 1.502 sec/iter\n",
      "Epoch: 86 | Batch: 008 / 011 | Total loss: 0.856 | Reg loss: 0.026 | Tree loss: 0.856 | Accuracy: 0.608500 | 1.501 sec/iter\n",
      "Epoch: 86 | Batch: 009 / 011 | Total loss: 0.881 | Reg loss: 0.026 | Tree loss: 0.881 | Accuracy: 0.577500 | 1.501 sec/iter\n",
      "Epoch: 86 | Batch: 010 / 011 | Total loss: 0.906 | Reg loss: 0.026 | Tree loss: 0.906 | Accuracy: 0.617747 | 1.501 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 87 | Batch: 000 / 011 | Total loss: 0.946 | Reg loss: 0.026 | Tree loss: 0.946 | Accuracy: 0.667000 | 1.504 sec/iter\n",
      "Epoch: 87 | Batch: 001 / 011 | Total loss: 0.946 | Reg loss: 0.026 | Tree loss: 0.946 | Accuracy: 0.650000 | 1.504 sec/iter\n",
      "Epoch: 87 | Batch: 002 / 011 | Total loss: 0.938 | Reg loss: 0.026 | Tree loss: 0.938 | Accuracy: 0.668000 | 1.504 sec/iter\n",
      "Epoch: 87 | Batch: 003 / 011 | Total loss: 0.902 | Reg loss: 0.026 | Tree loss: 0.902 | Accuracy: 0.678500 | 1.504 sec/iter\n",
      "Epoch: 87 | Batch: 004 / 011 | Total loss: 0.894 | Reg loss: 0.026 | Tree loss: 0.894 | Accuracy: 0.664500 | 1.503 sec/iter\n",
      "Epoch: 87 | Batch: 005 / 011 | Total loss: 0.870 | Reg loss: 0.026 | Tree loss: 0.870 | Accuracy: 0.642500 | 1.503 sec/iter\n",
      "Epoch: 87 | Batch: 006 / 011 | Total loss: 0.862 | Reg loss: 0.026 | Tree loss: 0.862 | Accuracy: 0.643500 | 1.502 sec/iter\n",
      "Epoch: 87 | Batch: 007 / 011 | Total loss: 0.864 | Reg loss: 0.026 | Tree loss: 0.864 | Accuracy: 0.604500 | 1.502 sec/iter\n",
      "Epoch: 87 | Batch: 008 / 011 | Total loss: 0.898 | Reg loss: 0.026 | Tree loss: 0.898 | Accuracy: 0.583500 | 1.502 sec/iter\n",
      "Epoch: 87 | Batch: 009 / 011 | Total loss: 0.868 | Reg loss: 0.026 | Tree loss: 0.868 | Accuracy: 0.595000 | 1.502 sec/iter\n",
      "Epoch: 87 | Batch: 010 / 011 | Total loss: 0.862 | Reg loss: 0.026 | Tree loss: 0.862 | Accuracy: 0.624573 | 1.501 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 88 | Batch: 000 / 011 | Total loss: 0.942 | Reg loss: 0.026 | Tree loss: 0.942 | Accuracy: 0.665000 | 1.505 sec/iter\n",
      "Epoch: 88 | Batch: 001 / 011 | Total loss: 0.931 | Reg loss: 0.026 | Tree loss: 0.931 | Accuracy: 0.662500 | 1.504 sec/iter\n",
      "Epoch: 88 | Batch: 002 / 011 | Total loss: 0.938 | Reg loss: 0.026 | Tree loss: 0.938 | Accuracy: 0.650500 | 1.504 sec/iter\n",
      "Epoch: 88 | Batch: 003 / 011 | Total loss: 0.900 | Reg loss: 0.026 | Tree loss: 0.900 | Accuracy: 0.686000 | 1.504 sec/iter\n",
      "Epoch: 88 | Batch: 004 / 011 | Total loss: 0.902 | Reg loss: 0.026 | Tree loss: 0.902 | Accuracy: 0.663500 | 1.503 sec/iter\n",
      "Epoch: 88 | Batch: 005 / 011 | Total loss: 0.880 | Reg loss: 0.026 | Tree loss: 0.880 | Accuracy: 0.660000 | 1.503 sec/iter\n",
      "Epoch: 88 | Batch: 006 / 011 | Total loss: 0.873 | Reg loss: 0.026 | Tree loss: 0.873 | Accuracy: 0.632000 | 1.502 sec/iter\n",
      "Epoch: 88 | Batch: 007 / 011 | Total loss: 0.881 | Reg loss: 0.026 | Tree loss: 0.881 | Accuracy: 0.607000 | 1.502 sec/iter\n",
      "Epoch: 88 | Batch: 008 / 011 | Total loss: 0.861 | Reg loss: 0.026 | Tree loss: 0.861 | Accuracy: 0.606500 | 1.502 sec/iter\n",
      "Epoch: 88 | Batch: 009 / 011 | Total loss: 0.869 | Reg loss: 0.026 | Tree loss: 0.869 | Accuracy: 0.608000 | 1.502 sec/iter\n",
      "Epoch: 88 | Batch: 010 / 011 | Total loss: 0.888 | Reg loss: 0.026 | Tree loss: 0.888 | Accuracy: 0.593857 | 1.501 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 89 | Batch: 000 / 011 | Total loss: 0.971 | Reg loss: 0.026 | Tree loss: 0.971 | Accuracy: 0.652000 | 1.505 sec/iter\n",
      "Epoch: 89 | Batch: 001 / 011 | Total loss: 0.926 | Reg loss: 0.026 | Tree loss: 0.926 | Accuracy: 0.676500 | 1.505 sec/iter\n",
      "Epoch: 89 | Batch: 002 / 011 | Total loss: 0.935 | Reg loss: 0.026 | Tree loss: 0.935 | Accuracy: 0.634500 | 1.505 sec/iter\n",
      "Epoch: 89 | Batch: 003 / 011 | Total loss: 0.909 | Reg loss: 0.026 | Tree loss: 0.909 | Accuracy: 0.660000 | 1.504 sec/iter\n",
      "Epoch: 89 | Batch: 004 / 011 | Total loss: 0.895 | Reg loss: 0.026 | Tree loss: 0.895 | Accuracy: 0.672500 | 1.504 sec/iter\n",
      "Epoch: 89 | Batch: 005 / 011 | Total loss: 0.859 | Reg loss: 0.026 | Tree loss: 0.859 | Accuracy: 0.673000 | 1.503 sec/iter\n",
      "Epoch: 89 | Batch: 006 / 011 | Total loss: 0.858 | Reg loss: 0.026 | Tree loss: 0.858 | Accuracy: 0.636000 | 1.503 sec/iter\n",
      "Epoch: 89 | Batch: 007 / 011 | Total loss: 0.857 | Reg loss: 0.026 | Tree loss: 0.857 | Accuracy: 0.615500 | 1.502 sec/iter\n",
      "Epoch: 89 | Batch: 008 / 011 | Total loss: 0.898 | Reg loss: 0.026 | Tree loss: 0.898 | Accuracy: 0.595000 | 1.502 sec/iter\n",
      "Epoch: 89 | Batch: 009 / 011 | Total loss: 0.850 | Reg loss: 0.026 | Tree loss: 0.850 | Accuracy: 0.614000 | 1.502 sec/iter\n",
      "Epoch: 89 | Batch: 010 / 011 | Total loss: 0.881 | Reg loss: 0.026 | Tree loss: 0.881 | Accuracy: 0.597270 | 1.501 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 90 | Batch: 000 / 011 | Total loss: 0.951 | Reg loss: 0.026 | Tree loss: 0.951 | Accuracy: 0.655500 | 1.506 sec/iter\n",
      "Epoch: 90 | Batch: 001 / 011 | Total loss: 0.951 | Reg loss: 0.026 | Tree loss: 0.951 | Accuracy: 0.661500 | 1.505 sec/iter\n",
      "Epoch: 90 | Batch: 002 / 011 | Total loss: 0.927 | Reg loss: 0.026 | Tree loss: 0.927 | Accuracy: 0.648500 | 1.505 sec/iter\n",
      "Epoch: 90 | Batch: 003 / 011 | Total loss: 0.911 | Reg loss: 0.026 | Tree loss: 0.911 | Accuracy: 0.661500 | 1.505 sec/iter\n",
      "Epoch: 90 | Batch: 004 / 011 | Total loss: 0.877 | Reg loss: 0.026 | Tree loss: 0.877 | Accuracy: 0.680500 | 1.504 sec/iter\n",
      "Epoch: 90 | Batch: 005 / 011 | Total loss: 0.853 | Reg loss: 0.026 | Tree loss: 0.853 | Accuracy: 0.664000 | 1.504 sec/iter\n",
      "Epoch: 90 | Batch: 006 / 011 | Total loss: 0.870 | Reg loss: 0.026 | Tree loss: 0.870 | Accuracy: 0.643000 | 1.503 sec/iter\n",
      "Epoch: 90 | Batch: 007 / 011 | Total loss: 0.864 | Reg loss: 0.026 | Tree loss: 0.864 | Accuracy: 0.605000 | 1.503 sec/iter\n",
      "Epoch: 90 | Batch: 008 / 011 | Total loss: 0.862 | Reg loss: 0.026 | Tree loss: 0.862 | Accuracy: 0.613000 | 1.503 sec/iter\n",
      "Epoch: 90 | Batch: 009 / 011 | Total loss: 0.886 | Reg loss: 0.026 | Tree loss: 0.886 | Accuracy: 0.590000 | 1.502 sec/iter\n",
      "Epoch: 90 | Batch: 010 / 011 | Total loss: 0.821 | Reg loss: 0.026 | Tree loss: 0.821 | Accuracy: 0.662116 | 1.502 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 91 | Batch: 000 / 011 | Total loss: 0.960 | Reg loss: 0.026 | Tree loss: 0.960 | Accuracy: 0.658000 | 1.503 sec/iter\n",
      "Epoch: 91 | Batch: 001 / 011 | Total loss: 0.942 | Reg loss: 0.026 | Tree loss: 0.942 | Accuracy: 0.667500 | 1.503 sec/iter\n",
      "Epoch: 91 | Batch: 002 / 011 | Total loss: 0.933 | Reg loss: 0.026 | Tree loss: 0.933 | Accuracy: 0.642500 | 1.502 sec/iter\n",
      "Epoch: 91 | Batch: 003 / 011 | Total loss: 0.898 | Reg loss: 0.026 | Tree loss: 0.898 | Accuracy: 0.669000 | 1.502 sec/iter\n",
      "Epoch: 91 | Batch: 004 / 011 | Total loss: 0.895 | Reg loss: 0.026 | Tree loss: 0.895 | Accuracy: 0.675000 | 1.501 sec/iter\n",
      "Epoch: 91 | Batch: 005 / 011 | Total loss: 0.884 | Reg loss: 0.026 | Tree loss: 0.884 | Accuracy: 0.664000 | 1.5 sec/iter\n",
      "Epoch: 91 | Batch: 006 / 011 | Total loss: 0.848 | Reg loss: 0.026 | Tree loss: 0.848 | Accuracy: 0.652000 | 1.5 sec/iter\n",
      "Epoch: 91 | Batch: 007 / 011 | Total loss: 0.876 | Reg loss: 0.026 | Tree loss: 0.876 | Accuracy: 0.626000 | 1.499 sec/iter\n",
      "Epoch: 91 | Batch: 008 / 011 | Total loss: 0.858 | Reg loss: 0.026 | Tree loss: 0.858 | Accuracy: 0.597500 | 1.499 sec/iter\n",
      "Epoch: 91 | Batch: 009 / 011 | Total loss: 0.854 | Reg loss: 0.026 | Tree loss: 0.854 | Accuracy: 0.613500 | 1.499 sec/iter\n",
      "Epoch: 91 | Batch: 010 / 011 | Total loss: 0.859 | Reg loss: 0.026 | Tree loss: 0.859 | Accuracy: 0.634812 | 1.498 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 92 | Batch: 000 / 011 | Total loss: 0.957 | Reg loss: 0.026 | Tree loss: 0.957 | Accuracy: 0.646000 | 1.503 sec/iter\n",
      "Epoch: 92 | Batch: 001 / 011 | Total loss: 0.918 | Reg loss: 0.026 | Tree loss: 0.918 | Accuracy: 0.679500 | 1.503 sec/iter\n",
      "Epoch: 92 | Batch: 002 / 011 | Total loss: 0.947 | Reg loss: 0.026 | Tree loss: 0.947 | Accuracy: 0.636000 | 1.503 sec/iter\n",
      "Epoch: 92 | Batch: 003 / 011 | Total loss: 0.897 | Reg loss: 0.026 | Tree loss: 0.897 | Accuracy: 0.686000 | 1.502 sec/iter\n",
      "Epoch: 92 | Batch: 004 / 011 | Total loss: 0.871 | Reg loss: 0.026 | Tree loss: 0.871 | Accuracy: 0.674500 | 1.502 sec/iter\n",
      "Epoch: 92 | Batch: 005 / 011 | Total loss: 0.879 | Reg loss: 0.026 | Tree loss: 0.879 | Accuracy: 0.662000 | 1.502 sec/iter\n",
      "Epoch: 92 | Batch: 006 / 011 | Total loss: 0.867 | Reg loss: 0.026 | Tree loss: 0.867 | Accuracy: 0.617000 | 1.501 sec/iter\n",
      "Epoch: 92 | Batch: 007 / 011 | Total loss: 0.868 | Reg loss: 0.026 | Tree loss: 0.868 | Accuracy: 0.603500 | 1.501 sec/iter\n",
      "Epoch: 92 | Batch: 008 / 011 | Total loss: 0.871 | Reg loss: 0.026 | Tree loss: 0.871 | Accuracy: 0.599000 | 1.501 sec/iter\n",
      "Epoch: 92 | Batch: 009 / 011 | Total loss: 0.870 | Reg loss: 0.026 | Tree loss: 0.870 | Accuracy: 0.600000 | 1.501 sec/iter\n",
      "Epoch: 92 | Batch: 010 / 011 | Total loss: 0.850 | Reg loss: 0.026 | Tree loss: 0.850 | Accuracy: 0.580205 | 1.501 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 93 | Batch: 000 / 011 | Total loss: 0.941 | Reg loss: 0.026 | Tree loss: 0.941 | Accuracy: 0.674500 | 1.503 sec/iter\n",
      "Epoch: 93 | Batch: 001 / 011 | Total loss: 0.945 | Reg loss: 0.026 | Tree loss: 0.945 | Accuracy: 0.652500 | 1.503 sec/iter\n",
      "Epoch: 93 | Batch: 002 / 011 | Total loss: 0.921 | Reg loss: 0.026 | Tree loss: 0.921 | Accuracy: 0.653000 | 1.503 sec/iter\n",
      "Epoch: 93 | Batch: 003 / 011 | Total loss: 0.923 | Reg loss: 0.026 | Tree loss: 0.923 | Accuracy: 0.666500 | 1.502 sec/iter\n",
      "Epoch: 93 | Batch: 004 / 011 | Total loss: 0.882 | Reg loss: 0.026 | Tree loss: 0.882 | Accuracy: 0.671500 | 1.502 sec/iter\n",
      "Epoch: 93 | Batch: 005 / 011 | Total loss: 0.886 | Reg loss: 0.026 | Tree loss: 0.886 | Accuracy: 0.659000 | 1.502 sec/iter\n",
      "Epoch: 93 | Batch: 006 / 011 | Total loss: 0.875 | Reg loss: 0.026 | Tree loss: 0.875 | Accuracy: 0.660500 | 1.501 sec/iter\n",
      "Epoch: 93 | Batch: 007 / 011 | Total loss: 0.854 | Reg loss: 0.026 | Tree loss: 0.854 | Accuracy: 0.641500 | 1.5 sec/iter\n",
      "Epoch: 93 | Batch: 008 / 011 | Total loss: 0.865 | Reg loss: 0.026 | Tree loss: 0.865 | Accuracy: 0.600000 | 1.5 sec/iter\n",
      "Epoch: 93 | Batch: 009 / 011 | Total loss: 0.863 | Reg loss: 0.026 | Tree loss: 0.863 | Accuracy: 0.607000 | 1.5 sec/iter\n",
      "Epoch: 93 | Batch: 010 / 011 | Total loss: 0.882 | Reg loss: 0.026 | Tree loss: 0.882 | Accuracy: 0.556314 | 1.5 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 94 | Batch: 000 / 011 | Total loss: 0.971 | Reg loss: 0.026 | Tree loss: 0.971 | Accuracy: 0.651000 | 1.504 sec/iter\n",
      "Epoch: 94 | Batch: 001 / 011 | Total loss: 0.947 | Reg loss: 0.026 | Tree loss: 0.947 | Accuracy: 0.658000 | 1.503 sec/iter\n",
      "Epoch: 94 | Batch: 002 / 011 | Total loss: 0.938 | Reg loss: 0.026 | Tree loss: 0.938 | Accuracy: 0.661500 | 1.503 sec/iter\n",
      "Epoch: 94 | Batch: 003 / 011 | Total loss: 0.898 | Reg loss: 0.026 | Tree loss: 0.898 | Accuracy: 0.656000 | 1.503 sec/iter\n",
      "Epoch: 94 | Batch: 004 / 011 | Total loss: 0.883 | Reg loss: 0.026 | Tree loss: 0.883 | Accuracy: 0.662000 | 1.502 sec/iter\n",
      "Epoch: 94 | Batch: 005 / 011 | Total loss: 0.848 | Reg loss: 0.026 | Tree loss: 0.848 | Accuracy: 0.666000 | 1.502 sec/iter\n",
      "Epoch: 94 | Batch: 006 / 011 | Total loss: 0.855 | Reg loss: 0.026 | Tree loss: 0.855 | Accuracy: 0.644500 | 1.501 sec/iter\n",
      "Epoch: 94 | Batch: 007 / 011 | Total loss: 0.871 | Reg loss: 0.026 | Tree loss: 0.871 | Accuracy: 0.606500 | 1.501 sec/iter\n",
      "Epoch: 94 | Batch: 008 / 011 | Total loss: 0.868 | Reg loss: 0.026 | Tree loss: 0.868 | Accuracy: 0.616000 | 1.501 sec/iter\n",
      "Epoch: 94 | Batch: 009 / 011 | Total loss: 0.836 | Reg loss: 0.026 | Tree loss: 0.836 | Accuracy: 0.622500 | 1.5 sec/iter\n",
      "Epoch: 94 | Batch: 010 / 011 | Total loss: 0.852 | Reg loss: 0.026 | Tree loss: 0.852 | Accuracy: 0.587031 | 1.5 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 95 | Batch: 000 / 011 | Total loss: 0.937 | Reg loss: 0.026 | Tree loss: 0.937 | Accuracy: 0.661000 | 1.504 sec/iter\n",
      "Epoch: 95 | Batch: 001 / 011 | Total loss: 0.936 | Reg loss: 0.026 | Tree loss: 0.936 | Accuracy: 0.650500 | 1.504 sec/iter\n",
      "Epoch: 95 | Batch: 002 / 011 | Total loss: 0.907 | Reg loss: 0.026 | Tree loss: 0.907 | Accuracy: 0.670000 | 1.503 sec/iter\n",
      "Epoch: 95 | Batch: 003 / 011 | Total loss: 0.898 | Reg loss: 0.026 | Tree loss: 0.898 | Accuracy: 0.665500 | 1.503 sec/iter\n",
      "Epoch: 95 | Batch: 004 / 011 | Total loss: 0.897 | Reg loss: 0.026 | Tree loss: 0.897 | Accuracy: 0.664500 | 1.503 sec/iter\n",
      "Epoch: 95 | Batch: 005 / 011 | Total loss: 0.884 | Reg loss: 0.026 | Tree loss: 0.884 | Accuracy: 0.658000 | 1.502 sec/iter\n",
      "Epoch: 95 | Batch: 006 / 011 | Total loss: 0.874 | Reg loss: 0.026 | Tree loss: 0.874 | Accuracy: 0.638500 | 1.502 sec/iter\n",
      "Epoch: 95 | Batch: 007 / 011 | Total loss: 0.852 | Reg loss: 0.026 | Tree loss: 0.852 | Accuracy: 0.630000 | 1.501 sec/iter\n",
      "Epoch: 95 | Batch: 008 / 011 | Total loss: 0.857 | Reg loss: 0.026 | Tree loss: 0.857 | Accuracy: 0.608000 | 1.501 sec/iter\n",
      "Epoch: 95 | Batch: 009 / 011 | Total loss: 0.874 | Reg loss: 0.026 | Tree loss: 0.874 | Accuracy: 0.591500 | 1.501 sec/iter\n",
      "Epoch: 95 | Batch: 010 / 011 | Total loss: 0.802 | Reg loss: 0.026 | Tree loss: 0.802 | Accuracy: 0.645051 | 1.5 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 96 | Batch: 000 / 011 | Total loss: 0.941 | Reg loss: 0.026 | Tree loss: 0.941 | Accuracy: 0.663000 | 1.505 sec/iter\n",
      "Epoch: 96 | Batch: 001 / 011 | Total loss: 0.936 | Reg loss: 0.026 | Tree loss: 0.936 | Accuracy: 0.664500 | 1.505 sec/iter\n",
      "Epoch: 96 | Batch: 002 / 011 | Total loss: 0.910 | Reg loss: 0.026 | Tree loss: 0.910 | Accuracy: 0.679000 | 1.504 sec/iter\n",
      "Epoch: 96 | Batch: 003 / 011 | Total loss: 0.895 | Reg loss: 0.026 | Tree loss: 0.895 | Accuracy: 0.658500 | 1.504 sec/iter\n",
      "Epoch: 96 | Batch: 004 / 011 | Total loss: 0.883 | Reg loss: 0.026 | Tree loss: 0.883 | Accuracy: 0.688000 | 1.504 sec/iter\n",
      "Epoch: 96 | Batch: 005 / 011 | Total loss: 0.879 | Reg loss: 0.026 | Tree loss: 0.879 | Accuracy: 0.648500 | 1.503 sec/iter\n",
      "Epoch: 96 | Batch: 006 / 011 | Total loss: 0.838 | Reg loss: 0.026 | Tree loss: 0.838 | Accuracy: 0.672500 | 1.503 sec/iter\n",
      "Epoch: 96 | Batch: 007 / 011 | Total loss: 0.871 | Reg loss: 0.026 | Tree loss: 0.871 | Accuracy: 0.613000 | 1.502 sec/iter\n",
      "Epoch: 96 | Batch: 008 / 011 | Total loss: 0.889 | Reg loss: 0.026 | Tree loss: 0.889 | Accuracy: 0.590500 | 1.502 sec/iter\n",
      "Epoch: 96 | Batch: 009 / 011 | Total loss: 0.868 | Reg loss: 0.026 | Tree loss: 0.868 | Accuracy: 0.598000 | 1.502 sec/iter\n",
      "Epoch: 96 | Batch: 010 / 011 | Total loss: 0.871 | Reg loss: 0.026 | Tree loss: 0.871 | Accuracy: 0.614334 | 1.501 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 97 | Batch: 000 / 011 | Total loss: 0.941 | Reg loss: 0.026 | Tree loss: 0.941 | Accuracy: 0.657000 | 1.503 sec/iter\n",
      "Epoch: 97 | Batch: 001 / 011 | Total loss: 0.952 | Reg loss: 0.026 | Tree loss: 0.952 | Accuracy: 0.638000 | 1.503 sec/iter\n",
      "Epoch: 97 | Batch: 002 / 011 | Total loss: 0.892 | Reg loss: 0.026 | Tree loss: 0.892 | Accuracy: 0.671500 | 1.502 sec/iter\n",
      "Epoch: 97 | Batch: 003 / 011 | Total loss: 0.901 | Reg loss: 0.026 | Tree loss: 0.901 | Accuracy: 0.675500 | 1.502 sec/iter\n",
      "Epoch: 97 | Batch: 004 / 011 | Total loss: 0.919 | Reg loss: 0.026 | Tree loss: 0.919 | Accuracy: 0.645500 | 1.501 sec/iter\n",
      "Epoch: 97 | Batch: 005 / 011 | Total loss: 0.856 | Reg loss: 0.026 | Tree loss: 0.856 | Accuracy: 0.677000 | 1.5 sec/iter\n",
      "Epoch: 97 | Batch: 006 / 011 | Total loss: 0.864 | Reg loss: 0.026 | Tree loss: 0.864 | Accuracy: 0.641500 | 1.5 sec/iter\n",
      "Epoch: 97 | Batch: 007 / 011 | Total loss: 0.860 | Reg loss: 0.026 | Tree loss: 0.860 | Accuracy: 0.612000 | 1.5 sec/iter\n",
      "Epoch: 97 | Batch: 008 / 011 | Total loss: 0.857 | Reg loss: 0.026 | Tree loss: 0.857 | Accuracy: 0.605500 | 1.5 sec/iter\n",
      "Epoch: 97 | Batch: 009 / 011 | Total loss: 0.852 | Reg loss: 0.026 | Tree loss: 0.852 | Accuracy: 0.623000 | 1.5 sec/iter\n",
      "Epoch: 97 | Batch: 010 / 011 | Total loss: 0.851 | Reg loss: 0.026 | Tree loss: 0.851 | Accuracy: 0.617747 | 1.5 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 98 | Batch: 000 / 011 | Total loss: 0.936 | Reg loss: 0.026 | Tree loss: 0.936 | Accuracy: 0.666000 | 1.503 sec/iter\n",
      "Epoch: 98 | Batch: 001 / 011 | Total loss: 0.934 | Reg loss: 0.026 | Tree loss: 0.934 | Accuracy: 0.656500 | 1.502 sec/iter\n",
      "Epoch: 98 | Batch: 002 / 011 | Total loss: 0.908 | Reg loss: 0.026 | Tree loss: 0.908 | Accuracy: 0.669000 | 1.502 sec/iter\n",
      "Epoch: 98 | Batch: 003 / 011 | Total loss: 0.876 | Reg loss: 0.026 | Tree loss: 0.876 | Accuracy: 0.681500 | 1.501 sec/iter\n",
      "Epoch: 98 | Batch: 004 / 011 | Total loss: 0.871 | Reg loss: 0.026 | Tree loss: 0.871 | Accuracy: 0.664500 | 1.501 sec/iter\n",
      "Epoch: 98 | Batch: 005 / 011 | Total loss: 0.873 | Reg loss: 0.026 | Tree loss: 0.873 | Accuracy: 0.642000 | 1.501 sec/iter\n",
      "Epoch: 98 | Batch: 006 / 011 | Total loss: 0.871 | Reg loss: 0.026 | Tree loss: 0.871 | Accuracy: 0.622500 | 1.5 sec/iter\n",
      "Epoch: 98 | Batch: 007 / 011 | Total loss: 0.851 | Reg loss: 0.026 | Tree loss: 0.851 | Accuracy: 0.613000 | 1.5 sec/iter\n",
      "Epoch: 98 | Batch: 008 / 011 | Total loss: 0.867 | Reg loss: 0.026 | Tree loss: 0.867 | Accuracy: 0.611000 | 1.5 sec/iter\n",
      "Epoch: 98 | Batch: 009 / 011 | Total loss: 0.851 | Reg loss: 0.026 | Tree loss: 0.851 | Accuracy: 0.603500 | 1.5 sec/iter\n",
      "Epoch: 98 | Batch: 010 / 011 | Total loss: 0.865 | Reg loss: 0.026 | Tree loss: 0.865 | Accuracy: 0.607509 | 1.499 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 99 | Batch: 000 / 011 | Total loss: 0.951 | Reg loss: 0.026 | Tree loss: 0.951 | Accuracy: 0.655000 | 1.503 sec/iter\n",
      "Epoch: 99 | Batch: 001 / 011 | Total loss: 0.935 | Reg loss: 0.026 | Tree loss: 0.935 | Accuracy: 0.651000 | 1.503 sec/iter\n",
      "Epoch: 99 | Batch: 002 / 011 | Total loss: 0.926 | Reg loss: 0.026 | Tree loss: 0.926 | Accuracy: 0.664000 | 1.502 sec/iter\n",
      "Epoch: 99 | Batch: 003 / 011 | Total loss: 0.897 | Reg loss: 0.026 | Tree loss: 0.897 | Accuracy: 0.673000 | 1.502 sec/iter\n",
      "Epoch: 99 | Batch: 004 / 011 | Total loss: 0.877 | Reg loss: 0.026 | Tree loss: 0.877 | Accuracy: 0.670000 | 1.502 sec/iter\n",
      "Epoch: 99 | Batch: 005 / 011 | Total loss: 0.864 | Reg loss: 0.026 | Tree loss: 0.864 | Accuracy: 0.670000 | 1.501 sec/iter\n",
      "Epoch: 99 | Batch: 006 / 011 | Total loss: 0.859 | Reg loss: 0.026 | Tree loss: 0.859 | Accuracy: 0.634000 | 1.501 sec/iter\n",
      "Epoch: 99 | Batch: 007 / 011 | Total loss: 0.862 | Reg loss: 0.026 | Tree loss: 0.862 | Accuracy: 0.607500 | 1.501 sec/iter\n",
      "Epoch: 99 | Batch: 008 / 011 | Total loss: 0.835 | Reg loss: 0.026 | Tree loss: 0.835 | Accuracy: 0.611500 | 1.5 sec/iter\n",
      "Epoch: 99 | Batch: 009 / 011 | Total loss: 0.849 | Reg loss: 0.026 | Tree loss: 0.849 | Accuracy: 0.621500 | 1.5 sec/iter\n",
      "Epoch: 99 | Batch: 010 / 011 | Total loss: 0.966 | Reg loss: 0.026 | Tree loss: 0.966 | Accuracy: 0.552901 | 1.5 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 100 | Batch: 000 / 011 | Total loss: 0.947 | Reg loss: 0.026 | Tree loss: 0.947 | Accuracy: 0.649500 | 1.503 sec/iter\n",
      "Epoch: 100 | Batch: 001 / 011 | Total loss: 0.926 | Reg loss: 0.026 | Tree loss: 0.926 | Accuracy: 0.671500 | 1.503 sec/iter\n",
      "Epoch: 100 | Batch: 002 / 011 | Total loss: 0.921 | Reg loss: 0.026 | Tree loss: 0.921 | Accuracy: 0.670500 | 1.503 sec/iter\n",
      "Epoch: 100 | Batch: 003 / 011 | Total loss: 0.891 | Reg loss: 0.026 | Tree loss: 0.891 | Accuracy: 0.657500 | 1.503 sec/iter\n",
      "Epoch: 100 | Batch: 004 / 011 | Total loss: 0.873 | Reg loss: 0.026 | Tree loss: 0.873 | Accuracy: 0.667000 | 1.502 sec/iter\n",
      "Epoch: 100 | Batch: 005 / 011 | Total loss: 0.869 | Reg loss: 0.026 | Tree loss: 0.869 | Accuracy: 0.672500 | 1.502 sec/iter\n",
      "Epoch: 100 | Batch: 006 / 011 | Total loss: 0.856 | Reg loss: 0.026 | Tree loss: 0.856 | Accuracy: 0.653500 | 1.501 sec/iter\n",
      "Epoch: 100 | Batch: 007 / 011 | Total loss: 0.855 | Reg loss: 0.026 | Tree loss: 0.855 | Accuracy: 0.606500 | 1.501 sec/iter\n",
      "Epoch: 100 | Batch: 008 / 011 | Total loss: 0.858 | Reg loss: 0.026 | Tree loss: 0.858 | Accuracy: 0.609500 | 1.501 sec/iter\n",
      "Epoch: 100 | Batch: 009 / 011 | Total loss: 0.859 | Reg loss: 0.026 | Tree loss: 0.859 | Accuracy: 0.612000 | 1.501 sec/iter\n",
      "Epoch: 100 | Batch: 010 / 011 | Total loss: 0.876 | Reg loss: 0.026 | Tree loss: 0.876 | Accuracy: 0.593857 | 1.5 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 101 | Batch: 000 / 011 | Total loss: 0.950 | Reg loss: 0.026 | Tree loss: 0.950 | Accuracy: 0.652000 | 1.504 sec/iter\n",
      "Epoch: 101 | Batch: 001 / 011 | Total loss: 0.918 | Reg loss: 0.026 | Tree loss: 0.918 | Accuracy: 0.668500 | 1.504 sec/iter\n",
      "Epoch: 101 | Batch: 002 / 011 | Total loss: 0.934 | Reg loss: 0.026 | Tree loss: 0.934 | Accuracy: 0.644500 | 1.503 sec/iter\n",
      "Epoch: 101 | Batch: 003 / 011 | Total loss: 0.889 | Reg loss: 0.026 | Tree loss: 0.889 | Accuracy: 0.681000 | 1.503 sec/iter\n",
      "Epoch: 101 | Batch: 004 / 011 | Total loss: 0.874 | Reg loss: 0.026 | Tree loss: 0.874 | Accuracy: 0.665000 | 1.503 sec/iter\n",
      "Epoch: 101 | Batch: 005 / 011 | Total loss: 0.864 | Reg loss: 0.026 | Tree loss: 0.864 | Accuracy: 0.686000 | 1.502 sec/iter\n",
      "Epoch: 101 | Batch: 006 / 011 | Total loss: 0.862 | Reg loss: 0.026 | Tree loss: 0.862 | Accuracy: 0.619500 | 1.502 sec/iter\n",
      "Epoch: 101 | Batch: 007 / 011 | Total loss: 0.872 | Reg loss: 0.026 | Tree loss: 0.872 | Accuracy: 0.606000 | 1.501 sec/iter\n",
      "Epoch: 101 | Batch: 008 / 011 | Total loss: 0.850 | Reg loss: 0.026 | Tree loss: 0.850 | Accuracy: 0.613000 | 1.501 sec/iter\n",
      "Epoch: 101 | Batch: 009 / 011 | Total loss: 0.835 | Reg loss: 0.026 | Tree loss: 0.835 | Accuracy: 0.626000 | 1.501 sec/iter\n",
      "Epoch: 101 | Batch: 010 / 011 | Total loss: 0.833 | Reg loss: 0.026 | Tree loss: 0.833 | Accuracy: 0.617747 | 1.5 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 102 | Batch: 000 / 011 | Total loss: 0.922 | Reg loss: 0.026 | Tree loss: 0.922 | Accuracy: 0.671000 | 1.505 sec/iter\n",
      "Epoch: 102 | Batch: 001 / 011 | Total loss: 0.932 | Reg loss: 0.026 | Tree loss: 0.932 | Accuracy: 0.650500 | 1.504 sec/iter\n",
      "Epoch: 102 | Batch: 002 / 011 | Total loss: 0.913 | Reg loss: 0.026 | Tree loss: 0.913 | Accuracy: 0.667500 | 1.504 sec/iter\n",
      "Epoch: 102 | Batch: 003 / 011 | Total loss: 0.904 | Reg loss: 0.026 | Tree loss: 0.904 | Accuracy: 0.664500 | 1.504 sec/iter\n",
      "Epoch: 102 | Batch: 004 / 011 | Total loss: 0.882 | Reg loss: 0.026 | Tree loss: 0.882 | Accuracy: 0.662000 | 1.503 sec/iter\n",
      "Epoch: 102 | Batch: 005 / 011 | Total loss: 0.862 | Reg loss: 0.026 | Tree loss: 0.862 | Accuracy: 0.662000 | 1.503 sec/iter\n",
      "Epoch: 102 | Batch: 006 / 011 | Total loss: 0.853 | Reg loss: 0.026 | Tree loss: 0.853 | Accuracy: 0.641000 | 1.502 sec/iter\n",
      "Epoch: 102 | Batch: 007 / 011 | Total loss: 0.856 | Reg loss: 0.026 | Tree loss: 0.856 | Accuracy: 0.614000 | 1.502 sec/iter\n",
      "Epoch: 102 | Batch: 008 / 011 | Total loss: 0.864 | Reg loss: 0.026 | Tree loss: 0.864 | Accuracy: 0.609000 | 1.502 sec/iter\n",
      "Epoch: 102 | Batch: 009 / 011 | Total loss: 0.846 | Reg loss: 0.026 | Tree loss: 0.846 | Accuracy: 0.615500 | 1.502 sec/iter\n",
      "Epoch: 102 | Batch: 010 / 011 | Total loss: 0.865 | Reg loss: 0.026 | Tree loss: 0.865 | Accuracy: 0.617747 | 1.501 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 103 | Batch: 000 / 011 | Total loss: 0.928 | Reg loss: 0.026 | Tree loss: 0.928 | Accuracy: 0.657000 | 1.503 sec/iter\n",
      "Epoch: 103 | Batch: 001 / 011 | Total loss: 0.899 | Reg loss: 0.026 | Tree loss: 0.899 | Accuracy: 0.684000 | 1.502 sec/iter\n",
      "Epoch: 103 | Batch: 002 / 011 | Total loss: 0.921 | Reg loss: 0.026 | Tree loss: 0.921 | Accuracy: 0.654500 | 1.502 sec/iter\n",
      "Epoch: 103 | Batch: 003 / 011 | Total loss: 0.909 | Reg loss: 0.026 | Tree loss: 0.909 | Accuracy: 0.649500 | 1.501 sec/iter\n",
      "Epoch: 103 | Batch: 004 / 011 | Total loss: 0.879 | Reg loss: 0.026 | Tree loss: 0.879 | Accuracy: 0.664500 | 1.501 sec/iter\n",
      "Epoch: 103 | Batch: 005 / 011 | Total loss: 0.879 | Reg loss: 0.026 | Tree loss: 0.879 | Accuracy: 0.668000 | 1.5 sec/iter\n",
      "Epoch: 103 | Batch: 006 / 011 | Total loss: 0.864 | Reg loss: 0.026 | Tree loss: 0.864 | Accuracy: 0.638500 | 1.5 sec/iter\n",
      "Epoch: 103 | Batch: 007 / 011 | Total loss: 0.845 | Reg loss: 0.026 | Tree loss: 0.845 | Accuracy: 0.627500 | 1.5 sec/iter\n",
      "Epoch: 103 | Batch: 008 / 011 | Total loss: 0.857 | Reg loss: 0.026 | Tree loss: 0.857 | Accuracy: 0.616500 | 1.5 sec/iter\n",
      "Epoch: 103 | Batch: 009 / 011 | Total loss: 0.870 | Reg loss: 0.026 | Tree loss: 0.870 | Accuracy: 0.592500 | 1.5 sec/iter\n",
      "Epoch: 103 | Batch: 010 / 011 | Total loss: 0.816 | Reg loss: 0.026 | Tree loss: 0.816 | Accuracy: 0.621160 | 1.499 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 104 | Batch: 000 / 011 | Total loss: 0.942 | Reg loss: 0.026 | Tree loss: 0.942 | Accuracy: 0.651000 | 1.502 sec/iter\n",
      "Epoch: 104 | Batch: 001 / 011 | Total loss: 0.900 | Reg loss: 0.026 | Tree loss: 0.900 | Accuracy: 0.661500 | 1.502 sec/iter\n",
      "Epoch: 104 | Batch: 002 / 011 | Total loss: 0.918 | Reg loss: 0.026 | Tree loss: 0.918 | Accuracy: 0.654500 | 1.502 sec/iter\n",
      "Epoch: 104 | Batch: 003 / 011 | Total loss: 0.909 | Reg loss: 0.026 | Tree loss: 0.909 | Accuracy: 0.655500 | 1.501 sec/iter\n",
      "Epoch: 104 | Batch: 004 / 011 | Total loss: 0.877 | Reg loss: 0.026 | Tree loss: 0.877 | Accuracy: 0.663500 | 1.501 sec/iter\n",
      "Epoch: 104 | Batch: 005 / 011 | Total loss: 0.869 | Reg loss: 0.026 | Tree loss: 0.869 | Accuracy: 0.655000 | 1.501 sec/iter\n",
      "Epoch: 104 | Batch: 006 / 011 | Total loss: 0.873 | Reg loss: 0.026 | Tree loss: 0.873 | Accuracy: 0.642500 | 1.5 sec/iter\n",
      "Epoch: 104 | Batch: 007 / 011 | Total loss: 0.847 | Reg loss: 0.026 | Tree loss: 0.847 | Accuracy: 0.630500 | 1.5 sec/iter\n",
      "Epoch: 104 | Batch: 008 / 011 | Total loss: 0.871 | Reg loss: 0.026 | Tree loss: 0.871 | Accuracy: 0.600500 | 1.5 sec/iter\n",
      "Epoch: 104 | Batch: 009 / 011 | Total loss: 0.862 | Reg loss: 0.026 | Tree loss: 0.862 | Accuracy: 0.592500 | 1.5 sec/iter\n",
      "Epoch: 104 | Batch: 010 / 011 | Total loss: 0.835 | Reg loss: 0.026 | Tree loss: 0.835 | Accuracy: 0.610922 | 1.499 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 105 | Batch: 000 / 011 | Total loss: 0.932 | Reg loss: 0.026 | Tree loss: 0.932 | Accuracy: 0.652000 | 1.503 sec/iter\n",
      "Epoch: 105 | Batch: 001 / 011 | Total loss: 0.917 | Reg loss: 0.026 | Tree loss: 0.917 | Accuracy: 0.662500 | 1.502 sec/iter\n",
      "Epoch: 105 | Batch: 002 / 011 | Total loss: 0.912 | Reg loss: 0.026 | Tree loss: 0.912 | Accuracy: 0.666500 | 1.502 sec/iter\n",
      "Epoch: 105 | Batch: 003 / 011 | Total loss: 0.883 | Reg loss: 0.026 | Tree loss: 0.883 | Accuracy: 0.686000 | 1.502 sec/iter\n",
      "Epoch: 105 | Batch: 004 / 011 | Total loss: 0.885 | Reg loss: 0.026 | Tree loss: 0.885 | Accuracy: 0.649500 | 1.502 sec/iter\n",
      "Epoch: 105 | Batch: 005 / 011 | Total loss: 0.868 | Reg loss: 0.026 | Tree loss: 0.868 | Accuracy: 0.674500 | 1.501 sec/iter\n",
      "Epoch: 105 | Batch: 006 / 011 | Total loss: 0.868 | Reg loss: 0.026 | Tree loss: 0.868 | Accuracy: 0.655500 | 1.501 sec/iter\n",
      "Epoch: 105 | Batch: 007 / 011 | Total loss: 0.860 | Reg loss: 0.026 | Tree loss: 0.860 | Accuracy: 0.622000 | 1.501 sec/iter\n",
      "Epoch: 105 | Batch: 008 / 011 | Total loss: 0.857 | Reg loss: 0.026 | Tree loss: 0.857 | Accuracy: 0.601500 | 1.5 sec/iter\n",
      "Epoch: 105 | Batch: 009 / 011 | Total loss: 0.857 | Reg loss: 0.026 | Tree loss: 0.857 | Accuracy: 0.618500 | 1.5 sec/iter\n",
      "Epoch: 105 | Batch: 010 / 011 | Total loss: 0.866 | Reg loss: 0.026 | Tree loss: 0.866 | Accuracy: 0.604096 | 1.5 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 106 | Batch: 000 / 011 | Total loss: 0.928 | Reg loss: 0.026 | Tree loss: 0.928 | Accuracy: 0.654500 | 1.503 sec/iter\n",
      "Epoch: 106 | Batch: 001 / 011 | Total loss: 0.919 | Reg loss: 0.026 | Tree loss: 0.919 | Accuracy: 0.671000 | 1.503 sec/iter\n",
      "Epoch: 106 | Batch: 002 / 011 | Total loss: 0.924 | Reg loss: 0.026 | Tree loss: 0.924 | Accuracy: 0.659500 | 1.503 sec/iter\n",
      "Epoch: 106 | Batch: 003 / 011 | Total loss: 0.892 | Reg loss: 0.026 | Tree loss: 0.892 | Accuracy: 0.674500 | 1.502 sec/iter\n",
      "Epoch: 106 | Batch: 004 / 011 | Total loss: 0.873 | Reg loss: 0.026 | Tree loss: 0.873 | Accuracy: 0.669500 | 1.502 sec/iter\n",
      "Epoch: 106 | Batch: 005 / 011 | Total loss: 0.861 | Reg loss: 0.026 | Tree loss: 0.861 | Accuracy: 0.668000 | 1.502 sec/iter\n",
      "Epoch: 106 | Batch: 006 / 011 | Total loss: 0.847 | Reg loss: 0.026 | Tree loss: 0.847 | Accuracy: 0.649500 | 1.501 sec/iter\n",
      "Epoch: 106 | Batch: 007 / 011 | Total loss: 0.859 | Reg loss: 0.026 | Tree loss: 0.859 | Accuracy: 0.627000 | 1.501 sec/iter\n",
      "Epoch: 106 | Batch: 008 / 011 | Total loss: 0.848 | Reg loss: 0.026 | Tree loss: 0.848 | Accuracy: 0.607000 | 1.501 sec/iter\n",
      "Epoch: 106 | Batch: 009 / 011 | Total loss: 0.871 | Reg loss: 0.026 | Tree loss: 0.871 | Accuracy: 0.610500 | 1.501 sec/iter\n",
      "Epoch: 106 | Batch: 010 / 011 | Total loss: 0.821 | Reg loss: 0.026 | Tree loss: 0.821 | Accuracy: 0.624573 | 1.501 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 107 | Batch: 000 / 011 | Total loss: 0.923 | Reg loss: 0.026 | Tree loss: 0.923 | Accuracy: 0.657500 | 1.504 sec/iter\n",
      "Epoch: 107 | Batch: 001 / 011 | Total loss: 0.937 | Reg loss: 0.026 | Tree loss: 0.937 | Accuracy: 0.645000 | 1.503 sec/iter\n",
      "Epoch: 107 | Batch: 002 / 011 | Total loss: 0.897 | Reg loss: 0.026 | Tree loss: 0.897 | Accuracy: 0.674500 | 1.503 sec/iter\n",
      "Epoch: 107 | Batch: 003 / 011 | Total loss: 0.902 | Reg loss: 0.026 | Tree loss: 0.902 | Accuracy: 0.664000 | 1.503 sec/iter\n",
      "Epoch: 107 | Batch: 004 / 011 | Total loss: 0.879 | Reg loss: 0.026 | Tree loss: 0.879 | Accuracy: 0.665000 | 1.502 sec/iter\n",
      "Epoch: 107 | Batch: 005 / 011 | Total loss: 0.860 | Reg loss: 0.026 | Tree loss: 0.860 | Accuracy: 0.681000 | 1.502 sec/iter\n",
      "Epoch: 107 | Batch: 006 / 011 | Total loss: 0.849 | Reg loss: 0.026 | Tree loss: 0.849 | Accuracy: 0.638000 | 1.502 sec/iter\n",
      "Epoch: 107 | Batch: 007 / 011 | Total loss: 0.864 | Reg loss: 0.026 | Tree loss: 0.864 | Accuracy: 0.615000 | 1.501 sec/iter\n",
      "Epoch: 107 | Batch: 008 / 011 | Total loss: 0.848 | Reg loss: 0.026 | Tree loss: 0.848 | Accuracy: 0.611000 | 1.501 sec/iter\n",
      "Epoch: 107 | Batch: 009 / 011 | Total loss: 0.842 | Reg loss: 0.026 | Tree loss: 0.842 | Accuracy: 0.636500 | 1.501 sec/iter\n",
      "Epoch: 107 | Batch: 010 / 011 | Total loss: 0.874 | Reg loss: 0.026 | Tree loss: 0.874 | Accuracy: 0.621160 | 1.501 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 108 | Batch: 000 / 011 | Total loss: 0.930 | Reg loss: 0.026 | Tree loss: 0.930 | Accuracy: 0.651000 | 1.504 sec/iter\n",
      "Epoch: 108 | Batch: 001 / 011 | Total loss: 0.926 | Reg loss: 0.026 | Tree loss: 0.926 | Accuracy: 0.663500 | 1.504 sec/iter\n",
      "Epoch: 108 | Batch: 002 / 011 | Total loss: 0.895 | Reg loss: 0.026 | Tree loss: 0.895 | Accuracy: 0.664500 | 1.504 sec/iter\n",
      "Epoch: 108 | Batch: 003 / 011 | Total loss: 0.894 | Reg loss: 0.026 | Tree loss: 0.894 | Accuracy: 0.667500 | 1.503 sec/iter\n",
      "Epoch: 108 | Batch: 004 / 011 | Total loss: 0.864 | Reg loss: 0.026 | Tree loss: 0.864 | Accuracy: 0.679000 | 1.503 sec/iter\n",
      "Epoch: 108 | Batch: 005 / 011 | Total loss: 0.857 | Reg loss: 0.026 | Tree loss: 0.857 | Accuracy: 0.664000 | 1.503 sec/iter\n",
      "Epoch: 108 | Batch: 006 / 011 | Total loss: 0.864 | Reg loss: 0.026 | Tree loss: 0.864 | Accuracy: 0.630000 | 1.502 sec/iter\n",
      "Epoch: 108 | Batch: 007 / 011 | Total loss: 0.857 | Reg loss: 0.026 | Tree loss: 0.857 | Accuracy: 0.603000 | 1.502 sec/iter\n",
      "Epoch: 108 | Batch: 008 / 011 | Total loss: 0.848 | Reg loss: 0.026 | Tree loss: 0.848 | Accuracy: 0.617000 | 1.502 sec/iter\n",
      "Epoch: 108 | Batch: 009 / 011 | Total loss: 0.851 | Reg loss: 0.026 | Tree loss: 0.851 | Accuracy: 0.618500 | 1.502 sec/iter\n",
      "Epoch: 108 | Batch: 010 / 011 | Total loss: 0.851 | Reg loss: 0.026 | Tree loss: 0.851 | Accuracy: 0.617747 | 1.501 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 109 | Batch: 000 / 011 | Total loss: 0.919 | Reg loss: 0.026 | Tree loss: 0.919 | Accuracy: 0.666000 | 1.502 sec/iter\n",
      "Epoch: 109 | Batch: 001 / 011 | Total loss: 0.913 | Reg loss: 0.026 | Tree loss: 0.913 | Accuracy: 0.669500 | 1.502 sec/iter\n",
      "Epoch: 109 | Batch: 002 / 011 | Total loss: 0.902 | Reg loss: 0.026 | Tree loss: 0.902 | Accuracy: 0.662500 | 1.501 sec/iter\n",
      "Epoch: 109 | Batch: 003 / 011 | Total loss: 0.893 | Reg loss: 0.026 | Tree loss: 0.893 | Accuracy: 0.670000 | 1.501 sec/iter\n",
      "Epoch: 109 | Batch: 004 / 011 | Total loss: 0.877 | Reg loss: 0.026 | Tree loss: 0.877 | Accuracy: 0.665000 | 1.5 sec/iter\n",
      "Epoch: 109 | Batch: 005 / 011 | Total loss: 0.859 | Reg loss: 0.026 | Tree loss: 0.859 | Accuracy: 0.677000 | 1.5 sec/iter\n",
      "Epoch: 109 | Batch: 006 / 011 | Total loss: 0.858 | Reg loss: 0.026 | Tree loss: 0.858 | Accuracy: 0.645500 | 1.499 sec/iter\n",
      "Epoch: 109 | Batch: 007 / 011 | Total loss: 0.861 | Reg loss: 0.026 | Tree loss: 0.861 | Accuracy: 0.625500 | 1.499 sec/iter\n",
      "Epoch: 109 | Batch: 008 / 011 | Total loss: 0.865 | Reg loss: 0.026 | Tree loss: 0.865 | Accuracy: 0.617000 | 1.499 sec/iter\n",
      "Epoch: 109 | Batch: 009 / 011 | Total loss: 0.849 | Reg loss: 0.026 | Tree loss: 0.849 | Accuracy: 0.591500 | 1.499 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 109 | Batch: 010 / 011 | Total loss: 0.832 | Reg loss: 0.026 | Tree loss: 0.832 | Accuracy: 0.614334 | 1.498 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 110 | Batch: 000 / 011 | Total loss: 0.941 | Reg loss: 0.026 | Tree loss: 0.941 | Accuracy: 0.632000 | 1.502 sec/iter\n",
      "Epoch: 110 | Batch: 001 / 011 | Total loss: 0.904 | Reg loss: 0.026 | Tree loss: 0.904 | Accuracy: 0.667500 | 1.502 sec/iter\n",
      "Epoch: 110 | Batch: 002 / 011 | Total loss: 0.904 | Reg loss: 0.026 | Tree loss: 0.904 | Accuracy: 0.669500 | 1.501 sec/iter\n",
      "Epoch: 110 | Batch: 003 / 011 | Total loss: 0.911 | Reg loss: 0.026 | Tree loss: 0.911 | Accuracy: 0.655500 | 1.501 sec/iter\n",
      "Epoch: 110 | Batch: 004 / 011 | Total loss: 0.862 | Reg loss: 0.026 | Tree loss: 0.862 | Accuracy: 0.675000 | 1.5 sec/iter\n",
      "Epoch: 110 | Batch: 005 / 011 | Total loss: 0.889 | Reg loss: 0.026 | Tree loss: 0.889 | Accuracy: 0.656500 | 1.5 sec/iter\n",
      "Epoch: 110 | Batch: 006 / 011 | Total loss: 0.827 | Reg loss: 0.026 | Tree loss: 0.827 | Accuracy: 0.661500 | 1.5 sec/iter\n",
      "Epoch: 110 | Batch: 007 / 011 | Total loss: 0.848 | Reg loss: 0.026 | Tree loss: 0.848 | Accuracy: 0.636000 | 1.499 sec/iter\n",
      "Epoch: 110 | Batch: 008 / 011 | Total loss: 0.855 | Reg loss: 0.026 | Tree loss: 0.855 | Accuracy: 0.602500 | 1.499 sec/iter\n",
      "Epoch: 110 | Batch: 009 / 011 | Total loss: 0.843 | Reg loss: 0.026 | Tree loss: 0.843 | Accuracy: 0.613500 | 1.499 sec/iter\n",
      "Epoch: 110 | Batch: 010 / 011 | Total loss: 0.821 | Reg loss: 0.026 | Tree loss: 0.821 | Accuracy: 0.648464 | 1.499 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 111 | Batch: 000 / 011 | Total loss: 0.940 | Reg loss: 0.026 | Tree loss: 0.940 | Accuracy: 0.663500 | 1.503 sec/iter\n",
      "Epoch: 111 | Batch: 001 / 011 | Total loss: 0.905 | Reg loss: 0.026 | Tree loss: 0.905 | Accuracy: 0.660000 | 1.503 sec/iter\n",
      "Epoch: 111 | Batch: 002 / 011 | Total loss: 0.897 | Reg loss: 0.026 | Tree loss: 0.897 | Accuracy: 0.662000 | 1.502 sec/iter\n",
      "Epoch: 111 | Batch: 003 / 011 | Total loss: 0.881 | Reg loss: 0.026 | Tree loss: 0.881 | Accuracy: 0.670000 | 1.502 sec/iter\n",
      "Epoch: 111 | Batch: 004 / 011 | Total loss: 0.879 | Reg loss: 0.026 | Tree loss: 0.879 | Accuracy: 0.658000 | 1.502 sec/iter\n",
      "Epoch: 111 | Batch: 005 / 011 | Total loss: 0.860 | Reg loss: 0.026 | Tree loss: 0.860 | Accuracy: 0.645000 | 1.501 sec/iter\n",
      "Epoch: 111 | Batch: 006 / 011 | Total loss: 0.854 | Reg loss: 0.026 | Tree loss: 0.854 | Accuracy: 0.643000 | 1.501 sec/iter\n",
      "Epoch: 111 | Batch: 007 / 011 | Total loss: 0.855 | Reg loss: 0.026 | Tree loss: 0.855 | Accuracy: 0.599500 | 1.501 sec/iter\n",
      "Epoch: 111 | Batch: 008 / 011 | Total loss: 0.842 | Reg loss: 0.026 | Tree loss: 0.842 | Accuracy: 0.620500 | 1.501 sec/iter\n",
      "Epoch: 111 | Batch: 009 / 011 | Total loss: 0.845 | Reg loss: 0.026 | Tree loss: 0.845 | Accuracy: 0.610000 | 1.501 sec/iter\n",
      "Epoch: 111 | Batch: 010 / 011 | Total loss: 0.830 | Reg loss: 0.026 | Tree loss: 0.830 | Accuracy: 0.614334 | 1.5 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 112 | Batch: 000 / 011 | Total loss: 0.939 | Reg loss: 0.026 | Tree loss: 0.939 | Accuracy: 0.652000 | 1.502 sec/iter\n",
      "Epoch: 112 | Batch: 001 / 011 | Total loss: 0.909 | Reg loss: 0.026 | Tree loss: 0.909 | Accuracy: 0.660000 | 1.502 sec/iter\n",
      "Epoch: 112 | Batch: 002 / 011 | Total loss: 0.910 | Reg loss: 0.026 | Tree loss: 0.910 | Accuracy: 0.664500 | 1.502 sec/iter\n",
      "Epoch: 112 | Batch: 003 / 011 | Total loss: 0.904 | Reg loss: 0.026 | Tree loss: 0.904 | Accuracy: 0.661000 | 1.502 sec/iter\n",
      "Epoch: 112 | Batch: 004 / 011 | Total loss: 0.881 | Reg loss: 0.026 | Tree loss: 0.881 | Accuracy: 0.664000 | 1.501 sec/iter\n",
      "Epoch: 112 | Batch: 005 / 011 | Total loss: 0.857 | Reg loss: 0.026 | Tree loss: 0.857 | Accuracy: 0.664500 | 1.501 sec/iter\n",
      "Epoch: 112 | Batch: 006 / 011 | Total loss: 0.845 | Reg loss: 0.026 | Tree loss: 0.845 | Accuracy: 0.646500 | 1.5 sec/iter\n",
      "Epoch: 112 | Batch: 007 / 011 | Total loss: 0.860 | Reg loss: 0.026 | Tree loss: 0.860 | Accuracy: 0.614500 | 1.5 sec/iter\n",
      "Epoch: 112 | Batch: 008 / 011 | Total loss: 0.856 | Reg loss: 0.026 | Tree loss: 0.856 | Accuracy: 0.606000 | 1.5 sec/iter\n",
      "Epoch: 112 | Batch: 009 / 011 | Total loss: 0.824 | Reg loss: 0.026 | Tree loss: 0.824 | Accuracy: 0.625000 | 1.5 sec/iter\n",
      "Epoch: 112 | Batch: 010 / 011 | Total loss: 0.823 | Reg loss: 0.026 | Tree loss: 0.823 | Accuracy: 0.617747 | 1.499 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 113 | Batch: 000 / 011 | Total loss: 0.920 | Reg loss: 0.026 | Tree loss: 0.920 | Accuracy: 0.657500 | 1.503 sec/iter\n",
      "Epoch: 113 | Batch: 001 / 011 | Total loss: 0.929 | Reg loss: 0.026 | Tree loss: 0.929 | Accuracy: 0.646500 | 1.503 sec/iter\n",
      "Epoch: 113 | Batch: 002 / 011 | Total loss: 0.904 | Reg loss: 0.026 | Tree loss: 0.904 | Accuracy: 0.660500 | 1.502 sec/iter\n",
      "Epoch: 113 | Batch: 003 / 011 | Total loss: 0.862 | Reg loss: 0.026 | Tree loss: 0.862 | Accuracy: 0.684500 | 1.502 sec/iter\n",
      "Epoch: 113 | Batch: 004 / 011 | Total loss: 0.859 | Reg loss: 0.026 | Tree loss: 0.859 | Accuracy: 0.686000 | 1.502 sec/iter\n",
      "Epoch: 113 | Batch: 005 / 011 | Total loss: 0.891 | Reg loss: 0.026 | Tree loss: 0.891 | Accuracy: 0.636500 | 1.502 sec/iter\n",
      "Epoch: 113 | Batch: 006 / 011 | Total loss: 0.886 | Reg loss: 0.026 | Tree loss: 0.886 | Accuracy: 0.613500 | 1.501 sec/iter\n",
      "Epoch: 113 | Batch: 007 / 011 | Total loss: 0.834 | Reg loss: 0.026 | Tree loss: 0.834 | Accuracy: 0.630500 | 1.501 sec/iter\n",
      "Epoch: 113 | Batch: 008 / 011 | Total loss: 0.854 | Reg loss: 0.026 | Tree loss: 0.854 | Accuracy: 0.612000 | 1.501 sec/iter\n",
      "Epoch: 113 | Batch: 009 / 011 | Total loss: 0.832 | Reg loss: 0.026 | Tree loss: 0.832 | Accuracy: 0.596500 | 1.501 sec/iter\n",
      "Epoch: 113 | Batch: 010 / 011 | Total loss: 0.778 | Reg loss: 0.026 | Tree loss: 0.778 | Accuracy: 0.641638 | 1.5 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 114 | Batch: 000 / 011 | Total loss: 0.920 | Reg loss: 0.026 | Tree loss: 0.920 | Accuracy: 0.661500 | 1.503 sec/iter\n",
      "Epoch: 114 | Batch: 001 / 011 | Total loss: 0.895 | Reg loss: 0.026 | Tree loss: 0.895 | Accuracy: 0.679000 | 1.503 sec/iter\n",
      "Epoch: 114 | Batch: 002 / 011 | Total loss: 0.897 | Reg loss: 0.026 | Tree loss: 0.897 | Accuracy: 0.658500 | 1.503 sec/iter\n",
      "Epoch: 114 | Batch: 003 / 011 | Total loss: 0.883 | Reg loss: 0.026 | Tree loss: 0.883 | Accuracy: 0.669500 | 1.503 sec/iter\n",
      "Epoch: 114 | Batch: 004 / 011 | Total loss: 0.884 | Reg loss: 0.026 | Tree loss: 0.884 | Accuracy: 0.669000 | 1.503 sec/iter\n",
      "Epoch: 114 | Batch: 005 / 011 | Total loss: 0.879 | Reg loss: 0.026 | Tree loss: 0.879 | Accuracy: 0.663500 | 1.502 sec/iter\n",
      "Epoch: 114 | Batch: 006 / 011 | Total loss: 0.845 | Reg loss: 0.026 | Tree loss: 0.845 | Accuracy: 0.647500 | 1.502 sec/iter\n",
      "Epoch: 114 | Batch: 007 / 011 | Total loss: 0.823 | Reg loss: 0.026 | Tree loss: 0.823 | Accuracy: 0.649000 | 1.501 sec/iter\n",
      "Epoch: 114 | Batch: 008 / 011 | Total loss: 0.854 | Reg loss: 0.026 | Tree loss: 0.854 | Accuracy: 0.613000 | 1.501 sec/iter\n",
      "Epoch: 114 | Batch: 009 / 011 | Total loss: 0.862 | Reg loss: 0.026 | Tree loss: 0.862 | Accuracy: 0.609000 | 1.501 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 114 | Batch: 010 / 011 | Total loss: 0.915 | Reg loss: 0.026 | Tree loss: 0.915 | Accuracy: 0.593857 | 1.501 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 115 | Batch: 000 / 011 | Total loss: 0.940 | Reg loss: 0.026 | Tree loss: 0.940 | Accuracy: 0.632000 | 1.502 sec/iter\n",
      "Epoch: 115 | Batch: 001 / 011 | Total loss: 0.884 | Reg loss: 0.026 | Tree loss: 0.884 | Accuracy: 0.674000 | 1.501 sec/iter\n",
      "Epoch: 115 | Batch: 002 / 011 | Total loss: 0.879 | Reg loss: 0.026 | Tree loss: 0.879 | Accuracy: 0.682500 | 1.501 sec/iter\n",
      "Epoch: 115 | Batch: 003 / 011 | Total loss: 0.868 | Reg loss: 0.026 | Tree loss: 0.868 | Accuracy: 0.676500 | 1.5 sec/iter\n",
      "Epoch: 115 | Batch: 004 / 011 | Total loss: 0.884 | Reg loss: 0.026 | Tree loss: 0.884 | Accuracy: 0.650500 | 1.5 sec/iter\n",
      "Epoch: 115 | Batch: 005 / 011 | Total loss: 0.869 | Reg loss: 0.026 | Tree loss: 0.869 | Accuracy: 0.651500 | 1.5 sec/iter\n",
      "Epoch: 115 | Batch: 006 / 011 | Total loss: 0.869 | Reg loss: 0.026 | Tree loss: 0.869 | Accuracy: 0.614000 | 1.499 sec/iter\n",
      "Epoch: 115 | Batch: 007 / 011 | Total loss: 0.841 | Reg loss: 0.026 | Tree loss: 0.841 | Accuracy: 0.618000 | 1.499 sec/iter\n",
      "Epoch: 115 | Batch: 008 / 011 | Total loss: 0.846 | Reg loss: 0.026 | Tree loss: 0.846 | Accuracy: 0.610000 | 1.499 sec/iter\n",
      "Epoch: 115 | Batch: 009 / 011 | Total loss: 0.860 | Reg loss: 0.026 | Tree loss: 0.860 | Accuracy: 0.607000 | 1.499 sec/iter\n",
      "Epoch: 115 | Batch: 010 / 011 | Total loss: 0.879 | Reg loss: 0.026 | Tree loss: 0.879 | Accuracy: 0.651877 | 1.498 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 116 | Batch: 000 / 011 | Total loss: 0.907 | Reg loss: 0.026 | Tree loss: 0.907 | Accuracy: 0.660500 | 1.502 sec/iter\n",
      "Epoch: 116 | Batch: 001 / 011 | Total loss: 0.903 | Reg loss: 0.026 | Tree loss: 0.903 | Accuracy: 0.670500 | 1.501 sec/iter\n",
      "Epoch: 116 | Batch: 002 / 011 | Total loss: 0.912 | Reg loss: 0.026 | Tree loss: 0.912 | Accuracy: 0.650500 | 1.501 sec/iter\n",
      "Epoch: 116 | Batch: 003 / 011 | Total loss: 0.875 | Reg loss: 0.026 | Tree loss: 0.875 | Accuracy: 0.677000 | 1.501 sec/iter\n",
      "Epoch: 116 | Batch: 004 / 011 | Total loss: 0.870 | Reg loss: 0.026 | Tree loss: 0.870 | Accuracy: 0.658000 | 1.5 sec/iter\n",
      "Epoch: 116 | Batch: 005 / 011 | Total loss: 0.867 | Reg loss: 0.026 | Tree loss: 0.867 | Accuracy: 0.657500 | 1.5 sec/iter\n",
      "Epoch: 116 | Batch: 006 / 011 | Total loss: 0.861 | Reg loss: 0.026 | Tree loss: 0.861 | Accuracy: 0.631000 | 1.5 sec/iter\n",
      "Epoch: 116 | Batch: 007 / 011 | Total loss: 0.846 | Reg loss: 0.026 | Tree loss: 0.846 | Accuracy: 0.619500 | 1.5 sec/iter\n",
      "Epoch: 116 | Batch: 008 / 011 | Total loss: 0.860 | Reg loss: 0.026 | Tree loss: 0.860 | Accuracy: 0.604000 | 1.499 sec/iter\n",
      "Epoch: 116 | Batch: 009 / 011 | Total loss: 0.849 | Reg loss: 0.026 | Tree loss: 0.849 | Accuracy: 0.597500 | 1.499 sec/iter\n",
      "Epoch: 116 | Batch: 010 / 011 | Total loss: 0.836 | Reg loss: 0.026 | Tree loss: 0.836 | Accuracy: 0.600683 | 1.499 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 117 | Batch: 000 / 011 | Total loss: 0.937 | Reg loss: 0.026 | Tree loss: 0.937 | Accuracy: 0.655500 | 1.502 sec/iter\n",
      "Epoch: 117 | Batch: 001 / 011 | Total loss: 0.911 | Reg loss: 0.026 | Tree loss: 0.911 | Accuracy: 0.664500 | 1.502 sec/iter\n",
      "Epoch: 117 | Batch: 002 / 011 | Total loss: 0.895 | Reg loss: 0.026 | Tree loss: 0.895 | Accuracy: 0.667500 | 1.501 sec/iter\n",
      "Epoch: 117 | Batch: 003 / 011 | Total loss: 0.875 | Reg loss: 0.026 | Tree loss: 0.875 | Accuracy: 0.675000 | 1.501 sec/iter\n",
      "Epoch: 117 | Batch: 004 / 011 | Total loss: 0.860 | Reg loss: 0.026 | Tree loss: 0.860 | Accuracy: 0.670500 | 1.501 sec/iter\n",
      "Epoch: 117 | Batch: 005 / 011 | Total loss: 0.869 | Reg loss: 0.026 | Tree loss: 0.869 | Accuracy: 0.652500 | 1.5 sec/iter\n",
      "Epoch: 117 | Batch: 006 / 011 | Total loss: 0.839 | Reg loss: 0.026 | Tree loss: 0.839 | Accuracy: 0.655000 | 1.5 sec/iter\n",
      "Epoch: 117 | Batch: 007 / 011 | Total loss: 0.856 | Reg loss: 0.026 | Tree loss: 0.856 | Accuracy: 0.611000 | 1.5 sec/iter\n",
      "Epoch: 117 | Batch: 008 / 011 | Total loss: 0.882 | Reg loss: 0.026 | Tree loss: 0.882 | Accuracy: 0.590500 | 1.5 sec/iter\n",
      "Epoch: 117 | Batch: 009 / 011 | Total loss: 0.849 | Reg loss: 0.026 | Tree loss: 0.849 | Accuracy: 0.611000 | 1.5 sec/iter\n",
      "Epoch: 117 | Batch: 010 / 011 | Total loss: 0.851 | Reg loss: 0.026 | Tree loss: 0.851 | Accuracy: 0.600683 | 1.499 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 118 | Batch: 000 / 011 | Total loss: 0.947 | Reg loss: 0.026 | Tree loss: 0.947 | Accuracy: 0.644000 | 1.503 sec/iter\n",
      "Epoch: 118 | Batch: 001 / 011 | Total loss: 0.916 | Reg loss: 0.026 | Tree loss: 0.916 | Accuracy: 0.648500 | 1.503 sec/iter\n",
      "Epoch: 118 | Batch: 002 / 011 | Total loss: 0.888 | Reg loss: 0.026 | Tree loss: 0.888 | Accuracy: 0.661500 | 1.502 sec/iter\n",
      "Epoch: 118 | Batch: 003 / 011 | Total loss: 0.884 | Reg loss: 0.026 | Tree loss: 0.884 | Accuracy: 0.658000 | 1.502 sec/iter\n",
      "Epoch: 118 | Batch: 004 / 011 | Total loss: 0.886 | Reg loss: 0.026 | Tree loss: 0.886 | Accuracy: 0.666000 | 1.502 sec/iter\n",
      "Epoch: 118 | Batch: 005 / 011 | Total loss: 0.843 | Reg loss: 0.026 | Tree loss: 0.843 | Accuracy: 0.677000 | 1.501 sec/iter\n",
      "Epoch: 118 | Batch: 006 / 011 | Total loss: 0.850 | Reg loss: 0.026 | Tree loss: 0.850 | Accuracy: 0.650000 | 1.501 sec/iter\n",
      "Epoch: 118 | Batch: 007 / 011 | Total loss: 0.822 | Reg loss: 0.026 | Tree loss: 0.822 | Accuracy: 0.641000 | 1.501 sec/iter\n",
      "Epoch: 118 | Batch: 008 / 011 | Total loss: 0.859 | Reg loss: 0.026 | Tree loss: 0.859 | Accuracy: 0.600000 | 1.501 sec/iter\n",
      "Epoch: 118 | Batch: 009 / 011 | Total loss: 0.839 | Reg loss: 0.026 | Tree loss: 0.839 | Accuracy: 0.608000 | 1.501 sec/iter\n",
      "Epoch: 118 | Batch: 010 / 011 | Total loss: 0.807 | Reg loss: 0.026 | Tree loss: 0.807 | Accuracy: 0.634812 | 1.5 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 119 | Batch: 000 / 011 | Total loss: 0.923 | Reg loss: 0.026 | Tree loss: 0.923 | Accuracy: 0.656500 | 1.503 sec/iter\n",
      "Epoch: 119 | Batch: 001 / 011 | Total loss: 0.911 | Reg loss: 0.026 | Tree loss: 0.911 | Accuracy: 0.640000 | 1.502 sec/iter\n",
      "Epoch: 119 | Batch: 002 / 011 | Total loss: 0.880 | Reg loss: 0.026 | Tree loss: 0.880 | Accuracy: 0.671000 | 1.502 sec/iter\n",
      "Epoch: 119 | Batch: 003 / 011 | Total loss: 0.862 | Reg loss: 0.026 | Tree loss: 0.862 | Accuracy: 0.683500 | 1.502 sec/iter\n",
      "Epoch: 119 | Batch: 004 / 011 | Total loss: 0.886 | Reg loss: 0.026 | Tree loss: 0.886 | Accuracy: 0.644000 | 1.502 sec/iter\n",
      "Epoch: 119 | Batch: 005 / 011 | Total loss: 0.840 | Reg loss: 0.026 | Tree loss: 0.840 | Accuracy: 0.674000 | 1.501 sec/iter\n",
      "Epoch: 119 | Batch: 006 / 011 | Total loss: 0.860 | Reg loss: 0.026 | Tree loss: 0.860 | Accuracy: 0.624500 | 1.501 sec/iter\n",
      "Epoch: 119 | Batch: 007 / 011 | Total loss: 0.862 | Reg loss: 0.026 | Tree loss: 0.862 | Accuracy: 0.608500 | 1.501 sec/iter\n",
      "Epoch: 119 | Batch: 008 / 011 | Total loss: 0.848 | Reg loss: 0.026 | Tree loss: 0.848 | Accuracy: 0.606500 | 1.501 sec/iter\n",
      "Epoch: 119 | Batch: 009 / 011 | Total loss: 0.850 | Reg loss: 0.026 | Tree loss: 0.850 | Accuracy: 0.611000 | 1.501 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 119 | Batch: 010 / 011 | Total loss: 0.857 | Reg loss: 0.026 | Tree loss: 0.857 | Accuracy: 0.624573 | 1.5 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 120 | Batch: 000 / 011 | Total loss: 0.908 | Reg loss: 0.026 | Tree loss: 0.908 | Accuracy: 0.664500 | 1.503 sec/iter\n",
      "Epoch: 120 | Batch: 001 / 011 | Total loss: 0.909 | Reg loss: 0.026 | Tree loss: 0.909 | Accuracy: 0.662500 | 1.503 sec/iter\n",
      "Epoch: 120 | Batch: 002 / 011 | Total loss: 0.915 | Reg loss: 0.026 | Tree loss: 0.915 | Accuracy: 0.648500 | 1.502 sec/iter\n",
      "Epoch: 120 | Batch: 003 / 011 | Total loss: 0.917 | Reg loss: 0.026 | Tree loss: 0.917 | Accuracy: 0.643500 | 1.502 sec/iter\n",
      "Epoch: 120 | Batch: 004 / 011 | Total loss: 0.855 | Reg loss: 0.026 | Tree loss: 0.855 | Accuracy: 0.679500 | 1.502 sec/iter\n",
      "Epoch: 120 | Batch: 005 / 011 | Total loss: 0.862 | Reg loss: 0.026 | Tree loss: 0.862 | Accuracy: 0.670000 | 1.502 sec/iter\n",
      "Epoch: 120 | Batch: 006 / 011 | Total loss: 0.839 | Reg loss: 0.026 | Tree loss: 0.839 | Accuracy: 0.638000 | 1.501 sec/iter\n",
      "Epoch: 120 | Batch: 007 / 011 | Total loss: 0.831 | Reg loss: 0.026 | Tree loss: 0.831 | Accuracy: 0.630500 | 1.501 sec/iter\n",
      "Epoch: 120 | Batch: 008 / 011 | Total loss: 0.837 | Reg loss: 0.026 | Tree loss: 0.837 | Accuracy: 0.614000 | 1.501 sec/iter\n",
      "Epoch: 120 | Batch: 009 / 011 | Total loss: 0.853 | Reg loss: 0.026 | Tree loss: 0.853 | Accuracy: 0.589000 | 1.5 sec/iter\n",
      "Epoch: 120 | Batch: 010 / 011 | Total loss: 0.869 | Reg loss: 0.026 | Tree loss: 0.869 | Accuracy: 0.580205 | 1.5 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 121 | Batch: 000 / 011 | Total loss: 0.916 | Reg loss: 0.026 | Tree loss: 0.916 | Accuracy: 0.662500 | 1.501 sec/iter\n",
      "Epoch: 121 | Batch: 001 / 011 | Total loss: 0.935 | Reg loss: 0.026 | Tree loss: 0.935 | Accuracy: 0.629000 | 1.501 sec/iter\n",
      "Epoch: 121 | Batch: 002 / 011 | Total loss: 0.903 | Reg loss: 0.026 | Tree loss: 0.903 | Accuracy: 0.654500 | 1.501 sec/iter\n",
      "Epoch: 121 | Batch: 003 / 011 | Total loss: 0.892 | Reg loss: 0.026 | Tree loss: 0.892 | Accuracy: 0.666000 | 1.5 sec/iter\n",
      "Epoch: 121 | Batch: 004 / 011 | Total loss: 0.842 | Reg loss: 0.026 | Tree loss: 0.842 | Accuracy: 0.679000 | 1.5 sec/iter\n",
      "Epoch: 121 | Batch: 005 / 011 | Total loss: 0.849 | Reg loss: 0.026 | Tree loss: 0.849 | Accuracy: 0.657000 | 1.499 sec/iter\n",
      "Epoch: 121 | Batch: 006 / 011 | Total loss: 0.865 | Reg loss: 0.026 | Tree loss: 0.865 | Accuracy: 0.614500 | 1.499 sec/iter\n",
      "Epoch: 121 | Batch: 007 / 011 | Total loss: 0.842 | Reg loss: 0.026 | Tree loss: 0.842 | Accuracy: 0.624000 | 1.499 sec/iter\n",
      "Epoch: 121 | Batch: 008 / 011 | Total loss: 0.842 | Reg loss: 0.026 | Tree loss: 0.842 | Accuracy: 0.607500 | 1.499 sec/iter\n",
      "Epoch: 121 | Batch: 009 / 011 | Total loss: 0.838 | Reg loss: 0.026 | Tree loss: 0.838 | Accuracy: 0.607000 | 1.499 sec/iter\n",
      "Epoch: 121 | Batch: 010 / 011 | Total loss: 0.853 | Reg loss: 0.026 | Tree loss: 0.853 | Accuracy: 0.624573 | 1.498 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 122 | Batch: 000 / 011 | Total loss: 0.912 | Reg loss: 0.026 | Tree loss: 0.912 | Accuracy: 0.648500 | 1.502 sec/iter\n",
      "Epoch: 122 | Batch: 001 / 011 | Total loss: 0.907 | Reg loss: 0.026 | Tree loss: 0.907 | Accuracy: 0.657000 | 1.502 sec/iter\n",
      "Epoch: 122 | Batch: 002 / 011 | Total loss: 0.902 | Reg loss: 0.026 | Tree loss: 0.902 | Accuracy: 0.660000 | 1.501 sec/iter\n",
      "Epoch: 122 | Batch: 003 / 011 | Total loss: 0.865 | Reg loss: 0.026 | Tree loss: 0.865 | Accuracy: 0.681500 | 1.501 sec/iter\n",
      "Epoch: 122 | Batch: 004 / 011 | Total loss: 0.871 | Reg loss: 0.026 | Tree loss: 0.871 | Accuracy: 0.677000 | 1.501 sec/iter\n",
      "Epoch: 122 | Batch: 005 / 011 | Total loss: 0.877 | Reg loss: 0.026 | Tree loss: 0.877 | Accuracy: 0.649000 | 1.5 sec/iter\n",
      "Epoch: 122 | Batch: 006 / 011 | Total loss: 0.848 | Reg loss: 0.026 | Tree loss: 0.848 | Accuracy: 0.640500 | 1.5 sec/iter\n",
      "Epoch: 122 | Batch: 007 / 011 | Total loss: 0.840 | Reg loss: 0.026 | Tree loss: 0.840 | Accuracy: 0.611500 | 1.5 sec/iter\n",
      "Epoch: 122 | Batch: 008 / 011 | Total loss: 0.841 | Reg loss: 0.026 | Tree loss: 0.841 | Accuracy: 0.606000 | 1.5 sec/iter\n",
      "Epoch: 122 | Batch: 009 / 011 | Total loss: 0.856 | Reg loss: 0.026 | Tree loss: 0.856 | Accuracy: 0.583500 | 1.5 sec/iter\n",
      "Epoch: 122 | Batch: 010 / 011 | Total loss: 0.866 | Reg loss: 0.026 | Tree loss: 0.866 | Accuracy: 0.607509 | 1.499 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 123 | Batch: 000 / 011 | Total loss: 0.922 | Reg loss: 0.026 | Tree loss: 0.922 | Accuracy: 0.645000 | 1.501 sec/iter\n",
      "Epoch: 123 | Batch: 001 / 011 | Total loss: 0.924 | Reg loss: 0.026 | Tree loss: 0.924 | Accuracy: 0.633000 | 1.501 sec/iter\n",
      "Epoch: 123 | Batch: 002 / 011 | Total loss: 0.873 | Reg loss: 0.026 | Tree loss: 0.873 | Accuracy: 0.676500 | 1.501 sec/iter\n",
      "Epoch: 123 | Batch: 003 / 011 | Total loss: 0.878 | Reg loss: 0.026 | Tree loss: 0.878 | Accuracy: 0.656500 | 1.501 sec/iter\n",
      "Epoch: 123 | Batch: 004 / 011 | Total loss: 0.882 | Reg loss: 0.026 | Tree loss: 0.882 | Accuracy: 0.666500 | 1.5 sec/iter\n",
      "Epoch: 123 | Batch: 005 / 011 | Total loss: 0.829 | Reg loss: 0.026 | Tree loss: 0.829 | Accuracy: 0.688000 | 1.5 sec/iter\n",
      "Epoch: 123 | Batch: 006 / 011 | Total loss: 0.868 | Reg loss: 0.026 | Tree loss: 0.868 | Accuracy: 0.630000 | 1.5 sec/iter\n",
      "Epoch: 123 | Batch: 007 / 011 | Total loss: 0.833 | Reg loss: 0.026 | Tree loss: 0.833 | Accuracy: 0.641500 | 1.5 sec/iter\n",
      "Epoch: 123 | Batch: 008 / 011 | Total loss: 0.867 | Reg loss: 0.026 | Tree loss: 0.867 | Accuracy: 0.586000 | 1.5 sec/iter\n",
      "Epoch: 123 | Batch: 009 / 011 | Total loss: 0.844 | Reg loss: 0.026 | Tree loss: 0.844 | Accuracy: 0.607000 | 1.5 sec/iter\n",
      "Epoch: 123 | Batch: 010 / 011 | Total loss: 0.865 | Reg loss: 0.026 | Tree loss: 0.865 | Accuracy: 0.576792 | 1.499 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 124 | Batch: 000 / 011 | Total loss: 0.929 | Reg loss: 0.026 | Tree loss: 0.929 | Accuracy: 0.634500 | 1.502 sec/iter\n",
      "Epoch: 124 | Batch: 001 / 011 | Total loss: 0.902 | Reg loss: 0.026 | Tree loss: 0.902 | Accuracy: 0.666500 | 1.502 sec/iter\n",
      "Epoch: 124 | Batch: 002 / 011 | Total loss: 0.907 | Reg loss: 0.026 | Tree loss: 0.907 | Accuracy: 0.660000 | 1.501 sec/iter\n",
      "Epoch: 124 | Batch: 003 / 011 | Total loss: 0.886 | Reg loss: 0.026 | Tree loss: 0.886 | Accuracy: 0.660500 | 1.501 sec/iter\n",
      "Epoch: 124 | Batch: 004 / 011 | Total loss: 0.856 | Reg loss: 0.026 | Tree loss: 0.856 | Accuracy: 0.679000 | 1.501 sec/iter\n",
      "Epoch: 124 | Batch: 005 / 011 | Total loss: 0.857 | Reg loss: 0.026 | Tree loss: 0.857 | Accuracy: 0.656000 | 1.5 sec/iter\n",
      "Epoch: 124 | Batch: 006 / 011 | Total loss: 0.831 | Reg loss: 0.026 | Tree loss: 0.831 | Accuracy: 0.651500 | 1.5 sec/iter\n",
      "Epoch: 124 | Batch: 007 / 011 | Total loss: 0.849 | Reg loss: 0.026 | Tree loss: 0.849 | Accuracy: 0.617500 | 1.5 sec/iter\n",
      "Epoch: 124 | Batch: 008 / 011 | Total loss: 0.856 | Reg loss: 0.026 | Tree loss: 0.856 | Accuracy: 0.603000 | 1.499 sec/iter\n",
      "Epoch: 124 | Batch: 009 / 011 | Total loss: 0.831 | Reg loss: 0.026 | Tree loss: 0.831 | Accuracy: 0.614000 | 1.499 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 124 | Batch: 010 / 011 | Total loss: 0.906 | Reg loss: 0.026 | Tree loss: 0.906 | Accuracy: 0.549488 | 1.499 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 125 | Batch: 000 / 011 | Total loss: 0.930 | Reg loss: 0.026 | Tree loss: 0.930 | Accuracy: 0.635500 | 1.503 sec/iter\n",
      "Epoch: 125 | Batch: 001 / 011 | Total loss: 0.895 | Reg loss: 0.026 | Tree loss: 0.895 | Accuracy: 0.654000 | 1.502 sec/iter\n",
      "Epoch: 125 | Batch: 002 / 011 | Total loss: 0.894 | Reg loss: 0.026 | Tree loss: 0.894 | Accuracy: 0.666500 | 1.502 sec/iter\n",
      "Epoch: 125 | Batch: 003 / 011 | Total loss: 0.899 | Reg loss: 0.026 | Tree loss: 0.899 | Accuracy: 0.655500 | 1.502 sec/iter\n",
      "Epoch: 125 | Batch: 004 / 011 | Total loss: 0.865 | Reg loss: 0.026 | Tree loss: 0.865 | Accuracy: 0.667000 | 1.501 sec/iter\n",
      "Epoch: 125 | Batch: 005 / 011 | Total loss: 0.846 | Reg loss: 0.026 | Tree loss: 0.846 | Accuracy: 0.674000 | 1.501 sec/iter\n",
      "Epoch: 125 | Batch: 006 / 011 | Total loss: 0.836 | Reg loss: 0.026 | Tree loss: 0.836 | Accuracy: 0.653500 | 1.501 sec/iter\n",
      "Epoch: 125 | Batch: 007 / 011 | Total loss: 0.868 | Reg loss: 0.026 | Tree loss: 0.868 | Accuracy: 0.592500 | 1.501 sec/iter\n",
      "Epoch: 125 | Batch: 008 / 011 | Total loss: 0.838 | Reg loss: 0.026 | Tree loss: 0.838 | Accuracy: 0.601000 | 1.501 sec/iter\n",
      "Epoch: 125 | Batch: 009 / 011 | Total loss: 0.817 | Reg loss: 0.026 | Tree loss: 0.817 | Accuracy: 0.627500 | 1.5 sec/iter\n",
      "Epoch: 125 | Batch: 010 / 011 | Total loss: 0.860 | Reg loss: 0.026 | Tree loss: 0.860 | Accuracy: 0.621160 | 1.5 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 126 | Batch: 000 / 011 | Total loss: 0.906 | Reg loss: 0.026 | Tree loss: 0.906 | Accuracy: 0.654000 | 1.503 sec/iter\n",
      "Epoch: 126 | Batch: 001 / 011 | Total loss: 0.902 | Reg loss: 0.026 | Tree loss: 0.902 | Accuracy: 0.666500 | 1.503 sec/iter\n",
      "Epoch: 126 | Batch: 002 / 011 | Total loss: 0.898 | Reg loss: 0.026 | Tree loss: 0.898 | Accuracy: 0.652000 | 1.503 sec/iter\n",
      "Epoch: 126 | Batch: 003 / 011 | Total loss: 0.896 | Reg loss: 0.026 | Tree loss: 0.896 | Accuracy: 0.657000 | 1.502 sec/iter\n",
      "Epoch: 126 | Batch: 004 / 011 | Total loss: 0.872 | Reg loss: 0.026 | Tree loss: 0.872 | Accuracy: 0.667000 | 1.502 sec/iter\n",
      "Epoch: 126 | Batch: 005 / 011 | Total loss: 0.841 | Reg loss: 0.026 | Tree loss: 0.841 | Accuracy: 0.676500 | 1.502 sec/iter\n",
      "Epoch: 126 | Batch: 006 / 011 | Total loss: 0.830 | Reg loss: 0.026 | Tree loss: 0.830 | Accuracy: 0.641500 | 1.501 sec/iter\n",
      "Epoch: 126 | Batch: 007 / 011 | Total loss: 0.844 | Reg loss: 0.026 | Tree loss: 0.844 | Accuracy: 0.622000 | 1.501 sec/iter\n",
      "Epoch: 126 | Batch: 008 / 011 | Total loss: 0.857 | Reg loss: 0.026 | Tree loss: 0.857 | Accuracy: 0.604000 | 1.501 sec/iter\n",
      "Epoch: 126 | Batch: 009 / 011 | Total loss: 0.856 | Reg loss: 0.026 | Tree loss: 0.856 | Accuracy: 0.600500 | 1.501 sec/iter\n",
      "Epoch: 126 | Batch: 010 / 011 | Total loss: 0.796 | Reg loss: 0.026 | Tree loss: 0.796 | Accuracy: 0.627986 | 1.5 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 127 | Batch: 000 / 011 | Total loss: 0.923 | Reg loss: 0.026 | Tree loss: 0.923 | Accuracy: 0.650000 | 1.501 sec/iter\n",
      "Epoch: 127 | Batch: 001 / 011 | Total loss: 0.892 | Reg loss: 0.026 | Tree loss: 0.892 | Accuracy: 0.664500 | 1.501 sec/iter\n",
      "Epoch: 127 | Batch: 002 / 011 | Total loss: 0.899 | Reg loss: 0.026 | Tree loss: 0.899 | Accuracy: 0.669500 | 1.501 sec/iter\n",
      "Epoch: 127 | Batch: 003 / 011 | Total loss: 0.892 | Reg loss: 0.026 | Tree loss: 0.892 | Accuracy: 0.658000 | 1.5 sec/iter\n",
      "Epoch: 127 | Batch: 004 / 011 | Total loss: 0.860 | Reg loss: 0.026 | Tree loss: 0.860 | Accuracy: 0.663000 | 1.5 sec/iter\n",
      "Epoch: 127 | Batch: 005 / 011 | Total loss: 0.836 | Reg loss: 0.026 | Tree loss: 0.836 | Accuracy: 0.654000 | 1.499 sec/iter\n",
      "Epoch: 127 | Batch: 006 / 011 | Total loss: 0.838 | Reg loss: 0.026 | Tree loss: 0.838 | Accuracy: 0.636000 | 1.499 sec/iter\n",
      "Epoch: 127 | Batch: 007 / 011 | Total loss: 0.841 | Reg loss: 0.026 | Tree loss: 0.841 | Accuracy: 0.628500 | 1.499 sec/iter\n",
      "Epoch: 127 | Batch: 008 / 011 | Total loss: 0.838 | Reg loss: 0.026 | Tree loss: 0.838 | Accuracy: 0.608500 | 1.499 sec/iter\n",
      "Epoch: 127 | Batch: 009 / 011 | Total loss: 0.854 | Reg loss: 0.026 | Tree loss: 0.854 | Accuracy: 0.599500 | 1.498 sec/iter\n",
      "Epoch: 127 | Batch: 010 / 011 | Total loss: 0.861 | Reg loss: 0.026 | Tree loss: 0.861 | Accuracy: 0.624573 | 1.498 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 128 | Batch: 000 / 011 | Total loss: 0.919 | Reg loss: 0.026 | Tree loss: 0.919 | Accuracy: 0.648500 | 1.501 sec/iter\n",
      "Epoch: 128 | Batch: 001 / 011 | Total loss: 0.901 | Reg loss: 0.026 | Tree loss: 0.901 | Accuracy: 0.651500 | 1.501 sec/iter\n",
      "Epoch: 128 | Batch: 002 / 011 | Total loss: 0.905 | Reg loss: 0.026 | Tree loss: 0.905 | Accuracy: 0.658500 | 1.501 sec/iter\n",
      "Epoch: 128 | Batch: 003 / 011 | Total loss: 0.899 | Reg loss: 0.026 | Tree loss: 0.899 | Accuracy: 0.658000 | 1.5 sec/iter\n",
      "Epoch: 128 | Batch: 004 / 011 | Total loss: 0.884 | Reg loss: 0.026 | Tree loss: 0.884 | Accuracy: 0.660500 | 1.5 sec/iter\n",
      "Epoch: 128 | Batch: 005 / 011 | Total loss: 0.870 | Reg loss: 0.026 | Tree loss: 0.870 | Accuracy: 0.655500 | 1.5 sec/iter\n",
      "Epoch: 128 | Batch: 006 / 011 | Total loss: 0.852 | Reg loss: 0.026 | Tree loss: 0.852 | Accuracy: 0.636000 | 1.499 sec/iter\n",
      "Epoch: 128 | Batch: 007 / 011 | Total loss: 0.832 | Reg loss: 0.026 | Tree loss: 0.832 | Accuracy: 0.647500 | 1.499 sec/iter\n",
      "Epoch: 128 | Batch: 008 / 011 | Total loss: 0.822 | Reg loss: 0.026 | Tree loss: 0.822 | Accuracy: 0.605000 | 1.499 sec/iter\n",
      "Epoch: 128 | Batch: 009 / 011 | Total loss: 0.827 | Reg loss: 0.026 | Tree loss: 0.827 | Accuracy: 0.610500 | 1.499 sec/iter\n",
      "Epoch: 128 | Batch: 010 / 011 | Total loss: 0.766 | Reg loss: 0.026 | Tree loss: 0.766 | Accuracy: 0.645051 | 1.499 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 129 | Batch: 000 / 011 | Total loss: 0.905 | Reg loss: 0.026 | Tree loss: 0.905 | Accuracy: 0.659500 | 1.502 sec/iter\n",
      "Epoch: 129 | Batch: 001 / 011 | Total loss: 0.902 | Reg loss: 0.026 | Tree loss: 0.902 | Accuracy: 0.657000 | 1.501 sec/iter\n",
      "Epoch: 129 | Batch: 002 / 011 | Total loss: 0.897 | Reg loss: 0.026 | Tree loss: 0.897 | Accuracy: 0.652000 | 1.501 sec/iter\n",
      "Epoch: 129 | Batch: 003 / 011 | Total loss: 0.890 | Reg loss: 0.026 | Tree loss: 0.890 | Accuracy: 0.670000 | 1.501 sec/iter\n",
      "Epoch: 129 | Batch: 004 / 011 | Total loss: 0.860 | Reg loss: 0.026 | Tree loss: 0.860 | Accuracy: 0.659000 | 1.5 sec/iter\n",
      "Epoch: 129 | Batch: 005 / 011 | Total loss: 0.848 | Reg loss: 0.026 | Tree loss: 0.848 | Accuracy: 0.676500 | 1.5 sec/iter\n",
      "Epoch: 129 | Batch: 006 / 011 | Total loss: 0.845 | Reg loss: 0.026 | Tree loss: 0.845 | Accuracy: 0.640000 | 1.5 sec/iter\n",
      "Epoch: 129 | Batch: 007 / 011 | Total loss: 0.843 | Reg loss: 0.026 | Tree loss: 0.843 | Accuracy: 0.632000 | 1.499 sec/iter\n",
      "Epoch: 129 | Batch: 008 / 011 | Total loss: 0.841 | Reg loss: 0.026 | Tree loss: 0.841 | Accuracy: 0.604000 | 1.5 sec/iter\n",
      "Epoch: 129 | Batch: 009 / 011 | Total loss: 0.838 | Reg loss: 0.026 | Tree loss: 0.838 | Accuracy: 0.617500 | 1.499 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 129 | Batch: 010 / 011 | Total loss: 0.840 | Reg loss: 0.026 | Tree loss: 0.840 | Accuracy: 0.607509 | 1.499 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 130 | Batch: 000 / 011 | Total loss: 0.928 | Reg loss: 0.026 | Tree loss: 0.928 | Accuracy: 0.642000 | 1.502 sec/iter\n",
      "Epoch: 130 | Batch: 001 / 011 | Total loss: 0.888 | Reg loss: 0.026 | Tree loss: 0.888 | Accuracy: 0.665000 | 1.501 sec/iter\n",
      "Epoch: 130 | Batch: 002 / 011 | Total loss: 0.922 | Reg loss: 0.026 | Tree loss: 0.922 | Accuracy: 0.644000 | 1.501 sec/iter\n",
      "Epoch: 130 | Batch: 003 / 011 | Total loss: 0.880 | Reg loss: 0.026 | Tree loss: 0.880 | Accuracy: 0.685500 | 1.501 sec/iter\n",
      "Epoch: 130 | Batch: 004 / 011 | Total loss: 0.860 | Reg loss: 0.026 | Tree loss: 0.860 | Accuracy: 0.660000 | 1.501 sec/iter\n",
      "Epoch: 130 | Batch: 005 / 011 | Total loss: 0.839 | Reg loss: 0.026 | Tree loss: 0.839 | Accuracy: 0.668000 | 1.5 sec/iter\n",
      "Epoch: 130 | Batch: 006 / 011 | Total loss: 0.854 | Reg loss: 0.026 | Tree loss: 0.854 | Accuracy: 0.626500 | 1.5 sec/iter\n",
      "Epoch: 130 | Batch: 007 / 011 | Total loss: 0.839 | Reg loss: 0.026 | Tree loss: 0.839 | Accuracy: 0.611000 | 1.5 sec/iter\n",
      "Epoch: 130 | Batch: 008 / 011 | Total loss: 0.836 | Reg loss: 0.026 | Tree loss: 0.836 | Accuracy: 0.619500 | 1.5 sec/iter\n",
      "Epoch: 130 | Batch: 009 / 011 | Total loss: 0.837 | Reg loss: 0.026 | Tree loss: 0.837 | Accuracy: 0.610500 | 1.499 sec/iter\n",
      "Epoch: 130 | Batch: 010 / 011 | Total loss: 0.763 | Reg loss: 0.026 | Tree loss: 0.763 | Accuracy: 0.689420 | 1.499 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 131 | Batch: 000 / 011 | Total loss: 0.923 | Reg loss: 0.026 | Tree loss: 0.923 | Accuracy: 0.646000 | 1.502 sec/iter\n",
      "Epoch: 131 | Batch: 001 / 011 | Total loss: 0.912 | Reg loss: 0.026 | Tree loss: 0.912 | Accuracy: 0.643000 | 1.502 sec/iter\n",
      "Epoch: 131 | Batch: 002 / 011 | Total loss: 0.896 | Reg loss: 0.026 | Tree loss: 0.896 | Accuracy: 0.671000 | 1.502 sec/iter\n",
      "Epoch: 131 | Batch: 003 / 011 | Total loss: 0.857 | Reg loss: 0.026 | Tree loss: 0.857 | Accuracy: 0.678500 | 1.502 sec/iter\n",
      "Epoch: 131 | Batch: 004 / 011 | Total loss: 0.865 | Reg loss: 0.026 | Tree loss: 0.865 | Accuracy: 0.658500 | 1.501 sec/iter\n",
      "Epoch: 131 | Batch: 005 / 011 | Total loss: 0.838 | Reg loss: 0.026 | Tree loss: 0.838 | Accuracy: 0.669000 | 1.501 sec/iter\n",
      "Epoch: 131 | Batch: 006 / 011 | Total loss: 0.855 | Reg loss: 0.026 | Tree loss: 0.855 | Accuracy: 0.637500 | 1.5 sec/iter\n",
      "Epoch: 131 | Batch: 007 / 011 | Total loss: 0.836 | Reg loss: 0.026 | Tree loss: 0.836 | Accuracy: 0.634500 | 1.5 sec/iter\n",
      "Epoch: 131 | Batch: 008 / 011 | Total loss: 0.826 | Reg loss: 0.026 | Tree loss: 0.826 | Accuracy: 0.622000 | 1.5 sec/iter\n",
      "Epoch: 131 | Batch: 009 / 011 | Total loss: 0.852 | Reg loss: 0.026 | Tree loss: 0.852 | Accuracy: 0.588000 | 1.5 sec/iter\n",
      "Epoch: 131 | Batch: 010 / 011 | Total loss: 0.855 | Reg loss: 0.026 | Tree loss: 0.855 | Accuracy: 0.600683 | 1.5 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 132 | Batch: 000 / 011 | Total loss: 0.901 | Reg loss: 0.026 | Tree loss: 0.901 | Accuracy: 0.653500 | 1.502 sec/iter\n",
      "Epoch: 132 | Batch: 001 / 011 | Total loss: 0.904 | Reg loss: 0.026 | Tree loss: 0.904 | Accuracy: 0.649000 | 1.502 sec/iter\n",
      "Epoch: 132 | Batch: 002 / 011 | Total loss: 0.888 | Reg loss: 0.026 | Tree loss: 0.888 | Accuracy: 0.664500 | 1.502 sec/iter\n",
      "Epoch: 132 | Batch: 003 / 011 | Total loss: 0.879 | Reg loss: 0.026 | Tree loss: 0.879 | Accuracy: 0.671000 | 1.502 sec/iter\n",
      "Epoch: 132 | Batch: 004 / 011 | Total loss: 0.864 | Reg loss: 0.026 | Tree loss: 0.864 | Accuracy: 0.662500 | 1.502 sec/iter\n",
      "Epoch: 132 | Batch: 005 / 011 | Total loss: 0.846 | Reg loss: 0.026 | Tree loss: 0.846 | Accuracy: 0.647500 | 1.501 sec/iter\n",
      "Epoch: 132 | Batch: 006 / 011 | Total loss: 0.854 | Reg loss: 0.026 | Tree loss: 0.854 | Accuracy: 0.619500 | 1.501 sec/iter\n",
      "Epoch: 132 | Batch: 007 / 011 | Total loss: 0.838 | Reg loss: 0.026 | Tree loss: 0.838 | Accuracy: 0.626500 | 1.501 sec/iter\n",
      "Epoch: 132 | Batch: 008 / 011 | Total loss: 0.822 | Reg loss: 0.026 | Tree loss: 0.822 | Accuracy: 0.625500 | 1.5 sec/iter\n",
      "Epoch: 132 | Batch: 009 / 011 | Total loss: 0.870 | Reg loss: 0.026 | Tree loss: 0.870 | Accuracy: 0.595000 | 1.5 sec/iter\n",
      "Epoch: 132 | Batch: 010 / 011 | Total loss: 0.762 | Reg loss: 0.026 | Tree loss: 0.762 | Accuracy: 0.686007 | 1.5 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 133 | Batch: 000 / 011 | Total loss: 0.921 | Reg loss: 0.026 | Tree loss: 0.921 | Accuracy: 0.640000 | 1.501 sec/iter\n",
      "Epoch: 133 | Batch: 001 / 011 | Total loss: 0.906 | Reg loss: 0.026 | Tree loss: 0.906 | Accuracy: 0.659500 | 1.5 sec/iter\n",
      "Epoch: 133 | Batch: 002 / 011 | Total loss: 0.890 | Reg loss: 0.026 | Tree loss: 0.890 | Accuracy: 0.669000 | 1.5 sec/iter\n",
      "Epoch: 133 | Batch: 003 / 011 | Total loss: 0.893 | Reg loss: 0.026 | Tree loss: 0.893 | Accuracy: 0.664000 | 1.5 sec/iter\n",
      "Epoch: 133 | Batch: 004 / 011 | Total loss: 0.840 | Reg loss: 0.026 | Tree loss: 0.840 | Accuracy: 0.679000 | 1.499 sec/iter\n",
      "Epoch: 133 | Batch: 005 / 011 | Total loss: 0.856 | Reg loss: 0.026 | Tree loss: 0.856 | Accuracy: 0.639500 | 1.499 sec/iter\n",
      "Epoch: 133 | Batch: 006 / 011 | Total loss: 0.846 | Reg loss: 0.026 | Tree loss: 0.846 | Accuracy: 0.633500 | 1.499 sec/iter\n",
      "Epoch: 133 | Batch: 007 / 011 | Total loss: 0.852 | Reg loss: 0.026 | Tree loss: 0.852 | Accuracy: 0.615500 | 1.498 sec/iter\n",
      "Epoch: 133 | Batch: 008 / 011 | Total loss: 0.829 | Reg loss: 0.026 | Tree loss: 0.829 | Accuracy: 0.609000 | 1.498 sec/iter\n",
      "Epoch: 133 | Batch: 009 / 011 | Total loss: 0.817 | Reg loss: 0.026 | Tree loss: 0.817 | Accuracy: 0.632500 | 1.498 sec/iter\n",
      "Epoch: 133 | Batch: 010 / 011 | Total loss: 0.860 | Reg loss: 0.026 | Tree loss: 0.860 | Accuracy: 0.645051 | 1.498 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 134 | Batch: 000 / 011 | Total loss: 0.930 | Reg loss: 0.026 | Tree loss: 0.930 | Accuracy: 0.636500 | 1.501 sec/iter\n",
      "Epoch: 134 | Batch: 001 / 011 | Total loss: 0.882 | Reg loss: 0.026 | Tree loss: 0.882 | Accuracy: 0.655500 | 1.5 sec/iter\n",
      "Epoch: 134 | Batch: 002 / 011 | Total loss: 0.886 | Reg loss: 0.026 | Tree loss: 0.886 | Accuracy: 0.666500 | 1.5 sec/iter\n",
      "Epoch: 134 | Batch: 003 / 011 | Total loss: 0.896 | Reg loss: 0.026 | Tree loss: 0.896 | Accuracy: 0.646500 | 1.5 sec/iter\n",
      "Epoch: 134 | Batch: 004 / 011 | Total loss: 0.870 | Reg loss: 0.026 | Tree loss: 0.870 | Accuracy: 0.659500 | 1.499 sec/iter\n",
      "Epoch: 134 | Batch: 005 / 011 | Total loss: 0.834 | Reg loss: 0.026 | Tree loss: 0.834 | Accuracy: 0.673000 | 1.499 sec/iter\n",
      "Epoch: 134 | Batch: 006 / 011 | Total loss: 0.819 | Reg loss: 0.026 | Tree loss: 0.819 | Accuracy: 0.653500 | 1.499 sec/iter\n",
      "Epoch: 134 | Batch: 007 / 011 | Total loss: 0.851 | Reg loss: 0.026 | Tree loss: 0.851 | Accuracy: 0.613000 | 1.498 sec/iter\n",
      "Epoch: 134 | Batch: 008 / 011 | Total loss: 0.839 | Reg loss: 0.026 | Tree loss: 0.839 | Accuracy: 0.623500 | 1.498 sec/iter\n",
      "Epoch: 134 | Batch: 009 / 011 | Total loss: 0.828 | Reg loss: 0.026 | Tree loss: 0.828 | Accuracy: 0.632500 | 1.498 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 134 | Batch: 010 / 011 | Total loss: 0.837 | Reg loss: 0.026 | Tree loss: 0.837 | Accuracy: 0.617747 | 1.498 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 135 | Batch: 000 / 011 | Total loss: 0.902 | Reg loss: 0.026 | Tree loss: 0.902 | Accuracy: 0.642000 | 1.501 sec/iter\n",
      "Epoch: 135 | Batch: 001 / 011 | Total loss: 0.899 | Reg loss: 0.026 | Tree loss: 0.899 | Accuracy: 0.637500 | 1.501 sec/iter\n",
      "Epoch: 135 | Batch: 002 / 011 | Total loss: 0.895 | Reg loss: 0.026 | Tree loss: 0.895 | Accuracy: 0.664500 | 1.501 sec/iter\n",
      "Epoch: 135 | Batch: 003 / 011 | Total loss: 0.889 | Reg loss: 0.026 | Tree loss: 0.889 | Accuracy: 0.658000 | 1.5 sec/iter\n",
      "Epoch: 135 | Batch: 004 / 011 | Total loss: 0.872 | Reg loss: 0.026 | Tree loss: 0.872 | Accuracy: 0.660000 | 1.5 sec/iter\n",
      "Epoch: 135 | Batch: 005 / 011 | Total loss: 0.812 | Reg loss: 0.026 | Tree loss: 0.812 | Accuracy: 0.676500 | 1.5 sec/iter\n",
      "Epoch: 135 | Batch: 006 / 011 | Total loss: 0.876 | Reg loss: 0.026 | Tree loss: 0.876 | Accuracy: 0.627000 | 1.499 sec/iter\n",
      "Epoch: 135 | Batch: 007 / 011 | Total loss: 0.830 | Reg loss: 0.026 | Tree loss: 0.830 | Accuracy: 0.625000 | 1.499 sec/iter\n",
      "Epoch: 135 | Batch: 008 / 011 | Total loss: 0.837 | Reg loss: 0.026 | Tree loss: 0.837 | Accuracy: 0.601000 | 1.499 sec/iter\n",
      "Epoch: 135 | Batch: 009 / 011 | Total loss: 0.833 | Reg loss: 0.026 | Tree loss: 0.833 | Accuracy: 0.619500 | 1.499 sec/iter\n",
      "Epoch: 135 | Batch: 010 / 011 | Total loss: 0.847 | Reg loss: 0.026 | Tree loss: 0.847 | Accuracy: 0.624573 | 1.499 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 136 | Batch: 000 / 011 | Total loss: 0.898 | Reg loss: 0.026 | Tree loss: 0.898 | Accuracy: 0.641000 | 1.502 sec/iter\n",
      "Epoch: 136 | Batch: 001 / 011 | Total loss: 0.914 | Reg loss: 0.026 | Tree loss: 0.914 | Accuracy: 0.636000 | 1.501 sec/iter\n",
      "Epoch: 136 | Batch: 002 / 011 | Total loss: 0.886 | Reg loss: 0.026 | Tree loss: 0.886 | Accuracy: 0.667500 | 1.501 sec/iter\n",
      "Epoch: 136 | Batch: 003 / 011 | Total loss: 0.876 | Reg loss: 0.026 | Tree loss: 0.876 | Accuracy: 0.656500 | 1.501 sec/iter\n",
      "Epoch: 136 | Batch: 004 / 011 | Total loss: 0.830 | Reg loss: 0.026 | Tree loss: 0.830 | Accuracy: 0.689000 | 1.501 sec/iter\n",
      "Epoch: 136 | Batch: 005 / 011 | Total loss: 0.836 | Reg loss: 0.026 | Tree loss: 0.836 | Accuracy: 0.657000 | 1.5 sec/iter\n",
      "Epoch: 136 | Batch: 006 / 011 | Total loss: 0.867 | Reg loss: 0.026 | Tree loss: 0.867 | Accuracy: 0.629000 | 1.5 sec/iter\n",
      "Epoch: 136 | Batch: 007 / 011 | Total loss: 0.867 | Reg loss: 0.026 | Tree loss: 0.867 | Accuracy: 0.602500 | 1.5 sec/iter\n",
      "Epoch: 136 | Batch: 008 / 011 | Total loss: 0.838 | Reg loss: 0.026 | Tree loss: 0.838 | Accuracy: 0.618000 | 1.5 sec/iter\n",
      "Epoch: 136 | Batch: 009 / 011 | Total loss: 0.831 | Reg loss: 0.026 | Tree loss: 0.831 | Accuracy: 0.613500 | 1.5 sec/iter\n",
      "Epoch: 136 | Batch: 010 / 011 | Total loss: 0.865 | Reg loss: 0.026 | Tree loss: 0.865 | Accuracy: 0.600683 | 1.499 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 137 | Batch: 000 / 011 | Total loss: 0.936 | Reg loss: 0.026 | Tree loss: 0.936 | Accuracy: 0.627000 | 1.502 sec/iter\n",
      "Epoch: 137 | Batch: 001 / 011 | Total loss: 0.887 | Reg loss: 0.026 | Tree loss: 0.887 | Accuracy: 0.643500 | 1.502 sec/iter\n",
      "Epoch: 137 | Batch: 002 / 011 | Total loss: 0.907 | Reg loss: 0.026 | Tree loss: 0.907 | Accuracy: 0.639000 | 1.502 sec/iter\n",
      "Epoch: 137 | Batch: 003 / 011 | Total loss: 0.851 | Reg loss: 0.026 | Tree loss: 0.851 | Accuracy: 0.683000 | 1.501 sec/iter\n",
      "Epoch: 137 | Batch: 004 / 011 | Total loss: 0.857 | Reg loss: 0.026 | Tree loss: 0.857 | Accuracy: 0.676500 | 1.501 sec/iter\n",
      "Epoch: 137 | Batch: 005 / 011 | Total loss: 0.845 | Reg loss: 0.026 | Tree loss: 0.845 | Accuracy: 0.654500 | 1.501 sec/iter\n",
      "Epoch: 137 | Batch: 006 / 011 | Total loss: 0.853 | Reg loss: 0.026 | Tree loss: 0.853 | Accuracy: 0.634000 | 1.5 sec/iter\n",
      "Epoch: 137 | Batch: 007 / 011 | Total loss: 0.835 | Reg loss: 0.026 | Tree loss: 0.835 | Accuracy: 0.607500 | 1.5 sec/iter\n",
      "Epoch: 137 | Batch: 008 / 011 | Total loss: 0.839 | Reg loss: 0.026 | Tree loss: 0.839 | Accuracy: 0.624000 | 1.5 sec/iter\n",
      "Epoch: 137 | Batch: 009 / 011 | Total loss: 0.830 | Reg loss: 0.026 | Tree loss: 0.830 | Accuracy: 0.624500 | 1.5 sec/iter\n",
      "Epoch: 137 | Batch: 010 / 011 | Total loss: 0.890 | Reg loss: 0.026 | Tree loss: 0.890 | Accuracy: 0.563140 | 1.5 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 138 | Batch: 000 / 011 | Total loss: 0.933 | Reg loss: 0.026 | Tree loss: 0.933 | Accuracy: 0.639000 | 1.502 sec/iter\n",
      "Epoch: 138 | Batch: 001 / 011 | Total loss: 0.891 | Reg loss: 0.026 | Tree loss: 0.891 | Accuracy: 0.662000 | 1.502 sec/iter\n",
      "Epoch: 138 | Batch: 002 / 011 | Total loss: 0.919 | Reg loss: 0.026 | Tree loss: 0.919 | Accuracy: 0.641500 | 1.502 sec/iter\n",
      "Epoch: 138 | Batch: 003 / 011 | Total loss: 0.866 | Reg loss: 0.026 | Tree loss: 0.866 | Accuracy: 0.665500 | 1.501 sec/iter\n",
      "Epoch: 138 | Batch: 004 / 011 | Total loss: 0.873 | Reg loss: 0.026 | Tree loss: 0.873 | Accuracy: 0.655500 | 1.501 sec/iter\n",
      "Epoch: 138 | Batch: 005 / 011 | Total loss: 0.816 | Reg loss: 0.026 | Tree loss: 0.816 | Accuracy: 0.666500 | 1.501 sec/iter\n",
      "Epoch: 138 | Batch: 006 / 011 | Total loss: 0.831 | Reg loss: 0.026 | Tree loss: 0.831 | Accuracy: 0.643500 | 1.501 sec/iter\n",
      "Epoch: 138 | Batch: 007 / 011 | Total loss: 0.838 | Reg loss: 0.026 | Tree loss: 0.838 | Accuracy: 0.605000 | 1.501 sec/iter\n",
      "Epoch: 138 | Batch: 008 / 011 | Total loss: 0.842 | Reg loss: 0.026 | Tree loss: 0.842 | Accuracy: 0.609000 | 1.5 sec/iter\n",
      "Epoch: 138 | Batch: 009 / 011 | Total loss: 0.819 | Reg loss: 0.026 | Tree loss: 0.819 | Accuracy: 0.631000 | 1.5 sec/iter\n",
      "Epoch: 138 | Batch: 010 / 011 | Total loss: 0.842 | Reg loss: 0.026 | Tree loss: 0.842 | Accuracy: 0.624573 | 1.5 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 139 | Batch: 000 / 011 | Total loss: 0.928 | Reg loss: 0.026 | Tree loss: 0.928 | Accuracy: 0.636000 | 1.501 sec/iter\n",
      "Epoch: 139 | Batch: 001 / 011 | Total loss: 0.879 | Reg loss: 0.026 | Tree loss: 0.879 | Accuracy: 0.678500 | 1.5 sec/iter\n",
      "Epoch: 139 | Batch: 002 / 011 | Total loss: 0.908 | Reg loss: 0.026 | Tree loss: 0.908 | Accuracy: 0.640500 | 1.5 sec/iter\n",
      "Epoch: 139 | Batch: 003 / 011 | Total loss: 0.863 | Reg loss: 0.026 | Tree loss: 0.863 | Accuracy: 0.656500 | 1.5 sec/iter\n",
      "Epoch: 139 | Batch: 004 / 011 | Total loss: 0.836 | Reg loss: 0.026 | Tree loss: 0.836 | Accuracy: 0.683500 | 1.499 sec/iter\n",
      "Epoch: 139 | Batch: 005 / 011 | Total loss: 0.839 | Reg loss: 0.026 | Tree loss: 0.839 | Accuracy: 0.654500 | 1.499 sec/iter\n",
      "Epoch: 139 | Batch: 006 / 011 | Total loss: 0.812 | Reg loss: 0.026 | Tree loss: 0.812 | Accuracy: 0.665500 | 1.499 sec/iter\n",
      "Epoch: 139 | Batch: 007 / 011 | Total loss: 0.851 | Reg loss: 0.026 | Tree loss: 0.851 | Accuracy: 0.611000 | 1.498 sec/iter\n",
      "Epoch: 139 | Batch: 008 / 011 | Total loss: 0.865 | Reg loss: 0.026 | Tree loss: 0.865 | Accuracy: 0.589500 | 1.498 sec/iter\n",
      "Epoch: 139 | Batch: 009 / 011 | Total loss: 0.861 | Reg loss: 0.026 | Tree loss: 0.861 | Accuracy: 0.595000 | 1.498 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 139 | Batch: 010 / 011 | Total loss: 0.823 | Reg loss: 0.026 | Tree loss: 0.823 | Accuracy: 0.617747 | 1.498 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 140 | Batch: 000 / 011 | Total loss: 0.948 | Reg loss: 0.026 | Tree loss: 0.948 | Accuracy: 0.608500 | 1.501 sec/iter\n",
      "Epoch: 140 | Batch: 001 / 011 | Total loss: 0.896 | Reg loss: 0.026 | Tree loss: 0.896 | Accuracy: 0.656500 | 1.501 sec/iter\n",
      "Epoch: 140 | Batch: 002 / 011 | Total loss: 0.903 | Reg loss: 0.026 | Tree loss: 0.903 | Accuracy: 0.646500 | 1.501 sec/iter\n",
      "Epoch: 140 | Batch: 003 / 011 | Total loss: 0.847 | Reg loss: 0.026 | Tree loss: 0.847 | Accuracy: 0.669500 | 1.501 sec/iter\n",
      "Epoch: 140 | Batch: 004 / 011 | Total loss: 0.847 | Reg loss: 0.026 | Tree loss: 0.847 | Accuracy: 0.669000 | 1.5 sec/iter\n",
      "Epoch: 140 | Batch: 005 / 011 | Total loss: 0.847 | Reg loss: 0.026 | Tree loss: 0.847 | Accuracy: 0.642000 | 1.5 sec/iter\n",
      "Epoch: 140 | Batch: 006 / 011 | Total loss: 0.821 | Reg loss: 0.026 | Tree loss: 0.821 | Accuracy: 0.656500 | 1.5 sec/iter\n",
      "Epoch: 140 | Batch: 007 / 011 | Total loss: 0.833 | Reg loss: 0.026 | Tree loss: 0.833 | Accuracy: 0.617000 | 1.5 sec/iter\n",
      "Epoch: 140 | Batch: 008 / 011 | Total loss: 0.845 | Reg loss: 0.026 | Tree loss: 0.845 | Accuracy: 0.593000 | 1.5 sec/iter\n",
      "Epoch: 140 | Batch: 009 / 011 | Total loss: 0.831 | Reg loss: 0.026 | Tree loss: 0.831 | Accuracy: 0.606500 | 1.5 sec/iter\n",
      "Epoch: 140 | Batch: 010 / 011 | Total loss: 0.838 | Reg loss: 0.026 | Tree loss: 0.838 | Accuracy: 0.638225 | 1.499 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 141 | Batch: 000 / 011 | Total loss: 0.912 | Reg loss: 0.026 | Tree loss: 0.912 | Accuracy: 0.646000 | 1.502 sec/iter\n",
      "Epoch: 141 | Batch: 001 / 011 | Total loss: 0.921 | Reg loss: 0.026 | Tree loss: 0.921 | Accuracy: 0.648500 | 1.502 sec/iter\n",
      "Epoch: 141 | Batch: 002 / 011 | Total loss: 0.883 | Reg loss: 0.026 | Tree loss: 0.883 | Accuracy: 0.648000 | 1.502 sec/iter\n",
      "Epoch: 141 | Batch: 003 / 011 | Total loss: 0.891 | Reg loss: 0.026 | Tree loss: 0.891 | Accuracy: 0.650500 | 1.501 sec/iter\n",
      "Epoch: 141 | Batch: 004 / 011 | Total loss: 0.848 | Reg loss: 0.026 | Tree loss: 0.848 | Accuracy: 0.679500 | 1.501 sec/iter\n",
      "Epoch: 141 | Batch: 005 / 011 | Total loss: 0.840 | Reg loss: 0.026 | Tree loss: 0.840 | Accuracy: 0.685000 | 1.501 sec/iter\n",
      "Epoch: 141 | Batch: 006 / 011 | Total loss: 0.844 | Reg loss: 0.026 | Tree loss: 0.844 | Accuracy: 0.652500 | 1.501 sec/iter\n",
      "Epoch: 141 | Batch: 007 / 011 | Total loss: 0.834 | Reg loss: 0.026 | Tree loss: 0.834 | Accuracy: 0.634500 | 1.501 sec/iter\n",
      "Epoch: 141 | Batch: 008 / 011 | Total loss: 0.822 | Reg loss: 0.026 | Tree loss: 0.822 | Accuracy: 0.619500 | 1.5 sec/iter\n",
      "Epoch: 141 | Batch: 009 / 011 | Total loss: 0.833 | Reg loss: 0.026 | Tree loss: 0.833 | Accuracy: 0.597500 | 1.5 sec/iter\n",
      "Epoch: 141 | Batch: 010 / 011 | Total loss: 0.852 | Reg loss: 0.026 | Tree loss: 0.852 | Accuracy: 0.552901 | 1.5 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 142 | Batch: 000 / 011 | Total loss: 0.900 | Reg loss: 0.026 | Tree loss: 0.900 | Accuracy: 0.652500 | 1.503 sec/iter\n",
      "Epoch: 142 | Batch: 001 / 011 | Total loss: 0.907 | Reg loss: 0.026 | Tree loss: 0.907 | Accuracy: 0.654000 | 1.502 sec/iter\n",
      "Epoch: 142 | Batch: 002 / 011 | Total loss: 0.883 | Reg loss: 0.026 | Tree loss: 0.883 | Accuracy: 0.663500 | 1.502 sec/iter\n",
      "Epoch: 142 | Batch: 003 / 011 | Total loss: 0.872 | Reg loss: 0.026 | Tree loss: 0.872 | Accuracy: 0.657500 | 1.502 sec/iter\n",
      "Epoch: 142 | Batch: 004 / 011 | Total loss: 0.870 | Reg loss: 0.026 | Tree loss: 0.870 | Accuracy: 0.652500 | 1.502 sec/iter\n",
      "Epoch: 142 | Batch: 005 / 011 | Total loss: 0.862 | Reg loss: 0.026 | Tree loss: 0.862 | Accuracy: 0.640500 | 1.501 sec/iter\n",
      "Epoch: 142 | Batch: 006 / 011 | Total loss: 0.826 | Reg loss: 0.026 | Tree loss: 0.826 | Accuracy: 0.666000 | 1.501 sec/iter\n",
      "Epoch: 142 | Batch: 007 / 011 | Total loss: 0.823 | Reg loss: 0.026 | Tree loss: 0.823 | Accuracy: 0.634000 | 1.501 sec/iter\n",
      "Epoch: 142 | Batch: 008 / 011 | Total loss: 0.856 | Reg loss: 0.026 | Tree loss: 0.856 | Accuracy: 0.594000 | 1.501 sec/iter\n",
      "Epoch: 142 | Batch: 009 / 011 | Total loss: 0.858 | Reg loss: 0.026 | Tree loss: 0.858 | Accuracy: 0.588000 | 1.501 sec/iter\n",
      "Epoch: 142 | Batch: 010 / 011 | Total loss: 0.855 | Reg loss: 0.026 | Tree loss: 0.855 | Accuracy: 0.610922 | 1.501 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 143 | Batch: 000 / 011 | Total loss: 0.901 | Reg loss: 0.026 | Tree loss: 0.901 | Accuracy: 0.640500 | 1.502 sec/iter\n",
      "Epoch: 143 | Batch: 001 / 011 | Total loss: 0.924 | Reg loss: 0.026 | Tree loss: 0.924 | Accuracy: 0.625000 | 1.502 sec/iter\n",
      "Epoch: 143 | Batch: 002 / 011 | Total loss: 0.889 | Reg loss: 0.026 | Tree loss: 0.889 | Accuracy: 0.645500 | 1.502 sec/iter\n",
      "Epoch: 143 | Batch: 003 / 011 | Total loss: 0.880 | Reg loss: 0.026 | Tree loss: 0.880 | Accuracy: 0.661500 | 1.501 sec/iter\n",
      "Epoch: 143 | Batch: 004 / 011 | Total loss: 0.866 | Reg loss: 0.026 | Tree loss: 0.866 | Accuracy: 0.645500 | 1.501 sec/iter\n",
      "Epoch: 143 | Batch: 005 / 011 | Total loss: 0.837 | Reg loss: 0.026 | Tree loss: 0.837 | Accuracy: 0.670500 | 1.501 sec/iter\n",
      "Epoch: 143 | Batch: 006 / 011 | Total loss: 0.835 | Reg loss: 0.026 | Tree loss: 0.835 | Accuracy: 0.638500 | 1.5 sec/iter\n",
      "Epoch: 143 | Batch: 007 / 011 | Total loss: 0.823 | Reg loss: 0.026 | Tree loss: 0.823 | Accuracy: 0.649500 | 1.5 sec/iter\n",
      "Epoch: 143 | Batch: 008 / 011 | Total loss: 0.826 | Reg loss: 0.026 | Tree loss: 0.826 | Accuracy: 0.628000 | 1.5 sec/iter\n",
      "Epoch: 143 | Batch: 009 / 011 | Total loss: 0.832 | Reg loss: 0.026 | Tree loss: 0.832 | Accuracy: 0.613000 | 1.5 sec/iter\n",
      "Epoch: 143 | Batch: 010 / 011 | Total loss: 0.832 | Reg loss: 0.026 | Tree loss: 0.832 | Accuracy: 0.610922 | 1.5 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 144 | Batch: 000 / 011 | Total loss: 0.921 | Reg loss: 0.026 | Tree loss: 0.921 | Accuracy: 0.637500 | 1.502 sec/iter\n",
      "Epoch: 144 | Batch: 001 / 011 | Total loss: 0.890 | Reg loss: 0.026 | Tree loss: 0.890 | Accuracy: 0.642000 | 1.502 sec/iter\n",
      "Epoch: 144 | Batch: 002 / 011 | Total loss: 0.875 | Reg loss: 0.026 | Tree loss: 0.875 | Accuracy: 0.660000 | 1.501 sec/iter\n",
      "Epoch: 144 | Batch: 003 / 011 | Total loss: 0.852 | Reg loss: 0.026 | Tree loss: 0.852 | Accuracy: 0.684500 | 1.501 sec/iter\n",
      "Epoch: 144 | Batch: 004 / 011 | Total loss: 0.880 | Reg loss: 0.026 | Tree loss: 0.880 | Accuracy: 0.647500 | 1.501 sec/iter\n",
      "Epoch: 144 | Batch: 005 / 011 | Total loss: 0.845 | Reg loss: 0.026 | Tree loss: 0.845 | Accuracy: 0.661000 | 1.501 sec/iter\n",
      "Epoch: 144 | Batch: 006 / 011 | Total loss: 0.837 | Reg loss: 0.026 | Tree loss: 0.837 | Accuracy: 0.636500 | 1.5 sec/iter\n",
      "Epoch: 144 | Batch: 007 / 011 | Total loss: 0.828 | Reg loss: 0.026 | Tree loss: 0.828 | Accuracy: 0.621500 | 1.5 sec/iter\n",
      "Epoch: 144 | Batch: 008 / 011 | Total loss: 0.855 | Reg loss: 0.026 | Tree loss: 0.855 | Accuracy: 0.602000 | 1.5 sec/iter\n",
      "Epoch: 144 | Batch: 009 / 011 | Total loss: 0.834 | Reg loss: 0.026 | Tree loss: 0.834 | Accuracy: 0.628000 | 1.5 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 144 | Batch: 010 / 011 | Total loss: 0.807 | Reg loss: 0.026 | Tree loss: 0.807 | Accuracy: 0.645051 | 1.5 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 145 | Batch: 000 / 011 | Total loss: 0.911 | Reg loss: 0.026 | Tree loss: 0.911 | Accuracy: 0.630500 | 1.501 sec/iter\n",
      "Epoch: 145 | Batch: 001 / 011 | Total loss: 0.906 | Reg loss: 0.026 | Tree loss: 0.906 | Accuracy: 0.638000 | 1.5 sec/iter\n",
      "Epoch: 145 | Batch: 002 / 011 | Total loss: 0.867 | Reg loss: 0.026 | Tree loss: 0.867 | Accuracy: 0.671000 | 1.5 sec/iter\n",
      "Epoch: 145 | Batch: 003 / 011 | Total loss: 0.862 | Reg loss: 0.026 | Tree loss: 0.862 | Accuracy: 0.666000 | 1.5 sec/iter\n",
      "Epoch: 145 | Batch: 004 / 011 | Total loss: 0.868 | Reg loss: 0.026 | Tree loss: 0.868 | Accuracy: 0.658000 | 1.499 sec/iter\n",
      "Epoch: 145 | Batch: 005 / 011 | Total loss: 0.850 | Reg loss: 0.026 | Tree loss: 0.850 | Accuracy: 0.671500 | 1.499 sec/iter\n",
      "Epoch: 145 | Batch: 006 / 011 | Total loss: 0.845 | Reg loss: 0.026 | Tree loss: 0.845 | Accuracy: 0.625500 | 1.499 sec/iter\n",
      "Epoch: 145 | Batch: 007 / 011 | Total loss: 0.845 | Reg loss: 0.026 | Tree loss: 0.845 | Accuracy: 0.615000 | 1.499 sec/iter\n",
      "Epoch: 145 | Batch: 008 / 011 | Total loss: 0.818 | Reg loss: 0.026 | Tree loss: 0.818 | Accuracy: 0.618000 | 1.498 sec/iter\n",
      "Epoch: 145 | Batch: 009 / 011 | Total loss: 0.836 | Reg loss: 0.026 | Tree loss: 0.836 | Accuracy: 0.606500 | 1.498 sec/iter\n",
      "Epoch: 145 | Batch: 010 / 011 | Total loss: 0.844 | Reg loss: 0.026 | Tree loss: 0.844 | Accuracy: 0.658703 | 1.498 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 146 | Batch: 000 / 011 | Total loss: 0.946 | Reg loss: 0.026 | Tree loss: 0.946 | Accuracy: 0.609500 | 1.5 sec/iter\n",
      "Epoch: 146 | Batch: 001 / 011 | Total loss: 0.908 | Reg loss: 0.026 | Tree loss: 0.908 | Accuracy: 0.642500 | 1.5 sec/iter\n",
      "Epoch: 146 | Batch: 002 / 011 | Total loss: 0.877 | Reg loss: 0.026 | Tree loss: 0.877 | Accuracy: 0.664000 | 1.5 sec/iter\n",
      "Epoch: 146 | Batch: 003 / 011 | Total loss: 0.862 | Reg loss: 0.026 | Tree loss: 0.862 | Accuracy: 0.674500 | 1.5 sec/iter\n",
      "Epoch: 146 | Batch: 004 / 011 | Total loss: 0.840 | Reg loss: 0.026 | Tree loss: 0.840 | Accuracy: 0.670000 | 1.499 sec/iter\n",
      "Epoch: 146 | Batch: 005 / 011 | Total loss: 0.833 | Reg loss: 0.026 | Tree loss: 0.833 | Accuracy: 0.663500 | 1.499 sec/iter\n",
      "Epoch: 146 | Batch: 006 / 011 | Total loss: 0.843 | Reg loss: 0.026 | Tree loss: 0.843 | Accuracy: 0.625000 | 1.499 sec/iter\n",
      "Epoch: 146 | Batch: 007 / 011 | Total loss: 0.827 | Reg loss: 0.026 | Tree loss: 0.827 | Accuracy: 0.632000 | 1.498 sec/iter\n",
      "Epoch: 146 | Batch: 008 / 011 | Total loss: 0.835 | Reg loss: 0.026 | Tree loss: 0.835 | Accuracy: 0.617500 | 1.498 sec/iter\n",
      "Epoch: 146 | Batch: 009 / 011 | Total loss: 0.833 | Reg loss: 0.026 | Tree loss: 0.833 | Accuracy: 0.606000 | 1.498 sec/iter\n",
      "Epoch: 146 | Batch: 010 / 011 | Total loss: 0.818 | Reg loss: 0.026 | Tree loss: 0.818 | Accuracy: 0.668942 | 1.498 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 147 | Batch: 000 / 011 | Total loss: 0.904 | Reg loss: 0.026 | Tree loss: 0.904 | Accuracy: 0.648500 | 1.501 sec/iter\n",
      "Epoch: 147 | Batch: 001 / 011 | Total loss: 0.895 | Reg loss: 0.026 | Tree loss: 0.895 | Accuracy: 0.642000 | 1.501 sec/iter\n",
      "Epoch: 147 | Batch: 002 / 011 | Total loss: 0.873 | Reg loss: 0.026 | Tree loss: 0.873 | Accuracy: 0.678500 | 1.5 sec/iter\n",
      "Epoch: 147 | Batch: 003 / 011 | Total loss: 0.881 | Reg loss: 0.026 | Tree loss: 0.881 | Accuracy: 0.654000 | 1.5 sec/iter\n",
      "Epoch: 147 | Batch: 004 / 011 | Total loss: 0.853 | Reg loss: 0.026 | Tree loss: 0.853 | Accuracy: 0.673500 | 1.5 sec/iter\n",
      "Epoch: 147 | Batch: 005 / 011 | Total loss: 0.849 | Reg loss: 0.026 | Tree loss: 0.849 | Accuracy: 0.661500 | 1.499 sec/iter\n",
      "Epoch: 147 | Batch: 006 / 011 | Total loss: 0.820 | Reg loss: 0.026 | Tree loss: 0.820 | Accuracy: 0.655500 | 1.499 sec/iter\n",
      "Epoch: 147 | Batch: 007 / 011 | Total loss: 0.844 | Reg loss: 0.026 | Tree loss: 0.844 | Accuracy: 0.628500 | 1.499 sec/iter\n",
      "Epoch: 147 | Batch: 008 / 011 | Total loss: 0.840 | Reg loss: 0.026 | Tree loss: 0.840 | Accuracy: 0.619500 | 1.499 sec/iter\n",
      "Epoch: 147 | Batch: 009 / 011 | Total loss: 0.823 | Reg loss: 0.026 | Tree loss: 0.823 | Accuracy: 0.622500 | 1.499 sec/iter\n",
      "Epoch: 147 | Batch: 010 / 011 | Total loss: 0.831 | Reg loss: 0.026 | Tree loss: 0.831 | Accuracy: 0.600683 | 1.499 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 148 | Batch: 000 / 011 | Total loss: 0.919 | Reg loss: 0.026 | Tree loss: 0.919 | Accuracy: 0.635500 | 1.501 sec/iter\n",
      "Epoch: 148 | Batch: 001 / 011 | Total loss: 0.894 | Reg loss: 0.026 | Tree loss: 0.894 | Accuracy: 0.648500 | 1.501 sec/iter\n",
      "Epoch: 148 | Batch: 002 / 011 | Total loss: 0.872 | Reg loss: 0.026 | Tree loss: 0.872 | Accuracy: 0.664500 | 1.501 sec/iter\n",
      "Epoch: 148 | Batch: 003 / 011 | Total loss: 0.855 | Reg loss: 0.026 | Tree loss: 0.855 | Accuracy: 0.673000 | 1.501 sec/iter\n",
      "Epoch: 148 | Batch: 004 / 011 | Total loss: 0.822 | Reg loss: 0.026 | Tree loss: 0.822 | Accuracy: 0.681500 | 1.5 sec/iter\n",
      "Epoch: 148 | Batch: 005 / 011 | Total loss: 0.851 | Reg loss: 0.026 | Tree loss: 0.851 | Accuracy: 0.633000 | 1.5 sec/iter\n",
      "Epoch: 148 | Batch: 006 / 011 | Total loss: 0.860 | Reg loss: 0.026 | Tree loss: 0.860 | Accuracy: 0.611000 | 1.5 sec/iter\n",
      "Epoch: 148 | Batch: 007 / 011 | Total loss: 0.850 | Reg loss: 0.026 | Tree loss: 0.850 | Accuracy: 0.614500 | 1.499 sec/iter\n",
      "Epoch: 148 | Batch: 008 / 011 | Total loss: 0.836 | Reg loss: 0.026 | Tree loss: 0.836 | Accuracy: 0.614000 | 1.499 sec/iter\n",
      "Epoch: 148 | Batch: 009 / 011 | Total loss: 0.842 | Reg loss: 0.026 | Tree loss: 0.842 | Accuracy: 0.611500 | 1.499 sec/iter\n",
      "Epoch: 148 | Batch: 010 / 011 | Total loss: 0.821 | Reg loss: 0.026 | Tree loss: 0.821 | Accuracy: 0.600683 | 1.499 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 149 | Batch: 000 / 011 | Total loss: 0.905 | Reg loss: 0.026 | Tree loss: 0.905 | Accuracy: 0.646000 | 1.501 sec/iter\n",
      "Epoch: 149 | Batch: 001 / 011 | Total loss: 0.907 | Reg loss: 0.026 | Tree loss: 0.907 | Accuracy: 0.642000 | 1.501 sec/iter\n",
      "Epoch: 149 | Batch: 002 / 011 | Total loss: 0.897 | Reg loss: 0.026 | Tree loss: 0.897 | Accuracy: 0.644500 | 1.501 sec/iter\n",
      "Epoch: 149 | Batch: 003 / 011 | Total loss: 0.880 | Reg loss: 0.026 | Tree loss: 0.880 | Accuracy: 0.665500 | 1.501 sec/iter\n",
      "Epoch: 149 | Batch: 004 / 011 | Total loss: 0.873 | Reg loss: 0.026 | Tree loss: 0.873 | Accuracy: 0.649000 | 1.501 sec/iter\n",
      "Epoch: 149 | Batch: 005 / 011 | Total loss: 0.834 | Reg loss: 0.026 | Tree loss: 0.834 | Accuracy: 0.670000 | 1.5 sec/iter\n",
      "Epoch: 149 | Batch: 006 / 011 | Total loss: 0.826 | Reg loss: 0.026 | Tree loss: 0.826 | Accuracy: 0.651500 | 1.5 sec/iter\n",
      "Epoch: 149 | Batch: 007 / 011 | Total loss: 0.833 | Reg loss: 0.026 | Tree loss: 0.833 | Accuracy: 0.631000 | 1.5 sec/iter\n",
      "Epoch: 149 | Batch: 008 / 011 | Total loss: 0.824 | Reg loss: 0.026 | Tree loss: 0.824 | Accuracy: 0.623000 | 1.5 sec/iter\n",
      "Epoch: 149 | Batch: 009 / 011 | Total loss: 0.818 | Reg loss: 0.026 | Tree loss: 0.818 | Accuracy: 0.621000 | 1.499 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 149 | Batch: 010 / 011 | Total loss: 0.800 | Reg loss: 0.026 | Tree loss: 0.800 | Accuracy: 0.658703 | 1.499 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 150 | Batch: 000 / 011 | Total loss: 0.925 | Reg loss: 0.026 | Tree loss: 0.925 | Accuracy: 0.623500 | 1.502 sec/iter\n",
      "Epoch: 150 | Batch: 001 / 011 | Total loss: 0.891 | Reg loss: 0.026 | Tree loss: 0.891 | Accuracy: 0.660000 | 1.502 sec/iter\n",
      "Epoch: 150 | Batch: 002 / 011 | Total loss: 0.886 | Reg loss: 0.026 | Tree loss: 0.886 | Accuracy: 0.649000 | 1.501 sec/iter\n",
      "Epoch: 150 | Batch: 003 / 011 | Total loss: 0.880 | Reg loss: 0.026 | Tree loss: 0.880 | Accuracy: 0.657500 | 1.501 sec/iter\n",
      "Epoch: 150 | Batch: 004 / 011 | Total loss: 0.856 | Reg loss: 0.026 | Tree loss: 0.856 | Accuracy: 0.669000 | 1.501 sec/iter\n",
      "Epoch: 150 | Batch: 005 / 011 | Total loss: 0.821 | Reg loss: 0.026 | Tree loss: 0.821 | Accuracy: 0.670000 | 1.501 sec/iter\n",
      "Epoch: 150 | Batch: 006 / 011 | Total loss: 0.821 | Reg loss: 0.026 | Tree loss: 0.821 | Accuracy: 0.635000 | 1.5 sec/iter\n",
      "Epoch: 150 | Batch: 007 / 011 | Total loss: 0.834 | Reg loss: 0.026 | Tree loss: 0.834 | Accuracy: 0.624000 | 1.5 sec/iter\n",
      "Epoch: 150 | Batch: 008 / 011 | Total loss: 0.818 | Reg loss: 0.026 | Tree loss: 0.818 | Accuracy: 0.627500 | 1.5 sec/iter\n",
      "Epoch: 150 | Batch: 009 / 011 | Total loss: 0.843 | Reg loss: 0.026 | Tree loss: 0.843 | Accuracy: 0.620000 | 1.5 sec/iter\n",
      "Epoch: 150 | Batch: 010 / 011 | Total loss: 0.828 | Reg loss: 0.026 | Tree loss: 0.828 | Accuracy: 0.627986 | 1.499 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 151 | Batch: 000 / 011 | Total loss: 0.920 | Reg loss: 0.026 | Tree loss: 0.920 | Accuracy: 0.632500 | 1.5 sec/iter\n",
      "Epoch: 151 | Batch: 001 / 011 | Total loss: 0.901 | Reg loss: 0.026 | Tree loss: 0.901 | Accuracy: 0.638000 | 1.5 sec/iter\n",
      "Epoch: 151 | Batch: 002 / 011 | Total loss: 0.880 | Reg loss: 0.026 | Tree loss: 0.880 | Accuracy: 0.648000 | 1.5 sec/iter\n",
      "Epoch: 151 | Batch: 003 / 011 | Total loss: 0.855 | Reg loss: 0.026 | Tree loss: 0.855 | Accuracy: 0.659500 | 1.499 sec/iter\n",
      "Epoch: 151 | Batch: 004 / 011 | Total loss: 0.860 | Reg loss: 0.026 | Tree loss: 0.860 | Accuracy: 0.667500 | 1.499 sec/iter\n",
      "Epoch: 151 | Batch: 005 / 011 | Total loss: 0.831 | Reg loss: 0.026 | Tree loss: 0.831 | Accuracy: 0.650000 | 1.499 sec/iter\n",
      "Epoch: 151 | Batch: 006 / 011 | Total loss: 0.832 | Reg loss: 0.026 | Tree loss: 0.832 | Accuracy: 0.638000 | 1.498 sec/iter\n",
      "Epoch: 151 | Batch: 007 / 011 | Total loss: 0.829 | Reg loss: 0.026 | Tree loss: 0.829 | Accuracy: 0.621500 | 1.498 sec/iter\n",
      "Epoch: 151 | Batch: 008 / 011 | Total loss: 0.818 | Reg loss: 0.026 | Tree loss: 0.818 | Accuracy: 0.610000 | 1.498 sec/iter\n",
      "Epoch: 151 | Batch: 009 / 011 | Total loss: 0.843 | Reg loss: 0.026 | Tree loss: 0.843 | Accuracy: 0.616000 | 1.498 sec/iter\n",
      "Epoch: 151 | Batch: 010 / 011 | Total loss: 0.815 | Reg loss: 0.026 | Tree loss: 0.815 | Accuracy: 0.648464 | 1.498 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 152 | Batch: 000 / 011 | Total loss: 0.907 | Reg loss: 0.026 | Tree loss: 0.907 | Accuracy: 0.652000 | 1.501 sec/iter\n",
      "Epoch: 152 | Batch: 001 / 011 | Total loss: 0.904 | Reg loss: 0.026 | Tree loss: 0.904 | Accuracy: 0.634000 | 1.5 sec/iter\n",
      "Epoch: 152 | Batch: 002 / 011 | Total loss: 0.869 | Reg loss: 0.026 | Tree loss: 0.869 | Accuracy: 0.671000 | 1.5 sec/iter\n",
      "Epoch: 152 | Batch: 003 / 011 | Total loss: 0.861 | Reg loss: 0.026 | Tree loss: 0.861 | Accuracy: 0.666000 | 1.5 sec/iter\n",
      "Epoch: 152 | Batch: 004 / 011 | Total loss: 0.862 | Reg loss: 0.026 | Tree loss: 0.862 | Accuracy: 0.663500 | 1.5 sec/iter\n",
      "Epoch: 152 | Batch: 005 / 011 | Total loss: 0.829 | Reg loss: 0.026 | Tree loss: 0.829 | Accuracy: 0.669500 | 1.499 sec/iter\n",
      "Epoch: 152 | Batch: 006 / 011 | Total loss: 0.830 | Reg loss: 0.026 | Tree loss: 0.830 | Accuracy: 0.645000 | 1.499 sec/iter\n",
      "Epoch: 152 | Batch: 007 / 011 | Total loss: 0.825 | Reg loss: 0.026 | Tree loss: 0.825 | Accuracy: 0.649500 | 1.499 sec/iter\n",
      "Epoch: 152 | Batch: 008 / 011 | Total loss: 0.836 | Reg loss: 0.026 | Tree loss: 0.836 | Accuracy: 0.595500 | 1.499 sec/iter\n",
      "Epoch: 152 | Batch: 009 / 011 | Total loss: 0.828 | Reg loss: 0.026 | Tree loss: 0.828 | Accuracy: 0.610000 | 1.499 sec/iter\n",
      "Epoch: 152 | Batch: 010 / 011 | Total loss: 0.909 | Reg loss: 0.026 | Tree loss: 0.909 | Accuracy: 0.576792 | 1.498 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 153 | Batch: 000 / 011 | Total loss: 0.892 | Reg loss: 0.026 | Tree loss: 0.892 | Accuracy: 0.643500 | 1.501 sec/iter\n",
      "Epoch: 153 | Batch: 001 / 011 | Total loss: 0.894 | Reg loss: 0.026 | Tree loss: 0.894 | Accuracy: 0.635500 | 1.501 sec/iter\n",
      "Epoch: 153 | Batch: 002 / 011 | Total loss: 0.890 | Reg loss: 0.026 | Tree loss: 0.890 | Accuracy: 0.652000 | 1.501 sec/iter\n",
      "Epoch: 153 | Batch: 003 / 011 | Total loss: 0.859 | Reg loss: 0.026 | Tree loss: 0.859 | Accuracy: 0.659000 | 1.5 sec/iter\n",
      "Epoch: 153 | Batch: 004 / 011 | Total loss: 0.846 | Reg loss: 0.026 | Tree loss: 0.846 | Accuracy: 0.680500 | 1.5 sec/iter\n",
      "Epoch: 153 | Batch: 005 / 011 | Total loss: 0.834 | Reg loss: 0.026 | Tree loss: 0.834 | Accuracy: 0.663000 | 1.5 sec/iter\n",
      "Epoch: 153 | Batch: 006 / 011 | Total loss: 0.832 | Reg loss: 0.026 | Tree loss: 0.832 | Accuracy: 0.646000 | 1.5 sec/iter\n",
      "Epoch: 153 | Batch: 007 / 011 | Total loss: 0.837 | Reg loss: 0.026 | Tree loss: 0.837 | Accuracy: 0.619000 | 1.5 sec/iter\n",
      "Epoch: 153 | Batch: 008 / 011 | Total loss: 0.849 | Reg loss: 0.026 | Tree loss: 0.849 | Accuracy: 0.610000 | 1.5 sec/iter\n",
      "Epoch: 153 | Batch: 009 / 011 | Total loss: 0.855 | Reg loss: 0.026 | Tree loss: 0.855 | Accuracy: 0.613000 | 1.499 sec/iter\n",
      "Epoch: 153 | Batch: 010 / 011 | Total loss: 0.793 | Reg loss: 0.026 | Tree loss: 0.793 | Accuracy: 0.662116 | 1.499 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 154 | Batch: 000 / 011 | Total loss: 0.909 | Reg loss: 0.026 | Tree loss: 0.909 | Accuracy: 0.651500 | 1.501 sec/iter\n",
      "Epoch: 154 | Batch: 001 / 011 | Total loss: 0.892 | Reg loss: 0.026 | Tree loss: 0.892 | Accuracy: 0.655500 | 1.501 sec/iter\n",
      "Epoch: 154 | Batch: 002 / 011 | Total loss: 0.882 | Reg loss: 0.026 | Tree loss: 0.882 | Accuracy: 0.650500 | 1.501 sec/iter\n",
      "Epoch: 154 | Batch: 003 / 011 | Total loss: 0.862 | Reg loss: 0.026 | Tree loss: 0.862 | Accuracy: 0.668000 | 1.501 sec/iter\n",
      "Epoch: 154 | Batch: 004 / 011 | Total loss: 0.834 | Reg loss: 0.026 | Tree loss: 0.834 | Accuracy: 0.673500 | 1.5 sec/iter\n",
      "Epoch: 154 | Batch: 005 / 011 | Total loss: 0.856 | Reg loss: 0.026 | Tree loss: 0.856 | Accuracy: 0.628500 | 1.5 sec/iter\n",
      "Epoch: 154 | Batch: 006 / 011 | Total loss: 0.834 | Reg loss: 0.026 | Tree loss: 0.834 | Accuracy: 0.635000 | 1.5 sec/iter\n",
      "Epoch: 154 | Batch: 007 / 011 | Total loss: 0.832 | Reg loss: 0.026 | Tree loss: 0.832 | Accuracy: 0.636000 | 1.5 sec/iter\n",
      "Epoch: 154 | Batch: 008 / 011 | Total loss: 0.817 | Reg loss: 0.026 | Tree loss: 0.817 | Accuracy: 0.649000 | 1.5 sec/iter\n",
      "Epoch: 154 | Batch: 009 / 011 | Total loss: 0.840 | Reg loss: 0.026 | Tree loss: 0.840 | Accuracy: 0.624500 | 1.5 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 154 | Batch: 010 / 011 | Total loss: 0.791 | Reg loss: 0.026 | Tree loss: 0.791 | Accuracy: 0.641638 | 1.5 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 155 | Batch: 000 / 011 | Total loss: 0.915 | Reg loss: 0.026 | Tree loss: 0.915 | Accuracy: 0.640500 | 1.501 sec/iter\n",
      "Epoch: 155 | Batch: 001 / 011 | Total loss: 0.891 | Reg loss: 0.026 | Tree loss: 0.891 | Accuracy: 0.655000 | 1.501 sec/iter\n",
      "Epoch: 155 | Batch: 002 / 011 | Total loss: 0.878 | Reg loss: 0.026 | Tree loss: 0.878 | Accuracy: 0.656000 | 1.501 sec/iter\n",
      "Epoch: 155 | Batch: 003 / 011 | Total loss: 0.872 | Reg loss: 0.026 | Tree loss: 0.872 | Accuracy: 0.650500 | 1.5 sec/iter\n",
      "Epoch: 155 | Batch: 004 / 011 | Total loss: 0.847 | Reg loss: 0.026 | Tree loss: 0.847 | Accuracy: 0.679500 | 1.5 sec/iter\n",
      "Epoch: 155 | Batch: 005 / 011 | Total loss: 0.834 | Reg loss: 0.026 | Tree loss: 0.834 | Accuracy: 0.657000 | 1.5 sec/iter\n",
      "Epoch: 155 | Batch: 006 / 011 | Total loss: 0.813 | Reg loss: 0.026 | Tree loss: 0.813 | Accuracy: 0.658500 | 1.5 sec/iter\n",
      "Epoch: 155 | Batch: 007 / 011 | Total loss: 0.840 | Reg loss: 0.026 | Tree loss: 0.840 | Accuracy: 0.611500 | 1.499 sec/iter\n",
      "Epoch: 155 | Batch: 008 / 011 | Total loss: 0.824 | Reg loss: 0.026 | Tree loss: 0.824 | Accuracy: 0.619500 | 1.499 sec/iter\n",
      "Epoch: 155 | Batch: 009 / 011 | Total loss: 0.833 | Reg loss: 0.026 | Tree loss: 0.833 | Accuracy: 0.602000 | 1.499 sec/iter\n",
      "Epoch: 155 | Batch: 010 / 011 | Total loss: 0.828 | Reg loss: 0.026 | Tree loss: 0.828 | Accuracy: 0.617747 | 1.499 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 156 | Batch: 000 / 011 | Total loss: 0.919 | Reg loss: 0.026 | Tree loss: 0.919 | Accuracy: 0.620500 | 1.501 sec/iter\n",
      "Epoch: 156 | Batch: 001 / 011 | Total loss: 0.904 | Reg loss: 0.026 | Tree loss: 0.904 | Accuracy: 0.645500 | 1.501 sec/iter\n",
      "Epoch: 156 | Batch: 002 / 011 | Total loss: 0.897 | Reg loss: 0.026 | Tree loss: 0.897 | Accuracy: 0.650500 | 1.501 sec/iter\n",
      "Epoch: 156 | Batch: 003 / 011 | Total loss: 0.873 | Reg loss: 0.026 | Tree loss: 0.873 | Accuracy: 0.663500 | 1.501 sec/iter\n",
      "Epoch: 156 | Batch: 004 / 011 | Total loss: 0.840 | Reg loss: 0.026 | Tree loss: 0.840 | Accuracy: 0.681500 | 1.5 sec/iter\n",
      "Epoch: 156 | Batch: 005 / 011 | Total loss: 0.843 | Reg loss: 0.026 | Tree loss: 0.843 | Accuracy: 0.662500 | 1.5 sec/iter\n",
      "Epoch: 156 | Batch: 006 / 011 | Total loss: 0.836 | Reg loss: 0.026 | Tree loss: 0.836 | Accuracy: 0.647000 | 1.5 sec/iter\n",
      "Epoch: 156 | Batch: 007 / 011 | Total loss: 0.829 | Reg loss: 0.026 | Tree loss: 0.829 | Accuracy: 0.635000 | 1.5 sec/iter\n",
      "Epoch: 156 | Batch: 008 / 011 | Total loss: 0.824 | Reg loss: 0.026 | Tree loss: 0.824 | Accuracy: 0.619000 | 1.5 sec/iter\n",
      "Epoch: 156 | Batch: 009 / 011 | Total loss: 0.821 | Reg loss: 0.026 | Tree loss: 0.821 | Accuracy: 0.636500 | 1.499 sec/iter\n",
      "Epoch: 156 | Batch: 010 / 011 | Total loss: 0.832 | Reg loss: 0.026 | Tree loss: 0.832 | Accuracy: 0.624573 | 1.499 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 157 | Batch: 000 / 011 | Total loss: 0.882 | Reg loss: 0.026 | Tree loss: 0.882 | Accuracy: 0.656500 | 1.5 sec/iter\n",
      "Epoch: 157 | Batch: 001 / 011 | Total loss: 0.901 | Reg loss: 0.026 | Tree loss: 0.901 | Accuracy: 0.647000 | 1.5 sec/iter\n",
      "Epoch: 157 | Batch: 002 / 011 | Total loss: 0.879 | Reg loss: 0.026 | Tree loss: 0.879 | Accuracy: 0.662000 | 1.5 sec/iter\n",
      "Epoch: 157 | Batch: 003 / 011 | Total loss: 0.848 | Reg loss: 0.026 | Tree loss: 0.848 | Accuracy: 0.671500 | 1.499 sec/iter\n",
      "Epoch: 157 | Batch: 004 / 011 | Total loss: 0.863 | Reg loss: 0.026 | Tree loss: 0.863 | Accuracy: 0.653500 | 1.499 sec/iter\n",
      "Epoch: 157 | Batch: 005 / 011 | Total loss: 0.842 | Reg loss: 0.026 | Tree loss: 0.842 | Accuracy: 0.651000 | 1.499 sec/iter\n",
      "Epoch: 157 | Batch: 006 / 011 | Total loss: 0.827 | Reg loss: 0.026 | Tree loss: 0.827 | Accuracy: 0.641500 | 1.499 sec/iter\n",
      "Epoch: 157 | Batch: 007 / 011 | Total loss: 0.827 | Reg loss: 0.026 | Tree loss: 0.827 | Accuracy: 0.631500 | 1.499 sec/iter\n",
      "Epoch: 157 | Batch: 008 / 011 | Total loss: 0.841 | Reg loss: 0.026 | Tree loss: 0.841 | Accuracy: 0.615000 | 1.498 sec/iter\n",
      "Epoch: 157 | Batch: 009 / 011 | Total loss: 0.818 | Reg loss: 0.026 | Tree loss: 0.818 | Accuracy: 0.624000 | 1.498 sec/iter\n",
      "Epoch: 157 | Batch: 010 / 011 | Total loss: 0.869 | Reg loss: 0.026 | Tree loss: 0.869 | Accuracy: 0.587031 | 1.498 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 158 | Batch: 000 / 011 | Total loss: 0.909 | Reg loss: 0.026 | Tree loss: 0.909 | Accuracy: 0.638500 | 1.5 sec/iter\n",
      "Epoch: 158 | Batch: 001 / 011 | Total loss: 0.904 | Reg loss: 0.026 | Tree loss: 0.904 | Accuracy: 0.645500 | 1.5 sec/iter\n",
      "Epoch: 158 | Batch: 002 / 011 | Total loss: 0.869 | Reg loss: 0.026 | Tree loss: 0.869 | Accuracy: 0.658500 | 1.5 sec/iter\n",
      "Epoch: 158 | Batch: 003 / 011 | Total loss: 0.853 | Reg loss: 0.026 | Tree loss: 0.853 | Accuracy: 0.662500 | 1.5 sec/iter\n",
      "Epoch: 158 | Batch: 004 / 011 | Total loss: 0.838 | Reg loss: 0.026 | Tree loss: 0.838 | Accuracy: 0.663000 | 1.5 sec/iter\n",
      "Epoch: 158 | Batch: 005 / 011 | Total loss: 0.832 | Reg loss: 0.026 | Tree loss: 0.832 | Accuracy: 0.650500 | 1.499 sec/iter\n",
      "Epoch: 158 | Batch: 006 / 011 | Total loss: 0.831 | Reg loss: 0.026 | Tree loss: 0.831 | Accuracy: 0.632000 | 1.499 sec/iter\n",
      "Epoch: 158 | Batch: 007 / 011 | Total loss: 0.845 | Reg loss: 0.026 | Tree loss: 0.845 | Accuracy: 0.612000 | 1.499 sec/iter\n",
      "Epoch: 158 | Batch: 008 / 011 | Total loss: 0.850 | Reg loss: 0.026 | Tree loss: 0.850 | Accuracy: 0.612500 | 1.499 sec/iter\n",
      "Epoch: 158 | Batch: 009 / 011 | Total loss: 0.823 | Reg loss: 0.026 | Tree loss: 0.823 | Accuracy: 0.629000 | 1.499 sec/iter\n",
      "Epoch: 158 | Batch: 010 / 011 | Total loss: 0.843 | Reg loss: 0.026 | Tree loss: 0.843 | Accuracy: 0.651877 | 1.499 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 159 | Batch: 000 / 011 | Total loss: 0.885 | Reg loss: 0.026 | Tree loss: 0.885 | Accuracy: 0.660500 | 1.501 sec/iter\n",
      "Epoch: 159 | Batch: 001 / 011 | Total loss: 0.889 | Reg loss: 0.026 | Tree loss: 0.889 | Accuracy: 0.663500 | 1.501 sec/iter\n",
      "Epoch: 159 | Batch: 002 / 011 | Total loss: 0.906 | Reg loss: 0.026 | Tree loss: 0.906 | Accuracy: 0.647500 | 1.501 sec/iter\n",
      "Epoch: 159 | Batch: 003 / 011 | Total loss: 0.848 | Reg loss: 0.026 | Tree loss: 0.848 | Accuracy: 0.661000 | 1.5 sec/iter\n",
      "Epoch: 159 | Batch: 004 / 011 | Total loss: 0.841 | Reg loss: 0.026 | Tree loss: 0.841 | Accuracy: 0.666500 | 1.5 sec/iter\n",
      "Epoch: 159 | Batch: 005 / 011 | Total loss: 0.865 | Reg loss: 0.026 | Tree loss: 0.865 | Accuracy: 0.623000 | 1.5 sec/iter\n",
      "Epoch: 159 | Batch: 006 / 011 | Total loss: 0.816 | Reg loss: 0.026 | Tree loss: 0.816 | Accuracy: 0.649500 | 1.5 sec/iter\n",
      "Epoch: 159 | Batch: 007 / 011 | Total loss: 0.850 | Reg loss: 0.026 | Tree loss: 0.850 | Accuracy: 0.608000 | 1.5 sec/iter\n",
      "Epoch: 159 | Batch: 008 / 011 | Total loss: 0.837 | Reg loss: 0.026 | Tree loss: 0.837 | Accuracy: 0.602000 | 1.5 sec/iter\n",
      "Epoch: 159 | Batch: 009 / 011 | Total loss: 0.816 | Reg loss: 0.026 | Tree loss: 0.816 | Accuracy: 0.623500 | 1.499 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 159 | Batch: 010 / 011 | Total loss: 0.773 | Reg loss: 0.026 | Tree loss: 0.773 | Accuracy: 0.641638 | 1.499 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 160 | Batch: 000 / 011 | Total loss: 0.926 | Reg loss: 0.026 | Tree loss: 0.926 | Accuracy: 0.634000 | 1.501 sec/iter\n",
      "Epoch: 160 | Batch: 001 / 011 | Total loss: 0.907 | Reg loss: 0.026 | Tree loss: 0.907 | Accuracy: 0.634000 | 1.5 sec/iter\n",
      "Epoch: 160 | Batch: 002 / 011 | Total loss: 0.850 | Reg loss: 0.026 | Tree loss: 0.850 | Accuracy: 0.677500 | 1.5 sec/iter\n",
      "Epoch: 160 | Batch: 003 / 011 | Total loss: 0.882 | Reg loss: 0.026 | Tree loss: 0.882 | Accuracy: 0.660500 | 1.5 sec/iter\n",
      "Epoch: 160 | Batch: 004 / 011 | Total loss: 0.865 | Reg loss: 0.026 | Tree loss: 0.865 | Accuracy: 0.667500 | 1.5 sec/iter\n",
      "Epoch: 160 | Batch: 005 / 011 | Total loss: 0.840 | Reg loss: 0.026 | Tree loss: 0.840 | Accuracy: 0.664000 | 1.5 sec/iter\n",
      "Epoch: 160 | Batch: 006 / 011 | Total loss: 0.812 | Reg loss: 0.026 | Tree loss: 0.812 | Accuracy: 0.657500 | 1.499 sec/iter\n",
      "Epoch: 160 | Batch: 007 / 011 | Total loss: 0.829 | Reg loss: 0.026 | Tree loss: 0.829 | Accuracy: 0.633000 | 1.499 sec/iter\n",
      "Epoch: 160 | Batch: 008 / 011 | Total loss: 0.829 | Reg loss: 0.026 | Tree loss: 0.829 | Accuracy: 0.604000 | 1.499 sec/iter\n",
      "Epoch: 160 | Batch: 009 / 011 | Total loss: 0.800 | Reg loss: 0.026 | Tree loss: 0.800 | Accuracy: 0.629500 | 1.499 sec/iter\n",
      "Epoch: 160 | Batch: 010 / 011 | Total loss: 0.844 | Reg loss: 0.026 | Tree loss: 0.844 | Accuracy: 0.590444 | 1.499 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 161 | Batch: 000 / 011 | Total loss: 0.899 | Reg loss: 0.026 | Tree loss: 0.899 | Accuracy: 0.661500 | 1.5 sec/iter\n",
      "Epoch: 161 | Batch: 001 / 011 | Total loss: 0.874 | Reg loss: 0.026 | Tree loss: 0.874 | Accuracy: 0.671500 | 1.5 sec/iter\n",
      "Epoch: 161 | Batch: 002 / 011 | Total loss: 0.895 | Reg loss: 0.026 | Tree loss: 0.895 | Accuracy: 0.639000 | 1.5 sec/iter\n",
      "Epoch: 161 | Batch: 003 / 011 | Total loss: 0.851 | Reg loss: 0.026 | Tree loss: 0.851 | Accuracy: 0.663000 | 1.5 sec/iter\n",
      "Epoch: 161 | Batch: 004 / 011 | Total loss: 0.862 | Reg loss: 0.026 | Tree loss: 0.862 | Accuracy: 0.650500 | 1.5 sec/iter\n",
      "Epoch: 161 | Batch: 005 / 011 | Total loss: 0.816 | Reg loss: 0.026 | Tree loss: 0.816 | Accuracy: 0.673000 | 1.499 sec/iter\n",
      "Epoch: 161 | Batch: 006 / 011 | Total loss: 0.834 | Reg loss: 0.026 | Tree loss: 0.834 | Accuracy: 0.647500 | 1.499 sec/iter\n",
      "Epoch: 161 | Batch: 007 / 011 | Total loss: 0.839 | Reg loss: 0.026 | Tree loss: 0.839 | Accuracy: 0.611500 | 1.499 sec/iter\n",
      "Epoch: 161 | Batch: 008 / 011 | Total loss: 0.827 | Reg loss: 0.026 | Tree loss: 0.827 | Accuracy: 0.611000 | 1.499 sec/iter\n",
      "Epoch: 161 | Batch: 009 / 011 | Total loss: 0.870 | Reg loss: 0.026 | Tree loss: 0.870 | Accuracy: 0.571500 | 1.499 sec/iter\n",
      "Epoch: 161 | Batch: 010 / 011 | Total loss: 0.800 | Reg loss: 0.026 | Tree loss: 0.800 | Accuracy: 0.627986 | 1.498 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 162 | Batch: 000 / 011 | Total loss: 0.895 | Reg loss: 0.026 | Tree loss: 0.895 | Accuracy: 0.644500 | 1.501 sec/iter\n",
      "Epoch: 162 | Batch: 001 / 011 | Total loss: 0.899 | Reg loss: 0.026 | Tree loss: 0.899 | Accuracy: 0.643500 | 1.501 sec/iter\n",
      "Epoch: 162 | Batch: 002 / 011 | Total loss: 0.865 | Reg loss: 0.026 | Tree loss: 0.865 | Accuracy: 0.659000 | 1.501 sec/iter\n",
      "Epoch: 162 | Batch: 003 / 011 | Total loss: 0.851 | Reg loss: 0.026 | Tree loss: 0.851 | Accuracy: 0.676500 | 1.5 sec/iter\n",
      "Epoch: 162 | Batch: 004 / 011 | Total loss: 0.859 | Reg loss: 0.026 | Tree loss: 0.859 | Accuracy: 0.658000 | 1.5 sec/iter\n",
      "Epoch: 162 | Batch: 005 / 011 | Total loss: 0.843 | Reg loss: 0.026 | Tree loss: 0.843 | Accuracy: 0.656500 | 1.5 sec/iter\n",
      "Epoch: 162 | Batch: 006 / 011 | Total loss: 0.856 | Reg loss: 0.026 | Tree loss: 0.856 | Accuracy: 0.618500 | 1.5 sec/iter\n",
      "Epoch: 162 | Batch: 007 / 011 | Total loss: 0.846 | Reg loss: 0.026 | Tree loss: 0.846 | Accuracy: 0.601500 | 1.499 sec/iter\n",
      "Epoch: 162 | Batch: 008 / 011 | Total loss: 0.845 | Reg loss: 0.026 | Tree loss: 0.845 | Accuracy: 0.599500 | 1.499 sec/iter\n",
      "Epoch: 162 | Batch: 009 / 011 | Total loss: 0.804 | Reg loss: 0.026 | Tree loss: 0.804 | Accuracy: 0.654500 | 1.499 sec/iter\n",
      "Epoch: 162 | Batch: 010 / 011 | Total loss: 0.813 | Reg loss: 0.026 | Tree loss: 0.813 | Accuracy: 0.631399 | 1.499 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 163 | Batch: 000 / 011 | Total loss: 0.907 | Reg loss: 0.026 | Tree loss: 0.907 | Accuracy: 0.635000 | 1.5 sec/iter\n",
      "Epoch: 163 | Batch: 001 / 011 | Total loss: 0.902 | Reg loss: 0.026 | Tree loss: 0.902 | Accuracy: 0.647500 | 1.499 sec/iter\n",
      "Epoch: 163 | Batch: 002 / 011 | Total loss: 0.861 | Reg loss: 0.026 | Tree loss: 0.861 | Accuracy: 0.667000 | 1.499 sec/iter\n",
      "Epoch: 163 | Batch: 003 / 011 | Total loss: 0.878 | Reg loss: 0.026 | Tree loss: 0.878 | Accuracy: 0.650500 | 1.499 sec/iter\n",
      "Epoch: 163 | Batch: 004 / 011 | Total loss: 0.847 | Reg loss: 0.026 | Tree loss: 0.847 | Accuracy: 0.641500 | 1.498 sec/iter\n",
      "Epoch: 163 | Batch: 005 / 011 | Total loss: 0.859 | Reg loss: 0.026 | Tree loss: 0.859 | Accuracy: 0.636500 | 1.498 sec/iter\n",
      "Epoch: 163 | Batch: 006 / 011 | Total loss: 0.830 | Reg loss: 0.026 | Tree loss: 0.830 | Accuracy: 0.646000 | 1.498 sec/iter\n",
      "Epoch: 163 | Batch: 007 / 011 | Total loss: 0.811 | Reg loss: 0.026 | Tree loss: 0.811 | Accuracy: 0.634500 | 1.498 sec/iter\n",
      "Epoch: 163 | Batch: 008 / 011 | Total loss: 0.826 | Reg loss: 0.026 | Tree loss: 0.826 | Accuracy: 0.605000 | 1.498 sec/iter\n",
      "Epoch: 163 | Batch: 009 / 011 | Total loss: 0.816 | Reg loss: 0.026 | Tree loss: 0.816 | Accuracy: 0.622500 | 1.498 sec/iter\n",
      "Epoch: 163 | Batch: 010 / 011 | Total loss: 0.890 | Reg loss: 0.026 | Tree loss: 0.890 | Accuracy: 0.604096 | 1.497 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 164 | Batch: 000 / 011 | Total loss: 0.898 | Reg loss: 0.026 | Tree loss: 0.898 | Accuracy: 0.648500 | 1.5 sec/iter\n",
      "Epoch: 164 | Batch: 001 / 011 | Total loss: 0.900 | Reg loss: 0.026 | Tree loss: 0.900 | Accuracy: 0.629000 | 1.5 sec/iter\n",
      "Epoch: 164 | Batch: 002 / 011 | Total loss: 0.880 | Reg loss: 0.026 | Tree loss: 0.880 | Accuracy: 0.659500 | 1.5 sec/iter\n",
      "Epoch: 164 | Batch: 003 / 011 | Total loss: 0.884 | Reg loss: 0.026 | Tree loss: 0.884 | Accuracy: 0.646500 | 1.5 sec/iter\n",
      "Epoch: 164 | Batch: 004 / 011 | Total loss: 0.832 | Reg loss: 0.026 | Tree loss: 0.832 | Accuracy: 0.671000 | 1.499 sec/iter\n",
      "Epoch: 164 | Batch: 005 / 011 | Total loss: 0.867 | Reg loss: 0.026 | Tree loss: 0.867 | Accuracy: 0.615500 | 1.499 sec/iter\n",
      "Epoch: 164 | Batch: 006 / 011 | Total loss: 0.826 | Reg loss: 0.026 | Tree loss: 0.826 | Accuracy: 0.633500 | 1.499 sec/iter\n",
      "Epoch: 164 | Batch: 007 / 011 | Total loss: 0.814 | Reg loss: 0.026 | Tree loss: 0.814 | Accuracy: 0.638500 | 1.499 sec/iter\n",
      "Epoch: 164 | Batch: 008 / 011 | Total loss: 0.827 | Reg loss: 0.026 | Tree loss: 0.827 | Accuracy: 0.617000 | 1.499 sec/iter\n",
      "Epoch: 164 | Batch: 009 / 011 | Total loss: 0.858 | Reg loss: 0.026 | Tree loss: 0.858 | Accuracy: 0.582500 | 1.499 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 164 | Batch: 010 / 011 | Total loss: 0.777 | Reg loss: 0.026 | Tree loss: 0.777 | Accuracy: 0.658703 | 1.499 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 165 | Batch: 000 / 011 | Total loss: 0.883 | Reg loss: 0.026 | Tree loss: 0.883 | Accuracy: 0.650000 | 1.5 sec/iter\n",
      "Epoch: 165 | Batch: 001 / 011 | Total loss: 0.907 | Reg loss: 0.026 | Tree loss: 0.907 | Accuracy: 0.632500 | 1.5 sec/iter\n",
      "Epoch: 165 | Batch: 002 / 011 | Total loss: 0.892 | Reg loss: 0.026 | Tree loss: 0.892 | Accuracy: 0.656500 | 1.5 sec/iter\n",
      "Epoch: 165 | Batch: 003 / 011 | Total loss: 0.846 | Reg loss: 0.026 | Tree loss: 0.846 | Accuracy: 0.670500 | 1.5 sec/iter\n",
      "Epoch: 165 | Batch: 004 / 011 | Total loss: 0.852 | Reg loss: 0.026 | Tree loss: 0.852 | Accuracy: 0.663000 | 1.499 sec/iter\n",
      "Epoch: 165 | Batch: 005 / 011 | Total loss: 0.845 | Reg loss: 0.026 | Tree loss: 0.845 | Accuracy: 0.650000 | 1.499 sec/iter\n",
      "Epoch: 165 | Batch: 006 / 011 | Total loss: 0.843 | Reg loss: 0.026 | Tree loss: 0.843 | Accuracy: 0.634500 | 1.499 sec/iter\n",
      "Epoch: 165 | Batch: 007 / 011 | Total loss: 0.814 | Reg loss: 0.026 | Tree loss: 0.814 | Accuracy: 0.616500 | 1.499 sec/iter\n",
      "Epoch: 165 | Batch: 008 / 011 | Total loss: 0.837 | Reg loss: 0.026 | Tree loss: 0.837 | Accuracy: 0.615000 | 1.499 sec/iter\n",
      "Epoch: 165 | Batch: 009 / 011 | Total loss: 0.844 | Reg loss: 0.026 | Tree loss: 0.844 | Accuracy: 0.608000 | 1.499 sec/iter\n",
      "Epoch: 165 | Batch: 010 / 011 | Total loss: 0.777 | Reg loss: 0.026 | Tree loss: 0.777 | Accuracy: 0.665529 | 1.499 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 166 | Batch: 000 / 011 | Total loss: 0.913 | Reg loss: 0.026 | Tree loss: 0.913 | Accuracy: 0.647500 | 1.501 sec/iter\n",
      "Epoch: 166 | Batch: 001 / 011 | Total loss: 0.881 | Reg loss: 0.026 | Tree loss: 0.881 | Accuracy: 0.659000 | 1.5 sec/iter\n",
      "Epoch: 166 | Batch: 002 / 011 | Total loss: 0.877 | Reg loss: 0.026 | Tree loss: 0.877 | Accuracy: 0.656500 | 1.5 sec/iter\n",
      "Epoch: 166 | Batch: 003 / 011 | Total loss: 0.852 | Reg loss: 0.026 | Tree loss: 0.852 | Accuracy: 0.655500 | 1.5 sec/iter\n",
      "Epoch: 166 | Batch: 004 / 011 | Total loss: 0.841 | Reg loss: 0.026 | Tree loss: 0.841 | Accuracy: 0.668000 | 1.5 sec/iter\n",
      "Epoch: 166 | Batch: 005 / 011 | Total loss: 0.891 | Reg loss: 0.026 | Tree loss: 0.891 | Accuracy: 0.617500 | 1.499 sec/iter\n",
      "Epoch: 166 | Batch: 006 / 011 | Total loss: 0.826 | Reg loss: 0.026 | Tree loss: 0.826 | Accuracy: 0.630000 | 1.499 sec/iter\n",
      "Epoch: 166 | Batch: 007 / 011 | Total loss: 0.810 | Reg loss: 0.026 | Tree loss: 0.810 | Accuracy: 0.624000 | 1.499 sec/iter\n",
      "Epoch: 166 | Batch: 008 / 011 | Total loss: 0.815 | Reg loss: 0.026 | Tree loss: 0.815 | Accuracy: 0.647500 | 1.499 sec/iter\n",
      "Epoch: 166 | Batch: 009 / 011 | Total loss: 0.809 | Reg loss: 0.026 | Tree loss: 0.809 | Accuracy: 0.637500 | 1.499 sec/iter\n",
      "Epoch: 166 | Batch: 010 / 011 | Total loss: 0.854 | Reg loss: 0.026 | Tree loss: 0.854 | Accuracy: 0.645051 | 1.499 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 167 | Batch: 000 / 011 | Total loss: 0.930 | Reg loss: 0.026 | Tree loss: 0.930 | Accuracy: 0.627500 | 1.501 sec/iter\n",
      "Epoch: 167 | Batch: 001 / 011 | Total loss: 0.874 | Reg loss: 0.026 | Tree loss: 0.874 | Accuracy: 0.655000 | 1.5 sec/iter\n",
      "Epoch: 167 | Batch: 002 / 011 | Total loss: 0.887 | Reg loss: 0.026 | Tree loss: 0.887 | Accuracy: 0.655500 | 1.5 sec/iter\n",
      "Epoch: 167 | Batch: 003 / 011 | Total loss: 0.853 | Reg loss: 0.026 | Tree loss: 0.853 | Accuracy: 0.660500 | 1.5 sec/iter\n",
      "Epoch: 167 | Batch: 004 / 011 | Total loss: 0.844 | Reg loss: 0.026 | Tree loss: 0.844 | Accuracy: 0.660000 | 1.5 sec/iter\n",
      "Epoch: 167 | Batch: 005 / 011 | Total loss: 0.820 | Reg loss: 0.026 | Tree loss: 0.820 | Accuracy: 0.656500 | 1.499 sec/iter\n",
      "Epoch: 167 | Batch: 006 / 011 | Total loss: 0.820 | Reg loss: 0.026 | Tree loss: 0.820 | Accuracy: 0.634000 | 1.499 sec/iter\n",
      "Epoch: 167 | Batch: 007 / 011 | Total loss: 0.849 | Reg loss: 0.026 | Tree loss: 0.849 | Accuracy: 0.624500 | 1.499 sec/iter\n",
      "Epoch: 167 | Batch: 008 / 011 | Total loss: 0.821 | Reg loss: 0.026 | Tree loss: 0.821 | Accuracy: 0.626000 | 1.499 sec/iter\n",
      "Epoch: 167 | Batch: 009 / 011 | Total loss: 0.835 | Reg loss: 0.026 | Tree loss: 0.835 | Accuracy: 0.614500 | 1.499 sec/iter\n",
      "Epoch: 167 | Batch: 010 / 011 | Total loss: 0.822 | Reg loss: 0.026 | Tree loss: 0.822 | Accuracy: 0.648464 | 1.499 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 168 | Batch: 000 / 011 | Total loss: 0.908 | Reg loss: 0.026 | Tree loss: 0.908 | Accuracy: 0.629000 | 1.501 sec/iter\n",
      "Epoch: 168 | Batch: 001 / 011 | Total loss: 0.892 | Reg loss: 0.026 | Tree loss: 0.892 | Accuracy: 0.636500 | 1.501 sec/iter\n",
      "Epoch: 168 | Batch: 002 / 011 | Total loss: 0.865 | Reg loss: 0.026 | Tree loss: 0.865 | Accuracy: 0.654500 | 1.5 sec/iter\n",
      "Epoch: 168 | Batch: 003 / 011 | Total loss: 0.877 | Reg loss: 0.026 | Tree loss: 0.877 | Accuracy: 0.644500 | 1.5 sec/iter\n",
      "Epoch: 168 | Batch: 004 / 011 | Total loss: 0.853 | Reg loss: 0.026 | Tree loss: 0.853 | Accuracy: 0.659500 | 1.5 sec/iter\n",
      "Epoch: 168 | Batch: 005 / 011 | Total loss: 0.821 | Reg loss: 0.026 | Tree loss: 0.821 | Accuracy: 0.677000 | 1.5 sec/iter\n",
      "Epoch: 168 | Batch: 006 / 011 | Total loss: 0.812 | Reg loss: 0.026 | Tree loss: 0.812 | Accuracy: 0.657500 | 1.499 sec/iter\n",
      "Epoch: 168 | Batch: 007 / 011 | Total loss: 0.843 | Reg loss: 0.026 | Tree loss: 0.843 | Accuracy: 0.599500 | 1.499 sec/iter\n",
      "Epoch: 168 | Batch: 008 / 011 | Total loss: 0.842 | Reg loss: 0.026 | Tree loss: 0.842 | Accuracy: 0.605000 | 1.499 sec/iter\n",
      "Epoch: 168 | Batch: 009 / 011 | Total loss: 0.824 | Reg loss: 0.026 | Tree loss: 0.824 | Accuracy: 0.620000 | 1.499 sec/iter\n",
      "Epoch: 168 | Batch: 010 / 011 | Total loss: 0.836 | Reg loss: 0.026 | Tree loss: 0.836 | Accuracy: 0.587031 | 1.499 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 169 | Batch: 000 / 011 | Total loss: 0.894 | Reg loss: 0.026 | Tree loss: 0.894 | Accuracy: 0.643000 | 1.499 sec/iter\n",
      "Epoch: 169 | Batch: 001 / 011 | Total loss: 0.897 | Reg loss: 0.026 | Tree loss: 0.897 | Accuracy: 0.646000 | 1.499 sec/iter\n",
      "Epoch: 169 | Batch: 002 / 011 | Total loss: 0.862 | Reg loss: 0.026 | Tree loss: 0.862 | Accuracy: 0.656000 | 1.499 sec/iter\n",
      "Epoch: 169 | Batch: 003 / 011 | Total loss: 0.874 | Reg loss: 0.026 | Tree loss: 0.874 | Accuracy: 0.655500 | 1.499 sec/iter\n",
      "Epoch: 169 | Batch: 004 / 011 | Total loss: 0.865 | Reg loss: 0.026 | Tree loss: 0.865 | Accuracy: 0.641500 | 1.498 sec/iter\n",
      "Epoch: 169 | Batch: 005 / 011 | Total loss: 0.825 | Reg loss: 0.026 | Tree loss: 0.825 | Accuracy: 0.658000 | 1.498 sec/iter\n",
      "Epoch: 169 | Batch: 006 / 011 | Total loss: 0.840 | Reg loss: 0.026 | Tree loss: 0.840 | Accuracy: 0.628000 | 1.498 sec/iter\n",
      "Epoch: 169 | Batch: 007 / 011 | Total loss: 0.854 | Reg loss: 0.026 | Tree loss: 0.854 | Accuracy: 0.597500 | 1.498 sec/iter\n",
      "Epoch: 169 | Batch: 008 / 011 | Total loss: 0.813 | Reg loss: 0.026 | Tree loss: 0.813 | Accuracy: 0.628500 | 1.498 sec/iter\n",
      "Epoch: 169 | Batch: 009 / 011 | Total loss: 0.823 | Reg loss: 0.026 | Tree loss: 0.823 | Accuracy: 0.622500 | 1.498 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 169 | Batch: 010 / 011 | Total loss: 0.805 | Reg loss: 0.026 | Tree loss: 0.805 | Accuracy: 0.634812 | 1.498 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 170 | Batch: 000 / 011 | Total loss: 0.918 | Reg loss: 0.026 | Tree loss: 0.918 | Accuracy: 0.635000 | 1.499 sec/iter\n",
      "Epoch: 170 | Batch: 001 / 011 | Total loss: 0.860 | Reg loss: 0.026 | Tree loss: 0.860 | Accuracy: 0.663000 | 1.499 sec/iter\n",
      "Epoch: 170 | Batch: 002 / 011 | Total loss: 0.892 | Reg loss: 0.026 | Tree loss: 0.892 | Accuracy: 0.643000 | 1.499 sec/iter\n",
      "Epoch: 170 | Batch: 003 / 011 | Total loss: 0.858 | Reg loss: 0.026 | Tree loss: 0.858 | Accuracy: 0.667000 | 1.499 sec/iter\n",
      "Epoch: 170 | Batch: 004 / 011 | Total loss: 0.857 | Reg loss: 0.026 | Tree loss: 0.857 | Accuracy: 0.663000 | 1.499 sec/iter\n",
      "Epoch: 170 | Batch: 005 / 011 | Total loss: 0.827 | Reg loss: 0.026 | Tree loss: 0.827 | Accuracy: 0.649000 | 1.498 sec/iter\n",
      "Epoch: 170 | Batch: 006 / 011 | Total loss: 0.848 | Reg loss: 0.026 | Tree loss: 0.848 | Accuracy: 0.621000 | 1.498 sec/iter\n",
      "Epoch: 170 | Batch: 007 / 011 | Total loss: 0.840 | Reg loss: 0.026 | Tree loss: 0.840 | Accuracy: 0.624000 | 1.498 sec/iter\n",
      "Epoch: 170 | Batch: 008 / 011 | Total loss: 0.813 | Reg loss: 0.026 | Tree loss: 0.813 | Accuracy: 0.642500 | 1.498 sec/iter\n",
      "Epoch: 170 | Batch: 009 / 011 | Total loss: 0.814 | Reg loss: 0.026 | Tree loss: 0.814 | Accuracy: 0.640500 | 1.498 sec/iter\n",
      "Epoch: 170 | Batch: 010 / 011 | Total loss: 0.814 | Reg loss: 0.026 | Tree loss: 0.814 | Accuracy: 0.648464 | 1.497 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 171 | Batch: 000 / 011 | Total loss: 0.917 | Reg loss: 0.026 | Tree loss: 0.917 | Accuracy: 0.635000 | 1.499 sec/iter\n",
      "Epoch: 171 | Batch: 001 / 011 | Total loss: 0.886 | Reg loss: 0.026 | Tree loss: 0.886 | Accuracy: 0.665500 | 1.499 sec/iter\n",
      "Epoch: 171 | Batch: 002 / 011 | Total loss: 0.880 | Reg loss: 0.026 | Tree loss: 0.880 | Accuracy: 0.650000 | 1.499 sec/iter\n",
      "Epoch: 171 | Batch: 003 / 011 | Total loss: 0.843 | Reg loss: 0.026 | Tree loss: 0.843 | Accuracy: 0.667500 | 1.499 sec/iter\n",
      "Epoch: 171 | Batch: 004 / 011 | Total loss: 0.874 | Reg loss: 0.026 | Tree loss: 0.874 | Accuracy: 0.645500 | 1.499 sec/iter\n",
      "Epoch: 171 | Batch: 005 / 011 | Total loss: 0.825 | Reg loss: 0.026 | Tree loss: 0.825 | Accuracy: 0.635500 | 1.499 sec/iter\n",
      "Epoch: 171 | Batch: 006 / 011 | Total loss: 0.832 | Reg loss: 0.026 | Tree loss: 0.832 | Accuracy: 0.629500 | 1.498 sec/iter\n",
      "Epoch: 171 | Batch: 007 / 011 | Total loss: 0.811 | Reg loss: 0.026 | Tree loss: 0.811 | Accuracy: 0.637500 | 1.498 sec/iter\n",
      "Epoch: 171 | Batch: 008 / 011 | Total loss: 0.843 | Reg loss: 0.026 | Tree loss: 0.843 | Accuracy: 0.616000 | 1.498 sec/iter\n",
      "Epoch: 171 | Batch: 009 / 011 | Total loss: 0.813 | Reg loss: 0.026 | Tree loss: 0.813 | Accuracy: 0.635000 | 1.498 sec/iter\n",
      "Epoch: 171 | Batch: 010 / 011 | Total loss: 0.784 | Reg loss: 0.026 | Tree loss: 0.784 | Accuracy: 0.645051 | 1.498 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 172 | Batch: 000 / 011 | Total loss: 0.918 | Reg loss: 0.026 | Tree loss: 0.918 | Accuracy: 0.616000 | 1.5 sec/iter\n",
      "Epoch: 172 | Batch: 001 / 011 | Total loss: 0.882 | Reg loss: 0.026 | Tree loss: 0.882 | Accuracy: 0.652000 | 1.5 sec/iter\n",
      "Epoch: 172 | Batch: 002 / 011 | Total loss: 0.876 | Reg loss: 0.026 | Tree loss: 0.876 | Accuracy: 0.660000 | 1.5 sec/iter\n",
      "Epoch: 172 | Batch: 003 / 011 | Total loss: 0.882 | Reg loss: 0.026 | Tree loss: 0.882 | Accuracy: 0.636000 | 1.5 sec/iter\n",
      "Epoch: 172 | Batch: 004 / 011 | Total loss: 0.826 | Reg loss: 0.026 | Tree loss: 0.826 | Accuracy: 0.669000 | 1.499 sec/iter\n",
      "Epoch: 172 | Batch: 005 / 011 | Total loss: 0.832 | Reg loss: 0.026 | Tree loss: 0.832 | Accuracy: 0.670000 | 1.499 sec/iter\n",
      "Epoch: 172 | Batch: 006 / 011 | Total loss: 0.826 | Reg loss: 0.026 | Tree loss: 0.826 | Accuracy: 0.634000 | 1.499 sec/iter\n",
      "Epoch: 172 | Batch: 007 / 011 | Total loss: 0.837 | Reg loss: 0.026 | Tree loss: 0.837 | Accuracy: 0.618500 | 1.499 sec/iter\n",
      "Epoch: 172 | Batch: 008 / 011 | Total loss: 0.823 | Reg loss: 0.026 | Tree loss: 0.823 | Accuracy: 0.603500 | 1.499 sec/iter\n",
      "Epoch: 172 | Batch: 009 / 011 | Total loss: 0.843 | Reg loss: 0.026 | Tree loss: 0.843 | Accuracy: 0.610000 | 1.498 sec/iter\n",
      "Epoch: 172 | Batch: 010 / 011 | Total loss: 0.792 | Reg loss: 0.026 | Tree loss: 0.792 | Accuracy: 0.641638 | 1.498 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 173 | Batch: 000 / 011 | Total loss: 0.866 | Reg loss: 0.026 | Tree loss: 0.866 | Accuracy: 0.658000 | 1.5 sec/iter\n",
      "Epoch: 173 | Batch: 001 / 011 | Total loss: 0.890 | Reg loss: 0.026 | Tree loss: 0.890 | Accuracy: 0.645000 | 1.5 sec/iter\n",
      "Epoch: 173 | Batch: 002 / 011 | Total loss: 0.883 | Reg loss: 0.026 | Tree loss: 0.883 | Accuracy: 0.657500 | 1.5 sec/iter\n",
      "Epoch: 173 | Batch: 003 / 011 | Total loss: 0.869 | Reg loss: 0.026 | Tree loss: 0.869 | Accuracy: 0.647000 | 1.5 sec/iter\n",
      "Epoch: 173 | Batch: 004 / 011 | Total loss: 0.838 | Reg loss: 0.026 | Tree loss: 0.838 | Accuracy: 0.678000 | 1.499 sec/iter\n",
      "Epoch: 173 | Batch: 005 / 011 | Total loss: 0.821 | Reg loss: 0.026 | Tree loss: 0.821 | Accuracy: 0.653500 | 1.499 sec/iter\n",
      "Epoch: 173 | Batch: 006 / 011 | Total loss: 0.821 | Reg loss: 0.026 | Tree loss: 0.821 | Accuracy: 0.631500 | 1.499 sec/iter\n",
      "Epoch: 173 | Batch: 007 / 011 | Total loss: 0.847 | Reg loss: 0.026 | Tree loss: 0.847 | Accuracy: 0.610000 | 1.499 sec/iter\n",
      "Epoch: 173 | Batch: 008 / 011 | Total loss: 0.845 | Reg loss: 0.026 | Tree loss: 0.845 | Accuracy: 0.588500 | 1.498 sec/iter\n",
      "Epoch: 173 | Batch: 009 / 011 | Total loss: 0.856 | Reg loss: 0.026 | Tree loss: 0.856 | Accuracy: 0.591000 | 1.498 sec/iter\n",
      "Epoch: 173 | Batch: 010 / 011 | Total loss: 0.788 | Reg loss: 0.026 | Tree loss: 0.788 | Accuracy: 0.668942 | 1.498 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 174 | Batch: 000 / 011 | Total loss: 0.911 | Reg loss: 0.026 | Tree loss: 0.911 | Accuracy: 0.626000 | 1.5 sec/iter\n",
      "Epoch: 174 | Batch: 001 / 011 | Total loss: 0.898 | Reg loss: 0.026 | Tree loss: 0.898 | Accuracy: 0.645000 | 1.5 sec/iter\n",
      "Epoch: 174 | Batch: 002 / 011 | Total loss: 0.887 | Reg loss: 0.026 | Tree loss: 0.887 | Accuracy: 0.658500 | 1.5 sec/iter\n",
      "Epoch: 174 | Batch: 003 / 011 | Total loss: 0.873 | Reg loss: 0.026 | Tree loss: 0.873 | Accuracy: 0.644000 | 1.5 sec/iter\n",
      "Epoch: 174 | Batch: 004 / 011 | Total loss: 0.845 | Reg loss: 0.026 | Tree loss: 0.845 | Accuracy: 0.659000 | 1.5 sec/iter\n",
      "Epoch: 174 | Batch: 005 / 011 | Total loss: 0.835 | Reg loss: 0.026 | Tree loss: 0.835 | Accuracy: 0.671000 | 1.5 sec/iter\n",
      "Epoch: 174 | Batch: 006 / 011 | Total loss: 0.840 | Reg loss: 0.026 | Tree loss: 0.840 | Accuracy: 0.640500 | 1.499 sec/iter\n",
      "Epoch: 174 | Batch: 007 / 011 | Total loss: 0.810 | Reg loss: 0.026 | Tree loss: 0.810 | Accuracy: 0.627000 | 1.499 sec/iter\n",
      "Epoch: 174 | Batch: 008 / 011 | Total loss: 0.821 | Reg loss: 0.026 | Tree loss: 0.821 | Accuracy: 0.617500 | 1.499 sec/iter\n",
      "Epoch: 174 | Batch: 009 / 011 | Total loss: 0.807 | Reg loss: 0.026 | Tree loss: 0.807 | Accuracy: 0.628000 | 1.499 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 174 | Batch: 010 / 011 | Total loss: 0.870 | Reg loss: 0.026 | Tree loss: 0.870 | Accuracy: 0.621160 | 1.499 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 175 | Batch: 000 / 011 | Total loss: 0.899 | Reg loss: 0.026 | Tree loss: 0.899 | Accuracy: 0.635000 | 1.499 sec/iter\n",
      "Epoch: 175 | Batch: 001 / 011 | Total loss: 0.905 | Reg loss: 0.026 | Tree loss: 0.905 | Accuracy: 0.632000 | 1.499 sec/iter\n",
      "Epoch: 175 | Batch: 002 / 011 | Total loss: 0.882 | Reg loss: 0.026 | Tree loss: 0.882 | Accuracy: 0.662000 | 1.499 sec/iter\n",
      "Epoch: 175 | Batch: 003 / 011 | Total loss: 0.851 | Reg loss: 0.026 | Tree loss: 0.851 | Accuracy: 0.663000 | 1.498 sec/iter\n",
      "Epoch: 175 | Batch: 004 / 011 | Total loss: 0.824 | Reg loss: 0.026 | Tree loss: 0.824 | Accuracy: 0.685500 | 1.498 sec/iter\n",
      "Epoch: 175 | Batch: 005 / 011 | Total loss: 0.853 | Reg loss: 0.026 | Tree loss: 0.853 | Accuracy: 0.651000 | 1.498 sec/iter\n",
      "Epoch: 175 | Batch: 006 / 011 | Total loss: 0.823 | Reg loss: 0.026 | Tree loss: 0.823 | Accuracy: 0.618500 | 1.498 sec/iter\n",
      "Epoch: 175 | Batch: 007 / 011 | Total loss: 0.837 | Reg loss: 0.026 | Tree loss: 0.837 | Accuracy: 0.618500 | 1.497 sec/iter\n",
      "Epoch: 175 | Batch: 008 / 011 | Total loss: 0.824 | Reg loss: 0.026 | Tree loss: 0.824 | Accuracy: 0.606000 | 1.497 sec/iter\n",
      "Epoch: 175 | Batch: 009 / 011 | Total loss: 0.828 | Reg loss: 0.026 | Tree loss: 0.828 | Accuracy: 0.611000 | 1.497 sec/iter\n",
      "Epoch: 175 | Batch: 010 / 011 | Total loss: 0.805 | Reg loss: 0.026 | Tree loss: 0.805 | Accuracy: 0.641638 | 1.497 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 176 | Batch: 000 / 011 | Total loss: 0.928 | Reg loss: 0.026 | Tree loss: 0.928 | Accuracy: 0.625500 | 1.5 sec/iter\n",
      "Epoch: 176 | Batch: 001 / 011 | Total loss: 0.873 | Reg loss: 0.026 | Tree loss: 0.873 | Accuracy: 0.659000 | 1.499 sec/iter\n",
      "Epoch: 176 | Batch: 002 / 011 | Total loss: 0.886 | Reg loss: 0.026 | Tree loss: 0.886 | Accuracy: 0.658000 | 1.499 sec/iter\n",
      "Epoch: 176 | Batch: 003 / 011 | Total loss: 0.868 | Reg loss: 0.026 | Tree loss: 0.868 | Accuracy: 0.641000 | 1.499 sec/iter\n",
      "Epoch: 176 | Batch: 004 / 011 | Total loss: 0.861 | Reg loss: 0.026 | Tree loss: 0.861 | Accuracy: 0.651500 | 1.499 sec/iter\n",
      "Epoch: 176 | Batch: 005 / 011 | Total loss: 0.820 | Reg loss: 0.026 | Tree loss: 0.820 | Accuracy: 0.664000 | 1.498 sec/iter\n",
      "Epoch: 176 | Batch: 006 / 011 | Total loss: 0.818 | Reg loss: 0.026 | Tree loss: 0.818 | Accuracy: 0.644500 | 1.498 sec/iter\n",
      "Epoch: 176 | Batch: 007 / 011 | Total loss: 0.826 | Reg loss: 0.026 | Tree loss: 0.826 | Accuracy: 0.619000 | 1.498 sec/iter\n",
      "Epoch: 176 | Batch: 008 / 011 | Total loss: 0.828 | Reg loss: 0.026 | Tree loss: 0.828 | Accuracy: 0.612500 | 1.498 sec/iter\n",
      "Epoch: 176 | Batch: 009 / 011 | Total loss: 0.824 | Reg loss: 0.026 | Tree loss: 0.824 | Accuracy: 0.607500 | 1.498 sec/iter\n",
      "Epoch: 176 | Batch: 010 / 011 | Total loss: 0.815 | Reg loss: 0.026 | Tree loss: 0.815 | Accuracy: 0.624573 | 1.498 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 177 | Batch: 000 / 011 | Total loss: 0.895 | Reg loss: 0.026 | Tree loss: 0.895 | Accuracy: 0.645000 | 1.501 sec/iter\n",
      "Epoch: 177 | Batch: 001 / 011 | Total loss: 0.868 | Reg loss: 0.026 | Tree loss: 0.868 | Accuracy: 0.662000 | 1.5 sec/iter\n",
      "Epoch: 177 | Batch: 002 / 011 | Total loss: 0.900 | Reg loss: 0.026 | Tree loss: 0.900 | Accuracy: 0.641500 | 1.5 sec/iter\n",
      "Epoch: 177 | Batch: 003 / 011 | Total loss: 0.872 | Reg loss: 0.026 | Tree loss: 0.872 | Accuracy: 0.644000 | 1.5 sec/iter\n",
      "Epoch: 177 | Batch: 004 / 011 | Total loss: 0.845 | Reg loss: 0.026 | Tree loss: 0.845 | Accuracy: 0.640500 | 1.5 sec/iter\n",
      "Epoch: 177 | Batch: 005 / 011 | Total loss: 0.830 | Reg loss: 0.026 | Tree loss: 0.830 | Accuracy: 0.665500 | 1.5 sec/iter\n",
      "Epoch: 177 | Batch: 006 / 011 | Total loss: 0.837 | Reg loss: 0.026 | Tree loss: 0.837 | Accuracy: 0.645000 | 1.499 sec/iter\n",
      "Epoch: 177 | Batch: 007 / 011 | Total loss: 0.833 | Reg loss: 0.026 | Tree loss: 0.833 | Accuracy: 0.633000 | 1.5 sec/iter\n",
      "Epoch: 177 | Batch: 008 / 011 | Total loss: 0.811 | Reg loss: 0.026 | Tree loss: 0.811 | Accuracy: 0.615000 | 1.499 sec/iter\n",
      "Epoch: 177 | Batch: 009 / 011 | Total loss: 0.830 | Reg loss: 0.026 | Tree loss: 0.830 | Accuracy: 0.620500 | 1.499 sec/iter\n",
      "Epoch: 177 | Batch: 010 / 011 | Total loss: 0.811 | Reg loss: 0.026 | Tree loss: 0.811 | Accuracy: 0.624573 | 1.499 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 178 | Batch: 000 / 011 | Total loss: 0.901 | Reg loss: 0.026 | Tree loss: 0.901 | Accuracy: 0.622500 | 1.5 sec/iter\n",
      "Epoch: 178 | Batch: 001 / 011 | Total loss: 0.886 | Reg loss: 0.026 | Tree loss: 0.886 | Accuracy: 0.653500 | 1.5 sec/iter\n",
      "Epoch: 178 | Batch: 002 / 011 | Total loss: 0.893 | Reg loss: 0.026 | Tree loss: 0.893 | Accuracy: 0.639500 | 1.499 sec/iter\n",
      "Epoch: 178 | Batch: 003 / 011 | Total loss: 0.847 | Reg loss: 0.026 | Tree loss: 0.847 | Accuracy: 0.662000 | 1.499 sec/iter\n",
      "Epoch: 178 | Batch: 004 / 011 | Total loss: 0.849 | Reg loss: 0.026 | Tree loss: 0.849 | Accuracy: 0.641000 | 1.499 sec/iter\n",
      "Epoch: 178 | Batch: 005 / 011 | Total loss: 0.823 | Reg loss: 0.026 | Tree loss: 0.823 | Accuracy: 0.648000 | 1.499 sec/iter\n",
      "Epoch: 178 | Batch: 006 / 011 | Total loss: 0.841 | Reg loss: 0.026 | Tree loss: 0.841 | Accuracy: 0.631500 | 1.499 sec/iter\n",
      "Epoch: 178 | Batch: 007 / 011 | Total loss: 0.812 | Reg loss: 0.026 | Tree loss: 0.812 | Accuracy: 0.628500 | 1.498 sec/iter\n",
      "Epoch: 178 | Batch: 008 / 011 | Total loss: 0.832 | Reg loss: 0.026 | Tree loss: 0.832 | Accuracy: 0.617500 | 1.498 sec/iter\n",
      "Epoch: 178 | Batch: 009 / 011 | Total loss: 0.821 | Reg loss: 0.026 | Tree loss: 0.821 | Accuracy: 0.632500 | 1.498 sec/iter\n",
      "Epoch: 178 | Batch: 010 / 011 | Total loss: 0.797 | Reg loss: 0.026 | Tree loss: 0.797 | Accuracy: 0.631399 | 1.498 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 179 | Batch: 000 / 011 | Total loss: 0.899 | Reg loss: 0.026 | Tree loss: 0.899 | Accuracy: 0.628500 | 1.5 sec/iter\n",
      "Epoch: 179 | Batch: 001 / 011 | Total loss: 0.921 | Reg loss: 0.026 | Tree loss: 0.921 | Accuracy: 0.630000 | 1.5 sec/iter\n",
      "Epoch: 179 | Batch: 002 / 011 | Total loss: 0.875 | Reg loss: 0.026 | Tree loss: 0.875 | Accuracy: 0.635000 | 1.5 sec/iter\n",
      "Epoch: 179 | Batch: 003 / 011 | Total loss: 0.847 | Reg loss: 0.026 | Tree loss: 0.847 | Accuracy: 0.672500 | 1.5 sec/iter\n",
      "Epoch: 179 | Batch: 004 / 011 | Total loss: 0.838 | Reg loss: 0.026 | Tree loss: 0.838 | Accuracy: 0.664500 | 1.499 sec/iter\n",
      "Epoch: 179 | Batch: 005 / 011 | Total loss: 0.823 | Reg loss: 0.026 | Tree loss: 0.823 | Accuracy: 0.665000 | 1.499 sec/iter\n",
      "Epoch: 179 | Batch: 006 / 011 | Total loss: 0.825 | Reg loss: 0.026 | Tree loss: 0.825 | Accuracy: 0.636500 | 1.499 sec/iter\n",
      "Epoch: 179 | Batch: 007 / 011 | Total loss: 0.833 | Reg loss: 0.026 | Tree loss: 0.833 | Accuracy: 0.620000 | 1.499 sec/iter\n",
      "Epoch: 179 | Batch: 008 / 011 | Total loss: 0.822 | Reg loss: 0.026 | Tree loss: 0.822 | Accuracy: 0.627500 | 1.499 sec/iter\n",
      "Epoch: 179 | Batch: 009 / 011 | Total loss: 0.821 | Reg loss: 0.026 | Tree loss: 0.821 | Accuracy: 0.626500 | 1.499 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 179 | Batch: 010 / 011 | Total loss: 0.831 | Reg loss: 0.026 | Tree loss: 0.831 | Accuracy: 0.651877 | 1.498 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 180 | Batch: 000 / 011 | Total loss: 0.898 | Reg loss: 0.026 | Tree loss: 0.898 | Accuracy: 0.635500 | 1.5 sec/iter\n",
      "Epoch: 180 | Batch: 001 / 011 | Total loss: 0.891 | Reg loss: 0.026 | Tree loss: 0.891 | Accuracy: 0.642500 | 1.5 sec/iter\n",
      "Epoch: 180 | Batch: 002 / 011 | Total loss: 0.854 | Reg loss: 0.026 | Tree loss: 0.854 | Accuracy: 0.665500 | 1.5 sec/iter\n",
      "Epoch: 180 | Batch: 003 / 011 | Total loss: 0.842 | Reg loss: 0.026 | Tree loss: 0.842 | Accuracy: 0.670500 | 1.5 sec/iter\n",
      "Epoch: 180 | Batch: 004 / 011 | Total loss: 0.829 | Reg loss: 0.026 | Tree loss: 0.829 | Accuracy: 0.685000 | 1.5 sec/iter\n",
      "Epoch: 180 | Batch: 005 / 011 | Total loss: 0.862 | Reg loss: 0.026 | Tree loss: 0.862 | Accuracy: 0.625500 | 1.499 sec/iter\n",
      "Epoch: 180 | Batch: 006 / 011 | Total loss: 0.823 | Reg loss: 0.026 | Tree loss: 0.823 | Accuracy: 0.641500 | 1.499 sec/iter\n",
      "Epoch: 180 | Batch: 007 / 011 | Total loss: 0.840 | Reg loss: 0.026 | Tree loss: 0.840 | Accuracy: 0.628500 | 1.499 sec/iter\n",
      "Epoch: 180 | Batch: 008 / 011 | Total loss: 0.825 | Reg loss: 0.026 | Tree loss: 0.825 | Accuracy: 0.623000 | 1.499 sec/iter\n",
      "Epoch: 180 | Batch: 009 / 011 | Total loss: 0.806 | Reg loss: 0.026 | Tree loss: 0.806 | Accuracy: 0.642500 | 1.499 sec/iter\n",
      "Epoch: 180 | Batch: 010 / 011 | Total loss: 0.876 | Reg loss: 0.026 | Tree loss: 0.876 | Accuracy: 0.583618 | 1.498 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 181 | Batch: 000 / 011 | Total loss: 0.886 | Reg loss: 0.026 | Tree loss: 0.886 | Accuracy: 0.645000 | 1.499 sec/iter\n",
      "Epoch: 181 | Batch: 001 / 011 | Total loss: 0.883 | Reg loss: 0.026 | Tree loss: 0.883 | Accuracy: 0.657500 | 1.499 sec/iter\n",
      "Epoch: 181 | Batch: 002 / 011 | Total loss: 0.858 | Reg loss: 0.026 | Tree loss: 0.858 | Accuracy: 0.655000 | 1.499 sec/iter\n",
      "Epoch: 181 | Batch: 003 / 011 | Total loss: 0.866 | Reg loss: 0.026 | Tree loss: 0.866 | Accuracy: 0.657000 | 1.498 sec/iter\n",
      "Epoch: 181 | Batch: 004 / 011 | Total loss: 0.865 | Reg loss: 0.026 | Tree loss: 0.865 | Accuracy: 0.642500 | 1.498 sec/iter\n",
      "Epoch: 181 | Batch: 005 / 011 | Total loss: 0.846 | Reg loss: 0.026 | Tree loss: 0.846 | Accuracy: 0.650500 | 1.498 sec/iter\n",
      "Epoch: 181 | Batch: 006 / 011 | Total loss: 0.833 | Reg loss: 0.026 | Tree loss: 0.833 | Accuracy: 0.626000 | 1.498 sec/iter\n",
      "Epoch: 181 | Batch: 007 / 011 | Total loss: 0.834 | Reg loss: 0.026 | Tree loss: 0.834 | Accuracy: 0.616500 | 1.498 sec/iter\n",
      "Epoch: 181 | Batch: 008 / 011 | Total loss: 0.834 | Reg loss: 0.026 | Tree loss: 0.834 | Accuracy: 0.615000 | 1.498 sec/iter\n",
      "Epoch: 181 | Batch: 009 / 011 | Total loss: 0.823 | Reg loss: 0.026 | Tree loss: 0.823 | Accuracy: 0.623000 | 1.498 sec/iter\n",
      "Epoch: 181 | Batch: 010 / 011 | Total loss: 0.817 | Reg loss: 0.026 | Tree loss: 0.817 | Accuracy: 0.617747 | 1.497 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 182 | Batch: 000 / 011 | Total loss: 0.907 | Reg loss: 0.026 | Tree loss: 0.907 | Accuracy: 0.641000 | 1.5 sec/iter\n",
      "Epoch: 182 | Batch: 001 / 011 | Total loss: 0.880 | Reg loss: 0.026 | Tree loss: 0.880 | Accuracy: 0.650500 | 1.499 sec/iter\n",
      "Epoch: 182 | Batch: 002 / 011 | Total loss: 0.896 | Reg loss: 0.026 | Tree loss: 0.896 | Accuracy: 0.646000 | 1.499 sec/iter\n",
      "Epoch: 182 | Batch: 003 / 011 | Total loss: 0.875 | Reg loss: 0.026 | Tree loss: 0.875 | Accuracy: 0.652000 | 1.499 sec/iter\n",
      "Epoch: 182 | Batch: 004 / 011 | Total loss: 0.862 | Reg loss: 0.026 | Tree loss: 0.862 | Accuracy: 0.644500 | 1.499 sec/iter\n",
      "Epoch: 182 | Batch: 005 / 011 | Total loss: 0.819 | Reg loss: 0.026 | Tree loss: 0.819 | Accuracy: 0.667500 | 1.498 sec/iter\n",
      "Epoch: 182 | Batch: 006 / 011 | Total loss: 0.835 | Reg loss: 0.026 | Tree loss: 0.835 | Accuracy: 0.632000 | 1.498 sec/iter\n",
      "Epoch: 182 | Batch: 007 / 011 | Total loss: 0.803 | Reg loss: 0.026 | Tree loss: 0.803 | Accuracy: 0.638000 | 1.498 sec/iter\n",
      "Epoch: 182 | Batch: 008 / 011 | Total loss: 0.816 | Reg loss: 0.026 | Tree loss: 0.816 | Accuracy: 0.623500 | 1.498 sec/iter\n",
      "Epoch: 182 | Batch: 009 / 011 | Total loss: 0.837 | Reg loss: 0.026 | Tree loss: 0.837 | Accuracy: 0.598000 | 1.498 sec/iter\n",
      "Epoch: 182 | Batch: 010 / 011 | Total loss: 0.811 | Reg loss: 0.026 | Tree loss: 0.811 | Accuracy: 0.593857 | 1.498 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 183 | Batch: 000 / 011 | Total loss: 0.915 | Reg loss: 0.026 | Tree loss: 0.915 | Accuracy: 0.626500 | 1.499 sec/iter\n",
      "Epoch: 183 | Batch: 001 / 011 | Total loss: 0.894 | Reg loss: 0.026 | Tree loss: 0.894 | Accuracy: 0.651000 | 1.499 sec/iter\n",
      "Epoch: 183 | Batch: 002 / 011 | Total loss: 0.856 | Reg loss: 0.026 | Tree loss: 0.856 | Accuracy: 0.664000 | 1.499 sec/iter\n",
      "Epoch: 183 | Batch: 003 / 011 | Total loss: 0.843 | Reg loss: 0.026 | Tree loss: 0.843 | Accuracy: 0.679000 | 1.499 sec/iter\n",
      "Epoch: 183 | Batch: 004 / 011 | Total loss: 0.839 | Reg loss: 0.026 | Tree loss: 0.839 | Accuracy: 0.653500 | 1.499 sec/iter\n",
      "Epoch: 183 | Batch: 005 / 011 | Total loss: 0.832 | Reg loss: 0.026 | Tree loss: 0.832 | Accuracy: 0.647500 | 1.498 sec/iter\n",
      "Epoch: 183 | Batch: 006 / 011 | Total loss: 0.832 | Reg loss: 0.026 | Tree loss: 0.832 | Accuracy: 0.630500 | 1.498 sec/iter\n",
      "Epoch: 183 | Batch: 007 / 011 | Total loss: 0.821 | Reg loss: 0.026 | Tree loss: 0.821 | Accuracy: 0.617000 | 1.498 sec/iter\n",
      "Epoch: 183 | Batch: 008 / 011 | Total loss: 0.834 | Reg loss: 0.026 | Tree loss: 0.834 | Accuracy: 0.634500 | 1.498 sec/iter\n",
      "Epoch: 183 | Batch: 009 / 011 | Total loss: 0.809 | Reg loss: 0.026 | Tree loss: 0.809 | Accuracy: 0.635000 | 1.498 sec/iter\n",
      "Epoch: 183 | Batch: 010 / 011 | Total loss: 0.796 | Reg loss: 0.026 | Tree loss: 0.796 | Accuracy: 0.658703 | 1.498 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 184 | Batch: 000 / 011 | Total loss: 0.888 | Reg loss: 0.026 | Tree loss: 0.888 | Accuracy: 0.657500 | 1.5 sec/iter\n",
      "Epoch: 184 | Batch: 001 / 011 | Total loss: 0.897 | Reg loss: 0.026 | Tree loss: 0.897 | Accuracy: 0.635000 | 1.499 sec/iter\n",
      "Epoch: 184 | Batch: 002 / 011 | Total loss: 0.874 | Reg loss: 0.026 | Tree loss: 0.874 | Accuracy: 0.662500 | 1.499 sec/iter\n",
      "Epoch: 184 | Batch: 003 / 011 | Total loss: 0.868 | Reg loss: 0.026 | Tree loss: 0.868 | Accuracy: 0.646000 | 1.499 sec/iter\n",
      "Epoch: 184 | Batch: 004 / 011 | Total loss: 0.833 | Reg loss: 0.026 | Tree loss: 0.833 | Accuracy: 0.661000 | 1.499 sec/iter\n",
      "Epoch: 184 | Batch: 005 / 011 | Total loss: 0.847 | Reg loss: 0.026 | Tree loss: 0.847 | Accuracy: 0.650500 | 1.499 sec/iter\n",
      "Epoch: 184 | Batch: 006 / 011 | Total loss: 0.832 | Reg loss: 0.026 | Tree loss: 0.832 | Accuracy: 0.633500 | 1.498 sec/iter\n",
      "Epoch: 184 | Batch: 007 / 011 | Total loss: 0.802 | Reg loss: 0.026 | Tree loss: 0.802 | Accuracy: 0.646500 | 1.498 sec/iter\n",
      "Epoch: 184 | Batch: 008 / 011 | Total loss: 0.831 | Reg loss: 0.026 | Tree loss: 0.831 | Accuracy: 0.630500 | 1.498 sec/iter\n",
      "Epoch: 184 | Batch: 009 / 011 | Total loss: 0.822 | Reg loss: 0.026 | Tree loss: 0.822 | Accuracy: 0.626000 | 1.498 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 184 | Batch: 010 / 011 | Total loss: 0.798 | Reg loss: 0.026 | Tree loss: 0.798 | Accuracy: 0.624573 | 1.498 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 185 | Batch: 000 / 011 | Total loss: 0.891 | Reg loss: 0.026 | Tree loss: 0.891 | Accuracy: 0.632500 | 1.5 sec/iter\n",
      "Epoch: 185 | Batch: 001 / 011 | Total loss: 0.878 | Reg loss: 0.026 | Tree loss: 0.878 | Accuracy: 0.670500 | 1.5 sec/iter\n",
      "Epoch: 185 | Batch: 002 / 011 | Total loss: 0.872 | Reg loss: 0.026 | Tree loss: 0.872 | Accuracy: 0.649000 | 1.5 sec/iter\n",
      "Epoch: 185 | Batch: 003 / 011 | Total loss: 0.850 | Reg loss: 0.026 | Tree loss: 0.850 | Accuracy: 0.665500 | 1.499 sec/iter\n",
      "Epoch: 185 | Batch: 004 / 011 | Total loss: 0.860 | Reg loss: 0.026 | Tree loss: 0.860 | Accuracy: 0.642500 | 1.499 sec/iter\n",
      "Epoch: 185 | Batch: 005 / 011 | Total loss: 0.850 | Reg loss: 0.026 | Tree loss: 0.850 | Accuracy: 0.644500 | 1.499 sec/iter\n",
      "Epoch: 185 | Batch: 006 / 011 | Total loss: 0.831 | Reg loss: 0.026 | Tree loss: 0.831 | Accuracy: 0.637500 | 1.499 sec/iter\n",
      "Epoch: 185 | Batch: 007 / 011 | Total loss: 0.814 | Reg loss: 0.026 | Tree loss: 0.814 | Accuracy: 0.612500 | 1.498 sec/iter\n",
      "Epoch: 185 | Batch: 008 / 011 | Total loss: 0.835 | Reg loss: 0.026 | Tree loss: 0.835 | Accuracy: 0.609500 | 1.498 sec/iter\n",
      "Epoch: 185 | Batch: 009 / 011 | Total loss: 0.822 | Reg loss: 0.026 | Tree loss: 0.822 | Accuracy: 0.630000 | 1.498 sec/iter\n",
      "Epoch: 185 | Batch: 010 / 011 | Total loss: 0.820 | Reg loss: 0.026 | Tree loss: 0.820 | Accuracy: 0.631399 | 1.498 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 186 | Batch: 000 / 011 | Total loss: 0.887 | Reg loss: 0.026 | Tree loss: 0.887 | Accuracy: 0.634000 | 1.5 sec/iter\n",
      "Epoch: 186 | Batch: 001 / 011 | Total loss: 0.892 | Reg loss: 0.026 | Tree loss: 0.892 | Accuracy: 0.656000 | 1.5 sec/iter\n",
      "Epoch: 186 | Batch: 002 / 011 | Total loss: 0.856 | Reg loss: 0.026 | Tree loss: 0.856 | Accuracy: 0.662000 | 1.5 sec/iter\n",
      "Epoch: 186 | Batch: 003 / 011 | Total loss: 0.838 | Reg loss: 0.026 | Tree loss: 0.838 | Accuracy: 0.662500 | 1.5 sec/iter\n",
      "Epoch: 186 | Batch: 004 / 011 | Total loss: 0.841 | Reg loss: 0.026 | Tree loss: 0.841 | Accuracy: 0.674000 | 1.5 sec/iter\n",
      "Epoch: 186 | Batch: 005 / 011 | Total loss: 0.846 | Reg loss: 0.026 | Tree loss: 0.846 | Accuracy: 0.650500 | 1.499 sec/iter\n",
      "Epoch: 186 | Batch: 006 / 011 | Total loss: 0.862 | Reg loss: 0.026 | Tree loss: 0.862 | Accuracy: 0.609500 | 1.499 sec/iter\n",
      "Epoch: 186 | Batch: 007 / 011 | Total loss: 0.829 | Reg loss: 0.026 | Tree loss: 0.829 | Accuracy: 0.634000 | 1.499 sec/iter\n",
      "Epoch: 186 | Batch: 008 / 011 | Total loss: 0.831 | Reg loss: 0.026 | Tree loss: 0.831 | Accuracy: 0.617500 | 1.499 sec/iter\n",
      "Epoch: 186 | Batch: 009 / 011 | Total loss: 0.817 | Reg loss: 0.026 | Tree loss: 0.817 | Accuracy: 0.624000 | 1.499 sec/iter\n",
      "Epoch: 186 | Batch: 010 / 011 | Total loss: 0.811 | Reg loss: 0.026 | Tree loss: 0.811 | Accuracy: 0.638225 | 1.498 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 187 | Batch: 000 / 011 | Total loss: 0.905 | Reg loss: 0.026 | Tree loss: 0.905 | Accuracy: 0.629500 | 1.499 sec/iter\n",
      "Epoch: 187 | Batch: 001 / 011 | Total loss: 0.884 | Reg loss: 0.026 | Tree loss: 0.884 | Accuracy: 0.657500 | 1.499 sec/iter\n",
      "Epoch: 187 | Batch: 002 / 011 | Total loss: 0.886 | Reg loss: 0.026 | Tree loss: 0.886 | Accuracy: 0.657500 | 1.499 sec/iter\n",
      "Epoch: 187 | Batch: 003 / 011 | Total loss: 0.869 | Reg loss: 0.026 | Tree loss: 0.869 | Accuracy: 0.655000 | 1.498 sec/iter\n",
      "Epoch: 187 | Batch: 004 / 011 | Total loss: 0.834 | Reg loss: 0.026 | Tree loss: 0.834 | Accuracy: 0.658500 | 1.498 sec/iter\n",
      "Epoch: 187 | Batch: 005 / 011 | Total loss: 0.837 | Reg loss: 0.026 | Tree loss: 0.837 | Accuracy: 0.640500 | 1.498 sec/iter\n",
      "Epoch: 187 | Batch: 006 / 011 | Total loss: 0.840 | Reg loss: 0.026 | Tree loss: 0.840 | Accuracy: 0.632500 | 1.498 sec/iter\n",
      "Epoch: 187 | Batch: 007 / 011 | Total loss: 0.835 | Reg loss: 0.026 | Tree loss: 0.835 | Accuracy: 0.611500 | 1.498 sec/iter\n",
      "Epoch: 187 | Batch: 008 / 011 | Total loss: 0.806 | Reg loss: 0.026 | Tree loss: 0.806 | Accuracy: 0.623000 | 1.498 sec/iter\n",
      "Epoch: 187 | Batch: 009 / 011 | Total loss: 0.811 | Reg loss: 0.026 | Tree loss: 0.811 | Accuracy: 0.629000 | 1.497 sec/iter\n",
      "Epoch: 187 | Batch: 010 / 011 | Total loss: 0.797 | Reg loss: 0.026 | Tree loss: 0.797 | Accuracy: 0.645051 | 1.497 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 188 | Batch: 000 / 011 | Total loss: 0.901 | Reg loss: 0.026 | Tree loss: 0.901 | Accuracy: 0.627500 | 1.5 sec/iter\n",
      "Epoch: 188 | Batch: 001 / 011 | Total loss: 0.868 | Reg loss: 0.026 | Tree loss: 0.868 | Accuracy: 0.644500 | 1.5 sec/iter\n",
      "Epoch: 188 | Batch: 002 / 011 | Total loss: 0.918 | Reg loss: 0.026 | Tree loss: 0.918 | Accuracy: 0.633000 | 1.499 sec/iter\n",
      "Epoch: 188 | Batch: 003 / 011 | Total loss: 0.855 | Reg loss: 0.026 | Tree loss: 0.855 | Accuracy: 0.653000 | 1.499 sec/iter\n",
      "Epoch: 188 | Batch: 004 / 011 | Total loss: 0.826 | Reg loss: 0.026 | Tree loss: 0.826 | Accuracy: 0.680500 | 1.499 sec/iter\n",
      "Epoch: 188 | Batch: 005 / 011 | Total loss: 0.825 | Reg loss: 0.026 | Tree loss: 0.825 | Accuracy: 0.663000 | 1.499 sec/iter\n",
      "Epoch: 188 | Batch: 006 / 011 | Total loss: 0.831 | Reg loss: 0.026 | Tree loss: 0.831 | Accuracy: 0.636000 | 1.499 sec/iter\n",
      "Epoch: 188 | Batch: 007 / 011 | Total loss: 0.830 | Reg loss: 0.026 | Tree loss: 0.830 | Accuracy: 0.634000 | 1.498 sec/iter\n",
      "Epoch: 188 | Batch: 008 / 011 | Total loss: 0.815 | Reg loss: 0.026 | Tree loss: 0.815 | Accuracy: 0.622000 | 1.498 sec/iter\n",
      "Epoch: 188 | Batch: 009 / 011 | Total loss: 0.827 | Reg loss: 0.026 | Tree loss: 0.827 | Accuracy: 0.616000 | 1.498 sec/iter\n",
      "Epoch: 188 | Batch: 010 / 011 | Total loss: 0.860 | Reg loss: 0.026 | Tree loss: 0.860 | Accuracy: 0.617747 | 1.498 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 189 | Batch: 000 / 011 | Total loss: 0.906 | Reg loss: 0.026 | Tree loss: 0.906 | Accuracy: 0.640500 | 1.499 sec/iter\n",
      "Epoch: 189 | Batch: 001 / 011 | Total loss: 0.893 | Reg loss: 0.026 | Tree loss: 0.893 | Accuracy: 0.635500 | 1.499 sec/iter\n",
      "Epoch: 189 | Batch: 002 / 011 | Total loss: 0.880 | Reg loss: 0.026 | Tree loss: 0.880 | Accuracy: 0.660500 | 1.499 sec/iter\n",
      "Epoch: 189 | Batch: 003 / 011 | Total loss: 0.837 | Reg loss: 0.026 | Tree loss: 0.837 | Accuracy: 0.668000 | 1.499 sec/iter\n",
      "Epoch: 189 | Batch: 004 / 011 | Total loss: 0.841 | Reg loss: 0.026 | Tree loss: 0.841 | Accuracy: 0.668500 | 1.499 sec/iter\n",
      "Epoch: 189 | Batch: 005 / 011 | Total loss: 0.837 | Reg loss: 0.026 | Tree loss: 0.837 | Accuracy: 0.655500 | 1.498 sec/iter\n",
      "Epoch: 189 | Batch: 006 / 011 | Total loss: 0.827 | Reg loss: 0.026 | Tree loss: 0.827 | Accuracy: 0.624500 | 1.498 sec/iter\n",
      "Epoch: 189 | Batch: 007 / 011 | Total loss: 0.849 | Reg loss: 0.026 | Tree loss: 0.849 | Accuracy: 0.614500 | 1.498 sec/iter\n",
      "Epoch: 189 | Batch: 008 / 011 | Total loss: 0.819 | Reg loss: 0.026 | Tree loss: 0.819 | Accuracy: 0.612500 | 1.498 sec/iter\n",
      "Epoch: 189 | Batch: 009 / 011 | Total loss: 0.819 | Reg loss: 0.026 | Tree loss: 0.819 | Accuracy: 0.619500 | 1.498 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 189 | Batch: 010 / 011 | Total loss: 0.771 | Reg loss: 0.026 | Tree loss: 0.771 | Accuracy: 0.638225 | 1.498 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 190 | Batch: 000 / 011 | Total loss: 0.910 | Reg loss: 0.026 | Tree loss: 0.910 | Accuracy: 0.615500 | 1.499 sec/iter\n",
      "Epoch: 190 | Batch: 001 / 011 | Total loss: 0.904 | Reg loss: 0.026 | Tree loss: 0.904 | Accuracy: 0.633500 | 1.499 sec/iter\n",
      "Epoch: 190 | Batch: 002 / 011 | Total loss: 0.870 | Reg loss: 0.026 | Tree loss: 0.870 | Accuracy: 0.654500 | 1.499 sec/iter\n",
      "Epoch: 190 | Batch: 003 / 011 | Total loss: 0.852 | Reg loss: 0.026 | Tree loss: 0.852 | Accuracy: 0.651500 | 1.499 sec/iter\n",
      "Epoch: 190 | Batch: 004 / 011 | Total loss: 0.854 | Reg loss: 0.026 | Tree loss: 0.854 | Accuracy: 0.654000 | 1.499 sec/iter\n",
      "Epoch: 190 | Batch: 005 / 011 | Total loss: 0.841 | Reg loss: 0.026 | Tree loss: 0.841 | Accuracy: 0.637500 | 1.499 sec/iter\n",
      "Epoch: 190 | Batch: 006 / 011 | Total loss: 0.823 | Reg loss: 0.026 | Tree loss: 0.823 | Accuracy: 0.639500 | 1.498 sec/iter\n",
      "Epoch: 190 | Batch: 007 / 011 | Total loss: 0.825 | Reg loss: 0.026 | Tree loss: 0.825 | Accuracy: 0.632000 | 1.498 sec/iter\n",
      "Epoch: 190 | Batch: 008 / 011 | Total loss: 0.816 | Reg loss: 0.026 | Tree loss: 0.816 | Accuracy: 0.616500 | 1.498 sec/iter\n",
      "Epoch: 190 | Batch: 009 / 011 | Total loss: 0.807 | Reg loss: 0.026 | Tree loss: 0.807 | Accuracy: 0.639500 | 1.498 sec/iter\n",
      "Epoch: 190 | Batch: 010 / 011 | Total loss: 0.813 | Reg loss: 0.026 | Tree loss: 0.813 | Accuracy: 0.631399 | 1.498 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 191 | Batch: 000 / 011 | Total loss: 0.897 | Reg loss: 0.026 | Tree loss: 0.897 | Accuracy: 0.640500 | 1.5 sec/iter\n",
      "Epoch: 191 | Batch: 001 / 011 | Total loss: 0.872 | Reg loss: 0.026 | Tree loss: 0.872 | Accuracy: 0.655000 | 1.5 sec/iter\n",
      "Epoch: 191 | Batch: 002 / 011 | Total loss: 0.869 | Reg loss: 0.026 | Tree loss: 0.869 | Accuracy: 0.638500 | 1.499 sec/iter\n",
      "Epoch: 191 | Batch: 003 / 011 | Total loss: 0.898 | Reg loss: 0.026 | Tree loss: 0.898 | Accuracy: 0.635000 | 1.499 sec/iter\n",
      "Epoch: 191 | Batch: 004 / 011 | Total loss: 0.835 | Reg loss: 0.026 | Tree loss: 0.835 | Accuracy: 0.655000 | 1.499 sec/iter\n",
      "Epoch: 191 | Batch: 005 / 011 | Total loss: 0.838 | Reg loss: 0.026 | Tree loss: 0.838 | Accuracy: 0.635500 | 1.499 sec/iter\n",
      "Epoch: 191 | Batch: 006 / 011 | Total loss: 0.811 | Reg loss: 0.026 | Tree loss: 0.811 | Accuracy: 0.644000 | 1.499 sec/iter\n",
      "Epoch: 191 | Batch: 007 / 011 | Total loss: 0.833 | Reg loss: 0.026 | Tree loss: 0.833 | Accuracy: 0.614500 | 1.498 sec/iter\n",
      "Epoch: 191 | Batch: 008 / 011 | Total loss: 0.822 | Reg loss: 0.026 | Tree loss: 0.822 | Accuracy: 0.619000 | 1.498 sec/iter\n",
      "Epoch: 191 | Batch: 009 / 011 | Total loss: 0.812 | Reg loss: 0.026 | Tree loss: 0.812 | Accuracy: 0.642000 | 1.498 sec/iter\n",
      "Epoch: 191 | Batch: 010 / 011 | Total loss: 0.860 | Reg loss: 0.026 | Tree loss: 0.860 | Accuracy: 0.617747 | 1.498 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 192 | Batch: 000 / 011 | Total loss: 0.898 | Reg loss: 0.026 | Tree loss: 0.898 | Accuracy: 0.633500 | 1.5 sec/iter\n",
      "Epoch: 192 | Batch: 001 / 011 | Total loss: 0.893 | Reg loss: 0.026 | Tree loss: 0.893 | Accuracy: 0.644000 | 1.5 sec/iter\n",
      "Epoch: 192 | Batch: 002 / 011 | Total loss: 0.868 | Reg loss: 0.026 | Tree loss: 0.868 | Accuracy: 0.646500 | 1.5 sec/iter\n",
      "Epoch: 192 | Batch: 003 / 011 | Total loss: 0.844 | Reg loss: 0.026 | Tree loss: 0.844 | Accuracy: 0.664000 | 1.5 sec/iter\n",
      "Epoch: 192 | Batch: 004 / 011 | Total loss: 0.851 | Reg loss: 0.026 | Tree loss: 0.851 | Accuracy: 0.657000 | 1.499 sec/iter\n",
      "Epoch: 192 | Batch: 005 / 011 | Total loss: 0.842 | Reg loss: 0.026 | Tree loss: 0.842 | Accuracy: 0.649500 | 1.499 sec/iter\n",
      "Epoch: 192 | Batch: 006 / 011 | Total loss: 0.808 | Reg loss: 0.026 | Tree loss: 0.808 | Accuracy: 0.647000 | 1.499 sec/iter\n",
      "Epoch: 192 | Batch: 007 / 011 | Total loss: 0.818 | Reg loss: 0.026 | Tree loss: 0.818 | Accuracy: 0.637000 | 1.499 sec/iter\n",
      "Epoch: 192 | Batch: 008 / 011 | Total loss: 0.812 | Reg loss: 0.026 | Tree loss: 0.812 | Accuracy: 0.620000 | 1.499 sec/iter\n",
      "Epoch: 192 | Batch: 009 / 011 | Total loss: 0.829 | Reg loss: 0.026 | Tree loss: 0.829 | Accuracy: 0.618500 | 1.498 sec/iter\n",
      "Epoch: 192 | Batch: 010 / 011 | Total loss: 0.860 | Reg loss: 0.026 | Tree loss: 0.860 | Accuracy: 0.593857 | 1.498 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 193 | Batch: 000 / 011 | Total loss: 0.893 | Reg loss: 0.026 | Tree loss: 0.893 | Accuracy: 0.627500 | 1.499 sec/iter\n",
      "Epoch: 193 | Batch: 001 / 011 | Total loss: 0.880 | Reg loss: 0.026 | Tree loss: 0.880 | Accuracy: 0.646000 | 1.499 sec/iter\n",
      "Epoch: 193 | Batch: 002 / 011 | Total loss: 0.854 | Reg loss: 0.026 | Tree loss: 0.854 | Accuracy: 0.664500 | 1.498 sec/iter\n",
      "Epoch: 193 | Batch: 003 / 011 | Total loss: 0.852 | Reg loss: 0.026 | Tree loss: 0.852 | Accuracy: 0.675500 | 1.498 sec/iter\n",
      "Epoch: 193 | Batch: 004 / 011 | Total loss: 0.836 | Reg loss: 0.026 | Tree loss: 0.836 | Accuracy: 0.662000 | 1.498 sec/iter\n",
      "Epoch: 193 | Batch: 005 / 011 | Total loss: 0.851 | Reg loss: 0.026 | Tree loss: 0.851 | Accuracy: 0.656000 | 1.498 sec/iter\n",
      "Epoch: 193 | Batch: 006 / 011 | Total loss: 0.832 | Reg loss: 0.026 | Tree loss: 0.832 | Accuracy: 0.642500 | 1.497 sec/iter\n",
      "Epoch: 193 | Batch: 007 / 011 | Total loss: 0.831 | Reg loss: 0.026 | Tree loss: 0.831 | Accuracy: 0.624500 | 1.497 sec/iter\n",
      "Epoch: 193 | Batch: 008 / 011 | Total loss: 0.815 | Reg loss: 0.026 | Tree loss: 0.815 | Accuracy: 0.612000 | 1.497 sec/iter\n",
      "Epoch: 193 | Batch: 009 / 011 | Total loss: 0.824 | Reg loss: 0.026 | Tree loss: 0.824 | Accuracy: 0.609000 | 1.497 sec/iter\n",
      "Epoch: 193 | Batch: 010 / 011 | Total loss: 0.841 | Reg loss: 0.026 | Tree loss: 0.841 | Accuracy: 0.593857 | 1.497 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 194 | Batch: 000 / 011 | Total loss: 0.892 | Reg loss: 0.026 | Tree loss: 0.892 | Accuracy: 0.643500 | 1.499 sec/iter\n",
      "Epoch: 194 | Batch: 001 / 011 | Total loss: 0.890 | Reg loss: 0.026 | Tree loss: 0.890 | Accuracy: 0.636000 | 1.499 sec/iter\n",
      "Epoch: 194 | Batch: 002 / 011 | Total loss: 0.879 | Reg loss: 0.026 | Tree loss: 0.879 | Accuracy: 0.651500 | 1.499 sec/iter\n",
      "Epoch: 194 | Batch: 003 / 011 | Total loss: 0.850 | Reg loss: 0.026 | Tree loss: 0.850 | Accuracy: 0.647500 | 1.499 sec/iter\n",
      "Epoch: 194 | Batch: 004 / 011 | Total loss: 0.846 | Reg loss: 0.026 | Tree loss: 0.846 | Accuracy: 0.646500 | 1.499 sec/iter\n",
      "Epoch: 194 | Batch: 005 / 011 | Total loss: 0.817 | Reg loss: 0.026 | Tree loss: 0.817 | Accuracy: 0.660000 | 1.498 sec/iter\n",
      "Epoch: 194 | Batch: 006 / 011 | Total loss: 0.832 | Reg loss: 0.026 | Tree loss: 0.832 | Accuracy: 0.644000 | 1.498 sec/iter\n",
      "Epoch: 194 | Batch: 007 / 011 | Total loss: 0.828 | Reg loss: 0.026 | Tree loss: 0.828 | Accuracy: 0.638000 | 1.498 sec/iter\n",
      "Epoch: 194 | Batch: 008 / 011 | Total loss: 0.831 | Reg loss: 0.026 | Tree loss: 0.831 | Accuracy: 0.604500 | 1.498 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 194 | Batch: 009 / 011 | Total loss: 0.835 | Reg loss: 0.026 | Tree loss: 0.835 | Accuracy: 0.604500 | 1.498 sec/iter\n",
      "Epoch: 194 | Batch: 010 / 011 | Total loss: 0.843 | Reg loss: 0.026 | Tree loss: 0.843 | Accuracy: 0.600683 | 1.498 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 195 | Batch: 000 / 011 | Total loss: 0.894 | Reg loss: 0.026 | Tree loss: 0.894 | Accuracy: 0.646500 | 1.5 sec/iter\n",
      "Epoch: 195 | Batch: 001 / 011 | Total loss: 0.884 | Reg loss: 0.026 | Tree loss: 0.884 | Accuracy: 0.643000 | 1.499 sec/iter\n",
      "Epoch: 195 | Batch: 002 / 011 | Total loss: 0.865 | Reg loss: 0.026 | Tree loss: 0.865 | Accuracy: 0.645500 | 1.499 sec/iter\n",
      "Epoch: 195 | Batch: 003 / 011 | Total loss: 0.861 | Reg loss: 0.026 | Tree loss: 0.861 | Accuracy: 0.651000 | 1.499 sec/iter\n",
      "Epoch: 195 | Batch: 004 / 011 | Total loss: 0.848 | Reg loss: 0.026 | Tree loss: 0.848 | Accuracy: 0.655500 | 1.499 sec/iter\n",
      "Epoch: 195 | Batch: 005 / 011 | Total loss: 0.818 | Reg loss: 0.026 | Tree loss: 0.818 | Accuracy: 0.665500 | 1.499 sec/iter\n",
      "Epoch: 195 | Batch: 006 / 011 | Total loss: 0.834 | Reg loss: 0.026 | Tree loss: 0.834 | Accuracy: 0.632000 | 1.498 sec/iter\n",
      "Epoch: 195 | Batch: 007 / 011 | Total loss: 0.829 | Reg loss: 0.026 | Tree loss: 0.829 | Accuracy: 0.617500 | 1.498 sec/iter\n",
      "Epoch: 195 | Batch: 008 / 011 | Total loss: 0.835 | Reg loss: 0.026 | Tree loss: 0.835 | Accuracy: 0.613500 | 1.498 sec/iter\n",
      "Epoch: 195 | Batch: 009 / 011 | Total loss: 0.811 | Reg loss: 0.026 | Tree loss: 0.811 | Accuracy: 0.619000 | 1.498 sec/iter\n",
      "Epoch: 195 | Batch: 010 / 011 | Total loss: 0.803 | Reg loss: 0.026 | Tree loss: 0.803 | Accuracy: 0.621160 | 1.498 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 196 | Batch: 000 / 011 | Total loss: 0.892 | Reg loss: 0.026 | Tree loss: 0.892 | Accuracy: 0.644500 | 1.499 sec/iter\n",
      "Epoch: 196 | Batch: 001 / 011 | Total loss: 0.861 | Reg loss: 0.026 | Tree loss: 0.861 | Accuracy: 0.659000 | 1.499 sec/iter\n",
      "Epoch: 196 | Batch: 002 / 011 | Total loss: 0.873 | Reg loss: 0.026 | Tree loss: 0.873 | Accuracy: 0.637000 | 1.499 sec/iter\n",
      "Epoch: 196 | Batch: 003 / 011 | Total loss: 0.889 | Reg loss: 0.026 | Tree loss: 0.889 | Accuracy: 0.646000 | 1.499 sec/iter\n",
      "Epoch: 196 | Batch: 004 / 011 | Total loss: 0.839 | Reg loss: 0.026 | Tree loss: 0.839 | Accuracy: 0.672500 | 1.498 sec/iter\n",
      "Epoch: 196 | Batch: 005 / 011 | Total loss: 0.834 | Reg loss: 0.026 | Tree loss: 0.834 | Accuracy: 0.653500 | 1.498 sec/iter\n",
      "Epoch: 196 | Batch: 006 / 011 | Total loss: 0.816 | Reg loss: 0.026 | Tree loss: 0.816 | Accuracy: 0.641500 | 1.498 sec/iter\n",
      "Epoch: 196 | Batch: 007 / 011 | Total loss: 0.817 | Reg loss: 0.026 | Tree loss: 0.817 | Accuracy: 0.636000 | 1.498 sec/iter\n",
      "Epoch: 196 | Batch: 008 / 011 | Total loss: 0.832 | Reg loss: 0.026 | Tree loss: 0.832 | Accuracy: 0.609000 | 1.498 sec/iter\n",
      "Epoch: 196 | Batch: 009 / 011 | Total loss: 0.807 | Reg loss: 0.026 | Tree loss: 0.807 | Accuracy: 0.630000 | 1.498 sec/iter\n",
      "Epoch: 196 | Batch: 010 / 011 | Total loss: 0.763 | Reg loss: 0.026 | Tree loss: 0.763 | Accuracy: 0.662116 | 1.497 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 197 | Batch: 000 / 011 | Total loss: 0.882 | Reg loss: 0.026 | Tree loss: 0.882 | Accuracy: 0.637000 | 1.499 sec/iter\n",
      "Epoch: 197 | Batch: 001 / 011 | Total loss: 0.877 | Reg loss: 0.026 | Tree loss: 0.877 | Accuracy: 0.663000 | 1.499 sec/iter\n",
      "Epoch: 197 | Batch: 002 / 011 | Total loss: 0.886 | Reg loss: 0.026 | Tree loss: 0.886 | Accuracy: 0.649000 | 1.499 sec/iter\n",
      "Epoch: 197 | Batch: 003 / 011 | Total loss: 0.854 | Reg loss: 0.026 | Tree loss: 0.854 | Accuracy: 0.656000 | 1.499 sec/iter\n",
      "Epoch: 197 | Batch: 004 / 011 | Total loss: 0.846 | Reg loss: 0.026 | Tree loss: 0.846 | Accuracy: 0.658500 | 1.499 sec/iter\n",
      "Epoch: 197 | Batch: 005 / 011 | Total loss: 0.839 | Reg loss: 0.026 | Tree loss: 0.839 | Accuracy: 0.645000 | 1.499 sec/iter\n",
      "Epoch: 197 | Batch: 006 / 011 | Total loss: 0.804 | Reg loss: 0.026 | Tree loss: 0.804 | Accuracy: 0.644000 | 1.499 sec/iter\n",
      "Epoch: 197 | Batch: 007 / 011 | Total loss: 0.835 | Reg loss: 0.026 | Tree loss: 0.835 | Accuracy: 0.624000 | 1.498 sec/iter\n",
      "Epoch: 197 | Batch: 008 / 011 | Total loss: 0.815 | Reg loss: 0.026 | Tree loss: 0.815 | Accuracy: 0.628500 | 1.498 sec/iter\n",
      "Epoch: 197 | Batch: 009 / 011 | Total loss: 0.833 | Reg loss: 0.026 | Tree loss: 0.833 | Accuracy: 0.600500 | 1.498 sec/iter\n",
      "Epoch: 197 | Batch: 010 / 011 | Total loss: 0.795 | Reg loss: 0.026 | Tree loss: 0.795 | Accuracy: 0.634812 | 1.498 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 198 | Batch: 000 / 011 | Total loss: 0.892 | Reg loss: 0.026 | Tree loss: 0.892 | Accuracy: 0.640500 | 1.5 sec/iter\n",
      "Epoch: 198 | Batch: 001 / 011 | Total loss: 0.889 | Reg loss: 0.026 | Tree loss: 0.889 | Accuracy: 0.649500 | 1.5 sec/iter\n",
      "Epoch: 198 | Batch: 002 / 011 | Total loss: 0.871 | Reg loss: 0.026 | Tree loss: 0.871 | Accuracy: 0.659500 | 1.5 sec/iter\n",
      "Epoch: 198 | Batch: 003 / 011 | Total loss: 0.855 | Reg loss: 0.026 | Tree loss: 0.855 | Accuracy: 0.649000 | 1.499 sec/iter\n",
      "Epoch: 198 | Batch: 004 / 011 | Total loss: 0.836 | Reg loss: 0.026 | Tree loss: 0.836 | Accuracy: 0.657500 | 1.499 sec/iter\n",
      "Epoch: 198 | Batch: 005 / 011 | Total loss: 0.827 | Reg loss: 0.026 | Tree loss: 0.827 | Accuracy: 0.651500 | 1.499 sec/iter\n",
      "Epoch: 198 | Batch: 006 / 011 | Total loss: 0.811 | Reg loss: 0.026 | Tree loss: 0.811 | Accuracy: 0.657500 | 1.499 sec/iter\n",
      "Epoch: 198 | Batch: 007 / 011 | Total loss: 0.832 | Reg loss: 0.026 | Tree loss: 0.832 | Accuracy: 0.618500 | 1.499 sec/iter\n",
      "Epoch: 198 | Batch: 008 / 011 | Total loss: 0.811 | Reg loss: 0.026 | Tree loss: 0.811 | Accuracy: 0.631000 | 1.499 sec/iter\n",
      "Epoch: 198 | Batch: 009 / 011 | Total loss: 0.842 | Reg loss: 0.026 | Tree loss: 0.842 | Accuracy: 0.621500 | 1.498 sec/iter\n",
      "Epoch: 198 | Batch: 010 / 011 | Total loss: 0.809 | Reg loss: 0.026 | Tree loss: 0.809 | Accuracy: 0.610922 | 1.498 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 199 | Batch: 000 / 011 | Total loss: 0.896 | Reg loss: 0.026 | Tree loss: 0.896 | Accuracy: 0.629000 | 1.499 sec/iter\n",
      "Epoch: 199 | Batch: 001 / 011 | Total loss: 0.865 | Reg loss: 0.026 | Tree loss: 0.865 | Accuracy: 0.648000 | 1.499 sec/iter\n",
      "Epoch: 199 | Batch: 002 / 011 | Total loss: 0.900 | Reg loss: 0.026 | Tree loss: 0.900 | Accuracy: 0.643500 | 1.498 sec/iter\n",
      "Epoch: 199 | Batch: 003 / 011 | Total loss: 0.849 | Reg loss: 0.026 | Tree loss: 0.849 | Accuracy: 0.667000 | 1.498 sec/iter\n",
      "Epoch: 199 | Batch: 004 / 011 | Total loss: 0.823 | Reg loss: 0.026 | Tree loss: 0.823 | Accuracy: 0.666500 | 1.498 sec/iter\n",
      "Epoch: 199 | Batch: 005 / 011 | Total loss: 0.846 | Reg loss: 0.026 | Tree loss: 0.846 | Accuracy: 0.648500 | 1.498 sec/iter\n",
      "Epoch: 199 | Batch: 006 / 011 | Total loss: 0.817 | Reg loss: 0.026 | Tree loss: 0.817 | Accuracy: 0.650000 | 1.497 sec/iter\n",
      "Epoch: 199 | Batch: 007 / 011 | Total loss: 0.828 | Reg loss: 0.026 | Tree loss: 0.828 | Accuracy: 0.620000 | 1.497 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 199 | Batch: 008 / 011 | Total loss: 0.832 | Reg loss: 0.026 | Tree loss: 0.832 | Accuracy: 0.602500 | 1.497 sec/iter\n",
      "Epoch: 199 | Batch: 009 / 011 | Total loss: 0.821 | Reg loss: 0.026 | Tree loss: 0.821 | Accuracy: 0.610500 | 1.497 sec/iter\n",
      "Epoch: 199 | Batch: 010 / 011 | Total loss: 0.805 | Reg loss: 0.026 | Tree loss: 0.805 | Accuracy: 0.634812 | 1.497 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 200 | Batch: 000 / 011 | Total loss: 0.879 | Reg loss: 0.026 | Tree loss: 0.879 | Accuracy: 0.641500 | 1.5 sec/iter\n",
      "Epoch: 200 | Batch: 001 / 011 | Total loss: 0.888 | Reg loss: 0.026 | Tree loss: 0.888 | Accuracy: 0.653000 | 1.499 sec/iter\n",
      "Epoch: 200 | Batch: 002 / 011 | Total loss: 0.851 | Reg loss: 0.026 | Tree loss: 0.851 | Accuracy: 0.671000 | 1.499 sec/iter\n",
      "Epoch: 200 | Batch: 003 / 011 | Total loss: 0.851 | Reg loss: 0.026 | Tree loss: 0.851 | Accuracy: 0.652500 | 1.499 sec/iter\n",
      "Epoch: 200 | Batch: 004 / 011 | Total loss: 0.845 | Reg loss: 0.026 | Tree loss: 0.845 | Accuracy: 0.645000 | 1.499 sec/iter\n",
      "Epoch: 200 | Batch: 005 / 011 | Total loss: 0.821 | Reg loss: 0.026 | Tree loss: 0.821 | Accuracy: 0.647000 | 1.499 sec/iter\n",
      "Epoch: 200 | Batch: 006 / 011 | Total loss: 0.832 | Reg loss: 0.026 | Tree loss: 0.832 | Accuracy: 0.624500 | 1.499 sec/iter\n",
      "Epoch: 200 | Batch: 007 / 011 | Total loss: 0.828 | Reg loss: 0.026 | Tree loss: 0.828 | Accuracy: 0.618000 | 1.498 sec/iter\n",
      "Epoch: 200 | Batch: 008 / 011 | Total loss: 0.832 | Reg loss: 0.026 | Tree loss: 0.832 | Accuracy: 0.614500 | 1.498 sec/iter\n",
      "Epoch: 200 | Batch: 009 / 011 | Total loss: 0.840 | Reg loss: 0.026 | Tree loss: 0.840 | Accuracy: 0.611000 | 1.498 sec/iter\n",
      "Epoch: 200 | Batch: 010 / 011 | Total loss: 0.806 | Reg loss: 0.026 | Tree loss: 0.806 | Accuracy: 0.627986 | 1.498 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 201 | Batch: 000 / 011 | Total loss: 0.882 | Reg loss: 0.026 | Tree loss: 0.882 | Accuracy: 0.640000 | 1.5 sec/iter\n",
      "Epoch: 201 | Batch: 001 / 011 | Total loss: 0.907 | Reg loss: 0.026 | Tree loss: 0.907 | Accuracy: 0.639000 | 1.5 sec/iter\n",
      "Epoch: 201 | Batch: 002 / 011 | Total loss: 0.868 | Reg loss: 0.026 | Tree loss: 0.868 | Accuracy: 0.661500 | 1.5 sec/iter\n",
      "Epoch: 201 | Batch: 003 / 011 | Total loss: 0.869 | Reg loss: 0.026 | Tree loss: 0.869 | Accuracy: 0.639000 | 1.499 sec/iter\n",
      "Epoch: 201 | Batch: 004 / 011 | Total loss: 0.821 | Reg loss: 0.026 | Tree loss: 0.821 | Accuracy: 0.673500 | 1.499 sec/iter\n",
      "Epoch: 201 | Batch: 005 / 011 | Total loss: 0.824 | Reg loss: 0.026 | Tree loss: 0.824 | Accuracy: 0.645000 | 1.499 sec/iter\n",
      "Epoch: 201 | Batch: 006 / 011 | Total loss: 0.824 | Reg loss: 0.026 | Tree loss: 0.824 | Accuracy: 0.644000 | 1.499 sec/iter\n",
      "Epoch: 201 | Batch: 007 / 011 | Total loss: 0.816 | Reg loss: 0.026 | Tree loss: 0.816 | Accuracy: 0.626500 | 1.499 sec/iter\n",
      "Epoch: 201 | Batch: 008 / 011 | Total loss: 0.829 | Reg loss: 0.026 | Tree loss: 0.829 | Accuracy: 0.620000 | 1.499 sec/iter\n",
      "Epoch: 201 | Batch: 009 / 011 | Total loss: 0.836 | Reg loss: 0.026 | Tree loss: 0.836 | Accuracy: 0.615000 | 1.499 sec/iter\n",
      "Epoch: 201 | Batch: 010 / 011 | Total loss: 0.796 | Reg loss: 0.026 | Tree loss: 0.796 | Accuracy: 0.617747 | 1.499 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 202 | Batch: 000 / 011 | Total loss: 0.893 | Reg loss: 0.026 | Tree loss: 0.893 | Accuracy: 0.635500 | 1.501 sec/iter\n",
      "Epoch: 202 | Batch: 001 / 011 | Total loss: 0.884 | Reg loss: 0.026 | Tree loss: 0.884 | Accuracy: 0.647000 | 1.501 sec/iter\n",
      "Epoch: 202 | Batch: 002 / 011 | Total loss: 0.882 | Reg loss: 0.026 | Tree loss: 0.882 | Accuracy: 0.632000 | 1.501 sec/iter\n",
      "Epoch: 202 | Batch: 003 / 011 | Total loss: 0.878 | Reg loss: 0.026 | Tree loss: 0.878 | Accuracy: 0.645000 | 1.501 sec/iter\n",
      "Epoch: 202 | Batch: 004 / 011 | Total loss: 0.844 | Reg loss: 0.026 | Tree loss: 0.844 | Accuracy: 0.651500 | 1.5 sec/iter\n",
      "Epoch: 202 | Batch: 005 / 011 | Total loss: 0.828 | Reg loss: 0.026 | Tree loss: 0.828 | Accuracy: 0.659500 | 1.5 sec/iter\n",
      "Epoch: 202 | Batch: 006 / 011 | Total loss: 0.823 | Reg loss: 0.026 | Tree loss: 0.823 | Accuracy: 0.647500 | 1.5 sec/iter\n",
      "Epoch: 202 | Batch: 007 / 011 | Total loss: 0.814 | Reg loss: 0.026 | Tree loss: 0.814 | Accuracy: 0.645000 | 1.5 sec/iter\n",
      "Epoch: 202 | Batch: 008 / 011 | Total loss: 0.833 | Reg loss: 0.026 | Tree loss: 0.833 | Accuracy: 0.599000 | 1.5 sec/iter\n",
      "Epoch: 202 | Batch: 009 / 011 | Total loss: 0.807 | Reg loss: 0.026 | Tree loss: 0.807 | Accuracy: 0.626500 | 1.5 sec/iter\n",
      "Epoch: 202 | Batch: 010 / 011 | Total loss: 0.837 | Reg loss: 0.026 | Tree loss: 0.837 | Accuracy: 0.614334 | 1.5 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 203 | Batch: 000 / 011 | Total loss: 0.891 | Reg loss: 0.026 | Tree loss: 0.891 | Accuracy: 0.631000 | 1.502 sec/iter\n",
      "Epoch: 203 | Batch: 001 / 011 | Total loss: 0.878 | Reg loss: 0.026 | Tree loss: 0.878 | Accuracy: 0.637000 | 1.501 sec/iter\n",
      "Epoch: 203 | Batch: 002 / 011 | Total loss: 0.877 | Reg loss: 0.026 | Tree loss: 0.877 | Accuracy: 0.639500 | 1.501 sec/iter\n",
      "Epoch: 203 | Batch: 003 / 011 | Total loss: 0.837 | Reg loss: 0.026 | Tree loss: 0.837 | Accuracy: 0.671500 | 1.501 sec/iter\n",
      "Epoch: 203 | Batch: 004 / 011 | Total loss: 0.847 | Reg loss: 0.026 | Tree loss: 0.847 | Accuracy: 0.653000 | 1.501 sec/iter\n",
      "Epoch: 203 | Batch: 005 / 011 | Total loss: 0.817 | Reg loss: 0.026 | Tree loss: 0.817 | Accuracy: 0.645500 | 1.501 sec/iter\n",
      "Epoch: 203 | Batch: 006 / 011 | Total loss: 0.829 | Reg loss: 0.026 | Tree loss: 0.829 | Accuracy: 0.636500 | 1.501 sec/iter\n",
      "Epoch: 203 | Batch: 007 / 011 | Total loss: 0.829 | Reg loss: 0.026 | Tree loss: 0.829 | Accuracy: 0.616500 | 1.501 sec/iter\n",
      "Epoch: 203 | Batch: 008 / 011 | Total loss: 0.822 | Reg loss: 0.026 | Tree loss: 0.822 | Accuracy: 0.620500 | 1.501 sec/iter\n",
      "Epoch: 203 | Batch: 009 / 011 | Total loss: 0.824 | Reg loss: 0.026 | Tree loss: 0.824 | Accuracy: 0.622000 | 1.5 sec/iter\n",
      "Epoch: 203 | Batch: 010 / 011 | Total loss: 0.804 | Reg loss: 0.026 | Tree loss: 0.804 | Accuracy: 0.645051 | 1.5 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 204 | Batch: 000 / 011 | Total loss: 0.881 | Reg loss: 0.026 | Tree loss: 0.881 | Accuracy: 0.634000 | 1.502 sec/iter\n",
      "Epoch: 204 | Batch: 001 / 011 | Total loss: 0.896 | Reg loss: 0.026 | Tree loss: 0.896 | Accuracy: 0.626500 | 1.502 sec/iter\n",
      "Epoch: 204 | Batch: 002 / 011 | Total loss: 0.854 | Reg loss: 0.026 | Tree loss: 0.854 | Accuracy: 0.656000 | 1.502 sec/iter\n",
      "Epoch: 204 | Batch: 003 / 011 | Total loss: 0.865 | Reg loss: 0.026 | Tree loss: 0.865 | Accuracy: 0.647500 | 1.501 sec/iter\n",
      "Epoch: 204 | Batch: 004 / 011 | Total loss: 0.818 | Reg loss: 0.026 | Tree loss: 0.818 | Accuracy: 0.679500 | 1.501 sec/iter\n",
      "Epoch: 204 | Batch: 005 / 011 | Total loss: 0.849 | Reg loss: 0.026 | Tree loss: 0.849 | Accuracy: 0.651500 | 1.501 sec/iter\n",
      "Epoch: 204 | Batch: 006 / 011 | Total loss: 0.805 | Reg loss: 0.026 | Tree loss: 0.805 | Accuracy: 0.640000 | 1.501 sec/iter\n",
      "Epoch: 204 | Batch: 007 / 011 | Total loss: 0.819 | Reg loss: 0.026 | Tree loss: 0.819 | Accuracy: 0.638000 | 1.5 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 204 | Batch: 008 / 011 | Total loss: 0.809 | Reg loss: 0.026 | Tree loss: 0.809 | Accuracy: 0.630000 | 1.5 sec/iter\n",
      "Epoch: 204 | Batch: 009 / 011 | Total loss: 0.835 | Reg loss: 0.026 | Tree loss: 0.835 | Accuracy: 0.631500 | 1.5 sec/iter\n",
      "Epoch: 204 | Batch: 010 / 011 | Total loss: 0.851 | Reg loss: 0.026 | Tree loss: 0.851 | Accuracy: 0.583618 | 1.5 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 205 | Batch: 000 / 011 | Total loss: 0.894 | Reg loss: 0.026 | Tree loss: 0.894 | Accuracy: 0.637000 | 1.5 sec/iter\n",
      "Epoch: 205 | Batch: 001 / 011 | Total loss: 0.873 | Reg loss: 0.026 | Tree loss: 0.873 | Accuracy: 0.652000 | 1.5 sec/iter\n",
      "Epoch: 205 | Batch: 002 / 011 | Total loss: 0.859 | Reg loss: 0.026 | Tree loss: 0.859 | Accuracy: 0.646500 | 1.5 sec/iter\n",
      "Epoch: 205 | Batch: 003 / 011 | Total loss: 0.832 | Reg loss: 0.026 | Tree loss: 0.832 | Accuracy: 0.663500 | 1.5 sec/iter\n",
      "Epoch: 205 | Batch: 004 / 011 | Total loss: 0.837 | Reg loss: 0.026 | Tree loss: 0.837 | Accuracy: 0.637500 | 1.5 sec/iter\n",
      "Epoch: 205 | Batch: 005 / 011 | Total loss: 0.832 | Reg loss: 0.026 | Tree loss: 0.832 | Accuracy: 0.627500 | 1.499 sec/iter\n",
      "Epoch: 205 | Batch: 006 / 011 | Total loss: 0.830 | Reg loss: 0.026 | Tree loss: 0.830 | Accuracy: 0.631500 | 1.499 sec/iter\n",
      "Epoch: 205 | Batch: 007 / 011 | Total loss: 0.828 | Reg loss: 0.026 | Tree loss: 0.828 | Accuracy: 0.627000 | 1.499 sec/iter\n",
      "Epoch: 205 | Batch: 008 / 011 | Total loss: 0.826 | Reg loss: 0.026 | Tree loss: 0.826 | Accuracy: 0.626000 | 1.499 sec/iter\n",
      "Epoch: 205 | Batch: 009 / 011 | Total loss: 0.805 | Reg loss: 0.026 | Tree loss: 0.805 | Accuracy: 0.630500 | 1.499 sec/iter\n",
      "Epoch: 205 | Batch: 010 / 011 | Total loss: 0.831 | Reg loss: 0.026 | Tree loss: 0.831 | Accuracy: 0.610922 | 1.499 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 206 | Batch: 000 / 011 | Total loss: 0.895 | Reg loss: 0.026 | Tree loss: 0.895 | Accuracy: 0.640500 | 1.5 sec/iter\n",
      "Epoch: 206 | Batch: 001 / 011 | Total loss: 0.884 | Reg loss: 0.026 | Tree loss: 0.884 | Accuracy: 0.652500 | 1.5 sec/iter\n",
      "Epoch: 206 | Batch: 002 / 011 | Total loss: 0.871 | Reg loss: 0.026 | Tree loss: 0.871 | Accuracy: 0.643500 | 1.5 sec/iter\n",
      "Epoch: 206 | Batch: 003 / 011 | Total loss: 0.846 | Reg loss: 0.026 | Tree loss: 0.846 | Accuracy: 0.647500 | 1.5 sec/iter\n",
      "Epoch: 206 | Batch: 004 / 011 | Total loss: 0.839 | Reg loss: 0.026 | Tree loss: 0.839 | Accuracy: 0.661000 | 1.5 sec/iter\n",
      "Epoch: 206 | Batch: 005 / 011 | Total loss: 0.827 | Reg loss: 0.026 | Tree loss: 0.827 | Accuracy: 0.649500 | 1.5 sec/iter\n",
      "Epoch: 206 | Batch: 006 / 011 | Total loss: 0.822 | Reg loss: 0.026 | Tree loss: 0.822 | Accuracy: 0.636000 | 1.5 sec/iter\n",
      "Epoch: 206 | Batch: 007 / 011 | Total loss: 0.817 | Reg loss: 0.026 | Tree loss: 0.817 | Accuracy: 0.629500 | 1.5 sec/iter\n",
      "Epoch: 206 | Batch: 008 / 011 | Total loss: 0.796 | Reg loss: 0.026 | Tree loss: 0.796 | Accuracy: 0.630500 | 1.499 sec/iter\n",
      "Epoch: 206 | Batch: 009 / 011 | Total loss: 0.824 | Reg loss: 0.026 | Tree loss: 0.824 | Accuracy: 0.622000 | 1.499 sec/iter\n",
      "Epoch: 206 | Batch: 010 / 011 | Total loss: 0.837 | Reg loss: 0.026 | Tree loss: 0.837 | Accuracy: 0.634812 | 1.499 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 207 | Batch: 000 / 011 | Total loss: 0.894 | Reg loss: 0.026 | Tree loss: 0.894 | Accuracy: 0.628500 | 1.5 sec/iter\n",
      "Epoch: 207 | Batch: 001 / 011 | Total loss: 0.899 | Reg loss: 0.026 | Tree loss: 0.899 | Accuracy: 0.622500 | 1.5 sec/iter\n",
      "Epoch: 207 | Batch: 002 / 011 | Total loss: 0.867 | Reg loss: 0.026 | Tree loss: 0.867 | Accuracy: 0.641500 | 1.5 sec/iter\n",
      "Epoch: 207 | Batch: 003 / 011 | Total loss: 0.861 | Reg loss: 0.026 | Tree loss: 0.861 | Accuracy: 0.656000 | 1.5 sec/iter\n",
      "Epoch: 207 | Batch: 004 / 011 | Total loss: 0.812 | Reg loss: 0.026 | Tree loss: 0.812 | Accuracy: 0.666500 | 1.5 sec/iter\n",
      "Epoch: 207 | Batch: 005 / 011 | Total loss: 0.821 | Reg loss: 0.026 | Tree loss: 0.821 | Accuracy: 0.671500 | 1.5 sec/iter\n",
      "Epoch: 207 | Batch: 006 / 011 | Total loss: 0.846 | Reg loss: 0.026 | Tree loss: 0.846 | Accuracy: 0.628000 | 1.5 sec/iter\n",
      "Epoch: 207 | Batch: 007 / 011 | Total loss: 0.830 | Reg loss: 0.026 | Tree loss: 0.830 | Accuracy: 0.622000 | 1.5 sec/iter\n",
      "Epoch: 207 | Batch: 008 / 011 | Total loss: 0.800 | Reg loss: 0.026 | Tree loss: 0.800 | Accuracy: 0.635000 | 1.5 sec/iter\n",
      "Epoch: 207 | Batch: 009 / 011 | Total loss: 0.802 | Reg loss: 0.026 | Tree loss: 0.802 | Accuracy: 0.633500 | 1.499 sec/iter\n",
      "Epoch: 207 | Batch: 010 / 011 | Total loss: 0.791 | Reg loss: 0.026 | Tree loss: 0.791 | Accuracy: 0.638225 | 1.499 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 208 | Batch: 000 / 011 | Total loss: 0.897 | Reg loss: 0.026 | Tree loss: 0.897 | Accuracy: 0.631500 | 1.501 sec/iter\n",
      "Epoch: 208 | Batch: 001 / 011 | Total loss: 0.873 | Reg loss: 0.026 | Tree loss: 0.873 | Accuracy: 0.643000 | 1.5 sec/iter\n",
      "Epoch: 208 | Batch: 002 / 011 | Total loss: 0.882 | Reg loss: 0.026 | Tree loss: 0.882 | Accuracy: 0.641500 | 1.5 sec/iter\n",
      "Epoch: 208 | Batch: 003 / 011 | Total loss: 0.886 | Reg loss: 0.026 | Tree loss: 0.886 | Accuracy: 0.647500 | 1.5 sec/iter\n",
      "Epoch: 208 | Batch: 004 / 011 | Total loss: 0.834 | Reg loss: 0.026 | Tree loss: 0.834 | Accuracy: 0.646000 | 1.5 sec/iter\n",
      "Epoch: 208 | Batch: 005 / 011 | Total loss: 0.830 | Reg loss: 0.026 | Tree loss: 0.830 | Accuracy: 0.630000 | 1.5 sec/iter\n",
      "Epoch: 208 | Batch: 006 / 011 | Total loss: 0.812 | Reg loss: 0.026 | Tree loss: 0.812 | Accuracy: 0.636500 | 1.5 sec/iter\n",
      "Epoch: 208 | Batch: 007 / 011 | Total loss: 0.812 | Reg loss: 0.026 | Tree loss: 0.812 | Accuracy: 0.625000 | 1.5 sec/iter\n",
      "Epoch: 208 | Batch: 008 / 011 | Total loss: 0.831 | Reg loss: 0.026 | Tree loss: 0.831 | Accuracy: 0.593000 | 1.5 sec/iter\n",
      "Epoch: 208 | Batch: 009 / 011 | Total loss: 0.793 | Reg loss: 0.026 | Tree loss: 0.793 | Accuracy: 0.643500 | 1.499 sec/iter\n",
      "Epoch: 208 | Batch: 010 / 011 | Total loss: 0.824 | Reg loss: 0.026 | Tree loss: 0.824 | Accuracy: 0.651877 | 1.499 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 209 | Batch: 000 / 011 | Total loss: 0.912 | Reg loss: 0.026 | Tree loss: 0.912 | Accuracy: 0.626000 | 1.501 sec/iter\n",
      "Epoch: 209 | Batch: 001 / 011 | Total loss: 0.899 | Reg loss: 0.026 | Tree loss: 0.899 | Accuracy: 0.627500 | 1.501 sec/iter\n",
      "Epoch: 209 | Batch: 002 / 011 | Total loss: 0.865 | Reg loss: 0.026 | Tree loss: 0.865 | Accuracy: 0.651500 | 1.5 sec/iter\n",
      "Epoch: 209 | Batch: 003 / 011 | Total loss: 0.841 | Reg loss: 0.026 | Tree loss: 0.841 | Accuracy: 0.677500 | 1.5 sec/iter\n",
      "Epoch: 209 | Batch: 004 / 011 | Total loss: 0.829 | Reg loss: 0.026 | Tree loss: 0.829 | Accuracy: 0.656000 | 1.5 sec/iter\n",
      "Epoch: 209 | Batch: 005 / 011 | Total loss: 0.837 | Reg loss: 0.026 | Tree loss: 0.837 | Accuracy: 0.652000 | 1.5 sec/iter\n",
      "Epoch: 209 | Batch: 006 / 011 | Total loss: 0.809 | Reg loss: 0.026 | Tree loss: 0.809 | Accuracy: 0.659000 | 1.5 sec/iter\n",
      "Epoch: 209 | Batch: 007 / 011 | Total loss: 0.828 | Reg loss: 0.026 | Tree loss: 0.828 | Accuracy: 0.618500 | 1.5 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 209 | Batch: 008 / 011 | Total loss: 0.800 | Reg loss: 0.026 | Tree loss: 0.800 | Accuracy: 0.634500 | 1.5 sec/iter\n",
      "Epoch: 209 | Batch: 009 / 011 | Total loss: 0.815 | Reg loss: 0.026 | Tree loss: 0.815 | Accuracy: 0.625000 | 1.5 sec/iter\n",
      "Epoch: 209 | Batch: 010 / 011 | Total loss: 0.763 | Reg loss: 0.026 | Tree loss: 0.763 | Accuracy: 0.662116 | 1.499 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 210 | Batch: 000 / 011 | Total loss: 0.893 | Reg loss: 0.026 | Tree loss: 0.893 | Accuracy: 0.631500 | 1.501 sec/iter\n",
      "Epoch: 210 | Batch: 001 / 011 | Total loss: 0.873 | Reg loss: 0.026 | Tree loss: 0.873 | Accuracy: 0.657000 | 1.501 sec/iter\n",
      "Epoch: 210 | Batch: 002 / 011 | Total loss: 0.879 | Reg loss: 0.026 | Tree loss: 0.879 | Accuracy: 0.636000 | 1.501 sec/iter\n",
      "Epoch: 210 | Batch: 003 / 011 | Total loss: 0.848 | Reg loss: 0.026 | Tree loss: 0.848 | Accuracy: 0.655500 | 1.5 sec/iter\n",
      "Epoch: 210 | Batch: 004 / 011 | Total loss: 0.867 | Reg loss: 0.026 | Tree loss: 0.867 | Accuracy: 0.630500 | 1.5 sec/iter\n",
      "Epoch: 210 | Batch: 005 / 011 | Total loss: 0.815 | Reg loss: 0.026 | Tree loss: 0.815 | Accuracy: 0.650500 | 1.5 sec/iter\n",
      "Epoch: 210 | Batch: 006 / 011 | Total loss: 0.813 | Reg loss: 0.026 | Tree loss: 0.813 | Accuracy: 0.647500 | 1.5 sec/iter\n",
      "Epoch: 210 | Batch: 007 / 011 | Total loss: 0.803 | Reg loss: 0.026 | Tree loss: 0.803 | Accuracy: 0.628500 | 1.499 sec/iter\n",
      "Epoch: 210 | Batch: 008 / 011 | Total loss: 0.803 | Reg loss: 0.026 | Tree loss: 0.803 | Accuracy: 0.643000 | 1.499 sec/iter\n",
      "Epoch: 210 | Batch: 009 / 011 | Total loss: 0.835 | Reg loss: 0.026 | Tree loss: 0.835 | Accuracy: 0.619500 | 1.499 sec/iter\n",
      "Epoch: 210 | Batch: 010 / 011 | Total loss: 0.797 | Reg loss: 0.026 | Tree loss: 0.797 | Accuracy: 0.651877 | 1.499 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 211 | Batch: 000 / 011 | Total loss: 0.910 | Reg loss: 0.026 | Tree loss: 0.910 | Accuracy: 0.628500 | 1.5 sec/iter\n",
      "Epoch: 211 | Batch: 001 / 011 | Total loss: 0.869 | Reg loss: 0.026 | Tree loss: 0.869 | Accuracy: 0.662500 | 1.5 sec/iter\n",
      "Epoch: 211 | Batch: 002 / 011 | Total loss: 0.864 | Reg loss: 0.026 | Tree loss: 0.864 | Accuracy: 0.645000 | 1.499 sec/iter\n",
      "Epoch: 211 | Batch: 003 / 011 | Total loss: 0.856 | Reg loss: 0.026 | Tree loss: 0.856 | Accuracy: 0.644500 | 1.499 sec/iter\n",
      "Epoch: 211 | Batch: 004 / 011 | Total loss: 0.854 | Reg loss: 0.026 | Tree loss: 0.854 | Accuracy: 0.645500 | 1.499 sec/iter\n",
      "Epoch: 211 | Batch: 005 / 011 | Total loss: 0.798 | Reg loss: 0.026 | Tree loss: 0.798 | Accuracy: 0.656500 | 1.499 sec/iter\n",
      "Epoch: 211 | Batch: 006 / 011 | Total loss: 0.835 | Reg loss: 0.026 | Tree loss: 0.835 | Accuracy: 0.629500 | 1.499 sec/iter\n",
      "Epoch: 211 | Batch: 007 / 011 | Total loss: 0.787 | Reg loss: 0.026 | Tree loss: 0.787 | Accuracy: 0.652500 | 1.499 sec/iter\n",
      "Epoch: 211 | Batch: 008 / 011 | Total loss: 0.834 | Reg loss: 0.026 | Tree loss: 0.834 | Accuracy: 0.614500 | 1.499 sec/iter\n",
      "Epoch: 211 | Batch: 009 / 011 | Total loss: 0.812 | Reg loss: 0.026 | Tree loss: 0.812 | Accuracy: 0.621000 | 1.499 sec/iter\n",
      "Epoch: 211 | Batch: 010 / 011 | Total loss: 0.844 | Reg loss: 0.026 | Tree loss: 0.844 | Accuracy: 0.624573 | 1.499 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 212 | Batch: 000 / 011 | Total loss: 0.905 | Reg loss: 0.026 | Tree loss: 0.905 | Accuracy: 0.627000 | 1.5 sec/iter\n",
      "Epoch: 212 | Batch: 001 / 011 | Total loss: 0.893 | Reg loss: 0.026 | Tree loss: 0.893 | Accuracy: 0.625000 | 1.5 sec/iter\n",
      "Epoch: 212 | Batch: 002 / 011 | Total loss: 0.844 | Reg loss: 0.026 | Tree loss: 0.844 | Accuracy: 0.663000 | 1.5 sec/iter\n",
      "Epoch: 212 | Batch: 003 / 011 | Total loss: 0.858 | Reg loss: 0.026 | Tree loss: 0.858 | Accuracy: 0.646000 | 1.5 sec/iter\n",
      "Epoch: 212 | Batch: 004 / 011 | Total loss: 0.829 | Reg loss: 0.026 | Tree loss: 0.829 | Accuracy: 0.663000 | 1.5 sec/iter\n",
      "Epoch: 212 | Batch: 005 / 011 | Total loss: 0.828 | Reg loss: 0.026 | Tree loss: 0.828 | Accuracy: 0.663500 | 1.5 sec/iter\n",
      "Epoch: 212 | Batch: 006 / 011 | Total loss: 0.825 | Reg loss: 0.026 | Tree loss: 0.825 | Accuracy: 0.632500 | 1.5 sec/iter\n",
      "Epoch: 212 | Batch: 007 / 011 | Total loss: 0.819 | Reg loss: 0.026 | Tree loss: 0.819 | Accuracy: 0.616000 | 1.5 sec/iter\n",
      "Epoch: 212 | Batch: 008 / 011 | Total loss: 0.808 | Reg loss: 0.026 | Tree loss: 0.808 | Accuracy: 0.616000 | 1.499 sec/iter\n",
      "Epoch: 212 | Batch: 009 / 011 | Total loss: 0.817 | Reg loss: 0.026 | Tree loss: 0.817 | Accuracy: 0.622000 | 1.499 sec/iter\n",
      "Epoch: 212 | Batch: 010 / 011 | Total loss: 0.789 | Reg loss: 0.026 | Tree loss: 0.789 | Accuracy: 0.634812 | 1.499 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 213 | Batch: 000 / 011 | Total loss: 0.872 | Reg loss: 0.026 | Tree loss: 0.872 | Accuracy: 0.657500 | 1.5 sec/iter\n",
      "Epoch: 213 | Batch: 001 / 011 | Total loss: 0.882 | Reg loss: 0.026 | Tree loss: 0.882 | Accuracy: 0.642000 | 1.5 sec/iter\n",
      "Epoch: 213 | Batch: 002 / 011 | Total loss: 0.859 | Reg loss: 0.026 | Tree loss: 0.859 | Accuracy: 0.656000 | 1.5 sec/iter\n",
      "Epoch: 213 | Batch: 003 / 011 | Total loss: 0.864 | Reg loss: 0.026 | Tree loss: 0.864 | Accuracy: 0.633500 | 1.5 sec/iter\n",
      "Epoch: 213 | Batch: 004 / 011 | Total loss: 0.852 | Reg loss: 0.026 | Tree loss: 0.852 | Accuracy: 0.659500 | 1.5 sec/iter\n",
      "Epoch: 213 | Batch: 005 / 011 | Total loss: 0.817 | Reg loss: 0.026 | Tree loss: 0.817 | Accuracy: 0.664500 | 1.5 sec/iter\n",
      "Epoch: 213 | Batch: 006 / 011 | Total loss: 0.823 | Reg loss: 0.026 | Tree loss: 0.823 | Accuracy: 0.643500 | 1.5 sec/iter\n",
      "Epoch: 213 | Batch: 007 / 011 | Total loss: 0.830 | Reg loss: 0.026 | Tree loss: 0.830 | Accuracy: 0.613000 | 1.499 sec/iter\n",
      "Epoch: 213 | Batch: 008 / 011 | Total loss: 0.809 | Reg loss: 0.026 | Tree loss: 0.809 | Accuracy: 0.620000 | 1.499 sec/iter\n",
      "Epoch: 213 | Batch: 009 / 011 | Total loss: 0.812 | Reg loss: 0.026 | Tree loss: 0.812 | Accuracy: 0.631000 | 1.499 sec/iter\n",
      "Epoch: 213 | Batch: 010 / 011 | Total loss: 0.823 | Reg loss: 0.026 | Tree loss: 0.823 | Accuracy: 0.645051 | 1.499 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 214 | Batch: 000 / 011 | Total loss: 0.894 | Reg loss: 0.026 | Tree loss: 0.894 | Accuracy: 0.623500 | 1.5 sec/iter\n",
      "Epoch: 214 | Batch: 001 / 011 | Total loss: 0.865 | Reg loss: 0.026 | Tree loss: 0.865 | Accuracy: 0.653000 | 1.5 sec/iter\n",
      "Epoch: 214 | Batch: 002 / 011 | Total loss: 0.878 | Reg loss: 0.026 | Tree loss: 0.878 | Accuracy: 0.650500 | 1.5 sec/iter\n",
      "Epoch: 214 | Batch: 003 / 011 | Total loss: 0.858 | Reg loss: 0.026 | Tree loss: 0.858 | Accuracy: 0.642500 | 1.5 sec/iter\n",
      "Epoch: 214 | Batch: 004 / 011 | Total loss: 0.828 | Reg loss: 0.026 | Tree loss: 0.828 | Accuracy: 0.654500 | 1.5 sec/iter\n",
      "Epoch: 214 | Batch: 005 / 011 | Total loss: 0.807 | Reg loss: 0.026 | Tree loss: 0.807 | Accuracy: 0.673500 | 1.499 sec/iter\n",
      "Epoch: 214 | Batch: 006 / 011 | Total loss: 0.822 | Reg loss: 0.026 | Tree loss: 0.822 | Accuracy: 0.634000 | 1.499 sec/iter\n",
      "Epoch: 214 | Batch: 007 / 011 | Total loss: 0.834 | Reg loss: 0.026 | Tree loss: 0.834 | Accuracy: 0.611000 | 1.499 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 214 | Batch: 008 / 011 | Total loss: 0.804 | Reg loss: 0.026 | Tree loss: 0.804 | Accuracy: 0.627500 | 1.499 sec/iter\n",
      "Epoch: 214 | Batch: 009 / 011 | Total loss: 0.819 | Reg loss: 0.026 | Tree loss: 0.819 | Accuracy: 0.625000 | 1.499 sec/iter\n",
      "Epoch: 214 | Batch: 010 / 011 | Total loss: 0.789 | Reg loss: 0.026 | Tree loss: 0.789 | Accuracy: 0.668942 | 1.499 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 215 | Batch: 000 / 011 | Total loss: 0.890 | Reg loss: 0.026 | Tree loss: 0.890 | Accuracy: 0.635500 | 1.5 sec/iter\n",
      "Epoch: 215 | Batch: 001 / 011 | Total loss: 0.880 | Reg loss: 0.026 | Tree loss: 0.880 | Accuracy: 0.652000 | 1.5 sec/iter\n",
      "Epoch: 215 | Batch: 002 / 011 | Total loss: 0.877 | Reg loss: 0.026 | Tree loss: 0.877 | Accuracy: 0.643000 | 1.5 sec/iter\n",
      "Epoch: 215 | Batch: 003 / 011 | Total loss: 0.834 | Reg loss: 0.026 | Tree loss: 0.834 | Accuracy: 0.657000 | 1.5 sec/iter\n",
      "Epoch: 215 | Batch: 004 / 011 | Total loss: 0.851 | Reg loss: 0.026 | Tree loss: 0.851 | Accuracy: 0.634000 | 1.5 sec/iter\n",
      "Epoch: 215 | Batch: 005 / 011 | Total loss: 0.835 | Reg loss: 0.026 | Tree loss: 0.835 | Accuracy: 0.622000 | 1.5 sec/iter\n",
      "Epoch: 215 | Batch: 006 / 011 | Total loss: 0.805 | Reg loss: 0.026 | Tree loss: 0.805 | Accuracy: 0.640000 | 1.5 sec/iter\n",
      "Epoch: 215 | Batch: 007 / 011 | Total loss: 0.828 | Reg loss: 0.026 | Tree loss: 0.828 | Accuracy: 0.637000 | 1.5 sec/iter\n",
      "Epoch: 215 | Batch: 008 / 011 | Total loss: 0.802 | Reg loss: 0.026 | Tree loss: 0.802 | Accuracy: 0.628000 | 1.5 sec/iter\n",
      "Epoch: 215 | Batch: 009 / 011 | Total loss: 0.812 | Reg loss: 0.026 | Tree loss: 0.812 | Accuracy: 0.634500 | 1.499 sec/iter\n",
      "Epoch: 215 | Batch: 010 / 011 | Total loss: 0.794 | Reg loss: 0.026 | Tree loss: 0.794 | Accuracy: 0.655290 | 1.499 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 216 | Batch: 000 / 011 | Total loss: 0.884 | Reg loss: 0.026 | Tree loss: 0.884 | Accuracy: 0.638500 | 1.5 sec/iter\n",
      "Epoch: 216 | Batch: 001 / 011 | Total loss: 0.853 | Reg loss: 0.026 | Tree loss: 0.853 | Accuracy: 0.653500 | 1.5 sec/iter\n",
      "Epoch: 216 | Batch: 002 / 011 | Total loss: 0.862 | Reg loss: 0.026 | Tree loss: 0.862 | Accuracy: 0.643000 | 1.5 sec/iter\n",
      "Epoch: 216 | Batch: 003 / 011 | Total loss: 0.853 | Reg loss: 0.026 | Tree loss: 0.853 | Accuracy: 0.652000 | 1.5 sec/iter\n",
      "Epoch: 216 | Batch: 004 / 011 | Total loss: 0.862 | Reg loss: 0.026 | Tree loss: 0.862 | Accuracy: 0.642000 | 1.5 sec/iter\n",
      "Epoch: 216 | Batch: 005 / 011 | Total loss: 0.828 | Reg loss: 0.026 | Tree loss: 0.828 | Accuracy: 0.659500 | 1.5 sec/iter\n",
      "Epoch: 216 | Batch: 006 / 011 | Total loss: 0.827 | Reg loss: 0.026 | Tree loss: 0.827 | Accuracy: 0.631500 | 1.499 sec/iter\n",
      "Epoch: 216 | Batch: 007 / 011 | Total loss: 0.790 | Reg loss: 0.026 | Tree loss: 0.790 | Accuracy: 0.632000 | 1.499 sec/iter\n",
      "Epoch: 216 | Batch: 008 / 011 | Total loss: 0.833 | Reg loss: 0.026 | Tree loss: 0.833 | Accuracy: 0.624500 | 1.499 sec/iter\n",
      "Epoch: 216 | Batch: 009 / 011 | Total loss: 0.817 | Reg loss: 0.026 | Tree loss: 0.817 | Accuracy: 0.627500 | 1.499 sec/iter\n",
      "Epoch: 216 | Batch: 010 / 011 | Total loss: 0.808 | Reg loss: 0.026 | Tree loss: 0.808 | Accuracy: 0.645051 | 1.499 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 217 | Batch: 000 / 011 | Total loss: 0.886 | Reg loss: 0.026 | Tree loss: 0.886 | Accuracy: 0.623500 | 1.499 sec/iter\n",
      "Epoch: 217 | Batch: 001 / 011 | Total loss: 0.899 | Reg loss: 0.026 | Tree loss: 0.899 | Accuracy: 0.646000 | 1.499 sec/iter\n",
      "Epoch: 217 | Batch: 002 / 011 | Total loss: 0.874 | Reg loss: 0.026 | Tree loss: 0.874 | Accuracy: 0.644000 | 1.499 sec/iter\n",
      "Epoch: 217 | Batch: 003 / 011 | Total loss: 0.869 | Reg loss: 0.026 | Tree loss: 0.869 | Accuracy: 0.651500 | 1.499 sec/iter\n",
      "Epoch: 217 | Batch: 004 / 011 | Total loss: 0.824 | Reg loss: 0.026 | Tree loss: 0.824 | Accuracy: 0.654000 | 1.499 sec/iter\n",
      "Epoch: 217 | Batch: 005 / 011 | Total loss: 0.800 | Reg loss: 0.026 | Tree loss: 0.800 | Accuracy: 0.648000 | 1.499 sec/iter\n",
      "Epoch: 217 | Batch: 006 / 011 | Total loss: 0.825 | Reg loss: 0.026 | Tree loss: 0.825 | Accuracy: 0.644000 | 1.499 sec/iter\n",
      "Epoch: 217 | Batch: 007 / 011 | Total loss: 0.815 | Reg loss: 0.026 | Tree loss: 0.815 | Accuracy: 0.640500 | 1.499 sec/iter\n",
      "Epoch: 217 | Batch: 008 / 011 | Total loss: 0.822 | Reg loss: 0.026 | Tree loss: 0.822 | Accuracy: 0.605500 | 1.498 sec/iter\n",
      "Epoch: 217 | Batch: 009 / 011 | Total loss: 0.815 | Reg loss: 0.026 | Tree loss: 0.815 | Accuracy: 0.623500 | 1.498 sec/iter\n",
      "Epoch: 217 | Batch: 010 / 011 | Total loss: 0.809 | Reg loss: 0.026 | Tree loss: 0.809 | Accuracy: 0.679181 | 1.498 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 218 | Batch: 000 / 011 | Total loss: 0.875 | Reg loss: 0.026 | Tree loss: 0.875 | Accuracy: 0.633000 | 1.499 sec/iter\n",
      "Epoch: 218 | Batch: 001 / 011 | Total loss: 0.878 | Reg loss: 0.026 | Tree loss: 0.878 | Accuracy: 0.650500 | 1.499 sec/iter\n",
      "Epoch: 218 | Batch: 002 / 011 | Total loss: 0.881 | Reg loss: 0.026 | Tree loss: 0.881 | Accuracy: 0.640000 | 1.499 sec/iter\n",
      "Epoch: 218 | Batch: 003 / 011 | Total loss: 0.851 | Reg loss: 0.026 | Tree loss: 0.851 | Accuracy: 0.648500 | 1.499 sec/iter\n",
      "Epoch: 218 | Batch: 004 / 011 | Total loss: 0.828 | Reg loss: 0.026 | Tree loss: 0.828 | Accuracy: 0.664000 | 1.499 sec/iter\n",
      "Epoch: 218 | Batch: 005 / 011 | Total loss: 0.830 | Reg loss: 0.026 | Tree loss: 0.830 | Accuracy: 0.650500 | 1.499 sec/iter\n",
      "Epoch: 218 | Batch: 006 / 011 | Total loss: 0.806 | Reg loss: 0.026 | Tree loss: 0.806 | Accuracy: 0.638000 | 1.499 sec/iter\n",
      "Epoch: 218 | Batch: 007 / 011 | Total loss: 0.822 | Reg loss: 0.026 | Tree loss: 0.822 | Accuracy: 0.610500 | 1.499 sec/iter\n",
      "Epoch: 218 | Batch: 008 / 011 | Total loss: 0.828 | Reg loss: 0.026 | Tree loss: 0.828 | Accuracy: 0.637500 | 1.499 sec/iter\n",
      "Epoch: 218 | Batch: 009 / 011 | Total loss: 0.807 | Reg loss: 0.026 | Tree loss: 0.807 | Accuracy: 0.634000 | 1.498 sec/iter\n",
      "Epoch: 218 | Batch: 010 / 011 | Total loss: 0.807 | Reg loss: 0.026 | Tree loss: 0.807 | Accuracy: 0.641638 | 1.498 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 219 | Batch: 000 / 011 | Total loss: 0.879 | Reg loss: 0.026 | Tree loss: 0.879 | Accuracy: 0.637500 | 1.499 sec/iter\n",
      "Epoch: 219 | Batch: 001 / 011 | Total loss: 0.867 | Reg loss: 0.026 | Tree loss: 0.867 | Accuracy: 0.643500 | 1.499 sec/iter\n",
      "Epoch: 219 | Batch: 002 / 011 | Total loss: 0.856 | Reg loss: 0.026 | Tree loss: 0.856 | Accuracy: 0.660500 | 1.499 sec/iter\n",
      "Epoch: 219 | Batch: 003 / 011 | Total loss: 0.860 | Reg loss: 0.026 | Tree loss: 0.860 | Accuracy: 0.641500 | 1.499 sec/iter\n",
      "Epoch: 219 | Batch: 004 / 011 | Total loss: 0.845 | Reg loss: 0.026 | Tree loss: 0.845 | Accuracy: 0.652500 | 1.499 sec/iter\n",
      "Epoch: 219 | Batch: 005 / 011 | Total loss: 0.833 | Reg loss: 0.026 | Tree loss: 0.833 | Accuracy: 0.655500 | 1.499 sec/iter\n",
      "Epoch: 219 | Batch: 006 / 011 | Total loss: 0.827 | Reg loss: 0.026 | Tree loss: 0.827 | Accuracy: 0.628500 | 1.499 sec/iter\n",
      "Epoch: 219 | Batch: 007 / 011 | Total loss: 0.803 | Reg loss: 0.026 | Tree loss: 0.803 | Accuracy: 0.647000 | 1.499 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 219 | Batch: 008 / 011 | Total loss: 0.831 | Reg loss: 0.026 | Tree loss: 0.831 | Accuracy: 0.615500 | 1.499 sec/iter\n",
      "Epoch: 219 | Batch: 009 / 011 | Total loss: 0.807 | Reg loss: 0.026 | Tree loss: 0.807 | Accuracy: 0.636500 | 1.498 sec/iter\n",
      "Epoch: 219 | Batch: 010 / 011 | Total loss: 0.834 | Reg loss: 0.026 | Tree loss: 0.834 | Accuracy: 0.665529 | 1.498 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 220 | Batch: 000 / 011 | Total loss: 0.920 | Reg loss: 0.026 | Tree loss: 0.920 | Accuracy: 0.595000 | 1.499 sec/iter\n",
      "Epoch: 220 | Batch: 001 / 011 | Total loss: 0.880 | Reg loss: 0.026 | Tree loss: 0.880 | Accuracy: 0.650000 | 1.499 sec/iter\n",
      "Epoch: 220 | Batch: 002 / 011 | Total loss: 0.825 | Reg loss: 0.026 | Tree loss: 0.825 | Accuracy: 0.668000 | 1.499 sec/iter\n",
      "Epoch: 220 | Batch: 003 / 011 | Total loss: 0.827 | Reg loss: 0.026 | Tree loss: 0.827 | Accuracy: 0.657000 | 1.499 sec/iter\n",
      "Epoch: 220 | Batch: 004 / 011 | Total loss: 0.840 | Reg loss: 0.026 | Tree loss: 0.840 | Accuracy: 0.651500 | 1.499 sec/iter\n",
      "Epoch: 220 | Batch: 005 / 011 | Total loss: 0.828 | Reg loss: 0.026 | Tree loss: 0.828 | Accuracy: 0.655500 | 1.499 sec/iter\n",
      "Epoch: 220 | Batch: 006 / 011 | Total loss: 0.823 | Reg loss: 0.026 | Tree loss: 0.823 | Accuracy: 0.626500 | 1.499 sec/iter\n",
      "Epoch: 220 | Batch: 007 / 011 | Total loss: 0.814 | Reg loss: 0.026 | Tree loss: 0.814 | Accuracy: 0.637000 | 1.499 sec/iter\n",
      "Epoch: 220 | Batch: 008 / 011 | Total loss: 0.819 | Reg loss: 0.026 | Tree loss: 0.819 | Accuracy: 0.613000 | 1.499 sec/iter\n",
      "Epoch: 220 | Batch: 009 / 011 | Total loss: 0.801 | Reg loss: 0.026 | Tree loss: 0.801 | Accuracy: 0.642500 | 1.499 sec/iter\n",
      "Epoch: 220 | Batch: 010 / 011 | Total loss: 0.828 | Reg loss: 0.026 | Tree loss: 0.828 | Accuracy: 0.617747 | 1.498 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 221 | Batch: 000 / 011 | Total loss: 0.883 | Reg loss: 0.026 | Tree loss: 0.883 | Accuracy: 0.653500 | 1.5 sec/iter\n",
      "Epoch: 221 | Batch: 001 / 011 | Total loss: 0.902 | Reg loss: 0.026 | Tree loss: 0.902 | Accuracy: 0.616500 | 1.5 sec/iter\n",
      "Epoch: 221 | Batch: 002 / 011 | Total loss: 0.858 | Reg loss: 0.026 | Tree loss: 0.858 | Accuracy: 0.661000 | 1.499 sec/iter\n",
      "Epoch: 221 | Batch: 003 / 011 | Total loss: 0.862 | Reg loss: 0.026 | Tree loss: 0.862 | Accuracy: 0.642500 | 1.499 sec/iter\n",
      "Epoch: 221 | Batch: 004 / 011 | Total loss: 0.833 | Reg loss: 0.026 | Tree loss: 0.833 | Accuracy: 0.661500 | 1.499 sec/iter\n",
      "Epoch: 221 | Batch: 005 / 011 | Total loss: 0.821 | Reg loss: 0.026 | Tree loss: 0.821 | Accuracy: 0.658500 | 1.499 sec/iter\n",
      "Epoch: 221 | Batch: 006 / 011 | Total loss: 0.811 | Reg loss: 0.026 | Tree loss: 0.811 | Accuracy: 0.654000 | 1.499 sec/iter\n",
      "Epoch: 221 | Batch: 007 / 011 | Total loss: 0.831 | Reg loss: 0.026 | Tree loss: 0.831 | Accuracy: 0.612500 | 1.499 sec/iter\n",
      "Epoch: 221 | Batch: 008 / 011 | Total loss: 0.806 | Reg loss: 0.026 | Tree loss: 0.806 | Accuracy: 0.639000 | 1.499 sec/iter\n",
      "Epoch: 221 | Batch: 009 / 011 | Total loss: 0.807 | Reg loss: 0.026 | Tree loss: 0.807 | Accuracy: 0.631000 | 1.499 sec/iter\n",
      "Epoch: 221 | Batch: 010 / 011 | Total loss: 0.754 | Reg loss: 0.026 | Tree loss: 0.754 | Accuracy: 0.665529 | 1.498 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 222 | Batch: 000 / 011 | Total loss: 0.889 | Reg loss: 0.026 | Tree loss: 0.889 | Accuracy: 0.632500 | 1.5 sec/iter\n",
      "Epoch: 222 | Batch: 001 / 011 | Total loss: 0.893 | Reg loss: 0.026 | Tree loss: 0.893 | Accuracy: 0.629000 | 1.5 sec/iter\n",
      "Epoch: 222 | Batch: 002 / 011 | Total loss: 0.868 | Reg loss: 0.026 | Tree loss: 0.868 | Accuracy: 0.654500 | 1.5 sec/iter\n",
      "Epoch: 222 | Batch: 003 / 011 | Total loss: 0.847 | Reg loss: 0.026 | Tree loss: 0.847 | Accuracy: 0.660000 | 1.499 sec/iter\n",
      "Epoch: 222 | Batch: 004 / 011 | Total loss: 0.835 | Reg loss: 0.026 | Tree loss: 0.835 | Accuracy: 0.652500 | 1.499 sec/iter\n",
      "Epoch: 222 | Batch: 005 / 011 | Total loss: 0.824 | Reg loss: 0.026 | Tree loss: 0.824 | Accuracy: 0.657000 | 1.499 sec/iter\n",
      "Epoch: 222 | Batch: 006 / 011 | Total loss: 0.800 | Reg loss: 0.026 | Tree loss: 0.800 | Accuracy: 0.651500 | 1.499 sec/iter\n",
      "Epoch: 222 | Batch: 007 / 011 | Total loss: 0.815 | Reg loss: 0.026 | Tree loss: 0.815 | Accuracy: 0.626500 | 1.498 sec/iter\n",
      "Epoch: 222 | Batch: 008 / 011 | Total loss: 0.835 | Reg loss: 0.026 | Tree loss: 0.835 | Accuracy: 0.620000 | 1.498 sec/iter\n",
      "Epoch: 222 | Batch: 009 / 011 | Total loss: 0.793 | Reg loss: 0.026 | Tree loss: 0.793 | Accuracy: 0.645500 | 1.498 sec/iter\n",
      "Epoch: 222 | Batch: 010 / 011 | Total loss: 0.844 | Reg loss: 0.026 | Tree loss: 0.844 | Accuracy: 0.621160 | 1.498 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 223 | Batch: 000 / 011 | Total loss: 0.892 | Reg loss: 0.026 | Tree loss: 0.892 | Accuracy: 0.625500 | 1.498 sec/iter\n",
      "Epoch: 223 | Batch: 001 / 011 | Total loss: 0.882 | Reg loss: 0.026 | Tree loss: 0.882 | Accuracy: 0.647500 | 1.498 sec/iter\n",
      "Epoch: 223 | Batch: 002 / 011 | Total loss: 0.865 | Reg loss: 0.026 | Tree loss: 0.865 | Accuracy: 0.645000 | 1.498 sec/iter\n",
      "Epoch: 223 | Batch: 003 / 011 | Total loss: 0.866 | Reg loss: 0.026 | Tree loss: 0.866 | Accuracy: 0.639500 | 1.498 sec/iter\n",
      "Epoch: 223 | Batch: 004 / 011 | Total loss: 0.838 | Reg loss: 0.026 | Tree loss: 0.838 | Accuracy: 0.649000 | 1.498 sec/iter\n",
      "Epoch: 223 | Batch: 005 / 011 | Total loss: 0.801 | Reg loss: 0.026 | Tree loss: 0.801 | Accuracy: 0.656000 | 1.498 sec/iter\n",
      "Epoch: 223 | Batch: 006 / 011 | Total loss: 0.821 | Reg loss: 0.026 | Tree loss: 0.821 | Accuracy: 0.620000 | 1.498 sec/iter\n",
      "Epoch: 223 | Batch: 007 / 011 | Total loss: 0.829 | Reg loss: 0.026 | Tree loss: 0.829 | Accuracy: 0.607000 | 1.498 sec/iter\n",
      "Epoch: 223 | Batch: 008 / 011 | Total loss: 0.801 | Reg loss: 0.026 | Tree loss: 0.801 | Accuracy: 0.634500 | 1.497 sec/iter\n",
      "Epoch: 223 | Batch: 009 / 011 | Total loss: 0.798 | Reg loss: 0.026 | Tree loss: 0.798 | Accuracy: 0.642500 | 1.497 sec/iter\n",
      "Epoch: 223 | Batch: 010 / 011 | Total loss: 0.803 | Reg loss: 0.026 | Tree loss: 0.803 | Accuracy: 0.665529 | 1.497 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 224 | Batch: 000 / 011 | Total loss: 0.879 | Reg loss: 0.026 | Tree loss: 0.879 | Accuracy: 0.641500 | 1.5 sec/iter\n",
      "Epoch: 224 | Batch: 001 / 011 | Total loss: 0.859 | Reg loss: 0.026 | Tree loss: 0.859 | Accuracy: 0.662000 | 1.5 sec/iter\n",
      "Epoch: 224 | Batch: 002 / 011 | Total loss: 0.842 | Reg loss: 0.026 | Tree loss: 0.842 | Accuracy: 0.660500 | 1.5 sec/iter\n",
      "Epoch: 224 | Batch: 003 / 011 | Total loss: 0.858 | Reg loss: 0.026 | Tree loss: 0.858 | Accuracy: 0.666500 | 1.499 sec/iter\n",
      "Epoch: 224 | Batch: 004 / 011 | Total loss: 0.857 | Reg loss: 0.026 | Tree loss: 0.857 | Accuracy: 0.629000 | 1.499 sec/iter\n",
      "Epoch: 224 | Batch: 005 / 011 | Total loss: 0.842 | Reg loss: 0.026 | Tree loss: 0.842 | Accuracy: 0.626500 | 1.499 sec/iter\n",
      "Epoch: 224 | Batch: 006 / 011 | Total loss: 0.833 | Reg loss: 0.026 | Tree loss: 0.833 | Accuracy: 0.622000 | 1.499 sec/iter\n",
      "Epoch: 224 | Batch: 007 / 011 | Total loss: 0.818 | Reg loss: 0.026 | Tree loss: 0.818 | Accuracy: 0.637500 | 1.499 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 224 | Batch: 008 / 011 | Total loss: 0.806 | Reg loss: 0.026 | Tree loss: 0.806 | Accuracy: 0.626000 | 1.499 sec/iter\n",
      "Epoch: 224 | Batch: 009 / 011 | Total loss: 0.780 | Reg loss: 0.026 | Tree loss: 0.780 | Accuracy: 0.646500 | 1.498 sec/iter\n",
      "Epoch: 224 | Batch: 010 / 011 | Total loss: 0.855 | Reg loss: 0.026 | Tree loss: 0.855 | Accuracy: 0.597270 | 1.498 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 225 | Batch: 000 / 011 | Total loss: 0.853 | Reg loss: 0.026 | Tree loss: 0.853 | Accuracy: 0.668500 | 1.5 sec/iter\n",
      "Epoch: 225 | Batch: 001 / 011 | Total loss: 0.901 | Reg loss: 0.026 | Tree loss: 0.901 | Accuracy: 0.631000 | 1.5 sec/iter\n",
      "Epoch: 225 | Batch: 002 / 011 | Total loss: 0.863 | Reg loss: 0.026 | Tree loss: 0.863 | Accuracy: 0.657000 | 1.5 sec/iter\n",
      "Epoch: 225 | Batch: 003 / 011 | Total loss: 0.866 | Reg loss: 0.026 | Tree loss: 0.866 | Accuracy: 0.650500 | 1.5 sec/iter\n",
      "Epoch: 225 | Batch: 004 / 011 | Total loss: 0.845 | Reg loss: 0.026 | Tree loss: 0.845 | Accuracy: 0.650500 | 1.5 sec/iter\n",
      "Epoch: 225 | Batch: 005 / 011 | Total loss: 0.822 | Reg loss: 0.026 | Tree loss: 0.822 | Accuracy: 0.640500 | 1.499 sec/iter\n",
      "Epoch: 225 | Batch: 006 / 011 | Total loss: 0.833 | Reg loss: 0.026 | Tree loss: 0.833 | Accuracy: 0.634000 | 1.499 sec/iter\n",
      "Epoch: 225 | Batch: 007 / 011 | Total loss: 0.794 | Reg loss: 0.026 | Tree loss: 0.794 | Accuracy: 0.648000 | 1.499 sec/iter\n",
      "Epoch: 225 | Batch: 008 / 011 | Total loss: 0.805 | Reg loss: 0.026 | Tree loss: 0.805 | Accuracy: 0.628500 | 1.499 sec/iter\n",
      "Epoch: 225 | Batch: 009 / 011 | Total loss: 0.796 | Reg loss: 0.026 | Tree loss: 0.796 | Accuracy: 0.641000 | 1.499 sec/iter\n",
      "Epoch: 225 | Batch: 010 / 011 | Total loss: 0.771 | Reg loss: 0.026 | Tree loss: 0.771 | Accuracy: 0.662116 | 1.499 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 226 | Batch: 000 / 011 | Total loss: 0.884 | Reg loss: 0.026 | Tree loss: 0.884 | Accuracy: 0.642000 | 1.5 sec/iter\n",
      "Epoch: 226 | Batch: 001 / 011 | Total loss: 0.864 | Reg loss: 0.026 | Tree loss: 0.864 | Accuracy: 0.642000 | 1.5 sec/iter\n",
      "Epoch: 226 | Batch: 002 / 011 | Total loss: 0.849 | Reg loss: 0.026 | Tree loss: 0.849 | Accuracy: 0.662000 | 1.5 sec/iter\n",
      "Epoch: 226 | Batch: 003 / 011 | Total loss: 0.860 | Reg loss: 0.026 | Tree loss: 0.860 | Accuracy: 0.650500 | 1.5 sec/iter\n",
      "Epoch: 226 | Batch: 004 / 011 | Total loss: 0.837 | Reg loss: 0.026 | Tree loss: 0.837 | Accuracy: 0.647000 | 1.5 sec/iter\n",
      "Epoch: 226 | Batch: 005 / 011 | Total loss: 0.822 | Reg loss: 0.026 | Tree loss: 0.822 | Accuracy: 0.662500 | 1.5 sec/iter\n",
      "Epoch: 226 | Batch: 006 / 011 | Total loss: 0.827 | Reg loss: 0.026 | Tree loss: 0.827 | Accuracy: 0.647500 | 1.499 sec/iter\n",
      "Epoch: 226 | Batch: 007 / 011 | Total loss: 0.787 | Reg loss: 0.026 | Tree loss: 0.787 | Accuracy: 0.652500 | 1.499 sec/iter\n",
      "Epoch: 226 | Batch: 008 / 011 | Total loss: 0.818 | Reg loss: 0.026 | Tree loss: 0.818 | Accuracy: 0.632000 | 1.499 sec/iter\n",
      "Epoch: 226 | Batch: 009 / 011 | Total loss: 0.819 | Reg loss: 0.026 | Tree loss: 0.819 | Accuracy: 0.620500 | 1.499 sec/iter\n",
      "Epoch: 226 | Batch: 010 / 011 | Total loss: 0.791 | Reg loss: 0.026 | Tree loss: 0.791 | Accuracy: 0.651877 | 1.499 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 227 | Batch: 000 / 011 | Total loss: 0.893 | Reg loss: 0.026 | Tree loss: 0.893 | Accuracy: 0.631000 | 1.501 sec/iter\n",
      "Epoch: 227 | Batch: 001 / 011 | Total loss: 0.870 | Reg loss: 0.026 | Tree loss: 0.870 | Accuracy: 0.653500 | 1.501 sec/iter\n",
      "Epoch: 227 | Batch: 002 / 011 | Total loss: 0.886 | Reg loss: 0.026 | Tree loss: 0.886 | Accuracy: 0.632500 | 1.5 sec/iter\n",
      "Epoch: 227 | Batch: 003 / 011 | Total loss: 0.830 | Reg loss: 0.026 | Tree loss: 0.830 | Accuracy: 0.667500 | 1.5 sec/iter\n",
      "Epoch: 227 | Batch: 004 / 011 | Total loss: 0.854 | Reg loss: 0.026 | Tree loss: 0.854 | Accuracy: 0.632500 | 1.5 sec/iter\n",
      "Epoch: 227 | Batch: 005 / 011 | Total loss: 0.817 | Reg loss: 0.026 | Tree loss: 0.817 | Accuracy: 0.650500 | 1.5 sec/iter\n",
      "Epoch: 227 | Batch: 006 / 011 | Total loss: 0.812 | Reg loss: 0.026 | Tree loss: 0.812 | Accuracy: 0.637000 | 1.5 sec/iter\n",
      "Epoch: 227 | Batch: 007 / 011 | Total loss: 0.785 | Reg loss: 0.026 | Tree loss: 0.785 | Accuracy: 0.656500 | 1.5 sec/iter\n",
      "Epoch: 227 | Batch: 008 / 011 | Total loss: 0.812 | Reg loss: 0.026 | Tree loss: 0.812 | Accuracy: 0.629500 | 1.499 sec/iter\n",
      "Epoch: 227 | Batch: 009 / 011 | Total loss: 0.808 | Reg loss: 0.026 | Tree loss: 0.808 | Accuracy: 0.630500 | 1.499 sec/iter\n",
      "Epoch: 227 | Batch: 010 / 011 | Total loss: 0.848 | Reg loss: 0.026 | Tree loss: 0.848 | Accuracy: 0.607509 | 1.499 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 228 | Batch: 000 / 011 | Total loss: 0.906 | Reg loss: 0.026 | Tree loss: 0.906 | Accuracy: 0.608500 | 1.501 sec/iter\n",
      "Epoch: 228 | Batch: 001 / 011 | Total loss: 0.866 | Reg loss: 0.026 | Tree loss: 0.866 | Accuracy: 0.647000 | 1.501 sec/iter\n",
      "Epoch: 228 | Batch: 002 / 011 | Total loss: 0.870 | Reg loss: 0.026 | Tree loss: 0.870 | Accuracy: 0.653000 | 1.501 sec/iter\n",
      "Epoch: 228 | Batch: 003 / 011 | Total loss: 0.848 | Reg loss: 0.026 | Tree loss: 0.848 | Accuracy: 0.654500 | 1.5 sec/iter\n",
      "Epoch: 228 | Batch: 004 / 011 | Total loss: 0.805 | Reg loss: 0.026 | Tree loss: 0.805 | Accuracy: 0.668000 | 1.5 sec/iter\n",
      "Epoch: 228 | Batch: 005 / 011 | Total loss: 0.806 | Reg loss: 0.026 | Tree loss: 0.806 | Accuracy: 0.664500 | 1.5 sec/iter\n",
      "Epoch: 228 | Batch: 006 / 011 | Total loss: 0.818 | Reg loss: 0.026 | Tree loss: 0.818 | Accuracy: 0.642000 | 1.5 sec/iter\n",
      "Epoch: 228 | Batch: 007 / 011 | Total loss: 0.841 | Reg loss: 0.026 | Tree loss: 0.841 | Accuracy: 0.608500 | 1.499 sec/iter\n",
      "Epoch: 228 | Batch: 008 / 011 | Total loss: 0.833 | Reg loss: 0.026 | Tree loss: 0.833 | Accuracy: 0.625000 | 1.499 sec/iter\n",
      "Epoch: 228 | Batch: 009 / 011 | Total loss: 0.783 | Reg loss: 0.026 | Tree loss: 0.783 | Accuracy: 0.637000 | 1.499 sec/iter\n",
      "Epoch: 228 | Batch: 010 / 011 | Total loss: 0.799 | Reg loss: 0.026 | Tree loss: 0.799 | Accuracy: 0.624573 | 1.499 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 229 | Batch: 000 / 011 | Total loss: 0.897 | Reg loss: 0.026 | Tree loss: 0.897 | Accuracy: 0.629500 | 1.499 sec/iter\n",
      "Epoch: 229 | Batch: 001 / 011 | Total loss: 0.853 | Reg loss: 0.026 | Tree loss: 0.853 | Accuracy: 0.642500 | 1.499 sec/iter\n",
      "Epoch: 229 | Batch: 002 / 011 | Total loss: 0.854 | Reg loss: 0.026 | Tree loss: 0.854 | Accuracy: 0.666500 | 1.499 sec/iter\n",
      "Epoch: 229 | Batch: 003 / 011 | Total loss: 0.820 | Reg loss: 0.026 | Tree loss: 0.820 | Accuracy: 0.674000 | 1.499 sec/iter\n",
      "Epoch: 229 | Batch: 004 / 011 | Total loss: 0.838 | Reg loss: 0.026 | Tree loss: 0.838 | Accuracy: 0.659000 | 1.499 sec/iter\n",
      "Epoch: 229 | Batch: 005 / 011 | Total loss: 0.817 | Reg loss: 0.026 | Tree loss: 0.817 | Accuracy: 0.662500 | 1.498 sec/iter\n",
      "Epoch: 229 | Batch: 006 / 011 | Total loss: 0.826 | Reg loss: 0.026 | Tree loss: 0.826 | Accuracy: 0.641500 | 1.498 sec/iter\n",
      "Epoch: 229 | Batch: 007 / 011 | Total loss: 0.842 | Reg loss: 0.026 | Tree loss: 0.842 | Accuracy: 0.622000 | 1.498 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 229 | Batch: 008 / 011 | Total loss: 0.839 | Reg loss: 0.026 | Tree loss: 0.839 | Accuracy: 0.608500 | 1.498 sec/iter\n",
      "Epoch: 229 | Batch: 009 / 011 | Total loss: 0.802 | Reg loss: 0.026 | Tree loss: 0.802 | Accuracy: 0.626500 | 1.498 sec/iter\n",
      "Epoch: 229 | Batch: 010 / 011 | Total loss: 0.824 | Reg loss: 0.026 | Tree loss: 0.824 | Accuracy: 0.634812 | 1.498 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 230 | Batch: 000 / 011 | Total loss: 0.893 | Reg loss: 0.026 | Tree loss: 0.893 | Accuracy: 0.624500 | 1.5 sec/iter\n",
      "Epoch: 230 | Batch: 001 / 011 | Total loss: 0.866 | Reg loss: 0.026 | Tree loss: 0.866 | Accuracy: 0.640000 | 1.5 sec/iter\n",
      "Epoch: 230 | Batch: 002 / 011 | Total loss: 0.865 | Reg loss: 0.026 | Tree loss: 0.865 | Accuracy: 0.649500 | 1.5 sec/iter\n",
      "Epoch: 230 | Batch: 003 / 011 | Total loss: 0.836 | Reg loss: 0.026 | Tree loss: 0.836 | Accuracy: 0.668000 | 1.499 sec/iter\n",
      "Epoch: 230 | Batch: 004 / 011 | Total loss: 0.819 | Reg loss: 0.026 | Tree loss: 0.819 | Accuracy: 0.651000 | 1.499 sec/iter\n",
      "Epoch: 230 | Batch: 005 / 011 | Total loss: 0.841 | Reg loss: 0.026 | Tree loss: 0.841 | Accuracy: 0.639000 | 1.499 sec/iter\n",
      "Epoch: 230 | Batch: 006 / 011 | Total loss: 0.825 | Reg loss: 0.026 | Tree loss: 0.825 | Accuracy: 0.636000 | 1.499 sec/iter\n",
      "Epoch: 230 | Batch: 007 / 011 | Total loss: 0.811 | Reg loss: 0.026 | Tree loss: 0.811 | Accuracy: 0.648000 | 1.499 sec/iter\n",
      "Epoch: 230 | Batch: 008 / 011 | Total loss: 0.821 | Reg loss: 0.026 | Tree loss: 0.821 | Accuracy: 0.629500 | 1.499 sec/iter\n",
      "Epoch: 230 | Batch: 009 / 011 | Total loss: 0.801 | Reg loss: 0.026 | Tree loss: 0.801 | Accuracy: 0.635500 | 1.498 sec/iter\n",
      "Epoch: 230 | Batch: 010 / 011 | Total loss: 0.791 | Reg loss: 0.026 | Tree loss: 0.791 | Accuracy: 0.634812 | 1.498 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 231 | Batch: 000 / 011 | Total loss: 0.880 | Reg loss: 0.026 | Tree loss: 0.880 | Accuracy: 0.640000 | 1.5 sec/iter\n",
      "Epoch: 231 | Batch: 001 / 011 | Total loss: 0.875 | Reg loss: 0.026 | Tree loss: 0.875 | Accuracy: 0.645000 | 1.5 sec/iter\n",
      "Epoch: 231 | Batch: 002 / 011 | Total loss: 0.856 | Reg loss: 0.026 | Tree loss: 0.856 | Accuracy: 0.650500 | 1.5 sec/iter\n",
      "Epoch: 231 | Batch: 003 / 011 | Total loss: 0.847 | Reg loss: 0.026 | Tree loss: 0.847 | Accuracy: 0.646500 | 1.5 sec/iter\n",
      "Epoch: 231 | Batch: 004 / 011 | Total loss: 0.820 | Reg loss: 0.026 | Tree loss: 0.820 | Accuracy: 0.645000 | 1.5 sec/iter\n",
      "Epoch: 231 | Batch: 005 / 011 | Total loss: 0.801 | Reg loss: 0.026 | Tree loss: 0.801 | Accuracy: 0.645000 | 1.499 sec/iter\n",
      "Epoch: 231 | Batch: 006 / 011 | Total loss: 0.832 | Reg loss: 0.026 | Tree loss: 0.832 | Accuracy: 0.626000 | 1.499 sec/iter\n",
      "Epoch: 231 | Batch: 007 / 011 | Total loss: 0.815 | Reg loss: 0.026 | Tree loss: 0.815 | Accuracy: 0.620500 | 1.499 sec/iter\n",
      "Epoch: 231 | Batch: 008 / 011 | Total loss: 0.811 | Reg loss: 0.026 | Tree loss: 0.811 | Accuracy: 0.619500 | 1.499 sec/iter\n",
      "Epoch: 231 | Batch: 009 / 011 | Total loss: 0.849 | Reg loss: 0.026 | Tree loss: 0.849 | Accuracy: 0.606500 | 1.499 sec/iter\n",
      "Epoch: 231 | Batch: 010 / 011 | Total loss: 0.765 | Reg loss: 0.026 | Tree loss: 0.765 | Accuracy: 0.665529 | 1.499 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 232 | Batch: 000 / 011 | Total loss: 0.888 | Reg loss: 0.026 | Tree loss: 0.888 | Accuracy: 0.625500 | 1.5 sec/iter\n",
      "Epoch: 232 | Batch: 001 / 011 | Total loss: 0.845 | Reg loss: 0.026 | Tree loss: 0.845 | Accuracy: 0.653000 | 1.5 sec/iter\n",
      "Epoch: 232 | Batch: 002 / 011 | Total loss: 0.862 | Reg loss: 0.026 | Tree loss: 0.862 | Accuracy: 0.640000 | 1.5 sec/iter\n",
      "Epoch: 232 | Batch: 003 / 011 | Total loss: 0.870 | Reg loss: 0.026 | Tree loss: 0.870 | Accuracy: 0.630000 | 1.5 sec/iter\n",
      "Epoch: 232 | Batch: 004 / 011 | Total loss: 0.831 | Reg loss: 0.026 | Tree loss: 0.831 | Accuracy: 0.651500 | 1.5 sec/iter\n",
      "Epoch: 232 | Batch: 005 / 011 | Total loss: 0.830 | Reg loss: 0.026 | Tree loss: 0.830 | Accuracy: 0.659000 | 1.5 sec/iter\n",
      "Epoch: 232 | Batch: 006 / 011 | Total loss: 0.817 | Reg loss: 0.026 | Tree loss: 0.817 | Accuracy: 0.644500 | 1.499 sec/iter\n",
      "Epoch: 232 | Batch: 007 / 011 | Total loss: 0.821 | Reg loss: 0.026 | Tree loss: 0.821 | Accuracy: 0.629500 | 1.499 sec/iter\n",
      "Epoch: 232 | Batch: 008 / 011 | Total loss: 0.814 | Reg loss: 0.026 | Tree loss: 0.814 | Accuracy: 0.618000 | 1.499 sec/iter\n",
      "Epoch: 232 | Batch: 009 / 011 | Total loss: 0.821 | Reg loss: 0.026 | Tree loss: 0.821 | Accuracy: 0.609000 | 1.499 sec/iter\n",
      "Epoch: 232 | Batch: 010 / 011 | Total loss: 0.834 | Reg loss: 0.026 | Tree loss: 0.834 | Accuracy: 0.587031 | 1.499 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 233 | Batch: 000 / 011 | Total loss: 0.895 | Reg loss: 0.026 | Tree loss: 0.895 | Accuracy: 0.634000 | 1.501 sec/iter\n",
      "Epoch: 233 | Batch: 001 / 011 | Total loss: 0.910 | Reg loss: 0.026 | Tree loss: 0.910 | Accuracy: 0.624000 | 1.501 sec/iter\n",
      "Epoch: 233 | Batch: 002 / 011 | Total loss: 0.847 | Reg loss: 0.026 | Tree loss: 0.847 | Accuracy: 0.661000 | 1.5 sec/iter\n",
      "Epoch: 233 | Batch: 003 / 011 | Total loss: 0.853 | Reg loss: 0.026 | Tree loss: 0.853 | Accuracy: 0.648500 | 1.5 sec/iter\n",
      "Epoch: 233 | Batch: 004 / 011 | Total loss: 0.832 | Reg loss: 0.026 | Tree loss: 0.832 | Accuracy: 0.653500 | 1.5 sec/iter\n",
      "Epoch: 233 | Batch: 005 / 011 | Total loss: 0.829 | Reg loss: 0.026 | Tree loss: 0.829 | Accuracy: 0.626000 | 1.5 sec/iter\n",
      "Epoch: 233 | Batch: 006 / 011 | Total loss: 0.801 | Reg loss: 0.026 | Tree loss: 0.801 | Accuracy: 0.640000 | 1.5 sec/iter\n",
      "Epoch: 233 | Batch: 007 / 011 | Total loss: 0.804 | Reg loss: 0.026 | Tree loss: 0.804 | Accuracy: 0.625500 | 1.5 sec/iter\n",
      "Epoch: 233 | Batch: 008 / 011 | Total loss: 0.798 | Reg loss: 0.026 | Tree loss: 0.798 | Accuracy: 0.631500 | 1.5 sec/iter\n",
      "Epoch: 233 | Batch: 009 / 011 | Total loss: 0.814 | Reg loss: 0.026 | Tree loss: 0.814 | Accuracy: 0.626000 | 1.499 sec/iter\n",
      "Epoch: 233 | Batch: 010 / 011 | Total loss: 0.833 | Reg loss: 0.026 | Tree loss: 0.833 | Accuracy: 0.597270 | 1.499 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 234 | Batch: 000 / 011 | Total loss: 0.893 | Reg loss: 0.026 | Tree loss: 0.893 | Accuracy: 0.617000 | 1.501 sec/iter\n",
      "Epoch: 234 | Batch: 001 / 011 | Total loss: 0.863 | Reg loss: 0.026 | Tree loss: 0.863 | Accuracy: 0.659000 | 1.501 sec/iter\n",
      "Epoch: 234 | Batch: 002 / 011 | Total loss: 0.859 | Reg loss: 0.026 | Tree loss: 0.859 | Accuracy: 0.652000 | 1.5 sec/iter\n",
      "Epoch: 234 | Batch: 003 / 011 | Total loss: 0.871 | Reg loss: 0.026 | Tree loss: 0.871 | Accuracy: 0.651500 | 1.5 sec/iter\n",
      "Epoch: 234 | Batch: 004 / 011 | Total loss: 0.846 | Reg loss: 0.026 | Tree loss: 0.846 | Accuracy: 0.637500 | 1.5 sec/iter\n",
      "Epoch: 234 | Batch: 005 / 011 | Total loss: 0.840 | Reg loss: 0.026 | Tree loss: 0.840 | Accuracy: 0.647500 | 1.5 sec/iter\n",
      "Epoch: 234 | Batch: 006 / 011 | Total loss: 0.790 | Reg loss: 0.026 | Tree loss: 0.790 | Accuracy: 0.662000 | 1.5 sec/iter\n",
      "Epoch: 234 | Batch: 007 / 011 | Total loss: 0.811 | Reg loss: 0.026 | Tree loss: 0.811 | Accuracy: 0.638000 | 1.5 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 234 | Batch: 008 / 011 | Total loss: 0.808 | Reg loss: 0.026 | Tree loss: 0.808 | Accuracy: 0.623500 | 1.499 sec/iter\n",
      "Epoch: 234 | Batch: 009 / 011 | Total loss: 0.816 | Reg loss: 0.026 | Tree loss: 0.816 | Accuracy: 0.601000 | 1.499 sec/iter\n",
      "Epoch: 234 | Batch: 010 / 011 | Total loss: 0.857 | Reg loss: 0.026 | Tree loss: 0.857 | Accuracy: 0.590444 | 1.499 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 235 | Batch: 000 / 011 | Total loss: 0.885 | Reg loss: 0.026 | Tree loss: 0.885 | Accuracy: 0.635500 | 1.5 sec/iter\n",
      "Epoch: 235 | Batch: 001 / 011 | Total loss: 0.865 | Reg loss: 0.026 | Tree loss: 0.865 | Accuracy: 0.651000 | 1.5 sec/iter\n",
      "Epoch: 235 | Batch: 002 / 011 | Total loss: 0.848 | Reg loss: 0.026 | Tree loss: 0.848 | Accuracy: 0.653500 | 1.499 sec/iter\n",
      "Epoch: 235 | Batch: 003 / 011 | Total loss: 0.854 | Reg loss: 0.026 | Tree loss: 0.854 | Accuracy: 0.639500 | 1.499 sec/iter\n",
      "Epoch: 235 | Batch: 004 / 011 | Total loss: 0.829 | Reg loss: 0.026 | Tree loss: 0.829 | Accuracy: 0.650500 | 1.499 sec/iter\n",
      "Epoch: 235 | Batch: 005 / 011 | Total loss: 0.839 | Reg loss: 0.026 | Tree loss: 0.839 | Accuracy: 0.626000 | 1.499 sec/iter\n",
      "Epoch: 235 | Batch: 006 / 011 | Total loss: 0.802 | Reg loss: 0.026 | Tree loss: 0.802 | Accuracy: 0.634000 | 1.499 sec/iter\n",
      "Epoch: 235 | Batch: 007 / 011 | Total loss: 0.820 | Reg loss: 0.026 | Tree loss: 0.820 | Accuracy: 0.619000 | 1.499 sec/iter\n",
      "Epoch: 235 | Batch: 008 / 011 | Total loss: 0.795 | Reg loss: 0.026 | Tree loss: 0.795 | Accuracy: 0.634000 | 1.498 sec/iter\n",
      "Epoch: 235 | Batch: 009 / 011 | Total loss: 0.822 | Reg loss: 0.026 | Tree loss: 0.822 | Accuracy: 0.621000 | 1.498 sec/iter\n",
      "Epoch: 235 | Batch: 010 / 011 | Total loss: 0.796 | Reg loss: 0.026 | Tree loss: 0.796 | Accuracy: 0.648464 | 1.498 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 236 | Batch: 000 / 011 | Total loss: 0.894 | Reg loss: 0.026 | Tree loss: 0.894 | Accuracy: 0.627500 | 1.5 sec/iter\n",
      "Epoch: 236 | Batch: 001 / 011 | Total loss: 0.906 | Reg loss: 0.026 | Tree loss: 0.906 | Accuracy: 0.617500 | 1.5 sec/iter\n",
      "Epoch: 236 | Batch: 002 / 011 | Total loss: 0.837 | Reg loss: 0.026 | Tree loss: 0.837 | Accuracy: 0.650500 | 1.5 sec/iter\n",
      "Epoch: 236 | Batch: 003 / 011 | Total loss: 0.829 | Reg loss: 0.026 | Tree loss: 0.829 | Accuracy: 0.659500 | 1.5 sec/iter\n",
      "Epoch: 236 | Batch: 004 / 011 | Total loss: 0.840 | Reg loss: 0.026 | Tree loss: 0.840 | Accuracy: 0.650000 | 1.499 sec/iter\n",
      "Epoch: 236 | Batch: 005 / 011 | Total loss: 0.824 | Reg loss: 0.026 | Tree loss: 0.824 | Accuracy: 0.633500 | 1.499 sec/iter\n",
      "Epoch: 236 | Batch: 006 / 011 | Total loss: 0.814 | Reg loss: 0.026 | Tree loss: 0.814 | Accuracy: 0.637000 | 1.499 sec/iter\n",
      "Epoch: 236 | Batch: 007 / 011 | Total loss: 0.802 | Reg loss: 0.026 | Tree loss: 0.802 | Accuracy: 0.640500 | 1.499 sec/iter\n",
      "Epoch: 236 | Batch: 008 / 011 | Total loss: 0.813 | Reg loss: 0.026 | Tree loss: 0.813 | Accuracy: 0.630500 | 1.499 sec/iter\n",
      "Epoch: 236 | Batch: 009 / 011 | Total loss: 0.802 | Reg loss: 0.026 | Tree loss: 0.802 | Accuracy: 0.633000 | 1.499 sec/iter\n",
      "Epoch: 236 | Batch: 010 / 011 | Total loss: 0.796 | Reg loss: 0.026 | Tree loss: 0.796 | Accuracy: 0.645051 | 1.499 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 237 | Batch: 000 / 011 | Total loss: 0.875 | Reg loss: 0.026 | Tree loss: 0.875 | Accuracy: 0.637000 | 1.5 sec/iter\n",
      "Epoch: 237 | Batch: 001 / 011 | Total loss: 0.889 | Reg loss: 0.026 | Tree loss: 0.889 | Accuracy: 0.631500 | 1.5 sec/iter\n",
      "Epoch: 237 | Batch: 002 / 011 | Total loss: 0.855 | Reg loss: 0.026 | Tree loss: 0.855 | Accuracy: 0.658000 | 1.5 sec/iter\n",
      "Epoch: 237 | Batch: 003 / 011 | Total loss: 0.859 | Reg loss: 0.026 | Tree loss: 0.859 | Accuracy: 0.649500 | 1.5 sec/iter\n",
      "Epoch: 237 | Batch: 004 / 011 | Total loss: 0.814 | Reg loss: 0.026 | Tree loss: 0.814 | Accuracy: 0.660500 | 1.499 sec/iter\n",
      "Epoch: 237 | Batch: 005 / 011 | Total loss: 0.817 | Reg loss: 0.026 | Tree loss: 0.817 | Accuracy: 0.661500 | 1.499 sec/iter\n",
      "Epoch: 237 | Batch: 006 / 011 | Total loss: 0.825 | Reg loss: 0.026 | Tree loss: 0.825 | Accuracy: 0.649000 | 1.499 sec/iter\n",
      "Epoch: 237 | Batch: 007 / 011 | Total loss: 0.801 | Reg loss: 0.026 | Tree loss: 0.801 | Accuracy: 0.638500 | 1.499 sec/iter\n",
      "Epoch: 237 | Batch: 008 / 011 | Total loss: 0.843 | Reg loss: 0.026 | Tree loss: 0.843 | Accuracy: 0.612500 | 1.499 sec/iter\n",
      "Epoch: 237 | Batch: 009 / 011 | Total loss: 0.807 | Reg loss: 0.026 | Tree loss: 0.807 | Accuracy: 0.619500 | 1.499 sec/iter\n",
      "Epoch: 237 | Batch: 010 / 011 | Total loss: 0.816 | Reg loss: 0.026 | Tree loss: 0.816 | Accuracy: 0.634812 | 1.499 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 238 | Batch: 000 / 011 | Total loss: 0.877 | Reg loss: 0.026 | Tree loss: 0.877 | Accuracy: 0.645500 | 1.5 sec/iter\n",
      "Epoch: 238 | Batch: 001 / 011 | Total loss: 0.888 | Reg loss: 0.026 | Tree loss: 0.888 | Accuracy: 0.641000 | 1.5 sec/iter\n",
      "Epoch: 238 | Batch: 002 / 011 | Total loss: 0.848 | Reg loss: 0.026 | Tree loss: 0.848 | Accuracy: 0.673000 | 1.5 sec/iter\n",
      "Epoch: 238 | Batch: 003 / 011 | Total loss: 0.830 | Reg loss: 0.026 | Tree loss: 0.830 | Accuracy: 0.658000 | 1.5 sec/iter\n",
      "Epoch: 238 | Batch: 004 / 011 | Total loss: 0.834 | Reg loss: 0.026 | Tree loss: 0.834 | Accuracy: 0.635000 | 1.5 sec/iter\n",
      "Epoch: 238 | Batch: 005 / 011 | Total loss: 0.843 | Reg loss: 0.026 | Tree loss: 0.843 | Accuracy: 0.629000 | 1.5 sec/iter\n",
      "Epoch: 238 | Batch: 006 / 011 | Total loss: 0.797 | Reg loss: 0.026 | Tree loss: 0.797 | Accuracy: 0.649500 | 1.499 sec/iter\n",
      "Epoch: 238 | Batch: 007 / 011 | Total loss: 0.803 | Reg loss: 0.026 | Tree loss: 0.803 | Accuracy: 0.620000 | 1.499 sec/iter\n",
      "Epoch: 238 | Batch: 008 / 011 | Total loss: 0.812 | Reg loss: 0.026 | Tree loss: 0.812 | Accuracy: 0.621500 | 1.499 sec/iter\n",
      "Epoch: 238 | Batch: 009 / 011 | Total loss: 0.806 | Reg loss: 0.026 | Tree loss: 0.806 | Accuracy: 0.639500 | 1.499 sec/iter\n",
      "Epoch: 238 | Batch: 010 / 011 | Total loss: 0.876 | Reg loss: 0.026 | Tree loss: 0.876 | Accuracy: 0.604096 | 1.499 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 239 | Batch: 000 / 011 | Total loss: 0.889 | Reg loss: 0.026 | Tree loss: 0.889 | Accuracy: 0.635500 | 1.501 sec/iter\n",
      "Epoch: 239 | Batch: 001 / 011 | Total loss: 0.872 | Reg loss: 0.026 | Tree loss: 0.872 | Accuracy: 0.658000 | 1.5 sec/iter\n",
      "Epoch: 239 | Batch: 002 / 011 | Total loss: 0.870 | Reg loss: 0.026 | Tree loss: 0.870 | Accuracy: 0.644000 | 1.5 sec/iter\n",
      "Epoch: 239 | Batch: 003 / 011 | Total loss: 0.828 | Reg loss: 0.026 | Tree loss: 0.828 | Accuracy: 0.671500 | 1.5 sec/iter\n",
      "Epoch: 239 | Batch: 004 / 011 | Total loss: 0.820 | Reg loss: 0.026 | Tree loss: 0.820 | Accuracy: 0.648500 | 1.5 sec/iter\n",
      "Epoch: 239 | Batch: 005 / 011 | Total loss: 0.828 | Reg loss: 0.026 | Tree loss: 0.828 | Accuracy: 0.651000 | 1.5 sec/iter\n",
      "Epoch: 239 | Batch: 006 / 011 | Total loss: 0.831 | Reg loss: 0.026 | Tree loss: 0.831 | Accuracy: 0.645000 | 1.5 sec/iter\n",
      "Epoch: 239 | Batch: 007 / 011 | Total loss: 0.815 | Reg loss: 0.026 | Tree loss: 0.815 | Accuracy: 0.633000 | 1.499 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 239 | Batch: 008 / 011 | Total loss: 0.808 | Reg loss: 0.026 | Tree loss: 0.808 | Accuracy: 0.626500 | 1.499 sec/iter\n",
      "Epoch: 239 | Batch: 009 / 011 | Total loss: 0.788 | Reg loss: 0.026 | Tree loss: 0.788 | Accuracy: 0.624000 | 1.499 sec/iter\n",
      "Epoch: 239 | Batch: 010 / 011 | Total loss: 0.815 | Reg loss: 0.026 | Tree loss: 0.815 | Accuracy: 0.607509 | 1.499 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 240 | Batch: 000 / 011 | Total loss: 0.874 | Reg loss: 0.026 | Tree loss: 0.874 | Accuracy: 0.634000 | 1.501 sec/iter\n",
      "Epoch: 240 | Batch: 001 / 011 | Total loss: 0.854 | Reg loss: 0.026 | Tree loss: 0.854 | Accuracy: 0.650000 | 1.501 sec/iter\n",
      "Epoch: 240 | Batch: 002 / 011 | Total loss: 0.887 | Reg loss: 0.026 | Tree loss: 0.887 | Accuracy: 0.621500 | 1.5 sec/iter\n",
      "Epoch: 240 | Batch: 003 / 011 | Total loss: 0.836 | Reg loss: 0.026 | Tree loss: 0.836 | Accuracy: 0.666500 | 1.5 sec/iter\n",
      "Epoch: 240 | Batch: 004 / 011 | Total loss: 0.837 | Reg loss: 0.026 | Tree loss: 0.837 | Accuracy: 0.631500 | 1.5 sec/iter\n",
      "Epoch: 240 | Batch: 005 / 011 | Total loss: 0.796 | Reg loss: 0.026 | Tree loss: 0.796 | Accuracy: 0.652500 | 1.5 sec/iter\n",
      "Epoch: 240 | Batch: 006 / 011 | Total loss: 0.797 | Reg loss: 0.026 | Tree loss: 0.797 | Accuracy: 0.643500 | 1.5 sec/iter\n",
      "Epoch: 240 | Batch: 007 / 011 | Total loss: 0.844 | Reg loss: 0.026 | Tree loss: 0.844 | Accuracy: 0.607500 | 1.499 sec/iter\n",
      "Epoch: 240 | Batch: 008 / 011 | Total loss: 0.823 | Reg loss: 0.026 | Tree loss: 0.823 | Accuracy: 0.626500 | 1.499 sec/iter\n",
      "Epoch: 240 | Batch: 009 / 011 | Total loss: 0.812 | Reg loss: 0.026 | Tree loss: 0.812 | Accuracy: 0.622000 | 1.499 sec/iter\n",
      "Epoch: 240 | Batch: 010 / 011 | Total loss: 0.806 | Reg loss: 0.026 | Tree loss: 0.806 | Accuracy: 0.645051 | 1.499 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 241 | Batch: 000 / 011 | Total loss: 0.894 | Reg loss: 0.026 | Tree loss: 0.894 | Accuracy: 0.624500 | 1.499 sec/iter\n",
      "Epoch: 241 | Batch: 001 / 011 | Total loss: 0.881 | Reg loss: 0.026 | Tree loss: 0.881 | Accuracy: 0.636000 | 1.499 sec/iter\n",
      "Epoch: 241 | Batch: 002 / 011 | Total loss: 0.844 | Reg loss: 0.026 | Tree loss: 0.844 | Accuracy: 0.662000 | 1.499 sec/iter\n",
      "Epoch: 241 | Batch: 003 / 011 | Total loss: 0.852 | Reg loss: 0.026 | Tree loss: 0.852 | Accuracy: 0.630500 | 1.499 sec/iter\n",
      "Epoch: 241 | Batch: 004 / 011 | Total loss: 0.822 | Reg loss: 0.026 | Tree loss: 0.822 | Accuracy: 0.649000 | 1.499 sec/iter\n",
      "Epoch: 241 | Batch: 005 / 011 | Total loss: 0.829 | Reg loss: 0.026 | Tree loss: 0.829 | Accuracy: 0.659000 | 1.499 sec/iter\n",
      "Epoch: 241 | Batch: 006 / 011 | Total loss: 0.826 | Reg loss: 0.026 | Tree loss: 0.826 | Accuracy: 0.633500 | 1.499 sec/iter\n",
      "Epoch: 241 | Batch: 007 / 011 | Total loss: 0.803 | Reg loss: 0.026 | Tree loss: 0.803 | Accuracy: 0.642500 | 1.499 sec/iter\n",
      "Epoch: 241 | Batch: 008 / 011 | Total loss: 0.812 | Reg loss: 0.026 | Tree loss: 0.812 | Accuracy: 0.630000 | 1.498 sec/iter\n",
      "Epoch: 241 | Batch: 009 / 011 | Total loss: 0.801 | Reg loss: 0.026 | Tree loss: 0.801 | Accuracy: 0.630000 | 1.498 sec/iter\n",
      "Epoch: 241 | Batch: 010 / 011 | Total loss: 0.730 | Reg loss: 0.026 | Tree loss: 0.730 | Accuracy: 0.696246 | 1.498 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 242 | Batch: 000 / 011 | Total loss: 0.859 | Reg loss: 0.026 | Tree loss: 0.859 | Accuracy: 0.647500 | 1.5 sec/iter\n",
      "Epoch: 242 | Batch: 001 / 011 | Total loss: 0.866 | Reg loss: 0.026 | Tree loss: 0.866 | Accuracy: 0.645500 | 1.5 sec/iter\n",
      "Epoch: 242 | Batch: 002 / 011 | Total loss: 0.866 | Reg loss: 0.026 | Tree loss: 0.866 | Accuracy: 0.641000 | 1.5 sec/iter\n",
      "Epoch: 242 | Batch: 003 / 011 | Total loss: 0.822 | Reg loss: 0.026 | Tree loss: 0.822 | Accuracy: 0.672500 | 1.5 sec/iter\n",
      "Epoch: 242 | Batch: 004 / 011 | Total loss: 0.841 | Reg loss: 0.026 | Tree loss: 0.841 | Accuracy: 0.637000 | 1.5 sec/iter\n",
      "Epoch: 242 | Batch: 005 / 011 | Total loss: 0.837 | Reg loss: 0.026 | Tree loss: 0.837 | Accuracy: 0.635500 | 1.499 sec/iter\n",
      "Epoch: 242 | Batch: 006 / 011 | Total loss: 0.816 | Reg loss: 0.026 | Tree loss: 0.816 | Accuracy: 0.631500 | 1.499 sec/iter\n",
      "Epoch: 242 | Batch: 007 / 011 | Total loss: 0.813 | Reg loss: 0.026 | Tree loss: 0.813 | Accuracy: 0.625500 | 1.499 sec/iter\n",
      "Epoch: 242 | Batch: 008 / 011 | Total loss: 0.808 | Reg loss: 0.026 | Tree loss: 0.808 | Accuracy: 0.637500 | 1.499 sec/iter\n",
      "Epoch: 242 | Batch: 009 / 011 | Total loss: 0.818 | Reg loss: 0.026 | Tree loss: 0.818 | Accuracy: 0.634000 | 1.499 sec/iter\n",
      "Epoch: 242 | Batch: 010 / 011 | Total loss: 0.774 | Reg loss: 0.026 | Tree loss: 0.774 | Accuracy: 0.634812 | 1.499 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 243 | Batch: 000 / 011 | Total loss: 0.903 | Reg loss: 0.026 | Tree loss: 0.903 | Accuracy: 0.623000 | 1.5 sec/iter\n",
      "Epoch: 243 | Batch: 001 / 011 | Total loss: 0.870 | Reg loss: 0.026 | Tree loss: 0.870 | Accuracy: 0.649000 | 1.5 sec/iter\n",
      "Epoch: 243 | Batch: 002 / 011 | Total loss: 0.863 | Reg loss: 0.026 | Tree loss: 0.863 | Accuracy: 0.642000 | 1.5 sec/iter\n",
      "Epoch: 243 | Batch: 003 / 011 | Total loss: 0.834 | Reg loss: 0.026 | Tree loss: 0.834 | Accuracy: 0.654000 | 1.5 sec/iter\n",
      "Epoch: 243 | Batch: 004 / 011 | Total loss: 0.837 | Reg loss: 0.026 | Tree loss: 0.837 | Accuracy: 0.653000 | 1.5 sec/iter\n",
      "Epoch: 243 | Batch: 005 / 011 | Total loss: 0.815 | Reg loss: 0.026 | Tree loss: 0.815 | Accuracy: 0.646000 | 1.5 sec/iter\n",
      "Epoch: 243 | Batch: 006 / 011 | Total loss: 0.831 | Reg loss: 0.026 | Tree loss: 0.831 | Accuracy: 0.634500 | 1.499 sec/iter\n",
      "Epoch: 243 | Batch: 007 / 011 | Total loss: 0.791 | Reg loss: 0.026 | Tree loss: 0.791 | Accuracy: 0.633000 | 1.499 sec/iter\n",
      "Epoch: 243 | Batch: 008 / 011 | Total loss: 0.816 | Reg loss: 0.026 | Tree loss: 0.816 | Accuracy: 0.624500 | 1.499 sec/iter\n",
      "Epoch: 243 | Batch: 009 / 011 | Total loss: 0.824 | Reg loss: 0.026 | Tree loss: 0.824 | Accuracy: 0.630000 | 1.499 sec/iter\n",
      "Epoch: 243 | Batch: 010 / 011 | Total loss: 0.767 | Reg loss: 0.026 | Tree loss: 0.767 | Accuracy: 0.651877 | 1.499 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 244 | Batch: 000 / 011 | Total loss: 0.909 | Reg loss: 0.025 | Tree loss: 0.909 | Accuracy: 0.610500 | 1.5 sec/iter\n",
      "Epoch: 244 | Batch: 001 / 011 | Total loss: 0.872 | Reg loss: 0.026 | Tree loss: 0.872 | Accuracy: 0.637500 | 1.5 sec/iter\n",
      "Epoch: 244 | Batch: 002 / 011 | Total loss: 0.853 | Reg loss: 0.026 | Tree loss: 0.853 | Accuracy: 0.655000 | 1.5 sec/iter\n",
      "Epoch: 244 | Batch: 003 / 011 | Total loss: 0.878 | Reg loss: 0.026 | Tree loss: 0.878 | Accuracy: 0.643000 | 1.5 sec/iter\n",
      "Epoch: 244 | Batch: 004 / 011 | Total loss: 0.826 | Reg loss: 0.026 | Tree loss: 0.826 | Accuracy: 0.649500 | 1.5 sec/iter\n",
      "Epoch: 244 | Batch: 005 / 011 | Total loss: 0.815 | Reg loss: 0.026 | Tree loss: 0.815 | Accuracy: 0.664500 | 1.499 sec/iter\n",
      "Epoch: 244 | Batch: 006 / 011 | Total loss: 0.806 | Reg loss: 0.026 | Tree loss: 0.806 | Accuracy: 0.661500 | 1.499 sec/iter\n",
      "Epoch: 244 | Batch: 007 / 011 | Total loss: 0.799 | Reg loss: 0.026 | Tree loss: 0.799 | Accuracy: 0.632500 | 1.499 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 244 | Batch: 008 / 011 | Total loss: 0.803 | Reg loss: 0.026 | Tree loss: 0.803 | Accuracy: 0.635500 | 1.499 sec/iter\n",
      "Epoch: 244 | Batch: 009 / 011 | Total loss: 0.817 | Reg loss: 0.026 | Tree loss: 0.817 | Accuracy: 0.614500 | 1.499 sec/iter\n",
      "Epoch: 244 | Batch: 010 / 011 | Total loss: 0.864 | Reg loss: 0.026 | Tree loss: 0.864 | Accuracy: 0.641638 | 1.498 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 245 | Batch: 000 / 011 | Total loss: 0.857 | Reg loss: 0.025 | Tree loss: 0.857 | Accuracy: 0.650500 | 1.5 sec/iter\n",
      "Epoch: 245 | Batch: 001 / 011 | Total loss: 0.868 | Reg loss: 0.025 | Tree loss: 0.868 | Accuracy: 0.636500 | 1.5 sec/iter\n",
      "Epoch: 245 | Batch: 002 / 011 | Total loss: 0.854 | Reg loss: 0.026 | Tree loss: 0.854 | Accuracy: 0.643500 | 1.5 sec/iter\n",
      "Epoch: 245 | Batch: 003 / 011 | Total loss: 0.876 | Reg loss: 0.026 | Tree loss: 0.876 | Accuracy: 0.652500 | 1.5 sec/iter\n",
      "Epoch: 245 | Batch: 004 / 011 | Total loss: 0.813 | Reg loss: 0.026 | Tree loss: 0.813 | Accuracy: 0.656000 | 1.5 sec/iter\n",
      "Epoch: 245 | Batch: 005 / 011 | Total loss: 0.829 | Reg loss: 0.026 | Tree loss: 0.829 | Accuracy: 0.643000 | 1.5 sec/iter\n",
      "Epoch: 245 | Batch: 006 / 011 | Total loss: 0.830 | Reg loss: 0.026 | Tree loss: 0.830 | Accuracy: 0.638000 | 1.499 sec/iter\n",
      "Epoch: 245 | Batch: 007 / 011 | Total loss: 0.817 | Reg loss: 0.026 | Tree loss: 0.817 | Accuracy: 0.625500 | 1.499 sec/iter\n",
      "Epoch: 245 | Batch: 008 / 011 | Total loss: 0.788 | Reg loss: 0.026 | Tree loss: 0.788 | Accuracy: 0.641500 | 1.499 sec/iter\n",
      "Epoch: 245 | Batch: 009 / 011 | Total loss: 0.810 | Reg loss: 0.026 | Tree loss: 0.810 | Accuracy: 0.638000 | 1.499 sec/iter\n",
      "Epoch: 245 | Batch: 010 / 011 | Total loss: 0.788 | Reg loss: 0.026 | Tree loss: 0.788 | Accuracy: 0.655290 | 1.499 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 246 | Batch: 000 / 011 | Total loss: 0.906 | Reg loss: 0.025 | Tree loss: 0.906 | Accuracy: 0.613000 | 1.501 sec/iter\n",
      "Epoch: 246 | Batch: 001 / 011 | Total loss: 0.875 | Reg loss: 0.025 | Tree loss: 0.875 | Accuracy: 0.642000 | 1.5 sec/iter\n",
      "Epoch: 246 | Batch: 002 / 011 | Total loss: 0.848 | Reg loss: 0.025 | Tree loss: 0.848 | Accuracy: 0.662500 | 1.5 sec/iter\n",
      "Epoch: 246 | Batch: 003 / 011 | Total loss: 0.829 | Reg loss: 0.026 | Tree loss: 0.829 | Accuracy: 0.666000 | 1.5 sec/iter\n",
      "Epoch: 246 | Batch: 004 / 011 | Total loss: 0.813 | Reg loss: 0.026 | Tree loss: 0.813 | Accuracy: 0.653500 | 1.5 sec/iter\n",
      "Epoch: 246 | Batch: 005 / 011 | Total loss: 0.814 | Reg loss: 0.026 | Tree loss: 0.814 | Accuracy: 0.672500 | 1.5 sec/iter\n",
      "Epoch: 246 | Batch: 006 / 011 | Total loss: 0.837 | Reg loss: 0.026 | Tree loss: 0.837 | Accuracy: 0.633000 | 1.499 sec/iter\n",
      "Epoch: 246 | Batch: 007 / 011 | Total loss: 0.816 | Reg loss: 0.026 | Tree loss: 0.816 | Accuracy: 0.617000 | 1.499 sec/iter\n",
      "Epoch: 246 | Batch: 008 / 011 | Total loss: 0.811 | Reg loss: 0.026 | Tree loss: 0.811 | Accuracy: 0.630000 | 1.499 sec/iter\n",
      "Epoch: 246 | Batch: 009 / 011 | Total loss: 0.801 | Reg loss: 0.026 | Tree loss: 0.801 | Accuracy: 0.633500 | 1.499 sec/iter\n",
      "Epoch: 246 | Batch: 010 / 011 | Total loss: 0.817 | Reg loss: 0.026 | Tree loss: 0.817 | Accuracy: 0.631399 | 1.499 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 247 | Batch: 000 / 011 | Total loss: 0.893 | Reg loss: 0.025 | Tree loss: 0.893 | Accuracy: 0.620500 | 1.499 sec/iter\n",
      "Epoch: 247 | Batch: 001 / 011 | Total loss: 0.883 | Reg loss: 0.026 | Tree loss: 0.883 | Accuracy: 0.634500 | 1.499 sec/iter\n",
      "Epoch: 247 | Batch: 002 / 011 | Total loss: 0.858 | Reg loss: 0.026 | Tree loss: 0.858 | Accuracy: 0.657500 | 1.499 sec/iter\n",
      "Epoch: 247 | Batch: 003 / 011 | Total loss: 0.849 | Reg loss: 0.026 | Tree loss: 0.849 | Accuracy: 0.668500 | 1.499 sec/iter\n",
      "Epoch: 247 | Batch: 004 / 011 | Total loss: 0.834 | Reg loss: 0.026 | Tree loss: 0.834 | Accuracy: 0.665000 | 1.499 sec/iter\n",
      "Epoch: 247 | Batch: 005 / 011 | Total loss: 0.825 | Reg loss: 0.026 | Tree loss: 0.825 | Accuracy: 0.646500 | 1.499 sec/iter\n",
      "Epoch: 247 | Batch: 006 / 011 | Total loss: 0.803 | Reg loss: 0.026 | Tree loss: 0.803 | Accuracy: 0.641000 | 1.498 sec/iter\n",
      "Epoch: 247 | Batch: 007 / 011 | Total loss: 0.804 | Reg loss: 0.026 | Tree loss: 0.804 | Accuracy: 0.647000 | 1.498 sec/iter\n",
      "Epoch: 247 | Batch: 008 / 011 | Total loss: 0.792 | Reg loss: 0.026 | Tree loss: 0.792 | Accuracy: 0.646000 | 1.498 sec/iter\n",
      "Epoch: 247 | Batch: 009 / 011 | Total loss: 0.802 | Reg loss: 0.026 | Tree loss: 0.802 | Accuracy: 0.644000 | 1.498 sec/iter\n",
      "Epoch: 247 | Batch: 010 / 011 | Total loss: 0.792 | Reg loss: 0.026 | Tree loss: 0.792 | Accuracy: 0.614334 | 1.498 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 248 | Batch: 000 / 011 | Total loss: 0.917 | Reg loss: 0.026 | Tree loss: 0.917 | Accuracy: 0.604500 | 1.5 sec/iter\n",
      "Epoch: 248 | Batch: 001 / 011 | Total loss: 0.855 | Reg loss: 0.026 | Tree loss: 0.855 | Accuracy: 0.656500 | 1.5 sec/iter\n",
      "Epoch: 248 | Batch: 002 / 011 | Total loss: 0.841 | Reg loss: 0.026 | Tree loss: 0.841 | Accuracy: 0.652000 | 1.499 sec/iter\n",
      "Epoch: 248 | Batch: 003 / 011 | Total loss: 0.841 | Reg loss: 0.026 | Tree loss: 0.841 | Accuracy: 0.644000 | 1.499 sec/iter\n",
      "Epoch: 248 | Batch: 004 / 011 | Total loss: 0.855 | Reg loss: 0.026 | Tree loss: 0.855 | Accuracy: 0.646500 | 1.499 sec/iter\n",
      "Epoch: 248 | Batch: 005 / 011 | Total loss: 0.813 | Reg loss: 0.026 | Tree loss: 0.813 | Accuracy: 0.644000 | 1.499 sec/iter\n",
      "Epoch: 248 | Batch: 006 / 011 | Total loss: 0.814 | Reg loss: 0.026 | Tree loss: 0.814 | Accuracy: 0.627500 | 1.499 sec/iter\n",
      "Epoch: 248 | Batch: 007 / 011 | Total loss: 0.799 | Reg loss: 0.026 | Tree loss: 0.799 | Accuracy: 0.658000 | 1.499 sec/iter\n",
      "Epoch: 248 | Batch: 008 / 011 | Total loss: 0.818 | Reg loss: 0.026 | Tree loss: 0.818 | Accuracy: 0.614500 | 1.499 sec/iter\n",
      "Epoch: 248 | Batch: 009 / 011 | Total loss: 0.814 | Reg loss: 0.026 | Tree loss: 0.814 | Accuracy: 0.614000 | 1.499 sec/iter\n",
      "Epoch: 248 | Batch: 010 / 011 | Total loss: 0.767 | Reg loss: 0.026 | Tree loss: 0.767 | Accuracy: 0.634812 | 1.498 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 249 | Batch: 000 / 011 | Total loss: 0.874 | Reg loss: 0.025 | Tree loss: 0.874 | Accuracy: 0.647000 | 1.5 sec/iter\n",
      "Epoch: 249 | Batch: 001 / 011 | Total loss: 0.875 | Reg loss: 0.026 | Tree loss: 0.875 | Accuracy: 0.650000 | 1.5 sec/iter\n",
      "Epoch: 249 | Batch: 002 / 011 | Total loss: 0.838 | Reg loss: 0.026 | Tree loss: 0.838 | Accuracy: 0.653000 | 1.5 sec/iter\n",
      "Epoch: 249 | Batch: 003 / 011 | Total loss: 0.838 | Reg loss: 0.026 | Tree loss: 0.838 | Accuracy: 0.655000 | 1.499 sec/iter\n",
      "Epoch: 249 | Batch: 004 / 011 | Total loss: 0.863 | Reg loss: 0.026 | Tree loss: 0.863 | Accuracy: 0.629500 | 1.499 sec/iter\n",
      "Epoch: 249 | Batch: 005 / 011 | Total loss: 0.810 | Reg loss: 0.026 | Tree loss: 0.810 | Accuracy: 0.649000 | 1.499 sec/iter\n",
      "Epoch: 249 | Batch: 006 / 011 | Total loss: 0.822 | Reg loss: 0.026 | Tree loss: 0.822 | Accuracy: 0.652500 | 1.499 sec/iter\n",
      "Epoch: 249 | Batch: 007 / 011 | Total loss: 0.827 | Reg loss: 0.026 | Tree loss: 0.827 | Accuracy: 0.608500 | 1.499 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 249 | Batch: 008 / 011 | Total loss: 0.814 | Reg loss: 0.026 | Tree loss: 0.814 | Accuracy: 0.630500 | 1.499 sec/iter\n",
      "Epoch: 249 | Batch: 009 / 011 | Total loss: 0.805 | Reg loss: 0.026 | Tree loss: 0.805 | Accuracy: 0.630500 | 1.499 sec/iter\n",
      "Epoch: 249 | Batch: 010 / 011 | Total loss: 0.772 | Reg loss: 0.026 | Tree loss: 0.772 | Accuracy: 0.624573 | 1.498 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 250 | Batch: 000 / 011 | Total loss: 0.908 | Reg loss: 0.026 | Tree loss: 0.908 | Accuracy: 0.607500 | 1.5 sec/iter\n",
      "Epoch: 250 | Batch: 001 / 011 | Total loss: 0.868 | Reg loss: 0.026 | Tree loss: 0.868 | Accuracy: 0.663500 | 1.5 sec/iter\n",
      "Epoch: 250 | Batch: 002 / 011 | Total loss: 0.860 | Reg loss: 0.026 | Tree loss: 0.860 | Accuracy: 0.650000 | 1.5 sec/iter\n",
      "Epoch: 250 | Batch: 003 / 011 | Total loss: 0.827 | Reg loss: 0.026 | Tree loss: 0.827 | Accuracy: 0.662500 | 1.5 sec/iter\n",
      "Epoch: 250 | Batch: 004 / 011 | Total loss: 0.832 | Reg loss: 0.026 | Tree loss: 0.832 | Accuracy: 0.653000 | 1.5 sec/iter\n",
      "Epoch: 250 | Batch: 005 / 011 | Total loss: 0.810 | Reg loss: 0.026 | Tree loss: 0.810 | Accuracy: 0.651500 | 1.499 sec/iter\n",
      "Epoch: 250 | Batch: 006 / 011 | Total loss: 0.835 | Reg loss: 0.026 | Tree loss: 0.835 | Accuracy: 0.627500 | 1.499 sec/iter\n",
      "Epoch: 250 | Batch: 007 / 011 | Total loss: 0.779 | Reg loss: 0.026 | Tree loss: 0.779 | Accuracy: 0.650500 | 1.499 sec/iter\n",
      "Epoch: 250 | Batch: 008 / 011 | Total loss: 0.805 | Reg loss: 0.026 | Tree loss: 0.805 | Accuracy: 0.631500 | 1.499 sec/iter\n",
      "Epoch: 250 | Batch: 009 / 011 | Total loss: 0.811 | Reg loss: 0.026 | Tree loss: 0.811 | Accuracy: 0.627500 | 1.499 sec/iter\n",
      "Epoch: 250 | Batch: 010 / 011 | Total loss: 0.795 | Reg loss: 0.026 | Tree loss: 0.795 | Accuracy: 0.621160 | 1.499 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 251 | Batch: 000 / 011 | Total loss: 0.877 | Reg loss: 0.026 | Tree loss: 0.877 | Accuracy: 0.638500 | 1.5 sec/iter\n",
      "Epoch: 251 | Batch: 001 / 011 | Total loss: 0.865 | Reg loss: 0.026 | Tree loss: 0.865 | Accuracy: 0.649500 | 1.5 sec/iter\n",
      "Epoch: 251 | Batch: 002 / 011 | Total loss: 0.845 | Reg loss: 0.026 | Tree loss: 0.845 | Accuracy: 0.663000 | 1.5 sec/iter\n",
      "Epoch: 251 | Batch: 003 / 011 | Total loss: 0.865 | Reg loss: 0.026 | Tree loss: 0.865 | Accuracy: 0.637000 | 1.5 sec/iter\n",
      "Epoch: 251 | Batch: 004 / 011 | Total loss: 0.825 | Reg loss: 0.026 | Tree loss: 0.825 | Accuracy: 0.655500 | 1.5 sec/iter\n",
      "Epoch: 251 | Batch: 005 / 011 | Total loss: 0.819 | Reg loss: 0.026 | Tree loss: 0.819 | Accuracy: 0.649500 | 1.5 sec/iter\n",
      "Epoch: 251 | Batch: 006 / 011 | Total loss: 0.822 | Reg loss: 0.026 | Tree loss: 0.822 | Accuracy: 0.637500 | 1.499 sec/iter\n",
      "Epoch: 251 | Batch: 007 / 011 | Total loss: 0.787 | Reg loss: 0.026 | Tree loss: 0.787 | Accuracy: 0.649500 | 1.499 sec/iter\n",
      "Epoch: 251 | Batch: 008 / 011 | Total loss: 0.820 | Reg loss: 0.026 | Tree loss: 0.820 | Accuracy: 0.613500 | 1.499 sec/iter\n",
      "Epoch: 251 | Batch: 009 / 011 | Total loss: 0.806 | Reg loss: 0.026 | Tree loss: 0.806 | Accuracy: 0.631000 | 1.499 sec/iter\n",
      "Epoch: 251 | Batch: 010 / 011 | Total loss: 0.867 | Reg loss: 0.026 | Tree loss: 0.867 | Accuracy: 0.559727 | 1.499 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 252 | Batch: 000 / 011 | Total loss: 0.871 | Reg loss: 0.026 | Tree loss: 0.871 | Accuracy: 0.639000 | 1.501 sec/iter\n",
      "Epoch: 252 | Batch: 001 / 011 | Total loss: 0.874 | Reg loss: 0.026 | Tree loss: 0.874 | Accuracy: 0.628500 | 1.5 sec/iter\n",
      "Epoch: 252 | Batch: 002 / 011 | Total loss: 0.870 | Reg loss: 0.026 | Tree loss: 0.870 | Accuracy: 0.640000 | 1.5 sec/iter\n",
      "Epoch: 252 | Batch: 003 / 011 | Total loss: 0.862 | Reg loss: 0.026 | Tree loss: 0.862 | Accuracy: 0.664500 | 1.5 sec/iter\n",
      "Epoch: 252 | Batch: 004 / 011 | Total loss: 0.821 | Reg loss: 0.026 | Tree loss: 0.821 | Accuracy: 0.658500 | 1.5 sec/iter\n",
      "Epoch: 252 | Batch: 005 / 011 | Total loss: 0.828 | Reg loss: 0.026 | Tree loss: 0.828 | Accuracy: 0.652000 | 1.5 sec/iter\n",
      "Epoch: 252 | Batch: 006 / 011 | Total loss: 0.822 | Reg loss: 0.026 | Tree loss: 0.822 | Accuracy: 0.630000 | 1.5 sec/iter\n",
      "Epoch: 252 | Batch: 007 / 011 | Total loss: 0.814 | Reg loss: 0.026 | Tree loss: 0.814 | Accuracy: 0.643000 | 1.499 sec/iter\n",
      "Epoch: 252 | Batch: 008 / 011 | Total loss: 0.790 | Reg loss: 0.026 | Tree loss: 0.790 | Accuracy: 0.641000 | 1.499 sec/iter\n",
      "Epoch: 252 | Batch: 009 / 011 | Total loss: 0.815 | Reg loss: 0.026 | Tree loss: 0.815 | Accuracy: 0.621500 | 1.499 sec/iter\n",
      "Epoch: 252 | Batch: 010 / 011 | Total loss: 0.809 | Reg loss: 0.026 | Tree loss: 0.809 | Accuracy: 0.645051 | 1.499 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 253 | Batch: 000 / 011 | Total loss: 0.900 | Reg loss: 0.026 | Tree loss: 0.900 | Accuracy: 0.610500 | 1.5 sec/iter\n",
      "Epoch: 253 | Batch: 001 / 011 | Total loss: 0.855 | Reg loss: 0.026 | Tree loss: 0.855 | Accuracy: 0.669000 | 1.5 sec/iter\n",
      "Epoch: 253 | Batch: 002 / 011 | Total loss: 0.864 | Reg loss: 0.026 | Tree loss: 0.864 | Accuracy: 0.643500 | 1.499 sec/iter\n",
      "Epoch: 253 | Batch: 003 / 011 | Total loss: 0.840 | Reg loss: 0.026 | Tree loss: 0.840 | Accuracy: 0.642500 | 1.499 sec/iter\n",
      "Epoch: 253 | Batch: 004 / 011 | Total loss: 0.817 | Reg loss: 0.026 | Tree loss: 0.817 | Accuracy: 0.658000 | 1.499 sec/iter\n",
      "Epoch: 253 | Batch: 005 / 011 | Total loss: 0.811 | Reg loss: 0.026 | Tree loss: 0.811 | Accuracy: 0.648000 | 1.499 sec/iter\n",
      "Epoch: 253 | Batch: 006 / 011 | Total loss: 0.809 | Reg loss: 0.026 | Tree loss: 0.809 | Accuracy: 0.639000 | 1.499 sec/iter\n",
      "Epoch: 253 | Batch: 007 / 011 | Total loss: 0.802 | Reg loss: 0.026 | Tree loss: 0.802 | Accuracy: 0.647000 | 1.499 sec/iter\n",
      "Epoch: 253 | Batch: 008 / 011 | Total loss: 0.798 | Reg loss: 0.026 | Tree loss: 0.798 | Accuracy: 0.635500 | 1.499 sec/iter\n",
      "Epoch: 253 | Batch: 009 / 011 | Total loss: 0.818 | Reg loss: 0.026 | Tree loss: 0.818 | Accuracy: 0.624000 | 1.499 sec/iter\n",
      "Epoch: 253 | Batch: 010 / 011 | Total loss: 0.827 | Reg loss: 0.026 | Tree loss: 0.827 | Accuracy: 0.614334 | 1.498 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 254 | Batch: 000 / 011 | Total loss: 0.887 | Reg loss: 0.026 | Tree loss: 0.887 | Accuracy: 0.622500 | 1.5 sec/iter\n",
      "Epoch: 254 | Batch: 001 / 011 | Total loss: 0.854 | Reg loss: 0.026 | Tree loss: 0.854 | Accuracy: 0.643500 | 1.5 sec/iter\n",
      "Epoch: 254 | Batch: 002 / 011 | Total loss: 0.862 | Reg loss: 0.026 | Tree loss: 0.862 | Accuracy: 0.661500 | 1.5 sec/iter\n",
      "Epoch: 254 | Batch: 003 / 011 | Total loss: 0.845 | Reg loss: 0.026 | Tree loss: 0.845 | Accuracy: 0.654000 | 1.5 sec/iter\n",
      "Epoch: 254 | Batch: 004 / 011 | Total loss: 0.843 | Reg loss: 0.026 | Tree loss: 0.843 | Accuracy: 0.639000 | 1.499 sec/iter\n",
      "Epoch: 254 | Batch: 005 / 011 | Total loss: 0.837 | Reg loss: 0.026 | Tree loss: 0.837 | Accuracy: 0.636000 | 1.499 sec/iter\n",
      "Epoch: 254 | Batch: 006 / 011 | Total loss: 0.814 | Reg loss: 0.026 | Tree loss: 0.814 | Accuracy: 0.641000 | 1.499 sec/iter\n",
      "Epoch: 254 | Batch: 007 / 011 | Total loss: 0.790 | Reg loss: 0.026 | Tree loss: 0.790 | Accuracy: 0.639500 | 1.499 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 254 | Batch: 008 / 011 | Total loss: 0.814 | Reg loss: 0.026 | Tree loss: 0.814 | Accuracy: 0.620000 | 1.499 sec/iter\n",
      "Epoch: 254 | Batch: 009 / 011 | Total loss: 0.785 | Reg loss: 0.026 | Tree loss: 0.785 | Accuracy: 0.642000 | 1.499 sec/iter\n",
      "Epoch: 254 | Batch: 010 / 011 | Total loss: 0.795 | Reg loss: 0.026 | Tree loss: 0.795 | Accuracy: 0.668942 | 1.499 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 255 | Batch: 000 / 011 | Total loss: 0.910 | Reg loss: 0.026 | Tree loss: 0.910 | Accuracy: 0.607500 | 1.5 sec/iter\n",
      "Epoch: 255 | Batch: 001 / 011 | Total loss: 0.859 | Reg loss: 0.026 | Tree loss: 0.859 | Accuracy: 0.655000 | 1.5 sec/iter\n",
      "Epoch: 255 | Batch: 002 / 011 | Total loss: 0.866 | Reg loss: 0.026 | Tree loss: 0.866 | Accuracy: 0.650500 | 1.5 sec/iter\n",
      "Epoch: 255 | Batch: 003 / 011 | Total loss: 0.855 | Reg loss: 0.026 | Tree loss: 0.855 | Accuracy: 0.645500 | 1.499 sec/iter\n",
      "Epoch: 255 | Batch: 004 / 011 | Total loss: 0.839 | Reg loss: 0.026 | Tree loss: 0.839 | Accuracy: 0.641000 | 1.499 sec/iter\n",
      "Epoch: 255 | Batch: 005 / 011 | Total loss: 0.804 | Reg loss: 0.026 | Tree loss: 0.804 | Accuracy: 0.662000 | 1.499 sec/iter\n",
      "Epoch: 255 | Batch: 006 / 011 | Total loss: 0.797 | Reg loss: 0.026 | Tree loss: 0.797 | Accuracy: 0.650000 | 1.499 sec/iter\n",
      "Epoch: 255 | Batch: 007 / 011 | Total loss: 0.816 | Reg loss: 0.026 | Tree loss: 0.816 | Accuracy: 0.636000 | 1.499 sec/iter\n",
      "Epoch: 255 | Batch: 008 / 011 | Total loss: 0.791 | Reg loss: 0.026 | Tree loss: 0.791 | Accuracy: 0.637500 | 1.499 sec/iter\n",
      "Epoch: 255 | Batch: 009 / 011 | Total loss: 0.800 | Reg loss: 0.026 | Tree loss: 0.800 | Accuracy: 0.637000 | 1.499 sec/iter\n",
      "Epoch: 255 | Batch: 010 / 011 | Total loss: 0.823 | Reg loss: 0.026 | Tree loss: 0.823 | Accuracy: 0.651877 | 1.498 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 256 | Batch: 000 / 011 | Total loss: 0.879 | Reg loss: 0.026 | Tree loss: 0.879 | Accuracy: 0.630000 | 1.5 sec/iter\n",
      "Epoch: 256 | Batch: 001 / 011 | Total loss: 0.868 | Reg loss: 0.026 | Tree loss: 0.868 | Accuracy: 0.637000 | 1.5 sec/iter\n",
      "Epoch: 256 | Batch: 002 / 011 | Total loss: 0.850 | Reg loss: 0.026 | Tree loss: 0.850 | Accuracy: 0.644000 | 1.5 sec/iter\n",
      "Epoch: 256 | Batch: 003 / 011 | Total loss: 0.851 | Reg loss: 0.026 | Tree loss: 0.851 | Accuracy: 0.650000 | 1.5 sec/iter\n",
      "Epoch: 256 | Batch: 004 / 011 | Total loss: 0.829 | Reg loss: 0.026 | Tree loss: 0.829 | Accuracy: 0.657000 | 1.5 sec/iter\n",
      "Epoch: 256 | Batch: 005 / 011 | Total loss: 0.823 | Reg loss: 0.026 | Tree loss: 0.823 | Accuracy: 0.657000 | 1.499 sec/iter\n",
      "Epoch: 256 | Batch: 006 / 011 | Total loss: 0.802 | Reg loss: 0.026 | Tree loss: 0.802 | Accuracy: 0.658000 | 1.499 sec/iter\n",
      "Epoch: 256 | Batch: 007 / 011 | Total loss: 0.804 | Reg loss: 0.026 | Tree loss: 0.804 | Accuracy: 0.618500 | 1.499 sec/iter\n",
      "Epoch: 256 | Batch: 008 / 011 | Total loss: 0.802 | Reg loss: 0.026 | Tree loss: 0.802 | Accuracy: 0.632000 | 1.499 sec/iter\n",
      "Epoch: 256 | Batch: 009 / 011 | Total loss: 0.806 | Reg loss: 0.026 | Tree loss: 0.806 | Accuracy: 0.630500 | 1.499 sec/iter\n",
      "Epoch: 256 | Batch: 010 / 011 | Total loss: 0.819 | Reg loss: 0.026 | Tree loss: 0.819 | Accuracy: 0.662116 | 1.499 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 257 | Batch: 000 / 011 | Total loss: 0.888 | Reg loss: 0.026 | Tree loss: 0.888 | Accuracy: 0.628500 | 1.5 sec/iter\n",
      "Epoch: 257 | Batch: 001 / 011 | Total loss: 0.894 | Reg loss: 0.026 | Tree loss: 0.894 | Accuracy: 0.626500 | 1.5 sec/iter\n",
      "Epoch: 257 | Batch: 002 / 011 | Total loss: 0.855 | Reg loss: 0.026 | Tree loss: 0.855 | Accuracy: 0.651500 | 1.5 sec/iter\n",
      "Epoch: 257 | Batch: 003 / 011 | Total loss: 0.838 | Reg loss: 0.026 | Tree loss: 0.838 | Accuracy: 0.646000 | 1.5 sec/iter\n",
      "Epoch: 257 | Batch: 004 / 011 | Total loss: 0.836 | Reg loss: 0.026 | Tree loss: 0.836 | Accuracy: 0.644500 | 1.5 sec/iter\n",
      "Epoch: 257 | Batch: 005 / 011 | Total loss: 0.811 | Reg loss: 0.026 | Tree loss: 0.811 | Accuracy: 0.652500 | 1.5 sec/iter\n",
      "Epoch: 257 | Batch: 006 / 011 | Total loss: 0.811 | Reg loss: 0.026 | Tree loss: 0.811 | Accuracy: 0.646000 | 1.5 sec/iter\n",
      "Epoch: 257 | Batch: 007 / 011 | Total loss: 0.784 | Reg loss: 0.026 | Tree loss: 0.784 | Accuracy: 0.646500 | 1.499 sec/iter\n",
      "Epoch: 257 | Batch: 008 / 011 | Total loss: 0.811 | Reg loss: 0.026 | Tree loss: 0.811 | Accuracy: 0.622000 | 1.499 sec/iter\n",
      "Epoch: 257 | Batch: 009 / 011 | Total loss: 0.810 | Reg loss: 0.026 | Tree loss: 0.810 | Accuracy: 0.622500 | 1.499 sec/iter\n",
      "Epoch: 257 | Batch: 010 / 011 | Total loss: 0.772 | Reg loss: 0.026 | Tree loss: 0.772 | Accuracy: 0.651877 | 1.499 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 258 | Batch: 000 / 011 | Total loss: 0.867 | Reg loss: 0.026 | Tree loss: 0.867 | Accuracy: 0.639500 | 1.501 sec/iter\n",
      "Epoch: 258 | Batch: 001 / 011 | Total loss: 0.871 | Reg loss: 0.026 | Tree loss: 0.871 | Accuracy: 0.630500 | 1.501 sec/iter\n",
      "Epoch: 258 | Batch: 002 / 011 | Total loss: 0.848 | Reg loss: 0.026 | Tree loss: 0.848 | Accuracy: 0.658500 | 1.5 sec/iter\n",
      "Epoch: 258 | Batch: 003 / 011 | Total loss: 0.841 | Reg loss: 0.026 | Tree loss: 0.841 | Accuracy: 0.667500 | 1.5 sec/iter\n",
      "Epoch: 258 | Batch: 004 / 011 | Total loss: 0.842 | Reg loss: 0.026 | Tree loss: 0.842 | Accuracy: 0.644000 | 1.5 sec/iter\n",
      "Epoch: 258 | Batch: 005 / 011 | Total loss: 0.807 | Reg loss: 0.026 | Tree loss: 0.807 | Accuracy: 0.664500 | 1.5 sec/iter\n",
      "Epoch: 258 | Batch: 006 / 011 | Total loss: 0.808 | Reg loss: 0.026 | Tree loss: 0.808 | Accuracy: 0.660500 | 1.5 sec/iter\n",
      "Epoch: 258 | Batch: 007 / 011 | Total loss: 0.801 | Reg loss: 0.026 | Tree loss: 0.801 | Accuracy: 0.650500 | 1.499 sec/iter\n",
      "Epoch: 258 | Batch: 008 / 011 | Total loss: 0.807 | Reg loss: 0.026 | Tree loss: 0.807 | Accuracy: 0.627000 | 1.499 sec/iter\n",
      "Epoch: 258 | Batch: 009 / 011 | Total loss: 0.819 | Reg loss: 0.026 | Tree loss: 0.819 | Accuracy: 0.623000 | 1.499 sec/iter\n",
      "Epoch: 258 | Batch: 010 / 011 | Total loss: 0.797 | Reg loss: 0.026 | Tree loss: 0.797 | Accuracy: 0.641638 | 1.499 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 259 | Batch: 000 / 011 | Total loss: 0.887 | Reg loss: 0.026 | Tree loss: 0.887 | Accuracy: 0.621500 | 1.499 sec/iter\n",
      "Epoch: 259 | Batch: 001 / 011 | Total loss: 0.845 | Reg loss: 0.026 | Tree loss: 0.845 | Accuracy: 0.670000 | 1.499 sec/iter\n",
      "Epoch: 259 | Batch: 002 / 011 | Total loss: 0.858 | Reg loss: 0.026 | Tree loss: 0.858 | Accuracy: 0.643500 | 1.499 sec/iter\n",
      "Epoch: 259 | Batch: 003 / 011 | Total loss: 0.831 | Reg loss: 0.026 | Tree loss: 0.831 | Accuracy: 0.651000 | 1.499 sec/iter\n",
      "Epoch: 259 | Batch: 004 / 011 | Total loss: 0.814 | Reg loss: 0.026 | Tree loss: 0.814 | Accuracy: 0.661500 | 1.499 sec/iter\n",
      "Epoch: 259 | Batch: 005 / 011 | Total loss: 0.836 | Reg loss: 0.026 | Tree loss: 0.836 | Accuracy: 0.626000 | 1.499 sec/iter\n",
      "Epoch: 259 | Batch: 006 / 011 | Total loss: 0.818 | Reg loss: 0.026 | Tree loss: 0.818 | Accuracy: 0.641500 | 1.498 sec/iter\n",
      "Epoch: 259 | Batch: 007 / 011 | Total loss: 0.812 | Reg loss: 0.026 | Tree loss: 0.812 | Accuracy: 0.632000 | 1.498 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 259 | Batch: 008 / 011 | Total loss: 0.807 | Reg loss: 0.026 | Tree loss: 0.807 | Accuracy: 0.630500 | 1.498 sec/iter\n",
      "Epoch: 259 | Batch: 009 / 011 | Total loss: 0.782 | Reg loss: 0.026 | Tree loss: 0.782 | Accuracy: 0.645500 | 1.498 sec/iter\n",
      "Epoch: 259 | Batch: 010 / 011 | Total loss: 0.819 | Reg loss: 0.026 | Tree loss: 0.819 | Accuracy: 0.621160 | 1.498 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 260 | Batch: 000 / 011 | Total loss: 0.885 | Reg loss: 0.026 | Tree loss: 0.885 | Accuracy: 0.627000 | 1.5 sec/iter\n",
      "Epoch: 260 | Batch: 001 / 011 | Total loss: 0.879 | Reg loss: 0.026 | Tree loss: 0.879 | Accuracy: 0.628500 | 1.5 sec/iter\n",
      "Epoch: 260 | Batch: 002 / 011 | Total loss: 0.857 | Reg loss: 0.026 | Tree loss: 0.857 | Accuracy: 0.649500 | 1.5 sec/iter\n",
      "Epoch: 260 | Batch: 003 / 011 | Total loss: 0.849 | Reg loss: 0.026 | Tree loss: 0.849 | Accuracy: 0.657000 | 1.5 sec/iter\n",
      "Epoch: 260 | Batch: 004 / 011 | Total loss: 0.817 | Reg loss: 0.026 | Tree loss: 0.817 | Accuracy: 0.658000 | 1.499 sec/iter\n",
      "Epoch: 260 | Batch: 005 / 011 | Total loss: 0.812 | Reg loss: 0.026 | Tree loss: 0.812 | Accuracy: 0.661500 | 1.499 sec/iter\n",
      "Epoch: 260 | Batch: 006 / 011 | Total loss: 0.796 | Reg loss: 0.026 | Tree loss: 0.796 | Accuracy: 0.640500 | 1.499 sec/iter\n",
      "Epoch: 260 | Batch: 007 / 011 | Total loss: 0.810 | Reg loss: 0.026 | Tree loss: 0.810 | Accuracy: 0.615500 | 1.499 sec/iter\n",
      "Epoch: 260 | Batch: 008 / 011 | Total loss: 0.799 | Reg loss: 0.026 | Tree loss: 0.799 | Accuracy: 0.637000 | 1.499 sec/iter\n",
      "Epoch: 260 | Batch: 009 / 011 | Total loss: 0.806 | Reg loss: 0.026 | Tree loss: 0.806 | Accuracy: 0.620500 | 1.499 sec/iter\n",
      "Epoch: 260 | Batch: 010 / 011 | Total loss: 0.787 | Reg loss: 0.026 | Tree loss: 0.787 | Accuracy: 0.648464 | 1.499 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 261 | Batch: 000 / 011 | Total loss: 0.873 | Reg loss: 0.026 | Tree loss: 0.873 | Accuracy: 0.641000 | 1.5 sec/iter\n",
      "Epoch: 261 | Batch: 001 / 011 | Total loss: 0.877 | Reg loss: 0.026 | Tree loss: 0.877 | Accuracy: 0.630000 | 1.5 sec/iter\n",
      "Epoch: 261 | Batch: 002 / 011 | Total loss: 0.868 | Reg loss: 0.026 | Tree loss: 0.868 | Accuracy: 0.660000 | 1.5 sec/iter\n",
      "Epoch: 261 | Batch: 003 / 011 | Total loss: 0.844 | Reg loss: 0.026 | Tree loss: 0.844 | Accuracy: 0.641000 | 1.499 sec/iter\n",
      "Epoch: 261 | Batch: 004 / 011 | Total loss: 0.829 | Reg loss: 0.026 | Tree loss: 0.829 | Accuracy: 0.662500 | 1.499 sec/iter\n",
      "Epoch: 261 | Batch: 005 / 011 | Total loss: 0.825 | Reg loss: 0.026 | Tree loss: 0.825 | Accuracy: 0.644000 | 1.499 sec/iter\n",
      "Epoch: 261 | Batch: 006 / 011 | Total loss: 0.815 | Reg loss: 0.026 | Tree loss: 0.815 | Accuracy: 0.654500 | 1.499 sec/iter\n",
      "Epoch: 261 | Batch: 007 / 011 | Total loss: 0.795 | Reg loss: 0.026 | Tree loss: 0.795 | Accuracy: 0.637500 | 1.499 sec/iter\n",
      "Epoch: 261 | Batch: 008 / 011 | Total loss: 0.793 | Reg loss: 0.026 | Tree loss: 0.793 | Accuracy: 0.640000 | 1.499 sec/iter\n",
      "Epoch: 261 | Batch: 009 / 011 | Total loss: 0.816 | Reg loss: 0.026 | Tree loss: 0.816 | Accuracy: 0.619500 | 1.499 sec/iter\n",
      "Epoch: 261 | Batch: 010 / 011 | Total loss: 0.797 | Reg loss: 0.026 | Tree loss: 0.797 | Accuracy: 0.627986 | 1.498 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 262 | Batch: 000 / 011 | Total loss: 0.856 | Reg loss: 0.026 | Tree loss: 0.856 | Accuracy: 0.654000 | 1.5 sec/iter\n",
      "Epoch: 262 | Batch: 001 / 011 | Total loss: 0.871 | Reg loss: 0.026 | Tree loss: 0.871 | Accuracy: 0.646000 | 1.5 sec/iter\n",
      "Epoch: 262 | Batch: 002 / 011 | Total loss: 0.850 | Reg loss: 0.026 | Tree loss: 0.850 | Accuracy: 0.654500 | 1.5 sec/iter\n",
      "Epoch: 262 | Batch: 003 / 011 | Total loss: 0.850 | Reg loss: 0.026 | Tree loss: 0.850 | Accuracy: 0.641000 | 1.5 sec/iter\n",
      "Epoch: 262 | Batch: 004 / 011 | Total loss: 0.818 | Reg loss: 0.026 | Tree loss: 0.818 | Accuracy: 0.654500 | 1.499 sec/iter\n",
      "Epoch: 262 | Batch: 005 / 011 | Total loss: 0.824 | Reg loss: 0.026 | Tree loss: 0.824 | Accuracy: 0.647000 | 1.499 sec/iter\n",
      "Epoch: 262 | Batch: 006 / 011 | Total loss: 0.856 | Reg loss: 0.026 | Tree loss: 0.856 | Accuracy: 0.613500 | 1.499 sec/iter\n",
      "Epoch: 262 | Batch: 007 / 011 | Total loss: 0.799 | Reg loss: 0.026 | Tree loss: 0.799 | Accuracy: 0.637000 | 1.499 sec/iter\n",
      "Epoch: 262 | Batch: 008 / 011 | Total loss: 0.817 | Reg loss: 0.026 | Tree loss: 0.817 | Accuracy: 0.627500 | 1.499 sec/iter\n",
      "Epoch: 262 | Batch: 009 / 011 | Total loss: 0.789 | Reg loss: 0.026 | Tree loss: 0.789 | Accuracy: 0.658500 | 1.499 sec/iter\n",
      "Epoch: 262 | Batch: 010 / 011 | Total loss: 0.787 | Reg loss: 0.026 | Tree loss: 0.787 | Accuracy: 0.638225 | 1.499 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 263 | Batch: 000 / 011 | Total loss: 0.885 | Reg loss: 0.026 | Tree loss: 0.885 | Accuracy: 0.624000 | 1.5 sec/iter\n",
      "Epoch: 263 | Batch: 001 / 011 | Total loss: 0.879 | Reg loss: 0.026 | Tree loss: 0.879 | Accuracy: 0.650500 | 1.5 sec/iter\n",
      "Epoch: 263 | Batch: 002 / 011 | Total loss: 0.866 | Reg loss: 0.026 | Tree loss: 0.866 | Accuracy: 0.666000 | 1.5 sec/iter\n",
      "Epoch: 263 | Batch: 003 / 011 | Total loss: 0.843 | Reg loss: 0.026 | Tree loss: 0.843 | Accuracy: 0.640000 | 1.5 sec/iter\n",
      "Epoch: 263 | Batch: 004 / 011 | Total loss: 0.831 | Reg loss: 0.026 | Tree loss: 0.831 | Accuracy: 0.646000 | 1.5 sec/iter\n",
      "Epoch: 263 | Batch: 005 / 011 | Total loss: 0.850 | Reg loss: 0.026 | Tree loss: 0.850 | Accuracy: 0.630000 | 1.5 sec/iter\n",
      "Epoch: 263 | Batch: 006 / 011 | Total loss: 0.792 | Reg loss: 0.026 | Tree loss: 0.792 | Accuracy: 0.653500 | 1.499 sec/iter\n",
      "Epoch: 263 | Batch: 007 / 011 | Total loss: 0.806 | Reg loss: 0.026 | Tree loss: 0.806 | Accuracy: 0.648500 | 1.499 sec/iter\n",
      "Epoch: 263 | Batch: 008 / 011 | Total loss: 0.773 | Reg loss: 0.026 | Tree loss: 0.773 | Accuracy: 0.659000 | 1.499 sec/iter\n",
      "Epoch: 263 | Batch: 009 / 011 | Total loss: 0.803 | Reg loss: 0.026 | Tree loss: 0.803 | Accuracy: 0.620000 | 1.499 sec/iter\n",
      "Epoch: 263 | Batch: 010 / 011 | Total loss: 0.853 | Reg loss: 0.026 | Tree loss: 0.853 | Accuracy: 0.600683 | 1.499 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 264 | Batch: 000 / 011 | Total loss: 0.872 | Reg loss: 0.026 | Tree loss: 0.872 | Accuracy: 0.635000 | 1.5 sec/iter\n",
      "Epoch: 264 | Batch: 001 / 011 | Total loss: 0.865 | Reg loss: 0.026 | Tree loss: 0.865 | Accuracy: 0.651000 | 1.5 sec/iter\n",
      "Epoch: 264 | Batch: 002 / 011 | Total loss: 0.839 | Reg loss: 0.026 | Tree loss: 0.839 | Accuracy: 0.678000 | 1.5 sec/iter\n",
      "Epoch: 264 | Batch: 003 / 011 | Total loss: 0.838 | Reg loss: 0.026 | Tree loss: 0.838 | Accuracy: 0.650000 | 1.5 sec/iter\n",
      "Epoch: 264 | Batch: 004 / 011 | Total loss: 0.836 | Reg loss: 0.026 | Tree loss: 0.836 | Accuracy: 0.666000 | 1.5 sec/iter\n",
      "Epoch: 264 | Batch: 005 / 011 | Total loss: 0.830 | Reg loss: 0.026 | Tree loss: 0.830 | Accuracy: 0.645000 | 1.499 sec/iter\n",
      "Epoch: 264 | Batch: 006 / 011 | Total loss: 0.797 | Reg loss: 0.026 | Tree loss: 0.797 | Accuracy: 0.646000 | 1.499 sec/iter\n",
      "Epoch: 264 | Batch: 007 / 011 | Total loss: 0.801 | Reg loss: 0.026 | Tree loss: 0.801 | Accuracy: 0.648000 | 1.499 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 264 | Batch: 008 / 011 | Total loss: 0.835 | Reg loss: 0.026 | Tree loss: 0.835 | Accuracy: 0.609500 | 1.499 sec/iter\n",
      "Epoch: 264 | Batch: 009 / 011 | Total loss: 0.788 | Reg loss: 0.026 | Tree loss: 0.788 | Accuracy: 0.639500 | 1.499 sec/iter\n",
      "Epoch: 264 | Batch: 010 / 011 | Total loss: 0.827 | Reg loss: 0.026 | Tree loss: 0.827 | Accuracy: 0.641638 | 1.499 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 265 | Batch: 000 / 011 | Total loss: 0.873 | Reg loss: 0.026 | Tree loss: 0.873 | Accuracy: 0.654500 | 1.499 sec/iter\n",
      "Epoch: 265 | Batch: 001 / 011 | Total loss: 0.874 | Reg loss: 0.026 | Tree loss: 0.874 | Accuracy: 0.637000 | 1.499 sec/iter\n",
      "Epoch: 265 | Batch: 002 / 011 | Total loss: 0.874 | Reg loss: 0.026 | Tree loss: 0.874 | Accuracy: 0.636500 | 1.499 sec/iter\n",
      "Epoch: 265 | Batch: 003 / 011 | Total loss: 0.838 | Reg loss: 0.026 | Tree loss: 0.838 | Accuracy: 0.656000 | 1.498 sec/iter\n",
      "Epoch: 265 | Batch: 004 / 011 | Total loss: 0.809 | Reg loss: 0.026 | Tree loss: 0.809 | Accuracy: 0.670500 | 1.498 sec/iter\n",
      "Epoch: 265 | Batch: 005 / 011 | Total loss: 0.813 | Reg loss: 0.026 | Tree loss: 0.813 | Accuracy: 0.658000 | 1.498 sec/iter\n",
      "Epoch: 265 | Batch: 006 / 011 | Total loss: 0.805 | Reg loss: 0.026 | Tree loss: 0.805 | Accuracy: 0.662000 | 1.498 sec/iter\n",
      "Epoch: 265 | Batch: 007 / 011 | Total loss: 0.819 | Reg loss: 0.026 | Tree loss: 0.819 | Accuracy: 0.612500 | 1.498 sec/iter\n",
      "Epoch: 265 | Batch: 008 / 011 | Total loss: 0.816 | Reg loss: 0.026 | Tree loss: 0.816 | Accuracy: 0.621500 | 1.498 sec/iter\n",
      "Epoch: 265 | Batch: 009 / 011 | Total loss: 0.802 | Reg loss: 0.026 | Tree loss: 0.802 | Accuracy: 0.631000 | 1.498 sec/iter\n",
      "Epoch: 265 | Batch: 010 / 011 | Total loss: 0.816 | Reg loss: 0.026 | Tree loss: 0.816 | Accuracy: 0.624573 | 1.497 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 266 | Batch: 000 / 011 | Total loss: 0.888 | Reg loss: 0.026 | Tree loss: 0.888 | Accuracy: 0.635500 | 1.499 sec/iter\n",
      "Epoch: 266 | Batch: 001 / 011 | Total loss: 0.875 | Reg loss: 0.026 | Tree loss: 0.875 | Accuracy: 0.644500 | 1.499 sec/iter\n",
      "Epoch: 266 | Batch: 002 / 011 | Total loss: 0.857 | Reg loss: 0.026 | Tree loss: 0.857 | Accuracy: 0.652500 | 1.499 sec/iter\n",
      "Epoch: 266 | Batch: 003 / 011 | Total loss: 0.836 | Reg loss: 0.026 | Tree loss: 0.836 | Accuracy: 0.657500 | 1.499 sec/iter\n",
      "Epoch: 266 | Batch: 004 / 011 | Total loss: 0.824 | Reg loss: 0.026 | Tree loss: 0.824 | Accuracy: 0.640000 | 1.499 sec/iter\n",
      "Epoch: 266 | Batch: 005 / 011 | Total loss: 0.824 | Reg loss: 0.026 | Tree loss: 0.824 | Accuracy: 0.629500 | 1.499 sec/iter\n",
      "Epoch: 266 | Batch: 006 / 011 | Total loss: 0.803 | Reg loss: 0.026 | Tree loss: 0.803 | Accuracy: 0.656000 | 1.499 sec/iter\n",
      "Epoch: 266 | Batch: 007 / 011 | Total loss: 0.793 | Reg loss: 0.026 | Tree loss: 0.793 | Accuracy: 0.639500 | 1.498 sec/iter\n",
      "Epoch: 266 | Batch: 008 / 011 | Total loss: 0.823 | Reg loss: 0.026 | Tree loss: 0.823 | Accuracy: 0.612500 | 1.498 sec/iter\n",
      "Epoch: 266 | Batch: 009 / 011 | Total loss: 0.815 | Reg loss: 0.026 | Tree loss: 0.815 | Accuracy: 0.619500 | 1.498 sec/iter\n",
      "Epoch: 266 | Batch: 010 / 011 | Total loss: 0.782 | Reg loss: 0.026 | Tree loss: 0.782 | Accuracy: 0.655290 | 1.498 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 267 | Batch: 000 / 011 | Total loss: 0.885 | Reg loss: 0.026 | Tree loss: 0.885 | Accuracy: 0.634500 | 1.5 sec/iter\n",
      "Epoch: 267 | Batch: 001 / 011 | Total loss: 0.880 | Reg loss: 0.026 | Tree loss: 0.880 | Accuracy: 0.632000 | 1.5 sec/iter\n",
      "Epoch: 267 | Batch: 002 / 011 | Total loss: 0.856 | Reg loss: 0.026 | Tree loss: 0.856 | Accuracy: 0.655000 | 1.499 sec/iter\n",
      "Epoch: 267 | Batch: 003 / 011 | Total loss: 0.832 | Reg loss: 0.026 | Tree loss: 0.832 | Accuracy: 0.660000 | 1.499 sec/iter\n",
      "Epoch: 267 | Batch: 004 / 011 | Total loss: 0.828 | Reg loss: 0.026 | Tree loss: 0.828 | Accuracy: 0.639500 | 1.499 sec/iter\n",
      "Epoch: 267 | Batch: 005 / 011 | Total loss: 0.809 | Reg loss: 0.026 | Tree loss: 0.809 | Accuracy: 0.660000 | 1.499 sec/iter\n",
      "Epoch: 267 | Batch: 006 / 011 | Total loss: 0.818 | Reg loss: 0.026 | Tree loss: 0.818 | Accuracy: 0.635500 | 1.499 sec/iter\n",
      "Epoch: 267 | Batch: 007 / 011 | Total loss: 0.821 | Reg loss: 0.026 | Tree loss: 0.821 | Accuracy: 0.626000 | 1.499 sec/iter\n",
      "Epoch: 267 | Batch: 008 / 011 | Total loss: 0.793 | Reg loss: 0.026 | Tree loss: 0.793 | Accuracy: 0.632500 | 1.499 sec/iter\n",
      "Epoch: 267 | Batch: 009 / 011 | Total loss: 0.813 | Reg loss: 0.026 | Tree loss: 0.813 | Accuracy: 0.626000 | 1.499 sec/iter\n",
      "Epoch: 267 | Batch: 010 / 011 | Total loss: 0.770 | Reg loss: 0.026 | Tree loss: 0.770 | Accuracy: 0.655290 | 1.499 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 268 | Batch: 000 / 011 | Total loss: 0.884 | Reg loss: 0.026 | Tree loss: 0.884 | Accuracy: 0.637500 | 1.5 sec/iter\n",
      "Epoch: 268 | Batch: 001 / 011 | Total loss: 0.872 | Reg loss: 0.026 | Tree loss: 0.872 | Accuracy: 0.635000 | 1.5 sec/iter\n",
      "Epoch: 268 | Batch: 002 / 011 | Total loss: 0.837 | Reg loss: 0.026 | Tree loss: 0.837 | Accuracy: 0.649000 | 1.5 sec/iter\n",
      "Epoch: 268 | Batch: 003 / 011 | Total loss: 0.860 | Reg loss: 0.026 | Tree loss: 0.860 | Accuracy: 0.648500 | 1.5 sec/iter\n",
      "Epoch: 268 | Batch: 004 / 011 | Total loss: 0.840 | Reg loss: 0.026 | Tree loss: 0.840 | Accuracy: 0.655500 | 1.5 sec/iter\n",
      "Epoch: 268 | Batch: 005 / 011 | Total loss: 0.812 | Reg loss: 0.026 | Tree loss: 0.812 | Accuracy: 0.646000 | 1.499 sec/iter\n",
      "Epoch: 268 | Batch: 006 / 011 | Total loss: 0.825 | Reg loss: 0.026 | Tree loss: 0.825 | Accuracy: 0.626500 | 1.499 sec/iter\n",
      "Epoch: 268 | Batch: 007 / 011 | Total loss: 0.806 | Reg loss: 0.026 | Tree loss: 0.806 | Accuracy: 0.636000 | 1.499 sec/iter\n",
      "Epoch: 268 | Batch: 008 / 011 | Total loss: 0.786 | Reg loss: 0.026 | Tree loss: 0.786 | Accuracy: 0.640500 | 1.499 sec/iter\n",
      "Epoch: 268 | Batch: 009 / 011 | Total loss: 0.802 | Reg loss: 0.026 | Tree loss: 0.802 | Accuracy: 0.627500 | 1.499 sec/iter\n",
      "Epoch: 268 | Batch: 010 / 011 | Total loss: 0.784 | Reg loss: 0.026 | Tree loss: 0.784 | Accuracy: 0.627986 | 1.499 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 269 | Batch: 000 / 011 | Total loss: 0.869 | Reg loss: 0.026 | Tree loss: 0.869 | Accuracy: 0.636000 | 1.5 sec/iter\n",
      "Epoch: 269 | Batch: 001 / 011 | Total loss: 0.875 | Reg loss: 0.026 | Tree loss: 0.875 | Accuracy: 0.649000 | 1.5 sec/iter\n",
      "Epoch: 269 | Batch: 002 / 011 | Total loss: 0.867 | Reg loss: 0.026 | Tree loss: 0.867 | Accuracy: 0.641500 | 1.5 sec/iter\n",
      "Epoch: 269 | Batch: 003 / 011 | Total loss: 0.848 | Reg loss: 0.026 | Tree loss: 0.848 | Accuracy: 0.660500 | 1.5 sec/iter\n",
      "Epoch: 269 | Batch: 004 / 011 | Total loss: 0.825 | Reg loss: 0.026 | Tree loss: 0.825 | Accuracy: 0.661000 | 1.5 sec/iter\n",
      "Epoch: 269 | Batch: 005 / 011 | Total loss: 0.859 | Reg loss: 0.026 | Tree loss: 0.859 | Accuracy: 0.632500 | 1.5 sec/iter\n",
      "Epoch: 269 | Batch: 006 / 011 | Total loss: 0.802 | Reg loss: 0.026 | Tree loss: 0.802 | Accuracy: 0.649500 | 1.499 sec/iter\n",
      "Epoch: 269 | Batch: 007 / 011 | Total loss: 0.798 | Reg loss: 0.026 | Tree loss: 0.798 | Accuracy: 0.642500 | 1.499 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 269 | Batch: 008 / 011 | Total loss: 0.817 | Reg loss: 0.026 | Tree loss: 0.817 | Accuracy: 0.616500 | 1.499 sec/iter\n",
      "Epoch: 269 | Batch: 009 / 011 | Total loss: 0.808 | Reg loss: 0.026 | Tree loss: 0.808 | Accuracy: 0.624500 | 1.499 sec/iter\n",
      "Epoch: 269 | Batch: 010 / 011 | Total loss: 0.765 | Reg loss: 0.026 | Tree loss: 0.765 | Accuracy: 0.641638 | 1.499 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 270 | Batch: 000 / 011 | Total loss: 0.868 | Reg loss: 0.026 | Tree loss: 0.868 | Accuracy: 0.641000 | 1.5 sec/iter\n",
      "Epoch: 270 | Batch: 001 / 011 | Total loss: 0.876 | Reg loss: 0.026 | Tree loss: 0.876 | Accuracy: 0.646500 | 1.5 sec/iter\n",
      "Epoch: 270 | Batch: 002 / 011 | Total loss: 0.870 | Reg loss: 0.026 | Tree loss: 0.870 | Accuracy: 0.655000 | 1.5 sec/iter\n",
      "Epoch: 270 | Batch: 003 / 011 | Total loss: 0.861 | Reg loss: 0.026 | Tree loss: 0.861 | Accuracy: 0.642000 | 1.5 sec/iter\n",
      "Epoch: 270 | Batch: 004 / 011 | Total loss: 0.832 | Reg loss: 0.026 | Tree loss: 0.832 | Accuracy: 0.637500 | 1.5 sec/iter\n",
      "Epoch: 270 | Batch: 005 / 011 | Total loss: 0.818 | Reg loss: 0.026 | Tree loss: 0.818 | Accuracy: 0.634000 | 1.5 sec/iter\n",
      "Epoch: 270 | Batch: 006 / 011 | Total loss: 0.796 | Reg loss: 0.026 | Tree loss: 0.796 | Accuracy: 0.636500 | 1.499 sec/iter\n",
      "Epoch: 270 | Batch: 007 / 011 | Total loss: 0.797 | Reg loss: 0.026 | Tree loss: 0.797 | Accuracy: 0.644500 | 1.499 sec/iter\n",
      "Epoch: 270 | Batch: 008 / 011 | Total loss: 0.802 | Reg loss: 0.026 | Tree loss: 0.802 | Accuracy: 0.617500 | 1.499 sec/iter\n",
      "Epoch: 270 | Batch: 009 / 011 | Total loss: 0.791 | Reg loss: 0.026 | Tree loss: 0.791 | Accuracy: 0.631500 | 1.499 sec/iter\n",
      "Epoch: 270 | Batch: 010 / 011 | Total loss: 0.858 | Reg loss: 0.026 | Tree loss: 0.858 | Accuracy: 0.597270 | 1.499 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 271 | Batch: 000 / 011 | Total loss: 0.875 | Reg loss: 0.026 | Tree loss: 0.875 | Accuracy: 0.641000 | 1.499 sec/iter\n",
      "Epoch: 271 | Batch: 001 / 011 | Total loss: 0.876 | Reg loss: 0.026 | Tree loss: 0.876 | Accuracy: 0.633000 | 1.499 sec/iter\n",
      "Epoch: 271 | Batch: 002 / 011 | Total loss: 0.848 | Reg loss: 0.026 | Tree loss: 0.848 | Accuracy: 0.661000 | 1.499 sec/iter\n",
      "Epoch: 271 | Batch: 003 / 011 | Total loss: 0.859 | Reg loss: 0.026 | Tree loss: 0.859 | Accuracy: 0.635000 | 1.499 sec/iter\n",
      "Epoch: 271 | Batch: 004 / 011 | Total loss: 0.842 | Reg loss: 0.026 | Tree loss: 0.842 | Accuracy: 0.660000 | 1.499 sec/iter\n",
      "Epoch: 271 | Batch: 005 / 011 | Total loss: 0.822 | Reg loss: 0.026 | Tree loss: 0.822 | Accuracy: 0.639000 | 1.499 sec/iter\n",
      "Epoch: 271 | Batch: 006 / 011 | Total loss: 0.800 | Reg loss: 0.026 | Tree loss: 0.800 | Accuracy: 0.650000 | 1.498 sec/iter\n",
      "Epoch: 271 | Batch: 007 / 011 | Total loss: 0.819 | Reg loss: 0.026 | Tree loss: 0.819 | Accuracy: 0.612000 | 1.498 sec/iter\n",
      "Epoch: 271 | Batch: 008 / 011 | Total loss: 0.821 | Reg loss: 0.026 | Tree loss: 0.821 | Accuracy: 0.620000 | 1.498 sec/iter\n",
      "Epoch: 271 | Batch: 009 / 011 | Total loss: 0.790 | Reg loss: 0.026 | Tree loss: 0.790 | Accuracy: 0.644000 | 1.498 sec/iter\n",
      "Epoch: 271 | Batch: 010 / 011 | Total loss: 0.729 | Reg loss: 0.026 | Tree loss: 0.729 | Accuracy: 0.696246 | 1.498 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 272 | Batch: 000 / 011 | Total loss: 0.897 | Reg loss: 0.026 | Tree loss: 0.897 | Accuracy: 0.628500 | 1.5 sec/iter\n",
      "Epoch: 272 | Batch: 001 / 011 | Total loss: 0.865 | Reg loss: 0.026 | Tree loss: 0.865 | Accuracy: 0.648000 | 1.499 sec/iter\n",
      "Epoch: 272 | Batch: 002 / 011 | Total loss: 0.859 | Reg loss: 0.026 | Tree loss: 0.859 | Accuracy: 0.652500 | 1.499 sec/iter\n",
      "Epoch: 272 | Batch: 003 / 011 | Total loss: 0.870 | Reg loss: 0.026 | Tree loss: 0.870 | Accuracy: 0.627000 | 1.499 sec/iter\n",
      "Epoch: 272 | Batch: 004 / 011 | Total loss: 0.825 | Reg loss: 0.026 | Tree loss: 0.825 | Accuracy: 0.654500 | 1.499 sec/iter\n",
      "Epoch: 272 | Batch: 005 / 011 | Total loss: 0.794 | Reg loss: 0.026 | Tree loss: 0.794 | Accuracy: 0.657000 | 1.499 sec/iter\n",
      "Epoch: 272 | Batch: 006 / 011 | Total loss: 0.802 | Reg loss: 0.026 | Tree loss: 0.802 | Accuracy: 0.663000 | 1.499 sec/iter\n",
      "Epoch: 272 | Batch: 007 / 011 | Total loss: 0.802 | Reg loss: 0.026 | Tree loss: 0.802 | Accuracy: 0.638000 | 1.499 sec/iter\n",
      "Epoch: 272 | Batch: 008 / 011 | Total loss: 0.800 | Reg loss: 0.026 | Tree loss: 0.800 | Accuracy: 0.619000 | 1.499 sec/iter\n",
      "Epoch: 272 | Batch: 009 / 011 | Total loss: 0.805 | Reg loss: 0.026 | Tree loss: 0.805 | Accuracy: 0.631000 | 1.499 sec/iter\n",
      "Epoch: 272 | Batch: 010 / 011 | Total loss: 0.819 | Reg loss: 0.026 | Tree loss: 0.819 | Accuracy: 0.634812 | 1.498 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 273 | Batch: 000 / 011 | Total loss: 0.876 | Reg loss: 0.026 | Tree loss: 0.876 | Accuracy: 0.646000 | 1.5 sec/iter\n",
      "Epoch: 273 | Batch: 001 / 011 | Total loss: 0.854 | Reg loss: 0.026 | Tree loss: 0.854 | Accuracy: 0.644000 | 1.5 sec/iter\n",
      "Epoch: 273 | Batch: 002 / 011 | Total loss: 0.854 | Reg loss: 0.026 | Tree loss: 0.854 | Accuracy: 0.647500 | 1.5 sec/iter\n",
      "Epoch: 273 | Batch: 003 / 011 | Total loss: 0.842 | Reg loss: 0.026 | Tree loss: 0.842 | Accuracy: 0.657500 | 1.499 sec/iter\n",
      "Epoch: 273 | Batch: 004 / 011 | Total loss: 0.819 | Reg loss: 0.026 | Tree loss: 0.819 | Accuracy: 0.654000 | 1.499 sec/iter\n",
      "Epoch: 273 | Batch: 005 / 011 | Total loss: 0.826 | Reg loss: 0.026 | Tree loss: 0.826 | Accuracy: 0.645000 | 1.499 sec/iter\n",
      "Epoch: 273 | Batch: 006 / 011 | Total loss: 0.812 | Reg loss: 0.026 | Tree loss: 0.812 | Accuracy: 0.629000 | 1.499 sec/iter\n",
      "Epoch: 273 | Batch: 007 / 011 | Total loss: 0.819 | Reg loss: 0.026 | Tree loss: 0.819 | Accuracy: 0.627500 | 1.499 sec/iter\n",
      "Epoch: 273 | Batch: 008 / 011 | Total loss: 0.812 | Reg loss: 0.026 | Tree loss: 0.812 | Accuracy: 0.616000 | 1.499 sec/iter\n",
      "Epoch: 273 | Batch: 009 / 011 | Total loss: 0.793 | Reg loss: 0.026 | Tree loss: 0.793 | Accuracy: 0.647000 | 1.499 sec/iter\n",
      "Epoch: 273 | Batch: 010 / 011 | Total loss: 0.818 | Reg loss: 0.026 | Tree loss: 0.818 | Accuracy: 0.641638 | 1.499 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 274 | Batch: 000 / 011 | Total loss: 0.883 | Reg loss: 0.026 | Tree loss: 0.883 | Accuracy: 0.613500 | 1.5 sec/iter\n",
      "Epoch: 274 | Batch: 001 / 011 | Total loss: 0.880 | Reg loss: 0.026 | Tree loss: 0.880 | Accuracy: 0.644500 | 1.5 sec/iter\n",
      "Epoch: 274 | Batch: 002 / 011 | Total loss: 0.838 | Reg loss: 0.026 | Tree loss: 0.838 | Accuracy: 0.669000 | 1.5 sec/iter\n",
      "Epoch: 274 | Batch: 003 / 011 | Total loss: 0.816 | Reg loss: 0.026 | Tree loss: 0.816 | Accuracy: 0.657000 | 1.5 sec/iter\n",
      "Epoch: 274 | Batch: 004 / 011 | Total loss: 0.836 | Reg loss: 0.026 | Tree loss: 0.836 | Accuracy: 0.656000 | 1.499 sec/iter\n",
      "Epoch: 274 | Batch: 005 / 011 | Total loss: 0.847 | Reg loss: 0.026 | Tree loss: 0.847 | Accuracy: 0.620500 | 1.499 sec/iter\n",
      "Epoch: 274 | Batch: 006 / 011 | Total loss: 0.803 | Reg loss: 0.026 | Tree loss: 0.803 | Accuracy: 0.647500 | 1.499 sec/iter\n",
      "Epoch: 274 | Batch: 007 / 011 | Total loss: 0.827 | Reg loss: 0.026 | Tree loss: 0.827 | Accuracy: 0.618500 | 1.499 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 274 | Batch: 008 / 011 | Total loss: 0.785 | Reg loss: 0.026 | Tree loss: 0.785 | Accuracy: 0.634000 | 1.499 sec/iter\n",
      "Epoch: 274 | Batch: 009 / 011 | Total loss: 0.788 | Reg loss: 0.026 | Tree loss: 0.788 | Accuracy: 0.646000 | 1.499 sec/iter\n",
      "Epoch: 274 | Batch: 010 / 011 | Total loss: 0.784 | Reg loss: 0.026 | Tree loss: 0.784 | Accuracy: 0.597270 | 1.499 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 275 | Batch: 000 / 011 | Total loss: 0.872 | Reg loss: 0.026 | Tree loss: 0.872 | Accuracy: 0.637000 | 1.5 sec/iter\n",
      "Epoch: 275 | Batch: 001 / 011 | Total loss: 0.867 | Reg loss: 0.026 | Tree loss: 0.867 | Accuracy: 0.625500 | 1.5 sec/iter\n",
      "Epoch: 275 | Batch: 002 / 011 | Total loss: 0.831 | Reg loss: 0.026 | Tree loss: 0.831 | Accuracy: 0.661500 | 1.5 sec/iter\n",
      "Epoch: 275 | Batch: 003 / 011 | Total loss: 0.847 | Reg loss: 0.026 | Tree loss: 0.847 | Accuracy: 0.658500 | 1.5 sec/iter\n",
      "Epoch: 275 | Batch: 004 / 011 | Total loss: 0.847 | Reg loss: 0.026 | Tree loss: 0.847 | Accuracy: 0.635000 | 1.5 sec/iter\n",
      "Epoch: 275 | Batch: 005 / 011 | Total loss: 0.847 | Reg loss: 0.026 | Tree loss: 0.847 | Accuracy: 0.647500 | 1.5 sec/iter\n",
      "Epoch: 275 | Batch: 006 / 011 | Total loss: 0.803 | Reg loss: 0.026 | Tree loss: 0.803 | Accuracy: 0.653500 | 1.5 sec/iter\n",
      "Epoch: 275 | Batch: 007 / 011 | Total loss: 0.808 | Reg loss: 0.026 | Tree loss: 0.808 | Accuracy: 0.630000 | 1.5 sec/iter\n",
      "Epoch: 275 | Batch: 008 / 011 | Total loss: 0.811 | Reg loss: 0.026 | Tree loss: 0.811 | Accuracy: 0.626000 | 1.499 sec/iter\n",
      "Epoch: 275 | Batch: 009 / 011 | Total loss: 0.793 | Reg loss: 0.026 | Tree loss: 0.793 | Accuracy: 0.647000 | 1.499 sec/iter\n",
      "Epoch: 275 | Batch: 010 / 011 | Total loss: 0.743 | Reg loss: 0.026 | Tree loss: 0.743 | Accuracy: 0.668942 | 1.499 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 276 | Batch: 000 / 011 | Total loss: 0.876 | Reg loss: 0.026 | Tree loss: 0.876 | Accuracy: 0.633500 | 1.5 sec/iter\n",
      "Epoch: 276 | Batch: 001 / 011 | Total loss: 0.868 | Reg loss: 0.026 | Tree loss: 0.868 | Accuracy: 0.639000 | 1.5 sec/iter\n",
      "Epoch: 276 | Batch: 002 / 011 | Total loss: 0.868 | Reg loss: 0.026 | Tree loss: 0.868 | Accuracy: 0.640500 | 1.5 sec/iter\n",
      "Epoch: 276 | Batch: 003 / 011 | Total loss: 0.861 | Reg loss: 0.026 | Tree loss: 0.861 | Accuracy: 0.637500 | 1.5 sec/iter\n",
      "Epoch: 276 | Batch: 004 / 011 | Total loss: 0.812 | Reg loss: 0.026 | Tree loss: 0.812 | Accuracy: 0.669000 | 1.5 sec/iter\n",
      "Epoch: 276 | Batch: 005 / 011 | Total loss: 0.804 | Reg loss: 0.026 | Tree loss: 0.804 | Accuracy: 0.643000 | 1.5 sec/iter\n",
      "Epoch: 276 | Batch: 006 / 011 | Total loss: 0.797 | Reg loss: 0.026 | Tree loss: 0.797 | Accuracy: 0.639500 | 1.499 sec/iter\n",
      "Epoch: 276 | Batch: 007 / 011 | Total loss: 0.834 | Reg loss: 0.026 | Tree loss: 0.834 | Accuracy: 0.614000 | 1.499 sec/iter\n",
      "Epoch: 276 | Batch: 008 / 011 | Total loss: 0.807 | Reg loss: 0.026 | Tree loss: 0.807 | Accuracy: 0.622000 | 1.499 sec/iter\n",
      "Epoch: 276 | Batch: 009 / 011 | Total loss: 0.787 | Reg loss: 0.026 | Tree loss: 0.787 | Accuracy: 0.634500 | 1.499 sec/iter\n",
      "Epoch: 276 | Batch: 010 / 011 | Total loss: 0.799 | Reg loss: 0.026 | Tree loss: 0.799 | Accuracy: 0.610922 | 1.499 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 277 | Batch: 000 / 011 | Total loss: 0.881 | Reg loss: 0.026 | Tree loss: 0.881 | Accuracy: 0.629500 | 1.499 sec/iter\n",
      "Epoch: 277 | Batch: 001 / 011 | Total loss: 0.861 | Reg loss: 0.026 | Tree loss: 0.861 | Accuracy: 0.663000 | 1.499 sec/iter\n",
      "Epoch: 277 | Batch: 002 / 011 | Total loss: 0.842 | Reg loss: 0.026 | Tree loss: 0.842 | Accuracy: 0.656000 | 1.499 sec/iter\n",
      "Epoch: 277 | Batch: 003 / 011 | Total loss: 0.856 | Reg loss: 0.026 | Tree loss: 0.856 | Accuracy: 0.642500 | 1.499 sec/iter\n",
      "Epoch: 277 | Batch: 004 / 011 | Total loss: 0.837 | Reg loss: 0.026 | Tree loss: 0.837 | Accuracy: 0.649500 | 1.498 sec/iter\n",
      "Epoch: 277 | Batch: 005 / 011 | Total loss: 0.808 | Reg loss: 0.026 | Tree loss: 0.808 | Accuracy: 0.649500 | 1.498 sec/iter\n",
      "Epoch: 277 | Batch: 006 / 011 | Total loss: 0.802 | Reg loss: 0.026 | Tree loss: 0.802 | Accuracy: 0.652000 | 1.498 sec/iter\n",
      "Epoch: 277 | Batch: 007 / 011 | Total loss: 0.813 | Reg loss: 0.026 | Tree loss: 0.813 | Accuracy: 0.633000 | 1.498 sec/iter\n",
      "Epoch: 277 | Batch: 008 / 011 | Total loss: 0.800 | Reg loss: 0.026 | Tree loss: 0.800 | Accuracy: 0.632500 | 1.498 sec/iter\n",
      "Epoch: 277 | Batch: 009 / 011 | Total loss: 0.810 | Reg loss: 0.026 | Tree loss: 0.810 | Accuracy: 0.637500 | 1.498 sec/iter\n",
      "Epoch: 277 | Batch: 010 / 011 | Total loss: 0.820 | Reg loss: 0.026 | Tree loss: 0.820 | Accuracy: 0.610922 | 1.498 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 278 | Batch: 000 / 011 | Total loss: 0.861 | Reg loss: 0.026 | Tree loss: 0.861 | Accuracy: 0.644000 | 1.499 sec/iter\n",
      "Epoch: 278 | Batch: 001 / 011 | Total loss: 0.865 | Reg loss: 0.026 | Tree loss: 0.865 | Accuracy: 0.647500 | 1.499 sec/iter\n",
      "Epoch: 278 | Batch: 002 / 011 | Total loss: 0.871 | Reg loss: 0.026 | Tree loss: 0.871 | Accuracy: 0.628500 | 1.499 sec/iter\n",
      "Epoch: 278 | Batch: 003 / 011 | Total loss: 0.846 | Reg loss: 0.026 | Tree loss: 0.846 | Accuracy: 0.637000 | 1.499 sec/iter\n",
      "Epoch: 278 | Batch: 004 / 011 | Total loss: 0.804 | Reg loss: 0.026 | Tree loss: 0.804 | Accuracy: 0.660500 | 1.499 sec/iter\n",
      "Epoch: 278 | Batch: 005 / 011 | Total loss: 0.816 | Reg loss: 0.026 | Tree loss: 0.816 | Accuracy: 0.654000 | 1.499 sec/iter\n",
      "Epoch: 278 | Batch: 006 / 011 | Total loss: 0.820 | Reg loss: 0.026 | Tree loss: 0.820 | Accuracy: 0.638500 | 1.499 sec/iter\n",
      "Epoch: 278 | Batch: 007 / 011 | Total loss: 0.808 | Reg loss: 0.026 | Tree loss: 0.808 | Accuracy: 0.640500 | 1.498 sec/iter\n",
      "Epoch: 278 | Batch: 008 / 011 | Total loss: 0.800 | Reg loss: 0.026 | Tree loss: 0.800 | Accuracy: 0.621500 | 1.498 sec/iter\n",
      "Epoch: 278 | Batch: 009 / 011 | Total loss: 0.821 | Reg loss: 0.026 | Tree loss: 0.821 | Accuracy: 0.632000 | 1.498 sec/iter\n",
      "Epoch: 278 | Batch: 010 / 011 | Total loss: 0.813 | Reg loss: 0.026 | Tree loss: 0.813 | Accuracy: 0.631399 | 1.498 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 279 | Batch: 000 / 011 | Total loss: 0.895 | Reg loss: 0.026 | Tree loss: 0.895 | Accuracy: 0.621500 | 1.5 sec/iter\n",
      "Epoch: 279 | Batch: 001 / 011 | Total loss: 0.874 | Reg loss: 0.026 | Tree loss: 0.874 | Accuracy: 0.638000 | 1.5 sec/iter\n",
      "Epoch: 279 | Batch: 002 / 011 | Total loss: 0.843 | Reg loss: 0.026 | Tree loss: 0.843 | Accuracy: 0.660000 | 1.499 sec/iter\n",
      "Epoch: 279 | Batch: 003 / 011 | Total loss: 0.826 | Reg loss: 0.026 | Tree loss: 0.826 | Accuracy: 0.658000 | 1.499 sec/iter\n",
      "Epoch: 279 | Batch: 004 / 011 | Total loss: 0.824 | Reg loss: 0.026 | Tree loss: 0.824 | Accuracy: 0.669500 | 1.499 sec/iter\n",
      "Epoch: 279 | Batch: 005 / 011 | Total loss: 0.833 | Reg loss: 0.026 | Tree loss: 0.833 | Accuracy: 0.654500 | 1.499 sec/iter\n",
      "Epoch: 279 | Batch: 006 / 011 | Total loss: 0.811 | Reg loss: 0.026 | Tree loss: 0.811 | Accuracy: 0.642000 | 1.499 sec/iter\n",
      "Epoch: 279 | Batch: 007 / 011 | Total loss: 0.806 | Reg loss: 0.026 | Tree loss: 0.806 | Accuracy: 0.645000 | 1.499 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 279 | Batch: 008 / 011 | Total loss: 0.809 | Reg loss: 0.026 | Tree loss: 0.809 | Accuracy: 0.627500 | 1.499 sec/iter\n",
      "Epoch: 279 | Batch: 009 / 011 | Total loss: 0.785 | Reg loss: 0.026 | Tree loss: 0.785 | Accuracy: 0.630500 | 1.499 sec/iter\n",
      "Epoch: 279 | Batch: 010 / 011 | Total loss: 0.885 | Reg loss: 0.026 | Tree loss: 0.885 | Accuracy: 0.583618 | 1.499 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 280 | Batch: 000 / 011 | Total loss: 0.878 | Reg loss: 0.026 | Tree loss: 0.878 | Accuracy: 0.629500 | 1.5 sec/iter\n",
      "Epoch: 280 | Batch: 001 / 011 | Total loss: 0.859 | Reg loss: 0.026 | Tree loss: 0.859 | Accuracy: 0.668000 | 1.5 sec/iter\n",
      "Epoch: 280 | Batch: 002 / 011 | Total loss: 0.838 | Reg loss: 0.026 | Tree loss: 0.838 | Accuracy: 0.664500 | 1.5 sec/iter\n",
      "Epoch: 280 | Batch: 003 / 011 | Total loss: 0.858 | Reg loss: 0.026 | Tree loss: 0.858 | Accuracy: 0.639000 | 1.5 sec/iter\n",
      "Epoch: 280 | Batch: 004 / 011 | Total loss: 0.849 | Reg loss: 0.026 | Tree loss: 0.849 | Accuracy: 0.650500 | 1.499 sec/iter\n",
      "Epoch: 280 | Batch: 005 / 011 | Total loss: 0.824 | Reg loss: 0.026 | Tree loss: 0.824 | Accuracy: 0.625000 | 1.499 sec/iter\n",
      "Epoch: 280 | Batch: 006 / 011 | Total loss: 0.795 | Reg loss: 0.026 | Tree loss: 0.795 | Accuracy: 0.650000 | 1.499 sec/iter\n",
      "Epoch: 280 | Batch: 007 / 011 | Total loss: 0.808 | Reg loss: 0.026 | Tree loss: 0.808 | Accuracy: 0.626000 | 1.499 sec/iter\n",
      "Epoch: 280 | Batch: 008 / 011 | Total loss: 0.808 | Reg loss: 0.026 | Tree loss: 0.808 | Accuracy: 0.615000 | 1.499 sec/iter\n",
      "Epoch: 280 | Batch: 009 / 011 | Total loss: 0.797 | Reg loss: 0.026 | Tree loss: 0.797 | Accuracy: 0.625500 | 1.499 sec/iter\n",
      "Epoch: 280 | Batch: 010 / 011 | Total loss: 0.870 | Reg loss: 0.026 | Tree loss: 0.870 | Accuracy: 0.634812 | 1.499 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 281 | Batch: 000 / 011 | Total loss: 0.884 | Reg loss: 0.026 | Tree loss: 0.884 | Accuracy: 0.629500 | 1.5 sec/iter\n",
      "Epoch: 281 | Batch: 001 / 011 | Total loss: 0.863 | Reg loss: 0.026 | Tree loss: 0.863 | Accuracy: 0.644500 | 1.5 sec/iter\n",
      "Epoch: 281 | Batch: 002 / 011 | Total loss: 0.850 | Reg loss: 0.026 | Tree loss: 0.850 | Accuracy: 0.663000 | 1.5 sec/iter\n",
      "Epoch: 281 | Batch: 003 / 011 | Total loss: 0.860 | Reg loss: 0.026 | Tree loss: 0.860 | Accuracy: 0.636000 | 1.5 sec/iter\n",
      "Epoch: 281 | Batch: 004 / 011 | Total loss: 0.858 | Reg loss: 0.026 | Tree loss: 0.858 | Accuracy: 0.650000 | 1.5 sec/iter\n",
      "Epoch: 281 | Batch: 005 / 011 | Total loss: 0.802 | Reg loss: 0.026 | Tree loss: 0.802 | Accuracy: 0.657000 | 1.5 sec/iter\n",
      "Epoch: 281 | Batch: 006 / 011 | Total loss: 0.825 | Reg loss: 0.026 | Tree loss: 0.825 | Accuracy: 0.650500 | 1.499 sec/iter\n",
      "Epoch: 281 | Batch: 007 / 011 | Total loss: 0.795 | Reg loss: 0.026 | Tree loss: 0.795 | Accuracy: 0.640000 | 1.499 sec/iter\n",
      "Epoch: 281 | Batch: 008 / 011 | Total loss: 0.809 | Reg loss: 0.026 | Tree loss: 0.809 | Accuracy: 0.625500 | 1.499 sec/iter\n",
      "Epoch: 281 | Batch: 009 / 011 | Total loss: 0.805 | Reg loss: 0.026 | Tree loss: 0.805 | Accuracy: 0.615000 | 1.499 sec/iter\n",
      "Epoch: 281 | Batch: 010 / 011 | Total loss: 0.779 | Reg loss: 0.026 | Tree loss: 0.779 | Accuracy: 0.634812 | 1.499 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 282 | Batch: 000 / 011 | Total loss: 0.873 | Reg loss: 0.026 | Tree loss: 0.873 | Accuracy: 0.626500 | 1.5 sec/iter\n",
      "Epoch: 282 | Batch: 001 / 011 | Total loss: 0.877 | Reg loss: 0.026 | Tree loss: 0.877 | Accuracy: 0.634000 | 1.5 sec/iter\n",
      "Epoch: 282 | Batch: 002 / 011 | Total loss: 0.848 | Reg loss: 0.026 | Tree loss: 0.848 | Accuracy: 0.651500 | 1.5 sec/iter\n",
      "Epoch: 282 | Batch: 003 / 011 | Total loss: 0.834 | Reg loss: 0.026 | Tree loss: 0.834 | Accuracy: 0.646500 | 1.5 sec/iter\n",
      "Epoch: 282 | Batch: 004 / 011 | Total loss: 0.861 | Reg loss: 0.026 | Tree loss: 0.861 | Accuracy: 0.636500 | 1.5 sec/iter\n",
      "Epoch: 282 | Batch: 005 / 011 | Total loss: 0.826 | Reg loss: 0.026 | Tree loss: 0.826 | Accuracy: 0.614500 | 1.5 sec/iter\n",
      "Epoch: 282 | Batch: 006 / 011 | Total loss: 0.781 | Reg loss: 0.026 | Tree loss: 0.781 | Accuracy: 0.656000 | 1.499 sec/iter\n",
      "Epoch: 282 | Batch: 007 / 011 | Total loss: 0.808 | Reg loss: 0.026 | Tree loss: 0.808 | Accuracy: 0.625500 | 1.499 sec/iter\n",
      "Epoch: 282 | Batch: 008 / 011 | Total loss: 0.792 | Reg loss: 0.026 | Tree loss: 0.792 | Accuracy: 0.640000 | 1.499 sec/iter\n",
      "Epoch: 282 | Batch: 009 / 011 | Total loss: 0.819 | Reg loss: 0.026 | Tree loss: 0.819 | Accuracy: 0.618000 | 1.499 sec/iter\n",
      "Epoch: 282 | Batch: 010 / 011 | Total loss: 0.826 | Reg loss: 0.026 | Tree loss: 0.826 | Accuracy: 0.624573 | 1.499 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 283 | Batch: 000 / 011 | Total loss: 0.862 | Reg loss: 0.026 | Tree loss: 0.862 | Accuracy: 0.655000 | 1.499 sec/iter\n",
      "Epoch: 283 | Batch: 001 / 011 | Total loss: 0.862 | Reg loss: 0.026 | Tree loss: 0.862 | Accuracy: 0.657500 | 1.499 sec/iter\n",
      "Epoch: 283 | Batch: 002 / 011 | Total loss: 0.851 | Reg loss: 0.026 | Tree loss: 0.851 | Accuracy: 0.643500 | 1.499 sec/iter\n",
      "Epoch: 283 | Batch: 003 / 011 | Total loss: 0.833 | Reg loss: 0.026 | Tree loss: 0.833 | Accuracy: 0.654000 | 1.499 sec/iter\n",
      "Epoch: 283 | Batch: 004 / 011 | Total loss: 0.808 | Reg loss: 0.026 | Tree loss: 0.808 | Accuracy: 0.654000 | 1.499 sec/iter\n",
      "Epoch: 283 | Batch: 005 / 011 | Total loss: 0.854 | Reg loss: 0.026 | Tree loss: 0.854 | Accuracy: 0.637000 | 1.498 sec/iter\n",
      "Epoch: 283 | Batch: 006 / 011 | Total loss: 0.819 | Reg loss: 0.026 | Tree loss: 0.819 | Accuracy: 0.649000 | 1.498 sec/iter\n",
      "Epoch: 283 | Batch: 007 / 011 | Total loss: 0.800 | Reg loss: 0.026 | Tree loss: 0.800 | Accuracy: 0.647000 | 1.498 sec/iter\n",
      "Epoch: 283 | Batch: 008 / 011 | Total loss: 0.791 | Reg loss: 0.026 | Tree loss: 0.791 | Accuracy: 0.647500 | 1.498 sec/iter\n",
      "Epoch: 283 | Batch: 009 / 011 | Total loss: 0.799 | Reg loss: 0.026 | Tree loss: 0.799 | Accuracy: 0.628500 | 1.498 sec/iter\n",
      "Epoch: 283 | Batch: 010 / 011 | Total loss: 0.861 | Reg loss: 0.026 | Tree loss: 0.861 | Accuracy: 0.607509 | 1.498 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 284 | Batch: 000 / 011 | Total loss: 0.889 | Reg loss: 0.026 | Tree loss: 0.889 | Accuracy: 0.633000 | 1.5 sec/iter\n",
      "Epoch: 284 | Batch: 001 / 011 | Total loss: 0.870 | Reg loss: 0.026 | Tree loss: 0.870 | Accuracy: 0.645000 | 1.5 sec/iter\n",
      "Epoch: 284 | Batch: 002 / 011 | Total loss: 0.839 | Reg loss: 0.026 | Tree loss: 0.839 | Accuracy: 0.664000 | 1.499 sec/iter\n",
      "Epoch: 284 | Batch: 003 / 011 | Total loss: 0.831 | Reg loss: 0.026 | Tree loss: 0.831 | Accuracy: 0.653000 | 1.499 sec/iter\n",
      "Epoch: 284 | Batch: 004 / 011 | Total loss: 0.805 | Reg loss: 0.026 | Tree loss: 0.805 | Accuracy: 0.659500 | 1.499 sec/iter\n",
      "Epoch: 284 | Batch: 005 / 011 | Total loss: 0.814 | Reg loss: 0.026 | Tree loss: 0.814 | Accuracy: 0.654000 | 1.499 sec/iter\n",
      "Epoch: 284 | Batch: 006 / 011 | Total loss: 0.823 | Reg loss: 0.026 | Tree loss: 0.823 | Accuracy: 0.634500 | 1.499 sec/iter\n",
      "Epoch: 284 | Batch: 007 / 011 | Total loss: 0.800 | Reg loss: 0.026 | Tree loss: 0.800 | Accuracy: 0.631500 | 1.499 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 284 | Batch: 008 / 011 | Total loss: 0.798 | Reg loss: 0.026 | Tree loss: 0.798 | Accuracy: 0.639000 | 1.499 sec/iter\n",
      "Epoch: 284 | Batch: 009 / 011 | Total loss: 0.815 | Reg loss: 0.026 | Tree loss: 0.815 | Accuracy: 0.618500 | 1.499 sec/iter\n",
      "Epoch: 284 | Batch: 010 / 011 | Total loss: 0.748 | Reg loss: 0.026 | Tree loss: 0.748 | Accuracy: 0.658703 | 1.499 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 285 | Batch: 000 / 011 | Total loss: 0.858 | Reg loss: 0.026 | Tree loss: 0.858 | Accuracy: 0.642500 | 1.5 sec/iter\n",
      "Epoch: 285 | Batch: 001 / 011 | Total loss: 0.876 | Reg loss: 0.026 | Tree loss: 0.876 | Accuracy: 0.639000 | 1.5 sec/iter\n",
      "Epoch: 285 | Batch: 002 / 011 | Total loss: 0.854 | Reg loss: 0.026 | Tree loss: 0.854 | Accuracy: 0.649500 | 1.5 sec/iter\n",
      "Epoch: 285 | Batch: 003 / 011 | Total loss: 0.827 | Reg loss: 0.026 | Tree loss: 0.827 | Accuracy: 0.660000 | 1.499 sec/iter\n",
      "Epoch: 285 | Batch: 004 / 011 | Total loss: 0.830 | Reg loss: 0.026 | Tree loss: 0.830 | Accuracy: 0.640000 | 1.499 sec/iter\n",
      "Epoch: 285 | Batch: 005 / 011 | Total loss: 0.799 | Reg loss: 0.026 | Tree loss: 0.799 | Accuracy: 0.661000 | 1.499 sec/iter\n",
      "Epoch: 285 | Batch: 006 / 011 | Total loss: 0.802 | Reg loss: 0.026 | Tree loss: 0.802 | Accuracy: 0.637500 | 1.499 sec/iter\n",
      "Epoch: 285 | Batch: 007 / 011 | Total loss: 0.834 | Reg loss: 0.026 | Tree loss: 0.834 | Accuracy: 0.610000 | 1.499 sec/iter\n",
      "Epoch: 285 | Batch: 008 / 011 | Total loss: 0.791 | Reg loss: 0.026 | Tree loss: 0.791 | Accuracy: 0.638000 | 1.499 sec/iter\n",
      "Epoch: 285 | Batch: 009 / 011 | Total loss: 0.819 | Reg loss: 0.026 | Tree loss: 0.819 | Accuracy: 0.634000 | 1.499 sec/iter\n",
      "Epoch: 285 | Batch: 010 / 011 | Total loss: 0.752 | Reg loss: 0.026 | Tree loss: 0.752 | Accuracy: 0.668942 | 1.499 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 286 | Batch: 000 / 011 | Total loss: 0.866 | Reg loss: 0.026 | Tree loss: 0.866 | Accuracy: 0.648000 | 1.5 sec/iter\n",
      "Epoch: 286 | Batch: 001 / 011 | Total loss: 0.863 | Reg loss: 0.026 | Tree loss: 0.863 | Accuracy: 0.647000 | 1.5 sec/iter\n",
      "Epoch: 286 | Batch: 002 / 011 | Total loss: 0.844 | Reg loss: 0.026 | Tree loss: 0.844 | Accuracy: 0.654500 | 1.5 sec/iter\n",
      "Epoch: 286 | Batch: 003 / 011 | Total loss: 0.858 | Reg loss: 0.026 | Tree loss: 0.858 | Accuracy: 0.645000 | 1.5 sec/iter\n",
      "Epoch: 286 | Batch: 004 / 011 | Total loss: 0.834 | Reg loss: 0.026 | Tree loss: 0.834 | Accuracy: 0.654000 | 1.5 sec/iter\n",
      "Epoch: 286 | Batch: 005 / 011 | Total loss: 0.802 | Reg loss: 0.026 | Tree loss: 0.802 | Accuracy: 0.672000 | 1.5 sec/iter\n",
      "Epoch: 286 | Batch: 006 / 011 | Total loss: 0.826 | Reg loss: 0.026 | Tree loss: 0.826 | Accuracy: 0.646500 | 1.499 sec/iter\n",
      "Epoch: 286 | Batch: 007 / 011 | Total loss: 0.824 | Reg loss: 0.026 | Tree loss: 0.824 | Accuracy: 0.618000 | 1.499 sec/iter\n",
      "Epoch: 286 | Batch: 008 / 011 | Total loss: 0.784 | Reg loss: 0.026 | Tree loss: 0.784 | Accuracy: 0.640500 | 1.499 sec/iter\n",
      "Epoch: 286 | Batch: 009 / 011 | Total loss: 0.800 | Reg loss: 0.026 | Tree loss: 0.800 | Accuracy: 0.634000 | 1.499 sec/iter\n",
      "Epoch: 286 | Batch: 010 / 011 | Total loss: 0.782 | Reg loss: 0.026 | Tree loss: 0.782 | Accuracy: 0.617747 | 1.499 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 287 | Batch: 000 / 011 | Total loss: 0.854 | Reg loss: 0.026 | Tree loss: 0.854 | Accuracy: 0.648000 | 1.5 sec/iter\n",
      "Epoch: 287 | Batch: 001 / 011 | Total loss: 0.869 | Reg loss: 0.026 | Tree loss: 0.869 | Accuracy: 0.649000 | 1.5 sec/iter\n",
      "Epoch: 287 | Batch: 002 / 011 | Total loss: 0.847 | Reg loss: 0.026 | Tree loss: 0.847 | Accuracy: 0.656000 | 1.5 sec/iter\n",
      "Epoch: 287 | Batch: 003 / 011 | Total loss: 0.845 | Reg loss: 0.026 | Tree loss: 0.845 | Accuracy: 0.637500 | 1.5 sec/iter\n",
      "Epoch: 287 | Batch: 004 / 011 | Total loss: 0.822 | Reg loss: 0.026 | Tree loss: 0.822 | Accuracy: 0.663000 | 1.5 sec/iter\n",
      "Epoch: 287 | Batch: 005 / 011 | Total loss: 0.829 | Reg loss: 0.026 | Tree loss: 0.829 | Accuracy: 0.629000 | 1.5 sec/iter\n",
      "Epoch: 287 | Batch: 006 / 011 | Total loss: 0.823 | Reg loss: 0.026 | Tree loss: 0.823 | Accuracy: 0.650500 | 1.499 sec/iter\n",
      "Epoch: 287 | Batch: 007 / 011 | Total loss: 0.794 | Reg loss: 0.026 | Tree loss: 0.794 | Accuracy: 0.646000 | 1.499 sec/iter\n",
      "Epoch: 287 | Batch: 008 / 011 | Total loss: 0.800 | Reg loss: 0.026 | Tree loss: 0.800 | Accuracy: 0.623000 | 1.499 sec/iter\n",
      "Epoch: 287 | Batch: 009 / 011 | Total loss: 0.807 | Reg loss: 0.026 | Tree loss: 0.807 | Accuracy: 0.634000 | 1.499 sec/iter\n",
      "Epoch: 287 | Batch: 010 / 011 | Total loss: 0.779 | Reg loss: 0.026 | Tree loss: 0.779 | Accuracy: 0.662116 | 1.499 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 288 | Batch: 000 / 011 | Total loss: 0.890 | Reg loss: 0.026 | Tree loss: 0.890 | Accuracy: 0.626500 | 1.501 sec/iter\n",
      "Epoch: 288 | Batch: 001 / 011 | Total loss: 0.861 | Reg loss: 0.026 | Tree loss: 0.861 | Accuracy: 0.636500 | 1.5 sec/iter\n",
      "Epoch: 288 | Batch: 002 / 011 | Total loss: 0.859 | Reg loss: 0.026 | Tree loss: 0.859 | Accuracy: 0.646500 | 1.5 sec/iter\n",
      "Epoch: 288 | Batch: 003 / 011 | Total loss: 0.842 | Reg loss: 0.026 | Tree loss: 0.842 | Accuracy: 0.647000 | 1.5 sec/iter\n",
      "Epoch: 288 | Batch: 004 / 011 | Total loss: 0.810 | Reg loss: 0.026 | Tree loss: 0.810 | Accuracy: 0.657500 | 1.5 sec/iter\n",
      "Epoch: 288 | Batch: 005 / 011 | Total loss: 0.834 | Reg loss: 0.026 | Tree loss: 0.834 | Accuracy: 0.637000 | 1.5 sec/iter\n",
      "Epoch: 288 | Batch: 006 / 011 | Total loss: 0.806 | Reg loss: 0.026 | Tree loss: 0.806 | Accuracy: 0.649500 | 1.5 sec/iter\n",
      "Epoch: 288 | Batch: 007 / 011 | Total loss: 0.785 | Reg loss: 0.026 | Tree loss: 0.785 | Accuracy: 0.654500 | 1.499 sec/iter\n",
      "Epoch: 288 | Batch: 008 / 011 | Total loss: 0.785 | Reg loss: 0.026 | Tree loss: 0.785 | Accuracy: 0.650000 | 1.499 sec/iter\n",
      "Epoch: 288 | Batch: 009 / 011 | Total loss: 0.802 | Reg loss: 0.026 | Tree loss: 0.802 | Accuracy: 0.632000 | 1.499 sec/iter\n",
      "Epoch: 288 | Batch: 010 / 011 | Total loss: 0.842 | Reg loss: 0.026 | Tree loss: 0.842 | Accuracy: 0.610922 | 1.499 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 289 | Batch: 000 / 011 | Total loss: 0.875 | Reg loss: 0.026 | Tree loss: 0.875 | Accuracy: 0.637500 | 1.5 sec/iter\n",
      "Epoch: 289 | Batch: 001 / 011 | Total loss: 0.860 | Reg loss: 0.026 | Tree loss: 0.860 | Accuracy: 0.651000 | 1.499 sec/iter\n",
      "Epoch: 289 | Batch: 002 / 011 | Total loss: 0.852 | Reg loss: 0.026 | Tree loss: 0.852 | Accuracy: 0.641500 | 1.499 sec/iter\n",
      "Epoch: 289 | Batch: 003 / 011 | Total loss: 0.827 | Reg loss: 0.026 | Tree loss: 0.827 | Accuracy: 0.669500 | 1.499 sec/iter\n",
      "Epoch: 289 | Batch: 004 / 011 | Total loss: 0.800 | Reg loss: 0.026 | Tree loss: 0.800 | Accuracy: 0.663500 | 1.499 sec/iter\n",
      "Epoch: 289 | Batch: 005 / 011 | Total loss: 0.817 | Reg loss: 0.026 | Tree loss: 0.817 | Accuracy: 0.654000 | 1.499 sec/iter\n",
      "Epoch: 289 | Batch: 006 / 011 | Total loss: 0.826 | Reg loss: 0.026 | Tree loss: 0.826 | Accuracy: 0.635500 | 1.499 sec/iter\n",
      "Epoch: 289 | Batch: 007 / 011 | Total loss: 0.807 | Reg loss: 0.026 | Tree loss: 0.807 | Accuracy: 0.635000 | 1.499 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 289 | Batch: 008 / 011 | Total loss: 0.800 | Reg loss: 0.026 | Tree loss: 0.800 | Accuracy: 0.631500 | 1.499 sec/iter\n",
      "Epoch: 289 | Batch: 009 / 011 | Total loss: 0.805 | Reg loss: 0.026 | Tree loss: 0.805 | Accuracy: 0.636500 | 1.499 sec/iter\n",
      "Epoch: 289 | Batch: 010 / 011 | Total loss: 0.773 | Reg loss: 0.026 | Tree loss: 0.773 | Accuracy: 0.587031 | 1.498 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 290 | Batch: 000 / 011 | Total loss: 0.889 | Reg loss: 0.026 | Tree loss: 0.889 | Accuracy: 0.626500 | 1.5 sec/iter\n",
      "Epoch: 290 | Batch: 001 / 011 | Total loss: 0.878 | Reg loss: 0.026 | Tree loss: 0.878 | Accuracy: 0.639500 | 1.499 sec/iter\n",
      "Epoch: 290 | Batch: 002 / 011 | Total loss: 0.834 | Reg loss: 0.026 | Tree loss: 0.834 | Accuracy: 0.656000 | 1.499 sec/iter\n",
      "Epoch: 290 | Batch: 003 / 011 | Total loss: 0.829 | Reg loss: 0.026 | Tree loss: 0.829 | Accuracy: 0.657000 | 1.499 sec/iter\n",
      "Epoch: 290 | Batch: 004 / 011 | Total loss: 0.825 | Reg loss: 0.026 | Tree loss: 0.825 | Accuracy: 0.662500 | 1.499 sec/iter\n",
      "Epoch: 290 | Batch: 005 / 011 | Total loss: 0.815 | Reg loss: 0.026 | Tree loss: 0.815 | Accuracy: 0.649500 | 1.499 sec/iter\n",
      "Epoch: 290 | Batch: 006 / 011 | Total loss: 0.798 | Reg loss: 0.026 | Tree loss: 0.798 | Accuracy: 0.652500 | 1.499 sec/iter\n",
      "Epoch: 290 | Batch: 007 / 011 | Total loss: 0.834 | Reg loss: 0.026 | Tree loss: 0.834 | Accuracy: 0.626000 | 1.499 sec/iter\n",
      "Epoch: 290 | Batch: 008 / 011 | Total loss: 0.797 | Reg loss: 0.026 | Tree loss: 0.797 | Accuracy: 0.636500 | 1.499 sec/iter\n",
      "Epoch: 290 | Batch: 009 / 011 | Total loss: 0.794 | Reg loss: 0.026 | Tree loss: 0.794 | Accuracy: 0.646500 | 1.499 sec/iter\n",
      "Epoch: 290 | Batch: 010 / 011 | Total loss: 0.798 | Reg loss: 0.026 | Tree loss: 0.798 | Accuracy: 0.645051 | 1.499 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 291 | Batch: 000 / 011 | Total loss: 0.878 | Reg loss: 0.026 | Tree loss: 0.878 | Accuracy: 0.636000 | 1.5 sec/iter\n",
      "Epoch: 291 | Batch: 001 / 011 | Total loss: 0.844 | Reg loss: 0.026 | Tree loss: 0.844 | Accuracy: 0.666000 | 1.5 sec/iter\n",
      "Epoch: 291 | Batch: 002 / 011 | Total loss: 0.859 | Reg loss: 0.026 | Tree loss: 0.859 | Accuracy: 0.646500 | 1.5 sec/iter\n",
      "Epoch: 291 | Batch: 003 / 011 | Total loss: 0.807 | Reg loss: 0.026 | Tree loss: 0.807 | Accuracy: 0.674500 | 1.5 sec/iter\n",
      "Epoch: 291 | Batch: 004 / 011 | Total loss: 0.828 | Reg loss: 0.026 | Tree loss: 0.828 | Accuracy: 0.636000 | 1.499 sec/iter\n",
      "Epoch: 291 | Batch: 005 / 011 | Total loss: 0.821 | Reg loss: 0.026 | Tree loss: 0.821 | Accuracy: 0.640500 | 1.499 sec/iter\n",
      "Epoch: 291 | Batch: 006 / 011 | Total loss: 0.808 | Reg loss: 0.026 | Tree loss: 0.808 | Accuracy: 0.630500 | 1.499 sec/iter\n",
      "Epoch: 291 | Batch: 007 / 011 | Total loss: 0.817 | Reg loss: 0.026 | Tree loss: 0.817 | Accuracy: 0.632500 | 1.499 sec/iter\n",
      "Epoch: 291 | Batch: 008 / 011 | Total loss: 0.805 | Reg loss: 0.026 | Tree loss: 0.805 | Accuracy: 0.627000 | 1.499 sec/iter\n",
      "Epoch: 291 | Batch: 009 / 011 | Total loss: 0.812 | Reg loss: 0.026 | Tree loss: 0.812 | Accuracy: 0.636500 | 1.499 sec/iter\n",
      "Epoch: 291 | Batch: 010 / 011 | Total loss: 0.799 | Reg loss: 0.026 | Tree loss: 0.799 | Accuracy: 0.624573 | 1.499 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 292 | Batch: 000 / 011 | Total loss: 0.890 | Reg loss: 0.026 | Tree loss: 0.890 | Accuracy: 0.636000 | 1.5 sec/iter\n",
      "Epoch: 292 | Batch: 001 / 011 | Total loss: 0.870 | Reg loss: 0.026 | Tree loss: 0.870 | Accuracy: 0.642000 | 1.5 sec/iter\n",
      "Epoch: 292 | Batch: 002 / 011 | Total loss: 0.843 | Reg loss: 0.026 | Tree loss: 0.843 | Accuracy: 0.644000 | 1.5 sec/iter\n",
      "Epoch: 292 | Batch: 003 / 011 | Total loss: 0.841 | Reg loss: 0.026 | Tree loss: 0.841 | Accuracy: 0.654000 | 1.5 sec/iter\n",
      "Epoch: 292 | Batch: 004 / 011 | Total loss: 0.827 | Reg loss: 0.026 | Tree loss: 0.827 | Accuracy: 0.645000 | 1.5 sec/iter\n",
      "Epoch: 292 | Batch: 005 / 011 | Total loss: 0.813 | Reg loss: 0.026 | Tree loss: 0.813 | Accuracy: 0.650000 | 1.5 sec/iter\n",
      "Epoch: 292 | Batch: 006 / 011 | Total loss: 0.815 | Reg loss: 0.026 | Tree loss: 0.815 | Accuracy: 0.649000 | 1.499 sec/iter\n",
      "Epoch: 292 | Batch: 007 / 011 | Total loss: 0.791 | Reg loss: 0.026 | Tree loss: 0.791 | Accuracy: 0.645000 | 1.499 sec/iter\n",
      "Epoch: 292 | Batch: 008 / 011 | Total loss: 0.810 | Reg loss: 0.026 | Tree loss: 0.810 | Accuracy: 0.612500 | 1.499 sec/iter\n",
      "Epoch: 292 | Batch: 009 / 011 | Total loss: 0.781 | Reg loss: 0.026 | Tree loss: 0.781 | Accuracy: 0.651500 | 1.499 sec/iter\n",
      "Epoch: 292 | Batch: 010 / 011 | Total loss: 0.843 | Reg loss: 0.026 | Tree loss: 0.843 | Accuracy: 0.580205 | 1.499 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 293 | Batch: 000 / 011 | Total loss: 0.871 | Reg loss: 0.026 | Tree loss: 0.871 | Accuracy: 0.631000 | 1.5 sec/iter\n",
      "Epoch: 293 | Batch: 001 / 011 | Total loss: 0.869 | Reg loss: 0.026 | Tree loss: 0.869 | Accuracy: 0.648000 | 1.5 sec/iter\n",
      "Epoch: 293 | Batch: 002 / 011 | Total loss: 0.849 | Reg loss: 0.026 | Tree loss: 0.849 | Accuracy: 0.658500 | 1.5 sec/iter\n",
      "Epoch: 293 | Batch: 003 / 011 | Total loss: 0.816 | Reg loss: 0.026 | Tree loss: 0.816 | Accuracy: 0.663500 | 1.5 sec/iter\n",
      "Epoch: 293 | Batch: 004 / 011 | Total loss: 0.814 | Reg loss: 0.026 | Tree loss: 0.814 | Accuracy: 0.660000 | 1.5 sec/iter\n",
      "Epoch: 293 | Batch: 005 / 011 | Total loss: 0.839 | Reg loss: 0.026 | Tree loss: 0.839 | Accuracy: 0.632500 | 1.5 sec/iter\n",
      "Epoch: 293 | Batch: 006 / 011 | Total loss: 0.802 | Reg loss: 0.026 | Tree loss: 0.802 | Accuracy: 0.649000 | 1.5 sec/iter\n",
      "Epoch: 293 | Batch: 007 / 011 | Total loss: 0.799 | Reg loss: 0.026 | Tree loss: 0.799 | Accuracy: 0.636000 | 1.5 sec/iter\n",
      "Epoch: 293 | Batch: 008 / 011 | Total loss: 0.810 | Reg loss: 0.026 | Tree loss: 0.810 | Accuracy: 0.614000 | 1.5 sec/iter\n",
      "Epoch: 293 | Batch: 009 / 011 | Total loss: 0.811 | Reg loss: 0.026 | Tree loss: 0.811 | Accuracy: 0.624000 | 1.499 sec/iter\n",
      "Epoch: 293 | Batch: 010 / 011 | Total loss: 0.792 | Reg loss: 0.026 | Tree loss: 0.792 | Accuracy: 0.658703 | 1.499 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 294 | Batch: 000 / 011 | Total loss: 0.867 | Reg loss: 0.026 | Tree loss: 0.867 | Accuracy: 0.649500 | 1.501 sec/iter\n",
      "Epoch: 294 | Batch: 001 / 011 | Total loss: 0.861 | Reg loss: 0.026 | Tree loss: 0.861 | Accuracy: 0.638500 | 1.5 sec/iter\n",
      "Epoch: 294 | Batch: 002 / 011 | Total loss: 0.852 | Reg loss: 0.026 | Tree loss: 0.852 | Accuracy: 0.636500 | 1.5 sec/iter\n",
      "Epoch: 294 | Batch: 003 / 011 | Total loss: 0.810 | Reg loss: 0.026 | Tree loss: 0.810 | Accuracy: 0.675000 | 1.5 sec/iter\n",
      "Epoch: 294 | Batch: 004 / 011 | Total loss: 0.846 | Reg loss: 0.026 | Tree loss: 0.846 | Accuracy: 0.621000 | 1.5 sec/iter\n",
      "Epoch: 294 | Batch: 005 / 011 | Total loss: 0.818 | Reg loss: 0.026 | Tree loss: 0.818 | Accuracy: 0.656000 | 1.5 sec/iter\n",
      "Epoch: 294 | Batch: 006 / 011 | Total loss: 0.817 | Reg loss: 0.026 | Tree loss: 0.817 | Accuracy: 0.637500 | 1.5 sec/iter\n",
      "Epoch: 294 | Batch: 007 / 011 | Total loss: 0.784 | Reg loss: 0.026 | Tree loss: 0.784 | Accuracy: 0.642000 | 1.499 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 294 | Batch: 008 / 011 | Total loss: 0.815 | Reg loss: 0.026 | Tree loss: 0.815 | Accuracy: 0.629500 | 1.499 sec/iter\n",
      "Epoch: 294 | Batch: 009 / 011 | Total loss: 0.816 | Reg loss: 0.026 | Tree loss: 0.816 | Accuracy: 0.614500 | 1.499 sec/iter\n",
      "Epoch: 294 | Batch: 010 / 011 | Total loss: 0.825 | Reg loss: 0.026 | Tree loss: 0.825 | Accuracy: 0.600683 | 1.499 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 295 | Batch: 000 / 011 | Total loss: 0.888 | Reg loss: 0.026 | Tree loss: 0.888 | Accuracy: 0.622500 | 1.499 sec/iter\n",
      "Epoch: 295 | Batch: 001 / 011 | Total loss: 0.858 | Reg loss: 0.026 | Tree loss: 0.858 | Accuracy: 0.663000 | 1.499 sec/iter\n",
      "Epoch: 295 | Batch: 002 / 011 | Total loss: 0.841 | Reg loss: 0.026 | Tree loss: 0.841 | Accuracy: 0.655000 | 1.499 sec/iter\n",
      "Epoch: 295 | Batch: 003 / 011 | Total loss: 0.849 | Reg loss: 0.026 | Tree loss: 0.849 | Accuracy: 0.644500 | 1.499 sec/iter\n",
      "Epoch: 295 | Batch: 004 / 011 | Total loss: 0.843 | Reg loss: 0.026 | Tree loss: 0.843 | Accuracy: 0.647000 | 1.499 sec/iter\n",
      "Epoch: 295 | Batch: 005 / 011 | Total loss: 0.809 | Reg loss: 0.026 | Tree loss: 0.809 | Accuracy: 0.655000 | 1.499 sec/iter\n",
      "Epoch: 295 | Batch: 006 / 011 | Total loss: 0.787 | Reg loss: 0.026 | Tree loss: 0.787 | Accuracy: 0.668500 | 1.499 sec/iter\n",
      "Epoch: 295 | Batch: 007 / 011 | Total loss: 0.814 | Reg loss: 0.026 | Tree loss: 0.814 | Accuracy: 0.640000 | 1.499 sec/iter\n",
      "Epoch: 295 | Batch: 008 / 011 | Total loss: 0.781 | Reg loss: 0.026 | Tree loss: 0.781 | Accuracy: 0.655000 | 1.499 sec/iter\n",
      "Epoch: 295 | Batch: 009 / 011 | Total loss: 0.820 | Reg loss: 0.026 | Tree loss: 0.820 | Accuracy: 0.624500 | 1.499 sec/iter\n",
      "Epoch: 295 | Batch: 010 / 011 | Total loss: 0.748 | Reg loss: 0.026 | Tree loss: 0.748 | Accuracy: 0.662116 | 1.498 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 296 | Batch: 000 / 011 | Total loss: 0.866 | Reg loss: 0.026 | Tree loss: 0.866 | Accuracy: 0.640000 | 1.5 sec/iter\n",
      "Epoch: 296 | Batch: 001 / 011 | Total loss: 0.877 | Reg loss: 0.026 | Tree loss: 0.877 | Accuracy: 0.630000 | 1.5 sec/iter\n",
      "Epoch: 296 | Batch: 002 / 011 | Total loss: 0.849 | Reg loss: 0.026 | Tree loss: 0.849 | Accuracy: 0.649500 | 1.5 sec/iter\n",
      "Epoch: 296 | Batch: 003 / 011 | Total loss: 0.835 | Reg loss: 0.026 | Tree loss: 0.835 | Accuracy: 0.658000 | 1.499 sec/iter\n",
      "Epoch: 296 | Batch: 004 / 011 | Total loss: 0.834 | Reg loss: 0.026 | Tree loss: 0.834 | Accuracy: 0.644000 | 1.499 sec/iter\n",
      "Epoch: 296 | Batch: 005 / 011 | Total loss: 0.811 | Reg loss: 0.026 | Tree loss: 0.811 | Accuracy: 0.661500 | 1.499 sec/iter\n",
      "Epoch: 296 | Batch: 006 / 011 | Total loss: 0.803 | Reg loss: 0.026 | Tree loss: 0.803 | Accuracy: 0.630000 | 1.499 sec/iter\n",
      "Epoch: 296 | Batch: 007 / 011 | Total loss: 0.790 | Reg loss: 0.026 | Tree loss: 0.790 | Accuracy: 0.635500 | 1.499 sec/iter\n",
      "Epoch: 296 | Batch: 008 / 011 | Total loss: 0.811 | Reg loss: 0.026 | Tree loss: 0.811 | Accuracy: 0.627000 | 1.499 sec/iter\n",
      "Epoch: 296 | Batch: 009 / 011 | Total loss: 0.808 | Reg loss: 0.026 | Tree loss: 0.808 | Accuracy: 0.640500 | 1.499 sec/iter\n",
      "Epoch: 296 | Batch: 010 / 011 | Total loss: 0.757 | Reg loss: 0.026 | Tree loss: 0.757 | Accuracy: 0.658703 | 1.499 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 297 | Batch: 000 / 011 | Total loss: 0.872 | Reg loss: 0.026 | Tree loss: 0.872 | Accuracy: 0.645500 | 1.5 sec/iter\n",
      "Epoch: 297 | Batch: 001 / 011 | Total loss: 0.859 | Reg loss: 0.026 | Tree loss: 0.859 | Accuracy: 0.646000 | 1.5 sec/iter\n",
      "Epoch: 297 | Batch: 002 / 011 | Total loss: 0.846 | Reg loss: 0.026 | Tree loss: 0.846 | Accuracy: 0.658000 | 1.5 sec/iter\n",
      "Epoch: 297 | Batch: 003 / 011 | Total loss: 0.861 | Reg loss: 0.026 | Tree loss: 0.861 | Accuracy: 0.648500 | 1.5 sec/iter\n",
      "Epoch: 297 | Batch: 004 / 011 | Total loss: 0.802 | Reg loss: 0.026 | Tree loss: 0.802 | Accuracy: 0.655500 | 1.5 sec/iter\n",
      "Epoch: 297 | Batch: 005 / 011 | Total loss: 0.827 | Reg loss: 0.026 | Tree loss: 0.827 | Accuracy: 0.630000 | 1.499 sec/iter\n",
      "Epoch: 297 | Batch: 006 / 011 | Total loss: 0.819 | Reg loss: 0.026 | Tree loss: 0.819 | Accuracy: 0.631000 | 1.499 sec/iter\n",
      "Epoch: 297 | Batch: 007 / 011 | Total loss: 0.815 | Reg loss: 0.026 | Tree loss: 0.815 | Accuracy: 0.630500 | 1.499 sec/iter\n",
      "Epoch: 297 | Batch: 008 / 011 | Total loss: 0.791 | Reg loss: 0.026 | Tree loss: 0.791 | Accuracy: 0.627000 | 1.499 sec/iter\n",
      "Epoch: 297 | Batch: 009 / 011 | Total loss: 0.805 | Reg loss: 0.026 | Tree loss: 0.805 | Accuracy: 0.633000 | 1.499 sec/iter\n",
      "Epoch: 297 | Batch: 010 / 011 | Total loss: 0.779 | Reg loss: 0.026 | Tree loss: 0.779 | Accuracy: 0.658703 | 1.499 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 298 | Batch: 000 / 011 | Total loss: 0.868 | Reg loss: 0.026 | Tree loss: 0.868 | Accuracy: 0.635500 | 1.5 sec/iter\n",
      "Epoch: 298 | Batch: 001 / 011 | Total loss: 0.849 | Reg loss: 0.026 | Tree loss: 0.849 | Accuracy: 0.635000 | 1.5 sec/iter\n",
      "Epoch: 298 | Batch: 002 / 011 | Total loss: 0.873 | Reg loss: 0.026 | Tree loss: 0.873 | Accuracy: 0.650000 | 1.5 sec/iter\n",
      "Epoch: 298 | Batch: 003 / 011 | Total loss: 0.859 | Reg loss: 0.026 | Tree loss: 0.859 | Accuracy: 0.642000 | 1.5 sec/iter\n",
      "Epoch: 298 | Batch: 004 / 011 | Total loss: 0.823 | Reg loss: 0.026 | Tree loss: 0.823 | Accuracy: 0.647500 | 1.5 sec/iter\n",
      "Epoch: 298 | Batch: 005 / 011 | Total loss: 0.814 | Reg loss: 0.026 | Tree loss: 0.814 | Accuracy: 0.651500 | 1.5 sec/iter\n",
      "Epoch: 298 | Batch: 006 / 011 | Total loss: 0.800 | Reg loss: 0.026 | Tree loss: 0.800 | Accuracy: 0.653500 | 1.499 sec/iter\n",
      "Epoch: 298 | Batch: 007 / 011 | Total loss: 0.789 | Reg loss: 0.026 | Tree loss: 0.789 | Accuracy: 0.644500 | 1.499 sec/iter\n",
      "Epoch: 298 | Batch: 008 / 011 | Total loss: 0.807 | Reg loss: 0.026 | Tree loss: 0.807 | Accuracy: 0.641500 | 1.499 sec/iter\n",
      "Epoch: 298 | Batch: 009 / 011 | Total loss: 0.811 | Reg loss: 0.026 | Tree loss: 0.811 | Accuracy: 0.623500 | 1.499 sec/iter\n",
      "Epoch: 298 | Batch: 010 / 011 | Total loss: 0.842 | Reg loss: 0.026 | Tree loss: 0.842 | Accuracy: 0.631399 | 1.499 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 299 | Batch: 000 / 011 | Total loss: 0.871 | Reg loss: 0.026 | Tree loss: 0.871 | Accuracy: 0.636500 | 1.5 sec/iter\n",
      "Epoch: 299 | Batch: 001 / 011 | Total loss: 0.845 | Reg loss: 0.026 | Tree loss: 0.845 | Accuracy: 0.674000 | 1.5 sec/iter\n",
      "Epoch: 299 | Batch: 002 / 011 | Total loss: 0.864 | Reg loss: 0.026 | Tree loss: 0.864 | Accuracy: 0.645500 | 1.5 sec/iter\n",
      "Epoch: 299 | Batch: 003 / 011 | Total loss: 0.851 | Reg loss: 0.026 | Tree loss: 0.851 | Accuracy: 0.650000 | 1.5 sec/iter\n",
      "Epoch: 299 | Batch: 004 / 011 | Total loss: 0.816 | Reg loss: 0.026 | Tree loss: 0.816 | Accuracy: 0.664000 | 1.5 sec/iter\n",
      "Epoch: 299 | Batch: 005 / 011 | Total loss: 0.813 | Reg loss: 0.026 | Tree loss: 0.813 | Accuracy: 0.638500 | 1.5 sec/iter\n",
      "Epoch: 299 | Batch: 006 / 011 | Total loss: 0.823 | Reg loss: 0.026 | Tree loss: 0.823 | Accuracy: 0.619000 | 1.5 sec/iter\n",
      "Epoch: 299 | Batch: 007 / 011 | Total loss: 0.805 | Reg loss: 0.026 | Tree loss: 0.805 | Accuracy: 0.637000 | 1.5 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 299 | Batch: 008 / 011 | Total loss: 0.810 | Reg loss: 0.026 | Tree loss: 0.810 | Accuracy: 0.620500 | 1.5 sec/iter\n",
      "Epoch: 299 | Batch: 009 / 011 | Total loss: 0.818 | Reg loss: 0.026 | Tree loss: 0.818 | Accuracy: 0.624500 | 1.5 sec/iter\n",
      "Epoch: 299 | Batch: 010 / 011 | Total loss: 0.746 | Reg loss: 0.026 | Tree loss: 0.746 | Accuracy: 0.662116 | 1.5 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 300 | Batch: 000 / 011 | Total loss: 0.868 | Reg loss: 0.026 | Tree loss: 0.868 | Accuracy: 0.632000 | 1.501 sec/iter\n",
      "Epoch: 300 | Batch: 001 / 011 | Total loss: 0.843 | Reg loss: 0.026 | Tree loss: 0.843 | Accuracy: 0.657500 | 1.501 sec/iter\n",
      "Epoch: 300 | Batch: 002 / 011 | Total loss: 0.848 | Reg loss: 0.026 | Tree loss: 0.848 | Accuracy: 0.648500 | 1.5 sec/iter\n",
      "Epoch: 300 | Batch: 003 / 011 | Total loss: 0.862 | Reg loss: 0.026 | Tree loss: 0.862 | Accuracy: 0.643500 | 1.5 sec/iter\n",
      "Epoch: 300 | Batch: 004 / 011 | Total loss: 0.841 | Reg loss: 0.026 | Tree loss: 0.841 | Accuracy: 0.645500 | 1.5 sec/iter\n",
      "Epoch: 300 | Batch: 005 / 011 | Total loss: 0.815 | Reg loss: 0.026 | Tree loss: 0.815 | Accuracy: 0.662000 | 1.5 sec/iter\n",
      "Epoch: 300 | Batch: 006 / 011 | Total loss: 0.806 | Reg loss: 0.026 | Tree loss: 0.806 | Accuracy: 0.655500 | 1.5 sec/iter\n",
      "Epoch: 300 | Batch: 007 / 011 | Total loss: 0.821 | Reg loss: 0.026 | Tree loss: 0.821 | Accuracy: 0.642000 | 1.5 sec/iter\n",
      "Epoch: 300 | Batch: 008 / 011 | Total loss: 0.798 | Reg loss: 0.026 | Tree loss: 0.798 | Accuracy: 0.642000 | 1.499 sec/iter\n",
      "Epoch: 300 | Batch: 009 / 011 | Total loss: 0.795 | Reg loss: 0.026 | Tree loss: 0.795 | Accuracy: 0.621000 | 1.499 sec/iter\n",
      "Epoch: 300 | Batch: 010 / 011 | Total loss: 0.790 | Reg loss: 0.026 | Tree loss: 0.790 | Accuracy: 0.662116 | 1.499 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 301 | Batch: 000 / 011 | Total loss: 0.864 | Reg loss: 0.026 | Tree loss: 0.864 | Accuracy: 0.638500 | 1.5 sec/iter\n",
      "Epoch: 301 | Batch: 001 / 011 | Total loss: 0.862 | Reg loss: 0.026 | Tree loss: 0.862 | Accuracy: 0.653500 | 1.5 sec/iter\n",
      "Epoch: 301 | Batch: 002 / 011 | Total loss: 0.858 | Reg loss: 0.026 | Tree loss: 0.858 | Accuracy: 0.636000 | 1.5 sec/iter\n",
      "Epoch: 301 | Batch: 003 / 011 | Total loss: 0.854 | Reg loss: 0.026 | Tree loss: 0.854 | Accuracy: 0.647000 | 1.499 sec/iter\n",
      "Epoch: 301 | Batch: 004 / 011 | Total loss: 0.830 | Reg loss: 0.026 | Tree loss: 0.830 | Accuracy: 0.645500 | 1.499 sec/iter\n",
      "Epoch: 301 | Batch: 005 / 011 | Total loss: 0.799 | Reg loss: 0.026 | Tree loss: 0.799 | Accuracy: 0.668500 | 1.499 sec/iter\n",
      "Epoch: 301 | Batch: 006 / 011 | Total loss: 0.802 | Reg loss: 0.026 | Tree loss: 0.802 | Accuracy: 0.656000 | 1.499 sec/iter\n",
      "Epoch: 301 | Batch: 007 / 011 | Total loss: 0.825 | Reg loss: 0.026 | Tree loss: 0.825 | Accuracy: 0.612000 | 1.499 sec/iter\n",
      "Epoch: 301 | Batch: 008 / 011 | Total loss: 0.778 | Reg loss: 0.026 | Tree loss: 0.778 | Accuracy: 0.638000 | 1.499 sec/iter\n",
      "Epoch: 301 | Batch: 009 / 011 | Total loss: 0.802 | Reg loss: 0.026 | Tree loss: 0.802 | Accuracy: 0.630500 | 1.499 sec/iter\n",
      "Epoch: 301 | Batch: 010 / 011 | Total loss: 0.850 | Reg loss: 0.026 | Tree loss: 0.850 | Accuracy: 0.610922 | 1.499 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 302 | Batch: 000 / 011 | Total loss: 0.898 | Reg loss: 0.026 | Tree loss: 0.898 | Accuracy: 0.615000 | 1.5 sec/iter\n",
      "Epoch: 302 | Batch: 001 / 011 | Total loss: 0.840 | Reg loss: 0.026 | Tree loss: 0.840 | Accuracy: 0.667000 | 1.5 sec/iter\n",
      "Epoch: 302 | Batch: 002 / 011 | Total loss: 0.845 | Reg loss: 0.026 | Tree loss: 0.845 | Accuracy: 0.654500 | 1.5 sec/iter\n",
      "Epoch: 302 | Batch: 003 / 011 | Total loss: 0.833 | Reg loss: 0.026 | Tree loss: 0.833 | Accuracy: 0.666500 | 1.5 sec/iter\n",
      "Epoch: 302 | Batch: 004 / 011 | Total loss: 0.835 | Reg loss: 0.026 | Tree loss: 0.835 | Accuracy: 0.643000 | 1.5 sec/iter\n",
      "Epoch: 302 | Batch: 005 / 011 | Total loss: 0.814 | Reg loss: 0.026 | Tree loss: 0.814 | Accuracy: 0.650000 | 1.5 sec/iter\n",
      "Epoch: 302 | Batch: 006 / 011 | Total loss: 0.807 | Reg loss: 0.026 | Tree loss: 0.807 | Accuracy: 0.627500 | 1.5 sec/iter\n",
      "Epoch: 302 | Batch: 007 / 011 | Total loss: 0.816 | Reg loss: 0.026 | Tree loss: 0.816 | Accuracy: 0.630500 | 1.5 sec/iter\n",
      "Epoch: 302 | Batch: 008 / 011 | Total loss: 0.795 | Reg loss: 0.026 | Tree loss: 0.795 | Accuracy: 0.624500 | 1.5 sec/iter\n",
      "Epoch: 302 | Batch: 009 / 011 | Total loss: 0.793 | Reg loss: 0.026 | Tree loss: 0.793 | Accuracy: 0.647000 | 1.5 sec/iter\n",
      "Epoch: 302 | Batch: 010 / 011 | Total loss: 0.840 | Reg loss: 0.026 | Tree loss: 0.840 | Accuracy: 0.607509 | 1.499 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 303 | Batch: 000 / 011 | Total loss: 0.877 | Reg loss: 0.026 | Tree loss: 0.877 | Accuracy: 0.640000 | 1.5 sec/iter\n",
      "Epoch: 303 | Batch: 001 / 011 | Total loss: 0.845 | Reg loss: 0.026 | Tree loss: 0.845 | Accuracy: 0.658500 | 1.5 sec/iter\n",
      "Epoch: 303 | Batch: 002 / 011 | Total loss: 0.881 | Reg loss: 0.026 | Tree loss: 0.881 | Accuracy: 0.632500 | 1.5 sec/iter\n",
      "Epoch: 303 | Batch: 003 / 011 | Total loss: 0.841 | Reg loss: 0.026 | Tree loss: 0.841 | Accuracy: 0.653000 | 1.5 sec/iter\n",
      "Epoch: 303 | Batch: 004 / 011 | Total loss: 0.824 | Reg loss: 0.026 | Tree loss: 0.824 | Accuracy: 0.655500 | 1.5 sec/iter\n",
      "Epoch: 303 | Batch: 005 / 011 | Total loss: 0.813 | Reg loss: 0.026 | Tree loss: 0.813 | Accuracy: 0.649500 | 1.5 sec/iter\n",
      "Epoch: 303 | Batch: 006 / 011 | Total loss: 0.783 | Reg loss: 0.026 | Tree loss: 0.783 | Accuracy: 0.644500 | 1.499 sec/iter\n",
      "Epoch: 303 | Batch: 007 / 011 | Total loss: 0.777 | Reg loss: 0.026 | Tree loss: 0.777 | Accuracy: 0.647000 | 1.499 sec/iter\n",
      "Epoch: 303 | Batch: 008 / 011 | Total loss: 0.815 | Reg loss: 0.026 | Tree loss: 0.815 | Accuracy: 0.626500 | 1.499 sec/iter\n",
      "Epoch: 303 | Batch: 009 / 011 | Total loss: 0.819 | Reg loss: 0.026 | Tree loss: 0.819 | Accuracy: 0.610500 | 1.499 sec/iter\n",
      "Epoch: 303 | Batch: 010 / 011 | Total loss: 0.832 | Reg loss: 0.026 | Tree loss: 0.832 | Accuracy: 0.580205 | 1.499 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 304 | Batch: 000 / 011 | Total loss: 0.871 | Reg loss: 0.026 | Tree loss: 0.871 | Accuracy: 0.637000 | 1.5 sec/iter\n",
      "Epoch: 304 | Batch: 001 / 011 | Total loss: 0.858 | Reg loss: 0.026 | Tree loss: 0.858 | Accuracy: 0.660500 | 1.5 sec/iter\n",
      "Epoch: 304 | Batch: 002 / 011 | Total loss: 0.855 | Reg loss: 0.026 | Tree loss: 0.855 | Accuracy: 0.647000 | 1.5 sec/iter\n",
      "Epoch: 304 | Batch: 003 / 011 | Total loss: 0.846 | Reg loss: 0.026 | Tree loss: 0.846 | Accuracy: 0.643500 | 1.5 sec/iter\n",
      "Epoch: 304 | Batch: 004 / 011 | Total loss: 0.839 | Reg loss: 0.026 | Tree loss: 0.839 | Accuracy: 0.642500 | 1.5 sec/iter\n",
      "Epoch: 304 | Batch: 005 / 011 | Total loss: 0.830 | Reg loss: 0.026 | Tree loss: 0.830 | Accuracy: 0.635000 | 1.5 sec/iter\n",
      "Epoch: 304 | Batch: 006 / 011 | Total loss: 0.802 | Reg loss: 0.026 | Tree loss: 0.802 | Accuracy: 0.666000 | 1.5 sec/iter\n",
      "Epoch: 304 | Batch: 007 / 011 | Total loss: 0.804 | Reg loss: 0.026 | Tree loss: 0.804 | Accuracy: 0.639500 | 1.5 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 304 | Batch: 008 / 011 | Total loss: 0.793 | Reg loss: 0.026 | Tree loss: 0.793 | Accuracy: 0.645000 | 1.499 sec/iter\n",
      "Epoch: 304 | Batch: 009 / 011 | Total loss: 0.813 | Reg loss: 0.026 | Tree loss: 0.813 | Accuracy: 0.624000 | 1.499 sec/iter\n",
      "Epoch: 304 | Batch: 010 / 011 | Total loss: 0.804 | Reg loss: 0.026 | Tree loss: 0.804 | Accuracy: 0.621160 | 1.499 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 305 | Batch: 000 / 011 | Total loss: 0.874 | Reg loss: 0.026 | Tree loss: 0.874 | Accuracy: 0.652500 | 1.501 sec/iter\n",
      "Epoch: 305 | Batch: 001 / 011 | Total loss: 0.855 | Reg loss: 0.026 | Tree loss: 0.855 | Accuracy: 0.651000 | 1.5 sec/iter\n",
      "Epoch: 305 | Batch: 002 / 011 | Total loss: 0.835 | Reg loss: 0.026 | Tree loss: 0.835 | Accuracy: 0.665000 | 1.5 sec/iter\n",
      "Epoch: 305 | Batch: 003 / 011 | Total loss: 0.852 | Reg loss: 0.026 | Tree loss: 0.852 | Accuracy: 0.639000 | 1.5 sec/iter\n",
      "Epoch: 305 | Batch: 004 / 011 | Total loss: 0.830 | Reg loss: 0.026 | Tree loss: 0.830 | Accuracy: 0.648000 | 1.5 sec/iter\n",
      "Epoch: 305 | Batch: 005 / 011 | Total loss: 0.838 | Reg loss: 0.026 | Tree loss: 0.838 | Accuracy: 0.619500 | 1.5 sec/iter\n",
      "Epoch: 305 | Batch: 006 / 011 | Total loss: 0.810 | Reg loss: 0.026 | Tree loss: 0.810 | Accuracy: 0.630000 | 1.5 sec/iter\n",
      "Epoch: 305 | Batch: 007 / 011 | Total loss: 0.822 | Reg loss: 0.026 | Tree loss: 0.822 | Accuracy: 0.613500 | 1.5 sec/iter\n",
      "Epoch: 305 | Batch: 008 / 011 | Total loss: 0.799 | Reg loss: 0.026 | Tree loss: 0.799 | Accuracy: 0.633000 | 1.5 sec/iter\n",
      "Epoch: 305 | Batch: 009 / 011 | Total loss: 0.797 | Reg loss: 0.026 | Tree loss: 0.797 | Accuracy: 0.631500 | 1.5 sec/iter\n",
      "Epoch: 305 | Batch: 010 / 011 | Total loss: 0.765 | Reg loss: 0.026 | Tree loss: 0.765 | Accuracy: 0.675768 | 1.5 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 306 | Batch: 000 / 011 | Total loss: 0.872 | Reg loss: 0.026 | Tree loss: 0.872 | Accuracy: 0.634000 | 1.501 sec/iter\n",
      "Epoch: 306 | Batch: 001 / 011 | Total loss: 0.875 | Reg loss: 0.026 | Tree loss: 0.875 | Accuracy: 0.640000 | 1.501 sec/iter\n",
      "Epoch: 306 | Batch: 002 / 011 | Total loss: 0.850 | Reg loss: 0.026 | Tree loss: 0.850 | Accuracy: 0.662000 | 1.5 sec/iter\n",
      "Epoch: 306 | Batch: 003 / 011 | Total loss: 0.853 | Reg loss: 0.026 | Tree loss: 0.853 | Accuracy: 0.652000 | 1.5 sec/iter\n",
      "Epoch: 306 | Batch: 004 / 011 | Total loss: 0.823 | Reg loss: 0.026 | Tree loss: 0.823 | Accuracy: 0.644000 | 1.5 sec/iter\n",
      "Epoch: 306 | Batch: 005 / 011 | Total loss: 0.823 | Reg loss: 0.026 | Tree loss: 0.823 | Accuracy: 0.665000 | 1.5 sec/iter\n",
      "Epoch: 306 | Batch: 006 / 011 | Total loss: 0.807 | Reg loss: 0.026 | Tree loss: 0.807 | Accuracy: 0.651500 | 1.5 sec/iter\n",
      "Epoch: 306 | Batch: 007 / 011 | Total loss: 0.811 | Reg loss: 0.026 | Tree loss: 0.811 | Accuracy: 0.624000 | 1.5 sec/iter\n",
      "Epoch: 306 | Batch: 008 / 011 | Total loss: 0.788 | Reg loss: 0.026 | Tree loss: 0.788 | Accuracy: 0.634500 | 1.5 sec/iter\n",
      "Epoch: 306 | Batch: 009 / 011 | Total loss: 0.790 | Reg loss: 0.026 | Tree loss: 0.790 | Accuracy: 0.638000 | 1.5 sec/iter\n",
      "Epoch: 306 | Batch: 010 / 011 | Total loss: 0.743 | Reg loss: 0.026 | Tree loss: 0.743 | Accuracy: 0.686007 | 1.499 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 307 | Batch: 000 / 011 | Total loss: 0.876 | Reg loss: 0.026 | Tree loss: 0.876 | Accuracy: 0.631500 | 1.5 sec/iter\n",
      "Epoch: 307 | Batch: 001 / 011 | Total loss: 0.888 | Reg loss: 0.026 | Tree loss: 0.888 | Accuracy: 0.640000 | 1.5 sec/iter\n",
      "Epoch: 307 | Batch: 002 / 011 | Total loss: 0.855 | Reg loss: 0.026 | Tree loss: 0.855 | Accuracy: 0.651000 | 1.5 sec/iter\n",
      "Epoch: 307 | Batch: 003 / 011 | Total loss: 0.835 | Reg loss: 0.026 | Tree loss: 0.835 | Accuracy: 0.650500 | 1.5 sec/iter\n",
      "Epoch: 307 | Batch: 004 / 011 | Total loss: 0.803 | Reg loss: 0.026 | Tree loss: 0.803 | Accuracy: 0.672000 | 1.5 sec/iter\n",
      "Epoch: 307 | Batch: 005 / 011 | Total loss: 0.817 | Reg loss: 0.026 | Tree loss: 0.817 | Accuracy: 0.628000 | 1.5 sec/iter\n",
      "Epoch: 307 | Batch: 006 / 011 | Total loss: 0.784 | Reg loss: 0.026 | Tree loss: 0.784 | Accuracy: 0.649500 | 1.5 sec/iter\n",
      "Epoch: 307 | Batch: 007 / 011 | Total loss: 0.807 | Reg loss: 0.026 | Tree loss: 0.807 | Accuracy: 0.635000 | 1.5 sec/iter\n",
      "Epoch: 307 | Batch: 008 / 011 | Total loss: 0.811 | Reg loss: 0.026 | Tree loss: 0.811 | Accuracy: 0.632000 | 1.5 sec/iter\n",
      "Epoch: 307 | Batch: 009 / 011 | Total loss: 0.799 | Reg loss: 0.026 | Tree loss: 0.799 | Accuracy: 0.633500 | 1.5 sec/iter\n",
      "Epoch: 307 | Batch: 010 / 011 | Total loss: 0.797 | Reg loss: 0.026 | Tree loss: 0.797 | Accuracy: 0.610922 | 1.499 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 308 | Batch: 000 / 011 | Total loss: 0.892 | Reg loss: 0.026 | Tree loss: 0.892 | Accuracy: 0.618500 | 1.5 sec/iter\n",
      "Epoch: 308 | Batch: 001 / 011 | Total loss: 0.870 | Reg loss: 0.026 | Tree loss: 0.870 | Accuracy: 0.632000 | 1.5 sec/iter\n",
      "Epoch: 308 | Batch: 002 / 011 | Total loss: 0.878 | Reg loss: 0.026 | Tree loss: 0.878 | Accuracy: 0.640000 | 1.5 sec/iter\n",
      "Epoch: 308 | Batch: 003 / 011 | Total loss: 0.824 | Reg loss: 0.026 | Tree loss: 0.824 | Accuracy: 0.657500 | 1.5 sec/iter\n",
      "Epoch: 308 | Batch: 004 / 011 | Total loss: 0.800 | Reg loss: 0.026 | Tree loss: 0.800 | Accuracy: 0.665000 | 1.5 sec/iter\n",
      "Epoch: 308 | Batch: 005 / 011 | Total loss: 0.822 | Reg loss: 0.026 | Tree loss: 0.822 | Accuracy: 0.640000 | 1.499 sec/iter\n",
      "Epoch: 308 | Batch: 006 / 011 | Total loss: 0.821 | Reg loss: 0.026 | Tree loss: 0.821 | Accuracy: 0.644500 | 1.499 sec/iter\n",
      "Epoch: 308 | Batch: 007 / 011 | Total loss: 0.778 | Reg loss: 0.026 | Tree loss: 0.778 | Accuracy: 0.647500 | 1.499 sec/iter\n",
      "Epoch: 308 | Batch: 008 / 011 | Total loss: 0.810 | Reg loss: 0.026 | Tree loss: 0.810 | Accuracy: 0.629500 | 1.499 sec/iter\n",
      "Epoch: 308 | Batch: 009 / 011 | Total loss: 0.803 | Reg loss: 0.026 | Tree loss: 0.803 | Accuracy: 0.641000 | 1.499 sec/iter\n",
      "Epoch: 308 | Batch: 010 / 011 | Total loss: 0.850 | Reg loss: 0.026 | Tree loss: 0.850 | Accuracy: 0.607509 | 1.499 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 309 | Batch: 000 / 011 | Total loss: 0.873 | Reg loss: 0.026 | Tree loss: 0.873 | Accuracy: 0.642500 | 1.5 sec/iter\n",
      "Epoch: 309 | Batch: 001 / 011 | Total loss: 0.864 | Reg loss: 0.026 | Tree loss: 0.864 | Accuracy: 0.649500 | 1.5 sec/iter\n",
      "Epoch: 309 | Batch: 002 / 011 | Total loss: 0.843 | Reg loss: 0.026 | Tree loss: 0.843 | Accuracy: 0.653500 | 1.5 sec/iter\n",
      "Epoch: 309 | Batch: 003 / 011 | Total loss: 0.823 | Reg loss: 0.026 | Tree loss: 0.823 | Accuracy: 0.674000 | 1.5 sec/iter\n",
      "Epoch: 309 | Batch: 004 / 011 | Total loss: 0.848 | Reg loss: 0.026 | Tree loss: 0.848 | Accuracy: 0.648500 | 1.5 sec/iter\n",
      "Epoch: 309 | Batch: 005 / 011 | Total loss: 0.823 | Reg loss: 0.026 | Tree loss: 0.823 | Accuracy: 0.631500 | 1.5 sec/iter\n",
      "Epoch: 309 | Batch: 006 / 011 | Total loss: 0.803 | Reg loss: 0.026 | Tree loss: 0.803 | Accuracy: 0.638000 | 1.5 sec/iter\n",
      "Epoch: 309 | Batch: 007 / 011 | Total loss: 0.804 | Reg loss: 0.026 | Tree loss: 0.804 | Accuracy: 0.625500 | 1.499 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 309 | Batch: 008 / 011 | Total loss: 0.802 | Reg loss: 0.026 | Tree loss: 0.802 | Accuracy: 0.619500 | 1.499 sec/iter\n",
      "Epoch: 309 | Batch: 009 / 011 | Total loss: 0.803 | Reg loss: 0.026 | Tree loss: 0.803 | Accuracy: 0.633500 | 1.499 sec/iter\n",
      "Epoch: 309 | Batch: 010 / 011 | Total loss: 0.819 | Reg loss: 0.026 | Tree loss: 0.819 | Accuracy: 0.614334 | 1.499 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 310 | Batch: 000 / 011 | Total loss: 0.872 | Reg loss: 0.026 | Tree loss: 0.872 | Accuracy: 0.631500 | 1.5 sec/iter\n",
      "Epoch: 310 | Batch: 001 / 011 | Total loss: 0.869 | Reg loss: 0.026 | Tree loss: 0.869 | Accuracy: 0.645500 | 1.5 sec/iter\n",
      "Epoch: 310 | Batch: 002 / 011 | Total loss: 0.849 | Reg loss: 0.026 | Tree loss: 0.849 | Accuracy: 0.662500 | 1.5 sec/iter\n",
      "Epoch: 310 | Batch: 003 / 011 | Total loss: 0.843 | Reg loss: 0.026 | Tree loss: 0.843 | Accuracy: 0.645500 | 1.5 sec/iter\n",
      "Epoch: 310 | Batch: 004 / 011 | Total loss: 0.832 | Reg loss: 0.026 | Tree loss: 0.832 | Accuracy: 0.641000 | 1.5 sec/iter\n",
      "Epoch: 310 | Batch: 005 / 011 | Total loss: 0.812 | Reg loss: 0.026 | Tree loss: 0.812 | Accuracy: 0.662000 | 1.5 sec/iter\n",
      "Epoch: 310 | Batch: 006 / 011 | Total loss: 0.784 | Reg loss: 0.026 | Tree loss: 0.784 | Accuracy: 0.669500 | 1.5 sec/iter\n",
      "Epoch: 310 | Batch: 007 / 011 | Total loss: 0.808 | Reg loss: 0.026 | Tree loss: 0.808 | Accuracy: 0.628000 | 1.5 sec/iter\n",
      "Epoch: 310 | Batch: 008 / 011 | Total loss: 0.812 | Reg loss: 0.026 | Tree loss: 0.812 | Accuracy: 0.615000 | 1.5 sec/iter\n",
      "Epoch: 310 | Batch: 009 / 011 | Total loss: 0.792 | Reg loss: 0.026 | Tree loss: 0.792 | Accuracy: 0.635500 | 1.5 sec/iter\n",
      "Epoch: 310 | Batch: 010 / 011 | Total loss: 0.796 | Reg loss: 0.026 | Tree loss: 0.796 | Accuracy: 0.627986 | 1.499 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 311 | Batch: 000 / 011 | Total loss: 0.880 | Reg loss: 0.026 | Tree loss: 0.880 | Accuracy: 0.628000 | 1.501 sec/iter\n",
      "Epoch: 311 | Batch: 001 / 011 | Total loss: 0.854 | Reg loss: 0.026 | Tree loss: 0.854 | Accuracy: 0.658500 | 1.501 sec/iter\n",
      "Epoch: 311 | Batch: 002 / 011 | Total loss: 0.864 | Reg loss: 0.026 | Tree loss: 0.864 | Accuracy: 0.640500 | 1.5 sec/iter\n",
      "Epoch: 311 | Batch: 003 / 011 | Total loss: 0.841 | Reg loss: 0.026 | Tree loss: 0.841 | Accuracy: 0.651500 | 1.5 sec/iter\n",
      "Epoch: 311 | Batch: 004 / 011 | Total loss: 0.816 | Reg loss: 0.026 | Tree loss: 0.816 | Accuracy: 0.656000 | 1.5 sec/iter\n",
      "Epoch: 311 | Batch: 005 / 011 | Total loss: 0.818 | Reg loss: 0.026 | Tree loss: 0.818 | Accuracy: 0.659000 | 1.5 sec/iter\n",
      "Epoch: 311 | Batch: 006 / 011 | Total loss: 0.819 | Reg loss: 0.026 | Tree loss: 0.819 | Accuracy: 0.640000 | 1.5 sec/iter\n",
      "Epoch: 311 | Batch: 007 / 011 | Total loss: 0.814 | Reg loss: 0.026 | Tree loss: 0.814 | Accuracy: 0.628500 | 1.5 sec/iter\n",
      "Epoch: 311 | Batch: 008 / 011 | Total loss: 0.796 | Reg loss: 0.026 | Tree loss: 0.796 | Accuracy: 0.625000 | 1.5 sec/iter\n",
      "Epoch: 311 | Batch: 009 / 011 | Total loss: 0.781 | Reg loss: 0.026 | Tree loss: 0.781 | Accuracy: 0.633500 | 1.5 sec/iter\n",
      "Epoch: 311 | Batch: 010 / 011 | Total loss: 0.769 | Reg loss: 0.026 | Tree loss: 0.769 | Accuracy: 0.631399 | 1.5 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 312 | Batch: 000 / 011 | Total loss: 0.873 | Reg loss: 0.026 | Tree loss: 0.873 | Accuracy: 0.644500 | 1.501 sec/iter\n",
      "Epoch: 312 | Batch: 001 / 011 | Total loss: 0.881 | Reg loss: 0.026 | Tree loss: 0.881 | Accuracy: 0.635000 | 1.501 sec/iter\n",
      "Epoch: 312 | Batch: 002 / 011 | Total loss: 0.857 | Reg loss: 0.026 | Tree loss: 0.857 | Accuracy: 0.647000 | 1.501 sec/iter\n",
      "Epoch: 312 | Batch: 003 / 011 | Total loss: 0.828 | Reg loss: 0.026 | Tree loss: 0.828 | Accuracy: 0.647500 | 1.5 sec/iter\n",
      "Epoch: 312 | Batch: 004 / 011 | Total loss: 0.828 | Reg loss: 0.026 | Tree loss: 0.828 | Accuracy: 0.645500 | 1.5 sec/iter\n",
      "Epoch: 312 | Batch: 005 / 011 | Total loss: 0.833 | Reg loss: 0.026 | Tree loss: 0.833 | Accuracy: 0.632500 | 1.5 sec/iter\n",
      "Epoch: 312 | Batch: 006 / 011 | Total loss: 0.794 | Reg loss: 0.026 | Tree loss: 0.794 | Accuracy: 0.645500 | 1.5 sec/iter\n",
      "Epoch: 312 | Batch: 007 / 011 | Total loss: 0.788 | Reg loss: 0.026 | Tree loss: 0.788 | Accuracy: 0.639500 | 1.5 sec/iter\n",
      "Epoch: 312 | Batch: 008 / 011 | Total loss: 0.793 | Reg loss: 0.026 | Tree loss: 0.793 | Accuracy: 0.643000 | 1.5 sec/iter\n",
      "Epoch: 312 | Batch: 009 / 011 | Total loss: 0.802 | Reg loss: 0.026 | Tree loss: 0.802 | Accuracy: 0.624000 | 1.5 sec/iter\n",
      "Epoch: 312 | Batch: 010 / 011 | Total loss: 0.841 | Reg loss: 0.026 | Tree loss: 0.841 | Accuracy: 0.607509 | 1.5 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 313 | Batch: 000 / 011 | Total loss: 0.845 | Reg loss: 0.026 | Tree loss: 0.845 | Accuracy: 0.657000 | 1.501 sec/iter\n",
      "Epoch: 313 | Batch: 001 / 011 | Total loss: 0.868 | Reg loss: 0.026 | Tree loss: 0.868 | Accuracy: 0.645500 | 1.501 sec/iter\n",
      "Epoch: 313 | Batch: 002 / 011 | Total loss: 0.863 | Reg loss: 0.026 | Tree loss: 0.863 | Accuracy: 0.642000 | 1.501 sec/iter\n",
      "Epoch: 313 | Batch: 003 / 011 | Total loss: 0.825 | Reg loss: 0.026 | Tree loss: 0.825 | Accuracy: 0.668500 | 1.5 sec/iter\n",
      "Epoch: 313 | Batch: 004 / 011 | Total loss: 0.847 | Reg loss: 0.026 | Tree loss: 0.847 | Accuracy: 0.634000 | 1.5 sec/iter\n",
      "Epoch: 313 | Batch: 005 / 011 | Total loss: 0.824 | Reg loss: 0.026 | Tree loss: 0.824 | Accuracy: 0.650500 | 1.5 sec/iter\n",
      "Epoch: 313 | Batch: 006 / 011 | Total loss: 0.815 | Reg loss: 0.026 | Tree loss: 0.815 | Accuracy: 0.642000 | 1.5 sec/iter\n",
      "Epoch: 313 | Batch: 007 / 011 | Total loss: 0.816 | Reg loss: 0.026 | Tree loss: 0.816 | Accuracy: 0.636000 | 1.5 sec/iter\n",
      "Epoch: 313 | Batch: 008 / 011 | Total loss: 0.804 | Reg loss: 0.026 | Tree loss: 0.804 | Accuracy: 0.627500 | 1.5 sec/iter\n",
      "Epoch: 313 | Batch: 009 / 011 | Total loss: 0.773 | Reg loss: 0.026 | Tree loss: 0.773 | Accuracy: 0.652000 | 1.5 sec/iter\n",
      "Epoch: 313 | Batch: 010 / 011 | Total loss: 0.774 | Reg loss: 0.026 | Tree loss: 0.774 | Accuracy: 0.624573 | 1.5 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 314 | Batch: 000 / 011 | Total loss: 0.880 | Reg loss: 0.026 | Tree loss: 0.880 | Accuracy: 0.639500 | 1.501 sec/iter\n",
      "Epoch: 314 | Batch: 001 / 011 | Total loss: 0.847 | Reg loss: 0.026 | Tree loss: 0.847 | Accuracy: 0.660000 | 1.501 sec/iter\n",
      "Epoch: 314 | Batch: 002 / 011 | Total loss: 0.859 | Reg loss: 0.026 | Tree loss: 0.859 | Accuracy: 0.627500 | 1.501 sec/iter\n",
      "Epoch: 314 | Batch: 003 / 011 | Total loss: 0.853 | Reg loss: 0.026 | Tree loss: 0.853 | Accuracy: 0.642000 | 1.501 sec/iter\n",
      "Epoch: 314 | Batch: 004 / 011 | Total loss: 0.844 | Reg loss: 0.026 | Tree loss: 0.844 | Accuracy: 0.645500 | 1.501 sec/iter\n",
      "Epoch: 314 | Batch: 005 / 011 | Total loss: 0.819 | Reg loss: 0.026 | Tree loss: 0.819 | Accuracy: 0.642000 | 1.501 sec/iter\n",
      "Epoch: 314 | Batch: 006 / 011 | Total loss: 0.796 | Reg loss: 0.026 | Tree loss: 0.796 | Accuracy: 0.637500 | 1.501 sec/iter\n",
      "Epoch: 314 | Batch: 007 / 011 | Total loss: 0.785 | Reg loss: 0.026 | Tree loss: 0.785 | Accuracy: 0.630000 | 1.501 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 314 | Batch: 008 / 011 | Total loss: 0.800 | Reg loss: 0.026 | Tree loss: 0.800 | Accuracy: 0.640000 | 1.501 sec/iter\n",
      "Epoch: 314 | Batch: 009 / 011 | Total loss: 0.800 | Reg loss: 0.026 | Tree loss: 0.800 | Accuracy: 0.637000 | 1.501 sec/iter\n",
      "Epoch: 314 | Batch: 010 / 011 | Total loss: 0.784 | Reg loss: 0.026 | Tree loss: 0.784 | Accuracy: 0.665529 | 1.501 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 315 | Batch: 000 / 011 | Total loss: 0.853 | Reg loss: 0.026 | Tree loss: 0.853 | Accuracy: 0.664500 | 1.502 sec/iter\n",
      "Epoch: 315 | Batch: 001 / 011 | Total loss: 0.872 | Reg loss: 0.026 | Tree loss: 0.872 | Accuracy: 0.637500 | 1.502 sec/iter\n",
      "Epoch: 315 | Batch: 002 / 011 | Total loss: 0.838 | Reg loss: 0.026 | Tree loss: 0.838 | Accuracy: 0.659000 | 1.502 sec/iter\n",
      "Epoch: 315 | Batch: 003 / 011 | Total loss: 0.851 | Reg loss: 0.026 | Tree loss: 0.851 | Accuracy: 0.649000 | 1.501 sec/iter\n",
      "Epoch: 315 | Batch: 004 / 011 | Total loss: 0.814 | Reg loss: 0.026 | Tree loss: 0.814 | Accuracy: 0.654500 | 1.501 sec/iter\n",
      "Epoch: 315 | Batch: 005 / 011 | Total loss: 0.831 | Reg loss: 0.026 | Tree loss: 0.831 | Accuracy: 0.638000 | 1.501 sec/iter\n",
      "Epoch: 315 | Batch: 006 / 011 | Total loss: 0.846 | Reg loss: 0.026 | Tree loss: 0.846 | Accuracy: 0.609000 | 1.501 sec/iter\n",
      "Epoch: 315 | Batch: 007 / 011 | Total loss: 0.787 | Reg loss: 0.026 | Tree loss: 0.787 | Accuracy: 0.639000 | 1.501 sec/iter\n",
      "Epoch: 315 | Batch: 008 / 011 | Total loss: 0.794 | Reg loss: 0.026 | Tree loss: 0.794 | Accuracy: 0.633500 | 1.501 sec/iter\n",
      "Epoch: 315 | Batch: 009 / 011 | Total loss: 0.777 | Reg loss: 0.026 | Tree loss: 0.777 | Accuracy: 0.647000 | 1.501 sec/iter\n",
      "Epoch: 315 | Batch: 010 / 011 | Total loss: 0.868 | Reg loss: 0.026 | Tree loss: 0.868 | Accuracy: 0.631399 | 1.501 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 316 | Batch: 000 / 011 | Total loss: 0.887 | Reg loss: 0.026 | Tree loss: 0.887 | Accuracy: 0.633000 | 1.502 sec/iter\n",
      "Epoch: 316 | Batch: 001 / 011 | Total loss: 0.852 | Reg loss: 0.026 | Tree loss: 0.852 | Accuracy: 0.650000 | 1.502 sec/iter\n",
      "Epoch: 316 | Batch: 002 / 011 | Total loss: 0.838 | Reg loss: 0.026 | Tree loss: 0.838 | Accuracy: 0.670500 | 1.502 sec/iter\n",
      "Epoch: 316 | Batch: 003 / 011 | Total loss: 0.839 | Reg loss: 0.026 | Tree loss: 0.839 | Accuracy: 0.652500 | 1.502 sec/iter\n",
      "Epoch: 316 | Batch: 004 / 011 | Total loss: 0.818 | Reg loss: 0.026 | Tree loss: 0.818 | Accuracy: 0.658500 | 1.502 sec/iter\n",
      "Epoch: 316 | Batch: 005 / 011 | Total loss: 0.836 | Reg loss: 0.026 | Tree loss: 0.836 | Accuracy: 0.649000 | 1.501 sec/iter\n",
      "Epoch: 316 | Batch: 006 / 011 | Total loss: 0.801 | Reg loss: 0.026 | Tree loss: 0.801 | Accuracy: 0.654000 | 1.501 sec/iter\n",
      "Epoch: 316 | Batch: 007 / 011 | Total loss: 0.797 | Reg loss: 0.026 | Tree loss: 0.797 | Accuracy: 0.634000 | 1.501 sec/iter\n",
      "Epoch: 316 | Batch: 008 / 011 | Total loss: 0.803 | Reg loss: 0.026 | Tree loss: 0.803 | Accuracy: 0.620500 | 1.501 sec/iter\n",
      "Epoch: 316 | Batch: 009 / 011 | Total loss: 0.819 | Reg loss: 0.026 | Tree loss: 0.819 | Accuracy: 0.620000 | 1.501 sec/iter\n",
      "Epoch: 316 | Batch: 010 / 011 | Total loss: 0.803 | Reg loss: 0.026 | Tree loss: 0.803 | Accuracy: 0.641638 | 1.501 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 317 | Batch: 000 / 011 | Total loss: 0.871 | Reg loss: 0.026 | Tree loss: 0.871 | Accuracy: 0.646500 | 1.502 sec/iter\n",
      "Epoch: 317 | Batch: 001 / 011 | Total loss: 0.844 | Reg loss: 0.026 | Tree loss: 0.844 | Accuracy: 0.667500 | 1.502 sec/iter\n",
      "Epoch: 317 | Batch: 002 / 011 | Total loss: 0.838 | Reg loss: 0.026 | Tree loss: 0.838 | Accuracy: 0.664500 | 1.502 sec/iter\n",
      "Epoch: 317 | Batch: 003 / 011 | Total loss: 0.861 | Reg loss: 0.026 | Tree loss: 0.861 | Accuracy: 0.652000 | 1.502 sec/iter\n",
      "Epoch: 317 | Batch: 004 / 011 | Total loss: 0.841 | Reg loss: 0.026 | Tree loss: 0.841 | Accuracy: 0.647500 | 1.502 sec/iter\n",
      "Epoch: 317 | Batch: 005 / 011 | Total loss: 0.817 | Reg loss: 0.026 | Tree loss: 0.817 | Accuracy: 0.645500 | 1.502 sec/iter\n",
      "Epoch: 317 | Batch: 006 / 011 | Total loss: 0.819 | Reg loss: 0.026 | Tree loss: 0.819 | Accuracy: 0.639500 | 1.501 sec/iter\n",
      "Epoch: 317 | Batch: 007 / 011 | Total loss: 0.815 | Reg loss: 0.026 | Tree loss: 0.815 | Accuracy: 0.626500 | 1.501 sec/iter\n",
      "Epoch: 317 | Batch: 008 / 011 | Total loss: 0.792 | Reg loss: 0.026 | Tree loss: 0.792 | Accuracy: 0.637500 | 1.501 sec/iter\n",
      "Epoch: 317 | Batch: 009 / 011 | Total loss: 0.813 | Reg loss: 0.026 | Tree loss: 0.813 | Accuracy: 0.628000 | 1.501 sec/iter\n",
      "Epoch: 317 | Batch: 010 / 011 | Total loss: 0.773 | Reg loss: 0.026 | Tree loss: 0.773 | Accuracy: 0.662116 | 1.501 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 318 | Batch: 000 / 011 | Total loss: 0.861 | Reg loss: 0.026 | Tree loss: 0.861 | Accuracy: 0.645500 | 1.501 sec/iter\n",
      "Epoch: 318 | Batch: 001 / 011 | Total loss: 0.878 | Reg loss: 0.026 | Tree loss: 0.878 | Accuracy: 0.620500 | 1.501 sec/iter\n",
      "Epoch: 318 | Batch: 002 / 011 | Total loss: 0.853 | Reg loss: 0.026 | Tree loss: 0.853 | Accuracy: 0.656000 | 1.501 sec/iter\n",
      "Epoch: 318 | Batch: 003 / 011 | Total loss: 0.837 | Reg loss: 0.026 | Tree loss: 0.837 | Accuracy: 0.652500 | 1.501 sec/iter\n",
      "Epoch: 318 | Batch: 004 / 011 | Total loss: 0.822 | Reg loss: 0.026 | Tree loss: 0.822 | Accuracy: 0.655000 | 1.501 sec/iter\n",
      "Epoch: 318 | Batch: 005 / 011 | Total loss: 0.815 | Reg loss: 0.026 | Tree loss: 0.815 | Accuracy: 0.654500 | 1.501 sec/iter\n",
      "Epoch: 318 | Batch: 006 / 011 | Total loss: 0.801 | Reg loss: 0.026 | Tree loss: 0.801 | Accuracy: 0.628500 | 1.5 sec/iter\n",
      "Epoch: 318 | Batch: 007 / 011 | Total loss: 0.802 | Reg loss: 0.026 | Tree loss: 0.802 | Accuracy: 0.626500 | 1.5 sec/iter\n",
      "Epoch: 318 | Batch: 008 / 011 | Total loss: 0.789 | Reg loss: 0.026 | Tree loss: 0.789 | Accuracy: 0.629000 | 1.5 sec/iter\n",
      "Epoch: 318 | Batch: 009 / 011 | Total loss: 0.810 | Reg loss: 0.026 | Tree loss: 0.810 | Accuracy: 0.637000 | 1.5 sec/iter\n",
      "Epoch: 318 | Batch: 010 / 011 | Total loss: 0.765 | Reg loss: 0.026 | Tree loss: 0.765 | Accuracy: 0.696246 | 1.5 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 319 | Batch: 000 / 011 | Total loss: 0.866 | Reg loss: 0.026 | Tree loss: 0.866 | Accuracy: 0.653500 | 1.502 sec/iter\n",
      "Epoch: 319 | Batch: 001 / 011 | Total loss: 0.888 | Reg loss: 0.026 | Tree loss: 0.888 | Accuracy: 0.623000 | 1.502 sec/iter\n",
      "Epoch: 319 | Batch: 002 / 011 | Total loss: 0.833 | Reg loss: 0.026 | Tree loss: 0.833 | Accuracy: 0.663000 | 1.501 sec/iter\n",
      "Epoch: 319 | Batch: 003 / 011 | Total loss: 0.844 | Reg loss: 0.026 | Tree loss: 0.844 | Accuracy: 0.654500 | 1.501 sec/iter\n",
      "Epoch: 319 | Batch: 004 / 011 | Total loss: 0.829 | Reg loss: 0.026 | Tree loss: 0.829 | Accuracy: 0.650000 | 1.501 sec/iter\n",
      "Epoch: 319 | Batch: 005 / 011 | Total loss: 0.810 | Reg loss: 0.026 | Tree loss: 0.810 | Accuracy: 0.656500 | 1.501 sec/iter\n",
      "Epoch: 319 | Batch: 006 / 011 | Total loss: 0.808 | Reg loss: 0.026 | Tree loss: 0.808 | Accuracy: 0.638500 | 1.501 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 319 | Batch: 007 / 011 | Total loss: 0.807 | Reg loss: 0.026 | Tree loss: 0.807 | Accuracy: 0.638500 | 1.501 sec/iter\n",
      "Epoch: 319 | Batch: 008 / 011 | Total loss: 0.791 | Reg loss: 0.026 | Tree loss: 0.791 | Accuracy: 0.641000 | 1.501 sec/iter\n",
      "Epoch: 319 | Batch: 009 / 011 | Total loss: 0.793 | Reg loss: 0.026 | Tree loss: 0.793 | Accuracy: 0.628500 | 1.501 sec/iter\n",
      "Epoch: 319 | Batch: 010 / 011 | Total loss: 0.774 | Reg loss: 0.026 | Tree loss: 0.774 | Accuracy: 0.631399 | 1.501 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 320 | Batch: 000 / 011 | Total loss: 0.869 | Reg loss: 0.026 | Tree loss: 0.869 | Accuracy: 0.645500 | 1.502 sec/iter\n",
      "Epoch: 320 | Batch: 001 / 011 | Total loss: 0.870 | Reg loss: 0.026 | Tree loss: 0.870 | Accuracy: 0.642000 | 1.502 sec/iter\n",
      "Epoch: 320 | Batch: 002 / 011 | Total loss: 0.834 | Reg loss: 0.026 | Tree loss: 0.834 | Accuracy: 0.658000 | 1.502 sec/iter\n",
      "Epoch: 320 | Batch: 003 / 011 | Total loss: 0.848 | Reg loss: 0.026 | Tree loss: 0.848 | Accuracy: 0.647000 | 1.502 sec/iter\n",
      "Epoch: 320 | Batch: 004 / 011 | Total loss: 0.818 | Reg loss: 0.026 | Tree loss: 0.818 | Accuracy: 0.652500 | 1.502 sec/iter\n",
      "Epoch: 320 | Batch: 005 / 011 | Total loss: 0.812 | Reg loss: 0.026 | Tree loss: 0.812 | Accuracy: 0.641000 | 1.502 sec/iter\n",
      "Epoch: 320 | Batch: 006 / 011 | Total loss: 0.819 | Reg loss: 0.026 | Tree loss: 0.819 | Accuracy: 0.639000 | 1.501 sec/iter\n",
      "Epoch: 320 | Batch: 007 / 011 | Total loss: 0.790 | Reg loss: 0.026 | Tree loss: 0.790 | Accuracy: 0.648000 | 1.501 sec/iter\n",
      "Epoch: 320 | Batch: 008 / 011 | Total loss: 0.806 | Reg loss: 0.026 | Tree loss: 0.806 | Accuracy: 0.636000 | 1.501 sec/iter\n",
      "Epoch: 320 | Batch: 009 / 011 | Total loss: 0.804 | Reg loss: 0.026 | Tree loss: 0.804 | Accuracy: 0.634500 | 1.501 sec/iter\n",
      "Epoch: 320 | Batch: 010 / 011 | Total loss: 0.793 | Reg loss: 0.026 | Tree loss: 0.793 | Accuracy: 0.648464 | 1.501 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 321 | Batch: 000 / 011 | Total loss: 0.868 | Reg loss: 0.026 | Tree loss: 0.868 | Accuracy: 0.651500 | 1.502 sec/iter\n",
      "Epoch: 321 | Batch: 001 / 011 | Total loss: 0.880 | Reg loss: 0.026 | Tree loss: 0.880 | Accuracy: 0.644500 | 1.502 sec/iter\n",
      "Epoch: 321 | Batch: 002 / 011 | Total loss: 0.861 | Reg loss: 0.026 | Tree loss: 0.861 | Accuracy: 0.644000 | 1.502 sec/iter\n",
      "Epoch: 321 | Batch: 003 / 011 | Total loss: 0.824 | Reg loss: 0.026 | Tree loss: 0.824 | Accuracy: 0.659000 | 1.502 sec/iter\n",
      "Epoch: 321 | Batch: 004 / 011 | Total loss: 0.824 | Reg loss: 0.026 | Tree loss: 0.824 | Accuracy: 0.642000 | 1.502 sec/iter\n",
      "Epoch: 321 | Batch: 005 / 011 | Total loss: 0.815 | Reg loss: 0.026 | Tree loss: 0.815 | Accuracy: 0.657500 | 1.502 sec/iter\n",
      "Epoch: 321 | Batch: 006 / 011 | Total loss: 0.787 | Reg loss: 0.026 | Tree loss: 0.787 | Accuracy: 0.649500 | 1.502 sec/iter\n",
      "Epoch: 321 | Batch: 007 / 011 | Total loss: 0.810 | Reg loss: 0.026 | Tree loss: 0.810 | Accuracy: 0.617500 | 1.502 sec/iter\n",
      "Epoch: 321 | Batch: 008 / 011 | Total loss: 0.821 | Reg loss: 0.026 | Tree loss: 0.821 | Accuracy: 0.613500 | 1.502 sec/iter\n",
      "Epoch: 321 | Batch: 009 / 011 | Total loss: 0.795 | Reg loss: 0.026 | Tree loss: 0.795 | Accuracy: 0.630000 | 1.501 sec/iter\n",
      "Epoch: 321 | Batch: 010 / 011 | Total loss: 0.773 | Reg loss: 0.026 | Tree loss: 0.773 | Accuracy: 0.672355 | 1.501 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 322 | Batch: 000 / 011 | Total loss: 0.868 | Reg loss: 0.026 | Tree loss: 0.868 | Accuracy: 0.649000 | 1.503 sec/iter\n",
      "Epoch: 322 | Batch: 001 / 011 | Total loss: 0.867 | Reg loss: 0.026 | Tree loss: 0.867 | Accuracy: 0.651500 | 1.503 sec/iter\n",
      "Epoch: 322 | Batch: 002 / 011 | Total loss: 0.854 | Reg loss: 0.026 | Tree loss: 0.854 | Accuracy: 0.651000 | 1.502 sec/iter\n",
      "Epoch: 322 | Batch: 003 / 011 | Total loss: 0.857 | Reg loss: 0.026 | Tree loss: 0.857 | Accuracy: 0.626500 | 1.502 sec/iter\n",
      "Epoch: 322 | Batch: 004 / 011 | Total loss: 0.837 | Reg loss: 0.026 | Tree loss: 0.837 | Accuracy: 0.648000 | 1.502 sec/iter\n",
      "Epoch: 322 | Batch: 005 / 011 | Total loss: 0.803 | Reg loss: 0.026 | Tree loss: 0.803 | Accuracy: 0.660000 | 1.502 sec/iter\n",
      "Epoch: 322 | Batch: 006 / 011 | Total loss: 0.802 | Reg loss: 0.026 | Tree loss: 0.802 | Accuracy: 0.654000 | 1.502 sec/iter\n",
      "Epoch: 322 | Batch: 007 / 011 | Total loss: 0.796 | Reg loss: 0.026 | Tree loss: 0.796 | Accuracy: 0.643500 | 1.502 sec/iter\n",
      "Epoch: 322 | Batch: 008 / 011 | Total loss: 0.804 | Reg loss: 0.026 | Tree loss: 0.804 | Accuracy: 0.621000 | 1.502 sec/iter\n",
      "Epoch: 322 | Batch: 009 / 011 | Total loss: 0.818 | Reg loss: 0.026 | Tree loss: 0.818 | Accuracy: 0.634000 | 1.502 sec/iter\n",
      "Epoch: 322 | Batch: 010 / 011 | Total loss: 0.784 | Reg loss: 0.026 | Tree loss: 0.784 | Accuracy: 0.638225 | 1.502 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 323 | Batch: 000 / 011 | Total loss: 0.882 | Reg loss: 0.026 | Tree loss: 0.882 | Accuracy: 0.630000 | 1.503 sec/iter\n",
      "Epoch: 323 | Batch: 001 / 011 | Total loss: 0.866 | Reg loss: 0.026 | Tree loss: 0.866 | Accuracy: 0.659500 | 1.503 sec/iter\n",
      "Epoch: 323 | Batch: 002 / 011 | Total loss: 0.839 | Reg loss: 0.026 | Tree loss: 0.839 | Accuracy: 0.666000 | 1.503 sec/iter\n",
      "Epoch: 323 | Batch: 003 / 011 | Total loss: 0.846 | Reg loss: 0.026 | Tree loss: 0.846 | Accuracy: 0.650500 | 1.503 sec/iter\n",
      "Epoch: 323 | Batch: 004 / 011 | Total loss: 0.814 | Reg loss: 0.026 | Tree loss: 0.814 | Accuracy: 0.668500 | 1.502 sec/iter\n",
      "Epoch: 323 | Batch: 005 / 011 | Total loss: 0.833 | Reg loss: 0.026 | Tree loss: 0.833 | Accuracy: 0.647500 | 1.502 sec/iter\n",
      "Epoch: 323 | Batch: 006 / 011 | Total loss: 0.778 | Reg loss: 0.026 | Tree loss: 0.778 | Accuracy: 0.658500 | 1.502 sec/iter\n",
      "Epoch: 323 | Batch: 007 / 011 | Total loss: 0.808 | Reg loss: 0.026 | Tree loss: 0.808 | Accuracy: 0.634500 | 1.502 sec/iter\n",
      "Epoch: 323 | Batch: 008 / 011 | Total loss: 0.807 | Reg loss: 0.026 | Tree loss: 0.807 | Accuracy: 0.636500 | 1.502 sec/iter\n",
      "Epoch: 323 | Batch: 009 / 011 | Total loss: 0.815 | Reg loss: 0.026 | Tree loss: 0.815 | Accuracy: 0.627000 | 1.502 sec/iter\n",
      "Epoch: 323 | Batch: 010 / 011 | Total loss: 0.798 | Reg loss: 0.026 | Tree loss: 0.798 | Accuracy: 0.634812 | 1.502 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 324 | Batch: 000 / 011 | Total loss: 0.870 | Reg loss: 0.026 | Tree loss: 0.870 | Accuracy: 0.643500 | 1.502 sec/iter\n",
      "Epoch: 324 | Batch: 001 / 011 | Total loss: 0.850 | Reg loss: 0.026 | Tree loss: 0.850 | Accuracy: 0.660000 | 1.502 sec/iter\n",
      "Epoch: 324 | Batch: 002 / 011 | Total loss: 0.846 | Reg loss: 0.026 | Tree loss: 0.846 | Accuracy: 0.656500 | 1.502 sec/iter\n",
      "Epoch: 324 | Batch: 003 / 011 | Total loss: 0.853 | Reg loss: 0.026 | Tree loss: 0.853 | Accuracy: 0.640500 | 1.502 sec/iter\n",
      "Epoch: 324 | Batch: 004 / 011 | Total loss: 0.806 | Reg loss: 0.026 | Tree loss: 0.806 | Accuracy: 0.654000 | 1.502 sec/iter\n",
      "Epoch: 324 | Batch: 005 / 011 | Total loss: 0.806 | Reg loss: 0.026 | Tree loss: 0.806 | Accuracy: 0.643500 | 1.502 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 324 | Batch: 006 / 011 | Total loss: 0.807 | Reg loss: 0.026 | Tree loss: 0.807 | Accuracy: 0.637500 | 1.502 sec/iter\n",
      "Epoch: 324 | Batch: 007 / 011 | Total loss: 0.828 | Reg loss: 0.026 | Tree loss: 0.828 | Accuracy: 0.621500 | 1.501 sec/iter\n",
      "Epoch: 324 | Batch: 008 / 011 | Total loss: 0.824 | Reg loss: 0.026 | Tree loss: 0.824 | Accuracy: 0.617000 | 1.501 sec/iter\n",
      "Epoch: 324 | Batch: 009 / 011 | Total loss: 0.796 | Reg loss: 0.026 | Tree loss: 0.796 | Accuracy: 0.653500 | 1.501 sec/iter\n",
      "Epoch: 324 | Batch: 010 / 011 | Total loss: 0.784 | Reg loss: 0.026 | Tree loss: 0.784 | Accuracy: 0.648464 | 1.501 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 325 | Batch: 000 / 011 | Total loss: 0.867 | Reg loss: 0.026 | Tree loss: 0.867 | Accuracy: 0.640000 | 1.502 sec/iter\n",
      "Epoch: 325 | Batch: 001 / 011 | Total loss: 0.875 | Reg loss: 0.026 | Tree loss: 0.875 | Accuracy: 0.632500 | 1.502 sec/iter\n",
      "Epoch: 325 | Batch: 002 / 011 | Total loss: 0.860 | Reg loss: 0.026 | Tree loss: 0.860 | Accuracy: 0.655500 | 1.502 sec/iter\n",
      "Epoch: 325 | Batch: 003 / 011 | Total loss: 0.822 | Reg loss: 0.026 | Tree loss: 0.822 | Accuracy: 0.671000 | 1.502 sec/iter\n",
      "Epoch: 325 | Batch: 004 / 011 | Total loss: 0.824 | Reg loss: 0.026 | Tree loss: 0.824 | Accuracy: 0.658500 | 1.502 sec/iter\n",
      "Epoch: 325 | Batch: 005 / 011 | Total loss: 0.808 | Reg loss: 0.026 | Tree loss: 0.808 | Accuracy: 0.653000 | 1.502 sec/iter\n",
      "Epoch: 325 | Batch: 006 / 011 | Total loss: 0.814 | Reg loss: 0.026 | Tree loss: 0.814 | Accuracy: 0.645500 | 1.502 sec/iter\n",
      "Epoch: 325 | Batch: 007 / 011 | Total loss: 0.799 | Reg loss: 0.026 | Tree loss: 0.799 | Accuracy: 0.629000 | 1.502 sec/iter\n",
      "Epoch: 325 | Batch: 008 / 011 | Total loss: 0.803 | Reg loss: 0.026 | Tree loss: 0.803 | Accuracy: 0.629500 | 1.502 sec/iter\n",
      "Epoch: 325 | Batch: 009 / 011 | Total loss: 0.814 | Reg loss: 0.026 | Tree loss: 0.814 | Accuracy: 0.623000 | 1.502 sec/iter\n",
      "Epoch: 325 | Batch: 010 / 011 | Total loss: 0.776 | Reg loss: 0.026 | Tree loss: 0.776 | Accuracy: 0.665529 | 1.501 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 326 | Batch: 000 / 011 | Total loss: 0.876 | Reg loss: 0.026 | Tree loss: 0.876 | Accuracy: 0.638000 | 1.502 sec/iter\n",
      "Epoch: 326 | Batch: 001 / 011 | Total loss: 0.867 | Reg loss: 0.026 | Tree loss: 0.867 | Accuracy: 0.640000 | 1.502 sec/iter\n",
      "Epoch: 326 | Batch: 002 / 011 | Total loss: 0.827 | Reg loss: 0.026 | Tree loss: 0.827 | Accuracy: 0.660500 | 1.502 sec/iter\n",
      "Epoch: 326 | Batch: 003 / 011 | Total loss: 0.856 | Reg loss: 0.026 | Tree loss: 0.856 | Accuracy: 0.647000 | 1.502 sec/iter\n",
      "Epoch: 326 | Batch: 004 / 011 | Total loss: 0.841 | Reg loss: 0.026 | Tree loss: 0.841 | Accuracy: 0.642000 | 1.502 sec/iter\n",
      "Epoch: 326 | Batch: 005 / 011 | Total loss: 0.817 | Reg loss: 0.026 | Tree loss: 0.817 | Accuracy: 0.659000 | 1.502 sec/iter\n",
      "Epoch: 326 | Batch: 006 / 011 | Total loss: 0.797 | Reg loss: 0.026 | Tree loss: 0.797 | Accuracy: 0.663000 | 1.502 sec/iter\n",
      "Epoch: 326 | Batch: 007 / 011 | Total loss: 0.791 | Reg loss: 0.026 | Tree loss: 0.791 | Accuracy: 0.641000 | 1.502 sec/iter\n",
      "Epoch: 326 | Batch: 008 / 011 | Total loss: 0.792 | Reg loss: 0.026 | Tree loss: 0.792 | Accuracy: 0.628500 | 1.502 sec/iter\n",
      "Epoch: 326 | Batch: 009 / 011 | Total loss: 0.811 | Reg loss: 0.026 | Tree loss: 0.811 | Accuracy: 0.642500 | 1.501 sec/iter\n",
      "Epoch: 326 | Batch: 010 / 011 | Total loss: 0.861 | Reg loss: 0.026 | Tree loss: 0.861 | Accuracy: 0.593857 | 1.501 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 327 | Batch: 000 / 011 | Total loss: 0.855 | Reg loss: 0.026 | Tree loss: 0.855 | Accuracy: 0.655500 | 1.502 sec/iter\n",
      "Epoch: 327 | Batch: 001 / 011 | Total loss: 0.865 | Reg loss: 0.026 | Tree loss: 0.865 | Accuracy: 0.646000 | 1.502 sec/iter\n",
      "Epoch: 327 | Batch: 002 / 011 | Total loss: 0.887 | Reg loss: 0.026 | Tree loss: 0.887 | Accuracy: 0.634500 | 1.502 sec/iter\n",
      "Epoch: 327 | Batch: 003 / 011 | Total loss: 0.828 | Reg loss: 0.026 | Tree loss: 0.828 | Accuracy: 0.655500 | 1.502 sec/iter\n",
      "Epoch: 327 | Batch: 004 / 011 | Total loss: 0.842 | Reg loss: 0.026 | Tree loss: 0.842 | Accuracy: 0.644000 | 1.502 sec/iter\n",
      "Epoch: 327 | Batch: 005 / 011 | Total loss: 0.807 | Reg loss: 0.026 | Tree loss: 0.807 | Accuracy: 0.644500 | 1.502 sec/iter\n",
      "Epoch: 327 | Batch: 006 / 011 | Total loss: 0.806 | Reg loss: 0.026 | Tree loss: 0.806 | Accuracy: 0.652500 | 1.502 sec/iter\n",
      "Epoch: 327 | Batch: 007 / 011 | Total loss: 0.799 | Reg loss: 0.026 | Tree loss: 0.799 | Accuracy: 0.618500 | 1.502 sec/iter\n",
      "Epoch: 327 | Batch: 008 / 011 | Total loss: 0.813 | Reg loss: 0.026 | Tree loss: 0.813 | Accuracy: 0.615500 | 1.501 sec/iter\n",
      "Epoch: 327 | Batch: 009 / 011 | Total loss: 0.787 | Reg loss: 0.026 | Tree loss: 0.787 | Accuracy: 0.635000 | 1.501 sec/iter\n",
      "Epoch: 327 | Batch: 010 / 011 | Total loss: 0.779 | Reg loss: 0.026 | Tree loss: 0.779 | Accuracy: 0.638225 | 1.501 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 328 | Batch: 000 / 011 | Total loss: 0.862 | Reg loss: 0.026 | Tree loss: 0.862 | Accuracy: 0.635500 | 1.502 sec/iter\n",
      "Epoch: 328 | Batch: 001 / 011 | Total loss: 0.867 | Reg loss: 0.026 | Tree loss: 0.867 | Accuracy: 0.616500 | 1.502 sec/iter\n",
      "Epoch: 328 | Batch: 002 / 011 | Total loss: 0.861 | Reg loss: 0.026 | Tree loss: 0.861 | Accuracy: 0.642500 | 1.502 sec/iter\n",
      "Epoch: 328 | Batch: 003 / 011 | Total loss: 0.839 | Reg loss: 0.026 | Tree loss: 0.839 | Accuracy: 0.656500 | 1.502 sec/iter\n",
      "Epoch: 328 | Batch: 004 / 011 | Total loss: 0.845 | Reg loss: 0.026 | Tree loss: 0.845 | Accuracy: 0.642500 | 1.502 sec/iter\n",
      "Epoch: 328 | Batch: 005 / 011 | Total loss: 0.822 | Reg loss: 0.026 | Tree loss: 0.822 | Accuracy: 0.633500 | 1.502 sec/iter\n",
      "Epoch: 328 | Batch: 006 / 011 | Total loss: 0.789 | Reg loss: 0.026 | Tree loss: 0.789 | Accuracy: 0.662500 | 1.502 sec/iter\n",
      "Epoch: 328 | Batch: 007 / 011 | Total loss: 0.796 | Reg loss: 0.026 | Tree loss: 0.796 | Accuracy: 0.626500 | 1.502 sec/iter\n",
      "Epoch: 328 | Batch: 008 / 011 | Total loss: 0.800 | Reg loss: 0.026 | Tree loss: 0.800 | Accuracy: 0.626500 | 1.502 sec/iter\n",
      "Epoch: 328 | Batch: 009 / 011 | Total loss: 0.811 | Reg loss: 0.026 | Tree loss: 0.811 | Accuracy: 0.628000 | 1.502 sec/iter\n",
      "Epoch: 328 | Batch: 010 / 011 | Total loss: 0.811 | Reg loss: 0.026 | Tree loss: 0.811 | Accuracy: 0.665529 | 1.502 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 329 | Batch: 000 / 011 | Total loss: 0.868 | Reg loss: 0.026 | Tree loss: 0.868 | Accuracy: 0.647500 | 1.503 sec/iter\n",
      "Epoch: 329 | Batch: 001 / 011 | Total loss: 0.862 | Reg loss: 0.026 | Tree loss: 0.862 | Accuracy: 0.644500 | 1.503 sec/iter\n",
      "Epoch: 329 | Batch: 002 / 011 | Total loss: 0.849 | Reg loss: 0.026 | Tree loss: 0.849 | Accuracy: 0.640500 | 1.502 sec/iter\n",
      "Epoch: 329 | Batch: 003 / 011 | Total loss: 0.852 | Reg loss: 0.026 | Tree loss: 0.852 | Accuracy: 0.643000 | 1.502 sec/iter\n",
      "Epoch: 329 | Batch: 004 / 011 | Total loss: 0.812 | Reg loss: 0.026 | Tree loss: 0.812 | Accuracy: 0.666000 | 1.502 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 329 | Batch: 005 / 011 | Total loss: 0.827 | Reg loss: 0.026 | Tree loss: 0.827 | Accuracy: 0.637500 | 1.502 sec/iter\n",
      "Epoch: 329 | Batch: 006 / 011 | Total loss: 0.821 | Reg loss: 0.026 | Tree loss: 0.821 | Accuracy: 0.632500 | 1.502 sec/iter\n",
      "Epoch: 329 | Batch: 007 / 011 | Total loss: 0.797 | Reg loss: 0.026 | Tree loss: 0.797 | Accuracy: 0.653000 | 1.502 sec/iter\n",
      "Epoch: 329 | Batch: 008 / 011 | Total loss: 0.801 | Reg loss: 0.026 | Tree loss: 0.801 | Accuracy: 0.618500 | 1.502 sec/iter\n",
      "Epoch: 329 | Batch: 009 / 011 | Total loss: 0.820 | Reg loss: 0.026 | Tree loss: 0.820 | Accuracy: 0.627000 | 1.501 sec/iter\n",
      "Epoch: 329 | Batch: 010 / 011 | Total loss: 0.815 | Reg loss: 0.026 | Tree loss: 0.815 | Accuracy: 0.655290 | 1.501 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 330 | Batch: 000 / 011 | Total loss: 0.871 | Reg loss: 0.026 | Tree loss: 0.871 | Accuracy: 0.644000 | 1.501 sec/iter\n",
      "Epoch: 330 | Batch: 001 / 011 | Total loss: 0.882 | Reg loss: 0.026 | Tree loss: 0.882 | Accuracy: 0.643500 | 1.501 sec/iter\n",
      "Epoch: 330 | Batch: 002 / 011 | Total loss: 0.849 | Reg loss: 0.026 | Tree loss: 0.849 | Accuracy: 0.648500 | 1.501 sec/iter\n",
      "Epoch: 330 | Batch: 003 / 011 | Total loss: 0.840 | Reg loss: 0.026 | Tree loss: 0.840 | Accuracy: 0.656500 | 1.501 sec/iter\n",
      "Epoch: 330 | Batch: 004 / 011 | Total loss: 0.799 | Reg loss: 0.026 | Tree loss: 0.799 | Accuracy: 0.676500 | 1.501 sec/iter\n",
      "Epoch: 330 | Batch: 005 / 011 | Total loss: 0.829 | Reg loss: 0.026 | Tree loss: 0.829 | Accuracy: 0.647000 | 1.501 sec/iter\n",
      "Epoch: 330 | Batch: 006 / 011 | Total loss: 0.798 | Reg loss: 0.026 | Tree loss: 0.798 | Accuracy: 0.647000 | 1.501 sec/iter\n",
      "Epoch: 330 | Batch: 007 / 011 | Total loss: 0.820 | Reg loss: 0.026 | Tree loss: 0.820 | Accuracy: 0.628000 | 1.5 sec/iter\n",
      "Epoch: 330 | Batch: 008 / 011 | Total loss: 0.794 | Reg loss: 0.026 | Tree loss: 0.794 | Accuracy: 0.634000 | 1.5 sec/iter\n",
      "Epoch: 330 | Batch: 009 / 011 | Total loss: 0.789 | Reg loss: 0.026 | Tree loss: 0.789 | Accuracy: 0.641000 | 1.5 sec/iter\n",
      "Epoch: 330 | Batch: 010 / 011 | Total loss: 0.814 | Reg loss: 0.026 | Tree loss: 0.814 | Accuracy: 0.641638 | 1.5 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 331 | Batch: 000 / 011 | Total loss: 0.882 | Reg loss: 0.026 | Tree loss: 0.882 | Accuracy: 0.641000 | 1.501 sec/iter\n",
      "Epoch: 331 | Batch: 001 / 011 | Total loss: 0.880 | Reg loss: 0.026 | Tree loss: 0.880 | Accuracy: 0.636500 | 1.501 sec/iter\n",
      "Epoch: 331 | Batch: 002 / 011 | Total loss: 0.845 | Reg loss: 0.026 | Tree loss: 0.845 | Accuracy: 0.654500 | 1.501 sec/iter\n",
      "Epoch: 331 | Batch: 003 / 011 | Total loss: 0.823 | Reg loss: 0.026 | Tree loss: 0.823 | Accuracy: 0.668000 | 1.501 sec/iter\n",
      "Epoch: 331 | Batch: 004 / 011 | Total loss: 0.822 | Reg loss: 0.026 | Tree loss: 0.822 | Accuracy: 0.638500 | 1.501 sec/iter\n",
      "Epoch: 331 | Batch: 005 / 011 | Total loss: 0.808 | Reg loss: 0.026 | Tree loss: 0.808 | Accuracy: 0.644500 | 1.501 sec/iter\n",
      "Epoch: 331 | Batch: 006 / 011 | Total loss: 0.782 | Reg loss: 0.026 | Tree loss: 0.782 | Accuracy: 0.675000 | 1.5 sec/iter\n",
      "Epoch: 331 | Batch: 007 / 011 | Total loss: 0.837 | Reg loss: 0.026 | Tree loss: 0.837 | Accuracy: 0.610000 | 1.5 sec/iter\n",
      "Epoch: 331 | Batch: 008 / 011 | Total loss: 0.792 | Reg loss: 0.026 | Tree loss: 0.792 | Accuracy: 0.634500 | 1.5 sec/iter\n",
      "Epoch: 331 | Batch: 009 / 011 | Total loss: 0.784 | Reg loss: 0.026 | Tree loss: 0.784 | Accuracy: 0.635500 | 1.5 sec/iter\n",
      "Epoch: 331 | Batch: 010 / 011 | Total loss: 0.867 | Reg loss: 0.026 | Tree loss: 0.867 | Accuracy: 0.600683 | 1.5 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 332 | Batch: 000 / 011 | Total loss: 0.879 | Reg loss: 0.026 | Tree loss: 0.879 | Accuracy: 0.625500 | 1.5 sec/iter\n",
      "Epoch: 332 | Batch: 001 / 011 | Total loss: 0.854 | Reg loss: 0.026 | Tree loss: 0.854 | Accuracy: 0.654500 | 1.5 sec/iter\n",
      "Epoch: 332 | Batch: 002 / 011 | Total loss: 0.857 | Reg loss: 0.026 | Tree loss: 0.857 | Accuracy: 0.651500 | 1.5 sec/iter\n",
      "Epoch: 332 | Batch: 003 / 011 | Total loss: 0.828 | Reg loss: 0.026 | Tree loss: 0.828 | Accuracy: 0.658500 | 1.5 sec/iter\n",
      "Epoch: 332 | Batch: 004 / 011 | Total loss: 0.813 | Reg loss: 0.026 | Tree loss: 0.813 | Accuracy: 0.666000 | 1.5 sec/iter\n",
      "Epoch: 332 | Batch: 005 / 011 | Total loss: 0.822 | Reg loss: 0.026 | Tree loss: 0.822 | Accuracy: 0.635500 | 1.5 sec/iter\n",
      "Epoch: 332 | Batch: 006 / 011 | Total loss: 0.785 | Reg loss: 0.026 | Tree loss: 0.785 | Accuracy: 0.649000 | 1.5 sec/iter\n",
      "Epoch: 332 | Batch: 007 / 011 | Total loss: 0.803 | Reg loss: 0.026 | Tree loss: 0.803 | Accuracy: 0.632000 | 1.5 sec/iter\n",
      "Epoch: 332 | Batch: 008 / 011 | Total loss: 0.805 | Reg loss: 0.026 | Tree loss: 0.805 | Accuracy: 0.630500 | 1.5 sec/iter\n",
      "Epoch: 332 | Batch: 009 / 011 | Total loss: 0.816 | Reg loss: 0.026 | Tree loss: 0.816 | Accuracy: 0.621000 | 1.499 sec/iter\n",
      "Epoch: 332 | Batch: 010 / 011 | Total loss: 0.858 | Reg loss: 0.026 | Tree loss: 0.858 | Accuracy: 0.610922 | 1.499 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 333 | Batch: 000 / 011 | Total loss: 0.859 | Reg loss: 0.026 | Tree loss: 0.859 | Accuracy: 0.645000 | 1.5 sec/iter\n",
      "Epoch: 333 | Batch: 001 / 011 | Total loss: 0.858 | Reg loss: 0.026 | Tree loss: 0.858 | Accuracy: 0.654000 | 1.499 sec/iter\n",
      "Epoch: 333 | Batch: 002 / 011 | Total loss: 0.847 | Reg loss: 0.026 | Tree loss: 0.847 | Accuracy: 0.649500 | 1.499 sec/iter\n",
      "Epoch: 333 | Batch: 003 / 011 | Total loss: 0.844 | Reg loss: 0.026 | Tree loss: 0.844 | Accuracy: 0.649500 | 1.499 sec/iter\n",
      "Epoch: 333 | Batch: 004 / 011 | Total loss: 0.832 | Reg loss: 0.026 | Tree loss: 0.832 | Accuracy: 0.643500 | 1.499 sec/iter\n",
      "Epoch: 333 | Batch: 005 / 011 | Total loss: 0.810 | Reg loss: 0.026 | Tree loss: 0.810 | Accuracy: 0.655500 | 1.499 sec/iter\n",
      "Epoch: 333 | Batch: 006 / 011 | Total loss: 0.812 | Reg loss: 0.026 | Tree loss: 0.812 | Accuracy: 0.636000 | 1.499 sec/iter\n",
      "Epoch: 333 | Batch: 007 / 011 | Total loss: 0.788 | Reg loss: 0.026 | Tree loss: 0.788 | Accuracy: 0.639500 | 1.499 sec/iter\n",
      "Epoch: 333 | Batch: 008 / 011 | Total loss: 0.828 | Reg loss: 0.026 | Tree loss: 0.828 | Accuracy: 0.611000 | 1.499 sec/iter\n",
      "Epoch: 333 | Batch: 009 / 011 | Total loss: 0.803 | Reg loss: 0.026 | Tree loss: 0.803 | Accuracy: 0.638500 | 1.499 sec/iter\n",
      "Epoch: 333 | Batch: 010 / 011 | Total loss: 0.816 | Reg loss: 0.026 | Tree loss: 0.816 | Accuracy: 0.631399 | 1.498 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 334 | Batch: 000 / 011 | Total loss: 0.888 | Reg loss: 0.026 | Tree loss: 0.888 | Accuracy: 0.631000 | 1.499 sec/iter\n",
      "Epoch: 334 | Batch: 001 / 011 | Total loss: 0.870 | Reg loss: 0.026 | Tree loss: 0.870 | Accuracy: 0.635000 | 1.499 sec/iter\n",
      "Epoch: 334 | Batch: 002 / 011 | Total loss: 0.847 | Reg loss: 0.026 | Tree loss: 0.847 | Accuracy: 0.663000 | 1.499 sec/iter\n",
      "Epoch: 334 | Batch: 003 / 011 | Total loss: 0.835 | Reg loss: 0.026 | Tree loss: 0.835 | Accuracy: 0.657500 | 1.499 sec/iter\n",
      "Epoch: 334 | Batch: 004 / 011 | Total loss: 0.812 | Reg loss: 0.026 | Tree loss: 0.812 | Accuracy: 0.654000 | 1.498 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 334 | Batch: 005 / 011 | Total loss: 0.795 | Reg loss: 0.026 | Tree loss: 0.795 | Accuracy: 0.666000 | 1.498 sec/iter\n",
      "Epoch: 334 | Batch: 006 / 011 | Total loss: 0.830 | Reg loss: 0.026 | Tree loss: 0.830 | Accuracy: 0.629000 | 1.498 sec/iter\n",
      "Epoch: 334 | Batch: 007 / 011 | Total loss: 0.798 | Reg loss: 0.026 | Tree loss: 0.798 | Accuracy: 0.639000 | 1.498 sec/iter\n",
      "Epoch: 334 | Batch: 008 / 011 | Total loss: 0.787 | Reg loss: 0.026 | Tree loss: 0.787 | Accuracy: 0.647000 | 1.498 sec/iter\n",
      "Epoch: 334 | Batch: 009 / 011 | Total loss: 0.813 | Reg loss: 0.026 | Tree loss: 0.813 | Accuracy: 0.629500 | 1.498 sec/iter\n",
      "Epoch: 334 | Batch: 010 / 011 | Total loss: 0.778 | Reg loss: 0.026 | Tree loss: 0.778 | Accuracy: 0.645051 | 1.498 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 335 | Batch: 000 / 011 | Total loss: 0.868 | Reg loss: 0.026 | Tree loss: 0.868 | Accuracy: 0.636000 | 1.498 sec/iter\n",
      "Epoch: 335 | Batch: 001 / 011 | Total loss: 0.863 | Reg loss: 0.026 | Tree loss: 0.863 | Accuracy: 0.656500 | 1.498 sec/iter\n",
      "Epoch: 335 | Batch: 002 / 011 | Total loss: 0.834 | Reg loss: 0.026 | Tree loss: 0.834 | Accuracy: 0.668500 | 1.498 sec/iter\n",
      "Epoch: 335 | Batch: 003 / 011 | Total loss: 0.824 | Reg loss: 0.026 | Tree loss: 0.824 | Accuracy: 0.670500 | 1.498 sec/iter\n",
      "Epoch: 335 | Batch: 004 / 011 | Total loss: 0.823 | Reg loss: 0.026 | Tree loss: 0.823 | Accuracy: 0.637500 | 1.498 sec/iter\n",
      "Epoch: 335 | Batch: 005 / 011 | Total loss: 0.816 | Reg loss: 0.026 | Tree loss: 0.816 | Accuracy: 0.639000 | 1.497 sec/iter\n",
      "Epoch: 335 | Batch: 006 / 011 | Total loss: 0.806 | Reg loss: 0.026 | Tree loss: 0.806 | Accuracy: 0.643500 | 1.497 sec/iter\n",
      "Epoch: 335 | Batch: 007 / 011 | Total loss: 0.808 | Reg loss: 0.026 | Tree loss: 0.808 | Accuracy: 0.635500 | 1.497 sec/iter\n",
      "Epoch: 335 | Batch: 008 / 011 | Total loss: 0.815 | Reg loss: 0.026 | Tree loss: 0.815 | Accuracy: 0.623000 | 1.497 sec/iter\n",
      "Epoch: 335 | Batch: 009 / 011 | Total loss: 0.821 | Reg loss: 0.026 | Tree loss: 0.821 | Accuracy: 0.625000 | 1.497 sec/iter\n",
      "Epoch: 335 | Batch: 010 / 011 | Total loss: 0.848 | Reg loss: 0.026 | Tree loss: 0.848 | Accuracy: 0.645051 | 1.497 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 336 | Batch: 000 / 011 | Total loss: 0.876 | Reg loss: 0.026 | Tree loss: 0.876 | Accuracy: 0.635000 | 1.497 sec/iter\n",
      "Epoch: 336 | Batch: 001 / 011 | Total loss: 0.875 | Reg loss: 0.026 | Tree loss: 0.875 | Accuracy: 0.649000 | 1.497 sec/iter\n",
      "Epoch: 336 | Batch: 002 / 011 | Total loss: 0.839 | Reg loss: 0.026 | Tree loss: 0.839 | Accuracy: 0.671500 | 1.497 sec/iter\n",
      "Epoch: 336 | Batch: 003 / 011 | Total loss: 0.811 | Reg loss: 0.026 | Tree loss: 0.811 | Accuracy: 0.666000 | 1.497 sec/iter\n",
      "Epoch: 336 | Batch: 004 / 011 | Total loss: 0.821 | Reg loss: 0.026 | Tree loss: 0.821 | Accuracy: 0.659500 | 1.497 sec/iter\n",
      "Epoch: 336 | Batch: 005 / 011 | Total loss: 0.798 | Reg loss: 0.026 | Tree loss: 0.798 | Accuracy: 0.659000 | 1.497 sec/iter\n",
      "Epoch: 336 | Batch: 006 / 011 | Total loss: 0.807 | Reg loss: 0.026 | Tree loss: 0.807 | Accuracy: 0.651000 | 1.497 sec/iter\n",
      "Epoch: 336 | Batch: 007 / 011 | Total loss: 0.799 | Reg loss: 0.026 | Tree loss: 0.799 | Accuracy: 0.643500 | 1.497 sec/iter\n",
      "Epoch: 336 | Batch: 008 / 011 | Total loss: 0.826 | Reg loss: 0.026 | Tree loss: 0.826 | Accuracy: 0.605500 | 1.496 sec/iter\n",
      "Epoch: 336 | Batch: 009 / 011 | Total loss: 0.819 | Reg loss: 0.026 | Tree loss: 0.819 | Accuracy: 0.620000 | 1.496 sec/iter\n",
      "Epoch: 336 | Batch: 010 / 011 | Total loss: 0.839 | Reg loss: 0.026 | Tree loss: 0.839 | Accuracy: 0.617747 | 1.496 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 337 | Batch: 000 / 011 | Total loss: 0.882 | Reg loss: 0.026 | Tree loss: 0.882 | Accuracy: 0.622000 | 1.497 sec/iter\n",
      "Epoch: 337 | Batch: 001 / 011 | Total loss: 0.871 | Reg loss: 0.026 | Tree loss: 0.871 | Accuracy: 0.655000 | 1.497 sec/iter\n",
      "Epoch: 337 | Batch: 002 / 011 | Total loss: 0.818 | Reg loss: 0.026 | Tree loss: 0.818 | Accuracy: 0.667500 | 1.497 sec/iter\n",
      "Epoch: 337 | Batch: 003 / 011 | Total loss: 0.826 | Reg loss: 0.026 | Tree loss: 0.826 | Accuracy: 0.656500 | 1.496 sec/iter\n",
      "Epoch: 337 | Batch: 004 / 011 | Total loss: 0.836 | Reg loss: 0.026 | Tree loss: 0.836 | Accuracy: 0.644500 | 1.496 sec/iter\n",
      "Epoch: 337 | Batch: 005 / 011 | Total loss: 0.817 | Reg loss: 0.026 | Tree loss: 0.817 | Accuracy: 0.640500 | 1.496 sec/iter\n",
      "Epoch: 337 | Batch: 006 / 011 | Total loss: 0.810 | Reg loss: 0.026 | Tree loss: 0.810 | Accuracy: 0.634500 | 1.496 sec/iter\n",
      "Epoch: 337 | Batch: 007 / 011 | Total loss: 0.823 | Reg loss: 0.026 | Tree loss: 0.823 | Accuracy: 0.627500 | 1.496 sec/iter\n",
      "Epoch: 337 | Batch: 008 / 011 | Total loss: 0.808 | Reg loss: 0.026 | Tree loss: 0.808 | Accuracy: 0.634500 | 1.496 sec/iter\n",
      "Epoch: 337 | Batch: 009 / 011 | Total loss: 0.790 | Reg loss: 0.026 | Tree loss: 0.790 | Accuracy: 0.630500 | 1.496 sec/iter\n",
      "Epoch: 337 | Batch: 010 / 011 | Total loss: 0.761 | Reg loss: 0.026 | Tree loss: 0.761 | Accuracy: 0.651877 | 1.496 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 338 | Batch: 000 / 011 | Total loss: 0.842 | Reg loss: 0.026 | Tree loss: 0.842 | Accuracy: 0.659000 | 1.496 sec/iter\n",
      "Epoch: 338 | Batch: 001 / 011 | Total loss: 0.885 | Reg loss: 0.026 | Tree loss: 0.885 | Accuracy: 0.634000 | 1.496 sec/iter\n",
      "Epoch: 338 | Batch: 002 / 011 | Total loss: 0.840 | Reg loss: 0.026 | Tree loss: 0.840 | Accuracy: 0.647500 | 1.496 sec/iter\n",
      "Epoch: 338 | Batch: 003 / 011 | Total loss: 0.828 | Reg loss: 0.026 | Tree loss: 0.828 | Accuracy: 0.662500 | 1.496 sec/iter\n",
      "Epoch: 338 | Batch: 004 / 011 | Total loss: 0.797 | Reg loss: 0.026 | Tree loss: 0.797 | Accuracy: 0.681500 | 1.496 sec/iter\n",
      "Epoch: 338 | Batch: 005 / 011 | Total loss: 0.832 | Reg loss: 0.026 | Tree loss: 0.832 | Accuracy: 0.631000 | 1.495 sec/iter\n",
      "Epoch: 338 | Batch: 006 / 011 | Total loss: 0.815 | Reg loss: 0.026 | Tree loss: 0.815 | Accuracy: 0.620000 | 1.495 sec/iter\n",
      "Epoch: 338 | Batch: 007 / 011 | Total loss: 0.802 | Reg loss: 0.026 | Tree loss: 0.802 | Accuracy: 0.624000 | 1.495 sec/iter\n",
      "Epoch: 338 | Batch: 008 / 011 | Total loss: 0.811 | Reg loss: 0.026 | Tree loss: 0.811 | Accuracy: 0.620000 | 1.495 sec/iter\n",
      "Epoch: 338 | Batch: 009 / 011 | Total loss: 0.812 | Reg loss: 0.026 | Tree loss: 0.812 | Accuracy: 0.636000 | 1.495 sec/iter\n",
      "Epoch: 338 | Batch: 010 / 011 | Total loss: 0.839 | Reg loss: 0.026 | Tree loss: 0.839 | Accuracy: 0.631399 | 1.495 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 339 | Batch: 000 / 011 | Total loss: 0.878 | Reg loss: 0.026 | Tree loss: 0.878 | Accuracy: 0.644500 | 1.495 sec/iter\n",
      "Epoch: 339 | Batch: 001 / 011 | Total loss: 0.843 | Reg loss: 0.026 | Tree loss: 0.843 | Accuracy: 0.651500 | 1.495 sec/iter\n",
      "Epoch: 339 | Batch: 002 / 011 | Total loss: 0.869 | Reg loss: 0.026 | Tree loss: 0.869 | Accuracy: 0.637000 | 1.495 sec/iter\n",
      "Epoch: 339 | Batch: 003 / 011 | Total loss: 0.847 | Reg loss: 0.026 | Tree loss: 0.847 | Accuracy: 0.634000 | 1.495 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 339 | Batch: 004 / 011 | Total loss: 0.831 | Reg loss: 0.026 | Tree loss: 0.831 | Accuracy: 0.642000 | 1.495 sec/iter\n",
      "Epoch: 339 | Batch: 005 / 011 | Total loss: 0.787 | Reg loss: 0.026 | Tree loss: 0.787 | Accuracy: 0.655500 | 1.495 sec/iter\n",
      "Epoch: 339 | Batch: 006 / 011 | Total loss: 0.795 | Reg loss: 0.026 | Tree loss: 0.795 | Accuracy: 0.649500 | 1.495 sec/iter\n",
      "Epoch: 339 | Batch: 007 / 011 | Total loss: 0.801 | Reg loss: 0.026 | Tree loss: 0.801 | Accuracy: 0.641000 | 1.495 sec/iter\n",
      "Epoch: 339 | Batch: 008 / 011 | Total loss: 0.796 | Reg loss: 0.026 | Tree loss: 0.796 | Accuracy: 0.606000 | 1.494 sec/iter\n",
      "Epoch: 339 | Batch: 009 / 011 | Total loss: 0.829 | Reg loss: 0.026 | Tree loss: 0.829 | Accuracy: 0.615000 | 1.494 sec/iter\n",
      "Epoch: 339 | Batch: 010 / 011 | Total loss: 0.826 | Reg loss: 0.026 | Tree loss: 0.826 | Accuracy: 0.634812 | 1.494 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 340 | Batch: 000 / 011 | Total loss: 0.858 | Reg loss: 0.026 | Tree loss: 0.858 | Accuracy: 0.646000 | 1.495 sec/iter\n",
      "Epoch: 340 | Batch: 001 / 011 | Total loss: 0.872 | Reg loss: 0.026 | Tree loss: 0.872 | Accuracy: 0.617500 | 1.495 sec/iter\n",
      "Epoch: 340 | Batch: 002 / 011 | Total loss: 0.854 | Reg loss: 0.026 | Tree loss: 0.854 | Accuracy: 0.645500 | 1.494 sec/iter\n",
      "Epoch: 340 | Batch: 003 / 011 | Total loss: 0.866 | Reg loss: 0.026 | Tree loss: 0.866 | Accuracy: 0.636000 | 1.494 sec/iter\n",
      "Epoch: 340 | Batch: 004 / 011 | Total loss: 0.817 | Reg loss: 0.026 | Tree loss: 0.817 | Accuracy: 0.659500 | 1.494 sec/iter\n",
      "Epoch: 340 | Batch: 005 / 011 | Total loss: 0.799 | Reg loss: 0.026 | Tree loss: 0.799 | Accuracy: 0.667500 | 1.494 sec/iter\n",
      "Epoch: 340 | Batch: 006 / 011 | Total loss: 0.814 | Reg loss: 0.026 | Tree loss: 0.814 | Accuracy: 0.654500 | 1.494 sec/iter\n",
      "Epoch: 340 | Batch: 007 / 011 | Total loss: 0.819 | Reg loss: 0.026 | Tree loss: 0.819 | Accuracy: 0.623000 | 1.494 sec/iter\n",
      "Epoch: 340 | Batch: 008 / 011 | Total loss: 0.798 | Reg loss: 0.026 | Tree loss: 0.798 | Accuracy: 0.640500 | 1.494 sec/iter\n",
      "Epoch: 340 | Batch: 009 / 011 | Total loss: 0.792 | Reg loss: 0.026 | Tree loss: 0.792 | Accuracy: 0.632500 | 1.494 sec/iter\n",
      "Epoch: 340 | Batch: 010 / 011 | Total loss: 0.763 | Reg loss: 0.026 | Tree loss: 0.763 | Accuracy: 0.662116 | 1.493 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 341 | Batch: 000 / 011 | Total loss: 0.891 | Reg loss: 0.026 | Tree loss: 0.891 | Accuracy: 0.631000 | 1.494 sec/iter\n",
      "Epoch: 341 | Batch: 001 / 011 | Total loss: 0.848 | Reg loss: 0.026 | Tree loss: 0.848 | Accuracy: 0.656000 | 1.493 sec/iter\n",
      "Epoch: 341 | Batch: 002 / 011 | Total loss: 0.864 | Reg loss: 0.026 | Tree loss: 0.864 | Accuracy: 0.649500 | 1.493 sec/iter\n",
      "Epoch: 341 | Batch: 003 / 011 | Total loss: 0.852 | Reg loss: 0.026 | Tree loss: 0.852 | Accuracy: 0.646500 | 1.493 sec/iter\n",
      "Epoch: 341 | Batch: 004 / 011 | Total loss: 0.820 | Reg loss: 0.026 | Tree loss: 0.820 | Accuracy: 0.652000 | 1.493 sec/iter\n",
      "Epoch: 341 | Batch: 005 / 011 | Total loss: 0.829 | Reg loss: 0.026 | Tree loss: 0.829 | Accuracy: 0.621500 | 1.493 sec/iter\n",
      "Epoch: 341 | Batch: 006 / 011 | Total loss: 0.796 | Reg loss: 0.026 | Tree loss: 0.796 | Accuracy: 0.638000 | 1.493 sec/iter\n",
      "Epoch: 341 | Batch: 007 / 011 | Total loss: 0.825 | Reg loss: 0.026 | Tree loss: 0.825 | Accuracy: 0.609500 | 1.493 sec/iter\n",
      "Epoch: 341 | Batch: 008 / 011 | Total loss: 0.791 | Reg loss: 0.026 | Tree loss: 0.791 | Accuracy: 0.629000 | 1.493 sec/iter\n",
      "Epoch: 341 | Batch: 009 / 011 | Total loss: 0.788 | Reg loss: 0.026 | Tree loss: 0.788 | Accuracy: 0.647500 | 1.492 sec/iter\n",
      "Epoch: 341 | Batch: 010 / 011 | Total loss: 0.786 | Reg loss: 0.026 | Tree loss: 0.786 | Accuracy: 0.638225 | 1.492 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 342 | Batch: 000 / 011 | Total loss: 0.864 | Reg loss: 0.026 | Tree loss: 0.864 | Accuracy: 0.647000 | 1.493 sec/iter\n",
      "Epoch: 342 | Batch: 001 / 011 | Total loss: 0.863 | Reg loss: 0.026 | Tree loss: 0.863 | Accuracy: 0.645000 | 1.493 sec/iter\n",
      "Epoch: 342 | Batch: 002 / 011 | Total loss: 0.829 | Reg loss: 0.026 | Tree loss: 0.829 | Accuracy: 0.653000 | 1.493 sec/iter\n",
      "Epoch: 342 | Batch: 003 / 011 | Total loss: 0.821 | Reg loss: 0.026 | Tree loss: 0.821 | Accuracy: 0.653000 | 1.493 sec/iter\n",
      "Epoch: 342 | Batch: 004 / 011 | Total loss: 0.834 | Reg loss: 0.026 | Tree loss: 0.834 | Accuracy: 0.651000 | 1.493 sec/iter\n",
      "Epoch: 342 | Batch: 005 / 011 | Total loss: 0.837 | Reg loss: 0.026 | Tree loss: 0.837 | Accuracy: 0.632500 | 1.493 sec/iter\n",
      "Epoch: 342 | Batch: 006 / 011 | Total loss: 0.829 | Reg loss: 0.026 | Tree loss: 0.829 | Accuracy: 0.628000 | 1.493 sec/iter\n",
      "Epoch: 342 | Batch: 007 / 011 | Total loss: 0.811 | Reg loss: 0.026 | Tree loss: 0.811 | Accuracy: 0.621000 | 1.492 sec/iter\n",
      "Epoch: 342 | Batch: 008 / 011 | Total loss: 0.797 | Reg loss: 0.026 | Tree loss: 0.797 | Accuracy: 0.636500 | 1.492 sec/iter\n",
      "Epoch: 342 | Batch: 009 / 011 | Total loss: 0.801 | Reg loss: 0.026 | Tree loss: 0.801 | Accuracy: 0.638000 | 1.492 sec/iter\n",
      "Epoch: 342 | Batch: 010 / 011 | Total loss: 0.747 | Reg loss: 0.026 | Tree loss: 0.747 | Accuracy: 0.699659 | 1.492 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 343 | Batch: 000 / 011 | Total loss: 0.858 | Reg loss: 0.026 | Tree loss: 0.858 | Accuracy: 0.654000 | 1.493 sec/iter\n",
      "Epoch: 343 | Batch: 001 / 011 | Total loss: 0.861 | Reg loss: 0.026 | Tree loss: 0.861 | Accuracy: 0.631000 | 1.493 sec/iter\n",
      "Epoch: 343 | Batch: 002 / 011 | Total loss: 0.864 | Reg loss: 0.026 | Tree loss: 0.864 | Accuracy: 0.637000 | 1.492 sec/iter\n",
      "Epoch: 343 | Batch: 003 / 011 | Total loss: 0.814 | Reg loss: 0.026 | Tree loss: 0.814 | Accuracy: 0.681500 | 1.492 sec/iter\n",
      "Epoch: 343 | Batch: 004 / 011 | Total loss: 0.825 | Reg loss: 0.026 | Tree loss: 0.825 | Accuracy: 0.665000 | 1.492 sec/iter\n",
      "Epoch: 343 | Batch: 005 / 011 | Total loss: 0.830 | Reg loss: 0.026 | Tree loss: 0.830 | Accuracy: 0.654000 | 1.492 sec/iter\n",
      "Epoch: 343 | Batch: 006 / 011 | Total loss: 0.830 | Reg loss: 0.026 | Tree loss: 0.830 | Accuracy: 0.622500 | 1.492 sec/iter\n",
      "Epoch: 343 | Batch: 007 / 011 | Total loss: 0.807 | Reg loss: 0.026 | Tree loss: 0.807 | Accuracy: 0.623500 | 1.492 sec/iter\n",
      "Epoch: 343 | Batch: 008 / 011 | Total loss: 0.806 | Reg loss: 0.026 | Tree loss: 0.806 | Accuracy: 0.638000 | 1.492 sec/iter\n",
      "Epoch: 343 | Batch: 009 / 011 | Total loss: 0.793 | Reg loss: 0.026 | Tree loss: 0.793 | Accuracy: 0.625500 | 1.492 sec/iter\n",
      "Epoch: 343 | Batch: 010 / 011 | Total loss: 0.789 | Reg loss: 0.026 | Tree loss: 0.789 | Accuracy: 0.634812 | 1.492 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 344 | Batch: 000 / 011 | Total loss: 0.870 | Reg loss: 0.026 | Tree loss: 0.870 | Accuracy: 0.642500 | 1.492 sec/iter\n",
      "Epoch: 344 | Batch: 001 / 011 | Total loss: 0.851 | Reg loss: 0.026 | Tree loss: 0.851 | Accuracy: 0.659500 | 1.492 sec/iter\n",
      "Epoch: 344 | Batch: 002 / 011 | Total loss: 0.858 | Reg loss: 0.026 | Tree loss: 0.858 | Accuracy: 0.640500 | 1.492 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 344 | Batch: 003 / 011 | Total loss: 0.855 | Reg loss: 0.026 | Tree loss: 0.855 | Accuracy: 0.643500 | 1.492 sec/iter\n",
      "Epoch: 344 | Batch: 004 / 011 | Total loss: 0.830 | Reg loss: 0.026 | Tree loss: 0.830 | Accuracy: 0.651000 | 1.491 sec/iter\n",
      "Epoch: 344 | Batch: 005 / 011 | Total loss: 0.803 | Reg loss: 0.026 | Tree loss: 0.803 | Accuracy: 0.663500 | 1.491 sec/iter\n",
      "Epoch: 344 | Batch: 006 / 011 | Total loss: 0.801 | Reg loss: 0.026 | Tree loss: 0.801 | Accuracy: 0.632500 | 1.491 sec/iter\n",
      "Epoch: 344 | Batch: 007 / 011 | Total loss: 0.807 | Reg loss: 0.026 | Tree loss: 0.807 | Accuracy: 0.626000 | 1.491 sec/iter\n",
      "Epoch: 344 | Batch: 008 / 011 | Total loss: 0.793 | Reg loss: 0.026 | Tree loss: 0.793 | Accuracy: 0.634000 | 1.491 sec/iter\n",
      "Epoch: 344 | Batch: 009 / 011 | Total loss: 0.812 | Reg loss: 0.026 | Tree loss: 0.812 | Accuracy: 0.623000 | 1.491 sec/iter\n",
      "Epoch: 344 | Batch: 010 / 011 | Total loss: 0.839 | Reg loss: 0.026 | Tree loss: 0.839 | Accuracy: 0.593857 | 1.491 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 345 | Batch: 000 / 011 | Total loss: 0.891 | Reg loss: 0.026 | Tree loss: 0.891 | Accuracy: 0.622000 | 1.491 sec/iter\n",
      "Epoch: 345 | Batch: 001 / 011 | Total loss: 0.862 | Reg loss: 0.026 | Tree loss: 0.862 | Accuracy: 0.655000 | 1.491 sec/iter\n",
      "Epoch: 345 | Batch: 002 / 011 | Total loss: 0.843 | Reg loss: 0.026 | Tree loss: 0.843 | Accuracy: 0.651500 | 1.491 sec/iter\n",
      "Epoch: 345 | Batch: 003 / 011 | Total loss: 0.857 | Reg loss: 0.026 | Tree loss: 0.857 | Accuracy: 0.636000 | 1.491 sec/iter\n",
      "Epoch: 345 | Batch: 004 / 011 | Total loss: 0.825 | Reg loss: 0.026 | Tree loss: 0.825 | Accuracy: 0.652500 | 1.491 sec/iter\n",
      "Epoch: 345 | Batch: 005 / 011 | Total loss: 0.808 | Reg loss: 0.026 | Tree loss: 0.808 | Accuracy: 0.646500 | 1.491 sec/iter\n",
      "Epoch: 345 | Batch: 006 / 011 | Total loss: 0.799 | Reg loss: 0.026 | Tree loss: 0.799 | Accuracy: 0.653500 | 1.491 sec/iter\n",
      "Epoch: 345 | Batch: 007 / 011 | Total loss: 0.813 | Reg loss: 0.026 | Tree loss: 0.813 | Accuracy: 0.616500 | 1.49 sec/iter\n",
      "Epoch: 345 | Batch: 008 / 011 | Total loss: 0.806 | Reg loss: 0.026 | Tree loss: 0.806 | Accuracy: 0.637500 | 1.49 sec/iter\n",
      "Epoch: 345 | Batch: 009 / 011 | Total loss: 0.793 | Reg loss: 0.026 | Tree loss: 0.793 | Accuracy: 0.630500 | 1.49 sec/iter\n",
      "Epoch: 345 | Batch: 010 / 011 | Total loss: 0.813 | Reg loss: 0.026 | Tree loss: 0.813 | Accuracy: 0.573379 | 1.49 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 346 | Batch: 000 / 011 | Total loss: 0.855 | Reg loss: 0.026 | Tree loss: 0.855 | Accuracy: 0.641000 | 1.491 sec/iter\n",
      "Epoch: 346 | Batch: 001 / 011 | Total loss: 0.848 | Reg loss: 0.026 | Tree loss: 0.848 | Accuracy: 0.656500 | 1.491 sec/iter\n",
      "Epoch: 346 | Batch: 002 / 011 | Total loss: 0.864 | Reg loss: 0.026 | Tree loss: 0.864 | Accuracy: 0.631500 | 1.49 sec/iter\n",
      "Epoch: 346 | Batch: 003 / 011 | Total loss: 0.841 | Reg loss: 0.026 | Tree loss: 0.841 | Accuracy: 0.658000 | 1.49 sec/iter\n",
      "Epoch: 346 | Batch: 004 / 011 | Total loss: 0.848 | Reg loss: 0.026 | Tree loss: 0.848 | Accuracy: 0.633500 | 1.49 sec/iter\n",
      "Epoch: 346 | Batch: 005 / 011 | Total loss: 0.794 | Reg loss: 0.026 | Tree loss: 0.794 | Accuracy: 0.669000 | 1.49 sec/iter\n",
      "Epoch: 346 | Batch: 006 / 011 | Total loss: 0.809 | Reg loss: 0.026 | Tree loss: 0.809 | Accuracy: 0.653000 | 1.49 sec/iter\n",
      "Epoch: 346 | Batch: 007 / 011 | Total loss: 0.794 | Reg loss: 0.026 | Tree loss: 0.794 | Accuracy: 0.653500 | 1.49 sec/iter\n",
      "Epoch: 346 | Batch: 008 / 011 | Total loss: 0.788 | Reg loss: 0.026 | Tree loss: 0.788 | Accuracy: 0.645000 | 1.49 sec/iter\n",
      "Epoch: 346 | Batch: 009 / 011 | Total loss: 0.822 | Reg loss: 0.026 | Tree loss: 0.822 | Accuracy: 0.628500 | 1.49 sec/iter\n",
      "Epoch: 346 | Batch: 010 / 011 | Total loss: 0.791 | Reg loss: 0.026 | Tree loss: 0.791 | Accuracy: 0.658703 | 1.49 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 347 | Batch: 000 / 011 | Total loss: 0.889 | Reg loss: 0.026 | Tree loss: 0.889 | Accuracy: 0.622000 | 1.49 sec/iter\n",
      "Epoch: 347 | Batch: 001 / 011 | Total loss: 0.836 | Reg loss: 0.026 | Tree loss: 0.836 | Accuracy: 0.672500 | 1.49 sec/iter\n",
      "Epoch: 347 | Batch: 002 / 011 | Total loss: 0.844 | Reg loss: 0.026 | Tree loss: 0.844 | Accuracy: 0.648000 | 1.49 sec/iter\n",
      "Epoch: 347 | Batch: 003 / 011 | Total loss: 0.824 | Reg loss: 0.026 | Tree loss: 0.824 | Accuracy: 0.662000 | 1.49 sec/iter\n",
      "Epoch: 347 | Batch: 004 / 011 | Total loss: 0.807 | Reg loss: 0.026 | Tree loss: 0.807 | Accuracy: 0.662000 | 1.49 sec/iter\n",
      "Epoch: 347 | Batch: 005 / 011 | Total loss: 0.814 | Reg loss: 0.026 | Tree loss: 0.814 | Accuracy: 0.637000 | 1.49 sec/iter\n",
      "Epoch: 347 | Batch: 006 / 011 | Total loss: 0.838 | Reg loss: 0.026 | Tree loss: 0.838 | Accuracy: 0.632500 | 1.489 sec/iter\n",
      "Epoch: 347 | Batch: 007 / 011 | Total loss: 0.803 | Reg loss: 0.026 | Tree loss: 0.803 | Accuracy: 0.633500 | 1.489 sec/iter\n",
      "Epoch: 347 | Batch: 008 / 011 | Total loss: 0.791 | Reg loss: 0.026 | Tree loss: 0.791 | Accuracy: 0.631500 | 1.489 sec/iter\n",
      "Epoch: 347 | Batch: 009 / 011 | Total loss: 0.808 | Reg loss: 0.026 | Tree loss: 0.808 | Accuracy: 0.639500 | 1.489 sec/iter\n",
      "Epoch: 347 | Batch: 010 / 011 | Total loss: 0.780 | Reg loss: 0.026 | Tree loss: 0.780 | Accuracy: 0.651877 | 1.489 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 348 | Batch: 000 / 011 | Total loss: 0.844 | Reg loss: 0.026 | Tree loss: 0.844 | Accuracy: 0.657500 | 1.489 sec/iter\n",
      "Epoch: 348 | Batch: 001 / 011 | Total loss: 0.876 | Reg loss: 0.026 | Tree loss: 0.876 | Accuracy: 0.640500 | 1.489 sec/iter\n",
      "Epoch: 348 | Batch: 002 / 011 | Total loss: 0.848 | Reg loss: 0.026 | Tree loss: 0.848 | Accuracy: 0.649000 | 1.489 sec/iter\n",
      "Epoch: 348 | Batch: 003 / 011 | Total loss: 0.841 | Reg loss: 0.026 | Tree loss: 0.841 | Accuracy: 0.639500 | 1.489 sec/iter\n",
      "Epoch: 348 | Batch: 004 / 011 | Total loss: 0.822 | Reg loss: 0.026 | Tree loss: 0.822 | Accuracy: 0.645500 | 1.489 sec/iter\n",
      "Epoch: 348 | Batch: 005 / 011 | Total loss: 0.812 | Reg loss: 0.026 | Tree loss: 0.812 | Accuracy: 0.648000 | 1.489 sec/iter\n",
      "Epoch: 348 | Batch: 006 / 011 | Total loss: 0.794 | Reg loss: 0.026 | Tree loss: 0.794 | Accuracy: 0.657500 | 1.489 sec/iter\n",
      "Epoch: 348 | Batch: 007 / 011 | Total loss: 0.813 | Reg loss: 0.026 | Tree loss: 0.813 | Accuracy: 0.629000 | 1.489 sec/iter\n",
      "Epoch: 348 | Batch: 008 / 011 | Total loss: 0.822 | Reg loss: 0.026 | Tree loss: 0.822 | Accuracy: 0.615500 | 1.489 sec/iter\n",
      "Epoch: 348 | Batch: 009 / 011 | Total loss: 0.783 | Reg loss: 0.026 | Tree loss: 0.783 | Accuracy: 0.638000 | 1.488 sec/iter\n",
      "Epoch: 348 | Batch: 010 / 011 | Total loss: 0.847 | Reg loss: 0.026 | Tree loss: 0.847 | Accuracy: 0.621160 | 1.488 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 349 | Batch: 000 / 011 | Total loss: 0.893 | Reg loss: 0.026 | Tree loss: 0.893 | Accuracy: 0.630500 | 1.489 sec/iter\n",
      "Epoch: 349 | Batch: 001 / 011 | Total loss: 0.872 | Reg loss: 0.026 | Tree loss: 0.872 | Accuracy: 0.641500 | 1.489 sec/iter\n",
      "Epoch: 349 | Batch: 002 / 011 | Total loss: 0.823 | Reg loss: 0.026 | Tree loss: 0.823 | Accuracy: 0.659500 | 1.488 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 349 | Batch: 003 / 011 | Total loss: 0.829 | Reg loss: 0.026 | Tree loss: 0.829 | Accuracy: 0.664500 | 1.488 sec/iter\n",
      "Epoch: 349 | Batch: 004 / 011 | Total loss: 0.811 | Reg loss: 0.026 | Tree loss: 0.811 | Accuracy: 0.657000 | 1.488 sec/iter\n",
      "Epoch: 349 | Batch: 005 / 011 | Total loss: 0.791 | Reg loss: 0.026 | Tree loss: 0.791 | Accuracy: 0.676000 | 1.488 sec/iter\n",
      "Epoch: 349 | Batch: 006 / 011 | Total loss: 0.818 | Reg loss: 0.026 | Tree loss: 0.818 | Accuracy: 0.611000 | 1.488 sec/iter\n",
      "Epoch: 349 | Batch: 007 / 011 | Total loss: 0.806 | Reg loss: 0.026 | Tree loss: 0.806 | Accuracy: 0.627000 | 1.488 sec/iter\n",
      "Epoch: 349 | Batch: 008 / 011 | Total loss: 0.799 | Reg loss: 0.026 | Tree loss: 0.799 | Accuracy: 0.628000 | 1.488 sec/iter\n",
      "Epoch: 349 | Batch: 009 / 011 | Total loss: 0.812 | Reg loss: 0.026 | Tree loss: 0.812 | Accuracy: 0.635000 | 1.488 sec/iter\n",
      "Epoch: 349 | Batch: 010 / 011 | Total loss: 0.805 | Reg loss: 0.026 | Tree loss: 0.805 | Accuracy: 0.668942 | 1.488 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 350 | Batch: 000 / 011 | Total loss: 0.869 | Reg loss: 0.026 | Tree loss: 0.869 | Accuracy: 0.653500 | 1.488 sec/iter\n",
      "Epoch: 350 | Batch: 001 / 011 | Total loss: 0.848 | Reg loss: 0.026 | Tree loss: 0.848 | Accuracy: 0.665000 | 1.488 sec/iter\n",
      "Epoch: 350 | Batch: 002 / 011 | Total loss: 0.855 | Reg loss: 0.026 | Tree loss: 0.855 | Accuracy: 0.653000 | 1.488 sec/iter\n",
      "Epoch: 350 | Batch: 003 / 011 | Total loss: 0.843 | Reg loss: 0.026 | Tree loss: 0.843 | Accuracy: 0.647000 | 1.488 sec/iter\n",
      "Epoch: 350 | Batch: 004 / 011 | Total loss: 0.814 | Reg loss: 0.026 | Tree loss: 0.814 | Accuracy: 0.654500 | 1.488 sec/iter\n",
      "Epoch: 350 | Batch: 005 / 011 | Total loss: 0.822 | Reg loss: 0.026 | Tree loss: 0.822 | Accuracy: 0.643000 | 1.488 sec/iter\n",
      "Epoch: 350 | Batch: 006 / 011 | Total loss: 0.812 | Reg loss: 0.026 | Tree loss: 0.812 | Accuracy: 0.628500 | 1.488 sec/iter\n",
      "Epoch: 350 | Batch: 007 / 011 | Total loss: 0.812 | Reg loss: 0.026 | Tree loss: 0.812 | Accuracy: 0.622000 | 1.488 sec/iter\n",
      "Epoch: 350 | Batch: 008 / 011 | Total loss: 0.792 | Reg loss: 0.026 | Tree loss: 0.792 | Accuracy: 0.638000 | 1.487 sec/iter\n",
      "Epoch: 350 | Batch: 009 / 011 | Total loss: 0.801 | Reg loss: 0.026 | Tree loss: 0.801 | Accuracy: 0.639500 | 1.487 sec/iter\n",
      "Epoch: 350 | Batch: 010 / 011 | Total loss: 0.785 | Reg loss: 0.026 | Tree loss: 0.785 | Accuracy: 0.631399 | 1.487 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 351 | Batch: 000 / 011 | Total loss: 0.883 | Reg loss: 0.026 | Tree loss: 0.883 | Accuracy: 0.628000 | 1.488 sec/iter\n",
      "Epoch: 351 | Batch: 001 / 011 | Total loss: 0.867 | Reg loss: 0.026 | Tree loss: 0.867 | Accuracy: 0.647500 | 1.487 sec/iter\n",
      "Epoch: 351 | Batch: 002 / 011 | Total loss: 0.845 | Reg loss: 0.026 | Tree loss: 0.845 | Accuracy: 0.658000 | 1.487 sec/iter\n",
      "Epoch: 351 | Batch: 003 / 011 | Total loss: 0.811 | Reg loss: 0.026 | Tree loss: 0.811 | Accuracy: 0.675000 | 1.487 sec/iter\n",
      "Epoch: 351 | Batch: 004 / 011 | Total loss: 0.837 | Reg loss: 0.026 | Tree loss: 0.837 | Accuracy: 0.647500 | 1.487 sec/iter\n",
      "Epoch: 351 | Batch: 005 / 011 | Total loss: 0.816 | Reg loss: 0.026 | Tree loss: 0.816 | Accuracy: 0.656000 | 1.487 sec/iter\n",
      "Epoch: 351 | Batch: 006 / 011 | Total loss: 0.803 | Reg loss: 0.026 | Tree loss: 0.803 | Accuracy: 0.642000 | 1.487 sec/iter\n",
      "Epoch: 351 | Batch: 007 / 011 | Total loss: 0.800 | Reg loss: 0.026 | Tree loss: 0.800 | Accuracy: 0.652500 | 1.487 sec/iter\n",
      "Epoch: 351 | Batch: 008 / 011 | Total loss: 0.793 | Reg loss: 0.026 | Tree loss: 0.793 | Accuracy: 0.639500 | 1.487 sec/iter\n",
      "Epoch: 351 | Batch: 009 / 011 | Total loss: 0.820 | Reg loss: 0.026 | Tree loss: 0.820 | Accuracy: 0.606000 | 1.486 sec/iter\n",
      "Epoch: 351 | Batch: 010 / 011 | Total loss: 0.787 | Reg loss: 0.026 | Tree loss: 0.787 | Accuracy: 0.634812 | 1.486 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 352 | Batch: 000 / 011 | Total loss: 0.883 | Reg loss: 0.026 | Tree loss: 0.883 | Accuracy: 0.620000 | 1.486 sec/iter\n",
      "Epoch: 352 | Batch: 001 / 011 | Total loss: 0.881 | Reg loss: 0.026 | Tree loss: 0.881 | Accuracy: 0.647500 | 1.486 sec/iter\n",
      "Epoch: 352 | Batch: 002 / 011 | Total loss: 0.835 | Reg loss: 0.026 | Tree loss: 0.835 | Accuracy: 0.668000 | 1.486 sec/iter\n",
      "Epoch: 352 | Batch: 003 / 011 | Total loss: 0.810 | Reg loss: 0.026 | Tree loss: 0.810 | Accuracy: 0.672000 | 1.486 sec/iter\n",
      "Epoch: 352 | Batch: 004 / 011 | Total loss: 0.835 | Reg loss: 0.026 | Tree loss: 0.835 | Accuracy: 0.650500 | 1.486 sec/iter\n",
      "Epoch: 352 | Batch: 005 / 011 | Total loss: 0.787 | Reg loss: 0.026 | Tree loss: 0.787 | Accuracy: 0.673500 | 1.486 sec/iter\n",
      "Epoch: 352 | Batch: 006 / 011 | Total loss: 0.815 | Reg loss: 0.026 | Tree loss: 0.815 | Accuracy: 0.631000 | 1.486 sec/iter\n",
      "Epoch: 352 | Batch: 007 / 011 | Total loss: 0.813 | Reg loss: 0.026 | Tree loss: 0.813 | Accuracy: 0.631000 | 1.486 sec/iter\n",
      "Epoch: 352 | Batch: 008 / 011 | Total loss: 0.814 | Reg loss: 0.026 | Tree loss: 0.814 | Accuracy: 0.617000 | 1.485 sec/iter\n",
      "Epoch: 352 | Batch: 009 / 011 | Total loss: 0.801 | Reg loss: 0.026 | Tree loss: 0.801 | Accuracy: 0.630500 | 1.485 sec/iter\n",
      "Epoch: 352 | Batch: 010 / 011 | Total loss: 0.802 | Reg loss: 0.026 | Tree loss: 0.802 | Accuracy: 0.621160 | 1.485 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 353 | Batch: 000 / 011 | Total loss: 0.873 | Reg loss: 0.026 | Tree loss: 0.873 | Accuracy: 0.640000 | 1.486 sec/iter\n",
      "Epoch: 353 | Batch: 001 / 011 | Total loss: 0.861 | Reg loss: 0.026 | Tree loss: 0.861 | Accuracy: 0.649000 | 1.486 sec/iter\n",
      "Epoch: 353 | Batch: 002 / 011 | Total loss: 0.849 | Reg loss: 0.026 | Tree loss: 0.849 | Accuracy: 0.642500 | 1.486 sec/iter\n",
      "Epoch: 353 | Batch: 003 / 011 | Total loss: 0.818 | Reg loss: 0.026 | Tree loss: 0.818 | Accuracy: 0.665500 | 1.486 sec/iter\n",
      "Epoch: 353 | Batch: 004 / 011 | Total loss: 0.846 | Reg loss: 0.026 | Tree loss: 0.846 | Accuracy: 0.630000 | 1.486 sec/iter\n",
      "Epoch: 353 | Batch: 005 / 011 | Total loss: 0.815 | Reg loss: 0.026 | Tree loss: 0.815 | Accuracy: 0.639000 | 1.486 sec/iter\n",
      "Epoch: 353 | Batch: 006 / 011 | Total loss: 0.824 | Reg loss: 0.026 | Tree loss: 0.824 | Accuracy: 0.632000 | 1.486 sec/iter\n",
      "Epoch: 353 | Batch: 007 / 011 | Total loss: 0.787 | Reg loss: 0.026 | Tree loss: 0.787 | Accuracy: 0.639000 | 1.486 sec/iter\n",
      "Epoch: 353 | Batch: 008 / 011 | Total loss: 0.807 | Reg loss: 0.026 | Tree loss: 0.807 | Accuracy: 0.632000 | 1.485 sec/iter\n",
      "Epoch: 353 | Batch: 009 / 011 | Total loss: 0.811 | Reg loss: 0.026 | Tree loss: 0.811 | Accuracy: 0.631500 | 1.485 sec/iter\n",
      "Epoch: 353 | Batch: 010 / 011 | Total loss: 0.818 | Reg loss: 0.026 | Tree loss: 0.818 | Accuracy: 0.638225 | 1.485 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 354 | Batch: 000 / 011 | Total loss: 0.868 | Reg loss: 0.026 | Tree loss: 0.868 | Accuracy: 0.641000 | 1.486 sec/iter\n",
      "Epoch: 354 | Batch: 001 / 011 | Total loss: 0.884 | Reg loss: 0.026 | Tree loss: 0.884 | Accuracy: 0.631000 | 1.486 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 354 | Batch: 002 / 011 | Total loss: 0.860 | Reg loss: 0.026 | Tree loss: 0.860 | Accuracy: 0.649000 | 1.485 sec/iter\n",
      "Epoch: 354 | Batch: 003 / 011 | Total loss: 0.860 | Reg loss: 0.026 | Tree loss: 0.860 | Accuracy: 0.643500 | 1.485 sec/iter\n",
      "Epoch: 354 | Batch: 004 / 011 | Total loss: 0.815 | Reg loss: 0.026 | Tree loss: 0.815 | Accuracy: 0.660000 | 1.485 sec/iter\n",
      "Epoch: 354 | Batch: 005 / 011 | Total loss: 0.820 | Reg loss: 0.026 | Tree loss: 0.820 | Accuracy: 0.653000 | 1.485 sec/iter\n",
      "Epoch: 354 | Batch: 006 / 011 | Total loss: 0.820 | Reg loss: 0.026 | Tree loss: 0.820 | Accuracy: 0.632500 | 1.485 sec/iter\n",
      "Epoch: 354 | Batch: 007 / 011 | Total loss: 0.807 | Reg loss: 0.026 | Tree loss: 0.807 | Accuracy: 0.623500 | 1.485 sec/iter\n",
      "Epoch: 354 | Batch: 008 / 011 | Total loss: 0.806 | Reg loss: 0.026 | Tree loss: 0.806 | Accuracy: 0.627500 | 1.485 sec/iter\n",
      "Epoch: 354 | Batch: 009 / 011 | Total loss: 0.779 | Reg loss: 0.026 | Tree loss: 0.779 | Accuracy: 0.648500 | 1.485 sec/iter\n",
      "Epoch: 354 | Batch: 010 / 011 | Total loss: 0.806 | Reg loss: 0.026 | Tree loss: 0.806 | Accuracy: 0.631399 | 1.485 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 355 | Batch: 000 / 011 | Total loss: 0.870 | Reg loss: 0.026 | Tree loss: 0.870 | Accuracy: 0.640500 | 1.485 sec/iter\n",
      "Epoch: 355 | Batch: 001 / 011 | Total loss: 0.882 | Reg loss: 0.026 | Tree loss: 0.882 | Accuracy: 0.640500 | 1.485 sec/iter\n",
      "Epoch: 355 | Batch: 002 / 011 | Total loss: 0.855 | Reg loss: 0.026 | Tree loss: 0.855 | Accuracy: 0.638500 | 1.485 sec/iter\n",
      "Epoch: 355 | Batch: 003 / 011 | Total loss: 0.827 | Reg loss: 0.026 | Tree loss: 0.827 | Accuracy: 0.663000 | 1.485 sec/iter\n",
      "Epoch: 355 | Batch: 004 / 011 | Total loss: 0.829 | Reg loss: 0.026 | Tree loss: 0.829 | Accuracy: 0.645000 | 1.485 sec/iter\n",
      "Epoch: 355 | Batch: 005 / 011 | Total loss: 0.785 | Reg loss: 0.026 | Tree loss: 0.785 | Accuracy: 0.672000 | 1.485 sec/iter\n",
      "Epoch: 355 | Batch: 006 / 011 | Total loss: 0.807 | Reg loss: 0.026 | Tree loss: 0.807 | Accuracy: 0.642000 | 1.484 sec/iter\n",
      "Epoch: 355 | Batch: 007 / 011 | Total loss: 0.812 | Reg loss: 0.026 | Tree loss: 0.812 | Accuracy: 0.635000 | 1.484 sec/iter\n",
      "Epoch: 355 | Batch: 008 / 011 | Total loss: 0.805 | Reg loss: 0.026 | Tree loss: 0.805 | Accuracy: 0.616500 | 1.484 sec/iter\n",
      "Epoch: 355 | Batch: 009 / 011 | Total loss: 0.792 | Reg loss: 0.026 | Tree loss: 0.792 | Accuracy: 0.638500 | 1.484 sec/iter\n",
      "Epoch: 355 | Batch: 010 / 011 | Total loss: 0.828 | Reg loss: 0.026 | Tree loss: 0.828 | Accuracy: 0.597270 | 1.484 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 356 | Batch: 000 / 011 | Total loss: 0.863 | Reg loss: 0.026 | Tree loss: 0.863 | Accuracy: 0.638500 | 1.485 sec/iter\n",
      "Epoch: 356 | Batch: 001 / 011 | Total loss: 0.868 | Reg loss: 0.026 | Tree loss: 0.868 | Accuracy: 0.645500 | 1.484 sec/iter\n",
      "Epoch: 356 | Batch: 002 / 011 | Total loss: 0.835 | Reg loss: 0.026 | Tree loss: 0.835 | Accuracy: 0.668000 | 1.484 sec/iter\n",
      "Epoch: 356 | Batch: 003 / 011 | Total loss: 0.838 | Reg loss: 0.026 | Tree loss: 0.838 | Accuracy: 0.647000 | 1.484 sec/iter\n",
      "Epoch: 356 | Batch: 004 / 011 | Total loss: 0.807 | Reg loss: 0.026 | Tree loss: 0.807 | Accuracy: 0.658000 | 1.484 sec/iter\n",
      "Epoch: 356 | Batch: 005 / 011 | Total loss: 0.819 | Reg loss: 0.026 | Tree loss: 0.819 | Accuracy: 0.642000 | 1.484 sec/iter\n",
      "Epoch: 356 | Batch: 006 / 011 | Total loss: 0.842 | Reg loss: 0.026 | Tree loss: 0.842 | Accuracy: 0.618500 | 1.484 sec/iter\n",
      "Epoch: 356 | Batch: 007 / 011 | Total loss: 0.809 | Reg loss: 0.026 | Tree loss: 0.809 | Accuracy: 0.629000 | 1.484 sec/iter\n",
      "Epoch: 356 | Batch: 008 / 011 | Total loss: 0.802 | Reg loss: 0.026 | Tree loss: 0.802 | Accuracy: 0.620500 | 1.484 sec/iter\n",
      "Epoch: 356 | Batch: 009 / 011 | Total loss: 0.778 | Reg loss: 0.026 | Tree loss: 0.778 | Accuracy: 0.634500 | 1.484 sec/iter\n",
      "Epoch: 356 | Batch: 010 / 011 | Total loss: 0.766 | Reg loss: 0.026 | Tree loss: 0.766 | Accuracy: 0.638225 | 1.484 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 357 | Batch: 000 / 011 | Total loss: 0.879 | Reg loss: 0.026 | Tree loss: 0.879 | Accuracy: 0.631500 | 1.484 sec/iter\n",
      "Epoch: 357 | Batch: 001 / 011 | Total loss: 0.884 | Reg loss: 0.026 | Tree loss: 0.884 | Accuracy: 0.647500 | 1.484 sec/iter\n",
      "Epoch: 357 | Batch: 002 / 011 | Total loss: 0.834 | Reg loss: 0.026 | Tree loss: 0.834 | Accuracy: 0.673000 | 1.484 sec/iter\n",
      "Epoch: 357 | Batch: 003 / 011 | Total loss: 0.829 | Reg loss: 0.026 | Tree loss: 0.829 | Accuracy: 0.655500 | 1.484 sec/iter\n",
      "Epoch: 357 | Batch: 004 / 011 | Total loss: 0.806 | Reg loss: 0.026 | Tree loss: 0.806 | Accuracy: 0.666000 | 1.484 sec/iter\n",
      "Epoch: 357 | Batch: 005 / 011 | Total loss: 0.804 | Reg loss: 0.026 | Tree loss: 0.804 | Accuracy: 0.665000 | 1.484 sec/iter\n",
      "Epoch: 357 | Batch: 006 / 011 | Total loss: 0.814 | Reg loss: 0.026 | Tree loss: 0.814 | Accuracy: 0.648000 | 1.483 sec/iter\n",
      "Epoch: 357 | Batch: 007 / 011 | Total loss: 0.817 | Reg loss: 0.026 | Tree loss: 0.817 | Accuracy: 0.621500 | 1.483 sec/iter\n",
      "Epoch: 357 | Batch: 008 / 011 | Total loss: 0.822 | Reg loss: 0.026 | Tree loss: 0.822 | Accuracy: 0.608500 | 1.483 sec/iter\n",
      "Epoch: 357 | Batch: 009 / 011 | Total loss: 0.780 | Reg loss: 0.026 | Tree loss: 0.780 | Accuracy: 0.657500 | 1.483 sec/iter\n",
      "Epoch: 357 | Batch: 010 / 011 | Total loss: 0.818 | Reg loss: 0.026 | Tree loss: 0.818 | Accuracy: 0.593857 | 1.483 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 358 | Batch: 000 / 011 | Total loss: 0.891 | Reg loss: 0.026 | Tree loss: 0.891 | Accuracy: 0.636000 | 1.484 sec/iter\n",
      "Epoch: 358 | Batch: 001 / 011 | Total loss: 0.884 | Reg loss: 0.026 | Tree loss: 0.884 | Accuracy: 0.636000 | 1.484 sec/iter\n",
      "Epoch: 358 | Batch: 002 / 011 | Total loss: 0.855 | Reg loss: 0.026 | Tree loss: 0.855 | Accuracy: 0.640000 | 1.484 sec/iter\n",
      "Epoch: 358 | Batch: 003 / 011 | Total loss: 0.831 | Reg loss: 0.026 | Tree loss: 0.831 | Accuracy: 0.655500 | 1.484 sec/iter\n",
      "Epoch: 358 | Batch: 004 / 011 | Total loss: 0.837 | Reg loss: 0.026 | Tree loss: 0.837 | Accuracy: 0.628000 | 1.484 sec/iter\n",
      "Epoch: 358 | Batch: 005 / 011 | Total loss: 0.814 | Reg loss: 0.026 | Tree loss: 0.814 | Accuracy: 0.647000 | 1.484 sec/iter\n",
      "Epoch: 358 | Batch: 006 / 011 | Total loss: 0.803 | Reg loss: 0.026 | Tree loss: 0.803 | Accuracy: 0.640000 | 1.484 sec/iter\n",
      "Epoch: 358 | Batch: 007 / 011 | Total loss: 0.804 | Reg loss: 0.026 | Tree loss: 0.804 | Accuracy: 0.628500 | 1.483 sec/iter\n",
      "Epoch: 358 | Batch: 008 / 011 | Total loss: 0.792 | Reg loss: 0.026 | Tree loss: 0.792 | Accuracy: 0.635500 | 1.483 sec/iter\n",
      "Epoch: 358 | Batch: 009 / 011 | Total loss: 0.771 | Reg loss: 0.026 | Tree loss: 0.771 | Accuracy: 0.650000 | 1.483 sec/iter\n",
      "Epoch: 358 | Batch: 010 / 011 | Total loss: 0.788 | Reg loss: 0.026 | Tree loss: 0.788 | Accuracy: 0.634812 | 1.483 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 359 | Batch: 000 / 011 | Total loss: 0.883 | Reg loss: 0.026 | Tree loss: 0.883 | Accuracy: 0.633000 | 1.484 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 359 | Batch: 001 / 011 | Total loss: 0.874 | Reg loss: 0.026 | Tree loss: 0.874 | Accuracy: 0.633000 | 1.484 sec/iter\n",
      "Epoch: 359 | Batch: 002 / 011 | Total loss: 0.864 | Reg loss: 0.026 | Tree loss: 0.864 | Accuracy: 0.645000 | 1.484 sec/iter\n",
      "Epoch: 359 | Batch: 003 / 011 | Total loss: 0.820 | Reg loss: 0.026 | Tree loss: 0.820 | Accuracy: 0.663000 | 1.484 sec/iter\n",
      "Epoch: 359 | Batch: 004 / 011 | Total loss: 0.816 | Reg loss: 0.026 | Tree loss: 0.816 | Accuracy: 0.670000 | 1.483 sec/iter\n",
      "Epoch: 359 | Batch: 005 / 011 | Total loss: 0.839 | Reg loss: 0.026 | Tree loss: 0.839 | Accuracy: 0.639500 | 1.483 sec/iter\n",
      "Epoch: 359 | Batch: 006 / 011 | Total loss: 0.801 | Reg loss: 0.026 | Tree loss: 0.801 | Accuracy: 0.651000 | 1.483 sec/iter\n",
      "Epoch: 359 | Batch: 007 / 011 | Total loss: 0.816 | Reg loss: 0.026 | Tree loss: 0.816 | Accuracy: 0.628000 | 1.483 sec/iter\n",
      "Epoch: 359 | Batch: 008 / 011 | Total loss: 0.785 | Reg loss: 0.026 | Tree loss: 0.785 | Accuracy: 0.637500 | 1.483 sec/iter\n",
      "Epoch: 359 | Batch: 009 / 011 | Total loss: 0.796 | Reg loss: 0.026 | Tree loss: 0.796 | Accuracy: 0.637000 | 1.483 sec/iter\n",
      "Epoch: 359 | Batch: 010 / 011 | Total loss: 0.766 | Reg loss: 0.026 | Tree loss: 0.766 | Accuracy: 0.668942 | 1.483 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 360 | Batch: 000 / 011 | Total loss: 0.886 | Reg loss: 0.026 | Tree loss: 0.886 | Accuracy: 0.625000 | 1.484 sec/iter\n",
      "Epoch: 360 | Batch: 001 / 011 | Total loss: 0.867 | Reg loss: 0.026 | Tree loss: 0.867 | Accuracy: 0.643500 | 1.483 sec/iter\n",
      "Epoch: 360 | Batch: 002 / 011 | Total loss: 0.836 | Reg loss: 0.026 | Tree loss: 0.836 | Accuracy: 0.652500 | 1.483 sec/iter\n",
      "Epoch: 360 | Batch: 003 / 011 | Total loss: 0.840 | Reg loss: 0.026 | Tree loss: 0.840 | Accuracy: 0.646000 | 1.483 sec/iter\n",
      "Epoch: 360 | Batch: 004 / 011 | Total loss: 0.835 | Reg loss: 0.026 | Tree loss: 0.835 | Accuracy: 0.644000 | 1.483 sec/iter\n",
      "Epoch: 360 | Batch: 005 / 011 | Total loss: 0.814 | Reg loss: 0.026 | Tree loss: 0.814 | Accuracy: 0.636000 | 1.483 sec/iter\n",
      "Epoch: 360 | Batch: 006 / 011 | Total loss: 0.779 | Reg loss: 0.026 | Tree loss: 0.779 | Accuracy: 0.657000 | 1.483 sec/iter\n",
      "Epoch: 360 | Batch: 007 / 011 | Total loss: 0.814 | Reg loss: 0.026 | Tree loss: 0.814 | Accuracy: 0.632500 | 1.483 sec/iter\n",
      "Epoch: 360 | Batch: 008 / 011 | Total loss: 0.805 | Reg loss: 0.026 | Tree loss: 0.805 | Accuracy: 0.626500 | 1.483 sec/iter\n",
      "Epoch: 360 | Batch: 009 / 011 | Total loss: 0.777 | Reg loss: 0.026 | Tree loss: 0.777 | Accuracy: 0.645000 | 1.483 sec/iter\n",
      "Epoch: 360 | Batch: 010 / 011 | Total loss: 0.848 | Reg loss: 0.026 | Tree loss: 0.848 | Accuracy: 0.604096 | 1.483 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 361 | Batch: 000 / 011 | Total loss: 0.887 | Reg loss: 0.026 | Tree loss: 0.887 | Accuracy: 0.622000 | 1.483 sec/iter\n",
      "Epoch: 361 | Batch: 001 / 011 | Total loss: 0.852 | Reg loss: 0.026 | Tree loss: 0.852 | Accuracy: 0.652500 | 1.483 sec/iter\n",
      "Epoch: 361 | Batch: 002 / 011 | Total loss: 0.859 | Reg loss: 0.026 | Tree loss: 0.859 | Accuracy: 0.649500 | 1.483 sec/iter\n",
      "Epoch: 361 | Batch: 003 / 011 | Total loss: 0.835 | Reg loss: 0.026 | Tree loss: 0.835 | Accuracy: 0.647000 | 1.483 sec/iter\n",
      "Epoch: 361 | Batch: 004 / 011 | Total loss: 0.814 | Reg loss: 0.026 | Tree loss: 0.814 | Accuracy: 0.659000 | 1.483 sec/iter\n",
      "Epoch: 361 | Batch: 005 / 011 | Total loss: 0.805 | Reg loss: 0.026 | Tree loss: 0.805 | Accuracy: 0.650000 | 1.483 sec/iter\n",
      "Epoch: 361 | Batch: 006 / 011 | Total loss: 0.815 | Reg loss: 0.026 | Tree loss: 0.815 | Accuracy: 0.636500 | 1.483 sec/iter\n",
      "Epoch: 361 | Batch: 007 / 011 | Total loss: 0.797 | Reg loss: 0.026 | Tree loss: 0.797 | Accuracy: 0.635000 | 1.483 sec/iter\n",
      "Epoch: 361 | Batch: 008 / 011 | Total loss: 0.807 | Reg loss: 0.026 | Tree loss: 0.807 | Accuracy: 0.620000 | 1.483 sec/iter\n",
      "Epoch: 361 | Batch: 009 / 011 | Total loss: 0.794 | Reg loss: 0.026 | Tree loss: 0.794 | Accuracy: 0.650000 | 1.483 sec/iter\n",
      "Epoch: 361 | Batch: 010 / 011 | Total loss: 0.853 | Reg loss: 0.026 | Tree loss: 0.853 | Accuracy: 0.580205 | 1.483 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 362 | Batch: 000 / 011 | Total loss: 0.879 | Reg loss: 0.026 | Tree loss: 0.879 | Accuracy: 0.629500 | 1.483 sec/iter\n",
      "Epoch: 362 | Batch: 001 / 011 | Total loss: 0.873 | Reg loss: 0.026 | Tree loss: 0.873 | Accuracy: 0.645000 | 1.483 sec/iter\n",
      "Epoch: 362 | Batch: 002 / 011 | Total loss: 0.835 | Reg loss: 0.026 | Tree loss: 0.835 | Accuracy: 0.663000 | 1.483 sec/iter\n",
      "Epoch: 362 | Batch: 003 / 011 | Total loss: 0.854 | Reg loss: 0.026 | Tree loss: 0.854 | Accuracy: 0.635500 | 1.483 sec/iter\n",
      "Epoch: 362 | Batch: 004 / 011 | Total loss: 0.827 | Reg loss: 0.026 | Tree loss: 0.827 | Accuracy: 0.653000 | 1.483 sec/iter\n",
      "Epoch: 362 | Batch: 005 / 011 | Total loss: 0.820 | Reg loss: 0.026 | Tree loss: 0.820 | Accuracy: 0.627500 | 1.483 sec/iter\n",
      "Epoch: 362 | Batch: 006 / 011 | Total loss: 0.789 | Reg loss: 0.026 | Tree loss: 0.789 | Accuracy: 0.648500 | 1.483 sec/iter\n",
      "Epoch: 362 | Batch: 007 / 011 | Total loss: 0.799 | Reg loss: 0.026 | Tree loss: 0.799 | Accuracy: 0.636500 | 1.483 sec/iter\n",
      "Epoch: 362 | Batch: 008 / 011 | Total loss: 0.810 | Reg loss: 0.026 | Tree loss: 0.810 | Accuracy: 0.637500 | 1.483 sec/iter\n",
      "Epoch: 362 | Batch: 009 / 011 | Total loss: 0.796 | Reg loss: 0.026 | Tree loss: 0.796 | Accuracy: 0.644000 | 1.483 sec/iter\n",
      "Epoch: 362 | Batch: 010 / 011 | Total loss: 0.729 | Reg loss: 0.026 | Tree loss: 0.729 | Accuracy: 0.692833 | 1.483 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 363 | Batch: 000 / 011 | Total loss: 0.860 | Reg loss: 0.026 | Tree loss: 0.860 | Accuracy: 0.658500 | 1.483 sec/iter\n",
      "Epoch: 363 | Batch: 001 / 011 | Total loss: 0.890 | Reg loss: 0.026 | Tree loss: 0.890 | Accuracy: 0.645500 | 1.483 sec/iter\n",
      "Epoch: 363 | Batch: 002 / 011 | Total loss: 0.854 | Reg loss: 0.026 | Tree loss: 0.854 | Accuracy: 0.650000 | 1.483 sec/iter\n",
      "Epoch: 363 | Batch: 003 / 011 | Total loss: 0.847 | Reg loss: 0.026 | Tree loss: 0.847 | Accuracy: 0.646000 | 1.483 sec/iter\n",
      "Epoch: 363 | Batch: 004 / 011 | Total loss: 0.805 | Reg loss: 0.026 | Tree loss: 0.805 | Accuracy: 0.673000 | 1.483 sec/iter\n",
      "Epoch: 363 | Batch: 005 / 011 | Total loss: 0.817 | Reg loss: 0.026 | Tree loss: 0.817 | Accuracy: 0.638500 | 1.483 sec/iter\n",
      "Epoch: 363 | Batch: 006 / 011 | Total loss: 0.811 | Reg loss: 0.026 | Tree loss: 0.811 | Accuracy: 0.629500 | 1.483 sec/iter\n",
      "Epoch: 363 | Batch: 007 / 011 | Total loss: 0.790 | Reg loss: 0.026 | Tree loss: 0.790 | Accuracy: 0.634500 | 1.483 sec/iter\n",
      "Epoch: 363 | Batch: 008 / 011 | Total loss: 0.802 | Reg loss: 0.026 | Tree loss: 0.802 | Accuracy: 0.615500 | 1.483 sec/iter\n",
      "Epoch: 363 | Batch: 009 / 011 | Total loss: 0.806 | Reg loss: 0.026 | Tree loss: 0.806 | Accuracy: 0.623500 | 1.483 sec/iter\n",
      "Epoch: 363 | Batch: 010 / 011 | Total loss: 0.749 | Reg loss: 0.026 | Tree loss: 0.749 | Accuracy: 0.655290 | 1.483 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 364 | Batch: 000 / 011 | Total loss: 0.849 | Reg loss: 0.026 | Tree loss: 0.849 | Accuracy: 0.654000 | 1.483 sec/iter\n",
      "Epoch: 364 | Batch: 001 / 011 | Total loss: 0.883 | Reg loss: 0.026 | Tree loss: 0.883 | Accuracy: 0.641500 | 1.483 sec/iter\n",
      "Epoch: 364 | Batch: 002 / 011 | Total loss: 0.861 | Reg loss: 0.026 | Tree loss: 0.861 | Accuracy: 0.637500 | 1.483 sec/iter\n",
      "Epoch: 364 | Batch: 003 / 011 | Total loss: 0.816 | Reg loss: 0.026 | Tree loss: 0.816 | Accuracy: 0.661000 | 1.483 sec/iter\n",
      "Epoch: 364 | Batch: 004 / 011 | Total loss: 0.826 | Reg loss: 0.026 | Tree loss: 0.826 | Accuracy: 0.659000 | 1.483 sec/iter\n",
      "Epoch: 364 | Batch: 005 / 011 | Total loss: 0.827 | Reg loss: 0.026 | Tree loss: 0.827 | Accuracy: 0.643000 | 1.483 sec/iter\n",
      "Epoch: 364 | Batch: 006 / 011 | Total loss: 0.806 | Reg loss: 0.026 | Tree loss: 0.806 | Accuracy: 0.646000 | 1.483 sec/iter\n",
      "Epoch: 364 | Batch: 007 / 011 | Total loss: 0.804 | Reg loss: 0.026 | Tree loss: 0.804 | Accuracy: 0.625500 | 1.483 sec/iter\n",
      "Epoch: 364 | Batch: 008 / 011 | Total loss: 0.821 | Reg loss: 0.026 | Tree loss: 0.821 | Accuracy: 0.616000 | 1.483 sec/iter\n",
      "Epoch: 364 | Batch: 009 / 011 | Total loss: 0.792 | Reg loss: 0.026 | Tree loss: 0.792 | Accuracy: 0.644000 | 1.483 sec/iter\n",
      "Epoch: 364 | Batch: 010 / 011 | Total loss: 0.797 | Reg loss: 0.026 | Tree loss: 0.797 | Accuracy: 0.607509 | 1.483 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 365 | Batch: 000 / 011 | Total loss: 0.861 | Reg loss: 0.026 | Tree loss: 0.861 | Accuracy: 0.643000 | 1.484 sec/iter\n",
      "Epoch: 365 | Batch: 001 / 011 | Total loss: 0.869 | Reg loss: 0.026 | Tree loss: 0.869 | Accuracy: 0.632000 | 1.484 sec/iter\n",
      "Epoch: 365 | Batch: 002 / 011 | Total loss: 0.855 | Reg loss: 0.026 | Tree loss: 0.855 | Accuracy: 0.644000 | 1.483 sec/iter\n",
      "Epoch: 365 | Batch: 003 / 011 | Total loss: 0.834 | Reg loss: 0.026 | Tree loss: 0.834 | Accuracy: 0.651500 | 1.483 sec/iter\n",
      "Epoch: 365 | Batch: 004 / 011 | Total loss: 0.811 | Reg loss: 0.026 | Tree loss: 0.811 | Accuracy: 0.652500 | 1.483 sec/iter\n",
      "Epoch: 365 | Batch: 005 / 011 | Total loss: 0.809 | Reg loss: 0.026 | Tree loss: 0.809 | Accuracy: 0.654500 | 1.483 sec/iter\n",
      "Epoch: 365 | Batch: 006 / 011 | Total loss: 0.824 | Reg loss: 0.026 | Tree loss: 0.824 | Accuracy: 0.632500 | 1.483 sec/iter\n",
      "Epoch: 365 | Batch: 007 / 011 | Total loss: 0.778 | Reg loss: 0.026 | Tree loss: 0.778 | Accuracy: 0.650500 | 1.483 sec/iter\n",
      "Epoch: 365 | Batch: 008 / 011 | Total loss: 0.816 | Reg loss: 0.026 | Tree loss: 0.816 | Accuracy: 0.619500 | 1.483 sec/iter\n",
      "Epoch: 365 | Batch: 009 / 011 | Total loss: 0.816 | Reg loss: 0.026 | Tree loss: 0.816 | Accuracy: 0.626000 | 1.483 sec/iter\n",
      "Epoch: 365 | Batch: 010 / 011 | Total loss: 0.800 | Reg loss: 0.026 | Tree loss: 0.800 | Accuracy: 0.631399 | 1.483 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 366 | Batch: 000 / 011 | Total loss: 0.880 | Reg loss: 0.026 | Tree loss: 0.880 | Accuracy: 0.633000 | 1.484 sec/iter\n",
      "Epoch: 366 | Batch: 001 / 011 | Total loss: 0.860 | Reg loss: 0.026 | Tree loss: 0.860 | Accuracy: 0.642500 | 1.484 sec/iter\n",
      "Epoch: 366 | Batch: 002 / 011 | Total loss: 0.862 | Reg loss: 0.026 | Tree loss: 0.862 | Accuracy: 0.647000 | 1.483 sec/iter\n",
      "Epoch: 366 | Batch: 003 / 011 | Total loss: 0.811 | Reg loss: 0.026 | Tree loss: 0.811 | Accuracy: 0.665500 | 1.483 sec/iter\n",
      "Epoch: 366 | Batch: 004 / 011 | Total loss: 0.817 | Reg loss: 0.026 | Tree loss: 0.817 | Accuracy: 0.646000 | 1.483 sec/iter\n",
      "Epoch: 366 | Batch: 005 / 011 | Total loss: 0.819 | Reg loss: 0.026 | Tree loss: 0.819 | Accuracy: 0.644000 | 1.483 sec/iter\n",
      "Epoch: 366 | Batch: 006 / 011 | Total loss: 0.803 | Reg loss: 0.026 | Tree loss: 0.803 | Accuracy: 0.641000 | 1.483 sec/iter\n",
      "Epoch: 366 | Batch: 007 / 011 | Total loss: 0.818 | Reg loss: 0.026 | Tree loss: 0.818 | Accuracy: 0.620000 | 1.483 sec/iter\n",
      "Epoch: 366 | Batch: 008 / 011 | Total loss: 0.811 | Reg loss: 0.026 | Tree loss: 0.811 | Accuracy: 0.628000 | 1.483 sec/iter\n",
      "Epoch: 366 | Batch: 009 / 011 | Total loss: 0.779 | Reg loss: 0.026 | Tree loss: 0.779 | Accuracy: 0.649000 | 1.483 sec/iter\n",
      "Epoch: 366 | Batch: 010 / 011 | Total loss: 0.773 | Reg loss: 0.026 | Tree loss: 0.773 | Accuracy: 0.655290 | 1.483 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 367 | Batch: 000 / 011 | Total loss: 0.887 | Reg loss: 0.026 | Tree loss: 0.887 | Accuracy: 0.637500 | 1.483 sec/iter\n",
      "Epoch: 367 | Batch: 001 / 011 | Total loss: 0.862 | Reg loss: 0.026 | Tree loss: 0.862 | Accuracy: 0.645000 | 1.483 sec/iter\n",
      "Epoch: 367 | Batch: 002 / 011 | Total loss: 0.853 | Reg loss: 0.026 | Tree loss: 0.853 | Accuracy: 0.658500 | 1.483 sec/iter\n",
      "Epoch: 367 | Batch: 003 / 011 | Total loss: 0.829 | Reg loss: 0.026 | Tree loss: 0.829 | Accuracy: 0.655500 | 1.483 sec/iter\n",
      "Epoch: 367 | Batch: 004 / 011 | Total loss: 0.800 | Reg loss: 0.026 | Tree loss: 0.800 | Accuracy: 0.653500 | 1.483 sec/iter\n",
      "Epoch: 367 | Batch: 005 / 011 | Total loss: 0.822 | Reg loss: 0.026 | Tree loss: 0.822 | Accuracy: 0.654500 | 1.483 sec/iter\n",
      "Epoch: 367 | Batch: 006 / 011 | Total loss: 0.782 | Reg loss: 0.026 | Tree loss: 0.782 | Accuracy: 0.662500 | 1.483 sec/iter\n",
      "Epoch: 367 | Batch: 007 / 011 | Total loss: 0.817 | Reg loss: 0.026 | Tree loss: 0.817 | Accuracy: 0.631500 | 1.483 sec/iter\n",
      "Epoch: 367 | Batch: 008 / 011 | Total loss: 0.794 | Reg loss: 0.026 | Tree loss: 0.794 | Accuracy: 0.632000 | 1.483 sec/iter\n",
      "Epoch: 367 | Batch: 009 / 011 | Total loss: 0.812 | Reg loss: 0.026 | Tree loss: 0.812 | Accuracy: 0.624000 | 1.483 sec/iter\n",
      "Epoch: 367 | Batch: 010 / 011 | Total loss: 0.765 | Reg loss: 0.026 | Tree loss: 0.765 | Accuracy: 0.672355 | 1.483 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 368 | Batch: 000 / 011 | Total loss: 0.860 | Reg loss: 0.026 | Tree loss: 0.860 | Accuracy: 0.643500 | 1.483 sec/iter\n",
      "Epoch: 368 | Batch: 001 / 011 | Total loss: 0.850 | Reg loss: 0.026 | Tree loss: 0.850 | Accuracy: 0.654000 | 1.483 sec/iter\n",
      "Epoch: 368 | Batch: 002 / 011 | Total loss: 0.836 | Reg loss: 0.026 | Tree loss: 0.836 | Accuracy: 0.651000 | 1.483 sec/iter\n",
      "Epoch: 368 | Batch: 003 / 011 | Total loss: 0.841 | Reg loss: 0.026 | Tree loss: 0.841 | Accuracy: 0.644000 | 1.483 sec/iter\n",
      "Epoch: 368 | Batch: 004 / 011 | Total loss: 0.816 | Reg loss: 0.026 | Tree loss: 0.816 | Accuracy: 0.652000 | 1.483 sec/iter\n",
      "Epoch: 368 | Batch: 005 / 011 | Total loss: 0.826 | Reg loss: 0.026 | Tree loss: 0.826 | Accuracy: 0.633000 | 1.483 sec/iter\n",
      "Epoch: 368 | Batch: 006 / 011 | Total loss: 0.818 | Reg loss: 0.026 | Tree loss: 0.818 | Accuracy: 0.623000 | 1.483 sec/iter\n",
      "Epoch: 368 | Batch: 007 / 011 | Total loss: 0.797 | Reg loss: 0.026 | Tree loss: 0.797 | Accuracy: 0.640500 | 1.483 sec/iter\n",
      "Epoch: 368 | Batch: 008 / 011 | Total loss: 0.834 | Reg loss: 0.026 | Tree loss: 0.834 | Accuracy: 0.611500 | 1.483 sec/iter\n",
      "Epoch: 368 | Batch: 009 / 011 | Total loss: 0.796 | Reg loss: 0.026 | Tree loss: 0.796 | Accuracy: 0.630500 | 1.483 sec/iter\n",
      "Epoch: 368 | Batch: 010 / 011 | Total loss: 0.764 | Reg loss: 0.026 | Tree loss: 0.764 | Accuracy: 0.655290 | 1.483 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 369 | Batch: 000 / 011 | Total loss: 0.875 | Reg loss: 0.026 | Tree loss: 0.875 | Accuracy: 0.630500 | 1.483 sec/iter\n",
      "Epoch: 369 | Batch: 001 / 011 | Total loss: 0.845 | Reg loss: 0.026 | Tree loss: 0.845 | Accuracy: 0.653500 | 1.483 sec/iter\n",
      "Epoch: 369 | Batch: 002 / 011 | Total loss: 0.865 | Reg loss: 0.026 | Tree loss: 0.865 | Accuracy: 0.636500 | 1.483 sec/iter\n",
      "Epoch: 369 | Batch: 003 / 011 | Total loss: 0.833 | Reg loss: 0.026 | Tree loss: 0.833 | Accuracy: 0.664000 | 1.483 sec/iter\n",
      "Epoch: 369 | Batch: 004 / 011 | Total loss: 0.835 | Reg loss: 0.026 | Tree loss: 0.835 | Accuracy: 0.647500 | 1.483 sec/iter\n",
      "Epoch: 369 | Batch: 005 / 011 | Total loss: 0.819 | Reg loss: 0.026 | Tree loss: 0.819 | Accuracy: 0.644000 | 1.483 sec/iter\n",
      "Epoch: 369 | Batch: 006 / 011 | Total loss: 0.798 | Reg loss: 0.026 | Tree loss: 0.798 | Accuracy: 0.653000 | 1.483 sec/iter\n",
      "Epoch: 369 | Batch: 007 / 011 | Total loss: 0.803 | Reg loss: 0.026 | Tree loss: 0.803 | Accuracy: 0.638500 | 1.483 sec/iter\n",
      "Epoch: 369 | Batch: 008 / 011 | Total loss: 0.818 | Reg loss: 0.026 | Tree loss: 0.818 | Accuracy: 0.637500 | 1.483 sec/iter\n",
      "Epoch: 369 | Batch: 009 / 011 | Total loss: 0.809 | Reg loss: 0.026 | Tree loss: 0.809 | Accuracy: 0.630500 | 1.483 sec/iter\n",
      "Epoch: 369 | Batch: 010 / 011 | Total loss: 0.794 | Reg loss: 0.026 | Tree loss: 0.794 | Accuracy: 0.631399 | 1.483 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 370 | Batch: 000 / 011 | Total loss: 0.892 | Reg loss: 0.026 | Tree loss: 0.892 | Accuracy: 0.628500 | 1.483 sec/iter\n",
      "Epoch: 370 | Batch: 001 / 011 | Total loss: 0.867 | Reg loss: 0.026 | Tree loss: 0.867 | Accuracy: 0.639500 | 1.483 sec/iter\n",
      "Epoch: 370 | Batch: 002 / 011 | Total loss: 0.833 | Reg loss: 0.026 | Tree loss: 0.833 | Accuracy: 0.674000 | 1.483 sec/iter\n",
      "Epoch: 370 | Batch: 003 / 011 | Total loss: 0.827 | Reg loss: 0.026 | Tree loss: 0.827 | Accuracy: 0.651500 | 1.483 sec/iter\n",
      "Epoch: 370 | Batch: 004 / 011 | Total loss: 0.809 | Reg loss: 0.026 | Tree loss: 0.809 | Accuracy: 0.657000 | 1.483 sec/iter\n",
      "Epoch: 370 | Batch: 005 / 011 | Total loss: 0.789 | Reg loss: 0.026 | Tree loss: 0.789 | Accuracy: 0.682500 | 1.483 sec/iter\n",
      "Epoch: 370 | Batch: 006 / 011 | Total loss: 0.813 | Reg loss: 0.026 | Tree loss: 0.813 | Accuracy: 0.626000 | 1.483 sec/iter\n",
      "Epoch: 370 | Batch: 007 / 011 | Total loss: 0.811 | Reg loss: 0.026 | Tree loss: 0.811 | Accuracy: 0.623000 | 1.483 sec/iter\n",
      "Epoch: 370 | Batch: 008 / 011 | Total loss: 0.822 | Reg loss: 0.026 | Tree loss: 0.822 | Accuracy: 0.625500 | 1.483 sec/iter\n",
      "Epoch: 370 | Batch: 009 / 011 | Total loss: 0.804 | Reg loss: 0.026 | Tree loss: 0.804 | Accuracy: 0.641500 | 1.483 sec/iter\n",
      "Epoch: 370 | Batch: 010 / 011 | Total loss: 0.828 | Reg loss: 0.026 | Tree loss: 0.828 | Accuracy: 0.648464 | 1.483 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 371 | Batch: 000 / 011 | Total loss: 0.857 | Reg loss: 0.026 | Tree loss: 0.857 | Accuracy: 0.636500 | 1.483 sec/iter\n",
      "Epoch: 371 | Batch: 001 / 011 | Total loss: 0.850 | Reg loss: 0.026 | Tree loss: 0.850 | Accuracy: 0.657000 | 1.483 sec/iter\n",
      "Epoch: 371 | Batch: 002 / 011 | Total loss: 0.838 | Reg loss: 0.026 | Tree loss: 0.838 | Accuracy: 0.664000 | 1.483 sec/iter\n",
      "Epoch: 371 | Batch: 003 / 011 | Total loss: 0.845 | Reg loss: 0.026 | Tree loss: 0.845 | Accuracy: 0.639000 | 1.483 sec/iter\n",
      "Epoch: 371 | Batch: 004 / 011 | Total loss: 0.832 | Reg loss: 0.026 | Tree loss: 0.832 | Accuracy: 0.654500 | 1.483 sec/iter\n",
      "Epoch: 371 | Batch: 005 / 011 | Total loss: 0.827 | Reg loss: 0.026 | Tree loss: 0.827 | Accuracy: 0.659500 | 1.483 sec/iter\n",
      "Epoch: 371 | Batch: 006 / 011 | Total loss: 0.816 | Reg loss: 0.026 | Tree loss: 0.816 | Accuracy: 0.635500 | 1.483 sec/iter\n",
      "Epoch: 371 | Batch: 007 / 011 | Total loss: 0.809 | Reg loss: 0.026 | Tree loss: 0.809 | Accuracy: 0.637500 | 1.483 sec/iter\n",
      "Epoch: 371 | Batch: 008 / 011 | Total loss: 0.791 | Reg loss: 0.026 | Tree loss: 0.791 | Accuracy: 0.613000 | 1.483 sec/iter\n",
      "Epoch: 371 | Batch: 009 / 011 | Total loss: 0.820 | Reg loss: 0.026 | Tree loss: 0.820 | Accuracy: 0.622500 | 1.483 sec/iter\n",
      "Epoch: 371 | Batch: 010 / 011 | Total loss: 0.715 | Reg loss: 0.026 | Tree loss: 0.715 | Accuracy: 0.709898 | 1.483 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 372 | Batch: 000 / 011 | Total loss: 0.868 | Reg loss: 0.026 | Tree loss: 0.868 | Accuracy: 0.639000 | 1.483 sec/iter\n",
      "Epoch: 372 | Batch: 001 / 011 | Total loss: 0.854 | Reg loss: 0.026 | Tree loss: 0.854 | Accuracy: 0.646000 | 1.483 sec/iter\n",
      "Epoch: 372 | Batch: 002 / 011 | Total loss: 0.855 | Reg loss: 0.026 | Tree loss: 0.855 | Accuracy: 0.645500 | 1.483 sec/iter\n",
      "Epoch: 372 | Batch: 003 / 011 | Total loss: 0.841 | Reg loss: 0.026 | Tree loss: 0.841 | Accuracy: 0.655500 | 1.483 sec/iter\n",
      "Epoch: 372 | Batch: 004 / 011 | Total loss: 0.814 | Reg loss: 0.026 | Tree loss: 0.814 | Accuracy: 0.641500 | 1.483 sec/iter\n",
      "Epoch: 372 | Batch: 005 / 011 | Total loss: 0.807 | Reg loss: 0.026 | Tree loss: 0.807 | Accuracy: 0.647500 | 1.483 sec/iter\n",
      "Epoch: 372 | Batch: 006 / 011 | Total loss: 0.795 | Reg loss: 0.026 | Tree loss: 0.795 | Accuracy: 0.647000 | 1.483 sec/iter\n",
      "Epoch: 372 | Batch: 007 / 011 | Total loss: 0.804 | Reg loss: 0.026 | Tree loss: 0.804 | Accuracy: 0.636000 | 1.483 sec/iter\n",
      "Epoch: 372 | Batch: 008 / 011 | Total loss: 0.793 | Reg loss: 0.026 | Tree loss: 0.793 | Accuracy: 0.630500 | 1.483 sec/iter\n",
      "Epoch: 372 | Batch: 009 / 011 | Total loss: 0.820 | Reg loss: 0.026 | Tree loss: 0.820 | Accuracy: 0.616500 | 1.483 sec/iter\n",
      "Epoch: 372 | Batch: 010 / 011 | Total loss: 0.835 | Reg loss: 0.026 | Tree loss: 0.835 | Accuracy: 0.593857 | 1.483 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 373 | Batch: 000 / 011 | Total loss: 0.867 | Reg loss: 0.026 | Tree loss: 0.867 | Accuracy: 0.632000 | 1.483 sec/iter\n",
      "Epoch: 373 | Batch: 001 / 011 | Total loss: 0.865 | Reg loss: 0.026 | Tree loss: 0.865 | Accuracy: 0.640000 | 1.483 sec/iter\n",
      "Epoch: 373 | Batch: 002 / 011 | Total loss: 0.840 | Reg loss: 0.026 | Tree loss: 0.840 | Accuracy: 0.663500 | 1.483 sec/iter\n",
      "Epoch: 373 | Batch: 003 / 011 | Total loss: 0.828 | Reg loss: 0.026 | Tree loss: 0.828 | Accuracy: 0.665000 | 1.483 sec/iter\n",
      "Epoch: 373 | Batch: 004 / 011 | Total loss: 0.814 | Reg loss: 0.026 | Tree loss: 0.814 | Accuracy: 0.663000 | 1.483 sec/iter\n",
      "Epoch: 373 | Batch: 005 / 011 | Total loss: 0.845 | Reg loss: 0.026 | Tree loss: 0.845 | Accuracy: 0.633500 | 1.483 sec/iter\n",
      "Epoch: 373 | Batch: 006 / 011 | Total loss: 0.805 | Reg loss: 0.026 | Tree loss: 0.805 | Accuracy: 0.654500 | 1.483 sec/iter\n",
      "Epoch: 373 | Batch: 007 / 011 | Total loss: 0.797 | Reg loss: 0.026 | Tree loss: 0.797 | Accuracy: 0.644000 | 1.483 sec/iter\n",
      "Epoch: 373 | Batch: 008 / 011 | Total loss: 0.810 | Reg loss: 0.026 | Tree loss: 0.810 | Accuracy: 0.620500 | 1.483 sec/iter\n",
      "Epoch: 373 | Batch: 009 / 011 | Total loss: 0.785 | Reg loss: 0.026 | Tree loss: 0.785 | Accuracy: 0.645500 | 1.483 sec/iter\n",
      "Epoch: 373 | Batch: 010 / 011 | Total loss: 0.779 | Reg loss: 0.026 | Tree loss: 0.779 | Accuracy: 0.665529 | 1.483 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 374 | Batch: 000 / 011 | Total loss: 0.894 | Reg loss: 0.026 | Tree loss: 0.894 | Accuracy: 0.623000 | 1.484 sec/iter\n",
      "Epoch: 374 | Batch: 001 / 011 | Total loss: 0.864 | Reg loss: 0.026 | Tree loss: 0.864 | Accuracy: 0.632000 | 1.484 sec/iter\n",
      "Epoch: 374 | Batch: 002 / 011 | Total loss: 0.844 | Reg loss: 0.026 | Tree loss: 0.844 | Accuracy: 0.652000 | 1.484 sec/iter\n",
      "Epoch: 374 | Batch: 003 / 011 | Total loss: 0.841 | Reg loss: 0.026 | Tree loss: 0.841 | Accuracy: 0.659000 | 1.484 sec/iter\n",
      "Epoch: 374 | Batch: 004 / 011 | Total loss: 0.809 | Reg loss: 0.026 | Tree loss: 0.809 | Accuracy: 0.636500 | 1.484 sec/iter\n",
      "Epoch: 374 | Batch: 005 / 011 | Total loss: 0.823 | Reg loss: 0.026 | Tree loss: 0.823 | Accuracy: 0.629500 | 1.484 sec/iter\n",
      "Epoch: 374 | Batch: 006 / 011 | Total loss: 0.789 | Reg loss: 0.026 | Tree loss: 0.789 | Accuracy: 0.640500 | 1.484 sec/iter\n",
      "Epoch: 374 | Batch: 007 / 011 | Total loss: 0.815 | Reg loss: 0.026 | Tree loss: 0.815 | Accuracy: 0.623000 | 1.484 sec/iter\n",
      "Epoch: 374 | Batch: 008 / 011 | Total loss: 0.782 | Reg loss: 0.026 | Tree loss: 0.782 | Accuracy: 0.635000 | 1.484 sec/iter\n",
      "Epoch: 374 | Batch: 009 / 011 | Total loss: 0.793 | Reg loss: 0.026 | Tree loss: 0.793 | Accuracy: 0.639500 | 1.484 sec/iter\n",
      "Epoch: 374 | Batch: 010 / 011 | Total loss: 0.847 | Reg loss: 0.026 | Tree loss: 0.847 | Accuracy: 0.641638 | 1.484 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 375 | Batch: 000 / 011 | Total loss: 0.890 | Reg loss: 0.026 | Tree loss: 0.890 | Accuracy: 0.626000 | 1.484 sec/iter\n",
      "Epoch: 375 | Batch: 001 / 011 | Total loss: 0.877 | Reg loss: 0.026 | Tree loss: 0.877 | Accuracy: 0.638500 | 1.484 sec/iter\n",
      "Epoch: 375 | Batch: 002 / 011 | Total loss: 0.852 | Reg loss: 0.026 | Tree loss: 0.852 | Accuracy: 0.641000 | 1.484 sec/iter\n",
      "Epoch: 375 | Batch: 003 / 011 | Total loss: 0.829 | Reg loss: 0.026 | Tree loss: 0.829 | Accuracy: 0.674000 | 1.484 sec/iter\n",
      "Epoch: 375 | Batch: 004 / 011 | Total loss: 0.833 | Reg loss: 0.026 | Tree loss: 0.833 | Accuracy: 0.649500 | 1.484 sec/iter\n",
      "Epoch: 375 | Batch: 005 / 011 | Total loss: 0.830 | Reg loss: 0.026 | Tree loss: 0.830 | Accuracy: 0.648000 | 1.484 sec/iter\n",
      "Epoch: 375 | Batch: 006 / 011 | Total loss: 0.811 | Reg loss: 0.026 | Tree loss: 0.811 | Accuracy: 0.638000 | 1.484 sec/iter\n",
      "Epoch: 375 | Batch: 007 / 011 | Total loss: 0.800 | Reg loss: 0.026 | Tree loss: 0.800 | Accuracy: 0.636500 | 1.484 sec/iter\n",
      "Epoch: 375 | Batch: 008 / 011 | Total loss: 0.782 | Reg loss: 0.026 | Tree loss: 0.782 | Accuracy: 0.638500 | 1.484 sec/iter\n",
      "Epoch: 375 | Batch: 009 / 011 | Total loss: 0.779 | Reg loss: 0.026 | Tree loss: 0.779 | Accuracy: 0.627500 | 1.484 sec/iter\n",
      "Epoch: 375 | Batch: 010 / 011 | Total loss: 0.777 | Reg loss: 0.026 | Tree loss: 0.777 | Accuracy: 0.610922 | 1.485 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 376 | Batch: 000 / 011 | Total loss: 0.865 | Reg loss: 0.026 | Tree loss: 0.865 | Accuracy: 0.644500 | 1.485 sec/iter\n",
      "Epoch: 376 | Batch: 001 / 011 | Total loss: 0.868 | Reg loss: 0.026 | Tree loss: 0.868 | Accuracy: 0.640000 | 1.485 sec/iter\n",
      "Epoch: 376 | Batch: 002 / 011 | Total loss: 0.873 | Reg loss: 0.026 | Tree loss: 0.873 | Accuracy: 0.634000 | 1.485 sec/iter\n",
      "Epoch: 376 | Batch: 003 / 011 | Total loss: 0.841 | Reg loss: 0.026 | Tree loss: 0.841 | Accuracy: 0.657000 | 1.485 sec/iter\n",
      "Epoch: 376 | Batch: 004 / 011 | Total loss: 0.825 | Reg loss: 0.026 | Tree loss: 0.825 | Accuracy: 0.650000 | 1.485 sec/iter\n",
      "Epoch: 376 | Batch: 005 / 011 | Total loss: 0.841 | Reg loss: 0.026 | Tree loss: 0.841 | Accuracy: 0.641500 | 1.485 sec/iter\n",
      "Epoch: 376 | Batch: 006 / 011 | Total loss: 0.808 | Reg loss: 0.026 | Tree loss: 0.808 | Accuracy: 0.653500 | 1.485 sec/iter\n",
      "Epoch: 376 | Batch: 007 / 011 | Total loss: 0.801 | Reg loss: 0.026 | Tree loss: 0.801 | Accuracy: 0.640000 | 1.485 sec/iter\n",
      "Epoch: 376 | Batch: 008 / 011 | Total loss: 0.815 | Reg loss: 0.026 | Tree loss: 0.815 | Accuracy: 0.615500 | 1.485 sec/iter\n",
      "Epoch: 376 | Batch: 009 / 011 | Total loss: 0.762 | Reg loss: 0.026 | Tree loss: 0.762 | Accuracy: 0.652000 | 1.485 sec/iter\n",
      "Epoch: 376 | Batch: 010 / 011 | Total loss: 0.798 | Reg loss: 0.026 | Tree loss: 0.798 | Accuracy: 0.641638 | 1.485 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 377 | Batch: 000 / 011 | Total loss: 0.854 | Reg loss: 0.026 | Tree loss: 0.854 | Accuracy: 0.661000 | 1.486 sec/iter\n",
      "Epoch: 377 | Batch: 001 / 011 | Total loss: 0.839 | Reg loss: 0.026 | Tree loss: 0.839 | Accuracy: 0.657000 | 1.486 sec/iter\n",
      "Epoch: 377 | Batch: 002 / 011 | Total loss: 0.848 | Reg loss: 0.026 | Tree loss: 0.848 | Accuracy: 0.650500 | 1.486 sec/iter\n",
      "Epoch: 377 | Batch: 003 / 011 | Total loss: 0.837 | Reg loss: 0.026 | Tree loss: 0.837 | Accuracy: 0.641000 | 1.486 sec/iter\n",
      "Epoch: 377 | Batch: 004 / 011 | Total loss: 0.830 | Reg loss: 0.026 | Tree loss: 0.830 | Accuracy: 0.642000 | 1.486 sec/iter\n",
      "Epoch: 377 | Batch: 005 / 011 | Total loss: 0.806 | Reg loss: 0.026 | Tree loss: 0.806 | Accuracy: 0.661500 | 1.486 sec/iter\n",
      "Epoch: 377 | Batch: 006 / 011 | Total loss: 0.819 | Reg loss: 0.026 | Tree loss: 0.819 | Accuracy: 0.644500 | 1.486 sec/iter\n",
      "Epoch: 377 | Batch: 007 / 011 | Total loss: 0.808 | Reg loss: 0.026 | Tree loss: 0.808 | Accuracy: 0.632000 | 1.486 sec/iter\n",
      "Epoch: 377 | Batch: 008 / 011 | Total loss: 0.801 | Reg loss: 0.026 | Tree loss: 0.801 | Accuracy: 0.636500 | 1.486 sec/iter\n",
      "Epoch: 377 | Batch: 009 / 011 | Total loss: 0.822 | Reg loss: 0.026 | Tree loss: 0.822 | Accuracy: 0.605000 | 1.486 sec/iter\n",
      "Epoch: 377 | Batch: 010 / 011 | Total loss: 0.817 | Reg loss: 0.026 | Tree loss: 0.817 | Accuracy: 0.624573 | 1.486 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 378 | Batch: 000 / 011 | Total loss: 0.870 | Reg loss: 0.026 | Tree loss: 0.870 | Accuracy: 0.640000 | 1.486 sec/iter\n",
      "Epoch: 378 | Batch: 001 / 011 | Total loss: 0.868 | Reg loss: 0.026 | Tree loss: 0.868 | Accuracy: 0.652500 | 1.486 sec/iter\n",
      "Epoch: 378 | Batch: 002 / 011 | Total loss: 0.860 | Reg loss: 0.026 | Tree loss: 0.860 | Accuracy: 0.627000 | 1.486 sec/iter\n",
      "Epoch: 378 | Batch: 003 / 011 | Total loss: 0.814 | Reg loss: 0.026 | Tree loss: 0.814 | Accuracy: 0.665500 | 1.486 sec/iter\n",
      "Epoch: 378 | Batch: 004 / 011 | Total loss: 0.831 | Reg loss: 0.026 | Tree loss: 0.831 | Accuracy: 0.639000 | 1.486 sec/iter\n",
      "Epoch: 378 | Batch: 005 / 011 | Total loss: 0.809 | Reg loss: 0.026 | Tree loss: 0.809 | Accuracy: 0.645500 | 1.486 sec/iter\n",
      "Epoch: 378 | Batch: 006 / 011 | Total loss: 0.784 | Reg loss: 0.026 | Tree loss: 0.784 | Accuracy: 0.643500 | 1.486 sec/iter\n",
      "Epoch: 378 | Batch: 007 / 011 | Total loss: 0.825 | Reg loss: 0.026 | Tree loss: 0.825 | Accuracy: 0.622000 | 1.486 sec/iter\n",
      "Epoch: 378 | Batch: 008 / 011 | Total loss: 0.817 | Reg loss: 0.026 | Tree loss: 0.817 | Accuracy: 0.617000 | 1.486 sec/iter\n",
      "Epoch: 378 | Batch: 009 / 011 | Total loss: 0.793 | Reg loss: 0.026 | Tree loss: 0.793 | Accuracy: 0.645000 | 1.486 sec/iter\n",
      "Epoch: 378 | Batch: 010 / 011 | Total loss: 0.809 | Reg loss: 0.026 | Tree loss: 0.809 | Accuracy: 0.610922 | 1.486 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 379 | Batch: 000 / 011 | Total loss: 0.863 | Reg loss: 0.026 | Tree loss: 0.863 | Accuracy: 0.640500 | 1.486 sec/iter\n",
      "Epoch: 379 | Batch: 001 / 011 | Total loss: 0.868 | Reg loss: 0.026 | Tree loss: 0.868 | Accuracy: 0.644500 | 1.486 sec/iter\n",
      "Epoch: 379 | Batch: 002 / 011 | Total loss: 0.855 | Reg loss: 0.026 | Tree loss: 0.855 | Accuracy: 0.643000 | 1.486 sec/iter\n",
      "Epoch: 379 | Batch: 003 / 011 | Total loss: 0.834 | Reg loss: 0.026 | Tree loss: 0.834 | Accuracy: 0.646000 | 1.486 sec/iter\n",
      "Epoch: 379 | Batch: 004 / 011 | Total loss: 0.820 | Reg loss: 0.026 | Tree loss: 0.820 | Accuracy: 0.644500 | 1.486 sec/iter\n",
      "Epoch: 379 | Batch: 005 / 011 | Total loss: 0.809 | Reg loss: 0.026 | Tree loss: 0.809 | Accuracy: 0.655000 | 1.486 sec/iter\n",
      "Epoch: 379 | Batch: 006 / 011 | Total loss: 0.789 | Reg loss: 0.026 | Tree loss: 0.789 | Accuracy: 0.661500 | 1.486 sec/iter\n",
      "Epoch: 379 | Batch: 007 / 011 | Total loss: 0.800 | Reg loss: 0.026 | Tree loss: 0.800 | Accuracy: 0.636000 | 1.486 sec/iter\n",
      "Epoch: 379 | Batch: 008 / 011 | Total loss: 0.809 | Reg loss: 0.026 | Tree loss: 0.809 | Accuracy: 0.621000 | 1.486 sec/iter\n",
      "Epoch: 379 | Batch: 009 / 011 | Total loss: 0.790 | Reg loss: 0.026 | Tree loss: 0.790 | Accuracy: 0.638000 | 1.486 sec/iter\n",
      "Epoch: 379 | Batch: 010 / 011 | Total loss: 0.837 | Reg loss: 0.026 | Tree loss: 0.837 | Accuracy: 0.614334 | 1.486 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 380 | Batch: 000 / 011 | Total loss: 0.875 | Reg loss: 0.026 | Tree loss: 0.875 | Accuracy: 0.648000 | 1.487 sec/iter\n",
      "Epoch: 380 | Batch: 001 / 011 | Total loss: 0.852 | Reg loss: 0.026 | Tree loss: 0.852 | Accuracy: 0.661500 | 1.487 sec/iter\n",
      "Epoch: 380 | Batch: 002 / 011 | Total loss: 0.846 | Reg loss: 0.026 | Tree loss: 0.846 | Accuracy: 0.657000 | 1.487 sec/iter\n",
      "Epoch: 380 | Batch: 003 / 011 | Total loss: 0.832 | Reg loss: 0.026 | Tree loss: 0.832 | Accuracy: 0.632500 | 1.487 sec/iter\n",
      "Epoch: 380 | Batch: 004 / 011 | Total loss: 0.833 | Reg loss: 0.026 | Tree loss: 0.833 | Accuracy: 0.627500 | 1.487 sec/iter\n",
      "Epoch: 380 | Batch: 005 / 011 | Total loss: 0.811 | Reg loss: 0.026 | Tree loss: 0.811 | Accuracy: 0.638500 | 1.487 sec/iter\n",
      "Epoch: 380 | Batch: 006 / 011 | Total loss: 0.808 | Reg loss: 0.026 | Tree loss: 0.808 | Accuracy: 0.625000 | 1.487 sec/iter\n",
      "Epoch: 380 | Batch: 007 / 011 | Total loss: 0.808 | Reg loss: 0.026 | Tree loss: 0.808 | Accuracy: 0.634500 | 1.487 sec/iter\n",
      "Epoch: 380 | Batch: 008 / 011 | Total loss: 0.817 | Reg loss: 0.026 | Tree loss: 0.817 | Accuracy: 0.623500 | 1.487 sec/iter\n",
      "Epoch: 380 | Batch: 009 / 011 | Total loss: 0.775 | Reg loss: 0.026 | Tree loss: 0.775 | Accuracy: 0.644000 | 1.487 sec/iter\n",
      "Epoch: 380 | Batch: 010 / 011 | Total loss: 0.794 | Reg loss: 0.026 | Tree loss: 0.794 | Accuracy: 0.631399 | 1.487 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 381 | Batch: 000 / 011 | Total loss: 0.876 | Reg loss: 0.026 | Tree loss: 0.876 | Accuracy: 0.635500 | 1.487 sec/iter\n",
      "Epoch: 381 | Batch: 001 / 011 | Total loss: 0.861 | Reg loss: 0.026 | Tree loss: 0.861 | Accuracy: 0.645000 | 1.487 sec/iter\n",
      "Epoch: 381 | Batch: 002 / 011 | Total loss: 0.817 | Reg loss: 0.026 | Tree loss: 0.817 | Accuracy: 0.687500 | 1.487 sec/iter\n",
      "Epoch: 381 | Batch: 003 / 011 | Total loss: 0.822 | Reg loss: 0.026 | Tree loss: 0.822 | Accuracy: 0.643500 | 1.487 sec/iter\n",
      "Epoch: 381 | Batch: 004 / 011 | Total loss: 0.825 | Reg loss: 0.026 | Tree loss: 0.825 | Accuracy: 0.651500 | 1.487 sec/iter\n",
      "Epoch: 381 | Batch: 005 / 011 | Total loss: 0.820 | Reg loss: 0.026 | Tree loss: 0.820 | Accuracy: 0.644500 | 1.487 sec/iter\n",
      "Epoch: 381 | Batch: 006 / 011 | Total loss: 0.806 | Reg loss: 0.026 | Tree loss: 0.806 | Accuracy: 0.631500 | 1.487 sec/iter\n",
      "Epoch: 381 | Batch: 007 / 011 | Total loss: 0.812 | Reg loss: 0.026 | Tree loss: 0.812 | Accuracy: 0.635500 | 1.487 sec/iter\n",
      "Epoch: 381 | Batch: 008 / 011 | Total loss: 0.797 | Reg loss: 0.026 | Tree loss: 0.797 | Accuracy: 0.624500 | 1.487 sec/iter\n",
      "Epoch: 381 | Batch: 009 / 011 | Total loss: 0.836 | Reg loss: 0.026 | Tree loss: 0.836 | Accuracy: 0.629500 | 1.487 sec/iter\n",
      "Epoch: 381 | Batch: 010 / 011 | Total loss: 0.826 | Reg loss: 0.026 | Tree loss: 0.826 | Accuracy: 0.631399 | 1.487 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 382 | Batch: 000 / 011 | Total loss: 0.852 | Reg loss: 0.026 | Tree loss: 0.852 | Accuracy: 0.664000 | 1.487 sec/iter\n",
      "Epoch: 382 | Batch: 001 / 011 | Total loss: 0.866 | Reg loss: 0.026 | Tree loss: 0.866 | Accuracy: 0.638000 | 1.487 sec/iter\n",
      "Epoch: 382 | Batch: 002 / 011 | Total loss: 0.865 | Reg loss: 0.026 | Tree loss: 0.865 | Accuracy: 0.661000 | 1.487 sec/iter\n",
      "Epoch: 382 | Batch: 003 / 011 | Total loss: 0.834 | Reg loss: 0.026 | Tree loss: 0.834 | Accuracy: 0.650500 | 1.487 sec/iter\n",
      "Epoch: 382 | Batch: 004 / 011 | Total loss: 0.825 | Reg loss: 0.026 | Tree loss: 0.825 | Accuracy: 0.650000 | 1.487 sec/iter\n",
      "Epoch: 382 | Batch: 005 / 011 | Total loss: 0.829 | Reg loss: 0.026 | Tree loss: 0.829 | Accuracy: 0.651500 | 1.487 sec/iter\n",
      "Epoch: 382 | Batch: 006 / 011 | Total loss: 0.803 | Reg loss: 0.026 | Tree loss: 0.803 | Accuracy: 0.651000 | 1.487 sec/iter\n",
      "Epoch: 382 | Batch: 007 / 011 | Total loss: 0.802 | Reg loss: 0.026 | Tree loss: 0.802 | Accuracy: 0.638000 | 1.487 sec/iter\n",
      "Epoch: 382 | Batch: 008 / 011 | Total loss: 0.796 | Reg loss: 0.026 | Tree loss: 0.796 | Accuracy: 0.618000 | 1.487 sec/iter\n",
      "Epoch: 382 | Batch: 009 / 011 | Total loss: 0.791 | Reg loss: 0.026 | Tree loss: 0.791 | Accuracy: 0.641500 | 1.487 sec/iter\n",
      "Epoch: 382 | Batch: 010 / 011 | Total loss: 0.776 | Reg loss: 0.026 | Tree loss: 0.776 | Accuracy: 0.655290 | 1.487 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 383 | Batch: 000 / 011 | Total loss: 0.863 | Reg loss: 0.026 | Tree loss: 0.863 | Accuracy: 0.639500 | 1.487 sec/iter\n",
      "Epoch: 383 | Batch: 001 / 011 | Total loss: 0.845 | Reg loss: 0.026 | Tree loss: 0.845 | Accuracy: 0.659000 | 1.487 sec/iter\n",
      "Epoch: 383 | Batch: 002 / 011 | Total loss: 0.859 | Reg loss: 0.026 | Tree loss: 0.859 | Accuracy: 0.637500 | 1.487 sec/iter\n",
      "Epoch: 383 | Batch: 003 / 011 | Total loss: 0.826 | Reg loss: 0.026 | Tree loss: 0.826 | Accuracy: 0.669500 | 1.487 sec/iter\n",
      "Epoch: 383 | Batch: 004 / 011 | Total loss: 0.840 | Reg loss: 0.026 | Tree loss: 0.840 | Accuracy: 0.631000 | 1.487 sec/iter\n",
      "Epoch: 383 | Batch: 005 / 011 | Total loss: 0.821 | Reg loss: 0.026 | Tree loss: 0.821 | Accuracy: 0.635000 | 1.487 sec/iter\n",
      "Epoch: 383 | Batch: 006 / 011 | Total loss: 0.806 | Reg loss: 0.026 | Tree loss: 0.806 | Accuracy: 0.647500 | 1.487 sec/iter\n",
      "Epoch: 383 | Batch: 007 / 011 | Total loss: 0.805 | Reg loss: 0.026 | Tree loss: 0.805 | Accuracy: 0.622000 | 1.487 sec/iter\n",
      "Epoch: 383 | Batch: 008 / 011 | Total loss: 0.806 | Reg loss: 0.026 | Tree loss: 0.806 | Accuracy: 0.625500 | 1.487 sec/iter\n",
      "Epoch: 383 | Batch: 009 / 011 | Total loss: 0.783 | Reg loss: 0.026 | Tree loss: 0.783 | Accuracy: 0.636500 | 1.487 sec/iter\n",
      "Epoch: 383 | Batch: 010 / 011 | Total loss: 0.765 | Reg loss: 0.026 | Tree loss: 0.765 | Accuracy: 0.651877 | 1.487 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 384 | Batch: 000 / 011 | Total loss: 0.857 | Reg loss: 0.026 | Tree loss: 0.857 | Accuracy: 0.642500 | 1.488 sec/iter\n",
      "Epoch: 384 | Batch: 001 / 011 | Total loss: 0.857 | Reg loss: 0.026 | Tree loss: 0.857 | Accuracy: 0.644500 | 1.488 sec/iter\n",
      "Epoch: 384 | Batch: 002 / 011 | Total loss: 0.866 | Reg loss: 0.026 | Tree loss: 0.866 | Accuracy: 0.638000 | 1.488 sec/iter\n",
      "Epoch: 384 | Batch: 003 / 011 | Total loss: 0.829 | Reg loss: 0.026 | Tree loss: 0.829 | Accuracy: 0.652500 | 1.488 sec/iter\n",
      "Epoch: 384 | Batch: 004 / 011 | Total loss: 0.812 | Reg loss: 0.026 | Tree loss: 0.812 | Accuracy: 0.657000 | 1.488 sec/iter\n",
      "Epoch: 384 | Batch: 005 / 011 | Total loss: 0.825 | Reg loss: 0.026 | Tree loss: 0.825 | Accuracy: 0.653500 | 1.488 sec/iter\n",
      "Epoch: 384 | Batch: 006 / 011 | Total loss: 0.810 | Reg loss: 0.026 | Tree loss: 0.810 | Accuracy: 0.640000 | 1.488 sec/iter\n",
      "Epoch: 384 | Batch: 007 / 011 | Total loss: 0.805 | Reg loss: 0.026 | Tree loss: 0.805 | Accuracy: 0.629500 | 1.488 sec/iter\n",
      "Epoch: 384 | Batch: 008 / 011 | Total loss: 0.793 | Reg loss: 0.026 | Tree loss: 0.793 | Accuracy: 0.636500 | 1.488 sec/iter\n",
      "Epoch: 384 | Batch: 009 / 011 | Total loss: 0.814 | Reg loss: 0.026 | Tree loss: 0.814 | Accuracy: 0.634000 | 1.488 sec/iter\n",
      "Epoch: 384 | Batch: 010 / 011 | Total loss: 0.845 | Reg loss: 0.026 | Tree loss: 0.845 | Accuracy: 0.610922 | 1.488 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 385 | Batch: 000 / 011 | Total loss: 0.856 | Reg loss: 0.026 | Tree loss: 0.856 | Accuracy: 0.643000 | 1.488 sec/iter\n",
      "Epoch: 385 | Batch: 001 / 011 | Total loss: 0.858 | Reg loss: 0.026 | Tree loss: 0.858 | Accuracy: 0.660000 | 1.488 sec/iter\n",
      "Epoch: 385 | Batch: 002 / 011 | Total loss: 0.869 | Reg loss: 0.026 | Tree loss: 0.869 | Accuracy: 0.634500 | 1.488 sec/iter\n",
      "Epoch: 385 | Batch: 003 / 011 | Total loss: 0.833 | Reg loss: 0.026 | Tree loss: 0.833 | Accuracy: 0.639000 | 1.488 sec/iter\n",
      "Epoch: 385 | Batch: 004 / 011 | Total loss: 0.801 | Reg loss: 0.026 | Tree loss: 0.801 | Accuracy: 0.666000 | 1.488 sec/iter\n",
      "Epoch: 385 | Batch: 005 / 011 | Total loss: 0.831 | Reg loss: 0.026 | Tree loss: 0.831 | Accuracy: 0.647000 | 1.488 sec/iter\n",
      "Epoch: 385 | Batch: 006 / 011 | Total loss: 0.820 | Reg loss: 0.026 | Tree loss: 0.820 | Accuracy: 0.643500 | 1.489 sec/iter\n",
      "Epoch: 385 | Batch: 007 / 011 | Total loss: 0.809 | Reg loss: 0.026 | Tree loss: 0.809 | Accuracy: 0.642000 | 1.489 sec/iter\n",
      "Epoch: 385 | Batch: 008 / 011 | Total loss: 0.784 | Reg loss: 0.026 | Tree loss: 0.784 | Accuracy: 0.648500 | 1.489 sec/iter\n",
      "Epoch: 385 | Batch: 009 / 011 | Total loss: 0.793 | Reg loss: 0.026 | Tree loss: 0.793 | Accuracy: 0.627500 | 1.488 sec/iter\n",
      "Epoch: 385 | Batch: 010 / 011 | Total loss: 0.841 | Reg loss: 0.026 | Tree loss: 0.841 | Accuracy: 0.624573 | 1.489 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 386 | Batch: 000 / 011 | Total loss: 0.872 | Reg loss: 0.026 | Tree loss: 0.872 | Accuracy: 0.634000 | 1.489 sec/iter\n",
      "Epoch: 386 | Batch: 001 / 011 | Total loss: 0.872 | Reg loss: 0.026 | Tree loss: 0.872 | Accuracy: 0.638500 | 1.489 sec/iter\n",
      "Epoch: 386 | Batch: 002 / 011 | Total loss: 0.850 | Reg loss: 0.026 | Tree loss: 0.850 | Accuracy: 0.642500 | 1.489 sec/iter\n",
      "Epoch: 386 | Batch: 003 / 011 | Total loss: 0.830 | Reg loss: 0.026 | Tree loss: 0.830 | Accuracy: 0.656000 | 1.489 sec/iter\n",
      "Epoch: 386 | Batch: 004 / 011 | Total loss: 0.800 | Reg loss: 0.026 | Tree loss: 0.800 | Accuracy: 0.670000 | 1.489 sec/iter\n",
      "Epoch: 386 | Batch: 005 / 011 | Total loss: 0.840 | Reg loss: 0.026 | Tree loss: 0.840 | Accuracy: 0.645000 | 1.489 sec/iter\n",
      "Epoch: 386 | Batch: 006 / 011 | Total loss: 0.817 | Reg loss: 0.026 | Tree loss: 0.817 | Accuracy: 0.618500 | 1.489 sec/iter\n",
      "Epoch: 386 | Batch: 007 / 011 | Total loss: 0.789 | Reg loss: 0.026 | Tree loss: 0.789 | Accuracy: 0.640500 | 1.489 sec/iter\n",
      "Epoch: 386 | Batch: 008 / 011 | Total loss: 0.792 | Reg loss: 0.026 | Tree loss: 0.792 | Accuracy: 0.625500 | 1.489 sec/iter\n",
      "Epoch: 386 | Batch: 009 / 011 | Total loss: 0.799 | Reg loss: 0.026 | Tree loss: 0.799 | Accuracy: 0.646500 | 1.489 sec/iter\n",
      "Epoch: 386 | Batch: 010 / 011 | Total loss: 0.775 | Reg loss: 0.026 | Tree loss: 0.775 | Accuracy: 0.641638 | 1.489 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 387 | Batch: 000 / 011 | Total loss: 0.869 | Reg loss: 0.026 | Tree loss: 0.869 | Accuracy: 0.643500 | 1.489 sec/iter\n",
      "Epoch: 387 | Batch: 001 / 011 | Total loss: 0.851 | Reg loss: 0.026 | Tree loss: 0.851 | Accuracy: 0.651000 | 1.489 sec/iter\n",
      "Epoch: 387 | Batch: 002 / 011 | Total loss: 0.845 | Reg loss: 0.026 | Tree loss: 0.845 | Accuracy: 0.655500 | 1.489 sec/iter\n",
      "Epoch: 387 | Batch: 003 / 011 | Total loss: 0.835 | Reg loss: 0.026 | Tree loss: 0.835 | Accuracy: 0.656000 | 1.489 sec/iter\n",
      "Epoch: 387 | Batch: 004 / 011 | Total loss: 0.831 | Reg loss: 0.026 | Tree loss: 0.831 | Accuracy: 0.645000 | 1.489 sec/iter\n",
      "Epoch: 387 | Batch: 005 / 011 | Total loss: 0.824 | Reg loss: 0.026 | Tree loss: 0.824 | Accuracy: 0.661500 | 1.489 sec/iter\n",
      "Epoch: 387 | Batch: 006 / 011 | Total loss: 0.792 | Reg loss: 0.026 | Tree loss: 0.792 | Accuracy: 0.665500 | 1.489 sec/iter\n",
      "Epoch: 387 | Batch: 007 / 011 | Total loss: 0.801 | Reg loss: 0.026 | Tree loss: 0.801 | Accuracy: 0.627000 | 1.489 sec/iter\n",
      "Epoch: 387 | Batch: 008 / 011 | Total loss: 0.807 | Reg loss: 0.026 | Tree loss: 0.807 | Accuracy: 0.625000 | 1.489 sec/iter\n",
      "Epoch: 387 | Batch: 009 / 011 | Total loss: 0.801 | Reg loss: 0.026 | Tree loss: 0.801 | Accuracy: 0.616000 | 1.489 sec/iter\n",
      "Epoch: 387 | Batch: 010 / 011 | Total loss: 0.831 | Reg loss: 0.026 | Tree loss: 0.831 | Accuracy: 0.607509 | 1.489 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 388 | Batch: 000 / 011 | Total loss: 0.884 | Reg loss: 0.026 | Tree loss: 0.884 | Accuracy: 0.637500 | 1.49 sec/iter\n",
      "Epoch: 388 | Batch: 001 / 011 | Total loss: 0.845 | Reg loss: 0.026 | Tree loss: 0.845 | Accuracy: 0.660000 | 1.49 sec/iter\n",
      "Epoch: 388 | Batch: 002 / 011 | Total loss: 0.837 | Reg loss: 0.026 | Tree loss: 0.837 | Accuracy: 0.647500 | 1.49 sec/iter\n",
      "Epoch: 388 | Batch: 003 / 011 | Total loss: 0.839 | Reg loss: 0.026 | Tree loss: 0.839 | Accuracy: 0.662000 | 1.49 sec/iter\n",
      "Epoch: 388 | Batch: 004 / 011 | Total loss: 0.815 | Reg loss: 0.026 | Tree loss: 0.815 | Accuracy: 0.650000 | 1.49 sec/iter\n",
      "Epoch: 388 | Batch: 005 / 011 | Total loss: 0.821 | Reg loss: 0.026 | Tree loss: 0.821 | Accuracy: 0.644500 | 1.49 sec/iter\n",
      "Epoch: 388 | Batch: 006 / 011 | Total loss: 0.808 | Reg loss: 0.026 | Tree loss: 0.808 | Accuracy: 0.643500 | 1.49 sec/iter\n",
      "Epoch: 388 | Batch: 007 / 011 | Total loss: 0.820 | Reg loss: 0.026 | Tree loss: 0.820 | Accuracy: 0.623500 | 1.49 sec/iter\n",
      "Epoch: 388 | Batch: 008 / 011 | Total loss: 0.812 | Reg loss: 0.026 | Tree loss: 0.812 | Accuracy: 0.630500 | 1.49 sec/iter\n",
      "Epoch: 388 | Batch: 009 / 011 | Total loss: 0.810 | Reg loss: 0.026 | Tree loss: 0.810 | Accuracy: 0.626500 | 1.49 sec/iter\n",
      "Epoch: 388 | Batch: 010 / 011 | Total loss: 0.813 | Reg loss: 0.026 | Tree loss: 0.813 | Accuracy: 0.645051 | 1.49 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 389 | Batch: 000 / 011 | Total loss: 0.861 | Reg loss: 0.026 | Tree loss: 0.861 | Accuracy: 0.640500 | 1.491 sec/iter\n",
      "Epoch: 389 | Batch: 001 / 011 | Total loss: 0.873 | Reg loss: 0.026 | Tree loss: 0.873 | Accuracy: 0.652000 | 1.491 sec/iter\n",
      "Epoch: 389 | Batch: 002 / 011 | Total loss: 0.830 | Reg loss: 0.026 | Tree loss: 0.830 | Accuracy: 0.671500 | 1.491 sec/iter\n",
      "Epoch: 389 | Batch: 003 / 011 | Total loss: 0.838 | Reg loss: 0.026 | Tree loss: 0.838 | Accuracy: 0.660000 | 1.491 sec/iter\n",
      "Epoch: 389 | Batch: 004 / 011 | Total loss: 0.846 | Reg loss: 0.026 | Tree loss: 0.846 | Accuracy: 0.644500 | 1.491 sec/iter\n",
      "Epoch: 389 | Batch: 005 / 011 | Total loss: 0.797 | Reg loss: 0.026 | Tree loss: 0.797 | Accuracy: 0.646500 | 1.491 sec/iter\n",
      "Epoch: 389 | Batch: 006 / 011 | Total loss: 0.821 | Reg loss: 0.026 | Tree loss: 0.821 | Accuracy: 0.640500 | 1.491 sec/iter\n",
      "Epoch: 389 | Batch: 007 / 011 | Total loss: 0.808 | Reg loss: 0.026 | Tree loss: 0.808 | Accuracy: 0.618000 | 1.491 sec/iter\n",
      "Epoch: 389 | Batch: 008 / 011 | Total loss: 0.802 | Reg loss: 0.026 | Tree loss: 0.802 | Accuracy: 0.633500 | 1.491 sec/iter\n",
      "Epoch: 389 | Batch: 009 / 011 | Total loss: 0.805 | Reg loss: 0.026 | Tree loss: 0.805 | Accuracy: 0.632500 | 1.491 sec/iter\n",
      "Epoch: 389 | Batch: 010 / 011 | Total loss: 0.829 | Reg loss: 0.026 | Tree loss: 0.829 | Accuracy: 0.593857 | 1.491 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 390 | Batch: 000 / 011 | Total loss: 0.882 | Reg loss: 0.026 | Tree loss: 0.882 | Accuracy: 0.622500 | 1.491 sec/iter\n",
      "Epoch: 390 | Batch: 001 / 011 | Total loss: 0.873 | Reg loss: 0.026 | Tree loss: 0.873 | Accuracy: 0.652000 | 1.491 sec/iter\n",
      "Epoch: 390 | Batch: 002 / 011 | Total loss: 0.842 | Reg loss: 0.026 | Tree loss: 0.842 | Accuracy: 0.660500 | 1.491 sec/iter\n",
      "Epoch: 390 | Batch: 003 / 011 | Total loss: 0.841 | Reg loss: 0.026 | Tree loss: 0.841 | Accuracy: 0.648000 | 1.491 sec/iter\n",
      "Epoch: 390 | Batch: 004 / 011 | Total loss: 0.803 | Reg loss: 0.026 | Tree loss: 0.803 | Accuracy: 0.656000 | 1.491 sec/iter\n",
      "Epoch: 390 | Batch: 005 / 011 | Total loss: 0.841 | Reg loss: 0.026 | Tree loss: 0.841 | Accuracy: 0.637500 | 1.491 sec/iter\n",
      "Epoch: 390 | Batch: 006 / 011 | Total loss: 0.798 | Reg loss: 0.026 | Tree loss: 0.798 | Accuracy: 0.661500 | 1.491 sec/iter\n",
      "Epoch: 390 | Batch: 007 / 011 | Total loss: 0.798 | Reg loss: 0.026 | Tree loss: 0.798 | Accuracy: 0.637000 | 1.491 sec/iter\n",
      "Epoch: 390 | Batch: 008 / 011 | Total loss: 0.794 | Reg loss: 0.026 | Tree loss: 0.794 | Accuracy: 0.636000 | 1.491 sec/iter\n",
      "Epoch: 390 | Batch: 009 / 011 | Total loss: 0.790 | Reg loss: 0.026 | Tree loss: 0.790 | Accuracy: 0.638000 | 1.491 sec/iter\n",
      "Epoch: 390 | Batch: 010 / 011 | Total loss: 0.795 | Reg loss: 0.026 | Tree loss: 0.795 | Accuracy: 0.600683 | 1.491 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 391 | Batch: 000 / 011 | Total loss: 0.890 | Reg loss: 0.026 | Tree loss: 0.890 | Accuracy: 0.619000 | 1.492 sec/iter\n",
      "Epoch: 391 | Batch: 001 / 011 | Total loss: 0.854 | Reg loss: 0.026 | Tree loss: 0.854 | Accuracy: 0.652500 | 1.492 sec/iter\n",
      "Epoch: 391 | Batch: 002 / 011 | Total loss: 0.852 | Reg loss: 0.026 | Tree loss: 0.852 | Accuracy: 0.658500 | 1.492 sec/iter\n",
      "Epoch: 391 | Batch: 003 / 011 | Total loss: 0.835 | Reg loss: 0.026 | Tree loss: 0.835 | Accuracy: 0.656000 | 1.492 sec/iter\n",
      "Epoch: 391 | Batch: 004 / 011 | Total loss: 0.822 | Reg loss: 0.026 | Tree loss: 0.822 | Accuracy: 0.648500 | 1.492 sec/iter\n",
      "Epoch: 391 | Batch: 005 / 011 | Total loss: 0.814 | Reg loss: 0.026 | Tree loss: 0.814 | Accuracy: 0.639000 | 1.492 sec/iter\n",
      "Epoch: 391 | Batch: 006 / 011 | Total loss: 0.794 | Reg loss: 0.026 | Tree loss: 0.794 | Accuracy: 0.651000 | 1.492 sec/iter\n",
      "Epoch: 391 | Batch: 007 / 011 | Total loss: 0.805 | Reg loss: 0.026 | Tree loss: 0.805 | Accuracy: 0.643500 | 1.492 sec/iter\n",
      "Epoch: 391 | Batch: 008 / 011 | Total loss: 0.783 | Reg loss: 0.026 | Tree loss: 0.783 | Accuracy: 0.639000 | 1.492 sec/iter\n",
      "Epoch: 391 | Batch: 009 / 011 | Total loss: 0.780 | Reg loss: 0.026 | Tree loss: 0.780 | Accuracy: 0.645500 | 1.492 sec/iter\n",
      "Epoch: 391 | Batch: 010 / 011 | Total loss: 0.847 | Reg loss: 0.026 | Tree loss: 0.847 | Accuracy: 0.590444 | 1.492 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 392 | Batch: 000 / 011 | Total loss: 0.878 | Reg loss: 0.026 | Tree loss: 0.878 | Accuracy: 0.637500 | 1.492 sec/iter\n",
      "Epoch: 392 | Batch: 001 / 011 | Total loss: 0.857 | Reg loss: 0.026 | Tree loss: 0.857 | Accuracy: 0.649500 | 1.492 sec/iter\n",
      "Epoch: 392 | Batch: 002 / 011 | Total loss: 0.843 | Reg loss: 0.026 | Tree loss: 0.843 | Accuracy: 0.659500 | 1.492 sec/iter\n",
      "Epoch: 392 | Batch: 003 / 011 | Total loss: 0.831 | Reg loss: 0.026 | Tree loss: 0.831 | Accuracy: 0.658000 | 1.492 sec/iter\n",
      "Epoch: 392 | Batch: 004 / 011 | Total loss: 0.820 | Reg loss: 0.026 | Tree loss: 0.820 | Accuracy: 0.649500 | 1.492 sec/iter\n",
      "Epoch: 392 | Batch: 005 / 011 | Total loss: 0.811 | Reg loss: 0.026 | Tree loss: 0.811 | Accuracy: 0.651500 | 1.492 sec/iter\n",
      "Epoch: 392 | Batch: 006 / 011 | Total loss: 0.795 | Reg loss: 0.026 | Tree loss: 0.795 | Accuracy: 0.645500 | 1.492 sec/iter\n",
      "Epoch: 392 | Batch: 007 / 011 | Total loss: 0.816 | Reg loss: 0.026 | Tree loss: 0.816 | Accuracy: 0.627500 | 1.492 sec/iter\n",
      "Epoch: 392 | Batch: 008 / 011 | Total loss: 0.811 | Reg loss: 0.026 | Tree loss: 0.811 | Accuracy: 0.625500 | 1.492 sec/iter\n",
      "Epoch: 392 | Batch: 009 / 011 | Total loss: 0.818 | Reg loss: 0.026 | Tree loss: 0.818 | Accuracy: 0.634500 | 1.492 sec/iter\n",
      "Epoch: 392 | Batch: 010 / 011 | Total loss: 0.771 | Reg loss: 0.026 | Tree loss: 0.771 | Accuracy: 0.655290 | 1.492 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 393 | Batch: 000 / 011 | Total loss: 0.888 | Reg loss: 0.026 | Tree loss: 0.888 | Accuracy: 0.623500 | 1.492 sec/iter\n",
      "Epoch: 393 | Batch: 001 / 011 | Total loss: 0.864 | Reg loss: 0.026 | Tree loss: 0.864 | Accuracy: 0.643500 | 1.492 sec/iter\n",
      "Epoch: 393 | Batch: 002 / 011 | Total loss: 0.834 | Reg loss: 0.026 | Tree loss: 0.834 | Accuracy: 0.662500 | 1.492 sec/iter\n",
      "Epoch: 393 | Batch: 003 / 011 | Total loss: 0.826 | Reg loss: 0.026 | Tree loss: 0.826 | Accuracy: 0.658000 | 1.492 sec/iter\n",
      "Epoch: 393 | Batch: 004 / 011 | Total loss: 0.830 | Reg loss: 0.026 | Tree loss: 0.830 | Accuracy: 0.642500 | 1.492 sec/iter\n",
      "Epoch: 393 | Batch: 005 / 011 | Total loss: 0.820 | Reg loss: 0.026 | Tree loss: 0.820 | Accuracy: 0.646500 | 1.492 sec/iter\n",
      "Epoch: 393 | Batch: 006 / 011 | Total loss: 0.807 | Reg loss: 0.026 | Tree loss: 0.807 | Accuracy: 0.653000 | 1.492 sec/iter\n",
      "Epoch: 393 | Batch: 007 / 011 | Total loss: 0.804 | Reg loss: 0.026 | Tree loss: 0.804 | Accuracy: 0.633000 | 1.492 sec/iter\n",
      "Epoch: 393 | Batch: 008 / 011 | Total loss: 0.811 | Reg loss: 0.026 | Tree loss: 0.811 | Accuracy: 0.632500 | 1.492 sec/iter\n",
      "Epoch: 393 | Batch: 009 / 011 | Total loss: 0.804 | Reg loss: 0.026 | Tree loss: 0.804 | Accuracy: 0.637000 | 1.492 sec/iter\n",
      "Epoch: 393 | Batch: 010 / 011 | Total loss: 0.852 | Reg loss: 0.026 | Tree loss: 0.852 | Accuracy: 0.617747 | 1.492 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 394 | Batch: 000 / 011 | Total loss: 0.865 | Reg loss: 0.026 | Tree loss: 0.865 | Accuracy: 0.641000 | 1.492 sec/iter\n",
      "Epoch: 394 | Batch: 001 / 011 | Total loss: 0.855 | Reg loss: 0.026 | Tree loss: 0.855 | Accuracy: 0.656500 | 1.492 sec/iter\n",
      "Epoch: 394 | Batch: 002 / 011 | Total loss: 0.846 | Reg loss: 0.026 | Tree loss: 0.846 | Accuracy: 0.659000 | 1.492 sec/iter\n",
      "Epoch: 394 | Batch: 003 / 011 | Total loss: 0.851 | Reg loss: 0.026 | Tree loss: 0.851 | Accuracy: 0.643000 | 1.492 sec/iter\n",
      "Epoch: 394 | Batch: 004 / 011 | Total loss: 0.824 | Reg loss: 0.026 | Tree loss: 0.824 | Accuracy: 0.649000 | 1.492 sec/iter\n",
      "Epoch: 394 | Batch: 005 / 011 | Total loss: 0.789 | Reg loss: 0.026 | Tree loss: 0.789 | Accuracy: 0.665500 | 1.492 sec/iter\n",
      "Epoch: 394 | Batch: 006 / 011 | Total loss: 0.803 | Reg loss: 0.026 | Tree loss: 0.803 | Accuracy: 0.656500 | 1.493 sec/iter\n",
      "Epoch: 394 | Batch: 007 / 011 | Total loss: 0.811 | Reg loss: 0.026 | Tree loss: 0.811 | Accuracy: 0.632000 | 1.492 sec/iter\n",
      "Epoch: 394 | Batch: 008 / 011 | Total loss: 0.815 | Reg loss: 0.026 | Tree loss: 0.815 | Accuracy: 0.620500 | 1.493 sec/iter\n",
      "Epoch: 394 | Batch: 009 / 011 | Total loss: 0.808 | Reg loss: 0.026 | Tree loss: 0.808 | Accuracy: 0.629000 | 1.492 sec/iter\n",
      "Epoch: 394 | Batch: 010 / 011 | Total loss: 0.825 | Reg loss: 0.026 | Tree loss: 0.825 | Accuracy: 0.607509 | 1.492 sec/iter\n",
      "Average sparseness: 0.9723756906077345\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "layer 7: 0.9723756906077345\n",
      "layer 8: 0.9723756906077345\n",
      "Epoch: 395 | Batch: 000 / 011 | Total loss: 0.895 | Reg loss: 0.026 | Tree loss: 0.895 | Accuracy: 0.624000 | 1.493 sec/iter\n",
      "Epoch: 395 | Batch: 001 / 011 | Total loss: 0.864 | Reg loss: 0.026 | Tree loss: 0.864 | Accuracy: 0.644500 | 1.493 sec/iter\n",
      "Epoch: 395 | Batch: 002 / 011 | Total loss: 0.840 | Reg loss: 0.026 | Tree loss: 0.840 | Accuracy: 0.649000 | 1.493 sec/iter\n",
      "Epoch: 395 | Batch: 003 / 011 | Total loss: 0.815 | Reg loss: 0.026 | Tree loss: 0.815 | Accuracy: 0.655500 | 1.493 sec/iter\n",
      "Epoch: 395 | Batch: 004 / 011 | Total loss: 0.801 | Reg loss: 0.026 | Tree loss: 0.801 | Accuracy: 0.660500 | 1.493 sec/iter\n",
      "Epoch: 395 | Batch: 005 / 011 | Total loss: 0.809 | Reg loss: 0.026 | Tree loss: 0.809 | Accuracy: 0.638500 | 1.493 sec/iter\n",
      "Epoch: 395 | Batch: 006 / 011 | Total loss: 0.794 | Reg loss: 0.026 | Tree loss: 0.794 | Accuracy: 0.647000 | 1.493 sec/iter\n",
      "Epoch: 395 | Batch: 007 / 011 | Total loss: 0.813 | Reg loss: 0.026 | Tree loss: 0.813 | Accuracy: 0.629000 | 1.493 sec/iter\n",
      "Epoch: 395 | Batch: 008 / 011 | Total loss: 0.824 | Reg loss: 0.026 | Tree loss: 0.824 | Accuracy: 0.618500 | 1.493 sec/iter\n",
      "Epoch: 395 | Batch: 009 / 011 | Total loss: 0.795 | Reg loss: 0.026 | Tree loss: 0.795 | Accuracy: 0.617500 | 1.493 sec/iter\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "iteration = 0\n",
    "for epoch in range(epochs):\n",
    "    # Training\n",
    "    avg_sp = show_sparseness(tree)\n",
    "    sparsity.append(avg_sp)\n",
    "    iteration = do_epoch(tree, tree_loader, device, log_interval, losses, accs, epoch, iteration)\n",
    "    \n",
    "    if epoch % 1 == 0:\n",
    "        prune_tree(tree, factor=5)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel('Iteration')\n",
    "plt.plot(accs, label='Accuracy vs iteration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel('Iteration')\n",
    "plt.plot(losses, label='Loss vs iteration')\n",
    "plt.yscale(\"log\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "weights = tree.inner_nodes.weight.cpu().detach().numpy().flatten()\n",
    "plt.hist(weights, bins=500)\n",
    "weights_std = np.std(weights)\n",
    "weights_mean = np.mean(weights)\n",
    "plt.axvline(weights_mean + weights_std, color='r')\n",
    "plt.axvline(weights_mean - weights_std, color='r')\n",
    "plt.title(f\"Mean: {weights_mean}   |   STD: {weights_std}\")\n",
    "plt.yscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10), dpi=80)\n",
    "avg_height, root = tree.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accumulate samples in the leaves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of patterns: {len(root.get_leaves())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = 'MLE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root.clear_leaves_samples()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (data, target) in enumerate(tree_loader):\n",
    "        root.accumulate_samples(data, method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tighten boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_names = dataset.dataset.all_signals\n",
    "normalizers = torch.tensor([])\n",
    "attr_names = []\n",
    "for signal_name in signal_names:\n",
    "    attr_names += [f\"T{i}.{signal_name}\" for i in range(sampled.shape[-1])]\n",
    "    sensor_norm = torch.tensor([torch.tensor(dataset.dataset.sensor_maxs[signal_name]) for _ in range(sampled.shape[-1])])\n",
    "    normalizers = torch.cat([normalizers, sensor_norm])\n",
    "    \n",
    "\n",
    "# print(attr_names)\n",
    "leaves = root.get_leaves()\n",
    "sum_comprehensibility = 0\n",
    "comprehensibilities = []\n",
    "for pattern_counter, leaf in enumerate(leaves):\n",
    "    leaf.reset_path()\n",
    "    leaf.tighten_with_accumulated_samples()\n",
    "    conds = leaf.get_path_conditions(attr_names)\n",
    "    for cond in conds:\n",
    "        cond.weights = cond.weights / normalizers\n",
    "    print(f\"============== Pattern {pattern_counter + 1} ==============\")\n",
    "    comprehensibilities.append(sum([cond.comprehensibility for cond in conds]))\n",
    "    \n",
    "print(f\"Average comprehensibility: {np.mean(comprehensibilities)}\")\n",
    "print(f\"std comprehensibility: {np.std(comprehensibilities)}\")\n",
    "print(f\"var comprehensibility: {np.var(comprehensibilities)}\")\n",
    "print(f\"minimum comprehensibility: {np.min(comprehensibilities)}\")\n",
    "print(f\"maximum comprehensibility: {np.max(comprehensibilities)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
