{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "%matplotlib inline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "import time\n",
    "import copy\n",
    "import os\n",
    "from queue import LifoQueue\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "from scipy.stats import kurtosis\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import network.cpc\n",
    "from network.cpc import CDCK2\n",
    "from utils.MatplotlibUtils import reduce_dims_and_plot\n",
    "from utils.ClassificationUtiols import onehot_coding\n",
    "from soft_decision_tree.sdt_model import SDT\n",
    "from sklearn import tree as tt\n",
    "\n",
    "# IPython.OutputArea.auto_scroll_threshold = 9999;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the model and the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load the model from: /home/eitan.k/EntangledExplainableClustering/knn_loss_batch_512_k_32/models/epoch_45.pt\n",
      "sensor names: (18 total)\n",
      "- speed\n",
      "- steering_angle\n",
      "- wheel_speed_0\n",
      "- wheel_speed_1\n",
      "- wheel_speed_2\n",
      "- wheel_speed_3\n",
      "- accelerometer_0\n",
      "- accelerometer_1\n",
      "- accelerometer_2\n",
      "- gyro_0\n",
      "- gyro_1\n",
      "- gyro_2\n",
      "- gyro_bias_0\n",
      "- gyro_bias_1\n",
      "- gyro_bias_2\n",
      "- gyro_uncalibrated_0\n",
      "- gyro_uncalibrated_1\n",
      "- gyro_uncalibrated_2\n",
      "Multihorizon size of the model: 30\n",
      "Test split ratio: 0.2\n",
      "Total number of windows in the dataset (without splitting): 101465\n"
     ]
    }
   ],
   "source": [
    "model_path = r'/home/eitan.k/EntangledExplainableClustering/knn_loss_batch_512_k_32/models/epoch_45.pt'\n",
    "dataset_path = r'/home/eitan.k/EntangledExplainableClustering/knn_loss_batch_512_k_32/data/test_data.file'\n",
    "\n",
    "print(f\"Load the model from: {model_path}\")\n",
    "model = torch.load(model_path, map_location='cpu')\n",
    "\n",
    "with open(dataset_path, 'rb') as fp:\n",
    "    dataset = pickle.load(fp)\n",
    "\n",
    "all_sensors = dataset.dataset.all_signals    \n",
    "print(f\"sensor names: ({len(all_sensors)} total)\")\n",
    "\n",
    "for s in all_sensors:\n",
    "    print(f\"- {s}\")\n",
    "    \n",
    "print(f\"Multihorizon size of the model: {model.timestep}\")\n",
    "print(f\"Test split ratio: {len(dataset) / len(dataset.dataset)}\")\n",
    "print(f\"Total number of windows in the dataset (without splitting): {len(dataset.dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 500\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Extract representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74ed208d205c4762b55f4dd82417578b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=20293.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "projections = torch.tensor([])\n",
    "samples = torch.tensor([])\n",
    "device = 'cuda'\n",
    "model = model.to(device).eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    bar = tqdm(total=len(loader.dataset))\n",
    "    for batch in loader:\n",
    "        hidden = CDCK2.init_hidden(len(batch))\n",
    "        batch = batch.to(device)\n",
    "        hidden = hidden.to(device)\n",
    "\n",
    "        y = model.predict(batch, hidden).detach().cpu()\n",
    "        projections = torch.cat([projections, y.detach().cpu()])\n",
    "        samples = torch.cat([samples, batch.detach().cpu()])\n",
    "        bar.update(y.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit GMM and calculate indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8577b8ce2b04594b767d6f8483062c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=75.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "best_score = float('inf')\n",
    "clusters = None\n",
    "range_ = list(range(5, 80))\n",
    "for k in tqdm(range_):\n",
    "    y = GaussianMixture(n_components=k).fit_predict(projections)\n",
    "    cur_score = davies_bouldin_score(projections, y)\n",
    "    scores.append(cur_score)\n",
    "    \n",
    "    if cur_score < best_score:\n",
    "        best_score = cur_score\n",
    "        clusters = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9}\n"
     ]
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('DB Score')\n",
    "plt.plot(range_, scores)\n",
    "best_k = range_[np.argmin(scores)]\n",
    "plt.axvline(best_k, color='r')\n",
    "plt.show()\n",
    "\n",
    "labels = set(clusters)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "distances = pairwise_distances(projections)\n",
    "# distances = np.triu(distances)\n",
    "distances_f = distances.flatten()\n",
    "\n",
    "plt.matshow(distances)\n",
    "plt.colorbar()\n",
    "plt.figure()\n",
    "plt.hist(distances_f[distances_f > 0], bins=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize with T-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "perplexity = 200\n",
    "\n",
    "p = reduce_dims_and_plot(projections,\n",
    "                         y=clusters,\n",
    "                         title=f'perplexity: {perplexity}',\n",
    "                         file_name=None,\n",
    "                         perplexity=perplexity,\n",
    "                         library='Multicore-TSNE',\n",
    "                         perform_PCA=False,\n",
    "                         projected=None,\n",
    "                         figure_type='2d',\n",
    "                         show_figure=True,\n",
    "                         close_figure=False,\n",
    "                         text=None)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Soft-Decision-Tree given the self-labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unnormalized_samples = samples.clone()\n",
    "\n",
    "# for col, sensor in enumerate(tqdm(dataset.dataset.all_signals)):\n",
    "#     denormalizer = dataset.dataset.get_denormalization_for_sensor(sensor)\n",
    "#     unnormalized_samples[:, col, :] = denormalizer(unnormalized_samples[:, col, :])\n",
    "\n",
    "sampled = samples[..., range(0, samples.shape[-1], 200)]\n",
    "\n",
    "samples_f = sampled.flatten(1)\n",
    "tree_dataset = list(zip(samples_f, clusters))\n",
    "batch_size = 2000\n",
    "tree_loader = torch.utils.data.DataLoader(tree_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define how we prune the weights of a node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_node(node_weights, factor=1):\n",
    "    w = node_weights.cpu().detach().numpy()\n",
    "    mean_ = np.mean(w)\n",
    "    std_ = np.std(w)\n",
    "    node_weights[((mean_ - std_ * factor) < node_weights) & (node_weights < (mean_ + std_ * factor))] = 0\n",
    "    return node_weights\n",
    "\n",
    "def prune_node_keep(node_weights, keep=4):\n",
    "    w = node_weights.cpu().detach().numpy()\n",
    "    throw_idx = np.argsort(abs(w))[:-keep]\n",
    "    node_weights[throw_idx] = 0\n",
    "    return node_weights\n",
    "\n",
    "def prune_tree(tree_, factor):\n",
    "    new_weights = tree_.inner_nodes.weight.clone()\n",
    "    for i in range(new_weights.shape[0]):\n",
    "        res = prune_node_keep(new_weights[i, :], factor)\n",
    "        new_weights[i, :] = res\n",
    "\n",
    "    with torch.no_grad():\n",
    "        tree_.inner_nodes.weight.copy_(new_weights)\n",
    "        \n",
    "def sparseness(x):\n",
    "    s = []\n",
    "    for i in range(x.shape[0]):\n",
    "        x_ = x[i, :]\n",
    "        sp = (len(x_) - torch.norm(x_, 0).item()) / len(x_)\n",
    "        s.append(sp)\n",
    "    return np.mean(s)\n",
    "\n",
    "def compute_regularization_by_level(tree):\n",
    "    total_reg = 0\n",
    "    for i in range(tree.inner_nodes.weight.shape[0]):\n",
    "        cur_level = np.floor(np.log2(i+1))\n",
    "        node_reg = torch.norm(tree.inner_nodes.weight[i].view(-1), 2)\n",
    "        total_reg += 2**(-cur_level) * node_reg\n",
    "    return total_reg\n",
    "\n",
    "def show_sparseness(tree):\n",
    "    avg_sp = sparseness(tree.inner_nodes.weight)\n",
    "    print(f\"Average sparseness: {avg_sp}\")\n",
    "    layer = 0\n",
    "    sps = []\n",
    "    for i in range(tree.inner_nodes.weight.shape[0]):\n",
    "        cur_layer = int(np.floor(np.log2(i+1)))\n",
    "        if cur_layer != layer:\n",
    "            print(f\"layer {layer}: {np.mean(sps)}\")\n",
    "            sps = []\n",
    "            layer = cur_layer\n",
    "\n",
    "        x_ = tree.inner_nodes.weight[i, :]\n",
    "        sp = (len(x_) - torch.norm(x_, 0).item()) / len(x_)\n",
    "        sps.append(sp)\n",
    "        \n",
    "    return avg_sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_epoch(model, loader, device, log_interval, losses, accs, epoch, iteration):\n",
    "    model = model.train()\n",
    "    for batch_idx, (data, target) in enumerate(loader):\n",
    "        iteration += 1\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        output, penalty = tree.forward(data)\n",
    "\n",
    "        # Loss\n",
    "        loss_tree = criterion(output, target.view(-1))\n",
    "\n",
    "        # Penalty\n",
    "        loss_tree += penalty\n",
    "\n",
    "        # Sparse regularization\n",
    "#         fc_params = torch.cat([x.view(-1) for x in tree.inner_nodes.parameters()])\n",
    "#         regularization = sparsity_lamda * torch.norm(fc_params, 2)\n",
    "        regularization = sparsity_lamda * compute_regularization_by_level(tree)\n",
    "        loss = loss_tree\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        pred = output.data.max(1)[1]\n",
    "        correct = pred.eq(target.view(-1).data).sum()\n",
    "        accs.append(correct.item() / data.size()[0])\n",
    "\n",
    "        # Print training status\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(f\"Epoch: {epoch:02d} | Batch: {batch_idx:03d} / {len(loader):03d} | Total loss: {loss.item():.3f} | Reg loss: {regularization.item():.3f} | Tree loss: {loss_tree.item():.3f} | Accuracy: {correct.item() / data.size()[0]:03f} | {round((time.time() - start_time) / iteration, 3)} sec/iter\")\n",
    "            \n",
    "    return iteration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 5e-3\n",
    "weight_decay = 5e-4\n",
    "sparsity_lamda = 2e-3\n",
    "epochs = 500\n",
    "output_dim = len(set(clusters))\n",
    "log_interval = 1\n",
    "tree_depth = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT accuracy: 0.9747203469176563\n"
     ]
    }
   ],
   "source": [
    "tree = SDT(input_dim=samples_f.shape[1], output_dim=len(labels), depth=tree_depth, lamda=1e-3, use_cuda=True)\n",
    "optimizer = torch.optim.Adam(tree.parameters(),\n",
    "                                 lr=lr,\n",
    "                                 weight_decay=weight_decay)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "tree = tree.to(device)\n",
    "clf = DecisionTreeClassifier(max_depth=tree_depth).fit(samples_f, clusters)\n",
    "print(f\"DT accuracy: {clf.score(samples_f, clusters)}\")\n",
    "tree.initialize_from_decision_tree(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "accs = []\n",
    "sparsity = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.5933701657458564\n",
      "layer 0: 0.988950276243094\n",
      "layer 1: 0.988950276243094\n",
      "layer 2: 0.988950276243094\n",
      "layer 3: 0.988950276243094\n",
      "layer 4: 0.9271408839779005\n",
      "layer 5: 0.7417127071823204\n",
      "layer 6: 0.6489986187845305\n",
      "Epoch: 00 | Batch: 000 / 011 | Total loss: 2.292 | Reg loss: 0.016 | Tree loss: 2.292 | Accuracy: 0.091000 | 3.684 sec/iter\n",
      "Epoch: 00 | Batch: 001 / 011 | Total loss: 2.286 | Reg loss: 0.016 | Tree loss: 2.286 | Accuracy: 0.087500 | 1.998 sec/iter\n",
      "Epoch: 00 | Batch: 002 / 011 | Total loss: 2.272 | Reg loss: 0.016 | Tree loss: 2.272 | Accuracy: 0.258000 | 1.426 sec/iter\n",
      "Epoch: 00 | Batch: 003 / 011 | Total loss: 2.265 | Reg loss: 0.016 | Tree loss: 2.265 | Accuracy: 0.284500 | 1.137 sec/iter\n",
      "Epoch: 00 | Batch: 004 / 011 | Total loss: 2.255 | Reg loss: 0.016 | Tree loss: 2.255 | Accuracy: 0.302000 | 0.966 sec/iter\n",
      "Epoch: 00 | Batch: 005 / 011 | Total loss: 2.246 | Reg loss: 0.016 | Tree loss: 2.246 | Accuracy: 0.348500 | 0.852 sec/iter\n",
      "Epoch: 00 | Batch: 006 / 011 | Total loss: 2.237 | Reg loss: 0.016 | Tree loss: 2.237 | Accuracy: 0.363000 | 0.772 sec/iter\n",
      "Epoch: 00 | Batch: 007 / 011 | Total loss: 2.224 | Reg loss: 0.016 | Tree loss: 2.224 | Accuracy: 0.400500 | 0.711 sec/iter\n",
      "Epoch: 00 | Batch: 008 / 011 | Total loss: 2.218 | Reg loss: 0.016 | Tree loss: 2.218 | Accuracy: 0.419500 | 0.661 sec/iter\n",
      "Epoch: 00 | Batch: 009 / 011 | Total loss: 2.207 | Reg loss: 0.016 | Tree loss: 2.207 | Accuracy: 0.438500 | 0.621 sec/iter\n",
      "Epoch: 00 | Batch: 010 / 011 | Total loss: 2.198 | Reg loss: 0.016 | Tree loss: 2.198 | Accuracy: 0.457338 | 0.59 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 01 | Batch: 000 / 011 | Total loss: 2.276 | Reg loss: 0.015 | Tree loss: 2.276 | Accuracy: 0.122500 | 1.004 sec/iter\n",
      "Epoch: 01 | Batch: 001 / 011 | Total loss: 2.266 | Reg loss: 0.015 | Tree loss: 2.266 | Accuracy: 0.147000 | 0.946 sec/iter\n",
      "Epoch: 01 | Batch: 002 / 011 | Total loss: 2.259 | Reg loss: 0.015 | Tree loss: 2.259 | Accuracy: 0.191000 | 0.897 sec/iter\n",
      "Epoch: 01 | Batch: 003 / 011 | Total loss: 2.247 | Reg loss: 0.015 | Tree loss: 2.247 | Accuracy: 0.235000 | 0.853 sec/iter\n",
      "Epoch: 01 | Batch: 004 / 011 | Total loss: 2.238 | Reg loss: 0.015 | Tree loss: 2.238 | Accuracy: 0.427500 | 0.815 sec/iter\n",
      "Epoch: 01 | Batch: 005 / 011 | Total loss: 2.223 | Reg loss: 0.015 | Tree loss: 2.223 | Accuracy: 0.414000 | 0.782 sec/iter\n",
      "Epoch: 01 | Batch: 006 / 011 | Total loss: 2.213 | Reg loss: 0.015 | Tree loss: 2.213 | Accuracy: 0.391500 | 0.752 sec/iter\n",
      "Epoch: 01 | Batch: 007 / 011 | Total loss: 2.202 | Reg loss: 0.016 | Tree loss: 2.202 | Accuracy: 0.407500 | 0.725 sec/iter\n",
      "Epoch: 01 | Batch: 008 / 011 | Total loss: 2.198 | Reg loss: 0.016 | Tree loss: 2.198 | Accuracy: 0.412500 | 0.703 sec/iter\n",
      "Epoch: 01 | Batch: 009 / 011 | Total loss: 2.182 | Reg loss: 0.016 | Tree loss: 2.182 | Accuracy: 0.465000 | 0.682 sec/iter\n",
      "Epoch: 01 | Batch: 010 / 011 | Total loss: 2.167 | Reg loss: 0.016 | Tree loss: 2.167 | Accuracy: 0.481229 | 0.662 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 02 | Batch: 000 / 011 | Total loss: 2.263 | Reg loss: 0.015 | Tree loss: 2.263 | Accuracy: 0.266000 | 0.83 sec/iter\n",
      "Epoch: 02 | Batch: 001 / 011 | Total loss: 2.250 | Reg loss: 0.015 | Tree loss: 2.250 | Accuracy: 0.387500 | 0.806 sec/iter\n",
      "Epoch: 02 | Batch: 002 / 011 | Total loss: 2.238 | Reg loss: 0.015 | Tree loss: 2.238 | Accuracy: 0.403500 | 0.783 sec/iter\n",
      "Epoch: 02 | Batch: 003 / 011 | Total loss: 2.223 | Reg loss: 0.015 | Tree loss: 2.223 | Accuracy: 0.419500 | 0.763 sec/iter\n",
      "Epoch: 02 | Batch: 004 / 011 | Total loss: 2.209 | Reg loss: 0.015 | Tree loss: 2.209 | Accuracy: 0.422500 | 0.743 sec/iter\n",
      "Epoch: 02 | Batch: 005 / 011 | Total loss: 2.199 | Reg loss: 0.015 | Tree loss: 2.199 | Accuracy: 0.419500 | 0.724 sec/iter\n",
      "Epoch: 02 | Batch: 006 / 011 | Total loss: 2.187 | Reg loss: 0.016 | Tree loss: 2.187 | Accuracy: 0.396500 | 0.706 sec/iter\n",
      "Epoch: 02 | Batch: 007 / 011 | Total loss: 2.173 | Reg loss: 0.016 | Tree loss: 2.173 | Accuracy: 0.429000 | 0.69 sec/iter\n",
      "Epoch: 02 | Batch: 008 / 011 | Total loss: 2.156 | Reg loss: 0.016 | Tree loss: 2.156 | Accuracy: 0.452000 | 0.674 sec/iter\n",
      "Epoch: 02 | Batch: 009 / 011 | Total loss: 2.150 | Reg loss: 0.016 | Tree loss: 2.150 | Accuracy: 0.440000 | 0.66 sec/iter\n",
      "Epoch: 02 | Batch: 010 / 011 | Total loss: 2.131 | Reg loss: 0.017 | Tree loss: 2.131 | Accuracy: 0.467577 | 0.646 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 03 | Batch: 000 / 011 | Total loss: 2.244 | Reg loss: 0.015 | Tree loss: 2.244 | Accuracy: 0.347500 | 0.677 sec/iter\n",
      "Epoch: 03 | Batch: 001 / 011 | Total loss: 2.229 | Reg loss: 0.015 | Tree loss: 2.229 | Accuracy: 0.387000 | 0.665 sec/iter\n",
      "Epoch: 03 | Batch: 002 / 011 | Total loss: 2.216 | Reg loss: 0.015 | Tree loss: 2.216 | Accuracy: 0.394500 | 0.652 sec/iter\n",
      "Epoch: 03 | Batch: 003 / 011 | Total loss: 2.200 | Reg loss: 0.016 | Tree loss: 2.200 | Accuracy: 0.446000 | 0.641 sec/iter\n",
      "Epoch: 03 | Batch: 004 / 011 | Total loss: 2.186 | Reg loss: 0.016 | Tree loss: 2.186 | Accuracy: 0.491000 | 0.63 sec/iter\n",
      "Epoch: 03 | Batch: 005 / 011 | Total loss: 2.167 | Reg loss: 0.016 | Tree loss: 2.167 | Accuracy: 0.482000 | 0.619 sec/iter\n",
      "Epoch: 03 | Batch: 006 / 011 | Total loss: 2.156 | Reg loss: 0.016 | Tree loss: 2.156 | Accuracy: 0.459500 | 0.61 sec/iter\n",
      "Epoch: 03 | Batch: 007 / 011 | Total loss: 2.135 | Reg loss: 0.016 | Tree loss: 2.135 | Accuracy: 0.465000 | 0.601 sec/iter\n",
      "Epoch: 03 | Batch: 008 / 011 | Total loss: 2.124 | Reg loss: 0.017 | Tree loss: 2.124 | Accuracy: 0.451500 | 0.593 sec/iter\n",
      "Epoch: 03 | Batch: 009 / 011 | Total loss: 2.109 | Reg loss: 0.017 | Tree loss: 2.109 | Accuracy: 0.489500 | 0.585 sec/iter\n",
      "Epoch: 03 | Batch: 010 / 011 | Total loss: 2.118 | Reg loss: 0.017 | Tree loss: 2.118 | Accuracy: 0.474403 | 0.577 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 04 | Batch: 000 / 011 | Total loss: 2.224 | Reg loss: 0.016 | Tree loss: 2.224 | Accuracy: 0.435000 | 0.636 sec/iter\n",
      "Epoch: 04 | Batch: 001 / 011 | Total loss: 2.210 | Reg loss: 0.016 | Tree loss: 2.210 | Accuracy: 0.447000 | 0.627 sec/iter\n",
      "Epoch: 04 | Batch: 002 / 011 | Total loss: 2.190 | Reg loss: 0.016 | Tree loss: 2.190 | Accuracy: 0.493500 | 0.618 sec/iter\n",
      "Epoch: 04 | Batch: 003 / 011 | Total loss: 2.169 | Reg loss: 0.016 | Tree loss: 2.169 | Accuracy: 0.527500 | 0.61 sec/iter\n",
      "Epoch: 04 | Batch: 004 / 011 | Total loss: 2.149 | Reg loss: 0.016 | Tree loss: 2.149 | Accuracy: 0.525500 | 0.602 sec/iter\n",
      "Epoch: 04 | Batch: 005 / 011 | Total loss: 2.133 | Reg loss: 0.016 | Tree loss: 2.133 | Accuracy: 0.503500 | 0.594 sec/iter\n",
      "Epoch: 04 | Batch: 006 / 011 | Total loss: 2.112 | Reg loss: 0.017 | Tree loss: 2.112 | Accuracy: 0.503500 | 0.586 sec/iter\n",
      "Epoch: 04 | Batch: 007 / 011 | Total loss: 2.096 | Reg loss: 0.017 | Tree loss: 2.096 | Accuracy: 0.514500 | 0.579 sec/iter\n",
      "Epoch: 04 | Batch: 008 / 011 | Total loss: 2.089 | Reg loss: 0.017 | Tree loss: 2.089 | Accuracy: 0.473500 | 0.573 sec/iter\n",
      "Epoch: 04 | Batch: 009 / 011 | Total loss: 2.075 | Reg loss: 0.018 | Tree loss: 2.075 | Accuracy: 0.499500 | 0.567 sec/iter\n",
      "Epoch: 04 | Batch: 010 / 011 | Total loss: 2.061 | Reg loss: 0.018 | Tree loss: 2.061 | Accuracy: 0.501706 | 0.561 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 05 | Batch: 000 / 011 | Total loss: 2.204 | Reg loss: 0.016 | Tree loss: 2.204 | Accuracy: 0.363500 | 0.63 sec/iter\n",
      "Epoch: 05 | Batch: 001 / 011 | Total loss: 2.184 | Reg loss: 0.016 | Tree loss: 2.184 | Accuracy: 0.414000 | 0.623 sec/iter\n",
      "Epoch: 05 | Batch: 002 / 011 | Total loss: 2.160 | Reg loss: 0.017 | Tree loss: 2.160 | Accuracy: 0.416000 | 0.617 sec/iter\n",
      "Epoch: 05 | Batch: 003 / 011 | Total loss: 2.136 | Reg loss: 0.017 | Tree loss: 2.136 | Accuracy: 0.530500 | 0.61 sec/iter\n",
      "Epoch: 05 | Batch: 004 / 011 | Total loss: 2.120 | Reg loss: 0.017 | Tree loss: 2.120 | Accuracy: 0.503000 | 0.604 sec/iter\n",
      "Epoch: 05 | Batch: 005 / 011 | Total loss: 2.094 | Reg loss: 0.017 | Tree loss: 2.094 | Accuracy: 0.503000 | 0.597 sec/iter\n",
      "Epoch: 05 | Batch: 006 / 011 | Total loss: 2.078 | Reg loss: 0.017 | Tree loss: 2.078 | Accuracy: 0.479000 | 0.591 sec/iter\n",
      "Epoch: 05 | Batch: 007 / 011 | Total loss: 2.054 | Reg loss: 0.018 | Tree loss: 2.054 | Accuracy: 0.471000 | 0.585 sec/iter\n",
      "Epoch: 05 | Batch: 008 / 011 | Total loss: 2.031 | Reg loss: 0.018 | Tree loss: 2.031 | Accuracy: 0.510000 | 0.579 sec/iter\n",
      "Epoch: 05 | Batch: 009 / 011 | Total loss: 2.019 | Reg loss: 0.018 | Tree loss: 2.019 | Accuracy: 0.489000 | 0.573 sec/iter\n",
      "Epoch: 05 | Batch: 010 / 011 | Total loss: 2.022 | Reg loss: 0.019 | Tree loss: 2.022 | Accuracy: 0.498294 | 0.568 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 06 | Batch: 000 / 011 | Total loss: 2.177 | Reg loss: 0.017 | Tree loss: 2.177 | Accuracy: 0.402500 | 0.574 sec/iter\n",
      "Epoch: 06 | Batch: 001 / 011 | Total loss: 2.157 | Reg loss: 0.017 | Tree loss: 2.157 | Accuracy: 0.403500 | 0.568 sec/iter\n",
      "Epoch: 06 | Batch: 002 / 011 | Total loss: 2.126 | Reg loss: 0.017 | Tree loss: 2.126 | Accuracy: 0.449000 | 0.563 sec/iter\n",
      "Epoch: 06 | Batch: 003 / 011 | Total loss: 2.098 | Reg loss: 0.017 | Tree loss: 2.098 | Accuracy: 0.485000 | 0.558 sec/iter\n",
      "Epoch: 06 | Batch: 004 / 011 | Total loss: 2.076 | Reg loss: 0.018 | Tree loss: 2.076 | Accuracy: 0.477000 | 0.553 sec/iter\n",
      "Epoch: 06 | Batch: 005 / 011 | Total loss: 2.051 | Reg loss: 0.018 | Tree loss: 2.051 | Accuracy: 0.467000 | 0.549 sec/iter\n",
      "Epoch: 06 | Batch: 006 / 011 | Total loss: 2.030 | Reg loss: 0.018 | Tree loss: 2.030 | Accuracy: 0.467500 | 0.545 sec/iter\n",
      "Epoch: 06 | Batch: 007 / 011 | Total loss: 2.010 | Reg loss: 0.018 | Tree loss: 2.010 | Accuracy: 0.456500 | 0.54 sec/iter\n",
      "Epoch: 06 | Batch: 008 / 011 | Total loss: 1.980 | Reg loss: 0.019 | Tree loss: 1.980 | Accuracy: 0.487500 | 0.536 sec/iter\n",
      "Epoch: 06 | Batch: 009 / 011 | Total loss: 1.975 | Reg loss: 0.019 | Tree loss: 1.975 | Accuracy: 0.473000 | 0.532 sec/iter\n",
      "Epoch: 06 | Batch: 010 / 011 | Total loss: 1.922 | Reg loss: 0.019 | Tree loss: 1.922 | Accuracy: 0.522184 | 0.528 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 07 | Batch: 000 / 011 | Total loss: 2.150 | Reg loss: 0.018 | Tree loss: 2.150 | Accuracy: 0.409000 | 0.563 sec/iter\n",
      "Epoch: 07 | Batch: 001 / 011 | Total loss: 2.124 | Reg loss: 0.018 | Tree loss: 2.124 | Accuracy: 0.402000 | 0.559 sec/iter\n",
      "Epoch: 07 | Batch: 002 / 011 | Total loss: 2.094 | Reg loss: 0.018 | Tree loss: 2.094 | Accuracy: 0.460000 | 0.555 sec/iter\n",
      "Epoch: 07 | Batch: 003 / 011 | Total loss: 2.064 | Reg loss: 0.018 | Tree loss: 2.064 | Accuracy: 0.461000 | 0.551 sec/iter\n",
      "Epoch: 07 | Batch: 004 / 011 | Total loss: 2.034 | Reg loss: 0.018 | Tree loss: 2.034 | Accuracy: 0.443000 | 0.547 sec/iter\n",
      "Epoch: 07 | Batch: 005 / 011 | Total loss: 1.999 | Reg loss: 0.019 | Tree loss: 1.999 | Accuracy: 0.435500 | 0.543 sec/iter\n",
      "Epoch: 07 | Batch: 006 / 011 | Total loss: 1.981 | Reg loss: 0.019 | Tree loss: 1.981 | Accuracy: 0.430000 | 0.539 sec/iter\n",
      "Epoch: 07 | Batch: 007 / 011 | Total loss: 1.954 | Reg loss: 0.019 | Tree loss: 1.954 | Accuracy: 0.421500 | 0.535 sec/iter\n",
      "Epoch: 07 | Batch: 008 / 011 | Total loss: 1.948 | Reg loss: 0.019 | Tree loss: 1.948 | Accuracy: 0.440500 | 0.531 sec/iter\n",
      "Epoch: 07 | Batch: 009 / 011 | Total loss: 1.926 | Reg loss: 0.020 | Tree loss: 1.926 | Accuracy: 0.438500 | 0.528 sec/iter\n",
      "Epoch: 07 | Batch: 010 / 011 | Total loss: 1.913 | Reg loss: 0.020 | Tree loss: 1.913 | Accuracy: 0.447099 | 0.524 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 08 | Batch: 000 / 011 | Total loss: 2.121 | Reg loss: 0.018 | Tree loss: 2.121 | Accuracy: 0.426500 | 0.54 sec/iter\n",
      "Epoch: 08 | Batch: 001 / 011 | Total loss: 2.092 | Reg loss: 0.019 | Tree loss: 2.092 | Accuracy: 0.435000 | 0.537 sec/iter\n",
      "Epoch: 08 | Batch: 002 / 011 | Total loss: 2.053 | Reg loss: 0.019 | Tree loss: 2.053 | Accuracy: 0.461500 | 0.533 sec/iter\n",
      "Epoch: 08 | Batch: 003 / 011 | Total loss: 2.014 | Reg loss: 0.019 | Tree loss: 2.014 | Accuracy: 0.523000 | 0.53 sec/iter\n",
      "Epoch: 08 | Batch: 004 / 011 | Total loss: 1.993 | Reg loss: 0.019 | Tree loss: 1.993 | Accuracy: 0.486500 | 0.526 sec/iter\n",
      "Epoch: 08 | Batch: 005 / 011 | Total loss: 1.955 | Reg loss: 0.019 | Tree loss: 1.955 | Accuracy: 0.493500 | 0.523 sec/iter\n",
      "Epoch: 08 | Batch: 006 / 011 | Total loss: 1.927 | Reg loss: 0.019 | Tree loss: 1.927 | Accuracy: 0.489500 | 0.52 sec/iter\n",
      "Epoch: 08 | Batch: 007 / 011 | Total loss: 1.915 | Reg loss: 0.020 | Tree loss: 1.915 | Accuracy: 0.490000 | 0.516 sec/iter\n",
      "Epoch: 08 | Batch: 008 / 011 | Total loss: 1.896 | Reg loss: 0.020 | Tree loss: 1.896 | Accuracy: 0.457500 | 0.513 sec/iter\n",
      "Epoch: 08 | Batch: 009 / 011 | Total loss: 1.866 | Reg loss: 0.020 | Tree loss: 1.866 | Accuracy: 0.471000 | 0.51 sec/iter\n",
      "Epoch: 08 | Batch: 010 / 011 | Total loss: 1.888 | Reg loss: 0.021 | Tree loss: 1.888 | Accuracy: 0.488055 | 0.507 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 09 | Batch: 000 / 011 | Total loss: 2.087 | Reg loss: 0.019 | Tree loss: 2.087 | Accuracy: 0.417000 | 0.514 sec/iter\n",
      "Epoch: 09 | Batch: 001 / 011 | Total loss: 2.056 | Reg loss: 0.019 | Tree loss: 2.056 | Accuracy: 0.460500 | 0.511 sec/iter\n",
      "Epoch: 09 | Batch: 002 / 011 | Total loss: 2.014 | Reg loss: 0.019 | Tree loss: 2.014 | Accuracy: 0.503500 | 0.508 sec/iter\n",
      "Epoch: 09 | Batch: 003 / 011 | Total loss: 1.981 | Reg loss: 0.019 | Tree loss: 1.981 | Accuracy: 0.502500 | 0.505 sec/iter\n",
      "Epoch: 09 | Batch: 004 / 011 | Total loss: 1.940 | Reg loss: 0.020 | Tree loss: 1.940 | Accuracy: 0.488500 | 0.503 sec/iter\n",
      "Epoch: 09 | Batch: 005 / 011 | Total loss: 1.921 | Reg loss: 0.020 | Tree loss: 1.921 | Accuracy: 0.493000 | 0.5 sec/iter\n",
      "Epoch: 09 | Batch: 006 / 011 | Total loss: 1.887 | Reg loss: 0.020 | Tree loss: 1.887 | Accuracy: 0.491500 | 0.497 sec/iter\n",
      "Epoch: 09 | Batch: 007 / 011 | Total loss: 1.866 | Reg loss: 0.020 | Tree loss: 1.866 | Accuracy: 0.485000 | 0.494 sec/iter\n",
      "Epoch: 09 | Batch: 008 / 011 | Total loss: 1.833 | Reg loss: 0.021 | Tree loss: 1.833 | Accuracy: 0.479500 | 0.492 sec/iter\n",
      "Epoch: 09 | Batch: 009 / 011 | Total loss: 1.831 | Reg loss: 0.021 | Tree loss: 1.831 | Accuracy: 0.487000 | 0.489 sec/iter\n",
      "Epoch: 09 | Batch: 010 / 011 | Total loss: 1.775 | Reg loss: 0.021 | Tree loss: 1.775 | Accuracy: 0.508532 | 0.486 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 10 | Batch: 000 / 011 | Total loss: 2.059 | Reg loss: 0.020 | Tree loss: 2.059 | Accuracy: 0.416500 | 0.491 sec/iter\n",
      "Epoch: 10 | Batch: 001 / 011 | Total loss: 2.015 | Reg loss: 0.020 | Tree loss: 2.015 | Accuracy: 0.464500 | 0.489 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 | Batch: 002 / 011 | Total loss: 1.983 | Reg loss: 0.020 | Tree loss: 1.983 | Accuracy: 0.535000 | 0.486 sec/iter\n",
      "Epoch: 10 | Batch: 003 / 011 | Total loss: 1.938 | Reg loss: 0.020 | Tree loss: 1.938 | Accuracy: 0.507000 | 0.484 sec/iter\n",
      "Epoch: 10 | Batch: 004 / 011 | Total loss: 1.894 | Reg loss: 0.020 | Tree loss: 1.894 | Accuracy: 0.510500 | 0.482 sec/iter\n",
      "Epoch: 10 | Batch: 005 / 011 | Total loss: 1.870 | Reg loss: 0.020 | Tree loss: 1.870 | Accuracy: 0.474500 | 0.479 sec/iter\n",
      "Epoch: 10 | Batch: 006 / 011 | Total loss: 1.835 | Reg loss: 0.021 | Tree loss: 1.835 | Accuracy: 0.484000 | 0.477 sec/iter\n",
      "Epoch: 10 | Batch: 007 / 011 | Total loss: 1.807 | Reg loss: 0.021 | Tree loss: 1.807 | Accuracy: 0.484500 | 0.475 sec/iter\n",
      "Epoch: 10 | Batch: 008 / 011 | Total loss: 1.800 | Reg loss: 0.021 | Tree loss: 1.800 | Accuracy: 0.475500 | 0.473 sec/iter\n",
      "Epoch: 10 | Batch: 009 / 011 | Total loss: 1.774 | Reg loss: 0.021 | Tree loss: 1.774 | Accuracy: 0.489000 | 0.471 sec/iter\n",
      "Epoch: 10 | Batch: 010 / 011 | Total loss: 1.773 | Reg loss: 0.022 | Tree loss: 1.773 | Accuracy: 0.515358 | 0.469 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 11 | Batch: 000 / 011 | Total loss: 2.016 | Reg loss: 0.020 | Tree loss: 2.016 | Accuracy: 0.401000 | 0.491 sec/iter\n",
      "Epoch: 11 | Batch: 001 / 011 | Total loss: 1.976 | Reg loss: 0.020 | Tree loss: 1.976 | Accuracy: 0.452500 | 0.489 sec/iter\n",
      "Epoch: 11 | Batch: 002 / 011 | Total loss: 1.937 | Reg loss: 0.021 | Tree loss: 1.937 | Accuracy: 0.553500 | 0.487 sec/iter\n",
      "Epoch: 11 | Batch: 003 / 011 | Total loss: 1.898 | Reg loss: 0.021 | Tree loss: 1.898 | Accuracy: 0.537000 | 0.485 sec/iter\n",
      "Epoch: 11 | Batch: 004 / 011 | Total loss: 1.861 | Reg loss: 0.021 | Tree loss: 1.861 | Accuracy: 0.486000 | 0.482 sec/iter\n",
      "Epoch: 11 | Batch: 005 / 011 | Total loss: 1.818 | Reg loss: 0.021 | Tree loss: 1.818 | Accuracy: 0.500000 | 0.48 sec/iter\n",
      "Epoch: 11 | Batch: 006 / 011 | Total loss: 1.778 | Reg loss: 0.021 | Tree loss: 1.778 | Accuracy: 0.503000 | 0.478 sec/iter\n",
      "Epoch: 11 | Batch: 007 / 011 | Total loss: 1.773 | Reg loss: 0.021 | Tree loss: 1.773 | Accuracy: 0.480000 | 0.476 sec/iter\n",
      "Epoch: 11 | Batch: 008 / 011 | Total loss: 1.757 | Reg loss: 0.022 | Tree loss: 1.757 | Accuracy: 0.499000 | 0.474 sec/iter\n",
      "Epoch: 11 | Batch: 009 / 011 | Total loss: 1.756 | Reg loss: 0.022 | Tree loss: 1.756 | Accuracy: 0.477000 | 0.472 sec/iter\n",
      "Epoch: 11 | Batch: 010 / 011 | Total loss: 1.695 | Reg loss: 0.022 | Tree loss: 1.695 | Accuracy: 0.494881 | 0.47 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 12 | Batch: 000 / 011 | Total loss: 1.982 | Reg loss: 0.021 | Tree loss: 1.982 | Accuracy: 0.418000 | 0.499 sec/iter\n",
      "Epoch: 12 | Batch: 001 / 011 | Total loss: 1.949 | Reg loss: 0.021 | Tree loss: 1.949 | Accuracy: 0.478000 | 0.497 sec/iter\n",
      "Epoch: 12 | Batch: 002 / 011 | Total loss: 1.896 | Reg loss: 0.021 | Tree loss: 1.896 | Accuracy: 0.531500 | 0.495 sec/iter\n",
      "Epoch: 12 | Batch: 003 / 011 | Total loss: 1.851 | Reg loss: 0.021 | Tree loss: 1.851 | Accuracy: 0.524000 | 0.494 sec/iter\n",
      "Epoch: 12 | Batch: 004 / 011 | Total loss: 1.822 | Reg loss: 0.021 | Tree loss: 1.822 | Accuracy: 0.488000 | 0.492 sec/iter\n",
      "Epoch: 12 | Batch: 005 / 011 | Total loss: 1.765 | Reg loss: 0.022 | Tree loss: 1.765 | Accuracy: 0.509000 | 0.49 sec/iter\n",
      "Epoch: 12 | Batch: 006 / 011 | Total loss: 1.754 | Reg loss: 0.022 | Tree loss: 1.754 | Accuracy: 0.516000 | 0.488 sec/iter\n",
      "Epoch: 12 | Batch: 007 / 011 | Total loss: 1.737 | Reg loss: 0.022 | Tree loss: 1.737 | Accuracy: 0.502000 | 0.486 sec/iter\n",
      "Epoch: 12 | Batch: 008 / 011 | Total loss: 1.714 | Reg loss: 0.022 | Tree loss: 1.714 | Accuracy: 0.495500 | 0.485 sec/iter\n",
      "Epoch: 12 | Batch: 009 / 011 | Total loss: 1.705 | Reg loss: 0.022 | Tree loss: 1.705 | Accuracy: 0.475500 | 0.483 sec/iter\n",
      "Epoch: 12 | Batch: 010 / 011 | Total loss: 1.719 | Reg loss: 0.023 | Tree loss: 1.719 | Accuracy: 0.453925 | 0.481 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 13 | Batch: 000 / 011 | Total loss: 1.952 | Reg loss: 0.021 | Tree loss: 1.952 | Accuracy: 0.434000 | 0.488 sec/iter\n",
      "Epoch: 13 | Batch: 001 / 011 | Total loss: 1.908 | Reg loss: 0.021 | Tree loss: 1.908 | Accuracy: 0.496000 | 0.486 sec/iter\n",
      "Epoch: 13 | Batch: 002 / 011 | Total loss: 1.856 | Reg loss: 0.022 | Tree loss: 1.856 | Accuracy: 0.552500 | 0.485 sec/iter\n",
      "Epoch: 13 | Batch: 003 / 011 | Total loss: 1.810 | Reg loss: 0.022 | Tree loss: 1.810 | Accuracy: 0.520000 | 0.483 sec/iter\n",
      "Epoch: 13 | Batch: 004 / 011 | Total loss: 1.773 | Reg loss: 0.022 | Tree loss: 1.773 | Accuracy: 0.504000 | 0.482 sec/iter\n",
      "Epoch: 13 | Batch: 005 / 011 | Total loss: 1.740 | Reg loss: 0.022 | Tree loss: 1.740 | Accuracy: 0.499500 | 0.48 sec/iter\n",
      "Epoch: 13 | Batch: 006 / 011 | Total loss: 1.711 | Reg loss: 0.022 | Tree loss: 1.711 | Accuracy: 0.496500 | 0.478 sec/iter\n",
      "Epoch: 13 | Batch: 007 / 011 | Total loss: 1.688 | Reg loss: 0.022 | Tree loss: 1.688 | Accuracy: 0.473000 | 0.477 sec/iter\n",
      "Epoch: 13 | Batch: 008 / 011 | Total loss: 1.679 | Reg loss: 0.023 | Tree loss: 1.679 | Accuracy: 0.478000 | 0.475 sec/iter\n",
      "Epoch: 13 | Batch: 009 / 011 | Total loss: 1.664 | Reg loss: 0.023 | Tree loss: 1.664 | Accuracy: 0.501000 | 0.474 sec/iter\n",
      "Epoch: 13 | Batch: 010 / 011 | Total loss: 1.626 | Reg loss: 0.023 | Tree loss: 1.626 | Accuracy: 0.481229 | 0.472 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 14 | Batch: 000 / 011 | Total loss: 1.923 | Reg loss: 0.022 | Tree loss: 1.923 | Accuracy: 0.423000 | 0.492 sec/iter\n",
      "Epoch: 14 | Batch: 001 / 011 | Total loss: 1.874 | Reg loss: 0.022 | Tree loss: 1.874 | Accuracy: 0.502000 | 0.491 sec/iter\n",
      "Epoch: 14 | Batch: 002 / 011 | Total loss: 1.825 | Reg loss: 0.022 | Tree loss: 1.825 | Accuracy: 0.562000 | 0.489 sec/iter\n",
      "Epoch: 14 | Batch: 003 / 011 | Total loss: 1.774 | Reg loss: 0.022 | Tree loss: 1.774 | Accuracy: 0.550500 | 0.487 sec/iter\n",
      "Epoch: 14 | Batch: 004 / 011 | Total loss: 1.747 | Reg loss: 0.022 | Tree loss: 1.747 | Accuracy: 0.534000 | 0.485 sec/iter\n",
      "Epoch: 14 | Batch: 005 / 011 | Total loss: 1.711 | Reg loss: 0.022 | Tree loss: 1.711 | Accuracy: 0.520500 | 0.484 sec/iter\n",
      "Epoch: 14 | Batch: 006 / 011 | Total loss: 1.678 | Reg loss: 0.023 | Tree loss: 1.678 | Accuracy: 0.503000 | 0.482 sec/iter\n",
      "Epoch: 14 | Batch: 007 / 011 | Total loss: 1.641 | Reg loss: 0.023 | Tree loss: 1.641 | Accuracy: 0.504000 | 0.48 sec/iter\n",
      "Epoch: 14 | Batch: 008 / 011 | Total loss: 1.646 | Reg loss: 0.023 | Tree loss: 1.646 | Accuracy: 0.487000 | 0.478 sec/iter\n",
      "Epoch: 14 | Batch: 009 / 011 | Total loss: 1.629 | Reg loss: 0.023 | Tree loss: 1.629 | Accuracy: 0.490500 | 0.477 sec/iter\n",
      "Epoch: 14 | Batch: 010 / 011 | Total loss: 1.623 | Reg loss: 0.023 | Tree loss: 1.623 | Accuracy: 0.518771 | 0.475 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 15 | Batch: 000 / 011 | Total loss: 1.884 | Reg loss: 0.022 | Tree loss: 1.884 | Accuracy: 0.425500 | 0.491 sec/iter\n",
      "Epoch: 15 | Batch: 001 / 011 | Total loss: 1.831 | Reg loss: 0.022 | Tree loss: 1.831 | Accuracy: 0.512000 | 0.489 sec/iter\n",
      "Epoch: 15 | Batch: 002 / 011 | Total loss: 1.803 | Reg loss: 0.022 | Tree loss: 1.803 | Accuracy: 0.546000 | 0.488 sec/iter\n",
      "Epoch: 15 | Batch: 003 / 011 | Total loss: 1.747 | Reg loss: 0.022 | Tree loss: 1.747 | Accuracy: 0.537000 | 0.486 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15 | Batch: 004 / 011 | Total loss: 1.710 | Reg loss: 0.023 | Tree loss: 1.710 | Accuracy: 0.511500 | 0.485 sec/iter\n",
      "Epoch: 15 | Batch: 005 / 011 | Total loss: 1.671 | Reg loss: 0.023 | Tree loss: 1.671 | Accuracy: 0.505000 | 0.483 sec/iter\n",
      "Epoch: 15 | Batch: 006 / 011 | Total loss: 1.634 | Reg loss: 0.023 | Tree loss: 1.634 | Accuracy: 0.525000 | 0.481 sec/iter\n",
      "Epoch: 15 | Batch: 007 / 011 | Total loss: 1.622 | Reg loss: 0.023 | Tree loss: 1.622 | Accuracy: 0.513500 | 0.48 sec/iter\n",
      "Epoch: 15 | Batch: 008 / 011 | Total loss: 1.600 | Reg loss: 0.023 | Tree loss: 1.600 | Accuracy: 0.530000 | 0.479 sec/iter\n",
      "Epoch: 15 | Batch: 009 / 011 | Total loss: 1.596 | Reg loss: 0.023 | Tree loss: 1.596 | Accuracy: 0.515000 | 0.477 sec/iter\n",
      "Epoch: 15 | Batch: 010 / 011 | Total loss: 1.560 | Reg loss: 0.024 | Tree loss: 1.560 | Accuracy: 0.505119 | 0.476 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 16 | Batch: 000 / 011 | Total loss: 1.850 | Reg loss: 0.023 | Tree loss: 1.850 | Accuracy: 0.427500 | 0.495 sec/iter\n",
      "Epoch: 16 | Batch: 001 / 011 | Total loss: 1.815 | Reg loss: 0.023 | Tree loss: 1.815 | Accuracy: 0.532500 | 0.494 sec/iter\n",
      "Epoch: 16 | Batch: 002 / 011 | Total loss: 1.763 | Reg loss: 0.023 | Tree loss: 1.763 | Accuracy: 0.547000 | 0.493 sec/iter\n",
      "Epoch: 16 | Batch: 003 / 011 | Total loss: 1.707 | Reg loss: 0.023 | Tree loss: 1.707 | Accuracy: 0.545000 | 0.491 sec/iter\n",
      "Epoch: 16 | Batch: 004 / 011 | Total loss: 1.680 | Reg loss: 0.023 | Tree loss: 1.680 | Accuracy: 0.503500 | 0.49 sec/iter\n",
      "Epoch: 16 | Batch: 005 / 011 | Total loss: 1.628 | Reg loss: 0.023 | Tree loss: 1.628 | Accuracy: 0.526000 | 0.489 sec/iter\n",
      "Epoch: 16 | Batch: 006 / 011 | Total loss: 1.609 | Reg loss: 0.023 | Tree loss: 1.609 | Accuracy: 0.521500 | 0.488 sec/iter\n",
      "Epoch: 16 | Batch: 007 / 011 | Total loss: 1.573 | Reg loss: 0.023 | Tree loss: 1.573 | Accuracy: 0.508500 | 0.487 sec/iter\n",
      "Epoch: 16 | Batch: 008 / 011 | Total loss: 1.569 | Reg loss: 0.024 | Tree loss: 1.569 | Accuracy: 0.532000 | 0.486 sec/iter\n",
      "Epoch: 16 | Batch: 009 / 011 | Total loss: 1.558 | Reg loss: 0.024 | Tree loss: 1.558 | Accuracy: 0.501500 | 0.485 sec/iter\n",
      "Epoch: 16 | Batch: 010 / 011 | Total loss: 1.536 | Reg loss: 0.024 | Tree loss: 1.536 | Accuracy: 0.535836 | 0.483 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 17 | Batch: 000 / 011 | Total loss: 1.812 | Reg loss: 0.023 | Tree loss: 1.812 | Accuracy: 0.466500 | 0.515 sec/iter\n",
      "Epoch: 17 | Batch: 001 / 011 | Total loss: 1.771 | Reg loss: 0.023 | Tree loss: 1.771 | Accuracy: 0.519500 | 0.513 sec/iter\n",
      "Epoch: 17 | Batch: 002 / 011 | Total loss: 1.726 | Reg loss: 0.023 | Tree loss: 1.726 | Accuracy: 0.540500 | 0.512 sec/iter\n",
      "Epoch: 17 | Batch: 003 / 011 | Total loss: 1.687 | Reg loss: 0.023 | Tree loss: 1.687 | Accuracy: 0.550000 | 0.51 sec/iter\n",
      "Epoch: 17 | Batch: 004 / 011 | Total loss: 1.656 | Reg loss: 0.023 | Tree loss: 1.656 | Accuracy: 0.517500 | 0.509 sec/iter\n",
      "Epoch: 17 | Batch: 005 / 011 | Total loss: 1.602 | Reg loss: 0.023 | Tree loss: 1.602 | Accuracy: 0.517500 | 0.507 sec/iter\n",
      "Epoch: 17 | Batch: 006 / 011 | Total loss: 1.570 | Reg loss: 0.024 | Tree loss: 1.570 | Accuracy: 0.522000 | 0.505 sec/iter\n",
      "Epoch: 17 | Batch: 007 / 011 | Total loss: 1.560 | Reg loss: 0.024 | Tree loss: 1.560 | Accuracy: 0.507500 | 0.504 sec/iter\n",
      "Epoch: 17 | Batch: 008 / 011 | Total loss: 1.535 | Reg loss: 0.024 | Tree loss: 1.535 | Accuracy: 0.536500 | 0.503 sec/iter\n",
      "Epoch: 17 | Batch: 009 / 011 | Total loss: 1.543 | Reg loss: 0.024 | Tree loss: 1.543 | Accuracy: 0.520000 | 0.502 sec/iter\n",
      "Epoch: 17 | Batch: 010 / 011 | Total loss: 1.555 | Reg loss: 0.024 | Tree loss: 1.555 | Accuracy: 0.477816 | 0.5 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 18 | Batch: 000 / 011 | Total loss: 1.792 | Reg loss: 0.023 | Tree loss: 1.792 | Accuracy: 0.471000 | 0.525 sec/iter\n",
      "Epoch: 18 | Batch: 001 / 011 | Total loss: 1.749 | Reg loss: 0.023 | Tree loss: 1.749 | Accuracy: 0.506000 | 0.524 sec/iter\n",
      "Epoch: 18 | Batch: 002 / 011 | Total loss: 1.701 | Reg loss: 0.023 | Tree loss: 1.701 | Accuracy: 0.545000 | 0.522 sec/iter\n",
      "Epoch: 18 | Batch: 003 / 011 | Total loss: 1.649 | Reg loss: 0.024 | Tree loss: 1.649 | Accuracy: 0.529000 | 0.521 sec/iter\n",
      "Epoch: 18 | Batch: 004 / 011 | Total loss: 1.600 | Reg loss: 0.024 | Tree loss: 1.600 | Accuracy: 0.533500 | 0.52 sec/iter\n",
      "Epoch: 18 | Batch: 005 / 011 | Total loss: 1.578 | Reg loss: 0.024 | Tree loss: 1.578 | Accuracy: 0.506500 | 0.518 sec/iter\n",
      "Epoch: 18 | Batch: 006 / 011 | Total loss: 1.554 | Reg loss: 0.024 | Tree loss: 1.554 | Accuracy: 0.496500 | 0.517 sec/iter\n",
      "Epoch: 18 | Batch: 007 / 011 | Total loss: 1.527 | Reg loss: 0.024 | Tree loss: 1.527 | Accuracy: 0.513500 | 0.516 sec/iter\n",
      "Epoch: 18 | Batch: 008 / 011 | Total loss: 1.513 | Reg loss: 0.024 | Tree loss: 1.513 | Accuracy: 0.522500 | 0.514 sec/iter\n",
      "Epoch: 18 | Batch: 009 / 011 | Total loss: 1.509 | Reg loss: 0.024 | Tree loss: 1.509 | Accuracy: 0.530000 | 0.513 sec/iter\n",
      "Epoch: 18 | Batch: 010 / 011 | Total loss: 1.526 | Reg loss: 0.024 | Tree loss: 1.526 | Accuracy: 0.477816 | 0.512 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 19 | Batch: 000 / 011 | Total loss: 1.777 | Reg loss: 0.024 | Tree loss: 1.777 | Accuracy: 0.464500 | 0.515 sec/iter\n",
      "Epoch: 19 | Batch: 001 / 011 | Total loss: 1.716 | Reg loss: 0.024 | Tree loss: 1.716 | Accuracy: 0.522500 | 0.514 sec/iter\n",
      "Epoch: 19 | Batch: 002 / 011 | Total loss: 1.670 | Reg loss: 0.024 | Tree loss: 1.670 | Accuracy: 0.562500 | 0.512 sec/iter\n",
      "Epoch: 19 | Batch: 003 / 011 | Total loss: 1.630 | Reg loss: 0.024 | Tree loss: 1.630 | Accuracy: 0.532000 | 0.511 sec/iter\n",
      "Epoch: 19 | Batch: 004 / 011 | Total loss: 1.578 | Reg loss: 0.024 | Tree loss: 1.578 | Accuracy: 0.540000 | 0.51 sec/iter\n",
      "Epoch: 19 | Batch: 005 / 011 | Total loss: 1.552 | Reg loss: 0.024 | Tree loss: 1.552 | Accuracy: 0.526000 | 0.509 sec/iter\n",
      "Epoch: 19 | Batch: 006 / 011 | Total loss: 1.507 | Reg loss: 0.024 | Tree loss: 1.507 | Accuracy: 0.531500 | 0.507 sec/iter\n",
      "Epoch: 19 | Batch: 007 / 011 | Total loss: 1.501 | Reg loss: 0.024 | Tree loss: 1.501 | Accuracy: 0.526500 | 0.506 sec/iter\n",
      "Epoch: 19 | Batch: 008 / 011 | Total loss: 1.477 | Reg loss: 0.025 | Tree loss: 1.477 | Accuracy: 0.518000 | 0.505 sec/iter\n",
      "Epoch: 19 | Batch: 009 / 011 | Total loss: 1.484 | Reg loss: 0.025 | Tree loss: 1.484 | Accuracy: 0.501000 | 0.504 sec/iter\n",
      "Epoch: 19 | Batch: 010 / 011 | Total loss: 1.460 | Reg loss: 0.025 | Tree loss: 1.460 | Accuracy: 0.511945 | 0.503 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 20 | Batch: 000 / 011 | Total loss: 1.737 | Reg loss: 0.024 | Tree loss: 1.737 | Accuracy: 0.480000 | 0.533 sec/iter\n",
      "Epoch: 20 | Batch: 001 / 011 | Total loss: 1.688 | Reg loss: 0.024 | Tree loss: 1.688 | Accuracy: 0.538000 | 0.532 sec/iter\n",
      "Epoch: 20 | Batch: 002 / 011 | Total loss: 1.637 | Reg loss: 0.024 | Tree loss: 1.637 | Accuracy: 0.550000 | 0.531 sec/iter\n",
      "Epoch: 20 | Batch: 003 / 011 | Total loss: 1.613 | Reg loss: 0.024 | Tree loss: 1.613 | Accuracy: 0.522500 | 0.53 sec/iter\n",
      "Epoch: 20 | Batch: 004 / 011 | Total loss: 1.562 | Reg loss: 0.024 | Tree loss: 1.562 | Accuracy: 0.538500 | 0.529 sec/iter\n",
      "Epoch: 20 | Batch: 005 / 011 | Total loss: 1.521 | Reg loss: 0.024 | Tree loss: 1.521 | Accuracy: 0.531000 | 0.528 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20 | Batch: 006 / 011 | Total loss: 1.498 | Reg loss: 0.025 | Tree loss: 1.498 | Accuracy: 0.518500 | 0.526 sec/iter\n",
      "Epoch: 20 | Batch: 007 / 011 | Total loss: 1.463 | Reg loss: 0.025 | Tree loss: 1.463 | Accuracy: 0.519500 | 0.525 sec/iter\n",
      "Epoch: 20 | Batch: 008 / 011 | Total loss: 1.468 | Reg loss: 0.025 | Tree loss: 1.468 | Accuracy: 0.521500 | 0.524 sec/iter\n",
      "Epoch: 20 | Batch: 009 / 011 | Total loss: 1.439 | Reg loss: 0.025 | Tree loss: 1.439 | Accuracy: 0.541500 | 0.523 sec/iter\n",
      "Epoch: 20 | Batch: 010 / 011 | Total loss: 1.427 | Reg loss: 0.025 | Tree loss: 1.427 | Accuracy: 0.484642 | 0.522 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 21 | Batch: 000 / 011 | Total loss: 1.705 | Reg loss: 0.024 | Tree loss: 1.705 | Accuracy: 0.505500 | 0.533 sec/iter\n",
      "Epoch: 21 | Batch: 001 / 011 | Total loss: 1.650 | Reg loss: 0.024 | Tree loss: 1.650 | Accuracy: 0.543000 | 0.532 sec/iter\n",
      "Epoch: 21 | Batch: 002 / 011 | Total loss: 1.610 | Reg loss: 0.024 | Tree loss: 1.610 | Accuracy: 0.555500 | 0.531 sec/iter\n",
      "Epoch: 21 | Batch: 003 / 011 | Total loss: 1.557 | Reg loss: 0.024 | Tree loss: 1.557 | Accuracy: 0.551000 | 0.529 sec/iter\n",
      "Epoch: 21 | Batch: 004 / 011 | Total loss: 1.530 | Reg loss: 0.025 | Tree loss: 1.530 | Accuracy: 0.518500 | 0.528 sec/iter\n",
      "Epoch: 21 | Batch: 005 / 011 | Total loss: 1.492 | Reg loss: 0.025 | Tree loss: 1.492 | Accuracy: 0.541500 | 0.527 sec/iter\n",
      "Epoch: 21 | Batch: 006 / 011 | Total loss: 1.481 | Reg loss: 0.025 | Tree loss: 1.481 | Accuracy: 0.498500 | 0.526 sec/iter\n",
      "Epoch: 21 | Batch: 007 / 011 | Total loss: 1.451 | Reg loss: 0.025 | Tree loss: 1.451 | Accuracy: 0.532000 | 0.525 sec/iter\n",
      "Epoch: 21 | Batch: 008 / 011 | Total loss: 1.445 | Reg loss: 0.025 | Tree loss: 1.445 | Accuracy: 0.526000 | 0.524 sec/iter\n",
      "Epoch: 21 | Batch: 009 / 011 | Total loss: 1.423 | Reg loss: 0.025 | Tree loss: 1.423 | Accuracy: 0.520000 | 0.523 sec/iter\n",
      "Epoch: 21 | Batch: 010 / 011 | Total loss: 1.388 | Reg loss: 0.025 | Tree loss: 1.388 | Accuracy: 0.559727 | 0.521 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 22 | Batch: 000 / 011 | Total loss: 1.704 | Reg loss: 0.025 | Tree loss: 1.704 | Accuracy: 0.489000 | 0.539 sec/iter\n",
      "Epoch: 22 | Batch: 001 / 011 | Total loss: 1.620 | Reg loss: 0.025 | Tree loss: 1.620 | Accuracy: 0.565000 | 0.537 sec/iter\n",
      "Epoch: 22 | Batch: 002 / 011 | Total loss: 1.581 | Reg loss: 0.025 | Tree loss: 1.581 | Accuracy: 0.561000 | 0.536 sec/iter\n",
      "Epoch: 22 | Batch: 003 / 011 | Total loss: 1.548 | Reg loss: 0.025 | Tree loss: 1.548 | Accuracy: 0.558000 | 0.535 sec/iter\n",
      "Epoch: 22 | Batch: 004 / 011 | Total loss: 1.505 | Reg loss: 0.025 | Tree loss: 1.505 | Accuracy: 0.537000 | 0.533 sec/iter\n",
      "Epoch: 22 | Batch: 005 / 011 | Total loss: 1.465 | Reg loss: 0.025 | Tree loss: 1.465 | Accuracy: 0.543000 | 0.532 sec/iter\n",
      "Epoch: 22 | Batch: 006 / 011 | Total loss: 1.455 | Reg loss: 0.025 | Tree loss: 1.455 | Accuracy: 0.520500 | 0.531 sec/iter\n",
      "Epoch: 22 | Batch: 007 / 011 | Total loss: 1.426 | Reg loss: 0.025 | Tree loss: 1.426 | Accuracy: 0.526000 | 0.529 sec/iter\n",
      "Epoch: 22 | Batch: 008 / 011 | Total loss: 1.413 | Reg loss: 0.025 | Tree loss: 1.413 | Accuracy: 0.518500 | 0.528 sec/iter\n",
      "Epoch: 22 | Batch: 009 / 011 | Total loss: 1.411 | Reg loss: 0.025 | Tree loss: 1.411 | Accuracy: 0.530500 | 0.527 sec/iter\n",
      "Epoch: 22 | Batch: 010 / 011 | Total loss: 1.394 | Reg loss: 0.025 | Tree loss: 1.394 | Accuracy: 0.552901 | 0.526 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 23 | Batch: 000 / 011 | Total loss: 1.648 | Reg loss: 0.025 | Tree loss: 1.648 | Accuracy: 0.538000 | 0.545 sec/iter\n",
      "Epoch: 23 | Batch: 001 / 011 | Total loss: 1.604 | Reg loss: 0.025 | Tree loss: 1.604 | Accuracy: 0.556000 | 0.544 sec/iter\n",
      "Epoch: 23 | Batch: 002 / 011 | Total loss: 1.559 | Reg loss: 0.025 | Tree loss: 1.559 | Accuracy: 0.570000 | 0.543 sec/iter\n",
      "Epoch: 23 | Batch: 003 / 011 | Total loss: 1.509 | Reg loss: 0.025 | Tree loss: 1.509 | Accuracy: 0.560000 | 0.542 sec/iter\n",
      "Epoch: 23 | Batch: 004 / 011 | Total loss: 1.476 | Reg loss: 0.025 | Tree loss: 1.476 | Accuracy: 0.523500 | 0.541 sec/iter\n",
      "Epoch: 23 | Batch: 005 / 011 | Total loss: 1.460 | Reg loss: 0.025 | Tree loss: 1.460 | Accuracy: 0.520500 | 0.54 sec/iter\n",
      "Epoch: 23 | Batch: 006 / 011 | Total loss: 1.414 | Reg loss: 0.025 | Tree loss: 1.414 | Accuracy: 0.529000 | 0.539 sec/iter\n",
      "Epoch: 23 | Batch: 007 / 011 | Total loss: 1.402 | Reg loss: 0.025 | Tree loss: 1.402 | Accuracy: 0.542500 | 0.538 sec/iter\n",
      "Epoch: 23 | Batch: 008 / 011 | Total loss: 1.390 | Reg loss: 0.025 | Tree loss: 1.390 | Accuracy: 0.519000 | 0.537 sec/iter\n",
      "Epoch: 23 | Batch: 009 / 011 | Total loss: 1.408 | Reg loss: 0.026 | Tree loss: 1.408 | Accuracy: 0.514000 | 0.536 sec/iter\n",
      "Epoch: 23 | Batch: 010 / 011 | Total loss: 1.314 | Reg loss: 0.026 | Tree loss: 1.314 | Accuracy: 0.552901 | 0.535 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 24 | Batch: 000 / 011 | Total loss: 1.625 | Reg loss: 0.025 | Tree loss: 1.625 | Accuracy: 0.548500 | 0.542 sec/iter\n",
      "Epoch: 24 | Batch: 001 / 011 | Total loss: 1.579 | Reg loss: 0.025 | Tree loss: 1.579 | Accuracy: 0.551500 | 0.541 sec/iter\n",
      "Epoch: 24 | Batch: 002 / 011 | Total loss: 1.534 | Reg loss: 0.025 | Tree loss: 1.534 | Accuracy: 0.559500 | 0.54 sec/iter\n",
      "Epoch: 24 | Batch: 003 / 011 | Total loss: 1.506 | Reg loss: 0.025 | Tree loss: 1.506 | Accuracy: 0.530500 | 0.539 sec/iter\n",
      "Epoch: 24 | Batch: 004 / 011 | Total loss: 1.468 | Reg loss: 0.025 | Tree loss: 1.468 | Accuracy: 0.534000 | 0.538 sec/iter\n",
      "Epoch: 24 | Batch: 005 / 011 | Total loss: 1.421 | Reg loss: 0.025 | Tree loss: 1.421 | Accuracy: 0.518500 | 0.537 sec/iter\n",
      "Epoch: 24 | Batch: 006 / 011 | Total loss: 1.400 | Reg loss: 0.025 | Tree loss: 1.400 | Accuracy: 0.510000 | 0.536 sec/iter\n",
      "Epoch: 24 | Batch: 007 / 011 | Total loss: 1.377 | Reg loss: 0.026 | Tree loss: 1.377 | Accuracy: 0.525500 | 0.535 sec/iter\n",
      "Epoch: 24 | Batch: 008 / 011 | Total loss: 1.366 | Reg loss: 0.026 | Tree loss: 1.366 | Accuracy: 0.542500 | 0.534 sec/iter\n",
      "Epoch: 24 | Batch: 009 / 011 | Total loss: 1.365 | Reg loss: 0.026 | Tree loss: 1.365 | Accuracy: 0.532000 | 0.533 sec/iter\n",
      "Epoch: 24 | Batch: 010 / 011 | Total loss: 1.347 | Reg loss: 0.026 | Tree loss: 1.347 | Accuracy: 0.552901 | 0.532 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 25 | Batch: 000 / 011 | Total loss: 1.607 | Reg loss: 0.025 | Tree loss: 1.607 | Accuracy: 0.515000 | 0.555 sec/iter\n",
      "Epoch: 25 | Batch: 001 / 011 | Total loss: 1.559 | Reg loss: 0.025 | Tree loss: 1.559 | Accuracy: 0.551500 | 0.554 sec/iter\n",
      "Epoch: 25 | Batch: 002 / 011 | Total loss: 1.514 | Reg loss: 0.025 | Tree loss: 1.514 | Accuracy: 0.573500 | 0.553 sec/iter\n",
      "Epoch: 25 | Batch: 003 / 011 | Total loss: 1.460 | Reg loss: 0.025 | Tree loss: 1.460 | Accuracy: 0.577500 | 0.552 sec/iter\n",
      "Epoch: 25 | Batch: 004 / 011 | Total loss: 1.421 | Reg loss: 0.025 | Tree loss: 1.421 | Accuracy: 0.557000 | 0.551 sec/iter\n",
      "Epoch: 25 | Batch: 005 / 011 | Total loss: 1.384 | Reg loss: 0.026 | Tree loss: 1.384 | Accuracy: 0.543000 | 0.55 sec/iter\n",
      "Epoch: 25 | Batch: 006 / 011 | Total loss: 1.405 | Reg loss: 0.026 | Tree loss: 1.405 | Accuracy: 0.519000 | 0.548 sec/iter\n",
      "Epoch: 25 | Batch: 007 / 011 | Total loss: 1.366 | Reg loss: 0.026 | Tree loss: 1.366 | Accuracy: 0.535500 | 0.547 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25 | Batch: 008 / 011 | Total loss: 1.372 | Reg loss: 0.026 | Tree loss: 1.372 | Accuracy: 0.516500 | 0.546 sec/iter\n",
      "Epoch: 25 | Batch: 009 / 011 | Total loss: 1.366 | Reg loss: 0.026 | Tree loss: 1.366 | Accuracy: 0.524500 | 0.545 sec/iter\n",
      "Epoch: 25 | Batch: 010 / 011 | Total loss: 1.357 | Reg loss: 0.026 | Tree loss: 1.357 | Accuracy: 0.484642 | 0.544 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 26 | Batch: 000 / 011 | Total loss: 1.593 | Reg loss: 0.025 | Tree loss: 1.593 | Accuracy: 0.523500 | 0.546 sec/iter\n",
      "Epoch: 26 | Batch: 001 / 011 | Total loss: 1.550 | Reg loss: 0.025 | Tree loss: 1.550 | Accuracy: 0.557500 | 0.545 sec/iter\n",
      "Epoch: 26 | Batch: 002 / 011 | Total loss: 1.504 | Reg loss: 0.025 | Tree loss: 1.504 | Accuracy: 0.559000 | 0.544 sec/iter\n",
      "Epoch: 26 | Batch: 003 / 011 | Total loss: 1.449 | Reg loss: 0.026 | Tree loss: 1.449 | Accuracy: 0.554500 | 0.543 sec/iter\n",
      "Epoch: 26 | Batch: 004 / 011 | Total loss: 1.409 | Reg loss: 0.026 | Tree loss: 1.409 | Accuracy: 0.533500 | 0.542 sec/iter\n",
      "Epoch: 26 | Batch: 005 / 011 | Total loss: 1.387 | Reg loss: 0.026 | Tree loss: 1.387 | Accuracy: 0.540000 | 0.541 sec/iter\n",
      "Epoch: 26 | Batch: 006 / 011 | Total loss: 1.356 | Reg loss: 0.026 | Tree loss: 1.356 | Accuracy: 0.525000 | 0.54 sec/iter\n",
      "Epoch: 26 | Batch: 007 / 011 | Total loss: 1.352 | Reg loss: 0.026 | Tree loss: 1.352 | Accuracy: 0.547500 | 0.539 sec/iter\n",
      "Epoch: 26 | Batch: 008 / 011 | Total loss: 1.329 | Reg loss: 0.026 | Tree loss: 1.329 | Accuracy: 0.535500 | 0.538 sec/iter\n",
      "Epoch: 26 | Batch: 009 / 011 | Total loss: 1.340 | Reg loss: 0.026 | Tree loss: 1.340 | Accuracy: 0.525000 | 0.537 sec/iter\n",
      "Epoch: 26 | Batch: 010 / 011 | Total loss: 1.313 | Reg loss: 0.026 | Tree loss: 1.313 | Accuracy: 0.535836 | 0.536 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 27 | Batch: 000 / 011 | Total loss: 1.552 | Reg loss: 0.026 | Tree loss: 1.552 | Accuracy: 0.547000 | 0.544 sec/iter\n",
      "Epoch: 27 | Batch: 001 / 011 | Total loss: 1.508 | Reg loss: 0.026 | Tree loss: 1.508 | Accuracy: 0.580500 | 0.543 sec/iter\n",
      "Epoch: 27 | Batch: 002 / 011 | Total loss: 1.488 | Reg loss: 0.026 | Tree loss: 1.488 | Accuracy: 0.559500 | 0.541 sec/iter\n",
      "Epoch: 27 | Batch: 003 / 011 | Total loss: 1.451 | Reg loss: 0.026 | Tree loss: 1.451 | Accuracy: 0.533500 | 0.54 sec/iter\n",
      "Epoch: 27 | Batch: 004 / 011 | Total loss: 1.385 | Reg loss: 0.026 | Tree loss: 1.385 | Accuracy: 0.558000 | 0.539 sec/iter\n",
      "Epoch: 27 | Batch: 005 / 011 | Total loss: 1.354 | Reg loss: 0.026 | Tree loss: 1.354 | Accuracy: 0.538500 | 0.538 sec/iter\n",
      "Epoch: 27 | Batch: 006 / 011 | Total loss: 1.357 | Reg loss: 0.026 | Tree loss: 1.357 | Accuracy: 0.525500 | 0.537 sec/iter\n",
      "Epoch: 27 | Batch: 007 / 011 | Total loss: 1.347 | Reg loss: 0.026 | Tree loss: 1.347 | Accuracy: 0.505500 | 0.536 sec/iter\n",
      "Epoch: 27 | Batch: 008 / 011 | Total loss: 1.309 | Reg loss: 0.026 | Tree loss: 1.309 | Accuracy: 0.525500 | 0.535 sec/iter\n",
      "Epoch: 27 | Batch: 009 / 011 | Total loss: 1.329 | Reg loss: 0.026 | Tree loss: 1.329 | Accuracy: 0.527500 | 0.534 sec/iter\n",
      "Epoch: 27 | Batch: 010 / 011 | Total loss: 1.301 | Reg loss: 0.026 | Tree loss: 1.301 | Accuracy: 0.505119 | 0.533 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 28 | Batch: 000 / 011 | Total loss: 1.549 | Reg loss: 0.026 | Tree loss: 1.549 | Accuracy: 0.549500 | 0.546 sec/iter\n",
      "Epoch: 28 | Batch: 001 / 011 | Total loss: 1.513 | Reg loss: 0.026 | Tree loss: 1.513 | Accuracy: 0.576000 | 0.544 sec/iter\n",
      "Epoch: 28 | Batch: 002 / 011 | Total loss: 1.444 | Reg loss: 0.026 | Tree loss: 1.444 | Accuracy: 0.565000 | 0.543 sec/iter\n",
      "Epoch: 28 | Batch: 003 / 011 | Total loss: 1.423 | Reg loss: 0.026 | Tree loss: 1.423 | Accuracy: 0.560000 | 0.542 sec/iter\n",
      "Epoch: 28 | Batch: 004 / 011 | Total loss: 1.378 | Reg loss: 0.026 | Tree loss: 1.378 | Accuracy: 0.528000 | 0.542 sec/iter\n",
      "Epoch: 28 | Batch: 005 / 011 | Total loss: 1.342 | Reg loss: 0.026 | Tree loss: 1.342 | Accuracy: 0.524500 | 0.541 sec/iter\n",
      "Epoch: 28 | Batch: 006 / 011 | Total loss: 1.337 | Reg loss: 0.026 | Tree loss: 1.337 | Accuracy: 0.531000 | 0.54 sec/iter\n",
      "Epoch: 28 | Batch: 007 / 011 | Total loss: 1.310 | Reg loss: 0.026 | Tree loss: 1.310 | Accuracy: 0.531000 | 0.539 sec/iter\n",
      "Epoch: 28 | Batch: 008 / 011 | Total loss: 1.304 | Reg loss: 0.026 | Tree loss: 1.304 | Accuracy: 0.534500 | 0.538 sec/iter\n",
      "Epoch: 28 | Batch: 009 / 011 | Total loss: 1.294 | Reg loss: 0.026 | Tree loss: 1.294 | Accuracy: 0.529500 | 0.537 sec/iter\n",
      "Epoch: 28 | Batch: 010 / 011 | Total loss: 1.318 | Reg loss: 0.026 | Tree loss: 1.318 | Accuracy: 0.508532 | 0.536 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 29 | Batch: 000 / 011 | Total loss: 1.521 | Reg loss: 0.026 | Tree loss: 1.521 | Accuracy: 0.550000 | 0.539 sec/iter\n",
      "Epoch: 29 | Batch: 001 / 011 | Total loss: 1.472 | Reg loss: 0.026 | Tree loss: 1.472 | Accuracy: 0.557500 | 0.538 sec/iter\n",
      "Epoch: 29 | Batch: 002 / 011 | Total loss: 1.440 | Reg loss: 0.026 | Tree loss: 1.440 | Accuracy: 0.573000 | 0.537 sec/iter\n",
      "Epoch: 29 | Batch: 003 / 011 | Total loss: 1.406 | Reg loss: 0.026 | Tree loss: 1.406 | Accuracy: 0.572500 | 0.536 sec/iter\n",
      "Epoch: 29 | Batch: 004 / 011 | Total loss: 1.369 | Reg loss: 0.026 | Tree loss: 1.369 | Accuracy: 0.545500 | 0.536 sec/iter\n",
      "Epoch: 29 | Batch: 005 / 011 | Total loss: 1.343 | Reg loss: 0.026 | Tree loss: 1.343 | Accuracy: 0.542000 | 0.535 sec/iter\n",
      "Epoch: 29 | Batch: 006 / 011 | Total loss: 1.315 | Reg loss: 0.026 | Tree loss: 1.315 | Accuracy: 0.544000 | 0.534 sec/iter\n",
      "Epoch: 29 | Batch: 007 / 011 | Total loss: 1.294 | Reg loss: 0.026 | Tree loss: 1.294 | Accuracy: 0.548500 | 0.533 sec/iter\n",
      "Epoch: 29 | Batch: 008 / 011 | Total loss: 1.288 | Reg loss: 0.026 | Tree loss: 1.288 | Accuracy: 0.528000 | 0.532 sec/iter\n",
      "Epoch: 29 | Batch: 009 / 011 | Total loss: 1.270 | Reg loss: 0.026 | Tree loss: 1.270 | Accuracy: 0.535000 | 0.531 sec/iter\n",
      "Epoch: 29 | Batch: 010 / 011 | Total loss: 1.241 | Reg loss: 0.026 | Tree loss: 1.241 | Accuracy: 0.556314 | 0.53 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 30 | Batch: 000 / 011 | Total loss: 1.497 | Reg loss: 0.026 | Tree loss: 1.497 | Accuracy: 0.556000 | 0.538 sec/iter\n",
      "Epoch: 30 | Batch: 001 / 011 | Total loss: 1.451 | Reg loss: 0.026 | Tree loss: 1.451 | Accuracy: 0.556500 | 0.537 sec/iter\n",
      "Epoch: 30 | Batch: 002 / 011 | Total loss: 1.418 | Reg loss: 0.026 | Tree loss: 1.418 | Accuracy: 0.546000 | 0.536 sec/iter\n",
      "Epoch: 30 | Batch: 003 / 011 | Total loss: 1.384 | Reg loss: 0.026 | Tree loss: 1.384 | Accuracy: 0.555500 | 0.535 sec/iter\n",
      "Epoch: 30 | Batch: 004 / 011 | Total loss: 1.353 | Reg loss: 0.026 | Tree loss: 1.353 | Accuracy: 0.532500 | 0.534 sec/iter\n",
      "Epoch: 30 | Batch: 005 / 011 | Total loss: 1.321 | Reg loss: 0.026 | Tree loss: 1.321 | Accuracy: 0.528000 | 0.533 sec/iter\n",
      "Epoch: 30 | Batch: 006 / 011 | Total loss: 1.289 | Reg loss: 0.026 | Tree loss: 1.289 | Accuracy: 0.528000 | 0.532 sec/iter\n",
      "Epoch: 30 | Batch: 007 / 011 | Total loss: 1.272 | Reg loss: 0.026 | Tree loss: 1.272 | Accuracy: 0.541500 | 0.531 sec/iter\n",
      "Epoch: 30 | Batch: 008 / 011 | Total loss: 1.284 | Reg loss: 0.026 | Tree loss: 1.284 | Accuracy: 0.526500 | 0.53 sec/iter\n",
      "Epoch: 30 | Batch: 009 / 011 | Total loss: 1.275 | Reg loss: 0.026 | Tree loss: 1.275 | Accuracy: 0.525000 | 0.53 sec/iter\n",
      "Epoch: 30 | Batch: 010 / 011 | Total loss: 1.234 | Reg loss: 0.026 | Tree loss: 1.234 | Accuracy: 0.563140 | 0.529 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 31 | Batch: 000 / 011 | Total loss: 1.496 | Reg loss: 0.026 | Tree loss: 1.496 | Accuracy: 0.519500 | 0.534 sec/iter\n",
      "Epoch: 31 | Batch: 001 / 011 | Total loss: 1.438 | Reg loss: 0.026 | Tree loss: 1.438 | Accuracy: 0.547500 | 0.534 sec/iter\n",
      "Epoch: 31 | Batch: 002 / 011 | Total loss: 1.399 | Reg loss: 0.026 | Tree loss: 1.399 | Accuracy: 0.571000 | 0.533 sec/iter\n",
      "Epoch: 31 | Batch: 003 / 011 | Total loss: 1.373 | Reg loss: 0.026 | Tree loss: 1.373 | Accuracy: 0.540000 | 0.532 sec/iter\n",
      "Epoch: 31 | Batch: 004 / 011 | Total loss: 1.321 | Reg loss: 0.026 | Tree loss: 1.321 | Accuracy: 0.551000 | 0.531 sec/iter\n",
      "Epoch: 31 | Batch: 005 / 011 | Total loss: 1.303 | Reg loss: 0.026 | Tree loss: 1.303 | Accuracy: 0.518000 | 0.53 sec/iter\n",
      "Epoch: 31 | Batch: 006 / 011 | Total loss: 1.279 | Reg loss: 0.026 | Tree loss: 1.279 | Accuracy: 0.539000 | 0.529 sec/iter\n",
      "Epoch: 31 | Batch: 007 / 011 | Total loss: 1.270 | Reg loss: 0.026 | Tree loss: 1.270 | Accuracy: 0.550000 | 0.529 sec/iter\n",
      "Epoch: 31 | Batch: 008 / 011 | Total loss: 1.250 | Reg loss: 0.026 | Tree loss: 1.250 | Accuracy: 0.541000 | 0.528 sec/iter\n",
      "Epoch: 31 | Batch: 009 / 011 | Total loss: 1.267 | Reg loss: 0.026 | Tree loss: 1.267 | Accuracy: 0.525500 | 0.527 sec/iter\n",
      "Epoch: 31 | Batch: 010 / 011 | Total loss: 1.199 | Reg loss: 0.026 | Tree loss: 1.199 | Accuracy: 0.576792 | 0.526 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 32 | Batch: 000 / 011 | Total loss: 1.473 | Reg loss: 0.026 | Tree loss: 1.473 | Accuracy: 0.555500 | 0.533 sec/iter\n",
      "Epoch: 32 | Batch: 001 / 011 | Total loss: 1.412 | Reg loss: 0.026 | Tree loss: 1.412 | Accuracy: 0.558500 | 0.532 sec/iter\n",
      "Epoch: 32 | Batch: 002 / 011 | Total loss: 1.391 | Reg loss: 0.026 | Tree loss: 1.391 | Accuracy: 0.552000 | 0.531 sec/iter\n",
      "Epoch: 32 | Batch: 003 / 011 | Total loss: 1.347 | Reg loss: 0.026 | Tree loss: 1.347 | Accuracy: 0.529500 | 0.53 sec/iter\n",
      "Epoch: 32 | Batch: 004 / 011 | Total loss: 1.306 | Reg loss: 0.026 | Tree loss: 1.306 | Accuracy: 0.544500 | 0.529 sec/iter\n",
      "Epoch: 32 | Batch: 005 / 011 | Total loss: 1.295 | Reg loss: 0.026 | Tree loss: 1.295 | Accuracy: 0.527000 | 0.529 sec/iter\n",
      "Epoch: 32 | Batch: 006 / 011 | Total loss: 1.274 | Reg loss: 0.026 | Tree loss: 1.274 | Accuracy: 0.532500 | 0.528 sec/iter\n",
      "Epoch: 32 | Batch: 007 / 011 | Total loss: 1.246 | Reg loss: 0.026 | Tree loss: 1.246 | Accuracy: 0.551000 | 0.527 sec/iter\n",
      "Epoch: 32 | Batch: 008 / 011 | Total loss: 1.261 | Reg loss: 0.026 | Tree loss: 1.261 | Accuracy: 0.526000 | 0.526 sec/iter\n",
      "Epoch: 32 | Batch: 009 / 011 | Total loss: 1.255 | Reg loss: 0.026 | Tree loss: 1.255 | Accuracy: 0.529500 | 0.525 sec/iter\n",
      "Epoch: 32 | Batch: 010 / 011 | Total loss: 1.264 | Reg loss: 0.026 | Tree loss: 1.264 | Accuracy: 0.511945 | 0.524 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 33 | Batch: 000 / 011 | Total loss: 1.439 | Reg loss: 0.026 | Tree loss: 1.439 | Accuracy: 0.554000 | 0.528 sec/iter\n",
      "Epoch: 33 | Batch: 001 / 011 | Total loss: 1.403 | Reg loss: 0.026 | Tree loss: 1.403 | Accuracy: 0.556000 | 0.527 sec/iter\n",
      "Epoch: 33 | Batch: 002 / 011 | Total loss: 1.376 | Reg loss: 0.026 | Tree loss: 1.376 | Accuracy: 0.557000 | 0.526 sec/iter\n",
      "Epoch: 33 | Batch: 003 / 011 | Total loss: 1.337 | Reg loss: 0.026 | Tree loss: 1.337 | Accuracy: 0.575500 | 0.526 sec/iter\n",
      "Epoch: 33 | Batch: 004 / 011 | Total loss: 1.313 | Reg loss: 0.026 | Tree loss: 1.313 | Accuracy: 0.544000 | 0.525 sec/iter\n",
      "Epoch: 33 | Batch: 005 / 011 | Total loss: 1.286 | Reg loss: 0.026 | Tree loss: 1.286 | Accuracy: 0.535000 | 0.524 sec/iter\n",
      "Epoch: 33 | Batch: 006 / 011 | Total loss: 1.252 | Reg loss: 0.026 | Tree loss: 1.252 | Accuracy: 0.542500 | 0.523 sec/iter\n",
      "Epoch: 33 | Batch: 007 / 011 | Total loss: 1.245 | Reg loss: 0.026 | Tree loss: 1.245 | Accuracy: 0.538000 | 0.522 sec/iter\n",
      "Epoch: 33 | Batch: 008 / 011 | Total loss: 1.247 | Reg loss: 0.026 | Tree loss: 1.247 | Accuracy: 0.529500 | 0.521 sec/iter\n",
      "Epoch: 33 | Batch: 009 / 011 | Total loss: 1.247 | Reg loss: 0.026 | Tree loss: 1.247 | Accuracy: 0.522000 | 0.52 sec/iter\n",
      "Epoch: 33 | Batch: 010 / 011 | Total loss: 1.264 | Reg loss: 0.026 | Tree loss: 1.264 | Accuracy: 0.508532 | 0.519 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 34 | Batch: 000 / 011 | Total loss: 1.436 | Reg loss: 0.026 | Tree loss: 1.436 | Accuracy: 0.513000 | 0.521 sec/iter\n",
      "Epoch: 34 | Batch: 001 / 011 | Total loss: 1.404 | Reg loss: 0.026 | Tree loss: 1.404 | Accuracy: 0.562000 | 0.52 sec/iter\n",
      "Epoch: 34 | Batch: 002 / 011 | Total loss: 1.336 | Reg loss: 0.026 | Tree loss: 1.336 | Accuracy: 0.577000 | 0.519 sec/iter\n",
      "Epoch: 34 | Batch: 003 / 011 | Total loss: 1.323 | Reg loss: 0.026 | Tree loss: 1.323 | Accuracy: 0.533000 | 0.519 sec/iter\n",
      "Epoch: 34 | Batch: 004 / 011 | Total loss: 1.268 | Reg loss: 0.026 | Tree loss: 1.268 | Accuracy: 0.558000 | 0.518 sec/iter\n",
      "Epoch: 34 | Batch: 005 / 011 | Total loss: 1.268 | Reg loss: 0.026 | Tree loss: 1.268 | Accuracy: 0.534000 | 0.517 sec/iter\n",
      "Epoch: 34 | Batch: 006 / 011 | Total loss: 1.215 | Reg loss: 0.026 | Tree loss: 1.215 | Accuracy: 0.562500 | 0.517 sec/iter\n",
      "Epoch: 34 | Batch: 007 / 011 | Total loss: 1.260 | Reg loss: 0.026 | Tree loss: 1.260 | Accuracy: 0.527000 | 0.516 sec/iter\n",
      "Epoch: 34 | Batch: 008 / 011 | Total loss: 1.252 | Reg loss: 0.026 | Tree loss: 1.252 | Accuracy: 0.503000 | 0.515 sec/iter\n",
      "Epoch: 34 | Batch: 009 / 011 | Total loss: 1.205 | Reg loss: 0.026 | Tree loss: 1.205 | Accuracy: 0.550000 | 0.515 sec/iter\n",
      "Epoch: 34 | Batch: 010 / 011 | Total loss: 1.220 | Reg loss: 0.026 | Tree loss: 1.220 | Accuracy: 0.566553 | 0.514 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 35 | Batch: 000 / 011 | Total loss: 1.414 | Reg loss: 0.026 | Tree loss: 1.414 | Accuracy: 0.542500 | 0.522 sec/iter\n",
      "Epoch: 35 | Batch: 001 / 011 | Total loss: 1.383 | Reg loss: 0.026 | Tree loss: 1.383 | Accuracy: 0.546000 | 0.521 sec/iter\n",
      "Epoch: 35 | Batch: 002 / 011 | Total loss: 1.344 | Reg loss: 0.026 | Tree loss: 1.344 | Accuracy: 0.555500 | 0.521 sec/iter\n",
      "Epoch: 35 | Batch: 003 / 011 | Total loss: 1.330 | Reg loss: 0.026 | Tree loss: 1.330 | Accuracy: 0.537500 | 0.52 sec/iter\n",
      "Epoch: 35 | Batch: 004 / 011 | Total loss: 1.272 | Reg loss: 0.026 | Tree loss: 1.272 | Accuracy: 0.527500 | 0.519 sec/iter\n",
      "Epoch: 35 | Batch: 005 / 011 | Total loss: 1.241 | Reg loss: 0.026 | Tree loss: 1.241 | Accuracy: 0.538500 | 0.518 sec/iter\n",
      "Epoch: 35 | Batch: 006 / 011 | Total loss: 1.223 | Reg loss: 0.026 | Tree loss: 1.223 | Accuracy: 0.559000 | 0.518 sec/iter\n",
      "Epoch: 35 | Batch: 007 / 011 | Total loss: 1.237 | Reg loss: 0.026 | Tree loss: 1.237 | Accuracy: 0.543500 | 0.517 sec/iter\n",
      "Epoch: 35 | Batch: 008 / 011 | Total loss: 1.204 | Reg loss: 0.026 | Tree loss: 1.204 | Accuracy: 0.539000 | 0.516 sec/iter\n",
      "Epoch: 35 | Batch: 009 / 011 | Total loss: 1.222 | Reg loss: 0.026 | Tree loss: 1.222 | Accuracy: 0.526500 | 0.515 sec/iter\n",
      "Epoch: 35 | Batch: 010 / 011 | Total loss: 1.175 | Reg loss: 0.026 | Tree loss: 1.175 | Accuracy: 0.556314 | 0.515 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 6: 0.9723756906077349\n",
      "Epoch: 36 | Batch: 000 / 011 | Total loss: 1.379 | Reg loss: 0.026 | Tree loss: 1.379 | Accuracy: 0.565500 | 0.524 sec/iter\n",
      "Epoch: 36 | Batch: 001 / 011 | Total loss: 1.364 | Reg loss: 0.026 | Tree loss: 1.364 | Accuracy: 0.564500 | 0.524 sec/iter\n",
      "Epoch: 36 | Batch: 002 / 011 | Total loss: 1.334 | Reg loss: 0.026 | Tree loss: 1.334 | Accuracy: 0.546000 | 0.523 sec/iter\n",
      "Epoch: 36 | Batch: 003 / 011 | Total loss: 1.302 | Reg loss: 0.026 | Tree loss: 1.302 | Accuracy: 0.536000 | 0.522 sec/iter\n",
      "Epoch: 36 | Batch: 004 / 011 | Total loss: 1.265 | Reg loss: 0.026 | Tree loss: 1.265 | Accuracy: 0.524000 | 0.522 sec/iter\n",
      "Epoch: 36 | Batch: 005 / 011 | Total loss: 1.237 | Reg loss: 0.026 | Tree loss: 1.237 | Accuracy: 0.536500 | 0.521 sec/iter\n",
      "Epoch: 36 | Batch: 006 / 011 | Total loss: 1.219 | Reg loss: 0.026 | Tree loss: 1.219 | Accuracy: 0.530500 | 0.52 sec/iter\n",
      "Epoch: 36 | Batch: 007 / 011 | Total loss: 1.234 | Reg loss: 0.026 | Tree loss: 1.234 | Accuracy: 0.516000 | 0.52 sec/iter\n",
      "Epoch: 36 | Batch: 008 / 011 | Total loss: 1.197 | Reg loss: 0.026 | Tree loss: 1.197 | Accuracy: 0.556500 | 0.519 sec/iter\n",
      "Epoch: 36 | Batch: 009 / 011 | Total loss: 1.211 | Reg loss: 0.026 | Tree loss: 1.211 | Accuracy: 0.536500 | 0.518 sec/iter\n",
      "Epoch: 36 | Batch: 010 / 011 | Total loss: 1.160 | Reg loss: 0.026 | Tree loss: 1.160 | Accuracy: 0.552901 | 0.517 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 37 | Batch: 000 / 011 | Total loss: 1.377 | Reg loss: 0.026 | Tree loss: 1.377 | Accuracy: 0.561000 | 0.519 sec/iter\n",
      "Epoch: 37 | Batch: 001 / 011 | Total loss: 1.359 | Reg loss: 0.026 | Tree loss: 1.359 | Accuracy: 0.561000 | 0.518 sec/iter\n",
      "Epoch: 37 | Batch: 002 / 011 | Total loss: 1.307 | Reg loss: 0.026 | Tree loss: 1.307 | Accuracy: 0.553500 | 0.517 sec/iter\n",
      "Epoch: 37 | Batch: 003 / 011 | Total loss: 1.285 | Reg loss: 0.026 | Tree loss: 1.285 | Accuracy: 0.548500 | 0.517 sec/iter\n",
      "Epoch: 37 | Batch: 004 / 011 | Total loss: 1.245 | Reg loss: 0.026 | Tree loss: 1.245 | Accuracy: 0.547500 | 0.516 sec/iter\n",
      "Epoch: 37 | Batch: 005 / 011 | Total loss: 1.241 | Reg loss: 0.026 | Tree loss: 1.241 | Accuracy: 0.522000 | 0.515 sec/iter\n",
      "Epoch: 37 | Batch: 006 / 011 | Total loss: 1.215 | Reg loss: 0.026 | Tree loss: 1.215 | Accuracy: 0.531000 | 0.515 sec/iter\n",
      "Epoch: 37 | Batch: 007 / 011 | Total loss: 1.219 | Reg loss: 0.026 | Tree loss: 1.219 | Accuracy: 0.537500 | 0.514 sec/iter\n",
      "Epoch: 37 | Batch: 008 / 011 | Total loss: 1.162 | Reg loss: 0.026 | Tree loss: 1.162 | Accuracy: 0.555500 | 0.513 sec/iter\n",
      "Epoch: 37 | Batch: 009 / 011 | Total loss: 1.209 | Reg loss: 0.026 | Tree loss: 1.209 | Accuracy: 0.522500 | 0.513 sec/iter\n",
      "Epoch: 37 | Batch: 010 / 011 | Total loss: 1.206 | Reg loss: 0.026 | Tree loss: 1.206 | Accuracy: 0.508532 | 0.512 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 38 | Batch: 000 / 011 | Total loss: 1.376 | Reg loss: 0.026 | Tree loss: 1.376 | Accuracy: 0.536000 | 0.518 sec/iter\n",
      "Epoch: 38 | Batch: 001 / 011 | Total loss: 1.351 | Reg loss: 0.026 | Tree loss: 1.351 | Accuracy: 0.533000 | 0.517 sec/iter\n",
      "Epoch: 38 | Batch: 002 / 011 | Total loss: 1.292 | Reg loss: 0.026 | Tree loss: 1.292 | Accuracy: 0.566000 | 0.516 sec/iter\n",
      "Epoch: 38 | Batch: 003 / 011 | Total loss: 1.268 | Reg loss: 0.026 | Tree loss: 1.268 | Accuracy: 0.534500 | 0.516 sec/iter\n",
      "Epoch: 38 | Batch: 004 / 011 | Total loss: 1.229 | Reg loss: 0.026 | Tree loss: 1.229 | Accuracy: 0.552000 | 0.515 sec/iter\n",
      "Epoch: 38 | Batch: 005 / 011 | Total loss: 1.214 | Reg loss: 0.026 | Tree loss: 1.214 | Accuracy: 0.549000 | 0.514 sec/iter\n",
      "Epoch: 38 | Batch: 006 / 011 | Total loss: 1.207 | Reg loss: 0.026 | Tree loss: 1.207 | Accuracy: 0.538500 | 0.514 sec/iter\n",
      "Epoch: 38 | Batch: 007 / 011 | Total loss: 1.179 | Reg loss: 0.026 | Tree loss: 1.179 | Accuracy: 0.557000 | 0.513 sec/iter\n",
      "Epoch: 38 | Batch: 008 / 011 | Total loss: 1.188 | Reg loss: 0.026 | Tree loss: 1.188 | Accuracy: 0.545500 | 0.512 sec/iter\n",
      "Epoch: 38 | Batch: 009 / 011 | Total loss: 1.195 | Reg loss: 0.026 | Tree loss: 1.195 | Accuracy: 0.523500 | 0.512 sec/iter\n",
      "Epoch: 38 | Batch: 010 / 011 | Total loss: 1.256 | Reg loss: 0.026 | Tree loss: 1.256 | Accuracy: 0.494881 | 0.511 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 39 | Batch: 000 / 011 | Total loss: 1.349 | Reg loss: 0.026 | Tree loss: 1.349 | Accuracy: 0.549500 | 0.52 sec/iter\n",
      "Epoch: 39 | Batch: 001 / 011 | Total loss: 1.311 | Reg loss: 0.026 | Tree loss: 1.311 | Accuracy: 0.559000 | 0.52 sec/iter\n",
      "Epoch: 39 | Batch: 002 / 011 | Total loss: 1.309 | Reg loss: 0.026 | Tree loss: 1.309 | Accuracy: 0.542000 | 0.519 sec/iter\n",
      "Epoch: 39 | Batch: 003 / 011 | Total loss: 1.266 | Reg loss: 0.026 | Tree loss: 1.266 | Accuracy: 0.553500 | 0.518 sec/iter\n",
      "Epoch: 39 | Batch: 004 / 011 | Total loss: 1.249 | Reg loss: 0.026 | Tree loss: 1.249 | Accuracy: 0.532000 | 0.518 sec/iter\n",
      "Epoch: 39 | Batch: 005 / 011 | Total loss: 1.203 | Reg loss: 0.026 | Tree loss: 1.203 | Accuracy: 0.544500 | 0.517 sec/iter\n",
      "Epoch: 39 | Batch: 006 / 011 | Total loss: 1.205 | Reg loss: 0.026 | Tree loss: 1.205 | Accuracy: 0.534500 | 0.516 sec/iter\n",
      "Epoch: 39 | Batch: 007 / 011 | Total loss: 1.174 | Reg loss: 0.026 | Tree loss: 1.174 | Accuracy: 0.569500 | 0.516 sec/iter\n",
      "Epoch: 39 | Batch: 008 / 011 | Total loss: 1.199 | Reg loss: 0.026 | Tree loss: 1.199 | Accuracy: 0.540500 | 0.515 sec/iter\n",
      "Epoch: 39 | Batch: 009 / 011 | Total loss: 1.162 | Reg loss: 0.026 | Tree loss: 1.162 | Accuracy: 0.537500 | 0.515 sec/iter\n",
      "Epoch: 39 | Batch: 010 / 011 | Total loss: 1.184 | Reg loss: 0.026 | Tree loss: 1.184 | Accuracy: 0.525597 | 0.514 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 40 | Batch: 000 / 011 | Total loss: 1.323 | Reg loss: 0.026 | Tree loss: 1.323 | Accuracy: 0.557500 | 0.516 sec/iter\n",
      "Epoch: 40 | Batch: 001 / 011 | Total loss: 1.294 | Reg loss: 0.026 | Tree loss: 1.294 | Accuracy: 0.544500 | 0.515 sec/iter\n",
      "Epoch: 40 | Batch: 002 / 011 | Total loss: 1.279 | Reg loss: 0.026 | Tree loss: 1.279 | Accuracy: 0.545500 | 0.515 sec/iter\n",
      "Epoch: 40 | Batch: 003 / 011 | Total loss: 1.245 | Reg loss: 0.026 | Tree loss: 1.245 | Accuracy: 0.535500 | 0.514 sec/iter\n",
      "Epoch: 40 | Batch: 004 / 011 | Total loss: 1.242 | Reg loss: 0.026 | Tree loss: 1.242 | Accuracy: 0.539500 | 0.513 sec/iter\n",
      "Epoch: 40 | Batch: 005 / 011 | Total loss: 1.221 | Reg loss: 0.026 | Tree loss: 1.221 | Accuracy: 0.512500 | 0.513 sec/iter\n",
      "Epoch: 40 | Batch: 006 / 011 | Total loss: 1.180 | Reg loss: 0.026 | Tree loss: 1.180 | Accuracy: 0.531500 | 0.512 sec/iter\n",
      "Epoch: 40 | Batch: 007 / 011 | Total loss: 1.186 | Reg loss: 0.026 | Tree loss: 1.186 | Accuracy: 0.534500 | 0.512 sec/iter\n",
      "Epoch: 40 | Batch: 008 / 011 | Total loss: 1.166 | Reg loss: 0.026 | Tree loss: 1.166 | Accuracy: 0.560500 | 0.511 sec/iter\n",
      "Epoch: 40 | Batch: 009 / 011 | Total loss: 1.180 | Reg loss: 0.026 | Tree loss: 1.180 | Accuracy: 0.535000 | 0.51 sec/iter\n",
      "Epoch: 40 | Batch: 010 / 011 | Total loss: 1.155 | Reg loss: 0.026 | Tree loss: 1.155 | Accuracy: 0.604096 | 0.51 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 41 | Batch: 000 / 011 | Total loss: 1.329 | Reg loss: 0.026 | Tree loss: 1.329 | Accuracy: 0.539500 | 0.518 sec/iter\n",
      "Epoch: 41 | Batch: 001 / 011 | Total loss: 1.292 | Reg loss: 0.026 | Tree loss: 1.292 | Accuracy: 0.558000 | 0.517 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 41 | Batch: 002 / 011 | Total loss: 1.271 | Reg loss: 0.026 | Tree loss: 1.271 | Accuracy: 0.563500 | 0.517 sec/iter\n",
      "Epoch: 41 | Batch: 003 / 011 | Total loss: 1.242 | Reg loss: 0.026 | Tree loss: 1.242 | Accuracy: 0.539500 | 0.516 sec/iter\n",
      "Epoch: 41 | Batch: 004 / 011 | Total loss: 1.206 | Reg loss: 0.026 | Tree loss: 1.206 | Accuracy: 0.543500 | 0.515 sec/iter\n",
      "Epoch: 41 | Batch: 005 / 011 | Total loss: 1.190 | Reg loss: 0.026 | Tree loss: 1.190 | Accuracy: 0.526000 | 0.515 sec/iter\n",
      "Epoch: 41 | Batch: 006 / 011 | Total loss: 1.186 | Reg loss: 0.026 | Tree loss: 1.186 | Accuracy: 0.539500 | 0.514 sec/iter\n",
      "Epoch: 41 | Batch: 007 / 011 | Total loss: 1.178 | Reg loss: 0.026 | Tree loss: 1.178 | Accuracy: 0.532000 | 0.513 sec/iter\n",
      "Epoch: 41 | Batch: 008 / 011 | Total loss: 1.178 | Reg loss: 0.026 | Tree loss: 1.178 | Accuracy: 0.536500 | 0.513 sec/iter\n",
      "Epoch: 41 | Batch: 009 / 011 | Total loss: 1.172 | Reg loss: 0.026 | Tree loss: 1.172 | Accuracy: 0.542000 | 0.512 sec/iter\n",
      "Epoch: 41 | Batch: 010 / 011 | Total loss: 1.117 | Reg loss: 0.026 | Tree loss: 1.117 | Accuracy: 0.563140 | 0.512 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 42 | Batch: 000 / 011 | Total loss: 1.327 | Reg loss: 0.026 | Tree loss: 1.327 | Accuracy: 0.542500 | 0.522 sec/iter\n",
      "Epoch: 42 | Batch: 001 / 011 | Total loss: 1.304 | Reg loss: 0.026 | Tree loss: 1.304 | Accuracy: 0.526500 | 0.521 sec/iter\n",
      "Epoch: 42 | Batch: 002 / 011 | Total loss: 1.251 | Reg loss: 0.026 | Tree loss: 1.251 | Accuracy: 0.557500 | 0.52 sec/iter\n",
      "Epoch: 42 | Batch: 003 / 011 | Total loss: 1.223 | Reg loss: 0.026 | Tree loss: 1.223 | Accuracy: 0.541000 | 0.52 sec/iter\n",
      "Epoch: 42 | Batch: 004 / 011 | Total loss: 1.215 | Reg loss: 0.026 | Tree loss: 1.215 | Accuracy: 0.554500 | 0.519 sec/iter\n",
      "Epoch: 42 | Batch: 005 / 011 | Total loss: 1.178 | Reg loss: 0.026 | Tree loss: 1.178 | Accuracy: 0.535500 | 0.519 sec/iter\n",
      "Epoch: 42 | Batch: 006 / 011 | Total loss: 1.173 | Reg loss: 0.026 | Tree loss: 1.173 | Accuracy: 0.535000 | 0.518 sec/iter\n",
      "Epoch: 42 | Batch: 007 / 011 | Total loss: 1.135 | Reg loss: 0.026 | Tree loss: 1.135 | Accuracy: 0.549000 | 0.518 sec/iter\n",
      "Epoch: 42 | Batch: 008 / 011 | Total loss: 1.165 | Reg loss: 0.026 | Tree loss: 1.165 | Accuracy: 0.547500 | 0.517 sec/iter\n",
      "Epoch: 42 | Batch: 009 / 011 | Total loss: 1.155 | Reg loss: 0.026 | Tree loss: 1.155 | Accuracy: 0.541000 | 0.516 sec/iter\n",
      "Epoch: 42 | Batch: 010 / 011 | Total loss: 1.159 | Reg loss: 0.026 | Tree loss: 1.159 | Accuracy: 0.518771 | 0.516 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 43 | Batch: 000 / 011 | Total loss: 1.297 | Reg loss: 0.026 | Tree loss: 1.297 | Accuracy: 0.567500 | 0.524 sec/iter\n",
      "Epoch: 43 | Batch: 001 / 011 | Total loss: 1.275 | Reg loss: 0.026 | Tree loss: 1.275 | Accuracy: 0.546000 | 0.524 sec/iter\n",
      "Epoch: 43 | Batch: 002 / 011 | Total loss: 1.261 | Reg loss: 0.026 | Tree loss: 1.261 | Accuracy: 0.528000 | 0.523 sec/iter\n",
      "Epoch: 43 | Batch: 003 / 011 | Total loss: 1.233 | Reg loss: 0.026 | Tree loss: 1.233 | Accuracy: 0.520000 | 0.523 sec/iter\n",
      "Epoch: 43 | Batch: 004 / 011 | Total loss: 1.180 | Reg loss: 0.026 | Tree loss: 1.180 | Accuracy: 0.538000 | 0.522 sec/iter\n",
      "Epoch: 43 | Batch: 005 / 011 | Total loss: 1.174 | Reg loss: 0.026 | Tree loss: 1.174 | Accuracy: 0.550500 | 0.522 sec/iter\n",
      "Epoch: 43 | Batch: 006 / 011 | Total loss: 1.172 | Reg loss: 0.026 | Tree loss: 1.172 | Accuracy: 0.534500 | 0.521 sec/iter\n",
      "Epoch: 43 | Batch: 007 / 011 | Total loss: 1.152 | Reg loss: 0.026 | Tree loss: 1.152 | Accuracy: 0.556000 | 0.521 sec/iter\n",
      "Epoch: 43 | Batch: 008 / 011 | Total loss: 1.155 | Reg loss: 0.026 | Tree loss: 1.155 | Accuracy: 0.534000 | 0.52 sec/iter\n",
      "Epoch: 43 | Batch: 009 / 011 | Total loss: 1.122 | Reg loss: 0.026 | Tree loss: 1.122 | Accuracy: 0.563500 | 0.52 sec/iter\n",
      "Epoch: 43 | Batch: 010 / 011 | Total loss: 1.192 | Reg loss: 0.026 | Tree loss: 1.192 | Accuracy: 0.515358 | 0.519 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 44 | Batch: 000 / 011 | Total loss: 1.299 | Reg loss: 0.026 | Tree loss: 1.299 | Accuracy: 0.550000 | 0.531 sec/iter\n",
      "Epoch: 44 | Batch: 001 / 011 | Total loss: 1.268 | Reg loss: 0.026 | Tree loss: 1.268 | Accuracy: 0.535000 | 0.53 sec/iter\n",
      "Epoch: 44 | Batch: 002 / 011 | Total loss: 1.232 | Reg loss: 0.026 | Tree loss: 1.232 | Accuracy: 0.550500 | 0.53 sec/iter\n",
      "Epoch: 44 | Batch: 003 / 011 | Total loss: 1.191 | Reg loss: 0.026 | Tree loss: 1.191 | Accuracy: 0.557500 | 0.529 sec/iter\n",
      "Epoch: 44 | Batch: 004 / 011 | Total loss: 1.194 | Reg loss: 0.026 | Tree loss: 1.194 | Accuracy: 0.553000 | 0.529 sec/iter\n",
      "Epoch: 44 | Batch: 005 / 011 | Total loss: 1.160 | Reg loss: 0.026 | Tree loss: 1.160 | Accuracy: 0.539000 | 0.528 sec/iter\n",
      "Epoch: 44 | Batch: 006 / 011 | Total loss: 1.166 | Reg loss: 0.026 | Tree loss: 1.166 | Accuracy: 0.530500 | 0.528 sec/iter\n",
      "Epoch: 44 | Batch: 007 / 011 | Total loss: 1.168 | Reg loss: 0.026 | Tree loss: 1.168 | Accuracy: 0.512000 | 0.527 sec/iter\n",
      "Epoch: 44 | Batch: 008 / 011 | Total loss: 1.132 | Reg loss: 0.026 | Tree loss: 1.132 | Accuracy: 0.535000 | 0.527 sec/iter\n",
      "Epoch: 44 | Batch: 009 / 011 | Total loss: 1.146 | Reg loss: 0.026 | Tree loss: 1.146 | Accuracy: 0.539000 | 0.526 sec/iter\n",
      "Epoch: 44 | Batch: 010 / 011 | Total loss: 1.143 | Reg loss: 0.026 | Tree loss: 1.143 | Accuracy: 0.525597 | 0.526 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 45 | Batch: 000 / 011 | Total loss: 1.273 | Reg loss: 0.026 | Tree loss: 1.273 | Accuracy: 0.555000 | 0.535 sec/iter\n",
      "Epoch: 45 | Batch: 001 / 011 | Total loss: 1.259 | Reg loss: 0.026 | Tree loss: 1.259 | Accuracy: 0.531000 | 0.535 sec/iter\n",
      "Epoch: 45 | Batch: 002 / 011 | Total loss: 1.228 | Reg loss: 0.026 | Tree loss: 1.228 | Accuracy: 0.537000 | 0.534 sec/iter\n",
      "Epoch: 45 | Batch: 003 / 011 | Total loss: 1.208 | Reg loss: 0.026 | Tree loss: 1.208 | Accuracy: 0.541000 | 0.534 sec/iter\n",
      "Epoch: 45 | Batch: 004 / 011 | Total loss: 1.179 | Reg loss: 0.026 | Tree loss: 1.179 | Accuracy: 0.553000 | 0.533 sec/iter\n",
      "Epoch: 45 | Batch: 005 / 011 | Total loss: 1.141 | Reg loss: 0.026 | Tree loss: 1.141 | Accuracy: 0.569500 | 0.533 sec/iter\n",
      "Epoch: 45 | Batch: 006 / 011 | Total loss: 1.146 | Reg loss: 0.026 | Tree loss: 1.146 | Accuracy: 0.540500 | 0.532 sec/iter\n",
      "Epoch: 45 | Batch: 007 / 011 | Total loss: 1.128 | Reg loss: 0.026 | Tree loss: 1.128 | Accuracy: 0.566000 | 0.531 sec/iter\n",
      "Epoch: 45 | Batch: 008 / 011 | Total loss: 1.171 | Reg loss: 0.026 | Tree loss: 1.171 | Accuracy: 0.507000 | 0.531 sec/iter\n",
      "Epoch: 45 | Batch: 009 / 011 | Total loss: 1.146 | Reg loss: 0.026 | Tree loss: 1.146 | Accuracy: 0.528500 | 0.53 sec/iter\n",
      "Epoch: 45 | Batch: 010 / 011 | Total loss: 1.089 | Reg loss: 0.026 | Tree loss: 1.089 | Accuracy: 0.587031 | 0.53 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 46 | Batch: 000 / 011 | Total loss: 1.278 | Reg loss: 0.026 | Tree loss: 1.278 | Accuracy: 0.559000 | 0.538 sec/iter\n",
      "Epoch: 46 | Batch: 001 / 011 | Total loss: 1.237 | Reg loss: 0.026 | Tree loss: 1.237 | Accuracy: 0.555000 | 0.538 sec/iter\n",
      "Epoch: 46 | Batch: 002 / 011 | Total loss: 1.236 | Reg loss: 0.026 | Tree loss: 1.236 | Accuracy: 0.539500 | 0.537 sec/iter\n",
      "Epoch: 46 | Batch: 003 / 011 | Total loss: 1.186 | Reg loss: 0.026 | Tree loss: 1.186 | Accuracy: 0.545000 | 0.537 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 46 | Batch: 004 / 011 | Total loss: 1.161 | Reg loss: 0.026 | Tree loss: 1.161 | Accuracy: 0.542500 | 0.536 sec/iter\n",
      "Epoch: 46 | Batch: 005 / 011 | Total loss: 1.174 | Reg loss: 0.026 | Tree loss: 1.174 | Accuracy: 0.510500 | 0.535 sec/iter\n",
      "Epoch: 46 | Batch: 006 / 011 | Total loss: 1.144 | Reg loss: 0.026 | Tree loss: 1.144 | Accuracy: 0.534500 | 0.535 sec/iter\n",
      "Epoch: 46 | Batch: 007 / 011 | Total loss: 1.132 | Reg loss: 0.026 | Tree loss: 1.132 | Accuracy: 0.547000 | 0.534 sec/iter\n",
      "Epoch: 46 | Batch: 008 / 011 | Total loss: 1.137 | Reg loss: 0.026 | Tree loss: 1.137 | Accuracy: 0.532500 | 0.534 sec/iter\n",
      "Epoch: 46 | Batch: 009 / 011 | Total loss: 1.119 | Reg loss: 0.026 | Tree loss: 1.119 | Accuracy: 0.548500 | 0.533 sec/iter\n",
      "Epoch: 46 | Batch: 010 / 011 | Total loss: 1.173 | Reg loss: 0.026 | Tree loss: 1.173 | Accuracy: 0.515358 | 0.533 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 47 | Batch: 000 / 011 | Total loss: 1.260 | Reg loss: 0.026 | Tree loss: 1.260 | Accuracy: 0.543500 | 0.54 sec/iter\n",
      "Epoch: 47 | Batch: 001 / 011 | Total loss: 1.235 | Reg loss: 0.026 | Tree loss: 1.235 | Accuracy: 0.564000 | 0.54 sec/iter\n",
      "Epoch: 47 | Batch: 002 / 011 | Total loss: 1.205 | Reg loss: 0.026 | Tree loss: 1.205 | Accuracy: 0.551500 | 0.539 sec/iter\n",
      "Epoch: 47 | Batch: 003 / 011 | Total loss: 1.192 | Reg loss: 0.026 | Tree loss: 1.192 | Accuracy: 0.544500 | 0.539 sec/iter\n",
      "Epoch: 47 | Batch: 004 / 011 | Total loss: 1.157 | Reg loss: 0.026 | Tree loss: 1.157 | Accuracy: 0.536000 | 0.538 sec/iter\n",
      "Epoch: 47 | Batch: 005 / 011 | Total loss: 1.133 | Reg loss: 0.026 | Tree loss: 1.133 | Accuracy: 0.546000 | 0.538 sec/iter\n",
      "Epoch: 47 | Batch: 006 / 011 | Total loss: 1.140 | Reg loss: 0.026 | Tree loss: 1.140 | Accuracy: 0.532000 | 0.537 sec/iter\n",
      "Epoch: 47 | Batch: 007 / 011 | Total loss: 1.119 | Reg loss: 0.026 | Tree loss: 1.119 | Accuracy: 0.543500 | 0.537 sec/iter\n",
      "Epoch: 47 | Batch: 008 / 011 | Total loss: 1.152 | Reg loss: 0.026 | Tree loss: 1.152 | Accuracy: 0.531500 | 0.536 sec/iter\n",
      "Epoch: 47 | Batch: 009 / 011 | Total loss: 1.130 | Reg loss: 0.026 | Tree loss: 1.130 | Accuracy: 0.532500 | 0.536 sec/iter\n",
      "Epoch: 47 | Batch: 010 / 011 | Total loss: 1.100 | Reg loss: 0.026 | Tree loss: 1.100 | Accuracy: 0.569966 | 0.535 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 48 | Batch: 000 / 011 | Total loss: 1.261 | Reg loss: 0.026 | Tree loss: 1.261 | Accuracy: 0.534500 | 0.546 sec/iter\n",
      "Epoch: 48 | Batch: 001 / 011 | Total loss: 1.231 | Reg loss: 0.026 | Tree loss: 1.231 | Accuracy: 0.552000 | 0.545 sec/iter\n",
      "Epoch: 48 | Batch: 002 / 011 | Total loss: 1.200 | Reg loss: 0.026 | Tree loss: 1.200 | Accuracy: 0.553500 | 0.544 sec/iter\n",
      "Epoch: 48 | Batch: 003 / 011 | Total loss: 1.178 | Reg loss: 0.026 | Tree loss: 1.178 | Accuracy: 0.534000 | 0.544 sec/iter\n",
      "Epoch: 48 | Batch: 004 / 011 | Total loss: 1.160 | Reg loss: 0.026 | Tree loss: 1.160 | Accuracy: 0.539000 | 0.544 sec/iter\n",
      "Epoch: 48 | Batch: 005 / 011 | Total loss: 1.134 | Reg loss: 0.026 | Tree loss: 1.134 | Accuracy: 0.554000 | 0.543 sec/iter\n",
      "Epoch: 48 | Batch: 006 / 011 | Total loss: 1.134 | Reg loss: 0.026 | Tree loss: 1.134 | Accuracy: 0.522500 | 0.543 sec/iter\n",
      "Epoch: 48 | Batch: 007 / 011 | Total loss: 1.102 | Reg loss: 0.026 | Tree loss: 1.102 | Accuracy: 0.546000 | 0.542 sec/iter\n",
      "Epoch: 48 | Batch: 008 / 011 | Total loss: 1.129 | Reg loss: 0.026 | Tree loss: 1.129 | Accuracy: 0.531000 | 0.542 sec/iter\n",
      "Epoch: 48 | Batch: 009 / 011 | Total loss: 1.102 | Reg loss: 0.026 | Tree loss: 1.102 | Accuracy: 0.553000 | 0.541 sec/iter\n",
      "Epoch: 48 | Batch: 010 / 011 | Total loss: 1.139 | Reg loss: 0.026 | Tree loss: 1.139 | Accuracy: 0.535836 | 0.541 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 49 | Batch: 000 / 011 | Total loss: 1.247 | Reg loss: 0.026 | Tree loss: 1.247 | Accuracy: 0.531500 | 0.545 sec/iter\n",
      "Epoch: 49 | Batch: 001 / 011 | Total loss: 1.220 | Reg loss: 0.026 | Tree loss: 1.220 | Accuracy: 0.558500 | 0.545 sec/iter\n",
      "Epoch: 49 | Batch: 002 / 011 | Total loss: 1.205 | Reg loss: 0.026 | Tree loss: 1.205 | Accuracy: 0.558500 | 0.544 sec/iter\n",
      "Epoch: 49 | Batch: 003 / 011 | Total loss: 1.176 | Reg loss: 0.026 | Tree loss: 1.176 | Accuracy: 0.551000 | 0.543 sec/iter\n",
      "Epoch: 49 | Batch: 004 / 011 | Total loss: 1.138 | Reg loss: 0.026 | Tree loss: 1.138 | Accuracy: 0.562000 | 0.543 sec/iter\n",
      "Epoch: 49 | Batch: 005 / 011 | Total loss: 1.115 | Reg loss: 0.026 | Tree loss: 1.115 | Accuracy: 0.561000 | 0.542 sec/iter\n",
      "Epoch: 49 | Batch: 006 / 011 | Total loss: 1.116 | Reg loss: 0.026 | Tree loss: 1.116 | Accuracy: 0.516500 | 0.542 sec/iter\n",
      "Epoch: 49 | Batch: 007 / 011 | Total loss: 1.126 | Reg loss: 0.026 | Tree loss: 1.126 | Accuracy: 0.531000 | 0.541 sec/iter\n",
      "Epoch: 49 | Batch: 008 / 011 | Total loss: 1.130 | Reg loss: 0.026 | Tree loss: 1.130 | Accuracy: 0.523500 | 0.54 sec/iter\n",
      "Epoch: 49 | Batch: 009 / 011 | Total loss: 1.117 | Reg loss: 0.026 | Tree loss: 1.117 | Accuracy: 0.547500 | 0.54 sec/iter\n",
      "Epoch: 49 | Batch: 010 / 011 | Total loss: 1.082 | Reg loss: 0.026 | Tree loss: 1.082 | Accuracy: 0.563140 | 0.539 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 50 | Batch: 000 / 011 | Total loss: 1.229 | Reg loss: 0.026 | Tree loss: 1.229 | Accuracy: 0.571500 | 0.544 sec/iter\n",
      "Epoch: 50 | Batch: 001 / 011 | Total loss: 1.194 | Reg loss: 0.026 | Tree loss: 1.194 | Accuracy: 0.583500 | 0.543 sec/iter\n",
      "Epoch: 50 | Batch: 002 / 011 | Total loss: 1.185 | Reg loss: 0.026 | Tree loss: 1.185 | Accuracy: 0.572500 | 0.543 sec/iter\n",
      "Epoch: 50 | Batch: 003 / 011 | Total loss: 1.174 | Reg loss: 0.026 | Tree loss: 1.174 | Accuracy: 0.536000 | 0.542 sec/iter\n",
      "Epoch: 50 | Batch: 004 / 011 | Total loss: 1.144 | Reg loss: 0.026 | Tree loss: 1.144 | Accuracy: 0.542000 | 0.542 sec/iter\n",
      "Epoch: 50 | Batch: 005 / 011 | Total loss: 1.118 | Reg loss: 0.026 | Tree loss: 1.118 | Accuracy: 0.551500 | 0.541 sec/iter\n",
      "Epoch: 50 | Batch: 006 / 011 | Total loss: 1.111 | Reg loss: 0.026 | Tree loss: 1.111 | Accuracy: 0.539000 | 0.541 sec/iter\n",
      "Epoch: 50 | Batch: 007 / 011 | Total loss: 1.107 | Reg loss: 0.026 | Tree loss: 1.107 | Accuracy: 0.538500 | 0.54 sec/iter\n",
      "Epoch: 50 | Batch: 008 / 011 | Total loss: 1.121 | Reg loss: 0.026 | Tree loss: 1.121 | Accuracy: 0.529000 | 0.539 sec/iter\n",
      "Epoch: 50 | Batch: 009 / 011 | Total loss: 1.126 | Reg loss: 0.026 | Tree loss: 1.126 | Accuracy: 0.517500 | 0.539 sec/iter\n",
      "Epoch: 50 | Batch: 010 / 011 | Total loss: 1.074 | Reg loss: 0.026 | Tree loss: 1.074 | Accuracy: 0.573379 | 0.538 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 51 | Batch: 000 / 011 | Total loss: 1.211 | Reg loss: 0.026 | Tree loss: 1.211 | Accuracy: 0.588500 | 0.54 sec/iter\n",
      "Epoch: 51 | Batch: 001 / 011 | Total loss: 1.216 | Reg loss: 0.026 | Tree loss: 1.216 | Accuracy: 0.560000 | 0.54 sec/iter\n",
      "Epoch: 51 | Batch: 002 / 011 | Total loss: 1.174 | Reg loss: 0.026 | Tree loss: 1.174 | Accuracy: 0.573000 | 0.539 sec/iter\n",
      "Epoch: 51 | Batch: 003 / 011 | Total loss: 1.169 | Reg loss: 0.026 | Tree loss: 1.169 | Accuracy: 0.551000 | 0.539 sec/iter\n",
      "Epoch: 51 | Batch: 004 / 011 | Total loss: 1.115 | Reg loss: 0.026 | Tree loss: 1.115 | Accuracy: 0.584500 | 0.538 sec/iter\n",
      "Epoch: 51 | Batch: 005 / 011 | Total loss: 1.123 | Reg loss: 0.026 | Tree loss: 1.123 | Accuracy: 0.539000 | 0.538 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 51 | Batch: 006 / 011 | Total loss: 1.113 | Reg loss: 0.026 | Tree loss: 1.113 | Accuracy: 0.534000 | 0.537 sec/iter\n",
      "Epoch: 51 | Batch: 007 / 011 | Total loss: 1.121 | Reg loss: 0.026 | Tree loss: 1.121 | Accuracy: 0.525500 | 0.537 sec/iter\n",
      "Epoch: 51 | Batch: 008 / 011 | Total loss: 1.110 | Reg loss: 0.026 | Tree loss: 1.110 | Accuracy: 0.536500 | 0.536 sec/iter\n",
      "Epoch: 51 | Batch: 009 / 011 | Total loss: 1.082 | Reg loss: 0.026 | Tree loss: 1.082 | Accuracy: 0.543000 | 0.535 sec/iter\n",
      "Epoch: 51 | Batch: 010 / 011 | Total loss: 1.084 | Reg loss: 0.026 | Tree loss: 1.084 | Accuracy: 0.573379 | 0.535 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 52 | Batch: 000 / 011 | Total loss: 1.212 | Reg loss: 0.026 | Tree loss: 1.212 | Accuracy: 0.586500 | 0.54 sec/iter\n",
      "Epoch: 52 | Batch: 001 / 011 | Total loss: 1.184 | Reg loss: 0.026 | Tree loss: 1.184 | Accuracy: 0.596500 | 0.54 sec/iter\n",
      "Epoch: 52 | Batch: 002 / 011 | Total loss: 1.177 | Reg loss: 0.026 | Tree loss: 1.177 | Accuracy: 0.586000 | 0.539 sec/iter\n",
      "Epoch: 52 | Batch: 003 / 011 | Total loss: 1.150 | Reg loss: 0.026 | Tree loss: 1.150 | Accuracy: 0.570000 | 0.539 sec/iter\n",
      "Epoch: 52 | Batch: 004 / 011 | Total loss: 1.124 | Reg loss: 0.026 | Tree loss: 1.124 | Accuracy: 0.556000 | 0.538 sec/iter\n",
      "Epoch: 52 | Batch: 005 / 011 | Total loss: 1.108 | Reg loss: 0.026 | Tree loss: 1.108 | Accuracy: 0.551000 | 0.537 sec/iter\n",
      "Epoch: 52 | Batch: 006 / 011 | Total loss: 1.109 | Reg loss: 0.026 | Tree loss: 1.109 | Accuracy: 0.543500 | 0.537 sec/iter\n",
      "Epoch: 52 | Batch: 007 / 011 | Total loss: 1.107 | Reg loss: 0.026 | Tree loss: 1.107 | Accuracy: 0.515500 | 0.536 sec/iter\n",
      "Epoch: 52 | Batch: 008 / 011 | Total loss: 1.102 | Reg loss: 0.026 | Tree loss: 1.102 | Accuracy: 0.545000 | 0.536 sec/iter\n",
      "Epoch: 52 | Batch: 009 / 011 | Total loss: 1.083 | Reg loss: 0.026 | Tree loss: 1.083 | Accuracy: 0.545500 | 0.535 sec/iter\n",
      "Epoch: 52 | Batch: 010 / 011 | Total loss: 1.108 | Reg loss: 0.026 | Tree loss: 1.108 | Accuracy: 0.501706 | 0.535 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 53 | Batch: 000 / 011 | Total loss: 1.219 | Reg loss: 0.026 | Tree loss: 1.219 | Accuracy: 0.599500 | 0.541 sec/iter\n",
      "Epoch: 53 | Batch: 001 / 011 | Total loss: 1.175 | Reg loss: 0.026 | Tree loss: 1.175 | Accuracy: 0.617000 | 0.541 sec/iter\n",
      "Epoch: 53 | Batch: 002 / 011 | Total loss: 1.162 | Reg loss: 0.026 | Tree loss: 1.162 | Accuracy: 0.598000 | 0.54 sec/iter\n",
      "Epoch: 53 | Batch: 003 / 011 | Total loss: 1.142 | Reg loss: 0.026 | Tree loss: 1.142 | Accuracy: 0.601000 | 0.54 sec/iter\n",
      "Epoch: 53 | Batch: 004 / 011 | Total loss: 1.118 | Reg loss: 0.026 | Tree loss: 1.118 | Accuracy: 0.568000 | 0.539 sec/iter\n",
      "Epoch: 53 | Batch: 005 / 011 | Total loss: 1.113 | Reg loss: 0.026 | Tree loss: 1.113 | Accuracy: 0.554500 | 0.539 sec/iter\n",
      "Epoch: 53 | Batch: 006 / 011 | Total loss: 1.112 | Reg loss: 0.026 | Tree loss: 1.112 | Accuracy: 0.554000 | 0.538 sec/iter\n",
      "Epoch: 53 | Batch: 007 / 011 | Total loss: 1.092 | Reg loss: 0.026 | Tree loss: 1.092 | Accuracy: 0.550000 | 0.538 sec/iter\n",
      "Epoch: 53 | Batch: 008 / 011 | Total loss: 1.075 | Reg loss: 0.026 | Tree loss: 1.075 | Accuracy: 0.549500 | 0.537 sec/iter\n",
      "Epoch: 53 | Batch: 009 / 011 | Total loss: 1.093 | Reg loss: 0.026 | Tree loss: 1.093 | Accuracy: 0.550500 | 0.537 sec/iter\n",
      "Epoch: 53 | Batch: 010 / 011 | Total loss: 1.048 | Reg loss: 0.026 | Tree loss: 1.048 | Accuracy: 0.549488 | 0.536 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 54 | Batch: 000 / 011 | Total loss: 1.221 | Reg loss: 0.026 | Tree loss: 1.221 | Accuracy: 0.595500 | 0.536 sec/iter\n",
      "Epoch: 54 | Batch: 001 / 011 | Total loss: 1.187 | Reg loss: 0.026 | Tree loss: 1.187 | Accuracy: 0.601000 | 0.536 sec/iter\n",
      "Epoch: 54 | Batch: 002 / 011 | Total loss: 1.148 | Reg loss: 0.026 | Tree loss: 1.148 | Accuracy: 0.609000 | 0.535 sec/iter\n",
      "Epoch: 54 | Batch: 003 / 011 | Total loss: 1.135 | Reg loss: 0.026 | Tree loss: 1.135 | Accuracy: 0.583000 | 0.535 sec/iter\n",
      "Epoch: 54 | Batch: 004 / 011 | Total loss: 1.104 | Reg loss: 0.026 | Tree loss: 1.104 | Accuracy: 0.578000 | 0.534 sec/iter\n",
      "Epoch: 54 | Batch: 005 / 011 | Total loss: 1.094 | Reg loss: 0.026 | Tree loss: 1.094 | Accuracy: 0.580500 | 0.534 sec/iter\n",
      "Epoch: 54 | Batch: 006 / 011 | Total loss: 1.082 | Reg loss: 0.026 | Tree loss: 1.082 | Accuracy: 0.561500 | 0.533 sec/iter\n",
      "Epoch: 54 | Batch: 007 / 011 | Total loss: 1.081 | Reg loss: 0.026 | Tree loss: 1.081 | Accuracy: 0.557500 | 0.533 sec/iter\n",
      "Epoch: 54 | Batch: 008 / 011 | Total loss: 1.083 | Reg loss: 0.026 | Tree loss: 1.083 | Accuracy: 0.565000 | 0.532 sec/iter\n",
      "Epoch: 54 | Batch: 009 / 011 | Total loss: 1.087 | Reg loss: 0.026 | Tree loss: 1.087 | Accuracy: 0.553500 | 0.532 sec/iter\n",
      "Epoch: 54 | Batch: 010 / 011 | Total loss: 1.171 | Reg loss: 0.026 | Tree loss: 1.171 | Accuracy: 0.484642 | 0.531 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 55 | Batch: 000 / 011 | Total loss: 1.203 | Reg loss: 0.026 | Tree loss: 1.203 | Accuracy: 0.611000 | 0.536 sec/iter\n",
      "Epoch: 55 | Batch: 001 / 011 | Total loss: 1.187 | Reg loss: 0.026 | Tree loss: 1.187 | Accuracy: 0.623500 | 0.535 sec/iter\n",
      "Epoch: 55 | Batch: 002 / 011 | Total loss: 1.141 | Reg loss: 0.026 | Tree loss: 1.141 | Accuracy: 0.615500 | 0.535 sec/iter\n",
      "Epoch: 55 | Batch: 003 / 011 | Total loss: 1.120 | Reg loss: 0.026 | Tree loss: 1.120 | Accuracy: 0.599000 | 0.534 sec/iter\n",
      "Epoch: 55 | Batch: 004 / 011 | Total loss: 1.114 | Reg loss: 0.026 | Tree loss: 1.114 | Accuracy: 0.604500 | 0.534 sec/iter\n",
      "Epoch: 55 | Batch: 005 / 011 | Total loss: 1.120 | Reg loss: 0.026 | Tree loss: 1.120 | Accuracy: 0.559000 | 0.533 sec/iter\n",
      "Epoch: 55 | Batch: 006 / 011 | Total loss: 1.069 | Reg loss: 0.026 | Tree loss: 1.069 | Accuracy: 0.578500 | 0.533 sec/iter\n",
      "Epoch: 55 | Batch: 007 / 011 | Total loss: 1.084 | Reg loss: 0.026 | Tree loss: 1.084 | Accuracy: 0.566500 | 0.532 sec/iter\n",
      "Epoch: 55 | Batch: 008 / 011 | Total loss: 1.075 | Reg loss: 0.026 | Tree loss: 1.075 | Accuracy: 0.579500 | 0.531 sec/iter\n",
      "Epoch: 55 | Batch: 009 / 011 | Total loss: 1.064 | Reg loss: 0.026 | Tree loss: 1.064 | Accuracy: 0.580500 | 0.531 sec/iter\n",
      "Epoch: 55 | Batch: 010 / 011 | Total loss: 1.058 | Reg loss: 0.026 | Tree loss: 1.058 | Accuracy: 0.566553 | 0.531 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 56 | Batch: 000 / 011 | Total loss: 1.190 | Reg loss: 0.026 | Tree loss: 1.190 | Accuracy: 0.632500 | 0.533 sec/iter\n",
      "Epoch: 56 | Batch: 001 / 011 | Total loss: 1.167 | Reg loss: 0.026 | Tree loss: 1.167 | Accuracy: 0.628000 | 0.532 sec/iter\n",
      "Epoch: 56 | Batch: 002 / 011 | Total loss: 1.129 | Reg loss: 0.026 | Tree loss: 1.129 | Accuracy: 0.624000 | 0.532 sec/iter\n",
      "Epoch: 56 | Batch: 003 / 011 | Total loss: 1.133 | Reg loss: 0.026 | Tree loss: 1.133 | Accuracy: 0.592500 | 0.531 sec/iter\n",
      "Epoch: 56 | Batch: 004 / 011 | Total loss: 1.103 | Reg loss: 0.026 | Tree loss: 1.103 | Accuracy: 0.577000 | 0.531 sec/iter\n",
      "Epoch: 56 | Batch: 005 / 011 | Total loss: 1.088 | Reg loss: 0.026 | Tree loss: 1.088 | Accuracy: 0.565000 | 0.53 sec/iter\n",
      "Epoch: 56 | Batch: 006 / 011 | Total loss: 1.075 | Reg loss: 0.026 | Tree loss: 1.075 | Accuracy: 0.571000 | 0.53 sec/iter\n",
      "Epoch: 56 | Batch: 007 / 011 | Total loss: 1.086 | Reg loss: 0.026 | Tree loss: 1.086 | Accuracy: 0.558000 | 0.529 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 56 | Batch: 008 / 011 | Total loss: 1.074 | Reg loss: 0.026 | Tree loss: 1.074 | Accuracy: 0.574000 | 0.529 sec/iter\n",
      "Epoch: 56 | Batch: 009 / 011 | Total loss: 1.065 | Reg loss: 0.026 | Tree loss: 1.065 | Accuracy: 0.565000 | 0.528 sec/iter\n",
      "Epoch: 56 | Batch: 010 / 011 | Total loss: 1.065 | Reg loss: 0.026 | Tree loss: 1.065 | Accuracy: 0.552901 | 0.528 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 57 | Batch: 000 / 011 | Total loss: 1.175 | Reg loss: 0.026 | Tree loss: 1.175 | Accuracy: 0.633500 | 0.534 sec/iter\n",
      "Epoch: 57 | Batch: 001 / 011 | Total loss: 1.150 | Reg loss: 0.026 | Tree loss: 1.150 | Accuracy: 0.625500 | 0.533 sec/iter\n",
      "Epoch: 57 | Batch: 002 / 011 | Total loss: 1.133 | Reg loss: 0.026 | Tree loss: 1.133 | Accuracy: 0.601000 | 0.533 sec/iter\n",
      "Epoch: 57 | Batch: 003 / 011 | Total loss: 1.119 | Reg loss: 0.026 | Tree loss: 1.119 | Accuracy: 0.611500 | 0.532 sec/iter\n",
      "Epoch: 57 | Batch: 004 / 011 | Total loss: 1.095 | Reg loss: 0.026 | Tree loss: 1.095 | Accuracy: 0.603500 | 0.532 sec/iter\n",
      "Epoch: 57 | Batch: 005 / 011 | Total loss: 1.096 | Reg loss: 0.026 | Tree loss: 1.096 | Accuracy: 0.587500 | 0.531 sec/iter\n",
      "Epoch: 57 | Batch: 006 / 011 | Total loss: 1.065 | Reg loss: 0.026 | Tree loss: 1.065 | Accuracy: 0.624500 | 0.531 sec/iter\n",
      "Epoch: 57 | Batch: 007 / 011 | Total loss: 1.084 | Reg loss: 0.026 | Tree loss: 1.084 | Accuracy: 0.572500 | 0.53 sec/iter\n",
      "Epoch: 57 | Batch: 008 / 011 | Total loss: 1.042 | Reg loss: 0.026 | Tree loss: 1.042 | Accuracy: 0.592500 | 0.53 sec/iter\n",
      "Epoch: 57 | Batch: 009 / 011 | Total loss: 1.081 | Reg loss: 0.026 | Tree loss: 1.081 | Accuracy: 0.582500 | 0.529 sec/iter\n",
      "Epoch: 57 | Batch: 010 / 011 | Total loss: 1.098 | Reg loss: 0.026 | Tree loss: 1.098 | Accuracy: 0.593857 | 0.529 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 58 | Batch: 000 / 011 | Total loss: 1.150 | Reg loss: 0.026 | Tree loss: 1.150 | Accuracy: 0.657000 | 0.53 sec/iter\n",
      "Epoch: 58 | Batch: 001 / 011 | Total loss: 1.157 | Reg loss: 0.026 | Tree loss: 1.157 | Accuracy: 0.636500 | 0.529 sec/iter\n",
      "Epoch: 58 | Batch: 002 / 011 | Total loss: 1.129 | Reg loss: 0.026 | Tree loss: 1.129 | Accuracy: 0.623000 | 0.529 sec/iter\n",
      "Epoch: 58 | Batch: 003 / 011 | Total loss: 1.099 | Reg loss: 0.026 | Tree loss: 1.099 | Accuracy: 0.632000 | 0.528 sec/iter\n",
      "Epoch: 58 | Batch: 004 / 011 | Total loss: 1.106 | Reg loss: 0.026 | Tree loss: 1.106 | Accuracy: 0.596500 | 0.528 sec/iter\n",
      "Epoch: 58 | Batch: 005 / 011 | Total loss: 1.099 | Reg loss: 0.026 | Tree loss: 1.099 | Accuracy: 0.590000 | 0.527 sec/iter\n",
      "Epoch: 58 | Batch: 006 / 011 | Total loss: 1.069 | Reg loss: 0.026 | Tree loss: 1.069 | Accuracy: 0.587000 | 0.527 sec/iter\n",
      "Epoch: 58 | Batch: 007 / 011 | Total loss: 1.085 | Reg loss: 0.026 | Tree loss: 1.085 | Accuracy: 0.552500 | 0.526 sec/iter\n",
      "Epoch: 58 | Batch: 008 / 011 | Total loss: 1.068 | Reg loss: 0.026 | Tree loss: 1.068 | Accuracy: 0.566500 | 0.526 sec/iter\n",
      "Epoch: 58 | Batch: 009 / 011 | Total loss: 1.066 | Reg loss: 0.026 | Tree loss: 1.066 | Accuracy: 0.580000 | 0.526 sec/iter\n",
      "Epoch: 58 | Batch: 010 / 011 | Total loss: 1.040 | Reg loss: 0.026 | Tree loss: 1.040 | Accuracy: 0.614334 | 0.525 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 59 | Batch: 000 / 011 | Total loss: 1.152 | Reg loss: 0.026 | Tree loss: 1.152 | Accuracy: 0.648500 | 0.529 sec/iter\n",
      "Epoch: 59 | Batch: 001 / 011 | Total loss: 1.151 | Reg loss: 0.026 | Tree loss: 1.151 | Accuracy: 0.630500 | 0.529 sec/iter\n",
      "Epoch: 59 | Batch: 002 / 011 | Total loss: 1.112 | Reg loss: 0.026 | Tree loss: 1.112 | Accuracy: 0.639000 | 0.528 sec/iter\n",
      "Epoch: 59 | Batch: 003 / 011 | Total loss: 1.123 | Reg loss: 0.026 | Tree loss: 1.123 | Accuracy: 0.598000 | 0.528 sec/iter\n",
      "Epoch: 59 | Batch: 004 / 011 | Total loss: 1.089 | Reg loss: 0.026 | Tree loss: 1.089 | Accuracy: 0.609500 | 0.527 sec/iter\n",
      "Epoch: 59 | Batch: 005 / 011 | Total loss: 1.059 | Reg loss: 0.026 | Tree loss: 1.059 | Accuracy: 0.620500 | 0.527 sec/iter\n",
      "Epoch: 59 | Batch: 006 / 011 | Total loss: 1.079 | Reg loss: 0.026 | Tree loss: 1.079 | Accuracy: 0.577000 | 0.526 sec/iter\n",
      "Epoch: 59 | Batch: 007 / 011 | Total loss: 1.079 | Reg loss: 0.026 | Tree loss: 1.079 | Accuracy: 0.569000 | 0.526 sec/iter\n",
      "Epoch: 59 | Batch: 008 / 011 | Total loss: 1.092 | Reg loss: 0.026 | Tree loss: 1.092 | Accuracy: 0.557000 | 0.526 sec/iter\n",
      "Epoch: 59 | Batch: 009 / 011 | Total loss: 1.040 | Reg loss: 0.026 | Tree loss: 1.040 | Accuracy: 0.606000 | 0.525 sec/iter\n",
      "Epoch: 59 | Batch: 010 / 011 | Total loss: 1.020 | Reg loss: 0.026 | Tree loss: 1.020 | Accuracy: 0.645051 | 0.525 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 60 | Batch: 000 / 011 | Total loss: 1.169 | Reg loss: 0.026 | Tree loss: 1.169 | Accuracy: 0.614000 | 0.531 sec/iter\n",
      "Epoch: 60 | Batch: 001 / 011 | Total loss: 1.137 | Reg loss: 0.026 | Tree loss: 1.137 | Accuracy: 0.620500 | 0.53 sec/iter\n",
      "Epoch: 60 | Batch: 002 / 011 | Total loss: 1.119 | Reg loss: 0.026 | Tree loss: 1.119 | Accuracy: 0.622000 | 0.53 sec/iter\n",
      "Epoch: 60 | Batch: 003 / 011 | Total loss: 1.094 | Reg loss: 0.026 | Tree loss: 1.094 | Accuracy: 0.620500 | 0.529 sec/iter\n",
      "Epoch: 60 | Batch: 004 / 011 | Total loss: 1.094 | Reg loss: 0.026 | Tree loss: 1.094 | Accuracy: 0.592000 | 0.529 sec/iter\n",
      "Epoch: 60 | Batch: 005 / 011 | Total loss: 1.076 | Reg loss: 0.026 | Tree loss: 1.076 | Accuracy: 0.592000 | 0.528 sec/iter\n",
      "Epoch: 60 | Batch: 006 / 011 | Total loss: 1.065 | Reg loss: 0.026 | Tree loss: 1.065 | Accuracy: 0.591000 | 0.528 sec/iter\n",
      "Epoch: 60 | Batch: 007 / 011 | Total loss: 1.054 | Reg loss: 0.026 | Tree loss: 1.054 | Accuracy: 0.607500 | 0.528 sec/iter\n",
      "Epoch: 60 | Batch: 008 / 011 | Total loss: 1.059 | Reg loss: 0.026 | Tree loss: 1.059 | Accuracy: 0.609000 | 0.527 sec/iter\n",
      "Epoch: 60 | Batch: 009 / 011 | Total loss: 1.031 | Reg loss: 0.026 | Tree loss: 1.031 | Accuracy: 0.607500 | 0.527 sec/iter\n",
      "Epoch: 60 | Batch: 010 / 011 | Total loss: 1.050 | Reg loss: 0.026 | Tree loss: 1.050 | Accuracy: 0.607509 | 0.526 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 61 | Batch: 000 / 011 | Total loss: 1.131 | Reg loss: 0.026 | Tree loss: 1.131 | Accuracy: 0.647500 | 0.528 sec/iter\n",
      "Epoch: 61 | Batch: 001 / 011 | Total loss: 1.140 | Reg loss: 0.026 | Tree loss: 1.140 | Accuracy: 0.621500 | 0.527 sec/iter\n",
      "Epoch: 61 | Batch: 002 / 011 | Total loss: 1.105 | Reg loss: 0.026 | Tree loss: 1.105 | Accuracy: 0.630500 | 0.527 sec/iter\n",
      "Epoch: 61 | Batch: 003 / 011 | Total loss: 1.101 | Reg loss: 0.026 | Tree loss: 1.101 | Accuracy: 0.613500 | 0.526 sec/iter\n",
      "Epoch: 61 | Batch: 004 / 011 | Total loss: 1.096 | Reg loss: 0.026 | Tree loss: 1.096 | Accuracy: 0.597500 | 0.526 sec/iter\n",
      "Epoch: 61 | Batch: 005 / 011 | Total loss: 1.063 | Reg loss: 0.026 | Tree loss: 1.063 | Accuracy: 0.614500 | 0.525 sec/iter\n",
      "Epoch: 61 | Batch: 006 / 011 | Total loss: 1.053 | Reg loss: 0.026 | Tree loss: 1.053 | Accuracy: 0.612500 | 0.525 sec/iter\n",
      "Epoch: 61 | Batch: 007 / 011 | Total loss: 1.039 | Reg loss: 0.026 | Tree loss: 1.039 | Accuracy: 0.610500 | 0.525 sec/iter\n",
      "Epoch: 61 | Batch: 008 / 011 | Total loss: 1.058 | Reg loss: 0.026 | Tree loss: 1.058 | Accuracy: 0.594000 | 0.524 sec/iter\n",
      "Epoch: 61 | Batch: 009 / 011 | Total loss: 1.063 | Reg loss: 0.026 | Tree loss: 1.063 | Accuracy: 0.600500 | 0.524 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 61 | Batch: 010 / 011 | Total loss: 1.062 | Reg loss: 0.026 | Tree loss: 1.062 | Accuracy: 0.597270 | 0.523 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 62 | Batch: 000 / 011 | Total loss: 1.162 | Reg loss: 0.026 | Tree loss: 1.162 | Accuracy: 0.637000 | 0.527 sec/iter\n",
      "Epoch: 62 | Batch: 001 / 011 | Total loss: 1.131 | Reg loss: 0.026 | Tree loss: 1.131 | Accuracy: 0.657000 | 0.527 sec/iter\n",
      "Epoch: 62 | Batch: 002 / 011 | Total loss: 1.102 | Reg loss: 0.026 | Tree loss: 1.102 | Accuracy: 0.628500 | 0.526 sec/iter\n",
      "Epoch: 62 | Batch: 003 / 011 | Total loss: 1.100 | Reg loss: 0.026 | Tree loss: 1.100 | Accuracy: 0.593500 | 0.526 sec/iter\n",
      "Epoch: 62 | Batch: 004 / 011 | Total loss: 1.065 | Reg loss: 0.026 | Tree loss: 1.065 | Accuracy: 0.618000 | 0.525 sec/iter\n",
      "Epoch: 62 | Batch: 005 / 011 | Total loss: 1.047 | Reg loss: 0.026 | Tree loss: 1.047 | Accuracy: 0.620500 | 0.525 sec/iter\n",
      "Epoch: 62 | Batch: 006 / 011 | Total loss: 1.041 | Reg loss: 0.026 | Tree loss: 1.041 | Accuracy: 0.589000 | 0.524 sec/iter\n",
      "Epoch: 62 | Batch: 007 / 011 | Total loss: 1.046 | Reg loss: 0.026 | Tree loss: 1.046 | Accuracy: 0.582500 | 0.524 sec/iter\n",
      "Epoch: 62 | Batch: 008 / 011 | Total loss: 1.051 | Reg loss: 0.026 | Tree loss: 1.051 | Accuracy: 0.575000 | 0.523 sec/iter\n",
      "Epoch: 62 | Batch: 009 / 011 | Total loss: 1.031 | Reg loss: 0.026 | Tree loss: 1.031 | Accuracy: 0.603500 | 0.523 sec/iter\n",
      "Epoch: 62 | Batch: 010 / 011 | Total loss: 1.120 | Reg loss: 0.026 | Tree loss: 1.120 | Accuracy: 0.580205 | 0.523 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 63 | Batch: 000 / 011 | Total loss: 1.124 | Reg loss: 0.026 | Tree loss: 1.124 | Accuracy: 0.666000 | 0.527 sec/iter\n",
      "Epoch: 63 | Batch: 001 / 011 | Total loss: 1.119 | Reg loss: 0.026 | Tree loss: 1.119 | Accuracy: 0.667000 | 0.526 sec/iter\n",
      "Epoch: 63 | Batch: 002 / 011 | Total loss: 1.095 | Reg loss: 0.026 | Tree loss: 1.095 | Accuracy: 0.637000 | 0.526 sec/iter\n",
      "Epoch: 63 | Batch: 003 / 011 | Total loss: 1.088 | Reg loss: 0.026 | Tree loss: 1.088 | Accuracy: 0.628000 | 0.526 sec/iter\n",
      "Epoch: 63 | Batch: 004 / 011 | Total loss: 1.068 | Reg loss: 0.026 | Tree loss: 1.068 | Accuracy: 0.628000 | 0.525 sec/iter\n",
      "Epoch: 63 | Batch: 005 / 011 | Total loss: 1.061 | Reg loss: 0.026 | Tree loss: 1.061 | Accuracy: 0.597500 | 0.525 sec/iter\n",
      "Epoch: 63 | Batch: 006 / 011 | Total loss: 1.055 | Reg loss: 0.026 | Tree loss: 1.055 | Accuracy: 0.586500 | 0.524 sec/iter\n",
      "Epoch: 63 | Batch: 007 / 011 | Total loss: 1.048 | Reg loss: 0.026 | Tree loss: 1.048 | Accuracy: 0.589000 | 0.524 sec/iter\n",
      "Epoch: 63 | Batch: 008 / 011 | Total loss: 1.033 | Reg loss: 0.026 | Tree loss: 1.033 | Accuracy: 0.610000 | 0.524 sec/iter\n",
      "Epoch: 63 | Batch: 009 / 011 | Total loss: 1.061 | Reg loss: 0.026 | Tree loss: 1.061 | Accuracy: 0.590000 | 0.523 sec/iter\n",
      "Epoch: 63 | Batch: 010 / 011 | Total loss: 1.067 | Reg loss: 0.026 | Tree loss: 1.067 | Accuracy: 0.607509 | 0.523 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 64 | Batch: 000 / 011 | Total loss: 1.142 | Reg loss: 0.026 | Tree loss: 1.142 | Accuracy: 0.665000 | 0.524 sec/iter\n",
      "Epoch: 64 | Batch: 001 / 011 | Total loss: 1.107 | Reg loss: 0.026 | Tree loss: 1.107 | Accuracy: 0.670000 | 0.524 sec/iter\n",
      "Epoch: 64 | Batch: 002 / 011 | Total loss: 1.093 | Reg loss: 0.026 | Tree loss: 1.093 | Accuracy: 0.630500 | 0.524 sec/iter\n",
      "Epoch: 64 | Batch: 003 / 011 | Total loss: 1.075 | Reg loss: 0.026 | Tree loss: 1.075 | Accuracy: 0.642000 | 0.523 sec/iter\n",
      "Epoch: 64 | Batch: 004 / 011 | Total loss: 1.063 | Reg loss: 0.026 | Tree loss: 1.063 | Accuracy: 0.616500 | 0.523 sec/iter\n",
      "Epoch: 64 | Batch: 005 / 011 | Total loss: 1.035 | Reg loss: 0.026 | Tree loss: 1.035 | Accuracy: 0.614000 | 0.523 sec/iter\n",
      "Epoch: 64 | Batch: 006 / 011 | Total loss: 1.043 | Reg loss: 0.026 | Tree loss: 1.043 | Accuracy: 0.602000 | 0.522 sec/iter\n",
      "Epoch: 64 | Batch: 007 / 011 | Total loss: 1.046 | Reg loss: 0.026 | Tree loss: 1.046 | Accuracy: 0.596000 | 0.522 sec/iter\n",
      "Epoch: 64 | Batch: 008 / 011 | Total loss: 1.038 | Reg loss: 0.026 | Tree loss: 1.038 | Accuracy: 0.608500 | 0.522 sec/iter\n",
      "Epoch: 64 | Batch: 009 / 011 | Total loss: 1.038 | Reg loss: 0.026 | Tree loss: 1.038 | Accuracy: 0.615000 | 0.521 sec/iter\n",
      "Epoch: 64 | Batch: 010 / 011 | Total loss: 1.039 | Reg loss: 0.026 | Tree loss: 1.039 | Accuracy: 0.631399 | 0.521 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 65 | Batch: 000 / 011 | Total loss: 1.122 | Reg loss: 0.026 | Tree loss: 1.122 | Accuracy: 0.656000 | 0.53 sec/iter\n",
      "Epoch: 65 | Batch: 001 / 011 | Total loss: 1.124 | Reg loss: 0.026 | Tree loss: 1.124 | Accuracy: 0.653500 | 0.529 sec/iter\n",
      "Epoch: 65 | Batch: 002 / 011 | Total loss: 1.090 | Reg loss: 0.026 | Tree loss: 1.090 | Accuracy: 0.652500 | 0.529 sec/iter\n",
      "Epoch: 65 | Batch: 003 / 011 | Total loss: 1.049 | Reg loss: 0.026 | Tree loss: 1.049 | Accuracy: 0.661000 | 0.529 sec/iter\n",
      "Epoch: 65 | Batch: 004 / 011 | Total loss: 1.063 | Reg loss: 0.026 | Tree loss: 1.063 | Accuracy: 0.627000 | 0.528 sec/iter\n",
      "Epoch: 65 | Batch: 005 / 011 | Total loss: 1.043 | Reg loss: 0.026 | Tree loss: 1.043 | Accuracy: 0.603500 | 0.528 sec/iter\n",
      "Epoch: 65 | Batch: 006 / 011 | Total loss: 1.060 | Reg loss: 0.026 | Tree loss: 1.060 | Accuracy: 0.588000 | 0.528 sec/iter\n",
      "Epoch: 65 | Batch: 007 / 011 | Total loss: 1.041 | Reg loss: 0.026 | Tree loss: 1.041 | Accuracy: 0.588500 | 0.527 sec/iter\n",
      "Epoch: 65 | Batch: 008 / 011 | Total loss: 1.026 | Reg loss: 0.026 | Tree loss: 1.026 | Accuracy: 0.603000 | 0.527 sec/iter\n",
      "Epoch: 65 | Batch: 009 / 011 | Total loss: 1.025 | Reg loss: 0.026 | Tree loss: 1.025 | Accuracy: 0.602500 | 0.526 sec/iter\n",
      "Epoch: 65 | Batch: 010 / 011 | Total loss: 0.969 | Reg loss: 0.026 | Tree loss: 0.969 | Accuracy: 0.668942 | 0.526 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 66 | Batch: 000 / 011 | Total loss: 1.119 | Reg loss: 0.026 | Tree loss: 1.119 | Accuracy: 0.654000 | 0.531 sec/iter\n",
      "Epoch: 66 | Batch: 001 / 011 | Total loss: 1.103 | Reg loss: 0.026 | Tree loss: 1.103 | Accuracy: 0.658000 | 0.531 sec/iter\n",
      "Epoch: 66 | Batch: 002 / 011 | Total loss: 1.083 | Reg loss: 0.026 | Tree loss: 1.083 | Accuracy: 0.649500 | 0.531 sec/iter\n",
      "Epoch: 66 | Batch: 003 / 011 | Total loss: 1.053 | Reg loss: 0.026 | Tree loss: 1.053 | Accuracy: 0.636000 | 0.53 sec/iter\n",
      "Epoch: 66 | Batch: 004 / 011 | Total loss: 1.054 | Reg loss: 0.026 | Tree loss: 1.054 | Accuracy: 0.625500 | 0.53 sec/iter\n",
      "Epoch: 66 | Batch: 005 / 011 | Total loss: 1.029 | Reg loss: 0.026 | Tree loss: 1.029 | Accuracy: 0.626000 | 0.53 sec/iter\n",
      "Epoch: 66 | Batch: 006 / 011 | Total loss: 1.064 | Reg loss: 0.026 | Tree loss: 1.064 | Accuracy: 0.591000 | 0.529 sec/iter\n",
      "Epoch: 66 | Batch: 007 / 011 | Total loss: 1.037 | Reg loss: 0.026 | Tree loss: 1.037 | Accuracy: 0.612500 | 0.529 sec/iter\n",
      "Epoch: 66 | Batch: 008 / 011 | Total loss: 1.017 | Reg loss: 0.026 | Tree loss: 1.017 | Accuracy: 0.628000 | 0.528 sec/iter\n",
      "Epoch: 66 | Batch: 009 / 011 | Total loss: 1.026 | Reg loss: 0.026 | Tree loss: 1.026 | Accuracy: 0.608500 | 0.528 sec/iter\n",
      "Epoch: 66 | Batch: 010 / 011 | Total loss: 1.056 | Reg loss: 0.026 | Tree loss: 1.056 | Accuracy: 0.617747 | 0.528 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 67 | Batch: 000 / 011 | Total loss: 1.111 | Reg loss: 0.026 | Tree loss: 1.111 | Accuracy: 0.664000 | 0.531 sec/iter\n",
      "Epoch: 67 | Batch: 001 / 011 | Total loss: 1.105 | Reg loss: 0.026 | Tree loss: 1.105 | Accuracy: 0.661500 | 0.53 sec/iter\n",
      "Epoch: 67 | Batch: 002 / 011 | Total loss: 1.087 | Reg loss: 0.026 | Tree loss: 1.087 | Accuracy: 0.639000 | 0.53 sec/iter\n",
      "Epoch: 67 | Batch: 003 / 011 | Total loss: 1.035 | Reg loss: 0.026 | Tree loss: 1.035 | Accuracy: 0.656000 | 0.53 sec/iter\n",
      "Epoch: 67 | Batch: 004 / 011 | Total loss: 1.049 | Reg loss: 0.026 | Tree loss: 1.049 | Accuracy: 0.602500 | 0.529 sec/iter\n",
      "Epoch: 67 | Batch: 005 / 011 | Total loss: 1.050 | Reg loss: 0.026 | Tree loss: 1.050 | Accuracy: 0.584500 | 0.529 sec/iter\n",
      "Epoch: 67 | Batch: 006 / 011 | Total loss: 1.018 | Reg loss: 0.026 | Tree loss: 1.018 | Accuracy: 0.603500 | 0.528 sec/iter\n",
      "Epoch: 67 | Batch: 007 / 011 | Total loss: 1.040 | Reg loss: 0.026 | Tree loss: 1.040 | Accuracy: 0.585500 | 0.528 sec/iter\n",
      "Epoch: 67 | Batch: 008 / 011 | Total loss: 1.019 | Reg loss: 0.026 | Tree loss: 1.019 | Accuracy: 0.602000 | 0.528 sec/iter\n",
      "Epoch: 67 | Batch: 009 / 011 | Total loss: 1.032 | Reg loss: 0.026 | Tree loss: 1.032 | Accuracy: 0.623000 | 0.528 sec/iter\n",
      "Epoch: 67 | Batch: 010 / 011 | Total loss: 1.009 | Reg loss: 0.026 | Tree loss: 1.009 | Accuracy: 0.621160 | 0.527 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 68 | Batch: 000 / 011 | Total loss: 1.105 | Reg loss: 0.026 | Tree loss: 1.105 | Accuracy: 0.654000 | 0.536 sec/iter\n",
      "Epoch: 68 | Batch: 001 / 011 | Total loss: 1.087 | Reg loss: 0.026 | Tree loss: 1.087 | Accuracy: 0.672500 | 0.536 sec/iter\n",
      "Epoch: 68 | Batch: 002 / 011 | Total loss: 1.074 | Reg loss: 0.026 | Tree loss: 1.074 | Accuracy: 0.654000 | 0.535 sec/iter\n",
      "Epoch: 68 | Batch: 003 / 011 | Total loss: 1.059 | Reg loss: 0.026 | Tree loss: 1.059 | Accuracy: 0.644500 | 0.535 sec/iter\n",
      "Epoch: 68 | Batch: 004 / 011 | Total loss: 1.062 | Reg loss: 0.026 | Tree loss: 1.062 | Accuracy: 0.640000 | 0.535 sec/iter\n",
      "Epoch: 68 | Batch: 005 / 011 | Total loss: 1.028 | Reg loss: 0.026 | Tree loss: 1.028 | Accuracy: 0.628000 | 0.534 sec/iter\n",
      "Epoch: 68 | Batch: 006 / 011 | Total loss: 1.029 | Reg loss: 0.026 | Tree loss: 1.029 | Accuracy: 0.616000 | 0.534 sec/iter\n",
      "Epoch: 68 | Batch: 007 / 011 | Total loss: 1.025 | Reg loss: 0.026 | Tree loss: 1.025 | Accuracy: 0.613500 | 0.534 sec/iter\n",
      "Epoch: 68 | Batch: 008 / 011 | Total loss: 1.032 | Reg loss: 0.026 | Tree loss: 1.032 | Accuracy: 0.608500 | 0.533 sec/iter\n",
      "Epoch: 68 | Batch: 009 / 011 | Total loss: 1.005 | Reg loss: 0.026 | Tree loss: 1.005 | Accuracy: 0.625000 | 0.533 sec/iter\n",
      "Epoch: 68 | Batch: 010 / 011 | Total loss: 1.003 | Reg loss: 0.026 | Tree loss: 1.003 | Accuracy: 0.648464 | 0.533 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 69 | Batch: 000 / 011 | Total loss: 1.100 | Reg loss: 0.026 | Tree loss: 1.100 | Accuracy: 0.666000 | 0.537 sec/iter\n",
      "Epoch: 69 | Batch: 001 / 011 | Total loss: 1.076 | Reg loss: 0.026 | Tree loss: 1.076 | Accuracy: 0.671500 | 0.536 sec/iter\n",
      "Epoch: 69 | Batch: 002 / 011 | Total loss: 1.068 | Reg loss: 0.026 | Tree loss: 1.068 | Accuracy: 0.661500 | 0.536 sec/iter\n",
      "Epoch: 69 | Batch: 003 / 011 | Total loss: 1.064 | Reg loss: 0.026 | Tree loss: 1.064 | Accuracy: 0.649500 | 0.535 sec/iter\n",
      "Epoch: 69 | Batch: 004 / 011 | Total loss: 1.023 | Reg loss: 0.026 | Tree loss: 1.023 | Accuracy: 0.650000 | 0.535 sec/iter\n",
      "Epoch: 69 | Batch: 005 / 011 | Total loss: 1.062 | Reg loss: 0.026 | Tree loss: 1.062 | Accuracy: 0.613000 | 0.535 sec/iter\n",
      "Epoch: 69 | Batch: 006 / 011 | Total loss: 1.026 | Reg loss: 0.026 | Tree loss: 1.026 | Accuracy: 0.625000 | 0.534 sec/iter\n",
      "Epoch: 69 | Batch: 007 / 011 | Total loss: 1.008 | Reg loss: 0.026 | Tree loss: 1.008 | Accuracy: 0.626000 | 0.534 sec/iter\n",
      "Epoch: 69 | Batch: 008 / 011 | Total loss: 1.017 | Reg loss: 0.026 | Tree loss: 1.017 | Accuracy: 0.609000 | 0.534 sec/iter\n",
      "Epoch: 69 | Batch: 009 / 011 | Total loss: 1.016 | Reg loss: 0.026 | Tree loss: 1.016 | Accuracy: 0.613500 | 0.533 sec/iter\n",
      "Epoch: 69 | Batch: 010 / 011 | Total loss: 0.985 | Reg loss: 0.026 | Tree loss: 0.985 | Accuracy: 0.641638 | 0.533 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 70 | Batch: 000 / 011 | Total loss: 1.078 | Reg loss: 0.026 | Tree loss: 1.078 | Accuracy: 0.683000 | 0.538 sec/iter\n",
      "Epoch: 70 | Batch: 001 / 011 | Total loss: 1.086 | Reg loss: 0.026 | Tree loss: 1.086 | Accuracy: 0.680000 | 0.537 sec/iter\n",
      "Epoch: 70 | Batch: 002 / 011 | Total loss: 1.074 | Reg loss: 0.026 | Tree loss: 1.074 | Accuracy: 0.644500 | 0.537 sec/iter\n",
      "Epoch: 70 | Batch: 003 / 011 | Total loss: 1.050 | Reg loss: 0.026 | Tree loss: 1.050 | Accuracy: 0.647500 | 0.537 sec/iter\n",
      "Epoch: 70 | Batch: 004 / 011 | Total loss: 1.014 | Reg loss: 0.026 | Tree loss: 1.014 | Accuracy: 0.655500 | 0.536 sec/iter\n",
      "Epoch: 70 | Batch: 005 / 011 | Total loss: 1.036 | Reg loss: 0.026 | Tree loss: 1.036 | Accuracy: 0.603500 | 0.536 sec/iter\n",
      "Epoch: 70 | Batch: 006 / 011 | Total loss: 0.992 | Reg loss: 0.026 | Tree loss: 0.992 | Accuracy: 0.650500 | 0.535 sec/iter\n",
      "Epoch: 70 | Batch: 007 / 011 | Total loss: 1.040 | Reg loss: 0.026 | Tree loss: 1.040 | Accuracy: 0.608500 | 0.535 sec/iter\n",
      "Epoch: 70 | Batch: 008 / 011 | Total loss: 1.008 | Reg loss: 0.026 | Tree loss: 1.008 | Accuracy: 0.617000 | 0.535 sec/iter\n",
      "Epoch: 70 | Batch: 009 / 011 | Total loss: 1.028 | Reg loss: 0.026 | Tree loss: 1.028 | Accuracy: 0.621500 | 0.534 sec/iter\n",
      "Epoch: 70 | Batch: 010 / 011 | Total loss: 1.043 | Reg loss: 0.026 | Tree loss: 1.043 | Accuracy: 0.610922 | 0.534 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 71 | Batch: 000 / 011 | Total loss: 1.104 | Reg loss: 0.026 | Tree loss: 1.104 | Accuracy: 0.657500 | 0.54 sec/iter\n",
      "Epoch: 71 | Batch: 001 / 011 | Total loss: 1.073 | Reg loss: 0.026 | Tree loss: 1.073 | Accuracy: 0.658500 | 0.54 sec/iter\n",
      "Epoch: 71 | Batch: 002 / 011 | Total loss: 1.069 | Reg loss: 0.026 | Tree loss: 1.069 | Accuracy: 0.633500 | 0.539 sec/iter\n",
      "Epoch: 71 | Batch: 003 / 011 | Total loss: 1.037 | Reg loss: 0.026 | Tree loss: 1.037 | Accuracy: 0.639500 | 0.539 sec/iter\n",
      "Epoch: 71 | Batch: 004 / 011 | Total loss: 1.028 | Reg loss: 0.026 | Tree loss: 1.028 | Accuracy: 0.618000 | 0.539 sec/iter\n",
      "Epoch: 71 | Batch: 005 / 011 | Total loss: 1.011 | Reg loss: 0.026 | Tree loss: 1.011 | Accuracy: 0.632000 | 0.538 sec/iter\n",
      "Epoch: 71 | Batch: 006 / 011 | Total loss: 1.019 | Reg loss: 0.026 | Tree loss: 1.019 | Accuracy: 0.620000 | 0.538 sec/iter\n",
      "Epoch: 71 | Batch: 007 / 011 | Total loss: 1.003 | Reg loss: 0.026 | Tree loss: 1.003 | Accuracy: 0.630500 | 0.538 sec/iter\n",
      "Epoch: 71 | Batch: 008 / 011 | Total loss: 1.009 | Reg loss: 0.026 | Tree loss: 1.009 | Accuracy: 0.634500 | 0.537 sec/iter\n",
      "Epoch: 71 | Batch: 009 / 011 | Total loss: 1.010 | Reg loss: 0.026 | Tree loss: 1.010 | Accuracy: 0.634500 | 0.537 sec/iter\n",
      "Epoch: 71 | Batch: 010 / 011 | Total loss: 0.983 | Reg loss: 0.026 | Tree loss: 0.983 | Accuracy: 0.672355 | 0.537 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 72 | Batch: 000 / 011 | Total loss: 1.093 | Reg loss: 0.026 | Tree loss: 1.093 | Accuracy: 0.659000 | 0.54 sec/iter\n",
      "Epoch: 72 | Batch: 001 / 011 | Total loss: 1.061 | Reg loss: 0.026 | Tree loss: 1.061 | Accuracy: 0.686000 | 0.54 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 72 | Batch: 002 / 011 | Total loss: 1.066 | Reg loss: 0.026 | Tree loss: 1.066 | Accuracy: 0.641500 | 0.539 sec/iter\n",
      "Epoch: 72 | Batch: 003 / 011 | Total loss: 1.042 | Reg loss: 0.026 | Tree loss: 1.042 | Accuracy: 0.650000 | 0.539 sec/iter\n",
      "Epoch: 72 | Batch: 004 / 011 | Total loss: 1.048 | Reg loss: 0.026 | Tree loss: 1.048 | Accuracy: 0.627500 | 0.539 sec/iter\n",
      "Epoch: 72 | Batch: 005 / 011 | Total loss: 1.018 | Reg loss: 0.026 | Tree loss: 1.018 | Accuracy: 0.626000 | 0.538 sec/iter\n",
      "Epoch: 72 | Batch: 006 / 011 | Total loss: 1.011 | Reg loss: 0.026 | Tree loss: 1.011 | Accuracy: 0.612500 | 0.538 sec/iter\n",
      "Epoch: 72 | Batch: 007 / 011 | Total loss: 0.987 | Reg loss: 0.026 | Tree loss: 0.987 | Accuracy: 0.636500 | 0.538 sec/iter\n",
      "Epoch: 72 | Batch: 008 / 011 | Total loss: 1.005 | Reg loss: 0.026 | Tree loss: 1.005 | Accuracy: 0.607500 | 0.537 sec/iter\n",
      "Epoch: 72 | Batch: 009 / 011 | Total loss: 0.993 | Reg loss: 0.026 | Tree loss: 0.993 | Accuracy: 0.639000 | 0.537 sec/iter\n",
      "Epoch: 72 | Batch: 010 / 011 | Total loss: 1.008 | Reg loss: 0.026 | Tree loss: 1.008 | Accuracy: 0.607509 | 0.536 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 73 | Batch: 000 / 011 | Total loss: 1.072 | Reg loss: 0.026 | Tree loss: 1.072 | Accuracy: 0.666000 | 0.541 sec/iter\n",
      "Epoch: 73 | Batch: 001 / 011 | Total loss: 1.079 | Reg loss: 0.026 | Tree loss: 1.079 | Accuracy: 0.664000 | 0.54 sec/iter\n",
      "Epoch: 73 | Batch: 002 / 011 | Total loss: 1.059 | Reg loss: 0.026 | Tree loss: 1.059 | Accuracy: 0.646000 | 0.54 sec/iter\n",
      "Epoch: 73 | Batch: 003 / 011 | Total loss: 1.039 | Reg loss: 0.026 | Tree loss: 1.039 | Accuracy: 0.639000 | 0.54 sec/iter\n",
      "Epoch: 73 | Batch: 004 / 011 | Total loss: 1.049 | Reg loss: 0.026 | Tree loss: 1.049 | Accuracy: 0.629000 | 0.539 sec/iter\n",
      "Epoch: 73 | Batch: 005 / 011 | Total loss: 1.012 | Reg loss: 0.026 | Tree loss: 1.012 | Accuracy: 0.620000 | 0.539 sec/iter\n",
      "Epoch: 73 | Batch: 006 / 011 | Total loss: 1.016 | Reg loss: 0.026 | Tree loss: 1.016 | Accuracy: 0.638000 | 0.538 sec/iter\n",
      "Epoch: 73 | Batch: 007 / 011 | Total loss: 0.995 | Reg loss: 0.026 | Tree loss: 0.995 | Accuracy: 0.637000 | 0.538 sec/iter\n",
      "Epoch: 73 | Batch: 008 / 011 | Total loss: 0.960 | Reg loss: 0.026 | Tree loss: 0.960 | Accuracy: 0.661000 | 0.538 sec/iter\n",
      "Epoch: 73 | Batch: 009 / 011 | Total loss: 0.992 | Reg loss: 0.026 | Tree loss: 0.992 | Accuracy: 0.639000 | 0.537 sec/iter\n",
      "Epoch: 73 | Batch: 010 / 011 | Total loss: 0.953 | Reg loss: 0.026 | Tree loss: 0.953 | Accuracy: 0.703072 | 0.537 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 74 | Batch: 000 / 011 | Total loss: 1.055 | Reg loss: 0.026 | Tree loss: 1.055 | Accuracy: 0.672000 | 0.54 sec/iter\n",
      "Epoch: 74 | Batch: 001 / 011 | Total loss: 1.052 | Reg loss: 0.026 | Tree loss: 1.052 | Accuracy: 0.679500 | 0.54 sec/iter\n",
      "Epoch: 74 | Batch: 002 / 011 | Total loss: 1.042 | Reg loss: 0.026 | Tree loss: 1.042 | Accuracy: 0.653000 | 0.539 sec/iter\n",
      "Epoch: 74 | Batch: 003 / 011 | Total loss: 1.030 | Reg loss: 0.026 | Tree loss: 1.030 | Accuracy: 0.634500 | 0.539 sec/iter\n",
      "Epoch: 74 | Batch: 004 / 011 | Total loss: 1.045 | Reg loss: 0.026 | Tree loss: 1.045 | Accuracy: 0.612500 | 0.539 sec/iter\n",
      "Epoch: 74 | Batch: 005 / 011 | Total loss: 1.017 | Reg loss: 0.026 | Tree loss: 1.017 | Accuracy: 0.613500 | 0.538 sec/iter\n",
      "Epoch: 74 | Batch: 006 / 011 | Total loss: 0.983 | Reg loss: 0.026 | Tree loss: 0.983 | Accuracy: 0.630500 | 0.538 sec/iter\n",
      "Epoch: 74 | Batch: 007 / 011 | Total loss: 0.994 | Reg loss: 0.026 | Tree loss: 0.994 | Accuracy: 0.639000 | 0.537 sec/iter\n",
      "Epoch: 74 | Batch: 008 / 011 | Total loss: 1.007 | Reg loss: 0.026 | Tree loss: 1.007 | Accuracy: 0.632000 | 0.537 sec/iter\n",
      "Epoch: 74 | Batch: 009 / 011 | Total loss: 1.003 | Reg loss: 0.026 | Tree loss: 1.003 | Accuracy: 0.629500 | 0.537 sec/iter\n",
      "Epoch: 74 | Batch: 010 / 011 | Total loss: 1.001 | Reg loss: 0.026 | Tree loss: 1.001 | Accuracy: 0.655290 | 0.536 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 75 | Batch: 000 / 011 | Total loss: 1.079 | Reg loss: 0.026 | Tree loss: 1.079 | Accuracy: 0.648500 | 0.537 sec/iter\n",
      "Epoch: 75 | Batch: 001 / 011 | Total loss: 1.071 | Reg loss: 0.026 | Tree loss: 1.071 | Accuracy: 0.641000 | 0.536 sec/iter\n",
      "Epoch: 75 | Batch: 002 / 011 | Total loss: 1.043 | Reg loss: 0.026 | Tree loss: 1.043 | Accuracy: 0.647000 | 0.536 sec/iter\n",
      "Epoch: 75 | Batch: 003 / 011 | Total loss: 1.008 | Reg loss: 0.026 | Tree loss: 1.008 | Accuracy: 0.652000 | 0.535 sec/iter\n",
      "Epoch: 75 | Batch: 004 / 011 | Total loss: 0.982 | Reg loss: 0.026 | Tree loss: 0.982 | Accuracy: 0.665500 | 0.535 sec/iter\n",
      "Epoch: 75 | Batch: 005 / 011 | Total loss: 1.002 | Reg loss: 0.026 | Tree loss: 1.002 | Accuracy: 0.641000 | 0.535 sec/iter\n",
      "Epoch: 75 | Batch: 006 / 011 | Total loss: 1.000 | Reg loss: 0.026 | Tree loss: 1.000 | Accuracy: 0.626500 | 0.534 sec/iter\n",
      "Epoch: 75 | Batch: 007 / 011 | Total loss: 1.009 | Reg loss: 0.026 | Tree loss: 1.009 | Accuracy: 0.617000 | 0.534 sec/iter\n",
      "Epoch: 75 | Batch: 008 / 011 | Total loss: 0.995 | Reg loss: 0.026 | Tree loss: 0.995 | Accuracy: 0.635500 | 0.534 sec/iter\n",
      "Epoch: 75 | Batch: 009 / 011 | Total loss: 0.997 | Reg loss: 0.026 | Tree loss: 0.997 | Accuracy: 0.639500 | 0.533 sec/iter\n",
      "Epoch: 75 | Batch: 010 / 011 | Total loss: 0.997 | Reg loss: 0.026 | Tree loss: 0.997 | Accuracy: 0.645051 | 0.533 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 76 | Batch: 000 / 011 | Total loss: 1.067 | Reg loss: 0.026 | Tree loss: 1.067 | Accuracy: 0.647500 | 0.538 sec/iter\n",
      "Epoch: 76 | Batch: 001 / 011 | Total loss: 1.077 | Reg loss: 0.026 | Tree loss: 1.077 | Accuracy: 0.636000 | 0.537 sec/iter\n",
      "Epoch: 76 | Batch: 002 / 011 | Total loss: 1.039 | Reg loss: 0.026 | Tree loss: 1.039 | Accuracy: 0.664000 | 0.537 sec/iter\n",
      "Epoch: 76 | Batch: 003 / 011 | Total loss: 1.028 | Reg loss: 0.026 | Tree loss: 1.028 | Accuracy: 0.632000 | 0.537 sec/iter\n",
      "Epoch: 76 | Batch: 004 / 011 | Total loss: 1.009 | Reg loss: 0.026 | Tree loss: 1.009 | Accuracy: 0.636500 | 0.536 sec/iter\n",
      "Epoch: 76 | Batch: 005 / 011 | Total loss: 0.998 | Reg loss: 0.026 | Tree loss: 0.998 | Accuracy: 0.627000 | 0.536 sec/iter\n",
      "Epoch: 76 | Batch: 006 / 011 | Total loss: 0.977 | Reg loss: 0.026 | Tree loss: 0.977 | Accuracy: 0.653500 | 0.536 sec/iter\n",
      "Epoch: 76 | Batch: 007 / 011 | Total loss: 1.005 | Reg loss: 0.026 | Tree loss: 1.005 | Accuracy: 0.624000 | 0.535 sec/iter\n",
      "Epoch: 76 | Batch: 008 / 011 | Total loss: 0.975 | Reg loss: 0.026 | Tree loss: 0.975 | Accuracy: 0.645500 | 0.535 sec/iter\n",
      "Epoch: 76 | Batch: 009 / 011 | Total loss: 0.987 | Reg loss: 0.026 | Tree loss: 0.987 | Accuracy: 0.638000 | 0.534 sec/iter\n",
      "Epoch: 76 | Batch: 010 / 011 | Total loss: 0.958 | Reg loss: 0.026 | Tree loss: 0.958 | Accuracy: 0.645051 | 0.534 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 77 | Batch: 000 / 011 | Total loss: 1.046 | Reg loss: 0.026 | Tree loss: 1.046 | Accuracy: 0.662000 | 0.538 sec/iter\n",
      "Epoch: 77 | Batch: 001 / 011 | Total loss: 1.066 | Reg loss: 0.026 | Tree loss: 1.066 | Accuracy: 0.653000 | 0.538 sec/iter\n",
      "Epoch: 77 | Batch: 002 / 011 | Total loss: 1.013 | Reg loss: 0.026 | Tree loss: 1.013 | Accuracy: 0.683000 | 0.537 sec/iter\n",
      "Epoch: 77 | Batch: 003 / 011 | Total loss: 1.017 | Reg loss: 0.026 | Tree loss: 1.017 | Accuracy: 0.657000 | 0.537 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 77 | Batch: 004 / 011 | Total loss: 1.000 | Reg loss: 0.026 | Tree loss: 1.000 | Accuracy: 0.633000 | 0.537 sec/iter\n",
      "Epoch: 77 | Batch: 005 / 011 | Total loss: 1.006 | Reg loss: 0.026 | Tree loss: 1.006 | Accuracy: 0.626000 | 0.536 sec/iter\n",
      "Epoch: 77 | Batch: 006 / 011 | Total loss: 1.012 | Reg loss: 0.026 | Tree loss: 1.012 | Accuracy: 0.637000 | 0.536 sec/iter\n",
      "Epoch: 77 | Batch: 007 / 011 | Total loss: 0.994 | Reg loss: 0.026 | Tree loss: 0.994 | Accuracy: 0.630000 | 0.536 sec/iter\n",
      "Epoch: 77 | Batch: 008 / 011 | Total loss: 0.981 | Reg loss: 0.026 | Tree loss: 0.981 | Accuracy: 0.632500 | 0.535 sec/iter\n",
      "Epoch: 77 | Batch: 009 / 011 | Total loss: 0.973 | Reg loss: 0.026 | Tree loss: 0.973 | Accuracy: 0.621000 | 0.535 sec/iter\n",
      "Epoch: 77 | Batch: 010 / 011 | Total loss: 1.045 | Reg loss: 0.026 | Tree loss: 1.045 | Accuracy: 0.631399 | 0.534 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 78 | Batch: 000 / 011 | Total loss: 1.052 | Reg loss: 0.026 | Tree loss: 1.052 | Accuracy: 0.639000 | 0.535 sec/iter\n",
      "Epoch: 78 | Batch: 001 / 011 | Total loss: 1.039 | Reg loss: 0.026 | Tree loss: 1.039 | Accuracy: 0.653000 | 0.535 sec/iter\n",
      "Epoch: 78 | Batch: 002 / 011 | Total loss: 1.025 | Reg loss: 0.026 | Tree loss: 1.025 | Accuracy: 0.649000 | 0.534 sec/iter\n",
      "Epoch: 78 | Batch: 003 / 011 | Total loss: 1.016 | Reg loss: 0.026 | Tree loss: 1.016 | Accuracy: 0.641000 | 0.534 sec/iter\n",
      "Epoch: 78 | Batch: 004 / 011 | Total loss: 0.994 | Reg loss: 0.026 | Tree loss: 0.994 | Accuracy: 0.637500 | 0.533 sec/iter\n",
      "Epoch: 78 | Batch: 005 / 011 | Total loss: 0.999 | Reg loss: 0.026 | Tree loss: 0.999 | Accuracy: 0.623500 | 0.533 sec/iter\n",
      "Epoch: 78 | Batch: 006 / 011 | Total loss: 0.995 | Reg loss: 0.026 | Tree loss: 0.995 | Accuracy: 0.626000 | 0.533 sec/iter\n",
      "Epoch: 78 | Batch: 007 / 011 | Total loss: 0.979 | Reg loss: 0.026 | Tree loss: 0.979 | Accuracy: 0.644500 | 0.532 sec/iter\n",
      "Epoch: 78 | Batch: 008 / 011 | Total loss: 0.995 | Reg loss: 0.026 | Tree loss: 0.995 | Accuracy: 0.635000 | 0.532 sec/iter\n",
      "Epoch: 78 | Batch: 009 / 011 | Total loss: 0.972 | Reg loss: 0.026 | Tree loss: 0.972 | Accuracy: 0.640000 | 0.532 sec/iter\n",
      "Epoch: 78 | Batch: 010 / 011 | Total loss: 1.013 | Reg loss: 0.026 | Tree loss: 1.013 | Accuracy: 0.627986 | 0.531 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 79 | Batch: 000 / 011 | Total loss: 1.062 | Reg loss: 0.026 | Tree loss: 1.062 | Accuracy: 0.649000 | 0.535 sec/iter\n",
      "Epoch: 79 | Batch: 001 / 011 | Total loss: 1.074 | Reg loss: 0.026 | Tree loss: 1.074 | Accuracy: 0.636500 | 0.535 sec/iter\n",
      "Epoch: 79 | Batch: 002 / 011 | Total loss: 1.025 | Reg loss: 0.026 | Tree loss: 1.025 | Accuracy: 0.648500 | 0.534 sec/iter\n",
      "Epoch: 79 | Batch: 003 / 011 | Total loss: 1.004 | Reg loss: 0.026 | Tree loss: 1.004 | Accuracy: 0.653000 | 0.534 sec/iter\n",
      "Epoch: 79 | Batch: 004 / 011 | Total loss: 0.989 | Reg loss: 0.026 | Tree loss: 0.989 | Accuracy: 0.649000 | 0.534 sec/iter\n",
      "Epoch: 79 | Batch: 005 / 011 | Total loss: 0.968 | Reg loss: 0.026 | Tree loss: 0.968 | Accuracy: 0.653000 | 0.533 sec/iter\n",
      "Epoch: 79 | Batch: 006 / 011 | Total loss: 0.969 | Reg loss: 0.026 | Tree loss: 0.969 | Accuracy: 0.643500 | 0.533 sec/iter\n",
      "Epoch: 79 | Batch: 007 / 011 | Total loss: 0.976 | Reg loss: 0.026 | Tree loss: 0.976 | Accuracy: 0.640000 | 0.533 sec/iter\n",
      "Epoch: 79 | Batch: 008 / 011 | Total loss: 0.991 | Reg loss: 0.026 | Tree loss: 0.991 | Accuracy: 0.615000 | 0.532 sec/iter\n",
      "Epoch: 79 | Batch: 009 / 011 | Total loss: 0.989 | Reg loss: 0.026 | Tree loss: 0.989 | Accuracy: 0.628000 | 0.532 sec/iter\n",
      "Epoch: 79 | Batch: 010 / 011 | Total loss: 0.951 | Reg loss: 0.026 | Tree loss: 0.951 | Accuracy: 0.614334 | 0.531 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 80 | Batch: 000 / 011 | Total loss: 1.051 | Reg loss: 0.026 | Tree loss: 1.051 | Accuracy: 0.641500 | 0.534 sec/iter\n",
      "Epoch: 80 | Batch: 001 / 011 | Total loss: 1.037 | Reg loss: 0.026 | Tree loss: 1.037 | Accuracy: 0.640000 | 0.533 sec/iter\n",
      "Epoch: 80 | Batch: 002 / 011 | Total loss: 1.022 | Reg loss: 0.026 | Tree loss: 1.022 | Accuracy: 0.637500 | 0.533 sec/iter\n",
      "Epoch: 80 | Batch: 003 / 011 | Total loss: 1.010 | Reg loss: 0.026 | Tree loss: 1.010 | Accuracy: 0.656500 | 0.533 sec/iter\n",
      "Epoch: 80 | Batch: 004 / 011 | Total loss: 0.996 | Reg loss: 0.026 | Tree loss: 0.996 | Accuracy: 0.660000 | 0.532 sec/iter\n",
      "Epoch: 80 | Batch: 005 / 011 | Total loss: 0.985 | Reg loss: 0.026 | Tree loss: 0.985 | Accuracy: 0.650500 | 0.532 sec/iter\n",
      "Epoch: 80 | Batch: 006 / 011 | Total loss: 0.987 | Reg loss: 0.026 | Tree loss: 0.987 | Accuracy: 0.620500 | 0.532 sec/iter\n",
      "Epoch: 80 | Batch: 007 / 011 | Total loss: 0.990 | Reg loss: 0.026 | Tree loss: 0.990 | Accuracy: 0.612500 | 0.531 sec/iter\n",
      "Epoch: 80 | Batch: 008 / 011 | Total loss: 0.960 | Reg loss: 0.026 | Tree loss: 0.960 | Accuracy: 0.646000 | 0.531 sec/iter\n",
      "Epoch: 80 | Batch: 009 / 011 | Total loss: 0.992 | Reg loss: 0.026 | Tree loss: 0.992 | Accuracy: 0.611500 | 0.531 sec/iter\n",
      "Epoch: 80 | Batch: 010 / 011 | Total loss: 0.964 | Reg loss: 0.026 | Tree loss: 0.964 | Accuracy: 0.638225 | 0.53 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 81 | Batch: 000 / 011 | Total loss: 1.061 | Reg loss: 0.026 | Tree loss: 1.061 | Accuracy: 0.644000 | 0.534 sec/iter\n",
      "Epoch: 81 | Batch: 001 / 011 | Total loss: 1.042 | Reg loss: 0.026 | Tree loss: 1.042 | Accuracy: 0.642500 | 0.534 sec/iter\n",
      "Epoch: 81 | Batch: 002 / 011 | Total loss: 1.031 | Reg loss: 0.026 | Tree loss: 1.031 | Accuracy: 0.615500 | 0.534 sec/iter\n",
      "Epoch: 81 | Batch: 003 / 011 | Total loss: 1.015 | Reg loss: 0.026 | Tree loss: 1.015 | Accuracy: 0.621500 | 0.533 sec/iter\n",
      "Epoch: 81 | Batch: 004 / 011 | Total loss: 0.980 | Reg loss: 0.026 | Tree loss: 0.980 | Accuracy: 0.658500 | 0.533 sec/iter\n",
      "Epoch: 81 | Batch: 005 / 011 | Total loss: 0.978 | Reg loss: 0.026 | Tree loss: 0.978 | Accuracy: 0.640500 | 0.532 sec/iter\n",
      "Epoch: 81 | Batch: 006 / 011 | Total loss: 0.989 | Reg loss: 0.026 | Tree loss: 0.989 | Accuracy: 0.639500 | 0.532 sec/iter\n",
      "Epoch: 81 | Batch: 007 / 011 | Total loss: 0.946 | Reg loss: 0.026 | Tree loss: 0.946 | Accuracy: 0.650500 | 0.532 sec/iter\n",
      "Epoch: 81 | Batch: 008 / 011 | Total loss: 0.952 | Reg loss: 0.026 | Tree loss: 0.952 | Accuracy: 0.645000 | 0.531 sec/iter\n",
      "Epoch: 81 | Batch: 009 / 011 | Total loss: 0.958 | Reg loss: 0.026 | Tree loss: 0.958 | Accuracy: 0.637000 | 0.531 sec/iter\n",
      "Epoch: 81 | Batch: 010 / 011 | Total loss: 0.994 | Reg loss: 0.026 | Tree loss: 0.994 | Accuracy: 0.631399 | 0.531 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 82 | Batch: 000 / 011 | Total loss: 1.060 | Reg loss: 0.026 | Tree loss: 1.060 | Accuracy: 0.636500 | 0.534 sec/iter\n",
      "Epoch: 82 | Batch: 001 / 011 | Total loss: 1.014 | Reg loss: 0.026 | Tree loss: 1.014 | Accuracy: 0.654500 | 0.533 sec/iter\n",
      "Epoch: 82 | Batch: 002 / 011 | Total loss: 0.993 | Reg loss: 0.026 | Tree loss: 0.993 | Accuracy: 0.660500 | 0.533 sec/iter\n",
      "Epoch: 82 | Batch: 003 / 011 | Total loss: 1.003 | Reg loss: 0.026 | Tree loss: 1.003 | Accuracy: 0.668500 | 0.533 sec/iter\n",
      "Epoch: 82 | Batch: 004 / 011 | Total loss: 0.970 | Reg loss: 0.026 | Tree loss: 0.970 | Accuracy: 0.669000 | 0.532 sec/iter\n",
      "Epoch: 82 | Batch: 005 / 011 | Total loss: 0.963 | Reg loss: 0.026 | Tree loss: 0.963 | Accuracy: 0.652000 | 0.532 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 82 | Batch: 006 / 011 | Total loss: 0.960 | Reg loss: 0.026 | Tree loss: 0.960 | Accuracy: 0.646000 | 0.532 sec/iter\n",
      "Epoch: 82 | Batch: 007 / 011 | Total loss: 0.991 | Reg loss: 0.026 | Tree loss: 0.991 | Accuracy: 0.600000 | 0.531 sec/iter\n",
      "Epoch: 82 | Batch: 008 / 011 | Total loss: 0.985 | Reg loss: 0.026 | Tree loss: 0.985 | Accuracy: 0.612500 | 0.531 sec/iter\n",
      "Epoch: 82 | Batch: 009 / 011 | Total loss: 0.990 | Reg loss: 0.026 | Tree loss: 0.990 | Accuracy: 0.609000 | 0.531 sec/iter\n",
      "Epoch: 82 | Batch: 010 / 011 | Total loss: 1.002 | Reg loss: 0.026 | Tree loss: 1.002 | Accuracy: 0.634812 | 0.53 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 83 | Batch: 000 / 011 | Total loss: 1.026 | Reg loss: 0.026 | Tree loss: 1.026 | Accuracy: 0.658000 | 0.532 sec/iter\n",
      "Epoch: 83 | Batch: 001 / 011 | Total loss: 1.027 | Reg loss: 0.026 | Tree loss: 1.027 | Accuracy: 0.661500 | 0.531 sec/iter\n",
      "Epoch: 83 | Batch: 002 / 011 | Total loss: 1.018 | Reg loss: 0.026 | Tree loss: 1.018 | Accuracy: 0.638000 | 0.531 sec/iter\n",
      "Epoch: 83 | Batch: 003 / 011 | Total loss: 1.007 | Reg loss: 0.026 | Tree loss: 1.007 | Accuracy: 0.623500 | 0.531 sec/iter\n",
      "Epoch: 83 | Batch: 004 / 011 | Total loss: 0.981 | Reg loss: 0.026 | Tree loss: 0.981 | Accuracy: 0.646000 | 0.53 sec/iter\n",
      "Epoch: 83 | Batch: 005 / 011 | Total loss: 0.993 | Reg loss: 0.026 | Tree loss: 0.993 | Accuracy: 0.638000 | 0.53 sec/iter\n",
      "Epoch: 83 | Batch: 006 / 011 | Total loss: 0.966 | Reg loss: 0.026 | Tree loss: 0.966 | Accuracy: 0.639500 | 0.53 sec/iter\n",
      "Epoch: 83 | Batch: 007 / 011 | Total loss: 0.968 | Reg loss: 0.026 | Tree loss: 0.968 | Accuracy: 0.622500 | 0.53 sec/iter\n",
      "Epoch: 83 | Batch: 008 / 011 | Total loss: 0.968 | Reg loss: 0.026 | Tree loss: 0.968 | Accuracy: 0.616500 | 0.529 sec/iter\n",
      "Epoch: 83 | Batch: 009 / 011 | Total loss: 0.955 | Reg loss: 0.026 | Tree loss: 0.955 | Accuracy: 0.647000 | 0.529 sec/iter\n",
      "Epoch: 83 | Batch: 010 / 011 | Total loss: 0.938 | Reg loss: 0.026 | Tree loss: 0.938 | Accuracy: 0.675768 | 0.529 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 84 | Batch: 000 / 011 | Total loss: 1.057 | Reg loss: 0.026 | Tree loss: 1.057 | Accuracy: 0.635500 | 0.533 sec/iter\n",
      "Epoch: 84 | Batch: 001 / 011 | Total loss: 1.027 | Reg loss: 0.026 | Tree loss: 1.027 | Accuracy: 0.632500 | 0.532 sec/iter\n",
      "Epoch: 84 | Batch: 002 / 011 | Total loss: 1.003 | Reg loss: 0.026 | Tree loss: 1.003 | Accuracy: 0.666500 | 0.532 sec/iter\n",
      "Epoch: 84 | Batch: 003 / 011 | Total loss: 0.995 | Reg loss: 0.026 | Tree loss: 0.995 | Accuracy: 0.659000 | 0.532 sec/iter\n",
      "Epoch: 84 | Batch: 004 / 011 | Total loss: 0.985 | Reg loss: 0.026 | Tree loss: 0.985 | Accuracy: 0.653500 | 0.531 sec/iter\n",
      "Epoch: 84 | Batch: 005 / 011 | Total loss: 0.958 | Reg loss: 0.026 | Tree loss: 0.958 | Accuracy: 0.651500 | 0.531 sec/iter\n",
      "Epoch: 84 | Batch: 006 / 011 | Total loss: 0.975 | Reg loss: 0.026 | Tree loss: 0.975 | Accuracy: 0.631500 | 0.531 sec/iter\n",
      "Epoch: 84 | Batch: 007 / 011 | Total loss: 0.942 | Reg loss: 0.026 | Tree loss: 0.942 | Accuracy: 0.656500 | 0.53 sec/iter\n",
      "Epoch: 84 | Batch: 008 / 011 | Total loss: 0.975 | Reg loss: 0.026 | Tree loss: 0.975 | Accuracy: 0.624500 | 0.53 sec/iter\n",
      "Epoch: 84 | Batch: 009 / 011 | Total loss: 0.948 | Reg loss: 0.026 | Tree loss: 0.948 | Accuracy: 0.639000 | 0.53 sec/iter\n",
      "Epoch: 84 | Batch: 010 / 011 | Total loss: 0.975 | Reg loss: 0.026 | Tree loss: 0.975 | Accuracy: 0.627986 | 0.53 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 85 | Batch: 000 / 011 | Total loss: 1.049 | Reg loss: 0.026 | Tree loss: 1.049 | Accuracy: 0.634500 | 0.533 sec/iter\n",
      "Epoch: 85 | Batch: 001 / 011 | Total loss: 1.025 | Reg loss: 0.026 | Tree loss: 1.025 | Accuracy: 0.628000 | 0.532 sec/iter\n",
      "Epoch: 85 | Batch: 002 / 011 | Total loss: 1.002 | Reg loss: 0.026 | Tree loss: 1.002 | Accuracy: 0.646500 | 0.532 sec/iter\n",
      "Epoch: 85 | Batch: 003 / 011 | Total loss: 1.001 | Reg loss: 0.026 | Tree loss: 1.001 | Accuracy: 0.649000 | 0.532 sec/iter\n",
      "Epoch: 85 | Batch: 004 / 011 | Total loss: 0.972 | Reg loss: 0.026 | Tree loss: 0.972 | Accuracy: 0.656000 | 0.531 sec/iter\n",
      "Epoch: 85 | Batch: 005 / 011 | Total loss: 0.970 | Reg loss: 0.026 | Tree loss: 0.970 | Accuracy: 0.636500 | 0.531 sec/iter\n",
      "Epoch: 85 | Batch: 006 / 011 | Total loss: 0.968 | Reg loss: 0.026 | Tree loss: 0.968 | Accuracy: 0.632000 | 0.531 sec/iter\n",
      "Epoch: 85 | Batch: 007 / 011 | Total loss: 0.955 | Reg loss: 0.026 | Tree loss: 0.955 | Accuracy: 0.630500 | 0.53 sec/iter\n",
      "Epoch: 85 | Batch: 008 / 011 | Total loss: 0.936 | Reg loss: 0.026 | Tree loss: 0.936 | Accuracy: 0.653500 | 0.53 sec/iter\n",
      "Epoch: 85 | Batch: 009 / 011 | Total loss: 0.952 | Reg loss: 0.026 | Tree loss: 0.952 | Accuracy: 0.635000 | 0.53 sec/iter\n",
      "Epoch: 85 | Batch: 010 / 011 | Total loss: 0.942 | Reg loss: 0.026 | Tree loss: 0.942 | Accuracy: 0.668942 | 0.529 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 86 | Batch: 000 / 011 | Total loss: 1.046 | Reg loss: 0.026 | Tree loss: 1.046 | Accuracy: 0.630000 | 0.53 sec/iter\n",
      "Epoch: 86 | Batch: 001 / 011 | Total loss: 1.038 | Reg loss: 0.026 | Tree loss: 1.038 | Accuracy: 0.610000 | 0.529 sec/iter\n",
      "Epoch: 86 | Batch: 002 / 011 | Total loss: 0.999 | Reg loss: 0.026 | Tree loss: 0.999 | Accuracy: 0.635500 | 0.529 sec/iter\n",
      "Epoch: 86 | Batch: 003 / 011 | Total loss: 0.987 | Reg loss: 0.026 | Tree loss: 0.987 | Accuracy: 0.619000 | 0.529 sec/iter\n",
      "Epoch: 86 | Batch: 004 / 011 | Total loss: 0.981 | Reg loss: 0.026 | Tree loss: 0.981 | Accuracy: 0.630500 | 0.528 sec/iter\n",
      "Epoch: 86 | Batch: 005 / 011 | Total loss: 0.975 | Reg loss: 0.026 | Tree loss: 0.975 | Accuracy: 0.636000 | 0.528 sec/iter\n",
      "Epoch: 86 | Batch: 006 / 011 | Total loss: 0.956 | Reg loss: 0.026 | Tree loss: 0.956 | Accuracy: 0.664000 | 0.528 sec/iter\n",
      "Epoch: 86 | Batch: 007 / 011 | Total loss: 0.942 | Reg loss: 0.026 | Tree loss: 0.942 | Accuracy: 0.643000 | 0.527 sec/iter\n",
      "Epoch: 86 | Batch: 008 / 011 | Total loss: 0.958 | Reg loss: 0.026 | Tree loss: 0.958 | Accuracy: 0.633000 | 0.527 sec/iter\n",
      "Epoch: 86 | Batch: 009 / 011 | Total loss: 0.949 | Reg loss: 0.026 | Tree loss: 0.949 | Accuracy: 0.646500 | 0.527 sec/iter\n",
      "Epoch: 86 | Batch: 010 / 011 | Total loss: 0.958 | Reg loss: 0.026 | Tree loss: 0.958 | Accuracy: 0.566553 | 0.527 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 87 | Batch: 000 / 011 | Total loss: 1.038 | Reg loss: 0.026 | Tree loss: 1.038 | Accuracy: 0.644500 | 0.53 sec/iter\n",
      "Epoch: 87 | Batch: 001 / 011 | Total loss: 1.013 | Reg loss: 0.026 | Tree loss: 1.013 | Accuracy: 0.635000 | 0.53 sec/iter\n",
      "Epoch: 87 | Batch: 002 / 011 | Total loss: 1.004 | Reg loss: 0.026 | Tree loss: 1.004 | Accuracy: 0.630500 | 0.529 sec/iter\n",
      "Epoch: 87 | Batch: 003 / 011 | Total loss: 1.014 | Reg loss: 0.026 | Tree loss: 1.014 | Accuracy: 0.626000 | 0.529 sec/iter\n",
      "Epoch: 87 | Batch: 004 / 011 | Total loss: 0.973 | Reg loss: 0.026 | Tree loss: 0.973 | Accuracy: 0.646500 | 0.529 sec/iter\n",
      "Epoch: 87 | Batch: 005 / 011 | Total loss: 0.977 | Reg loss: 0.026 | Tree loss: 0.977 | Accuracy: 0.635500 | 0.528 sec/iter\n",
      "Epoch: 87 | Batch: 006 / 011 | Total loss: 0.941 | Reg loss: 0.026 | Tree loss: 0.941 | Accuracy: 0.653000 | 0.528 sec/iter\n",
      "Epoch: 87 | Batch: 007 / 011 | Total loss: 0.945 | Reg loss: 0.026 | Tree loss: 0.945 | Accuracy: 0.649000 | 0.528 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 87 | Batch: 008 / 011 | Total loss: 0.940 | Reg loss: 0.026 | Tree loss: 0.940 | Accuracy: 0.638000 | 0.528 sec/iter\n",
      "Epoch: 87 | Batch: 009 / 011 | Total loss: 0.954 | Reg loss: 0.026 | Tree loss: 0.954 | Accuracy: 0.636500 | 0.527 sec/iter\n",
      "Epoch: 87 | Batch: 010 / 011 | Total loss: 0.941 | Reg loss: 0.026 | Tree loss: 0.941 | Accuracy: 0.651877 | 0.527 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 88 | Batch: 000 / 011 | Total loss: 1.027 | Reg loss: 0.026 | Tree loss: 1.027 | Accuracy: 0.631000 | 0.531 sec/iter\n",
      "Epoch: 88 | Batch: 001 / 011 | Total loss: 1.019 | Reg loss: 0.026 | Tree loss: 1.019 | Accuracy: 0.653000 | 0.531 sec/iter\n",
      "Epoch: 88 | Batch: 002 / 011 | Total loss: 1.008 | Reg loss: 0.026 | Tree loss: 1.008 | Accuracy: 0.650000 | 0.531 sec/iter\n",
      "Epoch: 88 | Batch: 003 / 011 | Total loss: 0.986 | Reg loss: 0.026 | Tree loss: 0.986 | Accuracy: 0.648000 | 0.531 sec/iter\n",
      "Epoch: 88 | Batch: 004 / 011 | Total loss: 0.980 | Reg loss: 0.026 | Tree loss: 0.980 | Accuracy: 0.637500 | 0.53 sec/iter\n",
      "Epoch: 88 | Batch: 005 / 011 | Total loss: 0.979 | Reg loss: 0.026 | Tree loss: 0.979 | Accuracy: 0.629000 | 0.53 sec/iter\n",
      "Epoch: 88 | Batch: 006 / 011 | Total loss: 0.932 | Reg loss: 0.026 | Tree loss: 0.932 | Accuracy: 0.680000 | 0.53 sec/iter\n",
      "Epoch: 88 | Batch: 007 / 011 | Total loss: 0.939 | Reg loss: 0.026 | Tree loss: 0.939 | Accuracy: 0.644000 | 0.53 sec/iter\n",
      "Epoch: 88 | Batch: 008 / 011 | Total loss: 0.945 | Reg loss: 0.026 | Tree loss: 0.945 | Accuracy: 0.644000 | 0.529 sec/iter\n",
      "Epoch: 88 | Batch: 009 / 011 | Total loss: 0.929 | Reg loss: 0.026 | Tree loss: 0.929 | Accuracy: 0.649500 | 0.529 sec/iter\n",
      "Epoch: 88 | Batch: 010 / 011 | Total loss: 0.940 | Reg loss: 0.026 | Tree loss: 0.940 | Accuracy: 0.645051 | 0.529 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 89 | Batch: 000 / 011 | Total loss: 1.024 | Reg loss: 0.026 | Tree loss: 1.024 | Accuracy: 0.627000 | 0.533 sec/iter\n",
      "Epoch: 89 | Batch: 001 / 011 | Total loss: 1.022 | Reg loss: 0.026 | Tree loss: 1.022 | Accuracy: 0.616500 | 0.533 sec/iter\n",
      "Epoch: 89 | Batch: 002 / 011 | Total loss: 0.992 | Reg loss: 0.026 | Tree loss: 0.992 | Accuracy: 0.653000 | 0.532 sec/iter\n",
      "Epoch: 89 | Batch: 003 / 011 | Total loss: 0.962 | Reg loss: 0.026 | Tree loss: 0.962 | Accuracy: 0.644000 | 0.532 sec/iter\n",
      "Epoch: 89 | Batch: 004 / 011 | Total loss: 0.971 | Reg loss: 0.026 | Tree loss: 0.971 | Accuracy: 0.643500 | 0.532 sec/iter\n",
      "Epoch: 89 | Batch: 005 / 011 | Total loss: 0.948 | Reg loss: 0.026 | Tree loss: 0.948 | Accuracy: 0.657000 | 0.532 sec/iter\n",
      "Epoch: 89 | Batch: 006 / 011 | Total loss: 0.950 | Reg loss: 0.026 | Tree loss: 0.950 | Accuracy: 0.632500 | 0.531 sec/iter\n",
      "Epoch: 89 | Batch: 007 / 011 | Total loss: 0.962 | Reg loss: 0.026 | Tree loss: 0.962 | Accuracy: 0.632500 | 0.531 sec/iter\n",
      "Epoch: 89 | Batch: 008 / 011 | Total loss: 0.955 | Reg loss: 0.026 | Tree loss: 0.955 | Accuracy: 0.642500 | 0.531 sec/iter\n",
      "Epoch: 89 | Batch: 009 / 011 | Total loss: 0.952 | Reg loss: 0.026 | Tree loss: 0.952 | Accuracy: 0.627500 | 0.53 sec/iter\n",
      "Epoch: 89 | Batch: 010 / 011 | Total loss: 0.921 | Reg loss: 0.026 | Tree loss: 0.921 | Accuracy: 0.648464 | 0.53 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 90 | Batch: 000 / 011 | Total loss: 1.034 | Reg loss: 0.026 | Tree loss: 1.034 | Accuracy: 0.604500 | 0.534 sec/iter\n",
      "Epoch: 90 | Batch: 001 / 011 | Total loss: 1.017 | Reg loss: 0.026 | Tree loss: 1.017 | Accuracy: 0.629000 | 0.534 sec/iter\n",
      "Epoch: 90 | Batch: 002 / 011 | Total loss: 1.001 | Reg loss: 0.026 | Tree loss: 1.001 | Accuracy: 0.638500 | 0.534 sec/iter\n",
      "Epoch: 90 | Batch: 003 / 011 | Total loss: 0.965 | Reg loss: 0.026 | Tree loss: 0.965 | Accuracy: 0.650500 | 0.533 sec/iter\n",
      "Epoch: 90 | Batch: 004 / 011 | Total loss: 0.963 | Reg loss: 0.026 | Tree loss: 0.963 | Accuracy: 0.644500 | 0.533 sec/iter\n",
      "Epoch: 90 | Batch: 005 / 011 | Total loss: 0.941 | Reg loss: 0.026 | Tree loss: 0.941 | Accuracy: 0.659500 | 0.533 sec/iter\n",
      "Epoch: 90 | Batch: 006 / 011 | Total loss: 0.955 | Reg loss: 0.026 | Tree loss: 0.955 | Accuracy: 0.633000 | 0.533 sec/iter\n",
      "Epoch: 90 | Batch: 007 / 011 | Total loss: 0.952 | Reg loss: 0.026 | Tree loss: 0.952 | Accuracy: 0.631500 | 0.532 sec/iter\n",
      "Epoch: 90 | Batch: 008 / 011 | Total loss: 0.947 | Reg loss: 0.026 | Tree loss: 0.947 | Accuracy: 0.631500 | 0.532 sec/iter\n",
      "Epoch: 90 | Batch: 009 / 011 | Total loss: 0.929 | Reg loss: 0.026 | Tree loss: 0.929 | Accuracy: 0.642500 | 0.532 sec/iter\n",
      "Epoch: 90 | Batch: 010 / 011 | Total loss: 0.972 | Reg loss: 0.026 | Tree loss: 0.972 | Accuracy: 0.634812 | 0.531 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 91 | Batch: 000 / 011 | Total loss: 1.015 | Reg loss: 0.026 | Tree loss: 1.015 | Accuracy: 0.635500 | 0.536 sec/iter\n",
      "Epoch: 91 | Batch: 001 / 011 | Total loss: 1.031 | Reg loss: 0.026 | Tree loss: 1.031 | Accuracy: 0.614500 | 0.535 sec/iter\n",
      "Epoch: 91 | Batch: 002 / 011 | Total loss: 1.002 | Reg loss: 0.026 | Tree loss: 1.002 | Accuracy: 0.648000 | 0.535 sec/iter\n",
      "Epoch: 91 | Batch: 003 / 011 | Total loss: 0.956 | Reg loss: 0.026 | Tree loss: 0.956 | Accuracy: 0.679000 | 0.535 sec/iter\n",
      "Epoch: 91 | Batch: 004 / 011 | Total loss: 0.942 | Reg loss: 0.026 | Tree loss: 0.942 | Accuracy: 0.669500 | 0.535 sec/iter\n",
      "Epoch: 91 | Batch: 005 / 011 | Total loss: 0.944 | Reg loss: 0.026 | Tree loss: 0.944 | Accuracy: 0.662000 | 0.534 sec/iter\n",
      "Epoch: 91 | Batch: 006 / 011 | Total loss: 0.964 | Reg loss: 0.026 | Tree loss: 0.964 | Accuracy: 0.626000 | 0.534 sec/iter\n",
      "Epoch: 91 | Batch: 007 / 011 | Total loss: 0.938 | Reg loss: 0.026 | Tree loss: 0.938 | Accuracy: 0.629000 | 0.534 sec/iter\n",
      "Epoch: 91 | Batch: 008 / 011 | Total loss: 0.948 | Reg loss: 0.026 | Tree loss: 0.948 | Accuracy: 0.645000 | 0.534 sec/iter\n",
      "Epoch: 91 | Batch: 009 / 011 | Total loss: 0.946 | Reg loss: 0.026 | Tree loss: 0.946 | Accuracy: 0.636000 | 0.533 sec/iter\n",
      "Epoch: 91 | Batch: 010 / 011 | Total loss: 0.982 | Reg loss: 0.026 | Tree loss: 0.982 | Accuracy: 0.621160 | 0.533 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 92 | Batch: 000 / 011 | Total loss: 1.024 | Reg loss: 0.026 | Tree loss: 1.024 | Accuracy: 0.617500 | 0.539 sec/iter\n",
      "Epoch: 92 | Batch: 001 / 011 | Total loss: 1.021 | Reg loss: 0.026 | Tree loss: 1.021 | Accuracy: 0.623500 | 0.539 sec/iter\n",
      "Epoch: 92 | Batch: 002 / 011 | Total loss: 1.011 | Reg loss: 0.026 | Tree loss: 1.011 | Accuracy: 0.631500 | 0.538 sec/iter\n",
      "Epoch: 92 | Batch: 003 / 011 | Total loss: 0.976 | Reg loss: 0.026 | Tree loss: 0.976 | Accuracy: 0.645500 | 0.538 sec/iter\n",
      "Epoch: 92 | Batch: 004 / 011 | Total loss: 0.946 | Reg loss: 0.026 | Tree loss: 0.946 | Accuracy: 0.667500 | 0.538 sec/iter\n",
      "Epoch: 92 | Batch: 005 / 011 | Total loss: 0.930 | Reg loss: 0.026 | Tree loss: 0.930 | Accuracy: 0.669500 | 0.538 sec/iter\n",
      "Epoch: 92 | Batch: 006 / 011 | Total loss: 0.928 | Reg loss: 0.026 | Tree loss: 0.928 | Accuracy: 0.647500 | 0.537 sec/iter\n",
      "Epoch: 92 | Batch: 007 / 011 | Total loss: 0.950 | Reg loss: 0.026 | Tree loss: 0.950 | Accuracy: 0.636000 | 0.537 sec/iter\n",
      "Epoch: 92 | Batch: 008 / 011 | Total loss: 0.933 | Reg loss: 0.026 | Tree loss: 0.933 | Accuracy: 0.641000 | 0.537 sec/iter\n",
      "Epoch: 92 | Batch: 009 / 011 | Total loss: 0.963 | Reg loss: 0.026 | Tree loss: 0.963 | Accuracy: 0.629000 | 0.537 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 92 | Batch: 010 / 011 | Total loss: 0.895 | Reg loss: 0.026 | Tree loss: 0.895 | Accuracy: 0.655290 | 0.536 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 93 | Batch: 000 / 011 | Total loss: 1.043 | Reg loss: 0.026 | Tree loss: 1.043 | Accuracy: 0.596000 | 0.541 sec/iter\n",
      "Epoch: 93 | Batch: 001 / 011 | Total loss: 0.999 | Reg loss: 0.026 | Tree loss: 0.999 | Accuracy: 0.641500 | 0.541 sec/iter\n",
      "Epoch: 93 | Batch: 002 / 011 | Total loss: 0.968 | Reg loss: 0.026 | Tree loss: 0.968 | Accuracy: 0.655000 | 0.54 sec/iter\n",
      "Epoch: 93 | Batch: 003 / 011 | Total loss: 0.968 | Reg loss: 0.026 | Tree loss: 0.968 | Accuracy: 0.623500 | 0.54 sec/iter\n",
      "Epoch: 93 | Batch: 004 / 011 | Total loss: 0.928 | Reg loss: 0.026 | Tree loss: 0.928 | Accuracy: 0.676000 | 0.54 sec/iter\n",
      "Epoch: 93 | Batch: 005 / 011 | Total loss: 0.949 | Reg loss: 0.026 | Tree loss: 0.949 | Accuracy: 0.658500 | 0.539 sec/iter\n",
      "Epoch: 93 | Batch: 006 / 011 | Total loss: 0.934 | Reg loss: 0.026 | Tree loss: 0.934 | Accuracy: 0.649000 | 0.539 sec/iter\n",
      "Epoch: 93 | Batch: 007 / 011 | Total loss: 0.954 | Reg loss: 0.026 | Tree loss: 0.954 | Accuracy: 0.633500 | 0.539 sec/iter\n",
      "Epoch: 93 | Batch: 008 / 011 | Total loss: 0.972 | Reg loss: 0.026 | Tree loss: 0.972 | Accuracy: 0.606500 | 0.539 sec/iter\n",
      "Epoch: 93 | Batch: 009 / 011 | Total loss: 0.937 | Reg loss: 0.026 | Tree loss: 0.937 | Accuracy: 0.643000 | 0.538 sec/iter\n",
      "Epoch: 93 | Batch: 010 / 011 | Total loss: 0.972 | Reg loss: 0.026 | Tree loss: 0.972 | Accuracy: 0.624573 | 0.538 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 94 | Batch: 000 / 011 | Total loss: 1.024 | Reg loss: 0.026 | Tree loss: 1.024 | Accuracy: 0.629500 | 0.542 sec/iter\n",
      "Epoch: 94 | Batch: 001 / 011 | Total loss: 1.018 | Reg loss: 0.026 | Tree loss: 1.018 | Accuracy: 0.613000 | 0.542 sec/iter\n",
      "Epoch: 94 | Batch: 002 / 011 | Total loss: 0.982 | Reg loss: 0.026 | Tree loss: 0.982 | Accuracy: 0.646000 | 0.541 sec/iter\n",
      "Epoch: 94 | Batch: 003 / 011 | Total loss: 0.964 | Reg loss: 0.026 | Tree loss: 0.964 | Accuracy: 0.657500 | 0.541 sec/iter\n",
      "Epoch: 94 | Batch: 004 / 011 | Total loss: 0.944 | Reg loss: 0.026 | Tree loss: 0.944 | Accuracy: 0.660000 | 0.541 sec/iter\n",
      "Epoch: 94 | Batch: 005 / 011 | Total loss: 0.932 | Reg loss: 0.026 | Tree loss: 0.932 | Accuracy: 0.673500 | 0.541 sec/iter\n",
      "Epoch: 94 | Batch: 006 / 011 | Total loss: 0.969 | Reg loss: 0.026 | Tree loss: 0.969 | Accuracy: 0.626500 | 0.54 sec/iter\n",
      "Epoch: 94 | Batch: 007 / 011 | Total loss: 0.921 | Reg loss: 0.026 | Tree loss: 0.921 | Accuracy: 0.644000 | 0.54 sec/iter\n",
      "Epoch: 94 | Batch: 008 / 011 | Total loss: 0.943 | Reg loss: 0.026 | Tree loss: 0.943 | Accuracy: 0.635000 | 0.54 sec/iter\n",
      "Epoch: 94 | Batch: 009 / 011 | Total loss: 0.908 | Reg loss: 0.026 | Tree loss: 0.908 | Accuracy: 0.653500 | 0.539 sec/iter\n",
      "Epoch: 94 | Batch: 010 / 011 | Total loss: 0.898 | Reg loss: 0.026 | Tree loss: 0.898 | Accuracy: 0.706485 | 0.539 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 95 | Batch: 000 / 011 | Total loss: 1.029 | Reg loss: 0.026 | Tree loss: 1.029 | Accuracy: 0.605500 | 0.54 sec/iter\n",
      "Epoch: 95 | Batch: 001 / 011 | Total loss: 0.985 | Reg loss: 0.026 | Tree loss: 0.985 | Accuracy: 0.620500 | 0.54 sec/iter\n",
      "Epoch: 95 | Batch: 002 / 011 | Total loss: 0.994 | Reg loss: 0.026 | Tree loss: 0.994 | Accuracy: 0.633500 | 0.539 sec/iter\n",
      "Epoch: 95 | Batch: 003 / 011 | Total loss: 0.980 | Reg loss: 0.026 | Tree loss: 0.980 | Accuracy: 0.632000 | 0.539 sec/iter\n",
      "Epoch: 95 | Batch: 004 / 011 | Total loss: 0.948 | Reg loss: 0.026 | Tree loss: 0.948 | Accuracy: 0.655500 | 0.539 sec/iter\n",
      "Epoch: 95 | Batch: 005 / 011 | Total loss: 0.933 | Reg loss: 0.026 | Tree loss: 0.933 | Accuracy: 0.653000 | 0.539 sec/iter\n",
      "Epoch: 95 | Batch: 006 / 011 | Total loss: 0.950 | Reg loss: 0.026 | Tree loss: 0.950 | Accuracy: 0.642000 | 0.538 sec/iter\n",
      "Epoch: 95 | Batch: 007 / 011 | Total loss: 0.926 | Reg loss: 0.026 | Tree loss: 0.926 | Accuracy: 0.644500 | 0.538 sec/iter\n",
      "Epoch: 95 | Batch: 008 / 011 | Total loss: 0.924 | Reg loss: 0.026 | Tree loss: 0.924 | Accuracy: 0.657000 | 0.538 sec/iter\n",
      "Epoch: 95 | Batch: 009 / 011 | Total loss: 0.928 | Reg loss: 0.026 | Tree loss: 0.928 | Accuracy: 0.649500 | 0.537 sec/iter\n",
      "Epoch: 95 | Batch: 010 / 011 | Total loss: 0.909 | Reg loss: 0.026 | Tree loss: 0.909 | Accuracy: 0.668942 | 0.537 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 96 | Batch: 000 / 011 | Total loss: 1.005 | Reg loss: 0.026 | Tree loss: 1.005 | Accuracy: 0.638500 | 0.541 sec/iter\n",
      "Epoch: 96 | Batch: 001 / 011 | Total loss: 0.998 | Reg loss: 0.026 | Tree loss: 0.998 | Accuracy: 0.622500 | 0.54 sec/iter\n",
      "Epoch: 96 | Batch: 002 / 011 | Total loss: 0.986 | Reg loss: 0.026 | Tree loss: 0.986 | Accuracy: 0.649500 | 0.54 sec/iter\n",
      "Epoch: 96 | Batch: 003 / 011 | Total loss: 0.983 | Reg loss: 0.026 | Tree loss: 0.983 | Accuracy: 0.627000 | 0.54 sec/iter\n",
      "Epoch: 96 | Batch: 004 / 011 | Total loss: 0.947 | Reg loss: 0.026 | Tree loss: 0.947 | Accuracy: 0.655000 | 0.54 sec/iter\n",
      "Epoch: 96 | Batch: 005 / 011 | Total loss: 0.938 | Reg loss: 0.026 | Tree loss: 0.938 | Accuracy: 0.655000 | 0.539 sec/iter\n",
      "Epoch: 96 | Batch: 006 / 011 | Total loss: 0.898 | Reg loss: 0.026 | Tree loss: 0.898 | Accuracy: 0.681500 | 0.539 sec/iter\n",
      "Epoch: 96 | Batch: 007 / 011 | Total loss: 0.934 | Reg loss: 0.026 | Tree loss: 0.934 | Accuracy: 0.642000 | 0.539 sec/iter\n",
      "Epoch: 96 | Batch: 008 / 011 | Total loss: 0.932 | Reg loss: 0.026 | Tree loss: 0.932 | Accuracy: 0.629500 | 0.539 sec/iter\n",
      "Epoch: 96 | Batch: 009 / 011 | Total loss: 0.946 | Reg loss: 0.026 | Tree loss: 0.946 | Accuracy: 0.631000 | 0.538 sec/iter\n",
      "Epoch: 96 | Batch: 010 / 011 | Total loss: 0.899 | Reg loss: 0.026 | Tree loss: 0.899 | Accuracy: 0.672355 | 0.538 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 97 | Batch: 000 / 011 | Total loss: 1.005 | Reg loss: 0.026 | Tree loss: 1.005 | Accuracy: 0.627500 | 0.54 sec/iter\n",
      "Epoch: 97 | Batch: 001 / 011 | Total loss: 0.991 | Reg loss: 0.026 | Tree loss: 0.991 | Accuracy: 0.628500 | 0.539 sec/iter\n",
      "Epoch: 97 | Batch: 002 / 011 | Total loss: 0.975 | Reg loss: 0.026 | Tree loss: 0.975 | Accuracy: 0.644000 | 0.539 sec/iter\n",
      "Epoch: 97 | Batch: 003 / 011 | Total loss: 0.984 | Reg loss: 0.026 | Tree loss: 0.984 | Accuracy: 0.642500 | 0.539 sec/iter\n",
      "Epoch: 97 | Batch: 004 / 011 | Total loss: 0.927 | Reg loss: 0.026 | Tree loss: 0.927 | Accuracy: 0.668500 | 0.538 sec/iter\n",
      "Epoch: 97 | Batch: 005 / 011 | Total loss: 0.916 | Reg loss: 0.026 | Tree loss: 0.916 | Accuracy: 0.674000 | 0.538 sec/iter\n",
      "Epoch: 97 | Batch: 006 / 011 | Total loss: 0.960 | Reg loss: 0.026 | Tree loss: 0.960 | Accuracy: 0.621500 | 0.538 sec/iter\n",
      "Epoch: 97 | Batch: 007 / 011 | Total loss: 0.932 | Reg loss: 0.026 | Tree loss: 0.932 | Accuracy: 0.660000 | 0.538 sec/iter\n",
      "Epoch: 97 | Batch: 008 / 011 | Total loss: 0.928 | Reg loss: 0.026 | Tree loss: 0.928 | Accuracy: 0.644500 | 0.537 sec/iter\n",
      "Epoch: 97 | Batch: 009 / 011 | Total loss: 0.928 | Reg loss: 0.026 | Tree loss: 0.928 | Accuracy: 0.657000 | 0.537 sec/iter\n",
      "Epoch: 97 | Batch: 010 / 011 | Total loss: 0.952 | Reg loss: 0.026 | Tree loss: 0.952 | Accuracy: 0.607509 | 0.537 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 98 | Batch: 000 / 011 | Total loss: 1.006 | Reg loss: 0.026 | Tree loss: 1.006 | Accuracy: 0.632500 | 0.538 sec/iter\n",
      "Epoch: 98 | Batch: 001 / 011 | Total loss: 1.001 | Reg loss: 0.026 | Tree loss: 1.001 | Accuracy: 0.609000 | 0.538 sec/iter\n",
      "Epoch: 98 | Batch: 002 / 011 | Total loss: 0.979 | Reg loss: 0.026 | Tree loss: 0.979 | Accuracy: 0.626000 | 0.538 sec/iter\n",
      "Epoch: 98 | Batch: 003 / 011 | Total loss: 0.965 | Reg loss: 0.026 | Tree loss: 0.965 | Accuracy: 0.654500 | 0.538 sec/iter\n",
      "Epoch: 98 | Batch: 004 / 011 | Total loss: 0.925 | Reg loss: 0.026 | Tree loss: 0.925 | Accuracy: 0.678000 | 0.537 sec/iter\n",
      "Epoch: 98 | Batch: 005 / 011 | Total loss: 0.906 | Reg loss: 0.026 | Tree loss: 0.906 | Accuracy: 0.680500 | 0.537 sec/iter\n",
      "Epoch: 98 | Batch: 006 / 011 | Total loss: 0.942 | Reg loss: 0.026 | Tree loss: 0.942 | Accuracy: 0.642500 | 0.537 sec/iter\n",
      "Epoch: 98 | Batch: 007 / 011 | Total loss: 0.924 | Reg loss: 0.026 | Tree loss: 0.924 | Accuracy: 0.652000 | 0.536 sec/iter\n",
      "Epoch: 98 | Batch: 008 / 011 | Total loss: 0.934 | Reg loss: 0.026 | Tree loss: 0.934 | Accuracy: 0.638000 | 0.536 sec/iter\n",
      "Epoch: 98 | Batch: 009 / 011 | Total loss: 0.940 | Reg loss: 0.026 | Tree loss: 0.940 | Accuracy: 0.621500 | 0.536 sec/iter\n",
      "Epoch: 98 | Batch: 010 / 011 | Total loss: 0.919 | Reg loss: 0.026 | Tree loss: 0.919 | Accuracy: 0.696246 | 0.535 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 99 | Batch: 000 / 011 | Total loss: 1.014 | Reg loss: 0.026 | Tree loss: 1.014 | Accuracy: 0.624000 | 0.538 sec/iter\n",
      "Epoch: 99 | Batch: 001 / 011 | Total loss: 0.994 | Reg loss: 0.026 | Tree loss: 0.994 | Accuracy: 0.618000 | 0.538 sec/iter\n",
      "Epoch: 99 | Batch: 002 / 011 | Total loss: 0.956 | Reg loss: 0.026 | Tree loss: 0.956 | Accuracy: 0.646000 | 0.538 sec/iter\n",
      "Epoch: 99 | Batch: 003 / 011 | Total loss: 0.966 | Reg loss: 0.026 | Tree loss: 0.966 | Accuracy: 0.652000 | 0.537 sec/iter\n",
      "Epoch: 99 | Batch: 004 / 011 | Total loss: 0.922 | Reg loss: 0.026 | Tree loss: 0.922 | Accuracy: 0.673500 | 0.537 sec/iter\n",
      "Epoch: 99 | Batch: 005 / 011 | Total loss: 0.924 | Reg loss: 0.026 | Tree loss: 0.924 | Accuracy: 0.655500 | 0.537 sec/iter\n",
      "Epoch: 99 | Batch: 006 / 011 | Total loss: 0.937 | Reg loss: 0.026 | Tree loss: 0.937 | Accuracy: 0.649500 | 0.537 sec/iter\n",
      "Epoch: 99 | Batch: 007 / 011 | Total loss: 0.937 | Reg loss: 0.026 | Tree loss: 0.937 | Accuracy: 0.641000 | 0.536 sec/iter\n",
      "Epoch: 99 | Batch: 008 / 011 | Total loss: 0.924 | Reg loss: 0.026 | Tree loss: 0.924 | Accuracy: 0.648500 | 0.536 sec/iter\n",
      "Epoch: 99 | Batch: 009 / 011 | Total loss: 0.925 | Reg loss: 0.026 | Tree loss: 0.925 | Accuracy: 0.638000 | 0.536 sec/iter\n",
      "Epoch: 99 | Batch: 010 / 011 | Total loss: 0.949 | Reg loss: 0.026 | Tree loss: 0.949 | Accuracy: 0.624573 | 0.535 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 100 | Batch: 000 / 011 | Total loss: 1.015 | Reg loss: 0.026 | Tree loss: 1.015 | Accuracy: 0.616000 | 0.536 sec/iter\n",
      "Epoch: 100 | Batch: 001 / 011 | Total loss: 0.999 | Reg loss: 0.026 | Tree loss: 0.999 | Accuracy: 0.615500 | 0.536 sec/iter\n",
      "Epoch: 100 | Batch: 002 / 011 | Total loss: 0.975 | Reg loss: 0.026 | Tree loss: 0.975 | Accuracy: 0.643500 | 0.536 sec/iter\n",
      "Epoch: 100 | Batch: 003 / 011 | Total loss: 0.949 | Reg loss: 0.026 | Tree loss: 0.949 | Accuracy: 0.643000 | 0.535 sec/iter\n",
      "Epoch: 100 | Batch: 004 / 011 | Total loss: 0.948 | Reg loss: 0.026 | Tree loss: 0.948 | Accuracy: 0.647500 | 0.535 sec/iter\n",
      "Epoch: 100 | Batch: 005 / 011 | Total loss: 0.912 | Reg loss: 0.026 | Tree loss: 0.912 | Accuracy: 0.677000 | 0.535 sec/iter\n",
      "Epoch: 100 | Batch: 006 / 011 | Total loss: 0.922 | Reg loss: 0.026 | Tree loss: 0.922 | Accuracy: 0.651500 | 0.535 sec/iter\n",
      "Epoch: 100 | Batch: 007 / 011 | Total loss: 0.921 | Reg loss: 0.026 | Tree loss: 0.921 | Accuracy: 0.647500 | 0.534 sec/iter\n",
      "Epoch: 100 | Batch: 008 / 011 | Total loss: 0.920 | Reg loss: 0.026 | Tree loss: 0.920 | Accuracy: 0.646000 | 0.534 sec/iter\n",
      "Epoch: 100 | Batch: 009 / 011 | Total loss: 0.919 | Reg loss: 0.026 | Tree loss: 0.919 | Accuracy: 0.651500 | 0.534 sec/iter\n",
      "Epoch: 100 | Batch: 010 / 011 | Total loss: 0.863 | Reg loss: 0.026 | Tree loss: 0.863 | Accuracy: 0.682594 | 0.534 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 101 | Batch: 000 / 011 | Total loss: 1.006 | Reg loss: 0.026 | Tree loss: 1.006 | Accuracy: 0.612500 | 0.537 sec/iter\n",
      "Epoch: 101 | Batch: 001 / 011 | Total loss: 0.994 | Reg loss: 0.026 | Tree loss: 0.994 | Accuracy: 0.610000 | 0.537 sec/iter\n",
      "Epoch: 101 | Batch: 002 / 011 | Total loss: 0.981 | Reg loss: 0.026 | Tree loss: 0.981 | Accuracy: 0.616500 | 0.536 sec/iter\n",
      "Epoch: 101 | Batch: 003 / 011 | Total loss: 0.946 | Reg loss: 0.026 | Tree loss: 0.946 | Accuracy: 0.655500 | 0.536 sec/iter\n",
      "Epoch: 101 | Batch: 004 / 011 | Total loss: 0.925 | Reg loss: 0.026 | Tree loss: 0.925 | Accuracy: 0.661000 | 0.536 sec/iter\n",
      "Epoch: 101 | Batch: 005 / 011 | Total loss: 0.927 | Reg loss: 0.026 | Tree loss: 0.927 | Accuracy: 0.666000 | 0.536 sec/iter\n",
      "Epoch: 101 | Batch: 006 / 011 | Total loss: 0.907 | Reg loss: 0.026 | Tree loss: 0.907 | Accuracy: 0.667000 | 0.535 sec/iter\n",
      "Epoch: 101 | Batch: 007 / 011 | Total loss: 0.937 | Reg loss: 0.026 | Tree loss: 0.937 | Accuracy: 0.637000 | 0.535 sec/iter\n",
      "Epoch: 101 | Batch: 008 / 011 | Total loss: 0.929 | Reg loss: 0.026 | Tree loss: 0.929 | Accuracy: 0.639500 | 0.535 sec/iter\n",
      "Epoch: 101 | Batch: 009 / 011 | Total loss: 0.924 | Reg loss: 0.026 | Tree loss: 0.924 | Accuracy: 0.637000 | 0.534 sec/iter\n",
      "Epoch: 101 | Batch: 010 / 011 | Total loss: 0.875 | Reg loss: 0.026 | Tree loss: 0.875 | Accuracy: 0.638225 | 0.534 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 102 | Batch: 000 / 011 | Total loss: 1.003 | Reg loss: 0.026 | Tree loss: 1.003 | Accuracy: 0.620000 | 0.537 sec/iter\n",
      "Epoch: 102 | Batch: 001 / 011 | Total loss: 0.993 | Reg loss: 0.026 | Tree loss: 0.993 | Accuracy: 0.623000 | 0.536 sec/iter\n",
      "Epoch: 102 | Batch: 002 / 011 | Total loss: 0.974 | Reg loss: 0.026 | Tree loss: 0.974 | Accuracy: 0.635500 | 0.536 sec/iter\n",
      "Epoch: 102 | Batch: 003 / 011 | Total loss: 0.962 | Reg loss: 0.026 | Tree loss: 0.962 | Accuracy: 0.642500 | 0.536 sec/iter\n",
      "Epoch: 102 | Batch: 004 / 011 | Total loss: 0.936 | Reg loss: 0.026 | Tree loss: 0.936 | Accuracy: 0.660000 | 0.535 sec/iter\n",
      "Epoch: 102 | Batch: 005 / 011 | Total loss: 0.943 | Reg loss: 0.026 | Tree loss: 0.943 | Accuracy: 0.652000 | 0.535 sec/iter\n",
      "Epoch: 102 | Batch: 006 / 011 | Total loss: 0.902 | Reg loss: 0.026 | Tree loss: 0.902 | Accuracy: 0.668500 | 0.535 sec/iter\n",
      "Epoch: 102 | Batch: 007 / 011 | Total loss: 0.902 | Reg loss: 0.026 | Tree loss: 0.902 | Accuracy: 0.651000 | 0.535 sec/iter\n",
      "Epoch: 102 | Batch: 008 / 011 | Total loss: 0.929 | Reg loss: 0.026 | Tree loss: 0.929 | Accuracy: 0.648500 | 0.534 sec/iter\n",
      "Epoch: 102 | Batch: 009 / 011 | Total loss: 0.921 | Reg loss: 0.026 | Tree loss: 0.921 | Accuracy: 0.643500 | 0.534 sec/iter\n",
      "Epoch: 102 | Batch: 010 / 011 | Total loss: 0.920 | Reg loss: 0.026 | Tree loss: 0.920 | Accuracy: 0.607509 | 0.534 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 103 | Batch: 000 / 011 | Total loss: 1.019 | Reg loss: 0.026 | Tree loss: 1.019 | Accuracy: 0.610000 | 0.535 sec/iter\n",
      "Epoch: 103 | Batch: 001 / 011 | Total loss: 1.007 | Reg loss: 0.026 | Tree loss: 1.007 | Accuracy: 0.615500 | 0.535 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 103 | Batch: 002 / 011 | Total loss: 0.955 | Reg loss: 0.026 | Tree loss: 0.955 | Accuracy: 0.672500 | 0.535 sec/iter\n",
      "Epoch: 103 | Batch: 003 / 011 | Total loss: 0.963 | Reg loss: 0.026 | Tree loss: 0.963 | Accuracy: 0.631000 | 0.534 sec/iter\n",
      "Epoch: 103 | Batch: 004 / 011 | Total loss: 0.932 | Reg loss: 0.026 | Tree loss: 0.932 | Accuracy: 0.676000 | 0.534 sec/iter\n",
      "Epoch: 103 | Batch: 005 / 011 | Total loss: 0.922 | Reg loss: 0.026 | Tree loss: 0.922 | Accuracy: 0.647000 | 0.534 sec/iter\n",
      "Epoch: 103 | Batch: 006 / 011 | Total loss: 0.918 | Reg loss: 0.026 | Tree loss: 0.918 | Accuracy: 0.642500 | 0.534 sec/iter\n",
      "Epoch: 103 | Batch: 007 / 011 | Total loss: 0.932 | Reg loss: 0.026 | Tree loss: 0.932 | Accuracy: 0.640000 | 0.533 sec/iter\n",
      "Epoch: 103 | Batch: 008 / 011 | Total loss: 0.918 | Reg loss: 0.026 | Tree loss: 0.918 | Accuracy: 0.644500 | 0.533 sec/iter\n",
      "Epoch: 103 | Batch: 009 / 011 | Total loss: 0.878 | Reg loss: 0.026 | Tree loss: 0.878 | Accuracy: 0.670500 | 0.533 sec/iter\n",
      "Epoch: 103 | Batch: 010 / 011 | Total loss: 0.918 | Reg loss: 0.026 | Tree loss: 0.918 | Accuracy: 0.665529 | 0.533 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 104 | Batch: 000 / 011 | Total loss: 0.996 | Reg loss: 0.026 | Tree loss: 0.996 | Accuracy: 0.615500 | 0.536 sec/iter\n",
      "Epoch: 104 | Batch: 001 / 011 | Total loss: 0.977 | Reg loss: 0.026 | Tree loss: 0.977 | Accuracy: 0.622500 | 0.536 sec/iter\n",
      "Epoch: 104 | Batch: 002 / 011 | Total loss: 0.956 | Reg loss: 0.026 | Tree loss: 0.956 | Accuracy: 0.643500 | 0.536 sec/iter\n",
      "Epoch: 104 | Batch: 003 / 011 | Total loss: 0.963 | Reg loss: 0.026 | Tree loss: 0.963 | Accuracy: 0.660000 | 0.535 sec/iter\n",
      "Epoch: 104 | Batch: 004 / 011 | Total loss: 0.906 | Reg loss: 0.026 | Tree loss: 0.906 | Accuracy: 0.681000 | 0.535 sec/iter\n",
      "Epoch: 104 | Batch: 005 / 011 | Total loss: 0.945 | Reg loss: 0.026 | Tree loss: 0.945 | Accuracy: 0.639000 | 0.535 sec/iter\n",
      "Epoch: 104 | Batch: 006 / 011 | Total loss: 0.949 | Reg loss: 0.026 | Tree loss: 0.949 | Accuracy: 0.650000 | 0.534 sec/iter\n",
      "Epoch: 104 | Batch: 007 / 011 | Total loss: 0.930 | Reg loss: 0.026 | Tree loss: 0.930 | Accuracy: 0.637500 | 0.534 sec/iter\n",
      "Epoch: 104 | Batch: 008 / 011 | Total loss: 0.896 | Reg loss: 0.026 | Tree loss: 0.896 | Accuracy: 0.660500 | 0.534 sec/iter\n",
      "Epoch: 104 | Batch: 009 / 011 | Total loss: 0.917 | Reg loss: 0.026 | Tree loss: 0.917 | Accuracy: 0.631000 | 0.534 sec/iter\n",
      "Epoch: 104 | Batch: 010 / 011 | Total loss: 0.925 | Reg loss: 0.026 | Tree loss: 0.925 | Accuracy: 0.648464 | 0.533 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 105 | Batch: 000 / 011 | Total loss: 1.018 | Reg loss: 0.026 | Tree loss: 1.018 | Accuracy: 0.620500 | 0.536 sec/iter\n",
      "Epoch: 105 | Batch: 001 / 011 | Total loss: 1.009 | Reg loss: 0.026 | Tree loss: 1.009 | Accuracy: 0.608000 | 0.536 sec/iter\n",
      "Epoch: 105 | Batch: 002 / 011 | Total loss: 0.963 | Reg loss: 0.026 | Tree loss: 0.963 | Accuracy: 0.628500 | 0.536 sec/iter\n",
      "Epoch: 105 | Batch: 003 / 011 | Total loss: 0.939 | Reg loss: 0.026 | Tree loss: 0.939 | Accuracy: 0.656500 | 0.535 sec/iter\n",
      "Epoch: 105 | Batch: 004 / 011 | Total loss: 0.922 | Reg loss: 0.026 | Tree loss: 0.922 | Accuracy: 0.663000 | 0.535 sec/iter\n",
      "Epoch: 105 | Batch: 005 / 011 | Total loss: 0.915 | Reg loss: 0.026 | Tree loss: 0.915 | Accuracy: 0.666500 | 0.535 sec/iter\n",
      "Epoch: 105 | Batch: 006 / 011 | Total loss: 0.909 | Reg loss: 0.026 | Tree loss: 0.909 | Accuracy: 0.672500 | 0.534 sec/iter\n",
      "Epoch: 105 | Batch: 007 / 011 | Total loss: 0.901 | Reg loss: 0.026 | Tree loss: 0.901 | Accuracy: 0.663500 | 0.534 sec/iter\n",
      "Epoch: 105 | Batch: 008 / 011 | Total loss: 0.919 | Reg loss: 0.026 | Tree loss: 0.919 | Accuracy: 0.633500 | 0.534 sec/iter\n",
      "Epoch: 105 | Batch: 009 / 011 | Total loss: 0.927 | Reg loss: 0.026 | Tree loss: 0.927 | Accuracy: 0.633500 | 0.534 sec/iter\n",
      "Epoch: 105 | Batch: 010 / 011 | Total loss: 0.932 | Reg loss: 0.026 | Tree loss: 0.932 | Accuracy: 0.631399 | 0.533 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 106 | Batch: 000 / 011 | Total loss: 1.019 | Reg loss: 0.026 | Tree loss: 1.019 | Accuracy: 0.619500 | 0.534 sec/iter\n",
      "Epoch: 106 | Batch: 001 / 011 | Total loss: 0.985 | Reg loss: 0.026 | Tree loss: 0.985 | Accuracy: 0.611000 | 0.533 sec/iter\n",
      "Epoch: 106 | Batch: 002 / 011 | Total loss: 0.960 | Reg loss: 0.026 | Tree loss: 0.960 | Accuracy: 0.634500 | 0.533 sec/iter\n",
      "Epoch: 106 | Batch: 003 / 011 | Total loss: 0.938 | Reg loss: 0.026 | Tree loss: 0.938 | Accuracy: 0.665000 | 0.533 sec/iter\n",
      "Epoch: 106 | Batch: 004 / 011 | Total loss: 0.936 | Reg loss: 0.026 | Tree loss: 0.936 | Accuracy: 0.661000 | 0.533 sec/iter\n",
      "Epoch: 106 | Batch: 005 / 011 | Total loss: 0.924 | Reg loss: 0.026 | Tree loss: 0.924 | Accuracy: 0.666000 | 0.532 sec/iter\n",
      "Epoch: 106 | Batch: 006 / 011 | Total loss: 0.912 | Reg loss: 0.026 | Tree loss: 0.912 | Accuracy: 0.662500 | 0.532 sec/iter\n",
      "Epoch: 106 | Batch: 007 / 011 | Total loss: 0.893 | Reg loss: 0.026 | Tree loss: 0.893 | Accuracy: 0.656000 | 0.532 sec/iter\n",
      "Epoch: 106 | Batch: 008 / 011 | Total loss: 0.931 | Reg loss: 0.026 | Tree loss: 0.931 | Accuracy: 0.636000 | 0.532 sec/iter\n",
      "Epoch: 106 | Batch: 009 / 011 | Total loss: 0.889 | Reg loss: 0.026 | Tree loss: 0.889 | Accuracy: 0.656500 | 0.531 sec/iter\n",
      "Epoch: 106 | Batch: 010 / 011 | Total loss: 0.885 | Reg loss: 0.026 | Tree loss: 0.885 | Accuracy: 0.679181 | 0.531 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 107 | Batch: 000 / 011 | Total loss: 1.000 | Reg loss: 0.026 | Tree loss: 1.000 | Accuracy: 0.612000 | 0.534 sec/iter\n",
      "Epoch: 107 | Batch: 001 / 011 | Total loss: 0.992 | Reg loss: 0.026 | Tree loss: 0.992 | Accuracy: 0.620000 | 0.534 sec/iter\n",
      "Epoch: 107 | Batch: 002 / 011 | Total loss: 0.977 | Reg loss: 0.026 | Tree loss: 0.977 | Accuracy: 0.631500 | 0.533 sec/iter\n",
      "Epoch: 107 | Batch: 003 / 011 | Total loss: 0.910 | Reg loss: 0.026 | Tree loss: 0.910 | Accuracy: 0.675000 | 0.533 sec/iter\n",
      "Epoch: 107 | Batch: 004 / 011 | Total loss: 0.935 | Reg loss: 0.026 | Tree loss: 0.935 | Accuracy: 0.651000 | 0.533 sec/iter\n",
      "Epoch: 107 | Batch: 005 / 011 | Total loss: 0.914 | Reg loss: 0.026 | Tree loss: 0.914 | Accuracy: 0.672000 | 0.532 sec/iter\n",
      "Epoch: 107 | Batch: 006 / 011 | Total loss: 0.887 | Reg loss: 0.026 | Tree loss: 0.887 | Accuracy: 0.682000 | 0.532 sec/iter\n",
      "Epoch: 107 | Batch: 007 / 011 | Total loss: 0.929 | Reg loss: 0.026 | Tree loss: 0.929 | Accuracy: 0.635500 | 0.532 sec/iter\n",
      "Epoch: 107 | Batch: 008 / 011 | Total loss: 0.945 | Reg loss: 0.026 | Tree loss: 0.945 | Accuracy: 0.622000 | 0.532 sec/iter\n",
      "Epoch: 107 | Batch: 009 / 011 | Total loss: 0.916 | Reg loss: 0.026 | Tree loss: 0.916 | Accuracy: 0.633000 | 0.531 sec/iter\n",
      "Epoch: 107 | Batch: 010 / 011 | Total loss: 0.871 | Reg loss: 0.026 | Tree loss: 0.871 | Accuracy: 0.720137 | 0.531 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 108 | Batch: 000 / 011 | Total loss: 0.984 | Reg loss: 0.026 | Tree loss: 0.984 | Accuracy: 0.632000 | 0.535 sec/iter\n",
      "Epoch: 108 | Batch: 001 / 011 | Total loss: 0.976 | Reg loss: 0.026 | Tree loss: 0.976 | Accuracy: 0.617000 | 0.534 sec/iter\n",
      "Epoch: 108 | Batch: 002 / 011 | Total loss: 0.949 | Reg loss: 0.026 | Tree loss: 0.949 | Accuracy: 0.650500 | 0.534 sec/iter\n",
      "Epoch: 108 | Batch: 003 / 011 | Total loss: 0.948 | Reg loss: 0.026 | Tree loss: 0.948 | Accuracy: 0.651500 | 0.534 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 108 | Batch: 004 / 011 | Total loss: 0.936 | Reg loss: 0.026 | Tree loss: 0.936 | Accuracy: 0.667500 | 0.534 sec/iter\n",
      "Epoch: 108 | Batch: 005 / 011 | Total loss: 0.916 | Reg loss: 0.026 | Tree loss: 0.916 | Accuracy: 0.664000 | 0.533 sec/iter\n",
      "Epoch: 108 | Batch: 006 / 011 | Total loss: 0.938 | Reg loss: 0.026 | Tree loss: 0.938 | Accuracy: 0.658500 | 0.533 sec/iter\n",
      "Epoch: 108 | Batch: 007 / 011 | Total loss: 0.936 | Reg loss: 0.026 | Tree loss: 0.936 | Accuracy: 0.641000 | 0.533 sec/iter\n",
      "Epoch: 108 | Batch: 008 / 011 | Total loss: 0.925 | Reg loss: 0.026 | Tree loss: 0.925 | Accuracy: 0.639500 | 0.533 sec/iter\n",
      "Epoch: 108 | Batch: 009 / 011 | Total loss: 0.874 | Reg loss: 0.026 | Tree loss: 0.874 | Accuracy: 0.658000 | 0.532 sec/iter\n",
      "Epoch: 108 | Batch: 010 / 011 | Total loss: 0.865 | Reg loss: 0.026 | Tree loss: 0.865 | Accuracy: 0.699659 | 0.532 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 109 | Batch: 000 / 011 | Total loss: 0.982 | Reg loss: 0.026 | Tree loss: 0.982 | Accuracy: 0.637500 | 0.533 sec/iter\n",
      "Epoch: 109 | Batch: 001 / 011 | Total loss: 0.987 | Reg loss: 0.026 | Tree loss: 0.987 | Accuracy: 0.621000 | 0.532 sec/iter\n",
      "Epoch: 109 | Batch: 002 / 011 | Total loss: 0.962 | Reg loss: 0.026 | Tree loss: 0.962 | Accuracy: 0.639000 | 0.532 sec/iter\n",
      "Epoch: 109 | Batch: 003 / 011 | Total loss: 0.937 | Reg loss: 0.026 | Tree loss: 0.937 | Accuracy: 0.648000 | 0.532 sec/iter\n",
      "Epoch: 109 | Batch: 004 / 011 | Total loss: 0.942 | Reg loss: 0.026 | Tree loss: 0.942 | Accuracy: 0.644500 | 0.532 sec/iter\n",
      "Epoch: 109 | Batch: 005 / 011 | Total loss: 0.926 | Reg loss: 0.026 | Tree loss: 0.926 | Accuracy: 0.637500 | 0.531 sec/iter\n",
      "Epoch: 109 | Batch: 006 / 011 | Total loss: 0.901 | Reg loss: 0.026 | Tree loss: 0.901 | Accuracy: 0.672000 | 0.531 sec/iter\n",
      "Epoch: 109 | Batch: 007 / 011 | Total loss: 0.900 | Reg loss: 0.026 | Tree loss: 0.900 | Accuracy: 0.645500 | 0.531 sec/iter\n",
      "Epoch: 109 | Batch: 008 / 011 | Total loss: 0.927 | Reg loss: 0.026 | Tree loss: 0.927 | Accuracy: 0.638500 | 0.531 sec/iter\n",
      "Epoch: 109 | Batch: 009 / 011 | Total loss: 0.890 | Reg loss: 0.026 | Tree loss: 0.890 | Accuracy: 0.664000 | 0.53 sec/iter\n",
      "Epoch: 109 | Batch: 010 / 011 | Total loss: 0.887 | Reg loss: 0.026 | Tree loss: 0.887 | Accuracy: 0.641638 | 0.53 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 110 | Batch: 000 / 011 | Total loss: 0.987 | Reg loss: 0.026 | Tree loss: 0.987 | Accuracy: 0.620500 | 0.532 sec/iter\n",
      "Epoch: 110 | Batch: 001 / 011 | Total loss: 0.979 | Reg loss: 0.026 | Tree loss: 0.979 | Accuracy: 0.640000 | 0.532 sec/iter\n",
      "Epoch: 110 | Batch: 002 / 011 | Total loss: 0.948 | Reg loss: 0.026 | Tree loss: 0.948 | Accuracy: 0.649000 | 0.532 sec/iter\n",
      "Epoch: 110 | Batch: 003 / 011 | Total loss: 0.943 | Reg loss: 0.026 | Tree loss: 0.943 | Accuracy: 0.669500 | 0.531 sec/iter\n",
      "Epoch: 110 | Batch: 004 / 011 | Total loss: 0.932 | Reg loss: 0.026 | Tree loss: 0.932 | Accuracy: 0.661000 | 0.531 sec/iter\n",
      "Epoch: 110 | Batch: 005 / 011 | Total loss: 0.896 | Reg loss: 0.026 | Tree loss: 0.896 | Accuracy: 0.676500 | 0.531 sec/iter\n",
      "Epoch: 110 | Batch: 006 / 011 | Total loss: 0.901 | Reg loss: 0.026 | Tree loss: 0.901 | Accuracy: 0.661000 | 0.53 sec/iter\n",
      "Epoch: 110 | Batch: 007 / 011 | Total loss: 0.927 | Reg loss: 0.026 | Tree loss: 0.927 | Accuracy: 0.647000 | 0.53 sec/iter\n",
      "Epoch: 110 | Batch: 008 / 011 | Total loss: 0.912 | Reg loss: 0.026 | Tree loss: 0.912 | Accuracy: 0.633000 | 0.53 sec/iter\n",
      "Epoch: 110 | Batch: 009 / 011 | Total loss: 0.880 | Reg loss: 0.026 | Tree loss: 0.880 | Accuracy: 0.650000 | 0.53 sec/iter\n",
      "Epoch: 110 | Batch: 010 / 011 | Total loss: 0.916 | Reg loss: 0.026 | Tree loss: 0.916 | Accuracy: 0.662116 | 0.529 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 111 | Batch: 000 / 011 | Total loss: 0.979 | Reg loss: 0.026 | Tree loss: 0.979 | Accuracy: 0.618500 | 0.533 sec/iter\n",
      "Epoch: 111 | Batch: 001 / 011 | Total loss: 0.994 | Reg loss: 0.026 | Tree loss: 0.994 | Accuracy: 0.602000 | 0.532 sec/iter\n",
      "Epoch: 111 | Batch: 002 / 011 | Total loss: 0.948 | Reg loss: 0.026 | Tree loss: 0.948 | Accuracy: 0.651500 | 0.532 sec/iter\n",
      "Epoch: 111 | Batch: 003 / 011 | Total loss: 0.942 | Reg loss: 0.026 | Tree loss: 0.942 | Accuracy: 0.656000 | 0.532 sec/iter\n",
      "Epoch: 111 | Batch: 004 / 011 | Total loss: 0.924 | Reg loss: 0.026 | Tree loss: 0.924 | Accuracy: 0.666500 | 0.532 sec/iter\n",
      "Epoch: 111 | Batch: 005 / 011 | Total loss: 0.918 | Reg loss: 0.026 | Tree loss: 0.918 | Accuracy: 0.659500 | 0.531 sec/iter\n",
      "Epoch: 111 | Batch: 006 / 011 | Total loss: 0.902 | Reg loss: 0.026 | Tree loss: 0.902 | Accuracy: 0.653000 | 0.531 sec/iter\n",
      "Epoch: 111 | Batch: 007 / 011 | Total loss: 0.890 | Reg loss: 0.026 | Tree loss: 0.890 | Accuracy: 0.666000 | 0.531 sec/iter\n",
      "Epoch: 111 | Batch: 008 / 011 | Total loss: 0.904 | Reg loss: 0.026 | Tree loss: 0.904 | Accuracy: 0.654000 | 0.531 sec/iter\n",
      "Epoch: 111 | Batch: 009 / 011 | Total loss: 0.914 | Reg loss: 0.026 | Tree loss: 0.914 | Accuracy: 0.631500 | 0.531 sec/iter\n",
      "Epoch: 111 | Batch: 010 / 011 | Total loss: 0.861 | Reg loss: 0.026 | Tree loss: 0.861 | Accuracy: 0.648464 | 0.53 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 112 | Batch: 000 / 011 | Total loss: 0.982 | Reg loss: 0.026 | Tree loss: 0.982 | Accuracy: 0.608000 | 0.534 sec/iter\n",
      "Epoch: 112 | Batch: 001 / 011 | Total loss: 0.969 | Reg loss: 0.026 | Tree loss: 0.969 | Accuracy: 0.638500 | 0.534 sec/iter\n",
      "Epoch: 112 | Batch: 002 / 011 | Total loss: 0.951 | Reg loss: 0.026 | Tree loss: 0.951 | Accuracy: 0.649500 | 0.534 sec/iter\n",
      "Epoch: 112 | Batch: 003 / 011 | Total loss: 0.922 | Reg loss: 0.026 | Tree loss: 0.922 | Accuracy: 0.674000 | 0.534 sec/iter\n",
      "Epoch: 112 | Batch: 004 / 011 | Total loss: 0.917 | Reg loss: 0.026 | Tree loss: 0.917 | Accuracy: 0.667500 | 0.533 sec/iter\n",
      "Epoch: 112 | Batch: 005 / 011 | Total loss: 0.925 | Reg loss: 0.026 | Tree loss: 0.925 | Accuracy: 0.658000 | 0.533 sec/iter\n",
      "Epoch: 112 | Batch: 006 / 011 | Total loss: 0.894 | Reg loss: 0.026 | Tree loss: 0.894 | Accuracy: 0.672000 | 0.533 sec/iter\n",
      "Epoch: 112 | Batch: 007 / 011 | Total loss: 0.916 | Reg loss: 0.026 | Tree loss: 0.916 | Accuracy: 0.648000 | 0.533 sec/iter\n",
      "Epoch: 112 | Batch: 008 / 011 | Total loss: 0.922 | Reg loss: 0.026 | Tree loss: 0.922 | Accuracy: 0.639000 | 0.532 sec/iter\n",
      "Epoch: 112 | Batch: 009 / 011 | Total loss: 0.920 | Reg loss: 0.027 | Tree loss: 0.920 | Accuracy: 0.636500 | 0.532 sec/iter\n",
      "Epoch: 112 | Batch: 010 / 011 | Total loss: 0.914 | Reg loss: 0.027 | Tree loss: 0.914 | Accuracy: 0.665529 | 0.532 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 113 | Batch: 000 / 011 | Total loss: 0.991 | Reg loss: 0.026 | Tree loss: 0.991 | Accuracy: 0.617000 | 0.534 sec/iter\n",
      "Epoch: 113 | Batch: 001 / 011 | Total loss: 0.993 | Reg loss: 0.026 | Tree loss: 0.993 | Accuracy: 0.601000 | 0.534 sec/iter\n",
      "Epoch: 113 | Batch: 002 / 011 | Total loss: 0.988 | Reg loss: 0.026 | Tree loss: 0.988 | Accuracy: 0.628500 | 0.534 sec/iter\n",
      "Epoch: 113 | Batch: 003 / 011 | Total loss: 0.926 | Reg loss: 0.026 | Tree loss: 0.926 | Accuracy: 0.663500 | 0.534 sec/iter\n",
      "Epoch: 113 | Batch: 004 / 011 | Total loss: 0.908 | Reg loss: 0.026 | Tree loss: 0.908 | Accuracy: 0.686500 | 0.533 sec/iter\n",
      "Epoch: 113 | Batch: 005 / 011 | Total loss: 0.903 | Reg loss: 0.026 | Tree loss: 0.903 | Accuracy: 0.661000 | 0.533 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 113 | Batch: 006 / 011 | Total loss: 0.904 | Reg loss: 0.026 | Tree loss: 0.904 | Accuracy: 0.663000 | 0.533 sec/iter\n",
      "Epoch: 113 | Batch: 007 / 011 | Total loss: 0.907 | Reg loss: 0.026 | Tree loss: 0.907 | Accuracy: 0.648000 | 0.533 sec/iter\n",
      "Epoch: 113 | Batch: 008 / 011 | Total loss: 0.894 | Reg loss: 0.027 | Tree loss: 0.894 | Accuracy: 0.653000 | 0.533 sec/iter\n",
      "Epoch: 113 | Batch: 009 / 011 | Total loss: 0.894 | Reg loss: 0.027 | Tree loss: 0.894 | Accuracy: 0.650000 | 0.532 sec/iter\n",
      "Epoch: 113 | Batch: 010 / 011 | Total loss: 0.871 | Reg loss: 0.027 | Tree loss: 0.871 | Accuracy: 0.672355 | 0.532 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 114 | Batch: 000 / 011 | Total loss: 1.004 | Reg loss: 0.026 | Tree loss: 1.004 | Accuracy: 0.612500 | 0.536 sec/iter\n",
      "Epoch: 114 | Batch: 001 / 011 | Total loss: 0.960 | Reg loss: 0.026 | Tree loss: 0.960 | Accuracy: 0.636500 | 0.536 sec/iter\n",
      "Epoch: 114 | Batch: 002 / 011 | Total loss: 0.938 | Reg loss: 0.026 | Tree loss: 0.938 | Accuracy: 0.657000 | 0.536 sec/iter\n",
      "Epoch: 114 | Batch: 003 / 011 | Total loss: 0.929 | Reg loss: 0.026 | Tree loss: 0.929 | Accuracy: 0.656000 | 0.535 sec/iter\n",
      "Epoch: 114 | Batch: 004 / 011 | Total loss: 0.907 | Reg loss: 0.026 | Tree loss: 0.907 | Accuracy: 0.673000 | 0.535 sec/iter\n",
      "Epoch: 114 | Batch: 005 / 011 | Total loss: 0.919 | Reg loss: 0.026 | Tree loss: 0.919 | Accuracy: 0.658500 | 0.535 sec/iter\n",
      "Epoch: 114 | Batch: 006 / 011 | Total loss: 0.894 | Reg loss: 0.027 | Tree loss: 0.894 | Accuracy: 0.661500 | 0.535 sec/iter\n",
      "Epoch: 114 | Batch: 007 / 011 | Total loss: 0.898 | Reg loss: 0.027 | Tree loss: 0.898 | Accuracy: 0.654500 | 0.534 sec/iter\n",
      "Epoch: 114 | Batch: 008 / 011 | Total loss: 0.902 | Reg loss: 0.027 | Tree loss: 0.902 | Accuracy: 0.670000 | 0.534 sec/iter\n",
      "Epoch: 114 | Batch: 009 / 011 | Total loss: 0.908 | Reg loss: 0.027 | Tree loss: 0.908 | Accuracy: 0.644000 | 0.534 sec/iter\n",
      "Epoch: 114 | Batch: 010 / 011 | Total loss: 0.856 | Reg loss: 0.027 | Tree loss: 0.856 | Accuracy: 0.696246 | 0.534 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 115 | Batch: 000 / 011 | Total loss: 1.013 | Reg loss: 0.026 | Tree loss: 1.013 | Accuracy: 0.618000 | 0.538 sec/iter\n",
      "Epoch: 115 | Batch: 001 / 011 | Total loss: 0.976 | Reg loss: 0.026 | Tree loss: 0.976 | Accuracy: 0.619500 | 0.538 sec/iter\n",
      "Epoch: 115 | Batch: 002 / 011 | Total loss: 0.935 | Reg loss: 0.026 | Tree loss: 0.935 | Accuracy: 0.655500 | 0.538 sec/iter\n",
      "Epoch: 115 | Batch: 003 / 011 | Total loss: 0.945 | Reg loss: 0.026 | Tree loss: 0.945 | Accuracy: 0.652000 | 0.538 sec/iter\n",
      "Epoch: 115 | Batch: 004 / 011 | Total loss: 0.891 | Reg loss: 0.026 | Tree loss: 0.891 | Accuracy: 0.685500 | 0.537 sec/iter\n",
      "Epoch: 115 | Batch: 005 / 011 | Total loss: 0.896 | Reg loss: 0.027 | Tree loss: 0.896 | Accuracy: 0.681000 | 0.537 sec/iter\n",
      "Epoch: 115 | Batch: 006 / 011 | Total loss: 0.924 | Reg loss: 0.027 | Tree loss: 0.924 | Accuracy: 0.649500 | 0.537 sec/iter\n",
      "Epoch: 115 | Batch: 007 / 011 | Total loss: 0.891 | Reg loss: 0.027 | Tree loss: 0.891 | Accuracy: 0.655000 | 0.537 sec/iter\n",
      "Epoch: 115 | Batch: 008 / 011 | Total loss: 0.886 | Reg loss: 0.027 | Tree loss: 0.886 | Accuracy: 0.658500 | 0.537 sec/iter\n",
      "Epoch: 115 | Batch: 009 / 011 | Total loss: 0.891 | Reg loss: 0.027 | Tree loss: 0.891 | Accuracy: 0.662500 | 0.536 sec/iter\n",
      "Epoch: 115 | Batch: 010 / 011 | Total loss: 0.912 | Reg loss: 0.027 | Tree loss: 0.912 | Accuracy: 0.692833 | 0.536 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 116 | Batch: 000 / 011 | Total loss: 0.986 | Reg loss: 0.026 | Tree loss: 0.986 | Accuracy: 0.622000 | 0.539 sec/iter\n",
      "Epoch: 116 | Batch: 001 / 011 | Total loss: 0.976 | Reg loss: 0.026 | Tree loss: 0.976 | Accuracy: 0.635500 | 0.539 sec/iter\n",
      "Epoch: 116 | Batch: 002 / 011 | Total loss: 0.953 | Reg loss: 0.026 | Tree loss: 0.953 | Accuracy: 0.641500 | 0.538 sec/iter\n",
      "Epoch: 116 | Batch: 003 / 011 | Total loss: 0.922 | Reg loss: 0.027 | Tree loss: 0.922 | Accuracy: 0.651000 | 0.538 sec/iter\n",
      "Epoch: 116 | Batch: 004 / 011 | Total loss: 0.911 | Reg loss: 0.027 | Tree loss: 0.911 | Accuracy: 0.655000 | 0.538 sec/iter\n",
      "Epoch: 116 | Batch: 005 / 011 | Total loss: 0.890 | Reg loss: 0.027 | Tree loss: 0.890 | Accuracy: 0.672500 | 0.538 sec/iter\n",
      "Epoch: 116 | Batch: 006 / 011 | Total loss: 0.880 | Reg loss: 0.027 | Tree loss: 0.880 | Accuracy: 0.683000 | 0.537 sec/iter\n",
      "Epoch: 116 | Batch: 007 / 011 | Total loss: 0.896 | Reg loss: 0.027 | Tree loss: 0.896 | Accuracy: 0.668000 | 0.537 sec/iter\n",
      "Epoch: 116 | Batch: 008 / 011 | Total loss: 0.893 | Reg loss: 0.027 | Tree loss: 0.893 | Accuracy: 0.662000 | 0.537 sec/iter\n",
      "Epoch: 116 | Batch: 009 / 011 | Total loss: 0.920 | Reg loss: 0.027 | Tree loss: 0.920 | Accuracy: 0.636500 | 0.537 sec/iter\n",
      "Epoch: 116 | Batch: 010 / 011 | Total loss: 0.951 | Reg loss: 0.027 | Tree loss: 0.951 | Accuracy: 0.627986 | 0.537 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 117 | Batch: 000 / 011 | Total loss: 0.965 | Reg loss: 0.027 | Tree loss: 0.965 | Accuracy: 0.638500 | 0.54 sec/iter\n",
      "Epoch: 117 | Batch: 001 / 011 | Total loss: 0.960 | Reg loss: 0.027 | Tree loss: 0.960 | Accuracy: 0.650500 | 0.54 sec/iter\n",
      "Epoch: 117 | Batch: 002 / 011 | Total loss: 0.944 | Reg loss: 0.027 | Tree loss: 0.944 | Accuracy: 0.653000 | 0.54 sec/iter\n",
      "Epoch: 117 | Batch: 003 / 011 | Total loss: 0.930 | Reg loss: 0.027 | Tree loss: 0.930 | Accuracy: 0.669500 | 0.54 sec/iter\n",
      "Epoch: 117 | Batch: 004 / 011 | Total loss: 0.900 | Reg loss: 0.027 | Tree loss: 0.900 | Accuracy: 0.673500 | 0.539 sec/iter\n",
      "Epoch: 117 | Batch: 005 / 011 | Total loss: 0.900 | Reg loss: 0.027 | Tree loss: 0.900 | Accuracy: 0.673500 | 0.539 sec/iter\n",
      "Epoch: 117 | Batch: 006 / 011 | Total loss: 0.889 | Reg loss: 0.027 | Tree loss: 0.889 | Accuracy: 0.673500 | 0.539 sec/iter\n",
      "Epoch: 117 | Batch: 007 / 011 | Total loss: 0.928 | Reg loss: 0.027 | Tree loss: 0.928 | Accuracy: 0.624000 | 0.539 sec/iter\n",
      "Epoch: 117 | Batch: 008 / 011 | Total loss: 0.911 | Reg loss: 0.027 | Tree loss: 0.911 | Accuracy: 0.646000 | 0.538 sec/iter\n",
      "Epoch: 117 | Batch: 009 / 011 | Total loss: 0.913 | Reg loss: 0.027 | Tree loss: 0.913 | Accuracy: 0.632500 | 0.538 sec/iter\n",
      "Epoch: 117 | Batch: 010 / 011 | Total loss: 0.888 | Reg loss: 0.027 | Tree loss: 0.888 | Accuracy: 0.634812 | 0.538 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 118 | Batch: 000 / 011 | Total loss: 0.992 | Reg loss: 0.027 | Tree loss: 0.992 | Accuracy: 0.618000 | 0.541 sec/iter\n",
      "Epoch: 118 | Batch: 001 / 011 | Total loss: 0.979 | Reg loss: 0.027 | Tree loss: 0.979 | Accuracy: 0.621500 | 0.54 sec/iter\n",
      "Epoch: 118 | Batch: 002 / 011 | Total loss: 0.948 | Reg loss: 0.027 | Tree loss: 0.948 | Accuracy: 0.645000 | 0.54 sec/iter\n",
      "Epoch: 118 | Batch: 003 / 011 | Total loss: 0.925 | Reg loss: 0.027 | Tree loss: 0.925 | Accuracy: 0.661000 | 0.54 sec/iter\n",
      "Epoch: 118 | Batch: 004 / 011 | Total loss: 0.900 | Reg loss: 0.027 | Tree loss: 0.900 | Accuracy: 0.670500 | 0.54 sec/iter\n",
      "Epoch: 118 | Batch: 005 / 011 | Total loss: 0.892 | Reg loss: 0.027 | Tree loss: 0.892 | Accuracy: 0.678500 | 0.539 sec/iter\n",
      "Epoch: 118 | Batch: 006 / 011 | Total loss: 0.872 | Reg loss: 0.027 | Tree loss: 0.872 | Accuracy: 0.666000 | 0.539 sec/iter\n",
      "Epoch: 118 | Batch: 007 / 011 | Total loss: 0.900 | Reg loss: 0.027 | Tree loss: 0.900 | Accuracy: 0.651000 | 0.539 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 118 | Batch: 008 / 011 | Total loss: 0.893 | Reg loss: 0.027 | Tree loss: 0.893 | Accuracy: 0.658500 | 0.539 sec/iter\n",
      "Epoch: 118 | Batch: 009 / 011 | Total loss: 0.910 | Reg loss: 0.027 | Tree loss: 0.910 | Accuracy: 0.645000 | 0.538 sec/iter\n",
      "Epoch: 118 | Batch: 010 / 011 | Total loss: 0.870 | Reg loss: 0.027 | Tree loss: 0.870 | Accuracy: 0.645051 | 0.538 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 119 | Batch: 000 / 011 | Total loss: 0.989 | Reg loss: 0.027 | Tree loss: 0.989 | Accuracy: 0.609000 | 0.539 sec/iter\n",
      "Epoch: 119 | Batch: 001 / 011 | Total loss: 0.987 | Reg loss: 0.027 | Tree loss: 0.987 | Accuracy: 0.611500 | 0.539 sec/iter\n",
      "Epoch: 119 | Batch: 002 / 011 | Total loss: 0.938 | Reg loss: 0.027 | Tree loss: 0.938 | Accuracy: 0.656500 | 0.539 sec/iter\n",
      "Epoch: 119 | Batch: 003 / 011 | Total loss: 0.914 | Reg loss: 0.027 | Tree loss: 0.914 | Accuracy: 0.661000 | 0.538 sec/iter\n",
      "Epoch: 119 | Batch: 004 / 011 | Total loss: 0.887 | Reg loss: 0.027 | Tree loss: 0.887 | Accuracy: 0.684000 | 0.538 sec/iter\n",
      "Epoch: 119 | Batch: 005 / 011 | Total loss: 0.910 | Reg loss: 0.027 | Tree loss: 0.910 | Accuracy: 0.663000 | 0.538 sec/iter\n",
      "Epoch: 119 | Batch: 006 / 011 | Total loss: 0.895 | Reg loss: 0.027 | Tree loss: 0.895 | Accuracy: 0.660000 | 0.538 sec/iter\n",
      "Epoch: 119 | Batch: 007 / 011 | Total loss: 0.891 | Reg loss: 0.027 | Tree loss: 0.891 | Accuracy: 0.660000 | 0.537 sec/iter\n",
      "Epoch: 119 | Batch: 008 / 011 | Total loss: 0.915 | Reg loss: 0.027 | Tree loss: 0.915 | Accuracy: 0.650000 | 0.537 sec/iter\n",
      "Epoch: 119 | Batch: 009 / 011 | Total loss: 0.895 | Reg loss: 0.027 | Tree loss: 0.895 | Accuracy: 0.648500 | 0.537 sec/iter\n",
      "Epoch: 119 | Batch: 010 / 011 | Total loss: 0.923 | Reg loss: 0.027 | Tree loss: 0.923 | Accuracy: 0.648464 | 0.537 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 120 | Batch: 000 / 011 | Total loss: 0.966 | Reg loss: 0.027 | Tree loss: 0.966 | Accuracy: 0.630500 | 0.54 sec/iter\n",
      "Epoch: 120 | Batch: 001 / 011 | Total loss: 0.959 | Reg loss: 0.027 | Tree loss: 0.959 | Accuracy: 0.639000 | 0.539 sec/iter\n",
      "Epoch: 120 | Batch: 002 / 011 | Total loss: 0.961 | Reg loss: 0.027 | Tree loss: 0.961 | Accuracy: 0.634000 | 0.539 sec/iter\n",
      "Epoch: 120 | Batch: 003 / 011 | Total loss: 0.938 | Reg loss: 0.027 | Tree loss: 0.938 | Accuracy: 0.662500 | 0.539 sec/iter\n",
      "Epoch: 120 | Batch: 004 / 011 | Total loss: 0.890 | Reg loss: 0.027 | Tree loss: 0.890 | Accuracy: 0.678500 | 0.539 sec/iter\n",
      "Epoch: 120 | Batch: 005 / 011 | Total loss: 0.890 | Reg loss: 0.027 | Tree loss: 0.890 | Accuracy: 0.686500 | 0.538 sec/iter\n",
      "Epoch: 120 | Batch: 006 / 011 | Total loss: 0.898 | Reg loss: 0.027 | Tree loss: 0.898 | Accuracy: 0.652500 | 0.538 sec/iter\n",
      "Epoch: 120 | Batch: 007 / 011 | Total loss: 0.919 | Reg loss: 0.027 | Tree loss: 0.919 | Accuracy: 0.655500 | 0.538 sec/iter\n",
      "Epoch: 120 | Batch: 008 / 011 | Total loss: 0.887 | Reg loss: 0.027 | Tree loss: 0.887 | Accuracy: 0.654500 | 0.538 sec/iter\n",
      "Epoch: 120 | Batch: 009 / 011 | Total loss: 0.900 | Reg loss: 0.027 | Tree loss: 0.900 | Accuracy: 0.640500 | 0.537 sec/iter\n",
      "Epoch: 120 | Batch: 010 / 011 | Total loss: 0.829 | Reg loss: 0.027 | Tree loss: 0.829 | Accuracy: 0.713311 | 0.537 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 121 | Batch: 000 / 011 | Total loss: 0.983 | Reg loss: 0.027 | Tree loss: 0.983 | Accuracy: 0.612500 | 0.539 sec/iter\n",
      "Epoch: 121 | Batch: 001 / 011 | Total loss: 0.960 | Reg loss: 0.027 | Tree loss: 0.960 | Accuracy: 0.621500 | 0.539 sec/iter\n",
      "Epoch: 121 | Batch: 002 / 011 | Total loss: 0.953 | Reg loss: 0.027 | Tree loss: 0.953 | Accuracy: 0.638000 | 0.539 sec/iter\n",
      "Epoch: 121 | Batch: 003 / 011 | Total loss: 0.926 | Reg loss: 0.027 | Tree loss: 0.926 | Accuracy: 0.661500 | 0.539 sec/iter\n",
      "Epoch: 121 | Batch: 004 / 011 | Total loss: 0.906 | Reg loss: 0.027 | Tree loss: 0.906 | Accuracy: 0.668000 | 0.539 sec/iter\n",
      "Epoch: 121 | Batch: 005 / 011 | Total loss: 0.887 | Reg loss: 0.027 | Tree loss: 0.887 | Accuracy: 0.666500 | 0.538 sec/iter\n",
      "Epoch: 121 | Batch: 006 / 011 | Total loss: 0.871 | Reg loss: 0.027 | Tree loss: 0.871 | Accuracy: 0.679500 | 0.538 sec/iter\n",
      "Epoch: 121 | Batch: 007 / 011 | Total loss: 0.896 | Reg loss: 0.027 | Tree loss: 0.896 | Accuracy: 0.651000 | 0.538 sec/iter\n",
      "Epoch: 121 | Batch: 008 / 011 | Total loss: 0.900 | Reg loss: 0.027 | Tree loss: 0.900 | Accuracy: 0.644000 | 0.538 sec/iter\n",
      "Epoch: 121 | Batch: 009 / 011 | Total loss: 0.874 | Reg loss: 0.027 | Tree loss: 0.874 | Accuracy: 0.663500 | 0.537 sec/iter\n",
      "Epoch: 121 | Batch: 010 / 011 | Total loss: 0.935 | Reg loss: 0.027 | Tree loss: 0.935 | Accuracy: 0.617747 | 0.537 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 122 | Batch: 000 / 011 | Total loss: 0.977 | Reg loss: 0.027 | Tree loss: 0.977 | Accuracy: 0.621500 | 0.538 sec/iter\n",
      "Epoch: 122 | Batch: 001 / 011 | Total loss: 0.946 | Reg loss: 0.027 | Tree loss: 0.946 | Accuracy: 0.648500 | 0.537 sec/iter\n",
      "Epoch: 122 | Batch: 002 / 011 | Total loss: 0.956 | Reg loss: 0.027 | Tree loss: 0.956 | Accuracy: 0.631500 | 0.537 sec/iter\n",
      "Epoch: 122 | Batch: 003 / 011 | Total loss: 0.927 | Reg loss: 0.027 | Tree loss: 0.927 | Accuracy: 0.668500 | 0.537 sec/iter\n",
      "Epoch: 122 | Batch: 004 / 011 | Total loss: 0.902 | Reg loss: 0.027 | Tree loss: 0.902 | Accuracy: 0.678000 | 0.537 sec/iter\n",
      "Epoch: 122 | Batch: 005 / 011 | Total loss: 0.893 | Reg loss: 0.027 | Tree loss: 0.893 | Accuracy: 0.682000 | 0.537 sec/iter\n",
      "Epoch: 122 | Batch: 006 / 011 | Total loss: 0.901 | Reg loss: 0.027 | Tree loss: 0.901 | Accuracy: 0.658000 | 0.536 sec/iter\n",
      "Epoch: 122 | Batch: 007 / 011 | Total loss: 0.868 | Reg loss: 0.027 | Tree loss: 0.868 | Accuracy: 0.665500 | 0.536 sec/iter\n",
      "Epoch: 122 | Batch: 008 / 011 | Total loss: 0.889 | Reg loss: 0.027 | Tree loss: 0.889 | Accuracy: 0.649500 | 0.536 sec/iter\n",
      "Epoch: 122 | Batch: 009 / 011 | Total loss: 0.899 | Reg loss: 0.027 | Tree loss: 0.899 | Accuracy: 0.662500 | 0.536 sec/iter\n",
      "Epoch: 122 | Batch: 010 / 011 | Total loss: 0.828 | Reg loss: 0.027 | Tree loss: 0.828 | Accuracy: 0.668942 | 0.536 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 123 | Batch: 000 / 011 | Total loss: 0.985 | Reg loss: 0.027 | Tree loss: 0.985 | Accuracy: 0.613000 | 0.538 sec/iter\n",
      "Epoch: 123 | Batch: 001 / 011 | Total loss: 0.953 | Reg loss: 0.027 | Tree loss: 0.953 | Accuracy: 0.638000 | 0.538 sec/iter\n",
      "Epoch: 123 | Batch: 002 / 011 | Total loss: 0.944 | Reg loss: 0.027 | Tree loss: 0.944 | Accuracy: 0.640000 | 0.537 sec/iter\n",
      "Epoch: 123 | Batch: 003 / 011 | Total loss: 0.932 | Reg loss: 0.027 | Tree loss: 0.932 | Accuracy: 0.641500 | 0.537 sec/iter\n",
      "Epoch: 123 | Batch: 004 / 011 | Total loss: 0.894 | Reg loss: 0.027 | Tree loss: 0.894 | Accuracy: 0.671500 | 0.537 sec/iter\n",
      "Epoch: 123 | Batch: 005 / 011 | Total loss: 0.870 | Reg loss: 0.027 | Tree loss: 0.870 | Accuracy: 0.680500 | 0.537 sec/iter\n",
      "Epoch: 123 | Batch: 006 / 011 | Total loss: 0.903 | Reg loss: 0.027 | Tree loss: 0.903 | Accuracy: 0.655000 | 0.536 sec/iter\n",
      "Epoch: 123 | Batch: 007 / 011 | Total loss: 0.908 | Reg loss: 0.027 | Tree loss: 0.908 | Accuracy: 0.651500 | 0.536 sec/iter\n",
      "Epoch: 123 | Batch: 008 / 011 | Total loss: 0.874 | Reg loss: 0.027 | Tree loss: 0.874 | Accuracy: 0.669500 | 0.536 sec/iter\n",
      "Epoch: 123 | Batch: 009 / 011 | Total loss: 0.885 | Reg loss: 0.027 | Tree loss: 0.885 | Accuracy: 0.666500 | 0.536 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 123 | Batch: 010 / 011 | Total loss: 0.874 | Reg loss: 0.027 | Tree loss: 0.874 | Accuracy: 0.645051 | 0.536 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 124 | Batch: 000 / 011 | Total loss: 0.965 | Reg loss: 0.027 | Tree loss: 0.965 | Accuracy: 0.625500 | 0.538 sec/iter\n",
      "Epoch: 124 | Batch: 001 / 011 | Total loss: 0.980 | Reg loss: 0.027 | Tree loss: 0.980 | Accuracy: 0.617000 | 0.538 sec/iter\n",
      "Epoch: 124 | Batch: 002 / 011 | Total loss: 0.942 | Reg loss: 0.027 | Tree loss: 0.942 | Accuracy: 0.646000 | 0.538 sec/iter\n",
      "Epoch: 124 | Batch: 003 / 011 | Total loss: 0.893 | Reg loss: 0.027 | Tree loss: 0.893 | Accuracy: 0.667500 | 0.537 sec/iter\n",
      "Epoch: 124 | Batch: 004 / 011 | Total loss: 0.899 | Reg loss: 0.027 | Tree loss: 0.899 | Accuracy: 0.660500 | 0.537 sec/iter\n",
      "Epoch: 124 | Batch: 005 / 011 | Total loss: 0.886 | Reg loss: 0.027 | Tree loss: 0.886 | Accuracy: 0.662000 | 0.537 sec/iter\n",
      "Epoch: 124 | Batch: 006 / 011 | Total loss: 0.888 | Reg loss: 0.027 | Tree loss: 0.888 | Accuracy: 0.664500 | 0.537 sec/iter\n",
      "Epoch: 124 | Batch: 007 / 011 | Total loss: 0.864 | Reg loss: 0.027 | Tree loss: 0.864 | Accuracy: 0.668000 | 0.537 sec/iter\n",
      "Epoch: 124 | Batch: 008 / 011 | Total loss: 0.915 | Reg loss: 0.027 | Tree loss: 0.915 | Accuracy: 0.641500 | 0.536 sec/iter\n",
      "Epoch: 124 | Batch: 009 / 011 | Total loss: 0.908 | Reg loss: 0.027 | Tree loss: 0.908 | Accuracy: 0.650000 | 0.536 sec/iter\n",
      "Epoch: 124 | Batch: 010 / 011 | Total loss: 0.984 | Reg loss: 0.027 | Tree loss: 0.984 | Accuracy: 0.624573 | 0.536 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 125 | Batch: 000 / 011 | Total loss: 0.972 | Reg loss: 0.027 | Tree loss: 0.972 | Accuracy: 0.615500 | 0.536 sec/iter\n",
      "Epoch: 125 | Batch: 001 / 011 | Total loss: 0.967 | Reg loss: 0.027 | Tree loss: 0.967 | Accuracy: 0.621500 | 0.536 sec/iter\n",
      "Epoch: 125 | Batch: 002 / 011 | Total loss: 0.938 | Reg loss: 0.027 | Tree loss: 0.938 | Accuracy: 0.627500 | 0.536 sec/iter\n",
      "Epoch: 125 | Batch: 003 / 011 | Total loss: 0.909 | Reg loss: 0.027 | Tree loss: 0.909 | Accuracy: 0.649500 | 0.536 sec/iter\n",
      "Epoch: 125 | Batch: 004 / 011 | Total loss: 0.902 | Reg loss: 0.027 | Tree loss: 0.902 | Accuracy: 0.671500 | 0.535 sec/iter\n",
      "Epoch: 125 | Batch: 005 / 011 | Total loss: 0.914 | Reg loss: 0.027 | Tree loss: 0.914 | Accuracy: 0.656500 | 0.535 sec/iter\n",
      "Epoch: 125 | Batch: 006 / 011 | Total loss: 0.897 | Reg loss: 0.027 | Tree loss: 0.897 | Accuracy: 0.657500 | 0.535 sec/iter\n",
      "Epoch: 125 | Batch: 007 / 011 | Total loss: 0.895 | Reg loss: 0.027 | Tree loss: 0.895 | Accuracy: 0.668500 | 0.535 sec/iter\n",
      "Epoch: 125 | Batch: 008 / 011 | Total loss: 0.857 | Reg loss: 0.027 | Tree loss: 0.857 | Accuracy: 0.668000 | 0.535 sec/iter\n",
      "Epoch: 125 | Batch: 009 / 011 | Total loss: 0.892 | Reg loss: 0.027 | Tree loss: 0.892 | Accuracy: 0.639500 | 0.534 sec/iter\n",
      "Epoch: 125 | Batch: 010 / 011 | Total loss: 0.894 | Reg loss: 0.027 | Tree loss: 0.894 | Accuracy: 0.655290 | 0.534 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 126 | Batch: 000 / 011 | Total loss: 0.982 | Reg loss: 0.027 | Tree loss: 0.982 | Accuracy: 0.621500 | 0.537 sec/iter\n",
      "Epoch: 126 | Batch: 001 / 011 | Total loss: 0.949 | Reg loss: 0.027 | Tree loss: 0.949 | Accuracy: 0.653500 | 0.537 sec/iter\n",
      "Epoch: 126 | Batch: 002 / 011 | Total loss: 0.938 | Reg loss: 0.027 | Tree loss: 0.938 | Accuracy: 0.668000 | 0.537 sec/iter\n",
      "Epoch: 126 | Batch: 003 / 011 | Total loss: 0.904 | Reg loss: 0.027 | Tree loss: 0.904 | Accuracy: 0.674500 | 0.537 sec/iter\n",
      "Epoch: 126 | Batch: 004 / 011 | Total loss: 0.909 | Reg loss: 0.027 | Tree loss: 0.909 | Accuracy: 0.654000 | 0.537 sec/iter\n",
      "Epoch: 126 | Batch: 005 / 011 | Total loss: 0.895 | Reg loss: 0.027 | Tree loss: 0.895 | Accuracy: 0.679500 | 0.536 sec/iter\n",
      "Epoch: 126 | Batch: 006 / 011 | Total loss: 0.900 | Reg loss: 0.027 | Tree loss: 0.900 | Accuracy: 0.658500 | 0.536 sec/iter\n",
      "Epoch: 126 | Batch: 007 / 011 | Total loss: 0.878 | Reg loss: 0.027 | Tree loss: 0.878 | Accuracy: 0.651500 | 0.536 sec/iter\n",
      "Epoch: 126 | Batch: 008 / 011 | Total loss: 0.892 | Reg loss: 0.027 | Tree loss: 0.892 | Accuracy: 0.648000 | 0.536 sec/iter\n",
      "Epoch: 126 | Batch: 009 / 011 | Total loss: 0.883 | Reg loss: 0.027 | Tree loss: 0.883 | Accuracy: 0.661000 | 0.536 sec/iter\n",
      "Epoch: 126 | Batch: 010 / 011 | Total loss: 0.829 | Reg loss: 0.027 | Tree loss: 0.829 | Accuracy: 0.686007 | 0.536 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 127 | Batch: 000 / 011 | Total loss: 0.968 | Reg loss: 0.027 | Tree loss: 0.968 | Accuracy: 0.618000 | 0.538 sec/iter\n",
      "Epoch: 127 | Batch: 001 / 011 | Total loss: 0.937 | Reg loss: 0.027 | Tree loss: 0.937 | Accuracy: 0.647500 | 0.538 sec/iter\n",
      "Epoch: 127 | Batch: 002 / 011 | Total loss: 0.919 | Reg loss: 0.027 | Tree loss: 0.919 | Accuracy: 0.648500 | 0.538 sec/iter\n",
      "Epoch: 127 | Batch: 003 / 011 | Total loss: 0.930 | Reg loss: 0.027 | Tree loss: 0.930 | Accuracy: 0.660000 | 0.538 sec/iter\n",
      "Epoch: 127 | Batch: 004 / 011 | Total loss: 0.892 | Reg loss: 0.027 | Tree loss: 0.892 | Accuracy: 0.693000 | 0.537 sec/iter\n",
      "Epoch: 127 | Batch: 005 / 011 | Total loss: 0.886 | Reg loss: 0.027 | Tree loss: 0.886 | Accuracy: 0.673500 | 0.537 sec/iter\n",
      "Epoch: 127 | Batch: 006 / 011 | Total loss: 0.889 | Reg loss: 0.027 | Tree loss: 0.889 | Accuracy: 0.651500 | 0.537 sec/iter\n",
      "Epoch: 127 | Batch: 007 / 011 | Total loss: 0.883 | Reg loss: 0.027 | Tree loss: 0.883 | Accuracy: 0.659500 | 0.537 sec/iter\n",
      "Epoch: 127 | Batch: 008 / 011 | Total loss: 0.872 | Reg loss: 0.027 | Tree loss: 0.872 | Accuracy: 0.653500 | 0.537 sec/iter\n",
      "Epoch: 127 | Batch: 009 / 011 | Total loss: 0.918 | Reg loss: 0.027 | Tree loss: 0.918 | Accuracy: 0.639500 | 0.536 sec/iter\n",
      "Epoch: 127 | Batch: 010 / 011 | Total loss: 0.896 | Reg loss: 0.027 | Tree loss: 0.896 | Accuracy: 0.645051 | 0.536 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 128 | Batch: 000 / 011 | Total loss: 0.959 | Reg loss: 0.027 | Tree loss: 0.959 | Accuracy: 0.638500 | 0.539 sec/iter\n",
      "Epoch: 128 | Batch: 001 / 011 | Total loss: 0.925 | Reg loss: 0.027 | Tree loss: 0.925 | Accuracy: 0.651500 | 0.538 sec/iter\n",
      "Epoch: 128 | Batch: 002 / 011 | Total loss: 0.939 | Reg loss: 0.027 | Tree loss: 0.939 | Accuracy: 0.641000 | 0.538 sec/iter\n",
      "Epoch: 128 | Batch: 003 / 011 | Total loss: 0.915 | Reg loss: 0.027 | Tree loss: 0.915 | Accuracy: 0.650000 | 0.538 sec/iter\n",
      "Epoch: 128 | Batch: 004 / 011 | Total loss: 0.887 | Reg loss: 0.027 | Tree loss: 0.887 | Accuracy: 0.682500 | 0.538 sec/iter\n",
      "Epoch: 128 | Batch: 005 / 011 | Total loss: 0.882 | Reg loss: 0.027 | Tree loss: 0.882 | Accuracy: 0.685000 | 0.537 sec/iter\n",
      "Epoch: 128 | Batch: 006 / 011 | Total loss: 0.893 | Reg loss: 0.027 | Tree loss: 0.893 | Accuracy: 0.667500 | 0.537 sec/iter\n",
      "Epoch: 128 | Batch: 007 / 011 | Total loss: 0.890 | Reg loss: 0.027 | Tree loss: 0.890 | Accuracy: 0.650500 | 0.537 sec/iter\n",
      "Epoch: 128 | Batch: 008 / 011 | Total loss: 0.900 | Reg loss: 0.027 | Tree loss: 0.900 | Accuracy: 0.645000 | 0.537 sec/iter\n",
      "Epoch: 128 | Batch: 009 / 011 | Total loss: 0.908 | Reg loss: 0.027 | Tree loss: 0.908 | Accuracy: 0.639000 | 0.536 sec/iter\n",
      "Epoch: 128 | Batch: 010 / 011 | Total loss: 0.870 | Reg loss: 0.027 | Tree loss: 0.870 | Accuracy: 0.662116 | 0.536 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 6: 0.9723756906077349\n",
      "Epoch: 129 | Batch: 000 / 011 | Total loss: 0.955 | Reg loss: 0.027 | Tree loss: 0.955 | Accuracy: 0.631500 | 0.538 sec/iter\n",
      "Epoch: 129 | Batch: 001 / 011 | Total loss: 0.940 | Reg loss: 0.027 | Tree loss: 0.940 | Accuracy: 0.643000 | 0.538 sec/iter\n",
      "Epoch: 129 | Batch: 002 / 011 | Total loss: 0.935 | Reg loss: 0.027 | Tree loss: 0.935 | Accuracy: 0.649000 | 0.538 sec/iter\n",
      "Epoch: 129 | Batch: 003 / 011 | Total loss: 0.915 | Reg loss: 0.027 | Tree loss: 0.915 | Accuracy: 0.673000 | 0.537 sec/iter\n",
      "Epoch: 129 | Batch: 004 / 011 | Total loss: 0.894 | Reg loss: 0.027 | Tree loss: 0.894 | Accuracy: 0.680500 | 0.537 sec/iter\n",
      "Epoch: 129 | Batch: 005 / 011 | Total loss: 0.898 | Reg loss: 0.027 | Tree loss: 0.898 | Accuracy: 0.676000 | 0.537 sec/iter\n",
      "Epoch: 129 | Batch: 006 / 011 | Total loss: 0.896 | Reg loss: 0.027 | Tree loss: 0.896 | Accuracy: 0.659500 | 0.537 sec/iter\n",
      "Epoch: 129 | Batch: 007 / 011 | Total loss: 0.867 | Reg loss: 0.027 | Tree loss: 0.867 | Accuracy: 0.668000 | 0.537 sec/iter\n",
      "Epoch: 129 | Batch: 008 / 011 | Total loss: 0.884 | Reg loss: 0.027 | Tree loss: 0.884 | Accuracy: 0.656000 | 0.536 sec/iter\n",
      "Epoch: 129 | Batch: 009 / 011 | Total loss: 0.903 | Reg loss: 0.027 | Tree loss: 0.903 | Accuracy: 0.647000 | 0.536 sec/iter\n",
      "Epoch: 129 | Batch: 010 / 011 | Total loss: 0.842 | Reg loss: 0.027 | Tree loss: 0.842 | Accuracy: 0.675768 | 0.536 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 130 | Batch: 000 / 011 | Total loss: 0.959 | Reg loss: 0.027 | Tree loss: 0.959 | Accuracy: 0.630000 | 0.537 sec/iter\n",
      "Epoch: 130 | Batch: 001 / 011 | Total loss: 0.955 | Reg loss: 0.027 | Tree loss: 0.955 | Accuracy: 0.643000 | 0.536 sec/iter\n",
      "Epoch: 130 | Batch: 002 / 011 | Total loss: 0.937 | Reg loss: 0.027 | Tree loss: 0.937 | Accuracy: 0.638500 | 0.536 sec/iter\n",
      "Epoch: 130 | Batch: 003 / 011 | Total loss: 0.906 | Reg loss: 0.027 | Tree loss: 0.906 | Accuracy: 0.681000 | 0.536 sec/iter\n",
      "Epoch: 130 | Batch: 004 / 011 | Total loss: 0.879 | Reg loss: 0.027 | Tree loss: 0.879 | Accuracy: 0.679500 | 0.536 sec/iter\n",
      "Epoch: 130 | Batch: 005 / 011 | Total loss: 0.892 | Reg loss: 0.027 | Tree loss: 0.892 | Accuracy: 0.667500 | 0.536 sec/iter\n",
      "Epoch: 130 | Batch: 006 / 011 | Total loss: 0.880 | Reg loss: 0.027 | Tree loss: 0.880 | Accuracy: 0.677000 | 0.535 sec/iter\n",
      "Epoch: 130 | Batch: 007 / 011 | Total loss: 0.897 | Reg loss: 0.027 | Tree loss: 0.897 | Accuracy: 0.649000 | 0.535 sec/iter\n",
      "Epoch: 130 | Batch: 008 / 011 | Total loss: 0.887 | Reg loss: 0.027 | Tree loss: 0.887 | Accuracy: 0.660500 | 0.535 sec/iter\n",
      "Epoch: 130 | Batch: 009 / 011 | Total loss: 0.885 | Reg loss: 0.027 | Tree loss: 0.885 | Accuracy: 0.657500 | 0.535 sec/iter\n",
      "Epoch: 130 | Batch: 010 / 011 | Total loss: 0.840 | Reg loss: 0.027 | Tree loss: 0.840 | Accuracy: 0.651877 | 0.535 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 131 | Batch: 000 / 011 | Total loss: 0.962 | Reg loss: 0.027 | Tree loss: 0.962 | Accuracy: 0.633000 | 0.537 sec/iter\n",
      "Epoch: 131 | Batch: 001 / 011 | Total loss: 0.950 | Reg loss: 0.027 | Tree loss: 0.950 | Accuracy: 0.642000 | 0.537 sec/iter\n",
      "Epoch: 131 | Batch: 002 / 011 | Total loss: 0.941 | Reg loss: 0.027 | Tree loss: 0.941 | Accuracy: 0.647000 | 0.537 sec/iter\n",
      "Epoch: 131 | Batch: 003 / 011 | Total loss: 0.904 | Reg loss: 0.027 | Tree loss: 0.904 | Accuracy: 0.665000 | 0.537 sec/iter\n",
      "Epoch: 131 | Batch: 004 / 011 | Total loss: 0.890 | Reg loss: 0.027 | Tree loss: 0.890 | Accuracy: 0.673000 | 0.536 sec/iter\n",
      "Epoch: 131 | Batch: 005 / 011 | Total loss: 0.877 | Reg loss: 0.027 | Tree loss: 0.877 | Accuracy: 0.668500 | 0.536 sec/iter\n",
      "Epoch: 131 | Batch: 006 / 011 | Total loss: 0.888 | Reg loss: 0.027 | Tree loss: 0.888 | Accuracy: 0.669000 | 0.536 sec/iter\n",
      "Epoch: 131 | Batch: 007 / 011 | Total loss: 0.878 | Reg loss: 0.027 | Tree loss: 0.878 | Accuracy: 0.657500 | 0.536 sec/iter\n",
      "Epoch: 131 | Batch: 008 / 011 | Total loss: 0.891 | Reg loss: 0.027 | Tree loss: 0.891 | Accuracy: 0.657000 | 0.536 sec/iter\n",
      "Epoch: 131 | Batch: 009 / 011 | Total loss: 0.879 | Reg loss: 0.027 | Tree loss: 0.879 | Accuracy: 0.653000 | 0.535 sec/iter\n",
      "Epoch: 131 | Batch: 010 / 011 | Total loss: 0.841 | Reg loss: 0.027 | Tree loss: 0.841 | Accuracy: 0.696246 | 0.535 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 132 | Batch: 000 / 011 | Total loss: 0.966 | Reg loss: 0.027 | Tree loss: 0.966 | Accuracy: 0.624000 | 0.537 sec/iter\n",
      "Epoch: 132 | Batch: 001 / 011 | Total loss: 0.947 | Reg loss: 0.027 | Tree loss: 0.947 | Accuracy: 0.637500 | 0.537 sec/iter\n",
      "Epoch: 132 | Batch: 002 / 011 | Total loss: 0.955 | Reg loss: 0.027 | Tree loss: 0.955 | Accuracy: 0.644000 | 0.537 sec/iter\n",
      "Epoch: 132 | Batch: 003 / 011 | Total loss: 0.930 | Reg loss: 0.027 | Tree loss: 0.930 | Accuracy: 0.649500 | 0.537 sec/iter\n",
      "Epoch: 132 | Batch: 004 / 011 | Total loss: 0.887 | Reg loss: 0.027 | Tree loss: 0.887 | Accuracy: 0.675500 | 0.536 sec/iter\n",
      "Epoch: 132 | Batch: 005 / 011 | Total loss: 0.890 | Reg loss: 0.027 | Tree loss: 0.890 | Accuracy: 0.666000 | 0.536 sec/iter\n",
      "Epoch: 132 | Batch: 006 / 011 | Total loss: 0.869 | Reg loss: 0.027 | Tree loss: 0.869 | Accuracy: 0.681500 | 0.536 sec/iter\n",
      "Epoch: 132 | Batch: 007 / 011 | Total loss: 0.886 | Reg loss: 0.027 | Tree loss: 0.886 | Accuracy: 0.661500 | 0.536 sec/iter\n",
      "Epoch: 132 | Batch: 008 / 011 | Total loss: 0.864 | Reg loss: 0.027 | Tree loss: 0.864 | Accuracy: 0.660500 | 0.535 sec/iter\n",
      "Epoch: 132 | Batch: 009 / 011 | Total loss: 0.845 | Reg loss: 0.027 | Tree loss: 0.845 | Accuracy: 0.683000 | 0.535 sec/iter\n",
      "Epoch: 132 | Batch: 010 / 011 | Total loss: 0.817 | Reg loss: 0.027 | Tree loss: 0.817 | Accuracy: 0.703072 | 0.535 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 133 | Batch: 000 / 011 | Total loss: 0.955 | Reg loss: 0.027 | Tree loss: 0.955 | Accuracy: 0.637500 | 0.535 sec/iter\n",
      "Epoch: 133 | Batch: 001 / 011 | Total loss: 0.947 | Reg loss: 0.027 | Tree loss: 0.947 | Accuracy: 0.628500 | 0.535 sec/iter\n",
      "Epoch: 133 | Batch: 002 / 011 | Total loss: 0.933 | Reg loss: 0.027 | Tree loss: 0.933 | Accuracy: 0.637000 | 0.535 sec/iter\n",
      "Epoch: 133 | Batch: 003 / 011 | Total loss: 0.915 | Reg loss: 0.027 | Tree loss: 0.915 | Accuracy: 0.660500 | 0.535 sec/iter\n",
      "Epoch: 133 | Batch: 004 / 011 | Total loss: 0.891 | Reg loss: 0.027 | Tree loss: 0.891 | Accuracy: 0.680500 | 0.535 sec/iter\n",
      "Epoch: 133 | Batch: 005 / 011 | Total loss: 0.901 | Reg loss: 0.027 | Tree loss: 0.901 | Accuracy: 0.647500 | 0.534 sec/iter\n",
      "Epoch: 133 | Batch: 006 / 011 | Total loss: 0.858 | Reg loss: 0.027 | Tree loss: 0.858 | Accuracy: 0.681000 | 0.534 sec/iter\n",
      "Epoch: 133 | Batch: 007 / 011 | Total loss: 0.878 | Reg loss: 0.027 | Tree loss: 0.878 | Accuracy: 0.658000 | 0.534 sec/iter\n",
      "Epoch: 133 | Batch: 008 / 011 | Total loss: 0.872 | Reg loss: 0.027 | Tree loss: 0.872 | Accuracy: 0.657000 | 0.534 sec/iter\n",
      "Epoch: 133 | Batch: 009 / 011 | Total loss: 0.854 | Reg loss: 0.027 | Tree loss: 0.854 | Accuracy: 0.671000 | 0.533 sec/iter\n",
      "Epoch: 133 | Batch: 010 / 011 | Total loss: 0.833 | Reg loss: 0.027 | Tree loss: 0.833 | Accuracy: 0.720137 | 0.533 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 134 | Batch: 000 / 011 | Total loss: 0.965 | Reg loss: 0.027 | Tree loss: 0.965 | Accuracy: 0.628500 | 0.536 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 134 | Batch: 001 / 011 | Total loss: 0.935 | Reg loss: 0.027 | Tree loss: 0.935 | Accuracy: 0.645000 | 0.535 sec/iter\n",
      "Epoch: 134 | Batch: 002 / 011 | Total loss: 0.935 | Reg loss: 0.027 | Tree loss: 0.935 | Accuracy: 0.633000 | 0.535 sec/iter\n",
      "Epoch: 134 | Batch: 003 / 011 | Total loss: 0.902 | Reg loss: 0.027 | Tree loss: 0.902 | Accuracy: 0.658500 | 0.535 sec/iter\n",
      "Epoch: 134 | Batch: 004 / 011 | Total loss: 0.890 | Reg loss: 0.027 | Tree loss: 0.890 | Accuracy: 0.675000 | 0.535 sec/iter\n",
      "Epoch: 134 | Batch: 005 / 011 | Total loss: 0.890 | Reg loss: 0.027 | Tree loss: 0.890 | Accuracy: 0.677500 | 0.535 sec/iter\n",
      "Epoch: 134 | Batch: 006 / 011 | Total loss: 0.876 | Reg loss: 0.027 | Tree loss: 0.876 | Accuracy: 0.680000 | 0.534 sec/iter\n",
      "Epoch: 134 | Batch: 007 / 011 | Total loss: 0.877 | Reg loss: 0.027 | Tree loss: 0.877 | Accuracy: 0.668000 | 0.534 sec/iter\n",
      "Epoch: 134 | Batch: 008 / 011 | Total loss: 0.860 | Reg loss: 0.027 | Tree loss: 0.860 | Accuracy: 0.671000 | 0.534 sec/iter\n",
      "Epoch: 134 | Batch: 009 / 011 | Total loss: 0.899 | Reg loss: 0.027 | Tree loss: 0.899 | Accuracy: 0.634500 | 0.534 sec/iter\n",
      "Epoch: 134 | Batch: 010 / 011 | Total loss: 0.884 | Reg loss: 0.027 | Tree loss: 0.884 | Accuracy: 0.658703 | 0.534 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 135 | Batch: 000 / 011 | Total loss: 0.944 | Reg loss: 0.027 | Tree loss: 0.944 | Accuracy: 0.629000 | 0.536 sec/iter\n",
      "Epoch: 135 | Batch: 001 / 011 | Total loss: 0.921 | Reg loss: 0.027 | Tree loss: 0.921 | Accuracy: 0.657000 | 0.536 sec/iter\n",
      "Epoch: 135 | Batch: 002 / 011 | Total loss: 0.931 | Reg loss: 0.027 | Tree loss: 0.931 | Accuracy: 0.663500 | 0.536 sec/iter\n",
      "Epoch: 135 | Batch: 003 / 011 | Total loss: 0.900 | Reg loss: 0.027 | Tree loss: 0.900 | Accuracy: 0.672500 | 0.536 sec/iter\n",
      "Epoch: 135 | Batch: 004 / 011 | Total loss: 0.881 | Reg loss: 0.027 | Tree loss: 0.881 | Accuracy: 0.674000 | 0.536 sec/iter\n",
      "Epoch: 135 | Batch: 005 / 011 | Total loss: 0.896 | Reg loss: 0.027 | Tree loss: 0.896 | Accuracy: 0.676000 | 0.536 sec/iter\n",
      "Epoch: 135 | Batch: 006 / 011 | Total loss: 0.890 | Reg loss: 0.027 | Tree loss: 0.890 | Accuracy: 0.662500 | 0.535 sec/iter\n",
      "Epoch: 135 | Batch: 007 / 011 | Total loss: 0.897 | Reg loss: 0.027 | Tree loss: 0.897 | Accuracy: 0.662500 | 0.535 sec/iter\n",
      "Epoch: 135 | Batch: 008 / 011 | Total loss: 0.889 | Reg loss: 0.027 | Tree loss: 0.889 | Accuracy: 0.659500 | 0.535 sec/iter\n",
      "Epoch: 135 | Batch: 009 / 011 | Total loss: 0.863 | Reg loss: 0.027 | Tree loss: 0.863 | Accuracy: 0.667000 | 0.535 sec/iter\n",
      "Epoch: 135 | Batch: 010 / 011 | Total loss: 0.807 | Reg loss: 0.027 | Tree loss: 0.807 | Accuracy: 0.696246 | 0.535 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 136 | Batch: 000 / 011 | Total loss: 0.966 | Reg loss: 0.027 | Tree loss: 0.966 | Accuracy: 0.618500 | 0.537 sec/iter\n",
      "Epoch: 136 | Batch: 001 / 011 | Total loss: 0.938 | Reg loss: 0.027 | Tree loss: 0.938 | Accuracy: 0.633000 | 0.537 sec/iter\n",
      "Epoch: 136 | Batch: 002 / 011 | Total loss: 0.916 | Reg loss: 0.027 | Tree loss: 0.916 | Accuracy: 0.659500 | 0.537 sec/iter\n",
      "Epoch: 136 | Batch: 003 / 011 | Total loss: 0.887 | Reg loss: 0.027 | Tree loss: 0.887 | Accuracy: 0.681500 | 0.537 sec/iter\n",
      "Epoch: 136 | Batch: 004 / 011 | Total loss: 0.911 | Reg loss: 0.027 | Tree loss: 0.911 | Accuracy: 0.667500 | 0.537 sec/iter\n",
      "Epoch: 136 | Batch: 005 / 011 | Total loss: 0.886 | Reg loss: 0.027 | Tree loss: 0.886 | Accuracy: 0.676500 | 0.536 sec/iter\n",
      "Epoch: 136 | Batch: 006 / 011 | Total loss: 0.853 | Reg loss: 0.027 | Tree loss: 0.853 | Accuracy: 0.681500 | 0.536 sec/iter\n",
      "Epoch: 136 | Batch: 007 / 011 | Total loss: 0.891 | Reg loss: 0.027 | Tree loss: 0.891 | Accuracy: 0.641000 | 0.536 sec/iter\n",
      "Epoch: 136 | Batch: 008 / 011 | Total loss: 0.870 | Reg loss: 0.027 | Tree loss: 0.870 | Accuracy: 0.663000 | 0.536 sec/iter\n",
      "Epoch: 136 | Batch: 009 / 011 | Total loss: 0.878 | Reg loss: 0.027 | Tree loss: 0.878 | Accuracy: 0.654500 | 0.536 sec/iter\n",
      "Epoch: 136 | Batch: 010 / 011 | Total loss: 0.868 | Reg loss: 0.027 | Tree loss: 0.868 | Accuracy: 0.648464 | 0.535 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 137 | Batch: 000 / 011 | Total loss: 0.957 | Reg loss: 0.027 | Tree loss: 0.957 | Accuracy: 0.624000 | 0.538 sec/iter\n",
      "Epoch: 137 | Batch: 001 / 011 | Total loss: 0.952 | Reg loss: 0.027 | Tree loss: 0.952 | Accuracy: 0.627000 | 0.538 sec/iter\n",
      "Epoch: 137 | Batch: 002 / 011 | Total loss: 0.926 | Reg loss: 0.027 | Tree loss: 0.926 | Accuracy: 0.660000 | 0.538 sec/iter\n",
      "Epoch: 137 | Batch: 003 / 011 | Total loss: 0.894 | Reg loss: 0.027 | Tree loss: 0.894 | Accuracy: 0.687500 | 0.537 sec/iter\n",
      "Epoch: 137 | Batch: 004 / 011 | Total loss: 0.864 | Reg loss: 0.027 | Tree loss: 0.864 | Accuracy: 0.707500 | 0.537 sec/iter\n",
      "Epoch: 137 | Batch: 005 / 011 | Total loss: 0.881 | Reg loss: 0.027 | Tree loss: 0.881 | Accuracy: 0.669000 | 0.537 sec/iter\n",
      "Epoch: 137 | Batch: 006 / 011 | Total loss: 0.857 | Reg loss: 0.027 | Tree loss: 0.857 | Accuracy: 0.683500 | 0.537 sec/iter\n",
      "Epoch: 137 | Batch: 007 / 011 | Total loss: 0.878 | Reg loss: 0.027 | Tree loss: 0.878 | Accuracy: 0.668000 | 0.537 sec/iter\n",
      "Epoch: 137 | Batch: 008 / 011 | Total loss: 0.883 | Reg loss: 0.027 | Tree loss: 0.883 | Accuracy: 0.650000 | 0.536 sec/iter\n",
      "Epoch: 137 | Batch: 009 / 011 | Total loss: 0.904 | Reg loss: 0.027 | Tree loss: 0.904 | Accuracy: 0.650000 | 0.536 sec/iter\n",
      "Epoch: 137 | Batch: 010 / 011 | Total loss: 0.867 | Reg loss: 0.027 | Tree loss: 0.867 | Accuracy: 0.634812 | 0.536 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 138 | Batch: 000 / 011 | Total loss: 0.951 | Reg loss: 0.027 | Tree loss: 0.951 | Accuracy: 0.633000 | 0.539 sec/iter\n",
      "Epoch: 138 | Batch: 001 / 011 | Total loss: 0.942 | Reg loss: 0.027 | Tree loss: 0.942 | Accuracy: 0.635000 | 0.539 sec/iter\n",
      "Epoch: 138 | Batch: 002 / 011 | Total loss: 0.921 | Reg loss: 0.027 | Tree loss: 0.921 | Accuracy: 0.646500 | 0.539 sec/iter\n",
      "Epoch: 138 | Batch: 003 / 011 | Total loss: 0.884 | Reg loss: 0.027 | Tree loss: 0.884 | Accuracy: 0.680500 | 0.538 sec/iter\n",
      "Epoch: 138 | Batch: 004 / 011 | Total loss: 0.888 | Reg loss: 0.027 | Tree loss: 0.888 | Accuracy: 0.682000 | 0.538 sec/iter\n",
      "Epoch: 138 | Batch: 005 / 011 | Total loss: 0.887 | Reg loss: 0.027 | Tree loss: 0.887 | Accuracy: 0.674500 | 0.538 sec/iter\n",
      "Epoch: 138 | Batch: 006 / 011 | Total loss: 0.873 | Reg loss: 0.027 | Tree loss: 0.873 | Accuracy: 0.672500 | 0.538 sec/iter\n",
      "Epoch: 138 | Batch: 007 / 011 | Total loss: 0.859 | Reg loss: 0.027 | Tree loss: 0.859 | Accuracy: 0.681500 | 0.538 sec/iter\n",
      "Epoch: 138 | Batch: 008 / 011 | Total loss: 0.892 | Reg loss: 0.027 | Tree loss: 0.892 | Accuracy: 0.651500 | 0.538 sec/iter\n",
      "Epoch: 138 | Batch: 009 / 011 | Total loss: 0.881 | Reg loss: 0.027 | Tree loss: 0.881 | Accuracy: 0.651500 | 0.537 sec/iter\n",
      "Epoch: 138 | Batch: 010 / 011 | Total loss: 0.825 | Reg loss: 0.027 | Tree loss: 0.825 | Accuracy: 0.689420 | 0.537 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 139 | Batch: 000 / 011 | Total loss: 0.963 | Reg loss: 0.027 | Tree loss: 0.963 | Accuracy: 0.620500 | 0.541 sec/iter\n",
      "Epoch: 139 | Batch: 001 / 011 | Total loss: 0.936 | Reg loss: 0.027 | Tree loss: 0.936 | Accuracy: 0.640000 | 0.541 sec/iter\n",
      "Epoch: 139 | Batch: 002 / 011 | Total loss: 0.928 | Reg loss: 0.027 | Tree loss: 0.928 | Accuracy: 0.644500 | 0.541 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 139 | Batch: 003 / 011 | Total loss: 0.888 | Reg loss: 0.027 | Tree loss: 0.888 | Accuracy: 0.670000 | 0.541 sec/iter\n",
      "Epoch: 139 | Batch: 004 / 011 | Total loss: 0.901 | Reg loss: 0.027 | Tree loss: 0.901 | Accuracy: 0.661500 | 0.54 sec/iter\n",
      "Epoch: 139 | Batch: 005 / 011 | Total loss: 0.864 | Reg loss: 0.027 | Tree loss: 0.864 | Accuracy: 0.684000 | 0.54 sec/iter\n",
      "Epoch: 139 | Batch: 006 / 011 | Total loss: 0.845 | Reg loss: 0.027 | Tree loss: 0.845 | Accuracy: 0.692500 | 0.54 sec/iter\n",
      "Epoch: 139 | Batch: 007 / 011 | Total loss: 0.903 | Reg loss: 0.027 | Tree loss: 0.903 | Accuracy: 0.653500 | 0.54 sec/iter\n",
      "Epoch: 139 | Batch: 008 / 011 | Total loss: 0.878 | Reg loss: 0.027 | Tree loss: 0.878 | Accuracy: 0.662500 | 0.54 sec/iter\n",
      "Epoch: 139 | Batch: 009 / 011 | Total loss: 0.859 | Reg loss: 0.027 | Tree loss: 0.859 | Accuracy: 0.659000 | 0.54 sec/iter\n",
      "Epoch: 139 | Batch: 010 / 011 | Total loss: 0.837 | Reg loss: 0.027 | Tree loss: 0.837 | Accuracy: 0.699659 | 0.539 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 140 | Batch: 000 / 011 | Total loss: 0.951 | Reg loss: 0.027 | Tree loss: 0.951 | Accuracy: 0.625000 | 0.542 sec/iter\n",
      "Epoch: 140 | Batch: 001 / 011 | Total loss: 0.917 | Reg loss: 0.027 | Tree loss: 0.917 | Accuracy: 0.661000 | 0.542 sec/iter\n",
      "Epoch: 140 | Batch: 002 / 011 | Total loss: 0.915 | Reg loss: 0.027 | Tree loss: 0.915 | Accuracy: 0.637500 | 0.542 sec/iter\n",
      "Epoch: 140 | Batch: 003 / 011 | Total loss: 0.893 | Reg loss: 0.027 | Tree loss: 0.893 | Accuracy: 0.668000 | 0.542 sec/iter\n",
      "Epoch: 140 | Batch: 004 / 011 | Total loss: 0.865 | Reg loss: 0.027 | Tree loss: 0.865 | Accuracy: 0.690500 | 0.542 sec/iter\n",
      "Epoch: 140 | Batch: 005 / 011 | Total loss: 0.896 | Reg loss: 0.027 | Tree loss: 0.896 | Accuracy: 0.661500 | 0.541 sec/iter\n",
      "Epoch: 140 | Batch: 006 / 011 | Total loss: 0.880 | Reg loss: 0.027 | Tree loss: 0.880 | Accuracy: 0.681500 | 0.541 sec/iter\n",
      "Epoch: 140 | Batch: 007 / 011 | Total loss: 0.883 | Reg loss: 0.027 | Tree loss: 0.883 | Accuracy: 0.664500 | 0.541 sec/iter\n",
      "Epoch: 140 | Batch: 008 / 011 | Total loss: 0.884 | Reg loss: 0.027 | Tree loss: 0.884 | Accuracy: 0.648000 | 0.541 sec/iter\n",
      "Epoch: 140 | Batch: 009 / 011 | Total loss: 0.895 | Reg loss: 0.027 | Tree loss: 0.895 | Accuracy: 0.645500 | 0.54 sec/iter\n",
      "Epoch: 140 | Batch: 010 / 011 | Total loss: 0.840 | Reg loss: 0.027 | Tree loss: 0.840 | Accuracy: 0.696246 | 0.54 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 141 | Batch: 000 / 011 | Total loss: 0.948 | Reg loss: 0.027 | Tree loss: 0.948 | Accuracy: 0.641500 | 0.54 sec/iter\n",
      "Epoch: 141 | Batch: 001 / 011 | Total loss: 0.983 | Reg loss: 0.027 | Tree loss: 0.983 | Accuracy: 0.606000 | 0.54 sec/iter\n",
      "Epoch: 141 | Batch: 002 / 011 | Total loss: 0.910 | Reg loss: 0.027 | Tree loss: 0.910 | Accuracy: 0.648000 | 0.54 sec/iter\n",
      "Epoch: 141 | Batch: 003 / 011 | Total loss: 0.888 | Reg loss: 0.027 | Tree loss: 0.888 | Accuracy: 0.676000 | 0.54 sec/iter\n",
      "Epoch: 141 | Batch: 004 / 011 | Total loss: 0.881 | Reg loss: 0.027 | Tree loss: 0.881 | Accuracy: 0.696000 | 0.54 sec/iter\n",
      "Epoch: 141 | Batch: 005 / 011 | Total loss: 0.862 | Reg loss: 0.027 | Tree loss: 0.862 | Accuracy: 0.700000 | 0.539 sec/iter\n",
      "Epoch: 141 | Batch: 006 / 011 | Total loss: 0.877 | Reg loss: 0.027 | Tree loss: 0.877 | Accuracy: 0.680500 | 0.539 sec/iter\n",
      "Epoch: 141 | Batch: 007 / 011 | Total loss: 0.878 | Reg loss: 0.027 | Tree loss: 0.878 | Accuracy: 0.651500 | 0.539 sec/iter\n",
      "Epoch: 141 | Batch: 008 / 011 | Total loss: 0.834 | Reg loss: 0.027 | Tree loss: 0.834 | Accuracy: 0.682500 | 0.539 sec/iter\n",
      "Epoch: 141 | Batch: 009 / 011 | Total loss: 0.875 | Reg loss: 0.027 | Tree loss: 0.875 | Accuracy: 0.651000 | 0.539 sec/iter\n",
      "Epoch: 141 | Batch: 010 / 011 | Total loss: 0.913 | Reg loss: 0.027 | Tree loss: 0.913 | Accuracy: 0.634812 | 0.538 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 142 | Batch: 000 / 011 | Total loss: 0.946 | Reg loss: 0.027 | Tree loss: 0.946 | Accuracy: 0.649000 | 0.541 sec/iter\n",
      "Epoch: 142 | Batch: 001 / 011 | Total loss: 0.924 | Reg loss: 0.027 | Tree loss: 0.924 | Accuracy: 0.644000 | 0.54 sec/iter\n",
      "Epoch: 142 | Batch: 002 / 011 | Total loss: 0.923 | Reg loss: 0.027 | Tree loss: 0.923 | Accuracy: 0.631500 | 0.54 sec/iter\n",
      "Epoch: 142 | Batch: 003 / 011 | Total loss: 0.894 | Reg loss: 0.027 | Tree loss: 0.894 | Accuracy: 0.660500 | 0.54 sec/iter\n",
      "Epoch: 142 | Batch: 004 / 011 | Total loss: 0.891 | Reg loss: 0.027 | Tree loss: 0.891 | Accuracy: 0.665500 | 0.54 sec/iter\n",
      "Epoch: 142 | Batch: 005 / 011 | Total loss: 0.873 | Reg loss: 0.027 | Tree loss: 0.873 | Accuracy: 0.681500 | 0.54 sec/iter\n",
      "Epoch: 142 | Batch: 006 / 011 | Total loss: 0.903 | Reg loss: 0.027 | Tree loss: 0.903 | Accuracy: 0.654500 | 0.539 sec/iter\n",
      "Epoch: 142 | Batch: 007 / 011 | Total loss: 0.870 | Reg loss: 0.027 | Tree loss: 0.870 | Accuracy: 0.674000 | 0.539 sec/iter\n",
      "Epoch: 142 | Batch: 008 / 011 | Total loss: 0.865 | Reg loss: 0.027 | Tree loss: 0.865 | Accuracy: 0.666000 | 0.539 sec/iter\n",
      "Epoch: 142 | Batch: 009 / 011 | Total loss: 0.852 | Reg loss: 0.027 | Tree loss: 0.852 | Accuracy: 0.667500 | 0.539 sec/iter\n",
      "Epoch: 142 | Batch: 010 / 011 | Total loss: 0.815 | Reg loss: 0.027 | Tree loss: 0.815 | Accuracy: 0.679181 | 0.539 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 143 | Batch: 000 / 011 | Total loss: 0.962 | Reg loss: 0.027 | Tree loss: 0.962 | Accuracy: 0.640000 | 0.541 sec/iter\n",
      "Epoch: 143 | Batch: 001 / 011 | Total loss: 0.936 | Reg loss: 0.027 | Tree loss: 0.936 | Accuracy: 0.642000 | 0.541 sec/iter\n",
      "Epoch: 143 | Batch: 002 / 011 | Total loss: 0.902 | Reg loss: 0.027 | Tree loss: 0.902 | Accuracy: 0.671500 | 0.541 sec/iter\n",
      "Epoch: 143 | Batch: 003 / 011 | Total loss: 0.884 | Reg loss: 0.027 | Tree loss: 0.884 | Accuracy: 0.689000 | 0.541 sec/iter\n",
      "Epoch: 143 | Batch: 004 / 011 | Total loss: 0.875 | Reg loss: 0.027 | Tree loss: 0.875 | Accuracy: 0.685500 | 0.54 sec/iter\n",
      "Epoch: 143 | Batch: 005 / 011 | Total loss: 0.856 | Reg loss: 0.027 | Tree loss: 0.856 | Accuracy: 0.688500 | 0.54 sec/iter\n",
      "Epoch: 143 | Batch: 006 / 011 | Total loss: 0.862 | Reg loss: 0.027 | Tree loss: 0.862 | Accuracy: 0.688500 | 0.54 sec/iter\n",
      "Epoch: 143 | Batch: 007 / 011 | Total loss: 0.903 | Reg loss: 0.027 | Tree loss: 0.903 | Accuracy: 0.653500 | 0.54 sec/iter\n",
      "Epoch: 143 | Batch: 008 / 011 | Total loss: 0.862 | Reg loss: 0.027 | Tree loss: 0.862 | Accuracy: 0.659000 | 0.54 sec/iter\n",
      "Epoch: 143 | Batch: 009 / 011 | Total loss: 0.892 | Reg loss: 0.027 | Tree loss: 0.892 | Accuracy: 0.640000 | 0.539 sec/iter\n",
      "Epoch: 143 | Batch: 010 / 011 | Total loss: 0.787 | Reg loss: 0.027 | Tree loss: 0.787 | Accuracy: 0.699659 | 0.539 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 144 | Batch: 000 / 011 | Total loss: 0.954 | Reg loss: 0.027 | Tree loss: 0.954 | Accuracy: 0.630500 | 0.539 sec/iter\n",
      "Epoch: 144 | Batch: 001 / 011 | Total loss: 0.931 | Reg loss: 0.027 | Tree loss: 0.931 | Accuracy: 0.661000 | 0.539 sec/iter\n",
      "Epoch: 144 | Batch: 002 / 011 | Total loss: 0.896 | Reg loss: 0.027 | Tree loss: 0.896 | Accuracy: 0.665500 | 0.539 sec/iter\n",
      "Epoch: 144 | Batch: 003 / 011 | Total loss: 0.927 | Reg loss: 0.027 | Tree loss: 0.927 | Accuracy: 0.651500 | 0.539 sec/iter\n",
      "Epoch: 144 | Batch: 004 / 011 | Total loss: 0.868 | Reg loss: 0.027 | Tree loss: 0.868 | Accuracy: 0.683500 | 0.539 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 144 | Batch: 005 / 011 | Total loss: 0.846 | Reg loss: 0.027 | Tree loss: 0.846 | Accuracy: 0.689500 | 0.538 sec/iter\n",
      "Epoch: 144 | Batch: 006 / 011 | Total loss: 0.868 | Reg loss: 0.027 | Tree loss: 0.868 | Accuracy: 0.674500 | 0.538 sec/iter\n",
      "Epoch: 144 | Batch: 007 / 011 | Total loss: 0.853 | Reg loss: 0.027 | Tree loss: 0.853 | Accuracy: 0.669500 | 0.538 sec/iter\n",
      "Epoch: 144 | Batch: 008 / 011 | Total loss: 0.867 | Reg loss: 0.027 | Tree loss: 0.867 | Accuracy: 0.659500 | 0.538 sec/iter\n",
      "Epoch: 144 | Batch: 009 / 011 | Total loss: 0.872 | Reg loss: 0.027 | Tree loss: 0.872 | Accuracy: 0.660500 | 0.538 sec/iter\n",
      "Epoch: 144 | Batch: 010 / 011 | Total loss: 0.911 | Reg loss: 0.027 | Tree loss: 0.911 | Accuracy: 0.641638 | 0.537 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 145 | Batch: 000 / 011 | Total loss: 0.943 | Reg loss: 0.027 | Tree loss: 0.943 | Accuracy: 0.646500 | 0.539 sec/iter\n",
      "Epoch: 145 | Batch: 001 / 011 | Total loss: 0.920 | Reg loss: 0.027 | Tree loss: 0.920 | Accuracy: 0.643500 | 0.539 sec/iter\n",
      "Epoch: 145 | Batch: 002 / 011 | Total loss: 0.904 | Reg loss: 0.027 | Tree loss: 0.904 | Accuracy: 0.653000 | 0.539 sec/iter\n",
      "Epoch: 145 | Batch: 003 / 011 | Total loss: 0.894 | Reg loss: 0.027 | Tree loss: 0.894 | Accuracy: 0.675000 | 0.538 sec/iter\n",
      "Epoch: 145 | Batch: 004 / 011 | Total loss: 0.895 | Reg loss: 0.027 | Tree loss: 0.895 | Accuracy: 0.670000 | 0.538 sec/iter\n",
      "Epoch: 145 | Batch: 005 / 011 | Total loss: 0.885 | Reg loss: 0.027 | Tree loss: 0.885 | Accuracy: 0.675000 | 0.538 sec/iter\n",
      "Epoch: 145 | Batch: 006 / 011 | Total loss: 0.885 | Reg loss: 0.027 | Tree loss: 0.885 | Accuracy: 0.672000 | 0.538 sec/iter\n",
      "Epoch: 145 | Batch: 007 / 011 | Total loss: 0.831 | Reg loss: 0.027 | Tree loss: 0.831 | Accuracy: 0.697000 | 0.538 sec/iter\n",
      "Epoch: 145 | Batch: 008 / 011 | Total loss: 0.873 | Reg loss: 0.027 | Tree loss: 0.873 | Accuracy: 0.655000 | 0.537 sec/iter\n",
      "Epoch: 145 | Batch: 009 / 011 | Total loss: 0.870 | Reg loss: 0.027 | Tree loss: 0.870 | Accuracy: 0.660000 | 0.537 sec/iter\n",
      "Epoch: 145 | Batch: 010 / 011 | Total loss: 0.877 | Reg loss: 0.027 | Tree loss: 0.877 | Accuracy: 0.662116 | 0.537 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 146 | Batch: 000 / 011 | Total loss: 0.945 | Reg loss: 0.027 | Tree loss: 0.945 | Accuracy: 0.648500 | 0.539 sec/iter\n",
      "Epoch: 146 | Batch: 001 / 011 | Total loss: 0.942 | Reg loss: 0.027 | Tree loss: 0.942 | Accuracy: 0.642000 | 0.539 sec/iter\n",
      "Epoch: 146 | Batch: 002 / 011 | Total loss: 0.920 | Reg loss: 0.027 | Tree loss: 0.920 | Accuracy: 0.674500 | 0.539 sec/iter\n",
      "Epoch: 146 | Batch: 003 / 011 | Total loss: 0.879 | Reg loss: 0.027 | Tree loss: 0.879 | Accuracy: 0.680000 | 0.539 sec/iter\n",
      "Epoch: 146 | Batch: 004 / 011 | Total loss: 0.868 | Reg loss: 0.027 | Tree loss: 0.868 | Accuracy: 0.690500 | 0.538 sec/iter\n",
      "Epoch: 146 | Batch: 005 / 011 | Total loss: 0.861 | Reg loss: 0.027 | Tree loss: 0.861 | Accuracy: 0.693500 | 0.538 sec/iter\n",
      "Epoch: 146 | Batch: 006 / 011 | Total loss: 0.862 | Reg loss: 0.027 | Tree loss: 0.862 | Accuracy: 0.679000 | 0.538 sec/iter\n",
      "Epoch: 146 | Batch: 007 / 011 | Total loss: 0.869 | Reg loss: 0.027 | Tree loss: 0.869 | Accuracy: 0.666000 | 0.538 sec/iter\n",
      "Epoch: 146 | Batch: 008 / 011 | Total loss: 0.865 | Reg loss: 0.027 | Tree loss: 0.865 | Accuracy: 0.661000 | 0.538 sec/iter\n",
      "Epoch: 146 | Batch: 009 / 011 | Total loss: 0.861 | Reg loss: 0.027 | Tree loss: 0.861 | Accuracy: 0.671000 | 0.537 sec/iter\n",
      "Epoch: 146 | Batch: 010 / 011 | Total loss: 0.849 | Reg loss: 0.027 | Tree loss: 0.849 | Accuracy: 0.686007 | 0.537 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 147 | Batch: 000 / 011 | Total loss: 0.946 | Reg loss: 0.027 | Tree loss: 0.946 | Accuracy: 0.640000 | 0.538 sec/iter\n",
      "Epoch: 147 | Batch: 001 / 011 | Total loss: 0.918 | Reg loss: 0.027 | Tree loss: 0.918 | Accuracy: 0.656000 | 0.537 sec/iter\n",
      "Epoch: 147 | Batch: 002 / 011 | Total loss: 0.913 | Reg loss: 0.027 | Tree loss: 0.913 | Accuracy: 0.655500 | 0.537 sec/iter\n",
      "Epoch: 147 | Batch: 003 / 011 | Total loss: 0.901 | Reg loss: 0.027 | Tree loss: 0.901 | Accuracy: 0.675000 | 0.537 sec/iter\n",
      "Epoch: 147 | Batch: 004 / 011 | Total loss: 0.849 | Reg loss: 0.027 | Tree loss: 0.849 | Accuracy: 0.699500 | 0.537 sec/iter\n",
      "Epoch: 147 | Batch: 005 / 011 | Total loss: 0.859 | Reg loss: 0.027 | Tree loss: 0.859 | Accuracy: 0.688000 | 0.537 sec/iter\n",
      "Epoch: 147 | Batch: 006 / 011 | Total loss: 0.876 | Reg loss: 0.027 | Tree loss: 0.876 | Accuracy: 0.663000 | 0.536 sec/iter\n",
      "Epoch: 147 | Batch: 007 / 011 | Total loss: 0.852 | Reg loss: 0.027 | Tree loss: 0.852 | Accuracy: 0.684000 | 0.536 sec/iter\n",
      "Epoch: 147 | Batch: 008 / 011 | Total loss: 0.867 | Reg loss: 0.027 | Tree loss: 0.867 | Accuracy: 0.664500 | 0.536 sec/iter\n",
      "Epoch: 147 | Batch: 009 / 011 | Total loss: 0.869 | Reg loss: 0.027 | Tree loss: 0.869 | Accuracy: 0.659000 | 0.536 sec/iter\n",
      "Epoch: 147 | Batch: 010 / 011 | Total loss: 0.852 | Reg loss: 0.027 | Tree loss: 0.852 | Accuracy: 0.651877 | 0.536 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 148 | Batch: 000 / 011 | Total loss: 0.945 | Reg loss: 0.027 | Tree loss: 0.945 | Accuracy: 0.642500 | 0.538 sec/iter\n",
      "Epoch: 148 | Batch: 001 / 011 | Total loss: 0.927 | Reg loss: 0.027 | Tree loss: 0.927 | Accuracy: 0.652000 | 0.538 sec/iter\n",
      "Epoch: 148 | Batch: 002 / 011 | Total loss: 0.912 | Reg loss: 0.027 | Tree loss: 0.912 | Accuracy: 0.655000 | 0.538 sec/iter\n",
      "Epoch: 148 | Batch: 003 / 011 | Total loss: 0.892 | Reg loss: 0.027 | Tree loss: 0.892 | Accuracy: 0.654500 | 0.538 sec/iter\n",
      "Epoch: 148 | Batch: 004 / 011 | Total loss: 0.864 | Reg loss: 0.027 | Tree loss: 0.864 | Accuracy: 0.677500 | 0.538 sec/iter\n",
      "Epoch: 148 | Batch: 005 / 011 | Total loss: 0.860 | Reg loss: 0.027 | Tree loss: 0.860 | Accuracy: 0.667000 | 0.537 sec/iter\n",
      "Epoch: 148 | Batch: 006 / 011 | Total loss: 0.885 | Reg loss: 0.027 | Tree loss: 0.885 | Accuracy: 0.655500 | 0.537 sec/iter\n",
      "Epoch: 148 | Batch: 007 / 011 | Total loss: 0.867 | Reg loss: 0.027 | Tree loss: 0.867 | Accuracy: 0.668500 | 0.537 sec/iter\n",
      "Epoch: 148 | Batch: 008 / 011 | Total loss: 0.832 | Reg loss: 0.027 | Tree loss: 0.832 | Accuracy: 0.674500 | 0.537 sec/iter\n",
      "Epoch: 148 | Batch: 009 / 011 | Total loss: 0.864 | Reg loss: 0.027 | Tree loss: 0.864 | Accuracy: 0.663500 | 0.537 sec/iter\n",
      "Epoch: 148 | Batch: 010 / 011 | Total loss: 0.876 | Reg loss: 0.027 | Tree loss: 0.876 | Accuracy: 0.627986 | 0.536 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 149 | Batch: 000 / 011 | Total loss: 0.947 | Reg loss: 0.027 | Tree loss: 0.947 | Accuracy: 0.636000 | 0.538 sec/iter\n",
      "Epoch: 149 | Batch: 001 / 011 | Total loss: 0.909 | Reg loss: 0.027 | Tree loss: 0.909 | Accuracy: 0.655500 | 0.538 sec/iter\n",
      "Epoch: 149 | Batch: 002 / 011 | Total loss: 0.924 | Reg loss: 0.027 | Tree loss: 0.924 | Accuracy: 0.649000 | 0.537 sec/iter\n",
      "Epoch: 149 | Batch: 003 / 011 | Total loss: 0.901 | Reg loss: 0.027 | Tree loss: 0.901 | Accuracy: 0.673000 | 0.537 sec/iter\n",
      "Epoch: 149 | Batch: 004 / 011 | Total loss: 0.891 | Reg loss: 0.027 | Tree loss: 0.891 | Accuracy: 0.679000 | 0.537 sec/iter\n",
      "Epoch: 149 | Batch: 005 / 011 | Total loss: 0.852 | Reg loss: 0.027 | Tree loss: 0.852 | Accuracy: 0.697500 | 0.537 sec/iter\n",
      "Epoch: 149 | Batch: 006 / 011 | Total loss: 0.864 | Reg loss: 0.027 | Tree loss: 0.864 | Accuracy: 0.670500 | 0.537 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 149 | Batch: 007 / 011 | Total loss: 0.834 | Reg loss: 0.027 | Tree loss: 0.834 | Accuracy: 0.677000 | 0.537 sec/iter\n",
      "Epoch: 149 | Batch: 008 / 011 | Total loss: 0.875 | Reg loss: 0.027 | Tree loss: 0.875 | Accuracy: 0.646500 | 0.536 sec/iter\n",
      "Epoch: 149 | Batch: 009 / 011 | Total loss: 0.852 | Reg loss: 0.027 | Tree loss: 0.852 | Accuracy: 0.670500 | 0.536 sec/iter\n",
      "Epoch: 149 | Batch: 010 / 011 | Total loss: 0.877 | Reg loss: 0.027 | Tree loss: 0.877 | Accuracy: 0.679181 | 0.536 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 150 | Batch: 000 / 011 | Total loss: 0.928 | Reg loss: 0.027 | Tree loss: 0.928 | Accuracy: 0.651500 | 0.538 sec/iter\n",
      "Epoch: 150 | Batch: 001 / 011 | Total loss: 0.923 | Reg loss: 0.027 | Tree loss: 0.923 | Accuracy: 0.648500 | 0.538 sec/iter\n",
      "Epoch: 150 | Batch: 002 / 011 | Total loss: 0.919 | Reg loss: 0.027 | Tree loss: 0.919 | Accuracy: 0.652000 | 0.538 sec/iter\n",
      "Epoch: 150 | Batch: 003 / 011 | Total loss: 0.877 | Reg loss: 0.027 | Tree loss: 0.877 | Accuracy: 0.691500 | 0.538 sec/iter\n",
      "Epoch: 150 | Batch: 004 / 011 | Total loss: 0.877 | Reg loss: 0.027 | Tree loss: 0.877 | Accuracy: 0.691000 | 0.538 sec/iter\n",
      "Epoch: 150 | Batch: 005 / 011 | Total loss: 0.855 | Reg loss: 0.027 | Tree loss: 0.855 | Accuracy: 0.689500 | 0.537 sec/iter\n",
      "Epoch: 150 | Batch: 006 / 011 | Total loss: 0.863 | Reg loss: 0.027 | Tree loss: 0.863 | Accuracy: 0.665500 | 0.537 sec/iter\n",
      "Epoch: 150 | Batch: 007 / 011 | Total loss: 0.872 | Reg loss: 0.027 | Tree loss: 0.872 | Accuracy: 0.663500 | 0.537 sec/iter\n",
      "Epoch: 150 | Batch: 008 / 011 | Total loss: 0.853 | Reg loss: 0.027 | Tree loss: 0.853 | Accuracy: 0.669000 | 0.537 sec/iter\n",
      "Epoch: 150 | Batch: 009 / 011 | Total loss: 0.847 | Reg loss: 0.027 | Tree loss: 0.847 | Accuracy: 0.670500 | 0.537 sec/iter\n",
      "Epoch: 150 | Batch: 010 / 011 | Total loss: 0.824 | Reg loss: 0.027 | Tree loss: 0.824 | Accuracy: 0.716724 | 0.537 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 151 | Batch: 000 / 011 | Total loss: 0.950 | Reg loss: 0.027 | Tree loss: 0.950 | Accuracy: 0.637500 | 0.539 sec/iter\n",
      "Epoch: 151 | Batch: 001 / 011 | Total loss: 0.921 | Reg loss: 0.027 | Tree loss: 0.921 | Accuracy: 0.630500 | 0.539 sec/iter\n",
      "Epoch: 151 | Batch: 002 / 011 | Total loss: 0.899 | Reg loss: 0.027 | Tree loss: 0.899 | Accuracy: 0.667000 | 0.538 sec/iter\n",
      "Epoch: 151 | Batch: 003 / 011 | Total loss: 0.872 | Reg loss: 0.027 | Tree loss: 0.872 | Accuracy: 0.685000 | 0.538 sec/iter\n",
      "Epoch: 151 | Batch: 004 / 011 | Total loss: 0.859 | Reg loss: 0.027 | Tree loss: 0.859 | Accuracy: 0.696500 | 0.538 sec/iter\n",
      "Epoch: 151 | Batch: 005 / 011 | Total loss: 0.872 | Reg loss: 0.027 | Tree loss: 0.872 | Accuracy: 0.679000 | 0.538 sec/iter\n",
      "Epoch: 151 | Batch: 006 / 011 | Total loss: 0.864 | Reg loss: 0.027 | Tree loss: 0.864 | Accuracy: 0.680000 | 0.538 sec/iter\n",
      "Epoch: 151 | Batch: 007 / 011 | Total loss: 0.852 | Reg loss: 0.027 | Tree loss: 0.852 | Accuracy: 0.679000 | 0.538 sec/iter\n",
      "Epoch: 151 | Batch: 008 / 011 | Total loss: 0.853 | Reg loss: 0.027 | Tree loss: 0.853 | Accuracy: 0.660000 | 0.537 sec/iter\n",
      "Epoch: 151 | Batch: 009 / 011 | Total loss: 0.849 | Reg loss: 0.027 | Tree loss: 0.849 | Accuracy: 0.680500 | 0.537 sec/iter\n",
      "Epoch: 151 | Batch: 010 / 011 | Total loss: 0.826 | Reg loss: 0.027 | Tree loss: 0.826 | Accuracy: 0.692833 | 0.537 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 152 | Batch: 000 / 011 | Total loss: 0.921 | Reg loss: 0.027 | Tree loss: 0.921 | Accuracy: 0.637500 | 0.537 sec/iter\n",
      "Epoch: 152 | Batch: 001 / 011 | Total loss: 0.910 | Reg loss: 0.027 | Tree loss: 0.910 | Accuracy: 0.653000 | 0.537 sec/iter\n",
      "Epoch: 152 | Batch: 002 / 011 | Total loss: 0.901 | Reg loss: 0.027 | Tree loss: 0.901 | Accuracy: 0.669000 | 0.537 sec/iter\n",
      "Epoch: 152 | Batch: 003 / 011 | Total loss: 0.890 | Reg loss: 0.027 | Tree loss: 0.890 | Accuracy: 0.681500 | 0.537 sec/iter\n",
      "Epoch: 152 | Batch: 004 / 011 | Total loss: 0.860 | Reg loss: 0.027 | Tree loss: 0.860 | Accuracy: 0.696000 | 0.536 sec/iter\n",
      "Epoch: 152 | Batch: 005 / 011 | Total loss: 0.849 | Reg loss: 0.027 | Tree loss: 0.849 | Accuracy: 0.692000 | 0.536 sec/iter\n",
      "Epoch: 152 | Batch: 006 / 011 | Total loss: 0.892 | Reg loss: 0.027 | Tree loss: 0.892 | Accuracy: 0.655000 | 0.536 sec/iter\n",
      "Epoch: 152 | Batch: 007 / 011 | Total loss: 0.855 | Reg loss: 0.027 | Tree loss: 0.855 | Accuracy: 0.672000 | 0.536 sec/iter\n",
      "Epoch: 152 | Batch: 008 / 011 | Total loss: 0.865 | Reg loss: 0.027 | Tree loss: 0.865 | Accuracy: 0.658000 | 0.536 sec/iter\n",
      "Epoch: 152 | Batch: 009 / 011 | Total loss: 0.862 | Reg loss: 0.027 | Tree loss: 0.862 | Accuracy: 0.663000 | 0.536 sec/iter\n",
      "Epoch: 152 | Batch: 010 / 011 | Total loss: 0.829 | Reg loss: 0.027 | Tree loss: 0.829 | Accuracy: 0.699659 | 0.535 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 153 | Batch: 000 / 011 | Total loss: 0.940 | Reg loss: 0.027 | Tree loss: 0.940 | Accuracy: 0.635000 | 0.537 sec/iter\n",
      "Epoch: 153 | Batch: 001 / 011 | Total loss: 0.901 | Reg loss: 0.027 | Tree loss: 0.901 | Accuracy: 0.646500 | 0.537 sec/iter\n",
      "Epoch: 153 | Batch: 002 / 011 | Total loss: 0.916 | Reg loss: 0.027 | Tree loss: 0.916 | Accuracy: 0.646500 | 0.536 sec/iter\n",
      "Epoch: 153 | Batch: 003 / 011 | Total loss: 0.890 | Reg loss: 0.027 | Tree loss: 0.890 | Accuracy: 0.678500 | 0.536 sec/iter\n",
      "Epoch: 153 | Batch: 004 / 011 | Total loss: 0.866 | Reg loss: 0.027 | Tree loss: 0.866 | Accuracy: 0.702500 | 0.536 sec/iter\n",
      "Epoch: 153 | Batch: 005 / 011 | Total loss: 0.854 | Reg loss: 0.027 | Tree loss: 0.854 | Accuracy: 0.694000 | 0.536 sec/iter\n",
      "Epoch: 153 | Batch: 006 / 011 | Total loss: 0.841 | Reg loss: 0.027 | Tree loss: 0.841 | Accuracy: 0.709500 | 0.536 sec/iter\n",
      "Epoch: 153 | Batch: 007 / 011 | Total loss: 0.859 | Reg loss: 0.027 | Tree loss: 0.859 | Accuracy: 0.678500 | 0.535 sec/iter\n",
      "Epoch: 153 | Batch: 008 / 011 | Total loss: 0.864 | Reg loss: 0.027 | Tree loss: 0.864 | Accuracy: 0.679000 | 0.535 sec/iter\n",
      "Epoch: 153 | Batch: 009 / 011 | Total loss: 0.850 | Reg loss: 0.027 | Tree loss: 0.850 | Accuracy: 0.690000 | 0.535 sec/iter\n",
      "Epoch: 153 | Batch: 010 / 011 | Total loss: 0.821 | Reg loss: 0.027 | Tree loss: 0.821 | Accuracy: 0.709898 | 0.535 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 154 | Batch: 000 / 011 | Total loss: 0.944 | Reg loss: 0.027 | Tree loss: 0.944 | Accuracy: 0.642000 | 0.537 sec/iter\n",
      "Epoch: 154 | Batch: 001 / 011 | Total loss: 0.918 | Reg loss: 0.027 | Tree loss: 0.918 | Accuracy: 0.651000 | 0.537 sec/iter\n",
      "Epoch: 154 | Batch: 002 / 011 | Total loss: 0.903 | Reg loss: 0.027 | Tree loss: 0.903 | Accuracy: 0.652000 | 0.537 sec/iter\n",
      "Epoch: 154 | Batch: 003 / 011 | Total loss: 0.875 | Reg loss: 0.027 | Tree loss: 0.875 | Accuracy: 0.682500 | 0.536 sec/iter\n",
      "Epoch: 154 | Batch: 004 / 011 | Total loss: 0.865 | Reg loss: 0.027 | Tree loss: 0.865 | Accuracy: 0.706000 | 0.536 sec/iter\n",
      "Epoch: 154 | Batch: 005 / 011 | Total loss: 0.848 | Reg loss: 0.027 | Tree loss: 0.848 | Accuracy: 0.696000 | 0.536 sec/iter\n",
      "Epoch: 154 | Batch: 006 / 011 | Total loss: 0.855 | Reg loss: 0.027 | Tree loss: 0.855 | Accuracy: 0.689500 | 0.536 sec/iter\n",
      "Epoch: 154 | Batch: 007 / 011 | Total loss: 0.858 | Reg loss: 0.027 | Tree loss: 0.858 | Accuracy: 0.674500 | 0.536 sec/iter\n",
      "Epoch: 154 | Batch: 008 / 011 | Total loss: 0.858 | Reg loss: 0.027 | Tree loss: 0.858 | Accuracy: 0.682000 | 0.536 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 154 | Batch: 009 / 011 | Total loss: 0.856 | Reg loss: 0.027 | Tree loss: 0.856 | Accuracy: 0.673500 | 0.535 sec/iter\n",
      "Epoch: 154 | Batch: 010 / 011 | Total loss: 0.854 | Reg loss: 0.027 | Tree loss: 0.854 | Accuracy: 0.679181 | 0.535 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 155 | Batch: 000 / 011 | Total loss: 0.948 | Reg loss: 0.027 | Tree loss: 0.948 | Accuracy: 0.642500 | 0.535 sec/iter\n",
      "Epoch: 155 | Batch: 001 / 011 | Total loss: 0.907 | Reg loss: 0.027 | Tree loss: 0.907 | Accuracy: 0.642000 | 0.535 sec/iter\n",
      "Epoch: 155 | Batch: 002 / 011 | Total loss: 0.905 | Reg loss: 0.027 | Tree loss: 0.905 | Accuracy: 0.658000 | 0.535 sec/iter\n",
      "Epoch: 155 | Batch: 003 / 011 | Total loss: 0.863 | Reg loss: 0.027 | Tree loss: 0.863 | Accuracy: 0.691000 | 0.535 sec/iter\n",
      "Epoch: 155 | Batch: 004 / 011 | Total loss: 0.846 | Reg loss: 0.027 | Tree loss: 0.846 | Accuracy: 0.716000 | 0.535 sec/iter\n",
      "Epoch: 155 | Batch: 005 / 011 | Total loss: 0.853 | Reg loss: 0.027 | Tree loss: 0.853 | Accuracy: 0.695500 | 0.535 sec/iter\n",
      "Epoch: 155 | Batch: 006 / 011 | Total loss: 0.835 | Reg loss: 0.027 | Tree loss: 0.835 | Accuracy: 0.713500 | 0.534 sec/iter\n",
      "Epoch: 155 | Batch: 007 / 011 | Total loss: 0.875 | Reg loss: 0.027 | Tree loss: 0.875 | Accuracy: 0.674500 | 0.534 sec/iter\n",
      "Epoch: 155 | Batch: 008 / 011 | Total loss: 0.855 | Reg loss: 0.027 | Tree loss: 0.855 | Accuracy: 0.675500 | 0.534 sec/iter\n",
      "Epoch: 155 | Batch: 009 / 011 | Total loss: 0.850 | Reg loss: 0.027 | Tree loss: 0.850 | Accuracy: 0.688000 | 0.534 sec/iter\n",
      "Epoch: 155 | Batch: 010 / 011 | Total loss: 0.896 | Reg loss: 0.027 | Tree loss: 0.896 | Accuracy: 0.665529 | 0.534 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 156 | Batch: 000 / 011 | Total loss: 0.938 | Reg loss: 0.027 | Tree loss: 0.938 | Accuracy: 0.642500 | 0.535 sec/iter\n",
      "Epoch: 156 | Batch: 001 / 011 | Total loss: 0.925 | Reg loss: 0.027 | Tree loss: 0.925 | Accuracy: 0.648000 | 0.535 sec/iter\n",
      "Epoch: 156 | Batch: 002 / 011 | Total loss: 0.899 | Reg loss: 0.027 | Tree loss: 0.899 | Accuracy: 0.666000 | 0.535 sec/iter\n",
      "Epoch: 156 | Batch: 003 / 011 | Total loss: 0.873 | Reg loss: 0.027 | Tree loss: 0.873 | Accuracy: 0.695500 | 0.535 sec/iter\n",
      "Epoch: 156 | Batch: 004 / 011 | Total loss: 0.883 | Reg loss: 0.027 | Tree loss: 0.883 | Accuracy: 0.689000 | 0.535 sec/iter\n",
      "Epoch: 156 | Batch: 005 / 011 | Total loss: 0.842 | Reg loss: 0.027 | Tree loss: 0.842 | Accuracy: 0.704500 | 0.534 sec/iter\n",
      "Epoch: 156 | Batch: 006 / 011 | Total loss: 0.826 | Reg loss: 0.027 | Tree loss: 0.826 | Accuracy: 0.708500 | 0.534 sec/iter\n",
      "Epoch: 156 | Batch: 007 / 011 | Total loss: 0.844 | Reg loss: 0.027 | Tree loss: 0.844 | Accuracy: 0.682000 | 0.534 sec/iter\n",
      "Epoch: 156 | Batch: 008 / 011 | Total loss: 0.840 | Reg loss: 0.027 | Tree loss: 0.840 | Accuracy: 0.689000 | 0.534 sec/iter\n",
      "Epoch: 156 | Batch: 009 / 011 | Total loss: 0.857 | Reg loss: 0.027 | Tree loss: 0.857 | Accuracy: 0.688000 | 0.534 sec/iter\n",
      "Epoch: 156 | Batch: 010 / 011 | Total loss: 0.852 | Reg loss: 0.027 | Tree loss: 0.852 | Accuracy: 0.675768 | 0.534 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 157 | Batch: 000 / 011 | Total loss: 0.927 | Reg loss: 0.027 | Tree loss: 0.927 | Accuracy: 0.654500 | 0.536 sec/iter\n",
      "Epoch: 157 | Batch: 001 / 011 | Total loss: 0.921 | Reg loss: 0.027 | Tree loss: 0.921 | Accuracy: 0.656500 | 0.535 sec/iter\n",
      "Epoch: 157 | Batch: 002 / 011 | Total loss: 0.904 | Reg loss: 0.027 | Tree loss: 0.904 | Accuracy: 0.654500 | 0.535 sec/iter\n",
      "Epoch: 157 | Batch: 003 / 011 | Total loss: 0.878 | Reg loss: 0.027 | Tree loss: 0.878 | Accuracy: 0.689000 | 0.535 sec/iter\n",
      "Epoch: 157 | Batch: 004 / 011 | Total loss: 0.857 | Reg loss: 0.027 | Tree loss: 0.857 | Accuracy: 0.706500 | 0.535 sec/iter\n",
      "Epoch: 157 | Batch: 005 / 011 | Total loss: 0.856 | Reg loss: 0.027 | Tree loss: 0.856 | Accuracy: 0.704000 | 0.535 sec/iter\n",
      "Epoch: 157 | Batch: 006 / 011 | Total loss: 0.843 | Reg loss: 0.027 | Tree loss: 0.843 | Accuracy: 0.706000 | 0.535 sec/iter\n",
      "Epoch: 157 | Batch: 007 / 011 | Total loss: 0.851 | Reg loss: 0.027 | Tree loss: 0.851 | Accuracy: 0.679500 | 0.535 sec/iter\n",
      "Epoch: 157 | Batch: 008 / 011 | Total loss: 0.830 | Reg loss: 0.027 | Tree loss: 0.830 | Accuracy: 0.685500 | 0.534 sec/iter\n",
      "Epoch: 157 | Batch: 009 / 011 | Total loss: 0.855 | Reg loss: 0.027 | Tree loss: 0.855 | Accuracy: 0.669500 | 0.534 sec/iter\n",
      "Epoch: 157 | Batch: 010 / 011 | Total loss: 0.861 | Reg loss: 0.027 | Tree loss: 0.861 | Accuracy: 0.682594 | 0.534 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 158 | Batch: 000 / 011 | Total loss: 0.934 | Reg loss: 0.027 | Tree loss: 0.934 | Accuracy: 0.654500 | 0.537 sec/iter\n",
      "Epoch: 158 | Batch: 001 / 011 | Total loss: 0.915 | Reg loss: 0.027 | Tree loss: 0.915 | Accuracy: 0.658500 | 0.537 sec/iter\n",
      "Epoch: 158 | Batch: 002 / 011 | Total loss: 0.888 | Reg loss: 0.027 | Tree loss: 0.888 | Accuracy: 0.669000 | 0.537 sec/iter\n",
      "Epoch: 158 | Batch: 003 / 011 | Total loss: 0.856 | Reg loss: 0.027 | Tree loss: 0.856 | Accuracy: 0.680000 | 0.537 sec/iter\n",
      "Epoch: 158 | Batch: 004 / 011 | Total loss: 0.873 | Reg loss: 0.027 | Tree loss: 0.873 | Accuracy: 0.693500 | 0.537 sec/iter\n",
      "Epoch: 158 | Batch: 005 / 011 | Total loss: 0.873 | Reg loss: 0.027 | Tree loss: 0.873 | Accuracy: 0.687000 | 0.536 sec/iter\n",
      "Epoch: 158 | Batch: 006 / 011 | Total loss: 0.818 | Reg loss: 0.027 | Tree loss: 0.818 | Accuracy: 0.716000 | 0.536 sec/iter\n",
      "Epoch: 158 | Batch: 007 / 011 | Total loss: 0.868 | Reg loss: 0.027 | Tree loss: 0.868 | Accuracy: 0.684000 | 0.536 sec/iter\n",
      "Epoch: 158 | Batch: 008 / 011 | Total loss: 0.850 | Reg loss: 0.027 | Tree loss: 0.850 | Accuracy: 0.691500 | 0.536 sec/iter\n",
      "Epoch: 158 | Batch: 009 / 011 | Total loss: 0.826 | Reg loss: 0.027 | Tree loss: 0.826 | Accuracy: 0.702000 | 0.536 sec/iter\n",
      "Epoch: 158 | Batch: 010 / 011 | Total loss: 0.857 | Reg loss: 0.027 | Tree loss: 0.857 | Accuracy: 0.686007 | 0.535 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 159 | Batch: 000 / 011 | Total loss: 0.926 | Reg loss: 0.027 | Tree loss: 0.926 | Accuracy: 0.647500 | 0.537 sec/iter\n",
      "Epoch: 159 | Batch: 001 / 011 | Total loss: 0.930 | Reg loss: 0.027 | Tree loss: 0.930 | Accuracy: 0.641000 | 0.537 sec/iter\n",
      "Epoch: 159 | Batch: 002 / 011 | Total loss: 0.894 | Reg loss: 0.027 | Tree loss: 0.894 | Accuracy: 0.672500 | 0.537 sec/iter\n",
      "Epoch: 159 | Batch: 003 / 011 | Total loss: 0.874 | Reg loss: 0.027 | Tree loss: 0.874 | Accuracy: 0.692000 | 0.537 sec/iter\n",
      "Epoch: 159 | Batch: 004 / 011 | Total loss: 0.857 | Reg loss: 0.027 | Tree loss: 0.857 | Accuracy: 0.701500 | 0.537 sec/iter\n",
      "Epoch: 159 | Batch: 005 / 011 | Total loss: 0.858 | Reg loss: 0.027 | Tree loss: 0.858 | Accuracy: 0.686000 | 0.537 sec/iter\n",
      "Epoch: 159 | Batch: 006 / 011 | Total loss: 0.861 | Reg loss: 0.027 | Tree loss: 0.861 | Accuracy: 0.700500 | 0.536 sec/iter\n",
      "Epoch: 159 | Batch: 007 / 011 | Total loss: 0.835 | Reg loss: 0.027 | Tree loss: 0.835 | Accuracy: 0.702000 | 0.536 sec/iter\n",
      "Epoch: 159 | Batch: 008 / 011 | Total loss: 0.825 | Reg loss: 0.027 | Tree loss: 0.825 | Accuracy: 0.693500 | 0.536 sec/iter\n",
      "Epoch: 159 | Batch: 009 / 011 | Total loss: 0.826 | Reg loss: 0.027 | Tree loss: 0.826 | Accuracy: 0.692000 | 0.536 sec/iter\n",
      "Epoch: 159 | Batch: 010 / 011 | Total loss: 0.843 | Reg loss: 0.027 | Tree loss: 0.843 | Accuracy: 0.696246 | 0.536 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 160 | Batch: 000 / 011 | Total loss: 0.924 | Reg loss: 0.027 | Tree loss: 0.924 | Accuracy: 0.660000 | 0.538 sec/iter\n",
      "Epoch: 160 | Batch: 001 / 011 | Total loss: 0.914 | Reg loss: 0.027 | Tree loss: 0.914 | Accuracy: 0.654000 | 0.538 sec/iter\n",
      "Epoch: 160 | Batch: 002 / 011 | Total loss: 0.879 | Reg loss: 0.027 | Tree loss: 0.879 | Accuracy: 0.678000 | 0.537 sec/iter\n",
      "Epoch: 160 | Batch: 003 / 011 | Total loss: 0.859 | Reg loss: 0.027 | Tree loss: 0.859 | Accuracy: 0.690000 | 0.537 sec/iter\n",
      "Epoch: 160 | Batch: 004 / 011 | Total loss: 0.867 | Reg loss: 0.027 | Tree loss: 0.867 | Accuracy: 0.686000 | 0.537 sec/iter\n",
      "Epoch: 160 | Batch: 005 / 011 | Total loss: 0.850 | Reg loss: 0.027 | Tree loss: 0.850 | Accuracy: 0.701000 | 0.537 sec/iter\n",
      "Epoch: 160 | Batch: 006 / 011 | Total loss: 0.840 | Reg loss: 0.027 | Tree loss: 0.840 | Accuracy: 0.701500 | 0.537 sec/iter\n",
      "Epoch: 160 | Batch: 007 / 011 | Total loss: 0.846 | Reg loss: 0.027 | Tree loss: 0.846 | Accuracy: 0.692000 | 0.537 sec/iter\n",
      "Epoch: 160 | Batch: 008 / 011 | Total loss: 0.843 | Reg loss: 0.027 | Tree loss: 0.843 | Accuracy: 0.692000 | 0.536 sec/iter\n",
      "Epoch: 160 | Batch: 009 / 011 | Total loss: 0.851 | Reg loss: 0.027 | Tree loss: 0.851 | Accuracy: 0.682500 | 0.536 sec/iter\n",
      "Epoch: 160 | Batch: 010 / 011 | Total loss: 0.825 | Reg loss: 0.027 | Tree loss: 0.825 | Accuracy: 0.682594 | 0.536 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 161 | Batch: 000 / 011 | Total loss: 0.921 | Reg loss: 0.027 | Tree loss: 0.921 | Accuracy: 0.653500 | 0.539 sec/iter\n",
      "Epoch: 161 | Batch: 001 / 011 | Total loss: 0.925 | Reg loss: 0.027 | Tree loss: 0.925 | Accuracy: 0.649500 | 0.539 sec/iter\n",
      "Epoch: 161 | Batch: 002 / 011 | Total loss: 0.885 | Reg loss: 0.027 | Tree loss: 0.885 | Accuracy: 0.677500 | 0.538 sec/iter\n",
      "Epoch: 161 | Batch: 003 / 011 | Total loss: 0.862 | Reg loss: 0.027 | Tree loss: 0.862 | Accuracy: 0.697000 | 0.538 sec/iter\n",
      "Epoch: 161 | Batch: 004 / 011 | Total loss: 0.858 | Reg loss: 0.027 | Tree loss: 0.858 | Accuracy: 0.694500 | 0.538 sec/iter\n",
      "Epoch: 161 | Batch: 005 / 011 | Total loss: 0.839 | Reg loss: 0.027 | Tree loss: 0.839 | Accuracy: 0.703000 | 0.538 sec/iter\n",
      "Epoch: 161 | Batch: 006 / 011 | Total loss: 0.845 | Reg loss: 0.027 | Tree loss: 0.845 | Accuracy: 0.700000 | 0.538 sec/iter\n",
      "Epoch: 161 | Batch: 007 / 011 | Total loss: 0.845 | Reg loss: 0.027 | Tree loss: 0.845 | Accuracy: 0.703000 | 0.538 sec/iter\n",
      "Epoch: 161 | Batch: 008 / 011 | Total loss: 0.841 | Reg loss: 0.027 | Tree loss: 0.841 | Accuracy: 0.708000 | 0.537 sec/iter\n",
      "Epoch: 161 | Batch: 009 / 011 | Total loss: 0.831 | Reg loss: 0.027 | Tree loss: 0.831 | Accuracy: 0.699500 | 0.537 sec/iter\n",
      "Epoch: 161 | Batch: 010 / 011 | Total loss: 0.784 | Reg loss: 0.027 | Tree loss: 0.784 | Accuracy: 0.740614 | 0.537 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 162 | Batch: 000 / 011 | Total loss: 0.935 | Reg loss: 0.027 | Tree loss: 0.935 | Accuracy: 0.648000 | 0.54 sec/iter\n",
      "Epoch: 162 | Batch: 001 / 011 | Total loss: 0.899 | Reg loss: 0.027 | Tree loss: 0.899 | Accuracy: 0.669000 | 0.54 sec/iter\n",
      "Epoch: 162 | Batch: 002 / 011 | Total loss: 0.883 | Reg loss: 0.027 | Tree loss: 0.883 | Accuracy: 0.672500 | 0.54 sec/iter\n",
      "Epoch: 162 | Batch: 003 / 011 | Total loss: 0.870 | Reg loss: 0.027 | Tree loss: 0.870 | Accuracy: 0.694000 | 0.54 sec/iter\n",
      "Epoch: 162 | Batch: 004 / 011 | Total loss: 0.865 | Reg loss: 0.027 | Tree loss: 0.865 | Accuracy: 0.696000 | 0.539 sec/iter\n",
      "Epoch: 162 | Batch: 005 / 011 | Total loss: 0.848 | Reg loss: 0.027 | Tree loss: 0.848 | Accuracy: 0.702000 | 0.539 sec/iter\n",
      "Epoch: 162 | Batch: 006 / 011 | Total loss: 0.849 | Reg loss: 0.027 | Tree loss: 0.849 | Accuracy: 0.712500 | 0.539 sec/iter\n",
      "Epoch: 162 | Batch: 007 / 011 | Total loss: 0.817 | Reg loss: 0.027 | Tree loss: 0.817 | Accuracy: 0.715000 | 0.539 sec/iter\n",
      "Epoch: 162 | Batch: 008 / 011 | Total loss: 0.834 | Reg loss: 0.027 | Tree loss: 0.834 | Accuracy: 0.706000 | 0.539 sec/iter\n",
      "Epoch: 162 | Batch: 009 / 011 | Total loss: 0.835 | Reg loss: 0.027 | Tree loss: 0.835 | Accuracy: 0.695500 | 0.539 sec/iter\n",
      "Epoch: 162 | Batch: 010 / 011 | Total loss: 0.842 | Reg loss: 0.027 | Tree loss: 0.842 | Accuracy: 0.672355 | 0.538 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 163 | Batch: 000 / 011 | Total loss: 0.925 | Reg loss: 0.027 | Tree loss: 0.925 | Accuracy: 0.652500 | 0.539 sec/iter\n",
      "Epoch: 163 | Batch: 001 / 011 | Total loss: 0.908 | Reg loss: 0.027 | Tree loss: 0.908 | Accuracy: 0.662500 | 0.538 sec/iter\n",
      "Epoch: 163 | Batch: 002 / 011 | Total loss: 0.884 | Reg loss: 0.027 | Tree loss: 0.884 | Accuracy: 0.671000 | 0.538 sec/iter\n",
      "Epoch: 163 | Batch: 003 / 011 | Total loss: 0.873 | Reg loss: 0.027 | Tree loss: 0.873 | Accuracy: 0.685500 | 0.538 sec/iter\n",
      "Epoch: 163 | Batch: 004 / 011 | Total loss: 0.840 | Reg loss: 0.027 | Tree loss: 0.840 | Accuracy: 0.722000 | 0.538 sec/iter\n",
      "Epoch: 163 | Batch: 005 / 011 | Total loss: 0.857 | Reg loss: 0.027 | Tree loss: 0.857 | Accuracy: 0.696500 | 0.538 sec/iter\n",
      "Epoch: 163 | Batch: 006 / 011 | Total loss: 0.828 | Reg loss: 0.027 | Tree loss: 0.828 | Accuracy: 0.718000 | 0.537 sec/iter\n",
      "Epoch: 163 | Batch: 007 / 011 | Total loss: 0.865 | Reg loss: 0.027 | Tree loss: 0.865 | Accuracy: 0.691000 | 0.537 sec/iter\n",
      "Epoch: 163 | Batch: 008 / 011 | Total loss: 0.828 | Reg loss: 0.027 | Tree loss: 0.828 | Accuracy: 0.692500 | 0.537 sec/iter\n",
      "Epoch: 163 | Batch: 009 / 011 | Total loss: 0.840 | Reg loss: 0.027 | Tree loss: 0.840 | Accuracy: 0.700000 | 0.537 sec/iter\n",
      "Epoch: 163 | Batch: 010 / 011 | Total loss: 0.856 | Reg loss: 0.027 | Tree loss: 0.856 | Accuracy: 0.679181 | 0.537 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 164 | Batch: 000 / 011 | Total loss: 0.915 | Reg loss: 0.027 | Tree loss: 0.915 | Accuracy: 0.661500 | 0.538 sec/iter\n",
      "Epoch: 164 | Batch: 001 / 011 | Total loss: 0.918 | Reg loss: 0.027 | Tree loss: 0.918 | Accuracy: 0.652500 | 0.538 sec/iter\n",
      "Epoch: 164 | Batch: 002 / 011 | Total loss: 0.910 | Reg loss: 0.027 | Tree loss: 0.910 | Accuracy: 0.650000 | 0.538 sec/iter\n",
      "Epoch: 164 | Batch: 003 / 011 | Total loss: 0.845 | Reg loss: 0.027 | Tree loss: 0.845 | Accuracy: 0.696000 | 0.537 sec/iter\n",
      "Epoch: 164 | Batch: 004 / 011 | Total loss: 0.848 | Reg loss: 0.027 | Tree loss: 0.848 | Accuracy: 0.695000 | 0.537 sec/iter\n",
      "Epoch: 164 | Batch: 005 / 011 | Total loss: 0.847 | Reg loss: 0.027 | Tree loss: 0.847 | Accuracy: 0.700000 | 0.537 sec/iter\n",
      "Epoch: 164 | Batch: 006 / 011 | Total loss: 0.827 | Reg loss: 0.027 | Tree loss: 0.827 | Accuracy: 0.708000 | 0.537 sec/iter\n",
      "Epoch: 164 | Batch: 007 / 011 | Total loss: 0.857 | Reg loss: 0.027 | Tree loss: 0.857 | Accuracy: 0.703000 | 0.537 sec/iter\n",
      "Epoch: 164 | Batch: 008 / 011 | Total loss: 0.821 | Reg loss: 0.027 | Tree loss: 0.821 | Accuracy: 0.722000 | 0.536 sec/iter\n",
      "Epoch: 164 | Batch: 009 / 011 | Total loss: 0.823 | Reg loss: 0.027 | Tree loss: 0.823 | Accuracy: 0.711500 | 0.536 sec/iter\n",
      "Epoch: 164 | Batch: 010 / 011 | Total loss: 0.911 | Reg loss: 0.027 | Tree loss: 0.911 | Accuracy: 0.621160 | 0.536 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 6: 0.9723756906077349\n",
      "Epoch: 165 | Batch: 000 / 011 | Total loss: 0.918 | Reg loss: 0.027 | Tree loss: 0.918 | Accuracy: 0.655000 | 0.537 sec/iter\n",
      "Epoch: 165 | Batch: 001 / 011 | Total loss: 0.916 | Reg loss: 0.027 | Tree loss: 0.916 | Accuracy: 0.651000 | 0.537 sec/iter\n",
      "Epoch: 165 | Batch: 002 / 011 | Total loss: 0.891 | Reg loss: 0.027 | Tree loss: 0.891 | Accuracy: 0.676000 | 0.537 sec/iter\n",
      "Epoch: 165 | Batch: 003 / 011 | Total loss: 0.879 | Reg loss: 0.027 | Tree loss: 0.879 | Accuracy: 0.689000 | 0.537 sec/iter\n",
      "Epoch: 165 | Batch: 004 / 011 | Total loss: 0.842 | Reg loss: 0.027 | Tree loss: 0.842 | Accuracy: 0.713000 | 0.537 sec/iter\n",
      "Epoch: 165 | Batch: 005 / 011 | Total loss: 0.841 | Reg loss: 0.027 | Tree loss: 0.841 | Accuracy: 0.705000 | 0.536 sec/iter\n",
      "Epoch: 165 | Batch: 006 / 011 | Total loss: 0.822 | Reg loss: 0.027 | Tree loss: 0.822 | Accuracy: 0.733000 | 0.536 sec/iter\n",
      "Epoch: 165 | Batch: 007 / 011 | Total loss: 0.839 | Reg loss: 0.027 | Tree loss: 0.839 | Accuracy: 0.689000 | 0.536 sec/iter\n",
      "Epoch: 165 | Batch: 008 / 011 | Total loss: 0.832 | Reg loss: 0.027 | Tree loss: 0.832 | Accuracy: 0.695500 | 0.536 sec/iter\n",
      "Epoch: 165 | Batch: 009 / 011 | Total loss: 0.835 | Reg loss: 0.027 | Tree loss: 0.835 | Accuracy: 0.678500 | 0.536 sec/iter\n",
      "Epoch: 165 | Batch: 010 / 011 | Total loss: 0.791 | Reg loss: 0.027 | Tree loss: 0.791 | Accuracy: 0.764505 | 0.536 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 166 | Batch: 000 / 011 | Total loss: 0.902 | Reg loss: 0.027 | Tree loss: 0.902 | Accuracy: 0.667500 | 0.537 sec/iter\n",
      "Epoch: 166 | Batch: 001 / 011 | Total loss: 0.917 | Reg loss: 0.027 | Tree loss: 0.917 | Accuracy: 0.663000 | 0.536 sec/iter\n",
      "Epoch: 166 | Batch: 002 / 011 | Total loss: 0.894 | Reg loss: 0.027 | Tree loss: 0.894 | Accuracy: 0.673500 | 0.536 sec/iter\n",
      "Epoch: 166 | Batch: 003 / 011 | Total loss: 0.860 | Reg loss: 0.027 | Tree loss: 0.860 | Accuracy: 0.701000 | 0.536 sec/iter\n",
      "Epoch: 166 | Batch: 004 / 011 | Total loss: 0.838 | Reg loss: 0.027 | Tree loss: 0.838 | Accuracy: 0.709500 | 0.536 sec/iter\n",
      "Epoch: 166 | Batch: 005 / 011 | Total loss: 0.841 | Reg loss: 0.027 | Tree loss: 0.841 | Accuracy: 0.706500 | 0.536 sec/iter\n",
      "Epoch: 166 | Batch: 006 / 011 | Total loss: 0.836 | Reg loss: 0.027 | Tree loss: 0.836 | Accuracy: 0.721500 | 0.536 sec/iter\n",
      "Epoch: 166 | Batch: 007 / 011 | Total loss: 0.838 | Reg loss: 0.027 | Tree loss: 0.838 | Accuracy: 0.718500 | 0.535 sec/iter\n",
      "Epoch: 166 | Batch: 008 / 011 | Total loss: 0.838 | Reg loss: 0.027 | Tree loss: 0.838 | Accuracy: 0.709000 | 0.535 sec/iter\n",
      "Epoch: 166 | Batch: 009 / 011 | Total loss: 0.843 | Reg loss: 0.028 | Tree loss: 0.843 | Accuracy: 0.691500 | 0.535 sec/iter\n",
      "Epoch: 166 | Batch: 010 / 011 | Total loss: 0.758 | Reg loss: 0.028 | Tree loss: 0.758 | Accuracy: 0.737201 | 0.535 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 167 | Batch: 000 / 011 | Total loss: 0.933 | Reg loss: 0.027 | Tree loss: 0.933 | Accuracy: 0.663500 | 0.536 sec/iter\n",
      "Epoch: 167 | Batch: 001 / 011 | Total loss: 0.882 | Reg loss: 0.027 | Tree loss: 0.882 | Accuracy: 0.678500 | 0.536 sec/iter\n",
      "Epoch: 167 | Batch: 002 / 011 | Total loss: 0.872 | Reg loss: 0.027 | Tree loss: 0.872 | Accuracy: 0.687500 | 0.536 sec/iter\n",
      "Epoch: 167 | Batch: 003 / 011 | Total loss: 0.851 | Reg loss: 0.027 | Tree loss: 0.851 | Accuracy: 0.703500 | 0.536 sec/iter\n",
      "Epoch: 167 | Batch: 004 / 011 | Total loss: 0.853 | Reg loss: 0.027 | Tree loss: 0.853 | Accuracy: 0.696000 | 0.535 sec/iter\n",
      "Epoch: 167 | Batch: 005 / 011 | Total loss: 0.835 | Reg loss: 0.027 | Tree loss: 0.835 | Accuracy: 0.703000 | 0.535 sec/iter\n",
      "Epoch: 167 | Batch: 006 / 011 | Total loss: 0.827 | Reg loss: 0.027 | Tree loss: 0.827 | Accuracy: 0.723500 | 0.535 sec/iter\n",
      "Epoch: 167 | Batch: 007 / 011 | Total loss: 0.842 | Reg loss: 0.027 | Tree loss: 0.842 | Accuracy: 0.700000 | 0.535 sec/iter\n",
      "Epoch: 167 | Batch: 008 / 011 | Total loss: 0.844 | Reg loss: 0.028 | Tree loss: 0.844 | Accuracy: 0.684000 | 0.535 sec/iter\n",
      "Epoch: 167 | Batch: 009 / 011 | Total loss: 0.851 | Reg loss: 0.028 | Tree loss: 0.851 | Accuracy: 0.705000 | 0.535 sec/iter\n",
      "Epoch: 167 | Batch: 010 / 011 | Total loss: 0.827 | Reg loss: 0.028 | Tree loss: 0.827 | Accuracy: 0.716724 | 0.534 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 168 | Batch: 000 / 011 | Total loss: 0.923 | Reg loss: 0.027 | Tree loss: 0.923 | Accuracy: 0.661000 | 0.535 sec/iter\n",
      "Epoch: 168 | Batch: 001 / 011 | Total loss: 0.893 | Reg loss: 0.027 | Tree loss: 0.893 | Accuracy: 0.665500 | 0.534 sec/iter\n",
      "Epoch: 168 | Batch: 002 / 011 | Total loss: 0.893 | Reg loss: 0.027 | Tree loss: 0.893 | Accuracy: 0.673000 | 0.534 sec/iter\n",
      "Epoch: 168 | Batch: 003 / 011 | Total loss: 0.863 | Reg loss: 0.027 | Tree loss: 0.863 | Accuracy: 0.687500 | 0.534 sec/iter\n",
      "Epoch: 168 | Batch: 004 / 011 | Total loss: 0.844 | Reg loss: 0.027 | Tree loss: 0.844 | Accuracy: 0.717000 | 0.534 sec/iter\n",
      "Epoch: 168 | Batch: 005 / 011 | Total loss: 0.846 | Reg loss: 0.027 | Tree loss: 0.846 | Accuracy: 0.716000 | 0.534 sec/iter\n",
      "Epoch: 168 | Batch: 006 / 011 | Total loss: 0.845 | Reg loss: 0.027 | Tree loss: 0.845 | Accuracy: 0.719000 | 0.534 sec/iter\n",
      "Epoch: 168 | Batch: 007 / 011 | Total loss: 0.832 | Reg loss: 0.027 | Tree loss: 0.832 | Accuracy: 0.723000 | 0.533 sec/iter\n",
      "Epoch: 168 | Batch: 008 / 011 | Total loss: 0.819 | Reg loss: 0.028 | Tree loss: 0.819 | Accuracy: 0.720500 | 0.533 sec/iter\n",
      "Epoch: 168 | Batch: 009 / 011 | Total loss: 0.809 | Reg loss: 0.028 | Tree loss: 0.809 | Accuracy: 0.716500 | 0.533 sec/iter\n",
      "Epoch: 168 | Batch: 010 / 011 | Total loss: 0.816 | Reg loss: 0.028 | Tree loss: 0.816 | Accuracy: 0.730375 | 0.533 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 169 | Batch: 000 / 011 | Total loss: 0.919 | Reg loss: 0.027 | Tree loss: 0.919 | Accuracy: 0.661500 | 0.536 sec/iter\n",
      "Epoch: 169 | Batch: 001 / 011 | Total loss: 0.903 | Reg loss: 0.027 | Tree loss: 0.903 | Accuracy: 0.657500 | 0.536 sec/iter\n",
      "Epoch: 169 | Batch: 002 / 011 | Total loss: 0.879 | Reg loss: 0.027 | Tree loss: 0.879 | Accuracy: 0.692500 | 0.536 sec/iter\n",
      "Epoch: 169 | Batch: 003 / 011 | Total loss: 0.859 | Reg loss: 0.027 | Tree loss: 0.859 | Accuracy: 0.691500 | 0.536 sec/iter\n",
      "Epoch: 169 | Batch: 004 / 011 | Total loss: 0.834 | Reg loss: 0.027 | Tree loss: 0.834 | Accuracy: 0.721000 | 0.536 sec/iter\n",
      "Epoch: 169 | Batch: 005 / 011 | Total loss: 0.831 | Reg loss: 0.027 | Tree loss: 0.831 | Accuracy: 0.702000 | 0.536 sec/iter\n",
      "Epoch: 169 | Batch: 006 / 011 | Total loss: 0.843 | Reg loss: 0.028 | Tree loss: 0.843 | Accuracy: 0.702000 | 0.535 sec/iter\n",
      "Epoch: 169 | Batch: 007 / 011 | Total loss: 0.823 | Reg loss: 0.028 | Tree loss: 0.823 | Accuracy: 0.712500 | 0.535 sec/iter\n",
      "Epoch: 169 | Batch: 008 / 011 | Total loss: 0.829 | Reg loss: 0.028 | Tree loss: 0.829 | Accuracy: 0.702000 | 0.535 sec/iter\n",
      "Epoch: 169 | Batch: 009 / 011 | Total loss: 0.855 | Reg loss: 0.028 | Tree loss: 0.855 | Accuracy: 0.708500 | 0.535 sec/iter\n",
      "Epoch: 169 | Batch: 010 / 011 | Total loss: 0.885 | Reg loss: 0.028 | Tree loss: 0.885 | Accuracy: 0.686007 | 0.535 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 170 | Batch: 000 / 011 | Total loss: 0.936 | Reg loss: 0.027 | Tree loss: 0.936 | Accuracy: 0.658500 | 0.535 sec/iter\n",
      "Epoch: 170 | Batch: 001 / 011 | Total loss: 0.904 | Reg loss: 0.027 | Tree loss: 0.904 | Accuracy: 0.657000 | 0.535 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 170 | Batch: 002 / 011 | Total loss: 0.894 | Reg loss: 0.027 | Tree loss: 0.894 | Accuracy: 0.658500 | 0.535 sec/iter\n",
      "Epoch: 170 | Batch: 003 / 011 | Total loss: 0.854 | Reg loss: 0.027 | Tree loss: 0.854 | Accuracy: 0.705000 | 0.535 sec/iter\n",
      "Epoch: 170 | Batch: 004 / 011 | Total loss: 0.831 | Reg loss: 0.027 | Tree loss: 0.831 | Accuracy: 0.709000 | 0.535 sec/iter\n",
      "Epoch: 170 | Batch: 005 / 011 | Total loss: 0.833 | Reg loss: 0.027 | Tree loss: 0.833 | Accuracy: 0.706000 | 0.535 sec/iter\n",
      "Epoch: 170 | Batch: 006 / 011 | Total loss: 0.820 | Reg loss: 0.028 | Tree loss: 0.820 | Accuracy: 0.715000 | 0.534 sec/iter\n",
      "Epoch: 170 | Batch: 007 / 011 | Total loss: 0.845 | Reg loss: 0.028 | Tree loss: 0.845 | Accuracy: 0.705500 | 0.534 sec/iter\n",
      "Epoch: 170 | Batch: 008 / 011 | Total loss: 0.844 | Reg loss: 0.028 | Tree loss: 0.844 | Accuracy: 0.716500 | 0.534 sec/iter\n",
      "Epoch: 170 | Batch: 009 / 011 | Total loss: 0.841 | Reg loss: 0.028 | Tree loss: 0.841 | Accuracy: 0.710000 | 0.534 sec/iter\n",
      "Epoch: 170 | Batch: 010 / 011 | Total loss: 0.786 | Reg loss: 0.028 | Tree loss: 0.786 | Accuracy: 0.740614 | 0.534 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 171 | Batch: 000 / 011 | Total loss: 0.922 | Reg loss: 0.027 | Tree loss: 0.922 | Accuracy: 0.661000 | 0.536 sec/iter\n",
      "Epoch: 171 | Batch: 001 / 011 | Total loss: 0.901 | Reg loss: 0.027 | Tree loss: 0.901 | Accuracy: 0.673000 | 0.536 sec/iter\n",
      "Epoch: 171 | Batch: 002 / 011 | Total loss: 0.879 | Reg loss: 0.027 | Tree loss: 0.879 | Accuracy: 0.670500 | 0.536 sec/iter\n",
      "Epoch: 171 | Batch: 003 / 011 | Total loss: 0.862 | Reg loss: 0.027 | Tree loss: 0.862 | Accuracy: 0.697000 | 0.536 sec/iter\n",
      "Epoch: 171 | Batch: 004 / 011 | Total loss: 0.857 | Reg loss: 0.027 | Tree loss: 0.857 | Accuracy: 0.696000 | 0.536 sec/iter\n",
      "Epoch: 171 | Batch: 005 / 011 | Total loss: 0.828 | Reg loss: 0.028 | Tree loss: 0.828 | Accuracy: 0.707500 | 0.535 sec/iter\n",
      "Epoch: 171 | Batch: 006 / 011 | Total loss: 0.831 | Reg loss: 0.028 | Tree loss: 0.831 | Accuracy: 0.707000 | 0.535 sec/iter\n",
      "Epoch: 171 | Batch: 007 / 011 | Total loss: 0.828 | Reg loss: 0.028 | Tree loss: 0.828 | Accuracy: 0.698500 | 0.535 sec/iter\n",
      "Epoch: 171 | Batch: 008 / 011 | Total loss: 0.838 | Reg loss: 0.028 | Tree loss: 0.838 | Accuracy: 0.701000 | 0.535 sec/iter\n",
      "Epoch: 171 | Batch: 009 / 011 | Total loss: 0.836 | Reg loss: 0.028 | Tree loss: 0.836 | Accuracy: 0.708000 | 0.535 sec/iter\n",
      "Epoch: 171 | Batch: 010 / 011 | Total loss: 0.846 | Reg loss: 0.028 | Tree loss: 0.846 | Accuracy: 0.699659 | 0.535 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 172 | Batch: 000 / 011 | Total loss: 0.926 | Reg loss: 0.027 | Tree loss: 0.926 | Accuracy: 0.644500 | 0.535 sec/iter\n",
      "Epoch: 172 | Batch: 001 / 011 | Total loss: 0.894 | Reg loss: 0.027 | Tree loss: 0.894 | Accuracy: 0.670500 | 0.535 sec/iter\n",
      "Epoch: 172 | Batch: 002 / 011 | Total loss: 0.871 | Reg loss: 0.027 | Tree loss: 0.871 | Accuracy: 0.676500 | 0.534 sec/iter\n",
      "Epoch: 172 | Batch: 003 / 011 | Total loss: 0.872 | Reg loss: 0.027 | Tree loss: 0.872 | Accuracy: 0.688000 | 0.534 sec/iter\n",
      "Epoch: 172 | Batch: 004 / 011 | Total loss: 0.847 | Reg loss: 0.028 | Tree loss: 0.847 | Accuracy: 0.702000 | 0.534 sec/iter\n",
      "Epoch: 172 | Batch: 005 / 011 | Total loss: 0.837 | Reg loss: 0.028 | Tree loss: 0.837 | Accuracy: 0.708500 | 0.534 sec/iter\n",
      "Epoch: 172 | Batch: 006 / 011 | Total loss: 0.816 | Reg loss: 0.028 | Tree loss: 0.816 | Accuracy: 0.726000 | 0.534 sec/iter\n",
      "Epoch: 172 | Batch: 007 / 011 | Total loss: 0.841 | Reg loss: 0.028 | Tree loss: 0.841 | Accuracy: 0.714500 | 0.534 sec/iter\n",
      "Epoch: 172 | Batch: 008 / 011 | Total loss: 0.831 | Reg loss: 0.028 | Tree loss: 0.831 | Accuracy: 0.713500 | 0.534 sec/iter\n",
      "Epoch: 172 | Batch: 009 / 011 | Total loss: 0.837 | Reg loss: 0.028 | Tree loss: 0.837 | Accuracy: 0.698500 | 0.534 sec/iter\n",
      "Epoch: 172 | Batch: 010 / 011 | Total loss: 0.829 | Reg loss: 0.028 | Tree loss: 0.829 | Accuracy: 0.696246 | 0.533 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 173 | Batch: 000 / 011 | Total loss: 0.899 | Reg loss: 0.027 | Tree loss: 0.899 | Accuracy: 0.665000 | 0.537 sec/iter\n",
      "Epoch: 173 | Batch: 001 / 011 | Total loss: 0.891 | Reg loss: 0.028 | Tree loss: 0.891 | Accuracy: 0.670500 | 0.536 sec/iter\n",
      "Epoch: 173 | Batch: 002 / 011 | Total loss: 0.875 | Reg loss: 0.028 | Tree loss: 0.875 | Accuracy: 0.674500 | 0.536 sec/iter\n",
      "Epoch: 173 | Batch: 003 / 011 | Total loss: 0.854 | Reg loss: 0.028 | Tree loss: 0.854 | Accuracy: 0.704500 | 0.536 sec/iter\n",
      "Epoch: 173 | Batch: 004 / 011 | Total loss: 0.860 | Reg loss: 0.028 | Tree loss: 0.860 | Accuracy: 0.714000 | 0.536 sec/iter\n",
      "Epoch: 173 | Batch: 005 / 011 | Total loss: 0.834 | Reg loss: 0.028 | Tree loss: 0.834 | Accuracy: 0.703000 | 0.536 sec/iter\n",
      "Epoch: 173 | Batch: 006 / 011 | Total loss: 0.837 | Reg loss: 0.028 | Tree loss: 0.837 | Accuracy: 0.714500 | 0.536 sec/iter\n",
      "Epoch: 173 | Batch: 007 / 011 | Total loss: 0.831 | Reg loss: 0.028 | Tree loss: 0.831 | Accuracy: 0.704500 | 0.536 sec/iter\n",
      "Epoch: 173 | Batch: 008 / 011 | Total loss: 0.844 | Reg loss: 0.028 | Tree loss: 0.844 | Accuracy: 0.696000 | 0.535 sec/iter\n",
      "Epoch: 173 | Batch: 009 / 011 | Total loss: 0.845 | Reg loss: 0.028 | Tree loss: 0.845 | Accuracy: 0.689000 | 0.535 sec/iter\n",
      "Epoch: 173 | Batch: 010 / 011 | Total loss: 0.891 | Reg loss: 0.028 | Tree loss: 0.891 | Accuracy: 0.689420 | 0.535 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 174 | Batch: 000 / 011 | Total loss: 0.902 | Reg loss: 0.028 | Tree loss: 0.902 | Accuracy: 0.671000 | 0.537 sec/iter\n",
      "Epoch: 174 | Batch: 001 / 011 | Total loss: 0.886 | Reg loss: 0.028 | Tree loss: 0.886 | Accuracy: 0.680000 | 0.537 sec/iter\n",
      "Epoch: 174 | Batch: 002 / 011 | Total loss: 0.888 | Reg loss: 0.028 | Tree loss: 0.888 | Accuracy: 0.682500 | 0.537 sec/iter\n",
      "Epoch: 174 | Batch: 003 / 011 | Total loss: 0.867 | Reg loss: 0.028 | Tree loss: 0.867 | Accuracy: 0.681500 | 0.537 sec/iter\n",
      "Epoch: 174 | Batch: 004 / 011 | Total loss: 0.852 | Reg loss: 0.028 | Tree loss: 0.852 | Accuracy: 0.709500 | 0.536 sec/iter\n",
      "Epoch: 174 | Batch: 005 / 011 | Total loss: 0.827 | Reg loss: 0.028 | Tree loss: 0.827 | Accuracy: 0.723500 | 0.536 sec/iter\n",
      "Epoch: 174 | Batch: 006 / 011 | Total loss: 0.842 | Reg loss: 0.028 | Tree loss: 0.842 | Accuracy: 0.708000 | 0.536 sec/iter\n",
      "Epoch: 174 | Batch: 007 / 011 | Total loss: 0.835 | Reg loss: 0.028 | Tree loss: 0.835 | Accuracy: 0.718500 | 0.536 sec/iter\n",
      "Epoch: 174 | Batch: 008 / 011 | Total loss: 0.840 | Reg loss: 0.028 | Tree loss: 0.840 | Accuracy: 0.707000 | 0.536 sec/iter\n",
      "Epoch: 174 | Batch: 009 / 011 | Total loss: 0.817 | Reg loss: 0.028 | Tree loss: 0.817 | Accuracy: 0.713500 | 0.536 sec/iter\n",
      "Epoch: 174 | Batch: 010 / 011 | Total loss: 0.779 | Reg loss: 0.028 | Tree loss: 0.779 | Accuracy: 0.740614 | 0.535 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 175 | Batch: 000 / 011 | Total loss: 0.915 | Reg loss: 0.028 | Tree loss: 0.915 | Accuracy: 0.645000 | 0.535 sec/iter\n",
      "Epoch: 175 | Batch: 001 / 011 | Total loss: 0.904 | Reg loss: 0.028 | Tree loss: 0.904 | Accuracy: 0.667000 | 0.535 sec/iter\n",
      "Epoch: 175 | Batch: 002 / 011 | Total loss: 0.893 | Reg loss: 0.028 | Tree loss: 0.893 | Accuracy: 0.661500 | 0.535 sec/iter\n",
      "Epoch: 175 | Batch: 003 / 011 | Total loss: 0.856 | Reg loss: 0.028 | Tree loss: 0.856 | Accuracy: 0.705000 | 0.535 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 175 | Batch: 004 / 011 | Total loss: 0.830 | Reg loss: 0.028 | Tree loss: 0.830 | Accuracy: 0.719000 | 0.535 sec/iter\n",
      "Epoch: 175 | Batch: 005 / 011 | Total loss: 0.841 | Reg loss: 0.028 | Tree loss: 0.841 | Accuracy: 0.705000 | 0.535 sec/iter\n",
      "Epoch: 175 | Batch: 006 / 011 | Total loss: 0.815 | Reg loss: 0.028 | Tree loss: 0.815 | Accuracy: 0.711500 | 0.535 sec/iter\n",
      "Epoch: 175 | Batch: 007 / 011 | Total loss: 0.811 | Reg loss: 0.028 | Tree loss: 0.811 | Accuracy: 0.721500 | 0.534 sec/iter\n",
      "Epoch: 175 | Batch: 008 / 011 | Total loss: 0.858 | Reg loss: 0.028 | Tree loss: 0.858 | Accuracy: 0.695500 | 0.534 sec/iter\n",
      "Epoch: 175 | Batch: 009 / 011 | Total loss: 0.827 | Reg loss: 0.028 | Tree loss: 0.827 | Accuracy: 0.709500 | 0.534 sec/iter\n",
      "Epoch: 175 | Batch: 010 / 011 | Total loss: 0.852 | Reg loss: 0.028 | Tree loss: 0.852 | Accuracy: 0.675768 | 0.534 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 176 | Batch: 000 / 011 | Total loss: 0.916 | Reg loss: 0.028 | Tree loss: 0.916 | Accuracy: 0.652500 | 0.536 sec/iter\n",
      "Epoch: 176 | Batch: 001 / 011 | Total loss: 0.903 | Reg loss: 0.028 | Tree loss: 0.903 | Accuracy: 0.652000 | 0.536 sec/iter\n",
      "Epoch: 176 | Batch: 002 / 011 | Total loss: 0.867 | Reg loss: 0.028 | Tree loss: 0.867 | Accuracy: 0.692000 | 0.535 sec/iter\n",
      "Epoch: 176 | Batch: 003 / 011 | Total loss: 0.854 | Reg loss: 0.028 | Tree loss: 0.854 | Accuracy: 0.723000 | 0.535 sec/iter\n",
      "Epoch: 176 | Batch: 004 / 011 | Total loss: 0.853 | Reg loss: 0.028 | Tree loss: 0.853 | Accuracy: 0.708000 | 0.535 sec/iter\n",
      "Epoch: 176 | Batch: 005 / 011 | Total loss: 0.821 | Reg loss: 0.028 | Tree loss: 0.821 | Accuracy: 0.726000 | 0.535 sec/iter\n",
      "Epoch: 176 | Batch: 006 / 011 | Total loss: 0.853 | Reg loss: 0.028 | Tree loss: 0.853 | Accuracy: 0.706500 | 0.535 sec/iter\n",
      "Epoch: 176 | Batch: 007 / 011 | Total loss: 0.843 | Reg loss: 0.028 | Tree loss: 0.843 | Accuracy: 0.700500 | 0.535 sec/iter\n",
      "Epoch: 176 | Batch: 008 / 011 | Total loss: 0.826 | Reg loss: 0.028 | Tree loss: 0.826 | Accuracy: 0.723000 | 0.534 sec/iter\n",
      "Epoch: 176 | Batch: 009 / 011 | Total loss: 0.807 | Reg loss: 0.028 | Tree loss: 0.807 | Accuracy: 0.716500 | 0.534 sec/iter\n",
      "Epoch: 176 | Batch: 010 / 011 | Total loss: 0.791 | Reg loss: 0.028 | Tree loss: 0.791 | Accuracy: 0.726962 | 0.534 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 177 | Batch: 000 / 011 | Total loss: 0.923 | Reg loss: 0.028 | Tree loss: 0.923 | Accuracy: 0.635500 | 0.536 sec/iter\n",
      "Epoch: 177 | Batch: 001 / 011 | Total loss: 0.899 | Reg loss: 0.028 | Tree loss: 0.899 | Accuracy: 0.661000 | 0.536 sec/iter\n",
      "Epoch: 177 | Batch: 002 / 011 | Total loss: 0.882 | Reg loss: 0.028 | Tree loss: 0.882 | Accuracy: 0.680000 | 0.535 sec/iter\n",
      "Epoch: 177 | Batch: 003 / 011 | Total loss: 0.851 | Reg loss: 0.028 | Tree loss: 0.851 | Accuracy: 0.705500 | 0.535 sec/iter\n",
      "Epoch: 177 | Batch: 004 / 011 | Total loss: 0.829 | Reg loss: 0.028 | Tree loss: 0.829 | Accuracy: 0.717000 | 0.535 sec/iter\n",
      "Epoch: 177 | Batch: 005 / 011 | Total loss: 0.838 | Reg loss: 0.028 | Tree loss: 0.838 | Accuracy: 0.701500 | 0.535 sec/iter\n",
      "Epoch: 177 | Batch: 006 / 011 | Total loss: 0.840 | Reg loss: 0.028 | Tree loss: 0.840 | Accuracy: 0.711000 | 0.535 sec/iter\n",
      "Epoch: 177 | Batch: 007 / 011 | Total loss: 0.829 | Reg loss: 0.028 | Tree loss: 0.829 | Accuracy: 0.700500 | 0.535 sec/iter\n",
      "Epoch: 177 | Batch: 008 / 011 | Total loss: 0.852 | Reg loss: 0.028 | Tree loss: 0.852 | Accuracy: 0.693000 | 0.534 sec/iter\n",
      "Epoch: 177 | Batch: 009 / 011 | Total loss: 0.816 | Reg loss: 0.028 | Tree loss: 0.816 | Accuracy: 0.713000 | 0.534 sec/iter\n",
      "Epoch: 177 | Batch: 010 / 011 | Total loss: 0.828 | Reg loss: 0.028 | Tree loss: 0.828 | Accuracy: 0.713311 | 0.534 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 178 | Batch: 000 / 011 | Total loss: 0.899 | Reg loss: 0.028 | Tree loss: 0.899 | Accuracy: 0.668000 | 0.534 sec/iter\n",
      "Epoch: 178 | Batch: 001 / 011 | Total loss: 0.893 | Reg loss: 0.028 | Tree loss: 0.893 | Accuracy: 0.672000 | 0.534 sec/iter\n",
      "Epoch: 178 | Batch: 002 / 011 | Total loss: 0.866 | Reg loss: 0.028 | Tree loss: 0.866 | Accuracy: 0.685000 | 0.534 sec/iter\n",
      "Epoch: 178 | Batch: 003 / 011 | Total loss: 0.885 | Reg loss: 0.028 | Tree loss: 0.885 | Accuracy: 0.687500 | 0.534 sec/iter\n",
      "Epoch: 178 | Batch: 004 / 011 | Total loss: 0.846 | Reg loss: 0.028 | Tree loss: 0.846 | Accuracy: 0.710000 | 0.534 sec/iter\n",
      "Epoch: 178 | Batch: 005 / 011 | Total loss: 0.828 | Reg loss: 0.028 | Tree loss: 0.828 | Accuracy: 0.725000 | 0.533 sec/iter\n",
      "Epoch: 178 | Batch: 006 / 011 | Total loss: 0.809 | Reg loss: 0.028 | Tree loss: 0.809 | Accuracy: 0.747500 | 0.533 sec/iter\n",
      "Epoch: 178 | Batch: 007 / 011 | Total loss: 0.824 | Reg loss: 0.028 | Tree loss: 0.824 | Accuracy: 0.724000 | 0.533 sec/iter\n",
      "Epoch: 178 | Batch: 008 / 011 | Total loss: 0.819 | Reg loss: 0.028 | Tree loss: 0.819 | Accuracy: 0.732000 | 0.533 sec/iter\n",
      "Epoch: 178 | Batch: 009 / 011 | Total loss: 0.837 | Reg loss: 0.028 | Tree loss: 0.837 | Accuracy: 0.712000 | 0.533 sec/iter\n",
      "Epoch: 178 | Batch: 010 / 011 | Total loss: 0.846 | Reg loss: 0.028 | Tree loss: 0.846 | Accuracy: 0.699659 | 0.533 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 179 | Batch: 000 / 011 | Total loss: 0.915 | Reg loss: 0.028 | Tree loss: 0.915 | Accuracy: 0.652500 | 0.534 sec/iter\n",
      "Epoch: 179 | Batch: 001 / 011 | Total loss: 0.883 | Reg loss: 0.028 | Tree loss: 0.883 | Accuracy: 0.695500 | 0.534 sec/iter\n",
      "Epoch: 179 | Batch: 002 / 011 | Total loss: 0.879 | Reg loss: 0.028 | Tree loss: 0.879 | Accuracy: 0.688500 | 0.534 sec/iter\n",
      "Epoch: 179 | Batch: 003 / 011 | Total loss: 0.854 | Reg loss: 0.028 | Tree loss: 0.854 | Accuracy: 0.698000 | 0.534 sec/iter\n",
      "Epoch: 179 | Batch: 004 / 011 | Total loss: 0.854 | Reg loss: 0.028 | Tree loss: 0.854 | Accuracy: 0.694000 | 0.534 sec/iter\n",
      "Epoch: 179 | Batch: 005 / 011 | Total loss: 0.830 | Reg loss: 0.028 | Tree loss: 0.830 | Accuracy: 0.703500 | 0.534 sec/iter\n",
      "Epoch: 179 | Batch: 006 / 011 | Total loss: 0.816 | Reg loss: 0.028 | Tree loss: 0.816 | Accuracy: 0.712500 | 0.533 sec/iter\n",
      "Epoch: 179 | Batch: 007 / 011 | Total loss: 0.849 | Reg loss: 0.028 | Tree loss: 0.849 | Accuracy: 0.681500 | 0.533 sec/iter\n",
      "Epoch: 179 | Batch: 008 / 011 | Total loss: 0.837 | Reg loss: 0.028 | Tree loss: 0.837 | Accuracy: 0.692500 | 0.533 sec/iter\n",
      "Epoch: 179 | Batch: 009 / 011 | Total loss: 0.826 | Reg loss: 0.028 | Tree loss: 0.826 | Accuracy: 0.707000 | 0.533 sec/iter\n",
      "Epoch: 179 | Batch: 010 / 011 | Total loss: 0.837 | Reg loss: 0.028 | Tree loss: 0.837 | Accuracy: 0.682594 | 0.533 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 180 | Batch: 000 / 011 | Total loss: 0.885 | Reg loss: 0.028 | Tree loss: 0.885 | Accuracy: 0.681000 | 0.535 sec/iter\n",
      "Epoch: 180 | Batch: 001 / 011 | Total loss: 0.922 | Reg loss: 0.028 | Tree loss: 0.922 | Accuracy: 0.653000 | 0.534 sec/iter\n",
      "Epoch: 180 | Batch: 002 / 011 | Total loss: 0.894 | Reg loss: 0.028 | Tree loss: 0.894 | Accuracy: 0.674000 | 0.534 sec/iter\n",
      "Epoch: 180 | Batch: 003 / 011 | Total loss: 0.847 | Reg loss: 0.028 | Tree loss: 0.847 | Accuracy: 0.699000 | 0.534 sec/iter\n",
      "Epoch: 180 | Batch: 004 / 011 | Total loss: 0.848 | Reg loss: 0.028 | Tree loss: 0.848 | Accuracy: 0.713500 | 0.534 sec/iter\n",
      "Epoch: 180 | Batch: 005 / 011 | Total loss: 0.826 | Reg loss: 0.028 | Tree loss: 0.826 | Accuracy: 0.745500 | 0.534 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 180 | Batch: 006 / 011 | Total loss: 0.813 | Reg loss: 0.028 | Tree loss: 0.813 | Accuracy: 0.743000 | 0.534 sec/iter\n",
      "Epoch: 180 | Batch: 007 / 011 | Total loss: 0.811 | Reg loss: 0.028 | Tree loss: 0.811 | Accuracy: 0.730500 | 0.533 sec/iter\n",
      "Epoch: 180 | Batch: 008 / 011 | Total loss: 0.829 | Reg loss: 0.028 | Tree loss: 0.829 | Accuracy: 0.714500 | 0.533 sec/iter\n",
      "Epoch: 180 | Batch: 009 / 011 | Total loss: 0.819 | Reg loss: 0.028 | Tree loss: 0.819 | Accuracy: 0.728000 | 0.533 sec/iter\n",
      "Epoch: 180 | Batch: 010 / 011 | Total loss: 0.842 | Reg loss: 0.028 | Tree loss: 0.842 | Accuracy: 0.703072 | 0.533 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 181 | Batch: 000 / 011 | Total loss: 0.900 | Reg loss: 0.028 | Tree loss: 0.900 | Accuracy: 0.653500 | 0.533 sec/iter\n",
      "Epoch: 181 | Batch: 001 / 011 | Total loss: 0.886 | Reg loss: 0.028 | Tree loss: 0.886 | Accuracy: 0.670500 | 0.533 sec/iter\n",
      "Epoch: 181 | Batch: 002 / 011 | Total loss: 0.857 | Reg loss: 0.028 | Tree loss: 0.857 | Accuracy: 0.690500 | 0.533 sec/iter\n",
      "Epoch: 181 | Batch: 003 / 011 | Total loss: 0.860 | Reg loss: 0.028 | Tree loss: 0.860 | Accuracy: 0.693000 | 0.533 sec/iter\n",
      "Epoch: 181 | Batch: 004 / 011 | Total loss: 0.825 | Reg loss: 0.028 | Tree loss: 0.825 | Accuracy: 0.711500 | 0.533 sec/iter\n",
      "Epoch: 181 | Batch: 005 / 011 | Total loss: 0.841 | Reg loss: 0.028 | Tree loss: 0.841 | Accuracy: 0.698500 | 0.532 sec/iter\n",
      "Epoch: 181 | Batch: 006 / 011 | Total loss: 0.832 | Reg loss: 0.028 | Tree loss: 0.832 | Accuracy: 0.698000 | 0.532 sec/iter\n",
      "Epoch: 181 | Batch: 007 / 011 | Total loss: 0.852 | Reg loss: 0.028 | Tree loss: 0.852 | Accuracy: 0.687500 | 0.532 sec/iter\n",
      "Epoch: 181 | Batch: 008 / 011 | Total loss: 0.830 | Reg loss: 0.028 | Tree loss: 0.830 | Accuracy: 0.711500 | 0.532 sec/iter\n",
      "Epoch: 181 | Batch: 009 / 011 | Total loss: 0.838 | Reg loss: 0.028 | Tree loss: 0.838 | Accuracy: 0.703500 | 0.532 sec/iter\n",
      "Epoch: 181 | Batch: 010 / 011 | Total loss: 0.791 | Reg loss: 0.028 | Tree loss: 0.791 | Accuracy: 0.730375 | 0.532 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 182 | Batch: 000 / 011 | Total loss: 0.905 | Reg loss: 0.028 | Tree loss: 0.905 | Accuracy: 0.664000 | 0.535 sec/iter\n",
      "Epoch: 182 | Batch: 001 / 011 | Total loss: 0.895 | Reg loss: 0.028 | Tree loss: 0.895 | Accuracy: 0.663000 | 0.534 sec/iter\n",
      "Epoch: 182 | Batch: 002 / 011 | Total loss: 0.891 | Reg loss: 0.028 | Tree loss: 0.891 | Accuracy: 0.668500 | 0.534 sec/iter\n",
      "Epoch: 182 | Batch: 003 / 011 | Total loss: 0.861 | Reg loss: 0.028 | Tree loss: 0.861 | Accuracy: 0.697000 | 0.534 sec/iter\n",
      "Epoch: 182 | Batch: 004 / 011 | Total loss: 0.829 | Reg loss: 0.028 | Tree loss: 0.829 | Accuracy: 0.714000 | 0.534 sec/iter\n",
      "Epoch: 182 | Batch: 005 / 011 | Total loss: 0.832 | Reg loss: 0.028 | Tree loss: 0.832 | Accuracy: 0.712000 | 0.534 sec/iter\n",
      "Epoch: 182 | Batch: 006 / 011 | Total loss: 0.804 | Reg loss: 0.028 | Tree loss: 0.804 | Accuracy: 0.741000 | 0.534 sec/iter\n",
      "Epoch: 182 | Batch: 007 / 011 | Total loss: 0.838 | Reg loss: 0.028 | Tree loss: 0.838 | Accuracy: 0.700000 | 0.534 sec/iter\n",
      "Epoch: 182 | Batch: 008 / 011 | Total loss: 0.818 | Reg loss: 0.028 | Tree loss: 0.818 | Accuracy: 0.717000 | 0.533 sec/iter\n",
      "Epoch: 182 | Batch: 009 / 011 | Total loss: 0.819 | Reg loss: 0.028 | Tree loss: 0.819 | Accuracy: 0.728000 | 0.533 sec/iter\n",
      "Epoch: 182 | Batch: 010 / 011 | Total loss: 0.784 | Reg loss: 0.028 | Tree loss: 0.784 | Accuracy: 0.764505 | 0.533 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 183 | Batch: 000 / 011 | Total loss: 0.914 | Reg loss: 0.028 | Tree loss: 0.914 | Accuracy: 0.657500 | 0.534 sec/iter\n",
      "Epoch: 183 | Batch: 001 / 011 | Total loss: 0.883 | Reg loss: 0.028 | Tree loss: 0.883 | Accuracy: 0.680500 | 0.534 sec/iter\n",
      "Epoch: 183 | Batch: 002 / 011 | Total loss: 0.878 | Reg loss: 0.028 | Tree loss: 0.878 | Accuracy: 0.683000 | 0.533 sec/iter\n",
      "Epoch: 183 | Batch: 003 / 011 | Total loss: 0.842 | Reg loss: 0.028 | Tree loss: 0.842 | Accuracy: 0.697500 | 0.533 sec/iter\n",
      "Epoch: 183 | Batch: 004 / 011 | Total loss: 0.867 | Reg loss: 0.028 | Tree loss: 0.867 | Accuracy: 0.706000 | 0.533 sec/iter\n",
      "Epoch: 183 | Batch: 005 / 011 | Total loss: 0.796 | Reg loss: 0.028 | Tree loss: 0.796 | Accuracy: 0.720500 | 0.533 sec/iter\n",
      "Epoch: 183 | Batch: 006 / 011 | Total loss: 0.827 | Reg loss: 0.028 | Tree loss: 0.827 | Accuracy: 0.713500 | 0.533 sec/iter\n",
      "Epoch: 183 | Batch: 007 / 011 | Total loss: 0.824 | Reg loss: 0.028 | Tree loss: 0.824 | Accuracy: 0.709500 | 0.533 sec/iter\n",
      "Epoch: 183 | Batch: 008 / 011 | Total loss: 0.834 | Reg loss: 0.028 | Tree loss: 0.834 | Accuracy: 0.703500 | 0.533 sec/iter\n",
      "Epoch: 183 | Batch: 009 / 011 | Total loss: 0.828 | Reg loss: 0.028 | Tree loss: 0.828 | Accuracy: 0.701500 | 0.533 sec/iter\n",
      "Epoch: 183 | Batch: 010 / 011 | Total loss: 0.828 | Reg loss: 0.028 | Tree loss: 0.828 | Accuracy: 0.713311 | 0.532 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 184 | Batch: 000 / 011 | Total loss: 0.905 | Reg loss: 0.028 | Tree loss: 0.905 | Accuracy: 0.663000 | 0.535 sec/iter\n",
      "Epoch: 184 | Batch: 001 / 011 | Total loss: 0.886 | Reg loss: 0.028 | Tree loss: 0.886 | Accuracy: 0.670000 | 0.535 sec/iter\n",
      "Epoch: 184 | Batch: 002 / 011 | Total loss: 0.883 | Reg loss: 0.028 | Tree loss: 0.883 | Accuracy: 0.663000 | 0.535 sec/iter\n",
      "Epoch: 184 | Batch: 003 / 011 | Total loss: 0.860 | Reg loss: 0.028 | Tree loss: 0.860 | Accuracy: 0.689000 | 0.534 sec/iter\n",
      "Epoch: 184 | Batch: 004 / 011 | Total loss: 0.840 | Reg loss: 0.028 | Tree loss: 0.840 | Accuracy: 0.723000 | 0.534 sec/iter\n",
      "Epoch: 184 | Batch: 005 / 011 | Total loss: 0.824 | Reg loss: 0.028 | Tree loss: 0.824 | Accuracy: 0.732000 | 0.534 sec/iter\n",
      "Epoch: 184 | Batch: 006 / 011 | Total loss: 0.808 | Reg loss: 0.028 | Tree loss: 0.808 | Accuracy: 0.742500 | 0.534 sec/iter\n",
      "Epoch: 184 | Batch: 007 / 011 | Total loss: 0.804 | Reg loss: 0.028 | Tree loss: 0.804 | Accuracy: 0.730000 | 0.534 sec/iter\n",
      "Epoch: 184 | Batch: 008 / 011 | Total loss: 0.821 | Reg loss: 0.028 | Tree loss: 0.821 | Accuracy: 0.725500 | 0.534 sec/iter\n",
      "Epoch: 184 | Batch: 009 / 011 | Total loss: 0.844 | Reg loss: 0.028 | Tree loss: 0.844 | Accuracy: 0.703500 | 0.533 sec/iter\n",
      "Epoch: 184 | Batch: 010 / 011 | Total loss: 0.824 | Reg loss: 0.028 | Tree loss: 0.824 | Accuracy: 0.750853 | 0.533 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 185 | Batch: 000 / 011 | Total loss: 0.903 | Reg loss: 0.028 | Tree loss: 0.903 | Accuracy: 0.670000 | 0.535 sec/iter\n",
      "Epoch: 185 | Batch: 001 / 011 | Total loss: 0.898 | Reg loss: 0.028 | Tree loss: 0.898 | Accuracy: 0.649000 | 0.535 sec/iter\n",
      "Epoch: 185 | Batch: 002 / 011 | Total loss: 0.874 | Reg loss: 0.028 | Tree loss: 0.874 | Accuracy: 0.678500 | 0.534 sec/iter\n",
      "Epoch: 185 | Batch: 003 / 011 | Total loss: 0.838 | Reg loss: 0.028 | Tree loss: 0.838 | Accuracy: 0.701000 | 0.534 sec/iter\n",
      "Epoch: 185 | Batch: 004 / 011 | Total loss: 0.850 | Reg loss: 0.028 | Tree loss: 0.850 | Accuracy: 0.695500 | 0.534 sec/iter\n",
      "Epoch: 185 | Batch: 005 / 011 | Total loss: 0.821 | Reg loss: 0.028 | Tree loss: 0.821 | Accuracy: 0.720500 | 0.534 sec/iter\n",
      "Epoch: 185 | Batch: 006 / 011 | Total loss: 0.816 | Reg loss: 0.028 | Tree loss: 0.816 | Accuracy: 0.712000 | 0.534 sec/iter\n",
      "Epoch: 185 | Batch: 007 / 011 | Total loss: 0.835 | Reg loss: 0.028 | Tree loss: 0.835 | Accuracy: 0.709500 | 0.534 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 185 | Batch: 008 / 011 | Total loss: 0.802 | Reg loss: 0.028 | Tree loss: 0.802 | Accuracy: 0.723500 | 0.533 sec/iter\n",
      "Epoch: 185 | Batch: 009 / 011 | Total loss: 0.849 | Reg loss: 0.028 | Tree loss: 0.849 | Accuracy: 0.704500 | 0.533 sec/iter\n",
      "Epoch: 185 | Batch: 010 / 011 | Total loss: 0.784 | Reg loss: 0.028 | Tree loss: 0.784 | Accuracy: 0.740614 | 0.533 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 186 | Batch: 000 / 011 | Total loss: 0.899 | Reg loss: 0.028 | Tree loss: 0.899 | Accuracy: 0.661500 | 0.534 sec/iter\n",
      "Epoch: 186 | Batch: 001 / 011 | Total loss: 0.897 | Reg loss: 0.028 | Tree loss: 0.897 | Accuracy: 0.659000 | 0.534 sec/iter\n",
      "Epoch: 186 | Batch: 002 / 011 | Total loss: 0.877 | Reg loss: 0.028 | Tree loss: 0.877 | Accuracy: 0.676500 | 0.533 sec/iter\n",
      "Epoch: 186 | Batch: 003 / 011 | Total loss: 0.863 | Reg loss: 0.028 | Tree loss: 0.863 | Accuracy: 0.689000 | 0.533 sec/iter\n",
      "Epoch: 186 | Batch: 004 / 011 | Total loss: 0.860 | Reg loss: 0.028 | Tree loss: 0.860 | Accuracy: 0.704000 | 0.533 sec/iter\n",
      "Epoch: 186 | Batch: 005 / 011 | Total loss: 0.812 | Reg loss: 0.028 | Tree loss: 0.812 | Accuracy: 0.722000 | 0.533 sec/iter\n",
      "Epoch: 186 | Batch: 006 / 011 | Total loss: 0.802 | Reg loss: 0.028 | Tree loss: 0.802 | Accuracy: 0.733000 | 0.533 sec/iter\n",
      "Epoch: 186 | Batch: 007 / 011 | Total loss: 0.834 | Reg loss: 0.028 | Tree loss: 0.834 | Accuracy: 0.705000 | 0.533 sec/iter\n",
      "Epoch: 186 | Batch: 008 / 011 | Total loss: 0.824 | Reg loss: 0.028 | Tree loss: 0.824 | Accuracy: 0.714000 | 0.532 sec/iter\n",
      "Epoch: 186 | Batch: 009 / 011 | Total loss: 0.816 | Reg loss: 0.028 | Tree loss: 0.816 | Accuracy: 0.710500 | 0.532 sec/iter\n",
      "Epoch: 186 | Batch: 010 / 011 | Total loss: 0.832 | Reg loss: 0.028 | Tree loss: 0.832 | Accuracy: 0.737201 | 0.532 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 187 | Batch: 000 / 011 | Total loss: 0.924 | Reg loss: 0.028 | Tree loss: 0.924 | Accuracy: 0.644000 | 0.534 sec/iter\n",
      "Epoch: 187 | Batch: 001 / 011 | Total loss: 0.882 | Reg loss: 0.028 | Tree loss: 0.882 | Accuracy: 0.660500 | 0.534 sec/iter\n",
      "Epoch: 187 | Batch: 002 / 011 | Total loss: 0.863 | Reg loss: 0.028 | Tree loss: 0.863 | Accuracy: 0.692500 | 0.534 sec/iter\n",
      "Epoch: 187 | Batch: 003 / 011 | Total loss: 0.860 | Reg loss: 0.028 | Tree loss: 0.860 | Accuracy: 0.683000 | 0.533 sec/iter\n",
      "Epoch: 187 | Batch: 004 / 011 | Total loss: 0.816 | Reg loss: 0.028 | Tree loss: 0.816 | Accuracy: 0.733500 | 0.533 sec/iter\n",
      "Epoch: 187 | Batch: 005 / 011 | Total loss: 0.845 | Reg loss: 0.028 | Tree loss: 0.845 | Accuracy: 0.710500 | 0.533 sec/iter\n",
      "Epoch: 187 | Batch: 006 / 011 | Total loss: 0.818 | Reg loss: 0.028 | Tree loss: 0.818 | Accuracy: 0.732500 | 0.533 sec/iter\n",
      "Epoch: 187 | Batch: 007 / 011 | Total loss: 0.806 | Reg loss: 0.028 | Tree loss: 0.806 | Accuracy: 0.744500 | 0.533 sec/iter\n",
      "Epoch: 187 | Batch: 008 / 011 | Total loss: 0.829 | Reg loss: 0.028 | Tree loss: 0.829 | Accuracy: 0.712500 | 0.533 sec/iter\n",
      "Epoch: 187 | Batch: 009 / 011 | Total loss: 0.818 | Reg loss: 0.028 | Tree loss: 0.818 | Accuracy: 0.719000 | 0.532 sec/iter\n",
      "Epoch: 187 | Batch: 010 / 011 | Total loss: 0.804 | Reg loss: 0.028 | Tree loss: 0.804 | Accuracy: 0.703072 | 0.532 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 188 | Batch: 000 / 011 | Total loss: 0.916 | Reg loss: 0.028 | Tree loss: 0.916 | Accuracy: 0.662500 | 0.532 sec/iter\n",
      "Epoch: 188 | Batch: 001 / 011 | Total loss: 0.889 | Reg loss: 0.028 | Tree loss: 0.889 | Accuracy: 0.660500 | 0.532 sec/iter\n",
      "Epoch: 188 | Batch: 002 / 011 | Total loss: 0.892 | Reg loss: 0.028 | Tree loss: 0.892 | Accuracy: 0.661500 | 0.532 sec/iter\n",
      "Epoch: 188 | Batch: 003 / 011 | Total loss: 0.835 | Reg loss: 0.028 | Tree loss: 0.835 | Accuracy: 0.712500 | 0.532 sec/iter\n",
      "Epoch: 188 | Batch: 004 / 011 | Total loss: 0.833 | Reg loss: 0.028 | Tree loss: 0.833 | Accuracy: 0.722500 | 0.532 sec/iter\n",
      "Epoch: 188 | Batch: 005 / 011 | Total loss: 0.833 | Reg loss: 0.028 | Tree loss: 0.833 | Accuracy: 0.688000 | 0.532 sec/iter\n",
      "Epoch: 188 | Batch: 006 / 011 | Total loss: 0.822 | Reg loss: 0.028 | Tree loss: 0.822 | Accuracy: 0.717000 | 0.531 sec/iter\n",
      "Epoch: 188 | Batch: 007 / 011 | Total loss: 0.801 | Reg loss: 0.028 | Tree loss: 0.801 | Accuracy: 0.729000 | 0.531 sec/iter\n",
      "Epoch: 188 | Batch: 008 / 011 | Total loss: 0.833 | Reg loss: 0.028 | Tree loss: 0.833 | Accuracy: 0.708500 | 0.531 sec/iter\n",
      "Epoch: 188 | Batch: 009 / 011 | Total loss: 0.799 | Reg loss: 0.028 | Tree loss: 0.799 | Accuracy: 0.720500 | 0.531 sec/iter\n",
      "Epoch: 188 | Batch: 010 / 011 | Total loss: 0.813 | Reg loss: 0.028 | Tree loss: 0.813 | Accuracy: 0.713311 | 0.531 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 189 | Batch: 000 / 011 | Total loss: 0.898 | Reg loss: 0.028 | Tree loss: 0.898 | Accuracy: 0.664500 | 0.533 sec/iter\n",
      "Epoch: 189 | Batch: 001 / 011 | Total loss: 0.902 | Reg loss: 0.028 | Tree loss: 0.902 | Accuracy: 0.672500 | 0.532 sec/iter\n",
      "Epoch: 189 | Batch: 002 / 011 | Total loss: 0.879 | Reg loss: 0.028 | Tree loss: 0.879 | Accuracy: 0.679500 | 0.532 sec/iter\n",
      "Epoch: 189 | Batch: 003 / 011 | Total loss: 0.861 | Reg loss: 0.028 | Tree loss: 0.861 | Accuracy: 0.695500 | 0.532 sec/iter\n",
      "Epoch: 189 | Batch: 004 / 011 | Total loss: 0.823 | Reg loss: 0.028 | Tree loss: 0.823 | Accuracy: 0.731500 | 0.532 sec/iter\n",
      "Epoch: 189 | Batch: 005 / 011 | Total loss: 0.816 | Reg loss: 0.028 | Tree loss: 0.816 | Accuracy: 0.729000 | 0.532 sec/iter\n",
      "Epoch: 189 | Batch: 006 / 011 | Total loss: 0.801 | Reg loss: 0.028 | Tree loss: 0.801 | Accuracy: 0.745000 | 0.532 sec/iter\n",
      "Epoch: 189 | Batch: 007 / 011 | Total loss: 0.798 | Reg loss: 0.028 | Tree loss: 0.798 | Accuracy: 0.731500 | 0.532 sec/iter\n",
      "Epoch: 189 | Batch: 008 / 011 | Total loss: 0.828 | Reg loss: 0.028 | Tree loss: 0.828 | Accuracy: 0.716500 | 0.531 sec/iter\n",
      "Epoch: 189 | Batch: 009 / 011 | Total loss: 0.825 | Reg loss: 0.028 | Tree loss: 0.825 | Accuracy: 0.705500 | 0.531 sec/iter\n",
      "Epoch: 189 | Batch: 010 / 011 | Total loss: 0.828 | Reg loss: 0.028 | Tree loss: 0.828 | Accuracy: 0.703072 | 0.531 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 190 | Batch: 000 / 011 | Total loss: 0.915 | Reg loss: 0.028 | Tree loss: 0.915 | Accuracy: 0.658500 | 0.531 sec/iter\n",
      "Epoch: 190 | Batch: 001 / 011 | Total loss: 0.875 | Reg loss: 0.028 | Tree loss: 0.875 | Accuracy: 0.674000 | 0.531 sec/iter\n",
      "Epoch: 190 | Batch: 002 / 011 | Total loss: 0.883 | Reg loss: 0.028 | Tree loss: 0.883 | Accuracy: 0.673000 | 0.531 sec/iter\n",
      "Epoch: 190 | Batch: 003 / 011 | Total loss: 0.846 | Reg loss: 0.028 | Tree loss: 0.846 | Accuracy: 0.693000 | 0.531 sec/iter\n",
      "Epoch: 190 | Batch: 004 / 011 | Total loss: 0.840 | Reg loss: 0.028 | Tree loss: 0.840 | Accuracy: 0.708000 | 0.531 sec/iter\n",
      "Epoch: 190 | Batch: 005 / 011 | Total loss: 0.810 | Reg loss: 0.028 | Tree loss: 0.810 | Accuracy: 0.726500 | 0.531 sec/iter\n",
      "Epoch: 190 | Batch: 006 / 011 | Total loss: 0.816 | Reg loss: 0.028 | Tree loss: 0.816 | Accuracy: 0.718000 | 0.53 sec/iter\n",
      "Epoch: 190 | Batch: 007 / 011 | Total loss: 0.821 | Reg loss: 0.028 | Tree loss: 0.821 | Accuracy: 0.722000 | 0.53 sec/iter\n",
      "Epoch: 190 | Batch: 008 / 011 | Total loss: 0.806 | Reg loss: 0.028 | Tree loss: 0.806 | Accuracy: 0.727000 | 0.53 sec/iter\n",
      "Epoch: 190 | Batch: 009 / 011 | Total loss: 0.826 | Reg loss: 0.028 | Tree loss: 0.826 | Accuracy: 0.706500 | 0.53 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 190 | Batch: 010 / 011 | Total loss: 0.858 | Reg loss: 0.028 | Tree loss: 0.858 | Accuracy: 0.730375 | 0.53 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 191 | Batch: 000 / 011 | Total loss: 0.888 | Reg loss: 0.028 | Tree loss: 0.888 | Accuracy: 0.664500 | 0.531 sec/iter\n",
      "Epoch: 191 | Batch: 001 / 011 | Total loss: 0.897 | Reg loss: 0.028 | Tree loss: 0.897 | Accuracy: 0.663000 | 0.531 sec/iter\n",
      "Epoch: 191 | Batch: 002 / 011 | Total loss: 0.864 | Reg loss: 0.028 | Tree loss: 0.864 | Accuracy: 0.676500 | 0.531 sec/iter\n",
      "Epoch: 191 | Batch: 003 / 011 | Total loss: 0.854 | Reg loss: 0.028 | Tree loss: 0.854 | Accuracy: 0.703000 | 0.531 sec/iter\n",
      "Epoch: 191 | Batch: 004 / 011 | Total loss: 0.827 | Reg loss: 0.028 | Tree loss: 0.827 | Accuracy: 0.719000 | 0.53 sec/iter\n",
      "Epoch: 191 | Batch: 005 / 011 | Total loss: 0.835 | Reg loss: 0.028 | Tree loss: 0.835 | Accuracy: 0.707500 | 0.53 sec/iter\n",
      "Epoch: 191 | Batch: 006 / 011 | Total loss: 0.824 | Reg loss: 0.028 | Tree loss: 0.824 | Accuracy: 0.710500 | 0.53 sec/iter\n",
      "Epoch: 191 | Batch: 007 / 011 | Total loss: 0.815 | Reg loss: 0.028 | Tree loss: 0.815 | Accuracy: 0.719500 | 0.53 sec/iter\n",
      "Epoch: 191 | Batch: 008 / 011 | Total loss: 0.820 | Reg loss: 0.028 | Tree loss: 0.820 | Accuracy: 0.725000 | 0.53 sec/iter\n",
      "Epoch: 191 | Batch: 009 / 011 | Total loss: 0.826 | Reg loss: 0.028 | Tree loss: 0.826 | Accuracy: 0.716500 | 0.53 sec/iter\n",
      "Epoch: 191 | Batch: 010 / 011 | Total loss: 0.811 | Reg loss: 0.028 | Tree loss: 0.811 | Accuracy: 0.720137 | 0.529 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 192 | Batch: 000 / 011 | Total loss: 0.888 | Reg loss: 0.028 | Tree loss: 0.888 | Accuracy: 0.661000 | 0.53 sec/iter\n",
      "Epoch: 192 | Batch: 001 / 011 | Total loss: 0.885 | Reg loss: 0.028 | Tree loss: 0.885 | Accuracy: 0.675500 | 0.53 sec/iter\n",
      "Epoch: 192 | Batch: 002 / 011 | Total loss: 0.888 | Reg loss: 0.028 | Tree loss: 0.888 | Accuracy: 0.659000 | 0.53 sec/iter\n",
      "Epoch: 192 | Batch: 003 / 011 | Total loss: 0.876 | Reg loss: 0.028 | Tree loss: 0.876 | Accuracy: 0.679500 | 0.53 sec/iter\n",
      "Epoch: 192 | Batch: 004 / 011 | Total loss: 0.832 | Reg loss: 0.028 | Tree loss: 0.832 | Accuracy: 0.714000 | 0.53 sec/iter\n",
      "Epoch: 192 | Batch: 005 / 011 | Total loss: 0.832 | Reg loss: 0.028 | Tree loss: 0.832 | Accuracy: 0.709000 | 0.53 sec/iter\n",
      "Epoch: 192 | Batch: 006 / 011 | Total loss: 0.812 | Reg loss: 0.028 | Tree loss: 0.812 | Accuracy: 0.726000 | 0.53 sec/iter\n",
      "Epoch: 192 | Batch: 007 / 011 | Total loss: 0.811 | Reg loss: 0.028 | Tree loss: 0.811 | Accuracy: 0.728000 | 0.53 sec/iter\n",
      "Epoch: 192 | Batch: 008 / 011 | Total loss: 0.798 | Reg loss: 0.028 | Tree loss: 0.798 | Accuracy: 0.727000 | 0.529 sec/iter\n",
      "Epoch: 192 | Batch: 009 / 011 | Total loss: 0.799 | Reg loss: 0.028 | Tree loss: 0.799 | Accuracy: 0.727000 | 0.529 sec/iter\n",
      "Epoch: 192 | Batch: 010 / 011 | Total loss: 0.854 | Reg loss: 0.028 | Tree loss: 0.854 | Accuracy: 0.713311 | 0.529 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 193 | Batch: 000 / 011 | Total loss: 0.914 | Reg loss: 0.028 | Tree loss: 0.914 | Accuracy: 0.651000 | 0.532 sec/iter\n",
      "Epoch: 193 | Batch: 001 / 011 | Total loss: 0.885 | Reg loss: 0.028 | Tree loss: 0.885 | Accuracy: 0.655000 | 0.532 sec/iter\n",
      "Epoch: 193 | Batch: 002 / 011 | Total loss: 0.871 | Reg loss: 0.028 | Tree loss: 0.871 | Accuracy: 0.691500 | 0.531 sec/iter\n",
      "Epoch: 193 | Batch: 003 / 011 | Total loss: 0.835 | Reg loss: 0.028 | Tree loss: 0.835 | Accuracy: 0.707500 | 0.531 sec/iter\n",
      "Epoch: 193 | Batch: 004 / 011 | Total loss: 0.833 | Reg loss: 0.028 | Tree loss: 0.833 | Accuracy: 0.729500 | 0.531 sec/iter\n",
      "Epoch: 193 | Batch: 005 / 011 | Total loss: 0.831 | Reg loss: 0.028 | Tree loss: 0.831 | Accuracy: 0.726000 | 0.531 sec/iter\n",
      "Epoch: 193 | Batch: 006 / 011 | Total loss: 0.814 | Reg loss: 0.028 | Tree loss: 0.814 | Accuracy: 0.723000 | 0.531 sec/iter\n",
      "Epoch: 193 | Batch: 007 / 011 | Total loss: 0.814 | Reg loss: 0.028 | Tree loss: 0.814 | Accuracy: 0.725000 | 0.531 sec/iter\n",
      "Epoch: 193 | Batch: 008 / 011 | Total loss: 0.812 | Reg loss: 0.028 | Tree loss: 0.812 | Accuracy: 0.728500 | 0.531 sec/iter\n",
      "Epoch: 193 | Batch: 009 / 011 | Total loss: 0.829 | Reg loss: 0.028 | Tree loss: 0.829 | Accuracy: 0.702500 | 0.531 sec/iter\n",
      "Epoch: 193 | Batch: 010 / 011 | Total loss: 0.733 | Reg loss: 0.028 | Tree loss: 0.733 | Accuracy: 0.754266 | 0.53 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 194 | Batch: 000 / 011 | Total loss: 0.877 | Reg loss: 0.028 | Tree loss: 0.877 | Accuracy: 0.675500 | 0.533 sec/iter\n",
      "Epoch: 194 | Batch: 001 / 011 | Total loss: 0.916 | Reg loss: 0.028 | Tree loss: 0.916 | Accuracy: 0.654000 | 0.532 sec/iter\n",
      "Epoch: 194 | Batch: 002 / 011 | Total loss: 0.898 | Reg loss: 0.028 | Tree loss: 0.898 | Accuracy: 0.647000 | 0.532 sec/iter\n",
      "Epoch: 194 | Batch: 003 / 011 | Total loss: 0.849 | Reg loss: 0.028 | Tree loss: 0.849 | Accuracy: 0.702000 | 0.532 sec/iter\n",
      "Epoch: 194 | Batch: 004 / 011 | Total loss: 0.816 | Reg loss: 0.028 | Tree loss: 0.816 | Accuracy: 0.714000 | 0.532 sec/iter\n",
      "Epoch: 194 | Batch: 005 / 011 | Total loss: 0.817 | Reg loss: 0.028 | Tree loss: 0.817 | Accuracy: 0.708500 | 0.532 sec/iter\n",
      "Epoch: 194 | Batch: 006 / 011 | Total loss: 0.807 | Reg loss: 0.028 | Tree loss: 0.807 | Accuracy: 0.711000 | 0.532 sec/iter\n",
      "Epoch: 194 | Batch: 007 / 011 | Total loss: 0.806 | Reg loss: 0.028 | Tree loss: 0.806 | Accuracy: 0.716500 | 0.532 sec/iter\n",
      "Epoch: 194 | Batch: 008 / 011 | Total loss: 0.820 | Reg loss: 0.028 | Tree loss: 0.820 | Accuracy: 0.722000 | 0.531 sec/iter\n",
      "Epoch: 194 | Batch: 009 / 011 | Total loss: 0.830 | Reg loss: 0.028 | Tree loss: 0.830 | Accuracy: 0.702500 | 0.531 sec/iter\n",
      "Epoch: 194 | Batch: 010 / 011 | Total loss: 0.789 | Reg loss: 0.028 | Tree loss: 0.789 | Accuracy: 0.771331 | 0.531 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 195 | Batch: 000 / 011 | Total loss: 0.906 | Reg loss: 0.028 | Tree loss: 0.906 | Accuracy: 0.662500 | 0.531 sec/iter\n",
      "Epoch: 195 | Batch: 001 / 011 | Total loss: 0.909 | Reg loss: 0.028 | Tree loss: 0.909 | Accuracy: 0.654000 | 0.531 sec/iter\n",
      "Epoch: 195 | Batch: 002 / 011 | Total loss: 0.871 | Reg loss: 0.028 | Tree loss: 0.871 | Accuracy: 0.676500 | 0.531 sec/iter\n",
      "Epoch: 195 | Batch: 003 / 011 | Total loss: 0.839 | Reg loss: 0.028 | Tree loss: 0.839 | Accuracy: 0.709500 | 0.531 sec/iter\n",
      "Epoch: 195 | Batch: 004 / 011 | Total loss: 0.832 | Reg loss: 0.028 | Tree loss: 0.832 | Accuracy: 0.718500 | 0.531 sec/iter\n",
      "Epoch: 195 | Batch: 005 / 011 | Total loss: 0.834 | Reg loss: 0.028 | Tree loss: 0.834 | Accuracy: 0.712500 | 0.531 sec/iter\n",
      "Epoch: 195 | Batch: 006 / 011 | Total loss: 0.796 | Reg loss: 0.028 | Tree loss: 0.796 | Accuracy: 0.745000 | 0.53 sec/iter\n",
      "Epoch: 195 | Batch: 007 / 011 | Total loss: 0.817 | Reg loss: 0.028 | Tree loss: 0.817 | Accuracy: 0.715000 | 0.53 sec/iter\n",
      "Epoch: 195 | Batch: 008 / 011 | Total loss: 0.811 | Reg loss: 0.028 | Tree loss: 0.811 | Accuracy: 0.713000 | 0.53 sec/iter\n",
      "Epoch: 195 | Batch: 009 / 011 | Total loss: 0.797 | Reg loss: 0.028 | Tree loss: 0.797 | Accuracy: 0.724000 | 0.53 sec/iter\n",
      "Epoch: 195 | Batch: 010 / 011 | Total loss: 0.850 | Reg loss: 0.028 | Tree loss: 0.850 | Accuracy: 0.689420 | 0.53 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 196 | Batch: 000 / 011 | Total loss: 0.887 | Reg loss: 0.028 | Tree loss: 0.887 | Accuracy: 0.674500 | 0.533 sec/iter\n",
      "Epoch: 196 | Batch: 001 / 011 | Total loss: 0.886 | Reg loss: 0.028 | Tree loss: 0.886 | Accuracy: 0.670000 | 0.532 sec/iter\n",
      "Epoch: 196 | Batch: 002 / 011 | Total loss: 0.858 | Reg loss: 0.028 | Tree loss: 0.858 | Accuracy: 0.674000 | 0.532 sec/iter\n",
      "Epoch: 196 | Batch: 003 / 011 | Total loss: 0.860 | Reg loss: 0.028 | Tree loss: 0.860 | Accuracy: 0.679000 | 0.532 sec/iter\n",
      "Epoch: 196 | Batch: 004 / 011 | Total loss: 0.820 | Reg loss: 0.028 | Tree loss: 0.820 | Accuracy: 0.715000 | 0.532 sec/iter\n",
      "Epoch: 196 | Batch: 005 / 011 | Total loss: 0.843 | Reg loss: 0.028 | Tree loss: 0.843 | Accuracy: 0.698500 | 0.532 sec/iter\n",
      "Epoch: 196 | Batch: 006 / 011 | Total loss: 0.820 | Reg loss: 0.028 | Tree loss: 0.820 | Accuracy: 0.722000 | 0.532 sec/iter\n",
      "Epoch: 196 | Batch: 007 / 011 | Total loss: 0.830 | Reg loss: 0.028 | Tree loss: 0.830 | Accuracy: 0.714000 | 0.532 sec/iter\n",
      "Epoch: 196 | Batch: 008 / 011 | Total loss: 0.792 | Reg loss: 0.028 | Tree loss: 0.792 | Accuracy: 0.733500 | 0.532 sec/iter\n",
      "Epoch: 196 | Batch: 009 / 011 | Total loss: 0.823 | Reg loss: 0.028 | Tree loss: 0.823 | Accuracy: 0.713000 | 0.531 sec/iter\n",
      "Epoch: 196 | Batch: 010 / 011 | Total loss: 0.868 | Reg loss: 0.028 | Tree loss: 0.868 | Accuracy: 0.723549 | 0.531 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 197 | Batch: 000 / 011 | Total loss: 0.893 | Reg loss: 0.028 | Tree loss: 0.893 | Accuracy: 0.665000 | 0.534 sec/iter\n",
      "Epoch: 197 | Batch: 001 / 011 | Total loss: 0.893 | Reg loss: 0.028 | Tree loss: 0.893 | Accuracy: 0.663000 | 0.534 sec/iter\n",
      "Epoch: 197 | Batch: 002 / 011 | Total loss: 0.898 | Reg loss: 0.028 | Tree loss: 0.898 | Accuracy: 0.664000 | 0.533 sec/iter\n",
      "Epoch: 197 | Batch: 003 / 011 | Total loss: 0.846 | Reg loss: 0.028 | Tree loss: 0.846 | Accuracy: 0.711000 | 0.533 sec/iter\n",
      "Epoch: 197 | Batch: 004 / 011 | Total loss: 0.834 | Reg loss: 0.028 | Tree loss: 0.834 | Accuracy: 0.698500 | 0.533 sec/iter\n",
      "Epoch: 197 | Batch: 005 / 011 | Total loss: 0.809 | Reg loss: 0.028 | Tree loss: 0.809 | Accuracy: 0.723000 | 0.533 sec/iter\n",
      "Epoch: 197 | Batch: 006 / 011 | Total loss: 0.801 | Reg loss: 0.028 | Tree loss: 0.801 | Accuracy: 0.757500 | 0.533 sec/iter\n",
      "Epoch: 197 | Batch: 007 / 011 | Total loss: 0.804 | Reg loss: 0.028 | Tree loss: 0.804 | Accuracy: 0.734000 | 0.533 sec/iter\n",
      "Epoch: 197 | Batch: 008 / 011 | Total loss: 0.806 | Reg loss: 0.028 | Tree loss: 0.806 | Accuracy: 0.731000 | 0.533 sec/iter\n",
      "Epoch: 197 | Batch: 009 / 011 | Total loss: 0.799 | Reg loss: 0.028 | Tree loss: 0.799 | Accuracy: 0.724500 | 0.532 sec/iter\n",
      "Epoch: 197 | Batch: 010 / 011 | Total loss: 0.865 | Reg loss: 0.028 | Tree loss: 0.865 | Accuracy: 0.699659 | 0.532 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 198 | Batch: 000 / 011 | Total loss: 0.889 | Reg loss: 0.028 | Tree loss: 0.889 | Accuracy: 0.664000 | 0.533 sec/iter\n",
      "Epoch: 198 | Batch: 001 / 011 | Total loss: 0.888 | Reg loss: 0.028 | Tree loss: 0.888 | Accuracy: 0.669000 | 0.533 sec/iter\n",
      "Epoch: 198 | Batch: 002 / 011 | Total loss: 0.857 | Reg loss: 0.028 | Tree loss: 0.857 | Accuracy: 0.698500 | 0.533 sec/iter\n",
      "Epoch: 198 | Batch: 003 / 011 | Total loss: 0.815 | Reg loss: 0.028 | Tree loss: 0.815 | Accuracy: 0.717500 | 0.533 sec/iter\n",
      "Epoch: 198 | Batch: 004 / 011 | Total loss: 0.843 | Reg loss: 0.028 | Tree loss: 0.843 | Accuracy: 0.711500 | 0.533 sec/iter\n",
      "Epoch: 198 | Batch: 005 / 011 | Total loss: 0.819 | Reg loss: 0.028 | Tree loss: 0.819 | Accuracy: 0.713500 | 0.533 sec/iter\n",
      "Epoch: 198 | Batch: 006 / 011 | Total loss: 0.820 | Reg loss: 0.028 | Tree loss: 0.820 | Accuracy: 0.709000 | 0.533 sec/iter\n",
      "Epoch: 198 | Batch: 007 / 011 | Total loss: 0.830 | Reg loss: 0.028 | Tree loss: 0.830 | Accuracy: 0.702500 | 0.532 sec/iter\n",
      "Epoch: 198 | Batch: 008 / 011 | Total loss: 0.808 | Reg loss: 0.028 | Tree loss: 0.808 | Accuracy: 0.729000 | 0.532 sec/iter\n",
      "Epoch: 198 | Batch: 009 / 011 | Total loss: 0.846 | Reg loss: 0.028 | Tree loss: 0.846 | Accuracy: 0.709000 | 0.532 sec/iter\n",
      "Epoch: 198 | Batch: 010 / 011 | Total loss: 0.879 | Reg loss: 0.028 | Tree loss: 0.879 | Accuracy: 0.703072 | 0.532 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 199 | Batch: 000 / 011 | Total loss: 0.894 | Reg loss: 0.028 | Tree loss: 0.894 | Accuracy: 0.653000 | 0.533 sec/iter\n",
      "Epoch: 199 | Batch: 001 / 011 | Total loss: 0.874 | Reg loss: 0.028 | Tree loss: 0.874 | Accuracy: 0.670500 | 0.533 sec/iter\n",
      "Epoch: 199 | Batch: 002 / 011 | Total loss: 0.883 | Reg loss: 0.028 | Tree loss: 0.883 | Accuracy: 0.681500 | 0.533 sec/iter\n",
      "Epoch: 199 | Batch: 003 / 011 | Total loss: 0.836 | Reg loss: 0.028 | Tree loss: 0.836 | Accuracy: 0.708000 | 0.533 sec/iter\n",
      "Epoch: 199 | Batch: 004 / 011 | Total loss: 0.838 | Reg loss: 0.028 | Tree loss: 0.838 | Accuracy: 0.710500 | 0.533 sec/iter\n",
      "Epoch: 199 | Batch: 005 / 011 | Total loss: 0.815 | Reg loss: 0.028 | Tree loss: 0.815 | Accuracy: 0.719000 | 0.532 sec/iter\n",
      "Epoch: 199 | Batch: 006 / 011 | Total loss: 0.813 | Reg loss: 0.028 | Tree loss: 0.813 | Accuracy: 0.719000 | 0.532 sec/iter\n",
      "Epoch: 199 | Batch: 007 / 011 | Total loss: 0.805 | Reg loss: 0.028 | Tree loss: 0.805 | Accuracy: 0.737000 | 0.532 sec/iter\n",
      "Epoch: 199 | Batch: 008 / 011 | Total loss: 0.825 | Reg loss: 0.028 | Tree loss: 0.825 | Accuracy: 0.705500 | 0.532 sec/iter\n",
      "Epoch: 199 | Batch: 009 / 011 | Total loss: 0.808 | Reg loss: 0.028 | Tree loss: 0.808 | Accuracy: 0.729500 | 0.532 sec/iter\n",
      "Epoch: 199 | Batch: 010 / 011 | Total loss: 0.842 | Reg loss: 0.028 | Tree loss: 0.842 | Accuracy: 0.703072 | 0.532 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 200 | Batch: 000 / 011 | Total loss: 0.892 | Reg loss: 0.028 | Tree loss: 0.892 | Accuracy: 0.658500 | 0.533 sec/iter\n",
      "Epoch: 200 | Batch: 001 / 011 | Total loss: 0.876 | Reg loss: 0.028 | Tree loss: 0.876 | Accuracy: 0.681000 | 0.533 sec/iter\n",
      "Epoch: 200 | Batch: 002 / 011 | Total loss: 0.906 | Reg loss: 0.028 | Tree loss: 0.906 | Accuracy: 0.674500 | 0.533 sec/iter\n",
      "Epoch: 200 | Batch: 003 / 011 | Total loss: 0.842 | Reg loss: 0.028 | Tree loss: 0.842 | Accuracy: 0.698500 | 0.533 sec/iter\n",
      "Epoch: 200 | Batch: 004 / 011 | Total loss: 0.836 | Reg loss: 0.028 | Tree loss: 0.836 | Accuracy: 0.710000 | 0.533 sec/iter\n",
      "Epoch: 200 | Batch: 005 / 011 | Total loss: 0.805 | Reg loss: 0.028 | Tree loss: 0.805 | Accuracy: 0.735500 | 0.533 sec/iter\n",
      "Epoch: 200 | Batch: 006 / 011 | Total loss: 0.814 | Reg loss: 0.028 | Tree loss: 0.814 | Accuracy: 0.702500 | 0.533 sec/iter\n",
      "Epoch: 200 | Batch: 007 / 011 | Total loss: 0.829 | Reg loss: 0.028 | Tree loss: 0.829 | Accuracy: 0.708500 | 0.532 sec/iter\n",
      "Epoch: 200 | Batch: 008 / 011 | Total loss: 0.806 | Reg loss: 0.028 | Tree loss: 0.806 | Accuracy: 0.715500 | 0.532 sec/iter\n",
      "Epoch: 200 | Batch: 009 / 011 | Total loss: 0.802 | Reg loss: 0.028 | Tree loss: 0.802 | Accuracy: 0.720000 | 0.532 sec/iter\n",
      "Epoch: 200 | Batch: 010 / 011 | Total loss: 0.789 | Reg loss: 0.028 | Tree loss: 0.789 | Accuracy: 0.737201 | 0.532 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 201 | Batch: 000 / 011 | Total loss: 0.906 | Reg loss: 0.028 | Tree loss: 0.906 | Accuracy: 0.644500 | 0.532 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 201 | Batch: 001 / 011 | Total loss: 0.904 | Reg loss: 0.028 | Tree loss: 0.904 | Accuracy: 0.645000 | 0.532 sec/iter\n",
      "Epoch: 201 | Batch: 002 / 011 | Total loss: 0.857 | Reg loss: 0.028 | Tree loss: 0.857 | Accuracy: 0.682500 | 0.532 sec/iter\n",
      "Epoch: 201 | Batch: 003 / 011 | Total loss: 0.838 | Reg loss: 0.028 | Tree loss: 0.838 | Accuracy: 0.700000 | 0.532 sec/iter\n",
      "Epoch: 201 | Batch: 004 / 011 | Total loss: 0.831 | Reg loss: 0.028 | Tree loss: 0.831 | Accuracy: 0.706000 | 0.532 sec/iter\n",
      "Epoch: 201 | Batch: 005 / 011 | Total loss: 0.832 | Reg loss: 0.028 | Tree loss: 0.832 | Accuracy: 0.713500 | 0.532 sec/iter\n",
      "Epoch: 201 | Batch: 006 / 011 | Total loss: 0.813 | Reg loss: 0.028 | Tree loss: 0.813 | Accuracy: 0.718000 | 0.531 sec/iter\n",
      "Epoch: 201 | Batch: 007 / 011 | Total loss: 0.799 | Reg loss: 0.028 | Tree loss: 0.799 | Accuracy: 0.750500 | 0.531 sec/iter\n",
      "Epoch: 201 | Batch: 008 / 011 | Total loss: 0.808 | Reg loss: 0.028 | Tree loss: 0.808 | Accuracy: 0.729000 | 0.531 sec/iter\n",
      "Epoch: 201 | Batch: 009 / 011 | Total loss: 0.786 | Reg loss: 0.028 | Tree loss: 0.786 | Accuracy: 0.738500 | 0.531 sec/iter\n",
      "Epoch: 201 | Batch: 010 / 011 | Total loss: 0.809 | Reg loss: 0.028 | Tree loss: 0.809 | Accuracy: 0.699659 | 0.531 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 202 | Batch: 000 / 011 | Total loss: 0.891 | Reg loss: 0.028 | Tree loss: 0.891 | Accuracy: 0.660000 | 0.532 sec/iter\n",
      "Epoch: 202 | Batch: 001 / 011 | Total loss: 0.890 | Reg loss: 0.028 | Tree loss: 0.890 | Accuracy: 0.658500 | 0.532 sec/iter\n",
      "Epoch: 202 | Batch: 002 / 011 | Total loss: 0.863 | Reg loss: 0.028 | Tree loss: 0.863 | Accuracy: 0.679500 | 0.532 sec/iter\n",
      "Epoch: 202 | Batch: 003 / 011 | Total loss: 0.833 | Reg loss: 0.028 | Tree loss: 0.833 | Accuracy: 0.713000 | 0.531 sec/iter\n",
      "Epoch: 202 | Batch: 004 / 011 | Total loss: 0.817 | Reg loss: 0.028 | Tree loss: 0.817 | Accuracy: 0.724000 | 0.531 sec/iter\n",
      "Epoch: 202 | Batch: 005 / 011 | Total loss: 0.826 | Reg loss: 0.028 | Tree loss: 0.826 | Accuracy: 0.718000 | 0.531 sec/iter\n",
      "Epoch: 202 | Batch: 006 / 011 | Total loss: 0.814 | Reg loss: 0.028 | Tree loss: 0.814 | Accuracy: 0.723000 | 0.531 sec/iter\n",
      "Epoch: 202 | Batch: 007 / 011 | Total loss: 0.812 | Reg loss: 0.028 | Tree loss: 0.812 | Accuracy: 0.738500 | 0.531 sec/iter\n",
      "Epoch: 202 | Batch: 008 / 011 | Total loss: 0.826 | Reg loss: 0.028 | Tree loss: 0.826 | Accuracy: 0.702000 | 0.531 sec/iter\n",
      "Epoch: 202 | Batch: 009 / 011 | Total loss: 0.822 | Reg loss: 0.028 | Tree loss: 0.822 | Accuracy: 0.715500 | 0.531 sec/iter\n",
      "Epoch: 202 | Batch: 010 / 011 | Total loss: 0.803 | Reg loss: 0.028 | Tree loss: 0.803 | Accuracy: 0.723549 | 0.53 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 203 | Batch: 000 / 011 | Total loss: 0.899 | Reg loss: 0.028 | Tree loss: 0.899 | Accuracy: 0.660500 | 0.532 sec/iter\n",
      "Epoch: 203 | Batch: 001 / 011 | Total loss: 0.882 | Reg loss: 0.028 | Tree loss: 0.882 | Accuracy: 0.669000 | 0.532 sec/iter\n",
      "Epoch: 203 | Batch: 002 / 011 | Total loss: 0.861 | Reg loss: 0.028 | Tree loss: 0.861 | Accuracy: 0.683500 | 0.532 sec/iter\n",
      "Epoch: 203 | Batch: 003 / 011 | Total loss: 0.835 | Reg loss: 0.028 | Tree loss: 0.835 | Accuracy: 0.704500 | 0.532 sec/iter\n",
      "Epoch: 203 | Batch: 004 / 011 | Total loss: 0.815 | Reg loss: 0.028 | Tree loss: 0.815 | Accuracy: 0.719000 | 0.532 sec/iter\n",
      "Epoch: 203 | Batch: 005 / 011 | Total loss: 0.813 | Reg loss: 0.028 | Tree loss: 0.813 | Accuracy: 0.711000 | 0.531 sec/iter\n",
      "Epoch: 203 | Batch: 006 / 011 | Total loss: 0.818 | Reg loss: 0.028 | Tree loss: 0.818 | Accuracy: 0.719000 | 0.531 sec/iter\n",
      "Epoch: 203 | Batch: 007 / 011 | Total loss: 0.809 | Reg loss: 0.028 | Tree loss: 0.809 | Accuracy: 0.728500 | 0.531 sec/iter\n",
      "Epoch: 203 | Batch: 008 / 011 | Total loss: 0.839 | Reg loss: 0.028 | Tree loss: 0.839 | Accuracy: 0.701500 | 0.531 sec/iter\n",
      "Epoch: 203 | Batch: 009 / 011 | Total loss: 0.821 | Reg loss: 0.028 | Tree loss: 0.821 | Accuracy: 0.717000 | 0.531 sec/iter\n",
      "Epoch: 203 | Batch: 010 / 011 | Total loss: 0.826 | Reg loss: 0.028 | Tree loss: 0.826 | Accuracy: 0.709898 | 0.531 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 204 | Batch: 000 / 011 | Total loss: 0.889 | Reg loss: 0.028 | Tree loss: 0.889 | Accuracy: 0.645000 | 0.531 sec/iter\n",
      "Epoch: 204 | Batch: 001 / 011 | Total loss: 0.916 | Reg loss: 0.028 | Tree loss: 0.916 | Accuracy: 0.645000 | 0.531 sec/iter\n",
      "Epoch: 204 | Batch: 002 / 011 | Total loss: 0.880 | Reg loss: 0.028 | Tree loss: 0.880 | Accuracy: 0.661000 | 0.531 sec/iter\n",
      "Epoch: 204 | Batch: 003 / 011 | Total loss: 0.849 | Reg loss: 0.028 | Tree loss: 0.849 | Accuracy: 0.686500 | 0.531 sec/iter\n",
      "Epoch: 204 | Batch: 004 / 011 | Total loss: 0.838 | Reg loss: 0.028 | Tree loss: 0.838 | Accuracy: 0.707000 | 0.531 sec/iter\n",
      "Epoch: 204 | Batch: 005 / 011 | Total loss: 0.816 | Reg loss: 0.028 | Tree loss: 0.816 | Accuracy: 0.726500 | 0.531 sec/iter\n",
      "Epoch: 204 | Batch: 006 / 011 | Total loss: 0.815 | Reg loss: 0.028 | Tree loss: 0.815 | Accuracy: 0.728000 | 0.53 sec/iter\n",
      "Epoch: 204 | Batch: 007 / 011 | Total loss: 0.808 | Reg loss: 0.028 | Tree loss: 0.808 | Accuracy: 0.723500 | 0.53 sec/iter\n",
      "Epoch: 204 | Batch: 008 / 011 | Total loss: 0.785 | Reg loss: 0.028 | Tree loss: 0.785 | Accuracy: 0.744000 | 0.53 sec/iter\n",
      "Epoch: 204 | Batch: 009 / 011 | Total loss: 0.785 | Reg loss: 0.028 | Tree loss: 0.785 | Accuracy: 0.738500 | 0.53 sec/iter\n",
      "Epoch: 204 | Batch: 010 / 011 | Total loss: 0.810 | Reg loss: 0.028 | Tree loss: 0.810 | Accuracy: 0.720137 | 0.53 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 205 | Batch: 000 / 011 | Total loss: 0.920 | Reg loss: 0.028 | Tree loss: 0.920 | Accuracy: 0.649500 | 0.531 sec/iter\n",
      "Epoch: 205 | Batch: 001 / 011 | Total loss: 0.896 | Reg loss: 0.028 | Tree loss: 0.896 | Accuracy: 0.650500 | 0.531 sec/iter\n",
      "Epoch: 205 | Batch: 002 / 011 | Total loss: 0.863 | Reg loss: 0.028 | Tree loss: 0.863 | Accuracy: 0.673000 | 0.531 sec/iter\n",
      "Epoch: 205 | Batch: 003 / 011 | Total loss: 0.823 | Reg loss: 0.028 | Tree loss: 0.823 | Accuracy: 0.714000 | 0.531 sec/iter\n",
      "Epoch: 205 | Batch: 004 / 011 | Total loss: 0.836 | Reg loss: 0.028 | Tree loss: 0.836 | Accuracy: 0.715000 | 0.531 sec/iter\n",
      "Epoch: 205 | Batch: 005 / 011 | Total loss: 0.811 | Reg loss: 0.028 | Tree loss: 0.811 | Accuracy: 0.711000 | 0.53 sec/iter\n",
      "Epoch: 205 | Batch: 006 / 011 | Total loss: 0.798 | Reg loss: 0.028 | Tree loss: 0.798 | Accuracy: 0.727500 | 0.53 sec/iter\n",
      "Epoch: 205 | Batch: 007 / 011 | Total loss: 0.810 | Reg loss: 0.028 | Tree loss: 0.810 | Accuracy: 0.715500 | 0.53 sec/iter\n",
      "Epoch: 205 | Batch: 008 / 011 | Total loss: 0.820 | Reg loss: 0.028 | Tree loss: 0.820 | Accuracy: 0.723000 | 0.53 sec/iter\n",
      "Epoch: 205 | Batch: 009 / 011 | Total loss: 0.813 | Reg loss: 0.028 | Tree loss: 0.813 | Accuracy: 0.719500 | 0.53 sec/iter\n",
      "Epoch: 205 | Batch: 010 / 011 | Total loss: 0.788 | Reg loss: 0.028 | Tree loss: 0.788 | Accuracy: 0.744027 | 0.53 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 206 | Batch: 000 / 011 | Total loss: 0.877 | Reg loss: 0.028 | Tree loss: 0.877 | Accuracy: 0.662500 | 0.532 sec/iter\n",
      "Epoch: 206 | Batch: 001 / 011 | Total loss: 0.899 | Reg loss: 0.028 | Tree loss: 0.899 | Accuracy: 0.652000 | 0.532 sec/iter\n",
      "Epoch: 206 | Batch: 002 / 011 | Total loss: 0.866 | Reg loss: 0.028 | Tree loss: 0.866 | Accuracy: 0.671500 | 0.532 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 206 | Batch: 003 / 011 | Total loss: 0.866 | Reg loss: 0.028 | Tree loss: 0.866 | Accuracy: 0.687000 | 0.532 sec/iter\n",
      "Epoch: 206 | Batch: 004 / 011 | Total loss: 0.810 | Reg loss: 0.028 | Tree loss: 0.810 | Accuracy: 0.725000 | 0.532 sec/iter\n",
      "Epoch: 206 | Batch: 005 / 011 | Total loss: 0.823 | Reg loss: 0.028 | Tree loss: 0.823 | Accuracy: 0.718000 | 0.531 sec/iter\n",
      "Epoch: 206 | Batch: 006 / 011 | Total loss: 0.789 | Reg loss: 0.028 | Tree loss: 0.789 | Accuracy: 0.750000 | 0.531 sec/iter\n",
      "Epoch: 206 | Batch: 007 / 011 | Total loss: 0.810 | Reg loss: 0.028 | Tree loss: 0.810 | Accuracy: 0.708000 | 0.531 sec/iter\n",
      "Epoch: 206 | Batch: 008 / 011 | Total loss: 0.848 | Reg loss: 0.028 | Tree loss: 0.848 | Accuracy: 0.700000 | 0.531 sec/iter\n",
      "Epoch: 206 | Batch: 009 / 011 | Total loss: 0.806 | Reg loss: 0.028 | Tree loss: 0.806 | Accuracy: 0.721000 | 0.531 sec/iter\n",
      "Epoch: 206 | Batch: 010 / 011 | Total loss: 0.803 | Reg loss: 0.028 | Tree loss: 0.803 | Accuracy: 0.713311 | 0.531 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 207 | Batch: 000 / 011 | Total loss: 0.905 | Reg loss: 0.028 | Tree loss: 0.905 | Accuracy: 0.653000 | 0.532 sec/iter\n",
      "Epoch: 207 | Batch: 001 / 011 | Total loss: 0.900 | Reg loss: 0.028 | Tree loss: 0.900 | Accuracy: 0.656500 | 0.531 sec/iter\n",
      "Epoch: 207 | Batch: 002 / 011 | Total loss: 0.880 | Reg loss: 0.028 | Tree loss: 0.880 | Accuracy: 0.657000 | 0.531 sec/iter\n",
      "Epoch: 207 | Batch: 003 / 011 | Total loss: 0.850 | Reg loss: 0.028 | Tree loss: 0.850 | Accuracy: 0.701500 | 0.531 sec/iter\n",
      "Epoch: 207 | Batch: 004 / 011 | Total loss: 0.807 | Reg loss: 0.028 | Tree loss: 0.807 | Accuracy: 0.720500 | 0.531 sec/iter\n",
      "Epoch: 207 | Batch: 005 / 011 | Total loss: 0.828 | Reg loss: 0.028 | Tree loss: 0.828 | Accuracy: 0.709000 | 0.531 sec/iter\n",
      "Epoch: 207 | Batch: 006 / 011 | Total loss: 0.799 | Reg loss: 0.028 | Tree loss: 0.799 | Accuracy: 0.732500 | 0.531 sec/iter\n",
      "Epoch: 207 | Batch: 007 / 011 | Total loss: 0.805 | Reg loss: 0.028 | Tree loss: 0.805 | Accuracy: 0.735500 | 0.531 sec/iter\n",
      "Epoch: 207 | Batch: 008 / 011 | Total loss: 0.792 | Reg loss: 0.028 | Tree loss: 0.792 | Accuracy: 0.745500 | 0.531 sec/iter\n",
      "Epoch: 207 | Batch: 009 / 011 | Total loss: 0.799 | Reg loss: 0.028 | Tree loss: 0.799 | Accuracy: 0.726000 | 0.531 sec/iter\n",
      "Epoch: 207 | Batch: 010 / 011 | Total loss: 0.811 | Reg loss: 0.028 | Tree loss: 0.811 | Accuracy: 0.713311 | 0.53 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 208 | Batch: 000 / 011 | Total loss: 0.924 | Reg loss: 0.028 | Tree loss: 0.924 | Accuracy: 0.647500 | 0.532 sec/iter\n",
      "Epoch: 208 | Batch: 001 / 011 | Total loss: 0.884 | Reg loss: 0.028 | Tree loss: 0.884 | Accuracy: 0.658000 | 0.532 sec/iter\n",
      "Epoch: 208 | Batch: 002 / 011 | Total loss: 0.884 | Reg loss: 0.028 | Tree loss: 0.884 | Accuracy: 0.662000 | 0.532 sec/iter\n",
      "Epoch: 208 | Batch: 003 / 011 | Total loss: 0.845 | Reg loss: 0.028 | Tree loss: 0.845 | Accuracy: 0.698000 | 0.531 sec/iter\n",
      "Epoch: 208 | Batch: 004 / 011 | Total loss: 0.810 | Reg loss: 0.028 | Tree loss: 0.810 | Accuracy: 0.716000 | 0.531 sec/iter\n",
      "Epoch: 208 | Batch: 005 / 011 | Total loss: 0.817 | Reg loss: 0.028 | Tree loss: 0.817 | Accuracy: 0.715500 | 0.531 sec/iter\n",
      "Epoch: 208 | Batch: 006 / 011 | Total loss: 0.805 | Reg loss: 0.028 | Tree loss: 0.805 | Accuracy: 0.722500 | 0.531 sec/iter\n",
      "Epoch: 208 | Batch: 007 / 011 | Total loss: 0.798 | Reg loss: 0.028 | Tree loss: 0.798 | Accuracy: 0.724000 | 0.531 sec/iter\n",
      "Epoch: 208 | Batch: 008 / 011 | Total loss: 0.780 | Reg loss: 0.028 | Tree loss: 0.780 | Accuracy: 0.737500 | 0.531 sec/iter\n",
      "Epoch: 208 | Batch: 009 / 011 | Total loss: 0.805 | Reg loss: 0.028 | Tree loss: 0.805 | Accuracy: 0.720000 | 0.531 sec/iter\n",
      "Epoch: 208 | Batch: 010 / 011 | Total loss: 0.780 | Reg loss: 0.028 | Tree loss: 0.780 | Accuracy: 0.744027 | 0.53 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 209 | Batch: 000 / 011 | Total loss: 0.899 | Reg loss: 0.028 | Tree loss: 0.899 | Accuracy: 0.653000 | 0.531 sec/iter\n",
      "Epoch: 209 | Batch: 001 / 011 | Total loss: 0.874 | Reg loss: 0.028 | Tree loss: 0.874 | Accuracy: 0.665500 | 0.531 sec/iter\n",
      "Epoch: 209 | Batch: 002 / 011 | Total loss: 0.882 | Reg loss: 0.028 | Tree loss: 0.882 | Accuracy: 0.659500 | 0.531 sec/iter\n",
      "Epoch: 209 | Batch: 003 / 011 | Total loss: 0.842 | Reg loss: 0.028 | Tree loss: 0.842 | Accuracy: 0.693500 | 0.531 sec/iter\n",
      "Epoch: 209 | Batch: 004 / 011 | Total loss: 0.827 | Reg loss: 0.028 | Tree loss: 0.827 | Accuracy: 0.707000 | 0.531 sec/iter\n",
      "Epoch: 209 | Batch: 005 / 011 | Total loss: 0.836 | Reg loss: 0.028 | Tree loss: 0.836 | Accuracy: 0.699000 | 0.531 sec/iter\n",
      "Epoch: 209 | Batch: 006 / 011 | Total loss: 0.813 | Reg loss: 0.028 | Tree loss: 0.813 | Accuracy: 0.724000 | 0.531 sec/iter\n",
      "Epoch: 209 | Batch: 007 / 011 | Total loss: 0.802 | Reg loss: 0.028 | Tree loss: 0.802 | Accuracy: 0.744000 | 0.53 sec/iter\n",
      "Epoch: 209 | Batch: 008 / 011 | Total loss: 0.810 | Reg loss: 0.028 | Tree loss: 0.810 | Accuracy: 0.727500 | 0.53 sec/iter\n",
      "Epoch: 209 | Batch: 009 / 011 | Total loss: 0.799 | Reg loss: 0.028 | Tree loss: 0.799 | Accuracy: 0.726000 | 0.53 sec/iter\n",
      "Epoch: 209 | Batch: 010 / 011 | Total loss: 0.796 | Reg loss: 0.028 | Tree loss: 0.796 | Accuracy: 0.737201 | 0.53 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 210 | Batch: 000 / 011 | Total loss: 0.901 | Reg loss: 0.028 | Tree loss: 0.901 | Accuracy: 0.656000 | 0.53 sec/iter\n",
      "Epoch: 210 | Batch: 001 / 011 | Total loss: 0.892 | Reg loss: 0.028 | Tree loss: 0.892 | Accuracy: 0.655000 | 0.53 sec/iter\n",
      "Epoch: 210 | Batch: 002 / 011 | Total loss: 0.869 | Reg loss: 0.028 | Tree loss: 0.869 | Accuracy: 0.674000 | 0.53 sec/iter\n",
      "Epoch: 210 | Batch: 003 / 011 | Total loss: 0.861 | Reg loss: 0.028 | Tree loss: 0.861 | Accuracy: 0.673500 | 0.53 sec/iter\n",
      "Epoch: 210 | Batch: 004 / 011 | Total loss: 0.823 | Reg loss: 0.028 | Tree loss: 0.823 | Accuracy: 0.717000 | 0.53 sec/iter\n",
      "Epoch: 210 | Batch: 005 / 011 | Total loss: 0.830 | Reg loss: 0.028 | Tree loss: 0.830 | Accuracy: 0.709500 | 0.529 sec/iter\n",
      "Epoch: 210 | Batch: 006 / 011 | Total loss: 0.797 | Reg loss: 0.028 | Tree loss: 0.797 | Accuracy: 0.724500 | 0.529 sec/iter\n",
      "Epoch: 210 | Batch: 007 / 011 | Total loss: 0.814 | Reg loss: 0.028 | Tree loss: 0.814 | Accuracy: 0.730500 | 0.529 sec/iter\n",
      "Epoch: 210 | Batch: 008 / 011 | Total loss: 0.774 | Reg loss: 0.028 | Tree loss: 0.774 | Accuracy: 0.732000 | 0.529 sec/iter\n",
      "Epoch: 210 | Batch: 009 / 011 | Total loss: 0.799 | Reg loss: 0.028 | Tree loss: 0.799 | Accuracy: 0.722000 | 0.529 sec/iter\n",
      "Epoch: 210 | Batch: 010 / 011 | Total loss: 0.785 | Reg loss: 0.028 | Tree loss: 0.785 | Accuracy: 0.730375 | 0.529 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 211 | Batch: 000 / 011 | Total loss: 0.883 | Reg loss: 0.028 | Tree loss: 0.883 | Accuracy: 0.672500 | 0.53 sec/iter\n",
      "Epoch: 211 | Batch: 001 / 011 | Total loss: 0.866 | Reg loss: 0.028 | Tree loss: 0.866 | Accuracy: 0.666000 | 0.53 sec/iter\n",
      "Epoch: 211 | Batch: 002 / 011 | Total loss: 0.859 | Reg loss: 0.028 | Tree loss: 0.859 | Accuracy: 0.684500 | 0.53 sec/iter\n",
      "Epoch: 211 | Batch: 003 / 011 | Total loss: 0.856 | Reg loss: 0.028 | Tree loss: 0.856 | Accuracy: 0.704000 | 0.53 sec/iter\n",
      "Epoch: 211 | Batch: 004 / 011 | Total loss: 0.844 | Reg loss: 0.028 | Tree loss: 0.844 | Accuracy: 0.695000 | 0.53 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 211 | Batch: 005 / 011 | Total loss: 0.843 | Reg loss: 0.028 | Tree loss: 0.843 | Accuracy: 0.693000 | 0.53 sec/iter\n",
      "Epoch: 211 | Batch: 006 / 011 | Total loss: 0.804 | Reg loss: 0.028 | Tree loss: 0.804 | Accuracy: 0.728500 | 0.53 sec/iter\n",
      "Epoch: 211 | Batch: 007 / 011 | Total loss: 0.798 | Reg loss: 0.028 | Tree loss: 0.798 | Accuracy: 0.731000 | 0.529 sec/iter\n",
      "Epoch: 211 | Batch: 008 / 011 | Total loss: 0.810 | Reg loss: 0.028 | Tree loss: 0.810 | Accuracy: 0.727000 | 0.529 sec/iter\n",
      "Epoch: 211 | Batch: 009 / 011 | Total loss: 0.796 | Reg loss: 0.028 | Tree loss: 0.796 | Accuracy: 0.717000 | 0.529 sec/iter\n",
      "Epoch: 211 | Batch: 010 / 011 | Total loss: 0.808 | Reg loss: 0.028 | Tree loss: 0.808 | Accuracy: 0.716724 | 0.529 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 212 | Batch: 000 / 011 | Total loss: 0.898 | Reg loss: 0.028 | Tree loss: 0.898 | Accuracy: 0.659500 | 0.53 sec/iter\n",
      "Epoch: 212 | Batch: 001 / 011 | Total loss: 0.888 | Reg loss: 0.028 | Tree loss: 0.888 | Accuracy: 0.664500 | 0.53 sec/iter\n",
      "Epoch: 212 | Batch: 002 / 011 | Total loss: 0.847 | Reg loss: 0.028 | Tree loss: 0.847 | Accuracy: 0.688000 | 0.53 sec/iter\n",
      "Epoch: 212 | Batch: 003 / 011 | Total loss: 0.817 | Reg loss: 0.028 | Tree loss: 0.817 | Accuracy: 0.724000 | 0.53 sec/iter\n",
      "Epoch: 212 | Batch: 004 / 011 | Total loss: 0.823 | Reg loss: 0.028 | Tree loss: 0.823 | Accuracy: 0.711000 | 0.53 sec/iter\n",
      "Epoch: 212 | Batch: 005 / 011 | Total loss: 0.806 | Reg loss: 0.028 | Tree loss: 0.806 | Accuracy: 0.726000 | 0.53 sec/iter\n",
      "Epoch: 212 | Batch: 006 / 011 | Total loss: 0.828 | Reg loss: 0.028 | Tree loss: 0.828 | Accuracy: 0.710500 | 0.53 sec/iter\n",
      "Epoch: 212 | Batch: 007 / 011 | Total loss: 0.788 | Reg loss: 0.028 | Tree loss: 0.788 | Accuracy: 0.736500 | 0.529 sec/iter\n",
      "Epoch: 212 | Batch: 008 / 011 | Total loss: 0.820 | Reg loss: 0.028 | Tree loss: 0.820 | Accuracy: 0.726000 | 0.529 sec/iter\n",
      "Epoch: 212 | Batch: 009 / 011 | Total loss: 0.801 | Reg loss: 0.028 | Tree loss: 0.801 | Accuracy: 0.728000 | 0.529 sec/iter\n",
      "Epoch: 212 | Batch: 010 / 011 | Total loss: 0.881 | Reg loss: 0.028 | Tree loss: 0.881 | Accuracy: 0.686007 | 0.529 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 213 | Batch: 000 / 011 | Total loss: 0.912 | Reg loss: 0.028 | Tree loss: 0.912 | Accuracy: 0.658500 | 0.529 sec/iter\n",
      "Epoch: 213 | Batch: 001 / 011 | Total loss: 0.872 | Reg loss: 0.028 | Tree loss: 0.872 | Accuracy: 0.669500 | 0.529 sec/iter\n",
      "Epoch: 213 | Batch: 002 / 011 | Total loss: 0.858 | Reg loss: 0.028 | Tree loss: 0.858 | Accuracy: 0.685500 | 0.529 sec/iter\n",
      "Epoch: 213 | Batch: 003 / 011 | Total loss: 0.831 | Reg loss: 0.028 | Tree loss: 0.831 | Accuracy: 0.705000 | 0.529 sec/iter\n",
      "Epoch: 213 | Batch: 004 / 011 | Total loss: 0.836 | Reg loss: 0.028 | Tree loss: 0.836 | Accuracy: 0.701500 | 0.529 sec/iter\n",
      "Epoch: 213 | Batch: 005 / 011 | Total loss: 0.812 | Reg loss: 0.028 | Tree loss: 0.812 | Accuracy: 0.716500 | 0.528 sec/iter\n",
      "Epoch: 213 | Batch: 006 / 011 | Total loss: 0.816 | Reg loss: 0.028 | Tree loss: 0.816 | Accuracy: 0.714500 | 0.528 sec/iter\n",
      "Epoch: 213 | Batch: 007 / 011 | Total loss: 0.789 | Reg loss: 0.028 | Tree loss: 0.789 | Accuracy: 0.736500 | 0.528 sec/iter\n",
      "Epoch: 213 | Batch: 008 / 011 | Total loss: 0.805 | Reg loss: 0.028 | Tree loss: 0.805 | Accuracy: 0.723500 | 0.528 sec/iter\n",
      "Epoch: 213 | Batch: 009 / 011 | Total loss: 0.806 | Reg loss: 0.028 | Tree loss: 0.806 | Accuracy: 0.733500 | 0.528 sec/iter\n",
      "Epoch: 213 | Batch: 010 / 011 | Total loss: 0.861 | Reg loss: 0.028 | Tree loss: 0.861 | Accuracy: 0.703072 | 0.528 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 214 | Batch: 000 / 011 | Total loss: 0.889 | Reg loss: 0.028 | Tree loss: 0.889 | Accuracy: 0.659500 | 0.53 sec/iter\n",
      "Epoch: 214 | Batch: 001 / 011 | Total loss: 0.881 | Reg loss: 0.028 | Tree loss: 0.881 | Accuracy: 0.645000 | 0.53 sec/iter\n",
      "Epoch: 214 | Batch: 002 / 011 | Total loss: 0.856 | Reg loss: 0.028 | Tree loss: 0.856 | Accuracy: 0.680500 | 0.529 sec/iter\n",
      "Epoch: 214 | Batch: 003 / 011 | Total loss: 0.823 | Reg loss: 0.028 | Tree loss: 0.823 | Accuracy: 0.714500 | 0.529 sec/iter\n",
      "Epoch: 214 | Batch: 004 / 011 | Total loss: 0.843 | Reg loss: 0.028 | Tree loss: 0.843 | Accuracy: 0.699000 | 0.529 sec/iter\n",
      "Epoch: 214 | Batch: 005 / 011 | Total loss: 0.813 | Reg loss: 0.028 | Tree loss: 0.813 | Accuracy: 0.715000 | 0.529 sec/iter\n",
      "Epoch: 214 | Batch: 006 / 011 | Total loss: 0.817 | Reg loss: 0.028 | Tree loss: 0.817 | Accuracy: 0.724000 | 0.529 sec/iter\n",
      "Epoch: 214 | Batch: 007 / 011 | Total loss: 0.801 | Reg loss: 0.028 | Tree loss: 0.801 | Accuracy: 0.738500 | 0.529 sec/iter\n",
      "Epoch: 214 | Batch: 008 / 011 | Total loss: 0.806 | Reg loss: 0.028 | Tree loss: 0.806 | Accuracy: 0.740000 | 0.529 sec/iter\n",
      "Epoch: 214 | Batch: 009 / 011 | Total loss: 0.815 | Reg loss: 0.028 | Tree loss: 0.815 | Accuracy: 0.720000 | 0.529 sec/iter\n",
      "Epoch: 214 | Batch: 010 / 011 | Total loss: 0.757 | Reg loss: 0.028 | Tree loss: 0.757 | Accuracy: 0.771331 | 0.528 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 215 | Batch: 000 / 011 | Total loss: 0.895 | Reg loss: 0.028 | Tree loss: 0.895 | Accuracy: 0.660500 | 0.531 sec/iter\n",
      "Epoch: 215 | Batch: 001 / 011 | Total loss: 0.892 | Reg loss: 0.028 | Tree loss: 0.892 | Accuracy: 0.650000 | 0.531 sec/iter\n",
      "Epoch: 215 | Batch: 002 / 011 | Total loss: 0.841 | Reg loss: 0.028 | Tree loss: 0.841 | Accuracy: 0.691500 | 0.531 sec/iter\n",
      "Epoch: 215 | Batch: 003 / 011 | Total loss: 0.861 | Reg loss: 0.028 | Tree loss: 0.861 | Accuracy: 0.685000 | 0.53 sec/iter\n",
      "Epoch: 215 | Batch: 004 / 011 | Total loss: 0.828 | Reg loss: 0.028 | Tree loss: 0.828 | Accuracy: 0.704000 | 0.53 sec/iter\n",
      "Epoch: 215 | Batch: 005 / 011 | Total loss: 0.809 | Reg loss: 0.028 | Tree loss: 0.809 | Accuracy: 0.732000 | 0.53 sec/iter\n",
      "Epoch: 215 | Batch: 006 / 011 | Total loss: 0.817 | Reg loss: 0.028 | Tree loss: 0.817 | Accuracy: 0.717000 | 0.53 sec/iter\n",
      "Epoch: 215 | Batch: 007 / 011 | Total loss: 0.792 | Reg loss: 0.028 | Tree loss: 0.792 | Accuracy: 0.743500 | 0.53 sec/iter\n",
      "Epoch: 215 | Batch: 008 / 011 | Total loss: 0.827 | Reg loss: 0.028 | Tree loss: 0.827 | Accuracy: 0.724500 | 0.53 sec/iter\n",
      "Epoch: 215 | Batch: 009 / 011 | Total loss: 0.775 | Reg loss: 0.028 | Tree loss: 0.775 | Accuracy: 0.755000 | 0.53 sec/iter\n",
      "Epoch: 215 | Batch: 010 / 011 | Total loss: 0.816 | Reg loss: 0.028 | Tree loss: 0.816 | Accuracy: 0.737201 | 0.53 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 216 | Batch: 000 / 011 | Total loss: 0.892 | Reg loss: 0.028 | Tree loss: 0.892 | Accuracy: 0.655000 | 0.531 sec/iter\n",
      "Epoch: 216 | Batch: 001 / 011 | Total loss: 0.887 | Reg loss: 0.028 | Tree loss: 0.887 | Accuracy: 0.664500 | 0.531 sec/iter\n",
      "Epoch: 216 | Batch: 002 / 011 | Total loss: 0.883 | Reg loss: 0.028 | Tree loss: 0.883 | Accuracy: 0.666000 | 0.531 sec/iter\n",
      "Epoch: 216 | Batch: 003 / 011 | Total loss: 0.843 | Reg loss: 0.028 | Tree loss: 0.843 | Accuracy: 0.702500 | 0.531 sec/iter\n",
      "Epoch: 216 | Batch: 004 / 011 | Total loss: 0.821 | Reg loss: 0.028 | Tree loss: 0.821 | Accuracy: 0.725000 | 0.531 sec/iter\n",
      "Epoch: 216 | Batch: 005 / 011 | Total loss: 0.807 | Reg loss: 0.028 | Tree loss: 0.807 | Accuracy: 0.726000 | 0.53 sec/iter\n",
      "Epoch: 216 | Batch: 006 / 011 | Total loss: 0.804 | Reg loss: 0.028 | Tree loss: 0.804 | Accuracy: 0.731000 | 0.53 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 216 | Batch: 007 / 011 | Total loss: 0.809 | Reg loss: 0.028 | Tree loss: 0.809 | Accuracy: 0.713500 | 0.53 sec/iter\n",
      "Epoch: 216 | Batch: 008 / 011 | Total loss: 0.813 | Reg loss: 0.028 | Tree loss: 0.813 | Accuracy: 0.719500 | 0.53 sec/iter\n",
      "Epoch: 216 | Batch: 009 / 011 | Total loss: 0.797 | Reg loss: 0.028 | Tree loss: 0.797 | Accuracy: 0.729500 | 0.53 sec/iter\n",
      "Epoch: 216 | Batch: 010 / 011 | Total loss: 0.738 | Reg loss: 0.028 | Tree loss: 0.738 | Accuracy: 0.761092 | 0.53 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 217 | Batch: 000 / 011 | Total loss: 0.919 | Reg loss: 0.028 | Tree loss: 0.919 | Accuracy: 0.655500 | 0.531 sec/iter\n",
      "Epoch: 217 | Batch: 001 / 011 | Total loss: 0.873 | Reg loss: 0.028 | Tree loss: 0.873 | Accuracy: 0.670000 | 0.531 sec/iter\n",
      "Epoch: 217 | Batch: 002 / 011 | Total loss: 0.867 | Reg loss: 0.028 | Tree loss: 0.867 | Accuracy: 0.693500 | 0.531 sec/iter\n",
      "Epoch: 217 | Batch: 003 / 011 | Total loss: 0.854 | Reg loss: 0.028 | Tree loss: 0.854 | Accuracy: 0.698000 | 0.531 sec/iter\n",
      "Epoch: 217 | Batch: 004 / 011 | Total loss: 0.829 | Reg loss: 0.028 | Tree loss: 0.829 | Accuracy: 0.716000 | 0.531 sec/iter\n",
      "Epoch: 217 | Batch: 005 / 011 | Total loss: 0.792 | Reg loss: 0.028 | Tree loss: 0.792 | Accuracy: 0.734500 | 0.531 sec/iter\n",
      "Epoch: 217 | Batch: 006 / 011 | Total loss: 0.801 | Reg loss: 0.028 | Tree loss: 0.801 | Accuracy: 0.743500 | 0.531 sec/iter\n",
      "Epoch: 217 | Batch: 007 / 011 | Total loss: 0.793 | Reg loss: 0.028 | Tree loss: 0.793 | Accuracy: 0.739000 | 0.53 sec/iter\n",
      "Epoch: 217 | Batch: 008 / 011 | Total loss: 0.792 | Reg loss: 0.028 | Tree loss: 0.792 | Accuracy: 0.749500 | 0.53 sec/iter\n",
      "Epoch: 217 | Batch: 009 / 011 | Total loss: 0.786 | Reg loss: 0.028 | Tree loss: 0.786 | Accuracy: 0.731500 | 0.53 sec/iter\n",
      "Epoch: 217 | Batch: 010 / 011 | Total loss: 0.804 | Reg loss: 0.028 | Tree loss: 0.804 | Accuracy: 0.713311 | 0.53 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 218 | Batch: 000 / 011 | Total loss: 0.898 | Reg loss: 0.028 | Tree loss: 0.898 | Accuracy: 0.650000 | 0.532 sec/iter\n",
      "Epoch: 218 | Batch: 001 / 011 | Total loss: 0.889 | Reg loss: 0.028 | Tree loss: 0.889 | Accuracy: 0.657000 | 0.532 sec/iter\n",
      "Epoch: 218 | Batch: 002 / 011 | Total loss: 0.848 | Reg loss: 0.028 | Tree loss: 0.848 | Accuracy: 0.695500 | 0.532 sec/iter\n",
      "Epoch: 218 | Batch: 003 / 011 | Total loss: 0.849 | Reg loss: 0.028 | Tree loss: 0.849 | Accuracy: 0.692000 | 0.532 sec/iter\n",
      "Epoch: 218 | Batch: 004 / 011 | Total loss: 0.824 | Reg loss: 0.028 | Tree loss: 0.824 | Accuracy: 0.715000 | 0.532 sec/iter\n",
      "Epoch: 218 | Batch: 005 / 011 | Total loss: 0.791 | Reg loss: 0.028 | Tree loss: 0.791 | Accuracy: 0.734500 | 0.532 sec/iter\n",
      "Epoch: 218 | Batch: 006 / 011 | Total loss: 0.819 | Reg loss: 0.028 | Tree loss: 0.819 | Accuracy: 0.719000 | 0.532 sec/iter\n",
      "Epoch: 218 | Batch: 007 / 011 | Total loss: 0.802 | Reg loss: 0.028 | Tree loss: 0.802 | Accuracy: 0.739000 | 0.532 sec/iter\n",
      "Epoch: 218 | Batch: 008 / 011 | Total loss: 0.796 | Reg loss: 0.028 | Tree loss: 0.796 | Accuracy: 0.745500 | 0.532 sec/iter\n",
      "Epoch: 218 | Batch: 009 / 011 | Total loss: 0.811 | Reg loss: 0.028 | Tree loss: 0.811 | Accuracy: 0.712000 | 0.531 sec/iter\n",
      "Epoch: 218 | Batch: 010 / 011 | Total loss: 0.807 | Reg loss: 0.028 | Tree loss: 0.807 | Accuracy: 0.716724 | 0.531 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 219 | Batch: 000 / 011 | Total loss: 0.900 | Reg loss: 0.028 | Tree loss: 0.900 | Accuracy: 0.661500 | 0.533 sec/iter\n",
      "Epoch: 219 | Batch: 001 / 011 | Total loss: 0.881 | Reg loss: 0.028 | Tree loss: 0.881 | Accuracy: 0.655000 | 0.533 sec/iter\n",
      "Epoch: 219 | Batch: 002 / 011 | Total loss: 0.848 | Reg loss: 0.028 | Tree loss: 0.848 | Accuracy: 0.688500 | 0.533 sec/iter\n",
      "Epoch: 219 | Batch: 003 / 011 | Total loss: 0.844 | Reg loss: 0.028 | Tree loss: 0.844 | Accuracy: 0.695500 | 0.533 sec/iter\n",
      "Epoch: 219 | Batch: 004 / 011 | Total loss: 0.817 | Reg loss: 0.028 | Tree loss: 0.817 | Accuracy: 0.727500 | 0.533 sec/iter\n",
      "Epoch: 219 | Batch: 005 / 011 | Total loss: 0.820 | Reg loss: 0.028 | Tree loss: 0.820 | Accuracy: 0.702500 | 0.533 sec/iter\n",
      "Epoch: 219 | Batch: 006 / 011 | Total loss: 0.790 | Reg loss: 0.028 | Tree loss: 0.790 | Accuracy: 0.722000 | 0.533 sec/iter\n",
      "Epoch: 219 | Batch: 007 / 011 | Total loss: 0.818 | Reg loss: 0.028 | Tree loss: 0.818 | Accuracy: 0.710000 | 0.532 sec/iter\n",
      "Epoch: 219 | Batch: 008 / 011 | Total loss: 0.796 | Reg loss: 0.028 | Tree loss: 0.796 | Accuracy: 0.731000 | 0.532 sec/iter\n",
      "Epoch: 219 | Batch: 009 / 011 | Total loss: 0.803 | Reg loss: 0.028 | Tree loss: 0.803 | Accuracy: 0.734000 | 0.532 sec/iter\n",
      "Epoch: 219 | Batch: 010 / 011 | Total loss: 0.867 | Reg loss: 0.028 | Tree loss: 0.867 | Accuracy: 0.686007 | 0.532 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 220 | Batch: 000 / 011 | Total loss: 0.905 | Reg loss: 0.028 | Tree loss: 0.905 | Accuracy: 0.645500 | 0.533 sec/iter\n",
      "Epoch: 220 | Batch: 001 / 011 | Total loss: 0.872 | Reg loss: 0.028 | Tree loss: 0.872 | Accuracy: 0.677000 | 0.533 sec/iter\n",
      "Epoch: 220 | Batch: 002 / 011 | Total loss: 0.861 | Reg loss: 0.028 | Tree loss: 0.861 | Accuracy: 0.696500 | 0.533 sec/iter\n",
      "Epoch: 220 | Batch: 003 / 011 | Total loss: 0.832 | Reg loss: 0.028 | Tree loss: 0.832 | Accuracy: 0.703000 | 0.533 sec/iter\n",
      "Epoch: 220 | Batch: 004 / 011 | Total loss: 0.841 | Reg loss: 0.028 | Tree loss: 0.841 | Accuracy: 0.696500 | 0.533 sec/iter\n",
      "Epoch: 220 | Batch: 005 / 011 | Total loss: 0.798 | Reg loss: 0.028 | Tree loss: 0.798 | Accuracy: 0.722500 | 0.533 sec/iter\n",
      "Epoch: 220 | Batch: 006 / 011 | Total loss: 0.804 | Reg loss: 0.028 | Tree loss: 0.804 | Accuracy: 0.728500 | 0.533 sec/iter\n",
      "Epoch: 220 | Batch: 007 / 011 | Total loss: 0.793 | Reg loss: 0.028 | Tree loss: 0.793 | Accuracy: 0.736500 | 0.533 sec/iter\n",
      "Epoch: 220 | Batch: 008 / 011 | Total loss: 0.803 | Reg loss: 0.028 | Tree loss: 0.803 | Accuracy: 0.739500 | 0.532 sec/iter\n",
      "Epoch: 220 | Batch: 009 / 011 | Total loss: 0.800 | Reg loss: 0.028 | Tree loss: 0.800 | Accuracy: 0.729500 | 0.532 sec/iter\n",
      "Epoch: 220 | Batch: 010 / 011 | Total loss: 0.876 | Reg loss: 0.028 | Tree loss: 0.876 | Accuracy: 0.706485 | 0.532 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 221 | Batch: 000 / 011 | Total loss: 0.880 | Reg loss: 0.028 | Tree loss: 0.880 | Accuracy: 0.666500 | 0.532 sec/iter\n",
      "Epoch: 221 | Batch: 001 / 011 | Total loss: 0.872 | Reg loss: 0.028 | Tree loss: 0.872 | Accuracy: 0.673500 | 0.532 sec/iter\n",
      "Epoch: 221 | Batch: 002 / 011 | Total loss: 0.860 | Reg loss: 0.028 | Tree loss: 0.860 | Accuracy: 0.688000 | 0.532 sec/iter\n",
      "Epoch: 221 | Batch: 003 / 011 | Total loss: 0.846 | Reg loss: 0.028 | Tree loss: 0.846 | Accuracy: 0.702000 | 0.532 sec/iter\n",
      "Epoch: 221 | Batch: 004 / 011 | Total loss: 0.833 | Reg loss: 0.028 | Tree loss: 0.833 | Accuracy: 0.697000 | 0.532 sec/iter\n",
      "Epoch: 221 | Batch: 005 / 011 | Total loss: 0.818 | Reg loss: 0.028 | Tree loss: 0.818 | Accuracy: 0.717500 | 0.532 sec/iter\n",
      "Epoch: 221 | Batch: 006 / 011 | Total loss: 0.798 | Reg loss: 0.028 | Tree loss: 0.798 | Accuracy: 0.731500 | 0.532 sec/iter\n",
      "Epoch: 221 | Batch: 007 / 011 | Total loss: 0.802 | Reg loss: 0.028 | Tree loss: 0.802 | Accuracy: 0.744000 | 0.531 sec/iter\n",
      "Epoch: 221 | Batch: 008 / 011 | Total loss: 0.816 | Reg loss: 0.028 | Tree loss: 0.816 | Accuracy: 0.718000 | 0.531 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 221 | Batch: 009 / 011 | Total loss: 0.781 | Reg loss: 0.028 | Tree loss: 0.781 | Accuracy: 0.739000 | 0.531 sec/iter\n",
      "Epoch: 221 | Batch: 010 / 011 | Total loss: 0.808 | Reg loss: 0.028 | Tree loss: 0.808 | Accuracy: 0.754266 | 0.531 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 222 | Batch: 000 / 011 | Total loss: 0.908 | Reg loss: 0.028 | Tree loss: 0.908 | Accuracy: 0.642000 | 0.533 sec/iter\n",
      "Epoch: 222 | Batch: 001 / 011 | Total loss: 0.899 | Reg loss: 0.028 | Tree loss: 0.899 | Accuracy: 0.656000 | 0.532 sec/iter\n",
      "Epoch: 222 | Batch: 002 / 011 | Total loss: 0.847 | Reg loss: 0.028 | Tree loss: 0.847 | Accuracy: 0.695500 | 0.532 sec/iter\n",
      "Epoch: 222 | Batch: 003 / 011 | Total loss: 0.802 | Reg loss: 0.028 | Tree loss: 0.802 | Accuracy: 0.718500 | 0.532 sec/iter\n",
      "Epoch: 222 | Batch: 004 / 011 | Total loss: 0.826 | Reg loss: 0.028 | Tree loss: 0.826 | Accuracy: 0.717000 | 0.532 sec/iter\n",
      "Epoch: 222 | Batch: 005 / 011 | Total loss: 0.816 | Reg loss: 0.028 | Tree loss: 0.816 | Accuracy: 0.726000 | 0.532 sec/iter\n",
      "Epoch: 222 | Batch: 006 / 011 | Total loss: 0.806 | Reg loss: 0.028 | Tree loss: 0.806 | Accuracy: 0.718000 | 0.532 sec/iter\n",
      "Epoch: 222 | Batch: 007 / 011 | Total loss: 0.796 | Reg loss: 0.028 | Tree loss: 0.796 | Accuracy: 0.719000 | 0.532 sec/iter\n",
      "Epoch: 222 | Batch: 008 / 011 | Total loss: 0.825 | Reg loss: 0.028 | Tree loss: 0.825 | Accuracy: 0.716500 | 0.532 sec/iter\n",
      "Epoch: 222 | Batch: 009 / 011 | Total loss: 0.793 | Reg loss: 0.028 | Tree loss: 0.793 | Accuracy: 0.742500 | 0.531 sec/iter\n",
      "Epoch: 222 | Batch: 010 / 011 | Total loss: 0.848 | Reg loss: 0.028 | Tree loss: 0.848 | Accuracy: 0.692833 | 0.531 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 223 | Batch: 000 / 011 | Total loss: 0.888 | Reg loss: 0.028 | Tree loss: 0.888 | Accuracy: 0.659000 | 0.533 sec/iter\n",
      "Epoch: 223 | Batch: 001 / 011 | Total loss: 0.892 | Reg loss: 0.028 | Tree loss: 0.892 | Accuracy: 0.658000 | 0.532 sec/iter\n",
      "Epoch: 223 | Batch: 002 / 011 | Total loss: 0.860 | Reg loss: 0.028 | Tree loss: 0.860 | Accuracy: 0.684500 | 0.532 sec/iter\n",
      "Epoch: 223 | Batch: 003 / 011 | Total loss: 0.841 | Reg loss: 0.028 | Tree loss: 0.841 | Accuracy: 0.708000 | 0.532 sec/iter\n",
      "Epoch: 223 | Batch: 004 / 011 | Total loss: 0.817 | Reg loss: 0.028 | Tree loss: 0.817 | Accuracy: 0.727000 | 0.532 sec/iter\n",
      "Epoch: 223 | Batch: 005 / 011 | Total loss: 0.807 | Reg loss: 0.028 | Tree loss: 0.807 | Accuracy: 0.720000 | 0.532 sec/iter\n",
      "Epoch: 223 | Batch: 006 / 011 | Total loss: 0.828 | Reg loss: 0.028 | Tree loss: 0.828 | Accuracy: 0.716500 | 0.532 sec/iter\n",
      "Epoch: 223 | Batch: 007 / 011 | Total loss: 0.785 | Reg loss: 0.028 | Tree loss: 0.785 | Accuracy: 0.737500 | 0.532 sec/iter\n",
      "Epoch: 223 | Batch: 008 / 011 | Total loss: 0.789 | Reg loss: 0.028 | Tree loss: 0.789 | Accuracy: 0.734500 | 0.532 sec/iter\n",
      "Epoch: 223 | Batch: 009 / 011 | Total loss: 0.805 | Reg loss: 0.028 | Tree loss: 0.805 | Accuracy: 0.729000 | 0.531 sec/iter\n",
      "Epoch: 223 | Batch: 010 / 011 | Total loss: 0.795 | Reg loss: 0.028 | Tree loss: 0.795 | Accuracy: 0.740614 | 0.531 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 224 | Batch: 000 / 011 | Total loss: 0.902 | Reg loss: 0.028 | Tree loss: 0.902 | Accuracy: 0.651500 | 0.531 sec/iter\n",
      "Epoch: 224 | Batch: 001 / 011 | Total loss: 0.887 | Reg loss: 0.028 | Tree loss: 0.887 | Accuracy: 0.667500 | 0.531 sec/iter\n",
      "Epoch: 224 | Batch: 002 / 011 | Total loss: 0.853 | Reg loss: 0.028 | Tree loss: 0.853 | Accuracy: 0.700500 | 0.531 sec/iter\n",
      "Epoch: 224 | Batch: 003 / 011 | Total loss: 0.831 | Reg loss: 0.028 | Tree loss: 0.831 | Accuracy: 0.705000 | 0.531 sec/iter\n",
      "Epoch: 224 | Batch: 004 / 011 | Total loss: 0.839 | Reg loss: 0.028 | Tree loss: 0.839 | Accuracy: 0.703500 | 0.531 sec/iter\n",
      "Epoch: 224 | Batch: 005 / 011 | Total loss: 0.803 | Reg loss: 0.028 | Tree loss: 0.803 | Accuracy: 0.721500 | 0.531 sec/iter\n",
      "Epoch: 224 | Batch: 006 / 011 | Total loss: 0.794 | Reg loss: 0.028 | Tree loss: 0.794 | Accuracy: 0.720000 | 0.531 sec/iter\n",
      "Epoch: 224 | Batch: 007 / 011 | Total loss: 0.797 | Reg loss: 0.028 | Tree loss: 0.797 | Accuracy: 0.718500 | 0.53 sec/iter\n",
      "Epoch: 224 | Batch: 008 / 011 | Total loss: 0.823 | Reg loss: 0.028 | Tree loss: 0.823 | Accuracy: 0.709000 | 0.53 sec/iter\n",
      "Epoch: 224 | Batch: 009 / 011 | Total loss: 0.777 | Reg loss: 0.028 | Tree loss: 0.777 | Accuracy: 0.741000 | 0.53 sec/iter\n",
      "Epoch: 224 | Batch: 010 / 011 | Total loss: 0.891 | Reg loss: 0.028 | Tree loss: 0.891 | Accuracy: 0.682594 | 0.53 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 225 | Batch: 000 / 011 | Total loss: 0.896 | Reg loss: 0.028 | Tree loss: 0.896 | Accuracy: 0.656500 | 0.532 sec/iter\n",
      "Epoch: 225 | Batch: 001 / 011 | Total loss: 0.873 | Reg loss: 0.028 | Tree loss: 0.873 | Accuracy: 0.656500 | 0.531 sec/iter\n",
      "Epoch: 225 | Batch: 002 / 011 | Total loss: 0.838 | Reg loss: 0.028 | Tree loss: 0.838 | Accuracy: 0.707000 | 0.531 sec/iter\n",
      "Epoch: 225 | Batch: 003 / 011 | Total loss: 0.852 | Reg loss: 0.028 | Tree loss: 0.852 | Accuracy: 0.684500 | 0.531 sec/iter\n",
      "Epoch: 225 | Batch: 004 / 011 | Total loss: 0.835 | Reg loss: 0.028 | Tree loss: 0.835 | Accuracy: 0.711500 | 0.531 sec/iter\n",
      "Epoch: 225 | Batch: 005 / 011 | Total loss: 0.834 | Reg loss: 0.028 | Tree loss: 0.834 | Accuracy: 0.711500 | 0.531 sec/iter\n",
      "Epoch: 225 | Batch: 006 / 011 | Total loss: 0.777 | Reg loss: 0.028 | Tree loss: 0.777 | Accuracy: 0.749000 | 0.531 sec/iter\n",
      "Epoch: 225 | Batch: 007 / 011 | Total loss: 0.817 | Reg loss: 0.028 | Tree loss: 0.817 | Accuracy: 0.735500 | 0.531 sec/iter\n",
      "Epoch: 225 | Batch: 008 / 011 | Total loss: 0.809 | Reg loss: 0.028 | Tree loss: 0.809 | Accuracy: 0.718500 | 0.531 sec/iter\n",
      "Epoch: 225 | Batch: 009 / 011 | Total loss: 0.795 | Reg loss: 0.028 | Tree loss: 0.795 | Accuracy: 0.717000 | 0.53 sec/iter\n",
      "Epoch: 225 | Batch: 010 / 011 | Total loss: 0.791 | Reg loss: 0.028 | Tree loss: 0.791 | Accuracy: 0.733788 | 0.53 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 226 | Batch: 000 / 011 | Total loss: 0.876 | Reg loss: 0.028 | Tree loss: 0.876 | Accuracy: 0.660500 | 0.532 sec/iter\n",
      "Epoch: 226 | Batch: 001 / 011 | Total loss: 0.881 | Reg loss: 0.028 | Tree loss: 0.881 | Accuracy: 0.644000 | 0.531 sec/iter\n",
      "Epoch: 226 | Batch: 002 / 011 | Total loss: 0.860 | Reg loss: 0.028 | Tree loss: 0.860 | Accuracy: 0.670500 | 0.531 sec/iter\n",
      "Epoch: 226 | Batch: 003 / 011 | Total loss: 0.842 | Reg loss: 0.028 | Tree loss: 0.842 | Accuracy: 0.712000 | 0.531 sec/iter\n",
      "Epoch: 226 | Batch: 004 / 011 | Total loss: 0.824 | Reg loss: 0.028 | Tree loss: 0.824 | Accuracy: 0.716500 | 0.531 sec/iter\n",
      "Epoch: 226 | Batch: 005 / 011 | Total loss: 0.807 | Reg loss: 0.028 | Tree loss: 0.807 | Accuracy: 0.712000 | 0.531 sec/iter\n",
      "Epoch: 226 | Batch: 006 / 011 | Total loss: 0.804 | Reg loss: 0.028 | Tree loss: 0.804 | Accuracy: 0.743000 | 0.531 sec/iter\n",
      "Epoch: 226 | Batch: 007 / 011 | Total loss: 0.811 | Reg loss: 0.028 | Tree loss: 0.811 | Accuracy: 0.731500 | 0.531 sec/iter\n",
      "Epoch: 226 | Batch: 008 / 011 | Total loss: 0.809 | Reg loss: 0.028 | Tree loss: 0.809 | Accuracy: 0.731000 | 0.531 sec/iter\n",
      "Epoch: 226 | Batch: 009 / 011 | Total loss: 0.784 | Reg loss: 0.028 | Tree loss: 0.784 | Accuracy: 0.734500 | 0.53 sec/iter\n",
      "Epoch: 226 | Batch: 010 / 011 | Total loss: 0.829 | Reg loss: 0.028 | Tree loss: 0.829 | Accuracy: 0.737201 | 0.53 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 227 | Batch: 000 / 011 | Total loss: 0.894 | Reg loss: 0.028 | Tree loss: 0.894 | Accuracy: 0.652500 | 0.53 sec/iter\n",
      "Epoch: 227 | Batch: 001 / 011 | Total loss: 0.863 | Reg loss: 0.028 | Tree loss: 0.863 | Accuracy: 0.673000 | 0.53 sec/iter\n",
      "Epoch: 227 | Batch: 002 / 011 | Total loss: 0.843 | Reg loss: 0.028 | Tree loss: 0.843 | Accuracy: 0.706000 | 0.53 sec/iter\n",
      "Epoch: 227 | Batch: 003 / 011 | Total loss: 0.837 | Reg loss: 0.028 | Tree loss: 0.837 | Accuracy: 0.699500 | 0.53 sec/iter\n",
      "Epoch: 227 | Batch: 004 / 011 | Total loss: 0.829 | Reg loss: 0.028 | Tree loss: 0.829 | Accuracy: 0.709500 | 0.53 sec/iter\n",
      "Epoch: 227 | Batch: 005 / 011 | Total loss: 0.817 | Reg loss: 0.028 | Tree loss: 0.817 | Accuracy: 0.722500 | 0.53 sec/iter\n",
      "Epoch: 227 | Batch: 006 / 011 | Total loss: 0.786 | Reg loss: 0.028 | Tree loss: 0.786 | Accuracy: 0.745500 | 0.53 sec/iter\n",
      "Epoch: 227 | Batch: 007 / 011 | Total loss: 0.812 | Reg loss: 0.028 | Tree loss: 0.812 | Accuracy: 0.727000 | 0.53 sec/iter\n",
      "Epoch: 227 | Batch: 008 / 011 | Total loss: 0.817 | Reg loss: 0.028 | Tree loss: 0.817 | Accuracy: 0.728500 | 0.529 sec/iter\n",
      "Epoch: 227 | Batch: 009 / 011 | Total loss: 0.811 | Reg loss: 0.028 | Tree loss: 0.811 | Accuracy: 0.715000 | 0.529 sec/iter\n",
      "Epoch: 227 | Batch: 010 / 011 | Total loss: 0.759 | Reg loss: 0.028 | Tree loss: 0.759 | Accuracy: 0.750853 | 0.529 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 228 | Batch: 000 / 011 | Total loss: 0.874 | Reg loss: 0.028 | Tree loss: 0.874 | Accuracy: 0.666000 | 0.532 sec/iter\n",
      "Epoch: 228 | Batch: 001 / 011 | Total loss: 0.874 | Reg loss: 0.028 | Tree loss: 0.874 | Accuracy: 0.659000 | 0.532 sec/iter\n",
      "Epoch: 228 | Batch: 002 / 011 | Total loss: 0.874 | Reg loss: 0.028 | Tree loss: 0.874 | Accuracy: 0.684000 | 0.531 sec/iter\n",
      "Epoch: 228 | Batch: 003 / 011 | Total loss: 0.849 | Reg loss: 0.028 | Tree loss: 0.849 | Accuracy: 0.687500 | 0.531 sec/iter\n",
      "Epoch: 228 | Batch: 004 / 011 | Total loss: 0.825 | Reg loss: 0.028 | Tree loss: 0.825 | Accuracy: 0.703000 | 0.531 sec/iter\n",
      "Epoch: 228 | Batch: 005 / 011 | Total loss: 0.814 | Reg loss: 0.028 | Tree loss: 0.814 | Accuracy: 0.721500 | 0.531 sec/iter\n",
      "Epoch: 228 | Batch: 006 / 011 | Total loss: 0.798 | Reg loss: 0.028 | Tree loss: 0.798 | Accuracy: 0.727500 | 0.531 sec/iter\n",
      "Epoch: 228 | Batch: 007 / 011 | Total loss: 0.788 | Reg loss: 0.028 | Tree loss: 0.788 | Accuracy: 0.735500 | 0.531 sec/iter\n",
      "Epoch: 228 | Batch: 008 / 011 | Total loss: 0.814 | Reg loss: 0.028 | Tree loss: 0.814 | Accuracy: 0.720000 | 0.531 sec/iter\n",
      "Epoch: 228 | Batch: 009 / 011 | Total loss: 0.825 | Reg loss: 0.028 | Tree loss: 0.825 | Accuracy: 0.703000 | 0.531 sec/iter\n",
      "Epoch: 228 | Batch: 010 / 011 | Total loss: 0.752 | Reg loss: 0.028 | Tree loss: 0.752 | Accuracy: 0.747440 | 0.531 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 229 | Batch: 000 / 011 | Total loss: 0.879 | Reg loss: 0.028 | Tree loss: 0.879 | Accuracy: 0.664000 | 0.531 sec/iter\n",
      "Epoch: 229 | Batch: 001 / 011 | Total loss: 0.876 | Reg loss: 0.028 | Tree loss: 0.876 | Accuracy: 0.669000 | 0.531 sec/iter\n",
      "Epoch: 229 | Batch: 002 / 011 | Total loss: 0.858 | Reg loss: 0.028 | Tree loss: 0.858 | Accuracy: 0.688500 | 0.531 sec/iter\n",
      "Epoch: 229 | Batch: 003 / 011 | Total loss: 0.851 | Reg loss: 0.028 | Tree loss: 0.851 | Accuracy: 0.699500 | 0.531 sec/iter\n",
      "Epoch: 229 | Batch: 004 / 011 | Total loss: 0.801 | Reg loss: 0.028 | Tree loss: 0.801 | Accuracy: 0.741000 | 0.531 sec/iter\n",
      "Epoch: 229 | Batch: 005 / 011 | Total loss: 0.810 | Reg loss: 0.028 | Tree loss: 0.810 | Accuracy: 0.723500 | 0.531 sec/iter\n",
      "Epoch: 229 | Batch: 006 / 011 | Total loss: 0.797 | Reg loss: 0.028 | Tree loss: 0.797 | Accuracy: 0.750500 | 0.531 sec/iter\n",
      "Epoch: 229 | Batch: 007 / 011 | Total loss: 0.826 | Reg loss: 0.028 | Tree loss: 0.826 | Accuracy: 0.712500 | 0.53 sec/iter\n",
      "Epoch: 229 | Batch: 008 / 011 | Total loss: 0.790 | Reg loss: 0.028 | Tree loss: 0.790 | Accuracy: 0.727000 | 0.53 sec/iter\n",
      "Epoch: 229 | Batch: 009 / 011 | Total loss: 0.820 | Reg loss: 0.028 | Tree loss: 0.820 | Accuracy: 0.719500 | 0.53 sec/iter\n",
      "Epoch: 229 | Batch: 010 / 011 | Total loss: 0.757 | Reg loss: 0.028 | Tree loss: 0.757 | Accuracy: 0.730375 | 0.53 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 230 | Batch: 000 / 011 | Total loss: 0.873 | Reg loss: 0.028 | Tree loss: 0.873 | Accuracy: 0.672500 | 0.531 sec/iter\n",
      "Epoch: 230 | Batch: 001 / 011 | Total loss: 0.885 | Reg loss: 0.028 | Tree loss: 0.885 | Accuracy: 0.678000 | 0.531 sec/iter\n",
      "Epoch: 230 | Batch: 002 / 011 | Total loss: 0.866 | Reg loss: 0.028 | Tree loss: 0.866 | Accuracy: 0.678500 | 0.531 sec/iter\n",
      "Epoch: 230 | Batch: 003 / 011 | Total loss: 0.838 | Reg loss: 0.028 | Tree loss: 0.838 | Accuracy: 0.684000 | 0.531 sec/iter\n",
      "Epoch: 230 | Batch: 004 / 011 | Total loss: 0.818 | Reg loss: 0.028 | Tree loss: 0.818 | Accuracy: 0.721000 | 0.53 sec/iter\n",
      "Epoch: 230 | Batch: 005 / 011 | Total loss: 0.816 | Reg loss: 0.028 | Tree loss: 0.816 | Accuracy: 0.707500 | 0.53 sec/iter\n",
      "Epoch: 230 | Batch: 006 / 011 | Total loss: 0.798 | Reg loss: 0.028 | Tree loss: 0.798 | Accuracy: 0.720000 | 0.53 sec/iter\n",
      "Epoch: 230 | Batch: 007 / 011 | Total loss: 0.809 | Reg loss: 0.028 | Tree loss: 0.809 | Accuracy: 0.708000 | 0.53 sec/iter\n",
      "Epoch: 230 | Batch: 008 / 011 | Total loss: 0.803 | Reg loss: 0.028 | Tree loss: 0.803 | Accuracy: 0.732000 | 0.53 sec/iter\n",
      "Epoch: 230 | Batch: 009 / 011 | Total loss: 0.799 | Reg loss: 0.028 | Tree loss: 0.799 | Accuracy: 0.734500 | 0.53 sec/iter\n",
      "Epoch: 230 | Batch: 010 / 011 | Total loss: 0.805 | Reg loss: 0.028 | Tree loss: 0.805 | Accuracy: 0.692833 | 0.53 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 231 | Batch: 000 / 011 | Total loss: 0.884 | Reg loss: 0.028 | Tree loss: 0.884 | Accuracy: 0.663500 | 0.531 sec/iter\n",
      "Epoch: 231 | Batch: 001 / 011 | Total loss: 0.895 | Reg loss: 0.028 | Tree loss: 0.895 | Accuracy: 0.646000 | 0.531 sec/iter\n",
      "Epoch: 231 | Batch: 002 / 011 | Total loss: 0.874 | Reg loss: 0.028 | Tree loss: 0.874 | Accuracy: 0.674000 | 0.531 sec/iter\n",
      "Epoch: 231 | Batch: 003 / 011 | Total loss: 0.816 | Reg loss: 0.028 | Tree loss: 0.816 | Accuracy: 0.701500 | 0.531 sec/iter\n",
      "Epoch: 231 | Batch: 004 / 011 | Total loss: 0.809 | Reg loss: 0.028 | Tree loss: 0.809 | Accuracy: 0.723500 | 0.531 sec/iter\n",
      "Epoch: 231 | Batch: 005 / 011 | Total loss: 0.795 | Reg loss: 0.028 | Tree loss: 0.795 | Accuracy: 0.727000 | 0.531 sec/iter\n",
      "Epoch: 231 | Batch: 006 / 011 | Total loss: 0.814 | Reg loss: 0.028 | Tree loss: 0.814 | Accuracy: 0.721000 | 0.53 sec/iter\n",
      "Epoch: 231 | Batch: 007 / 011 | Total loss: 0.805 | Reg loss: 0.028 | Tree loss: 0.805 | Accuracy: 0.732500 | 0.53 sec/iter\n",
      "Epoch: 231 | Batch: 008 / 011 | Total loss: 0.804 | Reg loss: 0.028 | Tree loss: 0.804 | Accuracy: 0.733000 | 0.53 sec/iter\n",
      "Epoch: 231 | Batch: 009 / 011 | Total loss: 0.803 | Reg loss: 0.028 | Tree loss: 0.803 | Accuracy: 0.727000 | 0.53 sec/iter\n",
      "Epoch: 231 | Batch: 010 / 011 | Total loss: 0.825 | Reg loss: 0.028 | Tree loss: 0.825 | Accuracy: 0.716724 | 0.53 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 232 | Batch: 000 / 011 | Total loss: 0.876 | Reg loss: 0.028 | Tree loss: 0.876 | Accuracy: 0.652000 | 0.53 sec/iter\n",
      "Epoch: 232 | Batch: 001 / 011 | Total loss: 0.863 | Reg loss: 0.028 | Tree loss: 0.863 | Accuracy: 0.678000 | 0.53 sec/iter\n",
      "Epoch: 232 | Batch: 002 / 011 | Total loss: 0.878 | Reg loss: 0.028 | Tree loss: 0.878 | Accuracy: 0.658000 | 0.53 sec/iter\n",
      "Epoch: 232 | Batch: 003 / 011 | Total loss: 0.831 | Reg loss: 0.028 | Tree loss: 0.831 | Accuracy: 0.696500 | 0.53 sec/iter\n",
      "Epoch: 232 | Batch: 004 / 011 | Total loss: 0.814 | Reg loss: 0.028 | Tree loss: 0.814 | Accuracy: 0.723000 | 0.53 sec/iter\n",
      "Epoch: 232 | Batch: 005 / 011 | Total loss: 0.816 | Reg loss: 0.028 | Tree loss: 0.816 | Accuracy: 0.718500 | 0.53 sec/iter\n",
      "Epoch: 232 | Batch: 006 / 011 | Total loss: 0.815 | Reg loss: 0.028 | Tree loss: 0.815 | Accuracy: 0.728500 | 0.529 sec/iter\n",
      "Epoch: 232 | Batch: 007 / 011 | Total loss: 0.791 | Reg loss: 0.028 | Tree loss: 0.791 | Accuracy: 0.738000 | 0.529 sec/iter\n",
      "Epoch: 232 | Batch: 008 / 011 | Total loss: 0.794 | Reg loss: 0.028 | Tree loss: 0.794 | Accuracy: 0.723000 | 0.529 sec/iter\n",
      "Epoch: 232 | Batch: 009 / 011 | Total loss: 0.811 | Reg loss: 0.028 | Tree loss: 0.811 | Accuracy: 0.726500 | 0.529 sec/iter\n",
      "Epoch: 232 | Batch: 010 / 011 | Total loss: 0.762 | Reg loss: 0.028 | Tree loss: 0.762 | Accuracy: 0.774744 | 0.529 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 233 | Batch: 000 / 011 | Total loss: 0.877 | Reg loss: 0.028 | Tree loss: 0.877 | Accuracy: 0.656500 | 0.53 sec/iter\n",
      "Epoch: 233 | Batch: 001 / 011 | Total loss: 0.883 | Reg loss: 0.028 | Tree loss: 0.883 | Accuracy: 0.671000 | 0.53 sec/iter\n",
      "Epoch: 233 | Batch: 002 / 011 | Total loss: 0.839 | Reg loss: 0.028 | Tree loss: 0.839 | Accuracy: 0.696500 | 0.53 sec/iter\n",
      "Epoch: 233 | Batch: 003 / 011 | Total loss: 0.836 | Reg loss: 0.028 | Tree loss: 0.836 | Accuracy: 0.721500 | 0.53 sec/iter\n",
      "Epoch: 233 | Batch: 004 / 011 | Total loss: 0.814 | Reg loss: 0.028 | Tree loss: 0.814 | Accuracy: 0.719000 | 0.53 sec/iter\n",
      "Epoch: 233 | Batch: 005 / 011 | Total loss: 0.824 | Reg loss: 0.028 | Tree loss: 0.824 | Accuracy: 0.703000 | 0.529 sec/iter\n",
      "Epoch: 233 | Batch: 006 / 011 | Total loss: 0.790 | Reg loss: 0.028 | Tree loss: 0.790 | Accuracy: 0.740000 | 0.529 sec/iter\n",
      "Epoch: 233 | Batch: 007 / 011 | Total loss: 0.806 | Reg loss: 0.028 | Tree loss: 0.806 | Accuracy: 0.725500 | 0.529 sec/iter\n",
      "Epoch: 233 | Batch: 008 / 011 | Total loss: 0.805 | Reg loss: 0.028 | Tree loss: 0.805 | Accuracy: 0.722000 | 0.529 sec/iter\n",
      "Epoch: 233 | Batch: 009 / 011 | Total loss: 0.814 | Reg loss: 0.028 | Tree loss: 0.814 | Accuracy: 0.725500 | 0.529 sec/iter\n",
      "Epoch: 233 | Batch: 010 / 011 | Total loss: 0.761 | Reg loss: 0.028 | Tree loss: 0.761 | Accuracy: 0.761092 | 0.529 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 234 | Batch: 000 / 011 | Total loss: 0.871 | Reg loss: 0.028 | Tree loss: 0.871 | Accuracy: 0.660500 | 0.529 sec/iter\n",
      "Epoch: 234 | Batch: 001 / 011 | Total loss: 0.867 | Reg loss: 0.028 | Tree loss: 0.867 | Accuracy: 0.673500 | 0.529 sec/iter\n",
      "Epoch: 234 | Batch: 002 / 011 | Total loss: 0.870 | Reg loss: 0.028 | Tree loss: 0.870 | Accuracy: 0.666500 | 0.529 sec/iter\n",
      "Epoch: 234 | Batch: 003 / 011 | Total loss: 0.822 | Reg loss: 0.028 | Tree loss: 0.822 | Accuracy: 0.710500 | 0.528 sec/iter\n",
      "Epoch: 234 | Batch: 004 / 011 | Total loss: 0.819 | Reg loss: 0.028 | Tree loss: 0.819 | Accuracy: 0.719500 | 0.528 sec/iter\n",
      "Epoch: 234 | Batch: 005 / 011 | Total loss: 0.806 | Reg loss: 0.028 | Tree loss: 0.806 | Accuracy: 0.714500 | 0.528 sec/iter\n",
      "Epoch: 234 | Batch: 006 / 011 | Total loss: 0.816 | Reg loss: 0.028 | Tree loss: 0.816 | Accuracy: 0.724500 | 0.528 sec/iter\n",
      "Epoch: 234 | Batch: 007 / 011 | Total loss: 0.802 | Reg loss: 0.028 | Tree loss: 0.802 | Accuracy: 0.721000 | 0.528 sec/iter\n",
      "Epoch: 234 | Batch: 008 / 011 | Total loss: 0.805 | Reg loss: 0.028 | Tree loss: 0.805 | Accuracy: 0.732500 | 0.528 sec/iter\n",
      "Epoch: 234 | Batch: 009 / 011 | Total loss: 0.813 | Reg loss: 0.028 | Tree loss: 0.813 | Accuracy: 0.721500 | 0.528 sec/iter\n",
      "Epoch: 234 | Batch: 010 / 011 | Total loss: 0.862 | Reg loss: 0.028 | Tree loss: 0.862 | Accuracy: 0.686007 | 0.528 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 235 | Batch: 000 / 011 | Total loss: 0.887 | Reg loss: 0.028 | Tree loss: 0.887 | Accuracy: 0.656500 | 0.529 sec/iter\n",
      "Epoch: 235 | Batch: 001 / 011 | Total loss: 0.899 | Reg loss: 0.028 | Tree loss: 0.899 | Accuracy: 0.664000 | 0.529 sec/iter\n",
      "Epoch: 235 | Batch: 002 / 011 | Total loss: 0.842 | Reg loss: 0.028 | Tree loss: 0.842 | Accuracy: 0.680500 | 0.529 sec/iter\n",
      "Epoch: 235 | Batch: 003 / 011 | Total loss: 0.838 | Reg loss: 0.028 | Tree loss: 0.838 | Accuracy: 0.707000 | 0.528 sec/iter\n",
      "Epoch: 235 | Batch: 004 / 011 | Total loss: 0.791 | Reg loss: 0.028 | Tree loss: 0.791 | Accuracy: 0.734500 | 0.528 sec/iter\n",
      "Epoch: 235 | Batch: 005 / 011 | Total loss: 0.787 | Reg loss: 0.028 | Tree loss: 0.787 | Accuracy: 0.732000 | 0.528 sec/iter\n",
      "Epoch: 235 | Batch: 006 / 011 | Total loss: 0.803 | Reg loss: 0.028 | Tree loss: 0.803 | Accuracy: 0.722500 | 0.528 sec/iter\n",
      "Epoch: 235 | Batch: 007 / 011 | Total loss: 0.813 | Reg loss: 0.028 | Tree loss: 0.813 | Accuracy: 0.725500 | 0.528 sec/iter\n",
      "Epoch: 235 | Batch: 008 / 011 | Total loss: 0.821 | Reg loss: 0.028 | Tree loss: 0.821 | Accuracy: 0.706000 | 0.528 sec/iter\n",
      "Epoch: 235 | Batch: 009 / 011 | Total loss: 0.805 | Reg loss: 0.028 | Tree loss: 0.805 | Accuracy: 0.734000 | 0.528 sec/iter\n",
      "Epoch: 235 | Batch: 010 / 011 | Total loss: 0.819 | Reg loss: 0.028 | Tree loss: 0.819 | Accuracy: 0.703072 | 0.528 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 236 | Batch: 000 / 011 | Total loss: 0.886 | Reg loss: 0.028 | Tree loss: 0.886 | Accuracy: 0.664500 | 0.529 sec/iter\n",
      "Epoch: 236 | Batch: 001 / 011 | Total loss: 0.866 | Reg loss: 0.028 | Tree loss: 0.866 | Accuracy: 0.671000 | 0.529 sec/iter\n",
      "Epoch: 236 | Batch: 002 / 011 | Total loss: 0.859 | Reg loss: 0.028 | Tree loss: 0.859 | Accuracy: 0.689500 | 0.529 sec/iter\n",
      "Epoch: 236 | Batch: 003 / 011 | Total loss: 0.828 | Reg loss: 0.028 | Tree loss: 0.828 | Accuracy: 0.706500 | 0.529 sec/iter\n",
      "Epoch: 236 | Batch: 004 / 011 | Total loss: 0.815 | Reg loss: 0.028 | Tree loss: 0.815 | Accuracy: 0.698500 | 0.529 sec/iter\n",
      "Epoch: 236 | Batch: 005 / 011 | Total loss: 0.805 | Reg loss: 0.028 | Tree loss: 0.805 | Accuracy: 0.727500 | 0.529 sec/iter\n",
      "Epoch: 236 | Batch: 006 / 011 | Total loss: 0.793 | Reg loss: 0.028 | Tree loss: 0.793 | Accuracy: 0.741500 | 0.528 sec/iter\n",
      "Epoch: 236 | Batch: 007 / 011 | Total loss: 0.803 | Reg loss: 0.028 | Tree loss: 0.803 | Accuracy: 0.720000 | 0.528 sec/iter\n",
      "Epoch: 236 | Batch: 008 / 011 | Total loss: 0.790 | Reg loss: 0.028 | Tree loss: 0.790 | Accuracy: 0.728500 | 0.528 sec/iter\n",
      "Epoch: 236 | Batch: 009 / 011 | Total loss: 0.822 | Reg loss: 0.028 | Tree loss: 0.822 | Accuracy: 0.721000 | 0.528 sec/iter\n",
      "Epoch: 236 | Batch: 010 / 011 | Total loss: 0.826 | Reg loss: 0.028 | Tree loss: 0.826 | Accuracy: 0.730375 | 0.528 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 237 | Batch: 000 / 011 | Total loss: 0.887 | Reg loss: 0.028 | Tree loss: 0.887 | Accuracy: 0.657000 | 0.529 sec/iter\n",
      "Epoch: 237 | Batch: 001 / 011 | Total loss: 0.893 | Reg loss: 0.028 | Tree loss: 0.893 | Accuracy: 0.657500 | 0.529 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 237 | Batch: 002 / 011 | Total loss: 0.853 | Reg loss: 0.028 | Tree loss: 0.853 | Accuracy: 0.680500 | 0.529 sec/iter\n",
      "Epoch: 237 | Batch: 003 / 011 | Total loss: 0.836 | Reg loss: 0.028 | Tree loss: 0.836 | Accuracy: 0.681500 | 0.529 sec/iter\n",
      "Epoch: 237 | Batch: 004 / 011 | Total loss: 0.832 | Reg loss: 0.028 | Tree loss: 0.832 | Accuracy: 0.705000 | 0.529 sec/iter\n",
      "Epoch: 237 | Batch: 005 / 011 | Total loss: 0.807 | Reg loss: 0.028 | Tree loss: 0.807 | Accuracy: 0.725000 | 0.528 sec/iter\n",
      "Epoch: 237 | Batch: 006 / 011 | Total loss: 0.787 | Reg loss: 0.028 | Tree loss: 0.787 | Accuracy: 0.721000 | 0.528 sec/iter\n",
      "Epoch: 237 | Batch: 007 / 011 | Total loss: 0.806 | Reg loss: 0.028 | Tree loss: 0.806 | Accuracy: 0.718500 | 0.528 sec/iter\n",
      "Epoch: 237 | Batch: 008 / 011 | Total loss: 0.800 | Reg loss: 0.028 | Tree loss: 0.800 | Accuracy: 0.730000 | 0.528 sec/iter\n",
      "Epoch: 237 | Batch: 009 / 011 | Total loss: 0.807 | Reg loss: 0.028 | Tree loss: 0.807 | Accuracy: 0.714500 | 0.528 sec/iter\n",
      "Epoch: 237 | Batch: 010 / 011 | Total loss: 0.844 | Reg loss: 0.028 | Tree loss: 0.844 | Accuracy: 0.689420 | 0.528 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 238 | Batch: 000 / 011 | Total loss: 0.890 | Reg loss: 0.028 | Tree loss: 0.890 | Accuracy: 0.652500 | 0.529 sec/iter\n",
      "Epoch: 238 | Batch: 001 / 011 | Total loss: 0.871 | Reg loss: 0.028 | Tree loss: 0.871 | Accuracy: 0.668000 | 0.529 sec/iter\n",
      "Epoch: 238 | Batch: 002 / 011 | Total loss: 0.875 | Reg loss: 0.028 | Tree loss: 0.875 | Accuracy: 0.684500 | 0.529 sec/iter\n",
      "Epoch: 238 | Batch: 003 / 011 | Total loss: 0.831 | Reg loss: 0.028 | Tree loss: 0.831 | Accuracy: 0.700000 | 0.529 sec/iter\n",
      "Epoch: 238 | Batch: 004 / 011 | Total loss: 0.802 | Reg loss: 0.028 | Tree loss: 0.802 | Accuracy: 0.722000 | 0.529 sec/iter\n",
      "Epoch: 238 | Batch: 005 / 011 | Total loss: 0.787 | Reg loss: 0.028 | Tree loss: 0.787 | Accuracy: 0.757500 | 0.529 sec/iter\n",
      "Epoch: 238 | Batch: 006 / 011 | Total loss: 0.800 | Reg loss: 0.028 | Tree loss: 0.800 | Accuracy: 0.741500 | 0.529 sec/iter\n",
      "Epoch: 238 | Batch: 007 / 011 | Total loss: 0.807 | Reg loss: 0.028 | Tree loss: 0.807 | Accuracy: 0.723000 | 0.529 sec/iter\n",
      "Epoch: 238 | Batch: 008 / 011 | Total loss: 0.835 | Reg loss: 0.028 | Tree loss: 0.835 | Accuracy: 0.704000 | 0.528 sec/iter\n",
      "Epoch: 238 | Batch: 009 / 011 | Total loss: 0.808 | Reg loss: 0.028 | Tree loss: 0.808 | Accuracy: 0.716000 | 0.528 sec/iter\n",
      "Epoch: 238 | Batch: 010 / 011 | Total loss: 0.830 | Reg loss: 0.028 | Tree loss: 0.830 | Accuracy: 0.665529 | 0.528 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 239 | Batch: 000 / 011 | Total loss: 0.883 | Reg loss: 0.028 | Tree loss: 0.883 | Accuracy: 0.664000 | 0.53 sec/iter\n",
      "Epoch: 239 | Batch: 001 / 011 | Total loss: 0.880 | Reg loss: 0.028 | Tree loss: 0.880 | Accuracy: 0.646000 | 0.529 sec/iter\n",
      "Epoch: 239 | Batch: 002 / 011 | Total loss: 0.866 | Reg loss: 0.028 | Tree loss: 0.866 | Accuracy: 0.676000 | 0.529 sec/iter\n",
      "Epoch: 239 | Batch: 003 / 011 | Total loss: 0.861 | Reg loss: 0.028 | Tree loss: 0.861 | Accuracy: 0.691500 | 0.529 sec/iter\n",
      "Epoch: 239 | Batch: 004 / 011 | Total loss: 0.860 | Reg loss: 0.028 | Tree loss: 0.860 | Accuracy: 0.684000 | 0.529 sec/iter\n",
      "Epoch: 239 | Batch: 005 / 011 | Total loss: 0.813 | Reg loss: 0.028 | Tree loss: 0.813 | Accuracy: 0.713000 | 0.529 sec/iter\n",
      "Epoch: 239 | Batch: 006 / 011 | Total loss: 0.789 | Reg loss: 0.028 | Tree loss: 0.789 | Accuracy: 0.728000 | 0.529 sec/iter\n",
      "Epoch: 239 | Batch: 007 / 011 | Total loss: 0.799 | Reg loss: 0.028 | Tree loss: 0.799 | Accuracy: 0.730500 | 0.529 sec/iter\n",
      "Epoch: 239 | Batch: 008 / 011 | Total loss: 0.802 | Reg loss: 0.028 | Tree loss: 0.802 | Accuracy: 0.727500 | 0.529 sec/iter\n",
      "Epoch: 239 | Batch: 009 / 011 | Total loss: 0.782 | Reg loss: 0.028 | Tree loss: 0.782 | Accuracy: 0.728000 | 0.529 sec/iter\n",
      "Epoch: 239 | Batch: 010 / 011 | Total loss: 0.751 | Reg loss: 0.028 | Tree loss: 0.751 | Accuracy: 0.767918 | 0.529 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 240 | Batch: 000 / 011 | Total loss: 0.891 | Reg loss: 0.028 | Tree loss: 0.891 | Accuracy: 0.655000 | 0.53 sec/iter\n",
      "Epoch: 240 | Batch: 001 / 011 | Total loss: 0.872 | Reg loss: 0.028 | Tree loss: 0.872 | Accuracy: 0.677000 | 0.53 sec/iter\n",
      "Epoch: 240 | Batch: 002 / 011 | Total loss: 0.872 | Reg loss: 0.028 | Tree loss: 0.872 | Accuracy: 0.672500 | 0.529 sec/iter\n",
      "Epoch: 240 | Batch: 003 / 011 | Total loss: 0.842 | Reg loss: 0.028 | Tree loss: 0.842 | Accuracy: 0.695000 | 0.529 sec/iter\n",
      "Epoch: 240 | Batch: 004 / 011 | Total loss: 0.787 | Reg loss: 0.028 | Tree loss: 0.787 | Accuracy: 0.737000 | 0.529 sec/iter\n",
      "Epoch: 240 | Batch: 005 / 011 | Total loss: 0.800 | Reg loss: 0.028 | Tree loss: 0.800 | Accuracy: 0.730500 | 0.529 sec/iter\n",
      "Epoch: 240 | Batch: 006 / 011 | Total loss: 0.809 | Reg loss: 0.028 | Tree loss: 0.809 | Accuracy: 0.729500 | 0.529 sec/iter\n",
      "Epoch: 240 | Batch: 007 / 011 | Total loss: 0.826 | Reg loss: 0.028 | Tree loss: 0.826 | Accuracy: 0.710500 | 0.529 sec/iter\n",
      "Epoch: 240 | Batch: 008 / 011 | Total loss: 0.803 | Reg loss: 0.028 | Tree loss: 0.803 | Accuracy: 0.728000 | 0.529 sec/iter\n",
      "Epoch: 240 | Batch: 009 / 011 | Total loss: 0.770 | Reg loss: 0.028 | Tree loss: 0.770 | Accuracy: 0.748500 | 0.529 sec/iter\n",
      "Epoch: 240 | Batch: 010 / 011 | Total loss: 0.780 | Reg loss: 0.028 | Tree loss: 0.780 | Accuracy: 0.720137 | 0.529 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 241 | Batch: 000 / 011 | Total loss: 0.906 | Reg loss: 0.028 | Tree loss: 0.906 | Accuracy: 0.654000 | 0.53 sec/iter\n",
      "Epoch: 241 | Batch: 001 / 011 | Total loss: 0.858 | Reg loss: 0.028 | Tree loss: 0.858 | Accuracy: 0.670500 | 0.53 sec/iter\n",
      "Epoch: 241 | Batch: 002 / 011 | Total loss: 0.856 | Reg loss: 0.028 | Tree loss: 0.856 | Accuracy: 0.689000 | 0.53 sec/iter\n",
      "Epoch: 241 | Batch: 003 / 011 | Total loss: 0.844 | Reg loss: 0.028 | Tree loss: 0.844 | Accuracy: 0.695500 | 0.53 sec/iter\n",
      "Epoch: 241 | Batch: 004 / 011 | Total loss: 0.803 | Reg loss: 0.028 | Tree loss: 0.803 | Accuracy: 0.715500 | 0.53 sec/iter\n",
      "Epoch: 241 | Batch: 005 / 011 | Total loss: 0.786 | Reg loss: 0.028 | Tree loss: 0.786 | Accuracy: 0.721500 | 0.529 sec/iter\n",
      "Epoch: 241 | Batch: 006 / 011 | Total loss: 0.800 | Reg loss: 0.028 | Tree loss: 0.800 | Accuracy: 0.716500 | 0.529 sec/iter\n",
      "Epoch: 241 | Batch: 007 / 011 | Total loss: 0.816 | Reg loss: 0.028 | Tree loss: 0.816 | Accuracy: 0.711000 | 0.529 sec/iter\n",
      "Epoch: 241 | Batch: 008 / 011 | Total loss: 0.805 | Reg loss: 0.028 | Tree loss: 0.805 | Accuracy: 0.714000 | 0.529 sec/iter\n",
      "Epoch: 241 | Batch: 009 / 011 | Total loss: 0.796 | Reg loss: 0.028 | Tree loss: 0.796 | Accuracy: 0.725000 | 0.529 sec/iter\n",
      "Epoch: 241 | Batch: 010 / 011 | Total loss: 0.810 | Reg loss: 0.028 | Tree loss: 0.810 | Accuracy: 0.723549 | 0.529 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 242 | Batch: 000 / 011 | Total loss: 0.879 | Reg loss: 0.028 | Tree loss: 0.879 | Accuracy: 0.668000 | 0.53 sec/iter\n",
      "Epoch: 242 | Batch: 001 / 011 | Total loss: 0.871 | Reg loss: 0.028 | Tree loss: 0.871 | Accuracy: 0.665000 | 0.53 sec/iter\n",
      "Epoch: 242 | Batch: 002 / 011 | Total loss: 0.846 | Reg loss: 0.028 | Tree loss: 0.846 | Accuracy: 0.670000 | 0.53 sec/iter\n",
      "Epoch: 242 | Batch: 003 / 011 | Total loss: 0.849 | Reg loss: 0.028 | Tree loss: 0.849 | Accuracy: 0.689500 | 0.53 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 242 | Batch: 004 / 011 | Total loss: 0.830 | Reg loss: 0.028 | Tree loss: 0.830 | Accuracy: 0.687000 | 0.53 sec/iter\n",
      "Epoch: 242 | Batch: 005 / 011 | Total loss: 0.805 | Reg loss: 0.028 | Tree loss: 0.805 | Accuracy: 0.723500 | 0.53 sec/iter\n",
      "Epoch: 242 | Batch: 006 / 011 | Total loss: 0.808 | Reg loss: 0.028 | Tree loss: 0.808 | Accuracy: 0.739500 | 0.53 sec/iter\n",
      "Epoch: 242 | Batch: 007 / 011 | Total loss: 0.804 | Reg loss: 0.028 | Tree loss: 0.804 | Accuracy: 0.722500 | 0.53 sec/iter\n",
      "Epoch: 242 | Batch: 008 / 011 | Total loss: 0.769 | Reg loss: 0.028 | Tree loss: 0.769 | Accuracy: 0.751500 | 0.53 sec/iter\n",
      "Epoch: 242 | Batch: 009 / 011 | Total loss: 0.806 | Reg loss: 0.028 | Tree loss: 0.806 | Accuracy: 0.721000 | 0.53 sec/iter\n",
      "Epoch: 242 | Batch: 010 / 011 | Total loss: 0.804 | Reg loss: 0.028 | Tree loss: 0.804 | Accuracy: 0.737201 | 0.529 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 243 | Batch: 000 / 011 | Total loss: 0.898 | Reg loss: 0.028 | Tree loss: 0.898 | Accuracy: 0.668000 | 0.53 sec/iter\n",
      "Epoch: 243 | Batch: 001 / 011 | Total loss: 0.869 | Reg loss: 0.028 | Tree loss: 0.869 | Accuracy: 0.675000 | 0.53 sec/iter\n",
      "Epoch: 243 | Batch: 002 / 011 | Total loss: 0.867 | Reg loss: 0.028 | Tree loss: 0.867 | Accuracy: 0.663000 | 0.529 sec/iter\n",
      "Epoch: 243 | Batch: 003 / 011 | Total loss: 0.822 | Reg loss: 0.028 | Tree loss: 0.822 | Accuracy: 0.697500 | 0.529 sec/iter\n",
      "Epoch: 243 | Batch: 004 / 011 | Total loss: 0.801 | Reg loss: 0.028 | Tree loss: 0.801 | Accuracy: 0.715500 | 0.529 sec/iter\n",
      "Epoch: 243 | Batch: 005 / 011 | Total loss: 0.778 | Reg loss: 0.028 | Tree loss: 0.778 | Accuracy: 0.724000 | 0.529 sec/iter\n",
      "Epoch: 243 | Batch: 006 / 011 | Total loss: 0.803 | Reg loss: 0.028 | Tree loss: 0.803 | Accuracy: 0.722500 | 0.529 sec/iter\n",
      "Epoch: 243 | Batch: 007 / 011 | Total loss: 0.795 | Reg loss: 0.028 | Tree loss: 0.795 | Accuracy: 0.736000 | 0.529 sec/iter\n",
      "Epoch: 243 | Batch: 008 / 011 | Total loss: 0.829 | Reg loss: 0.028 | Tree loss: 0.829 | Accuracy: 0.715000 | 0.529 sec/iter\n",
      "Epoch: 243 | Batch: 009 / 011 | Total loss: 0.800 | Reg loss: 0.028 | Tree loss: 0.800 | Accuracy: 0.719000 | 0.529 sec/iter\n",
      "Epoch: 243 | Batch: 010 / 011 | Total loss: 0.835 | Reg loss: 0.028 | Tree loss: 0.835 | Accuracy: 0.713311 | 0.529 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 244 | Batch: 000 / 011 | Total loss: 0.880 | Reg loss: 0.028 | Tree loss: 0.880 | Accuracy: 0.664000 | 0.529 sec/iter\n",
      "Epoch: 244 | Batch: 001 / 011 | Total loss: 0.856 | Reg loss: 0.028 | Tree loss: 0.856 | Accuracy: 0.682500 | 0.529 sec/iter\n",
      "Epoch: 244 | Batch: 002 / 011 | Total loss: 0.868 | Reg loss: 0.028 | Tree loss: 0.868 | Accuracy: 0.676000 | 0.529 sec/iter\n",
      "Epoch: 244 | Batch: 003 / 011 | Total loss: 0.835 | Reg loss: 0.028 | Tree loss: 0.835 | Accuracy: 0.706000 | 0.529 sec/iter\n",
      "Epoch: 244 | Batch: 004 / 011 | Total loss: 0.796 | Reg loss: 0.028 | Tree loss: 0.796 | Accuracy: 0.732000 | 0.529 sec/iter\n",
      "Epoch: 244 | Batch: 005 / 011 | Total loss: 0.812 | Reg loss: 0.028 | Tree loss: 0.812 | Accuracy: 0.711500 | 0.529 sec/iter\n",
      "Epoch: 244 | Batch: 006 / 011 | Total loss: 0.799 | Reg loss: 0.028 | Tree loss: 0.799 | Accuracy: 0.744000 | 0.529 sec/iter\n",
      "Epoch: 244 | Batch: 007 / 011 | Total loss: 0.778 | Reg loss: 0.028 | Tree loss: 0.778 | Accuracy: 0.736500 | 0.529 sec/iter\n",
      "Epoch: 244 | Batch: 008 / 011 | Total loss: 0.849 | Reg loss: 0.028 | Tree loss: 0.849 | Accuracy: 0.686000 | 0.528 sec/iter\n",
      "Epoch: 244 | Batch: 009 / 011 | Total loss: 0.806 | Reg loss: 0.028 | Tree loss: 0.806 | Accuracy: 0.726500 | 0.528 sec/iter\n",
      "Epoch: 244 | Batch: 010 / 011 | Total loss: 0.810 | Reg loss: 0.028 | Tree loss: 0.810 | Accuracy: 0.689420 | 0.528 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 245 | Batch: 000 / 011 | Total loss: 0.902 | Reg loss: 0.028 | Tree loss: 0.902 | Accuracy: 0.652000 | 0.529 sec/iter\n",
      "Epoch: 245 | Batch: 001 / 011 | Total loss: 0.872 | Reg loss: 0.028 | Tree loss: 0.872 | Accuracy: 0.658500 | 0.529 sec/iter\n",
      "Epoch: 245 | Batch: 002 / 011 | Total loss: 0.870 | Reg loss: 0.028 | Tree loss: 0.870 | Accuracy: 0.679500 | 0.529 sec/iter\n",
      "Epoch: 245 | Batch: 003 / 011 | Total loss: 0.846 | Reg loss: 0.028 | Tree loss: 0.846 | Accuracy: 0.689500 | 0.529 sec/iter\n",
      "Epoch: 245 | Batch: 004 / 011 | Total loss: 0.820 | Reg loss: 0.028 | Tree loss: 0.820 | Accuracy: 0.704500 | 0.529 sec/iter\n",
      "Epoch: 245 | Batch: 005 / 011 | Total loss: 0.803 | Reg loss: 0.028 | Tree loss: 0.803 | Accuracy: 0.728500 | 0.528 sec/iter\n",
      "Epoch: 245 | Batch: 006 / 011 | Total loss: 0.808 | Reg loss: 0.028 | Tree loss: 0.808 | Accuracy: 0.740500 | 0.528 sec/iter\n",
      "Epoch: 245 | Batch: 007 / 011 | Total loss: 0.793 | Reg loss: 0.028 | Tree loss: 0.793 | Accuracy: 0.740000 | 0.528 sec/iter\n",
      "Epoch: 245 | Batch: 008 / 011 | Total loss: 0.785 | Reg loss: 0.028 | Tree loss: 0.785 | Accuracy: 0.739500 | 0.528 sec/iter\n",
      "Epoch: 245 | Batch: 009 / 011 | Total loss: 0.778 | Reg loss: 0.028 | Tree loss: 0.778 | Accuracy: 0.747500 | 0.528 sec/iter\n",
      "Epoch: 245 | Batch: 010 / 011 | Total loss: 0.755 | Reg loss: 0.028 | Tree loss: 0.755 | Accuracy: 0.740614 | 0.528 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 246 | Batch: 000 / 011 | Total loss: 0.898 | Reg loss: 0.028 | Tree loss: 0.898 | Accuracy: 0.658000 | 0.528 sec/iter\n",
      "Epoch: 246 | Batch: 001 / 011 | Total loss: 0.870 | Reg loss: 0.028 | Tree loss: 0.870 | Accuracy: 0.673500 | 0.528 sec/iter\n",
      "Epoch: 246 | Batch: 002 / 011 | Total loss: 0.838 | Reg loss: 0.028 | Tree loss: 0.838 | Accuracy: 0.686500 | 0.528 sec/iter\n",
      "Epoch: 246 | Batch: 003 / 011 | Total loss: 0.836 | Reg loss: 0.028 | Tree loss: 0.836 | Accuracy: 0.705000 | 0.528 sec/iter\n",
      "Epoch: 246 | Batch: 004 / 011 | Total loss: 0.817 | Reg loss: 0.028 | Tree loss: 0.817 | Accuracy: 0.718000 | 0.527 sec/iter\n",
      "Epoch: 246 | Batch: 005 / 011 | Total loss: 0.825 | Reg loss: 0.028 | Tree loss: 0.825 | Accuracy: 0.712000 | 0.527 sec/iter\n",
      "Epoch: 246 | Batch: 006 / 011 | Total loss: 0.792 | Reg loss: 0.028 | Tree loss: 0.792 | Accuracy: 0.721000 | 0.527 sec/iter\n",
      "Epoch: 246 | Batch: 007 / 011 | Total loss: 0.801 | Reg loss: 0.028 | Tree loss: 0.801 | Accuracy: 0.713500 | 0.527 sec/iter\n",
      "Epoch: 246 | Batch: 008 / 011 | Total loss: 0.805 | Reg loss: 0.028 | Tree loss: 0.805 | Accuracy: 0.718000 | 0.527 sec/iter\n",
      "Epoch: 246 | Batch: 009 / 011 | Total loss: 0.813 | Reg loss: 0.028 | Tree loss: 0.813 | Accuracy: 0.710500 | 0.527 sec/iter\n",
      "Epoch: 246 | Batch: 010 / 011 | Total loss: 0.805 | Reg loss: 0.028 | Tree loss: 0.805 | Accuracy: 0.730375 | 0.527 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 247 | Batch: 000 / 011 | Total loss: 0.894 | Reg loss: 0.028 | Tree loss: 0.894 | Accuracy: 0.651000 | 0.528 sec/iter\n",
      "Epoch: 247 | Batch: 001 / 011 | Total loss: 0.883 | Reg loss: 0.028 | Tree loss: 0.883 | Accuracy: 0.670500 | 0.527 sec/iter\n",
      "Epoch: 247 | Batch: 002 / 011 | Total loss: 0.859 | Reg loss: 0.028 | Tree loss: 0.859 | Accuracy: 0.682000 | 0.527 sec/iter\n",
      "Epoch: 247 | Batch: 003 / 011 | Total loss: 0.834 | Reg loss: 0.028 | Tree loss: 0.834 | Accuracy: 0.692500 | 0.527 sec/iter\n",
      "Epoch: 247 | Batch: 004 / 011 | Total loss: 0.810 | Reg loss: 0.028 | Tree loss: 0.810 | Accuracy: 0.709500 | 0.527 sec/iter\n",
      "Epoch: 247 | Batch: 005 / 011 | Total loss: 0.810 | Reg loss: 0.028 | Tree loss: 0.810 | Accuracy: 0.725000 | 0.527 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 247 | Batch: 006 / 011 | Total loss: 0.788 | Reg loss: 0.028 | Tree loss: 0.788 | Accuracy: 0.752500 | 0.527 sec/iter\n",
      "Epoch: 247 | Batch: 007 / 011 | Total loss: 0.812 | Reg loss: 0.028 | Tree loss: 0.812 | Accuracy: 0.721500 | 0.527 sec/iter\n",
      "Epoch: 247 | Batch: 008 / 011 | Total loss: 0.798 | Reg loss: 0.028 | Tree loss: 0.798 | Accuracy: 0.737000 | 0.527 sec/iter\n",
      "Epoch: 247 | Batch: 009 / 011 | Total loss: 0.775 | Reg loss: 0.028 | Tree loss: 0.775 | Accuracy: 0.737000 | 0.527 sec/iter\n",
      "Epoch: 247 | Batch: 010 / 011 | Total loss: 0.776 | Reg loss: 0.028 | Tree loss: 0.776 | Accuracy: 0.726962 | 0.526 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 248 | Batch: 000 / 011 | Total loss: 0.884 | Reg loss: 0.028 | Tree loss: 0.884 | Accuracy: 0.673500 | 0.528 sec/iter\n",
      "Epoch: 248 | Batch: 001 / 011 | Total loss: 0.855 | Reg loss: 0.028 | Tree loss: 0.855 | Accuracy: 0.681000 | 0.528 sec/iter\n",
      "Epoch: 248 | Batch: 002 / 011 | Total loss: 0.851 | Reg loss: 0.028 | Tree loss: 0.851 | Accuracy: 0.676500 | 0.528 sec/iter\n",
      "Epoch: 248 | Batch: 003 / 011 | Total loss: 0.836 | Reg loss: 0.028 | Tree loss: 0.836 | Accuracy: 0.689000 | 0.528 sec/iter\n",
      "Epoch: 248 | Batch: 004 / 011 | Total loss: 0.837 | Reg loss: 0.028 | Tree loss: 0.837 | Accuracy: 0.691500 | 0.528 sec/iter\n",
      "Epoch: 248 | Batch: 005 / 011 | Total loss: 0.814 | Reg loss: 0.028 | Tree loss: 0.814 | Accuracy: 0.719000 | 0.528 sec/iter\n",
      "Epoch: 248 | Batch: 006 / 011 | Total loss: 0.794 | Reg loss: 0.028 | Tree loss: 0.794 | Accuracy: 0.713500 | 0.527 sec/iter\n",
      "Epoch: 248 | Batch: 007 / 011 | Total loss: 0.789 | Reg loss: 0.028 | Tree loss: 0.789 | Accuracy: 0.741000 | 0.527 sec/iter\n",
      "Epoch: 248 | Batch: 008 / 011 | Total loss: 0.806 | Reg loss: 0.028 | Tree loss: 0.806 | Accuracy: 0.730500 | 0.527 sec/iter\n",
      "Epoch: 248 | Batch: 009 / 011 | Total loss: 0.808 | Reg loss: 0.028 | Tree loss: 0.808 | Accuracy: 0.735000 | 0.527 sec/iter\n",
      "Epoch: 248 | Batch: 010 / 011 | Total loss: 0.784 | Reg loss: 0.028 | Tree loss: 0.784 | Accuracy: 0.750853 | 0.527 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 249 | Batch: 000 / 011 | Total loss: 0.876 | Reg loss: 0.028 | Tree loss: 0.876 | Accuracy: 0.671500 | 0.527 sec/iter\n",
      "Epoch: 249 | Batch: 001 / 011 | Total loss: 0.866 | Reg loss: 0.028 | Tree loss: 0.866 | Accuracy: 0.670500 | 0.527 sec/iter\n",
      "Epoch: 249 | Batch: 002 / 011 | Total loss: 0.841 | Reg loss: 0.028 | Tree loss: 0.841 | Accuracy: 0.701000 | 0.527 sec/iter\n",
      "Epoch: 249 | Batch: 003 / 011 | Total loss: 0.848 | Reg loss: 0.028 | Tree loss: 0.848 | Accuracy: 0.689000 | 0.527 sec/iter\n",
      "Epoch: 249 | Batch: 004 / 011 | Total loss: 0.792 | Reg loss: 0.028 | Tree loss: 0.792 | Accuracy: 0.718000 | 0.527 sec/iter\n",
      "Epoch: 249 | Batch: 005 / 011 | Total loss: 0.809 | Reg loss: 0.028 | Tree loss: 0.809 | Accuracy: 0.717500 | 0.526 sec/iter\n",
      "Epoch: 249 | Batch: 006 / 011 | Total loss: 0.807 | Reg loss: 0.028 | Tree loss: 0.807 | Accuracy: 0.718000 | 0.526 sec/iter\n",
      "Epoch: 249 | Batch: 007 / 011 | Total loss: 0.804 | Reg loss: 0.028 | Tree loss: 0.804 | Accuracy: 0.723000 | 0.526 sec/iter\n",
      "Epoch: 249 | Batch: 008 / 011 | Total loss: 0.816 | Reg loss: 0.028 | Tree loss: 0.816 | Accuracy: 0.715000 | 0.526 sec/iter\n",
      "Epoch: 249 | Batch: 009 / 011 | Total loss: 0.809 | Reg loss: 0.028 | Tree loss: 0.809 | Accuracy: 0.712000 | 0.526 sec/iter\n",
      "Epoch: 249 | Batch: 010 / 011 | Total loss: 0.767 | Reg loss: 0.028 | Tree loss: 0.767 | Accuracy: 0.754266 | 0.526 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 250 | Batch: 000 / 011 | Total loss: 0.875 | Reg loss: 0.028 | Tree loss: 0.875 | Accuracy: 0.669000 | 0.527 sec/iter\n",
      "Epoch: 250 | Batch: 001 / 011 | Total loss: 0.863 | Reg loss: 0.028 | Tree loss: 0.863 | Accuracy: 0.681000 | 0.527 sec/iter\n",
      "Epoch: 250 | Batch: 002 / 011 | Total loss: 0.875 | Reg loss: 0.028 | Tree loss: 0.875 | Accuracy: 0.661000 | 0.527 sec/iter\n",
      "Epoch: 250 | Batch: 003 / 011 | Total loss: 0.826 | Reg loss: 0.028 | Tree loss: 0.826 | Accuracy: 0.703000 | 0.526 sec/iter\n",
      "Epoch: 250 | Batch: 004 / 011 | Total loss: 0.818 | Reg loss: 0.028 | Tree loss: 0.818 | Accuracy: 0.710500 | 0.526 sec/iter\n",
      "Epoch: 250 | Batch: 005 / 011 | Total loss: 0.818 | Reg loss: 0.028 | Tree loss: 0.818 | Accuracy: 0.717500 | 0.526 sec/iter\n",
      "Epoch: 250 | Batch: 006 / 011 | Total loss: 0.819 | Reg loss: 0.028 | Tree loss: 0.819 | Accuracy: 0.706000 | 0.526 sec/iter\n",
      "Epoch: 250 | Batch: 007 / 011 | Total loss: 0.777 | Reg loss: 0.028 | Tree loss: 0.777 | Accuracy: 0.738000 | 0.526 sec/iter\n",
      "Epoch: 250 | Batch: 008 / 011 | Total loss: 0.789 | Reg loss: 0.028 | Tree loss: 0.789 | Accuracy: 0.731500 | 0.526 sec/iter\n",
      "Epoch: 250 | Batch: 009 / 011 | Total loss: 0.788 | Reg loss: 0.028 | Tree loss: 0.788 | Accuracy: 0.746500 | 0.526 sec/iter\n",
      "Epoch: 250 | Batch: 010 / 011 | Total loss: 0.809 | Reg loss: 0.028 | Tree loss: 0.809 | Accuracy: 0.703072 | 0.526 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 251 | Batch: 000 / 011 | Total loss: 0.897 | Reg loss: 0.028 | Tree loss: 0.897 | Accuracy: 0.663000 | 0.526 sec/iter\n",
      "Epoch: 251 | Batch: 001 / 011 | Total loss: 0.851 | Reg loss: 0.028 | Tree loss: 0.851 | Accuracy: 0.665500 | 0.526 sec/iter\n",
      "Epoch: 251 | Batch: 002 / 011 | Total loss: 0.856 | Reg loss: 0.028 | Tree loss: 0.856 | Accuracy: 0.682000 | 0.525 sec/iter\n",
      "Epoch: 251 | Batch: 003 / 011 | Total loss: 0.834 | Reg loss: 0.028 | Tree loss: 0.834 | Accuracy: 0.715000 | 0.525 sec/iter\n",
      "Epoch: 251 | Batch: 004 / 011 | Total loss: 0.814 | Reg loss: 0.028 | Tree loss: 0.814 | Accuracy: 0.724000 | 0.525 sec/iter\n",
      "Epoch: 251 | Batch: 005 / 011 | Total loss: 0.816 | Reg loss: 0.028 | Tree loss: 0.816 | Accuracy: 0.723500 | 0.525 sec/iter\n",
      "Epoch: 251 | Batch: 006 / 011 | Total loss: 0.781 | Reg loss: 0.028 | Tree loss: 0.781 | Accuracy: 0.748000 | 0.525 sec/iter\n",
      "Epoch: 251 | Batch: 007 / 011 | Total loss: 0.777 | Reg loss: 0.028 | Tree loss: 0.777 | Accuracy: 0.748500 | 0.525 sec/iter\n",
      "Epoch: 251 | Batch: 008 / 011 | Total loss: 0.802 | Reg loss: 0.028 | Tree loss: 0.802 | Accuracy: 0.719000 | 0.525 sec/iter\n",
      "Epoch: 251 | Batch: 009 / 011 | Total loss: 0.821 | Reg loss: 0.028 | Tree loss: 0.821 | Accuracy: 0.714500 | 0.525 sec/iter\n",
      "Epoch: 251 | Batch: 010 / 011 | Total loss: 0.743 | Reg loss: 0.028 | Tree loss: 0.743 | Accuracy: 0.750853 | 0.525 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 252 | Batch: 000 / 011 | Total loss: 0.902 | Reg loss: 0.028 | Tree loss: 0.902 | Accuracy: 0.660500 | 0.525 sec/iter\n",
      "Epoch: 252 | Batch: 001 / 011 | Total loss: 0.876 | Reg loss: 0.028 | Tree loss: 0.876 | Accuracy: 0.660500 | 0.525 sec/iter\n",
      "Epoch: 252 | Batch: 002 / 011 | Total loss: 0.850 | Reg loss: 0.028 | Tree loss: 0.850 | Accuracy: 0.691000 | 0.525 sec/iter\n",
      "Epoch: 252 | Batch: 003 / 011 | Total loss: 0.827 | Reg loss: 0.028 | Tree loss: 0.827 | Accuracy: 0.710000 | 0.525 sec/iter\n",
      "Epoch: 252 | Batch: 004 / 011 | Total loss: 0.830 | Reg loss: 0.028 | Tree loss: 0.830 | Accuracy: 0.696500 | 0.525 sec/iter\n",
      "Epoch: 252 | Batch: 005 / 011 | Total loss: 0.804 | Reg loss: 0.028 | Tree loss: 0.804 | Accuracy: 0.718000 | 0.525 sec/iter\n",
      "Epoch: 252 | Batch: 006 / 011 | Total loss: 0.794 | Reg loss: 0.028 | Tree loss: 0.794 | Accuracy: 0.728000 | 0.525 sec/iter\n",
      "Epoch: 252 | Batch: 007 / 011 | Total loss: 0.778 | Reg loss: 0.028 | Tree loss: 0.778 | Accuracy: 0.725000 | 0.524 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 252 | Batch: 008 / 011 | Total loss: 0.804 | Reg loss: 0.028 | Tree loss: 0.804 | Accuracy: 0.716000 | 0.524 sec/iter\n",
      "Epoch: 252 | Batch: 009 / 011 | Total loss: 0.800 | Reg loss: 0.028 | Tree loss: 0.800 | Accuracy: 0.718000 | 0.524 sec/iter\n",
      "Epoch: 252 | Batch: 010 / 011 | Total loss: 0.773 | Reg loss: 0.028 | Tree loss: 0.773 | Accuracy: 0.709898 | 0.524 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 253 | Batch: 000 / 011 | Total loss: 0.867 | Reg loss: 0.028 | Tree loss: 0.867 | Accuracy: 0.683500 | 0.525 sec/iter\n",
      "Epoch: 253 | Batch: 001 / 011 | Total loss: 0.890 | Reg loss: 0.028 | Tree loss: 0.890 | Accuracy: 0.659500 | 0.525 sec/iter\n",
      "Epoch: 253 | Batch: 002 / 011 | Total loss: 0.862 | Reg loss: 0.028 | Tree loss: 0.862 | Accuracy: 0.683000 | 0.525 sec/iter\n",
      "Epoch: 253 | Batch: 003 / 011 | Total loss: 0.846 | Reg loss: 0.028 | Tree loss: 0.846 | Accuracy: 0.687500 | 0.525 sec/iter\n",
      "Epoch: 253 | Batch: 004 / 011 | Total loss: 0.788 | Reg loss: 0.028 | Tree loss: 0.788 | Accuracy: 0.731000 | 0.524 sec/iter\n",
      "Epoch: 253 | Batch: 005 / 011 | Total loss: 0.792 | Reg loss: 0.028 | Tree loss: 0.792 | Accuracy: 0.716500 | 0.524 sec/iter\n",
      "Epoch: 253 | Batch: 006 / 011 | Total loss: 0.796 | Reg loss: 0.028 | Tree loss: 0.796 | Accuracy: 0.743000 | 0.524 sec/iter\n",
      "Epoch: 253 | Batch: 007 / 011 | Total loss: 0.832 | Reg loss: 0.028 | Tree loss: 0.832 | Accuracy: 0.712000 | 0.524 sec/iter\n",
      "Epoch: 253 | Batch: 008 / 011 | Total loss: 0.782 | Reg loss: 0.028 | Tree loss: 0.782 | Accuracy: 0.752500 | 0.524 sec/iter\n",
      "Epoch: 253 | Batch: 009 / 011 | Total loss: 0.805 | Reg loss: 0.028 | Tree loss: 0.805 | Accuracy: 0.727500 | 0.524 sec/iter\n",
      "Epoch: 253 | Batch: 010 / 011 | Total loss: 0.777 | Reg loss: 0.028 | Tree loss: 0.777 | Accuracy: 0.716724 | 0.524 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 254 | Batch: 000 / 011 | Total loss: 0.858 | Reg loss: 0.028 | Tree loss: 0.858 | Accuracy: 0.679000 | 0.524 sec/iter\n",
      "Epoch: 254 | Batch: 001 / 011 | Total loss: 0.863 | Reg loss: 0.028 | Tree loss: 0.863 | Accuracy: 0.671500 | 0.524 sec/iter\n",
      "Epoch: 254 | Batch: 002 / 011 | Total loss: 0.841 | Reg loss: 0.028 | Tree loss: 0.841 | Accuracy: 0.695500 | 0.524 sec/iter\n",
      "Epoch: 254 | Batch: 003 / 011 | Total loss: 0.859 | Reg loss: 0.028 | Tree loss: 0.859 | Accuracy: 0.673500 | 0.523 sec/iter\n",
      "Epoch: 254 | Batch: 004 / 011 | Total loss: 0.832 | Reg loss: 0.028 | Tree loss: 0.832 | Accuracy: 0.715000 | 0.523 sec/iter\n",
      "Epoch: 254 | Batch: 005 / 011 | Total loss: 0.823 | Reg loss: 0.028 | Tree loss: 0.823 | Accuracy: 0.706000 | 0.523 sec/iter\n",
      "Epoch: 254 | Batch: 006 / 011 | Total loss: 0.818 | Reg loss: 0.028 | Tree loss: 0.818 | Accuracy: 0.719500 | 0.523 sec/iter\n",
      "Epoch: 254 | Batch: 007 / 011 | Total loss: 0.775 | Reg loss: 0.028 | Tree loss: 0.775 | Accuracy: 0.735000 | 0.523 sec/iter\n",
      "Epoch: 254 | Batch: 008 / 011 | Total loss: 0.804 | Reg loss: 0.028 | Tree loss: 0.804 | Accuracy: 0.731000 | 0.523 sec/iter\n",
      "Epoch: 254 | Batch: 009 / 011 | Total loss: 0.773 | Reg loss: 0.028 | Tree loss: 0.773 | Accuracy: 0.752500 | 0.523 sec/iter\n",
      "Epoch: 254 | Batch: 010 / 011 | Total loss: 0.724 | Reg loss: 0.028 | Tree loss: 0.724 | Accuracy: 0.767918 | 0.523 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 255 | Batch: 000 / 011 | Total loss: 0.889 | Reg loss: 0.028 | Tree loss: 0.889 | Accuracy: 0.655500 | 0.524 sec/iter\n",
      "Epoch: 255 | Batch: 001 / 011 | Total loss: 0.874 | Reg loss: 0.028 | Tree loss: 0.874 | Accuracy: 0.651500 | 0.524 sec/iter\n",
      "Epoch: 255 | Batch: 002 / 011 | Total loss: 0.848 | Reg loss: 0.028 | Tree loss: 0.848 | Accuracy: 0.692000 | 0.524 sec/iter\n",
      "Epoch: 255 | Batch: 003 / 011 | Total loss: 0.803 | Reg loss: 0.028 | Tree loss: 0.803 | Accuracy: 0.713500 | 0.524 sec/iter\n",
      "Epoch: 255 | Batch: 004 / 011 | Total loss: 0.818 | Reg loss: 0.028 | Tree loss: 0.818 | Accuracy: 0.705000 | 0.524 sec/iter\n",
      "Epoch: 255 | Batch: 005 / 011 | Total loss: 0.820 | Reg loss: 0.028 | Tree loss: 0.820 | Accuracy: 0.715000 | 0.524 sec/iter\n",
      "Epoch: 255 | Batch: 006 / 011 | Total loss: 0.786 | Reg loss: 0.028 | Tree loss: 0.786 | Accuracy: 0.741500 | 0.524 sec/iter\n",
      "Epoch: 255 | Batch: 007 / 011 | Total loss: 0.798 | Reg loss: 0.028 | Tree loss: 0.798 | Accuracy: 0.729500 | 0.524 sec/iter\n",
      "Epoch: 255 | Batch: 008 / 011 | Total loss: 0.815 | Reg loss: 0.028 | Tree loss: 0.815 | Accuracy: 0.719000 | 0.523 sec/iter\n",
      "Epoch: 255 | Batch: 009 / 011 | Total loss: 0.795 | Reg loss: 0.028 | Tree loss: 0.795 | Accuracy: 0.725000 | 0.523 sec/iter\n",
      "Epoch: 255 | Batch: 010 / 011 | Total loss: 0.769 | Reg loss: 0.028 | Tree loss: 0.769 | Accuracy: 0.754266 | 0.523 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 256 | Batch: 000 / 011 | Total loss: 0.920 | Reg loss: 0.028 | Tree loss: 0.920 | Accuracy: 0.643500 | 0.524 sec/iter\n",
      "Epoch: 256 | Batch: 001 / 011 | Total loss: 0.874 | Reg loss: 0.028 | Tree loss: 0.874 | Accuracy: 0.658500 | 0.524 sec/iter\n",
      "Epoch: 256 | Batch: 002 / 011 | Total loss: 0.853 | Reg loss: 0.028 | Tree loss: 0.853 | Accuracy: 0.683500 | 0.524 sec/iter\n",
      "Epoch: 256 | Batch: 003 / 011 | Total loss: 0.833 | Reg loss: 0.028 | Tree loss: 0.833 | Accuracy: 0.683000 | 0.524 sec/iter\n",
      "Epoch: 256 | Batch: 004 / 011 | Total loss: 0.803 | Reg loss: 0.028 | Tree loss: 0.803 | Accuracy: 0.720000 | 0.524 sec/iter\n",
      "Epoch: 256 | Batch: 005 / 011 | Total loss: 0.782 | Reg loss: 0.028 | Tree loss: 0.782 | Accuracy: 0.729500 | 0.524 sec/iter\n",
      "Epoch: 256 | Batch: 006 / 011 | Total loss: 0.811 | Reg loss: 0.028 | Tree loss: 0.811 | Accuracy: 0.715000 | 0.524 sec/iter\n",
      "Epoch: 256 | Batch: 007 / 011 | Total loss: 0.783 | Reg loss: 0.028 | Tree loss: 0.783 | Accuracy: 0.727500 | 0.524 sec/iter\n",
      "Epoch: 256 | Batch: 008 / 011 | Total loss: 0.794 | Reg loss: 0.028 | Tree loss: 0.794 | Accuracy: 0.730000 | 0.524 sec/iter\n",
      "Epoch: 256 | Batch: 009 / 011 | Total loss: 0.811 | Reg loss: 0.028 | Tree loss: 0.811 | Accuracy: 0.727000 | 0.523 sec/iter\n",
      "Epoch: 256 | Batch: 010 / 011 | Total loss: 0.766 | Reg loss: 0.028 | Tree loss: 0.766 | Accuracy: 0.747440 | 0.523 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 257 | Batch: 000 / 011 | Total loss: 0.878 | Reg loss: 0.028 | Tree loss: 0.878 | Accuracy: 0.664000 | 0.524 sec/iter\n",
      "Epoch: 257 | Batch: 001 / 011 | Total loss: 0.866 | Reg loss: 0.028 | Tree loss: 0.866 | Accuracy: 0.670500 | 0.524 sec/iter\n",
      "Epoch: 257 | Batch: 002 / 011 | Total loss: 0.836 | Reg loss: 0.028 | Tree loss: 0.836 | Accuracy: 0.688500 | 0.524 sec/iter\n",
      "Epoch: 257 | Batch: 003 / 011 | Total loss: 0.825 | Reg loss: 0.028 | Tree loss: 0.825 | Accuracy: 0.703000 | 0.524 sec/iter\n",
      "Epoch: 257 | Batch: 004 / 011 | Total loss: 0.819 | Reg loss: 0.028 | Tree loss: 0.819 | Accuracy: 0.706000 | 0.524 sec/iter\n",
      "Epoch: 257 | Batch: 005 / 011 | Total loss: 0.798 | Reg loss: 0.028 | Tree loss: 0.798 | Accuracy: 0.730500 | 0.524 sec/iter\n",
      "Epoch: 257 | Batch: 006 / 011 | Total loss: 0.802 | Reg loss: 0.028 | Tree loss: 0.802 | Accuracy: 0.725500 | 0.523 sec/iter\n",
      "Epoch: 257 | Batch: 007 / 011 | Total loss: 0.816 | Reg loss: 0.028 | Tree loss: 0.816 | Accuracy: 0.727500 | 0.523 sec/iter\n",
      "Epoch: 257 | Batch: 008 / 011 | Total loss: 0.815 | Reg loss: 0.028 | Tree loss: 0.815 | Accuracy: 0.730500 | 0.523 sec/iter\n",
      "Epoch: 257 | Batch: 009 / 011 | Total loss: 0.798 | Reg loss: 0.028 | Tree loss: 0.798 | Accuracy: 0.723000 | 0.523 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 257 | Batch: 010 / 011 | Total loss: 0.752 | Reg loss: 0.028 | Tree loss: 0.752 | Accuracy: 0.740614 | 0.523 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 258 | Batch: 000 / 011 | Total loss: 0.876 | Reg loss: 0.028 | Tree loss: 0.876 | Accuracy: 0.673500 | 0.525 sec/iter\n",
      "Epoch: 258 | Batch: 001 / 011 | Total loss: 0.856 | Reg loss: 0.028 | Tree loss: 0.856 | Accuracy: 0.668000 | 0.525 sec/iter\n",
      "Epoch: 258 | Batch: 002 / 011 | Total loss: 0.850 | Reg loss: 0.028 | Tree loss: 0.850 | Accuracy: 0.694000 | 0.525 sec/iter\n",
      "Epoch: 258 | Batch: 003 / 011 | Total loss: 0.842 | Reg loss: 0.028 | Tree loss: 0.842 | Accuracy: 0.692500 | 0.524 sec/iter\n",
      "Epoch: 258 | Batch: 004 / 011 | Total loss: 0.809 | Reg loss: 0.028 | Tree loss: 0.809 | Accuracy: 0.712000 | 0.524 sec/iter\n",
      "Epoch: 258 | Batch: 005 / 011 | Total loss: 0.814 | Reg loss: 0.028 | Tree loss: 0.814 | Accuracy: 0.712500 | 0.524 sec/iter\n",
      "Epoch: 258 | Batch: 006 / 011 | Total loss: 0.802 | Reg loss: 0.028 | Tree loss: 0.802 | Accuracy: 0.717000 | 0.524 sec/iter\n",
      "Epoch: 258 | Batch: 007 / 011 | Total loss: 0.812 | Reg loss: 0.028 | Tree loss: 0.812 | Accuracy: 0.703500 | 0.524 sec/iter\n",
      "Epoch: 258 | Batch: 008 / 011 | Total loss: 0.802 | Reg loss: 0.028 | Tree loss: 0.802 | Accuracy: 0.718000 | 0.524 sec/iter\n",
      "Epoch: 258 | Batch: 009 / 011 | Total loss: 0.826 | Reg loss: 0.028 | Tree loss: 0.826 | Accuracy: 0.713500 | 0.524 sec/iter\n",
      "Epoch: 258 | Batch: 010 / 011 | Total loss: 0.761 | Reg loss: 0.028 | Tree loss: 0.761 | Accuracy: 0.761092 | 0.524 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 259 | Batch: 000 / 011 | Total loss: 0.888 | Reg loss: 0.028 | Tree loss: 0.888 | Accuracy: 0.678500 | 0.524 sec/iter\n",
      "Epoch: 259 | Batch: 001 / 011 | Total loss: 0.857 | Reg loss: 0.028 | Tree loss: 0.857 | Accuracy: 0.681500 | 0.524 sec/iter\n",
      "Epoch: 259 | Batch: 002 / 011 | Total loss: 0.847 | Reg loss: 0.028 | Tree loss: 0.847 | Accuracy: 0.697500 | 0.524 sec/iter\n",
      "Epoch: 259 | Batch: 003 / 011 | Total loss: 0.848 | Reg loss: 0.028 | Tree loss: 0.848 | Accuracy: 0.683500 | 0.523 sec/iter\n",
      "Epoch: 259 | Batch: 004 / 011 | Total loss: 0.810 | Reg loss: 0.028 | Tree loss: 0.810 | Accuracy: 0.707000 | 0.523 sec/iter\n",
      "Epoch: 259 | Batch: 005 / 011 | Total loss: 0.795 | Reg loss: 0.028 | Tree loss: 0.795 | Accuracy: 0.727500 | 0.523 sec/iter\n",
      "Epoch: 259 | Batch: 006 / 011 | Total loss: 0.813 | Reg loss: 0.028 | Tree loss: 0.813 | Accuracy: 0.733000 | 0.523 sec/iter\n",
      "Epoch: 259 | Batch: 007 / 011 | Total loss: 0.819 | Reg loss: 0.028 | Tree loss: 0.819 | Accuracy: 0.727500 | 0.523 sec/iter\n",
      "Epoch: 259 | Batch: 008 / 011 | Total loss: 0.804 | Reg loss: 0.028 | Tree loss: 0.804 | Accuracy: 0.707000 | 0.523 sec/iter\n",
      "Epoch: 259 | Batch: 009 / 011 | Total loss: 0.776 | Reg loss: 0.028 | Tree loss: 0.776 | Accuracy: 0.746000 | 0.523 sec/iter\n",
      "Epoch: 259 | Batch: 010 / 011 | Total loss: 0.760 | Reg loss: 0.028 | Tree loss: 0.760 | Accuracy: 0.740614 | 0.523 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 260 | Batch: 000 / 011 | Total loss: 0.882 | Reg loss: 0.028 | Tree loss: 0.882 | Accuracy: 0.655500 | 0.524 sec/iter\n",
      "Epoch: 260 | Batch: 001 / 011 | Total loss: 0.870 | Reg loss: 0.028 | Tree loss: 0.870 | Accuracy: 0.664000 | 0.524 sec/iter\n",
      "Epoch: 260 | Batch: 002 / 011 | Total loss: 0.839 | Reg loss: 0.028 | Tree loss: 0.839 | Accuracy: 0.683000 | 0.524 sec/iter\n",
      "Epoch: 260 | Batch: 003 / 011 | Total loss: 0.819 | Reg loss: 0.028 | Tree loss: 0.819 | Accuracy: 0.709000 | 0.524 sec/iter\n",
      "Epoch: 260 | Batch: 004 / 011 | Total loss: 0.827 | Reg loss: 0.028 | Tree loss: 0.827 | Accuracy: 0.705500 | 0.524 sec/iter\n",
      "Epoch: 260 | Batch: 005 / 011 | Total loss: 0.809 | Reg loss: 0.028 | Tree loss: 0.809 | Accuracy: 0.709000 | 0.524 sec/iter\n",
      "Epoch: 260 | Batch: 006 / 011 | Total loss: 0.837 | Reg loss: 0.028 | Tree loss: 0.837 | Accuracy: 0.697000 | 0.524 sec/iter\n",
      "Epoch: 260 | Batch: 007 / 011 | Total loss: 0.758 | Reg loss: 0.028 | Tree loss: 0.758 | Accuracy: 0.739500 | 0.524 sec/iter\n",
      "Epoch: 260 | Batch: 008 / 011 | Total loss: 0.809 | Reg loss: 0.028 | Tree loss: 0.809 | Accuracy: 0.718500 | 0.524 sec/iter\n",
      "Epoch: 260 | Batch: 009 / 011 | Total loss: 0.807 | Reg loss: 0.028 | Tree loss: 0.807 | Accuracy: 0.716000 | 0.523 sec/iter\n",
      "Epoch: 260 | Batch: 010 / 011 | Total loss: 0.826 | Reg loss: 0.028 | Tree loss: 0.826 | Accuracy: 0.706485 | 0.523 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 261 | Batch: 000 / 011 | Total loss: 0.888 | Reg loss: 0.028 | Tree loss: 0.888 | Accuracy: 0.660500 | 0.525 sec/iter\n",
      "Epoch: 261 | Batch: 001 / 011 | Total loss: 0.868 | Reg loss: 0.028 | Tree loss: 0.868 | Accuracy: 0.672500 | 0.525 sec/iter\n",
      "Epoch: 261 | Batch: 002 / 011 | Total loss: 0.848 | Reg loss: 0.028 | Tree loss: 0.848 | Accuracy: 0.684000 | 0.525 sec/iter\n",
      "Epoch: 261 | Batch: 003 / 011 | Total loss: 0.831 | Reg loss: 0.028 | Tree loss: 0.831 | Accuracy: 0.700500 | 0.525 sec/iter\n",
      "Epoch: 261 | Batch: 004 / 011 | Total loss: 0.794 | Reg loss: 0.028 | Tree loss: 0.794 | Accuracy: 0.722000 | 0.524 sec/iter\n",
      "Epoch: 261 | Batch: 005 / 011 | Total loss: 0.812 | Reg loss: 0.028 | Tree loss: 0.812 | Accuracy: 0.716500 | 0.524 sec/iter\n",
      "Epoch: 261 | Batch: 006 / 011 | Total loss: 0.784 | Reg loss: 0.028 | Tree loss: 0.784 | Accuracy: 0.748500 | 0.524 sec/iter\n",
      "Epoch: 261 | Batch: 007 / 011 | Total loss: 0.806 | Reg loss: 0.028 | Tree loss: 0.806 | Accuracy: 0.731500 | 0.524 sec/iter\n",
      "Epoch: 261 | Batch: 008 / 011 | Total loss: 0.790 | Reg loss: 0.028 | Tree loss: 0.790 | Accuracy: 0.749500 | 0.524 sec/iter\n",
      "Epoch: 261 | Batch: 009 / 011 | Total loss: 0.791 | Reg loss: 0.028 | Tree loss: 0.791 | Accuracy: 0.742000 | 0.524 sec/iter\n",
      "Epoch: 261 | Batch: 010 / 011 | Total loss: 0.827 | Reg loss: 0.028 | Tree loss: 0.827 | Accuracy: 0.686007 | 0.524 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 262 | Batch: 000 / 011 | Total loss: 0.914 | Reg loss: 0.028 | Tree loss: 0.914 | Accuracy: 0.641500 | 0.524 sec/iter\n",
      "Epoch: 262 | Batch: 001 / 011 | Total loss: 0.879 | Reg loss: 0.028 | Tree loss: 0.879 | Accuracy: 0.669000 | 0.524 sec/iter\n",
      "Epoch: 262 | Batch: 002 / 011 | Total loss: 0.855 | Reg loss: 0.028 | Tree loss: 0.855 | Accuracy: 0.695000 | 0.524 sec/iter\n",
      "Epoch: 262 | Batch: 003 / 011 | Total loss: 0.822 | Reg loss: 0.028 | Tree loss: 0.822 | Accuracy: 0.711500 | 0.524 sec/iter\n",
      "Epoch: 262 | Batch: 004 / 011 | Total loss: 0.788 | Reg loss: 0.028 | Tree loss: 0.788 | Accuracy: 0.733500 | 0.524 sec/iter\n",
      "Epoch: 262 | Batch: 005 / 011 | Total loss: 0.800 | Reg loss: 0.028 | Tree loss: 0.800 | Accuracy: 0.714000 | 0.524 sec/iter\n",
      "Epoch: 262 | Batch: 006 / 011 | Total loss: 0.827 | Reg loss: 0.028 | Tree loss: 0.827 | Accuracy: 0.714500 | 0.523 sec/iter\n",
      "Epoch: 262 | Batch: 007 / 011 | Total loss: 0.772 | Reg loss: 0.028 | Tree loss: 0.772 | Accuracy: 0.749000 | 0.523 sec/iter\n",
      "Epoch: 262 | Batch: 008 / 011 | Total loss: 0.788 | Reg loss: 0.028 | Tree loss: 0.788 | Accuracy: 0.724500 | 0.523 sec/iter\n",
      "Epoch: 262 | Batch: 009 / 011 | Total loss: 0.779 | Reg loss: 0.028 | Tree loss: 0.779 | Accuracy: 0.745000 | 0.523 sec/iter\n",
      "Epoch: 262 | Batch: 010 / 011 | Total loss: 0.849 | Reg loss: 0.028 | Tree loss: 0.849 | Accuracy: 0.709898 | 0.523 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 263 | Batch: 000 / 011 | Total loss: 0.870 | Reg loss: 0.028 | Tree loss: 0.870 | Accuracy: 0.678000 | 0.525 sec/iter\n",
      "Epoch: 263 | Batch: 001 / 011 | Total loss: 0.871 | Reg loss: 0.028 | Tree loss: 0.871 | Accuracy: 0.664500 | 0.524 sec/iter\n",
      "Epoch: 263 | Batch: 002 / 011 | Total loss: 0.836 | Reg loss: 0.028 | Tree loss: 0.836 | Accuracy: 0.682500 | 0.524 sec/iter\n",
      "Epoch: 263 | Batch: 003 / 011 | Total loss: 0.833 | Reg loss: 0.028 | Tree loss: 0.833 | Accuracy: 0.691500 | 0.524 sec/iter\n",
      "Epoch: 263 | Batch: 004 / 011 | Total loss: 0.811 | Reg loss: 0.028 | Tree loss: 0.811 | Accuracy: 0.719500 | 0.524 sec/iter\n",
      "Epoch: 263 | Batch: 005 / 011 | Total loss: 0.813 | Reg loss: 0.028 | Tree loss: 0.813 | Accuracy: 0.717500 | 0.524 sec/iter\n",
      "Epoch: 263 | Batch: 006 / 011 | Total loss: 0.787 | Reg loss: 0.028 | Tree loss: 0.787 | Accuracy: 0.719500 | 0.524 sec/iter\n",
      "Epoch: 263 | Batch: 007 / 011 | Total loss: 0.813 | Reg loss: 0.028 | Tree loss: 0.813 | Accuracy: 0.711000 | 0.524 sec/iter\n",
      "Epoch: 263 | Batch: 008 / 011 | Total loss: 0.791 | Reg loss: 0.028 | Tree loss: 0.791 | Accuracy: 0.731500 | 0.524 sec/iter\n",
      "Epoch: 263 | Batch: 009 / 011 | Total loss: 0.806 | Reg loss: 0.028 | Tree loss: 0.806 | Accuracy: 0.737500 | 0.524 sec/iter\n",
      "Epoch: 263 | Batch: 010 / 011 | Total loss: 0.820 | Reg loss: 0.028 | Tree loss: 0.820 | Accuracy: 0.703072 | 0.524 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 264 | Batch: 000 / 011 | Total loss: 0.893 | Reg loss: 0.028 | Tree loss: 0.893 | Accuracy: 0.667500 | 0.524 sec/iter\n",
      "Epoch: 264 | Batch: 001 / 011 | Total loss: 0.876 | Reg loss: 0.028 | Tree loss: 0.876 | Accuracy: 0.663000 | 0.524 sec/iter\n",
      "Epoch: 264 | Batch: 002 / 011 | Total loss: 0.851 | Reg loss: 0.028 | Tree loss: 0.851 | Accuracy: 0.682500 | 0.523 sec/iter\n",
      "Epoch: 264 | Batch: 003 / 011 | Total loss: 0.811 | Reg loss: 0.028 | Tree loss: 0.811 | Accuracy: 0.705500 | 0.523 sec/iter\n",
      "Epoch: 264 | Batch: 004 / 011 | Total loss: 0.799 | Reg loss: 0.028 | Tree loss: 0.799 | Accuracy: 0.724000 | 0.523 sec/iter\n",
      "Epoch: 264 | Batch: 005 / 011 | Total loss: 0.786 | Reg loss: 0.028 | Tree loss: 0.786 | Accuracy: 0.724000 | 0.523 sec/iter\n",
      "Epoch: 264 | Batch: 006 / 011 | Total loss: 0.787 | Reg loss: 0.028 | Tree loss: 0.787 | Accuracy: 0.750000 | 0.523 sec/iter\n",
      "Epoch: 264 | Batch: 007 / 011 | Total loss: 0.799 | Reg loss: 0.028 | Tree loss: 0.799 | Accuracy: 0.731500 | 0.523 sec/iter\n",
      "Epoch: 264 | Batch: 008 / 011 | Total loss: 0.819 | Reg loss: 0.028 | Tree loss: 0.819 | Accuracy: 0.703500 | 0.523 sec/iter\n",
      "Epoch: 264 | Batch: 009 / 011 | Total loss: 0.801 | Reg loss: 0.028 | Tree loss: 0.801 | Accuracy: 0.713000 | 0.523 sec/iter\n",
      "Epoch: 264 | Batch: 010 / 011 | Total loss: 0.874 | Reg loss: 0.028 | Tree loss: 0.874 | Accuracy: 0.672355 | 0.523 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 265 | Batch: 000 / 011 | Total loss: 0.883 | Reg loss: 0.028 | Tree loss: 0.883 | Accuracy: 0.665000 | 0.523 sec/iter\n",
      "Epoch: 265 | Batch: 001 / 011 | Total loss: 0.869 | Reg loss: 0.028 | Tree loss: 0.869 | Accuracy: 0.653500 | 0.523 sec/iter\n",
      "Epoch: 265 | Batch: 002 / 011 | Total loss: 0.877 | Reg loss: 0.028 | Tree loss: 0.877 | Accuracy: 0.670500 | 0.523 sec/iter\n",
      "Epoch: 265 | Batch: 003 / 011 | Total loss: 0.823 | Reg loss: 0.028 | Tree loss: 0.823 | Accuracy: 0.706000 | 0.523 sec/iter\n",
      "Epoch: 265 | Batch: 004 / 011 | Total loss: 0.845 | Reg loss: 0.028 | Tree loss: 0.845 | Accuracy: 0.684000 | 0.523 sec/iter\n",
      "Epoch: 265 | Batch: 005 / 011 | Total loss: 0.820 | Reg loss: 0.028 | Tree loss: 0.820 | Accuracy: 0.718000 | 0.523 sec/iter\n",
      "Epoch: 265 | Batch: 006 / 011 | Total loss: 0.784 | Reg loss: 0.028 | Tree loss: 0.784 | Accuracy: 0.729000 | 0.523 sec/iter\n",
      "Epoch: 265 | Batch: 007 / 011 | Total loss: 0.792 | Reg loss: 0.028 | Tree loss: 0.792 | Accuracy: 0.716000 | 0.523 sec/iter\n",
      "Epoch: 265 | Batch: 008 / 011 | Total loss: 0.769 | Reg loss: 0.028 | Tree loss: 0.769 | Accuracy: 0.731500 | 0.523 sec/iter\n",
      "Epoch: 265 | Batch: 009 / 011 | Total loss: 0.803 | Reg loss: 0.028 | Tree loss: 0.803 | Accuracy: 0.719500 | 0.523 sec/iter\n",
      "Epoch: 265 | Batch: 010 / 011 | Total loss: 0.768 | Reg loss: 0.028 | Tree loss: 0.768 | Accuracy: 0.747440 | 0.522 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 266 | Batch: 000 / 011 | Total loss: 0.882 | Reg loss: 0.028 | Tree loss: 0.882 | Accuracy: 0.655500 | 0.522 sec/iter\n",
      "Epoch: 266 | Batch: 001 / 011 | Total loss: 0.864 | Reg loss: 0.028 | Tree loss: 0.864 | Accuracy: 0.676000 | 0.522 sec/iter\n",
      "Epoch: 266 | Batch: 002 / 011 | Total loss: 0.858 | Reg loss: 0.028 | Tree loss: 0.858 | Accuracy: 0.675000 | 0.522 sec/iter\n",
      "Epoch: 266 | Batch: 003 / 011 | Total loss: 0.819 | Reg loss: 0.028 | Tree loss: 0.819 | Accuracy: 0.699000 | 0.522 sec/iter\n",
      "Epoch: 266 | Batch: 004 / 011 | Total loss: 0.793 | Reg loss: 0.028 | Tree loss: 0.793 | Accuracy: 0.734500 | 0.522 sec/iter\n",
      "Epoch: 266 | Batch: 005 / 011 | Total loss: 0.792 | Reg loss: 0.028 | Tree loss: 0.792 | Accuracy: 0.732000 | 0.522 sec/iter\n",
      "Epoch: 266 | Batch: 006 / 011 | Total loss: 0.806 | Reg loss: 0.028 | Tree loss: 0.806 | Accuracy: 0.725500 | 0.522 sec/iter\n",
      "Epoch: 266 | Batch: 007 / 011 | Total loss: 0.808 | Reg loss: 0.028 | Tree loss: 0.808 | Accuracy: 0.713500 | 0.522 sec/iter\n",
      "Epoch: 266 | Batch: 008 / 011 | Total loss: 0.785 | Reg loss: 0.028 | Tree loss: 0.785 | Accuracy: 0.739500 | 0.522 sec/iter\n",
      "Epoch: 266 | Batch: 009 / 011 | Total loss: 0.825 | Reg loss: 0.028 | Tree loss: 0.825 | Accuracy: 0.704500 | 0.521 sec/iter\n",
      "Epoch: 266 | Batch: 010 / 011 | Total loss: 0.824 | Reg loss: 0.028 | Tree loss: 0.824 | Accuracy: 0.733788 | 0.521 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 267 | Batch: 000 / 011 | Total loss: 0.882 | Reg loss: 0.028 | Tree loss: 0.882 | Accuracy: 0.671500 | 0.522 sec/iter\n",
      "Epoch: 267 | Batch: 001 / 011 | Total loss: 0.871 | Reg loss: 0.028 | Tree loss: 0.871 | Accuracy: 0.672000 | 0.522 sec/iter\n",
      "Epoch: 267 | Batch: 002 / 011 | Total loss: 0.846 | Reg loss: 0.028 | Tree loss: 0.846 | Accuracy: 0.672500 | 0.522 sec/iter\n",
      "Epoch: 267 | Batch: 003 / 011 | Total loss: 0.848 | Reg loss: 0.028 | Tree loss: 0.848 | Accuracy: 0.696500 | 0.522 sec/iter\n",
      "Epoch: 267 | Batch: 004 / 011 | Total loss: 0.820 | Reg loss: 0.028 | Tree loss: 0.820 | Accuracy: 0.715000 | 0.522 sec/iter\n",
      "Epoch: 267 | Batch: 005 / 011 | Total loss: 0.818 | Reg loss: 0.028 | Tree loss: 0.818 | Accuracy: 0.710500 | 0.521 sec/iter\n",
      "Epoch: 267 | Batch: 006 / 011 | Total loss: 0.789 | Reg loss: 0.028 | Tree loss: 0.789 | Accuracy: 0.727000 | 0.521 sec/iter\n",
      "Epoch: 267 | Batch: 007 / 011 | Total loss: 0.797 | Reg loss: 0.028 | Tree loss: 0.797 | Accuracy: 0.730500 | 0.521 sec/iter\n",
      "Epoch: 267 | Batch: 008 / 011 | Total loss: 0.788 | Reg loss: 0.028 | Tree loss: 0.788 | Accuracy: 0.726000 | 0.521 sec/iter\n",
      "Epoch: 267 | Batch: 009 / 011 | Total loss: 0.777 | Reg loss: 0.028 | Tree loss: 0.777 | Accuracy: 0.746500 | 0.521 sec/iter\n",
      "Epoch: 267 | Batch: 010 / 011 | Total loss: 0.741 | Reg loss: 0.028 | Tree loss: 0.741 | Accuracy: 0.757679 | 0.521 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 268 | Batch: 000 / 011 | Total loss: 0.899 | Reg loss: 0.028 | Tree loss: 0.899 | Accuracy: 0.635000 | 0.522 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 268 | Batch: 001 / 011 | Total loss: 0.875 | Reg loss: 0.028 | Tree loss: 0.875 | Accuracy: 0.675500 | 0.522 sec/iter\n",
      "Epoch: 268 | Batch: 002 / 011 | Total loss: 0.853 | Reg loss: 0.028 | Tree loss: 0.853 | Accuracy: 0.684500 | 0.521 sec/iter\n",
      "Epoch: 268 | Batch: 003 / 011 | Total loss: 0.827 | Reg loss: 0.028 | Tree loss: 0.827 | Accuracy: 0.713500 | 0.521 sec/iter\n",
      "Epoch: 268 | Batch: 004 / 011 | Total loss: 0.805 | Reg loss: 0.028 | Tree loss: 0.805 | Accuracy: 0.721000 | 0.521 sec/iter\n",
      "Epoch: 268 | Batch: 005 / 011 | Total loss: 0.815 | Reg loss: 0.028 | Tree loss: 0.815 | Accuracy: 0.710000 | 0.521 sec/iter\n",
      "Epoch: 268 | Batch: 006 / 011 | Total loss: 0.780 | Reg loss: 0.028 | Tree loss: 0.780 | Accuracy: 0.730500 | 0.521 sec/iter\n",
      "Epoch: 268 | Batch: 007 / 011 | Total loss: 0.779 | Reg loss: 0.028 | Tree loss: 0.779 | Accuracy: 0.737000 | 0.521 sec/iter\n",
      "Epoch: 268 | Batch: 008 / 011 | Total loss: 0.791 | Reg loss: 0.028 | Tree loss: 0.791 | Accuracy: 0.727500 | 0.521 sec/iter\n",
      "Epoch: 268 | Batch: 009 / 011 | Total loss: 0.802 | Reg loss: 0.028 | Tree loss: 0.802 | Accuracy: 0.736500 | 0.521 sec/iter\n",
      "Epoch: 268 | Batch: 010 / 011 | Total loss: 0.786 | Reg loss: 0.028 | Tree loss: 0.786 | Accuracy: 0.716724 | 0.521 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 269 | Batch: 000 / 011 | Total loss: 0.882 | Reg loss: 0.028 | Tree loss: 0.882 | Accuracy: 0.667500 | 0.521 sec/iter\n",
      "Epoch: 269 | Batch: 001 / 011 | Total loss: 0.882 | Reg loss: 0.028 | Tree loss: 0.882 | Accuracy: 0.648500 | 0.521 sec/iter\n",
      "Epoch: 269 | Batch: 002 / 011 | Total loss: 0.848 | Reg loss: 0.028 | Tree loss: 0.848 | Accuracy: 0.685500 | 0.52 sec/iter\n",
      "Epoch: 269 | Batch: 003 / 011 | Total loss: 0.838 | Reg loss: 0.028 | Tree loss: 0.838 | Accuracy: 0.689000 | 0.52 sec/iter\n",
      "Epoch: 269 | Batch: 004 / 011 | Total loss: 0.842 | Reg loss: 0.028 | Tree loss: 0.842 | Accuracy: 0.692000 | 0.52 sec/iter\n",
      "Epoch: 269 | Batch: 005 / 011 | Total loss: 0.798 | Reg loss: 0.028 | Tree loss: 0.798 | Accuracy: 0.714500 | 0.52 sec/iter\n",
      "Epoch: 269 | Batch: 006 / 011 | Total loss: 0.805 | Reg loss: 0.028 | Tree loss: 0.805 | Accuracy: 0.731500 | 0.52 sec/iter\n",
      "Epoch: 269 | Batch: 007 / 011 | Total loss: 0.780 | Reg loss: 0.028 | Tree loss: 0.780 | Accuracy: 0.737000 | 0.52 sec/iter\n",
      "Epoch: 269 | Batch: 008 / 011 | Total loss: 0.795 | Reg loss: 0.028 | Tree loss: 0.795 | Accuracy: 0.725000 | 0.52 sec/iter\n",
      "Epoch: 269 | Batch: 009 / 011 | Total loss: 0.789 | Reg loss: 0.028 | Tree loss: 0.789 | Accuracy: 0.716000 | 0.52 sec/iter\n",
      "Epoch: 269 | Batch: 010 / 011 | Total loss: 0.850 | Reg loss: 0.028 | Tree loss: 0.850 | Accuracy: 0.713311 | 0.52 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 270 | Batch: 000 / 011 | Total loss: 0.880 | Reg loss: 0.028 | Tree loss: 0.880 | Accuracy: 0.668000 | 0.521 sec/iter\n",
      "Epoch: 270 | Batch: 001 / 011 | Total loss: 0.864 | Reg loss: 0.028 | Tree loss: 0.864 | Accuracy: 0.671500 | 0.521 sec/iter\n",
      "Epoch: 270 | Batch: 002 / 011 | Total loss: 0.864 | Reg loss: 0.028 | Tree loss: 0.864 | Accuracy: 0.688500 | 0.521 sec/iter\n",
      "Epoch: 270 | Batch: 003 / 011 | Total loss: 0.836 | Reg loss: 0.028 | Tree loss: 0.836 | Accuracy: 0.694000 | 0.521 sec/iter\n",
      "Epoch: 270 | Batch: 004 / 011 | Total loss: 0.804 | Reg loss: 0.028 | Tree loss: 0.804 | Accuracy: 0.722500 | 0.521 sec/iter\n",
      "Epoch: 270 | Batch: 005 / 011 | Total loss: 0.802 | Reg loss: 0.028 | Tree loss: 0.802 | Accuracy: 0.719000 | 0.521 sec/iter\n",
      "Epoch: 270 | Batch: 006 / 011 | Total loss: 0.813 | Reg loss: 0.028 | Tree loss: 0.813 | Accuracy: 0.721500 | 0.521 sec/iter\n",
      "Epoch: 270 | Batch: 007 / 011 | Total loss: 0.784 | Reg loss: 0.028 | Tree loss: 0.784 | Accuracy: 0.747500 | 0.52 sec/iter\n",
      "Epoch: 270 | Batch: 008 / 011 | Total loss: 0.787 | Reg loss: 0.028 | Tree loss: 0.787 | Accuracy: 0.739000 | 0.52 sec/iter\n",
      "Epoch: 270 | Batch: 009 / 011 | Total loss: 0.800 | Reg loss: 0.028 | Tree loss: 0.800 | Accuracy: 0.718500 | 0.52 sec/iter\n",
      "Epoch: 270 | Batch: 010 / 011 | Total loss: 0.767 | Reg loss: 0.028 | Tree loss: 0.767 | Accuracy: 0.744027 | 0.52 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 271 | Batch: 000 / 011 | Total loss: 0.875 | Reg loss: 0.028 | Tree loss: 0.875 | Accuracy: 0.667500 | 0.52 sec/iter\n",
      "Epoch: 271 | Batch: 001 / 011 | Total loss: 0.864 | Reg loss: 0.028 | Tree loss: 0.864 | Accuracy: 0.679000 | 0.52 sec/iter\n",
      "Epoch: 271 | Batch: 002 / 011 | Total loss: 0.847 | Reg loss: 0.028 | Tree loss: 0.847 | Accuracy: 0.685000 | 0.52 sec/iter\n",
      "Epoch: 271 | Batch: 003 / 011 | Total loss: 0.823 | Reg loss: 0.028 | Tree loss: 0.823 | Accuracy: 0.700000 | 0.52 sec/iter\n",
      "Epoch: 271 | Batch: 004 / 011 | Total loss: 0.818 | Reg loss: 0.028 | Tree loss: 0.818 | Accuracy: 0.704500 | 0.52 sec/iter\n",
      "Epoch: 271 | Batch: 005 / 011 | Total loss: 0.805 | Reg loss: 0.028 | Tree loss: 0.805 | Accuracy: 0.718000 | 0.52 sec/iter\n",
      "Epoch: 271 | Batch: 006 / 011 | Total loss: 0.790 | Reg loss: 0.028 | Tree loss: 0.790 | Accuracy: 0.721500 | 0.52 sec/iter\n",
      "Epoch: 271 | Batch: 007 / 011 | Total loss: 0.813 | Reg loss: 0.028 | Tree loss: 0.813 | Accuracy: 0.703000 | 0.52 sec/iter\n",
      "Epoch: 271 | Batch: 008 / 011 | Total loss: 0.800 | Reg loss: 0.028 | Tree loss: 0.800 | Accuracy: 0.710500 | 0.52 sec/iter\n",
      "Epoch: 271 | Batch: 009 / 011 | Total loss: 0.790 | Reg loss: 0.028 | Tree loss: 0.790 | Accuracy: 0.725500 | 0.52 sec/iter\n",
      "Epoch: 271 | Batch: 010 / 011 | Total loss: 0.770 | Reg loss: 0.028 | Tree loss: 0.770 | Accuracy: 0.740614 | 0.519 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 272 | Batch: 000 / 011 | Total loss: 0.892 | Reg loss: 0.028 | Tree loss: 0.892 | Accuracy: 0.659000 | 0.52 sec/iter\n",
      "Epoch: 272 | Batch: 001 / 011 | Total loss: 0.862 | Reg loss: 0.028 | Tree loss: 0.862 | Accuracy: 0.672500 | 0.52 sec/iter\n",
      "Epoch: 272 | Batch: 002 / 011 | Total loss: 0.841 | Reg loss: 0.028 | Tree loss: 0.841 | Accuracy: 0.694000 | 0.52 sec/iter\n",
      "Epoch: 272 | Batch: 003 / 011 | Total loss: 0.841 | Reg loss: 0.028 | Tree loss: 0.841 | Accuracy: 0.681000 | 0.52 sec/iter\n",
      "Epoch: 272 | Batch: 004 / 011 | Total loss: 0.823 | Reg loss: 0.028 | Tree loss: 0.823 | Accuracy: 0.704000 | 0.52 sec/iter\n",
      "Epoch: 272 | Batch: 005 / 011 | Total loss: 0.795 | Reg loss: 0.028 | Tree loss: 0.795 | Accuracy: 0.705000 | 0.519 sec/iter\n",
      "Epoch: 272 | Batch: 006 / 011 | Total loss: 0.811 | Reg loss: 0.028 | Tree loss: 0.811 | Accuracy: 0.713000 | 0.519 sec/iter\n",
      "Epoch: 272 | Batch: 007 / 011 | Total loss: 0.800 | Reg loss: 0.028 | Tree loss: 0.800 | Accuracy: 0.720000 | 0.519 sec/iter\n",
      "Epoch: 272 | Batch: 008 / 011 | Total loss: 0.782 | Reg loss: 0.028 | Tree loss: 0.782 | Accuracy: 0.749000 | 0.519 sec/iter\n",
      "Epoch: 272 | Batch: 009 / 011 | Total loss: 0.798 | Reg loss: 0.028 | Tree loss: 0.798 | Accuracy: 0.723000 | 0.519 sec/iter\n",
      "Epoch: 272 | Batch: 010 / 011 | Total loss: 0.794 | Reg loss: 0.028 | Tree loss: 0.794 | Accuracy: 0.716724 | 0.519 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 273 | Batch: 000 / 011 | Total loss: 0.879 | Reg loss: 0.028 | Tree loss: 0.879 | Accuracy: 0.667500 | 0.52 sec/iter\n",
      "Epoch: 273 | Batch: 001 / 011 | Total loss: 0.868 | Reg loss: 0.028 | Tree loss: 0.868 | Accuracy: 0.670500 | 0.52 sec/iter\n",
      "Epoch: 273 | Batch: 002 / 011 | Total loss: 0.856 | Reg loss: 0.028 | Tree loss: 0.856 | Accuracy: 0.677000 | 0.52 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 273 | Batch: 003 / 011 | Total loss: 0.830 | Reg loss: 0.028 | Tree loss: 0.830 | Accuracy: 0.702000 | 0.519 sec/iter\n",
      "Epoch: 273 | Batch: 004 / 011 | Total loss: 0.799 | Reg loss: 0.028 | Tree loss: 0.799 | Accuracy: 0.724500 | 0.519 sec/iter\n",
      "Epoch: 273 | Batch: 005 / 011 | Total loss: 0.821 | Reg loss: 0.028 | Tree loss: 0.821 | Accuracy: 0.696500 | 0.519 sec/iter\n",
      "Epoch: 273 | Batch: 006 / 011 | Total loss: 0.795 | Reg loss: 0.028 | Tree loss: 0.795 | Accuracy: 0.735000 | 0.519 sec/iter\n",
      "Epoch: 273 | Batch: 007 / 011 | Total loss: 0.794 | Reg loss: 0.028 | Tree loss: 0.794 | Accuracy: 0.730000 | 0.519 sec/iter\n",
      "Epoch: 273 | Batch: 008 / 011 | Total loss: 0.774 | Reg loss: 0.028 | Tree loss: 0.774 | Accuracy: 0.742500 | 0.519 sec/iter\n",
      "Epoch: 273 | Batch: 009 / 011 | Total loss: 0.793 | Reg loss: 0.028 | Tree loss: 0.793 | Accuracy: 0.718500 | 0.519 sec/iter\n",
      "Epoch: 273 | Batch: 010 / 011 | Total loss: 0.838 | Reg loss: 0.028 | Tree loss: 0.838 | Accuracy: 0.706485 | 0.519 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 274 | Batch: 000 / 011 | Total loss: 0.878 | Reg loss: 0.028 | Tree loss: 0.878 | Accuracy: 0.658500 | 0.519 sec/iter\n",
      "Epoch: 274 | Batch: 001 / 011 | Total loss: 0.874 | Reg loss: 0.028 | Tree loss: 0.874 | Accuracy: 0.658500 | 0.519 sec/iter\n",
      "Epoch: 274 | Batch: 002 / 011 | Total loss: 0.838 | Reg loss: 0.028 | Tree loss: 0.838 | Accuracy: 0.687500 | 0.519 sec/iter\n",
      "Epoch: 274 | Batch: 003 / 011 | Total loss: 0.845 | Reg loss: 0.028 | Tree loss: 0.845 | Accuracy: 0.685000 | 0.518 sec/iter\n",
      "Epoch: 274 | Batch: 004 / 011 | Total loss: 0.828 | Reg loss: 0.028 | Tree loss: 0.828 | Accuracy: 0.699000 | 0.518 sec/iter\n",
      "Epoch: 274 | Batch: 005 / 011 | Total loss: 0.766 | Reg loss: 0.028 | Tree loss: 0.766 | Accuracy: 0.728500 | 0.518 sec/iter\n",
      "Epoch: 274 | Batch: 006 / 011 | Total loss: 0.798 | Reg loss: 0.028 | Tree loss: 0.798 | Accuracy: 0.725500 | 0.518 sec/iter\n",
      "Epoch: 274 | Batch: 007 / 011 | Total loss: 0.802 | Reg loss: 0.028 | Tree loss: 0.802 | Accuracy: 0.724000 | 0.518 sec/iter\n",
      "Epoch: 274 | Batch: 008 / 011 | Total loss: 0.800 | Reg loss: 0.028 | Tree loss: 0.800 | Accuracy: 0.731500 | 0.518 sec/iter\n",
      "Epoch: 274 | Batch: 009 / 011 | Total loss: 0.778 | Reg loss: 0.028 | Tree loss: 0.778 | Accuracy: 0.733500 | 0.518 sec/iter\n",
      "Epoch: 274 | Batch: 010 / 011 | Total loss: 0.826 | Reg loss: 0.028 | Tree loss: 0.826 | Accuracy: 0.720137 | 0.518 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 275 | Batch: 000 / 011 | Total loss: 0.882 | Reg loss: 0.028 | Tree loss: 0.882 | Accuracy: 0.679500 | 0.518 sec/iter\n",
      "Epoch: 275 | Batch: 001 / 011 | Total loss: 0.878 | Reg loss: 0.028 | Tree loss: 0.878 | Accuracy: 0.664500 | 0.518 sec/iter\n",
      "Epoch: 275 | Batch: 002 / 011 | Total loss: 0.848 | Reg loss: 0.028 | Tree loss: 0.848 | Accuracy: 0.678000 | 0.518 sec/iter\n",
      "Epoch: 275 | Batch: 003 / 011 | Total loss: 0.809 | Reg loss: 0.028 | Tree loss: 0.809 | Accuracy: 0.721000 | 0.518 sec/iter\n",
      "Epoch: 275 | Batch: 004 / 011 | Total loss: 0.810 | Reg loss: 0.028 | Tree loss: 0.810 | Accuracy: 0.716000 | 0.518 sec/iter\n",
      "Epoch: 275 | Batch: 005 / 011 | Total loss: 0.787 | Reg loss: 0.028 | Tree loss: 0.787 | Accuracy: 0.731000 | 0.518 sec/iter\n",
      "Epoch: 275 | Batch: 006 / 011 | Total loss: 0.799 | Reg loss: 0.028 | Tree loss: 0.799 | Accuracy: 0.729500 | 0.518 sec/iter\n",
      "Epoch: 275 | Batch: 007 / 011 | Total loss: 0.779 | Reg loss: 0.028 | Tree loss: 0.779 | Accuracy: 0.731500 | 0.518 sec/iter\n",
      "Epoch: 275 | Batch: 008 / 011 | Total loss: 0.828 | Reg loss: 0.028 | Tree loss: 0.828 | Accuracy: 0.709000 | 0.518 sec/iter\n",
      "Epoch: 275 | Batch: 009 / 011 | Total loss: 0.789 | Reg loss: 0.028 | Tree loss: 0.789 | Accuracy: 0.730500 | 0.517 sec/iter\n",
      "Epoch: 275 | Batch: 010 / 011 | Total loss: 0.753 | Reg loss: 0.028 | Tree loss: 0.753 | Accuracy: 0.784983 | 0.517 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 276 | Batch: 000 / 011 | Total loss: 0.869 | Reg loss: 0.028 | Tree loss: 0.869 | Accuracy: 0.673000 | 0.519 sec/iter\n",
      "Epoch: 276 | Batch: 001 / 011 | Total loss: 0.890 | Reg loss: 0.028 | Tree loss: 0.890 | Accuracy: 0.652500 | 0.519 sec/iter\n",
      "Epoch: 276 | Batch: 002 / 011 | Total loss: 0.845 | Reg loss: 0.028 | Tree loss: 0.845 | Accuracy: 0.687000 | 0.519 sec/iter\n",
      "Epoch: 276 | Batch: 003 / 011 | Total loss: 0.811 | Reg loss: 0.028 | Tree loss: 0.811 | Accuracy: 0.696000 | 0.519 sec/iter\n",
      "Epoch: 276 | Batch: 004 / 011 | Total loss: 0.818 | Reg loss: 0.028 | Tree loss: 0.818 | Accuracy: 0.702000 | 0.518 sec/iter\n",
      "Epoch: 276 | Batch: 005 / 011 | Total loss: 0.788 | Reg loss: 0.028 | Tree loss: 0.788 | Accuracy: 0.727500 | 0.518 sec/iter\n",
      "Epoch: 276 | Batch: 006 / 011 | Total loss: 0.807 | Reg loss: 0.028 | Tree loss: 0.807 | Accuracy: 0.726000 | 0.518 sec/iter\n",
      "Epoch: 276 | Batch: 007 / 011 | Total loss: 0.825 | Reg loss: 0.028 | Tree loss: 0.825 | Accuracy: 0.715500 | 0.518 sec/iter\n",
      "Epoch: 276 | Batch: 008 / 011 | Total loss: 0.771 | Reg loss: 0.028 | Tree loss: 0.771 | Accuracy: 0.741000 | 0.518 sec/iter\n",
      "Epoch: 276 | Batch: 009 / 011 | Total loss: 0.795 | Reg loss: 0.028 | Tree loss: 0.795 | Accuracy: 0.730000 | 0.518 sec/iter\n",
      "Epoch: 276 | Batch: 010 / 011 | Total loss: 0.777 | Reg loss: 0.028 | Tree loss: 0.777 | Accuracy: 0.733788 | 0.518 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 277 | Batch: 000 / 011 | Total loss: 0.881 | Reg loss: 0.028 | Tree loss: 0.881 | Accuracy: 0.667500 | 0.518 sec/iter\n",
      "Epoch: 277 | Batch: 001 / 011 | Total loss: 0.882 | Reg loss: 0.028 | Tree loss: 0.882 | Accuracy: 0.664500 | 0.518 sec/iter\n",
      "Epoch: 277 | Batch: 002 / 011 | Total loss: 0.834 | Reg loss: 0.028 | Tree loss: 0.834 | Accuracy: 0.689500 | 0.518 sec/iter\n",
      "Epoch: 277 | Batch: 003 / 011 | Total loss: 0.826 | Reg loss: 0.028 | Tree loss: 0.826 | Accuracy: 0.698500 | 0.518 sec/iter\n",
      "Epoch: 277 | Batch: 004 / 011 | Total loss: 0.803 | Reg loss: 0.028 | Tree loss: 0.803 | Accuracy: 0.721000 | 0.518 sec/iter\n",
      "Epoch: 277 | Batch: 005 / 011 | Total loss: 0.805 | Reg loss: 0.028 | Tree loss: 0.805 | Accuracy: 0.706000 | 0.518 sec/iter\n",
      "Epoch: 277 | Batch: 006 / 011 | Total loss: 0.791 | Reg loss: 0.028 | Tree loss: 0.791 | Accuracy: 0.721500 | 0.518 sec/iter\n",
      "Epoch: 277 | Batch: 007 / 011 | Total loss: 0.796 | Reg loss: 0.028 | Tree loss: 0.796 | Accuracy: 0.731500 | 0.518 sec/iter\n",
      "Epoch: 277 | Batch: 008 / 011 | Total loss: 0.829 | Reg loss: 0.028 | Tree loss: 0.829 | Accuracy: 0.708000 | 0.518 sec/iter\n",
      "Epoch: 277 | Batch: 009 / 011 | Total loss: 0.805 | Reg loss: 0.028 | Tree loss: 0.805 | Accuracy: 0.723500 | 0.517 sec/iter\n",
      "Epoch: 277 | Batch: 010 / 011 | Total loss: 0.791 | Reg loss: 0.028 | Tree loss: 0.791 | Accuracy: 0.757679 | 0.517 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 278 | Batch: 000 / 011 | Total loss: 0.870 | Reg loss: 0.028 | Tree loss: 0.870 | Accuracy: 0.663500 | 0.519 sec/iter\n",
      "Epoch: 278 | Batch: 001 / 011 | Total loss: 0.882 | Reg loss: 0.028 | Tree loss: 0.882 | Accuracy: 0.669500 | 0.519 sec/iter\n",
      "Epoch: 278 | Batch: 002 / 011 | Total loss: 0.852 | Reg loss: 0.028 | Tree loss: 0.852 | Accuracy: 0.687500 | 0.519 sec/iter\n",
      "Epoch: 278 | Batch: 003 / 011 | Total loss: 0.846 | Reg loss: 0.028 | Tree loss: 0.846 | Accuracy: 0.694500 | 0.519 sec/iter\n",
      "Epoch: 278 | Batch: 004 / 011 | Total loss: 0.821 | Reg loss: 0.028 | Tree loss: 0.821 | Accuracy: 0.712000 | 0.518 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 278 | Batch: 005 / 011 | Total loss: 0.785 | Reg loss: 0.028 | Tree loss: 0.785 | Accuracy: 0.729000 | 0.518 sec/iter\n",
      "Epoch: 278 | Batch: 006 / 011 | Total loss: 0.793 | Reg loss: 0.028 | Tree loss: 0.793 | Accuracy: 0.710000 | 0.518 sec/iter\n",
      "Epoch: 278 | Batch: 007 / 011 | Total loss: 0.780 | Reg loss: 0.028 | Tree loss: 0.780 | Accuracy: 0.740000 | 0.518 sec/iter\n",
      "Epoch: 278 | Batch: 008 / 011 | Total loss: 0.779 | Reg loss: 0.028 | Tree loss: 0.779 | Accuracy: 0.734000 | 0.518 sec/iter\n",
      "Epoch: 278 | Batch: 009 / 011 | Total loss: 0.806 | Reg loss: 0.028 | Tree loss: 0.806 | Accuracy: 0.723000 | 0.518 sec/iter\n",
      "Epoch: 278 | Batch: 010 / 011 | Total loss: 0.728 | Reg loss: 0.028 | Tree loss: 0.728 | Accuracy: 0.781570 | 0.518 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 279 | Batch: 000 / 011 | Total loss: 0.879 | Reg loss: 0.028 | Tree loss: 0.879 | Accuracy: 0.673500 | 0.519 sec/iter\n",
      "Epoch: 279 | Batch: 001 / 011 | Total loss: 0.867 | Reg loss: 0.028 | Tree loss: 0.867 | Accuracy: 0.662500 | 0.519 sec/iter\n",
      "Epoch: 279 | Batch: 002 / 011 | Total loss: 0.838 | Reg loss: 0.028 | Tree loss: 0.838 | Accuracy: 0.694000 | 0.519 sec/iter\n",
      "Epoch: 279 | Batch: 003 / 011 | Total loss: 0.842 | Reg loss: 0.028 | Tree loss: 0.842 | Accuracy: 0.677500 | 0.519 sec/iter\n",
      "Epoch: 279 | Batch: 004 / 011 | Total loss: 0.793 | Reg loss: 0.028 | Tree loss: 0.793 | Accuracy: 0.719000 | 0.519 sec/iter\n",
      "Epoch: 279 | Batch: 005 / 011 | Total loss: 0.821 | Reg loss: 0.028 | Tree loss: 0.821 | Accuracy: 0.715500 | 0.519 sec/iter\n",
      "Epoch: 279 | Batch: 006 / 011 | Total loss: 0.782 | Reg loss: 0.028 | Tree loss: 0.782 | Accuracy: 0.730500 | 0.519 sec/iter\n",
      "Epoch: 279 | Batch: 007 / 011 | Total loss: 0.800 | Reg loss: 0.028 | Tree loss: 0.800 | Accuracy: 0.716500 | 0.519 sec/iter\n",
      "Epoch: 279 | Batch: 008 / 011 | Total loss: 0.794 | Reg loss: 0.028 | Tree loss: 0.794 | Accuracy: 0.718500 | 0.519 sec/iter\n",
      "Epoch: 279 | Batch: 009 / 011 | Total loss: 0.801 | Reg loss: 0.028 | Tree loss: 0.801 | Accuracy: 0.727000 | 0.519 sec/iter\n",
      "Epoch: 279 | Batch: 010 / 011 | Total loss: 0.760 | Reg loss: 0.028 | Tree loss: 0.760 | Accuracy: 0.757679 | 0.519 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 280 | Batch: 000 / 011 | Total loss: 0.877 | Reg loss: 0.028 | Tree loss: 0.877 | Accuracy: 0.666500 | 0.519 sec/iter\n",
      "Epoch: 280 | Batch: 001 / 011 | Total loss: 0.860 | Reg loss: 0.028 | Tree loss: 0.860 | Accuracy: 0.665000 | 0.518 sec/iter\n",
      "Epoch: 280 | Batch: 002 / 011 | Total loss: 0.850 | Reg loss: 0.028 | Tree loss: 0.850 | Accuracy: 0.690500 | 0.518 sec/iter\n",
      "Epoch: 280 | Batch: 003 / 011 | Total loss: 0.819 | Reg loss: 0.028 | Tree loss: 0.819 | Accuracy: 0.708500 | 0.518 sec/iter\n",
      "Epoch: 280 | Batch: 004 / 011 | Total loss: 0.827 | Reg loss: 0.028 | Tree loss: 0.827 | Accuracy: 0.710500 | 0.518 sec/iter\n",
      "Epoch: 280 | Batch: 005 / 011 | Total loss: 0.785 | Reg loss: 0.028 | Tree loss: 0.785 | Accuracy: 0.720500 | 0.518 sec/iter\n",
      "Epoch: 280 | Batch: 006 / 011 | Total loss: 0.789 | Reg loss: 0.028 | Tree loss: 0.789 | Accuracy: 0.726000 | 0.518 sec/iter\n",
      "Epoch: 280 | Batch: 007 / 011 | Total loss: 0.815 | Reg loss: 0.028 | Tree loss: 0.815 | Accuracy: 0.697500 | 0.518 sec/iter\n",
      "Epoch: 280 | Batch: 008 / 011 | Total loss: 0.800 | Reg loss: 0.028 | Tree loss: 0.800 | Accuracy: 0.714000 | 0.518 sec/iter\n",
      "Epoch: 280 | Batch: 009 / 011 | Total loss: 0.782 | Reg loss: 0.028 | Tree loss: 0.782 | Accuracy: 0.738000 | 0.518 sec/iter\n",
      "Epoch: 280 | Batch: 010 / 011 | Total loss: 0.754 | Reg loss: 0.028 | Tree loss: 0.754 | Accuracy: 0.720137 | 0.518 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 281 | Batch: 000 / 011 | Total loss: 0.874 | Reg loss: 0.028 | Tree loss: 0.874 | Accuracy: 0.661000 | 0.519 sec/iter\n",
      "Epoch: 281 | Batch: 001 / 011 | Total loss: 0.870 | Reg loss: 0.028 | Tree loss: 0.870 | Accuracy: 0.664500 | 0.519 sec/iter\n",
      "Epoch: 281 | Batch: 002 / 011 | Total loss: 0.835 | Reg loss: 0.028 | Tree loss: 0.835 | Accuracy: 0.711000 | 0.519 sec/iter\n",
      "Epoch: 281 | Batch: 003 / 011 | Total loss: 0.844 | Reg loss: 0.028 | Tree loss: 0.844 | Accuracy: 0.692500 | 0.519 sec/iter\n",
      "Epoch: 281 | Batch: 004 / 011 | Total loss: 0.809 | Reg loss: 0.028 | Tree loss: 0.809 | Accuracy: 0.716000 | 0.519 sec/iter\n",
      "Epoch: 281 | Batch: 005 / 011 | Total loss: 0.795 | Reg loss: 0.028 | Tree loss: 0.795 | Accuracy: 0.725500 | 0.519 sec/iter\n",
      "Epoch: 281 | Batch: 006 / 011 | Total loss: 0.797 | Reg loss: 0.028 | Tree loss: 0.797 | Accuracy: 0.732500 | 0.519 sec/iter\n",
      "Epoch: 281 | Batch: 007 / 011 | Total loss: 0.789 | Reg loss: 0.028 | Tree loss: 0.789 | Accuracy: 0.737500 | 0.519 sec/iter\n",
      "Epoch: 281 | Batch: 008 / 011 | Total loss: 0.780 | Reg loss: 0.028 | Tree loss: 0.780 | Accuracy: 0.731500 | 0.518 sec/iter\n",
      "Epoch: 281 | Batch: 009 / 011 | Total loss: 0.794 | Reg loss: 0.028 | Tree loss: 0.794 | Accuracy: 0.733500 | 0.518 sec/iter\n",
      "Epoch: 281 | Batch: 010 / 011 | Total loss: 0.881 | Reg loss: 0.028 | Tree loss: 0.881 | Accuracy: 0.706485 | 0.518 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 282 | Batch: 000 / 011 | Total loss: 0.870 | Reg loss: 0.028 | Tree loss: 0.870 | Accuracy: 0.684000 | 0.519 sec/iter\n",
      "Epoch: 282 | Batch: 001 / 011 | Total loss: 0.882 | Reg loss: 0.028 | Tree loss: 0.882 | Accuracy: 0.648000 | 0.519 sec/iter\n",
      "Epoch: 282 | Batch: 002 / 011 | Total loss: 0.874 | Reg loss: 0.028 | Tree loss: 0.874 | Accuracy: 0.665500 | 0.519 sec/iter\n",
      "Epoch: 282 | Batch: 003 / 011 | Total loss: 0.831 | Reg loss: 0.028 | Tree loss: 0.831 | Accuracy: 0.697000 | 0.519 sec/iter\n",
      "Epoch: 282 | Batch: 004 / 011 | Total loss: 0.814 | Reg loss: 0.028 | Tree loss: 0.814 | Accuracy: 0.718500 | 0.519 sec/iter\n",
      "Epoch: 282 | Batch: 005 / 011 | Total loss: 0.794 | Reg loss: 0.028 | Tree loss: 0.794 | Accuracy: 0.729000 | 0.518 sec/iter\n",
      "Epoch: 282 | Batch: 006 / 011 | Total loss: 0.797 | Reg loss: 0.028 | Tree loss: 0.797 | Accuracy: 0.711500 | 0.518 sec/iter\n",
      "Epoch: 282 | Batch: 007 / 011 | Total loss: 0.795 | Reg loss: 0.028 | Tree loss: 0.795 | Accuracy: 0.711000 | 0.518 sec/iter\n",
      "Epoch: 282 | Batch: 008 / 011 | Total loss: 0.789 | Reg loss: 0.028 | Tree loss: 0.789 | Accuracy: 0.734500 | 0.518 sec/iter\n",
      "Epoch: 282 | Batch: 009 / 011 | Total loss: 0.770 | Reg loss: 0.028 | Tree loss: 0.770 | Accuracy: 0.746000 | 0.518 sec/iter\n",
      "Epoch: 282 | Batch: 010 / 011 | Total loss: 0.796 | Reg loss: 0.028 | Tree loss: 0.796 | Accuracy: 0.761092 | 0.518 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 283 | Batch: 000 / 011 | Total loss: 0.871 | Reg loss: 0.028 | Tree loss: 0.871 | Accuracy: 0.664000 | 0.519 sec/iter\n",
      "Epoch: 283 | Batch: 001 / 011 | Total loss: 0.864 | Reg loss: 0.028 | Tree loss: 0.864 | Accuracy: 0.664000 | 0.519 sec/iter\n",
      "Epoch: 283 | Batch: 002 / 011 | Total loss: 0.837 | Reg loss: 0.028 | Tree loss: 0.837 | Accuracy: 0.678500 | 0.519 sec/iter\n",
      "Epoch: 283 | Batch: 003 / 011 | Total loss: 0.825 | Reg loss: 0.028 | Tree loss: 0.825 | Accuracy: 0.707000 | 0.519 sec/iter\n",
      "Epoch: 283 | Batch: 004 / 011 | Total loss: 0.824 | Reg loss: 0.028 | Tree loss: 0.824 | Accuracy: 0.694000 | 0.519 sec/iter\n",
      "Epoch: 283 | Batch: 005 / 011 | Total loss: 0.781 | Reg loss: 0.028 | Tree loss: 0.781 | Accuracy: 0.725500 | 0.519 sec/iter\n",
      "Epoch: 283 | Batch: 006 / 011 | Total loss: 0.793 | Reg loss: 0.028 | Tree loss: 0.793 | Accuracy: 0.732000 | 0.518 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 283 | Batch: 007 / 011 | Total loss: 0.825 | Reg loss: 0.028 | Tree loss: 0.825 | Accuracy: 0.705000 | 0.518 sec/iter\n",
      "Epoch: 283 | Batch: 008 / 011 | Total loss: 0.792 | Reg loss: 0.028 | Tree loss: 0.792 | Accuracy: 0.740500 | 0.518 sec/iter\n",
      "Epoch: 283 | Batch: 009 / 011 | Total loss: 0.798 | Reg loss: 0.028 | Tree loss: 0.798 | Accuracy: 0.712500 | 0.518 sec/iter\n",
      "Epoch: 283 | Batch: 010 / 011 | Total loss: 0.806 | Reg loss: 0.028 | Tree loss: 0.806 | Accuracy: 0.730375 | 0.518 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 284 | Batch: 000 / 011 | Total loss: 0.897 | Reg loss: 0.028 | Tree loss: 0.897 | Accuracy: 0.643000 | 0.52 sec/iter\n",
      "Epoch: 284 | Batch: 001 / 011 | Total loss: 0.863 | Reg loss: 0.028 | Tree loss: 0.863 | Accuracy: 0.677000 | 0.519 sec/iter\n",
      "Epoch: 284 | Batch: 002 / 011 | Total loss: 0.857 | Reg loss: 0.028 | Tree loss: 0.857 | Accuracy: 0.672000 | 0.519 sec/iter\n",
      "Epoch: 284 | Batch: 003 / 011 | Total loss: 0.828 | Reg loss: 0.028 | Tree loss: 0.828 | Accuracy: 0.694000 | 0.519 sec/iter\n",
      "Epoch: 284 | Batch: 004 / 011 | Total loss: 0.800 | Reg loss: 0.028 | Tree loss: 0.800 | Accuracy: 0.730000 | 0.519 sec/iter\n",
      "Epoch: 284 | Batch: 005 / 011 | Total loss: 0.809 | Reg loss: 0.028 | Tree loss: 0.809 | Accuracy: 0.725000 | 0.519 sec/iter\n",
      "Epoch: 284 | Batch: 006 / 011 | Total loss: 0.788 | Reg loss: 0.028 | Tree loss: 0.788 | Accuracy: 0.727500 | 0.519 sec/iter\n",
      "Epoch: 284 | Batch: 007 / 011 | Total loss: 0.791 | Reg loss: 0.028 | Tree loss: 0.791 | Accuracy: 0.735500 | 0.519 sec/iter\n",
      "Epoch: 284 | Batch: 008 / 011 | Total loss: 0.770 | Reg loss: 0.028 | Tree loss: 0.770 | Accuracy: 0.744000 | 0.519 sec/iter\n",
      "Epoch: 284 | Batch: 009 / 011 | Total loss: 0.793 | Reg loss: 0.028 | Tree loss: 0.793 | Accuracy: 0.737000 | 0.519 sec/iter\n",
      "Epoch: 284 | Batch: 010 / 011 | Total loss: 0.809 | Reg loss: 0.028 | Tree loss: 0.809 | Accuracy: 0.699659 | 0.519 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 285 | Batch: 000 / 011 | Total loss: 0.900 | Reg loss: 0.028 | Tree loss: 0.900 | Accuracy: 0.655000 | 0.519 sec/iter\n",
      "Epoch: 285 | Batch: 001 / 011 | Total loss: 0.868 | Reg loss: 0.028 | Tree loss: 0.868 | Accuracy: 0.682000 | 0.519 sec/iter\n",
      "Epoch: 285 | Batch: 002 / 011 | Total loss: 0.852 | Reg loss: 0.028 | Tree loss: 0.852 | Accuracy: 0.682000 | 0.519 sec/iter\n",
      "Epoch: 285 | Batch: 003 / 011 | Total loss: 0.800 | Reg loss: 0.028 | Tree loss: 0.800 | Accuracy: 0.723500 | 0.518 sec/iter\n",
      "Epoch: 285 | Batch: 004 / 011 | Total loss: 0.797 | Reg loss: 0.028 | Tree loss: 0.797 | Accuracy: 0.722000 | 0.518 sec/iter\n",
      "Epoch: 285 | Batch: 005 / 011 | Total loss: 0.797 | Reg loss: 0.028 | Tree loss: 0.797 | Accuracy: 0.707000 | 0.518 sec/iter\n",
      "Epoch: 285 | Batch: 006 / 011 | Total loss: 0.804 | Reg loss: 0.028 | Tree loss: 0.804 | Accuracy: 0.715000 | 0.518 sec/iter\n",
      "Epoch: 285 | Batch: 007 / 011 | Total loss: 0.799 | Reg loss: 0.028 | Tree loss: 0.799 | Accuracy: 0.732500 | 0.518 sec/iter\n",
      "Epoch: 285 | Batch: 008 / 011 | Total loss: 0.803 | Reg loss: 0.028 | Tree loss: 0.803 | Accuracy: 0.715000 | 0.518 sec/iter\n",
      "Epoch: 285 | Batch: 009 / 011 | Total loss: 0.792 | Reg loss: 0.028 | Tree loss: 0.792 | Accuracy: 0.739500 | 0.518 sec/iter\n",
      "Epoch: 285 | Batch: 010 / 011 | Total loss: 0.804 | Reg loss: 0.028 | Tree loss: 0.804 | Accuracy: 0.747440 | 0.518 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 286 | Batch: 000 / 011 | Total loss: 0.875 | Reg loss: 0.028 | Tree loss: 0.875 | Accuracy: 0.682000 | 0.519 sec/iter\n",
      "Epoch: 286 | Batch: 001 / 011 | Total loss: 0.860 | Reg loss: 0.028 | Tree loss: 0.860 | Accuracy: 0.670500 | 0.519 sec/iter\n",
      "Epoch: 286 | Batch: 002 / 011 | Total loss: 0.844 | Reg loss: 0.028 | Tree loss: 0.844 | Accuracy: 0.686000 | 0.518 sec/iter\n",
      "Epoch: 286 | Batch: 003 / 011 | Total loss: 0.842 | Reg loss: 0.028 | Tree loss: 0.842 | Accuracy: 0.695500 | 0.518 sec/iter\n",
      "Epoch: 286 | Batch: 004 / 011 | Total loss: 0.813 | Reg loss: 0.028 | Tree loss: 0.813 | Accuracy: 0.721000 | 0.518 sec/iter\n",
      "Epoch: 286 | Batch: 005 / 011 | Total loss: 0.810 | Reg loss: 0.028 | Tree loss: 0.810 | Accuracy: 0.702000 | 0.518 sec/iter\n",
      "Epoch: 286 | Batch: 006 / 011 | Total loss: 0.773 | Reg loss: 0.028 | Tree loss: 0.773 | Accuracy: 0.740500 | 0.518 sec/iter\n",
      "Epoch: 286 | Batch: 007 / 011 | Total loss: 0.796 | Reg loss: 0.028 | Tree loss: 0.796 | Accuracy: 0.723500 | 0.518 sec/iter\n",
      "Epoch: 286 | Batch: 008 / 011 | Total loss: 0.805 | Reg loss: 0.028 | Tree loss: 0.805 | Accuracy: 0.721000 | 0.518 sec/iter\n",
      "Epoch: 286 | Batch: 009 / 011 | Total loss: 0.788 | Reg loss: 0.028 | Tree loss: 0.788 | Accuracy: 0.730000 | 0.518 sec/iter\n",
      "Epoch: 286 | Batch: 010 / 011 | Total loss: 0.827 | Reg loss: 0.028 | Tree loss: 0.827 | Accuracy: 0.689420 | 0.518 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 287 | Batch: 000 / 011 | Total loss: 0.882 | Reg loss: 0.028 | Tree loss: 0.882 | Accuracy: 0.671500 | 0.518 sec/iter\n",
      "Epoch: 287 | Batch: 001 / 011 | Total loss: 0.872 | Reg loss: 0.028 | Tree loss: 0.872 | Accuracy: 0.656000 | 0.518 sec/iter\n",
      "Epoch: 287 | Batch: 002 / 011 | Total loss: 0.830 | Reg loss: 0.028 | Tree loss: 0.830 | Accuracy: 0.693000 | 0.518 sec/iter\n",
      "Epoch: 287 | Batch: 003 / 011 | Total loss: 0.827 | Reg loss: 0.028 | Tree loss: 0.827 | Accuracy: 0.711500 | 0.518 sec/iter\n",
      "Epoch: 287 | Batch: 004 / 011 | Total loss: 0.809 | Reg loss: 0.028 | Tree loss: 0.809 | Accuracy: 0.704000 | 0.518 sec/iter\n",
      "Epoch: 287 | Batch: 005 / 011 | Total loss: 0.798 | Reg loss: 0.028 | Tree loss: 0.798 | Accuracy: 0.714000 | 0.518 sec/iter\n",
      "Epoch: 287 | Batch: 006 / 011 | Total loss: 0.780 | Reg loss: 0.028 | Tree loss: 0.780 | Accuracy: 0.728500 | 0.518 sec/iter\n",
      "Epoch: 287 | Batch: 007 / 011 | Total loss: 0.797 | Reg loss: 0.028 | Tree loss: 0.797 | Accuracy: 0.713000 | 0.518 sec/iter\n",
      "Epoch: 287 | Batch: 008 / 011 | Total loss: 0.799 | Reg loss: 0.028 | Tree loss: 0.799 | Accuracy: 0.723500 | 0.518 sec/iter\n",
      "Epoch: 287 | Batch: 009 / 011 | Total loss: 0.820 | Reg loss: 0.028 | Tree loss: 0.820 | Accuracy: 0.696000 | 0.518 sec/iter\n",
      "Epoch: 287 | Batch: 010 / 011 | Total loss: 0.851 | Reg loss: 0.028 | Tree loss: 0.851 | Accuracy: 0.689420 | 0.518 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 288 | Batch: 000 / 011 | Total loss: 0.875 | Reg loss: 0.028 | Tree loss: 0.875 | Accuracy: 0.667000 | 0.518 sec/iter\n",
      "Epoch: 288 | Batch: 001 / 011 | Total loss: 0.865 | Reg loss: 0.028 | Tree loss: 0.865 | Accuracy: 0.666000 | 0.518 sec/iter\n",
      "Epoch: 288 | Batch: 002 / 011 | Total loss: 0.873 | Reg loss: 0.028 | Tree loss: 0.873 | Accuracy: 0.676500 | 0.518 sec/iter\n",
      "Epoch: 288 | Batch: 003 / 011 | Total loss: 0.810 | Reg loss: 0.028 | Tree loss: 0.810 | Accuracy: 0.713000 | 0.517 sec/iter\n",
      "Epoch: 288 | Batch: 004 / 011 | Total loss: 0.816 | Reg loss: 0.028 | Tree loss: 0.816 | Accuracy: 0.723000 | 0.517 sec/iter\n",
      "Epoch: 288 | Batch: 005 / 011 | Total loss: 0.824 | Reg loss: 0.028 | Tree loss: 0.824 | Accuracy: 0.692500 | 0.517 sec/iter\n",
      "Epoch: 288 | Batch: 006 / 011 | Total loss: 0.781 | Reg loss: 0.028 | Tree loss: 0.781 | Accuracy: 0.733500 | 0.517 sec/iter\n",
      "Epoch: 288 | Batch: 007 / 011 | Total loss: 0.792 | Reg loss: 0.028 | Tree loss: 0.792 | Accuracy: 0.730000 | 0.517 sec/iter\n",
      "Epoch: 288 | Batch: 008 / 011 | Total loss: 0.806 | Reg loss: 0.028 | Tree loss: 0.806 | Accuracy: 0.713500 | 0.517 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 288 | Batch: 009 / 011 | Total loss: 0.792 | Reg loss: 0.028 | Tree loss: 0.792 | Accuracy: 0.738000 | 0.517 sec/iter\n",
      "Epoch: 288 | Batch: 010 / 011 | Total loss: 0.767 | Reg loss: 0.028 | Tree loss: 0.767 | Accuracy: 0.723549 | 0.517 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 289 | Batch: 000 / 011 | Total loss: 0.882 | Reg loss: 0.028 | Tree loss: 0.882 | Accuracy: 0.665500 | 0.517 sec/iter\n",
      "Epoch: 289 | Batch: 001 / 011 | Total loss: 0.862 | Reg loss: 0.028 | Tree loss: 0.862 | Accuracy: 0.659500 | 0.517 sec/iter\n",
      "Epoch: 289 | Batch: 002 / 011 | Total loss: 0.854 | Reg loss: 0.028 | Tree loss: 0.854 | Accuracy: 0.679500 | 0.517 sec/iter\n",
      "Epoch: 289 | Batch: 003 / 011 | Total loss: 0.835 | Reg loss: 0.028 | Tree loss: 0.835 | Accuracy: 0.696500 | 0.517 sec/iter\n",
      "Epoch: 289 | Batch: 004 / 011 | Total loss: 0.810 | Reg loss: 0.028 | Tree loss: 0.810 | Accuracy: 0.713000 | 0.517 sec/iter\n",
      "Epoch: 289 | Batch: 005 / 011 | Total loss: 0.797 | Reg loss: 0.028 | Tree loss: 0.797 | Accuracy: 0.707000 | 0.517 sec/iter\n",
      "Epoch: 289 | Batch: 006 / 011 | Total loss: 0.822 | Reg loss: 0.028 | Tree loss: 0.822 | Accuracy: 0.701500 | 0.517 sec/iter\n",
      "Epoch: 289 | Batch: 007 / 011 | Total loss: 0.789 | Reg loss: 0.028 | Tree loss: 0.789 | Accuracy: 0.725500 | 0.517 sec/iter\n",
      "Epoch: 289 | Batch: 008 / 011 | Total loss: 0.787 | Reg loss: 0.028 | Tree loss: 0.787 | Accuracy: 0.722000 | 0.517 sec/iter\n",
      "Epoch: 289 | Batch: 009 / 011 | Total loss: 0.779 | Reg loss: 0.028 | Tree loss: 0.779 | Accuracy: 0.735000 | 0.517 sec/iter\n",
      "Epoch: 289 | Batch: 010 / 011 | Total loss: 0.761 | Reg loss: 0.028 | Tree loss: 0.761 | Accuracy: 0.757679 | 0.517 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 290 | Batch: 000 / 011 | Total loss: 0.881 | Reg loss: 0.028 | Tree loss: 0.881 | Accuracy: 0.669000 | 0.518 sec/iter\n",
      "Epoch: 290 | Batch: 001 / 011 | Total loss: 0.856 | Reg loss: 0.028 | Tree loss: 0.856 | Accuracy: 0.668500 | 0.518 sec/iter\n",
      "Epoch: 290 | Batch: 002 / 011 | Total loss: 0.855 | Reg loss: 0.028 | Tree loss: 0.855 | Accuracy: 0.679000 | 0.518 sec/iter\n",
      "Epoch: 290 | Batch: 003 / 011 | Total loss: 0.821 | Reg loss: 0.028 | Tree loss: 0.821 | Accuracy: 0.696500 | 0.518 sec/iter\n",
      "Epoch: 290 | Batch: 004 / 011 | Total loss: 0.807 | Reg loss: 0.028 | Tree loss: 0.807 | Accuracy: 0.720000 | 0.518 sec/iter\n",
      "Epoch: 290 | Batch: 005 / 011 | Total loss: 0.806 | Reg loss: 0.028 | Tree loss: 0.806 | Accuracy: 0.710500 | 0.518 sec/iter\n",
      "Epoch: 290 | Batch: 006 / 011 | Total loss: 0.794 | Reg loss: 0.028 | Tree loss: 0.794 | Accuracy: 0.737000 | 0.518 sec/iter\n",
      "Epoch: 290 | Batch: 007 / 011 | Total loss: 0.780 | Reg loss: 0.028 | Tree loss: 0.780 | Accuracy: 0.742000 | 0.517 sec/iter\n",
      "Epoch: 290 | Batch: 008 / 011 | Total loss: 0.798 | Reg loss: 0.028 | Tree loss: 0.798 | Accuracy: 0.732500 | 0.517 sec/iter\n",
      "Epoch: 290 | Batch: 009 / 011 | Total loss: 0.789 | Reg loss: 0.028 | Tree loss: 0.789 | Accuracy: 0.729000 | 0.517 sec/iter\n",
      "Epoch: 290 | Batch: 010 / 011 | Total loss: 0.855 | Reg loss: 0.028 | Tree loss: 0.855 | Accuracy: 0.716724 | 0.517 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 291 | Batch: 000 / 011 | Total loss: 0.873 | Reg loss: 0.028 | Tree loss: 0.873 | Accuracy: 0.679000 | 0.517 sec/iter\n",
      "Epoch: 291 | Batch: 001 / 011 | Total loss: 0.881 | Reg loss: 0.028 | Tree loss: 0.881 | Accuracy: 0.651500 | 0.517 sec/iter\n",
      "Epoch: 291 | Batch: 002 / 011 | Total loss: 0.839 | Reg loss: 0.028 | Tree loss: 0.839 | Accuracy: 0.686000 | 0.517 sec/iter\n",
      "Epoch: 291 | Batch: 003 / 011 | Total loss: 0.831 | Reg loss: 0.028 | Tree loss: 0.831 | Accuracy: 0.693000 | 0.517 sec/iter\n",
      "Epoch: 291 | Batch: 004 / 011 | Total loss: 0.816 | Reg loss: 0.028 | Tree loss: 0.816 | Accuracy: 0.708000 | 0.517 sec/iter\n",
      "Epoch: 291 | Batch: 005 / 011 | Total loss: 0.813 | Reg loss: 0.028 | Tree loss: 0.813 | Accuracy: 0.718000 | 0.517 sec/iter\n",
      "Epoch: 291 | Batch: 006 / 011 | Total loss: 0.798 | Reg loss: 0.028 | Tree loss: 0.798 | Accuracy: 0.724000 | 0.517 sec/iter\n",
      "Epoch: 291 | Batch: 007 / 011 | Total loss: 0.800 | Reg loss: 0.028 | Tree loss: 0.800 | Accuracy: 0.710000 | 0.517 sec/iter\n",
      "Epoch: 291 | Batch: 008 / 011 | Total loss: 0.769 | Reg loss: 0.028 | Tree loss: 0.769 | Accuracy: 0.720000 | 0.517 sec/iter\n",
      "Epoch: 291 | Batch: 009 / 011 | Total loss: 0.804 | Reg loss: 0.028 | Tree loss: 0.804 | Accuracy: 0.724500 | 0.516 sec/iter\n",
      "Epoch: 291 | Batch: 010 / 011 | Total loss: 0.771 | Reg loss: 0.028 | Tree loss: 0.771 | Accuracy: 0.709898 | 0.516 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 292 | Batch: 000 / 011 | Total loss: 0.894 | Reg loss: 0.028 | Tree loss: 0.894 | Accuracy: 0.644000 | 0.517 sec/iter\n",
      "Epoch: 292 | Batch: 001 / 011 | Total loss: 0.865 | Reg loss: 0.028 | Tree loss: 0.865 | Accuracy: 0.676000 | 0.517 sec/iter\n",
      "Epoch: 292 | Batch: 002 / 011 | Total loss: 0.860 | Reg loss: 0.028 | Tree loss: 0.860 | Accuracy: 0.666000 | 0.517 sec/iter\n",
      "Epoch: 292 | Batch: 003 / 011 | Total loss: 0.834 | Reg loss: 0.028 | Tree loss: 0.834 | Accuracy: 0.699000 | 0.517 sec/iter\n",
      "Epoch: 292 | Batch: 004 / 011 | Total loss: 0.791 | Reg loss: 0.028 | Tree loss: 0.791 | Accuracy: 0.731500 | 0.517 sec/iter\n",
      "Epoch: 292 | Batch: 005 / 011 | Total loss: 0.783 | Reg loss: 0.028 | Tree loss: 0.783 | Accuracy: 0.728000 | 0.517 sec/iter\n",
      "Epoch: 292 | Batch: 006 / 011 | Total loss: 0.785 | Reg loss: 0.028 | Tree loss: 0.785 | Accuracy: 0.737000 | 0.516 sec/iter\n",
      "Epoch: 292 | Batch: 007 / 011 | Total loss: 0.789 | Reg loss: 0.028 | Tree loss: 0.789 | Accuracy: 0.739500 | 0.516 sec/iter\n",
      "Epoch: 292 | Batch: 008 / 011 | Total loss: 0.801 | Reg loss: 0.028 | Tree loss: 0.801 | Accuracy: 0.724500 | 0.516 sec/iter\n",
      "Epoch: 292 | Batch: 009 / 011 | Total loss: 0.787 | Reg loss: 0.028 | Tree loss: 0.787 | Accuracy: 0.745000 | 0.516 sec/iter\n",
      "Epoch: 292 | Batch: 010 / 011 | Total loss: 0.818 | Reg loss: 0.028 | Tree loss: 0.818 | Accuracy: 0.737201 | 0.516 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 293 | Batch: 000 / 011 | Total loss: 0.891 | Reg loss: 0.028 | Tree loss: 0.891 | Accuracy: 0.666500 | 0.517 sec/iter\n",
      "Epoch: 293 | Batch: 001 / 011 | Total loss: 0.854 | Reg loss: 0.028 | Tree loss: 0.854 | Accuracy: 0.669500 | 0.517 sec/iter\n",
      "Epoch: 293 | Batch: 002 / 011 | Total loss: 0.877 | Reg loss: 0.028 | Tree loss: 0.877 | Accuracy: 0.668000 | 0.517 sec/iter\n",
      "Epoch: 293 | Batch: 003 / 011 | Total loss: 0.835 | Reg loss: 0.028 | Tree loss: 0.835 | Accuracy: 0.703000 | 0.517 sec/iter\n",
      "Epoch: 293 | Batch: 004 / 011 | Total loss: 0.810 | Reg loss: 0.028 | Tree loss: 0.810 | Accuracy: 0.717500 | 0.517 sec/iter\n",
      "Epoch: 293 | Batch: 005 / 011 | Total loss: 0.808 | Reg loss: 0.028 | Tree loss: 0.808 | Accuracy: 0.717500 | 0.517 sec/iter\n",
      "Epoch: 293 | Batch: 006 / 011 | Total loss: 0.818 | Reg loss: 0.028 | Tree loss: 0.818 | Accuracy: 0.709500 | 0.516 sec/iter\n",
      "Epoch: 293 | Batch: 007 / 011 | Total loss: 0.768 | Reg loss: 0.028 | Tree loss: 0.768 | Accuracy: 0.732000 | 0.516 sec/iter\n",
      "Epoch: 293 | Batch: 008 / 011 | Total loss: 0.774 | Reg loss: 0.028 | Tree loss: 0.774 | Accuracy: 0.733000 | 0.516 sec/iter\n",
      "Epoch: 293 | Batch: 009 / 011 | Total loss: 0.780 | Reg loss: 0.028 | Tree loss: 0.780 | Accuracy: 0.723500 | 0.516 sec/iter\n",
      "Epoch: 293 | Batch: 010 / 011 | Total loss: 0.818 | Reg loss: 0.028 | Tree loss: 0.818 | Accuracy: 0.692833 | 0.516 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 6: 0.9723756906077349\n",
      "Epoch: 294 | Batch: 000 / 011 | Total loss: 0.907 | Reg loss: 0.028 | Tree loss: 0.907 | Accuracy: 0.639000 | 0.516 sec/iter\n",
      "Epoch: 294 | Batch: 001 / 011 | Total loss: 0.865 | Reg loss: 0.028 | Tree loss: 0.865 | Accuracy: 0.671000 | 0.516 sec/iter\n",
      "Epoch: 294 | Batch: 002 / 011 | Total loss: 0.852 | Reg loss: 0.028 | Tree loss: 0.852 | Accuracy: 0.674500 | 0.516 sec/iter\n",
      "Epoch: 294 | Batch: 003 / 011 | Total loss: 0.853 | Reg loss: 0.028 | Tree loss: 0.853 | Accuracy: 0.694000 | 0.516 sec/iter\n",
      "Epoch: 294 | Batch: 004 / 011 | Total loss: 0.800 | Reg loss: 0.028 | Tree loss: 0.800 | Accuracy: 0.721000 | 0.516 sec/iter\n",
      "Epoch: 294 | Batch: 005 / 011 | Total loss: 0.794 | Reg loss: 0.028 | Tree loss: 0.794 | Accuracy: 0.715500 | 0.516 sec/iter\n",
      "Epoch: 294 | Batch: 006 / 011 | Total loss: 0.783 | Reg loss: 0.028 | Tree loss: 0.783 | Accuracy: 0.743000 | 0.515 sec/iter\n",
      "Epoch: 294 | Batch: 007 / 011 | Total loss: 0.783 | Reg loss: 0.028 | Tree loss: 0.783 | Accuracy: 0.747500 | 0.515 sec/iter\n",
      "Epoch: 294 | Batch: 008 / 011 | Total loss: 0.793 | Reg loss: 0.028 | Tree loss: 0.793 | Accuracy: 0.729500 | 0.515 sec/iter\n",
      "Epoch: 294 | Batch: 009 / 011 | Total loss: 0.802 | Reg loss: 0.028 | Tree loss: 0.802 | Accuracy: 0.717500 | 0.515 sec/iter\n",
      "Epoch: 294 | Batch: 010 / 011 | Total loss: 0.744 | Reg loss: 0.028 | Tree loss: 0.744 | Accuracy: 0.784983 | 0.515 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 295 | Batch: 000 / 011 | Total loss: 0.870 | Reg loss: 0.028 | Tree loss: 0.870 | Accuracy: 0.658000 | 0.516 sec/iter\n",
      "Epoch: 295 | Batch: 001 / 011 | Total loss: 0.854 | Reg loss: 0.028 | Tree loss: 0.854 | Accuracy: 0.671500 | 0.516 sec/iter\n",
      "Epoch: 295 | Batch: 002 / 011 | Total loss: 0.851 | Reg loss: 0.028 | Tree loss: 0.851 | Accuracy: 0.684000 | 0.516 sec/iter\n",
      "Epoch: 295 | Batch: 003 / 011 | Total loss: 0.822 | Reg loss: 0.028 | Tree loss: 0.822 | Accuracy: 0.697500 | 0.515 sec/iter\n",
      "Epoch: 295 | Batch: 004 / 011 | Total loss: 0.812 | Reg loss: 0.028 | Tree loss: 0.812 | Accuracy: 0.714500 | 0.515 sec/iter\n",
      "Epoch: 295 | Batch: 005 / 011 | Total loss: 0.799 | Reg loss: 0.028 | Tree loss: 0.799 | Accuracy: 0.714000 | 0.515 sec/iter\n",
      "Epoch: 295 | Batch: 006 / 011 | Total loss: 0.795 | Reg loss: 0.028 | Tree loss: 0.795 | Accuracy: 0.713500 | 0.515 sec/iter\n",
      "Epoch: 295 | Batch: 007 / 011 | Total loss: 0.802 | Reg loss: 0.028 | Tree loss: 0.802 | Accuracy: 0.723500 | 0.515 sec/iter\n",
      "Epoch: 295 | Batch: 008 / 011 | Total loss: 0.790 | Reg loss: 0.028 | Tree loss: 0.790 | Accuracy: 0.730500 | 0.515 sec/iter\n",
      "Epoch: 295 | Batch: 009 / 011 | Total loss: 0.807 | Reg loss: 0.028 | Tree loss: 0.807 | Accuracy: 0.717500 | 0.515 sec/iter\n",
      "Epoch: 295 | Batch: 010 / 011 | Total loss: 0.782 | Reg loss: 0.028 | Tree loss: 0.782 | Accuracy: 0.754266 | 0.515 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 296 | Batch: 000 / 011 | Total loss: 0.875 | Reg loss: 0.028 | Tree loss: 0.875 | Accuracy: 0.662000 | 0.516 sec/iter\n",
      "Epoch: 296 | Batch: 001 / 011 | Total loss: 0.846 | Reg loss: 0.028 | Tree loss: 0.846 | Accuracy: 0.681500 | 0.516 sec/iter\n",
      "Epoch: 296 | Batch: 002 / 011 | Total loss: 0.848 | Reg loss: 0.028 | Tree loss: 0.848 | Accuracy: 0.687000 | 0.516 sec/iter\n",
      "Epoch: 296 | Batch: 003 / 011 | Total loss: 0.818 | Reg loss: 0.028 | Tree loss: 0.818 | Accuracy: 0.710500 | 0.516 sec/iter\n",
      "Epoch: 296 | Batch: 004 / 011 | Total loss: 0.812 | Reg loss: 0.028 | Tree loss: 0.812 | Accuracy: 0.705000 | 0.516 sec/iter\n",
      "Epoch: 296 | Batch: 005 / 011 | Total loss: 0.822 | Reg loss: 0.028 | Tree loss: 0.822 | Accuracy: 0.723500 | 0.516 sec/iter\n",
      "Epoch: 296 | Batch: 006 / 011 | Total loss: 0.813 | Reg loss: 0.028 | Tree loss: 0.813 | Accuracy: 0.712000 | 0.516 sec/iter\n",
      "Epoch: 296 | Batch: 007 / 011 | Total loss: 0.784 | Reg loss: 0.028 | Tree loss: 0.784 | Accuracy: 0.739000 | 0.516 sec/iter\n",
      "Epoch: 296 | Batch: 008 / 011 | Total loss: 0.779 | Reg loss: 0.028 | Tree loss: 0.779 | Accuracy: 0.739500 | 0.516 sec/iter\n",
      "Epoch: 296 | Batch: 009 / 011 | Total loss: 0.808 | Reg loss: 0.028 | Tree loss: 0.808 | Accuracy: 0.708500 | 0.516 sec/iter\n",
      "Epoch: 296 | Batch: 010 / 011 | Total loss: 0.811 | Reg loss: 0.028 | Tree loss: 0.811 | Accuracy: 0.713311 | 0.515 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 297 | Batch: 000 / 011 | Total loss: 0.846 | Reg loss: 0.028 | Tree loss: 0.846 | Accuracy: 0.693500 | 0.516 sec/iter\n",
      "Epoch: 297 | Batch: 001 / 011 | Total loss: 0.860 | Reg loss: 0.028 | Tree loss: 0.860 | Accuracy: 0.671500 | 0.515 sec/iter\n",
      "Epoch: 297 | Batch: 002 / 011 | Total loss: 0.851 | Reg loss: 0.028 | Tree loss: 0.851 | Accuracy: 0.678500 | 0.515 sec/iter\n",
      "Epoch: 297 | Batch: 003 / 011 | Total loss: 0.827 | Reg loss: 0.028 | Tree loss: 0.827 | Accuracy: 0.702500 | 0.515 sec/iter\n",
      "Epoch: 297 | Batch: 004 / 011 | Total loss: 0.844 | Reg loss: 0.028 | Tree loss: 0.844 | Accuracy: 0.705500 | 0.515 sec/iter\n",
      "Epoch: 297 | Batch: 005 / 011 | Total loss: 0.794 | Reg loss: 0.028 | Tree loss: 0.794 | Accuracy: 0.710500 | 0.515 sec/iter\n",
      "Epoch: 297 | Batch: 006 / 011 | Total loss: 0.806 | Reg loss: 0.028 | Tree loss: 0.806 | Accuracy: 0.703500 | 0.515 sec/iter\n",
      "Epoch: 297 | Batch: 007 / 011 | Total loss: 0.780 | Reg loss: 0.028 | Tree loss: 0.780 | Accuracy: 0.739000 | 0.515 sec/iter\n",
      "Epoch: 297 | Batch: 008 / 011 | Total loss: 0.791 | Reg loss: 0.028 | Tree loss: 0.791 | Accuracy: 0.724000 | 0.515 sec/iter\n",
      "Epoch: 297 | Batch: 009 / 011 | Total loss: 0.812 | Reg loss: 0.028 | Tree loss: 0.812 | Accuracy: 0.727500 | 0.515 sec/iter\n",
      "Epoch: 297 | Batch: 010 / 011 | Total loss: 0.724 | Reg loss: 0.028 | Tree loss: 0.724 | Accuracy: 0.747440 | 0.515 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 298 | Batch: 000 / 011 | Total loss: 0.878 | Reg loss: 0.028 | Tree loss: 0.878 | Accuracy: 0.660000 | 0.516 sec/iter\n",
      "Epoch: 298 | Batch: 001 / 011 | Total loss: 0.855 | Reg loss: 0.028 | Tree loss: 0.855 | Accuracy: 0.672500 | 0.516 sec/iter\n",
      "Epoch: 298 | Batch: 002 / 011 | Total loss: 0.867 | Reg loss: 0.028 | Tree loss: 0.867 | Accuracy: 0.671500 | 0.516 sec/iter\n",
      "Epoch: 298 | Batch: 003 / 011 | Total loss: 0.821 | Reg loss: 0.028 | Tree loss: 0.821 | Accuracy: 0.698500 | 0.516 sec/iter\n",
      "Epoch: 298 | Batch: 004 / 011 | Total loss: 0.798 | Reg loss: 0.028 | Tree loss: 0.798 | Accuracy: 0.716000 | 0.516 sec/iter\n",
      "Epoch: 298 | Batch: 005 / 011 | Total loss: 0.826 | Reg loss: 0.028 | Tree loss: 0.826 | Accuracy: 0.704500 | 0.516 sec/iter\n",
      "Epoch: 298 | Batch: 006 / 011 | Total loss: 0.780 | Reg loss: 0.028 | Tree loss: 0.780 | Accuracy: 0.741500 | 0.516 sec/iter\n",
      "Epoch: 298 | Batch: 007 / 011 | Total loss: 0.776 | Reg loss: 0.028 | Tree loss: 0.776 | Accuracy: 0.731000 | 0.516 sec/iter\n",
      "Epoch: 298 | Batch: 008 / 011 | Total loss: 0.808 | Reg loss: 0.028 | Tree loss: 0.808 | Accuracy: 0.712500 | 0.515 sec/iter\n",
      "Epoch: 298 | Batch: 009 / 011 | Total loss: 0.783 | Reg loss: 0.028 | Tree loss: 0.783 | Accuracy: 0.741000 | 0.515 sec/iter\n",
      "Epoch: 298 | Batch: 010 / 011 | Total loss: 0.861 | Reg loss: 0.028 | Tree loss: 0.861 | Accuracy: 0.696246 | 0.515 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 299 | Batch: 000 / 011 | Total loss: 0.858 | Reg loss: 0.028 | Tree loss: 0.858 | Accuracy: 0.675500 | 0.517 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 299 | Batch: 001 / 011 | Total loss: 0.873 | Reg loss: 0.028 | Tree loss: 0.873 | Accuracy: 0.658000 | 0.517 sec/iter\n",
      "Epoch: 299 | Batch: 002 / 011 | Total loss: 0.862 | Reg loss: 0.028 | Tree loss: 0.862 | Accuracy: 0.674500 | 0.517 sec/iter\n",
      "Epoch: 299 | Batch: 003 / 011 | Total loss: 0.832 | Reg loss: 0.028 | Tree loss: 0.832 | Accuracy: 0.717000 | 0.516 sec/iter\n",
      "Epoch: 299 | Batch: 004 / 011 | Total loss: 0.849 | Reg loss: 0.028 | Tree loss: 0.849 | Accuracy: 0.678500 | 0.516 sec/iter\n",
      "Epoch: 299 | Batch: 005 / 011 | Total loss: 0.791 | Reg loss: 0.028 | Tree loss: 0.791 | Accuracy: 0.721500 | 0.516 sec/iter\n",
      "Epoch: 299 | Batch: 006 / 011 | Total loss: 0.775 | Reg loss: 0.028 | Tree loss: 0.775 | Accuracy: 0.734000 | 0.516 sec/iter\n",
      "Epoch: 299 | Batch: 007 / 011 | Total loss: 0.786 | Reg loss: 0.028 | Tree loss: 0.786 | Accuracy: 0.727500 | 0.516 sec/iter\n",
      "Epoch: 299 | Batch: 008 / 011 | Total loss: 0.792 | Reg loss: 0.028 | Tree loss: 0.792 | Accuracy: 0.727000 | 0.516 sec/iter\n",
      "Epoch: 299 | Batch: 009 / 011 | Total loss: 0.789 | Reg loss: 0.028 | Tree loss: 0.789 | Accuracy: 0.727000 | 0.516 sec/iter\n",
      "Epoch: 299 | Batch: 010 / 011 | Total loss: 0.847 | Reg loss: 0.028 | Tree loss: 0.847 | Accuracy: 0.689420 | 0.516 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 300 | Batch: 000 / 011 | Total loss: 0.879 | Reg loss: 0.028 | Tree loss: 0.879 | Accuracy: 0.659000 | 0.516 sec/iter\n",
      "Epoch: 300 | Batch: 001 / 011 | Total loss: 0.855 | Reg loss: 0.028 | Tree loss: 0.855 | Accuracy: 0.679500 | 0.516 sec/iter\n",
      "Epoch: 300 | Batch: 002 / 011 | Total loss: 0.851 | Reg loss: 0.028 | Tree loss: 0.851 | Accuracy: 0.677000 | 0.516 sec/iter\n",
      "Epoch: 300 | Batch: 003 / 011 | Total loss: 0.835 | Reg loss: 0.028 | Tree loss: 0.835 | Accuracy: 0.711500 | 0.516 sec/iter\n",
      "Epoch: 300 | Batch: 004 / 011 | Total loss: 0.825 | Reg loss: 0.028 | Tree loss: 0.825 | Accuracy: 0.708500 | 0.516 sec/iter\n",
      "Epoch: 300 | Batch: 005 / 011 | Total loss: 0.761 | Reg loss: 0.028 | Tree loss: 0.761 | Accuracy: 0.738000 | 0.516 sec/iter\n",
      "Epoch: 300 | Batch: 006 / 011 | Total loss: 0.786 | Reg loss: 0.028 | Tree loss: 0.786 | Accuracy: 0.731000 | 0.515 sec/iter\n",
      "Epoch: 300 | Batch: 007 / 011 | Total loss: 0.819 | Reg loss: 0.028 | Tree loss: 0.819 | Accuracy: 0.703500 | 0.515 sec/iter\n",
      "Epoch: 300 | Batch: 008 / 011 | Total loss: 0.791 | Reg loss: 0.028 | Tree loss: 0.791 | Accuracy: 0.720000 | 0.515 sec/iter\n",
      "Epoch: 300 | Batch: 009 / 011 | Total loss: 0.794 | Reg loss: 0.028 | Tree loss: 0.794 | Accuracy: 0.728500 | 0.515 sec/iter\n",
      "Epoch: 300 | Batch: 010 / 011 | Total loss: 0.846 | Reg loss: 0.028 | Tree loss: 0.846 | Accuracy: 0.686007 | 0.515 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 301 | Batch: 000 / 011 | Total loss: 0.875 | Reg loss: 0.028 | Tree loss: 0.875 | Accuracy: 0.672000 | 0.517 sec/iter\n",
      "Epoch: 301 | Batch: 001 / 011 | Total loss: 0.872 | Reg loss: 0.028 | Tree loss: 0.872 | Accuracy: 0.654500 | 0.517 sec/iter\n",
      "Epoch: 301 | Batch: 002 / 011 | Total loss: 0.869 | Reg loss: 0.028 | Tree loss: 0.869 | Accuracy: 0.665500 | 0.516 sec/iter\n",
      "Epoch: 301 | Batch: 003 / 011 | Total loss: 0.822 | Reg loss: 0.028 | Tree loss: 0.822 | Accuracy: 0.694500 | 0.516 sec/iter\n",
      "Epoch: 301 | Batch: 004 / 011 | Total loss: 0.808 | Reg loss: 0.028 | Tree loss: 0.808 | Accuracy: 0.714000 | 0.516 sec/iter\n",
      "Epoch: 301 | Batch: 005 / 011 | Total loss: 0.791 | Reg loss: 0.028 | Tree loss: 0.791 | Accuracy: 0.717500 | 0.516 sec/iter\n",
      "Epoch: 301 | Batch: 006 / 011 | Total loss: 0.795 | Reg loss: 0.028 | Tree loss: 0.795 | Accuracy: 0.711500 | 0.516 sec/iter\n",
      "Epoch: 301 | Batch: 007 / 011 | Total loss: 0.781 | Reg loss: 0.028 | Tree loss: 0.781 | Accuracy: 0.739500 | 0.516 sec/iter\n",
      "Epoch: 301 | Batch: 008 / 011 | Total loss: 0.817 | Reg loss: 0.028 | Tree loss: 0.817 | Accuracy: 0.719500 | 0.516 sec/iter\n",
      "Epoch: 301 | Batch: 009 / 011 | Total loss: 0.802 | Reg loss: 0.028 | Tree loss: 0.802 | Accuracy: 0.713500 | 0.516 sec/iter\n",
      "Epoch: 301 | Batch: 010 / 011 | Total loss: 0.815 | Reg loss: 0.028 | Tree loss: 0.815 | Accuracy: 0.716724 | 0.516 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 302 | Batch: 000 / 011 | Total loss: 0.893 | Reg loss: 0.028 | Tree loss: 0.893 | Accuracy: 0.656000 | 0.516 sec/iter\n",
      "Epoch: 302 | Batch: 001 / 011 | Total loss: 0.851 | Reg loss: 0.028 | Tree loss: 0.851 | Accuracy: 0.686500 | 0.516 sec/iter\n",
      "Epoch: 302 | Batch: 002 / 011 | Total loss: 0.843 | Reg loss: 0.028 | Tree loss: 0.843 | Accuracy: 0.682500 | 0.516 sec/iter\n",
      "Epoch: 302 | Batch: 003 / 011 | Total loss: 0.833 | Reg loss: 0.028 | Tree loss: 0.833 | Accuracy: 0.682500 | 0.515 sec/iter\n",
      "Epoch: 302 | Batch: 004 / 011 | Total loss: 0.797 | Reg loss: 0.028 | Tree loss: 0.797 | Accuracy: 0.724500 | 0.515 sec/iter\n",
      "Epoch: 302 | Batch: 005 / 011 | Total loss: 0.809 | Reg loss: 0.028 | Tree loss: 0.809 | Accuracy: 0.708500 | 0.515 sec/iter\n",
      "Epoch: 302 | Batch: 006 / 011 | Total loss: 0.796 | Reg loss: 0.028 | Tree loss: 0.796 | Accuracy: 0.724500 | 0.515 sec/iter\n",
      "Epoch: 302 | Batch: 007 / 011 | Total loss: 0.783 | Reg loss: 0.028 | Tree loss: 0.783 | Accuracy: 0.732500 | 0.515 sec/iter\n",
      "Epoch: 302 | Batch: 008 / 011 | Total loss: 0.786 | Reg loss: 0.028 | Tree loss: 0.786 | Accuracy: 0.725500 | 0.515 sec/iter\n",
      "Epoch: 302 | Batch: 009 / 011 | Total loss: 0.809 | Reg loss: 0.028 | Tree loss: 0.809 | Accuracy: 0.716500 | 0.515 sec/iter\n",
      "Epoch: 302 | Batch: 010 / 011 | Total loss: 0.763 | Reg loss: 0.028 | Tree loss: 0.763 | Accuracy: 0.716724 | 0.515 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 303 | Batch: 000 / 011 | Total loss: 0.883 | Reg loss: 0.028 | Tree loss: 0.883 | Accuracy: 0.657500 | 0.516 sec/iter\n",
      "Epoch: 303 | Batch: 001 / 011 | Total loss: 0.883 | Reg loss: 0.028 | Tree loss: 0.883 | Accuracy: 0.667500 | 0.516 sec/iter\n",
      "Epoch: 303 | Batch: 002 / 011 | Total loss: 0.838 | Reg loss: 0.028 | Tree loss: 0.838 | Accuracy: 0.689000 | 0.516 sec/iter\n",
      "Epoch: 303 | Batch: 003 / 011 | Total loss: 0.809 | Reg loss: 0.028 | Tree loss: 0.809 | Accuracy: 0.705000 | 0.516 sec/iter\n",
      "Epoch: 303 | Batch: 004 / 011 | Total loss: 0.816 | Reg loss: 0.028 | Tree loss: 0.816 | Accuracy: 0.716500 | 0.516 sec/iter\n",
      "Epoch: 303 | Batch: 005 / 011 | Total loss: 0.795 | Reg loss: 0.028 | Tree loss: 0.795 | Accuracy: 0.724000 | 0.516 sec/iter\n",
      "Epoch: 303 | Batch: 006 / 011 | Total loss: 0.812 | Reg loss: 0.028 | Tree loss: 0.812 | Accuracy: 0.726500 | 0.516 sec/iter\n",
      "Epoch: 303 | Batch: 007 / 011 | Total loss: 0.777 | Reg loss: 0.028 | Tree loss: 0.777 | Accuracy: 0.738000 | 0.516 sec/iter\n",
      "Epoch: 303 | Batch: 008 / 011 | Total loss: 0.812 | Reg loss: 0.028 | Tree loss: 0.812 | Accuracy: 0.716500 | 0.516 sec/iter\n",
      "Epoch: 303 | Batch: 009 / 011 | Total loss: 0.774 | Reg loss: 0.028 | Tree loss: 0.774 | Accuracy: 0.733500 | 0.516 sec/iter\n",
      "Epoch: 303 | Batch: 010 / 011 | Total loss: 0.796 | Reg loss: 0.028 | Tree loss: 0.796 | Accuracy: 0.709898 | 0.516 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 304 | Batch: 000 / 011 | Total loss: 0.887 | Reg loss: 0.028 | Tree loss: 0.887 | Accuracy: 0.654000 | 0.517 sec/iter\n",
      "Epoch: 304 | Batch: 001 / 011 | Total loss: 0.843 | Reg loss: 0.028 | Tree loss: 0.843 | Accuracy: 0.682000 | 0.517 sec/iter\n",
      "Epoch: 304 | Batch: 002 / 011 | Total loss: 0.847 | Reg loss: 0.028 | Tree loss: 0.847 | Accuracy: 0.693000 | 0.517 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 304 | Batch: 003 / 011 | Total loss: 0.807 | Reg loss: 0.028 | Tree loss: 0.807 | Accuracy: 0.711000 | 0.517 sec/iter\n",
      "Epoch: 304 | Batch: 004 / 011 | Total loss: 0.811 | Reg loss: 0.028 | Tree loss: 0.811 | Accuracy: 0.721500 | 0.517 sec/iter\n",
      "Epoch: 304 | Batch: 005 / 011 | Total loss: 0.814 | Reg loss: 0.028 | Tree loss: 0.814 | Accuracy: 0.704500 | 0.517 sec/iter\n",
      "Epoch: 304 | Batch: 006 / 011 | Total loss: 0.776 | Reg loss: 0.028 | Tree loss: 0.776 | Accuracy: 0.740500 | 0.517 sec/iter\n",
      "Epoch: 304 | Batch: 007 / 011 | Total loss: 0.795 | Reg loss: 0.028 | Tree loss: 0.795 | Accuracy: 0.726500 | 0.516 sec/iter\n",
      "Epoch: 304 | Batch: 008 / 011 | Total loss: 0.801 | Reg loss: 0.028 | Tree loss: 0.801 | Accuracy: 0.734000 | 0.516 sec/iter\n",
      "Epoch: 304 | Batch: 009 / 011 | Total loss: 0.806 | Reg loss: 0.028 | Tree loss: 0.806 | Accuracy: 0.717000 | 0.516 sec/iter\n",
      "Epoch: 304 | Batch: 010 / 011 | Total loss: 0.849 | Reg loss: 0.028 | Tree loss: 0.849 | Accuracy: 0.703072 | 0.516 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 305 | Batch: 000 / 011 | Total loss: 0.866 | Reg loss: 0.028 | Tree loss: 0.866 | Accuracy: 0.673000 | 0.516 sec/iter\n",
      "Epoch: 305 | Batch: 001 / 011 | Total loss: 0.874 | Reg loss: 0.028 | Tree loss: 0.874 | Accuracy: 0.653000 | 0.516 sec/iter\n",
      "Epoch: 305 | Batch: 002 / 011 | Total loss: 0.858 | Reg loss: 0.028 | Tree loss: 0.858 | Accuracy: 0.672000 | 0.516 sec/iter\n",
      "Epoch: 305 | Batch: 003 / 011 | Total loss: 0.827 | Reg loss: 0.028 | Tree loss: 0.827 | Accuracy: 0.697500 | 0.516 sec/iter\n",
      "Epoch: 305 | Batch: 004 / 011 | Total loss: 0.798 | Reg loss: 0.028 | Tree loss: 0.798 | Accuracy: 0.721000 | 0.516 sec/iter\n",
      "Epoch: 305 | Batch: 005 / 011 | Total loss: 0.791 | Reg loss: 0.028 | Tree loss: 0.791 | Accuracy: 0.716000 | 0.516 sec/iter\n",
      "Epoch: 305 | Batch: 006 / 011 | Total loss: 0.819 | Reg loss: 0.028 | Tree loss: 0.819 | Accuracy: 0.709000 | 0.516 sec/iter\n",
      "Epoch: 305 | Batch: 007 / 011 | Total loss: 0.797 | Reg loss: 0.028 | Tree loss: 0.797 | Accuracy: 0.727500 | 0.516 sec/iter\n",
      "Epoch: 305 | Batch: 008 / 011 | Total loss: 0.760 | Reg loss: 0.028 | Tree loss: 0.760 | Accuracy: 0.740000 | 0.516 sec/iter\n",
      "Epoch: 305 | Batch: 009 / 011 | Total loss: 0.801 | Reg loss: 0.028 | Tree loss: 0.801 | Accuracy: 0.731500 | 0.515 sec/iter\n",
      "Epoch: 305 | Batch: 010 / 011 | Total loss: 0.762 | Reg loss: 0.028 | Tree loss: 0.762 | Accuracy: 0.737201 | 0.515 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 306 | Batch: 000 / 011 | Total loss: 0.873 | Reg loss: 0.028 | Tree loss: 0.873 | Accuracy: 0.655500 | 0.516 sec/iter\n",
      "Epoch: 306 | Batch: 001 / 011 | Total loss: 0.872 | Reg loss: 0.028 | Tree loss: 0.872 | Accuracy: 0.669500 | 0.516 sec/iter\n",
      "Epoch: 306 | Batch: 002 / 011 | Total loss: 0.849 | Reg loss: 0.028 | Tree loss: 0.849 | Accuracy: 0.695000 | 0.516 sec/iter\n",
      "Epoch: 306 | Batch: 003 / 011 | Total loss: 0.804 | Reg loss: 0.028 | Tree loss: 0.804 | Accuracy: 0.702500 | 0.516 sec/iter\n",
      "Epoch: 306 | Batch: 004 / 011 | Total loss: 0.809 | Reg loss: 0.028 | Tree loss: 0.809 | Accuracy: 0.707000 | 0.516 sec/iter\n",
      "Epoch: 306 | Batch: 005 / 011 | Total loss: 0.824 | Reg loss: 0.028 | Tree loss: 0.824 | Accuracy: 0.705000 | 0.516 sec/iter\n",
      "Epoch: 306 | Batch: 006 / 011 | Total loss: 0.794 | Reg loss: 0.028 | Tree loss: 0.794 | Accuracy: 0.730500 | 0.516 sec/iter\n",
      "Epoch: 306 | Batch: 007 / 011 | Total loss: 0.777 | Reg loss: 0.028 | Tree loss: 0.777 | Accuracy: 0.735000 | 0.516 sec/iter\n",
      "Epoch: 306 | Batch: 008 / 011 | Total loss: 0.789 | Reg loss: 0.028 | Tree loss: 0.789 | Accuracy: 0.724500 | 0.515 sec/iter\n",
      "Epoch: 306 | Batch: 009 / 011 | Total loss: 0.794 | Reg loss: 0.028 | Tree loss: 0.794 | Accuracy: 0.728000 | 0.515 sec/iter\n",
      "Epoch: 306 | Batch: 010 / 011 | Total loss: 0.810 | Reg loss: 0.028 | Tree loss: 0.810 | Accuracy: 0.737201 | 0.515 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 307 | Batch: 000 / 011 | Total loss: 0.866 | Reg loss: 0.028 | Tree loss: 0.866 | Accuracy: 0.683000 | 0.516 sec/iter\n",
      "Epoch: 307 | Batch: 001 / 011 | Total loss: 0.840 | Reg loss: 0.028 | Tree loss: 0.840 | Accuracy: 0.674500 | 0.516 sec/iter\n",
      "Epoch: 307 | Batch: 002 / 011 | Total loss: 0.855 | Reg loss: 0.028 | Tree loss: 0.855 | Accuracy: 0.674000 | 0.516 sec/iter\n",
      "Epoch: 307 | Batch: 003 / 011 | Total loss: 0.826 | Reg loss: 0.028 | Tree loss: 0.826 | Accuracy: 0.701000 | 0.516 sec/iter\n",
      "Epoch: 307 | Batch: 004 / 011 | Total loss: 0.804 | Reg loss: 0.028 | Tree loss: 0.804 | Accuracy: 0.705000 | 0.516 sec/iter\n",
      "Epoch: 307 | Batch: 005 / 011 | Total loss: 0.791 | Reg loss: 0.028 | Tree loss: 0.791 | Accuracy: 0.723000 | 0.516 sec/iter\n",
      "Epoch: 307 | Batch: 006 / 011 | Total loss: 0.788 | Reg loss: 0.028 | Tree loss: 0.788 | Accuracy: 0.730000 | 0.516 sec/iter\n",
      "Epoch: 307 | Batch: 007 / 011 | Total loss: 0.801 | Reg loss: 0.028 | Tree loss: 0.801 | Accuracy: 0.715500 | 0.515 sec/iter\n",
      "Epoch: 307 | Batch: 008 / 011 | Total loss: 0.788 | Reg loss: 0.028 | Tree loss: 0.788 | Accuracy: 0.729500 | 0.515 sec/iter\n",
      "Epoch: 307 | Batch: 009 / 011 | Total loss: 0.817 | Reg loss: 0.028 | Tree loss: 0.817 | Accuracy: 0.724000 | 0.515 sec/iter\n",
      "Epoch: 307 | Batch: 010 / 011 | Total loss: 0.819 | Reg loss: 0.028 | Tree loss: 0.819 | Accuracy: 0.699659 | 0.515 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 308 | Batch: 000 / 011 | Total loss: 0.890 | Reg loss: 0.028 | Tree loss: 0.890 | Accuracy: 0.647000 | 0.515 sec/iter\n",
      "Epoch: 308 | Batch: 001 / 011 | Total loss: 0.879 | Reg loss: 0.028 | Tree loss: 0.879 | Accuracy: 0.658000 | 0.515 sec/iter\n",
      "Epoch: 308 | Batch: 002 / 011 | Total loss: 0.860 | Reg loss: 0.028 | Tree loss: 0.860 | Accuracy: 0.678500 | 0.515 sec/iter\n",
      "Epoch: 308 | Batch: 003 / 011 | Total loss: 0.822 | Reg loss: 0.028 | Tree loss: 0.822 | Accuracy: 0.697500 | 0.515 sec/iter\n",
      "Epoch: 308 | Batch: 004 / 011 | Total loss: 0.809 | Reg loss: 0.028 | Tree loss: 0.809 | Accuracy: 0.706500 | 0.515 sec/iter\n",
      "Epoch: 308 | Batch: 005 / 011 | Total loss: 0.782 | Reg loss: 0.028 | Tree loss: 0.782 | Accuracy: 0.722500 | 0.515 sec/iter\n",
      "Epoch: 308 | Batch: 006 / 011 | Total loss: 0.766 | Reg loss: 0.028 | Tree loss: 0.766 | Accuracy: 0.742000 | 0.515 sec/iter\n",
      "Epoch: 308 | Batch: 007 / 011 | Total loss: 0.789 | Reg loss: 0.028 | Tree loss: 0.789 | Accuracy: 0.729500 | 0.515 sec/iter\n",
      "Epoch: 308 | Batch: 008 / 011 | Total loss: 0.803 | Reg loss: 0.028 | Tree loss: 0.803 | Accuracy: 0.721500 | 0.515 sec/iter\n",
      "Epoch: 308 | Batch: 009 / 011 | Total loss: 0.796 | Reg loss: 0.028 | Tree loss: 0.796 | Accuracy: 0.732500 | 0.515 sec/iter\n",
      "Epoch: 308 | Batch: 010 / 011 | Total loss: 0.816 | Reg loss: 0.028 | Tree loss: 0.816 | Accuracy: 0.706485 | 0.515 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 309 | Batch: 000 / 011 | Total loss: 0.871 | Reg loss: 0.028 | Tree loss: 0.871 | Accuracy: 0.654500 | 0.515 sec/iter\n",
      "Epoch: 309 | Batch: 001 / 011 | Total loss: 0.850 | Reg loss: 0.028 | Tree loss: 0.850 | Accuracy: 0.673500 | 0.515 sec/iter\n",
      "Epoch: 309 | Batch: 002 / 011 | Total loss: 0.864 | Reg loss: 0.028 | Tree loss: 0.864 | Accuracy: 0.661000 | 0.515 sec/iter\n",
      "Epoch: 309 | Batch: 003 / 011 | Total loss: 0.827 | Reg loss: 0.028 | Tree loss: 0.827 | Accuracy: 0.703000 | 0.515 sec/iter\n",
      "Epoch: 309 | Batch: 004 / 011 | Total loss: 0.823 | Reg loss: 0.028 | Tree loss: 0.823 | Accuracy: 0.697500 | 0.515 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 309 | Batch: 005 / 011 | Total loss: 0.812 | Reg loss: 0.028 | Tree loss: 0.812 | Accuracy: 0.711000 | 0.515 sec/iter\n",
      "Epoch: 309 | Batch: 006 / 011 | Total loss: 0.781 | Reg loss: 0.028 | Tree loss: 0.781 | Accuracy: 0.736000 | 0.514 sec/iter\n",
      "Epoch: 309 | Batch: 007 / 011 | Total loss: 0.813 | Reg loss: 0.028 | Tree loss: 0.813 | Accuracy: 0.716000 | 0.514 sec/iter\n",
      "Epoch: 309 | Batch: 008 / 011 | Total loss: 0.789 | Reg loss: 0.028 | Tree loss: 0.789 | Accuracy: 0.725500 | 0.514 sec/iter\n",
      "Epoch: 309 | Batch: 009 / 011 | Total loss: 0.788 | Reg loss: 0.028 | Tree loss: 0.788 | Accuracy: 0.727000 | 0.514 sec/iter\n",
      "Epoch: 309 | Batch: 010 / 011 | Total loss: 0.811 | Reg loss: 0.028 | Tree loss: 0.811 | Accuracy: 0.720137 | 0.514 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 310 | Batch: 000 / 011 | Total loss: 0.884 | Reg loss: 0.028 | Tree loss: 0.884 | Accuracy: 0.665000 | 0.516 sec/iter\n",
      "Epoch: 310 | Batch: 001 / 011 | Total loss: 0.865 | Reg loss: 0.028 | Tree loss: 0.865 | Accuracy: 0.671000 | 0.515 sec/iter\n",
      "Epoch: 310 | Batch: 002 / 011 | Total loss: 0.844 | Reg loss: 0.028 | Tree loss: 0.844 | Accuracy: 0.689000 | 0.515 sec/iter\n",
      "Epoch: 310 | Batch: 003 / 011 | Total loss: 0.829 | Reg loss: 0.028 | Tree loss: 0.829 | Accuracy: 0.694500 | 0.515 sec/iter\n",
      "Epoch: 310 | Batch: 004 / 011 | Total loss: 0.812 | Reg loss: 0.028 | Tree loss: 0.812 | Accuracy: 0.700500 | 0.515 sec/iter\n",
      "Epoch: 310 | Batch: 005 / 011 | Total loss: 0.806 | Reg loss: 0.028 | Tree loss: 0.806 | Accuracy: 0.712000 | 0.515 sec/iter\n",
      "Epoch: 310 | Batch: 006 / 011 | Total loss: 0.782 | Reg loss: 0.028 | Tree loss: 0.782 | Accuracy: 0.737500 | 0.515 sec/iter\n",
      "Epoch: 310 | Batch: 007 / 011 | Total loss: 0.779 | Reg loss: 0.028 | Tree loss: 0.779 | Accuracy: 0.737500 | 0.515 sec/iter\n",
      "Epoch: 310 | Batch: 008 / 011 | Total loss: 0.785 | Reg loss: 0.028 | Tree loss: 0.785 | Accuracy: 0.745000 | 0.515 sec/iter\n",
      "Epoch: 310 | Batch: 009 / 011 | Total loss: 0.789 | Reg loss: 0.028 | Tree loss: 0.789 | Accuracy: 0.720500 | 0.515 sec/iter\n",
      "Epoch: 310 | Batch: 010 / 011 | Total loss: 0.786 | Reg loss: 0.028 | Tree loss: 0.786 | Accuracy: 0.733788 | 0.515 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 311 | Batch: 000 / 011 | Total loss: 0.854 | Reg loss: 0.028 | Tree loss: 0.854 | Accuracy: 0.682000 | 0.515 sec/iter\n",
      "Epoch: 311 | Batch: 001 / 011 | Total loss: 0.864 | Reg loss: 0.028 | Tree loss: 0.864 | Accuracy: 0.670500 | 0.515 sec/iter\n",
      "Epoch: 311 | Batch: 002 / 011 | Total loss: 0.855 | Reg loss: 0.028 | Tree loss: 0.855 | Accuracy: 0.682500 | 0.515 sec/iter\n",
      "Epoch: 311 | Batch: 003 / 011 | Total loss: 0.819 | Reg loss: 0.028 | Tree loss: 0.819 | Accuracy: 0.702500 | 0.514 sec/iter\n",
      "Epoch: 311 | Batch: 004 / 011 | Total loss: 0.802 | Reg loss: 0.028 | Tree loss: 0.802 | Accuracy: 0.721000 | 0.514 sec/iter\n",
      "Epoch: 311 | Batch: 005 / 011 | Total loss: 0.811 | Reg loss: 0.028 | Tree loss: 0.811 | Accuracy: 0.696500 | 0.514 sec/iter\n",
      "Epoch: 311 | Batch: 006 / 011 | Total loss: 0.810 | Reg loss: 0.028 | Tree loss: 0.810 | Accuracy: 0.723500 | 0.514 sec/iter\n",
      "Epoch: 311 | Batch: 007 / 011 | Total loss: 0.789 | Reg loss: 0.028 | Tree loss: 0.789 | Accuracy: 0.725500 | 0.514 sec/iter\n",
      "Epoch: 311 | Batch: 008 / 011 | Total loss: 0.794 | Reg loss: 0.028 | Tree loss: 0.794 | Accuracy: 0.732500 | 0.514 sec/iter\n",
      "Epoch: 311 | Batch: 009 / 011 | Total loss: 0.784 | Reg loss: 0.028 | Tree loss: 0.784 | Accuracy: 0.730000 | 0.514 sec/iter\n",
      "Epoch: 311 | Batch: 010 / 011 | Total loss: 0.779 | Reg loss: 0.028 | Tree loss: 0.779 | Accuracy: 0.730375 | 0.514 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 312 | Batch: 000 / 011 | Total loss: 0.877 | Reg loss: 0.028 | Tree loss: 0.877 | Accuracy: 0.669000 | 0.515 sec/iter\n",
      "Epoch: 312 | Batch: 001 / 011 | Total loss: 0.864 | Reg loss: 0.028 | Tree loss: 0.864 | Accuracy: 0.666000 | 0.515 sec/iter\n",
      "Epoch: 312 | Batch: 002 / 011 | Total loss: 0.830 | Reg loss: 0.028 | Tree loss: 0.830 | Accuracy: 0.689000 | 0.514 sec/iter\n",
      "Epoch: 312 | Batch: 003 / 011 | Total loss: 0.811 | Reg loss: 0.028 | Tree loss: 0.811 | Accuracy: 0.703000 | 0.514 sec/iter\n",
      "Epoch: 312 | Batch: 004 / 011 | Total loss: 0.804 | Reg loss: 0.028 | Tree loss: 0.804 | Accuracy: 0.724500 | 0.514 sec/iter\n",
      "Epoch: 312 | Batch: 005 / 011 | Total loss: 0.834 | Reg loss: 0.028 | Tree loss: 0.834 | Accuracy: 0.685000 | 0.514 sec/iter\n",
      "Epoch: 312 | Batch: 006 / 011 | Total loss: 0.789 | Reg loss: 0.028 | Tree loss: 0.789 | Accuracy: 0.735000 | 0.514 sec/iter\n",
      "Epoch: 312 | Batch: 007 / 011 | Total loss: 0.802 | Reg loss: 0.028 | Tree loss: 0.802 | Accuracy: 0.717000 | 0.514 sec/iter\n",
      "Epoch: 312 | Batch: 008 / 011 | Total loss: 0.793 | Reg loss: 0.028 | Tree loss: 0.793 | Accuracy: 0.735500 | 0.514 sec/iter\n",
      "Epoch: 312 | Batch: 009 / 011 | Total loss: 0.787 | Reg loss: 0.028 | Tree loss: 0.787 | Accuracy: 0.714500 | 0.514 sec/iter\n",
      "Epoch: 312 | Batch: 010 / 011 | Total loss: 0.795 | Reg loss: 0.028 | Tree loss: 0.795 | Accuracy: 0.733788 | 0.514 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 313 | Batch: 000 / 011 | Total loss: 0.873 | Reg loss: 0.028 | Tree loss: 0.873 | Accuracy: 0.681500 | 0.515 sec/iter\n",
      "Epoch: 313 | Batch: 001 / 011 | Total loss: 0.858 | Reg loss: 0.028 | Tree loss: 0.858 | Accuracy: 0.676000 | 0.515 sec/iter\n",
      "Epoch: 313 | Batch: 002 / 011 | Total loss: 0.839 | Reg loss: 0.028 | Tree loss: 0.839 | Accuracy: 0.691000 | 0.514 sec/iter\n",
      "Epoch: 313 | Batch: 003 / 011 | Total loss: 0.835 | Reg loss: 0.028 | Tree loss: 0.835 | Accuracy: 0.685000 | 0.514 sec/iter\n",
      "Epoch: 313 | Batch: 004 / 011 | Total loss: 0.823 | Reg loss: 0.028 | Tree loss: 0.823 | Accuracy: 0.709000 | 0.514 sec/iter\n",
      "Epoch: 313 | Batch: 005 / 011 | Total loss: 0.818 | Reg loss: 0.028 | Tree loss: 0.818 | Accuracy: 0.709000 | 0.514 sec/iter\n",
      "Epoch: 313 | Batch: 006 / 011 | Total loss: 0.780 | Reg loss: 0.028 | Tree loss: 0.780 | Accuracy: 0.712000 | 0.514 sec/iter\n",
      "Epoch: 313 | Batch: 007 / 011 | Total loss: 0.771 | Reg loss: 0.028 | Tree loss: 0.771 | Accuracy: 0.730000 | 0.514 sec/iter\n",
      "Epoch: 313 | Batch: 008 / 011 | Total loss: 0.810 | Reg loss: 0.028 | Tree loss: 0.810 | Accuracy: 0.714500 | 0.514 sec/iter\n",
      "Epoch: 313 | Batch: 009 / 011 | Total loss: 0.803 | Reg loss: 0.028 | Tree loss: 0.803 | Accuracy: 0.722000 | 0.514 sec/iter\n",
      "Epoch: 313 | Batch: 010 / 011 | Total loss: 0.818 | Reg loss: 0.028 | Tree loss: 0.818 | Accuracy: 0.747440 | 0.514 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 314 | Batch: 000 / 011 | Total loss: 0.875 | Reg loss: 0.028 | Tree loss: 0.875 | Accuracy: 0.671000 | 0.514 sec/iter\n",
      "Epoch: 314 | Batch: 001 / 011 | Total loss: 0.876 | Reg loss: 0.028 | Tree loss: 0.876 | Accuracy: 0.670500 | 0.514 sec/iter\n",
      "Epoch: 314 | Batch: 002 / 011 | Total loss: 0.835 | Reg loss: 0.028 | Tree loss: 0.835 | Accuracy: 0.683000 | 0.514 sec/iter\n",
      "Epoch: 314 | Batch: 003 / 011 | Total loss: 0.829 | Reg loss: 0.028 | Tree loss: 0.829 | Accuracy: 0.701500 | 0.514 sec/iter\n",
      "Epoch: 314 | Batch: 004 / 011 | Total loss: 0.808 | Reg loss: 0.028 | Tree loss: 0.808 | Accuracy: 0.705500 | 0.514 sec/iter\n",
      "Epoch: 314 | Batch: 005 / 011 | Total loss: 0.781 | Reg loss: 0.028 | Tree loss: 0.781 | Accuracy: 0.724500 | 0.514 sec/iter\n",
      "Epoch: 314 | Batch: 006 / 011 | Total loss: 0.795 | Reg loss: 0.028 | Tree loss: 0.795 | Accuracy: 0.725500 | 0.514 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 314 | Batch: 007 / 011 | Total loss: 0.801 | Reg loss: 0.028 | Tree loss: 0.801 | Accuracy: 0.714000 | 0.513 sec/iter\n",
      "Epoch: 314 | Batch: 008 / 011 | Total loss: 0.804 | Reg loss: 0.028 | Tree loss: 0.804 | Accuracy: 0.735500 | 0.513 sec/iter\n",
      "Epoch: 314 | Batch: 009 / 011 | Total loss: 0.806 | Reg loss: 0.028 | Tree loss: 0.806 | Accuracy: 0.726000 | 0.513 sec/iter\n",
      "Epoch: 314 | Batch: 010 / 011 | Total loss: 0.750 | Reg loss: 0.028 | Tree loss: 0.750 | Accuracy: 0.747440 | 0.513 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 315 | Batch: 000 / 011 | Total loss: 0.875 | Reg loss: 0.028 | Tree loss: 0.875 | Accuracy: 0.669000 | 0.513 sec/iter\n",
      "Epoch: 315 | Batch: 001 / 011 | Total loss: 0.841 | Reg loss: 0.028 | Tree loss: 0.841 | Accuracy: 0.700500 | 0.513 sec/iter\n",
      "Epoch: 315 | Batch: 002 / 011 | Total loss: 0.834 | Reg loss: 0.028 | Tree loss: 0.834 | Accuracy: 0.698000 | 0.513 sec/iter\n",
      "Epoch: 315 | Batch: 003 / 011 | Total loss: 0.844 | Reg loss: 0.028 | Tree loss: 0.844 | Accuracy: 0.676000 | 0.513 sec/iter\n",
      "Epoch: 315 | Batch: 004 / 011 | Total loss: 0.806 | Reg loss: 0.028 | Tree loss: 0.806 | Accuracy: 0.701000 | 0.513 sec/iter\n",
      "Epoch: 315 | Batch: 005 / 011 | Total loss: 0.798 | Reg loss: 0.028 | Tree loss: 0.798 | Accuracy: 0.728500 | 0.513 sec/iter\n",
      "Epoch: 315 | Batch: 006 / 011 | Total loss: 0.811 | Reg loss: 0.028 | Tree loss: 0.811 | Accuracy: 0.696500 | 0.513 sec/iter\n",
      "Epoch: 315 | Batch: 007 / 011 | Total loss: 0.791 | Reg loss: 0.028 | Tree loss: 0.791 | Accuracy: 0.715000 | 0.513 sec/iter\n",
      "Epoch: 315 | Batch: 008 / 011 | Total loss: 0.812 | Reg loss: 0.028 | Tree loss: 0.812 | Accuracy: 0.705500 | 0.513 sec/iter\n",
      "Epoch: 315 | Batch: 009 / 011 | Total loss: 0.805 | Reg loss: 0.028 | Tree loss: 0.805 | Accuracy: 0.719000 | 0.513 sec/iter\n",
      "Epoch: 315 | Batch: 010 / 011 | Total loss: 0.838 | Reg loss: 0.028 | Tree loss: 0.838 | Accuracy: 0.692833 | 0.513 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 316 | Batch: 000 / 011 | Total loss: 0.866 | Reg loss: 0.028 | Tree loss: 0.866 | Accuracy: 0.662500 | 0.514 sec/iter\n",
      "Epoch: 316 | Batch: 001 / 011 | Total loss: 0.855 | Reg loss: 0.028 | Tree loss: 0.855 | Accuracy: 0.667000 | 0.514 sec/iter\n",
      "Epoch: 316 | Batch: 002 / 011 | Total loss: 0.859 | Reg loss: 0.028 | Tree loss: 0.859 | Accuracy: 0.682000 | 0.514 sec/iter\n",
      "Epoch: 316 | Batch: 003 / 011 | Total loss: 0.853 | Reg loss: 0.028 | Tree loss: 0.853 | Accuracy: 0.697000 | 0.514 sec/iter\n",
      "Epoch: 316 | Batch: 004 / 011 | Total loss: 0.830 | Reg loss: 0.028 | Tree loss: 0.830 | Accuracy: 0.698000 | 0.514 sec/iter\n",
      "Epoch: 316 | Batch: 005 / 011 | Total loss: 0.795 | Reg loss: 0.028 | Tree loss: 0.795 | Accuracy: 0.726000 | 0.514 sec/iter\n",
      "Epoch: 316 | Batch: 006 / 011 | Total loss: 0.789 | Reg loss: 0.028 | Tree loss: 0.789 | Accuracy: 0.735500 | 0.513 sec/iter\n",
      "Epoch: 316 | Batch: 007 / 011 | Total loss: 0.790 | Reg loss: 0.028 | Tree loss: 0.790 | Accuracy: 0.740500 | 0.513 sec/iter\n",
      "Epoch: 316 | Batch: 008 / 011 | Total loss: 0.776 | Reg loss: 0.028 | Tree loss: 0.776 | Accuracy: 0.723000 | 0.513 sec/iter\n",
      "Epoch: 316 | Batch: 009 / 011 | Total loss: 0.792 | Reg loss: 0.028 | Tree loss: 0.792 | Accuracy: 0.731000 | 0.513 sec/iter\n",
      "Epoch: 316 | Batch: 010 / 011 | Total loss: 0.747 | Reg loss: 0.028 | Tree loss: 0.747 | Accuracy: 0.788396 | 0.513 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 317 | Batch: 000 / 011 | Total loss: 0.863 | Reg loss: 0.028 | Tree loss: 0.863 | Accuracy: 0.665000 | 0.513 sec/iter\n",
      "Epoch: 317 | Batch: 001 / 011 | Total loss: 0.848 | Reg loss: 0.028 | Tree loss: 0.848 | Accuracy: 0.683000 | 0.513 sec/iter\n",
      "Epoch: 317 | Batch: 002 / 011 | Total loss: 0.838 | Reg loss: 0.028 | Tree loss: 0.838 | Accuracy: 0.699500 | 0.513 sec/iter\n",
      "Epoch: 317 | Batch: 003 / 011 | Total loss: 0.839 | Reg loss: 0.028 | Tree loss: 0.839 | Accuracy: 0.692000 | 0.513 sec/iter\n",
      "Epoch: 317 | Batch: 004 / 011 | Total loss: 0.814 | Reg loss: 0.028 | Tree loss: 0.814 | Accuracy: 0.700000 | 0.513 sec/iter\n",
      "Epoch: 317 | Batch: 005 / 011 | Total loss: 0.798 | Reg loss: 0.028 | Tree loss: 0.798 | Accuracy: 0.716500 | 0.513 sec/iter\n",
      "Epoch: 317 | Batch: 006 / 011 | Total loss: 0.773 | Reg loss: 0.028 | Tree loss: 0.773 | Accuracy: 0.740000 | 0.513 sec/iter\n",
      "Epoch: 317 | Batch: 007 / 011 | Total loss: 0.821 | Reg loss: 0.028 | Tree loss: 0.821 | Accuracy: 0.686500 | 0.513 sec/iter\n",
      "Epoch: 317 | Batch: 008 / 011 | Total loss: 0.796 | Reg loss: 0.028 | Tree loss: 0.796 | Accuracy: 0.717500 | 0.513 sec/iter\n",
      "Epoch: 317 | Batch: 009 / 011 | Total loss: 0.802 | Reg loss: 0.028 | Tree loss: 0.802 | Accuracy: 0.714500 | 0.513 sec/iter\n",
      "Epoch: 317 | Batch: 010 / 011 | Total loss: 0.763 | Reg loss: 0.028 | Tree loss: 0.763 | Accuracy: 0.774744 | 0.512 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 318 | Batch: 000 / 011 | Total loss: 0.864 | Reg loss: 0.028 | Tree loss: 0.864 | Accuracy: 0.676500 | 0.514 sec/iter\n",
      "Epoch: 318 | Batch: 001 / 011 | Total loss: 0.862 | Reg loss: 0.028 | Tree loss: 0.862 | Accuracy: 0.674000 | 0.514 sec/iter\n",
      "Epoch: 318 | Batch: 002 / 011 | Total loss: 0.844 | Reg loss: 0.028 | Tree loss: 0.844 | Accuracy: 0.682500 | 0.514 sec/iter\n",
      "Epoch: 318 | Batch: 003 / 011 | Total loss: 0.836 | Reg loss: 0.028 | Tree loss: 0.836 | Accuracy: 0.698000 | 0.514 sec/iter\n",
      "Epoch: 318 | Batch: 004 / 011 | Total loss: 0.819 | Reg loss: 0.028 | Tree loss: 0.819 | Accuracy: 0.705500 | 0.513 sec/iter\n",
      "Epoch: 318 | Batch: 005 / 011 | Total loss: 0.773 | Reg loss: 0.028 | Tree loss: 0.773 | Accuracy: 0.730500 | 0.513 sec/iter\n",
      "Epoch: 318 | Batch: 006 / 011 | Total loss: 0.803 | Reg loss: 0.028 | Tree loss: 0.803 | Accuracy: 0.723500 | 0.513 sec/iter\n",
      "Epoch: 318 | Batch: 007 / 011 | Total loss: 0.796 | Reg loss: 0.028 | Tree loss: 0.796 | Accuracy: 0.729000 | 0.513 sec/iter\n",
      "Epoch: 318 | Batch: 008 / 011 | Total loss: 0.782 | Reg loss: 0.028 | Tree loss: 0.782 | Accuracy: 0.736000 | 0.513 sec/iter\n",
      "Epoch: 318 | Batch: 009 / 011 | Total loss: 0.802 | Reg loss: 0.028 | Tree loss: 0.802 | Accuracy: 0.723000 | 0.513 sec/iter\n",
      "Epoch: 318 | Batch: 010 / 011 | Total loss: 0.727 | Reg loss: 0.028 | Tree loss: 0.727 | Accuracy: 0.730375 | 0.513 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 319 | Batch: 000 / 011 | Total loss: 0.875 | Reg loss: 0.028 | Tree loss: 0.875 | Accuracy: 0.673500 | 0.514 sec/iter\n",
      "Epoch: 319 | Batch: 001 / 011 | Total loss: 0.870 | Reg loss: 0.028 | Tree loss: 0.870 | Accuracy: 0.670500 | 0.514 sec/iter\n",
      "Epoch: 319 | Batch: 002 / 011 | Total loss: 0.829 | Reg loss: 0.028 | Tree loss: 0.829 | Accuracy: 0.691500 | 0.514 sec/iter\n",
      "Epoch: 319 | Batch: 003 / 011 | Total loss: 0.834 | Reg loss: 0.028 | Tree loss: 0.834 | Accuracy: 0.684500 | 0.514 sec/iter\n",
      "Epoch: 319 | Batch: 004 / 011 | Total loss: 0.805 | Reg loss: 0.028 | Tree loss: 0.805 | Accuracy: 0.721000 | 0.514 sec/iter\n",
      "Epoch: 319 | Batch: 005 / 011 | Total loss: 0.800 | Reg loss: 0.028 | Tree loss: 0.800 | Accuracy: 0.722500 | 0.514 sec/iter\n",
      "Epoch: 319 | Batch: 006 / 011 | Total loss: 0.801 | Reg loss: 0.028 | Tree loss: 0.801 | Accuracy: 0.724000 | 0.514 sec/iter\n",
      "Epoch: 319 | Batch: 007 / 011 | Total loss: 0.775 | Reg loss: 0.028 | Tree loss: 0.775 | Accuracy: 0.733000 | 0.514 sec/iter\n",
      "Epoch: 319 | Batch: 008 / 011 | Total loss: 0.791 | Reg loss: 0.028 | Tree loss: 0.791 | Accuracy: 0.726000 | 0.514 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 319 | Batch: 009 / 011 | Total loss: 0.794 | Reg loss: 0.028 | Tree loss: 0.794 | Accuracy: 0.718500 | 0.514 sec/iter\n",
      "Epoch: 319 | Batch: 010 / 011 | Total loss: 0.769 | Reg loss: 0.028 | Tree loss: 0.769 | Accuracy: 0.713311 | 0.514 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 320 | Batch: 000 / 011 | Total loss: 0.869 | Reg loss: 0.028 | Tree loss: 0.869 | Accuracy: 0.667500 | 0.514 sec/iter\n",
      "Epoch: 320 | Batch: 001 / 011 | Total loss: 0.862 | Reg loss: 0.028 | Tree loss: 0.862 | Accuracy: 0.677000 | 0.514 sec/iter\n",
      "Epoch: 320 | Batch: 002 / 011 | Total loss: 0.860 | Reg loss: 0.028 | Tree loss: 0.860 | Accuracy: 0.676000 | 0.513 sec/iter\n",
      "Epoch: 320 | Batch: 003 / 011 | Total loss: 0.846 | Reg loss: 0.028 | Tree loss: 0.846 | Accuracy: 0.677500 | 0.513 sec/iter\n",
      "Epoch: 320 | Batch: 004 / 011 | Total loss: 0.805 | Reg loss: 0.028 | Tree loss: 0.805 | Accuracy: 0.707500 | 0.513 sec/iter\n",
      "Epoch: 320 | Batch: 005 / 011 | Total loss: 0.808 | Reg loss: 0.028 | Tree loss: 0.808 | Accuracy: 0.711000 | 0.513 sec/iter\n",
      "Epoch: 320 | Batch: 006 / 011 | Total loss: 0.758 | Reg loss: 0.028 | Tree loss: 0.758 | Accuracy: 0.771000 | 0.513 sec/iter\n",
      "Epoch: 320 | Batch: 007 / 011 | Total loss: 0.799 | Reg loss: 0.028 | Tree loss: 0.799 | Accuracy: 0.732500 | 0.513 sec/iter\n",
      "Epoch: 320 | Batch: 008 / 011 | Total loss: 0.793 | Reg loss: 0.028 | Tree loss: 0.793 | Accuracy: 0.736000 | 0.513 sec/iter\n",
      "Epoch: 320 | Batch: 009 / 011 | Total loss: 0.784 | Reg loss: 0.028 | Tree loss: 0.784 | Accuracy: 0.731000 | 0.513 sec/iter\n",
      "Epoch: 320 | Batch: 010 / 011 | Total loss: 0.716 | Reg loss: 0.028 | Tree loss: 0.716 | Accuracy: 0.788396 | 0.513 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 321 | Batch: 000 / 011 | Total loss: 0.847 | Reg loss: 0.028 | Tree loss: 0.847 | Accuracy: 0.690500 | 0.514 sec/iter\n",
      "Epoch: 321 | Batch: 001 / 011 | Total loss: 0.862 | Reg loss: 0.028 | Tree loss: 0.862 | Accuracy: 0.671500 | 0.514 sec/iter\n",
      "Epoch: 321 | Batch: 002 / 011 | Total loss: 0.845 | Reg loss: 0.028 | Tree loss: 0.845 | Accuracy: 0.692500 | 0.514 sec/iter\n",
      "Epoch: 321 | Batch: 003 / 011 | Total loss: 0.808 | Reg loss: 0.028 | Tree loss: 0.808 | Accuracy: 0.710500 | 0.514 sec/iter\n",
      "Epoch: 321 | Batch: 004 / 011 | Total loss: 0.811 | Reg loss: 0.028 | Tree loss: 0.811 | Accuracy: 0.695000 | 0.514 sec/iter\n",
      "Epoch: 321 | Batch: 005 / 011 | Total loss: 0.827 | Reg loss: 0.028 | Tree loss: 0.827 | Accuracy: 0.694500 | 0.514 sec/iter\n",
      "Epoch: 321 | Batch: 006 / 011 | Total loss: 0.782 | Reg loss: 0.028 | Tree loss: 0.782 | Accuracy: 0.724500 | 0.514 sec/iter\n",
      "Epoch: 321 | Batch: 007 / 011 | Total loss: 0.782 | Reg loss: 0.028 | Tree loss: 0.782 | Accuracy: 0.726500 | 0.514 sec/iter\n",
      "Epoch: 321 | Batch: 008 / 011 | Total loss: 0.809 | Reg loss: 0.028 | Tree loss: 0.809 | Accuracy: 0.716500 | 0.514 sec/iter\n",
      "Epoch: 321 | Batch: 009 / 011 | Total loss: 0.818 | Reg loss: 0.028 | Tree loss: 0.818 | Accuracy: 0.708500 | 0.514 sec/iter\n",
      "Epoch: 321 | Batch: 010 / 011 | Total loss: 0.806 | Reg loss: 0.028 | Tree loss: 0.806 | Accuracy: 0.713311 | 0.513 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 322 | Batch: 000 / 011 | Total loss: 0.883 | Reg loss: 0.028 | Tree loss: 0.883 | Accuracy: 0.660500 | 0.513 sec/iter\n",
      "Epoch: 322 | Batch: 001 / 011 | Total loss: 0.857 | Reg loss: 0.028 | Tree loss: 0.857 | Accuracy: 0.682000 | 0.513 sec/iter\n",
      "Epoch: 322 | Batch: 002 / 011 | Total loss: 0.827 | Reg loss: 0.028 | Tree loss: 0.827 | Accuracy: 0.696500 | 0.513 sec/iter\n",
      "Epoch: 322 | Batch: 003 / 011 | Total loss: 0.842 | Reg loss: 0.028 | Tree loss: 0.842 | Accuracy: 0.691500 | 0.513 sec/iter\n",
      "Epoch: 322 | Batch: 004 / 011 | Total loss: 0.818 | Reg loss: 0.028 | Tree loss: 0.818 | Accuracy: 0.694500 | 0.513 sec/iter\n",
      "Epoch: 322 | Batch: 005 / 011 | Total loss: 0.787 | Reg loss: 0.028 | Tree loss: 0.787 | Accuracy: 0.718500 | 0.513 sec/iter\n",
      "Epoch: 322 | Batch: 006 / 011 | Total loss: 0.779 | Reg loss: 0.028 | Tree loss: 0.779 | Accuracy: 0.733000 | 0.513 sec/iter\n",
      "Epoch: 322 | Batch: 007 / 011 | Total loss: 0.795 | Reg loss: 0.028 | Tree loss: 0.795 | Accuracy: 0.731500 | 0.513 sec/iter\n",
      "Epoch: 322 | Batch: 008 / 011 | Total loss: 0.803 | Reg loss: 0.028 | Tree loss: 0.803 | Accuracy: 0.724500 | 0.513 sec/iter\n",
      "Epoch: 322 | Batch: 009 / 011 | Total loss: 0.786 | Reg loss: 0.028 | Tree loss: 0.786 | Accuracy: 0.730000 | 0.513 sec/iter\n",
      "Epoch: 322 | Batch: 010 / 011 | Total loss: 0.790 | Reg loss: 0.028 | Tree loss: 0.790 | Accuracy: 0.750853 | 0.513 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 323 | Batch: 000 / 011 | Total loss: 0.897 | Reg loss: 0.028 | Tree loss: 0.897 | Accuracy: 0.645500 | 0.514 sec/iter\n",
      "Epoch: 323 | Batch: 001 / 011 | Total loss: 0.850 | Reg loss: 0.028 | Tree loss: 0.850 | Accuracy: 0.668500 | 0.514 sec/iter\n",
      "Epoch: 323 | Batch: 002 / 011 | Total loss: 0.836 | Reg loss: 0.028 | Tree loss: 0.836 | Accuracy: 0.688500 | 0.514 sec/iter\n",
      "Epoch: 323 | Batch: 003 / 011 | Total loss: 0.817 | Reg loss: 0.028 | Tree loss: 0.817 | Accuracy: 0.716000 | 0.514 sec/iter\n",
      "Epoch: 323 | Batch: 004 / 011 | Total loss: 0.799 | Reg loss: 0.028 | Tree loss: 0.799 | Accuracy: 0.713500 | 0.514 sec/iter\n",
      "Epoch: 323 | Batch: 005 / 011 | Total loss: 0.820 | Reg loss: 0.028 | Tree loss: 0.820 | Accuracy: 0.695500 | 0.514 sec/iter\n",
      "Epoch: 323 | Batch: 006 / 011 | Total loss: 0.801 | Reg loss: 0.028 | Tree loss: 0.801 | Accuracy: 0.721000 | 0.514 sec/iter\n",
      "Epoch: 323 | Batch: 007 / 011 | Total loss: 0.779 | Reg loss: 0.028 | Tree loss: 0.779 | Accuracy: 0.742000 | 0.514 sec/iter\n",
      "Epoch: 323 | Batch: 008 / 011 | Total loss: 0.789 | Reg loss: 0.028 | Tree loss: 0.789 | Accuracy: 0.736500 | 0.513 sec/iter\n",
      "Epoch: 323 | Batch: 009 / 011 | Total loss: 0.783 | Reg loss: 0.028 | Tree loss: 0.783 | Accuracy: 0.734000 | 0.513 sec/iter\n",
      "Epoch: 323 | Batch: 010 / 011 | Total loss: 0.797 | Reg loss: 0.028 | Tree loss: 0.797 | Accuracy: 0.720137 | 0.513 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 324 | Batch: 000 / 011 | Total loss: 0.897 | Reg loss: 0.028 | Tree loss: 0.897 | Accuracy: 0.652500 | 0.515 sec/iter\n",
      "Epoch: 324 | Batch: 001 / 011 | Total loss: 0.858 | Reg loss: 0.028 | Tree loss: 0.858 | Accuracy: 0.654000 | 0.515 sec/iter\n",
      "Epoch: 324 | Batch: 002 / 011 | Total loss: 0.843 | Reg loss: 0.028 | Tree loss: 0.843 | Accuracy: 0.677500 | 0.514 sec/iter\n",
      "Epoch: 324 | Batch: 003 / 011 | Total loss: 0.823 | Reg loss: 0.028 | Tree loss: 0.823 | Accuracy: 0.700500 | 0.514 sec/iter\n",
      "Epoch: 324 | Batch: 004 / 011 | Total loss: 0.830 | Reg loss: 0.028 | Tree loss: 0.830 | Accuracy: 0.697000 | 0.514 sec/iter\n",
      "Epoch: 324 | Batch: 005 / 011 | Total loss: 0.805 | Reg loss: 0.028 | Tree loss: 0.805 | Accuracy: 0.711000 | 0.514 sec/iter\n",
      "Epoch: 324 | Batch: 006 / 011 | Total loss: 0.777 | Reg loss: 0.028 | Tree loss: 0.777 | Accuracy: 0.720000 | 0.514 sec/iter\n",
      "Epoch: 324 | Batch: 007 / 011 | Total loss: 0.781 | Reg loss: 0.028 | Tree loss: 0.781 | Accuracy: 0.746500 | 0.514 sec/iter\n",
      "Epoch: 324 | Batch: 008 / 011 | Total loss: 0.786 | Reg loss: 0.028 | Tree loss: 0.786 | Accuracy: 0.731000 | 0.514 sec/iter\n",
      "Epoch: 324 | Batch: 009 / 011 | Total loss: 0.788 | Reg loss: 0.028 | Tree loss: 0.788 | Accuracy: 0.729500 | 0.514 sec/iter\n",
      "Epoch: 324 | Batch: 010 / 011 | Total loss: 0.778 | Reg loss: 0.028 | Tree loss: 0.778 | Accuracy: 0.723549 | 0.514 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 325 | Batch: 000 / 011 | Total loss: 0.878 | Reg loss: 0.028 | Tree loss: 0.878 | Accuracy: 0.654500 | 0.514 sec/iter\n",
      "Epoch: 325 | Batch: 001 / 011 | Total loss: 0.867 | Reg loss: 0.028 | Tree loss: 0.867 | Accuracy: 0.670000 | 0.514 sec/iter\n",
      "Epoch: 325 | Batch: 002 / 011 | Total loss: 0.843 | Reg loss: 0.028 | Tree loss: 0.843 | Accuracy: 0.700500 | 0.514 sec/iter\n",
      "Epoch: 325 | Batch: 003 / 011 | Total loss: 0.819 | Reg loss: 0.028 | Tree loss: 0.819 | Accuracy: 0.710500 | 0.514 sec/iter\n",
      "Epoch: 325 | Batch: 004 / 011 | Total loss: 0.790 | Reg loss: 0.028 | Tree loss: 0.790 | Accuracy: 0.715500 | 0.514 sec/iter\n",
      "Epoch: 325 | Batch: 005 / 011 | Total loss: 0.796 | Reg loss: 0.028 | Tree loss: 0.796 | Accuracy: 0.717500 | 0.513 sec/iter\n",
      "Epoch: 325 | Batch: 006 / 011 | Total loss: 0.787 | Reg loss: 0.028 | Tree loss: 0.787 | Accuracy: 0.717500 | 0.513 sec/iter\n",
      "Epoch: 325 | Batch: 007 / 011 | Total loss: 0.805 | Reg loss: 0.028 | Tree loss: 0.805 | Accuracy: 0.730000 | 0.513 sec/iter\n",
      "Epoch: 325 | Batch: 008 / 011 | Total loss: 0.797 | Reg loss: 0.028 | Tree loss: 0.797 | Accuracy: 0.728000 | 0.513 sec/iter\n",
      "Epoch: 325 | Batch: 009 / 011 | Total loss: 0.783 | Reg loss: 0.028 | Tree loss: 0.783 | Accuracy: 0.730500 | 0.513 sec/iter\n",
      "Epoch: 325 | Batch: 010 / 011 | Total loss: 0.764 | Reg loss: 0.028 | Tree loss: 0.764 | Accuracy: 0.737201 | 0.513 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 326 | Batch: 000 / 011 | Total loss: 0.886 | Reg loss: 0.028 | Tree loss: 0.886 | Accuracy: 0.672500 | 0.514 sec/iter\n",
      "Epoch: 326 | Batch: 001 / 011 | Total loss: 0.854 | Reg loss: 0.028 | Tree loss: 0.854 | Accuracy: 0.672000 | 0.514 sec/iter\n",
      "Epoch: 326 | Batch: 002 / 011 | Total loss: 0.850 | Reg loss: 0.028 | Tree loss: 0.850 | Accuracy: 0.667500 | 0.514 sec/iter\n",
      "Epoch: 326 | Batch: 003 / 011 | Total loss: 0.842 | Reg loss: 0.028 | Tree loss: 0.842 | Accuracy: 0.679000 | 0.514 sec/iter\n",
      "Epoch: 326 | Batch: 004 / 011 | Total loss: 0.812 | Reg loss: 0.028 | Tree loss: 0.812 | Accuracy: 0.708500 | 0.514 sec/iter\n",
      "Epoch: 326 | Batch: 005 / 011 | Total loss: 0.798 | Reg loss: 0.028 | Tree loss: 0.798 | Accuracy: 0.719500 | 0.514 sec/iter\n",
      "Epoch: 326 | Batch: 006 / 011 | Total loss: 0.782 | Reg loss: 0.028 | Tree loss: 0.782 | Accuracy: 0.719500 | 0.513 sec/iter\n",
      "Epoch: 326 | Batch: 007 / 011 | Total loss: 0.788 | Reg loss: 0.028 | Tree loss: 0.788 | Accuracy: 0.725000 | 0.513 sec/iter\n",
      "Epoch: 326 | Batch: 008 / 011 | Total loss: 0.794 | Reg loss: 0.028 | Tree loss: 0.794 | Accuracy: 0.725500 | 0.513 sec/iter\n",
      "Epoch: 326 | Batch: 009 / 011 | Total loss: 0.779 | Reg loss: 0.028 | Tree loss: 0.779 | Accuracy: 0.739000 | 0.513 sec/iter\n",
      "Epoch: 326 | Batch: 010 / 011 | Total loss: 0.761 | Reg loss: 0.028 | Tree loss: 0.761 | Accuracy: 0.733788 | 0.513 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 327 | Batch: 000 / 011 | Total loss: 0.891 | Reg loss: 0.028 | Tree loss: 0.891 | Accuracy: 0.645000 | 0.514 sec/iter\n",
      "Epoch: 327 | Batch: 001 / 011 | Total loss: 0.850 | Reg loss: 0.028 | Tree loss: 0.850 | Accuracy: 0.689000 | 0.514 sec/iter\n",
      "Epoch: 327 | Batch: 002 / 011 | Total loss: 0.840 | Reg loss: 0.028 | Tree loss: 0.840 | Accuracy: 0.689000 | 0.514 sec/iter\n",
      "Epoch: 327 | Batch: 003 / 011 | Total loss: 0.821 | Reg loss: 0.028 | Tree loss: 0.821 | Accuracy: 0.708000 | 0.514 sec/iter\n",
      "Epoch: 327 | Batch: 004 / 011 | Total loss: 0.801 | Reg loss: 0.028 | Tree loss: 0.801 | Accuracy: 0.709500 | 0.514 sec/iter\n",
      "Epoch: 327 | Batch: 005 / 011 | Total loss: 0.810 | Reg loss: 0.028 | Tree loss: 0.810 | Accuracy: 0.700000 | 0.513 sec/iter\n",
      "Epoch: 327 | Batch: 006 / 011 | Total loss: 0.790 | Reg loss: 0.028 | Tree loss: 0.790 | Accuracy: 0.726500 | 0.513 sec/iter\n",
      "Epoch: 327 | Batch: 007 / 011 | Total loss: 0.790 | Reg loss: 0.028 | Tree loss: 0.790 | Accuracy: 0.731000 | 0.513 sec/iter\n",
      "Epoch: 327 | Batch: 008 / 011 | Total loss: 0.793 | Reg loss: 0.028 | Tree loss: 0.793 | Accuracy: 0.730000 | 0.513 sec/iter\n",
      "Epoch: 327 | Batch: 009 / 011 | Total loss: 0.788 | Reg loss: 0.028 | Tree loss: 0.788 | Accuracy: 0.737000 | 0.513 sec/iter\n",
      "Epoch: 327 | Batch: 010 / 011 | Total loss: 0.773 | Reg loss: 0.028 | Tree loss: 0.773 | Accuracy: 0.754266 | 0.513 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 328 | Batch: 000 / 011 | Total loss: 0.857 | Reg loss: 0.028 | Tree loss: 0.857 | Accuracy: 0.685500 | 0.513 sec/iter\n",
      "Epoch: 328 | Batch: 001 / 011 | Total loss: 0.860 | Reg loss: 0.028 | Tree loss: 0.860 | Accuracy: 0.667000 | 0.513 sec/iter\n",
      "Epoch: 328 | Batch: 002 / 011 | Total loss: 0.865 | Reg loss: 0.028 | Tree loss: 0.865 | Accuracy: 0.678500 | 0.513 sec/iter\n",
      "Epoch: 328 | Batch: 003 / 011 | Total loss: 0.840 | Reg loss: 0.028 | Tree loss: 0.840 | Accuracy: 0.686500 | 0.513 sec/iter\n",
      "Epoch: 328 | Batch: 004 / 011 | Total loss: 0.816 | Reg loss: 0.028 | Tree loss: 0.816 | Accuracy: 0.709500 | 0.513 sec/iter\n",
      "Epoch: 328 | Batch: 005 / 011 | Total loss: 0.791 | Reg loss: 0.028 | Tree loss: 0.791 | Accuracy: 0.736500 | 0.513 sec/iter\n",
      "Epoch: 328 | Batch: 006 / 011 | Total loss: 0.784 | Reg loss: 0.028 | Tree loss: 0.784 | Accuracy: 0.721500 | 0.513 sec/iter\n",
      "Epoch: 328 | Batch: 007 / 011 | Total loss: 0.792 | Reg loss: 0.028 | Tree loss: 0.792 | Accuracy: 0.721500 | 0.513 sec/iter\n",
      "Epoch: 328 | Batch: 008 / 011 | Total loss: 0.777 | Reg loss: 0.028 | Tree loss: 0.777 | Accuracy: 0.733500 | 0.513 sec/iter\n",
      "Epoch: 328 | Batch: 009 / 011 | Total loss: 0.795 | Reg loss: 0.028 | Tree loss: 0.795 | Accuracy: 0.717000 | 0.512 sec/iter\n",
      "Epoch: 328 | Batch: 010 / 011 | Total loss: 0.768 | Reg loss: 0.028 | Tree loss: 0.768 | Accuracy: 0.706485 | 0.512 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 329 | Batch: 000 / 011 | Total loss: 0.876 | Reg loss: 0.028 | Tree loss: 0.876 | Accuracy: 0.674500 | 0.513 sec/iter\n",
      "Epoch: 329 | Batch: 001 / 011 | Total loss: 0.853 | Reg loss: 0.028 | Tree loss: 0.853 | Accuracy: 0.682500 | 0.513 sec/iter\n",
      "Epoch: 329 | Batch: 002 / 011 | Total loss: 0.842 | Reg loss: 0.028 | Tree loss: 0.842 | Accuracy: 0.684500 | 0.513 sec/iter\n",
      "Epoch: 329 | Batch: 003 / 011 | Total loss: 0.837 | Reg loss: 0.028 | Tree loss: 0.837 | Accuracy: 0.701500 | 0.513 sec/iter\n",
      "Epoch: 329 | Batch: 004 / 011 | Total loss: 0.801 | Reg loss: 0.028 | Tree loss: 0.801 | Accuracy: 0.729500 | 0.513 sec/iter\n",
      "Epoch: 329 | Batch: 005 / 011 | Total loss: 0.786 | Reg loss: 0.028 | Tree loss: 0.786 | Accuracy: 0.716500 | 0.512 sec/iter\n",
      "Epoch: 329 | Batch: 006 / 011 | Total loss: 0.804 | Reg loss: 0.028 | Tree loss: 0.804 | Accuracy: 0.721000 | 0.512 sec/iter\n",
      "Epoch: 329 | Batch: 007 / 011 | Total loss: 0.791 | Reg loss: 0.028 | Tree loss: 0.791 | Accuracy: 0.731000 | 0.512 sec/iter\n",
      "Epoch: 329 | Batch: 008 / 011 | Total loss: 0.783 | Reg loss: 0.028 | Tree loss: 0.783 | Accuracy: 0.739000 | 0.512 sec/iter\n",
      "Epoch: 329 | Batch: 009 / 011 | Total loss: 0.780 | Reg loss: 0.028 | Tree loss: 0.780 | Accuracy: 0.731500 | 0.512 sec/iter\n",
      "Epoch: 329 | Batch: 010 / 011 | Total loss: 0.770 | Reg loss: 0.028 | Tree loss: 0.770 | Accuracy: 0.740614 | 0.512 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 330 | Batch: 000 / 011 | Total loss: 0.865 | Reg loss: 0.028 | Tree loss: 0.865 | Accuracy: 0.680000 | 0.513 sec/iter\n",
      "Epoch: 330 | Batch: 001 / 011 | Total loss: 0.871 | Reg loss: 0.028 | Tree loss: 0.871 | Accuracy: 0.668500 | 0.513 sec/iter\n",
      "Epoch: 330 | Batch: 002 / 011 | Total loss: 0.862 | Reg loss: 0.028 | Tree loss: 0.862 | Accuracy: 0.670500 | 0.513 sec/iter\n",
      "Epoch: 330 | Batch: 003 / 011 | Total loss: 0.811 | Reg loss: 0.028 | Tree loss: 0.811 | Accuracy: 0.698000 | 0.513 sec/iter\n",
      "Epoch: 330 | Batch: 004 / 011 | Total loss: 0.801 | Reg loss: 0.028 | Tree loss: 0.801 | Accuracy: 0.712000 | 0.513 sec/iter\n",
      "Epoch: 330 | Batch: 005 / 011 | Total loss: 0.808 | Reg loss: 0.028 | Tree loss: 0.808 | Accuracy: 0.705000 | 0.513 sec/iter\n",
      "Epoch: 330 | Batch: 006 / 011 | Total loss: 0.788 | Reg loss: 0.028 | Tree loss: 0.788 | Accuracy: 0.718000 | 0.513 sec/iter\n",
      "Epoch: 330 | Batch: 007 / 011 | Total loss: 0.790 | Reg loss: 0.028 | Tree loss: 0.790 | Accuracy: 0.716500 | 0.513 sec/iter\n",
      "Epoch: 330 | Batch: 008 / 011 | Total loss: 0.792 | Reg loss: 0.028 | Tree loss: 0.792 | Accuracy: 0.724500 | 0.513 sec/iter\n",
      "Epoch: 330 | Batch: 009 / 011 | Total loss: 0.804 | Reg loss: 0.028 | Tree loss: 0.804 | Accuracy: 0.731000 | 0.513 sec/iter\n",
      "Epoch: 330 | Batch: 010 / 011 | Total loss: 0.727 | Reg loss: 0.028 | Tree loss: 0.727 | Accuracy: 0.767918 | 0.513 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 331 | Batch: 000 / 011 | Total loss: 0.876 | Reg loss: 0.028 | Tree loss: 0.876 | Accuracy: 0.662000 | 0.514 sec/iter\n",
      "Epoch: 331 | Batch: 001 / 011 | Total loss: 0.876 | Reg loss: 0.028 | Tree loss: 0.876 | Accuracy: 0.656500 | 0.514 sec/iter\n",
      "Epoch: 331 | Batch: 002 / 011 | Total loss: 0.867 | Reg loss: 0.028 | Tree loss: 0.867 | Accuracy: 0.670500 | 0.513 sec/iter\n",
      "Epoch: 331 | Batch: 003 / 011 | Total loss: 0.799 | Reg loss: 0.028 | Tree loss: 0.799 | Accuracy: 0.708500 | 0.513 sec/iter\n",
      "Epoch: 331 | Batch: 004 / 011 | Total loss: 0.825 | Reg loss: 0.028 | Tree loss: 0.825 | Accuracy: 0.694500 | 0.513 sec/iter\n",
      "Epoch: 331 | Batch: 005 / 011 | Total loss: 0.778 | Reg loss: 0.028 | Tree loss: 0.778 | Accuracy: 0.727000 | 0.513 sec/iter\n",
      "Epoch: 331 | Batch: 006 / 011 | Total loss: 0.794 | Reg loss: 0.028 | Tree loss: 0.794 | Accuracy: 0.714500 | 0.513 sec/iter\n",
      "Epoch: 331 | Batch: 007 / 011 | Total loss: 0.805 | Reg loss: 0.028 | Tree loss: 0.805 | Accuracy: 0.717500 | 0.513 sec/iter\n",
      "Epoch: 331 | Batch: 008 / 011 | Total loss: 0.786 | Reg loss: 0.028 | Tree loss: 0.786 | Accuracy: 0.733500 | 0.513 sec/iter\n",
      "Epoch: 331 | Batch: 009 / 011 | Total loss: 0.803 | Reg loss: 0.028 | Tree loss: 0.803 | Accuracy: 0.725500 | 0.513 sec/iter\n",
      "Epoch: 331 | Batch: 010 / 011 | Total loss: 0.743 | Reg loss: 0.028 | Tree loss: 0.743 | Accuracy: 0.750853 | 0.513 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 332 | Batch: 000 / 011 | Total loss: 0.887 | Reg loss: 0.028 | Tree loss: 0.887 | Accuracy: 0.664500 | 0.513 sec/iter\n",
      "Epoch: 332 | Batch: 001 / 011 | Total loss: 0.876 | Reg loss: 0.028 | Tree loss: 0.876 | Accuracy: 0.666500 | 0.513 sec/iter\n",
      "Epoch: 332 | Batch: 002 / 011 | Total loss: 0.834 | Reg loss: 0.028 | Tree loss: 0.834 | Accuracy: 0.706000 | 0.513 sec/iter\n",
      "Epoch: 332 | Batch: 003 / 011 | Total loss: 0.814 | Reg loss: 0.028 | Tree loss: 0.814 | Accuracy: 0.694000 | 0.513 sec/iter\n",
      "Epoch: 332 | Batch: 004 / 011 | Total loss: 0.834 | Reg loss: 0.028 | Tree loss: 0.834 | Accuracy: 0.697000 | 0.513 sec/iter\n",
      "Epoch: 332 | Batch: 005 / 011 | Total loss: 0.780 | Reg loss: 0.028 | Tree loss: 0.780 | Accuracy: 0.728500 | 0.512 sec/iter\n",
      "Epoch: 332 | Batch: 006 / 011 | Total loss: 0.781 | Reg loss: 0.028 | Tree loss: 0.781 | Accuracy: 0.723500 | 0.512 sec/iter\n",
      "Epoch: 332 | Batch: 007 / 011 | Total loss: 0.776 | Reg loss: 0.028 | Tree loss: 0.776 | Accuracy: 0.738000 | 0.512 sec/iter\n",
      "Epoch: 332 | Batch: 008 / 011 | Total loss: 0.789 | Reg loss: 0.028 | Tree loss: 0.789 | Accuracy: 0.727000 | 0.512 sec/iter\n",
      "Epoch: 332 | Batch: 009 / 011 | Total loss: 0.800 | Reg loss: 0.028 | Tree loss: 0.800 | Accuracy: 0.726000 | 0.512 sec/iter\n",
      "Epoch: 332 | Batch: 010 / 011 | Total loss: 0.795 | Reg loss: 0.028 | Tree loss: 0.795 | Accuracy: 0.713311 | 0.512 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 333 | Batch: 000 / 011 | Total loss: 0.872 | Reg loss: 0.028 | Tree loss: 0.872 | Accuracy: 0.666000 | 0.512 sec/iter\n",
      "Epoch: 333 | Batch: 001 / 011 | Total loss: 0.853 | Reg loss: 0.028 | Tree loss: 0.853 | Accuracy: 0.664500 | 0.512 sec/iter\n",
      "Epoch: 333 | Batch: 002 / 011 | Total loss: 0.845 | Reg loss: 0.028 | Tree loss: 0.845 | Accuracy: 0.687000 | 0.512 sec/iter\n",
      "Epoch: 333 | Batch: 003 / 011 | Total loss: 0.830 | Reg loss: 0.028 | Tree loss: 0.830 | Accuracy: 0.696000 | 0.512 sec/iter\n",
      "Epoch: 333 | Batch: 004 / 011 | Total loss: 0.784 | Reg loss: 0.028 | Tree loss: 0.784 | Accuracy: 0.725000 | 0.512 sec/iter\n",
      "Epoch: 333 | Batch: 005 / 011 | Total loss: 0.797 | Reg loss: 0.028 | Tree loss: 0.797 | Accuracy: 0.707000 | 0.512 sec/iter\n",
      "Epoch: 333 | Batch: 006 / 011 | Total loss: 0.802 | Reg loss: 0.028 | Tree loss: 0.802 | Accuracy: 0.718500 | 0.512 sec/iter\n",
      "Epoch: 333 | Batch: 007 / 011 | Total loss: 0.777 | Reg loss: 0.028 | Tree loss: 0.777 | Accuracy: 0.719000 | 0.512 sec/iter\n",
      "Epoch: 333 | Batch: 008 / 011 | Total loss: 0.808 | Reg loss: 0.028 | Tree loss: 0.808 | Accuracy: 0.717000 | 0.512 sec/iter\n",
      "Epoch: 333 | Batch: 009 / 011 | Total loss: 0.800 | Reg loss: 0.028 | Tree loss: 0.800 | Accuracy: 0.727500 | 0.512 sec/iter\n",
      "Epoch: 333 | Batch: 010 / 011 | Total loss: 0.822 | Reg loss: 0.028 | Tree loss: 0.822 | Accuracy: 0.709898 | 0.512 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 334 | Batch: 000 / 011 | Total loss: 0.882 | Reg loss: 0.028 | Tree loss: 0.882 | Accuracy: 0.665500 | 0.512 sec/iter\n",
      "Epoch: 334 | Batch: 001 / 011 | Total loss: 0.875 | Reg loss: 0.028 | Tree loss: 0.875 | Accuracy: 0.651000 | 0.512 sec/iter\n",
      "Epoch: 334 | Batch: 002 / 011 | Total loss: 0.864 | Reg loss: 0.028 | Tree loss: 0.864 | Accuracy: 0.668500 | 0.512 sec/iter\n",
      "Epoch: 334 | Batch: 003 / 011 | Total loss: 0.820 | Reg loss: 0.028 | Tree loss: 0.820 | Accuracy: 0.699000 | 0.512 sec/iter\n",
      "Epoch: 334 | Batch: 004 / 011 | Total loss: 0.799 | Reg loss: 0.028 | Tree loss: 0.799 | Accuracy: 0.716500 | 0.512 sec/iter\n",
      "Epoch: 334 | Batch: 005 / 011 | Total loss: 0.782 | Reg loss: 0.028 | Tree loss: 0.782 | Accuracy: 0.715000 | 0.512 sec/iter\n",
      "Epoch: 334 | Batch: 006 / 011 | Total loss: 0.806 | Reg loss: 0.028 | Tree loss: 0.806 | Accuracy: 0.708000 | 0.512 sec/iter\n",
      "Epoch: 334 | Batch: 007 / 011 | Total loss: 0.776 | Reg loss: 0.028 | Tree loss: 0.776 | Accuracy: 0.743000 | 0.512 sec/iter\n",
      "Epoch: 334 | Batch: 008 / 011 | Total loss: 0.777 | Reg loss: 0.028 | Tree loss: 0.777 | Accuracy: 0.744500 | 0.512 sec/iter\n",
      "Epoch: 334 | Batch: 009 / 011 | Total loss: 0.767 | Reg loss: 0.028 | Tree loss: 0.767 | Accuracy: 0.745500 | 0.512 sec/iter\n",
      "Epoch: 334 | Batch: 010 / 011 | Total loss: 0.862 | Reg loss: 0.028 | Tree loss: 0.862 | Accuracy: 0.706485 | 0.512 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 335 | Batch: 000 / 011 | Total loss: 0.871 | Reg loss: 0.028 | Tree loss: 0.871 | Accuracy: 0.666000 | 0.512 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 335 | Batch: 001 / 011 | Total loss: 0.861 | Reg loss: 0.028 | Tree loss: 0.861 | Accuracy: 0.667000 | 0.512 sec/iter\n",
      "Epoch: 335 | Batch: 002 / 011 | Total loss: 0.849 | Reg loss: 0.028 | Tree loss: 0.849 | Accuracy: 0.686000 | 0.511 sec/iter\n",
      "Epoch: 335 | Batch: 003 / 011 | Total loss: 0.815 | Reg loss: 0.028 | Tree loss: 0.815 | Accuracy: 0.705500 | 0.511 sec/iter\n",
      "Epoch: 335 | Batch: 004 / 011 | Total loss: 0.804 | Reg loss: 0.028 | Tree loss: 0.804 | Accuracy: 0.706500 | 0.511 sec/iter\n",
      "Epoch: 335 | Batch: 005 / 011 | Total loss: 0.815 | Reg loss: 0.028 | Tree loss: 0.815 | Accuracy: 0.708000 | 0.511 sec/iter\n",
      "Epoch: 335 | Batch: 006 / 011 | Total loss: 0.777 | Reg loss: 0.028 | Tree loss: 0.777 | Accuracy: 0.728000 | 0.511 sec/iter\n",
      "Epoch: 335 | Batch: 007 / 011 | Total loss: 0.789 | Reg loss: 0.028 | Tree loss: 0.789 | Accuracy: 0.720000 | 0.511 sec/iter\n",
      "Epoch: 335 | Batch: 008 / 011 | Total loss: 0.786 | Reg loss: 0.028 | Tree loss: 0.786 | Accuracy: 0.730000 | 0.511 sec/iter\n",
      "Epoch: 335 | Batch: 009 / 011 | Total loss: 0.788 | Reg loss: 0.028 | Tree loss: 0.788 | Accuracy: 0.737500 | 0.511 sec/iter\n",
      "Epoch: 335 | Batch: 010 / 011 | Total loss: 0.745 | Reg loss: 0.028 | Tree loss: 0.745 | Accuracy: 0.733788 | 0.511 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 336 | Batch: 000 / 011 | Total loss: 0.874 | Reg loss: 0.028 | Tree loss: 0.874 | Accuracy: 0.666000 | 0.512 sec/iter\n",
      "Epoch: 336 | Batch: 001 / 011 | Total loss: 0.835 | Reg loss: 0.028 | Tree loss: 0.835 | Accuracy: 0.688500 | 0.512 sec/iter\n",
      "Epoch: 336 | Batch: 002 / 011 | Total loss: 0.862 | Reg loss: 0.028 | Tree loss: 0.862 | Accuracy: 0.673000 | 0.512 sec/iter\n",
      "Epoch: 336 | Batch: 003 / 011 | Total loss: 0.816 | Reg loss: 0.028 | Tree loss: 0.816 | Accuracy: 0.685500 | 0.512 sec/iter\n",
      "Epoch: 336 | Batch: 004 / 011 | Total loss: 0.801 | Reg loss: 0.028 | Tree loss: 0.801 | Accuracy: 0.705500 | 0.512 sec/iter\n",
      "Epoch: 336 | Batch: 005 / 011 | Total loss: 0.790 | Reg loss: 0.028 | Tree loss: 0.790 | Accuracy: 0.711500 | 0.512 sec/iter\n",
      "Epoch: 336 | Batch: 006 / 011 | Total loss: 0.781 | Reg loss: 0.028 | Tree loss: 0.781 | Accuracy: 0.729000 | 0.512 sec/iter\n",
      "Epoch: 336 | Batch: 007 / 011 | Total loss: 0.804 | Reg loss: 0.028 | Tree loss: 0.804 | Accuracy: 0.724500 | 0.512 sec/iter\n",
      "Epoch: 336 | Batch: 008 / 011 | Total loss: 0.816 | Reg loss: 0.028 | Tree loss: 0.816 | Accuracy: 0.711500 | 0.512 sec/iter\n",
      "Epoch: 336 | Batch: 009 / 011 | Total loss: 0.772 | Reg loss: 0.028 | Tree loss: 0.772 | Accuracy: 0.753000 | 0.511 sec/iter\n",
      "Epoch: 336 | Batch: 010 / 011 | Total loss: 0.831 | Reg loss: 0.028 | Tree loss: 0.831 | Accuracy: 0.750853 | 0.511 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 337 | Batch: 000 / 011 | Total loss: 0.884 | Reg loss: 0.028 | Tree loss: 0.884 | Accuracy: 0.677000 | 0.513 sec/iter\n",
      "Epoch: 337 | Batch: 001 / 011 | Total loss: 0.857 | Reg loss: 0.028 | Tree loss: 0.857 | Accuracy: 0.689000 | 0.513 sec/iter\n",
      "Epoch: 337 | Batch: 002 / 011 | Total loss: 0.843 | Reg loss: 0.028 | Tree loss: 0.843 | Accuracy: 0.689500 | 0.513 sec/iter\n",
      "Epoch: 337 | Batch: 003 / 011 | Total loss: 0.806 | Reg loss: 0.028 | Tree loss: 0.806 | Accuracy: 0.703500 | 0.512 sec/iter\n",
      "Epoch: 337 | Batch: 004 / 011 | Total loss: 0.827 | Reg loss: 0.028 | Tree loss: 0.827 | Accuracy: 0.698500 | 0.512 sec/iter\n",
      "Epoch: 337 | Batch: 005 / 011 | Total loss: 0.794 | Reg loss: 0.028 | Tree loss: 0.794 | Accuracy: 0.712500 | 0.512 sec/iter\n",
      "Epoch: 337 | Batch: 006 / 011 | Total loss: 0.801 | Reg loss: 0.028 | Tree loss: 0.801 | Accuracy: 0.726500 | 0.512 sec/iter\n",
      "Epoch: 337 | Batch: 007 / 011 | Total loss: 0.783 | Reg loss: 0.028 | Tree loss: 0.783 | Accuracy: 0.720000 | 0.512 sec/iter\n",
      "Epoch: 337 | Batch: 008 / 011 | Total loss: 0.781 | Reg loss: 0.028 | Tree loss: 0.781 | Accuracy: 0.727000 | 0.512 sec/iter\n",
      "Epoch: 337 | Batch: 009 / 011 | Total loss: 0.792 | Reg loss: 0.028 | Tree loss: 0.792 | Accuracy: 0.731000 | 0.512 sec/iter\n",
      "Epoch: 337 | Batch: 010 / 011 | Total loss: 0.810 | Reg loss: 0.028 | Tree loss: 0.810 | Accuracy: 0.709898 | 0.512 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 338 | Batch: 000 / 011 | Total loss: 0.882 | Reg loss: 0.028 | Tree loss: 0.882 | Accuracy: 0.659000 | 0.512 sec/iter\n",
      "Epoch: 338 | Batch: 001 / 011 | Total loss: 0.867 | Reg loss: 0.028 | Tree loss: 0.867 | Accuracy: 0.676000 | 0.512 sec/iter\n",
      "Epoch: 338 | Batch: 002 / 011 | Total loss: 0.867 | Reg loss: 0.028 | Tree loss: 0.867 | Accuracy: 0.675000 | 0.512 sec/iter\n",
      "Epoch: 338 | Batch: 003 / 011 | Total loss: 0.827 | Reg loss: 0.028 | Tree loss: 0.827 | Accuracy: 0.692000 | 0.512 sec/iter\n",
      "Epoch: 338 | Batch: 004 / 011 | Total loss: 0.804 | Reg loss: 0.028 | Tree loss: 0.804 | Accuracy: 0.720000 | 0.512 sec/iter\n",
      "Epoch: 338 | Batch: 005 / 011 | Total loss: 0.780 | Reg loss: 0.028 | Tree loss: 0.780 | Accuracy: 0.715500 | 0.512 sec/iter\n",
      "Epoch: 338 | Batch: 006 / 011 | Total loss: 0.794 | Reg loss: 0.028 | Tree loss: 0.794 | Accuracy: 0.732500 | 0.512 sec/iter\n",
      "Epoch: 338 | Batch: 007 / 011 | Total loss: 0.775 | Reg loss: 0.028 | Tree loss: 0.775 | Accuracy: 0.749000 | 0.512 sec/iter\n",
      "Epoch: 338 | Batch: 008 / 011 | Total loss: 0.793 | Reg loss: 0.028 | Tree loss: 0.793 | Accuracy: 0.724500 | 0.511 sec/iter\n",
      "Epoch: 338 | Batch: 009 / 011 | Total loss: 0.788 | Reg loss: 0.028 | Tree loss: 0.788 | Accuracy: 0.728500 | 0.511 sec/iter\n",
      "Epoch: 338 | Batch: 010 / 011 | Total loss: 0.779 | Reg loss: 0.028 | Tree loss: 0.779 | Accuracy: 0.730375 | 0.511 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 339 | Batch: 000 / 011 | Total loss: 0.882 | Reg loss: 0.028 | Tree loss: 0.882 | Accuracy: 0.653500 | 0.513 sec/iter\n",
      "Epoch: 339 | Batch: 001 / 011 | Total loss: 0.860 | Reg loss: 0.028 | Tree loss: 0.860 | Accuracy: 0.669500 | 0.513 sec/iter\n",
      "Epoch: 339 | Batch: 002 / 011 | Total loss: 0.837 | Reg loss: 0.028 | Tree loss: 0.837 | Accuracy: 0.679500 | 0.512 sec/iter\n",
      "Epoch: 339 | Batch: 003 / 011 | Total loss: 0.839 | Reg loss: 0.028 | Tree loss: 0.839 | Accuracy: 0.676000 | 0.512 sec/iter\n",
      "Epoch: 339 | Batch: 004 / 011 | Total loss: 0.792 | Reg loss: 0.028 | Tree loss: 0.792 | Accuracy: 0.725500 | 0.512 sec/iter\n",
      "Epoch: 339 | Batch: 005 / 011 | Total loss: 0.787 | Reg loss: 0.028 | Tree loss: 0.787 | Accuracy: 0.725000 | 0.512 sec/iter\n",
      "Epoch: 339 | Batch: 006 / 011 | Total loss: 0.801 | Reg loss: 0.028 | Tree loss: 0.801 | Accuracy: 0.716000 | 0.512 sec/iter\n",
      "Epoch: 339 | Batch: 007 / 011 | Total loss: 0.785 | Reg loss: 0.028 | Tree loss: 0.785 | Accuracy: 0.740500 | 0.512 sec/iter\n",
      "Epoch: 339 | Batch: 008 / 011 | Total loss: 0.794 | Reg loss: 0.028 | Tree loss: 0.794 | Accuracy: 0.720500 | 0.512 sec/iter\n",
      "Epoch: 339 | Batch: 009 / 011 | Total loss: 0.775 | Reg loss: 0.028 | Tree loss: 0.775 | Accuracy: 0.742000 | 0.512 sec/iter\n",
      "Epoch: 339 | Batch: 010 / 011 | Total loss: 0.819 | Reg loss: 0.028 | Tree loss: 0.819 | Accuracy: 0.713311 | 0.512 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 340 | Batch: 000 / 011 | Total loss: 0.890 | Reg loss: 0.028 | Tree loss: 0.890 | Accuracy: 0.646500 | 0.513 sec/iter\n",
      "Epoch: 340 | Batch: 001 / 011 | Total loss: 0.857 | Reg loss: 0.028 | Tree loss: 0.857 | Accuracy: 0.677500 | 0.513 sec/iter\n",
      "Epoch: 340 | Batch: 002 / 011 | Total loss: 0.852 | Reg loss: 0.028 | Tree loss: 0.852 | Accuracy: 0.694000 | 0.513 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 340 | Batch: 003 / 011 | Total loss: 0.810 | Reg loss: 0.028 | Tree loss: 0.810 | Accuracy: 0.707000 | 0.513 sec/iter\n",
      "Epoch: 340 | Batch: 004 / 011 | Total loss: 0.798 | Reg loss: 0.028 | Tree loss: 0.798 | Accuracy: 0.724500 | 0.513 sec/iter\n",
      "Epoch: 340 | Batch: 005 / 011 | Total loss: 0.792 | Reg loss: 0.028 | Tree loss: 0.792 | Accuracy: 0.703500 | 0.512 sec/iter\n",
      "Epoch: 340 | Batch: 006 / 011 | Total loss: 0.772 | Reg loss: 0.028 | Tree loss: 0.772 | Accuracy: 0.739500 | 0.512 sec/iter\n",
      "Epoch: 340 | Batch: 007 / 011 | Total loss: 0.807 | Reg loss: 0.028 | Tree loss: 0.807 | Accuracy: 0.714500 | 0.512 sec/iter\n",
      "Epoch: 340 | Batch: 008 / 011 | Total loss: 0.819 | Reg loss: 0.028 | Tree loss: 0.819 | Accuracy: 0.721500 | 0.512 sec/iter\n",
      "Epoch: 340 | Batch: 009 / 011 | Total loss: 0.799 | Reg loss: 0.028 | Tree loss: 0.799 | Accuracy: 0.719500 | 0.512 sec/iter\n",
      "Epoch: 340 | Batch: 010 / 011 | Total loss: 0.787 | Reg loss: 0.028 | Tree loss: 0.787 | Accuracy: 0.720137 | 0.512 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 341 | Batch: 000 / 011 | Total loss: 0.884 | Reg loss: 0.028 | Tree loss: 0.884 | Accuracy: 0.666500 | 0.512 sec/iter\n",
      "Epoch: 341 | Batch: 001 / 011 | Total loss: 0.875 | Reg loss: 0.028 | Tree loss: 0.875 | Accuracy: 0.657500 | 0.512 sec/iter\n",
      "Epoch: 341 | Batch: 002 / 011 | Total loss: 0.830 | Reg loss: 0.028 | Tree loss: 0.830 | Accuracy: 0.698000 | 0.512 sec/iter\n",
      "Epoch: 341 | Batch: 003 / 011 | Total loss: 0.853 | Reg loss: 0.028 | Tree loss: 0.853 | Accuracy: 0.666000 | 0.512 sec/iter\n",
      "Epoch: 341 | Batch: 004 / 011 | Total loss: 0.819 | Reg loss: 0.028 | Tree loss: 0.819 | Accuracy: 0.708500 | 0.512 sec/iter\n",
      "Epoch: 341 | Batch: 005 / 011 | Total loss: 0.787 | Reg loss: 0.028 | Tree loss: 0.787 | Accuracy: 0.718500 | 0.512 sec/iter\n",
      "Epoch: 341 | Batch: 006 / 011 | Total loss: 0.780 | Reg loss: 0.028 | Tree loss: 0.780 | Accuracy: 0.727500 | 0.512 sec/iter\n",
      "Epoch: 341 | Batch: 007 / 011 | Total loss: 0.782 | Reg loss: 0.028 | Tree loss: 0.782 | Accuracy: 0.734500 | 0.512 sec/iter\n",
      "Epoch: 341 | Batch: 008 / 011 | Total loss: 0.790 | Reg loss: 0.028 | Tree loss: 0.790 | Accuracy: 0.722500 | 0.512 sec/iter\n",
      "Epoch: 341 | Batch: 009 / 011 | Total loss: 0.801 | Reg loss: 0.028 | Tree loss: 0.801 | Accuracy: 0.721000 | 0.512 sec/iter\n",
      "Epoch: 341 | Batch: 010 / 011 | Total loss: 0.816 | Reg loss: 0.028 | Tree loss: 0.816 | Accuracy: 0.713311 | 0.512 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 342 | Batch: 000 / 011 | Total loss: 0.868 | Reg loss: 0.028 | Tree loss: 0.868 | Accuracy: 0.661500 | 0.513 sec/iter\n",
      "Epoch: 342 | Batch: 001 / 011 | Total loss: 0.875 | Reg loss: 0.028 | Tree loss: 0.875 | Accuracy: 0.674500 | 0.513 sec/iter\n",
      "Epoch: 342 | Batch: 002 / 011 | Total loss: 0.850 | Reg loss: 0.028 | Tree loss: 0.850 | Accuracy: 0.674500 | 0.513 sec/iter\n",
      "Epoch: 342 | Batch: 003 / 011 | Total loss: 0.833 | Reg loss: 0.028 | Tree loss: 0.833 | Accuracy: 0.690000 | 0.513 sec/iter\n",
      "Epoch: 342 | Batch: 004 / 011 | Total loss: 0.817 | Reg loss: 0.028 | Tree loss: 0.817 | Accuracy: 0.722000 | 0.513 sec/iter\n",
      "Epoch: 342 | Batch: 005 / 011 | Total loss: 0.777 | Reg loss: 0.028 | Tree loss: 0.777 | Accuracy: 0.727000 | 0.513 sec/iter\n",
      "Epoch: 342 | Batch: 006 / 011 | Total loss: 0.798 | Reg loss: 0.028 | Tree loss: 0.798 | Accuracy: 0.718000 | 0.513 sec/iter\n",
      "Epoch: 342 | Batch: 007 / 011 | Total loss: 0.799 | Reg loss: 0.028 | Tree loss: 0.799 | Accuracy: 0.731500 | 0.512 sec/iter\n",
      "Epoch: 342 | Batch: 008 / 011 | Total loss: 0.780 | Reg loss: 0.028 | Tree loss: 0.780 | Accuracy: 0.743500 | 0.512 sec/iter\n",
      "Epoch: 342 | Batch: 009 / 011 | Total loss: 0.783 | Reg loss: 0.028 | Tree loss: 0.783 | Accuracy: 0.728500 | 0.512 sec/iter\n",
      "Epoch: 342 | Batch: 010 / 011 | Total loss: 0.728 | Reg loss: 0.028 | Tree loss: 0.728 | Accuracy: 0.747440 | 0.512 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 343 | Batch: 000 / 011 | Total loss: 0.884 | Reg loss: 0.028 | Tree loss: 0.884 | Accuracy: 0.663500 | 0.512 sec/iter\n",
      "Epoch: 343 | Batch: 001 / 011 | Total loss: 0.856 | Reg loss: 0.028 | Tree loss: 0.856 | Accuracy: 0.673000 | 0.512 sec/iter\n",
      "Epoch: 343 | Batch: 002 / 011 | Total loss: 0.842 | Reg loss: 0.028 | Tree loss: 0.842 | Accuracy: 0.686500 | 0.512 sec/iter\n",
      "Epoch: 343 | Batch: 003 / 011 | Total loss: 0.813 | Reg loss: 0.028 | Tree loss: 0.813 | Accuracy: 0.701000 | 0.512 sec/iter\n",
      "Epoch: 343 | Batch: 004 / 011 | Total loss: 0.800 | Reg loss: 0.028 | Tree loss: 0.800 | Accuracy: 0.718500 | 0.512 sec/iter\n",
      "Epoch: 343 | Batch: 005 / 011 | Total loss: 0.810 | Reg loss: 0.028 | Tree loss: 0.810 | Accuracy: 0.721000 | 0.512 sec/iter\n",
      "Epoch: 343 | Batch: 006 / 011 | Total loss: 0.803 | Reg loss: 0.028 | Tree loss: 0.803 | Accuracy: 0.714000 | 0.512 sec/iter\n",
      "Epoch: 343 | Batch: 007 / 011 | Total loss: 0.788 | Reg loss: 0.028 | Tree loss: 0.788 | Accuracy: 0.725500 | 0.512 sec/iter\n",
      "Epoch: 343 | Batch: 008 / 011 | Total loss: 0.812 | Reg loss: 0.028 | Tree loss: 0.812 | Accuracy: 0.709500 | 0.512 sec/iter\n",
      "Epoch: 343 | Batch: 009 / 011 | Total loss: 0.779 | Reg loss: 0.028 | Tree loss: 0.779 | Accuracy: 0.742500 | 0.512 sec/iter\n",
      "Epoch: 343 | Batch: 010 / 011 | Total loss: 0.810 | Reg loss: 0.028 | Tree loss: 0.810 | Accuracy: 0.696246 | 0.512 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 344 | Batch: 000 / 011 | Total loss: 0.894 | Reg loss: 0.028 | Tree loss: 0.894 | Accuracy: 0.649000 | 0.513 sec/iter\n",
      "Epoch: 344 | Batch: 001 / 011 | Total loss: 0.859 | Reg loss: 0.028 | Tree loss: 0.859 | Accuracy: 0.669500 | 0.513 sec/iter\n",
      "Epoch: 344 | Batch: 002 / 011 | Total loss: 0.862 | Reg loss: 0.028 | Tree loss: 0.862 | Accuracy: 0.680000 | 0.513 sec/iter\n",
      "Epoch: 344 | Batch: 003 / 011 | Total loss: 0.821 | Reg loss: 0.028 | Tree loss: 0.821 | Accuracy: 0.696500 | 0.513 sec/iter\n",
      "Epoch: 344 | Batch: 004 / 011 | Total loss: 0.799 | Reg loss: 0.028 | Tree loss: 0.799 | Accuracy: 0.718000 | 0.513 sec/iter\n",
      "Epoch: 344 | Batch: 005 / 011 | Total loss: 0.785 | Reg loss: 0.028 | Tree loss: 0.785 | Accuracy: 0.730000 | 0.512 sec/iter\n",
      "Epoch: 344 | Batch: 006 / 011 | Total loss: 0.784 | Reg loss: 0.028 | Tree loss: 0.784 | Accuracy: 0.739500 | 0.512 sec/iter\n",
      "Epoch: 344 | Batch: 007 / 011 | Total loss: 0.797 | Reg loss: 0.028 | Tree loss: 0.797 | Accuracy: 0.732500 | 0.512 sec/iter\n",
      "Epoch: 344 | Batch: 008 / 011 | Total loss: 0.791 | Reg loss: 0.028 | Tree loss: 0.791 | Accuracy: 0.722000 | 0.512 sec/iter\n",
      "Epoch: 344 | Batch: 009 / 011 | Total loss: 0.802 | Reg loss: 0.028 | Tree loss: 0.802 | Accuracy: 0.712500 | 0.512 sec/iter\n",
      "Epoch: 344 | Batch: 010 / 011 | Total loss: 0.771 | Reg loss: 0.028 | Tree loss: 0.771 | Accuracy: 0.723549 | 0.512 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 345 | Batch: 000 / 011 | Total loss: 0.882 | Reg loss: 0.028 | Tree loss: 0.882 | Accuracy: 0.668000 | 0.513 sec/iter\n",
      "Epoch: 345 | Batch: 001 / 011 | Total loss: 0.853 | Reg loss: 0.028 | Tree loss: 0.853 | Accuracy: 0.667000 | 0.513 sec/iter\n",
      "Epoch: 345 | Batch: 002 / 011 | Total loss: 0.870 | Reg loss: 0.028 | Tree loss: 0.870 | Accuracy: 0.672000 | 0.513 sec/iter\n",
      "Epoch: 345 | Batch: 003 / 011 | Total loss: 0.835 | Reg loss: 0.028 | Tree loss: 0.835 | Accuracy: 0.692000 | 0.513 sec/iter\n",
      "Epoch: 345 | Batch: 004 / 011 | Total loss: 0.786 | Reg loss: 0.028 | Tree loss: 0.786 | Accuracy: 0.712000 | 0.513 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 345 | Batch: 005 / 011 | Total loss: 0.803 | Reg loss: 0.028 | Tree loss: 0.803 | Accuracy: 0.711000 | 0.513 sec/iter\n",
      "Epoch: 345 | Batch: 006 / 011 | Total loss: 0.785 | Reg loss: 0.028 | Tree loss: 0.785 | Accuracy: 0.724500 | 0.512 sec/iter\n",
      "Epoch: 345 | Batch: 007 / 011 | Total loss: 0.780 | Reg loss: 0.028 | Tree loss: 0.780 | Accuracy: 0.715500 | 0.512 sec/iter\n",
      "Epoch: 345 | Batch: 008 / 011 | Total loss: 0.799 | Reg loss: 0.028 | Tree loss: 0.799 | Accuracy: 0.717500 | 0.512 sec/iter\n",
      "Epoch: 345 | Batch: 009 / 011 | Total loss: 0.780 | Reg loss: 0.028 | Tree loss: 0.780 | Accuracy: 0.726000 | 0.512 sec/iter\n",
      "Epoch: 345 | Batch: 010 / 011 | Total loss: 0.809 | Reg loss: 0.028 | Tree loss: 0.809 | Accuracy: 0.744027 | 0.512 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 346 | Batch: 000 / 011 | Total loss: 0.881 | Reg loss: 0.028 | Tree loss: 0.881 | Accuracy: 0.666000 | 0.512 sec/iter\n",
      "Epoch: 346 | Batch: 001 / 011 | Total loss: 0.874 | Reg loss: 0.028 | Tree loss: 0.874 | Accuracy: 0.668000 | 0.512 sec/iter\n",
      "Epoch: 346 | Batch: 002 / 011 | Total loss: 0.829 | Reg loss: 0.028 | Tree loss: 0.829 | Accuracy: 0.702500 | 0.512 sec/iter\n",
      "Epoch: 346 | Batch: 003 / 011 | Total loss: 0.818 | Reg loss: 0.028 | Tree loss: 0.818 | Accuracy: 0.693500 | 0.512 sec/iter\n",
      "Epoch: 346 | Batch: 004 / 011 | Total loss: 0.799 | Reg loss: 0.028 | Tree loss: 0.799 | Accuracy: 0.714000 | 0.512 sec/iter\n",
      "Epoch: 346 | Batch: 005 / 011 | Total loss: 0.774 | Reg loss: 0.028 | Tree loss: 0.774 | Accuracy: 0.723000 | 0.512 sec/iter\n",
      "Epoch: 346 | Batch: 006 / 011 | Total loss: 0.802 | Reg loss: 0.028 | Tree loss: 0.802 | Accuracy: 0.733000 | 0.512 sec/iter\n",
      "Epoch: 346 | Batch: 007 / 011 | Total loss: 0.795 | Reg loss: 0.028 | Tree loss: 0.795 | Accuracy: 0.722500 | 0.512 sec/iter\n",
      "Epoch: 346 | Batch: 008 / 011 | Total loss: 0.772 | Reg loss: 0.028 | Tree loss: 0.772 | Accuracy: 0.743500 | 0.512 sec/iter\n",
      "Epoch: 346 | Batch: 009 / 011 | Total loss: 0.803 | Reg loss: 0.028 | Tree loss: 0.803 | Accuracy: 0.735000 | 0.512 sec/iter\n",
      "Epoch: 346 | Batch: 010 / 011 | Total loss: 0.746 | Reg loss: 0.028 | Tree loss: 0.746 | Accuracy: 0.744027 | 0.511 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 347 | Batch: 000 / 011 | Total loss: 0.868 | Reg loss: 0.028 | Tree loss: 0.868 | Accuracy: 0.679000 | 0.512 sec/iter\n",
      "Epoch: 347 | Batch: 001 / 011 | Total loss: 0.884 | Reg loss: 0.028 | Tree loss: 0.884 | Accuracy: 0.660500 | 0.512 sec/iter\n",
      "Epoch: 347 | Batch: 002 / 011 | Total loss: 0.863 | Reg loss: 0.028 | Tree loss: 0.863 | Accuracy: 0.679000 | 0.512 sec/iter\n",
      "Epoch: 347 | Batch: 003 / 011 | Total loss: 0.812 | Reg loss: 0.028 | Tree loss: 0.812 | Accuracy: 0.705000 | 0.512 sec/iter\n",
      "Epoch: 347 | Batch: 004 / 011 | Total loss: 0.828 | Reg loss: 0.028 | Tree loss: 0.828 | Accuracy: 0.712500 | 0.512 sec/iter\n",
      "Epoch: 347 | Batch: 005 / 011 | Total loss: 0.792 | Reg loss: 0.028 | Tree loss: 0.792 | Accuracy: 0.712000 | 0.512 sec/iter\n",
      "Epoch: 347 | Batch: 006 / 011 | Total loss: 0.767 | Reg loss: 0.028 | Tree loss: 0.767 | Accuracy: 0.726000 | 0.512 sec/iter\n",
      "Epoch: 347 | Batch: 007 / 011 | Total loss: 0.783 | Reg loss: 0.028 | Tree loss: 0.783 | Accuracy: 0.726500 | 0.512 sec/iter\n",
      "Epoch: 347 | Batch: 008 / 011 | Total loss: 0.788 | Reg loss: 0.028 | Tree loss: 0.788 | Accuracy: 0.741000 | 0.511 sec/iter\n",
      "Epoch: 347 | Batch: 009 / 011 | Total loss: 0.772 | Reg loss: 0.028 | Tree loss: 0.772 | Accuracy: 0.733500 | 0.511 sec/iter\n",
      "Epoch: 347 | Batch: 010 / 011 | Total loss: 0.776 | Reg loss: 0.028 | Tree loss: 0.776 | Accuracy: 0.750853 | 0.511 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 348 | Batch: 000 / 011 | Total loss: 0.875 | Reg loss: 0.028 | Tree loss: 0.875 | Accuracy: 0.676000 | 0.512 sec/iter\n",
      "Epoch: 348 | Batch: 001 / 011 | Total loss: 0.879 | Reg loss: 0.028 | Tree loss: 0.879 | Accuracy: 0.667000 | 0.512 sec/iter\n",
      "Epoch: 348 | Batch: 002 / 011 | Total loss: 0.824 | Reg loss: 0.028 | Tree loss: 0.824 | Accuracy: 0.688500 | 0.512 sec/iter\n",
      "Epoch: 348 | Batch: 003 / 011 | Total loss: 0.824 | Reg loss: 0.028 | Tree loss: 0.824 | Accuracy: 0.691000 | 0.512 sec/iter\n",
      "Epoch: 348 | Batch: 004 / 011 | Total loss: 0.792 | Reg loss: 0.028 | Tree loss: 0.792 | Accuracy: 0.713000 | 0.512 sec/iter\n",
      "Epoch: 348 | Batch: 005 / 011 | Total loss: 0.819 | Reg loss: 0.028 | Tree loss: 0.819 | Accuracy: 0.700000 | 0.512 sec/iter\n",
      "Epoch: 348 | Batch: 006 / 011 | Total loss: 0.795 | Reg loss: 0.028 | Tree loss: 0.795 | Accuracy: 0.735500 | 0.511 sec/iter\n",
      "Epoch: 348 | Batch: 007 / 011 | Total loss: 0.769 | Reg loss: 0.028 | Tree loss: 0.769 | Accuracy: 0.737000 | 0.511 sec/iter\n",
      "Epoch: 348 | Batch: 008 / 011 | Total loss: 0.779 | Reg loss: 0.028 | Tree loss: 0.779 | Accuracy: 0.734000 | 0.511 sec/iter\n",
      "Epoch: 348 | Batch: 009 / 011 | Total loss: 0.792 | Reg loss: 0.028 | Tree loss: 0.792 | Accuracy: 0.731500 | 0.511 sec/iter\n",
      "Epoch: 348 | Batch: 010 / 011 | Total loss: 0.861 | Reg loss: 0.028 | Tree loss: 0.861 | Accuracy: 0.713311 | 0.511 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 349 | Batch: 000 / 011 | Total loss: 0.883 | Reg loss: 0.028 | Tree loss: 0.883 | Accuracy: 0.658000 | 0.512 sec/iter\n",
      "Epoch: 349 | Batch: 001 / 011 | Total loss: 0.851 | Reg loss: 0.028 | Tree loss: 0.851 | Accuracy: 0.675000 | 0.512 sec/iter\n",
      "Epoch: 349 | Batch: 002 / 011 | Total loss: 0.838 | Reg loss: 0.028 | Tree loss: 0.838 | Accuracy: 0.695500 | 0.512 sec/iter\n",
      "Epoch: 349 | Batch: 003 / 011 | Total loss: 0.819 | Reg loss: 0.028 | Tree loss: 0.819 | Accuracy: 0.697000 | 0.512 sec/iter\n",
      "Epoch: 349 | Batch: 004 / 011 | Total loss: 0.832 | Reg loss: 0.028 | Tree loss: 0.832 | Accuracy: 0.708000 | 0.512 sec/iter\n",
      "Epoch: 349 | Batch: 005 / 011 | Total loss: 0.791 | Reg loss: 0.028 | Tree loss: 0.791 | Accuracy: 0.729500 | 0.512 sec/iter\n",
      "Epoch: 349 | Batch: 006 / 011 | Total loss: 0.792 | Reg loss: 0.028 | Tree loss: 0.792 | Accuracy: 0.710000 | 0.512 sec/iter\n",
      "Epoch: 349 | Batch: 007 / 011 | Total loss: 0.777 | Reg loss: 0.028 | Tree loss: 0.777 | Accuracy: 0.727500 | 0.512 sec/iter\n",
      "Epoch: 349 | Batch: 008 / 011 | Total loss: 0.778 | Reg loss: 0.028 | Tree loss: 0.778 | Accuracy: 0.725500 | 0.512 sec/iter\n",
      "Epoch: 349 | Batch: 009 / 011 | Total loss: 0.799 | Reg loss: 0.028 | Tree loss: 0.799 | Accuracy: 0.722500 | 0.512 sec/iter\n",
      "Epoch: 349 | Batch: 010 / 011 | Total loss: 0.778 | Reg loss: 0.028 | Tree loss: 0.778 | Accuracy: 0.750853 | 0.512 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 350 | Batch: 000 / 011 | Total loss: 0.889 | Reg loss: 0.028 | Tree loss: 0.889 | Accuracy: 0.654500 | 0.512 sec/iter\n",
      "Epoch: 350 | Batch: 001 / 011 | Total loss: 0.854 | Reg loss: 0.028 | Tree loss: 0.854 | Accuracy: 0.679500 | 0.512 sec/iter\n",
      "Epoch: 350 | Batch: 002 / 011 | Total loss: 0.834 | Reg loss: 0.028 | Tree loss: 0.834 | Accuracy: 0.684500 | 0.512 sec/iter\n",
      "Epoch: 350 | Batch: 003 / 011 | Total loss: 0.815 | Reg loss: 0.028 | Tree loss: 0.815 | Accuracy: 0.702500 | 0.512 sec/iter\n",
      "Epoch: 350 | Batch: 004 / 011 | Total loss: 0.788 | Reg loss: 0.028 | Tree loss: 0.788 | Accuracy: 0.718500 | 0.512 sec/iter\n",
      "Epoch: 350 | Batch: 005 / 011 | Total loss: 0.796 | Reg loss: 0.028 | Tree loss: 0.796 | Accuracy: 0.726500 | 0.511 sec/iter\n",
      "Epoch: 350 | Batch: 006 / 011 | Total loss: 0.792 | Reg loss: 0.028 | Tree loss: 0.792 | Accuracy: 0.730000 | 0.511 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 350 | Batch: 007 / 011 | Total loss: 0.787 | Reg loss: 0.028 | Tree loss: 0.787 | Accuracy: 0.741500 | 0.511 sec/iter\n",
      "Epoch: 350 | Batch: 008 / 011 | Total loss: 0.777 | Reg loss: 0.028 | Tree loss: 0.777 | Accuracy: 0.731500 | 0.511 sec/iter\n",
      "Epoch: 350 | Batch: 009 / 011 | Total loss: 0.816 | Reg loss: 0.028 | Tree loss: 0.816 | Accuracy: 0.706500 | 0.511 sec/iter\n",
      "Epoch: 350 | Batch: 010 / 011 | Total loss: 0.830 | Reg loss: 0.028 | Tree loss: 0.830 | Accuracy: 0.716724 | 0.511 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 351 | Batch: 000 / 011 | Total loss: 0.876 | Reg loss: 0.028 | Tree loss: 0.876 | Accuracy: 0.664500 | 0.512 sec/iter\n",
      "Epoch: 351 | Batch: 001 / 011 | Total loss: 0.852 | Reg loss: 0.028 | Tree loss: 0.852 | Accuracy: 0.678000 | 0.512 sec/iter\n",
      "Epoch: 351 | Batch: 002 / 011 | Total loss: 0.844 | Reg loss: 0.028 | Tree loss: 0.844 | Accuracy: 0.692500 | 0.512 sec/iter\n",
      "Epoch: 351 | Batch: 003 / 011 | Total loss: 0.816 | Reg loss: 0.028 | Tree loss: 0.816 | Accuracy: 0.689000 | 0.512 sec/iter\n",
      "Epoch: 351 | Batch: 004 / 011 | Total loss: 0.817 | Reg loss: 0.028 | Tree loss: 0.817 | Accuracy: 0.701000 | 0.511 sec/iter\n",
      "Epoch: 351 | Batch: 005 / 011 | Total loss: 0.808 | Reg loss: 0.028 | Tree loss: 0.808 | Accuracy: 0.715500 | 0.511 sec/iter\n",
      "Epoch: 351 | Batch: 006 / 011 | Total loss: 0.799 | Reg loss: 0.028 | Tree loss: 0.799 | Accuracy: 0.720500 | 0.511 sec/iter\n",
      "Epoch: 351 | Batch: 007 / 011 | Total loss: 0.785 | Reg loss: 0.028 | Tree loss: 0.785 | Accuracy: 0.720000 | 0.511 sec/iter\n",
      "Epoch: 351 | Batch: 008 / 011 | Total loss: 0.820 | Reg loss: 0.028 | Tree loss: 0.820 | Accuracy: 0.708500 | 0.511 sec/iter\n",
      "Epoch: 351 | Batch: 009 / 011 | Total loss: 0.779 | Reg loss: 0.028 | Tree loss: 0.779 | Accuracy: 0.721500 | 0.511 sec/iter\n",
      "Epoch: 351 | Batch: 010 / 011 | Total loss: 0.805 | Reg loss: 0.028 | Tree loss: 0.805 | Accuracy: 0.689420 | 0.511 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 352 | Batch: 000 / 011 | Total loss: 0.869 | Reg loss: 0.028 | Tree loss: 0.869 | Accuracy: 0.668500 | 0.512 sec/iter\n",
      "Epoch: 352 | Batch: 001 / 011 | Total loss: 0.842 | Reg loss: 0.028 | Tree loss: 0.842 | Accuracy: 0.674500 | 0.512 sec/iter\n",
      "Epoch: 352 | Batch: 002 / 011 | Total loss: 0.859 | Reg loss: 0.028 | Tree loss: 0.859 | Accuracy: 0.678000 | 0.511 sec/iter\n",
      "Epoch: 352 | Batch: 003 / 011 | Total loss: 0.824 | Reg loss: 0.028 | Tree loss: 0.824 | Accuracy: 0.694000 | 0.511 sec/iter\n",
      "Epoch: 352 | Batch: 004 / 011 | Total loss: 0.801 | Reg loss: 0.028 | Tree loss: 0.801 | Accuracy: 0.708500 | 0.511 sec/iter\n",
      "Epoch: 352 | Batch: 005 / 011 | Total loss: 0.796 | Reg loss: 0.028 | Tree loss: 0.796 | Accuracy: 0.722500 | 0.511 sec/iter\n",
      "Epoch: 352 | Batch: 006 / 011 | Total loss: 0.774 | Reg loss: 0.028 | Tree loss: 0.774 | Accuracy: 0.733500 | 0.511 sec/iter\n",
      "Epoch: 352 | Batch: 007 / 011 | Total loss: 0.807 | Reg loss: 0.028 | Tree loss: 0.807 | Accuracy: 0.724500 | 0.511 sec/iter\n",
      "Epoch: 352 | Batch: 008 / 011 | Total loss: 0.783 | Reg loss: 0.028 | Tree loss: 0.783 | Accuracy: 0.735500 | 0.511 sec/iter\n",
      "Epoch: 352 | Batch: 009 / 011 | Total loss: 0.803 | Reg loss: 0.028 | Tree loss: 0.803 | Accuracy: 0.731500 | 0.511 sec/iter\n",
      "Epoch: 352 | Batch: 010 / 011 | Total loss: 0.817 | Reg loss: 0.028 | Tree loss: 0.817 | Accuracy: 0.726962 | 0.511 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 353 | Batch: 000 / 011 | Total loss: 0.889 | Reg loss: 0.028 | Tree loss: 0.889 | Accuracy: 0.658500 | 0.511 sec/iter\n",
      "Epoch: 353 | Batch: 001 / 011 | Total loss: 0.872 | Reg loss: 0.028 | Tree loss: 0.872 | Accuracy: 0.663000 | 0.511 sec/iter\n",
      "Epoch: 353 | Batch: 002 / 011 | Total loss: 0.843 | Reg loss: 0.028 | Tree loss: 0.843 | Accuracy: 0.694000 | 0.511 sec/iter\n",
      "Epoch: 353 | Batch: 003 / 011 | Total loss: 0.812 | Reg loss: 0.028 | Tree loss: 0.812 | Accuracy: 0.708000 | 0.511 sec/iter\n",
      "Epoch: 353 | Batch: 004 / 011 | Total loss: 0.809 | Reg loss: 0.028 | Tree loss: 0.809 | Accuracy: 0.704000 | 0.511 sec/iter\n",
      "Epoch: 353 | Batch: 005 / 011 | Total loss: 0.801 | Reg loss: 0.028 | Tree loss: 0.801 | Accuracy: 0.719500 | 0.511 sec/iter\n",
      "Epoch: 353 | Batch: 006 / 011 | Total loss: 0.795 | Reg loss: 0.028 | Tree loss: 0.795 | Accuracy: 0.723500 | 0.511 sec/iter\n",
      "Epoch: 353 | Batch: 007 / 011 | Total loss: 0.783 | Reg loss: 0.028 | Tree loss: 0.783 | Accuracy: 0.728500 | 0.511 sec/iter\n",
      "Epoch: 353 | Batch: 008 / 011 | Total loss: 0.789 | Reg loss: 0.028 | Tree loss: 0.789 | Accuracy: 0.727000 | 0.511 sec/iter\n",
      "Epoch: 353 | Batch: 009 / 011 | Total loss: 0.788 | Reg loss: 0.028 | Tree loss: 0.788 | Accuracy: 0.733000 | 0.51 sec/iter\n",
      "Epoch: 353 | Batch: 010 / 011 | Total loss: 0.721 | Reg loss: 0.028 | Tree loss: 0.721 | Accuracy: 0.764505 | 0.51 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 354 | Batch: 000 / 011 | Total loss: 0.866 | Reg loss: 0.028 | Tree loss: 0.866 | Accuracy: 0.665500 | 0.511 sec/iter\n",
      "Epoch: 354 | Batch: 001 / 011 | Total loss: 0.852 | Reg loss: 0.028 | Tree loss: 0.852 | Accuracy: 0.670500 | 0.511 sec/iter\n",
      "Epoch: 354 | Batch: 002 / 011 | Total loss: 0.844 | Reg loss: 0.028 | Tree loss: 0.844 | Accuracy: 0.692500 | 0.511 sec/iter\n",
      "Epoch: 354 | Batch: 003 / 011 | Total loss: 0.813 | Reg loss: 0.028 | Tree loss: 0.813 | Accuracy: 0.701000 | 0.511 sec/iter\n",
      "Epoch: 354 | Batch: 004 / 011 | Total loss: 0.800 | Reg loss: 0.028 | Tree loss: 0.800 | Accuracy: 0.722000 | 0.51 sec/iter\n",
      "Epoch: 354 | Batch: 005 / 011 | Total loss: 0.809 | Reg loss: 0.028 | Tree loss: 0.809 | Accuracy: 0.715000 | 0.51 sec/iter\n",
      "Epoch: 354 | Batch: 006 / 011 | Total loss: 0.809 | Reg loss: 0.028 | Tree loss: 0.809 | Accuracy: 0.715500 | 0.51 sec/iter\n",
      "Epoch: 354 | Batch: 007 / 011 | Total loss: 0.791 | Reg loss: 0.028 | Tree loss: 0.791 | Accuracy: 0.732500 | 0.51 sec/iter\n",
      "Epoch: 354 | Batch: 008 / 011 | Total loss: 0.796 | Reg loss: 0.028 | Tree loss: 0.796 | Accuracy: 0.731500 | 0.51 sec/iter\n",
      "Epoch: 354 | Batch: 009 / 011 | Total loss: 0.773 | Reg loss: 0.028 | Tree loss: 0.773 | Accuracy: 0.727500 | 0.51 sec/iter\n",
      "Epoch: 354 | Batch: 010 / 011 | Total loss: 0.811 | Reg loss: 0.028 | Tree loss: 0.811 | Accuracy: 0.740614 | 0.51 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 355 | Batch: 000 / 011 | Total loss: 0.863 | Reg loss: 0.028 | Tree loss: 0.863 | Accuracy: 0.676500 | 0.511 sec/iter\n",
      "Epoch: 355 | Batch: 001 / 011 | Total loss: 0.858 | Reg loss: 0.028 | Tree loss: 0.858 | Accuracy: 0.668000 | 0.511 sec/iter\n",
      "Epoch: 355 | Batch: 002 / 011 | Total loss: 0.827 | Reg loss: 0.028 | Tree loss: 0.827 | Accuracy: 0.712500 | 0.511 sec/iter\n",
      "Epoch: 355 | Batch: 003 / 011 | Total loss: 0.831 | Reg loss: 0.028 | Tree loss: 0.831 | Accuracy: 0.683000 | 0.511 sec/iter\n",
      "Epoch: 355 | Batch: 004 / 011 | Total loss: 0.819 | Reg loss: 0.028 | Tree loss: 0.819 | Accuracy: 0.686500 | 0.511 sec/iter\n",
      "Epoch: 355 | Batch: 005 / 011 | Total loss: 0.804 | Reg loss: 0.028 | Tree loss: 0.804 | Accuracy: 0.713000 | 0.511 sec/iter\n",
      "Epoch: 355 | Batch: 006 / 011 | Total loss: 0.790 | Reg loss: 0.028 | Tree loss: 0.790 | Accuracy: 0.717000 | 0.511 sec/iter\n",
      "Epoch: 355 | Batch: 007 / 011 | Total loss: 0.782 | Reg loss: 0.028 | Tree loss: 0.782 | Accuracy: 0.718000 | 0.511 sec/iter\n",
      "Epoch: 355 | Batch: 008 / 011 | Total loss: 0.824 | Reg loss: 0.028 | Tree loss: 0.824 | Accuracy: 0.700000 | 0.511 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 355 | Batch: 009 / 011 | Total loss: 0.761 | Reg loss: 0.028 | Tree loss: 0.761 | Accuracy: 0.735500 | 0.511 sec/iter\n",
      "Epoch: 355 | Batch: 010 / 011 | Total loss: 0.798 | Reg loss: 0.028 | Tree loss: 0.798 | Accuracy: 0.740614 | 0.511 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 356 | Batch: 000 / 011 | Total loss: 0.893 | Reg loss: 0.028 | Tree loss: 0.893 | Accuracy: 0.649000 | 0.511 sec/iter\n",
      "Epoch: 356 | Batch: 001 / 011 | Total loss: 0.881 | Reg loss: 0.028 | Tree loss: 0.881 | Accuracy: 0.670500 | 0.511 sec/iter\n",
      "Epoch: 356 | Batch: 002 / 011 | Total loss: 0.826 | Reg loss: 0.028 | Tree loss: 0.826 | Accuracy: 0.688000 | 0.511 sec/iter\n",
      "Epoch: 356 | Batch: 003 / 011 | Total loss: 0.818 | Reg loss: 0.028 | Tree loss: 0.818 | Accuracy: 0.697000 | 0.51 sec/iter\n",
      "Epoch: 356 | Batch: 004 / 011 | Total loss: 0.790 | Reg loss: 0.028 | Tree loss: 0.790 | Accuracy: 0.731000 | 0.51 sec/iter\n",
      "Epoch: 356 | Batch: 005 / 011 | Total loss: 0.793 | Reg loss: 0.028 | Tree loss: 0.793 | Accuracy: 0.719500 | 0.51 sec/iter\n",
      "Epoch: 356 | Batch: 006 / 011 | Total loss: 0.781 | Reg loss: 0.028 | Tree loss: 0.781 | Accuracy: 0.741000 | 0.51 sec/iter\n",
      "Epoch: 356 | Batch: 007 / 011 | Total loss: 0.786 | Reg loss: 0.028 | Tree loss: 0.786 | Accuracy: 0.732500 | 0.51 sec/iter\n",
      "Epoch: 356 | Batch: 008 / 011 | Total loss: 0.789 | Reg loss: 0.028 | Tree loss: 0.789 | Accuracy: 0.742500 | 0.51 sec/iter\n",
      "Epoch: 356 | Batch: 009 / 011 | Total loss: 0.783 | Reg loss: 0.028 | Tree loss: 0.783 | Accuracy: 0.743500 | 0.51 sec/iter\n",
      "Epoch: 356 | Batch: 010 / 011 | Total loss: 0.788 | Reg loss: 0.028 | Tree loss: 0.788 | Accuracy: 0.730375 | 0.51 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 357 | Batch: 000 / 011 | Total loss: 0.867 | Reg loss: 0.028 | Tree loss: 0.867 | Accuracy: 0.666500 | 0.511 sec/iter\n",
      "Epoch: 357 | Batch: 001 / 011 | Total loss: 0.860 | Reg loss: 0.028 | Tree loss: 0.860 | Accuracy: 0.656500 | 0.511 sec/iter\n",
      "Epoch: 357 | Batch: 002 / 011 | Total loss: 0.845 | Reg loss: 0.028 | Tree loss: 0.845 | Accuracy: 0.687000 | 0.511 sec/iter\n",
      "Epoch: 357 | Batch: 003 / 011 | Total loss: 0.823 | Reg loss: 0.028 | Tree loss: 0.823 | Accuracy: 0.697000 | 0.511 sec/iter\n",
      "Epoch: 357 | Batch: 004 / 011 | Total loss: 0.809 | Reg loss: 0.028 | Tree loss: 0.809 | Accuracy: 0.707500 | 0.511 sec/iter\n",
      "Epoch: 357 | Batch: 005 / 011 | Total loss: 0.778 | Reg loss: 0.028 | Tree loss: 0.778 | Accuracy: 0.718000 | 0.511 sec/iter\n",
      "Epoch: 357 | Batch: 006 / 011 | Total loss: 0.795 | Reg loss: 0.028 | Tree loss: 0.795 | Accuracy: 0.726000 | 0.511 sec/iter\n",
      "Epoch: 357 | Batch: 007 / 011 | Total loss: 0.774 | Reg loss: 0.028 | Tree loss: 0.774 | Accuracy: 0.732500 | 0.511 sec/iter\n",
      "Epoch: 357 | Batch: 008 / 011 | Total loss: 0.790 | Reg loss: 0.028 | Tree loss: 0.790 | Accuracy: 0.729500 | 0.511 sec/iter\n",
      "Epoch: 357 | Batch: 009 / 011 | Total loss: 0.819 | Reg loss: 0.028 | Tree loss: 0.819 | Accuracy: 0.717000 | 0.51 sec/iter\n",
      "Epoch: 357 | Batch: 010 / 011 | Total loss: 0.732 | Reg loss: 0.028 | Tree loss: 0.732 | Accuracy: 0.747440 | 0.51 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 358 | Batch: 000 / 011 | Total loss: 0.895 | Reg loss: 0.028 | Tree loss: 0.895 | Accuracy: 0.648000 | 0.512 sec/iter\n",
      "Epoch: 358 | Batch: 001 / 011 | Total loss: 0.865 | Reg loss: 0.028 | Tree loss: 0.865 | Accuracy: 0.671500 | 0.512 sec/iter\n",
      "Epoch: 358 | Batch: 002 / 011 | Total loss: 0.836 | Reg loss: 0.028 | Tree loss: 0.836 | Accuracy: 0.693500 | 0.511 sec/iter\n",
      "Epoch: 358 | Batch: 003 / 011 | Total loss: 0.824 | Reg loss: 0.028 | Tree loss: 0.824 | Accuracy: 0.695000 | 0.511 sec/iter\n",
      "Epoch: 358 | Batch: 004 / 011 | Total loss: 0.790 | Reg loss: 0.028 | Tree loss: 0.790 | Accuracy: 0.715500 | 0.511 sec/iter\n",
      "Epoch: 358 | Batch: 005 / 011 | Total loss: 0.789 | Reg loss: 0.028 | Tree loss: 0.789 | Accuracy: 0.719000 | 0.511 sec/iter\n",
      "Epoch: 358 | Batch: 006 / 011 | Total loss: 0.803 | Reg loss: 0.028 | Tree loss: 0.803 | Accuracy: 0.705500 | 0.511 sec/iter\n",
      "Epoch: 358 | Batch: 007 / 011 | Total loss: 0.779 | Reg loss: 0.028 | Tree loss: 0.779 | Accuracy: 0.714500 | 0.511 sec/iter\n",
      "Epoch: 358 | Batch: 008 / 011 | Total loss: 0.801 | Reg loss: 0.028 | Tree loss: 0.801 | Accuracy: 0.724500 | 0.511 sec/iter\n",
      "Epoch: 358 | Batch: 009 / 011 | Total loss: 0.776 | Reg loss: 0.028 | Tree loss: 0.776 | Accuracy: 0.728000 | 0.511 sec/iter\n",
      "Epoch: 358 | Batch: 010 / 011 | Total loss: 0.721 | Reg loss: 0.028 | Tree loss: 0.721 | Accuracy: 0.791809 | 0.511 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 359 | Batch: 000 / 011 | Total loss: 0.884 | Reg loss: 0.028 | Tree loss: 0.884 | Accuracy: 0.667000 | 0.511 sec/iter\n",
      "Epoch: 359 | Batch: 001 / 011 | Total loss: 0.873 | Reg loss: 0.028 | Tree loss: 0.873 | Accuracy: 0.664000 | 0.511 sec/iter\n",
      "Epoch: 359 | Batch: 002 / 011 | Total loss: 0.851 | Reg loss: 0.028 | Tree loss: 0.851 | Accuracy: 0.686000 | 0.511 sec/iter\n",
      "Epoch: 359 | Batch: 003 / 011 | Total loss: 0.799 | Reg loss: 0.028 | Tree loss: 0.799 | Accuracy: 0.697000 | 0.511 sec/iter\n",
      "Epoch: 359 | Batch: 004 / 011 | Total loss: 0.792 | Reg loss: 0.028 | Tree loss: 0.792 | Accuracy: 0.721000 | 0.511 sec/iter\n",
      "Epoch: 359 | Batch: 005 / 011 | Total loss: 0.816 | Reg loss: 0.028 | Tree loss: 0.816 | Accuracy: 0.718000 | 0.511 sec/iter\n",
      "Epoch: 359 | Batch: 006 / 011 | Total loss: 0.755 | Reg loss: 0.028 | Tree loss: 0.755 | Accuracy: 0.751500 | 0.511 sec/iter\n",
      "Epoch: 359 | Batch: 007 / 011 | Total loss: 0.797 | Reg loss: 0.028 | Tree loss: 0.797 | Accuracy: 0.733000 | 0.511 sec/iter\n",
      "Epoch: 359 | Batch: 008 / 011 | Total loss: 0.782 | Reg loss: 0.028 | Tree loss: 0.782 | Accuracy: 0.735000 | 0.51 sec/iter\n",
      "Epoch: 359 | Batch: 009 / 011 | Total loss: 0.776 | Reg loss: 0.028 | Tree loss: 0.776 | Accuracy: 0.739500 | 0.51 sec/iter\n",
      "Epoch: 359 | Batch: 010 / 011 | Total loss: 0.822 | Reg loss: 0.028 | Tree loss: 0.822 | Accuracy: 0.737201 | 0.51 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 360 | Batch: 000 / 011 | Total loss: 0.876 | Reg loss: 0.028 | Tree loss: 0.876 | Accuracy: 0.665500 | 0.511 sec/iter\n",
      "Epoch: 360 | Batch: 001 / 011 | Total loss: 0.869 | Reg loss: 0.028 | Tree loss: 0.869 | Accuracy: 0.678500 | 0.511 sec/iter\n",
      "Epoch: 360 | Batch: 002 / 011 | Total loss: 0.841 | Reg loss: 0.028 | Tree loss: 0.841 | Accuracy: 0.691000 | 0.511 sec/iter\n",
      "Epoch: 360 | Batch: 003 / 011 | Total loss: 0.814 | Reg loss: 0.028 | Tree loss: 0.814 | Accuracy: 0.701500 | 0.511 sec/iter\n",
      "Epoch: 360 | Batch: 004 / 011 | Total loss: 0.823 | Reg loss: 0.028 | Tree loss: 0.823 | Accuracy: 0.689500 | 0.511 sec/iter\n",
      "Epoch: 360 | Batch: 005 / 011 | Total loss: 0.796 | Reg loss: 0.028 | Tree loss: 0.796 | Accuracy: 0.710000 | 0.511 sec/iter\n",
      "Epoch: 360 | Batch: 006 / 011 | Total loss: 0.792 | Reg loss: 0.028 | Tree loss: 0.792 | Accuracy: 0.716500 | 0.511 sec/iter\n",
      "Epoch: 360 | Batch: 007 / 011 | Total loss: 0.807 | Reg loss: 0.028 | Tree loss: 0.807 | Accuracy: 0.712500 | 0.511 sec/iter\n",
      "Epoch: 360 | Batch: 008 / 011 | Total loss: 0.786 | Reg loss: 0.028 | Tree loss: 0.786 | Accuracy: 0.721000 | 0.511 sec/iter\n",
      "Epoch: 360 | Batch: 009 / 011 | Total loss: 0.771 | Reg loss: 0.028 | Tree loss: 0.771 | Accuracy: 0.743000 | 0.511 sec/iter\n",
      "Epoch: 360 | Batch: 010 / 011 | Total loss: 0.723 | Reg loss: 0.028 | Tree loss: 0.723 | Accuracy: 0.744027 | 0.511 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 361 | Batch: 000 / 011 | Total loss: 0.859 | Reg loss: 0.028 | Tree loss: 0.859 | Accuracy: 0.671500 | 0.511 sec/iter\n",
      "Epoch: 361 | Batch: 001 / 011 | Total loss: 0.848 | Reg loss: 0.028 | Tree loss: 0.848 | Accuracy: 0.668500 | 0.511 sec/iter\n",
      "Epoch: 361 | Batch: 002 / 011 | Total loss: 0.833 | Reg loss: 0.028 | Tree loss: 0.833 | Accuracy: 0.689500 | 0.511 sec/iter\n",
      "Epoch: 361 | Batch: 003 / 011 | Total loss: 0.806 | Reg loss: 0.028 | Tree loss: 0.806 | Accuracy: 0.717500 | 0.511 sec/iter\n",
      "Epoch: 361 | Batch: 004 / 011 | Total loss: 0.794 | Reg loss: 0.028 | Tree loss: 0.794 | Accuracy: 0.724000 | 0.511 sec/iter\n",
      "Epoch: 361 | Batch: 005 / 011 | Total loss: 0.808 | Reg loss: 0.028 | Tree loss: 0.808 | Accuracy: 0.695000 | 0.511 sec/iter\n",
      "Epoch: 361 | Batch: 006 / 011 | Total loss: 0.812 | Reg loss: 0.028 | Tree loss: 0.812 | Accuracy: 0.704500 | 0.511 sec/iter\n",
      "Epoch: 361 | Batch: 007 / 011 | Total loss: 0.823 | Reg loss: 0.028 | Tree loss: 0.823 | Accuracy: 0.714000 | 0.511 sec/iter\n",
      "Epoch: 361 | Batch: 008 / 011 | Total loss: 0.820 | Reg loss: 0.028 | Tree loss: 0.820 | Accuracy: 0.710500 | 0.511 sec/iter\n",
      "Epoch: 361 | Batch: 009 / 011 | Total loss: 0.805 | Reg loss: 0.028 | Tree loss: 0.805 | Accuracy: 0.724000 | 0.511 sec/iter\n",
      "Epoch: 361 | Batch: 010 / 011 | Total loss: 0.766 | Reg loss: 0.028 | Tree loss: 0.766 | Accuracy: 0.744027 | 0.511 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 362 | Batch: 000 / 011 | Total loss: 0.862 | Reg loss: 0.028 | Tree loss: 0.862 | Accuracy: 0.675500 | 0.511 sec/iter\n",
      "Epoch: 362 | Batch: 001 / 011 | Total loss: 0.859 | Reg loss: 0.028 | Tree loss: 0.859 | Accuracy: 0.675500 | 0.511 sec/iter\n",
      "Epoch: 362 | Batch: 002 / 011 | Total loss: 0.858 | Reg loss: 0.028 | Tree loss: 0.858 | Accuracy: 0.672000 | 0.511 sec/iter\n",
      "Epoch: 362 | Batch: 003 / 011 | Total loss: 0.818 | Reg loss: 0.028 | Tree loss: 0.818 | Accuracy: 0.700000 | 0.511 sec/iter\n",
      "Epoch: 362 | Batch: 004 / 011 | Total loss: 0.831 | Reg loss: 0.028 | Tree loss: 0.831 | Accuracy: 0.708000 | 0.511 sec/iter\n",
      "Epoch: 362 | Batch: 005 / 011 | Total loss: 0.804 | Reg loss: 0.028 | Tree loss: 0.804 | Accuracy: 0.708000 | 0.511 sec/iter\n",
      "Epoch: 362 | Batch: 006 / 011 | Total loss: 0.782 | Reg loss: 0.028 | Tree loss: 0.782 | Accuracy: 0.731500 | 0.511 sec/iter\n",
      "Epoch: 362 | Batch: 007 / 011 | Total loss: 0.790 | Reg loss: 0.028 | Tree loss: 0.790 | Accuracy: 0.721500 | 0.511 sec/iter\n",
      "Epoch: 362 | Batch: 008 / 011 | Total loss: 0.813 | Reg loss: 0.028 | Tree loss: 0.813 | Accuracy: 0.719500 | 0.511 sec/iter\n",
      "Epoch: 362 | Batch: 009 / 011 | Total loss: 0.772 | Reg loss: 0.028 | Tree loss: 0.772 | Accuracy: 0.752000 | 0.511 sec/iter\n",
      "Epoch: 362 | Batch: 010 / 011 | Total loss: 0.771 | Reg loss: 0.028 | Tree loss: 0.771 | Accuracy: 0.716724 | 0.511 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 363 | Batch: 000 / 011 | Total loss: 0.838 | Reg loss: 0.028 | Tree loss: 0.838 | Accuracy: 0.683500 | 0.512 sec/iter\n",
      "Epoch: 363 | Batch: 001 / 011 | Total loss: 0.867 | Reg loss: 0.028 | Tree loss: 0.867 | Accuracy: 0.671500 | 0.512 sec/iter\n",
      "Epoch: 363 | Batch: 002 / 011 | Total loss: 0.832 | Reg loss: 0.028 | Tree loss: 0.832 | Accuracy: 0.681500 | 0.512 sec/iter\n",
      "Epoch: 363 | Batch: 003 / 011 | Total loss: 0.832 | Reg loss: 0.028 | Tree loss: 0.832 | Accuracy: 0.686000 | 0.512 sec/iter\n",
      "Epoch: 363 | Batch: 004 / 011 | Total loss: 0.796 | Reg loss: 0.028 | Tree loss: 0.796 | Accuracy: 0.708500 | 0.512 sec/iter\n",
      "Epoch: 363 | Batch: 005 / 011 | Total loss: 0.796 | Reg loss: 0.028 | Tree loss: 0.796 | Accuracy: 0.707000 | 0.512 sec/iter\n",
      "Epoch: 363 | Batch: 006 / 011 | Total loss: 0.795 | Reg loss: 0.028 | Tree loss: 0.795 | Accuracy: 0.724000 | 0.512 sec/iter\n",
      "Epoch: 363 | Batch: 007 / 011 | Total loss: 0.794 | Reg loss: 0.028 | Tree loss: 0.794 | Accuracy: 0.717500 | 0.511 sec/iter\n",
      "Epoch: 363 | Batch: 008 / 011 | Total loss: 0.800 | Reg loss: 0.028 | Tree loss: 0.800 | Accuracy: 0.726000 | 0.511 sec/iter\n",
      "Epoch: 363 | Batch: 009 / 011 | Total loss: 0.808 | Reg loss: 0.028 | Tree loss: 0.808 | Accuracy: 0.712000 | 0.511 sec/iter\n",
      "Epoch: 363 | Batch: 010 / 011 | Total loss: 0.747 | Reg loss: 0.028 | Tree loss: 0.747 | Accuracy: 0.771331 | 0.511 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 364 | Batch: 000 / 011 | Total loss: 0.886 | Reg loss: 0.028 | Tree loss: 0.886 | Accuracy: 0.660500 | 0.511 sec/iter\n",
      "Epoch: 364 | Batch: 001 / 011 | Total loss: 0.884 | Reg loss: 0.028 | Tree loss: 0.884 | Accuracy: 0.657000 | 0.511 sec/iter\n",
      "Epoch: 364 | Batch: 002 / 011 | Total loss: 0.848 | Reg loss: 0.028 | Tree loss: 0.848 | Accuracy: 0.679000 | 0.511 sec/iter\n",
      "Epoch: 364 | Batch: 003 / 011 | Total loss: 0.812 | Reg loss: 0.028 | Tree loss: 0.812 | Accuracy: 0.693500 | 0.511 sec/iter\n",
      "Epoch: 364 | Batch: 004 / 011 | Total loss: 0.785 | Reg loss: 0.028 | Tree loss: 0.785 | Accuracy: 0.721500 | 0.511 sec/iter\n",
      "Epoch: 364 | Batch: 005 / 011 | Total loss: 0.800 | Reg loss: 0.028 | Tree loss: 0.800 | Accuracy: 0.705000 | 0.511 sec/iter\n",
      "Epoch: 364 | Batch: 006 / 011 | Total loss: 0.798 | Reg loss: 0.028 | Tree loss: 0.798 | Accuracy: 0.729500 | 0.511 sec/iter\n",
      "Epoch: 364 | Batch: 007 / 011 | Total loss: 0.768 | Reg loss: 0.028 | Tree loss: 0.768 | Accuracy: 0.738000 | 0.511 sec/iter\n",
      "Epoch: 364 | Batch: 008 / 011 | Total loss: 0.782 | Reg loss: 0.028 | Tree loss: 0.782 | Accuracy: 0.737000 | 0.511 sec/iter\n",
      "Epoch: 364 | Batch: 009 / 011 | Total loss: 0.780 | Reg loss: 0.028 | Tree loss: 0.780 | Accuracy: 0.736500 | 0.511 sec/iter\n",
      "Epoch: 364 | Batch: 010 / 011 | Total loss: 0.849 | Reg loss: 0.028 | Tree loss: 0.849 | Accuracy: 0.709898 | 0.511 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 365 | Batch: 000 / 011 | Total loss: 0.875 | Reg loss: 0.028 | Tree loss: 0.875 | Accuracy: 0.667500 | 0.511 sec/iter\n",
      "Epoch: 365 | Batch: 001 / 011 | Total loss: 0.858 | Reg loss: 0.028 | Tree loss: 0.858 | Accuracy: 0.670500 | 0.511 sec/iter\n",
      "Epoch: 365 | Batch: 002 / 011 | Total loss: 0.832 | Reg loss: 0.028 | Tree loss: 0.832 | Accuracy: 0.692000 | 0.511 sec/iter\n",
      "Epoch: 365 | Batch: 003 / 011 | Total loss: 0.837 | Reg loss: 0.028 | Tree loss: 0.837 | Accuracy: 0.691000 | 0.511 sec/iter\n",
      "Epoch: 365 | Batch: 004 / 011 | Total loss: 0.806 | Reg loss: 0.028 | Tree loss: 0.806 | Accuracy: 0.704500 | 0.511 sec/iter\n",
      "Epoch: 365 | Batch: 005 / 011 | Total loss: 0.795 | Reg loss: 0.028 | Tree loss: 0.795 | Accuracy: 0.698000 | 0.511 sec/iter\n",
      "Epoch: 365 | Batch: 006 / 011 | Total loss: 0.789 | Reg loss: 0.028 | Tree loss: 0.789 | Accuracy: 0.729000 | 0.511 sec/iter\n",
      "Epoch: 365 | Batch: 007 / 011 | Total loss: 0.765 | Reg loss: 0.028 | Tree loss: 0.765 | Accuracy: 0.746500 | 0.511 sec/iter\n",
      "Epoch: 365 | Batch: 008 / 011 | Total loss: 0.787 | Reg loss: 0.028 | Tree loss: 0.787 | Accuracy: 0.728000 | 0.511 sec/iter\n",
      "Epoch: 365 | Batch: 009 / 011 | Total loss: 0.805 | Reg loss: 0.028 | Tree loss: 0.805 | Accuracy: 0.725500 | 0.511 sec/iter\n",
      "Epoch: 365 | Batch: 010 / 011 | Total loss: 0.742 | Reg loss: 0.028 | Tree loss: 0.742 | Accuracy: 0.740614 | 0.51 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 6: 0.9723756906077349\n",
      "Epoch: 366 | Batch: 000 / 011 | Total loss: 0.876 | Reg loss: 0.028 | Tree loss: 0.876 | Accuracy: 0.662500 | 0.511 sec/iter\n",
      "Epoch: 366 | Batch: 001 / 011 | Total loss: 0.871 | Reg loss: 0.028 | Tree loss: 0.871 | Accuracy: 0.681000 | 0.511 sec/iter\n",
      "Epoch: 366 | Batch: 002 / 011 | Total loss: 0.830 | Reg loss: 0.028 | Tree loss: 0.830 | Accuracy: 0.701000 | 0.511 sec/iter\n",
      "Epoch: 366 | Batch: 003 / 011 | Total loss: 0.819 | Reg loss: 0.028 | Tree loss: 0.819 | Accuracy: 0.692500 | 0.511 sec/iter\n",
      "Epoch: 366 | Batch: 004 / 011 | Total loss: 0.783 | Reg loss: 0.028 | Tree loss: 0.783 | Accuracy: 0.734500 | 0.511 sec/iter\n",
      "Epoch: 366 | Batch: 005 / 011 | Total loss: 0.784 | Reg loss: 0.028 | Tree loss: 0.784 | Accuracy: 0.712000 | 0.511 sec/iter\n",
      "Epoch: 366 | Batch: 006 / 011 | Total loss: 0.794 | Reg loss: 0.028 | Tree loss: 0.794 | Accuracy: 0.728000 | 0.511 sec/iter\n",
      "Epoch: 366 | Batch: 007 / 011 | Total loss: 0.796 | Reg loss: 0.028 | Tree loss: 0.796 | Accuracy: 0.721500 | 0.511 sec/iter\n",
      "Epoch: 366 | Batch: 008 / 011 | Total loss: 0.814 | Reg loss: 0.028 | Tree loss: 0.814 | Accuracy: 0.720500 | 0.511 sec/iter\n",
      "Epoch: 366 | Batch: 009 / 011 | Total loss: 0.780 | Reg loss: 0.028 | Tree loss: 0.780 | Accuracy: 0.728500 | 0.51 sec/iter\n",
      "Epoch: 366 | Batch: 010 / 011 | Total loss: 0.783 | Reg loss: 0.028 | Tree loss: 0.783 | Accuracy: 0.709898 | 0.51 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 367 | Batch: 000 / 011 | Total loss: 0.895 | Reg loss: 0.028 | Tree loss: 0.895 | Accuracy: 0.654500 | 0.51 sec/iter\n",
      "Epoch: 367 | Batch: 001 / 011 | Total loss: 0.886 | Reg loss: 0.028 | Tree loss: 0.886 | Accuracy: 0.634500 | 0.51 sec/iter\n",
      "Epoch: 367 | Batch: 002 / 011 | Total loss: 0.824 | Reg loss: 0.028 | Tree loss: 0.824 | Accuracy: 0.708500 | 0.51 sec/iter\n",
      "Epoch: 367 | Batch: 003 / 011 | Total loss: 0.810 | Reg loss: 0.028 | Tree loss: 0.810 | Accuracy: 0.725000 | 0.51 sec/iter\n",
      "Epoch: 367 | Batch: 004 / 011 | Total loss: 0.808 | Reg loss: 0.028 | Tree loss: 0.808 | Accuracy: 0.707000 | 0.51 sec/iter\n",
      "Epoch: 367 | Batch: 005 / 011 | Total loss: 0.783 | Reg loss: 0.028 | Tree loss: 0.783 | Accuracy: 0.716500 | 0.51 sec/iter\n",
      "Epoch: 367 | Batch: 006 / 011 | Total loss: 0.800 | Reg loss: 0.028 | Tree loss: 0.800 | Accuracy: 0.706500 | 0.51 sec/iter\n",
      "Epoch: 367 | Batch: 007 / 011 | Total loss: 0.804 | Reg loss: 0.028 | Tree loss: 0.804 | Accuracy: 0.722000 | 0.51 sec/iter\n",
      "Epoch: 367 | Batch: 008 / 011 | Total loss: 0.788 | Reg loss: 0.028 | Tree loss: 0.788 | Accuracy: 0.725500 | 0.51 sec/iter\n",
      "Epoch: 367 | Batch: 009 / 011 | Total loss: 0.791 | Reg loss: 0.028 | Tree loss: 0.791 | Accuracy: 0.724000 | 0.51 sec/iter\n",
      "Epoch: 367 | Batch: 010 / 011 | Total loss: 0.783 | Reg loss: 0.028 | Tree loss: 0.783 | Accuracy: 0.757679 | 0.51 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 368 | Batch: 000 / 011 | Total loss: 0.880 | Reg loss: 0.028 | Tree loss: 0.880 | Accuracy: 0.673000 | 0.51 sec/iter\n",
      "Epoch: 368 | Batch: 001 / 011 | Total loss: 0.852 | Reg loss: 0.028 | Tree loss: 0.852 | Accuracy: 0.685000 | 0.51 sec/iter\n",
      "Epoch: 368 | Batch: 002 / 011 | Total loss: 0.843 | Reg loss: 0.028 | Tree loss: 0.843 | Accuracy: 0.684000 | 0.51 sec/iter\n",
      "Epoch: 368 | Batch: 003 / 011 | Total loss: 0.834 | Reg loss: 0.028 | Tree loss: 0.834 | Accuracy: 0.684000 | 0.51 sec/iter\n",
      "Epoch: 368 | Batch: 004 / 011 | Total loss: 0.772 | Reg loss: 0.028 | Tree loss: 0.772 | Accuracy: 0.726500 | 0.51 sec/iter\n",
      "Epoch: 368 | Batch: 005 / 011 | Total loss: 0.808 | Reg loss: 0.028 | Tree loss: 0.808 | Accuracy: 0.698500 | 0.51 sec/iter\n",
      "Epoch: 368 | Batch: 006 / 011 | Total loss: 0.795 | Reg loss: 0.028 | Tree loss: 0.795 | Accuracy: 0.721500 | 0.51 sec/iter\n",
      "Epoch: 368 | Batch: 007 / 011 | Total loss: 0.775 | Reg loss: 0.028 | Tree loss: 0.775 | Accuracy: 0.723000 | 0.51 sec/iter\n",
      "Epoch: 368 | Batch: 008 / 011 | Total loss: 0.818 | Reg loss: 0.028 | Tree loss: 0.818 | Accuracy: 0.723500 | 0.51 sec/iter\n",
      "Epoch: 368 | Batch: 009 / 011 | Total loss: 0.776 | Reg loss: 0.028 | Tree loss: 0.776 | Accuracy: 0.737500 | 0.51 sec/iter\n",
      "Epoch: 368 | Batch: 010 / 011 | Total loss: 0.766 | Reg loss: 0.028 | Tree loss: 0.766 | Accuracy: 0.737201 | 0.509 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 369 | Batch: 000 / 011 | Total loss: 0.859 | Reg loss: 0.028 | Tree loss: 0.859 | Accuracy: 0.684500 | 0.511 sec/iter\n",
      "Epoch: 369 | Batch: 001 / 011 | Total loss: 0.860 | Reg loss: 0.028 | Tree loss: 0.860 | Accuracy: 0.675000 | 0.511 sec/iter\n",
      "Epoch: 369 | Batch: 002 / 011 | Total loss: 0.841 | Reg loss: 0.028 | Tree loss: 0.841 | Accuracy: 0.690000 | 0.51 sec/iter\n",
      "Epoch: 369 | Batch: 003 / 011 | Total loss: 0.819 | Reg loss: 0.028 | Tree loss: 0.819 | Accuracy: 0.699000 | 0.51 sec/iter\n",
      "Epoch: 369 | Batch: 004 / 011 | Total loss: 0.805 | Reg loss: 0.028 | Tree loss: 0.805 | Accuracy: 0.712500 | 0.51 sec/iter\n",
      "Epoch: 369 | Batch: 005 / 011 | Total loss: 0.797 | Reg loss: 0.028 | Tree loss: 0.797 | Accuracy: 0.715000 | 0.51 sec/iter\n",
      "Epoch: 369 | Batch: 006 / 011 | Total loss: 0.794 | Reg loss: 0.028 | Tree loss: 0.794 | Accuracy: 0.719500 | 0.51 sec/iter\n",
      "Epoch: 369 | Batch: 007 / 011 | Total loss: 0.783 | Reg loss: 0.028 | Tree loss: 0.783 | Accuracy: 0.736000 | 0.51 sec/iter\n",
      "Epoch: 369 | Batch: 008 / 011 | Total loss: 0.782 | Reg loss: 0.028 | Tree loss: 0.782 | Accuracy: 0.727500 | 0.51 sec/iter\n",
      "Epoch: 369 | Batch: 009 / 011 | Total loss: 0.796 | Reg loss: 0.028 | Tree loss: 0.796 | Accuracy: 0.729000 | 0.51 sec/iter\n",
      "Epoch: 369 | Batch: 010 / 011 | Total loss: 0.776 | Reg loss: 0.028 | Tree loss: 0.776 | Accuracy: 0.747440 | 0.51 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 370 | Batch: 000 / 011 | Total loss: 0.867 | Reg loss: 0.028 | Tree loss: 0.867 | Accuracy: 0.665000 | 0.51 sec/iter\n",
      "Epoch: 370 | Batch: 001 / 011 | Total loss: 0.878 | Reg loss: 0.028 | Tree loss: 0.878 | Accuracy: 0.650500 | 0.51 sec/iter\n",
      "Epoch: 370 | Batch: 002 / 011 | Total loss: 0.825 | Reg loss: 0.028 | Tree loss: 0.825 | Accuracy: 0.712500 | 0.51 sec/iter\n",
      "Epoch: 370 | Batch: 003 / 011 | Total loss: 0.823 | Reg loss: 0.028 | Tree loss: 0.823 | Accuracy: 0.697500 | 0.51 sec/iter\n",
      "Epoch: 370 | Batch: 004 / 011 | Total loss: 0.792 | Reg loss: 0.028 | Tree loss: 0.792 | Accuracy: 0.725000 | 0.51 sec/iter\n",
      "Epoch: 370 | Batch: 005 / 011 | Total loss: 0.788 | Reg loss: 0.028 | Tree loss: 0.788 | Accuracy: 0.718500 | 0.51 sec/iter\n",
      "Epoch: 370 | Batch: 006 / 011 | Total loss: 0.801 | Reg loss: 0.028 | Tree loss: 0.801 | Accuracy: 0.706000 | 0.51 sec/iter\n",
      "Epoch: 370 | Batch: 007 / 011 | Total loss: 0.810 | Reg loss: 0.028 | Tree loss: 0.810 | Accuracy: 0.708000 | 0.51 sec/iter\n",
      "Epoch: 370 | Batch: 008 / 011 | Total loss: 0.755 | Reg loss: 0.028 | Tree loss: 0.755 | Accuracy: 0.735500 | 0.51 sec/iter\n",
      "Epoch: 370 | Batch: 009 / 011 | Total loss: 0.814 | Reg loss: 0.028 | Tree loss: 0.814 | Accuracy: 0.718500 | 0.509 sec/iter\n",
      "Epoch: 370 | Batch: 010 / 011 | Total loss: 0.747 | Reg loss: 0.028 | Tree loss: 0.747 | Accuracy: 0.764505 | 0.509 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 371 | Batch: 000 / 011 | Total loss: 0.878 | Reg loss: 0.028 | Tree loss: 0.878 | Accuracy: 0.667500 | 0.51 sec/iter\n",
      "Epoch: 371 | Batch: 001 / 011 | Total loss: 0.863 | Reg loss: 0.028 | Tree loss: 0.863 | Accuracy: 0.675500 | 0.51 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 371 | Batch: 002 / 011 | Total loss: 0.828 | Reg loss: 0.028 | Tree loss: 0.828 | Accuracy: 0.701000 | 0.51 sec/iter\n",
      "Epoch: 371 | Batch: 003 / 011 | Total loss: 0.830 | Reg loss: 0.028 | Tree loss: 0.830 | Accuracy: 0.700500 | 0.51 sec/iter\n",
      "Epoch: 371 | Batch: 004 / 011 | Total loss: 0.796 | Reg loss: 0.028 | Tree loss: 0.796 | Accuracy: 0.716500 | 0.51 sec/iter\n",
      "Epoch: 371 | Batch: 005 / 011 | Total loss: 0.808 | Reg loss: 0.028 | Tree loss: 0.808 | Accuracy: 0.715500 | 0.51 sec/iter\n",
      "Epoch: 371 | Batch: 006 / 011 | Total loss: 0.784 | Reg loss: 0.028 | Tree loss: 0.784 | Accuracy: 0.744500 | 0.51 sec/iter\n",
      "Epoch: 371 | Batch: 007 / 011 | Total loss: 0.764 | Reg loss: 0.028 | Tree loss: 0.764 | Accuracy: 0.744000 | 0.51 sec/iter\n",
      "Epoch: 371 | Batch: 008 / 011 | Total loss: 0.786 | Reg loss: 0.028 | Tree loss: 0.786 | Accuracy: 0.738000 | 0.509 sec/iter\n",
      "Epoch: 371 | Batch: 009 / 011 | Total loss: 0.811 | Reg loss: 0.028 | Tree loss: 0.811 | Accuracy: 0.720500 | 0.509 sec/iter\n",
      "Epoch: 371 | Batch: 010 / 011 | Total loss: 0.823 | Reg loss: 0.028 | Tree loss: 0.823 | Accuracy: 0.709898 | 0.509 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 372 | Batch: 000 / 011 | Total loss: 0.894 | Reg loss: 0.028 | Tree loss: 0.894 | Accuracy: 0.658000 | 0.51 sec/iter\n",
      "Epoch: 372 | Batch: 001 / 011 | Total loss: 0.857 | Reg loss: 0.028 | Tree loss: 0.857 | Accuracy: 0.673000 | 0.51 sec/iter\n",
      "Epoch: 372 | Batch: 002 / 011 | Total loss: 0.853 | Reg loss: 0.028 | Tree loss: 0.853 | Accuracy: 0.682000 | 0.51 sec/iter\n",
      "Epoch: 372 | Batch: 003 / 011 | Total loss: 0.808 | Reg loss: 0.028 | Tree loss: 0.808 | Accuracy: 0.711000 | 0.51 sec/iter\n",
      "Epoch: 372 | Batch: 004 / 011 | Total loss: 0.796 | Reg loss: 0.028 | Tree loss: 0.796 | Accuracy: 0.697500 | 0.51 sec/iter\n",
      "Epoch: 372 | Batch: 005 / 011 | Total loss: 0.763 | Reg loss: 0.028 | Tree loss: 0.763 | Accuracy: 0.732500 | 0.51 sec/iter\n",
      "Epoch: 372 | Batch: 006 / 011 | Total loss: 0.778 | Reg loss: 0.028 | Tree loss: 0.778 | Accuracy: 0.729500 | 0.509 sec/iter\n",
      "Epoch: 372 | Batch: 007 / 011 | Total loss: 0.804 | Reg loss: 0.028 | Tree loss: 0.804 | Accuracy: 0.708500 | 0.509 sec/iter\n",
      "Epoch: 372 | Batch: 008 / 011 | Total loss: 0.790 | Reg loss: 0.028 | Tree loss: 0.790 | Accuracy: 0.729000 | 0.509 sec/iter\n",
      "Epoch: 372 | Batch: 009 / 011 | Total loss: 0.797 | Reg loss: 0.028 | Tree loss: 0.797 | Accuracy: 0.713000 | 0.509 sec/iter\n",
      "Epoch: 372 | Batch: 010 / 011 | Total loss: 0.806 | Reg loss: 0.028 | Tree loss: 0.806 | Accuracy: 0.723549 | 0.509 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 373 | Batch: 000 / 011 | Total loss: 0.871 | Reg loss: 0.028 | Tree loss: 0.871 | Accuracy: 0.675500 | 0.509 sec/iter\n",
      "Epoch: 373 | Batch: 001 / 011 | Total loss: 0.877 | Reg loss: 0.028 | Tree loss: 0.877 | Accuracy: 0.665000 | 0.509 sec/iter\n",
      "Epoch: 373 | Batch: 002 / 011 | Total loss: 0.844 | Reg loss: 0.028 | Tree loss: 0.844 | Accuracy: 0.679000 | 0.509 sec/iter\n",
      "Epoch: 373 | Batch: 003 / 011 | Total loss: 0.825 | Reg loss: 0.028 | Tree loss: 0.825 | Accuracy: 0.698000 | 0.509 sec/iter\n",
      "Epoch: 373 | Batch: 004 / 011 | Total loss: 0.803 | Reg loss: 0.028 | Tree loss: 0.803 | Accuracy: 0.718500 | 0.509 sec/iter\n",
      "Epoch: 373 | Batch: 005 / 011 | Total loss: 0.804 | Reg loss: 0.028 | Tree loss: 0.804 | Accuracy: 0.717500 | 0.509 sec/iter\n",
      "Epoch: 373 | Batch: 006 / 011 | Total loss: 0.775 | Reg loss: 0.028 | Tree loss: 0.775 | Accuracy: 0.733500 | 0.509 sec/iter\n",
      "Epoch: 373 | Batch: 007 / 011 | Total loss: 0.784 | Reg loss: 0.028 | Tree loss: 0.784 | Accuracy: 0.733500 | 0.509 sec/iter\n",
      "Epoch: 373 | Batch: 008 / 011 | Total loss: 0.789 | Reg loss: 0.028 | Tree loss: 0.789 | Accuracy: 0.730500 | 0.509 sec/iter\n",
      "Epoch: 373 | Batch: 009 / 011 | Total loss: 0.791 | Reg loss: 0.028 | Tree loss: 0.791 | Accuracy: 0.729500 | 0.509 sec/iter\n",
      "Epoch: 373 | Batch: 010 / 011 | Total loss: 0.758 | Reg loss: 0.028 | Tree loss: 0.758 | Accuracy: 0.733788 | 0.509 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 374 | Batch: 000 / 011 | Total loss: 0.879 | Reg loss: 0.028 | Tree loss: 0.879 | Accuracy: 0.651000 | 0.509 sec/iter\n",
      "Epoch: 374 | Batch: 001 / 011 | Total loss: 0.845 | Reg loss: 0.028 | Tree loss: 0.845 | Accuracy: 0.672000 | 0.509 sec/iter\n",
      "Epoch: 374 | Batch: 002 / 011 | Total loss: 0.844 | Reg loss: 0.028 | Tree loss: 0.844 | Accuracy: 0.678500 | 0.509 sec/iter\n",
      "Epoch: 374 | Batch: 003 / 011 | Total loss: 0.803 | Reg loss: 0.028 | Tree loss: 0.803 | Accuracy: 0.707000 | 0.509 sec/iter\n",
      "Epoch: 374 | Batch: 004 / 011 | Total loss: 0.787 | Reg loss: 0.028 | Tree loss: 0.787 | Accuracy: 0.732000 | 0.509 sec/iter\n",
      "Epoch: 374 | Batch: 005 / 011 | Total loss: 0.821 | Reg loss: 0.028 | Tree loss: 0.821 | Accuracy: 0.702000 | 0.509 sec/iter\n",
      "Epoch: 374 | Batch: 006 / 011 | Total loss: 0.794 | Reg loss: 0.028 | Tree loss: 0.794 | Accuracy: 0.732500 | 0.509 sec/iter\n",
      "Epoch: 374 | Batch: 007 / 011 | Total loss: 0.799 | Reg loss: 0.028 | Tree loss: 0.799 | Accuracy: 0.722000 | 0.509 sec/iter\n",
      "Epoch: 374 | Batch: 008 / 011 | Total loss: 0.815 | Reg loss: 0.028 | Tree loss: 0.815 | Accuracy: 0.701000 | 0.509 sec/iter\n",
      "Epoch: 374 | Batch: 009 / 011 | Total loss: 0.781 | Reg loss: 0.028 | Tree loss: 0.781 | Accuracy: 0.720000 | 0.508 sec/iter\n",
      "Epoch: 374 | Batch: 010 / 011 | Total loss: 0.733 | Reg loss: 0.028 | Tree loss: 0.733 | Accuracy: 0.764505 | 0.508 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 375 | Batch: 000 / 011 | Total loss: 0.879 | Reg loss: 0.028 | Tree loss: 0.879 | Accuracy: 0.651500 | 0.51 sec/iter\n",
      "Epoch: 375 | Batch: 001 / 011 | Total loss: 0.844 | Reg loss: 0.028 | Tree loss: 0.844 | Accuracy: 0.683500 | 0.51 sec/iter\n",
      "Epoch: 375 | Batch: 002 / 011 | Total loss: 0.838 | Reg loss: 0.028 | Tree loss: 0.838 | Accuracy: 0.698500 | 0.509 sec/iter\n",
      "Epoch: 375 | Batch: 003 / 011 | Total loss: 0.820 | Reg loss: 0.028 | Tree loss: 0.820 | Accuracy: 0.704000 | 0.509 sec/iter\n",
      "Epoch: 375 | Batch: 004 / 011 | Total loss: 0.804 | Reg loss: 0.028 | Tree loss: 0.804 | Accuracy: 0.709500 | 0.509 sec/iter\n",
      "Epoch: 375 | Batch: 005 / 011 | Total loss: 0.822 | Reg loss: 0.028 | Tree loss: 0.822 | Accuracy: 0.696000 | 0.509 sec/iter\n",
      "Epoch: 375 | Batch: 006 / 011 | Total loss: 0.777 | Reg loss: 0.028 | Tree loss: 0.777 | Accuracy: 0.746000 | 0.509 sec/iter\n",
      "Epoch: 375 | Batch: 007 / 011 | Total loss: 0.783 | Reg loss: 0.028 | Tree loss: 0.783 | Accuracy: 0.734000 | 0.509 sec/iter\n",
      "Epoch: 375 | Batch: 008 / 011 | Total loss: 0.783 | Reg loss: 0.028 | Tree loss: 0.783 | Accuracy: 0.741500 | 0.509 sec/iter\n",
      "Epoch: 375 | Batch: 009 / 011 | Total loss: 0.800 | Reg loss: 0.028 | Tree loss: 0.800 | Accuracy: 0.719500 | 0.509 sec/iter\n",
      "Epoch: 375 | Batch: 010 / 011 | Total loss: 0.787 | Reg loss: 0.028 | Tree loss: 0.787 | Accuracy: 0.720137 | 0.509 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 376 | Batch: 000 / 011 | Total loss: 0.866 | Reg loss: 0.028 | Tree loss: 0.866 | Accuracy: 0.655500 | 0.509 sec/iter\n",
      "Epoch: 376 | Batch: 001 / 011 | Total loss: 0.870 | Reg loss: 0.028 | Tree loss: 0.870 | Accuracy: 0.672000 | 0.509 sec/iter\n",
      "Epoch: 376 | Batch: 002 / 011 | Total loss: 0.823 | Reg loss: 0.028 | Tree loss: 0.823 | Accuracy: 0.698000 | 0.509 sec/iter\n",
      "Epoch: 376 | Batch: 003 / 011 | Total loss: 0.823 | Reg loss: 0.028 | Tree loss: 0.823 | Accuracy: 0.684000 | 0.509 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 376 | Batch: 004 / 011 | Total loss: 0.807 | Reg loss: 0.028 | Tree loss: 0.807 | Accuracy: 0.713500 | 0.509 sec/iter\n",
      "Epoch: 376 | Batch: 005 / 011 | Total loss: 0.816 | Reg loss: 0.028 | Tree loss: 0.816 | Accuracy: 0.708500 | 0.509 sec/iter\n",
      "Epoch: 376 | Batch: 006 / 011 | Total loss: 0.802 | Reg loss: 0.028 | Tree loss: 0.802 | Accuracy: 0.710500 | 0.509 sec/iter\n",
      "Epoch: 376 | Batch: 007 / 011 | Total loss: 0.779 | Reg loss: 0.028 | Tree loss: 0.779 | Accuracy: 0.716000 | 0.509 sec/iter\n",
      "Epoch: 376 | Batch: 008 / 011 | Total loss: 0.786 | Reg loss: 0.028 | Tree loss: 0.786 | Accuracy: 0.721500 | 0.509 sec/iter\n",
      "Epoch: 376 | Batch: 009 / 011 | Total loss: 0.787 | Reg loss: 0.028 | Tree loss: 0.787 | Accuracy: 0.724500 | 0.508 sec/iter\n",
      "Epoch: 376 | Batch: 010 / 011 | Total loss: 0.809 | Reg loss: 0.028 | Tree loss: 0.809 | Accuracy: 0.696246 | 0.508 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 377 | Batch: 000 / 011 | Total loss: 0.890 | Reg loss: 0.028 | Tree loss: 0.890 | Accuracy: 0.656000 | 0.51 sec/iter\n",
      "Epoch: 377 | Batch: 001 / 011 | Total loss: 0.867 | Reg loss: 0.028 | Tree loss: 0.867 | Accuracy: 0.657000 | 0.51 sec/iter\n",
      "Epoch: 377 | Batch: 002 / 011 | Total loss: 0.824 | Reg loss: 0.028 | Tree loss: 0.824 | Accuracy: 0.697000 | 0.509 sec/iter\n",
      "Epoch: 377 | Batch: 003 / 011 | Total loss: 0.847 | Reg loss: 0.028 | Tree loss: 0.847 | Accuracy: 0.688000 | 0.509 sec/iter\n",
      "Epoch: 377 | Batch: 004 / 011 | Total loss: 0.781 | Reg loss: 0.028 | Tree loss: 0.781 | Accuracy: 0.738000 | 0.509 sec/iter\n",
      "Epoch: 377 | Batch: 005 / 011 | Total loss: 0.776 | Reg loss: 0.028 | Tree loss: 0.776 | Accuracy: 0.728000 | 0.509 sec/iter\n",
      "Epoch: 377 | Batch: 006 / 011 | Total loss: 0.820 | Reg loss: 0.028 | Tree loss: 0.820 | Accuracy: 0.721500 | 0.509 sec/iter\n",
      "Epoch: 377 | Batch: 007 / 011 | Total loss: 0.785 | Reg loss: 0.028 | Tree loss: 0.785 | Accuracy: 0.730500 | 0.509 sec/iter\n",
      "Epoch: 377 | Batch: 008 / 011 | Total loss: 0.806 | Reg loss: 0.028 | Tree loss: 0.806 | Accuracy: 0.704000 | 0.509 sec/iter\n",
      "Epoch: 377 | Batch: 009 / 011 | Total loss: 0.781 | Reg loss: 0.028 | Tree loss: 0.781 | Accuracy: 0.744000 | 0.509 sec/iter\n",
      "Epoch: 377 | Batch: 010 / 011 | Total loss: 0.782 | Reg loss: 0.028 | Tree loss: 0.782 | Accuracy: 0.740614 | 0.509 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 378 | Batch: 000 / 011 | Total loss: 0.839 | Reg loss: 0.028 | Tree loss: 0.839 | Accuracy: 0.682500 | 0.51 sec/iter\n",
      "Epoch: 378 | Batch: 001 / 011 | Total loss: 0.863 | Reg loss: 0.028 | Tree loss: 0.863 | Accuracy: 0.665500 | 0.51 sec/iter\n",
      "Epoch: 378 | Batch: 002 / 011 | Total loss: 0.858 | Reg loss: 0.028 | Tree loss: 0.858 | Accuracy: 0.665000 | 0.51 sec/iter\n",
      "Epoch: 378 | Batch: 003 / 011 | Total loss: 0.843 | Reg loss: 0.028 | Tree loss: 0.843 | Accuracy: 0.682500 | 0.51 sec/iter\n",
      "Epoch: 378 | Batch: 004 / 011 | Total loss: 0.810 | Reg loss: 0.028 | Tree loss: 0.810 | Accuracy: 0.700500 | 0.51 sec/iter\n",
      "Epoch: 378 | Batch: 005 / 011 | Total loss: 0.811 | Reg loss: 0.028 | Tree loss: 0.811 | Accuracy: 0.714000 | 0.51 sec/iter\n",
      "Epoch: 378 | Batch: 006 / 011 | Total loss: 0.788 | Reg loss: 0.028 | Tree loss: 0.788 | Accuracy: 0.729500 | 0.51 sec/iter\n",
      "Epoch: 378 | Batch: 007 / 011 | Total loss: 0.791 | Reg loss: 0.028 | Tree loss: 0.791 | Accuracy: 0.734500 | 0.51 sec/iter\n",
      "Epoch: 378 | Batch: 008 / 011 | Total loss: 0.777 | Reg loss: 0.028 | Tree loss: 0.777 | Accuracy: 0.734000 | 0.51 sec/iter\n",
      "Epoch: 378 | Batch: 009 / 011 | Total loss: 0.778 | Reg loss: 0.028 | Tree loss: 0.778 | Accuracy: 0.739500 | 0.51 sec/iter\n",
      "Epoch: 378 | Batch: 010 / 011 | Total loss: 0.743 | Reg loss: 0.028 | Tree loss: 0.743 | Accuracy: 0.723549 | 0.509 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 379 | Batch: 000 / 011 | Total loss: 0.877 | Reg loss: 0.028 | Tree loss: 0.877 | Accuracy: 0.663500 | 0.509 sec/iter\n",
      "Epoch: 379 | Batch: 001 / 011 | Total loss: 0.881 | Reg loss: 0.028 | Tree loss: 0.881 | Accuracy: 0.649500 | 0.509 sec/iter\n",
      "Epoch: 379 | Batch: 002 / 011 | Total loss: 0.826 | Reg loss: 0.028 | Tree loss: 0.826 | Accuracy: 0.697500 | 0.509 sec/iter\n",
      "Epoch: 379 | Batch: 003 / 011 | Total loss: 0.819 | Reg loss: 0.028 | Tree loss: 0.819 | Accuracy: 0.697500 | 0.509 sec/iter\n",
      "Epoch: 379 | Batch: 004 / 011 | Total loss: 0.795 | Reg loss: 0.028 | Tree loss: 0.795 | Accuracy: 0.718000 | 0.509 sec/iter\n",
      "Epoch: 379 | Batch: 005 / 011 | Total loss: 0.784 | Reg loss: 0.028 | Tree loss: 0.784 | Accuracy: 0.714500 | 0.509 sec/iter\n",
      "Epoch: 379 | Batch: 006 / 011 | Total loss: 0.810 | Reg loss: 0.028 | Tree loss: 0.810 | Accuracy: 0.704500 | 0.509 sec/iter\n",
      "Epoch: 379 | Batch: 007 / 011 | Total loss: 0.781 | Reg loss: 0.028 | Tree loss: 0.781 | Accuracy: 0.728000 | 0.509 sec/iter\n",
      "Epoch: 379 | Batch: 008 / 011 | Total loss: 0.776 | Reg loss: 0.028 | Tree loss: 0.776 | Accuracy: 0.748500 | 0.509 sec/iter\n",
      "Epoch: 379 | Batch: 009 / 011 | Total loss: 0.768 | Reg loss: 0.028 | Tree loss: 0.768 | Accuracy: 0.736500 | 0.509 sec/iter\n",
      "Epoch: 379 | Batch: 010 / 011 | Total loss: 0.776 | Reg loss: 0.028 | Tree loss: 0.776 | Accuracy: 0.723549 | 0.509 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 380 | Batch: 000 / 011 | Total loss: 0.886 | Reg loss: 0.028 | Tree loss: 0.886 | Accuracy: 0.648500 | 0.51 sec/iter\n",
      "Epoch: 380 | Batch: 001 / 011 | Total loss: 0.859 | Reg loss: 0.028 | Tree loss: 0.859 | Accuracy: 0.687500 | 0.51 sec/iter\n",
      "Epoch: 380 | Batch: 002 / 011 | Total loss: 0.867 | Reg loss: 0.028 | Tree loss: 0.867 | Accuracy: 0.676000 | 0.51 sec/iter\n",
      "Epoch: 380 | Batch: 003 / 011 | Total loss: 0.824 | Reg loss: 0.028 | Tree loss: 0.824 | Accuracy: 0.700000 | 0.51 sec/iter\n",
      "Epoch: 380 | Batch: 004 / 011 | Total loss: 0.781 | Reg loss: 0.028 | Tree loss: 0.781 | Accuracy: 0.739000 | 0.51 sec/iter\n",
      "Epoch: 380 | Batch: 005 / 011 | Total loss: 0.792 | Reg loss: 0.028 | Tree loss: 0.792 | Accuracy: 0.717000 | 0.51 sec/iter\n",
      "Epoch: 380 | Batch: 006 / 011 | Total loss: 0.777 | Reg loss: 0.028 | Tree loss: 0.777 | Accuracy: 0.724500 | 0.51 sec/iter\n",
      "Epoch: 380 | Batch: 007 / 011 | Total loss: 0.795 | Reg loss: 0.028 | Tree loss: 0.795 | Accuracy: 0.722000 | 0.51 sec/iter\n",
      "Epoch: 380 | Batch: 008 / 011 | Total loss: 0.776 | Reg loss: 0.028 | Tree loss: 0.776 | Accuracy: 0.727000 | 0.509 sec/iter\n",
      "Epoch: 380 | Batch: 009 / 011 | Total loss: 0.783 | Reg loss: 0.028 | Tree loss: 0.783 | Accuracy: 0.744500 | 0.509 sec/iter\n",
      "Epoch: 380 | Batch: 010 / 011 | Total loss: 0.782 | Reg loss: 0.028 | Tree loss: 0.782 | Accuracy: 0.726962 | 0.509 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 381 | Batch: 000 / 011 | Total loss: 0.882 | Reg loss: 0.028 | Tree loss: 0.882 | Accuracy: 0.657500 | 0.51 sec/iter\n",
      "Epoch: 381 | Batch: 001 / 011 | Total loss: 0.847 | Reg loss: 0.028 | Tree loss: 0.847 | Accuracy: 0.691000 | 0.51 sec/iter\n",
      "Epoch: 381 | Batch: 002 / 011 | Total loss: 0.837 | Reg loss: 0.028 | Tree loss: 0.837 | Accuracy: 0.683500 | 0.51 sec/iter\n",
      "Epoch: 381 | Batch: 003 / 011 | Total loss: 0.811 | Reg loss: 0.028 | Tree loss: 0.811 | Accuracy: 0.700500 | 0.51 sec/iter\n",
      "Epoch: 381 | Batch: 004 / 011 | Total loss: 0.798 | Reg loss: 0.028 | Tree loss: 0.798 | Accuracy: 0.711000 | 0.51 sec/iter\n",
      "Epoch: 381 | Batch: 005 / 011 | Total loss: 0.804 | Reg loss: 0.028 | Tree loss: 0.804 | Accuracy: 0.718500 | 0.51 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 381 | Batch: 006 / 011 | Total loss: 0.774 | Reg loss: 0.028 | Tree loss: 0.774 | Accuracy: 0.755000 | 0.51 sec/iter\n",
      "Epoch: 381 | Batch: 007 / 011 | Total loss: 0.797 | Reg loss: 0.028 | Tree loss: 0.797 | Accuracy: 0.733000 | 0.51 sec/iter\n",
      "Epoch: 381 | Batch: 008 / 011 | Total loss: 0.786 | Reg loss: 0.028 | Tree loss: 0.786 | Accuracy: 0.736000 | 0.51 sec/iter\n",
      "Epoch: 381 | Batch: 009 / 011 | Total loss: 0.802 | Reg loss: 0.028 | Tree loss: 0.802 | Accuracy: 0.717000 | 0.51 sec/iter\n",
      "Epoch: 381 | Batch: 010 / 011 | Total loss: 0.787 | Reg loss: 0.028 | Tree loss: 0.787 | Accuracy: 0.726962 | 0.51 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 382 | Batch: 000 / 011 | Total loss: 0.883 | Reg loss: 0.028 | Tree loss: 0.883 | Accuracy: 0.670000 | 0.51 sec/iter\n",
      "Epoch: 382 | Batch: 001 / 011 | Total loss: 0.864 | Reg loss: 0.028 | Tree loss: 0.864 | Accuracy: 0.667500 | 0.51 sec/iter\n",
      "Epoch: 382 | Batch: 002 / 011 | Total loss: 0.821 | Reg loss: 0.028 | Tree loss: 0.821 | Accuracy: 0.693500 | 0.51 sec/iter\n",
      "Epoch: 382 | Batch: 003 / 011 | Total loss: 0.824 | Reg loss: 0.028 | Tree loss: 0.824 | Accuracy: 0.696500 | 0.51 sec/iter\n",
      "Epoch: 382 | Batch: 004 / 011 | Total loss: 0.795 | Reg loss: 0.028 | Tree loss: 0.795 | Accuracy: 0.709500 | 0.51 sec/iter\n",
      "Epoch: 382 | Batch: 005 / 011 | Total loss: 0.789 | Reg loss: 0.028 | Tree loss: 0.789 | Accuracy: 0.722000 | 0.51 sec/iter\n",
      "Epoch: 382 | Batch: 006 / 011 | Total loss: 0.801 | Reg loss: 0.028 | Tree loss: 0.801 | Accuracy: 0.724000 | 0.51 sec/iter\n",
      "Epoch: 382 | Batch: 007 / 011 | Total loss: 0.806 | Reg loss: 0.028 | Tree loss: 0.806 | Accuracy: 0.718500 | 0.509 sec/iter\n",
      "Epoch: 382 | Batch: 008 / 011 | Total loss: 0.783 | Reg loss: 0.028 | Tree loss: 0.783 | Accuracy: 0.722000 | 0.509 sec/iter\n",
      "Epoch: 382 | Batch: 009 / 011 | Total loss: 0.768 | Reg loss: 0.028 | Tree loss: 0.768 | Accuracy: 0.737000 | 0.509 sec/iter\n",
      "Epoch: 382 | Batch: 010 / 011 | Total loss: 0.850 | Reg loss: 0.028 | Tree loss: 0.850 | Accuracy: 0.713311 | 0.509 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 383 | Batch: 000 / 011 | Total loss: 0.892 | Reg loss: 0.028 | Tree loss: 0.892 | Accuracy: 0.659000 | 0.51 sec/iter\n",
      "Epoch: 383 | Batch: 001 / 011 | Total loss: 0.856 | Reg loss: 0.028 | Tree loss: 0.856 | Accuracy: 0.668000 | 0.51 sec/iter\n",
      "Epoch: 383 | Batch: 002 / 011 | Total loss: 0.834 | Reg loss: 0.028 | Tree loss: 0.834 | Accuracy: 0.683000 | 0.51 sec/iter\n",
      "Epoch: 383 | Batch: 003 / 011 | Total loss: 0.828 | Reg loss: 0.028 | Tree loss: 0.828 | Accuracy: 0.691000 | 0.51 sec/iter\n",
      "Epoch: 383 | Batch: 004 / 011 | Total loss: 0.815 | Reg loss: 0.028 | Tree loss: 0.815 | Accuracy: 0.692500 | 0.51 sec/iter\n",
      "Epoch: 383 | Batch: 005 / 011 | Total loss: 0.784 | Reg loss: 0.028 | Tree loss: 0.784 | Accuracy: 0.717500 | 0.51 sec/iter\n",
      "Epoch: 383 | Batch: 006 / 011 | Total loss: 0.772 | Reg loss: 0.028 | Tree loss: 0.772 | Accuracy: 0.743000 | 0.51 sec/iter\n",
      "Epoch: 383 | Batch: 007 / 011 | Total loss: 0.792 | Reg loss: 0.028 | Tree loss: 0.792 | Accuracy: 0.730000 | 0.51 sec/iter\n",
      "Epoch: 383 | Batch: 008 / 011 | Total loss: 0.778 | Reg loss: 0.028 | Tree loss: 0.778 | Accuracy: 0.736500 | 0.51 sec/iter\n",
      "Epoch: 383 | Batch: 009 / 011 | Total loss: 0.797 | Reg loss: 0.028 | Tree loss: 0.797 | Accuracy: 0.719000 | 0.51 sec/iter\n",
      "Epoch: 383 | Batch: 010 / 011 | Total loss: 0.839 | Reg loss: 0.028 | Tree loss: 0.839 | Accuracy: 0.733788 | 0.51 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 384 | Batch: 000 / 011 | Total loss: 0.873 | Reg loss: 0.028 | Tree loss: 0.873 | Accuracy: 0.652000 | 0.51 sec/iter\n",
      "Epoch: 384 | Batch: 001 / 011 | Total loss: 0.867 | Reg loss: 0.028 | Tree loss: 0.867 | Accuracy: 0.669500 | 0.51 sec/iter\n",
      "Epoch: 384 | Batch: 002 / 011 | Total loss: 0.833 | Reg loss: 0.028 | Tree loss: 0.833 | Accuracy: 0.694000 | 0.51 sec/iter\n",
      "Epoch: 384 | Batch: 003 / 011 | Total loss: 0.822 | Reg loss: 0.028 | Tree loss: 0.822 | Accuracy: 0.693500 | 0.51 sec/iter\n",
      "Epoch: 384 | Batch: 004 / 011 | Total loss: 0.800 | Reg loss: 0.028 | Tree loss: 0.800 | Accuracy: 0.712500 | 0.51 sec/iter\n",
      "Epoch: 384 | Batch: 005 / 011 | Total loss: 0.778 | Reg loss: 0.028 | Tree loss: 0.778 | Accuracy: 0.728500 | 0.51 sec/iter\n",
      "Epoch: 384 | Batch: 006 / 011 | Total loss: 0.793 | Reg loss: 0.028 | Tree loss: 0.793 | Accuracy: 0.715500 | 0.51 sec/iter\n",
      "Epoch: 384 | Batch: 007 / 011 | Total loss: 0.806 | Reg loss: 0.028 | Tree loss: 0.806 | Accuracy: 0.710000 | 0.51 sec/iter\n",
      "Epoch: 384 | Batch: 008 / 011 | Total loss: 0.779 | Reg loss: 0.028 | Tree loss: 0.779 | Accuracy: 0.738500 | 0.51 sec/iter\n",
      "Epoch: 384 | Batch: 009 / 011 | Total loss: 0.783 | Reg loss: 0.028 | Tree loss: 0.783 | Accuracy: 0.726000 | 0.51 sec/iter\n",
      "Epoch: 384 | Batch: 010 / 011 | Total loss: 0.777 | Reg loss: 0.028 | Tree loss: 0.777 | Accuracy: 0.744027 | 0.51 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 385 | Batch: 000 / 011 | Total loss: 0.864 | Reg loss: 0.028 | Tree loss: 0.864 | Accuracy: 0.677000 | 0.51 sec/iter\n",
      "Epoch: 385 | Batch: 001 / 011 | Total loss: 0.856 | Reg loss: 0.028 | Tree loss: 0.856 | Accuracy: 0.672500 | 0.51 sec/iter\n",
      "Epoch: 385 | Batch: 002 / 011 | Total loss: 0.848 | Reg loss: 0.028 | Tree loss: 0.848 | Accuracy: 0.684500 | 0.51 sec/iter\n",
      "Epoch: 385 | Batch: 003 / 011 | Total loss: 0.820 | Reg loss: 0.028 | Tree loss: 0.820 | Accuracy: 0.697500 | 0.51 sec/iter\n",
      "Epoch: 385 | Batch: 004 / 011 | Total loss: 0.793 | Reg loss: 0.028 | Tree loss: 0.793 | Accuracy: 0.712000 | 0.51 sec/iter\n",
      "Epoch: 385 | Batch: 005 / 011 | Total loss: 0.802 | Reg loss: 0.028 | Tree loss: 0.802 | Accuracy: 0.725500 | 0.51 sec/iter\n",
      "Epoch: 385 | Batch: 006 / 011 | Total loss: 0.780 | Reg loss: 0.028 | Tree loss: 0.780 | Accuracy: 0.737000 | 0.509 sec/iter\n",
      "Epoch: 385 | Batch: 007 / 011 | Total loss: 0.788 | Reg loss: 0.028 | Tree loss: 0.788 | Accuracy: 0.726500 | 0.509 sec/iter\n",
      "Epoch: 385 | Batch: 008 / 011 | Total loss: 0.797 | Reg loss: 0.028 | Tree loss: 0.797 | Accuracy: 0.721000 | 0.509 sec/iter\n",
      "Epoch: 385 | Batch: 009 / 011 | Total loss: 0.812 | Reg loss: 0.028 | Tree loss: 0.812 | Accuracy: 0.710000 | 0.509 sec/iter\n",
      "Epoch: 385 | Batch: 010 / 011 | Total loss: 0.680 | Reg loss: 0.028 | Tree loss: 0.680 | Accuracy: 0.771331 | 0.509 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 386 | Batch: 000 / 011 | Total loss: 0.852 | Reg loss: 0.028 | Tree loss: 0.852 | Accuracy: 0.682500 | 0.51 sec/iter\n",
      "Epoch: 386 | Batch: 001 / 011 | Total loss: 0.844 | Reg loss: 0.028 | Tree loss: 0.844 | Accuracy: 0.688000 | 0.51 sec/iter\n",
      "Epoch: 386 | Batch: 002 / 011 | Total loss: 0.842 | Reg loss: 0.028 | Tree loss: 0.842 | Accuracy: 0.671000 | 0.509 sec/iter\n",
      "Epoch: 386 | Batch: 003 / 011 | Total loss: 0.812 | Reg loss: 0.028 | Tree loss: 0.812 | Accuracy: 0.690000 | 0.509 sec/iter\n",
      "Epoch: 386 | Batch: 004 / 011 | Total loss: 0.805 | Reg loss: 0.028 | Tree loss: 0.805 | Accuracy: 0.701000 | 0.509 sec/iter\n",
      "Epoch: 386 | Batch: 005 / 011 | Total loss: 0.819 | Reg loss: 0.028 | Tree loss: 0.819 | Accuracy: 0.711000 | 0.509 sec/iter\n",
      "Epoch: 386 | Batch: 006 / 011 | Total loss: 0.787 | Reg loss: 0.028 | Tree loss: 0.787 | Accuracy: 0.735500 | 0.509 sec/iter\n",
      "Epoch: 386 | Batch: 007 / 011 | Total loss: 0.774 | Reg loss: 0.028 | Tree loss: 0.774 | Accuracy: 0.729500 | 0.509 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 386 | Batch: 008 / 011 | Total loss: 0.805 | Reg loss: 0.028 | Tree loss: 0.805 | Accuracy: 0.723000 | 0.509 sec/iter\n",
      "Epoch: 386 | Batch: 009 / 011 | Total loss: 0.792 | Reg loss: 0.028 | Tree loss: 0.792 | Accuracy: 0.733000 | 0.509 sec/iter\n",
      "Epoch: 386 | Batch: 010 / 011 | Total loss: 0.772 | Reg loss: 0.028 | Tree loss: 0.772 | Accuracy: 0.757679 | 0.509 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 387 | Batch: 000 / 011 | Total loss: 0.883 | Reg loss: 0.028 | Tree loss: 0.883 | Accuracy: 0.659000 | 0.51 sec/iter\n",
      "Epoch: 387 | Batch: 001 / 011 | Total loss: 0.870 | Reg loss: 0.028 | Tree loss: 0.870 | Accuracy: 0.662000 | 0.509 sec/iter\n",
      "Epoch: 387 | Batch: 002 / 011 | Total loss: 0.837 | Reg loss: 0.028 | Tree loss: 0.837 | Accuracy: 0.683500 | 0.509 sec/iter\n",
      "Epoch: 387 | Batch: 003 / 011 | Total loss: 0.830 | Reg loss: 0.028 | Tree loss: 0.830 | Accuracy: 0.695500 | 0.509 sec/iter\n",
      "Epoch: 387 | Batch: 004 / 011 | Total loss: 0.795 | Reg loss: 0.028 | Tree loss: 0.795 | Accuracy: 0.724500 | 0.509 sec/iter\n",
      "Epoch: 387 | Batch: 005 / 011 | Total loss: 0.785 | Reg loss: 0.028 | Tree loss: 0.785 | Accuracy: 0.728500 | 0.509 sec/iter\n",
      "Epoch: 387 | Batch: 006 / 011 | Total loss: 0.798 | Reg loss: 0.028 | Tree loss: 0.798 | Accuracy: 0.717500 | 0.509 sec/iter\n",
      "Epoch: 387 | Batch: 007 / 011 | Total loss: 0.802 | Reg loss: 0.028 | Tree loss: 0.802 | Accuracy: 0.723000 | 0.509 sec/iter\n",
      "Epoch: 387 | Batch: 008 / 011 | Total loss: 0.779 | Reg loss: 0.028 | Tree loss: 0.779 | Accuracy: 0.726000 | 0.509 sec/iter\n",
      "Epoch: 387 | Batch: 009 / 011 | Total loss: 0.762 | Reg loss: 0.028 | Tree loss: 0.762 | Accuracy: 0.735500 | 0.509 sec/iter\n",
      "Epoch: 387 | Batch: 010 / 011 | Total loss: 0.769 | Reg loss: 0.028 | Tree loss: 0.769 | Accuracy: 0.726962 | 0.509 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 388 | Batch: 000 / 011 | Total loss: 0.862 | Reg loss: 0.028 | Tree loss: 0.862 | Accuracy: 0.663500 | 0.509 sec/iter\n",
      "Epoch: 388 | Batch: 001 / 011 | Total loss: 0.881 | Reg loss: 0.028 | Tree loss: 0.881 | Accuracy: 0.656000 | 0.509 sec/iter\n",
      "Epoch: 388 | Batch: 002 / 011 | Total loss: 0.841 | Reg loss: 0.028 | Tree loss: 0.841 | Accuracy: 0.683500 | 0.509 sec/iter\n",
      "Epoch: 388 | Batch: 003 / 011 | Total loss: 0.796 | Reg loss: 0.028 | Tree loss: 0.796 | Accuracy: 0.710000 | 0.509 sec/iter\n",
      "Epoch: 388 | Batch: 004 / 011 | Total loss: 0.815 | Reg loss: 0.028 | Tree loss: 0.815 | Accuracy: 0.700500 | 0.509 sec/iter\n",
      "Epoch: 388 | Batch: 005 / 011 | Total loss: 0.787 | Reg loss: 0.028 | Tree loss: 0.787 | Accuracy: 0.732000 | 0.509 sec/iter\n",
      "Epoch: 388 | Batch: 006 / 011 | Total loss: 0.802 | Reg loss: 0.028 | Tree loss: 0.802 | Accuracy: 0.731000 | 0.509 sec/iter\n",
      "Epoch: 388 | Batch: 007 / 011 | Total loss: 0.797 | Reg loss: 0.028 | Tree loss: 0.797 | Accuracy: 0.732000 | 0.508 sec/iter\n",
      "Epoch: 388 | Batch: 008 / 011 | Total loss: 0.788 | Reg loss: 0.028 | Tree loss: 0.788 | Accuracy: 0.743000 | 0.508 sec/iter\n",
      "Epoch: 388 | Batch: 009 / 011 | Total loss: 0.788 | Reg loss: 0.028 | Tree loss: 0.788 | Accuracy: 0.713000 | 0.508 sec/iter\n",
      "Epoch: 388 | Batch: 010 / 011 | Total loss: 0.792 | Reg loss: 0.028 | Tree loss: 0.792 | Accuracy: 0.696246 | 0.508 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 389 | Batch: 000 / 011 | Total loss: 0.885 | Reg loss: 0.028 | Tree loss: 0.885 | Accuracy: 0.656000 | 0.509 sec/iter\n",
      "Epoch: 389 | Batch: 001 / 011 | Total loss: 0.868 | Reg loss: 0.028 | Tree loss: 0.868 | Accuracy: 0.671500 | 0.509 sec/iter\n",
      "Epoch: 389 | Batch: 002 / 011 | Total loss: 0.839 | Reg loss: 0.028 | Tree loss: 0.839 | Accuracy: 0.685000 | 0.509 sec/iter\n",
      "Epoch: 389 | Batch: 003 / 011 | Total loss: 0.830 | Reg loss: 0.028 | Tree loss: 0.830 | Accuracy: 0.702500 | 0.509 sec/iter\n",
      "Epoch: 389 | Batch: 004 / 011 | Total loss: 0.808 | Reg loss: 0.028 | Tree loss: 0.808 | Accuracy: 0.711500 | 0.509 sec/iter\n",
      "Epoch: 389 | Batch: 005 / 011 | Total loss: 0.785 | Reg loss: 0.028 | Tree loss: 0.785 | Accuracy: 0.723500 | 0.509 sec/iter\n",
      "Epoch: 389 | Batch: 006 / 011 | Total loss: 0.798 | Reg loss: 0.028 | Tree loss: 0.798 | Accuracy: 0.718500 | 0.509 sec/iter\n",
      "Epoch: 389 | Batch: 007 / 011 | Total loss: 0.782 | Reg loss: 0.028 | Tree loss: 0.782 | Accuracy: 0.716000 | 0.509 sec/iter\n",
      "Epoch: 389 | Batch: 008 / 011 | Total loss: 0.749 | Reg loss: 0.028 | Tree loss: 0.749 | Accuracy: 0.748000 | 0.509 sec/iter\n",
      "Epoch: 389 | Batch: 009 / 011 | Total loss: 0.779 | Reg loss: 0.028 | Tree loss: 0.779 | Accuracy: 0.746000 | 0.509 sec/iter\n",
      "Epoch: 389 | Batch: 010 / 011 | Total loss: 0.831 | Reg loss: 0.028 | Tree loss: 0.831 | Accuracy: 0.723549 | 0.509 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 390 | Batch: 000 / 011 | Total loss: 0.876 | Reg loss: 0.028 | Tree loss: 0.876 | Accuracy: 0.678000 | 0.509 sec/iter\n",
      "Epoch: 390 | Batch: 001 / 011 | Total loss: 0.848 | Reg loss: 0.028 | Tree loss: 0.848 | Accuracy: 0.673000 | 0.509 sec/iter\n",
      "Epoch: 390 | Batch: 002 / 011 | Total loss: 0.836 | Reg loss: 0.028 | Tree loss: 0.836 | Accuracy: 0.699000 | 0.509 sec/iter\n",
      "Epoch: 390 | Batch: 003 / 011 | Total loss: 0.833 | Reg loss: 0.028 | Tree loss: 0.833 | Accuracy: 0.693500 | 0.509 sec/iter\n",
      "Epoch: 390 | Batch: 004 / 011 | Total loss: 0.793 | Reg loss: 0.028 | Tree loss: 0.793 | Accuracy: 0.717500 | 0.509 sec/iter\n",
      "Epoch: 390 | Batch: 005 / 011 | Total loss: 0.784 | Reg loss: 0.028 | Tree loss: 0.784 | Accuracy: 0.723500 | 0.509 sec/iter\n",
      "Epoch: 390 | Batch: 006 / 011 | Total loss: 0.791 | Reg loss: 0.028 | Tree loss: 0.791 | Accuracy: 0.745500 | 0.509 sec/iter\n",
      "Epoch: 390 | Batch: 007 / 011 | Total loss: 0.779 | Reg loss: 0.028 | Tree loss: 0.779 | Accuracy: 0.742000 | 0.509 sec/iter\n",
      "Epoch: 390 | Batch: 008 / 011 | Total loss: 0.787 | Reg loss: 0.028 | Tree loss: 0.787 | Accuracy: 0.734000 | 0.509 sec/iter\n",
      "Epoch: 390 | Batch: 009 / 011 | Total loss: 0.813 | Reg loss: 0.028 | Tree loss: 0.813 | Accuracy: 0.713000 | 0.509 sec/iter\n",
      "Epoch: 390 | Batch: 010 / 011 | Total loss: 0.750 | Reg loss: 0.028 | Tree loss: 0.750 | Accuracy: 0.761092 | 0.509 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 391 | Batch: 000 / 011 | Total loss: 0.872 | Reg loss: 0.028 | Tree loss: 0.872 | Accuracy: 0.667000 | 0.509 sec/iter\n",
      "Epoch: 391 | Batch: 001 / 011 | Total loss: 0.851 | Reg loss: 0.028 | Tree loss: 0.851 | Accuracy: 0.677500 | 0.509 sec/iter\n",
      "Epoch: 391 | Batch: 002 / 011 | Total loss: 0.850 | Reg loss: 0.028 | Tree loss: 0.850 | Accuracy: 0.681500 | 0.509 sec/iter\n",
      "Epoch: 391 | Batch: 003 / 011 | Total loss: 0.823 | Reg loss: 0.028 | Tree loss: 0.823 | Accuracy: 0.704500 | 0.509 sec/iter\n",
      "Epoch: 391 | Batch: 004 / 011 | Total loss: 0.818 | Reg loss: 0.028 | Tree loss: 0.818 | Accuracy: 0.702500 | 0.509 sec/iter\n",
      "Epoch: 391 | Batch: 005 / 011 | Total loss: 0.795 | Reg loss: 0.028 | Tree loss: 0.795 | Accuracy: 0.710000 | 0.509 sec/iter\n",
      "Epoch: 391 | Batch: 006 / 011 | Total loss: 0.782 | Reg loss: 0.028 | Tree loss: 0.782 | Accuracy: 0.723500 | 0.508 sec/iter\n",
      "Epoch: 391 | Batch: 007 / 011 | Total loss: 0.812 | Reg loss: 0.028 | Tree loss: 0.812 | Accuracy: 0.698500 | 0.508 sec/iter\n",
      "Epoch: 391 | Batch: 008 / 011 | Total loss: 0.770 | Reg loss: 0.028 | Tree loss: 0.770 | Accuracy: 0.730000 | 0.508 sec/iter\n",
      "Epoch: 391 | Batch: 009 / 011 | Total loss: 0.766 | Reg loss: 0.028 | Tree loss: 0.766 | Accuracy: 0.738500 | 0.508 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 391 | Batch: 010 / 011 | Total loss: 0.764 | Reg loss: 0.028 | Tree loss: 0.764 | Accuracy: 0.757679 | 0.508 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 392 | Batch: 000 / 011 | Total loss: 0.868 | Reg loss: 0.028 | Tree loss: 0.868 | Accuracy: 0.665000 | 0.509 sec/iter\n",
      "Epoch: 392 | Batch: 001 / 011 | Total loss: 0.868 | Reg loss: 0.028 | Tree loss: 0.868 | Accuracy: 0.661500 | 0.509 sec/iter\n",
      "Epoch: 392 | Batch: 002 / 011 | Total loss: 0.837 | Reg loss: 0.028 | Tree loss: 0.837 | Accuracy: 0.685000 | 0.508 sec/iter\n",
      "Epoch: 392 | Batch: 003 / 011 | Total loss: 0.822 | Reg loss: 0.028 | Tree loss: 0.822 | Accuracy: 0.696500 | 0.508 sec/iter\n",
      "Epoch: 392 | Batch: 004 / 011 | Total loss: 0.795 | Reg loss: 0.028 | Tree loss: 0.795 | Accuracy: 0.711000 | 0.508 sec/iter\n",
      "Epoch: 392 | Batch: 005 / 011 | Total loss: 0.834 | Reg loss: 0.028 | Tree loss: 0.834 | Accuracy: 0.692000 | 0.508 sec/iter\n",
      "Epoch: 392 | Batch: 006 / 011 | Total loss: 0.777 | Reg loss: 0.028 | Tree loss: 0.777 | Accuracy: 0.740000 | 0.508 sec/iter\n",
      "Epoch: 392 | Batch: 007 / 011 | Total loss: 0.772 | Reg loss: 0.028 | Tree loss: 0.772 | Accuracy: 0.738500 | 0.508 sec/iter\n",
      "Epoch: 392 | Batch: 008 / 011 | Total loss: 0.774 | Reg loss: 0.028 | Tree loss: 0.774 | Accuracy: 0.747000 | 0.508 sec/iter\n",
      "Epoch: 392 | Batch: 009 / 011 | Total loss: 0.784 | Reg loss: 0.028 | Tree loss: 0.784 | Accuracy: 0.733000 | 0.508 sec/iter\n",
      "Epoch: 392 | Batch: 010 / 011 | Total loss: 0.785 | Reg loss: 0.028 | Tree loss: 0.785 | Accuracy: 0.723549 | 0.508 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 393 | Batch: 000 / 011 | Total loss: 0.859 | Reg loss: 0.028 | Tree loss: 0.859 | Accuracy: 0.678000 | 0.508 sec/iter\n",
      "Epoch: 393 | Batch: 001 / 011 | Total loss: 0.857 | Reg loss: 0.028 | Tree loss: 0.857 | Accuracy: 0.677000 | 0.508 sec/iter\n",
      "Epoch: 393 | Batch: 002 / 011 | Total loss: 0.849 | Reg loss: 0.028 | Tree loss: 0.849 | Accuracy: 0.683000 | 0.508 sec/iter\n",
      "Epoch: 393 | Batch: 003 / 011 | Total loss: 0.845 | Reg loss: 0.028 | Tree loss: 0.845 | Accuracy: 0.685500 | 0.508 sec/iter\n",
      "Epoch: 393 | Batch: 004 / 011 | Total loss: 0.814 | Reg loss: 0.028 | Tree loss: 0.814 | Accuracy: 0.703500 | 0.508 sec/iter\n",
      "Epoch: 393 | Batch: 005 / 011 | Total loss: 0.812 | Reg loss: 0.028 | Tree loss: 0.812 | Accuracy: 0.702000 | 0.508 sec/iter\n",
      "Epoch: 393 | Batch: 006 / 011 | Total loss: 0.757 | Reg loss: 0.028 | Tree loss: 0.757 | Accuracy: 0.726500 | 0.508 sec/iter\n",
      "Epoch: 393 | Batch: 007 / 011 | Total loss: 0.796 | Reg loss: 0.028 | Tree loss: 0.796 | Accuracy: 0.715000 | 0.508 sec/iter\n",
      "Epoch: 393 | Batch: 008 / 011 | Total loss: 0.786 | Reg loss: 0.028 | Tree loss: 0.786 | Accuracy: 0.721500 | 0.508 sec/iter\n",
      "Epoch: 393 | Batch: 009 / 011 | Total loss: 0.806 | Reg loss: 0.028 | Tree loss: 0.806 | Accuracy: 0.706000 | 0.508 sec/iter\n",
      "Epoch: 393 | Batch: 010 / 011 | Total loss: 0.770 | Reg loss: 0.028 | Tree loss: 0.770 | Accuracy: 0.784983 | 0.508 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 394 | Batch: 000 / 011 | Total loss: 0.876 | Reg loss: 0.028 | Tree loss: 0.876 | Accuracy: 0.669500 | 0.508 sec/iter\n",
      "Epoch: 394 | Batch: 001 / 011 | Total loss: 0.871 | Reg loss: 0.028 | Tree loss: 0.871 | Accuracy: 0.650000 | 0.508 sec/iter\n",
      "Epoch: 394 | Batch: 002 / 011 | Total loss: 0.832 | Reg loss: 0.028 | Tree loss: 0.832 | Accuracy: 0.696000 | 0.508 sec/iter\n",
      "Epoch: 394 | Batch: 003 / 011 | Total loss: 0.811 | Reg loss: 0.028 | Tree loss: 0.811 | Accuracy: 0.705500 | 0.508 sec/iter\n",
      "Epoch: 394 | Batch: 004 / 011 | Total loss: 0.807 | Reg loss: 0.028 | Tree loss: 0.807 | Accuracy: 0.704500 | 0.508 sec/iter\n",
      "Epoch: 394 | Batch: 005 / 011 | Total loss: 0.805 | Reg loss: 0.028 | Tree loss: 0.805 | Accuracy: 0.703000 | 0.508 sec/iter\n",
      "Epoch: 394 | Batch: 006 / 011 | Total loss: 0.773 | Reg loss: 0.028 | Tree loss: 0.773 | Accuracy: 0.736000 | 0.507 sec/iter\n",
      "Epoch: 394 | Batch: 007 / 011 | Total loss: 0.788 | Reg loss: 0.028 | Tree loss: 0.788 | Accuracy: 0.729000 | 0.507 sec/iter\n",
      "Epoch: 394 | Batch: 008 / 011 | Total loss: 0.782 | Reg loss: 0.028 | Tree loss: 0.782 | Accuracy: 0.729500 | 0.507 sec/iter\n",
      "Epoch: 394 | Batch: 009 / 011 | Total loss: 0.792 | Reg loss: 0.028 | Tree loss: 0.792 | Accuracy: 0.725500 | 0.507 sec/iter\n",
      "Epoch: 394 | Batch: 010 / 011 | Total loss: 0.756 | Reg loss: 0.028 | Tree loss: 0.756 | Accuracy: 0.730375 | 0.507 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 395 | Batch: 000 / 011 | Total loss: 0.879 | Reg loss: 0.028 | Tree loss: 0.879 | Accuracy: 0.666000 | 0.508 sec/iter\n",
      "Epoch: 395 | Batch: 001 / 011 | Total loss: 0.850 | Reg loss: 0.028 | Tree loss: 0.850 | Accuracy: 0.677000 | 0.508 sec/iter\n",
      "Epoch: 395 | Batch: 002 / 011 | Total loss: 0.836 | Reg loss: 0.028 | Tree loss: 0.836 | Accuracy: 0.697000 | 0.508 sec/iter\n",
      "Epoch: 395 | Batch: 003 / 011 | Total loss: 0.809 | Reg loss: 0.028 | Tree loss: 0.809 | Accuracy: 0.701000 | 0.508 sec/iter\n",
      "Epoch: 395 | Batch: 004 / 011 | Total loss: 0.831 | Reg loss: 0.028 | Tree loss: 0.831 | Accuracy: 0.689500 | 0.508 sec/iter\n",
      "Epoch: 395 | Batch: 005 / 011 | Total loss: 0.771 | Reg loss: 0.028 | Tree loss: 0.771 | Accuracy: 0.719000 | 0.508 sec/iter\n",
      "Epoch: 395 | Batch: 006 / 011 | Total loss: 0.793 | Reg loss: 0.028 | Tree loss: 0.793 | Accuracy: 0.714000 | 0.508 sec/iter\n",
      "Epoch: 395 | Batch: 007 / 011 | Total loss: 0.785 | Reg loss: 0.028 | Tree loss: 0.785 | Accuracy: 0.717000 | 0.508 sec/iter\n",
      "Epoch: 395 | Batch: 008 / 011 | Total loss: 0.781 | Reg loss: 0.028 | Tree loss: 0.781 | Accuracy: 0.736500 | 0.508 sec/iter\n",
      "Epoch: 395 | Batch: 009 / 011 | Total loss: 0.802 | Reg loss: 0.028 | Tree loss: 0.802 | Accuracy: 0.716000 | 0.508 sec/iter\n",
      "Epoch: 395 | Batch: 010 / 011 | Total loss: 0.756 | Reg loss: 0.028 | Tree loss: 0.756 | Accuracy: 0.750853 | 0.508 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 396 | Batch: 000 / 011 | Total loss: 0.861 | Reg loss: 0.028 | Tree loss: 0.861 | Accuracy: 0.678500 | 0.509 sec/iter\n",
      "Epoch: 396 | Batch: 001 / 011 | Total loss: 0.850 | Reg loss: 0.028 | Tree loss: 0.850 | Accuracy: 0.667500 | 0.509 sec/iter\n",
      "Epoch: 396 | Batch: 002 / 011 | Total loss: 0.827 | Reg loss: 0.028 | Tree loss: 0.827 | Accuracy: 0.686000 | 0.509 sec/iter\n",
      "Epoch: 396 | Batch: 003 / 011 | Total loss: 0.817 | Reg loss: 0.028 | Tree loss: 0.817 | Accuracy: 0.711000 | 0.509 sec/iter\n",
      "Epoch: 396 | Batch: 004 / 011 | Total loss: 0.821 | Reg loss: 0.028 | Tree loss: 0.821 | Accuracy: 0.702000 | 0.509 sec/iter\n",
      "Epoch: 396 | Batch: 005 / 011 | Total loss: 0.795 | Reg loss: 0.028 | Tree loss: 0.795 | Accuracy: 0.723500 | 0.509 sec/iter\n",
      "Epoch: 396 | Batch: 006 / 011 | Total loss: 0.802 | Reg loss: 0.028 | Tree loss: 0.802 | Accuracy: 0.706000 | 0.509 sec/iter\n",
      "Epoch: 396 | Batch: 007 / 011 | Total loss: 0.784 | Reg loss: 0.028 | Tree loss: 0.784 | Accuracy: 0.720000 | 0.509 sec/iter\n",
      "Epoch: 396 | Batch: 008 / 011 | Total loss: 0.791 | Reg loss: 0.028 | Tree loss: 0.791 | Accuracy: 0.728500 | 0.508 sec/iter\n",
      "Epoch: 396 | Batch: 009 / 011 | Total loss: 0.783 | Reg loss: 0.028 | Tree loss: 0.783 | Accuracy: 0.724500 | 0.508 sec/iter\n",
      "Epoch: 396 | Batch: 010 / 011 | Total loss: 0.724 | Reg loss: 0.028 | Tree loss: 0.724 | Accuracy: 0.754266 | 0.508 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 397 | Batch: 000 / 011 | Total loss: 0.874 | Reg loss: 0.028 | Tree loss: 0.874 | Accuracy: 0.667500 | 0.508 sec/iter\n",
      "Epoch: 397 | Batch: 001 / 011 | Total loss: 0.853 | Reg loss: 0.028 | Tree loss: 0.853 | Accuracy: 0.673000 | 0.508 sec/iter\n",
      "Epoch: 397 | Batch: 002 / 011 | Total loss: 0.827 | Reg loss: 0.028 | Tree loss: 0.827 | Accuracy: 0.708000 | 0.508 sec/iter\n",
      "Epoch: 397 | Batch: 003 / 011 | Total loss: 0.800 | Reg loss: 0.028 | Tree loss: 0.800 | Accuracy: 0.707500 | 0.508 sec/iter\n",
      "Epoch: 397 | Batch: 004 / 011 | Total loss: 0.798 | Reg loss: 0.028 | Tree loss: 0.798 | Accuracy: 0.729000 | 0.508 sec/iter\n",
      "Epoch: 397 | Batch: 005 / 011 | Total loss: 0.801 | Reg loss: 0.028 | Tree loss: 0.801 | Accuracy: 0.705500 | 0.508 sec/iter\n",
      "Epoch: 397 | Batch: 006 / 011 | Total loss: 0.819 | Reg loss: 0.028 | Tree loss: 0.819 | Accuracy: 0.712000 | 0.508 sec/iter\n",
      "Epoch: 397 | Batch: 007 / 011 | Total loss: 0.782 | Reg loss: 0.028 | Tree loss: 0.782 | Accuracy: 0.727500 | 0.508 sec/iter\n",
      "Epoch: 397 | Batch: 008 / 011 | Total loss: 0.797 | Reg loss: 0.028 | Tree loss: 0.797 | Accuracy: 0.740000 | 0.508 sec/iter\n",
      "Epoch: 397 | Batch: 009 / 011 | Total loss: 0.772 | Reg loss: 0.028 | Tree loss: 0.772 | Accuracy: 0.735500 | 0.508 sec/iter\n",
      "Epoch: 397 | Batch: 010 / 011 | Total loss: 0.839 | Reg loss: 0.028 | Tree loss: 0.839 | Accuracy: 0.703072 | 0.508 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 398 | Batch: 000 / 011 | Total loss: 0.888 | Reg loss: 0.028 | Tree loss: 0.888 | Accuracy: 0.665500 | 0.509 sec/iter\n",
      "Epoch: 398 | Batch: 001 / 011 | Total loss: 0.842 | Reg loss: 0.028 | Tree loss: 0.842 | Accuracy: 0.669000 | 0.509 sec/iter\n",
      "Epoch: 398 | Batch: 002 / 011 | Total loss: 0.828 | Reg loss: 0.028 | Tree loss: 0.828 | Accuracy: 0.692000 | 0.509 sec/iter\n",
      "Epoch: 398 | Batch: 003 / 011 | Total loss: 0.829 | Reg loss: 0.028 | Tree loss: 0.829 | Accuracy: 0.685000 | 0.509 sec/iter\n",
      "Epoch: 398 | Batch: 004 / 011 | Total loss: 0.786 | Reg loss: 0.028 | Tree loss: 0.786 | Accuracy: 0.727500 | 0.509 sec/iter\n",
      "Epoch: 398 | Batch: 005 / 011 | Total loss: 0.778 | Reg loss: 0.028 | Tree loss: 0.778 | Accuracy: 0.723500 | 0.509 sec/iter\n",
      "Epoch: 398 | Batch: 006 / 011 | Total loss: 0.783 | Reg loss: 0.028 | Tree loss: 0.783 | Accuracy: 0.720500 | 0.509 sec/iter\n",
      "Epoch: 398 | Batch: 007 / 011 | Total loss: 0.802 | Reg loss: 0.028 | Tree loss: 0.802 | Accuracy: 0.728500 | 0.508 sec/iter\n",
      "Epoch: 398 | Batch: 008 / 011 | Total loss: 0.792 | Reg loss: 0.028 | Tree loss: 0.792 | Accuracy: 0.730000 | 0.508 sec/iter\n",
      "Epoch: 398 | Batch: 009 / 011 | Total loss: 0.794 | Reg loss: 0.028 | Tree loss: 0.794 | Accuracy: 0.735000 | 0.508 sec/iter\n",
      "Epoch: 398 | Batch: 010 / 011 | Total loss: 0.824 | Reg loss: 0.028 | Tree loss: 0.824 | Accuracy: 0.703072 | 0.508 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 399 | Batch: 000 / 011 | Total loss: 0.885 | Reg loss: 0.028 | Tree loss: 0.885 | Accuracy: 0.663500 | 0.509 sec/iter\n",
      "Epoch: 399 | Batch: 001 / 011 | Total loss: 0.866 | Reg loss: 0.028 | Tree loss: 0.866 | Accuracy: 0.659500 | 0.509 sec/iter\n",
      "Epoch: 399 | Batch: 002 / 011 | Total loss: 0.828 | Reg loss: 0.028 | Tree loss: 0.828 | Accuracy: 0.699500 | 0.509 sec/iter\n",
      "Epoch: 399 | Batch: 003 / 011 | Total loss: 0.823 | Reg loss: 0.028 | Tree loss: 0.823 | Accuracy: 0.711500 | 0.509 sec/iter\n",
      "Epoch: 399 | Batch: 004 / 011 | Total loss: 0.812 | Reg loss: 0.028 | Tree loss: 0.812 | Accuracy: 0.720000 | 0.509 sec/iter\n",
      "Epoch: 399 | Batch: 005 / 011 | Total loss: 0.778 | Reg loss: 0.028 | Tree loss: 0.778 | Accuracy: 0.730500 | 0.509 sec/iter\n",
      "Epoch: 399 | Batch: 006 / 011 | Total loss: 0.778 | Reg loss: 0.028 | Tree loss: 0.778 | Accuracy: 0.724000 | 0.509 sec/iter\n",
      "Epoch: 399 | Batch: 007 / 011 | Total loss: 0.787 | Reg loss: 0.028 | Tree loss: 0.787 | Accuracy: 0.736000 | 0.509 sec/iter\n",
      "Epoch: 399 | Batch: 008 / 011 | Total loss: 0.786 | Reg loss: 0.028 | Tree loss: 0.786 | Accuracy: 0.731500 | 0.509 sec/iter\n",
      "Epoch: 399 | Batch: 009 / 011 | Total loss: 0.785 | Reg loss: 0.028 | Tree loss: 0.785 | Accuracy: 0.730500 | 0.509 sec/iter\n",
      "Epoch: 399 | Batch: 010 / 011 | Total loss: 0.763 | Reg loss: 0.028 | Tree loss: 0.763 | Accuracy: 0.754266 | 0.509 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 400 | Batch: 000 / 011 | Total loss: 0.877 | Reg loss: 0.028 | Tree loss: 0.877 | Accuracy: 0.667000 | 0.509 sec/iter\n",
      "Epoch: 400 | Batch: 001 / 011 | Total loss: 0.851 | Reg loss: 0.028 | Tree loss: 0.851 | Accuracy: 0.676000 | 0.509 sec/iter\n",
      "Epoch: 400 | Batch: 002 / 011 | Total loss: 0.854 | Reg loss: 0.028 | Tree loss: 0.854 | Accuracy: 0.690000 | 0.509 sec/iter\n",
      "Epoch: 400 | Batch: 003 / 011 | Total loss: 0.816 | Reg loss: 0.028 | Tree loss: 0.816 | Accuracy: 0.692500 | 0.509 sec/iter\n",
      "Epoch: 400 | Batch: 004 / 011 | Total loss: 0.826 | Reg loss: 0.028 | Tree loss: 0.826 | Accuracy: 0.693000 | 0.509 sec/iter\n",
      "Epoch: 400 | Batch: 005 / 011 | Total loss: 0.795 | Reg loss: 0.028 | Tree loss: 0.795 | Accuracy: 0.720000 | 0.509 sec/iter\n",
      "Epoch: 400 | Batch: 006 / 011 | Total loss: 0.789 | Reg loss: 0.028 | Tree loss: 0.789 | Accuracy: 0.737000 | 0.509 sec/iter\n",
      "Epoch: 400 | Batch: 007 / 011 | Total loss: 0.772 | Reg loss: 0.028 | Tree loss: 0.772 | Accuracy: 0.744000 | 0.508 sec/iter\n",
      "Epoch: 400 | Batch: 008 / 011 | Total loss: 0.754 | Reg loss: 0.028 | Tree loss: 0.754 | Accuracy: 0.742000 | 0.508 sec/iter\n",
      "Epoch: 400 | Batch: 009 / 011 | Total loss: 0.794 | Reg loss: 0.028 | Tree loss: 0.794 | Accuracy: 0.733000 | 0.508 sec/iter\n",
      "Epoch: 400 | Batch: 010 / 011 | Total loss: 0.737 | Reg loss: 0.028 | Tree loss: 0.737 | Accuracy: 0.781570 | 0.508 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 401 | Batch: 000 / 011 | Total loss: 0.862 | Reg loss: 0.028 | Tree loss: 0.862 | Accuracy: 0.673000 | 0.509 sec/iter\n",
      "Epoch: 401 | Batch: 001 / 011 | Total loss: 0.869 | Reg loss: 0.028 | Tree loss: 0.869 | Accuracy: 0.672000 | 0.509 sec/iter\n",
      "Epoch: 401 | Batch: 002 / 011 | Total loss: 0.821 | Reg loss: 0.028 | Tree loss: 0.821 | Accuracy: 0.705500 | 0.509 sec/iter\n",
      "Epoch: 401 | Batch: 003 / 011 | Total loss: 0.849 | Reg loss: 0.028 | Tree loss: 0.849 | Accuracy: 0.671500 | 0.509 sec/iter\n",
      "Epoch: 401 | Batch: 004 / 011 | Total loss: 0.808 | Reg loss: 0.028 | Tree loss: 0.808 | Accuracy: 0.704500 | 0.509 sec/iter\n",
      "Epoch: 401 | Batch: 005 / 011 | Total loss: 0.804 | Reg loss: 0.028 | Tree loss: 0.804 | Accuracy: 0.694500 | 0.509 sec/iter\n",
      "Epoch: 401 | Batch: 006 / 011 | Total loss: 0.775 | Reg loss: 0.028 | Tree loss: 0.775 | Accuracy: 0.722000 | 0.509 sec/iter\n",
      "Epoch: 401 | Batch: 007 / 011 | Total loss: 0.771 | Reg loss: 0.028 | Tree loss: 0.771 | Accuracy: 0.717500 | 0.509 sec/iter\n",
      "Epoch: 401 | Batch: 008 / 011 | Total loss: 0.820 | Reg loss: 0.028 | Tree loss: 0.820 | Accuracy: 0.698500 | 0.509 sec/iter\n",
      "Epoch: 401 | Batch: 009 / 011 | Total loss: 0.796 | Reg loss: 0.028 | Tree loss: 0.796 | Accuracy: 0.723500 | 0.509 sec/iter\n",
      "Epoch: 401 | Batch: 010 / 011 | Total loss: 0.792 | Reg loss: 0.028 | Tree loss: 0.792 | Accuracy: 0.706485 | 0.509 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 402 | Batch: 000 / 011 | Total loss: 0.873 | Reg loss: 0.028 | Tree loss: 0.873 | Accuracy: 0.665000 | 0.51 sec/iter\n",
      "Epoch: 402 | Batch: 001 / 011 | Total loss: 0.842 | Reg loss: 0.028 | Tree loss: 0.842 | Accuracy: 0.679500 | 0.51 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 402 | Batch: 002 / 011 | Total loss: 0.871 | Reg loss: 0.028 | Tree loss: 0.871 | Accuracy: 0.657500 | 0.51 sec/iter\n",
      "Epoch: 402 | Batch: 003 / 011 | Total loss: 0.809 | Reg loss: 0.028 | Tree loss: 0.809 | Accuracy: 0.701000 | 0.51 sec/iter\n",
      "Epoch: 402 | Batch: 004 / 011 | Total loss: 0.805 | Reg loss: 0.028 | Tree loss: 0.805 | Accuracy: 0.703500 | 0.509 sec/iter\n",
      "Epoch: 402 | Batch: 005 / 011 | Total loss: 0.784 | Reg loss: 0.028 | Tree loss: 0.784 | Accuracy: 0.722500 | 0.509 sec/iter\n",
      "Epoch: 402 | Batch: 006 / 011 | Total loss: 0.801 | Reg loss: 0.028 | Tree loss: 0.801 | Accuracy: 0.727000 | 0.509 sec/iter\n",
      "Epoch: 402 | Batch: 007 / 011 | Total loss: 0.759 | Reg loss: 0.028 | Tree loss: 0.759 | Accuracy: 0.750500 | 0.509 sec/iter\n",
      "Epoch: 402 | Batch: 008 / 011 | Total loss: 0.814 | Reg loss: 0.028 | Tree loss: 0.814 | Accuracy: 0.712000 | 0.509 sec/iter\n",
      "Epoch: 402 | Batch: 009 / 011 | Total loss: 0.784 | Reg loss: 0.028 | Tree loss: 0.784 | Accuracy: 0.714500 | 0.509 sec/iter\n",
      "Epoch: 402 | Batch: 010 / 011 | Total loss: 0.825 | Reg loss: 0.028 | Tree loss: 0.825 | Accuracy: 0.686007 | 0.509 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 403 | Batch: 000 / 011 | Total loss: 0.855 | Reg loss: 0.028 | Tree loss: 0.855 | Accuracy: 0.658000 | 0.509 sec/iter\n",
      "Epoch: 403 | Batch: 001 / 011 | Total loss: 0.853 | Reg loss: 0.028 | Tree loss: 0.853 | Accuracy: 0.685500 | 0.509 sec/iter\n",
      "Epoch: 403 | Batch: 002 / 011 | Total loss: 0.862 | Reg loss: 0.028 | Tree loss: 0.862 | Accuracy: 0.672500 | 0.509 sec/iter\n",
      "Epoch: 403 | Batch: 003 / 011 | Total loss: 0.822 | Reg loss: 0.028 | Tree loss: 0.822 | Accuracy: 0.703000 | 0.509 sec/iter\n",
      "Epoch: 403 | Batch: 004 / 011 | Total loss: 0.814 | Reg loss: 0.028 | Tree loss: 0.814 | Accuracy: 0.711000 | 0.509 sec/iter\n",
      "Epoch: 403 | Batch: 005 / 011 | Total loss: 0.803 | Reg loss: 0.028 | Tree loss: 0.803 | Accuracy: 0.709000 | 0.509 sec/iter\n",
      "Epoch: 403 | Batch: 006 / 011 | Total loss: 0.789 | Reg loss: 0.028 | Tree loss: 0.789 | Accuracy: 0.722000 | 0.509 sec/iter\n",
      "Epoch: 403 | Batch: 007 / 011 | Total loss: 0.771 | Reg loss: 0.028 | Tree loss: 0.771 | Accuracy: 0.720500 | 0.509 sec/iter\n",
      "Epoch: 403 | Batch: 008 / 011 | Total loss: 0.805 | Reg loss: 0.028 | Tree loss: 0.805 | Accuracy: 0.714000 | 0.509 sec/iter\n",
      "Epoch: 403 | Batch: 009 / 011 | Total loss: 0.770 | Reg loss: 0.028 | Tree loss: 0.770 | Accuracy: 0.730500 | 0.509 sec/iter\n",
      "Epoch: 403 | Batch: 010 / 011 | Total loss: 0.852 | Reg loss: 0.028 | Tree loss: 0.852 | Accuracy: 0.655290 | 0.509 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 404 | Batch: 000 / 011 | Total loss: 0.865 | Reg loss: 0.028 | Tree loss: 0.865 | Accuracy: 0.675000 | 0.509 sec/iter\n",
      "Epoch: 404 | Batch: 001 / 011 | Total loss: 0.889 | Reg loss: 0.028 | Tree loss: 0.889 | Accuracy: 0.651500 | 0.509 sec/iter\n",
      "Epoch: 404 | Batch: 002 / 011 | Total loss: 0.854 | Reg loss: 0.028 | Tree loss: 0.854 | Accuracy: 0.681000 | 0.509 sec/iter\n",
      "Epoch: 404 | Batch: 003 / 011 | Total loss: 0.827 | Reg loss: 0.028 | Tree loss: 0.827 | Accuracy: 0.692000 | 0.509 sec/iter\n",
      "Epoch: 404 | Batch: 004 / 011 | Total loss: 0.787 | Reg loss: 0.028 | Tree loss: 0.787 | Accuracy: 0.734500 | 0.509 sec/iter\n",
      "Epoch: 404 | Batch: 005 / 011 | Total loss: 0.801 | Reg loss: 0.028 | Tree loss: 0.801 | Accuracy: 0.713500 | 0.509 sec/iter\n",
      "Epoch: 404 | Batch: 006 / 011 | Total loss: 0.794 | Reg loss: 0.028 | Tree loss: 0.794 | Accuracy: 0.719000 | 0.509 sec/iter\n",
      "Epoch: 404 | Batch: 007 / 011 | Total loss: 0.771 | Reg loss: 0.028 | Tree loss: 0.771 | Accuracy: 0.735500 | 0.509 sec/iter\n",
      "Epoch: 404 | Batch: 008 / 011 | Total loss: 0.760 | Reg loss: 0.028 | Tree loss: 0.760 | Accuracy: 0.757500 | 0.509 sec/iter\n",
      "Epoch: 404 | Batch: 009 / 011 | Total loss: 0.799 | Reg loss: 0.028 | Tree loss: 0.799 | Accuracy: 0.724500 | 0.509 sec/iter\n",
      "Epoch: 404 | Batch: 010 / 011 | Total loss: 0.816 | Reg loss: 0.028 | Tree loss: 0.816 | Accuracy: 0.706485 | 0.509 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 405 | Batch: 000 / 011 | Total loss: 0.859 | Reg loss: 0.028 | Tree loss: 0.859 | Accuracy: 0.668500 | 0.509 sec/iter\n",
      "Epoch: 405 | Batch: 001 / 011 | Total loss: 0.863 | Reg loss: 0.028 | Tree loss: 0.863 | Accuracy: 0.674500 | 0.509 sec/iter\n",
      "Epoch: 405 | Batch: 002 / 011 | Total loss: 0.829 | Reg loss: 0.028 | Tree loss: 0.829 | Accuracy: 0.701500 | 0.509 sec/iter\n",
      "Epoch: 405 | Batch: 003 / 011 | Total loss: 0.817 | Reg loss: 0.028 | Tree loss: 0.817 | Accuracy: 0.687000 | 0.509 sec/iter\n",
      "Epoch: 405 | Batch: 004 / 011 | Total loss: 0.802 | Reg loss: 0.028 | Tree loss: 0.802 | Accuracy: 0.713000 | 0.509 sec/iter\n",
      "Epoch: 405 | Batch: 005 / 011 | Total loss: 0.789 | Reg loss: 0.028 | Tree loss: 0.789 | Accuracy: 0.702000 | 0.509 sec/iter\n",
      "Epoch: 405 | Batch: 006 / 011 | Total loss: 0.777 | Reg loss: 0.028 | Tree loss: 0.777 | Accuracy: 0.739000 | 0.509 sec/iter\n",
      "Epoch: 405 | Batch: 007 / 011 | Total loss: 0.801 | Reg loss: 0.028 | Tree loss: 0.801 | Accuracy: 0.723500 | 0.509 sec/iter\n",
      "Epoch: 405 | Batch: 008 / 011 | Total loss: 0.793 | Reg loss: 0.028 | Tree loss: 0.793 | Accuracy: 0.716500 | 0.509 sec/iter\n",
      "Epoch: 405 | Batch: 009 / 011 | Total loss: 0.791 | Reg loss: 0.028 | Tree loss: 0.791 | Accuracy: 0.746500 | 0.509 sec/iter\n",
      "Epoch: 405 | Batch: 010 / 011 | Total loss: 0.838 | Reg loss: 0.028 | Tree loss: 0.838 | Accuracy: 0.699659 | 0.509 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 406 | Batch: 000 / 011 | Total loss: 0.877 | Reg loss: 0.028 | Tree loss: 0.877 | Accuracy: 0.667500 | 0.509 sec/iter\n",
      "Epoch: 406 | Batch: 001 / 011 | Total loss: 0.867 | Reg loss: 0.028 | Tree loss: 0.867 | Accuracy: 0.675000 | 0.509 sec/iter\n",
      "Epoch: 406 | Batch: 002 / 011 | Total loss: 0.837 | Reg loss: 0.028 | Tree loss: 0.837 | Accuracy: 0.690000 | 0.508 sec/iter\n",
      "Epoch: 406 | Batch: 003 / 011 | Total loss: 0.815 | Reg loss: 0.028 | Tree loss: 0.815 | Accuracy: 0.702000 | 0.508 sec/iter\n",
      "Epoch: 406 | Batch: 004 / 011 | Total loss: 0.796 | Reg loss: 0.028 | Tree loss: 0.796 | Accuracy: 0.720000 | 0.508 sec/iter\n",
      "Epoch: 406 | Batch: 005 / 011 | Total loss: 0.790 | Reg loss: 0.028 | Tree loss: 0.790 | Accuracy: 0.714500 | 0.508 sec/iter\n",
      "Epoch: 406 | Batch: 006 / 011 | Total loss: 0.808 | Reg loss: 0.028 | Tree loss: 0.808 | Accuracy: 0.699500 | 0.508 sec/iter\n",
      "Epoch: 406 | Batch: 007 / 011 | Total loss: 0.790 | Reg loss: 0.028 | Tree loss: 0.790 | Accuracy: 0.721500 | 0.508 sec/iter\n",
      "Epoch: 406 | Batch: 008 / 011 | Total loss: 0.775 | Reg loss: 0.028 | Tree loss: 0.775 | Accuracy: 0.738500 | 0.508 sec/iter\n",
      "Epoch: 406 | Batch: 009 / 011 | Total loss: 0.789 | Reg loss: 0.028 | Tree loss: 0.789 | Accuracy: 0.730000 | 0.508 sec/iter\n",
      "Epoch: 406 | Batch: 010 / 011 | Total loss: 0.770 | Reg loss: 0.028 | Tree loss: 0.770 | Accuracy: 0.740614 | 0.508 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 407 | Batch: 000 / 011 | Total loss: 0.862 | Reg loss: 0.028 | Tree loss: 0.862 | Accuracy: 0.672500 | 0.508 sec/iter\n",
      "Epoch: 407 | Batch: 001 / 011 | Total loss: 0.831 | Reg loss: 0.028 | Tree loss: 0.831 | Accuracy: 0.677500 | 0.508 sec/iter\n",
      "Epoch: 407 | Batch: 002 / 011 | Total loss: 0.835 | Reg loss: 0.028 | Tree loss: 0.835 | Accuracy: 0.688500 | 0.508 sec/iter\n",
      "Epoch: 407 | Batch: 003 / 011 | Total loss: 0.822 | Reg loss: 0.028 | Tree loss: 0.822 | Accuracy: 0.695500 | 0.508 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 407 | Batch: 004 / 011 | Total loss: 0.809 | Reg loss: 0.028 | Tree loss: 0.809 | Accuracy: 0.715000 | 0.508 sec/iter\n",
      "Epoch: 407 | Batch: 005 / 011 | Total loss: 0.808 | Reg loss: 0.028 | Tree loss: 0.808 | Accuracy: 0.723000 | 0.508 sec/iter\n",
      "Epoch: 407 | Batch: 006 / 011 | Total loss: 0.776 | Reg loss: 0.028 | Tree loss: 0.776 | Accuracy: 0.737000 | 0.508 sec/iter\n",
      "Epoch: 407 | Batch: 007 / 011 | Total loss: 0.809 | Reg loss: 0.028 | Tree loss: 0.809 | Accuracy: 0.717000 | 0.508 sec/iter\n",
      "Epoch: 407 | Batch: 008 / 011 | Total loss: 0.770 | Reg loss: 0.028 | Tree loss: 0.770 | Accuracy: 0.740000 | 0.508 sec/iter\n",
      "Epoch: 407 | Batch: 009 / 011 | Total loss: 0.802 | Reg loss: 0.028 | Tree loss: 0.802 | Accuracy: 0.717000 | 0.508 sec/iter\n",
      "Epoch: 407 | Batch: 010 / 011 | Total loss: 0.809 | Reg loss: 0.028 | Tree loss: 0.809 | Accuracy: 0.703072 | 0.508 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 408 | Batch: 000 / 011 | Total loss: 0.853 | Reg loss: 0.028 | Tree loss: 0.853 | Accuracy: 0.670500 | 0.509 sec/iter\n",
      "Epoch: 408 | Batch: 001 / 011 | Total loss: 0.874 | Reg loss: 0.028 | Tree loss: 0.874 | Accuracy: 0.656000 | 0.509 sec/iter\n",
      "Epoch: 408 | Batch: 002 / 011 | Total loss: 0.829 | Reg loss: 0.028 | Tree loss: 0.829 | Accuracy: 0.690000 | 0.509 sec/iter\n",
      "Epoch: 408 | Batch: 003 / 011 | Total loss: 0.851 | Reg loss: 0.028 | Tree loss: 0.851 | Accuracy: 0.678500 | 0.509 sec/iter\n",
      "Epoch: 408 | Batch: 004 / 011 | Total loss: 0.788 | Reg loss: 0.028 | Tree loss: 0.788 | Accuracy: 0.730000 | 0.509 sec/iter\n",
      "Epoch: 408 | Batch: 005 / 011 | Total loss: 0.777 | Reg loss: 0.028 | Tree loss: 0.777 | Accuracy: 0.723500 | 0.509 sec/iter\n",
      "Epoch: 408 | Batch: 006 / 011 | Total loss: 0.764 | Reg loss: 0.028 | Tree loss: 0.764 | Accuracy: 0.732500 | 0.509 sec/iter\n",
      "Epoch: 408 | Batch: 007 / 011 | Total loss: 0.798 | Reg loss: 0.028 | Tree loss: 0.798 | Accuracy: 0.718500 | 0.509 sec/iter\n",
      "Epoch: 408 | Batch: 008 / 011 | Total loss: 0.810 | Reg loss: 0.028 | Tree loss: 0.810 | Accuracy: 0.711500 | 0.508 sec/iter\n",
      "Epoch: 408 | Batch: 009 / 011 | Total loss: 0.797 | Reg loss: 0.028 | Tree loss: 0.797 | Accuracy: 0.717500 | 0.508 sec/iter\n",
      "Epoch: 408 | Batch: 010 / 011 | Total loss: 0.783 | Reg loss: 0.028 | Tree loss: 0.783 | Accuracy: 0.730375 | 0.508 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 409 | Batch: 000 / 011 | Total loss: 0.873 | Reg loss: 0.028 | Tree loss: 0.873 | Accuracy: 0.665500 | 0.508 sec/iter\n",
      "Epoch: 409 | Batch: 001 / 011 | Total loss: 0.856 | Reg loss: 0.028 | Tree loss: 0.856 | Accuracy: 0.665000 | 0.508 sec/iter\n",
      "Epoch: 409 | Batch: 002 / 011 | Total loss: 0.859 | Reg loss: 0.028 | Tree loss: 0.859 | Accuracy: 0.685500 | 0.508 sec/iter\n",
      "Epoch: 409 | Batch: 003 / 011 | Total loss: 0.805 | Reg loss: 0.028 | Tree loss: 0.805 | Accuracy: 0.699500 | 0.508 sec/iter\n",
      "Epoch: 409 | Batch: 004 / 011 | Total loss: 0.807 | Reg loss: 0.028 | Tree loss: 0.807 | Accuracy: 0.710500 | 0.508 sec/iter\n",
      "Epoch: 409 | Batch: 005 / 011 | Total loss: 0.786 | Reg loss: 0.028 | Tree loss: 0.786 | Accuracy: 0.716500 | 0.508 sec/iter\n",
      "Epoch: 409 | Batch: 006 / 011 | Total loss: 0.779 | Reg loss: 0.028 | Tree loss: 0.779 | Accuracy: 0.721500 | 0.508 sec/iter\n",
      "Epoch: 409 | Batch: 007 / 011 | Total loss: 0.784 | Reg loss: 0.028 | Tree loss: 0.784 | Accuracy: 0.739500 | 0.508 sec/iter\n",
      "Epoch: 409 | Batch: 008 / 011 | Total loss: 0.772 | Reg loss: 0.028 | Tree loss: 0.772 | Accuracy: 0.744000 | 0.508 sec/iter\n",
      "Epoch: 409 | Batch: 009 / 011 | Total loss: 0.803 | Reg loss: 0.028 | Tree loss: 0.803 | Accuracy: 0.733000 | 0.508 sec/iter\n",
      "Epoch: 409 | Batch: 010 / 011 | Total loss: 0.759 | Reg loss: 0.028 | Tree loss: 0.759 | Accuracy: 0.703072 | 0.508 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 410 | Batch: 000 / 011 | Total loss: 0.870 | Reg loss: 0.028 | Tree loss: 0.870 | Accuracy: 0.666000 | 0.508 sec/iter\n",
      "Epoch: 410 | Batch: 001 / 011 | Total loss: 0.879 | Reg loss: 0.028 | Tree loss: 0.879 | Accuracy: 0.649500 | 0.508 sec/iter\n",
      "Epoch: 410 | Batch: 002 / 011 | Total loss: 0.826 | Reg loss: 0.028 | Tree loss: 0.826 | Accuracy: 0.719500 | 0.508 sec/iter\n",
      "Epoch: 410 | Batch: 003 / 011 | Total loss: 0.836 | Reg loss: 0.028 | Tree loss: 0.836 | Accuracy: 0.689500 | 0.508 sec/iter\n",
      "Epoch: 410 | Batch: 004 / 011 | Total loss: 0.799 | Reg loss: 0.028 | Tree loss: 0.799 | Accuracy: 0.725500 | 0.508 sec/iter\n",
      "Epoch: 410 | Batch: 005 / 011 | Total loss: 0.786 | Reg loss: 0.028 | Tree loss: 0.786 | Accuracy: 0.728000 | 0.508 sec/iter\n",
      "Epoch: 410 | Batch: 006 / 011 | Total loss: 0.784 | Reg loss: 0.028 | Tree loss: 0.784 | Accuracy: 0.723000 | 0.508 sec/iter\n",
      "Epoch: 410 | Batch: 007 / 011 | Total loss: 0.777 | Reg loss: 0.028 | Tree loss: 0.777 | Accuracy: 0.732500 | 0.508 sec/iter\n",
      "Epoch: 410 | Batch: 008 / 011 | Total loss: 0.778 | Reg loss: 0.028 | Tree loss: 0.778 | Accuracy: 0.728500 | 0.508 sec/iter\n",
      "Epoch: 410 | Batch: 009 / 011 | Total loss: 0.800 | Reg loss: 0.028 | Tree loss: 0.800 | Accuracy: 0.724500 | 0.508 sec/iter\n",
      "Epoch: 410 | Batch: 010 / 011 | Total loss: 0.718 | Reg loss: 0.028 | Tree loss: 0.718 | Accuracy: 0.788396 | 0.508 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 411 | Batch: 000 / 011 | Total loss: 0.901 | Reg loss: 0.028 | Tree loss: 0.901 | Accuracy: 0.642000 | 0.508 sec/iter\n",
      "Epoch: 411 | Batch: 001 / 011 | Total loss: 0.831 | Reg loss: 0.028 | Tree loss: 0.831 | Accuracy: 0.685500 | 0.508 sec/iter\n",
      "Epoch: 411 | Batch: 002 / 011 | Total loss: 0.827 | Reg loss: 0.028 | Tree loss: 0.827 | Accuracy: 0.688500 | 0.508 sec/iter\n",
      "Epoch: 411 | Batch: 003 / 011 | Total loss: 0.818 | Reg loss: 0.028 | Tree loss: 0.818 | Accuracy: 0.690000 | 0.508 sec/iter\n",
      "Epoch: 411 | Batch: 004 / 011 | Total loss: 0.806 | Reg loss: 0.028 | Tree loss: 0.806 | Accuracy: 0.705500 | 0.508 sec/iter\n",
      "Epoch: 411 | Batch: 005 / 011 | Total loss: 0.778 | Reg loss: 0.028 | Tree loss: 0.778 | Accuracy: 0.733000 | 0.508 sec/iter\n",
      "Epoch: 411 | Batch: 006 / 011 | Total loss: 0.797 | Reg loss: 0.028 | Tree loss: 0.797 | Accuracy: 0.719000 | 0.508 sec/iter\n",
      "Epoch: 411 | Batch: 007 / 011 | Total loss: 0.790 | Reg loss: 0.028 | Tree loss: 0.790 | Accuracy: 0.729000 | 0.508 sec/iter\n",
      "Epoch: 411 | Batch: 008 / 011 | Total loss: 0.788 | Reg loss: 0.028 | Tree loss: 0.788 | Accuracy: 0.728500 | 0.508 sec/iter\n",
      "Epoch: 411 | Batch: 009 / 011 | Total loss: 0.791 | Reg loss: 0.028 | Tree loss: 0.791 | Accuracy: 0.733000 | 0.508 sec/iter\n",
      "Epoch: 411 | Batch: 010 / 011 | Total loss: 0.801 | Reg loss: 0.028 | Tree loss: 0.801 | Accuracy: 0.689420 | 0.508 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 412 | Batch: 000 / 011 | Total loss: 0.890 | Reg loss: 0.028 | Tree loss: 0.890 | Accuracy: 0.658000 | 0.508 sec/iter\n",
      "Epoch: 412 | Batch: 001 / 011 | Total loss: 0.853 | Reg loss: 0.028 | Tree loss: 0.853 | Accuracy: 0.669000 | 0.508 sec/iter\n",
      "Epoch: 412 | Batch: 002 / 011 | Total loss: 0.828 | Reg loss: 0.028 | Tree loss: 0.828 | Accuracy: 0.687500 | 0.508 sec/iter\n",
      "Epoch: 412 | Batch: 003 / 011 | Total loss: 0.834 | Reg loss: 0.028 | Tree loss: 0.834 | Accuracy: 0.688000 | 0.508 sec/iter\n",
      "Epoch: 412 | Batch: 004 / 011 | Total loss: 0.809 | Reg loss: 0.028 | Tree loss: 0.809 | Accuracy: 0.710500 | 0.508 sec/iter\n",
      "Epoch: 412 | Batch: 005 / 011 | Total loss: 0.794 | Reg loss: 0.028 | Tree loss: 0.794 | Accuracy: 0.716500 | 0.508 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 412 | Batch: 006 / 011 | Total loss: 0.805 | Reg loss: 0.028 | Tree loss: 0.805 | Accuracy: 0.714000 | 0.508 sec/iter\n",
      "Epoch: 412 | Batch: 007 / 011 | Total loss: 0.755 | Reg loss: 0.028 | Tree loss: 0.755 | Accuracy: 0.747500 | 0.508 sec/iter\n",
      "Epoch: 412 | Batch: 008 / 011 | Total loss: 0.780 | Reg loss: 0.028 | Tree loss: 0.780 | Accuracy: 0.732000 | 0.508 sec/iter\n",
      "Epoch: 412 | Batch: 009 / 011 | Total loss: 0.783 | Reg loss: 0.028 | Tree loss: 0.783 | Accuracy: 0.734500 | 0.508 sec/iter\n",
      "Epoch: 412 | Batch: 010 / 011 | Total loss: 0.815 | Reg loss: 0.028 | Tree loss: 0.815 | Accuracy: 0.686007 | 0.508 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 413 | Batch: 000 / 011 | Total loss: 0.878 | Reg loss: 0.028 | Tree loss: 0.878 | Accuracy: 0.662500 | 0.508 sec/iter\n",
      "Epoch: 413 | Batch: 001 / 011 | Total loss: 0.867 | Reg loss: 0.028 | Tree loss: 0.867 | Accuracy: 0.670000 | 0.508 sec/iter\n",
      "Epoch: 413 | Batch: 002 / 011 | Total loss: 0.842 | Reg loss: 0.028 | Tree loss: 0.842 | Accuracy: 0.690000 | 0.508 sec/iter\n",
      "Epoch: 413 | Batch: 003 / 011 | Total loss: 0.814 | Reg loss: 0.028 | Tree loss: 0.814 | Accuracy: 0.707000 | 0.508 sec/iter\n",
      "Epoch: 413 | Batch: 004 / 011 | Total loss: 0.821 | Reg loss: 0.028 | Tree loss: 0.821 | Accuracy: 0.691000 | 0.508 sec/iter\n",
      "Epoch: 413 | Batch: 005 / 011 | Total loss: 0.791 | Reg loss: 0.028 | Tree loss: 0.791 | Accuracy: 0.714000 | 0.508 sec/iter\n",
      "Epoch: 413 | Batch: 006 / 011 | Total loss: 0.732 | Reg loss: 0.028 | Tree loss: 0.732 | Accuracy: 0.752500 | 0.507 sec/iter\n",
      "Epoch: 413 | Batch: 007 / 011 | Total loss: 0.779 | Reg loss: 0.028 | Tree loss: 0.779 | Accuracy: 0.735000 | 0.507 sec/iter\n",
      "Epoch: 413 | Batch: 008 / 011 | Total loss: 0.793 | Reg loss: 0.028 | Tree loss: 0.793 | Accuracy: 0.728000 | 0.507 sec/iter\n",
      "Epoch: 413 | Batch: 009 / 011 | Total loss: 0.800 | Reg loss: 0.028 | Tree loss: 0.800 | Accuracy: 0.723000 | 0.507 sec/iter\n",
      "Epoch: 413 | Batch: 010 / 011 | Total loss: 0.773 | Reg loss: 0.028 | Tree loss: 0.773 | Accuracy: 0.723549 | 0.507 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 414 | Batch: 000 / 011 | Total loss: 0.862 | Reg loss: 0.028 | Tree loss: 0.862 | Accuracy: 0.670000 | 0.508 sec/iter\n",
      "Epoch: 414 | Batch: 001 / 011 | Total loss: 0.854 | Reg loss: 0.028 | Tree loss: 0.854 | Accuracy: 0.685500 | 0.508 sec/iter\n",
      "Epoch: 414 | Batch: 002 / 011 | Total loss: 0.834 | Reg loss: 0.028 | Tree loss: 0.834 | Accuracy: 0.688000 | 0.508 sec/iter\n",
      "Epoch: 414 | Batch: 003 / 011 | Total loss: 0.815 | Reg loss: 0.028 | Tree loss: 0.815 | Accuracy: 0.691500 | 0.508 sec/iter\n",
      "Epoch: 414 | Batch: 004 / 011 | Total loss: 0.831 | Reg loss: 0.028 | Tree loss: 0.831 | Accuracy: 0.697000 | 0.508 sec/iter\n",
      "Epoch: 414 | Batch: 005 / 011 | Total loss: 0.810 | Reg loss: 0.028 | Tree loss: 0.810 | Accuracy: 0.706500 | 0.508 sec/iter\n",
      "Epoch: 414 | Batch: 006 / 011 | Total loss: 0.786 | Reg loss: 0.028 | Tree loss: 0.786 | Accuracy: 0.719000 | 0.508 sec/iter\n",
      "Epoch: 414 | Batch: 007 / 011 | Total loss: 0.767 | Reg loss: 0.028 | Tree loss: 0.767 | Accuracy: 0.734000 | 0.508 sec/iter\n",
      "Epoch: 414 | Batch: 008 / 011 | Total loss: 0.783 | Reg loss: 0.028 | Tree loss: 0.783 | Accuracy: 0.730500 | 0.508 sec/iter\n",
      "Epoch: 414 | Batch: 009 / 011 | Total loss: 0.769 | Reg loss: 0.028 | Tree loss: 0.769 | Accuracy: 0.741000 | 0.508 sec/iter\n",
      "Epoch: 414 | Batch: 010 / 011 | Total loss: 0.819 | Reg loss: 0.028 | Tree loss: 0.819 | Accuracy: 0.726962 | 0.508 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 415 | Batch: 000 / 011 | Total loss: 0.863 | Reg loss: 0.028 | Tree loss: 0.863 | Accuracy: 0.680500 | 0.509 sec/iter\n",
      "Epoch: 415 | Batch: 001 / 011 | Total loss: 0.833 | Reg loss: 0.028 | Tree loss: 0.833 | Accuracy: 0.692000 | 0.509 sec/iter\n",
      "Epoch: 415 | Batch: 002 / 011 | Total loss: 0.846 | Reg loss: 0.028 | Tree loss: 0.846 | Accuracy: 0.689500 | 0.509 sec/iter\n",
      "Epoch: 415 | Batch: 003 / 011 | Total loss: 0.822 | Reg loss: 0.028 | Tree loss: 0.822 | Accuracy: 0.702000 | 0.509 sec/iter\n",
      "Epoch: 415 | Batch: 004 / 011 | Total loss: 0.808 | Reg loss: 0.028 | Tree loss: 0.808 | Accuracy: 0.713000 | 0.509 sec/iter\n",
      "Epoch: 415 | Batch: 005 / 011 | Total loss: 0.777 | Reg loss: 0.028 | Tree loss: 0.777 | Accuracy: 0.732500 | 0.508 sec/iter\n",
      "Epoch: 415 | Batch: 006 / 011 | Total loss: 0.780 | Reg loss: 0.028 | Tree loss: 0.780 | Accuracy: 0.719000 | 0.508 sec/iter\n",
      "Epoch: 415 | Batch: 007 / 011 | Total loss: 0.800 | Reg loss: 0.028 | Tree loss: 0.800 | Accuracy: 0.720000 | 0.508 sec/iter\n",
      "Epoch: 415 | Batch: 008 / 011 | Total loss: 0.793 | Reg loss: 0.028 | Tree loss: 0.793 | Accuracy: 0.722000 | 0.508 sec/iter\n",
      "Epoch: 415 | Batch: 009 / 011 | Total loss: 0.784 | Reg loss: 0.028 | Tree loss: 0.784 | Accuracy: 0.735500 | 0.508 sec/iter\n",
      "Epoch: 415 | Batch: 010 / 011 | Total loss: 0.786 | Reg loss: 0.028 | Tree loss: 0.786 | Accuracy: 0.737201 | 0.508 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 416 | Batch: 000 / 011 | Total loss: 0.876 | Reg loss: 0.028 | Tree loss: 0.876 | Accuracy: 0.656500 | 0.508 sec/iter\n",
      "Epoch: 416 | Batch: 001 / 011 | Total loss: 0.849 | Reg loss: 0.028 | Tree loss: 0.849 | Accuracy: 0.684000 | 0.508 sec/iter\n",
      "Epoch: 416 | Batch: 002 / 011 | Total loss: 0.853 | Reg loss: 0.028 | Tree loss: 0.853 | Accuracy: 0.683000 | 0.508 sec/iter\n",
      "Epoch: 416 | Batch: 003 / 011 | Total loss: 0.817 | Reg loss: 0.028 | Tree loss: 0.817 | Accuracy: 0.687000 | 0.508 sec/iter\n",
      "Epoch: 416 | Batch: 004 / 011 | Total loss: 0.788 | Reg loss: 0.028 | Tree loss: 0.788 | Accuracy: 0.708000 | 0.508 sec/iter\n",
      "Epoch: 416 | Batch: 005 / 011 | Total loss: 0.796 | Reg loss: 0.028 | Tree loss: 0.796 | Accuracy: 0.703000 | 0.508 sec/iter\n",
      "Epoch: 416 | Batch: 006 / 011 | Total loss: 0.771 | Reg loss: 0.028 | Tree loss: 0.771 | Accuracy: 0.749000 | 0.508 sec/iter\n",
      "Epoch: 416 | Batch: 007 / 011 | Total loss: 0.795 | Reg loss: 0.028 | Tree loss: 0.795 | Accuracy: 0.732500 | 0.508 sec/iter\n",
      "Epoch: 416 | Batch: 008 / 011 | Total loss: 0.795 | Reg loss: 0.028 | Tree loss: 0.795 | Accuracy: 0.740500 | 0.508 sec/iter\n",
      "Epoch: 416 | Batch: 009 / 011 | Total loss: 0.773 | Reg loss: 0.028 | Tree loss: 0.773 | Accuracy: 0.744000 | 0.508 sec/iter\n",
      "Epoch: 416 | Batch: 010 / 011 | Total loss: 0.776 | Reg loss: 0.028 | Tree loss: 0.776 | Accuracy: 0.737201 | 0.508 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 417 | Batch: 000 / 011 | Total loss: 0.854 | Reg loss: 0.028 | Tree loss: 0.854 | Accuracy: 0.665500 | 0.509 sec/iter\n",
      "Epoch: 417 | Batch: 001 / 011 | Total loss: 0.856 | Reg loss: 0.028 | Tree loss: 0.856 | Accuracy: 0.681500 | 0.508 sec/iter\n",
      "Epoch: 417 | Batch: 002 / 011 | Total loss: 0.847 | Reg loss: 0.028 | Tree loss: 0.847 | Accuracy: 0.670500 | 0.508 sec/iter\n",
      "Epoch: 417 | Batch: 003 / 011 | Total loss: 0.815 | Reg loss: 0.028 | Tree loss: 0.815 | Accuracy: 0.700500 | 0.508 sec/iter\n",
      "Epoch: 417 | Batch: 004 / 011 | Total loss: 0.823 | Reg loss: 0.028 | Tree loss: 0.823 | Accuracy: 0.690500 | 0.508 sec/iter\n",
      "Epoch: 417 | Batch: 005 / 011 | Total loss: 0.804 | Reg loss: 0.028 | Tree loss: 0.804 | Accuracy: 0.714000 | 0.508 sec/iter\n",
      "Epoch: 417 | Batch: 006 / 011 | Total loss: 0.799 | Reg loss: 0.028 | Tree loss: 0.799 | Accuracy: 0.714500 | 0.508 sec/iter\n",
      "Epoch: 417 | Batch: 007 / 011 | Total loss: 0.799 | Reg loss: 0.028 | Tree loss: 0.799 | Accuracy: 0.709000 | 0.508 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 417 | Batch: 008 / 011 | Total loss: 0.769 | Reg loss: 0.028 | Tree loss: 0.769 | Accuracy: 0.739500 | 0.508 sec/iter\n",
      "Epoch: 417 | Batch: 009 / 011 | Total loss: 0.775 | Reg loss: 0.028 | Tree loss: 0.775 | Accuracy: 0.741000 | 0.508 sec/iter\n",
      "Epoch: 417 | Batch: 010 / 011 | Total loss: 0.806 | Reg loss: 0.028 | Tree loss: 0.806 | Accuracy: 0.709898 | 0.508 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 418 | Batch: 000 / 011 | Total loss: 0.873 | Reg loss: 0.028 | Tree loss: 0.873 | Accuracy: 0.663000 | 0.509 sec/iter\n",
      "Epoch: 418 | Batch: 001 / 011 | Total loss: 0.847 | Reg loss: 0.028 | Tree loss: 0.847 | Accuracy: 0.671500 | 0.509 sec/iter\n",
      "Epoch: 418 | Batch: 002 / 011 | Total loss: 0.834 | Reg loss: 0.028 | Tree loss: 0.834 | Accuracy: 0.695500 | 0.509 sec/iter\n",
      "Epoch: 418 | Batch: 003 / 011 | Total loss: 0.809 | Reg loss: 0.028 | Tree loss: 0.809 | Accuracy: 0.688000 | 0.509 sec/iter\n",
      "Epoch: 418 | Batch: 004 / 011 | Total loss: 0.782 | Reg loss: 0.028 | Tree loss: 0.782 | Accuracy: 0.717000 | 0.509 sec/iter\n",
      "Epoch: 418 | Batch: 005 / 011 | Total loss: 0.786 | Reg loss: 0.028 | Tree loss: 0.786 | Accuracy: 0.718000 | 0.509 sec/iter\n",
      "Epoch: 418 | Batch: 006 / 011 | Total loss: 0.795 | Reg loss: 0.028 | Tree loss: 0.795 | Accuracy: 0.739500 | 0.509 sec/iter\n",
      "Epoch: 418 | Batch: 007 / 011 | Total loss: 0.791 | Reg loss: 0.028 | Tree loss: 0.791 | Accuracy: 0.723000 | 0.509 sec/iter\n",
      "Epoch: 418 | Batch: 008 / 011 | Total loss: 0.790 | Reg loss: 0.028 | Tree loss: 0.790 | Accuracy: 0.731500 | 0.509 sec/iter\n",
      "Epoch: 418 | Batch: 009 / 011 | Total loss: 0.808 | Reg loss: 0.028 | Tree loss: 0.808 | Accuracy: 0.724500 | 0.509 sec/iter\n",
      "Epoch: 418 | Batch: 010 / 011 | Total loss: 0.823 | Reg loss: 0.028 | Tree loss: 0.823 | Accuracy: 0.689420 | 0.508 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 419 | Batch: 000 / 011 | Total loss: 0.865 | Reg loss: 0.028 | Tree loss: 0.865 | Accuracy: 0.670500 | 0.508 sec/iter\n",
      "Epoch: 419 | Batch: 001 / 011 | Total loss: 0.850 | Reg loss: 0.028 | Tree loss: 0.850 | Accuracy: 0.666500 | 0.508 sec/iter\n",
      "Epoch: 419 | Batch: 002 / 011 | Total loss: 0.840 | Reg loss: 0.028 | Tree loss: 0.840 | Accuracy: 0.685500 | 0.508 sec/iter\n",
      "Epoch: 419 | Batch: 003 / 011 | Total loss: 0.841 | Reg loss: 0.028 | Tree loss: 0.841 | Accuracy: 0.671000 | 0.508 sec/iter\n",
      "Epoch: 419 | Batch: 004 / 011 | Total loss: 0.805 | Reg loss: 0.028 | Tree loss: 0.805 | Accuracy: 0.714500 | 0.508 sec/iter\n",
      "Epoch: 419 | Batch: 005 / 011 | Total loss: 0.781 | Reg loss: 0.028 | Tree loss: 0.781 | Accuracy: 0.733000 | 0.508 sec/iter\n",
      "Epoch: 419 | Batch: 006 / 011 | Total loss: 0.775 | Reg loss: 0.028 | Tree loss: 0.775 | Accuracy: 0.723000 | 0.508 sec/iter\n",
      "Epoch: 419 | Batch: 007 / 011 | Total loss: 0.797 | Reg loss: 0.028 | Tree loss: 0.797 | Accuracy: 0.720500 | 0.508 sec/iter\n",
      "Epoch: 419 | Batch: 008 / 011 | Total loss: 0.770 | Reg loss: 0.028 | Tree loss: 0.770 | Accuracy: 0.746000 | 0.508 sec/iter\n",
      "Epoch: 419 | Batch: 009 / 011 | Total loss: 0.807 | Reg loss: 0.028 | Tree loss: 0.807 | Accuracy: 0.718000 | 0.508 sec/iter\n",
      "Epoch: 419 | Batch: 010 / 011 | Total loss: 0.758 | Reg loss: 0.028 | Tree loss: 0.758 | Accuracy: 0.723549 | 0.508 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 420 | Batch: 000 / 011 | Total loss: 0.882 | Reg loss: 0.028 | Tree loss: 0.882 | Accuracy: 0.655500 | 0.509 sec/iter\n",
      "Epoch: 420 | Batch: 001 / 011 | Total loss: 0.874 | Reg loss: 0.028 | Tree loss: 0.874 | Accuracy: 0.663500 | 0.509 sec/iter\n",
      "Epoch: 420 | Batch: 002 / 011 | Total loss: 0.830 | Reg loss: 0.028 | Tree loss: 0.830 | Accuracy: 0.718000 | 0.509 sec/iter\n",
      "Epoch: 420 | Batch: 003 / 011 | Total loss: 0.850 | Reg loss: 0.028 | Tree loss: 0.850 | Accuracy: 0.676500 | 0.509 sec/iter\n",
      "Epoch: 420 | Batch: 004 / 011 | Total loss: 0.801 | Reg loss: 0.028 | Tree loss: 0.801 | Accuracy: 0.717500 | 0.509 sec/iter\n",
      "Epoch: 420 | Batch: 005 / 011 | Total loss: 0.782 | Reg loss: 0.028 | Tree loss: 0.782 | Accuracy: 0.721000 | 0.509 sec/iter\n",
      "Epoch: 420 | Batch: 006 / 011 | Total loss: 0.786 | Reg loss: 0.028 | Tree loss: 0.786 | Accuracy: 0.718500 | 0.509 sec/iter\n",
      "Epoch: 420 | Batch: 007 / 011 | Total loss: 0.757 | Reg loss: 0.028 | Tree loss: 0.757 | Accuracy: 0.744500 | 0.508 sec/iter\n",
      "Epoch: 420 | Batch: 008 / 011 | Total loss: 0.767 | Reg loss: 0.028 | Tree loss: 0.767 | Accuracy: 0.745000 | 0.508 sec/iter\n",
      "Epoch: 420 | Batch: 009 / 011 | Total loss: 0.774 | Reg loss: 0.028 | Tree loss: 0.774 | Accuracy: 0.740500 | 0.508 sec/iter\n",
      "Epoch: 420 | Batch: 010 / 011 | Total loss: 0.811 | Reg loss: 0.028 | Tree loss: 0.811 | Accuracy: 0.696246 | 0.508 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 421 | Batch: 000 / 011 | Total loss: 0.898 | Reg loss: 0.028 | Tree loss: 0.898 | Accuracy: 0.653000 | 0.509 sec/iter\n",
      "Epoch: 421 | Batch: 001 / 011 | Total loss: 0.873 | Reg loss: 0.028 | Tree loss: 0.873 | Accuracy: 0.650000 | 0.509 sec/iter\n",
      "Epoch: 421 | Batch: 002 / 011 | Total loss: 0.843 | Reg loss: 0.028 | Tree loss: 0.843 | Accuracy: 0.688500 | 0.509 sec/iter\n",
      "Epoch: 421 | Batch: 003 / 011 | Total loss: 0.814 | Reg loss: 0.028 | Tree loss: 0.814 | Accuracy: 0.704500 | 0.509 sec/iter\n",
      "Epoch: 421 | Batch: 004 / 011 | Total loss: 0.800 | Reg loss: 0.028 | Tree loss: 0.800 | Accuracy: 0.723500 | 0.509 sec/iter\n",
      "Epoch: 421 | Batch: 005 / 011 | Total loss: 0.803 | Reg loss: 0.028 | Tree loss: 0.803 | Accuracy: 0.710000 | 0.509 sec/iter\n",
      "Epoch: 421 | Batch: 006 / 011 | Total loss: 0.797 | Reg loss: 0.028 | Tree loss: 0.797 | Accuracy: 0.710500 | 0.509 sec/iter\n",
      "Epoch: 421 | Batch: 007 / 011 | Total loss: 0.772 | Reg loss: 0.028 | Tree loss: 0.772 | Accuracy: 0.727500 | 0.509 sec/iter\n",
      "Epoch: 421 | Batch: 008 / 011 | Total loss: 0.785 | Reg loss: 0.028 | Tree loss: 0.785 | Accuracy: 0.724500 | 0.509 sec/iter\n",
      "Epoch: 421 | Batch: 009 / 011 | Total loss: 0.763 | Reg loss: 0.028 | Tree loss: 0.763 | Accuracy: 0.753500 | 0.509 sec/iter\n",
      "Epoch: 421 | Batch: 010 / 011 | Total loss: 0.747 | Reg loss: 0.028 | Tree loss: 0.747 | Accuracy: 0.774744 | 0.509 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 422 | Batch: 000 / 011 | Total loss: 0.869 | Reg loss: 0.028 | Tree loss: 0.869 | Accuracy: 0.667500 | 0.509 sec/iter\n",
      "Epoch: 422 | Batch: 001 / 011 | Total loss: 0.856 | Reg loss: 0.028 | Tree loss: 0.856 | Accuracy: 0.675500 | 0.509 sec/iter\n",
      "Epoch: 422 | Batch: 002 / 011 | Total loss: 0.831 | Reg loss: 0.028 | Tree loss: 0.831 | Accuracy: 0.696500 | 0.509 sec/iter\n",
      "Epoch: 422 | Batch: 003 / 011 | Total loss: 0.824 | Reg loss: 0.028 | Tree loss: 0.824 | Accuracy: 0.680000 | 0.509 sec/iter\n",
      "Epoch: 422 | Batch: 004 / 011 | Total loss: 0.820 | Reg loss: 0.028 | Tree loss: 0.820 | Accuracy: 0.698000 | 0.509 sec/iter\n",
      "Epoch: 422 | Batch: 005 / 011 | Total loss: 0.800 | Reg loss: 0.028 | Tree loss: 0.800 | Accuracy: 0.723000 | 0.509 sec/iter\n",
      "Epoch: 422 | Batch: 006 / 011 | Total loss: 0.765 | Reg loss: 0.028 | Tree loss: 0.765 | Accuracy: 0.743000 | 0.509 sec/iter\n",
      "Epoch: 422 | Batch: 007 / 011 | Total loss: 0.784 | Reg loss: 0.028 | Tree loss: 0.784 | Accuracy: 0.733000 | 0.509 sec/iter\n",
      "Epoch: 422 | Batch: 008 / 011 | Total loss: 0.783 | Reg loss: 0.028 | Tree loss: 0.783 | Accuracy: 0.722500 | 0.509 sec/iter\n",
      "Epoch: 422 | Batch: 009 / 011 | Total loss: 0.788 | Reg loss: 0.028 | Tree loss: 0.788 | Accuracy: 0.727000 | 0.509 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 422 | Batch: 010 / 011 | Total loss: 0.798 | Reg loss: 0.028 | Tree loss: 0.798 | Accuracy: 0.747440 | 0.509 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 423 | Batch: 000 / 011 | Total loss: 0.871 | Reg loss: 0.028 | Tree loss: 0.871 | Accuracy: 0.667500 | 0.509 sec/iter\n",
      "Epoch: 423 | Batch: 001 / 011 | Total loss: 0.843 | Reg loss: 0.028 | Tree loss: 0.843 | Accuracy: 0.673500 | 0.509 sec/iter\n",
      "Epoch: 423 | Batch: 002 / 011 | Total loss: 0.848 | Reg loss: 0.028 | Tree loss: 0.848 | Accuracy: 0.685000 | 0.509 sec/iter\n",
      "Epoch: 423 | Batch: 003 / 011 | Total loss: 0.783 | Reg loss: 0.028 | Tree loss: 0.783 | Accuracy: 0.715000 | 0.509 sec/iter\n",
      "Epoch: 423 | Batch: 004 / 011 | Total loss: 0.808 | Reg loss: 0.028 | Tree loss: 0.808 | Accuracy: 0.710500 | 0.509 sec/iter\n",
      "Epoch: 423 | Batch: 005 / 011 | Total loss: 0.780 | Reg loss: 0.028 | Tree loss: 0.780 | Accuracy: 0.716000 | 0.509 sec/iter\n",
      "Epoch: 423 | Batch: 006 / 011 | Total loss: 0.794 | Reg loss: 0.028 | Tree loss: 0.794 | Accuracy: 0.717500 | 0.509 sec/iter\n",
      "Epoch: 423 | Batch: 007 / 011 | Total loss: 0.780 | Reg loss: 0.028 | Tree loss: 0.780 | Accuracy: 0.736000 | 0.508 sec/iter\n",
      "Epoch: 423 | Batch: 008 / 011 | Total loss: 0.786 | Reg loss: 0.028 | Tree loss: 0.786 | Accuracy: 0.739000 | 0.508 sec/iter\n",
      "Epoch: 423 | Batch: 009 / 011 | Total loss: 0.801 | Reg loss: 0.028 | Tree loss: 0.801 | Accuracy: 0.720000 | 0.508 sec/iter\n",
      "Epoch: 423 | Batch: 010 / 011 | Total loss: 0.861 | Reg loss: 0.028 | Tree loss: 0.861 | Accuracy: 0.686007 | 0.508 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 424 | Batch: 000 / 011 | Total loss: 0.869 | Reg loss: 0.028 | Tree loss: 0.869 | Accuracy: 0.659000 | 0.509 sec/iter\n",
      "Epoch: 424 | Batch: 001 / 011 | Total loss: 0.855 | Reg loss: 0.028 | Tree loss: 0.855 | Accuracy: 0.671500 | 0.509 sec/iter\n",
      "Epoch: 424 | Batch: 002 / 011 | Total loss: 0.825 | Reg loss: 0.028 | Tree loss: 0.825 | Accuracy: 0.692000 | 0.509 sec/iter\n",
      "Epoch: 424 | Batch: 003 / 011 | Total loss: 0.831 | Reg loss: 0.028 | Tree loss: 0.831 | Accuracy: 0.696000 | 0.509 sec/iter\n",
      "Epoch: 424 | Batch: 004 / 011 | Total loss: 0.809 | Reg loss: 0.028 | Tree loss: 0.809 | Accuracy: 0.702500 | 0.509 sec/iter\n",
      "Epoch: 424 | Batch: 005 / 011 | Total loss: 0.781 | Reg loss: 0.028 | Tree loss: 0.781 | Accuracy: 0.721000 | 0.508 sec/iter\n",
      "Epoch: 424 | Batch: 006 / 011 | Total loss: 0.801 | Reg loss: 0.028 | Tree loss: 0.801 | Accuracy: 0.719000 | 0.508 sec/iter\n",
      "Epoch: 424 | Batch: 007 / 011 | Total loss: 0.788 | Reg loss: 0.028 | Tree loss: 0.788 | Accuracy: 0.733000 | 0.508 sec/iter\n",
      "Epoch: 424 | Batch: 008 / 011 | Total loss: 0.764 | Reg loss: 0.028 | Tree loss: 0.764 | Accuracy: 0.743500 | 0.508 sec/iter\n",
      "Epoch: 424 | Batch: 009 / 011 | Total loss: 0.792 | Reg loss: 0.028 | Tree loss: 0.792 | Accuracy: 0.736500 | 0.508 sec/iter\n",
      "Epoch: 424 | Batch: 010 / 011 | Total loss: 0.778 | Reg loss: 0.028 | Tree loss: 0.778 | Accuracy: 0.733788 | 0.508 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 425 | Batch: 000 / 011 | Total loss: 0.855 | Reg loss: 0.028 | Tree loss: 0.855 | Accuracy: 0.680500 | 0.509 sec/iter\n",
      "Epoch: 425 | Batch: 001 / 011 | Total loss: 0.854 | Reg loss: 0.028 | Tree loss: 0.854 | Accuracy: 0.666000 | 0.509 sec/iter\n",
      "Epoch: 425 | Batch: 002 / 011 | Total loss: 0.846 | Reg loss: 0.028 | Tree loss: 0.846 | Accuracy: 0.674500 | 0.509 sec/iter\n",
      "Epoch: 425 | Batch: 003 / 011 | Total loss: 0.829 | Reg loss: 0.028 | Tree loss: 0.829 | Accuracy: 0.677000 | 0.508 sec/iter\n",
      "Epoch: 425 | Batch: 004 / 011 | Total loss: 0.794 | Reg loss: 0.028 | Tree loss: 0.794 | Accuracy: 0.714500 | 0.508 sec/iter\n",
      "Epoch: 425 | Batch: 005 / 011 | Total loss: 0.800 | Reg loss: 0.028 | Tree loss: 0.800 | Accuracy: 0.709500 | 0.508 sec/iter\n",
      "Epoch: 425 | Batch: 006 / 011 | Total loss: 0.790 | Reg loss: 0.028 | Tree loss: 0.790 | Accuracy: 0.746000 | 0.508 sec/iter\n",
      "Epoch: 425 | Batch: 007 / 011 | Total loss: 0.777 | Reg loss: 0.028 | Tree loss: 0.777 | Accuracy: 0.742500 | 0.508 sec/iter\n",
      "Epoch: 425 | Batch: 008 / 011 | Total loss: 0.794 | Reg loss: 0.028 | Tree loss: 0.794 | Accuracy: 0.718000 | 0.508 sec/iter\n",
      "Epoch: 425 | Batch: 009 / 011 | Total loss: 0.779 | Reg loss: 0.028 | Tree loss: 0.779 | Accuracy: 0.734000 | 0.508 sec/iter\n",
      "Epoch: 425 | Batch: 010 / 011 | Total loss: 0.720 | Reg loss: 0.028 | Tree loss: 0.720 | Accuracy: 0.757679 | 0.508 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 426 | Batch: 000 / 011 | Total loss: 0.857 | Reg loss: 0.028 | Tree loss: 0.857 | Accuracy: 0.669500 | 0.508 sec/iter\n",
      "Epoch: 426 | Batch: 001 / 011 | Total loss: 0.851 | Reg loss: 0.028 | Tree loss: 0.851 | Accuracy: 0.678500 | 0.508 sec/iter\n",
      "Epoch: 426 | Batch: 002 / 011 | Total loss: 0.824 | Reg loss: 0.028 | Tree loss: 0.824 | Accuracy: 0.705000 | 0.508 sec/iter\n",
      "Epoch: 426 | Batch: 003 / 011 | Total loss: 0.807 | Reg loss: 0.028 | Tree loss: 0.807 | Accuracy: 0.694500 | 0.508 sec/iter\n",
      "Epoch: 426 | Batch: 004 / 011 | Total loss: 0.814 | Reg loss: 0.028 | Tree loss: 0.814 | Accuracy: 0.713000 | 0.508 sec/iter\n",
      "Epoch: 426 | Batch: 005 / 011 | Total loss: 0.778 | Reg loss: 0.028 | Tree loss: 0.778 | Accuracy: 0.730000 | 0.508 sec/iter\n",
      "Epoch: 426 | Batch: 006 / 011 | Total loss: 0.788 | Reg loss: 0.028 | Tree loss: 0.788 | Accuracy: 0.717000 | 0.508 sec/iter\n",
      "Epoch: 426 | Batch: 007 / 011 | Total loss: 0.799 | Reg loss: 0.028 | Tree loss: 0.799 | Accuracy: 0.707000 | 0.508 sec/iter\n",
      "Epoch: 426 | Batch: 008 / 011 | Total loss: 0.801 | Reg loss: 0.028 | Tree loss: 0.801 | Accuracy: 0.732000 | 0.508 sec/iter\n",
      "Epoch: 426 | Batch: 009 / 011 | Total loss: 0.795 | Reg loss: 0.028 | Tree loss: 0.795 | Accuracy: 0.733000 | 0.508 sec/iter\n",
      "Epoch: 426 | Batch: 010 / 011 | Total loss: 0.826 | Reg loss: 0.028 | Tree loss: 0.826 | Accuracy: 0.706485 | 0.508 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 427 | Batch: 000 / 011 | Total loss: 0.868 | Reg loss: 0.028 | Tree loss: 0.868 | Accuracy: 0.669000 | 0.509 sec/iter\n",
      "Epoch: 427 | Batch: 001 / 011 | Total loss: 0.862 | Reg loss: 0.028 | Tree loss: 0.862 | Accuracy: 0.670500 | 0.509 sec/iter\n",
      "Epoch: 427 | Batch: 002 / 011 | Total loss: 0.844 | Reg loss: 0.028 | Tree loss: 0.844 | Accuracy: 0.690500 | 0.508 sec/iter\n",
      "Epoch: 427 | Batch: 003 / 011 | Total loss: 0.825 | Reg loss: 0.028 | Tree loss: 0.825 | Accuracy: 0.693500 | 0.508 sec/iter\n",
      "Epoch: 427 | Batch: 004 / 011 | Total loss: 0.808 | Reg loss: 0.028 | Tree loss: 0.808 | Accuracy: 0.714000 | 0.508 sec/iter\n",
      "Epoch: 427 | Batch: 005 / 011 | Total loss: 0.788 | Reg loss: 0.028 | Tree loss: 0.788 | Accuracy: 0.710500 | 0.508 sec/iter\n",
      "Epoch: 427 | Batch: 006 / 011 | Total loss: 0.799 | Reg loss: 0.028 | Tree loss: 0.799 | Accuracy: 0.725000 | 0.508 sec/iter\n",
      "Epoch: 427 | Batch: 007 / 011 | Total loss: 0.786 | Reg loss: 0.028 | Tree loss: 0.786 | Accuracy: 0.720500 | 0.508 sec/iter\n",
      "Epoch: 427 | Batch: 008 / 011 | Total loss: 0.762 | Reg loss: 0.028 | Tree loss: 0.762 | Accuracy: 0.748500 | 0.508 sec/iter\n",
      "Epoch: 427 | Batch: 009 / 011 | Total loss: 0.771 | Reg loss: 0.028 | Tree loss: 0.771 | Accuracy: 0.736500 | 0.508 sec/iter\n",
      "Epoch: 427 | Batch: 010 / 011 | Total loss: 0.827 | Reg loss: 0.028 | Tree loss: 0.827 | Accuracy: 0.696246 | 0.508 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 6: 0.9723756906077349\n",
      "Epoch: 428 | Batch: 000 / 011 | Total loss: 0.884 | Reg loss: 0.028 | Tree loss: 0.884 | Accuracy: 0.664500 | 0.509 sec/iter\n",
      "Epoch: 428 | Batch: 001 / 011 | Total loss: 0.837 | Reg loss: 0.028 | Tree loss: 0.837 | Accuracy: 0.678500 | 0.508 sec/iter\n",
      "Epoch: 428 | Batch: 002 / 011 | Total loss: 0.852 | Reg loss: 0.028 | Tree loss: 0.852 | Accuracy: 0.676000 | 0.508 sec/iter\n",
      "Epoch: 428 | Batch: 003 / 011 | Total loss: 0.843 | Reg loss: 0.028 | Tree loss: 0.843 | Accuracy: 0.684500 | 0.508 sec/iter\n",
      "Epoch: 428 | Batch: 004 / 011 | Total loss: 0.793 | Reg loss: 0.028 | Tree loss: 0.793 | Accuracy: 0.707000 | 0.508 sec/iter\n",
      "Epoch: 428 | Batch: 005 / 011 | Total loss: 0.778 | Reg loss: 0.028 | Tree loss: 0.778 | Accuracy: 0.730000 | 0.508 sec/iter\n",
      "Epoch: 428 | Batch: 006 / 011 | Total loss: 0.785 | Reg loss: 0.028 | Tree loss: 0.785 | Accuracy: 0.723000 | 0.508 sec/iter\n",
      "Epoch: 428 | Batch: 007 / 011 | Total loss: 0.787 | Reg loss: 0.028 | Tree loss: 0.787 | Accuracy: 0.719000 | 0.508 sec/iter\n",
      "Epoch: 428 | Batch: 008 / 011 | Total loss: 0.802 | Reg loss: 0.028 | Tree loss: 0.802 | Accuracy: 0.728000 | 0.508 sec/iter\n",
      "Epoch: 428 | Batch: 009 / 011 | Total loss: 0.771 | Reg loss: 0.028 | Tree loss: 0.771 | Accuracy: 0.730500 | 0.508 sec/iter\n",
      "Epoch: 428 | Batch: 010 / 011 | Total loss: 0.783 | Reg loss: 0.028 | Tree loss: 0.783 | Accuracy: 0.726962 | 0.508 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 429 | Batch: 000 / 011 | Total loss: 0.881 | Reg loss: 0.028 | Tree loss: 0.881 | Accuracy: 0.672500 | 0.508 sec/iter\n",
      "Epoch: 429 | Batch: 001 / 011 | Total loss: 0.827 | Reg loss: 0.028 | Tree loss: 0.827 | Accuracy: 0.684500 | 0.508 sec/iter\n",
      "Epoch: 429 | Batch: 002 / 011 | Total loss: 0.839 | Reg loss: 0.028 | Tree loss: 0.839 | Accuracy: 0.681500 | 0.508 sec/iter\n",
      "Epoch: 429 | Batch: 003 / 011 | Total loss: 0.818 | Reg loss: 0.028 | Tree loss: 0.818 | Accuracy: 0.695500 | 0.508 sec/iter\n",
      "Epoch: 429 | Batch: 004 / 011 | Total loss: 0.807 | Reg loss: 0.028 | Tree loss: 0.807 | Accuracy: 0.706000 | 0.508 sec/iter\n",
      "Epoch: 429 | Batch: 005 / 011 | Total loss: 0.791 | Reg loss: 0.028 | Tree loss: 0.791 | Accuracy: 0.713500 | 0.508 sec/iter\n",
      "Epoch: 429 | Batch: 006 / 011 | Total loss: 0.764 | Reg loss: 0.028 | Tree loss: 0.764 | Accuracy: 0.748000 | 0.508 sec/iter\n",
      "Epoch: 429 | Batch: 007 / 011 | Total loss: 0.807 | Reg loss: 0.028 | Tree loss: 0.807 | Accuracy: 0.718000 | 0.508 sec/iter\n",
      "Epoch: 429 | Batch: 008 / 011 | Total loss: 0.800 | Reg loss: 0.028 | Tree loss: 0.800 | Accuracy: 0.719500 | 0.508 sec/iter\n",
      "Epoch: 429 | Batch: 009 / 011 | Total loss: 0.782 | Reg loss: 0.028 | Tree loss: 0.782 | Accuracy: 0.742500 | 0.507 sec/iter\n",
      "Epoch: 429 | Batch: 010 / 011 | Total loss: 0.787 | Reg loss: 0.028 | Tree loss: 0.787 | Accuracy: 0.713311 | 0.507 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 430 | Batch: 000 / 011 | Total loss: 0.874 | Reg loss: 0.028 | Tree loss: 0.874 | Accuracy: 0.657000 | 0.508 sec/iter\n",
      "Epoch: 430 | Batch: 001 / 011 | Total loss: 0.864 | Reg loss: 0.028 | Tree loss: 0.864 | Accuracy: 0.673000 | 0.508 sec/iter\n",
      "Epoch: 430 | Batch: 002 / 011 | Total loss: 0.830 | Reg loss: 0.028 | Tree loss: 0.830 | Accuracy: 0.700000 | 0.508 sec/iter\n",
      "Epoch: 430 | Batch: 003 / 011 | Total loss: 0.813 | Reg loss: 0.028 | Tree loss: 0.813 | Accuracy: 0.704500 | 0.508 sec/iter\n",
      "Epoch: 430 | Batch: 004 / 011 | Total loss: 0.802 | Reg loss: 0.028 | Tree loss: 0.802 | Accuracy: 0.705000 | 0.508 sec/iter\n",
      "Epoch: 430 | Batch: 005 / 011 | Total loss: 0.789 | Reg loss: 0.028 | Tree loss: 0.789 | Accuracy: 0.714500 | 0.508 sec/iter\n",
      "Epoch: 430 | Batch: 006 / 011 | Total loss: 0.783 | Reg loss: 0.028 | Tree loss: 0.783 | Accuracy: 0.720000 | 0.508 sec/iter\n",
      "Epoch: 430 | Batch: 007 / 011 | Total loss: 0.801 | Reg loss: 0.028 | Tree loss: 0.801 | Accuracy: 0.718500 | 0.507 sec/iter\n",
      "Epoch: 430 | Batch: 008 / 011 | Total loss: 0.753 | Reg loss: 0.028 | Tree loss: 0.753 | Accuracy: 0.749500 | 0.507 sec/iter\n",
      "Epoch: 430 | Batch: 009 / 011 | Total loss: 0.787 | Reg loss: 0.028 | Tree loss: 0.787 | Accuracy: 0.721500 | 0.507 sec/iter\n",
      "Epoch: 430 | Batch: 010 / 011 | Total loss: 0.777 | Reg loss: 0.028 | Tree loss: 0.777 | Accuracy: 0.747440 | 0.507 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 431 | Batch: 000 / 011 | Total loss: 0.872 | Reg loss: 0.028 | Tree loss: 0.872 | Accuracy: 0.667000 | 0.508 sec/iter\n",
      "Epoch: 431 | Batch: 001 / 011 | Total loss: 0.865 | Reg loss: 0.028 | Tree loss: 0.865 | Accuracy: 0.667500 | 0.508 sec/iter\n",
      "Epoch: 431 | Batch: 002 / 011 | Total loss: 0.823 | Reg loss: 0.028 | Tree loss: 0.823 | Accuracy: 0.720000 | 0.508 sec/iter\n",
      "Epoch: 431 | Batch: 003 / 011 | Total loss: 0.828 | Reg loss: 0.028 | Tree loss: 0.828 | Accuracy: 0.692500 | 0.508 sec/iter\n",
      "Epoch: 431 | Batch: 004 / 011 | Total loss: 0.811 | Reg loss: 0.028 | Tree loss: 0.811 | Accuracy: 0.702000 | 0.508 sec/iter\n",
      "Epoch: 431 | Batch: 005 / 011 | Total loss: 0.770 | Reg loss: 0.028 | Tree loss: 0.770 | Accuracy: 0.729000 | 0.507 sec/iter\n",
      "Epoch: 431 | Batch: 006 / 011 | Total loss: 0.788 | Reg loss: 0.028 | Tree loss: 0.788 | Accuracy: 0.724500 | 0.507 sec/iter\n",
      "Epoch: 431 | Batch: 007 / 011 | Total loss: 0.782 | Reg loss: 0.028 | Tree loss: 0.782 | Accuracy: 0.732500 | 0.507 sec/iter\n",
      "Epoch: 431 | Batch: 008 / 011 | Total loss: 0.781 | Reg loss: 0.028 | Tree loss: 0.781 | Accuracy: 0.737500 | 0.507 sec/iter\n",
      "Epoch: 431 | Batch: 009 / 011 | Total loss: 0.794 | Reg loss: 0.028 | Tree loss: 0.794 | Accuracy: 0.724500 | 0.507 sec/iter\n",
      "Epoch: 431 | Batch: 010 / 011 | Total loss: 0.819 | Reg loss: 0.028 | Tree loss: 0.819 | Accuracy: 0.716724 | 0.507 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 432 | Batch: 000 / 011 | Total loss: 0.888 | Reg loss: 0.028 | Tree loss: 0.888 | Accuracy: 0.655500 | 0.507 sec/iter\n",
      "Epoch: 432 | Batch: 001 / 011 | Total loss: 0.866 | Reg loss: 0.028 | Tree loss: 0.866 | Accuracy: 0.668000 | 0.507 sec/iter\n",
      "Epoch: 432 | Batch: 002 / 011 | Total loss: 0.830 | Reg loss: 0.028 | Tree loss: 0.830 | Accuracy: 0.693000 | 0.507 sec/iter\n",
      "Epoch: 432 | Batch: 003 / 011 | Total loss: 0.807 | Reg loss: 0.028 | Tree loss: 0.807 | Accuracy: 0.703000 | 0.507 sec/iter\n",
      "Epoch: 432 | Batch: 004 / 011 | Total loss: 0.793 | Reg loss: 0.028 | Tree loss: 0.793 | Accuracy: 0.716000 | 0.507 sec/iter\n",
      "Epoch: 432 | Batch: 005 / 011 | Total loss: 0.783 | Reg loss: 0.028 | Tree loss: 0.783 | Accuracy: 0.717500 | 0.507 sec/iter\n",
      "Epoch: 432 | Batch: 006 / 011 | Total loss: 0.805 | Reg loss: 0.028 | Tree loss: 0.805 | Accuracy: 0.723500 | 0.507 sec/iter\n",
      "Epoch: 432 | Batch: 007 / 011 | Total loss: 0.794 | Reg loss: 0.028 | Tree loss: 0.794 | Accuracy: 0.732000 | 0.507 sec/iter\n",
      "Epoch: 432 | Batch: 008 / 011 | Total loss: 0.772 | Reg loss: 0.028 | Tree loss: 0.772 | Accuracy: 0.736500 | 0.507 sec/iter\n",
      "Epoch: 432 | Batch: 009 / 011 | Total loss: 0.789 | Reg loss: 0.028 | Tree loss: 0.789 | Accuracy: 0.724500 | 0.507 sec/iter\n",
      "Epoch: 432 | Batch: 010 / 011 | Total loss: 0.759 | Reg loss: 0.028 | Tree loss: 0.759 | Accuracy: 0.747440 | 0.507 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 433 | Batch: 000 / 011 | Total loss: 0.894 | Reg loss: 0.028 | Tree loss: 0.894 | Accuracy: 0.658500 | 0.508 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 433 | Batch: 001 / 011 | Total loss: 0.852 | Reg loss: 0.028 | Tree loss: 0.852 | Accuracy: 0.675000 | 0.508 sec/iter\n",
      "Epoch: 433 | Batch: 002 / 011 | Total loss: 0.821 | Reg loss: 0.028 | Tree loss: 0.821 | Accuracy: 0.707000 | 0.508 sec/iter\n",
      "Epoch: 433 | Batch: 003 / 011 | Total loss: 0.823 | Reg loss: 0.028 | Tree loss: 0.823 | Accuracy: 0.699000 | 0.508 sec/iter\n",
      "Epoch: 433 | Batch: 004 / 011 | Total loss: 0.786 | Reg loss: 0.028 | Tree loss: 0.786 | Accuracy: 0.726000 | 0.508 sec/iter\n",
      "Epoch: 433 | Batch: 005 / 011 | Total loss: 0.783 | Reg loss: 0.028 | Tree loss: 0.783 | Accuracy: 0.728000 | 0.508 sec/iter\n",
      "Epoch: 433 | Batch: 006 / 011 | Total loss: 0.784 | Reg loss: 0.028 | Tree loss: 0.784 | Accuracy: 0.735000 | 0.507 sec/iter\n",
      "Epoch: 433 | Batch: 007 / 011 | Total loss: 0.778 | Reg loss: 0.028 | Tree loss: 0.778 | Accuracy: 0.734500 | 0.507 sec/iter\n",
      "Epoch: 433 | Batch: 008 / 011 | Total loss: 0.782 | Reg loss: 0.028 | Tree loss: 0.782 | Accuracy: 0.733500 | 0.507 sec/iter\n",
      "Epoch: 433 | Batch: 009 / 011 | Total loss: 0.808 | Reg loss: 0.028 | Tree loss: 0.808 | Accuracy: 0.716500 | 0.507 sec/iter\n",
      "Epoch: 433 | Batch: 010 / 011 | Total loss: 0.779 | Reg loss: 0.028 | Tree loss: 0.779 | Accuracy: 0.733788 | 0.507 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 434 | Batch: 000 / 011 | Total loss: 0.871 | Reg loss: 0.028 | Tree loss: 0.871 | Accuracy: 0.664500 | 0.508 sec/iter\n",
      "Epoch: 434 | Batch: 001 / 011 | Total loss: 0.840 | Reg loss: 0.028 | Tree loss: 0.840 | Accuracy: 0.676500 | 0.508 sec/iter\n",
      "Epoch: 434 | Batch: 002 / 011 | Total loss: 0.856 | Reg loss: 0.028 | Tree loss: 0.856 | Accuracy: 0.679500 | 0.508 sec/iter\n",
      "Epoch: 434 | Batch: 003 / 011 | Total loss: 0.821 | Reg loss: 0.028 | Tree loss: 0.821 | Accuracy: 0.684500 | 0.508 sec/iter\n",
      "Epoch: 434 | Batch: 004 / 011 | Total loss: 0.790 | Reg loss: 0.028 | Tree loss: 0.790 | Accuracy: 0.733500 | 0.508 sec/iter\n",
      "Epoch: 434 | Batch: 005 / 011 | Total loss: 0.811 | Reg loss: 0.028 | Tree loss: 0.811 | Accuracy: 0.702000 | 0.508 sec/iter\n",
      "Epoch: 434 | Batch: 006 / 011 | Total loss: 0.777 | Reg loss: 0.028 | Tree loss: 0.777 | Accuracy: 0.732000 | 0.508 sec/iter\n",
      "Epoch: 434 | Batch: 007 / 011 | Total loss: 0.777 | Reg loss: 0.028 | Tree loss: 0.777 | Accuracy: 0.729500 | 0.508 sec/iter\n",
      "Epoch: 434 | Batch: 008 / 011 | Total loss: 0.799 | Reg loss: 0.028 | Tree loss: 0.799 | Accuracy: 0.735000 | 0.508 sec/iter\n",
      "Epoch: 434 | Batch: 009 / 011 | Total loss: 0.780 | Reg loss: 0.028 | Tree loss: 0.780 | Accuracy: 0.737000 | 0.508 sec/iter\n",
      "Epoch: 434 | Batch: 010 / 011 | Total loss: 0.796 | Reg loss: 0.028 | Tree loss: 0.796 | Accuracy: 0.744027 | 0.508 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 435 | Batch: 000 / 011 | Total loss: 0.876 | Reg loss: 0.028 | Tree loss: 0.876 | Accuracy: 0.658000 | 0.508 sec/iter\n",
      "Epoch: 435 | Batch: 001 / 011 | Total loss: 0.861 | Reg loss: 0.028 | Tree loss: 0.861 | Accuracy: 0.678500 | 0.508 sec/iter\n",
      "Epoch: 435 | Batch: 002 / 011 | Total loss: 0.830 | Reg loss: 0.028 | Tree loss: 0.830 | Accuracy: 0.674000 | 0.508 sec/iter\n",
      "Epoch: 435 | Batch: 003 / 011 | Total loss: 0.823 | Reg loss: 0.028 | Tree loss: 0.823 | Accuracy: 0.701000 | 0.508 sec/iter\n",
      "Epoch: 435 | Batch: 004 / 011 | Total loss: 0.799 | Reg loss: 0.028 | Tree loss: 0.799 | Accuracy: 0.734500 | 0.507 sec/iter\n",
      "Epoch: 435 | Batch: 005 / 011 | Total loss: 0.801 | Reg loss: 0.028 | Tree loss: 0.801 | Accuracy: 0.708500 | 0.507 sec/iter\n",
      "Epoch: 435 | Batch: 006 / 011 | Total loss: 0.794 | Reg loss: 0.028 | Tree loss: 0.794 | Accuracy: 0.734000 | 0.507 sec/iter\n",
      "Epoch: 435 | Batch: 007 / 011 | Total loss: 0.759 | Reg loss: 0.028 | Tree loss: 0.759 | Accuracy: 0.749500 | 0.507 sec/iter\n",
      "Epoch: 435 | Batch: 008 / 011 | Total loss: 0.793 | Reg loss: 0.028 | Tree loss: 0.793 | Accuracy: 0.719000 | 0.507 sec/iter\n",
      "Epoch: 435 | Batch: 009 / 011 | Total loss: 0.771 | Reg loss: 0.028 | Tree loss: 0.771 | Accuracy: 0.741500 | 0.507 sec/iter\n",
      "Epoch: 435 | Batch: 010 / 011 | Total loss: 0.844 | Reg loss: 0.028 | Tree loss: 0.844 | Accuracy: 0.716724 | 0.507 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 436 | Batch: 000 / 011 | Total loss: 0.869 | Reg loss: 0.028 | Tree loss: 0.869 | Accuracy: 0.682500 | 0.508 sec/iter\n",
      "Epoch: 436 | Batch: 001 / 011 | Total loss: 0.854 | Reg loss: 0.028 | Tree loss: 0.854 | Accuracy: 0.683500 | 0.508 sec/iter\n",
      "Epoch: 436 | Batch: 002 / 011 | Total loss: 0.838 | Reg loss: 0.028 | Tree loss: 0.838 | Accuracy: 0.679000 | 0.508 sec/iter\n",
      "Epoch: 436 | Batch: 003 / 011 | Total loss: 0.810 | Reg loss: 0.028 | Tree loss: 0.810 | Accuracy: 0.691000 | 0.508 sec/iter\n",
      "Epoch: 436 | Batch: 004 / 011 | Total loss: 0.806 | Reg loss: 0.028 | Tree loss: 0.806 | Accuracy: 0.700500 | 0.508 sec/iter\n",
      "Epoch: 436 | Batch: 005 / 011 | Total loss: 0.807 | Reg loss: 0.028 | Tree loss: 0.807 | Accuracy: 0.708000 | 0.508 sec/iter\n",
      "Epoch: 436 | Batch: 006 / 011 | Total loss: 0.790 | Reg loss: 0.028 | Tree loss: 0.790 | Accuracy: 0.729500 | 0.508 sec/iter\n",
      "Epoch: 436 | Batch: 007 / 011 | Total loss: 0.792 | Reg loss: 0.028 | Tree loss: 0.792 | Accuracy: 0.712500 | 0.508 sec/iter\n",
      "Epoch: 436 | Batch: 008 / 011 | Total loss: 0.761 | Reg loss: 0.028 | Tree loss: 0.761 | Accuracy: 0.744500 | 0.508 sec/iter\n",
      "Epoch: 436 | Batch: 009 / 011 | Total loss: 0.770 | Reg loss: 0.028 | Tree loss: 0.770 | Accuracy: 0.745000 | 0.508 sec/iter\n",
      "Epoch: 436 | Batch: 010 / 011 | Total loss: 0.796 | Reg loss: 0.028 | Tree loss: 0.796 | Accuracy: 0.747440 | 0.508 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 437 | Batch: 000 / 011 | Total loss: 0.868 | Reg loss: 0.028 | Tree loss: 0.868 | Accuracy: 0.651000 | 0.508 sec/iter\n",
      "Epoch: 437 | Batch: 001 / 011 | Total loss: 0.850 | Reg loss: 0.028 | Tree loss: 0.850 | Accuracy: 0.680000 | 0.508 sec/iter\n",
      "Epoch: 437 | Batch: 002 / 011 | Total loss: 0.852 | Reg loss: 0.028 | Tree loss: 0.852 | Accuracy: 0.692000 | 0.508 sec/iter\n",
      "Epoch: 437 | Batch: 003 / 011 | Total loss: 0.824 | Reg loss: 0.028 | Tree loss: 0.824 | Accuracy: 0.698000 | 0.507 sec/iter\n",
      "Epoch: 437 | Batch: 004 / 011 | Total loss: 0.789 | Reg loss: 0.028 | Tree loss: 0.789 | Accuracy: 0.723000 | 0.507 sec/iter\n",
      "Epoch: 437 | Batch: 005 / 011 | Total loss: 0.788 | Reg loss: 0.028 | Tree loss: 0.788 | Accuracy: 0.719500 | 0.507 sec/iter\n",
      "Epoch: 437 | Batch: 006 / 011 | Total loss: 0.785 | Reg loss: 0.028 | Tree loss: 0.785 | Accuracy: 0.729500 | 0.507 sec/iter\n",
      "Epoch: 437 | Batch: 007 / 011 | Total loss: 0.782 | Reg loss: 0.028 | Tree loss: 0.782 | Accuracy: 0.726000 | 0.507 sec/iter\n",
      "Epoch: 437 | Batch: 008 / 011 | Total loss: 0.780 | Reg loss: 0.028 | Tree loss: 0.780 | Accuracy: 0.730500 | 0.507 sec/iter\n",
      "Epoch: 437 | Batch: 009 / 011 | Total loss: 0.783 | Reg loss: 0.028 | Tree loss: 0.783 | Accuracy: 0.742000 | 0.507 sec/iter\n",
      "Epoch: 437 | Batch: 010 / 011 | Total loss: 0.808 | Reg loss: 0.028 | Tree loss: 0.808 | Accuracy: 0.744027 | 0.507 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 438 | Batch: 000 / 011 | Total loss: 0.868 | Reg loss: 0.028 | Tree loss: 0.868 | Accuracy: 0.669000 | 0.508 sec/iter\n",
      "Epoch: 438 | Batch: 001 / 011 | Total loss: 0.872 | Reg loss: 0.028 | Tree loss: 0.872 | Accuracy: 0.669500 | 0.508 sec/iter\n",
      "Epoch: 438 | Batch: 002 / 011 | Total loss: 0.853 | Reg loss: 0.028 | Tree loss: 0.853 | Accuracy: 0.675000 | 0.508 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 438 | Batch: 003 / 011 | Total loss: 0.824 | Reg loss: 0.028 | Tree loss: 0.824 | Accuracy: 0.694000 | 0.508 sec/iter\n",
      "Epoch: 438 | Batch: 004 / 011 | Total loss: 0.801 | Reg loss: 0.028 | Tree loss: 0.801 | Accuracy: 0.712500 | 0.508 sec/iter\n",
      "Epoch: 438 | Batch: 005 / 011 | Total loss: 0.782 | Reg loss: 0.028 | Tree loss: 0.782 | Accuracy: 0.723500 | 0.508 sec/iter\n",
      "Epoch: 438 | Batch: 006 / 011 | Total loss: 0.771 | Reg loss: 0.028 | Tree loss: 0.771 | Accuracy: 0.735500 | 0.508 sec/iter\n",
      "Epoch: 438 | Batch: 007 / 011 | Total loss: 0.790 | Reg loss: 0.028 | Tree loss: 0.790 | Accuracy: 0.733500 | 0.508 sec/iter\n",
      "Epoch: 438 | Batch: 008 / 011 | Total loss: 0.766 | Reg loss: 0.028 | Tree loss: 0.766 | Accuracy: 0.742000 | 0.508 sec/iter\n",
      "Epoch: 438 | Batch: 009 / 011 | Total loss: 0.769 | Reg loss: 0.028 | Tree loss: 0.769 | Accuracy: 0.739500 | 0.508 sec/iter\n",
      "Epoch: 438 | Batch: 010 / 011 | Total loss: 0.813 | Reg loss: 0.028 | Tree loss: 0.813 | Accuracy: 0.703072 | 0.507 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 439 | Batch: 000 / 011 | Total loss: 0.892 | Reg loss: 0.028 | Tree loss: 0.892 | Accuracy: 0.659500 | 0.508 sec/iter\n",
      "Epoch: 439 | Batch: 001 / 011 | Total loss: 0.849 | Reg loss: 0.028 | Tree loss: 0.849 | Accuracy: 0.683500 | 0.508 sec/iter\n",
      "Epoch: 439 | Batch: 002 / 011 | Total loss: 0.828 | Reg loss: 0.028 | Tree loss: 0.828 | Accuracy: 0.687000 | 0.508 sec/iter\n",
      "Epoch: 439 | Batch: 003 / 011 | Total loss: 0.818 | Reg loss: 0.028 | Tree loss: 0.818 | Accuracy: 0.696000 | 0.508 sec/iter\n",
      "Epoch: 439 | Batch: 004 / 011 | Total loss: 0.784 | Reg loss: 0.028 | Tree loss: 0.784 | Accuracy: 0.718500 | 0.508 sec/iter\n",
      "Epoch: 439 | Batch: 005 / 011 | Total loss: 0.776 | Reg loss: 0.028 | Tree loss: 0.776 | Accuracy: 0.731500 | 0.508 sec/iter\n",
      "Epoch: 439 | Batch: 006 / 011 | Total loss: 0.794 | Reg loss: 0.028 | Tree loss: 0.794 | Accuracy: 0.713000 | 0.508 sec/iter\n",
      "Epoch: 439 | Batch: 007 / 011 | Total loss: 0.804 | Reg loss: 0.028 | Tree loss: 0.804 | Accuracy: 0.715000 | 0.508 sec/iter\n",
      "Epoch: 439 | Batch: 008 / 011 | Total loss: 0.794 | Reg loss: 0.028 | Tree loss: 0.794 | Accuracy: 0.720500 | 0.508 sec/iter\n",
      "Epoch: 439 | Batch: 009 / 011 | Total loss: 0.785 | Reg loss: 0.028 | Tree loss: 0.785 | Accuracy: 0.733000 | 0.508 sec/iter\n",
      "Epoch: 439 | Batch: 010 / 011 | Total loss: 0.785 | Reg loss: 0.028 | Tree loss: 0.785 | Accuracy: 0.740614 | 0.508 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 440 | Batch: 000 / 011 | Total loss: 0.858 | Reg loss: 0.028 | Tree loss: 0.858 | Accuracy: 0.693500 | 0.508 sec/iter\n",
      "Epoch: 440 | Batch: 001 / 011 | Total loss: 0.855 | Reg loss: 0.028 | Tree loss: 0.855 | Accuracy: 0.666500 | 0.508 sec/iter\n",
      "Epoch: 440 | Batch: 002 / 011 | Total loss: 0.821 | Reg loss: 0.028 | Tree loss: 0.821 | Accuracy: 0.705500 | 0.508 sec/iter\n",
      "Epoch: 440 | Batch: 003 / 011 | Total loss: 0.802 | Reg loss: 0.028 | Tree loss: 0.802 | Accuracy: 0.700500 | 0.508 sec/iter\n",
      "Epoch: 440 | Batch: 004 / 011 | Total loss: 0.815 | Reg loss: 0.028 | Tree loss: 0.815 | Accuracy: 0.706000 | 0.508 sec/iter\n",
      "Epoch: 440 | Batch: 005 / 011 | Total loss: 0.795 | Reg loss: 0.028 | Tree loss: 0.795 | Accuracy: 0.705500 | 0.508 sec/iter\n",
      "Epoch: 440 | Batch: 006 / 011 | Total loss: 0.789 | Reg loss: 0.028 | Tree loss: 0.789 | Accuracy: 0.729000 | 0.508 sec/iter\n",
      "Epoch: 440 | Batch: 007 / 011 | Total loss: 0.784 | Reg loss: 0.028 | Tree loss: 0.784 | Accuracy: 0.726500 | 0.508 sec/iter\n",
      "Epoch: 440 | Batch: 008 / 011 | Total loss: 0.804 | Reg loss: 0.028 | Tree loss: 0.804 | Accuracy: 0.720500 | 0.508 sec/iter\n",
      "Epoch: 440 | Batch: 009 / 011 | Total loss: 0.765 | Reg loss: 0.028 | Tree loss: 0.765 | Accuracy: 0.741500 | 0.508 sec/iter\n",
      "Epoch: 440 | Batch: 010 / 011 | Total loss: 0.819 | Reg loss: 0.028 | Tree loss: 0.819 | Accuracy: 0.696246 | 0.507 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 441 | Batch: 000 / 011 | Total loss: 0.878 | Reg loss: 0.028 | Tree loss: 0.878 | Accuracy: 0.667500 | 0.508 sec/iter\n",
      "Epoch: 441 | Batch: 001 / 011 | Total loss: 0.863 | Reg loss: 0.028 | Tree loss: 0.863 | Accuracy: 0.671500 | 0.508 sec/iter\n",
      "Epoch: 441 | Batch: 002 / 011 | Total loss: 0.820 | Reg loss: 0.028 | Tree loss: 0.820 | Accuracy: 0.699000 | 0.508 sec/iter\n",
      "Epoch: 441 | Batch: 003 / 011 | Total loss: 0.833 | Reg loss: 0.028 | Tree loss: 0.833 | Accuracy: 0.688500 | 0.508 sec/iter\n",
      "Epoch: 441 | Batch: 004 / 011 | Total loss: 0.798 | Reg loss: 0.028 | Tree loss: 0.798 | Accuracy: 0.719000 | 0.508 sec/iter\n",
      "Epoch: 441 | Batch: 005 / 011 | Total loss: 0.782 | Reg loss: 0.028 | Tree loss: 0.782 | Accuracy: 0.721500 | 0.508 sec/iter\n",
      "Epoch: 441 | Batch: 006 / 011 | Total loss: 0.790 | Reg loss: 0.028 | Tree loss: 0.790 | Accuracy: 0.724000 | 0.508 sec/iter\n",
      "Epoch: 441 | Batch: 007 / 011 | Total loss: 0.774 | Reg loss: 0.028 | Tree loss: 0.774 | Accuracy: 0.736000 | 0.508 sec/iter\n",
      "Epoch: 441 | Batch: 008 / 011 | Total loss: 0.790 | Reg loss: 0.028 | Tree loss: 0.790 | Accuracy: 0.720500 | 0.508 sec/iter\n",
      "Epoch: 441 | Batch: 009 / 011 | Total loss: 0.778 | Reg loss: 0.028 | Tree loss: 0.778 | Accuracy: 0.740000 | 0.508 sec/iter\n",
      "Epoch: 441 | Batch: 010 / 011 | Total loss: 0.813 | Reg loss: 0.028 | Tree loss: 0.813 | Accuracy: 0.703072 | 0.508 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 442 | Batch: 000 / 011 | Total loss: 0.868 | Reg loss: 0.028 | Tree loss: 0.868 | Accuracy: 0.669500 | 0.508 sec/iter\n",
      "Epoch: 442 | Batch: 001 / 011 | Total loss: 0.851 | Reg loss: 0.028 | Tree loss: 0.851 | Accuracy: 0.669000 | 0.508 sec/iter\n",
      "Epoch: 442 | Batch: 002 / 011 | Total loss: 0.867 | Reg loss: 0.028 | Tree loss: 0.867 | Accuracy: 0.665500 | 0.508 sec/iter\n",
      "Epoch: 442 | Batch: 003 / 011 | Total loss: 0.817 | Reg loss: 0.028 | Tree loss: 0.817 | Accuracy: 0.711500 | 0.508 sec/iter\n",
      "Epoch: 442 | Batch: 004 / 011 | Total loss: 0.786 | Reg loss: 0.028 | Tree loss: 0.786 | Accuracy: 0.725000 | 0.508 sec/iter\n",
      "Epoch: 442 | Batch: 005 / 011 | Total loss: 0.803 | Reg loss: 0.028 | Tree loss: 0.803 | Accuracy: 0.711500 | 0.508 sec/iter\n",
      "Epoch: 442 | Batch: 006 / 011 | Total loss: 0.791 | Reg loss: 0.028 | Tree loss: 0.791 | Accuracy: 0.735500 | 0.508 sec/iter\n",
      "Epoch: 442 | Batch: 007 / 011 | Total loss: 0.780 | Reg loss: 0.028 | Tree loss: 0.780 | Accuracy: 0.741000 | 0.508 sec/iter\n",
      "Epoch: 442 | Batch: 008 / 011 | Total loss: 0.780 | Reg loss: 0.028 | Tree loss: 0.780 | Accuracy: 0.728500 | 0.508 sec/iter\n",
      "Epoch: 442 | Batch: 009 / 011 | Total loss: 0.781 | Reg loss: 0.028 | Tree loss: 0.781 | Accuracy: 0.742500 | 0.508 sec/iter\n",
      "Epoch: 442 | Batch: 010 / 011 | Total loss: 0.769 | Reg loss: 0.028 | Tree loss: 0.769 | Accuracy: 0.737201 | 0.508 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 443 | Batch: 000 / 011 | Total loss: 0.865 | Reg loss: 0.028 | Tree loss: 0.865 | Accuracy: 0.676500 | 0.508 sec/iter\n",
      "Epoch: 443 | Batch: 001 / 011 | Total loss: 0.845 | Reg loss: 0.028 | Tree loss: 0.845 | Accuracy: 0.663500 | 0.508 sec/iter\n",
      "Epoch: 443 | Batch: 002 / 011 | Total loss: 0.847 | Reg loss: 0.028 | Tree loss: 0.847 | Accuracy: 0.681000 | 0.508 sec/iter\n",
      "Epoch: 443 | Batch: 003 / 011 | Total loss: 0.822 | Reg loss: 0.028 | Tree loss: 0.822 | Accuracy: 0.703000 | 0.508 sec/iter\n",
      "Epoch: 443 | Batch: 004 / 011 | Total loss: 0.827 | Reg loss: 0.028 | Tree loss: 0.827 | Accuracy: 0.692000 | 0.508 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 443 | Batch: 005 / 011 | Total loss: 0.782 | Reg loss: 0.028 | Tree loss: 0.782 | Accuracy: 0.707000 | 0.508 sec/iter\n",
      "Epoch: 443 | Batch: 006 / 011 | Total loss: 0.799 | Reg loss: 0.028 | Tree loss: 0.799 | Accuracy: 0.731000 | 0.507 sec/iter\n",
      "Epoch: 443 | Batch: 007 / 011 | Total loss: 0.772 | Reg loss: 0.028 | Tree loss: 0.772 | Accuracy: 0.747500 | 0.507 sec/iter\n",
      "Epoch: 443 | Batch: 008 / 011 | Total loss: 0.781 | Reg loss: 0.028 | Tree loss: 0.781 | Accuracy: 0.735000 | 0.507 sec/iter\n",
      "Epoch: 443 | Batch: 009 / 011 | Total loss: 0.771 | Reg loss: 0.028 | Tree loss: 0.771 | Accuracy: 0.738000 | 0.507 sec/iter\n",
      "Epoch: 443 | Batch: 010 / 011 | Total loss: 0.788 | Reg loss: 0.028 | Tree loss: 0.788 | Accuracy: 0.709898 | 0.507 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 444 | Batch: 000 / 011 | Total loss: 0.872 | Reg loss: 0.028 | Tree loss: 0.872 | Accuracy: 0.658000 | 0.508 sec/iter\n",
      "Epoch: 444 | Batch: 001 / 011 | Total loss: 0.852 | Reg loss: 0.028 | Tree loss: 0.852 | Accuracy: 0.683000 | 0.508 sec/iter\n",
      "Epoch: 444 | Batch: 002 / 011 | Total loss: 0.849 | Reg loss: 0.028 | Tree loss: 0.849 | Accuracy: 0.678000 | 0.508 sec/iter\n",
      "Epoch: 444 | Batch: 003 / 011 | Total loss: 0.797 | Reg loss: 0.028 | Tree loss: 0.797 | Accuracy: 0.708500 | 0.508 sec/iter\n",
      "Epoch: 444 | Batch: 004 / 011 | Total loss: 0.792 | Reg loss: 0.028 | Tree loss: 0.792 | Accuracy: 0.719000 | 0.508 sec/iter\n",
      "Epoch: 444 | Batch: 005 / 011 | Total loss: 0.804 | Reg loss: 0.028 | Tree loss: 0.804 | Accuracy: 0.707000 | 0.508 sec/iter\n",
      "Epoch: 444 | Batch: 006 / 011 | Total loss: 0.777 | Reg loss: 0.028 | Tree loss: 0.777 | Accuracy: 0.754000 | 0.507 sec/iter\n",
      "Epoch: 444 | Batch: 007 / 011 | Total loss: 0.788 | Reg loss: 0.028 | Tree loss: 0.788 | Accuracy: 0.713500 | 0.507 sec/iter\n",
      "Epoch: 444 | Batch: 008 / 011 | Total loss: 0.768 | Reg loss: 0.028 | Tree loss: 0.768 | Accuracy: 0.748000 | 0.507 sec/iter\n",
      "Epoch: 444 | Batch: 009 / 011 | Total loss: 0.805 | Reg loss: 0.028 | Tree loss: 0.805 | Accuracy: 0.712000 | 0.507 sec/iter\n",
      "Epoch: 444 | Batch: 010 / 011 | Total loss: 0.812 | Reg loss: 0.028 | Tree loss: 0.812 | Accuracy: 0.713311 | 0.507 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 445 | Batch: 000 / 011 | Total loss: 0.852 | Reg loss: 0.028 | Tree loss: 0.852 | Accuracy: 0.681000 | 0.508 sec/iter\n",
      "Epoch: 445 | Batch: 001 / 011 | Total loss: 0.869 | Reg loss: 0.028 | Tree loss: 0.869 | Accuracy: 0.670500 | 0.508 sec/iter\n",
      "Epoch: 445 | Batch: 002 / 011 | Total loss: 0.829 | Reg loss: 0.028 | Tree loss: 0.829 | Accuracy: 0.696500 | 0.508 sec/iter\n",
      "Epoch: 445 | Batch: 003 / 011 | Total loss: 0.807 | Reg loss: 0.028 | Tree loss: 0.807 | Accuracy: 0.703000 | 0.508 sec/iter\n",
      "Epoch: 445 | Batch: 004 / 011 | Total loss: 0.802 | Reg loss: 0.028 | Tree loss: 0.802 | Accuracy: 0.715000 | 0.507 sec/iter\n",
      "Epoch: 445 | Batch: 005 / 011 | Total loss: 0.801 | Reg loss: 0.028 | Tree loss: 0.801 | Accuracy: 0.711000 | 0.507 sec/iter\n",
      "Epoch: 445 | Batch: 006 / 011 | Total loss: 0.784 | Reg loss: 0.028 | Tree loss: 0.784 | Accuracy: 0.712000 | 0.507 sec/iter\n",
      "Epoch: 445 | Batch: 007 / 011 | Total loss: 0.781 | Reg loss: 0.028 | Tree loss: 0.781 | Accuracy: 0.728500 | 0.507 sec/iter\n",
      "Epoch: 445 | Batch: 008 / 011 | Total loss: 0.766 | Reg loss: 0.028 | Tree loss: 0.766 | Accuracy: 0.738500 | 0.507 sec/iter\n",
      "Epoch: 445 | Batch: 009 / 011 | Total loss: 0.798 | Reg loss: 0.028 | Tree loss: 0.798 | Accuracy: 0.713500 | 0.507 sec/iter\n",
      "Epoch: 445 | Batch: 010 / 011 | Total loss: 0.805 | Reg loss: 0.028 | Tree loss: 0.805 | Accuracy: 0.733788 | 0.507 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 446 | Batch: 000 / 011 | Total loss: 0.861 | Reg loss: 0.028 | Tree loss: 0.861 | Accuracy: 0.681000 | 0.507 sec/iter\n",
      "Epoch: 446 | Batch: 001 / 011 | Total loss: 0.846 | Reg loss: 0.028 | Tree loss: 0.846 | Accuracy: 0.683000 | 0.507 sec/iter\n",
      "Epoch: 446 | Batch: 002 / 011 | Total loss: 0.827 | Reg loss: 0.028 | Tree loss: 0.827 | Accuracy: 0.703500 | 0.507 sec/iter\n",
      "Epoch: 446 | Batch: 003 / 011 | Total loss: 0.804 | Reg loss: 0.028 | Tree loss: 0.804 | Accuracy: 0.703500 | 0.507 sec/iter\n",
      "Epoch: 446 | Batch: 004 / 011 | Total loss: 0.786 | Reg loss: 0.028 | Tree loss: 0.786 | Accuracy: 0.721500 | 0.507 sec/iter\n",
      "Epoch: 446 | Batch: 005 / 011 | Total loss: 0.806 | Reg loss: 0.028 | Tree loss: 0.806 | Accuracy: 0.711000 | 0.507 sec/iter\n",
      "Epoch: 446 | Batch: 006 / 011 | Total loss: 0.773 | Reg loss: 0.028 | Tree loss: 0.773 | Accuracy: 0.733000 | 0.507 sec/iter\n",
      "Epoch: 446 | Batch: 007 / 011 | Total loss: 0.794 | Reg loss: 0.028 | Tree loss: 0.794 | Accuracy: 0.723000 | 0.507 sec/iter\n",
      "Epoch: 446 | Batch: 008 / 011 | Total loss: 0.791 | Reg loss: 0.028 | Tree loss: 0.791 | Accuracy: 0.724500 | 0.507 sec/iter\n",
      "Epoch: 446 | Batch: 009 / 011 | Total loss: 0.811 | Reg loss: 0.028 | Tree loss: 0.811 | Accuracy: 0.724000 | 0.507 sec/iter\n",
      "Epoch: 446 | Batch: 010 / 011 | Total loss: 0.799 | Reg loss: 0.028 | Tree loss: 0.799 | Accuracy: 0.720137 | 0.507 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 447 | Batch: 000 / 011 | Total loss: 0.867 | Reg loss: 0.028 | Tree loss: 0.867 | Accuracy: 0.654500 | 0.508 sec/iter\n",
      "Epoch: 447 | Batch: 001 / 011 | Total loss: 0.866 | Reg loss: 0.028 | Tree loss: 0.866 | Accuracy: 0.672000 | 0.508 sec/iter\n",
      "Epoch: 447 | Batch: 002 / 011 | Total loss: 0.841 | Reg loss: 0.028 | Tree loss: 0.841 | Accuracy: 0.687000 | 0.508 sec/iter\n",
      "Epoch: 447 | Batch: 003 / 011 | Total loss: 0.802 | Reg loss: 0.028 | Tree loss: 0.802 | Accuracy: 0.691000 | 0.507 sec/iter\n",
      "Epoch: 447 | Batch: 004 / 011 | Total loss: 0.803 | Reg loss: 0.028 | Tree loss: 0.803 | Accuracy: 0.716000 | 0.507 sec/iter\n",
      "Epoch: 447 | Batch: 005 / 011 | Total loss: 0.797 | Reg loss: 0.028 | Tree loss: 0.797 | Accuracy: 0.723500 | 0.507 sec/iter\n",
      "Epoch: 447 | Batch: 006 / 011 | Total loss: 0.806 | Reg loss: 0.028 | Tree loss: 0.806 | Accuracy: 0.716500 | 0.507 sec/iter\n",
      "Epoch: 447 | Batch: 007 / 011 | Total loss: 0.787 | Reg loss: 0.028 | Tree loss: 0.787 | Accuracy: 0.732500 | 0.507 sec/iter\n",
      "Epoch: 447 | Batch: 008 / 011 | Total loss: 0.790 | Reg loss: 0.028 | Tree loss: 0.790 | Accuracy: 0.727000 | 0.507 sec/iter\n",
      "Epoch: 447 | Batch: 009 / 011 | Total loss: 0.772 | Reg loss: 0.028 | Tree loss: 0.772 | Accuracy: 0.727500 | 0.507 sec/iter\n",
      "Epoch: 447 | Batch: 010 / 011 | Total loss: 0.718 | Reg loss: 0.028 | Tree loss: 0.718 | Accuracy: 0.757679 | 0.507 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 448 | Batch: 000 / 011 | Total loss: 0.856 | Reg loss: 0.028 | Tree loss: 0.856 | Accuracy: 0.664500 | 0.508 sec/iter\n",
      "Epoch: 448 | Batch: 001 / 011 | Total loss: 0.850 | Reg loss: 0.028 | Tree loss: 0.850 | Accuracy: 0.671500 | 0.508 sec/iter\n",
      "Epoch: 448 | Batch: 002 / 011 | Total loss: 0.864 | Reg loss: 0.028 | Tree loss: 0.864 | Accuracy: 0.678000 | 0.508 sec/iter\n",
      "Epoch: 448 | Batch: 003 / 011 | Total loss: 0.834 | Reg loss: 0.028 | Tree loss: 0.834 | Accuracy: 0.683500 | 0.507 sec/iter\n",
      "Epoch: 448 | Batch: 004 / 011 | Total loss: 0.794 | Reg loss: 0.028 | Tree loss: 0.794 | Accuracy: 0.720000 | 0.507 sec/iter\n",
      "Epoch: 448 | Batch: 005 / 011 | Total loss: 0.803 | Reg loss: 0.028 | Tree loss: 0.803 | Accuracy: 0.718000 | 0.507 sec/iter\n",
      "Epoch: 448 | Batch: 006 / 011 | Total loss: 0.791 | Reg loss: 0.028 | Tree loss: 0.791 | Accuracy: 0.725500 | 0.507 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 448 | Batch: 007 / 011 | Total loss: 0.786 | Reg loss: 0.028 | Tree loss: 0.786 | Accuracy: 0.725500 | 0.507 sec/iter\n",
      "Epoch: 448 | Batch: 008 / 011 | Total loss: 0.767 | Reg loss: 0.028 | Tree loss: 0.767 | Accuracy: 0.743000 | 0.507 sec/iter\n",
      "Epoch: 448 | Batch: 009 / 011 | Total loss: 0.769 | Reg loss: 0.028 | Tree loss: 0.769 | Accuracy: 0.733000 | 0.507 sec/iter\n",
      "Epoch: 448 | Batch: 010 / 011 | Total loss: 0.781 | Reg loss: 0.028 | Tree loss: 0.781 | Accuracy: 0.737201 | 0.507 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 449 | Batch: 000 / 011 | Total loss: 0.894 | Reg loss: 0.028 | Tree loss: 0.894 | Accuracy: 0.645000 | 0.507 sec/iter\n",
      "Epoch: 449 | Batch: 001 / 011 | Total loss: 0.852 | Reg loss: 0.028 | Tree loss: 0.852 | Accuracy: 0.671500 | 0.507 sec/iter\n",
      "Epoch: 449 | Batch: 002 / 011 | Total loss: 0.818 | Reg loss: 0.028 | Tree loss: 0.818 | Accuracy: 0.681000 | 0.507 sec/iter\n",
      "Epoch: 449 | Batch: 003 / 011 | Total loss: 0.809 | Reg loss: 0.028 | Tree loss: 0.809 | Accuracy: 0.685000 | 0.507 sec/iter\n",
      "Epoch: 449 | Batch: 004 / 011 | Total loss: 0.815 | Reg loss: 0.028 | Tree loss: 0.815 | Accuracy: 0.695000 | 0.507 sec/iter\n",
      "Epoch: 449 | Batch: 005 / 011 | Total loss: 0.802 | Reg loss: 0.028 | Tree loss: 0.802 | Accuracy: 0.716500 | 0.507 sec/iter\n",
      "Epoch: 449 | Batch: 006 / 011 | Total loss: 0.780 | Reg loss: 0.028 | Tree loss: 0.780 | Accuracy: 0.726000 | 0.507 sec/iter\n",
      "Epoch: 449 | Batch: 007 / 011 | Total loss: 0.797 | Reg loss: 0.028 | Tree loss: 0.797 | Accuracy: 0.719500 | 0.507 sec/iter\n",
      "Epoch: 449 | Batch: 008 / 011 | Total loss: 0.763 | Reg loss: 0.028 | Tree loss: 0.763 | Accuracy: 0.750500 | 0.507 sec/iter\n",
      "Epoch: 449 | Batch: 009 / 011 | Total loss: 0.799 | Reg loss: 0.028 | Tree loss: 0.799 | Accuracy: 0.735000 | 0.507 sec/iter\n",
      "Epoch: 449 | Batch: 010 / 011 | Total loss: 0.764 | Reg loss: 0.028 | Tree loss: 0.764 | Accuracy: 0.788396 | 0.507 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 450 | Batch: 000 / 011 | Total loss: 0.876 | Reg loss: 0.028 | Tree loss: 0.876 | Accuracy: 0.658000 | 0.507 sec/iter\n",
      "Epoch: 450 | Batch: 001 / 011 | Total loss: 0.868 | Reg loss: 0.028 | Tree loss: 0.868 | Accuracy: 0.662500 | 0.507 sec/iter\n",
      "Epoch: 450 | Batch: 002 / 011 | Total loss: 0.833 | Reg loss: 0.028 | Tree loss: 0.833 | Accuracy: 0.686000 | 0.507 sec/iter\n",
      "Epoch: 450 | Batch: 003 / 011 | Total loss: 0.824 | Reg loss: 0.028 | Tree loss: 0.824 | Accuracy: 0.693000 | 0.507 sec/iter\n",
      "Epoch: 450 | Batch: 004 / 011 | Total loss: 0.798 | Reg loss: 0.028 | Tree loss: 0.798 | Accuracy: 0.720000 | 0.507 sec/iter\n",
      "Epoch: 450 | Batch: 005 / 011 | Total loss: 0.806 | Reg loss: 0.028 | Tree loss: 0.806 | Accuracy: 0.703000 | 0.507 sec/iter\n",
      "Epoch: 450 | Batch: 006 / 011 | Total loss: 0.785 | Reg loss: 0.028 | Tree loss: 0.785 | Accuracy: 0.726500 | 0.506 sec/iter\n",
      "Epoch: 450 | Batch: 007 / 011 | Total loss: 0.772 | Reg loss: 0.028 | Tree loss: 0.772 | Accuracy: 0.742000 | 0.506 sec/iter\n",
      "Epoch: 450 | Batch: 008 / 011 | Total loss: 0.801 | Reg loss: 0.028 | Tree loss: 0.801 | Accuracy: 0.731500 | 0.506 sec/iter\n",
      "Epoch: 450 | Batch: 009 / 011 | Total loss: 0.746 | Reg loss: 0.028 | Tree loss: 0.746 | Accuracy: 0.748000 | 0.506 sec/iter\n",
      "Epoch: 450 | Batch: 010 / 011 | Total loss: 0.788 | Reg loss: 0.028 | Tree loss: 0.788 | Accuracy: 0.709898 | 0.506 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 451 | Batch: 000 / 011 | Total loss: 0.872 | Reg loss: 0.028 | Tree loss: 0.872 | Accuracy: 0.668000 | 0.507 sec/iter\n",
      "Epoch: 451 | Batch: 001 / 011 | Total loss: 0.881 | Reg loss: 0.028 | Tree loss: 0.881 | Accuracy: 0.664500 | 0.507 sec/iter\n",
      "Epoch: 451 | Batch: 002 / 011 | Total loss: 0.865 | Reg loss: 0.028 | Tree loss: 0.865 | Accuracy: 0.666500 | 0.507 sec/iter\n",
      "Epoch: 451 | Batch: 003 / 011 | Total loss: 0.797 | Reg loss: 0.028 | Tree loss: 0.797 | Accuracy: 0.724500 | 0.507 sec/iter\n",
      "Epoch: 451 | Batch: 004 / 011 | Total loss: 0.777 | Reg loss: 0.028 | Tree loss: 0.777 | Accuracy: 0.728500 | 0.507 sec/iter\n",
      "Epoch: 451 | Batch: 005 / 011 | Total loss: 0.806 | Reg loss: 0.028 | Tree loss: 0.806 | Accuracy: 0.713500 | 0.507 sec/iter\n",
      "Epoch: 451 | Batch: 006 / 011 | Total loss: 0.747 | Reg loss: 0.028 | Tree loss: 0.747 | Accuracy: 0.748500 | 0.506 sec/iter\n",
      "Epoch: 451 | Batch: 007 / 011 | Total loss: 0.790 | Reg loss: 0.028 | Tree loss: 0.790 | Accuracy: 0.722500 | 0.506 sec/iter\n",
      "Epoch: 451 | Batch: 008 / 011 | Total loss: 0.805 | Reg loss: 0.028 | Tree loss: 0.805 | Accuracy: 0.717000 | 0.506 sec/iter\n",
      "Epoch: 451 | Batch: 009 / 011 | Total loss: 0.779 | Reg loss: 0.028 | Tree loss: 0.779 | Accuracy: 0.741000 | 0.506 sec/iter\n",
      "Epoch: 451 | Batch: 010 / 011 | Total loss: 0.747 | Reg loss: 0.028 | Tree loss: 0.747 | Accuracy: 0.750853 | 0.506 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 452 | Batch: 000 / 011 | Total loss: 0.891 | Reg loss: 0.028 | Tree loss: 0.891 | Accuracy: 0.655500 | 0.507 sec/iter\n",
      "Epoch: 452 | Batch: 001 / 011 | Total loss: 0.863 | Reg loss: 0.028 | Tree loss: 0.863 | Accuracy: 0.657500 | 0.507 sec/iter\n",
      "Epoch: 452 | Batch: 002 / 011 | Total loss: 0.846 | Reg loss: 0.028 | Tree loss: 0.846 | Accuracy: 0.684500 | 0.507 sec/iter\n",
      "Epoch: 452 | Batch: 003 / 011 | Total loss: 0.838 | Reg loss: 0.028 | Tree loss: 0.838 | Accuracy: 0.681500 | 0.507 sec/iter\n",
      "Epoch: 452 | Batch: 004 / 011 | Total loss: 0.772 | Reg loss: 0.028 | Tree loss: 0.772 | Accuracy: 0.753500 | 0.507 sec/iter\n",
      "Epoch: 452 | Batch: 005 / 011 | Total loss: 0.809 | Reg loss: 0.028 | Tree loss: 0.809 | Accuracy: 0.700000 | 0.507 sec/iter\n",
      "Epoch: 452 | Batch: 006 / 011 | Total loss: 0.776 | Reg loss: 0.028 | Tree loss: 0.776 | Accuracy: 0.729000 | 0.507 sec/iter\n",
      "Epoch: 452 | Batch: 007 / 011 | Total loss: 0.760 | Reg loss: 0.028 | Tree loss: 0.760 | Accuracy: 0.745500 | 0.507 sec/iter\n",
      "Epoch: 452 | Batch: 008 / 011 | Total loss: 0.759 | Reg loss: 0.028 | Tree loss: 0.759 | Accuracy: 0.742500 | 0.507 sec/iter\n",
      "Epoch: 452 | Batch: 009 / 011 | Total loss: 0.768 | Reg loss: 0.028 | Tree loss: 0.768 | Accuracy: 0.741500 | 0.507 sec/iter\n",
      "Epoch: 452 | Batch: 010 / 011 | Total loss: 0.805 | Reg loss: 0.028 | Tree loss: 0.805 | Accuracy: 0.771331 | 0.507 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 453 | Batch: 000 / 011 | Total loss: 0.870 | Reg loss: 0.028 | Tree loss: 0.870 | Accuracy: 0.670000 | 0.507 sec/iter\n",
      "Epoch: 453 | Batch: 001 / 011 | Total loss: 0.853 | Reg loss: 0.028 | Tree loss: 0.853 | Accuracy: 0.679000 | 0.507 sec/iter\n",
      "Epoch: 453 | Batch: 002 / 011 | Total loss: 0.832 | Reg loss: 0.028 | Tree loss: 0.832 | Accuracy: 0.704000 | 0.507 sec/iter\n",
      "Epoch: 453 | Batch: 003 / 011 | Total loss: 0.822 | Reg loss: 0.028 | Tree loss: 0.822 | Accuracy: 0.705000 | 0.507 sec/iter\n",
      "Epoch: 453 | Batch: 004 / 011 | Total loss: 0.808 | Reg loss: 0.028 | Tree loss: 0.808 | Accuracy: 0.701500 | 0.507 sec/iter\n",
      "Epoch: 453 | Batch: 005 / 011 | Total loss: 0.815 | Reg loss: 0.028 | Tree loss: 0.815 | Accuracy: 0.701500 | 0.506 sec/iter\n",
      "Epoch: 453 | Batch: 006 / 011 | Total loss: 0.766 | Reg loss: 0.028 | Tree loss: 0.766 | Accuracy: 0.732000 | 0.506 sec/iter\n",
      "Epoch: 453 | Batch: 007 / 011 | Total loss: 0.759 | Reg loss: 0.028 | Tree loss: 0.759 | Accuracy: 0.743000 | 0.506 sec/iter\n",
      "Epoch: 453 | Batch: 008 / 011 | Total loss: 0.791 | Reg loss: 0.028 | Tree loss: 0.791 | Accuracy: 0.729500 | 0.506 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 453 | Batch: 009 / 011 | Total loss: 0.791 | Reg loss: 0.028 | Tree loss: 0.791 | Accuracy: 0.724000 | 0.506 sec/iter\n",
      "Epoch: 453 | Batch: 010 / 011 | Total loss: 0.732 | Reg loss: 0.028 | Tree loss: 0.732 | Accuracy: 0.757679 | 0.506 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 454 | Batch: 000 / 011 | Total loss: 0.872 | Reg loss: 0.028 | Tree loss: 0.872 | Accuracy: 0.668500 | 0.507 sec/iter\n",
      "Epoch: 454 | Batch: 001 / 011 | Total loss: 0.850 | Reg loss: 0.028 | Tree loss: 0.850 | Accuracy: 0.673000 | 0.507 sec/iter\n",
      "Epoch: 454 | Batch: 002 / 011 | Total loss: 0.827 | Reg loss: 0.028 | Tree loss: 0.827 | Accuracy: 0.694000 | 0.507 sec/iter\n",
      "Epoch: 454 | Batch: 003 / 011 | Total loss: 0.842 | Reg loss: 0.028 | Tree loss: 0.842 | Accuracy: 0.672500 | 0.507 sec/iter\n",
      "Epoch: 454 | Batch: 004 / 011 | Total loss: 0.793 | Reg loss: 0.028 | Tree loss: 0.793 | Accuracy: 0.724500 | 0.507 sec/iter\n",
      "Epoch: 454 | Batch: 005 / 011 | Total loss: 0.802 | Reg loss: 0.028 | Tree loss: 0.802 | Accuracy: 0.704500 | 0.507 sec/iter\n",
      "Epoch: 454 | Batch: 006 / 011 | Total loss: 0.783 | Reg loss: 0.028 | Tree loss: 0.783 | Accuracy: 0.731000 | 0.507 sec/iter\n",
      "Epoch: 454 | Batch: 007 / 011 | Total loss: 0.780 | Reg loss: 0.028 | Tree loss: 0.780 | Accuracy: 0.732000 | 0.507 sec/iter\n",
      "Epoch: 454 | Batch: 008 / 011 | Total loss: 0.775 | Reg loss: 0.028 | Tree loss: 0.775 | Accuracy: 0.728500 | 0.507 sec/iter\n",
      "Epoch: 454 | Batch: 009 / 011 | Total loss: 0.802 | Reg loss: 0.028 | Tree loss: 0.802 | Accuracy: 0.715500 | 0.507 sec/iter\n",
      "Epoch: 454 | Batch: 010 / 011 | Total loss: 0.849 | Reg loss: 0.028 | Tree loss: 0.849 | Accuracy: 0.696246 | 0.507 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 455 | Batch: 000 / 011 | Total loss: 0.860 | Reg loss: 0.028 | Tree loss: 0.860 | Accuracy: 0.666500 | 0.507 sec/iter\n",
      "Epoch: 455 | Batch: 001 / 011 | Total loss: 0.845 | Reg loss: 0.028 | Tree loss: 0.845 | Accuracy: 0.680000 | 0.507 sec/iter\n",
      "Epoch: 455 | Batch: 002 / 011 | Total loss: 0.857 | Reg loss: 0.028 | Tree loss: 0.857 | Accuracy: 0.678000 | 0.507 sec/iter\n",
      "Epoch: 455 | Batch: 003 / 011 | Total loss: 0.836 | Reg loss: 0.028 | Tree loss: 0.836 | Accuracy: 0.669000 | 0.507 sec/iter\n",
      "Epoch: 455 | Batch: 004 / 011 | Total loss: 0.775 | Reg loss: 0.028 | Tree loss: 0.775 | Accuracy: 0.725500 | 0.507 sec/iter\n",
      "Epoch: 455 | Batch: 005 / 011 | Total loss: 0.793 | Reg loss: 0.028 | Tree loss: 0.793 | Accuracy: 0.712000 | 0.507 sec/iter\n",
      "Epoch: 455 | Batch: 006 / 011 | Total loss: 0.776 | Reg loss: 0.028 | Tree loss: 0.776 | Accuracy: 0.730500 | 0.507 sec/iter\n",
      "Epoch: 455 | Batch: 007 / 011 | Total loss: 0.773 | Reg loss: 0.028 | Tree loss: 0.773 | Accuracy: 0.734000 | 0.507 sec/iter\n",
      "Epoch: 455 | Batch: 008 / 011 | Total loss: 0.807 | Reg loss: 0.028 | Tree loss: 0.807 | Accuracy: 0.723000 | 0.507 sec/iter\n",
      "Epoch: 455 | Batch: 009 / 011 | Total loss: 0.805 | Reg loss: 0.028 | Tree loss: 0.805 | Accuracy: 0.716000 | 0.507 sec/iter\n",
      "Epoch: 455 | Batch: 010 / 011 | Total loss: 0.728 | Reg loss: 0.028 | Tree loss: 0.728 | Accuracy: 0.761092 | 0.507 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 456 | Batch: 000 / 011 | Total loss: 0.870 | Reg loss: 0.028 | Tree loss: 0.870 | Accuracy: 0.660500 | 0.507 sec/iter\n",
      "Epoch: 456 | Batch: 001 / 011 | Total loss: 0.874 | Reg loss: 0.028 | Tree loss: 0.874 | Accuracy: 0.653000 | 0.507 sec/iter\n",
      "Epoch: 456 | Batch: 002 / 011 | Total loss: 0.834 | Reg loss: 0.028 | Tree loss: 0.834 | Accuracy: 0.698500 | 0.507 sec/iter\n",
      "Epoch: 456 | Batch: 003 / 011 | Total loss: 0.832 | Reg loss: 0.028 | Tree loss: 0.832 | Accuracy: 0.683000 | 0.507 sec/iter\n",
      "Epoch: 456 | Batch: 004 / 011 | Total loss: 0.800 | Reg loss: 0.028 | Tree loss: 0.800 | Accuracy: 0.714000 | 0.507 sec/iter\n",
      "Epoch: 456 | Batch: 005 / 011 | Total loss: 0.813 | Reg loss: 0.028 | Tree loss: 0.813 | Accuracy: 0.693500 | 0.507 sec/iter\n",
      "Epoch: 456 | Batch: 006 / 011 | Total loss: 0.763 | Reg loss: 0.028 | Tree loss: 0.763 | Accuracy: 0.732500 | 0.507 sec/iter\n",
      "Epoch: 456 | Batch: 007 / 011 | Total loss: 0.782 | Reg loss: 0.028 | Tree loss: 0.782 | Accuracy: 0.730000 | 0.507 sec/iter\n",
      "Epoch: 456 | Batch: 008 / 011 | Total loss: 0.797 | Reg loss: 0.028 | Tree loss: 0.797 | Accuracy: 0.727000 | 0.507 sec/iter\n",
      "Epoch: 456 | Batch: 009 / 011 | Total loss: 0.759 | Reg loss: 0.028 | Tree loss: 0.759 | Accuracy: 0.743000 | 0.507 sec/iter\n",
      "Epoch: 456 | Batch: 010 / 011 | Total loss: 0.775 | Reg loss: 0.028 | Tree loss: 0.775 | Accuracy: 0.726962 | 0.507 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 457 | Batch: 000 / 011 | Total loss: 0.864 | Reg loss: 0.028 | Tree loss: 0.864 | Accuracy: 0.657000 | 0.507 sec/iter\n",
      "Epoch: 457 | Batch: 001 / 011 | Total loss: 0.840 | Reg loss: 0.028 | Tree loss: 0.840 | Accuracy: 0.680500 | 0.507 sec/iter\n",
      "Epoch: 457 | Batch: 002 / 011 | Total loss: 0.857 | Reg loss: 0.028 | Tree loss: 0.857 | Accuracy: 0.669000 | 0.507 sec/iter\n",
      "Epoch: 457 | Batch: 003 / 011 | Total loss: 0.813 | Reg loss: 0.028 | Tree loss: 0.813 | Accuracy: 0.693000 | 0.507 sec/iter\n",
      "Epoch: 457 | Batch: 004 / 011 | Total loss: 0.800 | Reg loss: 0.028 | Tree loss: 0.800 | Accuracy: 0.707000 | 0.507 sec/iter\n",
      "Epoch: 457 | Batch: 005 / 011 | Total loss: 0.794 | Reg loss: 0.028 | Tree loss: 0.794 | Accuracy: 0.715500 | 0.507 sec/iter\n",
      "Epoch: 457 | Batch: 006 / 011 | Total loss: 0.783 | Reg loss: 0.028 | Tree loss: 0.783 | Accuracy: 0.738500 | 0.507 sec/iter\n",
      "Epoch: 457 | Batch: 007 / 011 | Total loss: 0.820 | Reg loss: 0.028 | Tree loss: 0.820 | Accuracy: 0.717000 | 0.507 sec/iter\n",
      "Epoch: 457 | Batch: 008 / 011 | Total loss: 0.766 | Reg loss: 0.028 | Tree loss: 0.766 | Accuracy: 0.742000 | 0.507 sec/iter\n",
      "Epoch: 457 | Batch: 009 / 011 | Total loss: 0.793 | Reg loss: 0.028 | Tree loss: 0.793 | Accuracy: 0.730500 | 0.507 sec/iter\n",
      "Epoch: 457 | Batch: 010 / 011 | Total loss: 0.771 | Reg loss: 0.028 | Tree loss: 0.771 | Accuracy: 0.744027 | 0.507 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 458 | Batch: 000 / 011 | Total loss: 0.872 | Reg loss: 0.028 | Tree loss: 0.872 | Accuracy: 0.654000 | 0.507 sec/iter\n",
      "Epoch: 458 | Batch: 001 / 011 | Total loss: 0.847 | Reg loss: 0.028 | Tree loss: 0.847 | Accuracy: 0.670000 | 0.507 sec/iter\n",
      "Epoch: 458 | Batch: 002 / 011 | Total loss: 0.825 | Reg loss: 0.028 | Tree loss: 0.825 | Accuracy: 0.696000 | 0.507 sec/iter\n",
      "Epoch: 458 | Batch: 003 / 011 | Total loss: 0.842 | Reg loss: 0.028 | Tree loss: 0.842 | Accuracy: 0.681500 | 0.507 sec/iter\n",
      "Epoch: 458 | Batch: 004 / 011 | Total loss: 0.823 | Reg loss: 0.028 | Tree loss: 0.823 | Accuracy: 0.709500 | 0.507 sec/iter\n",
      "Epoch: 458 | Batch: 005 / 011 | Total loss: 0.811 | Reg loss: 0.028 | Tree loss: 0.811 | Accuracy: 0.723000 | 0.507 sec/iter\n",
      "Epoch: 458 | Batch: 006 / 011 | Total loss: 0.788 | Reg loss: 0.028 | Tree loss: 0.788 | Accuracy: 0.720000 | 0.507 sec/iter\n",
      "Epoch: 458 | Batch: 007 / 011 | Total loss: 0.795 | Reg loss: 0.028 | Tree loss: 0.795 | Accuracy: 0.718500 | 0.507 sec/iter\n",
      "Epoch: 458 | Batch: 008 / 011 | Total loss: 0.759 | Reg loss: 0.028 | Tree loss: 0.759 | Accuracy: 0.740500 | 0.507 sec/iter\n",
      "Epoch: 458 | Batch: 009 / 011 | Total loss: 0.767 | Reg loss: 0.028 | Tree loss: 0.767 | Accuracy: 0.741500 | 0.506 sec/iter\n",
      "Epoch: 458 | Batch: 010 / 011 | Total loss: 0.795 | Reg loss: 0.028 | Tree loss: 0.795 | Accuracy: 0.686007 | 0.506 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 459 | Batch: 000 / 011 | Total loss: 0.841 | Reg loss: 0.028 | Tree loss: 0.841 | Accuracy: 0.684500 | 0.507 sec/iter\n",
      "Epoch: 459 | Batch: 001 / 011 | Total loss: 0.839 | Reg loss: 0.028 | Tree loss: 0.839 | Accuracy: 0.676000 | 0.507 sec/iter\n",
      "Epoch: 459 | Batch: 002 / 011 | Total loss: 0.883 | Reg loss: 0.028 | Tree loss: 0.883 | Accuracy: 0.669500 | 0.507 sec/iter\n",
      "Epoch: 459 | Batch: 003 / 011 | Total loss: 0.850 | Reg loss: 0.028 | Tree loss: 0.850 | Accuracy: 0.669500 | 0.507 sec/iter\n",
      "Epoch: 459 | Batch: 004 / 011 | Total loss: 0.819 | Reg loss: 0.028 | Tree loss: 0.819 | Accuracy: 0.701500 | 0.507 sec/iter\n",
      "Epoch: 459 | Batch: 005 / 011 | Total loss: 0.799 | Reg loss: 0.028 | Tree loss: 0.799 | Accuracy: 0.701000 | 0.507 sec/iter\n",
      "Epoch: 459 | Batch: 006 / 011 | Total loss: 0.772 | Reg loss: 0.028 | Tree loss: 0.772 | Accuracy: 0.738000 | 0.507 sec/iter\n",
      "Epoch: 459 | Batch: 007 / 011 | Total loss: 0.784 | Reg loss: 0.028 | Tree loss: 0.784 | Accuracy: 0.722000 | 0.507 sec/iter\n",
      "Epoch: 459 | Batch: 008 / 011 | Total loss: 0.779 | Reg loss: 0.028 | Tree loss: 0.779 | Accuracy: 0.739000 | 0.507 sec/iter\n",
      "Epoch: 459 | Batch: 009 / 011 | Total loss: 0.773 | Reg loss: 0.028 | Tree loss: 0.773 | Accuracy: 0.734500 | 0.507 sec/iter\n",
      "Epoch: 459 | Batch: 010 / 011 | Total loss: 0.793 | Reg loss: 0.028 | Tree loss: 0.793 | Accuracy: 0.713311 | 0.507 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 460 | Batch: 000 / 011 | Total loss: 0.886 | Reg loss: 0.028 | Tree loss: 0.886 | Accuracy: 0.659500 | 0.508 sec/iter\n",
      "Epoch: 460 | Batch: 001 / 011 | Total loss: 0.855 | Reg loss: 0.028 | Tree loss: 0.855 | Accuracy: 0.679000 | 0.508 sec/iter\n",
      "Epoch: 460 | Batch: 002 / 011 | Total loss: 0.839 | Reg loss: 0.028 | Tree loss: 0.839 | Accuracy: 0.691000 | 0.508 sec/iter\n",
      "Epoch: 460 | Batch: 003 / 011 | Total loss: 0.817 | Reg loss: 0.028 | Tree loss: 0.817 | Accuracy: 0.692000 | 0.508 sec/iter\n",
      "Epoch: 460 | Batch: 004 / 011 | Total loss: 0.794 | Reg loss: 0.028 | Tree loss: 0.794 | Accuracy: 0.698500 | 0.508 sec/iter\n",
      "Epoch: 460 | Batch: 005 / 011 | Total loss: 0.788 | Reg loss: 0.028 | Tree loss: 0.788 | Accuracy: 0.708500 | 0.508 sec/iter\n",
      "Epoch: 460 | Batch: 006 / 011 | Total loss: 0.790 | Reg loss: 0.028 | Tree loss: 0.790 | Accuracy: 0.724500 | 0.507 sec/iter\n",
      "Epoch: 460 | Batch: 007 / 011 | Total loss: 0.788 | Reg loss: 0.028 | Tree loss: 0.788 | Accuracy: 0.723500 | 0.507 sec/iter\n",
      "Epoch: 460 | Batch: 008 / 011 | Total loss: 0.791 | Reg loss: 0.028 | Tree loss: 0.791 | Accuracy: 0.726000 | 0.507 sec/iter\n",
      "Epoch: 460 | Batch: 009 / 011 | Total loss: 0.790 | Reg loss: 0.028 | Tree loss: 0.790 | Accuracy: 0.728500 | 0.507 sec/iter\n",
      "Epoch: 460 | Batch: 010 / 011 | Total loss: 0.838 | Reg loss: 0.028 | Tree loss: 0.838 | Accuracy: 0.675768 | 0.507 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 461 | Batch: 000 / 011 | Total loss: 0.867 | Reg loss: 0.028 | Tree loss: 0.867 | Accuracy: 0.678000 | 0.507 sec/iter\n",
      "Epoch: 461 | Batch: 001 / 011 | Total loss: 0.889 | Reg loss: 0.028 | Tree loss: 0.889 | Accuracy: 0.655500 | 0.507 sec/iter\n",
      "Epoch: 461 | Batch: 002 / 011 | Total loss: 0.837 | Reg loss: 0.028 | Tree loss: 0.837 | Accuracy: 0.681500 | 0.507 sec/iter\n",
      "Epoch: 461 | Batch: 003 / 011 | Total loss: 0.834 | Reg loss: 0.028 | Tree loss: 0.834 | Accuracy: 0.681000 | 0.507 sec/iter\n",
      "Epoch: 461 | Batch: 004 / 011 | Total loss: 0.803 | Reg loss: 0.028 | Tree loss: 0.803 | Accuracy: 0.715500 | 0.507 sec/iter\n",
      "Epoch: 461 | Batch: 005 / 011 | Total loss: 0.784 | Reg loss: 0.028 | Tree loss: 0.784 | Accuracy: 0.713000 | 0.507 sec/iter\n",
      "Epoch: 461 | Batch: 006 / 011 | Total loss: 0.789 | Reg loss: 0.028 | Tree loss: 0.789 | Accuracy: 0.728500 | 0.507 sec/iter\n",
      "Epoch: 461 | Batch: 007 / 011 | Total loss: 0.802 | Reg loss: 0.028 | Tree loss: 0.802 | Accuracy: 0.717000 | 0.507 sec/iter\n",
      "Epoch: 461 | Batch: 008 / 011 | Total loss: 0.764 | Reg loss: 0.028 | Tree loss: 0.764 | Accuracy: 0.741500 | 0.507 sec/iter\n",
      "Epoch: 461 | Batch: 009 / 011 | Total loss: 0.771 | Reg loss: 0.028 | Tree loss: 0.771 | Accuracy: 0.737500 | 0.507 sec/iter\n",
      "Epoch: 461 | Batch: 010 / 011 | Total loss: 0.727 | Reg loss: 0.028 | Tree loss: 0.727 | Accuracy: 0.737201 | 0.507 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 462 | Batch: 000 / 011 | Total loss: 0.881 | Reg loss: 0.028 | Tree loss: 0.881 | Accuracy: 0.670000 | 0.507 sec/iter\n",
      "Epoch: 462 | Batch: 001 / 011 | Total loss: 0.893 | Reg loss: 0.028 | Tree loss: 0.893 | Accuracy: 0.654500 | 0.507 sec/iter\n",
      "Epoch: 462 | Batch: 002 / 011 | Total loss: 0.796 | Reg loss: 0.028 | Tree loss: 0.796 | Accuracy: 0.719000 | 0.507 sec/iter\n",
      "Epoch: 462 | Batch: 003 / 011 | Total loss: 0.825 | Reg loss: 0.028 | Tree loss: 0.825 | Accuracy: 0.686000 | 0.507 sec/iter\n",
      "Epoch: 462 | Batch: 004 / 011 | Total loss: 0.800 | Reg loss: 0.028 | Tree loss: 0.800 | Accuracy: 0.718500 | 0.507 sec/iter\n",
      "Epoch: 462 | Batch: 005 / 011 | Total loss: 0.796 | Reg loss: 0.028 | Tree loss: 0.796 | Accuracy: 0.707500 | 0.507 sec/iter\n",
      "Epoch: 462 | Batch: 006 / 011 | Total loss: 0.792 | Reg loss: 0.028 | Tree loss: 0.792 | Accuracy: 0.716500 | 0.507 sec/iter\n",
      "Epoch: 462 | Batch: 007 / 011 | Total loss: 0.755 | Reg loss: 0.028 | Tree loss: 0.755 | Accuracy: 0.744500 | 0.507 sec/iter\n",
      "Epoch: 462 | Batch: 008 / 011 | Total loss: 0.801 | Reg loss: 0.028 | Tree loss: 0.801 | Accuracy: 0.721000 | 0.507 sec/iter\n",
      "Epoch: 462 | Batch: 009 / 011 | Total loss: 0.774 | Reg loss: 0.028 | Tree loss: 0.774 | Accuracy: 0.734500 | 0.507 sec/iter\n",
      "Epoch: 462 | Batch: 010 / 011 | Total loss: 0.769 | Reg loss: 0.028 | Tree loss: 0.769 | Accuracy: 0.730375 | 0.507 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 463 | Batch: 000 / 011 | Total loss: 0.874 | Reg loss: 0.028 | Tree loss: 0.874 | Accuracy: 0.667000 | 0.507 sec/iter\n",
      "Epoch: 463 | Batch: 001 / 011 | Total loss: 0.839 | Reg loss: 0.028 | Tree loss: 0.839 | Accuracy: 0.683000 | 0.507 sec/iter\n",
      "Epoch: 463 | Batch: 002 / 011 | Total loss: 0.851 | Reg loss: 0.028 | Tree loss: 0.851 | Accuracy: 0.679500 | 0.507 sec/iter\n",
      "Epoch: 463 | Batch: 003 / 011 | Total loss: 0.833 | Reg loss: 0.028 | Tree loss: 0.833 | Accuracy: 0.675500 | 0.507 sec/iter\n",
      "Epoch: 463 | Batch: 004 / 011 | Total loss: 0.792 | Reg loss: 0.028 | Tree loss: 0.792 | Accuracy: 0.718000 | 0.507 sec/iter\n",
      "Epoch: 463 | Batch: 005 / 011 | Total loss: 0.786 | Reg loss: 0.028 | Tree loss: 0.786 | Accuracy: 0.723500 | 0.507 sec/iter\n",
      "Epoch: 463 | Batch: 006 / 011 | Total loss: 0.819 | Reg loss: 0.028 | Tree loss: 0.819 | Accuracy: 0.703500 | 0.507 sec/iter\n",
      "Epoch: 463 | Batch: 007 / 011 | Total loss: 0.774 | Reg loss: 0.028 | Tree loss: 0.774 | Accuracy: 0.754500 | 0.507 sec/iter\n",
      "Epoch: 463 | Batch: 008 / 011 | Total loss: 0.767 | Reg loss: 0.028 | Tree loss: 0.767 | Accuracy: 0.728000 | 0.507 sec/iter\n",
      "Epoch: 463 | Batch: 009 / 011 | Total loss: 0.783 | Reg loss: 0.028 | Tree loss: 0.783 | Accuracy: 0.730500 | 0.506 sec/iter\n",
      "Epoch: 463 | Batch: 010 / 011 | Total loss: 0.810 | Reg loss: 0.028 | Tree loss: 0.810 | Accuracy: 0.709898 | 0.506 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 464 | Batch: 000 / 011 | Total loss: 0.883 | Reg loss: 0.028 | Tree loss: 0.883 | Accuracy: 0.651000 | 0.506 sec/iter\n",
      "Epoch: 464 | Batch: 001 / 011 | Total loss: 0.875 | Reg loss: 0.028 | Tree loss: 0.875 | Accuracy: 0.659500 | 0.506 sec/iter\n",
      "Epoch: 464 | Batch: 002 / 011 | Total loss: 0.855 | Reg loss: 0.028 | Tree loss: 0.855 | Accuracy: 0.669500 | 0.506 sec/iter\n",
      "Epoch: 464 | Batch: 003 / 011 | Total loss: 0.799 | Reg loss: 0.028 | Tree loss: 0.799 | Accuracy: 0.722500 | 0.506 sec/iter\n",
      "Epoch: 464 | Batch: 004 / 011 | Total loss: 0.802 | Reg loss: 0.028 | Tree loss: 0.802 | Accuracy: 0.713500 | 0.506 sec/iter\n",
      "Epoch: 464 | Batch: 005 / 011 | Total loss: 0.784 | Reg loss: 0.028 | Tree loss: 0.784 | Accuracy: 0.717000 | 0.506 sec/iter\n",
      "Epoch: 464 | Batch: 006 / 011 | Total loss: 0.802 | Reg loss: 0.028 | Tree loss: 0.802 | Accuracy: 0.719500 | 0.506 sec/iter\n",
      "Epoch: 464 | Batch: 007 / 011 | Total loss: 0.765 | Reg loss: 0.028 | Tree loss: 0.765 | Accuracy: 0.738000 | 0.506 sec/iter\n",
      "Epoch: 464 | Batch: 008 / 011 | Total loss: 0.783 | Reg loss: 0.028 | Tree loss: 0.783 | Accuracy: 0.726500 | 0.506 sec/iter\n",
      "Epoch: 464 | Batch: 009 / 011 | Total loss: 0.775 | Reg loss: 0.028 | Tree loss: 0.775 | Accuracy: 0.738000 | 0.506 sec/iter\n",
      "Epoch: 464 | Batch: 010 / 011 | Total loss: 0.787 | Reg loss: 0.028 | Tree loss: 0.787 | Accuracy: 0.726962 | 0.506 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 465 | Batch: 000 / 011 | Total loss: 0.885 | Reg loss: 0.028 | Tree loss: 0.885 | Accuracy: 0.658500 | 0.506 sec/iter\n",
      "Epoch: 465 | Batch: 001 / 011 | Total loss: 0.849 | Reg loss: 0.028 | Tree loss: 0.849 | Accuracy: 0.673500 | 0.506 sec/iter\n",
      "Epoch: 465 | Batch: 002 / 011 | Total loss: 0.838 | Reg loss: 0.028 | Tree loss: 0.838 | Accuracy: 0.689000 | 0.506 sec/iter\n",
      "Epoch: 465 | Batch: 003 / 011 | Total loss: 0.806 | Reg loss: 0.028 | Tree loss: 0.806 | Accuracy: 0.697000 | 0.506 sec/iter\n",
      "Epoch: 465 | Batch: 004 / 011 | Total loss: 0.810 | Reg loss: 0.028 | Tree loss: 0.810 | Accuracy: 0.713000 | 0.506 sec/iter\n",
      "Epoch: 465 | Batch: 005 / 011 | Total loss: 0.801 | Reg loss: 0.028 | Tree loss: 0.801 | Accuracy: 0.707500 | 0.506 sec/iter\n",
      "Epoch: 465 | Batch: 006 / 011 | Total loss: 0.763 | Reg loss: 0.028 | Tree loss: 0.763 | Accuracy: 0.732000 | 0.506 sec/iter\n",
      "Epoch: 465 | Batch: 007 / 011 | Total loss: 0.792 | Reg loss: 0.028 | Tree loss: 0.792 | Accuracy: 0.722500 | 0.506 sec/iter\n",
      "Epoch: 465 | Batch: 008 / 011 | Total loss: 0.773 | Reg loss: 0.028 | Tree loss: 0.773 | Accuracy: 0.727000 | 0.506 sec/iter\n",
      "Epoch: 465 | Batch: 009 / 011 | Total loss: 0.804 | Reg loss: 0.028 | Tree loss: 0.804 | Accuracy: 0.718500 | 0.506 sec/iter\n",
      "Epoch: 465 | Batch: 010 / 011 | Total loss: 0.801 | Reg loss: 0.028 | Tree loss: 0.801 | Accuracy: 0.740614 | 0.506 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 466 | Batch: 000 / 011 | Total loss: 0.849 | Reg loss: 0.028 | Tree loss: 0.849 | Accuracy: 0.679500 | 0.507 sec/iter\n",
      "Epoch: 466 | Batch: 001 / 011 | Total loss: 0.851 | Reg loss: 0.028 | Tree loss: 0.851 | Accuracy: 0.664500 | 0.507 sec/iter\n",
      "Epoch: 466 | Batch: 002 / 011 | Total loss: 0.838 | Reg loss: 0.028 | Tree loss: 0.838 | Accuracy: 0.690500 | 0.507 sec/iter\n",
      "Epoch: 466 | Batch: 003 / 011 | Total loss: 0.825 | Reg loss: 0.028 | Tree loss: 0.825 | Accuracy: 0.694500 | 0.507 sec/iter\n",
      "Epoch: 466 | Batch: 004 / 011 | Total loss: 0.816 | Reg loss: 0.028 | Tree loss: 0.816 | Accuracy: 0.704500 | 0.507 sec/iter\n",
      "Epoch: 466 | Batch: 005 / 011 | Total loss: 0.805 | Reg loss: 0.028 | Tree loss: 0.805 | Accuracy: 0.708000 | 0.507 sec/iter\n",
      "Epoch: 466 | Batch: 006 / 011 | Total loss: 0.778 | Reg loss: 0.028 | Tree loss: 0.778 | Accuracy: 0.716500 | 0.506 sec/iter\n",
      "Epoch: 466 | Batch: 007 / 011 | Total loss: 0.787 | Reg loss: 0.028 | Tree loss: 0.787 | Accuracy: 0.734500 | 0.506 sec/iter\n",
      "Epoch: 466 | Batch: 008 / 011 | Total loss: 0.783 | Reg loss: 0.028 | Tree loss: 0.783 | Accuracy: 0.746500 | 0.506 sec/iter\n",
      "Epoch: 466 | Batch: 009 / 011 | Total loss: 0.774 | Reg loss: 0.028 | Tree loss: 0.774 | Accuracy: 0.733000 | 0.506 sec/iter\n",
      "Epoch: 466 | Batch: 010 / 011 | Total loss: 0.800 | Reg loss: 0.028 | Tree loss: 0.800 | Accuracy: 0.713311 | 0.506 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 467 | Batch: 000 / 011 | Total loss: 0.876 | Reg loss: 0.028 | Tree loss: 0.876 | Accuracy: 0.653500 | 0.506 sec/iter\n",
      "Epoch: 467 | Batch: 001 / 011 | Total loss: 0.857 | Reg loss: 0.028 | Tree loss: 0.857 | Accuracy: 0.674500 | 0.506 sec/iter\n",
      "Epoch: 467 | Batch: 002 / 011 | Total loss: 0.867 | Reg loss: 0.028 | Tree loss: 0.867 | Accuracy: 0.668000 | 0.506 sec/iter\n",
      "Epoch: 467 | Batch: 003 / 011 | Total loss: 0.818 | Reg loss: 0.028 | Tree loss: 0.818 | Accuracy: 0.706000 | 0.506 sec/iter\n",
      "Epoch: 467 | Batch: 004 / 011 | Total loss: 0.803 | Reg loss: 0.028 | Tree loss: 0.803 | Accuracy: 0.698000 | 0.506 sec/iter\n",
      "Epoch: 467 | Batch: 005 / 011 | Total loss: 0.777 | Reg loss: 0.028 | Tree loss: 0.777 | Accuracy: 0.712000 | 0.506 sec/iter\n",
      "Epoch: 467 | Batch: 006 / 011 | Total loss: 0.759 | Reg loss: 0.028 | Tree loss: 0.759 | Accuracy: 0.731500 | 0.506 sec/iter\n",
      "Epoch: 467 | Batch: 007 / 011 | Total loss: 0.796 | Reg loss: 0.028 | Tree loss: 0.796 | Accuracy: 0.715500 | 0.506 sec/iter\n",
      "Epoch: 467 | Batch: 008 / 011 | Total loss: 0.777 | Reg loss: 0.028 | Tree loss: 0.777 | Accuracy: 0.722500 | 0.506 sec/iter\n",
      "Epoch: 467 | Batch: 009 / 011 | Total loss: 0.816 | Reg loss: 0.028 | Tree loss: 0.816 | Accuracy: 0.708500 | 0.506 sec/iter\n",
      "Epoch: 467 | Batch: 010 / 011 | Total loss: 0.785 | Reg loss: 0.028 | Tree loss: 0.785 | Accuracy: 0.713311 | 0.506 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 468 | Batch: 000 / 011 | Total loss: 0.862 | Reg loss: 0.028 | Tree loss: 0.862 | Accuracy: 0.677000 | 0.506 sec/iter\n",
      "Epoch: 468 | Batch: 001 / 011 | Total loss: 0.856 | Reg loss: 0.028 | Tree loss: 0.856 | Accuracy: 0.663500 | 0.506 sec/iter\n",
      "Epoch: 468 | Batch: 002 / 011 | Total loss: 0.875 | Reg loss: 0.028 | Tree loss: 0.875 | Accuracy: 0.663500 | 0.506 sec/iter\n",
      "Epoch: 468 | Batch: 003 / 011 | Total loss: 0.832 | Reg loss: 0.028 | Tree loss: 0.832 | Accuracy: 0.709500 | 0.506 sec/iter\n",
      "Epoch: 468 | Batch: 004 / 011 | Total loss: 0.774 | Reg loss: 0.028 | Tree loss: 0.774 | Accuracy: 0.721500 | 0.506 sec/iter\n",
      "Epoch: 468 | Batch: 005 / 011 | Total loss: 0.795 | Reg loss: 0.028 | Tree loss: 0.795 | Accuracy: 0.707500 | 0.506 sec/iter\n",
      "Epoch: 468 | Batch: 006 / 011 | Total loss: 0.783 | Reg loss: 0.028 | Tree loss: 0.783 | Accuracy: 0.724500 | 0.506 sec/iter\n",
      "Epoch: 468 | Batch: 007 / 011 | Total loss: 0.791 | Reg loss: 0.028 | Tree loss: 0.791 | Accuracy: 0.723500 | 0.506 sec/iter\n",
      "Epoch: 468 | Batch: 008 / 011 | Total loss: 0.789 | Reg loss: 0.028 | Tree loss: 0.789 | Accuracy: 0.724500 | 0.506 sec/iter\n",
      "Epoch: 468 | Batch: 009 / 011 | Total loss: 0.787 | Reg loss: 0.028 | Tree loss: 0.787 | Accuracy: 0.733500 | 0.506 sec/iter\n",
      "Epoch: 468 | Batch: 010 / 011 | Total loss: 0.745 | Reg loss: 0.028 | Tree loss: 0.745 | Accuracy: 0.726962 | 0.506 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 469 | Batch: 000 / 011 | Total loss: 0.864 | Reg loss: 0.028 | Tree loss: 0.864 | Accuracy: 0.674000 | 0.506 sec/iter\n",
      "Epoch: 469 | Batch: 001 / 011 | Total loss: 0.837 | Reg loss: 0.028 | Tree loss: 0.837 | Accuracy: 0.674500 | 0.506 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 469 | Batch: 002 / 011 | Total loss: 0.853 | Reg loss: 0.028 | Tree loss: 0.853 | Accuracy: 0.678500 | 0.506 sec/iter\n",
      "Epoch: 469 | Batch: 003 / 011 | Total loss: 0.804 | Reg loss: 0.028 | Tree loss: 0.804 | Accuracy: 0.710500 | 0.506 sec/iter\n",
      "Epoch: 469 | Batch: 004 / 011 | Total loss: 0.812 | Reg loss: 0.028 | Tree loss: 0.812 | Accuracy: 0.695500 | 0.506 sec/iter\n",
      "Epoch: 469 | Batch: 005 / 011 | Total loss: 0.803 | Reg loss: 0.028 | Tree loss: 0.803 | Accuracy: 0.695000 | 0.506 sec/iter\n",
      "Epoch: 469 | Batch: 006 / 011 | Total loss: 0.781 | Reg loss: 0.028 | Tree loss: 0.781 | Accuracy: 0.735500 | 0.506 sec/iter\n",
      "Epoch: 469 | Batch: 007 / 011 | Total loss: 0.810 | Reg loss: 0.028 | Tree loss: 0.810 | Accuracy: 0.727500 | 0.506 sec/iter\n",
      "Epoch: 469 | Batch: 008 / 011 | Total loss: 0.777 | Reg loss: 0.028 | Tree loss: 0.777 | Accuracy: 0.732000 | 0.506 sec/iter\n",
      "Epoch: 469 | Batch: 009 / 011 | Total loss: 0.786 | Reg loss: 0.028 | Tree loss: 0.786 | Accuracy: 0.712000 | 0.506 sec/iter\n",
      "Epoch: 469 | Batch: 010 / 011 | Total loss: 0.796 | Reg loss: 0.028 | Tree loss: 0.796 | Accuracy: 0.713311 | 0.506 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 470 | Batch: 000 / 011 | Total loss: 0.885 | Reg loss: 0.028 | Tree loss: 0.885 | Accuracy: 0.662500 | 0.506 sec/iter\n",
      "Epoch: 470 | Batch: 001 / 011 | Total loss: 0.845 | Reg loss: 0.028 | Tree loss: 0.845 | Accuracy: 0.673000 | 0.506 sec/iter\n",
      "Epoch: 470 | Batch: 002 / 011 | Total loss: 0.859 | Reg loss: 0.028 | Tree loss: 0.859 | Accuracy: 0.688500 | 0.506 sec/iter\n",
      "Epoch: 470 | Batch: 003 / 011 | Total loss: 0.798 | Reg loss: 0.028 | Tree loss: 0.798 | Accuracy: 0.714000 | 0.506 sec/iter\n",
      "Epoch: 470 | Batch: 004 / 011 | Total loss: 0.815 | Reg loss: 0.028 | Tree loss: 0.815 | Accuracy: 0.704500 | 0.506 sec/iter\n",
      "Epoch: 470 | Batch: 005 / 011 | Total loss: 0.793 | Reg loss: 0.028 | Tree loss: 0.793 | Accuracy: 0.721500 | 0.506 sec/iter\n",
      "Epoch: 470 | Batch: 006 / 011 | Total loss: 0.780 | Reg loss: 0.028 | Tree loss: 0.780 | Accuracy: 0.722000 | 0.506 sec/iter\n",
      "Epoch: 470 | Batch: 007 / 011 | Total loss: 0.786 | Reg loss: 0.028 | Tree loss: 0.786 | Accuracy: 0.722000 | 0.506 sec/iter\n",
      "Epoch: 470 | Batch: 008 / 011 | Total loss: 0.794 | Reg loss: 0.028 | Tree loss: 0.794 | Accuracy: 0.719500 | 0.506 sec/iter\n",
      "Epoch: 470 | Batch: 009 / 011 | Total loss: 0.771 | Reg loss: 0.028 | Tree loss: 0.771 | Accuracy: 0.737500 | 0.505 sec/iter\n",
      "Epoch: 470 | Batch: 010 / 011 | Total loss: 0.826 | Reg loss: 0.028 | Tree loss: 0.826 | Accuracy: 0.713311 | 0.505 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 471 | Batch: 000 / 011 | Total loss: 0.874 | Reg loss: 0.028 | Tree loss: 0.874 | Accuracy: 0.666000 | 0.506 sec/iter\n",
      "Epoch: 471 | Batch: 001 / 011 | Total loss: 0.881 | Reg loss: 0.028 | Tree loss: 0.881 | Accuracy: 0.656500 | 0.506 sec/iter\n",
      "Epoch: 471 | Batch: 002 / 011 | Total loss: 0.846 | Reg loss: 0.028 | Tree loss: 0.846 | Accuracy: 0.692500 | 0.506 sec/iter\n",
      "Epoch: 471 | Batch: 003 / 011 | Total loss: 0.810 | Reg loss: 0.028 | Tree loss: 0.810 | Accuracy: 0.696500 | 0.506 sec/iter\n",
      "Epoch: 471 | Batch: 004 / 011 | Total loss: 0.809 | Reg loss: 0.028 | Tree loss: 0.809 | Accuracy: 0.693500 | 0.506 sec/iter\n",
      "Epoch: 471 | Batch: 005 / 011 | Total loss: 0.768 | Reg loss: 0.028 | Tree loss: 0.768 | Accuracy: 0.722000 | 0.505 sec/iter\n",
      "Epoch: 471 | Batch: 006 / 011 | Total loss: 0.788 | Reg loss: 0.028 | Tree loss: 0.788 | Accuracy: 0.740500 | 0.505 sec/iter\n",
      "Epoch: 471 | Batch: 007 / 011 | Total loss: 0.776 | Reg loss: 0.028 | Tree loss: 0.776 | Accuracy: 0.726500 | 0.505 sec/iter\n",
      "Epoch: 471 | Batch: 008 / 011 | Total loss: 0.786 | Reg loss: 0.028 | Tree loss: 0.786 | Accuracy: 0.742000 | 0.505 sec/iter\n",
      "Epoch: 471 | Batch: 009 / 011 | Total loss: 0.784 | Reg loss: 0.028 | Tree loss: 0.784 | Accuracy: 0.731500 | 0.505 sec/iter\n",
      "Epoch: 471 | Batch: 010 / 011 | Total loss: 0.754 | Reg loss: 0.028 | Tree loss: 0.754 | Accuracy: 0.716724 | 0.505 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 472 | Batch: 000 / 011 | Total loss: 0.885 | Reg loss: 0.028 | Tree loss: 0.885 | Accuracy: 0.654000 | 0.506 sec/iter\n",
      "Epoch: 472 | Batch: 001 / 011 | Total loss: 0.865 | Reg loss: 0.028 | Tree loss: 0.865 | Accuracy: 0.671500 | 0.506 sec/iter\n",
      "Epoch: 472 | Batch: 002 / 011 | Total loss: 0.832 | Reg loss: 0.028 | Tree loss: 0.832 | Accuracy: 0.688000 | 0.506 sec/iter\n",
      "Epoch: 472 | Batch: 003 / 011 | Total loss: 0.831 | Reg loss: 0.028 | Tree loss: 0.831 | Accuracy: 0.686000 | 0.506 sec/iter\n",
      "Epoch: 472 | Batch: 004 / 011 | Total loss: 0.787 | Reg loss: 0.028 | Tree loss: 0.787 | Accuracy: 0.716500 | 0.506 sec/iter\n",
      "Epoch: 472 | Batch: 005 / 011 | Total loss: 0.769 | Reg loss: 0.028 | Tree loss: 0.769 | Accuracy: 0.728000 | 0.506 sec/iter\n",
      "Epoch: 472 | Batch: 006 / 011 | Total loss: 0.756 | Reg loss: 0.028 | Tree loss: 0.756 | Accuracy: 0.740000 | 0.506 sec/iter\n",
      "Epoch: 472 | Batch: 007 / 011 | Total loss: 0.800 | Reg loss: 0.028 | Tree loss: 0.800 | Accuracy: 0.720500 | 0.506 sec/iter\n",
      "Epoch: 472 | Batch: 008 / 011 | Total loss: 0.782 | Reg loss: 0.028 | Tree loss: 0.782 | Accuracy: 0.734000 | 0.506 sec/iter\n",
      "Epoch: 472 | Batch: 009 / 011 | Total loss: 0.791 | Reg loss: 0.028 | Tree loss: 0.791 | Accuracy: 0.722500 | 0.506 sec/iter\n",
      "Epoch: 472 | Batch: 010 / 011 | Total loss: 0.810 | Reg loss: 0.028 | Tree loss: 0.810 | Accuracy: 0.689420 | 0.506 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 473 | Batch: 000 / 011 | Total loss: 0.864 | Reg loss: 0.028 | Tree loss: 0.864 | Accuracy: 0.673500 | 0.506 sec/iter\n",
      "Epoch: 473 | Batch: 001 / 011 | Total loss: 0.855 | Reg loss: 0.028 | Tree loss: 0.855 | Accuracy: 0.668000 | 0.506 sec/iter\n",
      "Epoch: 473 | Batch: 002 / 011 | Total loss: 0.860 | Reg loss: 0.028 | Tree loss: 0.860 | Accuracy: 0.669000 | 0.506 sec/iter\n",
      "Epoch: 473 | Batch: 003 / 011 | Total loss: 0.825 | Reg loss: 0.028 | Tree loss: 0.825 | Accuracy: 0.698000 | 0.506 sec/iter\n",
      "Epoch: 473 | Batch: 004 / 011 | Total loss: 0.806 | Reg loss: 0.028 | Tree loss: 0.806 | Accuracy: 0.736500 | 0.505 sec/iter\n",
      "Epoch: 473 | Batch: 005 / 011 | Total loss: 0.781 | Reg loss: 0.028 | Tree loss: 0.781 | Accuracy: 0.729000 | 0.505 sec/iter\n",
      "Epoch: 473 | Batch: 006 / 011 | Total loss: 0.773 | Reg loss: 0.028 | Tree loss: 0.773 | Accuracy: 0.743000 | 0.505 sec/iter\n",
      "Epoch: 473 | Batch: 007 / 011 | Total loss: 0.802 | Reg loss: 0.028 | Tree loss: 0.802 | Accuracy: 0.720000 | 0.505 sec/iter\n",
      "Epoch: 473 | Batch: 008 / 011 | Total loss: 0.782 | Reg loss: 0.028 | Tree loss: 0.782 | Accuracy: 0.735000 | 0.505 sec/iter\n",
      "Epoch: 473 | Batch: 009 / 011 | Total loss: 0.782 | Reg loss: 0.028 | Tree loss: 0.782 | Accuracy: 0.739500 | 0.505 sec/iter\n",
      "Epoch: 473 | Batch: 010 / 011 | Total loss: 0.789 | Reg loss: 0.028 | Tree loss: 0.789 | Accuracy: 0.720137 | 0.505 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 474 | Batch: 000 / 011 | Total loss: 0.886 | Reg loss: 0.028 | Tree loss: 0.886 | Accuracy: 0.647000 | 0.506 sec/iter\n",
      "Epoch: 474 | Batch: 001 / 011 | Total loss: 0.863 | Reg loss: 0.028 | Tree loss: 0.863 | Accuracy: 0.666500 | 0.506 sec/iter\n",
      "Epoch: 474 | Batch: 002 / 011 | Total loss: 0.814 | Reg loss: 0.028 | Tree loss: 0.814 | Accuracy: 0.702500 | 0.506 sec/iter\n",
      "Epoch: 474 | Batch: 003 / 011 | Total loss: 0.836 | Reg loss: 0.028 | Tree loss: 0.836 | Accuracy: 0.682000 | 0.506 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 474 | Batch: 004 / 011 | Total loss: 0.789 | Reg loss: 0.028 | Tree loss: 0.789 | Accuracy: 0.727000 | 0.506 sec/iter\n",
      "Epoch: 474 | Batch: 005 / 011 | Total loss: 0.785 | Reg loss: 0.028 | Tree loss: 0.785 | Accuracy: 0.701000 | 0.506 sec/iter\n",
      "Epoch: 474 | Batch: 006 / 011 | Total loss: 0.786 | Reg loss: 0.028 | Tree loss: 0.786 | Accuracy: 0.717000 | 0.506 sec/iter\n",
      "Epoch: 474 | Batch: 007 / 011 | Total loss: 0.779 | Reg loss: 0.028 | Tree loss: 0.779 | Accuracy: 0.738500 | 0.506 sec/iter\n",
      "Epoch: 474 | Batch: 008 / 011 | Total loss: 0.799 | Reg loss: 0.028 | Tree loss: 0.799 | Accuracy: 0.707000 | 0.506 sec/iter\n",
      "Epoch: 474 | Batch: 009 / 011 | Total loss: 0.787 | Reg loss: 0.028 | Tree loss: 0.787 | Accuracy: 0.736000 | 0.506 sec/iter\n",
      "Epoch: 474 | Batch: 010 / 011 | Total loss: 0.780 | Reg loss: 0.028 | Tree loss: 0.780 | Accuracy: 0.720137 | 0.506 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 475 | Batch: 000 / 011 | Total loss: 0.878 | Reg loss: 0.028 | Tree loss: 0.878 | Accuracy: 0.658500 | 0.506 sec/iter\n",
      "Epoch: 475 | Batch: 001 / 011 | Total loss: 0.867 | Reg loss: 0.028 | Tree loss: 0.867 | Accuracy: 0.671500 | 0.506 sec/iter\n",
      "Epoch: 475 | Batch: 002 / 011 | Total loss: 0.826 | Reg loss: 0.028 | Tree loss: 0.826 | Accuracy: 0.691500 | 0.506 sec/iter\n",
      "Epoch: 475 | Batch: 003 / 011 | Total loss: 0.821 | Reg loss: 0.028 | Tree loss: 0.821 | Accuracy: 0.710500 | 0.506 sec/iter\n",
      "Epoch: 475 | Batch: 004 / 011 | Total loss: 0.784 | Reg loss: 0.028 | Tree loss: 0.784 | Accuracy: 0.733500 | 0.506 sec/iter\n",
      "Epoch: 475 | Batch: 005 / 011 | Total loss: 0.799 | Reg loss: 0.028 | Tree loss: 0.799 | Accuracy: 0.729000 | 0.506 sec/iter\n",
      "Epoch: 475 | Batch: 006 / 011 | Total loss: 0.793 | Reg loss: 0.028 | Tree loss: 0.793 | Accuracy: 0.734500 | 0.506 sec/iter\n",
      "Epoch: 475 | Batch: 007 / 011 | Total loss: 0.780 | Reg loss: 0.028 | Tree loss: 0.780 | Accuracy: 0.725500 | 0.506 sec/iter\n",
      "Epoch: 475 | Batch: 008 / 011 | Total loss: 0.781 | Reg loss: 0.028 | Tree loss: 0.781 | Accuracy: 0.737000 | 0.506 sec/iter\n",
      "Epoch: 475 | Batch: 009 / 011 | Total loss: 0.778 | Reg loss: 0.028 | Tree loss: 0.778 | Accuracy: 0.734500 | 0.506 sec/iter\n",
      "Epoch: 475 | Batch: 010 / 011 | Total loss: 0.746 | Reg loss: 0.028 | Tree loss: 0.746 | Accuracy: 0.750853 | 0.506 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 476 | Batch: 000 / 011 | Total loss: 0.860 | Reg loss: 0.028 | Tree loss: 0.860 | Accuracy: 0.668000 | 0.506 sec/iter\n",
      "Epoch: 476 | Batch: 001 / 011 | Total loss: 0.864 | Reg loss: 0.028 | Tree loss: 0.864 | Accuracy: 0.668500 | 0.506 sec/iter\n",
      "Epoch: 476 | Batch: 002 / 011 | Total loss: 0.867 | Reg loss: 0.028 | Tree loss: 0.867 | Accuracy: 0.683000 | 0.506 sec/iter\n",
      "Epoch: 476 | Batch: 003 / 011 | Total loss: 0.787 | Reg loss: 0.028 | Tree loss: 0.787 | Accuracy: 0.724000 | 0.506 sec/iter\n",
      "Epoch: 476 | Batch: 004 / 011 | Total loss: 0.786 | Reg loss: 0.028 | Tree loss: 0.786 | Accuracy: 0.720000 | 0.506 sec/iter\n",
      "Epoch: 476 | Batch: 005 / 011 | Total loss: 0.799 | Reg loss: 0.028 | Tree loss: 0.799 | Accuracy: 0.713500 | 0.506 sec/iter\n",
      "Epoch: 476 | Batch: 006 / 011 | Total loss: 0.787 | Reg loss: 0.028 | Tree loss: 0.787 | Accuracy: 0.715000 | 0.506 sec/iter\n",
      "Epoch: 476 | Batch: 007 / 011 | Total loss: 0.791 | Reg loss: 0.028 | Tree loss: 0.791 | Accuracy: 0.719500 | 0.506 sec/iter\n",
      "Epoch: 476 | Batch: 008 / 011 | Total loss: 0.795 | Reg loss: 0.028 | Tree loss: 0.795 | Accuracy: 0.709500 | 0.506 sec/iter\n",
      "Epoch: 476 | Batch: 009 / 011 | Total loss: 0.780 | Reg loss: 0.028 | Tree loss: 0.780 | Accuracy: 0.733500 | 0.506 sec/iter\n",
      "Epoch: 476 | Batch: 010 / 011 | Total loss: 0.787 | Reg loss: 0.028 | Tree loss: 0.787 | Accuracy: 0.744027 | 0.506 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 477 | Batch: 000 / 011 | Total loss: 0.853 | Reg loss: 0.028 | Tree loss: 0.853 | Accuracy: 0.680500 | 0.506 sec/iter\n",
      "Epoch: 477 | Batch: 001 / 011 | Total loss: 0.845 | Reg loss: 0.028 | Tree loss: 0.845 | Accuracy: 0.675500 | 0.506 sec/iter\n",
      "Epoch: 477 | Batch: 002 / 011 | Total loss: 0.844 | Reg loss: 0.028 | Tree loss: 0.844 | Accuracy: 0.684000 | 0.506 sec/iter\n",
      "Epoch: 477 | Batch: 003 / 011 | Total loss: 0.804 | Reg loss: 0.028 | Tree loss: 0.804 | Accuracy: 0.716500 | 0.506 sec/iter\n",
      "Epoch: 477 | Batch: 004 / 011 | Total loss: 0.805 | Reg loss: 0.028 | Tree loss: 0.805 | Accuracy: 0.712500 | 0.506 sec/iter\n",
      "Epoch: 477 | Batch: 005 / 011 | Total loss: 0.822 | Reg loss: 0.028 | Tree loss: 0.822 | Accuracy: 0.715000 | 0.506 sec/iter\n",
      "Epoch: 477 | Batch: 006 / 011 | Total loss: 0.783 | Reg loss: 0.028 | Tree loss: 0.783 | Accuracy: 0.734500 | 0.506 sec/iter\n",
      "Epoch: 477 | Batch: 007 / 011 | Total loss: 0.782 | Reg loss: 0.028 | Tree loss: 0.782 | Accuracy: 0.734000 | 0.506 sec/iter\n",
      "Epoch: 477 | Batch: 008 / 011 | Total loss: 0.798 | Reg loss: 0.028 | Tree loss: 0.798 | Accuracy: 0.726500 | 0.506 sec/iter\n",
      "Epoch: 477 | Batch: 009 / 011 | Total loss: 0.769 | Reg loss: 0.028 | Tree loss: 0.769 | Accuracy: 0.739000 | 0.506 sec/iter\n",
      "Epoch: 477 | Batch: 010 / 011 | Total loss: 0.768 | Reg loss: 0.028 | Tree loss: 0.768 | Accuracy: 0.747440 | 0.506 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 478 | Batch: 000 / 011 | Total loss: 0.875 | Reg loss: 0.028 | Tree loss: 0.875 | Accuracy: 0.673500 | 0.507 sec/iter\n",
      "Epoch: 478 | Batch: 001 / 011 | Total loss: 0.845 | Reg loss: 0.028 | Tree loss: 0.845 | Accuracy: 0.681000 | 0.507 sec/iter\n",
      "Epoch: 478 | Batch: 002 / 011 | Total loss: 0.809 | Reg loss: 0.028 | Tree loss: 0.809 | Accuracy: 0.705500 | 0.507 sec/iter\n",
      "Epoch: 478 | Batch: 003 / 011 | Total loss: 0.823 | Reg loss: 0.028 | Tree loss: 0.823 | Accuracy: 0.701500 | 0.507 sec/iter\n",
      "Epoch: 478 | Batch: 004 / 011 | Total loss: 0.802 | Reg loss: 0.028 | Tree loss: 0.802 | Accuracy: 0.710000 | 0.507 sec/iter\n",
      "Epoch: 478 | Batch: 005 / 011 | Total loss: 0.798 | Reg loss: 0.028 | Tree loss: 0.798 | Accuracy: 0.704500 | 0.507 sec/iter\n",
      "Epoch: 478 | Batch: 006 / 011 | Total loss: 0.787 | Reg loss: 0.028 | Tree loss: 0.787 | Accuracy: 0.715500 | 0.507 sec/iter\n",
      "Epoch: 478 | Batch: 007 / 011 | Total loss: 0.791 | Reg loss: 0.028 | Tree loss: 0.791 | Accuracy: 0.721000 | 0.507 sec/iter\n",
      "Epoch: 478 | Batch: 008 / 011 | Total loss: 0.807 | Reg loss: 0.028 | Tree loss: 0.807 | Accuracy: 0.711500 | 0.507 sec/iter\n",
      "Epoch: 478 | Batch: 009 / 011 | Total loss: 0.778 | Reg loss: 0.028 | Tree loss: 0.778 | Accuracy: 0.732500 | 0.506 sec/iter\n",
      "Epoch: 478 | Batch: 010 / 011 | Total loss: 0.830 | Reg loss: 0.028 | Tree loss: 0.830 | Accuracy: 0.682594 | 0.506 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 479 | Batch: 000 / 011 | Total loss: 0.866 | Reg loss: 0.028 | Tree loss: 0.866 | Accuracy: 0.666000 | 0.507 sec/iter\n",
      "Epoch: 479 | Batch: 001 / 011 | Total loss: 0.865 | Reg loss: 0.028 | Tree loss: 0.865 | Accuracy: 0.657000 | 0.507 sec/iter\n",
      "Epoch: 479 | Batch: 002 / 011 | Total loss: 0.861 | Reg loss: 0.028 | Tree loss: 0.861 | Accuracy: 0.675500 | 0.507 sec/iter\n",
      "Epoch: 479 | Batch: 003 / 011 | Total loss: 0.830 | Reg loss: 0.028 | Tree loss: 0.830 | Accuracy: 0.688000 | 0.507 sec/iter\n",
      "Epoch: 479 | Batch: 004 / 011 | Total loss: 0.790 | Reg loss: 0.028 | Tree loss: 0.790 | Accuracy: 0.723500 | 0.507 sec/iter\n",
      "Epoch: 479 | Batch: 005 / 011 | Total loss: 0.783 | Reg loss: 0.028 | Tree loss: 0.783 | Accuracy: 0.721500 | 0.507 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 479 | Batch: 006 / 011 | Total loss: 0.785 | Reg loss: 0.028 | Tree loss: 0.785 | Accuracy: 0.728000 | 0.507 sec/iter\n",
      "Epoch: 479 | Batch: 007 / 011 | Total loss: 0.763 | Reg loss: 0.028 | Tree loss: 0.763 | Accuracy: 0.754500 | 0.507 sec/iter\n",
      "Epoch: 479 | Batch: 008 / 011 | Total loss: 0.791 | Reg loss: 0.028 | Tree loss: 0.791 | Accuracy: 0.730000 | 0.507 sec/iter\n",
      "Epoch: 479 | Batch: 009 / 011 | Total loss: 0.798 | Reg loss: 0.028 | Tree loss: 0.798 | Accuracy: 0.715500 | 0.507 sec/iter\n",
      "Epoch: 479 | Batch: 010 / 011 | Total loss: 0.717 | Reg loss: 0.028 | Tree loss: 0.717 | Accuracy: 0.761092 | 0.507 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 480 | Batch: 000 / 011 | Total loss: 0.862 | Reg loss: 0.028 | Tree loss: 0.862 | Accuracy: 0.664500 | 0.507 sec/iter\n",
      "Epoch: 480 | Batch: 001 / 011 | Total loss: 0.863 | Reg loss: 0.028 | Tree loss: 0.863 | Accuracy: 0.673000 | 0.507 sec/iter\n",
      "Epoch: 480 | Batch: 002 / 011 | Total loss: 0.826 | Reg loss: 0.028 | Tree loss: 0.826 | Accuracy: 0.702500 | 0.507 sec/iter\n",
      "Epoch: 480 | Batch: 003 / 011 | Total loss: 0.807 | Reg loss: 0.028 | Tree loss: 0.807 | Accuracy: 0.714500 | 0.507 sec/iter\n",
      "Epoch: 480 | Batch: 004 / 011 | Total loss: 0.803 | Reg loss: 0.028 | Tree loss: 0.803 | Accuracy: 0.713500 | 0.507 sec/iter\n",
      "Epoch: 480 | Batch: 005 / 011 | Total loss: 0.768 | Reg loss: 0.028 | Tree loss: 0.768 | Accuracy: 0.715500 | 0.507 sec/iter\n",
      "Epoch: 480 | Batch: 006 / 011 | Total loss: 0.775 | Reg loss: 0.028 | Tree loss: 0.775 | Accuracy: 0.723500 | 0.507 sec/iter\n",
      "Epoch: 480 | Batch: 007 / 011 | Total loss: 0.789 | Reg loss: 0.028 | Tree loss: 0.789 | Accuracy: 0.721000 | 0.507 sec/iter\n",
      "Epoch: 480 | Batch: 008 / 011 | Total loss: 0.811 | Reg loss: 0.028 | Tree loss: 0.811 | Accuracy: 0.710500 | 0.507 sec/iter\n",
      "Epoch: 480 | Batch: 009 / 011 | Total loss: 0.812 | Reg loss: 0.028 | Tree loss: 0.812 | Accuracy: 0.718000 | 0.507 sec/iter\n",
      "Epoch: 480 | Batch: 010 / 011 | Total loss: 0.811 | Reg loss: 0.028 | Tree loss: 0.811 | Accuracy: 0.730375 | 0.506 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 481 | Batch: 000 / 011 | Total loss: 0.864 | Reg loss: 0.028 | Tree loss: 0.864 | Accuracy: 0.664000 | 0.507 sec/iter\n",
      "Epoch: 481 | Batch: 001 / 011 | Total loss: 0.868 | Reg loss: 0.028 | Tree loss: 0.868 | Accuracy: 0.654000 | 0.507 sec/iter\n",
      "Epoch: 481 | Batch: 002 / 011 | Total loss: 0.823 | Reg loss: 0.028 | Tree loss: 0.823 | Accuracy: 0.706000 | 0.507 sec/iter\n",
      "Epoch: 481 | Batch: 003 / 011 | Total loss: 0.826 | Reg loss: 0.028 | Tree loss: 0.826 | Accuracy: 0.680500 | 0.507 sec/iter\n",
      "Epoch: 481 | Batch: 004 / 011 | Total loss: 0.830 | Reg loss: 0.028 | Tree loss: 0.830 | Accuracy: 0.696500 | 0.507 sec/iter\n",
      "Epoch: 481 | Batch: 005 / 011 | Total loss: 0.807 | Reg loss: 0.028 | Tree loss: 0.807 | Accuracy: 0.700000 | 0.507 sec/iter\n",
      "Epoch: 481 | Batch: 006 / 011 | Total loss: 0.782 | Reg loss: 0.028 | Tree loss: 0.782 | Accuracy: 0.734500 | 0.507 sec/iter\n",
      "Epoch: 481 | Batch: 007 / 011 | Total loss: 0.779 | Reg loss: 0.028 | Tree loss: 0.779 | Accuracy: 0.740000 | 0.507 sec/iter\n",
      "Epoch: 481 | Batch: 008 / 011 | Total loss: 0.779 | Reg loss: 0.028 | Tree loss: 0.779 | Accuracy: 0.732000 | 0.506 sec/iter\n",
      "Epoch: 481 | Batch: 009 / 011 | Total loss: 0.766 | Reg loss: 0.028 | Tree loss: 0.766 | Accuracy: 0.735000 | 0.506 sec/iter\n",
      "Epoch: 481 | Batch: 010 / 011 | Total loss: 0.769 | Reg loss: 0.028 | Tree loss: 0.769 | Accuracy: 0.713311 | 0.506 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 482 | Batch: 000 / 011 | Total loss: 0.864 | Reg loss: 0.028 | Tree loss: 0.864 | Accuracy: 0.674500 | 0.507 sec/iter\n",
      "Epoch: 482 | Batch: 001 / 011 | Total loss: 0.873 | Reg loss: 0.028 | Tree loss: 0.873 | Accuracy: 0.659000 | 0.507 sec/iter\n",
      "Epoch: 482 | Batch: 002 / 011 | Total loss: 0.848 | Reg loss: 0.028 | Tree loss: 0.848 | Accuracy: 0.687500 | 0.507 sec/iter\n",
      "Epoch: 482 | Batch: 003 / 011 | Total loss: 0.838 | Reg loss: 0.028 | Tree loss: 0.838 | Accuracy: 0.687500 | 0.507 sec/iter\n",
      "Epoch: 482 | Batch: 004 / 011 | Total loss: 0.788 | Reg loss: 0.028 | Tree loss: 0.788 | Accuracy: 0.709000 | 0.507 sec/iter\n",
      "Epoch: 482 | Batch: 005 / 011 | Total loss: 0.798 | Reg loss: 0.028 | Tree loss: 0.798 | Accuracy: 0.713000 | 0.507 sec/iter\n",
      "Epoch: 482 | Batch: 006 / 011 | Total loss: 0.758 | Reg loss: 0.028 | Tree loss: 0.758 | Accuracy: 0.742000 | 0.506 sec/iter\n",
      "Epoch: 482 | Batch: 007 / 011 | Total loss: 0.775 | Reg loss: 0.028 | Tree loss: 0.775 | Accuracy: 0.720000 | 0.506 sec/iter\n",
      "Epoch: 482 | Batch: 008 / 011 | Total loss: 0.815 | Reg loss: 0.028 | Tree loss: 0.815 | Accuracy: 0.700500 | 0.506 sec/iter\n",
      "Epoch: 482 | Batch: 009 / 011 | Total loss: 0.793 | Reg loss: 0.028 | Tree loss: 0.793 | Accuracy: 0.713500 | 0.506 sec/iter\n",
      "Epoch: 482 | Batch: 010 / 011 | Total loss: 0.766 | Reg loss: 0.028 | Tree loss: 0.766 | Accuracy: 0.754266 | 0.506 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 483 | Batch: 000 / 011 | Total loss: 0.862 | Reg loss: 0.028 | Tree loss: 0.862 | Accuracy: 0.665500 | 0.506 sec/iter\n",
      "Epoch: 483 | Batch: 001 / 011 | Total loss: 0.864 | Reg loss: 0.028 | Tree loss: 0.864 | Accuracy: 0.664500 | 0.506 sec/iter\n",
      "Epoch: 483 | Batch: 002 / 011 | Total loss: 0.847 | Reg loss: 0.028 | Tree loss: 0.847 | Accuracy: 0.683500 | 0.506 sec/iter\n",
      "Epoch: 483 | Batch: 003 / 011 | Total loss: 0.811 | Reg loss: 0.028 | Tree loss: 0.811 | Accuracy: 0.696000 | 0.506 sec/iter\n",
      "Epoch: 483 | Batch: 004 / 011 | Total loss: 0.802 | Reg loss: 0.028 | Tree loss: 0.802 | Accuracy: 0.725000 | 0.506 sec/iter\n",
      "Epoch: 483 | Batch: 005 / 011 | Total loss: 0.784 | Reg loss: 0.028 | Tree loss: 0.784 | Accuracy: 0.733000 | 0.506 sec/iter\n",
      "Epoch: 483 | Batch: 006 / 011 | Total loss: 0.779 | Reg loss: 0.028 | Tree loss: 0.779 | Accuracy: 0.743000 | 0.506 sec/iter\n",
      "Epoch: 483 | Batch: 007 / 011 | Total loss: 0.781 | Reg loss: 0.028 | Tree loss: 0.781 | Accuracy: 0.725500 | 0.506 sec/iter\n",
      "Epoch: 483 | Batch: 008 / 011 | Total loss: 0.800 | Reg loss: 0.028 | Tree loss: 0.800 | Accuracy: 0.724000 | 0.506 sec/iter\n",
      "Epoch: 483 | Batch: 009 / 011 | Total loss: 0.797 | Reg loss: 0.028 | Tree loss: 0.797 | Accuracy: 0.720500 | 0.506 sec/iter\n",
      "Epoch: 483 | Batch: 010 / 011 | Total loss: 0.789 | Reg loss: 0.028 | Tree loss: 0.789 | Accuracy: 0.692833 | 0.506 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 484 | Batch: 000 / 011 | Total loss: 0.860 | Reg loss: 0.028 | Tree loss: 0.860 | Accuracy: 0.672000 | 0.506 sec/iter\n",
      "Epoch: 484 | Batch: 001 / 011 | Total loss: 0.852 | Reg loss: 0.028 | Tree loss: 0.852 | Accuracy: 0.681500 | 0.506 sec/iter\n",
      "Epoch: 484 | Batch: 002 / 011 | Total loss: 0.841 | Reg loss: 0.028 | Tree loss: 0.841 | Accuracy: 0.679000 | 0.506 sec/iter\n",
      "Epoch: 484 | Batch: 003 / 011 | Total loss: 0.814 | Reg loss: 0.028 | Tree loss: 0.814 | Accuracy: 0.701500 | 0.506 sec/iter\n",
      "Epoch: 484 | Batch: 004 / 011 | Total loss: 0.814 | Reg loss: 0.028 | Tree loss: 0.814 | Accuracy: 0.696000 | 0.506 sec/iter\n",
      "Epoch: 484 | Batch: 005 / 011 | Total loss: 0.796 | Reg loss: 0.028 | Tree loss: 0.796 | Accuracy: 0.707000 | 0.506 sec/iter\n",
      "Epoch: 484 | Batch: 006 / 011 | Total loss: 0.785 | Reg loss: 0.028 | Tree loss: 0.785 | Accuracy: 0.711000 | 0.506 sec/iter\n",
      "Epoch: 484 | Batch: 007 / 011 | Total loss: 0.771 | Reg loss: 0.028 | Tree loss: 0.771 | Accuracy: 0.736500 | 0.506 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 484 | Batch: 008 / 011 | Total loss: 0.801 | Reg loss: 0.028 | Tree loss: 0.801 | Accuracy: 0.720500 | 0.506 sec/iter\n",
      "Epoch: 484 | Batch: 009 / 011 | Total loss: 0.797 | Reg loss: 0.028 | Tree loss: 0.797 | Accuracy: 0.716000 | 0.506 sec/iter\n",
      "Epoch: 484 | Batch: 010 / 011 | Total loss: 0.742 | Reg loss: 0.028 | Tree loss: 0.742 | Accuracy: 0.723549 | 0.506 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 485 | Batch: 000 / 011 | Total loss: 0.866 | Reg loss: 0.028 | Tree loss: 0.866 | Accuracy: 0.681000 | 0.507 sec/iter\n",
      "Epoch: 485 | Batch: 001 / 011 | Total loss: 0.870 | Reg loss: 0.028 | Tree loss: 0.870 | Accuracy: 0.670500 | 0.506 sec/iter\n",
      "Epoch: 485 | Batch: 002 / 011 | Total loss: 0.813 | Reg loss: 0.028 | Tree loss: 0.813 | Accuracy: 0.703500 | 0.506 sec/iter\n",
      "Epoch: 485 | Batch: 003 / 011 | Total loss: 0.821 | Reg loss: 0.028 | Tree loss: 0.821 | Accuracy: 0.695000 | 0.506 sec/iter\n",
      "Epoch: 485 | Batch: 004 / 011 | Total loss: 0.819 | Reg loss: 0.028 | Tree loss: 0.819 | Accuracy: 0.709500 | 0.506 sec/iter\n",
      "Epoch: 485 | Batch: 005 / 011 | Total loss: 0.797 | Reg loss: 0.028 | Tree loss: 0.797 | Accuracy: 0.701500 | 0.506 sec/iter\n",
      "Epoch: 485 | Batch: 006 / 011 | Total loss: 0.801 | Reg loss: 0.028 | Tree loss: 0.801 | Accuracy: 0.713500 | 0.506 sec/iter\n",
      "Epoch: 485 | Batch: 007 / 011 | Total loss: 0.780 | Reg loss: 0.028 | Tree loss: 0.780 | Accuracy: 0.725000 | 0.506 sec/iter\n",
      "Epoch: 485 | Batch: 008 / 011 | Total loss: 0.773 | Reg loss: 0.028 | Tree loss: 0.773 | Accuracy: 0.738000 | 0.506 sec/iter\n",
      "Epoch: 485 | Batch: 009 / 011 | Total loss: 0.776 | Reg loss: 0.028 | Tree loss: 0.776 | Accuracy: 0.731000 | 0.506 sec/iter\n",
      "Epoch: 485 | Batch: 010 / 011 | Total loss: 0.752 | Reg loss: 0.028 | Tree loss: 0.752 | Accuracy: 0.754266 | 0.506 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 486 | Batch: 000 / 011 | Total loss: 0.868 | Reg loss: 0.028 | Tree loss: 0.868 | Accuracy: 0.664000 | 0.506 sec/iter\n",
      "Epoch: 486 | Batch: 001 / 011 | Total loss: 0.851 | Reg loss: 0.028 | Tree loss: 0.851 | Accuracy: 0.676000 | 0.506 sec/iter\n",
      "Epoch: 486 | Batch: 002 / 011 | Total loss: 0.832 | Reg loss: 0.028 | Tree loss: 0.832 | Accuracy: 0.692000 | 0.506 sec/iter\n",
      "Epoch: 486 | Batch: 003 / 011 | Total loss: 0.821 | Reg loss: 0.028 | Tree loss: 0.821 | Accuracy: 0.699500 | 0.506 sec/iter\n",
      "Epoch: 486 | Batch: 004 / 011 | Total loss: 0.799 | Reg loss: 0.028 | Tree loss: 0.799 | Accuracy: 0.724500 | 0.506 sec/iter\n",
      "Epoch: 486 | Batch: 005 / 011 | Total loss: 0.801 | Reg loss: 0.028 | Tree loss: 0.801 | Accuracy: 0.707500 | 0.506 sec/iter\n",
      "Epoch: 486 | Batch: 006 / 011 | Total loss: 0.795 | Reg loss: 0.028 | Tree loss: 0.795 | Accuracy: 0.711000 | 0.506 sec/iter\n",
      "Epoch: 486 | Batch: 007 / 011 | Total loss: 0.776 | Reg loss: 0.028 | Tree loss: 0.776 | Accuracy: 0.734000 | 0.506 sec/iter\n",
      "Epoch: 486 | Batch: 008 / 011 | Total loss: 0.797 | Reg loss: 0.028 | Tree loss: 0.797 | Accuracy: 0.717500 | 0.506 sec/iter\n",
      "Epoch: 486 | Batch: 009 / 011 | Total loss: 0.785 | Reg loss: 0.028 | Tree loss: 0.785 | Accuracy: 0.735000 | 0.506 sec/iter\n",
      "Epoch: 486 | Batch: 010 / 011 | Total loss: 0.768 | Reg loss: 0.028 | Tree loss: 0.768 | Accuracy: 0.757679 | 0.506 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 487 | Batch: 000 / 011 | Total loss: 0.861 | Reg loss: 0.028 | Tree loss: 0.861 | Accuracy: 0.671500 | 0.506 sec/iter\n",
      "Epoch: 487 | Batch: 001 / 011 | Total loss: 0.861 | Reg loss: 0.028 | Tree loss: 0.861 | Accuracy: 0.665500 | 0.506 sec/iter\n",
      "Epoch: 487 | Batch: 002 / 011 | Total loss: 0.834 | Reg loss: 0.028 | Tree loss: 0.834 | Accuracy: 0.702000 | 0.506 sec/iter\n",
      "Epoch: 487 | Batch: 003 / 011 | Total loss: 0.819 | Reg loss: 0.028 | Tree loss: 0.819 | Accuracy: 0.691500 | 0.506 sec/iter\n",
      "Epoch: 487 | Batch: 004 / 011 | Total loss: 0.814 | Reg loss: 0.028 | Tree loss: 0.814 | Accuracy: 0.702500 | 0.506 sec/iter\n",
      "Epoch: 487 | Batch: 005 / 011 | Total loss: 0.797 | Reg loss: 0.028 | Tree loss: 0.797 | Accuracy: 0.713000 | 0.506 sec/iter\n",
      "Epoch: 487 | Batch: 006 / 011 | Total loss: 0.771 | Reg loss: 0.028 | Tree loss: 0.771 | Accuracy: 0.728500 | 0.506 sec/iter\n",
      "Epoch: 487 | Batch: 007 / 011 | Total loss: 0.784 | Reg loss: 0.028 | Tree loss: 0.784 | Accuracy: 0.719500 | 0.506 sec/iter\n",
      "Epoch: 487 | Batch: 008 / 011 | Total loss: 0.776 | Reg loss: 0.028 | Tree loss: 0.776 | Accuracy: 0.729000 | 0.506 sec/iter\n",
      "Epoch: 487 | Batch: 009 / 011 | Total loss: 0.814 | Reg loss: 0.028 | Tree loss: 0.814 | Accuracy: 0.719500 | 0.506 sec/iter\n",
      "Epoch: 487 | Batch: 010 / 011 | Total loss: 0.805 | Reg loss: 0.028 | Tree loss: 0.805 | Accuracy: 0.703072 | 0.506 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 488 | Batch: 000 / 011 | Total loss: 0.883 | Reg loss: 0.028 | Tree loss: 0.883 | Accuracy: 0.662000 | 0.506 sec/iter\n",
      "Epoch: 488 | Batch: 001 / 011 | Total loss: 0.845 | Reg loss: 0.028 | Tree loss: 0.845 | Accuracy: 0.689000 | 0.506 sec/iter\n",
      "Epoch: 488 | Batch: 002 / 011 | Total loss: 0.847 | Reg loss: 0.028 | Tree loss: 0.847 | Accuracy: 0.670500 | 0.506 sec/iter\n",
      "Epoch: 488 | Batch: 003 / 011 | Total loss: 0.842 | Reg loss: 0.028 | Tree loss: 0.842 | Accuracy: 0.688500 | 0.506 sec/iter\n",
      "Epoch: 488 | Batch: 004 / 011 | Total loss: 0.806 | Reg loss: 0.028 | Tree loss: 0.806 | Accuracy: 0.714500 | 0.506 sec/iter\n",
      "Epoch: 488 | Batch: 005 / 011 | Total loss: 0.780 | Reg loss: 0.028 | Tree loss: 0.780 | Accuracy: 0.729500 | 0.506 sec/iter\n",
      "Epoch: 488 | Batch: 006 / 011 | Total loss: 0.790 | Reg loss: 0.028 | Tree loss: 0.790 | Accuracy: 0.719000 | 0.506 sec/iter\n",
      "Epoch: 488 | Batch: 007 / 011 | Total loss: 0.787 | Reg loss: 0.028 | Tree loss: 0.787 | Accuracy: 0.724500 | 0.506 sec/iter\n",
      "Epoch: 488 | Batch: 008 / 011 | Total loss: 0.761 | Reg loss: 0.028 | Tree loss: 0.761 | Accuracy: 0.745000 | 0.506 sec/iter\n",
      "Epoch: 488 | Batch: 009 / 011 | Total loss: 0.784 | Reg loss: 0.028 | Tree loss: 0.784 | Accuracy: 0.731500 | 0.506 sec/iter\n",
      "Epoch: 488 | Batch: 010 / 011 | Total loss: 0.753 | Reg loss: 0.028 | Tree loss: 0.753 | Accuracy: 0.737201 | 0.505 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 489 | Batch: 000 / 011 | Total loss: 0.868 | Reg loss: 0.028 | Tree loss: 0.868 | Accuracy: 0.662000 | 0.505 sec/iter\n",
      "Epoch: 489 | Batch: 001 / 011 | Total loss: 0.865 | Reg loss: 0.028 | Tree loss: 0.865 | Accuracy: 0.654000 | 0.505 sec/iter\n",
      "Epoch: 489 | Batch: 002 / 011 | Total loss: 0.831 | Reg loss: 0.028 | Tree loss: 0.831 | Accuracy: 0.697500 | 0.505 sec/iter\n",
      "Epoch: 489 | Batch: 003 / 011 | Total loss: 0.830 | Reg loss: 0.028 | Tree loss: 0.830 | Accuracy: 0.684500 | 0.505 sec/iter\n",
      "Epoch: 489 | Batch: 004 / 011 | Total loss: 0.791 | Reg loss: 0.028 | Tree loss: 0.791 | Accuracy: 0.718500 | 0.505 sec/iter\n",
      "Epoch: 489 | Batch: 005 / 011 | Total loss: 0.792 | Reg loss: 0.028 | Tree loss: 0.792 | Accuracy: 0.715500 | 0.505 sec/iter\n",
      "Epoch: 489 | Batch: 006 / 011 | Total loss: 0.781 | Reg loss: 0.028 | Tree loss: 0.781 | Accuracy: 0.736000 | 0.505 sec/iter\n",
      "Epoch: 489 | Batch: 007 / 011 | Total loss: 0.798 | Reg loss: 0.028 | Tree loss: 0.798 | Accuracy: 0.725500 | 0.505 sec/iter\n",
      "Epoch: 489 | Batch: 008 / 011 | Total loss: 0.783 | Reg loss: 0.028 | Tree loss: 0.783 | Accuracy: 0.740500 | 0.505 sec/iter\n",
      "Epoch: 489 | Batch: 009 / 011 | Total loss: 0.763 | Reg loss: 0.028 | Tree loss: 0.763 | Accuracy: 0.749500 | 0.505 sec/iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 489 | Batch: 010 / 011 | Total loss: 0.789 | Reg loss: 0.028 | Tree loss: 0.789 | Accuracy: 0.703072 | 0.505 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 490 | Batch: 000 / 011 | Total loss: 0.873 | Reg loss: 0.028 | Tree loss: 0.873 | Accuracy: 0.654500 | 0.505 sec/iter\n",
      "Epoch: 490 | Batch: 001 / 011 | Total loss: 0.841 | Reg loss: 0.028 | Tree loss: 0.841 | Accuracy: 0.680000 | 0.505 sec/iter\n",
      "Epoch: 490 | Batch: 002 / 011 | Total loss: 0.830 | Reg loss: 0.028 | Tree loss: 0.830 | Accuracy: 0.696000 | 0.505 sec/iter\n",
      "Epoch: 490 | Batch: 003 / 011 | Total loss: 0.829 | Reg loss: 0.028 | Tree loss: 0.829 | Accuracy: 0.688500 | 0.505 sec/iter\n",
      "Epoch: 490 | Batch: 004 / 011 | Total loss: 0.791 | Reg loss: 0.028 | Tree loss: 0.791 | Accuracy: 0.724000 | 0.505 sec/iter\n",
      "Epoch: 490 | Batch: 005 / 011 | Total loss: 0.805 | Reg loss: 0.028 | Tree loss: 0.805 | Accuracy: 0.711000 | 0.505 sec/iter\n",
      "Epoch: 490 | Batch: 006 / 011 | Total loss: 0.810 | Reg loss: 0.028 | Tree loss: 0.810 | Accuracy: 0.699000 | 0.505 sec/iter\n",
      "Epoch: 490 | Batch: 007 / 011 | Total loss: 0.781 | Reg loss: 0.028 | Tree loss: 0.781 | Accuracy: 0.731500 | 0.505 sec/iter\n",
      "Epoch: 490 | Batch: 008 / 011 | Total loss: 0.782 | Reg loss: 0.028 | Tree loss: 0.782 | Accuracy: 0.720000 | 0.505 sec/iter\n",
      "Epoch: 490 | Batch: 009 / 011 | Total loss: 0.782 | Reg loss: 0.028 | Tree loss: 0.782 | Accuracy: 0.728500 | 0.505 sec/iter\n",
      "Epoch: 490 | Batch: 010 / 011 | Total loss: 0.792 | Reg loss: 0.028 | Tree loss: 0.792 | Accuracy: 0.716724 | 0.505 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 491 | Batch: 000 / 011 | Total loss: 0.866 | Reg loss: 0.028 | Tree loss: 0.866 | Accuracy: 0.670500 | 0.506 sec/iter\n",
      "Epoch: 491 | Batch: 001 / 011 | Total loss: 0.861 | Reg loss: 0.028 | Tree loss: 0.861 | Accuracy: 0.667000 | 0.506 sec/iter\n",
      "Epoch: 491 | Batch: 002 / 011 | Total loss: 0.839 | Reg loss: 0.028 | Tree loss: 0.839 | Accuracy: 0.689500 | 0.506 sec/iter\n",
      "Epoch: 491 | Batch: 003 / 011 | Total loss: 0.833 | Reg loss: 0.028 | Tree loss: 0.833 | Accuracy: 0.690500 | 0.506 sec/iter\n",
      "Epoch: 491 | Batch: 004 / 011 | Total loss: 0.805 | Reg loss: 0.028 | Tree loss: 0.805 | Accuracy: 0.696500 | 0.506 sec/iter\n",
      "Epoch: 491 | Batch: 005 / 011 | Total loss: 0.766 | Reg loss: 0.028 | Tree loss: 0.766 | Accuracy: 0.725500 | 0.506 sec/iter\n",
      "Epoch: 491 | Batch: 006 / 011 | Total loss: 0.804 | Reg loss: 0.028 | Tree loss: 0.804 | Accuracy: 0.714000 | 0.505 sec/iter\n",
      "Epoch: 491 | Batch: 007 / 011 | Total loss: 0.769 | Reg loss: 0.028 | Tree loss: 0.769 | Accuracy: 0.743500 | 0.505 sec/iter\n",
      "Epoch: 491 | Batch: 008 / 011 | Total loss: 0.773 | Reg loss: 0.028 | Tree loss: 0.773 | Accuracy: 0.726000 | 0.505 sec/iter\n",
      "Epoch: 491 | Batch: 009 / 011 | Total loss: 0.784 | Reg loss: 0.028 | Tree loss: 0.784 | Accuracy: 0.732000 | 0.505 sec/iter\n",
      "Epoch: 491 | Batch: 010 / 011 | Total loss: 0.840 | Reg loss: 0.028 | Tree loss: 0.840 | Accuracy: 0.703072 | 0.505 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 492 | Batch: 000 / 011 | Total loss: 0.878 | Reg loss: 0.028 | Tree loss: 0.878 | Accuracy: 0.659500 | 0.505 sec/iter\n",
      "Epoch: 492 | Batch: 001 / 011 | Total loss: 0.857 | Reg loss: 0.028 | Tree loss: 0.857 | Accuracy: 0.669500 | 0.505 sec/iter\n",
      "Epoch: 492 | Batch: 002 / 011 | Total loss: 0.838 | Reg loss: 0.028 | Tree loss: 0.838 | Accuracy: 0.692000 | 0.505 sec/iter\n",
      "Epoch: 492 | Batch: 003 / 011 | Total loss: 0.833 | Reg loss: 0.028 | Tree loss: 0.833 | Accuracy: 0.677000 | 0.505 sec/iter\n",
      "Epoch: 492 | Batch: 004 / 011 | Total loss: 0.782 | Reg loss: 0.028 | Tree loss: 0.782 | Accuracy: 0.712000 | 0.505 sec/iter\n",
      "Epoch: 492 | Batch: 005 / 011 | Total loss: 0.776 | Reg loss: 0.028 | Tree loss: 0.776 | Accuracy: 0.719500 | 0.505 sec/iter\n",
      "Epoch: 492 | Batch: 006 / 011 | Total loss: 0.770 | Reg loss: 0.028 | Tree loss: 0.770 | Accuracy: 0.740000 | 0.505 sec/iter\n",
      "Epoch: 492 | Batch: 007 / 011 | Total loss: 0.771 | Reg loss: 0.028 | Tree loss: 0.771 | Accuracy: 0.726000 | 0.505 sec/iter\n",
      "Epoch: 492 | Batch: 008 / 011 | Total loss: 0.808 | Reg loss: 0.028 | Tree loss: 0.808 | Accuracy: 0.711000 | 0.505 sec/iter\n",
      "Epoch: 492 | Batch: 009 / 011 | Total loss: 0.801 | Reg loss: 0.028 | Tree loss: 0.801 | Accuracy: 0.721000 | 0.505 sec/iter\n",
      "Epoch: 492 | Batch: 010 / 011 | Total loss: 0.782 | Reg loss: 0.028 | Tree loss: 0.782 | Accuracy: 0.754266 | 0.505 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 493 | Batch: 000 / 011 | Total loss: 0.864 | Reg loss: 0.028 | Tree loss: 0.864 | Accuracy: 0.663000 | 0.506 sec/iter\n",
      "Epoch: 493 | Batch: 001 / 011 | Total loss: 0.870 | Reg loss: 0.028 | Tree loss: 0.870 | Accuracy: 0.664500 | 0.506 sec/iter\n",
      "Epoch: 493 | Batch: 002 / 011 | Total loss: 0.829 | Reg loss: 0.028 | Tree loss: 0.829 | Accuracy: 0.687500 | 0.506 sec/iter\n",
      "Epoch: 493 | Batch: 003 / 011 | Total loss: 0.809 | Reg loss: 0.028 | Tree loss: 0.809 | Accuracy: 0.706500 | 0.506 sec/iter\n",
      "Epoch: 493 | Batch: 004 / 011 | Total loss: 0.822 | Reg loss: 0.028 | Tree loss: 0.822 | Accuracy: 0.701500 | 0.506 sec/iter\n",
      "Epoch: 493 | Batch: 005 / 011 | Total loss: 0.786 | Reg loss: 0.028 | Tree loss: 0.786 | Accuracy: 0.717000 | 0.506 sec/iter\n",
      "Epoch: 493 | Batch: 006 / 011 | Total loss: 0.776 | Reg loss: 0.028 | Tree loss: 0.776 | Accuracy: 0.749500 | 0.506 sec/iter\n",
      "Epoch: 493 | Batch: 007 / 011 | Total loss: 0.772 | Reg loss: 0.028 | Tree loss: 0.772 | Accuracy: 0.746000 | 0.506 sec/iter\n",
      "Epoch: 493 | Batch: 008 / 011 | Total loss: 0.791 | Reg loss: 0.028 | Tree loss: 0.791 | Accuracy: 0.726500 | 0.506 sec/iter\n",
      "Epoch: 493 | Batch: 009 / 011 | Total loss: 0.806 | Reg loss: 0.028 | Tree loss: 0.806 | Accuracy: 0.725000 | 0.506 sec/iter\n",
      "Epoch: 493 | Batch: 010 / 011 | Total loss: 0.781 | Reg loss: 0.028 | Tree loss: 0.781 | Accuracy: 0.750853 | 0.506 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 494 | Batch: 000 / 011 | Total loss: 0.848 | Reg loss: 0.028 | Tree loss: 0.848 | Accuracy: 0.675500 | 0.507 sec/iter\n",
      "Epoch: 494 | Batch: 001 / 011 | Total loss: 0.844 | Reg loss: 0.028 | Tree loss: 0.844 | Accuracy: 0.669500 | 0.507 sec/iter\n",
      "Epoch: 494 | Batch: 002 / 011 | Total loss: 0.859 | Reg loss: 0.028 | Tree loss: 0.859 | Accuracy: 0.671000 | 0.507 sec/iter\n",
      "Epoch: 494 | Batch: 003 / 011 | Total loss: 0.857 | Reg loss: 0.028 | Tree loss: 0.857 | Accuracy: 0.675500 | 0.507 sec/iter\n",
      "Epoch: 494 | Batch: 004 / 011 | Total loss: 0.806 | Reg loss: 0.028 | Tree loss: 0.806 | Accuracy: 0.709000 | 0.507 sec/iter\n",
      "Epoch: 494 | Batch: 005 / 011 | Total loss: 0.795 | Reg loss: 0.028 | Tree loss: 0.795 | Accuracy: 0.697000 | 0.507 sec/iter\n",
      "Epoch: 494 | Batch: 006 / 011 | Total loss: 0.775 | Reg loss: 0.028 | Tree loss: 0.775 | Accuracy: 0.730000 | 0.507 sec/iter\n",
      "Epoch: 494 | Batch: 007 / 011 | Total loss: 0.755 | Reg loss: 0.028 | Tree loss: 0.755 | Accuracy: 0.751500 | 0.507 sec/iter\n",
      "Epoch: 494 | Batch: 008 / 011 | Total loss: 0.813 | Reg loss: 0.028 | Tree loss: 0.813 | Accuracy: 0.709500 | 0.507 sec/iter\n",
      "Epoch: 494 | Batch: 009 / 011 | Total loss: 0.774 | Reg loss: 0.028 | Tree loss: 0.774 | Accuracy: 0.743000 | 0.507 sec/iter\n",
      "Epoch: 494 | Batch: 010 / 011 | Total loss: 0.795 | Reg loss: 0.028 | Tree loss: 0.795 | Accuracy: 0.750853 | 0.506 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 495 | Batch: 000 / 011 | Total loss: 0.883 | Reg loss: 0.028 | Tree loss: 0.883 | Accuracy: 0.660000 | 0.507 sec/iter\n",
      "Epoch: 495 | Batch: 001 / 011 | Total loss: 0.853 | Reg loss: 0.028 | Tree loss: 0.853 | Accuracy: 0.672000 | 0.507 sec/iter\n",
      "Epoch: 495 | Batch: 002 / 011 | Total loss: 0.834 | Reg loss: 0.028 | Tree loss: 0.834 | Accuracy: 0.688000 | 0.507 sec/iter\n",
      "Epoch: 495 | Batch: 003 / 011 | Total loss: 0.814 | Reg loss: 0.028 | Tree loss: 0.814 | Accuracy: 0.683500 | 0.507 sec/iter\n",
      "Epoch: 495 | Batch: 004 / 011 | Total loss: 0.805 | Reg loss: 0.028 | Tree loss: 0.805 | Accuracy: 0.689500 | 0.507 sec/iter\n",
      "Epoch: 495 | Batch: 005 / 011 | Total loss: 0.807 | Reg loss: 0.028 | Tree loss: 0.807 | Accuracy: 0.699000 | 0.507 sec/iter\n",
      "Epoch: 495 | Batch: 006 / 011 | Total loss: 0.772 | Reg loss: 0.028 | Tree loss: 0.772 | Accuracy: 0.738500 | 0.507 sec/iter\n",
      "Epoch: 495 | Batch: 007 / 011 | Total loss: 0.776 | Reg loss: 0.028 | Tree loss: 0.776 | Accuracy: 0.734000 | 0.507 sec/iter\n",
      "Epoch: 495 | Batch: 008 / 011 | Total loss: 0.771 | Reg loss: 0.028 | Tree loss: 0.771 | Accuracy: 0.733500 | 0.507 sec/iter\n",
      "Epoch: 495 | Batch: 009 / 011 | Total loss: 0.801 | Reg loss: 0.028 | Tree loss: 0.801 | Accuracy: 0.721000 | 0.507 sec/iter\n",
      "Epoch: 495 | Batch: 010 / 011 | Total loss: 0.780 | Reg loss: 0.028 | Tree loss: 0.780 | Accuracy: 0.761092 | 0.507 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 496 | Batch: 000 / 011 | Total loss: 0.893 | Reg loss: 0.028 | Tree loss: 0.893 | Accuracy: 0.654000 | 0.508 sec/iter\n",
      "Epoch: 496 | Batch: 001 / 011 | Total loss: 0.869 | Reg loss: 0.028 | Tree loss: 0.869 | Accuracy: 0.656500 | 0.508 sec/iter\n",
      "Epoch: 496 | Batch: 002 / 011 | Total loss: 0.834 | Reg loss: 0.028 | Tree loss: 0.834 | Accuracy: 0.684000 | 0.508 sec/iter\n",
      "Epoch: 496 | Batch: 003 / 011 | Total loss: 0.815 | Reg loss: 0.028 | Tree loss: 0.815 | Accuracy: 0.696500 | 0.508 sec/iter\n",
      "Epoch: 496 | Batch: 004 / 011 | Total loss: 0.827 | Reg loss: 0.028 | Tree loss: 0.827 | Accuracy: 0.702500 | 0.508 sec/iter\n",
      "Epoch: 496 | Batch: 005 / 011 | Total loss: 0.796 | Reg loss: 0.028 | Tree loss: 0.796 | Accuracy: 0.711000 | 0.508 sec/iter\n",
      "Epoch: 496 | Batch: 006 / 011 | Total loss: 0.788 | Reg loss: 0.028 | Tree loss: 0.788 | Accuracy: 0.728000 | 0.508 sec/iter\n",
      "Epoch: 496 | Batch: 007 / 011 | Total loss: 0.769 | Reg loss: 0.028 | Tree loss: 0.769 | Accuracy: 0.738000 | 0.508 sec/iter\n",
      "Epoch: 496 | Batch: 008 / 011 | Total loss: 0.787 | Reg loss: 0.028 | Tree loss: 0.787 | Accuracy: 0.726000 | 0.508 sec/iter\n",
      "Epoch: 496 | Batch: 009 / 011 | Total loss: 0.765 | Reg loss: 0.028 | Tree loss: 0.765 | Accuracy: 0.749000 | 0.508 sec/iter\n",
      "Epoch: 496 | Batch: 010 / 011 | Total loss: 0.749 | Reg loss: 0.028 | Tree loss: 0.749 | Accuracy: 0.774744 | 0.508 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 497 | Batch: 000 / 011 | Total loss: 0.888 | Reg loss: 0.028 | Tree loss: 0.888 | Accuracy: 0.658000 | 0.508 sec/iter\n",
      "Epoch: 497 | Batch: 001 / 011 | Total loss: 0.845 | Reg loss: 0.028 | Tree loss: 0.845 | Accuracy: 0.666500 | 0.508 sec/iter\n",
      "Epoch: 497 | Batch: 002 / 011 | Total loss: 0.823 | Reg loss: 0.028 | Tree loss: 0.823 | Accuracy: 0.697500 | 0.508 sec/iter\n",
      "Epoch: 497 | Batch: 003 / 011 | Total loss: 0.816 | Reg loss: 0.028 | Tree loss: 0.816 | Accuracy: 0.685000 | 0.508 sec/iter\n",
      "Epoch: 497 | Batch: 004 / 011 | Total loss: 0.812 | Reg loss: 0.028 | Tree loss: 0.812 | Accuracy: 0.697000 | 0.508 sec/iter\n",
      "Epoch: 497 | Batch: 005 / 011 | Total loss: 0.765 | Reg loss: 0.028 | Tree loss: 0.765 | Accuracy: 0.729500 | 0.508 sec/iter\n",
      "Epoch: 497 | Batch: 006 / 011 | Total loss: 0.801 | Reg loss: 0.028 | Tree loss: 0.801 | Accuracy: 0.713500 | 0.508 sec/iter\n",
      "Epoch: 497 | Batch: 007 / 011 | Total loss: 0.787 | Reg loss: 0.028 | Tree loss: 0.787 | Accuracy: 0.721500 | 0.508 sec/iter\n",
      "Epoch: 497 | Batch: 008 / 011 | Total loss: 0.782 | Reg loss: 0.028 | Tree loss: 0.782 | Accuracy: 0.726000 | 0.508 sec/iter\n",
      "Epoch: 497 | Batch: 009 / 011 | Total loss: 0.779 | Reg loss: 0.028 | Tree loss: 0.779 | Accuracy: 0.739000 | 0.508 sec/iter\n",
      "Epoch: 497 | Batch: 010 / 011 | Total loss: 0.813 | Reg loss: 0.028 | Tree loss: 0.813 | Accuracy: 0.716724 | 0.508 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 498 | Batch: 000 / 011 | Total loss: 0.860 | Reg loss: 0.028 | Tree loss: 0.860 | Accuracy: 0.662000 | 0.508 sec/iter\n",
      "Epoch: 498 | Batch: 001 / 011 | Total loss: 0.849 | Reg loss: 0.028 | Tree loss: 0.849 | Accuracy: 0.679000 | 0.508 sec/iter\n",
      "Epoch: 498 | Batch: 002 / 011 | Total loss: 0.839 | Reg loss: 0.028 | Tree loss: 0.839 | Accuracy: 0.694000 | 0.508 sec/iter\n",
      "Epoch: 498 | Batch: 003 / 011 | Total loss: 0.804 | Reg loss: 0.028 | Tree loss: 0.804 | Accuracy: 0.715000 | 0.508 sec/iter\n",
      "Epoch: 498 | Batch: 004 / 011 | Total loss: 0.816 | Reg loss: 0.028 | Tree loss: 0.816 | Accuracy: 0.711000 | 0.508 sec/iter\n",
      "Epoch: 498 | Batch: 005 / 011 | Total loss: 0.790 | Reg loss: 0.028 | Tree loss: 0.790 | Accuracy: 0.710500 | 0.508 sec/iter\n",
      "Epoch: 498 | Batch: 006 / 011 | Total loss: 0.751 | Reg loss: 0.028 | Tree loss: 0.751 | Accuracy: 0.743000 | 0.508 sec/iter\n",
      "Epoch: 498 | Batch: 007 / 011 | Total loss: 0.798 | Reg loss: 0.028 | Tree loss: 0.798 | Accuracy: 0.728500 | 0.508 sec/iter\n",
      "Epoch: 498 | Batch: 008 / 011 | Total loss: 0.811 | Reg loss: 0.028 | Tree loss: 0.811 | Accuracy: 0.711500 | 0.508 sec/iter\n",
      "Epoch: 498 | Batch: 009 / 011 | Total loss: 0.802 | Reg loss: 0.028 | Tree loss: 0.802 | Accuracy: 0.724500 | 0.508 sec/iter\n",
      "Epoch: 498 | Batch: 010 / 011 | Total loss: 0.725 | Reg loss: 0.028 | Tree loss: 0.725 | Accuracy: 0.795222 | 0.508 sec/iter\n",
      "Average sparseness: 0.9723756906077348\n",
      "layer 0: 0.9723756906077348\n",
      "layer 1: 0.9723756906077348\n",
      "layer 2: 0.9723756906077348\n",
      "layer 3: 0.9723756906077348\n",
      "layer 4: 0.9723756906077348\n",
      "layer 5: 0.9723756906077348\n",
      "layer 6: 0.9723756906077349\n",
      "Epoch: 499 | Batch: 000 / 011 | Total loss: 0.841 | Reg loss: 0.028 | Tree loss: 0.841 | Accuracy: 0.685500 | 0.508 sec/iter\n",
      "Epoch: 499 | Batch: 001 / 011 | Total loss: 0.858 | Reg loss: 0.028 | Tree loss: 0.858 | Accuracy: 0.659500 | 0.508 sec/iter\n",
      "Epoch: 499 | Batch: 002 / 011 | Total loss: 0.823 | Reg loss: 0.028 | Tree loss: 0.823 | Accuracy: 0.690500 | 0.508 sec/iter\n",
      "Epoch: 499 | Batch: 003 / 011 | Total loss: 0.815 | Reg loss: 0.028 | Tree loss: 0.815 | Accuracy: 0.695500 | 0.508 sec/iter\n",
      "Epoch: 499 | Batch: 004 / 011 | Total loss: 0.824 | Reg loss: 0.028 | Tree loss: 0.824 | Accuracy: 0.686500 | 0.508 sec/iter\n",
      "Epoch: 499 | Batch: 005 / 011 | Total loss: 0.798 | Reg loss: 0.028 | Tree loss: 0.798 | Accuracy: 0.711500 | 0.508 sec/iter\n",
      "Epoch: 499 | Batch: 006 / 011 | Total loss: 0.772 | Reg loss: 0.028 | Tree loss: 0.772 | Accuracy: 0.728500 | 0.508 sec/iter\n",
      "Epoch: 499 | Batch: 007 / 011 | Total loss: 0.799 | Reg loss: 0.028 | Tree loss: 0.799 | Accuracy: 0.729500 | 0.508 sec/iter\n",
      "Epoch: 499 | Batch: 008 / 011 | Total loss: 0.793 | Reg loss: 0.028 | Tree loss: 0.793 | Accuracy: 0.719500 | 0.508 sec/iter\n",
      "Epoch: 499 | Batch: 009 / 011 | Total loss: 0.779 | Reg loss: 0.028 | Tree loss: 0.779 | Accuracy: 0.727000 | 0.508 sec/iter\n",
      "Epoch: 499 | Batch: 010 / 011 | Total loss: 0.815 | Reg loss: 0.028 | Tree loss: 0.815 | Accuracy: 0.689420 | 0.508 sec/iter\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "iteration = 0\n",
    "for epoch in range(epochs):\n",
    "    # Training\n",
    "    avg_sp = show_sparseness(tree)\n",
    "    sparsity.append(avg_sp)\n",
    "    iteration = do_epoch(tree, tree_loader, device, log_interval, losses, accs, epoch, iteration)\n",
    "    \n",
    "    if epoch % 1 == 0:\n",
    "        prune_tree(tree, factor=5)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel('Iteration')\n",
    "plt.plot(accs, label='Accuracy vs iteration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel('Iteration')\n",
    "plt.plot(losses, label='Loss vs iteration')\n",
    "plt.yscale(\"log\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "weights = tree.inner_nodes.weight.cpu().detach().numpy().flatten()\n",
    "plt.hist(weights, bins=500)\n",
    "weights_std = np.std(weights)\n",
    "weights_mean = np.mean(weights)\n",
    "plt.axvline(weights_mean + weights_std, color='r')\n",
    "plt.axvline(weights_mean - weights_std, color='r')\n",
    "plt.title(f\"Mean: {weights_mean}   |   STD: {weights_std}\")\n",
    "plt.yscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average height: 6.861111111111111\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1200x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(15, 10), dpi=80)\n",
    "avg_height, root = tree.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accumulate samples in the leaves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of patterns: 36\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of patterns: {len(root.get_leaves())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = 'MLE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eitan.k/miniconda3/envs/rambo/lib/python3.8/site-packages/torch/nn/functional.py:1709: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    }
   ],
   "source": [
    "root.clear_leaves_samples()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (data, target) in enumerate(tree_loader):\n",
    "        root.accumulate_samples(data, method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tighten boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Pattern 1 ==============\n",
      "============== Pattern 2 ==============\n",
      "============== Pattern 3 ==============\n",
      "============== Pattern 4 ==============\n",
      "============== Pattern 5 ==============\n",
      "============== Pattern 6 ==============\n",
      "============== Pattern 7 ==============\n",
      "============== Pattern 8 ==============\n",
      "============== Pattern 9 ==============\n",
      "============== Pattern 10 ==============\n",
      "============== Pattern 11 ==============\n",
      "============== Pattern 12 ==============\n",
      "============== Pattern 13 ==============\n",
      "============== Pattern 14 ==============\n",
      "============== Pattern 15 ==============\n",
      "============== Pattern 16 ==============\n",
      "============== Pattern 17 ==============\n",
      "============== Pattern 18 ==============\n",
      "============== Pattern 19 ==============\n",
      "============== Pattern 20 ==============\n",
      "============== Pattern 21 ==============\n",
      "============== Pattern 22 ==============\n",
      "============== Pattern 23 ==============\n",
      "============== Pattern 24 ==============\n",
      "============== Pattern 25 ==============\n",
      "============== Pattern 26 ==============\n",
      "============== Pattern 27 ==============\n",
      "============== Pattern 28 ==============\n",
      "============== Pattern 29 ==============\n",
      "============== Pattern 30 ==============\n",
      "============== Pattern 31 ==============\n",
      "============== Pattern 32 ==============\n",
      "============== Pattern 33 ==============\n",
      "============== Pattern 34 ==============\n",
      "============== Pattern 35 ==============\n",
      "============== Pattern 36 ==============\n",
      "Average comprehensibility: 61.94444444444444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eitan.k/EntangledExplainableClustering/soft_decision_tree/sdt_model.py:14: RuntimeWarning: divide by zero encountered in log\n",
      "  return np.log(1 / (1 - x))\n"
     ]
    }
   ],
   "source": [
    "signal_names = dataset.dataset.all_signals\n",
    "normalizers = torch.tensor([])\n",
    "attr_names = []\n",
    "for signal_name in signal_names:\n",
    "    attr_names += [f\"T{i}.{signal_name}\" for i in range(sampled.shape[-1])]\n",
    "    sensor_norm = torch.tensor([torch.tensor(dataset.dataset.sensor_maxs[signal_name]) for _ in range(sampled.shape[-1])])\n",
    "    normalizers = torch.cat([normalizers, sensor_norm])\n",
    "    \n",
    "\n",
    "# print(attr_names)\n",
    "leaves = root.get_leaves()\n",
    "sum_comprehensibility = 0\n",
    "\n",
    "for pattern_counter, leaf in enumerate(leaves):\n",
    "    leaf.reset_path()\n",
    "    leaf.tighten_with_accumulated_samples()\n",
    "    conds = leaf.get_path_conditions(attr_names)\n",
    "    for cond in conds:\n",
    "        cond.weights = cond.weights / normalizers\n",
    "    print(f\"============== Pattern {pattern_counter + 1} ==============\")\n",
    "    sum_comprehensibility += sum([cond.comprehensibility for cond in conds])\n",
    "    \n",
    "print(f\"Average comprehensibility: {sum_comprehensibility / len(leaves)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prune_tree(tree, factor=1.5)\n",
    "correct = 0\n",
    "tree = tree.eval()\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (data, target) in enumerate(tree_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = tree.forward(data)\n",
    "            pred = output.data.max(1)[1]\n",
    "            correct += pred.eq(target.view(-1).data).sum()\n",
    "\n",
    "print(f\"Accuracy: {correct / len(tree_loader.dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Sparseness: {sparseness(tree.inner_nodes.weight)}\")\n",
    "layer = 0\n",
    "sps = []\n",
    "for i in range(tree.inner_nodes.weight.shape[0]):\n",
    "    cur_layer = np.floor(np.log2(i+1))\n",
    "    if cur_layer != layer:\n",
    "        print(f\"layer {layer}: {np.mean(sps)}\")\n",
    "        sps = []\n",
    "        layer = cur_layer\n",
    "    \n",
    "    x_ = tree.inner_nodes.weight[i, :]\n",
    "    sp = (len(x_) - torch.norm(x_, 0).item()) / len(x_)\n",
    "    sps.append(sp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prune the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = tree.inner_nodes.weight.cpu().detach().numpy()\n",
    "for i in range(0, weights.shape[0], 20):\n",
    "    plt.figure()\n",
    "    weights_layer = weights[i, :]\n",
    "    plt.hist(weights_layer, bins=500)\n",
    "    weights_std = np.std(weights_layer)\n",
    "    weights_mean = np.mean(weights_layer)\n",
    "    plt.axvline(weights_mean + weights_std, color='r')\n",
    "    plt.axvline(weights_mean - weights_std, color='r')\n",
    "    plt.title(f\"Mean: {weights_mean}   |   STD: {weights_std}\\n Kurtosis: {kurtosis(weights_layer)}\")\n",
    "    plt.yscale(\"log\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify that the accuracy didn't change too much"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "tree_copy = tree_copy.eval()\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (data, target) in enumerate(tree_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = tree_copy.forward(data)\n",
    "            pred = output.data.max(1)[1]\n",
    "            correct += pred.eq(target.view(-1).data).sum()\n",
    "\n",
    "print(f\"Accuracy: {correct / len(tree_loader.dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = tree_copy.inner_nodes.weight.cpu().detach().numpy()\n",
    "for i in range(0, weights.shape[0], 20):\n",
    "    plt.figure()\n",
    "    weights_layer = weights[i, :]\n",
    "    plt.hist(weights_layer, bins=500)\n",
    "    weights_std = np.std(weights_layer)\n",
    "    weights_mean = np.mean(weights_layer)\n",
    "    plt.axvline(weights_mean + weights_std, color='r')\n",
    "    plt.axvline(weights_mean - weights_std, color='r')\n",
    "    plt.title(f\"Mean: {weights_mean}   |   STD: {weights_std}\")\n",
    "    plt.yscale(\"log\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stack = LifoQueue()\n",
    "edge_stack = LifoQueue()\n",
    "stack.put(root)\n",
    "rule_counter = 0\n",
    "root.reset()\n",
    "while not stack.empty():\n",
    "    node = stack.get()\n",
    "    if node.is_leaf():\n",
    "        print(f\"============== Rule {rule_counter} ==============\")\n",
    "        for stack_node, cond in zip(stack.queue, edge_stack.queue[1:]):\n",
    "            print(repr(stack_node.get_condition(attr_names)) + cond)\n",
    "            print()\n",
    "        \n",
    "        rule_counter += 1\n",
    "        edge_stack.get()\n",
    "        continue\n",
    "          \n",
    "    if node.left is not None and not node.left.visited:\n",
    "        stack.put(node)\n",
    "        stack.put(node.left)\n",
    "        node.left.visited = True\n",
    "        edge_stack.put(' < 0')\n",
    "        continue\n",
    "        \n",
    "    if node.right is not None and not node.right.visited:\n",
    "        stack.put(node)\n",
    "        stack.put(node.right)\n",
    "        node.right.visited = True\n",
    "        edge_stack.put(' > 0')\n",
    "        continue\n",
    "        \n",
    "    if node is not root:\n",
    "        edge_stack.get()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
